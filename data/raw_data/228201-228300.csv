question_id,title,body,tags
4721452,"Doubt in solving the initail value problem $2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2}$?","The initial value problem $2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2}$ has (a) unique solution (b) more than one but finitely many solutions (c) Infinitely many solutions (d) no solutions If I solve $2x\frac{dy}{dx}=3(2y-1),y(0)=\frac{1}{2}$ the equations using variable seperable, I get $\log \frac{|2y-1|}{|x^3|}=c$ implies $e^c=\frac{2y-1}{x^3}$ but how should I move further. Cannot use $y(0)=\frac{1}{2}$ to obtain $c$ , it will require to put zero in the denominator. How should I move further? Thanks in advance!!",['ordinary-differential-equations']
4721527,Proving Fatou's Lemma using the limit inequalities of measures,"Let $(\Omega,\mathcal A, \mu)$ be a measure space. Let $\left(A_i\right)_{i\in\mathbb{N}}$ be a sequence of sets in $\mathcal A$ . Then it holds that: $$
\mu\left(\liminf_{n\rightarrow\infty}{A_n}\right)\le\ \liminf_{n\rightarrow\infty}{\mu\left(A_n\right)}\tag{1}
$$ Is further $\mu\left(\bigcup_{i\in\mathbb{N}}A_i\right)<\infty$ , then it also holds that $$
\mu\left(\limsup_{n\rightarrow\infty}{A_n}\right)\geq\limsup_{n\rightarrow\infty}{\mu\left(A_n\right)} \tag{2}
$$ I want to use (1) and (2) to prove the generalized Fatou's Lemma: Let $\left(f_n\right)_{n\in\mathbb{N}}$ a sequence of ( $\mathcal A$ - $\mathcal B(\bar{\mathbb R})$ )-measurable functions $f_n:\Omega\to[0,\infty]$ . If there exists an integrable $f:\Omega\to[0,\infty]$ such that $\int_\Omega f \, d\mu<\infty$ and $\forall n: \quad f_n\le f$ , then we have $$ \int_{\Omega}\lim{\inf_{n\rightarrow\infty}{f_n}}\, d\mu \le\lim{\inf_{n\rightarrow\infty}{\int_{\Omega} f_n}}\, d\mu 
\le\lim{\sup_{n\rightarrow\infty}{\int_{\Omega} f_n}}\, d\mu
\le
\int_{\Omega}\lim{\sup_{n\rightarrow\infty}{f_n}}\, d\mu  
\tag{3}$$ The intuitive approach seems to be to construct a pointwise convergent sequence of simple functions for each $f_n$ . However, both constructing the sequence, as well as arguing that we can later on swap the limit of the sequence with the limit inferior seem rather technical. Here is an approach I tried using the common approximation of a function by simple functions: Let $g≔\liminf_{n→∞}f_n$ , and $$
g_n\left(x\right)≔n⋅\mathbb{1}_{g^{-1}[n,∞)}(x)+\sum_{j=0}^{n⋅2n-1} \frac{j}{2^n} \mathbb{1}_{ g^{-1}([\frac j{2^n},\frac{j+1}{2^n}))}(x)
$$ Since $f_n\geq0$ , we have that $\lim{\inf_{n\rightarrow\infty}{f_n}}\geq0$ , and by construction then $\left(g_n\right)_{n\in\mathbb{N}}$ is a monotonously increasing sequence that is pointwise convergent against $g$ . Therefore we have: $$
\int_{\Omega}\lim{\inf_{n\rightarrow\infty}{f_n}}d\mu=\int_{\Omega}\lim_{k\rightarrow\infty}{g_k}d\mu
$$ Using monotonous convergence: $$
=\lim_{k\rightarrow\infty}\int_{\Omega} g_kd\mu
$$ Substituting in $g_k$ : $$
=\lim_{k\rightarrow\infty}\int_{\Omega}{k\cdot\mathbb{1}_{{\ g}^{-1}\left(\left[k,\infty\right)\right)}+\sum_{j=0}^{k\cdot2^k-1}{\frac{j}{2^k}\mathbb{1}_{{\ g}^{-1}\left(\left[\frac{j}{2^k},\frac{j+1}{2^k}\right)\right)}}}d\mu
$$ Here, I now would like to go  from $\mathbb{1}_{({\lim{\inf_{n\rightarrow\infty}{f_n}}})^{-1}\left(\left[...\right)\right)}$ to $\mathbb{1}_{{\lim{\inf_{n\rightarrow\infty}\big(f_n^{-1}}}\left(\left[...\right)\right)\big)}$ But since $[...)$ isn't a compact interval, there is no set inclusion between the first and second indicator sets.",['measure-theory']
4721538,Show that $\int_0^\pi \frac{x\cos x \sinh x}{\sin^2x+\sinh^2x}dx=\frac{\pi}{2}\ln\left(\frac{e^\pi+1}{e^\pi-1}\right)$,"I am trying to show that $$I=\int_0^\pi \frac{x\cos x \sinh x}{\sin^2x+\sinh^2x}dx=\frac{\pi}{2}\ln\left(\frac{e^\pi+1}{e^\pi-1}\right)$$ I know there is an antiderivative in terms of dilogarithms and logarithms, but this nice closed form makes me think there is a clever way to get this result for these specific bounds of integration. So please avoid posting proofs with the antiderivative. What I managed to do for now is to notice that $$I=-\Im\int_0^\pi\frac{x}{\sin(x(1+i))}dx$$ which you get by simple computation. From here how to proceed? I'm not sure if the substitution $t=x(1+i)$ is allowed here, since I think this would become a problem of contour integration, which I usually avoid. Here is my question: Is there a way to obtain this result without using the antiderivative and contour integration?","['integration', 'definite-integrals', 'real-analysis', 'complex-analysis', 'contour-integration']"
4721553,Harmonic ratio in a parabola and a peaked kissing circle,"Using the GeoGebra program a few days ago, I came up with a wonderful feature about the parabola and the circle kissing its peak. I don't know if it is new or previously discovered. Please, if it was discovered previously, put a reference mentioning it in the comments, and in any case, can anyone prove it
I don't think a lot of words are needed, this picture includes intuition","['conic-sections', 'geometry', 'reference-request']"
4721558,Find all functions whose convolution is the same as their square.,"Find all functions who's self-convolution is the same as their square. To make it explicit, find all $f:\mathbb{R} \rightarrow \mathbb{R}$ such that: $$
\int_{-\infty}^{\infty} f(t - \tau)f(\tau) d\tau = (f(t))^2
$$ Or the same problem on a limited domain $f:(-a,a) \rightarrow \mathbb{R}$ $$
\int_{-a}^{a} f(t - \tau)f(\tau) d\tau = (f(t))^2
$$ Edit:
Anne Bauval has asked for a motivation for this problem.  If you consider any non-linear ODE in 1D with derivatives beyond the first, there are no simple methods of solution.  For example: $$x''' + (x'')^2 + x^2 x' = 0$$ The fourier transform of these types of problems become polynomials of multiplication and convolution for example: $$(i\omega)^3\hat{x}(\omega) + [(i\omega)^2\hat{x}(\omega)] * [(i\omega)^2\hat{x}(\omega)] + [(i\omega)\hat{x}(\omega)] * \hat{x}(\omega) * \hat{x}(\omega) = 0$$ The above example is the simplest polynomial using multiplication and convolution. So, solving this problem would be a first step towards solving more complex convolution and multiplication polynomials. At some point, this method could allow you to solve previously unsolvable non-linear ODEs. Bruno B.: Pointed out a related, but not identical question math.stackexchange.com/q/3336615/1104384","['complex-analysis', 'convolution', 'functional-analysis', 'ordinary-differential-equations']"
4721560,Hint for showing that the equilibrium of a nonlinear system is a center,"Consider the second-order nonlinear dynamical system \begin{align*}
\dot{x}&=x^3-y\\
\dot{y}&=x-x^2y
\end{align*} The (0,0) equilibrium is obviously a center but I cannot find a way to prove this. I tried using some reversibility argument but this does not seem to work. Certainly the invariance of the equations under the transformation $\bar{x}=-x$ , $\bar{y}=-y$ , $\bar{t}=t$ is obvious but is this sufficient? EDIT: My original claim (which was based on the streamplot diagram) was wrong. As pointed by @Artem the equilibrium is an unstable focus .","['stability-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4721568,"Let $f(x)$ be a function on $(-\infty, \infty)$ and $f(x+2)=f(x-2)$. Find number of roots of $f(x)=0$ in $(-8,10]$ with given condtion","Let $f(x)$ be a function on $(-\infty, \infty)$ and $f(x+2)=f(x-2)$ . If $f(x)=0$ has only three real roots in $[0,4]$ and one of them is $4$ , then the number of real roots of $f(x)=0$ in $(-8,10]$ is My Solution: $f(x+2)=f(x-2)$ Replace $x$ by $x+2$ in given functional equation Hence I obtained $f(x+4)=f(x)\implies$ $f(x)$ is periodic with period $4$ Since $f(x)=0$ has $3$ real roots in $[0,4]$ It is given that $f(4)=0\implies f(0)=f(4)=0$ and one roots in between $[0,4]$ $\implies$ in interval $(-8,10]$ it can have either $8$ or $9$ real roots. But given answer is $9$ and reasoning given is that $f(x)=\sin\left(\dfrac{\pi x}{2}\right)$ satisfies all condition. Author of book is considering $f(2)=0$ but How can he be sure that $f(2)=0$","['calculus', 'functions', 'algebra-precalculus']"
4721579,Convergence in distribution of two sequences,"Let $\left\{\xi_n\right\}_{n = 1} ^ \infty, \left\{\eta_n\right\}_{n = 1} ^ \infty, \left\{\zeta_n\right\}_{n = 1} ^ \infty$ be sequences such that: $$
\xi_n \xrightarrow{d} \xi\\
|\xi_n - \eta_n| \leq \zeta_n|\xi_n|\\
\zeta_n \xrightarrow{\mathbb{P}}0
$$ then why does $\eta_n \xrightarrow{d} \xi$ , where $\xrightarrow{\mathbb{P}}$ is a convergence in probability and $\xrightarrow{d}$ is a convergence in distribution?
Unfortunately, I did not have any useful ideas for solving this problem. I'd be grateful if someone could give me a hint as I don't quite understand where to start.","['statistics', 'probability']"
4721582,The golden ratio in a parabola,"It is nice that the golden ratio appears automatically when we are not looking for it. This is what happened to me when I was using GeoGebra and trying to solve a different problem that occurred to me: Data: $C$ parabola Featured F and L, Point A moving freely on L, Point B represents Muscat-based F on L, Point M represents Intersection of a F widget with C. Required: Find the appropriate position for a point A which makes AM=FB. My solution: Was it previously discovered or not?
In any case, how do we prove that?","['golden-ratio', 'euclidean-geometry', 'conic-sections', 'geometry']"
4721585,"$\forall\lambda\in \Lambda $, $\int_{F_\lambda}f\ge1$. Prove that $\int_{\cap F_\lambda} f\ge 1$","Suppose f is a non-negtive measurable function on $[a,b]$ , and $\left \{ F_{\lambda}\right \}_{\lambda\in\Lambda} $ is a family of closed sets in $[a,b]$ such that $\forall \lambda_1,\lambda_2\in\Lambda$ ,we have either $F_{\lambda_1}\subset F_{\lambda_2}$ or $F_{\lambda_2}\subset F_{\lambda_1}$ . Also $\forall\lambda\in \Lambda  $ , $\int_{F_\lambda}f\ge1$ . Prove that $\int_{\cap F_\lambda} f\ge 1$ . If $\Lambda$ is finite or countable,then it's easy to cope with. But if $\Lambda$ is uncountable, it's becoming hard to deal with.I think if we can choose a sequence of decreasing closed sets $\left \{ F_n\right \} $ so that $\forall \lambda,\exists n_0 $ , $F_{n_0}\subset F_\lambda$ and $\cap F_\lambda=\cap F_n=\lim\limits_{n\to\infty}F_n$ .But I don't know whether we can select such a sequence from $\left \{ F_{\lambda}\right \}_{\lambda\in\Lambda} $ . Any help will be appreciated.","['measure-theory', 'analysis', 'real-analysis']"
4721601,"Any Strategies to Factorise Expressions (, Faster)?","I recently, while problem -solving came accross various circumstances where I must factorise expressions and proceed. These things are generally taught to high school/middle school students. However, it was up until recently, I found huge expressions say, $$p^4-(x+2y+1)p^3+(x+2y+2xy)p^2-2xyp=0$$ Now, this really looks simple and it is! It is nothing but a polynomial in $p.$ Things turn out more simple, if we observe $1$ and $0$ as roots of the polynomial. Then, dividing the given expression by $p(p-1)$ we get, $(p-x)(p-2y),$ and we are done. So, $$p^4-(x+2y+1)p^3+(x+2y+2xy)p^2-2xyp=p(p-1)(p-x)(p-2y).$$ This was easy. Now, I encountered another expression, $$xyp^2+(x^2+xy+y^2)p+x^2+2y=0$$ This is where things start to get troublesome, if not complicated. To be honest, I had a hard time dealing with this. I broke up all the products and found a huge expression: $$xyp^2+x^2p+xyp+y^2p+x^2+2y=0$$ I tried all possible ways, of grouping the terms say, by $(xyp^2,x^2p)$ , anticipating that we would get $(p+1)$ as a factor but then, it was not the case. This turned out to be misadventure.  Next, I tried grouping $(xyp^2,xy)$ and this time, I hoped $(p^2+1)$ comes up in each grouping, but again, things went down the hill. Next, I tried grouping $(xyp^2,xyp,xy)$ . My intuition for this, was simply that $xy$ was common in these $3$ terms and I felt it might be a good idea to start by grouping these terms. But it was not fruitful. The thing was indeed deceptive. Suddenly, $15$ minutes later,  it struck me that $yp+x$ is a factor of this ""thing."" Then things turned simpler and I was able to factorise the expression as $$xyp^2+(x^2+xy+y^2)p+x^2+2y=(xp+x+y)(yp+x).$$ Another expression was $$(x^2+x)p^2+(x^2+x-2xy-y)p+y^2-xy=0$$ This was probably the hardest. I was completely tired of grouping stuffs and factoring out things. I gave up, and found the answer to be, $[(x+1)p-y][xp+x-y]=0,$ using a calculator. This was least expected by me! Now, of course the question ,""Factorise this expressions..."" were not given to me. They are pretty much intended towards middle/high-school students. But this terms came out while solving a problem, or better say, while solving a problem, I found that factorising some expressions would be really helpful and would simplify the approach. However, the thing is, I want to know, if there are any standard "" tricks "" to factorise expressions. Since this was a part of a numerical calculation, I shouldn't ideally get bogged down with these stages of it. It would be really a waste of time. Any standard/commonly known strategies to factorise these sort of expressions(which I mentioned above) if shared will be much helpful in my case.","['algebra-precalculus', 'factoring', 'problem-solving']"
4721608,"Proving that every torsion-free Abelian group $(A, +)$ can be linearly ordered","Show that every torsion-free Abelian group (A, +) can be linearly ordered so
that $$(a < b) ∧ (c \le d) → (a+c < b+d).$$ (Hint: First show this for finitely generated groups. Then use compactness.) I got this question in logic class and I have yet to partake in a group course, my proffesor gave me this hint:
Any finitely generated torsion-free Abelian group is a direct sum of a finite number of groups of integers ( $Z$ ,+) (which I don't understand how can it help me here..). I am just wondering if my attempt makes sense:
given $A$ which is a finite generated torsion-free Abelian group let $$(g_1,g_2,....,g_n)$$ be $A$ 's basis, let $v$ belong to $A$ then exist $$\beta_1,\beta_2,\ldots,\beta_n \in F$$ such that $$v = \sum_{i=1}^n g_i*\beta_i$$ lets create a function from $A$ to $R^n$ lets call her $f$ such that for any $v$ in $A$ $$f(v) = (b_1,b_2,\ldots,b_n),$$ I am sending v to its coordinate vector.
Then I will create a linear order $R$ as follows: $$aRb = f(a) \le f(b).$$ I have two probeloms: first lets say I define $\le$ as Product order relation , then does it work for a vector with n coordinates? and if it does and I managed to find a relation that linearly orders a finite generated group $A$ then how does compactness theorm help me? Can I claim that given B which is torsion-free Abelian group such that $A \subseteq B$ , since $A$ is general finite generated group who has a model for that linear order then by compactness therom $B$ also has a model? Thx in advance any help would be appreciated!","['order-theory', 'group-theory', 'abelian-groups', 'logic']"
4721609,Convergence in distribution to the derivative,"Let $f(x)$ has a derivative in point $x = 0$ . Then if for sequences of random variables $\left\{\xi_n\right\}_{n = 1} ^ \infty, \left\{\eta_n\right\}_{n = 1} ^ \infty$ we have: $$
\xi_n\eta_n \xrightarrow{d} \eta\\
\eta_n \xrightarrow{\mathbb{P}} 0
$$ then $\xi_n(f(\eta_n) - f(0)) \xrightarrow{d} f'(0)\eta$ . My solution looks something like this:
By the hereditary convergence theorem for continuous function $h(x) = 1/x$ i can write $\frac{1}{\xi_n\eta_n} \xrightarrow{d} \frac{1}{\eta}$ . Since $\eta_n \xrightarrow{\mathbb{P}} 0$ , then $\eta_n \xrightarrow{d} 0$ . So, i can write by Slutsky's Theorem that $\frac{\eta_n}{\xi_n\eta_n} \xrightarrow{d} 0$ . Then i use following theorem. Let $\xi_n \xrightarrow{d} \xi$ . For a function $h(x)$ differentiable at a point and a sequence $b_n \rightarrow 0, b_n \neq 0$ , the following holds: $$
\frac{h(a + \xi_nb_n) - h(a)}{b_n} \xrightarrow{d} h'(a)\xi
$$ where I put $b_n = \frac{1}{\xi_n}$ and $\xi_n = \xi_n\eta_n$ But I'm not sure about this solution because the function $\frac{1}{x}$ is not continuous at zero. I would be very grateful for the hint","['statistics', 'probability']"
4721665,How are two definitions of quasi-convexity equivalent? [duplicate],"This question already has an answer here : Difference in Definitions of Quasiconvexity (1 answer) Closed last year . Why are these two definitions equivalent? (Assuming $C$ is convex) $f: C \rightarrow \mathbb{R} $ is quasi-convex if for any $x, y \in  C$ and $\lambda \in [0,1]$ , $$ f((1-\lambda)x + \lambda y) \leq \max\{f(x), f(y)\} .$$ $f: C \rightarrow \mathbb{R} $ is quasi-convex if for any $\alpha \in  \mathbb{R}$ , $\operatorname{Lev} (f, \alpha) $ is convex. Where $\operatorname{Lev} (f, \alpha) $ is defined as: $$ \{x \in S: f(x) < \alpha \}$$ I was successful to show that if we have the first definition, it implies the second one, but got stuck on the reverse side. assume $ x, y \in \operatorname{Lev} (f, \alpha) $ so: $$f(x) < \alpha,$$ $$f(y) < \alpha.$$ Then we have: $$ max\{f(x), f(y)\} < \alpha $$ So by the first definition, we have: $$ f((1-\lambda)x + \lambda y) < \alpha $$ Which implies that: $$ ((1-\lambda)x + \lambda y) \in \operatorname{Lev}(f, \alpha) .$$","['optimization', 'convex-analysis', 'analysis', 'real-analysis']"
4721693,Rank of a matrix with $a_{ii}=0$ and $a_{ij}+a_{ji}=1$ [duplicate],"This question already has an answer here : $a_{ii}=0, a_{ij}+a_{ji}=1, 1\leq i<j\leq n$. This matrix has rank $\geq n-1$? [closed] (1 answer) Closed 11 months ago . Let $A=(a_{ij})$ and an $n\times n$ real matrix such that $$a_{ii}=0~(1\le i\le n),\quad a_{ij}+a_{ji}=1~(1\le i<j\le n).$$ Then prove that ${\rm rank}(A)\ge n-1 $ . This is a problem I saw in an exercise book. It is clear that $A+A^{\rm T}$ is a matrix with all diagonal entries $0$ and off-diagonal entries $1$ . Can we get the ${\rm rank}(A)$ from it? Or how can we prove it in other ways? Thanks.","['matrices', 'matrix-rank', 'linear-algebra']"
4721705,Advanced calculus book recommendations.,"What calculus books should I use to learn more advanced calculus after Stewart's, Larson's book and Thomas' book? I just finished a calculus $3$ course and I want to learn more about calculus. Also, it seems that famous calculus book like Thomas', Larson's and Stewart's books are considered basic and elementary books and they don't cover many topics in calculus like special functions, proofs of many theorems and rigorous arguments, etc.I figured that rigour and proofs is in a separate course called Real analysis and I found some good sources to learn it, but I don't want to learn rigorous mathematics yet,I want a book that has more theorems of calculus that elementary books like Stewart didn't cover like (the proof of $\pi$ is irrational, more techniques of integrals and special functions, etc ...) or cover them in more detail and depth and I am not sure what books to use to learn more about these topics  or to learn more advanced calculus.","['integration', 'book-recommendation', 'reference-request', 'calculus', 'derivatives']"
4721756,Solve the following differential equation $x^2\frac{d^2y}{dx^2}+4x\frac{dy}{dx}+2y=e^x.$,"Solve the following differential equation $x^2\frac{d^2y}{dx^2}+4x\frac{dy}{dx}+2y=e^x.$ I tried solving this problem as follows: Given, $x^2\frac{d^2y}{dx^2}+4x\frac{dy}{dx}+2y=e^x.$ This is a Cauchy-Euler Equation because it is of the form $$\frac{d^ny}{dx^n}+ p_1x^{n-1}\frac{d^{n-1}y}{dx^{n-1}}+p_2x^{n-2}\frac{d^{n-2}y}{dx^{n-2}}+\cdots +p_{n-1}x\frac{dy}{dx}+p_ny=X,$$ where the $p_i's$ are constants and $X$ is a function of $x.$ We try to change the variables from $x$ to $z$ by using the transformation $x=e^z\implies z=\log x.$ Now, $\frac{dy}{dx}=\frac{dy}{dz}\frac{dz}{dx}=\frac{dy}{dz}\frac 1x,\frac{d^2y}{dx^2}=\frac{d(\frac{dy}{dz}\frac 1x)}{dx}=\frac 1x\frac{d(\frac{dy}{dz})}{dx}-\frac1{x^2}\frac{dy}{dz}=\frac{1}{x^2}\frac{d^2y}{dz^2}-\frac{1}{x^2}\frac{dy}{dz}=\frac{1}{x^2}(\frac{d^2y}{dz^2}-\frac{dy}{dz})$ . Using these values we have, $x^2\frac{d^2y}{dx^2}+4x\frac{dy}{dx}+2y=e^x\implies \frac{d^2y}{dz^2}+3\frac{dy}{dz}+2y=e^{e^z}.$ But here's the problem, I successfully converted the given differential equation into a linear differential equation with constant coefficients. But the problem arises when we try to calculate the particular integral as follows: We have, $\frac{d^2y}{dz^2}+3\frac{dy}{dz}+2y=e^{e^z}.$ Hence, if $f(D)$ is the differential operator then, in this case $f(D)=D^2+3D+2$ so, $f(D)y=e^{e^z}.$ Now, $y_p=\frac{1}{f(D)}e^{e^z}\implies y_p=\frac{1}{f(D)}e^{e^z},$ where $y_p$ is the particular integral. But I don't understand how to evaluate $y_p=\frac{1}{f(D)}e^{e^z}.$ I know that, $\frac{1}{f(D)}e^{ax}=\frac{1}{f(a)}e^{ax}$ , but this doesn't seem to apply here. Any help regarding the way to evaluate this, will be greatly appreciated.","['calculus', 'ordinary-differential-equations']"
4721773,Calculate the value of $f(4)$ given the following conditions (Korean SAT),"This problem comes from the Korean SAT (수능) last year: The translation reads: $f(x)$ is a cubic function with leading coefficient $1$ , and $g(x)$ is continuous under real number, and they satisfy the following conditions. Calculate $f(4)$ . for all real number $x$ , $f(x) = f(1) + (x-1)f'(g(x))$ function $g(x)$ has minimum value of $\frac{5}{2}$ $f(0) = -3, f(g(1)) = 6 $ My attempt Let $ g(1) = k $ and $ f(x) = x^3 + ax^2 + bx -3 $ from $ f(0) = -3 $ Then, $$f'(x) = 3x^2 + 2ax + b $$ and from first condition in the list we have $$\dfrac{f(x) - f(1)}{x-1} = f'(g(x)) $$ We can take limit on both sides: $$\lim_{x \to 1}\dfrac{f(x) - f(1)}{x-1} = \lim_{x \to 1} f'(g(x)) $$ The left hand side is just $f'(1)$ while the right hand side can be written $$  f'( \lim_{x \to 1} g(x)) = f'(g(1))  = f'(k) = f'(1) $$ Because of $g$ is continuous under real. But $k = g(1)$ cannot be $1$ because $g(x)$ has a minimum value of 5/2. Thus $f(x)$ draws a positive cubic function, and at $(1,f(1))$ and $(k,6)$ it draws two parallel tangent lines, with $ k > 1 $ . Also, $f'(x) $ draws a positive quadratic function with equal values at $x = 1$ and $x = k$ . But because quadratic functions are symmetric about its axis, $f'(x)$ is symmetric about $x = -\dfrac{a}{3}$ . And from these we can conclude $$2a = -3(k+1)$$ But here the problem remains quite elusive, because we have three variables $a,b,k$ we only have 2 relations, the other being: $$f(k) = k^3 + ak^2 + bk -3 = 6 \rightarrow k^3 + ak^2 + bk = 9 $$ One could ask what about $$f'(k) = f'(1) = 3k^2 + 2ak + b = 3 + 2a + b$$ but unfortunately this relation reduces to identity when combined with $2a = -3(k+1)$ above.","['calculus', 'derivatives', 'algebra-precalculus']"
4721837,A Tough Series: $\sum_{n=0}^\infty2^na_{n+1}\sqrt{a_n}=\frac{\Gamma^2(\frac14)-4\cdot\Gamma^2(\frac34)}{8\sqrt{2\pi}}$,"If $\displaystyle a_0=\frac12$ and $\displaystyle a_{n+1}=\frac{1-\sqrt{1-a_n}}{1+\sqrt{1-2a_n}}$ , show that $$\sum_{n=0}^\infty2^na_{n+1}\sqrt{a_n}=\frac{\Gamma^2(\frac14)-4\cdot\Gamma^2(\frac34)}{8\sqrt{2\pi}}$$ The closed form for this series involves the square of gamma function, hence I try to connect it with integrals, which requires us first to find the explicit form from the recursion equation. But this recusion equation is highly non-linear. I try to multiply $1-\sqrt{1-2a_n}$ to rationalize the denominator but seems no help. I also tried some non-linear sub, such as $$\tan(x-y)=\frac{\tan x-\tan y}{1+\tan x\tan y}$$ where $x=\frac\pi4, \tan y=\sqrt{1-a_n}$ , but the $2a_n$ term inside the square root kills this attempt. If let $a_n=\sin^2\theta_n$ , then we get $$1-\cos^2\theta_{n+1}=\sin^2\theta_{n+1}=\frac{1-\cos\theta_n}{1+\sqrt{\cos2\theta_n}}$$ Is there any hint? Thank you!","['integration', 'recurrence-relations', 'real-analysis', 'elliptic-functions', 'sequences-and-series']"
4721865,Find all continuous functions from $\mathbb{R}$ to $\mathbb{R}$ such that $(f(x+y))^2-(f(x-y))^2=4f(x)f(y)$,"I encountered the following question in a math-contest Find all  continuous functions from $\mathbb{R}$ to $\mathbb{R}$ such that $$(f(x+y))^2-(f(x-y))^2=4f(x)f(y).$$ I substituted $y$ by $0$ to get that either $f$ is the zero function or the image of $0$ by $f$ is $0$ . Now I found one solution (the zero function). Moving to the second case I substituted $y$ by $x$ to get $f(2x)^2=4f(x)^2$ . I separated the two subcases: Either $f(2x)=2f(x)$ Or $f(2x)=-2f(x)$ . Also flipping $x$ and $y$ gives us the fact that $f$ is an odd function. However I couldn't get any further.
I hope you would like to help me finish the solution of this problem.","['contest-math', 'continuity', 'functions', 'functional-equations']"
4721879,Counterexample: locally compact non-Hausdorff space where open sets are sigma-compact and compact sets are closed?,"Question. Does there exist a topological space $(X, \tau)$ which simultaneously satisfies all four of the following criteria? It is locally compact , meaning every point $x \in X$ has a compact neighbourhood. Every compact set is closed. Every open set $\mathcal{U} \in \tau$ is σ-compact , i.e., can be written as an at most countable union of compact sets. It is non-Hausdorff . No finite counterexample can exist. For then all subsets are compact, therefore closed, yielding the metrisable discrete topology. The cocountable topology on an uncountable set would satisfy 2. and 4., but neither 1. nor 3. hold. For example, 3) fails because precisely finite sets are compact which makes writing the uncountable $X$ unfeasible with anything other than an uncountable union. Any ideas? The question here is self-posed. It comes from an analysis of a proof in introductory measure theory. A certain regularity result with Hausdorffness as an assumption seems to only use the weaker assumption that all compact sets be closed. The details do not matter, but essentially the question is whether any generality would be gained by switching Hausdorfness with assumption 2.","['general-topology', 'examples-counterexamples', 'measure-theory', 'compactness']"
4721941,How to derive zeroth order bessel function as solution to exponential of sin/cos,"I've come across an integral which I'm stumped on how the solution was reached.  I want to know how it was derived so I can understand if it's possible to vary the limits of integration in the definite integral. $\int_0^{2 \pi} \exp(j A \cos(\theta))\ d\theta = 2\pi J_0(A)$ (and for $\sin$ in place of $\cos$ ) which can be extended to: $\int_0^{2 \pi} \exp(j A \cos(\theta))\exp(j B \sin(\theta))\ d\theta = 2\pi J_0(\sqrt{A^2 + B^2})$ I've seen these (or similar forms) listed in ""Tables of Integrals, Series, and Products"" by  Gradshteyn and Ryzhik (who reference Bierens de Haan, D., Nouvelles tables d^integrales definies. Amsterdam, 1867), and Wolfram gives me the same answer. I understand there's a use of the identity for Bessel function first kind and modified Bessel function first kind, but I'm not sure how the first integral is achieved.  Any insight into how the first integral is derived would be much appreciated. EDIT: How to solve integral $\int_0^{2\pi} e^{i(a\cos\phi + b\sin\phi)} \cos\phi\ d\phi$ shows me how to effectively get the second integral.  Does anyone know how the first integral is derived?  I'm curious to know if I can integrate from $0$ to $\frac{\pi}{3}$ in a situation where I have 6-fold rotational symmetry?","['definite-integrals', 'derivation-of-formulae', 'trigonometry', 'exponential-function', 'bessel-functions']"
4721985,Maximum volume enclosed by a piece of cloth in the shape of a unit circle,"I have a piece of cloth in the shape of a unit circle (or disk), a needle and thread. I want to stitch the cloth together so that it can contain stuff (say, lots of small beans) and the stuff cannot escape. What is the maximum volume that the cloth can contain? I guess the optimal shape will be made by folding the cloth in half into a double-layered semicircle, then stitching along the arc (sort of like a dumpling, but with an infinitesimally narrow sealed strip). I suppose we could physically make such a shape, stuff it with small beans, then pour the beans into another container that can measure volume. But is there a mathematical way to answer this question? A crude upper bound for the maximum volume can be obtained by (erroneously) assuming that the cloth can be shaped into a sphere without wasting any of the cloth. Then the radius of the sphere, $r$ , would satisfy $4\pi r^2=\pi$ , so the volume of the sphere, i.e. the crude upper bound, would be $\pi/6\approx0.524$ . I did an internet search and found how to maximize the volume enclosed by a square net ( here , here and a mathoverflow question ), and an article called Profiles of inflated surfaces . From the comments, we have the paper bag problem . But I haven't seen the shape in this question. It seems that my question may be quite difficult; perhaps we can only find lower and upper bounds.","['volume', 'circles', 'geometry', 'solid-geometry', 'optimization']"
4721987,Definite integral of infinite product,"I have been struggling for a while on evaluating this definite infinite product integral: $$\int_{-\frac{\pi}{4}}^0(1+\tan{x})(1+\tan^2x)(1+\tan^4x)(1+\tan^8x)(1+\tan^{16}x)...dx$$ This is a question given by my maths teacher a while back and I have been struggling with it ever since. I have tried so many different substitutions and I have even tried integrating by parts (do NOT do this), but nothing has led me even close to an answer. I'm guessing there is trig identity I must be missing in order to simplify the inside of the integral? or some wonder substitution? Any help would be greatly appreciated.","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'trigonometric-integrals']"
4721993,Differentiability without limits,"There are four concepts which are studied in Calculus and Analysis: Convergence, Continuity, Differentiability and Integrability. In Calculus, you can define the latter three in terms of the first, but also you can define integrability without convergence using Darboux approach. In topology, you can define convergence and continuity in terms of neighborhoods, and in measure theory you can define integrability in terms of measurable functions. I wonder if there is a definition of differentiability without making any reference to the concept of limit or convergence, thank you so much by your help.","['limits', 'derivatives', 'analysis']"
4722055,Identity for convolution of central binomial coefficients: $\sum\limits_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}=2^{2n}$,"It's not difficult to show that $$(1-z^2)^{-1/2}=\sum_{n=0}^\infty \binom{2n}{n}2^{-2n}z^{2n}$$ On the other hand, we have $(1-z^2)^{-1}=\sum z^{2n}$.  Squaring the first power series and comparing terms gives us $$\sum_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}2^{-2n}=1$$ that is, $$\sum_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}=2^{2n}$$ My question: is there a more direct, combinatorial proof of this identity?  I've been racking my brains trying to come up with one but I'm not having much success.","['summation', 'combinatorial-proofs', 'convolution', 'binomial-coefficients', 'combinatorics']"
4722058,Proving $\cos(\tfrac \pi{18})$ cannot be written in the form $\frac{\sqrt a-\sqrt b}{\sqrt c -\sqrt d}$,"For reasons I'm not going to get too into, I'm interested in a number of the form $p+q \cos(\tfrac \pi{18})$ , where $p^2,q^2$ are rational. A question that occurred to me is whether or not such a representation is unique, namely could there exist some different $p',q'$ such that $p'+q'\cos(\tfrac \pi{18})$ is equal to the same value. If this were to occur, then we could write $$\cos \left(\frac \pi{18} \right)=\frac{p-p^{\prime}}{q^{\prime}-q}=\frac{\left(p-p^{\prime} \right) \left(q+q^{\prime} \right)}{q^{{\prime}^{2}}-q^2}$$ . I suspect that this is not the case, as any closed form of $\cos(\tfrac \pi {18})$ I can find online is an absolute mess. That being said, I'm not totally sure how to prove it cannot be done. I'm quite rusty with abstract algebra, so everything I am about to say might be totally wrong, but a potential approach that occurred to me is to consider field extensions. Namely, the minimal polynomial of $\cos(\tfrac \pi {18})$ over $\mathbb Q$ has degree $6$ , while $\frac{(p-p')(q+q')}{q'^2-q^2}$ can be obtained by adding (at most) four square roots to $\mathbb Q$ , which has degree some factor of $16$ . In particular, one side has a factor of $3$ , while the other does not, which leads me to believe we have some sort of contradiction, but I'm not solid enough on field extensions to suss out the details. Does this general idea work? If so, could someone possibly help me fill in the missing details on how to finish; if not, are there any other approaches that could work?","['algebra-precalculus', 'abstract-algebra']"
4722064,"Proving Theorem 1.16, Folland's Real Analysis","I am confused about the proof of Theorem 1.16 in Folland's Real Analysis, in particular the two statements underlined below: In the part underlined in red, the book uses both continuity from above and from below, but in my proof I only used continuity from above. I probably make mistakes and please point it out: My attempt: Since $\mu$ is continuous from above, denote $E_n := (x, x+\frac{1}{n}]$ and $\mu(E_1) < \infty$ , then we have $\mu(\bigcap_1^{\infty} E_n) = \lim_{n\to\infty}\mu(E_n)$ , i.e., $\forall \epsilon > 0, \exists \delta > 0 $ s.t. $\mu((x, x+\delta]) < \epsilon$ . Therefore, when $x \geq 0$ : $$\mu((0, x+\delta]) - \mu((0, x]) = F(x+\delta) - F(x) < \epsilon$$ When $x<0$ : $$\mu((x, 0]) - \mu((x+\delta, 0]) = F(x+\delta) - F(x) < \epsilon$$ So $\mu$ is right-continuous. In the part underlined in blue, the book says it's evident on $\mathcal{A}$ , but I don't understand why it's only on $\mathcal{A}$ (why not evident on $\mathcal{B}_{\mathbb{R}}$ ): My attempt: $a=b$ : $$\mu((a,b]) = \mu(\emptyset) = 0 = \mu_F((a,a]) = F(a) - F(a)$$ $a<b\leq 0$ : $$\mu((a,b]) = \mu((a,0]) - \mu((b,0]) = F(b) - F(a) = \mu_F((a,b])$$ $0\leq a<b$ : $$\mu((a,b]) = \mu((0,b]) - \mu((0,a]) = F(b) - F(a) = \mu_F((a,b])$$ $a \leq 0 <b$ : $$\mu((a,b]) = \mu((a,0]) + \mu((0,b]) = F(b) - F(a) = \mu_F((a,b])$$ The above are probably my misunderstanding and please feel free to corret me. Thanks!","['measure-theory', 'real-analysis']"
4722067,Is this why there can only be three pairings out of 4 people?,"I am unsure whether the below logic is accurate as to why you can only form three pairings of four people: Every pairing can be completely described by whom a single person is paired with. Given this, we could certainly use the same person to identify each pairing, therefore, the question of the number of pairings there are devolves into  how many people can be paired with a single person out of the four, which is $3.$ I am feeling shaky about this due to not being confident that we can model each of the pairings using only whom a single person is paired with; how we rigorously justify this further?","['combinatorics', 'discrete-mathematics']"
4722072,"Number of permutations of $\{1, 2, 3, \dots, n\}$ such that every number is less than or equal to the average of its $2$ neighbours (if any).","Question: Find the number of permutations of $\{1, 2, 3, \dots, n\}$ such that every number is less than or equal to the average of its $2$ neighbours (if any). My attempt: I immediately noticed that $n$ must be at the $2$ ends. This encourages us to use recursion since every time $n$ increases, it is just added to either of the $2$ ends. Let the number of such permutations be $x_n$ . Suppose that a possible sequence is $\{a_1, a_2, a_3, \dots, a_n\}$ and we want to insert $n+1$ . As I've said above, either $a_1$ is $n$ or $a_n$ is $n$ . Here's where I got stuck. Suppose that $a_1$ is $n$ . Then, we do not know whether letting $a_0=n+1$ (i.e. inserting $n+1$ in front) works, because we have to ensure that $a_1=n$ is less than or equal to the average of $a_0$ and $a_2$ , and we cannot ensure this, so I got stuck. I have no other methods except to brute-force everything. Any help will be appreciated! Thanks in advance.","['permutations', 'combinatorics', 'contest-math']"
4722090,A lower bound for the complex logarithm,"Let Log $(z)$ denote the principal branch of the logarithm for a complex number $z$ which is not real $\leq 0$ . In various textbooks on complex analysis, I found an inequality which states $$\frac{1}{2}|z-1| \leq |\text{Log}(z)|$$ for all $z$ with $|z-1| \leq 1/2$ . The inequality is given without proofs and I have absolutely no idea how to prove this. Any hints? Background: The inequality is given in the context of infinite products $\prod_n z_n$ and used to show the implication $$\sum_n \text{Log}(z_n)\text{ converges absolutely}\phantom{aaa}\Longrightarrow\phantom{aaa} \sum_n z_n-1\text{ converges absolutely}$$ where all complex numbers $z_n$ are as above (i.e. not real $\leq 0$ ).","['complex-analysis', 'complex-numbers', 'logarithms']"
4722147,Finding the closest point to a Manifold,"Hello differential geometry community! Suppose that I have a smooth Riemannian manifold $\mathcal{N}$ embedded into a Euclidean space $\mathcal{P}$ . Assume that I have two position vectors $p_1,p_2 \in \mathcal{N}$ ; s.t. $p_1 \neq p_2$ . Now, imagine the straight line segment passing through these two points in $\mathcal{P}$ called, say, $\ell$ . Let $p_k \in \ell$ . Is it possible to find the shortest line segment passing through both $p_k$ and another point in $\mathcal{N}$ ? It could also be an iterative algorithm like in Newton’s method to find the root; not necessarily a closed form mathematical expression. By the way there is not limit on the distance between $p_1$ and $p_2$ w.r.t. Euclidean norm in $\mathcal{P}$ .","['smooth-manifolds', 'differential-geometry']"
4722175,An interesting trigonometric integral,"Yesterday I took part in an entry competition to one of the MSc programs in my university, and during one of the mathematical tests, I had to solve some integral and differential equations. One of those integrals had the following trigonometric form: $$
I(0,\pi)= \int\limits_0^\pi\left[\cos^2(\cos(x))+\sin^2(\sin(x))\right]dx
$$ Unfortunately, I didn’t manage to solve it analytically during the test, but later, as I came home, I tried to evaluate it numerically using Mathematica. As it turned out, the answer seems to be $I(0,\pi)=\pi$ . Still, I do not understand what transformations or substitutions I had to perform so that to solve this problem analytically during the exam. This is exactly the reason, I am posting my question here, as I haven’t found this kind of problem even posted or solved anywhere yet, and so I am kindly asking for any help to attempt this integral analytically. Thank you in advance!","['integration', 'trigonometric-integrals', 'definite-integrals', 'contest-math']"
4722177,A power series solution to Hermite's differential equation,"I'm trying to solve the Hermite's differential equation \begin{equation*}
y''(x)-2xy'(x)+2ny(x) = 0
\end{equation*} I look for power series solutions of the form \begin{equation}
y(x) = \displaystyle\sum_{\nu=0}^\infty a_\nu x^\nu
\end{equation} Then I found the recurrence, \begin{equation}
a_{\nu+2} = \frac{2(\nu-n)}{(\nu+2)(\nu+1)} a_\nu
\label{eq:anHer}
\end{equation} for $\nu = 0,...,n$ . So i am looking for even and odd powers of x solutions. For the even solutions I have \begin{equation}
a_{2k} = \frac{4\left(k-1-\frac{n}{2}\right)}{2k(2k-1)}a_{2(k-1)}
\end{equation} for $k = 1,2,...$ In terms of $a_0$ I have, \begin{align*}
&a_2=\frac{4(-\frac{n}{2})}{2\cdot1}a_0\\
&a_4 =\frac{4\left(1-\frac{n}{2}\right)}{4\cdot3}a_2 = \frac{4^2\left(1-\frac{n}{2}\right)\left(-\frac{n}{2}\right)}{4\cdot3\cdot2\cdot1}a_0   \\
& a_6 = \frac{4\left(2-\frac{n}{2}\right)}{6\cdot5}a_4 =\frac{4^3\left(2-\frac{n}{2}\right)\left(1-\frac{n}{2}\right)\left(-\frac{n}{2}\right)}{6\cdot5\cdot4\cdot3\cdot2\cdot1}a_0 \\
\vdots\\
&a_{2k} = \frac{2^{2k}(-\frac{n}{2})_k}{(2k)!}a_0
\end{align*} where $(x)_\mu$ is the Pochhammer symbol. Then the even solutions are \begin{equation}
y_{even}(x) = \displaystyle\sum_{k = 0}^\infty a_{2k}x^{2k} = a_0\left(1+ \displaystyle\sum_{k=1}^\infty \frac{2^{2k}(-\frac{n}{2})_k}{(2k)!}x^{2k}\right)
\end{equation} in https://mathworld.wolfram.com/HermitePolynomial.html the even solutions are given by \begin{equation}
y_{2n}(x) = c_0 \left(1+\displaystyle\sum_{k=1}^n\frac{2^{2k}(-n)_k}{(2k)!}x^{2k}\right)
\end{equation} I don't see why I have $(-\frac{n}{2})_k$ instead of $(-n)_k$ . Since the series I've obtained terminates for $k = 1+\frac{n}{2}$ maybe I can truncate the series and write my series in this form but I'm not sure. Or maybe is just because the normalization of $c_0$ given in the previous link, which is $c_0= (-1)^n2^n(2n-1)!!$ , while my $a_0$ is a general normalization constant without a particular expression so is a ''more general'' solution.","['power-series', 'hermite-polynomials', 'ordinary-differential-equations']"
4722185,Existence of a weird cumulative function,Does it exist a non discrete random variable with CDF $F$ with the following hypothesis : $F$ is constant in a neighborhood of each point of continuity I had the idea of Cantor function but it does not work in my case...,"['cumulative-distribution-functions', 'probability']"
4722187,Computing a limit of a probability using CLT,"Suppose $X\sim\mathsf{Bin}\left({18},\frac13\right)$ and $Y\sim\mathsf{Bin}\left({m},\frac13\right)$ independent random variables, compute: $$\displaystyle{ \lim_{m\to\infty}\mathbb{P}\left[X\leq t-Y\right] }$$ while $t=\frac{m}{4}.$ I've set $Z=X+Y$ and then said that $Z$ is a sum of $m+18$ independent bernouli rv's with $p=\frac{1}{3}$ but then I've tried using the central limit theorem but since $t=\frac{m}{4}$ I cant get the thing inside the probability to look like $\frac{S_n-n\mu}{\sigma\sqrt{n}}\leq k$ for some $k$ .","['central-limit-theorem', 'probability-theory']"
4722210,Prove that $a$ commutes with every element of the group $G$.,"Let $G$ be a finite group having an odd number of elements. Suppose $a$ is an element of $G$ of order $3$ such that the cyclic subgroup $ H$ generated by $a$ is normal in $G$ . Prove that $a$ commutes with every element of $G$ . My attempt: $|a| = 3$ , so $H= \{ e, a,a^2\}$ Let $g \in G$ be arbitrary element. Now $H$ is normal in $G$ , so $gag^{-1} \in H$ In particular, $ga\in \{g,ag,a^2g\}.$ If $ga = g$ , then $a=e$ (contradiction) If $ga = ag$ , then $a$ commutes with $g$ , so we are done. If I can show that $ ga \ne a^2g$ then we get our required result. I think I need to use the order of the group, but couldn't understand how to use that. So any hint/solution to show $ ga \ne a^2g$ or any better way to solve this problem will be helpful for me. Thanks in advance.","['normal-subgroups', 'group-theory', 'abstract-algebra', 'finite-groups']"
4722252,Bijection between $\mathbb{N}$ and the set of finite parts of $\mathbb{N}$,"Let $\mathbb{N} = \{0,1,2,3,...\}$ be the set of natural numbers (with $0$ ) and $\mathbb{F}$ the set of finite parts of $\mathbb{N}$ . I want to find a bijection, as simple as possible, between $\mathbb{N}$ and $\mathbb{F}$ . One that I have is $$f : \mathbb{F} \to \mathbb{N}, s \mapsto \sum_{i\in s} 2^i$$ with the convention that the sum over $\emptyset$ is $0$ . Do you have other examples which are perhaps more elementary ?","['elementary-set-theory', 'arithmetic', 'natural-numbers']"
4722310,Is the following proof of the Du Bois-Reymond lemma valid?,"I am suspicious about the validity of the following proof. I have two doubts that are bugging me: 1) If the proof, is it true that $\psi \in D_0^1([a,b];\mathbb{R}^n)$ ? I mean $\psi$ is the integral of $u$ : $\psi = \int_a^x (u(s)-c)ds$ which is is piecewise-continuous, so in the eventual discontinuity points,  also the integral is discontinuous, isn't it? So why would the $\psi$ be necessarily continuous? Note : $D_0^1([a,b];\mathbb{R}^n)$ means functions in $D^1([a,b];\mathbb{R}^n)$ that  vanish the endpoints. And $D^1([a,b];\mathbb{R}^n)$ is the class of  piecewise continuously differentiable functions (which are continuous by definition): 2)   Is the statement actually correct? I mean,everywhere I look I can just find a version of the lemma for $u$ continuous and not a single one for $u$ piecewise continuous as in my case. Where exactly in the proof is that hypothesis used? (I guess only in the step regarding my first question only?) Do you know where can I find the proof for my case?(In case this one is correct, it is missing the first step that they argue follows by an approximation argument) Or can you provide one?","['functional-analysis', 'analysis', 'calculus-of-variations']"
4722350,Understanding a step in: find the minimum value of $f(x)=\left|x-1\right| + \left|2x-1\right| + \left|3x-1\right| + \cdots + \left|119x - 1 \right|$.,"Problem statement : What is the minimum value of $f(x)=\left|x-1\right| + \left|2x-1\right| + \left|3x-1\right| + \cdots + \left|119x - 1 \right|$ ? Quoted portion of a solution I am not understanding :
""If we graph each term separately, we will notice that all of the zeros occur at $\frac{1}{m}$ , where $m$ is any integer from $1$ to $119$ , inclusive: $|mx-1|=0\implies mx=1\implies x=\frac{1}{m}$ . The minimum value of $f(x)$ occurs where the absolute value of the sum of the slopes is at a minimum $\ge 0$ , since it is easy to see that the value will be increasing on either side. That means the minimum must happen at some $\frac{1}{m}$ ."" Question : Is the above clearly explained? I am not understanding it. Can someone break it down for me? Source: https://artofproblemsolving.com/wiki/index.php/2010_AMC_12A_Problems/Problem_22",['algebra-precalculus']
4722353,Does the epsilon-delta definition of limits truly capture our intuitive understanding of limits?,"I've been delving into the concept of limits and the Epsilon-Delta definition. The most basic definition, as I understand it, states that for every real number $\epsilon \gt 0$ , there exists a real number $\delta \gt 0$ such that if $0 \lt |x - a| \lt \delta$ then $|f(x) - L| \lt \epsilon$ , where $a$ is the limit point and $L$ is the limit of the function $f$ at $a$ . While I grasp the formal definition, I'm grappling with the philosophical aspect of it. Specifically, I'm questioning whether this definition truly encapsulates our intuitive understanding of what a limit is. The idea of a limit, as I see it, is about a function's behavior as it approaches a certain point. However, the Epsilon-Delta definition seems to be more about the precision of the approximation rather than the behavior of the function. In the book ""The Philosophy of Mathematics Today"" by Matthias Schirn,
on page 159, it is stated that: ""At one point, Etchemendy asks: 'How
do we know that our semantic definition of consequence is
extensionally correct?' He goes on to say: 'That [this question] now
strikes us odd just indicates how deeply ingrained is our assumption
that the standard semantic definition captures, or comes close to
capturing, the genuine notion of consequence' (Etchemendy 1990, 4-5).
I do not think that this diagnosis is correct for some people: for
some logicians, the question is similar to: How do we know that our
epsilon-delta definition of continuity is correct?"". This quote resonates with my current dilemma. Does the Epsilon-Delta definition truly capture the essence of what we mean by a 'limit'? though the epsilon-delta definition is a mathematical construct, what evidence do we have that it accurately reflects our intuitive concept of a limit? How can we be sure it is not merely a useful formalism, but a true representation of the limit as a variable approaching some value? Are there alternative definitions or perspectives that might align more closely with our intuitive understanding of limits? I would appreciate any insights or resources that could help me reconcile these aspects of the concept of limits.
Thank you in advance for your help. edit:i think i should add my motivation of asking the question, what i really want is an argument which can demonstrate that this definition of limit is the definition of limit which no better definition can come up, i can accept the definition as it is in its own axiomatic system and in itself, but whats the certainty that a hundred years from now we come up with a better definition still? its not about the thing that we cant understand i am more worried it there is something out our sphere of recognition if we are not taking note of, because everybody just seem to accept the definition without any further doubt an examination.","['epsilon-delta', 'real-analysis', 'calculus', 'philosophy', 'soft-question']"
4722359,Brain Teaser: Card Drawing until One Color is eliminated,"I just found such a quant interview question but could not find a similar post in StackExchange. The question is pasted below. Jane offers to play the following game with you. Starting with a standard deck of 52 cards, you take turns drawing two cards. Jane goes first. If the two cards are black, Jane keeps them. If the cards are red, you keep them. If there is one red and one black card, the cards are discarded. When you have gone through the deck, you and Jane combine your cards, shuffle them, and resume, playing until one of the colors is eliminated. If there are red cards remaining, you win $100. What is a fair price to pay to play this game?","['statistics', 'probability']"
4722399,Differentiation and Chain Rule on the Hilbert Space $L^2$. (Reisz Representation).,"Let $F:L^2(\mathbb{R}^d)\to \mathbb{R}$ be a functional on the Hilbert space $L^2(\mathbb{R}^d)$ and $\rho:\mathbb{R}\to L^2(\mathbb{R}^d)$ a curve in the space $L^2(\mathbb{R}^d)$ . I want to calculate $\frac{d}{dt}F(\rho(t))$ . Let $DF(\rho(t))$ be the Frechet derivative at $\rho(t)$ . What `chain rule' do I apply to get $\frac{d}{dt}F(\rho(t))=DF(\rho(t))\dot{\rho}(t)$ ? (since this is not the usual chain rule in Euclidean space). Since $DF(\rho(t))$ is a linear functional on $L^2(\mathbb{R}^d)$ it can be associated to an element $f$ of $L^2(\mathbb{R}^d)$ so that $DF(\rho(t))\dot{\rho}(t)=\langle f , \dot{\rho}(t) \rangle_{L^2(\mathbb{R}^d)}$ . Is $f$ known in this case, does it relate to this Wiki article ? (note I have denoted the derivative of $\rho$ with respect to $t$ as $\dot{\rho}(t)$ )","['real-analysis', 'hilbert-spaces', 'frechet-derivative', 'derivatives', 'chain-rule']"
4722436,Area under two curves and between two curves are equal.,"In the accompanying figure, y=f(x) is the graph of a one to one continuous function f. At each point P on the graph of y=2x^2, assume that the areas OAP and OBP are equal. Here PA, PB are the horizontal and vertical segments. Determine the function f. Now my approach is this:
First area under $y=2x^2$ and $y=x^2$ at a point x is $x^3/3$ . Now we need area $OAP$ . If we rotate the axes by π/2 radians and reflect the rotated graph about the y axis, we should still preserve the area of the region OAP. So, first we rotate the axes by π/2 radians and get the equation of $y=2x^2$ and $y=f(x)$ in the rotated system as $y=\sqrt{\frac{-x}{2}}$ and $y=f^{-1}(-x)$ Now to reflect the graph about the y-axis, we replace x with -x to get the transformed equations as $y=\sqrt{\frac{x}{2}}$ ---(1) and $y=f^{-1}(x)$ ---(2) Now we need to find the area under the two curves (1) and (2) from 0 to f(x). So we have Area $OAP = \int_{t=0}^{f(x)}{\sqrt{\frac{t}{2}}-f^{-1}(t) dt}$ Since Area $OAP = x^3/3$ , upon differentiating we have $\sqrt{\frac{f(x)}{2}} - (x-f^{-1}(0)) = x^2$ . Thus we have $f(x)= 2(x^2+x-f^{-1}(0))^2$ And from the figure, and the fact that area OPB tends to 0 as x tends to 0, $f^{-1}(0)=0$ Im unable to find the mistake here, and would be grateful to anyone that could point it out.","['integration', 'calculus', 'area', 'ordinary-differential-equations']"
4722485,"If a subgroup of a surface group surjects onto first homology, does it generate the surface group?","Let $\Sigma_g$ denote a closed genus- $g$ surface, write $G = \pi_1(\Sigma_g)$ , and consider a subgroup $H \le G$ . Suppose we know that $H$ generates the abelianzation $G^\text{ab} = H_1(\Sigma_g)$ . Does it follow that $H$ generates $G$ ? For an arbitrary group this is false (e.g. when $G^\text{ab}$ is trivial), but I wonder if the hypothesis that $G$ is a surface group saves us.","['group-theory', 'surfaces', 'algebraic-topology']"
4722509,"Bounding ""Jensen's Gap"": Elementary Approaches","The point of this post is to explore some ""elementary"" but general ways one can quantify the ""gap"" in Jensen's inequality. Specifically, let $h:\mathbb R\rightarrow\mathbb R$ be a convex function and let $X$ be a real-valued random variable, then how can we bound $$?\leq \mathbb E\big[h(X)\big]-h\big(\mathbb EX\big)\leq ? $$ There does exist a good amount of research on this question, which i will detail in Section 2 of this post, but first i want to give a simple example to explain what i mean by (a) elementary and (b) a ""non-trivial bound"" on Jensen's gap: Section 1: Bounded Second Derivatives Suppose $h$ is twice differentiable and there exists a $\lambda>0$ such that $h''>\lambda$ . Then it is easily seen by computing second derivatives that the function $g(x)=h(x)-\frac12 \lambda x^2$ is convex. Thus, applying Jensen's inequality to $g$ yields $$\mathbb E\bigg[h(X)-\frac12\lambda X^2\bigg]\geq h\big(\mathbb EX\big)-\frac12 \lambda (\mathbb E X)^2$$ and rearranging gives $$\mathbb E\big[ h(X)\big]-h\big(\mathbb E X\big)\geq \frac12\lambda\bigg[\mathbb EX^2-(\mathbb EX)^2\bigg]=\frac12 \lambda \text{Var}(X).$$ Thus we have achieved a lower bound on Jensen's gap. Similarly, if we assume $h''<\Lambda$ then the same argument shows the upper bound $$\mathbb E\big[ h(X)\big]-h\big(\mathbb E X\big)\leq \frac12\Lambda \text{Var}(X).$$ (As a small remark, we technically have only used the assumption that $h-\frac12\lambda x^2$ is convex. This can be true even if $h$ is not twice differentiable) I hope you would agree that such a result is ""elementary"" in the sense that the condition on $h$ is both intuitive and likely easy to verify. The proof also is easily understood. The bound this achieves is meaningful in the sense that it uses some score of ""how strictly convex"" the function $h$ is (quantified by $\lambda$ ) and uses a simple property of the distribution of $X$ . Clearly these two types of information are the least we will need to obtain an interesting bound. I am essentially looking for arguments and results of similar simplicity that give you a bound on Jensen's gap. Section 2: Some existing Research Before i present some research papers, let me first refer to a few existing StackExchange posts about Jensen's gap: First, there is a question about the gap for $h(x)=\frac{1}{1+x}$ and $X\geq 0$ , which again finds a meaningful upper bound on the gap based on the first two moments of $X$ . There's a post about the minimal Eigenvalue of a Random Matrix , which again represents a concave function, where no good bound on the gap was found. Also worth noting is this post about $h(x)=|x|$ . Now to the interesting part: Here are two research papers i have found on this question: First, there is ""Bound on the Jensen Gap, and Implications for Mean-Concentrated Distributions"" . Among other things, it first mentions very elementary upper and lower bound the gap for $\alpha$ -Hölder-continuous functions $h$ based on the $\alpha$ -th moment $\mathbb E|X-\mathbb EX|^\alpha$ . Some nice examples where e.g. $h(x)=\log x$ or $h(x)=\sqrt x$ are also presented. Their main results (Theorem 2.1 and Theorem 3.1) seem to generalize the bound based on Hölder-continuity to some functions ""locally Hölder-continuous around the mean of $X$ "" As another example, there is ""Some new estimates of the ‘Jensen gap’"" . Their main results seem to be based on a condition on the Taylor series of the convex function $h$ . Though the result they obtain in Theorem 1 seems to be of a similar type to the result obtained in Section 1. Final Remarks: I want to reiterate that i am looking for elementary appraoches to this question, i.e. results where the conditions on $h$ and $X$ are easy to verify and ideally results with a concise proof. Section 2 was only there to have a ""reference post"" here on StackExchange for results on Jensen's gap.","['measure-theory', 'jensen-inequality', 'inequality', 'convex-analysis', 'probability']"
4722512,Constructing a Kripke model where $p \rightarrow \Box \Diamond q$ is false.,"I have constructed the following Kripke model for this problem: My idea is the following: Implication is false iff we have $ \top \implies \bot$ . For world $0$ , we have that $p$ is true. Now we need to evaluate $\Box \Diamond q$ . For world $0$ , $\Box \Diamond q$ is true iff $\Diamond q$ is  true in every world which is reachable from world $0$ (that would be world $1$ ). Now, in world $1$ , $\Diamond q$ is true if $q$ is true in at least one world reachable from world $1$ , and the only such world is $2$ . Now, since $q$ is false in world $2$ , we can say that $\Diamond q$ is false in $1$ , and so $\Box \Diamond q$ is false in $0$ . We have our implication $\top \implies \bot$ . My questions are: Obviously, the value of $q$ in world $1$ is $\neg q$ . However, we don't really need that value for anything because we only use world $1$ as an ""intermediary"" world of sorts, right? Can different worlds in the same Kripke model have the same truth values for the same variables? I.e., could I have added a world $3$ with $p,q$ as values for $p,q$ ?","['propositional-calculus', 'modal-logic', 'logic', 'discrete-mathematics', 'kripke-models']"
4722516,Continuity of function on $C^\infty_c(K)$,"While learning the concept of test functions, I am stuck on the following exercise. Following the book's notation we define $C^\infty_c (\mathbb{R}^d)$ as the set of smooth, compactly supported functions mapping from $\mathbb{R^d} \to \mathbb{C}$ , and $C^\infty_c(K) \subset C^\infty_c(\mathbb{R}^d)$ the subset who are supported in $K$ . Let $K$ be a compact set in $\mathbb{R}^d$ . Show that a linear map $T: C^\infty_c(K) \to X$ into a normed vector space $X$ is continuous if and only if there exists $k\geq 0$ and $C > 0$ such that $\|Tf\|_{K} \leq C \| f\|_{C^k}$ for all $f\in C^\infty_c(K)$ . Prior to this exercise my textbook defines the topology on $C_c^\infty (K)$ as the topology generated by the normS $$\| f\|_{C^k}=:\sup_{x\in K}\sum_{j=1}^k |\nabla^j f(x)|,$$ with $k = 1, 2, \dots$ and $\nabla^j f(x)$ being the $d^j$ -dimensional vector. I found much trouble proving the forward direction ( $\implies$ ). Here is what I have tried. Let $\tau_k$ be the topology generated by the norm $\|\cdot\|_{C^k}$ . Since $\| f\|_{C^{k'}}\geq \|f\|_{C^k}$ whenever $k' \geq k$ , we know that $\tau_k \subset \tau_{k'}$ . It follows that the topology of $C_c^\infty(K)$ is given by $\cup_{i=1}^\infty \tau_k$ . We will prove the forward direction if we can show that there exists $k>0$ such that for all open set $O$ in $X$ , $T^{-1}(O)\in \tau_k$ . As this will then imply $\|Tf\|_{K} \leq C \| f\|_{C^k}$ due to the equivalence between boundedness and continuity of an operator. However, I cannot convince myself that such finite $k$ exists. Specifically, what if there is a sequence of $O_k$ open in $X$ such that $O_k\in \tau_k $ but $O_k \notin \tau_{k-1}$ for every $k>1$ ? If this possibility is true, then it would contradict the statement of the exercise. I would appreciate it if someone could point out any conceptual error or provide hints to solving this exercise.","['general-topology', 'distribution-theory', 'real-analysis']"
4722564,Asymptotics of $J_n=\int^\pi_0 \max_{1\leq k\leq n} |\sin(kt)|\frac{dt}{t}$,"This is a problem in an old qualifier test in analysis that I am trying to solve. Show that $I_n=\int^\pi_0\frac{|\sin nt|}{t}\,dt \sim \frac{\pi}{2} \log n$ as $n\rightarrow\infty$ $J_n=\int^\pi_0 \max_{1\leq k\leq n} |\sin(kt)|\frac{dt}{t}\sim \log(n)$ as $n\rightarrow\infty$ I worked out the first part by splitting the integral in several pieces: \begin{align}
\int^\pi_0 \frac{|\sin nt|}{t}\,dt &= \int^{n\pi}_0 \frac{|\sin t|}{t}\,dt\\
&=\sum^{n-1}_{k=0}\int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt\\
&=\int^{\pi}_0\frac{\sin t}{t}\,dt +\sum^{n-1}_{k=1} \int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt
\end{align} For $k\geq 2$ , $$\frac{2}{\pi}\frac{1}{k+1}=\frac{1}{(k+1)\pi}\int^\pi_0|\sin t|\,dt \leq \int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt\leq \frac{1}{\pi k}\int^\pi_0|\sin t|\,dt=\frac{2}{\pi}\frac{1}{k}$$ Therefore $$
\int^\pi_0\frac{\sin t}{t}\,dt+\frac{\pi}{2}\sum^n_{k=2}\frac{1}{k}\leq 
I_n \leq \int^\pi_0\frac{\sin t}{t}\,dt+\frac{\pi}{2}\sum^{n-1}_{k=1}\frac1k
$$ From this, part 1 follows. The part I am not making much progress is the second one. Any hints/ideas will be appreciated. Thank you!","['integration', 'asymptotics', 'real-analysis']"
4722602,"Evaluating $\int_{0}^{\infty}\frac{\sin^2(x)}{x^2(1+x^2)}\,dx$","As the title says I’m wondering what is wrong with my solution process in evaluating $$\int_{0}^{\infty}\frac{\sin^2(x)}{x^2(1+x^2)}\,dx$$ Here is what I do: $$I=\int_{0}^{\infty}\frac{\sin^2(x)}{x^2(1+x^2)}\,dx$$ Consider $$\frac{1}{x^2}=\int_{0}^{\infty}e^{-tx^2}\,dt$$ So $$I=\int_{0}^{\infty}\int_{0}^{\infty}\frac{\sin^2(x)e^{-tx^2}}{1+x^2}\,dtdx$$ By Fubini’s theorem we can say $$I=\int_{0}^{\infty}\int_{0}^{\infty}\frac{\sin^2(x)e^{-tx^2}}{1+x^2}\,dxdt$$ Now we will evaluate the inner integral which is $$J=\int_{0}^{\infty}\frac{\sin^2(x)e^{-tx^2}}{x^2+1}\,dx$$ Consider $$f(z)=\frac{(e^{iz}-e^{-iz})^2e^{-tz^2}}{4(z^2+1)}$$ Consider a semi-circular contour $C$ in the upper-half of the complex plane with a radius of $R$ , and the upper curve from $R$ to $-R$ being demoted as capital gamma. So we have $$\oint_{C}f(z)\,dz=\int_{\Gamma}f(z)\,dz+\int_{-R}^{R}f(x)\,dx$$ Calculating the residues of $f(z)$ in the contour $C$ $$\frac{1}{4}\lim_{x\to i}(z-i)\frac{(e^{iz}-e^{-iz})^2e^{-tz^2}}{(z+i)(z-i)}$$ $$=\frac{e^t(e^{-1}-e)^2}{8i}$$ And so $$\oint_{C}f(z)\,dz=2\pi i\sum Res(f(z))= \frac{1}{4}\pi e^t(e^{-1}-e)^2$$ So this yields $$\int_{\Gamma}f(z)\,dz+\int_{-R}^{R}f(x)\,dx=\frac{1}{4}\pi e^t(e^{-1}-e)^2$$ And I’m the limit as R goes to infinity the Gamma integral goes to zero by Jordan’s lemma. And by symmetry of even functions we have $$J=\int_{0}^{\infty}\frac{\sin^2(x)e^{-tx^2}}{x^2+1}\,dx=\frac{\pi}{8} e^t(e^{-1}-e)^2$$ Finally this means that $$I=\frac{\pi}{8}(e^{-1}-e)^2\int_{0}^{\infty}e^t\,dt$$ But this is divergent. Where did I go wrong?","['integration', 'definite-integrals', 'complex-analysis', 'contour-integration', 'residue-calculus']"
4722655,what's wrong here ? in solving $y'+2y=1$,"I was dealing with Ordinary differential equation, $y'+2y=1$ In my book it is solved using seperable variable form, $\begin{equation}\begin{aligned}y'&=1-2y\\\frac{dy}{dx}&=1-2y\\\frac{dy}{1-2y}&=dx\\\int \frac{dy}{1-2y}&=\int dx\\\frac{\log(1-2y)}{-2}&=x+c\end{aligned}\end{equation}$ But what I did, $\begin{equation}\begin{aligned}y'&=1-2y\\\int y'dx&=\int 1-2ydx\\y&=x-2yx\\y+2xy&=x+c\\y(1+2x)&=x+c\\y&=\frac{x+c}{1+2x}\end{aligned}\end{equation}$ So I want to know, my way is also correct no? or where I am wrong..? Thanks in advance!!",['ordinary-differential-equations']
4722665,Solve a coupled ODE system $x'(t)$ and $y'(t)$ including time $t$,"I met this ODE system of functions $x(t), y(t)$ , with initial values $x(0)=\frac{\sqrt{2}}{2}, y(0)=\frac{\sqrt{2}}{2}$ . \begin{equation}
\frac{\mathrm{d}x}{\mathrm{d}t}=-y+x(x^2+y^2)\sin\frac{\pi}{\sqrt{x^2+y^2}}
\end{equation} \begin{equation}
\frac{\mathrm{d}y}{\mathrm{d}t}=x+y(x^2+y^2)\sin\frac{\pi}{\sqrt{x^2+y^2}}
\end{equation} I tried to eliminate the second term including function $\sin$ \begin{equation}
\frac{\frac{\mathrm{d}x}{\mathrm{d}t}+y}{\frac{\mathrm{d}y}{\mathrm{d}t}-x}=\frac{x}{y}
\end{equation} After substituting $\frac{x}{y}$ with $u$ , the above equation becomes \begin{equation}
\mathrm{d}u=-(1+u^2)\Rightarrow \arctan u=-u+C
\end{equation} Use initial values, $\arctan 1=-1+C$ meaning $C=\frac{\pi}{4}+1$ . But it seems more difficult when I tried to substitute $u$ back to the ODE system. Can we have a better solution?",['ordinary-differential-equations']
4722722,In cases of one multiple root (differential equation characteristic polynomial). How do we know that $xe^{rx}$ is another solution?,"Given a differential equation, say: $$y''+2y'+y=0$$ we might get two equal roots to the characteristic polynomial. Here we get $r_1=r_2=-1$ . So, $e^{-x}$ is a solution. In order to get another independent solution for $e^{rx}$ , usually, $xe^{rx}$ is taken. It is clear to me that $e^{rx},xe^{rx}$ are independent. But, how do we know that $xe^{rx}$ is also a solution to the original differential equation?","['homogeneous-equation', 'ordinary-differential-equations']"
4722752,Sum of two difference sequences for consecutive numbers,"Problem Setting: If there are two difference sequences for consective integers $\{a_i \}_{i=1}^{n}$ and $\{b_i \}_{i=1}^{n}$ and they have a relationship that for any $i = 1,2,...,n$ , $$\delta = a_i - b_i, $$ and $\delta \in R$ , how can we prove that $$
\left[ \sum_{i=1}^{n}a_i y_i \right] \left[ \sum_{i=1}^{n}a_i \right] - \left[ \sum_{i=1}^{n}y_i \right]\left[ \sum_{i=1}^{n}a_i^2 \right] \neq \left[ \sum_{i=1}^{n}b_i y_i \right] \left[ \sum_{i=1}^{n}b_i \right] - \left[ \sum_{i=1}^{n}y_i \right]\left[ \sum_{i=1}^{n}b_i^2 \right],
$$ where $y_i \in R$ . Solved Attempt: I tried to use proof by contradiction. Assuming that the above inequality is not hold so LHS = RHS, and plug in their relationship $\delta = a_i - b_i$ , I can obtain $$
\left[ \sum_{i=1}^{n}a_i y_i \right] \left[ \sum_{i=1}^{n}a_i \right] - \left[ \sum_{i=1}^{n}y_i \right]\left[ \sum_{i=1}^{n}a_i^2 \right] - \left[ \sum_{i=1}^{n}b_i y_i \right] \left[ \sum_{i=1}^{n}b_i \right] + \left[ \sum_{i=1}^{n}y_i \right]\left[ \sum_{i=1}^{n}b_i^2 \right] = 0,
$$ after some calculation, $$
\sum_{i=1}^{n}\left[-(n-1)n\delta^2 - \delta(2n-1)\left(\sum_{i=1}^{n}b_i\right) + n \delta b_i \right]y_i = 0.
$$ However, I didn't know how to continue this process. I am wondering how I can continue the proof or if I make any mistakes.","['statistics', 'sequences-and-series', 'summation', 'real-analysis']"
4722761,"Symmetry group of an hourglass shape, the molecule Ferrocene.","I am starting to learn (Visual) Group Theory and I saw this hourglass shaped molecule Ferrocene: I am wondering what group is described by its symmetries. On top of the five rotations I think there are vertical $v$ and horizontal $h$ flips. Mapping its Cayley Diagram gave me $4$ cycles with flips acting as bridges between them. I've also noticed that $hv = vh$ which is reminiscent of the Klein group $V_4$ . Is there a name for this ""hourglass group""? And I feel like it could be separated into subcomponents like $V_4$ and a Cycle group of five elements $C_5$ ? Is there a way to formalize this? Thanks a lot !
Pierrick","['symmetric-groups', 'group-theory', 'symmetry', 'chemistry']"
4722780,Value of mixed partial derivatives at critical points.,"Suppose $f(x,y):\mathbb{R}^2\rightarrow\mathbb{R}$ is smooth, and $$\left.\frac{\partial f}{\partial x}\right\vert_{(x_0,y_0)}=0\;,\qquad
\left.\frac{\partial f}{\partial y}\right\vert_{(x_0,y_0)}=0$$ Can I claim : $$\left.\frac{\partial^2f}{\partial x\partial y}\right|_{(x_0,y_0)}=0$$ My intuition is that this is valid, and I have yet to find a conter-example. Here is my intuition: At extremas, i.e minimas and maximas, the gradient is 0. Taking a small step in any direction from the extrema (say in the x direction), the direction of the maximum slope will be either towards or away from the extremum (depending on whether it is either a maxima or minima). Then the slope in the direction perpendicular to that (y direction) is 0, hence $\left.\frac{\partial^2f}{\partial x\partial y}\right|_{(x_0,y_0)}=0$ I think the same logic is valid for saddle points as well, but I'm not sure. Thanks in advance! Edit: A counter-example has been given in the comments for this question (a saddle-point), now my question is basically if there is anything wrong with my logic for extremas.","['partial-derivative', 'multivariable-calculus', 'calculus']"
4722786,Is group associativity equivalent to assuming equivalence of left and right inverses?,"Associativity is usually taken as one of the axioms that define a group, however I'm sure I recall reading that it can instead be proved if it is taken as axiomatic that the left and right inverses are the same.  In other words, given a set, $G$ , and a binary relation, $\cdot$ , can we prove that $(a\cdot b)\cdot c = a\cdot(b\cdot c)$ for all $a,b,c \in G$ from the following axioms? Closure : $a\cdot b \in G$ for all $a, b \in G$ , . Identity :  There exists a unique element $e \in G$ such that $e\cdot a = a\cdot e = a$ for all $x \in G$ . Inverses :  For all $x \in G$ , there exists a unique element $x^{-1} \in G$ such that $x\cdot x^{-1} = x^{-1}\cdot x = e$ . If not, am I misremembering and is there similar axiom that allows associativity to be proved?",['group-theory']
4722789,"Reduction formula for $I_n (a, b) := \int \frac {a x + b} {(x^2 + A x + B)^n} \mathrm d x$","Context: Oxford Concise Dictionary of Mathematics by Richard Earl and James Nicholson, 6th Edition (2021). Self-study as a hobbyist. We are given: For $I_n (a, b) := \displaystyle \int \dfrac {a x + b} {(x^2 + A x + B)^n} \mathrm d x$ where $n \ge 2$ , then: $$I_n (a, b) = \dfrac {b A - 2 a B + (2 b - a A) x} {(n - 1) (4 B - A^2) (x^2 + A x + B)^n} + \dfrac {(2 n - 3) (2 b - a A)} {(n - 1) (4 B - A^2)} I_{n - 1} (0, 1)$$ My mission is to try to prove this. I have used a similar technique to that used on this question: Reduction formula for $I_{n, k} = \int x^k \left({x^2 + A x + B}\right)^n \ \mathrm d x$ as follows. Let $h$ be the real function defined as: $$\forall x \in \mathbb R: h (x) = x^2 + A x + B$$ Thus we have: $$I_n (a, b) := \int \dfrac {a x + b} {(h (x))^n} \mathrm d x$$ Then, in order to get the suggestive coefficients in the answer given above, we do: $$\dfrac {\mathrm d} {\mathrm d x} \left({\dfrac {a x + b} {(h (x))^{n - 1} } }\right) = (a x + b) \dfrac {-(n - 1)} {(h (x))^n} (2 x + A) + \dfrac a {(h (x))^{n - 1} }$$ using the product rule, chain rule and power rules for diffing. After considerable algebra, we reach: $$\dfrac {\mathrm d} {\mathrm d x} \left({\dfrac {a x + b} {(h (x))^{n - 1} } }\right) = -(n - 1) \dfrac {b A - 2 a B + (2 b - a A) x} {(h (x))^n} + \dfrac {a (3 - 2 n)} {(h (x))^{n - 1} }$$ The idea is then that we integrate both sides wrt $x$ to get: $$\dfrac {a x + b} {(h (x))^{n - 1} } + a (2 n - 3) \int \dfrac {\mathrm d x} {(h (x))^{n - 1} } = -(n - 1) \int \dfrac {b A - 2 a B + (2 b - a A) x} {(h (x))^n} \mathrm d x$$ Expressing in terms of $I_n$ this is: $$\dfrac {a x + b} {(x^2 + A x + B)^{n - 1} } + a (2 n - 3) I_{n - 1} (0, 1) = -(n - 1) (b A - 2 a B + (2 b - a A)) I_n (1, 0)$$ which is nowhere near where we are aiming. We note that we have that required $I_{n - 1} (0, 1)$ , and the coefficients $(n - 1$ and $(2 n - 3)$ in ''almost'' the right places, but even after we bend and strain the result by piling on the algebra to force $I_n (a, b)$ to appear, the fact remains that can't seem to get the term $\displaystyle \int \dfrac {a x + b} {(h (n))^n} \mathrm d x$ without leaving a term in $\displaystyle \int \dfrac Q {(h (n))^n} \mathrm d x$ where $Q$ is messy. Nothing else I've tried comes remotely close to anything even approaching the expected coefficients, and nothing seems to allow me to eliminate spurious terms of $I_n$ and/or $I_{n - 1}$ . Anyone have any ideas as to what my approach might need to be?","['integration', 'indefinite-integrals', 'reduction-formula']"
4722801,"Does uniform distribution on every square $[0,a]^2$ along diagonal imply uniform CDF on the entire $ [0,1]^2$","Let $F: [0,1]^2\to R $ be a continuous cdf with uniform marginals, i.e., $F(x,1)=x$ and $F(1,y)=y$ . Suppose $F$ is symmetric, i.e., $F(x,y)=F(y,x)$ . Suppose we also know that $F(a,a)=a^2$ for all $a\in [0,1]$ . Can we conclude that $F$ is uniform distribution on $[0,1]^2$ ? Thanks.","['statistics', 'probability-distributions', 'correlation', 'copula', 'probability']"
4722814,Uniform distribution parameter estimate,"Consider parameter estimate $\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{X_{(n)}}{2}$ , where $X_1, X_2, ..., X_n$ independent identically distributed random variables with uniform distribution on the interval $[0, \theta], \theta>0$ . I want to check if the given parameter estimate is unbiased, consistent, strongly consistent. So, i need to check that: $$E\left(\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{X_{(n)}}{2}\right) = \theta\\ \lim_{n\to\infty}P\left(\left|\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{X_{(n)}}{2} - \theta \right| \geq \epsilon\right) = 0, \forall\epsilon>0\\
P\left(\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{X_{(n)}}{2} = \theta\right) = 1
$$ To test for unbias, I can use the linearity of the expectation, so $$E(\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{X_{(n)}}{2}) = E(\frac{1}{n}\sum_{i=1}^{n}X_i) + E(\frac{X_{(n)}}{2}) = \theta/2  + \frac{n\theta}{2(n+1)} = \frac{(2n+1)\theta}{2(n+1)} \neq \theta$$ (Just want to check if the solution is correct). To prove consistency I need to check the second statement. My idea was to use Chebyshev's inequality: $$
\lim_{n\to\infty}P(|\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{X_{(n)}}{2} - \theta| \geq \epsilon) =\\
= \lim_{n\to\infty}P(|\frac{2n + 1}{2(n+1)}\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{2n + 1}{2(n+1)}\frac{X_{(n)}}{2} - \frac{2n + 1}{2(n+1)}\theta| \geq \frac{2n + 1}{2(n+1)}\epsilon) \leq \frac{(\frac{2n + 1}{2(n+1)})^2Var(\frac{1}{n}\sum_{i=1}^{n}X_i + \frac{X_{(n)}}{2})}{\epsilon^2}
$$ But I find it difficult to calculate the variance of this estimate. And for the proof of the third statement, I do not have any ideas. I will be extremely grateful for your help or hints.","['statistics', 'convergence-divergence', 'probability-theory', 'uniform-distribution']"
4722824,Gaussian curvature of a two-dimensional manifold with $C^1$ metric,"I am working on a problem in geometric analysis and I arrived at the following question: what would be the minimum of regularity I could require from a metric in order to define a first fundamental form on an abstract surface (two-dimensional manifold) with Gaussian curvature ${\cal K}=-1$ ? More precisely, let us consider the following: assume the existence of $C^1$ functions $f_{ij}=f_{ij}(x,t),\,\,1\leq i\leq 3,\,\,1\leq j\leq 2$ , defined on an open set $U\subseteq{\mathbb R}^2$ (non-empty, of course) such that the one-forms $\omega_i=f_{i1}dx+f_{i2}dt,\quad 1\leq i\leq 3$ , satisfy the equations \begin{equation}
d\omega_1=\omega_3\wedge\omega_2,\quad d\omega_2=\omega_1\wedge\omega_3,\quad d\omega_3=\omega_1\wedge\omega_2.\quad\quad (1)
\end{equation} The equations above make sense with the required regularity and, as a result, $I=\omega_1^2+\omega_2^2$ would define a $C^1$ first fundamental form ( $C^1$ -metric) for a manifold with Gaussian curvature ${\cal K}=-1$ . The problem is that, usually, in differential geometry one requires the metric as being a tensor with $C^2$ components, e.g, see Theorem 4.24, page 153, in [W. Kühnel, Differential Geometry, 3nd Ed., AMS, (2015)]. Actually, most of the books I studied consider $C^\infty$ metrics. This one by Künhel is one of the few I found requiring finite regularity, but being $C^2$ would contradict what I wrote above. On the other hand, on page 760 of the paper [P. Hartman and A. Wintner, On the fundamental equations of differential geometry, Amer. J. Math., vol. 72, 757--774, (1950)], the authors consider $C^1$ metrics, but in conjunction with a $C^0$ second fundamental form. My questions is: Would it be possible to require $C^1$ regularity for the metric (or the forms satisfying (1))  and still have a well defined Gaussian curvature? The paper by Hartman and Wintner apparently supports a positive answer, but the fact that usually books on differential geometry require higher regularity makes me worry about if I could or not have it.","['curvature', 'differential-forms', 'differential-geometry']"
4722913,What is the interpretation/intuition of $e^{itA}$ for a self-adjoint unbounded operator?,"Let $A : i \frac{d}{dt} : D(A) \to H^1([0,1])$ with domain $D(A)=H^1_*([0,1])=\{u \in H^1([0,1]): u(0)=u(1)\} \subseteq H^1([0,1])$ . Then I know that $A$ is self-adjoint. Using the spectral theorem, we find that for any $u \in L^2([0,1])$ , an explicit formula for $(e^{itA}u)(x,t)$ is given by $\tilde{u}(x,t)= u(x-t)$ . This makes sense to me, because $\tilde{u}$ satisfies the PDE $\ \partial_t \tilde{u} = -\partial_x \tilde{u}$ with initial condition $u$ , which is what one finds when formally taking the following derivative : $$ \partial_t (e^{itA}u) = iA (e^{itA}u) = -\partial_x (e^{itA}u).$$ Now, I have trouble understanding the following. If we take $A_{\alpha}: i \frac{d}{dt} : D(A_\alpha) \to H^1$ this time with domain $D(A_\alpha) = \{u \in H^1([0,1]) : u(0)=e^{i\alpha}u(1)\}$ , then $A_\alpha$ is also self-adjoint. With the spectral theorem, we find $(e^{itA_\alpha}u)(x,t)= u(x-t)e^{-i \alpha t},$ which this time satisfies the PDE $\partial_t \tilde{u} + \partial_x \tilde{u} = -i\alpha \tilde{u}.$ But in my opinion, this contradicts the intuition provided by the formal derivative (which hasn't changed) : $$ \partial_t (e^{itA}u) = iA (e^{itA}u) = -\partial_x (e^{itA}u). $$ So we see that even though we have taken the ""same"" operator $A \sim A_{\alpha}$ , the PDE solved by applying $e^{itA}$ has changed, just by changing the domain of definition of the operator and the formal derivation doesn't work (at least the same way) anymore. I thought the point of $e^{itA}$ was just to solve the 'heat equation' $\partial_t \tilde{u} = iA \tilde{u}$ ? What is an interpretation of this phenomenon ? Is there a way to know what domain we should work on if we are given a specific PDE to 'solve' ?","['operator-theory', 'functional-analysis', 'partial-differential-equations']"
4722939,"Determining four rational numbers, given their pairwise sums","The pairwise sums of four weights (rational numbers) are: 6, 8, 10, 12, 15, 16. What are the four weights? I tried intuitively solving as well as making equations to solve it. If we assume weights in ascending order as: A, B, C, D, then the smallest two weights should sum to 6 (A + B = 6), and the largest two should sum to 16 (C + D = 16). We can't comment on the order of the rest of the pairs. I tried hit and trial approach with some pairs and working on 4 equations at a time, but I haven't been able to get values which fit all 6 equations. What would be be a good way to solve this?","['contest-math', 'puzzle', 'linear-algebra', 'discrete-mathematics', 'recreational-mathematics']"
4722991,A Probability Distribution Whose Variance Doesn't Change as You Raise it to Higher and Higher Positive Integer Powers,"Are there any continuous probability distributions whose random variables have variances that do not change as you raise them to higher and higher integer powers? If there are, what are they? I've done some simulations, and a few things are clear. For a uniform distribution with min = 0 and max = 1 , the variances of the random variables converge to 0 as higher and higher positive integer powers are used (I know that this is expected since raising numbers to positive integer powers that are between 0 and 1 makes them smaller). For a Cauchy distribution with location = 0 and scale = 1 , variances diverge to Inf (I know that this is expected since Cauchy distributions have variances of infinity). It's not clear to me if the variances of a normal distribution with mean = 0 and sd = 1 converge or diverge. It'd be great if I could find a probability distribution (or multiple probability distributions) whose random variables have variances that remain constant as higher and higher integer powers are used. To be clear, I'm not looking for variances that converge - I'm looking for variances that remain constant. I'm looking for a theoretical basis and not just an empirical one. Thanks! My R code is below for reference. # Constants
Highest_Integer_Power_to_Investigate <- 10
Number_or_Observations_per_Sample <- 1000
Number_of_Iterations <- 1000

# Data Generation
Uniform_Distribution_Values <- lapply(seq_len(Number_of_Iterations), function (x) {
  runif(Number_or_Observations_per_Sample)
})
Normal_Distribution_Values <- lapply(seq_len(Number_of_Iterations), function (x) {
  rnorm(Number_or_Observations_per_Sample)
})
Cauchy_Distribution_Values <- lapply(seq_len(Number_of_Iterations), function (x) {
  rcauchy(Number_or_Observations_per_Sample)
})
Values <- list(Uniform_Distribution = Uniform_Distribution_Values, Normal_Distribution = 
Normal_Distribution_Values, Cauchy_Distribution = Cauchy_Distribution_Values)

# Calculations
Output <- lapply(Values, function (x) {
  sapply(x, function (y) {
    sapply(seq_len(Highest_Integer_Power_to_Investigate), function (z) {
      var(y ^ z)
    })
  })
})

# Formatting
Variances <- as.data.frame(lapply(Output, rowMeans))
Variances$Power <- seq_len(Highest_Integer_Power_to_Investigate)
Variances <- Variances[, c(which(colnames(Variances) == 'Power'), which(colnames(Variances) != 'Power'))]

# Output
Variances
#    Power Uniform_Distribution Normal_Distribution Cauchy_Distribution
# 1      1           0.08336001        1.003362e+00        2.004660e+06
# 2      2           0.08886279        2.013401e+00        1.954473e+18
# 3      3           0.08029366        1.515248e+01        2.209213e+30
# 4      4           0.07103040        9.744815e+01        2.626589e+42
# 5      5           0.06304486        9.661652e+02        3.208136e+54
# 6      6           0.05642795        1.046558e+04        3.973433e+66
# 7      7           0.05095627        1.386277e+05        4.955865e+78
# 8      8           0.04639468        2.018680e+06        6.202649e+90
# 9      9           0.04255073        3.260260e+07       7.776300e+102
# 10    10           0.03927592        5.601374e+08       9.757302e+114","['statistics', 'probability-distributions', 'convergence-divergence', 'variance']"
4723006,What happens when we take the derivative of an implicitly defined function?,"Consider the function y(x) implicitly decided by this equation: $\sin(xy)=\cos(x+y)$ . The graph on plane looks like this: According to my textbook, we can take the derivative of both sides with respect to x to obtain the derivative y'(x)=[some function of x and y]. In this case y'(x) = -(\sin(xy)+y\cos(xy))/(\sin(x+y)+x\cos(xy)). In this case $y'(x) = -\dfrac{\sin(x+y)+y\cos xy }{\sin(x+y)+x\cos xy }$ . Now evaluate $y'(x)$ at the point $A$ . There seems to be two tangent lines at point $A$ . Then which slope does $y'(x)$ at $A$ describe?","['calculus', 'implicit-differentiation', 'derivatives']"
4723044,What is a canonical embedding?,"I‘m taking a course in functional analysis and have to show that $ L^1(\mathbb R) \subsetneq (L^\infty(\mathbb R))^* $ ""in the sense of canonical embedding"". What does this mean exactly? Unfortunately, ""canonical embedding"" is no term we defined in the lecture so I assume it is some ""common term"". I think I have to show that there is a function $$ \iota: L^1(\mathbb R) \to (L^\infty(\mathbb R))^* $$ such that $ \iota(L^1(\mathbb R)) \subsetneq (L^\infty(\mathbb R))^* $ . But  just any function is probably not enough and $\iota$ has to satisfy some more properties. For example, i think injectivity would make sense. Can somebody tell me what one means with a ""canonical embedding""? Is it just a topological embedding, i.e. a homeomorphism onto its image as it is mentioned at Wikipedia?",['functional-analysis']
4723068,Do homeomorphisms preserve topological structure?,"I think something is missing in the definition of homeomorphism I saw. It just said it maps the collection of open sets to the collection of open sets in a bijective way. What exactly makes this preserve topology? I can think of weird situations where each individual open set is mapped to totally disjoint open sets in a bijective way, throwing the topological structure out the window.",['general-topology']
4723124,Finding the double integral over a polar region,"I am not sure if I simplified wrong, and if I didn't, how would I go about integrating this? In the problem statement, the region is $$D = \Big\{(r,\theta) \mid 2\le r \le 3, \frac{\pi}{3} \le \theta \le \frac{2\pi}{3}\Big\},$$ and the function is $$f(x,y) = x^4+2x^2y^2+y^4.$$ So converting this to polar coordinates and setting up the double integral over the region I achieve $$ \int_{\frac{\pi}{3}}^{\frac{2\pi}{3}} \left(\int_{2}^{3}\left[r^4\cos^4(\theta)+2\left(r^2\cos^2(\theta)r^2\sin^2(\theta)\right)+r^4\sin^4(\theta)\right]r \;\mathrm{d}r \right) \mathrm{d}\theta$$ After all my simplifying through grouping and recognizing trig identities I can use I achieve $$\int_{\frac{\pi}{3}}^{\frac{2\pi}{3}} \left(\int_{2}^{3} r^5\left(\cos^4(\theta)+\sin^4(\theta)\right)+2r^3 \; \mathrm{d}r \right) \mathrm{d}\theta = \int_{\frac{\pi}{3}}^{\frac{2\pi}{3}} \left[\frac{665}{6}\left(\cos^4(\theta)+\sin^4(\theta)\right) + \frac{65}{2} \right]\mathrm{d}\theta.$$ I can easily integrate $\frac{65}{2}$ by itself, but how would I integrate the former with $\cos^4(\theta)+\sin^4(\theta)$ ? Obviously, I can pull the constant out and then split them up into two different integrals (and multiply the constant back after integrating both) but to integrate $\cos^4(\theta)$ and $\sin^4(\theta)$ doesn't seem like it would be pretty to do. Did I create the right setup and simplify correctly?","['integration', 'multivariable-calculus', 'definite-integrals']"
4723134,"Are points and vectors (in $\mathbb{R}^n$) different objects? If yes, then why can we switch between them in a proof?","Context In Hubbard and Hubbard's book on vector calculus, $\mathbb{R}^{n}$ is defined as the space of ordered lists of $n$ real numbers. The authors then say that a given element of $\mathbb{R}^{n}$ can be interpreted in the following two ways: An element of $\mathbb{R}^{n}$ is said to be a "" point "" (in $\mathbb{R}^{n}$ ) if it represents some sort of position/state. An element of $\mathbb{R}^{n}$ is said to be a "" vector "" (in $\mathbb{R}^{n}$ ) if it represents some sort of change/increment. Then the book pauses to emphasize that two points cannot be added $(\text{New York}+\text{Boston}=???),$ but two vectors can. It also goes on to define scalar multiplication for vectors, difference of two points, sum of a point and a vector and sum of two vectors. The authors give the following remark: ""An element of $\mathbb{R}^{n}$ is an ordered list of numbers whether it is interpreted as a point or as a vector. But we have very different images of points and vectors, and we hope that sharing them with you explicitly will help you build a sound intuition. In linear algebra, you should think of elements of $\mathbb{R}^{n}$ as vectors. However, differential calculus is all about increments to points. It is because the increments are vectors that linear algebra is a prerequisite for multivariate calculus: it provides the right language and tools for discussing these increments."" Another important remark is also given: ""Sometimes, often at a key point in a proof, we will suddenly start thinking of points as vectors, or vice versa."" Reflections on H+H's Explanation It seems like Hubbard and Hubbard are suggesting that points and vectors are nothing more than labels we give to the same object (a real $n$ -tuple). We choose which label we want to give a $n$ -tuple based on what we are using the $n$ -tuple for/how we are thinking about it. In this case, points and vectors are the same mathematical object ( $n$ -tuple), and the only reason for why we can't scale points, or add points to other points is because it breaks our ""mental model/interpretation"" of points being $n$ -tuples that represent locations/states ( $\text{New York+Boston=???, and 5}\cdot \text{New York=???}$ ). Also, points and vectors being the same objects means there is no harm in switching between the two (provided we don't switch one of them in a way that breaks our interpretation of points/vectors. E.g., if we have $x+y$ , for vectors $x$ and $y$ , we can switch $x$ with a point and leave $y$ alone (or vice versa), but we cannot switch both $x$ and $y$ out for points because we can't add points). This justifies the second remark from the book. This all makes sense to me. Points and vectors in $\mathbb{R}^{n}$ are the same objects ( $n$ -tuples), and we just use these two different terms to provide additional context to how we are thinking about/visualizing/using the $n$ -tuple. Points represent locations/states, vectors represent changes/increments. The ""rules"" of not being able to add two points and not being scale a point are solely there to ensure that our interpretation of what the $n$ -tuple is representing is consistent with our intuition $(\text{New York + Boston=???, 5}\cdot \text{Boston=???}).$ But I have run into a few problems. Questions In some of the popular threads of similar questions it seems like some people are claiming that points and vectors in $\mathbb{R}^{n}$ actually ARE different mathematical objects (See What is the difference between a point and a vector? ). That's not what I thought at first reading H+H, but ok, it seems reasonable that points and vectors are different mathematical objects, rather than just contextual indicators for the same object. They are used to represent different things (location vs displacement) after all, and they have different operations that can be performed on them. So I have the following question. Question 1: Are points and vectors (in $\mathbb{R}^{n}$ ) simply different interpretations of the same mathematical object ( $n$ -tuple), or are they fundamentally different objects? If they are different, how are each of them defined? And if points and vectors ARE different mathematical objects, then how can we reconcile this with the fact that we want to be able to switch between points and vectors in the middle of proofs (Remark 2 from H+H)? This is the second question I have. Question 2: If points and vectors in $\mathbb{R}^{n}$ are different mathematical objects, how are we able to switch between these two distinct objects in the middle of a proof and still have our proof be valid (i.e., How do we reconcile the fact they are different with remark $2$ from the book)? Any help at all would be extremely appreciated!","['real-numbers', 'multivariable-calculus', 'linear-algebra', 'differential-geometry']"
4723138,Pages and book problem - Tearing of a page from book,"There are a certain number of pages in a book. Bob tore a certain page out of the book and later found that the average of the remaining page numbers is $46 \frac{10}{13}$ . What were the page numbers that Bob had torn ? Please let me know where am I going wrong with my approach :- Since the average of the remaining ""page numbers"" has 13 in the denominator , I can say that the remaining number of ""page numbers"" have to be a multiple of 13 So the remaining number of ""page numbers"" can be 13, 26, 39, 52, 65, 78, 91, 104..... Accordingly the original number of ""page numbers"" in the book could be 15, 28, 41, 54, 67, 80, 93, 106... Now, Sum of remaining ""page numbers"" + sum of the torn ""page numbers"" = sum of total ""page numbers"" initially I started making cases now, Case 1:- When original number of ""page numbers"" were 15 So it would have ""page numbers"" from 1 till 15, therefore total of all ""page numbers"" would be (15*16)/2 = 120 Sum of remaining ""page numbers"" = $46 \frac{10}{13}$ * 13 Let sum of torn pages = x x + ( $46 \frac{10}{13}$ * 13) = 120 which gives x to be a negative number Case 2:- When original number of ""page numbers"" were 28 So it would have ""page numbers"" from 1 till 28, therefore total of all ""page numbers"" would be (28*29)/2 = 406 Sum of remaining ""page numbers"" = $46 \frac{10}{13}$ * 26 Let sum of torn pages = x x + ( $46 \frac{10}{13}$ * 26) = 406 x is still negative Case 3:- When original number of ""page numbers"" were 41 So it would have ""page numbers"" from 1 till 28, therefore total of all ""page numbers"" would be (41*42)/2 = 861 Sum of remaining ""page numbers"" = $46 \frac{10}{13}$ * 39 Let sum of torn pages = x x + ( $46 \frac{10}{13}$ * 39) = 861 x is still negative :( Case 4:- When original number of ""page numbers"" were 54 So it would have ""page numbers"" from 1 till 54, therefore total of all ""page numbers"" would be (54*55)/2 = 1485 Sum of remaining ""page numbers"" = $46 \frac{10}{13}$ * 52 Let sum of torn pages = x x + ( $46 \frac{10}{13}$ * 52) = 1485 x<0 Case 5:- When original number of ""page numbers"" were 67 So it would have ""page numbers"" from 1 till 67, therefore total of all ""page numbers"" would be (67*68)/2 = 2278 Sum of remaining ""page numbers"" = $46 \frac{10}{13}$ * 65 Let sum of torn pages = x x + ( $46 \frac{10}{13}$ * 65) = 2278 x<0 Case 6:- When original number of ""page numbers"" were 80 So it would have ""page numbers"" from 1 till 54, therefore total of all ""page numbers"" would be (80*81)/2 = 3240 Sum of remaining ""page numbers"" = $46 \frac{10}{13}$ * 78 Let sum of torn pages = x x + ( $46 \frac{10}{13}$ * 78) = 3240 x<0 Case 7:- When original number of ""page numbers"" were 93 So it would have ""page numbers"" from 1 till 93, therefore total of all ""page numbers"" would be (93*94)/2 = 4371 Sum of remaining ""page numbers"" = $46 \frac{10}{13}$ * 91 Let sum of torn pages = x x + ( $46 \frac{10}{13}$ * 91) = 4371 x=115   (:O) now is it that 2 consecutive page numbers m+m+1=115 one page number will be 57 and other 58 and both of them even lie in the range of 93 ""page numbers"" as well","['word-problem', 'algebra-precalculus', 'solution-verification']"
4723159,A function in $L^1(\mathbb{R}^d)$ satisfying $f(x)+f\ast g(x)=e^{-|x|^2}.$,"Let $g\in L^1(\mathbb{R}^d)$ with $\|g\|_{L^1(\mathbb{R}^d)}<1$ . I would like to show that there is a unique $f\in L^1(\mathbb{R}^d)$ such that $$f(x)+f\ast g(x)=e^{-|x|^2}.$$ My first instinct is to take Fourier transforms to find that, with $\hat{f}(\eta)=\int_{\mathbb{R}^d}e^{-i\eta\cdot x}f(x)\,dx$ , $$\hat{f}(1+\hat{g})=\pi^{d/2}e^{-|\eta|^2/4}.$$ From my hypothesis, I know that $$\|\hat{g}\|_{L^\infty (\mathbb{R}^d)}\le \|g\|_{L^1 (\mathbb{R}^d)}<1.$$ Therefore, I may divide by $1+\hat{g}$ to obtain $$\hat{f}=\frac{\pi^{d/2}e^{-|\eta|^2/4}}{1+\hat{g}}. $$ I am unsure how to go from the conclusion to a unique $f\in L^1 (\mathbb{R}^d)$ .","['functional-equations', 'fourier-analysis', 'functional-analysis']"
4723181,"How should I think about ""multiplicity"" for polynomial roots?","I'm having trouble conceptualizing the idea of multiplicity when it comes to finding roots of polynomials. Consider the example from Multiplicity on Wikipedia: The polynomial $p(x) = x^3 + 2x^2 - 7x + 4$ has $1$ and $-4$ as roots, and can be written as $p(x) = (x+4)(x - 1)^2$ . I understand that, in an algebraic sense, there are three monomials here, each representing a root: $(x+4) \Rightarrow -4$ $(x-1) \Rightarrow 1$ $(x-1) \Rightarrow 1$ What I don't understand is that, since the graph of this polynomial intersects the x axis only twice, why do we say there are three solutions to $x^3 + 2x^2 - 7x + 4 = 0$ ?","['algebra-precalculus', 'roots', 'polynomials']"
4723197,On The Role of The Axiom of Specification,"In Halmos' book on Naive Set Theory, the author elucidates the role of the axiom of specification in developing the realm of set theory. According to Halmos, the axiom of specification is useful for asserting the existences of sets given by other axioms. Examples he gives in the book have to do with the Axiom of Pairing. The axiom of pairing states that for any two given sets, there exists a set that both belong to. However, he says, the axiom of pairing doesn't assert the existence of the pair itself, it merely says that there exists a larger set in which both sets are a part of. It is the duty of the axiom of specification to assert then that there also exists a set that contains both of those sets, and nothing else. I don't understand this then: why not just state the axiom of pairing as ""for any two given sets, there exists a set that both are a member of, and no other set is"". I also don't completely understand the necessity of the axiom of pairing, isn't it possible to derive it as a conclusion of the axiom of specification and a few logical operators? By simply specifying: $A = \{x: x = B$ or $x = C\}$ . Doesn't this already define $A$ as the set that is the pairing of $B$ and $C$ ? Thank you in advance.","['elementary-set-theory', 'axioms', 'set-theory']"
4723246,"Given $f(x_1,x_2,x_3,...,x_n)=0$, does a $g$ exist such that $g(x_1,x_2,x_3,...,x_{n-1})=x_n$?","Given $f(x_1,x_2,x_3,...,x_n)=0$ , is it true that there exists a function $g$ such that $g(x_1,x_2,x_3,...,x_{n-1})=x_n$ ? If so, how could I go about proving it for the general case? If not, what would be the broadest additional given that would make it true, i.e. the definition which would be true for the largest set of possible functions. For example, If I changed the statement to ""given a linear function $f(x_1,x_2,x_3,...,x_n)=0$ "" it is fairly straightforward to show that there exists a function $g$ such that $g(x_1,x_2,x_3,...,x_{n-1})=x_n$ , but restricting the given function to only linear functions would be a pretty exclusive given.","['proof-explanation', 'proof-writing', 'analysis', 'real-analysis', 'functions']"
4723315,How to prove $AP$ is constant in this plane geometry problem?,"As is shown in the picture, $ ∠ACB=∠EDB=90^{o},AC=2,BC=4,AE=3,ED=DB,∠PCD=45^{o}$ , how to prove that $AP=1$ ? By adding additional conditions (like $AE$ is parallel to $CB$ ) I can verify that $AP=1$ . I try to find similar triangles in the graph, but I haven't found.","['euclidean-geometry', 'geometry', 'plane-geometry']"
4723318,Comparing the Sample Variance with the Population Variance,"I read in the following paper ( https://iopscience.iop.org/article/10.1088/0026-1394/41/3/004/pdf ) on page 133 "" The conditional standard deviation (...) is necessarily an underestimate of its unconditional standard deviation. "" . I am trying to understand why this statement is true. I will now outline my understanding of this in 3 parts. Part 1: In this link ( Proving that Sample Variance is an unbiased estimator of Population Variance ), a proof is given that shows the sample variance is an unbiased estimator of the population variance: $$E(S^2) = \frac{n-1}{n}E(X_1-Y_1)^2 = \frac{n-1}{n}\text{var}(X_1-Y_1) = \frac{n-1}{n}\left(\sigma^2 + \frac{\sigma^2}{n-1}\right) = \sigma^2$$ Part 2: In this link ( https://stats.stackexchange.com/questions/496424/how-to-prove-s2-is-a-consistent-estimator-of-sigma2 ), a proof is given that shows the sample variance is a consistent estimator of the population variance: \begin{align*}
&\mathbb{P}(\mid s^2 - \sigma^2 \mid > \varepsilon )\\
&= \mathbb{P}(\mid s^2 - \mathbb{E}(s^2) \mid > \varepsilon )\\
&\leqslant \dfrac{\text{var}(s^2)}{\varepsilon^2}\\
&=\dfrac{1}{(n-1)^2}\cdot \text{var}\left[\sum (X_i - \overline{X})^2)\right]\\
&=\dfrac{\sigma^4}{(n-1)^2}\cdot \text{var}\left[\frac{\sum (X_i - \overline{X})^2}{\sigma^2}\right]\\
&=\dfrac{\sigma^4}{(n-1)^2}\cdot\text{var}(Z_n)\\
&=\dfrac{\sigma^4}{(n-1)^2}\cdot 2(n-1) = \dfrac{2\sigma^4}{n-1} \stackrel{n\to\infty}{\longrightarrow} 0
\end{align*} Thus, $ \displaystyle\lim_{n\to\infty} \mathbb{P}(\mid s^2 - \sigma^2 \mid > \varepsilon ) = 0$ , i.e. $ s^2 \stackrel{\mathbb{P}}{\longrightarrow} \sigma^2 $ as $n\to\infty$ , which tells us that $s^2$ is a consistent estimator of $\sigma^2$ . Part 3: Using some algebraic manipulation, I can see that the sample variance appears to always be less than the population variance : The sample variance is defined as: $$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$$ where $n$ is the sample size, $x_i$ are the individual observations, and $\bar{x}$ is the sample mean and the population variance is denoted as $\sigma^2$ . OLS (Ordinary Least Squares) tell us that $\bar{x}$ minimizes the sum of squared deviations - thus: $$\sum_{i=1}^n (x_i - \bar{x})^2 \leq \sum_{i=1}^n (x_i - \mu)^2$$ Dividing both sides by $n-1$ : $$\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu)^2$$ Further simplifying and substituting (for large enough $n$ ): $$\sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2$$ $$\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \frac{1}{n-1} n\sigma^2 = \sigma^2$$ $$s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \sigma^2$$ This proves that the sample variance $s^2$ is always less than or equal to the population variance $\sigma^2$ . On another note, using informal logic, an argument can be made that the sample variance might not include extreme outliers, whereas the population would include these extreme outliers. Extreme outliers have large deviations from the mean - thus, the presence of extreme outliers would increase the variance calculations. Thus, the sample variance can technically never be smaller than the population variance. My Question: How can all 3 parts simultaneously be correct at the same time? If the sample variance is said to estimate the population variance without any bias, the sample variance is said to converge to the population variance for large samples - then how can the sample variance always be guaranteed to be less than the population variance ?  Is this not a contradiction? Thanks! References: Why does sample standard deviation underestimate population standard deviation?",['probability']
4723328,"If $P \iff Q$ is true, why does any proof of $Q$ have to use the fact that $P$ is true?","If $P \iff Q$ is true, does any proof of $Q$ need to use the fact that $P$ is true? I am asking this because my course notes say that because a stable matching does not always exist with only one group, and any proof of a stable matching always existing must use the fact that there are two different groups of people where people of the the same group cannot match with each other. My understanding is that here, (there being two distinct groups of people to match) $\iff$ (there will always be a stable matching). When trying to prove that a stable matching always exists, I am trying to see why we must use the condition that is sufficient and necessary for it. More generally, when $P \iff Q$ is true and trying to prove that $Q$ is true, why we must use the fact that $P$ is true? It seems that when $P \iff Q$ is true, we can prove $Q$ using the fact that $P$ is either true or false, and if $P$ is false then the principle of explosion can help us prove $Q,$ so we don't need to use the fact that it's true.","['propositional-calculus', 'proof-writing', 'logic', 'discrete-mathematics', 'algorithms']"
4723349,Why doesn't the trace operator preserve regularity?,"The trace operator maps $H^1(\Omega)$ into $H^{1/2}(\partial \Omega)$ , I have used this fact several times, and know of references where to find the proof. Let us assume that the boundaries are arbitrarily smooth.
The trace operator then maps $C^1(\Omega)$ into $C^1(\partial \Omega)$ . Do you have an intuitive reason why we lose regularity in the weak case? I think this may be a good way to get valuable understanding of how weak and strong derivatives differ.","['partial-differential-equations', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
4723370,Does $\sum_{n=0}^{\infty}|f_n| r^n \leq C \|f\|_{L^{\infty}(|z| \leq r)}$ when $f(z)$ is holomorphic?,"Let $$f(z) = \sum_{n=0}^{\infty}f_nz^n$$ be holomorphic on $|z| < R$ and let $0 < r < R$ . I'm wondering if there exists some universal constant $C > 0$ (i.e. independent of $r,R,f$ ) for which $$\sum_{n=0}^{\infty} |f_n| r^n \leq C \|f\|_{L^{\infty}(|z| \leq r)}$$ I'm aware of similar, but different results Cauchy's inequality : $|f_n|r^n \leq \|f\|_{L^{\infty}(|z| \leq r)}$ for all $n \geq 0$ Parseval's identity : $\sum_{n=0}^{\infty} |f_n|^2 r^{2n} = \frac{1}{2\pi}\int_0^{2\pi} |f(re^{i\theta})|^2 d\theta$ which implies $$\left( \sum_{n=0}^{\infty} |f_n|^2 r^{2n} \right)^{\frac{1}{2}} \leq \|f\|_{L^{\infty}(|z| \leq r)}$$ Hadamard Multiplication Theorem Hardy's inequality","['complex-analysis', 'upper-lower-bounds']"
4723389,A question about a limit with two parameters,Determine the values of the two real parameters $a$ and $b$ such that $$\lim_{x \to 0}\frac{\sin x-(ax^3+bx)}{x^3}=1$$ I immediately have thought of the Hopital theorem. We have: $$\lim_{x \to 0}\frac{\sin x-(ax^3+bx)}{x^3}=\lim_{x \to 0}\frac{\cos x-3ax^2-b}{3x^2}=\lim_{x \to 0}\frac{1-b}{3x^2}\equiv 1$$ Now I am not able to find $a$ and $b$ .,"['limits', 'calculus']"
4723402,Modelling tide vs time with trigonometric functions,"I have been asked to collect tidal data (time and height) for a location over a seven day period to answer the following question... "" Model the scenario using a single trigonometric function . Plot the height verse elapsed time in a suitable graphing software and interpret the observable amplitude, vertical shift and period of the data set, to fit a trigonometric model to the data. Provide reasons for your choices."" I'm assuming they want a sinusoidal function that best fits the data since the minima and maxima, over the 7 day interval, vary quite a bit and cannot be accurately modelled with a single trig function. However, the question later asks to predict heights of high and low tides at a future time, which will be difficult given the inaccuracy of the model. Any advice will be much appreciated.","['algebra-precalculus', 'functions', 'trigonometry', 'graphing-functions']"
4723453,Compositum of field extensions in context of $\mathbb Z_p$ extension,"Suppose I have a $\Gamma \simeq \mathbb Z_p $ extension $F_\infty /F$ of a number field $F$ . Let $F_n$ be the fixed field of $\Gamma _n \simeq p^n \mathbb Z_p$ . Denote by $L_n$ the maximal unramified abelian $p$ extension of $F_n$ in which all the primes above $p$ in $F_n$ split completely. Similarly, we may define $L_\infty$ . Let $\mathcal L _n$ be the maximal abelian extension of $F_n$ contained in $L_\infty$ . We know that there exists an $n_0$ such that for $n\geq n_0$ all primes above $p$ are totally ramified in $F_\infty /F_n$ . I want to prove that $L_n \mathcal L_{n_0} = \mathcal L_n$ . It is easy to see that $L_n \mathcal L_{n_0}$ is an abelian extension of $F_n$ . I don't know how to show that $L_n\mathcal L_{n_0} \subset L_\infty$ to conclude that $L_n \mathcal L_{n_0}\subset \mathcal L _n$ . Also, how do I prove the other side? Edit Is it true that $F_\infty =L_nF_\infty$ ?","['galois-theory', 'number-theory', 'algebraic-number-theory', 'class-field-theory']"
4723456,A problem of minimum,"Problem : Among all rectangular parallelepipeds having for base a square and of constant volume $V$ , determine the one of minimum total area also has diagonal of minimum length. Solution : Given $x$ and $y$ as the respective measurements of the base edge and height, the volume $V$ is given by $V=x^2y \iff y = \frac{V}{x^2}$ . In principle there is nothing to prevent the base edge from being as large as
as desired, so it must be $x ≥ 0$ . The total area will be the sum of the two bases i.e. $2x^2$ and of the lateral faces $4xy$ so $$S(x)=2x^2+4\frac{V}{x}$$ The derivate is: $$S'(x)=\frac{4(x^3-V)}{x^2}$$ cancel if $x=\sqrt[3]{V}$ . The corresponding value of $y$ is $$y=\frac{V}{\sqrt[3]{V^2}}=x$$ so the solution is
a cube. That it is a minimum is confirmed by the range of positivity of the first derivative,
given by the inequality $x>\sqrt[3]{V}$ . For $x$ between $0$ and $\sqrt[3]{V}$ the derivative is negative, which
confirms that it is a point of minimum. Question : How can I determine the one of minimum total area also has diagonal of minimum length ?","['calculus', 'derivatives']"
4723486,On summability of functions in weighted Lebesgue spaces,"I consider a (continuous) function $V \colon \mathbb{R}^n \to \mathbb{R}$ such that $V \geq 1$ and $\lim_{|x|\to +\infty} V(x)=+\infty$ . Let $u$ be a measurable function on $\mathbb{R}^n$ such that $$\int V(x) |u(x)|^2 \, dx < \infty.$$ Let now $p>2$ be a real number. Under what (general) assumptions on $V$ and possibly on $u$ can we conclude that $$\int V(x) |u(x)|^p \, dx$$ is also finite? It seems to me that some additional condition must be imposed, since $p>2$ . I do not know how to use the condition that $V$ diverges at infinity (and maybe this condition is useless here). Any idea or suggestion? The case $V \to 0$ at infinity seems to be more popular in the literature, but I am working in the opposite setting.","['measure-theory', 'lp-spaces', 'sobolev-spaces']"
4723509,Riemann Zeta function's behavior near the pole and Euler-Mascheroni constant,"I want to show that $$\zeta(z) = \frac{1}{z-1} + \gamma   + O(z-1)$$ for $z \rightarrow 1 $ . Why is it enough to show this statement for real $z = s \in (1,2)$ ? We know that $\zeta$ is meromorphic and has only one simple pole at $z=1$ but how does this help? Now, for $s \in (1,2)$ we can write $$\zeta(z) - \frac{1}{z-1} = \sum_{n=1}^{\infty} k^{-s} - \int_{1}^{\infty}t^{-s}dt
=\sum_{k=1}^{\infty}\int_k^{k+1}(k^{-s} - t^{-s}) dt$$ and since $|k^{-s}-t^{-s}| = |\int_{k}^{t}su^{-s-1}du| \leq |s|k^{-s-1}$ we know that there exists a constant M such that $$\sum_{k=1}^{\infty}\int_k^{k+1}(k^{-s} - t^{-s}) dt< M$$ for all $s \in (1,2)$ . This implies the whole thing is uniform there and that we are allowed pull in limit, in the end we have: $$\lim_{s \rightarrow 1}(\zeta(z) - \frac{1}{z-1}) = \sum_{k=1}^{\infty} \int_k^{k+1}(k^{-1}-t^{-1}) dt = \sum_{k=1}^{\infty}(k^{-1}-ln(k+1)+ln(k)) = \gamma$$ . I'm still a little confused with big O-notation, how does this imply our statement for general $z$ ?","['complex-analysis', 'euler-mascheroni-constant', 'riemann-zeta']"
4723523,"Elementary proof of $\left(1+\frac{t}{\sqrt{n}}\right)^{n}e^{-\sqrt{n}t}\le\left(1+t\right)e^{-t}$ for $n\in\mathbb{N},t\ge0$ .","I am looking for an elementary proof for the inequality $\left(1+\frac{t}{\sqrt{n}}\right)^{n}e^{-\sqrt{n}t}\leq\left(1+t\right)e^{-t}$ for $n\in\mathbb{N},t\geq0 $ I encountered this inequality while writing an advanced exercise for first year students. I assumed it can be done by defining the function $$\varphi\left(x\right)=\left(1+\frac{t}{x}\right)^{x^{2}}e^{-xt}\,,$$ and show it monotonically decreases for $x\geq1$ and positive $t$ , but failed to do so.
Does anyone know an elementary way to prove the above inequality?","['multivariable-calculus', 'calculus', 'exponential-function', 'inequality']"
4723530,Randomly landing on every member of a set while the set is growing: Does it help to play before all items are added?,"This question has come up with me in two different Final Fantasy games, where you want or need to collect every memory or skill through random selection where not all memories/skills are available until late in the (main) game (not in the random selection ""game""). Collecting a memory/skill does not remove it from the set (you can randomly land on it again) My question is: In terms of getting every entity in the set, does it help (in terms of the expected number of plays to pick every entity) to play this sub-game before all entities have been added to the set (assuming even probability distribution over all entities)? If what I am asking doesn't make sense to you, please ask let me know in a comment and I will try to clarify the question.","['statistics', 'probability']"
4723564,"Proving that a smaller inscribed, tangent circle to another circle makes an angle bisector of the given angle, joining the two tangent points","Here is the problem: A chord $AB$ is drawn in a circle. Another circle is tangent to this chord at the point $M$ and to the given circle at the point $K$ . Prove that $KM$ is the bisector of the angle $AKB$ A figure to the problem: * All the data that is not given in the problem's description is marked with blue or red. On my own I couldn't solve it, neither could I with a hint from the textbook, which is: Let the segments $AK$ and $BK$ intersect the circle of smaller radius at points $E$ and $F$ , respectively. Prove that $EF \parallel AB$ . To do this, draw a tangent to the two given circles, that they both will share, through the point K. I have indeed proved that they are parallel, but don't see where to move next: After making all the additional drawings suggested by the hint, I've also put two points $L$ and $J$ on the tangent line that passes through the point $K$ . By a theorem in my book(which I think is a commonly known fact), since the $LJ$ line is tangent to both circles: $\angle LKA = \frac{1}{2} \cup AK = \frac{1}{2} \cup EK \implies \angle KFE = \angle KBA \implies EF \parallel AB$ . The same way it can be proven that $\angle JKB = \angle KEF = \angle KAB$ , but it's not necessary, I guess. From this point on I've tried a variety of different approaches, but none of them brought me even a bit closer to the proof. What am I missing? How to prove it?","['tangent-line', 'circles', 'geometry']"
4723577,Computing $\iint_S (y+z)\mathrm{d}y\mathrm{d}z+(z+x)\mathrm{d}z\mathrm{d}x+(x+y)\mathrm{d}x\mathrm{d}y$,"Computing $$I=\iint_S (y+z)\mathrm{d}y\mathrm{d}z+(z+x)\mathrm{d}z\mathrm{d}x+(x+y)\mathrm{d}x\mathrm{d}y,$$ where $S$ is the upper side of the plane $x+y+z=1$ located inside the interior of the sphere $x^2+y^2+z^2 \leq 1$ . I have tried two methods: First, I attempted to close the surface and use Divergence Theorem, but the integral over the sphere surface was also difficult to calculate. Second, I directly projected the integral onto the $xy$ -plane, but the projected surface became an ellipse, and I am not familiar with handling such cases. I wonder if there are any other solutions available. I found a stupid mistake in my second solution. I've corrected it and wrote an answer.","['multivariable-calculus', 'surface-integrals']"
4723640,"Generalizing a result in $\mathbb{C}[x]$ to a result in $\mathbb{C}[x,y]$","Let $u,v \in \mathbb{C}[x]$ . Assume that $\mathbb{C}(u,v)=\mathbb{C}(x)$ .
Then by this answer , there exists $\alpha,\beta,\gamma \in \mathbb{C}$ such that $\langle u-\alpha,v-\beta \rangle = \langle x-\gamma \rangle$ ,
where $\langle * \rangle$ is the ideal generated by $*$ . Is this result can be generalized to $\mathbb{C}[x,y]$ ? First attempt Let $s,t,u,v \in \mathbb{C}[x,y]$ .
Assume that $\mathbb{C}(s,t,u,v)=\mathbb{C}(x,y)$ .
Is it true that there exist $\alpha,\beta,\gamma,\delta,\epsilon_1,\epsilon_2 \in \mathbb{C}$ such that $\langle s-\alpha,t-\beta,u-\gamma,v-\delta \rangle = \langle x-\epsilon_1,y-\epsilon_2 \rangle$ ? Second attempt: Let $u,v \in \mathbb{C}[x,y]$ .
Assume that $\mathbb{C}(u,v)=\mathbb{C}(x,y)$ .
Is it true that there exist $\alpha,\beta,\gamma,\delta \in \mathbb{C}$ such that $\langle u-\alpha,v-\beta \rangle = \langle x-\gamma,y-\delta \rangle$ ? Are there counterexamples for both attempts? Thank you very much! Edit: Is the converse also true?
Namely: Let $u,v \in \mathbb{C}[x,y]$ and assume that there exist $\alpha,\beta,\gamma,\delta \in \mathbb{C}$ such that $\langle u-\alpha,v-\beta \rangle = \langle x-\gamma,y-\delta \rangle$ ( $u$ and $v$ generate a maximal ideal).
Then $\mathbb{C}(u,v)=\mathbb{C}(x,y)$ . I guess the answer to the edit is positive, adjusting this one-dimensional answer to our two-dimensional case. Remarks about the edit: (1) A generalized edit question: Let $u_i \in \mathbb{C}[x,y]$ , $1
\leq i \leq n$ , $2 \leq n \in \mathbb{N}$ , and assume that there exist $\alpha_i, \gamma, \delta \in \mathbb{C}$ such that $\langle u_i-\alpha_i \rangle_{i=1}^{n} = \langle x-\gamma,y-\delta \rangle$ (the $u_i$ 's generate a maximal ideal).
Then $\mathbb{C}(u_1,\ldots,u_n)=\mathbb{C}(x,y)$ . (2) A special case of the edit:
Let $u,v \in \mathbb{C}[x,y]$ and assume that there exist $\alpha,\beta,\gamma,\delta \in \mathbb{C}$ such that $\langle u-\alpha,v-\beta \rangle = \langle x-\gamma,y-\delta \rangle$ ( $u$ and $v$ generate a maximal ideal).
Assume, in addition, that the Jacobian of $u$ and $v$ is a non-zero scalar, namely, $Jac(u,v):=u_xv_y-u_yv_x \in \mathbb{C}^{\times}$ .
Then $\mathbb{C}[u,v]=\mathbb{C}[x,y]$ .
Indeed, $(u,v)$ is a Jacobian pair, then $(u-\alpha,v-\beta)$ is clearly also a Jacobian pair and, by assumption, $\langle u-\alpha,v-\beta \rangle$ is a maximal ideal of $\mathbb{C}[x,y]$ , hence by a known theorem $(u-\alpha,v-\beta)$ is an automorphic pair, and then $(u,v)$ is an automorphic pair, $\mathbb{C}[u,v]=\mathbb{C}[x,y]$ . The later assumption on the invertibility of the Jacobian of $u$ and $v$ does not follow from the former assumption that $u$ and $v$ generate a maximal ideal; for example, $u=x$ and $v=y+xy$ . From $y=v-xy=v-uy$ , we see that $\langle u,v \rangle = \langle x,y \rangle$ , and the Jacobian of $u$ and $v$ is not invertible (not a scalar).
Notice that $\mathbb{C}(u,v)=\mathbb{C}(x,y)$ because, $y=\frac{v}{1+u}$ , so the question in the edit may have a positive answer. Any comments about the edit are welcome!","['field-theory', 'ring-theory', 'algebraic-geometry', 'polynomials', 'commutative-algebra']"
4723667,Do the odd coefficients of the power series for tangent form a decreasing sequence?,"The beginning of the power series of the tangent function is given by $$\tan(x) = x+\frac{x^3}{3}+\frac{2\,x^5}{15}+\frac{17\,x^7}{315}+
 \frac{62\,x^9}{2835}+\frac{1382\,x^{11}}{155925}+\frac{21844\,
 x^{13}}{6081075}+\frac{929569\,x^{15}}{638512875}+\frac{6404582
 \,x^{17}}{10854718875}+\frac{443861162\,x^{19}}{1856156927625}
 +\cdots $$ Do the odd coefficients of the series form a decreasing sequence? It seems so. The (2n-1)th coefficient of the series is given by $$
a_n = 4^n(4^n-1)|B_{2n}|/(2n)!
$$ where $B_n$ is the Bernoulli numbers. I don't know how to evaluate the quotients of the successive even Bernoulli numbers though.","['power-series', 'generating-functions', 'trigonometry', 'sequences-and-series']"
4723668,Change of variable formula for multivariate integral with constrain,"Suppose $g: \mathbb{R}^{n} \to \mathbb{R}$ is $C^{1}$ and satisfies $\nabla g(x) \neq 0$ for every $x \in \mathbb{R}^{n}$ . Let $E \in \mathbb{R}$ be fixed. I want to find an explicit expression for the integral: $$\int_{g(x) = E}f(x)dx,$$ provided the integral exists. I sketched some calculations, but I am not completely sure if it is correct. There are some steps I am not still very convinced. My idea is the following. Let $\phi: \mathbb{R}^{n}\to U\subset \mathbb{R}^{n}$ be a change of variable transformation (diffeomorphism) given by: $$\phi(x) = (\phi_{1}(x),\phi_{2}(x),...,\phi_{n}(x)) \tag{1}\label{1}$$ where I choose $\phi_{1}(x) = g(x)$ and, for each $i=2,...,n$ , $\phi_{i}: \mathbb{R}^{n} \to \mathbb{R}$ are $C^{1}$ functions such that the change of variables $\phi$ is orthogonal, in the sense that $\langle \nabla \phi_{i}(x),\nabla\phi_{j}(x)\rangle = 0$ for every $i\neq j$ . Consider the region: $$R = \{y = (y_{1},...,y_{n}) \in \mathbb{R}^{n}: y_{1} = E\}\tag{2}\label{2}$$ so we have: $$\{x \in \mathbb{R}^{n}: g(x) = E\} = \phi^{-1}(R) = \{x \in \mathbb{R}^{n}: \phi_{1}(x) = g(x) = E\}.$$ Using the change of variables formula, we obtain: $$\int_{g(x) = E}f(x) dx = \int_{R}f(\phi^{-1}(y))\frac{1}{|\det D\phi(\phi^{-1}(y))|}dy = \int_{\mathbb{R}^{n}}f(E,y_{2},...,y_{n})\frac{1}{\|\nabla g(E,y_{2},...,y_{n})\|}d\Sigma_{E}$$ where $\|x\| = \sqrt{x_{1}+\cdots +x_{n}^{2}}$ is the usual Euclidean norm and: $$d\Sigma_{E} = \frac{dy_{2}\cdots dy_{n}}{\prod_{i=2}^{n}\|(\nabla\phi_{i})(E,y_{2},...,y_{n})\|}$$ First of all, I would like to know if my reasoning and formula are correct. If so: Can I always find a change of variable $\phi$ which is orthogonal and has one of its components, say, $\phi_{1} = g$ as I did? I am not entirely convinced about it.","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'change-of-variable']"
4723702,A particular nonlinear second-order ODE with parameter,"I am trying to solve the ODE: $$ y'' = \frac{ay(y')^2}{1+ay^2}$$ where $a \geq 0$ is a given parameter, and suppose we have initial conditions $y(0) = \mu, y'(0) = \eta$ . WolframAlpha tells me that when $a = 1$ we have a general solution of the form $y = \sinh(c_1(c_2+x))$ . Of course if $a = 0$ we get a solution of the form $y = c_1 x + c_2$ . Is there anything that can be said about the case of general $a$ ?",['ordinary-differential-equations']
4723740,What kind of object is the (second) exterior derivative of a moving point $R$ in $\mathbb R^n$? What does $dR\wedge\omega \mathbf e$ mean?,"TL;DR: Given $f:\mathbb R \to \mathbb R^n$ smooth enough, what are $df$ and $d^2f$ and how are they computed wrt possibly moving bases $\{e_i(t)\}$ ? I (believe that I) understand the answers in the ""reverse"" case where $g:\mathbb R^n \to \mathbb R$ - then $dg$ is the second component of the natural extension of $f$ to a fiber bundle homomorphism $T\mathbb R^n\to T\mathbb R$ , that is, $dg:\mathbb R^n \to (R^n)^*$ , and $d^2g:\mathbb R^n\to(R^n)^*\wedge (R^n)^*$ , or $dg\in A^1(T\mathbb R^n)$ and $d^2g\in A^2(T\mathbb R^n)$ . But with my $f$ , the $df$ and $d^2f$ just don't ""type check"" because the ""star"" gives scalar-valued functions, not vector-valued ones. Long story : I'm reading a physics book and I just can't get my head around what they're doing. Assume every function here to be $C^\infty$ . Using Einstein notation , i.e. every expression $a^i b_i$ is meant to read $\sum_i a^ib_i$ . Setup. In a smooth manifold $M\subseteq \mathbb R^n$ we have a rigid body moving within a time interval $I\subseteq \mathbb R$ . The body movement is described by a point $\mathbf R:I\to M$ and a coordinate system (basis) $e_i : I\to \mathbb R^n,i=1,\ldots,n$ , thought of as ""attached"" to the body for each $t\in I$ at the point $\mathbf R(t)$ . For brevity I'll omit the dependence on time and write $\mathbf R$ even if $\mathbf R(t)$ is meant (and $e_i$ instead of $e_i(t)$ ). Denote the standard basis in $\mathbb R^n$ with $\boldsymbol{\epsilon}_{i}$ . Now $\mathbf R=R^i \boldsymbol\epsilon_i$ for some $R^i(t),i=1,\ldots,n$ . Since (for each $t\in I$ ) $\{e_i(t)\}$ forms a basis in $\mathbb R^n$ , too, we can express $\boldsymbol{e}_{i}=g_{i}^{j}(\mathbf{R})\boldsymbol{\epsilon}_{j}$ and $\boldsymbol{\epsilon}_{i}=\left(g^{-1}(\mathbf{R})\right)_{i}^{j}\boldsymbol{e}_{j}$ for some invertible matrices $g(\textbf R(t))$ . First differential. Next in the book they take the differential of $\mathbf R$ and do some computations: $$ d\mathbf{R}
=d\left(R^{i}\boldsymbol{\epsilon}_{i}\right)
=\left(dR^{i}\right)\boldsymbol{\epsilon}_{i}
=\left(dR^{i}\right)\left(g^{-1}(\mathbf{R})\right)_{i}^{j}\boldsymbol{e}_{j}$$ I'm uneasy about the second equality $d\left(R^{i}\boldsymbol{\epsilon}_{i}\right) =\left(dR^{i}\right)\boldsymbol{\epsilon}_{i}$ transforming the differential of a vector into a vector multiplied by the differential of a scalar function . Though suspicious, at least symbolically it seems legit because $\boldsymbol{\epsilon}_{i}$ does not depend on the time $t\in I$ , so it somehow makes sense to reduce the action of $d$ only to the scalar multiplier $R^i$ . What is the nature of the product between a differential 1-form $dR^i$ and a vector $\boldsymbol \epsilon_i$ ? Second differential. Here's where it gets too wild for me to move forward easily. They take the second differential of $\mathbf{R}$ like this: $$0=dd\mathbf{R}=d(\omega^j \boldsymbol{e}_{j})=d\omega^j \boldsymbol{e}_{j}-\omega^j \wedge d\boldsymbol{e}_{j}$$ where the $\omega^j$ are defined s.t. $d\mathbf{R}=\omega^j \boldsymbol{e}_{j}$ . We have now a product of a two-form $d\omega^j$ with a vector $\boldsymbol{e}_{j}$ , plus a wedge product of a one-form $\omega^j$ with a ""vector-form"" $d\boldsymbol{e}_{j}$ , and both are supposed to be of the same type. $\omega^j \wedge d\boldsymbol{e}_{j}$ looks like complete non-sense to me, because a wedge product can take only same-type objects. There is $V\wedge V$ but never $V\wedge W$ for $V\neq W$ . And yet, here $\omega^j$ is a ""scalar"" one-form while $d\boldsymbol{e}_{j}$ is a ""vector"" one-form, and they're wedged together. And I'm not even sure if I have to write the vector in the second position, it seems if I write $-d\boldsymbol{e}_{j} \wedge \omega^j$ it'd be ok, too. I suspected something like a ""vector-valued form"" should exist, and indeed there's a wikipedia article on it. I read through it a few times, but I got too confused with the abstract construction done in two parts - defining a ""bundle-valued"" form and then taking a tensor product of bundles. Neither could I really understand what are the rules I can use in my more ""practical"" problem. My question s are: what is the ""type signature"" of $\mathbf R$ as a (vector-valued) diffferential (0-)form, allowing us take its exterior derivatives $d\mathbf R$ and $d^2\mathbf R$ ? what does it mean to multiply ""scalar"" and ""vector"" forms? how is such a mixed-type product computed? Why does $d(fg)=df\wedge g-f\wedge dg$ still apply, if $f$ and $g$ are differently typed?","['differential', 'exterior-derivative', 'smooth-manifolds', 'differential-geometry']"
4723745,Taxi distance count to all lattice points at specified distance from the lattice origin and of a specified dimensionality?,"Setup of the problem Description of the lattice The problem is stated on a non-negative integer ( $ℤ_{\ge 0}$ ) lattice. The specification of distance is also a non-negative integer ( $ℤ_{\ge 0}$ ). The specification of dimensionality is positive integer ( $ℤ_{\gt 0}$ ). Lattice may considered to be labeled by vectors of non-negative integers ( $ℤ_{\ge 0}$ ) of dimensionality length. Description of paths involved in the question A trips will start from the origin, a constant array of $0$ s, which is a lattice point in the lattice. The set of destinations (a set of lattice points) is the set of lattice point whose sum of vector coordinates is distance . The permissible transitions between lattice points are those which add an integer $1$ to exactly one of the vector components of the current lattice point's label vector. A path is an ordered set of legal transitions from one lattice point to another. The problem statement Starting from the origin and transitioning under the above rules to a destination set, how many distinct paths are there? The answer should be a formula or method that gives single non-negative integer answer for a given distance and dimensionality . Remark: Therefore, there are dimensionality legal transitions from a given lattice point. There is not stated upper limit of the magnitude of the vector components; but, effectively, a matrix of size $(\text{distance}+1)^{\text{dimensionality}}$ ought to be a sufficiently large work area for a computer implementation.","['permutations', 'programming', 'combinatorics']"
4723811,Closed form solution for indexing combinations from n choose r,"I have a list of combinations resulting from n choose r where order doesn't matter and without repeats, and the list is ordered based on the first choice. To make this concrete, in my specific case, it's 6 choose 2, so the list is as follows: (1,2) (1,3) ... (1,6) (2,3) (2,4) (2,5) (2,6) (3,4) ... (5,6) for a total of 15 combinations. If I give you both choices, what is the closed form solution for it's position in this list? For example, the outputs from the closed form solution for some sample inputs would be as follows: i.e. (1,6) -> 5, (2,3) -> 6, (3,5) -> 11 I feel like there must be a closed form solution to this involving factorials, but I can't seem to figure it out, and it's actually relevant for my work. Wasn't sure what exact search terms to use for this, so let me know if there's an answer out there already.","['permutations', 'statistics', 'combinatorics', 'factorial']"
4723830,Guessing position of a random number out of three random numbers,"Let's say there are three random numbers in the interval $[0,1]$ (i.i.d. from the uniform distribution). We do not know what the numbers are. We are shown one of the numbers and our goal is to determine if this shown number is the smallest, largest, or the one in the middle. What is the best strategy that maximizes our probability of success? First though: if the number is less than 1/3, say it is the smallest. If it is between 1/3 and 2/3, say it is the middle one. Otherwise, say it is the largest one. Second thought: Generalize the first thought and find the optimal ""pivot"". that is, if the number is less than x, say it is the smallest. If it is between x and 1-x, say it is the middle one. Otherwise, say it is the largest one. Compute the probability of success as a function of x. Find the global maximum for x values in $[0,1/2]$ . My attempt for the 'Second thought':
I computed that the probability of success is given by the following $$p(x) = \frac{1}{2} (2-x)^2 x + \frac{1}{2}(1-2x)$$ and I concluded that the maximum is attained at $x = \frac{4-\sqrt{10}}{3}$ . However, I was running a simulation and it seems that the pivot x=1/3 gives the best success rate. I am confused. Can anyone help me? Here is the simulation code (you can change the value of pivot to see what happens): import numpy as np
import random
import math
def sim():

    a1 = random.uniform(0,1)
    a2 = random.uniform(0,1)
    a3 = random.uniform(0,1)

    pivot = 0.33

    solution = 0
    if a1 < a2 and a1 < a3:
        solution = -1
    elif a1 > a2 and a1 > a3:
        solution = 1
    else:
        solution = 0


    
    my_guess = 0

    if a1 < pivot:
        my_guess = -1
    elif pivot < a1 and a1 < (1-pivot):
        my_guess = 0
    else:
        my_guess = 1

    return int(solution==my_guess)

wins = [sim() for x in range(10**7)]

print(f""Avg wins: {np.mean(wins)}"") ```","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4723838,"Are algebroids ""just matrices?""","Algebroids are particularly interesting structures: they are basically categories enriched over $K$ -Vect for some field $K$ . This means the hom-sets are all vector spaces, and composition is ""bilinear"" in a certain sense. (Let's just focus on when $K$ is a field for now rather than an arbitrary ring.) Most examples I can think of are equivalent to some subset of matrices with the usual addition and multiplication rules, as long as we are willing to be creative and allow ""infinite matrices"" to exist. In general, for any $n$ and $m$ , the set of $n \times m$ matrices forms an algebroid. The union of any such sets also forms an algebroid, as long as matrix compositions exist when expected (meaning if we have $5 \times 4$ and $4 \times 3$ matrices, we must also have $5 \times 3$ matrices). This is also true if $n$ and $m$ are arbitrary infinite cardinals, with the caveat that only finitely many elements of each column of the matrix can be nonzero. So, we can ask if this is basically ""what algebroids are,"" in the following sense: Let's say that the category $\text{Mat}^+_K$ is basically an extension of the usual $\text{Mat}_K$ , but with the objects as all possible cardinals rather than only natural numbers. For objects $\kappa, \lambda$ , the morphisms from $\kappa \to \lambda$ are (possibly infinite) matrices of size $\kappa \times \lambda$ (treating these cardinals as initial ordinals), with finitely many nonzero coefficients in each column, taking values in $K$ . Naive Pre-Question : is every possible $K$ -algebroid equivalent to a subcategory of $\text{Mat}_K^+$ ? Now, the answer to this first question is ""no,"" because we can have, for instance, a disjoint union of $\text{Mat}_K^+$ with itself, which is a $K$ -algebroid but not equivalent to $\text{Mat}_K^+$ . This would be like having, for instance, ""red"" and ""blue"" matrices, where the product of red and blue matrices is undefined. So, to salvage the spirit of this question, we can note that $\text{Mat}_K^+$ is a skeleton category of $K$ -Vect, which has infinitely many copies of $\text{Mat}_K^+$ with all possible linear transformations (and thus isomorphisms, when they exist). So, we can get to the better question: Real Question : is every possible $K$ -algebroid equivalent to a subcategory of $\text{Vect}_K$ ?","['enriched-category-theory', 'abstract-algebra', 'category-theory']"
4723839,Does there exist a field where all even degree equations have solutions but not all odd degree equations?,"Does there exist a field $F$ , preferably a subfield of the complex numbers, such that all even degree polynomial equations have solutions in $F$ , but such that for every positive odd integer $k$ greater than or equal to $3$ , there exist $k$ -th degree polynomial equations which do not have solutions in $F$ ? Basically, such a field $F$ , if it exists, would be an ""anti""-real-closed field. In a real closed field, every odd degree polynomial equation has solutions, but for every positive even integer $k$ , there exist $k$ -th degree polynomial equations with no solutions.","['field-theory', 'abstract-algebra', 'polynomials']"
4723943,"Showing that $(x,r) \mapsto A_r f(x)$ is continuous","I am studying Measure Theory from these notes . The author passes off the fact that the function $(x,r)\mapsto A_r f(x)$ is continuous where $f$ is locally integrable function as something easy. Here's the context: Here's my attempt at proving this:
Let $f\in L^{1}_{loc} \left( m_{n} \right)$ . Consider the map $F: \mathbb R^{n} \times (0, \infty) \to \mathbb R$ given by $F(x,r)=A_r f(x)$ for each $(x,r) \in \mathbb R ^{n} \times (0, \infty)$ . We need to show that this map is continuous. Let $(x,r)\in \mathbb R^{n} \times (0, \infty)$ and let $\left( x_{n}, r_{n} \right)$ be a sequence in $\mathbb R ^{n} \times (0,\infty)$ converging to $(x,r)$ . We wish to show that $A_{r_n} f(x_n) \to A_r f(x)$ . We first show that $\int_{B(x_{n}, r_{n})} f(y) dy \to \int_{B(x,r)} f(y) dy$ (Call it goal $(\star)$ ). Since the sequence $\left( x_n, r_{n} \right)$ converges, it must be bounded. Therefore, there exists a constant $K>0$ such that $\lVert(x_n, r_n)\rVert_{2}<K$ for each $n \in \mathbb N$ . Hence, we have that $(x_{n}, r_{n}) \in B[0, K]$ for each $n \in \mathbb N$ . Now, note that $|f\chi_{B(x_{n}, r_{n})}| \le |f\chi_{B[0,K]}|$ for each $n\in \mathbb N$ and $f\chi_{B[0,K]}$ is in $L^{1} \left( m_{n} \right)$ by our assumption that it is locally integrable. We now show that $\chi_{B(x_{n}, r_n)}  \to \chi_{B(x,r)}$ pointwise. Let $y \in B(x,r)$ . Then we have that $\chi_{B(x,r)}(y)=1$ and hence we need to show that $\chi_{B\left( x_{n},r_{n} \right)} =1$ eventually. To see this, consider the sequences $|y-x_{n}|$ and $r_n$ . Since $|y-x_{n}| \to |y-x|$ , $r_n \to r$ and $|y-x|<r$ , we have that $|y-x_n|<r_n$ eventually. This shows that $\chi_{B(x_{n}, r_{n})}(y)=1$ eventually. A similar argument shows that if $y\not\in B(x,r)$ then $y\not\in B(x_{n},r_n)$ eventually. Hence, we have that $f\chi_{B(x_{n},r_{n})} \to f\chi_{B(x,r)}$ pointwise. Since we meet all the requirements to apply the dominated convergence, we do so to conclude $(\star)$ . Since $r_{n} \to r$ , we have that $\limsup_{n\in \mathbb N} B(0,r_{n})=\liminf_{n\in \mathbb N} B(0,r_n) = B(0,r)$ . Hence, we have by continuity of measures that $\lim_{k\in \mathbb N} m_{n} \left( B(0,r_{k}) \right) = m_{n} \left( B(0,r) \right)$ . Also, since Lebesgue measure is translation invariant, we have that $\lim_{k\in \mathbb N} m_{n} \left( B(x_k,r_{k}) \right) = m_{n} \left( B(x,r) \right)$ . Using these two facts, we have shown that $(x,r) \mapsto A_{r} f(x)$ is continuous. Although I think my argument is correct, I feel there must be easier way to prove this else the author would not have skipped the proof. I would appreciate it if people can offer an alternative and shorter proof of this.","['alternative-proof', 'measure-theory', 'lp-spaces', 'real-analysis']"
