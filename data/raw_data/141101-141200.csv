question_id,title,body,tags
2279471,Difference between complement and orthogonal complement (vector spaces),"I have been learning some linear algebra and, specifically, solutions to simultaneous equations using matrices and the different cases that arise according to the relative number of unknowns and linearly independent equations that we have. The term 'kernel' came up when this was being taught but wasn't properly explained. On looking into this myself a bit further, I came to the conclusion that the kernel was the complement of the space spanned by the rows of the matrix, in the n dimensional space (there are n unknowns). I tried to look this up to verify if I am using the terms right (I know with complements there are sometimes difficulties with something being 'open' or 'closed'- I think in topological spaces anyway. But I am not too familiar with this and am not sure if it applies to vector spaces anyway. So I just wanted to check if the kernel really could be described as the 'complement' of the subspace spanned by the rows). On looking into this, I keep seeing the terms 'orthogonal complement' pop up, and was wondering if 'orthogonal complement' is strictly the same thing as 'complement'? I get that the vectors in the row space are orthogonal to those in the kernel, but i'm not sure if the terms are the same. Also, I was wondering how one could generate a set of vectors spanning the kernel of some matrix (I am not asking for someone to give this method here- I see it has been asked several times!). In the simple case of 2 linearly independent equations and three unknowns, you can take the cross product of the two linearly independent vectors that are two of your equations. I thought that perhaps this could be extended, and perhaps you could find all the vectors by taking the cross product of all of your linearly independent vectors/equations in different orders, maybe all of the cyclic permutations would do it. For example, say that you have n-k linearly independent equations which give the row vectors $\textbf{r}_1,\textbf{r}_2,...,\textbf{r}_{n-k}$, then you could find a set of vectors spanning the kernel by $(\textbf{r}_{n-k} \times (\textbf{r}_{n-k-1} \times ... (\textbf{r}_2 \times \textbf{r}_1)...))$ And you would cycle these around. However on looking at other posts here I have not seen this being used (instead, curiously, I have seen integrals being used). So I was wondering: would this work to generate a set of vectors spanning the kernel? Why/why not?","['matrices', 'matrix-equations', 'vector-spaces']"
2279532,Characterize joint distribution from marginals,"Let $Z$ be an arbitrary set. Let $X=\prod_{i=1}^n Z_i$ and $Z_i=Z$ for each $i=1,\ldots,n$, $n$ fixed. Consider the $n$-tuple $(\mu_1,\ldots,\mu_n)$ with $\mu_i\in\Delta_s(Z)$ for each $i=1,\ldots,n$, i.e., each $\mu_i$ is a simple probability measure on $Z$ (it has finite support). Let $J=\{\mu\in \Delta_s(X): \operatorname*{marg}_{Z_i}\mu=\mu_i \;\;\forall $i$ \}$, i.e. the set of all simple probability measures on $X$ that have $(\mu_1,\ldots,\mu_n)$ as marginals. Is there any way to characterize the set $J$? I know that for real valued random variables this problem can be tackled with copulas, but I don't know if copulas can be used in this case. We can assume that there is some complete and transitive order $\succeq$ on $Z$.","['probability-theory', 'decision-theory', 'probability-distributions', 'statistics', 'probability']"
2279533,Computing an integral related to a unique zero of $ze^z-w$,"I proved that the equation $ze^z=w$ for $w\in D(0,1/e),z\in D(0,1)$ has a unique solution using Rouché's Theorem noted $h(w)$. Now the goal is to prove that $h(w)=\sum_{n=1}^{\infty}\frac{n^{n-1}}{n!}w^n.$ I succeed to prove that if $a_n:=\int_{\partial D(0,1)}(1-z)z^{-n}e^{nz}dz=\frac{n^{n-1}}{n!}$ using Cauchy integral formula, and if $$\int_{\partial D(0,1)}\frac{z(z-1)e^{-z}}{ze^{-z}-w}dz=h(w)$$ then it follow by interchanging ""integrals and series"". Now I would like to prove this, I was trying to using Kronecker's formula but I have $\frac{f'(z)}{f(z)}\times z:=g(z).$ So I tried Residue theorem to the meromorphic function $g(z).$ As there is a unique pole $h(w)$ it follow directly by the theorem if $$Res(g,h(w))=h(w).$$ I don't see how it's true ?",['complex-analysis']
2279549,Algebra: Prove inequality $\sum_{n=1}^{2015} \frac1{n^3} < \frac 54$,"Can someone prove inequality (n is natural):$$\sum_{n=1}^{2015} \frac{1}{n^3} < \frac 5 4$$
I have tried some predictions like $a^3 > a(a - 1)(a - 2) $ but couldn't get anything out of them.","['algebra-precalculus', 'inequality', 'arithmetic']"
2279567,"Is $f(x)=x^2\cos\left(\frac{1}{x}\right)$ of bounded variation on $[-1,1]$?","Is $f(x)=x^2\cos\left(\frac{1}{x}\right)$ with $f(0)=0$ of bounded variation on $[-1,1]$? I know that $g(x)=x\cos\left(\frac{1}{x}\right)$ and $h(x)=x^2\cos\left(\frac{1}{x^2}\right)$ are both NOT of bounded variation, so I'm guessing that $f$ isn't either...? I just don't know which partition to take, since the proofs of the ones for $g$ and $h$ involved expressing the Variation into a harmonic series, which diverges when the number of points in the partition increases, but I don't know how to do the same for $f$. Thanks!","['real-analysis', 'analysis']"
2279568,Formula to create a Reuleaux polygon,"The Wikipedia articles for Reuleaux triangle and curve of constant width do a good job of describing the properties of a Reuleaux polygon, but they don't give a straightforward formula for computing or drawing such a figure, except in terms of the manual compass-and-straightedge construction. Is there a formula or algorithm that, given the number of sides and the width/diameter, would give some data representation of a Reuleaux polygon that could be used to recreate it programmatically? In particular, I'm looking for the coordinates of the vertices (or the angle/direction from one vertex to another) and the details of the arc connecting them.","['polygons', 'algorithms', 'plane-curves', 'geometry']"
2279602,Closed form for Binomial Power Series,"The complex power series $$\sum_{n=0}^\infty \dbinom{n+m}{m} z^n$$ has radius of convergence by Ratio Test also it converges to $\frac{1}{(1-z)^{m+1}}$ in $D(0,1)$. I proved this by noting $\frac{1}{1-z} = \sum z^n$ and that power series have infinite derivatives in their raidus of convergence, with their derivatives equal to the term-wise derivatives sum. I was wondering if there is a proof from first principles without using infintie sum differentiation.","['complex-analysis', 'power-series']"
2279684,Maximal open set where $(e^{nz}/n)_{n}$ converges uniformly,"I would like to find the maximal open set such that the sequence $\left(\frac{e^{nz}}{n}\right)_{n\in\Bbb{N^*}}$ converge uniformly for $z\in\Bbb{C}$. We have $\exp(nz)=\sum_{k=0}^{\infty}\frac{n^kz^k}{k!},$ so that $\frac{e^{nz}}{n}=\sum_{k=0}^\infty\frac{n^{k-1}z^k}{k!}.$ If I denote $a_k=\frac{n^{k-1}}{k!}$ we have $a_k\ne 0$ for all $k\in\Bbb{N.}$ We have $$\frac{a_{k+1}}{a_k}=\frac{n^{k}k!}{n^{k-1}(k+1)!}=\frac{n}{k+1}\to0 \quad\mbox{as}\quad k\to\infty.$$ Then the radius of convergence of  $\sum_{k=0}^\infty\frac{n^{k-1}z^k}{k!}$ is $\infty,$ so that it converges everywhere. So if I am not mistaken the sequence converge uniformly on $\Bbb{C}.$ Am-I correct ?","['complex-analysis', 'sequences-and-series', 'uniform-convergence']"
2279731,Help in understanding what is going on in $A[G]$,"Ok, so Im given a group $G$ and a ring $A$, and define: $$A[G]=\left\{\sum_{g \in G} f(g)  g : f:G \to A, \text{ such that $f$ has finite support}  \right\}$$ Define the sum $(+)$: $$\sum_{g \in G} f(g)  g + \sum_{g \in G} h(g)  g = \sum_{g \in G} (f(g)+h(g))  g$$ And multiplication $(\cdot)$: $$\sum_{\alpha \in G} f(\alpha)  \alpha \cdot \sum_{\beta \in G} h(\beta)  \beta = \sum_{g\in G}\left(\sum_{\alpha \beta = g} f(\alpha)h(\beta)\right)  g$$ And then $(A, +, \cdot)$ is a ring. Now, Im trying to calculate $Z(A[G])$, and Ive got a suggestion that says to first show that if $\sum_{g \in G} f(g)  g \in Z(A[G])$ then $f(g) \in Z(A)$ for every $g\in G$. So I take  $\sum_{g \in G} f(g)  g \in Z(A[G])$ and then for any  $\sum_{g \in G} h(g)  g \in A[G]$, we have that $$\sum_{\alpha \in G} f(\alpha)  \alpha \cdot \sum_{\beta \in G} h(\beta)  \beta =\sum_{\beta \in G} h(\beta)  \beta \cdot \sum_{\alpha \in G} f(\alpha)  \alpha$$ So I thought, ok, given $x\in A$ lets take $h:G \to A$ given by $h(1_{G})=x$ and $h(g)=0$ for every $g \in G-\{1\}$, in order to show that $f(g) \in Z(A)$ for every $g \in G$. So here comes my misunderstanding: For this given $h$, $$\sum_{\alpha \in G} f(\alpha)  \alpha \cdot \sum_{\beta \in G} h(\beta)  \beta = \sum_{\alpha \in G} f(\alpha)  \alpha \cdot (x \cdot 1_{G} + \sum_{g \in G-\{1\}} 0 \cdot g)$$ But $0 \in A$ and $g \in G$ and (I've have not studied Modules until now) and therefore, In principle I don't know how to multiply $0 \cdot g$. In fact I don't know how to multiply any $a \in A$ times any $g\in G$. So what's the issue? I don't need that? Where am I lost?","['abstract-algebra', 'ring-theory', 'group-theory', 'group-rings']"
2279772,$2017$ people move from $123$ rooms. Prove that they end up in the same room.,"Suppose $2017$ people reside in the rooms of a $123$ room mansion. Each minute, so long as not all the people are in the same room, somebody walks from one room into a different room with at least as many people in it. Prove that eventually all people end up in the same room. I really have no idea what to do. Plus I need help with tagging it.","['number-theory', 'pigeonhole-principle']"
2279801,Number of phone locking patterns,"LG android cell phones have locking screens with $9$ points to be traced in any pre-specified fashion (drawing pattern) so as to join $\geq 4$ points without including any points more than once. Quoting from a Quora post ... You cannot go over an unlit dot without lighting it. For example, the
pattern [0 2 1 4] is illegal, because moving your finger between 0 and
2 will light 1. Once a dot is lit, you can use it to reach another
unlit dot. For example, both [0 4 3 5] and [0 4 5 3] are legal. The numbers are assigned as: $$\begin{matrix}0&1&2\\3&4&5\\6&7&8\end{matrix}$$ Right off the bat, the maximum options available depend on the position within the grid: The question is two-fold: How many patterns are possible, given that not all points present the same number of choices for the next move, as well as a minimum ( $4$ ) and maximum ( $9$ ) number of points included in the pattern? And is there a way of obtaining a closed-form calculation as opposed to a computer simulation as in this Quora post ? Does the ""pattern"" inherent to the drawing on a rigid structure makes some patterns more likely than others (e.g. a letter ""C"")?","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2279825,Derivative of a bounded Integral?,"Just revising for my advanced calculus exam and came across this question: Consider the function $f(x)$ defined by the integral equation:
  $$
f(x) = x^2 + \int_0^x(x − t)f(t) dt.
$$
  Derive an $ODE$ and boundary conditions for $f(x)$, and solve this
  to determine $f(x)$. I would assume you take the derivative of both sides to get: $$
f'(x) = 2x + {d \over dx}{\int_0^x(x − t)f(t) dt}.
$$ But where do I go from here? My first guess is: $$
f'(x) = 2x + (x-x)f(x) - x.f(0).
$$
$$
f'(x) = 2x  - x.f(0).
$$ Surely that's wrong? What am I missing?","['integration', 'calculus']"
2279827,Proof of multiplicative commutativity for all real numbers,"I have seen proofs for commutativity for all integers, and these can be extended to rationals easily because a rational number is just the ratio of two integers. However, I have yet to see a proof that multiplication of real numbers is commutative. How would you prove this one?",['analysis']
2279847,Discrete Irwin-Hall distribution? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question What is the Irwin-Hall distribution for discrete random variables? That is, if I roll $n$ $k$ -sided die and add their sum together, how will the result be distributed? John D. Cook says: By using generating functions, you can see that the probability of getting a sum of $k$ spots on the five [six-sided] dice is the coefficient of $x^k$ in the expansion of $(x + x^2 + x^3 + x^4 + x^5 + x^6)^5 / 6^5$ But this is all the explanation he gives, and I don't follow. Additionally, I'm wondering if there is a name for this distribution.","['probability', 'probability-distributions']"
2279851,Applied Probability- Bayes theorem,"I need help in all things related to identifying, defining conditions and solution feed back and reasoning most importantly. 1) A blood test indicates the presence of a particular disease 95 % of the time when the diseases is actually present. The same test indicates the presence of the disease .5% of the time when the disease is not present. One percent of the population actually has the disease. Calculate the probability that a person has a disease given that the test indicates the presence of the disease. First and foremost one might think that because the word ""given"" is followed by the indication of the disease they might think that, the indication of the disease is the is the condition However my understanding is that a condition related to a mathematical definition is the state of affairs that must occur or exist before something else can happen. Well clearly one must first either have the disease or not have the disease before a test can indicate whether or not the disease is in fact present. Solution: $C=$ The individual has the disease $.01%$ $E=$ The test indicates that it is present in those with the disease $.95%$ SO we want the proportion of those that have the disease and test positive to the proportion of those that have the disease: $P(E \vert C) = \frac{P(E \cap C)}{P(E \cap C)+P(E^c \cap C)}$ $= \frac{(.01)(.95)}{(.01)(.95)+(.05)(.01)}= \frac{.0095}{.01}=.950$ My answer choices were $.324,.657,.945,.950,.995$ This one made the most sense to me? The other method I used by using the condition as the test didnt even give me anything close to 1%. Was I supposed to do it the other way and was I somehow supposed to take into account that of those that don't have the disease 50% test positive but that wouldnt make sense to me. 2) Ninety-eight percent of all babies survive delivery, However 15 percent of all births involve a c section and when they are done the baby survives 96 percent of the time. If a randomly chosen pregnant woman does not have a c section what is the probability that her baby survives? By the same logic as the previous problem the condition would only make sense to be the method of delivery because if we are talking about a baby surviving delivery then its survival is conditioned on the method of delivery. Correct? $C=$ Does not have a C-section $.85%$ $E=$ her baby survives without a c-section this is difficult to figure out: if 96% of all 15% c section births survive and since we have the percentage of the total 98 percent that survive from both non c and c section then thus 1 minus this should give us the percentage that survive when they do not have a C-section, this makes sense correct? then the probability of a baby surviving a c-section is $.98-.144=.836$ Right? Okay now we set up the question: Using this we want: $P(E \vert C)= \frac{P(EC)}{P(E \cap C)+ P(E^c \cap C)}= \frac{P(E \cap C)}{P(C)}=\frac{(.836)(.85)}{(.836)(.85)+ ?}$ ?= we want the percentage that didn't survive when they had a c-section and didn't have a c-section $(.85)(1-.836?)$ recopying and pasting we get: $P(E \vert C)= \frac{P(EC)}{P(E \cap C)+ P(E^c \cap C)}= \frac{P(E \cap C)}{P(C)}=\frac{(.836)(.85)}{(.836)(.85)+(1-.836)}=\frac{.7106}{(.7106+.164)}$ Some how this was wrong as the supposed answer is slightly above .98. I want to rant a bunch of improper things and swear at the top of my lungs who the hell does my logic not follow I used the exact definition of condition and and properly applied bayes theoreom please label every step and the reason behind each intersection obtained.",['probability']
2279853,A non-empty closed convex subset of a Banach space contains an open ball,"In a Banach space $X$, I construct a non-empty closed and convex set $A$. The set $A$ satisfies two conditions: If $x\in A$, then $-x \in A$. $\displaystyle{X=\bigcup_{n\geq1} nA}$ Then to my intuition, I feel that $A$ contains an open ball centered at $0$. For each $x\in X$, we always can have $x/n\in A$ for some $n$. Then intuitively I can have many points clustered around $0$. What is the right way to prove this?","['functional-analysis', 'banach-spaces']"
2279872,Find a smooth function $f \in C^\infty$ such that $A=f^{-1}(0)$ and $B=f^{-1}(1)$,"The problem is: Let $A$ and $B$ disjoint closed subsets of a smooth manifold $M$. Show that there exist a function $f \in C^\infty(M)$ such that $A=f^{-1}(0)$ and $B=f^{-1}(1)$. I thought this problem could be solved by using the Smooth Urysohn's Lemma, but can't see how to guarantee that $0 < f(x) < 1$ for all $x\in M\backslash (A\cup B)$. Tips?","['smooth-manifolds', 'differential-geometry', 'differential-topology']"
2279880,Tractrix & limits,"I'm in calculus 1 and I'm having quite a bit of trouble with this problem. Any advice as to how to obtain a solution or anything would be much appreciated. Thanks! To determine the points on the curve, we need to calculate the intersection point of adjacent threads (the red lines), then determine the limit of that intersection point as the end points of the threads approach zero. Tractrix 1 This is the situation shown in the example above: a given thread runs from the point with coördinates $(r,0)$ to the point with coördinates $(0,1-r)$, $0 ≤ r ≤ 1$.  In other words, the sum of the $x$ coördinate on the $x$-axis at the lower end of the thread and the $y$-coördinate on the $y$-axis at the upper end of the thread is one. To determine a point on the tangent curve, consider two adjacent threads: one runs from $(r,0)$ to $(0,1-r)$; the other runs from $(r+h,0)$ to $(0,1-(r+h))$.  These lines intersect at a point $(x_{r+h},y_{r+h})$. The point on the curve is $(x_r,y_r)=\lim\limits_{h\to 0}  (x_{r+h},y_{r+h})$. Write the equation for the line through the points $(r,0)$ and $(0,1-r)$. Write the equation for the line through the points $(r+h,0)$ and $(0,1-(r+h))$. Calculate the point $(x_{r+h},y_{r+h})$ where these two lines intersect. Calculate $\lim\limits_{h\to 0}  (x_{r+h},y_{r+h})$. Use the point just determined to write the equation for the curve that passes through all such points.  (Note: the equation should be symmetric in $x$ and $y$.)","['plane-curves', 'limits', 'calculus', 'conic-sections', 'envelope']"
2279904,Limit distribution of Markov Chain,"I have to determine the limiting distribution for the MC
so I determine the equations first: $\Pi_0 +\Pi_1 + \Pi_2 + \Pi_3 +\Pi_4 =1$ $\Pi_0 = \Pi_0q + \Pi_1q + \Pi_2q +\Pi_3q +\Pi_4$ $\Pi_1 = \Pi_0p$ $\Pi_2 = \Pi_1p$ $\Pi_3 = \Pi_2p$ $\Pi_4 = \Pi_3p$ and then I tried to solve the system and I got this: $0= \Pi_0 (-1+q+pq+p^2q+p^3q+p^4) $ So I conclude $0= \Pi_0$ therefore $\Pi_1 = \Pi_2 = \Pi_3 =\Pi_4 =0$ but this doesn't match the answer of the book. Can somebody tell me where is my mistake?","['stochastic-processes', 'transition-matrix', 'markov-chains', 'statistics', 'markov-process']"
2279929,Are there an infinite number of primes of the form $\lfloor \pi n \rfloor$?,"Are there an infinite number of primes 
of the form
$\lfloor \pi n \rfloor$? This is sort of
a clickbait title.
I would really like to show that,
for any real irrational
$r > 1$,
there are an infinite number of primes
of the form
$\lfloor r n \rfloor$
for positive integer $n$. I can show that
there are an
infinite number of primes
of the form
$\lfloor r n \rfloor$
for
$r < 1+1/g$
for a fixed $g > 1$,
but the best known value
of $g$
is $246$ unconditionally
and
$6$ assuming
an unproved conjecture. To prove this,
I use the idea
of Beatty sequences
( https://en.wikipedia.org/wiki/Beatty_sequence ).
For a real $r > 1$,
let
$B(r)
=\{\lfloor nr \rfloor
\mid n \in \mathbb{N}^+\}$.
($\mathbb{N}^+$
is the set of positive integers.)
Then
Beatty's theorem states that
$B(r)$ and
$B(r/(r-1))$
make up a
disjoint partition
of $\mathbb{N}^+$. If there are
only a finite number of primes
in $B(r)$,
then all the primes
above a certain value
are in
$B(r/(r-1))$. We have
$\lfloor (n+1)r \rfloor
=\lfloor nr+r \rfloor
\ge \lfloor nr \rfloor +\lfloor r \rfloor
$.
Therefore,
if $r > 3$,
$B(r)$
can not contain
any twin primes.
Therefore,
if there are an
infinite number of twin primes,
$B(r/(r-1))$
must contain
an infinite number
of primes
for all 
$r > 3$,
or
$r/(r-1)
\lt 3/2
$. The use of
an unproved conjecture
can be removed
at the cost of
a weaker conclusion
by using the recent results
on prime gaps
( https://en.wikipedia.org/wiki/Prime_gap ).
It has been shown that
there is a constant $g$
such that
there are an
infinite number of
consecutive primes
that differ by
at most $g$. It has been shown that
$g \le 246$
unconditionally
and that,
assuming the
Elliott–Halberstam conjecture
( https://en.wikipedia.org/wiki/Elliott%E2%80%93Halberstam_conjecture ),
$g \le 12$
($g \le 6$
assuming a generalized form
of the conjecture.). Arguing as before,
if $r > g+1$,
then
$B(r)$
can not contain
all the primes
above any finite value.
For,
if it did,
there are an
infinite numer of
consecutive primes
$p$ and $q$
such that
$q-p \le g$,
and $p$ and $q$
can not both be in
$B(r)$. Therefore
$B(r/(r-1))$
must contain an
infinite number of primes
for $r > g+1$.
Restating,
$B(r)$
must contain an
infinite number of primes
for $r < (g+1)/g
=1+1/g$. I don't know how to go
beyond this.","['prime-numbers', 'sequences-and-series']"
2279937,Prove that the sum is less than $6/5$.,"How to prove that $$\sum_{n=1}^{15} \frac{1}{n^3}\lt\frac 65$$ I tried to compare this sum to the infinite sum, but Apery 's constant is just above $1.2$ so this approach doesn't work. Then I typed this into wolfy and the sum seems as if it is just under $6/5$. However, I was wondering if there was a better way of proving this result rather than summing all the terms.","['algebra-precalculus', 'summation', 'sequences-and-series']"
2279964,Asymptotics of a double integral,"I want to  calculate the asymptotic form as $x\to 0$ of the following integral. \begin{alignat}{2}
I_2(x) = \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^{\frac{3}{2}}}\exp\left(-\frac{x}{u+v}\right)  
\end{alignat} How can we solve? This question is related with this post ( Asymptotics of a double integral: $ \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^2}\exp\left(-\frac{x}{u+v}\right)$ ), 
or the solutions in three-dimensional space for this post ( https://physics.stackexchange.com/questions/61498/a-four-dimensional-integral-in-peskin-schroeder# ) Thank you so much","['limits', 'multiple-integral', 'asymptotics', 'integration', 'definite-integrals']"
2280020,"For $x \ge 1$, is $2^{\sqrt{x}} \ge x$","For $x \ge 1$, is $2^{\sqrt{x}} \ge x$ The answer appears to me to be yes since: $$\sqrt{x} \ge \log_2 x$$ Here's my reasoning: (1) For $x > 16$, $5\sqrt{x} > 4\sqrt{x} + 4$ (2) For $x > 25$, $x > 5\sqrt{x}$ (3) So, $x > 25$, $2x > x + 4\sqrt{x} + 4$ and it follows that $\sqrt{2x} > \sqrt{x} +2$ (4) So, as $x$ doubles, $\sqrt{2x} > \sqrt{x} + 2$ but $\log_2(2x) = \log_2(x) +1$ Is my reasoning correct?  Is there a simpler way to analyze this?","['logarithms', 'inequality', 'exponential-function', 'functions']"
2280028,Finding rank-nullity of a given matices,"Let $A$ be a $4 \times 7$ real matrix and $B$ be a $7 \times 4$ real matrix such that $AB=I_4$. Which of the following are true? 1) rank$(A)=4$ 2) rank$(B)=7$ 3) nullity$(B)=0$ 4) $BA=I_7$. My attempt: $4=\operatorname{rank}(AB) \leq \min\{{\operatorname{rank}(A),\operatorname{rank}(B)}\}$. So $\operatorname{rank}(A)$ must be $4$. It shows that 1) is true 2) is false by Dimension theorem How to check 3) and 4) ?","['matrices', 'matrix-rank', 'linear-algebra']"
2280029,Riemann Zeta Function's Analytic Continuation,"I understand that it is claimed that the zeta function is only valid for $\operatorname{Re}(s)>1,$ which is why negative arguments require analytic continuation. But why is the function as it is written not defined for $\operatorname{Re}(s)<1?$ Is this because the series diverges when $\operatorname{Re}(s)<1?$ And if so, why may we claim that the function is undefined for those values just because it is divergent for those values? It will really take some convincing for me to believe that we can change a function just because it is divergent at some values. Background: I am doing some independent learning on the Riemann Zeta function just out of curiosity.  I have searched the site, and I know that there are plenty of questions on the Riemann Zeta function.  However, this particular one has not been answered for me nor have I been able to find it easily on google. I teach high school math and adjunct at a local community college in lower level classes such as Trig and College Algebra.  I say that so that you would keep in mind the level of mathematics that I have been exposed to and that you will word your answers accordingly (at an elementary level with little sophistication).","['number-theory', 'zeta-functions', 'riemann-zeta']"
2280043,Problem underestanding holomorphic quadratic diferentials,"I am trying to understand what a holomorphic quadratic differential is, i have read a local definition on two books: Jürgen-Jost-""Compact Riemman Surfaces"" and Kurt Strebel-""Quadratic-Differentials"". The definition that they use is  local: Definition: Let ( M , g) be a Riemann surface with a conformal metric and $z$ a local conformal coordinate, we say that $\varphi dz^2$ is a holomorphic quadratic differential if $\varphi$ is holomorphic. That is the definition in Jürgen-Jost book, in Kurt Strebel ""Quadratic-Differentials"" They only add that a transformation rule for other coordinates is needed. I would like to understand a global definition in terms of sections, in wikipedia i found this: ""a quadratic differential on a Riemann surface is a section of the symmetric square of the holomorphic cotangent bundle"" I guess holomorphic cotangent bundle means the bundle of holomorphic 1-forms. I don't understand the ""symmetric square part"". I would like to have a global definition and a local coordinate representation. Of course I don't trust at all about a wikipedia link. So I would like to ask, is this definition  right? Is there a good reference  i should  read ?","['riemann-surfaces', 'differential-geometry']"
2280047,How to prove PCA using induction?,"In Deep Learning (Goodfellow, et al) , the optimization objective of PCA is formulated as $$D^* = \arg\min_D ||X - XDD^T||_F^2 \quad \text{s.t.} \quad D^T D=I$$ The book gives the proof of the $1$ -dimension case, i.e. $$\arg\min_{d} || X - X dd^T||_F^2 \quad \text{s.t.} \quad d^T d = 1$$ equals the eigenvector of $X^TX$ with the largest eigenvalue. And the author says the general case (when $D$ is an $m \times l$ matrix, where $l>1$ ) can be easily proved by induction. Could anyone please show me how I can prove that using induction? I know that when $D^T D = I$ : $$D^* = \arg\min_D ||X - XDD^T||_F^2 = \arg\min_D tr D^T X^T X D$$ and $$tr D^T X^T X D = \left(\sum_{i=1}^{l-1} \left(d^{(i)}\right)^T X^TX d^{(i)}\right) + \left(d^{(l)}\right)^T X^TX d^{(l)}$$ where the left-hand side of the addition reaches maximum when $d^{(i)}$ is the $ith$ largest eigenvector of $X^T X$ according to induction hypothesis. But how can I be sure that the result of the addition in a whole is also maximal?","['matrices', 'linear-algebra', 'principal-component-analysis', 'optimization']"
2280067,Outer automorphism of SLn,"What is the outer automorphism of $SL_{2n}$ and how do you get the Symplectic group as a fixed locus of this automorphism ? I know that the automorphism reverses the Dynkin diagram which is a line in this case. I heard that it is $X \mapsto (X^T)^{-1}$ but the fixed points in this case is the orthogonal group. Then how do you get the Symplectic group out of this ? If at all you get the Symplectic group, then what could be the Symplectic form associated to this automorphism ?","['algebraic-geometry', 'representation-theory', 'algebraic-groups', 'lie-algebras', 'lie-groups']"
2280085,Conditional distribution of $(X_i)_i$ given $\sum\limits_i X_i$ when $(X_i)_i$ is i.i.d.,"Suppose $X_1,X_2,\ldots, X_n$ are i.i.d. random variables. Is there some way to determine the distribution of $(X_1,\ldots,X_n)$ given $S_n := X_1+\ldots+X_n$? In the discrete case it is easy just using the definition of conditional expected value, but what about in the continuous case?","['probability-theory', 'conditional-expectation', 'probability-distributions']"
2280149,Differential Equation Integrating Factor Problem,"I'm doing some practice to solve any exact(not) differential equations and i found this one.
I'm having difficulty to determine the integrating factor which is really troublesome. I can't figure out the integrating at all, since it's not depends on one of the form $x$, $y$, $xy$, $x^2+y^2$. $$ \left(2x + \ln y\right) dx + \left( xy \right)dy = 0$$
My attempt to determine the integrating factor: $M_y=\frac{1}{y}\neq N_x =y$, $\frac{M_y-N_x}{N}={\frac{1-y^2}{xy^2}}\\$ $\frac{N_x-M_y}{M}=\frac{y^2-1}{y}.\frac{1}{2x+\ln y}$ $\mu(x,y)=\frac{M_y-N_x}{yN-xM}=\frac{\frac{1}{y}-y}{xy^2-x(2x+\ln y)}$ $\mu(x^2+y^2)=\frac{M_y-N_x}{2(xN-yM)}=\frac{\frac{1}{y}-y}{2(x^2y-y(2x+\ln y))}$ I have used both wolframalpha and maple, but i found no helps. Anybody could help me out of here?
Thanks in advance.",['ordinary-differential-equations']
2280167,Flow inside and outside Cylinder,"Incompressible fluid with constant density ρ fills the three-dimensional domain below the free surface z = η(r) in cylindrical polar coordinates. The flow is axisymmetric and steady, and the only non-zero velocity component is $u_θ$. Gravity acts upon the fluid. Suppose the fluid in r < a rotates rigidly about the z-axis with angular velocity Ω, and the fluid in r 􏰌 a is irrotational. Show that the velocity in $r \ge$􏰌 a is given by
$$u_θ = \frac{{Ωa^2}}{r}$$ I got stuck about how to show rigidly. My idea: For incompressible: $$\nabla\bullet u= \frac{1}{r}\frac{∂u_θ}{∂_θ}=0$$
For irrotational: $$\frac{∂u_θ}{∂_r}=0$$
For as $r \rightarrow$ a, $u_0$ approaches the tangential velocity Ωr. the second part of the problem is suppose the free surface position satisfies  η$\rightarrow$ 0 as r $\rightarrow \infty$. Show that the free surface position in $r\ge a$ is $$η= -\frac{Ω^2a^4}{2gr^2}$$ My idea is to use the Bernoulli's theorem which is expressed in the boundary condition of the free surface: $\frac{∂\phi }{∂t}+ \frac{1}{2}|\nabla\phi|^2+gη=0$  at y =η $\phi$ is the velocity potential, then it is $\frac{Ωa^2z}{r}$ $\frac{Ωa^2z}{r}$=-gη we can get η. But how to get z from the condition?","['cylindrical-coordinates', 'ordinary-differential-equations', 'fluid-dynamics']"
2280178,"If a function is Riemann integrable and Lebesgue integrable, the two integrals are the same?","We know that if a proper Riemann integrable function is Lebesgue integrable, the two are equal. An improper Riemann integrable function is not necessarily Lebesgue integrable, like $\frac{\sin(x)}{x}$. The question is that if a function $f$ (proper or improper)is Riemann Integrable and Lebesgue integrable, are the two integrals the same?","['riemann-integration', 'calculus', 'functions', 'improper-integrals', 'lebesgue-integral']"
2280200,Gluing integral curves of two smooth vector fields,"Let $X$ and $Y$ be two smooth vector fields defined in a smooth $n$-dimensional manifold $M$, with $X$ and $Y$ never vanishing simultaneously. Say $\gamma:[0,\varepsilon_1]\to M$ is an integral curve of $X$ and $\rho:[0,\varepsilon_2]\to M$ is an integral curve of $Y$, with $\gamma(\varepsilon_1)=\rho(0)$. Can I construct two smooth functions $\alpha,\beta \in C^{\infty}(M)$ such that the vector field $\alpha X+\beta Y$ has an integral curve that joins $\gamma(0)$ and $\rho(\varepsilon_2)$? The problem, of course, is with the point of intersection $\gamma(\varepsilon_1)=\rho(0)$. I tried putting a coordinate patch around that point and using the tubular flow theorem to glue the two curves, but couldn't make it work. Also, is it possible to do it without both functions ($\alpha$ and $\beta$) vanishing simultaneously? I appreciate any suggestions!","['smooth-manifolds', 'differential-geometry', 'differential-topology']"
2280212,Find all solutions to $|x^2-2|x||=1$,"Firstly, we have that $$\left\{
  \begin{array}{rcr}
    |x| & = & x, \ \text{if} \ x\geq 0 \\
    |x| & = & -x, \ \text{if} \ x<0 \\
  \end{array}
\right.$$ So, this means that 
$$\left\{
\begin{array}{rcr}
    |x^2-2x| & = & 1, \ \text{if} \ x\geq 0 \\
    |x^2+2x| & = & 1, \ \text{if} \ x<0 \\
  \end{array}
\right.$$ For the first equation, we have $$|x^2-2x|\Rightarrow\left\{\begin{array}{rcr}
    x^2-2x & = & 1, \ \text{if} \ x^2\geq 2x \\
    x^2-2x & = & -1, \ \text{if} \ x^2<2x \\
  \end{array}
\right.$$ and for the second equation, we have $$|x^2+2x|\Rightarrow\left\{\begin{array}{rcr}
    x^2+2x & = & 1, \ \text{if} \ x^2+2x\geq 0 \\
    x^2+2x & = & -1, \ \text{if} \ x^2+2x<0 \\
  \end{array}
\right.$$ Solving for all of these equations, we get $$\left\{\begin{array}{rcr}
    x^2-2x & = 1  \Rightarrow& x_1=1+\sqrt{2} \ \ \text{and} \ \ x_2=1-\sqrt{2}\\
    x^2-2x & =-1  \Rightarrow& x_3=1 \ \ \text{and} \ \ x_4=1\\
    x^2+2x & = 1  \Rightarrow& x_5=-1-\sqrt{2} \ \ \text{and} \ \ x_6=-1+\sqrt{2}\\
    x^2+2x & =-1 \Rightarrow& x_7=-1 \ \ \text{and} \ \ x_8=-1 
  \end{array}
\right.$$ So we have the roots $$\begin{array}{lcl}
x_1 = & 1+\sqrt{2} \\
x_2 = & 1-\sqrt{2} \\
x_3 = & -1+\sqrt{2} \\
x_4 = & -1-\sqrt{2} \\
x_5 = & 1 \\
x_6 = & -1
\end{array}$$ But according to the book, the answer is \begin{array}{lcl}
x_1 & = & 1+\sqrt{2} \\
x_4 & = & -1-\sqrt{2} \\
x_5 & = & 1 \\
x_6 & = & -1
\end{array} What happened to $x_2$ and $x_3$? Any other way to solve this equation quicker?",['algebra-precalculus']
2280243,Tribonacci Sequence Term,"A tribonacci sequence is a sequence of numbers such that each term from the fourth onward is the sum of the previous three terms. The first three terms in a tribonacci sequence are called its seeds For example, if the three seeds of a tribonacci sequence are $1,2$,and $3$, it's 4th terms is $6$ ($1+2+3$),then $11(2+3+6)$. Find the smallest 5 digit term in a tribonacci sequence if the seeds are $6,19,22$ I'm having trouble with this. I don't know where to start. The formula for the tribonacci sequence in relation to its seeds is $$u_{n+3} = u_{n} + u_{n+1} + u_{n+2}$$
This tribonacci formula holds for all integer $n$. But that's all I know how to work out. And just if it helps, the next few numbers in the sequence mentioned in the question are $47,88,157,292$. Is there some shortcut to it, because I need to show some working out and having two pages full of addition doesn't sound very easy to mark, does it?",['sequences-and-series']
2280251,How to show a possion binomial random variable dominates another possion binomial random variable with a smaller probability value?,"The definition of Poisson binomial distribution is shown as https://en.wikipedia.org/wiki/Poisson_binomial_distribution , where $n$ independent trails with success probabilities $p_1,p_2,\ldots,p_n$. (The binomial distribution is a special case of the Poisson binomial distribution that $p_1=p_2=\cdots=p_n$.) Let $X$ be a random variable following a Poisson binomial distribution, where $n$ independent trails with success probabilities $p_1,p_2,\ldots,p_i,\ldots,p_n$. Suppose that we arbitrarily reduce a probability $p_i$ to $p'_i$ ($p_i > p'_i$), and have another random variable $Y$, which follows a Poisson binomial distribution with success probabilities $p_1,p_2,\ldots,p'_i,\ldots,p_n$. Compared to the Poisson binomial distribution followed by $X$, the Poisson binomial distribution followed by $Y$ only has the same $p_1,\ldots,p_n$ except a lower $p'_i$. My goal is to show that $P(X\geq k) \geq P(Y \geq k)$, for any fixed $k \in \{1,\ldots,n\}$. In other words, I want to prove that $P(X\geq k)$ is a monotonic increasing function of $p_i$ for all $i = 1$ to $n$. The result looks simple, but I am struggling to prove that. Note that $P(X\geq k)  = \sum_{l=k}^n \sum_{A\in F_l} \prod_{i\in A} p_i \prod_{j\in A^c} (1-p_j) $ from Wikipedia. It is very hard to use an algebraic proof to show that $P(X\geq k) \geq P(Y \geq k)$. Can somebody give me a help?","['binomial-distribution', 'probability', 'probability-distributions']"
2280258,"What does Pr(dx, dy) mean?","The book The Elements of Statistical Learning by Hastie and others (page 18) defines the expected value of prediction error as \begin{align}
 \operatorname{EPE}(f) &= \operatorname E(Y - f(X))^2\\
 & = \int [y - f(x)]^2 \Pr(dx, dy)
\end{align} Why is it like above?
Why not as below to be consistent with any expected value definition? $$ \operatorname{EPE}(f) = E(Y - f(x))^2 = \iint  [y - f(x)]^2  \Pr(x,y) d(x) d(y)$$ What does $\Pr(dx, dy)$ even mean?","['machine-learning', 'statistics', 'integration', 'probability', 'measure-theory']"
2280289,A Jacobi field along a geodesic segment is the variation field of a family of geodesics,"I am reading an introductory textbook in Riemannian geometry (Lee), and met this problem: Any Jacobi field $V$ along a geodesic segment $\gamma$ is the variation field of some variation of $\gamma$ through geodesics. A hint is given: try to write the family $\Gamma (s,t)$ as $\exp_{\sigma(s)}tW(s)$ for some proper curve $\sigma(s)$ and vector field $W(s)$ along $\sigma$. This is reasonable since for each $s$, $\Gamma (s,t)$ gives a geodesic. The problem is of course how to find such $\sigma$ and $W$. Clearly we should require $\dot \sigma (0) = V(0)$, and $W(0)$ be determined by the original geodesic $\gamma$. But what's next? I plan to find $\partial / \partial t \ \Gamma $ and $\partial / \partial s\  \Gamma $, and see whether I can get something from the Jacobi equation. But I do not know how to differentiate  $\exp$ with respect to $\sigma(s)$! Anyone has more hint(s)? Thank a lot!","['riemannian-geometry', 'ordinary-differential-equations', 'differential-geometry']"
2280319,How does the gradient/jacobian relate to the first derivative of one dimensional functions?,"My question is about this answer by Simon S ( Is the gradient vector of a function the derivative of the function ) The derivative of a function $f : U \subset \mathbb{R}^m \to
\mathbb{R}$ at a point $p \in U$ is a linear map $Df_p : \mathbb{R}^m
\to \mathbb{R}$. We can identify the gradient of $f$ with $D_p$, if
  everything exists, by $$Df_p(v) = \langle \nabla f(p), v\rangle$$ Equivalently, $Df_p$ is a co-tangent vector, i.e., a member of the set
  of linear maps acting on $T_p$, the vector space of tangent vectors at
  $p$: i.e., $Df_p = \langle \nabla f(p), \cdot \rangle : T_p \to
\mathbb{R}$. This formalism is helpful when we want to generalize from
  $\mathbb{R}^n$ to manifolds. In an abstract sense, I think I understand the differential being a linear map. But in $\mathbb{R}$ we just have one derivative $f'(x)$ where we plug in a value for x and get a scalar that tells us the derivative at that point. But in higher dimensions, we establish a gradient or a Jacobian at a point  and then we use the dot product to calculate the derivative as stated above. Of what importance is it at which point we create the gradient and of what importance which vector we multiply with the gradient? Is $\nabla f(3,3)$ the derivative of the function $f:\mathbb{R^2}\rightarrow \mathbb{R}$? Or is it $\langle\nabla f(x,y),(3,3)\rangle$ Or is it  $\langle\nabla f(3,3),(3,3)\rangle$ ? How can one understand this as just the higher dimensional derivative comparable to a single dimensional $f'(3)$? Why do we need that dot product with an additional vector? Where is the connection between the derivative being a linear map and the derivative just being the sum of the partial derivatives? EDIT: If it isn't clear, what I'm implying consider sites like these http://www.solitaryroad.com/c353.html or https://en.wikipedia.org/wiki/Total_derivative which essentially state The total differential is the sum of the partial differentials. But I can not fathom how that is equivalent to the linear map definition above.","['multivariable-calculus', 'calculus']"
2280323,Linear transformation and its matrix with respect to unknown bases,"I am given a linear transformation
$$T:\mathbb{R^3} \rightarrow \mathbb{R^2}$$
$$T((x,y,z)) = (x+y,-y+z)$$
The task if to find a basis in $\mathbb{R^3}$, let's call it $B=\{e_1, e_2, e_3\}$ and $\mathbb{R^2}$, let's call it $B'=\{f_1, f_2\}$ such that $A$ is the matrix of this transformation with respect to the found bases. Here is the matrix $A$:
$$A = \begin{bmatrix} 1&0&0\\0&2&0\end{bmatrix}$$ I think I am not sure how to interpret the given matrix $A$.","['matrices', 'linear-algebra', 'linear-transformations', 'vector-spaces']"
2280360,"If the Darboux sums converge to the integral, does the parameter of the partition tend to zero?","Let $f:[a,b] \to \mathbb{R}$ be a (Riemann) integrable function, which is nowhere locally constant (i.e $f$ is not constant on any open subinterval). Let $P_n$ be a sequence of partitions of $[a,b]$. Suppose that the upper-Darboux sums $\lim_{n \to \infty}U(f,P_n)=\int_a^b f(x)dx$. Is it true that 
$\lambda(P_n) \to 0$? ($\lambda(P_n)$ is the parameter of the partition, i.e the maximal length of a subinterval in the partition). One could probably replace the upper-sums with lower-sums or even arbitrary Riemann sum, to get variants of the question...","['real-analysis', 'integration', 'calculus', 'riemann-integration']"
2280401,A question about orientation and real plane curves,"Let $C$ be a smooth projective curve in $\mathbb{R}P^2$ given by a polynomial equation $F(x,y,z)=0$ of even degree (so $C$ consists of closed loops). Then $C$ divides $\mathbb{R}P^2$ into 2 parts, only one of which is orientable. Given a point $P\in\mathbb{R}P^2$, is there a way (in terms of $F$, say) to determine to which part $P$ belongs?","['algebraic-curves', 'plane-curves', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
2280406,Second derivative of a geodesic,"I am reading a paper , in which the author considers a geodesic $\gamma(t)$ on a surface with $\gamma(0)=p,\gamma'(0)=v, \|v\|=1$ and wants to express the second derivative. He gets $$\gamma''(t) = - \gamma(t) - h(\gamma',\gamma')\nu(\gamma(t)),$$
where $\nu$ is the gauss map and $h$ is the Weingarten map/Second fundamental form. I see how he gets the first part, but i can't see how he gets the normal part. Can anyone help?","['minimal-surfaces', 'differential-geometry', 'geodesic']"
2280456,Solutions to this fractional differential equation,"So we all know that $\frac d{dx}e^x=e^x$ and that the $n$th derivative of $e^x$ is still $e^x$, but upon entering fractional calculus, this is ruined.  Let $D^\alpha$ be the $\alpha$th derivative with respect to $x$.  Then, as we can see , when $\alpha\in[0,1)$, $$D^\alpha e^x=-\frac1{\Gamma(1-\alpha)}e^x\gamma(\alpha,x)\ne e^x$$ where we use the lower incomplete gamma function. Which raises the interesting question: What are the solutions to the following fractional differential equation? $$D^\alpha f(x)=f(x)$$ where we have $$D^\alpha f(x)=\frac1{\Gamma(n-\alpha)}\int_0^x\frac{f^{(n)}(t)}{(x-t)^{\alpha+1-n}}\ dt$$ with $n=\lfloor\alpha+1\rfloor$. $f$ may be a function of $\alpha$.","['calculus', 'fractional-calculus', 'definite-integrals', 'fractional-differential-equations', 'ordinary-differential-equations']"
2280517,What does this characterization of the determinant theorem show?,"I don't understand what this theorem for a characterization of the determinant actually means, and what use it has. Could you please explain it? Let $D$ be a function mapping from the set of square matrices $M$ with $n$ rows/columns over the field $F$, to a field $F$. Also let $D$ be a function that is multilinear and alternating in columns. Then $$D(A) = D(I) \det A$$","['matrices', 'linear-algebra', 'determinant']"
2280526,$y'' + y' = \frac 43x - 1$,"I was able to get the solution to $y'' + y' = \frac 43x - 1$ by using variation of parameters. I got $y = \frac 23 x^2 - \frac 73 x+c_1+c_2e^{-x}.$ But I wanted to know how to solve this using undetermined coefficients since that method is easier. The characteristic polynomial is $r^2+r=r(r+1)=0$ which has roots $r=0,-1$ so $y_c = c_1+c_2 e^{-x}.$ What I learned is that if the ""forcing function"" ($\frac 43x -1$ in this problem) shares a term that is $x^k$ times a term in $y_c,$ where k is a nonnegative integer, then $y_p$ will contain a term that is $x^{k+1}$ times the shared term. Since $y_c$ and the forcing function have the term ""1"" in common, I thought this meant that $y_p$ would contain a term $x^{0+1}*1=x.$ So $y_p$ would have the form $Ax+B.$ Since B is accounted for in $y_c$, I tried to use $y_p=Ax.$ This gave me the wrong answer, which made me think I need to try a solution of the form $y_p=Ax^2+Bx.$ However, I don't know where the $x^2$ term comes from! If this is the right form, can someone explain where the quadratic term comes from? If I'm wrong, please correct me. Also, sorry if there are any typos. Thank you. :)",['ordinary-differential-equations']
2280552,"Graph polar equation $r = 6 cos(\theta)$, issues with plotting on xy-plane","I will be grateful for your help and explanation on how to decipher what authors of the Precalculus book did when plotting the equation from $r,\theta$ -plane into xy-plane. Below I quote 3 parts of their explanations, and attach screen shots of graphs Example on how to graph the polar equation
$$r = 6 cos(θ)$$ Quote part 1: We graph one cycle of $r = 6 cos(θ)$ on the polar plane and use it to help graph the equation on the xy-plane. We see that as $θ$ ranges from $0$ to $π/2$ , $r$ ranges from $6$ to $0$. In the xy-plane, this means that the curve starts 6 units from the origin on the positive x-axis (θ = 0) and gradually returns to the origin by the time the curve reaches the y-axis (θ = π/2 ). The arrows drawn in the figure below are meant to help you visualize this process. In the θr-plane, the arrows are drawn from the θ-axis to the curve r = 6 cos(). In the xy-plane, each of these arrows starts at the origin and is rotated through the corresponding angle , in accordance with how we plot polar coordinates.
End of the quote part 1. Picture attached. Quote part 2: Next, we repeat the process as θ ranges from π/2 to π. Here, the r values are all negative. This means that in the xy-plane, instead of graphing in Quadrant II, we graph in Quadrant IV, with all of the angle rotations starting from the negative x-axis.
End of the quote part 2. So, if θ = 3π/4, then r = -3√2
θ = π , then r = -6 In the first part we started at the angle θ = 0 and thus r = 6, which we plotted as x = 6; then rotating counter-clockwise as all values of r become smaller as θ approaches π/2. This is clear to me. And now I am confused by the second part. It is said that r values are negative, so I don't understand why we plot these values along the positive x-axis and rotate clockwise. How did they come up with this rotation, what is the reason that I fail to understand? The phrase on the picture saying ""r < 0 so we plot here"" gives a sense that this is obvious, but not to me. Please, help me to understand it. It seems they are plotting absolute values of r along x-axis, so all x values are positive. But how's this justified mathematically? Here is also the next, even more confusing, quote:
As θ ranges from π to 3π/2, the r values are still negative, which means the graph is traced out in Quadrant I instead of Quadrant III.
End of quote. Interesting. The second part stated that as values of r are negative, we have to plot in QIV; and the third quote says that as values are still negative, we obviously have to plot in QI. I am utterly confused. :) Please, help. Summarizing a bit: (1) when the interval is [0, π/2] and r = 6, they start at y = x = 6 and go all the way ""up"" (counter-clockwise) till angle reaches π/2; it's clear;
(2) when the interval is [π/2 , π] and the value of r = -6 at angle π, they most likely start at the angle -π/2, move counter-clockwise ""up"" till 2π, and surprisingly end at y = x = 6, not x = -6;
(3) and then, when even more puzzling thing happens - even though values are still negative and interval is [π, 3π/2], they get back to the interval [0, π/2] in QI;
(4) and for the last interval of [3π/2, 2π], they get back to QIV.
I genuinely don't get it. It seems there is something very easy-peasy in all this, some very basic notion that I miss. Thank you very much!","['algebra-precalculus', 'trigonometry']"
2280587,$a/b < (1 + \sqrt{5})/2 \iff a^2 - ab - b^2 < 0$?,"For positive integers $a$ and $b$, I want to show that $a/b < (1 + \sqrt{5})/2$ if and only if $a^2 - ab - b^2 < 0$. I had a loose proof ready to go, but I noticed a fatal flaw. Perhaps there is a way to work around this though. My tactic was to start from $a^2 - ab - b^2 < 0$ and complete the square on the LHS for $a$. I ended up with
$$
\left(a-\frac{b}{2}\right)^2 - \frac{5b^2}{4} < 0 \iff \left(a - \frac{b}{2} \right)^2 < \left( \frac{\sqrt{5} \, b}{2} \right)^2.
$$ Now the tempting thing to do is to show this is equivalent to saying
$$
\quad \quad \qquad \quad \, \, \iff a-\frac{b}{2} < \frac{\sqrt{5} \, b}{2}
$$ but obviously it is necessary for $a > b / 2$ for this to work. This is not necessarily the case because if $a = 1$ and $b = 5$, then $a^2 - ba - b^2 = -29 < 0$ and $a/b = 1/5 < (1+\sqrt{5})/2$ so the initial claim is true but $a \leq b/2$. So is there some kind of assumption I can make to get around this, and without loss of generality? Or should I rethink the entire structure of the proof? Cheers!","['fibonacci-numbers', 'golden-ratio', 'algebra-precalculus', 'proof-writing', 'sequences-and-series']"
2280604,Axiom of Dependent Choice and the von Neumann Universe,"Reading Roitman's revised edition of her 'Introduction to Modern Set Theory', I got  a bit confused about her proof of the claim that under regularity $V$, the universe of sets is equal to $\bigcup_{\alpha \in ON} V_\alpha$, the von Neumann universe, especially about the role of choice principles in that proof. Here's her proof. Let a set $x$ be big, if $x \not \in \bigcup_{\alpha \in ON} V_\alpha$. Every big $x$ has some big element. Otherwise we would have $x \subseteq \bigcup_{\alpha \in ON} V_\alpha$, which is impossible. So, if there's some big $x$, $x$ is the beginning of some infinite descending $\in$-chain, contradicting regularity. Here's my question: Doesn't inferring the existence of some infinite descending $\in$-chain require the axiom of Dependent Choice (DC)? For under DC from every big $x$ containg some big $y$ we can conclude that there's some countably infinite sequence of big sets $f$ with $f(0) = x$ and $f(n +1) \in f(n)$. Is there a way of avoiding choice principles in Roitman's proof?","['logic', 'elementary-set-theory']"
2280612,"Projective variety with $H^i(X, \bigwedge^j T_X) \cong 0$ and $H^i(X, \Omega^j_X) \cong 0$","Let $X$ be a smooth projective variety over a field. I know that $$
H^i(X, \bigwedge^j T_X) \cong 0,
$$ for $i+j >0$ and also $$
H^i(X, \Omega^j_X) \cong 0,
$$ for $i \neq j$ . Is it enough to conclude that $X$ is a point?",['algebraic-geometry']
2280669,Prove differentiability of $\sin(z)$ with use of $\phi(z)$.,"Prove that $\sin(z)$ is differentiable in $a$ with the use of a function $\phi$, that satisfies $\sin(z) = \sin(a) + (z-a)\dot\phi(z)$ and $\phi$ is continuous at $a$. I need to show this by useing powerseries. A hint given is that $\sin(z) = \sin(a + z - a)$ It is clear that $\phi(z) = \cos(z)$, but I'm having trouble showing this with powerseries. All help is much appreciated","['complex-analysis', 'analysis']"
2280671,Definiteness of a general partitioned matrix $\mathbf M=\left[\begin{matrix}\bf A & \bf B\\\bf B^\top & \bf D \\\end{matrix}\right]$,"If $$\mathbf M=\left[\begin{matrix}\bf A & \bf b\\\bf b^\top & \bf d \\\end{matrix}\right]$$ such that $\bf A$ is positive definite, under what conditions is $\bf M$ positive definite, positive semidefinite and indefinite? It is readily seen that $\det(\mathbf M)=\alpha\det(\mathbf A)$ , where $\alpha=\mathbf d-\bf b^\top A^{-1}b$ Now, $\alpha>0\Rightarrow \det(\mathbf M)>0$ $\quad(\det(\mathbf A)>0$ by hypothesis $)$ This is not enough for $\bf M$ to be p.d. as I need all its leading principal minors to be positive as well. But the answers tell me that $\alpha>0$ ensures all of that. $\alpha=0\Rightarrow \bf M$ is singular As it turns out, this condition ensures that $\bf M$ is p.s.d, but as far as I know this is not the case in general. $\alpha<0\Rightarrow \det(\mathbf M)<0$ This condition is apparently sufficient in this case for $\bf M$ to be indefinite. But if $\bf M$ is indefinite, it may be a singular matrix as well (i.e., $\alpha$ might as well have vanished in this case). Moreover, as I note in the above two cases, this isn't a sufficient condition either for a matrix to be indefinite. I would like to clarify my doubts and a possible proof sketch would be helpful too. EDIT. Extending the problem from a bordered matrix to a partitioned matrix: Suppose $\mathbf M=\left[\begin{matrix}\bf A & \bf B\\\bf B^\top & \bf D \\\end{matrix}\right]$ is symmetric and $\bf A$ is square. Then show that $\bf M$ is p.d. iff $\bf A$ and $\mathbf D-\bf B^\top A^{-1}B$ are p.d. What should be the correct approach now? I can calculate the determinant like before but the same problem still bothers me. Thinking in terms of quadratic forms and using the definitions of definiteness do not seem to be the way to go about it.","['matrices', 'positive-definite', 'positive-semidefinite', 'quadratic-forms', 'linear-algebra']"
2280703,"Limit of $(1+1/n)^n$ is not equal to one, but why ?","The fact that :  $$\lim_{n\to\infty} (1+1/n)^n \ne 1$$ is  conterintuitive to me. Why this doesn't work : $\lim_{n\to\infty} 1/n = 0$, then by composition : $\lim_{n\to\infty} (1+1/n)^n = 1$ ? Is there a calculus way and intuitive way to understand why this is false ?",['limits']
2280704,Find points A and B from their angle bisectors,"I need to find points $A$ and $B$ in $ℝ^2$, when I only know three points $X_1, X_2, X_3$ and angle bisectors of the angles $A X_1 B$, $A X_2 B$ and $A X_3 B$. How do I do this geometrically? . If I used only two points – $X_1$ and $X_2$ – and their respective lines, I'm able to find a point $B'$ for every $A'$, so that it fulfils the condition. This means it has infinite solutions. . For the two points the solution was pretty easy – I just needed draw the line $AX_1$ and then another one that is axially symmetric to it, then repeat for $X_2$. $B$ is at the intersection of the two axially symmetric lines. However this approach isn't of much use when I use all the three points, as the lines usually don't meet at a single point. . There's always one point A' such that the three lines do meet in one point. This is the solution, however I wasn't able to find it otherwise than moving it here and there using trial and error.","['plane-geometry', 'euclidean-geometry', 'geometric-construction', 'geometry']"
2280743,Prove that a function $g$ is differentiable in 0,"Let $f : \mathbb{R} \to \mathbb{R}$ continu in 0 and let $g : \mathbb{R} \to \mathbb{R}$ defined by $g(x) = x f(x)$, for $x \in \mathbb{R}$. Prove that $g$ is differentiable at 0 and determine $g'(0)$. This is an assignment for my class, but I'm having trouble finding the right argumentation. My thoughts were the following : If I can prove that the function f is differentiable at 0, and I can prove that a function $h : \mathbb{R} \to \mathbb{R}$ defined by $h(x) = x$ is differentiable at 0, then also the function $g$ is differentiable at 0? Well, I know how to prove that the function $h$ is differentiable in 0 and what the derivative is. But how do I prove that the function $f$ is differentiable?","['derivatives', 'real-analysis']"
2280760,Poisson as limit of Binomial distribution,"Suppose that we have a large number $n$ of independent trials, but the probability $p$ of success is very small, in such a way the the expectation $\mu=np$ of the number of successes is moderate. Keep $\mu=np$ fixed and let $n$ tend to infinity. Then the probability $r$ successes is:
$b(n,p;r)=b(n,\frac{\mu}{n};r)=C^n_r\times (\frac{\mu}{n})^r(1-\frac{\mu}{n})^{n-r}=\frac{\mu^r}{r!}\frac{n(n-1...(n-r+1)}{(n-\mu)^r}(1-\frac{\mu}{n})^n$
As $n\to\infty$ 
$b(n,p;r)\to\mu^r \frac{e^{-\mu}}{r!}$ After taking a course on measure theory I am studying probability theory as the application of measure theory but I am having some problems on the convergence of functions. I tried to use Taylor expansion to approximate the Binomial of a Poisson but unsuccessfully. (1) How do I to get from $\frac{\mu^r}{r!}\frac{n(n-1...(n-r+1)}{(n-\mu)^r}(1-\frac{\mu}{n})^n$ to $\mu^r \frac{e^{-\mu}}{r!}$ (2)What is the difference between a Binomial and Poission distribution? Is it the $p$? Thanks in advance!","['real-analysis', 'calculus', 'probability-theory']"
2280770,Gaps between numbers of the form $pq$,"Mathematicians keep improving Zhang's bound on gaps between primes. According to Wikipedia, there are infinitely many pairs of primes such that their difference is no more than 246. This is all very exciting. What does it tell us about gaps between products of two distinct primes? Can we say that there are infinitely many pairs of numbers of the form $pq$, for distinct primes $p$ and $q$, such that their difference is less than some $K$? To be clear, I mean pairs of numbers $(a,b)$, where $a=p_1q_1$ and $b=p_2q_2$, where $p_i\neq q_i$, but it's okay if $p_i=q_j$, or $p_i=p_j$ when $i\neq j$. Thanks in advance for any insights on this.","['number-theory', 'prime-gaps', 'prime-numbers']"
2280909,a little problem in the proof of Peter-Weyl theorem,"I am trying to understand the proof of Peter-Weyl theorem which has the following form. $Theorem.$ If $G$ is a compact group, then the linear span of all matrix coefficients for all finite-dimensional irreducible unitary representations of $G$ is dense in $L^2(G)$. Let $U$ be the closure in $L^2(G)$ of the linear span of all matrix coefficients of all finite-dimensional irreducible unitary representations of $G$.  Arguing by contradiction, suppose $U\neq L^2(G)$. Then $U^\perp\neq 0$. Note that if $h(x)=\langle R(x)u,v\rangle$ is a matrix coefficient in $U$, then the following functions of $x$ are also matrix coefficients for the same representation $R$:
\begin{align*}
        \overline{h(x^{-1})}&=\overline{\langle R(x^{-1})u,v\rangle}=\langle v,R(x^{-1})u\rangle=\langle R(x)v,u\rangle \\
        h(gx)&=\langle R(gx)u,v\rangle=\langle R(x)u,R(g^{-1})v\rangle \\
        h(xg)&=\langle R(xg)u,v\rangle=\langle R(x)R(g)u,v\rangle 
\end{align*}
 How can we conclude that for any $h\in U^\perp$, the function $h'$ defined by $h'(x):=h(y^{-1}x)$ for some $y\in G$ is in $U^\perp$ ? Thanks. $My \ attempt.$ If $h=0$ then there is nothing to do. So let $h\in U^\perp \setminus \{0\}.$ Suppose $h'\notin U^\perp$. Let $\mathcal{M}$ be a basis  for $U$ consisting of some matrix coefficients in $U$. Then we may write $$h'=g+\sum_{i=1}^n a_if_i$$ for some $f_1,...,f_n\in \mathcal{M}$ and $g\in U^\perp$ and for some scalars $a_1,...,a_n$. Note that at least one of these scalars is nonzero. Now we  have for all $x\in G$
\begin{equation}
h'(x)=h(y^{-1}x)=g(x)+\sum_{i=1}^n a_i f_i(x)\qquad (*)
\end{equation}
For each $i$, put $f_i(x)=\langle R_i(x)u_i,v_i\rangle$  where $R_i$ is the corresponding representation of $G$ for $f_i$ and also $u_i$ and $v_i$ are corresponding vectors for $f_i$. By the unitarity, we have 
$$f_i(x)=\langle R_i(y^{-1}x)u_i,R_i(y^{-1})v_i\rangle$$ 
So for any $t\in G$, the equation ($*$) becomes 
$$h'(yt)=h(t)=g_1(t)+\sum_{i=1}^n a_if'_i(t)$$ where  $g_1(t):=g(yt)$ and $f'_i(t):=\langle R_i(t)u_i,R_i(y^{-1})v_i\rangle$. Notice that the set $\{f'_i:i=1,...n\}$ is also linearly independent. For simplicity, we write 
$$h=g_1+\sum_{i=1}^n a_if'_i$$ where $g_1=r+s$ for some $r\in U\setminus\{0\},s\in U^\perp\setminus\{0\}$. Then we get 
$$h-s=r+\sum_{i=1}^na_if'_i\in U\cap U^\perp =\{0\}$$ I couldn't get any contradiction from here. Could anyone help me? Thanks.","['representation-theory', 'analysis']"
2280929,Solve $\sqrt{3+\sqrt{3+x}}=x$,"Solve
  $$\sqrt{3+\sqrt{3+x}}=x$$ My try: $$\sqrt{3+\sqrt{3+x}}=x \\ 
3+\sqrt{3+x}=x^2\\\sqrt{3+x}=x^2-3\\3+x=(x^2-3)^2$$ $$x^4-6x^2+9=x+3\\x^4-6x^2-x+6=0$$ Now ?",['algebra-precalculus']
2280948,How to show that a product is a product in $\text{Set}$,"Suppose $F:\text{Set}\rightarrow\text{Set}$ is a functor defined as follows: If $X$ is a set, $F(X) = \text{the set of ultrafilters on X}$ And if $f:X\rightarrow Y$ is a morphism in $\text{Set}$ then 
$F(f): F(X)\rightarrow F(Y)$ by taking an ultrafilter, $\mathcal{F}$, on $X$ to the ultrafilter on $Y$ defined as $f_*\mathcal{F} = \{B\subseteq Y: f^{-1}(B)\in\mathcal{F}\}$. I am trying to show that $$F(\pi_i): F(\Pi_{a\in I} X_a)\rightarrow F(X_i)$$
is a terminal cone (here $\pi_i$ are projections of the product defined as usual). So for any other cone on the same family, say $f_i: C\rightarrow F(X_i)$ I need to find a unique morphism $g:C\rightarrow F(\Pi_{a\in I} X_a)$ such that $F(\pi_i)\circ g = f_i$ for each $i$. I had an idea of defining $g(x)$ as the ultrafilter containing the filter  $\{\Pi_{a\in I} B_a : B_a \in F(X_a) \text{ for all } a\in I  \}$. But the problem here is that $g$ is not unique as we can choose from possibly multiple ultrafilters. Question: How should I choose $g$?","['category-theory', 'filters', 'elementary-set-theory']"
2281000,Convergence acceleration technique for $\zeta(4)$ (or $\eta(4)$) via creative telescoping?,"Question Is it already known whether the $\zeta(4):=\sum_{n=1}^{\infty}1/n^4$ accelerated convergence series $(1)$ , proved for instance in [1, Corollaire 5.3], could be obtained by a similar technique to the ones explained by Alf van der Poorten in [2, section 1] for $\zeta(3)$ and $\zeta(2)$ ? $$\zeta(4)=\frac{36}{17}\sum_{n=1}^{\infty}\frac{1}{n^{4}\binom{2n}{n}}.\tag{1}$$ (a) In other words, does there exist a pair of functions $F(n,k), G(n,k)$ obeying equation $$F(n+1,k)-F(n,k)=G(n,k+1)-G(n,k)\tag{$\ast$}$$ from which $(1)$ can be proved? That is, is it possible to transform  the defining series for $\zeta(4):=\sum_{n=1}^{\infty}1/n^4$ by means of the Wilf-Zeilberger method (or the Markov-WZ Method ) into the faster series $(1)$ ? (b) Most likely there isn't any such a pair $(F, G)$ , but I do not have the means to use these methods on my own. Short description of section 1 of Alf van der Poorten's paper The defining series for $\zeta(3):=\sum_{n=1}^{\infty}1/n^3$ and $\zeta(2):=\sum_{n=1}^{\infty}1/n^2$ are accelerated resulting in \begin{equation*}
\zeta (2)=3\sum_{n=1}^{\infty }
\frac{1}{n^{2}\binom{2n}{n}},\tag{2}
\end{equation*} \begin{equation*}
\zeta (3)=\frac{5}{2}\sum_{n=1}^{\infty }
\frac{(-1)^{n-1}}{n^{3}\binom{2n}{n}}\tag{3}.
\end{equation*} For instance, $(3)$ follows from the identity \begin{equation*}
\sum_{n=1}^{N}\frac{1}{n^{3}}-2\sum_{n=1}^{N}\frac{\left( -1\right) ^{n-1}}{n^{3}\binom{2n}{n}}=\sum_{k=1}^{N}\frac{(-1)^{k}}{2k^{3}\binom{N+k}{k}\binom{N}{k}}-\sum_{k=1}^{N}\frac{(-1)^{k}}{2k^{3}\binom{2k}{k}}\tag{4},
\end{equation*} by letting $N\rightarrow \infty $ and noticing that \begin{equation*}
\lim_{N\to\infty}\sum_{k=1}^{\infty}\frac{(-1)^{k}}{2k^{3}\binom{N+k}{k}\binom{N}{k}}=0.
\end{equation*} Equality $(4)$ can be explained as follows: Write \begin{equation*}
X_{n,k}=\frac{(-1)^{k-1}}{k^{2}\binom{n+k}{k}\binom{n-1}{k}},\qquad D_{n,k}=\frac{(-1)^{k}}{n^{2}\binom{n+k}{k}\binom{n-1}{k}}\qquad k<n.
\end{equation*} Notice that $$X_{n,k}=D_{n,k-1}-D_{n,k}.\tag{5}$$ Hence \begin{eqnarray*}
\sum_{k=1}^{n-1}\frac{X_{n,k}}{n} &=&\sum_{k=1}^{n-1}\left( \frac{D_{n,k-1}}{
n}-\frac{D_{n,k}}{n}\right) =\frac{D_{n,0}}{n}-\frac{D_{n,n-1}}{n} \\
&=&\frac{1}{n^{3}}-2\frac{\left( -1\right) ^{n-1}}{n^{3}\binom{2n}{n}},\qquad\frac{D_{n,0}}{n} =\frac{1}{n^{3}},\quad \frac{D_{n,n-1}}{n}=2\frac{
\left( -1\right) ^{n-1}}{n^{3}\binom{2n}{n}}
\end{eqnarray*} Sum over $k$ , $1\leq
k\leq n-1$ \begin{equation*}
\sum_{k=1}^{n-1}X_{n,k}=\sum_{k=1}^{n-1}\left( D_{n,k-1}-D_{n,k}\right)
=D_{n,0}-D_{n,n-1}.
\end{equation*} Now, summing over $n$ , $1\leq n\leq N$ , and noticing that \begin{equation*}
\frac{X_{n,k}}{n}=E_{n,k}-E_{n-1,k},\qquad E_{n,k}=\frac{(-1)^{k}}{2k^{3}\binom{n+k}{k}\binom{n}{k}},\tag{6}
\end{equation*} we obtain \begin{equation*}
\sum_{k=1}^{N-1}\sum_{n=k+1}^{N}\frac{X_{n,k}}{n}=\sum_{k=1}^{N-1}
\sum_{n=k+1}^{N}\left( E_{n,k}-E_{n-1,k}\right) =\sum_{k=1}^{N}\left(
E_{N,k}-E_{k,k}\right). 
\end{equation*} So, on the one hand \begin{eqnarray*}
\sum_{n=1}^{N}\sum_{k=1}^{n-1}\frac{X_{n,k}}{n} &=&\sum_{n=1}^{N}\frac{1}{
n^{3}}-2\sum_{n=1}^{N}\frac{\left( -1\right) ^{n-1}}{n^{3}\binom{2n}{n}},\tag{7}
\end{eqnarray*} and on the other hand \begin{eqnarray*}
\sum_{n=1}^{N}\sum_{k=1}^{n-1}\frac{X_{n,k}}{n} 
&=&\sum_{k=1}^{N}E_{N,k}-\sum_{k=1}^{N}E_{k,k} \\
&=&\sum_{k=1}^{N}\frac{(-1)^{k}}{2k^{3}\binom{N+k}{k}\binom{N}{k}}
-\sum_{k=1}^{N}\frac{(-1)^{k}}{2k^{3}\binom{2k}{k}}.\tag{8}
\end{eqnarray*} The identity $(4)$ follows. Remarks The combination of equations $(5)$ and $(6)$ forms an identity of the form $(\ast)$ , which is equation $(6.1.2)$ of [3, chapter 6] (Zeilberger's Algorithm). As for $(2)$ , [2, section 1] actually explains how to accelerate $\eta(2):=\sum_{n=1}^{\infty }(-1)^{n-1}/n^{2}$ and obtain $(2)$ , using the
relation $\eta(s) = \left(1-2^{1-s}\right) \zeta(s)$ . As such, if feasible, I expect that accelerating $\eta(4)$ might be easier than $\zeta(4)$ . References Henri Cohen, Généralisation d'une Construction de R. Apéry Alfred van der Poorten, Some wonderful formulae... Footnotes to Apery's proof of the irrationality of $\zeta(3)$ Marko Petkovsek, Herbert Wilf, Doron Zeilberger, A = B","['sequences-and-series', 'convergence-acceleration', 'number-theory', 'summation', 'experimental-mathematics']"
2281007,How many groups of 4 primes exist such that their sum is a prime and that $p^2+qs$ and $p^2+qr$ are squares?,"How many groups of 4 primes of the form $(p, q, r, s)$ exist such that their sum is a prime and that $p^2+qs$ and $p^2+qr$ are both square numbers? I'm having trouble finding a quadruple satisfying all conditions. I've found a few satisfying the last condition, but they've always failed the first condition. Any help? Thanks! Can someone give me a start or solution?",['number-theory']
2281032,"$\int_\Omega |\nabla u |^2-2u^2\sin{x} \, dx$ - first variation, natural boundary conditions and solution for","I am taking calculus of variations course and I am stuck with the following problem: I need to find first varation , Euler equation, natural boundary conditions and solution for $I[u]=\int_\Omega (|\nabla u |^2-2u^2\sin{x} )\, dx$. First of all I am comfortable with problems like this one but for ""ordinary"", more straightforward functionals. E.g. I can solve the same problem if $I[u]=\int_0^1(y'')^2+2x\, dx + x^2(0)$ without any issues. But I never met integrals like this - it looks like it is ""ordinary"" integral but it is defined on a domain $\Omega$. Moreover, we do not have any information not only about boundaries of $\Omega$, but also even about dimension of $\Omega$! I guess it is either does not depend on the dimension or if it depends on it please suppose that the dimension is 2. But still, there is no information about the boundary of $\Omega$. I understand the general idea about first steps. We calculate the first variation by definition and use it to obtain Euler Equation and natural boundary conditions.
$$\begin{align}
0 =\frac{d}{dt}\Bigg[ I[u+th]\Bigg ]_{t=0} = \frac{d}{dt}\Bigg [\int_\Omega (|\nabla (u+th) |^2-2(u+th)^2\sin{x} )\, dx\Bigg ]_{t=0} =\\
= \frac{d}{dt}\Bigg [\int_\Omega (\sum_i{(u'_i+th'_i)^2}-2(u^2+2thu+t^2h^2)\sin{x} )\, dx\Bigg ]_{t=0} =\\
=2\cdot\int_\Omega \sum_i{u'_ih'_i}-2hu\sin{x} \, dx= -4\cdot\int_\Omega \ hu\sin{x}\, dx + 2\cdot\int_\Omega \ \sum_i{u'_ih'_i}\, dx
\end{align}$$ What one is trying do do next with an ""ordinary"" functional is the following:
We should represent first variation with an integral having the form $\int Eh \, dx$ (where E is Euler equation) and another terms, which have the form $h(x_j)B_j$ (where $B_j$ will give us natural boundary conditions). In ""ordinary"" case at this moment we use integration by parts in order to remove terms with $h'$ and obtain everything else.
But I do see how to do it here. I guess that I have to modify the last summand $$\int_\Omega \ \sum_i{u'_ih'_i}\, dx$$ with some kind of multidimensional integration by parts, but I do not see how to do it. Moreover, I do not see how to solve the problem because I do not see the form of Euler equation and natural boundary conditions. I do not even feel sure the solution (by ""solution"" I mean explicit form of $u$) can be found. Please help me to understand this.
Thanks a lot for your time and help! Update: Gio67 advised to use Divergence theorem I can move forward a bit: $$\begin{align}
= -2\cdot\int_\Omega \ h(2u\sin{x} + \Delta u)\, dx + 2\cdot\int_{\partial\Omega} \ h\sum_i{u'_i\nu_i}\, ds
\end{align}$$ Can you please advice what to do with the last summand and how to get Euler equation and natural boundary conditions?","['multivariable-calculus', 'calculus-of-variations', 'integration', 'calculus']"
2281072,Order of a holomorphic function along an analytic hypersurface,"Let $Y\subset \mathbb{C}^n$ be an analytic subset, i.e. for every $x \in Y$ there exists a holomorphic function (germ) $g \in \mathcal{O}_x$ such that $Y$ is locally cut out by $x$. Let $f$ be a holomorphic function, and suppose that $Y$ is locally cut out by irreducible $g\in \mathcal{O}_x$ around $x$, then the order of $f$ at $x \in Y$ is defined to be the integer $a=ord_x f$ such that $f=g^a h$ in the factorization of $\mathcal{O}_x$ (since $\mathcal{O}_x$ is a UFD). It is claimed, in many complex geometry books like Huybrechts and Griffiths, that $ord_x f$ is locally constant, yet I don't see why this is true. I suppose a proof would be along the lines of: $Y$ at a point $y$ near $x$ is also locally cut out by $g\in \mathcal{O}_y$, and $h$ remains to be relatively prime to $g$ if $y$ is close enough, so we still have $f=g^{ord_x f} h$ in $\mathcal{O}_y$. But my problem with the above proof is that $g$ may not be a locally defining function of $Y$ at $y$, in particular $g$ could be reducible in $\mathcal{O}_y$, since there are examples of irreducible holomorphic function germs becoming reducible in a neighborhood (see: Irreducibility of holomorphic functions in a neighborhood of a point ). In this case we can only conclude $ord_y f \geq ord_x f$, i.e. the order is locally non-decreasing. An explanation of how to reason that the order is locally constant would be great. Thank you.","['complex-geometry', 'algebraic-geometry', 'local-rings']"
2281073,Is $|f(b)-f(a)| > |b-a|$ true for $f(x)=x+(1+e^x)^{-1}$?,"I'd like to use this as part of a proof, but I couldn't realize how to show this (and if it) is true. The function is: $f(x)=x+(1+e^x)^{-1}$","['real-analysis', 'analysis']"
2281128,Is there an intuitive explanation of $n$-dimensional sphere's surface and volume being maximal in dimensions 5 and 6? [duplicate],"This question already has answers here : Volumes of n-balls: what is so special about n=5? (4 answers) Closed 7 years ago . I was wondering if there is an intuitive explanation why the surface of an $n$-dimensional sphere is maximal at $n=6$ and for the volume $n=5$.
I know that for the surface you have:
$$A_n=\frac{2\pi^\frac{(n+1)}{2}}{\Gamma(\frac{n+1}{2})}$$ And for the volume $$V_n=\frac{A_{n-1}}{n}$$ Finally you get $$A_n(R)=A_nR^n$$
$$V_n(R)=V_nR^n$$ Where $R$ is the radius of the ball. Plugging in the dimensions it's easy to show that $V_n$ is max for $n=5$ and the surface for $n=6$ Is there an intuitive way of explaining this?","['volume', 'geometry']"
2281142,Find the angle $MBA$.,"Triangle ABC has $\angle CAB=30 ^\circ$ and $\angle CBA=70^\circ$. Point $M$ lies inside triangle $ABC$ so that $\angle BAM=20^\circ= \angle ACM$. Find $\angle MBA$. I've already drawn the diagram, but I can't get the angle. Also, I would greatly appreciate if someone could help me with the latex. Thanks!",['geometry']
2281154,Probability that $\sin\theta \cos\phi < 0.999772$,I am solving a kinematics question in particle scattering. The final answers lies in finding the probability such that $$\sin\theta \cos\phi < 0.999772$$ The ranges of $\theta$ and $\phi$ are $0\leq\theta<\pi$ and $0\leq\phi<2\pi$.,['probability']
2281164,Uniformly Convergent Subsequence,"This is almost surely an Arzela-Ascoli question, since it comes from an old exam of which such problems are quite common. Unfortunately, I can't seem to get it though. $\{ f_n \}$ is a sequence of functions $[0,1] \to \mathbb{R}$ satisfying $|f_n'(x)| \leq \frac{1 + |\ln (x)|}{\sqrt{x}}$ and $-10 \leq \int_0^1 f_n(x) dx \leq 10$ for every $n$. The question is to show the existence of a uniformly convergent subsequence. In these kinds of situations, the bounds given usually turn into something nice  to force uniform boundedness. Maybe I could hope to establish equicontinuity using the integral somehow, but I'm not told that the $f_n$ are always positive, so the absolute values would probably screw things up anyway. Is it an easy problem I'm missing or is this totally the wrong track? If so, what's the right track?",['real-analysis']
2281243,a good book for Introduction to Mathematical Statistics,"I am trying to read this book (Introduction
to Mathematical Statistics by Robert,Joseph, Allen,7th edition) I find it hard to follow. So, can anyone please recommend another book has similar content?
My background is Math bachelor level and I will continue my master in Stat  next Fall. 
Thank you in advanced.",['statistics']
2281257,"Differentials on curve, Fulton Ex. 8.23","Let $X$ be a smooth projective curve, $P\in X$, $f\in k(X)$. I am currently trying to show that if $f\in\mathcal{O}_{P}$ then $\frac{df}{dt}\in\mathcal{O}_{P}$ using the method of Ex. 8.23 in Fulton's Algebraic Curves. So far I have considered the map $\varphi:\mathcal{O}_{P}\rightarrow k[[T]],\ t\mapsto T$ where $t$ is a uniformising parameter at $P$, so far I have shown that if we consider the power series expansion of $f=\sum_{i=0}^{\infty}a_{i}t^{i}$ then applying the usual derivation gives $df=(\sum_{i=1}^{\infty}ia_{i}t^{i-1})dt$, hence the image of $\frac{df}{dt}$ in $k[[T]]$ is $\sum_{i=1}^{\infty}ia_{i}T^{i-1}$, which is the formal derivative of $\varphi(f)$. I'm just not sure how this shows that $\frac{df}{dt}\in\mathcal{O}_{P}$. Does it have anything to do with the fact that $\varphi$ is an injective homomorphism?","['abstract-algebra', 'algebraic-geometry']"
2281262,Find solution of maximum of a function with conditions,"\begin{equation} \label{eqgen}
\begin{aligned}
& \underset{x}{\text{maximize}}
& & F(x)\\
& \text{subject to}
& & x \geq 0, \\
& & &x + f(x)\leq d,
\end{aligned}
\end{equation}
  where $F(x), f(x)$ are continuous functions from $\mathbb{R}^+ \to \mathbb{R}^+$. Here is my approach. Intuitively, $x+f(x)\leq d, x\geq 0$ is equivalent to $x \in [a_1, b_1] \cup [a_2, b_2]\cup \cdots \cup [a_{n}, b_{n}]$,
where $a_i\leq b_i$, $a_1$ is either 0 or solution of $x+f(x) = d$, $b_n$ is either d or solution of $x+f(x) = d$, and $a_i, b_i$ are solutions of $x+f(x) = d$ for other cases. Hence, the optimization problem becomes
\begin{equation} 
\begin{aligned}
& \underset{x}{\text{maximize}}
& & F(x)\\
& \text{subject to}
& & x \in  [a_1, b_1] \cup [a_2, b_2]\cup \cdots \cup [a_{n}, b_{n}].
\end{aligned}
\end{equation} max $F(x)$ on $[a_i, b_i]$ is attained at either $a_i, b_i$ or maximum of $F(x)$ on $[a_i, b_i]$. We can conclude that the optimal solution is either solutions of $x+f(x) = d, or x=0, x= d$  or local maximum of function $F(x)$. This is also what I want to prove. How can I formally write down the solution of the above approach? I wrote this and my professor does not accept my solution. Thank you in advance!","['nonlinear-optimization', 'real-analysis', 'optimization', 'calculus', 'analysis']"
2281272,Subbasis for Cartesian Product Topology,"Let $\aleph(\mathcal{A})$ be arbitrary (i.e., let $\mathcal{A}$ be an indexing set of arbitrary cardinality). Then, in the space $\prod \{Y_{\alpha}|\alpha \in \mathcal{A}\}$, for each $\alpha \in \mathcal{A}$, let $\displaystyle \Sigma_{\alpha}$ be a subbasis for the topology $\mathcal{T}_{\alpha}$ of $Y_{\alpha}$. I need to prove that the family $\{ \langle V_{\beta}\rangle \,|\,\text{all}\,v_{\beta}\in \Sigma_{\beta}; \, \text{all}\, \beta \in \mathcal{A} \}$ is also a subbasis for the cartesian product topology in $\displaystyle \prod_{\alpha}Y_{\alpha}$ Please note that this is NOT homework! I have my final exam on Wednesday and I am trying to read through Chapter IV of Dugundji's Topology to help me prepare, but I still am having a lot of trouble even understanding how this cartesian product topology works. This problem appears in the text as part of a given theorem, and then for the proof, it says ""This is immediate from I, 9.5 , and is left for the reader"". I. 9.5 states that ""Let $\{Y_{\alpha}|\alpha \in \mathcal{A}\}$ be a family of nonempty sets; for each $\alpha \in \mathcal{A}$, let $A_{\alpha}, B_{\alpha}$ be subsets of $Y_{\alpha}$. Then, $$(1) \, \displaystyle \prod_{\alpha}A_{\alpha} \cap \prod_{\alpha}B_{\alpha} $$
  $$(2) \displaystyle \prod_{\alpha}A_{\alpha} \cup \prod_{\alpha}B_{\alpha} \subset \prod_{\alpha}\left(A_{\alpha} \cup B_{\alpha} \right)"" $$ I don't really see how this result is ""immediate"" from this - and even if it were, why would it be left to the reader? If something is left to the reader that implies that it requires a little bit of work and mechanics and manipulation, right? On that front, I believe the way to show that this family $\{\langle V_{\beta} \rangle\}$ is a subbasis for the product topology, call it $\mathcal{T}_{p}$ is to show that the topology generated by $\{\langle V_{\beta} \rangle \}$, call it just $\mathcal{T}$ is the same topology as $\mathcal{T}_{p}$. However, I am not sure how to go about doing this. Especially since we need to be a little bit careful, because isn't it true that a set of the form $\displaystyle \prod U_{\alpha}$ where each $U_{\alpha}$ is a proper open subset of $Y_{\alpha}$ is never an open set in the cartesian product $\displaystyle \prod Y_{\alpha}$? (This is what I meant when I said I was having trouble understanding how the cartesian topology works.) Anyway, to start, I'd say, suppose $\langle V_{\beta} \rangle = p_{\beta}^{-1}(V_{\beta})$ is a subbasis for $\mathcal{T}$. Then, taking finite intersections of these $p_{\beta}^{-1}(V_{\beta})$, we obtain basic elements for the topology $\mathcal{T}$. Now, from a different result related to I. 9.5 , we have that $\displaystyle \cap_{\beta} \langle V_{\beta} \rangle = \prod_{\beta} V_{\beta}$, and so our basic elements look like $\displaystyle \cup_{\beta}\prod_{\beta}V_{\beta}$. Then, by I. 9.5 , we have that $\displaystyle \cup_{\beta}\prod_{\beta}V_{\beta} \subset \prod_{\beta}\left( \cup_{\beta}V_{\beta} \right)$. But then, how do I get that this gives me the open sets in $\mathcal{T}_{p}$? Then, going in the other direction is also a problem for me. How do I show that $\mathcal{T}_{p} \subset \mathcal{T}$ here? There is a question already posted on here where the OP asked for a proof of this result using the approach of proving that the product topology is the smallest topology containing $\{ V_{\beta} \}$, but I'm not sure I really understand the language the answer uses regarding maximal elements, or even why proving that it is the smallest such topology is helpful (for reference, the question/answer is here ). What I would like is for someone to provide me with a worked out detailed proof building on what I've already said here (supposing it's right, of course) explaining all the why's. Like I said, this is not homework - I was just trying to work through it on my own to prepare for my exam, and I got stuck. I have a tendency of doing that - getting to a point and then getting stuck for hours in a quagmire of details and losing loads of valuable study time because of it. Therefore, any help you could give me would be greatly appreciated! Thanks ahead of time.","['general-topology', 'elementary-set-theory']"
2281278,Proving continuity of function $f:\mathbb{R}^2\to \mathbb{R}$,"Let $f: \mathbb{R}^2 \to \mathbb{R}$ be defined by
$
f( \, (s,t) \, ) = 
\begin{cases}
\frac{s \, t \, \sqrt{t}}{s^2 + t^2}, & \text{if $(s,t) \neq (0,0)$,}  \\[6pt]
0,                       & \text{if $(s,t) = (0,0)$.}
\end{cases}
$ I want to prove that $f$ is continuous at $(0,0)$. My proof: Let $\delta > 0$ so that $\left|(s,t)-(0,0)\right|=\sqrt{s^2+t^2}<\delta$, and define $\varepsilon:= (2\delta^2)^{1/4}$. Now,
$$
\left|\frac{st\sqrt{t}}{s^2+t^2}\right|=\left|\frac{\sqrt{s^2}\sqrt{t}\sqrt{t^2}}{s^2+t^2}\right|\le \left|\frac{\sqrt{t}(s^2+t^2)}{s^2+t^2}\right|=\sqrt{t}.
$$
Now, since $\delta^2>s^2+t^2$, $\sqrt{t}<\sqrt[4]{\delta^2+s^2}\le\sqrt[4]{\delta^2+s^2+t^2}<\sqrt[4]{2\delta^2}=\varepsilon$. Hence, $f$ is continuous at $(0,0)$. Do you think this is correct? Another approach might have to do with the definition of the derivative, but I don't think that would be a better/easier approach.","['continuity', 'real-analysis', 'functions', 'proof-verification']"
2281290,Understanding the logic behind finding the Parent Function given an Exact Differential,"I know the procedure to find the parent function but I do not understand the logic behind this procedure. Lets suppose we are greeted with an exact differential :
$$du=y\,dx+(x+2y)dy\tag{a}$$ and we wish to find the parent function $u=u(x,y)$. For the case of a general differential $df$ $$df(x,y)=\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy\tag{b}$$ Using the form of $(\mathrm{b})$ in the specific case of $(a)$ we have $$\frac{\partial u}{\partial x}=y\tag{1}$$
and
$$\frac{\partial u}{\partial y}=x+2y\tag{2}$$ Starting with equation $(1)$; I attempt via integration wrt $x$ to find the parent function $u$: $$u(x,y)=\fbox{$\color{red}{\int\frac{\partial u}{\partial x}dx=xy+g(y)}$}$$ Where $g(y)$ is an unknown function of $y$. Now it is at this point that I am already confused for $2$ reasons: 1. The LHS of the boxed equation is $$\int\frac{\partial u}{\partial x}\color{blue}{dx}$$ But, shouldn't this really be $$\int\frac{\partial u}{\partial x}\color{blue}{\partial x}$$ since we are integrating a partial derivative? Therefore, Is this really the ' partial integral '? I write this as I see no justification in integrating a partial derivative when the integration differential is not partial. 2. The RHS has $g(y)$ as an unknown function of $y$. But why is this? Why not have an unknown function of $x$; $h(x)$ or an unknown function of $x$ & $y$; $q(x,y)$? Or lastly; since I know that only one of these $3$ possibilities can be correct why not write the RHS as $xy+g(y)+\text{constant}$ Proceeding anyway to equation $(2)$ and following the same recipe as before
I find that 
$$u(x,y)=\fbox{$\color{#180}{\int\frac{\partial u}{\partial y}dy=xy+y^2+r(x)}$}$$
Where $r(x)$ is an unknown function of $x$. Iff $u(x,y)$ is equal to both $\color{red}{xy+g(y)}$ and $\color{#180}{xy + y^2 + r(x)}$ we must match them up I guess and conclude that $g(y)=y^2$ and $r(x)=0$. Is this logic correct? If so we can write the parent function as $$u(x,y)=xy + y^2$$ The problem is that the correct answer is $$\color{#F80}{u(x,y)=xy + y^2+C}$$ where $C$ is a constant. But where on earth did the constant $C$ come from? I just matched up the two equations for $u(x,y)$ and found that there is no extra constant $C$. I know this is an awful lot of questions for one post, but if anyone is able to answer some or any of them it will be greatly appreciated. Just in case you were wondering; I got the idea for this question from another users' question on this site which can be found here and I have also read this similar post from the same user, but I'm still confused, sorry.","['intuition', 'partial-differential-equations', 'partial-derivative', 'multivariable-calculus', 'integration']"
2281321,"If $f$ and $g$ are continuously increasing functions, then prove that $(f∘g)$ is also increasing.","I am studying for my real analysis class, and one question involves two increasing functions $f$ and $g$. I have already proven that if $f$ and $g$ are increasing, then so is ($f+g$), and that the product ($fg$) is not necessarily increasing. The third part that I am working on asks about the composition ($f∘g$). My thoughts are that plugging one increasing function into another increasing function would mean their composition is also increasing. Here is what I have: Let $a<b$. It follows that $g(a) < g(b)$. Applying $f$ to both sides, we have $f(g(a))<f(g(b))$. Is this proof sufficient? I apologize if this question has been already answered as I imagine it has - but I could not find the proof. Thanks in advance!","['real-analysis', 'proof-writing', 'function-and-relation-composition', 'functions']"
2281328,Proofs involving a set of infinite dimensional vectors,"Given: A = $\{  \vec{v} = (v_{1}, ....v_{k}, ...) \in \mathbb{R}^{\infty} | \sum_{i=1}^\infty v_i^2 \text{converges} \}$ Prove that the set is a subspace of $\mathbb{R}^\infty$ and:
< $\vec{v}, \vec{u} > = \sum_{i=1}^\infty v_iu_i$ defines an inner product on A. I am really struggling with this problem, particularly showing closure under addition to show the set is a subspace, and proving positive definiteness of the inner product. I have been given the hint by a professor ""use the Cauchy-Schwarz inequality"" but I lack confidence in this advice, it is my understanding that a well defined inner product is a pre-condition for using Cauchy-Schwarz inequality. This is associated with an intro Linear Algebra take home exam, I have found a lot of information that leads me to believe this is regarding an $\mathscr{l}^2$ - Hilbert space, a concept that is not actually in my textbook. How to go about this proof? Ideally in a way that someone with 1 semester of linear algebra could understand.","['proof-writing', 'linear-algebra', 'hilbert-spaces']"
2281363,Is there a nonmeasurable set in R in which all the measurable subsets are countable?,"I think because there exist so many uncountable zero measure sets, it ought to be very easy to create a counterexample. However I failed to get an idea about this.",['measure-theory']
2281374,distance SO(3) rotation matrix,"According to M. Moakher's Means and averaging in the group of rotations and I. Sharf's Arithmetic and geometric solutions for average rigid-body rotation the distance between two rotation matrices is $$\| R_1 - R_2 \|_{\text{F}}$$ where $\| \cdot \|_{\text{F}}$ denotes the Frobenious norm. Does it mean $\left\| R_1 - R_2 \right\|_{\text{F}}$ or $\left\|  R_1^T R_2 \right\|_{\text{F}}$ ? $\left\| R_1 - R_2 \right\|_{\text{F}}$ does not make sense as $(R_1-R_2) \notin$ SO (3). $\left\|  R_1^T R_2 \right\|_{\text{F}}$ is also strange: suppose $R_1=R_2$ , the distance is $3$ . I find out that  the distance between two rotation matrices is less than $3$ . Such metric is against my intuition (the distance between two identical elements is largest!).","['matrices', 'orthogonal-matrices', 'lie-groups']"
2281378,Inverse Z transform of $\frac {-81}{z^4(z-3)(z-1)}$,"The question given like this: Find the convolution of $3^nu(-n+3)*u(n-2)$ Attempt: I tried to solve it through z transform method Let $Y(z)=X1(z)X2(z)$ Now $Z$ transform of $x1(n)=3^nu(-n+3)$ will be $\frac{-81z^{-3}}{z-3}$ Z transform of $u(n-2)=\frac{1}{z(z-1)}$ now $Y(z)=\frac {-81}{z^4(z-3)(z-1)}$ 
I tried to take inverse z transform of this function by residue theorem like this $Residue at z=3,will be \frac{-3^{n-1}}{2}$. $Residue at z=1,will be \frac{-81}{2}$ $Residue at z=0,will be -40$ BUT the solution given like this
$$
y(n)=
\begin{cases}
\frac{81}{2},n>=5\\
\frac{3^{n-1}}{2},n<5\\
\end{cases}
$$
Now What mistake i am doing,any other method other than Residue throem will be helpful.
Thanks","['signal-processing', 'computational-mathematics', 'z-transform', 'discrete-mathematics']"
2281381,Convergence of $ I=\int_0^\infty \sin x\sin(x^2)\mathrm{d}x$,"I am trying to prove that the improper integral
$$ I=\int_0^\infty \sin x\sin(x^2)\mathrm{d}x$$
converges. Here's my work: It suffices to show that
$$\int_\frac{\pi}{2}^\infty \sin (x) \sin(x^2)\mathrm{d}x$$
converges. Using integration by parts, \begin{align*}
\int_\frac{\pi}{2}^\infty \sin (t) \sin(t^2)\mathrm{d}t&=\int_\frac{\pi}{2}^\infty \frac{\sin (t)}{2t}\cdot 2t\sin(t^2)\mathrm{d}t\\
&=\underline{\bigg[ -\frac{\sin (t)}{2t}\cos(t^2)\bigg]_\frac{\pi}{2}^\infty}+{\int_\frac{\pi}{2}^\infty \left(\frac{\sin (t)}{2t}\right)'\cos(t^2)\mathrm{d}t}
\end{align*}
The underlined part is a constant... Then I got stuck. I'd like to use ""sandwich rule"" using the fact that $-1\leq \cos(t^2)\leq 1$, but I can't find a way to apply it properly. How can I proceed from here? Any correction and/or help would be appreciated. :)","['improper-integrals', 'integration', 'definite-integrals', 'calculus']"
2281437,Why does Rudin define $k = \frac{y^n-x}{n y^{n-1}}$ or $h < \frac{x - y^n}{n(y+1)^{n-1}}$ when he tries to prove that every real x has a nth root?,"tl;dr (Too Long To Read): What is the intuition /conceptual idea to why Rudin used the number: $$ k = \frac{y^n-x}{n y^{n-1}} $$ in his proof and not some other number? It seems that that number is not random, so how could he have come up with it? My attempt: I was trying to prove theorem 1.21 from Rudin's real analysis book on my own without looking at Rudin's proof. The way I attempted to prove it was to try to prove what seemed true from my already collected intuition on the real numbers before I started to study real analysis. So I drew lots of picture which lead me to think of defining $E = \{ \bar y \in R_{>0} : \bar y < x^{1/n} \}$: Thus, I knew that $E = \{ \bar{y} \in R_{>0} : \bar y^n < x \}$ is just the same as $E = \{ \bar y \in R_{>0} : \bar y < x^{1/n} \}$. So it was obvious that what I had to show was that the $\alpha = supE = y = x^{1/n}$ (also the reason for defining sets like that is cuz we probably need to use the Least Upper Bound (LUB) property cuz its one of the few things we are suppose to know about analysis so far). Thus, I proceed to show $E$ is bounded and non-empty so that I was guaranteed that the sup existed since we assumed the Least Upper Bound (LUB) property (a.k.a. the completeness axiom). Then I thought I want $y = \alpha$ so one option was to try showing $y < \alpha$ AND $y > \alpha$ are false, so by trichotomy $y=\alpha$. Since I didn't have direct access to $y$ I decided to take the same strategy except with $y^n = x$ instead of $y$ and $\alpha^n$ instead of $\alpha$. Intuitively I thought well lets assume $\alpha^n < x$ and $ x < \alpha^n $. The first one is too small so hopefully it should lead to some contradiction and perhaps show $\alpha < y$ is false. Similarly the other one $ x < \alpha^n $ should be too large somehow. Then maybe we can use trichotomy to get $\alpha^n = x$ which completes the proof. I attempted the first one $\alpha^n < x$. The only other thing I knew about $\alpha$ was that $\forall \bar y \in E, \bar y^n < x$. Then I decided to combine both equation (since I was hinted to use $a^n - b^n$ cuz I saw that identity in the soln when I check for my solution that E was bounded and non-empty): $$ \alpha^n - \bar y^n < x - \bar y ^n < 0$$ then because of the hint (that I probably wouldn't had realized I needed to use) I was extremely lucky and decided to subtract $\bar y^n$ from both sides (notice that if I would have decided to subtract by $\alpha^n$ it wouldn't have worked): $$  \alpha^n - \bar y^n = (\alpha - \bar y)\left(\sum_{0\leq i+j \leq n-1} \bar y^i \alpha^j \right) = (\alpha - \bar y)K < 0$$ where I noticed that $K > 0$ since every element of E is greater than zero and so is its least upper bound $\alpha$. Thus $K > 0$ and getting rid of it gets me: $$ (\alpha - \bar y) < 0 \implies \alpha < \bar y$$ which is obviously false since that would imply that $ \alpha$ is not an upper bound. Thus $\alpha^n < x$ is false. Now assume $x < \alpha^n $. One can't use the same argument as in my previous attempt because this time we are trying to create an element that is an upper bound smaller than $\alpha$ and its not clear how elements form $E$ are useful. Its clear to me we have to choose an $h$ such that: $$ x < (\alpha - h)^n < \alpha^n$$ I have given it a proper attempt (see at the end of my question) but I am unable to prove the desired result no matter how much I play with the algebra and the known facts I have. Thus, my question is how did Rudin come up with the following: $$k = \frac{y^n-x}{n y^{n-1}}$$ from his explanation it seems it just came out of a hat. I am sure if I plugged it in I would see that ""it works"" however, I wanted to know/see how to come up with it myself. Similarly I don't see how/why he came up with this one: $$h < \frac{x - y^n}{n(y+1)^{n-1}}$$ it seems that its not even required considering my first proof/argument, but I assume it must use the same idea considering it seems it used the same identity $b^n - a^n = (b-a)(b^{n-1}+b^{n-2}a+ \cdots + b a^{n-2} + a^{n-1})$. Anyone care to share what is the trick I missed? Is there some way to understand how one would have come up with using that? Is there some conceptual idea for the proof that he did not make explicit that I missed? I am hoping to get a more satisfying proof than just feeling I played around with symbols until I forced the paper to tell me the truth. It seems I missed some insights because even with the hints (like using the identity) didn't yield me a solution. What I tried: If we have: $$ x < \alpha^n$$ then at least intuitively, that must imply that there must be some element $y_{BAD}$ smaller than our supremum $\alpha$ that is still an upper bound (this intuition is because we are going under the assumption that $\alpha^n = x \iff \alpha = x^{1/n}$). Therefore it seems reasonable to try to decrease $\alpha$ the right amount $h$ such that: $$ x < (\alpha - h)^n < \alpha^n $$ then since $h$ is some distance that we go down from $\alpha$ we probably don't need to go down further than $\alpha$ so it seems reasonable to require $0 < h < \alpha$. With that we have using algebra: $$ x < (\alpha - h)^n = (\alpha - h)\left( \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} \right) = (\alpha - h) K < \alpha^n $$ the reason we did that factorization is so that we can hopefully get some inequality for $h$ (intuitively, think that we are trying to make $h$ the subject so that we can choose the right one to get the contradiction we need). Therefore lets try to remove all the nasty exponents with $h$ by assuming $ h < \alpha$ (otherwise $\alpha$ decreases by too much): $$ K = \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} < n \alpha^{n-1}$$ so lets plug it into: $$ x < (\alpha - h) K < \alpha^n $$ after plugging the inequality and leaving $h$ alone and some algebra I skipped I got: $$ \alpha - \frac{ \alpha^n }{K} < h < \alpha - \frac{x}{n \alpha^{n-1}}$$ unfortunately I don't I didn't manage to plug in $K < n \alpha^{n-1}$ successfully to both sides so I got stuck with the above (which is still in terms of $h^j, j>1$) which still unfortunately has high order terms for $h$...so close it feels... (note I've also tried more things but it would be ridiculous to put it all in here). so I feel got some of the main insights: Require the constraint $x < (\alpha - y)^n < \alpha^n$ using $x<\alpha^n$ and $h>0$. use $(\alpha - h)^n = (\alpha - h)\left( \alpha^{n-1}+\alpha^{n-2}h + \dots + \alpha h^{n-2} + h^{n-1} < n \alpha^{n-1} \right)$ to get $h$ alone. Use the upper bound on $K < n \alpha^{n-1}$ to remove the higher order terms of $h$ that are annoying (since we are assuming we don't know how to take roots). i.e. (\alpha - h)K < (\alpha - h)n \alpha^{n-1} those seem the main ingredients but when I tried putting them together it seemed I was still missing something because playing with the algebra didn't lead to the answer. Someone know what it is?","['real-analysis', 'alternative-proof', 'proof-verification', 'proof-writing', 'proof-explanation']"
2281474,(Verification) Zorn's Lemma is Equivalent to Hausdorff Maximal Principle,"Let $(X, \le)$ be a partially ordered set $X$. Claim Zorn's Lemma and Hausdorff Maximal Principle are equivalent. Zorn's Lemma Suppose $X$ has the property that every chain has an upper bound in $X$. Then the set $X$ contains at least one maximal element. Hausdorff Maximal Principle $X$ holds maximal chain. $1.\;$Zorn's Lemma $\rightarrow$ Hausdorff Maximal Princple Let $\Bbb C(X)$ be the family of every chain of $X$ and Let $\Bbb C$ be the chain of $\Bbb C(X)$ and Let $C= \cup\Bbb C$. Then for $a,b, \in C$ there $\exists C_1, C_2 \;\text{s.t}. a \in C_1 \in \Bbb C \;\text{and}\;  b \in C_2 \in \Bbb C$ But $C_1 \subset C_2 \;\text{or}\;C_2\subset C_1 $ since $C$ is chain. If  $C_1 \subset C_2 $,  $a,b \in C_2$. Then $a \le b \;\text{or}\; b \le a$ since $C_2$ is a chain of $X$ and $vice\;versa$ Thus $C$ is a chain of $X$ Now Hausdorff Maximal Principle holds since $\Bbb C \subset \Bbb C(X)$ has maximal chain $C$ $2.\;$ Hausdorff Maximal Princple $\rightarrow$ Zorn's Lemma Suppose every chain of $X$ has an upper bound. Then for the maximal chain of $X$,$\;C$,  let $m\in X$ be the upperbound of $C$. Now suppose $x \in X \;\text{and}\; x>m$ Then $C \cup \{x\}$ is also a chain since x is comparable with an element in $C$ But it contradicts to the fact that $C$ is maximal chain since $C \cup \{x\} \supsetneq C$ Thus m is a maximal element of $X$",['elementary-set-theory']
2281481,Normal approximation to a binomial probability,"I am studying for the final tomorrow and have been using a quiz as reference. A fair die is rolled 720 times. Let the random variable Y be the number of 6's obtained. Approximate the probabilities. Since n is such a high number we can treat it like a normal distribution using the Central Limit Theorem. In my professor's answer he changes the Y to X and add/sub a .5 depending on which ""tail"" we are trying to find. Ex. P(Y<98) -> P(X<98.5)
      P(Y>115) -> P(X > 114.5) What exactly is happening here?","['normal-distribution', 'statistics', 'central-limit-theorem', 'probability', 'approximation']"
2281492,Interesting convergence in divisor sums up to $10^k$,"Let $S(k)$ be the sum of divisors across each of $1, 2, ..., 10^k$. For example, $$\begin{align}S(1) &= 1 + 3 + 4 + 7 + 6 + 12 + 8 + 15 + 3 + 17 \\&= 87\end{align}$$ where each of $1 + 3 + ... + 17$ are the sums of divisors of $1, 2, ..., 10^1$ respectively. I noticed that as $k$ increases, the decimal expansion of $S(k)$ seems to ""converge"" to some number. For the first few values of $k$, computed using some suitable script, we have $$\begin{align}
S(1) &=87\\
S(2) &=8299\\
S(3) &= 823081\\
...\\
S(10) &= 82246703352400266400\\
...\\
S(15) &=822467033424114009326065894639\\
S(16) &=82246703342411333689227187822414\\
S(17) &= 8224670334241132270081671519064067\\
\end{align}$$ If we take the decimal expansion of $S(k)$ as a fractional part and let this be $S'(k)$, we have $$S'(\infty) = 0.82246703342411...$$ But hey! $$\frac{\pi^2}{12} = 0.82246703342411...$$ So my question is this : Is the limit indeed $$S'(\infty) = \frac{\pi^2}{12}$$?","['number-theory', 'divisor-sum']"
2281507,Proving oscillation of function on subset of domain,"Let $ \emptyset\ne A\subset \mathbb{R}^n$ and let $f:A\to \mathbb{R}$ be a bounded function. There is the following definition: For every non-empty subset $B$ of $A$, the oscillation of $f$ on $B$ is the number osc$(f)_B:=\sup(f)_B - \inf(f)_B$. I want to prove that osc$(f)=\sup\{|(f(x))-(f(y))|:x,y \in B\}.$ I'm not quite sure how to go about a proof, but here's my attempt. $\sup_B(f)-\inf_B(f)=\sup_B(f)+\sup_B(-f)=\sup\{f(x):x\in B\}+\sup\{-f(y):y\in B\}$
$=\sup\{f(x)+(-f(y)):x,y\in B\}$
$=\sup\{|f(x)-f(y)|:x,y\in B\}$ (the absolute value can be applied, since $\sup_B(f)\ge \inf_B(f)$). Do you think this is correct / rigorous enough?","['real-analysis', 'supremum-and-infimum', 'functions', 'proof-verification']"
2281510,Why do we replace y by x and then calculate y for calculating the inverse of a function?,"Why do we replace y by x and then calculate y for calculating the inverse of a function? So, my teacher said that in order to find the inverse of any function, we need to replace y by x and x by y and then calculate y. The reason being inverse takes y as input and produces x as output. My question is- Why do we have to calculate y after swapping? I do not get this part.","['algebra-precalculus', 'functions']"
2281559,How to prove the following in general: $\lim\limits_{x\rightarrow a} \frac{x^n-a^n}{x-a}=na^{n-1}$?,To prove $$\lim_{x\rightarrow a} \frac{x^n-a^n}{x-a}=na^{n-1}$$ The proof is easy when we take $n$ as positive integer and $a$ any positive real number. In my book it is given that the result is true even when $n$ is any rational number and $a$ any positive real number. But the proof is not given. Please provide some hint to construct the proof for general case.,['limits']
2281576,How do I prove if a sum of two specific irrational numbers is irrational?,"Prove that $\sqrt 2 + \sqrt 6$ is irrational. (Note that, in general, the sum of two irrational numbers could be rational.) I tried attempting to use proof by contradiction but I'm unsure of how to go from even there. I have no clue other than that. Please help.","['irrational-numbers', 'discrete-mathematics']"
2281590,Integral inequality with two increasing functions,"Let $f,g:[0,1]\rightarrow\mathbb{R}^+$ be increasing functions such that $f\leq g$. Is there a constant $c>0$ (independent of $f,g$) for which there exists some $r\geq 0$ (possibly dependent on $f,g$) such that
$$\int_{x:f(x)\leq r\leq g(x)}g(x)dx+\int_{f(x)\geq r}f(x)dx\geq c\int_0^1g(x)dx ?$$ As an example, let $g(x)=x$ and $f(x)=x^2$. The integral $\int_0^1g(x)dx$ is $\frac{1}{2}$. On the left-hand side, for fixed $r$, the first integral is from $x=r$ to $x=\sqrt{r}$ and amounts to $\frac{1}{2}(r-r^2)$. The second integral is from $x=\sqrt{r}$ to $x=1$ and amounts to $\frac{1}{3}(1-r\sqrt{r})$. Suppose $g(x)=x^t$ and $f(x)=x^s$ for $s\geq t\geq 1$. Then the right-hand side is $\frac{1}{t+1}$. The left-hand side is
$$\frac{r^{\frac{t+1}{s}}}{t+1}-\frac{r^{\frac{t+1}{t}}}{t+1}+\frac{1}{s+1}-\frac{r^{\frac{s+1}{s}}}{s+1}.$$","['real-analysis', 'integration']"
2281648,$f: \mathbb R^2 \to \mathbb R^2$ be continuous ; then is there a non-empty proper closed $A \subseteq \mathbb R^2$ s.t. $ A \subseteq f(A)$?,Let $f: \mathbb R^2 \to \mathbb R^2$ be a continuous function ; then is it true that there is a non-empty proper closed subset $A \subseteq \mathbb R^2$ such that $ A \subseteq f(A)$ ? I can show that if $f: \mathbb R^n \to \mathbb R^n$ is continuous then there is a non-empty proper closed subset $A \subseteq \mathbb R^n$ such that $ f(A) \subseteq A$ ; but I have no idea on what happens if we want a reverse inclusion . Please help . Thanks in advance,"['continuity', 'metric-spaces', 'analysis']"
2281660,Induction proof. Explain in detail why it’s incorrect [duplicate],"This question already has answers here : Flawed proof that all positive integers are equal (2 answers) Closed 7 years ago . Can somebody give a clear explanation why this is incorrect? thank you Theorem 1: All positive integers are equal. Proof: We show that any two positive integers are equal, from which the result follows. We do this by induction
on the maximum of the two numbers. Let $P(n)$ be the statement “if $r$ and $s$ are positive integers and $\max \{ r, s \} =
n$ then $r = s$.” Clearly $P(1)$ is true. Suppose that $P(n)$ is true and let $r$ and $s$ be positive integers whose maximum is $n + 1$. Then
$\max \{ r − 1, s − 1 \} = n$. By the inductive hypothesis, $r − 1 = s − 1$ and hence $r = s$. Thus $P(n + 1)$ is true. The result is now proved by mathematical induction.","['fake-proofs', 'induction', 'integers', 'logic', 'discrete-mathematics']"
2281721,Sum of singular values of a matrix,"Is there a ""trick"" to calculate the sum of singular values of a matrix $A$, without actually finding them? For example, the sum of the squared singular values is $\operatorname{trace}(A^TA)$.","['nuclear-norm', 'matrices', 'svd', 'singular-values', 'linear-algebra']"
2281839,Locally ringed spaces and varieties,"I'm trying to define varieties in term of locally ringed spaces. My main problem is to verify that a locally ringed space morphism when applied to varieties has already an intrinsic sheaf morphism, that is, the one given by composition. 
Let $(f,f^\natural)$ be a map of locally ringed spaces between the following varieties, $(X,o_X)\rightarrow (Y,o_Y)$. I want to prove that the sheaf morphism correlated is given by
$g\mapsto g\circ f$. I think that the proof will mainly rely on the fact that varieties are reduced, so that I can consider the sheaves as functions, but I really can't get out of it. Thank you for your attention. I hope it is comprehensible enough.","['schemes', 'affine-varieties', 'sheaf-theory', 'algebraic-geometry']"
2281857,"If $x = y^3$ in $\mathbb{Z} + \frac{-1 + \sqrt{-3}}{2}\mathbb{Z}$, then there is some $w \in \mathbb{Z} + \sqrt{-3}\mathbb{Z}$ such that $x = w^3$.","So the title contains the question I have to solve:
""If $x \in \mathbb{Z} + \frac{\bf{\color{red}{-}}1 + \sqrt{-3}}{2}\mathbb{Z}$ such that it can be written as the third power of something in $\mathbb{Z} + \frac{\bf{\color{red}{-}}1 + \sqrt{-3}}{2}\mathbb{Z}$, then it can be written as the third power of something in $\mathbb{Z} + \sqrt{-3}\mathbb{Z}$."" I was able to show that if $R$ is a UFD and $a,b,c \in R$ such that $ab = c^p$ with $p > 1$ some integer and if $g = \text{gcd}(a,b)$, then there exists $d, x \in R$ such that $d \mid g^{p-1}$ and $a = dx^p$. The follow up question was the above, but I have no idea how to start this question: I have tried brute force, i.e. say $x = a + b\phi$, $y = c + d\phi$ with $\phi = \frac{\bf{\color{red}{-}}1 + \sqrt{-3}}{2}$ and then I have tried to show that $d$ would need to be an even number. However, this approach was not really helpful. Moreover, I suspect that I would need the above, but I do not see how. (I know that $\mathbb{Z} + \phi\mathbb{Z}$ is a UFD, whereas $\mathbb{Z} + \sqrt{-3}\mathbb{Z}$ is not). any hints? Remark I have initially made a mistake in the sign of $1$ in the fraction, I edited this in red.","['number-theory', 'unique-factorization-domains', 'ring-theory']"
2281877,Cutting an arbitrary object into equal sized pieces,"Not homework, this problem occurred to me while chopping vegetables. Consider an arbitrary object in $\mathbb{R}^n$, is it possible to cut it into $2^n$ pieces of equal volume using $n$ orthogonal slices?  Suppose that the volume of the original object is $2^n$ so I want want each piece to have volume $1$, If the object has sufficient symmetry, e.g. a sphere or a cube, then it is easy.  I would like to consider arbitrary objects, my tomatoes are not sufficiently symmetric. Various reasonable conditions may be assumed.  Measurabuility is an obvious requirement.  Boundedness if necessary but if it would be nice if it wasn't.  Similarly, connectedness if necessary but again it would be nice if it wasn't; obviously the answer might not be unique for non-connected objects but that is acceptable. For $n = 1$, the problem is easy. For $n = 2$, possible but a little more care is needed.  First, we can slice it along a line with constant $x$ into two equal pieces.  Now do the same with a line with constant $y$.  We have 4 pieces but they are not necessarily all equal.  Diagonally opposite pieces must be equal but adjacent ones need not be.  However, I can now rotate my cuts and solve that. For $n = 3$, it gets harder.  I can start similarly with planes of constant $x$, $y$, and $z$ followed by rotations but I don't seem to have sufficient degrees of freedom to balance all 8 pieces. I have not attempted $n > 3$ yet, let's walk before I can run.  Anyway, I don't have any $4d$ tomatoes.",['measure-theory']
2281890,To find function satisfying given partial derivatives,"Let $F_1,F_2:\mathbb{R^2} \to  \mathbb{R}$ be the functions $ F_1(x_1,x_2)={\frac{-x_2}{x_1^2+x_2^2}}$ and $ F_2(x_1,x_2)={\frac{x_1}{x_1^2+x_2^2}}$. Which of the following is correct? ${\frac{\partial F_1}{\partial x_2}}$ =${\frac{\partial F_2}{\partial x_1}}$ There exists a function $f: \mathbb{R^2}\setminus\{(0,0)\}\to \mathbb{R}$ such that ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ There exists no function $f:\mathbb{R^2}\setminus\{(0,0)\}\to \mathbb{R}$ such that ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ There exists a function $f:D\to \mathbb{R}$ where $D$ is the open disc of radius $1$ centred at $(2,0)$, which satisfies  ${\frac{\partial f}{\partial x_1}}=F_1$ and ${\frac{\partial f}{\partial x_2}}=F_2$ on $D$. I got first option. 2 $\to$ suppose such function exist then ${\frac{\partial f}{\partial x_1}}=F_1$ gives $f=-\tan^{-1} \left(\frac{x_1}{x_2}\right) +V(x_2)$ where $V(x_2)$ is some function of $x_2$  and ${\frac{\partial f}{\partial x_2}}=F_2$ gives $V'(x_2)=0$ hence $V(x_2)=C$, a constant . hence such function $f=-\tan^{-1}\left(\frac{x_1}{x_2}\right) +C$ exists. 3 $\to$ not true how to conclude for 4 is my explanation for 3 is perfect?","['multivariable-calculus', 'real-analysis', 'partial-derivative']"
2281894,The Hardy space is a Hilbert space,"The Hardy space $H^2(\mathbb{D})$ is defined to be the space of all functions $f$ holomorphic on the unit disk $\mathbb{D}$ with the norm $\lVert \cdot \rVert_H$ $\lVert f \rVert_H^2=\sup_{0<r<1}\int_0^{2\pi}|f(re^{i\theta})|^2 d\theta$ is finite. Show that $H^2(\mathbb{D})$ is a Hilbert space. I have shown that if $f(z)=\sum_n c_nz^n$ , then $\lVert f \rVert_H^2=2\pi \sum_n|c_n|^2$ . How does this imply $H^2(\mathbb{D})$ is a Hilbert space? What is the inner product induced by the norm?","['functional-analysis', 'hilbert-spaces']"
2281932,Do Peano axioms uniquely characterize the natural numbers?,"If Peano axioms uniquely determine the natural numbers, doesn't this mean that Peano axioms are categorical and hence complete? If above is true, how is it explained by Goedel's incompleteness theorem?",['number-theory']
2281940,Splitting infinite sets into disjoint sets of equal cardinality,"When given an infinite set $X$, it seems to me very reasonable that one can 'split' it up into two disjoint subsets $A$ and $B$ such that all three have the same cardinality. For countable sets, this is rather easy, for then we can index the elements in $X$ by $(x_n)_{n \in \mathbb{N}}$, and we can take $A$ as the elements indexed by even $n$, and $B$ those by odd ones. Of course, countability isn't needed per se; with a set indexed by $\mathbb{R}$, it can be split up rather easily as well. Where I run into difficulty is a general infinite set, where this indexing trick doesn't seem to work, because the indexing set isn't 'known' well enough (in the sense that I can't make an easy choice, as with $\mathbb{N}$ or $\mathbb{R}$). My question is if this is possible for any infinite set $X$.",['elementary-set-theory']
