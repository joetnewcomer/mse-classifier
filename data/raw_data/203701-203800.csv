question_id,title,body,tags
4025955,"Ordered ""sums"" of natural number k using natural number addends no smaller than 2","Using natural numbers no smaller than 2, we can express the number 3 as one ordered ""sum"": 3, and we can express the number 6 as 5 ordered ""sums"": 6 = 4 + 2 = 3 + 3 = 2 + 4 = 2 + 2 + 2. In how many ways can the number k be expressed as an ordered ""sum"" with natural number addends no smaller than 2?",['discrete-mathematics']
4025957,Infinite set notation,"Is it acceptable to write "" $X$ is an infinite set"" as "" $|X| = \infty$ ""? By ""acceptable"" I mean that I can use it in a research paper or in a textbook, and a reasonable person won't be against it. Possible arguments against it: $\infty$ is not actually a value, so it doesn't fit into a standard set-size notation. $|\mathbb R| = \infty$ and $|\mathbb N| = \infty$ , so one may think that $|\mathbb R| = |\mathbb N|$ . (I looked for duplicates with some variations of the title, and found none)","['elementary-set-theory', 'notation']"
4025983,Finite-Dimensional Homogeneous Contractible Spaces,"Suppose that $X \subset \mathbb{R}^n$ is compact, homogeneous and contractible (and thus connected).  Does $X$ have to be a point? I couldn't think of a non-trivial example, and there isn't a counterexample in the plane.  The homogeneous planar continua have been classified (point, circle, pseudo-arc, circle of pseudo-arcs) and the only contractible one is a point.  Maybe there is some twisty sort of example in three or four dimensions, though? Does it become true if $\dim(X) = n $ and is embeddable in $\mathbb{R}^{n+1}$ ? By homogeneous I mean for any $x, y \in X$ there is a homeomorphism $f$ of $X$ with $f(x) = y$ .  By contractible I mean the identity map on $X$ is homotopic to a constant map.  For example the circle is homogeneous but not contractible, and the closed disc is contractible but not homogeneous.","['continuum-theory', 'examples-counterexamples', 'geometric-topology', 'general-topology', 'algebraic-topology']"
4026002,best way to show tr(AB) = tr(BA) for non square A and b matrices,"I have matrices $A \in \mathbb{K}^{n \times m}$ and $B \in \mathbb{K}^{m \times n}$ What is the best way to prove that tr(AB) = tr(BA). I found a prove in Matrix Analysis by Horn and Johnson, but they only prove it if $n \leq m$ .","['matrices', 'trace', 'linear-algebra', 'eigenvalues-eigenvectors']"
4026028,"An explicit construction of the ""Veronese embedding"".","I am reading the text ""An invitation to Algebraic geometry"" by Karen Smith et. al. In the text there is a proposition on Veronese mappings: Prop: The Veronese mapping $v_d$ defines an isomorphism of $\mathbb{P}^n$ onto its image. In the proof, it is explicitly shown how $\mathbb{P}^n \rightarrow v_d(\mathbb{P}^n) \rightarrow \mathbb{P}^n$ is an identity map, but the other part of the proof is left for the reader. I am struggling to prove that $v_d(\mathbb{P}^n) \rightarrow \mathbb{P}^n \rightarrow v_d(\mathbb{P}^n)$ is an identity map too. Could someone please help me with this? I am a beginner at Algebraic Geometry, so a detailed explanation would be appreciated. Thanks in advance!","['algebraic-geometry', 'projective-geometry', 'projective-varieties']"
4026029,"What manifolds $M$ have a $CW-$structure so that the $n-$skeleton, $M_n$, is a manifold for all $n$ aswell?","If you have a $CW-$ structure on a connected manifold $M$ we obtain a filtration $M_n$ of $M$ where $M_n$ is the $n-$ skeleton. If in addition we have that the $M_n$ are also all manifolds (like with the standard $CW-$ decomposition of $\mathbb RP^n$ or $\mathbb CP^n$ ) what can we say about the manifold $M$ ? Not all manifolds $M$ have such a CW structure. Like $S^1 \times S^1 = M$ for example, $M_1$ has to be either a point or a circle since it can't be a wedge of circles since that's not a manifold, it also has to be connected since if $M_1$ is disconnected $M_2$ will always be disconnected but that's impossible since $M_2$ is supposed to be $S^1 \times S^1$ , and if we denote by $m_i$ the number of $i-$ cells in $M$ we have that $\text{rank } H_i(M) \leq m_i$ (by a $CW-$ homology argument) which is not true for $i = 1$ in $S^1 \times S^1$ since $H_1(S^1 \times S^1) = \mathbb Z^2$ and $m_1 = 1$ or $0$ . How far can we go with this? Is there some easy way to see through invariants like homology or cohomology wether such a filtration $M_n$ exists or not? All of the spaces $S^n, \mathbb RP^n, \mathbb CP^n, \mathbb HP^n$ are examples of such manifolds. Don't know of any else from the top of my head.","['cw-complexes', 'manifolds', 'general-topology', 'algebraic-topology', 'differential-geometry']"
4026034,Probabilistic Inequalities with Lower Bounds,"Many inequalities (Markov, Chevyshev, Chernoff) give some sort of upper bound to $P(X \geq a)$ for some random variable $X$ . I am trying to find some results that give a lower bound to this probability. I have found this result, but unfortunately this only holds for $X \geq 0$ . Any there any known results that hold more generally? If it makes any difference, the specific case I'm trying to apply this to has $X$ symmetric and $E(X^n) < \infty$ for all $n \in \mathbb{N}$ , but I would be happy to explore any results on this topic.","['statistics', 'probability-theory', 'probability']"
4026064,How to find $F=\left(\frac{B}{2}\right)^{2H}-1$ when $H$ is maximum of $W_{1}$ and $B$ is minimum of $W_{2}$?,"The problem is as follows: A pair of wavelenghts are $W_{1}$ and $W_{2}$ : $W_{1}=5\sin\alpha-5+12\cos\alpha$ $W_{2}=3\sin\beta+\sqrt{3}\cos\beta$ Assuming $H$ and $B$ represent the maximum and the minimum value for $W_{1}$ and $W_{2}$ respectively. Find: $F=\left(\frac{B}{2}\right)^{2H}-1$ The alternatives given in my book are as follows: $\begin{array}{ll}
1.&\textrm{6560}\\
2.&\textrm{5860}\\
3.&\textrm{6562}\\
4.&\textrm{5680}\\
\end{array}$ I'm confused exactly how to tackle this problem from my precalculus workbook. What I attempted to do was to use the boundaries of the sine and the cosine function individually to see if I could obtain the maximum and the minimum values: $-1<\sin\alpha<1$ $-10<5\sin\alpha-5<0$ Then for $\cos\alpha$ : $-1<\cos\alpha<1$ $-12<12\cos\alpha<12$ Then adding both: $-22<5\sin\alpha-5+12\cos\alpha<12$ Thus the maximum value for the earlier function would be $12$ . Whereas for the other would be: $-1<\sin\beta<1$ $-3<3\sin\beta<3$ and for the other term: $-\sqrt{3}<\sqrt{3}\cos\beta<\sqrt{3}$ Then when adding both: $-3-\sqrt{3}<3\sin\beta+\sqrt{3}\cos\beta<3+\sqrt{3}$ Then the minimum value would be for the function would be $-3-\sqrt{3}$ . Hence: $B=12$ and $H=-3-\sqrt{3}$ But the problemas has not ended yet, they are asking the result of $F$ Thus: $F=\left(\frac{-3-\sqrt{3}}{2}\right)^{2(2)}-1$ Then this is where I got stuck. What could possibly be wrong here?. Which part went wrong?. Can someone help me here?. Could it be that the problem lies in the method to get the minimum and the maximum value for those functions?. The sort of approach which I'm looking to get is derivative-free in other words without using derivatives as this problem was intended to be solved relying only using precalculus. Can someone help me please?. I'd appreciate an answer with an explanation step-by-step. Thanks in advance.","['maxima-minima', 'algebra-precalculus', 'trigonometry']"
4026128,How do I find the probability of getting an average when selecting from a normal distribution?,"If I have that scores follow a normal distribution, I know that the probability of selecting any person at random and them getting a score between 1 and 2 sd's above the mean is 13.6%. If I randomly select 4 people what is the probability that their scores will average to a value between 1 and 2 sd's above the mean? I dont care where any of their individual scores are only that they average to a value between 1 and 2 sds above the mean.","['statistics', 'normal-distribution', 'probability']"
4026154,In a von Neumann regular commutative ring with unity every finitely generated ideal is principal,"Let $R$ be a commutative, von Neumann regular ring with unity. How to show that every finitely generated ideal in $R$ is principal? I can see, in view of mathematical induction, it suffices to show that any ideal generated by two elements of $R$ must be principal. Let $I=(a,b)$ be an ideal of $R.$ Since $I$ is commutative with unity, $I=\{xa+yb:x,y\in R\}.$ Also since $R$ is regular there exist $r,s\in R$ such that $a=ara=ra^2$ and $b=bsb=sb^2.$ However I cannot figureout which element would generate $I.$ Please help.","['ring-theory', 'abstract-algebra']"
4026185,Number of combinations with small intersection,"I have set $S$ of $n$ elements. I want to understand, in how many ways can I choose $n/2$ elements - let each such choice be set $S_i$ - such that no two choices have more than $n/4$ elements in common i.e $|S_i\cap S_j|\leq n/4$ .
I have been thinking about it and I think that it should be (polynomial) function in $n$ - for $n=4$ , it is $6$ , for $n=8$ , I could write $10$ ways, but I don't have an expression for general growth. Any approach on how to proceed would appreciated.","['combinatorial-designs', 'coding-theory', 'combinatorics']"
4026187,Finding area based on only the sides,"I came across this simple looking question: The perimeter of a triangle is $42$ cm. One side of a triangle is $8$ cm longer than the smallest side and the third side is $1$ cm less than $3$ times the smallest side. Find the area of the triangle. At first it seemed easy enough. The smallest side is $7$ (which can be derived from: $\frac{42-8+1}{5}$ , where subtracting $8$ takes care of the long side and now we have $2$ units of the shortest side, then adding one takes care of the third side now we have $5$ units of the third side and dividing by $5$ we can get the length of the smallest side, $7$ ) and with that knowledge the length of the first and third side is $15$ and $20$ respectively. That's when I got stuck. The question gives us no indication of the angles in the triangle. I thought we could use the Pythagorean theorem to see if the triangle is right-angled, but it's not. Is there a rule we can use to find the area? Yes, I'm familiar with trigonometry and $sin()$ , $cos()$ and $tan()$ so you can use those in your answer. If you don't mind can you give me the rule to finding the area, not the answer to the problem? Because I want to solve this question myself. Many thanks to whoever answer's this!","['triangles', 'trigonometry', 'area', 'geometry']"
4026190,Exercise III.5.10 in Grillet's Abstract Algebra,"Let $R$ be commutative, with characteristic either $0$ or greater than $m$ . Show that a root $r$ of $A \in R[X]$ has multiplicity $m$ if and only if $A^{(k)}(r)=0$ for all $k<m$ and $A^{m}(r) \neq 0$ . Show that the hypothesis about the characteristic of $R$ cannot be omitted from this result. By definition of multiplicity $A(x)=(x-r)^{m}B(x)$ , where $B(r) \neq 0$ . It's easy to see, that $A^{k}(r)=0$ for all $k<m$ and $A^{m}(r)=m!B(r)$ . But why $m!B(r) \neq 0$ ? Shouldn't $R$ necessarily be a domain?","['abstract-algebra', 'polynomial-rings']"
4026207,"Proof verification: $\iint_{R}dA=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|dA$ using Green's theorem.","In James Stewart's Calculus: Early Transcendentals ( $8$ e), problem $31$ of Section $16.4$ asks us to prove the change of variables formula $$\iint_{R}dx\text{ }dy=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|du\text{ }dv$$ using Green's theorem. Here, $R$ is the image of $S$ under the transformation $T$ defined by the equations $x=g(u,v)$ , $y=h(u,v)$ . Here's my reasoning: Notice that $\iint_{R}dA$ is the area of the region $R$ . It can be shown that Green's theorem implies that this area is given by the line integral $\int_{\partial R}x\text{ }dy$ . Thus, if $\partial R$ can be parameterized by $\textbf{r}(t)=\left<x(t),y(t)\right>$ ( $a\leq t\leq b$ ), we can write \begin{align*}
\iint_{R}dA &= \int_{\partial R}x\text{ }dy\\
&= \int_{a}^{b}x(t)y'(t)\text{ }dt
\end{align*} It was assumed that the image of $S$ under $T$ is $R$ , so if $T$ preserves orientation, it must be the case that $T$ maps $\partial S$ to $\partial R$ . It follows that \begin{align*}
\left<x(t),y(t)\right> &= \textbf{r}(t)\\
&= T\left(\textbf{r}_{S}(t)\right)\\
&= \left<g(\textbf{r}_{S}(t)),h(\textbf{r}_{S}(t))\right>\\
&= \left<g(u(t),v(t)),h(u(t),v(t))\right>
\end{align*} for all $t\in[a,b]$ , where $\textbf{r}_{S}(t)=\left<u(t),v(t)\right>$ parameterizes $\partial S$ (the parameter domain doesn't change because $\partial S$ is a closed curve, and thus must return to $\textbf{r}_{S}(a)$ once $t$ reaches $b$ ). We deduce that \begin{align*}
x(t) &= g(u(t),v(t))\\
y(t) &= h(u(t),v(t))\\
y'(t) &= h_{u}(u(t),v(t))u'(t)+h_{v}(u(t),v(t))v'(t)
\end{align*} and consequently \begin{align*}
\int_{a}^{b}x(t)y'(t)\text{ }dt &= \int_{a}^{b}g(u(t),v(t))\left[h_{u}(u(t),v(t))u'(t)+h_{v}(u(t),v(t))v'(t)\right]dt\\
&= \int_{a}^{b}[g(u(t),v(t))h_{u}(u(t),v(t))u'(t)\\
& +g(u(t),v(t))h_{v}(u(t),v(t))v'(t)]dt\\
&= \int_{a}^{b}g(u(t),v(t))h_{u}(u(t),v(t))u'(t)\text{ }dt\\
& +\int_{a}^{b}g(u(t),v(t))h_{v}(u(t),v(t))v'(t)\text{ }dt\\
&= \int_{\partial S}g(u,v)h_{u}(u,v)du+g(u,v)h_{v}(u,v)dv
\end{align*} Applying Green's theorem gives \begin{align*}
\int_{\partial S}g(u,v)h_{u}(u,v)du+g(u,v)h_{v}(u,v)dv &= \iint_{S}\left(\frac{\partial}{\partial u}\left(g(u,v)h_{v}(u,v)\right)-\frac{\partial}{\partial v}\left(g(u,v)h_{u}(u,v)\right)\right)dA\\
&= \iint_{S}(g_{u}(u,v)h_{v}(u,v)+g(u,v)h_{vu}(u,v)\\
& -g_{v}(u,v)h_{u}(u,v)-g(u,v)h_{uv}(u,v))\text{ }dA\\
&= \iint_{S}\left(g_{u}(u,v)h_{v}(u,v)-g_{v}(u,v)h_{u}(u,v)\right)dA
\end{align*} The last expression is precisely $\left|\frac{\partial (x,y)}{\partial (u,v)}\right|$ (the orientation has not changed), so we may finally write $$\iint_{R}dA=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|dA$$ I appreciate any and all feedback. If you identify any mistakes, I kindly ask that you only give me hints so I can correct them myself. Extra : with a few tweaks, this argument can be used to prove the more general result $$\iint_{R}f(x,y)\text{ }dA=\iint_{S}f(g(u,v),h(u,v))\left|\frac{\partial (x,y)}{\partial (u,v)}\right|dA$$","['greens-theorem', 'multivariable-calculus', 'solution-verification', 'line-integrals']"
4026213,The variation of a Ukrainian Olympiad problem: 10982,"Given a recursion $a_{n+ 1}= \dfrac{a_{n}}{n}+ \dfrac{n}{a_{n}}$ with $a_{1}= 1.$ Prove that $$\lim a_{n}^{2}- n= \frac{1}{2}$$ Source: StachMath/@RiverLi _ The limit and asymptotic analysis of $a_n^2 - n$ from $a_{n+1} = \frac{a_n}{n} + \frac{n}{a_n}$ The original problem already has an answer, I'll suggest this way of thinking, which is not mine, but @twelve_sakuya Let $b_{n}:=a_{n}^{2}- n,$ so $$a_{n+ 1}= \frac{a_{n}}{n}+ \frac{n}{a_{n}}\Leftrightarrow b_{n+ 1}- \frac{1}{2}= -\frac{n}{b_{n}+ n}\left ( b_{n}- \frac{1}{2} \right )+ \frac{b_{n}}{2\left ( b_{n}+ n \right )}+ \frac{b_{n}}{n^{2}}+ \frac{1}{n}$$ That means $\left | b_{n+ 1}- \dfrac{1}{2} \right |\leq\dfrac{n}{\left | b_{n}+ n \right |}\left | b_{n}- \dfrac{1}{2} \right |+ \dfrac{\left | b_{n} \right |}{2\left | b_{n}+ n \right |}+ \dfrac{\left | b_{n} \right |}{n^{2}}+ \dfrac{1}{n}.$ Therefore, if we can get the evaluations of $\left | b_{n} \right |$ or $\dfrac{\left | b_{n} \right |}{n},$ maybe there exists a number $\beta\in\left ( 0, 1 \right )$ so that $$\left | b_{n}- \frac{1}{2} \right |\leq\beta^{n}B\left ( n \right )\rightarrow 0\,{\rm as}\,n\rightarrow\infty$$ My friend has no confidence to continue, he also said that is the variation of a Ukrainian Olympiad problem #10982 (I searched and got the result, but I couldn't access it). I need to the help, thanks a real lot !","['laurent-series', 'recurrence-relations', 'asymptotics', 'recursion', 'limits']"
4026217,Determine whether the given points are interior point of the given set,"Let $B = \{(x,y) \in \Bbb{R}^2 \mid -1 \le x \lt 2, 0 \lt y \le 2 \} 
\cup \{(x,y) \in \Bbb{R}^2 \mid 5 \lt x \le 7, y = 1 \}$ . Determine whether a point $(0,1)$ is an interior point of $B$ . I got a little bit confuse here, since a point $(0,1)$ exactly lies on vertical line $y=1$ , in which it does mean that $(0,1)$ be an interior point of not. Here's my attempt: Fix $0 < r = \frac{1}{2}$ . Note that \begin{equation*}
B((0,1), \frac{1}{2}) = \{(x,y) \in \Bbb{R} \mid x^2 + (y-1)^2 < \frac{1}{4} \}.
\end{equation*} Let $(x,y) \in B((0,1), \frac{1}{4})$ . Then, \begin{align*}
x^2 + (y-1)^2 < \frac{1}{4} \\
x^2 < \frac{1}{4} \wedge (y-1)^2 < \frac{1}{4} \\
-\frac{1}{2} < x < \frac{1}{2} \wedge -\frac{1}{2} < y-1 < \frac{1}{2} \\
-\frac{1}{2} < x < \frac{1}{2} \wedge \frac{1}{2} < y < \frac{3}{2}.
\end{align*} Hence, $x \in (-\frac{1}{2}, \frac{1}{2}) \subseteq [-1,2)$ and $y \in (\frac{1}{2}, \frac{3}{2})
\subseteq (0,2]$ . Thus, forall $(x,y) \in B((0,1), \frac{1}{2})$ , we have $(x,y) \in B$ .
In another words, $B((0,1), \frac{1}{2}) \subseteq B$ . Therefore, there exists $r>0$ such that $B((0,1), \frac{1}{2}) \subseteq B$ . Hence, $(0,1)$ is an interior point of $B$ . Am I true? If not, any explain how to show it?
Thanks in advanced.",['general-topology']
4026220,compute line integral using Greens theorem,"The question is: $$\int_\gamma \frac{(x^2+y^2-2)\ dx+(4y-x^2-y^2-2)\ dy}{x^2+y^2-2x-2y+2}$$ $$\gamma:y=2\sin\frac{\pi x}{2} \quad \text{from}\  (2,0)\ \text{to}\ (0,0)$$ Here how i have tried to solve it:
I thought that due to the singularity at $(1,1)$ it would be best to use Greens theorem with one circle around the singularity call it $\sigma$ and the line segment between $(0,0)$ to $(2,0)$ and call it $\varphi$ and then $\int_\gamma=-\int_\sigma-\int_\varphi$ but the problem is that the first integral (circle around the point $(1,1)$ ) is very complicated with $x=1+Rcost$ and $y=1+Rsint$ . Do any of you guys have any suggestion or am i on the right track at all?","['integration', 'greens-theorem', 'multivariable-calculus', 'line-integrals']"
4026236,Understanding algorithm to show $(x+\partial_x)e^{-x^2/2}=0$ using python?,"I am given the following Python code which is supposed to verify numerically that $$(\partial_x + x)e^{-x^2/2}=0.$$ The algorithm does this, by transforming everything into a Fourier basis and then verifying it using the Fourier transform. However, I have some difficulties understanding the mathematical basis of this algorithm. In short, my question is: When the final result is computed as np.dot( D, yl ) , what is that actually in mathematical terms that we are computing here? import numpy as np

 ## non-normalized gaussian with sigma=1
 def gauss( x ):
return np.exp( -x**2 / 2 )

  ## interval on which the gaussian is evaluated
 L = 10
 ## number of sampling points
 N = 21
 ## sample rate
 dl = L / N
 ## highest frequency detectable
 kmax= 1 / ( 2 * dl )

 ## array of x values
 xl = np.linspace( -L/2, L/2, N )
 ## array of k values
 kl = np.linspace( -kmax, kmax, N )

 ## matrix of exponents
## the Fourier transform is defined via sum f * exp( -2 pi j k x)
  ## i.e. the 2 pi is in the exponent
 ## normalization is sqrt(N) where n is the number of sampling points
 ## this definition makes it forward-backward symmetric
 ## ""outer"" also exists in Matlab and basically does the same
exponent = np.outer( -1j * 2 * np.pi * kl, xl ) 
## linear operator for the standard Fourier transformation
 A = np.exp( exponent ) / np.sqrt( N )

 ## nth derivative is given via partial integration as  ( 2 pi j k)^n f(k)
 ## every row needs to be multiplied by the according k
 B = np.array( [ 1j * 2 * np.pi * kk * An for kk, An in zip( kl, A ) ] )

 ## for the part with the linear term, every column needs to be multiplied
## by the according x or--as here---every row is multiplied element 
 ## wise with the x-vector
C = np.array( [ xl * An for An in  A ] )

 ## thats the according linear operator
 D = B + C

 ## the gaussian
 yl = gauss( xl )

 ## the transformation with the linear operator
 print(  np.dot( D, yl ).round( decimals=9 ) ) 
## ...results in a zero-vector, as expected","['numerical-methods', 'gaussian', 'matlab', 'ordinary-differential-equations']"
4026245,Number of possible outcomes when rolling indistinguishable dice?,"So I know the answer is $$N(n) = \frac{( n + 5 )!}{ n! \cdot 5! }$$ where $n$ is the number of dice. but I'm not able to prove it either mathematically or intuitively,
although I'm able to verify for various cases. I tried to prove it by first assuming the dice are different, and then eliminating duplicates but that method doesn't look very promising. How do I prove the above result.","['dice', 'probability-theory', 'probability']"
4026262,Some problems on locally finite sets,"This is a series of problems on locally finite sets with a few I am stuck on. Can the community provide some help on how to proceed? Definition. Let $(X,\tau)$ be topological space. A set $\mathcal S$ of subsets of $X$ is said to be locally finite in $(X,\tau)$ if each point $x \in X$ has a neighborhood $N_x$ such that $N_x \cap S= \varnothing$ for all but a finite number of $S \in \mathcal S$ . Prove the following statements: $(\rm i)$ If the set $\mathcal S$ of subsets of $X$ is finite, then $\mathcal S$ is locally finite. This is trivial. $(\rm ii)$ If the set $\mathcal S$ is such that every point of $X$ lies in at most one $S \in \mathcal S$ , then $\mathcal S$ is locally finite. I am not sure if this statement is true. Let us assume that $\mathcal S$ is infinite, otherwise, from part $(\rm i)$ , it is locally finite. Now $X$ must also contain an infinite number of elements. If $T$ is the indiscrete topology, where the only open sets are the whole set and the empty set, then even if every point of $X$ lies in one $S$ , $\mathcal S$ is not locally finite because the whole space is the only open neighborhood of each point, and its intersection with any $S$ is non-empty because they are all part of the whole space, so I think $(\rm ii)$ is not a true statement. $(\rm iii)$ Let $X$ be an infinite set and $\tau$ the finite-closed topology on $X$ . If $\mathcal S$ is the set of all open sets in $(X,\tau)$ , then $\mathcal S$ is not locally finite. Every open set in the cofinite topology has a non empty intersection with every other open set so $\mathcal S$ cannot be locally finite. $(\rm iv)$ Let $\mathcal S$ be a locally finite. Define $\mathcal T$ to be the set of all closed sets $T=\operatorname{cl}(S)$ for $S \in \mathcal S$ . Then $\mathcal T$ is locally finite. So, this is saying that the closure of each $S \in \mathcal S$ is still locally finite. Is this a proof by contradiction or is a direct proof possible? If a open neighborhood of a point has an empty intersection with all but finite number of $S$ , is there a reason to believe this would change if we took the closure of all $S$ ? $(\rm v)$ If $\mathcal S$ is an infinite set of subsets of a infinite set $X$ , and $(X,\tau)$ is a compact space, then $\mathcal S$ is not locally finite. Suppose $\mathcal S$ is locally finite. Then for each $x \in X$ , there exist a $U_i$ such that $U_i$ has an empty intersection with all but finite number of $S \in \mathcal S$ . The set $U_i$ covers $X$ , so there exist a finite subcover. How can I show a contradiction from here? $(\rm vi)$ Let $\mathcal S$ be an uncountable set of subsets of $X$ . If $\mathcal S$ is a cover of the space $(X,\tau)$ and $(X,\tau)$ is either a Lindelöf space or a second countable space, then $\mathcal S$ is not locally finite. Let us assume $X$ is second countable first and also assume $\mathcal S$ is locally finite. Then for each $x \in X$ , there exist a $U_i$ such that $U_i$ has an empty intersection with all but finite number of $S \in \mathcal S$ . How should I proceed to show a contradiction?",['general-topology']
4026269,Notation for a function definition,"I came across the following notation in a research paper Suppose we have a function $f(x) \in [1,l] \rightarrow \mathbb{R}$ It is the first time, I am looking such a notation. And the paper is using the same notation for all functions used in it. Is it same as $f(x) : [1,l] \rightarrow \mathbb{R}$ If yes, is it an abuse of notation? If no, what does the notation mean?","['notation', 'functions']"
4026279,$\lim_{n\rightarrow\infty}\sum_{k=1}^{n}{2n\choose k}\frac{1}{4^{n}}$ and $\lim_{n\rightarrow\infty}\sum_{k=1}^{2n}{2n\choose k}\frac{1}{4^{n}}$ is?,$\lim_{n\rightarrow\infty}\sum_{k=1}^{n}{2n\choose k}\frac{1}{4^{n}}=\lim_{n\rightarrow\infty}(1+\frac{1}{4^n})^{2n}$ using $(1+x)^n=1+nx+\frac{n(n-1)x^2}{2!}....$ $\lim_{n\rightarrow\infty}=1+\frac{2n}{4^n}+\frac{2n(2n-1)}{4^{2n}2!}....$ $\lim_{n\rightarrow\infty}=\frac{2n}{4^n}=\frac{\infty}{\infty}=\frac{2}{4^n\ln4}$ all terms vanish we are left with $1$ only. Am I correct that limit is $1$ ?,['limits']
4026381,Algebraic inequality $\sum \frac{x^3}{(x+y)(x+z)(x+t)}\geq \frac{1}{2}$,"The inequality is $$\frac{x^3}{(x+y)(x+z)(x+t)}+\frac{y^3}{(y+x)(y+z)(y+t)}+\frac{z^3}{(z+x)(z+y)(z+t)}+\frac{t^3}{(t+x)(t+y)(t+z)}\geq \frac{1}{2},$$ for $x,y,z,t>0$ . It originates from a 3-D geometry problem involving volumes of tetrahedra etc. Actually, it is equivalent with that problem (see Let ABCD be a tetrahedron of volume 1 and M,N,P,Q,R,S on AB,BC,CD,DA,AC,BD s.t. MP,NQ,RS are concurrent. Then the volume of MNRSPQ is less than 1/2. ). The three variables simpler case $$\frac{x^2}{(x+y)(x+z)}+\frac{y^2}{(y+x)(y+z)}+\frac{z^2}{(z+x)(z+y)}\geq \frac{3}{4}$$ can be proved using the Cauchy-Schwarz inequality in ""Engel's form"". I have tried variants of Holder type inequalities, until now unsuccessfully.","['summation', 'inequality', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'holder-inequality']"
4026442,Analysis question from a past exam paper,"Assume that $f \in C^1 [a,b].$ Prove that $$\forall \epsilon \  \exists \delta > 0 : \\
x, y \in [a,b] , |x-y| < \delta \implies \left\vert\frac{f(x)-f(y)}{x-y} - f'(x) \right\vert < \epsilon $$ I did he following proof and I wonder if its entirely correct as the question itself would have carried 10 % of a 2hr exam. Edit: edited the proof to make $\delta$ independent of $x$ Let $x,y \in [a,b]$ Since $f'$ is continuous on a compact set, then its uniformly continuous.
Hence $\forall x,y \in [a,b], |x-y| < \delta \implies |f'(y)-f'(x)| < \epsilon $ By MVT, since $f \in C^1 [a,b], \ \exists c \in (x,y) : f'(c) = \frac{f(x)-f(y)}{x-y}$ . Thus in particular, $|x-c|<|x-y| < \delta \implies |f'(c)-f'(x)| < \epsilon $ and we're done.","['derivatives', 'real-analysis']"
4026486,Reference for Multivariate Taylors theorem on closed domains,"I'm currently looking for a source (preferably a book) that provides a proof of the multivariate Taylors Theorem in the setting that a function $f$ is defined on a closed domain $D$ and is $m$ -times continuously differentiable in the interior of the domain such that all partial derivatives extend continuously to $D$ . The univariate case can be found here , but I am particularly interested in a quality reference (book) of the multivariate case.","['derivatives', 'taylor-expansion', 'reference-request']"
4026529,"What's the difference between an ergodic measure, ergodic sequence or ergodic mapping?","So I work a lot with ergodic theorems but it is a bit confusing.
We have a probability space $(\Omega,\mathcal{F},P\}$ and a measure preserving mapping $T:\Omega\rightarrow\Omega$ .
We say that $T$ is ergodic (or $P$ -ergodic) if any event in the $T$ -invariant $\sigma$ -algebra has probability $1$ or $0$ . This is not entirely clear to me.
As example consider a simple random walk in a random environment, with $\{\omega_i\}_{i\in\mathbb{Z}}$ and $\omega_i$ the probability to jump to the right at position $i$ .
Now how do I show that this sequence is ergodic? There is no mapping $T$ involved.","['measure-theory', 'random-walk', 'ergodic-theory', 'discrete-mathematics', 'probability-theory']"
4026530,Differential equation with one univariate function and several variables,"I have a functional equation where I need to find one univariate function which satisfies conditions involving two variables, for example, find a function $f$ which satisfies, for all $x,y$ : $$
x (x+y) f'(x)(f(y)-f(x)) = (x-y)(f(x)-f^2(x))
$$ How are such equations called? I would like to find literature, but I don't even know where to begin, and what terms to use. This is not an ODE, but also not a PDE.","['functional-equations', 'ordinary-differential-equations', 'partial-differential-equations']"
4026742,I need to find a sum expressed as a function of $\sum_{n=1}^\infty -\frac{1}{2^n \cdot n}$,"$$\sum_{n=1}^\infty -\frac{1}{2^n \cdot  n}$$ I also got the first $4$ terms of this, which are pretty obvious, but I've written them down below anyways. I'm supposed to find a sum expressed as a function based on known Maclaurin series. I really suck at manipulating known series, or seeing what series I can manipulate to get what I need. Anything that can get me started is appreciated! First four terms, just for practicing MathJax: $-\frac{1}{2}-\frac{1}{2^2\cdot 2}-\frac{1}{2^3 \cdot 3}-\frac{1}{2^4 \cdot 4}$","['calculus', 'functions', 'sequences-and-series', 'real-analysis']"
4026771,"Knowing that $X$ independent of itself and $ \mathbb{E}[X^2] < \infty $, how to show that $X$ is a constant?","I have a random variable $X$ that is independent of itself.
How knowing that $$ \mathbb{E}[X^2] < \infty $$ is suppose to help me finding that $X$ is a constant. I already found out that if $X$ is independent, $ \mathbb{P}(X \in A) = 0  $ or $ \mathbb{P}(X \in A) = 1 $ . And I just pick $x \in \mathbb{R}$ and I have $\mathbb{P}(X \le x)^2 = \mathbb{P}(X \le x) \ge \mathbb{P}(X \le x) = 0  $ or $ \mathbb{P}(X \le x) = 1$ But how is it related to that expected value ? I really tried but I do not come with any ideas. Many thanks ! If the question is ambiguous or unclear, please edit it :)","['expected-value', 'statistics', 'probability', 'random-variables']"
4026812,"If I start walking on the border of a bounded domain, will I end up where I started?","If I am on the border of a bounded domain with continuous border (or maybe a manifold?) and I start moving in some direction, will I eventually end up where I started? And if so, will I eventually reach the point where I started moving in the starting direction?","['general-topology', 'geometry', 'differential-geometry']"
4026824,Coordinates of a regular unit n-simplex,"The Cartesian coordinates of a regular $n$ -simplex in $\mathbb{R}^n$ with all edge lengths 1 and centered on the origin is given in Wikipedia by $\frac{1}{\sqrt{2}}\mathbf{e}_i - \frac{1}{n\sqrt{2}}(1 \pm \frac{1}{\sqrt{n + 1}}) \cdot (1, \dots, 1),$ for $1 \le i \le n$ , and additional point $\mp\frac{1}{2\sqrt{n + 1}} \cdot (1, \dots, 1)$ where $\mathbf{e}_i$ are the base vectors in $\mathbb{R}^n$ . These expressions seem to be wrong. For example for the simplest case of the 1-simplex we have $n=1$ in $\mathbb{R}^1$ , $\mathbf{e}_1=1$ and get the coordinates $\frac{1}{\sqrt{2}}\cdot1 - \frac{1}{1\cdot\sqrt{2}}(1 + \frac{1}{\sqrt{1 + 1}})=-\frac{1}{2}$ and $-\frac{1}{2\sqrt{1 + 1}}=-\frac{1}{2\sqrt{2}}$ As we have $\pm$ and $\mp$ in the original formulas I used for the first formula the $+$ and in the second formula the $-$ sign. The coordinates for the 1-simplex are neither centered nor have edge length 1. The correct coordinates are $\pm \frac{1}{2}$ . Furthermore the notation $(1,\ldots,1)$ seems to be superficial in the first formula. What is the right formula? Or is the formula right and just misinterpreted?","['geometry', 'simplex']"
4026847,Uniformly continuous function on bounded open interval is bounded,"Let $f(x)$ be uniformly continuous on a bounded open interval $a<x<b$ . Show that $f$ is bounded (i.e. $\exists M$ such that $|f(x)|\le M \ \forall x\in (a,b)$ ). To be honest, I have no idea how to solve thus problem. I tried to pass directly by the definition of uniformly continuous function and extract something, and I passed by cases distinction ( $f$ monotonic or not), but I still can't conclude. Intuitively I see why it is true but can't find a good approach to this problem. If someone could give a hint, I would appreciate it. Thanks in advance","['continuity', 'functions', 'uniform-continuity', 'upper-lower-bounds']"
4026851,What's the intuition behind the Co-Area formula?,"I mainly work in statistics and I know only basic measure theory. I was trying to understand the Co-Area formula by Federer. If $f:\mathbb{R}^M\to \mathbb{R}^N$ is a Lipschitz function with $M \geq N$ then $$
\int_A J_N f(x) d\mathcal{L}^M x = \int_{R^N} \mathcal{H}^{M-N} (A\cap f^{-1}(y)) d\mathcal{L}^N y
$$ Could anyone please give me an intuition of what the formula implies and what it means practically? In particular, this is a simple example I am trying to wrap my head around: suppose we are in 2 dimensions and there is a curve defined by a function What does the co-area formula tell us in this context? Here are some practical questions: My understanding is that we have some space of dimension $M$ and there a manifold in it of dimension $M-N$ and we would like to compute the area of this manifold using the Hausdorff measure. Is this what is happening? I am completely lost as to why we need $A\cap f^{-1}(y))$ what's the intuition behind this? On the right hand side we have $\mathcal{H}^{M-N}$ . I am familiar with the Lebesgue measure $d\lambda$ however I am lost as to why we seem to have both the Lebesgue and the Hausdorff measure on the right hand side. I thought we should only have the Hausdorff measure on the RHS?","['measure-theory', 'geometric-measure-theory', 'multivariable-calculus', 'differential-topology', 'differential-geometry']"
4026857,Principle of inclusion exclusion and counting surjective functions,"Let $f$ be a function from $A \rightarrow B$ . $|A| = 4, |B| = 3$ The number of surjective functions by applying the principle of inclusion exclusion is given by: $3^{4} - \binom{3}{1} 2^{4} + \binom{3}{2}1^{4}$ . The rationale is that we begin with the set of all possible functions, and subtract off the functions with one element in the codomain that is not in the range, and add back the functions with two elements in the codomain not in the range. However, I don't understand why we are adding back $\binom{3}{2}1^{4}$ . If $\binom{3}{1}2^{4}$ already include functions with two elements from the codomain not in the range, then wouldn't it be done already, as that's all the non-surjective functions?","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
4026864,Hodge star operator independent of orthonormal basis,"I'm reading the book on differential geometry by Gockeler and Schucker and I came across a proof which I don't understand regarding the Hodge star operator. In the book, they define the Hodge star operator on an $n$ -dimensional oriented vector space by picking some positively oriented orthonormal basis $\{e_i\}_{i=1}^n$ and defining the map on the forms induced by the duals: $$*:\Lambda^p V\rightarrow \Lambda^{n-p}V$$ $$ *(e^{i_1}\wedge\cdots\wedge e^{i_p})=\epsilon_{i_1\ldots i_n}\eta^{i_1i_1}\cdots\eta^{i_pi_p}e^{i_{p+1}}\wedge\cdots\wedge e^{i_n} $$ Where $\epsilon$ is the Levi-Civita symbol and $\eta$ is a pseudo-Riemannian metric, which is diagonal with $r$ diagonal entries equal to $1$ and $s$ equal to $-1$ . They then claim that this definition does not depend on the choice of oriented orthonormal basis, and they explain it as follows: Where equation $3.16$ is the equation written above and equation $3.9$ is the equation which defines the elements of $O(r,s)$ : $$(\Lambda ^{-1})^t\eta \Lambda=\eta$$ I don't really understand their argument, and I'd appreciate any help in understanding it, even for the simple case of $\eta$ being the identity matrix (so just the usual Euclidean metric). I came across this related post, but it didn't contain a complete solution I could understand. I tried proving it myself, by taking another positive orthonormal basis $\{x_i\}_{i=1}^n$ , and writing $e_i=Ax_i$ for some matrix $A\in SO(r,s)$ and plugging this into the formula above, but I'm not sure on how to proceed from here. Thanks in advance.","['riemannian-geometry', 'tensors', 'multilinear-algebra', 'differential-forms', 'differential-geometry']"
4026871,How can I prove that the limit doesn't exist?,"I have to study the following limit $$\lim_{(x,y)\to(0,0)}\frac{1-\cos(\sqrt{|xy|})}{x}.$$ I think that this limit does not exist, so I'm trying to prove it. First, I discovered that, if $x=y$ , then, the limit is equal to zero. Is there any other variable changing that I can use?","['limits', 'calculus', 'analysis']"
4026940,Show integral is continuously differentiable with bounded derivative,"I have a question to this notes , page 52. It is written that $$
\left\lvert\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\right\rvert\leq \frac{\lVert g\rVert_\infty}{\sqrt{t-s}}\frac{\lvert z\rvert e^{-\lvert z\rvert^2}}{(4\pi (t-s))^{n/2}},\quad z=\frac{x-y}{\sqrt{t-s}}
$$ As the volume element has the form $dz=\frac{dx}{(t-s)^{n/2}}$ , this shows that the $x_i$ -derivative of the integrand [of $J(t,x)$ , see below] is dominated by an integrable function in $t$ and $z$ . I have two questions to this. (1) The first is a minor thing: Shouldn't the right hand side be $$
\frac{\lVert g\rVert_\infty}{\sqrt{t-s}}\frac{\lvert z\rvert e^{-\lvert z\rvert^2/4}}{(4\pi (t-s))^{n/2}}?
$$ Because when I substitute $x-y=z\sqrt{t-s}$ , I get the additional $4$ in the exponent of the exponential. (2) It is also said that one can use the estimate to conclude that $J(t,x)$ is $C^1$ in $x$ with bounded derivative. How can one deduce that? Here $J(t,x)$ is defined on page 51, namely $$
J(t,x)=\int_0^t\int_{\mathbb{R}^n}\frac{1}{(4\pi (t-s))^{n/2}}e^{-(x-y)^2/4 (t-s)} g(s,y)\, dy\, ds
$$ I think the boundedness of $\frac{\partial}{\partial x_i}J(t,x)$ is a consequence of the dominated convergence theorem which tells us that, due to the domination of the derivative of the integrand by an integrable function, we have $$
\frac{\partial}{\partial x_i}J(t,x)=\int_{0}^t\int_{\mathbb{R}^n}\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\, dy\, ds.\tag{*}
$$ Now I use this to estimate $\left\lvert\frac{\partial}{\partial x_i}J(t,x)\right\rvert$ by substituting $z=\frac{y-x}{(t-s)^{n/2}}$ (I don't know why in the link they use $z=\frac{x-y}{(t-s)^{n/2}}$ because this produces a minus sign which is not helpful at all...): $$
\begin{align*}
\left\lvert\frac{\partial}{\partial x_i}J(t,x)\right\rvert&\leq\int_0^t\int_\mathbb{R}\left\lvert\frac{\partial}{\partial x_i}\left(\frac{e^{-\lvert x-y\rvert^2/4(t-s)}}{(4\pi (t-s))^{n/2}}\right)g(s,y)\right\rvert\, dy\, ds\\
&\leq \frac{\lVert g\rVert_\infty}{(4\pi)^{n/2}}\int_0^t\int_\mathbb{R} \frac{\lvert z\rvert e^{-\lvert z\rvert^2/4}}{\sqrt{t-s}}\, dz\, ds=\frac{8\sqrt{t}\lVert g\rVert_\infty}{(4\pi)^{n/2}}<\infty
\end{align*}
$$ Thus, the derivative is bounded. Does this make sense? As to the continuity of $\frac{\partial}{\partial x_i}J(t,x)$ , this should be a direct consequence of the dominant convergence theorem and $(*)$ , see for instance Theorem 4.4.1 . Applying this theorem to the integrand of $(*)$ , we should get the continuity directly, shouldn't we? Is there a more direct way to see the continuity? Would be nice.","['solution-verification', 'analysis', 'real-analysis']"
4026947,The Method Of Frobenius,"The ODE $xy'' + y = 0$ has a real degeneracy.  Use The Method Of
Frobenius to find a fundamental set of solutions. Here is the procedure, as I understand it: 1)  Plug the guess $y = x^s \sum_{n = 0}^\infty a_n x^n$ into the ODE and do the algebra/calculus to separate out the indicial equation and the recurrence relation. $xy'' + y = 0$ $x(x^s \sum_{n = 0}^\infty a_n x^n)'' + (x^s \sum_{n = 0}^\infty a_n x^n) = 0$ $\sum_{n = 0}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 0}^\infty a_n x^{n + s} = 0$ $\sum_{n = 0}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 1}^\infty a_{n - 1} x^{n + s - 1} = 0$ $s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 1}^\infty a_{n - 1} x^{n + s - 1} = 0$ $s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + a_{n - 1} x^{n + s - 1} = 0$ $s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty [(n + s)(n + s - 1) a_n + a_{n - 1}] x^{n + s - 1} = 0$ Indicial Equation: $s(s - 1) = 0$ Recurrence Relation: $(n + s)(n + s - 1) a_n + a_{n - 1} = 0$ , where $n \geq 1$ 2)  Solve the recurrence relation, treating $s$ as a constant, but without plugging in any indicial equation solutions. $(n + s)(n + s - 1) a_n + a_{n - 1} = 0$ $a_n = -\frac{1}{(n + s)(n + s - 1)}a_{n - 1}$ $a_n = a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}$ Here I would simplify using a formula I was given, $\Pi_{j = c}^d (j + k) = \frac{(d + k)!}{(c + k - 1)!}$ . $a_n = a_0 [\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}] [\frac{1}{\Pi_{m = 1}^n (m + s - 1)}]$ $a_n = a_0 [(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}]$ $a_n = \frac{(-1)^n s! (s - 1)!}{(n + s)! (n + s - 1)!} a_0$ 3)  To find one fundamental solution, plug the largest indicial equation solution $s_1$ and its associated recurrence relation solution into the guess, and make the particular choice $a_0 = 1$ . $s(s - 1) = 0$ $s = 0, 1$ $s_1 = 1$ $y = x^1 \sum_{n = 0}^\infty \frac{(-1)^n 1! (1 - 1)!}{(n + 1)! (n + 1 - 1)!} 1 x^n$ $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1}}{n! (n + 1)!}\leftarrow$ FIRST FUNDAMENTAL SOLUTION 4)  To find another fundamental solution, plug the other indicial equation solution $s_2$ and its associated recurrence relation solution into $y = [$ FIRST FUNDAMENTAL SOLUTION $]ln|x| +\ x^{s_2} \sum_{n = 0}^\infty [\frac{\partial}{\partial s} ((s - s_2)a_n)]_{s = s_2} x^n$ , and make the particular choice $a_0 = 1$ . $s_2 = 0$ Here I use the earlier, less simplified expression for $a_n$ which is in $\Pi$ product notation, in anticipation of taking the logarithmic partial derivative. $y = (\sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1}}{n! (n + 1)!})ln|x| + x^0 \sum_{n = 0}^\infty [\frac{\partial}{\partial s} ((s - 0) a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})]_{s = 0} x^n$ $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty [\frac{\partial}{\partial s} (s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})]_{s = 0} x^n$ I make the substitution $b_n = s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}$ , appearing in the solution as $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!}$ $+ \sum_{n = 0}^\infty [\frac{\partial}{\partial s} b_n]_{s = 0} x^n$ . $b_n = s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}$ $ln|b_n| = ln|s| + ln|a_0| + \sum_{m = 1}^n ln|\frac{1}{m + s}| + ln|\frac{1}{1 - m - s}|$ $\frac{\partial}{\partial s} ln|b_n| = \frac{\partial}{\partial s} ln|s| + \frac{\partial}{\partial s} ln|a_0| + \sum_{m = 1}^n \frac{\partial}{\partial s} ln|\frac{1}{m + s}| + \frac{\partial}{\partial s} ln|\frac{1}{1 - m - s}|$ I set $\frac{\partial}{\partial s} ln|a_0|$ to $0$ because the chapter stipulates that this procedure only seeks solutions where $a_0$ is nonzero and does not depend on $s$ . $\frac{\frac{\partial b_n}{\partial s}}{b_n} = \frac{1}{s} + 0 + \sum_{m = 1}^n -\frac{1}{m + s} + \frac{1}{1 - m - s}$ $\frac{\partial b_n}{\partial s} = \frac{b_n}{s} + b_n \sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)}$ $\frac{\partial b_n}{\partial s} = \frac{s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}}{s} + s a_0 (\Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)})$ Now I can simplify the $\Pi$ products using the above formula.  I also make the choice for $a_0$ here. $\frac{\partial b_n}{\partial s} = \frac{s [\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}] [\frac{1}{\Pi_{m = 1}^n (m + s - 1)}]}{s} + s ([\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}][\frac{1}{\Pi_{m = 1}^n (m + s - 1)}])(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)})$ $\frac{\partial b_n}{\partial s} = \frac{s [(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}]}{s} + s ([(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}])(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)})$ $\frac{\partial b_n}{\partial s} = \frac{(-1)^n s! (s - 1)! + (-1)^{n + 1} s!^2 \sum_{m = 1}^n \frac{2m + 2s - 1}{(m + s)(m + s - 1)}}{(n + s)! (n + s - 1)!}$ Unwinding the substitution, I plug the expression for $\frac{\partial b_n}{\partial s}$ back into the fundamental solution. $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty [\frac{(-1)^n s! (s - 1)! + (-1)^{n + 1} s!^2 \sum_{m = 1}^n \frac{2m + 2s - 1}{(m + s)(m + s - 1)}}{(n + s)! (n + s - 1)!}]_{s = 0} x^n$ $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty \frac{(-1)^n (-1)! x^n + (-1)^{n + 1} x^n \sum_{m = 1}^n \frac{2m - 1}{m^2 - m}}{n! (n - 1)!}$ And now we see that something is broken. $(-1)!$ is undefined, and can't even be rescued by being generalized to the Gamma or Pi function.  What am I doing wrong, and how can I use this method to solve this ODE?","['frobenius-method', 'ordinary-differential-equations', 'solution-verification', 'fundamental-solution', 'power-series']"
4027037,How do I compute this integral with a Dirac's delta?,"While studying probability I encountered this integral $$I=\int_{\mathbb{R}^2}\exp\left({-\frac{x_1^2+x_2^2}{2}}\right)\delta\left(r-\sqrt{x_1^2+x_2^2}\right)\,dx_1\,dx_2$$ If I compute this in polar coordinates i get $$I=\int_0^{2\pi}\,d\theta \int_0^{+\infty}\exp\left(-\dfrac{\rho^2}{2} \right)\rho\delta(r-\rho)\,d\rho=2\pi r\exp\left(-\dfrac{r^2}{2}\right)$$ but in cartesian coordinates I only get $$I=\exp\left(-\frac{r^2}{2}\right)$$ I don't understand why. I just thougth that I was using the Dirac's delta's properties in both cases. I think the first result is the correct one and there is something I don't know about Dirac's delta with more than one variable. Which result is correct and why?","['multivariable-calculus', 'definite-integrals', 'dirac-delta']"
4027097,How to maximize the product of scalar products?,"Given $N$ unit vectors $v_1,\ldots,v_N$ of $\mathbb R^d$ , I was curious about finding an explicit maximizer for the product mapping $$
f(x) := \prod_{j=1}^N \langle x,v_j\rangle,\qquad x\in\mathbb R^d,
$$ assuming that, say, $\|x\|=1$ . If one replaces the product by a sum, then the Cauchy-Schwarz inequality would directly yield that the maximum is reached at the unit vector proportional to the sum $\sum_jv_j$ , but for the product I was not able to find such a simple solution. I've tried to use the tensor product formalism to write $f(x) = \langle x^{\otimes N},\otimes_{j=1}^N v_j\rangle$ , so that we see that maximizing $f$ boils down to minimize $$
g(x):=\|x^{\otimes N}-\otimes_{j=1}^N v_j\|^2,
$$ and thus the solution is the first tensor component of the ""orthogonal projection"" of $\otimes_{j=1}^N v_j$ onto the subset $\Delta := \{x^{\otimes N}: x\in\mathbb R^d\}\subset(\mathbb R^d)^{\otimes N}$ . Unfortunately, $\Delta$ is not a vector space and is not even convex, hence the quotation marks. I don't know how to continue (and gradient derivations did not bring me anywhere). Any ideas? I'm pretty sure that clever people have worked on how to approximate a tensor product by a tensor product of the same vector, but I guess I didn't give the good keywords to Google.","['optimization', 'linear-algebra', 'tensor-products', 'differential-geometry']"
4027106,Solving congruences using Euclidean's algorithm or otherwise [duplicate],"This question already has answers here : What is the best way to solve modular arithmetic equations such as $9x \equiv 33 \pmod{43}$? (5 answers) How to solve a linear congruence $8n+9\equiv 0\pmod{\!1163}$ (5 answers) Closed 3 years ago . ""Solve the following congruence. Make sure that the number you enter is in the range $[0,M−1]$ where $M$ is the modulus of the congruence. If there is more than one solution, enter the answer as a list separated by commas. If there is no answer, enter $N.$ "" $102x = 220 \pmod{266}$ $x = ?$ So I've tried using Euclidean's algorithm on this nearly 10 times already(no exaggeration) but every time I try getting past the point below the whole thing falls apart and I just end up getting another incorrect answer for x. Would anyone mind showing how to continue on from this point and if there are any methods other than Euclidean's that one can use to calculate such equations, thanks :) $102x = 220\pmod{266}$ $\gcd(102,266) = 2$ $266 = (102 \times 2) + 62$ $102 = (62 \times 1) + 40$ $62 = (40 \times 1) + 22$ $40 = (22 \times 1) + 18$ $22 = (18 \times 1) + 4$ $18 = (4 \times 4) + 2$ $4 = (2 \times 2) + 0$","['number-theory', 'exponential-function', 'discrete-mathematics', 'prime-numbers']"
4027134,Is the following proof of the continuity of a function correct?,"I just started learning analysis and encountered my first task on continuity today, namely proving that $f: \mathbb{R}\to \mathbb{R}, \space f(x) = \inf\{|x-k\rvert \space \mid k\in \mathbb{Z} \}$ is continuous in every point. I attempted to prove this as follows: Let $x_{0} \in \mathbb{R}$ be arbitrary. Let $x \in \mathbb{R}$ . By the triangle inequality we have $\left|x_{0}-k\right| \leq\left|x_{0}-x\right|+|x-k|$ for all $k \in \mathbb{Z}$ . Thus, one obtains $$
\inf \left\{\left|x_{0}-k\right| \mid k \in \mathbb{Z}\right\} \leq\left|x_{0}-x\right|+\inf \{|x-k| \mid k \in \mathbb{Z}\}
$$ so by the definition of $f$ $$
f\left(x_{0}\right)-f(x) \leq\left|x_{0}-x\right|
$$ so we obtain $f(x)-f\left(x_{0}\right) \leq$ $\left|x_{0}-x\right|$ (applying the same argumentation). We get $$
\left|f\left(x_{0}\right)-f(x)\right| \leq\left|x_{0}-x\right|
$$ Now, let $\epsilon>0$ be arbitrary. To show that $f$ is continuous in $x_{0}$ we must find some $\delta>0$ , s.t. for all $x \in \mathbb{R}$ with $\left|x-x_{0}\right|<\delta$ the inequality $\left|f\left(x_{0}\right)-f(x)\right|<\epsilon$ holds. From the above we can clearly see that $\delta=\epsilon$ is a sufficient choice is. Since $x_{0}$ was arbitrary, we are done. Does this attempt seem valid? Also, what are other ways one could prove this (assuming that the above works). I assume that one often can just plug in the definition of the respective function and then rearrange the inequality, but this didn‘t quite work for me in this case.","['continuity', 'solution-verification', 'real-analysis']"
4027155,"Why differentiate between continuous on $[a,b]$ and differentiable on $(a,b)$?","Before starting I want to say that my main language is not english so you may find some grammar mistakes. I will try to avoid making them as much as I can, so please forgive me. A few months ago I decided that I wanted to learn Calculus so I got some books and started to study, during that time I faced a statement that is pretty common among math definitions related to it, and that statement is: $f(x)$ is continuous at $[a,b]$ and differentiable at $(a,b)$ What I was thinking was that a limit needs a closed interval to compute its value, but after making some research I found that the function $f(x) = \sqrt{x}$ is continuous at $0$ , so I started to make myself this question. If we consider, for example $f(x) = x^2$ . and the interval $[1,3]$ . We know that the function is differentiable and continuous at its whole domain but we do we say that it has to be continuous at $[1,3]$ and differentiable at $(1,3)$ , this of course, considering the function over the interval mentioned before, when in fact it is differentiable at $1$ and $3$ . Also the function $\sqrt{x}$ is considered continuous at its whole domain, even also at $x = 0$ but it is not differentiable at 0. Why? Why do we do this difference between $[a,b]$ and $(a,b)?$ Sorry if I did not express myself the best way possible, again, English is not my main language.","['limits', 'calculus', 'derivatives']"
4027169,Conditional Probability Multivariate Question,"Two life insurance policies, each with a death benefit of 10,000 and a one-time premium of 500, are sold to a couple, one for each person. The policies will expire at the end of the 10th year. The probability that only the wife will survive at least 10 years is 0.025, the probability that only the husband will survive at least 10 years is 0.01, and the probability that both of them will survive at least 10 years is 0.96. what is the expected excess of premiums over claims, given that the husband survives at least 10 years? I understood that it was a conditional probability problem but I tried to solve it a different way and I'm not sure where my thinking is wrong. Here's what I tried: X: wife survives 10 years, Y: husband survives 10 years Since we are assuming the husband survives the 10 years, I disregarded the probability that only the wife survives the 10 years. -(Premium for both policies) + (Death Benefit For One) * (Probability Only Husband Survives) + (No Death Benefit) * (Probability Husband and Wife Both Survive) $$-1000 + 10000\times P(X' \cap Y) +0\times P(X \cap Y)$$ I have the solution so I know how to ""correctly"" solve it, I'm just confused about why this method I tried won't work.
Thanks in advance!","['conditional-probability', 'probability', 'actuarial-science']"
4027194,"First countability of $[0,1]^\mathbb{R}$","The proofs I know for the fact that the space $[0,1]^\mathbb{R}$ is not first countable, use the product topology in some step of the demonstration. ( Reference ) So, I would like to know if this space is also not first countable in the box topology and also if there is any topology that makes this space first countable. Edit:
As Brian commented in his answer , I was looking for a topology that was different from the trivial and the discrete.","['box-topology', 'analysis', 'functional-analysis', 'general-topology', 'first-countable']"
4027240,"Topologically, what is a 'string' from string theory?","To begin: I am not a crank. I am not sure how well-founded my titular question is, but it was interesting enough that I decided to bring it to MSE. For context: I am an undergraduate mathematics student. I am taking a course in algebraic topology, and during class it was asked how algebraic topology applies to string theory. Our professor explained that he was not sure, but he knew there was some connection between knot theory and string theory. He said it may interesting to understand the topological role of a 'string,' and I assume this is contingent on the connection between knot theory and string theory being strong enough that such an analogy can be realized. So I am here to ask: Is the connection between string theory and (algebraic) topology strong enough that it makes sense to ask how a 'string' can be viewed topologically? If so, what is a string from this topological point-of-view?","['algebraic-topology', 'string-theory', 'knot-theory', 'general-topology', 'mathematical-physics']"
4027247,Conditional probability on a multiple choice test,"A student is taking a multiple-choice test. Each question on the test has five possible answers and only one choice is correct. The probability this student knows the correct answer is 70%. If the student does not know the answer, they select an answer at random with each answer having an equal probability of being picked. Calculate the conditional probability that the student knew the answer given they answered the question correctly. I started off like this:
Let B denote the event that the student knew the answer. Let A denote the event that the student answered the question correctly. I was able to work out that $P(A)=\frac{3}{10}*\frac{1}{5} + \frac{7}{10} = \frac{19}{25}$ And I know the formula $P(B|A)=\frac{P(A \cap B)}{P(A)}$ I am unsure on how to work out $P(A \cap B)$ . Any advice would be greatly apprectiated.","['conditional-probability', 'probability']"
4027259,Linear Independence of Angles Corresponding to Primes $p \equiv 1 \pmod 4$,"Any prime $p \equiv 1 \pmod 4$ can be written uniquely as the sum of two squares $p = a^2 + b^2$ . In the complex plane, this corresponds to the 8 Gaussian integers with norm $p$ . We can write these points as $$
\sqrt{p}e^{2\pi i (\pm\Phi + t/4)}
$$ where we define $\Phi$ as the argument of the point in the first octant ( $\Phi = \arg(a+bi)$ where $a^2 + b^2 = p$ and $a \geq b$ ) and $t \in \{0,1,2,3\}$ corresponds to multiplication by a unit. I want to show that any finite set of the type $\left\{ 1, p_1, \ldots , p_N \right\}$ where $p_j \equiv 1 \mod 4$ , $j = 1, \cdots, N$ is linearly independent over $\mathbb{Q}$ . In other words, we want to show $$
\sum_{j=1}^{N} \alpha_j \Phi_j + \alpha_0 = 0
\iff
\alpha_0 = \alpha_1 = \cdots =\alpha_N = 0
$$ where the $\Phi_j$ is defined for each $p_j$ as described above. I encountered this problem while self-studying some number theory and have been stuck for a bit. I'd appreciate some hints! The approach I tried is to go by contradiction and try to show that something will contradict the definition of a prime number, but I think I've gotten lost!","['number-theory', 'complex-numbers', 'prime-numbers']"
4027283,Find the total length of two line segments of two overlapping right triangles,"Find the length of $AE+EB$ ? (A) $\frac{128}{7}$ (B) $\frac{112}{7}$ (C) $\frac{100}{7}$ (D) $\frac{96}{7}$ (E) $\frac{56}{7}$ My solution: For $\Delta AEB$ : $\angle BAC = sin^{-1}(\frac{5}{13}) = 22.62^{\circ}$ $\angle ABD = sin^{-1}(\frac{9}{15}) = 36.87^{\circ}$ $\angle AEB = 180 - \angle BAC - \angle ABD = 120.52^{\circ}$ Use Law of Sines: $\frac{AE}{sin(\angle ABD)} = \frac{AB}{sin(\angle AEB)}$ $AE = \frac{AB}{sin(\angle AEB)} \times sin(\angle ABD)$ $AE = \frac{12 \times \frac{9}{15}}{sin 120.51} = 8,372$ $\frac{EB}{sin(\angle BAC)} = \frac{AB}{sin(\angle AEB)}$ $EB = \frac{AB}{sin(\angle AEB)} \times sin(\angle BAC)$ $EB = \frac{12 \times \frac{5}{13}}{sin 120.51} = 5.367$ $AE+EB = 8.372 + 5.367 = 13,7387 \approx \frac{96}{7}$ Answer: (D) My question: Is there a solution without having to compute both arcsin $\angle BAC$ & $\angle ABD$ ? The reason I'm asking is because the choices all have 7 as denominators. So I'm guessing there may be a solution that contains only integers.","['triangles', 'trigonometry', 'solution-verification', 'geometry']"
4027350,Find all complex roots of $(z+1+i)^4 - 1 + i =0$.,"Find all complex roots of $(z+1+i)^4 - 1 + i =0$ . Attempt: I got \begin{align*}
(z+1+i)^4 &= 1 - i \\
(z + \sqrt{2} e^{i(\frac{\pi}{4})})^4 &= \sqrt{2} e^{i(-\frac{\pi}{4})} \\
z + \sqrt{2} e^{i(\frac{\pi}{4})} &= \sqrt[8]{2} \exp\left(i\frac{-\frac{\pi}{4} + 2k\pi}{4}  \right) \\
z &= \sqrt[8]{2} \exp\left(i\frac{-\frac{\pi}{4} + 2k\pi}{4}  \right) - \sqrt{2}e^{i(\frac{\pi}{4})}
\end{align*} for $k=0,1,2,3$ . Am I true? If not yet, how to find it?","['complex-analysis', 'complex-numbers']"
4027351,Show that $\triangle ABC$ satisfying $B^2=AC$ and $2b=a+c$ is equilateral,"A $\triangle ABC$ satisfies the conditions below: $$B^2=AC \qquad 2b=a+c$$ Show that it's an equilateral one, where $a=|BC|$ , $b=|AC|$ , $c=|AB|$ and $B=\angle ABC$ , $A=\angle BAC$ , $C=\angle ACB$ . What I have done: When trying to solve these question, I want to find a solution in all triangles which satisfy $2b=a+c$ . It means that point $B$ can be regarded as a point in an ellipse. Then I try to prove that for all points $B$ in that ellipse, we have $B^2\leq AC$ . When proving it I think I can get the equal conditions and then show that $AC=B^2$ if and only if $A=B=C$ or $AC=0$ . Then I meet a terrible problem, meaning that I need to show ( $x=A$ ): $$
\frac{4-5\cos x}{5-4\cos x}\leq\cos\frac{(2\pi-x)-\sqrt{x(4\pi-3x)}}{2}=-\cos \frac{x+\sqrt{x(4\pi -3x)}}{2}，x\in[0,\frac{\pi}{3}]\\
$$ I have never seen such a hard problem before. I try to prove the inequality by segment amplification and minification, derivative, and many other ways but all failed. Can you help me or give me some hints on this problems?","['contest-math', 'analytic-geometry', 'geometry', 'functions', 'inequality']"
4027414,Lemma 1 - Banach-Mazur Game - Dan Ma's topology Blog,"I have been given a presentation for a course based on this particular post in Dan Ma's topology blog. The post itself is very clear and educational, but there is one proof that I'm not sure why it is complete. Lemma 1 has a rather standard application of Zorn's lemma, but I feel that I'm missing something in the second paragraph of the proof. Lemma 1: Let $X$ be a space, Let $O \subset X$ be a nonempty open set. Let $\tau$ be the set of all nonempty open subsets of $O$ . Let $f: \tau \longrightarrow \tau$ be a function such that for each $V \in \tau$ , $f(V) \subset V$ . Then, there exists a disjoint collection $\mathcal{U}$ consisting of elements of $f(\tau)$ such that $\bigcup \mathcal{U}$ is dense in $O$ . The proof begins by explaining the trivial case of $O$ having just one point, then if I understand correctly everything holds in a rather obvious way, namely: the property of $f$ gives me the same set as an image and clearly this is dense in $O$ . Then, we assume $O$ has two points and the author shows that $\mathcal{P}$ (the set of all collections $\mathcal{F}$ such that $\mathcal{F}$ is a disjoint collection of elements of $f(\tau)$ ) is non-empty. For this, two disjoint open subsets of $O$ are taken based on the assumption that $O$ has at least two points. Why must these subsets be disjoint? Isn't the author assuming anything in addition? I underestand that by applying $f$ later those sets become exactly of the form of an element of $\mathcal{P}$ which is the goal of that particular part of the proof. I thought we could possibly assume they are disjoint and otherwise apply $f$ again and again until they are but no assumption really guarantees this process would stop.",['general-topology']
4027421,Optimal control of a certain Poisson process,"I am trying to understand this paper in which the optimal expected value of a certain Poisson process is computed.
Following the notation of op. cit, let $N_s$ ( $0 \le s \le t$ ) be the associated counting process.
The intensity of the process is given by $\lambda_s = \lambda (p_s)$ , where $\lambda$ is a known decreasing function and $p_s$ is a real-valued stochastic process depending on the control $u$ .
We wish to find a control $u$ such that $$J_u (n, t) = \mathbf{E}_u \left[ \int_0^t p_s \mathrm{d} N_s \right]$$ is maximised, where $\mathbf{E}_u$ denotes the expectation operator under control $u$ . Let $J^* (n, t) = \sup \{ J_u (n, t) : u \in \mathscr{U} \}$ , where $\mathscr{U}$ is the set of controls such that $N_t \le n$ a.s.
In op. cit., there is a heuristic dynamic programming argument showing that $$\frac{\partial J^* (n, t)}{\partial t} = \lambda (p^* (n, t)) \left( p^* (n, t) - J^* (n, t) + J^* (n - 1, t) \right)$$ where $p^* (n, t)$ is the $p$ maximising $\lambda (p) \left( p - J^* (n, t) + J^* (n - 1, t) \right)$ .
This argument seems plausible enough to me, so I have no issue here. Now suppose $\lambda (p) = a \exp (- \alpha p)$ .
The following solution is given in op. cit.: $$J^* (n, t) = p^* (n, t) \log \left( \sum_{k = 0}^n \frac{(\lambda (1 / \alpha) t)^k}{k !} \right)$$ $$p^* (n, t) = \frac{1}{\alpha} + J^* (n, t) - J^* (n - 1, t)$$ How do I verify that this is indeed a solution, or better yet, how might I have found this solution in the first place?","['optimal-control', 'stochastic-processes', 'probability']"
4027430,Show that operator is diagonalizable,"Let $T$ be the self-adjoint operator defined by $$(T x)_{n} = x_{n + 1} + x_{n - 1} \text{ for } n \in \mathbb{Z}, (x_{n})_{n \in \mathbb{Z}}$$ on $\ell^{2}(\mathbb{Z})$ . Show that $T$ is diagonalizable by giving an explicit unitary operator $B : L^{2}(\mathbb{T}, \frac{d \theta}{2 \pi}) \rightarrow \ell^{2}(\mathbb{Z})$ such that $B M_{f} = TB$ , where $f : \mathbb{T} \rightarrow \mathbb{R}$ is $f(e^{i \theta}) = 2 \cos(\theta)$ . Here $M_f$ is multiplication by $f$ such that $(M_f g)(\xi) = f(\xi)g(\xi)$ . Note also that $\mathbb{T}$ here means the unit circle. I am not sure what $B$ should be. Usually, to show diagonalizability I use the Spectral Theorem which states that every normal operator (acting on a separable
Hilbert space) is diagonalizable. But here one asks to explicitly give a unitary operator.","['measure-theory', 'spectral-theory', 'functional-analysis', 'analysis']"
4027493,Accessible Proof of Denjoy-Riesz Theorem?,"The Denjoy-Riesz Theorem states that any compact zero-dimensional subset of the plane is contained in the image of an embedded copy of the arc $[0,1]$ .  The only place I've seen this proved is in Kuratowski's Topology II, and it's proved as a trivial consequence of a huge body of theory on unicoherence, Janiszewski spaces, characterizations of the sphere etc. The ""original proof"" by Riesz is known to have flaws.  Actually I've also seen it proved in a paper by Moore, but there he proves a generalization and it's atrocious to read as well. Is there a more self-contained, modern proof accessible anywhere?","['general-topology', 'plane-geometry', 'reference-request']"
4027537,Find the asymptotic analysis of $\frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}- \sqrt{5}$,"Given a recursion $a_{n+ 1}= a_{n}^{2}- 2$ with $a_{0}= 3.$ Find the asymptotic analysis of $$\frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}- \sqrt{5}$$ Maybe this helps - https://artofproblemsolving.com/community/c4h2308107p18323223 - And this is my way of thinking, I use two inequalities $$\sqrt{5}\leq \frac{a_{n+ 1}^{2}}{\prod_{0}^{n+ 1}a_{i}}\leq\sqrt{5}+ \frac{2}{3}\cdot\left ( \varphi- 1 \right)^{-2^{n+ 1}}$$ with $\varphi= \frac{3+ \sqrt{5}}{2}.$ From the recurrence sequence $a_{n+ 1}= a_{n}^{2}- 2\Rightarrow a_{n+ 1}= \varphi^{2^{n}}+ \varphi^{-2^{n}},$ we have $$\frac{a_{n+ 1}^{2}}{\prod_{i= 0}^{n+ 1}a_{i}}= \frac{\varphi^{2^{n+ 1}}+ \varphi^{-2^{n+ 1}}}{\prod_{i= 1}^{n}\left ( \varphi^{2^{i}}+ \frac{1}{\varphi^{2^{i}}} \right )}= (\varphi- \frac{1}{\varphi})\left ( 1+ \frac{2\varphi^{-2^{n+ 2}}}{1- \varphi^{-2^{n+ 2}}} \right )> \varphi- \frac{1}{\varphi}= \sqrt{5}$$ Because I'm busy now.. OK, I'll get it done by tonight, I haven't posted the solution for the rightside inequality, and how can we use it to find the $o\left ( 1 \right ).$ I need to the help, thanks a real lot ! Edit . The rightside inequality is just single variable of $f\left ( n \right )\leq g\left ( n \right )$ with $$f\left ( n \right )= \sqrt{5}+ \frac{2}{3}\cdot\varphi^{-2^{n+ 1}}, g\left ( n \right )= \sqrt{5}\left ( 1+ \frac{2\varphi^{-2^{n+ 2}}}{1- \varphi^{-2^{n+ 2}}} \right ), 0< \varphi^{-2^{n+ 1}}\leq\frac{7-3\sqrt{5}}{2}$$","['recurrence-relations', 'asymptotics', 'recursion', 'limits', 'inequality']"
4027668,$y'=\frac{x-y+2}{x+y-1}$,"First, I substitute $x=\xi + h$ , $y=\eta+k$ . The equation becomes $$y'=\frac{\xi+h-(\eta+k)+2}{\xi+h+\eta+k-1}$$ and this becomes homogenous if we let $h=-\frac{1}{2}$ and $k=\frac{3}{2}$ ( We solve the constant terms such that they become zero ). I think it's correct this far, but what next? At this point we'd have $$\eta'=\frac{\xi-\eta}{\xi+\eta}$$ which is homogenous, and I tried substituting $\eta = u\xi$ . This would lead to the equation $$u'\xi+u=\frac{1-u}{1+u}$$ I reduce $u$ from both sides. This will give $$u'\xi=\frac{1-u}{1+u}-u=\frac{1-2u-u^2}{1+u}$$ or $$\frac{du}{d\xi}\xi=\frac{1-2u-u^2}{1+u}$$ inverting this gives the relation $$\frac{1}{\xi}d\xi= \frac{1+u}{1-2u-u^2}du$$ So there has been some mistake made. The correct answer does not involve logarithms, and $\ln(\xi)$ will inevitably be a part of the solution. Can somebody spot my mistake?",['ordinary-differential-equations']
4027704,Find the coefficient of $x^{100}$ in $\frac{x^4}{\prod_{i=1}^{4}(1-x^{i})}$,"Find the coefficient  of $x^{100}$ in $$\frac{x^4}{\prod_{i=1}^{4}(1-x^{i})}$$ This problem came from finding  total number of 4-partition of an integer 100.
One uses mathematica and it maybe easy, but I want to find more Mathematical proof. f[m_, n_] := 
  Coefficient[Series[x^m/Product[(1 - x^i), {i, 1, m}], {x, 0, n}], x,
    n];
f[4, 100] $7153$ is obtained. More general: FullSimplify[
 SeriesCoefficient[x^4/Product[(1 - x^i), {i, 1, 4}], {x, 0, n}], 
 Assumptions -> Element[n, PositiveIntegers]] $$
\frac{1}{288} \left(-32 U_n\left(-\frac{1}{2}\right)+(n+1) \left(2 n
   (n+2)+9 (-1)^n-13\right)+36 \cos \left(\frac{\pi  n}{2}\right)\right)$$",['combinatorics']
4027748,Are complete metric spaces of finite Hausdorff dimension locally compact,"Is every complete metric $(M,d)$ of Hausdorff dimension $n<\infty$ a locally compact space? More generally, given a complete metric space of finite Hausdorff dimension, can we always find a locally identical metric $d'$ on $M$ so that $(M,d')$ has the Heine-Borel property, i.e. so that every bounded and closed subset is compact?","['general-topology', 'metric-spaces']"
4027789,Is it always true that $Aut_F(H_0) \cong Aut_F(H_1)$?,"Suppose $G$ is a group. Let's call a total recursive function $F: \mathbb{N} \times \mathbb{N} \to \mathbb{N}$ a functional representation if, for all $i \in \mathbb{N}$ , the function $n \mapsto F(i, n)$ is a bijection and $\langle \{ n \mapsto F(i, n) \}_{i \in \mathbb{N}} \rangle \cong G$ . Let's call $G$ functionally constructive iff it accepts a functional representation and let's call $\langle \{ n \mapsto F(i, n) \}_{i \in \mathbb{N}} \rangle < \operatorname{Sym}(\mathbb{N})$ its functional embedding . Now, suppose $G$ is a functionally constructive group and $H < \operatorname{Sym}(\mathbb{N})$ is its functional embedding. Let's define a functional representation of an automorphism $\alpha \in \operatorname{Aut}(H)$ as a pair of functional representations of $G$ , $F_0$ and $F_1$ , such that: $\langle \{ n \mapsto F_0(i, n) \}_{i \in \mathbb{N}} \rangle = \langle \{ n \mapsto F_1(i, n) \}_{i \in \mathbb{N}} \rangle = H$ ; $(\forall i \in \mathbb{N}) \quad \alpha((n \mapsto F_0(i, n))) =  (n \mapsto F_1(i, n))$ . Let's call an automorphism of $H$ that has a functional representation functionally constructive . Let's call the set of all functionally constructive automorphisms of $H$ as $\operatorname{Aut}_F(H)$ . It is not hard to see, that it is a group. My question is: Suppose $G$ is a functionally constructive group and $H_0$ and $H_1$ are its functional embeddings. Is it always true that $\operatorname{Aut}_F(H_0) \cong \operatorname{Aut}_F(H_1)$ ? For finite $G$ it is obviously true, because all automorphisms of finite groups are functionally constructive for any their functional embedding. However, I have no idea, what happens when $G$ is infinite...","['automorphism-group', 'abstract-algebra', 'discrete-mathematics', 'group-theory', 'computability']"
4027868,Essential spectrum of operators whose resolvent difference is compact,"Suppose that $T,S$ are densely defined, closed (unbounded) operators on a separable Hilbert space such that there exists $z \in \mathbb C$ in the intersection of resolvent sets of $T$ and $S$ for which $(T-z)^{-1}-(S-z)^{-1}$ is compact. Does it follow that $T$ and $S$ have the same essential spectrum? Remark: in the application I have in mind $S-T$ is unbounded and domains of $S$ and $T$ are not necessarily equal.","['operator-theory', 'spectral-theory', 'compact-operators', 'functional-analysis']"
4027870,How Many Uniquely Enumerated $4\times4$ Sudoku Grids Exist?,"I'm looking to find how many uniquely enumerated $4\times4$ Sudoku grids exist. I am aware that there are other questions with solutions to this question, however I am asking again as there seem to be conflicting answers online. Some say $384$ whilst some say $288$ . I make my solution to be 384, as I see that there are $4!$ ways of enumerating the first (top left) $2\times2$ block, and then there are $2!$ ways of enumerating the remaining cells of each of the first column, second column, first row and second row. Once the first block and the adjacent blocks (blocks two and three, if you will) are fully enumerated, the fourth block (bottom right) can only be enumerated in one way due to the restrictions on number placement caused by the enumeration of blocks 2 (top right) and 3 (bottom left). From this, I therefore make my total number of possible enumerations to be: $$4!\times2!\times2!\times2!\times2!=384.$$ It is clear that quite a few others have calculated it in a slightly different way and made their solution to be 288. This doesn't make sense to me as it seems that the total should be divisible by 8, as the any given enumerated grid should have 8 symmetries (rotations and flips) which are all equally valid sudoku enumerations. Is my calculation incorrect or are some of the answers online incorrect? Edit: I made a typo initially and wrote 388 instead of 288. I was concerned that 8 did not divide 388, but it does in fact divide 288 - which makes the issue of divisibility by 8 go away.","['permutations', 'sudoku', 'combinatorics']"
4027876,Uncertainty quantification Frequentist vs Bayesian,"Is it actually possible to quantify the uncertainty in a frequentist setting? (e.g. using Maximum Likelihood Estimator). Say that we have a dataset $\mathcal{D} = \{(x_i,y_i)\}_{i=1}^n$ and assume that $y_i$ are i.i.d given some parameter $\theta$ . if I have a regression problem it is common to assume the likelihood function $p(\mathcal{D}|\theta)$ being Gaussian. Then since observations are indipendent and identically distributed we have $$p(\mathcal{D}|\theta) = \prod_{i=1}^n p(y_i|x_i,\theta) $$ where each $p(y_i|x_i,\theta) \sim \mathcal{N}(y_i|\mu,\sigma^2)$ Using maximum likelihood estimator we find \begin{align}
&\mu_{ML} = \frac{1}{n}\sum_{i=1}^n y_i\\
& \sigma^2_{ML} = \frac{1}{n}\sum_{i=1}^n (y_i-\mu_{ML})^2 
\end{align} Based on this procedure, the uncertainty I get does not depend on $x$ but it is just a fixed quantity (i.e. the $\bf{spread}$ of the distribution remains fixed once computed). Instead, using a Bayesian inference approach and putting a gaussian conjugate prior dist. on the parameters we end up having a predictive distribution that looks like this $$p(y_{new}|x_{new}) = \mathcal{N}(y_{new}|\mu(x),\sigma^2(x))$$ which takes specifically into account the input. So, is it true that a Bayesian approach is more suitable to $\bf{quantify}$ uncertainty? Or we can do something similar even with a classical inference approach using MLE? EDIT: This is a plot of Bayesian linear regression. As you can see, according to the notation I've used above, the red line corresponds to $\mu(x)$ while the light red-shaded region is the uncertainty associated to each input (\sigma^2(x)), that clearly depends on it. So my question is, while Bayesian inference clearly quantifies input-depending uncertainty, can we do a similar thing under a classical inference setting? Thanks, James","['statistical-inference', 'statistics', 'bayesian']"
4027887,Levy-Ito decomposition intuition,"So a Levy process $(X_t)_{t\geq0}$ can be decomposed into three parts $$X_t = \mu t + \sigma^2B_t + L_\nu(t)$$ where $L_\nu(t)$ is ""a compound Poisson process with Levy measure $\nu$ "". I know the Levy measure of a set $A$ is the expected number of steps of $X_t$ having step size in $A$ in a time interval t. I.e. $\nu(A) = \mathbb{E}[N(1,A)]$ where $N(t,A)$ is the Poisson random measure of a set of counts $A$ in a time interval $t$ . In the paper I'm looking at, $$L_\nu(t) = \int_{|h|>1} h N_\nu (t,dh) + \int_{|h|\leq 1} h \big(N_\nu(t,dh) - t\nu(dh)\big).$$ I understand the first two terms of the decomposition, linear drift and Brownian motion, but not the third. Roughly, the third component seems to contribute information about how fast the process is moving. But I don't understand the details -- like, why are we adding $L_\nu(t)$ rather than scaling somehow? And why are we integrating over $h$ in this way?","['stochastic-integrals', 'stochastic-processes', 'measure-theory', 'brownian-motion']"
4027896,Pattern for all the binary chains divisible by 5,"For instance, $x = 101$ is divisible by $5$ because it is the integer 5. Same thing for $x=1111$ is also divisible by 5 as it is the integer 15. However, $x=1100$ is not divisible by $5$ as it is the integer 12. Is there a pattern the recognise the binary chains divisible by 5?","['number-theory', 'binary', 'modular-arithmetic']"
4027902,Why is the inverse image sheaf a sheaf of rings,"Given an inclusion of topological spaces $i:X \hookrightarrow Y$ where $(Y,\mathcal{O}_Y)$ is a ringed topological space, I'm trying to understand why $(X,i^{-1}\mathcal{O}_Y)$ is a ringed topological space. Now I see why $(i^{-1}\mathcal{O}_Y)_x$ for any $x \in X$ must be a local ring, given that stalks of the inverse image sheaf are the same as stalks on the sheaf $\mathcal{O}_Y$ . But while trying to write the entire proof out I realized I didn't quite understand what the ring structure on the inverse image sheaf looks like (for this inclusion case and in general). By definition, the inverse image sheaf is the sheaf associated to the presheaf $U \mapsto$ lim $_{W \supset i(U)}\mathcal{O}_Y(W)$ , which going by Hartsehorne's construction of the sheafification (page 64, Prop-Def 1.2), is the set of functions $\{f:U \rightarrow \bigcup_{p \in U} ($ lim $_{W \supset U}\mathcal{O}_Y(W))_p\}$ But I don't see explicitly what the ring structure on this looks like. Any help would be appreciated","['algebraic-geometry', 'sheaf-theory']"
4027907,$L^2$ convergence of product of $L^2$ random variables,"Assume $X_n\to X$ in $L^2$ and $Y_n\to Y$ in $L^2$ and further $X_nY_n\in L^2$ . Does this imply $XY\in L^2$ ? Hoelder of course gives $XY\in L^1$ and we can also conclude $X_nY_n\to XY$ in $L^1$ . This does not use the assumption $X_nY_n\in L^2$ . Is this assumption enough to conclude $XY\in L^2$ ? What additional assumptions might be sufficient? Additionally being dominated, i.e. $|X_nY_n|\leq Z$ for another $L^2$ random variable $Z$ , should be enough using dominated convergence. This assumption is however quite strong. I may be able to assume that $Y$ has a normal distribution with $0$ mean. Does this help?","['measure-theory', 'lp-spaces', 'functional-analysis', 'probability-theory', 'random-variables']"
4027952,Axiom of choice equivalence,"A  family of sets F is called interesting if $$\exists n \in \omega \forall X (X \in F \iff \forall Y (Y \subseteq X \land |Y| \leq n \implies Y \in F)$$ I have the following lemma : If F is an interesting family and $x\in F$ , then there is a maximal $Y \in F$ (according to the $\subseteq$ ), such that $X \subseteq Y$ . I need to shouw that this lemma is equivalent to the axiom of choice . My attempt: I have noticed that this lemma has a tight connection with Teichmüller–Tukey lemma .(which is equivalent to the Axiom of Choice) It's easy to see that every interesting Family is of finite character .
So the given lemma is a bit ""stronger"" than the Teichmüller–Tukey lemma .
But can I conclude that lemma and the AC are equivalent from that? Probably just the first direction (Lemma $\implies$ Axiom of Choice). But I can't figure the other direction.
Any tips? PS: I think the question was wrongfully closed : The given solution has nothing to do with the task.","['elementary-set-theory', 'axiom-of-choice', 'set-theory']"
4028018,A puzzling KKT for LMI vs. scalar constraint,"I am trying to understand the KKT conditions for LMI constraints in order to solve my original question in KKT conditions for $\max \log \det(X)$ with LMI constraints . In the meantime, I found a much simpler problem that does not go through when extending the KKT conditions from scalar case to the vector case. The problem is \begin{align}
& \max_{X\succeq0} \log \det(I + B XB^T)\\ \\
& s.t. 
\begin{pmatrix} AXA^T - X + Q & AXB^T \\
BXA^T& I + BXB^T
\end{pmatrix}\succeq0,
\end{align} and the goal is to show $(A - K(X^\ast)B)(A-K(X^\ast)B )^T\prec I$ where $K(X)\triangleq AXB^T(I + BXB^T)^{-1}$ . This is an important consequence in control theory since it implies that the optimal solution $X^*$ is the stabilizing solution for the corresponding system. From here, I elaborate on my modest progress. The constraints can be either written as a ""big LMI"": \begin{align}
R(X)&=\begin{pmatrix} AXA^T - X + Q & AXB^T &0\\
BXA^T& I + BXB^T &0 \\
0&0&X
\end{pmatrix}\succeq 0
\end{align} or \begin{align}
AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T\succeq0\\\
X\succeq0,
\end{align} where we used the Schur complement along with $I+BXB^T\succ0$ . The scalar case (solution): By the KKT stationarity condition \begin{align}\label{eq:lagra_1}
0&= -B^2 -\lambda_1+ \lambda_2\{ 1 - A^2 + 2 A^2B^2X(I + BXB^T)^{-1} - A^2B^4X^2(I + BXB^T)^{-2}\}
\end{align} for $\lambda_1,\lambda_2\ge0$ and correspond to the constraints. If $B\neq0$ , it follows that $\lambda_2>0$ and \begin{align}
0&<1 - A^2 + 2 A^2B^2X^\ast(I + BX^\ast B^T)^{-1} - A^2B^4X^{2\ast}(I + BX^\ast B^T)^{-2}\\
  &= 1 - (A - K(X^\ast)B)^2
\end{align} where $K(X)$ is defined above. If $B=0$ , the stability condition holds only if $|A|<1$ . The combination of this conditions lead to the following known result: there exists a stabilizing solution $X$ iff $(A,B)$ is detectable. The vector case using the Schur complement constraint (unsolved): I could make the following progress, but I am not sure how to complete the proof. The Lagrangian in the vector case is: \begin{align}
L(X,\Lambda_1,\Lambda_2)=  - \log \det(I + B XB^T) - \text{Tr}(X\Lambda_1) - \text{Tr}((AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T) \Lambda_2).
\end{align} From the stationarity condition, \begin{align}
0&=\frac{\partial L(X,\Lambda_1,\Lambda_2)}{\partial X}\\
&= - B^T(I+BX B^T)^{-1}B - \Lambda_1 \\ 
&\ \ - A^T\Lambda_2A + \Lambda_2  + B^TK^T\Lambda_2 A + A^T\Lambda_2 K B  - B^TK^T \Lambda_2 KB  \\
&= - B^T(I+BX B^T)^{-1}B - \Lambda_1 + \Lambda_2 - (A- KB)^T\Lambda_2(A-KB)
\end{align} with $\Lambda_1,\Lambda_2\succeq0$ . By the primal feasibility constraint $X\succeq0$ . Thus, first summand is positive semidefinite so that \begin{align}
(\Lambda_2 - (A-KB)^T\Lambda_2(A-KB)) \succeq 0
\end{align} The complementary slackness conditions read as: \begin{align}
0&= \Lambda_1X\\
0&= (AXA^T-X+Q - AXB^T(I + BXB^T)^{-1}BXA^T))\Lambda_2\\
 &= [(A-KB)X(A-KB)^T - X + Q + KK^T ]\Lambda_2\\
\end{align} I do not know how to proceed from here, but I try to enlighten when I am aiming to arrive. A necessary condition for the existence of stabilizing solution is detectability, i.e., if $Ax = \lambda x$ for a vector $x$ and $|\lambda|\ge1$ then $Bx\neq0$ . Let's try to show this fact using contradiction:
Assume there exists a vector $x\neq0$ such that $Ax = \lambda x$ and $Bx=0$ with $|\lambda|\ge1$ . We can now pre- and post-multiplying the stationarity condition with $x^T$ and $x$ and have \begin{align}
0&= -x^T\Lambda_1x - x^T(I - (A-KB)^T(A-KB))\Lambda_2x
\end{align} The vector case using the big LMI: Without loss of generality, the dual variable is \begin{align}
Z=\begin{pmatrix}
S & U&0\\
U^T & T&0\\
0&0&W
\end{pmatrix}.
\end{align} The Lagrangian in this case is: \begin{align}
L(X,Z)&=  - \log \det(I + B XB^T) - \text{Tr}(R(X)Z).
\end{align} The KKT stationarity condition gives: \begin{align}
0&= - \text{det}(I+BX B^T)\text{Tr}((I+BX B^T)^{-1}BB^T) \\&
\ \ -\text{Tr}( ASA^T - S + B^TU^TA + A^TUB + B^TTB + W )
\end{align} and the complementary slackness condition $R(X)Z=0$ simplifies to: \begin{align}
0&= (AXA^T - X + Q)S + AXB^T U^T\\
0&= (AXA^T - X + Q)U + AXB^T T\\
0&= BXA^T S + (I+BXB^T)U^T\\
0&= BXA^T U + (I+BXB^T)T\\
0&= XW
\end{align} If $B=I$ , it follows that $S\succ0$ (Assume $Sx=0$ for some $x$ and conclude that $x=0$ ). For the general case, $Sx=0$ implies $Bx=0  \& \& Wx=0$ .","['karush-kuhn-tucker', 'linear-matrix-inequality', 'convex-optimization', 'linear-algebra', 'optimization']"
4028084,Ricci curvature tensor arguments,"i do not understand the application of the Weizenböck identity, $$
\Delta = \nabla^*\nabla + \text{Ric},
$$ where $\Delta = \text{d}\delta+\delta\text{d}$ . It is applied like this $$
\eqalign{
(\text{d}\delta\text{d} u)\cdot \text{d}v &= ((\text{d}\delta+\delta\text{d})\text{d}u)\cdot(\text{d}v) \\
&=(\nabla^*\nabla\text{d}u)\cdot(\text{d}v)+\text{Ric}(\text{d}u,\text{d}v)
}
$$ and i do not understand why the second argument in $\text{Ric}$ is $\text{d}v$ and not $\text{d}u$ since the operator that is exchanged operates on $\text{d}u$ . Please correct me :).","['curvature', 'differential-geometry']"
4028096,Meaning of G(k) being dense in G for an algebraic group G/k,"Let $G$ be an algebraic group over a field $k$ of characteristic 0. I have read that: if $G$ is connected, then $G(k)$ is dense in $G$ for the Zariski topology. I do not understand what kind of density this might be since $G(k)$ is abstractly defined as the set of morphisms $\text{Spec}(k) \to G$ . If $G$ is an affine variety in $\mathbb{A}_k^n$ for instance, one might think of $G(k)$ as a subset of $k^n$ . But how does this connect to the abstract definition of $G$ as a scheme or as a functor from $k$ -algebras to groups? I do not see a direct relation or any mean to relate the group $G(k)$ (a set of points) to the abstract scheme (or functor) $G$ , let alone saying that $G(k)$ is dense in $G$ for the Zariski topology. How is this topology even defined on $G(k)$ and how is $G(k)$ even a subset of $G$ ? Could someone please resolve this issue for me? Thank you in advance.","['algebraic-geometry', 'algebraic-groups']"
4028149,Multiplying a Probability distribution function by a constant,"This is a pretty basic question, I understand, but in my head there is something that doesn't make sense. Let's say I have a uniform probability density function X that goes from [0,6]. The pdf would thus be 1/(6-0), which is just 1/6. Now, let's say I multiply this pdf X by 2, to get Y= 2X. Would the pdf then be 2*(1/6)? If so, wouldn't this break the condition that the integral of a pdf has to be equal to 1? What would I do? And for whatever answer I get, would that work for all pdf's (binomial, Poisson, uniform, normal, etc).","['statistics', 'probability', 'density-function']"
4028197,Assume a real-valued net has a cluster point $a\in \mathbb R$. Is there a cofinal subnet converging to $a$?,"Let $(D, \geq)$ be a directed set and let $(n_d)_{d\in D}$ be a real-valued net. Assume $a\in \mathbb R$ is a cluster point of $(n_d)$ , i.e., for every neighborhood $U$ of $a$ and every $d\in D$ there exists $d'\geq d$ such that $n_{d'}\in U$ . It is a standard fact that there exists a subnet of $(n_d)$ that converges to $a$ . But is it possible to find a cofinal subnet (see for example wikipedia ) with the same property? To rephrase: is there a cofinal sub set $D'\subset D$ such that $\lim_{d\in D'}n_d = a$ ? If $(n_d)$ takes value in an arbitrary topological space $X$ , then this is not true. See for example this answer and this one . My question is specific to the case $X=\mathbb R$ , and for this space I could not find any counter-example nor proof.","['elementary-set-theory', 'limits', 'general-topology', 'nets']"
4028224,Relationship Between Power Series ODE Solution Techniques?,"When solving an ODE via a power series at an ordinary (nonsingular) point, the initial guess is $y = \sum_{n = 0}^\infty a_n x^n$ . When solving an Euler ODE, the second order equation $x^2 y'' + pxy' + qy = 0$ where $p$ and $q$ are constant coefficients for example, the initial guess is $y = x^s$ . When solving an ODE via The Method Of Frobenius at a regular singular point, the initial guess is said to be the product of the two prior guesses, $y = x^s \sum_{n = 0}^\infty a_n x^n$ . My question is whether The Method Of Frobenius is a generalization of each of the two prior solution techniques, and if not, what its relationship is to those techniques.  To break this down: Does applying The Method Of Frobenius to an ODE at an ordinary point always produce $s = 0$ with algebraic multiplicity equal to the order of the ODE? Does applying The Method Of Frobenius to an ODE and finding that $s = 0$ with algebraic multiplicity equal to the order of the ODE entail that the point approximated around was ordinary? Does applying The Method Of Frobenius to an Euler ODE always produce a power series factor $\sum_{n = 0}^\infty a_n x^n$ in the guess identically equal to $1$ ? Does applying The Method Of Frobenius to an ODE and finding that the power series factor $\sum_{n = 0}^\infty a_n x^n$ in the guess is identically equal to $1$ entail that the ODE was an Euler ODE? If the answers are not all ""yes,"" then The Method Of Frobenius is not a generalization of each of the other solution techniques.  In this case, what is the correct, high-level way to think about how these solution techniques relate, aside from symbolic similarity?","['frobenius-method', 'approximation', 'ordinary-differential-equations', 'localization', 'power-series']"
4028287,How would I use Green's Theorem to evaluate this Line Integral?,"If I have the closed loop $C$ ( https://ibb.co/hK7xC5V ) which is the union of $C_1,C_2,C_3$ and $C_4$ where $$C_1=\sin(x)-2\pi,\text{where $x$ goes from 0 to $2\pi$}$$ $$C_2=-\sin(y)+2\pi,\text{where $y$ goes from $-2\pi$ to 0}$$ $$C_3=\sin(x),\text{where $x$ goes from $2\pi$ to 0}$$ $$C_4=-\sin(y),\text{where $y$ goes from 0 to $-2\pi$}$$ How would I evaluate $$\oint\limits_C\vec{F}\cdot d\vec{r}$$ using Green's Theorem? (An example would also be appreciated with any field $\vec{F}$ which has nonzero $x$ and $y$ components)","['vector-fields', 'greens-theorem', 'multivariable-calculus', 'vector-analysis', 'line-integrals']"
4028367,Liu's Algebraic Geometry Ex 2.12,"The exercise to be shown is Let $f:X \rightarrow Y$ be a morphism of ringed topological spaces. Let $V$ be an open subset of $Y$ containing $f(X)$ . Show that there exists a unique morphism $g:X \rightarrow V$ whose composition with the open immersion $i:V \hookrightarrow Y$ is f. My attempt so far: Define the map $g$ on topological spaces to be $g(x) = f(x)$ . Define the map $g^\#:\mathcal{O}_V \rightarrow g_*\mathcal{O}_X$ as given by for every $W \subset V$ open, $g^\#_W:\mathcal{O}_V(W) \rightarrow g_*\mathcal{O}_X(W) = \mathcal{O}_X(g^{-1}(W))$ by $s \mapsto f^\#(s)|_{g^{-1}(W)}$ . I'm not sure if that latter map is right. Clearly we have that the composition of the maps of topological spaces is f. But I'm not sure about the map of sheaves. Clearly, we have that $(i \circ g)^\#_V:\mathcal{O}_Y(V) \rightarrow \mathcal{O}_X ((i \circ g)^{-1}(V)) = \mathcal{O}_X(f^{-1}(V))$ goes to the right place. But I'm confused about the induced map of sheaves. Will it is given by $s \mapsto g^\#(i^\#(s)) = g^\#(s) = f^\#_{f^{-1}(V)}(s)$ ? I'm fairly certain I'm misunderstanding something about these maps. Any help understanding the sheaf map and the latter part of this exercise would be appreciated","['algebraic-geometry', 'ringed-spaces', 'sheaf-theory']"
4028472,Can complex sine be understood using the unit circle?,"When working in the real numbers, $\sin x$ has a nice geometric interpretation as the vertical coordinate that you arrive at after tracing an arc of $x$ units anticlockwise around the unit circle. Is there a similar way of defining $\sin x$ when $x$ is a complex number? I’m not particularly interested in the graph of $\sin x$ ; rather, I’m looking for a geometric interpretation of what $\sin x$ ""means"" in a way that is analogous to how we understand it using the unit circle.","['trigonometry', 'visualization', 'geometry', 'complex-numbers']"
4028523,"If $X$ is such that any étale $X$-scheme is locally factorial, is any smooth $X$-scheme locally factorial?","We say that a local ring is geometrically factorial if its strict henselization is a UFD, and that a scheme is geometrically factorial if all of its local rings are. If $X$ is a normal, locally Noetherian and geometrically factorial scheme, is any smooth $X$ -scheme geometrically factorial? $\textbf{Edit:}$ As Minsheon Shin pointed out in his answer, this question is equivalent to the one in the title by Proposition 1 of Danilov, $\textit{On a conjecture of Samuel}$ . In the same article, Danilov conjectures that if $R$ is local, Noetherian and geometrically factorial, then $R[[T]]$ is geometrically factorial; and his main theorem is a special case of this conjecture. If $R$ is excellent, a positive answer to my question would imply the conjecture by https://stacks.math.columbia.edu/tag/07GC . $\textbf{Other equivalent formulations:}$ By the local structure of smooth morphisms+induction, the question is equivalent to: ""If $X$ is a normal, Noetherian, geometrically factorial scheme, then $\mathbb A^1_X$ is geometrically factorial."" Working locally, this, in turn, reduces to: ""If $R$ is a strictly henselian local UFD, then the strict localizations of $R[T]$ at $(\mathfrak m_R,T)$ and at $\mathfrak m_R$ are UFDs."" The strict localization at $(\mathfrak m_R,T)$ has completion $R[[T]]$ so it is a UFD if Danilov's conjecture holds.","['algebraic-geometry', 'commutative-algebra']"
4028533,"Showing that for every $e > 0$, there exist quasi-equilateral triangles with error $e$ whose vertices have integer coordinates in the plane","I'm currently working on the problem stated below, but i'm still kind of a beginner with proofs, so I would appreciate some tips ( not a solution ) on how to approach this proof. A triangle is quasi-equilateral with error $e > 0$ , if, for every angle $A$ , $B$ , $C$ in the triangle, we have that $\vert{A-60^{\circ}}\vert < e$ , $\vert{B-60^{\circ}}\vert < e$ and $\vert{C-60^{\circ}}\vert < e $ . Show that for every $e > 0$ , there exist quasi-equilateral triangles with error $e$ that have vertices with integer coordinates in the plane. I know how to prove that there are no equilateral triangles with integer coordinates, however, this looks much more difficult and I can't find a way to begin.","['euclidean-geometry', 'triangles', 'proof-writing', 'geometry']"
4028568,"Robot moves from $(x,y)$ to $(x+y, y)$ or $(x,x+y)$","I was working on some coding related to this topic I found on Stack Overflow. This lead me to a math problem I thought would be interesting. I was wondering if one was given a starting point, what points could the robot reach. For instance, if the robot started at $(10,15)$ , which coordinates would be reachable. To restate the problem, A robot moves in the following way. If it is at the point $(x,y)$ , it can move to either $(x+y,y)$ or $(x,x+y)$ . If the robot starts at the point $(10,15)$ , what points are reachable in a finite number of moves? I noticed that each move preserves the greatest common divisor of the two coordinates, so all the coordinates that are reachable must have gcd equal to 5. Moreover, for relatively prime integers $a$ and $b$ , if the robot can move from $(a,b)$ to $(c,d)$ , then it can move from $(na, nb)$ to $(nc, nd)$ . Therefore, we just have to consider the coordinates that are reachable from $(2,3)$ and just multiply each coordinate by 5. However, I'm not sure what coordinates are reachable. There are coordinates like $(35, 75)$ which are not reachable, even though their greatest common divisor is 5. Any help on the question would be great. Thanks!","['recursive-algorithms', 'combinatorics', 'recursion', 'algorithms']"
4028574,SUPPOSE versus IF [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question What is the difference between suppose .  .  ., then .  .  . and if .  .  ., then .  .  . ? For instance, in this... ""Let P(n) be a statement that is defined for all nEZ and let a be a fixed integer. Suppose that both of the following statements are true: P(a) is true for every integer, k ≥ a, if p(k) is true then p(k+1) is true. Then the statement: ""for all integers n ≥ a, p(n)"" is true."" Why is it suppose that as opposed to if",['discrete-mathematics']
4028579,Finding probability distribution from given moments,Some quantum mechanics derivation gave me the expectation values of some operator. I got some clean analytical result which makes me think that there is an analytical answer for the probability distribution.The odd moments are all $0$ and the even moments are given by: $$\langle p^{2n}\rangle=2^{-2n}\binom{2n}{n}(k)^{2n}$$ $ghk$ is just some parameter. The most significant result for me will happen if this will result in some probability distribution based on Bessel $J$ functions but anything is good.,"['complex-analysis', 'moment-generating-functions', 'probability-distributions', 'probability-theory']"
4028593,"Numerically evaluating $\iiint_{C} \frac{ {\rm d}x \, {\rm d}y \, {\rm d}z}{x^2 +y^2 + z^2}$","Numerically evaluate $$\iiint_{C} \frac{ {\rm d}x \, {\rm d}y \, {\rm d}z}{x^2 +y^2 + z^2}$$ where $$C = [-1,1] \times [-1,1] \times [-1,1] = [-1,1]^3$$ I do not expect an analytic solution. I would be satisfied with a numeric value. But even this is not so easy. How to deal with the singularity at the origin? One idea is to first integrate over the unit ball, which can be done analytically, then use monte carlo to integrate over the rest irregular region. But I am not sure whether this will converge fast enough.","['integration', 'definite-integrals', 'multivariable-calculus', 'multiple-integral', 'numerical-methods']"
4028633,Which is the algorithm for knn density estimator?,"I am reading Pattern Recognition and Machine Learning by Christopher Bishop. In chapter two he talk about using knn to density estimation. I want to replicate a plot using python/R/matlab.  He is doing it with synthetic data, but I do not know how to update the value V (volume of Region containing X from p(x)) in the following formula $$P(x)=\frac{K}{NV}$$ . I could not find any implementation of this algorithm for density estimation.
This is the plot:","['statistics', 'probability-distributions', 'machine-learning', 'pattern-recognition', 'probability']"
4028661,Proving that this Function is Surjective,"Let's say we have a function $g: \mathbb{R} \rightarrow [0,1)$ and $g$ is defined by $g(x) = x - \lfloor x \rfloor$ . So far I thought about having $g(a) = b$ meaning we find an $a$ that makes $g(a) = b$ . Thus, $$\begin{align*}g(a) = b \text{ where $b\in [0,1)$} \\ g(a) = a- \lfloor a\rfloor = b \\ \end{align*}$$ Here is where I am stuck. How can I choose an $a$ to show that this function is surjective?",['functions']
4028751,"Doubt in handling double sums of the type: $\sum_{n,k\in\mathbb{N}_1}\frac{1}{n^2k^2(n+k)}$","I'm having a little trouble calculating the double series given by the proposition below: $$\int_0^1\frac{\left[\text{Li}_2(x)\right]^2}{x}\mathrm dx=\sum_{n=1}^\infty\sum_{k=1}^\infty\frac{1}{n^2k^2(n+k)}=2\zeta(2)\zeta(3)-3\zeta(5).\tag{1}$$ In fact, this integral has already been evaluated here and here , but i have no intention of evaluating it by integration or by using harmonic series. My interest is to attack the double sum in $(1)$ only using the manipulation of sums. Having clarified my interest in this topic, i naively proceeded as follows: $$\begin{align*}\sum_{n=1}^\infty\sum_{k=1}^\infty\frac{1}{n^2k^2(n+k)}&=\sum_{n,k\in\mathbb{N}_1}\frac{1}{n^2k^2(n+k)}\\
&=\sum_{\substack{{n,k\in\mathbb{N}_1}\\{n=k}}}\frac{1}{n^2k^2(n+k)}+\sum_{\substack{{n,k\in\mathbb{N}_1}\\{n>k}}}\frac{1}{n^2k^2(n+k)}+\sum_{\substack{{n,k\in\mathbb{N}_1}\\{n<k}}}\frac{1}{n^2k^2(n+k)}\\
&=\frac{\zeta(5)}{2}+\sum_{\substack{{n,k\in\mathbb{N}_1}\\{n>k}}}\frac{1}{n^2k^2(n+k)}+\sum_{\substack{{n,k\in\mathbb{N}_1}\\{n<k}}}\frac{1}{n^2k^2(n+k)}\tag{a}\end{align*}$$ Here's my difficulty with that approach! How can i solve the last two double sums in $(\text a)$ with the indexes $ n <k $ and $ n> k $ ?. Studying other posts (links at the end of the question) related to my question, i found here another possible approach using the fact that: $ \frac{1}{n^2k^2}=\frac{1}{(n+k)^2n^2}+\frac{1}{(n+k)^2k^2}+\frac{2}{(n+k)^3k}+\frac{2}{(n+k)^3n}$ , then multiplying by $\frac{1}{n+k}$ on both sides and taking the double sum, we get: $$\sum_{n,k\in\mathbb{N}_1}\frac{1}{n^2k^2(n+k)}=2\sum_{n,k\in\mathbb{N}_1}\frac{1}{(n+k)^3n^2}+4\sum_{n,k\in\mathbb{N}_1}\frac{1}{(n+k)^4n}.\tag{b} $$ According to this article , $(\text b)$ is equal to $T(2,2,1)=2T(2,0,3)+4T(1,0,4)$ , where $T(a,b,c)$ is Tornheim’s double series defined by $T(a,b,c)=\sum_{n,m=1}\frac{1}{n^am^b(n+m)^c}$ . In this second approach, the expression in $(\text b)$ seems to me a little less aggressive, but if i apply the concepts of the mentioned article, i would feel a little difficult to understand, since it starts with already established formulas. In short, two questions hang in my mind: How to work $(\text a)$ and $(\text b)$ without using integrals or harmonic numbers? Related links: A. $\displaystyle \sum_{m,n=1\, m\neq n}^\infty{\frac{m^2+n^2}{mn(m^2-n^2)^2}}$ B. $\displaystyle \sum_{k=1}^{\infty} \sum_{n=1}^{\infty} \frac{1}{n^2k^2(n+k)^2}$ C. $\displaystyle \sum_{k=1}^{\infty} \frac{(-1)^{k+1} H_k}{k} = \frac{1}{2} \zeta(2) - \frac{1}{2} \log^2 2$","['harmonic-numbers', 'definite-integrals', 'sequences-and-series']"
4028768,"Usefulness of the ""Lagrange function"" $\Lambda(\mathbf x,\mathbf \lambda) = f(\mathbf x) - \sum_{i=1}^m \mathbf \lambda_i \Phi_i(\mathbf x)$","Suppose that our surface $M\subset \Bbb R^n$ (or manifold) is defined to be the level set of a sufficiently nice function $\Phi:\Bbb R^n\to \Bbb R^m$ ( $m<n$ ), i.e. $M = \{\mathbf x\in\Bbb R^n: \Phi(\mathbf x) =\mathbf 0 \}$ . Let $f:\Bbb R^n\to\Bbb R$ be a $C^1$ function, we wish to solve the problem $$\begin{align} 
\text{minimize}\quad &f(\mathbf x) \\
\text{subject to}\quad &\mathbf x\in M.
\end{align}$$ It is well-known that if $\mathbf x^*\in M$ is a solution to the above problem, then there exists a vector $\mathbf \lambda^*\in\Bbb R^m$ (or a real number if $m=1$ ) such that $$\tag{*}\label{a}
\nabla_{\mathbf x} f(\mathbf x^*) = \sum_{i=1}^m \lambda^*_i \nabla_{\mathbf x}\Phi_i(\mathbf x^*).
$$ This is called the method of Lagrange multipliers . It is also very popular to rewrite the above using the auxiliary function called the Lagrange function $\Lambda:\Bbb R^{n+m}\to \Bbb R$ defined as $$
\Lambda(\mathbf x,\mathbf \lambda) = f(\mathbf x) - \sum_{i=1}^m \mathbf \lambda_i \Phi_i(\mathbf x),
$$ so that \eqref{a} and the constraint $\mathbf x^*\in M$ can be recasted as $$
\nabla_{(\mathbf x,\mathbf \lambda)} \Lambda(\mathbf x^*,\mathbf \lambda^*) = \mathbf 0,  
$$ or written separately as $$
\partial_{ x_j} \Lambda(\mathbf x^*,\mathbf \lambda^*) = 0, \quad  
\partial_{\lambda_i} \Lambda(\mathbf x^*,\mathbf \lambda^*) = 0 \quad\text{for all $j=1,\dots,n$ and $i=1,\dots,m$}.
$$ I have always found the formulation $\nabla_{(\mathbf x,\mathbf \lambda)} \Lambda(\mathbf x^*,\mathbf \lambda^*) = \mathbf 0$ to be a bit unenlightening. To me it is nothing but a shorthand of writing \eqref{a} and the fact that $\mathbf x^*\in M$ together in a single line, and at times even obscures the important geometric meaning that \eqref{a} conveys (from my experience of being a TA on this subject). Hence my questions today: -Is there anything inherently interesting about the function $\Lambda(\mathbf x,\mathbf \lambda)$ itself (beside the fact that it's an auxiliary function for the Lagrange multiplier method)? -Is there any geometric (or otherwise) intuition that is naturally associated with $\Lambda(\mathbf x,\mathbf \lambda)$ ? -Is there a better answer I can give to my students when they ask ""What does $\Lambda(\mathbf x,\mathbf \lambda)$ represent?"" except ""It's just a shorthand for writing \eqref{a}.""? I'm hopeful that there's probably some good ways of interpreting $\Lambda(\mathbf x,\mathbf \lambda)$ that I'm not aware of yet. Personally, I've been trying to avoid using it altogether if I can because I'm not sure what to make of it. Today might be a good day to fix that.","['lagrange-multiplier', 'real-analysis', 'multivariable-calculus', 'intuition', 'differential-geometry']"
4028841,A conditional probability problem where the next day depends on the last 3 days,"For many years, Meteorologists have spent long visits (5 days) at the Bigtown.
They have observed that, for three consecutive days, if there are EXACTLY two sunny days, the next day is a sunny day*, while half of all the cloudy days are followed by another cloudy day. Assuming days are either cloudy or sunny, estimate how many days have been cloudy in their last ten visits. Visits are not necessarily following each other. ''half of all the cloudy days are followed by another cloudy day'' so $P(C\mid C)=50\%$ Is this possible? If not why not? *i.e.
SSC -> S SCS -> S CSS -> S where -> means followed by","['conditional-probability', 'markov-chains', 'probability']"
4028873,Why don't introductory calculus textbooks usually introduce the notion of differentiability on closed intervals?,"The title says it all. I find the apparent neglect of this idea rather unfortunate because (1) the notion of a function being differentiable on a closed interval is intuitively reasonable (and ought to be discussed), and (2) several theorems from calculus can be strengthened if they're stated in terms of closed interval differentiability. Before proceeding further, it's worth taking time to make sure we agree on what we mean we say that a function is ""differentiable over a closed interval"". Here's the definition I had in mind: A function $f:[a,b]\to\mathbb{R}$ is differentiable over $[a,b]$ if it is differentiable (in the ordinary sense) over $(a,b)$ , right-differentiable at $a$ , and left-differentiable at $b$ . As you can probably guess, a function $f$ is said to be left-differentiable at a number $x_0$ if the limit $$\lim_{h\to 0^-}\frac{f(x_0+h)-f(x_0)}{h}$$ exists, with a similar definition applying to right-differentiability. The limits are then defined to be the left-hand and right-hand derivatives of $f$ , respectively. With that said, here's an example of a theorem (FTC1) that, under its usual hypotheses, can be strengthened (albeit slightly) if the notion of closed-interval differentiability is used: If $f:[a,b]\to\mathbb{R}$ is continuous, then the function $F:[a,b]\to\mathbb{R}$ defined by $F(x)=\int_{a}^{x}f(t)\text{ }dt$ is differentiable over $[a,b]$ . Moreover, for all $x\in[a,b]$ , $F'(x)=f(x)$ (at the endpoints, $F'(x)$ is understood to be a left or right-hand derivative) The return you get by applying this notion to this example is obviously minimal. That said, I truly believe that making these extra definitions is worthwhile anyway, namely because the resulting concepts strongly align with our intuitions. To bring everything back together, I'll restate my question one last time: why don't introductory calculus textbooks usually introduce the notion of differentiability on closed intervals? Any and all responses are greatly appreciated. Note : the ""theorems"" I mentioned earlier do not include existence theorems like Rolle's theorem or the MVT. I understand why these only require differentiability in the interior and why we shouldn't strengthen that requirement.","['calculus', 'soft-question', 'derivatives']"
4028882,Need help understanding the logic behind the intersection of infinite sets.,"As the post topic states, I need some help understanding the logic behind the intersection of infinite sets. I will take this time to thank anyone who reaches out with a comment or answer. The following example has been taken from a real analysis book I've purchased. Say the following sets exist, $$ A_1 = N = \{1, 2, 3, ...\}\\
A_2 = \{2, 3, 4, ...\}\\
A_3 = \{3, 4, 5, ... \}\\
A_n = \{n, n+1, n+2, ...\}$$ Now, i'm okay with this. $$\bigcup_{n=1}^{\infty}A_{n} = A_1$$ This is what I can't seem to grasp. $$\bigcap_{n=1}^{\infty}A_{n} = \emptyset$$ I can't make a mental picture of this since we have literally an infinite number of sets. I can see that $A_1 \cap A_2 = A_2$ applying that logic to each of these sets, which are by definition ""infinite"" there will always be some set $A_{n+1}$ for every $A_{n}$ So how can this ever be the empty set?","['elementary-set-theory', 'real-analysis']"
4028938,How many times does the line $y=\frac{1}{4} x$ cut the curve $y=\sin(x)+\cos(2x)$?,"This is a multiple choice question, where using calculators is not allowed. Candidates have, on average, two minutes to solve the problem. PROBLEM: How many times does the line $y=\frac{1}{4} x$ cut the curve $y=\sin(x)+\cos(2x)$ ? CHOICES: (A) $4$ (B) $5$ (C) $6$ (D) $7$ (E) $8$ What I have tried was not feasible: Using differentiation, I found that the curve is bounded between $y=\frac{9}{8}$ and $y=-2$ . When $x<-8$ , $\frac{1}{4}x<-2$ , and when $x>\frac{9}{2}$ , $\frac{1}{4}x>\frac{9}{8}$ . So, any solution to the equation $y=\sin(x)+\cos(2x)=\frac{1}{4}x$ must lie in the interval $(-8,\frac{9}{2})$ . I do not know how to proceed from these two points. Even I know, that takes a long time. Also, I do not know if using the fact that the curve is periodic will really help. By Desmos, I took a look to the graph, the line cuts the curve $7$ times. So the correct choice is $D$ . Any help on how to solve this would be appreciated. THANKS!","['periodic-functions', 'trigonometry', 'systems-of-equations']"
4028957,"Given a sequence of sets $\{E_j\}$, define $F_1=E_1$ and $F_j=E_j-(E_1\cup\dots\cup E_{j-1})$ for $j>1$. Why does $\bigcup E_j=\bigcup F_j$?","$\{ E_j \}_{j=1}^{\infty}$ : sequence of sets We define $\{ F_j \}_{j=1}^{\infty}$ with \begin{align}
&F_1=E_1 \\
&F_j=E_j-(E_1 \cup \cdots \cup E_{j-1}) \ (j\geqq 2)
\end{align} Then, prove that ・ $ F_j \cap F_k= \varnothing$ for $j\neq k$ . ・ $\bigcup_{j=1}^{\infty} E_j =\bigcup_{j=1}^{\infty} F_j $ . I could prove $ F_j \cap F_k=\varnothing$ for $j\neq k$ but I cannot prove $\bigcup_{j=1}^{\infty} E_j =\bigcup_{j=1}^{\infty} F_j $ . Because $F_j=E_j-(E_1 \cup \cdots \cup E_{j-1})\subset E_j$ for all $j$ , $\bigcup_{j=1}^{\infty} F_j \subset\bigcup_{j=1}^{\infty} E_j $ holds. I have no idea to prove $\bigcup_{j=1}^{\infty} F_j \supset\bigcup_{j=1}^{\infty} E_j .$",['elementary-set-theory']
4028997,Can the tangent line be defined independently of the derivative?,"The graph of the function $f:x \mapsto x^{1/3}$ has a 'vertical tangent' at $x=0$ : Although this idea is certainly geometrically sound, from what I understand the tangent line is defined by the derivative, not vice versa. In other words, the tangent line to a function at the point $(a,f(a))$ is simply the line given by the equation $$
y - f(a) = f'(a)(x-a) \, ,
$$ where $f'(a)$ is of course defined as a limit. Since $f'(0)$ does not exist in this case, I'm unsure if we can truly say that the graph has a vertical tangent. The intuitive idea of a tangent 'just touching' the curve breaks down when we consider, for instance, the graph of a linear function, where the tangent touches the graph of the function itself at infinitely many points. Nevertheless, I have heard people say that the tangent line is fundamentally a geometric concept. Although the slope of the tangent line 'agrees' with the derivative if the derivative exists, there are instances where the tangent line is a meaningful concept even when the derivative doesn't exist. If this be the case, then what is the formal definition of a tangent?","['calculus', 'derivatives', 'tangent-line']"
4029021,Derivative and inverse,"Is there a function whos derivative of its inverse equals the inverse of its derivative? This may also hold on a specific interval only. $$\frac{d}{dx}f^{\langle-1\rangle}(x)=\left(\frac{d}{dx}f(x)\right)^{\langle-1\rangle}$$ Considering that $f(x)=x$ is its own inverse and $e^x$ is its own derivative, neither of them satisfies the equation, but both are the only functions with the respective properties (over the reels). Is this already a proof that there is no solution to the above equation? Also, I dont know much about complex analysis but if a complex-valued function meets the criteria I'd be happy to know too :)","['complex-analysis', 'inverse-function', 'derivatives', 'real-analysis']"
4029101,"If $X$ and $Z$ are independent: $\mathbb{E}(Y|X) = \mathbb{E}(Y) \implies \mathbb{E}(Y|X,Z) = \mathbb{E}(Y|Z)$?","My question is, given that $X$ and $Z$ are independent is it true that $$
\mathbb{E}(Y|X) = \mathbb{E}(Y) \implies \mathbb{E}(Y|X,Z) = \mathbb{E}(Y|Z) 
$$ I do not seem to be able to prove it but I cannot come up with a counterexample or disprove it. Does the answer change if on top of $X$ and $Z$ being independent we also impose that $\mathbb{E}(Y|Z) \neq \mathbb{E}(Y)$ ?","['independence', 'probability']"
4029156,"Set theory, injective and surjective functions task","Prove that if $$|Z \times Z| = |X \cup Y| $$ then: there exists injective function (one-to-one) $f: Z \rightarrow Y$ OR there exists a surjective function $k : X \rightarrow Z$ . If Z is finite, I've come with this: If $|X| \geq|Z|$ , then there is a surjective  for X to Z.
If $|X| < Z$ then $|Y| >=|Z|$ so there is an injective function from Z to Y. But I can't express it in the language of set theory.
And I can't figure out when Z is infinite. Any tups?",['elementary-set-theory']
4029186,A very strange continuous stochastic process: can we build one ? Does it exist?,"Let $(\Omega, \mathcal A, \mathbb P)$ be a probability space and $X_t:\Omega \times \mathbb [0,T] \rightarrow \mathbb R$ a continuous stochastic process with the following property: There exists an $n>1$ such that for any real sequence $0<s_1<\ldots<s_n<t<T$ , denoting $\mathbb X=(X_{s_1},\ldots, X_{s_n})$ and $\mathbb A_i= Cov[X_{s_i},\mathbb X]$ , we have $Cov[X_{t},\mathbb X]= \sum\limits_{i=1}^n a_i \mathbb A_i$ for some real coefficients $a_i \neq 0 $ for all $i=1,\ldots, n$ , and if we remove any term from $\sum\limits_{i=1}^n a_i \mathbb A_i$ , then the equation has no solutions anymore. Does such stochastic process exist ? If so, can you give an example ? If not, why ? For $n=1$ , the Brownian bridge has this property. I would be very happy to find such a process for $n=2$ , and over the moon if we can build such a process for any $n$ . EDIT: In the case of Brownian bridge (so $n=1$ ), we have (writing $s_1$ as $s$ ) $Cov[X_{t},\mathbb X]=Cov[X_{t},X_s]$ and $\mathbb A_1= Cov[X_{s}, X_s]$ . We get $$Cov[X_{t},\mathbb X]= \frac{T-t}{T-s} \mathbb A_1$$ so $a_1=\frac{T-t}{T-s}$ which is never $0$ .","['covariance', 'probability-distributions', 'stochastic-processes', 'probability-theory', 'probability']"
4029205,Prove that a subsequence converges.,"Suppose that $b_n^{2} \rightarrow4$ . Prove that $b_n$ has a
subsequence that converges to $2$ or $-2$ . I'm not really sure on the approach, I see that simply taking the root yields something similar, but doesn't say much about a subsequence and doesn't really seem like a proof.","['cauchy-sequences', 'analysis', 'sequences-and-series']"
4029243,What is an example of an etale morphism which does not locally factor as a composition of an open immersion and a finite etale map?,"Let $f:X\rightarrow Y$ be an etale morphism of schemes. By $f$ factoring locally as a composition of an open immersion and a finite etale map, I mean that for every $x \in X$ there exists an open $U$ of $x$ and an open $V \subset Y$ containing $f(U)$ such that the restriction $f:U \rightarrow V$ factors as $U \hookrightarrow W \rightarrow V$ , where $U \hookrightarrow W$ is an open immersion and $W \rightarrow V$ is finite etale. According to section 7.5 of the ""Berkeley lectures"" by Scholze and Weinstein, it is not true that every etale $f$ factors locally as a composition of an open immersion and a finite etale map in the category of schemes (though it is true in the category of analytic adic spaces). What is an example of this failure for schemes? Thanks!",['algebraic-geometry']
4029255,Critical points of polar curve of given ode,"How to find maximum minimum radii and inflection point of a curve with the polar symmetry given by the following ode ? Primed on arc,the angle $\psi$ is made between curve and radial lines. $\theta \text{=const.}$ $$ 2a \psi'=\left(1-\dfrac{T^2}{r^2}\right);\;r'= \cos \psi \; \tag1 $$ Patterns above are drawn for constants $ a=1,T= (4,2) ,\theta_i=\pi/4, r_i=1,s_{max=}=320 a, $ after numerical integration. The outer maximum radii  are $ (\approx \sqrt2,2)$ which could not be found analytically. In polar coordinates ... is the following true for local straightness ? $$\psi^{'}_{inflection}= {0}. \tag 2 $$ Pictorially it does not agree for the first pattern (no inflection), but seems a better tally for the second case where: $$ r_{inflection} =T \;$$ To find radial extrema $$\cos \psi=0,\; \psi= \frac{\pi}{2}, \frac{3\pi}{2}..\; \tag 3$$ Differentiating (1) $$ 2a \psi{''}=\dfrac{2T^2\cos \psi}{r^3} \tag4$$ from (3) $$ \psi^{''}=0 \tag5 $$ Is it then a correct criterion for finding max/min radii in general ? And further on how can we find $ (r, \theta ) ? $ Towards a solution for $\psi$ Let $ u=\dfrac{\sin \psi}{r} \to u'= ( r \cos \psi\psi^{'} -\sin \psi \cos \psi)/r^2 \tag6$ Eliminating $ \psi^{'}$ between (1) and (6) and simplifying we obtain $$\dfrac{u'}{\sqrt{1-u^2r^2}}+ \dfrac{u}{r}= \dfrac{1-T^2/r^2}{2ar} \tag 7 $$ whose integral analytical solution $ u=f(r) $ is required. Thanking you for the attention.","['polar-coordinates', 'ordinary-differential-equations', 'differential-geometry']"
