question_id,title,body,tags
2433034,Understanding Scatterplot relationships and its causes,"I'm trying to get started with statistics and probability and stumbled upon a scatterplot. I just don't understand the answer of the question, it's probably pretty easy but I just don't get it. Your mathematical understanding can hopefully help me. Question with Scatterplot I get why a and c is wrong and why b is correct. But I don't get why d and e is not correct. If x increments, y increments too. So if the head length increments, the skull width increments too. That's what the chart shows, doesn't it? In addition, would this Skull width and head length are positively associated. be correct? Why, or why not? Kind regards! Source: Openintro.org, Page 12","['statistics', 'probability', 'graphing-functions']"
2433038,"Prove that $D:C^1[0,1]\to C[0,1]$ is not continuous.","Let $C^1[0,1]$ be the space of continuous real valued functions on $[0,1]$ with continuous first derivative. Let $D:C^1[0,1]\to C[0,1]$ be the differentiation operator given by $D(f)(x)=f^\prime(x),\forall x\in [0,1]$. Suppose both spaces are given the sup norm $\|.\|_\infty$. Now it is required to prove that $D$ is not continuous. The following is my attempt: $D$ is a linear operator by the properties of differentiation. Hence $D$ is continuous if and only if $D$ is bounded on $C^1[0,1]$. Suppose $D$ is bounded. Then there exists $k>0$ such that for each $f\in C^1[0,1]$, $\|D(f)(x)\|_\infty\leq k\|f(x)\|_\infty$, i.e. $\|f'(x)\|_\infty\leq k\|f(x)\|_\infty$. Consider the function $g(x)=\sin(357kx)$ for each $x\in[0,1]$. Clearly $g\in C^1[0,1]$. Therefore $357k\leq\|g'(x)\|_\infty\leq k\|g(x)\|_\infty \leq k.1=k$ which is a contradiction. Therefore $D$ is not bounded and hence not continuous. Could someone please tell me if my argument is correct? Thanks.","['functional-analysis', 'proof-verification']"
2433051,Sine inequality: How to prove that $|\sin(x)| \le |x|$ for $ x \in \mathbb{R}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Following Wolfram Sine inequalities I found that $$|\sin(x)| \le |x| \quad \text{for}  \quad x \in \mathbb{R}$$
How can I prove this relation?","['inequality', 'trigonometry', 'calculus']"
2433055,Group isomorphic to its automorphism group,"Any complete group (that is, with trivial center and outer automorphism group) is isomorphic to its automorphism group. The inverse is not true, as the dihedral group of order 8 is isomorphic to its automorphism group but isn't complete. Problem 15.29 in the Kourovka Notebook asks to find another exemple of a p-group, but I was wondering, is any other exemple known? Is there any other finite (or even infinite) group (not necessarily a p-group) which is isomorphic to its automorphism group but isn't complete?","['finite-groups', 'abstract-algebra', 'group-theory', 'automorphism-group']"
2433152,"Point by point, the transformation $T(x, y) = (4x - y, 3x -2y)$ sends the line $x + 2y = 6$ onto an image line. What is the slope of the image?","The question is as follows: Point by point, the transformation $T(x, y) = (4x - y, 3x -2y)$ sends the line $x + 2y = 6$ onto an image line. What is the slope of the image? I don't understand what this question basically. How is it that the transformation is able to send a line into an ""image line"" (a term that I am not even familiarized with)? Any help will be greatly appreciated.","['transformation', 'geometry']"
2433186,"$(a-3)^3+(b-2)^3+(c-2)^3=0$, $a+b+c=2$, $a^2+b^2+c^2=6$. Prove that at least one from $a,b,c$ is 2.","Assume that $\{a,b,c\} \subset \Bbb R$ , $(a-3)^3+(b-2)^3+(c-2)^3=0$ , $a+b+c=2$ , $a^2+b^2+c^2=6$ . Prove that at least one of the numbers $a, b, c\ $ is 2. This is from a list of problems used for training a team for a math olympics. I tried to use known Newton identities and other symmetric polynomial results but without success (perhaps a wrong approach). Sorry if it is a duplicate. Hints and answers are always welcomed. Edit: There is a problem with the original statement of the question in the original source. Under these assumptions it is impossible to have $a, b, c$ with value 2, as spotted in the comments and proved by the answers below","['algebra-precalculus', 'contest-math']"
2433195,Limit of $\frac{\sqrt{|x |-\lfloor x\rfloor }}{x^2}$ using only definition,"How can I show that $$\lim _{|x|\to \infty \:}\frac{\sqrt{\left|x\right|-\lfloor \:x\rfloor }}{x^2}=0$$ by using only definition of limit $$  \lim _{x\to +\infty}f(x)=L \iff \forall \varepsilon >0\quad \exists A >0\quad \forall x\in U\quad x> A \Rightarrow |f(x)-L|<\varepsilon $$ As first step I tired to show that $$\left|\frac{\sqrt{\left|x\right|-\lfloor \:x\rfloor }}{x^2} -0 \right| \leq \frac{1}{x^2}$$
by with no luck 
indeed, $$ x-1 <\lfloor x \rfloor \leq x < \lfloor x \rfloor+1 $$
$$x-1 <\lfloor x \rfloor \leq x \implies -x \leq -\lfloor x \rfloor <|x|+x-1 \implies 
 |x|-x \leq |x|-\lfloor x \rfloor <x-1 $$ Second step 
we have for all $x >0$
$$\left|\frac{\sqrt{\left|x\right|-\lfloor \:x\rfloor }}{x^2} -0 \right| \leq \frac{1}{x^2}$$
Let $\varepsilon >0$","['epsilon-delta', 'limits-without-lhopital', 'calculus', 'limits']"
2433253,Prove that $\sqrt {f(x)}$ is Lipschitz,"Let $f(x)\in C^2(\mathbb{R}), f(x)\geq0,f''(x)\leq1,$ prove that $\sqrt{f(x)}$ is a Lipschitz function . I can prove that $f(x)$ is uniformly continuous by the inequality without the condition of the $f''(x)$, so I want to ask someone for a better answer. Thanks in advance.","['real-analysis', 'lipschitz-functions', 'analysis']"
2433265,"$f : \mathbb R^n\to\mathbb R$ is differentiable at $c$ if and only if $f(c+h)-f(c) = o(\langle v, h\rangle)$ for some $v$ : constant vector","This question is from Introduction to Mathematical analysis, Steven A. Douglass Exercise 8.3 (d). Let $f$ be a real-valued function defined on an open set $U$ in $\mathbb R^n$. Let $c$ be a point of $U$. Show that $f$ is differentiable at $c$ in $U$ if and only if $f(c+h) - f(c) = o(\langle v ,h\rangle)$ for some constant vector $v$ in $R^n$.
(The vector $h$ is the variable.)
($\langle v,h\rangle$ denotes the inner product of $v$ and $h$.) I tried to solve the question, but I couldn't prove both direction, and here's what I tried. $f$ is differentiable at c $\Rightarrow$ for every $\varepsilon > 0,\  \exists \delta>0,\  |f(c+h)-f(c)- \langle \nabla f(c) , h\rangle| \le \varepsilon|h|$. Since I know that there exists some vector $v$ $s.t$
for every $\varepsilon > 0$, $\exists \delta>0,\ |f(c+h)-f(c)- \langle v , h\rangle| \le \varepsilon|h|$ $\Rightarrow$ $f$ is differentiable at $c$,
I wanted to show the problem's condition for vector $v$ and the equation above are equivalent, but I think it doesn't work. From the differentiablity, I know that $|f(c+h)-f(c)|$ $\le$ $|\langle \nabla f, h\rangle|+ \varepsilon |h|$ as |h| goes to $0$. Of course, dividing by $|\langle \nabla f, h\rangle|$ does not satisfy the condition of the problem. Even if we divide the both side by $|\langle v , h\rangle|$ for general vector $v$, the term $\varepsilon \frac{|h|}{\langle v,h\rangle}$ is being arbitrarily large if vector $v$ is perpendicular to $h$, so I even doubt if the 
theorem of the problem is true.
Any ideas for the question?","['multivariable-calculus', 'analysis', 'derivatives']"
2433270,How many ways are there to deal 13 cards from a deck to 4 players s.t. none receive all hearts?,"Out of a 52 card deck, how many ways can we deal 13 cards to four players and none of them receive a hand of all hearts? My approach thus far has been to take the total number of ways to deal each player a 13-card hand, that is 
$${52 \choose 13} \cdot {39 \choose 13} \cdot {26 \choose 13}$$ 
and subtract the number of ways that you could have a hand of all hearts at each step (which I believe is just one?). Is this intuition correct? Any ideas on how to proceed? I'm very new to combinatorics/discrete math and would appreciate any advice. Thanks!","['combinatorics', 'discrete-mathematics']"
2433305,Summation of Floor Function series,"Let $p, q$ be co-prime natural numbers. Show that they satisfy : $$\sum_{k=1}^{q-1}\left\lfloor\frac{kp}{q}\right\rfloor=\sum_{k=1}^{p-1}\left\lfloor\frac{kq}{p}\right\rfloor=\frac{(p-1)(q-1)}{2}$$ There exists a proof using lattice points. 
But I would like to see a normal proof using the basic way of solving integer functions like taking $\left\lfloor\frac{p}{q}\right\rfloor = x$, i.e, $p=qx+r \ni r < x$ and then substitution or whatsoever. In case someone is interested in the lattice proof, here it goes:
Consider a the lattice points with $1\leq x \leq q-1, 1\leq y \leq p-1$. They lie inside the rectangle $OABC$ where $O$ is the origin, $OA=x-\text {axis}$ and $OB=y-\text {axis}$. Here, $|OA|=q, |OC|=p$. None of the considered lattice points $\in$ diagonal $OB$. This would contradict $\gcd(p,q)=1$. We doing the number of lattice points below $OB$ in two ways: 1) $\text {Their count is } \dfrac{1}{2}(p-1)(q-1)$ 2) $\text {It equals} \displaystyle\sum_{k=1}^{q-1}\left\lfloor\frac {kp}{q}\right\rfloor$","['number-theory', 'integers', 'integer-lattices', 'prime-numbers', 'elementary-number-theory']"
2433322,What does power-set mean here?,"Let $\mu (V) = V_1 \cup V_2 \cup \cdots \cup V_k$, where each $|V_i| \le n$; means each $V_i$ contains at most $n$ points and Intersection between any two $V_i$ and $V_j$ is empty. What does $P(\mu (V))$ mean? Does it mean $P(V_1,V_2, \ldots, V_k)$ or anything else? Is it true that a typical element in power-set will be $\{V_1,V_2\}$.
Are these two valid elements in a power -set?","['terminology', 'notation', 'elementary-set-theory']"
2433326,Smallest root of a set of polynomials,"I am looking for $$ t:=  \inf\{\alpha \in \mathbb{R} : \exists P \in \mathcal{P}_{0,1}, \, P(\alpha)=0\}$$ where $\mathcal{P}_{0,1}$ is the set of all real polynomials with coefficients in $\{0,1\}$. By Rouche's theorem, it can be easily shown that the roots of any polynomial of $ \mathcal{P}_{0,1}$ lie inside the open disk of radius $2$ centered at the origin. I believe though $t$ is actually equal to $-1$ (I cannot find any polynomial with a smaller real root... ) Does anyone have an idea?","['complex-analysis', 'roots', 'polynomials']"
2433344,flat connection has zero christoffel symbols in some coordinate,"Let $M$ be a smooth manifold and $\nabla$ be a covariant derivative on its tangent bundle. The question is that if $\nabla$ is flat then is there any locally  coordinate system on $M$ such that in this coordinate the christoffel symbols are zero? In the case that $\nabla$ is the Levi-Civita connection of a Riemannian manifold, then the answer is yes.","['riemannian-geometry', 'smooth-manifolds', 'differential-geometry', 'vector-bundles', 'connections']"
2433366,"If you know all angles and the area of the triangle, how do you find the sides?","If we know the angles and area of the triangle, what would be the formula to find the side lengths? For example, we know the area is 400 ft and the angles are 40, 30, and 110 degrees, how could we find the area? I've been stumped with this question and could really use a suggestion to help me figure out.
Thanks","['trigonometry', 'triangles', 'geometry']"
2433389,How does the identity follow?,"I can show the simple algebra for the first part, but then having issues showing, how $|z_1||z_2| = |z_1z_2|$ follows from it?","['complex-numbers', 'algebra-precalculus', 'complex-analysis', 'proof-writing', 'discrete-mathematics']"
2433417,"If $f(x+1) +f(x-1) =\sqrt{3}\,f(x)$ and $f(2) =2$, what is the value of $f(4)$?","My Attempt $f(2)=2$. So,  $f(1) + f(3)=2\sqrt{3}$ and  $f(2) + f(4)=\sqrt{3}\,f(3)$.  After solving these equations I got the value of $f(3)=2\sqrt{3}$ and $f(4)=4$.  But are there any other methods than this? Any suggestions are welcome. Update:- @ProfessorVector pointed out that the above solutions are only true if $f(1)=0$. After checking I find that it is true. So, my above attempt is a failure. Is there a way to solve this question? Update 2:- Is there a way to find the period of this function?","['algebra-precalculus', 'functions']"
2433482,Sum of best X dice in Y dice rolled (or roll X pick best Y) odds/calculation,"Background: In many pen and paper RPGs there is often an option or bonus/penalty to rolls that incorporates rolling multiples of the required die and taking the best or worst of those rolls for your roll. Example: Advantage/Disadvantage in D&D 5e is best/worst die in 2 twenty-sided die rolls. This generally alters the average roll (of 1d20 or one twenty-sided die equaling 10.5) by 3.325 total (average of 13.825 with Advantage, 7.175 with Disadvantage). Example 2: Best two dice of three six-sided dice rolled. This usually improves the average roll (of 2d6 or two six-sided dice summed, 7) by 1.45833 (8.4583 3-repeating if best, 5.5416 6-repeating if worst) total. Example 3: Best two of four six-sided dice rolled. This usually improves the average roll (of 2d6 or two six-sided dice summed, 7) by about 2.344136 (9.344136 if best, 4.655864 if worst) total. Questions: I was wondering what the specific means of calculating certain total rolls (such as rolls totaling a specific number, or that number and higher) and averages would be? Or if that was more easy to calculate than simply finding all the possible combinations and totaling them. Part A: Given X best of Y dice with Z number of sides, is there a simple expression one can write to calculate this easier than just summing all the combinations? Part B: Given the above scenario, is there a simple expression one can write to calculate the number of rolls at/equaling a specific number? Part C: Given the above scenarios, is there a simple expression one can write to calculate the number of rolls at/equaling or above a specific number? Other notes: I imagine that this will include a lot of factorial math and the Permutation/Combination functions, but I'm not sure how to proceed.","['permutations', 'statistics', 'dice', 'combinations']"
2433528,cardinality of the complement of a countable subset of R,"The excercise is to prove that for any countable set of $\mathbb{R}$ , let s call it A, its complement $\mathbb{R}/A$ has the same cardinality as $\mathbb{R}$. The solution I am aware of is straightforward, assuming countable choice, making a second countable subset, taking the union of those two sets, assuming a bijection and so on, but i wanted to try something diffirent, without assuming choice, and i don't know if it s correct.
My thought is that for any countable set, i can write it down as $A=\{a_0, a_1,....\}$ but not necceserily in increasing order. If it was in increasing order, i could simply say that $(a_0, a_1) \in \mathbb{R}-A$ and thus $|\mathbb{R}-A|>=\mathbb{R}$, so i have the equality. I am stuck at proving that there are $a_i, a_j \in A $ such that $a_n \notin (a_i,a_j ) $for any $a_n \in A$. The problem is that this does not hold for any choice of $a_i$. For example, if $A= \{0\} \cup \{ 1/n \}$, then obviously for $0 $ this does not hold, although it holds for any other element. Is there any easy and fast way to get past this?",['elementary-set-theory']
2433543,Is this proof of the Archimedean Property valid?,"Archimedean property: The set of natural numbers $\mathbb{N}$ is not bounded above. Proof : Suppose $\mathbb{N}$ is bounded above. Then, by the supremum property, there exits a lowest upper bound ""$s$"" for all $n \in\mathbb{N}$. Call ""$k$"" the biggest natural number, which is smaller than $s$. Then $k+1>s$ and since $k+1$ is a natural number, $s$ is not an upper bound of $\mathbb{N}$. Thus, we have reached a contradiction and can conclude that $\mathbb{N}$ is not bounded above.","['proof-verification', 'elementary-set-theory', 'elementary-number-theory']"
2433554,Pointwise a.e. convergence implies $\int_E f d\mu \leq \lim (\int f_n d\mu)$.,"I'm trying to prove / disprove the following: If $f_n \geq 0$ is a sequence of integrable functions and $f_n \to f$ a.e., then $\lim \int_E f_n d\mu$ exists and $\int_E fd\mu \leq \lim \int_E f_n d\mu$. All limits here are as $n \to \infty$. What I have so far:
As $f_n \to f$, we have that $\liminf f_n = f$. If $(\int_Ef_nd\mu)$ converges, it is equal to $\liminf (\int_E f_n d\mu)$, and so by Fatou's Lemma, $$\int_E fd\mu = \int_E\left(\liminf f_n\right)d\mu \leq \liminf \left(\int_E f_nd\mu\right) = \lim\left(\int_Efd\mu\right).$$ However, I'm stuck on showing that $\lim (\int_E f_n d\mu)$ exists, if at all.","['real-analysis', 'limsup-and-liminf', 'measure-theory', 'limits']"
2433615,What is the final shape and the area of this fractal going to be?,"Start with an isosceles triangle with a vertex angle $\alpha>\frac{\pi}{2}$ and draw two similar triangles on its sides, continue ad infinum. Each triangle will have sides $\frac{1}{2 \sin \alpha/2}$ smaller, or the area $\frac{1}{4 \sin^2 \alpha/2}$ smaller. For $\alpha>\frac{\pi}{2}$ the final area of the fractal will be finite. In the general case the boundary of the resulting figure will have a fractal shape. However, I'm interested in the borderline case of: $$\alpha=\frac{2 \pi}{3}$$ Then for the first 2 steps we obtain: And in fact every ""groove"" like the central one will be filled completely by the smaller triangles, because we have: $$S_n=\frac{1}{3} S_{n-1}$$ As $2$ new triangles are created at each step, we have: $$S_0 \sum_{n=1}^\infty \frac{2^n}{3^n}=2S_0$$ One half of this area ($=S_0$) will be spent to fill the groove, the other will stay ""outside"". (This is wrong see the paragrath in the end. Actually the everything goes inside the groove, since half of the triangles overlap the older ones). As I ask in the title: what is the final shape of the boundary of this fractal going to be? Will it have a smooth $1D$ boundary or will it stay fractal? Unfortunately, I haven't been able to just program this fractal yet, but also I would like to hear a theoretical answer if it's possible to determine without doing the experiment. I have started to figure out how to build the program in cartesian coordinates. Basically we have a one dimensional array of points (collected over all previous iterations) and between each neighbour points we build a perpendicular with the length $l_n=l_{n-1}/ \sqrt{3}$. Then we add this new point to our array exactly between the two points, and continue further down the array. If two initial points have the coordinates $(x_1,y_1)$ and $(x_2,y_2)$ then the new point will be: $$x=\frac{x_1+x_2}{2} \pm \frac{y_1-y_2}{2 \sqrt{3}}$$ $$y=\frac{y_1+y_2}{2} \pm \frac{x_1-x_2}{2 \sqrt{3}}$$ The choice of signs is not always easy, I have not determined the universal rule yet. Here's the result of 4 full iterations and the start of the 5th, made by hand: An important correction based on the comment below: the figure eventually starts to overlap itself. But because the triangles add up perfectly to fill the ""grooves"", let's just add a rule to stop constructing new triangles in places the old ones touch each other with their sides. I used LibreOffice Draw to get more iterations in a crude way (by copying and scaling the whole previous pattern) and I got this at just the 3rd step. Looks like a fractal boundary after all (though the impression might be wrong, since more iterations should cover all the ""grooves""). Because of the overlap reveal I have also added a second question: what is the area of the fractal if we don't count the triangles which overlap the previous ones?","['fractals', 'geometry']"
2433644,How do I find Jordan basis?,"I have a matrix: $$A=\begin{pmatrix}0&1&0\\-4&4&0\\-2&1&2\end{pmatrix}$$ solving $\det|A-\lambda{I}|$ I got characteristic polynom that equals to $(2-\lambda)^3 = 0$ for eigenvalue found two eigenvectors and one generalized eigenvector: $v_1=(1,2,0)\quad v_2=(0,0,1) \quad v_3=(1,0,0)$ What do I have to do to find Jordan basis here? (and what do I need to find Jordan basis in general, I mean is there appropriate alghoritm?, What I read did not make things more clear).","['eigenvalues-eigenvectors', 'jordan-normal-form', 'linear-algebra']"
2433696,Proving symmetric matrices are diagonalizable using fact eigenvectors must be orthogonal,"I'm treating the fact that we know eigenvectors of symmetric matrices corresponding to distinct eigenvalues must be orthogonal as a given and trying to show (real) symmetric matrices are diagonalizable. Knowing the aforementioned fact, we can conclude that there exists an orthogonal basis of eigenvectors for any symmetric matrix. We know a matrix A is diagonalizable iff there exists a basis of eigenvectors. So therefore, symmetric matrices are diagonalizable. ∎ I'm not sure if I'm making a bit of a logical leap when I can conclude that there exists an orthogonal basis of eigenvectors, would appreciate any criticism/advice.","['matrices', 'diagonalization', 'symmetric-matrices', 'linear-algebra']"
2433765,"In the following diagram of a triangle, AB = BC = CD and AD = BD. Find the measure of angle D.","In the following diagram of a triangle, $\overline{AB} = \overline{BC} = \overline{CD}$ and $\overline{AD} = \overline{BD}$. Find the measure of angle $D$. I know this should be easy but I am stuck.  I started by saying angle $\widehat{ACB} = \theta$ and that the supplement angle $\widehat{BCD} = 180^\circ-\theta$.  I know that angle $\widehat{CAB}=\theta$ as well and that angle $\widehat{ABC} = 180^\circ-2\theta$.  In addition, angles $\widehat{CDB}$ and $\widehat{CBD}$ are equal.  I am not sure how to solve for angle $\widehat{CDB}$ ... is it possible to find an exact numerical measure?  I hate overlooking something obvious.  Thank you for your help.","['plane-geometry', 'geometry']"
2433829,Determinant-free proof that a real $n \times n$ matrix has at least one real eigenvalue when $n$ is odd.,"Is there a determinant-free proof that $\textbf{A} \in \mathbb{R}^{n \times n}$ must have at least one real eigenvalue when $n$ is odd? I have seen a few other definitions of ""the set of eigenvalues"" which do not invoke the determinant, specifically the complement of the resolvent set , or the set of points for which $\lambda\textbf{I}_{n \times n} - \textbf{A}$ is singular.","['matrices', 'linear-algebra']"
2433855,Left noetherian ring but not right noetherian ring,"I am starting to learn about noetherian ring. Actually, I see that many books mention of right noetherian but not left noetherian ring. I would like to find some example of left noetherian but not right noetherian ring. I tried to work on the set of $2\times 2$ - matrices but not effective. I also think about the case that ""Left noetherian implies right noetherian"" but there is no clue to prove. Can you please give me a hint? Thank you so much.","['abstract-algebra', 'noetherian']"
2433943,SICP: Why does this recursion-based sine approximation work?,"Here is the question and solution to Structure and Interpretation of Computer Programs ' exercise 1.15 ( see here ). My problem is, I don't know how the combination of these formulae actually work: $$sin(x) = 3sin(x/3) - 4sin^3(x/3)$$
and
$$sin(x) = x$$
for small $x$ radian values. I understand the idea that the closer the radian angle gets to zero, the more it approximates the sine of that angle. I've seen excellent explanations (MIT OCW, Khan Academy). I also have worked out how the $sin(3x)= 3sin(x) - 4sin^3(x)$ formula is derived. But how are they being used together to derive an answer to $sin(x)$? The p function seems to simply be taking the variable angle divided by $3$ each recursive pass until angle is down below $0.1$ Then on the way back, we perform p as many times as we had to divide by $3$. So it seems
$$sin(x) = 3sin(x/3) - 4sin^3(x/3)$$
magically becomes the same as
$$sin(x) = 3(x) - 4(x^3)$$
through recursive application. How? I'm not very deeply versed in recursion theory. Also, if this is logarithmically getting closer to $0.1$, it's not as if we're totaling up lots of small $x$'s a la integration. This seems to be doing something vaguely like the Y-combinator -- which I also don't grasp that well yet. Also, when we see the recursive steps (recursion) repeatedly dividing angle by $3$, what tells you definitively this is logarithmic? I mean, it looks like it's taking those giant order of magnitude leaps at each division, but is there another analytical way to call this logarithmic reduction?","['computer-arithmetic', 'recursive-algorithms', 'trigonometry']"
2433953,Universal property of basis (converse part in functional analysis),"Here is the statement of Universal property of basis: Let $X$ be a $\mathbb K-$ linear space and let $E\subseteq X$.Then E is a basis of X if and only if for every $\mathbb K-$ linear space $Y$ and for every $f:E\to Y$ , there exists a unique $\mathbb K-$ linear extension $T:X \to Y$ of $f$. Now the ($\implies$) part is clear and I have solved it by taking $\displaystyle T\left(\sum_{i\in I}c_ie_i\right)=\sum_{i\in I}c_if(e_i)$, where $\{ e_i\}_{i\in I}=E, c_i\in \mathbb K.$ I have stuck to solve the converse$(\impliedby )$ part. Please someone help.. Thank you..","['functional-analysis', 'linear-algebra']"
2433977,How prove $\cos 20^{\circ}$ is not rational?,"How prove $\cos 20^{\circ}$ is not rational ? I tried something to make a proof .Let me show you my work .
$$\cos 60^{\circ}=4\cos ^320^{\circ}-3\cos 20 ^{\circ}\\
\frac{1}{2}=4\cos ^320^{\circ}-3\cos 20 ^{\circ}\\\cos 20^{\circ}=x\\8x^3-6x-1=0$$ possible rational roots are $\in\{\pm 1,\pm \frac{1}2,\pm \frac 14,\pm\frac18\}$ but no one of them work in the equation ,so the equation has no rational root(s) ,hence $x \in \mathbb{Q^c}$ Is my trial true  ? Is there other idea(s) to show $\cos 20^{\circ}$ is not rational ?","['trigonometry', 'alternative-proof', 'calculus', 'proof-verification']"
2433990,Variables that will make this matrix positive semi-definite,"I have a matrix $$M=\begin{bmatrix} 1+t+m &n&t+n&m+c \\
n &1+t-m&m-c & t-n \\
t+n & m-c&1-t-m & -n \\
m+c & t-n & -n & 1-t+m \end{bmatrix}$$ where I know that $0 \leq c \leq 1$ and $ t=a-(m+n)b$ for some fixed $0 \leq a,b\leq 1$. Here $m$ and $n$ are free parameters with $t$ depending on $m,n$. I'm trying to find a pair of real numbers $(m,n)$ which ensure that $M$ is positive semi-definite. For a fixed $a,b,c \in \mathbb R$, what is the best way to determine some $m,n$ which make $M$ positive semi-definite? The eigenvalues of this matrix are $$\lambda=1 + (m+n) \pm \sqrt{c^2+m^2+n^2+2cm-2cn-2mn+2t^2}$$ and $$\lambda=1 - (m+n) \pm \sqrt{c^2+m^2+n^2-2cm+2cn-2mn+2t^2}.$$ If not, are there conditions on $a,b$ so that $m,n$ exist?","['matrices', 'positive-semidefinite', 'linear-algebra']"
2434035,Definition of a Stochastic Integral for Simple/Elementary Stochastic Processes being well defined,"Often authors in stochastic calculus books define the set of all ""simple"" or elementary stochastic processes to be the set of all functions $H:\Omega\times[0,1]\longrightarrow \mathbb{R}$ such that: \begin{equation*}
H(t,\omega):= \sum_{i=0}^N h_i(\omega) \chi_{(t_i,t_{i+1}]}(t) \qquad \forall (t,\omega) \in \Omega\times[0,1]
\end{equation*}
where: 1) $\chi$ is the indicator function of the interval $(t_i, t_{i+1}]$ 2) $0 = t_0 < t_1 <...< t_{N+1} = 1$ is a partition of the interval $[0,1]$ 3) $h_i$ is a $\mathscr{F}_{t_i}$-measurable function. Let us define $\mathscr{H}$ to be the set of all such ""simple/elementary stochastic processes"". Now let $M:\Omega\times [0,1]\longrightarrow \mathbb{R}$ be a martingale such that $M_0 = 0$. Then they define the ""stochastic integral with respect to the martingale $M$"" to be:
\begin{equation*}
H\bullet M(\omega):= \sum_{i=0}^N h_i(\omega) \left( M_{t_{i+1}}(\omega) - M_{t_i}(\omega)\right)
\end{equation*} My serious concern is, how do I know that this definition is well defined?!?! That is, if I have two representations of the function $H$, i.e.
\begin{equation*}
H(t,\omega)= \sum_{i=0}^N h_i(\omega) \chi_{(t_i,t_{i+1}]}(t) \qquad \forall (t,\omega) \in \Omega\times[0,1]
\end{equation*}
and also:
\begin{equation*}
H(t,\omega)= \sum_{j=0}^N h_j(\omega) \chi_{(s_j,s_{j+1}]}(t) \qquad \forall (t,\omega) \in \Omega\times[0,1]
\end{equation*}
Then how can I be sure that:
\begin{equation*}
\sum_{i=0}^N h_i(\omega) \left( M_{t_{i+1}}(\omega) - M_{t_i}(\omega)\right) = \sum_{j=0}^N h_j(\omega) \left( M_{s_{j+1}}(\omega) - M_{s_j}(\omega)\right)
\end{equation*} This just does not seem obvious to me that this stochastic integral is well defined! In fact I have tried to painstakingly prove this carefully, but I can't prove it! Could someone please provide a careful proof of this? Many thanks! PS: Bonus question.... It also seems to be obvious to most authors that the set of elementary stochastic processes is stable under pairwise products. That is, if $H, G\in \mathscr{H}$ then $HG$ is also a simple stochastic process. To me this statement is intuitively correct, but once again I tried to write out HG but I get really stuck on the notation trying to write out/characterise HG as a simple function. Any help on this would be greatly appreciated as well!!!","['stochastic-processes', 'probability-theory', 'stochastic-integrals', 'simple-functions', 'stochastic-calculus']"
2434064,Proof - raising adjacency matrix to $n$-th power gives $n$-length walks between two vertices,"I came across the formula to find the number of walks of length $n$ between two vertices by raising the adjacency matrix of their graph to the $n$-th power. I took me quite some time to understand why it actually works. I thought it would be useful to write the proof by induction for this in my own words. Theorem: Raising an adjacency matrix $A$ of simple graph $G$ to the $n$-th power gives the number of $n$-length walks between two vertices $v_i$, $v_j$ of $G$ in the resulting matrix. Proof by induction: Let $P(n)$ be the predicate that the theorem is true for $n$. We let $F^{(n)}_{ij}$ be the number of $n$-length walks between vertex $v_i$ and $v_j$. $P(n)$ is then the predicate that $F^{(n)}_{ij} = A^n_{ij}$. We proceed by induction on $n$. Base case: $P(1)$ Case 1: $F^{(1)}_{ij} = A^{(1)}_{ij} = 1$ if $\{v_i, v_j\} \in E$, so there is is a walk of length $1$ between $v_i$, $v_j$. Case 2: $F^{(1)}_{ij} = A^{(1)}_{ij} = 0$ if $\{v_i, v_j\} \notin E$, so there can't be any walk of length $1$ between $v_i$ and $v_j$. In both cases $F^{(1)}_{i j} = A^{(1)}_{ij}$ holds, hence $P(1)$ is true. Inductive step: $P(n+1)$ For purpose of induction, we assume $P(n)$ is true, that is $F^{(n)}_{i j} = A^{(n)}_{ij}$ holds for $n$. We can express a walk of length $n+1$ from $v_i$ to $v_j$ as a $n$-length walk from $v_i$ to $v_k$ and a walk of length 1 from $v_k$ to $v_j$. That means, the number of $n+1$-length walks from $v_i$ to $v_j$ is the sum over all walks from $v_i$ to $v_k$ times the number of ways to walk in one step from $v_k$ to $v_j$. Thus: $$F^{(n+1)}_{ij} = \sum_{k=1}^{|V|} A_{kj}F^{(n)}_{ik} = \sum_{k=1}^{|V|} A_{kj}A^{(n)}_{ik}$$ Which is the formula for the dot-product, used in matrix multplications. Any feedback appreciated.","['graph-theory', 'matrices', 'proof-verification', 'adjacency-matrix', 'discrete-mathematics']"
2434088,What method should be used to solve the following differential equation?,"I have the differential equation:
$$y^{2}dy = x\left ( x dy - y dx \right ) e^{\frac{x}{y}}$$ and I need to solve for a general solution. I'm getting stuck trying to solve for a general solution, as there is no definite Substitution that seems appropriate, and solving for $dy/dx$ is harder that I first thought. What method should be applied to solve for $Y$ with independent variable $x$?",['ordinary-differential-equations']
2434090,"Induction proof of $F(n)^2+F(n+1)^2=F(2n+1)$, where $F(n)$ is the $n$th Fibonacci number.","Let $F(n)$ denotes the $n$th number in Fibonacci sequence.  Then for all $n\in\mathbb{N}$,
  $$F(n)^2+F(n+1)^2=F(2n+1).$$ I know how to prove it by using the formula 
$$F(n)=\frac{\left(\frac{1+\sqrt5}{2}\right)^n-\left(\frac{1-\sqrt5}{2}\right)^n}{\sqrt{5}},$$
but is there a way to prove it by induction? I am year 12 standard so please don't go too deep.","['induction', 'fibonacci-numbers', 'sequences-and-series']"
2434114,"R is symmetric and transitive relation, if and only if $R = R^{-1}\circ R$","How do I prove that the following: Let $R$ be a (binary) relation on a set. Prove that $R$ is symmetric and transitive relation, if and only if $R = R^{-1}\circ R$ . Here, $R^{-1} \circ R$ is defined as the relation $\{(u,v) | \exists x, uR^{-1}x \land xRv\}$ , where $xRu$ means $\left(x,u\right) \in R$ . I only have a problem with assuming the 2nd, and deriving the first, any tips on the proof technique?","['relations', 'equivalence-relations', 'function-and-relation-composition', 'elementary-set-theory']"
2434125,Probability of two events occurring at overlapping times,"I am trying to workout something at work and would like some input/feedback. I have a 1km section of road and I would like to figure out the probability of 2 vehicles passing each other over an hour period. Lets say the road runs north-to-south and there are  26 vehicles travelling northbound and 22 vehicles travelling southbound. If the vehicles travel at 50km/h and their arrival is evenly distributed over the hour for each direction, what is the probability that two vehicles will pass each other over the 1km section? Initially I worked through it as follows but i suspect it is not as simple as this;
1km @ 50km/h - 72 seconds per trip. Therefore, over an hour there are 50 72 second blocks. $(26/50)*(22/50) = 22.9%$ This is clearly wrong because it doesn't consider overlapping time periods. Which i'm not sure how to consider. Any help would be great! Thanks.","['probability-theory', 'probability']"
2434146,Question on Wasserstein Metric,"Consider two random variables $X,Y$. Given some metric $d(X,Y)$, the Wasserstein distance, with respect to $d$, is $$d_W(X,Y)=\inf_{\text{couplings}}\mathbb{E}(d(X,Y))$$ where the infimum is over all couplings of $X,Y$. How does the expected value vary with respect to different couplings?
That is, if $X',Y'$ are any random variables such that $X,Y$ are the marginals of the random vector $(X'Y')$ (i.e., $(X',Y')$ is a coupling of $X,Y$) isn't $\mathbb{E}(d(X',Y')=\mathbb{E}(d(X,Y))$ since $X',Y'$ have the same distributions as $X,Y$, respectively?","['probability-theory', 'metric-spaces', 'coupling', 'probability-distributions']"
2434154,Correct notation for functions with multiple inputs,"This question is related to What is the correct notation for a multivariable function? Consider a function $f(x, y)$ with codomain $C$, such that both $x$ and $y$ belong to ${D}_{1}$. In this case we can denote $f: {D}_{1}^{2} \rightarrow C$. Now, if $x$ belongs to ${D}_{1}$ and $y$ belongs to ${D}_{2}$, we no longer can define the function that way, because the domain is actually a set of heterogeneous tuples. In functional programming languages, such as Haskell, one can define such a function as either $f: ({D}_{1}, {D}_{2}) \rightarrow C$ or a curried function $f: {D}_{1} \rightarrow {D}_{2} \rightarrow C$. At the same time in mathematical notation a function is usually depicted as a mapping between two sets, which raises concern over the acceptability of Haskell'ish notation in mathematical texts?","['notation', 'functions']"
2434168,"Prob. 6, Sec. 18, in Munkres' TOPOLOGY, 2nd ed: A function $f \colon \mathbb{R} \to \mathbb{R}$ that is continuous at exactly one point","Here is the definition of continuity given in Theorem 18.1 in Topology by James R. Munkres, 2nd edition: Let $X$ and $Y$ be topological spaces, let $p \in X$, and let $f \colon X \to Y$ be a function. Then $f$ is said to be continuous at point $p$ if, for every open set $V$ in $Y$ such that $f(p) \in V$, there is an open set $U$ in $X$ such that $p \in U$ and $f(U) \subset V$. Now here is Prob. 6, Sec. 18: Find a function $f \colon \mathbb{R} \to \mathbb{R}$ that is continuous at precisely one point. My Attempt: Let $f \colon \mathbb{R} \to \mathbb{R}$ be defined by 
  $$ f(x) = \begin{cases} 0 \ & \ \mbox{ if } \ x \in \mathbb{Q}, \\ x \ & \ \mbox{ if } \ x \in \mathbb{R} \setminus \mathbb{Q}. \end{cases} $$ First, let $p \colon= 0$. Then $p \in \mathbb{Q}$ of course, and so $f(p) = 0$. Let $\left( x_n \right)_{n \in \mathbb{N}}$ be any sequence of real numbers such that 
  $$ \lim_{ n \to \infty } x_n = p = 0. \tag{0} $$
  Then, for each $n \in \mathbb{N}$, we have $$ f \left( x_n \right) = \begin{cases} 0 \ & \ \mbox{ if } \ x_n \in \mathbb{Q}, \\ x_n \ & \ \mbox{ if } \ x_n \in \mathbb{R} \setminus \mathbb{Q}. \end{cases} $$ If $\left( x_n \right)_{n\in \mathbb{N}}$ has only finitely many rational terms, then there is a natural number $N$ such that $x_n \in \mathbb{R} \setminus \mathbb{Q}$ for all $n > N$, and so $f \left( x_n \right) = x_n$ for all $n > N$. Therefore, 
  $$ \lim_{n \to \infty} f \left( x_n \right) = \lim_{n \to \infty} x_n = 0 = f(p), $$
  by (0) above. If $\left( x_n \right)_{n \in \mathbb{N} }$ has only finitely many irrational terms, then there is a natural number $N$ such that $x_n \in \mathbb{Q}$ for all $n > N$, and so $f \left( x_n \right) = 0$ for all $n > N$. Therefore, 
  $$ \lim_{n \to \infty} f \left( x_n \right) = 0 = f(p). $$ Now suppose that $\left( x_n \right)_{n \in \mathbb{N} }$ has infinitely many rational terms and infinitely many irrational terms. Then there are strictly increasing functions $\phi \colon \mathbb{N} \to \mathbb{N}$ and 
  $\psi \colon \mathbb{N} \to \mathbb{N}$ such that 
  $$ \phi( \mathbb{N} ) \cap \psi ( \mathbb{N} ) = \emptyset, \qquad  \phi( \mathbb{N} ) \cup \psi ( \mathbb{N} ) = \mathbb{N},  \qquad  
x_{\phi(n)} \in \mathbb{Q}, \qquad x_{\psi(n)} \in \mathbb{R} \setminus \mathbb{Q}.  \tag{1} $$
  Then for each $n \in \mathbb{N}$, we have $$ f \left( x_{ \phi(n) }  \right) = 0, \qquad  f \left( x_{ \psi(n) }  \right) = x_{\psi(n)} .$$
  So 
  $$ \lim_{n \to \infty } f \left( x_{ \phi(n) }  \right) = 0, \qquad \lim_{n \to \infty } f \left( x_{ \psi(n) }  \right) = x_{\psi(n)}  = p = 0. \tag{2} $$
  Here we have used the fact that, since $\left( x_n \right)_{n \in \mathbb{N}}$ converges to $p$, therefore every subsequence of $\left( x_n \right)_{n \in \mathbb{N}}$ also converges to $p$. Moreover we have taken $p$ to be $0$ here. From (1) and (2) we can conclude that 
  $$ \lim_{n \to \infty} f \left( x_n \right) = 0 = f(0) = f(p), $$
  as required. Thus we have shown that the image sequence $\left( f \left( x_n \right)  \right)_{n \in \mathbb{N} }$ of every sequence $\left( x_n \right)_{n \in \mathbb{N} }$ that converges to $p = 0$ converges to $f(p) = 0$. So $f$ is continuous at $p = 0$. Next, suppose that $p \neq 0$. If $p$ is rational, then $f(p) = 0$. Let $\left( x_n \right)_{n \in \mathbb{N} }$ be a sequence of irrational numbers converging to $p$. Then 
  $$ \lim_{n \to \infty} f \left( x_n \right) = \lim_{n \to \infty } x_n = p \neq 0 = f(p).  $$ 
  On the other hand, if $p$ is irrational, then $f(p) = p \neq  0$. Let 
  $\left( x_n \right)_{n \in \mathbb{N} }$ be a sequence of rational numbers converging to $p$. Then 
  $$ \lim_{n \to \infty} f \left( x_n \right) = \lim_{n \to \infty } 0 = 0 \neq p = f(p).  $$ 
  Thus we have shown that $f$ cannot be continuous at any real  $p$ such that $p \neq 0$. Is what I have done so far correct? However, so far in Munkres, we do not have the sequential criterion for continuity at our disposal. So we must have recourse to the definition given above. Let $V$ be an open set in the range space $\mathbb{R}$ such that 
  $$0 = f(0) \in V.$$
  Then there is an open interval $(c, d)$ such that $$0 \in (c, d) \subset V. $$ Now what should be our open set $U$ in the domain space $\mathbb{R}$ such that $0 \in U$ and $f(U) \subset V$? Now suppose that $p \neq 0$. First, suppose $p$ is irrational. Then $f(p) = p \neq 0$, and there is an open interval $(a, b)$ such that $p \in (a, b)$ and $0 \not\in (a, b)$. Let us take $V \colon= (a, b)$. Then $V$ is open in the range space $\mathbb{R}$ and $f(p) \in V$. However, if $U$ is any open set in the domain space $\mathbb{R}$ such that $p \in U$, then there is an open interval $(c, d)$ such that $p \in (c, d) \subset U$. But 
  $$ f \left( \ (c, d) \ \right) = \left[ \ (c, d) \cap (\mathbb{R} \setminus \mathbb{Q} ) \ \right] \cup \left\{ \ 0 \ \right\} \not\subset V, $$
  because $0 \not\in V$. But as $(c, d) \subset U$, so $ f \left( \ (c, d) \ \right) \subset f(U)$. Therefore $f(U) \not\subset V$. Now suppose that $p$ is rational and $p \neq 0$. Then $f(p) = 0$. Let $V$ be the open interval $(-1, 1)$. Then $V$ is open in the range space $\mathbb{R}$, and $f(p) \in V$. However, if $U$ is any open set in the domain space $\mathbb{R}$ such that $p \in U$, then we can find an open interval $(a, b)$ such that $p \in (a, b) \subset U$ and such that $0 \not\in (a, b)$. What next? How to proceed from here?","['general-topology', 'real-analysis', 'continuity']"
2434241,How to prove the tightness of Markov's bound?,"Show that Markov's inequality is as tight as it possible. Given a positive integer $k$, describe a random variable $X$ that assumes only non-negative values: $$\Pr[X \geq k E[X] ] = 1/k.$$ Using Markov's bound, we can show at most $1/k$. But how to show equality?! My question to be exact what is the idea to prove the tightness of this bound!","['probability-theory', 'inequality', 'probability', 'random-variables']"
2434271,Show that $V=\operatorname{null} T^{n-1}\oplus \operatorname{range} T^{n-1}$,"This question comes from A. Sheldon's Linear Algebra Done Right, 3rd Edition, Exercises 8.A. Suppose that $T\in\mathcal{L}(V)$ is not nilpotent. Let $n=\dim V$. Show that $V=\operatorname{null} T^{n-1}\oplus \operatorname{range} T^{n-1}$. After pondering over and over this question for a long time, I've tried expanding basis, constructing quotient space, and many others, but I cannot unravel this stuff. Unfortunately, my university hasn't adopted this excellent textbook for linear algebra course, and I have to learn it on my own. I've met many hard exercises in this book but this one seems to defy all my attempts. Any help or hint is welcome, best in the manner of ""Axler's way"", thank you so much.","['linear-algebra', 'linear-transformations']"
2434295,Truncation error and the second centered difference approximation $\frac{d^2u}{dx^2}$ at $x = x_{j}$ ?.,"The second centered difference approximation $\frac{d^2u}{dx^2}$ at $x = x_{j}$?. By expanding the terms $u(x_{j}+h)$ and $u(x_{j} - h)$ about the point $x_{j}$ with a Taylor series,we need to prove that the truncation error $T_{j}$ in the approximation satisfies $T_{j} =  -\frac{h^2}{12}\frac{d^4 u(x_{j})}{dx^4} +$ terms with higher powers of $h$. From central difference approximation $$u'(x_{j}) = \frac{u(x_{j}+h) - u(x_{j}-h)}{2h}$$
$$u''(x_{j}) = \frac{u'(x_{j}+h) - u'(x_{j}-h)}{2h}$$
$$u''(x_{j}) = \frac{u(x_{j}+2h) - 2u(x_{j}) + u(x_{j}-2h)}{4h^2}$$
From Taylor series expansion
$u(x_{j}+2h) = u(x_{j}) + 2hu'(x_{j}) + \frac{(2h)^2}{2!}u''(x_{j})+ \frac{(2h)^3}{3!}u'''(x_{j}) + \frac{(2h)^4}{4!}u''''(x_{j})+h.o.t$ $u(x_{j}-2h) = u(x_{j}) - 2hu'(x_{j}) + \frac{(2h)^2}{2!}u''(x_{j})- \frac{(2h)^3}{3!}u'''(x_{j}) + \frac{(2h)^4}{4!}u''''(x_{j}) -h.o.t$ So that $$u(x_{j}+2h)+u(x_{j}-2h)-2u(x_{j}) = 4h^2u''(x_{j})+\frac{4}{3}h^4 u''''(x_{j})+...$$ $$u''(x_{j}) = \frac{u(x_{j}+2h)+u(x_{j}-2h)-2u(x_{j})}{4h^2} = u''(x_{j})+\frac{1}{3}h^2 u''''(x_{j})+...$$ Now i get that error $T_{j} = \frac{1}{3}h^2 u''''(x_{j})$ 
but I am not getting the required one $T_{j} = -\frac{h^2}{12} u''''(x_{j})$. Where am I doing wrong?","['derivatives', 'taylor-expansion', 'finite-differences', 'truncation-error', 'numerical-methods']"
2434320,Modulus and square root,"How is this statement true?
""For any real number $x$ we have $\sqrt{x^2} = |x|$""?
Because putting $x=2$
$\sqrt{x^2}$ gives BOTH $2$ and $-2$
But $|x|$ only gives $2$","['functions', 'graphing-functions']"
2434327,Chord and diameter in circle,"Question: In circle with radious $R$ we have diameter $AB$ and chord $CD$. The chord intersects $AB$ in point $M$ such that $\angle CMB = 45^o$. Show that $CM^2 + DM^2 = 2R^2$. My attempt: I'm sure there is nice solution, but all I can get to are some awful calucations. I know that $CM \cdot DM = AM \cdot BM$, but I'm not sure it will be any help. Thanks for hints/solutions in advance.",['geometry']
2434341,Determine for what values of $x$ the given series converges,"The given series is $\sum_{n=1}^∞
(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+...+\frac{1}{n}
)x^{n} $. I tried it by using Cauchy Root Test as follows- Let 
$y=\lim_{n\to\infty}(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+...+\frac{1}{n} )^{1/n}$, then by taking logarithm both sides,we get $\log(y)=\lim_{n\to\infty}\frac{1}{n}\log(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+...+\frac{1}{n} )$ Since, $\log(0)=-\infty$. So, $\log(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+...+\frac{1}{n} )=-\infty$. Applying,L'Hôpital's rule,we get $\log(y)=\lim_{n\to\infty}-\frac{(\frac{1}{n^2}+\frac{1}{(n-1)^2}+\frac{1}{(n-2)^2}+...+...+... )}{(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+...+\frac{1}{n} )}$.( Please Check this step!! ) Since,$\sum_{k=1}^\infty\frac{1}{k^2}=\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{k^2}=\frac{\pi^2}{6}$ & $\sum_{k=1}^\infty\frac{1}{k}=\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{k}=\infty$.So,$\log(y)=0\implies y=1$. Now let $a_n=(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+...+\frac{1}{n} )x^{n} $. Then,$$(a_n)^{1/n}=\lim_{n\to\infty}(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+...+\frac{1}{n} )^{1/n}x$$ $\implies \lim_{n\to\infty}\vert (a_n)^{1/n}\vert=1.x$.By , Cauchy root test the given series converges if $\lim_{n\to\infty}(a_n)^{1/n}<1$.Hence, the given series converges if $\vert x\vert<1$. I  NEED TO KNOW WHETHER MY SOLUTION IS CORRECT OR NOT?","['real-analysis', 'sequences-and-series', 'proof-verification', 'proof-explanation', 'power-series']"
2434349,Confusion about $\mathbb Q$ subset suprema?,"This one is very confusing for me, I am probably missing some essential piece of information. Besides the things they told us in school, I also reassured myself with these: Is every real number the supremum of a set of rational numbers? and Is there a rational number between any two irrationals? . Though the latter probably does not add anything to my confusion. I found proofs that there are rational numbers between any two irrational numbers. And also every irrational number $\iota$ can be represented as a supremum of a subset of Rationals $S=\{ x \in  \mathbb Q  | x< \iota \}$. And that is the problem for me, because irrational numbers with ordering $\leq$ form a linear ordering. Therefore the map $f:\mathbb I \text{(irrationals)}\to 2^{\mathbb Q}$ given by already mentioned rule should implicate that $\iota_1<\iota_2 \iff f(\iota_1)\subset f(\iota_2)$. They cannot have same elements and there must be inclusion in way or another, because I take all the rational numbers smaller than some upper bound. This is the part that I think is probably not correct, but I do not see why at this moment. However that would mean there is a set of subsets of $\mathbb Q$ of uncountable size forming a linear ordering, which is impossible, because that would require an uncountable set to begin with, right? Where is(are) the mistake(s) in reasoning and why? And also, which is probably obvious, I do not know much about the construction of the suprema, so there might be another key information explaining this seemingly contradictory conclusions.","['real-numbers', 'irrational-numbers', 'elementary-set-theory']"
2434372,"Policy Gradients, Log Trick, Expectations, Softmax Policy","In so-called ""policy based"" game playing algoritms (reinforcement learning in particular), there is a policy function $\pi_\theta(s,a)$ that gives the probability of action given state of the game and hidden parameter $\theta$, as shown in pg. 16 of lecture notes below. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf In order to improve the score (let me drop the underscript $\theta$ from now on), instead of $\nabla \pi$ one usually tries to compute $\nabla \log \pi(s,a)$ based on this trick $$ 
\nabla \pi(s,a) = \pi(s,a) \frac{\nabla \pi(s,a)}{\pi(s,a)}
$$ $$
= \pi(s,a)\nabla \log \pi(s,a)
$$ Now the author above calls $\nabla \log \pi(s,a)$ the score function and claims ""he can compute it easier because the expectation of it is easy"". As an example he picks softmax as $\pi$ which is, $$
\pi(s,a) \propto e^{\phi(s,a)^T\theta}
$$ and jumps to score function $$
\nabla \log \pi(s,a) = \phi(s,a) - E[\phi(s,\cdot)]
$$ I don't understand how the derivation could reach this statement. After some laboring, I have $h(s,a,\theta) = \phi(s,a)^T\theta$ $$ 
\pi_\theta(s,a) = \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} }
$$ The gradient of the log $$ 
\nabla_\theta \log \pi_\theta = 
\nabla_\theta \log \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} }
$$ $$ = \nabla_\theta \big[ \log e^{h(s,a,\theta)} - \log \sum_b e^{h(s,b,\theta)}\big]$$ because $$ \log(\frac{x}{y}) = \log x - \log y $$ We continue $$ = \nabla_\theta \big[ h(s,a,\theta) - \log \sum_b e^{h(s,b,\theta)} \big]$$ $$ 
= \phi(s,a) - \sum_b h(s,b,\theta)\frac{e^{h(s,b,\theta)}}{\sum_b e^{h(s,b,\theta)}} 
$$ $$ 
= \phi(s,a) - \sum_b h(s,b,\theta) \pi_\theta(b,s)
$$ The last term can be seen as expectation. Would this work? I am not sure of that last derivative of log of sum.","['multivariable-calculus', 'probability']"
2434395,All the solutions of $f'(x)=f(x+\pi/2)$,"Consider the following equation (with $f \in C^{\infty}(\mathbb{R})$): $$f'(x)=f(x+\pi/2)$$ This equation is satisfied by $f(x) = A\cos(x) +B\sin(x)$, for any $A,B \in \mathbb{R}$. Question : What are all the (other) solutions of this equation (if any)?","['trigonometry', 'ordinary-differential-equations', 'delay-differential-equations', 'calculus']"
2434399,Find mistake in solving $\sin 2x=\sin x + \cos x$,"I am solving $\sin 2x = \sin x +\cos x $ for $0\le x \le 360$ $$\sin 2x = \sin x +\cos x$$
$$2 \sin x \cos x =\sin x + \cos x$$
$$(\cos x + \sin x) ^{2} - (\sin x)^{2} - (\cos x)^{2} =(\cos x + \sin x)^{2} - 1=\sin x + \cos x$$
Let $\cos x + \sin x = y$
$$y^{2} - 1= y$$
After solving the quadratic gets $1.618$(I think this is not accepted)  and $-0.618$
$$y=\cos x + \sin x =\sin 2x = -0.618$$
$$2x=218.17,321.83,578.17,681.87$$
$$x=109.09,160.92,289.09,340.34$$
But the problem is $109.09$ and $340.34$ is not the solution. I didn't purposely square anything to produce extra solution. The two extra solution satisfy $\sin 2x=-0.618$ but not $\cos x + \sin x=-0.618$.Is there any mistake in my calculations, or is there anyplace I introduced accidentally extra solution? Thanks.","['algebra-precalculus', 'trigonometry']"
2434432,An inequality concerning $\pi$,"Given $x_0=0,x_1,\ldots,x_n>0,x_1+\ldots+x_n=1$. We want to prove $$ \sum_{i=1}^n \frac{x_i}{\sqrt{1+x_0+\cdots +x_{i-1}}\sqrt{x_i+\cdots +x_n}}<\frac{\pi}2$$ My progress: Let $x_0+\cdots+x_i=\sin \alpha_i$ We have $0=\alpha_0<\alpha_1<\cdots<\alpha_n=\frac{\pi}2$ Then the inequality becomes $$ \sum_{i=1}^n \frac{\sin \alpha_i-\sin \alpha_{i-1}}{\cos \alpha_{i-1}}<\frac{\pi}2$$ Then I don't know how to deal with this trigonometric inequality.","['inequality', 'analysis']"
2434456,Only One Field with Four Elements,"As I started typing this up, I realized I made a mistake somewhere but I cannot identify it. Prove that any two fields having exactly four elements are isomorphic. [Hint: First prove that $1+1=0$, and then show that the nonzero elements form a cyclic group of order $3$ under multiplication] First I will prove a few things about fields with four elements before demonstrating the isomorphism. Let $F= \{0,1,a,b\}$ be a field with four elements. First of all, since $F$ is a field, every nonzero element is a unit, meaning that the group of units $U(F)$ is a multiplicative group of $3$ which makes it isomorphic to $\Bbb{Z}_3$, and hence $U(F)$ is cyclic. Now I will prove that $1+1=0$. Note that $(1+1)^2 = (1+1)(1+1) = 1 + 1 + 1 + 1 = 0$, where the last equality follows because $F$ is an additive group of order $4$. Since $F$ is a field, there can be no nonzero divisors, implying that $1 + 1 = 0$. Now I will prove that $a+b =0$, which can also be used to prove $1+1$ since $(-a)^3 = 1$ implies $-1 = 1$ (is this right?). If $a+b$ weren't zero, then it would be a unit and hence have a multiplicative order of $3$. That is, $(a+b)^3 = 1$ or $1 + 3a^2b + 3a b^2 = 0$. Multiplying by $a$ and then $b$ yields the two equations $a + 3b + 3a^2 b^2 = 0$ and $b + 3a^2 b^2 + 3a = 0$, and adding the two gives $4a + 4b + 6a^2 b^2 = 0$ or $6a^2 b^2 = 0$, which is a contradiction since neither $a=0$ nor $b=0$.  Hence $a + b = 0$ or $b = -a$. Therefore $F = \{0,1,a,-a\}$ (Perhaps the mistake was made here?) Now we compute some products and sums of elements in $F$. It is clear that $1+a = -a$, for $1+a=0$ implies $a=1$, a contradiction, and $1+a=a$ implies $1=0$, another contradiction...Hold on! Either $a^2 = 1$, $a^2 = a$, or $a^2 = -a$, but all lead to a contradiction...I must have made a mistake somewhere. EDIT: Okay. We still have $(1+1)^2 = 0$ implies $1+1 = 0$, and because $U(F)$ is a cyclic group, it's easy to see that $b = a^2$. Now let $F_1 = \{0,1,a,a^2\}$ and $F_2 = \{0',1',b,b^2\}$ be two fields with four elements. Define the map $f : F_1 \to F_2$ by $f(0)=0'$, $f(1)=1'$, $f(a) = b$, and $f(a^2)=b^2$. Clearly this is bijective, and so all that remains is to show that it is a ring homomorphism. Let us first prove additivity. First note that $a+a = a(1+1) = a \cdot 0 = 0$; and $a+1 = a^2$ otherwise we obtain a contradiction; and finally $a + a^2 = a(a+1) = a \cdot a^2 = 1$. Hence $f(a + a ) = f(0) = 0' = b + b = f(a) + f(a)$; and $f(a+1) = f(a^2) = b^2 = b+1 = f(a) + f(1)$; and finally $f(a+a^2) = f(1) = 1 = b + b^2 = f(a) + f(a^2)$. Now we prove that $f$ is multiplicative. Note that $f(1 \cdot x) = f(x) = f(1) f(x)$; and that $f(a \cdot a) = f(a^2) = b^2 = b \cdot b = f(a) f(a)$; and finally $f(a \cdot a^2) = f(a^3 ) = f(1) = 1 = b^3 = b \cdot b^2 = f(a) f(a^2)$. How does this sound?","['abstract-algebra', 'ring-theory', 'field-theory']"
2434472,Constructing Injective Maps with Disjoint Images,"Suppose I have to construct $m$ injective maps, $\mathbb{F}=\{f_0,f_1,f_2,\ldots f_{m-1}\}$ from $\mathbb{W_n}=\{0,1,2,\ldots,n-1\}$ to $\mathbb{W_{mn}}=\{0,1,2,\ldots,mn-1\}$ such that $\forall i,j\in\mathbb{N}$ where $i,j\leq m$ and $i\neq j,\;f_{i}\left(\mathbb{W_n}\right)\cap f_{j}\left(\mathbb{W_n}\right)=\emptyset$. That is, I need $m$ different injective maps whose images are all disjoint. One such example is,$$\mathbb{F}=\{f_i\mid f_i(x)=mx+i\},\;0\leq i\lt m.$$
But here, the mapping is very straightforward and easily predictable, even to a person who has not been exposed to the function. I'm hoping I'd get some help in constructing these sort of maps, but in a less intuitive way using, say, modular arithmetic and such.","['elementary-set-theory', 'functions']"
2434486,Why is the z =z' when using Milne-Thomson Method for determining a holomorphic function?,"When we use Milne-Thomson method, we substitute $x$ and $y$ with $\frac{z+z^*}2$ and $\frac{z-z^*}{2i}$. This gives us... $f(z) = u(x,y) + iv(x,y) = u(\frac{z+z^*}2, \frac{z-z^*}{2i}) + i v(\frac{z+z^*}2, \frac{z-z^*}{2i})$ $z^*$ represents the conjugate of $z$. We then proceed to say that for the above relation $z=z^*$. Why did we we do that? And how can we say that? I mean, this seems like a massive over-simplification.","['complex-analysis', 'complex-numbers']"
2434499,Formula for expanding powers [duplicate],"This question already has answers here : Prove that $a^n-b^n = (a-b)(a^{n-1} + a^{n-2}b + \cdots + b^{n-1})$. (3 answers) Closed 6 years ago . I can't seem to understand the following: 
$$A^n-B^n=\left(A-B\right)\cdot \left(A^{n-1}+A^{n-2}\cdot B+\cdots+A\cdot B^{n-2}+B^{n-1}\right)$$ How can i derive this formula ? Also, when does the $A^\text{something}$ end and when does $B^\text{something}$ start. Thank you.",['algebra-precalculus']
2434516,Convexity vs Quasi-convexity,"It is well known that in optimization the concept of quasi-convex function is the following: $f: \Omega\subset \mathbb{R}^n \rightarrow \mathbb{R}$ is a quasi convex function if for every $\alpha\in\mathbb{R}$, we have that $level_{\alpha}(f):=\{x\in\Omega; f(x)\leq \alpha\}$ is a convex set. Every convex function is quasi convex in this sense. Although the converse is not true, as one can see from, for example, the level sets of the Heavyside function. My question: Which additional conditions make a quasi-convex function $f: \Omega\rightarrow \mathbb{R}$ convex? One should ask at least that f is locally Lipschitz, since every convex function satisfies this hypothesis. Thanks for the attention.","['variational-analysis', 'optimization', 'analysis', 'convex-analysis']"
2434525,"Compute limit by induction (""inductive step of existence"" issue)","I was asked to compute the limit 
$\lim\limits_{x \to 1} \dfrac{\displaystyle\prod_{k=1}^n (1+x^k) - 2^n}{x-1}$ My approach was as follows i) Define $L(n):=\lim\limits_{x \to 1} \dfrac{\displaystyle\prod_{k=1}^n (1+x^k) - 2^n}{x-1}$ ii) $L(1)$ exists and trying to write $L(n+1)$ in terms of $L(n)$, it turns out that $L(n+1)$ exists iff and $L(n)$ exists (from second line of the following equalities) $$\begin{align*} L(n+1) & = \lim\limits_{x \to 1} \dfrac{\displaystyle\prod_{k=1}^{n+1} (1+x^k) - 2^{n+1}}{x-1} \\ & = \lim\limits_{x \to 1} \left( 2\times \dfrac{\displaystyle\prod_{k=1}^n (1+x^k) - 2^n}{x-1} + \lim\limits_{x \to 1} \dfrac{x^{n+1}-1}{x-1}\times \prod_{k=1}^n (1+x^k)\right) \\ & = 2L(n) + (n+1)\times 2^n \end{align*}$$ iii) the algebraïc identity linking $L(n)$ and $L(n+1)$ is $L(n+1) = 2L(n) + (n+1)\times2^n$ iv) Then I defined $l(n):=\dfrac{L(n)}{2^n}$ noticing that it satisfies the telescopable relation $l(n+1)-l(n) = \dfrac{n+1}{2}$ v) Computing $l(n)$ then $L(n)$ now is easy and it yields $L(n) = 2^{n-1}\times \dfrac{n(n+1)}{2}$ My question is that a friend objected step (ii) (third line in aligned equations) telling me that the problem of existence is not solved yet the way I did it, and that we can't do such a manipulation when expressions we manipulate are not justified beforehand to exist.  As I see it, since $L(1)$ exists and figuring out that $L(n+1)$ exists iff $L(n)$ does, is enough to this issue of existence. I wonder whether I'm right or wrong and is the proof I provided rigorous enough, otherwise what do I need it to add to get it better Thanks in advance for any suggestions / advice.","['induction', 'limits-without-lhopital', 'calculus', 'limits']"
2434534,The sum of $2012$ positive terms is $10.$,"A sequence has $2012$ positive terms adding to $10$
Prove that the sum of product of two consecutive terms is less than $25.$ I've tried using the Cauchy-Schwarz inequality but couldn't find the proof. Also find the maximum value (where equality holds).",['algebra-precalculus']
2434544,Simplify Propositional Form,"I was solving this problem.
Simplify propositional form:
 $$[(p\implies q) \vee (p \implies r)] \implies (q \vee r)$$ I was using this fact for solving:
$$(p \implies q) \equiv (\neg p \vee q)$$ but end up with
$$(p \implies (q \vee r)) \implies (q \vee r)$$ I need help to further simplifying this expression. Any kind of help would be appreciated. Thanks!","['propositional-calculus', 'logic', 'discrete-mathematics']"
2434545,"Given the definition of Kuratowski pairs, Pairs have right identity conditions","I'm supposed to prove that $\{\{a\}, \{a,b\}\} = \{\{c\}, \{c,d\}\}$ iff $a = c \wedge  b = d.$ I'm trying to show that if {{a}, {a,b}} = {{c}, {c,d}} then ((a = c) & (b = d)) on the assumption that a = b. However, I'm only ever able to show that (a = c) follows and not that (b = d). 
Does anyone have any advice?",['elementary-set-theory']
2434577,determinant of the $7\times7$ matrix [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to find determinant of the following $7\times7$ matrix
\begin{bmatrix} 
    a&1&0&0&0&0&1&\\
    1&a&1&0&0&0&0&\\
    0&1&a&1&0&0&0&\\
    0&0&1&a&1&0&0&\\
    0&0&0&1&a&1&0&\\
    0&0&0&0&1&a&1&\\
    1&0&0&0&0&1&a&
\end{bmatrix}","['matrices', 'jordan-normal-form', 'linear-algebra']"
2434582,A problem about periodic functions,"Suppose that $f(x):\mathbb{R}\rightarrow \mathbb{R}$ is a periodic function with a minimal positive period $T$. Can $g(x)=f(x^2)$ be periodic? I know it is impossible if the condition $\forall x \in (0,T),f(x)\not=f(0)$ is added. But Is there a counterexample for the general case? Thanks for any help in advance.","['periodic-functions', 'analysis', 'functions']"
2434615,Is $y'(x) = y(y(x))$ an ODE?,I'm starting the course of ordinary differential equations and I'm not really sure if $y'(x) = y(y(x))$ is an ODE. More than the answer what I need is an explanation of the reason of it. Thanks!,['ordinary-differential-equations']
2434647,Card Probability using inclusion exclusion,"You are dealt $13$ cards from a shuffled deck of $52$ cards. Compute the probability that you get all
four cards of at least one denomination (all Aces, or all Kings, or all Queens, . . . , or all Twos). The answer to this is $\frac{{13\choose 1}{48 \choose 9}}{{52 \choose 13}} + \frac{{13\choose 2}{44 \choose 5}}{{52 \choose 13}} + \frac{{13\choose 3}{40\choose 1}}{{52 \choose 13}}$ I'm having a hard time understanding this solution. I think it depicts the probability of choosing all four of a first denomination plus all four of a first and second denomination plus all 4 of a first, second and third denomination, but that would mean that the third fraction should be smaller than the first two, but it isn't Where am I going wrong? Perhaps this is the probability of the first four of a first domination plus four of a second plus four of  a third, but in that case I do not understand why we do ${48\choose 9}$, ${44\choose 5}$ and ${40\choose 1}$ because isn't that populating the rest of our selection of $13$ choices? And if that's the case then why are we populating it $3$ times?","['inclusion-exclusion', 'statistics', 'probability']"
2434678,Finding the limiting value in the form of recurrence,"Now, this is actually a weird one, because I usually wouldn't suspect one of these to pop up on facebook; but is there anything to this problem, is it solvable, or is it just pure jitter, and if not, how is it solved? $$\mathrm{For\space every\space integer\space} n \geq 0,$$ $$ I_n=\int_0^{\huge\frac{\pi}{2}}\cos^{2n}x\space\mathrm{d}x\space;\space J_n=\int^{\huge\frac{\pi}{2}}_0 x^2\cos^{2n}x\space \mathrm{d}x$$ $$\mathrm{Find\space the\space limit \space below:}$$ $$ \lim_{n \to + \infty} 2 \sum_{k=1}^n\left(\frac{J_{k-1}}{I_{k-1}}-\frac{J_k}{I_k}\right)$$","['real-analysis', 'limits', 'calculus', 'integration', 'analysis']"
2434684,On Fundamental theorem of projective geometry,"Fundamental theorem of projective geometry states that given two projective frames $P=\{p_1,\ldots,p_{n+2}\}$ and $P'=\{p_1',\ldots, p_{n+2}'\}$ of a projective space $\mathbb{P}^{n}_k$, there is exactly one projective transformation $A\in PGL_{n+1}(k)$ that maps the first frame onto the second one. Assume now that $K/k$ is a Galois extension with the Galois group $G$ and the sets $P$ and $P'$ (defined over $K$) are both $G$-invariant and have identical orbit structure w.r.t. $G$. Does there still exist $A\in PGL_{n+1}(k)$ (over small $k$) mapping $P$ to $P'$?","['projective-geometry', 'projective-space', 'linear-algebra', 'algebraic-geometry']"
2434749,Associativity of symmetric difference,"I have the definition $A+B=(A - B)\cup (B- A)$ for $A,B\in P(X)$, where $X$ is a set and $P(X)$ is the power set. $+$ should be associative since $P(X)$ is a group, but I cannot prove that $(A+B)+C=A+(B+C)$. I computed $(A+B)+C$ as follows
$$(A+B)+C=\{[(A - B)\cup (B- A)]+ C\}
\\=  \{ [(A - B)\cup (B- A)] -C \}\cup \{C- [ (A - B)\cup (B- A) ]   \} 
\\ =  \{ [(A\cap B^c)\cup (B\cap A^c)] -C \}\cup \{C- [ (A\cap B^c)\cup (B\cap A^c) ]   \} 
\\= \{ [(A\cap B^c)\cup (B\cap A^c)]\cap C^c \}\cup \{C \cap [ (A\cap B^c)\cup (B\cap A^c) ]^c   \}
\\=  \{ [(A\cap B^c)\cup (B\cap A^c)]\cap C^c \}\cup \{C \cap [ (A\cap B^c)^c\cap (B\cap A^c)^c ]   \}
\\=  \{ [(A\cap B^c)\cup (B\cap A^c)]\cap C^c \}\cup \{C \cap [ (A^c\cup B)\cap (B^c\cup A) ]   \}   $$ On the other hand $A+(B+C)$ is $$ A+(B+C)= A + [ (B-C)\cup (C-B) ]
\\= \{  A-[ (B-C)\cup (C-B) ] \}\cup \{[ (B-C)\cup (C-B) ]-A  \} 
\\= \{  A\cap[ (B\cap C^c)\cup (C\cap B^c ) ]^c \}\cup \{[ (B\cap C^c)\cup (C\cap B^c) ]\cap A^c  \}
\\= \{  A\cap[ (B^c\cup C)\cap (C^c\cup B ) ] \}\cup \{[ (B\cap C^c)\cup (C\cap B^c) ]\cap A^c  \}$$ I tried to calculate further but I didn't get the same expression for both sides. Any ideas?",['elementary-set-theory']
2434758,A question on the proof of induced metric from Riemannian metric,"The following Picture is the proof of "" distance function on Riemannian manifold is a metric "". How the author deduced that length of any curve from $p$ to the boundary of given ball is at least $r/\lambda_0$? Thanks.","['metric-spaces', 'riemannian-geometry', 'differential-geometry']"
2434802,Find a particular path for Vanishing the integral,"Is it possible to find a path from P to Q such that: $$\int_C xy^2dx+ydy=0;\, P=(0,0)\, \text{and}\, Q=(1,1)$$ $$\int_C \frac{-ydx+xdy}{x^2+y^2}=0;\, P(-1,0)\, \text{and}\, Q=(1,0)$$ I've asked before for the possibility to find a path for vanishing in general, but now I want to know if there's a path for that particular points, Hints?","['multivariable-calculus', 'integration', 'contour-integration', 'vector-analysis']"
2434822,A quicker way to approach the indefinite integral $\int{\frac{x^3dx}{1+x^5}}$,"Problem:   Evaluate the integral $$\int{\frac{x^3dx}{1+x^5}}$$ Source: Given to me as a challenge. I can't seem to find a valid substitution for the integral. Also the way should be quick enough to apply in a timed test. I don't know the answer and I have almost given up. I called the challenger and he said that I can ask for help but not look up on Wolfram or any such website as it won't be a proper solution. I don't think it is solvable though :p My try: Well, I have tried to get around by using many substitutions which make the thing more complicated than it already is. Also I'm looking for the QUICKEST way to approach this problem in case  it happens to meet me on a test.","['indefinite-integrals', 'integration']"
2434830,When is $\mathbb Z[\zeta_n]$ a Euclidean Domain?,"After having accidentally duplicated this question , I thought I'd follow up with a related question.  In an answer to the linked question, Zev Chonoles quotes the first page of Chapter 11 of Washington's Introduction to Cyclotomic Fields which states that the only $\mathbb Z[\zeta_n]$ with $\zeta_n$ a root of unity that are PID s (with class number 1) are extensions with the following values of $n$: 1,  3,  4,  5,  7,  8,  
 9, 11, 12, 13, 15, 16,
17, 19, 20, 21, 24, 25,
27, 28, 32, 33, 35, 36,
40, 44, 45, 48, 60, 84. In addition, values of $n \equiv 2 \mod 4$ are also allowed, because for example, $\mathbb Z[\zeta_{30}] = \mathbb Z[\zeta_{15}]$.  So my question is, which are known to be Euclidean Domains?  I'm especially interested if $n = 60$ admits a form of the Euclidean division algorithm.","['abstract-algebra', 'euclidean-domain', 'roots-of-unity', 'principal-ideal-domains', 'ring-theory']"
2434847,What can be said about $e_x(f)=f(x)$ in a group?,"This is a continuation of my previous question(linked below). Let $X$ be a set and $(G, \star)$ a group. We denote $G^X$ the set of a mappings from $X$ to $G$. Show that $G^X$ has a group structure induced by $G$ The proof lies here Now I am asked: If $x \in X$, what can be said about this function that maps from $G^X$ to $G$  such that $e_x(f)=f(x)$? And to be honest, I don't know what to say except that it seems to be a group homomorphism because $\forall f,g \in G^X$, $e_x(f \star g) = (f \star g )(x) \in G$ this $(f \star g)(x) = f(x) \star g(x)$. Is there anything else I could notice?",['group-theory']
2434883,Chernoff style bounds for Poisson distribution,"I read today that for a Poisson distributed random variable $Z$ with expectation $\mathbb E(Z) = \lambda$, $$P
(Z \ge \lambda +t) \le \exp\left( -\frac {t^2}{2(\lambda + t/3)} \right).
$$ This is true if $Z$ is binomial $(\lambda, 1)$. In fact, the author cited a reference for binomial distributions. But I do not see how this could transfer to Poisson distribution. As far as I remember, it seems all these Chernoff type inequalities require the random variable under consideration being a sum of bounded random variables. So I am a bit suspicious of this inequality. Do you think it is still true for Poisson distribution?","['reference-request', 'probability-theory']"
2434886,Variance of first n binary digits of pi?,I recently asked myself a question but didn't have the slightest inkling on how to solve it. Question If we convert $\pi$ to a binary digit. We know the first $n$ digits the average number of $0$'s are the same as the average number of $1$'s. My question is what is the variance as a function of the first $n$ digits?,"['statistics', 'pi']"
2434894,When to use forward difference over central difference/three point difference?,"I understand that central difference and three point difference namely $\frac{−3 f(x) +4 f(x+h)− f(x+2h)}{2h}$ provide approximations of the first derivative up to a term of order $h^2$. Forward difference only approximates up to a term of order $h$. So for most situations central difference would be preferred over both three point difference (denominator contains 3! rather than 3) and forward difference. In what situations would forward difference be better than both central or three point difference? In what situations would three point difference be better? I can really only think of one situation where three point would be better where we want to approximate an x for which we know nothing about $f(x_0), x_0<x$. I cannot think of any situations that forward difference would be better assuming f(x) is smooth","['numerical-methods', 'ordinary-differential-equations', 'finite-differences', 'approximation']"
2434955,A property of positive convergent series,"Let $(a_k)_{k\geq 1}$ be a positive sequence such that the series $\sum_{k\geq 1} a_k$ is convergent to $L$. Then, by C-S inequality, for any $n\geq 1$, 
$$f(n):=\sum_{k=1}^n \frac{1}{a_k}\geq \frac{n^2}{\sum_{k=1}^n a_k}> \frac{n^2}{L}.$$
Is it true that eventually $f(n)>n^2\ln(n)$ (i.e. there is $n_0$ such that the inequality holds for all $n\geq n_0$).","['inequality', 'sequences-and-series']"
2434956,If $|G|=2n$ where $n$ is odd. Then $\prod_{g\in G}g\notin H$,"If $|G|=2n$ where $n$ is odd and $H$ is a subgroup of order $n$. Then $\underset{g\in G}{\prod}g\notin H$ pf: Since $G$ has even order, it must have a element of order $2$, then $\underset{g\in G}{\prod}g$ has even order. So $\underset{g\in G}{\prod}g\notin H$ as the order of everything in $H$ must divide $n$. Is this correct?","['abstract-algebra', 'group-theory', 'proof-verification']"
2434965,Find the number of $n$ husband's placing,"Let there be $n$ pairs of husband-wife, and a round table with $2n$ chairs. Suppose that $n$ wives are already sat down , and between any two neighboring wives there is exactly one free chair (there is an alternation of empty chairs and occupied wives, $\dots,\text{chair}_n,\text{wife}_1,\text{chair}_1,\text{wife}_2, \text{chair}_2,\dots$). Find the number of $n$ husband's placing on
the remaining chairs, where only $r$ husbands, $ 0 \leq r \leq n $, are directly near with their wives? I think, about Inclusion–exclusion principle in this issue; we can called $\alpha_i$ - ""$i$-th man sitting with his wife"". Then look at $ \alpha_{i_1}\alpha_{i_2}\cdots\alpha_{i_r}$ - ""$\geq r$ husbands sitting near their wives"": $$ n! - |\bar\alpha_{i_1}\bar\alpha_{i_2}\cdots\bar\alpha_{i_r}| = |\alpha_{i_1}\cup\alpha_{i_2}\cup\dots\cup\alpha_{i_r}| = |\alpha_{i_1}| + |\alpha_{i_2}| +\dots + |\alpha_{i_r}| - |\alpha_{i_1}\alpha_{i_2}| - |\alpha_{i_1}\alpha_{i_3}| - \dots - |\alpha_{i_r}\alpha_{i_{r-1}}| + \dots + (-1)^{r}|\alpha_{i_1}\alpha_{i_2}\cdots\alpha_{i_r}|$$ If we know about $A_{\geq r}$, we can find $A_{=r} = A_{\geq r} -  A_{\geq r+1}$. I stuck with calculation of it; or may be my $\alpha_i$ are not adequate for this problem.","['inclusion-exclusion', 'combinatorics']"
2434982,Why is the definition of a proper group action the way it is?,"Let $G$ be a topological group acting continuously on a topological space $X$ .  This means that $G \times X \rightarrow X$ is a continuous function. A continuous map $Y \rightarrow Z$ is said to be proper if the preimage of a compact set is compact. If $G$ acts continuously on $X$ , then the action is said to be proper if the map $$G \times X \rightarrow X \times X, (g,x) \mapsto (x, g\cdot x)$$ is proper. I would have expected the definition of a proper group action to be that $G \times X \rightarrow X$ is proper, not $G \times X \rightarrow X \times X$ .  Why is the definition the way it is?  What is the most natural way to think about this? There are two situations in which I have encountered this notion which I want to understand better: 1 .  An analytic Lie group $H$ over a local field of characteristic zero is acting properly and freely on an analytic manifold $X$ , and for each $x \in X$ , the map $h \mapsto h.x$ is an immersion $H \rightarrow X$ .  Then some result in Bourbaki, Differential and Analytic Manifolds says that the quotient space $H \setminus X$ has the natural structure of an analytic manifold. 2 .  There is an arrangement $\mathscr H$ of hyperplanes in a real affine space $E$ , and $W$ , the group of affine transformations in $E$ generated by the reflections about the hyperplanes, is assumed to stabilize the set of hyperplanes $\mathscr H$ and act properly on $E$ .  This is the situation for many results described in Chapter V of Bourbaki, Lie Groups and Lie Algebras .","['topological-groups', 'compactness', 'general-topology', 'group-theory', 'lie-groups']"
2434983,$a!b!$ multiple of $a! + b!$ implies $3a\geq 2b + 2$,"A colleague asked me to show that if $a,b\in\mathbb{N}^*$ are such that $a!b!$  is a multiple of $a! + b!$, then $3a\geq 2b + 2$. While I could verify this inequality numerically for many solutions $(a,b)$, I did not find anything relevant mathematically. I must say I have never faced such inequalities in arithmetic... 
The problem is symmetric in $a$ and $b$, and actually I could see that the solutions where all such that $3a\geq 2b + 2$ and $3b\geq 2a + 2$. Here is a plot of the first solutions $(a,b)$ and the bounds: The solutions can be generated with the Mathematica code: Table[If[IntegerQ[a!*b!/(a! + b!)], {a, b}, 
 Unevaluated[Sequence[]]], {a, 1, 100}, {b, 2, 100}]","['inequality', 'divisibility', 'number-theory', 'contest-math', 'factorial']"
2434998,Find limits of sequences,"Prove that $$a) \lim\limits_{n \to \infty} (\frac{1^p+2^p+...+n^p}{n^p} - \frac{n}{p+1})=\frac{1}{2},$$ $$b) \lim\limits_{n \to \infty} \frac{1^p+3^p+...+(2n-1)^p}{n^{p+1}}=\frac{2^p}{p+1},$$ where is $p \in \Bbb N $. Thanks to Stolz–Cesàro theorem in $a)$ I went to $$\lim\limits_{n \to \infty} \frac{(n+1)^p}{(n+1)^p-n^p}$$ which after dividing by $n^p$ goes to $$\lim\limits_{n \to \infty} \frac{1+\frac{p-1}{n}+\frac{p(p-1)}{2n^2}+...+\frac{1}{n^p}}{\frac{p-1}{n}+\frac{p(p-1)}{2n^2}+...+\frac{1}{n^p}} = \frac{1}{\infty}$$
At the same time $$\lim\limits_{n \to \infty} \frac{n}{p+1}=\infty,$$ so initial limit should be $\frac{1}{\infty}-\infty = ?$ I can't figure out what I'm missing ( $b)$ seems to complicated to me so I didn't even try it).",['real-analysis']
2435010,What's the next logical step after studying multivariable/vector calculus?,"As part of an engineering degree, I've taken courses on single variable calculus, linear algebra, multivariable/vector calculus and ordinary differential equations. I'm also about to study an advanced engineering mathematics course, which covers three topics (albeit quite superficially): Complex variable calculus, fourier series/transform and partial differential equations. Seeing how multivariable/vector calulus generalizes concepts like limits, derivatives and integrals in elegant and beautiful ways, and introduces new concepts like vector fields that feel natural and intuitive, I was wondering if there's a branch in mathematical analysis that does the same thing, extending multivariable calculus to something else, which I would be able to study by myself using the knowledge I have from the courses previously mentioned. I've heard about things like tensor calculus, calculus of variations and differential geometry, but I'm not sure if any of those would be what I'm looking for (a logical next step form multivariable calculus). Any recommendations on books for self-studying the subject/subjects you consider appropiate are greatly appreciated.","['multivariable-calculus', 'self-learning', 'analysis']"
2435018,Solve $\frac{1}{\sin^2{(\arctan{x})}}-\frac{1}{\tan^2{(\arcsin{x})}}=4x^2.$,"I just need help to finetune this solution. Only having a correct answer is not sufficient to comb home 5/5 points on a problem like this. Thoughts on improvements on stringency? Is there any logical fallacy or ambiguity? Any input is very welcome! Solution: The domain for $\arctan{x}$ is $\mathbb{R},$ thus the function $\sin{(\arctan{x})}$ is defined over the entire $\mathbb{R}$. However, the restricting factor comes from the fact that the domain for $\arcsin{x}$ is $[-1,1]$ which implies that the domain for $\tan{(\arcsin{x})}$ is $[-\arctan{1},\arctan{1}]=[-\frac{\pi}{4},\frac{\pi}{4}].$ We can thus conclude that if there exists a solution $x$ to the above equation, then $x\in[-\frac{\pi}{4},\frac{\pi}{4}]$. First I note that $$\sin{b}=\pm\frac{\tan{b}}{\sqrt{\tan^2{b}+1}}\Rightarrow\sin^2{(\arctan{x})}=\left(\pm\frac{\tan{(\arctan{x})}}{\sqrt{\tan^2{(\arctan{x})}+1}}\right)^2=\frac{x^2}{x^2+1}.$$ Secondly; $$\tan{c}=\pm\frac{\sin{c}}{\sqrt{1-\sin^2{c}}}\Rightarrow \tan^2{\arcsin{x}}=\left(\pm\frac{\sin{(\arcsin{x})}}{\sqrt{1-\sin^2{(\arcsin{x})}}}\right)^2=\frac{x^2}{1-x^2}.$$ So the equation becomes $$\frac{x^2+1-1+x^2}{x^2}=\frac{2x^2}{x^2} =4x^2\Longleftrightarrow x=\pm\frac{1}{\sqrt{2}}.$$ What's left to do now is to show that these $x:$s both lie in the desired interval. So it boils down to showing that $\frac{1}{\sqrt{2}}\leq\frac{\pi}{4}$. This can be done by multiplying the inequality by $\sqrt{2}/\sqrt{2}$: $$\frac{\sqrt{2}}{4}\leq \frac{\pi}{4}\Longleftrightarrow \sqrt{2}\leq \pi,$$ which clearly is true.","['trigonometry', 'calculus']"
2435036,Exactly two functions,"Show that there exist exactly two functions $f : \Bbb Q → \Bbb  Q$ with the property
  $f(x + y) = f(x) + f(y)$ and $f(x · y) = f(x) · f(y)$
  for all $x, y \in \Bbb  Q$. I am unsure how to prove that there are no more than two functions that meet the requirements. I can come up with an example $f(x) = x$ which obviously satisfy the constraints but I can't see a way to prove that there are only two functions.",['functions']
2435055,How does this application of the chain rule work?,"Let $\bar{x_1} = x_1 \cos(x_2)$ and $ \bar{x_2} = x_1 \sin(x_2)$
Suppose that $f:\mathbb{R}^{2} \to \mathbb{R}^{2}$ is a smooth function of $\bar{x_1}$ and $\bar{x_2}.$ Show that: $(\frac{\partial{f}}{\partial\bar{x_1}})^2+(\frac{\partial{f}}{\partial\bar{x_2}})^2 = (\frac{\partial{f}}{\partial{x_1}})^2+ \frac{1}{x_1^2}(\frac{\partial{f}}{\partial{x_2}})^2$. Using the Chain Rule: $\frac{\partial{f}}{\partial{x_1}} = \frac{\partial{f}}{\partial{\bar{x_1}}} \frac{\partial{\bar{x_1}}}{\partial{x_1}}  + \frac{\partial{f}}{\partial{\bar{x_2}}}\frac{\partial{\bar{x_2}}}{\partial{x_1}}$
 and $\frac{\partial{f}}{\partial{x_2}} = \frac{\partial{f}}{\partial{\bar{x_1}}} \frac{\partial{\bar{x_1}}}{\partial{x_2}}  + \frac{\partial{f}}{\partial{\bar{x_2}}}\frac{\partial{\bar{x_2}}}{\partial{x_2}}$ $\frac{\partial{\bar{x_1}}}{\partial{x_1}} = \cos{x_2}$, $\frac{\partial{\bar{x_2}}}{\partial{x_1}}=  \sin{x_2
}$, $\frac{\partial{\bar{x_1}}}{\partial{x_2}} = -x_{1}\sin{x_2}$, $\frac{\partial{\bar{x_2}}}{\partial{x_2}} = x_{1}\cos{x_2}$ $\partial_{x_1}{f} = \cos{x_2}\partial_{\bar{x_1}}{f}+\sin{x_2}\partial_{\bar{x_2}}{f}$ $\partial_{x_2}{f} = -x_{1}\sin{x_2}\partial_{\bar{x_1}}{f}+x_{1}\cos{x_2}\partial_{\bar{x_2}}{f}$ $(\partial_{x_1}{f})^{2} = (\frac{\partial f}{\partial \bar{x}_1} \cos{x_2}+\frac{\partial f}{\partial \bar{x}_2} \sin{x}_2) ^{2} = (\frac{\partial f}{\partial \bar{x}_1}) ^{2} \cos^{2}x_2 + 2 \frac{\partial f}{\partial \bar{x}_1} \frac{\partial f}{\partial \bar{x}_2} \cos{x}_2 \sin{x}_2+(\frac{\partial f}{\partial \bar{x}_2})^{2}\sin^{2}x_2$ $(\partial_{x_2}{f})^{2} = (\frac{\partial f}{\partial \bar{x}_1} (-x_1\sin x_2) + \frac{\partial f}{\partial \bar{x}_2}x_1\cos x_2)^{2} =  \frac{\partial f}{\partial \bar{x}_1}^{2}(x_1^{2}\sin^{2}x_2)+2\frac{\partial f}{\partial \bar{x}_1}\frac{\partial f}{\partial \bar{x}_2}(-x_1\sin x_2)(x_2\cos x_2) + (\frac{\partial f}{\partial \bar{x}_2})^{2}x_1^{2}\cos^{2}x_2$ $\frac{\partial f}{\partial \bar{x}_2}^{2} = (x_1)^{2}[(\frac{\partial f}{\partial \bar{x}_1})^{2}\sin^{2}x_{2}-2\sin x_2\cos x_2 \frac{\partial f}{\partial \bar{x}_1}\frac{\partial f}{\partial \bar{x}_2}+(\frac{\partial f}{\partial \bar{x}_2})^{2}\cos^{2}x_2]$ So $(\frac{\partial f}{\partial x_1})^{2}+ \frac{1}{x_1^{2}}(\frac{\partial f}{\partial x_2})^{2} = \frac{\partial f}{\partial \bar{x}_1}^{2}(\cos^2 x_2 + \sin^2 x_2) + (\frac{\partial f}{\partial \bar x_2})^2(\sin^2 x_2+\cos^2 x_2)$ $= (\frac{\partial f}{\partial \bar{x}_1})^{2}+(\frac{\partial f}{\partial \bar{x}_2})^{2}$ When solving for $\frac{\partial f}{\partial x_1}$, isn't $x_1$ dependent on $x_2$ and $\bar{x}_1$ and $\bar{x}_2$? So why is his $\frac{\partial f}{\partial x_1}$ formulated in a way that does not include $x_2$? And also My professor only showed the right side of the equation without expanding the LHS. How would someone know that both the RHS and LHS are equivalent without the answer? Would they need to expand the LHS to see? and if you were to expand the LHS, how would $\frac{\partial f}{\partial \bar{x}_1}$ look?",['multivariable-calculus']
2435065,Poincaré-Bendixson Theorem and diffusion,"In this paper , the author proved the existence of a global compact attractor of the following system And then the author deduced the existence of a periodic solution: I feel like he somehow used a variant of the Poincaré-Bendixson Theorem for systems with diffusion despite the infinite dimensional aspect of such systems. I know that the the Poincaré-Bendixson Theorem fails even for dimension 3 ODE systems. Maybe here the diffusion does not affect the behavior. Do such Poincaré-Bendixson type theorems exist?","['periodic-functions', 'dynamical-systems', 'partial-differential-equations', 'functional-analysis', 'ordinary-differential-equations']"
2435077,Why does subtracting the equations of two intersecting circles give the line that intersects the points of intersection?,"Why does subtracting the equations of two intersecting circles give the line that intersects the points of intersection? In other words, why does the elimination method give a line that also intersects the point of intersection?","['circles', 'linear-algebra', 'algebraic-geometry']"
2435117,Prove that there exists a $c$ in $\mathbb{R}$ such that $f(c)=g(c)$,"There is this problem with an exercise in my maths book, it's included in the chapter on continuity. Consider two functions $f:\mathbb{R}\to\mathbb{R}$ and $g:\mathbb{R}\to\mathbb{R}$, with the property that there exist $a,b\in\mathbb{R}$ so that $f(a)=g(b)$ and $f(b)=g(a)$. 
  Prove that there exists a $c$ in $\mathbb{R}$ such that $f(c)=g(c)$ Also, I've considered another function $h$ and I have come at the end to $h(a)=h(b)$, then two cases arose: $a=b$ or $a\neq b$. For the first case I did prove that the $c$ exists, but for the second one I couldn't think of anything. How could you use continuity (and maybe intermediate values)  in this problem?","['continuity', 'functions']"
2435118,Proof of the delta method,"The classical, well known delta method states the following:
If $\sqrt{n}(X_{n}-\theta)\overset{law}{\longrightarrow}N(0,\sigma^{2})$. Then the following holds:
$\sqrt{n}(g(X_{n})-g(\theta))\overset{law}{\longrightarrow}N(0,\sigma^{2}(g'(\theta))^{2})$ for any function $g$ satisfying the property that $g'(\theta)$ exists and is non-zero valued. 
The key step, proving this result, is the following expression:
$g(X_{n})=g(\theta)+g'(\overline{\theta})(X_{n}-\theta)$ for some intermediate value $\overline{\theta}$ with $X_{n}<\overline{\theta}<\theta$. 
What exactly ensures the existence of such a $\overline{\theta}$? It should follow using Taylor's theorem, but I am not able to argue rigorously.","['probability-theory', 'probability', 'statistics']"
2435122,Change of variable on a probability density function - $\sin(x)$,"Problem: Let $y = \sin{X}$, where $X$ is a uniformly distributed over $(0, 2 \pi)$. Find the pdf of $Y$. Answer: In this case we have:
\begin{eqnarray*}
f_x(x) &=& \frac{1}{2 \pi} \\
h(x) &=& \sin(x) \\
\end{eqnarray*}
We know that:
\begin{eqnarray*}
f_y(y) &=& f_y(x) \Big| \frac{dx}{dy} \Big| \\
\end{eqnarray*}
\begin{eqnarray*}
\sin^{-1}y &=& x \\
\frac{dx}{dy} &=& \frac{1}{\sqrt{1 - y^2}} \\
f_y(y) &=& \frac{1}{2 \pi} \Big| \frac{1}{\sqrt{1 - y^2}} \Big| \\
\end{eqnarray*}
Now we have to consider the limits. The maximum value of $\sin(x)$ is $1$. The minimum value of $\sin(x)$ is $-1$.
\begin{eqnarray*}
f_y(y) = \begin{cases}
\frac{1}{2 \pi \sqrt{1 - y^2}} & -1< y < 1 \\
0	& otherwise \\
\end{cases}
\end{eqnarray*}
However, the book gets:
\begin{eqnarray*}
f_y(y) &=& \begin{cases}
\frac{1}{\pi \sqrt{1 - y^2}} & -1 < y < 1 \\
0	& otherwise \\
\end{cases}
\end{eqnarray*}
What am I missing? Thanks, Bob","['probability-theory', 'probability']"
2435179,Is there an algebraic formula that gives this weird multiplication?: $(-x)\circ(-x)=(-x)\circ(x)$,"I'd like to know whether it's possible to give an equivalent algebraic formula, in terms of normal algebraic operations (i.e. $+, -, ×, ÷, x^y$ ), if possible avoiding $|x|$ , for an operator $\circ$ , in the domain ℤ such that: \begin{array}{|r | r r r r | r r r | r r r r}
\hline
\circ & ... & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & ... \\ \hline
\vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} \\
-4 & ... & -16 & -12 & -8 & -1 & 0 & -4 & -8 & -12 & -16 & ... \\
-3 & ... & -12 & -9 & -6 & -1 & 0 & -3 & -6 & -9 & -12 & ... \\
-2 & ... & -8 & -6 & -4 & -1 & 0 & -2 & -4 & -6 & -8 & ... \\ \hline
-1 & ... & -4 & -3 & -2 & -1 & 0 & -1 & -2 & -3 & -4 & ... \\
0 & ... & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & ... \\
1 & ... & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & ... \\ \hline
2 & ... & -8 & -6 & -4 & -2 & 0 & 2 & 4 & 6 & 8 & ... \\
3 & ... & -12 & -9 & -6 & -3 & 0 & 3 & 6 & 9 & 12 & ... \\
4 & ... & -16 & -12 & -8 & -4 & 0 & 4 & 8 & 12 & 16 & ... \\
\vdots & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
\end{array} As you see, this is some sort of weird multiplication where $(-)\cdot(-)=(-)\cdot(+)=(+)\cdot(-)=(-)$ but $(+)\cdot(+)=(+)$ . I am mostly interested in the subcase of the square in the middle and, if possible, all the $\circ$ operations where at least one of $-1$ , $0$ or $1$ is an argument. If that operation can satisfy at least the square at the middle, that will be enough for me. As a last resort I'm disposed to accept division by zero defined in such a way that for all $x$ , $x/0=0$ . But it is important to note that, even if it doesn't matter too much what happens outside the domain $\{-1, 0, 1\}$ , the operation must be defined for all integers: no modules, no restricted domains. If that isn't possible, what strategy shall I use to prove it? ps: Since I'm not a mathematician, I'd like to apologise for any formal or conceptual error I've made. Corrections, though, are more than encouraged.","['abelian-groups', 'abstract-algebra', 'group-theory']"
2435196,What am I not understanding about Sequences?,"So I have the following equation: $$c(1)=6$$
$$c(n)=c(n-1)-16$$ -Find the third sequence- The way I do it:
$$c(2)=6(2-1)-16 = -10$$
$$c(3)=-10(3-1)-16 = -36$$
What is it that I do wrong? Why is the correct answer: $$c(3) = -26$$","['algebra-precalculus', 'sequences-and-series']"
2435252,Automorphisms of $\mathbb P^1_{\mathbb k}$ over a algebraically closed field $\mathbb k$.,"Let's consider an algebraically closed field $\mathbb k$. Consider the birational map $\phi: \mathbb P^1_{\mathbb k} \to \mathbb P^1_{\mathbb k}$. How do I show that if there are $f,g$, homogeneous poynomials, such that $$\phi[x:y] = [f(x,y):g(x,y)],$$  then $\deg(f) = \deg(g)=1$? In other words, how do I show that $\phi$ is a element of $\mathrm{PGL}_2(\mathbb k)$? So far I was able to prove that $\deg(f) = \deg(g)$. Let me give you this part of the argument: Take $[x:y]\in\mathbb P^1_{\mathbb k}$ such that $\phi[x:y]=[1:1]$. We must have $$ f(\mu x,\mu y)g(x,y) -g(\mu x,\mu y)f(x,y)=0\quad \forall  \mu \in \mathbb k\setminus\{0\},$$
because $[f(x,y):g(x,y)]=[f(\mu x,\mu y):g(\mu x,\mu y)]$. Using that $f(x,y)=g(x,y)\neq 0$ we conclued that the above equation can be written as $$ [\mu^{\deg(f)}-\mu^{\deg(g)}]f(x,y)^2=0\quad \forall  \mu \in \mathbb k\setminus\{0\}$$
and therefore $\deg(f)=\deg(g)$.","['mobius-transformation', 'algebraic-geometry', 'abstract-algebra', 'algebraic-curves', 'field-theory']"
2435270,Moscow Math Olympiad 1962 2 reflecting equilateral triangle [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have difficulty to prove this problem from Moscow Mathematical Olympiad 1962. We reflect an equilateral triangle with one marked side through one of its sides. Then we similarly reflect the resulting triangle, etc., until at a certain step the triangle returns to its initial position. Prove that the marked side also returns to its initial position and also another one follows: Prove that the number of reflections is even.","['contest-math', 'triangles', 'reflection', 'geometry']"
2435373,Can we extend a linearly independent list in a f.g. module of a commutative ring?,"Let's say we're given a commutative ring $A$ with identity. We are given that $M$ is a f.g. $A$ -module. For any elements $m_1,\dots,m_n$ of $M$ , we have an $A$ -module homomorphism $\phi_{m_1,\dots,m_n}$ from $A^n$ to $M$ defined by $\phi_{m_1,\dots,m_n}(a_1,\dots,a_n)=a_1m_1+\dots+a_nm_n$ . Thus $m_1,\dots,m_n$ becomes a linearly independent list if and only if $\phi_{m_1,\dots,m_n}$ is injective, a span list if and only if $\phi_{m_1,\dots,m_n}$ is surjective. Now we say that $m_1,\dots,m_n$ is a basis for $M$ if and only if $\phi_{m_1,\dots,m_n}$ is bijective. Suppose that we are given a linearly independent list $n_1,\dots,n_k$ of $M$ , we say $n_1,\dots,n_k$ can be extended to a basis of $M$ if $n_1,\dots,n_k,m'_1,\dots,m'_r$ is a basis for $M$ for some $m'_1,\dots,m'_r \in M$ . I came up with this question when I was trying to generalize the following proposition on Sheldon Axler's Linear Algebra Done Right : Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.","['modules', 'linear-algebra', 'commutative-algebra']"
2435456,Betti numbers of quotients of spheres by circle actions,"Suppose $X=\mathbb{S}^{n+1}/\mathbb{S}^1$, the action of $\mathbb{S}^1$ being isometric and with closed orbits, and $n=\dim X$ (as an Alexandrov space) being even . My question is: Is is true that all odd Betti numbers of $X$ vanish? I know this is true when the action is almost free because in this case $X$ is a weighted projective space, by the classification of riemannian $1$-foliations on spheres, but i think there must be some simpler way to show this in general.","['algebraic-topology', 'group-actions', 'differential-geometry', 'geometric-topology']"
2435473,A question about Sturm Liouville of transforming it into normal form,"I have a question about Sturm Liouville problem. Given a SL equation, say $\frac{d^2 y}{dx^2}+p(x)\frac{d y}{dx}+(q(x)+\lambda r(x))y=0$, how to transform it to its normal form $\frac{d^2 \eta}{d\xi^2}+(\phi(\xi)+\lambda)\eta=0$? How one can think of the transformation? Note that I need to change both of independent and dependent variables. Indeed, I can find corresponding transformation but I would like to know how to think of the corresponding transformation. What's more, if I do not consider changing independent variables, I can change to normal form as follow: Let $y=u(x)v(x)$, differentiating and let the coefficients involving first order term to be 0, one can solve $u(x)$ and get an equation of $v(x)$ in normal form. However, many authors use the transformation of both independent and dependent variables when I read their papers. The question is what are the benefits of considering transformation of both independent and dependent variables?","['sturm-liouville', 'ordinary-differential-equations', 'proof-verification']"
2435479,PDE with strange Auxiliary Conditions,"I am currently trying to find an $\textit{explicit solution}$ to this question but am getting stuck. So the question reads: Find an explicit solution for the PDE:
  $$u_t+u\cdot u_x=0, \>\> u(x,0)=g(x)=\begin{cases}
1 &= \text{ if } x\leq 0\\
1-x &= \text{ if } 0\leq x \leq 1\\
0 &= \text{ if } x\geq 1
\end{cases}$$ 
  For all $x$ and $0\leq t\leq 1$. Now, I found the Characteristics:
$$\frac{\partial x}{\partial t}=z(t), \>\>\> \frac{\partial z}{\partial t}=0$$
Where $z(t)$ is the term on the RHS of the PDE. Now, setting the initial conditions:
$$x(0)=x_0, \>\> z(0)=g(x_0)$$
And the solutions are given by:
$$z(t)=g(x_0), \>\>\>\> x(t)=g(x_0)\cdot t +x_0$$ Can I conclude that $u(x,t)=g(x_0)$? This seems a bit too easy, however I know that I can set $u=\frac{x-x_0}{t}$, so would I have to rearrange the solution in terms of $x_0$ for each case?","['ordinary-differential-equations', 'partial-differential-equations']"
2435490,Show that $\lim_{n\to\infty}\frac{1}{n^4}\sum_{j=1}^{n}\left((2j-1)\sum_{k=1}^{n+1-j}k\right)=\frac{1}{12}$,"Show that$$\lim_{n\rightarrow \infty} \frac{1}{n^4} \left(1\left(\sum_{k=1}^{n}k\right)+ 3\left(\sum_{k=1}^{n-1}k\right)+5\left(\sum_{k=1}^{n-2}k\right)+\cdots+(2n-1)\cdot1\right)=\frac{1}{12}$$ 
This is getting more and more complicated as number of terms are decreasing.","['summation', 'sequences-and-series', 'limits']"
2435501,"If $a,b,c$ are positive reals, then prove that $(a+b)(a+c)\ge 2 \sqrt {abc(a+b+c)}$","If $a,b,c$ are positive reals, then prove that $(a+b)(a+c)\ge 2 \sqrt {abc(a+b+c)}$ My tries: By applying AM-GM inequality, $$a+b\ge 2\sqrt 
{ab}$$
and,
$$a+c\ge2\sqrt{ac}$$
and clearly LHS $=a^2+ac+ba+bc$, which is $\ge4a\sqrt{bc}$ Similarly, $(b+a)(b+c)\ge4b\sqrt{ac}$ and $(c+a)(c+b)\ge4c\sqrt{ab}$ What next?(I gave the Cauchy Schwarz tag because I do not know if this can be solved by Cauchy Schwarz inequality.)","['inequality', 'a.m.-g.m.-inequality', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'contest-math']"
