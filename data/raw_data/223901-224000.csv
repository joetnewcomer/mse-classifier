question_id,title,body,tags
4606549,Quasinormal subgroup which is not subnormal,"A subgroup of a group $G$ is deemed quasinormal (or permutable ) if it permutes with all other subgroups of $G$ (i.e., $N$ is quasinormal in $G$ iff $NH = HN$ for all $H \leq G$ ). It is known that, if $G$ is finite, then every quasinormal subgroup of $G$ is subnormal (meaning there is a finite chain of type $H_1 \lhd H_2 \lhd … \lhd G$ ). This post contains an explicit example that the converse is not true. However, another question that comes up, especially after seeing the Wikipedia article on the subject, which states that “finiteness is essencial”, is: What is an example of a group (which must be infinite and nonabelian) and a quasinormal subgroup that is not subnormal? I immediately thought of matrix groups as a possible place to look, but I don’t really know what subgroups to look for. Thanks in advance!","['group-theory', 'normal-subgroups', 'infinite-groups', 'examples-counterexamples']"
4606557,A person A was issued a $10$ digit mobile phone number having his $6$ digit birth date (DDMMYY format) in it.,"A person $'A'$ was issued a $10$ digit mobile phone number having his $6$ digit birth date (DDMMYY format) in it. Let $E$ be the event that his birth was in first $9$ days of a month, then the probability of the occurrence of the event $E$ is My Approach: There are $5$ ways to choose $6$ places for birth date in (DDMMYY) format. $n(E)=5\cdot9\cdot 12\cdot 100 \cdot 10^{4}$ Note: There are $9$ ways to select DD, $12$ ways to select MM and $100$ ways to select YY remaining $4$ places can be filled in $10^{4}$ ways. Sample Space: $n(S)=5\cdot30\cdot12\cdot100\cdot 10^{4}$ Note: There are $30$ ways to select DD considering $30$ days in a month. $12$ ways to select MM and $100$ ways to select YY remaining $4$ places can be filled in $10^{4}$ ways. So, $P(E)=\dfrac{n(E)}{n(s)}=\dfrac{9}{30}=0.3$ but given answer is $(.72)$ . Where am I going Wrong? Similar Question What are the odds are getting your 6-digit birth date in your 10-digit phone number?","['algebra-precalculus', 'combinatorics', 'probability']"
4606582,When are the partition numbers squares?,"I'm unsure if this question is even interesting. I am playing around with partition numbers $p(n) :=$ # partitions of $n$ , and I noticed that $p(n)$ never really is a square number, except for of course $n = 0, 1$ . Is there a simple reason for this? It also seems to be similar for other modified partition numbers, such as $p_{t}(n) :=$ #t-regular partitions of $n$ (none of its parts are divisible by $t$ ), at least for $t = 2, 3, 4$ and a few more. In these cases the only square $p_{t}(n)$ are when $p_{t}(n) \in \{0,1\}$ . For example, $p_{11}(n)$ is square (apparently) only when $0 \leq n \leq 6$ , in which case $p_{11}(n) \in \{0, 1\}$ . It gets a little weirder when we consider $p_{s, t}(n) :=$ #s-regular and t-distinct partitions of $n$ (no part appears $t$ or more times). For example, when $(s, t) = (7, 9)$ we have $p_{7, 9}(0) = 1 = p_{7, 9}(1)$ and $p_{7, 9}(11) = 49$ , $p_{7, 9}(44) = 35721 = 189^2$ . (It is also somewhat interesting that the input/outputs are either divisible by 7 or 9). These are the only ones up to $10^4$ (very slow program so hard to go past this). For convenience, the generating function for $p_{7, 9}(n)$ is $\prod_{n\geq 1} \frac{(1-q^{7n})(1-q^{9n})}{(1-q^{n})(1-q^{63n})}$ . Somewhat similar results for other pairs of $(s, t)$ . Maybe this has to do with the generating functions?","['integer-partitions', 'number-theory', 'combinatorics']"
4606619,Is it true that $\limsup_{n\to\infty}(\sup_{x\in B}f_n(x))\leq\sup_{x\in B}(\lim\sup_{n\to\infty}f_n(x))$?,"Let $\left(f_n\right)_n$ be a sequence of functions $f_n:A\to [-\infty,\infty]$ and $\varnothing\neq B\subset A$ . Is it then true that $\limsup_{n\to\infty}\left(\sup_{x\in B}f_n(x)\right)\leq\sup_{x\in B}(\lim\sup_{n\to\infty}f_n(x))$ ? This is really a question on whether I understand the definitions of limsup and sup and as of now the answer seems to be no. We can write both expressions as $$\lim\sup_{n\to\infty}\left(\sup_{x\in B}f_n(x)\right) = \inf_{n\to\infty}\left(\sup_{m\geq n}\left(\sup_{x\in B}f_m(x)\right)\right)\Longleftrightarrow$$ $$\lim\sup_{n\to\infty}\left(\sup_{x\in B}f_n(x)\right) = \inf_{n\to\infty}\left(\sup_{m\geq n,x\in B}f_m(x)\right)$$ and $$\sup_{x\in B}\left(\lim\sup_{n\to\infty}f_n(x)\right) = \sup_{x\in B}\left(\inf_{n\to\infty}\left(\sup_{m\geq n}f_m(x)\right)\right)$$ If we were to consider the finite version for max and min, I would like to say that the inequality should be reversed for any $x\in A$ , maximizer or not, $f_n(x)\leq \max_{x\in A}\{f_n(x)\}$ . But I don't know how to properly analyze the limsup in this case.","['limsup-and-liminf', 'real-analysis', 'sequences-and-series', 'limits', 'supremum-and-infimum']"
4606621,Reconciliation of definition of presentable module/quasi-coherent sheaf with the fact that all modules have free presentation,"Presentable module is defined in nLab ( https://ncatlab.org/nlab/show/presentable+module ) as “the cokernel of a homomorphism of free modules”. What I’m confused about is that, according to https://en.wikipedia.org/wiki/Free_presentation , given a module $M$ over $R$ , “a free presentation always exists”, so shouldn’t that mean any module is presentable? So what would it look like for a module not to be presentable? Are there particular constraints on what sequences “count” that I’m missing? Or maybe $M$ isn’t always a “cokernel” in these sequences? Or maybe I’m muddling two different definitions? Put another way, in the definition of a quasi-coherent sheaf ( https://en.wikipedia.org/wiki/Coherent_sheaf ), we require the existence of a certain exact sequence $$
\mathcal{O}_X^{\oplus I}|_{U} \longrightarrow \mathcal{O}_X^{\oplus J}|_{U} \longrightarrow \mathcal{F}|_{U} \longrightarrow 0 \,,
$$ but by the above, given that $\mathcal{F}|_{U}$ is by definition a module over $\mathcal{O}_X|_{U}$ , shouldn’t such a sequence always exist?","['quasicoherent-sheaves', 'definition', 'algebraic-geometry', 'modules']"
4606630,How to determine if this graph is planar?,"I was doing a graph theory text book where one of the problems asks: Is this graph planar? As this graph contains a triangle, the best bound for $e$ is $3v-6$ which this satisfies $(14<18)$ So then I tried to look for a subgraph that could be smoothed into $K_5$ but quickly realised it wouldn't be possible because in this graph, Maximum degree is only $4$ . So there can only be 2 ways to end this problem now, either draw a planar embedding or find a subgraph that contains $K_{3,3}$ . For the first one, the thing to notice is that the inner square is basically a $K_4$ and the only way I know how to draw it as a plane graph is kind of like a triangle and its centroid system, in that case it is quiet clear that a planar representation won't be possible (because for instance the edge that joins centroid and outer square vertex must cross triangle). I also tried for long to find a subdivision on $K_{3,3}$ that is isomorphic to a subgraph of the graph but I could not do so... I would really appreciate any help... Thank you!","['graph-theory', 'puzzle', 'combinatorics', 'planar-graphs']"
4606642,Is this PDE $xu_x-yu_y=u$ Well posed?,"We have the given problem $$xu_x-yu_y=u,\ x>0, \ y>0 \  (1)$$ $$u(x,x)=x^2, \ x>0, \ (2)$$ They ask to check if the problem is well-posed and solve it next. I know that a problem is well-posed if: It has a solution The solution is unique A small change to PDE/side conditions produce only small changes in the solution. Also, we can observe that the PDE is linear, first order and homogeneous if we write it in the form $$xu_x-yu_y-u=0,\ x>0, \ y>0 \  (1)$$ $$u(x,x)=x^2, \ x>0, \ (2)$$ With characteristic curves $$\frac{dx}{dt}=x\Leftrightarrow \frac{1}{x} \frac{dx}{dt}=1$$ $$\frac{dy}{dt}=-y \Leftrightarrow -\frac{1}{y} \frac{dy}{dt}=1 $$ and $$\frac{du}{dt}=u \Leftrightarrow \frac{1}{u} \frac{du}{dt}=1$$ From which we obtain : $\begin{cases}
yx=c_1 \\
\frac{u}{x}=c_2 \\
\end{cases}$ Combine the forms: $$c_2=G(c_1) \Leftrightarrow \frac{u}{x}=G(xy)\Leftrightarrow u=xG(yx)$$ Now, using the initial condition $u(x,x)=x^2$ gives $$x^2=xG(x*x) \Leftrightarrow G(x^2)=x $$ So, how could I check if it is well-posed before solving it? And if yes!!! How? Any suggestion?","['analysis', 'linear-pde', 'partial-differential-equations', 'partial-derivative', 'problem-solving']"
4606644,Subgroup of free abelian group is free abelian via transfinite induction,"There are two common proofs of ""if $G$ is free abelian then any subgroup $H$ is also free abelian"" when $G$ has infinite rank, one using Zorn's lemma as in Lang's Algebra , one using a well ordering of the basis (in a neat way!) as in Kaplansky's Set Theory and Metric Spaces . I am wondering if the following ""brute force attempt"" can be made to work. First assume $G$ is of finite rank $n$ . Fix a basis of $G$ , and express every element of $H$ as a column vector. Arrange them into an "" $n$ by $H$ "" matrix, denoted $W=(w_{ij})$ . Consider the ideal generated by the entries in the first row. It has to be of form $(a)$ where $a=k_1w_{1j_1}+\cdots k_sw_{1j_s}$ , $k_i$ integers and $w_{1j_i}$ some of the entries in the first row (it could be the zero ideal). Now on the left of $W$ create an empty $n\times n$ matrix $W'$ , and write the vector $w=k_1w_{j_1}+\cdots+k_sw_{j_s}$ in the first column, where $w_{j_i}=(w_{1j_i},\dots,w_{nj_i})^T$ . By subtracting suitable multiples of $w$ from (the infinitely many) columns of $W$ , we can make all entries in the first row of $W$ zero. Repeat this $n$ times, $W$ will become a zero matrix, and the nonzero columns in $W'$ (they increase in the row index of the first nonzero entry) would be a basis for $H$ . I guess this is essentially the ""column operation"" part of Smith normal form algorithm. Now what to do if $G$ is of infinite rank? Given a well-ordered basis $E$ for $G$ , we create an $E$ by $H$ matrix and try to repeat the above procedure transfinitely; at the $\omega$ -th step I want to take the ""limit"" of the matrix $W$ ; it's not clear that this is well defined, or even if it is, whether some column can now have infinitely many nonzero entries, so they don't represent elements in $H$ any more.","['free-abelian-group', 'set-theory', 'group-theory', 'commutative-algebra']"
4606699,Property of a positive operator,"This question was asked in my assignment of operators and I am not able to prove this particular result. Question: Let $H$ be a Hilbert space. Show that if $\,T\in L(H)\,$ is a positive operator then for every $x\in H:\lVert Tx\rVert^2 \leqslant\lVert x\rVert\lVert T^2 x\rVert$ . Now, if $H$ is a Hilbert space, then $T$ is called a positive operator if $T$ is auto-adjoint and for every $x\in H : \langle Tx,x\rangle\geqslant0$ . Then there are results that $\,T\!\cdot\!T\,$ is always auto-adjoint and Cauchy-Schwarz inequality. But, I am not able to understand what result I should use to solve this question. Do you mind helping me?","['operator-theory', 'functional-analysis']"
4606720,"Why is $\big\{X:X\subseteq\{3,2,a\}\text{ and }|X|=4\big\}$ empty?","Write out the following set by listing its elements between braces: $$\big\{X:X\subseteq\{3,2,a\}\text{ and }|X|=4\big\}=\emptyset$$ The above answer is correct, however can someone explain why this is so? If the size of $X$ must be $4$ , why can the only element be none?","['elementary-set-theory', 'discrete-mathematics']"
4606760,Question about a Galois group,"I am working on a problem which seems to be troubling me quite a lot. This is how it goes:
Let $\alpha=\sqrt{\frac{2\sqrt{3}-3}{3}}$ and $\beta=\sqrt{\frac{2\alpha}{\sqrt{3}}}$ . It is easy to show that the minimal polynomial of $\alpha$ and $\beta$ are $3x^4+6x^2-1$ and $27x^8+72x^4-16$ , respectively. One can see that $\mathbb{Q}(\beta,i)$ contains ""the"" splitting field of $3x^4+6x^2-1$ . The part I am having trouble with is to show that this field also contains ""the"" splitting field of $27x^8+72x^4-16$ .
I was wondering if anyone could point me in the right direction.","['field-theory', 'number-theory']"
4606762,Question regarding approximation for $\sin^3(x)$,"I encountered the following approximation for $\sin(x)$ in a physics book: $$ \textrm{for }x\in[0,\pi]\textrm{, }\quad
\sin^3(x)\approx \left[\frac{\cos\left(\frac{\pi}{2}\cos(x)\right)}{\sin(x)}\right]^2
$$ Plotting both of these functions on the specified interval produces the following Mathematica Plot , indicating that the approximation is quite good. I was wondering if there was a way to get an intuition for where this approximation comes from or even a rigorous derivation. Here's what I've attempted so far: Because there's a half in the argument of the outer cosine, I tried using the half angle Identity to simplify the RHS of the approximation and got $$
\frac{\left(\cos\left(\pi\cos\left(x\right)\right)+1\right)}{2\sin^{2}\left(x\right)}$$ Because both functions are equal at $x=\frac{\pi}{2}$ I tried using a Maclaurin series in $u=x-\frac{\pi}{2}$ . However, the coefficients never match because the LHS always has rational coefficients whereas the RHS has coefficients which are rational multiples of powers of $\pi$ . I then tried expanding either side in terms of Legendre polynomials in u (renormalized to be orthonormal with the inner product $(L_m,L_n)=\int_{-\frac{\pi}{2}}
^{\frac{\pi}{2}}L_mL_ndu$ and got the following values: $$n=0: 0.447, 0.424$$ $$n=2: −0.736, −0.745$$ $$n=4: 0.376, 0.427$$ $$n=6: −0.110, −0.122$$ With the first value corresponding to the coefficient for the RHS and the second value corresponding to the coefficient for the LHS. These coefficients don't get closer for larger values of $n$ as far as I could tell, so this method failed as well, indicating that this approximation didn't come from messing around with Legendre polynomials (I'm assuming that other sets of orthogonal polynomials will fail as well). Any insight on this problem would be greatly appreciated!","['trigonometry', 'approximation']"
4606778,Understanding the > symbol,"I'm coming from a computer science background, there was recently an elementary math question I discussed with someone else that seemed to have me a bit stumped on the solution. I have seen this question reposted here : Given that exactly one of the 4 statements is correct, which one is it? Which is also the following question: A multiple-choice test question offered the following four options relating to a certain statement: A) The statement is true if and only if x > 1 B) The statement is true if x > 1 C) The statement is true if and only if x > 2 D) The statement is true if x > 2 Given that exactly one of these options was correct, which one was it? The solution has made me realize that there is something I fundamentally do not understand about inequalities, that hasn't caused me many problems up until now. Namely, this can be illustrated with my limited knowledge of set theory. When I see a statement such as if x > 1 my natural assumption is that the consideration is $$x \in \{\mathbb{R} \; | \; x > 1\}$$ However from the solution it is evident that it is rather the consideration that $$ x = \{\mathbb{R} \; | \; x > 1\}$$ Putting this another way, the consideration is that the condition applies to x > 1 rather than to x which must be greater than 1. I however cannot see how this is evident from the question. This to me indicates that I perhaps do not understand how inequalities work. What is the formal point in the definition for the symbol > that illustrates the problem in my understanding of this question?","['elementary-set-theory', 'inequality', 'functions', 'logic']"
4606833,"Having trouble understanding the solution of $\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+\lambda y=0,$ $y(0)=y(1)=1$","Good day. I was working on this problem from my lecture notes. ""Find eigenvalues and corresponding eigenfunctions
for the BVP $\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+\lambda y=0,$ $y(0)=y(1)=1$ and verify their orthogonality by direct calculation"" There's also a solution provided, which I've followed along with. Though solving the whole exercise is the end goal, I'm just concerned with finding the eigenvalues for the time being. I understand how to set up the auxiliary equation, and how to solve the general system of the ODE for $\lambda < 1$ and $\lambda = 1$ , but I don't understand what the solution does to solve it for $\lambda > 1$ . ""For $\lambda > 1$ , say $\lambda = 1 + \mu^2, \mu = \sqrt{\lambda-1} > 0$ , the general solution is $y(x) = (A\cos(\mu x) + B\sin(\mu x))e^{-x},$ $y(0) = 0 \to A=0,$ then $y(1) = 0 \Rightarrow B = 0,$ or $\sin(\mu) = 0,$ $\mu = n\pi,$ $n = 1, 2, 3...,$ giving the result: $\lambda_{n} = 1 + n^2\pi^2,$ $\phi_n(x) = e^{-x}\sin(n\pi x),$ $n = 1,2,3,...,$ "" (It then checks orthogonality) I don't understand why it substitutes $\mu$ . I thought when dealing with complex roots, you need it to be in form $m = p \pm qi$ , to substitute into $y = e^{px}(C_{1}\cos(qx)+C_{2}\sin(qx))$ . I've thought about it for a bit, but the link isn't clear yet.","['ordinary-differential-equations', 'eigenfunctions']"
4606866,Why does the AM-GM inequality not show $25 \csc^2(\theta) +16 \sin^2(\theta)$ has a minimum of $41$ as the graph indicates?,Let's say we have to find range of $f(\theta) = 25 \csc^2(\theta) +16 \sin^2(\theta)$ If I use $AM \ge GM$ Then $f(\theta) \ge 40$ Which tells minimum value of $f(\theta)$ will be $40$ But I checked it on graphing calculator and it is showing $41$ will be minimum value Then I tried for $f(\theta) = 16 \csc^2(\theta) +25 \sin^2(\theta)$ Now using $AM \ge GM$ I am getting 40 as answer of minimum value of function and also checked on graphing calculator Now my question is why  my first question answer is wrong using $AM \ge GM$ I think it is something related to coefficient but I am not getting it how it is wrong,"['trigonometry', 'a.m.-g.m.-inequality', 'inequality']"
4606872,Proving that a pole is simple,"I’m trying to show that $z=0$ is a simple pole for the function $f(z)=\dfrac{\mathop{\text{Log}}(1+z)}{(e^z-1)^2}$ . I can see that $z=0$ is a pole of the function, but how do I show it? And that the order of this pole is one? I was hoping that I could write the function as $f(z)=\dfrac{g(z)}{(z-0)^1}$ with the function $g(z)$ being analytic at $z=0$ and $g(0)≠0$ and suddenly not being sure this is even a viable approach.",['complex-analysis']
4606876,Proving that $n$-component Brunnian link is nontrivial,"I stumbled upon the attached image. It shows a way to construct an $n$ -component Brunnian link for any $n\geq 3$ . That is, this link is not trivial, but deleting any of its components makes the new link trivial. The latter property is obvious from the picture, however I would like to have a strict proof of nontriviality. I have not managed to come up with such a proof, although I do have some kind of a plan. Let us denote this link by $L=K_1\cup K_2 \cup \dotsb \cup K_{n-1}\cup K_n$ . Also, it might be useful to examine $L'=L\setminus K_n=K_1\cup K_2 \cup \dotsb \cup K_{n-1}$ . I look at the link group (i.e., fundamental group of the link's complement) $\pi_1(L').$ Since $L'$ is equivalent to $n-1$ disjoint circles, I conclude that $\pi_1(L')=F_{n-1}$ . Then I ask what $[K_n]\in \pi_1(L')$ would be in the case if $L$ were trivial. In that case I would be able to move $K_n$ a little bit and get a homotopic loop that is a clean circle, thus $[K_n]\in \pi_1(L')$ is the identity.
Now, for contradiction I would like to calculate explicit form of $[K_n]\in \pi_1(L')$ (or to somehow just prove that it is not the identity). I tried doing it with Wirtinger presentation, but it was messy and, even if I did find explicit form for $[K_n]\in \pi_1(L')$ in terms of group’s generators, determining whether $[K_n]$ is equal to the identity in this finitely generated group is a well-known undecidable problem. Is there a way to improve my approach in order to prove nontriviality? If not, do you see another way to prove that? Thank you.","['knot-invariants', 'group-theory', 'fundamental-groups', 'knot-theory', 'low-dimensional-topology']"
4606899,What is the motivation behind the concept of definition of Gaussian prime numbers?,What is the motivation behind the concept of definition of Gaussian prime numbers? I am interested about the part of the definition when both real and imaginary coefficients are non-zero. Then a complex number is called a Gaussian prime number iff $a^2+b^2$ is an ordinary prime number. Why?,"['number-theory', 'gaussian-integers', 'prime-numbers']"
4606906,What's the relation between the expectation of p-norm squared of a random vector before and after centering?,"Let $a$ be a random vector with $\mathbb{E} a = b$ and $\mathbb{E} \|a\|_p^2 =\sigma^2$ , where $1\leq p \leq \infty$ . Is it true that $\mathbb{E} \|a-b\|_p^2 \leq 2\sigma^2$ ? It is clear to me how to obtain the relation for $p=2$ (in fact, with $\sigma^2$ in place of $2\sigma^2$ ) by expressing the squared norm as a scalar product. However, I have trouble generalizing the property. I tried using the triangle inequality for $\|\cdot\|_p$ , the Jensen's inequality for the expectation of a norm or a squared norm, but didn't arrive at the result. I also suspect that the Hölder's inequality might be useful. I would appreciate any help. edit: actually, generalizing to $p\in[1,2]$ would already suffice for my purpose.","['normed-spaces', 'jensen-inequality', 'expected-value', 'inequality', 'probability-theory']"
4606911,"Convergence of a series, what is wrong with my solution?","I have just attempted the following question Show that if $ (b_n) _{n \in \mathbb N }$ is a bounded sequence in $\mathbb R $ , then the series $\sum_{n=1}^{\infty }b_n2^{-n}$ is convergent. This is my solution. If $(b_n)$ is bounded then $\exists M \in \mathbb R$ such that $\lvert b_n \rvert \le M$ $\forall n \in \mathbb N$ . Then, by taking every term in the following series to be $M$ and the fact $\sum_{n=1}^{\infty} 2^{-n}$ converges to 1, we deduce that $$
\sum_{n=1}^{\infty } b_n2^{-n} \le \sum_{n=1}^{\infty } M2^{-n} = M
$$ Therefore, by the comparison test $\sum_{n=1}^{\infty } b_n2^{-n}$ converges. However, the official solution uses Cauchy sequences and partial sums. Is there a major flaw in my solution?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4606917,How does a divergence-free field $\vec F$ imply $\vec F=\vec\nabla\times\vec G$?,"I am experiencing a slight dilemma here. Starting from the Divergence theorem, I am trying to show how a divergence-free field $\vec F$ implies that $\vec F$ can be written as the curl of another vector, say $\vec G$ . Below I have written some steps which outline my current chain of logic. Note that I'm assuming we don't already know that $\vec F =\vec\nabla\times \vec G$ , since this is what I am trying to prove. Also, I am not using anything like the Helmholtz theorem either. Here is goes: Step $0$ : Start with the divergence theorem in $\mathbb R^3$ : $$\iint_S\vec F\cdot d\vec S=\iiint _V\text{div}(\vec F)\;dV,$$ where $S$ is of course a closed surface. Step $1$ : If $\vec F$ is a $C^1$ vector field on an open region containing the volume V and $\text{div}(\vec F)=0$ everywhere, then $$\iint_S\vec F\cdot d\vec S=0,$$ which says the flux through any closed surface must be zero. Step $2$ : Suppose we break the closed surface $S$ into two orientable surfaces $S_1$ and $S_2$ (note that $S_1$ and $S_2$ share the same boundary curve). Let one surface assume the orientation of the other. E.g, if $S_2$ assumes the orientation of $S_1$ , then $S_2$ starts off with ""negative"" orientation, which we will denote by $-S_2$ . Since $\iint_{-S_2}\vec F\cdot d\vec S =-\iint_{S_2}\vec F\cdot d\vec S$ , hence $$0=\iint_S\vec F\cdot d\vec S=\iint_{S_1} \vec F\cdot d\vec S + \iint_{-S_2} \vec F\cdot d\vec S=\iint_{S_1} \vec F\cdot d\vec S - \iint_{S_2} \vec F\cdot d\vec S.$$ Therefore, $$\iint_{S_1}\vec F \cdot d\vec S=\iint_{S_2}\vec F \cdot d\vec S.$$ This says that the flux of a divergence-free vector field through any two open surfaces is surface independent and only depends on the surface boundary. In other words, for any fixed non-empty boundary, we may deform the surface $S$ . You may view this thread for more details regarding this step: Let $F$ be a vector field in $\mathbb{R}^3$. If $F$ is divergence free, we may deform the surface. Why? Step $3$ : This is where I am stuck. I've seen several articles somehow relate this surface independence property back to Stokes' theorem. In particular, I've seen: If $\text{div}(\vec F)=0$ everywhere, then $$\tag{$\star$}\iint_{\text{any open surface}}\!\!\!\!\vec F\cdot d\vec S=\text{constant}=\oint_{\partial S}\vec G\cdot d\vec r=\iint_S\text{curl}(\vec G)\cdot d\vec S,$$ which implies $\vec F=\vec\nabla\times\vec G$ . Here's the link to the article where that's from (on page 3): https://physics56.files.wordpress.com/2016/01/tutorial-7-scalar-and-vector-potential3.pdf My confusion lies with equation $(\star)$ . I just don't see how one simply relates the LHS to Stokes' theorem. I know this is analogous to the case when $\text{curl}(\vec F)=0$ , which by Stokes' theorem implies path independence and moreover, the existence of a scalar function $\psi$ , such that $\vec F=\vec\nabla \psi$ . We can show that this is indeed the case for curl-free fields by using the fundament theorem of calculus. Again, I'm trying to come to this conclusion using vector calculus but not Helmholtz theorem or calculus on manifolds. Just following my outlined chain of logic starting from the Divergence theorem. I appreciate any input I can get. :)","['divergence-theorem', 'vector-fields', 'multivariable-calculus', 'stokes-theorem', 'vector-analysis']"
4606936,Invertibility of a linear combination of self-adjoint operators,"Assume a bounded linear operator $T:\mathcal{H}\to\mathcal{H},$ where $\mathcal{H}$ is an infinite dimensional separable Hilbert space, is invertible. Let $$A={1\over 2}(T+T^*),\quad B={1\over 2i}(T-T^*)$$ Does there exist   a real constant $r$ such that the operator $A+rB$ is injective ? The operators $A$ and $B$ are self-adjoint and $T=A+iB.$ The assumptions imply that $\ker A\cap \ker B=\{0\}.$ Assume  that $\ker (A-rB)\neq \{0\}$ for any positive value of $r.$ Let $0\neq x_r\in\ker (A-rB).$ Then for $s\neq r$ we get $$\langle Ax_r,x_s\rangle =r\langle Bx_r,x_s\rangle,\quad \langle x_r,Ax_s\rangle =s\langle x_r,Bx_s\rangle$$ Therefore $$\langle Ax_r,x_s\rangle=0,\quad s\neq r$$ The conclusion is valid if one of the operators $A$ or $B$ is nonnegative. Indeed, consider the case $A\ge 0.$ Then $$0=\langle Ax_r,x_s\rangle =\langle A^{1/2}x_r,A^{1/2}x_s\rangle \quad s\neq r$$ The family of nonzero orthogonal vectors $\{A^{1/2}x_r\}_{r>0}$ is uncountable, which leads to a contradiction. The conclusion is valid also for any  normal invertible operator $T.$ In that case the operators $A$ and $B$ commute. We have $$\ker(A^2-r^2B^2)=\ker [(A+rB)(A-rB)]\supset \ker (A-rB)$$ The operator $A^2$ is nonnegative, hence there is $r\neq 0$ such that $\ker(A^2-r^2B^2)=\{0\}.$ Thus $A-rB$ is injective. We cannot expect the invertibility of the operator $A+rB.$ I was able (spoiler) to get an example of an invertible operator $T$ such $A+rB$ is not invertible for any real value $r.$ Let $\mathcal{H}=L^2(0,\pi)$ and $(Tf)(x)=e^{ix}f(x).$ Then $(Af)(x)=\cos x\,f(x) $ and $(Bf)(x)=\sin x\, f(x).$ Thus $[(A+rB)f](x)=(\cos x+r\sin x)\,f(x).$ The function $x\mapsto \cos x+r\sin x$ is not bounded away from $0,$ Therefore the operator $A+rB$ is not invertible.","['linear-algebra', 'functional-analysis']"
4606965,Troubles understanding when certain quantity is bounded :,"I'm trying to study the following function : $$f(x,y) = \begin{cases}\dfrac{x^2y^2}{x^2+y^4} & (x,y)\neq (0,0)\\ 0 & (x,y)=(0,0) \end{cases}$$ I started by showing it's continuous at $(0,0)$ : \begin{align*}
\lim_{(x,y)\to (0,0)} f(x,y) &= \lim_{(x,y)\to (0,0)} \dfrac{x^2y^2}{x^2+y^4}\\
&= \lim_{r\to 0 } r^2 \dfrac{\cos^2 \theta \sin^2 \theta}{\cos^2 \theta + r^2 \sin^4\theta}\\
&= 0
\end{align*} It's $0$ because the $r^2 \sin^4\theta\to 0$ so as the whole expression, and I checked my answer using Wolfram alpha and it is true. Well when it comes to check if the function is $\mathcal{C}^1$ , I must prove that the partial derivatives exist and they're continuous : $$\partial_x f(0,0) = \lim_{x\to 0 } \dfrac{f(x,0)-f(0,0)}{x}=0$$ But : $$\lim_{(x,y)\to (0,0)} \partial_x f(x,y) = \lim_{(x,y)\to (0,0)} \dfrac{2xy^6}{(x^2+y^4)^2} $$ Its limit doesn't exist as Wolfram Alpha says, because : $$\dfrac{2r^5\cos\theta \sin^6\theta}{(\cos^2\theta+r^2\sin^4\theta)^2}$$ Isn't bounded, but according to my philosophy that I used to solve the continuity it is since $r^2\sin^4 \theta\to 0$ and the limit as well. Any help to overcome this confusion ?","['limits', 'multivariable-calculus']"
4606974,Why m + n is 85?,"Suppose $4$ balls are placed at random into $4$ boxes. The probability that exactly two boxes remain empty is $\frac mn$ in its simplest form. I divided it into $2$ cases, where $2$ balls are in two boxes each; $3$ in one box and $1$ in another. For the first case, I think there are ${4 \choose 2}$$\times$${4 \choose 1}$$\times$${3 \choose 1}$ = $72$ different ways. For the second case, I think there are ${4 \choose 3}$$\times$${4 \choose 1}$$\times$${3 \choose 1}$ = $48$ different ways. The total number of ways to place the balls is $4^4$ = $256$ . Thus, I calculated that $\frac mn$ = $\frac {15}{32}$ However, the answer said that m+n is $85$ . Where did I do wrong?","['statistics', 'probability']"
4606979,Prove that the solution exists for all time.,"Suppose that $x^{\prime}=-x+f(x)$ where $f(x)$ is Lipschitz continuous and $0<f(x)<1$ for all $x \in R$ . Prove that the solution exists for all time. I am reading from Teschl ODE From the notation of page 37-38( I am basically using Picard Lindeloff theorem). If I choose any $T,\delta >>0$ then $M=max_{(t,x)\in V} |f(t,x)|\leq \delta+1$ and then $\frac{\delta}{M}=\frac{\delta}{\delta+1}$ (approx) and then $T_0=\min\{T,\frac{\delta}{M}\}\leq 1$ . Hence, I am stuck about how to show that the solution exists for all time. I am confused about the fallacy here as the function $-x+f(x)$ is globally Lipschitz the solution should exist for all $t$ . Please help me or give me a proof.","['ordinary-differential-equations', 'lipschitz-functions', 'real-analysis', 'derivatives', 'dynamical-systems']"
4606981,Prove that $x$ is bounded.,"Suppose that $x: \mathbb R^+ \to \mathbb R^+$ and that $$
\frac{d x}{d t}  \leq x^2 $$ $$\int_0^{\infty} x(s) d s  <\infty $$ Post edit:( from the valuable comments) Prove that $x$ is bounded. (Note: this is not obvious since the integral constraint does not eliminate singularities like $1 / \sqrt{t}$ . As a hint I am thinking $x^2$ as $x(t) x(t)$ and thinking about how to solve $x^{\prime}=a(t) x(t)$ but didn't get any help so far.","['ordinary-differential-equations', 'lipschitz-functions', 'analysis', 'derivatives', 'dynamical-systems']"
4607031,When does there exist a change of variables such that it maps a compact subset of $R^d$ to a d-cube?,"I recently learned about Jacobians and how the change of variables is used to ease out the calculations of an integral in context of double integrals. This led me to wonder when there's a change of variables possible for every integral such that it results in integration over constant limits, ie, it would map the region of integration to a rectangle(for a double integral, boxes for triple integrals, etc.): Edit: After going through the comments, I have now realised how general and deep this question was. As someone into their first multivariable calculus course, I must admit I don't know a lot about topology or advanced calculus. Feel free to make any edits and thoughts in the post that you feel maybe necessary to make the question more accurate.","['integration', 'jacobian', 'multivariable-calculus', 'change-of-variable', 'differential-topology']"
4607052,Question to first order PDE with Characteristics Method,"We have the given problem \begin{align} 
-2yu_{x} + u_{y} + 2yu &= 2y \tag 1 \\
u(1,y) &= 1 + e^{-1-2y^2} \tag 2
\end{align} where $x > 0, y \in \mathbb{R^*}$ . We can modify the formula $(1)$ to get \begin{align}
-2yu_{x} + u_{y} &= 2y - 2yu \\
\implies -2yu_{x} + u_{y} &= 2y(1-u) \\
\implies -u_{x} + \frac{u_{y}}{2y} &= 1-u \\
\implies u_{x} - \frac{u_{y}}{2y} &= u-1
\end{align} so our problem would be \begin{align}
u_{x} - \frac{u_{y}}{2y} &= u-1 \tag 3 \\
u(1,y) &= 1 + e^{-1-2y^2} \tag 4
\end{align} The method of characteristics gives $$\dfrac{dx}{1} = -2y dy = \dfrac{du}{u-1}$$ Edited after @MatthewCassell suggestion Now, we have \begin{align}
\dfrac{dx}{1} =  -2y dy \implies 
 C_{1} &= y^2+x
\end{align} and \begin{align}
\dfrac{dx}{1} = \dfrac{du}{u-1} \implies u &= C_{2} e^x + 1 \\
\implies C_{2} &= \frac{u-1}{e^x}
\end{align} The general solution of the PDE expressed in the form of implicit equation $$C_2=G(C_1)$$ So $$u=e^x G(x+y^2)+1$$ Now, using the initial condition $u(1,y)=1+e^{-1-2y^2}$ gives $$1+e^{-1-2y^2} = e G(1+y^2)+1 \implies G(1+y^2) = \frac{e^{-1-2y^2}}{e}$$ I get something strange (from what I've seen at least). Is this possible?","['analysis', 'linear-pde', 'partial-differential-equations', 'partial-derivative', 'problem-solving']"
4607054,Find the number of possible ways to fill the entries of the grid.,"Let $m,n$ be positive integers. Consider an $m\times m$ grid with a 1 in the upper left corner and an n in the lower right corner that satisfies the following property. Let the entry in the ith row and jth column be $a_{ij}$ . Suppose the grid satisfies that for any entry $a_{ij}$ , any entry directly to the right of it is divisible by $a_{ij}$ and any entry directly below it is divisible by $a_{ij}$ . Find the number of possible ways to fill the entries of the grid. Now it easily follows from the problem's condition that if $a_{ij}$ is the entry at row i and column j, then every entry $a_{kl}$ with $i\leq k, l\leq j$ satisfies that $a_{ij} | a_{kl}.$ Let $D_{m,n}$ denote the desired entry of ways for an $m\times m$ grid satisfying the problem's conditions. Thus every entry in the grid is a divisor of $n$ . For a positive integer e and prime p, let $v_p(e)$ denote the highest exponent b of p such that $p^b $ divides e. Let $n=\prod_{i=1}^k p_i^{e_i}$ be the prime factorization of $n,$ where each $e_i\ge 1$ . Note that we may choose the highest exponents of a prime factor of n that divides each entry in the grid independently, so we have that $D_{m,n} = \prod_{i=1}^k D_{m,p_i^{e_i}}.$ Hence it suffices to consider the case when $n$ is a prime power, say $n=p^k, k\ge 0$ . If $n=1$ there is clearly only one way, so suppose otherwise. For $0\leq j\leq k$ , let $b_{i,j}$ denote the number of entries e in the $i$ th column such that $v_p(e) = j$ . Note that for fixed j, $b_{i,j}$ is a nondecreasing sequence of size m. Fact 1: The number of nondecreasing sequences of size k consisting of elements from a set of integers of size n is precisely ${n+k-1\choose k}.$ Note that we cannot have $(b_{1,j}, b_{2,j},\cdots, b_{m,j}) = (0,0,\cdots, 0)$ or $(k,k,\cdots, k)$ for any $j > 0$ , since the last column will always have $n,$ which is divisible by $p^j$ for any $0<j\leq k$ and the first column will always have 1, which is not divisible by $p^j$ . The case where $k=1$ is straightforward; we just have by fact 1 and the above observation that the answer is ${2m\choose m}-2$ (indeed once we have the sequence $(b_{1,1},\cdots, b_{m,1})$ we must place p's in the last $b_{i,1}$ spots of the ith column and 1's elsewhere, so each way to fill the grid's entries uniquely corresponds to a sequence of $b_{i,1}$ 's). So assume $k>1$ . It may help to exploit symmetry somehow. If $m$ is odd for instance, then it may be useful to consider $v_p(c)$ , where c is the entry in the center of the grid. Now the entries in the first row and first column are in increasing order of $v_p(e)$ . It's easy to count the number of possibilities for the first row, but the issue is that the entries in the second row depend on those in the first. Question: how can I complete the argument above to deal with the case where $k>1$ ?","['contest-math', 'divisibility', 'elementary-number-theory', 'combinatorics', 'discrete-mathematics']"
4607057,Example vector field with index $-2$,"I am looking at theory about vector fields in the plane and about their indices, I have found classic examples but I was wondering if anyone knows a plane vector field that behaves as follows: Considering that the lines in the image start the plane in equal parts, I have managed to obtain that the index is $-2$ , but I do not know which face this vector field has. If anyone has a reference to where I can find examples of vector fields that have indices $+2$ , $+3$ or $-3$ , $-4$ , I would ask you to mention a reference. Thanks in advance.","['vector-fields', 'differential-topology', 'ordinary-differential-equations', 'dynamical-systems']"
4607061,Evaluation of Trigonometric Limit having 5 terms,"Evaluation of $\displaystyle \lim_{h\rightarrow 0}\bigg[\frac{\sin(60^\circ+4h)-4\sin(60^\circ+3h)+6\sin(60^\circ+2h)-4\sin(60^\circ+h)+\sin(60^\circ)}{h^4}\bigg]$ Here above limit is in $(0/0)$ form So we have using D, L Hopital rule $\displaystyle \lim_{h\rightarrow 0}\bigg[\frac{4\cos(60^\circ+4h)-12\cos(60^\circ+3h)+12\cos(60^\circ+2h)-4\cos(60^\circ+h)+0}{4h^3}\bigg]$ Again above limit is in $(0/0)$ form So using D, L Hopital rule $\displaystyle \lim_{h\rightarrow 0}\frac{-16\sin(60^\circ+4h)+36\sin(60^\circ+3h)-24\sin(60^\circ+2h)+4\sin(60^\circ+h)}{12h^2}$ Above limit is in $(0/0)$ form So agian using D, L Rule $\displaystyle \lim_{h\rightarrow 0}\frac{-64\cos(60^\circ+4h)+108\cos(60^\circ+3h)-48\cos(60^\circ+2h)+4\cos(60+h)}{24h}$ Again using D, L rule , We get $\displaystyle \lim_{h\rightarrow 0}\frac{256\sin(60^\circ+4h)-324\sin(60^\circ+3h)+96\sin(60^\circ+2h)-4\sin(60+h)}{24}$ $\displaystyle \lim_{h\rightarrow 0}\frac{24\sin(60^\circ)}{24}=\frac{\sqrt{3}}{2}$ Above is very lengthy way Please explain me some short way Thanks",['limits']
4607095,Norm of an integral functional,"Let $k\in C[0,1]$ and let $T$ be the functional on $C[0,1]$ (equipped with sup norm) given by $f\mapsto \int_0^1 kf$ . Show that $\Vert T\Vert = \Vert k\Vert_1$ . My attempt: The inequality $\Vert T\Vert \le \Vert k\Vert_1$ is obvious. Indeed, $\Vert Tf\Vert \le \Vert f\Vert_{\infty} \int_0^1 |k|=\Vert k\Vert_1 \Vert f\Vert_{\infty}$ . On the other hand, for $f=k$ , we have $$Tf=\int_0^1 k^2=\Vert k\Vert_2^2 \ge \Vert k\Vert_1^2\ge \Vert k \Vert_1$$ by Hölder's inequality. It follows that $\Vert T\Vert = \Vert k\Vert_1$ . Is my solution correct? PS: This question was actually answered here but there $f=sgn(k)$ which is not a continuous function, so I think the solution there is incorrect.","['operator-theory', 'solution-verification', 'functional-analysis']"
4607116,Positive integral,"Let $\alpha \geq0.$ Let $f:\mathbb{R^q} \to \mathbb{R}$ be a bounded measurable function such that $$\int_{\mathbb{R}^q\times\mathbb{R}^q}\frac{|f(x)f(y)|}{|x-y|^{\alpha}}dxdy<\infty.$$ Prove that $$\int_{\mathbb{R}^q\times\mathbb{R}^q}\frac{f(x)f(y)}{|x-y|^{\alpha}}dxdy\geq 0.$$ I tried to use Fubini's theorem and a change of variable $u=x-y,$ but this didn't work. Any ideas? Does it help to find $c,\beta>0$ such that $|x|^{-\alpha}=|\,\unicode{x2219}\,|^{-\beta}*|\,\unicode{x2219}\,|^{-\beta}(x)$ ? How?","['integration', 'measure-theory', 'real-analysis']"
4607123,Closed curves of the form $F(x)+G(y)=0$,"For certain problem in mechanics, it is useful to assume that a simple (smooth probably, but not strictly necessary) closed curve can be expressed in implicit form as \begin{align}
F\left (x\right)+G\left (y\right)=0,
\end{align} being $F(x)$ and $G(y)$ respectively $C^1$ functions of the Cartesian coordinates $x$ and $y$ . For instance, in the case of a circle this is true with $F(x)=x^2,\:G(y)=y^2-c^2$ , and similarly for an ellipse. The question is Which (simple closed) curves can be expressed in such a Cartesian separable way ? It is likely that not any closed curve can be put in such form, but probably the family is much bigger than simply ellipses. I'm looking for some criteria that a curve may or may not satisfy, like convexity.","['plane-curves', 'curves', 'analytic-geometry', 'geometry']"
4607148,Proof of relationship between Dirac Delta and Co-Area formula,"In the Wikipedia page for the Dirac Delta function this formula appears under ""Properties in $n$ dimension"". $$
\int f(x) \delta(g(x)) dx = \int_{g^{-1}(0)} \frac{f(x)}{|\nabla g(x)|} d\sigma(x)
$$ It is said that this is a consequence of the Co-Area formula but no proof is given and the only reference (""Hörmander (1983), The analysis of linear partial differential operators I"") doesn't seem to have this formula in it. I have a few questions, in order of importance. What is a proof of this statement? What other references are there about this statement and its generalizations to a function $g:\mathbb{R}^n\to\mathbb{R}^m$ with $n > m > 1$ ? In the above the author uses $\delta (g(x)) dx$ as if $\delta$ was a function, where in fact it is a Schwartz distribution or a measure. What did they mean? Especially because now it is concatenated with another function. Definition of Dirac Distribution It's a linear functional that maps test functions $\varphi$ to $$
\delta_x[\varphi] = \int \varphi(y) \delta_x^{\text{measure}}(dy) = \varphi(y)
$$ where $\delta_x^{\text{measure}}$ is the Dirac Measure which for any measurable set $A$ is defined as $$
\delta_x^{\text{measure}}(A) = \begin{cases}
    1 & x\in A \\
    0 & x\notin A
\end{cases}
$$ Co-Area Formula for Lipschitz Functions If $g:\mathbb{R}^n\to\mathbb{R}^m$ with $n > m$ then $$
\int_{\mathbb{R}^n} f(x) dx = \int_{\mathbb{R}^m} \left[\int_{g^{-1}(y)} f(x) |J_g(x) J_g(x)^\top|^{-1/2} \mathcal{H}^{n-m}(dx) \right]dy 
$$ where $J_g(x)$ is the Jacobian matrix of $g$ .","['measure-theory', 'functional-analysis', 'geometric-measure-theory', 'distribution-theory']"
4607249,Is there a natural bijective proof of the identity $(2^i)^j = (2^j)^i$?,"As is well-known, iterating exponentials is a commutative operation. Assuming that $i$ and $j$ are positive integers, one way to view the integer $2^i$ is as the cardinality of the power set $2^{\{ 1 , \dots , i \}}$ (and likewise for $2^j$ ). The equality $(2^i)^j = (2^j)^i$ implies that the sets $(2^{\{ 1 , \dots , i \}})^{\times j}$ (this means take the Cartesian product with itself $j$ times) and $(2^{\{ 1 , \dots , j \} })^{\times i}$ are in bijection with each other. My question is the following: is there a natural choice of bijection between these two sets that implies the equality $(2^i)^j = (2^j)^i$ ? Just as an example: we know that $(2^1)^2 = (2^2)^1$ . Enumerating $(2^{\{ 1 \} })^2$ and $2^{\{1,2 \}}$ , the most natural correspondence between these two sets seems to be the following: $$(\varnothing , \varnothing) \leftrightarrow \varnothing,$$ $$(\{ 1 \} , \varnothing) \leftrightarrow \{1 \},$$ $$( \varnothing , \{ 1 \} ) \leftrightarrow \{ 2 \},$$ $$(\{ 1 \} , \{ 1 \}) \leftrightarrow \{ 1 , 2 \}.$$ It seems then that one way to make this task a little easier is to take advantage of the Boolean poset structure, since the product of ranked posets remains a ranked poset. This means we really reduce the problem to finding a bijection between elements of the same rank in the corresponding posets... in any case, I assume that this bijection is considered standard, I just don't know what it should be. Edit : In view of the answers below, it seems that the correct way to do this naturally is to identify the power set $2^S$ with $\{ 0 , 1 \}^{S} := \operatorname{Hom} (S , \{ 0 ,1 \})$ (in the category of sets), then use the naturality of the bijection $(A^B)^C = A^{B \times C} = (A^C)^B$ . Following this bijection along explicitly, this corresponds to ``repacking"" your brackets. What I mean by this is the following: let $(\{1 \} , \{ 2 \} , \{ 1,2 \} ) \subset (2^{\{ 1 , 2 \}})^3$ . We know that this should correspond to a tuple $(2^{1,2,3})^2$ . This is done by the following string of identifications: $$(\{ 1 \} , \{2 \} , \{ 1,2 \} ) \leftrightarrow ( \{ 1,0 \} , \{ 0 ,1 \} , \{ 1,1 \} )$$ $$\leftrightarrow (1,0,0,1,1,1 )$$ $$\leftrightarrow ( \{ 1 , 0, 0 \} , \{ 1 ,1,1 \} ) \leftrightarrow (\{1 \} , \{ 1,2,3 \} ) \in (2^{\{1,2,3\}})^2.$$ So, to recover the anticipated correspondence from my original question we see: $$(\varnothing , \varnothing )  \leftrightarrow ( \{ 0 \} , \{ 0 \} ) \leftrightarrow (\{ 0 , 0 \} ) \leftrightarrow \varnothing,$$ $$(\{1 \} , \varnothing ) \leftrightarrow ( \{ 1 \} , \{ 0 \} ) \leftrightarrow (\{ 1 ,0 \} ) \leftrightarrow \{ 1 \},$$ $$( \varnothing , \{ 1 \} ) \leftrightarrow ( \{ 0 \} , \{ 1 \} ) \leftrightarrow (\{ 0 ,1 \} ) \leftrightarrow \{ 2 \},$$ $$(\{ 1 \} , \{ 1 \} ) \leftrightarrow (\{ 1 \} , \{ 1 \} ) \leftrightarrow (\{ 1 , 1 \} ) \leftrightarrow \{ 1 ,2 \}.$$","['combinatorics', 'combinatorial-proofs']"
4607250,Prove that a convex curve C has tangent lines everywhere except at countably many points.,"Today, I was faced with the following question: Suppose E was a convex region in the plane bounded by a curve C. Show that C has a tangent line except at a countable number of points. Since the next question asks me to show that every convex function $f:I\subset \mathbb{R}\rightarrow \mathbb{R}$ has a derivative everywhere except at a countable number of points, I am trying to avoid proving the first statement using derivatives. However, I have no idea even how to attempt this question because I don't know how I can characterize a curve that bounds a convex region. I have done some reading and it led me to a field I know nothing about so I am pretty much stuck here. The question gives the hint of considering circles, triangles and so on but I don't know where that is supposed to lead me because I can easily think of convex regions which are not polygons (or convex regions with 2 points without a tangent line, for example). Even when I tried to use derivatives to solve the problem I had no idea how to proceed, so I would greatly appreciate some help. EDIT: After noticing that there's probably no way around using derivatives, I would like to point out that answers using derivatives are welcome too.","['functions', 'convex-analysis', 'real-analysis']"
4607267,Expected number of random diagonals before intersection,"Background Motivation: There are various questions asked about randomly drawn chords and their number of intersections in a circle; for example, MSE 73033 and MO 284124 . I am interested here as to a discretized version where the circle is replaced by an $n$ -gon and, if all goes well, then one can consider what happens as $n \rightarrow \infty$ . (I originally asked this question on twitter, and, although there are some proposed small case computations, I cannot vouch for their correctness. See the thread here .) Question: Consider an $n$ -gon ( $n \geq 4$ ) and a list of all non-adjacent vertex pairs. Pick an unchosen pair from the list at random and connect the vertices. If you continue this process without replacement, then what is the expected number of diagonals drawn before two intersect in the interior of the $n$ -gon? (I am, in particular, looking for a formula that is a function of $n$ .) Note 1: If there is an alternative formulation of the problem with a different method of choosing the diagonals ""at random"" that yields a different result, then I would welcome such answers in that direction, too. Note 2: If this result is already known, then a pointer to its answer would be appreciated! I did not locate an answer in my exploration of MSE, but it seems a natural enough question for me to have missed it here (or elsewhere).","['geometry', 'reference-request', 'recreational-mathematics', 'problem-solving', 'probability']"
4607351,Intuition for Local vs. Global notions of Metric Entropy in Statistics,"I am looking for intuition regarding the following statement on page 4 of this paper by Gassiat and Van Handel: However, in finite dimensional settings, global entropy bounds are known to yield sub-optimal results, and here local entropy bounds are essential to obtain optimal convergence rates of estimators. The paper itself and the cited references all show this claim generally by arguing that using global entropy bounds yield suboptimal rates, and then showing that the localized approach achieves the optimal/better rates. So from that perspective, the statement is clear, and I guess it is enough justification for the claim. What I am looking for though is some intuition on why this happens to be the case. What is it (from an intuitive perspective) about localized analyses that makes them sharper? How should one think about localized notions of dimension as opposed to global ones? I'm also interested in how other branches of mathematics (analysis, geometry etc) approach this question. Some background: For a subset $T$ of a metric space $(X,d)$ , the covering number $N(T,\epsilon)$ is the smallest possible number of $\epsilon$ -balls that can cover $T$ . Formally, $$
N(T,\epsilon) := \inf \{ n:  \exists \{x_i\}_{i=1}^n \subset X: T \subset \bigcup_{i=1}^n B_{x_i}(\epsilon) 
 \}
$$ where $B_{x_i}(\epsilon)$ is the ball (in the metric $d$ ) that is centered at $x_i$ and has radius $\epsilon$ . The metric entropy is $\log N(T, \epsilon)$ . In a statistical setting, covering numbers are useful for establishing uniform laws. For example, suppose $X, X_1,\dots, X_n$ are i.i.d. random vectors and $F$ is a class of functions, results of the following flavor are common: $$
\mathbb{P} \left( \sup_{f \in F} \left | \frac{1}{n} \sum_{i=1}^n f(X_i) - \mathbb{E} [f(X)] \right | > \epsilon \right) \le g( N(F, \epsilon'), ...),
$$ where $g$ is some function of the covering number and possibly other parameters of the problem. The result basically says that if the function class $F$ is not too large/complex (as measured by the covering number), then with high probability (over the sample), the worst case fluctuation of an empirical average from it's true mean is not too large. Other related notions here are the Rademacher complexity, and VC dimension, or if $F$ is finite, then we can use the cardinality of $F$ . Localization: A localized covering number is given by $$
N(T \cap B_{t_0} (\delta), \epsilon),
$$ where $t_0 \in T$ is a fixed point. Generally, $t_0$ is taken to be the 'true' parameter/function that generated the data, say $f^*$ , and so the RHS of the above bound becomes something like: $$
g'( N(F \cap B_{f^*} (\delta), \epsilon), ...).
$$ for some different function $g'$ . This generally yields better (smaller) upper bounds than in the non-localized analyses. This style of analysis has become popular in the statistics literature over the last 15-20 years, for example this paper uses a localized version of the Rademacher complexity .","['statistics', 'geometry', 'analysis', 'machine-learning', 'probability']"
4607355,"For integer $x \ge 1$, does it follow that $\left(\frac{x^2+2x+1}{x^2+2x}\right)^{x+1} > \frac{x+2}{x+1}$","Would I be correct that the answer here is yes? Here is my thinking: For $x \ge 1$ , $\dfrac{x+2}{x+1}$ is strictly decreasing. For $x \ge 1$ , $\left(\dfrac{x^2 + 2x + 1}{x^2 + 2x}\right)^{x+1}$ is strictly increasing $\dfrac{16}{9} > \dfrac{3}{2}$ To show that $\left(\dfrac{x^2 + 2x + 1}{x^2 + 2x}\right)^{x+1}$ is strictly increasing I take the derivative which is positive for $x > 0$","['inequality', 'derivatives']"
4607361,"How to formalize ""this looks like a sine wave?""","I was plotting random functions on Desmos and then I noticed an interesting phenomenon with the function $x^2 \sin(1/x).$ When I zoomed in at the number $1/k$ by a factor of $k^2$ , where $-k$ is a large number, I noticed that the graph looked like a sine wave with amplitude $1$ and period $2\pi.$ Is there any way to formalize the idea of ""looks like a sine wave?"" If so, does it really look like the sine wave that I described? I thought of taking a limit as $-k \rightarrow \infty,$ but phase differences wreck this plan. I don't see how I can normalize these phase differences.","['algebra-precalculus', 'graphing-functions', 'trigonometry']"
4607417,Valid Elementary Proof of the Bertrand-Chebyshev Theorem/Bertrand's Postulate? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question $\textbf{Theorem}$ (Bertrand-Chebyshev theorem/Bertrand's postulate): For all integers $n\geq 2$ , there exists an odd prime number $p\geq 3$ satisfying $n<p<2n$ . $\textit{Proof }$ : For $n=2$ , we can simply take $p=3$ which is of course an odd prime number, so assume henceforth that $n\geq 3$ . Now make the strong inductive hypothesis that the Bertrand-Chebyshev theorem is true for all integers $2\leq m<n$ . In particular, since $n\geq 3\Rightarrow 2\leq n-1<n$ , we see that there exists an odd prime number $p\geq 3$ satisfying $n-1<p<2(n-1)\Rightarrow n\leq p<2n$ . We now divide into $2$ cases: $\textbf{Case }1$ : If $n\neq p$ , then we can improve the above inequality to say $n<p<2n$ , which would mean that we're done. $\textbf{Case }2$ : If $n=p$ , then assume for sake of contradiction that all of the integers $p'$ satisfying $p<p'<2p$ are composite numbers. Now construct a well-defined function $f:\{p+1,p+2,...,2p-2,2p-1\}\to\{2,3,...,p-2,p-1\}$ which maps each of those $p-1$ composite numbers to their respective $\textit{smallest prime factor}$ . To elaborate a little more on why $f$ is well-defined, the reason that $f(p')<p$ for all integers $p'$ satisfying $p<p'<2p$ is because first, $f(p')\leq p'$ because it is a $\textit{factor}$ of $p'$ , second $f(p')\notin\{p+1,p+2,...,p'\}$ because it is a $\textit{prime}$ factor of $p'$ (while all those integers were by assumption composite numbers), and finally $f(p')\not = p$ because $p<p'<2p\Rightarrow 1<p'/p<2$ , so $p'/p$ cannot possibly be an integer, hence $p$ cannot possibly be a factor of $p'$ , let alone be the smallest prime factor $f(p')$ of $p'$ . Continuing in Case $2$ above, we remark that the codomain $\{2,3,...,p-2,p-1\}$ of $f$ has only $p-2$ integers in it while $f$ 's domain of composite numbers $\{p+1,p+2,...,2p-2,2p-1\}$ has $p-1>p-2$ elements in it. Therefore, by the pigeonhole principle, there must be two composite numbers $a,b$ satisfying $p<a<b<2p$ such that $f(a)=f(b)$ . Let us denote this prime number as $q:=f(a)=f(b)$ . From the discussion in the preceding paragraph, we see that $2\leq q<p$ , so by the original strong inductive hypothesis that we made, we see that there exists an odd prime number $r\geq 3$ satisfying $q<r<2q$ . Now within this Case $2$ , we have to break into $3$ more subcases and show that in each of the $3$ subcases, a contradiction arises. $\textbf{Subcase }1$ : If $r>p$ , then since $r<2q<2p$ , we see that we would have $p<r<2p$ , which would directly give a contradiction to the original assumption that all the integers $p'$ satisfying $p<p'<2p$ are composite. $\textbf{Subcase }2$ : If $r=p$ , then we would have $q<p<2q\Rightarrow p<2q<2p\Rightarrow 1/p<1/q<2/p$ . Recall that we also had $p<a<2p$ and $p<b<2p$ , so multiplying each of these inequalities with the one above, we have: $$1<a/q<4$$ $$1<b/q<4$$ But since by construction $q$ was a factor of $a$ and was also a factor of $b$ , we see that $a/q$ and $b/q$ must both be integers, so in fact we can improve the above inequalities to say: $$2\leq a/q\leq 3$$ $$2\leq b/q\leq 3$$ This means that $a\in\{2q,3q\}$ and similarly $b\in\{2q,3q\}$ . But recalling that $a<b$ and $q\geq 2>0\Rightarrow 3q>2q$ , we in fact deduce that $a=2q$ and $b=3q$ . Since $q$ was also the $\textit{smallest prime}$ factor of $a$ , and $2$ is clearly seen to be a prime factor of $a$ , we conclude that we must have $q=2$ , hence $a=4$ and $b=6$ . But recalling the inequalities $p<a$ and $b<2p$ , we see this would imply $p<4$ and $p>3$ , but there are no integers strictly between $3$ and $4$ . $\textbf{Subcase }3$ : If $r<p$ , then we would just apply the strong inductive hypothesis yet again to locate an odd prime number $s\geq 3$ satisfying $r<s<2r$ . If $s\geq p$ , then we could reason as in the above $2$ subcases, and if $s<p$ , then we would just apply the strong inductive hypothesis $\textit{yet again}$ , doing it as many times as necessary until we obtain an odd prime number which is greater than or equal to $p$ , and hence handled by the above $2$ subcases (note that this algorithm would be guaranteed to terminate because otherwise we would have a strictly monotone increasing sequence of odd prime numbers that is bounded from above by $p$ ).","['prime-factorization', 'number-theory', 'solution-verification', 'prime-gaps', 'prime-numbers']"
4607434,"Axler ""Linear Algebra Done Right"" Exercise 6.B.13","This exercise appears in Section 6.B ""Orthonormal Bases"" in Linear Algebra Done Right by Sheldon Axler.  Inner product spaces, norms, orthogonality, and orthonormal bases have been introduced. The unofficial solution manual I am using presents a somewhat involved proof of this exercise that involves induction and the Gram-Schmidt procedure, which I will not reproduce here.  However, I arrived at an alternative proof that is much simpler in my opinion.  I would like to check if my proof is correct. $(V, \langle \cdot, \cdot \rangle)$ is an inner product space over the field $\mathbb{F}$ , which stands for either $\mathbb{R}$ or $\mathbb{C}$ .  (Axler, pp. 4, 167) Suppose $v_1, \dotsc, v_m$ is a linearly independent list in $V$ .  Show that there exists $w \in V$ such that $\langle w, v_j \rangle > 0$ for all $j \in \{1, \dotsc, m\}$ .  (Axler, p. 191) Proof.  Let $U = \operatorname{span}(v_1, \dotsc, v_m)$ .  Consider the linear map $\phi : U \to \mathbb{F}^m$ defined by $$
\phi(u) = (\langle u, v_1 \rangle, \dotsc, \langle u, v_m \rangle).
$$ We show that $\phi$ is injective.  Suppose that $u \in U$ and $\phi(u) = 0$ .  Then, $$
\langle u, v_1 \rangle = \dotsb = \langle u, v_m \rangle = 0.
$$ Since $u \in \operatorname{span}(v_1, \dotsc, v_m)$ ,
we have $\langle u, u \rangle = 0$ , so $u = 0$ .  Hence, $\phi$ is injective.  But $\dim U = \dim \mathbb{F}^m = m$ , so $\phi$ is surjective.  Choose $w \in U$ such that $$
\phi(w) = (1, \dotsc, 1).
$$ Therefore, $\langle w, v_j \rangle = 1 > 0$ for all $j \in \{1, \dotsc, m\}$ .","['inner-products', 'vector-spaces', 'solution-verification', 'linear-algebra', 'linear-transformations']"
4607468,Regarding common zeros of a pair of solutions to an ODE,"$\mathbf{Question}:$ Let $y_1$ and $y_2$ be the solutions to $y''x^2+y'+\sin(x)y=0$ which satisfy the boundary conditions $y_1(0)=0$ and $y_1'(1)=1$ and $y_2(0)=1$ and $y_2'(1)=0$ respectively. Then, $y_1$ and $y_2$ do not have common zeros $y_1$ and $y_2$ do have common zeros either $y_1$ or $y_2$ has a zero of order $2$ both $y_1$ and $y_2$ have zeros of order $2$ $\mathbf{Attempt}:$ Each of the particular solutions can be written in terms of linear combinations that abide by the specified conditions.
Let $A\phi_1(x)+B\phi_2(x)=y_1(x)$ and $C\phi_1(x)+D\phi_2(x)=y_2(x)$ be the solutions, with $\phi_1$ and $\phi_2$ being the fundamental ones. Supposing that they indeed have at least one common zero, say $x_0$ , then: $A\phi_1(x_0)+B\phi_2(x_0)=0$ and $C\phi_1(x_0)+D\phi_2(x_0)=0$ . Now, we know from the Sturm Separation Theorem that the roots of the fundamental solutions alternate, hence, they cannot vanish together; implying that $\begin{vmatrix} A & B\\ C & D \end{vmatrix}=0$ . This implies that $y_1(x)$ and $y_2(x)$ are linearly dependent, thus violating the boundary condition $y_1(0)=0 \neq y_2(0)$ . Is this the correct approach?
Kindly Verify.","['ordinary-differential-equations', 'sturm-liouville', 'solution-verification', 'fundamental-solution', 'boundary-value-problem']"
4607493,Intuition behind polar coordinate to find limit in multivariate calculus,"Find limit using the polar coordinate for the function at $(0,0)$ $$ 
f(x,y) = \frac{x+y}{\sqrt{x^2+y^2}} 
$$ I started using $x = r\cos(\theta),\, y = r\sin(\theta)$ Then $(x,y) \to (0,0) \implies r \to 0$ Then we get as the following $$ 
f\bigl(r\cos(\theta),\, r\sin(\theta)\bigr) 
= \cos(\theta) + \sin(\theta) 
$$ Now I have no idea how to proceed from here. Some of the ideas from youtube videos I had was: $\theta$ is a free variable and the limit is more like spiraling into $(0,0)$ . Can someone explain to me why is this happening? What is the idea behind $r \to 0$ ?","['multivariable-calculus', 'calculus', 'polar-coordinates', 'intuition', 'limits']"
4607496,Limit of limits converges,"Let $(a_k^\ell)_{k,\ell\in\mathbb N}$ be a double sequence that is Cauchy in the lower coordinate, i.e. $$\sup_{\ell\in\mathbb N}|a_m^\ell-a_n^\ell|<\varepsilon$$ for all $n,m\geq N(\varepsilon)$ . I know that that $a_k:=\lim\limits_{\ell\rightarrow\infty} a_k^\ell$ exists for all $k$ . Let's further assume that $a^\ell:=\lim\limits_{k\rightarrow\infty} a_k^\ell$ exists for all $\ell$ (i.e. we are in $c$ -space). I want to show that $(a^\ell)_{\ell\in\mathbb N}$ converges. My idea is to show it is Cauchy via an $\frac{\varepsilon}{3}$ -argument as follows: $$|a^n-a^m|\leq|a^n-a_k^n|+|a_k^n-a_k^m|+|a_k^m-a^m|<\varepsilon$$ The problem is that the minimal $N(\varepsilon)$ is not well-defined in this situation, because the $k$ is dependent on $n$ and $m$ , and $n$ and $m$ are dependent on $k$ . Can I salvage this or do I have to prove that $(a_k)_{k\in\mathbb N}$ is convergent first?","['limits', 'functional-analysis', 'real-analysis']"
4607519,Calculus Problem: polar-coordinate integration of a slice/wedge of a right circular cylinder,"This image states the problem: As shown in the diagram, a solid right circular cylinder of radius 12 cm is sliced by a plane that passes through the center of the base circle.
Find the volume of the wedge-shaped piece that is created, given that its height is 16 cm (and its base has a radius of 12). 1 I realized the easiest way to do this is to integrate with triangles from each side but I was curious to see if it would be possible to integrate this using polar coordinates. I attempted to do this by integrating using the half-ellipses that are formed from the plane as cross sections and rotating that plane around the center of the base of the cylinder to create more ellipses. However, when I tried this I got $248$ instead of $\textbf{1536}$ (the correct answer). I'm confident in how I computed the integral--I think the mistake I made was in setting up the integration in the first place. Here is my working: Apologies if it's hard to follow Could anyone help me set it up properly? Can you spot what I did wrong? Thanks!","['integration', 'calculus', 'polar-coordinates', 'differential-geometry']"
4607525,How to linearize a base-10 exponential function,"I have a base-10 exponential function with the equation: $$y = 0.000346 \cdot 10^{0.00676x} + 0.148$$ I am trying to linearize this equation but unable to do so. I tried taking $\log(y)$ in the $y$ -axis and $\log(x)$ in the $x$ -axis but I am never getting a straight line. In fact, taking $\log(y)$ gives me negative $y$ -values. Any help would be appreciated. I have attached an imgur link to my graph below since I am unable to embed it here. Thank you so much.","['linearization', 'logarithms', 'graphing-functions', 'functions', 'exponential-function']"
4607531,"Sequence of continuous fractions 1 , 1/2 , (1/2)/(3/4) ... [duplicate]","This question already has answers here : Tall fraction puzzle (3 answers) Closed 1 year ago . I found this interesting fraction sequence: $$1,\frac{1}{2},\frac{\frac{1}{2}}{\frac{3}{4}}, \frac{\frac{\frac{1}{2}}{\frac{3}{4}}}{\frac{\frac{5}{6}}{\frac{7}{8}}} ... $$ This sequence surprisingly converges to $\frac{\sqrt{2}}{2}$ Can someone provide a proof or an idea for this ? Here's my work. I took the 5th term of the sequence and reciprocated everything to get a 1 2 3 4 5 6 pattern in the manner shown by the arrows. It repeats itself at the dotted edge with the dotted edge being common to the two recurring units. Here's the INCORRECT image attached. edit : I am very sorry for switching 11 and 12. This was a silly error on my part. This then changes the pattern and it becomes more confusing now. here's the correct pattern for the first five terms: I have also written the rules I have identified. maybe someone can help me spot a pattern just like @C Squared did. And I seem to like the pattern that @Stinking Bishop has spotted. The binary one. Maybe those lines work. But we want a general (probably recursive) formula. Note- I concluded that it converges to $\frac{\sqrt{2}}{2}$ by anaysing the first 7 terms. p.s. - And i did it by hand because i don't know programming :(","['fractions', 'convergence-divergence', 'sequences-and-series']"
4607557,Degree of an holomorphic map and ramification points,how to determine deg f $f(z)=\frac {z^{3}}{1-z^{2}} $ and what are its ramification points The ramification point i found $p \in \mathbb{C}^{\infty}$ is a point with $multp(F) \geq 2$ . I found for example the following point: $p=0$ because its a zero of f and so $multp(F)=ordp(F)=3 \geq 2$ The poles $+1$ and $−1$ are no ramification points sich their order are just $−1$ . Also $\infty$ is no ramification points since $f(\frac{1}{z}) = \frac{\frac{1}{z^3}}{1-\frac{1}{z^2}} = \frac{\frac{z^2}{z^2-1}}{z^3} =  \frac{1}{z(z^2-1)}$ has a pole of order just 1 in zero (ord1(F)=−1).,"['complex-analysis', 'riemann-surfaces', 'abstract-algebra', 'algebraic-curves']"
4607567,"For which numbers $a$ does the sequence of iterated base changing $(a, a_a, a_{a_a}, a_{a_{a_a}}, ...)$ converge?","Defining "" $a\text{ in base }b$ "" as $$a_b=\sum^{\infty}_{k=0}\frac{a_{[k]}}{b^k}$$ with $a_{[k]}$ being the digits of $a\in (1,10)$ and $b\gt 1$ . For example, if $a=3.1416$ and $b=4.6$ , then $$a_b=\frac{3}{4.6^0}+\frac{1}{4.6^1}+\frac{4}{4.6^2}+\frac{1}{4.6^3}+\frac{6}{4.6^4}\approx 3.43010138$$ Now we define a sequence, placing brackets to indicate order of operations: $$S(a)=(a, a_a, a_{(a_a)}, a_{(a_{(a_a)})}, ...)$$ The most general (and seemingly quite difficult) question that comes to mind is: Given an $a\in (1,10)$ , what is the long term behavior of $S(a)$ ? A (maybe) simpler question, that I also pondered is: If $a_{[k]}$ is finite, does that imply, that $S(a)$ doesn't diverge? Behavior of $S(a)$ for numbers $a$ between $1$ and $2$ . For each $a\in\left\{1.000, 1.001, 1.002, ..., 1.998, 1.999 \right\}$ the entries of $S(a)$ in positions $10^6$ and $10^6-1$ are computed and drawn as circles in the picture, they are then connected with a line. More info If $a=2$ , it is easy to see, that $S(a)$ stays fixed at $2$ . Indeed for any integer $a\in(1,10)$ , it holds, that $S(a)$ stays fixed at $a$ . Other curious examples are For $a = 1.1$ , the sequence converges to $\frac{1+\sqrt{5}}{2}=\phi$ For $a = 1.04$ , the sequence converges to $2$ A few other things can happen, for particular values of $a$ . For example if $a=1.05$ , then $S(a)$ doesn't converge to a single value, but to a two-term limit cycle. (and interestingly, the sum of the two terms is $5$ ). It has been found in [1] , that only one of five things can happen: $a$ is a fixed point (as with $a=2$ ) $S(a)$ converges to a single value (as with $a=1.1$ ) $S(a)$ falls into a two-term cycle (as with $a=1.\overline{1}$ ) $S(a)$ converges to a two-term limit cycle (as with $a=1.05$ ) $S(a)$ diverges, alternately approaching $1$ and $\infty$ (as with $a=1.\overline{01}$ ) Background Watched a video on YouTube called ""Dungeon Numbers (extra) - Numberphile"". I got obsessed with it for a while and tried doing something similar for complex numbers some years back. I came back to the problem again and felt it was solvable not using too crazy techniques. My attempt (EDIT: This doesn't seem to work quite right as it breaks for $a=1.\overline{1}$ as will be shown below) After many days of trying things, I finally came up with the following: Given $a\in(1,2)$ define the function: $$\mathcal{F_a}(x)=\sum_{k=0}^{\infty}\frac{a_{[k]}}{x^k}$$ , consider the roots of $$\mathcal{D_a}(x)=\mathcal{F_a}(x) - \mathcal{F_a}^3(x)$$ (EDIT: After some more fiddling, I realized $\mathcal{D_a}(x)$ can be replaced with $\mathcal{F_a}^2(x)-x$ , which makes things somewhat easier) Where $\mathcal{F_a}^3(x)$ stands for $\mathcal{F_a}\left(\mathcal{F_a}(\mathcal{F_a}(x))\right)$ . The number $3$ may be exchanged for any odd number greater than it and the following seems to hold: Except for the trivial case, when $a$ is an integer, the roots of $\mathcal{D_a}(x)$ describe the long term behavior of $S(a)$ in this way: If $S(a)$ converges to a single number $x_0$ , then $\mathcal{D_a}(x)$ has a single root in $\mathbb{R}^+$ . This root is $x_0$ If $S(a)$ converges to the two-term limit cycle $(x_0, x_2)$ , then $\mathcal{D_a}(x)$ has three roots in $\mathbb{R}^+$ . These roots are $x_0, x_1, x_2$ , with $x_0\lt x_1 \lt x_2$ I'm not sure, if this holds outside the interval $(1,2)$ , or if it even holds inside that interval, but everything I tested seemed to agree with it. I don't understand at all why this works though, so I'm struggling to prove things.
Any help would be much appreciated. I came up with this method when playing around with iterating the function $\mathcal{F_a}(x)$ and plotting the results. I noticed, that all the plots seemed to intersect and certain points and found that those were exactly the values, that $S(a)$ would converge to Example Calculations: $a=1.1$ $$\mathcal{F_a}(x)=1+\frac{1}{x}=\frac{x+1}{x}$$ $$\mathcal{F}_a^3(x)= 1+\frac{1}{1+\frac{1}{1+\frac{1}{x}}} = \frac{3x+2}{2x+1}$$ Finding positive roots of $\mathcal{D_a}(x)$ (intersection of $\mathcal{F_a}$ and $\mathcal{F}_a^3$ ) $$\frac{3x+2}{2x+1}=\frac{x+1}{x}$$ $$x^2-x-1$$ $$x=\frac{1}{2}+\frac{\sqrt{5}}{2}$$ $a=1.28$ $$\mathcal{F}_a(x)=1+\frac{2}{x}+\frac{8}{x^2}$$ $$\mathcal{F}_a^3(x)=1+\frac{2}{\left(1+\frac{2}{\left(1+\frac{2}{x}+\frac{8}{x^2}\right)}+\frac{8}{\left(1+\frac{2}{x}+\frac{8}{x^2}\right)^2}\right)}+\frac{8}{\left(1+\frac{2}{\left(1+\frac{2}{x}+\frac{8}{x^2}\right)}+\frac{8}{\left(1+\frac{2}{x}+\frac{8}{x^2}\right)^2}\right)^2}$$ Using WolframAlpha, it is found, that the roots of $\mathcal{D_a}(x)$ in $\mathbb{R}^+$ are: $x_0 = 2$ $x_1 = \frac{1}{3}\left(1+\sqrt[3]{118+3\sqrt{1509}}+\sqrt[3]{118-3\sqrt{1509}}\right)\approx 2.7673$ $x_2 = 4$ and indeed, when doing the iteration, the sequence does converge to the limit cycle $(2,4)$ $a=1.\overline{1}$ This doesn't work, because $$\mathcal{F_a}(x)=\sum_{k=0}^{\infty}\frac{1}{x^k}=\frac{x}{x-1}$$ $$\mathcal{F}_a^3(x)=\frac{\frac{\frac{x}{x-1}}{\frac{x}{x-1}-1}}{\frac{\frac{x}{x-1}}{\frac{x}{x-1}-1}-1} = \frac{x}{x-1} = \mathcal{F_a}(x)$$ $$\implies \mathcal{D_a}(x) \equiv 0$$ But $$S(a)=(1.\overline{1}, 10, 1.\overline{1}, 10, ...)$$ $a=1.1110000099$ This case is also an exception, but it doesn't break completely. Since The formulas get a little unwieldy, the roots of (using the new method) $\mathcal{D_a}(x)=\mathcal{F_a}^2(x)-x$ were found using maple. In this case there are five of them. When iterating $a$ , it converges to a two-cyle: $(1.1077.., 10.2682..)$ . These match the first and the fifth root of $\mathcal{D_a}(x)$ . I'm not quite sure how much this adds, but here are three cobweb plots for $a=1.03, a=1.04, a=1.05$ . The curves are the functions $\mathcal{F_a}$ . The origin-line is $y=x$ . In the plot for $1.03$ we see a nice and quick convergence. In the plot for $1.04$ we see much slower convergence - this is reflected by the fact, that $\mathcal{F^2_a}(x)-x$ has a higher multiplicity root, than in the case for $1.03$ . Finally, in the third plot, we see a convergence to a two term limit cycle. Any and all comments, tips or critiques are very much appreciated.",['sequences-and-series']
4607654,"Supplementary exercise 51, chapter 3 from 'A walk through combinatorics' 4th edition","I'm selfstudying 'A walk through combinatorics' by Miklós Bóna. This book has some supplementary exercises at the end of each chapter, no solution provided. I'm trying exercise 51, chapter 3. Problemstatement: A store has $n$ different products for sale. Each of them has a different price that is at least one dollar, at most $n$ dollars, and is a whole dollar. A customer only has the time to inspect $k$ different products. After doing so, she buys the product that has the lowest price among the $k$ products she inspected. Prove that on average she will pay $\frac{n+1}{k+1}$ dollars. Attempt at solution: The customer pays: 1 dollar if the product with price 1 dollar is the cheapest among the $k$ inspected products. This is possible in $\binom{n-1}{k-1}$ ways, since we need to pick $k-1$ products with prices in $\{2, \ldots, n\}$ . 2 dollars if the product with price 2 dollars is the cheapest among the $k$ inspected products. This is possible in $\binom{n-2}{k-1}$ ways, since we need to pick $k-1$ products with prices in $\{3, \ldots, n\}$ . $\ldots$ $n-k+1$ dollar if the product with price $n-k+1$ is the cheapest among the $k$ inspected products. There are only $k-1$ products which are more expensive, so this is possible in $\binom{n-(n-k+1)}{k-1} = \binom{k-1}{k-1}$ ways. This implies that the customer has to pay, on average $$\frac{1}{\binom{n}{k}}\cdot\left(1\cdot \binom{n-1}{k-1} + 2 \cdot \binom{n-2}{k-1} + \ldots + (n-k+1) \cdot \binom{k-1}{k-1}\right).$$ This is where I'm stuck : I want to show that the sum in brackets equals $\binom{n+1}{k+1}$ (guess based on what we need to prove, checked with some values of $n,k$ . This seems to be correct). Question: How to prove that $$1\cdot \binom{n-1}{k-1} + 2 \cdot \binom{n-2}{k-1} + \ldots + (n-k+1) \cdot \binom{k-1}{k-1} = \binom{n+1}{k+1},$$ a hint would be appreciated. I tried to give a combinatorial proof, but failed.","['combinations', 'combinatorics', 'combinatorial-proofs', 'probability']"
4607675,How is Siegel's lemma applied in number theory?,"In the Wikipedia page , Siegel's lemma is stated as follows: Consider the system $$ \begin{cases} \sum_{i=1}^Na_{1i}X_i=0\\
\vdots\\ \sum_{i=1}^Na_{Mi}X_i=0 \end{cases}, $$ where the
coefficients $a_{11}, a_{12}, \dots, a_{MN}$ are integers between $-B$ and $B$ . The system has a nonzero integer solution $(X_1, X_2, \dots,
 X_N)$ such that $\max_i \left|X_i\right| \le (NB)^{M/(N-M)}$ . In my understanding, Siegel's lemma is of great importance in Diophantine approximation. However, I haven't been able to find an example of a problem where this formulation of the lemma has been applied to problems in Diophantine approximation. How is this applied in number theory?","['number-theory', 'linear-algebra', 'diophantine-approximation', 'transcendental-numbers']"
4607689,Does this ODE have non trivial periodic orbits?,"I am dealing with the following system of differential equations, where $\alpha,\beta$ are two angles of a triangle (so I'm only interested in the region $\alpha,\beta\geq0,\alpha+\beta\leq\pi$ ): $$\alpha'=-\frac{\cos(\frac{\alpha}{2}+\beta)\sin(\alpha)}{\sin(\beta/2)\cos(\frac{\alpha+\beta}{2})}$$ $$\beta'=\frac{\cos(\frac{\beta}{2}+\alpha)\sin(\beta)}{\sin(\alpha/2)\cos(\frac{\alpha+\beta}{2})}$$ After representing it in Mathematica (see the program and vector plot below), I find it likely that the solution of the differential equation is composed by closed orbits surrounding the point $(\frac{\pi}{3},\frac{\pi}{3})$ . The easiest way I know to prove that is to find a function $H:\mathbb{R}^2\to\mathbb{R}$ such that $\frac{\partial H}{\partial \alpha}\alpha'+\frac{\partial H}{\partial \beta}\beta'=0$ (so that $H$ is invariant along the orbits of the differential equation). However I haven't found anything like that. I am looking for any kind of help with understanding the behaviour of this differential equation, including proving whether the orbits are periodic or not, by any method.","['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
4607703,Examine for conditional and absolute convergence,"The problem: $$\sum_{n=1}^\infty \sqrt{n+1} - \sqrt[4]{n^2+n+1} $$ What I've done trying the root ratio test: $$\lim_{n\to \infty} |\frac{\sqrt{n+2} - \sqrt[4]{n^2+2n+1+n+2}}{\sqrt{n+1} - \sqrt[4]{n^2+n+1}}|<1 $$ $$\lim_{n\to \infty} |\frac{\sqrt{n+2} - \sqrt[4]{n^2+3n+3}}{\sqrt{n+1} - \sqrt[4]{n^2+n+1}} |<1$$ $$\lim_{n\to \infty} |\frac{e^{\frac{1}{2}ln(n+2)} - e^{\frac{1}{4}ln(n^2+3n+3)}}{e^{\frac{1}{2}ln(n+2)} - e^{\frac{1}{4}ln(n^2+3n+3)}}|<1 $$ The question: I don't know where to go from here. Maybe the criteria I am using is wrong. I think it could be solved if we compare it with something, but I don't know what.","['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
4607713,Understanding Product-Sigma-Algebra,"I know the following defintion for a Product-Sigma-Algebra: Let be $ (\Omega_1,\mathcal{A}_1,\mu_1) $ and $ (\Omega_2,\mathcal{A}_2,\mu_2) $ two measure spaces. The Sigma-Algebra over $ \Omega_1\times \Omega_2 $ generated by the sets of the form $ A_1\times A_2 , A_i\in \mathcal{A}_i, i=1,2 $ is called Product-Sigma-Alegbra of $ \mathcal{A}_1 $ and $ \mathcal{A}_2 $ and is named by $ \mathcal{A}_1\otimes \mathcal{A}_2  $ such that $ \mathcal{A}_1\otimes \mathcal{A}_2=\sigma\left(\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\right) $ . Why does contain $ \mathcal{A}_1\otimes \mathcal{A}_2  $ the set $ \mathcal{A}_1\times \mathcal{A}_2  $ and in which relation? Is it like "" $\in$ "" or "" $\subseteq$ ""? I tried to comprehend it by an example: $ \Omega_1=\{7\}, \mathcal{A}_1=\{\emptyset, \{7\}\}$ and $\Omega_2=\{1,2\},\mathcal{A}_2=\{\emptyset, \{1\},\{2\},\{1,2\}\} $ and $ \Omega_1\times \Omega_2=\{(7,1),(7,2)\} $ . Now I calculate $$\mathcal{A}_1\times \mathcal{A}_2=\{(\emptyset,\emptyset),(\emptyset,\{1\}),(\emptyset,\{2\}),(\emptyset,\{1,2\}),(\{7\},\emptyset),(\{7\},\{1\}),(\{7\},\{2\}),(\{7\},\{1,2\})\}$$ and $$ \begin{aligned}&\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\\[10pt]=&\{\emptyset\times \emptyset,\emptyset\times \{1\},\emptyset\times \{2\},\emptyset\times \{1,2\},\{7\}\times \emptyset, \{7\}\times \{1\}, \{7\}\times \{2\}, \{7\}\times \{1,2\}\}\\[10pt]=&\{\emptyset,\emptyset,\emptyset,\emptyset,\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\}\\[10pt]=&\{\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\}. \end{aligned} $$ Finally I get $$ \mathcal{A}_1\otimes \mathcal{A}_2=\sigma\left(\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\right)=\{\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\} $$ but the elements in the set $ \mathcal{A}_1\times \mathcal{A}_2 $ are different. What went wrong?","['measure-theory', 'analysis']"
4607715,Complex Banach spaces and invertibility in Spectral Theory,"The following question is from my assignment in spectral theory and I am not able to prove the assertions I am asked to prove. Question: Let X be a Complex Banach space, and let $A,B \in L(X)$ . It is assumed that $AB=I$ and $BA\neq I$ . Consider any $\lambda \in \mathbb{C}$ such that $I-\lambda B$ is invertible. Show that $(A- \lambda I) B (I-\lambda B)^{-1} =I $ and $B ( I-\lambda B)^{-1} ( A-\lambda I) \neq I$ . Also deduce that $A-\lambda I$ is not invertible (provided by that $I-\lambda B$ is invertible). Very unfortunately, I am not able to prove any of the assertions asked. For spectral theory, I have been following my class notes only. The problem I think I am facing is that I am not sure which exact proposition could be helpful in the question. I have studied the following concepts in this section: K-algebra, Normed algebra, Spectrum, Spectral radius, unitary Banach algebras, invertible elements. Can you please outline which results I should use to prove the asked assertions. No need to give complete proofs. I shall be grateful!","['banach-spaces', 'operator-theory', 'spectral-theory', 'functional-analysis']"
4607760,What is the value of $\lim \limits_{n\to \infty} \left(\sin\left(n\pi\sqrt[3]{n^3+3n^2+4n-5}\right)\right)$?,"This question was posted recently on a Facebook group.  Find the limit: $$\lim \limits_{n\to \infty} \left(\sin\left(n\pi\sqrt[3]{n^3+3n^2+4n-5}\right)\right)$$ My original analysis was: Regarding the term inside the cube root, as $n \to \infty$ , the $n^3$ term dominates over the other three, which means the whole expression tends to $n^3$ , which means the limit becomes $\sin(\pi n^2),$ which is indeterminate. However when I write a program using quad precision math, the limit appears to come out as $\frac{\sqrt3}{2}$ . n                  function
10.00000000000 0.338982167791830794609548974616303
100.0000000000 0.822015011893709733119339192500436
1000.000000000 0.861814054083077403616129702505113
10000.00000000 0.865606297805291762325586009448498
100000.0000000 0.865983513611437523662920376306691
1000000.000000 0.866021214971522960916097562022775
10000000.00000 0.866024984905191057632658510545867
100000000.0000 0.866025361896534314025779227654027
1000000000.000 0.866025399595646736775927996413246
10000000000.00 0.866025403365387551257815743216080
100000000000.0 0.866025403723528925571455996123769 It is not clear to me analytically why. Update: n is not necessarily an integer. In fact if 0.6 is added to $n$ in my program, the limit is now a different number: n                  function
10.6000000000000    -0.25072893003135069931292
100.600000000000    -0.74448187296497062922343
1000.60000000000    -0.79144938488268274939869
10000.6000000000    -0.79602325415859850300212
100000.600000000    -0.79647926568469037482324
1000000.60000000    -0.79652485293086939316975
10000000.6000000    -0.79652941151626998534906
100000000.600000    -0.79652986737341769811125
1000000000.60000    -0.79652991295911686316286
10000000000.6000    -0.79652991751749772070301
100000000000.600    -0.79652991795050127245904
1000000000000.60    -0.79652991562289847750918
10000000000000.6    -0.79652965390217094955424",['limits']
4607774,Durrett Exercise 1.4.2,"1.4.2 Let $f\geq 0$ and $E_{n,m}=\{x: m/2^n \leq f(x) \leq (m+1) /2^n\}$ . As $n \uparrow \infty$ , show that: $$\sum_{m=1}^{\infty} \frac{m}{2^n} \mu(E_{m,n}) \uparrow \int f d\mu$$ I was trying to solve this exercise without any of the convergence theorems.
(To solve with Monotone convergence theorem, we only need to define $g_n=\sum_{m=1}^{\infty} \frac{m}{2^n} 1_{E_{n,m}}$ , which is an increasing sequence of functions that converges to $f$ ) I was able to find this answer , which defines a simple function approximation to $f$ $$\phi_n(x)=\begin{cases}
m/2^n,&m/2^n\leq f(x)<(m+1)/2^n \\
n,&f(x)\geq n
\end{cases}
$$ and claims that It is easy to show $\displaystyle \int f=\lim_{n\to\infty} \int \phi_n$ . I fail to see why this is true. Also, the answer suggests that we could use the representation $$\phi_n=\sum_{m=0}^{n2^n-1}m/2^n 1_{E_{n,m}}+\sum_{m=n2^n}^\infty n 1_{E_{n,m}}$$ and finish by noting $$\phi_n(x) \leq g_n(x) \leq f(x)$$ Is this limit well-defined when $n \to \infty$ ?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
4607776,Divergence Theorem,"I'm looking for an alternate proof of a result. Let $A,B,C\in\mathbb{S}^d$ . Let $T$ be the intersection of $\mathbb{S}^d$ with the cone generated by $A,B,C$ . Then, $T$ is a spherical triangle. The vertices $A,B,C$ are opposite arcs with lengths $a,b,c$ . Call $[T]$ the area of $T$ . The centroid of $T$ is $$g:=\frac{1}{[T]}\int_T xd\mu$$ where $\mu$ is uniform on the sphere. JE Brock found the centroid of a spherical triangle $T=\triangle ABC$ to be (paraphrased, but it checks out for me) $$g=\frac{1}{2[T]}\left(\frac{A\times B}{|A\times B|}c+\frac{B\times C}{|B\times C|}a+\frac{C\times A}{|C\times A|}b\right)$$ Thinking of $A\times B/|A\times B|$ as the unit vector perpendicular to side $c$ , we can write this as $$\int_T xd\mu=g*[T]=\frac{1}{2}\int_{\partial T} \vec{n} ds$$ where $\vec{n}$ is the inward pointing unit vector. This formulation looks like the divergence theorem. So, my question is how to prove it with the divergence or Stokes Theorem. $$\int_T\mathrm{div}F=\int_{\partial T} F\cdot\vec{n}$$ I've tried separating components setting the vector field $F$ to $(x_1^2,0,\dots,0)$ to get a linear term in the divergence, but that doesn't match the right side where I expect $F=(1,0,\dots,0)$ to get just the normal around the boundary. So, I suspect I need to use Stoke's Theorem on manifolds. The form can be the unit tangent so that the derivative points toward the center of the sphere. That would fit the form I'm looking for, I just need some details. Does that seem like the right course? Thanks!
Happy Holidays!","['geometry', 'multivariable-calculus', 'multiple-integral', 'stokes-theorem', 'differential-geometry']"
4607791,Gradient of a homogenous function - proof with a help function ; Unsure about a derivative and about using the multidimensional chain rule,"Let $k$ be an integer. A function $f : \mathbb{R^n} \to \mathbb{R}$ is called homogenous of degree $k$ if $f(\lambda x) = \lambda^k f(x)$ for all $\lambda \in \mathbb{R}$ and $x \in \mathbb{R^n}$ . Prove that if $f$ is homogenous of degree $k$ then $x \cdot \nabla f(x) = kf(x)$ . Task: Prove this statement using the function $g:\mathbb{R}^+\rightarrow \mathbb{R}, \lambda \mapsto f(\lambda x)$ for a fixed $x\in \mathbb{R^n}$ Proof: Let $x\in \mathbb{R^n}$ be arbitrary but fixed. i) $g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)=\frac{d}{d\lambda}(\lambda^kf(x))=k \lambda^{k-1}f(x)$ $\Rightarrow g'(1)=kf(x)$ ii) $g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)=x\cdot f'(\lambda x)\Rightarrow g'(1)=x\cdot f'(x)$ $(i),(ii)\Rightarrow g'(1)=x\cdot f'(x)=kf(x)$ almost q.e.d. I have problems with $\frac{d}{d\lambda}f(\lambda x)=x\cdot f'(\lambda x)$ in (ii) because it should probably be $\frac{d}{d\lambda}f(\lambda x)=x\cdot \nabla f(\lambda x)$ but I cannot justify it.
Is the notation $g'(\lambda)=\frac{d}{d\lambda}f(\lambda x)$ wrong? I used this notation  because $g$ is a onedimensional function. I tried to use the multivariable chain rule with $h:\mathbb{R}^+\rightarrow \mathbb{R^n}, \lambda \mapsto \lambda x$ $g(\lambda)=f(h(\lambda))\Rightarrow g'(\lambda)=Dg(\lambda)=D(f\circ h)(\lambda)=Df(\lambda x)\cdot Dh(\lambda)=\nabla f(\lambda x)\cdot h'(\lambda)=\nabla f(\lambda x)\cdot x= x\cdot \nabla f(\lambda x)=0$ because $\nabla f(\lambda x)=0$ for a fixed $x$ . Apart from the point mentioned, is the proof correct?","['multivariable-calculus', 'solution-verification', 'real-analysis']"
4607810,Prove that $x \in B - {\bigcap_{j \in J} A_j}$ is equivalent to $x \in \bigcup_{j \in J} B-{A_j} $,"I have to prove that $x \in B -\{ \bigcap_{j \in J} A_j\}$ is equivalent to $x \in \bigcup_{j \in J} B-\{A_j\} $ .
The first statement can be written as $x \in B$ and $x \notin\bigcap_{j \in J} A_j$ I thought what one can maybe use is deMorgan's theorem and write this as: $\neg (x\notin B$ or $x\in \bigcap_{j \in J} A_j)$ I am not sure if this is correct and if how to proceed. Hope somebody can help.",['elementary-set-theory']
4607869,"In a triangle $\triangle ABC$, the sum of two sides is $x$, and their product is $y$. If $x^2-c^2=y$, find $\frac{r}{R}$ in terms of $x, c, y$","This problem comes from a previous JEE Advanced examination, the problem is as follows: In a triangle $\triangle ABC$ , the sum of two sides is $x$ , and their product is $y$ . If $x^2-c^2=y$ where $c$ is the third side, find the ratio of the inradius to the circumradius in terms of $x, c, y$ This is indeed a pretty challenging and tricky problem. Upon initial examination, spamming algebra seems to not lead anywhere. I found a solution using some angle chasing, which I'll share below, please share your own approaches as well! Let $a+b=x$ and $ab=y$ We know that: $$x^2-c^2=y$$ $$(a+b)^2-c^2=ab$$ $$a^2+b^2-c^2=-ab$$ Dividing by $2ab$ and exploiting the Law of Cosines, we can say that: $$\frac{a^2+b^2-c^2}{2ab}=-\frac{1}{2}$$ Now, this implies that $\angle C=120^\circ$ Now, using the Law of Sines, we get: $$\frac{1}{2R}=\frac{\sin{C}}{c}$$ $$\frac{c}{2R}=\frac{\sqrt3}{2}$$ $$R=\frac{c}{\sqrt3}$$ We also know that: $$\Delta= \frac{1}{2}ab\sin{C}$$ $$\Delta=\frac{\sqrt{3}y}{4}$$ Further, we can note that: $$s=\frac{a+b+c}{2}$$ $$s=\frac{x+c}{2}$$ Also, $r=\frac{\Delta}{s}$ , therefore: $$\frac{2\sqrt{3}y}{4(x+c)}=r$$ $$r=\frac{\sqrt{3}y}{2(x+c)}$$ Finally: $$\frac{r}{R}=(\frac{\sqrt{3}y}{2(x+c)})(\frac{\sqrt{3}}{c})$$ $$\frac{r}{R}=\frac{3y}{2c(x+c)}$$","['trigonometry', 'euclidean-geometry', 'algebra-precalculus', 'geometry']"
4607908,Why matrix representation of a linear transformation doesn't encode choice of basis for its range and domain?,"A linear transformation T is defined to be: $T: V \mapsto W$ , $Tv_k= \sum A_{i,j} *w_j$ , where $v_i$ 's are basises for $v$ and $w_j$ 's are basises for $W$ . A matrix representation of $T$ only encodes the $A_{i,j}$ leaving the choice of basis for domain and range( $v_i$ 's and $v_j$ 's). To me it sounds like a big flaw. Choice of basis for $V$ and $W$ affects $T$ 's characteristics like being Identity of not. For example if $V$ and $W$ are both $\mathbb{R^2}$ , $v_1 = (1,0)$ , $v_2 =(1,0)$ and $w_1 =  (0, 1)$ and $w_2 = (1,0)$ , let \begin{equation}
T = \begin{bmatrix}
0 & 1\\
1& 0
\end{bmatrix} .
\end{equation} $T$ is going to map elements of $V$ to same elements of $W$ . Thus, $T$ is identity while it doesn't look like a identity matrix. Similarly, an Identity matrix where choice of basis for its domain and range are not the same, is not going to be identity. This was annoying me for a while until I pinpointed the problem.","['matrices', 'linear-algebra']"
4607919,Can a solution to an ODE/PDE stop solving the equation without losing regularity?,"Suppose you have an initial value problem for a possibly non-linear ODE $$ \frac{dy}{dt} = f(t,y(t)), \hspace{20pt} y(t_{0}) = y_{0}, $$ where $f$ is Lipschitz continuous in $t$ and $y$ for now. Suppose we know that there exists a solution on $[0,T^{*})$ . Question: Is it possible for a solution to stay regular for all time but  it stops solving the ODE from some time $T$ onwards? In other words, to prove the global existence of a solution, is it enough to show that the solution does not lose regularity? I don't know if this is a dumb question but it doesn't seem obvious to me. Does anything change in the PDE setting?","['analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
4607938,Show that $ A − A^2$ is invertible given $A$'s eigenvalues?,"Suppose that the $2 \times 2$ matrix $A$ has the characteristic polynomial $p(\lambda) = (\lambda + 1)(\lambda + 2).$ Show that $ A − A^2$ is invertible and determine the eigenvalues to the inverse. So this is how I tried. $p(\lambda)=0$ gives me that $\lambda_1 = -1$ and $\lambda_2 = -2$ This means that we have at least two linearly independent vectors which means that the matrix $A$ is diagonalizable.
So we have: $A = PDP^{-1}$ $A - A^2 = PDP^{-1} - PDP^{-1} PDP^{-1} = PDP^{-1} - PD^2 P^{-1} = P(D - D^2) P^{-1} $ $D = ([-1, 0]^T , [0, -2]^T)$ $D - D^2 = ([-2, 0]^T [0, -6]^T)$ But this all feels unnecessary and I feel lost. Am I even thinking right?","['matrices', 'linear-algebra', 'inverse', 'matrix-equations', 'diagonalization']"
4607957,Getting different values of the integral when integrating with different order,"I want to determine the volume of the shape given by the following $$ K = \{(x,y,z): x^2+y^2+z^2 \leq 2 \, \, , x+y>0 \, \, , \, z\leq 1\} $$ I thought that i can integrate with respect to $xy$ -plane first by using polar coordinates $$ \int_{-\sqrt{2}}^1 \biggl( \int_{-\frac{\pi}{4}}^{\frac{3 \pi}{4}} \int_0^{\sqrt{2-z^2}} r \, \, drd\theta \biggr) \, dz \quad=\quad \frac{\pi}{2} \int_{-\sqrt{2}}^1 2-z^2 \, dz \quad=\quad ..... \quad=\quad \frac{\pi}{2} \biggl( \frac{5-2\sqrt{2}}{3} + 2\sqrt{2} \biggr) $$ But if I integrate with respect to $z$ first I get the following $$ \iint_D \biggl( \int_{-\sqrt{2-(x^2+y^2)}}^1 \, dz \biggr) \, dxdy \quad=\quad \int_{-\frac{\pi}{4}}^{\frac{3 \pi}{4}} \int_0^{\sqrt{2}} \biggl(1+\sqrt{2-r^2} \biggr) r \, \, drd\theta  = \, \, ...... \, \, = \quad \frac{\pi}{3} \biggl( 3 + 2 \sqrt{2} \biggr) $$ Which is very confusing since the value of the integral should be the same regardless of the order of integration! Can anyone see where I missed up?","['integration', 'volume', 'multivariable-calculus', 'calculus', 'polar-coordinates']"
4607989,Can Strong LLN and Weak LLN apply to continuous distributions?,"Can Strong LLN and Weak LLN apply to continuous distributions? Or it can only apply to discrete distributions? Representation of LLN: $X_1,X_2,\ldots,X_n$ are i.i.d, and their expectation values are finite, then $$(X_1+X_2+\cdots+X_n)/n \longrightarrow  \mathrm{E}(X),$$ as $n$ goes to infinity. Moreover, just to confirm, Central limit theorem also can apply to continuous distributions, right?","['statistical-inference', 'statistics', 'probability-distributions', 'probability-theory']"
4608027,Representation vs group action?,"I'm working this problem from Howe's An Invitation to Representation Theory : Let $G = (\mathbb{R}^*, \times)$ be the group of non-zero real numbers under multiplication, and let $P_2$ denote the vector space of polynomials (in one variable) of degree two or less. Verify that the action of $G$ on $P_2$ given by $a\mathbin{.}p(x) = p(a^{-1}x)$ is a representation of $G$ on $P_2$ . I'm not sure where to start because I'm having trouble differentiating the group action from the representation. Any advice?","['representation-theory', 'abstract-algebra', 'linear-algebra', 'group-theory', 'group-actions']"
4608034,Find particular solution of system of linear ODE,"I would like to solve the ODE system $$
\begin{pmatrix}x'(t)\\y'(t)\end{pmatrix}=\begin{pmatrix}0 & 12\\3 & 0\end{pmatrix}\begin{pmatrix}x(t)\\y(t)\end{pmatrix}-\begin{pmatrix}36\\9\end{pmatrix}
$$ with initial conditions $x(0)=3$ and $2y(1)-x(1)=7$ . I already determined the fundamental matrix for the homogeneous problem to be $$
\begin{pmatrix}2e^{6t} & -2e^{-6t}\\e^{6t} & e^{-6t}\end{pmatrix}.
$$ My problem is how to find a particular solution $x_p,y_p$ . Is it suitable to make the ansatz $x_p(t)=c_1$ and $y_p(t)=c_2$ for constants $c_1,c_2$ ? Then I get $x_p=y_p=3$ and the general solution $$
\begin{align}
x(t)&=2C_1e^{6t}-2C_2e^{-6t}+3,\\
y(t)&=C_1e^{6t}+C_2e^{-6t}+3.
\end{align}
$$ Using the initial value conditions, my solution is $$
\begin{align}
x(t)&=2e^6\left(e^{6t}-e^{-6t}\right)+3,\\
y(t)&=e^6\left(e^{6t}+e^{-6t}\right)+3.
\end{align}
$$",['ordinary-differential-equations']
4608052,Existence of Banach limits: Translation invariance,"A positive functional $\Phi$ on $\ell^{\infty}$ is said to be a Banach limit if $\Phi(1,1,1,\ldots)=1$ and $\Phi\circ L=\Phi$ where $L$ is the left shift operator on $\ell^{\infty}$ . Show that there exists a Banach limit. My attempt: Consider the functionals $p: (x_n)\mapsto \limsup x_n$ on $\ell^{\infty}$ and $f:(x_n)\mapsto \lim x_n$ on $c$ (the space of convergent sequences). Then $f$ is linear, $p$ is sublinear, and $f\le p$ on $c$ . Therefore  by Hahn Banach theorem, there exists a functional $\Phi$ on $\ell^{\infty}$ such that $\Phi=f$ on $c$ and $\Phi\le p$ on $\ell^{\infty}$ . As $f$ is positive, so is $\Phi$ . Moreover, $\Phi(1,1,1,\ldots)=f(1,1,1,\ldots)=1$ . My question: How to show that $\Phi\circ L=\Phi$ ? I observed the following: $f\circ L=f=p$ on $c$ and $p\circ L=p$ on $\ell^{\infty}$ but don't know if it's useful.","['operator-theory', 'hahn-banach-theorem', 'functional-analysis']"
4608055,System of quadratic autonomous ODEs - convexity of the solution curve,"Crossposted on MathOverflow Problem: For a given parameter $a>0$ , consider the following autonomous system of ODEs for $(x,y,z): \mathbb R_+\to [0,1)^3$ : \begin{align*}
\dot{x}_t &= (1-x_t) (z_t-x_ty_t)
&=:F^x(x_t,y_t,z_t) \\
\dot{y}_t &= \tfrac 1 2 y_t^2 - (a+x_ty_t)(1-y_t)
&=:F^y(x_t,y_t,z_t) \\
\dot{z}_t &= \tfrac 1 2 z_t^2 - \tfrac 1 2 y_t^2 + (a+x_ty_t)z_t
&=:F^z(x_t,y_t,z_t)
\end{align*} together with the initial condition $x_0=0$ . By solution I refer to the profile $(x_t,y_t,z_t)\in [0,1)^3$ that is defined for all $t\geq 0$ (there is unique point $(0,y_0,z_0)\in [0,1)^3$ such that the orbit passing through it does not explode). Prove (analytically) that a) the curve consisting of points $(x_t,y_t)_{t\geq 0}$ is a convex; b) the curve consisting of points $(x_t,z_t)_{t\geq 0}$ is decreasing. Question: Reference to any related paper / book. Figure 1: Solution as a function of $t$ (for $a=0.1$ ). $t$ "" /> Figure 2: Solution represented by functions $Y,Z$ such that $(x_t,y_t,z_t)= (x_t,Y(x_t),Z(x_t))$ for all $t\geq 0$ (the red point represents the critical point of the system): $Y$ and $Z$ ."" /> Note: I called the system quadratic in the title because substituting $u_t = x_t y_t$ converts the original (cubic) system into a quadratic one. Context: I obtained this system of ODEs when studying patent race with private information : Two firms compete making a patent, they exert a costly effort that translates into the hazard rate of making a discovery. To patent a firm has to make two consecutive discoveries, the first one that does so wins (reward 1) whilst the other loses (reward 0). Having made the first discovery is firms private information, the rival only infers a posterior belief about it based on the fact that nobody patented yet. In the presented equations this belief is represented by variable $x_t$ ; $y_t$ is the effort of a firm that has made the first discovery already; $z_t$ is the effort of a firm that has made no discovery yet, and the parameter $a>0$ is the rate at which future payoffs are being discounted. Basic Observations I have analytical proofs of the following properties: The system has a unique critical point $(x_*,y_*,z_*)\in [0,1)^3$ . For every solution, $t\mapsto x_t$ is increasing and the trajectory $(x_t,y_t,z_t)$ converges to the critical point $(x_*,y_*,z_*)$ . The Jacobian $J$ of the system has one negative eigenvalue $\lambda_1$ and the other eigenvalues have strictly positive real parts. Thus, by Hartman-Grobman Theorem the eigenvector $v=(v^x,v^y,v^z)$ associated with $\lambda_1$ determines the direction in which a trajectory can converge to the critical point, and there is unique local solution near the critical point with $x_t$ increasing. The local solution can be extended till $x=0$ is reaches, so there exist unique functions $Y(x),Z(x):[0,x_*)\to [0,1)$ such that $y_t = Y(x_t)$ and $z_t = Z(x_t)$ for any solution of the initial problem with the initial condition $x_0=\hat x$ . Current state of research: I found a way to prove some basic properties like that $Y(x)$ is increasing and $Z(x)$ is decreasing and I'm working on proving that $Y(x)$ is convex. However, I prove everything using methods that I develop using just elementary mathematical analysis. I belive there should be some standard methods that I could apply instead of developing my own. More details on the methods that I use: The method that I've been using utilises the fact that at a given $\alpha>0$ and $x\in [0,x^*]$ $$y\mapsto G(y;x,\alpha)=F^y(x,y,Z(x)) - \alpha F^x(x,y,Z(x))$$ is a quadratic function and it can be shown that $Y'(x) \geq \alpha$ whenever $G(y;x,\alpha)>0$ . Then I consider the isocline $\hat Y(x;\alpha)$ given implicitly by $G(\hat Y(x;\alpha);x,\alpha)=0$ , i.e. the value that $y$ would need to have at $x$ in order for the slope $Y'(x;y)$ of the orbit passing by $(x,y,Z(x))$ to be equal to $\alpha$ . In the next step I show that $\hat Y'(x;\alpha) < \alpha$ and so the curves $Y(x)$ and $\hat Y(x;\alpha)$ only cross at the critical point $x_*$ . This way I can show some desirable properties of $Y(x)$ , using some assumptions on $Z(x)$ . Then I use similar method to prove properties of $Z(x)$ using assumptions on $Y(x)$ . Finally, I show that both properties of $Y(x)$ and $Z(x)$ hold (without the argument being cyclical). I tried to formulate an abstraction / simplification of this question: Convexity / concavity of $(g_t,x_t)$, where $x_t$ solves the ODE $\dot x_t=\tfrac 1 2 x_t^2 - (1-x_t) g_t$. Solution $x:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at})$ is a concave function of $g(t)=1-e^{-at}$. Objective: I'd be grateful for any reference to a related studies.","['ordinary-differential-equations', 'real-analysis', 'differential-games', 'convex-analysis', 'dynamical-systems']"
4608083,Regarding a converse to Hopf's Umlaufsatz,"I read in a differential geometry textbook that the total signed curvature of a closed plane curve is an integer multiple of $2\pi$ . In that same textbook, I also read about Hopf's Umlaufsatz, which states that the total signed curvature of a simple closed plane curve is either $2\pi$ or $-2\pi$ . Now, I am interested in a converse of the previous statement. If a closed plane curve has total signed curvature either $2\pi$ or $-2\pi$ , must that curve be a simple closed plane curve?",['differential-geometry']
4608181,The word problem for groups preceded PH Theorem as a practical undecidability result,The Paris-Harrington theorem is often cited as the first practical instance of an undecidable problem since it doesn't depend on self-reference or diagonalization and is an interesting mathematical question in its own right. But why isn't the unsolvability of the word problem on groups considered for that distinction? It came two decades before PH theorem. Am I misunderstanding some subtle difference between the two ideas? Or is PH overshadowing the unsolvability for the word problem on groups in the history of undecidability logic?,"['incompleteness', 'group-theory', 'logic', 'combinatorics-on-words']"
4608186,Inner products on $M_n(F)$ inducing the same norm on all matrices of rank one,"This question is related to exercise 11 on page 310 of Hoffman and Kunze's Linear Algebra book. Let $F:=\mathbb R/\mathbb C$ . Let $\langle \ , \ \rangle_1,\langle \ , \ \rangle_2$ be two inner products on $M_n(F)$ with induced norms $\lVert \ \rVert_1,\lVert \ \rVert_2$ , respectively. If $\lVert A\rVert_1=\lVert A\rVert_2$ for all $n\times n$ matrices $A$ of rank one, does it follow that $\langle \ , \ \rangle_1=\langle \ , \ \rangle_2$ ?","['matrices', 'inner-products', 'linear-algebra']"
4608219,Can Peano's 9th axiom be expressed using a self-referential set definition?,"According to Wikipedia, the 9th axiom I am asking about is this . I am wondering, is the following a valid way of expressing the same idea? If not, why not, and what would be the proper self-referential expression (if there is any)? If $K = \{0\} \cup \{S(k)\mid k \in K\}$ , then $K=\mathbb{N}$ .","['elementary-set-theory', 'induction', 'definition']"
4608296,Correspondence between $H^1_{dR}$ and harmonic 1-forms,"Let $S$ be a compact oriented Riemannian surface. And let $\mathcal{H}^1(S)$ be the space of the harmonic $1$ -forms of $S$ . I’m trying to prove that there is a linear isomorphism: $$H^1_{dR}(S)\to  \mathcal{H}^1(S).$$ By Hodge decomposition theorem, given a $1$ -form $\omega$ : $$[\omega]=[\omega’+d\eta+\delta \varphi]= [\omega’+\delta \varphi]$$ where $\omega’$ is harmomic. The proof will be basically complete if we manage to prove that $\delta \varphi=0$ . By Hodge decomposition theorem it suffices to prove that $d \delta \varphi=0$ but I don’t know how to prove this.","['hodge-theory', 'riemannian-geometry', 'de-rham-cohomology', 'homology-cohomology', 'differential-geometry']"
4608298,Existence of the complement of product of two normal subgroups having complement,"Let $G$ be a group. If $H\le G$ , $G=HK$ and $H\cap K=1$ , then $K$ is called a complement of $H$ in $G$ . In general complements do not exist. For example, the center of a quaternion group $Q$ does not have a complement since there is only one involution in $Q$ ; also, nontrivial proper subgroups of cyclic groups of prime power order do not have complements. There are some nice theorems ensuring the existence of complement under some conditions, like the one of Schur-Zassenhaus and the one of Gaschütz. When two normal subgroups both have their complements, it can be seen that $N_1N_2$ does not necessarily has complement. But it seems that the complement exists provided that some additional information is given, for example one the two normal subgroups is contained in a complement of another subgroup. So the following question arises: Let $N_1$ and $N_2$ be normal subgroups of $G$ . If $N_i$ has a complement $L_i$ ( $i=1,2$ ) such that $N_2\le L_1$ , then also $N_1N_2$ has a complement. Is the statement true? Any help is appreciated.","['group-theory', 'abstract-algebra', 'finite-groups']"
4608317,Almost Dominated Convergence Exercise (not Generalized Dominated Convergence Theorem),"I just had a question regarding an exercise from Tao's Intro to Measure Theory (Exercise 1.4.46). The question is as follows: Let $(X, B, \mu)$ be a measure space, and let $f_1, f_2, ...: X \to C$ be a sequence of measurable functions that converge pointwise $\mu-a.e.$ to a measurable limit $f: X \to C$ . Suppose that there is an unsigned absolutely integrable function $G, g_1, g_2, ... : X \to [0, +\infty]$ such that $|f_n|$ are pointwise a.e.  bounded by $G + g_n$ , and that $\int_X g_n d\mu \to 0$ as $n \to \infty$ . Show that $\lim_{n\to\infty} \int_X f_n d\mu \to \int_X f d\mu$ . I've seen the generalized Dominated Convergence Theorem before, but I'm having some serious trouble trying to figure out how to proceed, since the convergence of the g_n integrals to zero only implies the convergence of g_n in measure, and doesn't directly imply anything regarding the pointwise convergence of g_n (I'm thinking about the typewriter function). My current direction is that I'd like to find something out regarding the pointwise convergence of the g_n's so that I appropriately use Fatou's lemma. So I suppose my questions are, is there a way to show that the g_n's converge pointwise to 0? Considering the assumptions in the question, I feel like any sequence of functions bounded by another sequence of functions whose integrals converge to zero, but do not converge pointwise, cannot itself converge pointwise. If this is not the right direction, .. is convergence in measure supposed to be enough? If both of these are not the right direction, please provide a small hint as to the correct direction... thanks!","['integration', 'measure-theory', 'real-analysis', 'functional-analysis', 'limits']"
4608337,How to show two equivalent projection in a $C^*$ algebra are not homotopic,"Show that two equivalent projections need not be homotopic. HINT: Let $P=\begin{pmatrix} 1&0\\0&0\end{pmatrix}$ and $Q=\begin{pmatrix}
        t&\sqrt{t(1-t)}\\\sqrt{t(1-t)}&1-t
    \end{pmatrix}\in M_2(C[0,1])$ be two projections. Show they're contained in a common $C^*$ subalgebra of $M_2(C[0,1])$ in which they're equivalent but not  homotopic. My attempt: for $X=\begin{pmatrix}
        \sqrt{t}&\sqrt{1-t}\\0&0
    \end{pmatrix}$ we have that: $$XX^*=\begin{pmatrix}
        \sqrt{t}&\sqrt{1-t}\\0&0
    \end{pmatrix}\cdot \begin{pmatrix}
        \sqrt{t}&0\\\sqrt{1-t}&0
    \end{pmatrix}=\begin{pmatrix}
        1&0\\0&0
    \end{pmatrix}=P$$ and $$X^*X=\begin{pmatrix}
        \sqrt{t}&0\\\sqrt{1-t}&0
    \end{pmatrix}\cdot \begin{pmatrix}
        \sqrt{t}&\sqrt{1-t}\\0&0
    \end{pmatrix}=\begin{pmatrix}
         t&\sqrt{t(1-t)}\\\sqrt{t(1-t)}&1-t
    \end{pmatrix}=Q $$ so $P,Q\in C^*(X)$ are equivalent. However I'm not sure how to show they're not homotopic as projections in $C^*(X)$ . Any help would be appreciated.","['c-star-algebras', 'equivalence-relations', 'matrices', 'homotopy-theory', 'projection-matrices']"
4608452,Any better way to find $\cot \left(10^{\circ}\right)+\cot \left(70^{\circ}\right)-\cot \left(50^{\circ}\right)$,"Find the value of $\cot \left(10^{\circ}\right)+\cot \left(70^{\circ}\right)-\cot \left(50^{\circ}\right)$ My Method: I used the following Identities: \begin{aligned}
& \sin A \cos B-\cos A \sin B=\sin (A-B) \\
& 2 \sin A \sin B=\cos (A-B)-\cos (A+B) \\
& \cos (2 A)=2 \cos ^2 A-1 \\
& \cos (3 A)=4 \cos ^3 A-3 \cos A \\
& \sin (2 A)=2 \sin A \cos A \\
& 2 \sin A \cos B=\sin (A+B)+\sin (A-B) \\
&
\end{aligned} $$\begin{aligned}
	S_1 & =\cot \left(10^{\circ}\right)+\cot \left(70^{\circ}\right)-\cot \left(50^{\circ}\right)=\cot(10^{\circ})-\cot \left(50^{\circ}\right)+\tan(20^{\circ}) \\
\\
	\Rightarrow S_1 & =\frac{\cos \left(10^{\circ}\right)}{\sin \left(10^{\circ}\right)}-\frac{\cos \left(50^{\circ}\right)}{\sin \left(50^{\circ}\right)}+\frac{\sin \left(20^{\circ}\right)}{\cos \left(20^{\circ}\right)} \\
\\
\\
	\Rightarrow S_1 & =\frac{2 \sin \left(40^{\circ}\right)}{2 \sin \left(10^{\circ}\right) \sin \left(50^{\circ}\right)}+\frac{\sin \left(20^{\circ}\right)}{\cos \left(20^{\circ}\right)} \\
\\
	\Rightarrow S_1 & =\frac{2 \sin \left(40^{\circ}\right)}{\cos \left(40^{\circ}\right)-\frac{1}{2}}+\frac{\sin \left(20^{\circ}\right)}{\cos \left(20^{\circ}\right)} \\ \\
	\Rightarrow S_1 & =\frac{4 \sin 40^{\circ}}{4 \cos ^2\left(20^{\circ}\right)-3}+\frac{\sin \left(20^{\circ}\right)}{\cos \left(20^{\circ}\right)}=\frac{6 \sin \left(40^{\circ}\right) \cos \left(20^{\circ}\right)-3 \sin \left(20^{\circ}\right)}{0.5}
\end{aligned}$$ $$\begin{aligned}
	& \Rightarrow S_1=\frac{3}{0.5}\left(2 \sin \left(40^{\circ}\right) \cos \left(20^{\circ}\right)-\sin \left(20^{\circ}\right)\right) \\ \\
	& \Rightarrow S_1=6\left(\sin \left(60^{\circ}\right)\right)=3 \sqrt{3}
\end{aligned}$$","['trigonometry', 'algebra-precalculus', 'geometry']"
4608478,"Find $\,\lim\limits_{n\to \infty}a_n\,$ such that $\,a_n=\left\lceil{\frac{2\pi}{a_{n-1}}}\right\rceil\!\cdot\!a_{n-1}-2\pi$ .","Question : Let $\{a_n\}$ be a real sequence such that $\,a_1=2\pi-6\;$ and $\;a_n=\left\lceil{\dfrac{2\pi}{a_{n-1}}}\right\rceil\!\cdot\!a_{n-1}-2\pi\;,$ where $\left\lceil\cdot\right\rceil\;$ is the smallest integer not less than $\dfrac{2\pi}{a_{n-1}}$ . Then $\;\lim\limits_{n\to\infty}a_n=\ldots$ $\{a_n\} $ is a monotonically decreasing sequence. I presumed that if $\,\lim\limits_{n\to \infty} a_n=l\,,\,$ i.e. convergent, then $l=\left\lceil\dfrac{2\pi}{l}\right\rceil\!\cdot\!l-2\pi\implies l\left(1-\left\lceil\dfrac{2\pi}{l}\right\rceil\right)=-2\pi$ . Afterwards I was looking for the possible solutions and found $l=-2\pi^+,\,$ i.e. $\,-2\pi\,$ from the right hand. I think this is not enough to say that the limit is $\,-2\pi^+.$ I am looking for a better solution to this as I have used a graphing calculator for the same, so my reasoning is incomplete. Thanks.","['limits', 'sequences-and-series']"
4608479,"Why can’t Pythagorean triples of this form exist, $\{a, b, c\}$, $\{2a, d, c\}$?","Why can’t there exist two Pythagorean triples with the same hypotenuse, but one has a leg that is twice as long as a leg of the other?  Or why is there no integer solution to $a^2 + b^2 = c^2$ $4a^2 + d^2 = c^2$ $a$ must be the even leg. The existence would imply that $c - 2a$ , $c - a$ , $c + a$ , $c + 2a$ are all squares.","['number-theory', 'pythagorean-triples', 'elementary-number-theory']"
4608591,Are (twisted) Edwards curves actually groups?,"Usually, an elliptic curve is defined to be a smooth cubic curve in $\mathbb{P}^2$ . But twisted Edward curves defined by $$
ax^2 + y^2 = 1 + dx^2y^2
$$ over fields of characteristics $\neq 2$ , have degree 4, and they have singularities at infinity: $(0:1:0)$ and $(1:0:0)$ . I know that a twisted Edwards curve is birationally equivalent to a smooth cubic curve in Montgomery form. But birational equivalence does not equal isomorphism. Is a twisted Edwards curve including points at infinity a group and why? If it's not, which part of it forms a group and why?","['algebraic-geometry', 'elliptic-curves']"
4608601,"If $\mu$ is $\sigma$-finite, then the $\sigma$-algebra of all $\mu^*$-measurable sets coincides with the completed $\sigma$-algebra","Let $(X, \mathcal X, \mu)$ be a measure space. Let $\mu^*$ be the outer measure induced from $\mu$ , i.e., $$
\mu^*(A):=\inf \left\{ \sum_{n=1}^{\infty} \mu (B_n) : (B_n) \subset \mathcal X , A \subset \bigcup_n B_n \right\} \quad \forall A \subset X.
$$ Let $\mathcal{M}$ be the collection of all $\mu^*$ -measurable sets, i.e., those sets $A \subseteq X$ such that $$
\mu^*(E)=\mu^*(E\cap A) +\mu^*(E\cap A^c) \quad \forall E\subset X.
$$ Let $(X, \overline{\mathcal X}, \overline \mu)$ be the completion of $(X, \mathcal X, \mu)$ .   Then $\mathcal M$ is a $\sigma$ -algebra on $X$ , $\mu^*|_{\mathcal M}$ is a complete measure , $\mathcal X \subset \overline{\mathcal X} \subset \mathcal M$ , $\overline \mu = \mu^*|_{\overline{\mathcal X}}$ , and $\mu = \overline \mu |_{\mathcal X}$ . I would like to prove a result mentioned in this answer , i.e., Theorem If $\mu$ is $\sigma$ -finite then $\overline{\mathcal X} = \mathcal M$ . Could you have a check on my attempt? Proof Fix $A \in \mathcal M$ . Let's prove that $A \in \overline{\mathcal X}$ . Let $\mathcal N$ be the collection of all subsets of $\mu$ -null subsets of $X$ , i.e., $$
\mathcal N := \{A \subset X :\exists N \in \mathcal X \text{ such that } A \subset N \text{ and } \mu (N)=0\}.
$$ Then $A \in \overline{\mathcal X}$ if and only if $A = B \cup C$ for some $B \in \mathcal X$ and $C \in \mathcal N$ . $\mu$ is finite. Because $\mu^* (A) \le \mu^*(X) = \mu (X) < \infty$ , for each $n \in \mathbb N$ there is a sequence $(B_{nm})_m \subset \mathcal X$ such that $A \subset \bigcup_m B_{nm}$ and $\mu^* (A) > \sum_{m=1}^{\infty} \mu (B_{nm}) - \frac{1}{n}$ . Let $B := \bigcap_n \bigcup_m B_{nm}$ . Then $A \subset B \in \mathcal X$ and $\mu^* (A) = \mu (B)$ . It follows from $\mu^* (A) < \infty$ that $\mu^* (A')=0$ with $A' := B \setminus A$ .  With similar reasoning, there is $B' \in \mathcal X$ such that $A' \subset B'$ and $\mu^* (A') = \mu (B')$ . It follows that $A' \in \mathcal N$ . We have $$
A^c := X \setminus A = (X \setminus B) \cup A'.
$$ It follows that $A^c \in \overline{\mathcal X}$ and thus $A \in \overline{\mathcal X}$ . $\mu$ is $\sigma$ -finite. There is a sequence of $(X_n)$ of pairwise disjoint sets in $\mathcal X$ such that $\bigcup_n X_n = X$ and $\mu (X_n) < \infty$ for all $n$ . Let $\mathcal X_n$ be the sub $\sigma$ -algebra that $\mathcal X$ induces on $X_n$ , i.e., $\mathcal X_n := \{A \cap X_n : A \in \mathcal X\}$ . Let $\mu_n$ be the restriction of $\mu$ to $\mathcal X_n$ , i.e., $\mu_n (A) := \mu (A)$ for all $A \in \mathcal X_n$ . We construct the corresponding objects $(\mu^*_n, \mathcal M_n, \overline{\mathcal X_n}, \overline{\mu_n})$ from the measure space $(X_n, \mathcal X_n, \mu_n)$ . Clearly, $\mu_n$ is finite. We need the following lemmas, i.e., Lemma 1 $\mathcal M_n = \{A \cap X_n : A \in \mathcal M\}$ . Lemma 2 $\overline{\mathcal X_n} = \{A \cap X_n : A \in \mathcal{\overline X}\}$ . Let $A_n := A \cap X_n$ . By Lemma 1 , $A_n \in \mathcal M_n$ . By (1.), $\overline{\mathcal X_n} = \mathcal M_n$ . So $A_n \in \overline{\mathcal X_n}$ . By Lemma 2 , $\overline{\mathcal X_n} \subset \mathcal{\overline X}$ . This implies $A_n \in \mathcal{\overline X}$ for all $n$ . On the other hand, $A = \bigcup_n A_n$ . It follows that $A \in \mathcal{\overline X}$ . This completes the proof.",['measure-theory']
4608615,Why isn't the directional derivative with respect to $-\vec v$ the same as the directional derivative with respect to $\vec v$?,"Suppose $f$ is a differentiable function from $\mathbb R^2$ to $\mathbb R$ , $\vec v$ is a unit vector in $\mathbb R^2$ , and $\vec a$ is a point in $\mathbb R^2$ . Then, visually, taking the directional derivative of $f$ with respect to $\vec v$ at $\vec a$ represents taking the vertical plane containing $\vec v$ , translating it to $\vec a$ , intersecting it with the graph of $f$ to obtain a curve, and then finding the slope of the tangent line to that curve at $\vec a$ . But vertical plane containing $\vec v$ is the same as the vertical plane containing $-\vec v$ , so why isn't the directional derivative of $f$ with respect to $-\vec v$ at $\vec a$ equal to the directional derivative of $f$ with respect to $\vec v$ at $\vec a$ ?","['partial-derivative', 'multivariable-calculus', 'derivatives']"
4608653,"Space with subtraction, but no addition","Is there a name for a mathematical space with a distance and ordering defined, and a subtraction operation defined, but no addition operation or scalar multiplication?  Essentially this is like the real number line with no notion of an origin. The concrete example I'm thinking of is datetimes; given two datetimes, we can subtract them to find an interval.  We can add/subtract an interval to/from a datetime, or multiply an interval by a scalar, but we can't add two datetimes or multiply a datetime by a scalar. So if d1 and d2 are datetimes, then d1 + (d1 - d2) has a well-defined meaning, but (d1 + d1) - d2 does not. I'm also just realizing - the result of subtraction is not an element in the same space, so I guess this is a family of two sets $A$ and $B$ , where $a_i - a_j \in B$ $a_i \pm b_j \in A$ $b_i \pm b_j \in B$ $k \cdot b_i \in B$",['abstract-algebra']
4608676,Approximating the solution to a system of 3 oscillatory ODEs?,"ODE System I have the following system of ODEs: $x'(t)=x(t)\frac{z(t)}{Z}-x(t)\frac{x(t)+y(t)}{J}$ $y'(t)=y(t)\left(1-\frac{z(t)}{Z}\right)(1-q)-y(t)\frac{x(t)+y(t)}{J}$ $z'(t)=y(t)\left(1-\frac{z(t)}{Z}\right)(1-q)-mz(t)$ , where all variables and parameters are positive and $0<q<1$ . Background The system exhibits an equilibrium, $\left(\overline{x},\overline{y},\overline{z}\right)$ , where $\overline{x}$ , $\overline{y}$ , and $\overline{z}$ are positive when $J$ is greater than a critical value $J_{Crit}$ . The real parts of the eigenvalues $\left(\lambda_1,\lambda_2,\lambda_3\right)$ corresponding to $\left(\overline{x},\overline{y},\overline{z}\right)$ are all negative when $J>J_{Crit}$ . Upon further analysis of the eigenvalues, one finds that $\left(\overline{x},\overline{y},\overline{z}\right)$ always exhibits oscillations (i.e., two of the eigenvalues are complex). The answer to this post showed that the solution to such a 3-dimensional system is well-approximated by $\overline{V} + Ae^{\sigma t}\cos{\left(\omega t+ \phi\right)} + be^{\lambda_3t} \ \forall \ \overline{V} \in \left(\overline{x},\overline{y},\overline{z}\right)$ , where $A$ , $\sigma$ , $\omega$ , and $\phi$ give the oscillations' amplitude, decay rate, frequency, and initial phase, respectively. Both $A$ and $b$ are functions of initial conditions, which are $x(0)$ , $y(0)$ , $z(0)$ . Question In general, how does one calculate $b$ for a 3-dimensional ODE system like the one presented here?","['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4608724,Why is the class group torsion?,"While wandering through math stack exchange I found an interesting question, namely this one: Why are the algebraic integers a Bezout Domain? (Found here: Is there an elementary way to prove that the algebraic integers are a Bézout domain? ) Now, this follows easily from both the principal ideal theorem or by the fact that the class group of any ring of algebraic integers of a finite extension has torsion class group, as shown here: https://math.stackexchange.com/a/1303278/917010 .
This is a obvious consequence of the finiteness of the class number, but it is a much weaker fact, which I would like to prove indipendently. I've now tried for a few days to prove this without any success. Of course we must at some point use the fact that we are not dealing with any Dedekind ring but with one which is the integral closure of the integers in a finite extension of $\Bbb{Q}.$ (Which is actually what makes of this a number theory question.) But I have found no plausible way to do this. My motivation for this is that I am trying to understand Algebraic number theory and Class field theory, and such a proof would give a maybe clearer idea of the Bezout property. Of course the argument with the Minkowski bound is clear to me, but I must admit that it does not give me a great understanding of what is going on. In conclusion, and for the sake of clarity, I will re-state the question: Why is the class group torsion? (Without using the finiteness of the class group) I must thank you in advance for any effort. Worst case scenario, this will be a good a lesson on how strong is the theorem on the finiteness of the class group.","['algebraic-number-theory', 'number-theory', 'dedekind-domain', 'abstract-algebra', 'class-field-theory']"
4608733,linear function composition,"Find all functions mapping the positive integers to the positive integers such that $f^5(x)=kx$ for a positive integer $k$ . My approach to this problem consisted of trying out functions, as I cannot figure out a methodical way to approach such a problem. An example is when I tried functions such as $f(x)=x\sqrt[5]{k}$ and $f(x)=k/x$ , these do not give positive integers for all values of $x$ .","['functional-equations', 'functions']"
4608773,Question on Cauchy-distribution.,"Question: Let $X, Y$ be independent and Cauchy-distributed and define $Z:=$ $X+Y$ . Show that $Z / 2=\frac{X+Y}{2}$ is Cauchy-distributed. My attempt: I think that we can find the density of $Z$ first. For any fixed $a \in \mathbb{R}$ , the derivative of the function $$
h(y):= \begin{cases}\frac{1}{4 a+a^3}\left(\log \left(\left(1+y^2\right) /\left(1+(a-y)^2\right)\right)+\operatorname{aatan}(y)-a \operatorname{atan}(a-y)\right) & a \neq 0, \\ \frac{1}{4}\left(\frac{2 y}{1+y^2}+2 \operatorname{atan}(y)\right) & a=0,\end{cases}
$$ is given by $$
h^{\prime}(y)=\frac{1}{1+(a-y)^2} \frac{1}{1+y^2} .
$$ where atan $: \mathbb{R} \rightarrow(-\pi / 2, \pi / 2)$ is the inverse of the tangent function which satisfies $h(\infty)=\frac{\pi}{4+a^2}$ and $h(-\infty)=\frac{-\pi}{4+a^2}$ where these expression are meant as the respective limits. And if $X_1, \ldots, X_n$ are independent Cauchy-distributed random variables, then the average $\frac{1}{n} \sum_{k=1}^n X_k$ is Cauchy-distributed. But I'm not sure how to solve this question. Help would be appreciated. Thank you.","['probability-distributions', 'probability-theory', 'probability']"
4608789,"Asymptotics of $\int_0^\infty \frac{x^{2z}}{\Gamma(1+z)}\,dz$","I'm interested in the asymptotics of $$\int_0^\infty \frac{x^{2z}}{\Gamma(1+z)}\,dz$$ as $x\to\infty$ . I expect the results to behave similarly to $e^{x^2}=\sum_{k\ge 0}\frac{x^{2k}}{k!}$ . However, I'm not quite sure how to develop the leading asymptotics of the integrals. I first thought that for large $x$ , the integral should be dominated by its integral over a small ball around the maximum of the integrand $\frac{e^{2z\log x}}{\Gamma(1+z)}$ . To find this maximum, I computed $$f'(z)=\left(2\log(x)-\psi^{(0)}(1+z)\right)f(z)$$ with $\psi^{(0)}(z)$ the logarithmic derivative of $\Gamma(z)$ . Since $f$ never vanishes, the maximum must occur at $z\in(0,\infty)$ such that $\psi^{(0)}(1+z)=2\log x$ . Assuming that we will take $x\to\infty$ as well as the asymptotics $\psi(z)=\log z +O(1/z)$ for large and positive $z$ , we seek to solve $2\log x= \log(1+z) +O(1/z)$ . Exponentiating, we find that $x^2=1+z+O(1)$ . Thus, we have that $\operatorname{arg max} f(z)\sim x^2$ is asymptotically correct for large $x$ . Let $z_0=x^2$ . We can now rewrite the integral as dominated by $$\frac{e^{2x^2\log x}}{\Gamma(1+x^2)}\int_{z_0-\epsilon}^{z_0+\epsilon} e^{2(z-z_0)\log x} \frac{\Gamma(1+z_0)}{\Gamma(1+z)}\,dz.$$ However, I'm not sure what size to take $\epsilon$ as a function of $x$ . I do know that the expression outside of the integral is asymptotic to $$(2\pi)^{-1/2} \frac{e^{x^2}}{x}.$$ I'm not sure how to deal with the actual integral though. Input is much appreciated.","['integration', 'improper-integrals', 'definite-integrals', 'asymptotics']"
4608801,Some properties of the Fermat hypersurfaces $X_0^d + \cdots + X_{n+1}^{d}=0$,"Fix two integers $n \geq 1$ and $d \geq 2$ . Let $X_{n,d} \subset \mathbb{P}^{n+1}_k$ be the Fermat hypersurface defined by the equation $$
F_{n,d}: X_0^{d}+ \cdots + X_{n+1}^{d} = 0.
$$ One can check that it is nonsingular of dimension $n$ . In the exam ( EDIT : This is from an exam paper in 2017, so this post is not cheating), I was asked to consider the following problem: Question : Let $p \in X_{n,d}$ be any point. Show that $X_{n,d} \smallsetminus \{p\}$ is not projective. When is it affine? I managed to see why it is not projective, but got stuck on considering its affineness. My attempts : When $n \geq 2$ , the singleton $\{p\}$ has codimension $\geq 2$ in $X_{n,d}$ . Hence by the Hartog extension theorem , the inclusion $i: X_{n,d} \smallsetminus \{p\} \hookrightarrow X_{n,d}$ induces an isomorphism $$
i^{\sharp}: \Gamma(X_{n,d}, \mathcal{O}_{X_{n,d}}) \rightarrow \Gamma(X_{n,d} \smallsetminus \{p\}, \mathcal{O}_{X_{n,d} \smallsetminus \{p\}})
$$ so the global section of $X_{n,d} \smallsetminus \{p\}$ is $k$ . If it were affine, then its dimension could be calculated by the Krull dimension of the global section, hence its dimension would be zero, which is absurd. Hence it is not affine. However , when $n=1$ , the Hartog extension theorem is not applicable. Then I do not know what to do. When $n=1$ and $d=2$ , if we use the "" huge input "" that every nonsingular conic in $\mathbb{P}^2$ is isomorphic to $\mathbb{P}^1$ , then we see $X_{1,2} \smallsetminus \{p\}$ is affine, isomorphic to $\mathbb{A}^1$ . But how about the general degree $d > 2$ ? (And as for the ""huge input"", I do not want to use it since the only proof of it I know is to see both the nonsingular conic in $\mathbb{P}^2$ and $\mathbb{P}^1$ has genus zero, and all genus zero curves are isomorphic by Riemann-Roch. So can we get rid of this input or can we use a more elementary approach for this ""huge input""?) Thank you so much for answering and commenting! Happy new year!","['algebraic-curves', 'algebraic-geometry']"
4608805,How is it possible for the population mean to be known,"Suppose that I have a class of 35 students whose average grade is 90. I randomly picked 5 students whose average came out to be 85. Assume their grades are i.i.d and of normal $N(\mu, \sigma^2)$ . From the example I have seen, $\mu$ is usually called the population mean and should be equal to $90$ . The sample me is usually referred to as $\frac{\sum{X_i}}{5}$ . When we do hypothesis testing we can ask whether the sample mean is equal to $90$ . Is the sample mean $\frac{\sum X_i}{5}$ or $85$ ? The population mean mathematically should be $\mu$ , but I think people also say that $90$ is the population mean. This does not make sense to mean since it is not obvious to me that why $90 = \mu$ . $90$ is calculated through the sum of the grades divided by 35, whereas $\mu$ is equal to some integral. I just do not see how they can be equal to each other.","['statistics', 'probability-theory', 'probability']"
4608838,Proof for $I_0(1)=\int_{0}^{\pi}e^{\cos x}\ \mathrm{d}x<\frac{3\pi}{2}$.,"I want to prove $$\int_{0}^{\pi}e^{\cos x}\ \mathrm{d}x<\frac{3\pi}{2}.$$ Here I can provide a proof as follows: \begin{align*}
\int_{0}^{\pi}e^{\cos x}\ \mathrm{d}x
 &=\int_{0}^{\pi}\sum_{n=0}^{\infty}\frac{\cos^nx}{n!}\ \mathrm{d}x\\
 &=\sum_{n=0}^{\infty}\frac{1}{n!}\int_{0}^{\pi}\cos^nx\ \mathrm{d}x\\
 &=\sum_{n=0}^{\infty}\frac{1}{(2n)!}\int_{0}^{\pi}\cos^{2n}x\ \mathrm{d}x\\
 &=\sum_{n=0}^{\infty}\frac{2}{(2n)!}\int_{0}^{\frac\pi2}\cos^{2n}x\ \mathrm{d}x\\
 &=\pi+\sum_{n=1}^{\infty}\frac{2}{(2n)!}\cdot\frac{(2n-1)!!}{(2n)!!}\cdot\frac{\pi}{2}\\
 &=\pi\left(1+\sum_{n=1}^{\infty}\frac{1}{((2n)!!)^2}\right)\\
 &<\pi\left(1+\sum_{n=1}^{\infty}\frac{1}{2^{n+1}}\right)\\
 &=\frac{3\pi}{2}.
\end{align*} Is there a more elmentary and easy proof. I know that $$I_0(x)=\int_{0}^{\pi}e^{x\cos t}\ \mathrm{d}t$$ is the $0$ th modified Bessel function of the first kind.","['definite-integrals', 'sequences-and-series']"
4608850,Perspective image of a rectangle,"The image above shows a perspective image of a rectangle, taken by a pinhole camera that is pointed to the center of the rectangle (This means that the axis of the pinhole camera is assumed to pass through the center of the viewed rectangle, or equivalently that the line connecting the center of the rectangle and its image is perpendicular to the viewing plane). If the side length $\overline{AB}$ is known to be $6$ units, can we find the side length $\overline{BC}$ ? Additionally, assume that in the image $AB$ is not parallel to $CD$ and $BC$ is not parallel to $AD$ .","['projective-geometry', 'geometry']"
4608868,Green's First and Second Identities,"I was doing some research on potential theory and Green's identities and noticed that most of the literature I find on the subject tends to define the vector field $$\tag{1}\vec F=\phi\,\text{grad}(\psi)$$ where $\phi$ and $\psi$ are $C^2$ scalar fields. Question 1 :
If $\vec F$ is supposed to be a conservative field (i.e, curl-free), then wouldn't the $\phi$ term make the curl of the gradient non-zero and hence make $\vec F$ non-conservative? If potential theory is supposed to be built on conservative fields, then what is the purpose of the $\phi$ term? Are we just assuming some general form of $\vec F$ ? Question 2 : By plugging in $(1)$ into the Divergence theorem in $\mathbb R^3$ $$\iint_{\partial V}\vec F\cdot \hat n\;dS=\iiint_V\text{div}(\vec F)\;dV,$$ we can obtain $$\tag{G1}\iint_{\partial V}\phi\frac{\partial \psi}{\partial \hat n}\;dS=\iiint_V(\vec\nabla\phi\cdot\vec\nabla\psi+\phi\nabla^2\psi)\;dV,$$ which is known as Green's first identity. By interchanging $\phi$ and $\psi$ in $(1)$ (i.e, letting $\vec F=\psi\,\text{grad}(\phi)$ ), we can obtain $$\tag{G1'}\iint_{\partial V}\psi\frac{\partial \phi}{\partial \hat n}\;dS=\iiint_V(\vec\nabla\psi\cdot\vec\nabla\phi+\psi\nabla^2\phi)\;dV.$$ Subtracting equation $(G1')$ from $(G1)$ will then yield the famous Green's second identity $$\tag{G2}\iint_{\partial V}\bigg(\phi\frac{\partial \psi}{\partial \hat n}-\psi\frac{\partial \phi}{\partial \hat n}\bigg)\;dS=\iiint_V\big(\phi\nabla^2\psi-\psi\nabla^2\phi\big)\;dV.$$ My question: Why are we allowed to interchange $\phi$ and $\psi$ in $(1)$ ? Doesn't switching these terms results in a completely new vector field? I appreciate any and all clarification! :)","['multivariable-calculus', 'vector-analysis', 'partial-differential-equations', 'potential-theory', 'greens-function']"
4608878,Using Green's formula on closed curves,"I want to determine the following curve integral $$ \int_{\gamma} \vec{F} \cdot d\vec{r} \qquad,\qquad \vec{F}= \bigl( \sin(y-x) , 2xy+\sin(x-y) \bigr) \quad,\quad \gamma \quad : \quad y=\sqrt{x} \quad 0 \leq x \leq 1. $$ So I thought that Green's formula would be useful, and therefore we need a closed curve to use it, i choose the following So we get $$ \int_\gamma + \int_u + \int_v \quad= \quad \iint_K  \qquad , \quad \text{K: The inside of the closed curve}. $$ For the first line segment we get the following parameterization $$ \int_u \quad : x=1 \quad \rightarrow \quad dx=0 \quad : \quad \int_1^0 \sin(t-1) \cdot 0 + 2t+\sin(1-t) \, dt = [t^2+ \cos(1-t)]_1^0 \quad=\quad \cos(1)-2 $$ And the second $$\int_v \quad : y=0 \quad \rightarrow \quad dy=0 \quad : \quad \int_1^0 \sin(0-t) \cdot dt + (0 + \sin(t-0) \cdot 0  dt = [\cos(t)]_1^0 \quad=\quad 1 - \cos(1) $$ Applying Green's formula: $$ \iint_K \bigl( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \bigr) dxdy = \int_0^1 \int_0^{\sqrt{x}} 2y- \cos(x-y) + \cos(x-y) dxdy =$$ $$ \int_0^1 \int_0^{\sqrt{x}} 2y dxdy = \int_0^1 [y^2]_0^{\sqrt{x}} dx \quad=\quad \int_0^1 x dx \quad=\quad \frac{1}{2} $$ Finally we get: $$ \int_\gamma = \iint_K - \int_u - \int_v \quad=\quad \frac{1}{2} - \cos(1) + 2 - 1 + \cos (1) =  \frac{3}{2} $$ But if try another approach with the following closed curve I get for the line integral $$\int_W \quad : y=t \quad \rightarrow \quad x=t \quad : \quad \int_1^0 sin(t-t) \cdot dt + (2t^2 + sin(t-t) \cdot  dt = [\frac{2t^3}{3}]_1^0 = \frac{-2}{3} $$ Green's formula $$ \iint_K \bigl( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \bigr) dxdy = \int_0^1 \int_x^{\sqrt{x}} 2y dxdy  \quad=\quad \int_0^1 [y^2]_x^{\sqrt{x}} dx  = \int_0^1 x-x^2 dx  = \frac{1}{6} $$ So we get $$ \int_\gamma = \frac{1}{6} + \frac{2}{3} = \frac{5}{6} $$ Which is not the same answer! But I should get the same answer regardless of the curve I choose right? What did I go wrong here?","['integration', 'curves', 'multivariable-calculus', 'multiple-integral']"
4608960,"Create samples that are the same when unordered, but have some specific correlation when ordered","I have 100,000 random samples (with replacement) X_i from an otherwise unknown distribution. I want to create a series Y_i that is the same as X_i except for the ordering and gives a correlation between X and Y of some given number (or a correlation close to that given number). Is there a known technique/procedure/algorithm for this?","['statistics', 'probability-distributions', 'correlation']"
4608981,Find all $f$ such that $X\sim\mathcal{G}(\lambda) \;\Rightarrow\; f(X)\sim \mathcal{G}(\mu)$,"I found a nice problem recently, but could not come up with a solution: Find all functions $f:\mathbb{Z}_{\geq 0}\to\mathbb{Z}_{\geq 0}$ such that for all $0< \lambda < 1$ , if $X\sim G(\lambda)$ , then there exists $0<\mu < 1$ with $f(X) \sim G(\mu)$ . Thanks to the answers below, I was able to understand how to solve it! Spoiler: the solutions are all the functions $f_d:x\mapsto \lfloor x/d\rfloor$ for a fixed $d\geq 1$ .","['probability-theory', 'geometric-distribution', 'functional-analysis', 'probability']"
