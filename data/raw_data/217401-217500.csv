question_id,title,body,tags
4431555,Identify an inverse of 7 modulo 26.,"I was tasked to identify an inverse for 7 modulo 26 Here is what I have done: 26 = 7(3) + 5 7 = 5 (1) + 2 5 = 2 (2) + 1 2 = 1 (2) + 0 Working backward: 1 = 5 - 2 (2) 1 = 5 - 2 (7 - 1(5)) 1 = 3(5) - 2(7) 1 = 3( 26 - 3(7) ) - 2(7) 1 = 3(26) - 11(7) So the inverse is -11 However, I know this is wrong as my professor marked it wrong The professor's feedback was -11 + 26 = 15 therefore the inverse is 15. When I sent a message asking why we added 26 he did not respond, so here I am asking why the addition of 26? I missed this part in my readings. Thanks",['discrete-mathematics']
4431607,Does every infinite summation with real terms converge to a real number?,"The title says it all, I'm curious if there are convergent infinite series whose terms are all real but which converge to a value that is not real. I want to phrase this as ""are the reals closed under infinite summation"" but I'm sure someone would on this site would fuss about that phrasing. Two facts that I know make me wonder this: First, not all series which are defined on rational numbers converge to a rational number. For example, the Basel problem wherein the sum of the squares of the reciprocal of natural numbers involves pi. Second, I know that there are types of numbers on the number line that are not reals. The surreal numbers are the only case I know of, not to say there are not others. These two facts together make me wonder if the reals can be used in this way (infinite summation) to define non-real numbers.","['summation', 'sequences-and-series']"
4431613,Adding two infinite shapes to get itself?,"This is at first a ridiculous question but on the other hand I'm not sure how to prove it. Take a solid 2D shape, $A$ , and duplicate it to give $A'$ . The shape $A$ together with the $A'$ make a new region called $B$ . Can we take the shape $A$ and using only translation and rotation make it fill exactly the region $B$ ? (That is to say it now covers the original position of $A$ and it's copy $A'$ ) Obviously if $A$ is a shape of finite extent and area this would be impossible because we are asking a shape to cover twice it's own area. But it is not so clear to me that this would be impossible if $A$ were an infinite shape, perhaps some sort of infinite wedge whose thickness follows an exponential. Another way of saying the same thing is can we split a shape $B$ into two parts such that they both are translated, rotated copies of the original shape $B$ ? If not, how can we provide a proof? (By the way without using such obscure things as the Banach–Tarski paradox. ). I can see how this might be done with a fractal (such as Sierpiński triangle) but I am thinking about solid shapes of fractal dimension 2.","['geometry', 'infinity']"
4431625,"Suppose an integer x is a square modulo every prime, show that x is a square integer",I was looking at Is every non-square integer a primitive root modulo some odd prime? (the answer by Hagen von Eitzen) but its a bit too concise so im having a hard time understanding it. Could someone expand/rewrite his answer or offer another one. In particular im not sure why $(\frac{a}{b}) =1$ for almost all primes why its determind by $q$ mod $8b$ confused on the CRT portion,"['number-theory', 'quadratic-residues']"
4431637,The trace theorem for functions in $H^{1/2}(\Omega)$,"In my textbook, it said that we have the trace operator on Sobolev space like this: (Suppose $\Omega$ is a nice domain in $R^d$ ) \begin{equation*}
   H^{s}(\Omega) \hookrightarrow H^{s-\frac{1}{2}}(\partial \Omega), \forall s > \frac{1}{2}
\end{equation*} I'm wondering if it is still valid in the case $s = \frac{1}{2}$ , where we consider $H^0(\Omega)$ as $L^2(\Omega)$ . I think it will have counterexample for this one, while I can not find any. And I'm also looking for a proof of the following problem: \begin{equation*}
\text{
There exists $C>0$ s.t. for any $f \in H^1(\Omega)$,we have }
 \|f\|_{L^2(\partial \Omega)}^2 \leq C \|f\|_{L^2(\Omega)} \|f\|_{H^1(\Omega)} \end{equation*} This problem is related to the previous problem in the following sense: suppose the trace theorem holds for $s = \frac{1}{2}$ , then the left hand side is controlled by the $H^{1/2}$ norm, and the results can be proved by interpolation inequality on Sobolev Space.","['fractional-sobolev-spaces', 'sobolev-spaces', 'functional-analysis', 'trace-map', 'inequality']"
4431643,"Give an example of three functions $f,g,h$ such that $h \circ g\circ f$ is a bijection, but $g$ is neither injective or surjective.","Give an example of three functions $f:A\rightarrow B, g:B\rightarrow C, h:C\rightarrow D$ such that $h \circ g\circ f$ is a bijection, but $g$ is neither injective or surjective. I know that in order for the composition to be bijective, this means that every domain element of $f$ must be mapped to exactly one element of the range of the whole composition, and also that every range element of the composition must be paired with some domain element. I have tried and failed to find an example. My first try was let $f:\mathbb{N}\rightarrow\mathbb{N}, g:\mathbb{N}\rightarrow\mathbb{N},h:\mathbb{N}\rightarrow\{1\},$ $f(x)=x, g(x)=1, h(x) =1$ . In this scenario then the domain of $f$ is all natural numbers and they all just get mapped to themselves under $f$ . Then, once you put those all into $g$ , the output is just 1 - this means that $g$ is not one-to-one. Also, if $g:\mathbb{N}\rightarrow\mathbb{N}$ , then it is also not surjective because 1 is the only element of the range that gets obtained. So this is okay so far. Finally, under $h$ , the output from $g$ gets mapped to 1, which is the entire range of $h$ . So I think this means this is surjective. However, then it fails to be injective because every element we put into $f$ at the beginning ends up just being mapped to 1 at the very end so this did not work. My other thought was let $f:\mathbb{R}\rightarrow\mathbb{R},g:\mathbb{R}\rightarrow\mathbb{R},h:\mathbb{R+}\rightarrow\mathbb{R+},$ where $f(x)=x,g(x)=x^2 ,h(x)=x$ . This way, $g$ fails to be injective and surjective because it does not obtain every value of the real numbers and also more than one input gets mapped to the same output. So that gets satisfied. All of these results get mapped to themselves under $h$ , seeming to make it surjective. However, I run into the same issue where the inputs of $f$ don't end up being mapped to unique values under $h$ . Any suggestions are welcome. Thank you in advance.",['functions']
4431786,General definition of area in calculus,"In Cartesian coordinate system, I know that we define the area as: \begin{equation}
A_1= \iint dy\ dx
\end{equation} But I already know that the area under a certain curve is: \begin{equation}
A_2= \int f(x) \ dx = \int y \ dx
\end{equation} or even between two curves so $(y_1-y_2)$ isnted of $y$ . In some books, I configure this relation: \begin{equation}
A_3 = \int x dy 
\end{equation} What is then the difference between the 3 forms of area?","['integration', 'calculus', 'area']"
4431816,Find $\mathbf{A}$ minimizing $\lVert \mathbf{A}^{\top}\mathbf{AX}-\mathbf{X} \rVert _{F}^{2}$ with a given $\mathbf{X}$,"$1.$ Problem: There are three subproblems: $(1.1)$ Given a data matrix $\mathbf{X}\in \mathbb{R}^{N\times D}$ , find a matrix $\mathbf{A}\in \mathbb{R}^{M\times N} (M\ll N)$ satisfying: $$
\mathbf{A}=\underset{\mathbf{A}}{\arg\min}\lVert\mathbf{A}^\top\mathbf{A}\mathbf{X}-\mathbf{X}\rVert_F^2.
$$ $(1.2)$ Given a data matrix $\mathbf{X}\in \mathbb{R}^{N\times D}$ , find a bipolar matrix $\mathbf{B}\in \mathbb{Z}^{M\times N} (M\ll N)$ and a scaling factor $\alpha\in\mathbb{R}$ satisfying: $$
\mathbf{C}=\underset{\mathbf{C}}{\arg\min}\lVert\mathbf{C}^\top\mathbf{C}\mathbf{X}-\mathbf{X}\rVert_F^2,~~~~\text{s.t.}~\mathbf{C}=\alpha\mathbf{B},
$$ $~~~~~~$ where each element of $\mathbf{B}$ can only be choosed from $\{-1,+1\}$ . $(1.3)$ Based on $(1.2)$ , could you please get an optimal binary matrix $\mathbf{C}$ ( i.e. , each element of $\mathbf{B}$ can only be choosed from $\{0,1\}$ )? $2.$ Background: (Compressed Sensing) $(2.1)$ I project an $N$ -dimensional dataset (with $D$ data points) into an $M$ -dimensional space by $\mathbf{Y}=\mathbf{AX}$ and then reconstruct it by $\mathbf{\hat{X}}=\mathbf{A^\top Y}=\mathbf{A^\top AX}$ . I want to get an optimal $\mathbf{A}$ such that the $\ell_2$ -error between $\mathbf{X}$ and $\mathbf{\hat{X}}$ is minimized. $(2.2)$ For higher compression efficiency and lower memory complexity of $\mathbf{A}$ , I want to get a bipolar $\mathbf{C}$ that may works well or close to floating-point $\mathbf{A}$ . $3.$ My Efforts: $(3.1)$ I think I can solve subproblem $(1.1)$ by singular value decomposition (SVD) of $\mathbf{X}=\mathbf{U\Sigma V^\top}$ with singular values $\sigma_i=\mathbf{\Sigma}_{i,i}$ satisfying $\sigma_1\ge \sigma_2\ge\cdots\ge \sigma_N$ . The optimal $\mathbf{A}$ should be formed by the first $M$ rows of $\mathbf{U}^\top$ [ Reference ]. $(3.2)$ For subproblem $(1.2)$ , I guess that the optimal scaling factor $\alpha=1/\sqrt{N}$ since the optimal $\mathbf{C}$ should be row-normalized, and I do not know how to get the optimal $\mathbf{B}$ . $(3.3)$ I guess that I can get a bipolar $\mathbf{B}$ by discretizing each element of $\mathbf{A}$ with $$
b_{i,j}=f\left( a_{i,j} \right) =\left\{ \begin{array}{l}
	-1,\ a_{i,j}<0\\
	1,\ a_{i,j}\ge 0\\
\end{array} \right.,
$$ $~~~~~~~~~$ which may be better than random ones (by Bernoulli sampling) but not optimal? $4.$ Some Other Minor Questions: $(4.1)$ I do not know if I can directly write that “ $\mathbf{B}\in\{-1,+1\}^{M\times N}$ ”?","['convex-optimization', 'matrices', 'linear-algebra', 'discrete-mathematics', 'optimization']"
4431885,Probability to get 170 1's,"Suppose we throw a fair die 1000 times. What is the probability we get 170 1's? The exact number is $P={1000\choose 170}\bigg(\frac{1}{6}\bigg)^{170}\bigg(\frac{5}{6}\bigg)^{830}$ . We can approximate with normal distribution because we have independent events, a large and constant number of trials and an a-priori known probability with only two outcomes, success (get an 1) or failure (get anything else). We have $μ = np = 1000 \cdot\frac{1}{6} = 166.66$ and $σ = \sqrt npq = 11.785$ . $z = \frac{170 - 166.66}{11.785} = 0.2828$ Hence the probability is 0.6103.
Is it correct? Thank you very much.","['normal-distribution', 'probability']"
4431886,How to solve discrete double summation consisting floor function,"I am new to math stack exchange and it is my first question. I know how to solve discrete double summation without floor function. Main problem : $$\sum_{x=1}^{\lfloor(n-1)/7\rfloor} \sum_{y=1}^{\lfloor(n-x)/(6x-1)\rfloor}(6x-1)(6y+1) \qquad, \{n \in \mathbb{Z^+} \}$$ After simplification to single summation, it comes out to be: $$\sum_{x=1}^{\lfloor(n-1)/7\rfloor} \sum_{y=1}^{\lfloor(n-x)/(6x-1)\rfloor}(6x-1)(6y+1)$$ $$=\sum_{x=1}^{\lfloor(n-1)/7\rfloor}(6x-1) \sum_{y=1}^{\lfloor(n-x)/(6x-1)\rfloor}(6y+1)$$ $$=\sum_{x=1}^{\lfloor(n-1)/7\rfloor}(6x-1)\left(3\left\lfloor\frac{(n-x)}{(6x-1)}\right\rfloor\left(\left\lfloor\frac{(n-x)}{(6x-1)}\right\rfloor+1\right)+\left\lfloor\frac{(n-x)}{(6x-1)}\right\rfloor\right)$$ $$=\sum_{x=1}^{\lfloor(n-1)/7\rfloor}(6x-1)\left(3\left\lfloor\frac{n-x}{6x-1}\right\rfloor^2\ +\ 4\left\lfloor\frac{n-x}{6x-1}\right\rfloor\right)$$ But after this I am stuck. Discrete Math is college level math and I am a high school student.(not brilliant as other students) So i don't know where to find materials for discrete sums with floor functions.","['summation', 'discrete-mathematics', 'ceiling-and-floor-functions']"
4431900,Space of bounded continuous functions complete [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Prove that the space of bounded continuous functions $C^0(F,X):=\{f: F\rightarrow X : f$ is continuous and bounded $\}$ with $X$ Banach space, $F\subset \mathbb{R}$ is complete with the sup norm. I have seen many proves of this for $X=\mathbb{R}$ but how do I do this for an arbitrary Banach space? My idea was to show that a Cauchy sequence $(f_n)_n\in C^0(F,X)$ is also a Cauchy sequence in $X$ and since $X$ is complete it follows $u_k \rightarrow u \in X$ . Now I need to show that $u$ is continuous and bounded. Problem: for $X=\mathbb{R}$ I can use uniformly convergence and the uniform limit theorem but I don't know if these theorems apply for any arbitrary Banach space? How do I prove this statement? Thanks you!","['banach-spaces', 'functional-analysis']"
4431902,Gauss map on trace of curve,"Suppose we have a curve $\gamma : I \to S$ where $I \subseteq \mathbb{R}$ is an interval and $S$ is a surface in $\mathbb{R}^3$ (not necessarily orientable). We know that the Gauss map $N$ can be locally defined at all points of $S$ , but there is not necessarily a way to define it continuously on the whole surface. Can we define a map $N_I : I \to S^2$ such that $N_I(t)$ is normal to the surface $S$ at $\gamma(t)$ ? Notice that $N_I$ is not necessarily $N|_{\gamma(I)}$ , since for two points $t_0, t_1$ with $\gamma(t_0) = \gamma(t_1)$ it is possible that $N_I(t_0) \neq N_I(t_1)$ (indeed, this must happen necessarily if the manifold is non orientable). This question came to me as a variation of the problem 2.6.6 of Do Carmo Differential Geometry of Curves and Surfaces .","['differential-geometry', 'geometry', 'real-analysis']"
4431929,"Quadratic first order recurrence relation, is there a solution?","I am trying to find a solution for $a_n$ , writing $a_n$ as a function of $n$ ,
according to the following recurrence relation: $$a_n=3.9*a_{n-1}(1-a_{n-1}) ;   a_0 = \frac{1}{2}$$ . I have tried expanding, substitution, and looking at the ratio.
By doing linear substitution such as: $$a_n = \frac{1}{2} + b_n$$ . a simpler form is found: $$b_n = \frac{1.9}{4}-3.9b_{n-1}^2; b_0 = 0$$ I tried to expand the expression and hit a wall. I also ran it in Wolframalpha, giving no solution, with this initial condition its easy to see that $0 < a_n < 1$ for all $n$ . My question is, is there a solution? if so, how do I find it, and if there isn't, can it be proven? Thanks!","['recurrence-relations', 'discrete-mathematics']"
4431999,$\iint (x+y) dx dy $ What is my mistake?,"Question Solve the indefinite integral: $\iint (x+y) dx dy $ Attempt When calculating this indefinite double integral, I would first start with x and then y such that my solution would be: $\frac{1}2x^2 y + \frac{1}2y^2 x + c_1y + c_2$ But Wolfram Alpha's Solution is: $\frac{1}2x^2 y + \frac{1}2y^2 x + c_1x + c_2$ What am I doing wrong?
Thanks in advance!","['integration', 'calculus', 'analysis']"
4432021,Showing $\arcsin(z)-i\operatorname{Log}(-iz)-i\log 2 \in H^2(\mathbb{H})$,"In order to prove a result about Hilbert transforms, I need to show the complex function $F(z) = \arcsin(z)-i\operatorname{Log}(-iz)-i\log 2$ lies in $H^2(\mathbb{H})$ , the Hardy Space for the upper half plane. In other words, I need to show that \begin{equation}
\sup_{y\in(0,\infty)}\int_{-\infty}^{\infty}|F(x+iy)|^2\,dx < \infty.
\end{equation} However, the nature of the function makes this very fiddly. I've had two main thoughts so far. (1) $\arcsin(z) = -i\operatorname{Log}(iz + \sqrt{1-z^2})$ , so with some more algebra bashing, we can write $F(z) = i\operatorname{Log}\left(\frac{1}{2} + \frac{i}{2}\frac{\sqrt{1-z^2}}{z}\right)$ , which is a significantly nicer form and highlights how $F(z) \to 0$ as $z \to \infty$ in $\mathbb{H}$ . It doesn't seem nice enough to bound however. (2) If you fix $x \in \mathbb{R}$ , it seems as if $|F(x+iy)|$ increases as $y$ decreases to $0$ from above. If I could prove this, I'd be done, as then we could bound $\sup_{y\in(0,\infty)}\int_{-\infty}^{\infty}|F(x+iy)|^2\,dx$ by $\int_{-\infty}^{\infty}|F(x)|^2\,dx$ which certainly is finite. However, it's proving very difficult to work with $|F(x+iy)|$ as a function of $y$ . Supposedly, this result is ""straightforward to prove"" so I'm hoping someone has a nice way of proving this result.","['integration', 'complex-analysis', 'complex-integration', 'hardy-spaces']"
4432032,Finding closed form expression for $n^{th}$ term of sequence with generating function $F(x)$?,"I have been asked the above question regarding the generating function $$F(x) = \frac{x^2(1-x)}{(1-x)^3}$$ I have no idea what procedure this type of question follows. The solution gives that $F(x)$ can be written as $$\frac{x^2}{(1-x)^3}-\frac{x^3}{(1-x)^3}$$ This I understand. But then it then says for $F(x) = \dfrac{1}{(1-x)^3}$ we get $$\frac{F^n(0)}{n!} = \frac{(n+1)(n+2)}{2} \tag1$$ From which we can get the coefficients. Can someone help me understand what is computed here? Why do we use $F(x) = \dfrac{1}{(1-x)^3}$ , and how is $(1)$ computed? All help appreciated!","['discrete-mathematics', 'generating-functions']"
4432061,Averaging multivariate density over a hypercube,"Using a variant of the  notation in Wikipedia's article about Multivariate normal distribution: http://en.wikipedia.org/wiki/Multivariate_normal_distribution , let $X=(X_1,\ldots,X_n)$ be an $n$ -dimensional vector and $A$ a positive definite (not necessarily symmetric) $n\times n$ matrix, then let the density function $$f(x)=
\exp\left(-\frac{1}{2}{x}^T{\boldsymbol A}{x}
\right)$$ Consider  averaging. Suppose that the elements $a_{i,j}$ of $\boldsymbol A$ are small and we are only interested in an averaging  result up to Order $\cal O (a^2)$ . If $X_1,\ldots,X_n$ are  independent $\mathcal{N}(0,1)$ random variables, then the average can be performed. Let $\boldsymbol I$ be the unit matrix. Since $\boldsymbol A$ is not necessarily symmetric, we need to consider the symmetric part $\frac12 (\boldsymbol A + \boldsymbol A^T)$ for the integration (see here ). We get $$<f> = \int \cdots \int f(x) \mathcal{N}(x_1) \cdots \mathcal{N}(x_n) {\rm d} x_1 \cdots {\rm d} x_n\\
=\int \cdots \int  \frac{1}{\sqrt{(2\pi)^n}}
\exp\left(-\frac{1}{2}{x}^T{(\boldsymbol I + \boldsymbol A)}{x}
\right) {\rm d} x_1 \cdots {\rm d} x_n\\
= \frac{1}{\sqrt{\det (\boldsymbol I + \frac12 (\boldsymbol A + \boldsymbol A^T))}}$$ Now if this is to be evaluated  to Order $\cal O (a^2)$ , then we can expand. For simplicity, let $a_{i,i} = 0$ . Then we have $$<f> = \frac{1}{\sqrt{\det  (\boldsymbol I + \frac12 (\boldsymbol A + \boldsymbol A^T))}} \\
\simeq \frac{1}{\sqrt{1 - \frac 14 \sum_{i>j} (a_{i,j} + a_{j,i})^2}} \simeq \exp{\frac18 \sum_{i>j} (a_{i,j} + a_{j,i})^2}$$ where the sum $\sum_{i>j}$ arises as the sum over all $\binom{n}{2}$ many pairs $i\ne j$ in the Leibniz formula for the determinant's expansion if $n-2$ product factors are kept on  the main diagonal, i.e. have value $1$ . Now the question : Under the same conditions, what is the average of $f(x)$ over the hypercube, i.e. for $X_1,\ldots,X_n$ being independent  binary random variables with $X_i \in \{-1,1\}$ ? Is there an equivalent (discrete?) procedure to the integration performed above? A conjecture would be that the approximate result is actually the same as for normally distributed variables, since the elements $a_{i,j}$ are small and we are only interested in an averaging  result up to Order $\cal O (a^2)$ .","['probability-distributions', 'normal-distribution', 'combinatorics', 'discrete-mathematics', 'probability']"
4432105,The source for the proof of the uniformization theorem in Gamelin's book,"The book on Complex Analysis by Gamelin has the singular feature that it has a more-or-less complete proof of the Uniformization theorem for Riemann surfaces ( countable topology not assumed! ) despite being an undergraduate textbook. I am currently lecturing on the proof to graduate students. It seems to me that, modulo some hand-waving here-and -there, the proof given here is both the shortest and most transparent especially the proof of the second-countability of Riemann surfaces. I have been asked by the students about the original source of the proof. I know that Marshall's book has a proof that is similar in spirit but that is obviously not the source. Does anybody know a source for the proof?","['complex-analysis', 'reference-request']"
4432160,What is the derivation for the fluid dynamics continuity equation,"In my Fluid Dynamics module I have begun learning about mass flux. In my most recent lecture I have been presented with the following text and equation: For a general fluid in some volume $V$ , enclosed by the surface $S$ , the conservation of mass is expressed through the continuity equation (integral form): $$\iiint_V \frac{\partial\rho}{\partial t}\text{d}V=-\iint_S \rho\vec{v}\cdot\hat{n}\text{d}S$$ where $\rho(\vec{x})=\rho$ . I do not understand how these two integrals are equal to one another and would appreciate if anybody would be able to help me understand.","['integration', 'multivariable-calculus', 'calculus', 'vector-analysis', 'fluid-dynamics']"
4432171,I feel like epsilon-delta is reversed,"The lim is about ""when x approachs a, then y approachs L"". Then, shouldn't the epsilon and delta be like ""For all delta, no matter how small the delta is, you can always find an epsilon that makes ε < f(x)-L < ε""? But, the conventional explanation says like ""for all epsilon, you find delta"", which feels like to me, ""when y approachs L, x goes to a"".","['limits', 'definition', 'epsilon-delta', 'real-analysis']"
4432185,"""Math Lotto"" Tickets - finding the minimum winning set","""Math lotto"" is played as follows:  a player marks six squares on a 6x6
square. Then six  ""losing squares"" are drawn. A player wins if none of the losing squares
are marked on his lottery ticket. 1)Prove that one can complete nine lottery tickets in such a way that at least one of
them wins. 2)Prove that this is not possible with only eight tickets. My attempt is as follows; First I divided the square into 6 rectangles (figure 1). If one rectangle doesn't contain a cross then some ticket (ticket 1 to ticket 6) would win the game. Now we consider the case where each rectangle has one cross each. Now take the two rectangles on the top left of the square (figure 2). These have a total of two crosses. The first two columns together contains one cross and the third and fourth columns together contains one cross. There are four cases and we need at least four tickets (ticket 7 to ticket 10) to ensure win. I am only getting a minimum of ten tickets. How do I prove only nine tickets is required and for eight tickets it is not possible? Reference: Combinatorics by Stephan Wagner, Page 42, Problem 49
. https://math.sun.ac.za/swagner/Combinatorics.pdf","['combinatorial-designs', 'coloring', 'pigeonhole-principle', 'combinatorics', 'lotteries']"
4432193,Define a perfect $N$-ary tree in math notation as a set of series,"Let's say I have a perfect $N$ -ary tree of depth $=2 (M)$ and each parent node has children $=3 (N)$ , and each parent-child relation is weighted, so it looks as a weighted unidirectional graph, with direction from root to leaves, like below: I need to define this tree as a set of series in math notation and then I need to express the optimal series with maximum sum, especially when the tree could have a higher value for depth and children.
For example, in the above tree we have $13$ nodes in total from which we can constitute $9 (N$ x $M)$ series: 1: (0,1,4)
2: (0,1,5)
3: (0,1,6)
4: (0,2,7)
5: (0,2,8)
6: (0,2,9)
7: (0,3,10)
8: (0,3,11)
9: (0,3,12) The optimal series is $(0, 3, 11)$ because the sum $= 1.2$ I tried to describe it as a set of series, but I'm not sure if it's correct: $$
T=\left\{\left(p_i\left(q_j\right)_j^m\right)_i^{n^m}:i,j,n,m\in\mathbb{N}\right\}
$$ and then I tried to express the optimal series from that set as: $$
max{\left[T\right]}=  (0, 3, 11)
$$ I'm not a mathematician, please kindly advise if something is wrong.","['elementary-set-theory', 'trees', 'graph-theory', 'sequences-and-series']"
4432234,Can someone help with this horrible curve for the path of the sun?,"So I am doing some astronomical calculations for fun and it turns out that the approximate path that the sun traces in the sky should look something like: $$
\Gamma(t) = \left(\arctan\left(\frac{c_{i}\sin\left(t\right)}{c_{l}s_{i}-s_{l}c_{i}\cos\left(t\right)}\right), \frac{\pi}{2}-\arccos\left(c_{l}c_{i}\cos\left(t\right)+s_{l}s_{i}\right)\right) =: (\alpha(t),\beta(t))
$$ where $c_i, s_i, c_l, s_l$ are constants (sine and cosine of the Earth inclination and my location's latitude) so that $s^2_i+c^2_i=s^2_l+c^2_l=1$ . The component $\alpha(t)$ should be the clockwise angle that the sun makes with the South direction at time $t$ (the azimuth minus $\pi$ ) and $\beta(t)$ is the angular height at time $t$ . The time $t=0$ represents the local noon. The algebraic formulation of the path is horrible. Plotting $\Gamma(t)$ for each of the 12 months (i.e. changing the parameters $c_i$ and $s_i$ ) the 6 resulting paths (there are 6 paths because each path counts for 2 different months, the lowest path is December-January, the highest is June-July) are relatively nice looking continuous curves: but (due to the mean $\arccos(t)$ I guess) I had to stitch together four traslated versions of $\Gamma$ to make this picture, that is: \begin{align*}
(\alpha(t),\beta(t))\quad &|\quad-\pi<t<0,\quad&\alpha(t) > 0 \\
(\alpha(t),\beta(t))\quad &|\quad 0<t<\pi,\quad&\alpha(t) < 0 \\
(\alpha(t)-\pi,\beta(t))\quad &|\quad 0<t<\pi,\quad&\alpha(t) > 0 \\
(\alpha(t)+\pi,\beta(t))\quad &|\quad-\pi<t<0,\quad&\alpha(t) < 0
\end{align*} Is there a way to somewhat ""prettify"" the poor $\Gamma(t)$ and plot it as a single path?","['calculus', 'graphing-functions', 'trigonometry']"
4432236,Understanding a proof that $|\mathbb{R}| = |\mathcal{P}(\mathbb{N})|$,"I'm trying to understand a specific proof that $|\mathbb{R}| = |\mathcal{P}(\mathbb{N})|$ . I'm paraphrasing it below. The proof relies on the Schroder-Bernstein theorem. For each $A \subset \mathbb{N}$ , we associate a real number $x_A$ in base $2$ by $$
x_A = x_0. x_1 x_2 x_3 \ldots 
$$ where for every $n \in \mathbb{N}$ , we let $x_n = 1$ if $n \in A$ and $0$ otherwise. As this $x_A$ is unique, this map from $\mathcal{P}(\mathbb{N})$ to $\mathbb{R}$ is unique. On the flip side, every real number is uniquely determined by the set $$ 
A_x = \{y \in \mathbb{Q} : y < x\} \subset \mathbb{Q}. 
$$ So $$
|\mathcal{P}(\mathbb{N})| \leq |\mathbb{R}| \leq |\mathcal{P}(\mathbb{Q})| = |\mathcal{P}(\mathbb{N})|.
$$ Here are the questions I have on this proof. Why is this base $2$ representation unique? If $A = \{0\}$ , then $x_0 = 1$ and $x_n = 0$ for every $n \geq 1$ . But in base $2$ , $1.000 \ldots$ and $0.111 \ldots$ are the same number. I'm not fully sure I understand why it is helpful to use binary here or how to treat this issue where the binary expansion, defined as above, is not unique. I assume the set $A_x$ is a Dedekind cut associated to $x$ . Is there another way to see that this set is unique other than by acknowledging the construction of the real line? For example, if $x$ and $y$ are real numbers with $A_x = A_y$ , then $$
\{a \in \mathbb{Q} \mid a < x\} = \{b \in \mathbb{Q} \mid b < y\}. 
$$ If $x \neq y$ , then without loss of generality, $x > y$ . Then I can find a rational $q$ such that $y < q < x$ by density. Then $q \in A_x$ but $q \not \in A_y$ , which is a contradiction. Does this proof work?","['elementary-set-theory', 'solution-verification']"
4432258,"An easy question about the separability of $L^p(X,\mathcal{A}, \mu)$ space, where $p\in [1,+\infty)$","The theorem we have to prove is the following: Theorem 1. Let $(X,\mathcal{A},\mu)$ be a measure space such that $(i)$ the space $(X,\mathcal{A})$ is separable, that is exists a countable collection $\mathcal{C}\subseteq\mathcal{P}(X)$ such that $\sigma_0(\mathcal{C})=\mathcal{A}$ ; $(ii)$ $\mu$ is a sigma-finite measure. Then for each $p\in [1,+\infty)$ the space $L^p(X,\mathcal{A},\mu)$ is separable, that is it has a dense subset. The proof makes use of the following important Lemma Let $(X,\mathcal{A},\mu)$ be a finite measure space. Let $\mathcal{C}\subseteq\mathcal{P}(X)$ a collection such that $\sigma_0(\mathcal{C})=\mathcal{A}$ ; let $\mathcal{A}_0:=\mathcal{A}_0(\mathcal{C})$ the algebra generated by $\mathcal{C}$ . Then for all $F\in\mathcal{A}$ and for all $\varepsilon >0$ exists $G\in\mathcal{A}_0$ such that $$\mu(F\setminus G)+\mu(G\setminus F)<\varepsilon$$ Notation $\sigma_0(\mathcal{C})$ is the sigma- algebra generated by $\mathcal{C}$ . We divide the proof of Theorem $1$ into steps: Step 1. Let $(X,\mathcal{A},\mu)$ be a measure space. Then for each $p\in [1,+\infty)$ the set of measurable simple functions with rational coefficient $$\mathcal{S}_{\mathbb{Q}}:=\{s\in \mathcal{S}(X,\mathcal{A})\;|\; s(X)\subseteq\mathbb{Q}\}$$ is dense in $L^p(X,\mathcal{A},\mu)$ Notation $\mathcal{S}(X,\mathcal{A})$ denote the collection of measurable simple function. Step 2. For all $f\in L^p$ e for all $\varepsilon > 0$ exists $t\in\mathcal{S}_{\mathbb{Q}}$ such that $$\boxed{\mu(\{t\ne 0\})<\infty,\quad ||f-t||_p<\varepsilon}$$ This step is proved using the step 1 and the fact that $\mu$ is sigma-finite. Step 3. We suppose that $\mu(X)<\infty$ . As the space $(X,\mathcal{A})$ is separable, exists a countable family $\mathcal{C}\subseteq\mathcal{P}(X)$ such that $\sigma_0(\mathcal{C})=\mathcal{A}$ . The algebra $\mathcal{A}_0:=\mathcal{A}_0(\mathcal{C})$ generated by $\mathcal{C}$ is countable and also the set $$\mathcal{S}_{\mathbb{Q},\mathcal{A}_0}:=\bigg\{\sum_{k=1}^n c_k\chi_{G_k}\;|\; G_k\in\mathcal{A}_0, c_k\in\mathbb{Q}, n\in \mathbb{N}\bigg\}.$$ It is simple to prove with the help of the Lemma that $\mathcal{S}_{\mathbb{Q},\mathcal{A}_0}$ is dense in $\mathcal{S}_{\mathbb{Q}}$ . Step 4. Therefore, let $f\in L^p$ and we fix $\varepsilon >0;$ then for the step 2. exists $t\in\mathcal{S}_{\mathbb{Q}}$ such that $$\mu(\{t\ne 0\})<\infty\quad ||f-t||_p<\varepsilon/2$$ then for what has just been said exists $s\in\mathcal{S}_{\mathbb{Q},\mathcal{A}_0}$ such that $$||t-s||_p<\varepsilon/2$$ therefore results: $$||f-s||_p\le ||f-t||_p+||t-s||_p<\varepsilon/2+\varepsilon/2,$$ where we used the step 2., since the space is finite. Question. Why can we deduce from step 2 that it is not restrictive to assume that $\mu(X)<\infty$ ? Answer: The only trivial idea that comes to mind is to consider the finite measure subspace $$\bigg(Y,Y\cap\mathcal{A}, \mu|Y\cap\mathcal{A}\bigg),$$ where $Y:=\{t\ne 0\}$ .","['measure-theory', 'separable-spaces', 'proof-writing', 'real-analysis', 'lp-spaces']"
4432269,Do I.I.D. n-tuples have the same distribution when shifted?,"Suppose as an example with $n=3$ we have that $X_1, X_2, \dots$ is a sequence of random variables satisfying the condition that $(X_1,X_2,X_3), (X_4,X_5,X_6), (X_7,X_8,X_9), ...$ and so on are all i.i.d. $3$ -tuples. Does this imply that, for example, $(X_2, X_3, X_4)$ and $(X_3, X_4, X_5)$ are i.i.d.? I certainly know that our condition above doesn't imply that all of the individual $X_i's$ are i.i.d. and could quickly construct a counter-example. However I can't think of a counter-example disprove the possibility of these shifted $3$ -tuples being i.i.d. Either a counter-example or some sketch of a proof showing why they're i.i.d. would be greatly appreciated. I also suspect this has some relation to the concept of stationarity.","['stationary-processes', 'probability-distributions', 'probability-theory', 'random-variables']"
4432299,Calculating a matrix-exponential [duplicate],"This question already has answers here : Finding the matrix exponential (3 answers) The matrix $e^A$ is defined by $e^A=\Sigma_{k=0}^{\infty}\frac {A^k}{k!}$ Suppose M=$\begin{bmatrix}1 & 1\\0 & 1\end{bmatrix}$. Calculate $e^M$ (3 answers) Closed 2 years ago . Let A be the following matrix. $$\begin{pmatrix}
1 & 1 \\
0 & 1 \\
\end{pmatrix}
$$ I have to calculate $e^A$ . My idea was to diagonalize A because then $e^A = Pe^DP^-1$ if $A = PDP^-1$ . But A cannot be diagonalized since 1 is a double eigenvalue and therefore A does not have 2 linearly independent eigenvectors. How else can I calculate $e^A$ ? Thank you!","['matrices', 'matrix-exponential', 'linear-algebra']"
4432324,Is there general formula for the inverse of this matrices?,"For some natural number $\alpha>2$ we set $\zeta=e^{\frac{2i\pi}{\alpha}}$ . Let's consider $\alpha$ -by- $\alpha$ matrix and choose its entries to be $\zeta^{(i-1)(j-1)}$ in the $i$ -th row and $j$ -th column. $$\begin{bmatrix}
\zeta^{0} & \zeta^{0} & \zeta^{0} & \dots &\zeta^{0}\\
\zeta^{0} & \zeta^{1} & \zeta^{2} & \dots &\zeta^{\alpha-1}\\
\zeta^{0} & \zeta^{2} & \zeta^{4} & \dots &\zeta^{2(\alpha-1)}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\zeta^{0} & \zeta^{(\alpha-1)} & \zeta^{2(\alpha-1)} & \dots &\zeta^{(\alpha-1)^2}
\end{bmatrix}$$ I would like to find the inverse of this matrix for given $\alpha$ . However I have very little experience with matrices and I don't even know if there is a chance of finding a solution to this problem. The only thing I noticed is that the inverse matrix will always have $\frac{1}{\alpha}$ in first row and first column since $\sum_{m=0}^{\alpha-1}\zeta^{nm}=0$ for $n \in \mathbb{N}_+$ I would be very thankful for any help.","['matrices', 'linear-algebra', 'complex-numbers']"
4432335,General formula for the derivative of a pentation?,"Now, many people probably know of the first three hyperoperations, such as addition, multiplication, and exponentiation. However, many don't know of the fourth, tetration, and even less then know about the fifth hyperoperation, pentation. Much how like multiplication is repeated addition, and exponentiation is repeated multiplication. Tetration is repeated exponentiation. E.g. $^2x$ = $x^x$ and $^3x$ = $x^{x^x}$ I was wondering if there was a general formula for the derivative of tetrations, turns out my friend and I found one for all n $\ge$ 2 where n $\in$$\mathbb{Z}$ , this being. $\frac{d}{dx}$ [ $^nx$ ] = $\displaystyle\sum_{k=1}^{n-2}$ $\left(_nx_{n-k}^{k-1}\right)\frac{1}{x}$ + $_nx$$_2^{n-2}$ + $_nx$$_2^{n-1}$ Take note of the notation im using here: $_ax$$_z$ = $^ax$ * $^bx$ * … $^zx$ $x^{k-1}$ = x * ln $^{k-1}(x)$ This will return the correct expression, but now I began to wonder if there was a formula for the derivative of pentated equation. Pentation being: $_2x$ = $^xx$ Which means x exponentiated, x many times. I have no clue where to even began differentiating this, the formula evidently wouldn't spit out a good answer and I've scoured the stack exchange and internet looking for an answer but all I've come across is a nice formula which I also found earlier with no prior knowledge of the post. That being: $\frac{d}{dx}$ [ $^nx$ ] = $^nx$ $\left(\frac{^{n-1}x}{x}+ \frac{d}{dx}[^{n-1}x]ln(x)\right)$ Here's the link to the question: $n^{th}$ derivative of a tetration function Despite this, it still doesn't help. I'm lost as to what to do so any help would be amazing. Alas, I'm just some random internet stranger not really knowing what he's talking about. So if I'm missing something painfully obvious feel free to point it out.","['calculus', 'tetration', 'derivatives']"
4432344,"When $y=f(x)+ax+b$ for some trig function $f$, is there a name (like ""oblique asymptote"") for the line $y=ax+b$?","Let $f$ be a trig function, and let $y:=ax+b$ be a linear function. Every time you have $f(x)+y$ as one function, you get a graph where $(C_f)$ follows the line $y=ax+b$ , but we don't have the known limit for an oblique asymptote ( $\lim_{x \rightarrow \infty} f(x)-y \neq 0 $ ). Can we still call the line $y:=ax+b$ an oblique asymptote, or does this line have another name? An example with the function $f(x):=\sin(x)+x$ :","['trigonometry', 'functions', 'terminology']"
4432346,"Prove that in any subset of $9$ elements of $C$, there are two elements whose product is $1240$.","Can someone please help me with this?
Let C= $\{1,2,4,5,8,10,20,31,40,62,124,155,248,310,620,1240\}$ be the set of positive divisors of $1240$ . Prove that in any subset of $9$ elements of $C$ , there are two elements whose product is $1240$ . I know that multiplying each element from the left with the ones on the right gives $1240$ , but how do I use that info to answer this?","['pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
4432373,How to calculate the probability or 50% or more matches on n randomly assigned unique pairs,"FTR: This could not be a lower priority question. It's so not important, but I really want to know. Let's say I have n unique pairs. They are like keys (A) and locks (B), and every one of the n locks can be opened by exactly one of the n keys. And, of course we are completely randomly assigning each key to a random lock to see if it works. (BTW, it took me an embarrassing amount of counting combinations to figure out the that number of combinations is n!, but, in my defense, I've never actually used factorials in anything.) So, given n pairs, there are n! possible combinations. How does one calculate the chances that there are at least x correct matches? I'm in particular looking to see the chances for at least one correct match, and for at least 50% correct matches. I've tried internet searches, but I haven't been able to word it to get what I'm looking for. I made a spreadsheet and went up to 5-pair combinations. But I have no idea how to turn that data into any kind of formula. It doesn't paste well, so I'll paste the text and include an image with better formatting. Total #     Possible       Combs w/      Probability of   Combs with      Probability 
of pairs    combinations   matches ≥ 1   matches ≥ 1      matches ≥ 50%   of matches ≥ 50%
1           1              1             100.00%          ?               ?
2           2              1              50.00%          ?               ? 
3           6              4              66.67%          ?               ? 
4           24             15             62.50%          ?               ?
5           120            73             60.83%          ?               ? I think I got all the requirements for a well-formed question. If I missed anything, it was accidental, let me know and I'll fix it. Again, this is not important, just something that's been bugging me. Thank you for any help you can provide. Especially replies that explain ""this is why/how it works this way"" In that ""teach a person to fish"" kinda way. Chances of correctly matching pairs",['probability']
4432418,Subsets sharing at most $i$ elements,"Assume I have a set $S$ of $N$ elements and I create subsets with $k$ elements from it. With no additional property the number of possible such subsets will be $N \choose k$ . Now I want my subsets to satisfy some additional rules $R_i$ : Given a set $X$ of subsets of $S$ with $k$ elements (i.e. $\forall A \in X, A \subset S , \#A=k$ ), $X$ satisfies $R_i$ if and only if for all $A$ and $B$ in $X$ , $A$ and $B$ share at most $i$ elements, i.e. $\forall A , B \in X, \#(A \cap B) \le i$ . Let's call $S_i$ the set of all such $X$ satisfying $R_i$ . I want to know how large a set satisfying $R_i$ can be, i.e. what is $M_i = \max_{X \in S_i} \#X $ For example if $N=1000$ and $k=10$ , trivially we have $M_9 = {1000 \choose 10}$ : the largest ensemble of subsets of $10$ elements that share at most $9$ with one another is exactly the set of all possible combinations of $10$ elements out of $1000$ . Similarly at the other extreme, still for $N=1000$ and $k=10$ , we have $M_0 = 1000/10=100$ . Is there a formula or clever (not brute force) algorithm to compute other less trivial cases for $i=1,2,\dots8$ ? BTW even if there is a nice formula, I'll ultimately want to implement a way to get a maximal $X$ ...",['combinatorics']
4432447,Every not-orientable $n$-manifold embeds in an orientable $(n+1)$-manifold,"I have to prove that every not orientable smooth $n$ -manifold $M$ can be embdedded in an orientable smooth $(n+1)$ -manifold . My idea is to use the embedding theorem of Whitney but it just says that there exists an embedding $f: M\to\mathbb{R}^h$ for some $h\ge 1$ . Can anyone help me, please?","['geometry', 'smooth-manifolds', 'orientation', 'differential-topology', 'differential-geometry']"
4432450,Understanding a proof of the Schroeder-Bernstein theorem,"I'm trying to understand a proof of the Schroeder-Bernstein theorem. I can link the set of lecture notes it originated from, but the document is quite large, so it's probably better if I paraphrase it, adding details I thought were helpful at understanding (so if I'm incorrect, I'd appreciate if someone would point it out). I'll stop at the steps I didn't follow. For anyone interested, the proof is located on page 10 of this document . Theorem. If $|A| \leq |B|$ and $|B| \leq |A|$ , then $|A| = |B|$ . Proof. Without loss of generality, we can assume $A$ and $B$ are disjoint. If not, replace $A$ with $A \times \{0\}$ and $B$ with $B \times \{1\}$ , where $0 = \emptyset$ and $1 = \{\emptyset\}$ . Then $|A| = |A \times \{0\}|$ via the bijection $a \mapsto (a,0)$ . and $|B| = |B \times \{1\}|$ via the bijection $b \mapsto (b,1)$ . Therefore, if we establish that $|A \times \{0\}| = |B \times \{1\}|$ , where $A \times \{0\}$ and $B \times \{1\}$ are disjoint, then composing maps gives $|A| = |B|$ . Let $f: A \to B$ and $g: B \to A$ be injections, and define $$
F = f \cup g: A \cup B \to A \cup B,
$$ which is well-defined since $A$ and $B$ are disjoint. As $f$ and $g$ are injective, so is $F$ . Indeed, if $F(x) = F(x')$ , then because $A$ and $B$ are disjoint, either $x,x' \in A$ or $x,x' \in B$ ; indeed, if $x \in A$ and $x' \in B$ (or vice-versa), then $F(x) = g\in B$ and $F(x') \in A$ , and $F(x) \neq F(x')$ by disjointness. If $x,x' \in A$ , then $F(x) = f(x)$ , $F(x') = f(x')$ , and we have $f(x) = f(x')$ , so $x = x'$ by injectivity of $f$ . Similarly, if $x,x' \in B$ , then $F(x) = F(x')$ , so $g(x) = g(x')$ , so $x = x'$ by injectivity of $g$ . So $F$ is injective. We say that $F(x)$ is the child of $x$ , and $x$ is the parent of $F(x)$ . Then every $x \in A \cup B$ has only one parent, and because $F$ is a well-defined function and sends every input to a unique output, every parent has exactly one child. A godfather is defined as a parent that is no one's child. I think I understand the concept of a ""godfather."" A godfather is a parent, meaning that it has a unique child $x \in A \cup B$ and is of the form $F(x)$ , but is not itself the image of an element of $x \in A \cup B$ ; if $y = F(x)$ is the godfather, then $y \neq f(t)$ for every $t \in A \cup B$ . So a godfather is an element of $A \cup B \setminus f(A \cup B)$ , an element of the codomain of $F$ but not its image. I don't understand the next sentence and it's crucial for defining the bijection, so I'm going to quote it in full. ""For any $x \in A \cup B$ , either $x$ is descended from a unique godfather (possibly from $x$ itself), or $x$ has no godfather; it has an infinite line of ancestors (or $x$ is descended from itself.)"" I can't tell whether, in the case where $x$ is descended from itself, which I assume means $F(x) = x$ , whether $x$ is a godfather or whether it counts in the section that has no godfather. The proof seems to include it in both. Without the second mention, I think I understand. Let's say $x$ is in the image of $F$ . Then I attempt to trace its lineage, first by reversing the arrow $F$ and then retracing $f$ and $g$ . If I end up at an element of $A \cup B \setminus f(A \cup B)$ , then it falls into the category of descending from a unique godfather. Because $f$ and $g$ are injective and $A$ and $B$ are disjoint, I can only trace back its lineage in a single way, so there's a unique such godfather. If its lineage regressed ad infinitum and doesn't end up in $A \cup B \setminus f(A \cup B)$ , then it's in the latter described category. Now suppose that $x$ is not in the image of $F$ . It's certainly in the domain of either $f$ or $g$ , so I simply start with these arrows and continue to follow them until I either end up at $A \cup B \setminus f(A \cup B)$ or it regressed ad infinitum. Moving on: Partition $A$ into three pieces as follows. Let $A_0$ be the elements of $A$ with no godfather, $A_A$ be the elements of $A$ whose godfather is in $A$ , and $A_B$ be the elements of $A$ whose godfather is in $B$ . Partition $B$ similarly. The proof then argues there is a bijection between $A_0$ and $B_0$ , a bijection between $A_A$ and $B_A$ , and a bijection between $A_B$ and $B_B$ . Between $A_0$ and $B_0$ : The map sends $a \in A_0$ to its child $F(a)$ . This map is injective since $F$ is injective, and so the restriction of $F$ to $A_0$ is not injective. It argues that the map is surjective because every element of $B_0$ has a parent and this element must live in $A_0$ . I'm confused on both of these points. If an element of $A \cup B$ is not in the image of $F$ , doesn't it lack a parent? Why must the parent live in $A_0$ ? This proof I'm not able to fully follow. I also don't see why $F(a) \in B_0$ . The map between $A_A$ and $B_A$ again sends $a \in A_A$ to $F(a)$ . I'm again having trouble understanding why the image is in $B_A$ and $B_B$ , respectively. I would greatly appreciate if someone could help me understand the proof. My questions are: On the line regarding $x$ being descended from itself: does that mean $F(x) = x$ , and does that correspond to $x$ having no godfather? Understanding the bijections between $A_0$ and $B_0$ , $A_A$ and $B_A$ , and $A_B$ and $B_B$ , why the image of $a$ under these maps is in the respective subset of $B$ , and how we can be certain that the maps are surjective.","['elementary-set-theory', 'solution-verification']"
4432489,"How to prove that the limit $\lim_{(x,y) \to (0,0)} \frac{xy^2}{|x|^5 + y^2}$ does not exist?","At first I thought that the limit existed, but since the degree of the numerator is less than the degree of the denominator, that gives a hint as to the nonexistence of the limit. As such, the squeeze theorem cannot be applied and I have to find 2 paths where the limits give different values. I've tried many ways but they all end up giving 0.","['limits', 'calculus']"
4432499,Is nearest point to $\mu$ also nearest to all other points?,"Suppose a set of points $\mathcal{X} = \{ x_1, …, x_n\} \subset \mathbb{R}^p$ . Define $$\mu = \frac1n \sum_i x_i$$ We know that $\mu$ is the point which minimizes (over $\mathbb{R}^p$ ) the sum of squared distances between it and all other points in $\mathcal{X}$ . Is it true that the point in $\mathcal{X}$ that is closest to $\mu$ is also the point in $\mathcal{X}$ that is closest to all other points in $\mathcal{X}$ ? In other words what I am asking is whether the following equality holds: $$\arg\min_{x_i \in \mathcal{X}} \| \mu - x_i \|_2^2 = \arg\min_{x_j \in \mathcal{X}} \sum_i \| x_j - x_i \|_2^2$$ How can this be shown? I’m finding it difficult to write a proof due to the fact that $\mathcal{X}$ is a set of points (non-convex, disconnected, countable, etc) If yes, does this still hold when we use other $p$ -norms in place of $\ell_2$ to compute distance?","['statistics', 'analysis', 'real-analysis', 'algorithms', 'optimization']"
4432678,Disconnected image of a derivative,"I would like to know if there exists a differentiable vector-valued function $f:[a,b]\rightarrow \Bbb R^2$ such that the image $f'([a,b])$ of its derivative is disconnected. My Attempt First of all I asked for $f$ to be vector-valued because, if it were real-valued, then Darboux's Theorem would imply that $f'([a,b])$ be connected. For a similar reason, if we let $x(t)$ and $y(t)$ denote the component functions of $f$ , then their derivatives can not both be continuous, for otherwise $f'$ would also be continuous hence preserving connectedness. Therefore, WLOG, $x'$ has to be discontinuous; however, Darboux's Theorem imply that it can not have discontinuities of the I kind or discontinuities of the II kind with infinite directional limits on $]a, b[$ . Now, all the functions I came up for $x$ are not enough to construct the desired $f$ and I am starting to believe that such a function doesn't actually exist but I am not able to prove it. Any help or hint, as always, is highly appreciated!","['connectedness', 'vectors', 'real-analysis', 'general-topology', 'derivatives']"
4432701,"How to prove $Cov(E(X_1|X_2), X_1 - $ $E(X_1|X_2))$ $= 0$","Question Determine whether or not the following result is True or False: $$Cov(E(X_1|X_2), X_1 - E(X_1|X_2)) = 0$$ Attempt I tried at first: $=E[((X_1|X_2)(X_1-E(X_1|X_2)) - E(X_1|X_2)E(X_1-E(X_1|X_2))]$ $E[(X_1(X_1|X_2) - (X_1|X_2)E(X_1|X_2)+E(X_1|X_2)^2-E(X_1|X_2)E(X_1)]$ $E(X_1)^2 - E(X_1|X_2)E(X_1) + E(E(X_1|X_2)^2) - E(X_1)^2$ $-E(X_1|X_2)E(X_1) + E(E(X_1|X_2)^2)$ From here I am lost. Further Comments I know $E(E(X_1|X_2)^2) = E(X_1)^2$ But what about $E(X_1|X_2)E(X_1)$ ? Is there way to simplify it?","['expected-value', 'statistics', 'covariance', 'probability']"
4432712,Randomly drawing socks out of a bag,"There are 5 pairs of socks in a bag, so a total of 10 socks. Each pair of socks is of a different color from the others. After having randomly drawn 4 socks from the bag, what's the probability of picking 4 socks of different colors? My answer was to first get the total number of possible draws as ${10 \choose 4} = 210$ . Then I'd proceed to compute the number of possible pickings of 2 pairs of socks out of the 4 draws: ${4 \choose 2} {4 \choose 2} = 36$ . Similarly, the number of pickings of 1 pair of socks + 2 loose socks would be ${4 \choose 2} {4 \choose 1} {4 \choose 1} = 96$ . The probability of picking 4 socks of different colours would then be the complementary set of events to the sum of the previous 2: $P($ no matching pair $) = 1 - \frac{36 + 96}{210} = 0.372$ . However the answer to the problem points to a different result. I understand the fact there are 5 colors should factor in somewhere, I only considered the number 4 (the draws) in my combinatorics and I guess this is the source of my mistake.","['combinatorics', 'probability']"
4432717,Why are causal inference diagrams so useful or effective?,"Is there a short explanation of why Pearl's casual inference diagrams are so highly-regarded, useful or effective? I can't help but think it's just so simple an idea that I can't tell why it could be such a big deal: It's just like a set of equations where functions are composed in an 'acyclic' way? And apparently it's like groundbreaking and there's a whole field of 'causality'?","['statistical-inference', 'statistics', 'causality', 'causal-diagrams']"
4432752,Gysin -/ Shriek map of projectivized bundle,"Consider the Euler sequence $$0\rightarrow \gamma \rightarrow \epsilon^4 \rightarrow Q \rightarrow 0$$ over $\mathbb{P}^3$ . Take the projectivized bundle $\pi:P(Q)\rightarrow \mathbb{P}^3$ and consider its respective short exact sequence $$0\rightarrow L \rightarrow \pi^*Q \rightarrow Q' \rightarrow 0,$$ where $L$ is the tautological line bundle of $P(Q)$ . Let $$t = c_1(L^*) \quad a = c_1(\gamma^*).$$ It is well - known by the Leray - Hirsch Theorem that the cohomology of $P(Q)$ is given by $$ H^*(P(Q)) \cong H^*(\mathbb{P}^3)[t]/(t^3 + at^2 + a^2t + a^3),$$ where the multiplication is defined as $\beta\cdot t^k = \pi^*\beta \cup t^k$ for $\beta \in H^*(\mathbb{P}^3) \cong \mathbb{Z}[a]/(a^4)$ . My question is the following: Denoting by $\pi^!: H^{2k}(P(Q))\rightarrow H^{2(k-2)}(\mathbb{P^3})$ the Gysin - or shriek - map of $\pi$ , how do I show that $$\pi^!(t^2) = 1\quad \pi^!(t^3) = -a \quad \pi^!(t^4) = \pi^!(t^5) = 0?$$ What I have so far is that I chose numbers $\mu_0, \mu_1, \mu_2$ and $\mu_3$ such that $$\pi^!(t^2) = \mu_0\cdot 1\quad \pi^!(t^3) = \mu_1 \cdot a,\quad \pi^!(t^4) = \mu_2 \cdot a^2\quad \pi^!(t^5) = \mu_3 \cdot a^3$$ and then used the relation $t^3 + at^2 + a^2t + a^3 = 0$ to obtain equation the equation $$0 = \pi^!(t^3 + at^2 + a^2t + a^3) = (\mu_0 + \mu_1) a$$ and thus, I can easily obtain $$ 0 = \pi^!(t^4 + at^3 + a^2t^2 + a^3 t) = \pi^!(t^4) =  \mu_2 a^2,$$ $$ 0 = \pi^!(t^5 + at^4 + a^2t^3 + a^3 t^2) = \pi^!(t^5) = \mu_3 a^3.$$ What I don't see is why $$\mu_0 = 1,\quad \mu_1= -1$$ since I only get $\mu_0 + \mu_1 = 0$ .
So there is one defining equation missing but I don't know where to get it from. Thank you in advance for any reply!","['algebraic-topology', 'vector-bundles', 'algebraic-geometry', 'homology-cohomology', 'characteristic-classes']"
4432871,"If balls are replaced with rectangles, does Lebesgue's differentiation theorem hold?","I know that the Lebesgue's differentiation theorem states that for $f\in L^1(\mathbb R^n)$ then $$\lim_{r\to 0^+}\frac1{B_r(x)}\int_{B_r(x)}f$$ exists and coincides with $f(x)$ a.e.
I am wondering whether balls can be replaced with rectangles.
To be precise, if $f\in L^1(\mathbb R^n)$ , does $$\lim_{r_1,\dots,r_n\to 0^+}\frac1{r_1\cdots r_n}\int_{[x_1-r_1/2,x_1+r_1/2]\times\cdots\times[x_n-r_n/2,x_n+r_n/2]}f$$ exist and coincide with $f(x)$ a.e. I have found the following more general results of Lebesgue's differentiation, but it seems to be not applicable in our current case.
So I guess the answer is no.
But I cannot find any example. Let $\mathcal {V}$ be any family with the property that
(i) for some $c > 0$ , each set $U$ from the family is contained in a ball $B$ with $ |U|\geq c\cdot |B|$ ;
(ii) every point $x\in\mathbb R^n$ is contained in arbitrarily small sets from ${\mathcal {V}}$ . Then the analogous limit exists and coincides with $f(x)$ a.e. as the sets $U$ shrink to $x$ .
So, for example, if we let $Q_r(x)$ denotes the cube centered at $x$ with diameter $r$ , then $$\lim_{r\to 0^+}\frac1{Q_r(x)}\int_{Q_r(x)}f.$$ exists and coincides with $f(x)$ a.e. Thanks!","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
4432911,A domain satisfying the weak cone condition but not the strong cone condition,"In the 1977 paper ""Cone Conditions and Properties of Sobolev Spaces"" by Adams and Fournier, the authors say that an open set $\Omega\subset\mathbb{R}^n$ satisfies the cone condition if each $x\in\Omega$ is the vertex of a cone $C_x$ contained in $\Omega$ congruent to a fixed cone $C$ . They then go on to define the weak cone condition. Let $x\in\Omega$ . Let $R(x)$ consist of all points $y\in\Omega$ such that the line segment from $x$ to $y$ lies completey $\Omega$ . Let $\Gamma(x)=\{y\in R(x) : |x-y|<1\}$ . $\Omega$ satisfies the weak cone condition if there is a number $\delta>0$ such that $\mu_n(\Gamma(x))\geq \delta$ . Here $\mu_n(\Gamma(x))$ denotes the Lebesgue measure of $\Gamma(x)$ . The authors note that there are many domains satisfying the weak cone condition, but not the ""strong"" cone condition. Does anyone know an example of such a domain? I've tried to construct one but have had no luck. Here are a couple of my thoughts: Any domain $\Omega$ failing the cone condition must not contain a smallest cone. That is, there cannot be an $x\in\Omega$ such that for any other $y\in\Omega$ , the cone $C_x$ may be rigidly transformed to have its vertex coincide with $y$ , and $C_x\subset C_y$ . If such an $x$ exists, then we can take our fixed reference cone to be $C_x$ and then $\Omega$ satisfies the cone condition. If $\Omega$ fails the cone condition, but satisfies the weak cone condition, then there should be a sequence $\Gamma(x_n)$ for which the diameter of the interior of $\Gamma(x_n)$ tends to 0 as $n\to\infty$ . If the diameter of the interior of $\Gamma(x)$ was bounded below for all $x$ , there there would be a smallest ball contained in $\Gamma(x)$ for all $x$ , and thus a smallest cone.","['regularity-theory-of-pdes', 'sobolev-spaces', 'functional-analysis', 'analysis']"
4432921,Lehmann–Scheffé theorem's statement,"In my notes I have the following L-S theorem statement: Let $T(X_1,...,X_n)$ be an estimator for $\theta \in \Theta$ . If $T$ is: unbiased a function of complete and sufficient statistic $S_c(X_1,...X_n)$ , so that we can write: $T=g(S_c(X_1,...X_n))$ $\implies$ $T$ is the unique estimator that is both unbiased and
function of a complete and sufficient statistic. My question is: does $g$ have to be a bijective function? In my opinion, it doesn't have to be: in the proof of this theorem, this condition isn't used. However, my lecturer said it is needed. I disagree.","['statistical-inference', 'statistics', 'estimation']"
4432972,Trigonometric integral inequality,"Let us consider the integral equation \begin{equation}
    f(x)=\lambda \int_{0}^{\pi} \cos (x-y) f(y) \hspace{1mm}dy+g(x), \quad x \in[0, \pi],
\end{equation} where $f$ is an unknown function on $[0, \pi]$ , $g(x)$ is a given continuous function on $[0, \pi]$ and $\lambda$ is a given real constant. Prove that the equation has a unique solution $f \in C[0, \pi]$ for each $\lambda \neq \frac{2}{\pi}$ .","['trigonometry', 'trigonometric-integrals', 'inequality']"
4432997,Conditions on tetrahedron side lengths that guarantee existence of a sphere tangent to all its edges,"I recently worked on the problem of finding a sphere tangent to the edges of an irregular tetrahedron.  I found that if one of the triangular faces of the tetrahedron is an isosceles triangle, then it is possible to find an infinite number of tetrahedrons that share this face, and each of them will have a unique sphere that is tangent to all its six edges. My question is , are there instances where a tetrahedron having all faces as scalene triangles can have such a tangent sphere to its edges ?","['analytic-geometry', '3d', 'geometry', 'solid-geometry', 'linear-algebra']"
4433041,How can we stay confidence replacing the population standard deviation by it's estimate?,"So imagine we take $n$ random samples from a Bernoulli Trial. Thus our random samples are composed by binary random variables $X_1, X_2, ..., X_n$ . So by central limit theorem we know that the distribution of $Z=\frac{\overline{X}-p}{\sigma/\sqrt{n}}$ such that $\overline{X}=\frac{X_1+X_2+...+X_n}{n}$ approximates a standard normal pdf when $n$ is big enough. So finding the probability that $Z$ lies between $-1.96$ and $1.96$ is: $$P(-1.96\le Z\le 1.96)=P(-1.96\le \frac{\overline{X}-p}{\sigma/\sqrt{n}} \le 1.96) = 0.95$$ We also know that the standard deviation of our Binary Random Variable is $\sigma=\sqrt{p(1-p)}$ . Thus: $$P(-1.96\le \frac{\overline{X}-p}{\sqrt\frac{p(1-p)}{n}} \le 1.96) = 0.95$$ The book I'm using just replace $p$ by it's estimate $\overline{X}$ without further explanation. Why can we do that? Thus: $$P(-1.96\le \frac{\overline{X}-p}{\sqrt\frac{\overline{X}(1-\overline{X})}{n}} \le 1.96) = 0.95$$ So transforming a little we have that: $$P(\overline{X} -1.96\sqrt\frac{\overline{X}(1-\overline{X})}{n} \le p \le \overline{X} + 1.96 \sqrt\frac{\overline{X}(1-\overline{X})}{n}) = 0.95$$ How can we still saying that this is true with 95% of confidence? what justifies replacing $p$ by $\overline{X}$ ? I mean the 95% confidence interval is true when we use the population standard deviation and not some estimate. Using an unbiased estimator for the standard deviation population will only tells us that we are going to have a 95% confidence interval in the long run. So, it's the estimator $\overline{X}(1-\overline{X})$ for $\sigma^2$ even unbiased?","['statistics', 'central-limit-theorem', 'statistical-inference', 'estimation', 'proof-explanation']"
4433046,"If square matrices $A^2 + B^2 = 2AB$, then prove that $p_A(x) = p_B(x)$","Original problem statement: Let $A, B \in M_n(\mathbb{C})$ such that $A^2 + B^2 = 2AB$ . Prove that
for any $x \in \mathbb{C}$ : $$det(A - xI_n) = det(B-xI_n)$$ Now the first observation, the equality that has to be proven is the definition of the characteristic polynomials of both matrices being equivalent, i.e. $p_A(x) = p_B(x)$ . My first thought seeing this was that I had to prove that $A \sim B$ (i.e. $A$ is similar to $B$ ), which in turn would imply equivalent characteristic polynomials. I tried some algebraic manipulations to the equality given in order to utilize Jordan canonical form and somehow reach similarity, but I didn't get anywhere, so maybe that's futile. I've got a hunch that it's something way simpler, but clever. One can also observe that we can express the commutator of both matrices: $$[A, B] := AB - BA = \stackrel{\text{from the given}}{\dots} = (A - B)^2$$ but I'm not that knowledgeable in terms of commutators, so I couldn't find anything useful following from that. Any hints are welcome (and I'd be glad to have a hint only, not a full solution... I need to figure it out for myself a bit, too :D)! Thank you in advance.","['matrices', 'matrix-equations', 'linear-algebra', 'characteristic-polynomial']"
4433065,Definite integral of the logarithm of a trigonometric polynomial,"Let $p$ and $q$ be two real numbers such that $q>p^2$ , so that $1+2px+qx^2>0$ for all $x$ . I need to calculate the integral \begin{equation*}
\int_0^\pi\ln\big(1+2p\cos\theta+q\cos^2\theta\big)d\theta.
\end{equation*} Any idea? Thank you! I am able to calculate the integral in the case $q<1$ . In fact, it is known that if $z\in\mathbb{C}$ satisfies $|z|<1$ , then \begin{equation*}
\int_0^\pi\ln\big(1+z\cos\theta\big)d\theta=\pi\ln\bigg(\frac{1+\sqrt{1-z^2}}{2}\bigg).
\end{equation*} We have $1+2p\cos\theta+q\cos^2\theta=(1+z\cos\theta)(1+\bar{z}\cos\theta)$ for all $\theta$ with \begin{equation*}
z:=p-\mathrm{i}\sqrt{q-p^2}.
\end{equation*} The hypothesis $q<1$ gives $|z|=\sqrt{q}<1$ . Thus, \begin{align}
\nonumber
 \int_0^\pi\ln\big(1+2p\cos\theta+q\cos^2\theta\big)d\theta&=
\int_0^\pi\ln\big(1+z\cos\theta\big)d\theta+\int_0^\pi\ln\big(1+\bar{z}\cos\theta\big)d\theta\\
\nonumber
&=\pi\ln\bigg(\frac{1+\sqrt{1-z^2}}{2}\bigg)+\pi\ln\bigg(\frac{1+\sqrt{1-\bar{z}^2}}{2}\bigg)\\
&=2\pi\ln\bigg|\frac{1+\sqrt{1-z^2}}{2}\bigg|.
\end{align} This result extends to $q=1$ by continuity. Numerical simulations suggest that this result is valid for all $q$ . So, what I am actually searching for is a way to prove it for $q>1$ .","['trigonometry', 'definite-integrals', 'polynomials', 'logarithms']"
4433073,Solution verification: showing $\lim \limits_{n \to \infty} \frac{n!}{n^n} = 0$,"In a textbook, I came upon the following excercise: Verify that $\displaystyle \lim_{n \to \infty} \frac{n!}{n^n} = 0$ I came up with the following solution: $$\begin{align*}
\lim_{n \to \infty} \frac{n!}{n^n} 
&= \lim_{n \to \infty} \prod_{i=1}^n \frac{n+1-i}{n} \\
&= \lim_{n \to \infty} \left(\prod_{i=1}^{n-1} \frac{n+1-i}{n} \cdot \frac{1}{n} \right) \\
&= \left( \lim_{n \to \infty} \prod_{i=1}^{n-1} \frac{n+1-i}{n} \right) \left( \lim_{n \to \infty} \frac{1}{n} \right)
\end{align*}$$ And since $$\lim_{n \to \infty} \frac{1}{n} = 0$$ we have $$\lim_{n \to \infty} \frac{n!}{n^n} = 0$$ as required. Since my textbook gives another solution, I post mine here with the question, if this is correct. Thanks in advance for any feedback.","['limits', 'calculus', 'solution-verification', 'sequences-and-series']"
4433133,Mapping square to a quadrilateral,"In 1 dimensional space, to map an interval $[0,1]$ into another interval $[a,b]$ we use the function $$T: [0,1] \to [a,b] :\quad x \mapsto (1-x)a+xb$$ What is the generalisation of this in 2D space (or higher), in other words, what is the function $T$ such that $T$ maps the square $\{(1,1);(-1,1);(-1,-1);(1,-1)\}$ into a quadrilateral $\{(x_1,y_1);(x_2,y_2);(x_3,y_3);(x_4,y_4)\}$","['multivariable-calculus', 'functions', 'multivariate-polynomial']"
4433137,What is the mathematically correct way to draw a sphere with great circles?,"I'm not sure if this is appropriate for math.SE. But I figured that my problem is with understanding, and not with execution, so I thought it to be more appropriate for math.SE instead of tex.SE. I would like to draw a sphere with some great circles on it, programmatically in TikZ. So I started out with two circles in the $xz$ -plane, and in the $zy$ -plane and some coordinate axes: \begin{tikzpicture}[scale=2]
    % coordinate axes
    \draw[->] (-1.5, 0, 0) -- (1.5, 0, 0);
    \draw[->] (0, -1.5, 0) -- (0, 1.5, 0);
    \draw[->] (0, 0, -1.5) -- (0, 0, 1.5);

    % the circles \begin{scope}[canvas is xz plane at y = 0]
    \draw (0,0) circle[radius=1];
\end{scope} \begin{scope}[canvas is zy plane at x=0]
    \draw (0,0) circle[radius=1];
\end{scope} \end{tikzpicture} This gives the following picture: Then I thought if we rotate one the latitude circle, this should give the sphere with more latitudes, right? The following code does exactly this \foreach \t in {120,125,...,285} {
    \begin{scope}[
            % draw on a rotated plane
            plane x = {(({sin(\t)},0,cos(\t))},
            plane y = {(0,1,0)},
            canvas is plane
        ]
        \draw (0,0) circle[radius=1];
    \end{scope}
} Adding this produces the following picture. I don't understand why the outline is not perfectly round. What's going on here? Is it because we are not doing dealing properly with perspective? Or did I miss something else?",['geometry']
4433233,Find $\int{\frac{d\theta}{\sqrt{b+a\cos\theta}}}$,"Trying to find: $$\int{\frac{d\theta}{\sqrt{b+a\cos\theta}}}$$ (generalizing the cases for $a,b = 1$ ). I've tried two approaches, none of which have gotten me far: By noticing that $\frac{d}{d\theta}\left(\sqrt{b+a\cos\theta}\right) = \frac{-a\sin\theta}{2\sqrt{b+a\cos\theta}}$ , I rewrote the integral as $$\int{\frac2{-a\sin\theta}\frac{-a\sin\theta}{2\sqrt{b+a\cos\theta}}}\,d\theta$$ Then I used integration by parts ( $u= -\frac2a\csc\theta, du = \frac2a\csc\theta\cot\theta \, d\theta,;dv=\frac{-a\sin\theta}{2\sqrt{b+a\cos\theta}}d\theta, v = \sqrt{b+a\cos\theta}$ ) to yield $$-\frac2a\csc\theta\sqrt{b+a\cos\theta} \,- \int{\frac2a\csc\theta\cot\theta \sqrt{b+a\cos\theta}}\,d\theta = -\frac{2\sqrt{b+a\cos\theta}}{a\sin\theta} \,- \int{\frac{2\cos{\theta}\sqrt{b+a\cos\theta}}{a\sin^2{}\theta}}\,d\theta$$ But I'm not sure where to go from here. u-substitution? By Weierstrass substitution, let $t=\tan{(\frac\theta{2})} \implies \cos\theta = \frac{1-t^2}{1+t^2}, d\theta = \frac{2dt}{1+t^2}$ $$\implies \int{\frac{d\theta}{\sqrt{b+a\cos\theta}}} = 
\int{\frac1{\sqrt{b+a\,\frac{1-t^2}{1+t^2}}}\frac{2dt}{1+t^2}}$$ $$\int\frac{2dt}{\sqrt{b(1+t^2)+a(1-t^2)}\cdot\sqrt{1+t^2}}$$ But this is very quickly becoming a mess. Any suggestions?","['integration', 'calculus', 'trigonometry']"
4433247,What right triangles can be constructed using sides of three distinct regular polygons with the same circumradius?,"I have been wondering if the following question requires advanced mathematics such as ring theory: What are the right triangles that we can construct using the side lengths of three distinct regular polygons having the same circumradius? An example would be the triangle constructed using an equilateral triangle, a square and a regular hexagon.","['trigonometry', 'abstract-algebra', 'geometry']"
4433252,"Evaluate $\int_{0}^{\infty} \ln(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}) \, dx$","Numerical evidence suggests the following $$I=\int_{0}^{\infty} \ln\left(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}\right) \, dx\stackrel{?}{=}4\pi W\left(\frac{1}{2}\right)=4.42\cdots$$ where $W(x)$ is the Lambert $W$ (product log) function. Here’s the context for the problem: I was playing around with other $\ln$ integrals, and evaluated the following: $$\int_{0}^{\infty} \ln \left(1+2x^{-2} \cos(\phi)+x^{-4}\right)\,dx=2\pi\cos \left(\frac{\phi}{2}\right)$$ There are several questions on MSE on similar integrals to the above, such as this one .
The above integral can be evaluated in several ways, but I just chose to introduce a $\ln(z)$ integral representation. I was curious to know what the integral would instead be if I replaced $\cos \phi$ with $\cos x$ . $I$ converges because it is bounded above by the integral $\int_{0}^{\infty} \ln (1+2x^{-2}+x^{-4})\,dx=2\pi$ . Here are my attempts: Although I’m much more comfortable with real analysis, I currently have no method that I can apply real analytic techniques or representations on $I$ . I tried instead contour integration: Firstly notice that $$I \stackrel{\text{IBP}}{=}2\int_{0}^{\infty} \frac{2+2x^2 \cos x+x^3 \sin x}{1+x^4 +2x^2 \cos x} \, dx$$ I then converted these trig functions into exponential forms and factorised the denominator to determine where the poles are- they are the four solutions to $x^2+e^{i x} = 0$ and $x^2+e^{-i x}=0$ which are $$x=\pm 2i W\left(\pm \frac{1}{2}\right)$$ I then took a semi-circular arc in the upper half plane to be my contour as the integrand is even, and computed the following residues: $$\text{Res} \left[f(z),\,2i W\left(\frac{1}{2}\right)\right] =-2i W\left(\frac{1}{2}\right)$$ $$\text{Res} \left[f(z),\,-2i W\left(-\frac{1}{2}\right)\right] =2i W\left(-\frac{1}{2}\right)$$ where $f(x)$ is the integrand function above. So we have then that $$\oint_C f(z) \, dz=2I + \int_{\Gamma} f(z) \, dz = -4\pi \left(W\left(-\frac{1}{2}\right) -W\left(\frac{1}{2}\right)\right)$$ where $\int_{\Gamma}$ traces the semi-circular arc path. Here is where I’m stuck however because I’m not sure how to evaluate this arc integral.","['integration', 'analysis', 'real-analysis', 'complex-analysis', 'calculus']"
4433263,Inequality help: $\frac{a+b+c}{3}\geq\sqrt{\frac{ab+bc+ca}{3}}\geq\sqrt[3]{abc}$,"I'm working through ""Calculus of One Variable"" by Joseph Kitchen, and this inequality (problem 3 from section 1.3) is causing me quite a bit of pain to prove: $$\frac{a+b+c}{3}\geq\sqrt{\frac{ab+bc+ca}{3}}\geq\sqrt[3]{abc},$$ where $a,b,c\geq 0.$ The main issue I'm having is that I cannot use the fact that $0\leq a < b$ if and only if $a^{2} < b^{2}$ , as this fact is proven in a later exercise. If you want to take a look at what is proven and what isn't, the first 80 pages of the book is available on a Google Books preview. Although a hint for this problem is provided in the back of the book, it has not been useful to me. The hint states: ""Use Example 2 of the text. Define $A = (x+y+z)/3$ and $H^{-1} = (x^{-1}+y^{-1}+z^{-1})/3$ ."" Note that $A$ and $H$ refer to the arithmetic and harmonic means, respectively. Example 2 proved that if $a$ , $b$ , and $c$ are positive numbers which are not all equal, then $(a+b+c)(bc+ca+ab) > 9abc$ . This was proved by noting that $(a+b+c)(bc+ca+ab)-9abc = a(b-c)^{2} + b(c-a)^{2} + c(a-b)^{2}$ . With little work, it is not hard to see that when $a$ , $b$ , and $c$ are non-negative , then $(a+b+c)(bc+ca+ab) \geq 9abc$ . However, I'm having a difficult time seeing how this fact is useful for my current problem. Using the hint to define $A = (x+y+z)/3$ and $H^{-1} = (x^{-1}+y^{-1}+z^{-1})/3$ , I proved that $A\geq H$ : $$A = \frac{x+y+z}{3}\geq^{(1)} \frac{9xyz}{yz+zx+xy} = \frac{9}{x^{-1}+y^{-1}+z^{-1}}\geq H,$$ where the first inequality (1) holds by Example 2. However, I do not see how this is at all helpful. Here's some of the things that I've tried (that do not involve the hint). Define $x = \sqrt[3]{a}$ , $y = \sqrt[3]{b}$ , and $z = \sqrt[3]{c}$ . Then, observe that $$\frac{a+b+c}{3} - \sqrt[3]{abc} \,=\, \frac{x^{3} + y^{3} + z^{3} - 3xyz}{3} = \frac{(x+y+z)\left((x-y)^{2} + (y-z)^{2} + (z-x)^{2}\right)}{6}\geq 0.$$ Thus, $(a+b+c)/3 \geq \sqrt[3]{abc}$ . I was also able to prove that $(a+b+c)/3\geq\sqrt{(ab+bc+ca)/3}$ , however, I used the fact that $0\leq a < b$ if and only if $a^{2} < b^{2}$ , which I'm not supposed to use. I only include this proof since it may give insight into the problem. (Some of these algebraic ""tricks"" were hard to figure out!) Since $\frac{1}{2}\left((a-b)^{2} + (b-c)^{2} + (c-a)^{2}\right) = (a+b+c)^{2} - 3(ab + bc + ca)\geq 0$ , it follows that $3(a+b+c)^{2}\geq 9(ab+bc+ca)$ . And therefore, $(a+b+c)^{2}/9 \geq (ab+bc+ca)/3 \geq 0$ . Thus, it follows that $(a+b+c)/3\geq\sqrt{(ab+bc+ca)/3}$ . So... to recap, using the appropriate methods, I have prove that $(a+b+c)/3 \geq \sqrt[3]{abc}$ and that $(a+b+c)/3 \geq3/(a^{-1}+b^{-1}+c^{-1})$ . However, the rest is giving me a headache. Perhaps I've missed something obvious, but I'm stuck...","['inequality', 'means', 'real-analysis', 'calculus', 'average']"
4433374,Application of residue at infinity,"I am trying to figure out how to find a contour to solve for this integral $$
\int_{-\infty}^{\infty}\frac{2x}{x^2+x+1}dx = -\frac{2\pi}{\sqrt{3}}
$$ using the residue theorem and the residue at infinity. Without using the idea of residue at infinity, which is equal to $-\frac{1}{2\pi i} \int_{C} f(z)dz$ , I will just choose the upper semi-circle. My question is how to choose a contour whose interior includes all the singularities of the function $\frac{2z}{z^2+z+1}$ ( $-\frac{1}{2}\pm \frac{\sqrt{3}}{2}i$ ) such that it also covers the whole real axis as we take appropriate radii to $0$ or $\infty$ , etc. I am thinking of a contour will be some sort of keyhole, but couldn't figure out a simpler contour. It seems like using residue at infinity makes the question 10 times more complicated than necessary.","['complex-analysis', 'contour-integration', 'residue-calculus']"
4433381,Does inverse mapping theorem holds for an incomplete subspace of a Banach space?,"Let $l^1$ be the space of all absolutely convergent series, and $$f:l^1\to l^1$$ be a $C^1$ (or $C^\infty$ if it is necessary) mapping satisfying $$f(0)=0$$ $$\nabla f(0) = I$$ then the inverse mapping theorem guarantees that $f$ has a $C^1$ inverse map $f^{-1}$ in a small ball $B$ centered at $0$ . My question is that if $X$ is an subspace of $l^1$ and $f(X)\subset X$ , does $f^{-1}(B\cap X)\subset X$ holds? In the case when $X$ is a closed subspace of $l^1$ , its easy because we can just use the inverse mapping on $X$ . However if $X$ is not closed, I have no idea, since the proof of the inverse mapping theorem uses contraction mapping theorem, which does not hold for incomplete space.",['functional-analysis']
4433449,Nearest point of the vertices of a triangle,"I would like to know if the point closest to the three vertices of an equilateral triangle is the centre of its circumcircle, and if so, how to prove it. By closest point, I mean that any other point has at least one distance to one of the vertices of the triangle greater than the radius of the circumcircle, i.e, if a point is not the centre of circumcircle, I want to show that the distance of that point to one of the vertices is greater than the distance of the centre of the circumcircle to the vertices.","['triangles', 'geometry']"
4433477,Compactness of the linear system when varying complex structure,"Let $M$ be a compact, simply connected smooth manifold and $L\rightarrow M$ a complex line bundle over $M$ . Assume there is a continuous path of Kähler structures $(g_t,I_t)\; t\in [0,1]$ on $M$ together with a compatible path of holomorphic structures $J_t$ of the line bundle $L$ . Now we know, that for each fixed $t\in [0,1]$ , the space of holomorhic sections $H^0(M,L)_t$ is finite dimensional, hence the linear system $|L|_t := \mathbb{P}(H^0(M,L)_t)$ is a finite dimensional projective space. Choosing a hermitian metric on $L$ , we can equip the space of all smooth sections $\Gamma(X,L)$ with the topology induced by the supremums norm, inducing a topology on the space $\mathbb{P}(\Gamma(X,L))$ . My question now is: Is the set $\cup_{t\in [0,1]} |L|_t \subseteq \mathbb{P}(\Gamma(X,L))$ compact?","['complex-geometry', 'algebraic-geometry', 'deformation-theory', 'differential-geometry']"
4433481,monotone functions agreeing with Holder functions on a large set,"Let $\alpha \in (0,1)$ , $f:[0,1]\rightarrow \mathbb{R}$ be a continuous monotone function and $\varepsilon>0$ . Does there exist a function $\phi_{\varepsilon} \in \mathcal{C}^{\alpha}$ such that $\lambda(\{t \in [0,1]: f(t)=\phi_{\varepsilon}(t)\})\geqslant 1-\varepsilon$ ? Ideas: A monotone continuous function is a.e. differentiable and therefore by regularity of the Lebesgue measure we can find an arbitrarily small open set that includes all the points where the function is not differentiable. Then one can write this open set as a countable disjoint union of open sets, on which we could ""smoothly interpolate"". Then we have a function that is smooth on this open set, but I don't know how to ensure regularity on the whole interval... The prototype of a counterexample, the Cantor-function, does not work as we can take an approximation with Lipschitz-functions agreeing up to small measure with the Cantor-function. Maybe the ""fat-Cantor-function"" could be an idea of a counterexample.","['measure-theory', 'monotone-functions', 'bounded-variation', 'real-analysis', 'holder-spaces']"
4433498,$ 2 \cos ^{2021} x+\sin ^{2022} x=2 $,"Someone sent me this task, it is meant to be solved quickly, nothing to think too much about. At first I tried representing $\sin ^{2022}x$ as $(\sin ^{2}x)^{1011}$ $$
2 \cdot \frac{\left(\cos ^{1011} x\right)^{2}}{\cos (x)}+\left(1-\cos ^{2} x\right)^{1011}=2.
$$ Then using using $t=\cos x = \sqrt{1-\sin ^{2}x}$ substitution and find $x$ . The expression got too complex so I gave up the idea. After some time considering Taylor's expansion, funtion series, etc I just realized the 'only' case when it satisfies the equality it's when $x=0$ . How would you solve it. How do you solve this equation analyticaly, I mean step by step, line by line. There is no answer proposed in the book.","['contest-math', 'trigonometry', 'sequences-and-series']"
4433503,logic and application to a combinatorics question (Two chess players),"I have a logic question regarding the approach of a combinatorics problem: ""Two chess players, 𝐴 and 𝐵, are going to play 7 games. Each game has three possible outcomes: a win for 𝐴 (which is a loss for 𝐵), a draw (tie), and a loss for 𝐴 (which is a win for 𝐵). A win is worth 1 point, a draw is worth 0.5 points, and a loss is worth 0 points. Assume that they are playing a best-of-7 match, where the match will end when either player has 4 points or when 7 games have been played, whichever is first. For example, if after 6 games the score is 4 to 2 in favor of 𝐴, then 𝐴 wins the match and they don’t play a 7th game. How many possible outcomes for the individual games are there, such that the match lasts for 7 games and 𝐴 wins by a score of 4 to 3?"" This question has being asked and answered here original question Since the original answer is rather long, hence, I am using only part of the answer: Player $A$ wins 3 games, draws 1 game and loses 2 games gives $\binom{6}{3} \binom{3}{1} \binom{2}{2}$ ; for my question. The logic from the posted answer is that player $A$ plans out his game 3 wins, 1 draw, 2 loses and winning the last game, hence, $n$ is adjusted after a grouped outcome. However, in real life situation shouldn't it be $\binom{7}{1} \binom{6}{1} \cdots \binom{1}{1}$ where the outcome is adjusted after every game and player $A$ is only able to do $n-1$ ""choose"" 1 till the 7th game? For clarity $n$ is from $\binom{n}{k}$ , and my question is not about questioning the correctness of the answer from the original post but the approach and logic in live settings. Kindly advise Added info for clarity; Sorry for the lack of vocabulary, there are probably better words/phases than my choice of ""sequential choices"". I'll try to express it in my best effort. $\require{cancel}$ There are 7 games of which the players are to play in sequence, one game after another till the game ends at the 7th game. Therefore, shouldn't it be $\cancel {\binom{7}{1} \binom{6}{1}\binom{5}{1}\binom{4}{1}\binom{3}{1}\binom{2}{1}\binom{1}{1}}$ when the choices are not determined until the current game ends. The logic  from Joffan and user2661923 answers makes sense to me where the 7 games are played simultaneously. Added info after seeing @user2661923's Addendum : Thanks to user2661923's Addendum, I realized that my alternate view of updating after each game should be using win,lose, and tie(WLT) instead(my original intuition = ""strike-thru""). And is very tedious and risk over and under count.","['elementary-set-theory', 'statistics', 'combinatorics']"
4433504,What is a non constructible graph?,"I'm working through ""Groups, Graphs and Trees"" by John Meier. In Chapter 5, he states that in a Cayley Graph, $\Gamma$ (or indeed any graph), the ball of radius n centred at the vertex v, $\mathcal{B}(v,n)$ , is the subgraph formed as the union of all paths in $\Gamma$ of length $\le n$ that start at the vertex $v$ . It is then stated that a Cayley graph Γ is constructible if given $n \in N$ one can construct $\mathcal{B}(e,n)$ in a finite amount of time. My question is what is an example of a non-constructible graph? In particular, it is implied that there are finitely generated groups which have non-constructible Cayley graphs. I cannot comprehend what such a group looks like.","['graph-theory', 'group-theory', 'algebraic-graph-theory']"
4433533,Prove or disprove a converse to l'Hospital's rule,"Assume that: 1) $$ \lim_{x\to c}f(x) = \lim_{x\to c}g(x) = 0, $$ where $c$ is a finite real number contained in an interval $I=(a,b)$ (which means that we have excluded the situation that $c=\pm\infty$ ); $f(x)$ and $g(x)$ are differentiable on the interval $I$ except possibly at the point $c$ above; $$\lim_{x\to c}f^{'}(x) = \lim_{x\to c}g^{'}(x) = 0;$$ $$\lim_{x\to c}\frac{f(x)}{g(x)}$$ exits and is equal to $L$ . Then prove or disprove: $\lim_{x\to c}\frac{f^{'}(x)}{g^{'}(x)}$ exits and is equal to $L$ . It's seems that most counterexamples I have found are under one of the following two circumstances: 1) $c=\pm\infty$ ,  and 2) $c\neq\pm\infty$ but either $\lim_{x\to c}f^{'}(x)$ or $\lim_{x\to c}g^{'}(x)$ is allowed to be nonzero, but it seems harder to find a counterexample given that both circumstances above are excluded . Can anyone be so kind to help me? Thanks in advance!","['limits', 'calculus', 'derivatives']"
4433579,"How to understand Eigenvalue/Eigenvectors of matrices, without considering linear transformations","I am struggling to DEEPLY understand eigenvectors/eigenvalues. Mainly because almost everywhere they are introduced in the context of transformation where we use matrices as functions and eigenvalues and eigenvectors, similar to the roots of polynomials, help us better understand the behavior of the function/transformation. However, my question is what if we don't want to transform anything and the matrix is just a book keeping tool that hosts some information like an image or a dataset. Then what's special about the eigenvector/eigenvalues of such matrices to deserve the name ""Eigen (special)""? The answer to this question gets close when it tries explaining PCA but it doesn't explain how eigenvectors determine orientation and eigenvalues determine distortion of an ellipse! I think matrices (whether as a function or a book keeping tool) have some secrets carved into their eigenvectors/eigenvalues and one only truly understands why those are Eigen (special) if he/she understands both WHY and WHAT secrets are hidden in them. I would really appreciate if anyone could answer these questions.","['matrices', 'eigenvalues-eigenvectors']"
4433613,Minimum area bound by the function,"Let $f:[0,1]->[0,1]$ be a continous function such that $$f(f(x))=1$$ for all $x$ in the domain. Required is the minimum and maximum possible area bound by this function from $x=0$ to $x=1$ As the function is bounded , I start by taking some $\alpha$ as the $x$ corresponding to the max value. Then $f(f(\alpha))=1$ as well, and $1$ is the maximum possible value in itself, $$f(\alpha)=f(1)=1$$ But from here I could not proceed to find the minimum value of $\int_{0}^{1}f(x)$ . A hint on how to proceed from here is highly appreciated. Thank you.","['functions', 'functional-analysis']"
4433624,$x\hat{f}(x)\in L^1(\mathbb{R})$ implies $x\hat{f}(x)\in L^\infty(\mathbb{R})$,Let $\hat{f}(x)=\int_\mathbb{R}f(t)e^{-itx}dt$ be the Fourier transform of the function $f\in L^1(\mathbb{R})\cap L^\infty(\mathbb{R})$ . If $\hat{f}$ has bounded derivative on $\mathbb{R}$ and $$x\hat{f}(x)\in L^1(\mathbb{R})$$ can we say that $$x\hat{f}(x)\in L^\infty(\mathbb{R})?$$,"['lp-spaces', 'fourier-analysis', 'real-analysis']"
4433647,Clique number and chromatic number,"It is known that $\chi(G) \geq \omega(G)$ . However, graph theorists love to sharpen their bounds. Are there known sufficient conditions to ensure that $ \chi(G) = \omega(G)$ , where $\omega(G)$ is the clique number (largest clique in simple undirected graph G) and $\chi(G)$ is the chromatic number of G? (A simple counter-example is an odd-cycle, where $\chi(G) = 3$ but $\omega(G) = 2$ ). I was thinking that if your graph doesn't have more than 1 clique of size $\omega(G)$ , then this has to be true.","['graph-theory', 'coloring', 'discrete-mathematics']"
4433663,Cyclic quotient group as a subgroup?,"There are many questions here along the lines of the following: Let $G$ be a finite group, and $H$ a normal subgroup. Does $G$ admit a subgroup which is isomorphic to $G/H$ ? This is necessarily true if $G$ is abelian, and there are counterexamples if $G$ is not, the most common counterexample being the quaternion group of order 8 (see here ), which has a quotient isomorphic to $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ (but no such subgroup). My question is: if we additionally assume that the quotient $G/H$ is cyclic , then is it the case that $G$ must admit a subgroup isomorphic to $G/H$ ? Or are there counterexamples in this situation also? My gut tells me it's the latter, that it's not necessarily true, but I so far haven't been able to find an example.","['quotient-group', 'group-theory', 'cyclic-groups', 'finite-groups']"
4433691,"What is $P(X+Y>0 \mid X>0)$ given that $X,Y$ two different normal?","X follows $N(0,\sigma_x^2)$ and Y follows $N(0,\sigma_y^2)$ , and $X$ and $Y$ are independent. What is $P(X+Y>0 \mid X>0)$ ? If $\sigma_x^2=\sigma_y^2$ we can use symmetry and easily get answer 3/4, but what if $\sigma_x^2 \neq \sigma_y^2$ ?","['conditional-probability', 'probability-theory', 'probability', 'normal-distribution']"
4433730,Bootstrap proof of Gronwall's inequality (exercise 1.26 in Tao),"I am trying to do Exercise 1.26 in Tao's book on nonlinear dispersive equations. I wanted to check if my proof seemed correct/in the spirit of the exercise (proving Gronwall via a bootstrap), since it is more convoluted than I would have thought. I would be interested if there is a more direct (bootstrap) proof. (More detailed questions listed below.) (Also included at the end is a lemma's proof that is needed, but is unrelated to the bootstrap method, so is included at the end.) Exercise: Use the bootstrap method to give another proof of Gronwall's integral inequality: for $A, B, u \geq 0$ continuous on an interval $[t_1, t_2]$ , if $$
u(t) \leq A + \int_{t_0}^t B(s) u(s)\, ds,
$$ then $$
u(t) \leq A\exp(\int_0^t B(s)\, ds).
$$ Proof of exercise. The proof is by bootstrap argument. Let $H(t)$ be the statement that $$
u(t') \leq (1 + 2\epsilon)A\exp(\int_{t_0}^{t'} B(s)\, ds) \hspace{5mm} \forall t' \in [t_0, t]
$$ and $C(t)$ the statement that $$
u(t') \leq (1 + \epsilon)A\exp(\int_{t_0}^{t'} B(s)\, ds) \hspace{5mm} \forall t' \in [t_0, t].
$$ To complete the bootstrap argument one must show $H(t) \implies C(t)$ . We do this as follows: \begin{align}
u(t) &\leq A + \int_{t_0}^t B(s) u(s)\, ds \\
&\leq A + \int_{t_0}^t B(s)(1 + 2\epsilon)A\exp(\int_{t_0}^s B(\tau)\, d\tau)\, ds \\
&=: F(t).
\end{align} We want to show that this is less than or equal to $(1 + \epsilon)A\exp(\int_{t_0}^t B(s)\, ds)$ , which we call $G(t)$ . To do this, we note that: $$
F(t_0) = A < (1 + \epsilon)A = G(t_0). \tag{1}
$$ Thus we can conclude there is some $\delta > 0$ such that on $[t_0, t_0 + \delta]$ , $F(t) < G(t)$ . Therefore $H(t) \implies C(t)$ , so we have proven the desired result, but only on $t\in [t_0, t_0 + \delta]$ . Since we are given so little hypotheses on $u$ , I don't see how to conclude that $t_0 + \delta$ can be immediately replaced by $t_1$ . Therefore, we need to do the following. First, we note that the above proof can be repeated on $[t_0 + \delta, t_0 + 2\delta]$ to show that $$
u(t) \leq A'\exp(\int_{t_0 + \delta}^t B(s)\, ds) \hspace{5mm} \forall t_0 + \delta \leq t \leq t_0 + 2\delta,
$$ where $$
A' = A + \int_{t_0}^{t_0 + \delta} B(s)u(s)\, ds. \tag{2}
$$ Note that in order to make this $\delta$ uniform one needs in (1) to choose it based on $B$ 's maximum, which is finite. Thus the above proof pushes through to show (2). Next, one needs the following lemma: Lemma. Suppose $t_0 \leq t_1 \leq t_2$ , and suppose we have: $$
u(t) \leq A\exp(\int_{t_0}^t B(s)\, ds) \hspace{5mm} \forall t_0 \leq t \leq t_1 \tag{A}
$$ and $$
u(t) \leq A'\exp(\int_{t_1}^t B(s)\, ds) \hspace{5mm} \forall t_1 \leq t \leq t_2 \tag{B},
$$ where $$
A' = A + \int_{t_0}^{t_1} B(s) u(s)\, ds.
$$ Then we have: $$
u(t) \leq A\exp(\int_{t_0}^t B(s)\, ds) \hspace{5mm} \forall t_0 \leq t \leq t_2. \tag{C}
$$ Essentially, this lemma says that we can ""glue"" the results above on $[t_0, t_0 + \delta], [t_0 + \delta, t_0 + 2\delta]$ to achieve the desired conslusion on $[t_0, t_0 + 2\delta]$ . Since $\delta$ was uniform one can continue until $t_1$ is reached, completing the proof. Questions: 1) Does the above proof seem correct? What I really want to know is if this seems like the intended ""bootstrap method"" proof, which is maybe harder to say (and more subjective). For instance, the bootstrap method came in only as one small portion of the argument; I could not immediately conclude the exercise after using the bootstrap method, more work had to be done. Is there a simpler/shorter proof where the bootstrap method is the main ingredient used? For completeness, here is a proof of the lemma. Note that since (A) implies (C) for $t_0 \leq t \leq t_1$ , we only need to show (C) for $t_1 \leq t \leq t_2$ . By (B), we have, for all $t_1 \leq t \leq t_2$ : \begin{align}
u(t) &\leq (A + \int_{t_0}^{t_1}B(s)u(s)\, ds)\exp(\int_{t_1}^t B(s)\, ds) \\
&= (A + \int_{t_0}^{t_1}B(s)u(s)\, ds)\exp(\int_{t_0}^t B(s)\, ds)\exp(-\int_{t_0}^{t_1} B(s)\, ds).
\end{align} The goal is to show that this is less or equal to $A\exp(\int_{t_0}^t B(s)\, ds)$ . Therefore it suffices to show that $$
F(t_1) := A + \int_{t_0}^{t_1}B(s)u(s)\, ds  \leq A\exp(\int_{t_0}^{t_1} B(s)\, ds) =: G(t_1).
$$ To show this, note that $F(t_0) = A = G(t_0)$ . Also, $$
F'(t) = B(t) u(t), \hspace{5mm} G'(t) = AB(t) \exp(\int_{t_0}^t B(s)\, ds).
$$ But we have already, by assumption (A), that $F'(t) \leq G'(t)$ exactly, for all $t_0 \leq t \leq t_1$ . Therefore integrating we find $F(t_1) \leq G(t_1)$ , which was the desired conclusion.","['inequality', 'solution-verification', 'analysis', 'ordinary-differential-equations']"
4433733,Error In Wade's Analysis (continuous function preserves compactness),"I would like to verify that the following proof is incorrect before raising it in another setting.
This is a proof from Wade's ""An Introduction to Analysis""; I do not take issue with the theorem itself...just this particular proof of it. I will first copy the proof as stated before addressing the issue of its correctness Theorem :  If $ H $ is compact in $\mathbb{R}^n$ and $\mathbf{f}: H \to \mathbb{R}^m$ is continuous on $H$ , then $\mathbf{f} \left(H \right)$ is compact in $\mathbb{R}^m$ Proof : Suppose $ \left\{ V_\alpha\right\}_{\alpha \in A \;} $ is an open covering of $\mathbf{f}\left(H \right)$ . Then $\left\{ \mathbf{f}^{-1} \left( V_\alpha \right)\right\}_{\alpha \in A}$ covers $H$ and, by continuity, its members are relatively open in $H$ . Thus for each $\alpha$ there exists an open set $O_\alpha$ such that $\mathbf{f}^{-1}\left( V_\alpha \right) = H \cap O_\alpha .\;$ Note that $\left\{ O_\alpha \right\}_{\alpha \in A}$ is an  open covering of $H$ and since $H$ is compact, there exists a finite subcovering $\left\{ O_{\alpha_j} \right\}^N_{j=1}$ . We conclude that $$ (*) \quad \quad \quad\mathbf{f}\left( H \right) \subseteq \mathbf{f} \left( \bigcup_{j=1}^N {O_{\alpha_j} \cap H}\right) = \bigcup_{j=1}^N {\mathbf{f} \left( \mathbf{f}^{-1} \left( V_{\alpha_j}\right) \right)} = \bigcup_{j=1}^N V_{\alpha_j}$$ In particular, $\left\{ V_{\alpha_j}\right\}^N_{j=1}$ is a finite subcovering of $\mathbf{f}\left( H \right)$ and thus $\mathbf{f}\left( H \right)$ is compact. $\\$ The Issue : The $\subseteq$ symbol in the line marked above by a $(*)$ is actually an equality. The author notes this in his errata, but this introduces a further problem. We now have all equalities, so that $$\mathbf{f}(H) = \bigcup_{j=1}^N V_{\alpha_j} \quad \text{i.e. compact set = union of open sets}$$ That is, the set we are claiming is compact we have also shown to be open, and this will not be true in general. It must therefore be the case that one of the other equalities is actually a $\subseteq$ . It is actually the rightmost one. Recall that the $V_{\alpha_j}$ are an open covering of the image of $\mathbf{f}, \,$ $\mathbf{f}(H).$ , and will in general need to intersect its complement. That is, some of the $V_{\alpha_j}$ will contain points outside of the range of $\mathbf{f}$ . For those sets $\mathbf{f} (\mathbf{f}^{-1}(V_{\alpha_j}) \neq V_{\alpha_j}$ . The confusion arises because we are taking the preimage of sets that do not completely lie within the function's image. The correct sequence is as follows: $$\mathbf{f}\left( H \right) = \mathbf{f} \left( \bigcup_{j=1}^N {O_{\alpha_j} \cap H}\right) = \bigcup_{j=1}^N {\mathbf{f} \left( \mathbf{f}^{-1} \left( V_{\alpha_j}\right) \right)} = \bigcup_{j=1}^N \left( V_{\alpha_j} \cap \text{Im}(\mathbf{f}) \right) \subseteq \bigcup_{j=1}^N V_{\alpha_j}$$","['general-topology', 'solution-verification', 'real-analysis']"
4433814,Changing the subject - more than one answer?,"Make $x$ the subject of $5(x+2y)=p(2x-3)$ My textbook says that it doesn't matter which side you move the $x$ terms to when solving the problem, but if I put the $x$ terms over to the right-hand side, the answer becomes: $\frac{10y+3p}{2p-5} = x$ Conversely, if I move them over to the left-hand side, the answer becomes: $x = \frac{3p-10y}{5-2p}$ Are both answers correct?",['algebra-precalculus']
4433863,Example of an algebra that is a Banach space but not a Banach algebra,"I'm looking for an example of a space $\mathbb{A} $ such, $\mathbb{A} $ is an algebra; $\mathbb{A}$ is equipped with a norm that makes it a Banach space; $\mathbb{A}$ is not a Banach algebra, i.e., the norm is not submultiplicative. I have not been able to think of any examples for this case, but I believe there must be a space satisfying this. And if I assume that $\mathbb{A}$ has unity, is it still possible to find any examples? Edit: The example given by José made me question whether it is possible to construct an example in which the norm is not equivalent to another norm that turns the space into a Banach algebra.","['banach-spaces', 'banach-algebras', 'normed-spaces', 'examples-counterexamples', 'functional-analysis']"
4433890,"prove that $(0,1)\cap \mathbb{Q}$ is order-isomorphic to $(-\pi,\pi)\cap \mathbb{Q}$","it's a part of my set theory HW. the definition of order-isomorphic in the course: if $(X,\leq),(Y,\leq)$ are orderly sets than $X \simeq Y$ if there is exist $f:X \to Y$ bijecton surjectiove and if $x_1 , x_2 \in X$ such that $x_2 > x_1$ then $f(x_2) > f(x_1)$ i saw that question: Prove that $(0 ,1)\cap\mathbb{Q}$ is order-isomorphc to $\mathbb{Q}$: is my proof correct? and i didn't figure out how to use that for my question i tried to prove a lemma: for all $a,b\in \mathbb{Q}$ such that $a<b$ and for all $c,d \in \mathbb{Q}$ such that $c<d$ : $(a,b)\cap \mathbb{Q} \simeq (c,d)\cap \mathbb{Q}$ i've proved the lemma and then define $f:(0,1)\cap \mathbb{Q} \to (-\pi,\pi)\cap \mathbb{Q}$ such that f(0)=0 and to use rational sequences that one is strictly decreasing to $-\pi$ and the other one is strictly decreasing to $0$ and build with that sequences two sequences $a_n,b_n$ such that $a_n <0$ for all $n\in \mathbb{N}$ and $a_n \stackrel{n\to\infty}{\to} -\pi$ and $b_n < \frac{1}{2}$ for all $n\in \mathbb{N}$ and $b_n \stackrel{n\to\infty}{\to} 0$ and define isomorphic functions $f_n : (b_{n+1}, b_n)\cap \mathbb{Q} \to (a_{n+1},a_n)\cap \mathbb{Q}$ for all $n\geq 2$ and for $n=1$ define $f_1: (b_1,\frac{1}{2}) \cap \mathbb{Q} \to (a_1, 0)\cap \mathbb{Q}$ and that the Union of all functions its the function i need (of course before that i need to do the same with $(\frac{1}{2},1)$ and $(0,\pi)$ ) but i need to union an infinitely amount of functions and i don't know if it's ok. i will be glad if someone give me a clue how to continue with my idea or give me an alternative idea to solve that one. thank you all in advance.","['order-theory', 'functions']"
4433937,"Evaluating $\lim_{x\to0}\,3^{(1-\sec^220x)/(\sec^210x-1)}$","I want to evaluate $$\lim_{x\to0}\;3^{(1-\sec^220x)/(\sec^210x-1)}$$ So far these have been my ideas, feel free to correct me: Find that if directly applied to the function, $x_0$ will cause indetermination. Manipulate the function trigonometrically, since $\sec x=\frac{1}{\cos x}$ , giving me: $1-\frac{1}{\cos^220x}$ and $\frac{1}{\cos^210x}-1$ respectively in the numerator and denominator of the exponent. Apply the property $\lim_{x\rightarrow0}a^{f(x)}=a^{\lim_{x\rightarrow0}f(x)}$ Then apply the property that allows me to split the limit of f/g(x) into \lim f(x)/\lim g(x) This is the last step I thought of, finding the LMC. I'm not sure where to go now, so I'd appreciate some insight.","['indeterminate-forms', 'limits', 'calculus', 'trigonometry']"
4433991,"$\iint_D(x^2-y^2)dxdy$ with D enclosed by $y=\frac2x$, $y=\frac4x$, $y=x$, $y=x-3$?","I have been presented with the following problem:
Calculate the double integral $$\iint_D(x^2-y^2)dxdy$$ where D is the area enclosed by the curves $y=\frac2x$ , $y=\frac4x$ , $y=x$ , and $y=x-3$ . Here's a visualisation of the area D: https://i.sstatic.net/Y7ewp.png I've been trying to use various substitutions, but I always fail in finding the new area or what x and y should be substituted with. I'm kind of stuck and would like a pointer in what I should substitute (or if I should even you substitution to begin with).
Thanks in advance!","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4434089,A sequence in $L^p$ that converges in measure but not weakly and vice versa,"I am able to cook up an example A sequence in $L^p$ that converges in measure but not weakly. Namely, let $f_n = n \chi_{[0, 1/n]}$ for $n \in \mathbb{N}.$ Then we have that $\{f_n \}$ converges in measure on $[0, 1]$ but it does not converge weakly in $L^p([0, 1]).$ My question is can we cook up an example for the reverse? i.e. can we find a sequence of functions in $L^p$ that converges weakly but not in measure?","['measure-theory', 'lp-spaces', 'convergence-divergence', 'weak-convergence']"
4434091,"Is the tangent plane to the surface $x^4+y^4-z^2=0$ at $(0,0,0)$ undefined?","In calculus, the tangent plane to a surface $f(x,y,z)=0$ at $(x_0,y_0,z_0)$ is defined as the plane passing through $(x_0,y_0,z_0)$ with the normal vector $\mathrm{grad} f(x_0,y_0,z_0)$ . According to this description, the tangent plane to the surface $x^4+y^4-z^2=0$ at $(0,0,0)$ should be undefined as the gradient of $x^4+y^4-z^2$ is the zero vector at $(0,0,0)$ . However, intuitively speaking, it seems that the plane $z=0$ should be the tangent plane to this surface at $(0,0,0)$ : So in this case, is the plane $z=0$ the tangent plane to the surface $x^4+y^4-z^2=0$ at $(0,0,0)$ , or is it undefined since the gradient at $(0,0,0)$ is the zero vector?","['multivariable-calculus', 'calculus', 'tangent-spaces']"
4434099,Rotationally invariant function after tensorizing,"Suppose that $f$ is a real valued function such that $$f(x)f(y)=f((x+y)/\sqrt 2) f((x-y)/\sqrt 2)$$ for all real numbers $x,y$ . Then is it necessarily true that $f(x)=Ae^{bx^2}$ for some real numbers $A,b$ ? I think I can prove it to be true if $f$ is nonnegative and integrable, though I’m not sure of a strategy to tackle less well behaved functions, perhaps there’s a weird $f$ that changes sign or has local blowups or is even nonmeasurable but satisfies that identity.","['functional-equations', 'harmonic-analysis', 'real-analysis', 'functions', 'probability']"
4434101,Lebesgue measurable sets which have the same measure when intersected with intervals of the same length have measure $0$ or cozero,"I was wondering if the following is true: Let $m$ be the Lebesgue measure and let $E$ be a Lebesgue-measurable subset of the reals $\mathbb R$ .
Suppose that for every two intervals $I, J$ such that $m(I)=m(J)$ we have $m(E\cap I)=m(E\cap J)$ . Then
either $m(E)=0$ or $m(\mathbb R\setminus E)=0$ . My intuition tells me that this is true, but I was not able to prove it. I do know that for every $\alpha \in (0, 1)$ there exists an interval $I$ such that $m(I\cap E)\geq \alpha m(I)$ .","['measure-theory', 'lebesgue-measure']"
4434123,How many ten digit numbers have the sum of their digits equal to $4$?,"How many ten digit numbers have the sum of their digits equal to $4$ ?
Well, my solution goes like this: The first digit of the $10$ digit number cannot be $0$ .
It can be $1,2,3$ or $4$ only .
If the first digit is $1$ then we can have $3$ other digits as $1$ so as the sum of digit is $4$ . This can be done in $9\choose3$ ways . Now when the 1st digit is $1$ then we can have two digits $2$ and $1$ as another case such as sum of digit remains $4$ . So, this can be done in $9\choose 2$ ways . Now, another case can be the one when 1st digit is $4$ but another digit is $3$ . This can be done in $9\choose 1$ ways . So the number of ways when 1st digit is $1$ is $9\choose3$$+$$9\choose2$$+$$9\choose1$ ways.
Now, if the 1st digit is $2$ then the other two digits can be $1$ each . This can done in $9\choose2$ ways . If the 1st digit is $2$ the other digit can be $2$ . This can be done in $9\choose1$ ways.The total of ways this can be done is $9\choose2$$+$$9\choose1$ ways. If the 1st digit is $3$ then another digit among those $9$ digits must be $1$ .this can be done in $9\choose1$ ways. If the 1st digit is $4$ then all the other digits are zero . This can be done in $1$ way only. So, the total number of ways in which the sum of digits can be $4$ in a $10$ - digit number is $9\choose3$$+$$9\choose2$$+$$9\choose1$$+$$9\choose2$$+$$9\choose1$$+$ $9\choose1$$+$$1$ ways $ =184$ ways However the answer in the book is given as: $1+2$$9\choose 1$$+$$9\choose1$$+$$9\choose2$$3!$$/2!$$+$$9\choose 3$$=$$220$ ways Where is the mistake? Where is the problem occuring?",['combinatorics']
4434147,Find $\lim_{n \to \infty}a_n$ where $a_1=1$ and $a_{n+1}=a_n+\frac{1}{2^na_n}$.,"There are some attempts as follows. Obviously, $\{a_n\}$ is increasing. Thus $a_n\ge a_1=1$ . Therefore $$a_{n+1}-a_n=\frac{1}{2^na_n}\le \frac{1}{2^n},$$ which gives that $$a_n=a_1+\sum_{k=1}^{n-1}(a_{k+1}-a_k)\le a_1+\sum_{k=1}^{n-1}\frac{1}{2^k}=2-\frac{1}{2^{n-1}}.$$ Hence, $a_n\le 2$ . Similarily, $$a_{n+1}-a_n=\frac{1}{2^na_n}\ge \frac{1}{2^{n+1}},$$ which gives that $$a_n=a_1+\sum_{k=1}^{n-1}(a_{k+1}-a_k)\ge a_1+\sum_{k=1}^{n-1}\frac{1}{2^{k+1}}=\frac{3}{2}-\frac{1}{2^{n}}.$$ Put the both aspects together, we have $$\frac{3}{2}-\frac{1}{2^n}\le a_n\le 2-\frac{1}{2^{n-1}}.$$ But this does not satisfy the applying conditions of the squeeze theorem. Any other solutions?","['limits', 'calculus', 'sequences-and-series']"
4434162,The suspension of $\mathbb{C}P^2$ is not homotopy equivalent to $S^3 \vee S^5$,"I am looking for a relatively simple way to see that $\Sigma \mathbb{C}P^2$ is not homotopy equivalent to $S^3 \vee S^5$ . Both the Euler characteristic of a space and its homology groups are invariant under homotopy equivalence, so I tried to consider these concepts. However, the Euler characteristic of both spaces is equal to $-1$ , and the homology groups of both spaces are also isomorphic. In the answer posted here , the author considers Steenrod squares to show that the homotopy equivalence can't exist, but this is a topic that I'm not quite familiar with. Should I try to use the Freudenthal suspension theorem or something related to homotopy groups? Or, perhaps, cohomology rings and the cup product? I do know that the cup product of $H^{\ast}(\Sigma \mathbb{C}P^2)$ is trivial (see the result here ); can I show that the cup product in $H^{\ast}(S^3 \vee S^5)$ is nontrivial? Edit: As commented below, the cup product in $H^{\ast}(S^3 \vee S^5)$ is also trivial, so this won't help here, either. Thanks!","['homotopy-theory', 'general-topology', 'homology-cohomology', 'algebraic-topology', 'projective-space']"
4434241,Weaker assumptions to define an equivalence relation,"Given a set $A$ , the standard definition (at least Wikipedia's one) says that a binary relation $\sim$ on $A$ is an equivalence relation iff: $\forall a\in A \quad a\sim a\quad$ (reflexivity); $\forall a,b\in A\quad a\sim b\;\implies\; b\sim a\quad$ (symmetry); $\forall a,b,c\in A\quad a\sim b$ and $b\sim c\;\implies a\sim c\quad$ (transitivity). I had been believing for a while that the first assumption (reflexivity) was unnecessary, given the other two. Indeed, if $a\sim b$ , then $b\sim a$ by symmetry, and by transitivity $a\sim a$ . However, I've then realised that actually these two conditions are not enough. Indeed, taking $A=\mathbb R$ and $a\sim b$ iff $ab>0$ we get that $0$ is not equivalent to $0$ , and so the reflexivity does not hold. The issue in the above example is that in order to use the symmetry and the transitivity to show the reflexivity for an element $a$ , we must assume that there exists an element $b\in A$ such that $a\sim b$ or $b\sim a$ . This observation leads to the following alternative definition. $\sim$ is an equivalence relation on $A$ iff: $\forall a\in A \quad \exists b:a\sim b$ or $b\sim a\quad$ (existence); $\forall a,b\in A\quad a\sim b\;\implies\; b\sim a\quad$ (symmetry); $\forall a,b,c\in A\quad a\sim b$ and $b\sim c\;\implies a\sim c\quad$ (transitivity). The two definitions are equivalent, but the single existence condition, without symmetry and transitivity, is weaker than the reflexivity assumption, as the latter implies the former. Are there other ways to make the assumptions in this definition weaker? By this I mean, is it possible to replace the existence, the symmetry or the transitivity with a weaker assumption (or possibly a set of weaker assumptions), and still recover the equivalence relation? Alternatively, if this is already a ""minimal"" definition (in the sense that none of the conditions can be replaced by a weaker one) can we find other weaker definitions of equivalence relation by relaxing the symmetry and transitivity? Is it possible to state all the ""minimal"" definitions of equivalence relation? Disclaimer: I am not familiar at all with mathematical logic. I believe this question fits in the logic tag, but if this is not the case please feel free to change it. When I speak about ""minimal definition"" I intuitively mean that each assumption $\alpha$ in the definition cannot be replaced by a weaker set of assumptions $\{\alpha'_i\}$ (where each $\alpha'_i$ is implied by $\alpha$ , but alone does not implies $\alpha$ ) which still leads to the same definition. Hopefully this makes sense, although I am not sure how to really formalise it.","['elementary-set-theory', 'equivalence-relations', 'logic']"
4434255,performing operations on a set to obtain desired sequence of sets,"Given $S \subseteq \{1,\ 2,\ \dots \ ,\ n\} $ and the following operations: if $1 \notin S, $ add the element $1$ ; if $n \in S, $ delete the element $n$ ; for $1 \leq r \leq n-1,\ $ if $r \in S\ $ and $r+1 \notin S, $ delete the element $r$ and add the element $r+1$ ; I'm wondering whether it's possible by these operations for some $n$ to obtain a sequence $$\emptyset \longrightarrow \{1\} \longrightarrow \{2\} \longrightarrow \dotsm \longrightarrow \{n\}, $$ starting with $\emptyset$ and ending with $\{n\},$ in which each of the $2^n$ subsets of $\{1,\ 2,\ \dots \ ,\ n\}$ appears exactly once. For which n is it possible? In what succession can we use the operations to obtain the desired sequence? Can we prove that it's not possible for any $n$ ? Note: I was working on a simpler problem and proved that $n$ being a number of the form $2^m-1$ is a necessary condition for it and now I'm trying to see if there actually exists such a number $n$ (or potentially a sufficient condition). So far my intuition and trial-and-error suggest that it's not possible.","['number-theory', 'combinatorics', 'elementary-number-theory']"
4434334,Peculiar nature of the different marks obtainable in a test,"In an examination of $20$ questions, a student gets $-1,0,4$ marks for incorrect, unattempted, and correct answers respectively. Let $S$ be set of all the marks that a student can score. What is the number of distinct elements in $S$ ? In the range of $101$ marks $-20,-19,\cdots80$ , the marks $\{+79,+78,+77,+74,+73,+69\}$ are unobtainable, leaving us $95$ elements in $S$ . I find this rather strange. What is so special about this set of marks that makes them unobtainable? I couldn't find any pattern in them, but they are very close to each other, lying around $70$ .","['number-theory', 'combinations']"
4434374,How to show that the solutions to one ODE give the solutions to an other?,"While working on a differential geometry problem I have come across a second order ODE that I am trying solve: \begin{equation}1+(h')^2-hh''=2ch(1+(h')^2)^{3/2}\end{equation} It turns out that is equation can't really be solved analytically but I am told that the following first order ODE gives all the solutions to the second order ODE: \begin{equation}h^2\pm\frac{h}{c\sqrt{1+(h')^2}}=\pm b^2\end{equation} where b is some real constant. The problem is that this equation can't be solved analytically either.
I must show that this first order ODE does indeed give all the solutions to the second order equation. So far I haven't been successful in doing so but I will list what I have done (my attempts have been a little desperate and I haven't been very confident with any of them): I've isolated for h' in the first order ODE, calculated h'' and plugged this in to both sides of the second order ODE to see if both sides are equal. This was done using math software (maple) but it didn't seem like the sides were equal. I've assumed that h must satisfy some other condition such that the second order ODE is simplified in a way that gives us the first order equation. For instance, I've assumed h must also satisfy $h''=k^2h$ in which case this second order ODE becomes a first order one. I did this because I figured that as long as I get two linearly independent solutions to the second order ODE, I have spanned the entire solution space. If my first order equation above can get me these two solutions I have unlocked all the solutions to the second order equation.
Is there a general way that I can show that these two equations share solutions? What would you do from here? What can I try in order to show that the solutions to the first order equation give me the solutions to the second order equation? PS: I am also quite confused about how a first order equation can have several linearly independent solutions. I don't see how it can. Am I expected to construct another linearly independent solution from the one that I get?",['ordinary-differential-equations']
4434384,Complement of small enough $\varepsilon$-neighbourhood of boundary is connected,"Let $U \subset \mathbb{R}^n$ be an open, bounded and connected subset of $\mathbb{R}^n$ for which $\partial U$ is smooth. I would like to show that for $\varepsilon > 0$ small enough, the set $$U_\varepsilon := \{x \in U \ \mid \ d(x, \partial U) > \varepsilon \} $$ is connected. However, I have not managed to do so. I have seen that there are counter-examples ( Is for open connected $U$ the set $U_\varepsilon$ for small $\varepsilon$ connected? ) if the assumption that $\partial U$ is smooth is dropped. However, I do not see how the assumption that $\partial U$ is smooth can be used to prove that $U_\varepsilon$ is connected. I suppose that this is the same as proving that the complement of a collar neighbourhood of $\partial U$ is connected (hence why $\partial U$ needs to be smooth), but I do not know how to proceed. This question comes up when proving, for instance, that if $u$ is a Sobolev function on $U$ with vanishing weak gradient, then it must be constant almost everywhere (the argument involves mollifying $u$ with a cut-off function, and the resulting function has vanishing weak gradient and is defined only on $U_\varepsilon$ ).","['general-topology', 'differential-topology', 'partial-differential-equations']"
4434496,Riesz representation theorem and integral equality in a Hilbert space [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm having trouble to understand how to tackle the following problem: Lets consider the Hilbert space $H = L^2(]0,1[)$ . Show that there exist a unique $u \in L^2(]0,1[)$ such that: $$
  \int_0^1
    \left( \int_0^xu(t) \,\mathrm{d}t \right)
    \left( \int_0^x v(t) \,\mathrm{d}t \right)
  \,\mathrm{d}x
  +
  \int_0^1 u(x)v(x) \,\mathrm{d}x
  =
  \int_0^1 f(x) v(x) \, \mathrm{d}x \,,
$$ for $f \in H$ given. I'm unsure how to even start, I know I will use Riesz representation theorem and that I need a linear form of $H$ but this integral equality is really disturbing me. Any hints or explanation would be appreciated. Thank you !","['hilbert-spaces', 'functional-analysis']"
4434498,Question about a step in a derangement proof,I read a proof about derangement and I didn't understand this step: $d_n - nd_{n-1} = -(d_{n-1}-(n-1)d_{n-2}) \implies d_n=nd_{n-1}+(-1)^n$ I see that we have $S_n = -S_{n-1}$ if $S$ is the stuff on one side of the equation but I don't get what happened then. Where does $(-1)^n$ come from?,"['derangements', 'recurrence-relations', 'discrete-mathematics']"
4434500,Homomorphisms from $S_3$ to $\mathbb{Z}/10\mathbb{Z}$,"I want to check if my line of thought is correct. We need to find all homomorphisms $\phi: G=S_3\rightarrow H=\mathbb{Z}/10\mathbb{Z}$ . We already know that $\phi(g)=\bar{0}$ for all $g\in G$ is a possible homomorphism, so we can assume that $\phi(g)=\bar{m}$ for some $0<m\leq 9$ . Now, if $n$ is the order of $g$ , $n=o(g)$ , we have that $n\bar{m}=\bar{0}$ so that $o(\bar{m})\mid n$ and, therefore, $o(\bar{m})$ is a common divisor of $10$ and $6$ , hence either $1$ or $2$ . Since we are excluding $m=0$ , we get that $m=5$ , that is, if $\phi$ is a non-trivial homomorphism from $G$ to $H$ we must carry some non-trivial element of $G$ to the element $\bar{5}$ in $H$ . Also, since $o(\bar{m})\mid n$ we must have that $n$ is an even number, and since $n\mid 6$ we can only have $n=2$ . The only elements in $S_3$ with order $2$ are the tranpositions $(12),(13),(23)$ . Hence, a non-trivial homomorphism must take some transposition to $\bar{5}$ . Now, it is easy to check that the multiplication of two tranpositions, $\tau_1\neq\tau_2$ is a $3\text{-cycle}$ . Hence: $$\bar{0}=\phi(3\text{-cycle})=\phi(\tau_1\tau_2)=\phi(\tau_1)+\phi(\tau_2)$$ The only way to make this equation work, since $\phi(\tau)=\bar{0},\bar{5}$ is that both $\phi(\tau_1)=\phi(\tau_2)=\bar{5}$ , hence if $\phi$ is a non-trivial homomorphism we must have that $\phi(\tau)=\bar{5}$ for all transpositions $\tau\in S_3$ and $\phi(g)=\bar{0}$ if $g$ is not a tranposition. It is easy to check that this is in fact a homomorphism, and because of the necessity of this conditions, we must have that $\text{Hom}(G,H)=\{\text{ trivial },\phi\}$ . Is there any flaw with my logic? Did I get all possible homomorphisms? Thanks for any help","['group-homomorphism', 'finite-groups', 'solution-verification', 'symmetric-groups', 'group-theory']"
4434514,Calculate kernel space of matrix map and its orthonormal space [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The question asks: a) What is kernel space of linear map defined by $$
M =
\begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
3 & 6 & 9 \\
\end{bmatrix}
$$ b) Give orthonormal basis for map's kernel from a) My attempt of a) I attempted to find the kernel space, but I am not sure if I am right, furthermore, I don't know how to approach b). Thank you for any help","['matrices', 'vector-spaces']"
4434531,Solve Non-Linear DOE with Dirac Delta Function in the denominator,"I found a differential ordinary equation that uses Diric Delta Function that I don't know how to solve. Thank you if you can help me. The ODE is like that $$
z' = \rho \cdot \sqrt{1+z^2} + w \cdot \delta(x-x_0)
$$ And my domain is $\left[a, \ b\right]$ with $a < x_0 < b$ . With $\rho$ and $w$ constants. How can I solve it? The solution I tried was: $$
\dfrac{z'}{\rho \cdot \sqrt{1+z^2} + w \cdot \delta(x-x_0)} = 1
$$ $$
I = \int_{a}^{b} \dfrac{z'}{\rho \sqrt{1+z^2}+w\delta} dx = \int_{a}^{b} 1 \ dx = b-a
$$ Now I divide the domain $\left[a, \ b\right]$ in three domains $\left[a, \ x_0 - \varepsilon\right] \cup \left[x_0 - \varepsilon, \ x_0 + \varepsilon\right] \cup \left[x_0 + \varepsilon, \ b\right]$ with $\varepsilon > 0$ small and then make three integrals separately $I_1$ , $I_2$ and $I_3$ . $$
I = \lim_{\varepsilon \to 0^{+}} I_{1}(\varepsilon)+I_{2}(\varepsilon)+I_{3}(\varepsilon) = \lim_{\varepsilon \to 0^{+}} \left(\underbrace{\int_{a}^{x_0 - \varepsilon} \square \ dx}_{I_1(\varepsilon)} + \underbrace{\int_{x_0 - \varepsilon}^{x_0 + \varepsilon} \square \ dx}_{I_2(\varepsilon)} + \underbrace{\int_{x_0 + \varepsilon}^{b} \square \ dx}_{I_3(\varepsilon)}\right)
$$ With \begin{align*}
I_{1}(\varepsilon) & = \int_{a}^{x_0-\varepsilon} \dfrac{z'}{\rho \sqrt{1+z^2}+w\underbrace{\delta}_{0}} dx  = \dfrac{1}{\rho}\int_{z(a)}^{z(x_0-\varepsilon)} \dfrac{dz}{\sqrt{1+z^2}} = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(x_0-\varepsilon)} \\
I_2(\varepsilon) & = \int_{x_0-\varepsilon}^{x_0+\varepsilon} \dfrac{z'}{\rho \sqrt{1+z^2}+w\delta} dx  \overset{\underset{\mathrm{?}}{}}{=}  \int_{x_0-\varepsilon}^{x_0+\varepsilon} 1  \ dx = 2\varepsilon  \\
I_{3}(\varepsilon) & = \int_{x_0+\varepsilon}^{b} \dfrac{z'}{\rho \sqrt{1+z^2}+w\underbrace{\delta}_{0}} dx  = \dfrac{1}{\rho}\int_{z(x_0+\varepsilon)}^{z(b)} \dfrac{dz}{\sqrt{1+z^2}} = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0+\varepsilon)}^{z(b)}
\end{align*} And therefore \begin{align*}
b - a = I & = \lim_{\varepsilon \to 0^{+}} I_1 + I_2 + I_3 \\ 
& = \lim_{\varepsilon^{+}} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(x_0-\varepsilon)}  + 2 \varepsilon + \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0+\varepsilon)}^{z(b)} \\
& = \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(b)} + \lim_{\varepsilon \to 0^{+}} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(x_0-\varepsilon)}^{z(x_0+\varepsilon)} \\
& \overset{\underset{\mathrm{?}}{}}{=} \dfrac{1}{\rho} \ln \left(z + \sqrt{z^2-1}\right)_{z(a)}^{z(b)}
\end{align*} I thought about using another function in the place of $\delta$ , like $$
\delta_\varepsilon(x-x_0) =
\begin{cases}
0 \ \ \ \text{if} \ x < x_0 - \varepsilon \\
\frac{1}{2\varepsilon} \ \ \ \text{if} \ x_0 - \varepsilon < x <  x_0 + \varepsilon \\
0 \ \ \ \text{if} \ x_0 + \varepsilon < x \\
\end{cases}
$$ or even a continuous function, but it's harder(or impossible) to calculate the integral: $$
\delta_\varepsilon(x-x_0) =
\dfrac{1}{\varepsilon \sqrt{\pi}} \exp\left(-\dfrac{1}{\varepsilon^2}(x-x_0)^2\right)
$$ Unfortunately, the physical problem (of this ODE) suggests that $z$ has a linear term of Heaviside Function at the point $x_0$ .","['definite-integrals', 'dirac-delta', 'ordinary-differential-equations']"
4434537,Least squares: can't find why $SSR = a_1 \sum Y_i + b_1 \sum Y_i X_i - n\overline Y^2$. Would anyone have references?,"A past lecture introduced the concept of the sum of the squared differences between the dependent variable's mean and its estimated values, $SSR := \sum \left(\hat Y_i - \overline Y\right)^2$ . My lecturer's notes offer the additional following formula for the sum of squares due to regression: $$SSR = a_1 \sum Y_i + b_1 \sum Y_i X_i - n\overline Y^2$$ where $a_1, b_1$ are the coefficients for the regression line $\hat Y_i = a_1 + b_1 X_i$ , and $\overline X, \overline Y$ are the arithmetic means for the values of the data set, $X_i, Y_i$ . My lecturer's notes offer no additional explanations (it's a statistics for economics class), and no matter how much I try to tinker with Gauss' normal equations, I can't figure out how exactly they arrive at this result. Would anyone have a reference I could use, or an explanation of how this equation comes about?","['regression', 'statistics', 'least-squares', 'reference-request']"
4434551,How to place optimally four electrons on a sphere?,"$\newcommand{\S}{\mathbb{S}^2}$ Let $x_1,x_2,x_3,x_4 \in \mathbb{S}^2$ be points on the unit sphere, that minimizes the quantity $$
E(x_1,x_2,x_3,x_4)=\sum_{i < j}\frac{1}{\| x_i - x_j \|},
$$ where $\| x_i - x_j \|$ denotes the Euclidean distance in $\mathbb{R}^3$ . $E$ is the electrostatic potential energy of $4$ electrons constrained to lie on the unit sphere. It is claimed in various sources that the $x_i$ are the vertices of a regular tetrahedron. (see e.g. Wikipedia ). Question: I would like to find a reference for a proof of that fact. (Or a self-contained proof produced here, if it is not too large). I found various sources on this problem, known as the Thomson problem, but could not actually find a paper containing a proof of the four-particle case. Edit: Here is a proof that the tetrahedron $(x_1,x_2,x_3,x_4)$ is at a local minimum: Let $\beta(t)$ be a path in $\mathbb{S}^2$ , $\beta(0)=x_4, \dot \beta(0)=w \in T_{x_4}\S$ . Consider the path $\alpha(t)=(x_1,x_2,x_3,\beta(t))$ . Since $\alpha$ changes only the fourth component, we get $$
\frac{d}{dt}E(\alpha(t))=\sum_{i=1}^3\frac{d}{dt}| x_i - \beta(t) |^{-1}.
$$ $$
\frac{d}{dt}| x_i - \beta(t) |^{-1}=\frac{d}{dt}(| x_i - \beta(t) |^2)^{-\frac{1}{2}}=| x_i - \beta(t) |^{-3}\langle x_i,\dot \beta(t)\rangle. \tag{1}
$$ Differentiating again, we get $$
\frac{d^2}{dt^2}| x_i - \beta(t) |^{-1}=| x_i - \beta(t) |^{-3}\big(3| x_i - \beta(t) |^{-2}\langle x_i,\dot \beta(t)\rangle^2+\langle x_i,\ddot \beta(t)\rangle \big)\tag{2}.
$$ In particular, for a tetrahedron, we get $$
dE_p(0,0,0,w)=\sum_{i=1}^3 | x_i - x_4 |^{-3}\langle x_i,w\rangle=a^{-3}  \langle \sum_{i=1}^3 x_i,w\rangle= 
\langle -x_4,w \rangle=0,
$$ where we used the facts that $\sum_{i=1}^4 x_i=0$ , and $\langle x_4,w \rangle$ , since $x \in T_{x_4}\S$ . Here $a=\sqrt\frac{8}{3}$ is the edge length of the tetrahedron. Moreover, $$
a^{3}\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3a^{-2}\sum_{i=1}^3\langle x_i,w\rangle^2+\langle \sum_{i=1}^3x_i,\ddot \beta(0)\rangle.
$$ Differentiating twice $\langle \beta(t),\beta(t) \rangle=1$ , we get $ \langle x_4,\ddot \beta(0) \rangle=-|w|^2$ , so $$
\frac{d^2}{dt^2}|_{t=0}E(\alpha(t))=3a^{-2}\sum_{i=1}^3\langle x_i,w\rangle^2+|w|^2 \ge 0,
$$ and is strictly positive whenever $w \neq 0$ . This proves that the tetrahedron is indeed a point of local minimum.","['geometry', 'physics', 'reference-request', 'multivariable-calculus', 'optimization']"
4434561,Geometric interpretation of $A^TA$,"For a transformation $A \in \mathbb{R}^{n\times m}$ what exactly is the geometric interpretation of the transformation $A^TA$ . If I understand it correctly the entries of $A^TA$ are the inner products or the columns of $A$ but how exactly should I interpret this geometrically as a linear transformation? And why is $A^TA$ often loosely called squaring the matrix, how does having the pairwise inner products (which normally are interpreted as projecting one vector on the other) yield us something close to a matrix squared? One thing I've noticed is that using the SVD for a real matrix $A$ we get $A = UDV^T \leftrightarrow A^TA=VD^TU^TUDV^T=VD^TDV^T$ where $U$ and $V$ are orthogonal, but how does changing basis to $V$ and scaling by the eigenvalues squared relate to the concept above?","['svd', 'matrices', 'linear-algebra', 'linear-transformations', 'geometric-interpretation']"
4434564,How to compute the limit $\lim\limits_{\alpha \to \pi} \frac{\sin{\alpha}}{\sqrt{2(1+\cos{\alpha})}}$?,"My question is simply how do we compute the limit $\lim\limits_{\alpha \to \pi} \frac{\sin{\alpha}}{\sqrt{2(1+\cos{\alpha})}}$ ? For context on where this came from, consider the following problem. Spivak, Chapter 11, problem 18: Ecological Ed must cross a circular lake of radius 1 mile. He can row
across at 2mph or walk around at 4mph, or he can row part way and walk
the rest. What route should he take so as to i) see as much scenery as possible i) seems to be asking what the longest path is. The solution manual says that this is obviously the going around the semicircle, ie walking around. I'd like to prove this. $$a^2=c^2+d^2$$ $$d=\sqrt{a^2-c^2}$$ $$r^2=c^2+(a+d)^2$$ $$=2a^2+2a\sqrt{a^2-c^2}$$ $$\sin{\alpha}=\frac{c}{a} \implies c =a\sin{\alpha}$$ Therefore $$r^2(\alpha)=2a^2+2a\sqrt{a^2-a^2\sin^2{\alpha}}$$ $$=2a^2(1+\sqrt{1-\sin^2{\alpha}})$$ $$=2a^2(1+\cos{\alpha})$$ Also $$w(\alpha)=\alpha a$$ The total distance is therefore $$l(\alpha)=r(\alpha)+w(\alpha)$$ $$=a\sqrt{2(1+\cos{\alpha}} +a\alpha$$ $$=a(\alpha+\sqrt{2(1+\cos{\alpha}})$$ Taking the derivative $$l'(\alpha)=a(1+\frac{-2\sin{\alpha}}{2\sqrt{2(1+\cos{\alpha}})}), \alpha \neq \pi$$ $$=a(1-\frac{\sin{\alpha}}{\sqrt{2(1+\cos{\alpha})}}), \alpha \neq \pi$$ $$l'(\alpha)=0 \implies \sin{\alpha}=\sqrt{2(1+\cos{\alpha})}, \alpha \neq \pi$$ If we square both sides $$\sin^2{\alpha}=2(1+\cos{\alpha}$$ $$1-\cos^2{\alpha}=2(1+\cos{\alpha})$$ $$\cos^2{\alpha}+2\cos{\alpha}+1=0$$ $$\Delta=4-4=0$$ $$\cos{\alpha}=\frac{-2}{2}=-1$$ $$\alpha=\pi$$ Therefore, since $l'$ isn't defined at $\pi$ , this isn't a critical point. In fact we can see that for $\alpha \in [0,\pi)$ , $l'>0$ . This would seem to indicate that the longest trajectory is at $\pi$ , one of the endpoints of the domain of $l$ . $$l(\pi)=\pi a = \frac{2\pi a}{2}$$ which makes sense. (note that $l(0)=2a$ ). I am curious about what happens to $l'$ when $\alpha$ approaches $\pi$ . To that end, I need to compute the limit $$\lim\limits_{\alpha \to \pi} \frac{\sin{\alpha}}{\sqrt{2(1+\cos{\alpha})}}$$ But how? L'Hopital doesn't seem to work. EDIT: My calculations, based on the hint given by Christophe Leuridan: $$1+\cos{\alpha}=2\cos^2{\alpha/2}$$ $$\sin{\alpha}=2\sin{\alpha/2}\cos{\alpha/2}$$ $$\lim\limits_{\alpha \to \pi} \frac{\sin{\alpha}}{\sqrt{2(1+\cos{\alpha})}}$$ $$=\lim\limits_{\alpha \to \pi} \frac{2\sin{\alpha/2}\cos{\alpha/2}}{\sqrt{2 \cdot 2\cos^2{\alpha/2}}}$$ $$=\lim\limits_{\alpha \to \pi} \sin{\alpha/2}$$ $$\sin{\pi/2}$$ $$=1$$ EDIT: The above calculations are incorrect, since I incorrectly cancelled $\cos{\alpha/2}$ with $|\cos{\alpha/2}|$ . See comment below for correct version, the result of which is that the limit doesn't exist.","['limits', 'calculus', 'derivatives']"
