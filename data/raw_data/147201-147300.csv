question_id,title,body,tags
2415734,Smallest twin-prime-pair above $2\uparrow\uparrow 5\ $?,"I searched the smallest prime larger than $$N:=2\uparrow\uparrow 5=2^{65536}$$ $N$ has $19\ 729$ digits. This is quite large and finding primes of this magnitude is not easy any more. I found $$N+44\ 061$$ This fits well to the fact that a number near $N$ is prime with probability $1:45\ 426$ But the next prime is surprisingly $$N+44\ 181$$ Did I miss any primes ? In other words, are the other numbers from $N$ to $N+44\ 181$ composite ? Even using the quite fast pfgw-prime-testing software, it took me about half a day to get those primes. So, I wonder whether we can calculate the smallest twin-prime-pair above $N$ (If this was not already done). Does anyone have an idea how we can find the smallest twin prime pair above $N$ with a reasonable amount of time ? The only idea I have is to sieve out the candidate-pairs by trial division, but there might be a better method.","['number-theory', 'big-numbers', 'prime-numbers', 'twin-primes']"
2415759,Lie derivative of the Christoffel symbol,"The Lie derivative of the Christoffel symbol is $$
 \mathcal{L}_\xi \Gamma^k_{ij} = \nabla_i \nabla_j \xi^k - R^k_{ijl} \xi^l \,.
$$ How can one prove that? And why does it make sense, because Christoffel symbols are functions? I know that the last question could be irrelevant, since the correct form of the LHS of the equation should be $(\mathcal{L}_\xi \Gamma)^k_{ij}$. But, I still cannot figure it out.",['differential-geometry']
2415785,How to prove $\ B \cap A^c = B\iff\ A\cap\ B^c =A$,I'm not sure if what i did is correct in order to prove this: $\ B \cap A^c = B\iff\ A\cap\ B^c =A$ What i tried: $\ B \cap A^c = B\rightarrow\ A\cup\ B^c =A$ $\ B \cap A^c=B \ $ then $A^c = B \ (\ idempotent  \ B\ \cap\ B = B) \ $or $A^c = U   \ (\ B\cap\ U =B) $ $\Rightarrow \ A=B^c \ or \ A= \emptyset $ $ \Rightarrow  \ A\ \cap\ A = A \ \Rightarrow  A\ \cap\ B^c = A  $ And for $\ B \cap A^c = B\leftarrow A\cup\ B^c =A$ is analog Thanks for the help,"['elementary-set-theory', 'proof-verification']"
2415847,The image of the d-uple embedding,"I have three questions of increasing generality that relate to an exercise in Chapter 1, Section 3 of Hartshorne's Algebraic Geometry concerning the $d$-uple embedding. I'd be happy to have any of them answered. $k$ is an algebraically closed field. For a fixed $n, d$ let $M_0, ..., M_n$ be the monomials in $n+1$ variables of degree $d$. The $d$-uple embedding $\varphi : \mathbb{P}^n \to \mathbb{P}^N$ is defined via $P \mapsto (M_0(P), ..., M_N(P))$. If $\theta$ is the $k$-algebra homomorphism $k[y_0,...,y_N] \to k[x_0,...,x_n]$ via $y_i \mapsto M_i$, then clearly $\text{im } \varphi$ is contained and Zariski dense in $Z(\ker \theta)$. How to show that in fact $\text{im } \varphi = Z(\ker \theta)$, i.e. $\text{im } \varphi$ is a variety? Whenever we have a map between affine (or projective) spaces given by componentwise polynomial (or monomial, of the same degree) maps, the image of the morphism will be contained and Zariski dense in the kernel of the corresponding $k$-algebra homomorphism. However, it need not be true that the image of the morphism is itself a variety in these circumstances (e.g. $(a,b) \mapsto (a, ab)$). Are there sufficient conditions to guarantee that the image corresponds precisely with the kernel of the $k$-algebra homomorphism that apply to the situation in (1)? Are there any sufficient conditions of a general morphism of varieties guaranteeing the image will be a variety that apply to the situation in (1)?",['algebraic-geometry']
2415881,"Group Isomorphic to $(\mathbb{Z} \times \mathbb{Z})/ \langle(a,b),(c,d)\rangle$","I have come across an interesting question regarding quotient groups of $\mathbb{Z} \times \mathbb{Z}$: Suppose $(a,b)$ and $(c,d)$ are two independent elements of $\mathbb{Z} \times \mathbb{Z}$. Suppose also that $\gcd(a,b) =1$. Show that $(\mathbb{Z} \times \mathbb{Z})/ \langle(a,b),(c,d)\rangle \, \cong \, \mathbb{Z}_{|ad-bc|}$. I have managed to find a little about the geometric interpretation of this result, but to provide a clear algebraic proof I believe it is necessary to use some linear algebra that I can't quite see. I would appreciate it if someone might be able to provide an elementary proof of this fact, perhaps using the First Isomorphism theory and a judicious choice of map.","['infinite-groups', 'integers', 'quotient-group', 'abelian-groups', 'group-theory']"
2415899,Proof of Hölder for Lorentz spaces (harmonic analysis),"So I was thinking about the proof of Hölder's inequality for Lorentz spaces $$ \left\lVert fg\right\rVert_{p,q} \lesssim \left\lVert f\right\rVert_{p_1,q_1} \left\lVert g\right\rVert_{p_2,q_2} $$ where the exponents are positive and finite ($q$ can be infinite, but let's ignore that) and $1/q = 1/{q_1} + 1/{q_2}, 1/p = 1/{p_1} + 1/{p_2}$. We all know that a Lorentz function can be characterized in 2 ways: dyadic decomposition by height: 
$f = \sum_n f_n$, $ 2^n <|f_n| \leq 2^{n+1}$, $f||_{p,q} \sim || \; \left\lVert f_n\right\rVert_{L^p} ||_{l^q_n}$ dyadic decomposition by width: 
$f = \sum_n f_n$, where $ f^*(2^{n+1}) < |f_n| \leq f^*(2^n)$, and  $ \left\lVert f\right\rVert_{p,q} \sim \left\lVert  \left\lVert f_n\right\rVert_{L^p} \right\rVert_{l^q_n}$ where $f^*$ is the decreasing rearrangement of $f$. The standard proof of Holder for Lorentz spaces uses dyadic decomposition by width (as can be seen here , Theorem 6.9). So I guess my question really is: Can we use decomposition by height to tackle this one? I'd like to think that it's possible. I tried to adapt the width method but the supports wouldn't play nice. If it is not possible, I'd like to know how one can figure out which characterization is better for a particular problem. Is there anything in the inequality that suggests the width approach would be better than the height approach?","['alternative-proof', 'harmonic-analysis', 'functional-analysis', 'proof-explanation', 'holder-inequality']"
2415903,"$A \subset \{1,2,3,...,40\} : \forall a,b\in A,\space |a-b|\ne4,\space|a-b|\ne9$. Prove $n(A)<20$","Question: $A \subset \{1,2,3,...,40\}$ such that
  \begin{align}
&\forall a,b\in A\\
&|a-b|\ne4,\quad|a-b|\ne9
\end{align}
  Prove that $n(A)<20$. I found a case of $n(A)=19$ that is $A=\{13m+n|m=0,1,2,\space n=1,3,4,6,9,11\}\cup\{40\}$, but couldn't prove that $19$ is the maximum. Thanks.","['algebra-precalculus', 'elementary-number-theory']"
2415904,Compact objects in category of schemes over a base scheme?,"I'm wondering if the compact objects in $\textbf{Scheme}/X$ (schemes over $X$) are the quasi-compact morphisms with codomain $X$. By ""compact object"" I mean that the covariant representative functor $\textbf{Scheme}(X, -)$ commutes with filtered colimits. If the compact objects aren't quasi-compact, what are they? Or, under what conditions does this statement hold?","['category-theory', 'schemes', 'algebraic-geometry']"
2415928,"Union of differences between three sets, where nothing intersects, how is it called?",I'm trying to find how to express the following: I think it would be something like: $$(A - (B \cup C)) \cup (B - (A \cup C)) \cup (C - (A \cup B))$$ But does this have a name? A simpler way to express it? Since this expression grows fast while one add more sets.,"['terminology', 'elementary-set-theory']"
2415934,"Armed with the information that $f(0)=0$ and $f'(0)=1$, evaluate: $\lim_{z\rightarrow 0}\frac{f(z^2)}{z}$","Armed with the information that $f(0)=0$ and $f'(0)=1$, evaluate: $(i)\lim_{z\rightarrow 0}\frac{f(z^2)}{z}, (ii) \lim_{z\rightarrow i}\frac{f(z^2+1)}{z-i}$ . For $(i)$ let $u:=z^2$ be so $\lim_{z\rightarrow 0}\frac{f(z^2)}{z}=\lim_{u\rightarrow 0}\sqrt{u}\frac{f(u)}{u}=\lim_{u\rightarrow 0}\sqrt{u}\lim_{u\rightarrow 0}\frac{f(u)-f(0)}{u-0}=0\cdot f'(0)=0\cdot 1=0$ Is this argument well? could you do the same with $(ii)$? thanks for your help.","['derivatives', 'complex-analysis', 'complex-numbers', 'limits']"
2415936,Make a point process from another?,"Blue boats are passing on a river and their arrival times are modeled by a renewal point process. Place red boats on the river according to a point process $P$ such that the arrival times of all of the boats (the superposition process) becomes another renewal process. Find $P$. In other words, find a point process whose superposition with a given renewal process is another renewal process. Note the causality constraint : when you place red boats, you don't have any information about the arrival times of the blue boats in the future. What I found was not enough: A solution for a similar question here , without causality Poisson and alternating renewal processes with superposition a renewal process Renewal Processes Decomposable into i.i.d. Components Superposition and Decomposition of Stationary Point Processes Almost Sure Comparisons of Renewal Processes and Poisson Processes Superposition of Renewal Processes On the Superposition of Point Processes Pairs of renewal processes whose superposition is a renewal process ON QUEUEING SYSTEMS VITH RENEWAL DEPARTURE PROCESSES","['stochastic-processes', 'probability-theory', 'statistics', 'probability', 'queueing-theory']"
2415993,Discrete mathematics proof union/intersect/difference [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Prove that $$A\cup B=(A\setminus B)\cup (B\setminus A)\cup (A\cap B)$$ I'm new to proofs so I'm not sure how to approach this problem. Any help is appreciated.","['elementary-set-theory', 'discrete-mathematics']"
2416001,How can e^x seemingly approach 0+ as x approaches negative infinity?,"The specific problem with regards to this graph is: $$\lim_{x \to -\infty} g(2 + e^x)$$ Now it's true that we technically can't apply ""the limit of a sum is the sum of the limits"" since the function is not continuous. But to me, this intuitively seemed like the solution would just be: $$ g(2 + 0) $$ $$ = g(2) $$ $$ = 0.5 $$ In hindsight, this wass grounded on foolish assumptions. But even if my approach isn't mathematically sound, I don't get why the provided approach on the answer key should be sound: $$ u = 2 + e^x $$ $$ as\ x\to - \infty, u\to2^+ $$ $$ \lim_{u \to 2+} g(u) = -2 $$ I understand that, when you look at the graph of $e^x$ as it approaches negative infinity, it gradually diminishes towards zero, so I somewhat understand how the value it approaches is sort of like approaching $0$ from the right ( $0^+$ ) -- but to me, this provided solution looks like it's defying the fact that $ \lim_{x \to -\infty} e^x = 0 $ , and it seems really alien to me. One thing I'm wondering is that things are different since we're working inside of the function $g$ , but I'm unable to find any such rule or explanation for this.","['limits-without-lhopital', 'limits']"
2416024,Similarity transform of a matrix preserves the determinant?,"It is posited (in chapter $11$ of the book Numerical Recipes in C ) that doing a similarity transform of a matrix $A$ preserves its eigenvalues. This is how a similarity transform is defined: $$A \mapsto Z^{-1}AZ$$
for some transformation matrix $Z$ whose determinant is one. The proof given uses the characteristic equation. $$|Z^{-1} A Z - \lambda I| = |Z^{-1}(A-\lambda I)Z| $$
I don't understand how this comes about. Once we accept this, the result follows quite easily: $$= |Z||A-\lambda I||Z^{-1}| = |A-\lambda I|.$$","['matrices', 'determinant', 'linear-transformations', 'proof-explanation', 'linear-algebra']"
2416037,How to prove the nonexistence of cycles in the game of Chain Reaction?,"Chain Reaction is a two player combinatorial game played on a $9 \times 6$ game board in which players Red and Green take turns to place a single atom of their color in an empty square or in a square that they control. Placing more than one atom in the same square creates a molecule. The more atoms in a molecule, the more unstable it becomes. When a molecule becomes too unstable it splits and its atoms move to orthogonally adjacent squares, changing the color of the atoms in those squares to match their color. This can cause a chain reaction of splitting molecules. Molecules in the four corner squares require two atoms to split. Molecules on the edge of the board require three atoms to split.  Molecules in the center of the board require four atoms to split. Atoms of a splitting molecule each move in a different direction. The game ends immediately after one player captures all of the atoms belonging to the other player. What I'm interested in is proving the nonexistence of cycles in any chain reaction because otherwise
the next turn would never begin. How would I go about proving this? I don't know where to  begin.","['combinatorics', 'proof-writing', 'combinatorial-game-theory']"
2416042,A probability problem involving two decks of cards,"Here is the problem: Player A chooses 5 cards from a deck of cards. Player B also chooses 5 cards from ANOTHER deck of cards. Player B wins if his cards match at least 3 cards of player A. What is the probability that the number of cards of player B matches that of player A's is A) 0
B) 1
C) 2
D) 3
E) 4
F) 5 I attempted to solve A) this way:
((52C5)* (47C5)) / ((52C5)*(52C5)) = 0.5902 but I'm not sure if this is right. Thanks in advance for any help!",['probability']
2416064,Existence of the limit of a certain integral,"Let $f_n:[0,1]\to\mathbb{R}$ be a sequence continuous functions which uniformly converges to continuous function $f:[0,1]\to\mathbb{R}$ . Let $$F_n=\int_0^1\frac{\sin(nx)}{2+\cos(nx)}f_n(x)dx,\:\:\:n\geq 1.$$ Is $F_n$ necessarily convergent? As $f_n\to f$ uniformly, the sequence is uniformly bounded so $\frac{\sin(nx)}{2+\cos(nx)}f_n(x)$ is bounded. I'm stuck on finding the limit of $\frac{\sin(nx)}{2+\cos(nx)}f_n(x)$ , if it has a Riemann integrable limit, then by Dominated convergent theorem, I can pass the limit inside the integral, I guess! However, I'm not sure about using the dominated convergent theorem I appreciate any help.","['real-analysis', 'riemann-integration', 'integration', 'lebesgue-integral', 'analysis']"
2416096,Can an event be independent to the Sample Space. Can a Null Event be independent to any other event?,"How do we show mathematically that event $A$ cannot be independent of $S$ the sample space?
How do we show event $A,$ if $P(A)=0$ cannot be independent of event $B$ if $0 < P(B) < 1?$ Seems like a dumb a question to me. A thing that exists cannot be independent of the universe that it is in, and a thing that does not exist is defined by the universe that it is not. Having tough time with the formulas.","['statistics', 'probability']"
2416151,"A mysterious way for solving ODE of type $P(x,y)dx+Q(x,y)dy=0$ where $P$ and $Q$ are homogenous function of degree $n$","Definition: We say $f(x,y)$ is homogenous of degree $n$ if for each $\lambda$, $f(\lambda x, \lambda y)=\lambda^n f(x,y)$ We call a differential equation of form $P(x,y)dx+Q(x,y)dy=0$ homogenous, If $P$ and $Q$ are two homogenous functions of degree $n$. I've read in a book that all of the homogenous ODE's can be solved by these substitutions: $y=vx$ , $dy=vdx+xdv$ Mysteriously, That seems to be working! I have two questions regarding this way of solving homogenous ODE's. $1$) How did they come up with this solution? They could do many other substitutions. Why this one? $2$) Why can we assume that there exists something like $v$ that $y=vx$? Maybe there is not a linear relation between $x$ and $y$! How are we sure? Note: I want to understand this method. That's the point.","['homogeneous-equation', 'ordinary-differential-equations']"
2416154,Theorem 7.17 in Baby Rudin: How is a shorter proof possible if the continuity of the $f_n^\prime$ is assumed in addition?,"Here is Theorem 7.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\left\{ f_n \right\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\left\{ f_n \left( x_0 \right) \right\}$ converges for some point $x_0$ on $[a, b]$. If $\left\{ f_n^\prime \right\}$ converges uniformly on $[a, b]$, then $\left\{ f_n \right\}$ converges uniformly on $[a, b]$, to a function $f$, and 
  $$\tag{27} f^\prime(x) = \lim_{n \to \infty } f_n^\prime(x) \qquad \qquad (a \leq x \leq b). $$ And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Choose $N$ such that $n \geq N$, $m \geq N$, implies 
  $$ \tag{28} \left\lvert f_n \left( x_0 \right) - f_m \left( x_0 \right) \right\rvert < \frac{\varepsilon}{2} $$
  and 
  $$ \tag{29} \left\lvert f_n^\prime(t) - f_m^\prime(t) \right\rvert < \frac{\varepsilon}{2(b-a)} \qquad \qquad (a \leq t \leq b). $$ If we apply the mean value theorem 5.19 to the function $f_n - f_m$, (29) shows that 
  $$ \tag{30} \left\lvert f_n (x) - f_m (x) - f_n (t) + f_m (t) \right\rvert \leq \frac{ \lvert x-t \rvert \varepsilon }{ 2(b-a) } \leq \frac{ \varepsilon }{ 2 } $$
  for any $x$ and $t$ on $[a, b]$, if $n \geq N$, $m \geq N$. The inequality 
  $$ \left\lvert f_n (x) - f_m (x) \right\rvert \leq \left\lvert f_n (x) - f_m (x) - f_n \left( x_0 \right) + f_m \left( x_0 \right) \right\rvert + \left\lvert f_n \left( x_0 \right) - f_m \left( x_0 \right) \right\rvert   $$
  implies, by (28) and (30), that 
  $$ \left\lvert f_n(x) - f_m(x) \right\rvert < \varepsilon \qquad \qquad (a \leq x \leq b, n \geq N, m \geq N), $$
  so that $\left\{ f_n \right\}$ converges uniformly on $[a, b]$. Let 
  $$ f(x) = \lim_{n \to \infty} f_n (x) \qquad (a \leq x \leq b). $$ Let us now fix a point $x$ on $[a, b]$ and define 
  $$ \tag{31} \phi_n(t) = \frac{ f_n(t) - f_n(x) }{ t-x}, \qquad \phi(t) = \frac{ f(t) - f(x) }{ t-x} $$
  for $a \leq t \leq b$, $t \neq x$. Then 
  $$ \tag{32}  \lim_{ t \to x } \phi_n (t) = f_n^\prime(x) \qquad (n = 1, 2, 3, \ldots). $$
  The first inequality in (30) shows that 
  $$ \left\lvert \phi_n(t) - \phi_m(t) \right\rvert \leq \frac{ \varepsilon}{2(b-a) } \qquad ( n \geq N, m \geq N), $$
  so that $\left\{ \phi_n \right\}$ converges uniformly, for $t \neq x$. Since $\left\{ f_n \right\}$ converges to $f$, we conclude from (31) that 
  $$\tag{33} \lim_{n \to \infty } \phi_n (t) = \phi(t) $$
  uniformly for $a \leq t \leq b$, $t \neq x$. If we now apply Theorem 7.11 to $\left\{ \phi_n \right\}$, (32) and (33) show that 
  $$ \lim_{t \to x} \phi(t) = \lim_{n \to \infty} f_n^\prime(x); $$
  and this is (27), by the definition of $\phi(t)$. Here is Theorem 5.19 in Baby Rudin: Suppose $\mathbf{f}$ is a continuous mapping of $[a, b]$ into $\mathbb{R}^k$ and $\mathbf{f}$ is differentiable in $(a, b)$. Then there exists $x \in (a, b)$ such that 
  $$ \lvert \mathbf{f} (b) - \mathbf{f} (a) \rvert \leq (b-a) \left\lvert \mathbf{f}^\prime (x) \right\rvert. $$ And, here is Theorem 7.11: Suppose $f_n \to f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose that 
  $$ \tag{15} \lim_{ t \to x } f_n (t) = A_n \qquad (n = 1, 2, 3, \ldots). $$ 
  Then $\left\{ A_n \right\}$ converges, and 
  $$ \tag{16} \lim_{t \to x} f(t) = \lim_{n \to \infty} A_n. $$ 
  In other words, the conclusion is that 
  $$ \tag{17} \lim_{t \to x} \lim_{ n \to \infty} f_n(t) = \lim_{ n \to \infty}  \lim_{t \to x} f_n(t). $$ Now here is the Remark following the proof of Theorem 7.17 in Baby Rudin: If the continuity of the functions $f_n^\prime$ is assumed in addition to the above hypotheses, then a much shorter proof of (27) can be based on Theorem 7.16 and the fundamental theorem of calculus. Here is Theorem 7.16 in Baby Rudin, 3rd edition: Let $\alpha$ be monotonically increasing on $[a, b]$. Suppose $f_n \in \mathscr{R}(\alpha)$ on $[a, b]$, for $n = 1, 2, 3, \ldots$, and suppose $f_n \to f$ uniformly on $[a, b]$. Then $f \in \mathscr{R}$ on $[a, b]$, and 
  $$ \tag{23} \int_a^b f \ \mathrm{d} \alpha = \lim_{n \to \infty} \int_a^b f_n \ \mathrm{d} \alpha. $$
  (The existence of the limit is part of the conclusion.) And, here is Theorem 6.21 in Baby Rudin (i.e. the fundamental theorem of calculus): If $f \in \mathscr{R}$ on $[a, b]$ and if there is a differentiable function $F$ on $[a, b]$ such that $F^\prime = f$, then 
  $$ \int_a^b f(x) \ \mathrm{d} x = F(b) - F(a). $$ Although I've understood the proof of Theorem 7.17, I do not know how to give the much shorter proof of (27) that Rudin has asserted in the Remark above?","['real-analysis', 'uniform-convergence', 'continuity', 'sequences-and-series', 'analysis']"
2416185,Interesting ODEs on Lie groups,"As a personal exercise, I was planning on implementing computationally some of the Lie group methods . So I'd like your input about differential equations where the space of configurations is a Lie group. I know about the isospectral flow on the Toda lattice, and the QR flow on orthogonal matrices. These two examples do the job, but they are sooooo well-worn that I wonder if there are other interesting (this is opinion-based, but anyway) examples. I thought about the Heisenberg group, whose Lie algebra is nilpotent, so the exponential can be exactly computed, and there is a global chart. Yet this global chart is so trivial, that any ODE on $H_3(\mathbb{R})$ could be easily rewritten on $\mathbb{R}^3$ itself, and there is no point in emphasizing the group structure. I can think of some sources of inspiration, where I am no expert though: Mechanics, e.g. non-holonomic systems. Machine learning, e.g. low-rank factorization considering the Stiefel manifold. Matrix nearness problems and similar topics in numerical analysis or optimization. Arguably there is non-empty intersection among these fields. Using an exceptional group, e.g. $G_2$, would be a bonus, because it would be... well, exceptional. To summarize: I would like to see explicit examples of ODEs, where trajectories lie on a manifold, with some appealing physical/mathematical/computational interpetation. Disclaimer: if the question has some obscure point, it is because of my lack of expertise. Please, comment nicely before downvoting :)","['numerical-methods', 'ordinary-differential-equations', 'lie-groups']"
2416242,Matrix function which is multiplicative and homogeneous of degree n is the determinant,"I'm solving the following problem: Let $\Delta : M_n(\mathbb{F}) \to \mathbb{F}$ satisfy the following properties: $\Delta(AB) = \Delta(A)\Delta(B), \quad\forall A, B \in M_n(\mathbb{F})$ $\Delta(\lambda I) = \lambda^n, \quad\forall \lambda \in \mathbb{F}$ Prove that $\Delta = \det$. $M_n(\mathbb{F})$ is the space of $n\times n$ matrices with entries from the field $\mathbb{F}$, and $I \in M_n(\mathbb{F})$ is the identity matrix. $\mathbb{F}$ can pretty much be assumed to be $\mathbb{R}$ or $\mathbb{C}$, since it was the general setting where the problem was given. $\DeclareMathOperator{\sgn}{sgn}$
The definition of the determinant I'm using is $\det A = \sum_{p\in S_n} \sgn p\cdot a_{1p(1)}a_{2p(2)}\cdots a_{np(n)}$, for $A = (a_{ij})$. I'm also aware of certain characterizations, such as: Let $f : M_n(\mathbb{F}) \to \mathbb{F}$ be a multilinear and alternating function in the columns of the matrix such that $f(I) = 1$. Then $f = \det$. My attempt: My idea was to prove that $\Delta = \det$ on all elementary matrices and then use the fact that every invertible matrix can be written as a product of elementary matrices. I would separately prove that $\Delta$ of all singular matrices is $0$. Some results I obtained along the way (using the notation for elementary matrices from the linked Wikipedia article): By plugging in $\lambda = 1$ in $(2)$ we get $\Delta(I) = 1$. $\lambda = 0$ gives $\Delta(0) = 0$. $\Delta$ is homogenous of degree $n$:
By pluggin in $\lambda = 1$ in $(2)$ we get $\Delta(I) = 1$. Then for any matrix $A$ we have $\Delta(\lambda A) = \Delta(\lambda I\cdot A) = \Delta(\lambda I)\Delta(A) = \lambda^n \Delta(A)$. Let $A$ be invertible. Then $1 = \Delta(I) = \Delta(AA^{-1}) = \Delta(A)\Delta(A^{-1})$. Thus, $\Delta(A) \ne 0$. ${T_{i,j}}^2 = I$ so $|\Delta(T_{i,j})| = 1$ For $\lambda \notin \{0, 1\}$ we have $$\Delta(D_1(\lambda))\,\Delta(D_2(\lambda))\cdots \Delta(D_n(\lambda))) = \Delta(D_1(\lambda)\,D_2(\lambda)\cdots D_n(\lambda)) = \Delta(\lambda I) = \lambda^n$$
so there surely exists $i_0 \in \{1, \ldots, n\}$ such that $|\Delta(D_{i_0}(\lambda))|\ne 1$. $\Delta$ of a matrix with a zero row is $0$. Let $A$ be such that its $i$-th row is $0$. Then there exists a sequence of row transpositions $T_1, T_2, \ldots, T_k$ of the type $T_{r,s}$ such that $A$ is transformed to a matrix with its $i_0$-th row is a zero row, where $|\Delta(D_{i_0}(\lambda))|\ne 1$. Now, since multiplying that row with $\lambda \notin \{0, 1\}$ does not change the matrix, by doing the row transpositions again in reverse order we return to the matrix $A$. Then we have
$$\Delta(A) = \Delta(T_1\ldots T_{n-1}T_nD_{i_0}(\lambda)T_n\ldots T_2T_1A) = \Delta(T_1)^2\Delta(T_2)^2\cdots\Delta(T_n)^2\Delta(D_{i_0}(\lambda))\Delta(A)$$
By taking the absolute value of both sides, since $|\Delta(T_{r,s})| = 1$ and $|\Delta(D_{i_0}(\lambda))|\ne 1$, we obtain $\Delta(A) = 0$. $\Delta$ of a singular matrix is $0$. Let $A$ be singular. Then $A$ can be transformed by row and column operations to a matrix $$E_r = \begin{bmatrix}
1 &  &  & & \\
& \ddots & & &  \\
& & 1 & & &  \\
& & & 0 & &  \\
& & & & \ddots \\
& & & & & 0 \\
\end{bmatrix}$$ with $r$ nonzero rows, where $r$ is the rank of $A$. Since $\Delta$ of elementary matrices is nonzero because they are invertible, and $\Delta(E_r) = 0$ because there is always a zero row in it, we can conclude that $\Delta(A) = 0$. $L_{i,j}(\lambda)$ can be diagonalized to $I$, and since $\Delta$ is obviously a similarity invariant we have $\Delta(L_{i,j}(\lambda)) = 1$. This is where I'm stuck, we still have to prove $\Delta(T_{i,j}) = -1$ and $\Delta(D_i(\lambda)) = \lambda$. I have had some further progress if we assume $\mathbb{F} = \mathbb{R}$: From $|\Delta(T_{i,j})| = 1$ we can conclude $\Delta(T_{i,j})$ = $\pm 1$. Notice that $D_i(\lambda) = T_{i,j}D_j(\lambda)$. Applying $\Delta$ we conclude that $\Delta(D_i(\lambda))$ and $\Delta(D_j(\lambda))$ can differ only by sign. Since we know $\Delta(D_1(\lambda))\,\Delta(D_2(\lambda))\cdots \Delta(D_n(\lambda))) = \lambda^n$, we get $|\Delta(D_i(\lambda))| = |\lambda|$. If we further assume $\lambda > 0$, we have $\Delta(D_i(\lambda)) = \Delta(D_i(\sqrt{\lambda})D_i(\sqrt{\lambda})) = \Delta\left(D_i(\sqrt{\lambda})\right)^2 > 0$. Finally we obtain $\Delta(D_i(\lambda)) = \lambda$.","['linear-algebra', 'determinant']"
2416253,Element of Largest Order in $S_n$,"What is the largest order of an element in the group of permutations of $5$ objects? Let $\sigma \in S_5$ be arbitrary. We know that $\sigma = \sigma_1...\sigma_k$, where the $\sigma_i$'s are disjoint cycles of some length. We want to maximize $|\sigma| = lcm(|\sigma_1|,...,|\sigma_k|)$, where $|\sigma_i|$ is the order of the permutation $\sigma_i$, which is just its length since it is a cycle. If any of the lengths are the same, then the lcm will be the same, so we may take each $\sigma_i$ to have a different length in our attempt to maximize $|\sigma|$. This leaves us with four different lengths (I am excluding $1$). Thus $\sigma = \sigma_1 ... \sigma_4$; however, as we shall see, it is not possible for $\sigma$ to be a product of 4 disjoint cycles if we are maximizing their lengths. If $\sigma_1$ is a 5-cycle, then none of the 5 numbers will appear in any of the other cycles, and so $\sigma = \sigma_1$ which implies $|\sigma| = 5$. If $\sigma_1$ is a 4-cycle, then 4 of the 5 numbers appear in it, leaving only number left $\sigma_1$ and none to ther others. But this would make $\sigma_2$ and the rest the identity, so that $\sigma = \sigma_1$. In this case $|\sigma| = 4$. Finally, if $\sigma_1$ is a 3-cycle, then $\sigma_2$ can have the rest of the numbers which makes it a 2-cycle. In this case, $|\sigma| = lcm(3,2) = 6$.  By symmetry, the rest of the cases are equivalent, and so the maximum value is $6$. How does that sound?","['abstract-algebra', 'combinatorics', 'group-theory', 'symmetric-groups']"
2416259,Factorization of $18(ab^2 + bc^2 + ca^2) - 12(a^2b + b^2c + c^2a) - 19abc$,"I solved a following problem in my mathematics homework today: Factorize $18(ab^2 + bc^2 + ca^2) - 12(a^2b + b^2c + c^2a) - 19abc$. That's easy. I solved by the following way and it was same as sample answer of the book: $\ \ \ \ 18(ab^2 + bc^2 + ca^2) - 12(a^2b + b^2c + c^2a) - 19abc$ $=(-12b+18c)a^2 + (18b^2 - 19bc - 12c^2) + (18bc^2 - 12b^2c)$ $=-6a^2 (2b-3c) + a(2b-3c)(9b+4c) - 6bc(2b-3c)$ $=(2b-3c)(-6a^2 + 9ab + 4ac - 6bc)$ $=(2b-3c)(2a-3b)(2c-3a)$. (if there is a typo, please inform this) Though the process took much times (highly implementing one), and I think it's too straight forward way. The question and answer looks simple, so I think there must be more elegant and way, which has less calculation time. So, I want to ask the simplier way to solve this.","['algebra-precalculus', 'polynomials', 'factoring']"
2416291,Any two bases of a free abelian group have the same cardinality.,"I am comfortable with understanding the theorem when the basis has a finite number of elements. In the case where the basis is infinite, I find the proof very overwhelming and not so elegant. Can someone give an accessible proof for the infinite case? Or at least a rough sketch of ideas involved in proving it? Update: I would be comfortable if some set theoretic approach is used(in particular Cardinal Numbers).","['abelian-groups', 'group-theory']"
2416330,Theorem 7.18 in PMA Rudin: Existence of an everywhere continuous but nowhere differentiable real function on the real line,"Here is Theorem 7.18 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: There exists a real continuous function on the real line which is nowhere differentiable. And, here is Rudin's proof ( steps wherein I've been unable to figure out on my own and hence would appreciate the help of the Math SE community): Define 
  $$\tag{34} \varphi(x) = \lvert x \rvert \qquad \qquad (-1 \leq x \leq 1) $$
  and extend the definition of $\varphi(x)$ to all real $x$ by requiring that 
  $$ \tag{35} \varphi(x+2) = \varphi(x). $$
  Then, for all $s$ and $t$, 
  $$\tag{36}  \lvert \varphi(s) - \varphi(t) \rvert \leq \lvert s-t \rvert. $$
  [ How to obtain the inequality in (36)? ] 
  In particular, $\varphi$ is continuous on $\mathbb{R}^1$. Define 
  $$ \tag{37} f(x) = \sum_{n=0}^\infty \left( \frac{3}{4} \right)^n \varphi \left( 4^n x \right). $$
  Since $0 \leq \varphi \leq 1$, Theorem 7.10 shows that the series (37) converges uniformly on $\mathbb{R}^1$. By Theorem 7.12, $f$ is continuous on $\mathbb{R}^1$. Now fix a real number $x$ and a positive integer $m$. Put 
  $$ \tag{38} \delta_m = \pm \frac{1}{2} \cdot 4^{-m} $$
  where the sign is so chosen that no integer lies between $4^m x$ and $4^m \left( x + \delta_m \right)$. This can be done, since $4^m \left\lvert \delta_m \right\rvert = \frac{1}{2}$. Define 
  $$ \tag{39} \gamma_n = \frac{ \varphi \left( 4^n \left( x + \delta_m \right)  \right) - \varphi \left( 4^n x \right)  }{ \delta_m }. $$
  When $n > m$, then $4^n \delta_m$ is an even integer, so that $\gamma_n = 0$. When $0 \leq n \leq m$, (36) implies that $\left\lvert \gamma_n \right\rvert \leq 4^n$. Since $\left\lvert \gamma_m \right\rvert = 4^m$ [ How? ], we conclude that 
  $$
\begin{align}
\left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\ 
&\geq 3^m - \sum_{n=0}^{m-1} 3^n \\
&= \frac{1}{2} \left( 3^m + 1 \right).
\end{align}
$$
  As $m \to \infty$, $\gamma_m \to 0$. It follows that $f$ is not differentiable at $x$. Here is Theorem 7.10 in Baby Rudin, 3rd edition: Suppose $\left\{ f_n \right\}$ is a sequence of functions defined on $E$, and suppose $$ \left\lvert f_n (x) \right\rvert \leq M_n \qquad \qquad (x \in E, \ n = 1, 2, 3, \ldots \ ). $$
  Then $ \sum f_n $ converges uniformly on $E$ if $ \sum M_n$ converges. Note that the converse is not asserted ( and is, in fact, not true). And, here is Theorem 7.12: If $\left\{ f_n \right\}$ is a sequence of continuous functions on $E$, and if $f_n \to f$ uniformly on $E$, then $f$ is continuous on $E$. The rest of the proof I understand, I think. However, I would appreciate if someone could give the crux of the procedure involved in the construction of this particular example and also give a blueprint for constructing this class of functions.","['derivatives', 'real-analysis', 'calculus', 'continuity', 'analysis']"
2416395,Why does this function have an analytic continuation and how can we compute it?,"For $q \in \Bbb C$, let $f_q(x) = \frac {3x + q(1-x)}{1 - q(1-x)}$ and for $n \in \Bbb N$, let $F_n(q) = (f_1 \circ f_q \circ f_{q^2} \circ \cdots \circ f_{q^n}) (0)$ For $|q| \notin \{ 1, \frac 13 \}$, the sequence $(F_n(q))$ converges to some $F(q) \in \Bbb P^1(\Bbb C)$, and it looks like $F$ is meromorphic there (should not be too difficult to prove) Here is a picture of $F_{100}$ : (surprisingly, it looks meromorphic on most of the unit circle, save some essential singularities, but it's almost surely deceiving. There should be a necklace of zeros and poles around the circle, but you don't see them because they are so close together) I am more interested in what happens for $|q| < \frac 13$, where $f_{q^n}$ converges too fast to the homothety $x \mapsto 3x$ for the argument $0$ to escape very far. Looking at the picture, it looks very continuable. Thankfully, I picked some coefficients so that the Laurent series of $F_n$ at $0$ converge in $\Bbb Z((q))$, which gives a first way to see past the $|q| = \frac 13$ boundary. Using the series $F(q) = \frac 1q(1-2q+4q^2-6q^3+20q^4-46q^5+\cdots)$ and truncating after a hundred terms, I get another picture The radius of convergence here seems to be around $0.41$, and it doesn't look any less well-behaved except for possibly a pole at $q = -0.41$. Pre and post composing the function with carefully chosen functions can also extend this even further. So the ""natural"" boundary of $|q|= \frac 13
$ isn't really one, and still it looks like it could be extended further, possibly up to the whole open disk of radius $1$. So, what gives ?
Is there an alternative way of computing the extension in the annulus $\frac 13 \le |q| < 0.41$ that would explain it and gives a further continuation ?","['complex-analysis', 'continued-fractions', 'modular-forms']"
2416437,product formula for the sum of cosine and sine,"Above is an exercise from Conway. I have proved the product formula for sine and cosine functions and they are like below. However, I am stuck at how to calculate the exercise problem. Could anyone help me how to compute product formula in the exercise?","['complex-analysis', 'infinite-product', 'products']"
2416485,Prove that: $(A-B)\cap B=\phi$,"Prove that: $(A-B)\cap B=\phi$. My Attempt: Let $x$ be an element of $(A-B)\cap B$. Then,
$$x\in (A-B)\cap B \iff x\in (A-B) \textrm {and} x\in B$$
$$\iff (x\in A \textrm {and} x\notin B) \textrm {and} x\in B$$.",['elementary-set-theory']
2416524,"$\mathfrak p$ be a prime ideal of a commutative ring $R$ , then $R_\mathfrak p/\mathfrak pR_\mathfrak p$ is the field of fractions of $R/\mathfrak p$?","Let $\mathfrak p$ be a prime ideal of a commutative ring $R$ with unity , then how to show that the field $R_\mathfrak p/\mathfrak pR_\mathfrak p$ is isomorphic with the field of fractions of the integral domain $R/\mathfrak p$ ? I can show that $R_\mathfrak p/\mathfrak pR_\mathfrak p \cong (R/\mathfrak p)_\mathfrak p$ as $R_\mathfrak p$ modules ... but I am not sure whether this implies the result I am asking or not","['localization', 'abstract-algebra', 'maximal-and-prime-ideals', 'ring-theory', 'commutative-algebra']"
2416546,"For which integers $n, 3\leq n\leq 11$ is there only one group of order $n$","Is the way of thinking in this question: similar to the way of thinking in that question? actually I do not know how to solve the first question, any hint will be appreciated.","['abstract-algebra', 'group-theory']"
2416551,How to evaluate contour integral of rational times trig function,"While working a problem, I came across an expression for a finite wave train. I want to know the standard deviation of the frequency representation of that wave train. I found using the operator method from QM that it's $\frac{\pi^2}{a^2}$. I'd like to show this more explicitly by evaluating the below integral of the Fourier Transform of that wave train. Mathematica confirms that the integral converges to the same answer. It's very similar to the integral of the Sinc squared function, but times a rational function: $$ \frac{4\pi}{a^3}\int^\infty_{-\infty} \frac{k^2 \cos^2{(xa/2)}}{(\pi/a-k)^2 (\pi/a+k)^2} \, dk $$ I've attempted to evaluate $$ \int_\gamma \frac{z^2 e^{2i(za/2)}}{(\pi/a-z)^2 (\pi/a+z)^2} \, dz $$ over the semicircle of Radius $R$ centered at the origin, deformed at the points $z = \pm\frac{\pi}{a} $ into a Semicircle of radius $\epsilon$, then taking limits as R goes to infinity and epsilon to zero. I see that it has two poles at $\pm\frac{\pi}{a}$ but the residues at those points are zero, so that doesn't help any. The first thing I've tried to do is evaluate this on the smaller semicircles, taking $ z = \frac{\pi}{a} + \epsilon e^{i\theta}$ and integrating with respect to $\theta$. This becomes an integral of an ugly rational expression that I can't seem to make any headway on using partial fractions. Should I have chosen a different ""complexified"" integral?","['complex-analysis', 'integration', 'trigonometric-integrals']"
2416560,"Is the set $\{2, 3, 4\}$ open in some metric spaces and not open in others?","I just want to check my understanding. This is from Baby Rudin: 2.18 Definition Let $X$ be a metric space. All points and sets mentioned below are understood to be elements and subsets of $X$ . $(a)$ A neighborhood of $p$ is a set $N_r(p)$ consisting of all $q$ such that $d(p, q)<r$ for some $r>0$ . The number $r$ is called the radius of $N_r( p)$ $(e)$ A point $p$ is an interior point of $E$ if there is a neighborhood $N$ of $p$ such that $N \subset E$ $(f)$ $E$ is open if every point of $E$ is an interior point of $E$ . Suppose we have the metric space with set $X=\{1, 2, 3, 4, 5\}$ and distance function $d(x, y)=|x-y|$ . Now $2$ is an interior point of $\{2, 3, 4\}$ because $N_{0.5}(2)=\{2\} \subset \{2, 3, 4\}$ (and a similar argument can be made for $3$ and $4$ as well. But if our metric space is $\mathbb{R}$ with the same distance function, then $\{2, 3, 4\}$ is not open because no neighborhood of $2$ is a subset of $\{2, 3, 4\}$ , so $2$ is not an interior point of $\{2, 3, 4\}$ , right?","['real-analysis', 'analysis']"
2416568,"If $ \int fg = 0 $ for all compactly supported continuous g, then f = 0 a.e.?","I was wondering whether the following statement is true and, if so, how it can be shown: If $ f \in L^{1}_{Loc}(\mathbb{R}^n) $ and if for all compactly supported continuous functions $ g: \mathbb{R}^n \to \mathbb{C} $ we have that the Lebesgue integral of $ f $ multiplied by $ g $ equals zero, i.e. $$ \int_{\mathbb{R}^n} f(x)g(x) \mathrm{d}x = 0 , $$ then $ f(x) = 0 $ almost everywhere. I would be very grateful for any answers or hints! N.B. I am aware that this question has already been addressed. In If $f\in L^1_{loc}(\mathbb{R})$ and $\int f\varphi=0$ for all $\varphi$ continuous with compact support, then $f=0$ a.e. , I am not quite sure about how to create a sequence of compactly supported continuous functions such that $ \varphi_n\to \frac{f}{|f|+1} $. This particular question may have its answer in If $f\in L^1(\mathbb{R})$ is such that $\int_{\mathbb{R}}f\phi=0$ for all continuous compactly supported $\phi$, then $f\equiv 0$. , however here I am unsure about the meaning of a ""regularizing sequence""; why does $ \phi_n\ast f\to f $ in $L^1$ sense if $ \phi_n(x) = n\phi(nx) $, where $ \phi\in \mathcal C^\infty_c(\Bbb R) $ with $ \phi\ge 0 $ and $ \int_{\Bbb R}\phi(x)dx=1 $? Once again, any answer would be much appreciated!","['almost-everywhere', 'real-analysis', 'lebesgue-measure', 'functional-analysis', 'lebesgue-integral']"
2416602,A short proof of this? $\frac{\phi(x)-\phi(0)}{x}\in\mathcal{C}^\infty(\mathbb{R})$,Let $\phi\in C^\infty(\mathbb{R})$ and let $\psi(x)=\frac{\phi(x)-\phi(0)}{x}$ extended to all $\mathbb{R}$ continuously. I know $\psi\in C^\infty(\mathbb{R})$ but I am looking for a short proof of this.,"['derivatives', 'real-analysis', 'calculus']"
2416613,Wronskian for multivariate functions,"I've been reading about the wronskian and I got stuck in the following: Suppose we are given a multivariate function describing e.g. a plane: $z = m_1 x + m_2 y + b$. How is the wronskian computed? We have here to two variables ($x,y$). How is this case dealt with the wronskian? Best regards","['multivariable-calculus', 'wronskian']"
2416633,Monotonicity property of $\chi^2$ quantiles,"Suppose $\alpha$ is ""small"". Let $a(v)$ be the quantile for $\chi^2(v)$ distribution corresponding to $\alpha$ probability i.e. if $A\sim \chi^2(v)$ then $P[A\leq a(v)]=\alpha$. Here $v$ is the degrees of freedom of the chi-square distribution. I have a feeling that whenever $v_1<v_2$, $\dfrac{a(v_1)}{v_1}<\dfrac{a(v_2)}{v_2}$. At least simulations show this. Is this correct? Can a rigorous proof be given? Or any reference for that matter?","['probability-theory', 'inequality', 'probability', 'probability-distributions']"
2416665,Law of large numbers for non-identically distributed Bernoulli random variables,"Let $(X_n)$ be a succession of independent r.v., such that $X_n$ ~ $Bern(p_n)$. I know then that $\lim_{n \to \infty}p_n=p$ and $p_n>p>0$ for each $n \in \mathbb{N}$. I have to prove that $\dfrac{X_1 + \dots + X_n}{n} \rightarrow p$ almost surely. Intuitively, by the strong law of large numbers, I would say it is true. The problem is that the succession $p_n$ is not constant, so I do not know how to conclude in a formal way. Thanks to who will solve my doubt.","['law-of-large-numbers', 'probability-theory', 'convergence-divergence']"
2416669,The inverse Laplace transform of $e^{-z}\textrm{Ei}(z)z^{-1}\log(z).$,"For a work I need to evaluate the following integral: $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{uz}e^{-z}\textrm{Ei}\left(z\right)z^{-1}\log\left(z\right)dz,\ c>0,\,u>2$$ where $\mathrm{Ei}\left(z\right)$ is the exponential integral function . I know that $$\mathfrak{L}^{-1}\left(z^{-1}\log\left(z\right)\right)\left(u\right)=(-\log\left(u\right)-\gamma)1_{u>0}$$ where $\gamma$ is the Euler-Mascheroni constant , so my idea was to find the inverse Laplace transform of $e^{-z}\textrm{Ei}\left(z\right)$ and then to use the convolution theorem. I found that $$e^{-z}\textrm{Ei}\left(z\right)=PV\int_{0}^{\infty}\frac{e^{-uz}}{1-u}du\tag{1}$$ so my question is: Can I use the convolution theorem even if the integral in $(1)$ exists only in the sense of the Cauchy Principal Value ? In other words, can I write $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{uz}e^{-z}\textrm{Ei}\left(z\right)z^{-1}\log\left(z\right)dz=PV\int_{0}^{u}\frac{\log\left(t\right)+\gamma}{u-t-1}dt?$$ I don't know if it is a standard property or is a idiocy. I searched in my textbooks but I didn't find anything like that. Thank you for your time.","['laplace-transform', 'integral-transforms', 'complex-analysis', 'integration', 'special-functions']"
2416673,Find closed formula by changing order of summation: $\sum_{i=1}^ni3^i$,"Working on homework for a probability and computing class, but my ability to work with summations is rusty to say the least, so I suspect this is going to turn out pretty straightforward. Problem asks to find a closed formula for $$\sum_{i=1}^ni3^i$$ by representing it as a double sum and changing the order of summation. I did that by following a hint from the instructor and came up with $$\sum_{k=1}^n\sum_{i=k}^n3^i,$$ but I'm not really sure what that accomplished. What's the next step? What am I looking for here?","['exponential-function', 'closed-form', 'algebra-precalculus', 'summation', 'geometric-progressions']"
2416690,Size of a $3$-colored square grid to produce a monochrome rectangle,"Given a square grid, dimension $k\times k$, how big does $k$ have to be so that a $3$-coloring  will always produce a monochrome rectangle - a set of some four same-colored points of the grid in a rectangle aligned to the grid axes? (There will be a monochrome rectangle when, for some choice of $a,b,c,d\in \big[1,k\big]$ with $a\neq b$ and $c\neq d$, all the points $(a,c),(a,d),(b,c),(b,d)\,$ have the same color.) This is a follow-on from Every point of a grid is colored in blue, red or green. How to prove there is a monochromatic rectangle? , where a $3$-colored $4\times 19$ grid is shown to contain a monochrome rectangle, so we already know $k\le 19$. My current work shows $k\le 12$ but I think the limit on $k$ can be smaller. ${\large k\le 12}$ For each column we have a number of grid points of each color present. These produce a number of pairs of matched colors. The minimum number of such pairs in a column in a $12\times 12$ grid is achieved when the count of different colors is $(4,4,4)$ giving $\binom 42+\binom 42+\binom 42=18$ pairs. Over $12$ columns we will have at least $12\times 18=216$ matched-color grid pairs. There are $\binom {12}{2} = 66$ possible pair positions and with a choice of $3$ colors $3\times 66= 198$ different color/pair combinations. Clearly with at least $216$ pairings in a $12\times 12$ $3$-colored grid there must be a color-matched pairing between columns and  hence a monochrome rectangle. Can you improve on this limit?","['combinatorics', 'coloring']"
2416735,Existence and Uniqueness Theorem of $y'=1+2\sqrt{x-y}$,"I solved the following differential equation: $$y'=1+2\sqrt{x-y}$$ the solution I found is $\sqrt{x-y}=-x+C$ and $x=y(y\ne0)$ after that I was asked the following questions: a) Find a solution that that maintain $y(1)=3$ and find it's domain. when I input $y(1)=3$ into the equation I get $(c-1)^2=-2$ and that's impossible. I don't understand why it is considered a solution and what is the domain. b) Find two different solutions that maintain $y(1)=1$ and explain why it does not contradict the Existence and Uniqueness Theorem. When I input $y(1)=1$ I get $C=1$ I think that the equation is continuous for $y\gt0$ and $y\lt 0$ so for each we have one solution. Is this true. Also, I don't fully understand what does it mean that a differential equation has a ""solution"". I would like a small explanation. Answer for any of the questions above would be highly appreciated.",['ordinary-differential-equations']
2416760,"Find all of the points of the form $(x, −1)$ which are $4$ units from the point $(3, 2)$","Find all  points of the form $(x, −1)$, which are $4$ units from the point 
  $(3, 2)$. I understand the distance formula, I think.  Also, I don't know how to format for math on here yet so I apologize for that. \begin{align}
&\text{dist} = 4 = \sqrt{(x-3)^2 + (2-(-1))^2)} \\
&\implies 16 = (x-3)^2 + (2 + 1)^2 \tag{square both sides} \\
&\implies 16 = x^2 -6x + 18 \tag{expand} \\
&\implies 0 = x^2 -6x +2 \tag{zero on the left}
\end{align} Next should be finding the factors which would give me the answers but I have no idea what would work for this. $$(x + \text{something})(x - \text{something}).$$ I must be doing something wrong or missing something.  Please correct me.","['systems-of-equations', 'roots', 'analytic-geometry', 'algebra-precalculus', 'quadratics']"
2416766,Find $\tan^{-1} (i\sqrt{2})$.,"Problem: Find $\tan^{-1} (i\sqrt{2})$. My attempt: We must find $z \in \mathbb{C}$ such that $\tan z = i\sqrt{2}$. $$\tan z = \frac{\sin z}{\cos z} = i\frac{e^{-iz} - e^{iz}}{e^{iz} + e^{-iz}}$$ Let $u = e^{iz}$. Then $$i\frac{u^{-1}-u}{u+u^{-1}} = i\sqrt{2}$$ so $$1-u^2 = \sqrt{2}(u^2 + 1)$$
and
$$u^2 = \frac{1-\sqrt{2}}{1+\sqrt{2}}$$
Then we have 
$$e^{iz} = \sqrt{\frac{1-\sqrt{2}}{1+\sqrt{2}}}$$
Taking the natural logarithm of both sides gives
$$z = \frac{1}{2i}\ln\frac{1-\sqrt{2}}{1+\sqrt{2}}$$ Since $\ln$ is multi-valued and $\frac{1-\sqrt{2}}{1+\sqrt{2}}+0i=\frac{1-\sqrt{2}}{1+\sqrt{2}}e^{i2n\pi},$
we have 
$$\frac{1}{2i}\ln{\frac{1-\sqrt{2}}{1+\sqrt{2}}}=\frac{-i}{2}\texttt{Ln}\frac{1-\sqrt{2}}{1+\sqrt{2}}+ n\pi$$ That's how far I got. The book lists the final answer as $$\pi/2 + n\pi -i\texttt{Ln}(\sqrt{2}-1)$$","['trigonometry', 'complex-numbers', 'inverse-function']"
2416817,Prove that $\left|\begin{smallmatrix}a&-b&-c&-d\\b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{smallmatrix}\right|=(a^2+b^2+c^2+d^2)^2$,"Let $a, b, c, d \in \mathbb K$ where $\mathbb K$ is a field. Prove that $$\det \begin{bmatrix}
a & -b & -c & -d\\ 
b & a & -d & c\\ 
c & d & a & -b\\ 
d & -c & b & a
\end{bmatrix} = (a^2+b^2+c^2+d^2)^2$$ I'm looking for a smart way to solve this problem. If we denote $$A = \begin{bmatrix}
a & -b \\ 
b & a \\ 
\end{bmatrix}$$ and $$B = \begin{bmatrix}
-c & -d \\ 
-d & c \\ 
\end{bmatrix}$$ we have that
$$ \begin{bmatrix}
a & -b & -c & -d\\ 
b & a & -d & c\\ 
c & d & a & -b\\ 
d & -c & b & a
\end{bmatrix} = \begin{bmatrix}
A & B \\ 
-B & A \\ 
\end{bmatrix} $$
So it's sufficient to proof that $$ \det \begin{bmatrix}
A & B \\ 
-B & A \\ 
\end{bmatrix} = (\det A - \det B)^2. $$ Help?","['matrices', 'linear-algebra', 'determinant']"
2416819,Matrix of absolute values and largest eigenvector,"Let $A$ be a complex Hermitian $n\times n$ matrix and define the matrix $B$ to be the entry-wise absolute value of $A$, i.e., $B_{ab}=\lvert A_{ab}\rvert$. Furthermore suppose that $B$ has a unique normalised eigenvector $x$ of maximal eigenvalue $\lambda>0$, $Bx=\lambda x$, in particular $\lVert B\lVert=\lambda$ (here $\lVert \cdot\rVert$ denotes the induced matrix norm from the Euclidean norm on $\mathbb C^n$). It is obvious that $\lVert A\rVert\le \lVert B\rVert$. I now first want to consider the extreme case when $\lVert A\rVert=\lVert B\lVert$. It follows that $A$ has an normalized eigenvector $y$ of eigenvalue $\lambda$, $Ay=\lambda y$ and we can compute 
\begin{align*} \lambda =\langle y, Ay\rangle= \sum_{ab}\overline{y_a} A_{ab}y_b\le \sum_{ab} \lvert y_a\rvert B_{ab} \lvert y_b\rvert =\langle\lvert y\rvert,B \lvert y\rvert\rangle\le \lambda =\langle x,Bx\rangle.\end{align*}
In particular, it follows that $\lvert y\rvert =x$. Now I am interested in how this can be made quantitative. For example, if $\lVert A\rVert \ge (1-\epsilon)\lVert B\rVert$, is it true that $\lVert \lvert y\rvert -x\rVert\le C \epsilon$ for some universal constant $C$? I think the above argument can be made quantitative to give $\lVert \lvert y\rvert -x\rVert\le C \sqrt\epsilon$, where $C$ depends on the spectral gap of $B$ below the eigenvalue $\lambda$.","['eigenvalues-eigenvectors', 'real-analysis', 'spectral-theory', 'linear-algebra']"
2416821,Continuous image of connected set is connected: Proof,"In De La Fuente's Mathematical Methods and Models for Economists , the following is said: Let $f:X\to Y$ be a continuous mapping between two metric spaces. If C is a connected subset of $X$ , then $f(C)$ is connected. The proof goes as in Rudin's Principles, and I cannot understand exactly what Rudin also does not explain: Suppose $f(C)$ is not connected. Then $f(C)=P\cup Q$ , where $P$ and $Q$ are nonempty, separated subsets of $Y$ , that is, $clP\cap Q = \emptyset$ and $P\cap clQ = \emptyset$ Let $$ A = C\cap f^{-1}(P) \\ B = C\cap f^{-1}(Q) $$ and notice that then $$C = A\cup B$$ where neither $A$ nor $B$ is empty, and $$f(A)=P \\ f(B) = Q$$ The proof goes on, but this is where De La Fuente loses me. I can clearly see that $f(A)\subseteq P$ , but not that $P\subseteq f(A)$ . Any thoughts? Thanks!","['general-topology', 'connectedness', 'elementary-set-theory', 'proof-explanation']"
2416824,Induced Subgraph,"Let $G = (V, E)$, be a connected graph with $V$ vertices and $E$ edges. We define $H(x)$ as the number of vertex induced subgraphs with $x$ edges in it. As per definition, $\sum_{i=0}^{i=m} H(i) = 2^V$. Assume we consider null graph as also one of the induced subgraphs. For example, let $G(V, E) = \{(1, 2), (2, 3), (1, 3)\}$, where $V = \{1, 2, 3\}$, then $H(0) = 4$, $H(1) = 3$, $H(2) = 0$, $H(3) = 1$. Also, suppose we write the polynomial $P(x) = \sum_{i=0}^{i=m} H(i) * x^i$, then, what can we say about $P(x)$ in terms of $V$ and $E$? This is because it can help us solve the above algorithm efficiently by differentiating the polynomial $E$ times, putting $x = 0$, every time and getting the corresponding coefficient. Is there an efficient algorithm for the same? Thanks for your help in advance. Update : Thanks to @Dap, my problem is solved. But I was curious to know if anyone can help with ideas to find $\sum_{i=0}^{i=m} {i}^{k} H(i)$ for fixed $k$ as for variable $k$, the solution will be '#-$P-hard$' as @Dap suggested in one of the comments. For example, what could be possible approaches for say $k = 2, 3, 4$. I found out that for $k = 0$, we have $\sum_{i=0}^{i=m} {i}^{0} H(i) = 2^V$, and for $k = 1$, we have $\sum_{i=0}^{i=m} {i}^{1} H(i) = E * 2^V$. Update 2: The modified problem based on summation $\sum_{i=0}^{i=m} {i}^{k} H(i)$ is also solved now, thanks to @Dap.","['combinatorics', 'graph-theory', 'algorithms']"
2416835,Using the Ricci identity for the third covariant derivative of a function - Bochner Weitzenböck identity.,"Let $\nabla$ be the Riemannian connection. I was able to proof the following identity for 1-forms $\eta \in \mathscr{T}^1(M)$ (I am following Lee's book ""Riemannian Manifolds - An Introduction to Curvature""): ""If $\eta$ is a 1-form, let $$\eta_{i;jk}dx^i \otimes dx^j \otimes dx^k$$ be the local expression for $\nabla^2 \eta$. Prove the Ricci Identity $$ \eta_{i;jk}-\eta_{i;kj} = R_{jki}^{~~~~l} \eta_l""$$ Here $R_{jki}^{~~~~l}$ are the coefficients of the Riemannian curvature Endomorphism $R$, which are defined by
$$R_{jki}^{~~~~l} \partial_l = R(\partial_j,\partial_k)\partial_i.$$
At the same time I'm reading R.Schoen's and S.-T. Yau's ""Lectures on Differential Geometry"". In a Proposition (2.2 p.15.) the Bochner Weitzenböck Identity is stated there in the following way: ""Let M be a Riemannian manifold and $f \in C^3(M)$. If $\{x^i\}$ is a normal coordinate system at a point $p \in M$, then we have at $p$ 
$$\Delta |\nabla f|^2 = 2 \sum |f_{ij}|^2 + 2 \sum R_{ij}f_if_j + 2 \sum f_i(\Delta f)_i .""$$
Following the arguments it is stated ""from $f_{ij} = f_{ji}$ and the Ricci formula $f_{jij} = f_{jji} + R_{ij}f_j$ it follows that[...]"". 
Now after some research i assume $f_{jij} = f_{j;ij}$ as far as my understanding goes. So these denote the coefficients of the 3rd covariant derivative $\nabla^3 f$.
It makes sense since $\nabla f$ is a 1-form and $f_{j;ij} = f_{j;ji}$ is indeed true by the symmetry of the Hessian (in the Levi-Cevita case).
Now I have the following problem, going after the Ricci identity in Lee's case I get
$$f_{i;jj} = f_{j;ij} = f_{j;ji} + R_{ijj}^{~~~~~l} f_l$$, but I unfortunately cannot see how it implies the formula with the Ricci tensor involved as firstly the indices seem to mixed up (Using some symmetry identies of the Riemannian curvature tensor did not help me) and secondly I don't quite understand how the contraction on both (!) sides work. Do I also need in this identity the normal coordinates?
I hope my question is stated clear enough. Any help is much appreciated. Kind regards,
Volker","['differential-geometry', 'curvature']"
2416923,"Let $F$ be a field of characteristic not $2$, and let $K$ be an extension of $F$ with $[K: F] = 2$. Show that $K = F (\sqrt{a})$ for some $a\in F$","Let $F$ be a field of characteristic not $2$, and let $K$ be an extension
of $F$ with $[K: F] = 2$. Show that $K = F (\sqrt{a})$ for some $a\in F$; that
is, show that $K = F(\alpha)$ with $\alpha^2= a\in F$. Moreover, show that $K$ is
Galois over $F$. I do not know how to try the first part, could someone help me please? For the second part I am using the following theorem: Let $K$ be a finite extension of $F$. Then $K/F$ is Galois if
and only if $|Gal(K/F)| = [K:F]$ Because $Gal(K/F)=\left \{ id,\sigma  \right \}  $ where $\sigma$ is such that $\sigma(\sqrt{a})=-\sqrt{a}$, then $K$ is Galois over $F$.","['abstract-algebra', 'galois-theory', 'extension-field', 'field-theory']"
2416945,the action of a Lie algebra on itself,"Currently I've read the first 5, almost 6 chapters of Serre's ""Lie algebras and Lie groups"". Let $L$ be a Lie algebra, and suppose $L$ is given as a subalgebra of $End(V)$, where $V$ is some finite dimensional vector space over an alg. closed field $k$. Now, as usual one can identify $End(V) = V\otimes V^*$, where the isomorphism is given on simple tensors by sending $v\otimes w$ to the endomorphism $\psi_{v\otimes w} : z\mapsto w(z)\cdot v$. Thus, $L$ is a subalgebra of $V\otimes V^*$, and it acts on $V\otimes V^*$ through its action on $V$ and hence on $V^*$, and hence on $V\otimes V^*$. I was trying to convince myself that through this action, $L$ is stable under the action of itself. A priori, there are a number of ways one might view $L\subset End(V)$ acting on $V^*$. For example, at first I thought the right action is just by ""precomposition"", ie given $f\in V^*$ and $\varphi\in End(V)$, $\varphi f = f\circ\varphi$. However, this turns out to be wrong. The right one is given by:
$$\varphi.f := - f\circ\varphi$$ Secondly, given an action of $\varphi$ on $V$, and on $W$, there is a natural action of $\varphi$ on $V\otimes W$ given on simple tensors by $\varphi(v\otimes w) = \varphi(v)\otimes\varphi(w)$, but again this turns out to be the wrong action. The right one in our context is instead
$$\varphi.(v\otimes w) = \varphi.v\otimes w + v\otimes\varphi.w$$
In the case $W = V^*$, if one views $v\otimes w$ as the endomorphism $\psi_{v\otimes w} : z\mapsto w(z)\cdot v$, then we get:
$$\varphi.(v\otimes w) = \varphi\circ \psi_{v\otimes w} + \psi_{v\otimes\varphi.w} = \varphi\circ\psi_{v\otimes w} - \psi_{v\otimes w}\circ\varphi$$ Thus, at last, if $x,\varphi\in End(V)$, then we get
$$\varphi.x = \varphi\circ x + x\circ\varphi = [\varphi,x]$$
Thus the action of $\varphi\in End(V)$ on $End(V)$ is just the adjoint action. In particular, $L$ is invariant under $L$. I suppose my question is - ""Why?"" As a novice to Lie algebras, this all seems rather strange to me. Why ""should"" $L$ leave itself invariant, viewed as a subspace of $End(V) = V\otimes V^*$? Intuitively, if $L$ is the tangent space of a certain Lie group, what is the meaning of this action of $L$ on itself? In the case $L = End(V)$, what do the other ""natural"" actions of $End(V)$ on itself by left composition, right composition, or both, mean in the Lie group context? I suppose I'd welcome some examples that might shed light on how to think of these computations. I get the feeling that a lot of this will become clear once I get into the Lie group parts of Serre's book, so I almost considered not posting this question, but I'm posting it anyway, partially just as a reminder to myself to continue to think about these questions as I continue reading.","['abstract-algebra', 'lie-algebras', 'algebraic-geometry', 'lie-groups']"
2416950,"Approximating the function $G(x,y) = \int_x^\infty \frac{\ln(t)^2}{(t^2 -1) ( \ln(t)^2 + y^{2} )^2} \,dt$","This question is related to this previous question of mine . Suppose I define the function $G : [1,\infty) \times \mathbb{R} \to \mathbb{R}$ as the following integral:
$$
G(x,y) = \int_x^\infty \frac{\ln(t)^2}{(t^2 -1) ( \ln(t)^2 + y^2 )^2} \,dt
$$ Again by goofing around on Wolfram Alpha, I know the following facts about this function: $G(x,y) \to 0$ as $x \to \infty$, for all $y \in \mathbb{R}$ (even $y=0$). $G(1,y)$ is of finite value for all $y \neq 0$. In fact, I think that this function has a singularity (over the domain $[1,\infty) \times \mathbb{R}$) $only$ at the point $(x,y) = (1,0)$ $G(x,y) \to 0$ as $y \to \infty$ for all $x \in [1, \infty)$ My question: what are the asymptotics of this function for $x \to \infty$? I know that
$$
\frac{\ln(t)^{2}}{(t^{2} -1) ( \ln(t)^2 + y^{2} )^2} \sim \frac{\ln(t)^{2}}{t^{2} ( \ln(t)^2 + y^{2} )^2}
$$
 as $t \to \infty$. I am unsure how I can use this to give the asymptotics of the function...is there a way to write the asymptotics as some function $f(x,y)$, which is valid for $x \to \infty$ and $all$ $y \in \mathbb{R}$? I am confused how I can understand asymptotics of a such a function when there are two variables involved.","['logarithms', 'asymptotics', 'functions', 'integration', 'approximation']"
2416951,Basis of $\mathbb{R}$ over $\mathbb{Q}$ - reversing Zorn's Lemma,"I was trying to prove the following statement: For $S \subseteq \mathbb{R}$, define $\mathbb{Q}(S)$ by: $$
\mathbb{Q}(S) = \{q_0s_0 + \cdots + q_ks_k : k \in \omega, q_i \in \mathbb{Q}, \text{ and } s_i \in S \text{ for } i \leq k\}
$$ $B \subseteq \mathbb{R}$ is a basis for $\mathbb{R}$ over $\mathbb{Q}$ if $B$ is $\subseteq$-minimal such that $\mathbb{R} = \mathbb{Q}(B)$, i.e. $\mathbb{R} = \mathbb{Q}(B)$ and for all $B' \subseteq B$ if $\mathbb{R} = \mathbb{Q}(B')$ then $B = B'$. Now I can imagine you can prove this by just noting that $\mathbb{R}$ is a vector space over $\mathbb{Q}$ and thus has a basis in the linear algebra sense and showing this aligns with the basis being $\subseteq$-minimal. However, I came up with this alternate proof and was wondering about its validity. Let $S_0 =\mathbb{R}$. It is evident that $\mathbb{Q}(S_0) = \mathbb{R}$, so if this is not a basis then there must be some $S_1$ s.t. $S_0 \supset S_1$ and $\mathbb{Q}(S_1) = \mathbb{R}$. Iterating this, either we will get a basis or we will have a chain $S_0 \supset S_1 \supset \ldots $ s.t. for any $i \in \mathbb{N}$, we have $\mathbb{Q}(S_i) = \mathbb{R}$. Then we can take the intersection $S_\omega = \bigcap\{S_i : i \in \mathbb{N} \}$ and  $\mathbb{Q}(S_\omega) = \mathbb{R}$. Eventually we have to hit a basis otherwise we will exhaust all the elements in the set since we are indexing by ordinals. This argument is exactly Zorn's lemma deconstructed for the $\supset$ relation and I was wondering if the reasoning is valid. More formally, you would consider the set $A = \{X : \mathbb{Q}(X) = \mathbb{R}\}$ and show that for any chain $B \subseteq A$, we have $\bigcap B \in A$ (i.e. $B$ has an upper bound) and thus the set $A$ must have a maximal element $S$, which is a basis for $\mathbb{R}$ over $\mathbb{Q}$.","['set-theory', 'linear-algebra', 'axiom-of-choice']"
2416964,Is the union of a chain of topologies a topology? [duplicate],"This question already has answers here : Union of ascending chain of Topologies (4 answers) Closed 6 years ago . Let's consider the following inclusion chain of topologies on space $X$:
$\tau_1\subset\tau_2\subset\cdots\subset\tau_n\subset\cdots$. 
Let $\tau=\bigcup_{n=1}^\infty \tau_n$. Is $\tau$ a topology? Obviously , the intersection of any two sets from $\tau$ belongs to $\tau$. However, it is not clear whether $\bigcup_{n=1}^\infty A_n\in\tau$ where $A_n\in\tau_n$. I think, in general $\tau$ is not a topology but cannot find a counterexample.",['general-topology']
2417019,Intuition on Kulkarni-Nomizu product,"$\renewcommand\vec[1]{{\boldsymbol #1}}$If $V$ is a vector space and $T,S \in \mathfrak{T}^0_{\;\;2}(V)$ are covariant tensors of type $(0,2)$, then we define their Kulkarni-Nomizu$\newcommand\KN{\bigcirc
\kern-2.5ex\wedge \;}$ product $T \KN S \in \mathfrak{T}^0_{\;\;4}(V)$ by $$(T \KN S)(\vec{x},\vec{y},\vec{z},\vec{w}) \doteq T(\vec{x},\vec{z})S(\vec{y},\vec{w}) + T(\vec{y},\vec{w})S(\vec{x},\vec{z})-T(\vec{x},\vec{w})S(\vec{y},\vec{z}) - T(\vec{y},\vec{z})S(\vec{x},\vec{w}).$$ I understand that $T \KN S$ is a curvature like tensor, in the sense that it satisfies the following symmetries: $(T \KN S)(\vec{x},\vec{y},\vec{z},\vec{w}) = -(T\KN S)(\vec{y},\vec{x},\vec{z},\vec{w}) = -(T \KN S)(\vec{x},\vec{y},\vec{w},\vec{z})$; $(T \KN S)(\vec{x},\vec{y},\vec{z},\vec{w}) = (T \KN S)(\vec{z},\vec{w},\vec{x},\vec{y})$; $(T \KN S)(\vec{x},\vec{y},\vec{z},\cdot)+(T \KN S)(\vec{y},\vec{z},\vec{x},\cdot)+(T \KN S)(\vec{z},\vec{x},\vec{y},\cdot)=0$, if $T$ and $S$ are symmetric. We also have the bonus property that $T \KN S = S \KN T$. And although this quantity appearing frequently in certain calculations is enough justificative for giving it a name and notation, this seems artificial to me so far. I do not have any intuition whatsoever for that formula, nor can I think of a way to write it neatly as $\sum_{\sigma \in S_4}{\rm something}$. We can write $$(T\KN S)(\vec{x},\vec{y},\vec{z},\vec{w}) = \begin{vmatrix} T(\vec{x},\vec{z}) & S(\vec{y},\vec{z}) \\ T(\vec{x},\vec{w}) & S(\vec{y},\vec{w})\end{vmatrix}-\begin{vmatrix} T(\vec{y},\vec{z}) & S(\vec{x},\vec{z}) \\ T(\vec{y},\vec{w}) & S(\vec{x},\vec{w})\end{vmatrix},$$but I'm failing to interpret this either. I'd like some insight on this definition. If I didn't mess up, we have the more ""symmetric"" expression $$(T\KN S)(\vec{x},\vec{y},\vec{z},\vec{w}) = \begin{vmatrix} T(\vec{x},\vec{z}) & S(\vec{x}+\vec{y},\vec{z}) \\ T(\vec{x},\vec{w}) & S(\vec{x}+\vec{y},\vec{w})\end{vmatrix}+\begin{vmatrix} S(\vec{x},\vec{z}) & T(\vec{x}+\vec{y},\vec{z}) \\ S(\vec{x},\vec{w}) & T(\vec{x}+\vec{y},\vec{w})\end{vmatrix}.$$Still not happy.","['tensor-products', 'riemannian-geometry', 'differential-geometry']"
2417029,How can points that have length zero result in a line segment with finite length? [duplicate],"This question already has answers here : If point is zero-dimensional, how can it form a finite one dimensional line? (7 answers) Closed 6 years ago . I have been told that a line segment is a set of points.
How can even infinitely many point, each of length zero, can make a line of positive length? Edit:
As an undergraduate I assumed it was due to having uncountably many points.
But the Cantor set has uncountably many elements and it has measure $0$. So having uncountably many points on a line is not sufficient for the measure to be positive. My question was: what else is needed?  It appears from the answers I've seen that the additional thing needed is the topology and/or the sigma algebra within which the points are set. My thanks to those who have helped me figure out where to look for full answers to my question.","['intuition', 'infinity', 'education', 'geometry']"
2417081,"Show that $\frac{1}{\sqrt{r_3}}=\frac{1}{\sqrt{r_1}}+\frac{1}{\sqrt{r_2}}$ for three mutually tangent circles, each tangent to a common line","Three circles are tangents to the line $AB$. Being $r_1$ the radius of the biggest one, $r_2$ the radius of the middle one and $r_3$ the radius of the smallest. Show that $$\dfrac{1}{\sqrt{r_3}}=\dfrac{1}{\sqrt{r_1}}+\dfrac{1}{\sqrt{r_2}}.$$ Hint:show that $AB = 2\sqrt{r_1r_2}$. I know I have to use the Pitagora`s Theorem.","['circles', 'radicals', 'geometry', 'tangent-line', 'fractions']"
2417109,Complex derivative from definition,"Working directly from the definition of a derivative, show that $f(z)=3ze^{iz}+|z|^2+4$ is differentiable at the origin an determine $f'(0)$. I got: $$f'(0)=\lim_{z \to 0}\frac{3ze^{iz}+|z|^2+4-4}{z}=\lim_{z \to 0}\frac{3ze^{iz}+|z|^2}{z}\frac{\bar{z}}{\bar{z}}$$ but at this point I don't know what do do with that $e^{iz}$, is that $e^{-y}(\cos x + i \sin x)$? (If $z = x+iy$) Also, $|z|$ is not differentiable anywhere, so, how is this even possible!?","['derivatives', 'complex-analysis']"
2417123,solution of a ODE $dy/dx = ((y+a)(y+b) + x)/(y+b)^2$,"In my work, I derived an ODE of the following form $\frac{dy}{dx} = \frac{(y+a)(y+b) + x}{(y+b)^2}$ where $a > 0$ and $b > 0$ are constants. I wonder if there is an analytical solution of this equation? Anyone can give a clue on how to solve it?",['ordinary-differential-equations']
2417128,"Find $\int \frac{\sqrt{2-3x}\,dx}{x^2+1}$","Find $$\int \frac{\sqrt{2-3x}\,dx}{x^2+1}$$ I used substitution $x=\frac{2}{3} \sin^2 y$ So we get $dx=\frac{4}{3} \sin y \cos y dy$ hence $$I= \frac{36\sqrt{2}}{3} \int \frac{\sin y \cos^2 y\, dy}{4 \sin^4 y+9}$$ Now if again i use substitution $\cos y=t$ we get
$$I=-12\sqrt{2} \int \frac{t^2 \,dt}{4t^4-8t^2+13}$$ $$I=-12 \sqrt{2} \int \frac{dt}{4t^2+\frac{13}{t^2}-8}$$ Any other approach?","['indefinite-integrals', 'integration', 'calculus']"
2417129,Proving that roots of $P_n(z) := \sum k^3 z^k$ satisfy $|z| < 1$.,"For each positive integer $n$, define the polynomial
$$
P_n(z) = z + 2^3z^2 + ... + n^3z^n.
$$ 
Prove that the roots of $P_n(z)$ lie inside the unit circle. Assuming the existence of $|\alpha| \geq 1, P_n(\alpha) = 0$, I tried using the triangle inequality: 
$$
|\alpha|^nn^3 = |\alpha + 8\alpha^3 + ... + (n-1)^3 \alpha^{n-1} | \leq |\alpha| + ... + (n-1)^3 |\alpha|^{n-1}
$$
which seems to be too weak to create a contradiction. Thoughts/solutions are appreciated.","['algebra-precalculus', 'complex-numbers']"
2417230,"Prove that if A is a subset of B intersect C, then A is a subset of B and C. Show why this doesn't work for union.","I try to prove that if $A \subset B \cap C$, then $A \subset B$ and $A \subset C$.
I go through: (1) For all $x \in A, x \in B \cap C$. (2) For all $x \in A, x \in B$ and $x \in C$ (3) For all $x \in A, x \in B$ and for all $x \in A, x \in C$ (4) So $A \subset B$ and $A \subset C$ I'm not sure if step 2 to 3 is sound. But my real question is why I can't replace all ""$\cap$""s with ""$\cup$""s and all ""and""s with ""or""s and prove the obviously false $A \subset B \cup C \implies A \subset B$ or $A \subset C$.",['elementary-set-theory']
2417236,Is it possible to construct a strictly monotonic sequence of all rational numbers?,"I know that the set of all rational numbers is countable, and can be enumerated by a sequence, say $\{a_n\}$.  But can we construct a monotonic $\{a_n\}_{n=1}^{\infty}$, e.g. with $a_k<a_{k+1}$?  It doesn't seem plausible to me, because then $a_1$ would be the smallest rational number, which clearly can't be any finite number.  Am I mistaken?","['real-analysis', 'rational-numbers', 'sequences-and-series']"
2417239,Frechet differentiable implies continuity in $\mathbb{R} ^n$,"Need help completing a proof. The statement is as follows: Let $E \subset \mathbb{R} ^m$ be an open set. If $f:E \longrightarrow \mathbb{R}^n $ is Frechet differentiable at $a \in E$ , then it is continuous at a. Proof : Let $f:E \longrightarrow \mathbb{R}^n $ be Frechet differentiable. Then  for $a \in E$ , $\exists$ a linear map $A:\mathbb{R}^m \longrightarrow \mathbb{R}^n $ s.t. $\forall \epsilon>0$ , $\exists\delta >0$ , if $h \in \mathbb{R}^m$ and $0<||h||<\delta$ , then $$||f(a+h)-f(a)-Ah||<\epsilon||h||$$ We then have for $$||f(a+h)-f(a)||\leq||h||\frac{||f(a+h)-f(a)-Ah||}{||h||}+||Ah||$$ by the triangle inequality. But I get stuck here. I feel that I'm very close to finishing it but just cant see it. Any hints or help will be appreciated!",['ordinary-differential-equations']
2417289,Show that determinant of a symmetric matrix of order $4$ is $0$,"Without expanding prove that the determinant of the following matrix is $0$.
$$ \begin{bmatrix}1^2 & 2^2 & 3^2 & 4^2\\2^2 & 3^2 & 4^2 & 5^2\\3^2 & 4^2 & 5^2 & 6^2\\4^2 & 5^2 & 6^2 & 7^2\end{bmatrix}$$ It is a symmetric matrix. I'm trying to show by operation that any two row or any two column are identical, but I'm unable to do that. Any hint.?","['matrices', 'linear-algebra', 'determinant']"
2417293,"How to enumerate the possible ""puzzles"" resulting from seven adjacent hexagonal tiles?","There are seven hexagonal tiles; one in the center and six surrounding it. Each tile may have from one to six lines. Here is an example: Each tile can be rotated (and indeed, to solve the puzzle, one must rotate each tile so that all of the lines meet), but this of course does not generate a new puzzle. Under the modest assumption that the center tile must have at least four lines , how many unique puzzles are possible? There are five unique center tiles - three with four lines, one with five lines, and one with six lines. Also, there are four unique edge tiles; the example shows two, and aside from that there is the tile with one line and the tile with two adjacent lines. The number of lines in the center tile poses certain constraints on the admissible edge tiles - for example, the edge tile with two lines that are not adjacent cannot be present if the center tile has six lines. In the context I encountered this problem, I only observed ten unique puzzles. I suspect there must be many more, although I do not know how to enumerate them beyond a brute force approach. Any suggestions?","['combinatorics', 'discrete-mathematics']"
2417356,Conditional expectation as a Radon-Nikodym derivative.,"I found the following very nice post yesterday which presented the conditional expectation in a way which I found intuitive; Conditional expectation with respect to a $\sigma$-algebra . I wonder if there is a way to see that $E(X\mid \mathcal{F}_n)(\omega)=\frac 1 {P(E_i)} \int_{E_i}X \, dP$ if $\omega \in E_i$, could be regarded a Radon-Nikodym derivative. I cant formally connect the dots with respect to for example to the Wikipedia discussion, https://en.wikipedia.org/wiki/Conditional_expectation . I am missing the part where the measure gets ""weighted"" i.e somthing analogous to $\frac 1 {P(E_i)}$, in the Wikipedia article. Update It just hit me that if one divides the defining relation by the measure of the set the one has $\frac{1}{P(E_{i})} \int_{E_i}X \, dP=\frac{1}{P(E_{i})} \int_{E_i}E(X\mid \mathcal{F}_n) \, dP$ for all $E_{i}\in \mathcal{F}_{n}$. This looks like we have a function which agrees with the avarages of $X$ a.e on every set of the algebra $\mathcal{F}_{n}$. This is not quite what @martini writes but maybe it is a reansable way to look at it aswell? This looks like somthing which would fit into the wikipedia disussion better tho. But it donst sit right on some other occasions. So the question remains, How do I think about this in the right way? If the second way is wrong, then how is the first consistant with the Wikipedia article? My comment of Sangchuls answer will also be of help when understadning my troubles!","['probability-theory', 'conditional-expectation']"
2417365,Euler squared sum of order 2 : $\sum \limits_{n=1}^{\infty} \frac{\left( \mathcal{H}_n^{(2)} \right)^2}{n^2}$,Let $\mathcal{H}_n$ denote the $n$ - th harmonic number. What techniques would one use to prove that $$\sum \limits_{n=1}^{\infty} \frac{\left( \mathcal{H}_n^{(2)} \right)^2}{n^2} = \zeta^2(3) + \frac{19 \zeta(6)}{24}$$ where $\zeta$ denotes the Riemann zeta function.,"['real-analysis', 'euler-sums', 'sequences-and-series']"
2417402,Do Gaussians have a Compact Support,"My understanding of a probability distribution with compact support,
means that the set of valid inputs to query the probability for much be compact. And that in this basically mean that that that set must be bounded, and closed. The support for a $n$ dimensional Gaussian distribution is all of $\mathbb{R}^n$.
It is not bounded, as far as I can tell -- you give me any point, and I can find a point that is further from the origin. (Or more generally, give me an open ball and I can find an open ball that encloses it).
And (importantly perhaps) all mentioned points (in the open balls) are all points that have a chance that they could be sampled from a Gausssian distribution -- so they are actually in the support Thus the Gaussian distribution does not have a compact support. Is my reasoning correct? I ask because I am looking at a paper describing a non-parametric estimator, that only works for estimating distributions with a compact support, and their first example is estimating a Gaussian.","['probability', 'probability-distributions']"
2417421,How to define a topological space of straight lines?,"Today in a differential geometry lecture, the lecturer put down a question for us to think about: Given a regular curve (differentiable function $\alpha:I\to\Bbb R^3$ on an open interval with $\alpha'$ nonzero everywhere) and a point $t\in I$, we consider the two points $\alpha(t+h),\alpha(t-k)$ close to $\alpha(t)$ where $h,k\gt0$. Hopefully when $h,k$ are small enough, the two points are distinct. Then we can define a straight line $L$ through $\alpha(t+h),\alpha(t-k)$, call this line $L(h,k)$. He then asks whether $$\lim\limits_{(h,k)\to(0^+,0^+)}L(h,k)$$ exists, and if it exists, what it would be. As far as I know, defining limits require the notion of topology. How do we define a good topology on the set of all straight lines (required to be infinite on both sides) in $\Bbb R^3$? I have heard of Grassmannian , which parametrises all vector subspaces of a fixed dimension, but this does not fully address my question because I do not require the lines to pass through a specific point. I have some idea on defining a good topology. First given a fixed unit vector $v\in\Bbb R^3$, for each point $x\in\Bbb R^3$, define the line $L(x,v):=\{x+tv\in\Bbb R^3:t\in\Bbb R\}$. Then the family of all lines $L(x,v)$ with $v$ fixed can be parametrised by $\Bbb R^2$. Denote this family of lines $S(v)$. Note that $S(v)=S(-v)$. The set of all straight lines in $\Bbb R^3$ is the disjoint union of all the $S(v)$, with $v$ parametrised by $\Bbb RP^2$ because of $S(v)=S(-v)$. Can the set of all straight lines be seen as a plane bundle over $\Bbb RP^2$? And explicitly how is the plane bundle defined? Is it simply the tangent bundle? Please assume that I know little about smooth manifolds. Edit : I should state some desirable properties of the topology explicitly. For the set $S(v)$ as a subspace of space of all lines, I hope $S(v)$ would be homeomorphic to $\Bbb R^2$. For example, if $v=(0,0,1)=e_3$, I expect the map $P\to S(e_3),(x,y,0)\mapsto L((x,y,0),e_3)$ is a homeomorphism, where $P=\{(x,y,0)\in\Bbb R^3:x,y\in\Bbb R\}$. For each point $x\in\Bbb R^3$, Define $T(x)$ as the set of all lines through $x$. I expect that the subspace $T(x)$ is homeomorphic to $T(0)$ (this $0$ is zero vector), i.e. the Grassmannian/$\Bbb RP^2$. Since no line is special (not even the origin is special), I also expect the whole space of all lines is homogeneous in this sense , that for every two ""points"" $x,y$ in this space, there is a self-homeomorphism sending $x$ to $y$. Edit 2 : There is a related question for a similar problem on $\Bbb R^2$. See A topology on the set of lines? . By the way, if there is any, I would like to see reference on similar problems with solutions , i.e. how to define natural topologies on the set of $k$-dimensional affine subspaces in $\Bbb R^n$, and on a family of curves in $\Bbb R^n$, and on a family of surfaces in $\Bbb R^n$, etc.","['reference-request', 'general-topology', 'smooth-manifolds', 'grassmannian']"
2417448,Could Goldbach's conjecture be proven unprovable using Gödel's first incompleteness theorem?,"""The first incompleteness theorem states that no consistent system of axioms whose theorems can be listed by an effective procedure (i.e., an algorithm) is capable of proving all truths about the arithmetic of the natural numbers. For any such formal system, there will always be statements about the natural numbers that are true, but that are unprovable within the system. The second incompleteness theorem, an extension of the first, shows that the system cannot demonstrate its own consistency."" Could Goldbach's conjecture be seen as a statement that is true but not be provable within that consistent system? Every even number can be written as a sum of two primes. This seems pretty obvious if we just think of it as another axiom. And until we can find an even number that can't be written as a sum of two primes Goldbach's conjecture is as true as 2+2=4.",['number-theory']
2417468,"How to solve this recurrence equation?$A_{n,k}=(n+1-k)A_{n-1,k-1}+(k+1)A_{n-1,k+1}$?","How to solve this recurrence equation?
$$A_{n,k}=(n+1-k)A_{n-1,k-1}+(k+1)A_{n-1,k+1},n=1,2,3,...;k=0,1,2,...$$
and
$$A_{1,0}=1,A_{1,1}=1,A_{2,0}=1,A_{2,1}=2$$
When I studied the following formula, I found the recursive formula, but I couldn't solve it：
$$g_n(x)=\sin^nx(-1)^{n-1}\frac{d^{n-1}}{dx^{n-1}}\left(\frac{1+\cos x}{\sin x}\right)\qquad(1)$$
\begin{align}
g_1(x)&=1+\cos x\\
g_2(x)&=1+2\cos x+\cos ^2x\\
...\\
g_n(x)&=\sum_{k=0}^{n}A_{n,k}\cos ^kx
\end{align}
Any help would be appreciated.","['combinations', 'recurrence-relations', 'trigonometry', 'sequences-and-series']"
2417479,Two dissimilar continued fractions that are equivalent $F(q)=G(q)$,"Given the following two continued fractions,with $|q|\lt1$ $F(q)= \cfrac{1}{1+\cfrac{q}{1+\cfrac{q^2}{1+\cfrac{q^5}{1+\cfrac{q^8}{1+\cfrac{q^{12}}{1+\cfrac{q^{16}}{1+\cfrac{q^{24}}{1+\dots}}}}}}}}\tag1$ where the exponents are given by the Generating function $f(z)=\frac{-2z^4-4z^3-3z^2-2z-1}{2z^2-1}$ and $G(q)= 1-\cfrac{q}{1+\cfrac{q}{1-\cfrac{q}{1+\cfrac{q}{1-\cfrac{q^4}{1+\cfrac{q^{4}}{1-\cfrac{q^{4}}{1+\cfrac{q^{4}}{1-\dots}}}}}}}}\tag2$ in which case the exponents are given by the generating function $g(z)=\frac{-2z^7-2z^6-2z^5-2z^4-z^3-z^2-z-1}{2z^4-1}$ and satisfy the recurrence relation $a(n+4)=2a(n)$ for $n\geq4$ How do we prove that $F(q)=G(q)$? Note:by comparing coefficients of each continued fraction,we observe that they are equal. Curiously though,by plotting each generating function on the complex plane yields beautiful images( f(z) and g(z) ) of bugs","['conjectures', 'continued-fractions', 'sequences-and-series']"
2417492,Precedence of trigonometric functions over multiplication,"In another question (see here ) appeared the expression $ \cos ~ x(5-4\sin~x) $, that I interpreted as $ \cos(~x(5-4\sin(x))~) $, erroneously because everybody  understood it as $ (\cos(x))~(5-4\sin(x)) $. However, it seems that the correct interpretation of $ \cos~2x $ is $ \cos(2x) $. I do not known which one is the correct one for $ \cos~xy $. Please, are you so kind of explain a few the rules of precedence when trigonometric functions appears ? Googling internet it appears references to PEMDAS/BODMAS/BIDMAS, but it seems these ones doesn't includes the trigonometric functions.","['trigonometry', 'binary-operations']"
2417497,"Show that if $f(tx)=tf(x)$ for all $t>0,x\in E\setminus\{0\}$ and $f\in C^1(E,F)$ then $f\in\mathcal L(E,F)$","Can someone confirm if this exercise is correctly done? The context of the exercise is multivariable calculus, that is, $E$ and $F$ are Banach spaces of unknown dimension. What I did: I must show that if $f\in C^1(E,F)$ and $f(tx)=tf(x)$ for all $t>0$ for all $x\in E\setminus\{0\}$ then $f\in\mathcal L(E,F)$. Using the chain rule I know that $$\partial[f(tx)]=t\partial f(tx)=t\partial f(x)=\partial[t f(x)]\implies \partial f(tx)=\partial f(x),\quad \forall t>0$$ Thus, by the continuity of $\partial f$, $\partial f(x)$ is a constant function for each $x\in E$, and by differentiation it can be seen that $$f(x)=\partial f(x)x+v\quad\text{and}\quad f(tx)=tf(x)\implies v=0$$ Thus $f$ is linear, as required. UPDATE: I now think that the above is not totally correct. I solved the exercise in a different way (with a more explicit result) using the fact that $f$ is differentiable at zero. From the first attempt above we knows that $\partial f$ is a constant function in the sets $\{tx:t>0\text{ and } x\in E\setminus\{0\}\}$. And because $f$ is differentiable at zero by assumption then using the directional derivatives of $f$ at zero we can see that $$D_vf(0):=\lim_{t\to 0}\frac{f(0+tv)-f(0)}t\in F\implies\matrix{f(0)=0\;\text{ and }\\D_vf(0)=f(v)=-f(v),\forall v\in E\setminus\{0\}}$$ Thus $f$ is a constant function and because $f(0)=0$ then $f=0$.","['multivariable-calculus', 'proof-verification', 'frechet-derivative']"
2417520,How can I compare $\log_2 3$ and $\log_3 5$ without using a calculator [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Compare $\log_2 3$ and $\log_3 5$ without using a calculator. I am not very good at math please explain it clearly","['algebra-precalculus', 'logarithms', 'number-comparison']"
2417522,Arrangement of colored cars in traffic,"Seeing helicoptor captures of car packed streets, and the sort order of colors can be observed, if for simplicity we limit the scope of the pictures to say 9 or 10 cars, I wondered whether it is possible to count all the unique arrangements of the 9 colored cars in a line (assuming we have 4 blue and 5 green ones) where the order g..g..b..g..b can always be found in the arrangement. That is, an order involving only 5 cars is sought for out of the 9. I have a feeling it is very similar to introductory problems in probability involving a bowl of colored balls, where we know how the bowl is composed, and we are asked how many arrangements of this and that color can be observed in order, if we pick a number of balls at a time.",['combinatorics']
2417523,Integration wrt countaing measure on uncountable set.,"I was reading integration with respect to counting measure . Couldn't we prove this without MCT?: \begin{align} \int_X f \, d\mu & = \sup \{ \int_X \phi \, d\mu \ ,: \, 0 \le \phi \le f \text{ and $\phi$ simple } \}  \\
&= \sup_{|A| < \infty, A \subseteq X} \Big\{ \sum_{x \in A} \phi(x) \, :\, 0 \le \phi \le f  \text{ and $\phi$ simple } \Big\} \\
&= \sup_{|A|<\infty, A \subseteq X} \Big \{ \sum_{x \in A} f(x) \Big \} \\
&= \sum_{x \in X} f(x) 
\end{align} Further, this definition chase, also works when $X$ is uncountable. I feel like this argument is a lot simpler, or is there something wrong?","['alternative-proof', 'integration', 'measure-theory']"
2417532,Angle from rotation matrix,"I have an exam later today and I have come across something which is not covered in my lecture notes (that I know of) that I need to know the answer for! Could someone please explain to me how you get the answer for the following question? I have been looking all over to try and solve this but not having much luck with my google searches :-) As you can tell I am a bit of a novice when it comes to Maths! Which angle has the following rotation matrix: ( 0 1 )
     (-1 0 )

 [A] 0
 [B] 1/2 π
 [C] π
 [D] 3/2 π Thank you in advance for any help that you may be able to give me","['matrices', 'angle']"
2417538,path connectedness implies connectedness,"Let $(X,d)$ be a path connected metric space. Prove that $X$ is a connected space. Towards contradiction assume that $X$ is not connected: $$\exists  U,V \subset X \text{ open, unempty} : U \cap V = \emptyset, U \cup V = X $$ 
Let $x \in U, y \in V$.
Since $X$ is path connected, we find  a continuous map $f: [0,1] \rightarrow X$ such that  $f(0) = x \text{ and } f(1)=y$. since $U,V$ are open with $U \cap V = \emptyset$ we get $S :=f([0,1])\cap(\delta U \cup\delta V) \neq \emptyset $ Now let $p \in S \Rightarrow p \notin U \cup V = X$ which is a contradiction. So X must be connected. Is there a problem with that proof?","['general-topology', 'metric-spaces', 'analysis', 'connectedness']"
2417548,True or false: $\left|\max\limits_{x\in D}f(x) - \max\limits_{x\in D}g(x)\right| \leq \max\limits_{x\in D}\left|f(x)-g(x)\right|$,Is this true or false? $$\left|\max\limits_{x\in D}f(x) - \max\limits_{x\in D}g(x)\right| \leq \max\limits_{x\in D}\left|f(x)-g(x)\right|$$ I know it is an easy question but I forget all analysis. Thank you for your help,"['real-analysis', 'absolute-value', 'calculus', 'functions']"
2417589,Solve this trigonometric equation ${\tan ^2}x + {\cot ^2}x - 3(\tan x - \cot x) = 0$,"Solve the following equation. $${\tan ^2}x + {\cot ^2}x - 3(\tan x - \cot x) = 0.$$ I can't figure out the way to solve this equation. This was my attempt
\begin{array}{l}
{\tan ^2}x + {\cot ^2}x - 3(\tan x - \cot x) = 0\\
 \Leftrightarrow \frac{{{{\sin }^2}x}}{{{{\cos }^2}x}} + \frac{{{{\cos }^2}x}}{{{{\sin }^2}x}} - 3(\frac{{\sin x}}{{\cos x}} - \frac{{\cos x}}{{\sin x}}) = 0\\
 \Leftrightarrow \frac{{{{\sin }^4}x + {{\cos }^4}x}}{{{{\cos }^2}x + {{\sin }^2}x}} - 3\frac{{{{\sin }^2}x - {{\cos }^2}x}}{{\sin x\cos x}} = 0\\
 \Leftrightarrow \frac{{1 + 2{{\sin }^2}x{{\cos }^2}x}}{{{{\sin }^2}x{{\cos }^2}x}} + 3\frac{{\cos 2x}}{{\sin x\cos x}} = 0\\
 \Leftrightarrow \frac{4}{{(1 - \cos 2x)(1 + \cos 2x)}} + 2 + 3\frac{{\cos 2x}}{{\sin x\cos x}} = 0\\
 \Leftrightarrow \frac{4}{{1 - {{\cos }^2}2x}} + 2 + 3\frac{{\cos 2x}}{{\sin x\cos x}} = 0
\end{array}
I don't know how to solve after that. Can anyone help me?","['algebra-precalculus', 'substitution', 'trigonometry']"
2417609,Finding number of unique dice with n sides with values between x and y that add up to z,"I've recently taken an interest in nontransitive dice; specifically corrected Grime dice. I want to see how many other unique dice there are that follow the same criteria of having $5n$ sides (possible by either displaying each value twice on a d10 or four times on a d20) with values between $1$ and $9$ that add up to $25$ (possibly so that I can see if I can find another set of 5 dice with a more consistent pair of cyclical chains than the currently known set later, but that will be another question if I set about it). So I want to find a formula that will tell me how many unique dice fit the criteria I put forth. Through manual work I think I've found that there are $72$ unique dice when $n=5$, $x=1$, $y=9$, and $z=25$; though I'm not 100% certain that I haven't missed any. I'll include the list that I've found so that people can double-check my work + manual method; if there's a way that I could format it to be easier to read while still not taking up too much space either please do suggest an edit: 1, 1, 5, 9, 9
1, 2, 4, 9, 9
1, 3, 3, 9, 9
2, 2, 3, 9, 9
1, 1, 6, 8, 9
1, 2, 5, 8, 9
1, 3, 4, 8, 9
2, 2, 4, 8, 9
2, 3, 3, 8, 9
1, 1, 7, 7, 9
1, 2, 6, 7, 9
1, 3, 5, 7, 9
1, 4, 4, 7, 9
2, 2, 5, 7, 9
2, 3, 4, 7, 9
3, 3, 3, 7, 9
1, 3, 6, 6, 9
1, 4, 5, 6, 9
2, 2, 6, 6, 9
2, 3, 5, 6, 9
2, 4, 4, 6, 9
3, 3, 4, 6, 9
1, 5, 5, 5, 9
2, 4, 5, 5, 9
3, 3, 5, 5, 9
3, 4, 4, 5, 9
4, 4, 4, 4, 9
1, 1, 7, 8, 8
1, 2, 6, 8, 8
1, 3, 5, 8, 8
1, 4, 4, 8, 8
2, 2, 5, 8, 8
2, 3, 4, 8, 8
3, 3, 3, 8, 8
1, 2, 7, 7, 8
1, 3, 6, 7, 8
1, 4, 5, 7, 8
2, 2, 6, 7, 8
2, 3, 5, 7, 8
2, 4, 4, 7, 8
3, 3, 4, 7, 8
1, 4, 6, 6, 8
1, 5, 5, 6, 8
2, 3, 6, 6, 8
2, 4, 5, 6, 8
3, 3, 5, 6, 8
3, 4, 4, 6, 8
2, 5, 5, 5, 8
3, 4, 5, 5, 8
4, 4, 4, 5, 8
1, 3, 7, 7, 7
1, 4, 6, 7, 7
1, 5, 5, 7, 7
2, 2, 7, 7, 7
2, 4, 5, 7, 7
3, 3, 5, 7, 7
3, 4, 4, 7, 7
1, 5, 6, 6, 7
2, 4, 6, 6, 7
2, 5, 5, 6, 7
3, 3, 6, 6, 7
3, 4, 5, 6, 7
4, 4, 4, 6, 7
3, 5, 5, 5, 7
4, 4, 5, 5, 7
1, 6, 6, 6, 6
2, 5, 6, 6, 6
3, 4, 6, 6, 6
3, 5, 5, 6, 6
4, 4, 5, 6, 6
4, 5, 5, 5, 6
5, 5, 5, 5, 5 I'd like it if $n=5$ $x=1$ $y=9$ $z=25$ could be used to demonstrate formulas so that I can see what an easier way to arrive at my conclusion would have been and if I came to the correct conclusion. I'd also like examples using $n=6$ $x=1$ $y=6$ $z=21$ so that I can also see an answer that I didn't (attempt to) manually work out. If I'm using any incorrect tags or I'm not using a tag that I should be using, please let me know.","['multivariable-calculus', 'combinatorics', 'extremal-combinatorics']"
2417632,What does reasonably behaved function mean in the context of mathematics?,"I was reading through computer networks, and I saw an expression that I had never seen before when discussing Fourier transformations: “ reasonably behaved function .” What does this mean about a function? I am not able to find an explanation online. Is this expression commonly used? EDIT: The actual question used in the book: In the early 19th century, the French mathematician Jean-Baptiste Fourier proved that any reasonably behaved periodic function, $g(t)$ with period $T$ can be constructed as the sum of a (possibly infinite) number of sines and cosines.","['real-analysis', 'calculus', 'functions', 'functional-analysis', 'terminology']"
2417670,"Munkres Topology, page 102, question 19:a","PROBLEM If $A \subset X$, we define the boundary of $A$ by the equation
  $$\text{Bd } A = \bar{A} \cap \overline{X - A}.$$ [Munkres Topology, page 102, question 19:a] Show that $\text{Int } A$ and $\text{Bd } A$ are disjoint, and $\bar{A} = \text{Int } A \cup \text{Bd } A$. MY ATTEMPT (via an EXAMPLE) Suppose that I have the closed disk
$$A = \left\{(x,y) \in \mathbb{R}^2 | x^2 + y^2 \leq 1\right\}.$$ Clearly, since $A$ is closed, then I have $\bar{A} = A$. I also have
$$\text{Int } A = \left\{(x,y) \in \mathbb{R}^2 | x^2 + y^2 < 1\right\},$$
and
$$\text{Bd } A = \left\{(x,y) \in \mathbb{R}^2 | x^2 + y^2 = 1\right\},$$
so that
$$\text{Int } A \cap \text{Bd } A = \emptyset$$
(i.e., $\text{Int } A$ and $\text{Bd } A$ are disjoint), and
$$\bar{A} = A = \text{Int } A \cup \text{Bd } A.$$ QUESTION This gives some intuition for how to solve the problem in case $A$ is closed (although admittedly, I currently have trouble articulating a general proof [i.e., a proof that does not depend on specific examples, such as what I have given above]).  How do I solve the problem generally for closed $A$?  How about the case when $A$ is open? Added September 05 2017 Of course, a set can be both open and closed, or can be neither open nor closed!  Now I am more confused...  =(","['general-topology', 'proof-writing']"
2417700,Does any Riemannian manifold admit Ricci flow and Ricci soliton? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I started studying Ricci flow and Ricci soliton (personally) and I have read some basic content of Ricci flow. I want to know the following: Does any Riemannian manifold admit Ricci flow and Ricci soliton? Is there difference between  the Ricci flow and the Ricci soliton? Thanks.","['ricci-flow', 'riemannian-geometry', 'differential-geometry']"
2417730,Inequality originated from order statistics,"The following question is given in V.K Rohatgi problem 7.2.4. Let $x_1,x_2,...,x_n$ be real numbers, and let $x_{(n)} = \max(x_1,x_2,...,x_n) $ for $n\geq 2$, and $x_{(1)} = \min(x_1,x_2,...,x_n)$. Show that for any set of real numbers $a_1,a_2,...,a_n$ such that $\sum_{i=1}^n a_i = 0$, the following inequality holds: 
       $$\left|\sum_{i=1}^n a_i.x_i\right| \leq \frac{1}{2}(x_{(n)} - x_{(1)})\sum_{i=1}^n |a_i|$$","['statistics', 'inequality', 'order-statistics']"
2417733,Proof verification: Image of the intersection is the intersection of the images,"$$ f\left(\bigcap\limits_{\lambda \in \wedge} A_{\lambda}\right) = \bigcap\limits_{\lambda \in \wedge} f(A_\lambda)$$ $(\rightarrow)$ Let $b \in f(\bigcap\limits_{\lambda \in \wedge} A_{\lambda})$
$\rightarrow b=f(a)$ for some $a \in (\bigcap\limits_{\lambda \in \wedge} A_{\lambda})$. Since $a \in (\bigcap\limits_{\lambda \in \wedge} A_{\lambda}) \rightarrow a \in A_{\lambda}$ for some $\lambda \in \wedge$. Since $b=f(a)$, then $b \in f(A_{\lambda})$ for some $\lambda \in \wedge$. $\rightarrow b \in \bigcap\limits_{\lambda \in \wedge} f(A_\lambda)$
$\rightarrow f(\bigcap\limits_{\lambda \in \wedge} A_{\lambda}) \subset \bigcap\limits_{\lambda \in \wedge} f(A_\lambda)$. $(\leftarrow)$ Let $b \in \bigcap\limits_{\lambda \in \wedge} f(A_\lambda) \rightarrow b \in f(A_\lambda)$ for some $\lambda \in \wedge$ $\rightarrow (*) b=f(a)$ for every $a \in A_\lambda$ and $\lambda \in \wedge$ $\rightarrow b=f(a)$ for every $a \in \bigcap\limits_{\lambda \in \wedge} A_\lambda$ In order for $(*)$ to be true, $f$ must be $1-1$ $b=f(a)$ for every $a \in \bigcap\limits_{\lambda \in \wedge} A_\lambda \rightarrow b \in f(\in \bigcap\limits_{\lambda \in \wedge} A_\lambda)$ Both $(\rightarrow)$ and $(\leftarrow)$ imply that $$ f\left(\bigcap\limits_{\lambda \in \wedge} A_{\lambda}\right) = \bigcap\limits_{\lambda \in \wedge} f(A_\lambda)$$ This problem is in Willards, ""General Topology."" Please let me know if this solution is accurate.","['elementary-set-theory', 'proof-verification']"
2417736,Asymptotic of a sum: $\sum_{k=n}^{\infty}{f(k)} = \int_{n}^{\infty}{f(t)dt} +\frac{f(n)}{2}+\mathcal{O}(f'(n))$,"Someone told me that the following formula holds for $f$ differentiable and decreasing, with $\lim_{x\rightarrow +\infty}{f(x)}=0$. $$\sum_{k=n}^{\infty}{f(k)} = \int_{n}^{\infty}{f(t)dt} +\frac{f(n)}{2}+\mathcal{O}(f'(n))$$ But I managed to prove only if the function is convex, with the help of the formula $$f(x+h)=f(x)+hf'(x)+\int_{0}^{1}{h\left[ f'(x+ht)-f'(x) \right]dt}$$ Which give us integrating it $$\int_{0}^{1}{f(x+\theta)d\theta}=f(x)+\frac{f'(x)}{2}+\int_{0}^{1}{\int_{0}^{1}{\theta\left[ f'(x+\theta t)-f'(x) \right]dt}d\theta}$$ And then $$\begin{align}
\int_{0}^{1}{f(x+\theta)d\theta}
&=\frac{f(x+1)+f(x)}{2}\\&\qquad+\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta(f'(x+\theta t)-f'(x))dt}\right) -\frac{(f'(x+\theta)-f'(x))}{2}\right]d\theta}\\
&=\frac{f(x+1)+f(x)}{2}+\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta f'(x+\theta t)dt}\right)-\frac{f'(x+\theta)}{2}\right]d\theta}
\end{align}$$ If we assume that the function is convex, then the term inside the integral is positive, because $f'(x+\theta t)\geq f'(x+\theta)$, and also holds that, as $f'(x+\theta t)\leq f'(x)$ and $f'(x+\theta)\geq f'(x+1)$ $$\begin{align}\int_{0}^{1}{\left[\left(\int_{0}^{1}{\theta f'(x+\theta t)dt}\right)-\frac{f'(x+\theta)}{2}\right]d\theta}
&\leq \frac{1}{2}\int_{0}^{1}{(f'(x)-f'(x+1))d \theta}\\&=\frac{f'(x)-f'(x+1)}{2}
\end{align}$$ Summing the expression and using these last inequalities, the result follows. Can someone help me to prove this for only differentiable functions, not necessarily convex?","['asymptotics', 'calculus', 'integration', 'summation', 'analysis']"
2417795,"If $8\sin x - \cos x=4$, then find possible values of $x$",I am not understanding what exactly can watch do here. First I thought that if I could square it but it was in vain. Please help me.,['trigonometry']
2417851,Is it true: 2 does not divide 1? [duplicate],"This question already has answers here : What does it mean to say ""a divides b"" (2 answers) Closed 6 years ago . Is this true? 2 does not divide 1? Does a proof exist for this? Is it because the outcome will be a fraction and that is not allowed in discrete mathematics? I am new to this.","['definition', 'discrete-mathematics']"
2417852,Prove that $\sum_{n=0}^{\infty}{{a_n}{z^n}}$ converges absolutely and uniformly in $D$.,"PROBLEM Suppose that the complex series $\displaystyle \sum_{n=0}^{\infty}{a_n}$ converges.  Let $r < 1$ and set $D = \{z \in \mathbb{C} : |z| < r\}$. Prove that $\displaystyle \sum_{n=0}^{\infty}{{a_n}{z^n}}$ converges absolutely and uniformly in $D$. MY FUTILE ATTEMPTS I know that I must somehow use the result(s) in this question: If the complex series $\displaystyle \sum_{n=0}^{\infty}{a_n}$ converges, show that there exists a positive number $A$ such that $|a_n| \leq A$ for all $n$. . Also, the easiest way that I can think of to solve this problem is to use the following theorem: THEOREM ( Weierstrass M-Test or Dominated Convergence Theorem ) Given the series of functions $\ \displaystyle \sum_{n=1}^{\infty}{{f_n}(z)}, z \in E$.  Suppose that $\{M_n\}$ is a sequence of positive real numbers such that
    (i) $|{f_n}(z)| \leq M_n, \forall n \in \mathbb{N}, \forall z \in E$.  (ii) $\displaystyle \sum_{n=1}^{\infty}{M_n}$ converges.  Then $\displaystyle \sum_{n=1}^{\infty}{{f_n}(z)}$ converges absolutely and uniformly on $E$. Of course I know that I need to set ${f_n}(z) = {a_n}{z^n}$.  Then ${f_n}(z)$ is a power series.  What I don't know is the sequence $\{M_n\}$. QUESTIONS (1) Is the Weierstrass M-Test indeed the best way to tackle this problem?  If so, what should be my $M_n$? (2) If the answer to the first question in (1) is NO , how can I be able to solve this problem?","['complex-analysis', 'absolute-convergence', 'power-series', 'uniform-convergence']"
2417904,Criterion for deciding whether matrix is diagonalizable,"Let $B \in$ GL$_n(\mathbb{C})$. In a paper I'm reading someone probably claims the following: Lemma : For showing that $B$ is diagonalizable it suffices to show the following: Let $\lambda$ be an eigenvalue of $B$ with algebraic multiplicity $\geq 2$ and assume $x, y \in \mathbb{C}^n$ with
\begin{align*}
& (B - \lambda I)x = y  \\
& (B - \lambda I)y = 0
\end{align*}
then we have $y = 0$. Why is this true? Does it have to do with the Jordan normal form?","['eigenvalues-eigenvectors', 'diagonalization', 'jordan-normal-form', 'linear-algebra']"
2417918,"Probability that the sum of $k$ distinct integers selected from $1, 2, \dots, n$ is divisible by $n$","Would you please help me solve Problem 7 of Section 4.5, ""Combinatorial Number Theory,"" in An Introduction to the Theory of Numbers, Niven, Zuckerman, Montgomery, 5th ed., Wiley (New York), 1991: Let $n$ and $k$ be positive, relatively-prime integers with $n > k$ .  Prove that if $k$ distinct integers are selected at random from $1, 2, \dots, n$ , the probability that their sum is divisible by $n$ is $1/n$ . The authors describe three methods for solving combinatorial problems: (1) the pigeonhole principle, (2) the one-to-one correspondence procedure, in which elements in a finite set or between two sets are paired off to determine the number of elements, and (3) the inclusion-exclusion principle.  I know that the denominator of the probability fraction is $n \choose k$ , so I tried (without success) to use the second method to count the number of desired outcomes to be $(n - 1)!/((n - k)! k!)$ . Because $n$ and $k$ are relatively prime, I know that $n \choose k$ is divisible by $n$ .  In that case, numerical evidence suggests that the sum in question is uniformly distributed modulo $n$ .  If I can prove that conjecture, I am done. I found similar problems with particular values for $n$ and $k$ , but the solutions do not help me solve this general case.","['probability', 'combinatorial-number-theory']"
2417934,Prove $\ln(1+x)\geq x-\frac{x^2}{2}$,"When $x\geq0$ prove that: 
  $$\ln(1+x)\geq x-\frac{x^2}{2}.$$ My effort: From the Taylor series: $$\ln(1+x)= x-\frac{x^2}{2}+\frac{x^3}{3}-\cdots$$ I don't know how to continue from here.","['derivatives', 'inequality', 'taylor-expansion', 'logarithms', 'calculus']"
2417935,Set of all real numbers with a '$2$' in their decimal expansion,"It is a common question in an elementary probability or combinatorics class to ask ""How many integers between $1$ and $10^k$ have at least one '$2$' in their decimal expansion?"" This result can then be generalized further, and analyzed as $k$ approaches infinity However, I'm curious about something slightly different; rather than the set of all integers with a '$2$' in their decimal expansion, I want to consider the set of all real numbers with a '$2$' in their decimal expansion. More specifically, I have the following questions: 1) Which has greater cardinality: the set of all real numbers with a '$2$' in their decimal expansion (which we will denote by $S$), or the set of all real numbers without a '$2$' in their decimal expansion (ie $\mathbb{R} \setminus S$)? 2) In comparing the above sets, $S$ and $\mathbb{R} \setminus S$, by 'how much' is one set's size greater than the other? I realize this is a poorly phrased question, and am not really looking for a formal answer. Anything that gives some general intuition would be much appreciated. For question 1, I am leaning towards thinking that the latter set, of all real numbers without a '$2$' in their decimal expansion, would be larger. To show this, I've tried using alternative bases and formulating some sort of mapping between the sets, to no avail. I also thought about simplifying the problem to just include numbers in the set $[0, 1]$, and then generalizing for $\mathbb{R}$. However, I haven't found anything by doing this either. I haven't yet attempted the second question, although I think the way I approach the first will largely dictate the answer here. I'd appreciate any and all help with this problem. Thanks!","['combinatorics', 'infinite-groups', 'elementary-set-theory']"
2417943,Symmetric distribution around zero and median equal to zero,"Let $X$ be a random variable and suppose that it is symmetrically distributed around $0$. Is this equivalent to assume that $X$ has median equal to $0$? If not, are the two assumptions completely unrelated or one implies the other?","['descriptive-statistics', 'statistics', 'symmetry', 'random-variables']"
2417965,Does $\operatorname{tr} (A)=0$ imply $\operatorname{tr} (A^3)=0$?,"Let $A_{(2n+1)\times(2n+1)}$ be a symmetric matrix of Rank $2n$. Then does $\operatorname{tr}A=0$ imply $\operatorname{tr}A^3=0$? If not, Under what condition?","['matrices', 'linear-algebra']"
2417992,Bolzano-Weierstrass is Equivalent to the Completeness Axiom,"How would I prove the following? Show that the assertion of the Bolzano-Weierstrass Theorem is equivalent to the Completeness Axiom for real numbers. I know that the Nested Set Theorem implies the Bolzano-Weierstrass Theorem, and that the Completeness Axiom implies the Nested Set Theorem, so I understand how Completeness Axiom $\Rightarrow$ Bolzano-Weierstrass. How do I show that the Bolzano-Weiertsrass Theorem implies the Completeness Axiom?","['real-analysis', 'real-numbers', 'sequences-and-series']"
2418007,The support of a sheaf is not necessarily closed,"This is paraphrased from an exercise in Hartshorne . Let $\mathcal{F}$ be a sheaf on $X$ , and for a point $P\in X$ let $\mathcal{F}_P$ denote the stalk of $\mathcal{F}$ at $P$ . We define the support of the sheaf $\mathcal{F}$ , denoted $\operatorname{Supp}\mathcal{F}$ , to be $\{P\in X \mid \mathcal{F}_P \neq 0\}$ . Show that $\operatorname{Supp}\mathcal{F}$ is not necessarily closed. The point of this exercise is to contrast with the fact that the support of a section defined over an open is closed . I imagine that there is a canonical example of such a sheaf $\mathcal{F}$ that I just don't know about.","['sheaf-theory', 'algebraic-geometry']"
