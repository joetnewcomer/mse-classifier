question_id,title,body,tags
3496778,A complex Banach space satisfying the parallelogram law is Hilbert,"Let $H$ be a Banach space with associated norm $\|-\|.$ Suppose that for any $x,y\in H,$ we have: $$\|x+y\|^2+\|x-y\|^2=2\left(\|x\|^2+\|y\|^2\right),$$ which we call the parallelogram law.
Then it is a well-known standard fact that $H$ becomes a Hilbert space. This is true both for real and complex coefficients. I managed to prove this fact for an Hilbert space over $\mathbb{R},$ defining the inner product $$(x,y) \mapsto \langle x,y\rangle=\frac{\|x+y\|^2-\|x-y\|^2}{4}= \frac{\|x+y\|^2-\|x\|^2-\|y\|^2}{2}.$$ Question How to prove this for the complex case? The inner product should be in this case $$(x,y)\mapsto\ \alpha(x,y)= \frac{1}{4} \left(\|x + y\|^2 - \|x-y\|^2 + i\|x + iy\|^2 -i\|x-iy\|^2 \right)=  \langle x,y \rangle + i \langle x,iy \rangle$$ but I am not able to replicate the proof of the real case. It would be awesome if there was a slick proof that used the real case to deduce the complex case, but I would still be happy with any kind of direct proof too. Since this is a standard result, if you can provide a reference where a detailed proof is given, that would be excellent too.","['banach-spaces', 'reference-request', 'hilbert-spaces', 'real-analysis', 'functional-analysis']"
3496791,Cross-product identity,"This page of vector identities lists the following (among many other identities): $$
(\mathbf{A}\cdot(\mathbf{B}\times\mathbf{C}))\,\mathbf{D}= (\mathbf{A}\cdot\mathbf{D} )\left(\mathbf{B}\times\mathbf{C}\right)+\left(\mathbf{B}\cdot\mathbf{D}\right)\left(\mathbf{C}\times\mathbf{A}\right)+\left(\mathbf{C}\cdot\mathbf{D}\right)\left(\mathbf{A}\times\mathbf{B}\right)
$$ which is presumably supposed to hold for vectors $\mathbf{A,B,C,D} \in \Bbb R^3$ . Unlike the other identities, this one is given without justification or citation.  With this in mind, my questions are: Is the identity true? (proven in answers below) Is the identity well-known? Is there a citation that can be used here? How can we prove it? Some answers have been given, but alternate approaches would be interesting to see. Thank you for your consideration. Quick thoughts on the problem: $\mathbf{A}\cdot(\mathbf{B}\times\mathbf{C})$ is a scalar-triple product and can be rewritten as $$ 
\det \pmatrix{\mathbf{A}& \mathbf{B} & \mathbf{C}}
$$ I have a hunch Cauchy-Binet can be applied here somehow This amounts to a statement about the map $$
D \mapsto [(A \times B)(C\cdot D) + (B \times C)(A\cdot D) + (C \times A)(B\cdot D)]
$$ A proof in Levi-Cevita notation might be quick.","['cross-product', 'reference-request', 'matrices', 'multivariable-calculus', 'linear-algebra']"
3496796,Proving the convergence of an infinite product,"I’m trying to prove the Taylor series of $e$ using binomial expansion: $$e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \dots$$ The steps I’ve tried so far are: $$\left(1+\frac{1}{n}\right)^n \\
= 1 + \binom{n}{1}\frac{1}{n} + \binom{n}{2}\frac{1}{n^2} + \binom{n}{3}\frac{1}{n^3} + \dots + \binom{n}{n}\frac{1}{n^n} \\
= 1+\dfrac {n}{1!}\left( \dfrac {1}{n}\right) +\dfrac {n\left( n-1\right) }{2!}\left( \dfrac {1}{n^{2}}\right) + \dfrac {n\left( n-1\right) \left( n-2\right) }{3!}\left( \dfrac {1}{n^{3}}\right) +\dots +\dfrac {n\left( n-1\right) \left( n-2\right) \dots 1}{n!}\left( \dfrac {1}{n^{n}}\right) \\
= 1+\dfrac {1}{1!}+\dfrac {1}{2!}\left( 1-\dfrac {1}{n}\right) +\dfrac {1}{3!}\left( 1-\dfrac {1}{n}\right) \left( 1-\dfrac {2}{n}\right) +\ldots +\dfrac {1}{n!}\left( 1-\dfrac {1}{n}\right) \left( 1-\dfrac {2}{n}\right) \dots
\left( 1-\dfrac {n-1}{n}\right) $$ As $n \to \infty$ , the last term becomes $$\lim_{n\to\infty} \left(\frac{1}{n!} \prod_{k=1}^n \left(1-\frac{k-1}{n}\right)\right)$$ and the infinite product on the right of the term should be proven to be convergent and equals to $1$ . So how do I prove that?","['euler-mascheroni-constant', 'taylor-expansion', 'infinite-product', 'limits', 'binomial-theorem']"
3496798,$GL_{n}(\mathbb{Z})$ and $GL_{n+1}(\mathbb{Z})$ are not isomorphic,"Can somebody help me on this problem? Thank you! Show that $GL_{n}(\mathbb{Z})$ and $GL_{n+1}(\mathbb{Z})$ are not isomorphic $\forall n \geq 2$ . My approach: I tried to create an isomorphism $f : GL_n(\mathbb{Z}) \to GL_{n+1}(\mathbb{Z}), f(X) = Y (X \in GL_n(\mathbb{Z}) \text{ and } Y \in GL_{n+1}(\mathbb{Z})$ but idk how to get a contradiction..","['group-theory', 'abstract-algebra']"
3496870,"Number of one-to-one functions $f: \{1, 2, 3, 4, 5\} \to \{0, 1, 2, 3, 4, 5\}$ such that $f(1) \neq 0, 1$ and $f(i) \neq i$ for $i = 2, 3, 4, 5$","This was a question from a test. There's a set $A$ with elements $\{1,2,3,4,5\}$ and another set $B$ with elements $\{0,1,2,3,4,5\}$ . Now $f$ is a function from $A$ to $B$ such that $f(1)$ is not equal to $0$ or $1$ and $f(i)$ is not equal to $i$ (for $i=2,3,4,5$ ). Then how many such one to one functions are possible? It looks like an application of the derangement formula, but it's getting way too complex when I apply it. Can anyone help me out in this?","['derangements', 'combinatorics']"
3496886,Estimation of sum of series with cosine,"Prove the following: $$
\sum_{{\large j = 1} \atop {\large j \neq k}}^{n}
{1 \over \left\vert\cos\left(k\pi/ n\right) -
\cos\left(j\pi/n\right)\right\vert} \leq cn^{2}
\qquad\mbox{where}\quad 1 \leq k \leq n\quad\mbox{is fixed.}
$$ I was able to get the upper bound to be $cn^{3}$ .
Any method can be used for getting the desired bound.
I also tried taking the upper bound in terms of an integral.
I will appreciate any suggestions.","['integration', 'trigonometric-series', 'calculus', 'sequences-and-series', 'trigonometry']"
3496917,How to find characteristic polynomial of $B$ in terms of $A$?,"$$B := \begin{pmatrix} A+nI && -E\\ -E^T&& nI\end{pmatrix}$$ where $E$ is the all-ones matrix. If the eigenvalues of $n \times n$ matrix $A$ are known, is it possible to find the eigenvalues of $B$ ? I find the characteristic polynomial of $B$ to be $$\begin{pmatrix} xI-(A+nI) && -E\\ -E^T&& (x-n)I\end{pmatrix}$$ I found that $$\det(xI-B)= \det(x-n)I\times \{(xI-(A+nI)-E(x-n)^{-1}E^T\}$$ but cant find the characteristic polynomial of $B$ in terms of $A$ . Can I get some help here?","['matrices', 'determinant', 'block-matrices', 'eigenvalues-eigenvectors']"
3496924,Is it possible for sum of three sequences $x_n + y_n+z_n$ to equal $0$?,"Define three sequences $x_n, y_n, z_n$ for $n=1, 2, \dots, $ by $x_1 = 2$ , $y_1 = 4$ , $z_1 = \frac{6}{7}$ and the recursion $$ x_{n+1} = \frac{2x_n}{x_n^2-1}, \quad
y_{n+1} = \frac{2y_n}{y_n^2-1}, \quad 
z_{n+1} = \frac{2z_n}{z_n^2-1} $$ Is it possible to have $x_n + y_n + z_n = 0$ for some $n$ ? Looking at each sequence separately, they look quite messy and are not monotone and also don't seem to converge to a limit.
I have tried an induction method, to start from $x_{n+1} + y_{n+1} + z_{n+1}$ , expand out using the recursion, and try to show that it is non-zero provided that $x_n+y_n+z_n$ is non-zero, but I am not getting anywhere because it is too messy. Maybe it can be simplified to showing that $x_n + y_n > z_n$ for all $n$ or something along those lines which will be sufficient to prove the result. Alternatively, I think it would be neater to find some invariant but I am really bad at finding invariants and I don't see any clue on how to start. The most obvious $x_n +y_n + z_n$ is not constant so that doesn't work. Another thing maybe is to look at the sequence of numerators and denominators i.e. let $a_n/b_n = x_n$ where $a_n$ and $b_n$ are coprime and work from there but it also gets messy very quickly. I have $\frac{a_{n+1}}{b_{n+1}} = \frac{2a_nb_n}{a_n^2 - b_n^2} = \frac{\sqrt{a_n^2b_n^2}}{\frac{a_n^2-b_n^2}{2}}$ which doesn't seem to resemble anything. How to approach this problem?",['sequences-and-series']
3496945,Function $f$ intersects with any line at most twice,"Function $f:\mathbb R\to\mathbb R$ intersects with any line at most $n$ times, what is $f$ ? Suppose that $n=1$ , it seems obvious that $f$ is a line. Suppose that $n=2$ , it seems that sgn $(f'')$ must be a constant. For example, $x^{2k}$ , $e^x$ , $\ln(x)$ . My questions are: How to classify all possible functions satisfying this condition? What about the general $n=n$ case? This is a function-equation problem. But only Math Overflow has a function-equation tag. So I added it here.","['functions', 'functional-analysis']"
3496972,Evaluating $\lim_{n\to\infty}\sum_{k=0}^n\frac{1}{2^k}\tan\frac{x}{2^k}$ [duplicate],"This question already has answers here : How to find the limit of a sequence? (2 answers) Closed 4 years ago . So I was solving a previous year question paper and stumbled upon the following question- $${\lim_{n\to {\infty}}}\biggl(\tan x+\frac{1}{2}\tan \frac{x}{2}+\frac{1}{2^2}\tan \frac{x}{2^2}+\frac{1}{2^3}\tan \frac{x}{2^3}+{\cdots}+\frac{1}{2^n}\tan \frac{x}{2^n}\biggl)$$ The only limit with $\tan$ that I have learnt is ${\lim_{x\to 0}}\frac{\tan x}{x}=1$ . I have tried the following with no success-
1)Representing $\tan x$ in terms of $\tan \frac x2$ 2)Simplifying into $\sin x$ and $\cos x$ I do not want a complete solution but would greatly appreciate a hint on how to simplify the series.","['limits', 'trigonometry', 'sequences-and-series']"
3497000,Limit $\lim_{n \to \infty }\frac{n!}{(n+1)!}\frac{\sin((n+1)x)}{\sin(nx)}$,"I am trying to evaluate this limit: $$ \lim_{n \to \infty }\frac{n!}{(n+1)!}\frac{\sin((n+1)x)}{\sin(nx)}$$ My thought was to split it up as follows: $$ = \lim_{n \to \infty }\frac{n!}{(n+1)!}\cdot\lim_{n \to \infty }\frac{\sin((n+1)x)}{\sin(nx)}
$$ $$
= 0\cdot \lim_{n \to \infty}\frac{\sin((n+1)x)}{\sin(nx)}=?$$ I am having difficulty continuing from here. Can anyone explain how this limit equals zero?","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
3497050,On the convergence of $\sum\frac{\log(n)}{n}\{x^n+x^{-n}\}$,"As stated in the title, I'm trying to determine the values of $x\in \mathbb{R}$ for which $$\sum_{n=1}^\infty \frac{\log(n)}{n}\{x^n+x^{-n}\}<+\infty$$ where $\{x\}$ is the fractional part (any convention on the fractional part of negatives numbers is accepted). Now, $\sum_{n=1}^{\infty} \frac {\log(n)}{n}=+\infty$ , so the trivial bound $\{x\}<1$ is no use. I thought about using the Dirichlet test, but still the problem of determining whether $\sum \{x^n+x^{-n}\}$ is bounded is out of my reach. Obviously, the problem is easily solvable if one restricts to $x \in \mathbb{Z}-\{0\}$ , but I don't see an obvious extension to the general case. Similarly, if the series converges for $x$ it does for $\frac{1}{x}$ too.
How should I approach the problem? I feel like there's an easy solution that I am missing.","['real-analysis', 'fractional-part', 'calculus', 'sequences-and-series', 'convergence-divergence']"
3497110,Evaluating $\lim_{x\to0}{\frac{x^2+2\ln(\cos x)}{x^4}}$ without l'Hopital's rule or Taylor series,"Can anyone please help me find this limit without l'Hopital's rule, I already used it to evaluate the limit, but I didn't know how to calculate it without l'Hopital's rule. $$\lim_{x\to0}{\frac{x^2+2\ln(\cos x)}{x^4}}$$ Any tips will be helpful. Sorry, but I don't want to use the Taylor series as well.","['limits', 'calculus', 'limits-without-lhopital']"
3497120,Prove $|A + B| ≥ |A| + |B| - 1$,"Let A and B be two non-empty sets of real numbers. Define A + B to be the set $$A+B = \{ a + b : a ∈ A, b ∈ B\}$$ For instance, if $A =\{1,3,4\}$ and $B = \{1,3\}$ , then $A + B =\{2,4,5,6,7\}$ . show that in any case, we have $$|A + B| ≥ |A| + |B| - 1$$","['elementary-set-theory', 'additive-combinatorics']"
3497135,"Way to find the remainder of two function, but the divisor is irreducible. ($f(x)$ divided by $g(x)$, but $g(x)$ is irreducible.)","Follow by my previous question , which is solved by the Little Bézout's theorem, they factored the function $g(x)$ and use its root. Now, I wonder, what if $g(x)$ is irreducible ? How do we solve it?","['functions', 'polynomials']"
3497177,solving a singular integral equation using Mellin transform,"Solve the following singular integral equation using suitable integral
  transform : $$\int_0^\infty u(t)\cos(xt)dt=e^{-x}$$ One easy method is if I use fourier cosine transform. But instead I chose to apply Mellin transform, to see what happens next. We know that $$\mathcal{M}\bigg(\int_0^\infty u(t)\cos(xt)dt;s\bigg)=U(1-s)\Gamma(s)\cos\bigg(\frac{s\pi}{2}\bigg)$$ where $U(s)=\mathcal{M}(u(x);s)$ and $$\mathcal{M}(e^{-x};s)=\Gamma(s)$$ Now, we have fom the given equation $$U(1-s)\Gamma(s)\cos\bigg(\frac{s\pi}{2}\bigg)=\Gamma(s)$$ $$\implies U(s)=\text{cosec}\bigg(\frac{s\pi}{2}\bigg)$$ Taking Mellin inverse $$u(x)=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty} x^{-s}\text{cosec}\bigg(\frac{s\pi}{2}\bigg)ds$$ Now I can't apply the method of residues as the cosecant term being present in the integrand. How to evaluate the above complex integral? Any help is appreciated.","['complex-analysis', 'integral-transforms']"
3497182,Method of multiple scales on Mathieu's equation,"I encountered a problem in Strogatz's ""Nonlinear Dynamics and Chaos."" Specifically 7.6.18. He takes the following equation : $$\ddot{x}+(a+\epsilon \cos t)x=0$$ where a is close to 1. There he asks us to use two timing method with slow time $\epsilon^2t$ and show the following: For $1-\frac{\epsilon^2}{12}+o(\epsilon^4)<a<1+\frac{5\epsilon^2}{12}+o(\epsilon^4)$ , the solution is unbounded. I set $a=1+\epsilon\delta$ where $\delta$ is a $O(1)$ constant so the equation turns to $\ddot{x}+x+\epsilon(\delta+\cos t)x=0$ . I take $x(t,\epsilon)=x_0(\tau,T)+\epsilon x_1(\tau,T)+\epsilon^2x_2(\tau,T)+O(\epsilon^3)$ , where $\tau=t$ and $T=\epsilon^2 t$ I get \begin{align}
\frac{dx}{dt}&=\frac{\partial x}{\partial \tau}+\epsilon^2\frac{\partial x}{\partial T}
\\
\frac{d^2x}{dt^2}
&=\partial_{\tau\tau}x+2\epsilon^2\partial_{T\tau }x+O(\epsilon^4)\\
&=\partial_{\tau\tau}(x_0+\epsilon x_1+\epsilon^2x_2+O(\epsilon^3))+2\epsilon^2\partial_{T\tau}(x_0+\epsilon x_1+..)+O(\epsilon^3)\\
&=\partial_{\tau\tau}x_0+\epsilon\partial_{\tau\tau}x_1+\epsilon^2(\partial_{\tau\tau}x_2+2\partial_{T\tau}x_0)+O(\epsilon^3)
\end{align} so $\ddot x+x+\epsilon(\delta+\cos t)x=0$ gets transformed into: $$
\partial_{\tau\tau}x_0+x_0+\epsilon(\partial_{\tau\tau}x_1+x_1+x_0(\delta+\cos t))+\epsilon^2(\partial_{\tau\tau}x_2+2\partial_{T\tau}x_0+x_1(\delta+\cos t))=O(\epsilon^3)
$$ So the perturbation equations till 2nd order are: \begin{align}
\partial_{\tau\tau}x_0+x_0&=0 \tag{1}
\\
\partial_{\tau\tau}x_1+x_1&=-x_0(\delta+\cos t)\tag{2}
\\
\partial_{\tau\tau}x_2+x_2&=-2\partial_{T\tau}x_0-x_1(\delta+\cos t)\tag3
\end{align} general solution to equation 1 is $$
x_0(\tau,T)=A(T)\cos\tau+B(T)\sin\tau\tag4
$$ using 4 in 2 we get: $$
\partial_{\tau\tau}x_1+x_1=-A\delta \cos\tau-B\delta \sin\tau-\frac{A}{2}(\cos2\tau+1)-\frac{B}{2}\sin2\tau
$$ for non secular solutions in equation 2, we set coefficients of resonant terms to 0 giving $A=0$ and $B=0$ which in turn gives us $x_0=0$ and equation 2 becomes $\partial_{\tau\tau}x_1+x_1=0$ and equation 3 becomes $\partial_{\tau\tau}x_2+x_2=-x_1(\delta+\cos t)$ This is what we started with for $x_0$ so $x_1=0$ and so on. So just doing the two timing with $T=\epsilon^2t$ does not seem to work. I tried using three timescales with $x(t,\epsilon)=x_0(\tau,T,\sigma)+\epsilon x_1(\tau,T,\sigma)+....$ where $\tau=t$ , $T=\epsilon t$ , $\sigma=\epsilon^2 t$ but the algebra got a bit unwieldy and suggested that this was not what the author wanted us to do. Can anybody prove the above result?
Any suggestions are appreciated.","['nonlinear-system', 'ordinary-differential-equations', 'perturbation-theory']"
3497188,how to find the number of subsets?,"Let A = {1, 2, 3, 4, 5, 6, 7, 8} $$S ⊆ P(A)$$ For every subset in S, each
  one contains 4 elements and for every element in A it belongs to 3
  subsets that are in S, how many subsets are in S? Any ideas how to approach this questions?","['abstract-algebra', 'combinatorics', 'discrete-mathematics']"
3497206,Prove that the sequence $b_n=b_{n-1}+b_{\left[\frac{n}{2}\right]}$ ($b_1=1$) contains infinitely many elements that are divisible by $7$,"Let the sequence $b_n$ such that $$b_1=1, \; b_n=b_{n-1}+b_{\left[\frac{n}{2}\right]} \;\; \text{for all}\;\; n \ge 2,$$ where $\left[\frac{n}{2}\right]$ is an integer part of a real number $\frac{n}{2}$ ( $\left[\frac{n}{2}\right]$ is a largest integer not exceeding $\frac{n}{2}$ ). 
  Prove that the sequence $b_n$ contains infinitely many elements that are divisible by $7$ . My work . The sequence $b_n$ is $1, 2, 3, 5, \fbox{7}, 
10, 13, 18, 23, 30, 37, 47, 57, \fbox{70}, 
83, 101, \fbox{119}, 
142, 165, 195, 225, 262, 299, 346, 393, 450, 507, 577, 647, 730, 813, 914, \fbox{1015}, 
\fbox{1134}, 
\fbox{1253}, 
1395, 1537, 1702, 1867, 2062, 2257, 2482, 2707, 2969, 3231, 3530, \fbox{3829}, 
4175, 4521, \fbox{4914}, 
5307, 5757, 6207, 6714, 7221, \fbox{7798}, 
8375, 9022, 9669, 10399, 11129, \fbox{11942}, 
12755, 13669, 14583, 15598, 16613, 17747, 18881, 20134, 21387, 22782, 24177, 25714, \fbox{27251}, 
28953, 30655, \fbox{32522}, 
34389, 36451, 38513, 40770, 43027, 45509, 47991, 50698, 53405, 56374, 59343, 62574, 65805, \fbox{69335}, 
72865, 76694, 80523, 84698, 88873, \fbox{93394}, 
97915.$ It is not easy to see what is the pattern here.","['contest-math', 'number-theory', 'divisibility', 'sequences-and-series']"
3497223,2nd order differential equation with non-constant coefficients,"Consider the second order differential equation $$y''-x^2y=0
$$ where $y$ itself is a function of $x$ . I do not know how to solve this equation. I tried a series expansion and failed, and because the coefficients are not constant, I can not use the characteristic equation to solve it either. Hence, here I am, looking for any hints on how to solve this equation for $y$ . I know there are tons of questions already out there concerning second order differential equations looking like this one, and I looked through just about every one of them, however all the solutions provided seem to be very situational for the given DE, and I have yet to find a general method that I can use to solve the above. I though about reducing the order of the equation. Thanks!",['ordinary-differential-equations']
3497341,Example of weak convergence,I'd like to understand better the concept of weak convergence. I know that a sequence of probability measures $\mu_n$ converges weakly to $\mu$ if $\int{f d\mu_n}$ converges to $\int{f d\mu}$ for each $f$ which is continuous and bounded. Could you please give me an example of a sequence of probability measures $\mu_n$ that converges weakly to $\mu$ and find a function $f$ such that $\int{f d\mu_n}$ does not converge to $\int{f d\mu}$ ?,"['weak-convergence', 'examples-counterexamples', 'probability-theory', 'probability']"
3497352,If $\int_0^1 f(x)g(x)dx <C \|g\|_1$ then there exists $M>0$ s.t. $f(x)<M$ a.e.,"Let $f$ be a function on the interval $[0,1]$ . If $\int_0^1 f(x)g(x)\,dx <C \|g\|_1$ for all nonnegative function $g\in L^1[0,1]$ then show that there is a constant $M>0$ such that $f(x)<M$ almost everywhere in $[0,1]$ . Attempt. Suppose not. Then there is a subset $A\subseteq [0,1]$ with $m(A)>0$ such that $f(x)=\infty$ for all $x\in A$ . So, $\int_0^1f(x)g(x)dx\geq\int_Af(x)g(x)\,dx=m(A)\cdot\infty=\infty$ , contradiction. I would be glad if someone could check my attempt or give a hint. Thanks!","['measure-theory', 'real-analysis']"
3497420,"Nature of the critical point $(0,0)$ of the function $f(x,y)=x^6-2x^2y-x^4y+2y^2$","Consider the function $$f(x,y)=x^6-2x^2y-x^4y+2y^2.$$ The point $(0,0)$ is a critical point. Observe, \begin{align*}
f_x & = 6x^5-4xy-4x^3y, f_x(0,0)=0\\ 
f_y & = 2x^2-x^4+4y. f_y(0,0)=0\\
f_{xx} & = 30x^4-4y-12x^2y, f_{xx}(0,0)=0\\
f_{xy} & = 4x-4x^3, f_{xy}(0,0)=0\\
f_{yy} & = 4, f_{yy}=4
\end{align*} So, in order to determine the nature of the above critical point, we need to check the Hessian at $(0,0)$ which is $0$ and hence the test is inconclusive. $$ H(x,y)= \det \begin{pmatrix} f_{xx} & f_{xy}\\ f_{yx} & f_{yy} \end{pmatrix}=\det \begin{pmatrix} 0 & 0 \\ 0 & 4 \end{pmatrix}=0$$ So, I tried to see the function on slices like $y=0$ and $y=x$ but nothing worked. So please suggest me how do I find the nature of the critical point in this case?","['multivariable-calculus', 'calculus']"
3497521,Determine the steady state temperature distribution for the given problems,"I have the following problem, where I'm kinda lost what to do: $${y}''=-T_{0}, \quad {y}'(0)=0, \quad y(1)=0$$ how can I solve this equation that has the constant $T_{0}$ . Any kind of help will be appreciated. Thank you.",['ordinary-differential-equations']
3497651,Volumes enclosed by $ \ln^2(x)+\ln^2(y)+\ln^2(z)+\cdots=1$,"Q: Find the volumes enclosed by $\ln^2(x)+\ln^2(y)+\ln^2(z)+\cdots=1$ I would like to find the volumes enclosed and then plot them to see their distribution as the dimension increases. I've seen the plots of the volumes of hyper-spheres as the dimension increases and want to do the same for the equation above. You might notice that the equation looks similar to the equation of a hyper-sphere but with logarithms. This is because this is the equation of a hyper-sphere in exponential space instead of regular $x,y,...$ space. I think the area enclosed by $\ln^2(x)+\ln^2(y)=1$ can be written using a Bessel function.","['spheres', 'graphing-functions', 'geometry', 'volume']"
3497679,Prove a limit does not exist,"I was given this function: $$
f(x)=
\begin{cases}
x+x^2, & x\in\Bbb Q\\
x, & x\notin \Bbb Q
\end{cases}
$$ I first proved that it is continuous at $x=0$ . Now I need to prove that that for every $x_0 \in \mathbb R\setminus\{0\}$ the limit $\lim \limits_{x \to x_0}f(x)$ does not exist. I know that I need to start by assuming that the limit does exist but I don't know how to reach a contradiction.","['limits', 'continuity', 'real-analysis']"
3497707,Reducing $\frac{\left(\left(\sqrt{\frac{a + b}{a - b}}\right)^2+1\right)\cdot c}{2c\sqrt{\frac{a + b}{a - b}}}$ to $\frac{1}{\sqrt{1 - b^2/a^2}} $,"I want to rearrange $$ \frac{\left(\left(\sqrt{\frac{a + b}{a - b}}\right)^2+1\right) \cdot c}{2c \cdot \sqrt{\frac{a + b}{a - b}}} $$ into $$ \frac{1}{\sqrt{1 - b^2/a^2}} $$ I would be grateful for some hints or full explanation.
I do know you can get rid of $c$ and the root above easily.",['algebra-precalculus']
3497722,Finitely generated ideals of the ring of smooth functions on a smooth manifold,"My question refers to an exercise in ""Topology of Real Algebraic Sets"" by Akbulut and King. This exercise is about one of the possible definition of blow-up in a differential framework, more precisely: Let $M$ be a real smooth manifold and $L\subset M$ a smooth proper submanifold of $M$ . Define the ideal: $\mathscr{I}_M^\infty(L)=\{f\in\mathcal{C}^\infty(M):f(x)=0\,\,\text{for every }x\in L\}$ . Then, $\mathscr{I}_M^\infty(L)$ is finitely generated. I am not so used to the properties of $\mathcal{C}^\infty(M)$ as a ring and I really do not understand where to use the hypothesis about the properness of the injection from $L$ to $M$ . I am grateful to anyone will give me some hint, solution or reference about this exercise.","['real-algebraic-geometry', 'commutative-algebra', 'differential-geometry']"
3497750,Abel´s formula solutions of the DE,"If $y_{1}$ and $y_{2}$ are linearly independent solutions of the second order DE ${y}''+a_{1}(x){y}'+a_{0}(x)y=0$ on some interval where $a_{1}(x)$ and $a_{0}(x)$ are continuous, show that the Wronskiant satisfies, for some constant $C$ . $W(y_{1},y_{2})(x)=C exp\left [ -\int a_{1}(x)dx \right ]$ Hint: Show that $\frac{\mathrm{dW} }{\mathrm{d} x}+a_{1}(x)W=0$ , and solve this  DE for $W$ Any kind of help will be appreciated. Thank you.",['ordinary-differential-equations']
3497752,How to find the directional derivatives of these two functions?,"So I've been given these two functions, as well as the points and vectors. I am supposed to find the directional derivatives of said functions at the point in the direction of the given vector. The formula I have received to do so is included at the bottom of the photo. Could anyone tell me why I have gotten these questions wrong, and how I can change my work or anything to fix these?","['multivariable-calculus', 'calculus', 'derivatives']"
3497811,Why is the number of elements in a union of 3 sets that?,"In my maths notebook, it says that if $A$ , $B$ , and $C$ are finite sets then: $$ 
n(A ∪ B ∪ C) = n(A) + n(B) + n(C) – n(A ∩ B) – n(B ∩ C) – n(A ∩ C) + n(A ∩ B ∩ C)
$$ However I don't know how this works properly. It does seem to work based on the previous identities on union of sets (i.e. $n(A ∪ B) = n(A) + n(B) – n(A ∩ B)$ ). However when I try to visualize it in Venn Diagram form, it just doesn't seem to work. I'm guessing this is because of my misconception between area and count of elements, but I still don't know why it does work. Any sort of help that helps me understand how to think of this equation will be greatly appreciated.",['discrete-mathematics']
3497813,"""Square-normal"" matrices are normal","This is from a practice exam for my quals. Let $A$ be an $n \times n$ complex matrix. Suppose $A$ satisfies the following property: $(AA^\dagger)^2 = (A^\dagger A)^2$ Prove that $A$ is normal, that is, that $AA^\dagger = A^\dagger A$ . My attempt: Recall that a matrix is normal if and only if it can be diagonalized by a unitary matrix. I will attempt to show this is the case. $A$ has a singular value decomposition $A = V \Sigma U^\dagger$ , where $V$ and $U$ are unitary and $\Sigma$ is diagonal with non-negative real diagonal entries. This factorization is unique up to permutation of diagonal elements of $\Sigma$ , since $A$ is square. $A =  V \Sigma U^\dagger, A^\dagger =  U \Sigma^\dagger V^\dagger$ and since $A A^\dagger$ and its conjugate transpose are normal, $AA^\dagger = V \Sigma U^\dagger U \Sigma^\dagger V^\dagger = V \Sigma \Sigma^\dagger V^\dagger$ and similarly for $A^\dagger A$ . I tried to use all of these assumptions to show that $U = V$ , but wasn't able to reach a contradiction by assuming otherwise.","['matrices', 'self-adjoint-operators', 'svd']"
3497828,"Metric defined on a measure space by $d(A,B)=\mu (A\Delta B)$?","Yesterday I suddenly came up with this idea: let $(X,\Sigma,\mu)$ be a measure space. For $A,B\in \Sigma$ , define $$
d(A,B)=\mu(A\Delta B), A\Delta B=(A\cup B)\backslash (A\cap B).
$$ By careful consideration on a Venn diagram, one can show that $d$ satisfy the triangular inequality, and is hence a pseudo-metric. If we identify all sets that have distance $0$ from each other, we get a metric space $\overline \Sigma$ . Is there any application of this metric in mathematics? Can this theory be developed any further?","['measure-theory', 'metric-spaces']"
3497857,Expected Value of a Coin Flipping Game with Variable Coins,"Main Question: My game works as follows: You start with 1 coin and flip it. When you move to the next round, you add another coin and repeat. You move on to the next round if the majority of the coins flipped in the current round come up heads. Otherwise, you lose the game. I've been trying to calculate the expected value of this game — the average round you get to before you lose. I've calculated that, for a given round R: $P(\text{win round R}) = \frac{1}{2^R}\sum^{R}_{k=floor(R/2)+1}{R \choose k}$ and with a simulation in Java, the expected value came out to be about $1.7229533856734633$ , but I've no clue of a closed form for this value. How would I find this expected value, analytically? Thank you! Simulation Code, if there's discrepancy between the analytic expected value and the simulated one: public static void main(String[] args) {
    int total = 0;
    double sim = Math.pow(2.0, 30.0);
    for (int i = 0; i < sim; i++) {
        total += game();
    }
    System.out.println((double) total / sim);
}

public static int flip(int coins) {
    int heads = 0;
    for (int i = 0; i < coins; i++) 
    {
        if (Math.random() >= 0.5) 
            heads++;
    }
    return heads;
}

public static int game() {
    int coins = 1;
    while (flip(coins) > coins/2) {
        coins++;
    }

    return coins;
}","['expected-value', 'probability']"
3497867,For what natural $n$ does there exist a cube composed of $n$ cubes and more,"This is a followup to For what natural $n$ does there exist a square composed of $n$ squares? Consider a natural $m$ . For what natural $n$ does there exist an $m$ -dimensional hypercube composed of $n$ many $m$ -dimensional hypercubes? In the case of $m=2$ , it is shown that $n$ can be any natural other than $2$ , $3$ , or $5$ . In the case of $m=3$ , I managed to show that $n$ can be any natural of the form $1+7a+19b+37c$ , which covers all $n\ge71$ . This was derived by noting that we can split a cube into $k^3$ pieces, and that we can also group $k^3$ equal sized pieces that are within a cube of each other into one big cube. By making a $(k+1)$ -cube and then merging $k^3$ of the cubes into one $k$ -cube, we can get $7$ , $19$ , and $37$ more cubes, as the cases of $k=1,2,3$ . By a bunch of testing, I couldn't seem to improve upon this any further. In general, we can show that we can have $n$ of the form $1+\alpha_1a+\alpha_2b+\alpha_3c+\alpha_4d+\dots$ , where $\alpha_k=(k+1)^m-k^m$ by the same process as above. Since I lack good means to visualize this, studying the $m=4$ case seems to be fairly challenging. If I've done this correctly, then it is possible to make a hypercube with $5^4$ unit hypercubes, merge $3^4$ of them into one, and then $2^4-1$ groups of $2^4$ hypercubes, the $-1$ due to one of the hypercubes intersecting the already merged $3^4$ hypercube. Doing this additionally lowers the above general bound down to all $n\ge1044$ . In general this process leads to letting us take $\alpha=(5^m-1)-(3^m-1)-(2^m-1)^2$ as a coefficient. I'm unsure if this captures all of the cases, which I would doubt, and if not, then how can I in general?","['recreational-mathematics', 'combinatorics', 'geometry', 'discrete-mathematics']"
3497896,"Evaluate $\int_0^{\infty } \log \left(\frac{a^2}{x^2}+1\right) \log \left(\frac{b^2}{x^2}+1\right) \log \left(\frac{c^2}{x^2}+1\right) \, dx$","We know that (G&R): $$\int_0^{\infty } \log \left(\frac{a^2}{x^2}+1\right) \log \left(\frac{b^2}{x^2}+1\right) \, dx=2 \pi  ((a+b) \log (a+b)-a \log (a)-b \log (b))  $$ Where $a, b>0$ . It can be proved by using Feynman's trick (i.e. differentiate w.r.t parameters) twice. The problem is: What are the closed-forms of following generalized integral : $$I=\int_0^{\infty } \log \left(\frac{a^2}{x^2}+1\right) \log \left(\frac{b^2}{x^2}+1\right) \log \left(\frac{c^2}{x^2}+1\right) \, dx $$ I've weakened the original problem and would like you to give some suggestions on it. Also this is related. Thank you.","['integration', 'definite-integrals']"
3497909,Matrix Kintchine inequality proof Exercise 5.4.13,"I have been trying to solve every question from Vershynin's book right now for self study. The following question I am having trouble proving is Exercise 5.4.13 part (b) from Vershynin's book, High Dimensional Probability. $\textbf{Exercise 5.4.13}$ (Matrix Kintchine's inequality) Let $\epsilon_1, \dots, \epsilon_N$ be independent symmetric Bernoulli random variables and let $A_1, \dots, A_N$ be symmetric $n\times n$ matrices(deterministic) (b) Prove that for every $p\in[1,\infty)$ we have $$\left(\mathbb{E} \left|\left| \sum_{i=1}^N \epsilon_i A_i\right|\right|^p\right)^{1/p} \leq C\sqrt{p+\ln(n)} \left|\left| \sum_{i=1}^N A_i^2 \right|\right|^{1/2}$$ Where C is an absolute constant. I have been trying to use the result of Exercise 5.4.12(Matrix Hoeffding's inequality) to solve Exercise 5.4.13 part (b). (Matrix Hoeffding's inequality) If $\epsilon_1,\cdots,\epsilon_N$ are independent symmetric Bernoulli random variables and $A_1,\cdots,A_N$ are symmetric $n\times n$ matrices then for any $t\geq 0$ we have $$P\left\{\left\lVert \sum_{i=1}^N \epsilon_i A_i \right\rVert \geq t\right\}\leq 2n\exp\left(-\frac{t^2}{2\sigma^2}\right)$$ where $\sigma^2 = \left\lVert\sum_{i=1}^N A_i^2\right\rVert$ . I have been trying to use Hoeffding's inequality above with the following simple relation If $X$ is a nonnegative random variable and $p\in [1,\infty)$ then $$\mathbb EX^p = \int_0^\infty pt^{p-1} P(X\geq t)dt$$ But I still haven't been able to prove the exercise. I was wondering if anyone had a hint or could sketch out a quick proof.","['statistics', 'probability-theory', 'inequality']"
3497919,Prove that $\sum_{k=0}^{\lfloor (n-1)/2 \rfloor} (-1)^k {n+1 \choose k} {2n-2k-1 \choose n} =\frac{ n(n+1)}2 $,"$$\sum_{k=0}^{\Big\lfloor \frac{(n-1)}{2} \Big\rfloor} (-1)^k {n+1 \choose k} {2n-2k-1 \choose n} = \frac{n(n+1)}{2} $$ So I feel like $(-1)^k$ is almost designed for the inclusion-exclusion principle. And the left-hand side looks like some sort of pairing, so I am interested in some combinatorics proof like below-linked question. But using a generating function is always helpful. [EDIT] now I am probably equally, if not more interested in a generating function solution now that I see below answer that completely makes sense to me, but with some issues in signs.. Evaluation of a sum of $(-1)^{k} {n \choose k} {2n-2k \choose n+1}$","['combinatorial-proofs', 'inclusion-exclusion', 'binomial-coefficients', 'combinatorics', 'generating-functions']"
3497942,Elementary algebra book having proofs,"Just wanted to know if there is any Elementary Algebra book, which covers proofs of all the basic theorems before getting into exercises. Like the Theorem, If $a > b > 0$ , then prove $\sqrt{a} > \sqrt{b}$ . and other theorems like that (for equations etc). Or a rigorous book with more emphasis on proofs than exercises.","['algebra-precalculus', 'discrete-mathematics', 'reference-request']"
3497959,Clean and clever proofs to show every homomorphism of two groups with coprime orders is trivial?,I'm practicing for a qualifying exam in algebra (the freebie attempt we get the week before my first semester of grad school). I don't have a lengthy or deep background with math so I'm especially interested in learning to write cleaner and more clever proofs or proofs that use different methods. Does anyone have feed back on my proof of the titular question or perhaps an alternative proof? use the fact that $G$ and $H$ don’t have any subgroups of the same order besides the subgroup which is just the identity then use that the kernel of a homomorphism is a normal subgroup and the kernel defines the homormorphism But $G/\ker(f)$ is isomorphic to H. Since the order of $G/\ker(f)$ is a number composed of the product of primes that divide the order of G because the order of $G$ is coprime to the order of $H$ we know $|G/\ker(f)| = 1$ and that therefore $f$ maps all of $G$ into the identity of $H$ hence $f$ is a trivial homomorphism,"['proof-writing', 'finite-groups', 'alternative-proof', 'abstract-algebra', 'group-theory']"
3497975,Is the series $X =\frac{1}{3}+\frac{1}{5}+\frac{1}{7}+\frac{1}{11}+..$ convergent or divergent.,The series is the reciprocal of twin primes. Let $Y=(y_n)$ be the series of reciprocal of natural numbers.  Now if I use the comparison test we can see that each term of $0 <(x_n) < (y_n)$ .So the divergence of series of reciprocal of  natural number should also imply the same for $Y$ . Now the problems of my proof: $1)$ I probably have used the comparison test in a wrong sense $2)$ I don't know how many twin primes are there Can someone help me to understand the intuition behind the statement. If it converges or diverges why? I don't need the entire proof but some hint would do..,"['limits', 'twin-primes', 'sequences-and-series', 'real-analysis']"
3497987,Is the series $X=1+\frac{1}{3}+\frac{1}{5}+\frac{1}{7}+..$ divergent or convergent.,I am trying to use the comparision test over here $Y= 1+ \frac{1}{2}+\frac{1}{3}+..$ . Show that $0<(x_n)<=(y_n)$ . Since $Y$ diverges we see that the $X$ also diverges. $(1)$ am I using the comparision test correctly? $(2)$ is there any other crude way to prove this instead of using theorems,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'geometric-progressions']"
3498199,Can I apply elementary row operation then find eigenvalues of a matrix?,"Suppose if a matrix is given as $$ \begin{bmatrix} 
4 & 6\\
2 & 9 
\end{bmatrix}$$ We have to find its eigenvalues and eigenvectors. Can we first apply elementary row operation . Then find eigenvalues. Is their any relation on the matrix if it is diagonalized or not.","['matrices', 'gaussian-elimination', 'linear-algebra', 'eigenvalues-eigenvectors']"
3498228,Pointwise convergence of probability densities imply weak convergence of probability measures,"Assume $P_n, n\in\mathbb{N}$ and $P$ are absolutely continuous probability measures with respect to a sigma finite measure $\mu$ on $(\mathbb{R},\mathcal{B})$ . Let $f_n, n\in \mathbb{N}$ and $f$ be the densities of above measures respectively. I want to prove that if $f_n$ converges pointwise to $f$ , then $P_n$ converges weakly to $P$ . Here is what I have tried so far. I've read that even strong (pointwise) convergence follows, so I tried to prove that and the weak convergence follows: Let $A \in \mathcal{B}$ and since $f_n$ are the the densities of $P_n$ the following equalities hold $$P_n(A) = \int\limits_{A} f_n d\mu = \int 1_{A} f_n d\mu.$$ Now since $f_n$ converges pointwise to $f$ , also $1_{A} f_n$ converges pointwise to $1_{A} f$ . Here is where I'm stuck because I want to use the theorem about dominated convergence. Hence, I have to find an integrable bound for $1_{A} f_n$ . If the theorem about dominated convergence applies I get $$\lim\limits_{n\rightarrow\infty}P_n(A)=\int \lim\limits_{n\rightarrow\infty} 1_{A} f_n d\mu = \int 1_{A} f d\mu=\int\limits_{A} f d\mu = P(A)$$ which completes the proof. Can someone help me to find a bound for $f_n$ ? Is this approach correct or am I missing something?","['measure-theory', 'weak-convergence', 'density-function']"
3498232,"What is $P(B_1 > 0, B_2 > 0)$ , where $B_t$ is a Brownian Motion at time $t$?","The following question is found from this MSE post .
For completeness, I restate the problem below. Question:  What is $P(B_1 > 0, B_2 > 0)$ , where $B_t$ is a Brownian Motion at time $t$ ? From that post, the OP calculates the probability as follows: $P(B_1 > 0, B_2 > 0) = P(B_1 > 0, B_2 - B_1 > -B_1) = P(Z_1 > 0, Z_2 > -Z_1) = \frac{3}{8}$ by applying a symmetry argument to the $(Z_1, Z_2) \sim N(0, I_2)$ distribution. I can understand all equalities except the last one which leads to the answer $\frac{3}{8}.$ In other words, I do not understand how symmetry argument to bivariate normal distribution is at play here.","['probability', 'normal-distribution', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3498294,Max packing of cars in square parking lot,"An $n\times n$ square grid represents a parking lot, each of the $n^2$ squares may be occupied by at most one car. However, each car (not on the boundary) must have a clear path to the boundary of the grid, moving only to adjacent squares sharing a side. What is the maximum number $M(n)$ of cars that can fit in the parking lot? This was posted recently by @Benenom but for some reason was deleted 3 mins after appearing. Clearly, $M(n)=n^2$ for $n<3$ . For $n=3$ we need at least one vacant square (any non-corner square will do). For $n=4$ we need at least 3 vacant squares (eg a line of three starting from a non-corner boundary square). At first I thought that the idea used for $n=4$ might work for all $n$ . Eg for $n=8$ make vacant the first 7 squares of the third and sixth rows. $$\begin{matrix}C&C&C&C&C&C&C&C\\
C&C&C&C&C&C&C&C\\
C&.&.&.&.&.&.&.\\
C&C&C&C&C&C&C&C\\
C&C&C&C&C&C&C&C\\
C&.&.&.&.&.&.&.\\
C&C&C&C&C&C&C&C\\
C&C&C&C&C&C&C&C\end{matrix}$$ That gives: $M(1)=1\\
M(2)=4\\
M(3)=9-1=8\\
M(4)=16-3=13\\
M(5)=25-4=21\\
M(6)=36-2\cdot5=26\\
M(7)=49-3\cdot6=37\\
M(8)=64-2\cdot7=50$ In general we would get $M(3n+2)=(3n+2)^2-n(3n+1)=6n^2+11n+4\\
M(3n+1)=(3n+1)^2-3n^2=6n^2+6n+1\\
M(3n)=(3n)^2-n(3n-1)=6n^2+n.$ [the last one for $n>1$ ] However, as @RobPratt points out in his answer below, we can do better than that for $M(6)$ , getting 36-8=28. Presumably the same is true for all $M(3n)$ . For $3n=6$ we can write the last four rows as: $$\begin{matrix}C&.&.&.&.&.\\
C&C&.&C&.&C\\
C&C&.&C&C&C\\
C&C&C&C&C&C\end{matrix}$$ In a similar way we could write the last four rows for $3n=9$ as: $$\begin{matrix}C&.&.&.&.&.&.&.&.\\
C&C&.&C&C&.&C&.&C\\
C&C&.&C&C&.&C&C&C\\
C&C&C&C&C&C&C&C&C\end{matrix}$$ and in general we would get $M(3n)=6n^2+2n$ . The other two cases are unchanged: $M(3n+1)=6n^2+6n+1$ and $M(3n+2)=6n^2+11n+4$ . In any case I have not yet got a proof of maximality even for $M(3n+2)$ .","['discrete-optimization', 'combinatorics']"
3498297,Find the least positive integer $M$ such that $M^{77} \equiv 14 \pmod{31}$,"Find the least positive integer $M$ such that $M^{77} \equiv 14 \pmod{31}.$ The way I have approached this question is by using Fermat's little theorem: $$a^{p-1} \equiv 1 \pmod p. $$ By trial and error -- I just started at $1$ , then $2$ , etc. -- eventually $M=18$ was the first integer (least positive) that gave the result congruent to $14$ mod $31$ : $$18^{30} \equiv 1 \pmod{31}.$$ $$ 18^{77} = 18^{2\cdot 30+17} \equiv 18^{17} \equiv 14 \pmod{31}.$$ This way of solving it is obviously quite long and tedious (especially without a calculator).  I am wondering if anybody can explain to me a more appropriate approach.","['number-theory', 'finite-fields', 'modular-arithmetic', 'discrete-mathematics']"
3498307,"Gaussian curvature of a surface which is equal to the $(x,y)$-plane outside a ball of radius $10$","The problem I'm working on is as follows Let $M \subseteq \mathbb{R}^3$ be a non-compact orientable surface without boundary which coincides with the $(x, y)$ -plane outside of the ball of radius $10$ centered at the origin. Prove that if the Gaussian curvature $K$ of $M$ is everywhere non-negative, then $K$ is everywhere $0$ . A few days ago, I posted here an idea I had for this problem, hoping someone could help me flesh out the missing step(s). The idea I had was to look at the part of the surface contained in the ball of radius $10$ , call it $R$ , a surface w/ boundary, and apply Gauss-Bonnet to it to establish that $\iint_R K \mathrm{d} M \leq 0$ . This would imply that $K$ is identically $0$ . I later realized that this method won't work, since we can't guarantee that $R$ is compact, a necessary assumption to use GB. For example, if $M$ was just the $(x, y)$ -plane minus the origin, it would fit the hypotheses of the problem, but $R$ would be a punctured disc, which is not compact. My second idea had been to look at the point in the surface where the $z$ -coordinate was maximized and see if I could come to some conclusion about the curvature there, but this won't work for similar reasons, since the $z$ -coordinate could be unbounded. I have in mind something where there's a ""singularity"" about the $z$ -axis. So I'm out of ideas for how to solve this problem. I don't really know what's left to try. I would love hints, but I'm studying for an exam in two days, so I'd also appreciate worked-out solutions to this problem. Thanks!","['curvature', 'riemannian-geometry', 'differential-geometry']"
3498311,Weak convergence in the space of tempered distributions and weighted Sobolev spaces,"It is well known that, at least as sets, $$\mathcal{S}'(\mathbb{R}^n)=\bigcup_{m\in\mathbb{N}}(1+|x|^2)^mH^{-m}(\mathbb{R}^n)$$ where $\mathcal{S}'(\mathbb{R}^n)$ is the space of tempered distribution (i.e. the dual of Schwartz space), $H^{-m}(\mathbb{R}^n)$ denotes the $L^2$ -Sobolev space of order $-m$ with norm $\Vert u\Vert^2=\int_{\mathbb{R}^n}(1+|\xi|^2)^{-m}|\hat{u}(\xi)|^2\,d\xi$ and $(1+|x|^2)^mH^{-m}(\mathbb{R}^n)$ is the weighted Sobolev space $$(1+|x|^2)^mH^{-m}(\mathbb{R}^n)=\{u\in\mathcal{S}'(\mathbb{R}^n): (1+|x|^2)^{-m}u\in H^{-m}(\mathbb{R}^n)\}$$ I also read somewhere that the strong topology on $\mathcal{S}'$ is the strict inductive limit topology induced by the RHS, however, I have not been able to find a proof of this fact. I would be content with a weaker sequential statement: I would like to prove that $\{u_n\}\subset\mathcal{S}'(\mathbb{R}^n)$ converges weakly to $u$ in $\mathcal{S}'(\mathbb{R}^n)$ (i.e. $u_n(\phi)\rightarrow u(\phi)$ in $\mathbb{C}$ as $n\rightarrow\infty$ for all $\phi\in\mathcal{S}(\mathbb{R}^n)$ ) if and only if there is some $m\in\mathbb{N}$ for which $u_m\rightarrow u$ in the topology of $(1+|x|^2)^mH^{-m}(\mathbb{R}^n)$ . Does anyone know a proof of this fact?","['topological-vector-spaces', 'distribution-theory', 'sobolev-spaces', 'functional-analysis', 'schwartz-space']"
3498424,Does Euler's Theorem for homogeneous functions require continuous differentiability?,"Euler's Theorem for homogeneous functions states that $f: \mathbb{R}^n \to \mathbb{R}$ is homogeneous of degree $k$ ( $f(cx) = c^k f(x)$ for all $c > 0$ , $x \in \mathbb{R}^n$ ), if and only if the partial derivatives of $f$ satisfy $$
k f(x) = \sum_{i=1}^n x_i \frac{\partial f(x)}{\partial x_i}
$$ Clearly this theorem requires differentiability of $f$ . However, it seems to often be stated with the requirement of continuous differentiability, e.g., here . On the other hand, it is also sometimes stated without this requirement, and I don't see how any of the proofs actually use continuity of partial derivatives (e.g., the proof here ). I would greatly appreciate if anyone could clear this up!","['multivariable-calculus', 'calculus']"
3498508,Find all solutions in $\mathbb{N}$ to $a^a=a^b+b^a$,"Problem: Find all positive integer solutions to the equation $a^a=a^b+b^a$ . Attempt at a solution: I first acknowledged that $a > b$ , since if $a \le b$ , then $a^a=a^b+b^a\ge a^a+a^a=2a^a$ , which isn't true for any positive integer $a$ . Then I used $a-b=d$ , which, of course means $a=b+d$ , and I substituted that into the original equation. I got $(d+q)^d[(d+q)^d-1]=q^dq^q$ . I'm stuck here. Any help would be appreciated.","['number-theory', 'divisibility', 'elementary-number-theory', 'exponentiation']"
3498531,Norms on polynomials,"Let $p_N (x) = d^N /dx^N ((x^2 -1)^N)$ for $N=0,1,2....$ Consider these polynomials as elements of the space $C[−1, 1]$ with
the norm $||.||_2$ Show that the inner product of $p_N$ and $p_M = 0$ if $N$ $\ne$$ M$ Find the norm $||p_N||_2$ I have been to to use IBP for the first part, but I'm unsure where to begin. I know how to find the innerproduct normally, but not of something like this. I also am confused by the second part, despite knowing that to find that norm, you do Pythagoras on the polynomial.","['real-analysis', 'calculus', 'linear-algebra', 'functional-analysis', 'polynomials']"
3498565,Prime solutions to $x^2-2y^2=\pm 1$,"It is easy to see that if $(x_0,y_0)$ is a solution to $x^2-2y^2=\pm 1$ , then so is $(x_i,y_i)=(3x_{i-1}+4y_{i-1},2x_{i-1}+3y_{i-1})$ (and with the same sign). A comment to this answer points out that if $x_0=41$ and $y_0=29$ , then it appears that $x_i$ and $y_i$ are simultaneously prime for infinitely many $i$ . I don't see how to approach proving this analytically. Of course, the statement is false if either $x_0$ or $y_0$ is $3$ - but is it true for every other pair of distinct odd primes $x_0$ and $y_0$ ?","['number-theory', 'pell-type-equations']"
3498617,Why the set of all subsets of a finite set $X$ is denoted by $2^X$?,Why the set of all subsets of a finite set $X$ is denoted by $2^X$ ? specifically why the number 2? what does it represents? Could anyone explain this for me please?,"['elementary-set-theory', 'functions', 'real-analysis']"
3498621,A map between Banach spaces is continuous - counterexample,"Today the following question came up ( A map between Banach spaces is continuous ): I'm trying to prove this statement: Let $(X_0, \| \cdot \|_{X_0})$ and $(X_1, \|\cdot \|_{X_1})$ be Banach spaces and $(Y_0, \| \cdot \|_{Y_0})$ and $(Y_1, \|\cdot \|_{Y_1})$ normed spaces so that $X_0$ is a vector subspace of $Y_0$ and $X_1$ is a vector subspace of $Y_1$ . Further assume that $i_0: (X_0, \|\cdot\|_{X_0}) \rightarrow (Y_0, \|\cdot\|_{Y_0}),\; x \mapsto x$ and $i_1: (X_1, \|\cdot\|_{X_1}) \rightarrow (Y_1, \|\cdot\|_{Y_1}),\; x \mapsto x$ are continuous. If $T \in L(Y_0, Y_1)$ so that $T(X_0) \subseteq X_1$ , define $S: (X_0, \|\cdot\|_{X_0}) \rightarrow (X_1, \|\cdot\|_{X_1}), \;x \mapsto Tx$ and show that $S$ is continous. Any ideas how to prove it? I was thinking about it for a moment and thought that I came up with a counterexample. Before I could post it, there was an answer showing that it is indeed true. The proof is rather short and looks very convincing. My question is therefore: What is wrong with my counterexample? For a counterexample pick a Banach space $(Z, \Vert \cdot \Vert_Z)$ , a discontinuous linear map $C: (Z, \Vert \cdot \Vert_Z)\rightarrow (Z, \Vert \cdot \Vert_Z)$ . We define $X:=Z\oplus Z$ with the norm $\Vert (z_1, z_2)\Vert := \Vert z_1\Vert_Z +\Vert z_2\Vert_Z$ . Then define two discontinuous linear maps $$A: (X, \Vert \cdot \Vert) \rightarrow (X, \Vert \cdot \Vert), A(z_1, z_2):= C(z_1)+z_2$$ and $$ B: (X, \Vert \cdot \Vert) \rightarrow (X, \Vert \cdot \Vert), B(z_1, z_2):=z_1+ C(z_2). $$ Then we define two new norms on $X$ . Namely, we define for $x\in X$ $$ \Vert x\Vert_A := \Vert x\Vert + \Vert Ax\Vert$$ and $$ \Vert x\Vert_B := \Vert x \Vert + \Vert Bx\Vert.$$ Now pick $$(X_0, \Vert \cdot \Vert_{X_0})= (X, \Vert \cdot \Vert_A ) = (Y_0, \Vert \cdot \Vert_{Y_0})$$ and $$ (X_1, \Vert \cdot \Vert_{X_1}) = (X, \Vert \cdot \Vert_B)$$ and $$ (X, \Vert \cdot\Vert) = (Y_1, \Vert \cdot \Vert).$$ We have $$ \Vert i_0 x\Vert_{Y_0} = \Vert x\Vert_{Y_0} =\Vert x\Vert_A = \Vert x\Vert_{X_0}$$ and $$ \Vert i_1 x\Vert_{Y_1} = \Vert x\Vert \leq \Vert x\Vert + \Vert Bx\Vert = \Vert x\Vert_{X_1}$$ Thus, $i_0$ and $i_1$ are continuous. Furthermore, we set $$ T: (Y_0, \Vert \cdot\Vert_{Y_0}) \rightarrow (Y_1, \Vert \cdot \Vert_{Y_1}), x\mapsto Ax.$$ We compute $$\Vert T x\Vert_{Y_1} = \Vert Ax \Vert \leq \Vert x\Vert_{Y_0}$$ Hence, also $T\in L(Y_0, Y_1)$ . You claim now that the map $S: (X_0, \Vert \cdot \Vert_{X_0}) \rightarrow (X_1, \Vert \cdot \Vert_{X_1}), x\mapsto Ax$ is continuous as well. This is not true. Note that $i: Z \rightarrow X, z \mapsto (0,z)$ is continuous. If $S$ was continuous, then also the map $F = S\circ i: (Z, \Vert \cdot \Vert_Z) \rightarrow (X_1, \Vert \cdot\Vert), z \mapsto A(0, z)$ was continuous. This would imply $$ \Vert z \Vert_Z + \Vert C(z) \Vert_Z 
= \Vert (0,z) \Vert +  \Vert B(0,z) \Vert
= \Vert (0,z) \Vert_{X_1} = \Vert A(0,z)\Vert
= \Vert F(z) \Vert \leq \Vert F \Vert_{op} \Vert z \Vert_Z $$ which tells us that $C$ is continuous, which is a contradiction.","['operator-theory', 'functional-analysis']"
3498630,Average number of strings with edit distance at most 4,"Consider a binary string of length $n \geq 4$ .   An edit operation is a single bit insert, delete or substitution.  The edit distance between two strings is the minimum number of edit operations needed to transform one string into the other one.  Given a string $S$ , my question relates to the number of distinct strings of length $n$ which are edit distance at most $4$ from $S$ . Let us write $g_k(S)$ for the number of distinct strings of length $n$ which are edit distance at most $k$ from $S$ . Let $X_n$ be a random variable representing a random binary string of length $n$ , with the bits chosen uniformly and independently. We can compute $\mathbb{E}(g_k(X_n))$ for $k = 0, 1, 2, 3$ explicitly. $\mathbb{E}(g_0(X_n)) = 1$ $\mathbb{E}(g_1(X_n)) = n+1$ $\mathbb{E}(g_2(X_n)) = \frac{13}{2} - \frac{5n}{2}  + n^2 - 6\cdot2^{-n}$ $\mathbb{E}(g_3(X_n)) =  -\frac{83}{2 }+ \frac{331n}{12} -6 n^2 + \frac{2n^3}{3} + 2^{-n}(40 + 6n -4n^2)$ ( Ref 1 and Ref 2 ) This leads directly to my question: Let $X_n$ be a random variable representing a random binary string of
  length $n$ , with the bits chosen uniformly and independently.  What is: $$\mathbb{E}(g_4(X_n))\;?$$ For small $n$ we can compute the value exactly: $\mathbb{E}(g_4(X_4)) = 16$ . $\mathbb{E}(g_4(X_5)) = 31 \frac{11}{16}$ . $\mathbb{E}(g_4(X_6)) = 61 \frac{21}{32}$ . $\mathbb{E}(g_4(X_7)) = 116 \frac{7}{8}$ . $\mathbb{E}(g_4(X_8)) = 214 \frac{43}{128}$ . $\mathbb{E}(g_4(X_9)) = 378 \frac{49}{246}$ . $\mathbb{E}(g_4(X_{10})) = 640 \frac{301}{512}$ . $\mathbb{E}(g_4(X_{11})) = 1042 \frac{1}{16}$ . $\mathbb{E}(g_4(X_{12})) = 1631 \frac{1345}{2048}$ . $\mathbb{E}(g_4(X_{13})) = 2466 \frac{3909}{4096}$ . $\mathbb{E}(g_4(X_{14})) = 3614 \frac{563}{8192}$ It seems tempting to guess the general form of $\mathbb{E}(g_4(X_n))$ from the examples of $\mathbb{E}(g_2(X_n))$ and $\mathbb{E}(g_3(X_n))$ but I have not succeeded in getting that to work.","['gap', 'combinatorics', 'probability']"
3498660,Is this sequence based on the Fibonacci numbers a prime generator?,"Consider the Fibonacci sequence $\text{fibo}(n)$ and the fractions $$ A(n) = \frac{\text{fibo}\left(n^2\right)}{\text{fibo}(n)^2} = \frac{b(n)}{c(n)},$$ where the fractions $\frac{b(n)}{c(n)}$ are in reduced form. Now, it appears that for all $n > 4$ : If $c(n) = 1$ , then $b(n)$ is of the form $2^q p$ where $p$ is a prime. If $c(n) > 1$ , then either $c(n) $ or $b(n)$ is of the form $2^q p$ where $p$ is a prime. So this formula generates a prime for every $n$ . A weaker conjecture is that this holds for the cases when $n$ is a prime. Are these conjectures true? How can we prove them?","['number-theory', 'fibonacci-numbers', 'prime-numbers']"
3498665,Distributing balls into bins randomly,"Problem: If $n$ balls are distributed at random into $r$ boxes (where $r \geq 3$ ), what is the probability that box $1$ at exactly $j$ balls for $0 \leq j  \leq n$ and box $2$ contains exactly $k$ balls for $0 \leq k  \leq n$ ? Answer: Let $p$ be the probability that we seek. First we consider a special case. If $j + k > n$ then $p = 0$ . Let $p_1$ be the probability that
a ball is placed in box $1$ .  Let $p_2$ be the probability that a ball is placed in box $2$ . Let $p_3$ be the probability that a ball is placed in box other than box $1$ and box $2$ . \begin{align*}
p_1 &= \frac{1}{r} \\
p_2 &= \frac{1}{r} \\
p_3 &= \frac{r-2}{r}
\end{align*} Now we have a multinomial distribution. \begin{align*}
P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \left( \frac{1}{r}\right) ^j \left( \frac{1}{r}\right) ^k \left( \frac{r-2}{r} \right)^{n - j - k} \\
P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \left( \frac{1}{r}\right) ^{j+k}  \left( \frac{r-2}{r} \right)^{n - j - k} \\
P &= \left( \frac{n!}{j!k!(n-j-k)!} \right) \frac{ (r-2)^{n-j-k} } {r^n}
\end{align*} So for example, if we have: $j = 2$ , $k = 2$ , $n = 8$ and $r = 8$ then \begin{align*}
P &= \left( \frac{8!}{2!2!(8-2-2)!} \right) \frac{ (8-2)^{8-2-2} } {8^8} = \left( \frac{8!}{4(8-2-2)!} \right) \frac{ (6)^{4} } {8^8} \\
P &= \left( \frac{8!}{4(4)!} \right) \frac{ (6)^{4} } {8^8} =  \left( \frac{8(7)(6)(5)}{4} \right) \frac{ 2^4(3^4) } {8^8} \\
P &= \frac{ 8(7)(6)(5)( 2^4)(3^4) } { 4(8^8) } = \frac{ 2(7)(6)(5)( 2^4)(3^4) } { 4(8^7) } \\
P &= \frac{ 7(6)(5)( 2^4)(3^4) } { 2(8^7) } =  \frac{ 7(6)(5)( 2^3 )(3^4) } { (8^7) }\\
P &= \frac{ 7(6)(5)(3^4) } { 8^6 } = \frac{ 17010 } { 262144 } \\
P &= \frac{8505 } { 131072 } \\
P &\doteq 0.064888
\end{align*} I think I have it right now. Do I?","['multinomial-distribution', 'probability']"
3498785,"What do ""function of"" and ""differentiate with respect to"" mean?","In maths and sciences, I see the phrases ""function of"" and ""with respect to"" used quite a lot. For example, one might say that $f$ is a function of $x$ , and then differentiate $f$ ""with respect to $x$ "". I am familiar with the definition of a function and of the derivative, but it's really not clear to me what a function of something is, or why we need to say ""with respect to"". I find all this a bit confusing, and it makes it hard for me to follow arguments sometimes. In my research, I've found this , but the answers here aren't quite what I'm looking for. The answers seemed to discuss either what a function is, but I know what a function is. I am also unsatisfied with the suggestion that $f$ is a function of $x$ if we just label its argument as $x$ , since labels are arbitrary. I could write $f(x)$ for some value in the domain of $f$ , but couldn't I equally well write $f(t)$ or $f(w)$ instead? To illustrate my confusion with a concrete example: consider the cumulative amount of wax burnt, $w$ as a candle burns. In a simple picture, we could say that $w$ depends on the amount of time for which the candle has been burning, and so we might say something like "" $w$ is a function of time"". In this simple picture, $w$ is a function of a single real variable. My confusion is, why do we actually say that $w$ is a function of time? Surely $w$ is just a function on some subset of the real numbers (depending specifically on how we chose to define $w$ ), rather than a function of time? Sure, $w$ only has the interpretation we think it does (cumulative amount of wax burnt) when we provide a time as its argument, but why does that mean it is a function of time ? There's nothing stopping me from putting any old argument (provided $w$ is defined at that point) in to $w$ , like the distance I have walked since the candle was lit. Sure, we can't really interpret $w$ in the same way if I did this, but there is nothing in the definition of $w$ which stops me from doing this. Also, what happens when I do some differentiation on $w$ . If I differentiate $w$ ""with respect to time"", then I'd get the time rate at which the candle is burning. If I differentiate $w$ ""with respect to"" the distance I have walked since the candle was lit, I'd expect to get either zero (since $w$ is not a function of this), or something more complicated (since the distance I have walked is related to time). I just can't see mathematically what is happening here: ultimately, no matter what we're calling our variables, $w$ is a function of a single variable, not of multiple, and so shouldn't there be absolutely no ambiguity in how to differentiate $w$ ? Shouldn't there just be ""the derivative of w"", found by differentiating $w$ with respect to its argument (writing ""with respect to its argument"" is redundant!). Can anyone help clarify what we mean by ""function of"" as opposed to function, and how this is important when we differentiate functions ""with respect to"" something? Thanks!","['calculus', 'derivatives', 'terminology']"
3498817,Why a particular choice of power series $f$ working for Lubin-Tate theory?,"This is related to Iwasawa's Local Class Field Theory Chpt 3 and 4. Let $k$ be a local field and $K$ be maximal unramified algebraic extension of $k$ . Set $\Omega$ the algebraic closure of $k$ . Take $\bar{K}$ and $\bar{\Omega}$ as completed fields under extension of discrete valuation from $k$ . Suppose $F$ is a discrete valued field. Denote $O_F$ as the valuation ring associated to $F$ . Let $F_q$ be the residue field. Set $q$ cardinality of residue field of $k$ . Let $f\in O_{\bar{K}}[[x]]$ s.t. $f-\pi x\in (x)^2$ and $f=x^q$ under reduction against prime ideal of $O_{\bar{K}}$ where $\pi$ is any prime ideal generator of $O_{\bar{K}}$ . Then it follows from the book that there is a unique formal group law $F_f(X,Y)$ associated to $f$ s.t. $f\in Hom(F_f,F_f^\phi)$ where $\phi$ is frobenius automorphism identified with $Gal(\bar{K},k)$ due to unramified extension... etc $\textbf{Q:}$ Why does this $f$ work and magically giving rise to formal group law?(One of possible reason is that this $f$ allows pinning down various power series $F_f$ uniquely but this is looking from back. The proof is not too hard but it is not short or obvious at first sight. Roughly, there is a unique $F$ s.t. $F(X_1,\dots, X_n)$ s.t. $f\circ F=F^\phi\circ f$ .) In other words, why should I think this $f$ is a ""godly"" given choice in that particular form?(Why I can't take $f-\pi x^2\in (x)^3$ and $f=x^{q^2}$ instead? It is possible to twist $f$ by invertible power series $\theta$ . Consider $f^\theta=\theta\circ f\circ \theta^{-1}$ where $\theta\in O_{\bar{K}}[[x]]$ . It is easy to check if $f$ has property boxed above, then $f^\theta$ will also have the same property.) It is clear that $f$ contains both prime element information by leading term and Frobenius map information $x^q$ .","['algebraic-number-theory', 'formal-groups', 'number-theory', 'abstract-algebra', 'class-field-theory']"
3498836,Each eigenvalue of $e^X$ is the exponential of an eigenvalue of $X$,"Let $X \in M(n)$ , I was able to show that if $v$ is an eigenvector of $X$ with corresponding eigenvalue $\lambda$ , that $v$ is also an eigenvector of $e^X$ with eigenvalue $e^{\lambda}$ , but I couldn't show that each eigenvalue of $e^X$ is the exponential of an eigenvalue of $X$ .","['matrices', 'matrix-exponential', 'linear-algebra', 'eigenvalues-eigenvectors']"
3498840,"How to evaluate $\int_{0}^{\pi}\frac{2x^3-3\pi x^2}{(1+\sin{x})^2}\,\textrm{d}x\,$?","Problem: Let $f$ be a bounded continuous function on the interval [0,1]. (a) show that $\int_{0}^{\pi} xf\,(\sin{x})\,\textrm{d}x = \frac{\pi}{2}\int_{0}^{\pi}f\,(\sin{x})\,\textrm{d}x\,$ . (b) Hence evaluate $\int_{0}^{\pi} \dfrac{x}{1+\sin{x}}\,\textrm{d}x\,$ . (c) Hence deduce that $\int_{0}^{\pi} \dfrac{2x^3-3\pi x^2}{(1+\sin{x})^2}\,\textrm{d}x = -\dfrac{2\pi ^3}{3}\,$ . I have completed Part (a) and Part (b). The answer of Part (b) is $\pi\,$ . I have tried to do Part (c) in the following way: $\textrm{Let }I=\int_{0}^{\pi} \dfrac{2x^3-3\pi x^2}{(1+\sin{x})^2}\,\textrm{d}x\\ 
\quad\;\;\;\,=\int_{0}^{\pi} \dfrac{2(\pi-x)^3-3\pi (\pi-x)^2}{(1+\sin{(\pi-x)})^2}\,\textrm{d}x\quad(\textrm{by letting }u=\pi-x\textrm{)}\\
\quad\;\;\;\,=\int_{0}^{\pi} \dfrac{2(\pi^3-3\pi^{2}x+3\pi x^2-x^3)-3\pi (\pi-x)^2}{(1+\sin{(\pi-x)})^2}\,\textrm{d}x\\
\quad\;\;\;\,=\int_{0}^{\pi} \dfrac{-\pi^3+3\pi^{2}x-2x^3}{(1+\sin{x})^2}\,\textrm{d}x\\
\quad\;\;\;\,=-\pi^3\int_{0}^{\pi}\dfrac{1}{(1+\sin{x})^2}\,\textrm{d}x-I\\
\quad\;\;\;\,=-\dfrac{\pi^3}{2}\int_{0}^{\pi}\dfrac{1}{(1+\sin{x})^2}\,\textrm{d}x\\
\quad\;\;\;\,=-\dfrac{\pi^3}{2}\cdot\dfrac{4}{3} \quad\textrm{(by letting }t=\tan{(\frac{x}{2})}\textrm{)}\\
\quad\;\;\;\,=-\dfrac{2\pi^3}{3}\\ \textrm{However, I did not make use of any results from Part (a) and Part (b).}\\
\textrm{I hope there is another solution that follows the hints of the problem.}$","['integration', 'definite-integrals']"
3498846,"What's the origin of the term ""chaotic topology""?","If $X$ is a set, some sources* refer to the topology $\{\emptyset,X \}$ as the chaotic topology . (I've also seen it called the trivial , codiscrete , and indiscrete topology.) What is the origin of and motivation for this term? The term discrete topology makes sense to me because the restriction of the Euclidean topology on $\mathbb R$ to a set of discrete points results in the discrete topology. But I can see no such explanation for the term chaotic topology . * [1] [2] [3] [4]","['general-topology', 'algebraic-geometry', 'terminology']"
3498848,Munkres Example 22.6,"This is a slight variation of Example 6 on pg 143 of Munkres. Let $X = \cup_{n\in \mathbb{N}} [0,1]\times \{\frac{1}{n}\}$ and $Z = \{x\times (\frac{x}{n}) | x\in[0,1], n\in\mathbb{N}]\}$ be subsets of $\mathbb{R}^2$ . Then is the map $g:X\to Z$ given by $g(x\times \frac{1}{n}) = x\times \frac{x}{n}$ a quotient map? The example in Munkres considers $X$ to be $\cup_{n\in \mathbb{N}} [0,1]\times \{n\}$ and shows that the above map is not a quotient map in that case. I was trying to figure out what exactly causes this map to not be a quotient map and came up with the above problem instead.","['general-topology', 'quotient-spaces']"
3498874,Every topological vector space over a connected field is connected?,"I think this is true but when I tried searching it I didn't find anything.  Let $V$ be a linear topological space over a connected field $\mathbb{K}$ .  Then for every $x\in V$ , the map $a \mapsto ax$ is continuous and hence has connected image $[x]=\{ax$ $|$ $a \in \mathbb{K}\}$ .  Since $V=\cup_{x\in V}[x]$ , and each $[x]$ share the point $0$ , $V$ is connected.  Am I missing something?","['general-topology', 'vector-spaces']"
3498881,Does set remain bounded if these integer constraints are removed?,"Question: Let $P$ be a nonempty polyhedron in $\mathbb{R}^n$ , and let $l_i, u_i \in \mathbb{R}$ for all $i \in I$ , where $I \subseteq \{1,\dots,n\}$ . I'm looking at a problem where the feasible region $$
F := P \cap \{x \in \mathbb{R}^n \mid l_i \leq x_i \leq u_i,\; x_i \in \mathbb{Z} \quad \forall i \in I\}
$$ is nonempty and bounded . Does it follow that $M := P \cap \{x \in \mathbb{R}^n \mid l_i \leq x_i \leq u_i\; \forall i \in I \}$ is also bounded? Thoughts: It seems like this should be the case. It works with the claim in my attempt below. But I haven't shown this claim to be true yet. Attempt: Set $M := P \cap \{x \in \mathbb R^n \mid l_i \leq x_i \leq u_i \; \forall i \in I\}$ and $Z := \{x \in \mathbb R^n \mid x_i \in \mathbb Z \; \forall i \in I\}$ . Want to show: If $M \cap Z$ is bounded, then $M$ is bounded. Claim: If $F = M \cap Z$ is bounded, then $\textrm{dist}\big(\textrm{conv}(M \cap Z),\; M \setminus \textrm{conv}(M \cap Z) \big) \leq 1$ with respect to the $\|\cdot\|_\infty$ norm. Proof: Haven't been able to show this yet. Define $B := B_{\leq}(0,1)$ to be the closed unit ball with respect to the $\|\cdot\|_\infty$ norm. Hence $$
W := \textrm{conv}(M \cap Z) + B \supseteq M
$$ Note that $\textrm{conv}(M \cap Z)$ is bounded, since convex hulls of bounded sets are bounded. Therefore $W$ is bounded as the sum of two bounded sets. The boundedness of $M$ follows from the boundedness of $W$ .","['polytopes', 'geometry', 'linear-programming', 'real-analysis', 'linear-algebra']"
3498888,Non-number theoretic formulation of Fermat's last theorem?,We have dozens of non-number theoretic formulations of Riemann hypothesis. I was wondering if there are any non-number theoretic formulations of Fermat's last theorem? I am in particular curious about some analytic formulation?,"['analytic-number-theory', 'number-theory', 'algebraic-geometry', 'reference-works']"
3498907,Seeking a Generalization of Rudin 9.40: A Multivariable Mean Value Theorem,"Let $n\in\mathbb{Z}^+$ .
Let $A$ be an open subset of $\mathbb{R}^n$ .
Let $f : A\to\mathbb{R}$ be a map.
Let $K = [a_1,b_1]\times\cdots\times[a_n,b_n]$ be an $n$ -cell contained in $A$ $(a_i<b_i \hspace{1mm}\forall i\in\{1,\ldots,n\})$ .
To be concise, suppose that for each $k\in\{1,\ldots,n\},$ the partial derivative $$
    \frac{\partial^k f(c)}{\partial x_k \partial x_{k-1} \cdots \partial x_1}(c)
$$ exists at every $c \in K$ . In POMA (3e), Rudin shows (Thm 9.40) that in the case $n=2$ , there exists a $(c_1,c_2) \in K$ such that $$
\frac{\partial^2f(c_1,c_2)}{\partial x_2 \partial x_1} (b_2-a_2)(b_1-a_1)
\hspace{1mm}=\hspace{1mm}
f(b_1,b_2) - f(b_1,a_2) - \big(f(a_1,b_2) - f(a_1,a_2)\big).
$$ I believe I understand the proof well, which essentially consists of applying the ordinary MVT to the function $[a_1,b_1]\to\mathbb{R} : t \mapsto f(t,b_2) - f(t,a_2)$ .
Then, $$
f(b_1,b_2) - f(b_1,a_2) - \big(f(a_1,b_2) - f(a_1,a_2)\big)
\hspace{1mm}=\hspace{1mm}
(b_1-a_1)\left(\frac{\partial f(c_1,b_2)}{\partial x_1} - \frac{\partial f(c_1,a_2)}{\partial x_1}\right)
\hspace{1mm}=\hspace{1mm}
(b_1-a_1)(b_2-a_2) \frac{\partial^2f(c_1,c_2)}{\partial x_2 \partial x_1}
$$ where the second equality follows from applying the MVT to the function $t \mapsto \frac{\partial f(c_1,t)}{\partial x_2}$ . My question is other. Preceding the theorem, he writes (pg. 235) For simplicity (and without loss of generality) we state our next two theorems for real functions of two variables. Man, that wouldn't have bothered me, if not for his unjustified claim that the omission of cases $n>2$ is WLOG. I'm currently stumped trying to derive an analogous formula for ${n=3}$ (I keep getting partial derivatives that don't cancel). My quesiton is: does someone happen to know the general formula? I'm really tempted to try to derive it, but realistically I just don't know if I can spare the time. If I were going to, a hint might not be a bad thing. My first several google searches have turned up nothing. Also, Rudin's choice of words suggests to me that we may yet get analogous results for functions which are not only real-valued. What would such results be? Thanks for your time.","['multivariable-calculus', 'analysis', 'real-analysis']"
3498908,Solving $\tan x= \tan 2x\tan3x\tan4x$ and a related geometric problem,"My friend asked me a geometric problem. In $\triangle ABC$ , $\angle B=\angle C=70^\circ$ . $D$ is an interior point of the triangle such that $\angle BCD=40^\circ$ and $\angle CBD=20^\circ$ . Find $\angle BAD$ . If $\angle BAD=\theta$ , it is not difficult to see that $\dfrac{\tan(40^\circ-\theta)}{\tan40^\circ}=\dfrac{\tan30^\circ}{\tan70^\circ}$ and I managed to show that $\tan(40^\circ-\theta)=\tan 10^\circ$ using trigonometric identities. I actually proved that $\tan10^\circ\tan70^\circ=\tan30^\circ\tan40^\circ$ , or equivalently, $\tan10^\circ=\tan20^\circ\tan30^\circ\tan40^\circ$ . This result is so beautiful and make me interested in the equation $\tan x= \tan2x\tan3x\tan4x$ , but I have difficulty in solving it. By plotting the graph, I can see that the solution is $180n^\circ$ or $60n^\circ\pm10^\circ$ . My questions are How to solve the original geometric problem without using trigonometry? How to solve the equation $\tan x= \tan2x\tan3x\tan4x$ ? Remark
Just find a solution to the second question a few minutes after posting it.  But I still want to see if there are other ways to solve it.","['euclidean-geometry', 'proof-writing', 'geometry', 'triangles', 'trigonometry']"
3498985,"Total number of distinct $x\in[0,1]$ for which $\int_{0}^{x}\frac{t^2}{1+t^4}dt=2x-1$","Find total number of distinct $x\in[0,1]$ for which $$\int_{0}^{x}\frac{t^2}{1+t^4}dt=2x-1$$ My multiple attempts are as follows:- Attempt $1$ : $$t^2=\tan\theta$$ $$2t\dfrac{dt}{d\theta}=\sec^2\theta$$ $$dt=\dfrac{\sec^2\theta}{2\sqrt{\tan\theta}}d\theta$$ $$\int_{0}^{\tan^{-1}x^2}\dfrac{\tan\theta}{1+\tan^2\theta}\dfrac{\sec^2\theta}{2\sqrt{\tan\theta}}d\theta=2x-1$$ $$\int_{0}^{\tan^{-1}x^2}\sqrt{\tan\theta}d\theta=4x-2$$ $$\int_{0}^{\tan^{-1}x^2}\dfrac{\sqrt{\sin\theta}}{\sqrt{\cos\theta}}=4x-2$$ $$\int_{0}^{\tan^{-1}x^2}\dfrac{\sin\theta}{\sqrt{\sin\theta}\sqrt{\cos\theta}}=4x-2$$ $$\cos\theta=y$$ $$\int_{1}^{\frac{1}{\sqrt{1+x^4}}}-\dfrac{dy}{\sqrt{y}\sqrt{\sqrt{1-y^2}}}=4x-2$$ Now it is unsolvable from here. Attempt $2$ : It seems like this integral is unsolvable, so let's apply Riemann sum $$\lim_{h\to0}h(f(0)+f(h)+f(2h)+f(3h)\cdots\cdots f(nh))=2x-1$$ $$nh=x$$ $$\lim_{h\to0}h\sum_{r=0}^{n}f(rh)=2x-1$$ $$\lim_{h\to0}h\sum_{r=0}^{n}\dfrac{r^2h^2}{1+r^4h^4}=2x-1$$ If we put the limit $h\rightarrow 0$ , then $0\cdot(0+0+0\cdots\cdots)$ where inside the bracket we have indeterminate form as $n\rightarrow \infty$ Now I was not getting how to proceed further.","['integration', 'analysis', 'real-analysis', 'calculus', 'sequences-and-series']"
3498999,Entire function that maps real axis to unit circle,"Question: Determine the most general entire function $f$ having the property $|f(z)|=1$ when $\text{Im}(z)=0$ . One possible class of such functions is $$
f(z)=e^{icz}
$$ with $c\in\mathbb{R}$ is a constant. But how to find other functions or how to prove all the entire function has such a form?",['complex-analysis']
3499002,What is the probability that a prime is less than the number formed by reversing its digits?,"Let $p_n$ ne the $n$ -th prime and let $r_n$ be the number formed by reversing the digits of $p_n$ in base $10$ notation. All prime numbers greater than $5$ end in $1,3,7$ or $9$ . Take the first $n$ primes, count the number of primes $r(n)$ of them which are less than the number formed by their reverse. To what limiting value does $\dfrac{r(n)}{n}$ converge to as $n \to \infty$ or does it exist in the first place? It is easy to evaluate this for primes whose first and last digits are different. However when primes begin and end with the same digit then we have to consider all the intermediate digits. Experimental data shows that the limiting value may not exist and oscillate between $0.506$ and $0.707$ .","['number-theory', 'elementary-number-theory', 'sequences-and-series', 'limits', 'prime-numbers']"
3499027,Prove/disprove that we cannot have $V = U_1 \oplus U_2$,"$V$ vector space of upper triangular $3\times3$ matrices. $𝑈1$ and $𝑈2$ are subspaces of $𝑉$ . Every non-zero member of $𝑈1$ is invertible. Every member of $𝑈2$ is non-invertible. Prove/disprove that we cannot have $𝑉=𝑈1⊕𝑈2$ I think that this is correct and My idea is to prove that $U1 \cap U2 \ne {\{0}\}$ But I""m having hard time to write General member for $U1$ and $U2$ any hint how to prove this ? thanks","['matrices', 'linear-algebra', 'vector-spaces', 'inverse']"
3499051,When does multiplication by an orthogonal matrix preserves the eigenvalues?,"Let $A$ be a real $n \times n$ matrix, with rank $\ge n-1$ . Suppose that the eigenvalues (counted with multiplicities) of $A$ are the same as the eigenvalues of $QA$ for some orthogonal matrix $Q$ . Must $Q$ be diagonal? The condition $\text{rank}(A)\ge n-1$ is necessary: If we allow $\text{rank}(A)< n-1$ , then one can take $A$ to be block diagonal with the $2 \times 2$ zero matrix as its first block. Then the entire $\text{O}(2) \times \text{Id}_{n-2}$ preserves the eigenvalues.","['eigenvalues-eigenvectors', 'orthogonal-matrices', 'matrix-calculus', 'linear-algebra', 'matrix-equations']"
3499085,Prove that $\lim\limits_{n\rightarrow \infty}P_n(A)=P(A)$ implies $\lim\limits_{n\rightarrow \infty} \int f ~dP_n = \int f ~dP$,"Let $P_n, n \in \mathbb{N}$ and $P$ be probability measures on the measurable space $(\Omega,\mathfrak{S})$ and assume $\forall A \in \mathfrak{S}: \lim\limits_{n\rightarrow \infty}P_n(A)=P(A)$ . I want to prove now that for all bounded measurable functions $f$ the following holds $$\lim\limits_{n\rightarrow \infty} \int f ~dP_n = \int f ~dP.$$ My approach was to first assume $f\geq0$ and approximate $f$ with step functions $(u_k)\in \mathcal{T}$ where $u_k$ converges uniformly to $f$ . Let $u_k = \sum\limits_{i} \alpha_i^{(k)} 1_{A_i^{(k)}}$ . Now I do the following: \begin{align}
\lim\limits_{n\rightarrow \infty} \int f ~dP_n &= \lim\limits_{n\rightarrow \infty} \int \lim\limits_{k\rightarrow \infty} u_k ~dP_n \\
&= \lim\limits_{n\rightarrow \infty} \lim\limits_{k\rightarrow \infty} \int u_k ~dP_n \\
&=\lim\limits_{n\rightarrow \infty} \lim\limits_{k\rightarrow \infty} \sum\limits_{i} \alpha_i^{(k)} P_n(A_i^{(k)}) \\
&\stackrel{(*)}{=} \lim\limits_{k\rightarrow \infty} \lim\limits_{n\rightarrow \infty} \sum\limits_{i} \alpha_i^{(k)} P_n(A_i^{(k)}) \\
&= \lim\limits_{k\rightarrow \infty} \sum\limits_{i} \alpha_i^{(k)} P(A_i^{(k)}) \\
&= \lim\limits_{k\rightarrow \infty} \int u_k ~dP \\
&= \int \lim\limits_{k\rightarrow \infty} u_k ~dP \\
&= \int f ~dP.
\end{align} I can justify all steps, except for $(*)$ . I now I would need uniform convergence of one of the two sequences with respect to the other but I don't see why this should be the case. On the other hand I don't have another approach for the proof. Does this break my argumentation? Could someone help me with this? For the general case I would split $f$ into a positive and a negative part $f=f^++f^-$ and do basically the same argumentation.","['measure-theory', 'weak-convergence', 'probability-theory', 'solution-verification', 'pointwise-convergence']"
3499127,Maximum point of a function,"I am looking to find the local maximum value of the function $$f(x,y,z) = 9+ \frac{(x-y)^2}{xy} + \frac{(y-z)^2}{yz}+ \frac{(z-x)^2}{zx}$$ at a point within the cube $[a,b] \times [a,b] \times [a,b],$ where $0<a<b$ . I took the partial derivative of f with respect to x, y, and z. I set those to zero. I used the second-derivative test for multi-variable functions. The second-derivative test requires the computation of a 3 by 3 matrix. (Because the function is a three-variable function) $D= \det \pmatrix{f_{xx} & f_{xy} &f_{xz} \\
f_{yx} & f_{yy} &f_{yz}
\\
f_{zx} & f_{zy} &f_{zz}}$ I found that the first dervatives equal to zero at $x=y=z$ . On the other hand, Wolfram Alpha confirms that the max for the $[1,3] \times [1,3] \times [1,3]$ is for $y=z=\frac{x}{3}$ and $x=\frac{y}{3}=\frac{z}{3}$ . I got confused","['maxima-minima', 'multivariable-calculus', 'calculus', 'partial-derivative', 'optimization']"
3499136,Show that $(g \circ f)^{-1}(C) = g^{-1}(f^{-1}(C)).$,Suppose that $f:S \rightarrow T$ and $g:T \rightarrow U$ are two functions and that $C \subseteq U.$ Show that $(g \circ f)^{-1}(C) = g^{-1}(f^{-1}(C)).$ Is the statement of this question is correct? or the correctness depends on whether the $(-1)$ represents the inverse function or the preimage (inverse image)?,"['elementary-set-theory', 'functions', 'inverse-function']"
3499143,Polar complex form and exponent question,"I don’t understand how exponents impact the angle $\phi$ . My understanding: $\quad$ Let’s say we have a number: $$(\cos{(3\pi/2)}+i\sin{(3\pi/2)})^{3} = (\cos{(\frac{3\pi*3}{2}+2k\pi)}+i\sin{(\frac{3\pi*3}{2}+2k\pi)})$$ I understand this, we multiply $\phi$ with exponent and $+2k\pi \,$ is the period of the function. And of course there is $|z|^{3}$ multiplied with the whole expression. $\quad$ I don’t understand this: $$(\cos{(3\pi/2)}+i\sin{(3\pi/2)})^{1/4}= (\cos{(\frac{3\pi+2k\pi}{2*4}}+i\sin{(\frac{3\pi+2k\pi}{2*4})})$$ Why is $2k\pi$ in the numerator if the exponent is less than one. What to do if the exponent is $3/4$ or $7/4$ or some other fraction? Maybe I written wrong in my notes and $+2k\pi$ is always out of the fraction","['complex-analysis', 'trigonometry', 'complex-numbers', 'polar-coordinates']"
3499173,Find minimum of $\frac{n}{S(n)}$,"For every Natural Number like $n$ consider: $\frac{n}{S(n)}$ so that $S(n)$ is sum 
of the digits of the number $n$ in base-10. find minimum of $\frac{n}{S(n)}$ when: a) $9<n<100$ b) $99<n<1000$ c) $999<n<10000$ d) $9999<n<100000$ for $9<n<100$ I tried: $n=10a+b$ and $Min(\frac{10a+b}{a+b})=Min(1+\frac{9a}{a+b})$ so It is obvious that $b$ should be 9. I put $a=1,2,3,...$ and realized that if $a=1$ it will be minimum so the answer of part (a) is 19 but I dont know How we can mathematically show that $a=1$ for part b,c ,d I cant find mathematically way to show when this fraction (for example for part b: $\frac{100a+10b+c}{a+b+c}$ ) is minimum",['number-theory']
3499304,"Complex Analysis 2.1.2 - 1 If g(w) and f(z) are analytic functions, show that g(f(z)) is also analytic. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Complex Analysis 2.1.2 - 1 If $g(w)$ and $f(z)$ are analytic functions, show that $g(f(z))$ is also analytic.","['complex-analysis', 'analyticity', 'analytic-functions']"
3499311,Calculate the limit of function,"I was trying to find it for some time, but couldn't, so please help me. $ \lim_{x\to\infty} ( \sqrt[100]{(x + 3*1)(x + 3*2)...(x +3*100)} - x)$","['limits', 'functions']"
3499507,Prove $\lim\limits_{n\rightarrow \infty} \int f_n ~dP_n = \int f ~dP$,"Assume $f_n, ~n\in \mathbb{N}$ and $f$ are continuous functions where $f_n$ converges uniformly to $f$ and $P_n, ~n\in\mathbb{N}$ and are probability measures on $(\mathbb{R},\mathcal{B})$ where $P_n$ converges weakly to $P$ . Edit: $f$ is also bounded. I want to prove that $$\lim\limits_{n\rightarrow \infty} \int f_n ~dP_n = \int f ~dP.$$ Solution: Uniform convergence of $f_n$ means $$\forall \varepsilon > 0 ~\exists n_1(\varepsilon) \in \mathbb{N} ~\forall n>n_1(\varepsilon): \Vert f_n - f \Vert_\infty < \frac{\varepsilon}{2}.$$ Weakly convegence of $P_n$ means $$\forall \varepsilon > 0 ~\exists n_2(\varepsilon) \in \mathbb{N} ~\forall n>n_2(\varepsilon): \vert \int g ~dP_n - \int g ~dP \vert < \frac{\varepsilon}{2}$$ for any bounded continous function $g$ . Now I fix $\varepsilon > 0$ and $k:=\max\{n_1(\varepsilon),n_2(\varepsilon)\}$ . \begin{align}
\forall n > k:~~&\vert \int f_n ~dP_n - \int f ~dP\vert \\ 
&= \vert \int (f_n-f) ~dP_n + \int f ~dP_n -\int f ~dP\vert \\
&\leq \int \vert f_n-f \vert ~dP_n + \vert\int f ~dP_n -\int f ~dP\vert \\
&\leq \Vert f_n - f \Vert_\infty P_n(\mathbb{R}) + \vert \int f ~dP_n -\int f ~dP\vert \\
&\leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\end{align} Since, $\varepsilon$ was arbitrary the claim follows.","['measure-theory', 'solution-verification', 'probability-theory', 'weak-convergence']"
3499533,"Show $\tan(n) < n^q$, conjectured $q < 1.1$","Show $\tan(n) < n^q$ , $n \in \mathbb{N}$ , $n > 1$ . The argument of the $\tan$ -function is in radians. It is conjectured that $q < 1.1$ . In fact, search for the maximum of $q$ in $n\in [2, 10^9]$ gives $\tan(260515)= 383610.707744 = 260515^{1.031031}$ and then the next higher $q$ 's only at $\tan(122925461)= 326900723.479835 = 122925461^{1.052508}$ , and further $\tan(534483448)= 1914547468.536829 =  534483448^{1.063489}$ . It is known that $\tan(n)$ is unbounded (see math.stackexchange.com/questions/1056119 ). So it is clear that with increasing $n$ ,  ever larger $\tan(n)$ will eventually be found. While we know that $n\ne(k+1/2)\pi$ ,  that doesn't mean that we know how close $n$ comes to some $(k+1/2)\pi$ with growing $n$ . In the range of $n$ above, it appears that no higher values of $q$ are found, and the conjecture is that such high $n$ are required for higher values of $\tan(n)$ , that no higher $q$ will be attained. Possibly the limit for $q$ will have to be made more loose than $q < 1.1$ , where derivations of such looser bounds would certainly be appreciated. For possible relations to the irrationality measure of $\pi$ , which is unknown, see discussions in here: math.stackexchange.com/questions/2977461 .","['number-theory', 'trigonometry', 'sequences-and-series', 'real-analysis']"
3499557,find positive real number x that satisfies $2001=x\lfloor x\lfloor x\lfloor x\rfloor\rfloor\rfloor$,"$$2001=x\lfloor x\lfloor x\lfloor x\rfloor\rfloor\rfloor\\
(x>0 ,x\in\mathbb R)$$ find $x$ that satisfies the expression above. My attempt Since $$(x+1)^4\gt\text{(right side of the expression given)}\geq x^4\\
7\gt x\geq6$$ And, $\frac{2001}x$ needs to be a integer. If $x$ is not a rational number, $\frac{2001}x$ is not a integer. So, $x$ can be thought as $\frac ab(a,b\in\mathbb Z,a|2001)$ But, I failed to go further.","['elementary-number-theory', 'algebra-precalculus', 'ceiling-and-floor-functions']"
3499645,"Why does a polynomial with real, simple roots change its sign between its roots?","In the mathematics book I have, there is a sub-chapter called ""Practical procedure to resolve inequalities"" that states: Given a polynomial $P(x)$ that has real, simple roots, and finding the solutions to the equation $P(x) = 0$ , afterwards sorting the solutions $x_1, x_2, ..., x_n$ , then the sign of $P$ over an interval $(x_i, x_{i + 1})$ is the opposite of its neighboring intervals $(x_{i - 1}, x_i)$ and $(x_{i + 1}, x_{i + 2})$ . I've plotted functions of the form $$a\prod_{i = 1}^{n}(x - a_i), \space a, a_1, a_2, ..., a_n \in [0, \infty), \space a_i \ne a_j \space \forall i, j \in \{1, 2, ..., n\} $$ What's an intuitive way of thinking about this and why it happens?","['real-numbers', 'calculus', 'polynomials', 'real-analysis']"
3499657,Discontinuity of a derivative,"We know that if $f'(x)$ is discontinuous at $x_{0}$ , then $x_{0}$ is a fundamental essential discontinuity of $f'(x)$ (because derivatives can't have removable discontinuity or a jump discontinuity). So why derivative of absolute value has jump discontinuity in $x=0$ ?","['limits', 'continuity', 'derivatives', 'absolute-value']"
3499670,"For any $k \gt 1$, if $n!+k$ is a square then will $n \le k$ always be true?","In Dabrowski's paper , he showed that it would follow from the abc conjecture that the equation $$n!+k=m^2$$ has a finite number of solutions $n, m$ for any given $k$ which was my motivation to find solutions for different values of $k$ . Using PARI/GP, I observed that for any $k \gt 1$ , if $n!+k$ is a square, then $n \le k$ . I didn't find any counterexample in my search that covered a range of $k\le 2500$ and $n\le 10^4$ for each $k$ . Questions: $(1)$ Can we prove that for any $k \gt 1$ , if $n!+k$ is a square then $n \le k$ , thereby restricting Dabrowski's original statement? $(2)$ If false then what would be the smallest counterexample? Update 1: This also seems true for $n!-k$ , when $k\gt 2$ . Update 2: After some more testing on PARI, I conjecture that for any $k \gt 3$ , if $n!+k$ is a perfect power, then $n\le k$ . This also seems true for $n!-k$ .","['number-theory', 'square-numbers', 'conjectures', 'elementary-number-theory']"
3499680,"Find al continuous functions $f:[0,1]\to \mathbb{R}$ such that $f(x) f(1-x) \ge \int_0^1 (f^2(x)-f(x)f(1-x)+f^2(1-x)) dx, \forall x\in [0,1]$","Find al continuous functions $f:[0,1]\to \mathbb{R}$ such that $f(x) f(1-x)  \ge \int_0^1 (f^2(x)-f(x)f(1-x)+f^2(1-x)) dx, \forall x\in [0,1]$ . I began by integrating this inequality on $[0,1]$ to get that $\int_0^1 (f(x)-f(1-x))^2 dx\le0$ . Since we also have that $\int_0^1 (f(x)-f(1-x))^2 dx\ge 0$ we obtain that $\int_0^1 (f(x)-f(1-x))^2 dx=0$ and because $f$ is continuous it follows that $f(x)=f(1-x), \forall x\in [0,1]$ . Now, this functional equation has infinitely many solutions, so we can't determine $f$ only from here. After substituting back into the original equation we get that $f^2(x) \ge \int_0^1 f^2(x) dx,  \forall x\in [0,1]$ . I honestly do not know how to proceed from here.","['functional-equations', 'calculus', 'functions', 'real-analysis']"
3499763,The conditional probability of the evidence on a crime scene?,"From page 88, Introduction to Probability (2019 2 edn) by Jessica Hwang and Joseph K. Blitzstein. Suppose that there are $5$ blood types in the population, named type $1$ through type $5$ ,
with probabilities $p_1, p_2,\cdots ,p_5$ . A crime was committed by two individuals. A suspect,who has blood type $1$ , has prior probability $p$ of being guilty. At the crime scene, blood
evidence is collected, which shows that one of the criminals has type $1$ and the other
has type $2$ . Find the posterior probability that the suspect is guilty, given the evidence. Does the evidence make it more likely or less likely that the suspect is guilty, or does this depend on the values of the parameters $p, p_1,. . . , p_5$ ? If it depends, give a simple criterion for when the evidence makes it more likely that the suspect is guilty. Proposed solution : Let $A_1$ be the event that the suspect (with blood type 1) is guilty. Let $X$ be the event that one of the criminals have blood type 1 and the other has type 2.  We can then use Bayes' rule to define $$
\begin{aligned}P(A_1|X) &=\frac{P(X|A_1)P(A_1)}{P(X)} \\         &=\frac{P(X|A_1)P(A_1)}{P(X|A_1)P(A_1)+P(X|A_1^c)P(A_1^c)} \\         &=\frac{P(X|A_1)p}{P(X|A_1)p+P(X|A_1^c)(1-p)} \\         &=\frac{P(X|A_1)p}{P(X|A_1)p+P(X|A_1^c)(1-p)} \\         &=\frac{1p}{1p_2p+2p_1p_2(1-p)} \\         \end{aligned}
$$ I have two questions on what happens on the last line? The last line I got from the following source here . First ,  where does this equality come from $P(X|A_1) = 1p_2$ ? Let $X_1$ be the event that the first criminal's blood type is blood type 1, and so $P(X_1)=p_1$ if the criminal is unknown. And, let $X_2$ be the event that the second criminal's blood type is blood type 2, so $P(X_2)=p_2$ if that criminal is still unknown.  Assume also that the two criminal's blood type is independent. Is it then that $P(X|A_1) = P(X_1\cap X_2|A_1) = P(X_1|A_1)P(X_2|A_1)=1p_2$ ? In other words, $P(X_1|A_1)=1$ since the suspect is now assumed to be the criminal, and $ P(X_2|A_1)=p_2$ since the second criminal is unknown and thus we equate this with the population parameter. Second ,  where does this other equality come from $P(X|A_1^c) = 2p_1p_2$ ? Here, with the notation and logic in my first question, I regard the following to be true $P(X|A_1^c) = P(X_1\cap X_2|A_1^c) = P(X_1|A_1^c)P(X_2|A_1^c) = p_1p_2$ . Hence, since the two criminals are unknown, the probability of the evidence is just the population parameters occurring simultaneously (the blood of the two criminals being spilled on the crime scene). In other words, where does the 2 come from?","['conditional-probability', 'statistics', 'probability']"
3499777,Residue Theorem if Pole is on Contour,"This is  a question on contour integration. The particular problem has a (simple) pole on the contour which prohibits a direct application of Cauchy's Residue Theorem. Daniel Fischer commented as follows Not really. [...] if the contour is smooth at the pole, it's as if half of the pole lies inside the contour and half outside. If the contour has a corner at the pole, with (inner) angle $\alpha$ , the fraction is $\frac{\alpha}{2\pi}$ , so you get $\alpha i$ times the residue of the pole instead of $2\pi i$ times as for singularities properly enclosed by the contour. The same result is mentioned in this question . Unfortunately, Daniel didn't know a reference for this (generalised) result. Can anyone point me to a book/paper/recourse which covers this result? I'd like to see a proof and some maths underlying this intuition. Thank you very much!","['integration', 'reference-request', 'complex-analysis', 'contour-integration', 'residue-calculus']"
3499854,"Presentation $\langle x,y\mid x^3=y^3=(xy)^3=1\rangle $ with normal abelian subgroup.","Let $G$ have the presentation $G=\langle x,y\mid x^3=y^3=(xy)^3=1\rangle $ and let $S=\{xyx,x^2y\}$ , $H=\langle\, S\,\rangle $ . Prove that $H$ is a normal abelian subgroup of $G$ . That the elements of $S$ commute is easy to see. And then $H$ is abelian. As to $H$ being normal, I think it is enough to prove that given $s\in S$ , $x^{-1}sx \in H$ and $y^{-1}sy \in H$ . But how do I do it? I have tried but I failed.","['group-presentation', 'group-theory', 'normal-subgroups', 'abelian-groups']"
3499860,Solve $\sin 5x = \sin x$.,"I have to solve the following equation: $$\sin 5x = \sin x$$ This is what I tried: $$5x = x + 2k \pi \hspace{5cm} 5x = \pi - x + 2k \pi$$ $$4x = 2k \pi \hspace{5cm} 6x = (2k + 1) \pi$$ $$\hspace{2cm} x =  k \dfrac{\pi}{2}, k\in \mathbb{Z} \hspace{4cm} x = (2k + 1)\dfrac{\pi}{6}, k \in \mathbb{Z} \hspace{2cm}$$ So we have that: $$x \in \bigg \{ k \dfrac{\pi}{2} | k \in \mathbb{Z} \bigg \} \cup \bigg \{ (2k+1) \dfrac{\pi}{6} | k \in \mathbb{Z} \bigg \}$$ The problem is that my textbook has the following answers listed: A. $ \bigg \{ \dfrac{k \pi}{ 5 - (-1)^k } | k \in \mathbb{Z} \bigg \}$ B. $\bigg \{ \dfrac{k\pi}{5} | k \in \mathbb{Z} \bigg\}$ C. $\bigg \{ \dfrac{k\pi}{10} | k \in \mathbb{Z} \bigg \}$ D. $\bigg \{ (-1)^k \arcsin \dfrac{1}{5} + k\pi | k \in \mathbb{Z} \bigg \}$ E. $\bigg \{ (-1)^k \dfrac{\pi}{3} + k\pi | k \in \mathbb{Z} \bigg \}$ None of the listed answers look even remotely like my own. So did I do something wrong or is it one of those cases where the same answer can be written in multiple ways?",['trigonometry']
3499863,"How to approach proofs similar to ""Show a group, $G$, is infinite if $G = \langle r, s, t\mid rst = 1\rangle $""","How to approach proofs similar to ""Show a group, $G$ , is infinite if $G = \langle r, s, t\mid rst = 1\rangle $ "" I have not worked much with relations and tend to get lost in notation. I am practicing solving problems like the one in the title but am having a hard time as I am not sure the tricks to try or areas to investigate first in trying to make a proof. What are some hints for starting a proof about some quality of a group defined by a relation? So far the only relations I know about are the dihedral groups of order $2n$ , the quaternions, and cyclically generated groups so comparisons to how we show properties of those might be illuminating.","['group-presentation', 'relations', 'combinatorial-group-theory', 'abstract-algebra', 'group-theory']"
3499869,show this inequality with the sum $\sum_{i=1}^{n}x_{i}=n$,"if $n>20$ ,I have prove let $x_{i}>0,i=1,2,\cdots,n$ ,and such $$x_{1}+x_{2}+\cdots+x_{n}=n$$ show that $$F(x_{1},x_{2},\cdots,x_{n})=\left(\prod_{i=1}^{n}x_{i}\right)\cdot (\sum_{i=1}^{n}x^3_{i})\le n$$ and find all  other postive integers $3\le n\le 19$ I want use $$F\left(\dfrac{x_{1}+x_{2}}{2},\dfrac{x_{1}+x_{2}}{2},x_{3},\cdots,x_{n}\right)-F(x_{1},x_{2},\cdots,x_{n})=x_{3}x_{4}\cdots x_{n}\left(\dfrac{(x_{1}+x_{2})^2}{4}\left(\dfrac{(x_{1}+x_{2})^3}{4}+\sum_{i=3}^{n}x^3_{i}\right)-x_{1}x_{2}\sum_{i=1}^nx^3_{i}\right)$$ if we show that $$\left(\dfrac{(x_{1}+x_{2})^2}{4}\left(\dfrac{(x_{1}+x_{2})^3}{4}+\sum_{i=3}^{n}x^3_{i}\right)-x_{1}x_{2}\sum_{i=1}^nx^3_{i}\right)\ge0$$ or $$\dfrac{(x_{1}+x_{2})^5}{16}-x_{1}x_{2}(x^3_{1}+x^3_{2})+\sum_{i=3}^{n}x^3_{i}\left(\dfrac{(x_{1}+x_{2})^2}{4}-x_{1}x_{2}\right)\ge0$$ or $$\dfrac{(x_{1}-x_{2})^2}{4}\left(\dfrac{1}{4}(x_{1}+x_{2})(x^2_{1}-10x_{1}x_{2}+x^2_{2})+\sum_{i=3}^{n}x^3_{i}\right)\ge 0$$","['multivariable-calculus', 'symmetric-polynomials', 'inequality']"
3499892,Sampling from a distribution with given pdf,"Given a continuous multivariate pdf in analytical form (i.e. in function form), how can one sample from the corresponding distribution? In other words, what are the ways of coming up with random (or psuedo-random) realizations from the distribution with probability matching the one corresponding to the given pdf? What is the general idea/principle behind the sampling procedure? Referring to related references would be helpful too.","['probability-distributions', 'reference-request', 'sampling', 'probability-theory', 'probability']"
3499928,"""A typical function is morse""","On the wikipedia page for Morse theory it states the following A smooth real-valued function on a manifold M is a Morse function if it has no degenerate critical points.  A basic result of Morse theory says that almost all functions are Morse functions. Technically, the Morse functions form an open, dense subset of all smooth functions M → R in the C2 topology. This is sometimes expressed as ""a typical function is Morse"" or ""a generic function is Morse"". however no reference is given. After searching for a bit I cannot find this basic result in any papers on Morse Theory. Can anyone provide a reference for this statement?","['morse-theory', 'smooth-manifolds', 'differential-geometry']"
3499951,Estimate the derivative with Cauchy integral formula,"Suppose a metric $d$ is defined on the space of entire functions as follows: $$d(f, g)=\sum_{n=1}^{\infty} \min \left\{\frac{1}{2^{n}}, \max _{|z| \leq n}|f(z)-g(z)|\right\}$$ Is the operator of differentiation (the operator sending $f$ to $f^\prime$ )  continuous on this metric space of functions? Explain why or why not. Here is my try with Cauchy's Integral Formula. For $|z|<n$ , $$\begin{aligned}|f^\prime(z)-g^\prime(z)|&=
|\frac{1}{2\pi i}\int_{|w|=n}\frac{f(w)-g(w)}{(w-z)^2} dw|\\
&\leq \frac{1}{2\pi} \max_{|w|\leq n}|f(w)-g(w)| \int_{|w|=n} \frac{1}{(w-z)^2} dw.
 \end{aligned}$$ Since $z$ can be very closed to the boundary, $|w-z|$ can be very small. I don't know how to bound the integral in the inequality. Any idea to solve this question?",['complex-analysis']
3500045,Formal Power Series as Initial Objects?,"We often apply formal power series in places where it seems, at face value, somewhat suspect to do so. I'm primarily interested in why these formal manipulations work so broadly. A prime example comes from Concrete Mathematics, page 470-471. Here, $(\Delta f)(x) = f(x+1) - f(x)$ , and $Df = f'$ We can express $\Delta$ in terms of $D$ using Taylor's formula as follows: $f(x + \epsilon) = f(x) + \frac{f'(x)}{1!}\epsilon + \frac{f''(x)}{2!}\epsilon^2 + \cdots$ Setting $\epsilon = 1$ tells us that $\Delta f(x) = \\
f(x+1) - f(x) = \\
f'(x)/1! + f''(x)/2! + f'''(x)/3! + \cdots = \\
(D/1! + D^2/2! + D^3/3! + \cdots)f(x) = \\
(e^D - 1)f(x)
$ The authors continue, saying the inverse operator $\sum = 1/\Delta$ should thus be $1/(e^D - 1)$ . (Here $\sum$ is meant as an operator, though the authors continue using $\sum$ in its traditional context as well, as in the following power series.) We recognize $z/(e^z-1) = \sum B_k z^k/k!$ as a known power series, and conclude, somewhat surprisingly, that $\sum = \frac{B_0}{D} + \frac{B_1}{1!} + \frac{B_2}{2!}D + \frac{B_3}{3!}D^2 + \cdots = \int + \sum \frac{B_k}{k!}D^{k-1}$ This is the asymptotic expansion for the Euler Summation Formula . This derivation seems like nonsense, except for the fact that it isn't. We get a reasonable result out the other side, and every step makes sense if you're willing to suspend your disbelief. I have seen multiple other arguments just like this, where we flippantly go back and forth between functions and their series, even in places where a topology is not clearly visible to make sense of the infinite sums! One idea that I had comes from a topic in Knapp's Basic Algebra, the Permanence of Identities (page 212-214). The idea here is that equations which are true over $\mathbb{Z}[x_1,\ldots,x_n]$ ought to remain true over general rings when we substitute ring elements for the $x_i$ . While Knapp doesn't dwell on it, I justified this to myself since $\mathbb{Z}[x_1, \ldots, x_n]$ is initial among rings with $n$ distinguished elements, and since ring homs preserve truth, we get that a formula $p = q$ in this polynomial ring implies $p(r_1,\ldots,r_n) = q(r_1,\ldots,r_n)$ is true of any $r_i$ in any (commutative) ring $R$ . By analogy, it seems reasonable that a ring of formal power series (perhaps with rational coefficients?) should be initial in a suitable category, and that the identities we derive by working formally will then be true in, say, rings of operators (which would justify the above argument, modulo convergence issues). Finally, then, Does anybody have references for the soundness of power-series methods being applied in somewhat surprising settings? Additionally, can the argument I've given be made formal? Are there broad outlines for when these formal methods are permissible, and when (if ever) they lead us astray? Thanks in advance ^_^","['logic', 'category-theory', 'reference-request', 'abstract-algebra', 'formal-power-series']"
3500046,Does this linear subspace of matrices contain an invertible matrix?,"Let $\mathrm{M}_n(\mathbb{C})$ denote the space of $n\times n$ complex matrices, let $\mathcal{A}\subset\mathrm{M}_n(\mathbb{C})$ be any nonempty subset of matrices, and consider the set of matrices $$
\mathcal{A}^*\mathcal{A} = \{A^*B\, :\, A,B\in\mathcal{A}\}.
$$ Suppose that $\mathcal{A}^*\mathcal{A}$ is a family of commuting matrices and suppose further that there exist matrices $A_1,\dots,A_N\in\mathcal{A}$ such that $A_1^*A_1+\cdots+A_N^*A_N=I$ where $I$ is the $n\times n$ identity matrix. Question : Is it necessarily the case that $\mathrm{span}(\mathcal{A})$ contains an invertible matrix? Here are some of my thoughts: One may suppose without loss of generality that $\mathcal{A}=\mathrm{span}(\mathcal{A})$ (i.e., $\mathcal{A}$ is a linear subspace of matrices), since $$
\mathrm{span}(\mathcal{A}^*\mathcal{A}) = \mathrm{span}\bigl((\mathrm{span}(\mathcal{A}))^*(\mathrm{span}(\mathcal{A})\bigr).
$$ ( Edit : Note that each matrix in $\mathcal{A}^*\mathcal{A}$ is normal, since $A^*B\in\mathcal{A}^*\mathcal{A}$ implies $(A^*B)^*=B^*A\in\mathcal{A}^*\mathcal{A}$ and these matrices must commute.) Since $\mathcal{A}^*\mathcal{A}$ is a family of normal commuting matrices, there exists a unitary matrix $V$ such that $V^*A^*BV$ is a diagonal matrix for each $A,B\in\mathcal{A}$ . We may write each of the matrices $A_1,\dots,A_N$ in their polar decomposition as $$
A_i = U_i P_i
$$ for some unitary matrices $U_1,\dots,U_N$ and positive semidefinite matrices $P_1,\dots,P_N$ . Now the matrix $V^*A_i^*A_iV=V^*P_i^2V$ is diagonal for each $i$ and thus $V^*P_iV$ is diagonal for each $i$ . One has that $$
(V^*P_1V)^2+ \cdots + (V^*P_NV)^2 = V^*(P_1^2+\cdots+P_N^2)V = V^*(A_1^*A_1+\cdots+A_N^*A_N)V=V^*V = I.
$$ In particular, it follows that $P_1^2 + \cdots + P_N^2=I$ . 
Since each of the matrices $V^*P_iV$ is diagonal and positive, we have that $$
V^*\bigl(\sum_{i=1}^NP_i^2\bigr)V  = I \quad\Rightarrow\quad V^*\bigl(\sum_{i=1}^NP_i\bigr)V >0
$$ hence $\sum_{i=1}^NP_i$ is positive definite and thus invertible. But this is not quite what I want because it is not in $\mathcal{A}$ ......","['matrices', 'diagonalization', 'linear-algebra']"
3500078,Find the supremum of $h(z)$.,"We put $$‎h(z) = \frac{|‎\frac{e}{e-1}(e^z-1)|\;-‎\frac{3}{2}e^{|z|}+1‎}{‎\frac{\pi}{\pi^2-1}‎‎‎‎\sinh‎(\pi|z|)\;+‎\frac{1}{\pi^2-1}‎‎‎‎‎(\cosh‎‎(\pi|z|)-1) -e^{|z|}+1},‎\; z\in\mathbb{C}\setminus\{0\}.
$$ ‎‎‎‎My question is: What is the $\displaystyle\sup_{z\in\mathbb{C}\setminus\{0\}}h(z)$ (in other words, the least $M\leq \infty$ such that $h(z)\leq M$ , for all $z\neq0$ )? To Find this I firstly checked that the denominator of $h(z)$ is positive. Then using Wolfram Alpha I get it approximately equals to $0.000759$ . Now I want to know that is it correct? Anyone can help me to obtain the supremum (and how to calculate)? thanks a lot.","['real-analysis', 'calculus', 'limits', 'inequality', 'supremum-and-infimum']"
