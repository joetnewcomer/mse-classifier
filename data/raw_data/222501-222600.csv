question_id,title,body,tags
4565681,Does there exist a section of $GL_n(\mathbb{Z}/9\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/3\mathbb{Z})$?,"There is the reduction map $r : GL_n(\mathbb{Z}/9\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/3\mathbb{Z})$ . When does there exist a group homomorphism $i : GL_n(\mathbb{Z}/3\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/9\mathbb{Z})$ such that $r \circ i = id$ ? After testing on Python I think that there exists such section for $n = 2$ but not for $n \geq 3$ , but I don't manage to prove it. I tried to use the fact that the group $GL_n(\mathbb{Z}/3\mathbb{Z})$ is generated by two matrices : an elementary matrix $E$ and a permutation matrix $P$ that acts as a $n$ -cycle on the basis (modulo a sign, depending if $n$ is odd or even). I wanted to prove that there doesn't exist any $(i(E),i(P))$ in $GL_n(\mathbb{Z}/9\mathbb{Z})$ which respect the group law. However according to Python for $n=3$ , there exists for instance $(i(E),i(P))$ which preserve at least the order of $E$ , $P$ , $PE$ , $P^2E$ , so it seems that an argument using this method would be quite complicated... I managed to prove that there doesn't exist a section of $GL_n(\mathbb{Z}/p^2\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/p\mathbb{Z})$ for any $n \geq 2$ if $p \geq 5$ , but the proof doesn't work if $p =3$ .","['finite-fields', 'finite-groups', 'matrices', 'linear-algebra', 'group-theory']"
4565729,Proving that there's exactly one function,"While going through my old notes, I found an interesting math problem/claim: Let $X$ , $Y$ and $I$ be nonempty sets. For every $i \in I$ there is a subset $X_i \subseteq X$ with $X = \bigcup_{i \in I}X_i$ . For every $i \in I$ there's also a function $f_i : X_i \to Y$ defined. Now my notes say that: ""If the functions satisfy $f_i(x) = f_j(x)$ for every $x \in X_i \cap X_j$ where $i, j \in I$ and $i \neq j$ and $X_i \cap X_j \neq \emptyset$ then there's only one function $f : X \to Y$ such that $f(x) = f_i(x)$ for every $x \in X_i$ and every $i \in I$ ."" Giving it some thought, I came to a conclusion that this just means that there's exactly one function $f : X \to Y$ that extends all possible functions $f_i, i \in I$ The claim doesn't seem trivial at all, how would one go about proving that there's only one such function?","['elementary-set-theory', 'proof-writing']"
4565763,Calculate $\sum_{n=2}^{\infty}\left (n^2 \ln (1-\frac{1}{n^2})+1\right)$,"I am interested in evaluating $$\sum_{n=2}^{\infty}\left (n^2 \ln\left(1-\frac{1}{n^2}\right)+1\right)$$ I am given the solution for the question is $\,\ln (\pi)-\frac{3}{2}\,.$ $$\sum_{n=2}^{\infty}\left(n^2\ln\left(\!1\!-\!\frac{1}{n^2}\!\right)+1\right)=4\ln\left(\!\frac{3}{4}\!\right)+1+9\ln\left(\!\frac{8}{9}\!\right)+1+\ldots$$ Any tricks to solve it?","['calculus', 'sequences-and-series']"
4565775,"Is my proof that ""Two lines perpendicular to the same line are parallel"" correct?","My older sibling and his/her teacher said that my proof to the following theorem is wrong: Let lines $r$ and $s$ be perpendicular  to line $t$ . Prove that $r$ and $s$ are parallel. My proof: If lines $r$ and $s$ intersect, then they will form a triangle. The sum of the angles in a triangle is $180^\circ$ . But since $r$ and $s$ are perpendicular to the same line, all the angles around them are $90^\circ$ , and since the sum of any two angles that are formed by $r, s$ and $t$ is $180^\circ$ , a triangle couldn't be formed. This makes $r$ and $s$ parallel. Is there anything wrong about this proof? What is the fallacy?","['solution-verification', 'geometry']"
4565815,Chain rule in a Hilbert space.,"Let $F:H\to \mathbb{R}$ be some functional on a Hilbert space $H$ . Denote its Frechet derivative at $h\in H$ as $\frac{\delta F}{\delta h}(h)$ . Suppose $h_t$ is a curve in $H$ i.e $$h_\cdot : \mathbb{R}\to H,~~~~ t\mapsto h_t$$ how do I use the chain rule in this case to show that $$ \frac{\partial}{\partial t}F(h_t)=\Big\langle \frac{\delta F}{\delta h}(h_t) , \partial_t h_t \Big\rangle ~~~~~?$$","['hilbert-spaces', 'frechet-derivative', 'multivariable-calculus', 'calculus', 'chain-rule']"
4565841,Spivak Calculus Chapter 10 problem 19,"I have a question about Spivak's Calculus, chapter 10 problem 19. I feel I must be missing something very simple, but I don't know what it is. Here is the question: Prove that if $f^{n}(g(a))$ and $g^{n}(a)$ both exist, then $(f \circ g)^{n}(a)$ exists. A little experimentation should convince you that it is unwise to seek a formula for $(f \circ g)^{n}(a)$ . In order to prove that $(f \circ g)^{n}(a)$ exists you will therefore have to devise a reasonable assertion about $(f\circ g)^{n}(a)$ which can be proved by induction. Here is my conjecture. $\textbf{Conjecture}$ . If $g^{n}(a)$ and $f^{n}(g(a))$ both exist, then $(f\circ g)^{n}(a)$ exists and is a sum of products of the form: $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a)) $$ where $c$ is a constant, $m_{1}, \dots , m_{n}$ are natural numbers, and $k$ is a natural number such that $k\leq n$ . The base case for $n=1$ is straightforward. If $g{'}(a)$ exists and $f{'}(g(a))$ exists, then the Chain Rule tells us that $(f\circ g)^{'}(a)$ exists and is equal to $g{'}(a)f{'}(g(a))$ .  Thus, the conjecture holds for $n=1$ if we let $c=m_{1}=k=1$ . Next we assume: $\textbf{Inductive Hypothesis}$ If $g^{n}(a)$ and $f^{n}(g(a))$ exist, then $(f\circ g)^{n}(a)$ exists and is a sum of products of the form: $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a)) $$ We now want to show: If $g^{n+1}(a)$ exists and $f^{n+1}(g(a))$ exists, then $(f\circ g)^{n+1}(a)$ exists and is a sum of products of the form (where $k\leq n+1$ ): $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}[g^{n+1}(a)]^{m_{n+1}}f^{k}(g(a)) $$ I am confused about the answers that I have found in the solution book and on stack exchange here: Spivak Chapter 10, Exercise 19 solution verification. Prove that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists. . For the inductive step, the author shows that if $f^{n+1}(g(a))$ exists and $g^{n+1}(a)$ exist, then it follows that for all x in some interval around a, $f^{n}(g(x))$ and $g^{n}(x)$ both exist.  They then conclude from the Inductive Hypothesis that, in this interval, $(f\circ g)^{n}$ exists and is equal to a sum of products of the form: $$c[g{'}(x)]^{m_{1}}[g^{2}(x)]^{m_{2}} \dots [g^{n}(x)]^{m_{n}}f^{k}(g(x)) $$ Here the author is not talking about $(f\circ g)^{n}$ at a single point a. Instead they are talking about $(f \circ g)^{n}(x)$ at all points in an interval around $a$ . This the part that I don't understand. $\textbf{Why do we need to establish that $(f\circ g)^{n}(x)$ exists for all x in some interval around $a$?}$ Why can't we just say: If $g^{n+1}(a)$ exists, then $g^{n}(a)$ exists.  By the definition of $g^{n+1}$ , we have: $$g^{n+1}(a) = \lim_{h\to 0}\frac{g^{n}(a+h) - g^{n}(a)}{h}$$ So $g^{n}(a)$ must exist. Similarly, we know that $f^{n}(g(a))$ exists since: $$f^{n+1}(g(a)) = \lim_{h\to 0}\frac{f^{n}(g(a) + h) - f^{n}(g(a))}{h}$$ So by the Inductive Hypothesis, it follows that $(f\circ g)^{n}(a)$ exists and is a sum of terms of products of the form: $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a)) $$ It is then easy to use the product, chain, and sum rules for the derivative to show that $(f\circ g)^{n+1}(a) is a sum of products that all have the required form.","['derivatives', 'analysis']"
4565847,Is there a formal system of expressing the context for mathematical proofs/problems,"Is there a formal way we can explain ideas in a mathematical problem/proof that are contextual and to the issue at hand? For example if I have a mechanics problem I may want to be specific that the symbol $t$ is a variable, the symbol $m$ is a parameter that allows me to explore different 'contexts' and the symbol $u_0$ is a constant such that $u_0=10$ ? If I have a proof that I want to show that for any real number it's additive inverse is gained by subtracting it from zero. So for any real number $x$ , in this case $x$ is a variable varying over all reals. $0-x=-x$ Is there any formal system or name for these 'scenarios' that allow us to speak about them more concretely For example we could see a 'physical formula' as a statement that can be true in each 'scenario', if we could formulate it formally. Formulas have an often limited context given and we just infer the relationship described will hold in many cases.","['logic', 'notation', 'definition', 'algebra-precalculus', 'terminology']"
4565899,Hypersimplex coefficients,"The $(n,k)$ -hypersimplex is the intersection of the unit hypercube with the hyperplane $\sum_i x_i = k$ . It contains all the points in $\text{conv}\{ 1_S: S\subset[n], |S|=k \}$ i.e., the convex hull of all binary $n$ -dimensional vectors with $k$ nonzero entries. My question is, for any given hypercube point $\mathbf{x} \in [0,1]^n$ (whose coordinates sum to $k$ ), how can we express the coordinates of that point in terms of the corners of a $(n,k)$ -hypersimplex (for arbitrarily chosen $k$ )? In other words, suppose I fix $k$ to some number, $\mathbf{x}$ can be written as a convex combination of $\binom{n}{k}$ binary vectors, each having $k$ ones. I am interested in ways of finding such convex combinations (their coefficients). In the case of the simplex, there are known ways of expressing any hypercube point in barycentric coordinates. Is there something similar for the hypersimplex? My first thought is to stack all $\binom{n}{k}$ indicator vectors on a matrix $\mathbf{A} \in \{0,1\}^{n \times \binom{n}{k}}$ and find the coordinates of $\mathbf{y}\in [0,1]^{ \binom{n}{k}}$ such that $\mathbf{Ay} = \mathbf{x}$ and $\sum_{i}y_i=1$ . I suppose this could be done with linear programming but it seems kind of expensive in the sense that the linear program will end up having a ton of constraints.
Computationally, maybe I could just randomly sample a bunch of them until I get a matrix of sufficiently high rank and then go on to formulate a linear program? Are there known constructions where the coefficients can be found in a more clever way (either analytically or computationally) without having to deal with all $\binom{n}{k}$ binary vectors?","['coordinate-systems', 'polytopes', 'combinatorics']"
4565900,"Semantics of the mean-value formulas for Laplace's equation in Evan's Partial Differential Equations: Do we need to have $\overline{B(x,r})\subset U$?","Let $U \subset \mathbb{R}^n$ be an open set and suppose that $u$ is harmonic on $U$ . The theorem 2 of Evan's Partial Differential Equations 2nd edition makes a connection between the pointwise and average behaviour of harmonic functions on $U$ . In the theorem itself the closure of the ball in question need not be inside of $U$ . Namely, (Mean-value formulas for the Laplace's equation) If $u \in C^2(U)$ is harmonic, then $$u(x) = \frac{\int_{\partial B(x, r)}udS}{|\partial B(x, r)|} = \frac{\int_{B(x, r)}u dy}{| B(x, r)|}$$ for each ball $B(x, r) \subset U$ . What I am wondering is that if $x \in U$ and $r > 0$ such that $B(x, r)\subset U$ , do we then also need to have that $\overline{B(x, r)}\subset U$ for us to be able so use the said formula? I am wondering this since do we not have some general PDE solutions which are harmonic in a punctured space/set, so that you could take a suitable open set whose boundary crosses such a punctured point? Or could we have a harmonic function which is positive on $U$ but vanishes on $\partial U$ ? Consequently, if we are not interested in the surface integral but only on the averages over the balls, is it then okay if $\overline{B(x, r)}\not\subset U$ , or more concisely $\partial B(x, r)\not\subset U$ ?","['harmonic-analysis', 'multivariable-calculus', 'harmonic-functions', 'partial-differential-equations']"
4565915,Solving $\frac{1}{x} + \frac{1}{1-x} > 0$. Where's my error?,"For which $x$ is the following true? $$\frac{1}{x} + \frac{1}{1-x} > 0$$ I’ve worked through the question a couple ways and got the correct answer, but the initial approach has a flaw and I’m not sure where in which step it is. I got the correct answer by adding the two numbers to get $\frac{1}{x-x^2} > 0$ and going from there. However my first attempt went like: $$\begin{align}\frac{1}{x} &> - \frac{1}{1-x} \tag1\\[4pt]
\Rightarrow \qquad \frac{1-x}{x} &> -1 \tag2\\[4pt]
\Rightarrow \qquad 1-x &> -x \tag3 \\[4pt]
\Rightarrow \qquad x-1 &< x \tag4 
\end{align}$$ For which the answer is all $x$ . However, the correct answer, which is clear from the first approach is $0 < x < 1$ . I’m new to working with inequalities and I figure there is just a simple property of them I’m being ignorant about which means one of my rearrangements isn’t true. All help appreciated. Thanks.","['algebra-precalculus', 'inequality']"
4565922,How can we relate the mean of a function to the mean of some corresponding PDF?,"Suppose $f:[a, b]\rightarrow\mathbb{R}$ is a real-valued function on a compact interval. For the sake of simplicity, let us also assume $f$ is continuously differentiable for now. The mean of $f$ is known to be \begin{align}\tag{1}
\mu = \frac{1}{b-a}\int_{a}^{b} f(x)\, dx. 
\end{align} What I am wondering is, how can we relate this to the mean of a PDF? The discrete analog of this question is easy enough. Given $\mu = (x_{1} + \cdots + x_{n})/n$ we can relate this to $\mu = \sum xp(x)$ by taking $p(x)$ to be the fraction of times $x$ appears in $x_{1}, \ldots, x_{n}$ . However, it doesn't seem as simple when we are dealing with continuous functions. Problem. To formulate my question more precisely, I'll state state my question in the form of a math problem. Given $f(x)$ can we find a probability density $p(y)$ such that if we choose $x\in [a, b]$ at uniform random (and apply $f$ ) then the probability density of obtaining $y$ is $p(y)$ ? In particular, given such a $p$ we should have \begin{align}\tag{2}
\mu = \int_{-\infty}^{\infty} yp(y) \, dy. 
\end{align} Example 1. Suppose $f:[a, b]\rightarrow\mathbb{R}$ is a constant function $f(x) = c$ . Then the corresponding probability density has to be $p(y) = \delta(y-c)$ where $\delta$ is the Dirac delta function. Example 2. If $f(x) = A + \frac{x-a}{b-a}(B-A)$ then the corresponding probability density has to be $$
p(y) = \begin{cases}
\frac{1}{B-A} &\text{ if } y\in[A, B], \\
0 &\text{ otherwise.}
\end{cases} 
$$ Approach 1. My first idea was to divide the codomain of $f$ into discrete intervals, writing $$ \mathbb{R} = \bigcup_{k} \,[\tfrac{k}{n}, \tfrac{k+1}{n}]. $$ Then define $$ p(y) = N \int_{a}^{b} I(\tfrac{\lfloor ny \rfloor}{n}\le f(x)\le \tfrac{\lfloor ny \rfloor + 1}{n}) \, dx $$ where $N$ is a normalization constant.
Here $I(\cdots)$ is the indicator function that is $1$ if and only if the condition in the parentheses is satisfied, and $0$ otherwise.
I imagine we obtain the desired PDF by sending $n\rightarrow\infty$ . Approach 2. Given the framing of my problem, $x$ has a uniform PDF $\lambda(x)$ on $[a, b]$ . It seems to be that $y= f(x)$ is a transformation of variables. Assuming $f$ is strictly increasing or strictly decreasing, the change of variables formula for PDFs gives us $$ p(y) = \lambda(f^{-1}(y)) \cdot |(f^{-1}(y))'| = \lambda(f^{-1}(y)) \frac{1}{|f'(f^{-1}(y))|}. $$ This makes sense because the steeper $f(x)$ is near output $y$ , the smaller probability density there is for getting $y$ . Unfortunately, this approach only seems to work when $f$ is strictly increasing or decreasing. I am wondering how we could incorporate the case where $f$ is constant like in the example above. My question is, is there a general way of approaching this that handles all examples? In particular, can we do this if we drop the condition that $f$ is injective (in Approach 2). What if we drop the condition that $f$ is continuously differentiable? Is this problem well-known or studied? It seems surprising I can't find anything immediately pertaining to this, because it seems like very a natural question to ask what is the relationship between $(1)$ and $(2)$ .","['real-analysis', 'expected-value', 'average', 'probability', 'density-function']"
4565934,"If $12$ distinct balls are distributed to $8$ numbered cells, what is the probability that there is no empty cell?","I am trying to solve this question: Let us assume we are distributing 12 different balls between 8 numbered cells, so that each distribution of balls into cells is obtained with equal probability. What is the probability that there is no empty cell? At first I gave to each ball an unique ID, such that: 1 = ball number 1, 2 = ball number 2, and so on. Now i defined $\Omega =\left\{ \left( x_{1},\ldots ,x_{12}\right) | \forall i,x_{i}\in \left[ 8\right] \right\} =\left[ 8\right] ^{12}$ , and so the event we want to compute is: $A=\{ \left( x_{1},\ldots ,x_{12}\right) \in \Omega | \forall i\in \left[ 8\right] \exists j\in \left[ 12\right] ,x_{j}=i \}$ . At first I tried to work with $A^{c}$ , but then I noticed that I have duplicates, so I tried to compute $|A|$ directly. I said, in order for no cell to be empty, we will choose 8 balls out of the 12 and distribute them into the eight cells, so that each cell contains exactly one ball. Then, we will distribute the remaining four balls into the eight cells and finish. So let's do the math: Choose 8 unique balls out of 12 is $\begin{pmatrix} 12 \\ 8 \end{pmatrix}$ Distribute the balls to the cells is $8!$ Then, distribute the remaining four is $8^{4}$ So, overall we have: $|A| = \begin{pmatrix} 12 \\ 8 \end{pmatrix} \cdot 8! \cdot 8^{4}$ Now, $|\Omega| = 8^{12}$ , so finally we get: $\mathbb{P}(A) = \dfrac{\begin{pmatrix} 12 \\ 8 \end{pmatrix}\cdot 8!\cdot 8^{4}}{8^{12}} = 1.18961 >1$ . I don't understand what I'm doing wrong, so would glad for help.","['combinatorics', 'probability']"
4565998,How is $F(FM)$ related to 2-Jet bundle $F^{2} M$?,"I get that the bundle of invertible 2-jets $F^{2}M$ over a manifold $M$ holds second order derivative information about smooth functions in $M$ . The Wikipedia article speaks of a close relation between the double tangent bundle $TTM$ and $F^{2}M$ .
I'm looking at a principal frame  bundle, over another principal frame bundle over a Riemannian manifold (denoted $F(FM)$ ). Given a manifold $M$ , We examine it's (tangent) frame bundle $FM$ . This is known to be in bijection with the bundle of invertible 1-jets $F^{1}(M)$ on $M$ . Considering the total space $FM$ , let us now examine it's frame bundle $F(FM)$ . Similarly, we have a bijection between $F^{1}\left(FM\right)$ and $F(FM)$ . But(as I understand it) this is just the 1-jet prolongation of $FM$ , namely $J^{1}FM$ . In principal bundle structure on jet prolongation of frame bundles page 1288, it's stated that $J^{1}FM$ is diffeomorphic to $\tilde{F}^{2}\left(M\right)$ ,the bundle of semi-holonomic 2-jets over $M$ . Further there is a bundle reduction to the holonomic 2-jet bundle such that: $$F^{2}(M)\rightarrow J^{1}FM$$ So I would expect that the frame bundle of the frame bundle $F(FM)$ of $M$ is diffeomorphic to the semi-holonomic bundle of 2-jets over $M$ , $\tilde{F}^{2}(M)$ . is this right or am I misunderstanding something? I would be satisfied if anyone can explain the relationship between the two bundles in the title. Example: Without getting into individual frames and coordinate patches, suppose I have some n-dimensional Riemannian manifold $M$ . $M$ will accordingly have a principle $GL(n)$ frame bundle $FM$ over $M$ that locally looks like $FM\approx\mathbb{\mathbb{R}}^{n}\times gl(n)$ . $FM$ is then a Riemannian manifold of dimension $n+n^{2}$ . Now if we take it's frame bundle we have a principle $GL(n+n^{2})$ frame bundle $FFM$ over $FM$ . Without getting into the Homotopy principle too much, we can say this $GL(n+n^{2})$ bundle is the bundle of aholonomic frames over $FM$ . We could also say it's a $GL(n+n^{2})\rtimes GL(n)$ bundle over $M$ . Does this coincide with the (aholonomic) second order frame bundle $F^{2}$ of $M$ ? Some of our frame components here are first and second order derivatives of our original frame components on $M$ (this is because connections on $M$ correspond to frames on $gl(n)$ )  When we start enforcing those relations, we move to semi holonomic (by enforcing the first order derivative conditions) and finally holonomic (by enforcing second order derivative conditions) frames. As We do this we will  have a reduction of the $GL(n+n^{2})$ to some subgroup. This is right along the lines of what is done in this paper but there they're using Jet bundles and calling them higher order frame bundles $F^{2}M$ . How are they related to what I'm looking at?
I apologize for any sloppiness, I'm new and trying to learn this, it is admittedly out if my main area of study.","['vector-bundles', 'riemannian-geometry', 'differential-geometry']"
4566007,Probability Theory by Klenke theorem 15.9. That a finite measure is characterized by its characteristic function. [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question The theorem 15.9 (Characteristic function) says: A finite measure $\mu \in \mathcal{M}_{f}(\mathbb{R}^{d})$ (here this symbol just means finite measures on $\mathbb{R}^{d}$ ) is characterized by its characteristic function. Proof: Let $\mu_{1}, \mu_{2} \in \mathcal{M}_{f}(\mathbb{R}^{d})$ with $\psi_{\mu_{1}}(t) = \psi_{\mu_{2}}(t)$ for all $t \in \mathbb{R}^{d}$ . By Theorem 13.11(ii), $C_{c}(\mathbb{R}^{d})$ is a separating class for $\mathcal{M}_{f}(\mathbb{R}^{d})$ . Hence it is enough to show that $\int f d\mu_{1} = \int f d\mu_{2}$ for all $f \in C_{c}(\mathbb{R}^{d})$ . Let $f: \mathbb{R}^{d} \xrightarrow{} \mathbb{R}$ be continuous with compact support and let $\epsilon > 0$ . Assume that $K > 0$ is large enough such that $f(x) = 0$ for $x \notin (-K/2, K/2)^{d}$ and such that $\mu_{i}(\mathbb{R}^{d} - (-K, K)^{d}) < \epsilon, i = 1, 2$ . Consider the torus $E := \mathbb{R}^{d} / (2K\mathbb{Z}^{d})$ and define $\tilde{f} : E \xrightarrow{} \mathbb{R}$ by $\tilde{f}(x + 2K\mathbb{Z}^{d}) = f(x)$ for $x \in [-K, K)^{d}$ . Since the support of $f$ is contained in $(-K, K)^{d}$ , $\tilde{f}$ is continuous. For $m \in \mathbb{Z}^{d}$ define $g_{m}: \mathbb{R}^{d} \xrightarrow{} \mathbb{C}, x \mapsto \text{exp}(i \langle \pi m / K, x \rangle)$ . Let $\mathcal{C}$ be the algebra of finite linear combinations of the $g_{m}$ . For $g \in \mathcal{C}$ , we have $g(x) = g(x + 2Kn)$ for all $x \in \mathbb{R}^{d}$ and $n \in \mathbb{Z}^{d}$ . Hence, the map $\tilde{g} : E \xrightarrow{} \mathbb{C}, \tilde{g}(x + 2K \mathbb{Z}^{d}) = g(x)$ is well-defined, continuous and bounded. Furthermore, $\tilde{\mathcal{C}} := \{\tilde{g} : g \in \mathcal{C}\} \subset C_{b}(E; \mathbb{C})$ is an algebra that separates points and is closed under complex conjugation. As $E$ is compact, by the Stone-Weierstrass theorem, there is a $g \in \mathcal{C}$ such that $||\tilde{g} - \tilde{f} ||_{\infty} < \epsilon$ . We infer $||(f-g) \chi_{[-K, K]^{d}} ||_{\infty} < \epsilon$ and $||(f-g)\chi_{\mathbb{R}^{d} - [-K, K]^{d}} ||_{\infty} \leq ||g||_{\infty} = ||\tilde{g}||_{\infty} \leq ||\tilde{f}||_{\infty} + \epsilon = ||f||_{\infty} + \epsilon$ . By assumption of the theorem, $\int g d \mu_{1} = \int g d \mu_{2}$ . Hence, using the triangle inequality, we conclude $|\int f d \mu_{1} - \int f d \mu_{2}| \leq \int |f - g| d \mu_{1} + \int |f - g| d \mu_{2} \leq \epsilon(2 ||f ||_{\infty} + 2\epsilon + \mu_{1}(\mathbb{R}^{d}) + \mu_{2}(\mathbb{R}^{d}))$ . As $\epsilon$ is arbitrary, the integrals coincide. Why is this $\tilde{f}$ well defined? That is, if $x, y$ are from the same left coset, i.e., $x + 2K\mathbb{Z}^{d} = y + 2K\mathbb{Z}^{d}$ , how to see $f(x) = f(y)$ ? I found here https://proofwiki.org/wiki/Elements_in_Same_Left_Coset_iff_Product_with_Inverse_in_Subgroup#:~:text=Let%20x%2Cy%E2%88%88G,x%E2%88%921y%E2%88%88H . that $x, y$ are in the same left coset of $H$ iff $x^{-1}y$ is in the $H$ . For any $z$ outside $[-K, K)^{d}$ , $z$ is equivalent to some $x \in [-K, K)^{d}$ , with $z + 2K\mathbb{Z}^{d} = x + 2K\mathbb{Z}^{d}$ and so by definition $\tilde{f}(z + 2K\mathbb{Z}^{d}) = \tilde{f}(x + 2K\mathbb{Z}^{d}) = f(x)$ . So to see it is well-defined it's enough to consider two points $x, y \in [-K, K)^{d}$ . If $x + 2K\mathbb{Z}^{d} = y + 2K\mathbb{Z}^{d}$ , then $y - x = 2Km$ , for some $m \in \mathbb{Z}^{d}$ . And recall that $\tilde{f}(x + 2K\mathbb{Z}^{d})$ is defined only for $x \in [-K, K)^{d}$ , and for any two vectors from $[-K, K)^{d}$ , their difference in each coordinate is less than $2K$ , so such $m$ must be zero, meaning $x = y$ . My question: What does it mean for $\tilde{f}$ to be continuous? Why does $f$ 's being supported within $(-K, K)^{d}$ imply $\tilde{f}$ is continuous? Here is the definition of the quotient space: https://en.wikipedia.org/wiki/Quotient_space_(topology) . Given a topological space $X$ , the quotient space under ~ is the quotient set $Y$ equipped with the quotient topology, which is the topology whose open sets are the subsets $U \subset Y = X / \text{~}$ such that $\{x \in X: [x] \in U\}$ is an open subset of $X$ ; that is, $U \subset X / \text{~}$ iff $\{x \in X: [x] \in U\} \in \tau_{X}$ . Now $Y = \{x + 2K\mathbb{Z}^{d}: x \in [-K, K)^{d}\}$ so any subset $U$ of $Y$ has the form $U = \{x_{i} + 2K\mathbb{Z}^{d}\}_{i \in I}$ . If such $U$ is open in the quotient topology, this means $\{x_{i}\}_{i \in I}$ is open in $X$ . For any open interval of the form $A = (a, b) \in \mathbb{R}$ , we want to show $(\tilde{f})^{-1}(A)$ is open in the quotient topology. Since $f$ is continuous, $f^{-1}(A)$ is open in $\mathbb{R}^{d}$ . What next? (I don't know if this works: Denote $A = \{x_{j}\}_{j \in J}$ . Then $\{[x_{j}]\}_{j \in J}$ is open in the quotient topology. Since for any $z \in \mathbb{R}^{d}$ , $z$ is in the same equivalence class as $z'$ for some $z' \in [-K, K)^{d}$ , we can rewrite $\{[x_{j}]\}_{j \in J} = \{[x_{t}]\}_{t \in T}$ where each $x_{t} \in [-K, K)^{d}$ . By the definition of quotient topology, that $\{[x_{j}]\}_{j \in J} = \{[x_{t}]\}_{t \in T}$ is open implies $\{x_{t}\}_{t \in T}$ is open. ) This theorem is really scary to me (I learned measure theory mainly from the first five chapters from Donald Cohn's book, and I really like his style where every step is explained clearly. So when the author just omits something when they could have used some more words to explain a bit further it causes difficulty to my understanding. Maybe the details missing here are obvious to you but I really can't understand)... Appreciate any help!","['characteristic-functions', 'probability-theory', 'probability', 'quotient-spaces']"
4566009,Help With Creating An Algorithm For Finding The Area Of A Polygon.,"Disclaimer: This is not homework, it is not for a class, and it is not for any kind of test. I'm trying to improve my skills with creating algorithms to solve word problems. I've been banging my head on this one and in the spirit of learning I would like help with a solution. Please explain your thought process in coming up with a solution.  I would like to discover what thoughts I am NOT having so that I can hopefully use that observation to improve my thought process. Problem: The side of each square is one. The area of each square is one. Come up with a formula where given n , find the area of a polygon n can be any integer Picture of the polygons. n area thoughts about the formula 1 1 each side is one, area is one 2 5 n -1 = 1.  1 square = 4 sides, each can attach a square, 4 squares, area = 4 + 1 3 13 n-1 = 2, 2 squares = 8 sides, each can attach a square, 8 new squares + 5 old squares, area = 13 4 25 n-1 = 3, 3 squares  = 12 sides, each can attach a square, 12 new squares + 13 old squares, area = 25 The part that is driving me nuts is how given just n , to come up with the number of preexisting squares to add to the area generated by (n-1) x 4. My intuition is that I may not be supposed to do that, but I can't think of how else to get the area of each progressively larger polygon given just n . Any clues would be greatly appreciated.",['geometry']
4566017,Strengthening Ax-Grothendieck,"I have cross-posted this on Math Overflow: https://mathoverflow.net/questions/433685/strengthening-ax-grothendieck The question is simple. The Ax-Grothendieck theorem says a polynomial map $p\colon\mathbb C^n\to\mathbb C^n$ that is injective is also surjective. Is assuming $p$ has finite fibers enough? I can't come up with any easy counter-examples, but the proof I know using finite fields does not work.","['complex-geometry', 'algebraic-geometry', 'polynomials']"
4566024,"How would i change my answers into a ""sensible set theory notation and formulae""? or are my answers fine to this question?","In a Sixth Form year there are 38 students, each taking at least one of
the subjects of Maths, English, or Science. You know the following
about the number of students enrolled on each of the subjects: 32 students are taking Maths; 26 students are taking English; 31 students are taking Science; 23 students are taking both Maths and English; 27 students are taking both Maths and Science; 24 students are taking both English and Science. With the information above and
using sensible Set Theory notation and formulae, determine: 1.Whether there are there any students who are taking both Maths and English, but who are not taking Science?
2.How many students are taking both Maths and Science, who are not also taking English?
3.How many students are taking only one subject? You may find a Venn diagram indicating the sizes of the sets to be
useful. Here are the answers I got for the following questions: NONE 4 10 Link to my Venn diagram on how I got my answers: M = Math,
E = English,
and S = Science So is this how the answer should look? Or is the question asking for a different kind of answer?",['elementary-set-theory']
4566035,Solving and plotting the 2-D Lorenz Equation,"We are asked to first solve and then plot the phase plane of \begin{align}
\begin{cases}
\dot{x}=\sigma x - \sigma y\\
\dot{y} = \rho x-y
\end{cases}, \ \sigma, \ \rho >0.
\end{align} Now the textbook way of going at this is to derive the first line, replace in second line and as such remove one variable. Doing this we get \begin{align}
\ddot{x} = \sigma \dot{x}-\sigma \dot{y}  = \sigma \dot{x}-\sigma \rho x+\sigma y = \sigma \dot{x} - \sigma \rho x + \sigma x-\dot{x}\\
\implies \ddot{x}+(1-\sigma)\dot{x}+\sigma (\rho-1)x=0\\
\implies P(\lambda)=\lambda^2+(1-\sigma)\lambda+\sigma(\rho-1)=0\\
\implies \Delta=\sigma^2+\sigma(2-4\rho)+1.
\end{align} At this point we have two variables $\sigma, \rho$ . How exactly are we supposed to continue? Take a painstakingly $16$ (!!!) numbers of cases? I am sure there is a faster way for this, please someone enlighten me. Thank you.","['nonlinear-dynamics', 'analysis', 'ordinary-differential-equations', 'dynamical-systems']"
4566074,Solve the equation $\sqrt{x^2+x+1}+\sqrt{x^2+\frac{3x}{4}}=\sqrt{4x^2+3x}$,"Solve the equation $$\sqrt{x^2+x+1}+\sqrt{x^2+\dfrac{3x}{4}}=\sqrt{4x^2+3x}$$ The domain is $$x^2+\dfrac{3x}{4}\ge0,4x^2+3x\ge0$$ as $x^2+x+1>0$ for every $x$ . Let's raise both sides to the power of 2: $$x^2+x+1+x^2+\dfrac{3x}{4}+2\sqrt{(x^2+x+1)\left(x^2+\dfrac{3x}{4}\right)}=4x^2+3x\\2\sqrt{(x^2+x+1)\left(x^2+\dfrac{3x}{4}\right)}=2x^2+\dfrac{5x}{4}$$ Let's raise both sides to the power of 2 again but this time the roots should also satisfy $A:2x^2+\dfrac54x\ge0$ : $$4(x^2+x+1)\left(x^2+\dfrac{3x}{4}\right)=(2x^2+\dfrac54x)^2$$ I came at $$x(2x^2+\dfrac{87}{16}x+3)=0$$ I obviously made a mistake as the answer is $x=-4$ , but is there an easier approach?",['algebra-precalculus']
4566088,Need help in linearizing a non-linear system of ODE,"We have the system \begin{align}
\begin{cases}
\dot{x} = (1-x)^2-y^2\\
\dot{y} = \epsilon^{xy}-1
\end{cases}
\end{align} and I must plot the phase graph. To do this I already know that I must linearize in the first order, since for Taylor degree $\geq2 \ $ , $\Delta x \to 0$ . However I am stuck and cannot even start. I have in my mind that I must be able to set the system in such a way such that \begin{align}
\begin{cases}
y=\text{something}\\
\dot{y}= \text{other something}
\end{cases}
\end{align} so that then I can run \begin{align}
F(x+h)=F(x_0)+h\cdot \dfrac{dF}{dx}|_{x=x_0}.
\end{align} The critical points of the starting system are $(0,1), (0,-1), (1,0)$ .","['linearization', 'derivatives', 'ordinary-differential-equations', 'dynamical-systems']"
4566145,Homeomorphic infinite products of non-homeomorphic spaces,"Let $X$ be be a set. Do there exist non-homeomorphic topologies $(X,\tau_1)$ and $(X, \tau_2)$ be on $X$ such that $$\prod_{i\in\mathbb{N}}(X, \tau_1)\cong \prod_{i\in\mathbb{N}}(X, \tau_2).$$ I ask because I recently learned that the infinite product of a discrete space with itself isn't necessarily discrete, and this led me to wondering what interesting things happen with infinite products of topological spaces. Further, I'm mostly interested in cases where $X$ is finite, but any examples work.",['general-topology']
4566198,Is there an analysis book with a proper introduction to mathematical proofs?,"I am taking my first analysis course in university. Since I am studying in Germany, there is no real distinction between calculus and real analysis, meaning this is the first university course on anything analysis that can be taken here. Therefore, I am new to the concepts of mathematical proofs. However, they are regularly used in the course and I often have problems following them. Since I am taking classes on mathematical principles and logic as well, my initial idea was to buy a dedicated textbook on proofs, e.g. Hammack's Book of Proof , Velleman's How to Prove It or Chartrand's Mathematical Proofs: A Transition to Advanced Mathematics , which I could use for analysis as well as the other classes. However, some answers to this question imply that this might not be the best way to go and I should rather buy a book on a certain mathematical field, in my case analysis, that contains a rigorous introduction to proofs in itself and applies them. Additionally, since time is limited, I am not quite sure whether it makes sense to focus on a book on mathematical proofs (which sometimes have several hundred pages) rather than the course itself. Since I am missing an introductory text to analysis anyway, I thought about following the recommendations in the answers and getting a book that teaches introductory analysis with a focus on formal notation as well as writing and understanding mathematical proofs. From what I have read so far, Abbott's Understanding Analysis and Tao's Analysis I + II might be great options for that purpose, however, one answer on the question I was referring to earlier suggests there might be a specific book which is specifically designed that way. Should I get a separate book on mathematical proofs or would I be better off with one of the analysis books I mentioned (or a completely different one)?","['proof-writing', 'book-recommendation', 'reference-request', 'real-analysis', 'calculus']"
4566203,How to create a curved triangle of specific dimensions,"I'm trying to make a triangle where all the sides are curved outward, like a Reuleaux triangle, but the sides must be of a specific length. The length must be measured along the curve , not from end to end in a straight line, and the shape must be a specific vertical height. Is there a way to do this, including using any software or websites? I've included a picture to show what I'm trying to do. The dimensions shown in the image are all I have to go off of. Thank you!",['geometry']
4566217,Generalization of mean and median,"It is well known that a median of a distribution $\mu$ can be defined as an $m$ such that $$m\in\operatorname*{arg\,min}_{c\in\mathbb{R}}\mathbb{E}_{X\sim\mu}[|X-c|].$$ Similarly, the mean of a distribution $\mu$ is defined as an $m$ such that $$m=\operatorname*{arg\,min}_{c\in\mathbb{R}}\mathbb{E}_{X\sim\mu}[(X-c)^2].$$ I am interested in whether there is any reference or literature on the generalization of this to higher powers $p$ - in particular, what can be said about $m$ such that $$m\in\operatorname*{arg\,min}_{c\in\mathbb{R}}\mathbb{E}_{X\sim\mu}[|X-c|^p]$$ for $p>2?$ Edit: Coming back to this I believe the last line can be written as $$m=\operatorname*{arg\,min}_{c\in\mathbb{R}}\mathbb{E}_{X\sim\mu}[|X-c|^p],$$ that is there is a unique minimizer (this is because for $p\in (1,\infty)$ , $|\cdot|^p$ is strictly convex).","['central-tendency', 'means', 'probability-theory', 'median']"
4566238,Can you square / cube the value of pi and use the degrees radians?,"I was working on a question in the context of integration. The question had a step where: $-2 \cos(\pi^3) - (-2\cos(0))$ which simplifies and was left as: $ -2(\cos(\pi^3) -1)$ or $2-2\cos(\pi^3)$ If is $\pi$ equal to 180 degrees radians: Can you cube $\pi$ so that it would be $5832000$ degrees radians, and use it to simplify the $\cos(\pi^3)$ further? Then, is it correct that $\cos(180^3) = 1$ ? So, the final answer is $0$ ?","['integration', 'trigonometry', 'pi']"
4566265,Derivative of square root of unitary matrix,"Given a (special) unitary matrix $U(x) \in SU(2)$ which is a function of $x\in\mathbb{R}$ , along with its unitary square root $u(x) = \sqrt{U(x)}$ such that $u(x)^2 = U(x)$ and $u(x)^\dagger u(x) = \mathbb{1}_{2\times2}$ , what is the derivative $$\frac{d}{dx}u(x)=\frac{d}{dx}\sqrt{U(x)}?$$ Can it be expressed purely in terms of products and sums of $u(x)$ , $u(x)^\dagger$ , $U(x)$ , $U(x)^\dagger$ , $\frac{d}{dx}U(x)$ and $\frac{d}{dx}U(x)^\dagger$ ? Naively, it seems like it should be something like (suppressing arguments) $$\frac{du}{dx} = \frac{1}{2}\left(\frac{1}{2} u^\dagger \frac{dU}{dx} + \frac{1}{2}\frac{dU}{dx}u^\dagger \right),$$ following from a ""symmetrized"" form of the regular chain rule $\tfrac{d}{dx}\left[f(x)\right]^{1/2}=\frac{1}{2} \left[f(x)\right]^{-1/2}f'(x)$ with the identification of $u\leftrightarrow \left[f(x)\right]^{1/2}$ and $u^\dagger = u^{-1} \leftrightarrow \left[ f(x)\right]^{-1/2}$ . Note: Here $U^\dagger$ is the conjugate transpose. Edit: I am actually interested in the quantity $$ u^\dagger \frac{du}{dx} + u \frac{du^\dagger}{dx}, $$ so even if there is not a way to write $\tfrac{du}{dx}$ simply, an expression for this would be sufficient.","['unitary-matrices', 'matrices', 'matrix-calculus', 'derivatives', 'lie-groups']"
4566308,Does every incomplete real inner product space admit a closed subspace of countable dimension?,"Suppose $X$ is an incomplete real inner product space. Does there exist a sequence $(y_n)_n \in X$ such that $Y = \operatorname{span}\{y_n\}_n$ is closed? This question cropped up as part of my research. It cannot be the case, by the Baire category theorem, that $Y$ is complete, hence admitting such a closed subspace would imply $X$ is incomplete, but is the converse true? My instinct is to take $(y_n)_n$ to be a non-convergent Cauchy sequence, and apply Gram-Schmidt (removing $0$ vectors as they come) to form orthonormal $(z_n)_n$ . This would produce a new basis for $Y$ whose coordinate maps are all continuous. So, if $(x_n)_n \in Y$ converges to $x \in X$ , then the coordinates $\langle x_n, z_i \rangle$ converge to $\langle x, z_i\rangle$ as $n \to \infty$ . If all but finitely many of these coordinates are $0$ , then $x \in Y$ . If not, then I'd like to find a way to parlay this into constructing a limit for $(y_n)_n$ , but I'm not sure how to do it. Does anyone have an idea? I don't even know if this result is true.","['inner-products', 'complete-spaces', 'functional-analysis']"
4566344,"Prove that $\{ (x, x): x\in G\}$ is a subgroup of $G\times G$, and is isomorphic to $G$.","Given the diagonal $H = \{(x, x): x\in G\}$ , show that
(i) $H$ is a subgroup of $G\times G = G^2$ and; (ii) $H \cong G$ My attempt: Proof.
(i) $G$ is a group, so an identity element $e\in G$ exists. By extension, $(e, e)\in H$ . Let $x = (a, a), y = (b, b)$ for any two elements $a, b \in G$ . Then $a, b \in H$ . $G$ contains an inverse element, so $a^{-1}, b^{-1} \in G$ . Then $x^{-1}, y^{-1}$ eixst on $H$ , too. Therefore, $$y^{-1} = (b^{-1}, b^{-1}) \\
\implies xy^{-1} = (ab^{-1}, ab^{-1})\in H$$ $H$ is a subgroup of $G^2$ . (ii) Let $\phi: H \rightarrow G$ be defined by $\phi(a, a) = b$ where $(a, a)\in H$ and $b\in G$ . If $\phi(a, a) = \phi (a', a')$ then necessarily we must have $a = a'$ . $\phi$ is injective. The mapping $\rho: G\rightarrow H $ determined by $\rho(b) = (a, a)$ is injective since $(a, a) = (b, b)$ if and only if $a = b$ . Hence, a bijection $H\rightarrow G$ exists.
Finally, consider the function $\mu(a) = (a, a)$ . Then \begin{align*}\mu(ab) &= 
 (ab, ab) \\ &= (a, a)(b, b) \\ &= \mu(a)\mu(b)\end{align*} gives us a homomorphism. Therefore, $H$ is isomorphic to $G$ . $\blacksquare$ Is there any flaw with my proof? Please let me know.","['group-homomorphism', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
4566357,"Putting $n$ balls into $n$ boxes, what is the expected value of the number of balls that are in the right boxes?","I have the following probability question, with two different solutions, and I would like to determine which one is right. Here is the problem: $n$ balls are labeled $1$ to $n$ , and $n$ boxes are also labeled $1$ to $n$ . Now put these $n$ balls randomly into these $n$ boxes, such that each box contains only one ball. If a ball is in the box with the same number it has (e.g. ball #3 is in box #3), then we say that this ball is in the right box. Let $X$ be the number of balls that are in the right boxes, what is the expected value of $X$ ? Here are the two solutions I have: Solution 1: The probability that at least $k$ balls are in the right box is $$P(X\ge k)=\frac{\binom{n}{k}(n-k)!}{n!}=\frac{1}{k!},\quad k=0,1,2,\dots,n,$$ so the expected value of $X$ is $$E(X)=\sum_{k=0}^n P(X\ge k)=\sum_{k=0}^n\frac{1}{k!}.$$ Solution 2: For $j=1,\dots,n$ , let $X_j$ be the random variable such that $$X_j=\begin{cases}1, &\text{if ball }\#j\text{ is in the right box},\\
0, &\text{otherwise},\end{cases}$$ Then we have $X=\sum_{j=1}^n X_j$ . For each $j=1,\dots,n$ , we have $$P(X_j=1)=\frac{(n-1)!}{n!}=\frac{1}{n},$$ therefore, $$E(X)=\sum_{j=1}^n E(X_j)=1.$$ From my instinct, I feel that the first solution is the right one, but I cannot tell what is wrong with the second solution (Maybe I am wrong?). Any suggestions would be greatly appreciated! Edit: Now I've figured out that the second one is right. The problem with the first one is that the relation $P(X\ge k)=\frac{\binom{n}{k}(n-k)!}{n!}$ is incorrect, and thus caused the wrong answer.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4566380,"Adding 0, multiplying by 1 and more...","Throughout my entire life studying mathematics, up until this point, one thing that I have noticed that is key to mathematical problem-solving is knowing when and how to manipulate certain terms. One such key technique is adding zero to any equation, or multiplying by one. I've seen applications of this in algebra, calculus, number theory and, in many cases, even geometry (where drawing auxiliary lines, as I tend to do, is very similar to this approach). Let's take a look at an easy example: Suppose we have the following cubic equation and we want to solve for every real and complex root of $x$ : $x^3+x^2+2x-4=0$ One way to solve it is to add $0$ , by adding and subtracting $8$ from this: $x^3+x^2+2x-4+8-8=0$ By rearranging the terms and doing some basic manipulations, we get: $(x^3-8)+(x^2+2x+4)=0$ $(x-2)(x^2+2x+4)+(x^2+2x+4)=0$ $(x-1)(x^2+2x+4)=0$ This can easily be solved now. As you can see, by merely adding $0$ , we managed to factor that seemingly challenging cubic. My question is, how do we ""know"" when and how to add zero, multiply by one, etc in these scenarios? Is it some sort of intuition that you need to build? Is there a specific method to do this? Please be sure to also include how your thought process works when doing something like this.","['algebra-precalculus', 'soft-question']"
4566423,"Solve for $n: \cos\left(\frac\pi4(n^2+2n)\right)=\sin\left(\frac\pi4(n^2+n+1)\right), n\in \mathbb Z$","Solve for $n: \cos\left(\frac\pi4(n^2+2n)\right)=\sin\left(\frac\pi4(n^2+n+1)\right), n\in  \mathbb Z$ My Attempt: $$\cos\left(\frac\pi4(n^2+2n)\right)=\cos\left(\frac\pi2-\frac\pi4(n^2+n+1)\right)\\\implies\frac\pi4(n^2+2n)=2p\pi\pm\left(\frac\pi2-\frac\pi4(n^2+n+1)\right), p\in  \mathbb Z$$ With minus sign, $$\frac\pi4(n^2+2n)=2p\pi-\left(\frac\pi2-\frac\pi4(n^2+n+1)\right)\\\implies n^2+2n=8p-(2-n^2-n-1)\\\implies n=8p-1$$ With plus sign, $$\frac\pi4(n^2+2n)=2p\pi+\left(\frac\pi2-\frac\pi4(n^2+n+1)\right)\\\implies n^2+2n=8p+(2-n^2-n-1)\\\implies 2n^2+3n=8p+1$$ How to conclude from this? The answer given is $8p-1$ or $8p-3$ .","['trigonometry', 'quadratics']"
4566452,Exercise of homogeneous function of two variables,"I am trying to solve the following exercise: ""Find all the class 2 functions $v=v(t)$ of the variable $t=x^2+y^2+z^2$ verifying the equation: $$\frac{\partial f}{\partial x}\left(f\frac{\partial v}{\partial x}\right)+\frac{\partial f}{\partial y}\left(f\frac{\partial v}{\partial y}\right)+\frac{\partial f}{\partial z}\left(f\frac{\partial v}{\partial z}\right)=0$$ where $f=f(x,y,z)$ is a differentiable, not identically zero and homogeneous of degree m function. I have applied the product rule, then the chain rule for the composition of functions and in the end Euler theorem for homogeneous functions. Finally I have obtained the equation: $m\frac{\partial v}{\partial t}+2\frac{\partial v^2}{\partial t^2}(x^2+y^2+z^2)=0$ . But the steps in which I have doubt are when I apply the chain rule. Would this be correct?: $$\frac{\partial v}{\partial x}=\frac{\partial v}{\partial t}\frac{\partial t}{\partial x}=\frac{\partial v}{\partial t}2x$$ .
Thanks for your help.","['real-analysis', 'multivariable-calculus', 'calculus', 'partial-derivative', 'derivatives']"
4566467,"What is the ""typical"" map between surfaces?","I've seen the concept of map between surfaces $S\to \overline{S}$ and at first, I was extremely confused because I spent a lot of time thinking on how to actually send points from $S$ to $\overline{S}$ ""directly"". I'll try to explain what I mean with the following example: Look at the map $f$ in this example: It is constructed by first inverting $x$ , making a change of coordinates and then computing $x^{*}$ so, this was not ""direct"", it took some ""detours"" in order to be made. Are all the maps we commonly use in differential geometry like this? Are there ways to make ""direct"" maps?",['differential-geometry']
4566505,"$ABCD$ is a square, point $E$ lies inside the square such that $BE=2$, $AE=6\sqrt{2}$, $CE=8$, calculate the area of $\triangle BEC$.","The question is as stated in the title, in the figure given below, find the area of $\triangle BEC$ . I must admit this was a challenging problem and the solution that I came up with (which will also be posted as an answer) is pretty complicated and ""messy"". So, I'd like to see if there are any better approaches that may be simpler as well.","['contest-math', 'euclidean-geometry', 'area', 'geometry', 'trigonometry']"
4566527,Finite signed measures: reconcile different types of convergence,"Let $X$ be a metric space, $\mathcal M(X)$ the space of all Borel signed measures on $X$ , $\mathcal C_b(X)$ be the space of real-valued continuous functions, $\mathcal C_0(X)$ the space of real-valued continuous functions that vanish at infinity, and $\mathcal C_c(X)$ the space of real-valued continuous functions with compact supports. I'm reading this answer about different types of convergence. Below is my understanding. Could you verify if it is fine? Notice that $\mathcal M (X)$ is a real Banach space with total variation norm. Also, $\mathcal C_b(X)$ and $\mathcal C_0(X)$ are real Banach space with supremum norm. Below we use the kind of topology induced by dual pairing . Because $$
\mathcal C_c(X) \subset \mathcal C_0(X) \subset \mathcal C_b(X),
$$ we have $$
\sigma(\mathcal M(X), \mathcal C_c(X)) \subset \sigma(\mathcal M(X), \mathcal C_0(X)) \subset \sigma(\mathcal M(X), \mathcal C_b (X)).
$$ $\sigma(\mathcal C_c(X),\mathcal M(X))$ and $\sigma(\mathcal C_0(X),\mathcal M(X))$ are the subspace topologies that $\sigma(\mathcal M(X), \mathcal C_b (X))$ induce on $\mathcal C_c(X), \mathcal C_0(X)$ respectively. The closure of $\mathcal C_c(X)$ in $\mathcal C_b(X)$ is $\mathcal C_0(X)$ , so $$
\sigma(\mathcal M(X), \mathcal C_c(X)) = \sigma(\mathcal M(X), \mathcal C_0(X)).
$$ Usually, $\sigma(\mathcal M(X), \mathcal C_b(X))$ and $\sigma(\mathcal M(X), \mathcal C_0(X))$ are called the weak and weak $^*$ topologies of $\mathcal M(X)$ respectively. We have $\mathcal M (X)$ can be isometrically embedded into the continuous dual $(\mathcal C_0 (X))^*$ of $\mathcal C_0 (X)$ , so $$
\sigma(\mathcal C_0 (X), \mathcal M(X)) \subset \sigma(\mathcal C_0 (X), (\mathcal C_0 (X))^*).
$$ If $X$ is locally compact, then $\mathcal{M}(X)$ is isometrically isomorphic to $(\mathcal C_0 (X))^*$ by Riesz–Markov–Kakutani theorem and thus $$
\sigma(\mathcal C_0 (X), \mathcal M(X)) = \sigma(\mathcal C_0 (X), (\mathcal C_0 (X))^*).
$$ $\sigma(\mathcal M(X), \mathcal C_0(X))$ and $\sigma((\mathcal C_0 (X))^*, \mathcal C_0(X))$ have exactly the same topological properties .","['measure-theory', 'weak-convergence', 'riesz-representation-theorem', 'borel-measures', 'metric-spaces']"
4566574,Confidence intervals for a population proportion: Why do we use a z-distribution (instead of a t-distribution) even who we estimate $\sigma_{\hat{p}}$,"When deriving a confidence interval for a population mean $\mu$ where we do not know the value of the population standard deviation $\sigma$ , we use the sample standard deviation $s$ to estimate $\sigma$ and so our margin of error ends up being related to a $t$ -distrubition: $E = t_c \frac{s}{\sqrt{n}}$ Now, when deriving a confidence interval for an unknown population proportion $p$ , we substitute $\hat{p}$ for $p$ into the formula for the standard deviation of $\hat{p}$ : $\sigma_{\hat{p}}=\sqrt{\frac{p(1-p)}{n}} \rightarrow \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ However, when we make this substitution, for some reason we still say that the margin of error on our resulting confidence interval depends on a $z$ -distribution!!! $E = z_c \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ I don't understand why this is. In both cases, we are using some type of sample standard deviation to estimate the population standard deviation. Why in the former case (for a population mean) do we use a $t$ distribution (which I feel is very appropriate), but in the case of a population proportion we still use a $z$ -distribution?? Insight appreciated. Here's me asking (what I view) to be the exact same question but in a different way: Does $\frac{\hat{p}-p}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}}$ ever have a t-distribution? It seems like it should since we are using $\hat{p}$ for $p$ in the formula for $\sigma_{\hat{p}}$ ; this closely resembles how we get to a t distribution for sample means (by substituting in $s$ for $\sigma$ ).","['statistics', 'soft-question']"
4566609,"What is $\lim_{t\rightarrow \infty }\int_{a}^{b}f(x,\sin(tx))dx$？ [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question Let f $\in C([a,b]\times [-1,1]) $ ,What is the value of this integral: $\lim_{t\rightarrow \infty }\int_{a}^{b}f(x,sin(tx))dx$ This is a topic I encountered when I was studying mathematical analysis recently, I want to know how to solve this problem, is there some theoretical background? First of all, f is a bounded continuous binary function, so we can get that this integral must also be bounded.Then I tried the commutation method so that tx=u, but I couldn't push the final result. This is the result of my appeal operation： $\lim_{t\rightarrow \infty }\frac{1}{t}\int_{at}^{bt}f(\frac{u}{t},sin(u))du$ I would appreciate it if you could solve it.","['integration', 'limits', 'calculus', 'analysis']"
4566629,Notation of recursive function with multiple arguments,"I want to formally describe an algorithm $f : E \times P \to P$ which you can use as an argument for itself. Right now, I have the following definition: \begin{equation}
\begin{split}
P_{e_{n+1}} &= f(e_n, P_{e_n}) \\
&= f(e_n, f(e_{n-1}, f(e_{n-2}, \dots f(e_1, P_{e_1}))))
\end{split}
\end{equation} The formula should show that you only need one initial $P_{e_1} \in P$ and a set of elements $e_{x} \in E$ to generate $P_{e_{n+1}}$ . Is there a name for such a function or a notation that is able to indicate such a characteristic?","['notation', 'functions', 'scientific-notation']"
4566646,The maximum number of intersections for a type of exponential function,"Given two real valued functions $f(x)$ and $g(x)$ which both satisfy the conditions that they are sums of exactly $k$ positive terms each, each term of the form $b_i^x$ where $b_i \in \mathbb{N}$ $(i\in \{1,...,k\})$ , can we come up with the maximum number $m$ of points in which $f(x)$ and $g(x)$ intersect (in general)? Consider $x\in R$ , $x\geq 0$ or even $x\in \mathbb{N}_{0}$ if it makes the question simpler. Examples: $f(x)=2^x+42^x, g(x)=3^x+4^x$ or $f(x)=30^x+20^x+10^x, g(x)=1^x+2^x+3^x$ . In both examples $f$ and $g$ at least intersect at $x=0$ since $f$ and $g$ both have $k$ terms. My guess was, at first, that it's exactly one point (being $x=0$ ), but I am not so sure anymore and now consider the answers $m=2$ or some $m(k)\in O(k)$ .
Is it important to know that the terms are convex to solve this problem? Edit 2: Since this is very interesting problem to me and I think it fits, can one also find out for what $c$ there cannot be any new intersection for all $x>c$ ? Edit: After trying to come up with examples on wolfram alpha that have more than 1 intersection I quickly found multiple examples, but I decided to stop after a while without being able to come up with an example where there are 3 or more intersections.","['functions', 'exponential-function', 'transcendental-equations']"
4566665,Prove that $\frac{\cos2\theta}{\sin6\theta}+\frac{\cos6\theta}{\sin18\theta}+\frac{\cos18\theta}{\sin54\theta}=\frac12(\cot2\theta-\cot54\theta)$,"Prove that $\frac{\cos2\theta}{\sin6\theta}+\frac{\cos6\theta}{\sin18\theta}+\frac{\cos18\theta}{\sin54\theta}=\frac12(\cot2\theta-\cot54\theta)$ I tried formulas for $\sin3\theta, \cos3\theta$ but couldn't conclude. I tried taking LCM on LHS and using formula for $2\sin A\cos B$ but couldn't conclude. I tried converting $\cot$ into $\sin, \cos$ , taking LCM, using $2\sin A\cos B$ but couldn't conclude.",['trigonometry']
4566668,"Using vectors to show that if the diagonals of a parallelogram have the same length, then show that the parallelogram is a rectangle.","Prove this statement by using vectors: If the diagonals of a parallelogram have the same length, then show that the parallelogram is a rectangle. I’ve tried using $\|x\|^2 = x \cdot x$ , where $\cdot$ is dot product, to show that the opposite sides are of equal length, but this just shows that the sides are facing the opposite direction, even though I set the sides facing the same direction. So I set the two diagonals as x⃗ and y ⃗. $\vec{a}$ and $\vec{c}$ are the opposite sides, $\vec{b}$ and $\vec{d}$ are the other opposite sides. x⃗ = - $\vec{a}$ + $\vec{d}$ = $\vec{b}$ - $\vec{c}$ y = $\vec{d}$ + $\vec{c}$ = $\vec{a}$ + $\vec{b}$ The length of the diagonals are the same. There are several ways I did this and this is one of them. $\|\vec{x}\| = \|\vec{y}\|$ $\|\vec{x}\|^2 = \|\vec{y}\|^2$ $ \vec{x}\cdot \vec{x} = \vec{y}\cdot \vec{y}$ ( $-\vec{a} + \vec{d}) \cdot (\vec{b} - \vec{c}$ ) = ( $\vec{d} + \vec{c}) \cdot (\vec{a} + \vec{b}$ ) $-\vec{a} \cdot \vec{b} + \vec{a} \cdot \vec{c} + \vec{d} \cdot \vec{b} - \vec{d} \cdot \vec{c} = \vec{d} \cdot \vec{a} + \vec{d} \cdot \vec{b} + \vec{a} \cdot \vec{c} + \vec{c} \cdot \vec{b}$ $-\vec{a} \cdot \vec{b} - \vec{d} \cdot \vec{c} = \vec{d} \cdot \vec{a} + \vec{c} \cdot \vec{b}$ $ 
-\vec{a} \cdot \vec{b} - \vec{d} \cdot \vec{a} = \vec{c} \cdot \vec{b} + \vec{d} \cdot \vec{c}
$ We can collect terms because dot product is commutative. $
-\vec{a} \cdot ( \vec{d} + \vec{b}) = \vec{c} ( \vec{d} + \vec{b})
$ We can assume that $- \vec{a} = \vec{c}$ . This is the part where I'm not sure where I've gone wrong. I set the direction of $ \vec{a}$ and $\vec{c}$ facing the same way, but now they are facing in opposite direction.","['vectors', 'geometry']"
4566671,Solve the equation $\frac{\sqrt{4+x}}{2+\sqrt{4+x}}=\frac{\sqrt{4-x}}{2-\sqrt{4-x}}$,"Solve the equation $$\dfrac{\sqrt{4+x}}{2+\sqrt{4+x}}=\dfrac{\sqrt{4-x}}{2-\sqrt{4-x}}$$ The domain is $4+x\ge0,4-x\ge0,2-\sqrt{4-x}\ne0$ . Note that the LHS is always positive, so the roots must also satisfy: $A:2-\sqrt{4-x}>0$ . Firstly, I decided to raise both sides of the equation to the power of $2$ . I came at $$\dfrac{4+x}{8+x+4\sqrt{4+x}}=\dfrac{4-x}{8-x-4\sqrt{4-x}}$$ Another thing I tried is to let $\sqrt{4+x}=u\ge0$ and $\sqrt{4-x}=v\ge0$ . Then $$\begin{cases}\dfrac{u}{2+u}=\dfrac{v}{2-v}\\4+x=u^2\\4-x=v^2\end{cases}$$ Adding the second to the third equation, we get $8=u^2+v^2$ . And the last thing: I cross-multiplied $$2\sqrt{4+x}-\sqrt{16-x^2}=2\sqrt{4-x}+\sqrt{16-x^2}\\\sqrt{4+x}=\sqrt{4-x}+\sqrt{16-x^2}$$ Raising to the power of 2 gives $$4+x=4-x+16-x^2+2\sqrt{(4-x)(16-x^2)}\\2\sqrt{(4-x)(16-x^2)}=x^2+2x-16$$ Is there something easier?",['algebra-precalculus']
4566780,Determinant of a matrix involving factorials,"Consider the matrix $$M_n=\begin{bmatrix}1&\frac1{2!}&\frac1{3!}&\dotsb&\frac1{n!}\\
\frac1{2!}&\frac1{3!}&\frac1{4!}&\dotsb&\frac1{(n+1)!}\\
\frac1{3!}&\frac1{4!}&\frac1{5!}&\dotsb&\frac1{(n+2)!}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
\frac1{n!}&\frac1{(n+1)!}&\frac1{(n+2)!}&\dotsb&\frac1{(2n-1)!}\end{bmatrix}$$ In other words, $M=[\frac1{(i+j-1)!}]_{1\le i,j\le n}$ . Is there any formula for $\det M_n$ ? Now, I know combinatorialists have lots of clever ways of evaluating determinants, so it doesn't seem so impossible to me that there might be a closed form solution to this. This is far from guaranteed, but I've been surprised before. (By the way, I conjecture that $\det M_n\ne0$ for all $n$ , but I don't immediately see a way to prove this.)","['matrices', 'determinant', 'combinatorics', 'factorial']"
4566854,How to prove that $\mu_n \rightharpoonup \mu$ IFF $\mu_n \overset{*}{\rightharpoonup} \mu$ and $\mu_n (X) \to \mu (X)$?,"Let $X$ be a metric space, $\mathcal M(X)$ the space of all finite signed Borel measures on $X$ , $\mathcal C_b(X)$ be the space of real-valued bounded continuous functions, $\mathcal C_0(X)$ be the space of real-valued continuous functions that vanish at infinity, and $\mathcal C_c(X)$ the space of real-valued continuous functions with compact supports. Then $\mathcal C_b(X)$ and $\mathcal C_0(X)$ are real Banach space with supremum norm $\|\cdot\|_\infty$ . Let $\mu_n,\mu \in \mathcal M(X)$ . We define weak convergence by $$
\mu_n \rightharpoonup \mu \overset{\text{def}}{\iff} \int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu \quad \forall f \in \mathcal C_b(X),
$$ and weak $^*$ convergence by $$
\mu_n \overset{*}{\rightharpoonup} \mu \overset{\text{def}}{\iff} \int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu \quad \forall f \in \mathcal C_c (X).
$$ Below ""not-too-hard"" theorem is mentioned in this thread , i.e., Theorem: $\mu_n \rightharpoonup \mu$ if and only if $\mu_n \overset{*}{\rightharpoonup} \mu$ and $\mu_n (X) \to \mu (X)$ . I'm trying to prove it, but I'm stuck at showing $$
\lim_m\lim_n \int_X (f-f_m) \mathrm d \mu_n = 0.
$$ Could you elaborate on how to finish the proof? My attempt: One direction is obvious. Let's prove the reverse. Assume $\mu_n \overset{*}{\rightharpoonup} \mu$ and $\mu_n (X) \to \mu (X)$ . Fix $f \in \mathcal C_b (X)$ and $\varepsilon>0$ .  Let $(\mu^+, \mu^-)$ with $\mu = \mu^+ - \mu^-$ be the Jordan decomposition of $\mu$ . Let $|\mu| := \mu^+ + \mu^-$ . By definition, $$
\int_X f \mathrm d \mu := \int_X f \mathrm d \mu^+ - \int_X f \mathrm d \mu^-.
$$ Notice that $\mathcal C_c(X)$ is dense in $(L_1 (|\mu|), \|\cdot\|_{L_1(|\mu|)})$ , so there is a sequence $(f_m) \subset L_1 (\mu|)$ such that $\|f_m - f\|_{L_1(|\mu|)} \to 0$ , i.e., $$
\int_X |f_m-f| \mathrm d |\mu| \to 0 \quad \text{as} \quad m \to \infty.
$$ Notice that $$
\begin{align}
\left | \int_X (f_m-f) \mathrm d \mu \right | &= \left | \int_X (f_m-f) \mathrm d \mu^+ - \int_X (f_m-f) \mathrm d \mu^- \right | \\
&\le \int_X |f_m-f| \mathrm d \mu^+ + \int_X |f_m-f| \mathrm d \mu^- \\
&= \int_X |f_m-f| \mathrm d |\mu| .
\end{align}
$$ This implies $$
\int_X f_m \mathrm d \mu \to \int_X f \mathrm d \mu \quad \text{as} \quad m \to \infty.
$$ We have a decomposition $$
\int_X f \mathrm d (\mu_n-\mu) = \int_X (f_m-f) \mathrm d \mu + \int_X f_m \mathrm d (\mu_n-\mu) + \int_X (f-f_m) \mathrm d \mu_n. 
$$","['weak-convergence', 'general-topology', 'signed-measures', 'measure-theory']"
4566894,"Why is for an algebraically closed field $k$, $\mathbb{A}^1_k \setminus \{0\} \simeq \operatorname{Spec} k[x, x^{-1}]$","Let $k$ be an algebraically closed field and $\mathbb{A}^1_k:=\operatorname{Spec}(k[x])$ be the affine 1-space. Why is then $\mathbb{A}^1_k \setminus \{0\} \simeq \operatorname{Spec}(k[x,x^{-1}])$ ? My ideas:
Let $S:= \{ x^n | \: n \in \mathbb{N}_0 \}$ . Then $S$ is multiplicatively closed and $S^{-1}k[x] = k[x,x^{-1}]$ .
The prime ideals of the localisation of $k[x]$ at $S$ are exactly the prime ideals of $k[x]$ which don't intersect $S$ . So $(0)$ and $(x-a)$ for $a \neq 0$ . Is this what is meant by $\mathbb{A}^1_k \setminus \{0\}$ ?","['localization', 'algebraic-geometry', 'affine-geometry', 'ideals']"
4566897,Implicit Differentiation $x^2 y^3 = 3$,"So I've been trying to implicitly differentiate $x^2 y^3 = 3$ which if I use product rule I get $$
-\frac{2y}{3x}
$$ but when I move $x^2$ to the right hand side and differentiate
I get $$
-\frac{2}{x^3 y^2} 
$$ which is completely different answer and I do not get what I did wrong. Help Please.","['calculus', 'implicit-differentiation', 'derivatives']"
4567031,Has this 'group product equivalence quotient' construction been substantially studied?,"Recently I've seen a few examples of an 'equivalence class' subgroup of a product of two groups: given $G$ and $H$ with homomorphisms $\gamma: G\mapsto K$ and $\eta: H\mapsto K$ , one can form a group $(G\times H)/K$ as the subgroup of $G\times H$ consisting of those elements $\langle g,h \rangle$ with $\gamma(g)=\eta(h)$ . It's easy to see that the order of the group is $\dfrac{|G|\cdot|H|}{|K|}$ so the notation makes sense (at least to me). Another way to think about this is as the coequalizer of $G\times H$ by the compositions of the two projection maps with the corresponding homomorphisms as maps from $G\times H\mapsto K$ . This comes up most notably (for me) in the classification of finite subgroups of $SO(4)$ where groups like $\frac12(S_4\times S_4)$ show up (the homomorphisms in this case being the usual parity homomorphism onto $C_2$ ), but I've seen the same construction or versions of it in a few places; it shows up in analyzing puzzles, for instance. This seems like such a natural notion that I'm surprised I haven't seen it covered more frequently; are there any references that anyone might be able to point me to?","['direct-product', 'group-theory', 'reference-request']"
4567060,Why is a finitely sheeted covering space of $\mathbb{R}^n$ never compact?,"It is common to see the following consequence of Cartan-Hadamard stated as a significant result relating curvature and topology: Let $M$ be a compact Riemannian manifold with non-positive sectional curvature $K \leq 0$ . Then the fundamental group of $M$ is infinite. A common proof is using the Cartan-Hadamard theorem to guarantee that $M$ is a quotient of $\mathbb{R}^n$ and concluding by stating without proof that a finite quotient of $\mathbb{R}^n$ is  never compact. I have seen a proof of this consequence using geometrical means (a compact group acting on $M$ by isometries has a fixed point), but I would really like to see a purely topological proof.","['riemannian-geometry', 'smooth-manifolds', 'covering-spaces', 'general-topology', 'differential-geometry']"
4567070,"If a set $A\subseteq[0,1]$ contains a large subinterval of every interval, must $A$ have full measure?","Suppose that $A$ is a subset of $[0,1]$ such that $A$ is open in $[0,1]$ , and there exists fixed constants $C, \mu > 0$ with $1\leq \mu < 2$ so that whenever $I$ is a sub-interval of $[0,1]$ , say $[a,b]$ , then $(A \cap I)$ contains an open subinterval of length at least $C(b-a)^{\mu}$ . Does $A$ have full measure in $[0,1]$ ? Note that when $\mu = 1$ the result is true by the Lebesgue density theorem. I chose $\mu < 2$ because I did not want ""trivial"" counterexamples (not that I know of any as of right now). This is not an assignment problem.","['measure-theory', 'lebesgue-measure']"
4567071,How can you show the results from substituting $x=a\cos{\theta}$ and $x=a\sin{\theta}$ are the same when finding $\int{\frac{16}{\sqrt{9-x^2}}}$?,"How can you show the results from substituting $x=a\cos{\theta}$ and $x=a\sin{\theta}$ are the same when finding $\int{\frac{16}{\sqrt{9-x^2}}}$ ? I am learning trig substitution right now. I know from seeing this problem I should substitute x for $a\sin{\theta}$ . After working this out: $$\int{\frac{16}{\sqrt{9-x^2}}}$$ $$x=3\sin{\theta}$$ $$dx=3\cos{\theta}d\theta$$ $$=\int{\frac{16}{\sqrt{9-x^2}}}=\int{\frac{48\cos{\theta}d\theta}{\sqrt{9-9\sin^2}{\theta}}}$$ $$=\int{\frac{48\cos{\theta}d\theta}{3\sqrt{1-\sin^2}{\theta}}}$$ $$=16\int{\frac{\cos{\theta}d\theta}{\sqrt{\cos^2}{\theta}}}$$ $$=16\int{\frac{\cos{\theta}d\theta}{\cos{\theta}}}=16\int{d\theta}$$ $$=16\theta=16\sin^{-1}\left({\frac{x}{3}}\right)+C_1$$ However, substituting $x=a\cos{\theta}$ yields a different result. $$\int{\frac{16}{\sqrt{9-x^2}}}$$ $$x=3\cos{\theta}$$ $$dx=-3\sin{\theta}d\theta$$ $$=\int{\frac{16}{\sqrt{9-x^2}}}=\int{\frac{-48\sin{\theta}d\theta}{\sqrt{9-9\cos^2}{\theta}}}$$ $$=\int{\frac{-48\sin{\theta}d\theta}{3\sqrt{1-\cos^2}{\theta}}}$$ $$=-16\int{\frac{\sin{\theta}d\theta}{\sqrt{\sin^2}{\theta}}}$$ $$=-16\int{\frac{\sin{\theta}d\theta}{\sin{\theta}}}=-16\int{d\theta}$$ $$=-16\theta=-16\cos^{-1}\left({\frac{x}{3}}\right)+C_2$$ $$\implies\quad{-16\cos^{-1}\left({\frac{x}{3}}\right)}+C_2={16\sin^{-1}\left({\frac{x}{3}}\right)}+C_1$$ Unless I did something incorrectly or there is something that prohibits different types of substitutions, how can this be true? I have tried to think it over how $-\cos^{-1}{x}$ can equal $\sin^{-1}{x}$ . I thought about it being a negative angle, perhaps making the opposite leg of the triangle negative. This however leads to a contradiction that says $-\cos^{-1}{x}=\sin^{-1}{x}$ but $\cos^{-1}{x}\neq-\sin^{-1}{x}$ . Any thoughts, answers, or corrections are appreciated.","['integration', 'calculus', 'trigonometry', 'trigonometric-integrals']"
4567082,"If $A< C$ and $B<D$, is it true that $\|AB\|_F \leq \|CD\|_F$ where $A, B, C, D$ are symmetric positive definite matrices?","I have symmetric positive definite matrices $A, B, C, D$ with \begin{equation}
A \leq C \quad\text{and}\quad
B \leq D
\end{equation} Is it true that $\|AB\|_F \leq \|CD\|_F$ where $\|\cdot\|_F$ denotes the Frobenius norm? The closest question I could find was: Does Frobenius norm (not operator 2 norm) preserve the positive semidefinite order of matrices? . Based on this we have $\|A\|_F \leq \|C\|_F$ and $\|B\|_F \leq \|D\|_F$ .","['matrices', 'normed-spaces', 'linear-algebra', 'matrix-norms']"
4567108,Does this type of matrix with the same diagonals have a name?,"Given real numbers $x_1, x_2, x_3, \dots, x_n$ , we define the real symmetric matrix $$
\begin{pmatrix} 
\color{red}{x_1} &    \color{red}{x_1} & \color{red}{x_1} & \cdots & \color{red}{x_1} \\ 
\color{red}{x_1} &    \color{blue}{x_2} & \color{blue}{x_2} & \color{blue}{\cdots} & \color{blue}{x_2} \\
\color{red}{x_1} &    \color{blue}{x_2} & \color{green}{x_3} & \cdots & \color{green}{x_3} \\
\color{red}{\vdots} & \color{blue}{\vdots} &  \color{green}{\vdots} & \ddots & \\ 
\color{red}{x_1} & \color{blue}{x_2} & \color{green}{x_3} & \cdots & x_n  
\end{pmatrix}
$$ So this matrix has all diagonals of the form $x_1, x_2, x_3, \dots, x_n$ . Does this matrix have a name? If not, I am curious if anything can be said about its eigenvalues and eigenvectors.","['matrices', 'linear-algebra', 'terminology', 'eigenvalues-eigenvectors']"
4567166,Intuitive explanation for elegant property of the unit circle: product of lengths converges to $2^{-n}$.,"Divide a unit circle into four quarter-circles, and flip each four quarter-circle inside the circle, using the line through the endpoints as the axis of rotation. Draw $n$ line segments from the centre to the flipped quarter-circles, so that the angles between neighboring line segments are equal. (Any collective orientation of the line segments will do.) Here is an example with $n=24$ . Let $P(n)=\text{product of the lengths of the $n$ line segments}$ It can be shown algebraically that $\lim\limits_{n\to\infty}2^nP(n)=1$ . Is there an intuitive explanation for the fact that $\lim\limits_{n\to\infty}2^nP(n)=1$ ? The numbers ( $1$ and $2$ ) are so simple that I wonder if there is some hidden, intuitive reason for this result. I kind of doubt that there is, but I don't want to dismiss the possibility too quickly. Example of an intuitive explanation To give you an idea of what I mean by an intuitive explanation, here is an example. On the graph of $y=\tan{x}$ , $0<x<\pi/2$ , draw $2n$ zigzag line segments that, with the x -axis, form equal-width isosceles triangles whose top vertices lie on the curve. Here is an example with $n=6$ . As $n\to\infty$ , the product of the lengths of the line segments converges to some positive number. Amazing? Not really: there is an intuitive explanation why it should converge. Almost all of the lengths of the zigzag line segments are very close to the value of $\tan{x}$ at odd multiples of $\frac{\pi}{2n}$ between $0$ and $\frac{\pi}{2}$ . These values can be paired together, taking them from the ends and working towards the middle. Each pair's product is $1$ , because they are reciprocal ratios of side lengths in a right triangle. So it is reasonable to expect (but not with certainty) that the product of the lengths of the zigzag line segments converges. (As to what it converges to, that is certainly not obvious, but it can be found .)","['infinite-product', 'circles', 'geometry', 'intuition']"
4567187,When does $\det\left(\begin{smallmatrix}1&1&1\\x_1&x_2&x_3\\x_2&x_3&x_1\end{smallmatrix}\right)=0$ imply $x_1=x_2=x_3$?,"Original question. Does the condition $$\det\begin{pmatrix}1&1&1\\x_1&x_2&x_3\\x_2&x_3&x_1\end{pmatrix}=0$$ imply $x_1=x_2=x_3$ ? a) Is this true if we assume $x_{1,2,3}\in\mathbb R$ ? b) Is this true if we assume $x_{1,2,3}\in\mathbb C$ ? I believe that this is true for real numbers and false for complex numbers. I posted my arguments in the answer below. (But still I would be interested in any alternative solutions and additional insights.) We might ask about an arbitrary field - but I would not be surprised it the generalized question might be difficult. Bonus question. For which fields the above condition holds? If this is difficult to answer, are there are at least some sufficient or necessary conditions? Some stuff which immediately comes into mind - and perhaps some of these might be reasonable ways to look at this problem. Let us denote $T(x_1,x_2,x_3)=(x_2,x_3,x_1)$ . The condition about the determinant basically says that the vectors $\vec u=(1,1,1)$ , $\vec x$ and $T(\vec x)$ are linearly dependent. Clearly, $T$ is a linear isomorphism and $T^3=id$ . If we are working over complex numbers, then the minimal polynomial it $m(t)=t^3-1$ and the eigenvalues are the roots of this polynomial: $1$ , $\omega$ , $\omega^2$ . For real numbers, the condition about determinant says that the points $(x_1,x_2)$ , $(x_2,x_3)$ , $(x_3,x_1)$ are collinear. We can forget the linear structure and simply view this as a problem about a polynomial in three variables - we're asking when $x_1x_2+x_2x_3+x_3x_1-x_1^2-x_2^2-x_3^2=0$ . I will add that this question originated from some discussion in chat - although in that discussion it was just an auxiliary result.","['determinant', 'linear-algebra', 'symmetric-polynomials']"
4567194,Extensions of Tower Property for Conditional Expectation,"Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space.
Let random variable $X \in \mathcal{F}$ and two sigma fields $\mathcal{F}_1, \mathcal{F}_2 \in \mathcal{F}$ .
We consider the general case where neither one of $\mathcal{F}_1, \mathcal{F}_2$ is contained in the other.
Do we have $\mathbb{E}[\mathbb{E}[X | \mathcal{F}_1] | \mathcal{F}_2] = \mathbb{E}[X | \mathcal{F}_1 \cap \mathcal{F}_2]$ ?
If not, what is a counterexample? On a related topic, let $X, Y, Z$ be pairwise independent random variables and $\mathcal{F}_1 = \sigma(X, Y), \mathcal{F}_2 = \sigma(X, Z)$ .
If $X, Y, Z$ are not mutually independent, then $\mathbb{E}[\mathbb{E}[f(X, Y, Z) | \sigma(X, Y)] | \sigma(X, Z)] = \mathbb{E}[f(X, Y, Z) | \sigma(X)]$ doesn't hold (e.g. let $X, Y \stackrel{iid}{\sim} 2 * Bernoulli(0.5) - 1, Z = XY$ and $f(X, Y, Z) = Z$ ).
What if $X, Y, Z$ are mutually independent - does $\mathbb{E}[\mathbb{E}[f(X, Y, Z) | \sigma(X, Y)] | \sigma(X, Z)] = \mathbb{E}[f(X, Y, Z) | \sigma(X)]$ hold then?","['conditional-expectation', 'probability-theory']"
4567207,How to identify all the right circular cones passing through six arbitrary points,"I have this interesting question.  Given $6$ arbitrary points, I want to identify all the possible circular cones passing through them. The equation of a right circular cone whose vertex is at $\mathbf{r_0}$ and whose axis is along the unit vector $\mathbf{a}$ , and whose semi-vertical angle is $\theta$ is given by $$ (\mathbf{r} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r} - \mathbf{r_0}) = 0 $$ where $\mathbf{r}$ is the position vector of a point on the surface of the cone.  Counting the number of parameters, we have $3$ for $\mathbf{r_0}$ , $2$ for $\mathbf{a}$ and $1$ for $\theta$ .  We therefore need at least $6$ points on the surface of the cone to specify it. Question :  What is the procedure for extracting the parameters of a right circular cone passing through $6$ arbitrary points? What I have tried :  I have parameterized the axis unit vector as $ \mathbf{a} = ( \sin t \cos s, \sin t \sin s , \cos t ) $ and written $\mathbf{r_0} = (x, y, z) $ Now define the functions $ f_1 =  (\mathbf{r_1} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_1} - \mathbf{r_0})  $ $ f_2 =  (\mathbf{r_2} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_2} - \mathbf{r_0})  $ $ f_3 =  (\mathbf{r_3} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_3} - \mathbf{r_0})  $ $ f_4 =  (\mathbf{r_4} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_4} - \mathbf{r_0})  $ $ f_5 =  (\mathbf{r_5} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_5} - \mathbf{r_0})  $ $ f_6 =  (\mathbf{r_6} - \mathbf{r_0})^T ( (\cos \theta)^2 \ \mathbf{I} - \mathbf{aa}^T ) (\mathbf{r_6} - \mathbf{r_0})  $ Now using the multivariate Newton-Raphson method I could iterate to find a solution for the parameter vector $(t, s, x, y, z , \theta )$ that will make $f_1, f_2, f_3, f_4, f_5, f_6$ all zero. This works, but it's iterative and at best converges to one of the possible right circular cones. Is there a way to generate all the possible right circular cones that are solutions, i.e. passing the $6$ given points?","['quadrics', 'linear-algebra', 'geometry', 'system-identification']"
4567209,Can the space of linear operators on a Hilbert space be made into a Hilbert space?,"Let $H$ be a Hilbert space, and $\mathcal L(H)$ be the space of linear operators on $H$ . Can we find an inner product on $\mathcal L(H)$ that induces an equivalent norm on $\mathcal L(H)$ , i.e., a norm that is equivalent to the operator norm? For finite-dimensional spaces, the Frobenius inner product answers the question in the affirmative. So any counter-examples necessarily is infinite-dimensional. I am aware of the negative result , that  there is no inner product that induces the operator norm if $\dim H\ge 2$ .","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4567237,Combination with repeated floor operations same as the whole combination?,"This question involves both math and programming, but the main inquiry in nature is mathematical. There is a LeetCode question called Unique Paths that asks the number of ways that one can reach the lower-right corner of a $m \times n$ grid if you start in the upper-left corner and want to reach the lower-right corner, only being able to move down or right at any step. Visualization of the $3 \times 7$ case, for example: One mathematical approach that was suggested is to compute $\begin{pmatrix}m + n - 2 \\ m - 1\end{pmatrix}$ as the question is equivalent to asking the number of ways to select $m - 1$ downwards moves from a total of $m + n - 2$ moves to reach the lower-right corner. However, the programming solution that was proposed essentially computes the following: $$
\left\lfloor \frac{\left\lfloor \frac{\left\lfloor \frac{\left\lfloor \frac{1 \cdot n}{1} \right\rfloor (n + 1)}{2} \right\rfloor (n + 2)}{3} \right\rfloor \cdots (n + m - 2)}{\vdots \atop m - 1} \right\rfloor
$$ So, my question is how does the solution guarantee that the above is equivalent to $\left\lfloor \frac{(m + n - 2)!}{(m - 1)!(n - 1)!} \right\rfloor$ without any loss of data from the repeated application of the floor operator? Edit: The accepted answer from Sil helped me out the most intuitively. Though, the inductive proof of how each step results in a combination by Mike also helped very much.","['products', 'ceiling-and-floor-functions', 'combinatorics', 'sequences-and-series']"
4567268,Prove that the characteristic function of a random variable is uniformly continuous.,"The definition of the characteristic function of a random variable $X$ is $$\varphi(\lambda)=E[e^{i\lambda X}]=\int_{-\infty}^\infty e^{i\lambda x}\,dP(x),$$ where $dP$ is the distribution of the random variable. Prove that the characteristic function of a random variable is uniformly continuous. My attempt of the proof is: $$|\varphi(\lambda) -\varphi(\lambda') | = |\int_{-\infty}^\infty (e^{i\lambda x} -e^{i\lambda' x} ) dP(x)|  $$ $$\leq    |\int_{-\infty}^\infty e^{i\lambda' x} (e^{i(\lambda - \lambda') x}-1)  dP(x)|  $$ $$\leq  \int_{-\infty}^\infty   |e^{i\lambda' x}|   |e^{i(\lambda - \lambda') x}-1|    dP(x)      $$ Then use the fact that the absolute value of a complex exponential is 1. $$=  \int_{-\infty}^\infty      |e^{i(\lambda - \lambda') x}-1|    dP(x)      $$ Then use the inequality that the inequality $|e^{ihx}-1|\le |hx|$ . $$\leq  \int_{-\infty}^\infty      |(\lambda - \lambda') x|    dP(x)      $$ I don't know how to do next. The textbook said I need to use Lebesgue Dominated Convergence Theorem. I don't know how to use that. I think I am stuck that how to prove one function is uniformly continuous. I only know this definition of uniformly continuous: Given $\epsilon > 0$ , we want to find $\delta > 0$ such that $$
|f(x) - f(y)| < \epsilon \quad  \text{whenever} \quad |x-y| < \delta
$$ and $\delta$ is independent of $x$ and $y$ .","['probability-theory', 'probability', 'real-analysis']"
4567289,Ant on a Cylinder,"The following question was asked in a mock test for CMI (Chennai Mathematical Institute):- There is a glass cylinder with radius $R$ . An ant is $d$ cm from the mouth of the cylinder. It is on the outer surface of the cylinder. There is a honey drop on the inner surface of cylinder that is also $d$ cm from the mouth of the cylinder and diametrically opposite to the ant. What is the shortest distance the ant has to travel to get to the honey drop? My attempt includes ""unwrapping"" the cylinder to get a rectangle with length equal to $2πR$ and breadth equal to $H$ which is the height of the cylinder. Now, the problem is reduced to finding the shortest path between two points on a rectangle given that the ant has to reach the edge first. I dropped a perpendicular from the ant's location to the edge nearer to it, and then drew a straight line from there straight to the honey. Using the Pythagorean Theorem, we get that the distance the ant travels is $d+\sqrt{d^2+π^2R^2}$ . Many of the students on the mock test got the same answer, but, a few friends of mine got a different answer. Problem is, I see no error in their reasoning/method, and the distance they get is smaller than mine, too. So, I would like to know if I've made a mistake.","['contest-math', 'solution-verification', 'geometry', '3d']"
4567290,Is there a way to infer periodicity of functions purely from their Taylor series?,"I have been working with Taylor series for a while and I am interested in the following. Suppose I have the Maclaurin (for simplicity) series: $f \left(x \right) = \sum_{n = 0}^{\infty}{a_{n}x^{n}}$ where the formula $a_{n} = g \left(n \right)$ is known. Furthermore, assume that the above series converges everywhere. Under these conditions, is it possible to deduce from $g \left(n \right)$ that $f \left(x \right)$ is periodic? For instance, for $f \left(x \right) = \cos{\left(x \right)}$ , $a_{2n} = \frac{\left(-1 \right)^{n}}{\left(2n \right)!}$ and $a_{2n + 1} = 0$ for $n \ge 0$ . Does something about the ""form"" (for lack of a better word) of these coefficients imply the periodicity of $\cos{ \left(x \right)}$ ? I have read in other questions on this site that if $g \left(n \right)$ is not known a priori, this is an undecidable problem: is this still the case when $g \left(n \right)$ is known? As an aside, here is what I have tried: if $f \left(x \right)$ is periodic then one has $f \left(x \right) = f \left(x + P \right)$ , where $P$ is the period and $P > 0$ . Then, one has: $f \left(x + P \right) = \sum_{n = 0}^{\infty}{a_{n} \left(x + P \right)^{n}}$ In particular, we have $f \left(0 \right) = f \left(P \right)$ , giving: $a_{0} = a_{0} + \sum_{n = 1}^{\infty}{a_{n}P^{n}}$ Giving $\sum_{n = 1}^{\infty}{a_{n}P^{n}} = 0$ One can also differentiate $f \left(x \right)$ , which yields: $f' \left(x \right) = \sum_{n = 1}^{\infty}{n a_{n} x^{n - 1}}$ Requiring that $f' \left(x \right) = f' \left(x + P \right)$ and setting $x = 0$ , one then has: $a_{1} = a_{1} + \sum_{n = 2}^{\infty}{n a_{n} P^{n - 1}}$ That is: $\sum_{n = 2}^{\infty}{n a_{n} P^{n - 1}} = 0$ The above procedure can be repeated ad infinitum for higher derivatives, which seems to give an infinite number of conditions which must be satisfied by the various $a_{n}$ 's. However, it seems really complicated to deduce periodicity from these conditons. Any help or insight would be greatly appreciated.","['power-series', 'calculus']"
4567324,Show that $ \mathbb{P}(A)=0 $ for each $ A \in \mathcal{F} $ with $ \sum \limits_{k=1}^{\infty} \mathbb{P}\left(A_{k} \cap A\right)<\infty $.,"Let $\{A_{n}\}_{n \in \mathbb{N}} \subset \mathcal{F} $ such that $ \sum \limits_{k=1}^{\infty} \mathbb{P}\left(A_{k}\right)=\infty $ holds. Further, suppose that $ A_{n} $ is a sequence of stochastically independent events. Show that $ \mathbb{P}(A)=0 $ for each $ A \in \mathcal{F} $ with $ \sum \limits_{k=1}^{\infty} \mathbb{P}\left(A_{k} \cap A\right)<\infty $ . Note: First show that for any $ C \in \mathcal{F} $ with $ \mathbb{P}(C)=1 $ , the equation $ \mathbb{P}(B \cap C)=\mathbb{P}(B) $ holds for all $ B \in \mathcal{F} $ . Then use Borel-Cantelli's lemma twice. Attempt: Step of the hint: If $P(C)=1$ , let $C^\complement$ denotes the complement of $C$ . First note that $$
P(C^\complement) = 1-P(C) = 0
$$ and by monotonicity we see that $P(C^\complement \cap B) = 0$ . Therefore, \begin{align*}
P(C\cap B) = P(C\cap B) + P(C^\complement \cap B)
&=P([C\cap B]\cup [C^\complement \cap B])\\
&=P([C\cup C^\complement]\cap B)\\ 
&= P(B)\\
&=P(B)\cdot P(C)
\end{align*} where the last line holds since $P(C)=1$ . So once again we have $P(C\cap B) = P(C)\cdot P(B)$ which shows that $C$ and $B$ are independent. But how can I use Borel-Cantelli's lemma on this? Any suggestions?","['probability-theory', 'probability']"
4567336,Prove that $\int_0^{2\pi} \frac{ab}{a^2\cos^2t+b^2\sin^2t}\mathrm dt=2\pi$.,"Probably equation $\int_0^{2\pi} \frac{ab}{a^2\cos^2t+b^2\sin^2t}\mathrm dt=\int_0^{2\pi} \frac{ab}{a^2\sin^2t+b^2\cos^2t}\mathrm dt$ is useful. Double integration is also a tool, but I don't know next step.","['integration', 'definite-integrals']"
4567376,Current research areas of Group Theory,"I'm finishing an undergraduate degree in mathematics and am really passionate about group theory. Currently, my research projects have been mostly in ring theory, however (since my university does not have any group theorists). Thus, I really don't know how to go about getting to know more research level problems in group theory - the broad areas, the big open questions, etc. I'm afraid of committing a graduate degree to it only to end up regretting it later. I took a look at ArXiv's ""Group Theory"" section, but it didn't really help me get the big picture, especially when it comes to algebraic aspects of the theory (many papers were a lot more geometric in nature, which does not please me much, personally). In short, what are the current big areas of group theory? And what problems do they try to answer? Thanks in advance!","['group-theory', 'soft-question', 'research']"
4567421,Sum of $(1/n^2)\sum_{k=2}^{n/2}\frac{1}{1-\cos\left(\frac{2\pi(k-1)}{n}\right)}$ in the large $n$ limit.,"This summation appears in studying circulant matrices . Consider an $n-$ dimensional circulant matrix with with entries $c_1=c_{n-1}=1$ and $c_0=-2$ (all other coefficients beign $0$ ). This represents, for example, a random walk on a closed ring of $n$ states. The following calculation can be viewed as the average decay time for all modes (notice that the denominator of the series' terms correspond to the negative eigenvalues of the circulant matrix, $-\lambda_{k}$ . Prefactor $1/n^2$ is a normalization constant). From the symmetry of the eigenvalues set one can distinguish two cases: $n$ odd or even. If $n$ is odd, $$\frac{1}{n^2}\sum_{k=2}^{(n+1)/2}\frac{1}{1-\cos\left(\frac{2\pi(k-1)}{n}\right)}\,.$$ If $n$ is even, $$\frac{1}{n^2}\sum_{k=2}^{n/2}\frac{1}{1-\cos\left(\frac{2\pi(k-1)}{n}\right)}\,.$$ I've attempted to approximate this summation by using the continuous limit but I get the wrong values when comparing to numerical evaluation. The goal is to find an expression for the large $n$ behavior.","['trigonometry', 'sequences-and-series', 'real-analysis']"
4567459,Agent-Based Wealth Model: Proving Inequality,"Consider the following agent-based model: There are $N$ agents Every agent starts with $1 At each time interval (i.e. at each step), every agent gives \$1 to a randomly chosen agent. I want to find how unequal the wealth distribution becomes over a long period of time. After running a simulation for a large number of agents, I find that the wealth distribution becomes over a long period of time approaches (what I am ""by eye"" guessing to be) a Boltzmann distribution. I am curious as to why this happens from a derivation standpoint. I have tried to find sample derivations online, but only find kinetic-model related Boltzmann distribution derivations. Can anyone point me to any resources or share a derivation that explain this result?","['statistics', 'proof-writing', 'probability', 'statistical-mechanics']"
4567473,Find all analytic functions $f: \mathbb{C} \backslash\{3\} \rightarrow \mathbb{C}$ such that $|f(z)| \leq \frac{\left|(z-2)^{2}\right|}{|z-3|}$,"This is a question on one of my homework sheets for a complex function theory module on my maths MSc. I believe I need to use the Laurent series for $f(z)$ around 3 here, so I wrote $f(z)$ as: $$f\left(z\right)\ =\ \sum_{n=-\infty}^{\infty}a_n\left(z-3\right)^n$$ And then I used the integral representation of the coefficients: $$a_n=\frac{1}{2\pi i}\int_{S_p^+\left(3\right)}^{ }\frac{f\left(z\right)}{\left(z-3\right)^{n+1}}dz$$ So this would mean that we have: $$f\left(z\right)\ =\ \left|\sum_{n=-\infty}^{\infty}\left(\frac{1}{2\pi i}\int_{S_p^+\left(3\right)}^{ }\frac{f\left(z\right)}{\left(z-3\right)^{n+1}}dz\right)z^n\right|\le\frac{\left|\left(z-2\right)^2\right|}{\left|z-3\right|}$$ I somehow need to determine how many coefficients of the Laurent series are nonzero, but I'm kind of stuck on where to go next. I noticed that by Cauchy's nth Derivative Formula that, since $$f^n\left(3\right)=\frac{n!}{2\pi i}\int_{S_p^+\left(3\right)}^{ }\frac{f\left(z\right)}{\left(z-3\right)^{n+1}}dz$$ I guess that means we could write $a_n=\frac{f^n\left(3\right)}{n!}$ , but I don't know if that's helpful here.","['complex-analysis', 'complex-integration', 'laurent-series', 'analytic-functions']"
4567478,Prove different expression of $\left\{\liminf\limits_{n\to\infty}X_n\geq X\right\}^c$,"Let be $M:=\left\{\omega\in\Omega\mid \liminf\limits_{n\to\infty}X_n(\omega)\geq X(\omega)\right\}$ . Show that the complement set $M^c$ satisfies \begin{align*}
&M^c=\\
&\bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X(\omega)-X_{j}(\omega)\geq\frac{1}{m}\right\}\\
&\cap \bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X_{j}(\omega)-X(\omega)\leq\frac{1}{m}\right\}.
\end{align*} My approach: Let's choose an arbitrary $\omega\in M$ and assume $\liminf\limits_{n\to\infty}X_n(\omega)= X(\omega)$ . As $X(\omega)$ is the smallest limit point, for any $\epsilon>0$ there only exist finite many $n\in\mathbb{N}$ such that $X(\omega)-X_n(\omega)>\epsilon$ . Otherwise there would be another smaller limit point which is a contradiction. Hence, for any $\epsilon>0$ there must be a $n_0\in\mathbb{N}$ such that for all $n>n_0$ we have $X(\omega)-X_n(\omega)\leq\epsilon$ . So in other words \begin{align*}
&\omega\in\bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X(\omega)-X_{j}(\omega)<\frac{1}{m}\right\}.
\end{align*} Now let's choose an arbitrary $\omega\in M$ and assume $\liminf\limits_{n\to\infty}X_n(\omega)-X(\omega)=\delta>0$ . As $\liminf\limits_{n\to\infty}X_n(\omega)$ is a limit point, for each $\epsilon$ -neighbourhood of $\liminf\limits_{n\to\infty}X_n(\omega)$ there are infinitely many $n\in\mathbb{N}$ such that $X_n(\omega)-\liminf\limits_{n\to\infty}X_n(\omega)<\epsilon$ . In particular this holds for any $\epsilon$ with $0<\epsilon<\frac{\delta}{2}$ so that we find infinitely many $n$ such that $X_{n}(\omega)-X(\omega)>\frac{\delta}{2}$ . This leads to \begin{align*}
&\omega \in\bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X_{j}(\omega)-X(\omega)>\frac{1}{m}\right\}.
\end{align*} Hence, \begin{align*}
&M\subseteq \bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X(\omega)-X_{j}(\omega)<\frac{1}{m}\right\}\\&\cup \bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X_{j}(\omega)-X(\omega)>\frac{1}{m}\right\}.
\end{align*} The other direction of the subset property seems obvious. Finally, \begin{align*}
&M^c=\\
&\left(\bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X(\omega)-X_{j}(\omega)<\frac{1}{m}\right\}\cup \bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X_{j}(\omega)-X(\omega)>\frac{1}{m}\right\}\right)^c\\
&=\bigcup\limits_{m=1}^{\infty}\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X(\omega)-X_{j}(\omega)\geq\frac{1}{m}\right\}\cap \bigcap\limits_{m=1}^{\infty}\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{j=n}^{\infty}\left\{\omega\in\Omega\mid X_{j}(\omega)-X(\omega)\leq\frac{1}{m}\right\}
\end{align*} by law of De Morgan. Is this correct?","['elementary-set-theory', 'limsup-and-liminf', 'solution-verification', 'real-analysis']"
4567501,"""Entanglement"" of continuous functions via Mercer's theorem","In quantum mechanics, the Schmidt decomposition of a vector $\vert \psi\rangle \in \mathbb{C}^n\otimes \mathbb{C}^m$ is $$ \vert \psi\rangle = \sum_{i=1}^{\min\{n,m\}} \lambda_i \vert e_i\rangle \otimes \vert f_i\rangle$$ where $\{\vert e_i\rangle\}$ are an orthonormal set of vectors in $\mathbb{C}^n$ , $\{\vert f_i\rangle\}$ are orthonormal in $\mathbb{C}^m$ , and each $\lambda_i\geq 0$ Mercer's theorem states that for a continuous symmetric non-negative function $K:[a,b]\times [a,b]\rightarrow \mathbb{R}$ , there is a sequence of functions $\{e_i\in L^2[a,b]\}_i$ and non-negative eigenvalues $\lambda_i$ such that $$ K(s,t) = \sum_{i=1}^\infty \lambda_i e_i(s)e_i(t)$$ These look so similar, and the proofs seem to be identical if you use the right measure on $\mathbb{C}^n$ . In quantum mechanics, entanglement is important and the coefficients in the Schmidt decomposition quantify it: an unentangled state has only one non-zero $\lambda_i$ , and we can even define ""entanglement entropy"" as the entropy of $\{\lambda_i^2\}$ (which adds up to $1$ if we start with a unit vector). Is there a similar notion of ""entanglement"" for continuous functions? If only one $\lambda_i$ is non-zero, it splits in a sense, but I'm wondering if this concept has a name and some applications or theory.","['quantum-information', 'functional-analysis', 'reproducing-kernel-hilbert-spaces']"
4567505,Polynomial in terms of coordinate functionals,"I am trying to write a m-homogeneous polynomial in terms of coordinate functionals by using permutations in $S_m$ . I am not sure about how should I go for getting this representation? Here, $P \in \mathcal P\left({ }^m E\right)$ denotes the vector space of all m-homogeneous polynomials from $E$ into $\Bbb C$ And $S_m$ is the symmetric group. Thanks in advance for any help.","['complex-analysis', 'banach-spaces', 'polynomials']"
4567516,Distance distribution in different dimensions,"This is a crosspost from this question on stackoverflow. It received no answers because ""it is a math question"". I don't know how to move so I crosspost. There are so many questions about distance distributions and the curse of dimensionality I did not read them all. But those that I did read, did not answer my question. I want to create the distance distribution of uniformly distributed points in different dimensions, to ultimately visualize the curse of dimensionality. My understanding is that the distribution goes from a flatter curve in low dimensions to a spike curve in higher dimensions. They look like this: Mean increases, variance decreases. Don't mind numbers, this is just a sketch (with gaussian distribution)
I have seen corresponding figures in papers. (Would be able to show and cite, but not sure about copyrights.) I want to recreate those figures. But when I create random points calculate the distances and put them in to a histgram it looks like this: The first one is the plain histogram of distances. The second one, the bin are scaled to the max distance of the repective dimensions. In the third one I overlayed them by subtracting the value of the lowest non-empty bin.
The first one obviously has the means, but neither increasing height nor decreasing variance. In the second you can see the concentration of the distances, but not height or means. This is not what I saw in those papers. The third indicates that the curves have all the same shape. Again, not what I expected. This is the code I used to create the second figure. import math
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import distance

if __name__ == '__main__':
    records = 1000
    binwidth = 0.01
    precision = 10
    fig, ((ax1), (ax2), (ax3)) = plt.subplots(3, 1)
    fig.set_size_inches(6, 10)
    for dimensions in [2, 10, 100, 1000]:
        data = np.random.uniform(low=0, high=1, size=(records, dimensions))
        data = np.around(data, decimals=precision)
        distlist = distance.pdist(data)

        variance = np.var(distlist)
        print('dim {dim}: var={var}'.format(dim=dimensions, var=variance))

        maxdist = math.pow(dimensions, 1 / 2)
        bins = [round(x * binwidth, 2) for x in range(round(maxdist / binwidth) + 2)]
        histo = np.histogram(distlist, bins)

        harr = np.stack((histo[1][1:], histo[0]), axis=-1)
        mask = harr[:, 1] > 0

        masked = harr[mask]
        ax1.plot(masked[:, 0], masked[:, 1], label='dim {dim}'.format(dim=dimensions))
        ax2.plot(masked[:, 0] / maxdist, masked[:, 1], label='dim {dim}'.format(dim=dimensions))
        min = np.min(masked[:, 0])
        ax3.plot(masked[:, 0] - min, masked[:, 1], label='dim {dim}'.format(dim=dimensions))
    ax1.set_title('histogram')
    ax1.legend()

    ax2.set_title('histogram, bins divided by max distance')
    ax2.legend()

    ax3.set_title('histogram, bin subtracted by minimum bin')
    ax3.legend()
    plt.show() I see two points where I may be wrong. My understanding of distance distributions the code does not what it's intended to do How do I draw a histogram with bins of width 0.01 showing the number of distances between pairs of uniformly distributed points in the unit cube? EDIT Ok, thanks to @Henry it seems what I did was correct but not what I should have done to produce the results I wanted. What I want is the to produce the following figures myself. The image is taken from Pestov, Vladimir (2007): Intrinsic dimension of a dataset: what  properties does one expect? In : 2007 IEEE International Joint Conference on Neural Networks Proceedings. International Joint Conference on Neural Networks. Fig. 14 of Chávez, Edgar; Navarro, Gonzalo; Baeza-Yates, Ricardo; Marroqu\’ın, José Luis (2001): Searching in metric spaces. In ACM Computing Surveys 33 (3), pp. 273–321. DOI: 10.1145/502807.502808. is a similar one. The text does not describe how to create them. How can I do this?",['statistics']
4567526,Lower shriek pushforward of injectives.,"Given an open subscheme $f: U\hookrightarrow X$ and an injective etale sheaf of abelian groups $\mathcal{I}$ on $U$ , then is it necessarily true that $f_!(\mathcal{I})$ is also injective? If not then how one proves that $H^i(X, f_!(\mathcal{F})) = H_c^i(U, \mathcal{F})$ when $X$ is proper? Although I still don't know whether $f_!$ preserves injectivity or not (it should be incorrect in general but might be true for open immersions) but the last statement follows from proper base change.","['injective-module', 'algebraic-geometry', 'sheaf-cohomology']"
4567532,Cycles on a discrete torus,"Consider an $m\times n$ grid graph whose opposing vertices are connected (i.e. the graph is a discrete torus). How many distinct cycles are there? I know that for the case of $1\times n$ or $n\times 1$ , there are $1+n$ distinct cycles (one of length $n$ , and $n$ of length $1$ ). I planned on brute-forcing a few cases to see if I could identify some kind of pattern. However, I feel like a combinatorial approach would be more effective. I don't know what to do from here to get a general formula.","['graph-theory', 'discrete-mathematics']"
4567581,Is there a topology on the reals such that the continuous functions of that topology are precisely the differentiable functions?,"Is there a topology $T$ on the set of real numbers $\mathbb{R}$ , such that the set of $T$ -continuous functions from $\mathbb{R}$ to $\mathbb{R}$ is precisely the set of differentiable functions on $\mathbb{R}$ ?","['continuity', 'general-topology', 'derivatives']"
4567649,Is the complementary of the circunference simply connected,Say we have $C = Z(x_0^2-x_1^2-x_2^2)$ in $\mathbb{C}P^2$ and we define $X = \mathbb{C}P^2 \setminus C$ . Is this space simply connected?,"['fundamental-groups', 'algebraic-geometry']"
4567729,"We construct 4-digit numbers, which are chosen from any of the following seven digits: 0, 1, 3, 5, 7, 8, 9.","So basically, we construct 4-digit numbers, which are chosen from any of the following seven digits: 0, 1, 3, 5, 7, 8, 9. What is the probability of a number having no digit ‘7’ and containing two or more identical digits? 4 digit numbers that starts with 0s count too, like 0177 For the moment I have the number of 4 digits numbers that uses these 7 digits : $7^4 = 2401$ . Then I have the number of 4 digits numbers with at least 2 identical numbers which is : $2401 - 840 = 1561$ the 840 comes from the $P(n,r) = \frac{n!}{(n-r)!}$ Then I have the number of 4 digit numbers without the digit 7 which is $6^4 = 1296$ But then I don't really know where to start to find the probability they ask.","['permutations', 'combinatorics', 'probability']"
4567795,Evaluate $\sum_{n\geq 0} \mbox{arccot}(n^2 + n + 1)$ [duplicate],"This question already has answers here : Explicitly finding the sum of $\arctan(1/(n^2+n+1))$ (2 answers) Closed 1 year ago . (This is a 1986 Putnam Challenge problem.) First, note that \begin{equation}
n^2 + n + 1 = \frac{n^3 - 1}{n - 1},
\end{equation} which is the slope of the secant line through $f(x) = x^3$ at $x = 1$ and $x = n$ .  So $\mbox{arccot}\Big(\frac{n^3 - 1}{n - 1}\Big)$ is the acute angle that this line makes with any vertical line.  Thus the requested sum is the sum of all these angles ... and this is where I get stuck. Surely I'm not meant to use a trig addition formula here; all the ones I'm aware of are too convoluted for the purpose. Am I at least on the right track?  The form of the polynomial seems to suggest so, but maybe it's a red herring.  Do they do that with Putnam problems? (It might also be relevant that $\mbox{arccot}(x)$ is convex over the positive reals.  But that merely provides inequalities, not identities ... or so it would seem.)","['contest-math', 'secant', 'closed-form', 'sequences-and-series', 'trigonometry']"
4567824,In a unit circle what is the average distance to the center of circle?,"In a circle with radius 1 what is the average distance between a randomly placed point and the center of the circle? So far I have tried a few things but gotten different results. Approach 1:
My idea here is to take the weighted average of all distances where the circumference of the corresponding circle is the weight. The definition of the weighted average is: $\frac{\sum_{0} ^{n} {w_n v_n}}{\sum_{0} ^{n} {w_n}}$ where $w_n$ and $v_n$ are lists of weights and values This leads me to: $\frac{\int_0^1 {2\pi r^2}dr}{\int_0^1 {2\pi r} dr} = \frac{2}{3}$ I believe this is correct due to a computer simulation I made, but I could have done that wrong. Approach 2: Since the probability of a point falling in a certain section of a circle is proportional to the area of the section my other idea was to find the distance where the probability of falling inside and outside is equal, or the area of the inner circle is equal to the area of the rest of the circle. $\pi x^2 = \pi 1^2 - \pi x^2$ $x = \frac{\sqrt{2}}{2}$ Why is this different than my first answer? Which approach is wrong and why?","['calculus', 'geometry']"
4567842,Finding an elliptic curve which has CM by $\mathbb{Q}(\sqrt{-143})$.,"Let $k = \mathbb{Q}(\sqrt{-143})$ and $\Lambda \subseteq \mathbb{C}$ the lattice with $\mathbb{Z}$ -basis $\{\omega_1, \omega_2\}$ , where $\omega_1 = 1$ and $\omega_2 = (1 + \sqrt{-143})/2$ .  I know that the elliptic curve $E = \mathbb{C}/\Lambda$ defined over $k(j)$ , ( $j$ is the $j$ -invariant of $E$ ) has CM by $\mathbb{Q}(\sqrt{-143})$ , but I would like to write $E$ in Weirstrass form.  We have $E: y^2 = 4x^3 -g_2x -g_3$ , where $g_2$ and $g_3$ come from the weights $4$ and $6$ Eisenstien series: \begin{align*}
g_2 = 60 \sum_{\omega \in \Lambda \atop \omega\neq 0} \frac{1}{\omega^4} \hskip10mm \text{and} \hskip10mm g_3 = 140 \sum_{\omega \in \Lambda \atop \omega\neq 0} \frac{1}{\omega^6}
\end{align*} Is there any way to compute these values in this example, or is this just to difficult to compute?  I know that some of these values are known like when $k = \mathbb{Q}(i)$ and $\mathbb{Q}(\sqrt{-3})$ .  I've also looked for this example in LMFDB and didn't come up with anything.","['number-theory', 'elliptic-functions', 'modular-forms', 'elliptic-curves']"
4567885,Growth rate of the summation of $i.i.d.$ random variables with only $(1+\epsilon)$-th moment,"Suppose we have $n$ i.i.d. random variables $X_1, \ldots, X_n$ with zero mean and $\mathbb E[X_1^{1+\epsilon}] < \infty$ , where $\epsilon$ is a constant real number in $(0,1)$ . Let $S_n := \sum_{i=1}^n X_i$ . Can we say anything about the convergence rate of $S_n/n$ ? The law of large numbers tells us $S_n/n \rightarrow 0$ almost surely and in $L^1$ , but that is all I know. If $X_i$ has a finite second moment, then the convergence rate is $O_p(n^{-0.5})$ . If $X_i$ has a regular-varying tail (e.g. with density $|f(x)|\sim x^{-2+\epsilon}$ ), then I can use the generalized central limit theorem for stable distribution, so $S_n/n$ is $O_p(n^{-\epsilon/ 1+\epsilon})$ . However, I wonder if there is any general result when we only know the existence of $(1+\epsilon)$ -th moment.","['probability-distributions', 'stochastic-processes', 'convergence-divergence', 'probability-theory', 'probability']"
4567904,Prove that $f(z)=az+b$.,"Let $f$ an analytic function in a domain $D$ such that $f''(z)=0$ for all $z\in D$ . Prove that there exist constants $a,b\in \Bbb C$ such that $f(z)=az+b$ . Try: If we consider the function $g(z)=f'(z)$ , we have that $g'(z)=f''(z)=0$ in $D$ . And we already have a theorem that guarantees us that $g$ is constant in $D$ , that is to say that $g(z)=a$ for $a\in \Bbb C$ . So, $f'(z)=a$ . But I don't know how to continue from here, any help. I have tried Cauchy equations.","['complex-analysis', 'analytic-functions']"
4567958,Reference book for these types of problems,"From time to time I come across problems similar to (min, max, max min, etc) \begin{equation}
\begin{aligned}
\min_{j,i} \quad & (j-i)\\
\textrm{s.t.} \quad & i \le j\\
  & n \ge i,j \ge 0    \\
  & \textrm{some nonlinear/combinatorial conditions}    \\
\end{aligned}
\end{equation} and I know I can change it to \begin{equation}
\begin{aligned}
\min_{j} \quad  (j- &\max_{i} i)\\
  & \textrm{s.t.} \quad   n \ge j \ge i \ge 0    \\
  & \textrm{some nonlinear/combinatorial conditions}    \\
\end{aligned}
\end{equation} and come up with an algorithm to solve it by iterating over j. Can you please tell me if there is a book in which these rules of separation, associative/commutative/etc, conversion of min to max or max to min, and more general principles  are taught?","['linear-programming', 'combinatorics', 'integer-programming', 'discrete-mathematics', 'optimization']"
4567960,Isotropy group of a totally geodesic submanifold in a Riemannian homogeneous space acts transitively on the submanifold,"I was wondering if the following holds: Let $G/H$ be a (complete) Riemannian homogeneous manifold (i.e., a homogeneous manifold with a prescribed Riemannian metric on it, and $G$ acts transitively on $G/H$ through isometries). Suppose $\xi_0$ is a totally geodesic submanifold of $G/H$ , and let $H_0 = \left\lbrace g \in G | g \cdot \xi_0 = \xi_0 \right\rbrace$ be its isotropy group. Can we say that $H_0$ acts transitively on $\xi_0$ ? It seems to be true in simple cases of $\mathbb{R}^n$ and $\mathbb{S}^n$ . In the first case, $G$ is the group of rigid motions characterized by the matrices of the type $\left[ \begin{matrix} A & b \\ 0& 1 \end{matrix} \right]$ , where $A \in O(n)$ and $b \in \mathbb{R}^n$ , with the action given by $\left[ \begin{matrix} A & b \\ 0& 1 \end{matrix} \right] \cdot x = Ax + b$ , for $x \in \mathbb{R}^n$ . Here, if I consider $\mathbb{R}^{n - 1}$ as a totally geodesic submanifold of $\mathbb{R}^n$ , we get its isotropy group to be $$H_0 = \left\lbrace \left[ \begin{matrix} A & 0 & b \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{matrix} \right] : A \in SO(n-1) \text{ and } b \in \mathbb{R}^{n - 1} \right\rbrace \cup \left\lbrace \left[ \begin{matrix} A & 0 & b \\ 0 & -1 & 0 \\ 0 & 0 & 1 \end{matrix} \right] : A \in O^{-}(n-1) \text{ and } b \in \mathbb{R}^{n - 1} \right\rbrace,$$ where $O^{-}(n-1)$ is the set of all orthogonal matrices with determinant negative. From this characterization, it is easy to see that $H_0$ does act transitively on $\xi_0$ (through translations alone!). Similar computations can be done for the case of $\mathbb{S}^n$ , where the totally geodesic submanifolds are lower dimensional spheres. Now, I want to generalize this to arbitrary Riemannian homogeneous manifolds (that need not be embedded in any Euclidean space, like $\mathbb{S}^n$ ). I have an intuition that this should be true. That is because, just like the case of $\mathbb{R}^n$ , there are group elements from $G$ that take one point to another, and then there are elements from $H_0$ and do not take us outside $\xi_0$ . Therefore, it seems that by choosing proper element, we will remain inside $\xi_0$ and yet be able to go from one point to another using elements of $H_0$ . Any help in formalizing these ideas (or disproving them) will be highly appreciated!","['riemannian-geometry', 'homogeneous-spaces', 'group-actions', 'lie-groups', 'differential-geometry']"
4568034,The largest eigenvalue of a singular random matrix,"Let $X$ be a $m \times n$ random matrix whose entries are i.i.d. random variables with mean $0$ and finite variance $\sigma^2$ . Let $$ Y_n =\frac{1}{n} X X^T.$$ Then according to the Marchenko-Pastur theorem, as $m, n \rightarrow \infty$ and $m/n \rightarrow c \in (0,\infty)$ the largest eigenvalue is bounded by $$ \lambda_{\max} =\sigma^2 \left (1+ \sqrt{\frac{m}{n}} \right )^2. $$ Now let's assume $n$ remains fixed while $m \rightarrow \infty $ . Can one formally show that the largest eigenvalue is not bounded and that its rate of increase is related to $m$ ?","['matrices', 'random-matrices', 'spectral-radius', 'eigenvalues-eigenvectors']"
4568041,Derivation of the exponential distribution from a process with instantaneous probabilities,"Assume we have a process $X$ with two states: $x_1$ and $x_0$ . The process itself is a Markov Process, so its behaviour depends only on the current state, and thus, it can be described by a transition matrix containing the instantaneous probabilities of transition between states. This matrix is usually written as: $$Q_X = \begin{pmatrix}-q_{x_0} & q_{x_0x_1}\\
q_{x_1x_0} & -q_{x_1}\end{pmatrix},$$ where the elements $q_{x_ix_j}$ stand for the probability of transition from state $x_i$ to state $x_j$ provided a transition from the $x_i$ state, and the elements in the diagonal describe the probability on remaining in a state, and are computed such that each row sums up to zero. I know that, in theory, the probability distribution of remaining in a certain state is an exponential. This can be proved by taking the instant probability of transition from $x_i$ to $x_j$ , $p_{x_ix_j}$ which is the probability of transition in $\delta t$ as $\delta t \rightarrow 0$ . In an interval of time $[0,T]$ and with $X(t=0)=x_i$ , the probability of remaining in the same state up to $t=T$ can be expressed as an infinite product of the probability of not transitioning in infinitely many smaller intervals: $$P(\textrm{not transitioning from }X(0)=x_i\textrm{ to }X(T)=x_i) = \lim_{n \rightarrow \infty}\prod_{i=0}^n(1-p_{x_ix_j}\frac{T}{n})=\lim_{n \rightarrow \infty}(1-p_{x_ix_j}\frac{T}{n})^n.$$ The last limit is the definition of the exponential, so, finally: $$P(\textrm{not transitioning from }X(0)=x_i\textrm{ to }X(T)=x_i)=e^{-p{x_ix_j}T}.$$ So far so good, but I can't figure out why the following reasoning is wrong: Instead of taking the instantaneousprobability of transitioning, let's try to get the same result from the instantaneous probability of remaining in the same state, $p_{x_i}$ , which should be $p_{x_i} = 1-p_{x_ix_j}$ (as there are only two states in this scenario). Clearly we can follow the same strategy again: $$P(\textrm{not transitioning from }X(0)=x_i\textrm{ to }X(T)=x_i) = \lim_{n \rightarrow \infty}\prod_{i=0}^n(p_{x_i}\frac{T}{n})=\lim_{n \rightarrow \infty}(p_{x_i}\frac{T}{n})^n,$$ but this time it is clear that the limit approaches $0$ as $n \rightarrow \infty$ , and even if we substitute $p_{x_i} = 1-p_{x_ix_j}$ , the limit vanishes as well. I know there's something phishy about $p_{x_i} = 1-p_{x_ix_j}$ , since it would be fixed in the case that this relation only held when multiplying the instantaneous probabilities by $\delta t = \frac{T}{n}$ , the infinitesimally small interval of time in which they are applied, but on the other hand this would mean that the instantaneous probabilities don't sum up to one, which bugs me as well. I will appreciate any information on the subject, even as sources to check. Thank you in advance.","['markov-process', 'statistics', 'exponential-distribution', 'probability']"
4568053,"Can expected values of the form $E[X^r e^{-sX}]$, for arbitrary $r>0$, uniquely determine the PDF of a random variable $X≥0$?","It is known that the probability distribution of a continuous non-negative random variable, $X$ , is uniquely determined by its associated Laplace transform, $$L(s) = E[e^{-sX}] = \int_0^\infty e^{-sx} f(x) dx,$$ for $s ≥ 0$ . It also seems that: (A) For any positive integer $k$ , $$L^{(k)}(s) = E[X^k e^{-sX}] = \int_0^\infty x^k e^{-sx} f(x) dx$$ uniquely determines the distribution of $X$ if $L^{(k)}(s)$ is well defined and all the lower-order moments ( $E[X^{k-1}]$ , $E[X^{k-2}]$ , ..., $E[X]$ ) are known. (This is because, starting with the higher-order expression $L^{(n)}(s)$ , one can solve for $L^{(n-1)}(s)$ by integration subject to the boundary condition $L^{(n-1)}(0)$ = $(-1)^{n-1}E[X^{n-1}]$ .  Proceeding from $n=k$ to $n=1$ ultimately yields the unique Laplace transform, $L(s) = L^{(0)}(s)$ .) My first question is whether assertion (A) is true; and if not, what is the flaw in the above argument? A second question, assuming (A) is true, is whether the more general assertion (B) is true: (B) For any positive real number $r$ , $$M^{(r)}(s) = E[X^r e^{-sX}] = \int_0^\infty x^r e^{-sx} f(x) dx$$ uniquely determines the distribution of $X$ if $M^{(r)}(s)$ is well defined and all the lower-order moments ( $E[X^{r-1}]$ , $E[X^{r-2}]$ , ..., $E[X^{r-[r]}]$ ) are known. Context and previous efforts: The above questions arose out of investigation of the identifiability of $Gamma(r,\lambda)$ mixture distributions (about which I asked this earlier question: Are continuous mixtures of the gamma distribution identifiable with respect to the scale parameter? ). Further research led to the results of Boas (1939; “On a Generalization of the Stieltjes Moment Problem”, Transactions of the American Mathematical Society ), which provide conditions for $E[X^r]$ , $E[X^{r-1}]$ , ..., $E[X^{r-[r]}]$ to characterize the distribution of $X$ uniquely. Also, I found this helpful question and answer  from a few years ago -- https://math.stackexchange.com/q/2801191 -- which addresses the case in which $L^{(k)}(1)$ is known for all non-negative integers $k$ . Thank you for reading this long post.  Any guidance would be greatly appreciated!","['probability-distributions', 'probability-theory', 'moment-problem', 'laplace-transform']"
4568072,Proof of the continuity axiom in the classical probability model,"I am reading a ""proof"" that in the classical probability model, the probability axioms of Kolmogorov are satisfied. I say ""proof"" because there's a serious flaw there. So I need some clarification i.e. a real rigorous proof (for one of the axioms). Definition: Let $(\Omega, F)$ be a measurable space, where $\Omega = \{w_1, w_2, w_3, ...\}$ is a countable infinite set, and let $F$ be the powerset of $\Omega$ . Let us assume that every elementary event/outcome $w_i$ is mapped to a non-negative number $p(w_i)$ and also let us assume $\sum_{i=1}^n p(w_i) = 1$ (that, I think, means the RHS series is convergent and its sum is 1). Then for every event $A \subseteq \Omega$ we define the probability of $A$ as $P(A) = \sum_{w \in A} p(w)$ OK... now having this definition, we need to prove the following axiom is satisfied. A4: For every sequence of events $A_1 \supseteq A_2 \supseteq A_3 \supseteq ...$ such that $$\bigcap_{i=1}^\infty{A_n} = \emptyset$$ the respective sequence of probabilities $P(A_1), P(A_2), P(A_3), ...$ is decreasing and goes to zero as $n \to \infty$ . Of course proving that the sequence of probabilities $P(A_1), P(A_2), P(A_3), ...$ is decreasing is not a problem. But regarding the limit being zero, I looked in several books, I also searched online. I don't find a decent proof of the fact that the sequence goes to $0$ as $n \to \infty$ . My book basically states that this is obvious because in the series $$P(A_n) = \sum_{w \in A_n} p(w)$$ ""we run out of terms"" as $n \to \infty$ . But that's not really a proof, is it? It's just some intuition-based note. So how do we prove that this axiom A4 is satisfied? Note 1: It seems to me that's actually a real analysis, in particular a series problem but also related to set theory. Somehow I feel like $P(A_n)$ is the remainder term in the series defining $P(A_1)$ which is a convergent series. So $P(A_n)$ must go to zero. But I cannot really formalize this argument, I get confused in my thoughts. For this argument to work, it seems we need to order somehow the elements of $A_1$ by first taking those elements which don't belong to $A_2$ , then those which don't belong to $A_3$ , then those which don't belong to $A_4$ and so on. And then it feels like $P(A_n)$ is somehow that remainder term of the series $P(A_1) = \sum_{w \in A_1} p(w)$ . But as I said, I can't really formalize my intuition. Note 2: Now I am thinking that my major confusion stems from the fact I am not even sure what is the n-th partial sum of this series $$\sum_{w \in A} p(w)$$ E.g. if $A = \{w_1, w_5, w_7, w_{90}, w_{100}, \dots \}$ , is the 5-th partial sum $w_1 + w_5 + w_7 + w_{90} + w_{100}$ , or is it $w_1 + 0 + 0 + 0 + w_5$ ? I think we need to work with the 2nd interpretation when proving that the axioms are satisfied (all axioms, not just A4 which I quoted above). If I use the 1st interpretation (of the partial sum), it's not quite clear how to prove the additivity axiom $P(A \cup B) = P(A) + P(B)$ , when $AB = \emptyset$ . And we must use the additivity axiom to prove A4. Also, it's not clear what is $P(B)$ if $B$ is finite.","['measure-theory', 'probability-theory', 'probability', 'sequences-and-series']"
4568109,is every bijective function a permutation?,"Can I call a continuous bijective function a permutation? For example the function, $$f: \mathbb{R}\rightarrow \mathbb{R} \quad f(x)=2x+1$$ maps the set of real numbers to itself and it's bijective. So is it a permutation? Or are permutations defined only on finite sets?","['permutations', 'functions']"
4568131,Does $\mu_n \overset{*}{\rightharpoonup} \mu$ (or $\mu_n \rightharpoonup \mu$) imply $\{[\mu_n] \mid n \in \mathbb N\}$ is bounded?,"Let $X$ be a metric space, $\mathcal M(X)$ the space of all finite signed Borel measures on $X$ , $\mathcal C_b(X)$ be the space of real-valued bounded continuous functions on $X$ , and $\mathcal C_0(X)$ be the space of real-valued continuous functions on $X$ that vanish at infinity. Then $\mathcal C_b(X)$ and $\mathcal C_0(X)$ are real Banach space with supremum norm $\|\cdot\|_\infty$ . We endow $\mathcal M(X)$ with the total variation norm $[\cdot]$ . Then $(\mathcal M(X), [\cdot])$ is a Banach space . Let $\mu_n,\mu \in \mathcal M(X)$ . We define weak convergence by $$
\mu_n \rightharpoonup \mu \overset{\text{def}}{\iff} \int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu \quad \forall f \in \mathcal C_b(X),
$$ and weak $^*$ convergence by $$
\mu_n \overset{*}{\rightharpoonup} \mu \overset{\text{def}}{\iff} \int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu \quad \forall f \in \mathcal C_0 (X).
$$ Now let $\mu_n,\mu \in \mathcal M(X)$ . If $\mu_n \rightharpoonup \mu$ , then $\mu_n (X) \to \mu (X)$ and thus $\{\mu_n(X) \mid n \in \mathbb N\}$ is bounded. On the other hand, $\mu_n \overset{*}{\rightharpoonup} \mu$ does not necessarily imply $\mu_n (X) \to \mu (X)$ . My questions: Does $\mu_n \rightharpoonup \mu$ imply $\{[\mu_n] \mid n \in \mathbb N\}$ is bounded? Does $\mu_n \overset{*}{\rightharpoonup} \mu$ imply $\{[\mu_n] \mid n \in \mathbb N\}$ is bounded? Thank you so much!","['measure-theory', 'signed-measures', 'weak-convergence', 'borel-measures', 'metric-spaces']"
4568144,Solve for $x: 2^{\sin x+1}+2^{\cos x+1}=2^{3x^2-2x^3}$,"Solve for $x: 2^{\sin x+1}+2^{\cos x+1}=2^{3x^2-2x^3}$ Attempt $1:$ $$2^{\sin x}+2^{\cos x}=2^{3x^2-2x^3-1}$$ Applying AM-GM inequality. $$\frac{2^{\sin x}+2^{\cos x}}2\ge\sqrt{2^{\sin x+\cos x}}\\\implies 2^{3x^2-2x^3-2}\ge2^{-\frac1{\sqrt2}}$$ Not able to proceed from here. Attempt $2:$ $\sin x$ minimum value is $-1$ . Here, $\cos x$ will be zero. Thus, minimum of $2^{\sin x}+2^{\cos x}$ is $\frac52$ Similarly, maximum is $5$ Therefore, $$\frac52\le2^{3x^2-2x^3-1}\le5$$ Not able to proceed from here. The answer given is No Solution.","['calculus', 'functions', 'trigonometry', 'inequality']"
4568163,Deriving the differential equations with solutions corresponding to classical orthogonal polynomials using the orthogonality condition,"I'm trying to understand a bit more about the classical orthogonal polynomials that appear throughout quantum mechanics. My question is whether it is possible to derive the differential equations with solutions corresponding to the classical orthogonal polynomials, using only the orthogonality condition. For example, if I require that a set of functions, $\{f_{n}(x)\}$ are orthogonal on the interval $I=[0,\infty]$ under weighting function $w(x)=e^{-x}$ , $$\int_{I}w(x)f_{n}(x)f_{m}(x)dx=h_{n}\delta_{nm}$$ for some constant $h_{n}$ ,
is it possible to derive Laguerre's equation $$xf_{n}''+(1-x)f_{n}'+nf_{n}=0$$ If so, is this the traditional historic approach, or were these functions discovered seperately, and then were coincidentally found to be orthogonal? Also, if anyone has any resources where they go through this process it would be massively appreciated, thanks.","['orthogonality', 'polynomials', 'ordinary-differential-equations']"
4568186,Normal form for mathematical expressions,"I have a somewhat general question regarding the notation of mathematical expressions. I am interested in further information, that is if you know a book, a pdf, or just a wikipedia page about my question that would help a lot. Of course, if you know a specific answer feel free to share. My question is: Is there a normal form / convention for the notation of mathematical expressions? As an example: $\frac{\sqrt{x^2+2xy+y^2}}{z}$ $\frac{x+y}{z}$ $\frac{x}{z}+\frac{y}{\sqrt{z^2}}$ Those three expressions are mathematically equal but semantically different. Is there a normal form i.e. a rule set that - when given a those three expressions - defines one ""normalized"" way to write them down? And if there is, is there an associated normalization algorithm that would, with absolute certainty, convert these three examples (that are mathematically equal) into the same (syntactically equal) expression? The resulting expression does not necessarily have to be the ""simplest form"" that contains the least amount of symbols or whatever. I am also aware that there are probably many different ""normal forms"". I am just searching for one of them. Normal forms exist in expression logic (disjunctive, conjunctive,...). I am searching for something analogous for math expressions. Thanks for the help in advance :) Edit:
I realized that my question is a bit to general. Maybe it is easier to first limit the expressions to algebraic expressions, that is expressions involving integer constants, variables, and the operations addition subtraction multiplication division exponentiation by a rational","['algebra-precalculus', 'algorithms']"
4568221,Evaluating double integral on different domains $D$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question $$\iint\limits_{D} \left(x^2+y^2\right)\mathrm{d}x \mathrm{d}y$$ where $D$ is given each time by $D=x^2-y^2=1,\hspace{0.5cm} x^2-y^2=9,\hspace{0.5cm} xy=2,\hspace{0.5cm} xy=4$ I try to use Polar coordinate transformation \begin{cases}
   x &= \rho \cos \theta  \\
   y &= \rho \sin \theta
\end{cases} but I don’t know how to find the range of $\rho$ and $\theta$","['integration', 'multivariable-calculus']"
4568242,Using calculus notation arithmetically,"Today in microeconomics class the following question was covered by the tutor: If $z = x^2 + y^2$ changes from $(1,2)$ to $(1.05,2.1)$ , compute the value of $\Delta z$ and $dz$ . If we write $f(x,y)=z$ , then simply $\Delta z = f(1.05,2.1) - f(1,2)$ . However in the case of $dz$ things get interesting. The tutor derived the formula: $$ df(x,y) = \frac{d}{dx}f(x,y)dx + \frac{d}{dy}f(x,y)dy \\
= 2xdx + 2ydy $$ Then we filled in $x=1,y=2,dx=0.05,dy=0.1$ to get $dz=df(1,2)=0.5$ (actually we arrived at $0.9$ but I can't see how that happened). The idea is that it is an approximation of $\Delta z$ . I was under the impression that you weren't allowed to use notation like this, to assign values to e.g. $dx$ . When I pressed the tutor about it he said he doesn't think  it's mathematically sound but that this is what the lecturer wants to see.",['multivariable-calculus']
4568244,"Let $x_i \in \mathbb Z,$ such that $|x_1|+|x_2|+|x_3|+...+|x_{10}|=100.$ Find number of solution.","Let $x_i \in \mathbb Z,$ such that $|x_1|+|x_2|+|x_3|+...+|x_{10}|=100.$ Find number of solution. I found this problem in Book named Pathfinder for Olympiad mathematics. I tried using making cases but there are many cases. Then i see the answer key but i don't know how did the obtained the following result. $$\sum_{r=0}^{9}2^{10-r}\cdot {10\choose r}\cdot{99\choose 9-r}$$ Also is there any other method to solve given problem?","['permutations', 'combinations', 'combinatorics']"
4568246,"Let $X$ and $Y$ be non negative i.i.d random variables such that $E[X]<\infty$. Show that $E[min(X,Y)^2]<\infty$","Let $X$ and $Y$ be non negative i.i.d random variables such that $E[X]<\infty$ . Without assuming that $E[X^2]<\infty$ , show that $E[\min(X,Y)^2]<\infty$ . We defined the expectation as:
Let $X:\Omega\rightarrow S$ be a random element of $(S,\mathcal{S})$ with distribution $\mu$ and let a measurable function $h:S\rightarrow \mathbb{R}$ then $$E[h(x)]=\int_{S}h(x)\mu(dx)$$ whenever LHS or RHS are well defined. First of all I am actually clueless on how to proceed to solve the question. But this might be due to the fact that I'm not so comfortable with this definition. I don't really understand what $\mu$ is and what does $\mu(dx)$ represent. I also feel like we should suppose that $E[Y]<\infty$ as well.","['expected-value', 'lebesgue-integral', 'probability-theory', 'probability']"
4568254,$\mathbb{E} \big[ e^{\mathbf{x}^T M_1 \mathbf{x} } \big] \hspace{5 pt}{\rm v.s.} \hspace{5 pt}\mathbb{E} \big[ e^{\mathbf{x}^T M_2 \mathbf{x} } \big]$,"Let $p(x)$ be a distribution with zero mean, $\int x p(x) \, dx = 0$ . Consider two $n \times n$ symmetric matrices $M_1, M_2$ . Is there anyway to compare the following two expectations: $$
\mathbb{E} \big[ e^{\mathbf{x}^T M_1 \mathbf{x} } \big] \hspace{5 pt}{\rm v.s.} \hspace{5 pt}\mathbb{E} \big[ e^{\mathbf{x}^T M_2 \mathbf{x} } \big]
$$ where $\mathbf{x}$ is a vector in $\mathbb{R}^n$ with i.i.d. components distributed according to $p$ . I am looking for a way to relate this comparison to comparing the determinant of the matrices $M_1,M_2$ . An additional assumption with might help: both $M_1, M_2$ has 0 diagonal, and the other entries are non-negative.","['expected-value', 'probability-distributions', 'probability-theory', 'probability']"
4568309,Count number of ways to distribute n distinct positive integers into $r$ identical bins such that the product of integers in each bin is $\le M$,"Problem Statement: We have $n$ distinct positive integers say $a_1,a_2....a_n$ and a given integer value $M$ .
We have to count number of ways to distribute these integers to $r$ identical bins subject to the constraints that the product of integers in any bin can not exceed $M$ and also no bin should be empty and we must distribute all the integers to some bin or the other. My thoughts: I believe this is a type of distribution of distinct objects into identical bins but with a constraint on each bin. The constraint function here is the product of integers in the bin, but it can be sum of integers in a bin or number of integers in a bin or anything else.
Like say this or this the problem's limiting function is number of balls in a bin. But unlike our case in both of them balls are identical Approach1 on top of my mind: As point out by @Sil If the problem was about limit on sum the following approach would have worked though not very efficient. Lets say my integers are $2,5,7,11$ . If $r=1$ then its equivalent to counting sum of all coefficients of $x^i$ for $1 \le i\le M$ in $(1+x^2)(1+x^5)(1+x^7)(1+x^{11})$ Similarly if $r=2$ then its equivalent to counting sum of all coefficients of $x_1^ix_2^j$ for $1 \le i,j\le M$ and  in $(1+x_1^2+x_2^2)(1+x_1^5+x_2^5)(1+x_1^7+x_2^7)(1+x_1^{11}+x_2^{11})$ ...... ...... Similarly if $r=r$ then its equivalent to counting sum of all coefficients of $x_1^ix_2^j..x_r^w$ for $1 \le i,j,..w\le M$ and  in $(1+x_1^2+x_2^2+..+x_r^2)(1+x_1^5+x_2^5+..+x_r^5)(1+x_1^7+x_2^7+..+x_r^7)(1+x_1^{11}+x_2^{11}+..+x_r^{11})$ This is $O(r^n)$ , not very efficient to say the least, so there surely exists more efficient ways to get the result. Another way of thinking is: I think it can be solved  by PIE let's call the resut $f(r$ , set of all bins $)$ , we count unrestricted, then fix a particular bins split its restrcted and unrestricted case an then call $f(r-1$ , remaining $r-1$ bins $)$ , now fix two boxed and add back the same $f(r-2$ , remaining $r-2$ bins $)$ and so on. My idea are still vauge, needs little bit more thought. Another problem: I have another part of this problem, let me know if solving this is easier.
Lets say we have the same numbers $a_1,a_2....a_n$ and the same $M$ . Now if I say we dont care about number of bins you use. Form new set of numbers fusing together(i.e product I mean) two or more of them such that no such fused number is greater than M. How many such sets can you form. Here we must use all the numbers. I was initially thinking about solving the first part and then sum for each r=1 to r for this part but now I feel it might be easier to approach this directly. Example clarifying this part: intial_nos = {2,5,7,11}, M =70 case 1:{2,5,7,11} case 2:{10,7,11} case 3:{5,14,11} case 4:{5,7,22} case 5:{2,11,35} case 6:{2,7,55} case 7:{70,11} So answer should be 7","['number-theory', 'set-partition', 'combinatorics', 'discrete-mathematics']"
4568338,Is there a near-repdigit prime of every size?,"Define a near-repdigit prime as a prime number where all but one of the digits are the same. For example $199, 94999$ and $76777$ are near-repdigit primes. Is there a near-repdigit prime with $n$ digits for all $n>1$ ? I realise that a lot of these types of questions are open. Is this in fact an open question? A stronger conjecture is the following. Does there in fact exist a near-repdigit prime with $n$ digits whose first digit is $1$ for all $n>1$ ? I have checked for near-repdigit primes starting with the digit 1 and there is at least one for all $n \leq 3495$ . Following a suggestion of @ErickWong I counted how many counter-examples there are for both the stronger conjecture and the original conjecture  in base 2,3, 4 and 5 for $2 \leq n \leq 2000$ . base 2: Stronger 269, original 269 base 3: Stronger 58, original 58 base 4: Stronger 3, original 3 base 5: There are 0 counter-examples for both the stronger and the original conjecture. For every case where there is no near-repdigit prime with the first digit being 1, there is no near-repdigit prime with that number of digits and in that base at all.","['number-theory', 'prime-numbers']"
4568344,What is the name for this process of creating an equivalence relation from a given relation?,"I have a relation $\sim$ on $X$ that is reflexive and symmetric. I want to form an equivalence relation $\approx$ from $\sim.$ For my purposes, the transitive closure of $\sim$ is too coarse, that is, its classes are too large. So I define $x\approx y$ if $x\sim y$ and for all $z\in X,$ we have $x\sim z$ if and only if $y\sim z.$ The relation $\approx$ so defined is an equivalence relation that is finer than the transitive closure of $\sim.$ Is there a name for the process I described above? Basically you throw out all relations that do not respect transitivity, rather than throw in extra relations to assure transitivity, the latter process being ""taking the transitive closure"". I also suspect that $\approx$ is the coarsest equivalence relation that is finer than $\sim,$ but I'm not sure that that is true and it is not important for my purposes.","['elementary-set-theory', 'relations', 'terminology']"
