question_id,title,body,tags
4834151,Characterizing a Function Using the Gradient,"I am watching one of Ted Shifrin's multivariable calculus lectures and I have a question about some of the reasoning used in one of the examples. I will state the reasoning below. Suppose $f:\mathbb{R}^2 \to \mathbb{R}$ is differentiable and $$x^2 \frac{\partial f}{\partial x} - y \frac{\partial f}{\partial y}  = 0$$ everywhere. We can use the gradient to determine what the graph of $f$ looks like.
Note that this equation above may be rewritten as: $$\nabla{f} \cdot \begin{bmatrix}x^2 \\ -y\end{bmatrix}=0$$ Thus, we see that $\begin{bmatrix}x^2 \\ -y\end{bmatrix}$ is tangent to the level curve through $\begin{bmatrix}x \\ y\end{bmatrix}$ . On a level curve, the slope at $\langle{x,y}\rangle$ is $-\frac{y}{x^2}$ . Suppose that the level curve is given by $y = g(x)$ . Then $$g'(x) = \frac{dy}{dx} = -\frac{y}{x^2}$$ We use separation of variables to compute $$ \begin{align*}\frac{dy}{dx} = -\frac{y}{x^2} &\iff \frac{1}{y} dy = -\frac{1}{x^2}dx \\ 
\log|y| &= \frac{1}{x} + C\end{align*} $$ Exponentiating, we see $$|y| = ce^{\frac{1}{x}}, \quad \text{ for } c \in \mathbb{R}$$ Consider an arbitrary differentiable function $\Phi(y), \enspace  y\geq 0$ . Setting $f(1,y) = \Phi(y)$ should uniquely determine $f$ for all $x,y \geq 0$ . Observe that $\Phi$ gives us information regarding the behavior of $f$ on the line $x=1$ . Given $\langle{X,Y}\rangle$ , we can determine the constant associated with the level curve by considering the restriction $$Y=ce^{\frac{1}{X}} \implies c = e^{-\frac{1}{X}}Y$$ Now, along the line $x=1$ , we know that it must intersect the level curve at $y = ce^{\frac{1}{1}}= ce$ . So, we see that $f(X,Y) = \Phi(e\cdot e^{-\frac{1}{X}} Y) $ . In summary: $$f(x,y) = \Phi(e^{1-\frac{1}{x}}y)$$ My questions are: How did we know to consider the form $y=g(x)$ for the level curve? In fact, we do not exactly get a function as our answer. Is the purpose of this just to specify some arbitrary function and then consider what possible functions satisfy the condition given by our knowledge of the slope at different points? How was $f(1,y) = \Phi(y)$ chosen? Could we have considered a line other than $x=1$ for our definition of $\Phi$ and determined functions that satisfy this differential equation? Why does considering the point $(1, ce)$ and $\Phi(ce)$ happen to give us the correct equation for $f(x,y)$ ?",['multivariable-calculus']
4834165,Blowing up along a point in the special fiber,"Let $p$ be a prime number, and let $\mathbb{Z}_p$ be the $p$ -adic numbers. In concrete terms, what is the blow up of $\mathbb{A}^1_{\mathbb{Z}_p} = \operatorname{Spec} \mathbb{Z}_p[x]$ along the closed subscheme corresponding to the ideal $(p,x)$ ? Edit: For context, this question comes up in the study of Néron models, but few authors bother to spell out all the details. Since my training in classical algebraic geometry is somewhat rudimentary it's not entirely clear to me how the construction goes. I looked in some standard references on algebraic geometry, but most of the examples are in the geometric context where one works over a field, and I don't immediately see how to convert these computations to my setting. I think the answer to this question should not be difficult, but I also could not find any similar examples worked out in detail (although I may not have looked hard enough). It might be helpful to more people than myself to have the answer available here.","['algebraic-geometry', 'arithmetic-geometry']"
4834174,"Prove the zeta log gamma integral $\int_0^1{\zeta(1-n,1-t)\ln\Gamma{(t)}}dt = \frac{H_n\zeta(-n)+\zeta’(-n)}{n}$","How to prove that $\displaystyle\int_0^1{\zeta(1-n,1-t)\ln\Gamma{(t)}}dt = \frac{H_n\zeta(-n)+\zeta’(-n)}{n}$ ? For integer values Wolfram Alpha gave me solutions to the integral in the form on the right hand side, so I guessed the form. It appears to break down when $n$ is less than around $-\frac12$ . $\zeta(s,a)$ and $\zeta(s)$ are the Hurwitz zeta and Riemann zeta functions. $H_n$ are the harmonic numbers. How can we prove it? Additional info In order to obtain the closed form for integer $n$ , in Wolfram Alpha, we can first rewrite it in terms of the Polygamma function. First change the zeta function to Bernoulli Polynomials for easier manipulation. $$\begin{align}
&\frac{1}{\left(n-1\right)!}\int_{0}^{1}\zeta\left(1-n,1-t\right)\ln\Gamma\left(t\right)dt
\\=&
\ -\frac{1}{n!}\int_{0}^{1}B_n\left(1-t\right)\ln\Gamma\left(t\right)dt
\\=&
\ \frac{1}{n!}\int_{0}^{1}n\left(1-t\right)^{n-1}\ln\Gamma\left(t\right)dt-\frac{1}{n!}\int_{0}^{1}B_n\left(2-t\right)\ln\Gamma\left(t\right)dt
\end{align}$$ This result can actually be generalized a bit by introducing the variable $a$ . $$\begin{align}
&\frac{1}{n!}\int_{0}^{a}n\left(a-t\right)^{n-1}\ln\Gamma\left(t\right)dt-\frac{1}{n!}\int_{0}^{1}B_n\left(a+1-t\right)\ln\Gamma\left(t\right)dt
\\=&
\ \psi^{(-n-1)}\left(a\right)-\frac{1}{n!}\int_{0}^{1}B_n\left(a+1-t\right)\ln\Gamma\left(t\right)dt
\end{align}$$ where $\psi^{(s)}(a)$ is the polygamma function. Here we are using this definition for negative order. Next we can utilize the definition of the bernoulli polynomials to obtain. $$\begin{align}
&\psi^{(-n-1)}\left(a\right)-\frac{1}{n!}\int_{0}^{1}\sum_{k=0}^{n}\binom{n}{k}B_k\left(a\right)\left(1-t\right)^{n-k}\ln\Gamma\left(t\right)dt
\\=&
\ \psi^{(-n-1)}\left(a\right)-\sum_{k=0}^{n}\frac{B_k\left(a\right)}{k!}\left(\frac{1}{\left(n-k\right)!}\int_{0}^{1}\left(1-t\right)^{n-k}\ln\Gamma\left(t\right)dt\right)
\\=&
\ \psi^{(-n-1)}\left(a\right)-\sum_{k=0}^{n}\frac{B_k\left(a\right)}{k!}\psi^{(-n-2+k)}\left(1\right)
\end{align}$$ This last form is useful for plugging into Wolfram Alpha and generating our desired form. After analyzing patterns for several integers we see it equals $$\begin{align}
&-H_n\frac{B_{n+1}\left(a\right)}{\left(n+1\right)!}+\frac{\zeta'\left(-n\right)}{n!}+\frac{1}{n!}\sum_{k=1}^{a-1}k^{n}\ln k
\\=&
\ \frac{H_n\zeta\left(-n,a\right)}{n!}+\frac{\zeta'\left(-n,a\right)}{n!}
\end{align}$$ Multiplying by $(n-1)!$ and setting $a=1$ yields the final result.","['integration', 'definite-integrals', 'special-functions', 'polygamma', 'riemann-zeta']"
4834220,Proof for Discrete Math Noticing,"The other day while biking I realized that if you multiply a single digit number by a multi digit number, and then add the digits together until you got to a single digit number, it would have the same final digit as if you added the multi digit number together before multiplying. For example, $423 \cdot 7=2961, 2+9+6+1=18, 1+8=9$ , while $(4+2+3) \cdot 7=63, 6+3=9$ . How would one go about writing a proof for this, or does one already exist? I'm not super into math, just like playing with numbers while I ride my bike, so thought this was interesting, and was curious as to the why of it.",['discrete-mathematics']
4834222,Describe the class of groups that satisfy $(x^2y^2)^2 \approx 1$,"I have trouble with the following assignment: Let $A$ denote the class of all Abelian groups satisfying the identity $x^2 \approx 1$ . Show that the class $$\{G \mid \exists N: N \trianglelefteq G \land N \in A \land G/N \in  A\}$$ is precisely the class (variety) of all groups satisfying the identity $(x^2y^2)^2 \approx 1$ I have no trouble showing that any group from the first class satisfies the identity in the second class, but I can not show the converse. Any time I come up with a subgroup it either fails to be normal (or I just can’t prove it), or it is normal subgroup and is element of $A$ , but the quotient group fails to be in $A$ (or I can not prove it to be true). My best attempts: $$N = \{ x \in G \mid x^2 = 1 \land \forall y \in G: xy=yx\}$$ One can easily verify that this is a normal subgroup of $G$ that is in $A$ , but I don’t think that $G/N \in A$ because $\forall y \in G/N \,\exists g \in G: y = Ng$ . Then $y^2 = 1$ iff $(Ng)^2 = N$ iff $N(g^2)=N$ iff $g^2 \in N$ (I don’t think this is true in general). So from this maybe we can force $N$ to contain $\{x^2\mid x \in G\}$ by taking something like $N = \langle\{x^2\mid x \in G\}\rangle$ , but I dont see how this would be normal or satisfy the identity $x^2 \approx 1$ . Thank you for suggestion!","['universal-algebra', 'group-theory', 'model-theory']"
4834225,Periods of difference sequences modulo N,"I am looking at what happens when the cyclic 1st order difference is repeatedly applied modulo $N$ to an initial sequence of length $N$ . The cyclic 1st order difference is the difference between consecutive elements assuming the sequence is cyclic. That means $x_{n+1}(1) = x_n(2)-x_n(1)$ , $x_{n+1}(2) = x_n(3)-x_n(2)$ , and the last element is $x_{n+1}(N) = x_n(1) - x_n(N)$ (the modulo operation is omitted for convenience). For example, if $N$ = 6 and the initial sequence is $x = \begin{array}{r}\{1&0&0&0&0&0\}\end{array}$ then the output eventually cycles through the six states $x_n = \begin{array}{r}\{5,1,1,4,2,5\}\end{array}$ $x_{n+1} = \begin{array}{r}\{2,0,3,4,3,0\}\end{array}$ $x_{n+2} = \begin{array}{r}\{4,3,1,5,3,2\}\end{array}$ $x_{n+3} = \begin{array}{r}\{5,4,4,4,5,2\}\end{array}$ $x_{n+4} = \begin{array}{r}\{5,0,0,1,3,3\}\end{array}$ $x_{n+5} = \begin{array}{r}\{1,0,1,2,0,2\}\end{array}$ Since for any N the number of states is finite they will eventually repeat with a fixed period, or end up as all zeros. I have an efficient implementation in C++ that can handle $N$ up to just over 100. Using an impulse as the initial state I have noticed the following two properties empirically. iff $N$ is a prime power the end state is zero the period is always a multiple of $N$ Here are the values of the period $p$ divided by $N$ for 50 <= $N$ <= 70. N=50, p=10,230 N=51, p=13,120 N=52, p=126 -- 53 prime N=54, p=4,599 N=55, p=284 N=56, p=12 N=57, p=118,092 N=58, p=229,362 -- 59 prime N=60, p=4 -- 61 prime N=62, p=5 N=63, p=2,964 -- 64 prime power N=65, p=336 N=66, p=3,410 -- 67 prime N=68, p=60 N=69, p=169,444 N=70, p=58,032 The highest period calculated by the C++ program is for N=106, p=1,744,830,438, corresponding to around 180 billion iterations of the difference operator. The smallest $N$ for which the period is so large that the C++ program does not find it during a night of number crunching is 115. My hope is to be able to find a way to calculate the period from $N$ and the initial state. As far as I can tell there is no obvious pattern in the periods I have already calculated. Any suggestions or links to relevant information are much appreciated.","['combinations', 'signal-processing', 'modular-arithmetic', 'discrete-mathematics']"
4834275,Exponential map of an $SO(n)$-invariant metric on $\mathbb R^n$ is diffeomorphism,"Let $g$ be a complete $SO(n)$ -invariant metric on $\mathbb R^n$ . That is, for each $A \in SO(n)$ , the map $\varphi_A:\mathbb R^n \rightarrow \mathbb R^n$ given by $x \mapsto Ax$ is a Riemannian isometry of $(\mathbb R^n,g)$ . Let $\exp_0:T_0\mathbb R^n \rightarrow \mathbb R^n$ denote the Riemannian exponential map at zero. Recall that $\exp_0(v) := \gamma_v(1)$ , where $\gamma_v:\mathbb R \rightarrow \mathbb R^n$ is the unique maximal geodesic starting at zero with initial velocity $v$ . Since $g$ is complete, $\exp_0$ is surjective (there exists a geodesic connecting any point in $\mathbb R^n$ to zero). It is well-known that $\exp_0$ is a diffeomorphism if restricted to a sufficiently small neighbourhood around zero. Question: Is $\exp_0$ a diffeomorphism? (I know that it remains to show $\exp_0$ is injective and a local diffeomorphism, but I'm not sure how to show these.) EDIT: I believe the answer is yes and I have come up with a solution. Please let me know if I have made a mistake somewhere. Consider the inner product space $(T_0 \mathbb R^n,g_0)$ . Observe that the isotropy subgroup of $0 \in \mathbb R^n$ is the entire group $SO(n)$ . Thus, $SO(n)$ acts by linear isometries on $(T_0 \mathbb R^n,g_0)$ via $A \cdot v := d \varphi_A(v)$ .
For each $r > 0$ , let us define $$S_r(0) := \left\{v \in T_0 \mathbb R^n \mid g_0(v,v) = r^2\right\}.$$ Lemma 1. For any $r > 0$ , $SO(n)$ acts transitively on $S_r(0)$ . Proof. First, let us identify $T_0 \mathbb R^n$ with $\mathbb R^n$ via the linear isomorphism $\partial_i|_0 \mapsto e_i$ . Here, $\partial_i|_0$ are the coordinate vectors induced by the identity chart, and $e_i$ denote the standard basis for $\mathbb R^n$ .
Recall that under this identification, for each $A \in SO(n)$ , the differential $d\varphi_A|_0:T_0 \mathbb R^n \rightarrow T_0\mathbb R^n$ is just $\varphi_A:\mathbb R^n \rightarrow \mathbb R^n$ , because $\varphi_A$ is linear. Thus, under this identification, the action of $SO(n)$ on $T_0 \mathbb R^n \cong \mathbb R^n$ is exactly the same as the original action. We know that $SO(n)$ acts transitively on the Euclidean spheres $\mathbb S^{n-1}(R)$ . Thus, we are done if we show that $g_0$ is a multiple of the standard dot product $\bullet$ on $\mathbb R^n$ . Set $c := g_0(e_1,e_1)$ . Fix an arbitrary element in $\mathbb R^n$ , which can be written as $tv$ , with $t \geq  0$ and $v\bullet v = 1$ . Since $SO(n)$ acts transitively on $\mathbb S^{n-1}(1)$ , there exists $A \in SO(n)$ such that $Av = e_1$ . Therefore, since each $d\varphi_A|_0=\varphi_A$ is a linear isometry of $(T_0 \mathbb R^n\cong \mathbb R^n,g_0)$ , we find $$g_0(tv,tv) = g_0(Atv, Atv) = t^2 g_0(Av,Av) = t^2 g_0(e_1,e_1) =  t^2 c = c (tv \bullet tv),$$ as desired. $\square$ Lemma 2. Fix $R > 0$ . Then there exists some $r > 0$ such that the restriction $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is a surjective smooth submersion. Proof. Since $\exp_0$ is surjective, there exists a unit speed geodesic $\gamma:\mathbb R \rightarrow \mathbb R^n$ such that $\gamma(0) = 0$ and $\gamma(r) \in \mathbb S^{n-1}(R)$ for some $r > 0$ . Let $v := \gamma'(0)$ . First, let us show that the image of $S_r(0)$ is contained in $S^{n-1}(R)$ . To this end,fix an arbitrary element of $S_r(0)$ , which can be written as $rw$ , where $r > 0$ and $w \in S_1(0)$ . We wish to show that $\exp_0(rw) \in \mathbb S^{n-1}(R)$ . By the previous lemma, we know that $SO(n)$ acts transitively on $S_1(0)$ , so there exists some $A \in SO(n)$ such that $w = d \varphi_A(v) = (\varphi_A \circ \gamma)'(0).$ Uniqueness of geodesics implies that the geodesic $\varphi_A \circ \gamma$ is given by $t \mapsto \exp_0(tw)$ . In particular, $A \gamma(r) = (\varphi_A \circ \gamma)(r) = \exp_0(rw)$ . Thus, $\exp_0(rw)$ belongs to $\mathbb S^{n-1}(R)$ . Next, let us show surjectivity. Suppose $p \in \mathbb S^{n-1}(R)$ . Then there exists $A \in SO(n)$ such that $p = A\gamma(r) = (\varphi_A \circ \gamma)(r)$ . We know that $\varphi_A \circ \gamma$ is a unit speed geodesic starting at zero. Set $w := (\varphi_A \circ \gamma)'(0)$ . By uniqueness of geodesics, we know that $\varphi_A \circ \gamma$ is given by $t \mapsto \exp_0(tw)$ . In particular, $\exp_0(rw) = (\varphi_A \circ \gamma)(r) = p$ . Now, let us show that $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is a smooth submersion. Since we have shown surjectivity, the Global Rank Theorem tells us that it suffices to show that $\exp_0|_{S_r(0)}$ has constant rank. By Lemma 1, we know that $SO(n)$ acts transitively on the domain $S_r(0)$ . Thus, by the Equivariant Rank Theorem, it suffices to show that $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is $SO(n)$ -equivariant. Fix $rv \in S_r(0)$ , where $r > 0$ and $v \in S_1(0)$ . Then $$A\cdot \exp_0(rv) = (\varphi_A \circ \exp_0)(rv) = (\exp_0 \circ d \varphi_A |_0)(rv) = \exp_0(A \cdot rv),$$ where we have used the naturality of the exponential map in the second equality. $\square$ Lemma 3. Fix $R > 0$ . Let $\gamma: \mathbb R \rightarrow \mathbb R^n$ be a unit speed geodesic such that $\gamma(0) = 0$ and $\gamma(r) \in \mathbb S^{n-1}(R)$ . Then $\gamma'(r)$ is orthogonal to $T_{\gamma(r)}S^{n-1}(R)$ . Proof. Let $v := \gamma'(0)$ . By Lemma 2, we know that $$d (\exp_0|_{S_r(0)})|_{rv}: T_{rv} S_r(0) \rightarrow T_{\gamma(r)} \mathbb S^{n-1}(R)$$ is a linear isomorphism. Let $w \in T_{\gamma(r)} \mathbb S^{n-1}(R)$ . Then there exists $u \in T_{rv} S_r(0)$ such that $dexp_0|_{rv}(u) = w$ . The Gauss Lemma tells us that $$g_{\gamma(r)}(w, \gamma'(r)) = g_{\gamma(r)}(d\exp_0|_{rv}(u),d\exp_0|_{rv}(v) ) = g_0(u,v) = 0.$$ This completes the proof. $\square$ Lemma 4. Let $\gamma: \mathbb R \rightarrow \mathbb R^n$ be a geodesic such that $\gamma(0) = 0$ . Then $\gamma(r) \neq 0$ for all $r > 0$ . Proof. For the sake of contradiction, suppose that $\gamma(r) = 0$ for some $r > 0$ . Since $[0,r]$ is compact and $\gamma$ is continuous, we know that the image $\gamma([0,r])$ is bounded in the Euclidean norm $\| \cdot \|$ . Thus, there exists $R > 0$ such that $\|\gamma(t) \| < R$ for all $t \in [0,r]$ . Now, fix $p \in \mathbb S^{n-1}(R)$ . Since $(\mathbb R^n,g)$ is complete, there exists a unit speed geodesic $\alpha: \mathbb R \rightarrow \mathbb R^n$ such that $\alpha(0) = 0$ , $\alpha(s) = p$ for some $s > 0$ , and $\alpha|_{[0,s]}$ is distance minimising with respect to $g$ . Choose $A \in SO(n)$ so that $\gamma'(0) = d\varphi_A(\alpha'(0)) = (\varphi_A \circ \alpha)'(0)$ . By uniqueness of geodesics, we know that $\gamma = \varphi_A \circ \alpha$ . Therefore, $\gamma|_{[0,s]}$ minimises the distance (with respect to the metric $g$ ) between $0$ and $\gamma(s) \in \mathbb S^{n-1}(R)$ . It follows that $r < s$ . Consider the reparameterisation $\beta: \mathbb R \rightarrow \mathbb R^n$ given by $\beta(t) := \gamma(t + r)$ . Then $\beta$ is a unit speed geodesic such that $\beta(0) = 0$ and $\beta(s-r) = \gamma(s-r + r) = \gamma(s)$ . However, observe that $$L_g(\beta|_{[0,s-r]}) = s-r < s = L_g(\gamma|_{[0,s]}),$$ so this contradicts the fact that $\gamma$ is distance minimising. $\square$ Proposition. The exponential map $\exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n$ is injective. Proof. For the sake of contradiction, suppose that $\exp_0(rv) = \exp_0(sw)$ for some $r,s \geq 0$ and $v,w \in S_1(0)$ . Let $\alpha,\beta:\mathbb R \rightarrow \mathbb R^n$ denote the unit speed geodesics given by $\alpha:t \mapsto \exp_0(tv)$ and $\beta:t \mapsto \exp_0(tw)$ . Then $\alpha(r) = \beta(s)$ .
If $\alpha(r) = \beta(s) = 0$ , then Lemma 4 implies that $r = s$ , so $rv = sw$ . Thus, let us suppose $\alpha(r) =\beta(s)$ is non-zero. Then $r,s > 0$ . Fix $R>0$ such that $\alpha(r) = \beta(s)$ belongs to $\mathbb S^{n-1}(R)$ .  By Lemma 3, we know that $\alpha'(r)$ and $\beta'(s)$ are both orthogonal to $T_{\alpha(r)}\mathbb S^{n-1}(R)$ . Since $\mathbb S^{n-1}(R)$ is a codimension one embedded submanifold, we conclude that either $\alpha'(r) = \beta'(s)$ or $\alpha'(r) = - \beta'(s)$ . Let us show that in $rv = sw$ in the first case, and the second case leads to contradiction. Now, suppose $\alpha'(r) = \beta'(s)$ . Without loss of generality, suppose $r \leq  s$ . Define $\gamma: \mathbb R \rightarrow \mathbb R^n$ to be the unit speed geodesic given by $\gamma(t) := \beta(t + s - r)$ . Now, observe that $\gamma(r) = \beta(s) = \alpha(r)$ and $\gamma'(r) = \beta'(s) = \alpha'(r)$ . Uniqueness of geodesics implies that $\gamma = \alpha$ . In particular, $0 = \beta(0) = \gamma(s-r) = \alpha(s-r)$ . Thus, by Lemma 4, we must have $s = r$ . Therefore, $\alpha = \gamma = \beta$ . In particular, $v = \alpha'(0) = \beta'(0) = w$ , so $sv = rw$ , as desired. Finally, suppose that $\alpha'(r) = -\beta'(s)$ . Let $\gamma: \mathbb R \rightarrow \mathbb R^n$ be the unit speed geodesic given by $\gamma(t) := \beta(-t + s + r)$ . Then observe that $\gamma(r) = \beta(-r + s + r) = \beta(s) = \alpha(r)$ and $\gamma'(r) = - \beta'(-r + s + r) = -\beta'(s) = \alpha'(r)$ , so uniqueness of geodesics implies that $\gamma = \alpha$ . In particular, $\alpha(s + r) = 0$ . However, Lemma 4 implies that this is a contradiction, since $s + r > 0$ . $\square$ Proposition. The exponential map $\exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n$ is a diffeomorphism. Proof. We have shown that $\exp_0:T_0 \mathbb R^n \rightarrow \mathbb R^n$ is bijective. By the Global Rank Theorem, it suffices to show that $\exp_0$ has constant rank. Thus, fix $r > 0$ and $v \in S_1(0)$ , and let $\gamma: \mathbb R \rightarrow \mathbb R^n$ denote the unit speed geodesic given by $\gamma(t) := \exp_0(tv)$ . Let $R > 0$ be such that $\gamma(r) \in \mathbb S^{n-1}(R)$ . Consider the differential $d\exp_0|_{rv}:T_0 \mathbb R^n \rightarrow T_{\gamma(r)} \mathbb R^n$ . Let $e_2,\ldots,e_n$ denote unit vectors in $T_0 \mathbb R^n$ such that $\gamma'(0),e_2,\ldots,e_n$ forms an orthonormal basis for $T_0 \mathbb R^n$ . Then we know that $e_2,\ldots,e_n$ must belong to $T_{rv} S_r(0)$ . By Lemma 2, we know that $\exp_0|_{S_r(0)}:S_r(0)  \rightarrow  \mathbb S^{n-1}(R)$ is a submersion, so in particular, $d \exp_0|_{rv}(e_2),\ldots,d \exp_0|_{rv}(e_n)$ forms a basis of $T_{\gamma(r)} \mathbb S^{n-1}(R)$ . Since $\gamma'(r) = d \exp_0|_{v}$ is orthogonal to these vectors, it follows that $d\exp_0|_{rv}$ is a linear isomorphism. This completes the proof. $\square$","['geodesic', 'diffeomorphism', 'riemannian-geometry', 'smooth-manifolds', 'differential-geometry']"
4834290,Expected number of tosses under a moving-window-based stopping condition,"Given two integers $N \geq M \gt 0$ and $0<p<1$ . Suppose I keep tossing a coin with a head probability $p$ until I get $M$ heads in the most recent $N$ tosses. What is the expected number of tosses? The game is stopped immediately if $M$ heads are reached before $N$ tosses are attempted. The question is not from my class or exercise; It just appears in various occasions in my everyday life that pique my curiosity. An example is the ""mastery"" in Minesweeper online , which is defined to be the number of wins out of consecutive 100 games. In the case of a global quest, players are asked to reach certain mastery from scratch, e.g., to win 70 out of 100 consecutive intermediate games. A skilled player with a winning probability of 0.8 is likely to finish the quest in less than 100 games, while a newbie with a winning probability of 0.6 is likely to make hundreds or thousands of attempts to complete the quest. Some players would like to estimate the expected number of games they need to play, hence it comes the above question. Intuitively, when $M\ll pN$ , the expected number of games should be very close to $M/p$ . As $M$ approaches or exceeds $pN$ , the expected number of games grows very quickly. So far I haven't find a way to solve the question for a general $M$ . But the question does reduce to some well-known problems for some specific $M$ . Let $E(M, N)$ denote the expectation: first head $$ E(1,N) = \frac{1}{p}$$ first N-consecutive heads $$E(N, N) = \frac{p^{-N}-1}{1-p}$$ $M=2$ seems to be another case where a relation can be built without too much effort. Let $X_{M,N}$ denote the number of tosses to stop the game and $Y$ denote the number of tosses to reach the first head, then $$E(2,N) = \sum_{y} E(X_{2,N}|Y=y) P(Y=y)$$ The conditional probability can be calculated by considering the next $N-1$ tosses. Let $q=1-p$ , \begin{align}
E(X_{2,N}|Y=y) &= p\sum_{k=0}^{N-2} q^k (y+k+1) + q^{N-1} [y+N-1+E(2,N)] \\ 
& = y + \frac{1-q^{N-1}}{p} + q^{N-1}E(2,N)
\end{align} which yields $$ E(2,N) = \frac{2-q^{N-1}}{p(1-q^{N-1})} $$ But I don't think the above approach for $M=2$ can be generalized to $M\geq 3$ . $M=N-1$ is another case where we only need to consider a state with $N$ possibilities, instead of $2^N$ . Let $Z_{r}^{N}$ denote the rest number of tosses to finish the game of $M=N-1$ with a most recent history of tail-( $r$ consecutive heads)-tail: $$ T\underbrace{H...H}_{r}T | (Z_{r}^N ~ \text{tosses to finish}) $$ We have $$ E(Z_{r}^{N}) = p^{N-r-1}(N-r-1) + q\sum_{k=0}^{N-r-2}p^k\left[k+1+E(Z_{k}^{N})\right] $$ Note that $E(Z_{0}^{N})=E(N-1, N)$ is the quantity we are looking for. Unfortunately I haven't found an explicit expression, though it's just  a system of $N-1$ linear equations: $$ (I - qA) \pmb{E} = \pmb{b}  $$ where, for $i,j=0,\ldots,N-2$ , $$ A_{ij} = \left\{\begin{matrix}
p^j & i+j\leq N-2 \\
0 & i+j\gt N-2
\end{matrix} \right.
$$ and $$b_i = \frac{1-p^{N-1-i}}{q} $$ With the help of sympy I got the following results for $p=2/3$ : \begin{align}
E(2,3) &= 51/16 & \approx 3.19 \\
E(3,4) &= 1551/296 & \approx 5.24\\
E(4,5) &= 156615/19936 & \approx 7.86\\
E(5,6) &= 38234649/3388448 & \approx 11.28 \\
E(6,7) &= 31121052081/1963106560 & \approx 15.85
\end{align} As pointed out by @VarunVejalla, a small reduction from $2^N$ can be achieved by considering where the heads are in the sequence of the last $N$ flips. This can be done by generalizing the method for $M=N-1$ . Let $Z_{r_1 \ldots r_{N-M}}^{N}$ denote the rest number of tosses to stop the game with a most recent history of $r_i$ consecutive heads separated apart by a tail each, i.e., $$ T\underbrace{H...H}_{r_1}T\underbrace{H...H}_{r_2}T\ldots T \underbrace{H...H}_{r_{N-M}}T | (Z_{r_1\ldots r_{N-M}}^{N} ~ \text{tosses to finish}) $$ where $0\leq r_i\leq M-1$ and $\sum_{i=1}^{N-M}r_i\leq M-1$ . We have $$E(Z_{r_1\ldots r_{N-M}}^{N}) - q\sum_{k=0}^{M-1-\sum r_i} p^k E(Z_{r_2\ldots r_{N-M}k}^{N}) = \frac{1-p^{M-\sum r_i}}{q} $$ The above system of linear equations works for any $M<N$ , but its size equals the number of non-negative integer solutions to $\sum_{i=1}^{N-M}r_i\leq M-1$ , which roughly scales as $O(N^{\textrm{min}(M, N-M)})$ . Does anyone have any thoughts on its closed-form solution (if it exists)?","['probability-distributions', 'probability']"
4834315,Sum of conditional expectations of a bounded stochastic process,"Is there a proof for the following statement or is there a counter-example? Let $\{X_t\}$ be a stochastic process
adapted to the filtration $\{\mathcal{F}_t\}$ .
Assuming $0 \leq X_t \leq 1$ ,
and $\sum_{t=1}^{T} X_t \leq c$ almost surely for some fixed $c \in \mathbb{R}$ .
then it holds that: \begin{align}
    \sum_{t=1}^{T} \mathbb{E}[X_t | \mathcal{F}_{t-1}] \leq c.
\end{align}","['conditional-expectation', 'stochastic-processes', 'markov-process', 'probability-theory', 'probability']"
4834318,What are the Units of Flux and How do They Relate to Their Physical Meaning,"While taking Calculus III, which included some vector calculus, we defined the flux of a vector field $\mathbf{F} \colon \mathbb{R}^3 \to \mathbb{R}^3$ through a surface $S \subset \mathbb{R}^3$ as $$\iint_S \mathbf{F} \cdot \mathbf{n} \, d\sigma.$$ We were told that if we thought of $\mathbf{F}$ as the vector field defining how a fluid moves through space, the flux calculates the rate at which this fluid flows through $S$ . Intuitively then, wouldn't the ""units"" then be something along the line of $$\frac{``\text{quantity""}}{\text{time}}.$$ So something like ""The flux through $S$ is the amount of quantity that moves through it, as defined by $\mathbf{F}$ , each unit of time."" However, looking at the integral, the units seem to be different. They seem to be $$\frac{``\text{quantity""}}{\text{time}} \cdot \text{area}.$$ I really don't understand what the extra area quantity is doing. If we want to know the rate at which a quantity moves through $S$ , doesn't this imply that $$\frac{``\text{quantity""}}{\text{time}}$$ measures the rate at which a quantity moves through $S$ per unit of area ? As if there lies in a secret dimension? Multiplying by area then gives the total flux for that small surface. Take for example finding the flux of an electric field through a surface. The units of electric flux is $$\frac{\text{N}}{\text{C}} \cdot \text{m}^2.$$ (More surprising here is that there isn't a notion of time; but I suppose that's because previously, we thought of the vector field defining motion?) What is the extra $\text{m}^2$ factor representing? Why wouldn't the total amount of ""electric field going through a surface"" simply be in the units of the electric field ( $\text{N}/\text{C}$ ) rather than including an extra ""area"" dimension?","['multivariable-calculus', 'dimensional-analysis']"
4834385,Finding more constructible solutions to polynomial equations,"Summary: I have two polynomial equations in four variables. How can I find solutions using constructible numbers ? Can I use the fact that I know one (or a few closely related) such solutions to find more beyond these? In my answer on constructing “double” circumscribed pentagons I characterized solutions to a geometric problem by two polynomial equations: \begin{align*}
0 &= a^2b^2c - a^2b^2d + a^2bd^2 - a^2cd^2 + ac^2d^2 - bc^2d^2 \\&\quad - a^2b - 4abc - b^2c - ac^2 + bc^2 + a^2d + b^2d + 4bcd - ad^2 + cd^2 + a - d
\\
0 &= a^2bc + ab^2c - ab^2d - a^2cd + ac^2d - bc^2d + abd^2 - bcd^2 - ab - ac + bd + cd
\end{align*} Or, if you want to copy and paste them: 0 = a^2*b^2*c - a^2*b^2*d + a^2*b*d^2 - a^2*c*d^2 + a*c^2*d^2 - b*c^2*d^2 - a^2*b - 4*a*b*c - b^2*c - a*c^2 + b*c^2 + a^2*d + b^2*d + 4*b*c*d - a*d^2 + c*d^2 + a - d
0 = a^2*b*c + a*b^2*c - a*b^2*d - a^2*c*d + a*c^2*d - b*c^2*d + a*b*d^2 - b*c*d^2 - a*b - a*c + b*d + c*d Four different permutations of the roots of $5x^4 - 10x^2 + 1$ solve these equations, e.g. $$
a = -\sqrt{1+\frac2{\sqrt5}}\qquad
b = -\sqrt{1-\frac2{\sqrt5}}\qquad
c = \sqrt{1-\frac2{\sqrt5}}\qquad
d = \sqrt{1+\frac2{\sqrt5}}
$$ This solution corresponds to a geometric arrangement with a regular pentagon. But the question statement I was trying to answer was explicitly looking for a non-regular solution, so this specific solution is not acceptable. At the same time the original question was asking for a construction, so only a solution using constructible numbers would have a chance to satisfy that aspect. In my original answer I drew a picture using \begin{align*}
a &= -1.2 & 5a+6 &= 0 \\
b &\approx -0.183 &
18032b^4 + 81385b^3 - 68156b^2 - 21040b - 1088 &= 0\\
c &= \phantom+0.5 & 2c-1 &= 0 \\
d &\approx \phantom+1.490 &
78608d^4 + 167620d^3 - 575824d^2 - 21430d + 368293 &= 0
\end{align*} But the Galois group of those polynomials for $b$ and $d$ is the $S_4$ with order $4!=24$ . Since that order is not a power of two, these algebraic numbers are proven not to be constructible. I could not provide a constructible answer, much less an actual construction. Core question: How can I find a solution where all four variables are distinct constructible numbers, and at least one of them is not a root of $5x^4 - 10x^2 + 1$ ? Ideally I would find some parametrization, where I can pick one rational number arbitrarily and derive all four variables from that. But even finding a single different solution should help understand this problem better. Given how surprisingly hard some similar-looking problems around diophantine equations turn out to be, I wouldn't be surprised at all if this is a fairly hard problem. So I'm also prepared to accept answers providing evidence showing this to indeed be a hard problem. Maybe by showing the existence question to be a reasonably generic instance of a known NP-hard problem, or pointers at literature discussing similar problems. I know usually I should show my own work for tackling the problem, but I don't have the first idea where to start. I attempted to isolate some additional relationships between the known regular pentagon arrangement, and combine that with the given equations using resultants. But the responses I got tended to either force the regular pentagon arrangement, or quickly lead to some numbers being equal to one another. I can't think of any systematic approach, just this hit and miss guesswork. One thing that I did find out is that if I use an affine combination of the four permutations of the regular solution, they will all satisfy the second equation. But not the first, and I haven't found a way to leverage this observation.","['galois-theory', 'number-theory', 'algebraic-geometry', 'geometric-construction']"
4834404,what does it mean by $\| A \|$ for a matrix $A$ here?,"I am trying to understand notation which I can not find the explanation for in the book...
It says:
Since $$
(f_1(\mathbf{x}), \ldots, f_n(\mathbf{x})) = A (\mathbf{x} - \mathbf{x}_0) + O(|\mathbf{x} - \mathbf{x}_0  |^2)
$$ we have $$
|\mathbf{x} - \mathbf{x}_0| \leq \| A^{-1} \| (|(f_1(\mathbf{x}), \ldots, f_n(\mathbf{x}))| + C|\mathbf{x} - \mathbf{x}_0  |^2 ).  
$$ Here $A$ is an invertible real matrix, $|\cdot |$ denotes $L^2$ norm on $\mathbb{R}^n$ and $\mathbf{x}_0$ is a fixed vector.
What exactly does $\| A^{-1}\|$ mean in this context? Thank you for the clarification.","['multivariable-calculus', 'real-analysis']"
4834425,Projection with same rank,"Let $H$ be a Hilbert space (over $\mathbb{R}$ or $\mathbb{C}$ ), and $P$ , $Q$ are two projections over $H$ such that $\|P-Q\|<1$ . I am searching for a simple proof for $$
\dim\operatorname{Ran}P=\dim\operatorname{Ran}Q,\quad \dim\operatorname{Ran}(I-P)=\dim\operatorname{Ran}(I-Q).
$$ To handle with this, we can define $R=(P-Q)^2$ and $U_1=QP+(I-Q)(I-P)$ , $V_1=PQ+(I-P)(I-Q)$ . Then it is easy to verify that $$
U_1V_1=V_1U_1=I-R,\quad U_1^*=V_1,\quad V_1^*=U_1.
$$ Since $\|R\|<1$ , $T=(I-R)^{-\frac12}$ makes sense (e.g. define it with series) and it commutes with $P$ , $Q$ . Set $$
U=U_1T,\quad V=V_1T,
$$ then $UV=VU=I$ and $$
Q=UPU^{-1},\quad P=VQV^{-1}.
$$ but this proof is too long and complicated, compared with the given condition. Therefore, I want to find an intuitive way to show that.","['projection', 'functional-analysis']"
4834457,A problem about how dominated convergence is used while reading a paper about find the saddle point of the functional.,"I'm reading Existence of solutions to a higher dimensional mean-field equation on manifolds and get stuck on Lemma6. They want to find the saddle point of $$
I_\lambda(u)=\frac{1}{2} \int_M|\Delta_g^{m/2} u|^2 d \mu_g-\frac{\lambda}{2 m} \log \left(\int_M e^{2 m u} d \mu_g\right)
$$ on $$
E:=\left\{u \in H^m(M): \int_M u d \mu_g=0\right\}.
$$ And equip $E$ with the norm $\|u\|:=\left(\int_M\left|\Delta_g^{\frac{m}{2}} u\right|^2 d \mu_g\right)^{\frac{1}{2}}$ . They have proved that there exists a bounded sequence $\left(u_n\right)$ in $E$ such that $I_\mu^{\prime}\left(u_n\right) \rightarrow 0$ and $I_\mu\left(u_n\right) \rightarrow c_\mu$ . Then they assume that $u_n$ converges weakly in $E$ and almost everywhere to a function $u$ , and proved that $e^{2 m u_n}$ and $e^{2 m u}$ are uniformly bounded in $L^4$ . My question arises in the next step, they wrote that : by dominated convergence one has for $N>0$ $$\tag{1}
\min \left\{e^{2 m u_n}, N\right\} \rightarrow \min \left\{e^{2 m u}, N\right\} \quad \text { in } L^2\left(M, d \mu_g\right)
$$ as $n \rightarrow \infty$ and that $$
\sup _{n \in \mathbb{N}}\left\|\min \left\{e^{2 m u_n}, N\right\}-e^{2 m u_n}\right\|_{L^2}^2 \leq \frac{1}{N^2} \sup _{n \in \mathbb{N}}\left\|e^{2 m u_n}\right\|_{L^4}^4 \rightarrow 0 \quad \text { as } N \rightarrow \infty,
$$ we infer that $e^{2 m u_n} \rightarrow e^{2 m u}$ in $L^2$ . I wonder why they need to choose a $N$ and wrote (1) ? Actually I don't even know how the dominated convergence is used here.","['calculus-of-variations', 'functional-analysis', 'real-analysis']"
4834478,Are the character tables for $Z$-groups known?,"A $Z$ -group is a group whose Sylow subgroups are all cyclic groups. I know from here that if two $Z$ -groups have the same character table, then they are in fact isomorphic groups. To my understanding, all finite $Z$ -groups are a semidirect product of two cyclic groups of the form $\mathscr{C}_n \rtimes \mathscr{C}_m$ , since they have presentation $\langle a, b \,\vert\, a^n=b^m=1, b^{-1}ab = a^r\rangle$ where $gcd((r-1)n, m)=1$ and $r^n\equiv 1\pmod{m}$ . I know for $n$ odd the dihedral groups $D_{2n}$ , and dicyclic groups $D_{4n}$ are $Z$ -groups, and the character tables are known for all $n$ . Is the character table for all $Z$ -groups known, and where could I find it?","['group-theory', 'abstract-algebra', 'sylow-theory', 'reference-request']"
4834480,Solving a quadratic functional equation in $f(2x)$,"Find all differentiable functions with $f'(0)=1$ , such that they satisfy the following functional equation: $$ f(2x) = 2f^2(x) -4f(x) + 3$$ So, here's what I tried: And now, I'm stuck because I have no clue how to simplify the final limit.
Plus, I think there must be a more general method to solve functional equations of the kind $f(nx) = g(f(x))$ where $g(x)$ is a polynomial of degree $n$ , or at least the case $f(2x) = quadratic in f(x)$ . So, what can I do ahead of this?","['functional-equations', 'functions', 'derivatives']"
4834510,Sturm-Liouville problem with singular weight,"I have the following eigenvalues problem: $ xy'' + \lambda y = 0 $ , $0 \leq x \leq 1$ , $y(0)=y(1)=0$ that if we rewrite becomes $y''+ \lambda \frac{1}{x} y = 0$ , a Sturm-Liouville problem with weight $w=1/x$ . The weight has a singularity at $x=0$ , so the problem is singular. I don't know how to proceed with this kind of issue so I'll be thankful for any help. I need to find the eigenvalues and eigenfunctions. If it helps, according to Wolfram Alpha the solution are the Bessel functions $y(x) = c_1\sqrt{λx} J_1(2 \sqrt{λx}) + 2 i c_2\sqrt{λx} Y_1(2 \sqrt{λx})$ .","['eigenfunctions', 'sturm-liouville', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4834609,A normal operator is the continuous functional calculus of an adjoint operator,"Let $H$ be a hilbert space and suppose $T \in B(H)$ is a normal operator. Is it true that there exist $S \in B(H)$ self-adjoint such that $T=f(S)$ being $f: \sigma(S) \longrightarrow \mathbb{C}$ a continuous function? At the outset I thought the result was false for the following reason: if $H$ is separable then there exist a normal operator $T \in B(H)$ such that $\sigma(T)=K$ , where $$
K=\overline{\{ t+ i sin(1/t): t \in (0,1] \} }
$$ Therefore, if such an $S$ exist, we would have that $$
K=\sigma(T)=\sigma(f(S))=f(\sigma(S))
$$ Since $\sigma(S) \subset \mathbb{R}$ (since $S$ is self-adjoint) this tells us that $K$ is the image of a continuous function $f: \sigma(S)\subset \mathbb{R} \longrightarrow \mathbb{C}$ . Writing $f$ as $f(t)=f_1(t)+if_2(t)$ , the continuity of $f$ implies that $f_1$ and $f_2$ are both continuous, also the definition of $K$ and the above equation tells us that $(0,1] \subset \sigma(S)$ . Putting these facts together we would have that the function $g(t)=sin(1/t)$ can be extended to a continuous function $f_2$ defined on a compact subset of $\mathbb{R}$ (namely, $\sigma(S)$ ) which contains $(0,1]$ (and this isn't possible according to me). However, in  the book An Introduction to Operator Algebras by Kehe Zhu it is claimed (without proof or hint) that the result is true.  That being said, these are my two questions: What is wrong with the ""counterexample"" I provided? How can I prove the claim? Any hint will be appreciated. In advance thank you very much.","['c-star-algebras', 'operator-algebras', 'operator-theory', 'hilbert-spaces', 'functional-analysis']"
4834619,Summation notation for the sum of roots of a polynomial N at a time,"I've been studying Vieta's formula and would like a to find the general Sigma notation for the sum of roots 2 at a time. For example consider the the polynomial: $$
  ( z - \alpha_1)( z - \alpha_2)( z - \alpha_3)( z - \alpha_4) = 0
$$ The roots are: $\alpha_1, \alpha_2, \alpha_3$ and $\alpha_4$ . The sum of the roots, taken 2 at a time is: $$
   \alpha_1 \alpha_2 + \alpha_1 \alpha_3 + \alpha_1  \alpha_4 + \alpha_2 \alpha_3 + \alpha_2 \alpha_4 + \alpha_3\alpha_4
$$ I would like to express this in Sigma notation.  I presume a $$
   \sum \sum \alpha_i\alpha_j
$$ is used but can't figure out the Sigma terminals.  Then extend this to $n$ roots. Thanks in advance.","['algebra-precalculus', 'polynomials', 'complex-numbers']"
4834645,"Let $f$ be continuous in $[0,2]$ and differentiable in $(0,2)$ such that $|f'(x)|\leqslant1$ and $f(0)=1=f(2)$. Prove that $f(x)\geqslant0$.","Let $f$ be a continuous function in $[0,2]$ and differentiable in $(0,2)$ , such that $|f'(x)|\leqslant1$ for all $x\in(0,2)$ and also $f(0)=1=f(2)$ . Prove that $f(x)\geqslant0$ for all $x\in(0,2)$ . My approach is to use Intermediate Mean value theorem and then we get, that there exists a point $c\in(0,2)$ such that $|f'(c)|=0$ , so $c$ is local extreme. If $c$ is local minimum than the proof is straight forward, bit I have difficulties proving if $c$ is local maximum. I also need to be sure that I can continue in this way or I need another way to prove the statement. Thanks a lot!","['analysis', 'maxima-minima', 'rolles-theorem', 'mean-value-theorem', 'derivatives']"
4834651,"Proof of ""The sum of a convergent and divergent sequence is divergent"" [duplicate]","This question already has an answer here : Convergent + divergent $\to$ divergent (1 answer) Closed 6 months ago . I am simply wondering if my proof of this statement works. Suppose $x_n$ converges to x and $y_n$ diverges. Proof by contradiction. Since $x_n$ converges, for any $ε$ , there exists a natural number $N_x$ s.t. $n≥N_x$ implies $|x_n - x| < ε$ . Now assume $x_n + y_n$ converges to $x + y$ . Thus for an arbitrary $ε$ , there exists a natural number $N_0$ s.t. $n≥N_0≥N_x$ implies $|(x_n + y_n) - (x+y)| < ε$ . Since $y_n$ diverges, we know for any real number $y$ , there exists $ε_y>0$ s.t. for any $N$ , $n≥N$ and $|y_n - y| ≥ ε_y$ . Thus, $|(x_n + y_n) - (x+y)| ≥ ||x_n - x| - |y_n - y||$ (Reverse triangle inequality) $||x_n - x| - |y_n - y|| ≥ ||x_n - x| - ε_y| ≥ 0$ ( $y_n$ diverges) Which then allows us to write $|x_n - x| ≥ ε_y$ for any $n≥N_0≥N_x$ . Since $x_n$ converges, $|x_n - x|$ should be less than any arbitrary $ε$ . Therefore, we have arrived at a contradiction. Hence $x_n + y_n$ diverges.","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4834716,Is there an analogue of the abelianization of a group for nilpotent groups?,"If $G$ is an arbitrary group we denote its abelianization as $G^{ab} := \frac{G}{[G,G]}$ , where $[G,G]$ is the commutator. As an abelian group it is characterized by the following universal property: Let $\pi : G \to G^{ab}$ be the quotient morphism. If $\varphi : G \to H$ is an morphism between $G$ and an abelian group $H$ , then there is a unique morphism $\hat{\varphi} : G^{ab} \to H$ such that $\varphi = \hat{\varphi}\circ\pi$ . In a sense the abelianization is the abelian group that is closest to $G$ . I wonder if there is a solution for the analogous universal property, but with nilpotent groups instead of abelian. (Or, perhaps only over nilpotent groups of class $n$ for fixed $n\in\mathbb{N}$ , the abelianization being the case $n=1$ )","['nilpotent-groups', 'group-theory']"
4834730,Formal Justification for Jacobian as Area Element,"Consider a function $\vec{f}:\Bbb R^2 \rightarrow \Bbb R^2$ with Frechet derivative $Df(\vec{a})$ at some $\vec{a} = (a_x, a_y) \in \Bbb R^2$ , and infinitesimal rectangle $R(\delta x, \delta y) = [a_x, a_x + \delta x] \times [a_y, a_y + \delta y]$ . Under transformation $\vec{f}$ , this infinitesimal rectangle is mapped to an infinitesimal parallelogram and its area is scaled by the Jacobian $J = \lvert \det(Df(\vec{a})) \rvert$ . Formally, for $R(\Delta x, \Delta y) = [a_x, a_x + \Delta x] \times [a_y, a_y + \Delta y]$ , $$ \lim_{\Delta \vec{x} \to \vec{0}} \frac{area(\vec{f}(R(\Delta \vec{x})))}{area(R(\Delta x, \Delta y))} = \lvert \det(D\vec{f}(a))) \rvert$$ Is it possible to formally prove this using the definition of the Frechet derivative? If not, how could it be done? EDIT: The definition of $area(\vec{f}(R(\Delta \vec{x})))$ is the value such that for any $\epsilon > 0$ there exists partition $P$ of some rectangle enclosing $S = \vec{f}(R(\Delta \vec{x}))$ such that upper and lower Riemann sums $U_P \vec{f} \chi_S - L_P \vec{f} \chi_S < \epsilon$ . Thanks in advance for your help.","['jacobian', 'multivariable-calculus', 'area', 'real-analysis']"
4834752,Provide a closed formula based on the generating function $\frac{x}{1+x+x^2}$,"Recently, when I self-learnt Discrete Mathematics and Its Applications 8th by Kenneth Rosen, I did only the even-numbered exercises which the author offers the detailed description instead of the odd ones because the odd ones and the corresponding even ones are very similar. I has some questions about exercise 8.4-8-g) which is related with the generating function for the recurrence relation. The problem is For each of these generating functions, provide a closed
formula for the sequence it determines. ... g) $x/(1+x+x^2)$ The answer says The key here is to recall the algebraic identity $1 − x^3 = (1 − x)(1 + x + x^2)$ . Therefore the given function
can be rewritten as $x(1 − x)/(1 − x^3)$ , which can then be split into $x/(1 − x^3)$ plus $−x^2/(1 − x^3)$ . From
Table 1 we know that $1/(1 − x^3) = 1 + x^3 + x^6 + x^9 + \cdots$ . Therefore $x/(1 − x^3) = x + x^4 + x^7 + x^{10} + \cdots$ ,
and $-x^2/(1 − x^3) = -x^2 - x^5 - x^8 - x^{11} - \cdots$ . Thus we see that $a_n$ is 0 when $n$ is a multiple of 3, it is 1
when $n$ is 1 greater than a multiple of 3, and it is −1 when $n$ is 2 greater than a multiple of 3 . One can
check this answer with Maple . I tried first by myself when doing this exercise without reading the answer: $$
\begin{align*}
      \frac{x}{1+x+x^2}&=x\cdot \sum_{n=0}^{\infty}(-x-x^2)^n\\
      &=x\cdot \sum_{n=0}^{\infty}(-1)^n\cdot x^n\cdot (1+x)^n\\
      &=x\cdot \sum_{n=0}^{\infty}(-1)^n\cdot x^n\cdot \sum_{k=0}^n\binom{n}{k}x^k\\
      &=\sum_{n=0}^{\infty}\sum_{k=0}^n\binom{n}{k}\cdot (-1)^n\cdot x^{n+k+1}\\
    \end{align*}\\
$$ Then $$
    \begin{align*}
      a_m&=\sum_{\substack{0\le k\le n\le m-1\\n+k+1=m}}\binom{n}{k}\cdot (-1)^n\\
      &=\sum_{n=\lceil\frac{m-1}{2}\rceil}^{m-1}\binom{n}{m-n-1}(-1)^n
    \end{align*}
$$ I checked some terms for $m=3k$ , $a_3=\binom{1}{1}\cdot (-1)+\binom{2}{0}\cdot 1=0$ and $a_6=\binom{5}{0}\cdot (-1)+\binom{4}{1}\cdot (1)+\binom{3}{2}\cdot (-1)=0$ . Then it seems that my equation $a_m=\sum_{n=\lceil\frac{m-1}{2}\rceil}^{m-1}\binom{n}{m-n-1}(-1)^n$ is true. Q: Then how to prove without using the relation of my try with the book answer? $$
a_m=\sum_{n=\lceil\frac{m-1}{2}\rceil}^{m-1}\binom{n}{m-n-1}(-1)^n
    =\begin{cases}
      0,m\equiv 0\pmod 3\\
      1,m\equiv 1\pmod 3\\
      -1,m\equiv 2\pmod 3
    \end{cases}
$$","['recursion', 'recurrence-relations', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
4834800,Polar Coordinate Calculation of Hypercomplex Fractals,"I've been doing some reading on how hypercomplex fractals are calculated using cartesian to n-spherical coordinate conversion. I've specifically been looking at these two sources: https://archive.bridgesmathart.org/2010/bridges2010-247.pdf http://www.bugman123.com/Hypercomplex/ To lay it out, both sources explain how the equation for a Mandelbrot set ( $z_{n+1} = z^2 + c$ ) can be converted away from using complex numbers and instead treat the complex number $c$ as a polar coordinate, with the real number being the $x$ value and the imaginary component being the $y$ value. This allows Mandelbrot sets to be expanded to n-dimensions by simply using n-dimensional Cartesian coordinates to n-Spherical coordinates, thus how the Mandelbulb was created. What I'm struggling with is how these cartesian to spherical conversion equations were derived in the two referenced sources. In both sources, they used the following equations to derive spherical coordinates from caresian coordinates: $\rho = \sqrt{x^2 + y^2 + z^2}$ $\theta = \arctan{\dfrac{y}{x}}$ $\phi = \arcsin{\dfrac{z}{\rho}}$ The first two make sense just using the generalized formula for cartesian to n-spherical coordinates shown in wikipedia here . Using the recipe listed there gives me: $\rho = \sqrt{x^2 + y^2 + z^2}$ $\theta = \arctan{\dfrac{y}{x}}$ $\phi = \arctan{\dfrac{\sqrt{x^2 + y^2}}{z}}$ The equation for $\phi$ I calculated myself is likewise what Daniel White originally formulated in his original proposal for using spherical coordinates. Understanding that $\arctan{\dfrac{O}{A}} = \arccos{\dfrac{A}{H}} = \arcsin{\dfrac{O}{H}}$ where $H = \sqrt{O^2 + A^2}$ , I arrive at this: $\phi = \arctan{\dfrac{\sqrt{x^2 + y^2}}{z}} = \arccos{\dfrac{z}{\sqrt{{\sqrt{x^2+y^2}}^2 + z^2}}} = \arccos{\dfrac{z}{\sqrt{x^2+y^2+z^2}}} = \arccos{\dfrac{z}{\rho}}$ If I were to instead to try an get $\phi$ in terms of $\arcsin$ I would get: $\phi = \arctan{\dfrac{\sqrt{x^2 + y^2}}{z}} = \arcsin{\dfrac{\sqrt{x^2 + y^2}}{\sqrt{x^2+y^2+z^2}}} = \arcsin{\dfrac{\sqrt{x^2 + y^2}}{\rho}}$ This result isn't as convenient computation-wise, but the main issue is I don't understand why the two referenced sources are showing $\phi = \arcsin{\dfrac{z}{\rho}}$ instead of $\phi = \arccos{\dfrac{z}{\rho}}$ . This surely can't be a mistake or typo on their part, as the second source I listed is the original Mandelbulb creator, so I'd appreciate some understanding as to what I missed.","['trigonometry', 'spherical-coordinates', 'fractals']"
4834854,"3D representation of $K_{3,3}$","I am looking for a way (or a proof of nonexistence) to represent $K_{3,3}$ in the following way: Each vertex of the graph should be one convex polyhedron and each edge of the graph should be equivalent to two polyhedra sharing a face. Any idea?","['graph-theory', 'combinatorics', 'geometry', 'discrete-mathematics']"
4834870,Using Rouche's theorem to prove injectivity,"$\textbf{Problem setup}$ : Let $F : R_{\tau} \rightarrow R_{\tau}$ be a biholomorphism onto its image (i.e. $F : R_{\tau} \rightarrow V$ is a holomorphic function with holomorphic inverse, here $V = F(R_{\tau}) \subset R_{\tau}$ is open), $R_{\tau} = \{ w \in \mathbb{C} : \Re(w) > \tau \}$ where $\tau > 0$ is some fixed real number.
Moreover assume that $F$ extends holomorphically to a slightly larger right half plane $R_{\tau’}$ with $\tau’ < \tau$ and is again a biholomorphism onto its image. Suppose $F(w) = w + 1 + \eta(w^{-\frac{1}{n}})$ (holds in $R_{\tau’}$ ), where $\eta(\zeta) = b_1 \zeta + b_2 \zeta^2 + ...$ , is some analytic function in a neighbourhood of $0$ , defined on $|\zeta| < r$ for some fixed $r > 0$ . Also assume $|\eta(\zeta)| \leq \frac{1}{2}$ for all $|\zeta| < r$ . Now suppose $\tau > \frac{1}{r^n} + 1$ is fixed, and let $x \geq \tau$ be a fixed real number. Define $E_{0,x} = \{ w \in \mathbb{C} : \Re(w) = x \}$ and $E_{1,x} = \{w \in \mathbb{C} : \Re(w) = x+1 \}$ . Then setting $H_x(w) = w$ if $w \in E_{0,x}$ and $H_{x}(w) = \Phi(w) = w + \eta((w-1)^{-\frac{1}{n}}) $ if $ w \in E_{1,x}$ we see that since $\Phi(w) = F(w-1)$ , $H_{x}(w)$ is injective on both $E_{0,x}$ and $E_{1,x}$ . Moreover since $\Re(H_{x}(w)) \geq \Re(w) - \frac{1}{2} = x+1 -\frac{1}{2} = x + \frac{1}{2}$ for $w \in E_{1,x}$ , the images of $E_{0,x}$ and $E_{1,x}$ under $H_{x}(w)$ do not intersect, thus $H_{x}(w)$ is injective. $\textbf{The problem}$ : Suppose we introduce a complex parameter $c \in \Delta$ where $\Delta = \mathbb{D}$ into $\eta(\zeta)$ as follows: Define $\eta(c,\zeta) = \eta(c r \zeta (x-1)^{\frac{1}{n}})$ for $|c| < 1$ and $|\zeta| \leq (x-1)^{-\frac{1}{n}}$ . Since $|c r \zeta (x-1)^{\frac{1}{n}}| < r$ , it follows that $\eta(c,\zeta)$ is a convergent power series and that $|\eta(c,\zeta)| \leq \frac{1}{2}$ for $|c| < 1$ and $|\zeta| \leq (x-1)^{-\frac{1}{n}}$ . Analogously to before, set $H_{x}(c,w) = w $ if $ (c,w) \in \Delta \times E_{0,x}$ and $H_{x}(c,w) = 
\Phi(c,w) = w + \eta(c,(w-1)^{-\frac{1}{n}})$ if $ (c,w)  \in \Delta \times E_{1,x}$ Apparently now we can conclude by Rouche's theorem that for a $\textbf{fixed}$ $ |c| < 1$ , $H_{x}(c,.)$ is injective on $E_{0,x}$ and $E_{1,x}$ . I am lost as to how one is supposed to apply Rouche's theorem to see this.","['complex-analysis', 'rouches-theorem', 'complex-numbers', 'analysis']"
4834882,Is there a use to L'Hopital rule for $*/∞$ when the function in the numerator doesn't converge? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 months ago . Improve this question When studying the L'Hopital's rule for limits in the case of $*/∞$ , we studied that the rule applies when the function in the denominator appraoches $∞$ , regardless to the limit of the function at the numerator. Usually, this rule is used in the case of $∞/∞$ or $-∞/∞$ , but I was wondering if there are other uses, since it's supposed to apply no matter what is the function in the numerator. If the function in the numerator converges to a real number $L$ , then by arithmetic of limits, the limit of the fraction will be zero.
But, if the function on the numerator does not converge, it could be interesting to try to use L'Hopital's rule to find the limit. Nonetheless, I could not find a non-converging funtion to put in the numerator, that one can caclulate the limit of the fraction using L'Hopital (for example when using sin or cos variations, affter differentiating I still remained with $\sin$ or $\cos$ and could not find a limit). I would love to hear if someone knows an example of such function, that shows there is a use for L'Hopital other than $\pm∞/∞$ .
Thank you!","['convergence-divergence', 'functions', 'derivatives']"
4834883,"Let $(a_n)_n$ be a sequence of real numbers. Let also, $\lim_{n\to \infty}\left(2a_n+\sin(a_n)\right)=\frac\pi3+\frac12$ then $a_n\to\frac \pi6$","Let $(a_n)_n$ be a sequence of real numbers. Let also, $\lim\limits_{n\to \infty}\left(2a_n+\sin(a_n)\right)=\dfrac\pi3+\dfrac12$ then $$\lim\limits_{n\to \infty}a_n=\dfrac \pi6$$ If we knew $\lim a_n$ exists then $\lim \sin(a_n)$ also exists by continuity, then I can find this desired limit. Without assuming existence of the limit $\lim a_n$ can we solve it?","['limits', 'sequences-and-series', 'real-analysis']"
4834891,Confusion regarding uniform boundedness,"I am estimating a sequence of functions and struggling with the following estimations. Suppose $F_n:[0,1]\rightarrow (0,\infty)$ , a continuous sequence of functions. We know the following For an arbitrary $n\in {\mathbb{N}}$ and $x_0\in [0,1]$ , there exists an $\epsilon_n>0$ (depending only on n not on $x_0$ ), and two positive constants $C_{1,x_0},C_{2,x_0}$ (only depending on $x_0$ , not on n) such that $$
C_{1,x_0}<F_n(x)<C_{2,x_0}\,\forall x\in B(x_0,\epsilon_n):=(x_0-\epsilon_n,x_0+\epsilon_n).
$$ Is $F_n$ uniformly bounded on $[0,1]$ by two positive constants? My approach: I was trying to use a compactness argument for this. I first fixed a $n$ then, on each $B(x_0,\epsilon_n)$ , $F_n$ is bounded by positive constants, not depending on $n$ . So, after taking finite subcover, I tried to establish the claim. However, the issue is if I take the finite subcover then the points $x_0$ will start to depend on $n$ . Is there any solution for this? Or is the claim false? Any suggestion would be of great help.","['complex-analysis', 'sequences-and-series', 'real-analysis']"
4834897,Hardy-Littlewood maximal function of a function supported in the unit ball,"This question comes from Stein’s Note on the class $L\log L$ . In the proof for Theorem 1, which establishes for an integrable function $f\in L^1(\mathbb R^n)$ supported on a finite ball $B$ , that the (centered) Hardy-Littlewood maximal function $Mf$ is integrable implies $f\in L\log L$ , i.e. $\int_B |f|\log^+|f|dx\lt\infty$ , the following is stated: We observe that if $f$ is supported in $B$ and $Mf$ is integrable over $B$ , then $Mf$ is integrable over $B’$ . In fact, suppose for simplicity that $B=\{x:|x|\le 1\}$ , and $B’=\{x:|x|\le 2\}$ . Then if $x\in B’-B$ , we can easily verify that $Mf(x)\le cMf(\bar x)$ , where $\bar x=x/|x|^2$ . Here $c=n^{n/2}\omega_n$ , where $n$ is the dimension of the space and $\omega_n$ is the volume of the unit $n$ -dimensional ball. However, I don’t find this obvious. It seems rather counterintuitive how the factor $n^{n/2}$ comes in, as that was the factor between the volume of a cube and a ball with the same (or proportional) diameter. This is how $c$ appeared in the paper at first. For reference, please see this other post . But in the part quoted above, there didn’t seem to be any obvious cubes involved. Some observations: $Mf(\bar x)$ is achieved at $B(x,r)$ with $r=\min(|\bar x|,1-|\bar x|)$ , since the argument inside the supremum (in the definition of $Mf$ ) could be thought of as a kind of concentration, and if the ball contains a region outside of the support of $f$ , this concentration will be diluted. If $n=1, \dfrac{1}{|B(x,r)|}\int_{B(x,r)\cap [-1,1]}|f|$ increases with $r$ for $x\in B’-B$ if $B(x,r)\cap [-1,1]$ is a nonempty proper subset of $[-1,1]$ . $Mf(x)$ is achieved when $B(x,r)$ covers all of $[-1,1]$ . It seems this is generalizable to higher dimensions, although I haven’t yet been successful in rigorously showing that. $|x-\bar x|=|x|-1/|x|$ . If $r\gt |x-\bar x|$ , then $B(\bar x,r)\supset B(x,r-|x-\bar x|$ . Either a rigorous proof of the quoted part or an intuitive argument would be appreciated. In the meantime I might add more stuff if I do find more related to this part.","['functions', 'lebesgue-measure', 'intuition', 'real-analysis']"
4834926,Biased estimator of number of edges in a graph,"I meet this problem when doing my scientific research. Let $G$ be a simple graph with $n$ vertices, i.e. $G$ is undirected and does not have any loops or multiple edges. We randomly sample some vertices using the following rule. Each vertex is sampled with the same probability $\alpha$ independently. Let $G_1$ be the subgraph induced by the sampled vertices, i.e. an edge of $G$ is also in $G_1$ if and only if both endpoints of this edge are sampled. Suppose we observes that $G_1$ has $n_1$ vertices and $m_1$ edges. Our goal to estimate the number of edges $\hat{m}$ in $G$ using $n,n_1,m_1$ but without $\alpha$ . Since $$ \hat{m}=\left(\frac{1}{\alpha}\right)^2 m_1 $$ is an unbiased estimator and $$ \alpha=\mathbb{E}\left[\frac{n_1}{n}\right], $$ we replace $\alpha$ in this estimator with $\frac{n_1}{n}$ and get
the following estimator $$\hat{m}=\left(\frac{n}{n_1}\right)^2 m_1.$$ However, this is a biased estimator, while the correct unbiased estimator is $$\hat{m}=\frac{n(n-1)}{n_1(n_1-1)} m_1.$$ This seems strange. Can you give me an explanation? What is the difference between these two estimators?","['graph-theory', 'statistics', 'probability', 'estimation']"
4834939,Picture of $\mathrm{Spec}(\mathbb{R}[x])$,"Follow-up to Comparing the prime spectra of $\mathbb{Q}[x],\mathbb{R}[x]$ and $\mathbb{C}[x]$ . I'm trying to draw a picture of $\mathrm{Spec}(\mathbb{R}[x])$ . I've already drawn a picture of $\mathrm{Spec}(\mathbb{C}[x]) = \{(0)\} \cup \{(x - \alpha) \mid \alpha \in \mathbb{C}\}$ as a complex plane plus the generic point $(0)$ : Now, $\mathrm{Spec}(\mathbb{R}[x]) = \{(0)\} \cup \{(p) \mid p \text{ irreducible in }\mathbb{R}\}$ .
At first, I thought of identifying each irreducible polynomial with a complex root, but this isn't well-defined as some irreducible polynomials will have more than one root.
But the irreducibles in $\mathbb{R}[x]$ are linear polynomials and quadratics with no real root, and the latter have two conjugate complex roots.
So I thought we can represent this spectrum as the upper half of the complex plane, identifying each quadratic irreducible with its complex root with positive imaginary part, as follows: Is this an accurate picture?","['affine-schemes', 'algebraic-geometry', 'commutative-algebra']"
4834959,Help with evaluating the integral $\int_0^{\infty} \frac{x \sinh x}{(1+\cosh^2(x))^2} dx$,"I was trying to compute the integral $$\ I = \int_0^{\infty} \frac{x \sinh(x)}{(1+\cosh^2(x))^2} dx$$ So, I tried defining: $$\ I(a) = \int_0^{\infty} \frac{ x \sinh(ax)}{(1+\cosh^2(x))^2}dx$$ Then $$\ I'(a) =  \int_0^{\infty} \frac{x\cosh(ax)}{(1+\cosh^2x)^2} dx$$ Then I substituted $\cosh(x) = t$ , but I was stuck with this method and could not proceed any further. So, I tried another approach. I used the fact that we can express $\sinh(x)$ as: $$ \sinh(x) = \sum_{n=0}^{\infty} \frac{x^{2n+1}}{(2n+1)!}$$ Then swapping the integral and summation signs, we get: $$\ I = \sum_{n=0}^{\infty}\frac{1}{(2n+1)!} \cdot  \int_0^{\infty} \frac{x^{2n+2}}{(1+\cosh^2(x))^2} dx$$ But I am unable to proceed with the evaluation of the inner integral. Can someone help me evaluate the value of this integral? Any help is appreciated. Thank you for reading!","['integration', 'calculus', 'definite-integrals']"
4834978,every rank one operator is a linear combination of rank one idempotents,"I'm trying to prove a theorem which makes use of the fact that every finite rank operator on a Banach space is a linear combination of rank one idempotents. It's enough to show this for rank one operators. Actually I have found a proof in a paper but I can't understand it at all. I whish someone could offer a new proof or help me clarify the following argumement: Given a rank-one operator $u\in B(X)$ , there exists $\tau(u)\in\mathbb{C}$ such that $u^2=\tau(u)u$ .Moreover, $\tau(u)=0$ or $\tau(u)$ is the only non-zero element in the spectrum of $u$ .(What on earth is $\tau$ ?) Thus if $\tau(u)\neq 0$ ,then $\tau(u)^{-1}u$ is a minimal idempotent (I think this means a rank-one idempotent), and $u=\tau(u)(\tau(u)^{-1}u)$ . Now for $\tau(u)=0$ , let $x\in B(X)$ , and $\lambda\in\mathbb{C}$ be such that $uxu=u$ and $\lambda\gt r(x)$ ( $r(x)$ denotes the spectral radius). Therefore, $e_1=ux$ and $e_2=u(x-\lambda)$ are minimal idempotents satisfying $u=\lambda^{-1}(e_1-e_2)$ , which completes the proof.","['operator-theory', 'functional-analysis', 'operator-algebras']"
4834991,Monotonicity of a multivariate function,"Let $f:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb R$ . Assume $f(x,y)$ is twise continuously differentiable and the derivative with respect to both arguments (i.e., $x$ and $y$ ) are monotone in the sense that the following holds true for any $x,x'\in\mathbb R^n$ such that $x\neq x'$ : $$(x'-x)(D_xf(x,x')-D_xf(x,x))>0~\textrm{and}$$ $$(x'-x)(D_yf(x',x)-D_yf(x,x))>0~\textrm.$$ Suppose now we have $$f(x,x')-f(x,x)>0.$$ Can we prove that the following statement is true: $f(x',x')-f(x',x)\geq0?$","['multivariable-calculus', 'derivatives', 'monotone-functions', 'hessian-matrix']"
4835023,Getting Modified Zernike Polynomial (Radial Part) ODE from Jacobi Polynomial ODE,"Question: The Jacobi differential equation in terms of the Jacobi polynomial $P_{n}^{(\alpha,0)}(x)$ is given by: \begin{equation}
	(1 - x^2) P_{n}^{(\alpha,0)''}(x) + (-\alpha - (\alpha + 2) x) P_{n}^{(\alpha,0)'}(x) + n(n + \alpha + 1). P_{n}^{(\alpha,0)}(x) = 0 \;\;\; (*)
\end{equation} Prove that $T_{n}(x)=x^{k+\frac{1}{2}} P_{n}^{(\alpha,0)}(1-2x^2) $ satisfies in the following ODE \begin{equation}
		(1 - x^2) T_{n}''(x)  -2x T_{n}'(x) + \frac{\frac{1}{4}-k^2}{x^{2}} T_{n}(x)= -\chi T_{n}(x). \;\;\; (**)
\end{equation} I understand that $$
T_{n}'(x) = (k+\frac{1}{2})x^{k-\frac{1}{2}} P_{n}^{(\alpha,0)}(1-2x^2) - 4x^{k+\frac{3}{2}} P_{n}^{(\alpha,0)'}(1-2x^2)
$$ ​
and $T_{n}''$ , however when I substitute $T_{n},\; T_{n}',\; T_{n}''$ in (**) I don't get anything close to (*). I have also thought about the change of variable $t=1-2x^{2}$ , but no result. I was wondering if someone could point out my mistake about this.","['orthogonal-polynomials', 'analysis', 'polynomials', 'ordinary-differential-equations']"
4835035,About four points lying on a sphere,"This seems to be a curious problem: Four points $A,B,C,D$ are chosen on a sphere of radius $1$ such that
the centre $O$ of the sphere lies inside the tetrahedron $ABCD$ . Prove
that $|\vec{OA}+\vec{OB}+\vec{OC}+\vec{OD}|<2$ . An analogous problem in the plane can be formulated like this: If three points $A,B,C$ are chosen on a circle of radius $1$ such that
the centre $O$ of the circle lies inside triangle $ABC$ , then $|\vec{OA}+\vec{OB}+\vec{OC}|<1$ . I don't know how the first problem is solved, but I also don't have a good solution to the second problem, just some thoughts: We can assume that the angles $AOB$ , $BOC$ , $AOC$ satisfy the inequalities $$
180^\circ\geq\ \angle AOB\ \geq\ \angle BOC\ \geq\ \angle AOC.
$$ Then $AOB\geq120^\circ$ and point $C$ lies inside arc $A'B'$ (see figure).
If $\vec{OP}=\vec{OA}+\vec{OB}$ and $\vec{OQ}=\vec{OP}+\vec{OC}$ , then $|OP|<1$ and
it is easy to see that $\angle POQ\>\angle OPQ$ .
(I wrote easy to see, but to be honest I could not strictly justify this point. These angles are equal if $C=A'$ or $C=B'$ ; if $C$ lies on arc $A'BAB'$ then the opposite inequality holds; if $C$ lies on arc $A'B'$ (as in the figure) then the above inequality holds.)
Consider $\triangle OCQ$ , $\angle CQO=\angle POQ\geq90^\circ$ $\Longrightarrow$ $OQ<OC=1$ . Added 2024/01/07. Patrick suggested another solution to the problem, but unfortunately his explanations are very vague. I will try to present his solution here, it seems to be the best one that is presented here. So, let $\vec{OS}=\vec{OA}+\vec{OB}+\vec{OC}+\vec{OD}$ .
Let us assume that $\vec{OS}\neq0$ , otherwise our statement is obvious.
Consider an axis $L$ passing through $O$ and parallel to $\vec{OS}$ .
We can look at $L$ as a number line, $1$ corresponds to the point of intersection of the ray $OS$ and the sphere.
Let $\pi$ be the orthogonal projection of $\mathbb{R}^3$ onto the $L$ .
If $\pi(A)=a,\pi(B)=b,\pi(C)=c,\pi(D)=d,\pi(S)=s$ , then $s=|\vec{OS}|$ and $a+b+c+d=s$ .
We can assume that $$
-1\leq a\leq b\leq c\leq d\leq1.
$$ It is clear, if $d=1$ , then $c<d=1$ .
(In the picture below, the named projections are marked in red.) If $a>0$ ,
then the tetrahedron $ABCD$ lies on one side of the plane perpendicular to $\vec{OS}$ and passing through the center $O$ and so $O$ is outside the tetrahedron. If $b>-a$ and $a\neq-1$ and $P$ is the plane passing through the three points $A$ , $O$ , $S$ (in this plane is the figure below),
then the tetrahedron $ABCD$ is located on one side of the plane perpendicular to $P$ and passing through $O$ and $A$ . If $a=-1$ , then $s=a+b+c+d=(a+d)+b+c=0+b+c<2$ . If $b\leq-a$ , then $s=a+b+c+d=(a+b)+c+d\leq0+c+d<2$ . The proof is now complete. But that still leaves the question. Are there any simpler
considerations?",['geometry']
4835037,Examples of PDEs that found application after discovery,"I am a 2nd year mathematics undergraduate in the UK and recently took an introductory elective in partial differential equations. The focus was on solving some classical examples, all arising from an application - this seems to be typical of most such introductory courses. From what I can see of more advanced PDE-focused electives, my impression is they seem to follow a similar trend,  'inspecting' or solving the PDE more rigorously with more advanced analysis, but mostly still application focused ie 'this type of PDE arises in this modelling problem/application, we study these properties and use these techniques to solve it' . My main question is, despite the general motivation of application, are there any well-known PDEs that were first discovered and studied from a pure perspective and then later found applications ? In other words, are there any notable examples of PDEs that could belong here ? A followup question from this would then be, are there any examples currently in the first stage, but not the second (ie derived and studied entirely in pure mathematics, but don't really have any obvious application yet)?","['mathematical-modeling', 'examples-counterexamples', 'analysis', 'partial-differential-equations', 'math-history']"
4835055,Distance between $e^x$ and $\ln x$ using tangent/normal method,"I have been trying to find the shortest distance between $f(x)=e^x$ and $g(x)=\ln(x)$ . All methods I have seen include taking the $y=x$ line as the mirror and find a point on both curves which have same slope as the mirror ( $1$ in this case). Now this method is working only because it is a very specific case as the functions are inverse of each other. The general method would be to find a common normal between the two curves and calculate the distance from where the normal intersects them. I am trying to apply this method for the given question but am getting unexpected results. First I found derivatives for both curves, took their reciprocals, and multiplied them by a minus sign to get the slope of the normal: $m_{f(x)normal}=-\frac{1}{e^x}$ $m_{g(x)normal}=-x$ Now, both these slopes must be equal in case of a common normal. But equating them yields: $xe^x=1$ This is far from the actual answer of $x=1$ and $x=0$ where the normal intersect the curves. Where am I going wrong?","['functions', 'exponential-function', 'graphing-functions', 'calculus-of-variations']"
4835086,Pointwise boundedness and uniform boundedness,"Suppose $f:[0,1]\times [1,\infty)\rightarrow \mathbb{R}$ a continous function (in both variable). Let $$
g(x)=\sup_{y\in[1,\infty)}f(x,y). (\text{Assume the supremum does exist for each x})
$$ For what kind of function $f$ is the function $g$ continuous or $g$ is bounded above? I came to know from an old post in the stack that it is true if we replace $[1,\infty)$ by any compact interval. Also, I think there will be functions in this case for which $g$ may not be continuous. But is there any result where $f$ ""nice"" enough so that $g$ is continuous?","['complex-analysis', 'real-analysis']"
4835118,"Is there a ""universal group""?","Is there a group $U$ such that for any group $G$ , $G$ is isomorphic to a subgroup of $U$ ?
0
For any group $G$ , it is isomorphic to a subgroup of a symmetric group (Cayley's theorem) so I'm wondering if we could do something like that to construct $U$ . For example, every finite group is a subgroup of $$S = \bigcup_{n \in \mathbb{N}} S_n,$$ I'm curious as to whether a construction of $U$ might be similar, if it exists.","['group-theory', 'cardinals', 'set-theory']"
4835198,Proving that a standard state-space model is linear,"I recently learned about the standard continuous-time state-space model ( wiki ): $$\begin{align}
\dot{x}(t) &= Ax(t) + Bu(t) \\
y(t) &= Cx(t) + Du(t).
\end{align}$$ It's known that these systems are LTI (linear time-invariant), but that's not entirely obvious to me. Take the linearity of some system $G$ , for example, which stipulates among other things that $$\alpha u(t) \rightarrow G \rightarrow \alpha y(t).$$ I don't see how this is implied by the CT SSM above. Am I making a super simple mistake in my analysis below? Let $C=1, D=0$ for simplicity so that $y(t) = x(t)$ . This reduces the problem to solving the first differential equation, whose solution is the sum of the homogeneous solution and some particular solution, which comes out via variation of parameters to $$y(t) = x_0 e^{At} + \int K e^{A(t-\tau)} \, Bu(\tau) \, d\tau.$$ From this, it doesn't seem like $y(t)$ should always be linear w.r.t. $u(t)$ . What am I missing / what have I got wrong? Why then are these systems said to be linear?","['signal-processing', 'ordinary-differential-equations']"
4835219,Characteristic function of product of two random variables with arbitrary normal distributions,"I have $X\sim N(0,5)$ and $Y\sim N(1,1)$ components of a gaussian random vector. The covariance of $X$ and $Y$ is 2. I've already proved that $\frac{X}{2}-Y$ is independent from $Y$ . I have to calculate the characteristic function of $\left(\frac{X}{2}-Y\right)\frac{Y}{2}$ . My attempt is the following. $$\begin{align}\varphi_{\left(\frac{X}{2}-Y\right)\frac{Y}{2}}(\theta)&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} e^{i\theta\left(\frac{x}{2}-y\right)\frac{y}{2}}f_{X,Y}(x,y) dxdy\\&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{i\theta \frac{xy}{4}}e^{-i\theta\frac{y^2}{2}}f_X(x)f_Y(y)dxdy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}f_Y(y)\left(\int_{-\infty}^{+\infty}e^{i\theta \frac{xy}{4}}f_X(x)dx \right)dy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}f_Y(y)\varphi_{X}\left(\frac{\theta y}{4}\right)dy\\&=\int_{-\infty}^{+\infty}e^{-i\theta\frac{y^2}{2}}e^{-\frac{1}{2}\left(\frac{5y^2}{16}\theta^2\right)}f_Y(y)dy\end{align}$$ Now, I'm stuck. How can I rewrite that last integral? Maybe I can view it as a Gaussian characteristic function...","['characteristic-functions', 'normal-distribution', 'probability']"
4835234,Covering a Square Floor with Square Rugs,"You are given a finite collection of axis-aligned square rugs. (You do not choose the collection of rugs that you receive and the rugs are not necessarily all the same size.) Your objective is to move the rugs, without rotating them, to completely cover an axis-aligned square floor. The rugs are allowed to overlap. Can you cover the entire floor if the total area of the rugs is three times the area of the floor? I made this problem up myself and I have not been able to prove it or find a counterexample. My work so far: Let the table have area 1. Let $N$ be the number of rugs. If $N \in {1,2,3}$ then the answer is trivial. So assume $N \geq 4$ . Let $R_i$ be the $i$ -th rug. Let $r_i$ be the area of the $i$ -th rug where $r_i \geq r_{i+1}$ for each $i$ . Assume for all $i$ , $r_i < 1.$ Let $N = 4$ . If each rug has area greater than $\frac{1}{4}$ . Then we can divide the floor into quarters and cover each quarter with a rug. So we assume otherwise and so $r_4 < \frac{1}{4}$ . Then $\frac{r_1 + r_2 + r_3}{3} > \frac{11}{12}$ . Therefore $r_1 > \frac{11}{12}$ . Suppose $r_1 \approx 1$ and $r_2 = r_3$ . We find $r_2 > \frac{7}{8}$ . Suppose $r_1 = r_2 \approx 1$ . We find $r_3 > \frac{3}{4}$ . Place $R_1$ and $R_2$ in opposite corners. This will leave two uncovered rectangles with side lengths $1 - \sqrt{r_1}$ and $1 - \sqrt{r_2}$ . Place $R_3$ in one of the open corners. Then we need that $r_4 \geq \left(1 - \sqrt{r_2}\right)^2$ to cover the floor. Now let $r_1$ and $r_3$ be at their maximums. So that $r_2 + r_4$ is as small as possible. Let $r_1 \approx 1$ . Let $x = r_2 = r_3$ then $r_4 = 3 - (1 + x + x) = 2 - 2x$ . Then using the previous inequality, we need that $2 - 2x > \left(1 - \sqrt{x}\right)^2$ . This inequality is true for $0 \leq x < 1$ . Therefore it is always possible to cover the floor for $N = 4$ . Generalizing, we can assume for $N \geq m^2$ , that there are only $m^2 - 1$ rugs with area greater than $\frac{1}{m^2}$ . So that $r_{m^2} < \frac{1}{m^2}$ . $r_1 > \frac{3 - \frac{N - m^2+1}{m^2}}{m^2-1}$ $r_2 > \frac{2 - \frac{N - m^2+1}{m^2}}{m^2-2}$ $r_3 > \frac{1 - \frac{N - m^2+1}{m^2}}{m^2-3}$ I have more to do.","['geometry', 'tiling']"
4835241,Product of matrices associated to bilinear forms,"Let $V$ be a real vector space of finite dimension where $f,g: V \times V \to \mathbb{R}$ are two symmetric positive semidefinite bilinear forms. For a given basis $B$ of $V$ , there are two symmetric positive semidefinite matrices $F$ and $G$ that represent $f$ and $g$ , respectively. The matrix product $H=FG$ represents a positive semidefinite bilinear form $h$ on $V$ . I think the definition of $h$ does not depend on the basis $B$ and therefore neither on the associated matrices. How do I define the operation $(f,g)\mapsto h$ without making reference to associated matrices?","['matrices', 'linear-algebra', 'linear-transformations']"
4835305,Is equality an algebraic structure?,"A set $S$ is called an algebraic structure with respect to binary operator $*$ if it follows closure property. $(\mathbb{N},+)$ is such example, as sum of natural numbers gives natural numbers. Is $(\mathbb{N},=)$ also algebraic structure? My professor said NO, but I guess it should be, as it clearly follows the closure property: $1 = 1$ $2 = 2$ $...$ Where am I going wrong?","['group-theory', 'abstract-algebra']"
4835316,Some examples of rigidity lemma,"We say that a category with finite products obeys the rigidity lemma if any morphism $X \times Y \rightarrow Z$ that is constant along some point $y \in Y$ is globally independent of $X$ . Such categories have remarkable features - all group objects are abelian, all pointed maps between them are homomorphism, group structures are unique. The canonical example is the category of proper smooth varieties, but I wonder if there are any other interesting ones (both toy and real ones will make me grateful). EDIT:
By a point I mean the morphism from terminal object, by constant map - factoring through terminal map, by independent - factoring through projection.","['algebraic-geometry', 'category-theory']"
4835341,Quantitative bound on Wasserstein distances by $L^p$ distances?,"Given two smooth probability densities $f$ and $g$ on $\mathbb{R}$ (or $\mathbb{R}_+$ ) with finite $p$ -th moments. I am wondering if anyone is aware of some explicit upper bound on $W_p(f,g)$ in terms of $\|f-g\|_{L^p}$ (especially for $p=1$ and $p=2$ ) ? As one typically view Wasserstein distances as ""weak"" metrics and $L^p$ metrics as ""strong"" metrics. It is very natural to search for some explicit bound on $W_p(f,g)$ in terms of $\|f-g\|_{L^p}$ . However, I did not find any useful references in this regard. Any help or pointers to literatures are greatly appreciated! Remark: It is well-known that $W_1(f,g)$ has a nice formulation in terms of cumulative distribution functions (while it seems that $W_2$ has no such nice representation). Indeed, let $F$ and $G$ represent the cumulative distribution functions corresponding to the densities $f$ and $g$ , respectively. Then we have $$W_1(f,g) = \int |F(x)-G(x)| \mathrm{d}x$$ So in some sense, $W_1(f,g)$ looks like some $L^1$ norm (except that we are using the cdf's instead the pdf's). I am very curious as to why $$\int |F(x)-G(x)| \mathrm{d}x$$ can be somehow controlled by $$\int |f(x)-g(x)| \mathrm{d}x,$$ and if possible, I would like to see an explicit bound.","['normed-spaces', 'metric-spaces', 'analysis', 'wasserstein', 'probability']"
4835342,Left tail bound for stopping time of a gaussian random walk,"Given a sequence of i.i.d. random variables $X_1,X_2,\cdots,X_n\sim N(\mu,1)$ and define the sum (gaussian random walk) as $S_n(\mu) = \sum\limits_{i=1}^n X_i$ . We want to say something about the quantity $N(\mu)$ , which is defined as $$
N(\mu): =\inf\left\{n\in \mathbb N^+ : e^{-\frac{1}{n}S_n(\mu)^2}\leq \alpha\right\} = \inf\left\{n\in \mathbb N^+: |S_n|\geq \sqrt{-n\log\alpha}\right\}
$$ where $\alpha = \Omega(1)\in (0,0.5)$ , say $0.025$ . The distribution of $N(\mu)$ itself is untractable, but the right tail is fairly easy to be bounded via any Hoeffding-type inequality. The question is about the left tail: $$
\mathbb P(N(\mu)\leq \beta)\text{ or any thing control N is small},
$$ where $\beta$ is presumably $\beta\sim -\log \alpha \cdot \mu^{-2}$ this order. I found this paper and reference therein study a similar case (in a fairly general framework with weaker assumptions), but they don't consider the left tail bound. The reason why I want to consider left tail bound is to upper bound $\text{Var}(\bar{X}_N)$ or equivalently $E[1/N]$ (or something like $E[1/N^2]$ ).","['statistics', 'concentration-of-measure', 'random-walk', 'stopping-times', 'probability-theory']"
4835355,"Prove that in $\mathbb R$, the integral of the derivative of an integrable function is $0$","$\quad$ Let $f$ be a differentiable function in $\mathbb R$ , and $f$ is also a Lebesgue integrable function in $\mathbb R$ . Let $f'$ be a Lebesgue integrable function in $\mathbb R$ , too. How to prove that: $$\int_{\mathbb R}f'(x)\mathrm dx=0$$ $\quad$ My intuition tells me that when a function is integrable over the whole space, that is, when its integral is finite, the value of the function always starts at $0$ and ends at $0$ . The way up is the way down, so the derivative's integration is $0$ . But I know that even if the function is integrable on the whole space, it doesn't have to be $$\lim_{x\to\infty}f(x)=0$$ This function is not necessarily an absolutely continuous function, so there seems to be no direct relationship between the integral of its derivative and the original function in my mind. I can't think any more. Is there any way to deal with this problem? Any discussion and suggestions are welcome! Edit:
According to PhoemueX, I learned that $f$ is absolutely continuous over any interval $[a,b]$ . Based on Kurt G's answers and Stefan's hints, I understand the significance of limits there. So it seems that we can get the result without doing an odd-even decomposition. My idea is that already known $$\int_a^bf'(x)\mathrm dx=f(b)- f(a)$$ and $f'(x)$ is integrable, so $$\lim_{a\to+\infty}\int_{-a}^0f'(x)\mathrm dx,\quad\lim_{b\to+\infty}\int_0^bf'(x)\mathrm dx$$ both exist, so $$\lim_{a\to+\infty}(f(0)- f(-a)),\quad\lim_{b\to+\infty}(f(b)- f(0))$$ both exist, therefore $f(\pm\infty)$ both exist. Then according to $f$ being integrable, $f(\pm\infty)$ can only be $0$ , so we get the final conclusion.","['integration', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'derivatives']"
4835369,Polynomial identity : $|P(z)|^2=|Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{D}$,"Let us denote by $\mathbb{D}=\{z : |z| <1\}$ and $\mathbb{T}=\{z: |z|=1\}$ . Suppose that $P, Q$ and $R$ are polynomials that satisfy the following: $|Q(z)| \geq |R(z)| $ for all $z \in \overline{\mathbb{D}}$ $|P(z)|^2=|Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{T}$ $P$ and $Q$ are non-vanishing on $\overline{\mathbb{D}}$ $|P(z)|^2 \leq |Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{D}$ . My question : $|P(z)|^2=|Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{D}$ ? I am trying to prove it. I think some version of maximum-modulus or identity theorem can be useful here. But what functions to choose to apply maximum modulus. Any hint how to proceed. My Attempt: Note that for any $z \in \mathbb{T}$ , we have $P(z)\overline{P(\frac{1}{\overline{z}})}=|P(z)|^2$ . Thus, condition (2) implies that $P(z)\overline{P(\frac{1}{\overline{z}})}= Q(z)\overline{Q(\frac{1}{\overline{z}})} -R(z)\overline{R(\frac{1}{\overline{z}})} $ for all $z \in \mathbb{T}$ . Hence, the map $F: \mathbb{D}\setminus \{0\} \to \mathbb{C}, F(z)= P(z)\overline{P(\frac{1}{\overline{z}})} - Q(z)\overline{Q(\frac{1}{\overline{z}})} - R(z)\overline{R(\frac{1}{\overline{z}})}$ is zero on $\mathbb{T}$ .","['complex-geometry', 'complex-analysis', 'maximum-principle', 'complex-numbers', 'analytic-functions']"
4835377,Evaluating $\int_0^1 \frac{x\cdot \tanh^{-1}(x)}{3+x^2} dx$,"I was trying to evaluate the integral $$ I = \int_0^1 \frac{x \tanh^{-1}x}{3+x^2} dx$$ To solve this, I considered the family of integrals $$ I(a) = \int_0^1 \frac{ x \tanh^{-1}ax}{3+x^2} dx$$ Note that $I(0) = 0$ . Using Leibnitz rule, we get: $$ I'(a) = \int_0^1 \frac{(x^2+3)-(3)}{(3+x^2)(1-a^2x^2)}dx$$ $$I'(a) = \int_0^1 \frac{dx}{1-a^2x^2} + 3\cdot \int_0^1 \frac{dx}{(x^2+3)(a^2x^2-1)}$$ Multiplying and dividing by $a^2$ for the second integral, we get: $$I'(a) = \frac{1}{2a} \cdot \ln(\frac{1+a}{1-a}) + 3\cdot a \cdot \int_0^1 \frac{adx}{(a^2x^2+3a^2)(a^2x^2-1)}$$ For the second integral, I substitute $ax = v$ $$I_0(a) = 3a\cdot \int_0^a \frac{dv}{(v^2+3a^2)(v^2-1)}$$ $$ = \frac{3a}{3a^2+1} \cdot (\int_0^a \frac{dv}{v^2-1} - \int_0^a \frac{dv}{v^2+3a^2})$$ $$I_0(a) = \frac{3a}{3a^2+1} \cdot ( \frac{1}{2}\cdot \ln(\frac{1-a}{1+a}) - \frac{1}{\sqrt3 a}\cdot \frac{\pi}{6})$$ Adding the integrals, we get: $$I'(a) = \frac{1}{2} \cdot (\frac{1}{a} - \frac{3a}{3a^2+1}) \ln(\frac{1+a}{1-a}) - \frac{\sqrt3}{3a^2+1} \cdot \frac{\pi}{6} $$ This means that $$ I(b) = \int_0^b I'(a) da$$ But I'm not sure how to integrate the first part of $I'(a)$ - that is, I am struggling to evaluate: $$I_1(b) = \int_0^b \frac{1}{2} \cdot (\frac{1}{a}-\frac{3a}{3a^2+1}) \ln(\frac{1+a}{1-a}) da$$ $$I_1(b) = \int_0^b \frac{1}{2} \cdot \frac{1}{a(3a^2+1)} \ln(\frac{1+a}{1-a}) da$$ We proceed by substituting $\frac{1-a}{1+a} = t$ in the integrand and obtain: $$I_1(b) = -\frac{1}{4}\cdot \int_{\frac{1-b}{1+b}}^1 \frac{\operatorname{ln}t\cdot (1+t)}{(t^2-t+1)\cdot (1-t)}dt$$ Can anyone help me in evaluating this specific integral? I would like to know the strategies used to evaluate integrals of such kinds. Any help is appreciated. Thanks for reading!","['integration', 'calculus', 'definite-integrals']"
4835400,How to prove that an absolutely continuous function space on a finite closed interval is a separable space in this norm sense?,"$\quad$ Let a finite closed interval be $[a,b]$ . Consider the absolutely continuous function space $\mathrm{AC}([a,b])$ on it, with a norm as follows: $$\|f\|_{\mathrm{AC}}=\sup_{x\in[a,b]}|f(x)|+\|f'\|_{\mathcal L^1([a,b])}$$ $\quad$ How to prove that $\mathrm{AC}([a,b],\|\cdot\|_{\mathrm{AC}})$ is separable? $\quad$ I already know that this space is Banach, and I want to find a countable dense subset of it. I've tried using Weierstrass approximation theorem, and found polynomials whose coefficients are rational, but I can't say that $\|f'-p'\|_{\mathcal L^1([a,b])}<\varepsilon$ . Although I know that this is equal to the total variation of a function, it doesn't help a lot. Is there a problem with my direction? If yes, how do you find such a subset?","['weierstrass-approximation', 'separable-spaces', 'real-analysis', 'absolute-continuity', 'functional-analysis']"
4835450,"Proof verification: Does $\sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1$ hold?","I am going back to a question I posted yesterday. This is because I think I have made some steps forward. Let $(E, \|\cdot\|)$ denote a Hilbert space such that $E$ is continuously embedded in $L^q(\mathbb R)$ for any $q\in [2, 4]$ . Let $(u_n)$ be a bounded sequence in $E$ , i.e. there exists $M>0$ such that $\|u_n\|_{E}\le M$ . Let $p\in (2, 3)$ and $F:\mathbb R\times \mathbb R\to\mathbb R$ be a continuous function such that $$|F(x, u)|\le c_1 |u| +c_2 |u|^{p-1}\quad\mbox{ for a.e. } x\in\mathbb R, \ \forall u\in\mathbb R,$$ for some $c_1, c_2>0$ . Under these assumptions, I think that it is possible to conclude that $$\sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1,$$ for a constant $M_1>0$ . Indeed: $$
\begin{split}
\int_{\mathbb R}|F(x, u_n)|^2 dx &\le c_1 \int_{\mathbb R} |u_n|^2 dx + c_1 c_2 \int_{\mathbb R} |u_n|^p dx + c_2^2 \int_{\mathbb R} |u_n|^{2(p-1)} dx \\
&\le k_1 \|u_n\|_E \le k_1 M =:M_1.
\end{split}
$$ Hence, passing to the supremum, one has $$\sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1.$$ Anyone could please tell me if I argued it right? Thank you in advance.","['calculus', 'solution-verification', 'lp-spaces', 'real-analysis']"
4835491,Probability of Non-Matching Colors in Stacked Book Piles.,"Three orange books, three blue books, and three green books are randomly stacked to form three piles of three books each.
What is the probability that no book will be the same color as the book directly above it? Using a decision tree in excel I found that the probability is 1/5. The R code below gives the same result. library(MASS)
library(RcppAlgos)
library(magrittr)

permuteGeneral(c(""A"", ""B"", ""C""), freqs = c(3, 3, 3), FUN = \(x) {
  if (x[1] != x[2] && x[2] != x[3] && x[4] != x[5] && x[5] != x[6] &&
    x[7] != x[8] && x[8] != x[9]) {
    TRUE
  } else {
    FALSE
  }
})  %>% unlist() %>% {fractions(sum(.)/length(.))} But surely there must be a more elegant solution. Can anyone help?","['combinatorics', 'probability']"
4835505,Inequality involving $f(x)=e^x-x^2/2$ with $f'(a)=f'(b)$ ($a\ne b$),"I'm currently working on a high-school level mathematical problem and have developed a solution approach, but I'm stuck at the final step. The problem is as follows: Consider the function $f\left(x\right)=\text{e}^x-\dfrac{x^2}{2}$ . Let $a$ and $b$ be real numbers such that $a\neq b$ and $f^\prime\left(a\right)=f^\prime\left(b\right)$ . Prove that $f\left(a\right)+f\left(b\right)<2$ . And here's my approach (failed): Given that $f'(x) = \text{e}^x - x$ , it implies that $\text{e}^a - a = \text{e}^b - b$ . Let's set $a = \ln m$ and $b = \ln n$ . By symmetry, we can assume $b < 0 < a$ , which leads to $0 < n < 1 < m$ . This results in $m - \ln m = n - \ln n$ . Hence, $m - n = \ln m - \ln n = \ln \dfrac{m}{n}$ . Let's denote $t = \dfrac{m}{n}$ , then $t > 1$ and $m = nt$ . Consequently, $nt - n = \ln t$ , which leads to $n = \dfrac{\ln t}{t - 1}$ and $m = nt = \dfrac{t \ln t}{t - 1}$ . Thus, $f(a) + f(b) = \text{e}^a + \text{e}^b - \dfrac{a^2}{2} - \dfrac{b^2}{2}$ $= m + n - \dfrac{1}{2}\left(\ln^2m + \ln^2n\right)$ $= \dfrac{t + 1}{t - 1} \cdot \ln t - \dfrac{1}{2}\left[\ln^2\left(\dfrac{t \ln t}{t - 1}\right) + \ln^2\left(\dfrac{\ln t}{t - 1}\right)\right]$ Now, the task is to prove that this function of $t$ , denoted as $g(t)$ , is always less than 2. However, I am unable to prove this. Upon graphing, I found that the function is monotonically increasing on the interval $(0, 1)$ , monotonically decreasing on $(1, +\infty)$ , and has a removable discontinuity at the point $(1, 2)$ , with $\lim\limits_{t \to 1} g(t) = 2$ . Here's what GeoGebra gave me on this function I would greatly appreciate any guidance or alternative approaches. This problem has been a challenging puzzle for me, and as a beginner with a strong interest in mathematics, I am eager to learn.","['real-analysis', 'calculus', 'inequality', 'exponential-function', 'problem-solving']"
4835565,Perpendicularity of the interior tangents of two circles to a parabola,"Among the large number of findings I have arrived at with the help of GeoGebra is this feature: If the two common interior tangents of two distant circles are perpendicular, each of which touches the same parabola at two points, then the circle passing through the four points of contact touches the same parabola in turn, and the distance between the center of the circle and the perpendicular point will be equal to twice the distance between the focus and the guide. The circle passing through the points $P,Q,R,T$ touches the parabola $C$ at two points. $AM=BM$ $MN=2OF$ I came up with this about four years ago but have no idea how to prove it Can someone help me please","['euclidean-geometry', 'conic-sections', 'geometry']"
4835595,No. of ways to arrange a duplicated list of numbers so that a copy of the list appears in order but not necessarily together,"I was making combinatorics exercises for some students when I stumped myself with a question I came up with. Question Bob has ten blue counters, labelled 1, 2, 3, 4, 5, 1, 2, 3, 4, 5. If he rearranges all ten randomly in a line, how many different arrangements include a subsequence of five counters that read 1, 2, 3, 4, 5 in that order (but not necessarily together)? For example, 1 , 3, 2 , 5, 1, 4, 3 , 4 , 5 , 2 would satisfy the conditions. So what have I tried? I tried simplifying the problem to: two counters (1, 1) in which the number of ways the subsequence 1 appears somewhere is (trivially) 1. four counters (1, 2, 1, 2) in which the number of ways the subsequence 1, 2 appears somewhere is 5. six counters (1, 2, 3, 1, 2, 3) in which the number of ways the subsequence 1, 2, 3 appears somewhere is 47. I verified this in two ways. Firstly, I wrote out all 90 cases and counted manually. Then, I realised that any case where the subsequence 1, 2, 3 appears in order has the subsequence 1, 2 appearing in order. So, any solution to the 6-counter problem is simply a solution to the 4-counter problem, plus two carefully-positioned 3s. This broke the question into 5 cases which were simpler to exhaust. For the question itself, I know the answer is lower than $10P5$ . $10P5$ is the answer to the following method: Given ten positions in a line, pick five to permute (i.e, place the counters 1, 2, 3, 4, 5 on, in a random order). Then place the remaining counters 1, 2, 3, 4, 5 in that order in the remaining positions. However, this grossly overcounts many cases. I think it overcounts each time a case has multiple subsequences of 1, 2, 3, 4, 5. For example, the case of 1, 1, 2, 2, 3, 3, 4, 4, 5, 5 is counted (I think) $2^5 = 32$ times. The case of 1 , 2, 4, 2, 3 , 4 , 3, 5, 5, 1 is counted four times. This is because every subsequence of 1, 2, 3, 4, 5 must include the bolded 1, 3 and 4, but there are two options for the 2s and two options for the 5s. So there are four subsequences of 1, 2, 3, 4, 5 in this case. As a side note, is there a way to generalise this for $2n$ counters, labelled 1 through $n$ twice over?","['permutations', 'combinatorics']"
4835603,Is the study on the theory of the integral of Denjoy's and Perron's an active field nowadays?,"I am an undergraduate student knowing the basic definition on the integral of Denjoy's and Perron's. I wonder if this topic is still active nowadays. Is the integral of Denjoy's and Perron's worth devoting time to study for a student planning to read a master/ PhD degree in analysis? Is the study on the theory of the integral of Denjoy's and Perron's an active field nowadays? What problem interests researchers on these two integrals most? Is there some ""big"" open problems students with the knowledge of measure theory and functional analysis can understand? What branch of analysis uses the integral of Denjoy's and Perron's frequently?","['integration', 'soft-question', 'analysis', 'real-analysis']"
4835643,"Conjecture: $\binom{n}{k } \mod m =0$ for all $k=1,2,3,\dots,n-1$ only when $m $ is a prime number and $n$ is a power of $m$","While playing with Pascal's triangle, I observed that $\binom{4}{k } \mod 2 =0$ for $k=1,2,3$ ,and $\binom{8}{k } \mod 2 =0$ for $k=1,2,3,4,5,6,7$ This made me curious about the values of $n>1$ and $m>1$ that satisfy $\binom{n}{k } \mod m =0$ for all $k=1,2,3,\dots,n-1$ I asked my friend to write a $C++$ code that could test this condition for $n$ up to $10000$ and $m$ up to $31$ and I found out that $\binom{n}{k } \mod m =0$ for all $k=1,2,3,\dots,n-1$ only when $m $ is a prime number and $n$ is a power of $m$ . #include <bits/stdc++.h>
using namespace std;`
typedef long long ll;
#define rep(i , st , ed) for(int i = st; i < ed; i++)
#define f first
#define s second
const int N = 10000;
const int mod = 31;
ll nCr[N][N] , frq[N];
void gen(){
nCr[0][0] = 1;
for (int i = 0; i < N; ++i) {
    nCr[i][0] = nCr[i][i] = 1;
    for (int j = 1; j < i; ++j) {
        nCr[i][j] = (nCr[i-1][j] + nCr[i-1][j-1]) % mod;
        if(nCr[i][j] == 0) frq[i]++;
    }
}
}
ll get(int n , int r){ return (n >= r) ? nCr[n][r] : 0;  }
int main(){
ios::sync_with_stdio(0); cin.tie(NULL); cout.tie(0);

gen();
for(int i = 1; i < N; ++i) if(frq[i] == i - 1){
    cout << i << '\n';
}
} After many attempts to prove or disprove this conjecture, I couldn’t find a solution. This statement is the same as $$\gcd\left( \binom{n}{1 }, \binom{n}{2 },\dots , \binom{n}{n-1 }     \right)=
\begin{cases}
p,  & \text{if $n$ is a power of a prime $p$} \\[2ex]
1, & \text{else. }
\end{cases}$$","['conjectures', 'modular-arithmetic', 'number-theory', 'binomial-coefficients', 'combinatorics']"
4835672,A fascinating sequence of polynomials,"So, I have a sequence of polynomials: $$
p_m(z) = 2\sum_{k=1}^{m-1}(1-\tfrac km)z^k = \frac{2z}{1-z}\Big(1-\frac 1m\sum_{k=0}^{m-1}z^k\Big),
$$ where the last expression only holds for $z\neq 1$ . Fascinatingly, when you plot the zeros of $p_m$ , you see that they lie outside of the unit circle line $\mathbb T$ and approach $\mathbb T$ for $m\to\infty$ so that for large $m$ the zeros of $p_m$ almost form the unit circle. However, $z=1$ is never a zero and the gap around $z=1$ is larger than around any other $z\in\mathbb T$ . This gap becomes smaller and smaller with growing $m$ . What I also observed (by plots for many values of $m$ ) is the following: $\operatorname{Re}p_m(e^{it})$ is very close to (but always larger than) $-1$ on an interval $[\delta,2\pi-\delta]$ and approaches $p_m(1) = m-1$ rapidly at the boundary of $[0,2\pi]$ . My question: For given $\delta>0$ , I'd like to find an $m_\delta$ such
that for $m\ge m_\delta$ we have $\operatorname{Re}p_m(e^{it})\le 0$ for all $t\in [\delta,2\pi-\delta]$ . I can write down the trigonometric polynomial $\operatorname{Re}p_m(e^{it}) = 2\sum_{k=1}^{m-1}(1-\tfrac km)\cos(kt)$ , but I cannot seem to bound this guy. I'm not even able to show that $\operatorname{Re}p_m(e^{it})\ge -1$ for all $t$ . Can anyone help?","['complex-analysis', 'polynomials', 'analysis', 'real-analysis']"
4835673,reciprocals of squarefree k-almost primes,"First of all, I'm a novice in math and English is not my native language, so I apologize in advance for any incorrect wording, etc. Definitions and specific example Let's define the set $A_4 = \{2,3,5,7\}$ , with the first 4 prime numbers. And $r(A_4) = \frac{1}{2} + \frac{1}{3} + \frac{1}{5} +\frac{1}{7} = \frac{247}{210}$ , as the sum of the reciprocals of the elements of $A_4$ . Let's now define the set $B_4 = \{6,10,14,15,21,35\}$ , with the squarefree semiprimes with prime factors $p$ such that $p \in A_4$ . Now, $r(B_4) = \frac{1}{6} + \frac{1}{10} + \frac{1}{14} +\frac{1}{15} +\frac{1}{21} +\frac{1}{35} = \frac{101}{210}$ . And in a similar way, $C_4 = \{30,42,70,105\}$ , with the squarefree 3-almost primes and $r(C_4) = \frac{1}{30} + \frac{1}{42} + \frac{1}{70} +\frac{1}{105} = \frac{17}{210}$ . And finally, $D_4 = \{210\}$ , with a square free 4-almost prime and $r(D_4) = \frac{1}{210}$ . Observation We see: $$r(A_4) > r(B_4) > r(C_4) > r(D_4)$$ Generalization Now change $A_4$ into $A_n$ , where $n$ is a positive integer. I assume my observation holds for other - higher - values of $n$ . So that: $ r(A_n) > r(B_n) > r(C_n) > \dots > r(Z_n)$ , where Z doesn't mean the 26th term of the sequence, but in general 'the last' or n-th term. (I was running out of capitals... ) Question Does my observation holds for all values of $n$ ? Who can I prove it (Euler probably
already did it for me...)? Useful links to literature are welcome. EDIT: counter example Following Greg Martin's thorough answer (thx!), I make a cautious attempt to formulate a 'counter argument'. Let's compare the first and second to last terms of the series based on $A_5 = \{a,b,c,d,e\}$ . Both terms consist of the same amount of sub-terms. first term: $$r(A_5) = u_1 = \frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}+\frac{1}{e}$$ second to last term: $$u_{n-1}=\frac{1}{acde}+\frac{1}{bcde}+\frac{1}{bcde}+\frac{1}{bcde}+\frac{1}{bcde}$$ It's clear that the first sub-term in $u_{n-1}$ is smaller than the first of $u_{1}$ . The same holds for the other sub-terms. So: $$u_{1} > u_{n-1}$$ This is true for all values of $n$ . It gives me the intuitive idea that the '<' and '>' signs 'flips' at some point. So, let me restate my question. Does my observation holds for all sufficiently large values of $n$ , starting from a certain term $N$ ? So that: $$r(A_n) < r(B_n) < r(C_n) < \dots < r(N-1_n) < r(N_n) > r(N+1_n) \dots r(Z_n)$$","['semiprimes', 'prime-numbers', 'sequences-and-series']"
4835684,"When flipping coins, what is the probability that you'll repeat yourself completely at some point?","You play the following game: you flip a coin over and over again and write down the sequence of Heads (H) and Tails (T). The game ends when you repeat the whole sequence from the beginning. So for example HH or HTHHHTHH would terminate. What is the probability that the game is finite? It has to be bigger than 50% since this is the probability of starting with HH or TT, ending the game instantly. But the longer the game goes, the less likely it becomes that it ends at all. I ran the game a few times with the computer and it seems that a probability of 75% may be the answer.","['stochastic-processes', 'probability', 'sequences-and-series']"
4835710,how to solve this trigonometric reduction question [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question The question asks to put $\sin^5 \theta$ in the form $$
A\sin 5\theta + B\sin 3\theta + C\sin \theta. 
$$ I have tried changing the $\sin$ into a $\sin^4$ and then putting it in terms of cos, and then using the reducing formula. But I end up with a very long equation, as it is only a 4 mark question on the paper I am doing I assume I have made a mistake along the way. Does anyone have any alternate ideas as how best to solve it?","['algebra-precalculus', 'trigonometry', 'reduction-formula']"
4835711,Is it ok to cancel out fractions while integrating?,"Is it ok to cancel out fractions while integrating? If yes, how?
For example: $$\int \frac{x^3+1}{x+1}dx = \int \frac{(x+1)(x^2-x+1)}{x+1}dx = \int (x^2-x+1)dx = \frac{x^3}3-\frac{x^2}2+x + C$$ Wouldn't it be undefined if $x = -1$ as the divisor is $0$ ?","['integration', 'fractions']"
4835712,"Given, $X_n \to \xi \sim N(0, A)$, Show $X_n = o_P(1)$ iff $A = 0$","The title is a general statement, and I am trying to apply it in the following problem. Suppose $$
X_n\stackrel{\mathcal{D}}{\rightarrow} N\left(0, E\left\{\left(\varphi-\varphi^*\right)\left(\varphi-\varphi^*\right)^T\right\}\right)
$$ In the textbook Semiparametric theory by Tsiatis Chapter 3, the author mentioned in order for this limiting normal distribution to be $o_p(1)$ , we would require that the covariance matrix $$E\left\{\left(\varphi-\varphi^*\right)\left(\varphi-\varphi^*\right)^T\right\}=0^{q \times q}$$ . Intuitively it makes sense,  but I don't know how the formal proof goes. Is it just simply because - if we want derive converges to 0 in probability, we need the variance to be zero?","['expected-value', 'random', 'statistics', 'probability']"
4835787,Subset relation for the power set of the Cartesian product,"I'm trying to assess whether the below subset relation is true: $$
\mathcal{P}(A \times B) \subseteq \{A_0 \times B_0 \mid (A_0, B_0) \in \mathcal{P}(A) \times \mathcal{P}(B)\}. 
$$ I am fairly sure that the statement is false, but I am not able to intuitively see why to the point where I can come up with a natural counterexample. The example I tried was: Let $A = \{0,1\}$ and $B = \{2\}$ . Then $A \times B = \{(0,2), (1,2)\}$ , and we have $$
\mathcal{P}(A \times B) = \{\emptyset, \{(0,2)\}, \{(1,2)\}, A \times B \}
$$ and $$
\mathcal{P}(A) \times \mathcal{P}(B) = \{\emptyset, \{0\}, \{1\}, A \} \times \{\emptyset, B\} = \{(\emptyset, \emptyset), (\emptyset, B), (\{0\}, \emptyset), (\{0\}, B), (\{1\}, \emptyset), (\{1\}, B), (A,\emptyset), (A,B) \}. 
$$ Notice then that $$
\{A_0 \times B_0 \mid (A_0, B_0) \in \mathcal{P}(A) \times \mathcal{P}(B)\} = \{\emptyset, \{(0,2)\}, \{(1,2)\}, A \times B \}.
$$ I think I must have done something wrong, as this came out to an equality. I'm not certain if I need to try, say, a case where $A$ and $B$ both have two elements, though it isn't clear to me why that would work and letting one of the sets be a singleton would not.",['elementary-set-theory']
4835885,Epsilon-Delta analysis for the Law of Large Numbers?,"Law of Large Numbers: For a sequence of independent and identically distributed random variables $X_1, X_2, X_3, ..., X_n$ , each with an expected value $E[X_i] = \mu$ and sample estimator $\overline{X_n}$ = $\frac{1}{n} \sum_{{i=1}}^{n} x_i$ : $$\lim_{{n \to \infty}} P\left( \left| \overline{X_n} - \mu \right| \geq \epsilon \right) = 0$$ My Question: Provided $X_1, X_2, X_3, ..., X_n$ are all iid and all come from some specific Probability Distribution Function $g(x; \theta)$ : What value of $n$ is required to ""probabilistically"" achieve a certain value of $\epsilon$ ? As an example, suppose I have 100 randomly generated iid data points. I believe these points  came from a Normal Distribution.  Suppose there was some ""magic"" way of knowing that these $n$ = 100 data points were actually generated from  a Normal Distribution with $\mu=5$ , $\sigma=5$ . Then, on average (i.e. if we had access to the infinite universe of all samples of size $n$ = 100 from a Normal Distribution with $\mu=5$ , $\sigma=5$ ) : For $n$ = 100, what would be the average value of $\epsilon$ ? Can this be answered using Cramer's Theorem of Large Deviations? https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_theorem_(large_deviations) ? Thanks! Note: For $n$ = 100 ,would this average value of $\epsilon$ vary for different Normal Distributions? e.g. ( $\mu=5$ , $\sigma=5$ ) vs ( $\mu=2$ , $\sigma=3$ ) For $n$ = 100, would this average value of $\epsilon$ vary for different Probability Distributions Functions? e.g.  Normal Distribution vs Exponential Distribution vs Gamma Distirbution, etc? Follow up Question: Applying Hoeffding's Inequality in Real Life","['law-of-large-numbers', 'probability']"
4835920,Solving the Functional Equation $f(f(x))=x$ gone wrong,"For a function to be its inverse (i.e. an involution), it needs to satisfy the functional equation $f(x)=f^{-1}(x)$ or $f(f(x))=x$ . I expressed $f^{\circ n}(x)$ (the composition of $f(x)$ to itself $n$ times, with $n=0$ corresponding to $x$ ) as a recurrence relation $a_n$ . So $a_n=a_{n-2}$ . The characteristic equation of this recurrence is $r^2-1=0$ and $r=\pm1$ . Therefore, $a_n=a(x)(1)^n+b(x)(-1)^n=a(x)+b(x)(-1)^n$ , where $a(x)$ and $b(x)$ are functions to be determined. When $n=0$ , we get that $a(x)+b(x)=x$ , and when $n=1$ we get that $a(x)-b(x)=f(x)$ . So I thought that, if $a(x)+b(x)=x$ , then $a(x)-b(x)$ is an involution. But this is easily contradicted by $a(x)=2x$ and $b(x)=-x$ . Where did I go wrong? If we let $n=2$ , we get $a(x)+b(x)=f(f(x))$ so it seems alright.","['involutions', 'functional-equations', 'functions', 'solution-verification']"
4835944,Insurance company with claims following a Poisson Process. Calculate the probability that the capital is always positive throughout the first 4 days.,"Suppose that claims are made to an insurance company according to a Poisson process with rate $10$ per day. The amount of a claim is a random variable that has an exponential distribution with mean $1,000$ dollars. The insurance company receives payments continuously in time at a constant rate of $11,000$ dollars per day. Starting with zero initial capital, find the probability that the firm’s capital is always positive throughout its first $4$ days. (Book: Simulation by Sheldon Ross, Chapter 7 Problem 11) I initially simulated this to approximate the answer as around $13.5$ % but I want to find an exact form. I note that the distribution of the total amount $x$ claimed after $d$ days can be represented with a Gamma( $10d,1000$ ) distribution: $$F(x) = \int_0^x \frac{1}{(1000)^{10d}\Gamma(10d)}t^{10d-1}e^{-\frac{t}{1000}}dt$$ For example, since the insurance company receives $11,000$ dollars after $1$ day, $F(11000)\approx 0.6595$ , which means that after $1$ day, the probability of having a negative capital is $1-0.6595 = 0.3405$ . However, this doesn't account for the firm once having a negative capital within the day, even if at the end of the day, the capital is positive. I'm having trouble finding how to deal with this part. My thinking is to find $$\epsilon \cdot P(\text{positive capital on days } [0,\epsilon]) + \epsilon \cdot P(\text{positive capital on } [\epsilon, 2\epsilon] \text{ given positive capital on } [0,\epsilon]) + \epsilon \cdot P(\text{positive capital on } [2\epsilon, 3\epsilon] \text{ given positive capital on } [0,2\epsilon]) + ...$$ but I run into a dead end here since finding $P(\text{positive capital on } [\epsilon, 2\epsilon])$ would require knowing the capital I start with after $\epsilon$ days.","['poisson-distribution', 'gamma-function', 'poisson-process', 'gamma-distribution', 'probability']"
4835954,"Gauss-Newton method, where did sigma inverse come from?","I'm studying the Gauss-Newton Method from ""slambook-en"" chapter 5 on optimization (the books is made free online by the author in case you need to see it). I've attached a picture of the example the author is using to elaborate the method. My doubt is with the sudden appearance of the sigma inverse in the final formulation. I understand how the general Gauss Newton method works out to approximate the hessian matrix, but I'm struggling to understand the sigma inverse. It is of course (or at least I think) related to the 'w' in page 1 that I have attached which is used to simulate the gaussian noise, but that is the extent to which I understand it. Any help or insights would be greatly appreciated!","['nonlinear-optimization', 'jacobian', 'multivariable-calculus', 'least-squares', 'optimization']"
4835962,Calculating $\sum_{n=1}^{\infty} \frac{\zeta(2n)}{n \cdot 4^n}$,"This is my first post on MSE, so I apologize in advance for any mistakes I may have made. I was trying to find the value of the sum $$ S = \sum_{n=1}^{\infty} \frac{\zeta(2n)}{n\cdot 4^n}$$ According to WolframAlpha, this sum evaluates to $$ S = 0.4515827052894548647....$$ Which WolframAlpha suggests it to be equal to $\ln{\frac{\pi}{2}}$ . I have confirmed that it matches exactly up to $20$ decimal places (may be even higher). So to think about this, I defined a function $$ S(a) = \sum_{n=1}^{\infty} \frac{\zeta(2n)}{n\cdot a^n}$$ Then differentiating, we have $$ S'(a) = \sum_{n=1}^{\infty} \frac{\zeta(2n)}{n} \cdot \frac{-n}{a^{n+1}} = \sum_{n=1}^{\infty} -\frac{\zeta(2n)}{a^{n+1}}$$ But I am unable to evaluate $S'(a)$ . Can someone give me some hint to find an expression for $S'(a)$ ? Many thanks for your efforts!",['sequences-and-series']
4835969,"Integral $ \int_0^{\pi/2} x \sqrt{\cot x} \, dx $","I cannot evaluate the following integral $$ \int_0^{\pi/2} x \sqrt{\cot x} \, dx $$ which means $$ \int_0^{\pi/2} \frac{x}{\sqrt{\tan x}} \, dx = 0.97482\ldots. $$ This integral seems to be equal to the value of $$ \frac\pi{2\sqrt2} \left( \frac\pi2 - \ln2 \right) = 0.97482\ldots. $$ If anyone knows how to solve this integral, I would be very interested to know. (Or do you know what would be a generalization of this integral? And do you know how to prove it?) P.S. This intgral is equal to $$ \frac12 \int_0^\infty \frac{\sqrt{\frac1x} \tan^{-1} x + \sqrt{x} \tan^{-1} \frac1x}{1+x^2} \, dx. $$",['integration']
4835973,Uniform Integrability and relative compactness,"I am trying to proof relative compactness in L2(0,1) for a specific set of functions $(\phi_n)_{n \in \mathbb{N}}$ with following properties: $\int_0^1 \phi(x) dx = 0 $ $||\phi_n^2||_{L1(0,1)} = 1 $ $(\phi_n)_{n \in \mathbb{N}}$ is uniformly integrable such that for each $\epsilon >0$ there exist a $\delta> 0$ fulfilling: $$\sup_{n \in \mathbb{N}} \int_0^1 \phi_n(x)^2*\mathbb{1}\left(|\phi_n(x)| > \delta\right) dx < \epsilon $$ Here, $\mathbb{1}$ is the indicator function resulting in 1 if the inner statement is true. Each $\phi$ is non decreasing and has at most countable many discontinuity points. (Otherwise it is continuous) I tried to use the theorem of Kolmogorow-Riesz, where I have to proof boundedness in the norm (which is already given above) and equicontinuity: $$ \lim_{h \rightarrow 0} \sup_{n \in \mathbb{N}} \int_0^1 \left(\phi_n(x+h) - \phi_n(x)\right)^2 dx$$ But I have difficulties to show the equicontinuity, since I do not find a possibility to use the given uniform integrability. I plotted many examples and I think that this property is necessary in this scenario. Without this property I think there could be a $\phi_n$ that has all its mass at the border of 0 and 1 so that the equicontinuity can not hold anymore. I Imagine that the uniform integrability should lead to distributing the mass uniformly so that the actual difference of $\phi_n(x+h)-\phi_n(x)$ is bounded in terms of h. Does anyone have a source where something like this is done or is there a more elegant way than Kolmogorow-Riesz for showing the relative compactness?","['uniform-integrability', 'real-analysis', 'functional-analysis', 'equicontinuity', 'compactness']"
4836010,ruler and compass construction of the multiplication of two constructible points,"I'm trying to show that the constructible elements form a field. My definition of constructible is that a point in the plane $P=(a,b)\in \mathbb{R}^2$ is constructible if there exists a set of points $ \{P_0, P_1, · · · , P_n=P\}$ containing $P_0=(0,0)$ and $P_1=(1,0)$ such that $P_i$ is either an intersection of two lines or an intersection of one line and one circles or an intersection of two circle obtained from the set $ \{P_0, P_1, · · · , P_{i-1}\}$ . I was able to show that if $P,P'$ are constructible, then so are $P+P'$ and $-P$ but I find it harder to show that $P^{-1}$ and $P\cdot P'$ are also constructible. For $P^{-1}$ I tried to write it like $P^{-1}=\frac{\bar{P}}{||P||}$ where $\bar{P}=(a,-b)$ ( the complex conjugute) but I couldn't go any further. Any hints on how to show that $P^{-1}$ and $P \cdot P'$ are also constructible?","['field-theory', 'galois-theory', 'algebraic-geometry', 'geometry']"
4836017,"Is $\mathbb{E}[X|\mathscr{B}]$ well-defined when $\mathbb{E}[X|\mathscr{A}]$ is, where $\mathscr{A}\subset\mathscr{B}$ and $X^\pm$ isn't integrable?","Let $X$ be a random variable on a probability space $(\Omega,\mathscr{F},\mathbb{P})$ ,
and let $\mathscr{A}$ and $\mathscr{B}$ be sub- $\sigma$ -fields of $\mathscr{F}$ such that $\mathscr{A}\subset\mathscr{B}$ . Question: Is $\mathbb{E}[X|\mathscr{B}]$ well-defined when $\mathbb{E}[X|\mathscr{A}]$ is,
but when both the negative part $X^-$ and the positive part $X^+$ of $X$ are not integrable? For what I understand by well-defined,
let's consider the definition of conditional expectation
as given by e.g. Shiryaev ( Probability , 2013; p. 213): The conditional expectation of $X^\pm$ with respect to $\mathscr{A}$ is an $\mathscr{A}$ -measurable random variable $\mathbb{E}[X^\pm|\mathscr{A}]$ satisfying for every $A\in\mathscr{A}$ that \begin{equation*}
\int_A\mathbb{E}[X^\pm|\mathscr{A}]d\mathbb{P} = \int_A X^\pm d\mathbb{P}.
\end{equation*} If $\mathbb{E}[X^-|\mathscr{A}]\wedge\mathbb{E}[X^+|\mathscr{A}] < \infty$ almost surely
then the conditional expectation $\mathbb{E}[X|\mathscr{A}]$ of $X$ with respect to $\mathscr{A}$ is said to be well-defined and is given by \begin{equation*}
\mathbb{E}[X|\mathscr{A}] = \mathbb{E}[X^+|\mathscr{A}] - \mathbb{E}[X^-|\mathscr{A}].
\end{equation*} In other words,
does $\mathbb{E}[X^-|\mathscr{A}]\wedge\mathbb{E}[X^+|\mathscr{A}] < \infty$ almost surely
imply $\mathbb{E}[X^-|\mathscr{B}]\wedge\mathbb{E}[X^+|\mathscr{B}] < \infty$ almost surely? My attempt so far: Let $B$ be the event in $\mathscr{B}$ on which $\mathbb{E}[X^-|\mathscr{B}] = \mathbb{E}[X^+|\mathscr{B}] = \infty$ .
For every $A\in\mathscr{A}$ satisfying $B\subseteq A$ it holds that \begin{equation*}
\infty\mathbb{P}(B)
=\int_B\mathbb{E}[X^\pm|\mathscr{B}]d\mathbb{P}
=\int_B X^\pm d\mathbb{P}
\leq
\int_A X^\pm d\mathbb{P}
=\int_A\mathbb{E}[X^\pm|\mathscr{A}]d\mathbb{P}.
\end{equation*} Suppose that $\mathscr{A}$ is generated by a finite or countable partition.
Then there is a smallest $A\in\mathscr{A}$ satisfying $B\subseteq A$ .
Either $\mathbb{P}(A) = 0$ ,
or on $A$ we have that $\mathbb{E}[X^\pm|\mathscr{A}]$ equals $\frac{1}{\mathbb{P}(A)}\int_A X^\pm d\mathbb{P}$ .
Since by assumption $\mathbb{E}[X|\mathscr{A}]$ is well-defined,
either $\mathbb{P}(A) = 0$ or
at least one of $\int_A X^-d\mathbb{P}$ and $\int_A X^+d\mathbb{P}$ is finite.
Both possibilities imply that $B$ must have probability zero,
which means that $\mathbb{E}[X|\mathscr{B}]$ is well-defined. Is there always a set $A\in\mathscr{A}$ satisfying $B\subseteq A$ and small enough such that
at least one of $\int_A X^-d\mathbb{P}$ and $\int_A X^+d\mathbb{P}$ is finite?
This would solve the problem. Or is there some other approach? (In the first version of this question
I only demanded $X$ to be non-integrable,
but this would still allow for one of $\int_\Omega X^-d\mathbb{P}$ and $\int_\Omega X^+d\mathbb{P}$ to be finite,
which by the above argument would then imply the well-definedness
of $\mathbb{E}[X|\mathscr{B}]$ .
So I restricted to both $X^-$ and $X^+$ being non-integrable.)","['conditional-expectation', 'probability-theory', 'probability']"
4836027,Hadamard products and sums of reciprocals of solutions to $x=\tan x$,"There are infinitely many real solutions to the equation $\tan x=x$ . Denote the increasing sequence of positive solutions by $ (\lambda_n)_{n=1}^{\infty}$ . I want to evaluate the sum $$ \sum_{n=1}^{\infty} \frac{1}{\lambda_n^2} \hspace{1cm} (1) $$ (the series can be seen to converge by using a crude approximation such as $\lambda_n \approx \frac{\pi(2n+1)}{2}$ ). To find the sum I'm thinking to use Hadamard's product theorem applied to a function whose roots are $\lambda_n$ and then to expand the product and compare coefficients as in Euler's evaluation of $\zeta(2)$ . The Hadamard product theorem states that any entire function $f(z)$ of growth order $\alpha<2$ can be written as $$ f(z) = z^m e^{Az+B} \prod_{\rho} e^{z/\rho} \left(1-\frac{z}{\rho} \right) $$ where the product is over all the non-zero complex roots $\rho$ of $f$ repeated according to multiplicity, $m \geq 0$ is the order of the root of $f$ at $z=0$ , and $A,B$ are two constants to be determined. Now $ \tan z-z $ is not entire but it has the same non-zero roots as the function $ f(z) = \sin z - z \cos z$ which is entire. These are all real roots of order 1 and they are symmetric since the function is odd. Thus the non-zero roots of $f(z)$ are at $\pm \lambda_n$ and there is also a triple root at $z=0$ . Further, the function has growth order 1, so applying Hadamard's product formula gives $$ \sin z-z \cos z = e^{Az+B} z^3 \prod_{n=1}^{\infty} e^{z/\lambda_n} \left(1-\frac{z}{\lambda_n} \right) e^{z/(-\lambda_n)} \left(1-\frac{z}{(-\lambda_n)} \right) = e^{Az+B} z^3 \prod_{n=1}^{\infty} \left(1-\frac{z^2}{\lambda_n^2} \right) $$ and we may compute $$e^B = \lim_{z \to 0} \frac{\sin z-z\cos z}{z^3} = \frac{1}{3} $$ and $A=0$ since $\frac{\sin z-z\cos z}{z^3}$ is an even function. Therefore $$ \sin z-z \cos z = \frac{z^3}{3} \prod_{n=1}^{\infty} \left(1-\frac{z^2}{\lambda_n^2} \right) \hspace{1cm} (2) $$ We may now expand the left side in a Taylor series and compare coefficients with the right side to obtain $$ \sum_{n=1}^{\infty} \frac{1}{\lambda_n^2} = \frac{1}{10} $$ as well as higher order sums such as $$ \sum_{n=1}^{\infty} \frac{1}{\lambda_n^4} = \frac{1}{350} $$ and so on. All sums of reciprocals of even powers will be rational numbers. My questions are: Is this solution correct? Are there other ways of evaluating the series $(1)$ without using the Hadamard product? Are there any general techniques (contour integration for example) available for evaluating sums of the form $$ \sum_{\rho} g(\rho) $$ where the sum is over all roots $\rho$ of an entire function $f$ and $g$ is some other analytic function for which the series converges? Thanks!","['complex-analysis', 'hadamard-product']"
4836056,Solving $\frac{df}{dx} = 2x$ using the definition of a derivative. [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 months ago . Improve this question Although it's pretty straightforward to solve the above equation but I was wondering, how about if we use the limit definition of a derivative and then try to solve it. So, the equation looks like $$\lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = 2x$$ Now how should I go about solving it? IMO guessing might be one of the ways but I am not aware of any of the ""formal"" ones. Any kind of help will be appreciated.","['limits-without-lhopital', 'limits', 'derivatives']"
4836093,"Describing Mean Value Theorem as ""Fundamental Theorem Of Differential Calculus""","Referring to ""Introduction to Real Analysis"" 4th Ed (2011) by Bartle and Sherbert. (ch 6, sec 2, page 174) In fact the Mean Value Theorem is a wolf in sheep's clothing and is the Fundamental Theorem of Differential Calculus. What could have lead them to state this? All the other calculus books (Stewart, Apostol, Thomas...) have given special prominence to this theorem. I am definitely not confusing this with Fundamental Theorem of Calculus. What I mean is : What could have lead to this heightened significance of this theorem? Although this could be a subjective opinion, I would love to receive some expansive insights. I further request relative comparison of all the significant theorems of differential calculus namely, Extreme value , Mean value and Intermediate value theorem. PS: I emphasize that this question is completely disassociated with FTOC. If someone feels like the intent of author in the quote above is ambiguous/subjective, they may feel free to disregard it in their answers.","['calculus', 'soft-question', 'intuition']"
4836095,"If the sum of digits of a number is 17 and none of its digits is zero, we called it a ‘good number’. Find the number of 5-digit ‘good number(s)’.","Here's what I have done:
since 5 digits are at least 1, that leaves the value 17-5 = 12 to be distributed in the rest of the digits
so right now, the number is 11,111.
the value 12 is distributed into the 5 digits, and using the stars and bars method, the answer should be 16C4 = 1820 What am I doing wrong
(btw, the answer shows as 1645, but I don't know how)","['algebra-precalculus', 'logic', 'combinatorics']"
4836122,Nonnegativity of the determinant of a commuting matrix,Let $A\in M_n(\mathbb{R})$ such that $A^2=-I_n$ and $AB=BA$ for some $B\in M_n(\mathbb{R})$ . Prove that $\det(B)\geq0$ . All the information I could extract from the relation $A^2=-I_n$ are as follows: $(a)$ $A$ is not diagonalizable. $(b)$ $\det(A)=1$ . $(c)$ $n$ must be even. Now how to conclude that $\det(B)$ is nonnegative using these $3$ informations alongwith $AB=BA$ is not clear to me. Any help is appreciated.,"['matrices', 'linear-algebra', 'contest-math']"
4836129,What's wrong with the following false-proof that a subspace of a normal space is normal?,"The fact that a subspace of a normal topological space is normal is known to be wrong. I'm trying to find the wrong step in the following proof: Let $X$ be a normal topological space and $Y$ be a subspace. Let $C_1, C_2$ be closed sets in $Y$ . So there exist $C_1', C_2'$ closed in $X$ such that $C_i = Y \cap C_i' $ for $i=1,2$ . Since $X$ is normal, there are open disjoint sets $U_1, U_2$ such that $C_i' \subset U_i$ . Thus $C_i=C_i' \cap Y \subset U_i \cap Y$ . So if we let $V_i = U_i \cap Y$ , we get that $V_1, V_2$ are open in $Y$ , disjoint, and satisfy $C_i \cap V_i$ . Therefore $Y$ is a normal topological space. Where is the mistake?",['general-topology']
4836137,Twists of morphisms and torsors,"Let $S$ be a scheme and $\tau$ some topology on $\mathrm{Sch}/S$ . (I'm primary interested in \etale topology). Let $\pi:Y\to X$ be a morphism of $S$ -schemes. A twist of $\pi$ is a morphism $\pi':Y'\to X$ , which becomes isomorphic to $\pi$ after some cover $\mathcal{U}$ in $\tau$ . I want to understand how twists relate to torsors/non-abelian Cech-cohomology. In the literature I have only found twists of varieties, ie. the case where $S=X=\mathrm{Spec}(k)$ and $Y$ is a $k$ -variety. Similarily we obtain a map (of isomorphism classes) $$\phi: \{\text{twists of $\pi$} \} \longrightarrow \{\mathrm{Aut}(\pi)\text{-torsors} \},  $$ by associating the $\mathrm{Aut}(\pi)$ -torsor $\mathrm{Iso}_X(Y,Y')$ to a twist $\pi':Y'\to X$ . Is there any general criterion, where $\phi$ is bijective? This seems like some kind of representability issue. Of course we can break this up into maps $$\phi_\mathcal{U}: \{\text{twists of $\pi$, trivial over $\mathcal{U}$} \} \longrightarrow \{\mathrm{Aut}(\pi)\text{-torsors, trivial over $\mathcal{U}$}  \},  $$ for all covers $\mathcal{U}$ . In Serre's Galois cohomology, in the variety case, he assumes that $Y$ is quasi-projective.","['algebraic-geometry', 'homology-cohomology', 'sheaf-cohomology', 'schemes']"
4836141,Is this group free abelian?,"Let $K$ be the subgroup of $\mathbb{Z}^\mathbb{Z}$ consisting of those functions $f : \mathbb{Z} \to \mathbb{Z}$ with finite image. Is $K$ free abelian? My guess is no , because $K$ feels too much like the Baer-Specker group $\mathbb{Z}^\mathbb{Z}$ , which is not free abelian. However, it's not obvious to me how to adapt the arguments in the proof of Baer-Specker to this case. If it helps, $K$ also has the following presentation: it is the abelian group generated by the subsets of $\mathbb{Z}$ , modulo the relations $\varnothing = 0$ and $A + B = (A \cup B) + (A \cap B)$ for all $A, B \subseteq \mathbb{Z}$ . The class of $A \subseteq \mathbb{Z}$ in this presented group corresponds to the indicator function of $A$ in the above definition of $K$ . Another note: $\operatorname{Hom}(K,\mathbb{Z})$ is the group of finitely-additive $\mathbb{Z}$ -valued measures on $\mathbb{Z}$ with the discrete $\sigma$ -algebra. Maybe this dual group is free abelian of countable rank? In which case it would follow that $K$ is not free. All thoughts appreciated!","['free-abelian-group', 'group-theory', 'abelian-groups']"
4836171,Split monomorphism and split epimorphisms of Banach spaces,"Consider the category Ban of Banach spaces with linear contractions. I want to find a characterization of the split monomorphisms and of the split epimorphisms in this category, however I couldn't make much progress. I know that every split monomorphism is a monomorphism in any category and in Ban these are precisely the injective morphisms, but it seems that not any injective linear contraction is also a split monomorphism. In fact, I am kind of confused. Let $f:X\to Y$ be an injective linear contraction. Then, we can show that it has a linear left inverse, call it $g:Y\to X$ . Now we have $1_X=g\circ f$ , which implies that $1=||g\circ f||\le ||g||\cdot ||f||\le ||g||$ . I guess that this means that $g$ must have norm $1$ in order for it to be a  contraction. But I don't know what other condition I should impose on $f$ so that such a $g$ exists. Similarly, every split epimorphism is an epimorphism, so this means that every split epimorphism in Ban has a dense image, but again I don't think the converse holds and I don't know how to find what other properties I should add.","['banach-spaces', 'functional-analysis', 'category-theory']"
4836179,Best Strategy for Predicting $p$ of a coin based after $n$ flips,"Consider a coin with probability of landing on heads, $p$ , where $p$ is sampled from a truncated normal distribution from [0,1]. You are offered to play a game where you can bet on a 12% interval containing $p$ of a coin after observing the outcome of $n$ flips, i.e. if you estimate an interval of [0.6, 0.72] and $p$ is contained in this interval you win the game. What is your optimal strategy for playing this game? What value of $n$ lets you confidently play this game (assuming a 1:1 payout structure, at what point does your strategy net you a win percentage greater than 50%)? The answer will of course depend on the standard deviation of the distribution, which for my simulation purposes I have just used a placeholder of $\sigma=0.2$ . I am wondering if anyone has thoughts on how to obtain a closed form solution or way of estimating what relations between $n$ and $\sigma$ make it worth playing this game. Initial Thoughts A trivial strategy is to have the 12% interval centered about 0.5 as this would be the interval containing the most of the distribution and would have probability of success $P(X\le0.56)-P(X\le0.44)$ . Another strategy I considered is to construct an interval centered about the observed probability of the coin, but with a bias headed toward 0.5. For example, after 4 flips I observe 3 heads, and thus construct an interval of [0.69, 0.81], and then consider adding an offset to my bounds to shift them closer to the center. I have simulated a scenario with $N(0.5, 0.2)$ to varying the number of observed flips, $n$ , and what is the best offset strategy at each $n$ . As predicted, the benefit of offsetting the interval becomes less significant as the number of flips you get to observe increases. From my simulations, it seems that $n=20$ leads is the cutoff for when it becomes worth playing this game. Is there anyway to approximate this result without doing simulation?","['game-theory', 'statistics', 'probability-distributions', 'probability']"
4836196,Integrating $\int_0^{\infty} \frac{\sin^n (ax)}{x^n \cdot (x^2+1)} dx$,"I was trying to evaluate an expression for the family of integrals $$ I(a,n) = \int_0^{\infty} \frac{\sin^n(ax)}{x^n(x^2+1)} dx$$ For positive $a$ and $n$ The expression for the first few $n$ can be computed by hand. For example, for the case when $n=1$ we have: $$ I(a,1) = \int_0^{\infty} \frac{\sin(ax)}{x(x^2+1)} dx$$ Differentiating, we get: $$ I'(a,1) = \int_0^{\infty} \frac{\cos(ax)}{x^2+1} dx = \frac{\pi}{2}\cdot e^{-a}$$ Since $I(0,1) = 0$ , we can write $I(a,1)$ as: $$ I(a,1) = \frac{\pi}{2} \cdot (1-e^{-a})$$ For $n=2$ , we again differentiate with respect to $a$ . We get: $$I'(a,2) = \int_0^{\infty} \frac{\sin(2ax)}{x(x^2+1)} dx = I(2a,1) = \frac{\pi}{2} \cdot (1-e^{-2a})$$ Again observing that $I(0,2) = 0$ , we can write $I(a,2)$ as: $$I(a,2) = \frac{\pi}{2} \cdot (a + \frac{1}{2}\cdot e^{-2a} - \frac{1}{2})$$ With some rather tedious effort in computing the third derivative, we can use the same strategy to evaluate $I(a,3)$ . I obtained: $$I(a,3) = \frac{\pi}{8} \cdot (3a^2+ 3e^{-a} -e^{-3a}-2)$$ I hypothesize after looking at the pattern that the expression for $I(a,n)$ must be some fraction of $\pi$ , multiplied by the sum of a polynomial of degree $n-1$ added to some exponentials of the form $e^{-ka}$ where $k$ ranges from $1$ to $n$ . But I was wondering if we can afford to keep doing this for higher and higher values of $n$ . I don't have a surefire way of obtaining an expression for general $n$ , except for repeated differentiation, simplification and reverse integration. Can anyone help me in evaluating an expression that holds for all natural $n$ ? Any help is appreciated. Thank you for reading!","['integration', 'calculus', 'definite-integrals']"
4836202,"Sum of Uniform([-1, 1]) random variables is dense in $\mathbb{R}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question Let $X_i, i \in \mathbb{N}$ be a sequence of i.i.d $\text{Uniform}([-1, 1])$ random variables. Let $S_n = X_1 + \cdots + X_n$ . Show that almost surely, the set $\{S_n: n \in \mathbb{N}\}$ is dense in $\mathbb{R}$ . I wonder if there is a way to approach this problem with the Kolmogorov's 0-1 law or the Hewitt-Savage 0-1 law? Besides, I think the Borel-Cantelli lemma could also be useful to solve this problem.","['probability-theory', 'probability']"
4836214,When does $\prod_i (1-\alpha X_i^2)^2\overset{P}{\rightarrow} 0$ for some $\alpha$?,"Define sequence of random variables $E_s$ below where $X_i$ are iid with the standard Cauchy distribution $$E_s = \prod_i^s (1-\alpha X_i^2)^2$$ Does $E_s$ converge to 0 in probability for any value of $\alpha$ ? (if $X_i$ were Gaussian, this would hold when $0<\alpha<2.421249$ ) What are the general conditions on $X_i$ for such $\alpha$ to exist? Bounded fourth moment gives sufficient condition, is it also necessary?","['statistics', 'probability-theory', 'probability']"
4836242,"Examples of ""unexpected"" characteristic subgroups","If $A$ is an abelian group, there are a number of characteristic subgroups of it that immediately come to mind: Any of the groups $nA$ , $A[n]$ (the $n$ -torsion), the full torsion subgroup $T(A)$ , the maximal divisible subgroup $D(A)$ etc. Any of these examples, however, can be obtained by (possibly transfinite) iteration of forming verbal $(nA)$ and marginal ( $A[n]$ ) subgroups and taking sums and intersections of those. Do you know any (specific or general) examples of characteristic subgroups that are not of this form? To be precise: Let $\mathcal{E}(A)$ denote the smallest (nontrivial) family of subgroups of $A$ that is closed under sums, intersections and formation of verbal and marginal subgroups $-$ these characteristic subgroups I call expected . What are some unexpected characteristic subgroups? Here is an example of what I am talking about: Consider the group $A=\mathbb{Z}/2^n\mathbb{Z}\times \mathbb{Z}/2\mathbb{Z}$ for $n\geq 3$ . It is straightforward to see that $A$ contains exactly three distinct subgroups of order $2^r$ for $1\leq r\leq n$ . Now, if both of these inequalities are strict we have the expectedly characteristic $2^{n-r}A$ and $A[2^{r-1}]$ among them. Hence, the third group (which is cyclic generated by $(2^{n-r},1)$ ) must be characteristic as well and is easily seen to lie outside of $\mathcal{E}(A)$ . If you happen to know that some class of groups cannot have such subgroups, you are more than welcome to share these as well. (For instance, underlying additive groups of vector spaces form such a class.) Lastly, even though I posed the question only for abelian groups, you may feel free to interpret it more liberally.","['characteristic-subgroups', 'examples-counterexamples', 'abstract-algebra', 'group-theory', 'soft-question']"
4836347,Product of a positive semi-definite matrix and a diagonal matrix [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question Let $A$ be a positive semi-definite matrix. $B$ is a diagonal matrix with positive diagonal entries. Let $x$ and $y$ be vectors. Is it true that $$
x'y > (<) 0 \Longrightarrow x'ABy \geq (\leq) 0
$$","['matrices', 'linear-algebra', 'positive-semidefinite']"
4836356,Can we find the exact value of a double sum with cosine without differentiation?,"After finding an interesting double sum $$\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{(-1)^{m+n}}{(m+n)^2}  = \frac{\pi^2}{12}-\ln 2 ,$$ I started to investigate a harder one $$\displaystyle \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2} .$$ Noting that $\displaystyle \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2}=\Re f(1)\tag*{}  $ where $\displaystyle f(x)=\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} e^{(m+n) x i}}{(m+n)^2}\tag*{}  $ Differentiating $f(x)$ once and twice yields $\displaystyle f^{\prime}(x)=\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} i e^{(m+n) x i}}{m+n}\tag*{} $ and $$ \displaystyle \begin{aligned}f^{\prime \prime}(x) =\sum_{m=1}^{\infty}(-1)^m\left[\sum_{n=1}^{\infty}(-1)^{n-1} e^{(m+n) x i}\right] =\sum_{m=1}^{\infty} \frac{(-1)^m e^{(m+1) x i}}{1+e^{x i}}=-\frac{e^{2 xi }}{\left(1+e^{xi}\right)^2}\end{aligned}\tag*{} $$ Integrating back gives $$f^{\prime}(x)-f^{\prime}(0)=\int_0^x f^{\prime \prime}(t) d t  =-\int_0^x \frac{e^{2 i t}}{\left(1+e^{i t}\right)^2} d t =i \ln \left(1+e^{i x}\right)+\frac{1}{2} \tan \frac{x}{2}-i \ln 2$$ Rearranging gives us $ \displaystyle f^{\prime}(x)=i f(0)+i \ln \left(1+e^{i x}\right)+\frac{1}{2} \tan \frac{x}{2}-i \ln 2\tag*{} $ Integrating once more gives $$\begin{aligned} f(1)-f(0)&= \int_0^1\left[i f(0)+i \ln \left(1+e^{i t}\right)+\frac{1}{2} \tan \frac{t}{2}-i \ln 2\right] d t \end{aligned}$$ $$= i f(0)+i \int_0^1 \ln \left(1+e^{i t}\right) d t+\frac{1}{2} \int_0^1 \tan \frac{t}{2} d t-i \ln 2 $$ $$=  i f(0)+i\left[-i \operatorname{Li_2}\left(-e^{-i x}\right)\right]_0^1+\ln \left(\sec \frac{1}{2}\right)-i \ln 2$$ Rearranging gives $\displaystyle \begin{aligned}f(1)&=f(0)+i f(0)-\operatorname{Li_2}\left(-e^{-i}\right)-\frac{\pi^2}{12}+\ln \left(\sec \frac 12\right) -i \ln 2\end{aligned}\tag*{} $ $\displaystyle \boxed{\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n} \cos (m+n)}{(m+n)^2}=\Re f(1)=-\ln 2-\Re\left(\operatorname{Li}_2\left(-e^{-i}\right)\right)+\ln \left(\sec \frac{1}{2}\right) \approx{0.0099041}} \tag*{} $ My question: Can we find the exact value of  the double sum with cosine without differentiation? Your comments and alternative solution are highly appreciated. Footnote: $$f(0) =\sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \frac{(-1)^{m+n}}{(m+n)^2}  =\sum_{m=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{m+n-1} \int_0^1 x^{m+n-1} \ln x d x \\ =\int_0^1\left[\sum_{m=1}^{\infty} \sum_{n=1}^{\infty}(-1)^{m+n-1} x^{m+n-1}\right] \ln x d x \\ =\int_0^1 \ln x \sum_{m=1}^{\infty} \frac{(-1)^m x^m}{1+x} d x=\int_0^1 \ln x \frac{-x}{(1+x)^2} d x \\  =-\int_0^1 \frac{x^2 \ln x}{(1+x)^2} d x=\frac{\pi^2}{12}-\ln 2   $$","['integration', 'summation', 'trigonometric-series', 'pi', 'derivatives']"
4836377,"Prove or Disprove: $\sum_{n\geq 0} (3p^2_n - p_n - 2p^3_n) \leq 0$ for any probability mass ${\bf p} = (p_0,p_1,\ldots)$","Assume that $N \in \mathbb{N}_+$ and ${\bf p} = (p_0,p_1,\ldots,p_{2N+1})$ is a probability mass function, i.e., $p_n \geq 0$ for all $0\leq n \leq 2N+1$ and $\sum_{n=0}^{2N+1} p_n = 1$ . For some reason, I am speculating that the following inequality holds $$ \sum_{n=0}^{2N+1} (3\,p^2_n - p_n - 2\,p^3_n) \leq 0 \tag{1}\label{1}$$ regardless of the choice of ${\bf p}$ . I attempted many specific examples and could not find a counter-example, may I know if someone can prove the conjecture \eqref{1} or disprove my conjecture by displaying a specific counter-example?","['inequality', 'probability-distributions', 'probability-theory', 'probability']"
4836388,Recover a matrix from its Schur complements,"Suppose I have a matrix: $$ M = \begin{bmatrix}A & B \\ C & D\end{bmatrix} $$ With Schur complements: $$
M/A = D - CA^{-1}B \\
M/D = A - BD^{-1}C \\
$$ Given only the Schur complements $M/A$ and $M/D$ , and the off-diagonal blocks $B$ and $C$ , can I recover the original matrix $M$ ?  (That is, without knowing $A$ or $D$ ?)  It seems like this should be a lemma in a textbook somewhere, but I haven't come across it. Note: in my actual application the matrix is symmetric positive definite such that $C=B^\top$ . Edit Some resources I've looked at include Henderson & Searles on deriving the inverse of a sum of matrices and Chris Yeh's blog, Schur Complements and the Matrix Inversion Lemma . The closest I've come is the following expressions obtained by expressing $M^{-1}$ in terms of $M/A$ and $M/D$ and then, because the matrix inverse is unique, equating the two different expressions for $M^{-1}$ : $$
(M/A)^{-1} = D^{-1} + D^{-1}C(M/D)^{-1}BD^{-1} \\
(M/D)^{-1} = A^{-1} + A^{-1}B(M/A)^{-1}CA^{-1}
$$ These at least express each of $A$ without knowing $D$ and $D$ without knowing $A$ , but I still cannot figure out how to actually solve for $A$ or $D$ from the expressions.","['matrices', 'schur-complement', 'block-matrices']"
4836396,Does there exist a positive integer value of $n$ such that the sum of $1!^n+2!^n+3!^n+\cdots+2024!^n$ is a perfect power?,"Does there exist a positive integer value of $n$ such that the sum of $1!^n+2!^n+3!^n+\cdots+2024!^n$ is a perfect power? I know that $n\neq1$ , as the last digit of the sum when $n\geq4$ is $3$ and no perfect square ends with $3$ , and it cannot be a perfect power since $3^3$ does not divide the sum when $n\geq8$ . I don’t know about when $n=2$ , since the only sure point when $1!^2+2!^2+3!^2+\cdots+n!^2$ cannot be a perfect power is when $n\geq1248828$ ,but I don’t have any idea if $1!^2+2!^2+3!^2+\cdots+2024!^2$ is a perfect power. If $n=3$ , then the sum isn’t a perfect square when $n\geq10$ as it is equal to $6\pmod{11}$ , and it cannot be a perfect power since $3^3\nmid1!^3+2!^3+3!^3+\cdots+n!^3$ when $n\geq5$ . If $n=5,7$ , then it is not a perfect power since $3^2$ dosen’t divide the sum. Also, for all $n$ that is even, the sum can’t be a perfect square because the sum is equal to $2\pmod{3}$ . Note also that if $n$ is odd, then $1!^n+2!^n+3!^n+\cdots+2024!^n$ isn’t a perfect powers because there is some power of three that does not divide the sum after some point(in some cases, $9$ even dosen’t divide the sum), and forces the sum to be perfect square in order to be a perfect power. But also, the sum will “land” on a prime, where $1!^n+2!^n+3!^n+\cdots+2024!^n\equiv a\pmod{p}$ , and $a$ isn’t a quadratic residue $\pmod{p}$ . So I suspect that $1!^n+2!^n+3!^n+\cdots+2024!^n$ is never a perfect square unless $n$ is divisible by $3$ . Now, it forces us that $n$ has to be even in order for $1!^n+2!^n+3!^n+\cdots+2024!^n$ to be a higher odd prime perfect power. I used Pari GP and checked the all values of $n\leq10^{4}$ to see if there is a value of $n$ such that $1!^n+2!^n+3!^n+\cdots+2024!^n$ is a perfect power, but so far, none has been found. Edit: This answer already ruled out about the sum being a perfect square. Can $1!^n+2!^n+3!^n+\cdots+2024!^n$ be a perfect power?","['number-theory', 'perfect-powers']"
4836414,What functions describe themselves with degree n,"I'm interested in understanding functions that describe themselves in a certain degree $ n $ . Let me define what I mean by this: A function $ f(x) $ is said to describe itself in degree $ n $ if there exists a function $ g_{n,f} $ with an inverse such that $$ f = g^{-1} \circ f^n \circ g $$ Notably, every function describes itself in degree 1, since $ g(t) = t $ in that case. An example to illustrate this concept is the function $ f(x) = 2x $ , which describes itself in degree 2 with $ g(x) = a|x|x $ and its inverse $ g^{-1}(x) = \frac{\sqrt{|\frac{x}{a}|} \cdot |\frac{x}{a}|}{(\frac{x}{a})} $ for $ x \neq 0 $ and 0 when $ x = 0 $ , where $ a $ is a non-zero real number. Through this exploration, I've found that $ g(x) $ for n degree is related to $ a \cdot x^n $ , but I'm struggling to find a $ g(x) $ for $f(x) = x^2$ for any other degree other than the trivial case of degree equaling $ 1 $ . my main question is this What types of functions describe themselves with degree $ n $ ? My follow up questions that I think are important as well are as follows: How does this change if we restrict our functions to complex-to-complex, real-to-real, positive-to-positive, or integer-to-integer mappings? What are the constraints required for a function to describe itself in a specific degree $ n $ ? Are there functions that describe themselves in some $ n > 1 $ but not in all such $ n $ ? Does the function $ x^2 $ describe itself in any degree other than degree 1? I appreciate any insights or references that could help in understanding these types of functions and their properties.",['functions']
4836423,Proving a matrix identity involving inverses,"Assuming that all matrix inverses involved below exist, show that $$(A-B)^{-1}=A^{-1} + A^{-1}(B^{-1} - A^{-1})^{-1}A^{-1}.$$ In particular $$\left(I+A\right)^{-1}=I-\left(A^{-1}+I\right)^{-1}$$ and $$\left\vert\left(I+A\right)^{-1}+\left(I+A^{-1}\right)^{-1}\right\vert=1.$$ Here's what I tried to do but I can't simplify that underlined part at all: So I thought I should maybe try finding some sort of infinite geometric series in whose summation the $(AB^{-1} - I)$ term appears but that doesn't seem to help either and I'm not certain if that's mathematically justified either. What should I do next? Also, I've just started studying linear algebra so I would really appreciate an answer using only elementary properties of matrices.","['matrices', 'determinant', 'linear-algebra', 'inverse']"
4836446,"A notion of ""differentiation"" based on secant rather than tangent","Given a differentiable real function $f$ , the derivative $f'(x)$ is the slope of the tangent to the graph of $f$ at $(x,f(x))$ . Suppose that, instead of the tangent, we look at the secant to the graph of $f$ , between the point $(x,f(x))$ and the point $(x^+,f(x^+))$ , where $x^+$ is chosen such that $x^+>x$ and the distance between $(x,f(x))$ and $(x^+,f(x^+))$ , along the curve of $f$ , is some fixed constant $d$ : We define the "" $d$ -secant-derivative"" of $f$ at $x$ as the slope of that secant. Note that, as $d\to 0$ , the $d$ -secant-derivative at $x$ approaches $f'(x)$ (if $f$ is differentiable), but here $d$ is constant. Is anything known about this ""secant-derivative"" operator? Is there a simple way to compute or approximate it, like there are simple rules for computing derivatives? NOTE: As I am interested in approximations, it does not matter very much what distance measure is used: it can also be Euclidean distance, $\ell_1$ distance or $\ell_\infty$ distance; whatever makes the problem solvable. The reason I chose the distance along the curve is that (I think) it defines $x^+$ uniquely.
But, I will also be happy for a solution using e.g. the $\ell_\infty$ metric, where $x+ = \inf\{y\geq x | \ell_\infty(y,x) = d\}$ .","['secant', 'tangent-line', 'derivatives', 'real-analysis']"
4836507,Integral asymptotic expansion [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question How can the following identity be proven: $$
\int_0^\infty dy \frac{y}{(1+y)^2}e^{-\alpha y} =  \ln{(1/\alpha)}+\mathcal{O}(1),
$$ for $\alpha\ll1$ ?","['integration', 'asymptotics']"
