question_id,title,body,tags
4428488,On the domain of a solution to a differential equation,"When solving elementary, separable ordinary differential equations (ODEs), and obtaining the original family of solutions: How do I know the domain over which the family of solutions satisfies the differential equation? Does it satisfy it for the entire $\Bbb R$ ? If so, can I always find a particular solution going through any point on the $xy$ plane?",['ordinary-differential-equations']
4428502,"If picking a 12-bit string uniformly at random, what is the probability that the string has more than one 0 and more than one 1?","I am kinda stumped on this question. I am not completely sure on where to start. Any pointers appreciated. I have tried to calculate the strings only containing one 1 and the strings containing one 0 and zero 1s and zero 0s and use the cardinality of this divided by the total number of strings, but I feel like I am missing something.","['discrete-mathematics', 'combinatorics', 'probability']"
4428538,"$f : \mathbb{R}\rightarrow\mathbb{R}$ be a continuous function such that for every fixed $y\in\mathbb{R}$, $f(x + y)-f (x)$ is a polynomial in $x$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $f : \mathbb{R}\rightarrow\mathbb{R}$ be a continuous function such that, for every fixed $y\in\mathbb{R}$ , $f(x + y) - f (x)$ is a polynomial in $ x$ . Prove that $f$ is a polynomial function. please give some hints for the problem. I tried to use the idea of differentiation.",['real-analysis']
4428553,Not sure how to demonstrate the base case in this problem of mathematical induction,"I am trying to prove the formula for the sum of the terms in a finite geometric sequence by using the principle of mathematical induction. Theorem: If $x$ is any real number other than 1, then: $$ \sum_{j=0}^{n-1} x^j = \frac{x^n-1}{x-1}.$$ I know to start by demonstrating the base case, for which $n = 0. $ $$x^0 = \frac{x^0-1}{x-1}$$ However, if I try to simplify this, I do not see how both sides are equal to each other. $$1 = \frac{0}{x-1}.$$ Is there someway to algebraically manipulate the base case that I am not seeing. Any help would be very useful. Thank you. Note: I apologize for including my equations as links to images. I have copied them from Word, as I do not yet understand how to write equations in StackExchange.","['number-theory', 'induction']"
4428583,Does there exist such a Riemann integrable function?,"I am curious, does there exist a Riemann integrable function $f: [a,b] \to \mathbb{R}$ that satisfies the following three criteria? $\hspace{20pt}$ $1$ . $f$ is a positive function, that is, $f(x) \geqslant 0$ for all $x \in [a,b]$ $\hspace{20pt}$ $2$ . There exists an infinite subset $I$ of $[a, b]$ such that $f(x) > 0$ for each $x \in E$ $\hspace{20pt}$ $3$ . $\int_a^b f(x)dx=0$ I was initially thinking about a constant function, but then the integral is greater than $0$ if criteria $2$ is met. If such a function exists, what is an elementary example?","['integration', 'real-analysis', 'calculus', 'functions', 'riemann-sum']"
4428586,Bounding the $L^q$ norm of the Fourier inversion,"Given $N>1$ , and $\psi \in C^\infty(\mathbb{R}^d)$ , $0 \leq \psi \leq 1$ , which is radial and satisfies: $$ \begin{cases} 
      \psi = 1 & \text{ on }\frac{3}{4}\leq |\xi| \leq \frac{5}4{} \\
      \psi = 0 & \text{ on }|\xi| \leq \frac{1}{2} \text{ and on } |\xi| \geq 2 
   \end{cases}
$$ We note that $\psi$ defined in this way is a smooth approximation of the characteristic function of the annulus $\frac{3}{4} \leq |\xi| \leq \frac{5}{4}$ . For the $\psi$ defined above, we define an operator $P_N$ on $L^2(\mathbb{R}^d)$ by: $$(P_Nf)(\xi) = (K_N * f)(\xi)$$ where $$K_N(x) = \mathcal{F}^{-1}\Big[\psi\Big(\frac{x}{N}\Big)\Big] = \int_{\mathbb{R}^d} e^{2\pi i x \cdot \zeta}\psi\Big( \frac{\zeta}{N}\Big) d\zeta $$ I need to show that all $1 \leq p \leq q \leq \infty$ , $$||P_N f||_{L^q(\mathbb{R}^d)} \leq CN^{\frac{d}{p}-\frac{d}{q}}||f||_{L^p(\mathbb{R}^d)}$$ So far, I have: Recall Young's convolution inequality which  states that for $f \in L^p(\mathbb{R}^d)$ and $g \in L^q(\mathbb{R}^d)$ such that $\frac{1}{q} = \frac{1}{p} + \frac{1}{r} - 1$ with $1 \leq p,q,r\leq \infty$ , we have $$||f*g||_q \leq ||f||_p ||g||_r$$ Now, we know that $P_Nf = K_N * f$ . Then, we have that $$||P_N f||_q  = ||K_N *f ||_q  = ||f* K_N||_q \leq  ||f||_p ||K_N||_r$$ Consider $||K_N||_r$ . By definition, $$K_N(\xi) = \int_{\mathbb{R}^d}e^{2\pi i \xi \cdot x}\psi\Big( \frac{x}{N}\Big) dx = N^d \int_{\mathbb{R}^d}e^{2\pi i \xi \cdot (Nu)}\psi (u) du$$ $$\Rightarrow ||K_N||_r = N^d \cdot C(r,\xi)$$ where $$C(r,\xi) = \Big| \Big|  \int_{\frac{1}{2}\leq |u|\leq \frac{3}{4}}e^{2\pi i \xi \cdot (Nu)} \psi( u ) du + \int_{\frac{3}{4} \leq |u|\leq \frac{5}{4}}e^{2\pi i \xi \cdot (Nu)} du + \int_{\frac{5}{4} \leq |u| \leq 2}e^{2\pi i \xi \cdot (Nu)}\psi (u) du\Big|\Big|_r$$ Where can I go from here?","['measure-theory', 'lp-spaces', 'fourier-analysis', 'functional-analysis']"
4428595,The derivative of $f(x)=x|x|$,"We are tasked to compute the derivative of $f(x)=x|x|$ . $$f(x) = \begin{cases} 
      x^2 & x> 0 \\
      0 & x=0\\
      -x^2 & x<0 
   \end{cases}
$$ We use the product rule to calculate the derivative of $f(x)$ . $$f'(x)=x(|x|)'+|x|$$ We know that $$(|x|)'= \begin{cases} 
      1 & x> 0 \\
      \text{undefined}& x=0\\
      -1 & x<0 
   \end{cases}$$ So $$f'(x) =\begin{cases} 
      2x & x> 0 \\
      \text{undefined}& x=0\\
      -2x & x<0 
\end{cases}$$ But if we take the limit as $x \to 0$ , we find the left limit and the right limit equal $0$ . Then $$f'(x) =\begin{cases} 
      2x & x> 0 \\
      0& x=0\\
      -2x & x<0
\end{cases}  $$","['solution-verification', 'derivatives', 'real-analysis']"
4428605,How to solve a hyperbolic (cosh) equality when the argument of the cosh function is different,"if $ \alpha+\lambda = c \cosh( \frac{a+d}{c})$ , and $ \alpha + \lambda = c \cosh( \frac{-a+d}{c})$ how does one get that $α + λ = c \cosh(\frac{a}{c})$ and $d = 0$ is there a rule that im missing? d is a constant edit: this is what i did:
since cosh is an even function and is bijective, $\cosh(u)= \cosh(v) \implies u=c$ so $ \frac{a+d}{c} = \frac{-a+d}{c} \implies -a+d = a+d \implies a = 0$ did i make a mistake ? i cant seem to get $d=0$","['complex-analysis', 'trigonometry', 'hyperbolic-functions', 'real-analysis']"
4428651,Minimum number of 4-way switches to allow any pairing of $N$ inputs,"I want to build a network of switches that with $N$ inputs that allows any pairing of the inputs to be created ( $N$ is an even number). For each number of inputs, I'm looking for a network with the minimal number of switches. Each switch has 4 terminals, each connected to a terminal of another switch or one of the inputs. In the following examples, I'll use this symbol to represent such a switch: The two inputs on the left a and b are either connected to the same outputs a and b on the right or with the outputs flipped, depending on the state of the switch, i.e. in one of these ways: The following is an example of a network that uses two switches to connect 4 inputs to each other. By setting each of the switches to the ""flipped"" or ""non-flipped"" state, it is possible to create any pairing of the inputs: Here are the paths that need to be connected for each possible pairing of the inputs (the states of the switches are not shown but can be deduced from the color-coded paths): Here is a network with 6 inputs and 5 switches plus the paths of 2 possible pairings: The network above allows all 15 pairings of the inputs to be made. This is the network with the lowest number of switches I was able to find for 6 inputs. There are $(N - 1)!!$ pairings of N inputs. A network of $k$ switches has $2^k$ different states. Thus a network with $N$ inputs will need at least $k = \lceil log_2((N - 1)!!) \rceil$ switches to allow for a different state for each pairing. Is the minimal number of switches of such networks known for larger $N$ ?","['graph-theory', 'combinatorics', 'network']"
4428658,"For random variables $X,Y$, show $E[X E(Y | \mathscr{G}) ] = E[E(X | \mathscr{G}) Y ]$","Show that for random variables $X,Y$ on probability space $(\Omega, \mathscr{F}, P)$ , for any $\sigma$ -field $\mathscr{G} \subset \mathscr{F}$ that: \begin{align*}
  E[X E(Y | \mathscr{G}) ] = E[E(X | \mathscr{G}) E(Y | \mathscr{G}) ] = E[E(X | \mathscr{G}) Y ]
\end{align*} FYI, this is from Karatzas + Shreve, on page 43, in the give solution to problem 4.11. Of course, $E[E(X | \mathscr{G})] = E[X]$ . And when $X \in \mathscr{G}$ , $E(X | \mathscr{G}) = X$ . But in the general case, I don't see how to prove the relationship given.","['conditional-probability', 'conditional-expectation', 'probability-theory']"
4428688,"Intuitively, what is the difference between a ""simply connected"" set and a ""locally connected"" set?","I was looking into the Mandelbrot Set and saw a note that said it has been proven that the Mandelbrot Set is ""simply connected"" but it is still an open question of whether or not it is ""locally connected"" (MLC). I can easily understand what simply connected means intuitively--you can draw a line between any two points in the set, it has no holes, etc. But I'm having trouble understanding visually what ""locally connected"" means, and what the difference is between simple connection and local connection. Every page I see on local connectedness only describes it in topological jargon that goes over my head. Is there any way to intuitively or visually describe what the difference is between a locally and simply connected set?","['locally-connected', 'general-topology', 'connectedness']"
4428714,Is every vector in $\mathbb Z^3$ a cross product?,"Is every $3$ -dimensional vector $v$ with integer coordinates a cross product of two other vectors with integer coordinates? I have written a program to check for $v$ with entries between $-7$ and $7$ . Every $v$ that small can be expressed as a cross product of two other vectors with integer coordinates. But I can't come up with a general proof. Apart from the empirical evidence from my experiment on small $v$ , another reason to think this is true is that it's almost enough to find two small independent vectors, $u$ and $w$ , with integer coordinates that are perpendicular to $v$ . Playing around with integer relation algorithms has taught me that such $u$ and $w$ should be plentiful. The cross product of $u$ and $w$ is a scalar multiple of $v$ - call it $kv$ . $k$ is an integer; in most cases $|k| = 1$ . If it isn't, pick a different $u$ and $w$ . A similar but much easier question was this: Is every vector in $\Bbb R^3$ a cross product? . Note: The answer given here was used to solve Diophantine equations so the question is about number theory.","['number-theory', 'vectors', 'geometry']"
4428729,How to find the coefficients of the second eigenvector?,"I have a $2\times 2$ real symmetric matrix: $$\begin{pmatrix}
A & C \\
C & B 
\end{pmatrix}
$$ and I know that the eigenvalues are: $$\lambda_{\pm} = \frac{1}{2}(A+B)\pm \frac{1}{2}\sqrt{(A-B)^{2}+C^{2}}$$ Define $x$ so that: $\lambda_{\pm} = e^{\pm x}$ . I know its first eigenvector, associated to $\lambda_{+}$ . It is given by: $$v = \cos \theta v_{1}+ \sin\theta v_{2}$$ where $v_{1}$ and $v_{2}$ form a basis for this two-dimensional space and the constant $\theta$ is defined by: $$\tan\theta = \frac{C}{e^{x}-A}.$$ Here is my problem. If I want to find an (orthogonal) eigenvector of the matrix associated to $\lambda_{-}$ , I would simply say it is given by: $$w = \cos \Delta v_{1} + \sin\Delta v_{2}$$ where now: $$\tan\Delta = \frac{C}{e^{-x}-A}$$ However, the result is supposed to be: $$w = -\sin\theta v_{1} + \cos\theta v_{2}.$$ Why is that? I am trying to prove that the change $x \to -x$ implies $\cos\theta \to -\sin\theta$ , but I really cannot prove it. Any help is useful!","['matrices', 'matrix-calculus', 'linear-algebra', 'eigenvalues-eigenvectors']"
4428741,Can systems of linear ODE's be solved when their matrix is not diagonalizable?,I'm working through my ODE homework right now and I've run into a repeated issue of ODE systems not being diagonalizable. I am not aware of any other methods to solve systems and my lecture notes do not have any comments on unsolvable systems. Do I need to approach the problem in a different way or is there simply not solutions to some systems? One problem I'm working with is $$x' =\pmatrix{1&1&1 \\ 2&1&-1 \\ -3&2&4}x$$ I can't find any generalized eigenvectors to work with and I'm stuck on how to solve it. I ask that you don't solve the problem and just nudge me in the right direction.,"['matrices', 'matrix-exponential', 'ordinary-differential-equations']"
4428744,"How to tackle $\int_{0}^{\frac{\pi}{2}}y \ln (1+\cos y)\,d y$?","I recently encounter an integral problem consisting the integral $$
I:=\int_{0}^{\frac{\pi}{2}} y \ln (1+\cos y) d y,
$$ I tried to tackle $I$ using the double angle formula and the result of $$
\int_{0}^{\frac{\pi}{4}} y\ln (\cos y) d y
$$ \begin{aligned}
I &=\int_{0}^{\frac{\pi}{2}} y \ln \left(2 \cos ^{2} \frac{y}{2}\right) d y \\
&=\ln 2 \int_{0}^{\frac{\pi}{2}} y d y+2 \int_{0}^{\frac{\pi}{2}} y \ln \left(\cos \frac{y}{2}\right) d y \\
&=\frac{\pi^{2}}{8} \ln 2+8 \int_{0}^{\frac{\pi}{4}} y \ln (\cos y) d y
\end{aligned} By my post , $$\int_{0}^{\frac{\pi}{4}} y\ln (\cos y) d y = \frac{\pi G}{8}-\frac{\pi^{2}}{32} \ln 2-\frac{21}{128} \zeta(3) $$ Now we can conclude that $$
\boxed{\int_{0}^{\frac{\pi}{2}} y \ln (1+\cos y) d y = \pi G-\frac{21}{16} \zeta(3)-\frac{\pi^{2}}{8} \ln 2}
$$ Suggestions for improvement and alternative methods are warmly welcome!","['integration', 'improper-integrals', 'calculus', 'trigonometry', 'catalans-constant']"
4428751,Showing that the integral of an implicit function is $\pi^2/6 -1 = \zeta(2) -1$,"For $x \ge 1$ , define $f(x) \in [0,1]$ implicitly by the equation $1 - f(x) = e^{-x f(x)}$ . Numerically, it seems that $\int_1^\infty (1-f(x)) dx = \pi^2/6 - 1$ . How can I show this analytically? This calculation arose in a problem I am studying on Erdős–Rényi random graphs -- thanks for the help!","['integration', 'calculus']"
4428781,Vector field tangent to a meridian,"I am reading the book Geometry theory of dynamical systems of Palis and de Melo, and I came across the following vector field $X(x,y,z)=(-xz,-yz,x^2+y^2)$ . The book mentions that this field is tangent to the meridians of $\mathbb S^2$ . I have graphed it and this is true, but how can I prove it? Any idea how to start? When it's a field in the plane it's easy, but now that the field is defined on a surface I can't see how to do it. All help is welcome. The real goal is to find the $\alpha-$ limit and $\omega-$ limit set of a point $p$ in $\mathbb S^2$ . What I don't understand well is because the stereographic projection guarantees me tangentiality, for example in the following image I can think of a vector field not tangent to a meridian, from which I extract a vector, and project it onto the plane, this projection would give us vectors with the same properties as the $\pi(X)$ projection.","['manifolds', 'vector-fields', 'ordinary-differential-equations', 'dynamical-systems']"
4428785,To show that there are two people in the tournament with the same starting hand and position in a Texas Hold'em poker tournament.,"A Texas Hold'em poker tournament has $1557$ players, in $9$ distinct positions at $173$ tables. Each table has a standard $52$ card deck, and each player is dealt $2$ cards. Hands are considered ""the same” if they have the same rank of cards and the same number of different suits. For example, the nine of hearts and three of hearts are the same as the nine of clubs and three of clubs, but neither are the same as the nine of spades and three of diamonds. $(a)$ Show that there are two people in the tournament with the same starting hand and position. $(b)$ Show that, for every position, there are two people with the same starting hand. $(c)$ Show that there are two people who are dealt identical cards (not just ""the same”, but with ranks and suits matching). What I have done so far? $(b)$ From Texas hold 'em starting hands I learnt that there are $169$ non-equivalent starting hands in hold 'em. Since there are $173$ tables by the pigeonhole principle, for every position, there are two people with the same starting hand. $(c)$ Total there are $\binom{52}{2} = 1326$ distinct possible combinations of two hole cards from a standard 52-card deck in hold 'em. Since there are $1557$ players, again using the pigeonhole principle, we can conclude that there are two people who are dealt identical cards. Can someone help me in doing the part $(a)$ ? Also, if the arguments for parts $(b)$ and $(c)$ are correct? Thanks.","['pigeonhole-principle', 'poker', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4428804,Expected number of isolated pairs,"An urn contains $a$ white balls and $b$ black balls. We draw balls from the urn uniformly at random without replacement, and line the balls up as we draw them out. We stop once we draw three white balls in a row. When lined up, what is the expected number of isolated pairs of white balls? Assume $a\geq 2b+3$ , so the game is guaranteed to end with drawing three consecutive white balls. I found this problem here , which gives a straightforward inductive approach. However, the answer of $\frac{b}{a+1}$ is surprisingly clean. I was wondering if there's a clean solution with a nice probablistic interpretation, rather than a purely algebraic one based on induction. Any ideas are appreciated!","['expected-value', 'probability', 'random-variables']"
4428846,Prove that $AP \cdot AQ+CP \cdot CQ=BP\cdot BQ$,"Let $G$ be the centroid of $\triangle ABC$ . A line $M$ through $G$ intersects the circumcircle of $\triangle ABC$ at $P$ and $Q$ , where $A$ and $C$ lie on same side of $M$ . Prove that $AP \cdot AQ +CP \cdot CQ=BP\cdot BQ$ .","['centroid', 'triangles', 'circles', 'geometry']"
4428857,Level curves representable by real-valued functions on $\mathbb{R}$,"EDIT: I incorrectly interpreted the original question. The bounty has been awarded based on the (incorrect) interpretation I wrote here. The final version is posted here . Consider the function $u : \mathbb{R}^2 \to \mathbb{R}$ that the following properties are satisfied: Every level curve is a function from $\mathbb{R}$ to $\mathbb{R}$ . (That is, a level curve $U(x,y)= c$ can be written as $y = f(x)$ for some $f : \mathbb{R} \to \mathbb{R}$ .) $\forall$ $p \in \mathbb{R}^2$ , the sets $L(p)$ and $U(p)$ are closed. (Definition below.) Definition: For $p \in \mathbb{R}^2$ , define the lower contour of $p$ as $L(p) := \{(x,y) \in \mathbb{R}^2 : u(x,y) \leq u(p)\}$ and the upper contour of $p$ as $U(p) := \{(x,y) \in \mathbb{R}^2 : u(x,y) \geq u(p)\}$ . Question: Let $X = \mathbb{R}^2$ . Given $u(\cdot, \cdot)$ such that one of the level curves $y = f(x)$ is discontinuous at $x = t$ , construct (or show the existence of) two points $p, l \in X$ such that $l$ is a limit point of both $L(p)$ and $U(p)$ but is contained in exactly one of the two (contour sets). (The other level curves may or may not be discontinuous.) I posted a question on Economics SE that I could solve only partially. This is essentially the same question presented in a different manner. Here's the original question: Let $X = \mathbb{R}^2$ . Suppose $\succeq$ denotes a continuous preference relation. If every indifference curve can be represented by functions from $\mathbb{R}$ to $\mathbb{R}$ , will it mean the ICs will be continuous functions? If you want to see my attempt (and/or post an answer), please click on this .","['economics', 'functions', 'real-analysis']"
4428878,The role of machine precision in the derivative approximation,"You wish to compute of $f^{\prime}(x)$ using the approximation $$\widehat{f^{\prime}}(x)=\frac{f({x}+h)-{f}(x)}{h}$$ for some fixed $0<h<0.01$ . Suppose that the computer already contains a built-in function for computing $f$ , with relative errors less than machine precision, $t$ , and you know the value of a constant $A$ satisfying $\left|f^{\prime \prime}(y)\right| \leq A$ for all $y$ with $|y-x|<0.01$ . (a)  Estimate the total error incurred when computing $f^{\prime}(x)$ . Give your answer in terms of $f(x), f^{\prime}(x)$ , $h$ and $t$ . My attempt: Taylors remainder theorem says $$f(x+h)=f(x)+hf'(x)+(h^2/2!)f''(\zeta),~$$ for some $\zeta \in (x,x+h)$ . Now $$
\widehat{f^{\prime}}(x)=\frac{f({x}+h)-{f}(x)}{h}=f'(x)+(h/2)f''(\zeta),
$$ implies $$|\widehat{f^{\prime}}(x)-f'(x)| \leq (h/2)A,$$ since $|\zeta-x|<0.01$ . Doubt: What exactly is the role of $t$ here? How the relative error in computing $f$ using the  built in function comes into play?","['numerical-methods', 'derivatives', 'real-analysis']"
4428892,proof sum of power using Stirling number,"question is prove $$ 1^{k} + 2^{k} + 3^{k} + \dots + n^{k} = \sum_{i=1}^{n} S(k,i) \cdot i! \cdot {{n+1}\choose{i+1}} $$ In my opinion, LHS means number of functions from X to Y (X has 'k' element and Y has 'n' element)
And $$\sum_{i=1}^{k} S(k,i) \cdot i!$$ on RHS means number of function from X to Y (X has 'k' element and Y has only 'i' element). But I can't get the meaning of ${n+1}\choose{i+1}$ on the RHS. I know ${n}\choose{i}$ means choose i element among n element in Y but I can't get why add 1 respectively.","['combinatorics', 'stirling-numbers', 'combinatorial-proofs']"
4428927,"I have the integral $\int_{-1}^{1} \frac{\arccos(x)}{1+x^2} \,dx $ and some questions. Any help appreciated!","$$\int_{-1}^{1} \frac{\arccos(x)}{1+x^2} \,dx $$ Hi everyone! Sorry for my poor formatting skills, I'm still quite new to this platform. I do not know how to solve this integral. Things that I tried but failed miserably: I've tried substituting $t = \frac{1}{1+x^2}$ but this (of course) does not work since its $\arccos(x)$ and not $\arctan(x)$ . I've tried solving it by parts but that also leads to a dead-end. Since the bounds are $1$ and $-1$ , I tried checking if the function is even or odd but $\arccos(x)$ is neither. My questions (even though they might sound ridiculous or noobish) are: Is there a Weierstrass-like substitution for inverse trigonometric functions? (tried googling it but to no avail) How would one go about solving this type of integral? Thanks in advance for help!","['integration', 'definite-integrals', 'substitution', 'trigonometric-integrals', 'trigonometry']"
4428954,Rigorously working with flat limits: lines meeting a curve by specialization,"I am trying to get comfortable with flat limits. This question is motivated by Section 3.5.3 of Eisenbud and Harris's '3264 And All That' and Exercises 3.35 and 3.36. This section and the surrounding ones are dedicated to working with flat limits, but unfortunately, a lot of the arguments seem ""handwavey"" to me. I feel like this is a general issue for arguments like this: they all either seem not entirely convincing to me, or rely on some software to compute a saturation of an ideal. Let $C$ be a curve of $\mathbb{P}^3$ over an algebraically closed field. Our goal is to compute the rational equivalence class of $\Gamma_C\subset\mathbb{G}(1,3)$ of lines meeting $C$ . Choose a plane $H$ intersecting $C$ transversely in points $p_1,\ldots,p_d$ , and a point $q$ not lying on $C$ . Choose coordinates $z_0,\ldots,z_3$ such that $q=(1:0:0:0)$ and $H=\{z_0=0\}$ . Consider the subgroup of $PGL_4$ consisting of the automorphisms with matrices $$A_t=\left(
\begin{matrix}
    1 & 0&0&0\\
    0&t&0&0\\
0&0&t&0\\
0&0&0&t
\end{matrix}\right),\ t\neq0.$$ To do this, we consider the closure $\overline{\Psi}$ of the locus $$\Psi=\{(t,L)\subset \mathbb{A}^1\times\mathbb{G}(1,3)\ |\ t\neq0,\ L\cap A_t(C)\neq\varnothing\}.$$ I would like to identify the fiber $\overline\Psi_0$ as the union of the Schubert cycles $\Sigma_1(\overline{p_i,q})$ of lines meeting the lines $\overline{p_i,q}$ with multiplicity 1 along each. The support of $\overline\Psi_0$ must be contained in the locus of lines meeting the flat limit $C_0=\lim_{t\to0} A_t(C)$ , which is supported on the union of lines $\overline{p_i,q}$ , and it certainly contains the lines $\overline{p_i,q}$ themselves, since they meet each of the $A_t(C)$ , so it is equal to the union of the Schubert cycles $\Sigma_1(\overline{p_i,q})$ . The above is basically the argument written for a different problem in the book, but I do not quite get it. How can I rigorously show that the support of $\overline\Psi_0$ must be contained in the locus of lines meeting the flat limit $C_0=\lim_{t\to0} A_t(C)$ ? It is certainly belieavable from a sort of 'continuity', but I have no idea how to prove something like this without digging into the ideals of the various subvarieties involved, which does not seem tractable by hand. The authors also write that the flat limit $C_0$ may have an embedded point at $q$ , but this does not happen for $\Psi_0$ , because it is a divisor in $\mathbb{G}(1,3)$ . Is it true that a flat limit of divisors is a divisor? I can see this from a Hilbert polynomial argument for divisors of $\mathbb{P}^n$ . Finally, it remains to compute the multiplicity of the limit along each $\Sigma_1(\overline{p_i,q})$ . I guess it is 1, since we have stable lines in $H$ that meet $A_t(C)$ for each $t\neq 0$ only at one of the points $p_i$ and these lines are certainly not the limit of any other lines. However, this is the point I consider the least convincing. I would like to avoid saying the word ""limit"" here, since most lines in $\Sigma_1(\overline{p_i,q})$ , the ones not passing through either $p_i$ or $q$ are not limits of anything. What would be the best way to compute the multiplicity of each component here?","['projective-geometry', 'grassmannian', 'algebraic-geometry', 'flatness', 'intersection-theory']"
4429028,Why does the limit $\log{2^\pi}/\log{2\pi}$ emerge as a ratio of a certain set of adjacent prime factors?,"Let $f(n)$ for $n\in\mathbb N$ be a function that increases the prime index of each prime factor of $n$ (with multiplicity) by $1$ . e.g. $f(20)=f(2^2\cdot 5)=f(p_1^2\cdot p_3)=p_{2}^2\cdot p_{4}=3^2\cdot 7=63$ . Let the set $S_n=\{n< s< 2n : f(s)\leq 2n\}$ , for any $n\in\mathbb N$ . If you take the mean of $\frac{f(s)}{s}$ for each element $s \in S_n$ , empirical results for $n \leq 10^8$ strongly suggest that this mean converges: $$\lim_{n\to\infty} \left(\frac{1}{|S_n|}\sum_{s \in S_n} \frac{f(s)}{s}\right)=\frac{\log 2^{\pi}}{\log {2\pi}}\approx 1.18484.$$ $$$$ Can anyone explain why this should be so? My motivation in asking is as part of a larger study of the behavior of $f$ , and this result seems too nice and tidy to ignore without investigation. If anyone can explain the origin of the RHS term, or show that it is not a limit after all, I'll consider this question answered. As a rough picture of what empirical results I'm referring to, here's a plot of this mean as $n$ increases by a factor of $1.1$ each step to nearly $10^8$ : It appears convincingly centered around $\frac{\log{2^\pi}}{\log{2\pi}}$ , which seems much more plausible than any other nearby constants I could find, and it's varying by around $10^{-6}$ at the tail end there. Per request, Mathematica code to generate plot. This was quick 'n dirty and I'm certain it could be optimized to run much faster. Use Alt+. to halt execution. ClearAll[f, dyn];

f[1] = 1;
f[n_, k_ : 1] := Times @@ (NextPrime[#1, k]^#2 &) @@@ FactorInteger@n;
SetAttributes[f, Listable];
SetAttributes[dyn, HoldAll];

dyn[expr_, symbols_List : {}, interval_ : \[Infinity]] := 
 PrintTemporary[
  Dynamic[Refresh[expr, TrackedSymbols :> symbols, 
    UpdateInterval -> interval]]]

tab = {};
disp := Column[{Length@tab, n, ListLinePlot[Last/@tab, ImageSize -> Large], 
   Grid[Prepend[
     Reverse@tab, {Style[""n"", Bold], Style[""S mean"", Bold]}], 
    Dividers -> All, Alignment -> {Left, Top}]}]
dyn[disp, {tab}];

n = 100;
While[True,
 s = Select[f[Range[n + 3 - Mod[n, 2, 1], 2 n, 2], -1], # > n &];
 m = Mean[N[f@#, 8]/# & /@ s];
 AppendTo[tab, {n, m}];
 n = Ceiling[1.1 n];
 ]
disp","['limits', 'elementary-number-theory', 'prime-numbers']"
4429042,Trouble with a triple integral on a region bounded by a sphere and two planes,"I would like to compute the integral $\int_A zdzdydx,$ where $A$ is the region bounded by the sphere $x^2+y^2+z^2=R^2,$ plane $\frac{x}a+\frac{y}b=1$ and coordinate planes (which doesn't contain the origin on its boundary and is in the first quadrant). I considered switching to either spherical or cylindrical coordinates, but I don't see any symmetry or pattern. In spherical coordinates, I tried expressing the lower bound for radius $r$ in terms of $\theta\in[0,\pi/2]$ $$r(\theta, \varphi)=\frac{ab}{a\sin\varphi\sin\theta+b\cos\varphi\sin\theta}$$ and the integral becomes \begin{aligned}&\color{white}=\int_0^{\pi/2}\int_0^{\pi/2}\int_{\frac{ab}{a\sin\theta\sin\varphi+b\sin\theta\cos\varphi}}^Rr^2\sin\theta r\cos\theta drd\theta d\varphi\\&=\int_0^{\pi/2}\int_0^{\pi/2}\sin\theta\int_{\frac{ab}{a\sin\theta\sin\varphi+b\sin\theta\cos\varphi}}^Rr^3drd\theta d\varphi\end{aligned} In cylindrical coordinates $$r(\varphi)=\frac{ab}{a\sin\varphi+b\cos\varphi}$$ and the integral is $$\int_0^{\pi/2}\int_{\frac{ab}{a\sin\varphi+b\cos\varphi}}^R\int_0^{\sqrt{R^2-r^2}}zrdzdrd\varphi$$ However, I have to deal with a powers of $a\sin\theta\sin\varphi+b\sin\theta\cos\varphi$ and $a\sin\varphi+b\cos\varphi$ too early, and if I swap the order of integration, I need to express the angles in terms of $r$ which seems worse. I saw the substitution in this answer, but it isn't so smooth here and I've seen the reduction formula . How should one attack this task?","['integration', 'multivariable-calculus', 'multiple-integral', 'real-analysis']"
4429126,Which is a better definition for connected sets?,"In many analysis courses and books there are two main definitions of connected sets that may not be the same (I know that in both cases a connected set is intended to be defined as a set that has no ""holes""). Which one would you say that is a more complete definition? One definition may be: ""A metric space $S$ is called disconnected if $S= A \cup B$ , where $A$ and $B$ are disjoint nonempty open sets in $S$ . We call $S$ connected if it is not disconnected."" Another definition may be: ""Two subsets $A$ and $B$ of a metric space $X$ are said to be separated is both $A \cap \bar{B}$ and $ \bar{A} \cap B$ are empty (where $\bar{A}$ and $\bar{B}$ represent the closure of $A$ and $B$ ). A set $E \subset X$ is said to be connected if $E$ is not a union of two nonempty separated sets."" As can be seen, the first definition requieres $A$ and $B$ to be open disjoint sets in a metric space while the second definition requires $A$ and $B$ to be separated which is not the same as open and disjoint, because closed disjoints sets are also separated. So it seems to me that the second definition is more general and maybe more complete. My question is, which is more useful for studying topology/analysis or mathematics in general?","['elementary-set-theory', 'calculus', 'general-topology', 'real-analysis']"
4429162,A circle of radius $1$ is tangent to $y=x^2$ at two points. Find the area bounded by the circle and the parabola. (Alternate solutions?),"This is from Rambo's Math subject GRE book. One solution to this problem is to note that the equation of the circle is $x^2+(y-k)^2=1$ . By taking the derivative of this and solving for $y'$ , and then setting this equal to the derivative of the parabola, $y'=2x$ , we can solve for $x$ and $k$ and get all the information we need to set up the necessary integeral: $2\int_0^{\frac{\sqrt{3}}{2}}\frac{5}{4}-\sqrt{1-x^2}-x^2dx=\frac{3\sqrt{3}}{4}-\frac{\pi}{3}$ Where the expression involving a square root can is evaluated with trig substitution and power reduction identities. The author of the text mentions that this problem also be solved by using some trigonometry and the fact that: $2\int_0^{\frac{\sqrt{3}}{2}}x^2dx=\frac{\sqrt{3}}{2}$ However I can't figure out what he means by this. Can anyone show me how to solve this problem using less calculus and more trigonometry? Also, if anyone has any other alternative methods, I'd greatly appreciate it. Thanks!","['calculus', 'trigonometry']"
4429215,Colouring 9 vertices on a circle.,"Today I've got a combinatorics problem that I've been trying to solve and I think I have found a good algorithm to get. The problem is the following Suppose that there are 9 points in a circle. Tagged from 1 to 9. We want to color theses points with the colours Green, Red and Orange respectively in the following way: There must be 3 points coloured with the same color for each of the colours. (No matter their position) Any two consecutive points will have a different colour. So my thoughts are as follows:
Construct a circle with 9 vertices on it $ v_{1}, v_{2}, \cdots, v_{9}$ . The vertices will represent any of the points. To get the desire configurations consider the following. We will meet any two vertices of the same color with an edge so that a triangle can be constructed of the 3 necessary vertices having the same color for each colour respectively. Now, How many configurations of triangles are there such that the given conditions hold? In this case we can and first of all, we can choose a vertex in 9 ways. Notice, we cannot color the two adjacent vertices aside that chosen vertex (in the second image if we choose vertex 1 and it is red, we cannot color $v_{2}$ neither $v_{9}$ red. Then there are 6 possible choices for the second vertex, once I've chosen this second vertex, I can't choose the two vertices adjacent to it, then I'll have another 4 options for the second vertex, and for the third for the same reasoning I'll have just 2 options. For making the first triangle configuration, there are $ 9( 6\cdot 4 \cdot 2) $ . For the second colored triangle, there are 6 remaining vertices, and by the same process made in the first there are thus $ 6( 4 \cdot 2 \cdot 1)$ triangles. The third triangle is fixed due to the selection of the last 2. Then by the addition principle the number of triangles is $ 6( 4 \cdot 2 \cdot 1) + 9( 6\cdot 4 \cdot 2) = 264 $ . Here I have a question, look that you can choose the color of  that first chosen vertex in 3 ways. Should I multiply by 3 all the results I've just done or am I be overcounting? Or am I wrong in something above?
Any corrections are welcome, and this is not homework.","['contest-math', 'coloring', 'combinatorics', 'circles']"
4429291,Does Tetrahedron maximize the total squared distance between $4$ points on a sphere?,"Let $x_1,x_2,x_3,x_4$ be points on the unit sphere $\mathbb{S}^2$ , that maximizes the quantity $$
\sum_{i < j}\| x_i - x_j \|^2,
$$ where $\| x_i - x_j \|$ denotes the Euclidean distance in $\mathbb{R}^3$ . Do the $x_i$ form the shape of a Tetrahedron? I saw this related question , which considers the total sum instead of sum of squares, but there is no proof or reference there for the case of four points. I tried using Lagrange's multipliers but this got a bit messy; I got that $$
\sum_{i \neq j} \big(\langle v_i,x_j \rangle+\langle x_i,v_j \rangle \big)=\sum_i\lambda_i \langle x_i,v_i \rangle, 
$$ for every $(v_1,v_2,v_3,v_4) \in (\mathbb{R}^3)^4$ , but I am not sure how to proceed from here.","['geometry', 'lagrange-multiplier', 'multivariable-calculus', 'optimization', 'symmetry']"
4429294,Representation of $\sum_{k=1}^{\infty} \frac{f^{(k)}(a)}{k!} k^{-p} $ in terms of another series,"I face a problem when I'm dealing with one of the transforms and the problem is as follows:
Suppose that we have $$ f(a+z) = \sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!}z^{k} \tag{1} $$ $$ f\left(a+e^{\theta x}\right) = \sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!}e^{\theta k x} \tag{2} $$ and I want to express the following series in terms of equation $\text{(1)}$ or $\text{(2)}$ $$\sum_{k=1}^{\infty} \frac{f^{(k)}(a)}{k!} k^{-p} $$ where $ 0 < p < 1 $ . Is there any way that we can do this thing? Or, is it impossible to represent the third series in terms of equation $\text{(1)}$ or $\text{(2)}$","['integration', 'fourier-transform', 'real-analysis', 'complex-analysis', 'fourier-series']"
4429307,"Finding $(a,b)\in\mathbb{N}^2$ such that $\dfrac{a^2+b^2+1}{a+b} \in \mathbb{N}$.","A pair $(a,b)\in\mathbb{N}^2$ is called good if $a < b$ and $$\frac{a^2+b^2+1}{a+b}\in\mathbb{N}.$$ I think I've shown that there are infinitely many good pairs. However, the family of good pairs that I've found is not all of them, and the argument is not as elementary as I'd like.
Observation: we can check small values to show that good pairs do indeed exist, e.g. $(1,2),\, (4,7),\,(5,12),\, (11,16)$ . To construct an infinite family of good pairs, suppose we have $$\frac{a^2+b^2+1}{a+b}=k\in\mathbb{N},$$ then $a^2+b^2+1 - k(a+b)=0$ , and completing the square for $a$ and $b$ gives the equation for a circle: $$\left(a-\frac{k}{2}\right)^2+\left(b-\frac{k}{2}\right)^2 = \frac{k^2}{2}-1$$ $$\implies a = \frac{k}{2} + \sqrt{\frac{k^2}{2}-1-\left(b-\frac{k}{2}\right)^2}.$$ Let $k=2j$ for some $j\in\mathbb{N}$ to eliminate the fractions, obtaining $a=j+\sqrt{j^2-1-b^2+2jb}$ . In order to have $a\in\mathbb{N}$ , we must have $j^2-1-b^2+2bj=n^2$ for some $n\in\mathbb{N}\cup\{0\}$ . We can view this as a quadratic in $b$ : $$b^2-2jb+(n^2+1-j^2)=0.$$ Since we require $b\in\mathbb{N}$ , the discriminant $\Delta$ must be of the form $\Delta = 4m^2$ for some $m\in\mathbb{N}\cup\{0\}$ . Thus we have the following expressions for $a$ and $b$ : $$b=\dfrac{2j\pm\sqrt{\Delta}}{2} = j+m, \qquad a = j + \sqrt{n^2} = j+n.$$ Considering the discriminant: $$\Delta = (-2j)^2 - 4(n^2+1-j^2) = 4m^2$$ $$\implies 2j^2-1 = m^2+n^2.$$ If $a<b$ we must have $n<m$ . In particular, we can assume $n=0$ without restricting $m$ . Then we have $m^2-2j^2=-1$ , and so we have a correspondence between certain good pairs $(a,b)$ and elements of norm $-1$ in $\mathbb{Q}(\sqrt{2})$ . For $x+y\sqrt{2}\in\mathbb{Z}[\sqrt{2}]$ , define $N(x+y\sqrt{2})=x^2-2y^2$ , and observe that this function is multiplicative. We have $N(1\pm\sqrt{2})=-1$ , and thus $N((1\pm\sqrt{2})^{2r-1})=(-1)^{2r-1}=-1$ for any $r\in\mathbb{N}$ . Let $m+j\sqrt{2} = (1+\sqrt{2})^{2r-1}$ and so $m-j\sqrt{2} = (1-\sqrt{2})^{2r-1}$ . We can combine these expressions to obtain the following general formulas for values of $m$ and $j$ : $$m_r = \frac{(1+\sqrt{2})^{2r-1}+(1-\sqrt{2})^{2r-1}}{2},\qquad j_r = \frac{(1+\sqrt{2})^{2r-1}-(1-\sqrt{2})^{2r-1}}{2\sqrt{2}}.$$ Then we have $a=j+n=j$ and $b=m+j$ , thus we have an infinite family of good pairs $(a_r,b_r)$ for $r\in\mathbb{N}$ given by: $$a_r = \frac{(1+\sqrt{2})^{2r-1}-(1-\sqrt{2})^{2r-1}}{2\sqrt{2}},\qquad b_r = \frac{(1+\sqrt{2})^{2r}-(1-\sqrt{2})^{2r}}{2\sqrt{2}}.$$ The first few values of $r$ gives the good pairs $(1,2),\,(5,12),\,(29,70),\,(169,408),(985,2378)$ with corresponding values of $k=2,10,58,338, 1970$ . I seek advice both on this proof itself (i.e. does it work?) and also in seeking a more elementary argument (i.e. without requiring knowledge about $\mathbb{Q}(\sqrt{2})$ and the like) - it doesn't necessarily need to be constructive, in fact it would be interesting to know if there exists a more concise argument to that effect. Regarding finding good pairs themselves, comparing the list constructed in my solution to thta of the observation, it's clear that this solution 'misses' some pairs - and this comes a no surprise since we disregard the cases that $k$ is odd, and $n>0$ . If $k=2j-1$ , then we can use a similar method to show $j^2-j-b^2+2jb-b=n^2-n$ for some $n\in\mathbb{N}$ , but I haven't been able to make any progress from this. The case for $k$ even and $n>0$ is essentially asking ""when can $2j^2+1$ be written as the sum of two squares"", which I'm not sure quite how to answer - using known results about sums of two square seems difficult as $2j^2+1$ has no general factorisation. Are there totally different methods that can be used to construct more families of good pairs? And what about any other 'structure' to good pairs? It seems interesting that units in $\mathbb{Z}[\sqrt{2}]$ give rise to the solutions, can a more general statement be made?","['sums-of-squares', 'number-theory', 'elementary-number-theory', 'integers', 'solution-verification']"
4429326,169 points are chosen at random inside an equilateral triangle of perimeter 300.,"I have this excercise: 169 points are chosen at random inside an equilateral triangle of perimeter 300. Prove that there are 3 of these points that determine a triangle of area at most 68. I think that i can use the generalized pigeonhole principle which i know as: Let $n,m,r$ be integers such that $n>rm$ . If $n$ objects are placed into $m$ boxes, then there is at least one box containing at least $r+1:=\lceil n/m\rceil$ objects. Since $n=169$ and I would like 3 points to fall in the same hole to form a triangle with an area smaller than that of the hole, I take $r=2$ , then $m$ is the largest square less than $\lfloor n/r\rfloor=84$ (This is because when dividing an equilateral triangle into triangles that are also equilateral, I can do it in a square number of them) so $m=81$ . Now by Heron's formula the area of triangle $XYZ$ is $$A=\sqrt{s(s-x)(s-y)(s-z)}$$ Where $s=\frac{x+y+z}{2}$ is the semiperimeter, in this case $x=y=z:=l=100$ , so since $m=81=9^2$ , we are dividing each side into $9$ equal segments, so the side of each small triangle measures $100/9:=b$ . The semiperimeter of the small triangle is $c=\frac{3b}{2}=\frac{50}{3}$ , apply now the Heron's formula $$a=\sqrt{c(c-b)^3}=(c-b)\sqrt{c(c-b)}=\frac{2500}{27\sqrt{3}}\approx53.4583\leq68$$ But i see that it's a bad approximation, so I thought that I could divide the triangle into fewer holes, that is, divide each side into one less space, where $m=64$ and in this way the calculations would be: $b=\frac{25}{2}$ , $c=\frac{75}{4}$ , $$a=\sqrt{c(c-b)^3}=\frac{25^2}{4^2}\sqrt{3}\approx67.6582\leq68$$ Which is a much better approximation, but I don't know how to justify this reasoning, or if I'm thinking it right, I need help.","['pigeonhole-principle', 'combinatorics', 'triangles', 'discrete-mathematics']"
4429363,How do I design the most unfair dice for this game?,"I was thinking about a simple dice game where the goal is to roll all the face values of a six-sided die consecutively in order (1,2,3,4,5,6). If I'm not mistaken, the probability of doing this with a fair die is $\left(\frac{1}{6}\right)^6$ . And I think I'm correct in saying that there is no way to bias the die to increase my chances of winning. In other words, a fair (uniform) die maximizes the probability of winning this game. So, I added a new rule: if the rolled value is smaller than the ""target"" value, then the player simply rolls again. For example, {1,2,3,4,5, 2 ,6} counts as a win because rolling a 2 when the target is 6 can be ignored because $2 < 6$ . By removing the ignored values from the equation, I think the probability of winning (with a fair die) has increased from $\left(\frac{1}{6}\times\frac{1}{6}\times\dots\times\frac{1}{6}\right)$ to $\left(\frac{1}{6}\times\frac{1}{5}\times\dots\times\frac{1}{1}\right)$ . In other words, it's now $\frac{1}{6!}$ . My question is: how should I (unfairly) weight my die in order to maximize the probability of winning? I feel like I should favor small values because they carry less risk (1 never loses, 2 only loses if the target is exactly 1, etc). However, I'm not sure how to formulate this problem. Also, I feel intimidated by the fact that 1 never loses. If I design a die that always rolls 1, then I can never lose... but I can never win either. So I'm not sure if maybe I need to constrain my question in some way.","['optimization', 'dice', 'probability']"
4429385,Why do we consider a critical region instead of individual values? Why does the alternative hypothesis determine which tail to consider for rejection?,"Setup : Suppose a coin is tossed 8 times and I'm trying to determine whether the coin is biased in favour of landing on heads. Let $X$ be the number of heads in 8 tosses, so $X \sim B(8,p)$ . Conducting a hypothesis test with a significance level of $5\%$ , my null hypothesis would be $H_0\!: p = 0.5$ and my alternative hypothesis would be $H_1\!: p > 0.5$ . The method of conducting this test that I have learned is that I would first need to consider $X \sim B(8,0.5)$ . Since the alternative hypothesis is p greater than 0.5, I would need only consider the right tail of the distribution. Then I would need to find the minimum value of $a$ such that $P(X\geq a) < 0.05$ , so $[a,8]$ is the critical region. Thus if we observe $X$ to be between $[a,8]$ , it is statistically probable that the die is biased. Question : Why do we consider a critical region instead of individual values? It makes sense to me why we would consider a region in the context of continuous distributions, but why should we do it in the case of a discrete distribution? Suppose from the setup, $P(X=8) = 0.03$ and $P(X=7) = 0.049$ , then $X=7$ would not be part of the critical region even though individually its probability is still below the set threshold of $5\%$ . I don't understand why we should disregard $X=7$ . Why does the alternative hypothesis determine which tail to consider for rejection? The rule I have been taught is, if the alternative hypothesis contains a "">"", consider the right tail and if it contains a ""<"", consider the left tail.  Suppose from the setup, $P(X\geq7) < 0.05$ , then it's also true that $P(X\leq2) < 0.05$ . Thus why are we only considering one of the tails even though both tails contain values which are below the $5\%$ significance level? As $p$ increases, both of these regions are going to change in values, so I don't understand why we're focusing on the right tail. How does the alternative hypothesis play into this? Thanks for taking the time to read my question. Please don't respond with too much jargon-heavy language since I'm still in high school! I appreciate any and all help.","['statistics', 'hypothesis-testing']"
4429553,Is there positive integer solution of $a^b - c^d = 6$?,"Question : is there any integer solution of $$a^b - c^d = e$$ where $e=6$ , and $a,b,c,d \geq 2$ ? Any idea to attack this type of problem? Thanks. Adam Rubinson comments that $a$ and $c$ are both odd by considering on $\mathbb Z/2 \mathbb Z$ . We have $b \neq d$ since otherwise $a-c = 2$ which is impossible because the value of $(a+2)^n - a^n$ increases with $n$ and $a$ , and $4^2 - 2^2 = 12 > 6$ . What I've done : for $1 \leq e \leq 9$ and $e \neq 6$ , there does exist a solution. But for $e = 6$ , I've searched through all integers under $10^{13}$ of the form $a^b$ with no luck. I'll list below the solutions found: $$
\begin{align}
1 & = 3^2 - 2^3 \\
2 & = 3^3 - 5^2 \\
3 & = 2^7 - 5^3 \\
4 & = 2^3 - 2^2 \\
5 & = 2^5 - 3^3 \\
7 & = 2^4 - 3^2 \\
8 & = (2^3 \times 3 \times 13)^2 - (2 \times 23)^3 \\
9 & = 5^2 - 2^4 \\
\end{align}
$$",['number-theory']
4429590,Average number of couples in dance competition finale,"11 pairs of dancers will participate in a dance competition finale. However, on the day of the competition, 7 dancers were tested Covid positive and had to be eliminated. Given that partners aren’t interchangeable, meaning, if two dancers are left without their pair, they can’t dance together to form another pair, how many pairs on average are going to compete? Well, I am not sure how to approach this. Best case scenario is when the 7 sick dancers form $3.5$ pairs, that is, $4$ pairs can't participate, so we are left with $7$ pairs. There are 22 ways to select the first sick dancer. Then for the second, there is $1$ of $21$ left, to choose his pair, so ${22\choose 2}$ for one pair, then ${20\choose 2}$ for a second one and ${18\choose 2}$ for a third. It is getting very complicated to consider all the remaining cases. I would appreciate your assistance as it's been years since I worked with combinatorics and probabilities. Thank you.","['combinatorics', 'probability']"
4429650,Closed points of $\operatorname{Spec} R$ maximal?,"Show that the closed points of $\operatorname{Spec} R$ are exactly the elements of $\operatorname{maxSpec} R$ that is, every closed point of $\operatorname{Spec} R$ is maximal. Proof attempt: Let $\{\mathfrak{p}\} \subset \operatorname{Spec}$ be closed. Then $\overline{\{\mathfrak{p}\}} = \{\mathfrak{p}\}$ , but $\overline{\{\mathfrak{p}\}}$ is the smallest closed set containing $\mathfrak{p}$ and since the closed sets of the spectrum are of form $V(I)=\{ \mathfrak{p} \in \operatorname{Spec}R \mid \mathfrak{p} \supset I\}$ the smallest one containing $\mathfrak{p}$ is $V(\mathfrak{p})$ , thus $$\{\mathfrak{p}\} = \overline{\{\mathfrak{p}\}} = V(\mathfrak{p})$$ and so there doesn't exist any prime ideal $\mathfrak{q}$ of $\operatorname{Spec}$ properly containg $\mathfrak{p}$ . Since every maximal ideal is prime this implies that no maximal ideal contains $\mathfrak{p}$ so $\mathfrak{p}$ must be maximal. Is the idea here correct? I'm wondering if there exists a maximal ideal that isn't a prime which would contain $\mathfrak{p}$ which would make this conclusion false.","['affine-schemes', 'algebraic-geometry', 'abstract-algebra']"
4429683,Proving isomorphism between two vector spaces of homomorphisms,"Let $(\rho,V)$ be a representation of $G$ , and we consider the following action of $G\times G$ on $\hom(V,V)$ : $$\forall(g,h)\in G\times G:(g,h).\phi=\rho_g\circ\phi\circ\rho_{h^-1}$$ Find an isomorphism between $\hom(V,V)^{G\times G}$ and $\hom(V^G,V^G)$ . First, clarification of definitions: $$\hom(V,V)^{G\times G}=\{\phi\in \hom (V,V):\forall(g,h)\in G\times G, \phi=\rho_g\circ\phi\circ\rho_{h^-1} \}$$ and $$V^G=\{v\in V:\forall g\in G,\rho_g (v)=v \}$$ Where $\rho_g$ is my notation to the linear transformation corresponding to $g$ . My attempt: I first showed that if $v\in V$ , considering the pair $(g,e)\in G\times G$ , I have: $$(\rho_g\circ\phi\circ\rho_e)(v)=\phi(v)$$ and therefore $$\rho_g(\phi(v))=\phi(v)$$ So $\phi(V)\subset V^G$ . Now I can define $\psi:\hom(V,V)^{G\times G}\to\hom(V^G)$ by $\psi(\phi)=\phi|_{V^G}$ , but I can't see if this is one-to-one\onto. I feel like there's a trick I'm missing. Any hint would be appreciated.","['group-homomorphism', 'representation-theory', 'linear-transformations', 'group-theory', 'group-actions']"
4429693,Expected number of questions when the student knows 50 out of 250 questions and the teacher selects 25,"A teacher has 250 sample test questions and he selects 25 of which to put on the test. Johnny only practices 50 out of the 250 questions. a) What's the expected number of questions that appear on the exam that Johnny have practiced on? My answer:
I'm quite confused so I don't think it's the right answer. I know that the expected value is just $$\sum_{k=1}^{25} \left[kp(k)\right]$$ However, I'm not sure what $p(k)$ is. I think it's $$ \frac{25}{250} \frac{50}{250}  = 0.02 $$ which would make $E(X) = 6.5$ b) What's the probability that Johnny hasn't practiced any of the exam questions? My answer:
Here I tried using the hypergeometric distribution and set N = 250, r = 25, n = 50, and x = 0. Obviously assuming here that if Johnny sees a question he's practiced he'll be able to solve it.
This means the final answer will be $$\frac {{50 \choose 0}{225 \choose 50}} {{250 \choose 50}}$$ which is $$\frac {{225 \choose 50}} {{250 \choose 50}}$$ Are those answers correct? Any help would be appreciated! Thank you.","['discrete-mathematics', 'combinatorics', 'probability']"
4429701,Can I prove Pythagoras Theorem by concatenating four right triangles?,"I was considering ways to visually prove Pythagoras' Theorem. I was reasoning if I could concatenate the same right triangle four times, as if it was reflected from both axis. Why can't I just calculate the area of this diamond to know c ? What am I doing wrong? Why does it make no sense?
Please, excuse me if I'm doing an obvious mistake or wrong assumption. Thanks.","['trigonometry', 'solution-verification', 'geometry']"
4429725,Lipschitz continuity of square root of sum of matrix squares,"Let $B\in \mathbb{R}^{n\times n}$ be a positive semi-definite matrix (i.e., $B \in \mathbb{S}^n_{\ge 0}$ ),   and consider the map $$
\mathbb{R}^{n\times n}\ni A \mapsto f(A):=(AA^\top +B)^{1/2}\in \mathbb{S}^n_{\ge 0},
$$ where $(\cdot)^{1/2}$ is unique positive semidefinite matrix square root. Is it possible to prove the map is Lipschitz continuous? How about local Lipschitz continuity? The answer is affirmative for one-dimensional case. By discussing whether $B=0$ or not, one can easily show $f(A)$ is Lipschitz continuous. In a multidimensional setting, if $B$ is positive definite, then the matrix square root $(\cdot)^{1/2}$ is Lipschitz continuous with the Lipschitz constant depending on the minimum eigenvalue of $B$ , and hence the map $f$ is locally Lipschitz. It is not clear to me how to relax the positive definite condition of $B$ for the locally Lipschitz continuity.","['matrices', 'linear-algebra', 'lipschitz-functions', 'matrix-decomposition']"
4429729,Are there any simple simplifications of $\sum_{n=k}^N \binom{n}{k}^2$?,"As the title states, is there any simplification to $\sum\limits_{n=k}^N \binom{n}{k}^2$ ? I found this which sums the squares of the ""rows"" of a pascal triangle, but here I'm trying to sum the diagonals. There is also this mathSE question from a few years ago with no answers. Update : I shall clarify that the answer is not a central binomial coefficient, nor using the hockey-stick identity. This is directly seen as $\binom{3}{3}^2 + \binom{4}{3}^2 + \binom{5}{3}^3 + \binom{6}{3}^2 = 517$ is not a central binomial coefficient.","['binomial-coefficients', 'combinatorics']"
4429809,"Orbits of symplectic group $Sp(2n,K)$ acting on $K^{2n}$","Let $J_n$ be a matrix of standard non-degenerate symplectic form $$J_n=\begin{bmatrix}
0 & -1 & 0 & 0 & \cdots & 0 & 0 \\
1 & 0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & 0 & -1 & \cdots & 0 & 0 \\
0 & 0 & 1 & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & 0 & -1 \\
0 & 0 & 0 & 0 & \cdots & 1 & 0 \\
\end{bmatrix}$$ With coeffitients in the field $K$ . I define the symplectic group to be $$Sp(2n,K)=\{A\in GL_{2n}(K) \ | \ A^TJ_nA=J_n\}$$ This group acts on vector space $K^{2n}$ by standard multiplication of matrix by vector $A \cdot v$ Question is to show that $Sp(2n,K)$ has 2 orbits: $K^{2n}\setminus\{0\}$ and $\{0\}$ . I tried to construct a matrix $A\in Sp(2n,K)$ that transforms vector $e_1=(1, 0, \cdots , 0)$ to nonzero vector $v=(x_1, x_2, \cdots, x_{2n})$ but managed to do it only for the case $n=1$ . How do I prove this for any $n$ ?","['group-theory', 'linear-algebra']"
4429828,Proving that disconnecting edges of a 3-edge-colorable graph are of the same color,"I'm struggling to prove the following: Edges of a connected cubic graph G can be colored with 3 colors in
such a way that no adjacent edges are of the same color. 2 edges were
removed from the graph making it disconnected. Proof that those edges
were of the same color. I've found an example of a cubic graph with two edges that disconnect it, and tried to color them with different colors and failed to do that, but I'm not sure how to prove that.","['coloring', 'graph-theory', 'graph-connectivity', 'combinatorics', 'discrete-mathematics']"
4429834,"For a family of short exact sequences of coherent sheaves, can we define the splitting subscheme?","Let $k$ be an algebraically closed field of characteristic zero. Let $X$ be a projective scheme over $k$ . We can talk about short exact sequences of coherent sheaves on $X$ . Suppose we have a family of SES's parametrized by $T$ , a scheme of finite type over $k$ , that is a short exact sequence of coherent sheaves on $X\times_kT$ $$0\to E\to F\to G\to0$$ such that $E,F,G$ are coherent sheaves on $X\times_kT$ , flat over $T$ ; for each closed point $t\in T$ , the fibre over $X_t:=X\times_k\mathrm{Spec}(\kappa(t))$ is exact (As commented, this condition can be derived from flatness and exactness of $0\to E\to F\to G\to0$ . ) $$0\to E|_{X_t}\to F|_{X_t}\to G|_{X_t}\to0\qquad(*)$$ My question is about the ''splitting locus'' $$\{t\in T:t\textrm{ is a closed point, }(*) \textrm{ is splitting}\}. $$ Now it is only defined as a subset of closed points. I wonder if this can be enhanced as a subscheme.","['coherent-sheaves', 'exact-sequence', 'algebraic-geometry', 'schemes']"
4429896,Prove that the directional derivative of a composition exists.,"Let $E \subseteq \mathbb{R}^{n}$ and $D \subseteq \mathbb{R}^{m}$ be two open sets, and let $f : E \rightarrow \mathbb{R}^{m}$ and $g : D \rightarrow \mathbb{R}^{k}$ be functions such that $f(E) \subseteq D$ . Suppose that $g$ is differentiable at $f(x)$ for some $x \in E$ , and
that for some $v \in \mathbb{R}^{n}$ with $\vert \vert v
\vert\vert= 1$ the directional derivative $D_{v}f(x)$ exists. Show that the
directional derivative $D_{v}(g \circ f)(x)$ exists and give a formula to compute it. I tried to begin by applying the definition of differentiability for $g$ , but I'm not sure how to do that with $f(x)$ . I am also not sure what exactly I am supposed to prove, i.e, what does ""show that the
directional derivative $D_{v}(g \circ f)(x)$ exists"" require? Would greatly appreciate any help!","['multivariable-calculus', 'chain-rule', 'real-analysis']"
4429910,MLE of the Geometric Distribution,"Suppose that $X_{1},X_{2},...,X_{n}$ are independently and identically distributed as $Ge(\theta)$ . (i) Find the maximum likelihood estimator of $\theta$ My solution: $\theta = \frac{n}{\sum_{i=1}^{n}x_{i}}$ Therefore, $E(\hat\theta) = \frac{1}{\theta}$ (ii) Hence show that the maximum likelihood estimator of $\psi = \frac{(1-\theta)}{\theta}$ is the sample mean $(\bar X)$ . Try as I might, I can't re-arrange the answer to question 1 into the form shown in question 2. Please may someone help me?","['statistics', 'probability-distributions', 'means', 'maximum-likelihood']"
4429952,Vakil's proof of Bertini's theorem (Theorem 12.4.2) - where do the linear conditions on the fiber come from?,"As I'm reading through Vakil's notes on algebraic geometry , I'm stumbling over some of the details in his proof of Bertini's theorem. The proof in question is discussed already on this site ( 1 , 2 and doubtless others). Summary of Vakil's proof for context. ( Please see page 346 of the above-linked notes for the source material. ) Fix an algebraically closed field $k$ for the remainder of the discussion. Further fix a smooth closed irreducible subvariety $X \subseteq \mathbb{P}^n$ . We wish to show that there is a dense open subset $U \subseteq (\mathbb{P}^n)^\vee$ such that for each closed point $H \in U$ , $H \cap X$ is smooth and $X \not\subseteq H$ . Essentially, Vakil proceeds by defining a closed subvariety of $Z \subseteq \mathbb{P}^n \times (\mathbb{P}^n)^\vee$ spanned by points $(p \in X, H)$ such that $p \in H$ and either $p$ is singular in $X \cap H$ or $X \subseteq H$ . He accomplishes this by cutting out the equations for $X$ , adding one equation enforcing $p \in H$ and then requiring the corank of the resultant Jacobian to be $\mathsf{dim}(X)$ . The details are in the following quote: 12.4.3. We first define $Z$ more precisely, in terms of equations on $\Bbb P^n\times\Bbb P^{n\vee}$ , where the coordinates on $\Bbb P^n$ are $x_0,\dots,x_n$ , and the dual coordinates on $\Bbb P^{n\vee}$ are $a_0,\dots,a_n$ . Suppose $X$ is cut out by $f_1,\dots,f_r$ . Then we take these equations as the first of the defining equations of $Z$ . (So far we have defined the subscheme $X\times\Bbb P^{n\vee}$ .) We also add the equation $a_0x_0+\cdots+a_nx_n=0$ . (So far we have described the subscheme of $\Bbb P^n\times\Bbb P^{n\vee}$ corresponding to the points $(p,H)$ where $p\in X$ and $p\in H$ .) Note that the Jacobian matrix ((12.1.6.1), except with variables starting with $x_0$ rather than $x_1$ ) $$\begin{pmatrix} \frac{\partial f_1}{\partial x_0}(p) & \cdots & \frac{\partial f_r}{\partial x_0}(p) \\ \vdots & \ddots & \vdots \\ \frac{\partial f_1}{\partial x_n}(p) & \cdots & \frac{\partial f_r}{\partial x_n}(p) \end{pmatrix}$$ has corank equal to $\dim X$ at all closed points of $X$ -- this is precisely the Jacobian criterion for regularity (Exercise 12.2.D). We then require that the Jacobian matrix with a new colum $$\begin{pmatrix} a_0 \\ \vdots \\ a_n\end{pmatrix}$$ appended has corank $\geq \dim X$ (hence $=\dim X$ ). This is cut out by equations (the determinants of certain minors). By the Jacobian description of the Zariski tangent space, this condition encodes the requirement that the Zariski tangent space of $H\cap X$ at $p$ has dimension precisely $\dim X$ , which is $\dim H\cap X+1$ (i.e., $H\cap X$ is singular at $p$ ) if $H$ does not contain $X$ , or if $H$ contains $X$ . This is precisely the notion we wished to capture. (Remark 12.4.4 works through an example, which may help clarify how this works.) There appears to be a small typo here: since we are considering the affine cone of $X$ in this, we should be careful to potentially replace $\mathsf{dim}(X)$ with $\mathsf{dim}(X + 1)$ . Question I am stuck attempting to understand the next part of the proof. Having defined $Z$ , Vakil claims the fiber of $Z$ over $\{p\} \times (\mathbb{P}^k)^\vee$ is cut out of $(\mathbb{P}^k)^\vee$ by $\mathsf{dim}(X) + 1$ linear conditions, but I'm afraid I don't see where these come from at all. The second linked question I've linked to above chases down the obvious answer: it's the requirement that certain minors vanish, but this doesn't seem to lead to the correct number of equations, but no clear alternatives present themselves. The set is clearly closed, but the requirement that it be determined by $\mathsf{dim}(X) + 1$ linear equations specifically is important and elusive. For completeness, I've included the direct quote from Vakil below: We next show that $\dim Z\leq n-1$ . For each closed point $p\in X$ , let $W_p$ be the locus of hyperplanes containing $p$ , such that $H\cap X$ is singular at $p$ , or else contains all of $X$ ; what is the dimension of $W_p$ ? Suppose $\dim X = d$ . Then the restrictions on the hyperplanes in definition of $W_p$ correspond to $d+1$ linear conditions. (Do you see why?) This means that $W_p$ is a codimension $d+1$ , or dimension $n-d-1$ , ...","['algebraic-geometry', 'projective-geometry', 'projective-schemes']"
4429957,Theorem 2.1.4 in Durrett's Probability Theory and Examples,"I am reading Durrett's Probability Theory and Examples proof of Theorem 2.1.4. Theorem 2.1.4: In order for $X_1, \dots, X_n$ to be independent, it is sufficient that for all $x_1, \dots, x_n \in (-\infty, \infty]$ that $P(X_1 \leq x_1, \dots, X_n \leq x_n) = \prod_{i=1}^n P(X_i \leq x_i)$ . Proof: Let $\mathcal{A}_i$ = sets of the form $\{X_i \leq x_i\}$ . [... (showing that $\mathcal{A}$ is a $\pi$ -system)] Since we have allowed $x_i = \infty, \Omega \in \mathcal{A}_i$ . [...] $\sigma(\mathcal{A}_i) = \sigma(X_i)$ , so the result follows from Theorem 2.1.3 (which states that if $\pi$ -systems $\mathcal{A}_1 \dots \mathcal{A}_n$ are independent, then the sigma-algebras generated by each of them are independent). My question is why do we need $\Omega \in \mathcal{A}_i$ ? The only condition for Theorem 2.1.3 is that each set is a $\pi$ -system, which only requires that $\mathcal{A}_i$ to be closed under intersection. Further, the $\sigma(\cdot)$ operation already ""adds"" in an $\Omega$ . Could someone please explain why is this sentence/step necessary? Thanks a lot!","['measure-theory', 'independence', 'probability']"
4429958,How to find an ideal of variety?,"I've been going through Ideals, Varieties, and Algorithms book by D. Cox et al. , and have been stuck on exercise 4.12. I've solved it in a half, but have stuck on the next question: how to find an ideal $I(V)$ of an variety $V$ ? Mainly, in the exercise there is an affine variety $$ V = \mathbf{V}(y^2-xz, z-x^2), $$ which I'm particularly interested in. I've already seen the pages on M.SE with similar question, e.g. this one , but I don't really understand why we just assume that the field is algebraically closed and then use Nullstellensatz. In the exercise there's nothing on this, and it seems it's assumed to be solved without Nullstellensatz. So how can one find an ideal of variety? Earlier in the book, D. Cox solves a similar exercise (p. 33). There, D. Cox wants to show that $$ I(V) = I(\mathbf{V}(y-x^2, z-x^3)) = \left<y-x^2, z-x^3\right>. $$ In order to do this, he factors $f \in I(V)$ by the generators, so $$ f = h_1(y-x^2) + h_2(z-x^3) + r, $$ such that $r$ is actually from $k[x]$ . Then, since $f \in I(V)$ , on $V$ we have following (using the parametrization of $V$ , for which $x=t, y=t^2, z=t^3$ ): $$ 0 = 0 + 0 + r(t), $$ and then $r=0$ , and we are done (here $k$ is assumed to be infinite). In the exercise it's said that the polynomial division algorithm will help in tackling this task, meaning that one can perform the same manipulations as above. But I can't still see how this can be done. The only intuition I had were the following: Since the generators are $y^2-xz, z-x^2$ , it seems that in ideal $\left<y^2-xz, z-x^2\right>$ any power of $x$ and $z$ can be obtained, whileas not every power of $y$ can be obtained (e.g. I can't get odd power). So the reminder in my case will be in $k[y]$ , and then the same steps as in the book applies. But I don't feel like it's the right direction. Moreover, I know that it's not right: consider $f = x^2 + yx + 5z$ , then the reminder is going to be $xy + 6z$ , which breaks everything stated above. (given $f \notin I(V)).$ So, can anyone help me to figure things out? Thanks!","['groebner-basis', 'algebraic-geometry', 'polynomials', 'ideals', 'commutative-algebra']"
4429968,Chain rule and generalized composition of multilinear maps,"I know that if functions $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^m \to \mathbb{R}^p$ are differentiable at $x \in \mathbb{R}^n$ and $f(x) \in \mathbb{R}^m$ , respectively, with derivative values $Df(x) \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $Dg(f(x)) \in L(\mathbb{R}^m, \mathbb{R}^p)$ , then it is possible to define composition function $(g \circ f) : \mathbb{R}^n \to \mathbb{R}^p$ for all $x \in \mathbb{R}^n$ as $(g \circ f)(x) = g(f(x))$ and show that it is also differentiable with a derivative, where for visual simplicity I use square brackets for evaluation of a linear function: $$ D(g \circ f)(x) = Dg(f(x)) \circ Df(x)$$ Now, my problem is how to generalize this result to higher order derivative and multilinear maps. To see where the problem arises, let me consider the second derivative. First, motivated by the result of the chain rule, define functions $Dg \circ f : \mathbb{R}^n \to L(\mathbb{R}^m, \mathbb{R}^p)$ for all $x \in \mathbb{R}^n$ as $(Dg \circ f)(x) = Dg(f(x))$ , and $Df : \mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m)$ for all $x \in \mathbb{R}^n$ as $Df(x)$ . Then, define composition of linear functions, where $x \in \mathbb{R}^n$ and $A,B$ are from correct $L$ spaces, as $(A \circ B)(x) = A(x) \circ B(x)$ . All of these definitions are made such that the chain rule for all $x \in \mathbb{R}^n$ can be written as follows. $$ D(g \circ f)(x) = ((Dg \circ f) \circ Df)(x) $$ Then, to motivate what the second derivative should be, consider the difference of $D(g \circ f)(x)$ , where we would like to use that $Dg \circ f$ and $Df$ are also differentiable. This means that for $y$ close to $x$ we have: $$(Dg \circ f)(y) - (Dg \circ f)(x) \approx D(Dg \circ f)(x)[y-x]$$ $$Df(y) - Df(x) \approx D(Df)(x)[y-x] $$ Here, $D(Dg \circ f)(x) \in L(\mathbb{R}^n, L(\mathbb{R}^m, \mathbb{R}^p)) =: L^2(\mathbb{R}^n, \mathbb{R}^m, \mathbb{R}^p)$ and $D(Df)(x) \in L(\mathbb{R}^n, L(\mathbb{R}^n, \mathbb{R}^m)) =: L^2(\mathbb{R}^n, \mathbb{R}^n, \mathbb{R}^m)$ . So, intuition for the second derivative chain rule formula would be as follows. $$ D(g \circ f)(y) - D(g \circ f)(x) = (Dg \circ f)(y) \circ Df(y) - (Dg \circ f)(x) \circ Df(x) =  ((Dg \circ f)(y) - (Dg \circ f)(x)) \circ Df(y) + (Dg \circ f)(x) \circ (Df(y) - Df(x)) \approx D(Dg \circ f)(x)[y-x] \circ Df(x) + (Dg \circ f)(x) \circ D(Df)(x)[y-x]$$ On the other hand, we want the previous equation to be equal to $D(D(g \circ f))(x)[y-x]$ . Therefore, I feel that one is motivated to define the following for appropriate multilinear maps $A,B$ , which would be what I call ""generalized composition"": $$ (A \circ_{L^2, L^1} B)[x] := A[x] \circ B,$$ $$ (A \circ_{L^1, L^2} B)[x] := A \circ B[x].$$ Then, the result would be written as follows. $$ D(D(g \circ f))(x) = D(Dg \circ f)(x) \circ_{L^2, L^1} Df(x) + (Dg \circ f)(x) \circ_{L^1, L^2} D(Df)(x)$$ However, if I now consider third derivative of $g \circ f$ , this approach seems to ""fail"" in the following way. If $A, B$ are both bilinear functions, so, belong to some $L^2$ , there is not a unique extension of my approach to generalized composition, as both are possible but I believe they are not equivalent. $$ (A \circ_{L^2, L^2} B)[x] = A[x] \circ_{L^1, L^2} B, $$ $$ (A \circ_{L^2, L^2} B)[x] = A \circ_{L^2, L^1} B[x].$$ Question : How to define and motivate unique generalized composition between multilinear functions so that it naturally appears in higher order chain rule results and that is well defined?","['frechet-derivative', 'multivariable-calculus', 'derivatives']"
4429999,How to prove $\int_0^\infty\frac {\tanh(x)-x\exp(-x)}{x^2}dx=\frac{\zeta'(0)}{\zeta(0)}-\frac{\zeta'(2)}{\zeta(2)}+\gamma-\frac73\log(2)$?,"By educated guessing, inspired by this solution of $\int_0^\infty\frac {\tanh^3(x)}{x^2}dx$ , I have found numerically: $$\int\limits_0^\infty\frac {\tanh(x)-x\exp(-x)}{x^2}dx=\frac{\zeta'(0)}{\zeta(0)}-\frac{\zeta'(2)}{\zeta(2)}+\gamma-\frac73\log(2).$$ The method should be similar to the quoted one, i.e. applying the residue theorem to a keyhole contour integral: $$  \frac{1}{2\pi i} \oint \frac {\tanh(z)-z\exp(-z)}{z^2}\log(-z) \, dz.$$ While I guessed correctly that, unlike in the original one, a term with $\gamma=-\int\limits_0^\infty\exp(-x)\log(x)dx$ will survive even in a more elegantly written closed form, I have a hard time understanding how/why the ""regulating part"" $\dfrac {\exp(-z)}{z}$ contributes to the sum of residues. Even if it contributed its own residue at $z=0$ (but how can it, as $z=0$ is outside the contour?), I wouldn't see how this residue could lead to $\gamma$ . Any hints, please?","['integration', 'complex-analysis', 'residue-calculus', 'riemann-zeta']"
4430029,"If a sequence converges, then if its quotient converges, then it converges to less than or equal to one","I want to show the following. Let $a_n$ converge. If $\frac{a_{n+1}}{a_n}$ converges then $|\lim\frac{a_{n+1}}{a_n}|\leq 1$ . I attach my proof below, but I would be happy to see if there is a more elegant and simple one. I would think one could use the following theorem, but I do not know how: If $a_n$ converges and $\frac{a_{n+1}}{a_n}$ converges, then $\sqrt[n]{a_n}$ converges and $\lim\left(\frac{a_{n+1}}{a_n}\right)=\lim\left(\sqrt[n]{a_n}\right).$ My proof - mind that the students have only learned the definitions regarding convergence, basic theorems regarding convergence and convergence to infinity. They still haven't learned the theorems regarding convergence and monotonic series, partial limits, Cauchy series, etc. Let us assume that $a_n$ converges and $\frac{a_{n+1}}{a_n}$ converges. Let us denote $\lim(a_n)=L$ . $$\lim(a_{n+1})=\lim(\frac{a_{n+1}}{a_n}\cdot a_n)=\lim(\frac{a_{n+1}}{a_n})\lim(a_n).$$ $$L=\lim(\frac{a_{n+1}}{a_n})L,$$ $$L(1-\lim(\frac{a_{n+1}}{a_n}))=0,$$ $$L=0 \vee \lim(\frac{a_{n+1}}{a_n})=1.$$ The second case - proves our goal instantly. Let us assume that $L=0$ . Let us assume in negation that $\lim(\frac{a_{n+1}}{a_n})>1$ (we would also need to prove the other case). Let us denote $\lim(\frac{a_{n+1}}{a_n})=D$ . Because the limit of $\lim(\frac{a_{n+1}}{a_n})$ exists then: $$\forall \epsilon >0. \exists n_0.\forall n>n_0 . \lim(\frac{a_{n+1}}{a_n}) \in (d-\epsilon,D+\epsilon).$$ Specifically for $\epsilon=\frac{D-1}{2}$ . So: $\exists n_0.\forall n>n_0. D-\frac{D-1}{2}<\frac{a_{n+1}}{a_n}$ $\exists n_0.\forall n>n_0. \frac{D+1}{2}a_n<a_{n+1}$ Now because $a_n$ converges to 0 then: $\forall \epsilon>0.\exists n_0. \forall n>n_0. a_n\in (-\epsilon,\epsilon)$ But because $\exists n_0.\forall n>n_0. \frac{D+1}{2}a_n<a_{n+1}$ for some $\epsilon$ for all $n_0$ we could find a $n>n_0$ that $a_n\notin (-\epsilon,\epsilon)$ in negation with the assumption that $a_n$ converges.","['calculus', 'solution-verification', 'sequences-and-series']"
4430076,Showing that $(a^2 - b^2)^2$ $ \ge $ $4ab(a-$ $b)^2$,"An inequality problem from Beckenbach and Bellman: Show that $(a^2 - b^2)^2 \ge 4ab(a-b)^2$ The given answer is simply Equivalent to $(a - b)^4 \ge 0$ I have tried two approaches, one which agrees with the given answer, and the other which does not. Approach one (agrees with the correct answer) \begin{align} 
(a^2 - b^2)^2 & \ge 4ab(a-b)^2\\ 
(a^2 - b^2)^2 - 4ab(a-b)^2 & \ge 0\\  
((a+b)(a-b))^2 - 4ab(a-b)^2 & \ge 0\\
(a+b)^2(a-b)^2 - 4ab(a-b)^2 & \ge 0\\
(a-b)^2((a+b)^2 - 4ab) & \ge 0 \\
(a-b)^2 (a^2 -2ab + b^2) &\ge 0 \\
(a-b)^2 (a-b)^2 & \ge 0\\
(a - b)^4 & \ge 0
 \end{align} Approach Two (differs from the answer) \begin{align}
(a^2 - b^2)^2 & \ge 4ab(a-b)^2\\
((a+b)(a-b))^2 & \ge 4ab(a-b)^2\\
(a+b)^2(a-b)^2 & \ge 4ab(a-b)^2\\
(a+b)^2 & \ge 4ab\\
(a^2 -2ab + b^2) &\ge 0 \\
(a-b)^2 & \ge 0
\end{align} Could someone point out where the second approach is going wrong?","['algebra-precalculus', 'inequality']"
4430087,Does a codimension 1 subspace of a representation of lie group intersect all orbits?,"Let $G$ be a nice lie groups, and $V$ a complex irred representation. I am interested in understanding for which codim 1 subspaces $U \subset V$ , we have $G \cdot U = V$ (does there exist such $U$ ?). Let me emphasize that in $G U = V$ on the left I mean pointwise, not the span (which correlates with the title of the question by moving $G$ to the right) Specifically in my case, I care about $SO(3)$ and its irreducible representations.","['representation-theory', 'lie-algebras', 'lie-groups', 'differential-geometry']"
4430161,A little help with Derangements,"In the textbook I'm using to train Olympic combinatorics Combinatorial problems in Mathematical competitions by Yao Zhang there is a problem which I personally found peculiarly interesting the Euler-Bernoulli misaddressed letter problem: How many ways to distribute $n$ distinct letters into $n$ envelops so that no letter gets to its corresponding envelop? At first I didn't get the points and they I tried different things of what I've learned, I couldn't go farther and eventually looked at the solution, but there are things I couldn't understand at all. The solution said that what we wanted to count was actually how many permutations of $\{1,2, \cdots, n\}$ there are such that $k$ is not at the $k^{th}$ place for any $1 \le k \le n$ . And that these permutations are called derangements. Also said that this kind of permutations are permutations with not fixed point. I know that this might be something not so difficult but I really get kind of confuse. Can someone tell with a little more detail what does it mean that stuff of permutations (With/without) fixed points? Going a little further in the solution we set as $S$ the set of all permutations of $\{1,2, \cdots , n\} $ and $A_{i}$ the set of permutations $\{a_{1},a_{2}, \cdots , a_{n}\} $ of $\{1,2, \cdots , n\} $ satisfying $a_{i} = i$ $( i = 1,2,3 ... n)$ . So, I can understand this because you can use The sieve formula to get exactly the the intersection of the complement of all the sets in which the $k_{th}$ permutation is in the $k_{th}$ place. What I can't understand very well is the cardinalities of $A_{i}$ 's sets, for example how do you know that $\lvert A_{i} \rvert = (n-1)!$ , $ \lvert A_{i} \cup A_{j} \rvert = (n-2)!$ or even that $ \lvert A_{i_{1}} \cup A_{i_{2}} \cup \cdots \cup  A_{i_{k}} \rvert = (n-k)! $ for $ ( 1 \le i_{1} < i_{2}< \cdots < i_{k} \le n)$ . I will show you the solution anyways, to help you understand better what I'm trying to ask for. Any help, idea or correction will be welcome! Thanks for your time and attention (:","['permutations', 'derangements', 'combinatorics']"
4430236,Finding the joint distribution of exponential random variable divide their sum,"Suppose $T_1, T_2, . . . , T_n$ is a sequence of independent, identically distributed random variables with the exponential distribution of the density function $$p(x)=\begin{cases}e^{-x},~~x\ge0;\\0,~~~~x<0.\end{cases}$$ Let $S_n = T_1 + T_2 + · · · + T_n$ . Find the distribution of the random vector $$V_n=\bigg\{\frac{T_1}{S_n},\frac{T_2}{S_n},\cdots,\frac{T_n}{S_n}\bigg\}$$ We know that $T_1$ and $S_n-T_1$ are independent, $S_n-T_1$ follows the Gamma distribution $\Gamma(n-1,1)$ , so we can calculate distribution of $\frac{T_1}{S_n}$ . Since all elements are iid, we know every elements' distributions. However, the sum of all elements is $1$ , so they are not  independent. My question is, how to present the distribution of $V_n$ , given that the sum of all elements in $V_n$ is $1$ ?","['probability-distributions', 'analysis', 'probability-theory', 'probability']"
4430269,"Partial derivative of a function of the form $g(x)=\int_V f(x,y)~dy$","Suppose $U,V$ are open boxes in $\Bbb R^n$ , $\Bbb R^m$ , respectively, and $f:U\times V\to \Bbb R$ is continuously differentiable. For $g:U\to \Bbb R$ defined by $g(x)=\int_V f(x,y)~dy$ , I am trying to compute $\partial g/\partial x_i$ $(i=1,\dots,n)$ . Note that $$ \dfrac{\partial g}{\partial x_i} (x)=\lim_{h\to 0}\dfrac{g(x+he_i)-g(x)}{h}=\lim_{h\to0} \frac{1}{h} \int_V f(x+he_i,y)-f(x,y)~dy , $$ so if we can interchange the integral and the limit, we get $\partial g(x)/\partial x_i= \int_V \partial_if (x,y)~dy$ . But can we interchange the integral and the limit in this case?","['integration', 'real-analysis', 'multivariable-calculus', 'limits', 'derivatives']"
4430302,Use injectivity of $\Bbb Z/n\Bbb Z$ over $\Bbb Z/n\Bbb Z$ to prove Prüfer's First Theorem,"I'm working on the part (b) of the exercise at P.67 of Brown's Cohomology of Groups, which is stated as follows: Let $R = \mathbb{Z}/n\mathbb{Z}$ . Show that $R$ is an injective $R$ -module. Deduce: (a) If $A$ is an abelian group such that $nA = 0$ , and $C\subseteq A$ is a cyclic subgroup of order $n$ , then $C$ is a direct summand of $A$ . (b) If $A$ is as in (a), then $A$ is a direct sum of cyclic groups. There are some proofs of (b) which is irrelevant to (a), i.e. in chapter 10 of ""Fundamentals of the Theory of Groups"" by Mikhail Ivanovich Kargapolov and Ju. I. Merzljakov, though I think these are not the type of proofs that Brown wants us to provide. After some searching, I find Wofsey's answer to be helpful, but I'm stuck at some point: (We may assume $n=p^k$ for some prime $p$ if necessary.) Since every cyclic subgroup of order $n$ of such $A$ is a direct summand, by transfinite induction, we may find a subgroup $B\subseteq A$ such that $B$ is the direct sum of copies $\mathbb{Z}/n\mathbb{Z}$ and that every element of $A\backslash B$ has order less than $n$ . The author claims that $B$ is a direct summand of $A$ , but I can't find out why it holds. I think it suffices to show the following statement, but I can't go any further. Statement Let $A$ be an abelian group, and let $\{B_m\}$ , $\{C_m\}$ , $\{C_m'\}$ be subgroups  of $A$ with $m\geq 1$ , such that $B_1\oplus C_1=A$ , $C_m = C_{m+1}\oplus C_{m+1}'$ for $m\geq 1$ , $B_{m+1}=B_m\oplus C_{m+1}'$ for $m\geq 1$ . Then $\bigcup_m B_m$ is a direct summand of $A$ . Any help is appreciated. P.S. This IS my homework assignment, but I've already submitted my answer. So grade is not relevant. Edit 1: Previous statement is wrong. If we take $A=\prod_{n=1}^{\infty}\mathbb{Z}$ , $B_m=\mathbb{Z}\times\dotsb\times\mathbb{Z}\times 0\times 0\times\dotsb$ ( $m$ copies of $\mathbb{Z}$ ), $C_m=0\times\dotsb\times 0\times\mathbb{Z}\times\mathbb{Z}\times\dotsb$ ( $m$ copies of $0$ ), $C_m'=0\times\dotsb\times 0\times\mathbb{Z}\times 0\times 0\times\dotsb$ ( $\mathbb{Z}$ appears as the $m$ -th term), then $\bigcup B_m=\bigoplus_{n=1}^\infty\mathbb{Z}$ is not a direct summand of $A$ according to this post .","['group-theory', 'abelian-groups', 'injective-module']"
4430412,"Continuous injective curve from $[0,1]$ to $\mathbb{R^2}$ containing the interval $[0,1]\times {0}$","I want to know if it is possible to have an injective continuous curve from $[0,1]$ to $\mathbb{R^2}$ whose image contains the interval $[0,1]\times \{0\}$ with $f(0)=(0,0)$ . There is one trivial solutions, that is, go straight across the interval: $f(x)= (x,0)$ . Let $f(t_f)=(1,0)$ . We say two solutions $f,g$ are different if $f([0,t_f])\neq g([0,t_g])$ . Does there exist an example different from the trivial solution? i.e. the curve leaves the interval at some point before finishing. If yes, please give a counter example. Otherwise, please put the solution in spoiler tags, I have been trying to solve this myself but I'm too worried there is a counter example. My current idea is to look at the sup $t_0=\{t: \exists s: f([0,t])= [0,s]\times \{0\} \}$ . Intuitively, to cover $(t_0+\epsilon,0)$ , we have to come back later. We find a sequence of epsilons converging to zero such that the time of return does not converge to $0$ . Then by continuity (+Bolzano weierstrass on epsilons) we get $f(\lim \epsilon_n)=(0,0)$ contradicting injectivity. Im having a hard time materialising the third sentence. edit: My solution:
Suppose $f$ is different from the trivial solution. Then we must have some $t'<t_f$ where we are not on the interval and so by continuity we can have an interval. Lets take a maximal one i.e. let $$t_0 := \inf \{t: \pi(f(t))\neq 0, t\le t' \} $$ $$t_1 := \inf \{t: \pi(f(t))=0 \cap t\ge t'\}$$ Then by continuity, $t_0\neq t_1$ $\pi_y(f(t_0)) = \pi_y(f(t_1))=0$ but $\pi_y(f(t)) \neq 0 $ for $t_0<t<t_1$ and by injectivity $\pi_x(f(t_0))\neq \pi_x(f(t_1))$ Some of the interval $(\pi_x(f(t_0)), \pi_x(f(t_1)))\times \{0\}$ may have been covered in $[0,t_0]$ , however, not all of it otherwise $f(t_1)$ is an intersection. What was covered is, by continuity, a closed set therefore the complement in the interval is $(\text{open set in }\mathbb{R}) \times \{0\} $ in particular we have a ""maximal"" subset $(a, \pi_x(f(t_1) ))\times \{0\}$ for which $(a,0)\in f([0,t_0])$ . $(a, \pi_x(f(t_1) ))\times \{0\}$ must be covered in $f[t_1,1]$ therefore by continuity $\exists t_2\in(t_1,1]$ with $f(t_2)=(a,0)$ , contradicting injectivity","['general-topology', 'analysis', 'real-analysis']"
4430432,Question on Chernoff bound type probability argument.,"The following result was given in the research article ( claim 6 ) and no justification was provided for the proof. I have presented the claim in simple terms below. Basically, I want to understand what theorems were used and how to prove the following two (To prove) parts. We have two devices $\mathscr{A}$ and $\mathscr{B}$ far apart. $\mathscr{A}$ takes input $x_i \in \{0,1,2\}$ and $\mathscr{B}$ takes input $y_i \in \{0,1\}$ . They both give random outputs $a_i, b_i \in \{0,1\}$ respectively for $i$ th input. Uniformly $m$ random input pairs are selected $$I=\{ (x_i,y_i) \mid x_i \in \{0,1,2\}, y_i \in \{0,1\} \text{ for } 1\leq i \leq m\}$$ and outputs are collected in the set $\mathbb{O}$ . Let $C$ denotes the set of inputs such that $(x_i,y_i) = (2,1)$ i.e., $C=\{ (x_i,y_i) \mid x_i = 2, y_i = 1 \}$ . At random for $0 < \gamma <1$ , a total of $\gamma m$ $\in \mathbb{N}$ inputs are selected from $I$ . Lets denote this set as $B$ . To Prove 1: The randomly chosen set $B$ contains a fraction of at least $\gamma /2$ fractions from the elements of the set $C$ , except with probability at most $e^{- \gamma |C| /8}$ . Let $C_B$ denotes the elements of $C$ in the set $B$ . If for $C_B$ the outputs satisfies $a_i \neq b_i$ for at most $\eta$ fractions of times. Then To Prove 2: With probability at least $1-e^{- \gamma |C| /200}$ the total fraction of rounds in $C$ such that $a_i \neq b_i$ is at most $1.1 \eta$ .","['cryptography', 'statistics', 'probability']"
4430492,"Find number of functions such that $f(f(x)) = f(x)$, where $f: \{1,2,3,4,5\}\to \{1,2,3,4,5\}$ [duplicate]","This question already has answers here : $f\colon\{1,2,3,4,5\}\longrightarrow\{1,2,3,4,5\}$ Find the total number of functions such that $f(f(x))=f(x)$ (2 answers) Closed 2 years ago . Consider set $A=\{1,2,3,4,5\}$ , and functions $f:A\to A.$ Find the number of functions such that $f\big(f(x)\big) = f(x)$ holds. I tried using recursion but could not form a recursion relation. I tried counting the cases manually, but it was too exhaustive and impractical. I tried drawing/mapping but that too included counting by making cases. Random variable was another approach I tried but couldn't make a summation. A general idea for such problems is needed. Thanks.","['functions', 'combinatorics', 'recursion', 'random-variables']"
4430513,Why does seperation of variables work to solve differential equations? [duplicate],"This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 2 years ago . When given this simple differential equation and a condition, $$\frac{dy}{dx}=e^{2x-y} \hspace{1em} , \hspace{1em} C_1 : y(0)=0$$ Using separation of variables, $$\frac{dy}{dx}=e^{2x}\cdot e^{-y} \Rightarrow \  \frac{dy}{e^{-y}}=e^{2x} \ dx$$ $$\int e^{y} \ dy= \int e^{2x} \ dx \Rightarrow \ e^{y} = \frac{e^{2x}}{2} +c $$ Solving for c , $$e^{0} = \frac{e^{2(0)}}{2} + c \ \Rightarrow 1 = \frac{1}{2} +c $$ $$ c = \frac{1}{2}$$ Solving for y , $$e^y = \frac{e^{2x}}{2} + \frac{1}{2} \ \Rightarrow e^y = \frac{1}{2}(e^{2x} + 1) $$ $$y = \ln\left(\frac{1}{2}\right) + \ln\left(e^{2x} + 1\right) $$ Finally, $$\boxed{y=\ln\left(e^{2x} + 1\right) -\ln(2)}$$ Question: In the above example we can see that the method of separation of variables indeed works in solving our differential equation, but why? What actually is going on when the method of separation of variables is used?","['integration', 'calculus', 'derivatives', 'ordinary-differential-equations']"
4430550,Does the limit $\lim_{x\to 0} \sqrt{x^3 - x^2}$ exist or not?,"I am having some arguments with a friend about the following limit: $$\lim_{x\to 0} \sqrt{x^3 - x^2}$$ FACTS: the domain of the function is $x\in \{0\}\cup [1,\ +\infty)$ and $0$ is an isolated point. My friend says the limit doesn't exist, whilst to me it is $0$ . Who is right? In my opinion, if we take $\epsilon > 0$ and $\delta = 1/2$ , we calculate $|f(x) - f(0)| < 0$ hence less than $\epsilon$ , so for any positive $\epsilon$ we have positive $\delta$ such that whenever $x$ is in $\delta$ -neighbourhood of $0$ , the quantity $|f(x) - f(0)|$ is less than $\epsilon$ so $f$ is continuous at $0$ . Am I right or wrong? Can someone pease make a limpid clarification of the existence (or not) of this limit?
Thank you in advance!","['epsilon-delta', 'limits-without-lhopital', 'analysis', 'calculus', 'limits']"
4430593,Can this be solved by FTC2?,"Question: let $f$ be differentiable and defined everywhere: simplify $$ \frac{d}{dx}  \left( \int_{0}^{x}   tf(x^2-t^2) \ dt\right)      $$ I'm confused becasuse if I attempt to use FTC2, it tells me that $$ xf(x^2-x^2) = xf(0)$$ which is obviously wrong. I may have violated the FTC2 definition; can anyone tell me how to use FTC2 to solve this problem.","['integration', 'calculus', 'derivatives']"
4430594,"Winning strategy for a game, solution verification","Let $n \in \mathbb{N}$ . Two players play a game. Both players have to write a $0$ or a $1$ each move. A player loses if his last number created a sequence of length $n$ that already existed (also if both positions overlap). A game for $n=4$ could look like this: $00100001101011110011$ (Player $2$ lost because  of $0011$ ). I have to show that player $2$ has a winning strategy for uneven $n$ . There are $2^n$ possible sequences. (For $n=1: 0, 1$ ; for $n = 3: 000, 111, 001, 100, 010, 110, 101, 011; ... $ ) Player $1$ has to make every uneven move ( $1, 3, 5, ...$ ). The first sequence will be created after $n$ moves and player $1$ will always create the first sequence if $n$ is uneven. Every move after that will create another sequence. $2^n$ is even and player $2$ has to make every even move, this means that he will always win. Is this already enough? I'm not sure about it but I also don't know how to continue. I think that player $2$ always wins if he's doing the opposite of player $1$ 's last move. I played a few games against myself with $n=3$ but I don't know how to generalize it, $01011001, 01101001, 01100101$","['game-theory', 'solution-verification', 'discrete-mathematics', 'combinatorial-game-theory']"
4430595,Stable vector bundles on an algebraic curve,"Let $E$ be a stable bundle on a smooth curve of genus $g$ . Assume $\chi(E)\leq 0$ or equivalently, $E$ has slope $\leq g-1$ . Is there a line bundle $L$ of degree 0 such that $H^0(E\otimes L)=0$ ?","['algebraic-curves', 'algebraic-vector-bundles', 'curves', 'vector-bundles', 'algebraic-geometry']"
4430640,Intuition of contour integration of |z| [duplicate],"This question already has answers here : What is a geometric explanation of complex integration in plain English? (5 answers) Closed 2 years ago . Given the following contour integral: $$ I=\int\limits_{-1}^1 |z| \ \mathrm{dz}, $$ with path of integration being the upper half of unit circle, it can be parameterized to give: $$ \int\limits_{-1}^1 |z| \ \mathrm{dz} = -\int\limits_\Gamma |z| \ \mathrm{dz}, $$ where $\Gamma = e^{it}, \ t\in[0,\pi]$ and $\mathrm{dz}=ie^{it}\ \mathrm{dt},$ so $$ -\int\limits_\Gamma |z| \ \mathrm{dz} = -\int\limits_0^\pi |e^{it}|\ ie^{it} \ \mathrm{dt}
= -i\int\limits_0^\pi e^{it} \ \mathrm{dt} = -i\left[ -ie^{it} \right]_0^\pi = -i^2-i^2 = 2. $$ Since $f:\Gamma \to \mathbb{R} \ $ given by $f(z)=|z|$ is a real-valued function of a complex variable, it can be visualized in 3D. And interpreting integral as ""area under curve"" it would seem natural to assume that $I=\pi\ $ which is incorrect. The question is: what's wrong with this intuition and what's the correct way to think about the geometry of these types of integrals?","['integration', 'complex-analysis', 'complex-integration']"
4430669,"Proof suggestion for ""The chromatic number of a $k$-degenerate graph is inferior or equal to $k + 1$""","I have developed a proof by induction but I don't feel confident in its validity: Base case ( $n = 1$ ) If $G$ only contains one vertex, then its degree is $0$ , then $k = 0$ and G is $1$ -colorable. This is true since only one colour is required. Inductive hypothesis: If $P(n)$ is true, then $P(n+1)$ is also true. Induction step: Assume $G$ is a $k$ -degenerate graph of $n + 1$ vertices. Then, $G$ constains at least one vertex $v$ where $deg(v) \le k$ .  If we remove this vertex, we are left with a graph $G - v$ which by definition of degeneracy, is still $k$ -degenerate. Therefore, by our assumption $P(n)$ , $G - v$ is ( $k + 1$ )-colourable. Now, we add $v$ back to the graph $G - v$ . Since $deg(v) \le k$ , $v$ has as at most $k$ neighbours, this leaves one more colour to use.
Thus we have proved by induction that a $k$ -degenerate graph is ( $k + 1$ )-colorable.","['graph-theory', 'solution-verification', 'coloring', 'discrete-mathematics']"
4430670,Limit of sum/difference condition which are not being followed here,"This was given as an example in a book : If $$\lim _{x \rightarrow a}[f(x)+g(x)]=2$$ and $$\lim _{x \rightarrow a}[f(x)-g(x)]=1,$$ then find the value of $$\lim _{x \rightarrow a} f(x) g(x)$$ Solution : $$\lim _{x \rightarrow a}[f(x)+g(x)]=2$$ or $$\lim _{x \rightarrow a} f(x)+\lim _{x \rightarrow a} g(x)=2$$ $\tag{1}$ $$\lim _{x \rightarrow a}[f(x)-g(x)]=1$$ or $$\lim _{x \rightarrow a} f(x)-\lim _{x \rightarrow a} g(x)=1$$ $\tag{2}$ Adding (1) and (2), $$2 \lim _{x \rightarrow a} f(x)=3$$ or $$\lim _{x \rightarrow a} f(x)=\frac{3}{2}$$ Subtracting (2) from (1), $$2 \lim _{x \rightarrow a} g(x) \neq 1$$ or $$\lim _{x \rightarrow a} g(x)=\frac{1}{2}$$ or $$\lim _{x \rightarrow a} f(x) g(x)=\lim _{x \rightarrow a} f(x) \lim _{x \rightarrow a} g(x)=\frac{3}{2} \times \frac{1}{2}=\frac{3}{4}.$$ My query is that isnt the lim of sum = individual limits sum only when we  already know that limit of individual sum  exists ? Here we actually dont know if it actually exists or not so shouldnt this method is totally wrong ?","['limits', 'calculus']"
4430689,"Closed form for $\sum_{n=2}^{\infty} \big{(}H_{n,n}-1\big{)} $?","It is known that $$\sum_{n=2}^{\infty}\big{(}\zeta(n)-1\big{)}=1 .$$ Many more series like these, called rational zeta series, can be evaluated in closed form. I wonder if we can also obtain similar results for series involving rational sums of generalized harmonic numbers. Such numbers are defined as: $$H_{n,m} := \sum_{k=1}^{n} \frac{1}{k^{m}} .$$ So they form a finite analogy of zeta values, because $\lim_{n \to \infty} H_{n,m} = \zeta(m). $ Question : can a closed form of the series $$\sum_{n=2}^{\infty} \big{(}H_{n,n}-1\big{)} \approx 0.561 $$ be obtained? And are any results on ""rational generalized harmonic series"" known?","['harmonic-numbers', 'analysis', 'sequences-and-series']"
4430706,$\lim_{s \to 1^+} 1/\zeta(s) = 0$ obvious or not?,"I read the statement that $$\frac{1}{\zeta(s)} = \sum_{n=1}^\infty \frac{\mu(n)}{n^s} \textrm{ for } \Re(s) > 1 \qquad (*)$$ In fact I can guess what the proof is: just expand both $\zeta$ and the right hand side of (*) as an Euler product, use $\Re(s) > 1$ to handwave away any concerns of changing the order of taking limits and taking products and conclude that the outcome of multiplying the right hand side of (*) by $\zeta(s)$ is the product of infinitely many $1$ 's, hence 1 itself. So far so good. Now suppose I wanted to evaluate the the limit $s \to 1^+$ of the right hand side of ( * ) , or in layman's terms, compute the sum $\sum_{n=1}^\infty \frac{\mu(n)}{n}$ . From (*) I would quickly conclude that $$\sum_{n=1}^\infty \frac{\mu(n)}{n} = 0 \qquad (**)$$ since $\zeta(1) = \sum_{n=1}^\infty \frac{1}{n}  = \infty$ (by the standard 'powers of 1/2'-argument) and, as we all know, $1/\infty = 0$ . Very nice, very elementary, or... so it seems. However the statement (**) is far from elementary: it is equivalent to the prime number theorem! (If you find that equivalence surprising: so did I. I asked a question about it three years ago, but the current question is about a different type of surprise that arises if we take this equivalence as given.) We all know that the PNT is hard to prove. So what is wrong with simply taking the the limit $s \to 1$ in (*) as I did above? Am I secretly interchanging two limits where that is not allowed? Where am I oversimplifying things? I have a very vague feeling what is going on here (but perhaps I am completely off, so correct me if I am wrong) and that is that there is some weird theorem lurking in the background that states that we can only extend the equation (*) to the point $s = 1$ if and only if we can extend it to the entire line $\{s \colon \Re(s) = 1\}$ . But what kind of weird theorem would that be? It goes again my nearly lifelong experience that one can do mathematics just fine without even realizing that complex numbers off the real line exist.","['complex-analysis', 'riemann-zeta', 'mobius-function']"
4430785,Proof that the set of discontinuous points of a Riemann integrable function has measure zero,"This comes from proving Lebesgue's Theorem for Riemann integrability as specified in Abbott's Understanding Analysis , Theorem 7.6.5. The theorem is as follows: Let $f$ be a bounded function defined on the interval $[a, b]$ . Then, $f$ is Riemann-integrable if and only if the set of points where $f$ is not continuous has measure zero. I've followed along with the exercises and have been able to prove the <= direction. I've been stuck on one core point in the proof of the => direction. The specific area I am struggling with is as follows. The proof starts: Let $\epsilon > 0$ be arbitrary, and fix $\alpha > 0$ . Because $f$ is Riemann-integrable, there exists a partition $P_\epsilon$ of $[a, b]$ such that $U(f, P_\epsilon) - L(f, P_\epsilon) < \alpha \epsilon$ . Prove that $D^\alpha$ has measure zero. Where $D^\alpha = \{ x \in [a, b] : f \textrm{ is not }\alpha\textrm{-continuous at }x\}$ . Edit: Including definition of $\alpha$ -continuous: Let $f$ be defined on $[a, b]$ , and let $\alpha > 0$ . The function $f$ is $\alpha$ -continuous at $x\in[a, b]$ if there exists $\delta > 0$ such that for all $y, z \in (x - \delta, x + \delta)$ it follows that $| f(y) - f(z) | < \alpha$ . If $f$ is uniformly $\alpha$ -continuous on $[a, b]$ , there is a single $\delta > 0$ such that $| x- y | < \delta \implies |f(x) - f(y)| < \alpha$ . It follows that if $f$ is $\alpha$ continuous at every point on a compact set $K$ , then $f$ is uniformly $\alpha$ continuous on $K$ . My attempt so far has taken me down the path of writing: $$U(f, P_\epsilon) - L(f, P_\epsilon) < \alpha \epsilon$$ $$\sum_k (M_k - m_k)\Delta x_k < \alpha \epsilon$$ Where $M_k$ and $m_k$ are the supremum and infimum, respectively, of $f$ on the interval $[x_{k-1}, x_k]$ . Then I attempt to split the intervals into those that contain points in $D^\alpha$ and those that do not. Since the intervals are compact, those intervals containing no points in $D^\alpha$ are uniformly $\alpha$ continuous, which bounds $M_k - m_k < \alpha$ on those intervals (this only holds within some $\delta$ , but we can refine the intervals to have smaller length than $\delta$ and I believe this point works out fine). From here I'm at a bit of a loss, I can't figure out how to take this argument further. I feel that I'm likely missing something relative obvious, so I'd appreciate any nudging in the right direction.","['integration', 'measure-theory', 'real-analysis']"
4430811,Cohomology group homomorphism induced by maps of spheres [duplicate],"This question already has an answer here : Compute $f^*: H^n(S^p \times S^q) \to H^n(S^n).$ (1 answer) Closed 2 years ago . Suppose I have a continuous map $f:S^n \rightarrow S^p \times S^q$ , where $n = p+q$ . Is it possible to say what the induced homomorphism in cohomology groups $f^{\ast}:H^n(S^p \times S^q;R) \rightarrow H^n(S^n;R)$ has to be for an arbitrary coefficient ring $R$ ? I'm not quite sure how to approach answering this. I know that each of the groups here is isomorphic to $R$ ; the temptation to use cup products to make a conclusion about the cohomology rings, using that we have cohomology ring $H^{\ast}(S^p \times S^q;R) \cong R[\alpha,\beta]/(\alpha^2,\beta^2)$ for $|\alpha| = p$ and $|\beta| = q$ , for example, seems to be a more difficult approach than what's needed here. Any thoughts would be appreciated. Thanks!","['ring-theory', 'abstract-algebra', 'group-theory', 'homology-cohomology', 'algebraic-topology']"
4430813,"Can ($X^I$, product topology) and ($X^I$, box topology) be homeomorphic for some nontrivial $X$ and infinite $I$?","Let $X$ be a nontrivial topological space, $I$ be a infinite set, we can endow $X^I$ (the set of all functions $I\to X$ ) with either the product topology or the box topology. We know that the box topology is strictly finer than product topology, but since a topology can be homeomorphic to a strictly finer topology, can the product topology be homeomorphic to the box topology anyway?","['general-topology', 'box-topology', 'product-space']"
4430820,Composing the empty function with itself,"Suppose $X = Y = \emptyset$ and $f: X \to Y$ is a map of sets. Then $f$ is vacuously bijective: for every $a,b \in X$ (there are none), $f(a) = f(b)$ implies $a = b$ . Also, for every $y \in Y$ (there are none again), there is $x \in X$ such that $f(x) = y$ . I believe it is a convention that $f$ is its own inverse. I need an inverse of $f: X \to Y$ to take the form $g: Y \to X$ , as a general rule, but in this case that is $g: \emptyset \to \emptyset$ . The empty function in this list is the empty list. Is there a notion of composing the empty function with itself? In particular, if the claim is that $f$ is its own inverse, can I say $$
f \circ f = \mathrm{id}_X = \mathrm{id}_Y. 
$$ I'm not fully sure what the identity function for the empty set is. Do I need such a notion?","['proof-explanation', 'functions']"
4430830,Hartshorne IV Exercise 6.9: How to show $Y$ is contained in the hyperplane section?,"The exercise is stated as follows: 6.9 . Let $X$ be an irreducible nonsingular curve in $\mathbf{P}^{3}$ . Then for each $m\gg0$ , there is a nonsingular surface $F$ of degree $m$ containing $X$ . [Hint: Let $\pi: \tilde{\mathbf{P}} \rightarrow \mathbf{P}^{3}$ be the blowing-up of $X$ and let $Y=\pi^{-1}(X)$ . Apply Bertini's theorem to the projective embedding of $\tilde{\mathbf{P}}$ corresponding to $\mathscr{I}_{Y} \otimes \pi^{*} \mathcal O_{\mathbf P^3}(m)$ .] By II Proposition 7,10 we know that there is a $m>0$ such that $\mathscr{I}_{Y} \otimes \pi^{*} \mathcal O_{\mathbf P^3}(m)$ is very ample, hence we can get a projective embedding $\theta:\tilde {\mathbf P}\to \mathbf P^N$ for some $N$ . Apply Bertini's theorem I can get a hyperplane $H$ in $\mathbf P^N$ such that $H\cap \tilde{\mathbf P}$ is irreducible and nonsingular. But how can I show that $Y \subset H\cap \tilde{\mathbf P}$ ? I know that $\mathscr I_Y$ can be identified with $\mathcal O_{\tilde{\mathbf P}}(1)$ , and the ideal sheaf of $H\cap \tilde{\mathbf P}$ in $\tilde{\mathbf P}$ can be identified with $\theta^{-1}(\mathcal O_{\mathbf P^N}(-1))$ . The only relationship of these invertible sheaves is $$\theta^*(\mathcal O_{\mathbf P^N}(1))=\mathscr{I}_{Y} \otimes \pi^{*} \mathcal O_{\mathbf P^3}(m).$$ What I need to show is $\theta^{-1}(\mathcal O_{\mathbf P^N}(-1))\subset \mathscr I_Y$ , but how? Any help is welcome, thanks!",['algebraic-geometry']
4430848,What is the best way to measure similarity between two histograms,"What is the best way to measure the similarity between two histograms? For example, in the following pictures, how can I tell if the distributions are similar enough? I now have 2 lists of values, and I've normalized them to fall between 0 and 1. I've tried multiple statistical tests including Pearson, Spearman, and Kolmogorov-Smirnov and it looks like Spearman is the best test to use. However, the Spearman is not consistent all the time, it sometimes gives me a high ""s"" value but the shapes of the distribution are not similar enough. In theory, a higher (positive) ""s"" means the values are strongly correlated. Am I even on the right track using correlation to measure similarities? Are there any other tests that can be used to do this? corr0.89_1 = [10.7441, 8.9568, 11.0018, 9.29803, 8.92043, 8.78492, 13.5503, 6.74334, 6.14392, 5.75271, 28.851, 26.8173, 6.52642, 6.56071, 5.7169, 7.3095, 6.36379, 5.74984, 7.10243, 5.87364, 11.2827, 2.94984, 2.84836, 22.8551, 24.8372, 10.6571, 9.7891, 11.3021, 5.89328, 10.1372, 24.0525, 3.49401, 2.16394, 11.2825, 11.6859, 7.9918, 13.2742, 11.1194, 2.49575, 16.733, 27.918, 3.27145, 14.3346, 20.4979, 13.0808, 13.6282, 14.1474, 25.0414, 8.06032, 280.803, 22.0135, 18.2725, 12.9601, 7.64593] corr0.89_2 = [9.14167, 6.30561, 7.7479, 8.05475, 7.14188, 7.62774, 9.18454, 1.48037, 1.5912, 2.07612, 21.302, 22.7082, 2.67858, 2.25732, 1.74804, 2.04191, 2.03539, 1.78882, 2.57568, 1.6512, 8.62473, 2.99236, 3.13484, 13.014, 16.2016, 9.17172, 7.97379, 9.12539, 4.8298, 8.42477, 16.0582, 2.68252, 1.92429, 5.6744, 4.70516, 5.20169, 11.0945, 9.10398, 2.68375, 13.6299, 17.3429, 3.19181, 9.41762, 12.2805, 9.92005, 11.5985, 11.7269, 17.4832, 6.66996, 60.8647, 13.9616, 14.9909, 10.4712, 6.13891] corr1.0_1 = [0.00905783, 0, 0.0075662, 0, 0.00583336, 0, 0.0101741, 0.00617847, 0.00474902, 0, 0.0243326, 0.0300779, 0.0062144, 0.00581433, 0, 0.00712057, 0.00703617, 0, 0, 0, 0.0101258, 0.00844863, 0.014988, 0.0248553, 0, 0.00680134, 0.00762619, 0.00701553, 0.0106525, 0.00425654, 0.0160354, 0, 0, 0, 0, 0, 0.0110151, 0.00874536, 0, 0.0182528, 0.0291939, 0, 0.0426431, 0.0141304, 0.0139076, 0.0182638, 0.0177141, 0.021119, 0, 12.3977, 0.0121492, 0.016053, 0.0148212, 0.00767271] corr1.0_2 = [0.128504, 0.119172, 0.0403692, 0.148327, 0.132162, 0.0366454, 0.139191, 0.0464803, 0.0235099, 0.0333772, 0.0427275, 0.0510047, 0.0278845, 0.0202271, 0.0918039, 0.129276, 0.0266636, 0.0399166, 0.693549, 0.131911, 0.134276, 0, 0, 0.248764, 0.17239, 0.0450586, 0.0932654, 0.0671463, 0.239433, 0.102551, 0.378029, 0.031807, 0.0181028, 0.107356, 0.145449, 0.0735069, 0.788291, 0.496569, 0.0209139, 0.0983066, 0.0530917, 0.0755444, 0.25198, 0.550969, 0.172254, 0.104131, 0.113987, 0.548016, 0.302768, 126.145, 0.886364, 0.107977, 0.4037, 1.23249]","['statistics', 'correlation']"
4430849,Find the derivative of the following function w.r.t $x:$ $y=(\sin x)^{(\cos x)^{(\cos x)^{(\cos x)^{\cdots\infty}}}}$,"Find $\frac{dy}{dx},$ if $y=(\sin x)^{(\cos x)^{(\cos x)^{(\cos x)^{\cdots\infty}}}}$ My attempt: If the base and power were the same function,then we could find it's derivative with respect to $x$ as $y=x^y.$ But,here in the question, $\sin x$ and $\cos x$ are two different functions.So,I have no idea how to differentiate it with respect to $x.$ Also,I don't know if this function exist or not.If exist,then find its derivative. Any help would be appreciated.Thank you!","['calculus', 'derivatives']"
4430854,Midpoint iterative ellipse,"A set of ordered points $(x_i, y_i)$ on the plane are connected end to end, and each iteration takes the midpoint and connects to form a new polygon. $$
(x_{k,i}, y_{k,i})=\left(
\frac{x_{k-1,i}+x_{k-1,i}}{2}, 
\frac{y_{k-1,i+1}+y_{k-1,i+1}}{2}
\right)
$$ The last point needs to be connected with the first point to calculate the midpoint: $$
(x_{k,n}, y_{k,n})=\left(
\frac{x_{k-1,n}+x_{k-1,n}}{2}, 
\frac{y_{k-1,1}+y_{k-1,1}}{2}
\right)
$$ After enough iterations, the graph approximates an ellipse. How to find the semi-major and semi-minor axes $a, b$ and the rotate angle $\alpha$ of this ellipse? Is it possible to calculate the shape parameters of the final ellipse directly from the initial points without iteration? Update 1 A similar transformation is required to zoom in when drawing, to prevent the precision from being reduced to zero, the code is as follows. next[{xs_, ys_}] := Block[
    {x, y},
    x = ListConvolve[{1 / 2, 1 / 2}, xs, -1];
    y = ListConvolve[{1 / 2, 1 / 2}, ys, -1];
    (* Move to origin, won't change the shape *)
    x = x - Mean[x];
    y = y - Mean[y];
    (* Similarity transformation prevents exponential shrinking *)
    {x, y} / Max[Max[x] - Min[x], Max[y] - Min[y]]
];
drawPoints[this_] := Graphics[
    {
        PointSize[0.02], Blue, Point /@ this,
        Black, Line@Join[this, {First@this}]
    },
    PlotRange -> 0.6,
    ImageSize -> {300, 300}
];
drawAnimation[points_, nests_] := Block[
    {seq, w = 2, h = 1},
    seq = Transpose /@ NestList[next, {RandomReal[{-w, w}, points], RandomReal[{-h, h}, points]}, nests];
    drawPoints /@ Rest[seq] // ListAnimate
];

drawAnimation[25, 200]","['conic-sections', 'geometry', 'computational-geometry']"
4430859,Generator of doubling map when calculating its Measure-theoretic entropy,"I saw an example to show how to calculate the Measure-theoretic entropy, and here is the example: Let $T:X\to X$ be the doubling map $T(x)=2x\;(mod\;1)$ and there is a partition $\alpha=\{[0,\frac{1}{2}),[\frac{1}{2},1)\}$ $\lor_{i=0}^{n-1}T^{-i}\alpha=\{[\frac{i}{2^n},\frac{i+1}{2^n})\,:i=0,...2^n-1\}$ and the example just say $\alpha$ is the generator, then calculate that the result is log2. What I don't unstand is that why $\alpha$ is a generator for T ? According to Kolmogorov-Sinai Theorem, is this $\lor_{i=0}^{\infty}T^{-i}\alpha$ generating $\sigma$ algebra of [0,1)? I think it's not, so I don't unstand what kind of $\sigma$ algebra it generates and why $\alpha$ can be a generator. I suspect the problem is my thought about X, maybe X is not [0,1)?","['measure-theory', 'ergodic-theory', 'entropy', 'dynamical-systems']"
4430895,Groups of the form $(\Bbb Z_{13} \times \Bbb Z_7)\rtimes \Bbb Z_3$,"Consider the semi-direct product: $(\Bbb Z_{13} \times \Bbb Z_7)\rtimes \Bbb Z_3$ To construct a group $G$ , we need homomorphisms $\theta$ : $\Bbb Z_3 \rightarrow \text{Aut}(\Bbb Z_7)$ and $\theta_2$ : $\Bbb Z_3 \rightarrow \text{Aut}(\Bbb Z_{13})$ . Consider the set of coordinates $(x,y,z)$ with $x\in \Bbb Z_{13}$ , $y\in \Bbb Z_7$ , and $z\in \Bbb Z_3$ . From the homomorphism, we can define a group law $(x,y,z) * (x_2,y_2,z_2) = (x+\theta_2(z)(x_2), y+\theta(z)(y_2), z+z_2)$ . More specifically, consider the groups given by their operations: $G_1 = (x,y,z) * (x_2,y_2,z_2) = (x+x_23^{z}, y+y_21^{z}, z+z_2)$ . $G_2 = (x,y,z) * (x_2,y_2,z_2) = (x+x_23^{z}, y+y_22^{z}, z+z_2)$ . $G_3 = (x,y,z) * (x_2,y_2,z_2) = (x+x_23^{z}, y+y_24^{z}, z+z_2)$ . It is clear why $G_1$ is not isomorphic to $G_2$ or $G_3$ : $G_1$ has $156$ elements of order $21$ , whereas the other two groups have no such elements of order $21$ . I am struggling to see why $G_2$ and $G_3$ are non-isomorphic? If two groups have the same number of elements of any given order, doesn't that imply they are isomorphic? Both $G_2$ and $G_3$ have $182$ elements of order $3$ , $6$ of order $7$ , $12$ of order $13$ , and $72$ of order $91$ .","['semidirect-product', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
4430926,"Possibility that all lights $\mathbf{X}=(X_1,X_2,\cdots)$ turn off again with every time turn a light with its number $n\sim\text{geom}(\frac{1}{2})$.","Problem: Let $\mathbf{X} = (\mathbb{Z}_2)^\mathbb N$ , i.e., $\mathbf{X} = (X_1,X_2,\cdots,X_N,\cdots)$ , $X_i\in \{0,1\} $ . It can be considered as countable lightbulbs. $0$ means off, $1$ means on. We start with $\mathbf{X}_0 = 0$ . Keep generating independent geometric random variables, whose distribution are $geom(1/2)$ . Denote them as $K_1, K_2,\cdots$ . Now let $\mathbf{X}_m$ (for $m \ge 1$ ) be as follows $$(\mathbf{X}_m-\mathbf{X}_{m-1})_k = \mathbf{1}(k = K_m), $$ i.e, in the $m$ -  th turn, we only change the status of the $K_m$ -th light bulb. Then what is the probability of all lights being off again, i.e., $$\mathbb P(\exists m>1, \mathbf{X}_m =0)$$ My first intuition below was wrong I’m afraid. My intuition is that since $1/4+1/16+1/64+\cdots=1/3$ , the final answer might be $1/2$ . Since all lights being off means all lights are encountered even times, I tried to use generating function but it doesn't work since it's hard to derive a correspondence between all lights are encountered even times and generating function coefficients, and brute force calculation seem also hard due to the same reason that there're too many cases to consider, and I'm stuck here. Are there any other thoughts to deal with this question? Thanks! New intuition: We may define a function $f(x_1,x_2,\cdots)$ as the probability that status $\mathbf{X}=(x_1,x_2,\cdots)$ will eventually become all $0$ , then our goal is to calculate $\sum_{n=1}^{\infty}\frac{1}{2^n}f(0,\cdots,0,1,0,\cdots)$ where $1$ only appears on $n$ -th term, and we can achieve all the transporting equations of $f$ , which is an uncountable dimensional equation system. We can see immediately that the equation system has a solution $f(x_1,x_2,\cdots)\equiv 1$ . Can we conclude that this solution is unique (Then the answer to this problem is $1$ )? Continue the intuition above, we define $$g(x_1,x_2,\cdots)=f(x_1,x_2,\cdots)-1,$$ moreover we define $g(0,0,\cdots,0,\cdots)=0$ . Obviously $f\le1$ , hence $g$ can achieve a maximum at $(0,0,0,\cdots,0,\cdots)$ . Note that after changing all the equations involving $f$ into $g$ , the constant term will be cancelled and the coefficients at the righthand side is strictly larger than $0$ and the sum of all coefficients are always $1$ . For example, the equation $$f(1,0,0,\cdots,0,\cdots)=\frac{1}{2}+\frac{1}{4}f(1,1,0,\cdots,0,\cdots)+\frac{1}{8}f(1,0,1,\cdots,0,\cdots)+\cdots$$ is changed into $$g(1,0,0,\cdots,0,\cdots)=\frac{1}{2}g(0,0,0,\cdots,0,\cdots)+\frac{1}{4}g(1,1,0,\cdots,0,\cdots)+\frac{1}{8}g(1,0,1,\cdots,0,\cdots)+\cdots$$ We want to use the maximum principle, but we lack an equation with $\text{LHS}=g(0,0,0,\cdots,0,\cdots)$ , are there any ways to supple this equation? Edit: Another possible intuition from here: ignore $g(0,0,0,\cdots,0,\cdots)$ , we only focus on the remaining term. We wish to discard the term that has infinity $1$ since the probability of getting this term is $0$ . If the maximum happens on one remaining element we're done, what we don't want is if the maximum happens on the limiting term, and we wish to prove that this won't happen because the limit is (somewhat?) decaying. (I don't know how to proceed here, but I'll post my intuitions these days anyway)","['markov-chains', 'analysis', 'markov-process', 'probability-theory', 'probability']"
4430987,How far does one have to zoom into an image that was rotated a certain amount of degrees in order to only see only the original pixels again?,"This question was asked by a work colleague of mine, but my days as a mathematician are long gone unfortunately. It does sound like a pretty basic geometry problem to me, doesn't it? I'm not expecting an extremly detailed answer here, but does anyone have any resources on this? I'm pretty sure this was already answered somewhere. Please let me know if the question is formulated too vaguely. Thanks a lot in advance! For clarification: I imagine that the original image looks something like this: And then it gets rotated like this: Only the red pixels are considered the ""original pixels"" while the white pixels are not. So the question would be how far do I have to zoom into the second picture in order to only see red pixels again?","['geometry', 'rotations']"
4431062,Symmetric Group of an amorphous set,"An amorphous set is an infinite set, which is not the disjoint union of two infinite sets. The existence of such sets is consistent with ZF. I am wondering, if there are any interesting remarks to be made about the structure of $G:=\operatorname{Sym}(A)$ , the symmetric group on some amorphous set $A$ . In particular, do we know something about the ""cardinality"" (in a ZF-sense) or a minimal generating set of $G$ ? One trivial thing I was able to notice is that any bijection in G either has finite or cofinite support. And at least those of finite support allow for a generating set of size $A$ , namely those permutations $(1\,\; a)$ where $1$ is some fixed element of $A$ and $a$ is arbitrary.","['symmetric-groups', 'group-theory', 'axiom-of-choice', 'set-theory']"
4431132,What is the minimum number of lego pieces required to complete a NxN maze and also what is total nummber of corresponding configurations?,"There should be only one path between any two empty cells and any empty cell should be reachable from any other empty cell. (in all of the following examples ""x"" denote lego position and ""0"" denote empty cell) For example for N=2, minmum number of lego required is one 0 0
0 x or 0 0
x 0 or 0 x
0 0 or x 0
0 0 So for N=2, answer is  minimum of 1 lego and 4 configurations. For N=3, some of the configurations are 0 0 0
0 x 0
0 0 x or 0 0 0
0 x x
0 0 0 or 0 0 0
x 0 x
0 0 0 there are 7 more of such configurations in total(note the reflections and rotations of above 3 can generate the remaining 7 configurations).
So for N=3 answer is  minimum of 2 legos and 10 configurations. What is the answer of N=13? My initial thought was to find if there is any pattern in the series, the series for configurations generated for N=1 to 5 are 0,4,10,32,22 but searching OEIS didn't give me anything useful, on the other hand the series for minimum 0,1,2,4,6 is too common. Edit - This is a computer science problem. The problem setter don't expect a $O(1)$ formula. An algorithmic approach better or equal to $O(N^6)$ time should suffice.","['graph-theory', 'combinatorics', 'recurrence-relations']"
4431158,Convolution inequality $\|f\star g\|_p \le \|f\|_1 \|g\|_p$ on locally compact group,"Consider the following fragment from Folland's book ""A course in abstract harmonic analysis"". Here $G$ is a locally compact Hausdorff group and the $L^p$ -spaces are considered w.r.t. a left Haar measure on $G$ . The proof of proposition 2.40 applies Minkowski's integral inequality, however the proofs of Minkowski's integral inequality I know make crucial use of the fact that the measure spaces involved are $\sigma$ -finite. However, $G$ with the left Haar measure is $\sigma$ -finite if and only if $G$ is $\sigma$ -compact, which is not an assumption here. How can I solve this technicality? Perhaps I can make use of the fact that if $f \in L^1(G)$ , then $f$ vanishes outside a $\sigma$ -finite set and similarly for $g \in L^p(G)$ with $p < \infty$ . Can someone fill in the details? And shouldn't the case $p=\infty$ be dealt with on its own?","['harmonic-analysis', 'measure-theory', 'fourier-analysis']"
4431263,$|f(x)-f(y)|\leq \|x-y\|^2$ implies $f$ is constant,"Let $f:\Bbb{R}^2\to \Bbb{R}$ be such that for all $(x,y) \in (\Bbb{R}^2)^2$ we have that $|f(x)-f(y)|\leq \|x-y\|^2$ . I need to show that $f$ is constant. First, by hypothesis we have that $\lim_{x\to y}\frac{|f(x)-f(y)|}{\|x-y\|}=0$ which implies that $f$ is differentiable (Taylor expension of order 1) and that $Df(y)=0 \ \forall y \in \Bbb{R}^2$ . Now by mean value theorem considering $[x,y]$ we have that there is $c\in ]x,y[$ such that $f(x)-f(y)=Df(c)(x-y)=0\implies f(x)=f(y)$ . As $x$ and $y$ are arbitrary we conclude that $f$ is constant. Is it correct? Thank you in advance.","['multivariable-calculus', 'derivatives']"
4431306,"When $A$ is skew-symmetric, $Q = e^{A t}$ is orthogonal, i.e. $Q^T Q = I$.","When $A$ is skew-symmetric, then $Q = e^{A t}$ is orthogonal or $Q^T Q = I$ . (For a linear autonomous system $\dot{x} = A x$ , the fundamental matrix of the system is $\Phi(t) = e^{A t}$ .) To prove this identity, I make use of the result $$
\left( e^{P} \right)^T = e^{P^T} \tag{1}
$$ for any matrix $P$ . Since $e^P$ can be expressed as the uniformly convergent series, $$
e^P = \sum\limits_{k = 0}^\infty \ {P^k \over k!},
$$ it follows that $$
\left( e^P \right)^T = \left( \sum\limits_{k = 0}^\infty \ {P^k \over k!} \right)^T
= \sum\limits_{k = 0}^\infty \ {(P^k)^T \over k!}
 = \sum\limits_{k = 0}^\infty \ {(P^T)^k \over k!} = e^{P^T}
$$ Assume that $A$ is skew-symmetric. Then $$
A^T = -A
$$ Define $$
Q = e^{A t}
$$ Then we have $$
Q^T = e^{( A t)^T} = e^{A^T t} = e^{- A t} = \left( e^{A t} \right)^{-1} = Q^{-1}
$$ Thus, it follows that $$
Q^T Q = Q^{-1} Q = I
$$ showing that $Q$ is orthogonal. I hope that this proof is OK!","['linear-algebra', 'ordinary-differential-equations']"
4431387,Proof of expansion of $e^{ix}$,"I am reading Lévy Processes and Infinitely Divisible Distributions by Ken-iti Sato and I don't understand how the expansion $$\tag{1}
e^{iu} = \sum_{k=0}^{n-1}\frac{(iu)^k}{k!} + \theta\frac{|u|^n}{n!}
$$ for each $u\in\mathbb{R}$ and some $\theta\in\mathbb{C}$ with $|\theta|\leq 1$ is derived in Lemma 8.6. The proof just states that it follows immediately from the identity $$\tag{2}
e^{iu} = \sum_{k=0}^{n-1}\frac{(iu)^k}{k!} + \frac{i^n}{(n-1)!}\int_0^u(u-v)^{n-1}e^{iv}dv\ .
$$ My questions are: Where does identity (2) come from? The Taylor formula for real-valued functions seems to yield this expression if applied to $f(u)=e^{iu}$ , but this is not a real-valued function and all versions for complex functions look different. How is the use of the standard Taylor formula justified for functions $f:\mathbb{R}\to\mathbb{C}$ ? How does (1) follow from (2)? I thought about some kind of mean value theorem, but wasn't able to see how the simple expression in (1) can be obtained from the integral in (2). For my purposes, it would be sufficient to derive the formula for $n=2$ , if that makes it easier to explain.","['real-analysis', 'complex-analysis', 'calculus', 'taylor-expansion', 'exponential-function']"
4431408,Evaluating $\{1+\sum\limits_{\mu=1}^{\infty}(-1)^{\mu}\frac{z^{2\mu}}{2^{2^{\mu}}\cdot(\mu !)^2}\}^2$,"Question Calculate the following formula, $$\{1+\sum\limits_{\mu=1}^{\infty}(-1)^{\mu}\frac{z^{2\mu}}{2^{2^{\mu}}\cdot(\mu !)^2}\}^2$$ where $z$ is arbitrary. We are given the hint that we can quote the identity: $$\sum\limits_{\mu=0}^{\nu}(\text{C}_{\nu}^{\mu})^2=\text{C}_{2\nu}^{\nu}=\frac{(2\nu)!}{(\nu!)^2}$$ Attempt Below is my attempt: \begin{align}
\{1+\sum\limits_{\mu=1}^{\infty}(-1)^{\mu}\frac{z^{2\mu}}{2^{2^{\mu}}\cdot(\mu !)^2}\}^2 &=\{\sum\limits_{\mu=0}^{\infty}(-1)^{\mu}\frac{z^{2\mu}}{2^{2^{\mu}}\cdot(\mu !)^2}\}^2\\ &=\sum\limits_{\mu=0}^{\infty}(-1)^{\mu}\frac{z^{2\mu}}{2^{2^{\mu}}\cdot(\mu !)^2}\sum\limits_{\nu=0}^{\infty}(-1)^{\nu}\frac{z^{2\nu}}{2^{2^{\nu}}\cdot(\nu !)^2}\\
&=\sum_{\mu+\nu=0}^{\infty}(-1)^{\mu+\nu}\frac{z^{2(\mu+\nu)}}{2^{2^{\mu}+2^{\nu}}\cdot(\nu!)^2 (\mu!)^2}
\end{align} But I can't find a proper way to use the identity which is supported by the hint.\ Thanks for your help in advance!","['calculus', 'analysis', 'sequences-and-series']"
4431415,"Existence of an integer matrix with maximal subdeterminants $a_1, \ldots, a_n$","Given $n \geq 2$ and integers $a_1, \ldots, a_n$ , does there exist an integer $(n-1) \times n$ matrix whose maximal subdeterminants are $a_1, \ldots, a_n$ (with fixed ordering)? Example: $n = 3$ , $(a_1, a_2, a_3) = (19, 4, 22)$ . The matrix $$\begin{pmatrix}0 & 11 & 2 \\
-2 &95&19\end{pmatrix}$$ has $i$ th subdeterminants (with $i$ th column removed) equal to $(19, 4, 22)$ . Context: The $n=3$ case is precisely this question from the newsletter: Is every vector in $\mathbb Z^3$ a cross product? . (This is where the example comes from.) The general case would give an alternative proof for this question: Can the determinant of an integer matrix with a given row be any multiple of the gcd of that row? by taking $(a_1, \ldots, a_n)$ to be coefficients in Bézout's theorem.","['matrices', 'determinant', 'linear-algebra', 'integers']"
4431417,Does every diffeomorphism between $\mathbb{B}^n$ and $\mathbb{R}^n$ satisfies that $\lim_{\|x\| \to 1} \|f(x)\| = \infty$,"In this question: Diffeomorphism: Unit Ball vs. Euclidean Space a number of diffeomorphisms between $\mathbb{B}^n$ and $\mathbb{R}^n$ are presented. They all have in common that the points closer to the border are sent ""further"". This is: $$\lim_{\|x\| \to 1} \|f(x)\| = \infty$$ Intuitively these diffeomorphisms ""extend"" the ball more or less uniformly, as to fill the space. Is there any diffeomorphism that doesn't have this property, so a sucession of points that converge to the border remain at a bounded distance from the origin?","['geometry', 'real-analysis', 'calculus', 'general-topology', 'differential-geometry']"
4431462,Right and left multiplication by scalar on a vector space,"This is a really basic question but I always wanted to know the answer to it. If we have a vector space $V$ over a field $K$ , then scalar multiplication is usually defined by taking $\alpha v$ , $\alpha \in K$ and $v \in V$ and the scalar always come on the left side. I know that in this case one can freely change the order of multiplication $\alpha v = v\alpha$ . However, what axiom of vector space justifies this exchange? Since scalar multiplication is always introduced by placing the scalar on the left, how can we even start to consider a scalar on the right? Shouldn't we have some property about, say, $1v = v1 = v$ ? I have never found a reference in which some property is explicitly stated.","['linear-algebra', 'vector-spaces']"
4431482,Integrate $\iint_D \frac1{(x^2+y^2)^2 }dx dy$ over a region bounded by four circles,"I'm thinking of doing the substitution $x = \frac{u}{u^2 + v^2}, y = \frac{v}{u^2 + v^2},$ but I'm not sure how to exactly compute the range of values $u$ and $v$ take. Clearly, $u=\frac{x}{x^2+y^2}$ and $v = \frac{y}{x^2 + y^2}.$ The Jacobian of the result is $-\frac{1}{u^2 + v^2}$ , and so the resulting integral equals $\iint_{D'} dudv,$ where $D'$ is the region of possible values for the pairs $(u,v)$ . In the circle $x^2 + y^2 - 2x= 0$ , for instance, we have the point $(1,0)$ , which corresponds to $u=1, v=0$ . Also, we have the point $(2,0)$ , corresponding to $u= \frac{1}2, v = 0$ . I can't seem to generalize what values u and v can take on.","['integration', 'geometry', 'real-analysis', 'multivariable-calculus', 'calculus']"
4431496,Asymptotic formula for $\sum_{n<x} \frac {d(n)}{\sqrt{n}}$,"I am looking for symptotic formula for $\sum_{n<x} \frac {d(n)}{\sqrt{n}}$ which doesn't use $\zeta(\frac{1}{2})$ My guess is - perhaps it is something like $A\sqrt{x}\log{x} + B \sqrt{x} +  O(\log{x})$ where $A, B$ are some constants, based on these formulas: $$\sum_{n<x} \frac {d(n)}{n} = \frac{1}{2}(\log{x})^2 + 2\gamma\log{x} + O(1)$$ $$\sum_{n<x} {d(n)} = x\log{x} + (2\gamma-1)x  + O(\sqrt{x})$$ Per Huxley the error term can be improved to $$\sum_{n<x} {d(n)} = x\log{x} + (2\gamma-1)x  + O(x^\theta)$$ with $ \inf \theta \le 131/416 = 0.31490384615 $ Similar questions: Asymptotic for $\sum_{n<x} \frac {d(n)}{\sqrt{n^a}}$ Asymptotic for $\sum_{n<x} d(n)$","['riemann-zeta', 'number-theory', 'sequences-and-series']"
4431523,Inconsistency in the definition of the connection coefficients,"I am new to general relativity and I am currently facing an apparent inconsistency in the definition of the connection coefficients. Some references I've been consulting (e.g. the lecture notes by S. Carroll or those by D. Tong) define them as follows: $$
\nabla_{\partial_i}\partial_j=\Gamma^k_{ij}\partial_k
$$ where $\{\partial_i\}$ are the chart-induced basis vector fields. Some other references, however (e.g. these beautiful lectures by prof. Schuller) define them with the lower indices swapped: $$
\nabla_{\partial_i}\partial_j=\Gamma^k_{ji}\partial_k
$$ I know that for a torsion-free connection the coefficients are symmetric in the lower indices, so it doesn't really matter which definition one chooses to adopt. Still, this disagreement is bothering me, since I would like to know how the coefficients are supposed to be defined in the general case of a non-torsion-free connection. Is this discrepancy due to a mere convention/choice of notation? And if not, which definition is to be assumed as the correct one?","['connections', 'general-relativity', 'differential-geometry']"
4431526,Maximum element of two integer sets with distinct pairwise sums?,"Let $A, B \in Z^+$ be two sets where $|A|=m, |B|=n$ . If all pairwise sums $a+b (a\in A, b\in B)$ are distinct, a.k.a. $|A+B|=|A||B|$ , what would be the minimum value of $max(A\cup B)$ ? A trivial bound would be $mn/2$ as the largest pairwise sum is at least $mn$ . Meanwhile constructively we can create 2 sets $A=\{1, 2, \ldots, m\}, B=\{1, m+1, 2m+1, \ldots, (n-1)m+1\}$ that have distinct pairwise sums. So $mn/2 \leq min\{max(A\cup B)\} \leq mn-\max\{n, m\}+1$ . What would be a better bound for this problem?","['elementary-number-theory', 'additive-combinatorics', 'combinatorics', 'extremal-combinatorics']"
