question_id,title,body,tags
2435515,"Looking for a family of functions that satisfy $f\left(\frac1 x\right)+f(x)=1$ and maps $[0,\infty) → [0, 1)$ one-to-one","Title has it all.  The faster the better, it's for collision calculations in a game.  I like math but haven't had diffy, so I'm stumped.  Thanks! If it helps, x is a ratio of two positive real numbers. I was messing with arctan of the difference of the two positive real numbers, but it didn't seem to be working quite right.  I figure I need to work it more, since all my scribbles thus far seem to be point towards arctan as my solution.","['asymptotics', 'ordinary-differential-equations', 'functions']"
2435548,Does $\sum_{n=1}^{\infty} \frac{1}{2^n}\left(\frac11+\frac12+\frac 13+...+\frac1 n\right)$ have a closed form solution?,"I need to know whether
$$\sum_{n=1}^{\infty} \frac{1}{2^n}\left(\frac11+\frac12+\frac 13+...+\frac1 n\right)$$has a close form solution? It is easy to prove this series converge ,because $\frac11+\frac12+\frac 13+...+\frac1 n \sim \ln n$ so $$\sum_{n=1}^{\infty} \frac{1}{2^n}\left(\frac11+\frac12+\frac 13+...+\frac1 n\right)\sim\sum_{n=1}^{\infty} \frac{\ln n}{2^n}<\sum_{n=1}^{\infty} \frac{n}{2^n}=4$$ I tried to get the answer by a MATLAb program: suppose $$\quad{a_k=\sum_{n=1}^{k} \frac{1}{2^n}\left(\frac11+\frac12+\frac 13+...+\frac1 n\right)\\a_=0.5\\a_2=    0.8750   \\a_3= 1.1042 \\a_4=   1.2344\\a_5=    1.3057\\\vdots\\a_{20}=1.3863\\\vdots\\a_{100}= 1.3863\\\vdots\\a_{10000}= 1.3863\\\vdots\\a_{10^6}<1.5}$$ So I think It converges to $1.3863 \leq \lim_{k\to \infty}a_k \leq 1.5$ My question is about an analytic solution, Does it exist? Thanks in advance for any Idea.","['power-series', 'sequences-and-series', 'calculus']"
2435566,Integrating $\int\frac{1}{cos(\theta)}d\theta$ by following certain steps,$\int\frac{1}{\cos\theta}d\theta$ So I have this integral here and the solution manual wants the solving to follow a specific level of steps. This integral is okay on it's own but I'm having trouble doing it the way the solutions have suggested. And I'd like to know how it's done. Write it as $\int\frac{\cos\theta}{\cos^2\theta}d\theta$ Make the substitution $u = \sin\theta$ Use partial fractions to complete the integration Multiply a fraction in your answer your answer top and bottom by $1+sin\theta$ Show that $\tan^2\theta+1 = \frac{1}{\cos^2\theta}$ Hence write your answer without using any trig functions except tan. This confuses me because I'm not sure how to follow this step process. I believe the answer without the tan only restriction to be $log(\tan(θ) + \sec(θ)) + C$. But I could be wrong in my substitution methods. Thank you in advance. :),"['trigonometry', 'calculus', 'indefinite-integrals', 'integration', 'fractions']"
2435570,Counterexample $(V^\perp )^\perp = V$ when $V$ is not a closed subspace,"Let $(X, \langle\cdot, \cdot \rangle)$ be a Hilbert space with inner product $\langle\cdot, \cdot \rangle$ and induced norm $\rVert \cdot \lVert$. We know that if $V\subseteq X$ is a closed subspace, then  $(V^\perp  )^\perp = V$, where $V^\perp$ denotes the orthogonal complement with respect to the inner product. Now let $X:= L^2([0,1], \mathcal L^1, \lambda^1)$ be the set of Lebesgue square integrable functions on $[0,1]$. Define $V:= \{\varphi \in X: \varphi(0) = 0 \}$. Clearly, $V$ is a subspace of $X$. Now I showed that $V \neq(V^\perp  )^\perp$, i.e. we have a strict inclusion, by explicitly constructing a function that is in $(V^\perp  )^\perp$, but not in $V$. To understand this better I tried to show that $V$ is not closed with respect to the $L^2$-norm. So I tried to construct a sequence $(\varphi_n)_{n\in \mathbb N}$ such that $\varphi_n(0) = 0$ for all $n\in \mathbb N$ but $\varphi_n \to \varphi$ with $\varphi(0) \neq 0$. However this was kind of tedious dealing with the square in the integrand. Is there an easy way to construct a sequence or some other, better/cleaner option to disprove closedness?","['real-analysis', 'hilbert-spaces', 'functional-analysis', 'lp-spaces', 'inner-products']"
2435607,Redundant Functions?,"Consider the function $f:\mathcal P(\Bbb N)\to\mathcal P(\Bbb N)$ such that $f(A)=\{a:a\in A,a\text{ prime}\}$. This has the property of not having any effect upon being iterated, i.e. $f(A)=f(f(A))=\cdots$. I was wondering what this property would be called and why/whether it is important to anything.","['terminology', 'functions']"
2435621,Matrices equipped with the rank distance form a metric space,"Right now, I'm taking a basic introductory analysis course.   I came across this on wikipedia and I'm Really struggling to prove it is a metric space. (Need help with the 3rd axiom of metric space the most and making the 2nd one more rigorous/less handwaving) The set of all $m$ by $b$ matrices over some field is a metric space with respect to the rank distance $d(X,Y)=\operatorname{rank}(Y-X) $ Facts that I know:
So I know that in order to prove something is a metric space $d(X,Y)=0$ iff $X=Y$   (have to prove both ways) $d(X,Y)=d(Y,X)$  (Symmetry) $d(X,Y)\leqslant d(X,Z) +d(Z,Y)$ (Triangle inequality). Also, it has been a long time since I have taken linear algebra"", but I now that to find the rank, you reduce a matrix to its row echelon form and the number of nonzero rows is the rank (or I think number of linearly independent rows, again it has been awhile). My attempt: If $Y=X$, We have $\operatorname{rank}(Y-X)=\operatorname{rank}(Y-Y)=\operatorname{rank}(O)$ (where $O$ is the null matrix) which is clearly equal to $0$.  Thus $d(X,Y)=0$.  Proving other direction if $d(X,Y)=0$, we have $\operatorname{rank}(Y-X)=0$.  In this case $Y$ must necessarily equal $X$.  If $Y$ does not equal $X$, rank must be at least $1$. Why must $d(X,Y)=d(Y,X)$?  The matrix $X-Y$ and $Y-X$ only differ by signs.  That doesn't change rank.  Hence $\operatorname{rank}(Y-X)=\operatorname{rank}(X-Y)$.  (feel like I hand waved this explanation a bit). Why must $d(X,Y)\leqslant d(X,Z)+ d(Z,Y)$? Need to show: $$\operatorname{rank}(Y-X)\leqslant\operatorname{rank}(Z-X)+ \operatorname{rank}(Y-Z)$$where $X,Y,Z$ are $m \times n$ matrices. This one I'm not sure at all how to approach this.  Why must $\operatorname{rank}(Y-X)\leqslant \operatorname{rank}(Z-X)+\operatorname{rank}(Y-Z)$?  I'm not sure why this must be the case and how to formally show this.","['general-topology', 'real-analysis', 'metric-spaces', 'linear-algebra']"
2435635,A game consists of tossing a fair die. A player wins if the number is even and loses if the number is odd.,"A game consists of tossing a fair die. A player wins if the number is even and loses if the number is odd. The winning or losing (dollar) payoff is equal to the number appearing. Find the player's mathematical expectation $E$. Winning chances or even #: 2,4,6 Lossing chances or odd #: 1,3,5 So $2\times \frac{1}{2} + 4\times \frac{1}{2}+ 6\times \frac{1}{2} = 6$ $1\times \frac{1}{2} + 3\times \frac{1}{2}+ 5\times \frac{1}{2} = 4.5$ so $6 - 4.5 = \$1.50$ But the answer in my book says it is $\$0.50$ What am I doing wrong?","['statistics', 'probability', 'expectation']"
2435744,simple probability question about mutually exclusive event,"If $P(A) = 1/3$ and $P(B^{\complement}) = 1/4$, then, can $A$ and $B$ be mutually exclusive? I already know that for $A$ and $B$ to be mutually exclusive,  $A \cap B = \varnothing$ and $P( A \cup B ) = P (A) + P(B)$. I just can't proceed further than this to prove if $A$ and $B$ are mutually exclusive though as I feel like this is not enough information to determine that. please help Thank You",['probability']
2435771,Calculating the limit $\lim\limits_{x \to 0^+} \frac{\sqrt{\sin x}-\sin\sqrt{ x}}{x\sqrt{x}}$,"Calculate $$\lim\limits_{x \to 0^+} \dfrac{\sqrt{\sin x}-\sin \sqrt{x}}{x\sqrt{x}}$$
  without use Taylor serie and L'Hôpital. $$\lim\limits_{x \to 0^+} \dfrac{\sqrt{\sin x}-\sin \sqrt{x}}{x\sqrt{x}}\cdot\dfrac{\sqrt{\sin x}+\sin \sqrt{x}}{\sqrt{\sin x}+\sin \sqrt{x}}=\lim\limits_{x \to 0^+} \dfrac{\sin x-\sin^2\sqrt{x}}{x\sqrt{x}(\sqrt{\sin x}+\sin \sqrt{x})}$$ now what ?","['derivatives', 'limits', 'trigonometry', 'calculus', 'limits-without-lhopital']"
2435816,A formula for the sum of the triangular numbers? [duplicate],This question already has answers here : Sum of all triangle numbers (6 answers) Closed 6 years ago . Let $t(n)$ denote the n$^{th}$ triangular number. Let $T(n)$ denote the sum of the first $n$ triangular numbers. Is there a formula for $T(n)$?,"['summation', 'discrete-mathematics']"
2435878,Area of surface $z=16-x^2-y^2$ on the first octant,"I am asked to find the area of surface $z=16-x^2-y^2$ on the first octant. I proceeded the following way: Reasoning $$A = \iint_D \left\vert \frac{\partial P}{\partial x} \times \frac{\partial P}{\partial y} \right\vert \ dA$$ Given $P = (x,y,16-x^2-y^2)$ we have $$\frac{\partial P}{\partial x} = (1,0,-2x) \quad \frac{\partial P}{\partial y} = (0,1,-2y)$$ so that $$\int_{0}^{\pi/2} \int_{0}^{4} r \sqrt{1+4r^2} \ drd\theta = \cdots = \frac{\pi}{24} \left( 65 \sqrt{65} -1 \right)$$ Is this correct? Thank you.","['multivariable-calculus', 'surfaces']"
2435913,Solutions of $x^2-6x-13 \equiv 0 \pmod{127}$,"I started learning number theory, specifically polynomial congruences, and need help with the following exercise. Here it is: Does the congruence $x^2-6x-13 \equiv 0 \pmod{127}$ has solutions? I tried to follow the method for solving general quadratic congruence but I didn't get really far. Here's what I've done so far: Since $(4, 127) = 1$, we may complete the square by multiplying by $4$ without having to change the modulus in order to get the following equivalent congruence $$(2x-6)^2 \equiv 36 - 4(-13) \pmod{127} \iff (2x-6)^2 \equiv 88 \pmod{127}.$$ If I'm heading in the right direction then I don't know how to continue from here. I suspect there is another method for solving this problem since I didn't make use of the fact that $127$ is prime. Also, the problem doesn't require to actually find the solutions but only determine if there are solutions. Possibly we can avoid computations and make use of some theorem/lemma to find if the congruence has solutions or not.","['number-theory', 'polynomial-congruences', 'modular-arithmetic']"
2435929,Asymptotic behaviour of $\frac{p_n-2}{p_n}$,"I am interested in how fast this prime product approaches to 0: $$
\prod_{n=2}^{N}\frac{p_n - 2}{p_n}
$$ Where $p_n$ is the nth prime number. Numerical computation suggests that for $N > 1312$, $k = 0.5$
we have the following upper bound: $$
\prod_{n=2}^{N}\frac{p_n - 2}{p_n} > \frac{1}{p_N^{k}}
$$ Can anyone prove this?","['number-theory', 'infinite-product', 'prime-numbers', 'convergence-divergence']"
2435941,Proving condition is sufficient for long term existence of ODE solution,"I am working on the following problem from Gerald Teschl's book on ODE's and am at a loss of how to proceed. Suppose $U=\mathbb{R} \times \mathbb{R}^n$ and that $|f(t,x)| \leq g(|x|)$ for some positive continuous function $g \in C([0,\infty))$ which satisfies
  $$\int_0^{\infty} \frac{dr}{g(r)} = \infty$$
  Then all solutions of the IVP $f(t,x) = \dot{x}$, $x(0) = x_0$ are defined for all $t \geq 0$. Show that the same conclusion still holds if there is such a function $g_T(r)$ for every $t \in [0,T]$. (Hint: Look at the differential equation for $r(t)^2 = |x(t)|^2$.) I am not sure how to use the hint and I have not been successful in any of my attempts at the problem.  Any help would be appreciated.  Thanks!",['ordinary-differential-equations']
2435958,Schilling's proof of the Feynman-Kac Formula for Brownian motion,"This is part of a proof to the Feynman-Kac formula from Schilling's Brownian motion. I need some help understanding the proof to this theorem. Theorem (Kac 1949). Let $(B_t)_{t\ge 0}$ be a $d$ -dimensional Brownian motion, $A$ be the generator of the Feller semigroup $P_t u(x)=E^x u(B_t)$ . If $f \in D(A)$ and $c \in C_b (\mathbb{R}^d)$ , then the unique solution to the initial value problem (8.8) is given by $$w(t,x)=E^x \bigg[ f(B_t) \exp \bigg( \int_0^t c(B_r)dr\bigg)\bigg].$$ This solution is bounded by $e^{\alpha t}$ where $\alpha=\sup_{x\in \mathbb{R}^d} c(x)$ . Proof. Denote by $P_t$ the Brownian semigroup. Set $C_t := \int_0^t c(B_r)dr$ and $T_t u(x):= E^x (u(B_t)e^{C_t})$ . If we can show that $(T_t)_{t\ge 0}$ is a Feller semigroup with generator $Lu=Au+cu$ , we are done. Existence follows from Lemma 7.10, uniqueness from the fact that $A$ has a resolvent which is uniquely determined by $T_t$ , cf. Proposition 7.13. Questions. 7.10 is the lemma below and 7.13 states that there is a one-one relationship between the semigroup and the resolvent operator. It seems to me that, to solve this problem, we need to use the uniqueness of the Laplace transform. For instance, below, the author uses it to prove the heat equation Lemma 8.1. However, I cannot directly apply this same method here because $c$ is not a positive constant, so I do not get something of the form $\alpha id - A$ , which would be the inverse of the resolvent operator. So, I don't understand how we get uniqueness from Proposition 7.13 as we did in Lemma 8.1 below. Moreover, I have a question about existence as well. It seems like the author states that existence follows naturally as by Lemma 7.10 we have $\frac{\partial}{\partial t} w(t,x) = Lw$ and $Aw=\frac{1}{2} \Delta w$ . However, it is shown in the text that $A= \frac{1}{2} \Delta$ when restricted to $C_\infty^2 (\mathbb{R}^d):=\{u\in C_\infty (\mathbb{R}^d): \partial_j u, \partial_j \partial_k u \in C_\infty (\mathbb{R}^d), j,k=1,\dots ,d\}.$ Also, $C_\infty^2 (\mathbb{R}^d) \subset D(A)$ , but is strictly smaller than $D(A)$ for $d>1$ . Hence, even if we have $Lu - Au - cu=0$ , we cannot guarantee that we have the form $(8.8a)$ where $A= \frac{1}{2} \Delta_x w(t,x)$ . So what allows us to conclude the proof here? I would greatly appreciate some help.","['stochastic-processes', 'real-analysis', 'partial-differential-equations', 'brownian-motion', 'analysis']"
2436022,Tsirelson norm construction,"I want to define the Tsirelson norm $\lVert\,\cdot\,\rVert_\mathcal{T}$on $c_{00}$ $$\lVert x\rVert_\mathcal{T} = \max\left\{\lVert x\rVert_0, \frac12\sup_{n\leq E_1 <\cdots <E_n}\sum_{i=1}^n\lVert E_ix\rVert_\mathcal{T}\right\},\quad\forall x\in c_{00},$$ as the limit of the sequence of norms $(\lVert\,\cdot\,\rVert_n)_{n\geq 0}$ where $$\lVert x\rVert_0 = \lVert x\rVert_\infty$$ $$\lVert x\rVert_{n+1} = \max\left\{\lVert x\rVert_0,\frac12\sup_{n\leq E_1 <\cdots <E_n}\sum_{i=1}^n\lVert E_ix\rVert_n\right\},$$ but I'm having troubles showing that this sequence is increasing. For context, this is an outline of what I'm trying: I'm trying to prove the inequality inductively, and the base case $\lVert\,\cdot\,\rVert_1 \geq \lVert\,\cdot\,\rVert_0$ is immediate. Suppose $n+1$ is such that $\lVert\,\cdot\,\rVert_{n+1}\geq\lVert\,\cdot\,\rVert_n$. Then, given a family $n\leq E_1<\cdots<E_n$ of natural numbers the case where $n < E_1$ is easy, because it is sufficient to choose an arbitrary $E_{n+1} > E_n$ and we have $$\sum_{i=1}^{n+1}\lVert E_ix\rVert_{n+1}\geq\sum_{i=1}^n\lVert E_ix\rVert_{n+1}\geq\sum_{i=1}^{n}\lVert E_ix\rVert_n,$$ where the last equality follows from the induction hypothesis, and hence $\lVert x\rVert_{n+2}\geq\lVert x\rVert_{n+1}$. The problem is the case where $n\in E_1$. I even tried to separate into further cases, where $E_1 = \{n\}$ and $\{n\}\varsubsetneq E_1$, but it was useless. I'd appreciate some help with this. Basic definitions: Given two sets $E,\,F$ of $\mathbb{N}$, we say that $E\leq F$ if $\sup E\leq \min F$. Given $E\subseteq\mathbb{N}$ and $x = (x(n))_n\in c_{00}$, we denote by $Ex$ the sequence $$ Ex = \left(x(n)\chi_E(n)\right)_n = \sum_{j\in E} x(j)e_j \in c_{00},$$ where $e_j$ denotes the $j$-th unit vector.","['functional-analysis', 'normed-spaces', 'banach-spaces']"
2436048,Do holomorphic functions necessarily blow up at the edge of their maximal domain of definition?,"If a holomorphic function $f$ is defined on some open set $\Omega$, then any connected open set $U$ containing $\Omega$ either has the property that there exists a holomorphic extension of $f$ on $U$ or does not. If there is a holomorphic extension, it's unique by analytic continuation. Thus, taking the union of all possible extensions, $f$ has a unique maximal connected, open domain on which it's holomorphic. (Right?) What can we say about the behavior of $f$ at the boundary of this domain? Does it necessarily blow up? Or can it be bounded?",['complex-analysis']
2436064,"Linear map $K : C[0,1] \to C[0,1]$ is given as $(Kf)(x) = \int^1_0 k(x,y)f(y)\,dy$. Find $\|K\|$.","I'm solving the following problem. Let $(C[0,1], \|\cdot\|_\infty)$ be the space of real continuous functions equipped with the uniform norm. Let $k : [0,1]^2 \to \mathbb{R}$ be a nonzero continuous function. Define $K : C[0,1] \to C[0,1]$ as: $$(Kf)(x) = \int^1_0 k(x,y)f(y)\,dy, \quad f \in C[0,1]$$ Show that $K$ is bounded and compute $\|K\|$. This question has beed asked several times before: here and here . However, no answer actually constructs the sequence of functions which is needed to prove that $\|K\|\ge \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$, nor at least precisely and elementary specifies the density argument which was used. So I have written a complete proof with an explicit construction of the approximating functions. Could you verify it? $$\|Kf\|_\infty = \sup_{x\in [0,1]}\left|\int^1_0 k(x,y)f(y)\,dy\right| \le \sup_{x\in [0,1]} \int^1_0 |k(x,y)||f(y)|\,dy \\
\le \left(\sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy\right)\|f\|_\infty$$ Thus, $K$ is bounded and $\|K\|\le \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$. Let show that $\|K\|= \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$. Since $x \mapsto \int^1_0 |k(x,y)|\,dy$ is continuous, there exists $x_0 \in [0,1]$ such that $$\int^1_0 |k(x_0,y)|\,dy= \sup_{x\in [0,1]} \int^1_0 |k(x,y)|\,dy$$ Now we $\DeclareMathOperator{\sgn}{sgn}$could take $f(y) = \sgn k(x_0,y)$ and notice that $\|f\|_\infty = 1$ and $$Kf = \int^1_0 |k(x_0,y)|\,dy$$ so we are done. However, $f$ is not necessarily continuous so let's try to approximate it with continuous functions. Let $\varepsilon > 0$, and $M = \max_{(x,y)\in [0,1]^2} |k(x,y)|$. Since $k(x_0, \cdot)$ is continuous, $k(x_0, \cdot)^{-1}(\langle 0, +\infty\rangle)$ is an open set so it can be written as a countable disjoint union of intervals: $$k(x_0, \cdot)^{-1}\big(\langle 0, +\infty\rangle\big) = \bigcup_{n=1}^\infty \langle a_n, b_n\rangle$$ Similarly: $$k(x_0, \cdot)^{-1}\big(\langle -\infty, 0\rangle\big) = \bigcup_{n=1}^\infty \langle c_n, d_n\rangle$$ Now define $$A = \bigcup_{n=1}^\infty \left\langle a_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}, b_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}\right\rangle \\
\cup \bigcup_{n=1}^\infty \left\langle c_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}, d_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}\right\rangle\\
\cup k(x_0, \cdot)^{-1}({0})$$ and define $f_\varepsilon \in C[0,1]$ to be equal to $\sgn k(x_0, \cdot)$ on $A$, and to an affine function on $[0,1]\setminus A$ connecting $\pm 1$ with $0$ so that $f_\varepsilon$ is continuous. Now we have: $$\begin{align}(Kf_\varepsilon)(x_0) &= \int_{[0,1]} k(x_0, y)f_\varepsilon(y)\,dy\\
&= \int_{A} \underbrace{k(x_0, y)f_\varepsilon(y)}_{=|k(x_0, y)|}\,dy + \underbrace{\int_{A^c} k(x_0, y)f_\varepsilon(y)\,dy}_{\geq 0}\\
&\geq \int_{A} |k(x_0, y)|\,dy\\
&= \int_{[0,1]} |k(x_0, y)|\,dy - \int_{A^c} |k(x_0, y)|\,dy\\
&\geq \int_{[0,1]} |k(x_0, y)|\,dy - M\lambda(A^c)\\
&\geq \int_{[0,1]} |k(x_0, y)|\,dy - \varepsilon\\
\end{align}$$ since $$\lambda(A^c) = \lambda\left(
\bigcup_{n=1}^\infty
\left[a_n,
a_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}
\right]
\cup
\left[b_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{b_n-a_n}{2}\right\}, b_n
\right]
\right)\\
+\lambda\left(
\bigcup_{n=1}^\infty
\left[a_n,
c_n+\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}
\right]
\cup
\left[d_n-\min\left\{\frac{\varepsilon}{2^{n+3}M}, \frac{d_n-c_n}{2}\right\}, d_n
\right]
\right)\\
\le \sum_{n=1}^\infty \lambda\left(\left[a_n, a_n+\frac{\varepsilon}{2^{n+3}M}\right]\right) + \lambda\left(\left[b_n-\frac{\varepsilon}{2^{n+3}M}, b_n\right]\right) + \lambda\left(\left[c_n, c_n+\frac{\varepsilon}{2^{n+3}M}\right]\right) + \lambda\left(\left[d_n- \frac{\varepsilon}{2^{n+3}M}, d_n\right]\right)\\
=\frac{\varepsilon}{M}\sum_{n=1}^\infty\frac{1}{2^{n+1}} = \frac{\varepsilon}{M}$$ Thus: $$\|K\| \geq \frac{\|Kf_\varepsilon\|_\infty}{\|f_\varepsilon\|_\infty} \ge |(Kf)(x_0)| = \int_{[0,1]} |k(x_0, y)|\,dy - \varepsilon \xrightarrow{\varepsilon\to 0} \int_{[0,1]} |k(x_0, y)|\,dy$$ So, $\|K\| = \int_{[0,1]} |k(x_0, y)|\,dy$ follows. The situation would be similar (only easier) if $k(x_0, \cdot)$ had only finitely many points where it changes sign.","['functional-analysis', 'normed-spaces', 'proof-verification']"
2436090,Is this function unimodal?,"Consider the real function: $$f\left(\xi\right)=\frac{1-\xi}{\xi+c\mathrm{e}^{k/\xi}\left(a+\xi-a\xi\right)},\quad0\le\xi\le1$$ where $a,c,k$ are positive parameters. For all combinations of parameters I have tried, this function is always unimodal (I plotted it). I have not been able to prove this because the derivative is quite messy. Is there a simpler way? On the other hand, by considering expansions near $\xi \approx 0$ and $\xi \approx 1$, I was able to show that this function is increasing near $\xi \approx 0$, and decreasing near $\xi \approx 1$.","['real-analysis', 'functions']"
2436094,Find the matrix projection of a symmetric matrix onto the set of symmetric positive semi definite (PSD) matrices,"Consider $\mathbb{S}^n$, the set of all $n\times n$ real symmetric matrices. Let $A\in \mathbb{S}^n$ and $A=U\Lambda U^{\top}$ be its spectral decomposition. I want to know how to prove that $\Pi_{\Bbb S_+^n}(A)=U\Lambda ^+U^{\top}$, where $\Lambda^+$ is the $n\times n$ diagonal matrix given by $\Lambda_{ii}^+=\max\{\Lambda_{ii}, 0\}$, for $i=1,...,n$；$\Pi_{S}(x)=\arg\min_{z\in S}\|x-z\|^2_2$. This projection means that we can project any symmetric matrix on $\mathbb{S}_+^n$ and get the closest positive semi-definite matrix, but how can we prove that it is the closest one?","['matrices', 'convex-optimization', 'positive-semidefinite', 'projection']"
2436128,Wave equation for a contracting circle,"I am trying to solve the wave equation for a periodically contracting and expanding circle, in two dimensions. It was so long ago that I did any kind of similar calculation that I have kinda forgotten how to proceed. What I remember is that the wave equation in polar coordinates leads to the Bessel equation. But is that really the way to go about it in this case? Edit: The reason for my confusion stems from the fact that: $$\partial_{\alpha}\partial_{\alpha}X^{\mu}=0$$
where $\alpha=(\tau,\lambda)$ and $X(\tau,\lambda)=(X^1,X^2)$, so for polar coordinates I would have that $r(\tau,\lambda)$ similarly for $\theta$. Therefore I doubt that I would need reparametrize the differential equation to Bassel equation. Am I correct in thinking that it would be unwise to do so?",['ordinary-differential-equations']
2436130,Find Taylor series of: $x^x$,"Find the Taylor series polynomial form of: $$x^x$$ My attempt: I started calculating derivatives of $x^x$ for the series, $$f(x)=x^x$$ $$f'(x)=x^x(\ln x+1)$$ $$f''(x)=x^{x-1}+x^x(\ln x+1)$$ $$f'''(x)=\cdots$$ $$\vdots$$ But i could get only till certain terms and got frustrated, Attempt no. 2: $$f(x)=x^x$$ $$f(x)=e^{x\ln x}$$ $$f(x)=1+x\ln x+\frac{x^2\ln^2 x}{2!}+\cdots$$ $$f(x)=1+x({x-1}-\frac{(x-1)^2}{2}+\frac{(x-1)^3}{3}+\cdots)+\frac{x^2}{2}({x-1}-\frac{(x-1)^2}{2}+\frac{(x-2)^3}{3}+\cdots)^2+\cdots$$ Now let's try to get $x^n$ 's coefficient, $$C(x^0)=1$$ In the bracket multiplied to x, $$1+x(x-1-x^2+2x-1+x^3-3x+3x-1+\cdots)+\cdots$$ Following the lead $$1+x(g(x)-1-1-1-1\cdots)+\cdots$$ $$C(x^1)=-1-1-1-\cdots$$ So, $$C(x^1)=-\infty\cdots?$$ How do we write this equation? Attempt 3: $$f(x)=1+(x+1)\ln (x+1)+\frac{(x+1)^2}{2!}\ln^2(x+1)+\cdots$$ Now, $$f(x)=1+(x+1)(x+\frac{x^2}{2}+\frac{x^3}{3}\cdots)+\cdots$$ Now, $$C(x^0)=1$$ Considering $(1+x)^2,(1+x)^3\cdots$ contribution to $C(x^1)$ $$C(x^1)=1+1+1+1+\cdots$$ $$C(x^1)=\infty\cdots ?$$ Still.....stuck","['derivatives', 'taylor-expansion', 'calculus']"
2436147,A question about simple circles and triangles,"Today I've encountered a question like the following I am adding a picture because I have to; The question paragraph says; $\text{Given} \quad |OF|=6 \quad \text{and} \quad |BF|=4$ What is $|CH|=x$ My Attempts; I have noticed that the diameter $r=10$ (1) I have drawn a line from $C$ to $O$ which also is $r$ (2) I have written $|HO|=\sqrt{100-x^2}$ but couldn't go further, What do you suggest?","['circles', 'triangles', 'geometry']"
2436178,Is the covariant Hessian preserved by isometries?,"Let $(M_1,g_1,\nabla^1)$ and $(M_2, g_2, \nabla^2)$ be pseudo-Riemannian manifolds equipped with their Levi-Civita connections and $F\colon M_1 \to M_2$ be an isometry. 
$\renewcommand\vec[1]{{\bf #1}}$
I wondered if given $f \in C^\infty(M_2)$, we have $${\rm Hess}(f\circ F)(\vec{X}, \vec{Y}) = {\rm Hess}(f)({\rm d}F(\vec{X}),{\rm d}F(\vec{Y})),$$for given $\vec{X},\vec{Y} \in \mathfrak{X}(M_1)$. I'm probably missing some subtlety on notation, but here's my attempt. Recalling that ${\rm d}F(\vec{X})(f) = \vec{X}(f\circ F)$, etc., we have $$\begin{align} {\rm Hess}(f\circ F)(\vec{X},\vec{Y}) &= \vec{X}(\vec{Y}(f\circ F)) - (\nabla^1_{\vec{X}}\vec{Y})(f\circ F) \\ &= \vec{X}({\rm d}F(\vec{Y})(f)) - {\rm d}F(\nabla^1_{\vec{X}}\vec{Y})(f) \\ &= \vec{X}({\rm d}F(\vec{Y})(f)) - (\nabla^2_{{\rm d}F(\vec{X})}{\rm d}F(\vec{Y}))(f).\end{align}$$I don't know how to justify that $\vec{X}({\rm d}F(\vec{Y})(f)) = {\rm d}F(\vec{X})({\rm d}F(\vec{Y})(f))$. I'm not sure that this is even true, since ${\rm d}F(\vec{Y})(f) \in C^\infty(M_1)$ and ${\rm d}F(\vec{X})\in\mathfrak{X}(M_2)$. Help?","['vector-fields', 'semi-riemannian-geometry', 'riemannian-geometry', 'differential-geometry']"
2436182,Need help with the integral $\int_{0}^\infty e^{-x^{2}}x^{2n+1}dx $,"Problem: Prove that $$I = \int_{0}^\infty  e^{-x^{2}}x^{2n+1}dx = \frac{n!}{2}     $$ Source: A problem I found on an integral test. The problem bugged me for long and I did end up leaving it on the exam. Back home I decided to tackle it again. Here's my go on it. My try: I have never encountered such a problem before, not even in my assignments and workbooks. Here's how I tried it We can first take an indefinite integral and then we can work up towards a reduction formula .We can plug in the limits later. I don't know if it's any good but I did write this up on the exam: $$I_{2n+1} = \int{e^{x^{-2}}}{x^{2n+1}}dx$$ On applying Integration by parts: $$I_{2n+1} = e^{-x^{2}}\frac{x^{2n+2}}{2n+2} +\frac{2}{2n+2} \int{e^{-x^{2}}}{x^{2n+3}}dx$$ or $$I_{2n+1} = e^{-x^{2}}\frac{x^{2n+2}}{2n+2} +\frac{I_{2n+3}}{n+1} $$
or $$I_{2n+1} = {e^{-x^{2}}\over 2}\Bigl(\frac{x^{2n+2}}{n+1}\Bigl) +\frac{I_{2n+3}}{n+1} $$ What can I do next? Am I doing it incorrectly? Is this the wrong way? Please help me as I can't stop thinking about this problem! Thanks! Edit 1: As suggested (already tried by me) lets put $u = -x^2$
$$ du = -2xdx$$
plug the above expression in the problem $$I_{n} = -{1 \over 2}\int{e^{u}}{u^{n}}du$$ $$I_{n} = -{1 \over 2}e^{-u}\frac{u^{n+1}}{n+1} + \frac{I_(n+1)}{n+1}$$ Certainly helps. Now what? Edit 2: Use the gamma function to get the result. Now unfortunately I didn't read it for the test that's why I didn't get it. Thank you all for pointing out the right direction!","['improper-integrals', 'integration', 'definite-integrals', 'calculus']"
2436231,Group action on a Cartesian product,"Let $G$ be a finite group acting on a finite set $X$. Then naturally $G$ acts on $X \times X$ by $g.(x,y)=(g.x,g.y)$. Is there any way to find the number of orbits of the action of $G$ on $X\times X$ using the action $g$ on $X$? Are they related?","['group-actions', 'group-theory']"
2436234,Clarification on explanation of mechanics of tensor product,"XylyXylyX has a great series of videos on manifolds and tensors. I would like to confirm a couple of points that are probably implied (or misunderstood (by me)). It makes reference to this point in the definition of tensor product : ... in the lower left-hand corner of the slide, he is illustrating the product tensor $e^1 \otimes e^2$ operating on a pair of vectors $A^\mu\,e_\mu, B^\nu\,e_\nu \in V,$ resulting in the real number $[e^1 \otimes e^2](A^\mu e_\mu, B^\nu e_\nu)=A^1 B^2.$ I would like to ask you to confirm two things: Even though $A^1 B^2$ is a real number, if we were to carry out the complete tensor product $[*]$, we'd have to express it as $$A^0 B^0 e_0 \otimes e_0 + A^0 B^1 e_0 \otimes e_1 + \cdots + A^4 B^4 e_4 \otimes e_4.$$ In other words, it wouldn't be just a single real number, but rather more like a matrix. This seems consistent with the result in the Wikipedia example of the tensor product of $v = \begin{bmatrix}1& 2& 3 \end{bmatrix}$ and $w = \begin{bmatrix}1 & 0 & 0 \end{bmatrix}:$ $$v\otimes w=\hat x \otimes \hat x + 2 \hat y\otimes \hat x + 3 \hat z \otimes\hat x $$ although there are no apparent covectors in this Wikipedia example, possibly explaining the difference. $[*]$ NOTE that this may be the main source of my misunderstanding: In the example on the slide posted, he picks out $e^1\otimes e^2$, but I don't know if one has to continue operating on the $15$ additional $e^i\otimes e^j$ pairs. When the covectors (linear functionals) are not just the basis of $V^*,$ and they have coefficients, the tensor product $\beta \otimes \gamma$ operating on these same two vectors would look like $$\beta_0 \gamma_0 A^0 B^0 e_0 \otimes e_0 + \beta_1 \gamma_0 A^1 B^0 e_1 \otimes e_0 + \cdots + \beta_4 \gamma_4 A^4 B^4 e_4 \otimes e_4$$ with $\beta_i, \gamma_i$ corresponding to the coefficients of the covectors $\beta, \gamma \in V^*.$","['differential-geometry', 'tensors', 'linear-algebra']"
2436258,"Given $\operatorname{E}[Y\mid X]=1$, then to show that $\operatorname{Var}(XY)>\operatorname{Var}(X)$","Given $\operatorname{E}[Y\mid X]=1$, then to show that $\operatorname{Var}(XY)>\operatorname{Var}(X)$. Tried apply a few inequality such as Schwartz and Jensen, but none seem to be working. If not entire solution some useful inequality would also work as a suggestion","['statistics', 'conditional-expectation', 'probability', 'random-variables']"
2436268,"Find the limit to $\lim\limits_{(x,y)\to(0,0)}\frac{x^5+y^2}{x^4+|y|}$","My problem is evaluating the following limit:
$$\lim_{(x,y)\to(0,0)}\frac{x^5+y^2}{x^4+|y|}$$
The answer should be 0. I tried to convert the limit into polar form, but it didn't help because I couldn't isolate the $r$ and $\theta$-variables of the expression. My ""toolbox"" for solving problems like these is very limited... If polar form doesn't work, then I usually have no clue on how to continue. Edit : I think this is the solution. 
$$
\lim_{(x,y)\to(0,0)}\left|\frac{x^5+y^2}{x^4+|y|}\right| = \frac{|x^5+y^2|}{|x^4+|y||}
$$
Applying the triangle inequality gives
$$
\frac{|x^5+y^2|}{|x^4+|y||} \leq \left|\frac{x^5}{x^4+|y|}\right| + \left|\frac{y^2}{x^4+|y|}\right|
$$
Inspecting the denominators on the RHS gives:
$$
\left|\frac{x^5}{x^4+|y|}\right| \leq |x|, \quad\left|\frac{y^2}{x^4+|y|}\right| \leq |y|
$$
So
$$
\left|\frac{x^5}{x^4+|y|}\right| + \left|\frac{y^2}{x^4+|y|}\right| \leq |x| + |y|
$$
Since $|x| + |y| \to 0$ when $x,y\to 0$, the sandwich theorem states that $|\frac{x^5+y^2}{x^4+|y|}| \to 0$. And if $\lim |f(x)|=0$ then $\lim f(x)=0$ which solves the original problem.","['multivariable-calculus', 'limits']"
2436269,Convergence in probability of renormalized running maximum of i.i.d. square integrable random variables,"Let $(X_n)_{n \ge1}$ be a sequence of i.i.d. real-valued random variables. Define
  $$M_n = \max_{1\le k \leq n}X_k$$
  Let $F$ be the distribution function of $X_1$. $F(x) < 1$ for all $x \in \mathbb{R}$. Assume that $EX_1^2$ is finite. Show that 
  $$\frac{M_n}{\sqrt{n}} \overset{P}{\to} 0$$ Attempt Let $\varepsilon > 0$ be given and note that $$
P\left( \frac{|M_n|}{\sqrt{n}} \geq \varepsilon \right) = P(M_n^2 \geq n \varepsilon^2) = P\left(\bigcup_{k=1}^n (X_k^2 \geq n \varepsilon^2)\right)\\ \leq \sum_{k=1}^n P(X_k^2 \geq n \varepsilon^2) =  nP(X_1^2 \geq n \varepsilon^2)
$$ I'm not quite sure what to do from here. It seems like this final thing should converge to zero but how do I prove this?","['probability-theory', 'convergence-divergence']"
2436288,How to find degree of a differential equation.,"I have a differential equation, $$e^{\large y^\prime} = x + x^3 + x^5 + y,$$ I need to find the degree of this equation. Using Wikipedia definition , In mathematics, the degree of a differential equation is the power of its highest derivative, after the equation has been made rational and integral in all of its derivatives. I would say that the degree is one because if I take $\log $ on both sides I get $${y^\prime} = \log(x + x^3 + x^5 + y).$$ My teacher says that the degree is not defined because this DE cannot be represented as sum of polynomials in derivatives of $y$. When I asked, what if we take $\log$ on both sides, he says that we are not allowed to perform any operations on the DE, that will change DE of which we have to find the degree. This contradicts the definition by Wikipedia. Who is correct? What is the degree of this DE, $1$ or not defined?","['real-analysis', 'ordinary-differential-equations', 'calculus']"
2436307,Atiyah-Singer Index Theorem applied to the Laplacian Operator on Riemann Surfaces,"Does applying the Atiyah-Singer index theorem to the Laplacian operator on a Riemann surface result in any known theorem? What are some interpretations of the analytic and topological indices? (I'm aware they are both 0, but I want to know if that implies anything). For example, if we use the $\bar{\partial}$ operator, the Atiyah-Singer index theorem gives use the Riemann-Roch theorem, I'm wondering if something similar happens in this case.","['complex-geometry', 'algebraic-geometry', 'partial-differential-equations', 'algebraic-topology', 'differential-geometry']"
2436326,Probability of flipping heads where the coin's bias is drawn from a uniform distribution,"What is the probability of flipping a head on a coin where the coin's probability of flipping a head is drawn uniformly from $[0,1]$. I think this question is easy, however I am getting a result that makes no sense. Please don't directly give me the answer. I can look that up in a book. Let $H$ be the binary event that the a head is flipped. Let $Q$ be Uniform($0,1$). Informal attempt: Firstly, it's easy to see that given that we know $Q=q$, the probability of flipping heads is simply $1/q$. Since the distribution of $Q$ is uniform, we just need the average value of the function $1/q$. In particular, the probability must be $\frac{1}{1-0} \int_0^1 \frac{1}{q} \, dq$, which does not converge! Formal attempt: We wish to compute $P(H= \text{True})$. By the total law of probability, we have:
$$P(H=\text{True}) = \int_0^1 P(H=\text{True}\mid Q=q)f_Q(q) \, dq = \int_0^1 \frac{1}{q} (1) \,dq$$ Again, this does not converge!? What am I doing wrong? I have a strong feeling my mistake lies in the fact that I am mixing continuous and discrete distributions. That is, I am conditioning on an event that actually cannot exist, i.e. $P(Q=q)=0$.","['statistics', 'probability']"
2436350,Why can complex numbers be written in exponential form? $z=r(\cos \theta+i\sin \theta)$ is $z=re^{i\theta}$.,Why can complex numbers be written in exponential form? $z=r(\cos \theta+i\sin \theta)$ is $z=re^{i\theta}$ . I have studied that the exponential form of a complex number $z=r(\cos \theta+i\sin \theta)$ is $z=re^{i\theta}$ . Can someone explain why?,"['algebra-precalculus', 'real-analysis', 'complex-numbers']"
2436430,Relation between loss function and the target function ?,"I would like to know how to prove that the minimizer of some loss function is equivalent to calculating some properties of the target function?
For example: Regression Loss : Square loss : $$ f_p(x)= \int_Y ydp(y|x)$$ Absolute loss $$f_(p)=median ~~~p(y|x)$$ where 
$$median~~ p(.)= y  ~~~~~~~s.t. ~~~~ \int_{-\infty}^y tdp(t)= \int_y^{+\infty} tdp(t).$$ Classification Loss : Hinge-loss $$ f_p(x)=sign(p(1|x)-p(-1|x))$$ Logistic loss $$ f_p(x)= \log \frac{ p(1|x)}{p(-1|x)} $$","['machine-learning', 'optimization', 'functions']"
2436479,Chebyshev vs Euclidean distance,"When calculating the distance in $\mathbb R^2$ with the euclidean and the chebyshev distance I would assume that the euclidean distance is always the shortest distance between two points. Considering the following example $P_{1}=(1,2)$, $P_{2}=(7,6)$ $Euclidean_{distance} = \sqrt{(1-7)^2+(2-6)^2} = \sqrt{52} \approx 7.21$ $Chebyshev_{distance} = max(|1-7|, |2-6|) = max(6,4)=6$ the chebyshev distance seems to be the shortest distance. Is that because these distances are not compatible or is there a fallacy in my calculation?",['geometry']
2436486,Can we find a periodic function $f$ with a non-zero smallest period such that $f(x^2)$ is also periodic?,"Let $f$ be a periodic function such that it has a (non-zero) fundamental period (Smallest nonzero period). Can 
 $f(x^2)$ also be periodic? So the constant functions and Dirichlet function are not examples we want here. If $f$ is continuous, then it is impossible, because $f(x^2)$ fails to be uniformly continuous.",['real-analysis']
2436488,Sequence of functions in an $ \mathcal{L}^p $ space,"Let $ X=[0,1] $, $ \mu $ the Lebesgue measure on $ X $ and $ \{ f_n \}_{n=1}^{\infty} $ a sequence of functions in $ \mathcal{L}^p(\mu) $, $ 1 < p < \infty $. Suppose that there exists an $ f \in \mathcal{L}^p(\mu) $ such that $ f_n \rightarrow f $ pointwise on $ X $. Also, suppose that there is a positive constant $ M $ such that $ || f ||_p \le M \: \:  \forall $ $ n \in \mathbb{N} $. Then prove that for each $ g \in \mathcal{L}^q(\mu) $, we have $$ \lim_{n \rightarrow \infty} \int_{X} (f_ng) d\mu = \int_{X} (fg) d\mu $$ where $ q $ is the conjugate exponent of $ p $. What I tried: By Holder's inequality, $$ \left| \int_{X} (f_ng) d\mu - \int_{X} (fg) d\mu  \right| \le \int_{X} |f_n-f||g| d\mu \le ||f_n-f||_p||g||_q$$ Therefore, to prove the assertion, it suffices to prove that $ ||f_n-f||_p \rightarrow 0 $, i.e $ f_n \rightarrow f $ in $ \mathcal{L}^p(\mu) $ Let $ \epsilon > 0 $. Note that $ |f|^p \in \mathcal{L}^1(\mu) $, so by absolute continuity, there exists a $ \delta > 0 $ such that $ \mu(G) < \delta $ implies $ \int_G |f|^p d\mu  < \epsilon $. By Egorov's theorem (applied using $ \delta $), there is a closed set $ F \subseteq [0,1] $ such that $ f_n \rightarrow f $ uniformly on $ F $ and $ \mu(X-F) < \delta $. We then have the estimates, $$ \int_X |f_n - f|^p d\mu = \int_F |f_n - f|^p d\mu + \int_{X-F} |f_n - f|^p d\mu \le \int_F |f_n - f|^p d\mu + \int_{X-F} (|f_n| + |f|)^p d\mu \le \int_F |f_n - f|^p d\mu + \left[\left(\int_{X-F} |f_n|^p d\mu \right)^{1/p} + \left(\int_{X-F} |f|^p d\mu \right)^{1/p} \right]^p $$ where the last inequality is by Minkowski. Now, by uniform convergence on $ F $, there exists $ N $ such that $ |f_n(x)-f(x)| < \epsilon $ for all $ n \ge N, x \in F $. Therefore, $ \int_F |f_n - f|^p d\mu \le \int_{F} \epsilon^p d\mu \le \epsilon^p $ for all $ n \ge N $. Since $ \mu(X-F) < \delta $, we have $ \int_{X-F} |f|^p d\mu < \epsilon $ and $ \left(\int_{X-F} |f_n|^p d\mu \right)^{1/p} \le ||f_n||_p \le M $ for all $ n \in \mathbb{N} $. Combining, we get for all $ n \ge N $, $$ \int_X |f_n - f|^p d\mu \le \epsilon^p + (M + \epsilon^{1/p})^p $$ This estimate is not good enough; the $ M $ causes a problem. Is there a way around or another way?","['real-analysis', 'lp-spaces', 'measure-theory']"
2436511,Prove: R is an equivalence relation.,"In $ \mathbb{R^2}$ we consider the relation $(x,y)R(a,b)$ if and only if exists $n \in  \mathbb{Z} $ such that $n-1<y\leq n \ $ and $ n-1<b\leq n \  $. Prove: $ R$ is an equivalence relation. $R$ is an equivalence relation if it is  reflexive, symmetric and  transitive . Reflexive : $\forall  (x,y)\in \mathbb{R^2} \Rightarrow (x,y)R(x,y) \Rightarrow n-1<y\leq n \ \wedge n-1<y\leq n  $ Symmetric : $\forall  (x,y)\in \mathbb{R^2} \Rightarrow (x,y)R(a,b) \Rightarrow n-1<y\leq n \ \wedge n-1<b\leq n\Rightarrow n-1<b\leq n \ \wedge n-1<y\leq n\Rightarrow (a,b)R(x,y)  $ Transitive $\forall  (x,y)\in \mathbb{R^2}   $
\begin{split}
(x,y)R(a,b)& \| n-1<y\leq n \ \wedge n-1<b\leq n \\ (a,b)R(c,d)& \|n-1<b\leq n \ \wedge n-1<d\leq n  \\ &\Rightarrow n-1<y\leq n \ \wedge n-1<d\leq n\\&\Rightarrow\ (x,y)R(c,d)
\end{split} Is correct my proof ?",['elementary-set-theory']
2436531,Find the equation of a parabola and a point on the parabola given a two tangent lines.,"I've been trying to find a solution to this problem but I'm not too sure how to go about solving it. I need to find the unknown values of $A, B$ and $C$ in the parabola
  equation $y(x) = Ax^2 + Bx + C$ given that the parabola passes through
  point (0,0) and is tangent to the line $y1(x) = 0.1x$ which also
  passes through the point (0,0). I also need to find an unknown point (x-coordinate, y-coordinate) on
  the parabola which is tangent to $y2(x) = -0.08x + 10$ given that
  $y2(x) = -0.08x + 10$ passes through (200,-6). I would greatly appreciate any help with solving this problem. So far I've tried to use the $y1(x)$ line to determine the vale of $B$ and $C$ in the parabola: As the parabola passes through (0,0) I tried, $$y(0) = 0$$ so 
$$A(0)^2 + B(0) + C = 0\\
C = 0$$ As the line $y1(x) = 0.1x$ is tangent to the parabola at (0,0) I tried, $$ y'(0) = 0.1$$ so $$ 2A(0) + B = 0.1\\               
B = 0.1$$ However when trying to find a point on the parabola where the parabola is tangent to $y2(x)$ it seems to me that the value for B would be different, so this has me really confused on how to solve this problem. Once again, any help with this would be very very appreciated :)","['derivatives', 'learning', 'calculus']"
2436538,Why is $|HK| = \frac{|H||K|}{|H \cap K|}$? [duplicate],"This question already has answers here : Order of a product of subgroups. Prove that $o(HK) = \frac{o(H)o(K)}{o(H \cap K)}$. (6 answers) Closed 6 years ago . Let $G$ be a finite group, and $H$,$K$ be subgroups. Then we can form the set $$HK = \{hk: h \in H, k \in K\}$$
It is well known that $HK$ is a subgroup of $G$ if and only if $HK=KH$. If $H \cap K = \{e\}$, then we clearly have $|HK| = |H||K|$ (indeed, the group multiplication $H \times K \to HK$ is a bijection). 
On the other extreme, if $H \subset K$ then $|HK| = |K|$. In general we have the formula
$$|HK| = \frac{|H||K|}{|H \cap K|}$$ 
which quantifies the idea that we ""$HK$ collapses in proportion to how many elements $H,K$ share"". I'm trying to get a bit more intuition for this fact. I've seen an argument by which we establish a bijection between the sets of cosets $HK/K$ and $H/(H\cap K)$, but it wasn't ""manual"" enough for me (that is to say, I'm willing to believe that it's an enlightening proof, to someone who is more savvy with cosets). One idea for an argument I had was counting: if we seek to prove $|HK||H \cap K| = |H||K|$ then it's attractive to try to show that every distinct element of $HK$ obtained by writing $g = hk$ is multiply counted $|H \cap K|$ times. Indeed, suppose that $h_{1}k_{1} = h_{2}k_{2}$. Then we obtain the element $h_{1}^{-1}h_{2} = k_{1}k_{2}^{-1} \in H \cap K$. Then I get a bit stuck, because I want to show the duplicate elements biject with $H \cap K$ but I can't tell how. Of course, it might not be possible, but my intuition says maybe. Any of the following would constitute a helpful answer (and be greatly appreciated!): A bit of exposition into why the coset-based argument is an intuitive one to make (especially without knowing the answer in advance) Some help finishing off the argument I've tried to start A oonvincing reason the argument I've started is hopeless","['finite-groups', 'intuition', 'group-theory']"
2436548,Is there a closed form for this integral $\int_{0}^{\infty}{e^x-1\over e^{ax}-1} dx?$,"Does this integral $(1)$ $$\int_{0}^{\infty}{e^x-1\over e^{ax}-1} dx=F(a)\tag1$$ $a\ge1$ has a closed form? We may rewrite it as $$\int_{0}^{\infty}{1\over e^{x(a-1)}-e^{-x}}-\int_{0}^{\infty}{1\over e^{ax}-1} dx=F(a)\tag2$$ we have $$\zeta(s)={1\over \Gamma(s)}\int_{0}^{\infty}{x^{s-1}\over e^{x}-1}dx\tag3$$ Guess of a closed form for $F(a)$ $$F(a)=\color{red}{{\pi\over 2a}\cot\left({\pi\over a}\right)}-{1\over a}\ln(2a)+
{2\over a}\sum_{n=1}^{\left\lfloor {a-1\over 2}\right\rfloor}\cos\left({2n\pi\over a}\right)\ln\sin\left({n\pi\over a}\right)\tag4$$ I was checking on the wolfram integral, the red part give part of the closed form but the black part seem to be a mistake somewhere I can't figured it out. According to wolfram integral: $$F(2)=\ln(2)$$ $$F(3)={1\over 2}\ln(3)-{\sqrt{3}\over 18}\pi$$ $$F(4)={3\over 4}\ln(2)-{\pi\over 8}$$","['improper-integrals', 'complex-analysis', 'real-analysis', 'integration']"
2436554,Does homoclinic orbit co-exists with periodic orbits?,"Assume that an autonomous differential equation 
$$ \frac{dx}{dt}=F(x), \quad x\in R^n,\quad F:R^n\to R^n, \quad n>2$$
has a homoclinic orbit $x=x_{h}(t).$ 
Does it ever imply that the equation has periodic orbits? I guess for $n=2$ this is the case. Can something be said about higher dimensions?","['ordinary-differential-equations', 'dynamical-systems']"
2436563,Finding the following integral.,"$f$ is continuous function $\forall x\in[0,a]$ $(a>0)$ $f(x)+f(a-x)\neq 0$ $\forall x\in[0,a]$ Find the following integral:
$$ \int^a_0\frac{f(x)}{f(x)+f(a-x)}dx$$ So I'm thinking: $y:=a-x$, then
$$ \int^a_0\frac{f(x)}{f(x)+f(a-x)}dx=\int^a_0\frac{f(a-y)}{f(a-y)+f(y)}d(a-y)=\int^a_0\frac{f(a-y)+f(y)-f(y)}{f(a-y)+f(y)}d(a-y)=\int^a_0\frac{f(a-y)+f(y)}{f(a-y)+f(y)}d(a-y)-\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(a-y)= (1)$$
Now let's take $d(a-y)=-dy$
$$(1)=-\int^a_01dy+\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(y)=-(y)\bigg|^a_0+\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(y)=(2)$$
Now, should I replace $y=a-x$ and get
$$ (2)=-a - \int^a_0\frac{f(a-x)}{f(x)+f(a-x)}dx$$ And finally combine the first and last equation?
Or how should act? I'm stuck with at the last part. How should I finish it?","['real-analysis', 'analysis']"
2436581,Understanding the use of Radial Basis Function in Linear Regression,"I am attempting to understand the use of Radial Basis Functions (RBFs) as used in linear regression. Building the problem : RBFs can be used as a means of separating data which is not linearly separable (see example scatter-plot by link containing different below) Non-linearly separable Scatter plot Positive (green) and negative (red) data-points are clustered together but no straight line can be found to separate them from each-other. We are seeking some function $f(x) = w\phi(x) + c$ which will plot a 'line' between the points. Note : $w$ represents a matrix of weights $\phi(x) = \exp(−(x−c)^⊤(x−c)/h^2)$ ( Note : Our Radial Basis function is Gaussian). $c$ is our 'y' intercept $h$ relates to how quickly $\phi(x)$ drops off towards zero My current ""understanding"" : We can use a linear combination of RBFs (a seperate RBF for each cluster, 3 in this case) to find a 'line' ( $f(x)$ ) which will separate positive data-points from negative data-points. The function will appear as follows: $f(x) = w_1\phi_1(x) + w_2\phi_2(x) + w_3\phi_3(x)$ So we choose values for centers $c_1, c_2, c_3$ to offset the respective RBFs $\phi_1(x), \phi_2(x), \phi_3(x)$ according to the distance the related scatter plot is from the origin of the 2-d plane ($x_1$,$x_2$). Appropriate values are then chosen for the $w_1, w_2, w_3$ matrix such that when multiplied by the respective $\phi$ the hope being that any new (previously unseen values) will be determined to be positive or negative depending on what side of the 'line' defined by $f(x)$ they reside on. My Questions : Am I correct, incorrect, partially correct in my current understanding? If correct are there things you feel I am leaving out? If incorrect, why? If partially correct, could you advise what is right or wrong?","['regression', 'machine-learning', 'statistics', 'radial-basis-functions']"
2436634,How to prove that this limit doesn't exits?,"The limit is $$ \lim_{(x,y)\to(0,0)} \frac{x\sin(y)-y\sin(x)}{x^2 + y^2}$$ My calculations: I substitute $y=mx$ \begin{align}\lim_{x\to 0} \frac{x\sin(mx)-mx\sin(x)}{x^2 + (mx)^2} &= \lim_{x\to 0} \frac{x(\sin(mx)-m\sin(x)}{x^2(1 + m^2)}\\ &= \lim_{x\to 0} \frac{1}{1+m^2}\bigg[\frac{\sin(mx)}{x}- \frac{m\sin(x)}{x}\bigg]\end{align} Can I say that the limit $$ \lim_{x\to 0}\frac{\sin(mx)}{x}$$ doesn't exist because it depends on $m$, so the entire limit doesn't exist?","['multivariable-calculus', 'limits']"
2436638,Determine the relationship between number of sales vs. percentage of the time that the phone is answered (instead of going to voicemail),"Problem Determine the relationship between the percentage of the time that the phone gets answered before going to voicemail and the number of sales that are made. So example output might be: f(43%) = 4 f(80%) = 10 f(95%) = 12 Note: I'm not a math professional so I apologize that I am unable to express this problem in the correct and formal way. Background I own a mobile auto detailing company. I wrote a computer program that routes all of our calls in and out of the company. It also has a scheduler where we put in all of our appointments (a new appointment is how I am defining a ""sale"" here.) So I have a huge list of phone calls about 13,000 in the past year - and I can go back at least 5 years if need be. I also have a huge list of all of the appointments that we've scheduled, including the times that the appointments were created. What I don't understand is how to process this data so that I can get an approximate relationship between the percentage of the time that we are answering the phone at any given time and how many appointments we are scheduling. This is especially difficult since we almost always return calls that are missed, and sometimes we do schedule appointments that way, so its hard to see the affect on answering the phone when they first call vs. calling them back. For all I know, the relationship could be inverted and we make more sales when we don't answer the phone as much (although this is highly unlikely) I am a computer programmer, so I can write up some code to process this data, but I don't know the correct statistical methods to process it properly. My end goal is to be able to understand things like ""Since we answered the phone 75% of the time today, and got 43 total
  calls, we probably would have scheduled 2 more jobs if we had answered
  the phone 20% more."" The only way I can think of to do this is do break all of the data into days, then plot it in a spreadsheet with one axis being percent of the time the phone was answered that day and the other axis being the ratio between appointments scheduled and total calls for that day - then draw a trend line - but this seems a little crude since the data is being broken up into days. For anyone who's interested, here's a screenshot of some of the information tallied by day: Edit I think what I had in mind when writing this original question was probably overkill for the problem. Essentially we are dealing with a few signals: Calls: _____1____1____0_1____1____1_____0___________0______ 
       0 = Missed, 1 = Answered

Sales: ______1_________1_________1____________1___________1
       1 = a sale was made There is probably some mathematical way to find the relationship between the two signals. Like looking at the froward time deltas between phone calls and sales being made that will expose the relationship between periods of time with greater and lesser answer rates vs. sales being made. The complexity here is that I was looking for a mathematical process that doesn't require the data to be pre-clustered into days (or some other time period). ...But enough of that! There is an easier and slightly less precise way to do this with ""common sense"" math if I just take the precision hit and pre-cluster into days. Frankly the results are more than good enough for the purposes here: For us to understand the approximate financial cost of missing calls (and the financial gain of higher answer rates). Thus I can balance this priority against other priorities. Now I can answer questions like Should I pay X dollars to hire another person to help answer the phone? Is it ok to miss a call during a team meeting? Do we have enough volume to hire another technician if we answer the phone more? Approximate Solution (Good-enough solution) I exported a year of data into Google Sheets, specifically I looked at:
   - Percent of phone calls answered 
   - Total appointments scheduled / Total calls for the day. (a higher percentage here means that we converted more of our inbound leads into sales) Each dot represents the ""Percent of Inbound Calls Answered"" for that day vs ""Number of Appointments Scheduled Divided by Total Inbound Calls"". As you can see, higher answer rates are correlated with a larger percentage of inbound leads being converted into sales (Note, all missed calls are always returned even if they don't leave a message): Then I added a trend line. If I understand what I did correctly, the trend line is basically saying that for each call we answer, there is about an 18% chance we will make an additional sale that day. Another way of looking at it is that for every 5.5 calls that we miss, we are losing  1 sale.","['statistics', 'computational-mathematics']"
2436668,"What is the domain and range of $f(x,y)=3x^2+2y^2-5$","What is the domain and range of $f(x,y)=3x^2+2y^2-5$ I'm just starting to learn multivariable, and need help on this question. Let $z=3x^2+2y^2-5$ The domain is $S=\{(x,y)\in\mathbb{R}^2|x,y\in(-\infty,\infty)\}$ There is no restriction on the domain. What about the range? I can't really imagine this curve and so I don't know for which values of $z$ it can cover. How can I solve this?","['multivariable-calculus', 'real-analysis', 'calculus']"
2436672,Show that $n^{\frac{n+1}{n}}$ is increasing,"How can one show that $n^{\frac{n+1}{n}}$ is increasing, possibly using logarithms but not derivatives?  I obtain $(n^2+2n)log(n+1)\geq (n^2+2n+1)log (n)$ but cannot proceed further.","['induction', 'real-analysis', 'sequences-and-series', 'limits']"
2436673,Support of Schemes,"I have a quesion about the confusing using of the term ""support of schemes"" in Lemma 2.3.41 in Liu's ""Algebraic Geometry"" (page 53): I know that the ""support of a sheaf $F$ on a scheme $X$"" is generally defined as the subset $S$ of $X$ so that for every $x \in S$ we have for the stalk $F_x \neq 0$. Why is this definition with the definition above the red line compatible (here for the closed subscheme of $\mathbb{P}_A ^n$ support $=$ underlying topol. space)?","['sheaf-theory', 'algebraic-geometry']"
2436680,Making sense of matrix derivative formula for determinant of symmetric matrix as a Fréchet derivative?,"I am familiar with the derivation of $D_A(\det(A)) := \dfrac{\partial \det(A)}{\partial A}$ as a Fréchet derivative by considering $$D_A[H] = \det(A+H) - \det(A) \; as \; \|H \| \rightarrow 0 = \det(A) \mathrm{tr} (A^{-1}H) = \det(A) A^{-T} \cdot H$$ for a general square matrix $A$. I have used the Frobenius inner product for writing the trace as an inner product. This leads to the same answer as that given in The Matrix Cookbook . But if $A$ is symmetric , how do I compute the Fréchet derivative  to be $$ \det(A) (2A^{-1} - \operatorname{diag}(A^{-1}) )$$
I am curious to know if we can derive this formula  that is quoted from The Matrix Cookbook in various posts, see for example -- What is the derivative of the determinant of a symmetric positive definite matrix?","['derivatives', 'frechet-derivative', 'matrix-calculus', 'determinant']"
2436692,Inverting a $2 \times 2$ matrix $\mod 26$,"I am trying to invert the matrix (in mod 26) \begin{bmatrix}
     19 & 7\\
    19 & 0 \\
\end{bmatrix} I compute the determinant 
((19*0)-(19*7)=23 (Reducing in mod 26) \begin{bmatrix}
     0 & -7\\
    -19 & 19 \\
\end{bmatrix}
Switch a & d and negate b & c Multiply by 23^-1 = 17 To get \begin{bmatrix}
     0 & 11\\
    15 & 25 \\
\end{bmatrix} But this can't be right since wolfram gives me \begin{bmatrix}
     0 & 15\\
    11 & 11\\
\end{bmatrix} What could I be doing wrong?","['number-theory', 'modular-arithmetic', 'linear-algebra']"
2436701,How many Big $O$ equivalence classes are there?,"I'm talking about the set theory definition of $O(f(n))$, which is the set of all functions $g$ such that $O(f(n))=O(g(n))$. What is the cardinality of $\{O(f(n)):f\in\mathbb{R}^\mathbb{R}\}$? We can denote this cardinality $\kappa$. We know that $\kappa\geq\mathfrak{c}$ (because for every real number $x$, $O(x^n)$ is distinct). We also know that $\kappa\leq\mathfrak{c}^\mathfrak{c}=2^\mathfrak{c}$, because the set it is the cardinality of has an injection onto $\{f:f\in\mathbb{R}^\mathbb{R}\}=\mathbb{R}^\mathbb{R}$. So, we have upper and lower bounds. Assuming GCH holds for $\aleph_0$ and $\aleph_1$ (i.e. $\mathfrak{c}=\aleph_1$ and $2^\mathfrak{c}=\aleph_2$), then it is shown that $\aleph_1\leq\kappa\leq\aleph_2$, meaning it is either $\mathfrak{c}$ or $2^\mathfrak{c}$. However, if ZFC can prove that $\mathfrak{c}<\kappa<2^\mathfrak{c}$, it would immediately disprove GCH. Assuming ZFC is consistent and $\mathfrak{c}<\kappa<2^\mathfrak{c}$, then ZFC can't prove this is true, and clearly can't give an exact cardinality for $\kappa$. So, I would encourage those solving this problem to try and either prove $\kappa=\mathfrak{c}$ or $\kappa=2^\mathfrak{c}$, or your won't get very far.","['asymptotics', 'elementary-set-theory']"
2436712,Comparing $\|f\|$ with $\|f''\|$ on an open interval,"So I was looking at one of my dad's old math exams and came across the following question: Suppose $f\in C^2((a,b))$ (open interval) and for some $a <x_1 <x_2 <b$ we have $f(x_1) =0 =f(x_2)$. Prove $\|f\| \le (b-a)^2 \|f''\|$ where $\|g\| = \sup_{x \in (a,b)} |g(x)|$. My first inclination was a FTC argument, but of course this can't work (for example, $1/x$ on $(0,1)$). I also considered the following: By Rolle's, there exists $c \in (x_1, x_2)$ such that $f'(c) =0$. Then Taylor expanding, we have $f(x) = f(c) + \displaystyle \frac12 (x-c)^2 f''(c) +g(x-c)$, where $g(x-c) \to 0$ as $x\to c$, but this was also fruitless. Any ideas? I feel like the problem must use Rolle's theorem somehow, but I'm not sure how.","['derivatives', 'real-analysis', 'calculus']"
2436717,Visualizing $\textbf{Q}_p$ vs. $\textbf{F}_p((t))$?,I have a few questions. How do I visualize the field $\textbf{Q}_p$ of $p$-adic numbers? How do I visualize the field $\textbf{F}_p((t))$ of Laurent series of $\textbf{F}_p$? How do I do 1 and 2 in a way as to make the similarities and differences between $\textbf{Q}_p$ and $\textbf{F}_q((t))$ as transparent as possible? Hopefully this question can be answered in a way that does not go too much off the deep end. I just want to visualize things! EDIT: I want pictures.,"['p-adic-number-theory', 'algebraic-geometry', 'algebraic-number-theory', 'arithmetic-geometry', 'visualization']"
2436730,Is the sum of two closed convex subsets of a real normed space closed?,"Let $K_1,K_2$ closed convex sets of a real normed space $V$, both containing 0. Is $K_1+K_2$ closed? Can you give-me a counterexample? I aim to prove the following result: Let $K_1,...,K_n $ closed convex sets of a normed space $V $, and let $c_1,...c_n $ positive real numbers. Prove that, if $x\in V$ can not be of the form $x=c_1x_1+\cdots c_nx_n $ for $x_i\in K_i$, $i=1,...,n $, then there exist $f\in V^*$ (the dual of $V $) such that $f (x)>1$ and $f (y)\leq \frac{1}{c_i}$ for all $y\in K$ and $i=1,...,n$. I can prove it if $c_1K_1+\cdots+c_nK_n $ is closed, but couldn't prove if this is true.","['functional-analysis', 'normed-spaces', 'general-topology', 'convex-analysis']"
2436745,$\operatorname{P}$ vs. $P$ in probability,"Below are three separate notations my textbook uses: $\operatorname{P}(X)$: the probability that an event $X$, which is a subset of the sample space, occurs $\operatorname{P}(X=x)$: the probability the random variable $X$ assumes the specific numerical value $x$ $P(x)$: probability distribution function , where $P(x)=\operatorname{P}(X=x)$ I understand that it’s generally claimed that upright font is meant to indicate operators while slanted or italicizes font is meant to indicate variables and functions, but in reality, the conventions are much more nuanced than that. So, why the care to distinguish $\operatorname{P}$ from $P$? How important is it to do so? Is this simply a regional convention? I have noticed that Europeans use upright symbols more than do Americans (e.g., $\mathrm{d}x$ vs. $dx$), which could account for what I’m seeing. If you’re curious, my textbook is Mathematics for the International Student: Mathematics HL (Core) third edition by Haese et al. and is Australian in origin.","['probability', 'notation', 'random-variables']"
2436786,"Show if an abelian group $G$ has a $\mathbb Q$-vector space structure, then it is unique.","Show if an abelian group $G$ has a $\mathbb Q$-vector space structure, then it is unique. Hint: show every element must have infinite order. Also, show the unique ring homomorphism $\mathbb Z\to \mathbb Q$ is an epimorphism. I have been able to prove both hints, but I can't put two and two together? Maybe use the fact that every abelian group is a $\mathbb Z$ module in exactly one way, and since the elements must have infinite order, the module structures of $\mathbb Z$ and $\mathbb Q$ must coincide? In that case, it would amount to show that the diagram created by the homomorphisms $\phi:\mathbb Z\to\mathbb Q$, $\sigma_1:\mathbb Z\to \mathrm{End}(G)$, and $\sigma_2:\mathbb Q\to \mathrm{End}(G)$ commutes, given that $\phi$ and $\sigma_1$ are unique.","['abstract-algebra', 'ring-theory', 'modules', 'vector-spaces']"
2436790,How to determine if a set is dense,"My question reads: Which of the following sets are dense? (Take $p\in\mathbb{Z}$ and $q\in\mathbb{N}$) ($a$) the set of all rational numbers $\frac{p}{q}$ with $q\leq\ 10$ ($b$) the set of all rational numbers $\frac{p}{q}$ with $q$ a power of 2. I am not too sure how to go about determining if the set if dense or not. I do know that for a set to be dense there must be an element in this set that can be found between any two real numbers $a<b$. For the first one I was looking at the fact that fractions would be a part of this set, but I am not sure if noticing this helps me. Then, for the second one, I think once I see a bit of how the first once works I will get it.","['real-analysis', 'rational-numbers', 'elementary-set-theory', 'discrete-mathematics']"
2436817,Probability Involving Summing the Number on the Balls,"Suppose three different balls will be randomly drawn without replacement from an urn containing nine balls, numbered $1$ through $9$. What is the probability that one of the three selected balls will be the one numbered $1$ given that the sum of the numbers on the three selected balls is $10$? Attempted Solution: The only cases where the three balls add up to $10$ are, {$1,4,5$},{$1,3,6$}, {$1,2,7$}, {$2,3,5$} Three of which have a one giving $p = .75$. Is this a valid solution? Is there a way to generalize this for balls numbered $1$ through $n$ with $m$ balls selected, and finding the probability of selecting a ball that is numbered $x$, given that the sum is $s$? Just curious about the last part. I think it ties in with partitions, except partitions allows for repeated values.","['combinatorics', 'statistics', 'probability']"
2436900,Calculate percentage of point in range that includes negative numbers,"I'm writing a computer program with a user interface that contains sliders with differing ranges. The range can be only positive, e.g. [1, 2] , or contain negative values, e.g. [-1,1] . So I need to be able to calculate the percentage of a number inside of a range, where the numbers can be negative or positive. For example I can have a range [-127, 127] , and if the value is 0 , it would be 50% . Another example using only positive numbers would be [0, 127] , where 0 would be 0% , but 63.5 would be 50% . I would also like to be be able to calculate a number on a range from a percentage, so I think this would be the inverse. I've been able to write functions that work for example 1 or 2, but not both. Introducing the negative numbers seems to add a lot of complexity (at least or me!) Many thanks.","['numerical-methods', 'linear-algebra']"
2436908,Every almost-Lebesgue measurable set is Lebesgue measurable.,"The following problem is from exercise 8 of Tao's introductory measure theory book. $\textbf{Prove:}$ If for all $\epsilon > 0$ one can find a Lebesgue measurable set $E_{\epsilon}$ such that $m^*(E_{\epsilon} \Delta E) \leq \epsilon$, then $E$ itself must be Lebesgue measurable. The hint that the book gives is: use the $\epsilon/2^n$ trick to show that $E \subset E_{\epsilon}'$ where $E_{\epsilon}'$ is measurable and $m^*(E_{\epsilon}' \Delta E) \leq \epsilon$; then I should take countable intersections to show that $E$ differs from a Lebesgue measurable set by a null set. The follow Lemma 10 will probably be useful: (i) Every open set is Lebesgue measurable. (ii) Every closed set is Lebesgue measurable. (iii) Every set of Lebesgue outer measure zero is measurable. (Such sets are called null sets.) (iv) The empty set is Lebesgue measurable. (v) If $E \subset {\bf R}^d$ is Lebesgue measurable, then so is its complement ${{\bf R}^d \backslash E}$. (vi) If ${E_1, E_2, E_3, \ldots \subset {\bf R}^d}$ are a sequence of Lebesgue measurable sets, then the union ${\bigcup_{n=1}^\infty E_n}$ is Lebesgue measurable. (vii) If ${E_1, E_2, E_3, \ldots \subset {\bf R}^d}$ are a sequence of Lebesgue measurable sets, then the intersection ${\bigcap_{n=1}^\infty E_n}$ is Lebesgue measurable. I am not sure at all how to follow the hint. Specifically I have been unable to come up with an $E_{\epsilon}'$ which satisfies the properties that I want. I find it very hard to work with $m^*(A \Delta B)$ in general. Does anyone have any tips how to construct $E_{\epsilon}'$? My guess is that we use the fact that $E_{\epsilon}$ is Lebesgue measurable in some way to approximate it from the outside, perhaps by an open set which contains $E$?","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2436923,How long would a track be?,"A and B are swimming in lanes right next to each other, but in different directions. They both start at the same time, and they pass each other after person A has swam 84 feet. When they reach the end, they turn around and swim back, meeting again 36 feet away from person A's starting point. How long is the pool? I would assume the two swimmers are swimming at different rates, but I don't know how I would go about this problem. I would assume trying to find ratios would be the best, but I don't know what to do. Thank you!",['algebra-precalculus']
2436943,finding limit of multivariate function using squeeze theorem,"$\lim \limits_{(x,y) \to (0,1)} \frac{x^2(y-1)^2}{x^2+(y-1)^2}$ trying all lines $y=mx+1$ and $x=0$ yields $0$ So, let's try this $0$ as a candidate of L. $$|\frac{x^2(y-1)^2}{x^2+(y-1)^2}-0| = \frac{x^2(y-1)^2}{x^2+(y-1)^2} \leq \frac{x^2(y-1)^2}{x^2} = (y-1)^2$$ which goes to $0$ as $y \rightarrow 1$ Hence by the squeeze/sandwich theorem $$\lim \limits_{(x,y) \to (0,1)} \frac{x^2(y-1)^2}{x^2+(y-1)^2} = 0$$ Would this be a complete answer?","['multivariable-calculus', 'limits']"
2436953,"In machine learning and statistics, why does the lasso path slope down to the right?","In Lasso regression, for a sparse estimate of coefficients $\beta$, we have: $$
\hat{\beta}(\lambda) = \arg \min_b \Bigl\{\frac{1}{2} \|y-Xb\|^2_{2} + \lambda\|b\|_1\Bigl\}
$$ One graph I saw that plotted out the coefficient values of $\beta$ vs. the $\lambda$ parameter is: My question is why the graphs slope downwards to zero? In other words, why is it that when we increase $\lambda$, our coefficient estimates tend to zero? I did a thought experiment where I let the $\lambda||b||_1$ term get large, but I fail to see the connection. In other words: 1) Is it the case Lasso Paths always slope to zero as $\lambda$ gets large? 2) Do coefficient values always start positive? (if $\lambda = 0$, then we have OLS). 3) What is the intuition here? Thanks!","['machine-learning', 'statistics']"
2436965,Showing Independence of quotient and sum of Chi-Squared Random Variables,"Let $X_i$ denote $\chi_{r_i}^2$ i.i.d random variables, where $r_i$ is a (possibly distinct) positive integer for each $i$.  I want to verify that $Y_1 = \frac{X_1}{X_2}$ and $Y_2 = X_1 + X_2$ are independent. I know that there are several ways of showing this - one way would be to try to compute the joint pdf of $Y_1$ and $Y_2$ and marginalize one out and show that it is the product of both of them - but I'm having trouble writing the joint pdf.  Another option would be to show that the conditional probabilities are equal to the individual probabilities.  But this too requires the pdf and while I know that $Y_2$ is a chi-squared random variable, it seems like it would be messy to do it this way too. Is there an more elegant way to show this?","['statistics', 'proof-writing', 'probability', 'probability-distributions']"
2436973,How many numbers $2^n-k$ are prime?,"We are all familiar with the Mersenne primes $$M_n = 2^n-1$$ and we indeed know that there are some $M_n$ that are prime. However, it is still open whether there are infinitly many $M_n$ that are prime. Now consider $$ F_n(k) = 2^n - k $$ with $k \geq 3$ and $k$ odd. I would like to know whether one can answer following questions for some $k$: (1) Is $F_n(k)$ prime for some $n$? (2) Is $F_n(k)$ prime for infinitely many $n$? (3) (If not) For how many $n$ is $F_n(k)$ prime? The first prime values of $F_n(3) = 2^n - 3$ are given in A050415 and the first prime values of $F_n(5) = 2^n - 5$ are given in A156560 . My hope is that it is easier to show (1), (2) and (3) for the form $2^n-k$ instead of $2^n-1$. In case someone is aware of papers, etc. concerning this 'generalization' I would be glad about the reference. Edit: Another example: For $k=5$ the first prime value is $F_{39}(5)=  549755813881$.","['mersenne-numbers', 'prime-factorization', 'number-theory', 'coprime', 'prime-numbers']"
2436976,Exponential of a block matrix,"Let $A=\begin{bmatrix}A_{11} & A_{12}\\0 & A_{22}\end{bmatrix}$ be a square matrix. Prove that $$e^{tA}=\begin{bmatrix}e^{tA_{11}} & F(t)\\0 & e^{tA_{22}}\end{bmatrix}$$ where
$$F(t)=\int_0^te^{(t-s)A_{11}}\cdot A_{12}\cdot e^{sA_{22}}ds.$$ I do not know where to start. Can you give me any hint?","['matrices', 'ordinary-differential-equations', 'linear-algebra']"
2437018,What is the closure of $\ell_1$ in $\ell_2$?,It is not difficult to show that the sequence space $\ell_1$ is a subspace of $\ell_2$ that is not closed in the $\ell_2$ topology (see Problem 2 in this pdf file for instance) What is the closure of $\ell_1$ in $\ell_2$?,"['functional-analysis', 'lp-spaces', 'sequences-and-series']"
2437026,Why is the square of a Bernoulli random variable still a Bernoulli random variable?,"Suppose that: $$
X \sim Bern(p)
$$ Then, intuitively $X^2 = X \sim Bern(p)$. However, when I try to think of it logically, it doesn't make any sense. As an example, $X$ is $1$ with probability $p$ and $0$ with probability $1-p$. Then, $X^2 = X\cdot X$ is $1$ only when both $X$'s are $1$, which occur with probability $p^2$, and so it doesn't seem like $X^2 = X$. Can someone tell me what is wrong here?","['probability-theory', 'probability']"
2437052,Density of multiplication table,"Is there any easy way to show $$\lim_{N \to \infty} \frac{1}{N^2}\#\{ab : 1 \le a,b \le N\} = 0$$ A quick calculation I did shows that the  number of positive integers $\le N^2$ with a prime divisor $p > N$ is at most the order of $(\log 2) \cdot N^2$, so just getting rid of the numbers with a high prime divisor is not sufficient.","['number-theory', 'analytic-number-theory', 'analysis', 'elementary-number-theory']"
2437065,Show that every subgroup of $S_n$ has either every member as even or exactly half the members as even permutations.,"Show that every subgroup of $S_n$ has either every member as even or exactly half the members  as even permutations. ATTEMPT : Consider the homomorphism $\phi:S_n\to \Bbb Z_2$
 as \begin{cases}
  1 & \text{if h is even}\\-1&\text{if h is odd}\end{cases}. Now if $H$ is a subgroup of $G$ so is $\phi(H)$ The only subgroups of $\Bbb Z_2$ are $\{1\},\{1,-1\}$. If $\phi(H)=1$ then $H$ has all permutations to be even. If $\phi(H)=\{1,-1\}$ then Number of even permutations $=$Number of elements of $S_n$ which are mapped to $1=\phi(H)=$Half of order of $\Bbb Z_2$ . Though there exist ways to prove it,I want to solve the problem using this procedure. Please tell if this way is correct or not.","['abstract-algebra', 'proof-verification', 'proof-explanation', 'group-theory', 'symmetric-groups']"
2437084,Vector Applications in Euclidean Geometry,"Pentagon $ABCDE$ is inscribed in a circle. For any edge of $ABCDE$, we can draw the line perpendicular to that edge that contains the centroid of the remaining three vertices. Show that these 5 lines are concurrent. We just finished a unit on vectors in my Pre-Calculus class but I have no idea on how to do this final problem. Any help would be greatly appreciated!","['algebra-precalculus', 'euclidean-geometry', 'linear-algebra', 'vectors']"
2437171,Complex integral with conjugate as an exponent,"What is the integral of 
  $$\int_{\Gamma}\pi e^{\pi\bar{z}}dz$$ where $\Gamma$ is the square with vertices at $0,1,1+i,i$ oriented anticlockwise? I am badly stuck at this problem. I thought of using the Residue theorem by using $\bar{z}=\frac{|z|^2}{z}$, but we get an essential singularity. Using Laurent series about zero, I get $a_{-1}=\pi|z|^2$. Is this correct? how do we proceed? Any hints. Thanks beforehand.","['complex-analysis', 'integration', 'contour-integration', 'residue-calculus']"
2437174,Rudin - Functional analysis excercise 12.7,"Suppose $U \in \mathcal{B}(H)$ is unitary, and $\epsilon > 0$. Prove that scalars $\alpha_{0}, ..., \alpha_{n}$ can be chosen so that 
\begin{align*}
\| U^{-1} - \alpha_{0}I - \alpha_{1}U - ... - \alpha_{n}U^{n} \| < \epsilon
\end{align*}
if $\sigma(U)$ is a proper subset of the unit circle, but that this norm is never less than $1$ if $\sigma(U)$ covers the whole circle.
For the first case, I have tried considering expressing $U$ in terms of $e^{iV}$ where $V$ is self adjoint, but can not proceed any further..Please Help!!!!!","['functional-analysis', 'spectral-theory']"
2437182,Prove that the perpendicular from the origin upon the straight line,"Prove that the perpendicular drawn from the origin upon the straight line joining the points $(c\cos \alpha, c\sin \alpha)$ and $(c\cos \beta, c\sin \beta)$ bisects the distance between them. My Attempt:
Equation of the line joining the points $(c\cos \alpha, c\sin \alpha)$ and $(c\cos \beta, c\sin \beta)$ is:
$$y-c\sin \beta=\dfrac {c\sin \alpha - c\sin \beta}{c\cos \alpha- c\cos \beta} (x-c\cos \beta)$$
$$y-c\sin \beta =\dfrac {\sin \alpha - \sin \beta}{\cos \alpha - \cos \beta} (x-c\cos \beta)$$
$$x(\sin \alpha - \sin \beta)-y(\cos \alpha - \cos \beta)=c \sin \alpha. \cos \beta - c \cos \alpha. \sin \beta$$
$$x(\sin \alpha - \sin \beta) - y (\cos \alpha - \cos \beta)= c\sin (\alpha - \beta)$$","['analytic-geometry', 'geometry']"
2437183,"Arrangements of $5$ $\alpha$s, $5$ $\beta$s and $5$ $\gamma$s with at least one $\beta$ and at least one $\gamma$ between each two $\alpha$s","How many arrangements of $5$ $\alpha$s, five $\beta$s and five $\gamma$s are there with at least one $\beta$ and at least one $\gamma$ between each successive pair of $\alpha$s? My attempt: case 1: exactly $1$ $\beta$ and $1$ $\gamma$ b/w each pair of $\alpha$s $= 96$ ways case 2: exactly $1$ $\beta$ and $2$ $\gamma$s b/w each pair of $\alpha$s $= 192$ ways case 3: exactly $2$ $\beta$ and $1$ $\gamma$ b/w each pair of $\alpha$s $= 192$ ways Don't know how to compute case 4. I mean I know but the answer is not matching. case 4 would be exactly $2$ $\beta$s and $2$ $\gamma$s b/w each pair of $\alpha$s. If anyone knows another best possible solution please provide or else just help me with case 4.
Thanks!","['permutations', 'combinatorics', 'combinations']"
2437191,"Prove that $f(x,y)$ is totally differentiable in $ (0,0)$","I have the following function: $f(x,y)=\frac{x^{2}y}{\sqrt{x^{2}+y^{2}}}$, when $(x,y)\neq (0,0)$ and $f(x,y) =(0,0)$, when $(x,y)=(0,0)$. I have to prove that f is totally differentiable, I tried doing this using the the theorem that $f$ is totally differentiable in the point $\xi $ if there exists a linear image $A$ such that: $lim \frac{\| f(x)-f(\xi)-A(x-\xi)\|}{\|x-\xi\|}=0$, when $x\rightarrow \xi$. I now substitute $A$ by the jacobi matrix of $f(\xi _{1},\xi _{2})$ to show that indeed this is true where the jacobi matrix is equal to: $A=(\frac{\partial}{\partial \xi _{1}}f(\xi _{1},\xi _{2}),\frac{\partial}{\partial \xi _{2}}f(\xi _{1},\xi _{2}))=(\frac{\xi _{2}(\xi _{1}^{3}+2\xi _{1}\xi _{2}^{2})}{(\xi _{1}^{2}+\xi _{2}^{2})^{3/2}},\frac{\xi _{1}^{4}}{(\xi _{1}^{2}+\xi _{2}^{2})^{3/2}})$ and thus it schould hold that: $lim \frac{\| f(x,y)-f(\xi_{1},\xi_{2})-A(\xi_{1},\xi_{2}).((x,y)-(\xi_{1},\xi_{2}))^{T}\|}{\|(x,y)-(\xi_{1},\xi_{2})\|}=0$, when $(x,y)\rightarrow (\xi_{1},\xi_{2})$ where we have that $(\xi_{1},\xi_{2})=(0,0)$, my problem is that $A(0,0)$ doesn't exist. I don't really know where to go from here, am I doing something wrong?","['multivariable-calculus', 'limits']"
2437203,Inversion set determine an element in a symmetric group.,"Let $S_n$ be the symmetric group over $\{1,2,\ldots,n\}$. The inversion set of a element $w$ in $S_n$ is defined by $Inv(w)=\{(i,j): i<j, w^{-1}(i)>w^{-1}(j)\}$. I think that if $w, w' \in S_n$ and $Inv(w)=Inv(w')$, then $w = w'$. Is this true? Thank you very much.",['group-theory']
2437207,Calculating area of quadrilateral when distance of vertices from an arbitrary point is known,"Given a convex quadrilateral $ABCD$ circumscribed about a circle of diameter $1$. Inside $ABCD$ there is a point $M$ such that $MA^2 + MB^2 +MC^2 + MD^2 =2$. Find the area of the quadrilateral. My attempt at a solution: I tried to solve it using coordinate geometry. I guessed $M$ to be on centre of circle and taking $ABCD$ as a square, the sides come out to be $1$. Hence, area comes out to be $1$ unit sq. However I was looking for a proper solution for the question.","['circles', 'a.m.-g.m.-inequality', 'quadrilateral', 'coordinate-systems', 'geometry']"
2437219,the correct notation for sequences,"My question is about something formal. I recently learned about sequences for the first time, and initially, they were denoted by $(a_n)_{n\in\mathbb{N}}$ (where each $a_n$ is in $\mathbb{N}\cup \{\infty\}$, or in $\mathbb{R}$ for instance) . But then, in different contexts, I have seen the notations $(a_n)_{n=1}^{\infty}$, $\{a_n\}_{n\in\mathbb{N}}$ and $\{a_n\}_{n=1}^{\infty}$. Can I vary the notations as above for writing down sequences? Or is there just one correct way how to do it and everything else is just sloppy notation (and strictly speaking, 'false notation')?","['notation', 'sequences-and-series', 'elementary-set-theory']"
2437246,Writing Formal version of a statement,How do i write the formal version of the following statement: No integer is both non-positive and non-negative unless it is the zero integer. Should i split the statement into the form $p$ unless $q$ with the predicate domain as $\mathbb{Z}$? I will appreciate a detailed explanation on the interpretation of the statement in its predicate statement form instead of only providing the answer. Thanks.,"['predicate-logic', 'logic', 'first-order-logic', 'discrete-mathematics']"
2437293,Properties of inverse distribution function - continuity,"I have some questions about inverse distribution functions. Let $F : \mathbb R \to [0,1]$ be a distribution function and define $F^{-1} : [0,1] \to \overline{\mathbb R}$ by $F^{-1}(y) := \inf\{x \in \mathbb R; F(x) \ge y\}$ with the convention that $\inf \emptyset := +\infty$. Futhermore, define $F^{-1+} : [0, 1] \to \overline{\mathbb R}$ by $F^{-1+}(y) := \sup\{x \in \mathbb R; F(x) \le y\}$, where $\sup \emptyset := -\infty$. The function $F^{-1}$ is the usual quantile function. I managed to prove the following properties: $F^{-1}$ and $F^{-1+}$ are non-decreasing. If $F^{-1} \in (-\infty, \infty)$, $F^{-1}$ is left-continuous at $y$ and admit a limit from the right at $y$. For $y \in \mathbb R$, set $A_y := \{x \in \mathbb R; F(x) = y\}$. Then, $(F^{-1}(y-), F^{-1}(y+)) \subseteq A_y$. Suppose that $F$ is strictly increasing. Then, $F^{-1}$ is continuous. It would be nice, if I could prove the properties 2 and 4 also for $F^{-1+}$. Do they hold also for $F^{-1+}$? I needed property 3 to prove 4.","['probability-theory', 'probability', 'probability-distributions']"
2437301,Nonlinear differential equation: reduce from second order to first order,"I've been working on this differential equation for days now but I can not find a solution. I understand that finding a solution in closed form is difficult to say (assuming there is).
What I ask is if there is a substitution (change of variables) that can ""transform"" this differential equation from the second order to the first order so that it is not too constrained.
Thank you for your interest and time. $$y''(x)+ a\ x\cos y(x)=0 $$ where $a \in \mathbb{R}\ $","['nonlinear-analysis', 'integration', 'special-functions', 'ordinary-differential-equations', 'trigonometric-integrals']"
2437307,Finding all non-negative integers solutions to $x_1+x_2+x_3+...+x_6=20$ such that $x_{2n+1} \le x_{2n+2}$ for $0 \le n \le 2$,"Solve $x_1+x_2+x_3+x_4+x_5+x_6=20$ such that  $x_{2n+1}\leq x_{2n+2}, 0\leq n \leq2$ Edit : I solved it. Let $0 \le a,b,c \le 20$ such that $0 \le a+b+c \le 20$ $x_1+x_2+x_3+x_4+x_5+x_6=20 \;and\ x_{2n+1}\leq x_{2n+2}\\
\implies x_2=x_1+a,\;x_4=x_3+b,\;x_6=x_5+c\\
\implies 2(x_1+x_2+x_3)+a+b+c=20\\
\implies 2(x_1+x_2+x_3)=20-a-b-c\;\;\;(1)$ So now we need for a fixed values of $a,b,c$ to find the number of non-negative integers solutions to the equation at $(1)$ Now, for all $0 \le i \le 20 $ Let $A_i$ be the set of all non negative solutions for $a+b+c=i$ Let $X_i$ be the set of all non negative solutions for $2(x_1+x_2+x_3)=20-i$ Now, $2(x_1+x_2+x_3)=20-i \implies (x_1+x_2+x_3)= \frac{20-i}{2}$ And since we are interesting in non-negative integers we can say that $20-i\nmid 2 \implies X_i=\emptyset \implies |X_i|=0\;\;\;(2)$ Otherwise, If $i$ is even we will want to compute $\left|A_i\right|\cdot \left|X_i\right|$  because for any solution a+b+c=i in $A_i$ there's the corresponding solution in $X_i$ Therefore we have: $\sum _{i=0}^{20}\:\left(\left|A_i\right|\cdot \left|X_i\right|\right)=\sum _{i=0}^{20}\:\begin{pmatrix}i+3-1\\ \:i\end{pmatrix}\begin{pmatrix}10-i+3-1\\ \:10-1\end{pmatrix}\\ \text{and from (2)}\\\sum _{i=0}^{20}\:\begin{pmatrix}i+3-1\\ \:i\end{pmatrix}\begin{pmatrix}10-i+3-1\\ \:10-1\end{pmatrix}= \sum \:_{i=0}^{10}\:\begin{pmatrix}2i+3-1\\ \:\:2i\end{pmatrix}\begin{pmatrix}10-i+3-1\\ \:\:10-i\end{pmatrix}=\\ \sum _{i=0}^{10}\:\left(\frac{\left(2i+2\right)\left(2i+1\right)}{2}\right)\left(\frac{\left(12-i\right)\left(11-i\right)}{2}\right)=9009$ And we've got a palindrome.","['combinatorics', 'discrete-mathematics']"
2437311,Computing $\int_0^1 \sin(3\pi x)\sin(2\pi x) \mathrm{d}x$,"How do I show
  $$\int_0^1 \sin(3\pi x)\sin(2\pi x) \mathrm{d}x=0.$$ I tried using
$$\sin (x)  \sin (y)  = \cos ( x \pm y ) \mp \cos (x)  \cos (y)  $$ Then the integral becomes $$\int_0^1\cos(3\pi x)\cos(2\pi x) \mathrm{d}x$$ Which doesn't help me. I also tried using Taylor series expansion of sinus but I don't see how it is helpful since we have the product of two infinite sums.","['definite-integrals', 'integration', 'trigonometry', 'calculus']"
2437313,Prove that $f$ is differentiable at $x=0$,"Need to prove: If $f$ is a real-valued function defined for all $x\in\mathbb{R}$ and $\forall x\in\mathbb{R},|f(x)|\leq x^2$ then $f$ is differentiable at $0$ I couldn’t find any leads, help is much appreciated.","['derivatives', 'real-analysis', 'calculus']"
2437345,Maximum value of $\sin (x)\;\sin (2x)\;\sin (3x)$ [duplicate],This question already has answers here : Prove that $\sin x \cdot \sin (2x) \cdot \sin(3x) < \tfrac{9}{16}$ for all $x$ (2 answers) Closed 6 years ago . To prove : $\sin (x)\;\sin (2x)\;\sin (3x) \lt 9/16$ With some transformations i was able to prove the above result using calculus but I am not getting the way to solve it without the use of calculus I've tried grouping $\sin(x)\;\sin(3x)$ and then using product to sum transformations. Also I've tried to find the maximum value of sinxsin3x and then using the fact that sin2x is a fraction. Although I was able to prove that sinxsin3x is less than 9/16 but the minimum value was less than -9/16 and since sin2x can be negative so i could not draw any conclusion.,"['inequality', 'trigonometry']"
2437350,There does not exist any holomorphic function $f$ in the open unit disc such that $f\left(\frac{1}{n}\right)=\frac{(-1)^n}{n^2}$,"I want to show that There does not exist any holomorphic function $f$ in the open unit disc such that $$f\left(\frac{1}{n}\right)=\frac{(-1)^n}{n^2}, n=2,3,\ldots $$ My attempt: Suppose there exists such a holomorphic function. Now 
$$f\left(\frac{1}{n}\right)=\begin{cases}\frac{1}{n^2}, & \text{if } n\ \text{is even.}\\
-\frac{1}{n^2}, & \text{if } n\ \text{is odd.} \end{cases}$$ Which says that $f(z)=z^2$ and $f(z)=-z^2$, which is not possible. Therefore, there does not exit any such holomorphic function. Is my argument fine? EDIT I used  the identity theorem to conclude that. Take $g(z)=z^2$. Since $$ f\left(\frac{1}{n}\right)=g\left(\frac{1}{n}\right)=\frac{1}{n^2} $$ As, $\frac{1}{n}\to 0\in \mathbb{D}(0,1)$ so using identity theorem I conclude that $f\equiv g$ on $\mathbb{D}$. Similar argument shows that $f(z)=-z^2$, and hence contradiction.","['complex-analysis', 'holomorphic-functions']"
2437450,Minimize $\|x\|_\infty$ such that $Ax=b$,"I have $A \in \mathbb{R^{n,m}}$ with $n\leq m$ and $b \in \mathbb{R^{n}}$. $A$ is of rank $n$ (maximal possible rank). I'm looking for $x \in\mathbb{R^{m}}$ such that $Ax=b$ and for which $\|x\|_\infty$ is minimal. Some looking around the Internet tells me the solution would be $x= (AA^T)^{-1}A^Tb$ if I were to be interested in minimizing $\|x\|_2$, but I'm really interested in minimizing $\|x\|_\infty$. If that can help give a more detailed solution, the case I'm particularly interested in is $(n,m)=(3,4)$. Do you see how to solve to this problem ?","['optimization', 'linear-algebra']"
2437459,"Example of tensor $(0,2)$ acting on two vectors","After much shifting through notational hurdles, I may have gotten the point. However, I'd like to confirm this unequivocally by working through an example. If $\beta \in V^*$ is $\beta=\begin{bmatrix}1 &2 &3 \end{bmatrix}$ and $\gamma\in V^*$ is $\gamma=\begin{bmatrix}2 &4 &6 \end{bmatrix}$. The $(2,0)$-tensor $\beta\otimes \gamma$ is the outer product: $$\beta\otimes_o \gamma=\begin{bmatrix}2\,e^1\otimes e^1&4\,e^1\otimes e^2&6\,e^1\otimes e^3\\4\,e^2\otimes e^1&8\,e^2\otimes e^2&12\,e^2\otimes e^3\\6\,e^3\otimes e^1&12\,e^3\otimes e^2&18\,e^3\otimes e^3\end{bmatrix}$$ Now if apply this tensor product on the vectors $$v=\begin{bmatrix}1\\-1\\5\end{bmatrix}, \; w = \begin{bmatrix}2\\0\\3\end{bmatrix}$$ $$\begin{align} (\beta \otimes \gamma)[v,w]&=\\[2ex]
& 2 \times  1 \times 2   \quad+\quad    4 \times   1  \times  0   \quad +\quad    6  \times  1  \times 3 \\
+\;&4 \times -1 \times 2  \quad + \quad   8 \times  -1  \times  0   \quad + \quad  12  \times -1  \times 3 \\
+\;&6 \times  5 \times 2  \quad + \quad  12 \times   5  \times  0  \quad  + \quad  18  \times  5  \times 3 \\[2ex]
&= 308\end{align}$$ Is this correct?","['tensor-products', 'differential-geometry', 'tensors', 'linear-algebra']"
2437511,If $f'/f=g'/g$ at every $1/n$ then $f=kg$ for some complex number $k$,"Suppose $f(z)$ and $g(z)$ are analytic in domain $D$, $f(z)$ and $g(z)$ never vanish at any $z\in D$ and that 
   $$ \frac{f'(z_n)}{f(z_n)}=\frac{g'(z_n)}{g(z_n)} $$ at a sequence  of points $\{z_n\}$ converging to $z_0\in D$. Show that $f=Kg$ for some $K\in \mathbb{C}$. My Solution: As $f,g$ are analytic in $D$ and none of them have any zero in $D$ implies $\frac{f'}{f}$ and $\frac{g'}{g}$ are also analytic on $D$. As $\frac{f'(z_n)}{f(z_n)}=\frac{g'(z_n)}{g(z_n)}$ and $z_n\to z_0
\in D$, using the identity theorem, $\frac{f'}{f}\equiv \frac{g'}{g}$ on $D$. 
Now, 
\begin{align*}
\frac{f'(z)}{f(z)}=\frac{g'(z)}{g(z)},\ \forall\ z\in D.
\end{align*}
Solving this differential equation gives me $f=Kg$ for some $K\in \mathbb{C}$. Now my question is after doing the integration we will get $\log(f)=\log(Kg)$, is it valid in the complex logarithm?",['complex-analysis']
2437513,Problem with Ordered Triples,"I have been doing set theory for a while, and for a long time I have understood Kuratowski's inductive definition for ordered $n$-tuples: $(x_1,x_2) := \{\{x_1\},\{x_1,x_2\}\}$ $(x_1,x_2,...,x_{k+1}) := ((x_1,x_2,...,x_k),x_{k+1})$ Pretty quickly I understood and accepted the beautiful result that $(x_1,...,x_n) = (y_1,...,y_n)$ if and only if $x_i = y_i$ for all $i \in \{1,...,n\}$, so I moved on and accepted this definition, thinking about ordered n-tuples as abstract objects again - happy that the set theory under the hood was working as it was supposed to. But today I noticed a problem with this definition that I didn't see, and it is to do with ordered triples (or ordered $n$-tuples for $n \geq 3$ to be honest). Given two arbitrary sets $a$ and $b$, the ordered triple $(a,a,b)$ is defined as $((a,a),b) = \{\{\color{blue}{(a,a)}\},\{\color{blue}{(a,a)},b\}\} = \{\{\color{blue}{\{\{a\}\}}\},\{\color{blue}{\{\{a\}\}},b\}\}$. In other words $(a,a,b) = (\{\{a\}\},b)$, and this holds for all sets. Now usually we don't have a domain which includes both $a$ and $\{\{a\}\}$ (or it is clear if we are talking about ordered pairs vs ordered triples), so the distinction is usually clear. But in abstract mathematics that is certainly not always the case. For example take the set $X = \{ \{\},\{\{\}\},\{\{\{\}\}\},...\}$. Then we can imagine the set of arbitrarily long $n$-tuples of $X$, the set $Y = \bigcup_{k=1}^{\infty}(\prod_{i=1}^k X)$. Then because of the above situation, many very simple and intuitive functions just straight up can't be defined. Take the function $P_3 \colon Y \to X \cup \{*\}$ given by $P_3(x_1,...,x_k) = \begin{cases} x_3 &\mbox{if } k \geq 3 \\ 
* & \mbox{if } k < 3 \end{cases}$ where the set $*$ is some set not contained in $X$. Straight away we run into problems as $P_3(\{\},\{\},\{\}) = \{\}$ and $P_3(\{\{\{\}\}\},\{\}) = *$, which gives us $P_3(\{\},\{\},\{\}) \neq P_3(\{\{\{\}\}\},\{\})$. This is a problem because under Kuratowski's definition of ordered $n$-tuples, we have that $(\{\},\{\},\{\}) = (\{\{\{\}\}\},\{\})$. I know that there are alternative definitions for ordered $n$-tuples, for example a definition such as: $(\color{red}{x_1},\color{blue}{x_2},\color{green}{x_3},...,x_k) := \{\color{red}{\{x_1\}},\color{blue}{\{x_2,\{x_2\}\}},\color{green}{\{x_3,\{x_3\},\{\{x_3\}\}\}},...\}$ where the $i$th entry nests $i-1$ times would solve this problem (at least when the axiom of regularity holds), but since Kuratowski's definition is so widely accepted there must be something I am overlooking. I would love to know why this is not an issue in general mathematics. EDIT: As (somewhat) pointed out in the comments, an even better definition for ordered $n$-tuples would be: $(x_1,...,x_n) := \{\{n\}, \{n, (x_1,...,x_n)_K\}\}$, where $(x_1,...,x_n)_K$ is Kuratowski's definition of ordered $n$-tuples as this solves the problem and holds even in the absence of the axiom of regularity . And even though it has been mentioned that this definition seems to depend on a prior construction of the natural numbers (i.e. the axiom of infinity ), I would disagree as this is an inductive definition - allowing you to define each $n$ sequentially at each inductive step, and does not require the set $\mathbb{N}$ to exist.","['set-theory', 'logic', 'functions']"
2437524,Asymptotics of $\sum_{0 \leq j \leq k \leq n-1} {\binom{2n}{j}}{\binom{2n-1}{k}}^{-1}$,"For any integer $n\geq 1$, define $$f(n) = \sum_{0 \leq j \leq k \leq n-1} \frac{\binom{2n}{j}}{\binom{2n-1}{k}}$$ Our lecture says that $f(n) = n +n\log n +O(1)$. But I cannot prove it, the best result I've gotten is $f(n) \leq cn^{\frac{3}{2}}$, and now I'm kind of doubtful about the result. Is it possible to prove it or deny it?","['combinatorics', 'binomial-coefficients', 'asymptotics', 'analysis']"
2437537,non-$\sigma$-finite translation invariant measures on $\mathbb{R}$,"It's known that the $\sigma$-finite translation invariant measures $\mu$ on $(\mathbb{R}, B(\mathbb{R}))$ are exactly the measures $ \mu = c \cdot \lambda$ where $c \in [0,\infty)$ and $\lambda$ is the Lebesgue measure. (An application of Fubini's theorem gives that any candidate for $\mu$ is absolutely continuous with respect to $\lambda$ and then it's not hard to check that the Radon-Nikodym derivative is a.s. constant) I'm interested in the case where we don't assume $\mu$ is $\sigma$-finite. It's clear that if $\mu$ has atoms then it is a scalar multiple of the counting measure. This leaves the case where $\mu$ is atomless. Such measures certainly exist, for example consider $\mu$ defined by
\begin{align*}
\mu(A) = \begin{cases} 
      0 & \mbox{if } A \mbox{ is countable } \\
      \infty & \mbox{otherwise}
   \end{cases}
\end{align*}
or $\mu = \infty \cdot \lambda$ with the convention $\infty \cdot 0 = 0$. Is there a nice classification of the non-$\sigma$-finite translation invariant measures on $(\mathbb{R},B(\mathbb{R}))$? If not can we easily find many more examples than the three I give above?","['real-analysis', 'measure-theory']"
2437653,"Calculate the line integral $I(a,b)=\int_{x^2+y^2=R^2}\limits\ln\frac{1}{\sqrt{(x-a)^2+(y-b)^2}} ds\quad(a^2+b^2\ne R^2).$","Calculate the line integral $$I(a,b)=\int_{x^2+y^2=R^2}\limits\ln\frac{1}{\sqrt{(x-a)^2+(y-b)^2}} ds\quad(a^2+b^2\ne R^2).$$ The parametrized integral path can be given as
$$x=R\cos t,y=R\sin t,t\in[0,2\pi].$$
Then I get into trouble when computing the integral
$$-\frac{1}{2}\int_{0}^{2\pi}R\ln(a^2+b^2+R^2-2aR\cos t-2bR\sin t) dt.$$
Should I take the derivative with respect to $R$ first? Or apply integration by parts? Update: I find the offered answer is
$$I(a,b)=-2\pi R\ln\max\{R,\sqrt{a^2+b^2}\}$$
and I have examined its correctness.","['line-integrals', 'calculus', 'multivariable-calculus', 'integration', 'definite-integrals']"
2437666,Flipping the order of an infinite series and assigning it a value?,"I was recently wondering if there was some method to ""flip"" the series and still assign it a value for divergent series. To illustrate what I mean: $$ S(x,n) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n $$ We would like to define the flipped sum series: $$ F(S(x,n)) = a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n} $$ Hence, to do this we begin by: assigning $x \to x^{-1} $ and take $x^n$ common: $$  S(x^{-1},n) = (a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n}) x^{-n}  $$ Multiplying both sides with $x^n$: $$ S(x^{-1},n) x^n =  (a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n})$$ Taking limit $n \to \infty$: $$ \lim_{n \to \infty}S(x^{-1},n) x^n =  \lim_{n \to \infty}(a_n  + a_{n-1} x^{1} + \dots + a_0 x^{n}) $$ Taking the second limit $x \to 1$: $$\lim_{x \to 1 } \lim_{n \to \infty}S(x^{-1},n) x^n = \lim_{n \to \infty} (a_n  + a_{n-1}  + \dots + a_0 )$$ Questions Is the above proof correct? Can this somehow be extended to zeta regularization?","['zeta-functions', 'summation', 'sequences-and-series', 'analysis']"
2437679,Counter example: strong convergence does not imply convergence in operator norm [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Consider a family of operators $T_n\in\mathcal{L}(X)$ where $X$ is a separable Hilbert space. Find examples in which $T_n$ converges strongly to $T\in \mathcal{L}(X)$, i.e. $\|Tx-T_nx\|\to 0$ as $n\to \infty$ for all $x\in X$, but not in operator norm topology, i.e. $\|T-T_n\|_{op}\to 0$ as $n\to \infty$.","['uniform-convergence', 'hilbert-spaces', 'operator-theory', 'functional-analysis', 'strong-convergence']"
2437708,sigmoid $\circ$ sigmoid = sigmoid?,"The question is whether the concatenation of two sigmoid function must again be a sigmoid function. Definition . A bounded and two-times differentiable function $f:\Bbb R\to\Bbb R$ is called sigmoid function if $f'\not=0$ and there is exactly one $x\in\Bbb R$ with $f''(x)=0$. Here is an example: Given two sigmoid-functions $f$ and $g$, the function $h:=f\circ g$ is obviously bounded and two times differentiable. Also $h'\not=0$ is not hard to see. I even know that there is an inflection point (i.e. a value $x$ with $h''(x)=0$). But I failed to show that there is only one! I admit that my initial motivation for this question comes from this post. I thought this would be one way to attack it in an elegant way. Still hard, though. Also no idea if there might be counterexamples.","['examples-counterexamples', 'function-and-relation-composition', 'functions']"
2437725,Property of normally embedded subgroups,"Definition : A subgroup $H$ of a group $G$ is said to be normally embedded in $G$ if each Sylow $p$-subgroup of $H$ is a Sylow $p$-subgroup of some normal subgroup $N$ of $G$. Definition : A subgroup of $H$ of $G$ is pronormal if for each $g\in G$, the subgroups $H$ and $H^g$ are conjugate in $\langle H, H^g \rangle$. Proposition : If $G$ is a finite solvable group with $H$ normally embedded in $G$, then $H$ is pronormal in $G$. The following is a exercise in Finite Soluble Groups by Doerk and Hawkes (Exercise 1.7.3) Let $G$ be a finite group with normally embedded subgroup $H$. Then $G = KN_G(H \cap K)$ for all normal subgroups $K$ of $G$. The above problem makes no mention of solvability of $G$. However I am not able to show the desired equality unless I assume is solvable. Let $K \unlhd G$. Then it is easy to show that $H \cap K$ is normally  embedded in $G$. If I assume $G$ is solvable, then $H\cap K$ is pronormal in $G$. So for $g \in G$, there is $x\in \langle H\cap K, H^g \cap K\rangle \leq K$ such that $(H\cap K)^x = (H \cap K)^g$. This implies that $x^{-1}g \in N_G(H \cap K)$, and so $g\in xN_G(H \cap K) \subseteq KN_G(H \cap K)$. Thus $G = KN_G(H\cap K)$. I'm not sure if this can be proven without the solvablity of $G$",['group-theory']
