question_id,title,body,tags
4251549,Eigenfunction differential equation with boundary values,"Consider the differential equation $$f'' + 2f' + (\lambda + 1)f = 0, \ \ \ \  f(0) + f'(0) = 0, f(L) = 0.$$ We can make $g(x) = e^x f(x)$ , so our differential equation becomes $$g'' + \lambda g = 0.$$ Equations like these have nice closed-form solutions. However, the boundary conditions are confusing me. Now we have $$g(0) + g'(0) = f(0), \ \ \ \ g(L) = 0.$$ Since we don't know what $f(0)$ , I don't understand how we can possibly solve this. Does anyone know how to solve this?","['ordinary-differential-equations', 'eigenfunctions']"
4251658,"Find all functions $f:\mathbb{R} \to \mathbb{R},$ which is continuous in $\mathbb{R}$ then $f(x-y)+f(y-z)+f(z-x)+27=0.$","Find all functions $f:\mathbb{R} \to \mathbb{R},$ which is continuous in $\mathbb{R}$ then $$f(x-y)+f(y-z)+f(z-x)+27=0.$$ I actually don't have any ideas to deal with it, but here is some tries: Let $x=y,y=z,z=x$ then we have $3f(0)=-27\Rightarrow f(0)=-9.$ Let $z=y$ then $f(x-y)+f(y-x)+18=0.$ Let $y=0$ then $f(x)=-18 -f(-x).$ Also, if we let $x-y=a;y-z=b;z-x=c\Rightarrow y-x=b+c.$ Thus $f(a)+f(b+c) +18 =0; f(a)+f(b)+f(c)+27=0,$ and $a+b+c=0.$","['functional-equations', 'functions']"
4251683,Deducing vanishing of a cohomology class from pairings,"Let $M$ be an oriented closed manifold and $G$ a group. Let $x$ be a class in $H^*(M;\mathbb{Q})$ . Suppose that for every class $y\in H^*(BG,\mathbb{Q})$ and every continuous map $$f\colon M\to BG,$$ we have that $$\langle x\cup f^*y,[M]\rangle=0,$$ where $[M]$ is the fundamental class in $H_*(M,\mathbb{Q})$ . Question: Does it follow that $x=0$ ? Comment: I guess it doesn't really matter whether we are working over the rationals or some other ring, or whether $M$ is a manifold, but this is the situation I'm looking at so I've included these details.","['classifying-spaces', 'algebraic-topology', 'differential-geometry']"
4251703,The lower bound of $|3^p - 2^q|$ - how to derive from Baker's theorem?,"In his blog , Terence Tao discussed the lower bound of $\vert 3^p - 2^q \vert$ in the following corollary. Corollary 4 (Separation between powers of $2$ and powers of $3$ ) For any positive integers $p$ , $q$ one has $$|3^p - 2^q| \geq \frac{c}{q^C} 3^p$$ for some effectively computable constants $c, C > 0$ (which may be slightly different from those in Proposition 3). This appears to be a well-established result. How do I derive this result from Baker's theorem?","['exponentiation', 'diophantine-approximation', 'number-theory', 'integers', 'asymptotics']"
4251713,"Computing zillions of digits of the ""derangement constant""","This is a sort of inspired sequel to the following question: Evaluating $\sum\limits_{x=2}^\infty \frac{1}{!x}$ in exact form. where the question is the discussion of the "" $e$ -like constant"" $$\sum_{n=2}^{\infty} \frac{1}{!n}$$ which is at least visually similar to the series for $e$ , i.e. $$\sum_{n=0}^{\infty} \frac{1}{n!}$$ except there's also an indexing difference. I call the former the ""derangement constant"" ( $ɘ$ ?), for lack of a better name. This number is about 1.638. I want more digits. Millions; billions maybe. Kind of like $\pi$ , but $\pi$ is old stuff; need a new kid on the block. :) Actually the reason is because that comments on that question were talking about how that the series for the derangement constant converges fast, superlinear actually in a digit-for-digit sense, which at first seems like we should be able to use it to compute millions of digits. But in reality, things aren't so simple - ""fast convergence"" alone by no means does a fast algorithm make for a computer. On a computer, it's not just the amount of terms that matters (i.e. the convergence rate), but also how many digits you need to keep for each term, because the computer has to go through the digits and add and multiply them. Addition, of course, is cheap; multiplication is not - in this domain, it's typically done using tricked out and once written, often kept secret, code based on Fast Fourier Transform (FFT) methods, which for numbers of $n$ digits have complexities $$O(n \log n\ a(n))$$ for some slow-grow function $a(n)$ . The ideal FFT has $a(n) = 1$ , but this is unachievable in practice because of precision limits of the computer processor. We can call the complexity above $M(n)$ , or complexity of multiplication of $n$ -digit numbers. Fortunately though, for a naive implementation of $e$ , we don't need this FFT multiplication. Naively, to sum a series of the type above, if we want millions of digits, we better evaluate each term to millions of digits precision. Given the factorial has a recurrence relation $$(n+1)! = (n+1) \cdot n!$$ which gives $$\frac{1}{n!} = \frac{1}{n} \frac{1}{(n-1)!}$$ we can get $n$ digits in time $$O\left(n\ e^{W(\ln n)}\right)$$ which isn't too bad, but is still bad enough for large $n$ (millions or more) that we want something better. So we need to do something more clever. The usual way to overcome this is to use methods based on binary splitting - basically, consider the following. Note that we can write a factorial as $$n! = \left(1 \cdot 2 \cdot 3 \cdot \cdots \cdot m\right) \cdot \left((m+1) \cdot (m+2) \cdot \cdots \cdot (n-1) \cdot n\right)$$ for some $m$ with $1 < m < n$ . Defining $$P(a, b) := (a+1)(a+2)\cdots(b-1)b$$ we have the identity $$P(a, b) = P(a, m) P(m, b)$$ which allows us to recursively compute $$P(0, n) = n!$$ using much smaller multiplications. Something similar can be done by splitting the sum for $e$ . Take $$e = \sum_{n=0}^{\infty} \frac{1}{n!}$$ and now write the equation $$\frac{P(a, b)}{Q(a, b)} = \sum_{n=a+1}^{b} \frac{1}{(a+1)(a+2)\cdots (n-1)n}$$ Then note that $$\frac{P(a, b)}{Q(a, b)} = \frac{P(a, m)}{Q(a, m)} + \frac{1}{Q(a, m)} \frac{P(m, b)}{Q(m, b)}$$ once we take $$Q(a, b) := (a+1)(a+2) \cdots (b-1)b$$ like before. In particular, we have the recurrence equations $$Q(a, b) = Q(a, m) Q(m, b)$$ and $$P(a, b) = P(a, m) + Q(m, b) P(m, b)$$ of which the second can be found by multiplying out. Then, $$e \approx 1 + \frac{P(0, N)}{Q(0, N)}$$ gives $N+1$ terms of the $e$ -series, with, again, smaller multiplies. This saves a lot; and permits efficient computation of $e$ by leveraging fast multiplication - in particular, the complexity to generate $n$ digits is now on the order instead of $O(\log(n)\ M(n \log n))$ ; obviously much better at least if $M(n)$ is suitably good, which it is for FFT-based multiplication methods (giving $O(n \log^2(n)\ a(n))$ ). However, this trick does not appear to generalize so easily to the derangements. In particular, while factorials expand nicely as a product, derangements $!n$ have a ""branching"" recurrence identity $$!(n+1) = n(!n\ +\ !(n-1))$$ which means that, in particular, we cannot simply parcel out the computation of $!n$ into a ""1 to $m$ "" and "" $m+1$ to $n$ "" part as we did above. Worse, because the derangement is in the denominator, we cannot do something analogous to $$\frac{1}{n!} = \frac{1}{n} \frac{1}{(n-1)!}$$ which was key to the naive method for $e$ ! Instead, we must do the full reciprocal, and now have horrible costs on the order of $O(e^{W(\ln n)} \lg(n)\ M(n))$ ! So is there some trick that we can use to compute $ɘ$ efficiently like how we can use binary splitting + FFT multiplication to compute $e$ efficiently? ADD: Not sure if this is useful. But I found on Wikipedia that $$!n = n! \sum_{i=0}^{n} \frac{(-1)^i}{i!}$$ Thus $$\frac{1}{!n} = \frac{1}{n!} \frac{1}{\sum_{i=0}^{n} \frac{(-1)^i}{i!}}$$ and then we can use the series reciprocal formula to get $$\frac{1}{!n} = \frac{1}{n!} \sum_{k=0}^{\infty} (-1)^k d_{k,n}$$ where $d_{0,n} = 1$ and $d_{k,n} = -\sum_{l=\max(0, k-n)}^{k-1} \frac{d_{l,n}}{(k-l)!}$ . See: https://mathoverflow.net/questions/53384/power-series-of-the-reciprocal-does-a-recursive-formula-exist-for-the-coeffic Thus $$ɘ = \sum_{n=2}^{\infty} \sum_{k=0}^{\infty} \frac{(-1)^k d_{k,n}}{n!}$$ however; I'm not sure if this recursion for this $d_{k,n}$ is any better. At least though we're no longer recursing in a denominator! ADD 2: The next thing I wonder about is the possibility that, because this is a nested sum, we should not consider a simple binary splitting, but a splitting into four ""squares"" or ""rectangles"", i.e. a quad splitting. In particular, if we want to sum the double sum $$\sum_{n=0}^{\infty} \sum_{m=0}^{\infty} a_{n,m}$$ as an approximant $$\sum_{n=0}^{N} \sum_{m=0}^{M} a_{n,m}$$ we should break into fours $$a_{0,0} + a_{0,1} + a_{1,0} + \sum_{n=1}^{N} \sum_{m=1}^{M} a_{n,m} = \left(\sum_{n=1}^{n_m} \sum_{m=1}^{m_m} a_{n, m}\right) + \left(\sum_{n=n_m+1}^{N} \sum_{m=1}^{m_m} a_{n, m}\right) + \left(\sum_{n=1}^{n_m} \sum_{m=m_m+1}^{M} a_{n, m}\right) + \left(\sum_{n=n_m+1}^{N} \sum_{m=m_m+1}^{M} a_{n, m}\right)$$ viz. $$a_{0,0} + a_{0,1} + a_{1,0} + \frac{P(0, 0, N, M)}{Q(0, 0, N, M)} = \frac{P(0, 0, n_m, m_m)}{Q(0, 0, n_m, m_m)} + \frac{P(n_m, 0, N, m_m)}{Q(n_m, 0, N, m_m)} + \frac{P(0, m_m, n_m, M)}{Q(0, m_m, n_m, M)} + \frac{P(n_m, m_m, N, M)}{Q(n_m, m_m, N, M)}$$","['constants', 'calculus', 'factorial', 'algorithms']"
4251740,Convex Hull of a Variety in Real Space,"I am a physicist currently working on a question posed as part of an algebraic geometric description of a physical set:
I did not find a question that is closely related to what I am searching for yet, but please feel free to just post the link below. My question concerns semialgebraic sets that are the convex hulls of affine varieties.
Imagine I am given an ideal $I_n$ of $k$ polynomials in $\mathbb{R}^n$ whose zero locus defines my variety. The variety is the image of all extreme points of a convex set in $\mathbb{R}^{m+n}$ given by the n-th elimination ideal of some ideal $I$ . As I experienced, it is not possible to recover the full set of Boolean combinations needed to describe the projection of my original set this way.(If you have any experience in that and know something that could help me, please let me know. I already tried Cylindrical Algebraic Decomposition but it turned out to be too complex.) Weakening my expectations, is it correct that if I take a point $P\in\mathbb{R}^n$ that from my preliminaries cannot be part of the projection and demand the ideal $I_n$ to be prime, I can find a sign condition on each of the $k$ polynomials $f_i$ by evaluating $f_i(P)$ ? Then $Q\in\text{conv}(Z(I_n))$ if $\lnot f_i(Q)*_i0$ for all $i=1,...,k$ where $*_i$ is the inequality coming from $f_i(P)*_i0$ . I thought it is possible to use Separation Theorem from Convex Geometry. The only subtlety is how to deal with singular points? I appreciate any kind of help!","['convex-geometry', 'algebraic-geometry', 'real-algebraic-geometry']"
4251809,How do you approach when completing the square?,"If $M = 3x^2 - 8xy + 9y^2 - 4x + 6y + 13$ , where $x,y\in\mathbb R$ , then $M$ must be: a) positive $\qquad$ b) negative $\qquad$ c) $0 \qquad$ d) an integer I somehow managed to figure it out by completing the square but in order to do so, it took me a lot of time and I'm not sure if every time I could solve such problems. This whole expression can be written as: $$ 2(x - 2y)^2 + (x - 2)^2 + (y + 3)^2$$ which implies $M$ is positive. My point is sometimes I'm lucky and I could group them in squares but other times not.
Is there any particular technique/method which always works? Secondly I also wanna know what you guys observe when completing the squares?","['maxima-minima', 'polynomials', 'optimization', 'algebra-precalculus', 'quadratics']"
4251826,Proof of this Linear Algebra Identity,"Let $x, w \in \mathbb{R}^n$ How to show that: $(w^tx)x = (xx^t)w$ $\hspace{1em}$ ( $t$ denotes transpose) Attempt at proof: $$
\begin{aligned}
\alpha &= (w^tx)x\\
\alpha^t &= x^t(w^tx)^t = x^t(x^tw)\\
(\alpha^t)^t &= (x^t)^t(x^tw) \hspace{1em} (\text{treating $x^tw$ as a scalar here})\\
\alpha &= x(x^tw) = (xx^t)w \hspace{1em} (\text{associativity})
\end{aligned}
$$ The above proof looks correct to me, but I'm still unsure as the last two steps feel like an ""hack"".","['matrices', 'linear-algebra', 'vectors']"
4251865,"Combinatorics - multi-group question - Choose 6 people out of 14, respecting conditions.","Here's the question: ""A congress will be attended by two representatives from Colombia, three from Chile, four from Argentina and five from Brazil.  Each of the $14$ representatives prepared their own speech, but only $6$ will be drawn to speak.  If the draw rule provides that each of the four countries must have at least one representative speaking, the number of different ways to compose the set of six speeches that will be heard at the congress, regardless of the order, is equal to how much?"" I've been working on this question for two weeks. My answer is $1450$ - but I solved it via brute force. I literally counted all the possibilities of the rule being broken and subtracted from the $3003$ possibilities to form groups of 6 people out of $14$ . I tried to look at the problem with fewer countries, and fewer choices. It helped me to count, but I couldn't decipher the pattern with which the problem changes when adding a new representative or a new country. Can someone help me?","['combinations', 'combinatorics']"
4251906,Does a continuous function map Cauchy sequences to Cauchy sequences?,"I am self-learning Real Analysis from the text Understanding Analysis by Stephen Abbott. I'd like someone to verify if my proofs/counterexamples to below exercise are rigorous and correct. [Abbott, 4.4.6] Give an example of each of the following, or state that such a request is impossible. For any that are impossible, supply a short explanation of why this is the case. (a) A continuous function $\displaystyle f:( 0,1)\rightarrow \mathbf{R}$ and a Cauchy sequence $\displaystyle ( x_{n})$ such that $\displaystyle f( x_{n})$ is not a Cauchy sequence. (b) A uniformly continuous function $\displaystyle f:( 0,1)\rightarrow \mathbf{R}$ and a Cauchy sequence $\displaystyle ( x_{n})$ such that $\displaystyle f( x_{n})$ is not Cauchy. (c) A continuous function $\displaystyle f:[ 0,\infty )\rightarrow \mathbf{R}$ and a Cauchy sequence $\displaystyle ( x_{n})$ such that $\displaystyle f( x_{n})$ is not a Cauchy sequence. Proof. (a) Consider $\displaystyle f( x) =\frac{1}{x}$ defined and continuous on $\displaystyle ( 0,1)$ . Let $\displaystyle ( x_{n}) =\frac{1}{n}$ be a Cauchy sequence in $\displaystyle ( 0,1)$ , where $\displaystyle ( x_{n})\rightarrow 0$ . $\displaystyle f( x_{n})$ is an unbounded sequence, and so it's not Cauchy. (b) This request is impossible. Suppose $\displaystyle ( x_{n})$ is a Cauchy sequence in $\displaystyle ( 0,1)$ . Then, for all $\displaystyle \delta  >0$ , there exists $\displaystyle N\in \mathbf{N}$ , $\displaystyle | x_{n} -x_{m}| < \delta $ for all $\displaystyle n >m\geq N$ . Since $\displaystyle f$ is uniformly continuous, for all $\displaystyle \epsilon  >0$ , there exists $\displaystyle \delta  >0$ , such that for all points $\displaystyle x,y$ satisfying $\displaystyle | x-y| < \delta $ , we have $\displaystyle | f( x) -f( y)| < \epsilon $ . Consequently, for all $\displaystyle \epsilon  >0$ , there exists $\displaystyle N\in \mathbf{N}$ , such that $\displaystyle | f( x_{n}) -f( x_{m})| < \epsilon $ for all $\displaystyle n >m\geq N$ . So, $\displaystyle f( x_{n})$ is Cauchy. (c) This request is impossible. $\displaystyle [ 0,\infty )$ is a closed set. For all Cauchy sequences $\displaystyle ( x_{n}) \subseteq [ 0,\infty )$ such that $\displaystyle ( x_{n})\rightarrow c$ , the image sequence $\displaystyle f( x_{n})\rightarrow f( c)$ . So, $\displaystyle f( x_{n})$ is Cauchy.","['uniform-continuity', 'cauchy-sequences', 'real-analysis', 'continuity', 'solution-verification']"
4252027,"Donald L. Cohn, ""Measure Theory"" (2nd ed., 2013), Exercises 2.4.9 and 2.5.5: are there unnecessary hypotheses, or have I misunderstood the questions?","From Donald L. Cohn, Measure Theory (2nd ed., 2013): [Exercise 2.4.9] Let $(X, \mathscr{A}, \mu)$ be a measure space, let $g$ be a $[0, +\infty]$ -valued integrable function on $X,$ and let $f$ and $f_t$ (for $t$ in $[0, +\infty)$ ) be real-valued $\mathscr{A}$ -measurable functions on $X$ such that $$
f(x) = \lim_{t \to +\infty} f_t(x)
$$ and $$
|f_t(x)| \leq g(x) \text{ for } t \text{ in } [0, +\infty)
$$ hold at almost every $x$ in $X.$ Show that $\int f\,d\mu = \lim_{t \to +\infty} \int f_t\, d\mu.$ (Hint: Give
a simplified version of the argument in Example 2.4.6.) Example 2.4.6, which I needn't quote, is a justification of
differentiation under the integral sign, whose relevance lies in its
use of a result from an appendix: C.7. Let $A$ be a subset of $\mathbb{R}^d,$ let $f$ be a
real- or complex-valued function on $A,$ and let $a$ be a limit
point of $A.$ [ $\ldots$ ] One can check that $\lim_{x \to a} f(x) = L$ if and only if $\lim_n f(x_n) = L$ for every sequence $\{x_n\}$ of
elements of $A,$ all different from $a,$ such that $\lim_n x_n = a.$ When doing the exercise, I was a bit worried, because I couldn't
understand the reason for requiring $f$ and $f_t$ to be real-valued
rather than $[-\infty, +\infty]$ -valued. I went ahead and gave this argument in my handwritten notes (because
I wasn't yet worried enough to type it up in $\LaTeX{}$ for checking,
or for asking a question in Maths.SE): Proposition. Let $(X, \mathscr{A}, \mu)$ be a measure space, let $g$ be a $[0, +\infty]$ -valued $\mu$ -integrable function on $X,$ and let $f$ be a $[-\infty, +\infty]$ -valued $\mathscr{A}$ -measurable function
on $X.$ Let $T$ be a first-countable topological space, $A$ a subset of $T,$ and $a$ a limit point of $A.$ Suppose that $f_t,$ for $t \in A,$ is
a $[-\infty, +\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ such that $$
f(x) = \lim_{\substack{t \to a \\ t \in A}} f_t(x)
$$ and $$
|f_t(x)| \leqslant g(x) \text{ for } t \text{ in } A
$$ hold at almost every $x$ in $X.$ Then $$
\label{cohn:measure:2:ex:2.4.9:eq:1}\tag{$1$}
\int f\,d\mu = \lim_{\substack{t\to a \\ t\in A}} \int f_t(x)\,d\mu.
$$ Proof. Let $(t_n)_{n \geqslant 1}$ be any sequence in $A \setminus \{a\}$ such that $\lim_{n \to \infty}t_n = a.$ Define a sequence $(h_n)_{n \geqslant 1}$ of $[-\infty, +\infty]$ -valued $\mathscr{A}$ -measurable functions on $X$ by $h_n(x) = f_{t_n}(x).$ Then $$
f(x) = \lim_{n \to \infty} h_n(x)
$$ and $$
|h_n(x)| \leqslant g(x) \quad (n \geqslant 1)
$$ hold for $\mu$ -almost all $x \in X.$ Therefore, by the DCT
(Theorem 2.4.5 in Cohn), $$
\int f\,d\mu = \lim_{n \to \infty} \int h_n\,d\mu =
\lim_{n \to \infty} \int f_{t_n}\,d\mu.
$$ The required result \eqref{cohn:measure:2:ex:2.4.9:eq:1} now follows
from the obvious generalisation of C.7. $\ \square$ This argument seemed (and still seems) so straightforward that I
never seriously thought there could be anything badly wrong with it,
until I came to the later exercise: [Exercise 2.5.5] Show that if $f \in  \mathscr{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda,
\mathbb{R}),$ then $$
\int_{\mathbb{R}} f\,d\lambda =
\lim_{\substack{a \to -\infty \\b \to +\infty}}
\int_{[a, b]} f\,d\lambda.
$$ (Hint: Use the dominated convergence theorem and a modification of
the hint given for Exercise 2.4.9.) I have two worries about this exercise.  The smaller worry is the
same as before: I do not see why $f$ has to be real-valued, rather
than $[-\infty, +\infty]$ -valued.  The vector space properties of
the set $\mathscr{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda,
\mathbb{R})$ - in particular, its closure under addition - seem
to play no role in the question.  Nor is Riemann integration
involved, in spite of the exercise's location at the end of a
section on that topic.  So these two possible explanations can be
ruled out. Cohn is a careful author. (I absolutely love this book, by the way!)
He does not throw unnecessary hypotheses around at random.  So
there is already reason, from these two exercises, to suspect that I
have misunderstood something at a pretty deep level.  Still, the
point is small enough - like several other smallish difficulties I
have had in studying the book on my own - that I would see no
need to ask a question about it in Maths.SE, were it not for a
larger worry. The larger worry is the appearance of the $\sigma$ -algebra $\mathscr{B}(\mathbb{R})$ of Borel subsets of $\mathbb{R}.$ Why
isn't it $\mathscr{M}_{\lambda^*}(\mathbb{R}),$ the $\sigma$ -algebra
of Lebesgue measurable subsets of $\mathbb{R}$ ? (I notice belatedly that Exercise 2.4.10 also contains a hypothesis
of Borel measurability, which my long and messy handwritten answer -
my computer has recently become so erratic that I have fallen
back on writing notes by hand - does not appear to have used.  I
won't complicate this already long question by detailing that
exercise too, but I may later ask a separate question about it.) Setting these worries aside, the exercise seems straightforward
(especially as I'd proved a general enough version of C.7 when
doing the first exercise). Thus: Let $f \colon \mathbb{R} \to [-\infty, +\infty]$ be a Lebesgue
integrable function.  The space $T = [-\infty, +\infty] \times [-\infty, +\infty]$ is
first-countable, and $(-\infty, +\infty)$ is a limit point of the
set $A = \{(a, b) \in T : -\infty < a < b < +\infty\},$ so the DCT
(with $|f|$ as the bound), together with the generalised version of
C.7, shows that $$
\int f\,d\lambda = \lim_{(a, b) \to (-\infty, +\infty)}
\int f\chi_{[a, b]}\,d\lambda =
\lim_{\substack{a \to -\infty \\ b \to +\infty}}
\int_a^b f\,d\lambda.
$$ But am I merely fooling myself into thinking that these are valid
solutions?","['measure-theory', 'solution-verification', 'lebesgue-integral', 'real-analysis']"
4252039,Is variance continuous for Gaussian martingale with continuous paths?,"Let $X: [0, \infty) \times \Omega \to \mathbb{R}$ be a Gaussian process, I.e. all $X_t$ lie in the same Gaussian space, a closed sub space of $L^2(\Omega)$ containing  only Gaussian random variables.  Assume $X$ also has continuous sample paths and $X_0=0$ .  Is this enough to conclude that the function $f: t\mapsto var(X_t)$ is continuous? What if X is also assumed to be a martingale?  The reason I ask is I’m asked to prove that that under these conditions, $f: t\mapsto var(X_t)$ is the quadratic variation of $X$ .  I can prove that $f$ is nondecreasing and that $X^2_t-f(t)$ is a martingale, but I need to show $f$ is continuous.  If pointwise convergence in a Gaussian space implied L^2 convergence, that would do it, but of course that is stronger than what I need.","['stochastic-processes', 'probability-theory', 'analysis']"
4252047,Does $\mathrm{\sum\limits_{-\infty}^\infty Ai(x)=1}$? Also on $\sum\limits_{-\infty}^0 \mathrm{Bi(x)}$.,"This question will be very similar to: On $$\mathrm{\sum\limits_{x=1}^\infty Ci(x)}$$ and On $$\mathrm{\sum_{x\in\Bbb Z}sech(x), \sum_{x=1}^\infty csch(x)}$$ all of which had closed forms, but it will use this Airy Ai function definition which is a type of Bessel function . Note that I will actually focus on 2 constants which each converge slowly. I will use $\sum\limits_{n=a}^b A_n=\sum\limits_a^b A_n$ for shorthand and use the following notations. The reason these are split up is because one of the constants may possibly diverge. Note that $\mathrm A_{0,1}$ both have a lower bound of $0$ , so the x= $0$ term will be subtracted: $$\mathrm{A_0=\sum_{-\infty}^0 Ai(x), A_1=\sum_0^\infty Ai(x),A=A_0+A_1-Ai(0)=\sum_{-\infty}^\infty Ai(x)}$$ Here is a graph of the summand: Here is a possible Abel-Plana formula computation. $$\mathrm{A_0=\sum_0^{\infty}Ai(-x)\mathop=^{Abel}_{Plana}\frac12 Ai(-0)+\int_0^\infty Ai(-x) \,dx+\int_0^\infty\frac{Ai(- -ix)-Ai(- ix)}{e^{2\pi x}-1}\,dx=\frac{1}{3^\frac23 2Γ\left(\frac23\right)}+\frac23+ \int_0^\infty\frac{Ai(ix)-Ai(- ix)}{e^{2\pi x}-1}\,dx}$$ $$\mathrm{A_1=\sum_0^\infty Ai(x)= \mathop=^{Abel}_{Plana}\frac12 Ai(0)+\int_0^\infty Ai(x) \,dx+\int_0^\infty\frac{Ai(-ix)-Ai( ix)}{e^{2\pi x}-1}\,dx= \frac{1}{3^\frac23 2Γ\left(\frac23\right)}+\frac13+ \int_0^\infty\frac{Ai(-ix)-Ai( ix)}{e^{2\pi x}-1}\,dx}$$ This means that the conjectured answer is: $$\mathrm{A=\sum_{x\in\Bbb Z}Ai(x)=A_0+A_1-Ai(0) \mathop=^{Abel}_{Plana} \frac{1}{3^\frac23 2Γ\left(\frac23\right)}+\frac23+ \int_0^\infty\frac{Ai(ix)-Ai(- ix)}{e^{2\pi x}-1} \, dx + \frac{1}{3^\frac23 2Γ\left(\frac23\right)}+\frac13+ \int_0^\infty\frac{Ai(-ix)-Ai( ix)}{e^{2\pi x}-1}\,dx-Ai(0)= \frac{1}{3^\frac23 Γ\left(\frac23\right)}+ 1+\int_0^\infty\frac{Ai(ix)-Ai( -ix)+Ai(-ix)-Ai(-ix)}{e^{2\pi x}-1}\,dx-\frac{1}{3^\frac23 Γ\left(\frac23\right)}}=1 $$ My final conjecture is the following with alternate forms. Note that some simplifications are possible, but change the definition. The following also uses Hypergeometric functions : $$\mathrm{1\mathop=^?A=\sum_{-\infty}^\infty Ai(x)=\sum_{x\in\Bbb Z}Ai(x)= \sum_{-\infty}^\infty \left(\frac{\,_0F_1\left(\frac23,\frac{x^3}{9}\right)}{3^\frac23Γ\left(\frac23\right)}-\frac{\,_0F_1\left(\frac43,\frac{x^3}{9}\right)}{\sqrt[3]3 Γ\left(\frac13\right)}\right)} =\sum_{-\infty}^\infty\left(\left(x^\frac32\right)^\frac13I_{-\frac13}\left(\frac{2x^\frac32}{3}\right)-x \left(x^\frac32\right)^{-\frac13} I_\frac13\left(\frac{2x^\frac32}{3}\right)\right)  $$ There are also variations with the Airy Bi function , and the Scorer functions Gi and Hi . Please correct me and give me feedback!","['special-functions', 'sequences-and-series', 'recreational-mathematics', 'airy-functions', 'bessel-functions']"
4252088,Mixture distribution,"I'm trying to come up with a reasonable way to determine the weights in a mixture distribution. Let us consider the following example: There are two districts ( $i=1,2$ ) in a city, both of which share one hospital. The distance between the districts and the hospital is denoted by $D_i, i=1,2.$ Incidents occur in both districts according to a Poisson process with intensity $\lambda$ (the same parameter for both districts). However, district 1 can transfer patients to the hospital at a unique rate of $\theta$ . In other words, transportation opportunities become available at rate $\theta$ and when it becomes available, it takes all patients waiting in that district to the hospital. Obviously, if there are not patients, there will be no dispatch and let us assume the vehicle is big enough to fit everyone! For district 2 however, there's only one vehicle that is always available but it must be full before it can be dispatched and let us denote the capacity of this vehicle by $\gamma$ If we denote the number of patients leaving every district by $X_i$ , for district 1, this is a random variable that follows a geometric distribution so we have $\mathbb{E}[X_1] = \frac{\lambda}{\theta}$ . For district 2, it is just a constant number that is $\mathbb{E}[X_2] = \gamma.$ To calculate $\mathbb{E}[Y]$ , that is the average number of patients arriving at the hospital at a given point in time, by definition, we have $$\mathbb{E}[Y] = \sum_{i=1,2} w_i \mathbb{E}[X_i].$$ Now what would be a neat way to determine $w_i, i = 1,2$ ? I think $$\text{probability}(Y = X_i) \propto \text{Frequency of dispatches from district } i,$$ which in turn depends on $\lambda$ and $\theta$ and $$\text{probability}(Y = X_i) \propto 1/D_i.$$ Any suggestions would be much appreciated.","['statistics', 'probability-distributions', 'probability']"
4252118,"If $f$ is an entire function such that $|f(z)| \le 2 \ln (|z|+1)$ for all $z \in \Bbb{C}$, then $f(z) = 0$ for all $z \in \Bbb{C}$.","Suppose $f$ is an entire function such that $|f(z)| \le 2 \ln (|z|+1)$ for all $z \in \Bbb{C}$ . Prove that $f(z) = 0$ for all $z \in \Bbb{C}$ . Here is my solution which I am hoping someone could check: First, it easily follows from the hypothesis that $f(0)= 0$ , so if we could prove $f$ is constant, then it would automatically follow that $f=0$ on $\Bbb{C}$ . Indeed, $$|f(0)| \le 2 \ln (|0|+1) = 2 \ln (1) = 0.$$ Now, let us prove that $f$ is constant. Let $z_0 \in \Bbb{C}$ and $r > 0$ arbitrary. Then, because $f$ is entire, we know it is holomorphic on a neighborhood of $\overline{B(z_0,r)}$ , so Cauchy estimates tell us that $$|f'(z_0)| \le \frac{\sup_{|z-z_0|=r} |f(z)|}{r} \le 2 \frac{\sup_{|z-z_0|=r} \ln (|z|+1)}{r}.$$ Let $z$ be any complex number satisfying $|z-z_0| = r$ . Then there exists a unit complex number $w_z$ such that $z = rw_z + z_0$ . Since $x \mapsto \ln x$ is an increasing function on $(0,\infty)$ , we have $$\ln(|z| + 1) = \ln(|rw_z + z_0| + 1) \le \ln (|rw_{z}| + |z_0| + 1) = \ln (r + z_0 + 1)$$ and hence, $$|f'(z_0)| \le 2 \frac{\ln (r + z_0 + 1)}{r}.$$ As $r \to \infty$ , the numerator and denominator of the fraction on the RHS go to $\infty$ . But L'Hôpital's rule says $$|f'(z_0)| \le 2 \lim_{r \to \infty} \frac{\ln (r + z_0 + 1)}{r} = 2 \lim_{r \to \infty} \frac{1}{r + |z_0| + 1} = 0,$$ so $f'(z_0)=0$ . Since $z_0$ was arbitrary, it follows that $f'=0$ and therefore $f$ is constant. But, by our initial remark, we know this means $f=0$ on $\Bbb{C}$ .","['complex-analysis', 'entire-functions', 'solution-verification']"
4252160,Integrating after using integrating factor,I have a question here $x\frac{dy}{dx}-2y=x^4\sin(x)$ . I rearranged it to $\frac{dy}{dx}-\frac{2y}{x}=x^3\sin(x)$ I need to find a general solution to this. I used the integrating factor method and know the integrating factor is $e^{\int\frac{2}{x}dx}$ which is $e^{2\ln|x|}$ . I multiplied everything by the integrating factor. The equation becomes $\frac{dy}{dx}2\ln|x|-\frac{2y}{x}\ln|x|y=x^3\sin(x)2\ln|x|$ Question. How do I integrate and find the general solution from here? General solution = having $y=....$ . So clearly I need to integrate. How do I arrange this in a way so I can integrate it? And what is the final answer?,"['integration', 'calculus', 'ordinary-differential-equations']"
4252173,"Real solution to the system $\frac{2}{m} + \frac{3}{n} = \frac{1}{p}, m^2 + n^2 = 1360, m + n + p = 47$","A friend of mine gave me the following problem. Let $m,n,$ and $p$ be the real solution to the system \begin{align*}
 \frac{2}{m} + \frac{3}{n} &= \frac{1}{p}\\ m^2 + n^2 &= 1360\\ m + n +
 p &= 47. \end{align*} What is the value of $n - m$ ? My friend got this problem in his test, which should only take around one minute to solve each problem. We know that the solution is $m = 8, n = 36, p = 3$ . Well, we used WolframAlpha to find it, to be honest (not on the test, of course!). But I think there might be no nice solution to this problem, let alone a solution that solves the problem in a short amount of time. And we think it might be that ""trap"" problem that wastes your time, precious time. I have tried to solve it without WolframAlpha, though. After getting rid of $p$ and simplifying I got the system \begin{align}
m^2 + n^2 &= 1360\tag{1}\\
3 m^2 + 6 m n - 141 m + 2 n^2 - 94 n & = 0\tag{2}
\end{align} See that? Equation $(2)$ is a mess. This is why I expect there is no ""nice"" solution. (I don't even find any solution yet, sad) Another way that come to mind was by finding $n - m$ directly, without finding the solution itself. But this also doesn't seem to work. Any hint? Any solution is welcome.","['algebra-precalculus', 'systems-of-equations']"
4252176,Is the Interior of Hyperbola homeomorphic to $\mathbb{R^{2}}$?,"I know that the interior of the unit disk is homeomorphic to $\mathbb{R^{2}}$ by the mapping $(r,\theta)\to(\tan(\frac{r\pi}{2}),\theta)$ . I am struggling to come up with a homeomorphic map from the interior of the hyperbola $x^{2}-y^{2}=1$ to $\mathbb{R}^{2}$ or to the interior of the unit disk. I think I am struggling mainly because I cannot find suitable coordinates to describe the interior. For example in the unit disk I just could write it as $r<1$ . By interior of the hyperbola I mean $\{(x,y):x^{2}-y^{2}<1\}$ Can anybody help me with a map from it to the disk or to $\mathbb{R}^{2}$ ? I am very very new to homeomorphisms (Just about learnt the definitions).",['general-topology']
4252188,Examples of non-trivial exclusively irrational integrals?,"One very famous integral is $$\int_{\mathbb{R}} \frac{\cos(x)}{x^2 +1} \, dx = \frac{\pi}{e} \tag{1}$$ as is shown in the answers to this question . I find this integral particularly interesting as the result is written exclusively as a combination (by ""combination"" I mean a product/quotient/addition/exponentiation/logarithm) of irrational numbers, where I'm using "" exclusively irrational "" here to mean that the answer doesn't involve other factors of rational numbers combined with the irrationals. For example, the integral: $$\int_{0}^{\infty} \frac{x^2}{e^x-1}\, dx = 2 \zeta(3)$$ I would not consider being ""exclusively"" irrational because of the factor of $2$ multiplying $\zeta(3)$ . I decided to look for other exclusively irrational integrals similar to $(1)$ which combine several irrational numbers in their result, but to my surprise, I couldn't find many examples similar to this. Most of the results I found where "" single-irrational "", like the following integrals: $$ \int_{\mathbb{R}} e^{-x^2}\, dx = \sqrt{\pi}, \qquad \int_{0}^{1} \ln\left(\ln\left(\frac{1}{x}\right)\right)\, dx=-\gamma, \qquad \int_{1}^{\infty}\frac{\ln(x)}{1+x^2} \, dx = G$$ which, although they are exclusively irrational, they can also be written in terms of a single famous irrational (hence the moniker I gave them). Some other common finds were ""near-misses"" like: $$\int _0^{\infty }e^{-x}\ln ^2\left(x\right)\ dx = \gamma^2 + \frac{\pi^2}{6}, \qquad \int_{1}^{\infty} \frac{(x^4 - 6x^2+1)\ln(\ln(x))}{(1+x^2)^3}\, dx = \frac{2G}{\pi}$$ In fact, the only other exclusively irrational integral which wasn't also a single-irrational that I found was the integral $$ \int_{0}^{\infty} \frac{(1-x^2) \, \text{sech}^2\left(\frac{\pi x}{2} \right)}{(1+x^2)^2}\, dx = \frac{\zeta(3)}{\pi}\tag{2}$$ Of course, there are trivial integrals that indeed give exclusively irrational results. For example $$\int_{0}^{\frac{\pi}{e}} 1 \, dx = \frac{\pi}{e} $$ but I would like to avoid these types of integrals. Another type is the ""disguised"" solution, which would be something like $$ \int_{\mathbb{R}} \frac{\sin(x)}{\color{purple}{e}x}\, dx= \frac{\pi}{e}, \qquad \int_{-1}^1\frac{1}{\color{purple}{4}x}\sqrt{\frac{1+x}{1-x}}\ln\left(\frac{2\,x^2+2\,x+1}{2\,x^2-2\,x+1}\right)\ dx =\pi \, \text{arccot}\left(\sqrt{\varphi} \right)$$ which in reality are just single-irrational solutions or near-misses where we just multiplied a $\color{purple}{\text{factor}}$ on both sides. I would also like to avoid these types of integrals. My question is: Does anyone know any exclusively irrational integrals like $(1)$ and $(2)$ where you combine several different irrationals in the result?  Preferably avoiding single-irrational, disguised, or trivial integrals like my other examples. Ideally I would like results that exclusively combine irrational (and also very likely but still unproven to be irrational ) numbers such as $e,\,\pi$ , Golden ratio $\varphi ,\, \zeta(\text{Odd integer}),\,\ln(\text{Prime number}),\, \sqrt{\text{Prime number}}$ , Euler-Mascheroni constant and Catalan's constant; Where by ""combination"" I mean that these numbers are being added/multiplied/divided/exponentiated or being the argument of a trig function, in a way that doesn't simplify to factors of rational numbers, i.e. without something like $\ln\left(e^2\right)$ . Any help or suggestions are greatly appreciated. Thank you very much!","['integration', 'definite-integrals', 'irrational-numbers', 'big-list', 'calculus']"
4252198,A natural discovery of $e$ as $\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$ - what is unrigorous here?,"I had an interesting conversation with my teacher today, and we were discussing what the best foot forward for me might be. He asked me to differentiate $e^x$ from first principles, which I did, and then challenged me on: Begin by defining $e$ as: $$\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$$ He wondered if I found this satisfactory, intuitively speaking. I said: ""this is just one of the fundamental and historic definitions of $e$ "", and he asked me - ""why?"" And I just couldn't answer beyond repeating: ""it's the definition"". On that note he then showed a more natural exploration that leads to a discovery of this definition of $e$ . He did declare this exploration to be ""cheating"", which is the focus of my question. Take $a$ to be a positive real number. Then: $$\begin{align}\lim_{h\to0}\frac{a^{x+h}-a^h}{h}=a^x\lim_{h\to0}\frac{a^h-1}{h}\end{align}$$ And by exploration with graphs, one intuitively sees that the latter term, the derivative of $a^x$ at $0$ , clearly should exist, and that there should exist some base $a$ for which this derivative is $1$ . Exploring this possibility, define: $$(h_n)_{n\in\Bbb N}=\frac{1}{n}$$ And for each $h_n$ , ask which $a$ satisfies: $$\frac{a^{h_n}-1}{h_n}=1$$ And naturally define a sequence $(a_n)$ that solves this equation: $$(a_n)_{n\in\Bbb N}=(1+h_n)^{1/h_n}$$ This leads to the idea that the mystery $a$ for which the derivative of $a^x$ at zero equals $1$ can be found by considering the limits of the sequence $a_n$ , as $h_n$ goes to $0$ . This finds: $$\lim_{n\to\infty}a_n=\lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$$ And we call this number $e$ . The exploration leads one to suspect that $e$ satisfies $\frac{d}{dx}e^x=1$ . However, he did mention that, in the interests of me studying real analysis, that this approach is ""cheating"" a little somehow, and that it is not a fully rigorous derivation of that limit. He hinted that I would see why this is shortly after properly studying real analysis. I have seen a good deal of calculus and analysis around the Internet, in a very erratic way, so I have only some sense of why this approach is unrigorous. However, there are many many people on this forum who have properly studied real analysis, and I am asking for your help in explaining either what's wrong, or how to make it right. My thoughts: We have not shown that $a^x$ is sequentially convergent, nor have we shown that $(a_n)$ converges. Showing that there exists $(a_n),(h_n)$ such that: $$\lim_{n\to\infty}h_n=0,\,\frac{a_n^{h_n}-1}{h_n}=1$$ Is not the same as showing that: $$\lim_{h\to0}\frac{(\lim_{n\to\infty}(a_n))^h-1}{h}=1$$ Any insight into $(1),(2)$ or anything else here would be greatly appreciated - especially any insight into how to make it fully rigorous! I just intuitively feel that $(2)$ is correct, but I cannot know for sure.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4252228,"Given a general solution, find its differential equation.","So usually, a differential equation question is asking to find a general solution. But this is the other way around. I have a general solution $$y=\frac{1}{c_1 \cos x+c_2 \sin x},$$ and I want to find the differential equation to it. This, I think, is about finding $c_1$ and $c_2$ . So, I calculated the derivative, $$y'=\frac{c_1 \sin x -c_1 \cos x}{(c_2 \sin x+c_2 \cos x)^2}.$$ Now, it's time to subtract $y-y'$ and let them cancel out to find $c_1$ , $c_2$ right? Or is the next step to find $y''$ and see if they have cancelling out terms and find $c_1$ and $c_2$ ?",['ordinary-differential-equations']
4252237,What does Big O actually tell you?,"Two days ago I felt very uncomfortable with Big O notation. I've already asked two questions: Why to calculate ""Big O"" if we can just calculate number of steps? The main idea behind Big O notation And now almost everything has become clear. But there are few questions that I still can't understand: Suppose we have an algorithm that runs in $1000n$ steps. Why people say that $1000$ coefficient becomes insignificant when $n$ gets really large (and that's why we can throw it away)? This really confuses me because no matter how large $n$ is but $1000n$ is going to be $1000$ times bigger than $n$ . And this is very significant (in my head). Any examples why it is considered insignificant as $n$ tends to infinity would be appreciated. Why Big O is told to estimate running time in worst case? Given running time $O(n)$ , how is it considered to be worst case behavior? I mean in this case we think that our algorithm is not slower than $n$ , right? But in reality the actual running time could be $1000n$ and it is indeed slower than $n$ . According to the definition, Big O gives us a scaled upper bound of $f$ as $n \to +\infty$ , where $f$ is our function of time. But how do we interpret it? I mean, given algorithm running in $O(n)$ , we will never be able to calculate the actual number of steps this algorithm takes. We just know that if we double the size of the input, we double the computation time as well, right? But if that $O(n)$ algorithm really takes $1000n$ steps then we also need to multiply the size of the input by $1000$ to be able to visualise how it grows, because $1000n$ and $n$ have very different slopes. Thus in this case if you just double the computation time for the doubled size of the input, you're going to get wrong idea about how the running time grows, right? So how then do you visualise its growth rate? I want to add that I know the definition of Big O and how to calculate it, so there is no need in explaining it. Also I've already googled all these questions tons of times and no luck. I'm learning calculus at the moment, so I hope I asked this question in the right place. Thank you in advance!","['asymptotics', 'real-analysis', 'calculus', 'algorithms', 'computational-complexity']"
4252310,Is this group a free group on these generators?,"Let $F_2$ be the free group on two elements $a,b$ and $C_2=\{0,1\},$ the cyclic group of order $2.$ Let $H$ be the kernel of the unique homomorphism $\phi:F_2\to C_2^2$ with $\phi(a)=(1,0)$ and $\phi(b)=(0,1).$ Any subgroup of a free group is free, by this theorem . In this answer I prove that $H$ is generated by: $$a^2,b^2,(ab)^2,(ba)^2, (a^2b)^2, (ab^2)^2$$ Is $H$ free on these generators? This is equivalent to asking if the group is free on: $$a^2,b^2,(ab)^2,(ba)^2,ab^2a,ba^2b,$$ which might be easier, because these are all “words” of length at most $4.$ But I have no idea how to show there is no reduce the set of generators.","['group-theory', 'free-groups']"
4252329,A differential equation and coordinate geometry problem,"If the normal drawn to a curve at any point P intersects the x-axis at G and the perpendicular from P on
the x-axis meets at N, such that the sum of the lengths of PG and NG is proportional to the abscissa of
the point P, the constant of proportionality being k. Form the differential equation and solve it to show
that the equation of the curve is, $$y^2=cx^{\frac{1}{k}}-\frac{k^2\cdot x^2}{2k-1}; \text{ or }y^2=\frac{k^2\cdot x^2}{2k+1}-cx^{-\frac{1}{k}}$$ where $c$ is an arbitrary constant What I did was pretty simple: I drew a diagram and then used the formula for the tangent( $PG=y \csc(\theta)$ ), subtangent( $y\cot(\theta)$ ) and subnormal( $y\tan(\theta)$ )(where $\tan(\theta)=y'|_P$ ) and I was left with this DE $$\frac{\sqrt{1+y'^2}}{y'}\cdot y +(y'+\frac{1}{y'})y=kx\\ \Rightarrow \frac{\sqrt{1+y'^2}}{y'}\cdot y +(\frac{1+y'^2}{y'})y=kx$$ I am pretty much stuck at this point, I got to an answer but it didn't match the answer and I figured it was all wrong anyway due to a mistake at the beginning. There are no standard forms that I can see which lead to simpler solutions. EDIT: Many are saying that $\tan(\theta)\neq y'$ but I have by definition defined that $y'=\tan(\theta)$ .Here's the type of diagram I have in mind","['analytic-geometry', 'calculus', 'ordinary-differential-equations']"
4252379,Interesting growing behaviour of hypergeometric function,"Doing some computations and plottings I've found out that the function $$_2F_1(2s-1,s-\tfrac{1}{2};s;-1)$$ behaves for large real $s$ like $4^{-s}$ . More precisely: It seems that $$_2F_1(2s-1,s-\tfrac{1}{2};s;-1)4^s$$ grows, but very very slowly. No exponential growth or decay at all. If you modify the $4$ only slightly this of course changes and you get exponential groth or decay. So my question is to explain this phenomenon. Is $$\lim_{s\to \infty}\ _2F_1(2s-1,s-\tfrac{1}{2};s;-1)4^s = \infty?$$ Is there an easy to detemine for every $x>0$ the respective $b(x)>0$ such that you have $$_2F_1(2s-1,s-\tfrac{1}{2};s;-x) \sim b^{-s}?$$ What is $$\lim_{s\to \infty}\  _2F_1(2s-1,s-\tfrac{1}{2};s;-x) b^{s} $$ then?","['complex-analysis', 'hypergeometric-function', 'asymptotics', 'real-analysis']"
4252385,How to prove the derivative of $\ln{f(x)}$?,"I've been trying to demonstrate the derivative formulas for a few functions. I could demonstrate that $\ln [x^{n}]=\frac{n}{x}$ . But when I tried to derive $f(x)=\ln[u(x)]$ I couldn't. I just started it and couldn't go foreward. What I did was: $$
\begin{align} 
f'(x)&=\lim_{h \to 0}\frac{\ln u(x+h)-\ln u(x)}{h}\\&=\lim_{h \to 0}\frac{\ln\frac{u(x+h)}{u(x)}}{h}\\
&=\lim_{h \to 0} \frac{1}{h}\ln\frac{u(x+h)}{u(x)}\\
&=\lim_{h \to 0} \ln\left(\frac{u(x+h)}{u(x)}\right)^{\frac{1}{h}}
\end{align}  $$ What do I do next?","['limits', 'calculus', 'derivatives']"
4252426,Intuition on chart compatibility for smooth manifolds,"Let $M= \mathbb{R}$ . Consider the charts, $A_1 =\{(\mathbb{R}, \phi_1:t\to t)\}$ and $A_2 =\{(\mathbb{R}, \phi_1),(\mathbb{R},\phi_2:t \to t^3) \}$ . Then it is clear that $M$ is a 1 dimensional manifold with respect to $A_1$ but with respect to $A_2$ , the transition map $\phi^{-1}_2 o$ $\phi_1: \mathbb{R}\to \mathbb{R}$ given by $t \to t^{1/3}$ is not a differentiable function at 0 and hence fails to provide a differentiable structure for $M$ . But what I am not able to understand (or confusing) is that, in $A_2$ one of the chart is simply the identity map. If we consider the chart $\{(\mathbb{R}, \phi_2)\}$ , then this is a differentiable structure for $M$ , however just adding an extra chart $(\mathbb{R},\phi_1)$ (which essentially is a identity map) to the chart $\{(\mathbb{R},\phi_2)\}$ fails to give a differentiable structure as described in previous paragraph. How is this idea captured via the condition that transition maps should be compatible at the non empty intersection between two different charts? I hope I make sense in what I ask. What am I getting wrong or failing to understand properly?","['smooth-manifolds', 'differential-geometry']"
4252434,Permutation in $S_{35}$ find $s^{12}$ and $s^{27}$,I was assigned the following Permutation in $S_{35}$ $$s=(1 2 3 4 5 6 7 8 ... 22)$$ I have to find the following $s^2$ $s^3$ $s^{12}$ and $s^{27}$ So I went ahead and calculated the first 2 problems simply by hand but I'im struggling to find how to do the other 2. Thoughts on the third one are non existent For the last one $s^{27}$ I know that the order of $s$ is $22$ since its length is $22$ so I can write $s^{27} = s^{22} \times s^3 \times s^2=s^3 \times s^2$ . Is this the solution to it?,"['permutations', 'group-theory', 'abstract-algebra', 'symmetric-groups']"
4252435,How Can I Visualize a PDE Boundary Condition?,"In this question, the comment suggests that the integration bounds in the Fourier Series should be chosen to avoid discontinuities in the boundary conditions.  I am trying to produce a nice visual to see this sort of discontinuity. The visual I have in mind is a curve in 3-dimensional space, which lives somewhere within the infinite cylinder $r = 1$ .  I want to know if there is an easy way to draw this, i.e., in Mathematica. As a workaround, I created a 2-dimensional graph in Desmos... I assume the way to determine from a 2-dimension graph like this that the boundary condition is continuous from $0$ to $2\pi$ is to note that the green line is continuous and that its endpoints are the same.  By contrast, the red line is also continuous, but its endpoints are not the same, so the boundary condition is not continuous from $-\pi$ to $\pi$ .  This is based on the idea of wrapping each of these intervals around the cylinder.  Is this approach on point? How can I actually graph the 3-dimensional boundary condition without knowing the solution to the PDE in Mathematica, MATLAB, or some online graphing tool?","['mathematica', 'graphing-functions', 'geometry', 'polar-coordinates', 'partial-differential-equations']"
4252443,"Find volume of a solid figure that lies between: $x^2+y^2+z^2\leq 4$, $2x^2+2y^2-2z^2\geq 1$, $2z^2\geq x^2+y^2$","Find volume of a solid figure that lies between: $x^2+y^2+z^2\leq 4$ , $2x^2+2y^2-2z^2\geq 1$ , $2z^2\geq x^2+y^2$ I just really can't figure out the limits of integration... any hint would be great -----edit---- Spherical coordinates: $x=\rho \sin\varphi \cos\theta$ $y=\rho \sin\varphi \sin\theta$ $z = \rho \cos\varphi$ From sphere: $\rho ^2 \leq 4$ or $\rho \leq 2$ . From hyperboloid: $ 2\rho ^2( \sin^2\varphi - \cos^2 \varphi)\geq 1$ From cone: $\rho ^2(2 \cos^2\varphi - \sin^2 \varphi)\geq 0$ Intersection of sphere and hyperboloid, I got the ellipse: $\frac{x^2}{\frac{9}{4}}+\frac{y^2}{\frac{9}{4}}=1$ or in spherical coordinates: $\rho^2 \sin^2\varphi=\frac{9}{4}$","['multivariable-calculus', 'volume']"
4252446,Is there an intuitive way to think about the product rule for derivatives?,"$\frac{d}{dx}(f(x)g(x)) = f'(x)g(x) + f(x)g'(x)$ Is there a reason why this makes sense? I mean really, if you break things down in terms of slopes of tangent lines and what not, there is a lot going on here. Is there some geometric intuition going on behind this that I've been blind to all of these years? Thanks in advance","['calculus', 'derivatives', 'intuition']"
4252458,Normal subgroup of a characteristic subgroup,"I came across the following question while reviewing for my qualifying exams: Prove or provide a counterexample: If $M$ is a normal subgroup of $N$ , and $N$ is a characteristic subgroup of $G$ , then $M$ is a normal subgroup of $G$ . Looking at our assumptions, it does not seem like we have enough information to deduce that $M$ is normal in $G$ . However, coming up with a counterexample has proven difficult. I have tried letting $G = D_4$ and $N = \langle r\rangle$ , but that did not prove fruitful. I also thought of using the quaternions, but all of its subgroups are normal, so that wouldn't be helpful here. Any advice for this problem would be greatly appreciated.","['characteristic-subgroups', 'group-theory', 'normal-subgroups', 'examples-counterexamples']"
4252506,Maximizing expected reward for inhomogenous exponential process,"Consider an inhomogenous exponential process $t \sim \lambda(p(t)) e^{-\int_{0}^{t} \lambda(p(s)) ds}$ where $\lambda > 0$ and monotonically decreases on the reals. Now define the reward function $\pi(p(t), t) = \begin{cases}
0 &\text{ if } t > T\\
p(t) &\text{ else }
\end{cases}$ The expected reward is then $E[\pi] = \int_{0}^{T} \lambda(p(t)) e^{-\int_{0}^{t} \lambda(p(s)) ds} p(t)dt$ Directly applying the calculus of variations, I get the Euler-Lagrange conditions for extremizing this $0 = \exp\big(- \int_{0}^{t} (\lambda \circ p)(t) ds\big) \big((\lambda' \circ p)(t) \times p(t) + (\lambda \circ p)(t) - (\lambda \circ p)(t) \times p(t) \times \int_{0}^{t} (\lambda' \circ p)(s) ds\big)$ $0 = (\lambda' \circ p)(t) \times p(t) + (\lambda \circ p)(t) - (\lambda \circ p)(t) \times p(t) \times \int_{0}^{t} (\lambda' \circ p)(s) ds$ This seems like an absurd result to me. How can the result be independent of the boundary conditions $t = T$ ? Putting this into concrete terms, say I'm choosing to set the price for a single good that expires at time $t = T$ . The sale time is inhomogenous exponential with a demand-curve like rate parameter $\lambda(p)$ , and I'm trying to choose the price trajectory through time that maximizes my expected revenue. This result tells me that my revenue maximizing price trajectory doesn't depend on how much time I have to sell the good? Alternatively, I can discretize the same problem with an equivalent sequence of poisson processes. I'll spare you the tedious details, but rescaling $\lambda$ for the timestep length, letting $\pi^{*}_{t}$ be the optimum price for a discrete time step and $\pi^{*}_{T+1} = 0$ , the inductive discrete solution is $0 = 1 + \exp(-\lambda(p_{t})) \big(\lambda'(p_{t}) (p_{t} - \pi^*_{t+1}) - 1\big)$ which is obviously completely different, and doesn't converge in the limit to the continuous solution. I'm pretty sure I did something wrong here, but for the life of me I can't think of what. EDIT: I've figured out my error. I can't naively apply the Euler-Lagrange result because my action here depends on both $p(t)$ and $\int_{0}^{t} \lambda(p(s))ds$ . Taking the rigorous functional derivative of the action, with perturbation $p + \epsilon \phi$ , wrt $\epsilon$ evaluated at $\epsilon = 0$ , I get the first order condition $$0 = \int_{0}^{T} \exp\bigg(- \int_{0}^{t} \lambda p ds\bigg) \bigg(\lambda' p \phi - \lambda p \int_{0}^{t} \phi \lambda' ds + \lambda \phi \bigg) dt$$ $$\forall \phi: \mathbb{R} \rightarrow \mathbb{R} \mid \phi(0) = \phi(T) = 0$$ Because $\phi$ can't be easily factored out of the inner integral, this is not as straightforward as the usual calculus of variations problem. But at least things are making sense now.","['statistics', 'calculus-of-variations', 'economics', 'stochastic-processes', 'probability']"
4252523,Lebesgue Measure of the Image of an Interval,"Given that $f : \mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary function, I'm looking for what can be said about the size of an interval $I$ under $f$ . If we define $D := \{x \in \mathbb{R} : f \text{ is differentiable at } x\}$ and take a point $d \in D$ , then since $|f'(d)|$ acts like a local scale factor of $f$ around $d$ , it seems reasonable to expect that: $$\lim_{h \rightarrow 0} \frac{\mu^*(f[I_{d,h}])}{2h} = |f'(d)|$$ where $\mu^*$ denotes Lebesgue outer measure, $I_{d,h} := (d-h,d+h)$ , and if $A$ is some set then $f[A]$ denotes the image of $A$ under $f$ . Unfortunately (as usual) mathematics is not so straightforward. Without too much effort, it follows by playing around with the definition of the derivative that: $$\limsup_{h \rightarrow 0} \frac{\mu^*(f[I_{d,h}])}{2h} \leq |f'(d)|$$ and hence we get the desired equality above if $f'(d) = 0$ . But the more general result can fail in seemingly pathologically counterexamples. For example it's possible to construct a measurable function $f : \mathbb{R} \rightarrow \mathbb{Q} \cup C$ (where $C$ is the Cantor set) which is differentiable on all of $C$ , and $f'(x) = 1$ for all $x \in C$ . However this is a pretty pathologically example, and even then it only fails on a null set. So my conjecture is the following: Given that $f : \mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary function, and $D := \{x \in \mathbb{R} : f \text{ is differentiable at } x\}$ . Then: $$\lim_{h \rightarrow 0} \frac{\mu^*(f[I_{d,h}])}{2h} = |f'(d)|$$ for almost all $d \in D$ . So Far If $f$ is a locally a Darboux function around $d$ (in other words: satisfies the intermediate value theorem in small enough intervals around $d$ ), then $f[I_{d,h}]$ is an interval for small enough $h$ , and it can be shown that the equality holds at $d$ . Therefore the conjecture is true for Darboux functions (and functions which are locally Darboux around almost all points in $D$ ). But what about the more general case?","['measure-theory', 'lebesgue-measure', 'derivatives', 'real-analysis']"
4252544,Abelian Subgroup of Normal Subgroup of Solvable Group [duplicate],"This question already has answers here : A Nontrivial Subgroup of a Solvable Group (3 answers) Closed 2 years ago . This is Exercise #11 from Section 3.4 of Dummit and Foote (and is a common question in various texts), but I have not found a satisfactory answer on this site (and in some cases people misstate the definition of a solvable group given in D&F).  My hope is that I will not only understand a solution to this problem but in doing so also see how to combine various big ideas, such as products of groups, the isomorphism theorems, and normal subgroups. The problem as set forth in D&F is ""Prove that if $H$ is a nontrivial normal subgroup of the solvable group $G$ then there is a nontrivial subgroup $A$ of $H$ with $A \underline{\triangleleft} G$ and $A$ abelian."" The D&F definition is ""A group $G$ is $solvable$ if there is a chain of subgroups $1 \, = G_0 \, \underline{\triangleleft} \, G_1 \, \underline{\triangleleft} \, G_2 \, \underline{\triangleleft} \ldots \underline{\triangleleft} \, G_s \, = \, G$ such that $G_{i+1}/G_i$ is abelian for $i = 0, \, 1,\ldots, \,s-1$ ."" My approach has been to attempt to find something isomorphic to $G_{i+1}/G_i$ because then I would have an isomorphism to an abelian.  Based on ideas from other postings, I considered letting $G_i$ be the largest index such that $G_i \cap H = {1}$ and then we would have $G_{i+1} \cap H \neq {1}$ .  Perhaps letting $A = G_{i+1} \cap H$ might lead me somewhere. This is the place where I am trying to combine the second (i.e., triangle) isomorphism with normal subgroups and am getting stuck.  Mightily stuck, I might add. It seems to me that $H \underline{\triangleleft} \, G_{i+1}$ and by definition $G_i \underline{\triangleleft} \, G_{i+1}$ so that $G_{i+1}H/H \cong G_{i+1}/(G_{i+1} \cap H)$ .  That did not seem to work, but it got me to consider another product. I decided to try $G_i (G_{i+1} \cap H)$ and put that at the top of the diamond (for the second isomorphism theorem).  Then $G_i$ is on the left and $(G_{i+1} \cap H)$ is on the right with $G_i \cap (G_{i+1} \cap H)$ at the bottom. Now my isomorphism is $G_i (G_{i+1} \cap H)/G_i \cong (G_{i+1} \cap H)/ (G_i \cap (G_{i+1} \cap H))$ . Here I observed that $G_i \cap (G_{i+1} \cap H) = (G_i \cap H) = {1}$ .  Recall that I am using $A = (G_{i+1} \cap H)$ .  That means my isomorphism is actually $G_i A/G_i \cong A/{1}$ . This seems ever so close.  If I knew that $G_i A/G_i$ is abelian then I would be there. Am I on the right track?  Am I completely in the wrong direction?  Is there a gap (simple or not) that I am missing?  Lastly, please excuse the length of this posting, but I wanted to be thorough with the statement of the problem, the related definitions, and my thinking.","['normal-subgroups', 'abstract-algebra', 'group-theory', 'abelian-groups', 'solvable-groups']"
4252561,Polya Enumeration: Generate Configurations off of Cycle Index,"I have a 3x3 grid. $$\begin{array}{|c|c|c|}
\hline 
1 & 2 & 3\\
\hline 
4 & 5 & 6\\
\hline 
7 & 8 & 9\\
\hline 
\end{array}$$ The symmetries are the 4 rotations (0, 90, 180, 270), and 4 reflections (top/down, left/right, 2 diagonals). Dihedral 4 group, I believe. The cycle index becomes: $$
\frac{1}{8}[x^9_1 + 2x^1_1x^2_4 + x^1_1x^4_2+4x^3_1x^3_2]
$$ I want to colour the grid two colours, so I expand with Polya's Enumeration Theorem: $$
\frac{1}{8}[(a+b)^9 + 2(a+b)(a^4+b^4)^2 + (a+b)(a^2+b^2)^4+4(a+b)^3(a^2+b^2)^3]
$$ Expanding just the first term $(a+b)^9 $ has terms $$
a^9 + 9 a^8 b + 36 a^7 b^2 + 84 a^6 b^3 + 126 a^5 b^4 + 126 a^4 b^5 + 84 a^3 b^6 + 36 a^2 b^7 + 9 a b^8 + b^9
$$ In this expanded form, the $126a^5b^4$ means that I have around 126 configurations with 5 of colour $a$ and 4 of colour $b$ (probably less since I didn't divide). Is there a way to enumerate these 126 unique configurations, other than brute force? I would like to extend this to something like a 6x6 grid, but these coefficients will only grow larger.","['dihedral-groups', 'polya-counting-theory', 'combinatorics', 'group-theory', 'symmetry']"
4252570,Expectation Value of Sample Mean,"When deriving the expectation value of the sample mean, I am uncertain about the following step: $E(x_i) = E(x)$ The context is the following: $
\frac{1}{n}\sum_{i}E(x_i) = \frac{1}{n}\sum_{i}E(x)
$ Why is that the expectation of a realized value of the random variable of x is equal to the expectation of x? Should I be looking at each $x_i$ as a random variable?","['expected-value', 'statistics', 'random-variables']"
4252607,"Let $f:[0,2\pi]\rightarrow[-1,1]$ satisfy $f(\theta)=\sum_{r=0}^n(a_r\sin(r\theta)+b_r\cos(r\theta))$ for $a_h,b_i\in \mathbb R$. If $|f(x)|=1$...","Let $f:[0,2\pi]\rightarrow[-1,1]$ satisfy $f(\theta)=\sum_{r=0}^n(a_r\sin(r\theta)+b_r\cos(r\theta))$ for $a_h,b_i\in\mathbb R$ . If $|f(x)|=1$ for exactly $2n$ distinct values in $[0,2\pi)$ , then prove that the number of distinct solutions of $(f''(x))^2+f'(x)f'''(x)=0$ can be $4n,4n-1$ or $4n-2$ . I know that $-\sqrt{a^2+b^2}+c\leq a\cos(\theta)+b\sin(\theta)+c\leq \sqrt{a^2+b^2}+c$ . But this same formula can't be extended to the entire series since it's not necessary that $\sin(2\theta)$ and $\sin(\theta)$ give the minimum value for the same value of $\theta$ . If we prove that $1$ is the maximum value of $f(\theta)$ , we can prove that $f'(x)=0$ has $2n$ roots. This means $f''(x)$ has $2n-1$ roots (by Rolle's theorem). $f'(x)=\sum_{r=0}^nra_r\cos(r\theta)-rb_r\sin(r\theta)$ $f''(x)=\sum_{r=0}^n-r^2a_r\sin(r\theta)-r^2b_r\cos(r\theta)$ $f'''(x)=\sum_{r=0}^n -r^3a_r\cos(r\theta)+r^3b_r\sin(r\theta)$","['trigonometry', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
4252619,Is $xyz=k^3$ a developable surface,"I don't know how to judge whether it's a developable surface or not.I think firstly I should find a way to write the surface as a form of a ruled surface,that is,write the surface something like $$r(u,v)=a(u)+vl(u)$$ ,but setting $x=u,y=v,z=k^3/uv$ I can't get the desired form.Does anyone know how to do it.Thank you Edit1:I find there is a fact that a surface is a developable surface iff there is a length-preserving mapping to a plane locally.So the surface above has a first fundamental form of $$(1+k^6/u^4v^2)(du)^2+(2k^6/u^3v^3)dudv+(1+k^6/u^2v^4)(dv)^2$$ ,hence letting $u'=k^3/uv,v'=u+v$ ,we can change the first fundamental form into something like $$(du')^2+(dv')^2$$ which is the first fundamental form of a plane,so I think it's a developable surface.The problem seems solved. Edit2:When I read afterward context of the book I find it's very easy to do it by judging whether the Gauss curvature is zero or not.Now another question is that if I change the wrong transformation above into a complex one,using $$u'=k^3/uv+0i,v'=u+vi$$ we can get $|du'|^2+|dv'|^2$ ,but I'm not sure that whether such complex transformation belongs to a length-preserving mapping or not?",['differential-geometry']
4252651,The convex cone of a compact set not including the origin is always closed?,"Based on the examples given in https://en.wikipedia.org/wiki/Conical_combination , we know that the convex cone generated by a set containing the origin point does not necessarily a closed set, even if the convex cone is generated by convex compact set. In addition, we also have the following known result. S is a non-empty convex compact set which does not contain the origin, the convex conical hull of S is a closed set. I am wondering if we relax the condition of convexity, is there a case such that the convex conical hull of compact set in $\mathbb{R}^n$ not including the origin is not closed.","['convex-geometry', 'analysis', 'general-topology', 'convex-analysis', 'compactness']"
4252672,Confused by the many versions of Hahn-Banach Separation Theorem,"I am studying the Hahn-Banach Separation Theorem. However, there are many versions of the theorem, which is quite confusing. So I want to ask some questions to clarify my confusion. First, some defintions. The following definitions are adopted from the book Infinite Dimensional Analysis - A Hitchhiker's Guide Let $X$ be a real vector space and $A, B \subseteq X$ be two
non-empty, disjoint sets. A hyperplane is a set of the form $\{f = c\}$ ,
where $f: X \to \mathbb{R}$ is a non-zero linear functional and $c \in \mathbb{R}$ . The hyperplane is closed if $X$ is topological
and $f$ is continuous . We say $A, B$ are weakly separated by a hyperplane $\{f = c\}$ if $ \sup f(A) \leq c \leq \inf f(B) $ . We say $A, B$ are properly separated by a hyperplane if it separates $A, B$ weakly and does not contain all of $A \cup B$ . We say $A, B$ are strictly separated by a hyperplane $\{f = c\}$ if $ f(a) < c < f(b) $ for all $a \in A, b \in B$ . We say $A, B$ are strongly separated by a hyperplane $\{f = c\}$ if $ \sup f(A) < c < \inf f(B) $ . Question 1 : Why do we need the extra condition in proper separation? Are there two non-empty, disjoint, convex sets that can be weakly separated but cannot be properly separated? The first version of Hahn-Banach Separation Theorem is purely algebraic and does not require any topological structure. Let $X$ be a real vector space and $A, B \subseteq X$ be two
non-empty, disjoint, convex sets. If one of $A, B$ has an internal point, then they can be properly separated by a hyperplane. Question 2: Is it true that any internal point $x_0$ of $A$ or $B$ must satisfy $f(x_0) < c$ or $f(x_0) > c$ ? The second version is from the wiki page . Let $X$ be a locally convex topological vector space and $A, B \subseteq X$ be two non-empty, disjoint, convex sets. If $A$ has a non-empty interior , then they can be properly separated by a closed hyperplane $\{f = c\}$ . Moreover, any $x_0 \in A^\circ$ must satisfy $f(x_0) < c$ . Question 3 : The wiki page requires $X$ to be locally convex. However, from what I have read from ""Infinite Dimensional Analysis"", it seems that $X$ is only required to be a topological vector space. Does this version still hold if $X$ is merely a topological vector space? Let $X$ be a topological vector space and $A, B \subseteq X$ be
two non-empty, disjoint, convex sets. If both $A, B$ are open , then
they can be strictly separated by a closed hyperplane. This seems to be a direct corollary of the previous version, provided that the previous version still holds if $X$ is merely a topological vector space . Let $X$ be a locally convex topological vector space and $A, B \subseteq X$ be two non-empty, disjoint, convex sets. If $A$ is closed and $B$ is compact ,
then they can be strongly separated by a closed hyperplane. Question 4 : I know that if $B$ is merely closed, then we cannot even separate $A, B$ strictly, let alone strong separation. What if we merely need proper separation or weak separation? Can we separate two closed, non-empty, disjoint, convex sets properly or weakly?","['general-topology', 'functional-analysis', 'real-analysis']"
4252707,Compactness and Schauder basis,"Let $X$ be a (real) Banach space and $(x_{n})_{n\geq 1}\subset X$ a Schauder basis, that is to say, for each $x\in X$ there are a unique sequence $(a_{n})_{n\geq 1}\subset \mathbb{R}$ such that $$
x =\sum_{n\geq 1}a_{n}x_{n}.
$$ ADDED (EDITED):  We assume $\|x_{n}\|=1$ for each $n\geq 1$ . In fact, for each $n\geq 1$ , taking a sequence $(x_{n}^{*})_{n\geq 1}\subset X^{*}$ (the topological dual of $X$ ) such that $x_{m}^{*}(x_{n})=\delta_{mn}$ (The Kronecker symbol) we have $a_{n}=x_{n}^{*}(x)$ . My question is the following: Assume $K\subset X$ is compact. Can we state that given any $\varepsilon>0$ there is $n_{0}:=n_{0}(\varepsilon)$ such that $|x_{n}^{*}(x)|\leq \varepsilon $ , for all $n\geq n_{0}$ and $x\in K$ ? I know that the above is true for the $\ell_{p}$ spaces ( $1\leq p<\infty$ ), the usual Banach space of the sequebces such that $\sum_{n\geq 1}|x_{n}|^{p}<\infty$ . Many thanks in advance for your comments.","['banach-spaces', 'general-topology', 'schauder-basis', 'functional-analysis']"
4252815,"You have letters: $A,A,C,D,E$ in a bag. You pick $3$ at random without putting them back. What is the total number of combinations you can make?","You have letters: $A,A,C,D,E$ in a bag. You pick $3$ at random without putting them back. What is the total number of combinations you can make? I am doing $\binom{5}{3}$ , however, this results in $10$ . I know this must be incorrect because these are the list of all combinations possible: $[a,a,c],[a,a,d],[a,a,e],[a,c,d],[a,c,e],[a,d,e],[c,d,e]$ I would really appreciate someone showing me how this kind of problem is solved. It is basic but I can't seem to wrap my head around what's happening.","['combinations', 'combinatorics']"
4252857,Analytical expression for the determinant of block tridiagonal matrix,"I have a $3n\times3n$ matrix $M$ that is in the following block tridiagonal form: $$M=\begin{pmatrix} 
A & B^T & 0\\
B & A & UBU \\
0 & UB^T U & A\\
\end{pmatrix}$$ where $A,B,U$ are $n\times n$ real matrices. I also know that $A$ and $U$ are symmetric and $U$ is orthogonal (it is the exchange matrix ), and that $\det{B} = 0$ . I want to know if a closed-form analytical expression for the determinant of $M$ exists. What I have tried so far: From the paper mentioned in this post , I found that one can write $$\det{M} = \det{\Lambda_1}\det{\Lambda_2}\det{\Lambda_3},$$ with $\Lambda_1 = A$ , $\Lambda_2 = A - B \Lambda_1^{-1} B^T$ , and $\Lambda_3 = A - (UB^TU)\Lambda_2^{-1}(UBU)$ . I have not been able to progress beyond this point as I do not know of some clever way to do the inversions . Thanks!","['determinant', 'matrices', 'linear-algebra', 'block-matrices', 'tridiagonal-matrices']"
4252926,"ABCD is a parallelogram. a straight line through A meets BD at X, BC at Y and DC at Z. Prove that AX:XZ = AY:AZ","ABCD is a parallelogram. a straight line through A meets BD at X, BC at Y and DC at Z. Prove that $$AX:XZ = AY:AZ$$ My Approach I realised that since the question seems to ""data insufficient"" , it has got to do something with constructions . Seeing the ""ratio"" I thought that it must be related with Similar Triangles. Extend $AB$ Drop perpendiculars from points $X$ , $Y$ , $Z$ on $AB$ . Name the points of intersection as $P$ , $Q$ , $R$ respectively. Call $XP$ as $a$ , $YQ$ as $b$ and $ZR$ as $c$ . I simplified the L.H.S. and R.H.S. of the required proof and obtained this expression: $$\color{blue}{\frac{1}{a} = \frac{1}{b} + \frac{1}{c}}$$ which I by no means was able to proof. Then I assumed $\frac{AP}{AX} = \frac{AQ}{AY} = \frac{AR}{AZ} = k$ from the property of similar triangles. $$\frac{AX}{XZ} = \frac{a}{c-a} = \frac{(1-k^2){AX}^2}{(1-k^2)({AZ}^2 - {AX}^2)}$$ $$\frac{AX}{XZ} = \frac{AX^2}{(AZ+AX)XZ}$$ which is a contradiction as $AZ ≠ 0$ . Where is my fault and how can I solve this problem? Addendum When I saw that the antecedent and consequent were part of the same line segment, I did not realise that it can be solved without additional construction (because if $∆ABC  \sim ∆A'B'C'$ we can write $\frac{AB}{A'B'} = \frac{BC}{B'C'}$ and since points $A$ , $B$ , $C$ cannot be collinear , so the terms of the ratio cannot be the part of the same straight line). Just for the sake of curiosity , I want to ask what algorithm is to be followed to find the required triangles that are to be proven similar?",['geometry']
4252989,"Solvable groups of order $mn$ with $(m,n)=1$","$\textbf{Problem.}$ Let $G$ be a solvable group of order $mn$ , with $m,n>1$ and $(m,n)=1$ . Let $H,\bar{H},K,\bar{K}$ be subgroups of $G$ such that $|H|=|\bar{H}|=m$ and $|K|=|\bar{K}|=n$ . Show that there is $g \in G$ such that $\bar{H}=H^g$ and $\bar{K}=K^g$ . I tried to use Hall's theorem, to guarantee the existence of subgroups of order $m$ and $n$ , and any others are conjugated, but I couldn't show why it's possible to get the same element $g$ in this case.","['group-theory', 'finite-groups', 'solvable-groups']"
4253011,Prove $A\preceq B\neq \emptyset$ and $C\preceq D$ $\implies$ $A^C\preceq B^D$,"Question I am working on:
Let A,B,C,D be sets such that there exists injections $f:A\rightarrow B$ and $g:C\rightarrow D$ . Prove whenever $B\neq \emptyset$ , $\exists h:A^C\rightarrow B^D$ such that h is injective. ( $A^C$ represents the set of functions from $C\rightarrow D$ ). I have solved similar problems to this before - however I am struggling on how to construct a function with domain D despite not knowing any surjections to D itself. (Note this question is equivalent to the question in the title)",['elementary-set-theory']
4253027,How many $6$ digit numbers are there that doesn't have number $2$ in them and have two $1$?,"Is this the right way to calculate it? If the first number is $1$ , then there are $5$ ways to choose another $1$ and $4$ digits left that can be equal to $8$ different numbers
So in total $5\cdot8^4=20480$ If the first number isn't $1$ , then it can have $7$ different values. We can choose two $1$ in $5\cdot4$ ways and there are left $3$ digits that can be equal to $8$ different values.
In total $7\cdot5\cdot4\cdot8^3=71680$ . Adding all of this I get $92160$ different numbers.",['combinatorics']
4253032,"Prove that $S=\{(x,y):|x+y|\leq 1, |xy|\leq 1\}$ is a compact set","In my analysis test today, I was asked the question Prove that the set $$S=\{(x,y):|x+y|\leq 1, |xy|\leq 1\}$$ is compact in $\mathbb R^2$ . Now, of course, to prove compactness, we need to show that $S$ is closed and bounded, and then use the Heine Borel Theorem. The bounded part is easy to show, since the set is contained in a ball of radius $2$ . It was the closed part that sucked my blood out of me. I know that it's intuitively clear since the set contains the boundary. But, that's not an answer you write in your analysis test. What I did was to write $$S=S_1\cap S_2$$ where \begin{align*}
S_1&=\{(x,y):|x+y|\leq 1\}\\
S_2&=\{(x,y):|xy|\leq 1\}
\end{align*} and then tried to prove that $S_1^\prime$ and $S_2^\prime$ are open (which proves that $S_1$ and $S_2$ are closed, and hence their intersection is closed). In the case of $S_1^\prime$ , the distance between an arbitrary point and the lines $|x+y|\leq 1$ was easy to calculate explicitly; in the case of $S_2^\prime$ , not really so. Anyways, I feel, I wasn't rigorous enough, and there must be a neater way to solve this. Especially, I spent some time to find a continuous function $f$ such that the preimage of $f$ under some closed set is $S$ , but couldn't find it. I'm quite sure, there must be a function which does the job. Is there a better (than explicitly calculating distances) way to slove this? Here's a graph of $S$ , in case you are interested.","['analysis', 'real-analysis', 'continuity', 'calculus', 'compactness']"
4253048,"Do these very VERY weak axioms guarantee a group? (Every element has left identity $e_L$ or right identity $e_R$, with same-sided inverse)","I can't help but ask, after we've come so far weakening the group axioms in these two posts , whether we can get even weaker? Let $A$ be a set with an associative binary operation $*$ , and suppose there exist two elements $e_L,e_R\in A$ such that, for all $x\in A$ , at least one of the following two conditions holds: $e_L*x=x$ and there exists an $x'\in A$ such that $x'*x=e_L$ ; $x*e_R=x$ and there exists an $x'\in A$ such that $x*x'=e_R$ . Must $(A,*)$ be a group? The second linked post proves that the answer is “yes” in the case where $e_L=e_R$ . This generalized version was posed by @Yakk in the comments to @Vincent's answer in the first linked post.","['group-theory', 'abstract-algebra', 'semigroups', 'axioms']"
4253056,Proving $\bigwedge^k(U\cap V) = \left(\bigwedge^kU\right) \bigcap \left(\bigwedge^k V\right)$ via the universal mapping property,"Let $V$ be a (finite dimensional) vector space, its exterior algebra of order $k$ is the vector space $\bigwedge^k V$ consisting of the formal sums of terms of the form $v_1 \wedge v_2 \wedge \dots \wedge v_k$ , where each $v_i \in V$ (with a few additional properties regarding $\wedge$ ). There are quite a few ways to define $\bigwedge^k V$ rigorously, one of them is via the universal mapping property: There exists a vector space $L$ and an alternating multilinear map $M\colon V^k \to L$ with the universal mapping property in the sense that for any alternating multilinear map $M'\colon V^k \to L'$ , there exists a unique linear map $T\colon L\to L'$ such that $M' = T\circ M$ . This space $L$ is unique up to isomorphism and we define $\bigwedge^k V := L$ and write $v_1 \wedge v_2 \wedge \dots \wedge v_k:= M(v_1,v_2,\dots,v_k)$ . From the above construction, all the usual properties of $\bigwedge^k V$ follow, e.g. if $\{ e_1, \dots , e_n \}$ is a basis for $V$ , then $\{ e_{i_1} \wedge \dots \wedge e_{i_k} \}_{i_1<\dots<i_k }$ is a basis for $\bigwedge^k V$ . If $U$ is a subspace of $V$ , then $\bigwedge^k U$ can be canonically identified with a subspace of $\bigwedge^k V$ via the universal property (the inclusion map $i\colon U^k\to V^k$ composes with $M$ is an alternating multilinear map from $U^k$ into $\bigwedge^k V$ ). Now, let $W$ be a vector space and $U,V$ be subspaces of $W$ . I believe it is true that $\bigwedge^k(U\cap V) = \left(\bigwedge^kU\right) \bigcap \left(\bigwedge^k V\right)$ , which should be straight forward to prove  by fixing a common basis for $U,V$ in $W$ . However, I want to know if it can be demonstrated from the perspective of category theory using the universal mapping property. It seems that this would require some characterization of $U\cap V$ in terms of morphisms but my working knowledge of techniques  from category theory has mostly faded away at this point (not that there was much of it from the beginning). Any help is highly appreciated, especially if it's accompanied by a diagram :-)","['category-theory', 'universal-property', 'multilinear-algebra', 'linear-algebra', 'exterior-algebra']"
4253071,Generating function of a polynomial,"Suppose I want to find a simple formula for the generating function of a general polynomial sequence $a_n=P(n)$ . Obviously it is enough to find the generating function of the sequence $a_n=n^k$ for any integer $k \geq 0$ . If we let $A_k(x) = \sum_{n=0}^{
\infty}n^kx^n$ , it is not hard to see that: $$A_0(x)=\frac{1}{1-x}, A_k(x)=xA'_{k-1}(x)$$ My question is, it is possible from here to deduce an explicit formula for $A_k$ ?","['power-series', 'combinatorics', 'formal-power-series', 'generating-functions']"
4253081,If $A$ is Lebesgue measurable there exist open sets $G_1\supset G_2\supset\dots$ containing $A$ such that $|(\bigcap_{k=1}^{\infty}G_k)\setminus A|=0$,"I have proved the following statement and I would like to know if it is correct and/or/if it could be improved somehow, thanks: ""If $A$ is Lebesgue measurable there exists a decreasing sequence $G_1\supset G_2\supset\dots$ of open sets containing $A$ such that $|\bigcap_{k=1}^{\infty}G_k\setminus A|=0$ "" My proof: Let $A$ be a Lebesgue measurable set: then we know that for every $\varepsilon>0$ there exists an open set $H\supset A$ such that $|H\setminus A|<\varepsilon$ . This implies that for every $n\geq 1$ there exists $H_n\supset A$ open such that $|H_n\setminus A|<\frac{1}{n}$ so if we define for every $k\geq 1$ the set $G_k:=\bigcap_{n=1}^{k} H_k$ we have that the $G_k$ s form a decreasing sequence ( $G_{k+1}=\bigcap_{n=1}^{k+1}H_k=\bigcap_{n=1}^{k}H_k \cap H_{k+1}\subset \bigcap_{n=1}^{k}H_k=G_k$ ) of open sets (since each $G_k$ is the intersection of finitely many open sets) containing $A$ ( $A\subset H_n$ for every $n\geq 1$ implies $A\subset \bigcap_{n=1}^{k}H_n=G_k$ ). Now, since for every $m\geq 1$ we have that $(\bigcap_{k=1}^{\infty}G_k)\setminus A\subset G_m\setminus A\subset H_m\setminus A$ and hence $|(\bigcap_{k=1}^{\infty}G_k)\setminus A|\leq |G_m\setminus A|\leq |H_m\setminus A|<\frac{1}{m}$ it follows that $|(\bigcap_{k=1}^{\infty}G_k)\setminus A|=0$ , as desired. $\square$","['measure-theory', 'solution-verification', 'lebesgue-measure', 'real-analysis']"
4253096,Section rings map,"Let $X$ be a projective variety contained in $\mathbb{P}^n$ , over the field of complex numbers. I think that the inclusion $X\subset \mathbb{P}^n$ is controvariantly given by the surjective map among finitely generated algebras $$i^*:\bigoplus_{m\geq 0} S^mH^0(X,\mathcal{O}_X(1))\to \bigoplus_{m\geq 0} H^0(X,\mathcal{O}_X(m))$$ I say I think because I'm not enirely sure, I've just use the definition of $\mathbb{P}$ as the Proj of the symmetric algebra (a reference regarding this controvariant surjective map would be very helpful). What I would like to understand is what this map is defined: since $X$ i know $H^0(X,\mathcal{O}_X(1))$ is a finite dimensional vector space of dimension $n+1$ ; let's call $s_1,\ldots,s_{n+1}$ the generating sections. By doing the symmetric algebra I learnt I'm creating a polynomial ring $\mathbb{C}[s_1,\ldots,s_n]$ . I obtain therefore $$\bigoplus_{m\geq 0}\mathbb{C}[s_1,\ldots,s_{n+1}]_m\to \bigoplus_{m\geq 0} H^0(X,\mathcal{O}_X(m))$$ But now I'm a bit confused: is this map the identity? It can't be since $X$ is not the whole projective space, so this is like a restriction of the sections on the points of $X$ ? I'm quite desperate since I don't know where to look to properly understand this passage.",['algebraic-geometry']
4253150,Contour integral around a triangle equals the remainder term of the derivative - why?,"This proof has two oddities in it. It is a proof of the Cauchy-Goursat theorem for triangle contours, i.e. the contour integral of a holomorphic function around the boundary of a triangle in $\Bbb C$ is zero. In the following, $\triangle$ is the original triangle and $\triangle_n$ is a sequence of triangles, where $\triangle_n\subseteq\triangle_{n-1}$ , and $\triangle_0$ is defined as $\triangle$ . $$\triangle\bigcap_{n=1}^\infty\triangle_n=\{z_0\}$$ For some unique $z_0$ , and they reference Cantor - what is the name of this theorem? The main  question however is about this: $$\int_{\partial\triangle_n}f(z)\,\mathrm{d}z=\int_{\partial\triangle_n}(f(z)-f(z_0)-f'(z_0)(z-z_0))\,\mathrm{d}z$$ They offer no explanation for this, other than that $f$ is complex differentiable at $z_0$ . How can it be that the contour integral is equal to this expression? By definition of differentiability, $f(z)=f(z_0)+f'(z_0)(z-z_0)+\psi(z)(z-z_0)$ , where $\psi$ is continuous and $\lim_{z\to z_0}\psi(z)=0$ , so the RHS of that integral is infact just the remainder term, $\psi(z)(z-z_0)$ , meaning that they have assumed that: $$\int_{\partial\triangle_n}f(z)\,\mathrm{d}z=\int_{\partial\triangle_n}\psi(z)(z-z_0)\,\mathrm{d}z$$ Which is only explainable (as far as I can tell) if you presuppose the conclusion of that proof, which is that the contour integral around a triangle is $0$ . What am I missing? Many thanks.","['complex-analysis', 'proof-explanation', 'contour-integration']"
4253167,Convergence in $\mathcal{M}$ implies convergence in measure on sets of finite measure,"Let $(\Omega,\mathcal{A},\mu)$ be a $\sigma$ -finite measure space such that $\Omega= \bigcup_{n=1}^\infty A_n$ where $\mu(A_n)<\infty$ for all $n \in \mathbb{N}$ . Denote by $\mathcal{M}$ the set of all scalar valued $\mu$ -measurable functions on $\Omega$ that are finite $\mu$ -a.e. I know that $d(f,g)= \sum_{n=1}^{\infty} \frac{1}{2^n} \cdot \frac{1}{\mu(A_n)} \cdot \int_{A_n}\frac{|f-g|}{1+|f-g|}d\mu$ defines a metric in $\mathcal{M}$ and I want to prove that if $d(f_m,f) \rightarrow 0$ , then $f_m \rightarrow f$ in measure on sets of finite measure (I have already proved the converse). My attempt We assume that $d(f_m,f) \rightarrow 0$ as $m \rightarrow + \infty$ .
Let $\varepsilon>0$ and $D \subset \Omega$ have finite measure.
We can write $\{x \in D:|f_m(x)-f(x)| \geq \varepsilon\}= \bigcup_{n=1}^{\infty}(A_n \cap \{x \in D:|f_m(x)-f(x)|\geq \varepsilon\})$ so that \begin{equation}
\begin{split}
\frac{\varepsilon}{1+ \varepsilon} \mu(\{x \in D:|f_m(x)-f(x)| \geq \varepsilon\}) &\leq \sum_{n=1}^{\infty} \frac{\varepsilon}{1+ \varepsilon}\mu(A_n \cap \{x \in D:|f_m(x)-f(x)| \geq \varepsilon\})\\
&= \sum_{n=1}^{\infty} \int_{A_n \cap \{x \in D:|f_m(x)-f(x)| \geq \varepsilon\}} \frac{\varepsilon}{1+ \varepsilon} d\mu\\
&\leq \sum_{n=1}^{\infty} \int_{A_n } \frac{|f_m-f|}{1+ |f_m-f|} d\mu
\end{split}
\end{equation} where I don't know how to continue due to the lack of the terms $\frac{1}{2^n} \cdot \frac{1}{\mu(A_n)}$ . If those were inside the sum, then the assumption $d(f_m,f) \rightarrow 0$ would  give the desired result.","['measure-theory', 'convergence-divergence', 'metric-spaces']"
4253267,Time dependent Sobolev spaces and parabolic PDE,"Let $X$ be a Banach space and $u \in L^p(0,T;X).$ Then $v \in L^p(0,T;X)$ is called weak derivative of $u$ if \begin{eqnarray}\tag{1}
\int\limits_0^T u(t)\,\phi_t \,\mathrm dt = -\int\limits_0^T v(t)\,\phi(t)\,\mathrm dt \quad \text{for all } \phi \in C_c^{\infty}((0,T)). 
\end{eqnarray} The weak derivative $v$ is denoted by $u'$ . While defining the weak formulation of Linear parabolic PDEs (for example, $u_t-\Delta u=0$ ), the weak formulation is defined for $u \in L^2(0,T;H^1_0(\Omega))$ and $u'\in L^2(0,T;H^{-1}(\Omega)).$ My doubts are the following: Why do we need $u'\in L^2(0,T;H^{-1}(\Omega))?$ and why not $u' \in L^2(0,T;H^1_0(\Omega))$ as in the definition of weak derivative? In view of (1), how to interpret $u'=v \in L^p(0,T;X^*)$ as the weak derivative of $u \in L^p(0,T;X)?$ P.S. : Notations are same as the ones used Chapter 5 and 7 of the book ""Partial Differential Equations'' by L.C. Evans.","['parabolic-pde', 'analysis', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4253274,If the trace of the matrices $A^k$ are equal to the size of $A$ why the spectrum of $A$ is $\{1\}$,"Let $A$ a matrix in $\mathcal{M}_n(\mathbb C)$ such that $\operatorname{Tr}(A^k)=n$ for all $k\in\{1,\ldots,n\}$ . I have to prove that $\operatorname{Sp}(A)=\{1\}$ . I denoted $\lambda_1,\ldots,\lambda_n$ the eigenvalues of $A$ with multiplicity so the hypothesis leads to the system of equations: $\lambda_1^k+\cdots+\lambda_n^k=n$ for all $k$ . I can see that $\lambda_1=\cdots=\lambda_n=1$ is a trivial solution of the system but it's not obvious for me why it would be the unique solution. Any suggestion?","['matrices', 'trace', 'eigenvalues-eigenvectors']"
4253292,Kernel of morphism of Kähler modules,"Let $f:X\to Y$ be a morphism of $k$ -schemes. Then we have a natural morphism of associated Kähler modules $\Omega_{Y}\to f_*\Omega_X$ derived by adjunction from the canonical morphism $f^*\Omega_Y\to \Omega_X$ . If $f$ is smooth, then this morphism is injective. My question is if we can express the kernel of this morphism in general, that is when $f$ is not smooth. You would assume that this is measured by some deformation-theoretic object, presumably heuristically measuring how ""unliftable"" some unliftable morphisms are, but I'm unaware of such a thing.","['algebraic-geometry', 'deformation-theory']"
4253369,"Why does the finiteness of the Haar measure of Siegel set implies that $SL(n,\mathbb Z)$ is a lattice?","Let $G=\operatorname{SL}(n,\mathbb R)$ and $\Gamma =\operatorname{SL}(n, \mathbb Z)$ . Suppose we have already shown that $G=\Sigma_{t,u} \Gamma$ for some subset $\Sigma_{t,u}$ of $G$ , called Siegel set. Let $\mu$ be a Haar measure on $G$ . Suppose also we have shown that $\mu(\Sigma_{t,u})$ is finite. I wonder how to show that there exists a $G$ -invariant measure on $G/\Gamma$ such that $\mu(G/\Gamma)$ is finite. First obstacle: Let $\pi\colon G\to G/\Gamma$ be the canonical projection. I am not sure if the pushforward measure $\pi_*\mu$ on $G/\Gamma$ is $G$ -invariant or not. Since it is unclear to me whether $$\mu(\pi(gU))=\mu(g\pi(U))$$ or not. Second obstacle: I don't see $\mu(\Sigma_{t,u})$ being finite implies $\pi_*\mu(G/\Gamma)$ being finite.","['measure-theory', 'haar-measure', 'lie-groups']"
4253378,"How to prove $\int_{0}^{\infty} \frac{(1-x^2) \, \text{sech}^2\left(\frac{\pi x}{2} \right)}{(1+x^2)^2}\, dx = \frac{\zeta(3)}{\pi}$?","I was recently searching for interesting looking integrals. In my search, I came upon the following result: $$ \int_{0}^{\infty} \frac{(1-x^2) \, \text{sech}^2\left(\frac{\pi x}{2} \right)}{(1+x^2)^2}\, dx = \frac{\zeta(3)}{\pi}$$ and I wanted to try and prove it. Inspired by this answer by Jack D'Aurizio, I took the Weierstrass product for $\cosh(x)$ to obtain $$
\cosh\left(\frac{\pi x}{2} \right) = \prod_{n \ge 1}\left(1 + \frac{x^2}{(2n-1)^2} \right)
$$ And by logarithmically differentiating twice we get $$
\frac{\pi^2}{4}\text{sech}^2\left(\frac{\pi x}{2} \right)  = \sum_{n \ge 1} \frac{4(2n-1)^2}{\left(x^2 + (2n-1)^2\right)^2} - \frac{2}{x^2 + (2n-1)^2}
$$ Which means we get \begin{align*}
\int_{0}^{\infty} \frac{(1-x^2) \, \text{sech}^2\left(\frac{\pi x}{2} \right)}{(1+x^2)^2}\, dx & =\frac{4}{\pi^2}\sum_{n\ge 1} \int_{0}^{\infty}  \frac{(1-x^2)}{(1+x^2)^2}\left( \frac{4(2n-1)^2}{\left(x^2 + (2n-1)^2\right)^2} - \frac{2}{x^2 + (2n-1)^2}\right)\, dx
\end{align*} However, after this, I couldn't figure out how to evaluate the resulting integral. Does anyone know how I could continue this method? Or alternatively, does anyone know another way in which the result can be proven? Thank you very much!! Edit: Per jimjim's request, I'll add that I found this integral on the Wikipedia article for $\zeta(3)$ . I believe the reference is to this text where the following formula is given $$
(s-1) \zeta(s) = 2\pi \int_{\mathbb{R}}\frac{\left(\frac{1}{2} + xi \right)^{1-s}}{\left(e^{\pi x} +e^{-\pi x} \right)^2}\, dx
$$ which for the case of $s=3$ reduces to the surprisingly concise $$
\int_{\mathbb{R}}\frac{\text{sech}^2(\pi x)}{(1+2xi)^2} \, dx = \frac{\zeta(3)}{\pi}
$$ And I presume that one can modify the previous equation to get to the original integral from the question, but it is not apparent to me how this may be done. Edit 2: Random Variable has kindly posted in the comments how to go from $\int_{\mathbb{R}}\frac{\text{sech}^2(\pi x)}{(1+2xi)^2} \, dx$ to $ \int_{0}^{\infty} \frac{(1-x^2) \, \text{sech}^2\left(\frac{\pi x}{2} \right)}{(1+x^2)^2}\, dx$ . Thank you very much!","['integration', 'definite-integrals', 'hyperbolic-functions', 'calculus', 'riemann-zeta']"
4253400,"Modified coupon collector's problem, where you can trade off excess coupons","I am struggling with a variation of the coupon collector's problem. Suppose that we need to collect $n$ distinct coupons by buying boxes of toys, where each box contains one coupon with uniform probability. The manufacturer however allows customers to trade any ten coupons (not necessarily the same) for any one coupon. Now suppose that my strategy is to keep buying boxes and hoarding coupons, until I can trade all my excess coupons to get an entire set of $n$ distinct coupons. What is then the expectation of the number of boxes that I need to buy? To be more precise, let $X_i$ be the number of unique coupons after buying $i$ boxes, and let $Y_i = i - X_i$ . Let $I = \min\{i\in \mathbb{N}:X_i + \frac{Y_i}{10} \geq n\}$ . I am then asking for $\mathbb{E}(I)$ . (Edit: cleaned up the notations in the last part)","['coupon-collector', 'probability']"
4253410,Show that $f$ is the zero function if$f''(x)+f(x)=0$ and $f(0)=f'(0)=0$,"Suppose that $f''(x)+f(x)=0$ for all x, and $f(0)=f'(0)=0$ . Note: we know $f^{(n)}$ (i.e. nth derivative of $f$ ) is continuous for all n. Note: $f$ can be complex. Note: I want to use this to prove Euler's formula $e^{ix}=\cos x+i\sin x$ . I started with $P=\frac{df}{dx}$ , then $f''(x)=P\frac{dP}{df}$ . Get $P\frac{dP}{df}+f=0$ and integrate at both sides, so that $P^2+f^2=0$ . And we can get $P=\pm i f \Rightarrow \frac{df}{dx}=\pm if \Rightarrow \frac{1}{f}df=\pm i dx \Rightarrow \ln f=\pm ix+C$ , and I stuck here. Is the steps above are right or there's another way of prove?","['complex-analysis', 'ordinary-differential-equations']"
4253418,Characterizing fppf-algebras,"Background: All rings are commutative with $1$ . Let $R$ be a ring. We say that an $R$ -algebra $\varphi : R \to S$ is an fppf- $R$ -algebra (see e.g. Jantzen ""Representations of Algebraic Groups"") if it is faithfully flat (as an $R$ -module) and finitely presented (as an $R$ -algebra). The latter condition means that (as an $R$ -algebra) $S$ is isomorphic to $R[x_1, \ldots, x_n]  / (f_1 , \ldots, f_m)$ for some $n$ and $m$ , and some polynomials $f_i$ . Question: Is there a nice characterization of when a finitely presented $R$ -algebra is faithfully flat in terms of the polynomials $f_i$ (or the ideal they generate)? Or if that's too much to ask, what can be said about these polynomials? Here are some easy observations along these lines: Observations: Since faithfully flat ring homomorphisms are injective, we must have $R \cap (f_1 , \ldots, f_m) = 0$ inside $R[x_1, \ldots, x_n]$ . (A re-phrasing of the previous observation.) Since a quotient $R \to R / I$ is faithfully flat if and only if $I = 0$ ( $I \otimes_R R / I = 0$ hence by faithful flatness $I = 0$ ), we cannot have $(x_1, \ldots, x_n) \subsetneq (f_1, \ldots, f_m)$ . If $f$ is monic, $R[x] / (f)$ is an fppf- $R$ -algebra. $R[x_1, \ldots, x_n]$ is an fppf- $R$ -algebra.","['homological-algebra', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4253422,explicit relation between interior derivative and alternation operation,"Let $X$ be a vector field, $L_X$ the Lie derivative, $i_X$ the interior derivative, $A$ the alternation operation (that assign for each covariant tensor field $K$ , a differential form $AK$ ). The definitions I am following are: $$(AK)(X_1,\cdots, X_r)=
 \frac{1}{r!}\sum_{\sigma\in S_r}{\rm sgn}(\sigma)K(X_{\sigma(1)},\cdots,X_{\sigma(r)}) $$ $$L_X(\omega)(X_1,\cdots,X_k)=X(\omega(X_1,\cdots,X_k))
-\sum_{i=1}^k \omega(X_1,\cdots, [X,X_i],\cdots, X_k),$$ $$(i_X\omega)(X_1,\cdots, X_{k-1})=k\omega(X,X_1,\cdots,X_{k-1}),$$ for $X_i\in \mathfrak{X}(M)$ ; $1\leq i\leq k$ . It is easy to see that the operators $L_X$ and $A$ commute with each other; that is, for a covariant tensor field $K$ , we have $$L_X(AK)=A(L_XK).$$ This commutativity condition fails in case of $i_X$ and $A$ . Let $K$ be a covariant tensor field of degree $r$ . Let $X_2,\cdots,X_r$ be vector fields on $M$ . We have $$\begin{align}i_{X_1}(AK)(X_2,\cdots,X_r)=&r (AK)(X_1,X_2,\cdots,X_r)\\
=&r\frac{1}{r!}\sum_{\sigma\in S_r}{\rm sgn}(\sigma) K(X_{\sigma(1)},\cdots,X_{\sigma(r)})\\ 
=&r\frac{1}{r!}\sum_{\sigma\in S_r|\sigma(1)=1}{\rm sgn}(\sigma) K(X_{\sigma(1)},\cdots,X_{\sigma(r)})+r\frac{1}{r!}\sum_{\sigma\in S_r|\sigma(1)\neq1}{\rm sgn}(\sigma) K(X_{\sigma(1)},\cdots,X_{\sigma(r)})\\ 
=&\frac{1}{(r-1)!}\sum_{\sigma\in S_{r-1}}{\rm sgn}(\sigma) K(X_1,X_{\sigma(2)},\cdots,X_{\sigma(r)})+({\rm something})\\
=&\frac{1}{(r-1)!}\sum_{\sigma\in S_{r-1}}{\rm sgn}(\sigma) i_{X_1}K(X_{\sigma(2)},\cdots,X_{\sigma(r)})+({\rm something})\\
=&A(i_{X_1}K)(X_2,\cdots,X_r)+({\rm something})
\end{align}$$ So, $i_X$ and $A$ does not really commute. We have $$i_X(AK)(X_1,\cdots,X_r)=A(i_XK)(X_1,\cdots,X_r)+({\rm something})$$ I am trying to see if there is a nice expression for the ``something"" part. It would just be an appropriate rearrangement of terms, but, I am not able to do it in a clever way. Any help is appreciated.",['differential-geometry']
4253427,Find the matrix representation of $A$ with respect to basis given.,"Let $A = \left[\begin{matrix} 2 & 3 & 4 \\ 8 & 5 & 1\end{matrix}\right]$ and consider $A$ as a linear transformation from $\mathbb{R}^3$ into $\mathbb{R}^2$ using the standard bases. Find the matrix representation of $A$ with resepect to the basis $\left\{\space\left[\begin{matrix}1 \\ 1 \\ 0\end{matrix}\right], \left[\begin{matrix}0 \\ 1 \\ 1\end{matrix}\right], \left[\begin{matrix}1 \\ 0 \\ 1\end{matrix}\right]\space\right\}$ for $\mathbb{R}^3$ and $\left\{ \left[\begin{matrix}3 \\ 1\end{matrix}\right], \left[\begin{matrix}2 \\ 1\end{matrix}\right] \right\}$ for $\mathbb{R}^2$ . Here is my attempt taking from these two posts post_1 and post_2 : Let $C = \left[\begin{matrix}\mathbf{c}_1 & \mathbf{c}_2  & \mathbf{c_3}\end{matrix}\right]$ , that is, the matrix representation of the vectors of the given basis for $\mathbb{R}^3$ . Similarly represent the basis vectors for $\mathbb{R}^2$ as $B = \left[\begin{matrix}\mathbf{b}_1 & \mathbf{b}_2 \end{matrix}\right]$ . Then, apply the transformation $T(\mathbf{c}_1) = A\mathbf{c}_1 = \mathbf{v}_1$ . We have a new vector now in $\mathbb{R}^2$ . Side note: formula for coordinate vector of $\mathbf{v}$ with respect to the basis B. $$(1)\quad\quad  \mathbf{v} = B[\mathbf{v}]_B \implies B^{-1}\mathbf{v} = [\mathbf{v}]_B$$ Now using formula (1) to find $\mathbf{v}_1$ in terms of our basis for $\mathbb{R}^2$ we have $B^{-1}\mathbf{v}_1$ . Thus, this is our first vector in our new matrix representation of A. Repeat this process for $\mathbf{c}_2$ and $\mathbf{c}_3$ . Generalizing this process with the help of post_2 we get $$(2)\quad\quad B^{-1}AC$$ This is our new matrix and our representation for matrix A. Is this correct? Is there something I am missing? Furthermore, my intuition for this is that we are given matrix $A$ that maps vectors $\mathbb{R}^3 \to \mathbb{R}^2$ . Then we need to map the given basis vectors (IDK ... thb) for $\mathbb{R}^3$ into $\mathbb{R}^2$ . But now we need to write these new vectors (hence new matrix) in terms of our given basis for $\mathbb{R}^2$ . So we take each vector $\mathbf{v}_i$ , $1 \leq i \leq 3$ , and do $\mathbf{v}_i = B[\mathbf{v}_i]_B \implies B^{-1}\mathbf{v}_i = [\mathbf{v}_i]_B$ . That is now we have each vector represented now in terms of the basis in $\mathbb{R}^2$ which gives us the matrix representation. Let me know if what I have done or my intuition needs correcting! I would love all the help!","['matrices', 'change-of-basis', 'solution-verification', 'linear-algebra', 'linear-transformations']"
4253444,"Clean proof for trigonometry identity? I know what the answer is, but I feel like there should be like a $1$-$2$ liner to compute this","Fix $j,k$ with $0 \leq j,k \leq N$ . If $j+k$ is even, (i.e. if $j,k$ have same parity), then $$
\sum_{n=1}^{N-1} \cos\left(\frac{j\pi}Nn\right)\sin\left(\frac{k \pi}Nn\right) = 0 
$$ and $$
\sum_{n=0}^{N-1} \cos\left(\frac{j\pi}Nn\right) \cos\left(\frac{k\pi}Nn\right) = 0 
$$ Except the second sum is $4$ for $j=k$ .","['algebra-precalculus', 'trigonometry', 'summation']"
4253521,Is logarithmic differentiation method missing the cases that $f(x)=0$?,"I am learning logarithmic differentiation. It goes like this: First we define a function $$L_0(x)=\log|x|=\int_1^{|x|}\frac{1}{t}dt$$ After studying the positive and negative ranges, we know $$L_0'(x)=\frac{1}{x}$$ for all real $x\ne 0$ .
Apply the above to a function $f(x)$ , we have $$g'(x)=(L_0(f(x)))'=L_0'(f(x))f'(x)=\frac{f'(x)}{f(x)}$$ So $f'(x)=g'(x)f(x)$ . However we made an assumption the moment we introduced $L_0(f(x))$ : $f(x)\ne 0$ . Is the logarithmic derivative method missing the cases when $f(x)=0$ ? Or to put it in another way, why do we trust the result that it will work for roots of $f(x)$ ?","['derivatives', 'logarithms']"
4253566,How to color a cube with exactly 6 colors using Polya enumeration theory,"I have seen in other questions ( Painting the faces of a cube with distinct colours ) , and found for myself there are 30 colorings of the cube with exactly 6 colors. My issue is when I use Polya enumeration theory I take the number of colorings up to 6 to be $\frac{1}{24}(6^6+3*6^4+12*6^3+8*6^2)$ ( https://en.wikipedia.org/wiki/P%C3%B3lya_enumeration_theorem ) and subtract the number of colorings up to 5, $\frac{1}{24}(5^6+3*5^4+12*5^3+8*5^2)$ to get 1426. What is wrong with my application of Polya enumeration theory? How could I fix it?","['polya-counting-theory', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4253634,Prove that the product of two relations is the identity relation if both relations are bijective maps,"So the question is: Suppose $R_1$ and $R_2$ are relations on a set $S$ with $R_1\circ R_2 = \operatorname{I}$ and $R_2\circ R_1 = \operatorname{I}$ . Prove that both $R_1$ and $R_2$ are bijective maps. I know that I is the identity relation which means $\operatorname{I} = \{(\alpha,\alpha)|\alpha\in S\}$ and so for $R_1\circ R_2 = \operatorname{I}$ we have to have $\forall x: \exists z: (x,z)\in R_1 \land (z,x)\in R_2$ . I also know that maps are functions which means that every object most have only one image, that surjective maps are function where all objects have one image but different objects can have same images, I also know that injective means that all objects have only one image and that different objects have different images, but not all images need to have an object and that a bijective map is when all objects have a different image and all images have a different object (so subjective and injective).
But I have no idea how to use all this information to solve my problem. Can anybody help?","['elementary-set-theory', 'functions', 'relations']"
4253659,"If $f$ is differentiable on [a,b], must there be a subinterval on which$f’(x)$ be bounded?","If $f(x)$ is differentiable on $[a,b]$ , then there must be $(\alpha,\beta)\subset [a,b]$ , such that $f’(x)$ is bounded on $(\alpha,\beta)$ . To prove it or to give an counter-example. The question may have something to do with the intermediate value theorem. I understand if there is no such $(\alpha,\beta)$ , then for every $x\in[a,b]$ , there exists ${x_n}$ such that $\lim_{n\rightarrow\infty} x_n=x$ , and $\lim_{n\rightarrow\infty} |f’(x_n)|=+\infty$ . But I still cannot work things out.","['derivatives', 'analysis']"
4253667,Total Derivative : How to correct the practical definition ? a+h out of set.,"Definition : Total Derivative over an open set of a $\mathbb{R}$ vectorial space : Let E and F be two $\mathbb{R}$ vectorial spaces. Let $U \subset E$ be an open subset of E. Let $f:U \rightarrow F$ be an application and $x_0 \in U$ . $f$ is said to be totally differentiable in $x_0$ if and only if there exist a continuous linear map $L:E \rightarrow F$ that respects $$f(x_0+h)=f(x_0)+L(h)+o(\Vert h \Vert)$$ It is the definition i find in most books i have , but it is very ambiguous to me , 1)First of all, $h$ needs to be defined. It cannot be said in E (without any other condition)
, or in U(without any other condition) , because else it would in all case contradict the definition of $f$ (first case , if we define h to be in E , no reason $x_0+h$ to be in U. second case if we define h to be in U , no reason $x_0+h$ to be in U.) A clear example is to consider $A\rightarrow A^{-1}$ from $GL_n(\mathbb{R})\rightarrow GL_n(\mathbb{R})$ . It is consider to be a differentiable function for a lot of author. But the sum of two inversible matrix has no reason to remain an inversible matrix. So what i understand is, we tinker it with hands , saying ""well consider h close enough from $x_0$ and everything will be fine as your set is open"" But L is unique , and so if i say Definition : Total Derivative over an open set of a $\mathbb{R}$ vectorial space : Let E and F be two $\mathbb{R}$ vectorial spaces. Let $U \subset E$ be an open subset of E. Let $f:U \rightarrow F$ be an application and $x_0 \in U$ . f is said to be totally differentiable in $x_0$ if and only if there exist a continuous linear map $L:E \rightarrow F$ that respects for all $h\in U$ such that $x_0+h\in U$ $$f(x_0+h)=f(x_0)+L(h)+o(\Vert h \Vert)$$ I feel like breaking the definition. Can someone give a reformulation of the definition that defines h properly and is well defined ?","['differential', 'frechet-derivative', 'derivatives']"
4253674,Calculate $\int_{0}^{\infty}{\frac{\ln{x}}{x^{\frac{2}{3}}(1+x)}}\mathrm dx$,"I am having some difficulty with this exercise: Calculate $$\int_{0}^{\infty}{\frac{\ln{x}}{x^{\frac{2}{3}}(1+x)}}\mathrm dx$$ By Wolfram Alpha, I know that the Answer is $\frac{-2}{3}\pi^2$ . I tried to change variable $\arctan(\sqrt{x})=t$ cause I see (1+x), and it lead to $\int_{0}^{\pi/2}{\frac{\ln({\tan(t)})}{\tan(t)^{1/3}}}\mathrm dx$ , and got stuck. Can anyone help me solve this or give me some ideas.","['integration', 'definite-integrals']"
4253693,Probability of X lying within one standard deviation for different distributions?,"We know that for the normal distribution (denote this $P_N(x)$ ), the probability of a random variable having a value within one standard deviation of the mean is approximately $68$ %. That is $$\int_{\mu-\sigma}^{\mu+\sigma} P_N(x)\approx 0.68$$ This is true for all Normal distributions. Similarly, for all normal distributions, the ratio of the standard deviation to the mean absolute deviation is $$R\equiv \frac{\sigma}{MAD}=\sqrt{\frac{\pi}{2}}\approx1.25$$ I will refer to this quantity as the characteristic ratio of the distribution and denote it by $R$ . I have noticed that for any distribution I analyze, if the distributions characteristic ratio ( $R=\frac{\sigma}{MAD}$ ) is small, then the the probability contained within one standard deviation is small. For example, for the constant distribution $P_a(x)=1/a$ , we have that $$R=\frac{\sigma}{MAD}\approx 1.1547$$ This is less than the characteristic ratio of the normal distribution and so sure enough, if we calculate the probability contained within one standard deviation of the mean, we get a value less than that of a normal distribution (which is $0.68$ ) $$\int_{\mu-\sigma}^{\mu+\sigma} P_a(x)\approx 0.577$$ I have tried numerous distributions and always get a higher probability contained within one standard deviation given a higher characteristic ratio and vice versa. Is there a theorem or proof relating these 2 quantities for a distribution? That is, given the characteristic ratio of a distribution, is there a formula that relates the characteristic ratio to the the probability contained within one standard deviation? My second question is: Is there an intuitive way to interpret why a large characteristic ratio implies a large probability within one standard deviation?","['statistics', 'probability-distributions', 'probability']"
4253708,"Contangent space as a jet space, (inconsistency ?), Renteln","In Renteln's, Manifolds, Tensors and Forms , p. 81, The cotangent space as a jet space $^*$ , we have the following definitions Let $f:M \to \mathbb R$ be a smooth function, $p \in M$ , and $\{x^i\}$ local coordinates around $p$ . We say the $f$ vanishes to first order at $p$ if $\partial f/\partial x^i$ vanishes at $p$ for all $i$ . But there is no requirement on $f(p)$ to vanish. Moreover Inductively, for $k \ge 1$ we say that $f$ vanishes to $k$ th order at $p$ if, for every $i$ , $\partial f/\partial x^i$ vanishes to $(k-1)$ th order at $p$ , where $f$ vanishes to zeroth order at $p$ if $f(p) = 0$ . Put another way, $f$ vanishes to $k$ th order at $p$ if the ﬁrst $k$ terms in its Taylor expansion vanish at $p$ . I'm not sure whether he meant ""orders"" instead of ""terms"" in the last sentence, or whether the ""first $k$ "" counts from 0 or from 1, but he continues For $k > 0$ let $M_p^k$ denote the set of all smooth functions on $M$ vanishing to $(k − 1)$ th order at $p$ , and set $M_p^0 := \Omega^0(M)$ and $M_p := M_p^1$ . Each $M_p^k$ is a vector space under the usual pointwise operations, and we have the series of inclusions $$
M_p^0 \supset M_p^1 \supset M_p^2 \dots
$$ So $M_p = M_p^1$ is the set of functions vanishing to zeroth order at $p$ , i.e. all those with $f(p) = 0$ . $M_p^2$ is the set of those with $\partial f/\partial x^i = 0$ but not necessarily $f(p)  = 0$ . So the inclusion are not correct? Finally We now define $T_p^*M$ , the cotangent space to $M$ at $p$ , to be the quotient space $$
T_p^*M = M_p/M_p^2.
$$ Which is the set of equivalence classes, each of which consists of functions different from each other by a function vanishing to the 1st-order (with zeroth 1st partial derivative). Question: Is it needed or not that $f \in M_p^k, k \ge 1$ should have $f(p) = 0$ ? If not then I suppose it's possible to define the cotangent space simply as $$
T_p^*M = \Omega^0(M)/M_p^2.
$$","['jet-bundles', 'co-tangent-space', 'smooth-manifolds', 'differential-geometry']"
4253717,A Simpler Solution to an Optimization Problem.,"The Problem: My Solution: Let me model this situation in the first quadrant of the $x$ - $y$ plane. Let us take the line $y=mx+c$ as the ladder (moving in the first quadrant). This is how the ladder moving in the corridor looks: The $y$ -intercept of the line is $c$ and the $x$ -intercept is $\left(-\frac{c}{m}\right)$ . Since the ladder is moving in the first quadrant, we have $$
m<0 \text{ and } c>0
$$ Let $L$ be the length of the ladder. Then, $$
L^2=c^2+\left(\frac{c}{m}\right)^2
$$ or $$
c=-\frac{mL}{\sqrt{1+m^2}} \text{ (Negative sign because m<0)}
$$ and therefore, the equation of the ladder becomes $$
y=mx-\frac{mL}{\sqrt{1+m^2}}
$$ As the ladder is transported through the corridor, the distance between the ladder and the corner becomes smaller up to a minimum and then start increasing again. The goal here is to choose $L$ such that the ladder just touches the corner as it clears the corridor. This $L$ would be the max length of the ladder. Let the gap along the $y$ -direction between the corner and the ladder be $s$ . Let the co-ordinates of the corner be $(a,b)$ . Then, $$
s=b-ma+\frac{mL}{\sqrt{1+m^2}}
$$ For a given ladder, this distance hits  a minimum value for a certain value of $m$ . Let's find that out $$
\frac{ds}{dm}=-a+\frac{L}{\sqrt{1+m^2}}-\frac{Lm^2}{(1+m^2)^\frac{3}{2}}=0
$$ Solving which we obtain the value of $m$ which makes $s$ hit its minimum value $$
m=-\left\{\left(\frac{L}{a}\right)^\frac{2}{3}-1\right\}^\frac{1}{2} \text{ (Negative because m<0)}
$$ Therefore, $$
s_{\text{min}}=b+a\left\{\left(\frac{L}{a}\right)^\frac{2}{3}-1\right\}^\frac{1}{2}+L\left\{1-\left(\frac{a}{L}\right)^\frac{2}{3}\right\}
$$ The value of $s_{\text{min}}$ in the case of this problem is zero. Therefore, solving the above equation, we get $$
L=a\left\{\left(\frac{b}{a}\right)^\frac{2}{3}+1\right\}^\frac{3}{2}=\left(b^\frac{2}{3}+a^\frac{2}{3}\right)^\frac{3}{2}
$$ Plugging, $a=8$ and $b=6$ as asked in the question we get $$
L=19.7313 \approx 20 \text{ feet}
$$ Is this correct? If yes, can we make an easier model to solve this problem? (Only in the context of calculus)","['optimization', 'derivatives']"
4253724,Proof verification of function that satisfies $f(xy)=f(x)+f(y)$,"A function $f:(0,\infty)\mapsto R$ satisfies the condition $f(xy)=f(x)+f(y) $ for all $x>0,y>0$ . If $f$ is differentiable at $1$ , prove that $f$ is differentiable at every $c\in (0,\infty)$ and $f'(c)=f'(1)/c$ My attempt: Consider, $\lim_{x \to c}\{\frac{f(x)-f(c)}{x-c}\}$ $=$ $\lim_{x \to c}\{\frac{2f(x)-f(xc)}{x-c}\}$ Now take $x=yc$ , then, $\lim_{y \to 1}\{\frac{2f(yc)-f(yc^2)}{(y-1)c}\}$ (*) Now $2f(yc)=2f(y)+2f(c)$ and $f(yc^2)=f(y)+f(c^2)$ Then (*) becomes, $\lim_{y \to 1}\{\frac{f(y)+2f(c)-f(c^2)}{(y-1)c}\}$ Now, $f(c^2)=2f(c)$ by definition This implies, $\lim_{y \to 1}\{\frac{f(y)}{(y-1)c}\}$ $=$ $f'(1)/c$ (since $f'(1)= \lim_{y \to 1}\{\frac{f(y)}{y-1}\}$ , as taking y=1 we get $f(x)=f(1)+f(x)$ so, $f(1)=0$ Is this correct? Thanks in advance!","['continuity', 'solution-verification', 'derivatives', 'real-analysis']"
4253770,"Solving a PDE by ""Conjugating by diffeomorphism?"" Why should this even work?","Consider a real analytic manifold $(M,g_1,\mathfrak{A})$ with metric $g_1$ and real analytic foliation, $\mathfrak{A}.$ And consider $(N,g_2,\mathfrak{B})$ with analytic diffeomorphism $f:M \to N,$ for $\dim(M)=\dim(N).$ Take the foliation $\mathfrak{B}$ and $g_1$ and form the object $(O,g_1,\mathfrak{B}),$ where $O$ is a properly embedded submanifold in $M.$ Due to the embedding, $O$ inherits the induced metric $g_1.$ An example I think could work is: $M:=\Bbb R^2\setminus\{0\}$ with $g_1$ the usual Euclidean metric, and $\mathfrak{A}$ the foliation by rectangular hyperbolae, with $f$ the $\exp$ mapping. You can push the metric from $M$ onto $N:=\Bbb R^2_+/\lbrace1\rbrace$ where you have $ds^2=dx^2+dy^2$ becoming $ds^2=\frac{du^2}{u^2}+\frac{dv^2}{v^2}.$ Then you can take the foliation on $N$ and embed it properly back into $M$ in a way that you preserve the foliation from $N$ and you preserve the metric from $M$ and package them both into one manifold. The significance of this process is that it yields a solution to the heat equation (for the example I sketched) with $\mathfrak B$ serving as the particular solution. This is surprising because it's not immediately apparent why the foliation on $N$ should solve the heat equation when $g_2 \mapsto g_1$ is applied. Note that $\mathfrak{A}$ does not solve the heat equation. In the commments it is argued that $O$ and $\mathfrak B$ are not related. Then my question becomes: If $O$ and $\mathfrak B$ are unrelated, then why does $\mathfrak B$ solve an important partial differential equation written in terms of the metric $g_1?$ Doesn't this imply that they are related since $\mathfrak B$ carries information about the solution to a differential equation on $O?$ Let's be a little more precise. We have the heat equation (with some additional diffusivity parameters): $$ x u_{xx}= \mp t u_t $$ where our foliational solution is: $$\mathfrak B=\big \lbrace e^{\pm \frac{x}{\log t}}: x>0, t\ne 1\big \rbrace $$ Is there a concrete reason why this process (for my sketched example) works to solve this heat equation?","['analytic-geometry', 'diffeomorphism', 'foliations', 'partial-differential-equations', 'differential-geometry']"
4253798,Prove that $f(n)=n^2$ where $f$ is a strictly increasing multiplicative function with $f(2)=4$.,"Let $f:\mathbb N\to\mathbb N$ be a strictly increasing function with $f(2)=4$ which is completely multiplicative i.e $f(ab)=f(a)f(b)$ for all $a,b\in\mathbb N$ . Prove that $f(n)=n^2$ for all $n\in\mathbb N$ . This is an exercise on induction. So I am looking for an inductive solution. Here is my progress: It is easy to see that $f(1)=1$ . For the base case, we need to find $f(3)$ which I had some difficulties to find. Now $$ f(3^2) > f(2^3) = 64 \implies f(3)>8\\ f(3^8)<f(2^{13})=67108864\implies f(3)<10
$$ So $8<f(3)<10$ or $f(3)=9$ . Now I can show that $f(2^k)=4^k$ for all $k\in\mathbb N$ . Then I tried to prove $f(n+1)=(n+1)^2$ assuming $f(i)=i^2$ for all $i\leq n$ . But this doesn't work. So how do I solve the problem?","['functional-equations', 'proof-writing', 'functions', 'multiplicative-function', 'induction']"
4253802,Evaluation of ${\sum\limits_{\Bbb N} (\text {ker}(x)+i\text{kei(x)})=\sum\limits_1^\infty \text K_0\left(\sqrt ix\right)= 0.133691… - 0.7256312… i}$?,"$\large \text{Introduction:}$ This summation is related to this other Bessel Summation question: $$\mathrm{\sum\limits_{-\infty}^\infty Ai(x)=1}\ \text{and}\ \sum\limits_{-\infty}^0 \mathrm{Bi(x)}$$ $\large \text{Goal Sum:}$ The actual question uses the Modified Bessel Function of the Second Kind and corresponding Kelvin functions The sum may have a slightly different value. It would probably be easiest to use integral representations as alternative forms of the functions may not work well. I am looking for 2 solutions: one closed form for the kei(x) sum and one closed form for the ker(x) sum. This is because all other summations of just a function ended up with a closed form. Also try contour representations of which I am unfamiliar: $${\sum_{\Bbb N} \text{ker}(x)+i\text{kei}(x)= \sum_1^\infty \text K_0\left(\sqrt ix\right)= \sum_{x=1}^\infty \int_0^\infty \frac{\cos\left(\sqrt{i}tx\right)}{\sqrt{t^2+1}}dt=\sum_{x=1}^\infty\int_0^\infty \cos\left(\sqrt ix\sinh(t)\right)dt=0.133691752819604391549325780771600891… - 0.725631207729182631737443031218031025… i}$$ Here is a summand plot. Note the sum starts for $x\ge 1$ : $\large \text{Abel-Plana Integral Representation:}$ Another method is to use the Abel-Plana formula which does work as shown here with the difference term evaluated . The simpler integral over the summand only has a complicated closed form . Here the integral of the summand in the last step can be found by evaluating the closed form antiderivative from $[-1,\infty]-[-1,1]$ : $${\sum_0^\infty \text{ker}(x+1)+i\ \text{kei}(x+1) =\frac{\text{ker}(1)+i \ \text{kei}(1)}{2}+\int_0^\infty \text {ker}(x+1)+i\ \text{kei}(x+1) dx+\int_0^\infty \frac{i\ \text{ker}(1-ix)-\ \text{kei}(1-ix)-i\ \text{ker}(1+ix)+\ \text{kei}(1+ix)}{e^{2\pi x}-1}dx=\frac12 \text K_0\left(\sqrt i\right)+\int_0^\infty \text K_0\left(\sqrt i(x+1) \right)dx +i\int_0^\infty \frac{\text K_0\left(\sqrt i(1-ix) \right)-\text K_0\left(\sqrt i(1+ix) \right)}{e^{2\pi x}-1}dx}$$ $\large \text{Other Integral Representations:}$ Using @Jack Barber’s Floor function integral solution in the following question gives this result . Please see the bolded “Kelvin functions” link for more Generalized Kelvin function information: Evaluation of $$\sum_{x=0}^\infty \text{erfc}(x)$$ $$\sum_\Bbb N(\text{ker(x)}+ i\text{kei}(x))=\sum_\Bbb N \text K_0\left(\sqrt i x\right) =\sqrt[4]{-1}\int_0^\infty\lfloor x\rfloor \text{kei}_1(x)dx+\sqrt[-4]{-1}\int_1^\infty \lfloor x\rfloor \text{ker}_1(x)dx=(-1)^{-\frac34}\int_1^\infty \lfloor x\rfloor \text K_1\left(\sqrt[4]{-1} x\right)dx$$ Here is the integrand plot: Let’s now use the Fractional Part/Sawtoothwave function . Note that the point discontinuities can be ignored as a result of the integral operator: $$\text{frac}(x)=\mod{(x,1)}=\text{sawtoothwave}(x)=\boxed{\{x\}}=x-\lfloor x\rfloor\implies x-\{x\}=\lfloor x\rfloor$$ Therefore our goal sum can be expressed as the following. This is the closed form integral of $x\text{kei}_1(x),x\text{ker}_1(x)$ . The integral on $[1,\infty]$ is the same as on $[0,\infty]-[0,1]$ , but the integral of the $x\text{ker}_1(x)$ function integral on $[0,\infty]$ is just $-\frac\pi 2$ while for $x\text{ker}_1(x)$ , it is just $0$ on the same interval meaning that: $$\sum_1^\infty (\text{ker}(x)+i\text{kei}(x))= \sqrt[4]{-1}\int_0^\infty (x-\{x\}) \text{kei}_1(x)dx+\sqrt[-4]{-1}\int_1^\infty (x-\{x\}) \text{ker}_1(x)dx = (-1)^{-\frac34}\left(\frac\pi 2+\int_0^1 x \text{kei}_1(x)dx+\int_1^\infty\{x\} \text{kei}_1(x)dx \right)+  (-1)^{\frac 34}\left(\int_0^1 x \text{ker}_1(x)dx+\int_1^\infty\{x\} \text{ker}_1(x)dx \right) $$ $$\sum\limits_1^\infty \text K_0\left(\sqrt ix\right) = (-1)^{-\frac34}\int_1^\infty (x-\{x\})\text K_1\left(-\sqrt[4]{-1} x\right)dx =\sqrt[4]{-1}\left(\frac{i\pi}2+\int_0^1x \text K_1\left(\sqrt[4]{-1} x\right)+\int_1^\infty\{x\}\text K_1\left(\sqrt[4]{-1} x\right)dx\right)$$ There are many alternate forms for the floor function, so many more integral representations are possible in terms of related functions. Here is a plot of that Fractional Part function times the Bessel-Kelvin type function integrand: $\large \text{Conclusion:}$ Note that I could have made a typo. An exact form answer is needed and a closed form is optional. This question was found as the summand graph decreases rapidly and after a lot of terms , and converges quickly to over $50$ digits in just the first $300$ terms . We now have $2$ working integral representations of the constant, so how can we evaluate them or the sum representations? Please correct me and give me feedback! Note that $\sqrt[-2]2=\frac1{\sqrt 2} $ is easier for MathJax $From @Yuri’s help, here is an integral representation for: $$\sum_\Bbb N \text{ker}(x)=\int_0^\infty \frac{e^{\cosh(x)\sqrt[-2]2} \cos(\cosh(x) \sqrt[-2]2)}{\big(e^{\cosh(x) \sqrt[-2]2} \cos(\cosh(x) \sqrt[-2]2) - 1\big)^2 + e^{\sqrt2\cosh(x)} \sin^2(\cosh(x) \sqrt[-2]2)} - \frac 1{\big(e^{\cosh(x) \sqrt[-2]2} \cos(\cosh(x) \sqrt[-2]2) - 1\big)^2 + e^{\sqrt2 \cosh(x)} \sin^2(\cosh(x) \sqrt[-2]2)}dx $$ $$\sum_\Bbb N \text{kei}(x)=-\int_0^\infty \frac{e^{\cosh(x)\sqrt[-2]2} \sin(\cosh(x) \sqrt[-2]2)}{\big(e^{\cosh(x) \sqrt[-2]2} \cos(\cosh(x) \sqrt[-2]2) - 1\big)^2 + e^{\sqrt2 \cosh(x)} \sin^2(\cosh(x) \sqrt[-2]2)} dx$$","['special-functions', 'trigonometric-integrals', 'sequences-and-series', 'complex-numbers', 'bessel-functions']"
4253808,Apostol's Calculus II vs. Mathematical Analysis books,"I'm a second year math undergraduate and I'm looking for a book for the three analysis courses I'm taking this year: Differentiation of Multivariable functions, Integration of Multivariable functions and Power Series and Lebesgue Integral. I have read almost all Spivak's Calculus and some of Apostol's Calculus book and done plenty of the exercises in both books as last year I took a Calculus course that covered most of the topics of single-variable calculus. I've also studied the basics of metric spaces (using Kaplansky's Set Theory and Metric Spaces as a reference) and some topology using Mendelson's introduction to topology (I'll be taking this semester a course on General Topology too). With this background, which book should I get? I was thinking as the two main options either Calculus II (Apostol) with a supplementary book on Lebesgue integral or Mathematical Analysis (Apostol). Is there any other book suitable for my courses? Are there any Dover books (or similar) related that could be useful as a supplementary material (as I've already found for other topics) ? I appreciate suggestions and comments about the books I've already mentioned. Edit (i): I've also seen a bit about Spivak's Calculus in Manifolds. Can it be suitable for studying multivariable calculus?","['advice', 'book-recommendation', 'reference-request', 'real-analysis', 'multivariable-calculus']"
4253813,$\Bbb P( \lim_{n\to\infty} X_n=\beta )=1 \implies \lim_{n\to\infty}\Bbb P( X_n>\alpha )=1 \quad \forall \alpha < \beta$? (Proof verification),"I believe the following claim is true. I'd be grateful for a check of the proof I provide below. Claim: Let $(X_n)$ be a sequence of random variables on a shared probability space.
Then $$
\mathbb P(\lim_{n \to \infty} X_n = \beta) = 1 \implies \lim_{n \to \infty} \mathbb P(X_n > \alpha ) = 1 \quad \forall \alpha < \beta.
$$ Note: The interesting thing here is how we can move the limit. Proof: Let $\alpha < \beta$ . Then $$
\begin{align}
1 &= \mathbb P(\lim_{n \to \infty} X_n = \beta) \\
&\leq \mathbb P(\liminf_{n \to \infty} X_n = \beta) \\
&\leq \mathbb P( \liminf_{n \to \infty} X_n \geq \beta) = \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta) \\
&\leq 1.
\end{align}
$$ Thus, we have $1 = \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta)$ . Claim *: $\{\lim_{n \to \infty} \inf_{k \geq n} X_k \geq \beta\} \subset \lim_{n \to \infty} \underbrace{\{X_k > \beta - \varepsilon \quad \forall k \geq n \}}_{=: A_n^{\beta - \varepsilon}}$ for all $\varepsilon > 0$ . Proof of Claim *: Let $\varepsilon > 0$ . Let $\omega \in \{\lim_{n \to 
  \infty} \inf_{k \geq n} X_k \geq \beta\}$ , i.e. $\lim_{n \to
  \infty} \inf_{k \geq n} X_k(\omega) \geq \beta$ . Then $\exists N_\varepsilon \in \mathbb N : \forall n \geq
  N_\varepsilon : \inf_{k \geq n} X_k(\omega) > \beta - \varepsilon/2$ .
In particular, $X_k > \beta - \varepsilon$ for all $k \geq
  N_\varepsilon$ , so $\omega \in A_{N_\varepsilon}^{\beta -
  \varepsilon}$ . Since $(A_n^{\beta - \varepsilon})$ is in increasing in $n$ , we know $\omega \in \lim_{n \to \infty} A_n^{\beta - \varepsilon}$ . $\square$ We set $\varepsilon := \beta - \alpha > 0$ , so that $\beta - \varepsilon = \alpha$ . Then $$
\begin{align}
1 &= \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta) \\
&\leq \mathbb P( \lim_{n \to \infty} \underbrace{ \{ X_k > \alpha \quad \forall k \geq n \} }_{=: A_n^\alpha}) &&\text{(by }\textit{Claim*}\text{ with } \varepsilon = \beta - \alpha >0 \text{)}\\
&= \lim_{n \to \infty} \mathbb P(X_k > \alpha \quad \forall k \geq n) &&\text{(since } A_n^\alpha \uparrow \lim_{n \to \infty} A_n^\alpha \text{ and } \mathbb P \text{ monotonically cont.)}\\
&\leq \lim_{n \to \infty} \mathbb P(X_n > \alpha) \\
&\leq 1
\end{align}
$$ Therefore, our our chain of inequalities is, in fact, a chain of equalities. In particular, $\lim_{n \to \infty} \mathbb P(X_n > \alpha) = 1$ . $\blacksquare$","['limits', 'probability-limit-theorems', 'solution-verification', 'probability']"
4253822,Irreducible representations of a non-split group extension by an Abelian group,"Consider the group extension $$1\longrightarrow A\longrightarrow G\longrightarrow K\longrightarrow 1$$ where $A$ is a discrete abelian group and $K$ is a finite group. If the sequence splits, we can write $G$ as a semi-direct product $$ G=K\ltimes A.$$ In section 8.2 of Serre (Linear representations of finite groups), the irreducible representations of this group is classified. Essentially, given a character $\chi$ of $A$ we have the stabilizer subgroup under $G$ action $$ A_\chi = K_\chi\ltimes A=\{g\in G |\chi(gag^{-1})=\chi(a), \forall a\in A\},$$ where $K_\chi\subset K$ .
Irreducible representations $\chi$ of $A$ and $\sigma$ of $K_\chi$ can both be lifted to $K_\chi\ltimes A$ . We then have the following induced representations of $G$ $$ \Gamma_{\chi,\sigma} = \text{Ind}_{K_\chi\ltimes A}^G(\sigma\otimes\chi).$$ Proposition 25 says that all irreducible representations of $G$ are of this form and thus parametrized by $\chi$ and $\sigma$ . My question is, how does the above construction change if $G$ is a non-split extension of $K$ by $A$ ? EDIT: Just in case it simplifies things, I am mainly interested in unitary representations.","['group-extensions', 'group-theory', 'semidirect-product', 'representation-theory']"
4253853,Thinking of rings as objects that contain ideals instead of elements,"In this question I refer to a ""commutative ring with a 1, not equal to 0"" as a ring. I have seen it mentioned a couple of times that it is more useful to think of rings as objects that contain ideals rather thinking about the elements at all. For example in some answers on thie site, they will say ""here is an element-free approach"". Why is this a good idea and what are the motivations behind it? From a group theory point of view it would be weird to ignore the elements, it sounds like we are just losing information. As I understand it, rings are useful because we can study number theory with them, but how could we do that element-free. References are welcome. Thanks.","['ring-theory', 'abstract-algebra', 'soft-question', 'reference-request']"
4253864,Solve the ODE $y'' = (y')^2$,"I am asked in a past question paper to solve the following ODE: $$ y'' = (y')^{2}$$ To solve this, I began by equating $$y' = u$$ Differentiating both sides $w.r.t.x$ $$ \frac{d^{2}y}{dx^{2}} = \frac{du}{dx}$$ And therefore substituting back to out original equation I got: $$  \frac{du}{dx} = u^{2}$$ Rearranging $$\frac{du}{u^{2}} = dx$$ Integrating $w.r.t.x$ I got $$ \frac{-1}{u} = x+C$$ and substituting back $u$ $$ \frac{-dx}{dy} = x+C$$ $$ =x + \frac{dx}{dy} = C$$ $$=\frac{-dx}{x+c} = dy$$ $$ \log(\frac{1}{x+c}) = y$$ $$ -(x+c) = e^{y}$$ Is my answer and method correct? EDIT As pointed out by @MtGlasser We can re-write the D.E as $$ \frac{y''}{y'} = y'$$ Which is of the form : $$(ln(y'))' = y'$$ Integrating on both sides we get $$ ln (y') = y + c$$ And taking exponent on both sides and re-arranging we get $$ e^{-1}e^{-y} dy = dx$$ Integrating we get the asnwer $$ e^{-(y+1)} - C = x$$","['solution-verification', 'ordinary-differential-equations']"
4253910,"Let $M\in \text{SO}(3,\mathbb{R})$, prove that $\det(M-I_3)=0$.","Let $M\in \text{SO}(3,\mathbb{R})$ , prove that $\det(M-I_3)=0$ . My attempt: $$
\begin{align} 
\det(M-I_3)&=\det(M-M^TM)\\&=\det((I_3-M^T)M)\\&=\underbrace{\det(M)}_{=1}\det(I_3-M^T)
\end{align} 
$$ Hence $$
\begin{align} 
\det(M-I_3)&=\det(I_3-M^T)\\&=\det((I_3-M)^T)\\&=\det(I_3-M)\\&=\det(-(M-I_3))\\&=\underbrace{(-1)^3}_{=-1}\det(M-I_3),
\end{align} 
$$ and thus $\det(M-I_3)=0.$ Is this proof correct or did I miss out on something?","['matrices', 'orthogonal-matrices', 'linear-algebra']"
4253919,"Another Smooth Structure on $\mathbb R$, clarification needed, John M. Lee","In Introduction to Smooth Manifolds by John M. Lee, we have two statements, I hope I got the situation correct The first is on p. 17, Example 1.23 (Another Smooth Structure on $\mathbb R$ ). Consider the homeomorphism $\psi: \mathbb R \to \mathbb R$ given by $$
\psi(x) = x^3 \tag{1.1}
$$ The atlas consisting of the single chart $(\mathbb R,\psi)$ deﬁnes a smooth structure on $\mathbb R$ . This chart is not smoothly compatible with the standard smooth structure, because the transition map $\text{Id}_\mathbb R \circ \psi^{-1} (y)=  y^{1/3}$ is not smooth at the origin. Therefore, the smooth structure deﬁned on $\mathbb R$ by $\psi$ is not the same as the standard one. ...// While on page 40, In fact, as you will see later, there is only one smooth structure on $\mathbb R$ up to diffeo-
morphism (see Problem 15-13). More precisely, if $\mathcal A_1$ and $\mathcal A_2$ are any two smooth structures on $\mathbb R$ , there exists a diffeomorphism $F :(\mathbb R,\mathcal A_1)  \to (\mathbb R,\mathcal A_2)$ . In fact, it follows from work of James Munkres [Mun60] and Edwin Moise [Moi77] that every topological manifold of dimension less than or equal to 3 has a smooth structure that is unique up to diffeomorphism. ... So to make things clear, in our case, one has to find a diffeomorphism $F:(\mathbb R,\psi) \to (\mathbb R,\text{Id}_\mathbb R )$ . If I got it correctly this map should be $F(x) = x^3$ , since $\text{Id}_\mathbb R \circ F \circ \psi^{-1} = \text{Id}_\mathbb R$ , correct ? I'm confused about diffeomorphic smooth structures and the differentiabilities they define. So it seems that if there is a diffeomorphism between two smooth structures on the same manifold, it doesn't mean that every map or function that is smooth wrt one of these structures will be smooth wrt the other","['smooth-manifolds', 'differential-geometry']"
4253990,"x amount of people owned a goat, y amount of people owned a camel, z amount of people had one animal or the other but not both","In a survey, people were asked if they owned a goat or a camel. One person in fifteen said they had a goat. One person in eighteen said they had a camel and a tenth of the people had one animal or the other but not both. What proportion of the people owned neither kind of animal? Proportion of people who had a goat = 1/15 Proportion of people who had a camel = 1/18 Proportion of people who had one animal or the other = 1/10 Proportion of people who owned neither kind of animal = 1 - 1/15 - 1/18 - 1/10 = 7/9 I'm told however that the answer is actually, 8/9. Any ideas on where my understanding is breaking down?","['statistics', 'probability']"
4254002,Union of collection of non-trivial intervals of $\mathbb{R}$ can be written as the union of a countable subset of that collection,"I am thinking about the following statement: ""the union of each collection of nontrivial intervals of $\mathbb{R}$ is the
union of a countable subset of that collection"" and I came up with the following explanation: Let $\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ , where $\mathcal{A}$ is uncountable be such a union of nontrivial intervals of $\mathbb{R}$ . Remove from this union every interval that is a subset of the union of other intervals: in this way we get a union of nontrivial intervals $\bigcup_{\alpha\in\mathcal{A'}}I_{\alpha},\ \mathcal{A'}\subset\mathcal{A}$ such that $\bigcup_{\alpha\in\mathcal{A'}}I_{\alpha}=\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ , in each interval we can pick a unique rational number (since each $I_{\alpha}$ is not completely inside any union of other intervals) and from this it follows that $\mathcal{A'}$ is countable, as desired. Now, I know that another question about the same problem has already been asked , and since the proofs posted there by experienced users are much more complicated I guess there must be something wrong with my reasoning above, but since I don't see what that is and I find it to be an appealing argument I would like to see where it fails and/or/how it could be improved, and, if there are any, other proofs of this interesting statement, thanks. EDIT 09/19: It might be worthwhile to try by contradiction: Suppose there existed an uncountable collection of nontrivial intervals $(I_{\alpha})_{\alpha\in\mathcal{A}}$ such that for every countable subset $\mathcal{C}\subset \mathcal{A}$ we have that $\bigcup_{\alpha\in\mathcal{C}}I_{\alpha}\subsetneqq\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ . Then it follows that: $(I_{\alpha})_{\alpha\in\mathcal{A}}$ cannot contain $(-\infty,\infty)$ ; $(I_{\alpha})_{\alpha\in\mathcal{A}}$ cannot contain any countable number of intervals whose union is $(-\infty,\infty)$ ; the $(I_{\alpha})_{\alpha\in\mathcal{A}}$ cannot be all disjoint, since in that case it is possible to establish an injection to the rational numbers by picking a distinct rational number from each of them, showing that $\mathcal{A}$ is countable, a contradiction. ( EDIT 09/20 ) $\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ cannot be bounded, because from the fact that every non-trivial interval has positive outer measure and $\bigcup_{\alpha\in\mathcal{C}}I_{\alpha}\subsetneqq\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ for every $\mathcal{C}\subset\mathcal{A}$ countable follows that $|\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}|=\infty$ . So, now the task is to get from these two pieces of information (and perhaps others I haven't yet found out) a contradiction. Comments are welcome. NOTE: I have never studied topology, only self-studied calculus/linear algebra/real analysis (and currently measure theory) so I am interested in proofs that don't use too much topological machinery, if that is at all possible.","['alternative-proof', 'real-analysis']"
4254041,Measure preserving transformation and ergodic implies unbounded return time for some measurable set,"Suppose that $(X,\mu)$ is a non-atomic borel probability space and that $T:X \to X$ is measure preserving, ergodic and invertible.  Show that there is a measurable set $A$ with positive measure and unbounded return time. So far I showed that for each $n$ and $\epsilon>0$ there is a set $B$ such that $B$ , $T^{-1}(B)$ , ..., $T^{-(n-1)}(B)$ are disjoint and cover $X$ up to a set of measure less that $\epsilon$ by using the return times.  This would imply that there are sets with arbitrarily large return time, but I'm not sure how to use this to show that there is a set with unbounded return time.  Any help would be appreciated.","['ergodic-theory', 'real-analysis', 'complex-analysis', 'functional-analysis', 'dynamical-systems']"
4254060,"Quotient representation, in linear algebra and in topology","A complex vector space $\,{\mathbb{V}}\,$ can be split into classes so that two vectors belong to the same class if their difference lies in a subspace $\,{\mathbb{V}}_B\subset \mathbb V\,$ . By choosing a representative $\,v\,$ in a class, we can present the class as $\,v+{\mathbb{V}}_B\,$ , $\,v\in{\mathbb{V}}\,$ . These classes constitute a complex vector space named factor space and denoted with ${\mathbb{V}}/{\mathbb{V}}_B$ . If a representation $A(G)$ of a group $G$ is acting in $\,{\mathbb{V}}\,$ , and its subrepresentation $B(G)$ is acting in $\,{\mathbb{V}}_B\,$ , then in the factor space ${\mathbb{V}}/{\mathbb{V}}_B$ is acting a homomorphism $$
 A/B\,:\;\;G\,\longrightarrow\,GL({\mathbb{V}}/{\mathbb{V}}_B)
 $$ $$
 (A/B)(g)\,(v\,+\,{\mathbb{V}}_B)\;\equiv\;A(g)\,v\;+\;{\mathbb{V}}_B
 $$ called quotient representation . Owing to the invariance of ${\mathbb{V}}_B$ under $A(G)$ or, equivalently, under $B(G)$ , this definition is invariant under the choice of the representative $v$ . Now, suppose that $\,{\mathbb{V}}\,$ is a topological vector space (TVS) equipped with a topology $\cal T$ and written down as $\,({\mathbb{V}},\,\cal T)\,$ . In this case, ${\mathbb{V}}_B$ also will be a TVS, in the induced-topology sense. Also, suppose that ${\mathbb{V}}_B$ is closed in $\,({\mathbb{V}},\,\cal T)\,$ . This makes $B(G)$ a subrepresentation not only in the algebraic but also in the topological sense. QUESTION Will the factor space $A/B$ be a topological space? MORE SPECIFICALLY Is it pointless to ask if ${\mathbb{V}}/{\mathbb{V}}_B$ is topological in the sense of $\cal T\,$ ? $\,$ Will the topology $\cal T$ of ${\mathbb{V}}$ naturally induce a quotient topology ${\cal{T}}^{\prime}$ in the factor space ${\mathbb{V}}/{\mathbb{V}}_B$ , so that ${\mathbb{V}}/{\mathbb{V}}_B$ could become a TVS space $({\mathbb{V}}/{\mathbb{V}}_B\,,\;{\cal{T}}^{\prime})\,$ ?","['general-topology', 'representation-theory', 'quotient-spaces']"
4254067,Intuition behind this strange heuristic for primitive roots modulo $p$?,"Let $p$ be an odd prime. Define $S(p)$ as the sum of all primitive roots modulo $p$ taken from $\left[-\frac{p-1}2,\frac{p-1}2\right]$ . Now here's the strange thing. If the primitive roots were 'random', you'd expect $S(p)$ to be negative about as often as it is positive. However, experiments suggest that, at the very least, $$\lim_{n\to\infty}\frac{\#\{p\le n:S(p)<0\}}{\#\{p\le n:S(p)>0\}}>2.$$ This is extremely counter-intuitive to me. I don't expect one can compute the limit, or even prove it exists. But is there some intuition behind this phenomenon? EDIT: If $p\equiv 1\pmod 4$ , it is easy to see $S(p)=0$ . Now consider $p\equiv 3\pmod 4$ . We can go further and split into cases mod $8$ . I've computed $S(p)$ for all primes $p<10^5$ with $p\equiv 3\pmod 4$ . The value of $p$ modulo $8$ turns out to matter a whole lot. For $p\equiv 3\pmod 8$ , I found $$
\begin{align*}
\#\{p\le 10^5:p\equiv 3\pmod 8,S(p)>0\}&=204;\\
\#\{p\le 10^5:p\equiv 3\pmod 8,S(p)=0\}&=10;\\
\#\{p\le 10^5:p\equiv 3\pmod 8,S(p)<0\}&=2196.
\end{align*}
$$ Meanwhile, the behavior for $p\equiv 7\pmod 8$ is completely different: $$
\begin{align*}
\#\{p\le 10^5:p\equiv 7\pmod 8,S(p)>0\}&=1278;\\
\#\{p\le 10^5:p\equiv 7\pmod 8,S(p)=0\}&=39;\\
\#\{p\le 10^5:p\equiv 7\pmod 8,S(p)<0\}&=1083.
\end{align*}
$$ Refining even further: up to $10^5$ there are $1203$ primes $p$ satisfying $p\equiv 11\pmod {24}$ . With the sole exception of the prime $65171$ , these primes satisfy $S(p)<0$ .","['number-theory', 'elementary-number-theory', 'primitive-roots', 'reference-request', 'prime-numbers']"
4254117,Proving that the disjoint union of two affine schemes is isomorphic to an affine scheme,"An ""Easy Exercise"" from Vakil's text: Show that the disjoint union of a finite number of affine schemes is also an affine
scheme. To me it's not so easy. Let's do it for two affine schemes $(Spec A_i,\mathscr O_{Spec A_i})$ where $i=1,2$ . We need to prove that there is an isomorphism of two ringed spaces: $(Spec A_1\coprod Spec A_2, \mathscr O)\simeq (Spec_{A_1\times A_2},\mathscr O_{Spec(A_1\times A_2)})$ where $\mathscr O$ is the sheaf that is constructed from the following sheaf on a base $O$ by the procedure described in Vakil's Theorem 2.5.1: $O(D(f_i))=(A_i)_{f_i}$ . Suppose we know somehow that $Spec A_1\coprod Spec A_2$ and $Spec(A_1\times A_2)$ are homeomorphic via $$\pi: Spec A_1\coprod Spec A_2\to Spec(A_1\times A_2)\\P_1\mapsto P_1\times A_2 \text{ if $P_1$ is a prime ideal of $A_1$}\\P_2\mapsto A_1\times P_2\text{ if $P_2$ is a prime ideal of $A_2$}$$ Then the only thing that remains to be proved is that there is a natural isomorphism of functors $\mathscr O_{Spec A_1\times A_2} \simeq \pi^\ast\mathscr O$ , and this is where problems arise. We need to define a natural transformation $\alpha$ whose components are isomorphisms in the category of rings. Is there a way to reduce this to defining only $\alpha_{D(f)}$ (the components of $\alpha$ at distinguished open sets)? (If we invoke the construction of $\mathscr O$ from Theorem 2.5.1 via tuples of compatible germs, I feel this will become very messy, but it's supposed to be an ""easy exercise"".) Even if it's enough to define only $\alpha_{D(f)}$ , I'm having trouble doing this. Each $\alpha_{D(f)}$ should be a ring isomorphism $$\alpha_{D(f)}:(A_1\times A_2)_f\to \mathscr O(\pi^{-1}D(f))=O(\pi^{-1}(D(f)))$$ I'm not sure how to simplify $O(\pi^{-1}(D(f)))$ and how to define $\alpha_{D(f)}$ . And if it's not enough to define $\alpha_{D(f)}$ , how to define $\alpha_U$ in general?","['commutative-algebra', 'affine-schemes', 'algebraic-geometry', 'abstract-algebra', 'schemes']"
4254127,Find the sum of series $ \sum_{n=1}^{\infty}\frac{1}{n^3(n+1)^3}$,"Let it be known that $$\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}
{6}.$$ Given such—find $$\sum_{n=1}^{\infty}\frac{1}{n^3(n+1)^3}$$ Attempt : I have tried using the fact that $\displaystyle \frac{1}{n(n+1)}=\frac{1}{n}-\frac{1}{n+1}$ and then expanding or using known sum types as $\displaystyle \sum_{k=1}^{n} k=\frac{n(n+1)}{2}$ or $\displaystyle \sum_{k=1}^{n} k^3=\frac{n^2(n+1)^2}{4}$ but nothing seems to lead to anything!","['analysis', 'sequences-and-series']"
4254154,How do we calculate the derivative of $f(x) = x^{3} - 3x$?,"Let the function : $$
f(x) =x^{3}-3x
$$ with its domain in the real numbers. Determine with the help of $$
f'(x) \equiv \lim _{h\rightarrow 0}\dfrac{f\left( x+h\right) -f\left( x\right) }{h}
$$ the derivative $f'$ of function $f$ . I tried to plug the values of $f$ inside of $f'$ : $$
f (x ) =\lim _{h \to 0}\frac{(x+h)^{3}-3(x+h) -(x^{3}-3x) }{h}
$$ I then tried to factorize but it didn't yeld results. I don't understand how I can expand $(x + h)^3$ properly.","['calculus', 'derivatives', 'algebra-precalculus']"
4254177,"$P$ be a convex polygon in the plane with a prime number $p$ of sides, all angles equal, and all sides of rational length. Show that $P$ is regular.","Let $P$ be a convex polygon in the plane with a prime number $p$ of sides, all angles equal, and all sides of rational length. Show that $P$ is regular (i.e. all sides also have equal length). It's hard for me to connect the dots between rational side lengths, prime number of sides, and equal angles. Some hints would be greatly appreciated.",['geometry']
4254250,Connection between the category-theoretic limit and the limit of a sequence or function in analysis (calculus),"This may be a repeated question, but I want to make it clear. I am learning basic category theory by myself and came across the concepts of limits and colimits. I understand, for example, the colimit of a sequence of decreasing sets indexed by the category of natural numbers with orders as morphisms is their intersection. But since the word ""limit"" originally came from analysis with the classical epsilon-delta definition, I wonder what the connection between the two kinds of limit (category-theoretical limit and limit in analysis) is? Also, can we give a new definition of the limit of a sequence or function (epsilon-delta definition) USING the category-theoretical one? I would appreciate any help.","['category-theory', 'analysis', 'sequences-and-series']"
4254282,Why is $\binom{n}{k}$ for fixed $n$ not Gosper summable?,I am trying to understand the book A = B which has many much more complicated expressions which are Gosper-summable but then on p. 102 he states $\binom{n}{k}$ for fixed $n$ as a function of $k$ is not Gosper-summable. How can this be? Can someone show why it is not Gosper-summable?,"['summation', 'combinatorics', 'discrete-mathematics', 'sequences-and-series']"
4254285,Help in understanding a proof used in numerical linear algebra,"I am studying numerical linear algebra with the help of Trefethen and Bau's book. I have come across a proof I do not fully understand. I will attempt to bring the statement and proof as they appear in the book. I apologize in advance for the long post, but I wish to bring the reasoning as it appears in the book. Let $A$ be an $m \times m$ real, symmetric matrix. Suppose we have $n$ linearly independent vectors $v_1^{(0)},v_2^{(0)},\dots,v_n^{(0)}$ , and define $V^{(0)}$ to be the $m \times n$ matrix whose columns are $v_1^{(0)},v_2^{(0)},\dots,v_n^{(0)}$ in this order. Now we define the matrix $V^{(k)}=A^kV^{(0)}$ (the result after $k$ applications of $A$ ) and we extract a reduced QR factorization of this matrix $\hat{Q}^{(k)}\hat{R}^{(k)}=V^{(k)}$ where $\hat{Q}^{(k)}$ is an $m \times n$ matrix whose columns form an orthonormal basis for the column space of $V^{(k)}$ . We make the following assumption on the eigenvalues of $A$ : $|\lambda_1|>|\lambda_2|>\dots>|\lambda_n|>|\lambda_{n+1}| \geq |\lambda_{n+2}| \geq \dots \geq |\lambda_{m}|$ . Next, we define the matrix $Q$ whose columns are the normalized eigenvectors of $A$ in the order corresponding to the ordering of the eigenvalues in the assumption, and we take $\hat{Q}$ to be the $m \times n$ matrix whose columns are the first $n$ eigenvectors of $A$ , $q_1,\dots,q_n$ . We now note that $\hat{Q}$ and $\hat{Q}^{(k)}$ are entirely different matrices despite the similar notation. Now, we assume that all the leading principal minors of $\hat{Q}^TV^{(0)}$ are nonsingular. Now, we have the following theorem Assume the notation and assumptions used above. Then as $k \to \infty$ , the columns of $\hat{Q}^{(k)}$ converge linearly to the eigenvectors of $A$ , so that $$|q_j^{(k)}- \pm q_j| = O(C^k)$$ for each $j$ with $1 \leq j \leq  n$ where $C < 1$ is the constant $\max_{1 \leq k \leq n} |\lambda_{k+1}/|\lambda_k|$ . The $\pm$ sign means that at each step $k$ one or the other choice of sign is to be taken. Proof: Extend $\hat{Q}$ to a full $m \times m$ orthogonal matrix $Q$ of eigenvectors of $A$ (in the order matching the ordering of the eigenvalues above), and let $\Lambda$ be the corresponding diagonal matrix of eigenvalues so that $A=Q\Lambda Q^T$ . Just as $\hat{Q}$ is the leading $m \times n$ section of $Q$ , define $\tilde{\Lambda}$ (still diagonal) to be the leading $n \times n$ section of $\Lambda$ . Then we have $$V^{(k)}=A^kV^{(0)}=Q\Lambda^k Q^TV^{(0)}=\hat{Q}\hat{\Lambda}^k\hat{Q}^TV^{(0)}+O(|\lambda_{n+1}|^k)$$ as $k \to \infty$ . Due to the assumption on $\hat{Q}^TV^{(0)}$ , then this matrix itself is nonsingular in particular. Thus, we can multiply the term $O(|\lambda_{n+1}|^k)$ on the right by $\left(\hat{Q}^TV^{(0)}\right)^{-1}\hat{Q}^TV^{(0)}$ to transform our equation to $$V^{(k)}=(\hat{Q}\hat{\Lambda}^k+O(|\lambda_{n+1}|^k))Q^TV^{(0)}.$$ Since $\hat{Q}^TV^{(0)}$ is nonsingular, the column space of this matrix is the same as the column space of $$\hat{Q}\hat{\Lambda}^k+O(|\lambda_{n+1}|^k).$$ From the form of $\hat{Q}\hat{\Lambda}^k$ and the assumption on the ordering of the eigenvalues, it is clear that this column space converges linearly to that of $\hat{Q}$ . This convergence can be quantified, for example, by defining angles between subspaces; we omit the details. Now, in fact, we have assumed that not only is $\hat{Q}^TV^{(0)}$ nonsingular but so are all of its leading principal minors. It follows that the argument above also applies to leading subsets of the columns of $V^{(k)}$ and $\hat{Q}$ : the first column, the first and second columns, the first second and third columns, and so on. In each case, we conclude that the space spanned by indicated columns of $V^{(k)}$ converges linearly to the space spanned by the corresponding columns of $\hat{Q}$ . From the convergence of the successive column spaces, together with the definition of the $QR$ factorization, the convergence result follows. Here is where I get confused: How did the authors obtain $$V^{(k)}=A^kV^{(0)}=Q\Lambda^k Q^TV^{(0)}=\hat{Q}\hat{\Lambda}^k\hat{Q}^TV^{(0)}+O(|\lambda_{n+1}|^k)$$ , and what is the exact meaning of the big O notation here for matrices? I do not understand the statement in boldface. How does the form of $\hat{Q}\hat{\Lambda}^k$ and the assumption on the ordering of the eigenvalues lead to linear convergence? I would appreciate any and all help in understanding and clarifying these points. I really want to know how the authors conclude linear convergence in the statement in boldface. I thank all helpers.","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'numerical-linear-algebra', 'diagonalization']"
4254289,How can I derive $~\text{opposite}\cdot\sin^{}\left(\theta_{}\right)+\text{adjacent}\cdot\cos^{}\left(\theta_{}\right)=\tan^{}\left(\theta_{}\right)$,Given the below equation . $$  b \cos^{}\left(\theta_{} \right) = a \cdot \sin^{}\left(\theta_{} \right)  $$ I have to derive the below equation . $$  b \sin^{}\left(\theta_{} \right) + a \cdot \cos^{}\left(\theta_{} \right) = \sqrt{ a ^2 + b^2 }  $$ My tries are as below . $$  \frac{  b   }{  a  } = \frac{  \sin^{}\left(\theta_{} \right)   }{  \cos^{}\left(\theta_{} \right)   }  $$ $$  \frac{  b   }{  a  } = \tan^{}\left( \theta_{}  \right)   $$ $$  \text{adjacent}= a   $$ $$  \text{opposite}= b   $$ $$  \text{hypotenuse}=\sqrt{ a^2+ b ^2 }   $$ $$  b \sin^{}\left(\theta_{} \right) + a \cdot \cos^{}\left(\theta_{} \right)  $$ $$ = b \left( \frac{  b \cos^{}\left(\theta_{} \right)   }{  a  }  \right) +a \left( \frac{  a \sin^{}\left(\theta_{} \right)   }{  b   }  \right)  $$ $$ = \frac{  b^2\cos^{}\left(\theta_{} \right)   }{  a  } + \frac{  a ^2 \sin^{}\left(\theta_{} \right)   }{  b   }  $$ I've been got stucked from here .,"['trigonometry', 'systems-of-equations']"
