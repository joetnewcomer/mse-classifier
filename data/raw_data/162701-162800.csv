question_id,title,body,tags
2826438,Size of countable product $\sigma$-algebra of power set,"Let $\Omega = \{\; 0, 1 \;\}^{\mathbb{N}}$ be the set of all functions $\mathbb{N} \to \{\; 0, 1 \;\}$.
Then $\displaystyle\mathcal{F} = \bigotimes_{n \in \mathbb{N}} \mathcal{P}(\{\; 0, 1 \;\})$ is a $\sigma$-algebra on $\Omega$, where $\mathcal{P}(\dots)$ denotes the power set and $\bigotimes$ the product-$\sigma$-algebra. Is $\mathcal{F} = \mathcal{P}(\Omega)$?
This seems unlikely, but how would one construct a non $\mathcal{F}$-measurable set? As $(\{\; 0, 1 \;\}, \mathcal{P}(\{\; 0, 1 \;\}))$ is a polish space, one can prove that $\mathcal{F}$ is the Borel-$\sigma$-algebra generated by the product topology on $\Omega$.","['borel-sets', 'measure-theory']"
2826479,"Log function derivative, in terms of the function's output?","given a function $$f(x) = \frac{1}{1+e^{-x}}$$ we can express its derivative in terms of the function's output: $$\frac{df}{dx} = f(x) - f(x)\cdot f(x)$$ But is it possible to express the derivative of the following function in terms of its output? $$f(x) = \ln(x+1)$$ the derivative is: $$\frac{df}{dx} = \frac{1}{x+1}$$ But it's in terms of the argument of the function, not its output value :( This would mean I need to store input argument in my code, and unfavorable, as it will reduce performance.",['derivatives']
2826507,Optimal conditions for Schrödinger semigroup to be Hilbert-Schmidt,"Let $V:\mathbb R\to[0,\infty)$ be (for simplicity) a smooth function. Consider the Schrödinger operator
$$H:=-\Delta/2+V$$
acting on $L^2(\mathbb R)$. By the Feynman-Kac formula, the semigroup of this operator $T_t:=e^{-t H},~(t\geq0)$, is an integral operator with integral kernel
$$u_t(x,y):=\frac{e^{-(x-y)^2/2t}}{\sqrt{2\pi t}}\mathbb E\left[\exp\left(-\int_0^tV(x+W_s)~ds\right)\bigg|x+W_t=y\right],$$
where $W$ is a standard Brownian motion. Question. Are there known sufficient and necessary conditions on the potential $V$ for $T_t$ to be a Hilbert-Schmidt operator, that is
  $$\int_{\mathbb R\times\mathbb R}u_t(x,y)^2~dxdy=\int_{\mathbb R}u_{2t}(x,x)~dx<\infty?$$
  (Equivalently, we may ask for conditions for $T_{2t}$ to be trace-class.) If we use elementary computations on the maxima of Brownian bridges, it is not too difficult to find sufficient conditions in terms of $V(x)$'s growth as $x\to\infty$. However, I was not able to find any results which attempted to identify optimal conditions.","['functional-analysis', 'probability-theory', 'compact-operators', 'operator-theory']"
2826508,Sketch of Spivaks proof of the Inverse Function Theorem for Multivariable functions,"Below is the proof of the inverse function theorem for multivariable functions given in Spivaks ""Calculus on Manifolds"".  I have made a few interpretations regarding his reasoning and want to know if I have interpreted correctly. He spends the majority of the first part of the proof proving that $f$ is bijective, as this implies the existence of an inverse. The only time we use that $Df(a)=I$ is when we define $\mu=Df(a)$ and write $\mu^{-1}\big(f(x_1)-f(x)\big)=\mu^{-1}\big(\mu(x_1-x)+\phi(x_1-x)\big)$ $=$ $\mu^{-1}\big(\mu(x_1-x)\big)+\mu^{-1}\big(\phi(x_1-x)\big)$, since $\mu=I$ implies that $\mu^{-1}=\mu=I$ ,which gives that $\mu^{-1}$ is linear.  Besides that I don't see any other application of the property $Df(a)=I$ we established in the opening lines of the proof. $detf'(a)\neq 0$ is only used in the proof to guarantee that there exists a neighborhood around $f'(a)$ such that every element is non-zero; and then this implication is used to show that we must have $y^{i}-f^i(x)=0$, $\forall i$ Thanks in advance!","['real-analysis', 'proof-verification', 'calculus', 'multivariable-calculus', 'analysis']"
2826513,How many different operations can be defined in a finite groupoid with a given property?,"Set $B=\left\{ 1, 2, ... 18 \right\}$ is given. How many different operations $*$ can be defined so that $(B,*)$ is a groupoid with a property that $|\left\{i|i*(19-i) \neq i ∧ i*(19-i) \neq (19-i)\right\}| = k$, where $k \in B∪\left\{ 0 \right\}$. What exactly is $k$ here? The solution is given as:
$\binom{18}{k}(18-2)^k2^{18-k}18^{18^2-18}$. I understand that the part within the parentheses say that eg. $18*1 \neq 1 ∧ \neq 18$ which explains the $18^{18^2-18}$
as a diagonal which follows these rules does not have 18 possibilities anymore. But what does this $k$ mean? And how does it change the result?","['combinatorics', 'magma', 'elementary-set-theory']"
2826537,Proof every operator has an upper-triangular matrix,"I am having trouble understanding this proof that every operator has an upper-triangular matrix. $\lambda=$ is an eigenvalue of $T$, for $T \in L(V)$ where $V$ is a vector space on $F^n$, they say : suppose $U = \mathcal{R}(T-\lambda I)$, then $\dim(U)<\dim(V)$ because $U$ is not surjective. $Tu=(T-\lambda I)u + \lambda u$ shows $T$ is invariant under $U$, because both terms are in U. So above they are showing that since $T$ is an operator on $U$, $T$ has an upper triangular matrix with respect to some basis of $U$, $u_1,...u_m$ because in this is a claim they are trying to prove  by induction. Because $T$ has an upper triangular matrix, this means $Tu_j=(T|_u)(u_j)\in \operatorname{span}(u_1...u_j)$ I think I get all of this so far. Here is what I don't understand : Extend $u_1...u_m$ to a basis of $V$, $u_1,...u_m,v_1,...,v_n$. For each $k$, $Tv_k=(T-\lambda I)v_k + \lambda v_k$ $(T-\lambda I)v_k \in U = \operatorname{span}(u_1,...,u_m) => Tv_k \in \operatorname{span}(u1,...,u_m,v_1,...,v_k)$ I guess $(T-\lambda I)v_k \in U $ because it equals zero, and zero is in $U?$ So why does it mean $Tv_k \in \operatorname{span}(u1,...,u_m,v_1,...,v_k)?$ $\lambda v_k$ is an eigenvector/eigenvalue, but is it true that there is only 1 independent eigenvector per eigenvalue? This one eigenvalue works on $v_1...v_k?$",['linear-algebra']
2826554,Vandermonde matrices nullspaces and notation,"I'm following the book Mimetic Discretization Methods by Castillo and Miranda, reading the page 209-211, it states the following: Let $V(m;G)$ be a Vandermonde matrix , of order $m$ with generator $G = \{ g_1, g_2, \ldots, g_n \}$. Where the order implies the highest power of Vandermonde matrix, therefore $V(m;G)$ has $(m+1)$ rows and $n$ columns. Let 
\begin{align}
G_1 &= \{ -1/2, 1/2, 3/2, 5/2, 7/2, 9/2 \} \\
G_2 &= \{ -3/2, -1/2, 1/2, 3/2, 5/2, 7/2 \} \\
G_3 &= \{ -5/2, -3/2, -1/2, 1/2, 3/2, 5/2 \} \\
G_4 &= \{ -7/2, -5/2, -3/2, -1/2, 1/2, 3/2 \}
\end{align} Generators of the following matrices \begin{align}
V_1 &=V(4;G_1) \\
V_2 &=V(4;G_2) \\
V_3 &= V(4;G_3) \\
V_4 &= V(4;G_4)
\end{align} Just for sake of example $V_1$ looks like: \begin{align}
V_1 = \begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
-1/2 & 1/2 & 3/2 & 5/2 & 7/2 & 9/2 \\
(-1/2)^2 & (1/2)^2 & (3/2)^2 & (5/2)^2 & (7/2)^2 & (9/2)^2 \\
(-1/2)^3 & (1/2)^3 & (3/2)^3 & (5/2)^3 & (7/2)^3 & (9/2)^3 \\
(-1/2)^4 & (1/2)^4 & (3/2)^4 & (5/2)^4 & (7/2)^4 & (9/2)^4 \\
\end{bmatrix}
\end{align} The null space of this matrix is given by $Ker(V_1) =[-1 \; 5 \; -10\; 10\; -5\; 1]^T$, that is easily verifiable just by $V_1 \cdot Ker(V_1)$ that results in the null vector, as expected. The other Vandermonde matrices $V_2, V_3, V_4$ share the same null space. The book then states: In general, the null space of a $k$-th order Vandermonde matrix has dimension $\frac{1}{2}k -1$. My first question : There is no references or proofs to support this claim. Is this true? How can I prove that? What are conditions to that be true. I could not find anywhere this affirmation. The text continues, and we need to solve the following system \begin{equation}
V_i a_i^T = b
\end{equation} Where $V_i$ are the Vandermonde matrices above, $a_i$ is a unknown vector and $b = [ 0 \;1 \;0 \;0\; 0 ]$ Lets only focus in $i=1$ since the construction to other matrices are equivalent. We then need to solve the following: \begin{equation}
V_1 a_1^T = b
\end{equation} completely written as \begin{equation}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 & 1 \\
-1/2 & 1/2 & 3/2 & 5/2 & 7/2 & 9/2 \\
(-1/2)^2 & (1/2)^2 & (3/2)^2 & (5/2)^2 & (7/2)^2 & (9/2)^2 \\
(-1/2)^3 & (1/2)^3 & (3/2)^3 & (5/2)^3 & (7/2)^3 & (9/2)^3 \\
(-1/2)^4 & (1/2)^4 & (3/2)^4 & (5/2)^4 & (7/2)^4 & (9/2)^4 \\
\end{bmatrix}\begin{bmatrix}
a_{11} \\a_{12} \\a_{13} \\a_{14} \\a_{15} \\a_{16}
\end{bmatrix} = \begin{bmatrix}
0 \\ 1 \\ 0 \\ 0 \\ 0
\end{bmatrix}
\end{equation} Then the text declares that \begin{align}
a_1 = \left[ -\frac{11}{12} \, \frac{17}{24} \, \frac{3}{8} \, -\frac{5}{24} \,  \frac{1}{24} \, 0 \right]  + \alpha_1 Ker(V_1)
\end{align} for some $\alpha_1 \in \mathbb{R}$. My second question is: where that vector in first term came from? Is that true? How can I show that? I'm able to solve that system using least squares, but could not find a correlation. Some tries: (EDIT) Definition ( Null Space ): $Ker(V)$ is a null space of $V$ if an only if $V \cdot \alpha Ker(V) = \vec{0}$ for any $\alpha \in \mathbb{R}$. Lets say by hypothesys that \begin{align}
a &= x + \alpha Ker(V)
\end{align} Since we are looking for (I will drop the indexes for sake of notation and typing): \begin{align}
V \cdot a &= b \\
a &= V^{-1} b
\end{align}
by the hypothesis
\begin{align}
x + \alpha Ker(V) &= V^{-1} b \\
Vx + V \left( \alpha Ker(V) \right) &= b
\end{align}
by Null Space definition
\begin{align}
Vx &= b
\end{align} We are just back where we started, seems a tautology in my view. Anyway I also know that \begin{align}
a &= x + \alpha Ker(V) \\
x &= a - \alpha Ker(V)
\end{align} I can find $a$ by least squares, and have $Ker(V)$ by the null space analysis. Therefore  I just need to find $\alpha$ to uniquely identify $x$.","['numerical-linear-algebra', 'linear-algebra']"
2826565,Solution to $\frac{d^2x(t)}{dt^2}+\omega(t)^2x(t)=0$,"In physics, the simple harmonic oscillator with a time-dependent frequency $\omega(t)$ obeys the differential equation \begin{align}
\frac{d^2x(t)}{dt^2}+\omega(t)^2x(t)=0\ .
\end{align} If $\omega$ is a constant, then \begin{align}
x(t)=C_1e^{i\omega t}+C_2e^{-i\omega t}\ .
\end{align} How would I find a solution to the differential equation if $\omega(t)$ is an arbitrary time-dependent function? I tried looking in books on solutions to differential equations (such as Handbook of exact solutions for ordinary differential equations ). All solutions I found are for a specific form of $\omega(t)$, rather than an arbitrary $\omega(t)$. I also tried the substitution $x(t)=s(t)e^{i\gamma(t)}$. Assuming $s(t)$ and $\gamma(t)$ are both real functions, then subbing $x(t)$ into the above differential equation and splitting the result into its real and imaginary terms, I find the $s(t)$ and $\gamma(t)$ satisfy \begin{align}
\frac{d^2s(t)}{dt^2}-s(t)\left(\frac{d\gamma(t)}{dt}\right)^2+\omega(t)^2s(t)=0\ ,\\
s(t)\frac{d^2\gamma(t)}{dt^2}+2\frac{ds(t)}{dt}\frac{d\gamma(t)}{dt}=0\ ,
\end{align} but these equations are more complicated than my starting point.","['physics', 'ordinary-differential-equations']"
2826571,"An astounding identity: $\int_0^{\pi/2}\ln\lvert\sin(mx)\rvert\cdot \ln\lvert\sin(nx)\rvert\, dx$","In this question, user Franklin Pezzuti Dyer gives the following surprising integral evaluation: $$\int_0^{\pi/2}\ln \lvert\sin(mx)\rvert \cdot \ln \lvert\sin(nx)\rvert \, dx = \frac{\pi^3}{24} \frac{\gcd^2(m,n)}{mn}+\frac{\pi\ln^2(2)}{2}$$ I've verified this numerically for small values for $m,n$ .  Is there a proof?  Also, can we generalize it to more factors in the integrand?","['integration', 'definite-integrals']"
2826623,Some concrete examples of $M_q(2)$ points,"Given $q \in \mathbb{C}$ invertible, Kassel says that an $M_q(2)$ point of an $R$ algebra is a $m=\left(\begin{array}{cc}
A & B\\
C & D
\end{array}\right)\in R^{4}$ such that $A,\,B,\,C,\,D \in R$ satisfy the following relations $$CA=qAC,$$ $$ DB=qBD, $$ $$BA=qAB,$$ $$DC=qCD,$$ $$BC=CB,$$ $$DA-qCB=AD-\left(q^{-1}\right)BC.$$ Can anybody give me a concrete example of $M_3(\mathbb{C})$ matrices that form such a point? If it's not possible in $M_3(\mathbb{C})$ , every other concrete example is well accepted.
Thanks in advance","['quantum-groups', 'matrices', 'abstract-algebra', 'representation-theory', 'hopf-algebras']"
2826636,Convergence of a series to an integral,"Suppose we have a real function $f$ bounded continuous on $[0,1]$. We know that $$ \frac 1 n \sum_{i=1}^n f(x_i) \to \int_0^1 f(x) \,dx$$ for $x_i \in [(i-1)/n, i/n]$, as $n\to \infty$. Now suppose we have a sequence $f_n$ of bounded continuous functions on $[0,1]$ converging pointwise to $f$. Is it true that $$ \frac 1 n \sum_{i=1}^n f_n(x_i) \to \int_0^1 f(x) \,dx$$","['real-analysis', 'sequences-and-series', 'functional-analysis', 'convergence-divergence', 'analysis']"
2826706,Equivalence between non-singular vector field without periodic orbits and the irrational flow on a torus.,"I'm reading the book ""The dynamics of vector Fields in dimension 3 - Matthias Moreno and Siddhartha Bhattacharya "" On page 6 the authors enunciate the following theorem: Theorem:(Poincaré, Denjoy:) Every non-singular $\mathcal{C}^2$ vector field on a compact surface
  that has no periodic orbits is topologically equivalent to a linear
  flow on a torus with the irrational slope. An interesting comment is that this result is not true for $\mathcal{C}^1$ vector
fields. I searched for this result on the Internet but I didn't find any book/paper that demonstrates the above theorem. Does anyone know how to prove this or can give me a reference where I can learn the proof of the theorem? N.B. I only need to know how to demonstrate the theorem when the vector field is on $\mathbb{T}^2$.","['ergodic-theory', 'ordinary-differential-equations', 'dynamical-systems', 'lie-groups']"
2826724,Moment map for Hamiltonian action $\Bbb S^1 \circlearrowright \Bbb C^n$.,"Consider $\Bbb C^n$ with the usual $\newcommand{\d}{{\rm d}}$symplectic form $$\omega =\frac{i}{2}\sum_{k=1}^n \d z^k \wedge \d \overline{z}^k$$and the action $\Bbb S^1 \circlearrowright \Bbb C^n$ given by $$e^{i\theta} \cdot (z^1,\ldots, z^n) \doteq (e^{i\theta}z^1,\ldots, e^{i\theta}z^n).$$It is easy to see that this action is symplectic. In my lecture notes it is stated that the infinitesimal action is given by $1\mapsto X_{(z^1,\ldots, z^n)} = (iz^1,\ldots, iz^n)$, and that the action is Hamiltonian with moment map $\mu(z) = \frac{1}{2}\sum_{k=1}^n |z^k|^2$. I'd like to check these assertions, using the bare minimum of identifications possible. The Lie algebra of $\Bbb S^1$ is the vertical line tangent to the circle at $1$, so it is $i\Bbb R \cong \Bbb R$. With this, given $a \in \Bbb R$, we consider the curve $\alpha\colon \Bbb R \to \Bbb S^1$ given by $\alpha(t) = e^{iat}$. Since $\alpha(0) = 1$ and $\alpha'(0) = ia$, we can use that to compute the infinitesimal generator associated to $a$ as $$(a^\#)_{(z^1,\ldots, z^n)} = \frac{\d}{\d{t}}\bigg|_{t=0} e^{iat} \cdot(z^1,\ldots, z^n) = (iaz^1,\ldots, iaz^n),$$good. Since the action is symplectic, the vector field $a^\#$ is symplectic, right?  But $$a^\# = ia\sum_{k=1}^n z^k \frac{\partial}{\partial z^k} \implies \iota_{a^\#}\omega = -\frac{a}{2}\sum_{k=1}^n z^k \d \overline{z}^k.$$The form $\iota_{a^\#}\omega$ is not even closed. What is going on? I was planning to find a primitive, then finding the comoment map, and finally computing the moment map from the comoment map.","['complex-geometry', 'group-actions', 'symplectic-geometry', 'differential-geometry', 'lie-groups']"
2826748,Chain rule for higher Fréchet derivatives?,"I'm having trouble with the proof of the following fact. Let $E,F,G$ be Banach spaces. Suppose $X$ is open in $E$ and $Y$ is open in $F$. Given functions $f\in C^m(X,F)$, $g\in C^m(Y,G)$ such that $f(X)\subseteq Y$. Then $g\circ f\in C^m(X,G)$. Here $C^m$ means $m$-times Fréchet differentiable. We write $\mathcal{L}(E,F)$ for the space of bounded linear operators from $E$ to $F$. My book (Amann & Escher, Analysis II , p.185) says the proof is induction on $m$ and left as an exercise. While I understand that $\partial(g\circ f)(x_0)=\partial g(f(x_0))\partial f(x_0)$, I got stuck for the case $m\geq2$. Here are my thoughts. So $\partial(g\circ f):X\to\mathcal{L}(E,G)$ is the composition of $(\partial g)\circ f$ and $\partial f$, where $X\xrightarrow{f}F\xrightarrow{\partial g}\mathcal{L}(F,G)$ and $X\xrightarrow{\partial f}\mathcal{L}(E,F)$. However, this does not mean $\partial(g\circ f)=(\partial g)\circ f\circ\partial f$, because the composition of $(\partial g)\circ f$ and $\partial f$ is actually the composition of the values of these functions. So we shall write $\partial(g\circ f)=\big((\partial g)\circ f\big)(\partial f)$. We can decompose the map like this:\begin{matrix}X&\xrightarrow{\varphi}&\mathcal{L}(E,F)\times\mathcal{L}(F,G)&\xrightarrow{\psi}&\mathcal{L}(E,G)\\x_0&\mapsto&(\partial f(x_0),\partial g(f(x_0)))&\mapsto&\partial g(f(x_0))\partial f(x_0)\end{matrix}
By assumption the map $x_0\mapsto\partial f(x_0)$ belongs to $C^{m-1}(X,\mathcal{L}(E,F))$. By induction the map $x_0\mapsto\partial g(f(x_0))$, being a composition of $\partial g$ and $f$, both of which are $C^{m-1}$, belongs to $C^{m-1}(X,\mathcal{L}(F,G))$. Hence $\varphi\in C^{m-1}(X,\mathcal{L}(E,F)\times\mathcal{L}(F,G))$. It suffices to show that $\psi\in C^{m-1}(\mathcal{L}(E,F)\times\mathcal{L}(F,G),\mathcal{L}(F,G))$, and then induction completes the proof. But how do I show that $\psi\in C^{m-1}(\mathcal{L}(E,F)\times\mathcal{L}(F,G),\mathcal{L}(F,G))$? And are there other proofs?","['multivariable-calculus', 'chain-rule', 'analysis', 'frechet-derivative']"
2826763,Are all the uncountable infinite always greater than countable infinite?,"I read on an article about there are more reals than rationals because reals are uncountable. Here are my questions: Are there more complex numbers than real numbers?
Are all the uncountable infinite always greater than countable infinite? Thanks in advance.","['infinity', 'elementary-set-theory']"
2826780,"$A \subseteq \mathbb{R}^{2}$ is not homeomorphic to an open subset of $\mathbb{R^{n}}$, $n\geq 3$","An open subset $A \subseteq \mathbb{R}^{2}$ is not homeomorphic to an open subset of $\mathbb{R^{n}}$ for $n\geq 3$. This question is from my General Topology class. We have seen up to part of algebraic topology. My attempt: Suppose a homeomorphism $f \colon X \to A$, where $X\subseteq \mathbb{R}^{n}$ is an open set. Then we can restrict this homeomorphism to an open ball contained in $X$ and, using some more homeomorphisms, suppose $X = \mathbb{R}^{n}$. Therefore we can also suppose $A \subseteq \mathbb{R}^{2}$ is an open simply connected set. Now, because of $\mathbb{R}^{n}-\{0\}$ is simply connected whenever $n\geq 3$, $f(\mathbb{R}^{n}-\{0\}) = A - \{f(0)\}$ is simply connected. We want to deny that $A - \{f(0)\}$ is simply connected. In order to do that pick a $\delta > 0$ such that $B_{f(0)}[\delta] \subset A$. Then define the following function: $$ \alpha \colon \mathbb{S}^{1} \to A - \{f(0)\}; ~ \alpha(z) = z\delta + f(0)$$ A set  $X \subseteq \mathbb{R}^{n}$ is simply connected when every continuous function $p \colon \mathbb{S}^{1}\to X$ has a continuous extension $\overline{p}\colon \mathbb{B}^{2} \to X$. I was not able to show that this function has not continuous extension to $\mathbb{B}^{2}$. Is it true? Any help would be appreciated but I would prefer help in my attempt. Thank you.","['algebraic-topology', 'general-topology', 'analysis']"
2826782,"Expected value of the shifted inverse of a binomial random variable, and application","Here is an exercise given by a colleague to a student : Let $X\hookrightarrow B(n,p)$ and $Y=\frac{1}{X+1}$ . Find ${\rm E}(Y)$ . It is not very difficult to prove that the answer is $${\rm E}(Y) = \frac{1-q^{n+1}}{p(n+1)}$$ where $q=1-p$ . But the answer can also be written $${\rm E}(Y) = \frac{1+q+q^2+\dots+q^n}{n+1}$$ First question: Is there any meaning to this form, which looks very much like a mean value of some sort? Or maybe another proof of this result which explains it in a more direct way? Second question : Is there some context which could make this exercise more ""concrete""?","['uniform-distribution', 'probability-theory', 'binomial-distribution', 'random-variables']"
2826808,What is the definition of local and global properties and how are they different?,"In brief, I am asking for two things: A rigorous definition of what ""global property"" means. More information on how that differs from (is a larger category than) a local property that is true everywhere within a space. As part of this question: is a local property that holds everywhere a global property of the space? If so, are there local properties that are not also global? Here is my thinking/ motivation: definitions of local properties abound online, such as in the answer to this question asking that very thing , which says Not only should the property be true for a neighborhood of each point.
  It must also be the case that having a neighborhood with the given property around each point implies that the entire space satisfies the property (satisfying a property locally is not the same as the property being local). That definition makes me think a local property must hold true everywhere within the space, and therefore also be a global property. This thought process is the origin of my second question, in which I'm confused about the difference in practice between local and global properties. With respect to my first question, I've spent about two hours searching online and in textbooks (Kasriel's Undergraduate Topology and Kreyszig's Differential Geometry) without finding any definition of ""global property"". Although it's intuitively pretty clear what the term means, my confusion in my second question suggests my intuition is not complete and I suspect having a rigorous definition of ""global property"" would help.",['general-topology']
2826813,"Norm of $(0,2)$ tensor","If we have a $(0,2)$ tensor $A$ (in my situation, it is actually second fundamental form), I am confused by this notation $|A|$.If we think it as Frobenius norm, write $A$ in local coordinate,i.e., $A=A_{ij}dx^i\otimes dx^j$ where $A_{ij}=A(\partial_i,\partial_j)$, then $$|A|=(\sum A^2_{ij})^{1/2}.$$
But if we think it is a norm induced by metric, then
$$|A|=<A,A>^{1/2}=<A_{ij}dx^i\otimes dx^j,A_{mn}dx^m\otimes dx^n>^{1/2}=(g^{im}g^{jn}A_{ij}A_{mn})^{1/2}.$$
It seems that if I choose local orthonormal frame, they are same since $g^{im}=\delta^i_m$. Or the truth is they are same all the time? Any explanation will be helpful.","['riemannian-geometry', 'differential-geometry']"
2826829,A discrete uncountable subset of $\ell^\infty$ in the weak topology,"Let $\ell^\infty$ denote the space of all bounded real sequences with the usual norm and let $A=\{0,1\}^\mathbb{N}$ denote the set of sequences taking values in $\{0,1\}$. It's easy to see that $A$ is closed and discrete in the norm topology of $\ell^\infty$. Does it remain closed or discrete in the weak topology of $\ell^\infty$ ? If this example does not work, can we construct another uncountable subset of $\ell^\infty$ which is discrete and closed in the weak topology? I have also a related question. In the space $L^\infty[0,1] $ of measurable essentially bounded functions, consider the subset $B$ of characteristic functions of intervals  $[0,t]$ for $t\in[0,1]$. Here also, we can show that $B$ is closed and discrete in the norm topology. Is it still discrete in the weak topology?","['functional-analysis', 'general-topology']"
2826843,Does each compact operator have a non–zero eigenvector? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have learned that 0 must be an eigehvalue for a compact operator. But I do not know if there exists a compact operator that have no non–zero eigenvector.
Any hint would be most appreciated.","['functional-analysis', 'compact-operators', 'spectral-theory', 'operator-theory']"
2826936,behavior of a meromorphic function around an essential singularity,"Suppose $f(z)$ is a meromorphic function in a punctured disc $\mathbb D^\times$ and $0$ is an essential singularity of $f(z)$. Is it possible that $f(z)$ takes all values in the extended complex plane $\hat{\mathbb C}$ in any neighborhood of $0$? That is, is it possible that $f(z)$ has a pole and also takes every finite complex value in arbitrarily small neighborhood of $0$?",['complex-analysis']
2826960,Determine the number of paths to move from the top-left cell to the bottom-right cell such that there are an even number of direction changes,"Determine the number of paths to move from the top-left cell to the bottom-right cell in the $8 \times 6$ (so the height is $6$ units and the length $8$ units) cell grid using a sequence of downwards moves and rightwards moves such that there are an even number of direction changes. I'm a little lost on this. I know that there are $(8+6)!/(8!\cdot 6!)$ ways to get from the top left corner to the bottom left corner, but otherwise I'm not sure what to do. Of course, I did try to ""get my hands dirty"" and worked out possible paths that have an even and an odd number of direction changes, but I'm not sure if they have helped me much so far.","['combinatorics', 'contest-math']"
2827013,Cartesian product of two unions of sets of sets $\subseteq$ union of products of sets,"I'm reading the book ""The Foundations of Mathematics” by Ian Stewart and David Tall, and I am puzzled about one of exercises: Chapter 4. Exercise 2. Given $S$ and $T$ are sets of sets, prove that 
  $$\left(\bigcup S\right) \times \left(\bigcup T\right) \subseteq \bigcup \{ X \times Y \mid X \in S, Y \in T \}$$ I can write the LHS as
$$\left(\bigcup S\right) \times \left(\bigcup T\right) = (S_1 \cup S_2 \cup S_3 \cup \ldots \cup S_n ) \times (T_1 \cup T_2 \cup T_3 \cup \ldots \cup T_n) $$ where each $S_i\in S$ and $T_i\in T$. Also, the RHS is $$ \bigcup \{ X \times Y \mid X \in S, Y \in T \} = (S_1 \times T_1) \cup (S_1 \times T_2) \cup \ldots \cup (S_1 \times T_n) \cup (S_2 \times T_1) \cup \ldots \cup (S_2 \times T_n) \cup \ldots \cup (S_n \times T_n) $$ If I understand correctly, it is a distributive property. Like in this answer . Then, in this case, it should have an $=$ sign, not $\subseteq$. 
But exercise also asks to show that there is no equality here. Update: Here is a quote from book: ""Show that in first formula we cannot replace $\subseteq$ by $=$.""","['products', 'elementary-set-theory']"
2827030,How to evaluate $\frac{d}{dx}\sin^2x$,I'm having trouble understanding how  $\frac{d}{dx}\sin^2x =2\sin(x)\cos(x)$. Please show as many steps of the proof as necessary so that I can apply this to other problems. Thank you for your time~! ^_^,"['algebra-precalculus', 'chain-rule', 'calculus']"
2827077,$\int_0^\infty \frac{\sqrt x}{1+x^4} dx$ by residues,"Evaluate $\int_0^\infty \frac{\sqrt x}{1+x^4} dx$ I think I'm on the right path, but I'm not getting the right answer (which is $\frac{\pi}{4 \cos(\frac{\pi}{8})}$). Here is what I have done: Define $f(z) = \frac{\sqrt z}{1+z^4}$ on the upper half circle $\alpha$. The singularities inside this half circle are $w_1 = e^{\frac{\pi i}{4}}$ and $w_2 = -e^{\frac{-\pi i}{4}} = - \bar{w_1}$. Then the integral can be calculated as follows: $$\oint_\alpha f(z) \,dz = 2\pi i (Res(f,w_1)+Res(f,w_2))$$ Calculating residues: $Res(f,w_1) = \frac{\sqrt w_1}{4w_1^3} = \frac{e^{\frac{- \pi i}{8}}}{4i}$ $Res(f,w_2) = \frac{\sqrt w_2}{4w_2^3} = \frac{e^{\frac{ \pi i}{8}}}{4}$ From this, I can already see that this won't give me the right answer, since the final answer does not contain $i$ and my $cos$ is in the numerator ($e^{\frac{- \pi i}{8}} + e^{\frac{ \pi i}{8}} = 2cos(\frac{\pi}{8}))$ Some help would be appreciated! May be I made some mistakes calculating the residues?",['complex-analysis']
2827130,Multivariable chain rule - notation?,"Assume we have four functions $\in C^\infty$ from $\mathbb R^2 $ to $\mathbb R$, written as follows:
$$
x=x(u,v) , \quad u=u(r,s)
$$
$$
y=y(u,v) , \quad v=v(r,s)
$$
then, assuming the composition $x(u(r,s),v(r,s))$ is properly defined,
$$
\frac{\partial x}{\partial r} = \frac{\partial x}{\partial u} \frac{\partial u}{\partial r} + \frac{\partial x}{\partial v} \frac{\partial v}{\partial r}.
$$ But, in the particular case where $(r,s)=(x,y)$, we have that $\frac{\partial x}{\partial x} =1 $, but the right hand side gives us
$$
 \frac{\partial x}{\partial u} \frac{\partial u}{\partial x} + \frac{\partial x}{\partial v} \frac{\partial v}{\partial x}
$$
which equals 2? What am I getting wrong here?
Does my misunderstanding follows from a notation error? Thanks","['multivariable-calculus', 'chain-rule']"
2827148,"Proof that every random variable has a median, proof check?","I already checked the question Does a median always exist? . But I am not convinced that the limits exist. Say I want to show $m:=\inf\{x\in\mathbb{R}\mid\mathbb{P}(X\leq x)\geq \frac{1}{2}\}$ . Then for $x_n$ decreasing sequence with $x_n \to m$ and $x_n>m$ , we have: $$\mathbb{P}(X\leq m)=\lim_{n\rightarrow \infty} \mathbb{P}(X\leq x_n)=F(m^+)\geq\frac{1}{2}.$$ Since $(-\infty,m]=\cap_n(-\infty,x_n)$ . Now for the other inequality:
For $x<m$ , $\mathbb{P}(X\leq x) < \frac{1}{2}$ since $m$ is the infimum. Now $x_n \to m$ increasing sequence and $\cup_n(-\infty,x_n]=(-\infty,m)$ since $m$ is in none of the sets $$\lim_{n\to< \infty}\mathbb{P}(X\leq x_n)=\mathbb{P}(X<m )<\frac{1}{2} \quad \text{or should it be $\leq$?}$$ and $\mathbb{P}(X\geq m)\geq \frac{1}{2}$","['probability-theory', 'median', 'limits']"
2827184,Pluriharmonic functions are harmonic on submanifolds?,"Let $X$ a Kähler manifold and $f:X\to \mathbb C$ a function. $f$ is said to be pluriharmonic if its restriction to each curve in $X$ is harmonic. Why is then $f$ harmonic? Does this simply follow from covering $X$ by curves? Does the same argument then imply that the restriction of $F$ to any complex submanifold of $X$ is harmonic? (I need to see that $f$ restricted to a special case of compact connected submanifolds is constant, which would follow from harmonicity) On a similar note, I have seen another of definition of $f$ being pluriharmonic, namely $\partial \overline{\partial} f=0$ Then it follows from a simple calculation that $f$ is harmonic. But how do I see that this definition I equivalent?","['complex-geometry', 'complex-analysis', 'harmonic-functions', 'differential-geometry']"
2827215,Using Lagrange Multiplier to prove identity,"Show that the maximum and minimum values of the function $u=x^2+y^2+xy$, where $ax^2+by^2=ab\ (a>b>0)$ are given by $$4(u-a)(u-b)=ab$$ My attempt- using Lagrange Multiplier method, $$F(x,y)= (x^2+y^2+xy)-\lambda(ax^2+by^2-ab)$$
$$dF= (2x+y-2a\lambda x)dx+(2y+x-2b\lambda y)dy$$ 
Equating to  zero to get,
$$y/x= 2(a\lambda -1); y/x= 1/(2(b\lambda-1))$$ $$\implies 4(a\lambda-1)(b\lambda-1)=1$$
$$\implies 4(a^2\lambda-a)(b^2\lambda-b)=ab$$ I am stuck here. Finding $(x_0,y_0)$ which finds extrema for $u$ and then substituting to prove $$4(u-a)(u-b)=ab$$ is very tedious. Can someone help on how I should go about it?","['multivariable-calculus', 'lagrange-multiplier', 'calculus']"
2827221,How to prove that $1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-1}(1+\frac1n))...)))=1+\frac1{1!}+\frac1{2!}+\frac1{3!}+...+\frac1{n!}$?,"I'm trying to prove that $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-1}(1+\frac1n))...)))=1+\frac1{1!}+\frac1{2!}+\frac1{3!}+...+\frac1{n!}$$ Using induction, suppose that $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{(n-1)}))...)))=1+\frac1{1!}+\frac1{2!}+...+\frac1{(n-1)!}$$ Then $$1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{n-1}(1+\frac 1n))...)))\\
=1+\frac11(1+\frac12(1+\frac13(...(1+\frac1{n-2}(1+\frac1{n-1}+\frac1{(n-1)n})...)))\\
=(1+\frac1{1!}+\frac1{2!}+...+\frac1{(n-1)!})+\frac1{n!}$$ But I couldn't completely justify the last equality. Could anyone explain this for me, please? Thanks!","['real-analysis', 'calculus', 'summation', 'sequences-and-series', 'analysis']"
2827249,"Can I prove that $f(x+1) - f(x)$ is a monotonic increasing function, given $f'(x) > x$ for every $x>0$?","Given: $f$ has a derivative for every $x \in (0,\infty)$ $f'(x) > x$ for every $x>0$ Can I prove that $f(x+1) - f(x)$ is a monotonic increasing function? From Lagrange I know that in every $I = [x,x+1]$ where $x>0$,
$f(x+1) - f(x) > x$. $f$ has derivative so $f'(x+1) - f'(x)$ is defined for $x>0$. But, How can I show that $f'(x+1) - f'(x) > 0$? Thanks! Edit: Thank you al for your answers! If I want to prove a weaker statement, that there exists some $M \in \mathbb R$ such that $f'(x+1) - f'(x)$ is increasing in $(M,\infty)$. Is it, then, can be proved? Thanks again!","['derivatives', 'calculus']"
2827257,Two exercises in functional analysis.,"I have met two questions which, after some attempts have I have yet been able to solve nor find any available solutions online. Could anyone please offer me some insights? Let $ H $ be a Hilbert space over $\mathbb{C}$ and $T \in B(H,H)$ an unitary operator. For $ n \in \mathbb{N} $ set $$ S_n : = \frac
{1}{n}( I + T + ... + T^{n-1} ) .$$
Show that $S_{n}v \longrightarrow P_{M}v $ as $ n \rightarrow \infty $ where $ P_{M} $ is the orthogonal projection onto the subspace $ \text{Ker}(I - T) $. Let $E$ be a normed vector space over $\mathbb{K}$, let $M \subseteq E $ be a subset and suppose that $ \sup_{ v \in M} |f(v)| < \infty $ for a all functional $f \in B(E, \mathbb{K})$. Show that $M$ is bounded. From what I have heard the second question is not meant to be difficult. However as $E$ is not complete there is not much theorem I could work with. I thought maybe by working with the Banach space $ B(E,\mathbb{K}) $ but came to no result. I'm still a noob in FA so any help would be much appreciated! Thanks!",['functional-analysis']
2827259,Convexity of a convex set minus points close to its frontier.,"Let $d\geq 1$. Let $K\subset\Bbb{R}^d$ be a convex set, $\varepsilon>0$, and $K(\varepsilon)\subset K$ the points of $K$ whose distance to $\partial K$ is less than $\varepsilon$. Is $K\setminus K(\varepsilon)$ convex?","['convex-geometry', 'general-topology', 'convex-analysis', 'geometry']"
2827283,Intuition behind binomial variance,"Suppose that I perform a stochastic task $n$ times (like tossing a coin) and that $p$ is the probability that one of the possible outcomes occurs. If $K$ is the stochastic variable that measures how many times this outcome occurred during the whole experiment, and if all the events are mutually independent, then the probability that $K$ is equal to a specific $k$ , with $0\leq k\leq n$ , is $$\mathrm{Pr}\{K=k\} = \binom n k p^k (1-p)^{n-k} =: b_n(k), $$ which makes intuitive sense to me. Now suppose that I want to know what the average , or expected , value of $K$ is going to be: the formulae tell me that $$\begin{split}
\langle K\rangle &= \sum_{k=0}^n k b_n(k) = \sum_{k=0}^n k \binom n k p^k (1-p)^{n-k} \\
&= \sum_{k=0}^n k \frac{n(n-1)!}{k(k-1)!(n-k)!} p p^{k-1} (1-p)^{n-k} \\
&= np \sum_{\kappa=1}^\nu \frac{\nu!}{\kappa!(\nu-\kappa)!} p^\kappa (1-p)^{\nu-\kappa} = np(p + 1- p)^{\nu} \\ &= np,
\end{split}$$ where I’ve made the substitutions $\kappa = k-1$ and $\nu = n-1$ . Notwithstanding the mathematical certainty of this derivation, it also makes perfect intuitive sense to me that $np$ should be the expected value of $K$ , since it is the product of the probability of the outcome times the number of trials performed: if there’s a $1/6$ chance that I roll a 5 on a fair dice, and I throw it $600$ times, then I expect to see a 5 about $100$ of those times. If instead I want to know how I should expect the outcomes to vary around the expected value, I may compute the variance of $K$ : with the same substitutions as before, $$\begin{split}
 \mathrm{Var}[K] &= \langle K^2\rangle - \langle K \rangle^2 = \left(\sum_{k=0}^n k^2 b_n(k) \right) -n^2p^2 \\
&= \left( \sum_{k=1}^n k^2 \binom n k p^k (1-p)^{n-k}\right) -n^2p^2 \\
&= \left( np \sum_{\kappa = 0}^\nu (\kappa +1) \binom \nu \kappa p^\kappa (1-p)^{\nu-\kappa} \right)-n^2p^2 \\
&= \left(np \Big(\sum_{\kappa=0}^\nu \kappa b_\nu(\kappa) +(p + 1-p)^\nu \Big) \right) -n^2p^2 \\
&= np(\nu p + 1) - n^2 p^2 = np( np - p + 1 - np) \\ &= np(1-p).
\end{split}$$ Again, the derivation is mathematically crystalline; but why should I expect that this be the formula for the variance of $K$ ? Why does multiplying the expected value of $K$ times the probability that my outcome doesn’t occur give me a measure of the dispersion of $K$ ? In other words, how can I justify this formula for variance intuitively in a similar way as I can with the formula for the mean? EDIT. Up until now, I’ve received answers that are just perfectly good explanations of how to derive the formula for the variance of $K$ in ways that differ from the one presented above. That’s not what I’m asking for. The ideal answer should contain as few formulae as possible, and use simple enough words to explain not why the formula is mathematically true, but why it’s reasonable and couldn’t possibly be otherwise – something like the intuitive explanation for $\langle K\rangle = np$ that I gave above.","['intuition', 'variance', 'probability-distributions', 'statistics', 'binomial-distribution']"
2827288,"If $\mathbb Z^m\twoheadrightarrow\left(\mathbb Z\big /2\mathbb Z\right)^n$ is a surjective homomorphism, why is then $m=n$?","If $\mathbb Z^m\twoheadrightarrow\left(\mathbb Z\big /2\mathbb Z\right)^n$ is a surjective homomorphism and $m\le n$, why does it force $m=n$ ? Is there no appropriate choice for a map when $m< n$ ?","['group-homomorphism', 'group-theory']"
2827310,"What are necessary and sufficient conditions for $a, b \in \mathbb{R}^2$ such that the curve $r(t) = a \cdot \cos(t) + b \cdot \sin(t)$ is a circle","So here's what I got so far: $r(t)$ is a circle iff $|r(t)| = \text{const.}$ iff $\sqrt{(a_1\cos(t) + b_1\sin(t))^2 + (a_2\cos(t) + b_2\sin(t))^2} = \text{const.}$ iff $$(a_1\cos(t) + b_1\sin(t))^2 + (a_2\cos(t) + b_2\sin(t))^2 = \text{const.}$$ iff $$a_1^2 \cos^2(t) + b_1^2\sin^2(t) + a_2^2\cos^2(t) + b_2^2\sin^2(t) + 2a_1b_1 \cos(t) \sin(t) + 2a_2b_2 \cos(t) \sin(t) = \text{const.}$$ iff $$a_1^2 \cos^2(t) + b_1^2\sin^2(t) + a_2^2\cos^2(t) + b_2^2\sin^2(t) + a_1b_1 \sin(2t) + a_2b_2 \sin(2t) = \text{const.}$$ iff $$a_1^2 \cos^2(t) + b_1^2\sin^2(t) + a_2^2\cos^2(t) + b_2^2\sin^2(t) + (a_1b_1 + a_2b_2) \sin(2t) = \text{const.}$$ iff $$(a_1^2 + a_2^2) \cos^2(t) + (b_1^2 + b_2^2)\sin^2(t) + (a_1b_1 + a_2b_2) \sin(2t) = \text{const.}$$ iff $$\langle a,a \rangle \cos^2(t) + \langle b,b \rangle\sin^2(t) + \langle a,b \rangle \sin(2t) = \text{const.}$$ iff $$\| a \|^2 \cos^2(t) + \| b \|^2 \sin^2(t) + \langle a,b \rangle \sin(2t) = \text{const.}$$ iff $$\| a \| \cos^2(t) + \| b \| \sin^2(t) + \langle a,b \rangle \sin(2t) = \text{const.}$$ It's easy for me to see that a sufficient condition is that the following two conditions hold $$ \langle a,b \rangle = 0 $$ and
$$ \| a \| = \| b \| $$ Now, did I do anything wrong up to this point? If not, I am really unsure if it's also a necessary condition and if there are any more sufficient and necessary conditions.","['algebra-precalculus', 'linear-algebra', 'geometry']"
2827321,Area preservation when transverse intersection,"This might be something way too trivial but I'm not seeing it yet so if you could explain to me the following I'd be thankful. On this page http://mathworld.wolfram.com/HomoclinicTangle.html the property ""area preservation"" is used, but I don't see where this comes from. Thanks in advance","['ordinary-differential-equations', 'dynamical-systems', 'analysis']"
2827331,To prove: $n!=\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n$ [duplicate],"This question already has answers here : Proof of the summation $n!=\sum_{k=0}^n \binom{n}{k}(n-k+1)^n(-1)^k$? (4 answers) Closed 6 years ago . How to algebraically prove that $$n!=\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n$$ I was trying to find number of onto functions from $A$ to $A$ containing $n$ elements. Using the inclusion-exclusion principle I am getting 
$$\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n.$$ We can also do it by simple combinatorics, as every element has to have a pre-image and number of elements in the domain equal to the number of elements in the codomain, the number of functions is $n!.$ Is there an algebraic way to prove these two are equal?","['permutations', 'abstract-algebra', 'combinatorics', 'linear-algebra']"
2827370,"Is $\langle x^2+1, 5\rangle$ a maximal ideal in $\mathbb{Z}[x]$?","I have some trouble with the following problem: Problem. Whether the ideal $I=\langle x^2+1, 5\rangle$ is a maximal ideal of $\mathbb{Z}[x]$ or not? Solution . For this I check whether the quotient $\mathbb{Z}[x]/I$ is a field or not. So if we take $J=\langle 5\rangle$ then by 2nd isomorphism theorem we have: $$\mathbb{Z}[x]/I \cong \frac{\mathbb{Z}[x]/J}{I/J}\cong \frac{\mathbb{Z}_5[x]}{I/J}.$$ Now we have to calculate the ring $I/J=\langle x^2+1, 5\rangle/\langle 5\rangle.$ I am suspecting it to be isomorphic to $\langle x^2+1\rangle.$ To prove this I try to use 1st isomorphism theorem. So I have to define a epimorphism from  $\langle x^2+1, 5\rangle$ to $\langle x^2+1\rangle$ whose kernel is $\langle 5\rangle.$  The natural intuition should be to define the map $\phi:\langle x^2+1, 5\rangle \to \langle x^2+1\rangle$ by $(x^2+1)f(x)+5g(x) \mapsto (x^2+1)f(x).$ Then it will be a homomorphism, moreover onto. Now clearly, $5\mathbb{Z}[x] \subset \ker \phi$. Again if an element is of the form $(x^2+1)f(x)$ then it doesn't map to $0$. So $\ker \phi=\langle 5\rangle.$ Therefore by First Isomorphism theorem $\langle x^2+1, 5\rangle/\langle 5\rangle \cong \langle x^2+1\rangle.$
 Then $$\mathbb{Z}[x]/\langle x^2+1, 5\rangle \cong \mathbb{Z}_5[x]/\langle x^2+1\rangle.$$ Now as the latter ring is not an field (since $x^2+1$ is not an irreducible in $\mathbb{Z}_5[x]$) so the given ideal is not an maximal ideal of $\mathbb{Z}[x]$. Is this proof is correct? Please tell if I made any mistake. Thank you.","['abstract-algebra', 'ring-theory', 'proof-verification']"
2827404,Characterization of discrete topology-like behavior about compact sets.,"It's well-known that in discrete topology a set is compact iff it is finite. There exist a lot of examples of topologies which are not discrete but with that fact still holding, and it's not hard to find some of them. Is there any theory about (infinite) topological spaces in which ""compact iff finite"" holds?",['general-topology']
2827416,"Suppose $X \sim Bin(n,p)$ and $Y \sim Bin(n,1-p)$, how is $X+Y$ distributed?","Suppose $X \sim Bin(n,p)$ and $Y \sim Bin(n,1-p)$, How is $X+Y$ distributed? I know that for independent variables one can do the same as: Sum of two independent binomial variables Furthermore i have seen this post: Addition of two Binomial Distribution However $X$ and $Y$ do not necessarily need to be independent of each other. Backstory: I am trying to calculate the entropy of some $Z = X_1 + X_2 + \dots + X_n$ where each $X_i$ is either $Bin(n,p)$ or $Bin(n,1-p)$ depending on its parent node. For example lets take a graph that has only outgoing edges of degree 2 beginning from some source $X_0 \sim (\frac{1}{2},\frac{1}{2})$. If we compare layer $2$ and layer $3$, we sent $2^3$ nodes to $2^4$ nodes. The probability for a set of child nodes to get certain states is $Bin(n,p)$ when the parent has state $1$ and $Bin(n,1-p)$ if the parent has state $-1$. We proceed this until we reached some threshold layer $d$. What is the probability distribution of $Z = \sum X_i^{(d)}$","['information-theory', 'probability']"
2827433,A subadditive bijection on the positive reals,"Question. Does there exist a subadditive bijection $f$ of the positive reals $(0,\infty)$ such that $$
\liminf_{x\to 0^+}f(x)=0 \,\,\,\text{ and }\,\,\,\limsup_{x\to 0^+}f(x)=1\,?
$$ Ps1. I guess the answer is no. However, it is affirmative if we replace ""bijective"" by ""injective"" by the following example: $f(x)=x$ if $x$ is rational, otherwise $f(x)=x+1$ . Ps2. The answer is also affirmative if we replace $$\limsup_{x\to 0^+}f(x)=1$$ with $$\limsup_{x\to 0^+}f(x)\neq 0.$$ Indeed, let $(x_i: i \in I)$ be an Hamel basis of the vector space $\mathbf{R}$ over $\mathbf{Q}$ . Fix a nontrivial permutation $\sigma$ of $I$ . For each $r=\sum_j \lambda_jx_j>0$ define $f(r)=|g(r)|$ , where $g(r)=\sum_j \lambda_j x_{\sigma(j)}$ ( thanks i707107 ). Note that $g$ is an additive bijection, hence $$
f(x+y)=|g(x)+g(y)| \le f(x)+f(y)$$ for all $x,y>0$ , so $f$ is subadditive. Now suppose $f(x)=f(y)$ with $x,y>0$ . Then either $g(x)=g(y)$ so $x=y$ , or $g(x)=-g(y)$ so each coefficient of $\lambda_j$ (in $x$ ) is the inverse of the corresponding (in $y$ ), so $x=-y$ : if $x,y>0$ , then $f(x)=f(y)$ implies $x=y$ so $f$ is injective. About surjectivity, fix $x=\sum_{j} \lambda_j x_j$ and define $y=\sum_j \lambda_j x_{\sigma^{-1}(j)}$ . Hence $z:=\max(y,-y)>0$ and $f(z)=x$ ; so $f$ is subjective. Finally, it is known that $g$ has a graph dense in $\mathbf{R}^2$ , from which it holds also $\liminf_{x\to 0^+}f(x)=0$ and $\limsup_{x\to 0^+}f(x)=\infty$ .","['real-analysis', 'limsup-and-liminf']"
2827445,"Prove that $f$ is constant, when $ \operatorname{Re}(f)^m = \operatorname{Im}(f)^n$","Take $f:U\rightarrow \mathbb{C}$ is holomorphic function in $U$ and $U$ connected open subset. If exist $m,n\in \mathbb{N}$ such that $$ [
 \operatorname{Re}(f(z))]^m = [ \operatorname{Im}(f(z))]^n ,$$ $f$ is constant in $U$ . Note: The exercise requires using the theorem and equations of Cauchy-Riemann. NB: An attempt is in the comments. Question : In my first attemp, I want to see that is not possible that $det(A_{(x,y)})=0 \ \ \forall (x,y) \in U$ One possibilities is $v^{2n-2} (x,y) =0$ . This implies that $f(x,y)=0$ . But, is in this point. And others? $f=0$ ever?",['complex-analysis']
2827457,Solve $\cos x +\cos y - \cos(x+y)=\frac 3 2$ [duplicate],"This question already has answers here : How do I prove equality of x and y? (4 answers) Closed 3 years ago . Solve $\cos x +\cos y - \cos(x+y)=\frac 3 2$ where $x,y\in [0,\pi]$. I am trying to  solve this but I am stuck. I know that $x=y=\pi/3$ is a solution but how do I show this is the only one? I think there are no others! Hints would be appreciated","['algebra-precalculus', 'trigonometry']"
2827464,How to show that $G=\mathrm{Gal}[\Bbb Q(\xi_{2^n})/\Bbb Q(\xi_{2^m})]$ cyclic?,"How to show that $G=\mathrm{Gal}[\Bbb Q(\xi_{2^n})/\Bbb Q(\xi_{2^m})]$ cyclic?
 $n>m>1$ are natural numbers.
$\xi_{2^n}$ and $\xi_{2^m}$ are cyclotomic roots. I know that the order of $ G $ is $|G|=2^{n-m}$","['abstract-algebra', 'galois-theory', 'field-theory']"
2827465,How many local extrema can a multidimensional polynomial have?,"So say I have a $n$ dimensional polynomial of degree $m$. Assume that $n\geq m$. Now for a degree $m$ polynomial in one dimension, we know that there can be at most $m-1$ local extrema. Is there a similar rule for multidimensional polynomials? One important thing to note is that the polynomial I am working with is only linear terms of each dimension, so there will never be a $x_k^y$ for any $y>1$, for example Good: $f(\vec x) = x_1x_2x_3 + x_1x_2 - x_3$ Bad: $f(\vec x) = x_1^2x_2 + x_3^3 - x_2$","['multivariable-calculus', 'polynomials', 'optimization', 'calculus']"
2827468,Simple way to write set of elements that are in either set but not both,Consider two sets $P$ and $P$'. I am trying to find the simplest notation denoting all elements that are in either $P$ or $P'$ but not both. I think this is $(P\cup P')\setminus (P\cap P')$. Is there any symbol in set notation already reserved for this operation?,"['notation', 'elementary-set-theory']"
2827485,Generating function of a recurrent relation,"Problem: Find the generating function for the recurrent relation:
$$f_n=2f_{n-1}+\frac 12 f_{n-2},$$
where
$$f_0=f_1=1.$$
My idea was to first find a few of the beginning values and then try to make into some sum of an infinite series, but  I had no luck. Any ideas?","['recurrence-relations', 'discrete-mathematics']"
2827488,Is the growth on an exponential function faster than on a power function? Is this explanation valid?,"I'm trying to find simple mathematical proof that lengthy passwords are more secure than complex (with multiple symbols) ones. I don't want to get into the concept of entropy - it is not useful to me atm . I've found this explanation at Security Stack , which it seemed pretty reasonable to me. My question is: is this explanation valid and, if so, is there a limit to it's validity? . My doubt comes from testing it out considering a password with $n$ digits, but restrict to a group of 26 characters (the alphabet) vs a password with fixed 10 digits, with $n$ possible characters: $$f(n) = 26^n \qquad\text{vs}\qquad f(n) = n^{10}.$$ For these two functions it just doesn't work out as the answer in Security Stack explained. So, how does this work? I guess the title may be a bit misleading, because this question markes as duplicate does not answer my question! I get that exponential growth is faster than polynomial growth, but why doesn't it apply to these two functions I'm comparing? I mean, what is wrong in my line of thought?","['exponential-function', 'probability', 'functions']"
2827493,"Does the list of ""number of groups of order $n$"" contain every natural number?","In other words: For every natural number $m$ , does there always exist an $n$ for which there are exactly $m$ groups of order $n$ up to isomorphism? Or is this an open question in mathematics? If it is an open question, are there any famous conjectures one way or the other? And what progress has been made in answering the question?","['finite-groups', 'abstract-algebra', 'groups-enumeration', 'group-theory', 'open-problem']"
2827516,"Prove that a functional in $L^2(0, \pi)$ is bounded","Let $\Phi$ be a functional defined on $L^2(0, \pi)$ such that $\Phi(\sin(nx))=a_n$ on the basis $\{\sin(nx)\}_{n=1}^{\infty}$ with $\{a_n\}$ sequence of complex numbers. What are the conditions on $\{a_n\}$ for which $\Phi$ is bounded? My attempt. For any $f\in L^2(0, \pi)$, I can write $f(x)=\sum_{k=1}^\infty f_k\sin(kx)$. Then $\Phi(f)=\sum_{k=1}^\infty f_ka_k$. By Cauchy-Schwarz inequality, we have that
$$
|\Phi(f)|^2\leq\sum_{k=1}^n|f_k|^2\sum_{k=1}^n|a_k|^2.
$$
Now, 
$$
\|f\|_{L^2(0, \pi)}^2=\sum_{k=1}^\infty\int_0^{\pi}f_k^2\sin^2(kx)\ dx=\frac{\pi}{2}\sum_{k=1}^\infty f_k^2
$$
and then $\sum_{k=1}^\infty |f_k|^2=\frac{2}{\pi}\|f\|_{L^2(0, \pi)}$. It follows that, if $\sum_{k=1}^\infty|a_k|^2=s<\infty$, then
$$
|\Phi(f)|^2\leq\frac{2s}{\pi}\|f\|^2_{L^2(0, \pi)}<\infty.
$$
Is my attempt correct? Thank You","['functional-analysis', 'lp-spaces', 'hilbert-spaces']"
2827524,Eigenpolynomial of a linear operator,"Let $V$ be a $n$-dimensional vector space over a field $F$, let $A\in \text{End}(V)$. Let $q\in F[x]$ be an irreducible polynomial and 
$$
V_q:=\{x∈V:q(A)x=0\}.
$$ I wish to prove that  $V_q$ is not trivial iff $q$ divides the characteristic polynomial $\chi$ of the operator $A$.",['linear-algebra']
2827546,Undetermined Coefficients for solving non homogeneous equation,"I can't seem to figure out where I am going wrong in my steps. I checked the answer and it is different. The question is: $$y'' + 2y' - 3y = 3te^t$$ The roots are: -3,1.
Thus the general solution is:
$$y=C_1e^{-3t} + C_2e^t$$ The particular solution i am going with is:
$$y_p = Ate^t$$
$$y'_p = Ae^t + Ate^t$$
$$y''_p = 2Ae^t + Ate^t$$ Therefore:
$$2Ae^t + Ate^t + 2Ae^t + 2Ate^t - 3Ate^t = 3te^t$$ $$4Ae^t = 3te^t$$ Then solving for A:
$$4A=3$$
$$A=3/4$$ Thus, $y_p = \frac{3}{4}(e^t + te^t)$ Then the general solution would be:
$y = C_1e^{-3t} + C_2e^t  + \frac{3}{4}(e^t + te^t)$ Any guidance with my mistake would be greatly appreciated. As an aside, what does it mean when a question asks to use the stability
result to determine they will have a globally stable solution of the above question. Thank you.","['stability-in-odes', 'ordinary-differential-equations', 'calculus']"
2827575,"For $a>0$ compute $\int_0^\infty \frac{\log x}{(a + x)^3}\,dx$","Assume that $a > 0$. Compute $$
  \int_0^\infty \frac{\log x}{(a + x)^3}\,dx.
$$ This seems resistant to the usual strategies—indeed, I can't even think of an appropriate contour. I tried shifting the integral, $$
   \int_a^\infty \frac{\log (x - a)}{x^3}\,dx,
$$ but this seemed to cause sections of my contour to be unbounded. I couldn't find a substantially similar question here, but please let me know if it has already been asked.","['complex-analysis', 'integration', 'contour-integration', 'complex-integration']"
2827591,Alternate proof for weighted alternating shifted central binomial sum relation,"In two recent posts, MSE 2824529 and MSE 2825442 , both initiated by user196574, I answered two asymptotic questions for $n \to \infty$ with the following identity: $$ [1]\,\,\,\,\,\sum_{k=1}^n (-1)^{k+1} \binom{2n}{n+k} k^s =
\binom{2n}{n} \sin(\pi s/2) \int_0^\infty \frac{dx \, \,x^s}{\sinh{\pi x}} \frac{n!^2}{(n+ix)!(n-ix)!}.
$$ I'm asking for a simpler proof or a generalization in which this formula is a special case. For scoring, Generalization > Simplicity. An example of a generalization might be a Dirichlet character analog. So on the left there is squeezed in some sequence of (+1,-1,0) and on the RHS the hyperbolic trig function might get replaced with something more complicated, like a ratio of a linear combination of hyperbolic trig functions. My proof, as requested from two contributors, will follow. I was also asked for the motivation for such a formula.  It wasn't for asymptotics, but instead to cobble a proof of the functional equation for Riemann's zeta function (at least formally) by using a hypergeometric identity.  Since there are many rigorous proofs for the functional equation (and I don't publish anyway) I'd almost forgotten about it. If someone can generalize the formula as suggested above, it may be worth a revisit. $\textbf{Proof}$ Establish the formula, valid for $0 < Re\,s < 2n$ $$ [2]\,\,\,\,\,\sum_{k=1}^n (-1)^{k+1} \binom{2n}{n+k} k^s =
2^{2n-s}\frac{s!}{\pi}\, \sin(\pi s/2) \int_0^\infty \frac{dt}{t^{s+1}}\, \sin^{2n}t.
$$ The next few lines reproduce a well-known trig ID. By the binomial theorem $$\sum_{k=0}^{2n}\binom{2n}{k}(-x)^k = (x-1)^{2n}=\sum_{k=-n}^n\binom{2n}{k+n}(-x)^{k+n} $$ By splitting the sum at $k=-1$ and $k=1$ we have $$\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\big(x^k + x^{-k} \big) = (-1)^n(x-1)^{2n}\,x^{-n} - \binom{2n}{n}.$$ Insert $x=\exp{(2it)}$ to get $$[3]\,\,\,\,\,(2\sin t)^{2n}= 2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\cos(2\, k\, t)
+ \binom{2n}{n}.$$ Insert Left Hand Side (LHS) of previous equation into integral on RHS of eq [2]. Also insert a regularizer $\exp{(-p\,t)}$ and we'll let $p \to 0$ : $$ \int_0^\infty \frac{(2\sin{t})^{2n}}{t^{s+1}}dt = \lim_{p\to 0} 
\int_0^\infty dt \Big(2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\cos(2\, k\, t)
+ \binom{2n}{n} \Big)\frac{e^{-pt}}{t^{s+1}}.
 $$ $$
= \lim_{p\to 0} 2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\Gamma(-s)\cos\big(s \arctan(2k/p)\big)(p^2+(2k)^2)^{s/2}
+ \binom{2n}{n}\Gamma(-s)p^{s}
$$ where the Euler integral for the $\Gamma$ function has been used.  The requirement $s>0$ and the assumption $s$ not an integer will mean the last term $\to$ 0. The last assumption will be relaxed upon analytic continuation.
Taking the limit, e.g. $lim_{p\to 0} \arctan(2k/p)=\pi/2$ and using the reflection formula for the $\Gamma$ function allows us to write the result as eq [2].  Analytic continuation permits the $s$ to be extended to the stated range.  The proof of [2] is complete. Now the $t^{-s-1}$ will be represented by the Euler integral to derive a double integral that will eventually give rise to the form in eq. [1]. $$s!\int_0^\infty \sin^{2n}\!t\frac{dt}{t^{s+1}}= \int_0^\infty dt \sin^{2n}t\int_0^\infty\exp{(-xt)}x^s dx = \int_0^\infty dx \, x^s \int_0^\infty dt\, e^{-xt}\sin^{2n}\!t$$ Now we'll show $$[4]\,\,\,\, J := \int_0^\infty dt\, e^{-xt}\big(2\,\sin{t}\big)^{2n} = (2n)! \frac{i}{2} \frac{\Gamma(ix/2)}{(n+ix/2)!}
  \frac{\Gamma(1-ix/2)}{(n-ix/2)!}$$ Again insert [3] into the LHS of 4, and again use the Euler integral to find $$J=2\sum_{k=1}^n (-1)^k\binom{2n}{n+k}\frac{x}{x^2+(2k)^2} + \binom{2n}{n}\frac{1}{x} $$ $$=\binom{2n}{n}\Big( 2\sum_{k=1}^n\frac{(-n)_k}{(n+1)_k}\frac{x}{x^2+(2k)^2} + \frac{1}{x} \Big) $$ where the expression in the last line uses the Pochhammer symbol. Now the following identity is known, e.g., Table of Series and Products , Hansen 6.6.57: $$ \sum_{k=1}^n \frac{(-n)_k}{(n+1)_k}\frac{1}{k^2-a^2} = \frac{1}{2a^2}
\Big(1-\frac{n!^2}{(1+a)_n\,(1-a)_n} \Big).$$ Use of it completes the proof of [4]. Use the $\Gamma$ reflection formula, rescale the integral and do some algebra and the proof of eq. [1] is complete.","['asymptotics', 'analysis', 'closed-form']"
2827603,Definition of a general point,"I'm actually seeking a formal definition of a general point. I already read the post in https://mathoverflow.net/questions/64089/general-point-general-line/64096 but it was not helpful. In addition, several Algebraic geometry references uses this as a well-known notion, but I don't know what is the formal formulation for it. Anyone can help me with a reference or a definition?
Thanks in advance. K.Y.","['algebraic-geometry', 'definition']"
2827618,Evaluate $\lim_{x \to 0+}{ (e^{1/\sin x}-e^{(1/x)})}$,"I'm a high school student. For the past few hours, I was practicing the calculation of limits to improve my skills and I got stuck in a simple but complicated (for me) limit. The limit in question is $$\lim_{x \to 0^+} \left( e^\frac{1}{\sin(x)} - e^\frac{1}{x} \right)$$ Calculating this limit I get zero, but Wolfram says it is infinite.
Could someone help me?","['calculus', 'limits']"
2827646,Simpler solution for EGMO 2018 Problem 3?,"This is a reformulation of problem number 3 from the 2018 European Girls Math Olympiad .  It took me several days to come up with a complete solution, and I'm wondering if there is an easier way. Consider a solitaire game with $n$ cards, labeled $1$ though $n$.  The cards are shuffled and placed in a line, face up..  If the card labeled $k$ has at least $k$ cards to its left, you are allowed to jump it $k$ places to the left, inserting it into the line.  There are no other legal moves. For example, if $n=7,$ and the layout is $2753146,$ you can jump the $3$, giving $3275146,$ or the $1,$ giving $2751346,$ or the $4$, giving $2475316,$ or the $6,$ giving $6275314.$ The problem is: 1) Show that the game cannot go on forever.  Eventually, you must reach a position in which no moves are possible. 2) What is the maximum possible number of jumps in a game with $n$ cards, if the initial position is as advantageous as possible, and you make the best moves? We have to do three things: 1) Prove that the game cannot last forever. 2) Establish an upper bound on the number of jumps. 3) Construct a game with the maximum number of jumps. Early on, I decided that the maximum number of jumps is $2^n-(n+1).$  It's not hard to come up with a game of this length.  Start with the cards in decreasing order from left to right, and jump until they are in increasing order.  To do this, recursively sort the last $n-1$ cards in $2^{n-1}-n$ jumps, then jump the cards from back to front, arriving at the order $$n-1,n-2,\dots,1,n$$ and then recursively reverse the first $n-1$ cards again. To show that the game cannot go on for ever, I proved the result for a more general game, where the cards are labeled with any $n$ distinct positive integers.  Let $N\ge n$ be the largest label. If $N$ is in the rightmost position, it can never move, and the result follows by induction.  If $N$ is not in the rightmost position, it is enough to show that the game cannot last forever without $N$ moving to the right, that is, without a card jumping from $N'$s right to its left.  This is true by induction  trivially for the cards to $N'$s left, and if the $k$ cards to $N'$ right can jump among themselves forever, then the $K-$card game comprising those cards can also last forever, contradicting the induction hypothesis. The part that gave me the most trouble was establishing that $2^n-(n+1)$ is an upper bound.  I realized early on that is the long path I'd constructed, $k$ jumps $2^{n-k}-1$ times.  Of course, proving this is true in general is enough to establish the upper bound.  It took me quite a while to see how to approach it, but finally I realized that when $k$ jumps, since it moves $k$ positions, it must jump over a number greater than $k$, and that if $K>k,$ then $k$ jumps over $K$ at most $2^{n-K}$ times.  I proved this by induction on $n-k$.  After $k$ first jumps over $K>k,\ K$ is on the right of $k$.  In order for $k$ to jump over $K$ another time, $K$ must first jump over $k$.  Then induction hypothesis implies that $K$ jumps at most $2^{n-K}-1$ times, so $k$ can jump over $K$ at most $2^{n-K}$ times. Do you see a simpler way to do this problem, particularly to establish the upper bound?  (Of course, I realize that part 2) implies part 1), but when I was working the problem it seemed natural to prove part 1) independently.)","['contest-math', 'extremal-combinatorics', 'discrete-mathematics']"
2827700,"A curve $C$ in the $x$-$y$ plane is such that the line joining the origin to any point $P(x,y)$ on the curve and the line parallel to?","A curve $C$ in the $x-y$ plane is such that the line joining the origin to any point $P(x,y)$ on the curve and the line parallel to the $y$ axis through P are equally inclined to the tangent to the curve at P. Find the differential equation of the curve $C$. Slope of the line from origin to $P(x,y)$ will be $\frac{y}x = m1$ Slope of the line from $P$ parallel to y axis = $\tan(90) = m2$ I am having trouble proceeding from here.","['ordinary-differential-equations', 'differential-geometry', 'calculus']"
2827702,Bijection $f\colon\mathbb{N}\to\mathbb{N}$ with $f(0)=0$ and $|f(n)-f(n-1)|=n$,"Let $\mathbb{N}=\{0,1,2,\ldots\}$ . Does there exist a bijection $f\colon\mathbb{N}\to\mathbb{N}$ such that $f(0)=0$ and $|f(n)-f(n-1)|=n$ for all $n\geq1$ ? The values $f(1)=1$ , $f(2)=3$ , and $f(3)=6$ are forced. After that, you might choose to continue with $$\begin{array}{c|c}n&f(n)\\\hline0&0\\1&1\\2&3\\3&6\\4&2\end{array}\qquad\begin{array}{c|c}n&f(n)\\\hline5&7\\6&13\\7&20\\8&28\\9&37\end{array}\qquad\begin{array}{c|c}n&f(n)\\\hline10&27\\11&16\\12&4\\13&17\\14&31\end{array}$$ Here, after deceasing to $f(4)=2$ , the next smallest remaining value was 4. I chose to continue to increase until there was a clear way down to 4 and back up. As mentioned in the comments, the greedy algorithm where you decrease whenever you can is given by sequence A005132 in the OEIS. However, this sequence gets stuck at $f(20)=42$ , $f(21)=63$ , $f(22)=41$ , $f(23)=18$ as there is no possible value for $f(24)$ . Also, this greedy approach would take longer to get to the value $4$ . In general, if $k$ is the smallest number that you haven't hit yet then the strategy of increasing until there is a clear way down to $k$ and a clear way back up seems to be a reasonable strategy. This is the strategy employed by the table above. Unfortunately, Symlic's answer shows that this strategy doesn't work (in fact, it will never hit the value 5). Therefore, a more sophisticated strategy is required. If you instead consider $f\colon\mathbb{N}\to\mathbb{Z}$ then alternating between increasing and decreasing is a valid bijection. However, employing this technique in original situation will often lead to you getting stuck in coming back up.","['combinatorics', 'natural-numbers', 'sequences-and-series']"
2827707,Relation between non-vanishing Vector Fields on $\mathbb{T}^2$ and Fundamental Group Maps,"Let X be a vector field on $\mathbb {T}^2$, we say that $\varphi: \mathbb {R} \to \mathbb {T}^2$ is a periodic orbit of $X $, if $\varphi $ is a periodic function and $\varphi'(t) = X (\varphi(t)), \forall t \in \mathbb {R} $. Let $H: T\mathbb{T}^2 \to \mathbb{T}^2 \times\mathbb{R}^2$ be a parallelization of $\mathbb{T}^2$, i.e ; $H$ is a smooth function such that $H(p,v) = (p, A(p) v)$, where $A(p)$ is a isomophism between $T_p\mathbb{T}^2$ and $\mathbb{R}^2$ definided in the following way: Consider $\mathbb{T}^2 = \mathbb{R}^2 / \mathbb{Z}^2$ (flat torus), then for each point $[(x,y)]\in\mathbb{T}^2$ there are two loops $[(x+t,y)]$ and $[(x,y+t)]$, we define $A([(x,y)])$ as the unique linear transformation between $T_{[(x,y)]} \mathbb {T}^2$ and $\mathbb {R}^2$ satisfying $$A([(x,y)]) \left.\frac{d}{dt} [(x+t,y)]\right|_{t=0} = (1,0) \quad\text{and,}\quad A([(x,y)]) \left.\frac{d}{dt} [(x,y+t)]\right|_{t=0} = (0,1). $$ I'm having trouble to solve the following exercise (This exercise can be found on page 6 of the book ""The dynamics of vector fields in dimension 3-Etienne Ghys""). Exercise: Suppose $X$ is a non-singular vector field on $\mathbb{T}^2$, and consider $X^∗$
  as the map $$ X^*: \pi_1\left(\mathbb{T}^2, x_0\right) \to \pi_1 \left(\mathbb{R}^2 \setminus \{0\}, A(x_0)X(x_0) \right)$$
  $$\left[\alpha(t)\right] \mapsto \left[A(\alpha(t)) X(\alpha(t))\right] $$
  If $X^*$ is a non-trivial homomorphism then $X$
  has a periodic orbit. I was trying to show that if $X$ doesn't admit a periodic orbit, then for every loop $\varphi$ we have $A(\varphi(t)) X(\varphi(t))$ homotopic to a constant curve, but I wasn't able to conclude such thing. Can anyone help me or can give me some hint? Update I noticed that if there exists $\alpha_1,\alpha_2:\mathbb{S}^{1}\to \mathbb{T}^2$, such that $[\alpha_1]$ and $[\alpha_2]$ are generators of $\pi_1(\mathbb{T}^2,x_0)$ and $\{ \alpha_i'(t),X(\alpha_i(t))\}$ is a basis of $T_{\alpha_i(t)} \mathbb{T}^2$, then $A(\alpha_i(t)) X(\alpha_i(t))$ is homotopic to $A(\alpha_i(t)) \alpha'_i (t) $ (the homotopy
$$\tilde{H}(s,t) = A(\alpha_i(s))\left( (1-t) X(\alpha_i(s)) + t \alpha_i'(s) \right) $$
do the job). And is relatively easy to demonstrate that $A(\alpha_i(t)) \alpha'_i (t)$ is homotopic to the constant map. (I realized that this homotopy doesn't work because  it doesn't fix the ends of the interval) Does anyone know if $X$ is a non-singular vector field on $\mathbb{T}^2$ without periodic orbits then there are $\alpha_1$ and $\alpha_2$ as described above? I think it's important to inform that in the chapter that this exercise is contained there is the following theorem Theorem: Every non-singular $\mathcal{C}^2$ vector field on a compact
  surface that has no periodic orbits is topologically equivalent to a linear
  flow on $\mathbb{T}^2$ with irrational slope I think that the key to solving the exercise is in the above theorem but I wasn't able to figure out how to do this.","['dynamical-systems', 'homotopy-theory', 'differential-topology', 'algebraic-topology', 'ordinary-differential-equations']"
2827710,Inverse Function Theorem in Immersions.,"Let $\varphi: U \to \mathbb{R}^{n}$ of class $C^{k}$ ($k\geq 1$) in the open $U \subset \mathbb{R}^{m}$. If $a \in U$ is such that $\varphi'(a): \mathbb{R}^{m} \to \mathbb{R}^{n}$ is injective, then there exists a decomposition in direct sum $\mathbb{R}^{n} = \mathbb{R}^{m}\oplus\mathbb{R}^{n-m}$ and an open $V$, with $a \in V$, such that $\varphi(V)$ is the graph of an aplication $f: W \to \mathbb{R}^{n-m}$, of class $C^{k}$ in the open $W \subset \mathbb{R}^{m}$. We can write $C^{k} \ni \varphi: U \to \mathbb{R}^{m+(n-m)}$. As a consequence of the Inverse Function Theorem applied to immersions and by hypothesis about $\varphi$, there is a diffeomorphism ($C^{k}$) $h: Z \to X \times Y$ where $Z \ni \varphi(a)$ is open in $\mathbb{R}^{m+(n-m)}$ and $X \times Y \ni (a,0)$ is open in  $\mathbb{R}^{m}\times \mathbb{R}^{m+(n-m)}$, such that $(h\circ \varphi)(x) = (x,0)$ for all $x \in X$ and $h$ is strongly differentiable in $\varphi(a)$. Seems intuitive that $``h$ is $f""$ and $``X$ is $W""$, but I couldn't develop more than this. A detail that confused me is: the dimmensions of $X \times Y$ and $\mathbb{R}^{n-m}$ are not equals. Maybe I'm using this result incorrectly. Can anybody help me? Thanks for the advance!","['derivatives', 'real-analysis', 'inverse-function-theorem']"
2827712,Prove that a linear combination of zero-sum vectors also sums to zero,"I have a matrix $A$ whose rows sum to zero, such that $\sum_j A_{ij} = 0, \forall i$. If I multiply it by any matrix, $B$, can it be proven that the resulting matrix, $C = BA$, must also have zero sum rows? I find that they are empirically. Is such a proof available as a reference in any text book?",['linear-algebra']
2827759,Meaning of Grothendieck quote: recover a topological space by its category of sheaves,"In ""Récoltes et Semailles""(- Grothendieck), there is a moment when the author talks about the idea of sheaves of sets over a topological space, then taking the category of sheaves (of sets over a topological space): It [the category of sheaves] functions as a kind of ""superstructure of measurement"", called the
  ""Category of Sheaves"" ( over the given space), which henceforth shall be
  taken to incoorporate all that is most essential about that space. This
  is in all respects a lawful procedure, ( in terms of ""mathematical common 
  sense"") because it turns out that one can ""reconstitute"" in all respects,
  the topological space by means of the associated ""category of sheaves"" ( or ""arsenal"" of measuring instruments)
  (For the mathematical reader) Strictly speaking, this is only true
  for so-called ""tame"" spaces. However these include virtually all of the
  spaces one has to deal with, notably the ""separable spaces"" so dear to
  functional analysts. ( The verification of this is a simple exercise- once someone thinks to
  pose the question, naturally) One needs nothing more ( if one feels the
  need for one reason or another), henceforth one can drop the initial space
  and only hold onto its associated ""category"" ( or its ""arsenal""), which
  ought to be considered as the most complete incarnation of the
  ""topological (or spatial) structure"" which it exemplifies (English translation by Roy Lisker) Now, my question is: what is the meaning of this part, especially the part when he says that one can ""reconstitute"" in all respects the topological space by means of the associated ""category of sheaves""?","['sheaf-theory', 'topos-theory', 'algebraic-geometry', 'category-theory', 'algebraic-topology']"
2827764,"100 blank cards, minimize the EV","I give you a hundred blank cards, and you can write a single positive integer on each card. I look at the cards when you are done, then I shuffle the deck. I guess the top card of the deck, and if I am right, I make the dollar which is written on the card. What numbers should you write on the cards to minimize the expected return of mine? Attempt: So this problem seems to me quite difficult. If I put a 1 on a cards, then the Expected value is 1. if I put two 2s, and the rest 1-99, the Expected value is 99/100. I feel  the minimum occurs when i is an integer on at least one of the cards, where $ip_{i} = jp_{j}$ for every i,j is nearly satisfied, otherwise you could minimize it further. So p1=2p2 = 3p3 =...=npn So if you used only 1 and 2, then could get EV close to 2/3. So to solve this I feel I need to work out the minimum G such that, p1 ≈ p2 ≈  p3 ≈ .. ≈  pn ≈  G where you cannot reorganise the cards, to make a closer approximation.","['algebra-precalculus', 'probability']"
2827856,Almost sure convergence of sequence of discrete uniforms to continuous uniform,"Let $X_n$ $(n=1,2,\dots)$ be a sequence of discrete random variables, where the distribution of $X_n$ is the discrete uniform over $\{0, 1/n, 2/n,\dots,1 \}$. Let $U$ be a random variable whose distribution is the continuous uniform over $[0,1]$. Does $X_n \to U$ almost surely? My approach is to use Borel-Cantelli to check that $\sum_n\Pr[|X_n-U|>\epsilon] < \infty$, but it appears that for $n>2$ and $\epsilon_n < 1/(n+1)$, we have $\Pr[|X_n-U|>\epsilon_n] = 1-2n{\epsilon}_n/(n+1)$, whose sum does not converge. Is there another approach? If the convergence does not happen almost surely, is it possible for any discrete random variable to converge almost surely to a continuous random variable? (It is clear in the example above that that $X_n \to^\mathrm{D} U$ i.e. convegence in distribution.)","['probability-theory', 'probability', 'borel-cantelli-lemmas', 'convergence-divergence', 'sequences-and-series']"
2827859,Question about central limit theorem on two exercises.,"I was presented two formulas, if random variables ${X_1,...,X_n}$ form a random sample of a distribution of mean $\mu$ and standard deviation $\sigma$ and $n \to \infty$:
\begin{align}
&P\left(\frac{\overline{X_n}-\mu}{\sigma/\sqrt n}< a\right) = \Phi(a) \:\:\:\: (1) \\
&P\left(\frac{\overline{X_n}-n\mu}{\sigma\sqrt n}< a\right) = \Phi(a) \:\: (2) 
\end{align}
where $\Phi$ is the cdf of a standard normal distribution. But I not sure when to use which. For example. Each minute a machine produces $4$ meters of rope with standard deviation of $0.4$ meters. Assuming the amount produced in differet minutes are i.i.d, what is the probability that the machine will produce at least $250$ meters. I'm supposed to use $(2)$ but why? Another example: $16$ digits are chosen at random from the set $\{0,...,9\}$. What is the probability that their average will lie between $4$ and $6$? I'm supposed to use $(1)$ but why?","['statistics', 'probability', 'central-limit-theorem']"
2827861,Formal power series: $F_i(x)$ converges if and only if $\lim_{i\to\infty}deg(F_{i+1}(x)- F_i(x))=\infty.$,"The following notations and definitions are taken from Richard Stanley's book Enumerative Combinatorics Volume $1,$ second edition. Recall that a formal power series $F(x)$ is of the form 
$$\sum_{n\geq 0} a_n x^n$$
where $x$ cannot take numerical value. If $F_1(x), F_2(x),...$ is a sequence of formal power series, and if $F(x) = \sum_{n\geq 0}a_n x^n$ then we say that $F_i(x)$ converges to $F(x)$ as $i\to\infty$ provided for every $n\geq 0,$ there is a number $\delta(n)$ such that the coefficient of $x^n$ in $F_i(x)$ is $a^n$ whenever $I\geq \delta(n).$ The degree of a nonzero formal power series $F(x) = \sum_{n\geq 0}a_nx^n$ is the least integer $n$ such that $a_n\neq 0.$
It is denoted by $deg(F(x)).$ Statement: $F_i(x)$ converges if and only if 
  $$\lim_{i\to\infty}deg(F_{i+1}(x)- F_i(x))=\infty.$$ If all $F_i(x) = F(x),$ wouldn't the statement false? Because $F(x), F(x),...$ clearly converges to $F(x)$ but 
$$\lim_{i\to\infty}deg(F(x)-F(x)) = 0 \neq \infty.$$
If we assume that all $F_i(x)$ are distinct from $F(x),$ then I do not know how to prove the statement.","['generating-functions', 'combinatorics', 'formal-power-series', 'discrete-mathematics']"
2827865,Variance of the number of copies of a random variable needed to exceed a given sum,"Let ${X_i}$ be independent, identically distributed, random variables each with mean $M$ and variance $\sigma^2$. Let $Y(z)$ be the number of these random variables we need to add together to exceed z, that is, the smallest integer such that $X_1 + X_2 + ... + X_Y > z.$ It seems clear that as $z$ approaches infinity, $Y(z))$ should be about $z/M$, so that $E(Y(z))M/z$ should approach $1$. But what is the variance of $Y(z)$ for large $z$? Intuitively, when we add about $z/M$ copies of $X_i$ together, the variance of the resulting sum is about $\sigma^2 z/M$. So typical values will differ from $z$ by about $\sigma \sqrt{z/M}$, which we can fix by adding or subtracting about $\sigma\sqrt{z/M}/M$ copies of $X_i$. So I would conjecture that if we denote the variance of $Y(z)$ by $V(z)$, then $$lim_{z \to \infty} V(z) M^3/(\sigma^2 z)$$ should exist and be positive. Is this true? What is the limit?","['statistics', 'probability']"
2827883,Example of a full set of sections on product scheme that is not one on each summand,"In Katz and Mazur's book ""Arithmetic Moduli of Elliptic Curves"" (available here ), the following statement is made about full set of sections and fibered product (Lemma 1.8.5, page 35). Let $Z_1/S$ and $Z_2/S$ be two non-empty finite flat $S$-schemes of finite presentation and ranks $N_1$, $N_2$ respectively. Let 
  $$P_1^{(1)},\ldots ,P_{N_1}^{(1)}\in Z_1(S)$$
  $$P_1^{(2)},\ldots ,P_{N_2}^{(2)}\in Z_2(S)$$
  be given sequences of $N_i$ points in $Z_i(S)$ for $i=1,2$. Suppose that $Z_1/S$ is finite étale. Then the following conditions are equivalent: 1. For $i=1,2$, the $N_i$ points $P_1^{(i)},\ldots ,P_{N_2}^{(i)}$ in $Z_i(S)$ form a full set of sections of $Z_i(S)$. 2. The $N_1N_2$ $S$-valued points $P_j^{(1)}\times P_k^{(2)}$ of $Z_1\times _S Z_2$ form of full set of sections of $Z_1\times _S Z_2$. (For the definition of a full set of sections , please look at the book or read this related question ). After the proof of this statement, it is stated that if we drop the hypothesis that $Z_1$ is étale over $S$, then we only have $1. \Rightarrow 2.$ I have been trying to find a counterexample to disprove $2. \Rightarrow 1.$, but I have been unable to find one. I tried to look at cases where $S=\operatorname{Spec}(k)$ is the spectrum of a field, and $Z_1$, $Z_2$ the spectra of non-reduced $k$-algebras (so that they are not étale), such as $k[\epsilon]/(\epsilon^2)$, but I couldn't find anything satisfying. Sadly, my intuition about étaleness is not developped enough to have a feeling of what could work here. Would someone here please be able to provide an example? I thank you very much for your help.","['schemes', 'examples-counterexamples', 'algebraic-geometry']"
2827898,Recurrence for Number of Words of Length $r$ over $[n]$ with no three consecutive letters the same,"Question Let $b_{n,r}$ be the number  words of length $r$ over $[n]=\{1,2,\dotsc, n\}$ with no three consecutive letters the same. Show that
  $$
b_{n,r}=(n-1)(b_{n,r-1}+b_{n,r-2})\quad (r>2)
$$
  with initial conditions $b_{n,1}=n$ and $b_{n, 2}=n^2$ This question is from Riordan's Introduction to Combinatorial Analysis . Context It is stated as a hint in the problem to consider those sequences in the question with a given element first (call these $b_{n,r}^\star$) and a given pair of elements first (call these $b_{n,r}^{\star\star}$) and derive a system of recurrences with $b_{n,r}$. Earlier I solved the corresponding problem (of no two consecutive letters the same) with correpsonding sequences $a_{n,r}$ and $a_{n,r}^\star$ (those sequences with a given element first) and derived the recurrences
$$
\begin{align}
a_{n,r}&=na_{n,r}^\star\\
a_{n,r}^\star&=(n-1)a_{n,r-1}^\star
\end{align}
$$
which imply that $a_{n,r}=n(n-1)^{r-1}$. This question is supposed to generalize this kind of method. My Attempt With notation discussed as before I was able to deduce that
$$
\begin{align}
b_{n,r}&=nb_{n,r}^\star\\
b_{n,r}^\star&=(n^2-1)b_{n,r-1}^\star
\end{align}
$$
since for the first position there are $n$ choices. Further, Once a given element is first, there are $n^2-1$ pairs that can follow. And here is where my doubts begin. For $b_{n,r}^{\star\star}$, there is no nice analysis can be done since beginning a sequence with $00$ and $01$ need two separate analyses. Also it seems that unlike in the previous problem the derived sequence $b_{n,r}^{\star\star}$ is not independent of choice of the given pair to start with. Any help with an attempt using the context is preferred but other solutions are welcome as well.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
2827921,Why is the infinite dimensional unit ball perfectoid?,"Reading Scholze and Weinstein: ""Berkeley lectures on p-adic geometry"" in the proof of lemma 19.3.5 they use that the infinite-dimensional ball $B^\infty_{\mathbb{C}_p}$ is affinoid perfectoid. I don't understand why this holds true. In contrast, the usual one-dimensional unit ball $B_{\mathbb{C}_p}=Spa(\mathbb{C}_p<T>,O_{\mathbb{C}_p}<T>)$ is not perfectoid. Can someone explain?","['number-theory', 'p-adic-number-theory', 'algebraic-geometry']"
2827935,Parallel transport in two different polar coordinates,"I need help with the basics of parallel transport. So I will write down what I have done in the plane $\mathbb{R}^2$ with non-cartesian coordinates, mixed with some small questions. First, I use polar coordinates $(r,\theta)$ and the coordinate frame for the tangent space, which is the plane itself, given by partial derivatives $(\partial_r,\partial_\theta)$. The operator $\partial_\theta$ generates rotations, in the sense that $e^{\phi\partial_\theta}$ is a rotation by angle $\phi$, independent of $r$.
As a tangent vector, $\partial_\theta$ is not a unit vector, however, since $\partial_\theta=x\partial_y-y\partial_x$ so $||\partial_\theta||^2=x^2+y^2=r^2$. The vector $\partial_\theta$ increases in size with increasing $r$. This is in line with the fact that it generates rotation by a fixed angle, so the arc sweeped by such a rotation indeed increases linearly with $r$. [is this reasoning correct?] The translated vector $\vec{U}$ will have coordinates related to those of the initial vector $\vec{V}$ by $U^\mu=V^\mu-V^\lambda\Gamma^{\mu}_{\nu\lambda}dx^\nu$. Take a vector $\vec{V}=V^r\partial_r+V^\theta\partial_\theta$ with $V^r=V\cos\phi$ and $V^\theta=V\sin\phi/r$ for some $\phi$. If we transport it along $r$ by some amount $dr$, the new coordinates are $U^r=V^r$ and $U^\theta=V^\theta-V^\theta dr/r$. Therefore, $\Gamma^r_{ra}=0$, $\Gamma^\theta_{rr}=0$, $\Gamma^\theta_{r\theta}=1/r$. If we transport it along $\theta$ by some amount $d\theta$, the coordinates become $U^r=V^r+V^\theta rd\theta$ and $U^\theta=V^\theta-V^r d\theta/r$. Therefore, $\Gamma^r_{\theta r}=\Gamma^\theta_{\theta \theta}=0$, $\Gamma^r_{\theta \theta}=-r$, $\Gamma^\theta_{\theta r}=1/r$. Since $\Gamma^\mu_{\nu\lambda}=\Gamma^\mu_{\lambda\nu}$, there is no torsion. Now, I want to change the coordinate system in the tangent space, to test my understanding. I want to use $(\partial_r,\partial_s)$, where $s=r\theta$ is an arc coordinate. Since $\partial_s=\frac{1}{r}\partial_\theta$, the operator $\partial_s$ has unit norm (it is the versor $\hat\theta$) and also produces rotations, but by a fixed arc instead of a fixed angle. Notice that $\partial_r$ and $\partial_s$ do not commute. Now $\vec{V}=V^r\partial_r+V^s\partial_s$ with $V^r=V\cos\phi$ and $V^s=V\sin\phi$. When we transport it along $r$, the coordinates do not change at all, so $\Gamma^{\mu}_{r\lambda}=0$. When 
we transport it along $s$ by some amount $ds$ the coordinates become $U^r=V^r+V^sds/r$ and $U^s=V^s-V^r ds/r$ so $\Gamma^r_{\theta r}=\Gamma^s_{s s}=0$, $\Gamma^r_{s s}=-1/r$, $\Gamma^s_{s r}=1/r$. This time $\Gamma$ is not symmetric, as $\Gamma^{s}_{rs}=0$ but $\Gamma^{s}_{sr}=1/r$. That means this way of doing it, which can be seen as a different connection on the plane from the usual, has torsion. Is this correct? I was expecting that things would turn out the same in the end. I mean, I thought I could choose whatever coordinate system I wanted and parallel transport and torsion would be invariant notions. I mean, I DEFINED the final vetor to be identical to the original vector, so... how can there be torsion??","['connections', 'differential-geometry']"
2828012,Characteristic polynomial of the matrix with zeros on the diagonal and ones elsewhere,"Find the characteristic polynomial of the matrix with zeros on the diagonal and ones elsewhere. I've been able (I believe) to guess how it looks like (by considering matrices of small orders): $(x-n+1)(x-1)^{n-1}$. I suppose I should prove it by induction. But I don't know how to obtain the characteristic polynomial of a matrix of order $n+1$ from that of a matrix of order $n$ (i.e., how to make the inductive step). Other methods of solution are also welcome. (Is it possible to use row reduction?)","['matrices', 'linear-algebra']"
2828050,Why does the Fibonacci sequence seem to have such a rich theory?,"Pick four integers $a,b,c$ and $d$. Then we get a corresponding sequence given by $$t_{n+2} = at_{n+1} +bt_n, \; t_1 = c, \;t_2 = d.$$ From what I can tell, we seem to get an especially rich theory when we choose $a=1,b=1,c=1,d=1$, thereby obtaining the Fibonacci sequence. Just take a look at the relevant wikipedia page ; it's simply huge, and full of interesting-looking identities and connections. Question. Why is this? What is about these four numbers that gives such a rich theory for the corresponding sequence? A good answer should either: Explain that most of the results about the Fibonacci sequence have analogs that work for any $a,b,c$ and $d$ satisfying some weak conditions, so really the Fibonacci sequence isn't that special, or: Specify a very strong constraint on the relationship between $a,b,c$ and $d$ and explain why this constraint makes this particular sequence and the (few) others like it to have a very rich theory.","['recurrence-relations', 'elementary-number-theory', 'combinatorics', 'sequences-and-series', 'discrete-mathematics']"
2828093,Subgroup scheme of a constant group scheme,"I am studying the basics of group-schemes, and I found this statement in a book. Let $A$ be a finite abelian group, and let $S$ be a connected scheme. We denote by $(A)_S$ the constant group scheme associated to $A$ over $S$. Let $G/S$ be a subgroup scheme of $(A)_S/S$. Then $G$ is a constant group scheme over $S$. This looks rather basic but it is actually bugging me. Here is what I've done. Because $S$ is connected, we know that the group $A(S)$ of sections of $(A)_S$ is isomorphic to $A$. It follows, by definition, that $G(S)$ is a subgroup of $A$, which we denote $K$. I would like to show that $G$ is none other than the constant group scheme $(K)_S$. For this, I need to show that for any scheme $T$ over $S$, the group of $T$-points of $G$ over $S$, denoted $G_S(T)$, is isomorphic to the group of locally constant functions from $T$ to $K$ (equipped with the discrete topology). By definition, we already know that $G_S(T)$ is a subgroup of the group of locally constant functions from $T$ to $A$. Then, I do not know how to continue further. Namely, I would like to show that any point of $G_S(T)$ maps $T$ to $K$, and that any such locally constant function belongs to $G_S(T)$. Is the way I am proceeding correct? Is there another way of prooving the statement? Any hint or sketch of the proof would be gladly appreciated. Thank you very much.","['group-schemes', 'algebraic-geometry']"
2828100,An alternative proof for $D_2 D_1 f(x) = D_1 D_2 f(x)$ when $f \in C^2$,"In the book of Analysis On Manifolds by Munkres, at page 103, question 4-b, it is asked to show that $$D_2 D_1 f(x) = D_1 D_2 f(x)$$ for all $x \in A$, where $A \subseteq
 \mathbb{R}^2 $ is open and $f\in C^2(A)$. Proof: Let $Q$ be a rectangle in $A$, so we do know that 
$$\int_Q D_2 D_1 f(x) = \int_Q D_1 D_2 f(x),$$
hence consider 
$$\int_Q |D_2 D_1 f(x) - D_1 D_2 f(x)| = $$
Since the integral above exists, and the integrand is non-negative and the integral is zero, by a theorem, the integrand $|D_2 D_1 f(x) - D_1 D_2 f(x)|$ vanishes except on a a set of measure zero. Now we claim that, the integrand $|D_2 D_1 f(x) - D_1 D_2 f(x)|$ vanishes everywhere on $Q$. Assume that is it not the case; then $\exists a \in Q$ s.t $|D_2 D_1 f(a) - D_1 D_2 f(a)| \not = 0$, so by a lemma, there exists a neighbourhood $U$ of $a$ s.t that the integrand is still non-zero, but that contradicts with the fact that the integrand vanishes on $Q$ except on a set of measure zero, since a open ball does not have a measure zero. So, $|D_2 D_1 f(x) - D_1 D_2 f(x)| = 0$ $\forall x \in Q$ implies 
$$D_2 D_1 f(x) = D_1 D_2 f(x) \quad \forall x \in Q.$$ Question: Is there any flaw in my proof ? or is there anything that is not clear ? or do you have any suggestion ?","['real-analysis', 'integration', 'analysis', 'riemann-integration']"
2828115,An integral involving Bessel function of the first kind of the Sonine-Gegenbauer sort,"Do you know how to do this integral?
$$\int\limits_{0}^{2\pi}\mathrm{d}\phi\,\frac{J_2\left(\sqrt{a^2+b^2-2ab\cos(\phi)}\right)}{a^2+b^2-2ab\cos(\phi)}\,,$$
where $J_2$ is the Bessel function of the first kind of second order, and a and b are two positive constants. I have tried various different tricks: using integral representation of the Bessel function, series expansion of the Bessel function, or converting the integral into complex integral over the unit circle, but I couldn't simplify the results I got afterward. Thanks.","['bessel-functions', 'real-analysis', 'complex-analysis', 'integration', 'definite-integrals']"
2828121,An urn with three kinds of balls... and a weird constraint!,"Consider an urn which contains three different kinds of balls A , B and C . We suppose that there is at least one ball of each kind in the urn. We define the event $A$ as ""to get, in $n$ independent trials, at least one ball of kind A "" and, similarly, we define the events $B$ and $C$. In each trial, we extract only one ball, and then we put it back in the urn. These three events are clearly linked to each other through the constraint $P(A\cup B\cup C)=1$. In which conditions does the constraint $P(A)=P(\overline{B})$ hold? This constraint can be formulated as follows: ""The chance to get at least one ball of kind A , in $n$ trials, is equal to the chance not to get any ball of kind B "". This question is a special case of the problem treated in this post A weird problem of probability! and, more in general, in this other post A problem of conditional probability .","['combinatorics', 'probability', 'discrete-mathematics', 'elementary-number-theory']"
2828167,Two sequences which have an average tend to zero,"Let $(a_n)_{n\geq 1}, (b_n)_{n\geq 1}$ be two sequence of positive real numbers such that $$ \lim_{n\to +\infty}\frac{a_1+a_2+\cdots+a_n}{n}=\lim_{n\to +\infty}\frac{b_1+b_2+\cdots+b_n}{n}=0.$$ Conjecture. For all $\epsilon>0$ , there are infinitely many values of indices $k$ such that $a_k<\epsilon$ and $b_k<\epsilon.$ I think that this is true but I can not prove it now. In the special case where $a_n = b_n$ , that is, there is only one sequence, then one can argue easily using a contradiction argument. In the general case, the hard part is to show that the same set of indices are shared by both sequences $(a_n)_{n\geq 1}, (b_n)_{n\geq 1}$ .","['conjectures', 'real-analysis', 'sequences-and-series', 'limits']"
2828191,Holomorphic function on unit disk,Suppose $f$ is a holomorphic function on the open unit disk $\mathbb{D}$ with $f(0)=0$ and  $| f(z) + zf^{'}(z)| <1$ for all $z \in \mathbb{D}$. I have to show that $|f(z)| \leq \frac{|z|}{2}$ for all $z\in \mathbb{D}$. I have tried to apply Schwarz Lemma but failed to obtain the inequality.,['complex-analysis']
2828202,Integrating a differential form,"I am currently going through Introduction to Smooth manifolds by John Lee and am a bit confused with the integration of differential forms. So given a smooth k-form $\omega = f dx^1 \wedge ... \wedge dx^k $ on some integral domain $D \subset \mathbb{R}^k$ we can integrate $\omega$ over $D$, which is given in the book by
$$\int_D f dx^1 \wedge ... \wedge dx^k = \int_D f dx^1 ... dx^k \, \, \, \,  \, \, (\star)$$ What I am confused about is that if we interchange say the $dx^1$ and the $dx^2$ in both sides of the equation we get a negative side arising on the left as $dx^1 \wedge dx^2 = - dx^2 \wedge dx^1$, whilst on the right hand side there will be no sign factor as $dx^1 dx^2 =dx^2 dx^1$. Is the reason that the above is not a contradiction a result of the fact we have fixed an orientation $(x^1, ..., x^k)$, and so with this orientation  we get that $(\star)$ holds but 
$$\int_D f dx^2 \wedge dx^1 \wedge ... \wedge dx^k \neq \int_D f dx^2 dx^1 dx^3 ... dx^k?$$ That is, we can only integrate once we have ordered the differential form into its chosen orientation?","['integration', 'differential-forms']"
2828243,Expressing the codifferential of a $p$-form in terms of covariant derivatives,"Let $\alpha$ be a $p$-form on an $n$-manifold $M$. I read somewhere that the codifferential $\delta\alpha$ of $\alpha$ can be expressed in terms of covariant derivatives as follow:
\begin{align}
(\delta\alpha)_{i_1\cdots i_{p-1}}=-g^{jk}\nabla_j\alpha_{ki_1\cdots i_{p-1}}
\end{align}
I would like to know how to prove this. Since only the definition of $\delta$: 
\begin{align}
\delta\alpha=(-1)^{np+n+1}*d*\alpha
\end{align}
(where $*$ is the Hodge star operator) was given to me (I'm aware that $\delta$ is usually defined as the adjoint of $d$, but this is the definition I was given; nevertheless, both can be assumed here), and the computation of $*\alpha$ usually involves the factor $\sqrt{\det g_{ij}}$, I'm not sure how to proceed to obtain the expression above in terms of only the covariant derivatives. Thanks in advance for any comment, hint, and answer.","['differential-forms', 'riemannian-geometry', 'differential-geometry']"
2828249,How to prove that $-x$ is not equal to $x$ just because they yield the same result when in $x^2$,"I know how incredibly stupid this sounds, but bear with me. Let's take any random $x$, say $3$, and any random $-x$, say $-3$. Let's plug it into $x^2$. They will both give the same result! I know this conclusion can't be right, that because of the above, $3 = -3$. But how do we logically prove it wrong? I want to know what is logically flawed about the argument above? I know from the instance above, we can draw 2 conclusions: $x$ is really equals to $-x$. Just because a function gives the same output for 2 separate numbers doesn't mean the 2 numbers are the same. Can you please explain why conclusion 2 is the right one to come to? As an extension, can you please also disprove conclusion 1 above? Why isn't the fact that both provide identical outputs when being plugged into the same function a legitimate reason to say that both inputs are the same? And when is this line of reasoning legitimate; when is it legitimate to say that because both inputs provide the same output, they are the same? Can you explain all this as simply as possible? I'm still a beginner, and will struggle to understand any rigorous mathematical notation without explanation.","['fake-proofs', 'functions', 'proof-explanation']"
2828261,Is there a name for the function $1 / (1 + x)$?,"Does the function $$f(x) = \frac{1}{1  + x}$$ have a recognizable name? For example a related function with a recognizable name is the logistic function , defined by: $$l(x) = \frac{1}{1  + e^{-x}}$$ Note: By the way I am quite happy with functions without name.... except when I have to write code for a program, then I wish for nice names.","['terminology', 'notation', 'functions']"
2828328,"Help with the proof: union $A\cup B$ of two countably infinite sets $A,B$ is countably infinite","There are many questions and answers on this proof here, but I would still appreciate the help with the proof that I tried on my own. Disjoined sets It is enough to show what happens with the countability of a union of two disjoint sets, because if they have an intersection, the union can be reformmulated as a union of the intersection and original sets with the intersection removed. If $A \cap B \ne \emptyset$, then if $C = A \cap B, A'=A \setminus C, B' = B  \setminus C$, we get $A \cup B = A' \cup B' \cup C$ and $A' \cap B' \cap C = \emptyset$, so it is enough to show that $A' \cup B'$ is countably infinite, as $(A' \cup B')$ and $C$ are also disjoint. Therefore, it makes no difference if $A \cap B = \emptyset$ or not, so it is assumed that $A \cap B = \emptyset$. Proof If $A$ is countably infinite, $\exists$ a bijection $f : A \to \mathbb{N}$. A bijection $g : A \to \mathbb{N}^{2n-1}$ is introduced $g(i) = 2 f(a_i) - 1, a_i \in A$. (1) If $B$ is countably infinite, $\exists$ a bijection $h : B \to \mathbb{N}$. A bijection $j : B \to \mathbb{N}^{2n}$ is introduced $j(i) = 2 f(b_i), b_i \in B$. (2) The elements of $A \cup B$ can be arbitrarily ordered, so they can be ordered in pairs, taking one element of A, then one from B, like this: $A \cup B = \{a_{1_1}, b_{2_1}, a_{3_2}, b_{4_2}, a_{5_3}, b_{6_3}, ...,
 a_{2n-1_n}, b_{2n_n} \}=\{a_{g(i)}, b_{j(i)}:  i \in \mathbb{N}, a_{g(i)} \in A, b_{g(i)} \in B \} $ Because $\forall i \in \mathbb{N}$ the pair $(2i-1, 2i)$ is unique. From $(g(i), j(i))$ a unique $i$ can be computed from either $g$ or $j$, giving $a_i \in A$ and $b_i \in B$. Therefore,  $i \to (g(i), j(i))$ is bijective so $A \cup B$ is also countably infinite. Is this OK? Edit : Jerry asked for the finite + countably infinite set combination. Say that $A$ is finite and $B$ is countably infinite. Then $|A| = C_A \in \mathbb{N}$. Their union can be written by inserting $A$ at the beginning of $B$ (because A is finite): $A \cup B = \{a_1, a_2, a_3, ... a_{C_A}, b_{1+C_A}, b_{2+C_A}, b_{3+C_A}, ... \} = \{b_i\}$ and $\{1,2,3,...,C_A,C_A+1,C_A+2,...\}=\mathbb{N}$, so $A\cup B$ is countably infinite. Is this OK as well?",['elementary-set-theory']
2828331,smooth projective variety with arbitary large degree irreducible divisors,"Does every  projective complex two dimensinal manifold have divisors $D$ of arbitary large degree (integral of chern class of $[D]$  with a fixed Kahler form) which cannot be written as $D= dD_1$ where $D_1$  is  a divisor of lower
degree? Example: Take $\mathbb{CP}^{2}$ and divisors which are zero sets of homogenous polynomials
 $(x_1)^{d} + (x_2)^{d} + (x_3)^{d}$ where $d$ varies up to infinity. If so a reference","['complex-geometry', 'differential-geometry', 'algebraic-geometry']"
2828352,"For any two finite sets $X$ and $Y$, prove that $\#(Y^X)=\#(Y)^{\#(X)}$ by induction","I came across the following exercise while self-studying Terrence Tao's book Analysis I : Exercise 3.6.4 Let $X$ and $Y$ be finite sets. Then $Y^X$, the set of functions with domain $X$ and range $Y$, is finite and $\#(Y^X)=\#(Y)^{\#(X)}$. Note: This question has been asked twice before ( here and here ) on this site. My attempt was a proof by induction, which was not present in either of these posts. So, I ask merely that any answers correct my inductive step, or any other errors, and not give alternate solutions. With that in mind, here is my go: Let $X$ and $Y$ be finite sets with bijections $f:X\to\Bbb N_n$ and $g:Y\to\Bbb N_m$ (where $\Bbb N_n$ denotes the set of natural numbers less than $n$, where I am using the convention that the natural numbers start at zero). In this proof, we fix $m$ and induct on $n$. If it happens that $n=0$, then $\#(Y^X)=1$, as there is a unique function $\emptyset\to Y$ (uniqueness is a vacuous truth, in this case). Since $m^0=1$, the claim is trivial in this case, and we may assume that $n>0$. If it happens that $n=1$, then $Y^X$ is the set of all functions $\{*\}\to Y$, as $X$ is a singleton set. Notice that if the image of $\{*\}$ under any of these maps is greater than one, the image of $x$ cannot be unique, which contradicts our assumption that $Y^X$ consists of only functions. Therefore, the image of any one of these maps is a singleton, and it suffices to count only the elemtnts of $Y$, as $\{*\}$ remains fixed: $$\#(Y^X)=\#\left(\bigcup_{y\in Y}\{y\}\right)=\#(Y)^{\#\{*\}}.$$
Furthermore, suppose our claim holds for some $n\in\Bbb N$. Then, if $x'\notin X$ and $Z=X\cup\{x'\}$, $$\begin{align}Y^Z :&=\{f\mid \operatorname{dom}(f)=Z\land \operatorname{ran}(f)=Y\} \\ &=\{(x,y)\in f\mid x\in X\cup\{x'\}\land y\in Y\} \\ &= (?) \\ &=Y^X\times Y^{\{x'\}},\end{align}$$
and hence, $$\#(Y^Z)=m^n\times m=m^{n+1}.$$
This closes induction. What am I missing in the inductive step? What should go in place of $(?)$ in the third line above? It seems only plausible that it works out to be $Y^X\times Y^{\{x'\}}$, otherwise I am not sure how to apply the inductive hypothesis. Also, one can arrive at a contradiction of the original claim, considering $$\begin{align}\#(Y^Z)&=\#\left(\{f\mid \operatorname{dom}(f)=X\land \operatorname{ran}(f)=Y\}\cup \{f\mid \operatorname{dom}(f)=\{x’\}\land \operatorname{ran}(f)=Y\}\right) \\ &= m^n+m\neq m^{n+1} \end{align}.$$
Why is this incorrect? Thanks in advance. Update: Combinatorially speaking, the carnality of $Y^Z$ should be $\#(Y^X)\times\#(Y)$, as one can pair $\{x'\}$ with each element of $Y$, which as we mentioned above, should yield the cardinality of $Y$. But, this is in contradiction with what I have written before, and I am still interested in why it is false.","['induction', 'elementary-set-theory', 'proof-verification']"
2828396,Representation of a natrual number by the sum of geometric progression with minimal scale factor,"This question is inspired by a leetcode question. Let's say we have a number $x$ and we want to represent it by a geometric progression. The easiest progression is $1+(x-1)$. But how to find the series with the minimal scaling factor? For example $13 = 1 + 12 = 1 + 3 + 9$ and $3$ is the right solution. I tried somehow to work with the equation
$$\dfrac{r^n - 1}{r-1} = x,$$
and then I came to
$$r(x-r^{n-1}) = x-1,$$
which means that both $r$ and the other part has to be dividers of $x-1$. I know how to make a numeric solution with a python script but how would a mathematician tackle this issue? Is there any formula which can help? I tried to google “representation of number by geometric progression” but did not find anything.",['number-theory']
2828404,How to prove that $\sum_{n=1}^\infty\frac{\sin n\cdot\sin n^2}{n}$ converges? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question The series is $$\sum_{n=1}^\infty\frac{\sin n\cdot\sin n^2}{n}$$
It seems to use Dirichlet's test, but I cannot prove $\sum\sin n\cdot\sin n^2$ is bounded.
This question may help -- Convergence of $\sum \limits_{n=1}^{\infty}\sin(n^k)/n$","['sequences-and-series', 'convergence-divergence']"
2828412,How to prove Cramér's theorem?,"Cramér's theorem: Let $X,Y$ two independent random variables such that $X+Y$ is normal distributed, then $X$ and $Y$ are normal distributed. I get the the original paper: Harald Cramér. ""Über eine Eigenschaft der normalen Verteilungsfunktion"". Mathematische Zeitschrift, 41 (1): 405–414, 1936. But I cannot understand German. Is there an English version? Thanks a lot.","['probability-theory', 'normal-distribution']"
2828422,Prove by induction that $n^4-4n^2$ is divisible by 3 for all integers $n\geq1$.,"For the induction case, we should show that $(n+1)^4-4(n+1)^2$ is also divisible by 3 assuming that 3 divides $n^4-4n^2$. So,
$$ \begin{align} (n+1)^4-4(n+1)&=(n^4+4n^3+6n^2+4n+1)-4(n^2+2n+1)
\\ &=n^4+4n^3+2n^2-4n-3
\\ &=n^4+2n^2+(-6n^2+6n^2)+4n^3-4n-3
\\ &=(n^4-4n^2) + (4n^3+6n^2-4n)-3
\end{align}$$ Now $(n^4-4n^2)$ is divisible by 3, and $-3$ is divisible by 3. Now I am stuck on what to do to the remaining expression. So, how to show that $4n^3+6n^2-4n$ should be divisible by 3? Or is there a better way to prove the statement in the title? Thank you!","['divisibility', 'induction', 'elementary-number-theory', 'integers', 'discrete-mathematics']"
2828475,Definition of Willmore energy in higher dimensions,"The Willmore energy is a well-known and studied quantity typically defined for surfaces immersed in a 3-dimensional manifold as the integral of the square of the mean curvature (see https://en.wikipedia.org/wiki/Willmore_energy ). I was wondering if there is any standard notion of Willmore energy  for a $n$-dimensional submanifold of a $(n+1)$-dimensional Riemannian manifold. I couldn't find much material online, so I thought that this was the right place where to ask. Any help will be very appreciated! Thanks a lot!","['riemannian-geometry', 'differential-geometry']"
2828478,Which Analysis books did you learn from and how many years/textbooks did it take to become a master? [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question It seems that nearly every graduate program in the U.S. requires an extensive education in Analysis clearly demonstrating its importance not only as a subfield itself, but as a foundation for other areas of abstract mathematics. I am interested in doing research in Analysis as a graduate student. I am curious how long it takes a student to get to the level of serious research in the subject. As subjective as this question is, I am curious to know from which texts some of you learned Analysis i.e. from Rudin (the family), Royden, Pugh, Bartle, Tao, Torchinsky, etc. How many classes did it take to be near or at the level of research in the area? Which texts helped you get there? Also, do you know of any summer programs in Analysis at your institution undergraduates can enroll in? Thanks in advance!","['reference-request', 'real-analysis', 'book-recommendation']"
2828511,Prove the number of symmetric relation is $2^{\frac{n^2+n}{2}}$,"If the number of set $A$ is given. let $n(A)=n$.
Prove the number of symmetric relation from $A$ to $A$   is $2^{\frac{n^2+n}{2}}$ We know that number of  relations from $A$ to $A$ is $2^{n^2}$ this is obtained by, number of subsets of $A$x$A$ $=2^{n^2}$ every subsets of $A$x$A$  is the relation from $A$ to $A$ like this is any proof is there?","['relations', 'elementary-set-theory']"
2828525,Prove the equation $AXB-BXA=0$ has at least one invertible solution in $M_n(F)$.,"Let $F$ be a field and $A,B \in M_n(F)$ (i.e., $A$ and $B$ are $n\times n$ matrices with entries in $F$ ).
If there exist a linear combination of $A$ and $B$ which is invertible, then
prove the equation $$AXB-BXA=0$$ has at least one invertible solution in $M_n(F)$ . Attempt. If $A$ is invertible, then from $AXB=BXA$ , we get $XBA^{-1}=A^{-1}BX$ .  Then, we see that $X=A^{-1}$ is a solution.  Similarly, if $B$ is invertible, then $X=B^{-1}$ is a solution.","['matrices', 'matrix-equations', 'linear-algebra', 'inverse']"
2828528,Linear subspace of codimension one in infinite dimensional Banach space,"Let $Y$ be a linear subspace of codimension 1 in an infinite dimensional Banach space $X$ i.e. $\dim (X/Y)=1$. Then how to prove that $X\setminus Y$ is path connected if and only if $Y$ is dense in $X$ ? I am unable to prove either direction. If $Y$ is not dense and of codimension 1, then we can conclude $Y$ is closed subset. Also, $Y$ is not dense in $X$ iff $\exists y \in X \setminus Y$ such that $B(y,1) \subseteq X \setminus Y$. I am unable to see how this could be equivalent with $X \setminus Y$ being not path connected. Please help.","['functional-analysis', 'general-topology', 'banach-spaces']"
2828552,An a priori energy estimate for non-homogeneous wave equation,"Let's consider the following wave equation in $B\times (0,\infty)$ in which $B$ is the open unit ball in $\Bbb R^n$:
$$
\begin{align}
u_{tt} - \Delta u = f,&\quad (x,t)\in B\times (0,\infty)\\
u = u_0,\,\, u_t=u_1,&\quad (x,t)\in B\times\{t=0\}\\
u = 0,&\quad (x,t)\in \partial B\times (0,\infty)
\end{align}
$$ Suppose $u_0$ is $C^2$, $u_1$ is $C^1$, $f$ is continuous, and $u$ is a $C^2$ solution. Prove the following energy estimate:
$$E(t)\le 2E(0) + 2(\int_0^t \|f(s,\cdot)\|_{L^2}ds)^2,\quad t\ge 0.$$
in which $E(t) = \|u_t\|^2_{L^2}+\|\nabla u\|_{L^2}^2$. The $L^2$-norm is taken over $B$. Usually we will consider $E'(t)$, upper bound it and then integrate. But since the RHS has $2E(0)$ instead of $E(t)$, this doesn't look very hopeful. Any thoughts?","['real-analysis', 'partial-differential-equations', 'functional-analysis', 'wave-equation', 'ordinary-differential-equations']"
2828574,Nilpotent groups and 2-generated subgroups,Do you know an example of a $2$-locally nipotent group $G$ which is not locally nilpotent ? $2$-locally nilpotent: every subgroup which is generated by $2$ elements is nilpotent. locally nilpotent: every finitely generated subgroup is nilpotent.,"['abstract-algebra', 'infinite-groups', 'examples-counterexamples', 'group-theory']"
