question_id,title,body,tags
2732860,A curious little conjecture about 3D cubes and possible generalizations,"Let $(C) \subset \mathbb{R}^3$ be a 3D solid cube with center $O = (0,0,0)$ and side length $=1$. Let $(H)$ be the projection of $(C)$ on the plane $z=0$, and $h$ be the smallest positive number satisfying $(C)$ is between two planes $z=h$ and $z=-h$ (points of $(C)$ are allowed to be on those planes). I notice that, for several cases of $(C)$, the area of $(H)$ equals $2h$ . So I propose a little conjecture that, indeed, $\text{Area}(H)=2h \; \; \forall (C)$. I haven't proved this conjecture. Actually, I think that, if it's true, somebody definitely did notice and prove this fact. So I only post it here to learn more. Also, I want to propose some generalizations of this fact. Namely: What are the necessary and sufficient conditions of $(C)$ so this fact still holds in $\mathbb{R}^3$? How to rightfully generalize this fact into $n$-cubes in $\mathbb{R}^n$ ($n > 3$)? What are the necessary and sufficient conditions of $(C)$ so this fact still holds in $\mathbb{R}^n$? Thanks in advance.","['analytic-geometry', 'euclidean-geometry', 'geometry']"
2732867,Is the reverse of a shelling of a polytope always a shelling?,"A shelling of a polytope $P$ is a linear ordering of facets $F_1, \dots, F_s$ such that for $j>1$, $F_j \cap (\cup_{i<j}F_i)=G_1 \cup \dots \cup G_s$ is the beginning segment of a shelling $G_1, \dots, G_s, \dots, G_r$ of $F_j$. In particular, $F_j \cap (\cup_{i<j}F_i)$ is pure complex, i.e. every face is contained in a facet. In Ziegler's book lectures on polytopes, lemma 8.10 states that reverse shelling of a polytope is a shelling. I am not completely convinced by his proof. In the proof, he argues that $F_j \cap (\cup_{i>j} F_i)$ is $G_r \cup \dots \cup G_{s+1}$. I think it is true that $G_r \cup \dots \cup G_{s+1} \subset F_j \cap (\cup_{i>j} F_i)$, but I think it might happen that $F_j \cap (\cup_{i>j} F_i)$ contains some smaller face that is not contained in any of the $G_i$. Here is a screenshot of Ziegler's proof. Thanks in advance.","['combinatorics', 'polytopes']"
2732868,"Writing integers as a product of as few elements of $\{\frac21, \frac32, \frac43, \frac54, \ldots\}$ as possible","This question is inspired by question 2 of the 2018 European Girls' Mathematical Olympiad . Also posted on mathoverflow . Any integer $x \ge 2$ can be written as a product of (not necessarily distinct) elements of the set $A = \{\frac21, \frac32, \frac43, \frac54, \ldots\}$ , as can be seen from a simple telescoping argument. Let $f(x)$ be the minimum number of elements of $A$ required. For example, $f(11)=5$ because $11 = \frac{33}{32} \cdot \frac{4}{3}  \cdot \frac{2}{1} \cdot \frac{2}{1} \cdot \frac{2}{1}$ (or, alternatively, $11 = \frac{11}{10} \cdot \frac{5}{4} \cdot \frac21 \cdot \frac21 \cdot \frac21$ ), but $11$ cannot be written as the product of $4$ or less elements of $A$ . In general, it seems difficult to compute $f(x)$ directly. Clearly we have $f(xy) \le f(x) + f(y)$ for any $x,y \ge 2$ . The EGMO question asks to show that this inequality is strict infinitely often (an example is $x=5$ , $y=13$ ). Here we ask: For integral $x \ge 2$ , let $f(x)$ be the smallest $k$ so that $x$ can be written as product of $k$ elements of $\{\frac21, \frac32, \frac43, \ldots\}$ . Is it true that $f(xy) =f(x) + f(y) - O(1)$ ? In other words, is the difference between $f(x)+f(y)$ and $f(xy)$ bounded? Some observations: $f(x) \ge \log_2(x)$ as $A$ has no elements larger than $2$ ; if it were true that $f(x) = \log_2(x) + O(1)$ , this would imply that the question above has a positive answer.","['analytic-number-theory', 'number-theory', 'telescopic-series', 'contest-math', 'elementary-number-theory']"
2732891,Is there any other function satisfying the system of equations involving integration?,"I've encountered this problem: Let $f(x)$ be a continuously differentiable (real) function on $[0,1]$ satisfying these equations: $$f(1)=0$$ $$\int_0^1 [f'(x)]^2 dx = 7$$ $$\int_0^1 x^2f(x) dx = \frac{1}{3}$$. Compute $\int_0^1f(x) dx$. I've managed to find a $f(x) = \frac{7}{4}(1-x^4)$ in a few trials. However, I cannot find any other solution (or at least any other elementary solution), which seems weird to me because these equations are not enough to uniquely define a function. Moreover, assume that there are some other solutions, how can the problem be so sure that $\int_0^1f(x) dx$ are all the same among those solutions? Is there any neat way to solve the problem without finding a solution? I highly doubt these two questions. I think the problem is wrong . But I'm not sure, so I post it here to discuss. Thanks in advance.","['derivatives', 'real-analysis', 'calculus', 'integration', 'definite-integrals']"
2732968,Prove that $E(e^{X}) < \frac{1}{2}(e^{-K} + e^{K})$ if $E(X)=0$ and $|X|<K$ almost surely,"I saw this question on a friend's problem set (I mean it, not MY homework) and kept thinking about it, although without success. Let $X$ be a a random variable such that $ -K < X < K$ for some $ K
>0$. Also, let $E(X) = 0$. Now, prove that $E(e^{X}) < \frac{1}{2}(e^{-K} + e^{K})$. I'm trying to use Jensen's inequality and so on but I made no progress. Also I tried fooling around with some smart squares but couldn't solve it this way either. Any ideas?","['probability-theory', 'inequality', 'expectation', 'jensen-inequality']"
2733025,Vector field of a flow's cotangent lift is Hamiltonian,"Let $M$ be a smooth manifold, and $X\in \mathfrak{X}(M)$ be a complete vector field. Then for all $t \in \Bbb R$ we have the flow $\Phi_{t,X}\colon M \to M$ of $X$. We can consider the cotangent lift $\widehat{\Phi_{t,X}}\colon T^*M \to T^*M$ of each flow time$^1$. By properties of the flow, we have that $\widehat{\Phi_{t,X}} = \Phi_{t,\hat{X}}$ for some complete vector field $\hat{X} \in\mathfrak{X}(T^*M)$. We consider in $T^*M$ the standard symplectic strutcure $\omega_{\rm can}$. I want to check that $\hat{X}$ is Hamiltonian, and find a Hamiltonian function $H\colon T^*M \to \Bbb R$ (i.e., such that $\omega_{\rm can}(\hat{X},\cdot) = {\rm d}H$). Since cotangent lifts are symplectomorphisms, I know that $\hat{X}$ is symplectic, and so locally Hamiltonian. Also: $$\pi\circ \Phi_{t,\hat{X}} = \Phi_{t,X}\circ \pi \implies {\rm d}\pi_\xi(\hat{X}_\xi) = X_{\pi(\xi)}, \qquad \mbox{for all }\xi \in T^*M.$$The only map $T^*M \to \Bbb R$ I can immediately think of is $\alpha(\hat{X})$, where $\alpha$ is the tautological $1$-form, but this doesn't seem to work and I don't know what else to do here. Maybe treat the situation with brute force in coordinates? Please help. If $f\colon M\to N$ is a diffeomorphism, the cotangent lift is $\widehat{f}\colon T^*M \to T^*N$, given by $\widehat{f}(\xi) = \xi \circ ({\rm d}f_{\pi(\xi)})^{-1}$, where $\pi\colon T^*M \to M$ is the canonical projection.","['symplectic-geometry', 'differential-forms', 'differential-geometry']"
2733048,example of two linearly independent functions to have a zero Wronskian??,"What is an example of two linearly independent functions to have a
  zero Wronskian?? This question is  in reference to http://tutorial.math.lamar.edu/Classes/DE/Wronskian.aspx","['real-analysis', 'ordinary-differential-equations', 'linear-algebra']"
2733069,Countable subset under irrational,"Argument: There exists a denumerable subset of the set of irrational numbers My argument is that if you add a rational number to an irrational number it will still be an irrational number but you can count to it ex: 1/n+sqrt(2) 
is this a vaild example?",['elementary-set-theory']
2733126,Mean curvature is the divergence of the normal,"As a definition, I was told that for a surface in 3D, $ 2H = -\nabla \cdot \nu$ where $H$ is the mean curvature and $\nu$ is the normal unit vector. In some results that I am studying, the factor 2 always disappears... Is this normal ? can we ignore the factor 2 and consider the definition ""up to a constant"" ?","['differential-geometry', 'definition']"
2733135,Finding the matrix for a matrix exponential,"I'm trying to find the matrix $A$ for which $$e^{tA}=\begin{pmatrix}
\frac{1}{2}(e^t+e^{-t}) & 0 & \frac{1}{2}(e^t-e^{-t}) \\
0 & e^t & 0 \\
\frac{1}{2}(e^t-e^{-t}) & 0 & \frac{1}{2}(e^t+e^{-t})
\end{pmatrix}$$ I know that $e^{tA}=\Psi(t)\cdot[\Psi(0)]^{-1}$, so $e^{tA}\cdot\Psi(0)=\Psi(t)$. Where $\Psi(t)=(\eta^{(1)}e^{\lambda_1x},\eta^{(2)}e^{\lambda_2x},\eta^{(3)}e^{\lambda_3x})$, with $\lambda_i$ the $i$-th eigenvalue with corresponding eigenvector $\eta^{(i)}$. However, this didn't really get me anywhere. Does anyone know how to do this?","['matrices', 'matrix-exponential', 'ordinary-differential-equations']"
2733212,"Find $\lim_{x\to\infty}\left( \sqrt{(x+a)(x+b)}-x \right)$ where $a,b \in \mathbb{R}$ [duplicate]","This question already has answers here : Values of the limit $\lim\limits_{x\to+\infty}\left(\sqrt{(x+a)(x+b)}-x\right)$ (7 answers) Closed 4 years ago . Find the value of the following (using an epsilon proof or basic limit properties (no L'hospital)): $$\lim_{x\to\infty}\left( \sqrt{(x+a)(x+b)}-x \right)\forall a,b\in\mathbb{R}$$ I've tried rewriting it in several ways, but I don't seem to bet getting very far; I always end up with something in indeterminate form.  How can you prove the value of this?  Any hints?","['real-analysis', 'limits-without-lhopital', 'calculus', 'limits']"
2733255,Defition of the Degree of a line bundle,"I was reading this notes , and there is some things that is unclear to me about the defition of the degree of a line bundle, page 16. First of all, here is the construction: Let $L$ be a complex line bundle on Riemann surface $C$. Consider a general section $\sigma : C \rightarrow L$. We can produce such a section by giving it locally and then gluing it together using a partition of unity. Locally, the line bundle $L$ is trivial, so it looks like $\mathbb{C}\times \Delta\rightarrow \Delta$, where $\Delta$ is the open unit disk. In this local picture, $\sigma$ is just a map $\Delta \rightarrow \mathbb{C}$.
  By perturbing $\sigma$ we can insist that it is transverse to the zero section.Then locally, the inverse image of $0 \in \mathbb{C}$ under the map $\sigma: \Delta \rightarrow \mathbb{C}$ is a finite number of points. Each point $p \in \mathbb{C}$ where $\sigma$ intersects the zero section is called a zero of $\sigma$. Around each such point p the section $\sigma$ is a map $\sigma: \Delta \rightarrow \mathbb{C}$ where $p=0 \in \Delta$ and $\sigma(0)=0$. The differential $d\sigma:T_0\Delta \rightarrow T_0\mathbb{C}$ is nonsingular two-by-two matrix. Notice that there was an ambiguity since the map $\sigma: \Delta \rightarrow \mathbb{C}$ is defined up to post-multiplication by $\mathbb{C}^{*}$. Fortunately, multiplying by a complex number does not change the sing of $\mathrm{det}\mbox{ }d\sigma$. Definition: The degree of $L$ is $\mathrm{deg}(L)=\sum_{p}\mathrm{sgn}(p) \in \mathbb{Z}$, where the sum is over all points where a transverse section $\sigma$ is zero. My questions: Q1) In the construction is used that $\sigma$ is of a specific type, satisfying that in the trivialization, as a function of $\Delta \rightarrow \mathbb{C}$, $\sigma$ is differentiable. But, in the definition he doesn't mention it. Why is it not important? Q2) Why the definition doesn't depend on the section I take? Here's another question that is not about the definition, but an application of it: Q3) How can I show using this definition that the degree of the tangent bundle on a Riemann surface of genus $g$ is $2-2g$.","['vector-bundles', 'riemann-surfaces', 'smooth-manifolds', 'differential-geometry']"
2733365,Existence of a coordinate system,"How can we formally show that a coordinate system $(x,y)$ exists or does not exist? For instance for some given coordinate system $(r,\phi,\theta)$ defined on the manifold $M =(1,\infty)\times\mathbb{S}^2$, does there exist a coordinate system $(s,\phi,\theta)$ for some tensor:
 $$h=ds⊗ds+g(s)^2(d\phi⊗d\phi+\cos^2\phi \space d\theta⊗d\theta)$$ where $g(s)$ is a positive smooth function. NOTE: There is some $2$-tensor given on $M$ by:
$$f=\frac{r}{r^3+r-2}dr \otimes dr +r^2d\phi \otimes d\phi + r^2\cos^2\phi \space d\theta\otimes d\theta$$ This is a problem I am stuck with for days already, just do not know the formal way of proving such coordinate systems exist, given some manifold $M.$ I would appreciate the help.",['differential-geometry']
2733400,"How to calculate $\int_0^\infty \frac{1}{(1+x^2)(1+x^{2018})}\,dx$?","$$\int_0^\infty \frac{1}{(1+x^2)(1+x^{2018})}\,dx$$ My Calculus professor asked a challenge problem to one of my friends and asked her to evaluate it. I tried partial fractions to no avail and the trig substitution $x = \tan\theta$, but that leaves me with $$\int_0^{\pi/2} \frac{1}{(1+\tan^{2018}\theta)}\,d\theta$$ which I do not know how to evaluate. Any help would be greatly appreciated!",['integration']
2733478,How many curvatures determine a manifold's embedding?,"It is easy to see that curvature determines a plane curve up to rigid motions. For space curves, you need two quantities: the curvature and the torsion. The latter can be observed from the Frenet-Serret equations
$$\left[\begin{array}{c}T'\\N'\\B'\end{array}\right] = \left[\omega\right]_\times \left[\begin{array}{c}T\\N\\B\end{array}\right]$$
where $[\omega]_{\times}\in\mathfrak{so}(3)$ has three degrees of freedom; however, one of these is redundant (my intuition for this is that the twist of the normal and binormal about the tangent is irrelevant to describing the shape of the curve.) For a surface in 3D, the second fundamental form is enough to determine the surface embedding up to rigid motions; but surely not all three distinct entries of the second fundamental form are needed? Can one recover the embedding (again, up to rigid motions) knowing only the mean curvature, for instance? In general, given a $d$-dimensional Riemannian manifold, how many ""curvatures"" must be known at each point on the manifold to uniquely determine an embedding in $\mathbb{R}^n$ up to rigid motions (isometries of the ambient space)?","['riemannian-geometry', 'differential-geometry']"
2733489,Conjugation with Pauli matrices,"Let $\{\sigma_j\}_{j=0}^3$ denote the Pauli basis of Hermitian matrices on $\mathbb C^2$ with $\sigma_0 := I$. Is it true that $$\frac{1}{4}\sum_{j=0}^3 \sigma_j A \sigma_j = \frac{\text{tr}(A)}{2}I$$ for any positive definite $2x2$ matrix $A$? If so, how would I go about showing this? I haven't been able to do so with the known properties of the Pauli matrices.","['matrices', 'linear-algebra', 'vector-spaces']"
2733508,"$X_n \Rightarrow X$ and $Y_n \Rightarrow c$, c a constant, implies $X_nY_n \Rightarrow Xc$","I am trying to show the following:
$X_n \Rightarrow X$ and $Y_n \Rightarrow c$, c a constant, implies $X_nY_n \Rightarrow Xc$. Using the fact that $X_n \Rightarrow X$ and $Y_n \Rightarrow c$, c a constant, implies $X_n + Y_n \Rightarrow X+ c$ (which I know how to prove) I found in another post ( Proving Slutsky's theorem ) a way to prove what I want to prove : ""We have 
$$X_nY_n=X_n(Y_n-c)+cX_n;$$
defining $Z_n:=cX_n$ and $Z'_n:=X_n(Y_n-c)$, we reach the wanted conclusion provided that we manage to show that $X_n(Y_n-c)\to 0$ in probability. But for a fixed $\varepsilon$, and each $R$
$$\mathbb P\{| X_n(Y_n-c)|\gt \varepsilon\}\leqslant\mathbb P\{|X_n|\gt R\}+\mathbb P\{|Y_n-c|\gt \varepsilon/R \}.$$
Choosing $R$ as a limiting point of the distribution function of $|X|$, we obtain from the convergence of $Y_n$ to $c$ in probability that 
$$\limsup_{n\to +\infty} \mathbb P\left\{| X_n(Y_n-c)|\gt \varepsilon\right\}\leqslant \mathbb P\{|X|\gt R\}.$$
Since $R$ can be chosen  arbitrarily large, we are done. "" I agree with the proof except the last step, to conclude that $P\{|X|\gt R\}$ goes to zero when R is arbitrary large we need the hypothesis that X is finite almost surely but I have no such hypothesis. Is there a way to modify the proof to work without any such hypothesis on X ? P.S: I want to prove my statement without using the fact that $(X_n,Y_n) \Rightarrow (X,c)$.","['weak-convergence', 'probability-theory', 'probability-distributions']"
2733543,Integral $n$ for $\sin^n(x) + \cos^n(x) = 1-\frac{n}{2}\sin^2(x)\cos^2(x)$,"Show that only integral solution of $\sin^n(x) + \cos^n(x) = 1-\frac{n}{2}\sin^2(x)\cos^{2}(x)$ is $n=4,6$. I have proved for $n=4,6$ it is true, for other integer, I tried to check range of functions on left and right side. This is not as effective, and I failed. Please suggest a method in this.","['trigonometry', 'functions']"
2733573,"Why $\mathbb{C}[f_1(t),f_2(t)]=\mathbb{C}[t]$ iff $(f'_1(t),f'_2(t))\neq0$ and $t\mapsto (f_1(t),f_2(t))$ is injective?","Let $k$ be a field of characteristic zero.
Let $f_1(t),f_2(t) \in k[t]$ and $f: k \to k^2$ defined by $f(t):=(f_1(t),f_2(t))$. First case $k=\mathbb{C}$:
According to A. van den Essen (page 2), the following claim holds:
$\mathbb{C}[f_1(t),f_2(t)]=\mathbb{C}[t]$ if and only if $f'(t)\neq (0,0)$ for all $t \in \mathbb{C}$ and $f$ is injective. (1) Can one please sketch a proof for this claim? Second case $k=\mathbb{R}$: It is not true that if $f'(t)\neq (0,0)$ for all $t \in \mathbb{R}$ and $f$ is injective, then $\mathbb{R}[f_1(t),f_2(t)]=\mathbb{R}[t]$, as the following example shows: $f_1(t)=t^2$, $f_2(t)=t+t^3$. (2) Is there an additional differential geometry condition, call it $C$, such that: $\mathbb{R}[f_1(t),f_2(t)]=\mathbb{R}[t]$ if and only if $f'(t)\neq (0,0)$ for all $t \in \mathbb{R}$, $f$ is injective, and $C$. Remarks : Concerning question (2): (i) I am interested in a 'not too strong' additional condition, namely, not something like $f_1'(t)=1$.
(ii) Perhaps the additional condition $C$ will involve the second derivative $f''(t)$? Concerning both questions: (iii) Please see the comments in this question , especially, how page 8, claim b is relevant to my question? (iv) Considering the fields of fractions $k(f_1(t),f_2(t))=k(t)$ instead of $k[f_1(t),f_2(t)]=k[t]$ seem also interesting. Edit: This paper is somewhat relevant. Any comments and hints are welcome!","['polynomials', 'differential-geometry', 'algebraic-geometry', 'commutative-algebra']"
2733680,Covariance of two standard normal random variables,"Let $X$ follow the standard normal distribution $N(0,1)$ Let $a>0$ Let $Y=X$ if $|X|<a$ $ $ $ $ $ $ $ $ $ $ $ $ $Y=-X$ if $|X|\geq a$ Then, it is easily shown that Y follows the standard normal distribution $N(0,1)$ What is $Cov(X,Y)$? Is the random vector $(X,Y)$ a multivariate normal? Firstly, since $E(X)=E(Y)=0$, $Cov(X,Y)=E(XY)$ I think $E(XY)=E(X^{2}1_{|X|<a}) - E(X^{2}1_{|X|\geq a})$ how do I proceed to solve this problem? Secondly, I think the random vector is not multivariate normal since if it were, it would be true that $X+Y$ is normal but I think it is not","['statistics', 'probability', 'normal-distribution']"
2733728,Closed form for $\int_0^1 \frac {\log^n(x)}{(1-x)^m} dx$,"How can one find a general form for  $\int_0^1 \frac {\log(x)}{(1-x)} dx=-\zeta(2)
\,?$ Namely $\int_0^1 \frac {\log^n(x)}{(1-x)^m} dx\,$ where $n,m\ge1$  Similar to the original  integral I let $1-x=u\,$ which gives $$\int_{-1}^0 \frac {\log^n(1+x)}{x^m} dx$$ and expanding into series we have: $\int_{-1}^0x^{-m}(\sum_{k=1}^{\infty}\frac{(-1)^{k+1}x^k}{k})^n\,dx$ Now this might be doable with a computer using Cauchy product's but otherwise it's  a madness. Another try is to let $I(k)=\int_0^1 \frac {x^k}{(1-x)^m}\,dx$ And take derivate n times while assuming $k\ge n$ so: $$\frac{d^n}{dx^n}I(k)=\int_0^1\frac{x^k\log^n(x)}{(1-x)^m}dx$$ Plugging $(1-x)^{-m}=\sum_{j=0}^{\infty} \binom{-m}{j}(-1)^jx^j $ in integral and make use of Tonelli
s theorem we get: $$\frac{d^n}{dx^n}I(k)=\sum_{j=0}^{\infty} \binom{-m}{j}(-1)^j\int_0^1 x^{(k+j)}\log^n(x)dx=\sum_{j=0}^{\infty} \binom{-m}{j}(-1)^{(n+j)} n! (k+j+1)^{-(n+1)}$$ But I don't know how to evaluate the latter series.","['integration', 'closed-form']"
2733742,How is the similarity of the structure of two functions defined?,"Consider the sets $X=\{1,2,4\}$ and $Y=\{A,B,C\}$ and then consider two functions $f:X\to X$ and $g:Y\to Y$ defined as $f=\{(1,4),(2,1),(4,1)\}$ and $g=\{(A,C),(B,A),(C,A)\}$. Certainly, these function have the same ""structure"", but what is it? What makes this functions more or less equal? What I've noticed is that there exists a function $h:X\to Y$ such that $f=h^{-1}\circ g\circ h$. Must this hold for any two functions to have the same ""structure""? What property does this make $f$ and $g$ have?",['functions']
2733750,Is $SO(n)$ a normal subgroup of $SO(n+1)$?,"I am trying to learn about homogeneous and symmetric spaces. I know that the quotient $SO(n+1)/SO(n)$ should look like the sphere $S^n$. But how so (i.e, what is the equivalence criterion)? Is $SO(n)$ a normal subgroup and the quotient has a group isomorphism with $S^n$, or we simply have a diffeomorphism between the coset space $SO(n+1)/SO(n)$ and the sphere $S^n$? Also, it would be really helpful if somebody can please suggest some survey/introductory literature on homogeneous spaces. Thank you very much for your help, comments are welcome!","['matrices', 'riemannian-geometry', 'differential-geometry', 'manifolds']"
2733768,Largest subgroup in which a given polynomial is invariant.,"I am trying to solve the following question; Given a polynomial $f\in \mathbb{C}[x_{1},x_{2},\ldots,x_{n}]$, find the largest subgroup $\Gamma\le GL(\mathbb{C}^{n})$ such that $f\in \mathbb{C}[x_{1},x_{2},\ldots,x_{n}]^{\Gamma}$. Here $f\in \mathbb{C}[x_{1},x_{2},\ldots,x_{n}]^{\Gamma}$ denotes the invariant ring of $\Gamma$, i.e the set of all $\sigma\in GL(\mathbb{C}^{n})$ such that $f\circ\sigma=f$. I have tried simple cases (quadratic and linear polynomials) by just taking a polynomial say $f=x^{2}+y^{2}$ and a generic matrix $A=\begin{bmatrix}p & q\\r & s\end{bmatrix}$, then computing $$A \begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}ax+by\\cx+dy\end{bmatrix},$$which I then plug into $f$ to get $$(px+qy)^{2}+(rx+sy)^{2}=p^2 x^2+r^2 x^2+2 p q x y+2 r s x y+q^2 y^2+s^2 y^2.$$ Then I equate coefficients to get a system of equations $$\{p^2+r^2-1, 2pq+2rs, q^2+s^2-1\}$$  which I solve using Groebner bases. This method works for these basic cases, however seems very basic and without much theory behind it. I was wondering if there was a more efficient and/or elegant way to compute these groups given a generic polynomial? Background wise, I am familiar with all content from the book https://www.springer.com/us/book/9783211774168 , which covers classical invariant theory (Hilbert Series, Molien Series, Noethers bound, etc.). Any help is appreciated.","['algebraic-geometry', 'geometric-invariant-theory', 'groebner-basis', 'invariant-theory', 'group-theory']"
2733769,Evaluate $\lim_{x\rightarrow\mathrm\pi}\frac{\sin(mx)}{\sin(nx)}$,"I have to evaluate $\lim_{x\rightarrow\mathrm\pi}\frac{\sin(mx)}{\sin(nx)}$ where $m,n \in\mathbb{N*}$. At first I thought I could just use the remarkable limit $\lim_{x\rightarrow0}\frac{\sin(x)}x = 1$ and the answer could just be $\frac {m}{n}$ but this is not the answer.... I mean it's a part of it but I don't understand why.","['trigonometry', 'limits-without-lhopital', 'limits']"
2733773,Let $A=(a_{ij})$ be an $n\times n$ matrix. Suppose that $A^2$ is diagonal. Must $A$ be diagonal?,"Let $A=(a_{ij})$ be an $n\times n$ matrix. Suppose that $A^2$ is diagonal? Must $A$ be diagonal. In other words, is it true that $$A^{2}\;\text{is diagonal}\;\Longrightarrow a_{ij}=0,\;i\neq j\;\;?$$","['matrices', 'matrix-decomposition', 'linear-algebra']"
2733801,Integration by parts on set $\{u>0\}$,"Consider a $C^\infty$ function satisfying $\Delta u=f$ in $\Bbb R^n$. Suppose that the super-level set $\{u>0\}$ is bounded, so $\partial\{u>0\}$ is compact. Can one integrate by parts on this? Namely, if $\partial\{u>0\}$ were smooth, then 
$$\int_{\{u>0\}} |Du|^2=\int_{\partial\{u>0\}}u \frac{\partial u}{\partial n}-\int_{\{u>0\}}fu=-\int_{\{u>0\}}fu.$$
But $\partial\{u>0\}$ could be something strange. Is this kind of thing ok because $u=0$ on $\partial\{u>0\}$ anyway? I tried showing that $\{u>0\}$ is a set of finite perimeter, but no luck. The literature only seems to have results of this type for when $u$ is harmonic.","['real-analysis', 'riemannian-geometry', 'partial-differential-equations', 'geometric-measure-theory', 'measure-theory']"
2733816,Prove a dynamical system has at least one periodic orbit,"Question : Prove that the dynamical system 
  $$
\dot{x}=x(1-x^2-y^2)-y+\frac{1}{2}xy,
\\
\dot{y}=y(1-x^2-y^2)+x+y^2+2x^2,
$$
  where $(x,y) \in \mathbb{R}^2$, has at least one periodic orbit. My Answer : I used $x=r\cos\theta$ and $y=r\sin\theta$ to write the system in polar coordinates. I got, 
$$
\dot{r}=r(1-r^2)+r^2\sin^3\theta+\frac{5}{2}r^2\cos^2\theta \sin\theta,
\\
\dot{\theta}=1+\frac{1}{2}r\cos\theta \sin^2\theta +2r\cos^3\theta.
$$
In previous examples we have looked at where $\dot{r}$ and $\dot{\theta}$ are greater than or less than $0$. But I am not sure how the $sine$ and $cosine$ components will affect $\dot{r}$ and $\dot{\theta}$. As $sine$ and $cosine$ oscillate do I only need to look at $\dot{r}=r(1-r^2)$, and $\dot{\theta}=1$ ?","['ordinary-differential-equations', 'dynamical-systems', 'polar-coordinates']"
2733852,A proof of a property of positive element in $C^{*}$-algebra.,"In W.Rudin's book P295, Theorem 11.28 is about the positive element(i.e. $a\geq 0$), the property: $\mathscr{A}$ is $C^{*}$-algebra, (e)If $a\in \mathscr{A}$, then $aa^{*}\geq 0$. (f)If $a\in \mathscr{A}$, then $e+aa^{*}$ is invertible in $\mathscr{A}$. Q1: I have other ways to approach the (e), but I do not totally sure it works. 
My method is in the same way of W.Rudin at the beginning, considering the Gelfand transform $\Gamma$ in a maximal normal set of $a$ $\mathscr{B}\subset \mathscr{A}$, we have $$\Gamma(aa^{*})=\Gamma(a)\Gamma(a^{*})=\vert \Gamma(a)\vert^{2} \geq 0,$$
then $aa^{*} \geq 0.$ Q2: How to use the (e) to show (f)? $e+aa^{*}$ is invertible if and only if $\Vert aa^{*}\Vert \leq 1$.How to use it to show the (f)?","['functional-analysis', 'c-star-algebras', 'operator-algebras', 'operator-theory']"
2733875,"Find the number of ways to split a group of 12 into groups of three. Also, Sam and Tom won't sit together.","Any strategy would help. 
Answer is $352,800$. My answer is correct, but it seems illogical and I don't understand it. Won't sit together= find all ways - Sam/Tom do sit together. My Answer: Won't sit together = find all ways - Sam/Tom do sit together. Won't sit together $= 12C3 \times 9C3 \times 6C3 \times 3C3 - 10C3 \times 7C3 \times 4C3 =352,800$ It's correct, but it's definitely not the right way to answer this. How can I calculate Sam/Tom sit together? Clarification: 12 into 4 distinct groups.
Each team A,B,C D has 3 players. 
Sam/Tom refuse to sit next to each other on any team. So I did find the answer, but it doesn't look right to me.","['combinations', 'combinatorics']"
2733876,Proof of uniqueness to the right,"There is a function $f: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ given. This function is non-increasing, so that we have:
$$\forall\,{x_1, x_2 \in \mathbb{R}},\; x_1< x_2 \implies f(t, x_1) - f(t, x_2) \ge 0.$$
There is also a differential equation given with the initial condition:
$$\begin{cases} x'=f(t,x)\\x(t_0) = x_0.\end{cases}$$
Let's consider two solutions of the equation above: $\phi_1, \phi_2$. To show uniqueness, it should be proved that $\phi_1 \equiv\phi_2$. This is my attempt: assume $\phi_1 \not\equiv \phi_2$ and $\phi_1 < \phi_2$. Because both $\phi_1$ and $\phi_2$ are solutions I can write:
$$\begin{cases} \phi_1'=f(t,\phi_1)\\\phi(t_0) = x_0\end{cases}$$
and also
$$\begin{cases} \phi_2'=f(t,\phi_2)\\\phi_2(t_0) = x_0.\end{cases}$$
Now I can consider this 
$$\phi_1' - \phi_2' = f(t,\phi_1) - f(t,\phi_2) \ge 0,$$
thus
$$\phi_1' \ge \phi_2'.$$
By integrating both sides of the equation above I do get
$$\phi_1 - \phi_2 \ge C.$$
However that doesn't lead me to anything useful. I was trying to show that $\phi_1$ and $\phi_2$ differ by at most a constant and then use Picard's theorem . Unfortunately my attempt failed. I would appreciate any hints or tips.","['ordinary-differential-equations', 'proof-verification']"
2733903,Proving $BAx = x$ for $B$ pseudo inverse of $A$,"Suppose $A$ is square but not necessarily invertible and has SVD $\displaystyle A=\sum_{i=1}^r \sigma_i u_i v_i^T$. Let$$
B=\sum_{i=1}^r \frac{1}{\sigma_i}v_i u_i^T.$$
  Show that $BA\,x = x$ for all $x$ in the span of the right-singular vectors of $A$. For this reason $B$ is sometimes called the pseudo-inverse of $A$ and can play the role of $A^{-1}$ in many applications. What I have done so far: \begin{align*} ABx &= \left(\sum_{i=1}^r \sigma_i u_i v_i^T\right)\left(\sum_{i=1}^r \frac{1}{\sigma_i}v_i u_i^T\right)x\\
 &= (\sigma_1u_1v_1^T + \sigma_2u_2v_2^T + \cdots + \sigma_ru_rv_r^T)\left(\frac{1}{\sigma_1}v_iu_i^T + \cdots + \frac{1}{\sigma_r}v_ru_r^T\right)x\\
&= (u_1u_1^T + u_2u_2^T + \cdots + u_ru_r^T)x,\end{align*}
and that is where I am stuck. I tried turning all the $u_iu_i^T$ to $\dfrac{Av_iv_i^TA^T}{\|Av_i\|_2^2}$, but it does not lead me anywhere. Help would be appreciated.","['eigenvalues-eigenvectors', 'svd', 'linear-algebra']"
2733915,"Determinant of a block matrix, blocks are transformations of the same matrix","Let's say that $\Phi, U_{11},U_{12},U_{21},U_{22}$ are $n\times n$ matrices with coefficients in $\mathbb{C}$. Consider the block matrix: $$
M = \begin{bmatrix} 
U_{11}\Phi & U_{12}\Phi \\
U_{21}\Phi & U_{22}\Phi 
\end{bmatrix}
$$ where $U_{ij}\Phi$ is the usual matrix product. Can anything be deduced about the determinant of this matrix in terms of the determinant of $\Phi$?","['matrices', 'linear-algebra']"
2733919,Joint pdf of uniform dependent random variables,"Take $n$ non-negative dependent random variables $X_1,...,X_n$ with $Pr(X_i \leq t) = t, t\in[0,1]$ for every $i$ (uniform marginal distributions). What is an example of a joint pdf for $X_1,...,X_n$ (with the given common marginal distribution), such that $E[\min_i X_i] = 1/2$?","['probability-theory', 'probability', 'expectation']"
2733923,Evaluate $\int_0^\infty{\frac{\ln x \sin x}{x}dx}$,"Evaluate$$\int_0^\infty{\frac{\ln x \sin x}{x}dx}$$
I don't know where to start with this integral. Mathematica shows that $-\frac{\gamma\pi}2$ is the answer, so I don't think that it is easy to be solved.","['improper-integrals', 'integration', 'definite-integrals', 'calculus']"
2733927,"What is meant by ""local equation"" for a closed subscheme?","This question relates specifically to exercise 6.2 of Chapter II of Hartshorne's Algebraic Geometry. In the situation of that exercise, we have projective space $\mathbb{P}_{k}^{n}$ over an algebraically closed field $k$, a projective variety $X$ embedded in $\mathbb{P}_{k}^{n}$, and an irreducible hyperplane $V \subseteq \mathbb{P}_{k}^{n}$ not containing $X$. Then the intersection has some decomposition into irreducible components
$$
V \cap X = Y_{1} \cap Y_{2} \cdots \cap Y_{r}
$$
Hartshorne then says the following: For each $i$, let $f_{i}$ be a local equation for $V$ on some open set $U_{i}$ of $\mathbb{P}_{k}^{n}$ for which $Y_{i} \cap U_{i} \not = \emptyset$ and let $\bar{f_{i}}$ be the restriction of $f_{i}$ to $U_{i} \cap X$ My problem is that I don't understand what ""local equation"" means in this context, nor what it means to restrict to $U_{i} \cap X$. My understanding of ""local equation"" in most contexts is the following: Let $\mathcal{I}$ be the quasi-coherent sheaf of ideals defining $V$ as a closed subscheme of $\mathbb{P}_{k}^{n}$. A ""local equation for $V$ on $U_{i}$"" would usually mean a section $f \in \Gamma(U, \mathcal{I})$. But then what does it mean to restrict this to $U_{i} \cap X$? Indeed $U_{i} \cap X$ is not an open subset in $U_{i}$.","['general-topology', 'sheaf-theory', 'projective-space', 'algebraic-geometry']"
2734016,"Prove that $\frac{g(x)}{\ln(1+x)}$ is integrable over $[0,1].$","Question : Let $f$ be a measurable function on $[0,1]$ such that 
  $$g(x) = |f(x)| \ln(1+x)$$
  is Lebsegue integrable over $[0,1].$
  Prove that $f$ is Lebesgue integrable over $[0,1]$ So the question is asking us to show that 
$$\int_0^1 |f(x)|\, dx = \int_0^1 \frac{|g(x)|}{\ln(1+x)}dx$$
is finite. However, I think that the question is not correct. 
Clearly $g(x) = 1$ is Lebesgue integrable over $[0,1],$ as 
$$\int_0^1 |g(x)| \, dx = 1,$$ 
but
$$\int_0^1 \frac{1}{\ln(1+x)}dx,$$
based on Wolfram Alpha , is not finite.
So $f$ is not Lebesgue integrable over $[0,1].$ Is my reasoning correct?","['integration', 'lebesgue-integral', 'measure-theory']"
2734037,Real roots of cubic polynomial,"Let's consider the polynomial $$f(X)=X^3+aX^2-(3+a)X+1\in\mathbb{Q}[X]$$ where $a\in\mathbb Z$. Simple observations show that it is irreducible and has 3 real roots. If $\alpha$ is one root we can even see that the splitting field is $\mathbb Q(\alpha)$ since a second root is $1/(1-\alpha)$. My question: Is there a way to write $1/(1-\alpha)$ as a linear combination of $\alpha$? And if so, does there exist a general method or trick to find it? So far I've tried to expand the fraction until I have an integer demoninator but without any success.","['irreducible-polynomials', 'abstract-algebra', 'field-theory']"
2734056,In Linear Modelling Why Would I Keep $X_2$ but drop $X_2$ and $X_3$?,"I'm having trouble with some intuition in Linear Statistical Modelling. I'm working with some data with three predictor variables $X_1$, $X_2$ and $X_3$. I've calculated the $F$ test for whether $X_2$ should be retained in the model containing just $X_1$ and $X_2$. Similarly I've calculated the $F$ test for whether $X_2$ and $X_3$ should be retained in the model containing $X_1$, $X_2$ and $X_3$. The conclusion from the first test is to retain $X_2$.  The $p$-value is less than $0.05$. The conclusion from the second test is to drop $X_2$ and $X_3$.  The $p$-value is greater than $0.1$. So my question is, is this not counter-intuitive?  How could adding another variable $X_3$ make things worse for $X_2$ than it was with just $X_2$ alone? My feeling is that I have made a calculation error because it seems the $p$-value should only get smaller with more information not larger. Any insight into this matter would be greatly apprecaited, thank you! I can't show you the exact data without getting fired.  But here's a baby data set that illustrates the same issue.  The $F$ test for whether you can drop $X_2$ in the model with $Y, X_1, X_2$ has $p$-value $0.06$ while the $F$ test for whether you can drop $X_2$ and $X_3$ in the model with $Y, X_1, X_2, X_3$ is $0.14$.  So how can it get larger like that?  This indicates you need $X_2$ on top of $X_1$ but you can get rid of both $X_2$ and $X_3$ on top of $X_1$.  To me that's a contradiction.  Am I doing something wrong here?
$$
Y\ \ \ X_1\ \ \ X_2\ \ \ X_3\\
48.0\ \ \ 50.0\ \ \ 51.0\ \ \ 2.3\\
57.0\ \ \ 36.0\ \ \ 46.0\ \ \ 2.3\\
66.0\ \ \ 40.0\ \ \ 48.0\ \ \ 2.2\\
70.0\ \ \ 41.0\ \ \ 44.0\ \ \ 1.8\\
89.0\ \ \ 28.0\ \ \ 43.0\ \ \ 1.8\\
36.0\ \ \ 49.0\ \ \ 54.0\ \ \ 2.9\\
46.0\ \ \ 42.0\ \ \ 50.0\ \ \ 2.2\\
54.0\ \ \ 45.0\ \ \ 48.0\ \ \ 2.4\\
26.0\ \ \ 52.0\ \ \ 62.0\ \ \ 2.9\\
77.0\ \ \ 29.0\ \ \ 50.0\ \ \ 2.1\\
89.0\ \ \ 29.0\ \ \ 48.0\ \ \ 2.4\\
67.0\ \ \ 43.0\ \ \ 53.0\ \ \ 2.4\\
47.0\ \ \ 38.0\ \ \ 55.0\ \ \ 2.2\\
51.0\ \ \ 34.0\ \ \ 51.0\ \ \ 2.3\\
57.0\ \ \ 53.0\ \ \ 54.0\ \ \ 2.2\\
66.0\ \ \ 36.0\ \ \ 49.0\ \ \ 2.0\\
79.0\ \ \ 33.0\ \ \ 56.0\ \ \ 2.5\\
88.0\ \ \ 29.0\ \ \ 46.0\ \ \ 1.9\\
60.0\ \ \ 33.0\ \ \ 49.0\ \ \ 2.1\\
49.0\ \ \ 55.0\ \ \ 51.0\ \ \ 2.4\\
77.0\ \ \ 29.0\ \ \ 52.0\ \ \ 2.3\\
52.0\ \ \ 44.0\ \ \ 58.0\ \ \ 2.9\\
60.0\ \ \ 43.0\ \ \ 50.0\ \ \ 2.3\\
$$","['statistics', 'statistical-inference']"
2734063,Result needed: outside curve longer than convex inside curve,"In order to complete a proof, I need to show the following (explained using figure attached): I am given two points A and B in the two-dimensional plane connected by the direct black line. Points A and B are also connected by the green vectors. The corresponding green curve is convex. Finally, points A and B are also connected by the red vectors. The corresponding red curve is convex goes from A to B ""on the same side"" as the green curve (with respect to the black line) goes ""outside"" the green curve (with respect to the black line) I want to prove that the length of the red line is greater or equal to the green line. By looking at the picture this is obvious. In order to formally prove this, I could use a lengthy algebraic calculation. However I feel that what I want to prove should follow immediately from some result from the field of analysis for example (of which I am not an expert). Can someone think about a theorem that immediately implies that the red curve is at least as long as the green curve? Many thanks!","['curves', 'real-analysis', 'linear-algebra']"
2734068,Convergence in Measure in a Finite Measure Space,"Assume $m(E) < \infty$ . For two measurable functions $g$ and $h$ , define $$\rho(g,h) = \int_E \frac{|g-h|}{1+|g-h|}.$$ Show that $f_n \to f$ in measure on $E$ if and only if $\lim_{n \to \infty} \rho(f_n,f)=0$ . This problem has been asked here ); however, I am having trouble understanding Davide's nebulous and somewhat misleading suggestions (at least to my mind), which I now reproduce: First, a remark: the map $x\mapsto \frac x{1+x}$ is increasing over the set of non-negative real numbers, and is bounded by $1$ . If $f_n\to f$ in measure, fix $\varepsilon$ and integrate over $\{|f_n-f|>\varepsilon\}$ and $\{|f_n-f|\leqslant \varepsilon\}$ . Conversely, if $d(f_n,f)\to 0$ , then $\frac{\varepsilon}{1+\varepsilon}\mu\{|f_n-f|>\varepsilon\}\to 0$ (integrating over the set $\{|f_n-f|>\varepsilon\}$ ). His first remark is somewhat confusing, for it seems to hint as using the Lebesgue Dominated Convergence theorem for convergence in measure, where $1$ serves as the integrable function that dominates the sequence $\frac{|f_n-f|}{1+|f_n-f|}$ of measurable functions converging to $0$ in measure. Hence, $$\lim_{n \to \infty} \int_E \frac{|f_n-f|}{1+|f_n-f|} = \int_E 0 = 0$$ Yet he goes on to suggest integrating over $\{|f_n-f| > \epsilon\}$ and $\{|f_n-f| \le \epsilon\}$ . Well, I tried this but I couldn't figure out it. For the first integral, we get $$\int_{\{|f_n-f| > \epsilon\}} \frac{|f_n-f|}{1+|f_n-f|} \le \int_{\{|f_n-f| > \epsilon\}}  \le m(\{x : |f_n(x)-f(x)| > \epsilon \} \to 0 \mbox{ as } n \to \infty $$ However, I couldn't figure out how to show that the second integral is zero or at least converges to $0$ as $n \to \infty$ . I tried showing that $\int_{\{|f_n-f| \le \epsilon \}} \frac{|f_n-f|}{1+|f_n-f|}$ is bounded above by $\epsilon$ and then letting $\epsilon \to 0^+$ , which would actually show that $\int_{\{|f_n-f| \le \epsilon \}} \frac{|f_n-f|}{1+|f_n-f|} = 0$ for every $n$ , but I couldn't get just $\epsilon$ alone. The best I could do is get $\epsilon \cdot m(\{x : |f_n(x)-f(x)| \le \epsilon\})$ , which doesn't obviously go $0$ as $\epsilon \to 0^+$ . As for the converse, I believe I was able to follow his suggestion. If $|f_n-f| > \epsilon$ , then $\frac{|f_n-f|}{1+|f_n-f|} > \frac{\epsilon}{1+\epsilon}$ and therefore $\int_E \frac{|f_n-f|}{1+|f_n-f|} > \int_E \frac{\epsilon}{1 + \epsilon} > \frac{\epsilon}{1+\epsilon} \int_{\{f_n-f| > \epsilon\}} = \frac{\epsilon}{1+\epsilon} m(\{x : |f_n(x)-f(x)| > \epsilon\})$ from which we get $$0 = \lim_{n \to \infty} \int_E \frac{|f_n-f|}{1+|f_n-f|} \ge \frac{\epsilon}{1+\epsilon} \lim_{n \to \infty} m(\{x : |f_n(x)-f(x)| > \epsilon\})$$ and hence $\lim_{n \to \infty} m(\{x : |f_n(x)-f(x)| > \epsilon\})=0$ .","['real-analysis', 'measure-theory', 'proof-explanation']"
2734088,Total order isomorphic to $\mathbb N$,"Suppose that we have a countable set $\mathcal C,$ and there is a total order $R$ on this set. By an analogy with the definition of the natural numbers, I guess that, if $R$ satisfy the following conditions, then $(\mathcal C,R)$ is isomorphic to $\mathbb N.$ $\bf 1)$ $\mathcal C$ has a minimum element with respect to $R$ ; $\bf 2)$ $\forall\ x\in \mathcal C,$ there is a minimum element of the set $\left\{y\in\mathcal C|\ x<_R y\right\}$ with respect to $R.$ Question: Is my guess correct? If not, what improvement should me made to the requirements of $R$ ? Any hints are welcomed.","['order-theory', 'elementary-set-theory']"
2734117,If $x^TAx$ = $x^TBx$ for all $x\in \mathbb R^n$. Then what can I say about the matrices? Are they congruent to each other?,If $x^TAx$ = $x^TBx$ for all $x\in \mathbb R^n$ . Then what can I say about the matrices? Are they congruent to each other? My attempt: $x^TAx$ = $x^TBx$ for all $x\in \mathbb R^n$ . Then I can say $A - B$ is skew-symmetric. I cannot see more than this. Can anyone please help me?,"['matrices', 'quadratic-forms', 'linear-algebra']"
2734148,What are different ways to compute $\int_{0}^{+\infty}\frac{\cos x}{a^2+x^2}dx$?,"I am interested to compute the following integral $$I=\int_{0}^{+\infty}\frac{\cos x}{a^2+x^2}dx$$ where $a\in\mathbb{R}^+$. Let me explain my first idea. As the integrand is an even function of $x$ then $$2I=\int_{-\infty}^{+\infty}\frac{\cos x}{a^2+x^2}dx=\lim_{R\to+\infty}\int_{-R}^{R}\frac{\cos x}{a^2+x^2}dx:=\lim_{R\to+\infty}J$$ So, I first focus on computing the $J$ integral by first modifying it as follows \begin{align*}
J&=\int_{-R}^{R}\frac{\cos x}{a^2+x^2}dx=\int_{-R}^{R}\frac{\cos x}{a^2+x^2}dx+i\int_{-R}^{R}\frac{\sin x}{a^2+x^2}dx \\
&= \int_{-R}^{R}\frac{(\cos x+i\sin x)}{a^2+x^2}dx = \int_{-R}^{R}\frac{\exp(ix)}{a^2+x^2}dx
\end{align*} Then, I use the well-known techniques of complex variable theory. First, I replace the real variable $x$ in $J$ with a complex variable $z$ and consider a contour integral over $C=C_1\cup C_2$ $$K:=\int_{C}\frac{\exp(iz)}{a^2+z^2}dz$$ Then, according to the Cauchy's integral theorem and the Residue theorem, I get \begin{align*}
K=J+\int_{C_2}\frac{\exp(iz)}{a^2+z^2}dz &= \int_{C_3}\frac{\exp(iz)}{a^2+z^2}dz=\int_{C_3}\frac{\exp(iz)}{(z+ia)(z-ia)}dz \\
&=2\pi i \frac{\exp(i^2a)}{2ia}=\frac{\pi}{a}\exp(-a)
\end{align*} Next, taking the limit $R\to+\infty$ from the above relation, we obtain $$2I+\lim_{R\to+\infty}\int_{C_2}\frac{\exp(iz)}{a^2+z^2}dz=\frac{\pi}{a}\exp(-a)$$ but, we can show that $$\lim_{R\to+\infty}\int_{C_2}\frac{\exp(iz)}{a^2+z^2}dz=0$$ and then we can obtain the final result $$I=\frac{\pi}{2a}\exp(-a)$$ First, please check my steps to see the final result is correct or not. Second, is there any other way to compute $I$?","['integration', 'definite-integrals', 'contour-integration']"
2734175,Matrix with only real eigenvalues is similar to upper triangular matrix,"I want to show Let $A$ be a $n \times n$ matrix with only real eigenvalues. Then there is a basis of $\Bbb R^n$ with respect to which $A$ becomes upper triangular. There is a hint which says: Construct a basis $B := \{v_1,v_2,\ldots,v_n\}$ of $\Bbb R^n$ where $v_1$ is an eigenvector. I can see why $[T]_B$ would be upper triangular, if all $v_i$ are eigenvectors, because then $[T]_B$ is diagonal. But if not all are eigenvectors, how do I choose the one which are not eigenvectors, such that $[T]_B$ is upper triangular? I've included what I have so far below, but it's basically the above just written down. And I realized to late that those multiple indices are a real pain. I should've have just assumed w.l.o.g. that there is only one eigenvector for each eigenvalue. You don't need to read all of it through, the question is the same as above. This might be related to this question , but I don't understand the answer to it. We also didn't discuss the Jordan canonical form yet, so if there is maybe a simpler way to proof this, I'd appreciate your help! Let $T:\Bbb R^n \to \Bbb R^n$ be the linear transformation associated with $A$ and let $\lambda_1,\ldots,\lambda_k$ be its eigenvalues with algebraic multiplicity $m_1,\ldots,m_k$, which are all real. Let $B_i := \{v^i_1,\ldots,v^i_{d_i}\}$ be a basis of the eigenspace $E_i$ of $\lambda_i$ which has $\text{dim}(E_i) =: d_i$. Now we try to build a basis for $\Bbb R^n$ consisting of eigenvectors. Let $$B := B_1 \cup \ldots \cup B_k = \{v^1_1,\ldots,v^1_{d_1},v^2_1,\ldots,v^2_{d_2},\ldots\ldots,v^k_1,\ldots,v^k_{d_k}\}$$ If $B$ is a basis of $\Bbb R^n$ then we are done, since there exists a basis of $\Bbb R^n$ of eigenvectors of $A$ and hence $A$ is diagonalizable, which means it is diagonal in this basis $B$. And a diagonal matrix is also upper triangular. If $B$ is not a basis of $\Bbb R^n$, then we need to add additional vectors $\{w_1,\ldots,w_l\}$, so that $B \cup \{w_1,\ldots,w_l\}$ is a basis of $\Bbb R^n$. We have $$[T]_B = \begin{bmatrix} \vert & & & \vert & \vert & & \vert \\ [T(v^1_1)]_B & \ldots & \ldots & [T(v^k_{d_k})]_B & [T(w_1)]_B & \ldots & [T(w_l)]_B \\ \vert & & & \vert & \vert & & \vert \end{bmatrix}$$ Now since $T(v^i_j)=\lambda_i v_j$ we have $$[T(v^i_j)]_B=\begin{bmatrix} 0 \\ \vdots \\ \lambda_i \\ \vdots \\ 0 \end{bmatrix}$$ where only the one entry corresponding to $v^i_j$ is a $\lambda_i$ and the rest zero. Leaving the $T(w_j)$ random first, we get almost a diagonal matrix with the eigenvalues on the diagonal except for the last $l$ columns. $$[T]_B = \begin{bmatrix} \lambda_1 & & & & & * & \ldots & * \\ & \ddots & & & & & & \\ & & \lambda_1 & & & \vdots & & \vdots \\ & & & \ddots & & & & \\ & & & & \lambda_k & * & \ldots & * \\ & & & & & & & & \\ & & & & & \vdots & & \vdots \\ & & & & & & & \\ & & & & & * & \ldots & * \end{bmatrix}$$ For $[T]_B$ to be diagonal, the $w_j$ needs to be chosen, so that there are only stars above the diagonal. This means that $T(w_j)$ needs to be a linear combination of only the basis vectors before this specific $w_j$, which are all the $v^i_j$ and some of the $w_j$, i.e. $T(w_j) = \sum_{i,j} c^i_j v^i_j + \sum_{k}^j c_k w_k$ for some constants $c^i_j,c_k$, because then $$[T(w_j)]_B=\begin{bmatrix} c^1_1 \\ \vdots \\ c^k_{d_k} \\ c_1 \\ \vdots \\ c_j \\ 0 \\ \vdots \\ 0 \end{bmatrix}$$ where the last possible non-zero entry $c_j$ corresponds to $w_j$. Therefore $[T]_B$ would be upper triangular. How do I know that such a linear combination for $w_j$ exists and how could I choose it?","['matrices', 'linear-algebra']"
2734179,"Show the sequence $\{{\sqrt{5}}~,{\sqrt{5+{\sqrt5}}}~,\sqrt{5+\sqrt{5+\sqrt{5}}}~,...\}$ converges and find its limit.","Show the sequence $\bigg\{{\sqrt{5}}~,{\sqrt{5+{\sqrt5}}}~,\sqrt{5+\sqrt{5+\sqrt{5}}}~,...\bigg\}$ converges and find its limit. Attempt : Let $a_{1}=\sqrt{5}$ and $a_{n+1}={\sqrt{5+a_{n}}}$ for $n=2,3,...$ Now we apply the induction to prove the sequence$(a_{n})$ is increasing , bounded above by $3.$ $(a_{n})$ is increasing $:$ Note first that $a_{n}\ge0$ for each $n\in{\bf N}$. As $n=1$, one has $a_{1}^{2}=5<5+\sqrt 5=a_{2}^{2}\Longrightarrow |a_{1}|<|a_{2}|\Longrightarrow a_{1}<a_{2}$ . Now assume $a_{n-1}<a_{n}$ for some $n\in\bf N$ . Then $a_{n}^{2}=5+a_{n-1}<5+a_{n}=a_{n+1}^{2}\Longrightarrow |a_{n}|<|a_{n+1}|\Longrightarrow a_{n}<a_{n+1}$ . $(a_{n})$ is bounded above by 3 $:$ As the same manner , we have $a_{1}=\sqrt 5<\sqrt 9=3$ for $n=1$. Suppose the process holds for some integer $n>0,$ that is , $a_{n}<3$ for some $n\in\bf N$ . Whence , $a_{n+1}=\sqrt{5+a_{n}}<\sqrt{5+3}<\sqrt9=3 .$ Therefore, we see on account of the monotonic sequence Theorem that $(a_{n})$ is convergent . Limit of $(a_{n}):$ Take $\displaystyle\lim_{n\rightarrow\infty}a_{n}=L$ for some $L\ge0$ since $a_{n}$ is non-negative for all $n\in\bf N$. Thus , $\sqrt{5+L}=\sqrt{5+\displaystyle\lim_{n\rightarrow\infty}a_{n-1}}\color{red}=\displaystyle\lim_{n\rightarrow\infty}\sqrt{5+a_{n-1}}=\displaystyle\lim_{n\rightarrow\infty}a_{n}=L$ , keep in mind that the red equality holds by $\sqrt{x}$ is continuous . Then , one has $L^{2}-L-5=0\Longrightarrow L=\displaystyle\frac{1+\sqrt{21}}{2}$ since $L$ is non-negative . Can anyone check my proof for validity if you have the time , otherwise ignore this, that is okay . Any comment or valuable suggestion I will be grateful .","['proof-verification', 'nested-radicals', 'calculus', 'analysis']"
2734204,"Show that $f(x)={1+\ln x\over x}, x>0$","Let $f:(0,+\infty)\to\mathbb R$ be a function with $f(1)=1$ which is differentiable and for which it applies: $$xf(x)=1-x^2f'(x), \forall x>0$$ Show that $f(x)={1+\ln x\over x}, x>0$ Personal work: $$xf(x)=1-x^2f'(x)\iff_{x\neq0} f(x)={1-x^2f'(x)\over x}$$ We know that $f(1)=1$ , so: $$f(1)=1\iff{1-x^2f'(x)\over x}=1\iff1-x^2f'(x)=x\iff-x^2f'(x)=x-1\iff f'(x)={x-1\over -x^2}$$ That's what I got. What's the problem? The actual derivative of $f(x)$ is ${-\ln x\over x^2}$ and not $$\color{red}{x-1\over -x^2}$$","['derivatives', 'ordinary-differential-equations']"
2734217,"Area of parallelograms in $\mathbb{R}^n$ (or more generally, the volume of parallelotopes in $\mathbb{R}^n$ )","It is known that in $\mathbb{R}^2$, the area of the parallelogram spanned by two vectors $(a,b)$ and $(d,e)$ is given by $$A=\begin{vmatrix}
a&b\\d&e
\end{vmatrix}$$
while in $\mathbb{R}^3$, the area of the parallelogram spanned by two vectors $(a,b,c)$ and $(d,e,f)$ is given by the norm of the cross product of $(a,b,c)$ and $(d,e,f)$. The two dimension formula can be easily proved from the three dimensional case by setting $c=f=0$. How do we generalise this to the $n-$dimensional space? Namely, what
  is the area of the parallelogram spanned by two vectors
  $(x_1,\ldots,x_n)$ and $(y_1,\ldots,y_n)$? A more generalised question: Given two positive integers $k$ and $n$, what is the $k-$dimensional
  volume of the parallelotope spanned by $k$ vectors
  $(a_{11},\ldots,a_{1n}),\ldots,(a_{k1},\ldots,a_{kn})$? I understand that in the special case that $k=n$, it is given by an $n\times n$ determinant. But how about the case that $k\neq n$? Edit : In the two dimensional case, the area should be the absolute value of $\begin{vmatrix}
a&b\\d&e
\end{vmatrix}$.","['volume', 'area', 'linear-algebra']"
2734237,An affine bundle has a global section?,"Let $X$ be a manifold. We say $\pi: Y \longrightarrow X$ is an rank $n$ affine bundle if there is an open cover $\{ U_\alpha \}$ of $X$ such that
$ Y \big|_{U_\alpha} \cong U_\alpha \times \mathbb{R}^n $ and the transition function from $U_\alpha$ to $U_\beta$ is given by 
$$ (x,v) \mapsto (x, \rho_{\beta \alpha }(x) v + u_{ \beta \alpha} (x)) $$ satisfying the cocycle condition
$ \rho_{\gamma \alpha} (x) = \rho_{\gamma \beta} (x) \rho_{\beta \alpha } (x) $ and
 $u_{\gamma \alpha}(x) =  \rho_{\gamma \beta} (x) u_{\beta \alpha} (x) + u_{\gamma \beta}(x)$. Wikipedia claims that an affine bundle has a global section so it can be identified
with the vector bundle glued by the cocycles $\{ \rho_{\gamma \alpha}  \}$ in a non-canonical way.
How can we construct one exactly? Someone claimed that local sections exist so one can glue them to a global one by
standard partition of unity argument. Since multiplying by constant doesn't make sense for affine bundle, I cannot see why this is obvious.","['differential-topology', 'geometry', 'manifolds', 'vector-bundles', 'differential-geometry']"
2734257,Why does $\;\lim_{n\to \infty }\frac{n}{n!^{1/n}}=e$?,"I have tried to show that this limit :
$$\lim\limits_{n\to \infty }\frac{n}{n!^{\frac 1 n}}=e$$ using  $ \lim  (1+\frac 1 n)^{\frac 1 n} , n \to \infty $ , I don't find any equivalence , however wolfram alpha says that is $e$ as shown here , then how do I evaluate it ?","['real-analysis', 'limits']"
2734265,"Number of n X n binary matrices with every 1 adjacent to some zero and every 0 adjacent to some one, horizontally or vertically.","I came across this integer sequence: A133792 .
It represents the number of $n\times n$ binary matrices with every 1 adjacent to some zero and every 0 adjacent to some one, horizontally or vertically. Now, assuming there exists a way except for bruteforce, how would I go about to calculate this number? (As enumerating valid matrices one-by-one seems hardly feasible, given the fast growing nature of the sequence.) For even ns, I think the problem is related to tiling patterns of dominoes on checkerboards (or at least, using two colored dominoes would ensure that the adjacency constraint won't be violated).","['matrices', 'combinatorics', 'binary']"
2734359,"Solving $\sqrt{a +\sqrt{a-x}}+\sqrt{a-\sqrt{a+x}}=2x, \ a\in\Bbb{R}$","Given
  $$\sqrt{a +\sqrt{a-x}}+\sqrt{a-\sqrt{a+x}}=2x$$
  and $a\in\Bbb{R}$, express $x$ in terms of $a$. I rationalised the above expression and then again rationalised which gave me : $$\sqrt{a+\sqrt{a-x}}-\sqrt{a-\sqrt{a+x}}=\frac{1}{\sqrt{a+x}-\sqrt{a-x}}$$ Now, what should I do? If didn't rationalised the original Equation, or if the squared both sides twice then too bi-quadratic or higher degree polynomial will be obtained. What should be done?","['algebra-precalculus', 'radicals', 'polynomials']"
2734426,Proof details of the fact that the unit tangent bundle is compact in $TM$ if $M$ is a compact manifold,"Let $M$ to be a manifold $m$-dimensional with a smooth hermitian metric $g$. The tangent bundle of $M$ is given by $TM= \bigcup_{p\in M} T_{p}M$, and the unit tangent bundle is given by $S=\{x \in TM: \|x\|_{g(p)}  =1$ where $x \in T_{p}M\} $. (1) In If M is a compact manifold, how to prove that the unitary tangent bundle is also compact? , the solutions assumed that given  $x$ in $M$, you can obtain $U$  open  neighborhood of $x$ such that $\pi^{-1}(U) \cap S \cong U \times S^{n-1}$. But this step is very important, and they do not show this homeomorphism. (2) In ""The unit ball fibration in a tangent bundle"" ,  Pete L. Clark showed that $S$ is a compact set of $TM$ if $M$ is a compact manifold $m$-dimensional: ""1) Do the problem in the special case that the ball fibration is trivial, i.e., isomorphic to a product. 2) Convince yourself that the ball fibration is locally trivial. 3) Since the base is compact, you can find a finite open cover $\{U_i\}$ such that the restriction of the fibration to each $U_i$ is trivial.  Now use the fact that a finite union of compact sets is compact."" I was trying to understand this answer, but  I failed to follow this steps. First of all, we consider $\pi: TM \to M$ the canonical projection. Using that $M$ is a compact  manifold, there exists a finite charts $(\phi_{i}, U_{i})_{i=1}^{n}$ that covers $M$. We note that if $M$ is a compact manifold then $TM$ is a trivial bundle, just consider $\Phi: TM \to M \times \mathbb{R}^{m}$ given by  $x \mapsto (\pi(x), \theta_{\pi(x)}^{(U_{i}, \varphi_{i})} (x)) $ where $\theta_{\pi(x)}^{(U_{i}, \varphi_{i})}:  T_{\pi(x)} M \to \mathbb{R}^{m}$ is a isomorphism linear and $i = \min\{j : \pi(x) \in U_{j}\}$. But I'm stuck in this part. Can some one give some hint? Thank you Recall the definition: A vector bundle of rank $m$ over $M$ is a topological space $E$ together with a surjective continuous map $\pi:E \to M$ satisfying the following conditions: ($1$) For each $p \in M$, the fiber $E_{p}= \pi^{-1}(p)$ over $p$ is endowed with the structure of a $k$-dimensional real vector space. ($2$) For each $p$ in $M$, there exists a neighborhood U of p in M and a homeomorphism $\Phi: \pi^{-1}(U) \to U \times \mathbb{R}^{m}$ (called a local trivialization of $E$ over $U$ ), satisfying the following conditions: (2.1) $\pi_{U} \circ \Phi = \pi$  (where $\pi_{U}:U\times \mathbb{R}^{m} \to U$ is the projection); (2.2) for each $q$ in $U$, the restriction of $\Phi$ to $E_{q}$ is a vector space isomorphism from $E_{q}$ to $\{q\}\times\mathbb{R}^{m}$.","['vector-bundles', 'fiber-bundles', 'differential-topology', 'tangent-bundle', 'differential-geometry']"
2734481,Conjecture : $\int_{0}^{\pi\over 2}\mathrm dt{\ln[2\cos(t)]\over t^2+\ln^2[2\cos(t)]}={\pi\over 4}?$,"Here is the link concerning the integral to be ask about, $$\int_{0}^{\pi}\mathrm dt{t^2\over t^2+4\ln^2[2\cos({t\over 2})]}={\pi\over 4}[1-\gamma+\ln(2\pi)]\tag1$$ Surprisingly the second integral has a very simple closed form (not proven yet only an assumption) $$\int_{0}^{\pi\over 2}\mathrm dt{\ln[2\cos(t)]\over t^2+\ln^2[2\cos(t)]}={\pi\over 4}\tag2$$ Integral $(1)$ is definitely correct, it is proven from the link. But integral $(2)$ is hasn't been proven. I would like to know what method are used to prove $(2)$ or disprove it is ${\pi\over 4}$.","['integration', 'calculus', 'proof-verification']"
2734484,Coordinate free proof that $\operatorname{trace}(A)=0\:\Longrightarrow\:A=BC-CB$,"As you probably know, the trace function on square matrices has the property that $$\operatorname{trace}(AB-BA)=0\,.$$ You might also know that the converse is true: $$\operatorname{trace}(A)=0\;\text{ implies } A=BC-CB\:\text{ for some matrices } B\text{ and }C.$$ This is in fact true of linear operators on a vector space, it's a coordinate free fact. BUT all proofs I'm aware of fix a basis and give a proof using coordinates. So the question is, does anybody know a basis-free proof that does not use coordinates? Thanks for any information - even if the information is that everything I've said here is wrong and I'm a complete idiot.","['operator-theory', 'trace', 'linear-algebra', 'linear-transformations']"
2734495,"If $|f(x+y) - f(x)| \leq g(x)|y|$ for some $g \in L^1(\mathbb{R})$, then $\int_a^b f'(x) dx = f(b) - f(a)$ if $f'(a)$ and $f'(b)$ exist","If $f$ is measurable and differentiable almost everywhere and if there is some $g \in L^1(\mathbb{R})$ such that $|f(x+y)-f(x)| \leq g(x)|y|$ for almost all $x \in \mathbb{R}$ and all $y \in \mathbb{R}$, then $f' \in L^1(\mathbb{R})$ and
  $$\int_a^b f'(x)dx = f(b) - f(a)$$
  if $f'(a)$ and $f'(b)$ exist. $f' \in L^1$ is obvious since $|f'| \leq g \in L^1$ almost everywhere, so I need advice on the second part. I have not found any way to make use of the hypotheses of $f'(a)$ and $f'(b)$ existing.  All of the relevant theorems I've reviewed for when $\int_a^b f'(x)dx = f(b) - f(a)$ hold for all $a$ and $b$ require stronger assumptions that do not seem to hold here.  So, the assumption that $f'(a)$ and $f'(b)$ exist must be vitally important, I presume. If we define $F(x) = \int_{-\infty}^x f'(t)dt$, then $F' = f'$ almost everywhere so $\int_a^b F' = \int_a^b f'$, but this doesn't seem to get me anywhere.  Any advice is appreciated.","['real-analysis', 'lebesgue-integral', 'measure-theory']"
2734505,Relationship between Laplace transform and moment generating function for queue,"(Again this is based on pp240 - 242 of the 1966 edition of Cox and Miller's ""The Theory of Stochastic Processes""). So we have, for a queue in equilibrium/stationary a probability density function for the delay (in virtual waiting time): $$p_0 + \int_0^\infty p(x)dx = 1$$ Where $x$ is the time taken for arriving customer to begin service and $p_0$ the probability that the wait will be zero. Now, the authors then state: $$p_0 + \int_0^\infty p(x)dx = p_0 + p^*(0) =1$$ Where the Laplace transform $\mathcal{L}\{p(x)\}$ = $p^*(s)$. That all seems fine to me, but they go on to say: $$w^*(s)=p_0+p^*(s) =\frac{...}{...}$$ (The rightmost term is not important here). They state ""we denote the m.g.f. of the equilibrium process by $w^*(s)$"". But isn't the mgf the (double sided) Laplace transform evaluated at $-s$? I don't suppose the double sized aspect matters much here: but does the $-s$ stipulation matter? (Perhaps not?) And how does $p_0$ fit in here? It doesn't seem to have been subject to any transform process, so why is it included in the mgf?","['stochastic-processes', 'laplace-transform', 'probability', 'queueing-theory', 'moment-generating-functions']"
2734550,Expected Number of Flips for a Sequence of 4 to Repeat,"I recently had this question in an interview: You are flipping a fair coin until a sequence of four flips repeats itself. The sequences are allowed to overlap. What is the expected number of flips? For example, if you flip HTHTHT, the sequence HTHT appears in flips 1-4 and 3-6. In this case, we are done after 6 flips. As another example, if you flip HHTTHHTT, the sequence HHTT repeats in flips 1-4 and 5-8, and we are done after 8 flips. What is the expected number of flips? I've been thinking about this question for a few days now, and I haven't come to an answer. I've tried the simpler problem where we consider a sequence of two flips instead of four flips, but it is still rather difficult. I suspect that there is a nice recursive way to solve this problem, but I can't figure it out. I am also interested in generalizations of this problem. For example, what is the expected number of flips for a sequence of $n$ to repeat itself? What happens if the coin isn't fair? I have another interview in a few days, so I would very much like to see how to solve this problem in advance. Any help is appreciated. Edit: Based on computational evidence (assuming I didn't make a mistake in the code), it appears that the expected number of flips is about 9.81. I would like to know the exact answer, as well as an analytic solution to this problem. Edit 2: Another piece of information that may be of use: I had 30 seconds to answer the question. This makes me believe that there is some ""easy"" way to get the answer, or they were looking for an approximate answer. Edit 3: @r.e.s. has kindly provided exact solutions for $n=1,2,3,4,5$. For $n=6$, numerical computation seems to indicate that the answer is around $18.977$ or $18.978$. I will be updating periodically with approximations for other values of $n$. Edit 4: $$
\begin{array}{|c|c|}
\hline
n & E(L_{n}) \\ \hline
1 & \frac{5}{2}\\ \hline
2 & \frac{35}{8}\\ \hline
3 & \frac{435}{64}\\ \hline
4 & \frac{2513}{256}\\ \hline
5 & \frac{57922047}{4194304}\\ \hline
6 & \approx 18.9775\\ \hline
7 & \approx 25.928\\ \hline
8 & \approx 35.288\\ \hline
\end{array}
$$","['expectation', 'probability']"
2734630,Derivative transpose (follow up),"I'm working through the Elements of Statistical learning, and I have a quick followup to the below question: derivative transpose In the answer accepted, it states the below: 
$$
\frac{\partial}{\partial\beta}(\beta^T X^TX\beta) = 2X^TX\beta
$$ For context, $X$ is a $n\times p$ matrix, and $\beta$ is a $p \times 1$ matrix. I'm a bit confused about that. When I do the product rule, I see the below:
$$
\frac{\partial}{\partial\beta}(\beta^T X^TX\beta) = (\beta^TX^T)'(X\beta)+(\beta^TX^T)(X\beta)' = X^TX\beta+ \beta^TX^TX
$$ My issue now is that $X^TX\beta$ is a $p\times 1$ matrix, and $\beta^TX^TX$ is a $1\times p$ matrix. I mean, I guess it doesn't matter since they're both vectors and each other's transposes, but this seems shaky from a dimensionality perspective. Am I missing something? Thanks!","['matrices', 'transpose', 'linear-regression', 'derivatives']"
2734644,"The topology induced by a ""good"" net convergence notion induces a net convergence notion as originally specified","The following is from Problem 11D in Willard's General topology textbook. Suppose we have some notion of convergence on a set $X$ satisfying the following properties. Fix $x\in X$ and let $I$ be a directed set. (a) If $x_i=x$ for each $i\in I$, then the net $(x_i)$ converges to $x$. (b) If $(x_i)$ converges to $x$, then every subnet of $(x_i)$ converges to $x$. (c) If every subnet of $(x_i)$ has a subnet converging to $x$, then $(x_i)$ converges to $x$. (d) (Diagonal principle) If $(x_i)$ converges to $x$ and, for each $i\in I$, a net $(x^i_j)_{j\in J_i}$ converges to $x_i$, then there is a diagonal net converging to $x$; i.e., the net $(x^i_j)_{i\in I,\,j\in J_i}$, ordered lexicographically by $I$, then by $J_i$, has a subnet which converges to $x$. Then if the closure of a subset $E$ of $X$ is defined by
  $$
\overline{E}:=\{x\in X \mid x_i\to x\ \text{for some net $(x_i)$ contained in $E$}\},
$$
  the result is a topological space in which the notion of net convergence is as originally specified. I'm struggling to show that the result is a topological space in which the notion of net convergence is as originally specified . It's pretty simple to show that, if a net $(x_i)_{i\in I}$ converges to $x$ in the original notion, then it also converges to $x$ in the topology induced by this original notion. Now, suppose such a net converges to $x$ in the induced topology. How do I show that it yet converges in the original notion? Since the original notion is given ""by imposition"", with no restrictions other than (a), (b), (c) and (d), at first sight it seems ""impossible"" to go back and guarantee the net converges in the original sense, at least, I haven't had any idea. For example: a direct proof seems really not to exist, because there is not such an explicit condition $\mathscr{C}=\mathscr{C}((x_i)_{i\in I},x)$ for which if $\scr C$ is verified for $(x_i)_{i\in I}$ and $x$, then $x_i\to x$ in the original sense; say we try to prove it by contradiction. Suppose $x_i\not\to x$ in the original sense. This seems to lack information just like above! Well, I still tried to go on: if $x_i\not\to x$, then the set $A=\{x_i\,:\, i\in I\}-\{x\}$ is not closed in the result topology, since $x\notin A$ and there is an obvious net on $A$ which converges (in the result topology, by hypothesis) to $x$. Then what? Edit: Indeed, the ""obvious net on $A$"" is not that obvious. Since we are taking $x$ out of $\{x_i:i\in I\}$, maybe we cannot consider the whole net and have to exclude some indexes $i\in I$ in order to get a net on A. This question is related but not equal to this one .","['general-topology', 'nets', 'convergence-divergence']"
2734663,"Show that $ \sum_{k=0}^{j} \binom{2j+1}{k+j+1} \ \frac{(-1)^k}{2k+1}=2^{2j-1}\ B(j+1,1/2)$.","I want to prove that 
$$ \sum_{k=0}^{j} \binom{2j+1}{k+j+1} \ \frac{(-1)^k}{2k+1}=2^{2j-1}\ B(j+1,1/2),$$ where $B(\cdot , \cdot)$ is the beta function . My idea was to change it to something like my previous question. Edit 1 .It follows from the absorption formula that 
$$ \binom{2j+1}{k+j+1} = \frac{(2j+1)(2j) \ldots(j+2)(j+1)}{(k+j+1)(k+j) \ldots(k+2)(k+1)} \binom{j}{k}.$$ 
How can I go further with this binomial series? Edit 2 . This sum is going to diverge very fast as $j \to \infty,$ so I guess something like $\binom{2j}{j}$ inolves. Edit 3 . Due to $(-1)^k$, we have got lots of cancellations, which makes the series to be controlled. Edit 4 . The problem still open. Edit 5 . [Getting some progress] Consider two polynomials $$p_j(t):= \sum_{k=0}^{j} \binom{2j+1}{k+j+1}  (-t)^k \ \, \text{and} \ \ q_j(t):=4^j \ (1-t)^{j}.$$ To prove our guess, it suffices to show that 
$$ \color{red}{\int_0^1 t^{-1/2} \, p_j(t) \, dt = \int_0^1 t^{-1/2} \, q_j(t) \, dt} \tag{*}$$
since 
$$ \begin{align} \sum_{k=0}^{j} \binom{2j+1}{k+j+1} \ \frac{(-1)^k}{2k+1}
& =  \sum_{k=0}^{j} \binom{2j+1}{k+j+1} (-1)^k \int_0^1 \frac{1}{2} t^{k-1/2} \, dt \\ 
& = \frac{1}{2} \int_0^1 t^{-1/2} \, p_j(t) \, dt \, ,
\end{align} $$
and
$$ \frac{1}{2} \int_0^1 t^{-1/2} \, q_j(t) \, dt= 2^{2j-1}\ B(j+1,\frac{1}{2}).$$
I guess we can use induction since for $j=1$, we have 
$$ \int_0^1 t^{-1/2} \, (3-t) \, dx = \int_0^1 t^{-1/2} \, 4(1-t) \, dt = 16/3.$$","['divergent-series', 'binomial-coefficients', 'sequences-and-series', 'beta-function']"
2734690,"Find the Generating Function from the sequence (0,1,0,3,0,5,...)","Find the Generating Function from the sequence (0,1,0,3,0,5,...) I can't conceptualize this one and I know it should be easy because it is just odds, but I am having a hard time figuring out how to cancel the even terms. I was thinking like k$x^k$ but that doesn't cancel the evens. Any help would be much appreciated.",['sequences-and-series']
2734724,Which values of Lambert W function are rational?,"I mean the branch of Lambert-W function when it is single-valued (The $W_0$ function):  Defined on $x\geq \dfrac{1}{e}$ and by the relation $w = W(x) \iff x = we^{w}$. I know that $0$ is one of the rational values, since $W(0)=0$, because $0e^{0} = 0$. I wonder what are the others? I see here ( https://cs.uwaterloo.ca/research/tr/1993/03/W.pdf ) that I can write $W_0(x) = \displaystyle\sum_{n=1}^{\infty} \frac{(-n)^{n-1}}{n!} x^n$, then it is analytic around $0$, and the range is $[-1,\infty)$, so by the Intermediate Value Theorem there must be infinite values where $W(x)$ is rational, but I have no idea how to analyse that. Any help would be appreciated.","['lambert-w', 'real-analysis', 'rational-numbers']"
2734743,"If $\sum_{n=1}^\infty b_n^{1/\alpha} < \infty$ for some $\alpha > 1$, then $\sum_{n=1}^\infty b_n|x-a_n|^{-\alpha}$ converges almost everywhere.","Let $(a_n)$ and $(b_n)$ two sequences of real numbers where $b_n \geq 0$ . If $\sum_{n=1}^\infty b_n^{1/\alpha} < \infty$ for some $\alpha > 1$ , then $\sum_{n=1}^\infty b_n|x-a_n|^{-\alpha}$ converges for almost every $x \in \mathbb{R}$ . I'm having trouble convincing myself that this is true, and I'm having trouble using the convergence of $\sum b_n^{1/\alpha}$ .  Also, if we let $(a_n)$ be some enumeration of $\mathbb{Q}$ , then every point $x$ would be a limit point, so there would be a subsequence $|x-a_{n_k}|^{-\alpha}$ which would grow without bound, it seems.  I've tried some special cases by letting $a_n = 0$ for all $n$ , but still could not prove the result.","['measure-theory', 'real-analysis', 'sequences-and-series']"
2734772,Convexity of a set defined by integrals of a function on measurable sets,"While studying measurable function, I want to know if the following statement is true. Let $\mu$ be a measure on $\mathbb R^n$ with a support with nonempty interior, $f$ polynomial function and $B$ a compact subset of $\mathbb R^n$ : $$A = \left\{\int_{\Omega} f \mathrm d\mu : \Omega \subset B \text{ $\mu$-measurable}\right\}$$ is convex. Can any one help me to prove or disprove this statement ?","['functional-analysis', 'measure-theory']"
2734799,How to avoid getting confused with the number of turns in a problem of interlocked wheels?,"The problem is as follows: The following machine consists of five interconnected wheels. The
  machine is let to work until the first wheel completes six turns. How
  many turns more than the third wheel will give the fifth?. Consider
  the order goes from left to the right. See the figure from below: $\hspace{2cm}$ In my attempt to solve this problem I though to use this equation: $$\textrm{1 turn =}\,2\pi r$$ The first wheel would become into: $$\textrm{turns}_{\textrm{1st}}=\,2\pi \left( 8\, \textrm{inches} \right)$$ The third wheel would become into $$\textrm{turns}_{\textrm{3rd}}=\,2\pi \left( 6\, \textrm{inches} \right)$$ Therefore the ratio between the two would be: $$\frac{\textrm{turns}_{1st}}{\textrm{turns}_{\textrm{3rd}}}=\frac{8}{6}$$ So from this can be established that: $$\textrm{turns}_{1st}=\frac{8}{6} \textrm{turns}_{\textrm{3rd}}$$ So the third wheel will give $\frac{8}{6}$ times of what the first wheel will do. Then I did performed the same logic between the third wheel and the fifth wheel. The third wheel is: $$\textrm{turns}_{\textrm{3rd}}=\,2\pi \left( 6\, \textrm{inches} \right)$$ while the fifth wheel is: $$\textrm{turns}_{\textrm{3rd}}=\,2\pi \left( 3\, \textrm{inches} \right)$$ The ratio between the two would be: $$\frac{\textrm{turns}_{3rd}}{\textrm{turns}_{\textrm{5th}}}=\frac{6}{3}$$ Therefore we can establish a relationship between the third and the fifth wheel: $$\textrm{turns}_{\textrm{3rd}}=\frac{6}{3}\textrm{turns}_{\textrm{5th}}$$ or $$\textrm{turns}_{\textrm{3rd}}=2\times\textrm{turns}_{\textrm{5th}}$$ Then I assumed that the number of turns will give the fifth will be two times of the first. Then If it has been said that the first wheel gives $6$ turns then: $$6\,\textrm{turns}_{\textrm{1st wheel}}\times\frac{8\,\textrm{turns}_{\textrm{3rd wheel}}}{6\,\textrm{turns}_{\textrm{1st wheel}}}\times \frac{2\,\textrm{turns}_{\textrm{5th wheel}}}{1\,\textrm{turns}_{\textrm{3rd wheel}}}=\,16\,\textrm{turns}_{\textrm{5th wheel}}$$ But because what it is being asked it is how many turns will give the fifth than the third, this can be obtained by subtracting the number of turns between the fifth with that of the third: $$16\,\textrm{turns}-8\,\textrm{turns}=8\,\textrm{turns}$$ So $8$ must be the answer. However I'm confused at the way how I did made some interpretations during this problem: Let's look at the equation I wrote lines above: $$\textrm{turns}_{1st}=\frac{8}{6} \textrm{turns}_{\textrm{3rd}}$$ Is it okay to say that third wheel will be $\frac{8}{6}$ times of the first wheel?. I got the same confusion when trying to make an interpretation of the second equation: $$\textrm{turns}_{\textrm{3rd}}=2\times\textrm{turns}_{\textrm{5th}}$$ From looking at the equation it seems to say that the third wheel is $2$ times the turns of the fifth wheel. Can somebody help me to clear these doubts? There is another aspect which I'm also confused and it is, what happens with the wheels in middle. During my attempt to solve this problem I just ignored them and considered the effects as constant. Is it okay to assume this? Then, other than turns. Is the number of turns related with the speed?. I'm aware of the tangential speed which is $\frac{2 \pi r}{t}$. So would the speed remain constant or be higher for the small wheel?.","['algebra-precalculus', 'trigonometry']"
2734802,How can I evaluate $\lim_{n \rightarrow \infty} \int_n^\infty \frac{n^2 \arctan {\frac{1}{x}}}{x^2+n^2}\ dx$?,"I'm here wondering if this integral that our math teacher gave us (students) is even possible to evaluate? I just started to study real analysis so I find this very disturbing. Here you go, and if anyone has any idea I will be very grateful. $$\lim_{n \rightarrow \infty} \int_n^\infty \frac{n^2 \arctan {\frac{1}{x}}}{x^2+n^2}\ dx$$","['real-analysis', 'integration', 'definite-integrals', 'riemann-integration']"
2734808,Smallest ball to contain a subset of diameter $d$ in $\mathbb{R}^n$,"The diameter of a subset $X$ of $\mathbb{R}^n$ is defined as $\sup\{|x-y|:x,y\in X\}$ . What is the smallest radius $r(d,n)$ such that any subset $X$ of diameter $d$ in $\mathbb{R}^n$ is contained in a ball of radius $r(d,n)$ ? What are the $X$ that realize this bound? I know that $r(d,n)\leq d$ . The equilateral triangle gives $r(d,2)\geq d/\sqrt(3)$ and I think we have equality here but I don't know how to prove it.",['geometry']
2734840,How to solve $\left(\frac3{p-3}-1\right)\left(2+\frac4{p-2}\right)=0$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I'm currently preparing myself for uni and thus learning on my own. This equation is killing me as the book doesn't explain how to solve it. $$\left(\frac3{p-3}-1\right)\left(2+\frac4{p-2}\right)=0$$",['algebra-precalculus']
2734866,Clairaut Theorem Counterexample,"Do you know one function $f:\mathbb{R}^2\to\mathbb{R}$ such that $f_{xy}(a,b)=f_{yx}(a,b)$ at point $(a,b)$ but $f_{xy}$ are not continuous at $(a,b)$?","['derivatives', 'calculus']"
2734875,Two different answers of same indefinite integral,Evaluate $$\int \frac {dx}{x^2-x+1}$$ Method 1 $$\int \frac {dx}{x^2-x+1}=\frac {4}{3}\int \frac {dx}{1+\frac {4(x-1/2)^2}{3}}$$ Put $u=x-1/2$ Hence $$\frac 43\int \frac {dx}{1+\frac {4(x-1/2)^2}{3}} =\frac 43\int \frac {du}{1+\frac {4u^2}{3}}$$ And then I could use $$\int \frac {dx}{1+x^2}=\arctan x$$ Method 2 Let $$I=\int \frac {dx}{x^2-x+1}$$ Put $x=\frac 1y$ Hence $dx=\frac {-dy}{y^2}$ Hence $$I=\int \frac {\frac {-dy}{y^2}}{\frac {1}{y^2}-\frac 1y+1}=-\int \frac {dy}{y^2-y+1}=-\int \frac {dx}{x^2-x+1}=-I$$ Hence $$I=-I\Rightarrow I=0$$ Why am I getting two different answers?  I guess i am missing some link in method 2.,"['indefinite-integrals', 'integration']"
2734885,How can I prove this function is bijective?,"I know I have to show it's injective and surjective, but up until now it's always been simple equations that I can prove are equal to each other. This equation is a little bit more complex. $$f(x) = (x-a)\frac{d-c}{b-a} + c$$ Im $99\%$ sure it's bijective, but I don't know how to prove it since there is nothing to set it equal to like other examples. Also $a \lt b$ and $c \lt d$",['functions']
2734886,Numbers whose decimal digits are the coefficients of its continued fraction form,"A curious question recently crossed my mind: can we construct decimal numbers of the form $$\text{""a.bcdefghij…""}$$ where each letter represents a digit $0-9$ (where the number may or may not be rational), so that it is equal to a continued fraction of the form $$a+\cfrac{1}{b+\cfrac{1}{c+\cfrac{1}{d+\cfrac{1}{e+\ddots}}}}$$ For example, I found that $$0.32062241134\dots=0+\cfrac{1}{3+\cfrac{1}{2+\cfrac{1}{0+\cfrac{1}{6+\cfrac{1}{2+\cfrac{1}{2+\cfrac{1}{4+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{3+\cfrac{1}{3+\cfrac{1}{5}}}}}}}}}}}}$$ Experimenting a bit, we see that we can certainly construct infinitely many numbers that are close solutions, but how can we make them the most efficient? To clarify, we prefer not to go too deeply nested into the continued fraction to make the previous digits work, because then we have to make those new digits work.  So some questions are: ~What algorithm will construct such numbers? ~Does that algorithm construct numbers that are “efficient” as described above? Is there one number that is the ""most efficient""? ~Does an exact non-repeating rational example exist?","['number-theory', 'recreational-mathematics', 'continued-fractions']"
2734889,Solving $45x-3795x^3 +95634x^5 - \cdots + 945x^{41}-45x^{43}+x^{45} = N$?,"We have the following excerpt from John Stillwell's Mathematics and its History : My question is, what are the hints which give away the fact that this comes from the expansion of $\sin 45 \theta$?","['algebra-precalculus', 'polynomials', 'trigonometry']"
2734914,Complete Statistics for Uniform Distribution,"Let $X_1, X_2, ..., X_n$ be independent and identically distributed uniform $U(0, \theta)$ distribution where $0 < \theta < \infty $. Show that $T(X)=max X_i$ is a complete statistics. My biggest problem here is how to find the uniform distribution?
Is it either: a) The pdf is $f(x; \theta )$=$ \frac{1}{ \theta -0 }$=$ \frac{1}{ \theta }$ or b) $f(x; \theta )$=$ \frac{1}{ \theta_2 - \theta_1 }$=$ \frac{1}{ \sqrt{3 \theta} + \sqrt{3 \theta } }$ = $ \frac{1}{ 2 \sqrt{3 \theta} }$ because the mean is $0$ and variance $ \theta $, where I find the value of $ \theta_1 $ and $ \theta_2 $ from it. Then to find the complete statistics, I need to find the likelihood function and because it's need to show the maximum value, so I need to find it using order statistics $f_{Y_n}$ But, I can't continue since I'm not sure with my pdf of Uniform distribution. Really appreciated if anyone could clear up the way to find the pdf of the Uniform distribution?","['statistical-inference', 'probability-distributions', 'statistics', 'uniform-distribution', 'order-statistics']"
2735036,"What is the limit of the average value of the first $n$ terms of $(1, 2, 1, 1, 1, 2, 1, 1, 2, 1, ...)$ as $n\to\infty$?","There exists a sequence $a_n$ that begins $(1, 2, 1, 1, 1, 2, 1, 1, 2, 1, ...)$. It is fully defined on the OEIS at A293630 , but I will give a simple explanation here. The sequence starts $1, 2$. The next part is generated by looking at the last term in the current sequence (currently $2$), and adding the rest of the sequence to the end that many times (resulting in $1, 2, 1, 1$). This continues $(1, 2, 1, 1, 1, 2, 1), (1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2)...$ $a_n$ represents the $n$th term of the final, infinite sequence. It appears that
$$\lim_{n\to\infty} \frac{\sum_{k=1}^{n} a_k}{n} = 1.2752618420911721359284772047801515149347600371...$$
While empirical evidence holds this to be true up to an absurd amount of terms, I have not been successful in even proving that the limit exists. I have found no relation of this number to other constants. Is there a way I could either prove the existence of the limit, prove this is the value of the limit, or find the significance of the number? Edit: Now that a proof has been provided by Sangchul Lee , I am looking for what this value may represent and what causes this value in particular to appear. An acquaintance with the same interest has put a bounty up for answers that may present an explanation for this.","['real-analysis', 'sequences-and-series', 'limits']"
2735078,"For a group $G$ such that $|G| = p^3$, for $p$ prime, either $|Z(G)| = p$ or $G$ is abelian.","I came with this proof and I found some other proofs online, but mine is different and I want to see if I made any mistakes. Problem: Suppose $|G| = p^3$, where $p$ is a prime. Show that either $|Z(G)|=p$ or $G$ is abelian. Case 1: $G$ is not abelian We have the class equation $$|G| = |Z(G)| + \sum_{g\in G\setminus Z(G) } \frac{|G|}{|\mathrm{Cent}(g)|}$$ where $Z(G)$ is the center of the group, and $|\mathrm{Cent} (g)|$ is the centralizer of $g$. if we solve for $|Z(G)|$ we get $$|Z(G)| = |G| - \sum_{g\in G\setminus Z(G) } \frac{|G|}{|\mathrm{Cent}(g)|} = p^3 \bigg( 1-\sum \frac{1}{\mathrm{Cent}(g)} \bigg)$$ Also, since $Z(G) \leq G$, by Lagrange's theorem, $|Z(G)|$ divides $|G| = p^3 = ppp$ Therefore, we have 4 possibilities: $|Z(G)| = \{1,p,p^2,p^3\}$ It can't be $p^3$ since that implies that $G$ is abelian, contradicting the initial assumption.
There's another theorem that states that if $|G|$ is a power of a prime number, then the center of $G$ contains nonidentity elements, so it can't be $1$. This leaves us with $p$ or $p^2$. If the size is $p^2$, we can go back to the class equation and obtain
$$p^3 = p^2 + \sum\frac{p^3}{|\mathrm{Cent}(g)|}$$ 
$$p\bigg( 1 - \sum\frac{p}{\mathrm{Cent}(g)}\bigg) = 1$$
$$\sum\frac{p}{\mathrm{Cent}(g)} = \frac{p-1}{p}$$ This number has to be an integer greater than or equal to $0$, and the only option is then $p=1$, but we already discarded that option, and $|Z(G)| \neq p^2$ Therefore, $|Z(G)| = p$ if $G$ is nonabelian. If it is abelian then $G \setminus Z(G) = \emptyset$ so the sum is over no elements giving $0$, and $Z(G)=G \leq G$.","['abstract-algebra', 'proof-verification']"
2735123,a surjective function is closed iff it maps Borel sets to Borel sets,"Let $X$, $Y$ be topological spaces, and $f:X\rightarrow Y$ surjective. Why is $f$ closed if and only if the image of any Borel set is a Borel set? For one direction, it would clearly be sufficient to prove that, if $f$ is closed, the image of any open set is Borel, but I don't know how to proceed from there. For the other direction, I don't have any idea how to proceed.","['general-topology', 'measure-theory']"
2735128,"show that $[0,1]$ is not compacts as a subspaces of $R_K$","Recall that $R_K$ denotes the R in the  $K -topology$. a)  Show  that  $[0,1]$ is not compacts as  a  subspaces of $R_K$ i know that  $R_K $  is finer then R  since  its  basis  contain the basis  of $R$ $R_K =(a,b)  - \frac{1}{n}$  which  is  not open   because  $\frac{1}{n} $ is not closed as complement of open  set is closed",['general-topology']
2735229,Schur's test : an infinite matrix defines a bounded operator on $\ell^2$,"I want to prove Schur test. Schur test says Let $\{\alpha_{ij}\}_{i,j=1}^{\infty}$ be an infinite matrix such that $\alpha_{ij} \ge 0$ for all $i,j$ and such that there are scalars $p_i>0$ and $\beta,\gamma>0$ with
  $\sum_{i=1}^{\infty}\alpha_{ij}p_{i} \le \beta p_j \quad$ , $\quad \sum_{j=1}^{\infty}\alpha_{ij}p_{j} \le \gamma p_i \quad$ for all $i,j\ge 1$ then there is an operator $A$ on $\ell^{2}(\mathbb{N})$ with $\langle Ae_{j},e_{i}\rangle =\alpha_{ij}$ and $\|A\|^2 \le \beta \gamma$. I know that I must use the orthogonal projection and Fourier series, But I don't know how.
This is an exercise from section1, Chapter2,  A Course in Functional Analysis Conway(second edition).","['functional-analysis', 'operator-theory']"
2735235,Approximate numbers by certains rationals,"Let $n \in \mathbb N$ and $k_n \in \left\{0,..,n \right\}$ then we define the numbers $$x_{n,k_n} = \frac{k_n+n^2}{n^3+n^2}.$$ It is easy to see that these numbers satisfy $$x_{n,0} = \frac{1}{n+1} \le x_{n,k_n} \le x_{n,n} =\frac{1}{n}.$$ I would like to know whether there exist three constants $C_1,C_2,C_3>0$ and an integer $i \in \mathbb N$ such that we can find for every $x_{n,k_n}$ a $reduced$ fraction $$\frac{p_{n,k_n}}{q_{n,k_n}}$$ such that two conditions hold: 1.) The denominator can be controlled nicely: $$ \frac{C_1}{n^i} \le \frac{1}{q_{n,k_n}} \le  \frac{C_2}{n^3}$$ and 2.) The approximation is sufficiently good: $$\left\vert x_{n,k_n}-\frac{p_{n,k_n}}{q_{n,k_n}} \right\vert \le \frac{C_3}{n^3}.$$ So to summarize: I am wondering whether one can approximate the $x_{n,k_n}$ by reduced fractions up to an error of order $1/n^3$ and whether those fractions can have a denominator that is always between two different powers of $1/n^k.$","['real-analysis', 'calculus', 'number-theory', 'analysis', 'elementary-number-theory']"
2735275,"non-linear to linear differential equation, Can $xz'=z^2e^{1/z}$ be solved?","$(E) :\qquad x^2y'=y^2e^{\frac{x}{y}}+xy$ Attempt : Let $z:=\dfrac{y}{x}$ thus $z'=\dfrac{xy'-y}{x^2}\iff x^2z'=xy'-y\iff y'=xz'+\dfrac{y}{x}\iff y'=xz'+z$ We deduce that $x^2(xz'+z)=(xz)^2e^{1/z}+x^2z\iff x^3z'=x^2z^2e^{1/z}\iff xz'=z^2e^{1/z}$ So $\displaystyle \dfrac{z'}{z^2e^{1/z}}=\dfrac{1}{x}\iff \int \dfrac{1}{s^2e^{1/s}}\cdot ds=\int\dfrac{1}{x}\cdot dx$, can we go further?",['ordinary-differential-equations']
2735316,maximum of iid exponential random variables,"I have a sequence of iid r.v. $(X_n,n\geq 1), X_i\sim \mathcal{E}(1)$. I am studying properties of the random variable $T_n = \max({X_1,\ldots,X_n})$.
Computing the cdf I get $$\mathbb{P}\left(\{T_n\leq t\}\right)= \prod_{i=1}^n\mathbb{P}(\{X_i\leq t\}) = F_X(t)^n = (1-e^{-t})^n$$ I was asked to study the convergence in distribution of $T_n-\log(n)$ I get that $$
\begin{split}
\mathbb{P}(\{T_n-\log n\leq t\})
  &=\mathbb{P}(\{T_n\leq t+\log n\}) \\
  &= \left(1-e^{-t-\log n}\right)^n \\
  &= \left(1+\frac{-e^{-t}}{n}\right)^n \to e^{((-e)^{-t})}, \quad n\to +\infty
\end{split}$$ But I was also asked to compute the expectation of $T_n$. Now, I know that since the $X_i$ are positive then the $T_n$ will also be positive and thus I can compute $$\mathbb{E}(T_n) = \int_{0}^{+\infty}\mathbb{P}(\{T_n>t\})dt$$ But $$\int_{0}^{+\infty}\mathbb{P}(\{T_n>t\})dt = \int_{0}^{+\infty}1-(1-e^{-t})^n dt = x\big|^{+\infty}_0 - \int_{0}^{+\infty}(1-e^{-t})^n dt$$ and I tried, for instance, to solve the last integral by substitution $y = (1-e^{-t})$ but I don't get something ""easy"" to compute, and wolfram alpha also does not provide me an answer. Am I on the wrong track?","['expectation', 'probability-theory', 'probability-distributions', 'integration', 'definite-integrals']"
2735317,Positive-semidefiniteness of certain matrix,"I have the following (real) matrix which I need to be positive-semidefinite, $P = \begin{bmatrix} P_1 & -\frac{1}{2}(P_1+P_2)\\-\frac{1}{2}(P_1+P_2) & P_2\end{bmatrix} \succeq 0$, where $P_1, P_2 \in \mathbb{R}^{n\times n}$ and $P_1, P_2 \succeq 0$. I think this is only the case when, $P_1 = P_2$, but I couldn't find a way to prove this (only for the case where $n=1$, through the eigenvalues). I was therefore wondering if this is even the case and if so how to prove this. (I already asked this question before but forgot to mention that $P_1$ and $P_2$ are matrices instead of scalars).","['matrices', 'positive-semidefinite', 'linear-algebra']"
2735329,Probability of finding a random mass,"Suppose we have a list of distinctive elements: 
$$X_0=\{x_1,x_2,x_3,\cdots,x_n\}$$
Each element has mass 1. Suppose we take two elements at random and make a new element with appropriate mass. For example we take $x_1$ and $x_n$ and we make a new elements $(x_{n+1})$ of mass two, we put the new element back in the list so, 
$$X_1=\{x_1,x_2,x_3,\cdots,x_n,x_{n+1}\}$$
Now if we repeat this procedure $t$ times we will have 
$$X_t=\{x_1,x_2,x_3,\cdots,x_n,x_{n+1},x_{n+2},x_{n+3},\cdots,x_{n+t}\}$$
Surely as the process of taking two elements and making a new one is random the masses also will be distributed randomly. Now I was wondering can anyone use probability theory and compute the mass of particle $x_i$ at a given time?","['random-walk', 'probability-theory', 'probability', 'sequences-and-series']"
2735383,Binomial summation $\sum\binom nkk^2$,Question: Find $$\sum_{k=0}^n\binom nkk^2$$ I know how to do $$\sum_{k=0}^n\binom nkk=n\space2^{n-1}$$ I tried applying same thing and reached $$\sum_{k=0}^n\binom nkk^2=n\sum_{k=0}^n\binom{n-1}{k-1}k$$ How to proceed?,"['generating-functions', 'combinatorics', 'summation', 'binomial-coefficients']"
2735405,Some may explain this example: the implicit function theorem,"In the Munkres analysis on manifolds, after proving the implicit function theorem and saying that the choice of the last coordinates is given only for convenience, the following example: suppose $A$ open in $R^5$ and $f:A \rightarrow R^2$ is a function of class $C^r$. Suppose one wishes to ""solve"" the equation $f(x,y,z,u,v)=0$ for the two unknowns $y$ and $u$ in terms of the other three. In this case, the implict function theorem tell us that if $a$ is a point of $A$ such that $f(a)=0$ and $det\dfrac{\partial f}{\partial(y,u)}(a) \ne 0$, then one can solve for $y$ and $u$ locally near that point, say $y= \phi (x,z,v)$ and $y= \psi (x,z,v)$. Furthermore, the derivatives of $\phi$ and $\psi$ satisfy the formula $\dfrac{\partial (\phi,\psi)}{\partial(x,z,v)} = - \left[ \dfrac{\partial f}{\partial(y,u)} \right] ^{-1}.\dfrac{\partial f}{\partial(x,z,v)}$. I'm not understanding how to get into this final formula, I've already tried to call $H (x,z,v) = (\phi,\psi)$, but it does not work. I can not get into this equation. Thanks for any tips.","['multivariable-calculus', 'real-analysis']"
2735437,Riemann $\zeta$-function and a series involving Stirling numbers of first kind.,"While answering a recent question I have realized that the series
$$S(n,m):=\sum_{k=n}^\infty{k \brack n}\frac{1}{k!(k-m)},\tag{1}$$
where ${k \brack n}$ are the Stirling numbers of the first kind , can be represented as a linear combination of Riemann $\zeta$-functions with rational coefficients. Namely, the following identity holds for $0\le m<n$ :
$$
S(n,m)=\frac{1}{m!}\sum_{i=0}^{m}{m \brack i}\zeta(n+1-i).\tag{2}
$$
Observe that the sum of coefficients at $\zeta$-functions is equal to 1. Surprisingly, Mathematica knows the analytical result of summation (1) only for $m=0$. Can the identity (2) be derived directly from (1) by ""elementary"" means? If necessary, the result for $m=0$ can be regarded as given. An ""elementary"" proof of it can be found elsewhere . My attempts to apply it onto the case of non-zero $m$ have failed.","['alternative-proof', 'riemann-zeta', 'sequences-and-series', 'stirling-numbers']"
2735449,Multiple roots of one polynomial of degree 4,"I have the following specific polynomial of degree 4:
\begin{align}
 P(x) &= (x_1^2 + x_2^2)^2 + a(x_1 + i x_2)^2(x_1^2 + x_2^2) + \overline{a}
 (x_1-ix_2)^2(x_1^2 + x_2^2)\\
&+b(x_1+ix_2)^4 + \overline{b}(x_1-ix_2)^4, \, x=(x_1,x_2)\in \mathbb{R}^2.
\end{align}
In addition, I know that coefficients $a,b$ are such that 
\begin{equation}
 P(x) \geq c(x_1^2+x^2_2)^2 , \, c > 0.
\end{equation} I have a conjecture that $P$ must have only simple (complex) roots. For example, if one will look for the similar polynomial of degree 2, i.e.:
\begin{align}
&Q(x) = (x_1^2 + x_2^2) + a(x_1 + i x_2)^2 + \overline{a}
 (x_1-ix_2)^2,\\
&Q(x) \geq c(x_1^2 + x_2^2)
\end{align}
then one can directly show that the corresponding roots will be only simple. And condition of positivity is used to have that the determinant is not zero.","['complex-analysis', 'polynomials', 'algebraic-geometry']"
2735493,When convergence in distribution implies stable convergence,"In a previous post I asked help to clarify a property of stable convergence in distribution: Definition Let $X_n$ be a sequence of random variables defined on a probability space $(\Omega,\mathcal{F},\mathbb{P})$
with value in $\mathbb{R}^N$. We say that the sequence $X_n$ converges stably in distribution
with limit $X$, written $X_n\stackrel{\text{st}}{\longrightarrow} X$, if and only
if,  for any bounded continuous function $f:\mathbb{R}^N\to\mathbb{R}$ and for any $\mathcal{F}$-measurable bounded random variable $W$, it happens that:
$$
\lim_{n\rightarrow \infty}\mathbb{E}[f(X_n)\,W]=\mathbb{E}[f(X)\,W].
$$ What I need to prove now is the following: Assume
$$
(Y_n,Z)\stackrel{\text{d}}{\longrightarrow}(Y,Z),
$$ for all measurable random variable $Z$, then $$
(Y_n,Z)\stackrel{\text{st}}{\longrightarrow}(Y,Z)
$$
for all measurable random variables $Z$. So I need to prove that, for any bounded continuous function $f$ and for any measurable $Z$ it holds that
$$
\lim_{n\rightarrow \infty}\mathbb{E}[f(Y_n,Z)\,W]=\mathbb{E}[f(Y,Z)\,W]
$$
for all bounded random variables $W$. I tried unsuccessfully with Portmanteau and Levy continuity theorem… ================================================================= In practice I am trying to prove this proposition from the paper by Podolskij and Vetter : I did this reasoning for (1)=>(3), but I am not so sure of its correctness.","['probability-theory', 'convergence-divergence', 'random-variables']"
2735533,If $f$ is entire and for some $r>0$ we have $f(z)=Cz^n$ for a constant $C \in\mathbb C$ on $|z|=r$ then does it follow that $f(z)=Cz^n$ for all $z$?,"If $f$ is entire and for some $r>0$ we have $f(z)=Cz^n$ for a constant $C \in\mathbb C$ on $|z|=r$ then does it follow that $f(z)=Cz^n$ for all $z$? Since a circle is not open the identity principle doesn't apply so I'm not sure how to prove it, assuming it is even true.","['complex-analysis', 'entire-functions']"
2735563,Is there an ideal $k$ such that the sequence $\mathrm{frac}(kn)$ is most evenly distributed?,"I am generating a sequence of numbers in $[0..1)$ where the $n$th number in the sequence is $f(n)=\mathrm{frac}(kn)$ for some constant $k$ and $\mathrm{frac}(x)=x-\mathrm{floor}(x)$. I want numbers in this sequence to be distributed as evenly as possible, i.e. they should statistically resemble a uniform random distribution. (The application is generating colours, a colour is $\mathrm{HSV}(f(n),1,1)$ and I want the set of colours to have optimal coverage across the spectrum.) Is there an ""ideal"" value of $k$ for this? By intuition I am taking $k=\phi$, which gives the following sequence: 0
0.618
0.236
0.854
0.472
0.09
0.708
0.326
0.944
0.562
0.18
0.798
0.416 This performs adequately for current purposes, but I am wondering if there is a stronger way to formalise this.",['statistics']
2735585,Ring of Witt vectors over finite fields,"I have just started studying Witt vectors and I have questions about the following identity $$W_n(\mathbb{F}_p)\cong \mathbb{Z}/p^n\mathbb{Z}$$ I would like proving this by finding an explicit map $\phi_n: W_n(\mathbb{F}_p)\to \mathbb{Z}/p^n\mathbb{Z}$ , but couldn't come up with a reasonable map. Although I have found the isomorphism $$\phi: W(\mathbb{F}_p)\to \mathbb{Z}_p$$ via $(a_0,a_1,...)\mapsto \chi(a_0)+\chi(a_1)p... $ , where $\chi$ is the Teichmüller character. Can I obtain $\phi_n$ by composing $\phi$ with $pr_n:\mathbb{Z}_p \to \mathbb{Z}/p^n\mathbb{Z}$ and identifying $W_n(\mathbb{F}_p)$ with $(a_0,...,a_{n-1},0,0,...)\in W(\mathbb{F}_p)$ ? Is there a nice explicit version of this map? More generally I am interested in the case $A=\mathbb{F}_q$ . I know that $W(\mathbb{F}_q)\cong \mathbb{Z}_p[\mu_{q-1}]$ should hold.
Is it possible to argue that this is an isomorphism because $\mathbb{Z}_p[\mu_{q-1}]$ and $W(\mathbb{F}_q)$ are both strict $p$ -ring with residue field $\mathbb{F}_q$ and as such canonically isomorphic?","['abstract-algebra', 'ring-theory', 'algebraic-number-theory', 'p-adic-number-theory']"
2735641,A finite ring of matrices is a finite field,"i have a prime number $p$ and an irreducible polynomial $R(x)$ of degree $n$ , $\alpha$ a root of $R$ then it's known that the field $\mathbb{F}_{p^n}$ is isomorphic to the field $\mathbb{F}_{p}[\alpha]$. let $M$ be the companion matrix of $R$ in particular . i wish to proof that $\mathbb{F}_{p}[M]$ is a field isomorphic to $\mathbb{F}_{p^n}$. i've noticed that $R(M)=O$ (Carley-Hamilton theorem) but i don't know whether or not $M$ lies in the algebraic closure of $\mathbb{F}_{p}$ if it dose that will give the result by construction of finite fields so i got stuck there... Next i tried to give an explicit isomorphism (ring isomorphism)
\begin{array}{ccccc}
\psi : & \mathbb{F}_{p^{n}} & \rightarrow  & \mathbb{F}_{p}[M] &  \\ 
& 0 & \rightarrow  & O &  \\ 
& \alpha ^{i} & \rightarrow  & M^{i} & 0<i<q^{n}-1%
\end{array}
with the assumption that $\alpha$ is a primitive element of  $\mathbb{F}_{p^{n}}$ it's easy to poof that $\psi(a*b)=\psi(a)*\psi(b)$ but i couldent proof that $\psi(a+b)=\psi(a)+\psi(b)$ UPDATE:
can i say that $M$ is algebraic over  $\mathbb{F}_{p}$ ?","['finite-fields', 'abstract-algebra']"
2735663,Bayesian parameter estimation for equation $r_t = Z_t^T\beta$?,"Consider $r_t = \mathbb{E}[Z_t]^T\beta$; $t= 1, 2, 3, \cdots$, where $r_t \in \mathbb{R}$, $\beta \in \mathbb{R}^d$ and $Z_t$ is a $d$-dimensional random vector. Let's say $\beta$ is an unknown parameter and we would like to estimate it with a Bayesian approach by observing $r_t, Z_t$ for $t=1, 2, 3, \cdots$ (There is no iid assumption on random variables $Z_t$ and their distribution is unknown).How should we proceed? Any ideas? My Approach: In the case that $d=1$, we may say $\mathbb{E}[Z_t] = \frac{1}{\beta}r_t$, and then say that assuming $\frac{1}{\beta} \sim f_\theta$ as a prior and $Z_t = \frac{1}{\beta}r_t$ we get a prior on $Z_t$. Then, we can update $\theta$ with posterior based on our observation $Z_t = z_t$. I have two questions here: First, is my understanding correct in the case that $d=1$? How can I extend it to $d > 1$?","['bayesian', 'statistics', 'estimation', 'probability', 'parameter-estimation']"
2735677,Domain of $f(x)=\sqrt{\sqrt{\sin x}+\sqrt{\cos x}-1}$,Find Domain of $f(x)=\sqrt{\sqrt{\sin x}+\sqrt{\cos x}-1}$ My try: First of all $x$ belongs to First quadrant Also  $$\sqrt{\sin x}+\sqrt{\cos x}-1 \ge 0$$  Squaring both sides we get $$\sin x+\cos x+2\sqrt{\sin x\cos x} \ge 1$$ Any clue here?,"['algebra-precalculus', 'inequality', 'functions']"
2735690,Properties of a cumulative distribution function and rescaling,"Consider a cumulative distribution function $F$. Take $a: (0,\infty) \rightarrow (0,\infty)$ and $b: (0,\infty) \rightarrow (-\infty,\infty)$. Let $a_s\equiv a(s)$ and $b_s\equiv b(s)$ for all $s \in (0,\infty)$. Assume
$$
[F(a_sx+b_s)]^s=F(x), \quad\forall s>0
$$
could you help me to understand how this implies that $$ [F(a_{st}x+b_{st})]^s=F\left(\frac{a_{st}}{a_s}x+\frac{b_{st}-b_s}{a_s}\right), \quad \forall s,t>0 $$ $$\left[F\left(\frac{a_{st}}{a_s}x+\frac{b_{st}-b_s}{a_s}\right)\right]^t=F\left(\frac{a_{st}}{a_ta_s}x+\frac{b_{st}-b_s-a_sb_t}{a_sa_t}\right). \quad\forall s,t>0 $$ I don't understand how things are rescaled.","['probability-theory', 'probability-distributions']"
2735694,"Suppose that $Z \subseteq Y \subseteq X$ and that $f:X \to Z$ is bijective, then there exists a bijection $g : X \to Y$","This is a lemma to prove Schröder-Bernstein theorem. I encountered this lemma when I read V. A. Zorich's Mathematical Analysis . In his textbook, prof Zorich does not give the detail of the proof. Now I will fill the gaps. I'm very concerned with the correctness of the part that I prove $g(X)=Y$. Please take care of my proof! Lemma: Suppose that $Z \subseteq Y \subseteq X$ and that $f:X \to Z$ is bijective, then there exists a bijection $g : X \to Y$. Proof: Let $A=X \setminus Y$ and $B=\bigcup_{i\in \mathbb{N}}f^i(A)$. Here $f^i=f \circ \cdots \circ f$ is the $n$th iteration of the mapping $f$ and $\mathbb{N}$ is the set of natural numbers (including $0$). $f(B)=f(\bigcup_{i \in \mathbb{N}}f^i(A))=\bigcup_{i \in \mathbb{N}}f(f^i(A))=\bigcup_{i \geq1}f^i(A)$ $B=f^0(A) \cup(\bigcup_{i \geq 1}f^i(A))=A \cup (\bigcup_{i \geq 1}f^i(A))=A \cup f(B)$. We define $g$ as follows: $$
g(x) =
\begin{cases}
\ f(x)      & \text {if $x \in B$} \\
x   & \text {if $x \in X \setminus B$} \\
\end{cases}
$$ Now we prove $g : X \to Y$ is the desired function. $g$ is surjective. $A \cap Y=\varnothing$ and $f(B) \subseteq Z \subseteq Y \implies A \cap f(B)=\varnothing \implies (A \cup f(B)) \setminus f(B)=A$. $g(X)=(X \setminus B) \cup f(B)$ $=(X \setminus (A \cup f(B))) \cup f(B)$ $=(X \cup f(B)) \setminus ((A \cup f(B)) \setminus f(B))$ $=X \setminus A$ $= X \setminus (X \setminus Y)=Y$ $\implies g(X)=Y$. Thus $g$ is surjective. $g$ is injective. $B=A \cup f(B) \implies f(B) \subseteq B \implies (X \setminus B) \cap f(B)= \varnothing \implies$ There does NOT exist $m \in B$ and $p \in X \setminus B$ such that $g(m)=g(p)$. Let $m,p \in X$ such that $g(m)=g(p)$. We have two cases in total. a. $m,p \in B$ $\implies g(m)=f(m)=f(p)=g(p) \implies m=p$ (Since $f$ is bijective). b. $m,p \in X \setminus B$ $\implies g(m)=m=p=g(p) \implies m=p$. Thus $g$ is injective. Thank you for your patience!","['elementary-set-theory', 'proof-verification']"
2735755,"$ a+b+c=0,\ a^2+b^2+c^2=1$ implies $ a^4+b^4+c^4=\frac{1}{2}$","This is a high school problem. It is solved by algebraic calculation
(not difficult) - $\ast$. But I have a problem in interpretation the result. Here intersection of two surfaces $a+b+c=0,\ a^2+b^2+c^2=1$ is a
circle $C$. That is, by $\ast$, $a^4+b^4+c^4$ is constant on it. What does make
it so ? Proof : Note that $S:=\{
(a,b,c)|a^4+b^4+c^4=\frac{1}{2}\}$ is a convex surface. In further,
there is $x,\ y\in S$ s.t. $|x|<1<|y|$. And I guess that $C$ is not a geodesic : Consider a norm on $\mathbb{R}^3$. For instance $\| (a,b,c) \|_1 := |a|+|b|+|c|$ In this case unit ball wrt this norm is octahedron. In further, unit balls of standard Euclidean norm, infinite norm are unit sphere $S_2$, a cube $S_3$, respectively. Clearly, intersection of $S_2$ and plane $\{ (a,b,c)| a+b+c=0\}$ is a geodesic (So is $S_3$). Here considering $S$ and $S_3$, we can assume that $S$ is like $S_3$. That is, $S$ has largest Gaussian curvature at eight points. Hence geodesic is far from these points, but $C$ is not. I enumerate some these exercises, but how can we interpret the answer ? [add] Consider $ \Delta':=\{ (x,y,z)\in S_2 | x,\ y,\ z\geq 0\} $, which
is an equilateral geodesic triangle of side length $\frac{\pi}{2}$. In further $\Delta \subset \Delta'$ is also an equilateral geodesic
triangle of side length $\frac{\pi}{3}$, which touches mid points of
sides in $\Delta'$. Clearly it is not in a plane. Then we have a claim that if $ f (x,y,z)=(x^4,y^4 ,z^4)$, then
$f(\partial \Delta )$ is in a plane. [my answer] Consider $SO(3)$-action on $S^2(1)$. Here what is $smallest$ invariant set ? It is $S^2$. Consider a finite subgroup of $SO(3)$. For instance, a group $H$ which acts on cube whose center is origin. So smallest invariant set of $H$ is not $S^2$. It may be union of finite number of great circles, $T$. And note that $F(x,y,z)=x^4+y^4+z^4$ is invariant under $H$. Hence we can guess that $F$ is constant on $T$.","['algebra-precalculus', 'convex-geometry', 'riemannian-geometry']"
2735761,How to divide $n$ distinct elements into $k$ groups of size of at least $m$,"I have never been good with combinatorics, so I have always stuck with memorizing the formula. However, I can't even begin to fathom the complexity of this problem. I have tried searching for helpful answers on this site, but I only found "" $n$ identical elements into groups of size at least $k$ "" and "" $n$ distinct elements into $k$ nonempty groups. I have also read about the Stirling numbers, but I did not see anything about the size of the groups. I would appreciate if someone could help me approach this problem, or direct me to an already posted question if this is a repost, for which I apologize in advance.","['permutations', 'combinatorics']"
