question_id,title,body,tags
4295874,"If $\int_1^\infty f(x) x^n dx=0$ for all $n>1$, is $f=0$?","For a continuous function $f$ , if $$
\int_1^\infty f(x) x^n dx =0
$$ For all integers $n>1$ , is it true that $f=0$ ? Here, $\int_1^\infty f(x) \text d x = 0$ means $\lim_{A \to \infty} \int_1^A f(x)\text dx =0$ . By looking at the real and imaginary parts, we can assume $f$ is real. If $f$ has compact support, then approximating $f$ with Stone-Weierstrass shows that $\int f^2=0$ so $f=0$ . If $f$ stops changing signs outside an interval $[1,a]$ , then $\int_1^a f(x) (x/a)^n \to 0$ by the DCT, and the rest of the integral must diverge, contradicting the assumption, so the sets where $f$ is positive or negative must be unbounded. The problem is that the approximation theorems I know only apply in compact domains, so integrating to infinity makes it more complicated. The fact that I do not know if $f$ is bounded also complicates matters, I know that if $f$ were decaying better than $e^{-x}$ this would be more tractable since I could just cut off at some point. So far, I am coming up empty. Is it true that $f = 0$ or is there some counterexample?","['integration', 'real-analysis']"
4295878,Probability of drawing an ace from a modified deck,"Given the following question, I have a deck of $52$ cards. I shuffle the deck, and drop $20$ cards randomly into a shredder. You then draw two cards from what remains. What is the probability that they are both aces? A lot of similar questions have been asked (with good answers). For example, ace in the 10th card. I know that the answer to my question is $\frac{4}{52}\cdot \frac{3}{51}$ , but I am having A LOT OF TROUBLE internalising the answer. Intuitively, why is it that randomly removing cards does not effect the probability of drawing aces? To be more specific: Shouldn't we condition for the fact that some aces were shredded? I find it very surprising that the answer remains the same after we condition for the fact that $0$ to $4$ aces could be shredded. If we added cards to the deck instead, does the same logic hold? If I were to randomly add $20$ cards from a shuffled deck to an existing deck and shuffle it, is the probability of drawing an ace still $4/52$ ? Or should I factor in the probability that aces were carried over to the deck? There was a similar question Combining random cards from two decks and calculating probability . Any help will be much appreciated.","['combinatorics', 'card-games', 'probability']"
4295879,How to show that the set of hyperplanes that are transverse to some projective variety is Zariski-open?,"Let $V$ be a analytic/algebraic variety in $\mathbb{CP}^n$ , that is $V$ is  the vanishing locus of some homogeneous polynomial. Let $d = \dim V$ . Everyone knows $^\mathrm{TM}$ that a ""generic $k$ -plane"" $H \subset \mathbb{CP}^n$ intersects $V$ transversely. ""Generic"" here means that this is true for all $H \in \mathrm{Gr}(k+1, n+1)$ that don't lie in a proper subvariety of $\mathrm{Gr}(k+1, n+1)$ . I have two questions / reference requests: First , what do people mean by ""transverse"" when $V$ is singular? Do you inductively define $H \pitchfork V$ iff $H \pitchfork V_\mathrm{reg}$ an $H \pitchfork V_\mathrm{sing}$ ? Second , how do you actually prove that this is generic in the Zariski-sense? My thoughts: Using some kind of parametric transversality theorem (e.g. Lee, Introduction to smooth manifolds Thm. 6.35) you can show that this is true for a full measure set of $\mathrm{Gr}(k+1, n+1)$ but this is quite far from showing it's Zariski-open. The following approach seems promising but I can't get it to work: Consider a parametrization of embeddings of $k$ -planes into $\mathbb{CP}^n$ by some complex manifold $S$ , i.e. we have a map $F: S \times \mathbb{CP}^k \to \mathbb{CP}^n$ s.t. the images of $\{s\} \times \mathbb{CP}^k$ are all possible $k$ -planes in $\mathbb{CP}^n$ . If $F$ restricted to $F^{-1}(V)$ is proper then I think the proof of Sard's theorem/parametric transversality together with Remmert's proper mapping theorem give you the claim. However I don't see how to construct a $F$ like this that is proper.","['transversality', 'complex-geometry', 'algebraic-geometry', 'reference-request']"
4295898,Weighted nuclear norm minimization,"Crossposted on Operations Research Stack Exchange The problem. Let $X,A \in\mathbb{R}^{n\times m}$ and let $W\in\mathbb{R}^{nm\times nm}$ be a positive definite matrix. I want to know if there is a closed-form solution to this problem $$
\min_{X} \frac{1}{2}\text{vec}(X-A)^\top W\text{vec}(X-A) + \|X\|_*,
$$ where $\|X\|_*$ denotes the nuclear norm of $X$ , and $\text{vec}(X)$ denotes the vectorization operation of the matrix $X$ . A solution in the unweighted case. For $W = I$ (the identity matrix), the problem is known as Singular Value Thresholding (SVT), and corresponds to: $$
\min_{X} \frac{1}{2}\|X-A\|_F^2 + \|X\|_*,
$$ where $\|X\|_F$ is the Frobenius norm of $X$ , and a solution is found in this paper (thm. 2.1) , and is nicely expressed as $$X = U\max(0,S-I)V^\top,$$ where $A = USV^\top$ is the SVD of the matrix $A$ . In the weighted case I am not sure whether is possible to express the solution in closed form. This problem is also known as Weighted Singular Value Thresholding (WSVT) and a closed-form solution has not been found yet. Related work. Paper 1 studies the problem of the Weighted Low-Rank Approximation (WLRA) problem which is more difficult than the one proposed in this question (the problem in the question is a convex relaxation of the one in the paper). They have a closed-form solution in the case $W$ can be expressed as $W = W_1 \otimes W_2$ . This was my initial problem, however ... Paper 2 proves that the WLRA problem is NP-hard in general and that is the reason why I moved from WLRA to WSVT (actually my initial problem includes also other constraints, but that is another story). Paper 3 studies the WSVT problem, but they come up with a numerical solution (no analytical solution). Is there really no way one can solve the problem in closed form as for the unweighted SVT problem? Paper 4 studies a related problem (the same as this related question ), where the nuclear norm is weighted by a vector $w$ . In this case, we have an analytical solution. However, I tried to recast my problem in this form with no success. Why bother with a closed form solution? I really need an analytical solution because the original problem is much more complex (includes other convex constraints and another outer optimization over another set of variables). To solve such optimization problem, I believe a closed-form solution for the simpler problem considered in this question would help considerably.","['nuclear-norm', 'convex-optimization', 'matrices', 'optimization', 'regularization']"
4295953,Proof of limit that involves integral,"I was trying to prove the following statement but I am not sure whether my proof is correct or not. Prove that for any $a > 0$ the following holds: $\lim_{t \to \infty}  \int_t^{t+a}\frac{sin(x)}{x}dx = 0$ My approach: Denote $f(t) = \int_t^{t+a}\frac{sin(x)}{x}dx$ , Then we need to prove that for any $\epsilon > 0$ there is $M \in R$ such that if $ t > M$ then $|f(t) - 0| < \epsilon$ Let $\epsilon = \frac{\epsilon}{a}$ ,
from the fact that $\lim_{x \to \infty} \frac{sin(x)}{x} = 0$ we know that there is an element $M1 \in R$ such that if $x>M1$ then $|\frac{sin(x)}{x}| < \frac{\epsilon}{a}$ Therefore for the same $M1$ if $t>M1$ then for any $x \in [t,t+a]$ : $|\frac{sin(x)}{x}| < \frac{\epsilon}{a} \Longleftrightarrow -\frac{\epsilon}{a} < \frac{sin(x)}{x} < \frac{\epsilon}{a}$ Therefore $f$ is bounded between $-\frac{\epsilon}{a}$ and $\frac{\epsilon}{a}$ , so: $-\frac{\epsilon}{a} (t+a-t) < \int_t^{t+a}\frac{sin(x)}{x}dx < \frac{\epsilon}{a} *(t+a-t) \Longleftrightarrow -\epsilon < \int_t^{t+a}\frac{sin(x)}{x}dx < \epsilon \Longleftrightarrow $ $|\int_t^{t+a}\frac{sin(x)}{x}dx| < \epsilon$ This is what I did but I am not sure that this is true. I will appreciate any help, thanks in advance.","['integration', 'limits', 'calculus', 'real-analysis']"
4296040,How can one solve polynomial equations with constant terms that have a high number of factors?,"Today I was given this question on a test: $x^4 - 20x^3 - 20x^2 + 1500x - 9000 = 0$ . Find the value(s) for $x$ . I know how to solve these types of equations. I must record the positive and negative integer factors of the y-intercept or constant term, use guess-and-check to find one factor, and then use synthetic division to factor the equation. However, there are a very high number of factors for 9000. I know that Descarte's Rule of Signs can help determine whether to consider the positive/negative factors, but even then the number is very high. I guessed and checked as much as I could before realizing I would not be able to solve the question in time. Is there a quicker way of finding the first x-intercept or zero?","['algebra-precalculus', 'quartics', 'polynomials']"
4296041,"Why does $ \lim_{x \rightarrow \pi }\frac{\sin(mx)}{\sin(nx)}=\left ( -1 \right )^{m-n}\;\frac{m}{n}$, for positive naturals $m$ and $n$?","Encountered this in a sample university admission exam. $$
\lim_{x \rightarrow \pi } \frac{\sin(mx)}{\sin(nx)} \quad n,m\in \mathbb N_{> 0}
$$ What surprised me was that the answare sheet suggested that the limit was equal to: $$
\left ( -1 \right )^{m-n}\;\frac{m}{n}
$$ Graphing the function made it clear for me that this is the correct answer, but i cannot understand why.",['limits']
4296096,Elementary Algebra and Trigonometry,"Hey mathstack community, recently i came across a 2 volume set of books written by G. Chrystal, and another one by Henry Fine titled ""Algebra: An Elementary Text-book Volumes 1-2"" and ""A College Algebra,"" respectively. These works are known for being how do i put it, comprehensive in their coverage and justify the movement from statement to statement, rather than spoon feed to you bits and pieces of math like the common textbook does without any substantiation. i know that people say ""practice, practice, practice"" a lot and this is very important, but my experience with the average pre-university textbook is that they are very weak on the theory side but very strong on the rote, drill and kill side. Given this, i have little to no genuine understanding of elementary mathematics at all. After skimming Chrystal and Fine, i wonder if there is any other textbooks like these. tl;dr : since i'm just a high school student with no genuine understanding of elementary algebra beyond a couple poorly knit techniques, i would like ask the more informed and educated people here whether they know of any other titles for elementary algebra and trigonometry, similar in content to Chrystal and Fine i also know about the Gelfand series which lack the structure and depth of Chrystal and Fine, but have good content regardless.","['algebra-precalculus', 'book-recommendation']"
4296108,Do nested commutators show up when commuting more than two matrices?,"If I want to commute two matrices $A$ and $B$ , I obtain their commutator as an error term: $$AB = BA + [A,B].$$ I would think that, to generalize this to $n$ matrices, I would obtain nested commutators in the error term, but I cannot seem to work this out (if it is indeed true). For example, for $3$ matrices: $$ABC = BCA + [A, BC],$$ which was obtained by commuting $A$ with $BC$ . Alternatively: $$ABC = (BA + [A,B])C = BAC + [A,B]C = B(CA + [A, C]) + [A,B]C = BCA + B[A,C] + [A,B]C,$$ which was obtained by commuting $A$ with $B$ and then $A$ with $C$ . Clearly the two end results above are equal. However, after trying to do some algebra, I cannot seem to write this as a nested commutator. My questions are: Is there a way to write $$A X_1 X_2 \cdots X_{n-1} = X_1 X_2 \cdots X_{n-1} A + E,$$ where $E$ contains a term with $n-1$ nested commutators containing all of the matrices $A$ , $X_i$ ? If the above is not possible, is it possible when all the $X_i$ commute?","['matrices', 'abstract-algebra', 'linear-algebra', 'lie-algebras']"
4296118,"(Durrett : Probability : Theory and Examples 5th ed, Excercise 3.4.13 ) Lindeberg Feller Theorem","The problem is Suppose $P(X_j=j)=P(X_j=-j)=1/2j^{\beta}$ and $P(X_j=0)=1-j^{-\beta}$ where $\beta>0$ . Show that if $\beta <1 $ then $S_n/n^{(3-\beta)/2}\Rightarrow c \chi$ . Here is one solution using characteristic function( Let $P(X_j=j)=P(X_j=-j)=1/2j^{\beta}$ and $P(X_j=0)=1-j^{-\beta}$ where $\beta\in(0,1)$, then $S_n/n^{(3-\beta)/2)}\Rightarrow c\chi$ ). But can we prove this using Lindeberg Feller Theorem? Letting $X_{n,m} = \frac{X_m}{n^{(3-\beta)/2}}$ ,
it is easy to show that $\lim_{n\rightarrow\infty}\sum\limits^n_{m=1}E[
X_{n,m}^2]\rightarrow \sigma$ . However I can't prove that $$ \lim_{n\rightarrow\infty}\sum\limits^n_{m=1}E[X_{n,m}^2;|X_{n,m}|>\epsilon]\rightarrow 0$$","['measure-theory', 'central-limit-theorem', 'real-analysis', 'probability-theory', 'probability']"
4296136,Solve: $\int_a^b\frac{1}{x\sqrt{x^2-a^2}}\left(\frac{\cos(x)}{x}+\sin(x)\right)dx$,"I am trying to solve this integral $$\int_a^b\frac{1}{x\sqrt{x^2-a^2}}\left(\frac{\cos(x)}{x}+\sin(x)\right)dx.$$ I have tried some basic substitution and integration by parts, but beyond that can seem to solve find an analytical solution. I was able to solve it numerically for which an example solution is shown in the picture with the integral on the $y$ axis and $x$ on the $x$ axis. It looks like a Bessel or sinusoidal exponential to me.","['integration', 'definite-integrals']"
4296158,Domain of composite function $\left( f \circ g \right)\left( x \right).$,"Question: Given that $f\left( x \right) = \sqrt{x - 3}$ and $g\left( x \right) = x + 1$ , find the domain of $\left( f \circ g \right)\left( x \right)$ . My attempt: $\left( f \circ g \right)\left( x \right) = \sqrt{x - 2}$ , hence the
domain of $\left( f \circ g \right)\left( x \right)$ is $\{x\mid x \geq 2\}$ . However, I also aware that a composite function can only exist over a
domain where both component functions exist. This implies that the
domain of $\left( f \circ g \right)\left( x \right)$ is $\{x\mid x \geq 3\}$ . Could someone please explain the second solution to me because it is
perplexing.","['elementary-functions', 'functions']"
4296168,Coordinate-free proof that dual Minkowski norm is indeed a Minkowski norm on the dual space,"Context: let's say that a Minkowski norm on a vector space $V$ is a map $F\colon V\to \Bbb R_{\geq 0}$ such that $F$ is smooth on $V\setminus \{0\}$ , $F$ is positive-homogeneous of degree $1$ , and for every $x\in V\setminus \{0\}$ , the symmetric bilinear form $g_x$ on $V$ defined by $$g_x(v,w) = \frac{1}{2} \frac{\partial^2}{\partial t\partial s}\bigg|_{t=s=0}F^2(x+tv+sw)= \frac{1}{2} D^2(F^2)(x)(v,w)$$ is positive-definite. Now define $F^*\colon V^* \to \Bbb R_{\geq 0}$ by $F^*(\xi) =\max \{\xi(x) \mid F(x)=1\}$ . I want to understand the proof that $F^*$ is a Minkowski norm, in the above sense, on $V^*$ . Everything is clear except the positivity condition. I am following the notes by Matias Dahl (clearly based on the book by Zhongmin Shen). He does a heavy coordinate computation on page 7 that, as I see it, hides the actual idea behind the proof. This is Lemma 3.1.2 on Shen's book. So, I want a proof without coordinates, but I'm struggling to translate what he did there. We are free to use the Legendre transform $\ell\colon V\to V^*$ given by $\ell(x) = g_x(x,\cdot)$ and $\ell(0) = 0$ , and its properties --- in particular that $F = F^*\circ \ell$ . From $F^2 = (F^*)^2 \circ \ell$ , we have that $$D(F^2)(x)(v) = D((F^*)^2)(\ell(x)) \circ D\ell(x)v,\tag{$1$}$$ and thus $$D^2(F^2)(x)(v,w) = D^2((F^*)^2)(\ell(x))(D\ell(x)v,D\ell(x)w) + D((F^*)^2)(\ell(x))((D^2\ell)(x)(v,w)),\tag{$2$}$$ so the proof is concluded once we establish that $$D((F^*)^2)(\ell(x))((D^2\ell)(x)(v,w)) = 0,\tag{$3$}$$ as it will follow that $g = \ell^*(g^*)$ , where $g$ is the assignment $x\mapsto g_x$ on $V\setminus \{0\}$ and $g^*$ is the assignment $\xi \mapsto g^*_\xi$ on $V^*\setminus \{0\}$ induced by $F^*$ . So each $g_x$ being positive-definite implies each $g^*_{\ell(x)}$ positive-definite as well, and we're done as $\ell$ is surjective. I cannot verify that $(3)$ is true. From Dahl's coordinate computation, the homogeneity relation $g_{\lambda x} = g_x$ (for $\lambda > 0$ ) must enter. This implies that $\ell(\lambda x) = \lambda\ell(x)$ (for $\lambda>0$ ), and (1) reads $g_x(x,v) = g^*_{\ell(x)}(\ell(x), D\ell(x)v)$ , while $(D\ell(x)v)w = g_x(v,w)$ , but we only have that $(D^2\ell)(x)(x,\cdot) = 0$ and I don't see how this is enough.","['finsler-geometry', 'normed-spaces', 'multivariable-calculus', 'duality-theorems', 'differential-geometry']"
4296186,$i = \frac{dq}{dt}$ implies $\Delta q = i \Delta t$? Incorrect mathematics used as some kind of hand-wavy justification for an engineering equation?,"I am reading an electrical engineering textbook that states that the relationship between current $i$ , charge $q$ , and time $t$ is $$i = \dfrac{dq}{dt} \tag{1}$$ Based on this, the authors then state that $$\Delta q = i \Delta t \tag{2}$$ This set off alarm bells in my head. Now, (2) may actually be true, but using (1) as some kind of implication for (2) just seems like incorrect mathematics. If I had to fill in the blanks of the authors' thinking, it seems to me that they were likely rationalising this through the derivation of the derivative $$\dfrac{df}{dx} = \lim_{\Delta x \to 0} \dfrac{f(x + \Delta x) - f(x)}{\Delta x} = \lim_{h \to 0} \dfrac{f(x + h) - f(x)}{h} = \lim_{\Delta x \to 0} \dfrac{\Delta y}{\Delta x}$$ But, nonetheless, I don't see how, mathematically, $i = \dfrac{dq}{dt}$ implies $\Delta q = i \Delta t$ . Am I mistaken here, or is this actually incorrect mathematics used as some kind of hand-wavy justification for an engineering equation?","['physics', 'electromagnetism', 'derivatives']"
4296247,Existence of point with prescribed value from given differential equation,"I came up with this problem while I was trying to prove the following geometric problem :
Let $A, B$ be the distinct points in $\mathbb{R}^2$ . If $r(t)$ is the non self-intersecting trajectory of particle in $\mathbb{R}^2$ starts at $A$ and ends at $B$ , is there a moment when velocity vector of particle is pointing same direction with the vector $B-A $ $?$ , that is, $r'(t)=c(B-A)$ for some $c>0$$?$ This is definitely not true in general for $\mathbb{R}^n$ with $n>2$ , but intuitively it must be true for $\mathbb{R}^2$ . First I formalized this problem as follows : Let $r:[a,b] \rightarrow \mathbb{R}^2$ : continuously differentiable unit speed curve in $\mathbb{R}^2$ with $r(a)=(0,0), r(b)=(1,0)$ . Then $v(t)=r'(t) \in S^1$ .
Now letting $r(t)=(r_1(t),r_2(t))$ , $v(t)=(r_1'(t),r_2'(t))$ we get differential equation $$
r_1'(t)^2 + r_2'(t)^2 = 1
$$ with boundary conditions $$
r_1(b) = 1\\
r_1(a) = r_2(b) =  r_2(a) = 0
$$ Or equivalently we get integral equations $$
\int_a^b r_1'(t) dt = r_1(b) -  r_1(a) = 1 \\
\int_a^b r_2'(t) dt = r_2(b) -  r_2(a) = 0 \\
r_1'(t)^2 + r_2'(t)^2 = 1
$$ Lastly we can parametrize $v(t)$ by angle function $\theta(t)$ s.t. $v(t)=(r_1'(t),r_2'(t))=(\cos\theta(t),\sin\theta(t))$ and then we get $$
\int_a^b \cos\theta(t) dt =  1 \\
\int_a^b \sin\theta(t) dt = 0 
$$ The goal is to prove $\exists t_0 \in [a,b] \ s.t. v(t_0)=(1,0)$ .(Under the last formalism it is to prove $\exists t_0 \in [a,b] \ s.t. \theta(t_0) = 2\pi n $ for some $  n\in \mathbb{Z}$ . Now how can I prove this?
Also since I have no knowledge about getting information of solution of this type of differential/integral equation, any comment on how/where I can learn about this type of problem would be helpful. EDIT : I noticed that I need to assume that trajectory is not self-intersecting, that is, $r(t)$ is need to be one-to-one. Otherwise there is an obvious counterexample: $r(t)$ is not one-to-one"" />","['integral-equations', 'ordinary-differential-equations', 'differential-geometry']"
4296252,Where did I make a mistake solving $x^3-6x^2+2x+3=0$ with Cardano's method?,"Solve $x^3-6x^2+2x+3=0$ for $x$ Here's my attempt at solving it via Cardano's Method as it applies to the general cubic: $$x^3-6x^2+2x+3=0$$ \begin{align*}y&=x+\frac{b}{3a} \\ y&= x+\frac{-6}{3} \\ y &= x-2 \\ y+2&=x\end{align*} Substitute $x$ for $y+2$ $$(y+2)^3-6(y+2)^2+2(y+2)+3=0$$ Expand out $$y^3+6y^2+12y+8-6y^2-24y-24+2y+4+3=0$$ Add like terms to get the depressed cubic $$y^3-10y-9=0$$ $$y=u+v$$ Substitute $y$ for $u+v$ $$y^3=(u+v)^3$$ \begin{align*}(u+v)^3 &= u^3+3u^2v+3uv^2+v^3 \\ &= u^3+v^3+3uv(u+v) \\ &= u^3+v^3+3uvy\end{align*} $$y^3=u^3+v^3+3uvy$$ Make the right side equal to $0$ to get the depressed cubic in terms of $u$ , $v$ , and $y$ $$y^3-3uvy-(u^3+v^3)=0$$ \begin{align*}-3uv &= -10 & u^3+v^3 &= 9 & uv &=\frac{10}{3}\end{align*} $$u^3v^3=\frac{10^3}{3^3}=\frac{1000}{27}$$ Factored form of desired quadratic $$(t-u^3)(t-v^3)=0$$ Expand out the quadratic \begin{align*}t^2-(u^3+v^3)t+u^3v^3 &= 0 \\ t^2-9t+\frac{1000}{27} &= 0\end{align*} Can't factor quadratic, use the quadratic formula to solve for $t$ $$t=\frac{9\pm \sqrt{81-\frac{4000}{108}}}{2}$$ Simplify the square root $$t=\frac{9 \pm \frac{\sqrt{3561}}{9}}{2}$$ Simplify the fraction $$t=\frac{81 \pm \sqrt{3561}}{18}$$ Take cube root of the entire fraction to get $u$ and $v$ \begin{align*}u &= \sqrt[3]{\frac{81 + \sqrt{3561}}{18}} \\ v &=\sqrt[3]{\frac{81 - \sqrt{3561}}{18}}\end{align*} Simplify root expression \begin{align*}u &= \frac{\sqrt[3]{972+12\sqrt{3561}}}{6} \\ v &= \frac{\sqrt[3]{972-12\sqrt{3561}}}{6} \\\\ y &= \frac{\sqrt[3]{972+12\sqrt{3561}}+\sqrt[3]{972-12\sqrt{3561}}}{6} \\\\ x &= \frac{\sqrt[3]{972+12\sqrt{3561}}+\sqrt[3]{972-12\sqrt{3561}}}{6}+2\end{align*} Now, to check that I got this wrong, I checked for the roots of the original cubic using a calculator, and this is what I should have gotten for $x$ : $$x= \left\{1, \frac{5+\sqrt{37}}{2}, \frac{5-\sqrt{37}}{2}\right\}$$ But instead, I got a whole mess of cube roots and square roots, so I definitely made a mistake somewhere trying to solve this cubic using Cardano's Method. So where is that mistake that led to the whole mess of $n$ th roots?","['cubics', 'algebra-precalculus', 'solution-verification', 'polynomials']"
4296297,Proof verification: $\sum_{n=1}^\infty \frac{1}{n^2}=\frac{\pi^2}{6}$,"While milking the integral $\int_0^\pi\sin^{-1}\left(\sin x\right)dx$ , I believe I may have conceived of a nice proof of $$\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}$$ It makes use of the established fact that an equivalent problem is to prove that $$\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8}$$ Proof : since $\sum 1/n^2$ and $\sum 1/(2n+1)^2$ both converge, the
chain of equalities \begin{align} \sum_{n=1}^{2k}\frac{1}{n^2} &= 1+\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{(2k-1)^2}+\frac{1}{(2k)^2}\\
&= 1+\frac{1}{3^2}+\cdots+\frac{1}{(2(k-1)+1)^2}+\frac{1}{2^2}+\frac{1}{4^2}+\cdots+\frac{1}{(2k)^2}\\
&= \sum_{n=0}^{k-1}\frac{1}{(2n+1)^2}+\sum_{n=1}^k\frac{1}{(2n)^2}\\
&= \sum_{n=0}^{k-1}\frac{1}{(2n+1)^2}+\frac{1}{4}\sum_{n=1}^k\frac{1}{n^2}
\end{align} implies that $\sum_{n=1}^\infty 1/n^2=\frac{4}{3}\sum_{n=0}^\infty 1/(2n+1)^2$ , so $$\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}\iff\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8}$$ I'd greatly appreciate it if someone took the time to review it! Proof : consider the integral $$\int_0^\pi\sin^{-1}\left(\sin x\right)dx$$ On one hand, \begin{align}
\int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \int_0^{\frac{\pi}{2}}\sin^{-1}\left(\sin x\right)dx+\int_{\frac{\pi}{2}}^\pi\sin^{-1}\left(\sin x\right)dx\\
&= \int_0^{\frac{\pi}{2}}xdx+\int_{\frac{\pi}{2}}^\pi(\pi-x)dx\\
&= \frac{1}{2}\left(\frac{\pi}{2}\right)^2+\pi\left(\pi-\frac{\pi}{2}\right)-\left[\frac{1}{2}\pi^2-\frac{1}{2}\left(\frac{\pi}{2}\right)^2\right]\\
&= \frac{\pi^2}{8}+\pi^2-\frac{\pi^2}{2}-\frac{\pi^2}{2}+\frac{\pi^2}{8}\\
&= \frac{\pi^2}{4}
\end{align} On the other, the fact that $-1\leq\sin(x)\leq 1$ and $\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}t^{2n+1}$ converges uniformly to $\sin^{-1}(t)$ over $[-1,1]$ as $k\to\infty$ justifies us writing \begin{align}
\int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \lim_{k\to\infty}\int_0^\pi\left(\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\sin^{2n+1}(x)\right)dx\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\int_0^\pi\sin^{2n+1}(x)dx\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}}^\pi\sin^{2n+1}(x)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}-\frac{\pi}{2}}^{\pi-\frac{\pi}{2}}\sin^{2n+1}\left(x+\frac{\pi}{2}\right)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_0^{\frac{\pi}{2}}\cos^{2n+1}(x)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_{\frac{\pi}{2}-0}^{\frac{\pi}{2}-\frac{\pi}{2}}\cos^{2n+1}\left(\frac{\pi}{2}-x\right)dx\right)\\
&= \lim_{k\to\infty}\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\left(\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx+\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx\right)\\
&= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx\\
\end{align} All that remains is to evaluate $\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx$ . Using the reduction formula for $\int\sin^n (x)dx$ , it can be shown that for every integer $n\geq 1$ , $$\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx=\frac{2n}{2n+1}\int_0^{\frac{\pi}{2}}\sin^{2n-1}(x)dx$$ and, after inducting on $n$ , \begin{align}
\int_0^{\frac{\pi}{2}}\sin^{2n+1}(x)dx &= \int_0^{\frac{\pi}{2}}\sin(x)dx\cdot\frac{2(1)}{2(1)+1}\cdot\frac{2(2)}{2(2)+1}\cdots\frac{2n}{2n+1}\\
&= \frac{2\cdot 4\cdots (2n)}{1\cdot 3\cdots (2n+1)}\\
&= \frac{2^n(1\cdot 2\cdots n)}{1\cdot 3\cdots(2n+1)}\\
&= \frac{2^n(1\cdot 2\cdots n)\cdot(2\cdot 4\cdots(2n))}{1\cdot 2\cdot 3\cdot 4\cdots(2n)\cdot(2n+1)}\\
&= \frac{2^n(1\cdot 2\cdots n)\cdot 2^n(1\cdot 2\cdots n)}{(2n+1)!}\\
&= \frac{\left(2^n n!\right)^2}{(2n+1)!}\\
\end{align} Substituting this expression into our limit, we get \begin{align}
\int_0^\pi\sin^{-1}\left(\sin x\right)dx &= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2^n n!)^2(2n+1)}\cdot\frac{\left(2^n n!\right)^2}{(2n+1)!}\\
&= \lim_{k\to\infty}2\sum_{n=0}^k\frac{(2n)!}{(2n+1)}\cdot\frac{1}{(2n)!(2n+1)}\\
&= \lim_{k\to\infty}2\sum_{n=0}^k\frac{1}{(2n+1)^2}\\
&= 2\sum_{n=0}^\infty\frac{1}{(2n+1)^2}\\
\end{align} which, together with the original result $\int_0^\pi\sin^{-1}\left(\sin x\right)dx=\frac{\pi^2}{4}$ , immediately yields $$\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{1}{2}\cdot\frac{\pi^2}{4}=\frac{\pi^2}{8}$$ Q.E.D. Let me know what you think! If you identify any errors or optimizations, please don't hesitate to share them with me.","['calculus', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4296303,How many negative real roots does the equation $x^3-x^2-3x-9=0$ have?,"How many negative real roots does the equation $x^3-x^2-3x-9=0$ have ? My approach :- f(x)= $x^3-x^2-3x-9$ Using rules of signs, there is 1 sign change , so there can be at most 1 positive real root f(-x)= $-x^3-x^2+3x-9$ 2 sign changes here, indicating at most 2 negative real roots I end up with following 2 possibilities:- 1)1 positive, 2 negative real roots 2)1 positive, 2 imaginary roots how to progress further ?","['algebra-precalculus', 'polynomials']"
4296305,"How many 20-digit numbers can be formed from {1,2,3,4,5,6,7,8,9} such that no 2 consecutive digit is both odd","How many 20-digit numbers can be formed from { ${1,2,3,4,5,6,7,8,9}$ } such that no 2 consecutive digit is both odd. (Repetition allowed) I've noticed that the number of odd digit in the number must be less than 11. But i can't progress more than that. Any help is appreciated. Thanks in advance.","['combinatorics', 'discrete-mathematics']"
4296324,Logical error in probability problem about postman delivering letters so that every house gets a wrong letter,"A postman has to deliver four letters to four different houses in a street. Unfortunately the rain has erased the addresses, so he just distributes them randomly, one letter per house. What is the probability that every house gets the right letter? (☆ What is the probability that every house gets a wrong letter?) This was  a problem I stumbled on to while revising combinatorics. For the first part it is pretty straightforward, as for four houses, say $A, B, C$ and $D$ , there are $4$ letters $L_A, L_B, L_C$ and $L_D$ , and only one combination exists such that the correct letter is delivered to the correct house. However, for the starred question, my logic goes as follows, but the answer quite doesn't agree with what the site says. My logic : Let the houses $A, B, C$ and $D$ be side by side in the same order as they appear in the alphabet. $A$ $B$ $C$ $D$ X_______________ X marks the location of the postman. At house $A$ , out of the four letters $L_A, L_B, L_C$ , and $L_D$ , there exist only three possibilities such that house $A$ gets the wrong letter. For house $B$ , two exist and for house $C$ only $1$ possibility, either of the remaining $2$ letters. The last house is guaranteed to have received the wrong letter provided the above conditions hold true. Therefore
the possibilities are $3 \cdot 2 \cdot 1 = 6$ . Therefore the probability is $6 / 24 = 1 / 4 = 0.25$ However the site says (quoted exactly from https://mathigon.org/world/Combinatorics#:~:text=To%20find%20the%20probability%20that,called%20the%20Inclusion%20Exclusion%20principle. ) To find the probability that every letter gets delivered to the wrong house is a bit more difficult. It is not simply $1 – 0.0417$ , since there are many cases in which one or two, but not all houses get the right letter. In this simple case, the easiest solution would be to write down all $24$ possibilities. You will find that in $9$ out of the $24$ cases every house gets a wrong letter, which gives a probability of $0.375 = 37.5\%$ . If there more too many houses to write down all possibilities, you can use an idea called the Inclusion Exclusion principle. Can someone explain the loophole in my logic, because the solution on the site asks us to bruteforce which happens to be a very frowned upon technique in exams?","['permutations', 'inclusion-exclusion', 'combinatorics', 'probability']"
4296382,Uncountable set of uncountable equivalence classes,"I am trying to find $A/\sim$ , a set $A$ with an equivalence relation $\sim$ such that the set of equivalence classes is uncountable and the equivalence classes contain an uncountable amount of elements. I already tried with $A = \Bbb{R}$ and $x \sim y := x - y \in \Bbb{Z}$ . The equivalence classes are the sets of reals with the same fractional part. You can show that $A/ \sim = [0,1)$ . Which is easy to show is uncountable with an injection $f: \{0,1\}^{\infty} \to [0,1)$ (this is the standard method I use to show a set is uncountable). However, I think all elements in my equivalence classes are countable, as they are all natural numbers plus a specific fractional part. Am I right to think like this, does it make my example false ? In case it is false, is there a way to ""fix"" it?","['discrete-mathematics', 'real-analysis']"
4296438,Partial derivatives seem to depend on how variables are defined,"Suppose I have $f_1(x,y) = x^2+xy+y^2$ . Then $\frac{\partial f_1}{\partial x} = 2x + y$ . But then if I define $u=x^2$ , and then $f_2(x,y,u)=u+xy+y^2$ . Then $\frac{\partial f_2}{\partial x} = y$ . Yet, $f_1$ and $f_2$ are in some sense exactly the same function. Is this expected, or am I misusing the partial derivative? How I think this paradox is resolved: The first function maps from $\mathbb{R}^2$ to $\mathbb{R}$ , and the second function could really be seen as a mapping from a particular 2-dimensional manifold $M\subseteq \mathbb{R}^3$ (defined by $M=\{(x,y,z):z=x^2\}$ ). If we first map $\mathbb{R}^2$ to $M$ , then compose with $f_2$ , that's exactly equal to $f_1$ . So: maybe the answer is that viewing $f_2$ as a function from $\mathbb{R}^3\rightarrow\mathbb{R}$ , then $\frac{\partial f_2}{\partial x}=y$ is exactly right. But if we use the fact that $u=x^2$ , then we view $f_2$ as a function from $\mathbb{R}^2\rightarrow\mathbb{R}$ , it's not right. The ambiguity seems to come from the fact that $\frac{\partial}{\partial x}$ could be a tangent vector of either $\mathbb{R}^3$ or $\mathbb{R}^2$ , and on a case-by-case basis, one would need to be clear about which one it is. Except, I don't see that clarity in most uses of partial derivatives. If there is a function defined on $n$ variables, and then it's decided that there is actually some relationship between those variables (so the function really describes a function from some lower-dimensional manifold in $\mathbb{R}^n$ ), how is that denoted, and does it change how I should approach partial derivatives? EDIT: To make the example more clear: If we define $f_1(x,y,u)=x^2+xy+y^2$ , then $f_1=f_2$ when restricted to $\{(x,y,u):u=x^2\}$ . Then both are most naturally thought of as functions from $\mathbb{R}^2$ to $\mathbb{R}$ , but both are defined using $\mathbb{R}^3$ .","['partial-derivative', 'multivariable-calculus']"
4296446,Problem from Sidney Resnick A Probability Path.,$X_{i}\sim bern(\frac{1}{i})$ $S_{n}=\sum_{i=1}^{n}X_{i}$ . Then $\frac{S_{n}}{\ln(n)}\xrightarrow{a.s} 1$ I think I somewhere have to use that $\frac{H_{n}}{\ln(n)}\to 1$ . Where $H_{n}$ is the nth harmonic number. As $\sum_{i=1}^{n}\frac{1}{i}$ and $\ln(n)$ just screams out Euler-Mascheroni constant to me. So I try to bring in expectation somewhere to make that $H_{n}$ appear. $$P\left(\left|\frac{S_{n}}{\ln(n)}-1\right|\geq \epsilon\right)\leq \frac{E\left[\frac{S_{n}}{\ln(n)}-1\right]}{\epsilon}$$ by Markov's inequality. This gives me that as $n\to\infty$ . $P(|\frac{S_{n}}{\ln(n)}-1|\geq \epsilon)\to 0$ . But this does not give me almost sure convergence. also I cannot apply Strong Law of Large numbers as the rv's are not iid. Can anyone tell me how I should proceed. I am lost for ideas .,"['probability-limit-theorems', 'convergence-divergence', 'probability-distributions', 'probability-theory']"
4296468,Short exact sequences of holomorphic vector bundles,"Consider a holomorphic bundle $F$ over a complex manifold $M$ . I have read on a comment here on mathstack that given a holomorphic subbundle $E$ of $F$ there are many non-isomorphic holomorphic bundles $G$ that complete the sort exact sequence $$0\to E \xrightarrow{h} F \to G \to 0\,.$$ I really can not understand how is this possible. By defintion of the short exact sequence it should be $F/im(h) = G$ , where $im(h) =E$ . Hence any two holomorphic bundles $G_1$ , $G_2$ that complete the short exact sequnce above should be isomorphic to $F/E$ . Note that by convention $h$ should have constant rank along the fibers.","['complex-geometry', 'vector-bundles', 'exact-sequence', 'differential-geometry']"
4296470,Bounded minimum value problem (from an exam today in Australia).,"A question appeared in today's exam in Victoria (Australia), in which a graphical calculator was also allowed to be used (just for context). This question is causing some 'controversy', both in the interpretation of the wording, as well as the actual answer. Consider the function $g_a(x)=\sin(\frac{x}{a})+\cos(ax), a\in \mathbb{Z}^{+}. \text{ and } x\in \mathbb{R}$ . State the greatest possible minimum value of $g_a$ (1 mark). (This is not necessarily word for word (except for the last sentence), but this was indeed the premise of the question) For this, I stated that this value is $-\sqrt2$ , occurring for $a=1$ . Upon graphing, if we call the minimum value $m$ , it becomes clear that $-2<m\leq-\sqrt{2}$ . (The $-2$ can be seen as this only occurs upon the superposition of minima, which never occurs but does get approached ). Now, a solution posted online is that: $$\lim_{a\to \infty}g_a(x)=\cos(ax) \\ \therefore \min(g_a)=-1$$ However, to me this limit is nonsensical as the value of $x$ for which the minimum occurs increases as $a\to \infty$ sufficiently so as for there to always exist a value of $x$ such that $-2<g_a(x)\leq-\sqrt{2}$ , even if this occurs for very large (negative or positive) values of $x$ . This can be seen just by graphing on something like Desmos, where if zoomed out, you will see the value go below $-1$ . What is a mathematically rigorous way of showing this, seeing as this was given to Year 12s?","['limits', 'functions', 'upper-lower-bounds']"
4296555,"If $f(f(x)) = x+1, f(x+1) = f(x) + 1$, is it true that $f(x) = x + 1/2$?","If $f(f(x)) = x+1, f(x+1) = f(x) + 1$ , where $f: \Bbb R \rightarrow \Bbb R$ is real-analytic, bijective, monotonically increasing, is it true that $f(x) = x + 1/2$ ? I have tried to represent $f(x)$ as power series in a neighborhood of arbitrary $x_{0}$ and $y_{0} = f(x_{0})$ . It's obvious that we can choose such a neighborhood $U_{0}(x_{0})$ that it's image by $f$ is $U_{1}(y_{0})$ . Then, we can try to find the power series of $f(f(x))$ , which is equal to $x+1$ . But I cannot see, how such an equation shows that $f(x) = x + 1/2$ .","['functional-equations', 'calculus', 'iterated-function-system']"
4296561,"Evaluating $\int_0^\infty\frac{\tan^{-1}av\cot^{-1}av}{1+v^2}\,dv$","The Weierstrass substitution stuck in my head after I used it to prove the rigidity of the braced hendecagon (and tridecagon). Thus I had another look at this question which I eventually answered in a very complicated way , and hit upon a potentially simpler approach. For reference the original question is reproduced below: Pick three points from the uniform distribution over a unit circle's circumference. What is the probability $P(x)$ of the triangle thus formed containing $(x,0)$ where $0\le x\le1$ ? My new approach represents the three points by their Weierstrass parameters – $\left(\frac{1-t^2}{1+t^2},\frac{2t}{1+t^2}\right)$ and similarly for $u$ and $v$ – and then computes the barycentric coordinates of $(x,0)$ ; they will all be positive iff the triangle contains the point. The raw expressions are a bit messy, but cylindrical algebraic decomposition shows that if $t<u<v$ containment occurs iff $0<v\land t<k/v\land k/v<u<k/t$ where $k=\frac{x-1}{x+1}$ , which leads to a single triple integral for the probability: $$P(x)=\frac6{\pi^3}\int_0^\infty\int_{-\infty}^{k/v}\int_{k/v}^{k/t}\frac1{(1+t^2)(1+u^2)(1+v^2)}\,du\,dt\,dv$$ Note that both integration region and integrand are invariant under the involution $(t,u,v)\to(v,-u,t)$ , which simplifies the region description and allows solving the first two levels easily: $$P(x)=\frac{12}{\pi^3}\int_0^\infty\int_{-\infty}^{k/v}\int_{k/v}^0\frac1{(1+t^2)(1+u^2)(1+v^2)}\,du\,dt\,dv$$ $$=\frac{12}{\pi^3}\int_0^\infty\frac{\tan^{-1}av\cot^{-1}av}{1+v^2}\,dv\qquad a=\frac{1+x}{1-x}=-\frac1k\tag1$$ I have not been able to solve this last parametric integral; both Mathematica and Rubi fail on it except at $a=1$ . Because I found the probability by another method, however, I know what the result must be: $$P(x)=\frac14-\frac3{2\pi^2}\operatorname{Li}_2(x^2)\qquad x=\frac{a-1}{a+1}\tag2$$ How can $(2)$ be obtained from $(1)$ ?","['integration', 'polylogarithm', 'definite-integrals']"
4296644,"If $X_{1},\ldots,X_{n}$ are independent Bernoulli random variables, calculate $E[S_{\tau_{2}}]$","Let $X_{1},\ldots,X_{n}$ be independent Bernoulli random variables. $P(X_{i}=1)=p$ , $P(X_{i}=0)=q=1-p$ . We denote $S_{n}=X_{1}+\cdots+X_{n}$ . There are $\tau_{1}=\min \{n: X_{n}=1\}$ and $\tau_{2}=\min \{n: X_{n}=X_{n-1}=1\}$ . How to calculate $E[S_{\tau_{2}}]$ ? I've tried through the total expectation formula. I got $$E[S_{\tau_{2}}]=\frac2p+\frac{E[S_{\tau_{2}-\tau_{1}}]+1}{1-p}$$ But it is not clear how to proceed further. A similar question for $\tau_{3}=\min \{n: X_{n}=X_{n-1}=X_{n-2}=1\}$ . How to calculate $E[S_{\tau_{3}}]$ ?","['expected-value', 'probability-distributions', 'probability']"
4296675,I am having trouble coming up with series solutions to differential equations,"I have a problem that needs to be solved by Power Series Method. The equation is $$y'+y=2$$ I know this is trivial to use seperable equations so I know what the answer is: $$y(x)=c_1e^{-x}+2$$ but I can't derive it using the Taylor Series expansion. using power series the DE becomes $$x^n\cdot\sum{[c_{n+1}\cdot(n+1)+c_n]}=2$$ My recurrence relationship is $$c_{n+1}=\frac{2-c_n}{n+1}$$ I used out to $n=5$ , I have the general solution as $$c_{m+1}=\frac{(-1)^m(c_0+2[what\ I\ can't\ find])}{(m+1)!}$$ at $n=5$ $$[what\ I\ can't\ find]=2(4!-3!+2!)$$ If you increase n then the number of factorial terms increases. Any help is appreciated.","['power-series', 'ordinary-differential-equations', 'sequences-and-series']"
4296696,How to write the realisation of a function whose domain is a function?,"Let $A$ and $B$ be arbitrary sets with arbitrary elements $a\in A$ and $b\in B$ and power sets $\mathcal{P}(A)$ and $\mathcal{P}(B)$ . Further, let $f:A\to\mathcal{P}(B)$ be a function. I want to define a real-valued function $g:f(a)\to\mathbb{R}$ that assigns one real number to each element $b\in f(a)$ . My question is as follows: how do I denote the realisation of the function $g:f(a)\to\mathbb{R}$ ? I have considered three (unsatisfactory) possibilities: Writing this function as $g(f(a))=x$ for some $x\in\mathbb{R}$ is unsatisfactory, since $g(f(a))\subseteq\mathbb{R}$ and therefore $g(f(a))\notin\mathbb{R}$ ; Writing this function as $g(a)=x$ for some $x\in\mathbb{R}$ is unsatisfactory, since $g(a)=g(f(a))\subseteq\mathbb{R}$ and therefore $g(a)\notin\mathbb{R}$ ; Writing this function as $g(f(a),a)=x$ for some $x\in\mathbb{R}$ is unsatisfactory, since $g(f(a),a’)$ is undefined if $a\neq a’$ . Any help will be much appriacted. Thank you all in advanced.","['elementary-set-theory', 'elementary-functions']"
4296721,Gateaux derivative and Frechet derivative in calculus of variation,"Let $X$ be some Banach space, if $f \in C(U,\Bbb{R})$ a continuous real value function.We define the Gateaux derivative  exist  if for any $h\in X$ exist $df(x_0,h) \in \Bbb{R}^1$ s.t: $$|f(x_0+th)-f(x_0)-tdf(x_0,h)|= o(t)$$ For Frechet derivative $f'(x_0)$ exist if for some $\xi\in X^*$ : $$|f(x)-f(x_0)-\langle\xi,x-x_0\rangle| = o(\|x-x_0\|)$$ Denote this $\xi = f'(x_0)$ the Frechet derivative Prove the following result:
If $f$ exist Gateaux derivative everywhere inside a neiborhood $V$ around $x_0$ ,for each $x\in V$ exist some $\xi(x)\in X^*$ such that $$\langle \xi(x),h\rangle = df(x,h)$$ Moreover $x\mapsto \xi(x)$ is continuous then $f$ also has Frechet derivative at $x_0$ . To check Frechet derivative exist at the point $x_0$ ,needs to find some $\xi$ satisfies the definition,it's natural to guess that $\xi(x_0)\in X^*$ it's the desired one. Choose $h = (x-x_0)/\|x-x_0\|$ then $\langle \xi(x_0),h\rangle = df(x_0,h)
$ Substitute into $$|f(x)-f(x_0)-\langle\xi(x_0),x-x_0\rangle|  =\\ |f(x_0 +(x-x_0))-f(x_0)-\|x-x_0\|\langle\xi(x_0),h\rangle|
\\=|f(x + \|x-x_0\|h)-f(x_0)- \|x-x_0\|df(x_0,h)|$$ As $\|x-x_0\|\to 0$ ,it goes to zero,but this gives a wrong proof,the reason is we take $x-x_0$ direction fixed ,and the convergence is not uniformly over all the directions. We need to use continuity of $\xi(x)$ to handle this problem.By definition we have for unit length $h$ , $$|df(h,x)-df(h,x_0)| = |\langle\xi(x)-\xi(x_0),h\rangle|\le \|\xi(x)-\xi(x_0)\| \to 0$$ when $\|x-x_0\|\to 0$ I have no idea how to use this condition.This condition says ""directional derivative"" is continuous in all the directions. My idea to proceed the proof is using the same trick in Rudin's mathematical analysis book p219 9.21:that construct a sequence of line segments. $$f(\mathbf{x}+\mathbf{h})-f(\mathbf{x})=\sum_{j=1}^{n}\left[f\left(\mathbf{x}+\mathbf{v}_{j}\right)-f\left(\mathbf{x}+\mathbf{v}_{j-1}\right)\right]$$ with $\mathbf{v}_{k}=h_{1} \mathbf{e}_{1}+\cdots+h_{k} \mathbf{e}_{k}$ the problem here is we lie in infinite dimension space,this trick seems not apply well in our case?","['calculus-of-variations', 'frechet-derivative', 'functional-analysis', 'real-analysis']"
4296737,Popular mistakes in probability,"Question: What not-trivial mistakes do students often make when solving problems in probability theory, mathematical statistics and random processes? Some examples of wrong solutions: Problem 1: Find distribution of $F_{\xi}(\xi)$ for continious $F_{\xi}$ . ""Solution"": $F_{\xi}(\xi) = P(\xi \le \xi) = 1$ Problem 2: Is it possible to guess if one of a pair of random numbers is larger with probability > $\frac{1}2$ ? ""Solution"". Obviously no (sometimes with some blurry reasoning, mentioning symmetry).
(if smb.is interested, see discussion, e.g., in https://mathoverflow.net/questions/9037/how-is-it-that-you-can-guess-if-one-of-a-pair-of-random-numbers-is-larger-with ) Problem 3: Find $Var(\min(X,Y))$ for independent $X,Y \sim exp(1)$ . ""Solution"". If $X \le Y$ then $\min(X,Y) = X$ and $Var(\min(X,Y)) = DX = 1$ , if $X > Y$ then $\min(X,Y) = Y$ and $Var(\min(X,Y)) = DY = 1$ , so in any case we got $Var(\min(X,Y)) = 1$ .
A more absurd version is problem 3b: find $D\xi$ for $\xi \sim Bern(p)$ . ""Solution"": $\xi$ takes values $0$ and $1$ , if it's equal to $0$ , then $Var\xi = Var(0)=0$ , in other case $Var\xi = Var(1)=0$ , hence $Var(\xi)=0$ . Problem 4: Suppose that systems of sets $\mathcal{A}_1$ and $\mathcal{A}_2$ are independent. Are $\sigma(\mathcal{A}_1)$ and $\sigma(\mathcal{A}_2)$ independent?
""Solution"". Obviously yes (sometimes with some blurry reasoning). Problem 5: $2n$ teams were divided to $2$ subgroups with n teams in each group. What is the chance that the $2$ strongest teams will play in the same subgroup? ""Solution"": $\frac12$ by symmetry.
(see discussion, e.g. math.stackexchange.com/questions/3369903/probability-problem-whats-the-chance-that-two-strongest-teams-will-play-in-the/3369914 ) Problem 6: ""A patient goes to see a doctor. The doctor performs a test with 99 percent reliability--that is, 99 percent of people who are sick test positive and 99 percent of the healthy people test negative. The doctor knows that only 0.01 percent of the people in the country are sick. If the patient test is positive, what are the chances the patient is sick?"" ""Solution"": 99 percent as follows immediately from the task. Some references: There are different paradoxes, such as Monty Hall problem, see, e.g. ""Paradoxes in Probability Theory and Mathematical Statistics"" by G. J. Székely.  There are some interesting examples of popular mistakes in ""The evolution with age of probabilistic, intuitively based misconceptions"" by E. Fischbein and D. Schnarch (and references therein). Of course, there are a huge number of mistakes that, in principle, can be made, but I mean, firstly, popular ones, and secondly, it is desirable that these were errors not on the most simple topics of combinatorics and not connected with typos and so on. What not-trivial mistakes did you see lot's of times? Addition: uninteresting examples : A coin is tossed twice at random. What is the probability of getting the same face?
""Solution:"" Three possible outcomes are HH, HT=TH, TT, where H = head, T = tail. So the probability is $\frac13$ . $E\xi^2 = (E \xi)^2$ because it's ""obvious"" because we are speaking about mean, and even reference: there was an equality $E \xi \eta = E\xi E \eta$ in one of the properties of expectation. For independent $\xi$ and $\eta$ we have $Var(\xi + \eta) = Var(\xi) + Var(\eta)$ ""hence"" $Var(\xi - \eta) = Var(\xi) - Var(\eta)$ . density of $U[0,1]$ is $F' = I_{[0,1]}(x)$ a.e., so density of $Pois(\lambda)$ is $F'= 0$ a.e.","['statistics', 'stochastic-processes', 'education', 'probability-theory', 'probability']"
4296809,Mistake computing $\int_0^\infty \left(\frac{\sinh(ax)}{\sinh(x)}-\frac{a}{e^{2x}} \right)\frac{dx}{x}$,"I am looking to evaluate the integral $$\int_0^\infty \left(\frac{\sinh(ax)}{\sinh(x)}-\frac{a}{e^{2x}} \right)\frac{dx}{x}=\ln\left(\frac{\pi}{\Gamma^2\left(\frac{1+a}{2b}\right)\cos\left(\frac{a\pi}{2}\right)}\right)$$ To this end I considered $$I(w)=\int_0^\infty \left(\frac{\sinh(ax)}{\sinh(bx)}-\frac{ad}{e^{cx}} \right)\frac{e^{-wx}}{x}\,dx \tag{1}$$ Note that as $w \to \infty$ the integrand vanishes. And as $w =0$ we recover the desired integral.
Differentiating $(1)$ w.r. to $w$ we obtain $$
\begin{aligned}
I^\prime(w)&=-\int_0^\infty \left(\frac{\sinh(ax)}{\sinh(bx)}-\frac{ad}{e^{cx}} \right)e^{-wx}\,dx\\
&=ad\int_0^\infty e^{-(c+w)x}\,dx- \int_0^\infty \frac{\sinh(ax)}{\sinh(bx)}e^{-wx}\,dx\\
&=\frac{ad}{c+w}-\int_0^\infty \frac{e^{ax}-e^{-ax}}{e^{bx}-e^{-bx}}e^{-wx}\,dx\\
&=\frac{ad}{c+w}-\int_0^\infty \frac{e^{-(w-a)x}-e^{-(w+a)x}}{e^{bx}-e^{-bx}}\,dx\\
&=\frac{ad}{c+w}-\int_0^\infty \frac{e^{-bx}}{e^{-bx}}\cdot\frac{e^{-(w-a)x}-e^{-(w+a)x}}{e^{bx}-e^{-bx}}\,dx\\
&=\frac{ad}{c+w}-\int_0^\infty \frac{e^{-(w-a+b)x}-e^{-(w+a+b)x}}{1-e^{-2bx}}\,dx\\
&=\frac{ad}{c+w}-\frac{1}{2b}\int_0^\infty \frac{e^{-\frac{(w-a+b)}{2b}x}-e^{-\frac{(w+a+b)}{2b}x}}{1-e^{-x}}\,dx \qquad (2bx \to x)\\
&=\frac{ad}{c+w}-\frac{1}{2b}\int_0^1 \frac{x^{\frac{(w-a+b)}{2b}-1}-x^{\frac{(w+a+b)}{2b}-1}}{1-x}\,dx \qquad (e^{-x} \to x)\\
&=\frac{ad}{c+w}-\frac{1}{2b}\left(\psi\left(\frac{w+a+b}{2b}\right)-\psi\left(\frac{w-a+b}{2b}\right)\right)\\
I(w)&=ad\int\frac{1}{c+w}\,dw-\frac{1}{2b}\left(\int\psi\left(\frac{w+a+b}{2b}\right)\,dw-\int\psi\left(\frac{w-a+b}{2b}\right)\,dw\right)\\
&=ad\ln(c+w)-\left(\ln\left(\Gamma\left(\frac{w+a+b}{2b}\right)\right)\,-\ln\left(\Gamma\left(\frac{w-a+b}{2b}\right)\right)\right)\\
&=ad\ln(c+w)+\ln\left(\frac{\Gamma\left(\frac{w-a+b}{2b}\right)}{\Gamma\left(\frac{w+a+b}{2b}\right)}\right)\\
\end{aligned}
$$ Now,our integral is equal to $$I=-\int_0^\infty I^\prime(w)\,dw=I(0)$$ Letting $w=0$ $$\begin{aligned}
\int_0^\infty \left(\frac{\sinh(ax)}{\sinh(bx)}-\frac{ad}{e^{cx}} \right)\frac{dx}{x}&=\ln\left(\frac{c^{ad}\Gamma\left(\frac12-\frac{a}{2b}\right)}{\Gamma\left(\frac12+\frac{a}{2b}\right)}\right)\\
&=\ln\left(\frac{c^{ad}\Gamma\left(\frac12-\frac{a}{2b}\right)\Gamma\left(\frac12+\frac{a}{2b}\right)}{\Gamma\left(\frac12+\frac{a}{2b}\right)\Gamma\left(\frac12+\frac{a}{2b}\right)}\right)\\
&=\ln\left(\frac{c^{ad}\pi}{\Gamma^2\left(\frac12+\frac{a}{2b}\right)\cos\left(\frac{a\pi}{2b}\right)}\right) \qquad \blacksquare\\
\end{aligned}$$ setting $b=1$ , $c=2$ and $d=1$ I obtained $$\int_0^\infty \left(\frac{\sinh(ax)}{\sinh(x)}-\frac{a}{e^{2x}} \right)\frac{dx}{x}=\ln\left(\frac{2^{a}\pi}{\Gamma^2\left(\frac{1+a}{2b}\right)\cos\left(\frac{a\pi}{2}\right)}\right)$$ Which has an extra term leading to an incorrect answer. Can someone please point out where I am mistaking?","['integration', 'calculus', 'gamma-function']"
4296810,An interesting problem involving recursion,"Given a continuous function $f:[0, 1] \rightarrow [0, 1]$ . Here we denote $f^n(x) = f(f^{n-1}(x))$ . For every $x_0 \in [0, 1]$ there exists $n \in \mathbb{N}$ such that $f^n(x_0) = 0$ . Prove that $f^N(x) \equiv 0$ for some $N$ . I've only managed to show that $f(0) = 0$ ( $f$ must have a fixed point but then it couldn't be anything than zero), hence all zeroes of $f^n(x)$ are also zeroes of $f^{n+1}(x)$ . Using Baire Theorem I concluded that there exists such $n_0 \in \mathbb{N}$ and a non-trivial open interval $(a, b)$ such that $\forall x \in (a, b) \ \ f^{n_0}(x) = 0$ (Let $A_n =$ $\{x \in [0,1] \mid f^n(x) = 0\}$ , then $\bigcup\limits_{i=1}^{\infty} A_{i} = [0, 1]$ and $[0, 1]$ is of second category) but I've no idea what to do next. How should I go about proving this statement?","['real-analysis', 'continuity', 'calculus', 'functional-analysis', 'baire-category']"
4296937,Egoroff's Theorem problem (Folland Real Analysis Ex 40),"The problem is to show that the theorem holds if we replace the condition that $\mu(X)<\infty$ with $|f_n|\leq g$ for all $n\ge 1$ , and $g\in L^1$ . I have completed half the problem but I am stuck on the other half. This is what I have so far Let $\epsilon>0$ . For $N\geq1$ , define $$G_N = \{x\;|\;g(x)\geq N^{-1}\}.$$ Then, since $g\in L^1$ , we have $\mu(G_n)$ is finite. Now, we can apply the original Egoroff's theroem, and get that there exists $E_N \subset G_N$ s.t. $\mu(E_N)<\frac{\epsilon}{2^N}$ and $f_n$ converges uniformly to $f$ on $G_N \setminus E_N.$ Let $$E=\bigcup_{N=1}^{\infty} E_N.$$ Then $$ \mu(E)\leq \displaystyle \sum \mu(E_N)=\epsilon$$ This is where I am stuck, I don't know how to show the uniform convergence on $E^c$ . Clearly $f_n$ converges uniformly to $f$ on $G^c$ , where $\displaystyle G=\bigcup_{N=1}^{\infty} G_N$ . So, at this point it would be enough to show uniform convergence on $G\setminus E$ . On any finite union of the $G_k \setminus E_k$ we have uniform convergence, so I imagine we need to use that. The one idea I have had is this: Define $$h_n(x)=\chi_{\bigcup\limits_{a=1}^n E_a \setminus G_a}(x)f(x)$$ The idea is to show that $h_n \rightarrow f$ uniformily, which, I think would be equivalent to showing that $f_n \rightarrow f$ uniformily, since $f_n \rightarrow f$ on every finite union of these sets, and also $h_n$ is in a sense equivalent to f restricted to $\displaystyle \bigcup_{a=1}^n E_a \setminus G_a$ . (please check this logic) Let $\epsilon > 0$ . Then, if $x\in \displaystyle\bigcup_{a=1}^N E_a \setminus G_a$ for some $N$ , $|h_n(x)-f(x)|=0<\epsilon$ for all $n\geq N$ . However, if $x$ is not in that union, then we need to show that $|f|<\epsilon$ . However, all we know is that $|f|\leq g$ . We do know that $g(x)\geq N^{-1}$ by construction but this doesn't seem helpful. What seems more helpful is that $g\in L^1$ , and hence $$\int_E g < \infty$$ For any subset $E\subset X$ . Perhaps we could use this? Although I don't know how.","['integration', 'measure-theory', 'analysis', 'real-analysis']"
4296954,Simple Random walk on $\mathbb{Z}$ find $P(S_3+S_1=S_4+S_6)$,Given state space $\mathbb{Z}$ and a markov chain $S_n=\sum_{k=1}^n X_k$ where $P(X_k=1)=1/2$ and $P(X_K=-1)=1/2$ Find $P(S_3+S_1=S_4+S_6)$ I have a formula to compute $P(S_n=k)$ so I believe I want to simplify to that form. My attempt: $P(S_3+S_1=S_4+S_6)=P(\sum_{k=1}^3 X_k+\sum_{k=1}^1 X_k=\sum_{k=1}^4 X_k+\sum_{k=1}^6 X_k)$ Rearranging the sums I get $P(X_1-X_4-\sum_{k=1}^6X_k=0)$ I think by symmetry I can say that $-\sum_{k=1}^6 X_k=\sum_{k=1}^6 -X_k=S_6$ So I could rearrange again to $P(S_6=X_4-X_1)$ I know that they are either $1$ or $-1$ with equal probability so do I have to compute the sum $P(S_6=2)+P(S_6=-2)+P(S_6=0)$ for the possible value of $X_4-X_1$ ?,"['statistics', 'markov-chains']"
4296968,Erroneous Proof that the Derivative is Continuous,"I have written the following which argues that for any arbitrary sequence of points such that $x_n \to c, x_n\neq c$ we must also have $f'(x_n)\to f'(c)$ which would imply that $f'(x)\to f'(c)$ as $x\to c$ , i.e., the derivative is continuous. However, I can't see where the error in my argument is. Let $f:[a,b] \to \mathbb{R}$ be a differentiable function and $c\in [a,b]$ .
Fix $\epsilon > 0$ . By assumption, for $c\in [a,b]$ , $f'(c)$ exists so put $\delta_1 > 0$ so small that whenever $0<\lvert x -c \rvert < \delta_1$ we must have $$ \left \lvert \frac{f(x)-f(c)}{x-c}-f'(c) \right \rvert < \frac{\epsilon}{2} $$ $[a,b]$ is closed so there exists a sequence such that $x_n \to c$ and $x_n \neq c$ . By assumption, $f'(x_n)$ exists so there exists a $\delta_2 >0$ such that whenever $0<\lvert x-x_n \rvert < \delta_2$ we must have $$ \left \lvert f'(x_n)-\frac{f(x)-f(x_n)}{x-x_n} \right \rvert < \frac{\epsilon}{2} $$ Now pick an integer $N$ so large that for $n\geq N$ we must have $0<\lvert x_n - c\rvert < \mathrm{min}(\delta_1,\delta_2)$ .
It follows that $$ \left \lvert \frac{f(x_n)-f(c)}{x_n-c}-f'(c)\right \rvert < \frac{\epsilon}{2}  \quad \textbf{and} \quad \left \lvert f'(x_n)-\frac{f(x_n)-f(c)}{x_n-c}\right \rvert < \frac{\epsilon}{2} $$ Applying the triangle inequality, we see that $\lvert f'(x_n)-f'(c) \rvert < \epsilon$ and we are done.","['continuity', 'derivatives', 'fake-proofs', 'real-analysis']"
4296977,"Find all positive integers $n$, such that $(\left\lfloor \sqrt{n} \right\rfloor^{2} +2) | (n^2 + 1) $","I tried to look at the cases when $n$ is a perfect square. Then $\left\lfloor \sqrt{n} \right\rfloor^{2} +2= n+2$ , $ n^2 + 1 =(n-2)(n+2) + 5$ . Then we must have $(n+2)|5$ .
But only $1$ and $5$ divide $5$ . Thus, $n=3$ , but that is not a solution since we assumed $n$ to be a perfect square. The problem therefore has no perfect-square solutions. I'm not sure how relevant this is to the general case, but I did not manage to get any further.
Thank you for your help in advance.","['number-theory', 'ceiling-and-floor-functions', 'divisibility']"
4296995,Additional conditions necessary to establish a property of the derivative of a bounded monotone function,"I am trying to find sufficient conditions for the following to be true. Suppose a function $f: \mathbb R \rightarrow \mathbb R$ is bounded, increasing and differentiable. What further conditions are necessary to establish that $\lim_{x\rightarrow \infty}x f'(x)=0$ ? Boundedness and monotonicity are sufficient to establish that $\lim \inf _{x\rightarrow \infty}𝑥𝑓′(𝑥)=0$ : Suppose $\lim \inf _{x\rightarrow \infty}𝑥𝑓′(𝑥)>0$ . Then exists a $\delta>0$ and $\epsilon>0$ such that when $x>\delta$ , $𝑥𝑓′(𝑥)>\epsilon$ . Then for 𝑥>𝛿, 𝑓′(𝑥)>𝜖/𝑥. The antiderivative of 𝜖/𝑥 is 𝜖ln𝑥 which converges to ∞ as 𝑥→∞. This contradicts the boundedness of 𝑓. Therefore $\lim \inf _{x\rightarrow \infty}𝑥𝑓′(𝑥)\leq 0$ . Since 𝑓 is increasing, it must be that $\lim \inf _{x\rightarrow \infty}𝑥𝑓′(𝑥)\geq 0$ . Combining these conditions yields $\lim \inf _{x\rightarrow \infty}𝑥𝑓′(𝑥)=0$ Also from 𝑓 increasing, we know $\lim \sup _{x\rightarrow \infty}𝑥𝑓′(𝑥)\geq 0$ . So it remains to find conditions that ensure that $\lim \sup _{x\rightarrow \infty}𝑥𝑓′(𝑥)= 0$ .","['calculus', 'functions']"
4297035,"Proving a function g: R^2 → {(x,y):x^2+y^2 ≤ 1} is injective. (function given in comments).","This is the function I am trying to prove is injective: $$g(x,y)=\Bigg(\frac{x}{\sqrt{x^2+y^2}+1},\frac{y}{\sqrt{x^2+y^2}+1}\Bigg).$$ I am proving it is injective by proving the contrapositive. Starting as so: $$g(x_1,y_1)=g(x_2,y_2)$$ $$\Bigg(\frac{x_1}{\sqrt{x_1^2+y_1^2}+1},\frac{y_1}{\sqrt{x_1^2+y_1^2}+1}\Bigg)=\Bigg(\frac{x_2}{\sqrt{x_2^2+y_2^2}+1},\frac{y_2}{\sqrt{x_2^2+y_2^2}+1}\Bigg)$$ I thought about trying to set each component of the ordered pair equal to each other like: $$\frac{x_1}{\sqrt{x_1^2+y_1^2}+1}=\frac{x_2}{\sqrt{x_2^2+y_2^2}+1}$$ And likewise for the second component. But I do not know where to start when it comes to the actual algebra (other than, it kind of is just obvious(?)) but yeah... any help to actually rigorously prove this is appreciated! The original question where this function originated is: Consider the subset $B=\{(x,y):x^2+y^2\leq 1\}\subseteq \mathbb{R}^2$ . Show that $\vert B\vert=\vert\mathbb{R}^2\vert$ . And using the Cantor-Bernstein-Schroeder theorem I found and proved an injective function from $B$ to $\mathbb{R}^2$ . This function was given by my textbook to prove, I would have no idea how to find such a function, and now I'm stuck proving it with no idea where to go forward.","['functions', 'relations']"
4297102,"For polynomials with integer coefficients $f(x)$, when is $\sqrt{f(x)+f'(x)}$ also a polynomial with integer coefficients?","Let $F$ be the family of polynomial functions with integer coefficients. When is it true that $f(x)\in F$ and $\sqrt{f(x)+f'(x)}\in F$ ? There are, of course, functions which meet this criteria. The most basic example is $f(x)=x^2+1$ , since $f'(x)=2x$ and thus $\sqrt{f(x)+f'(x)}=\sqrt{x^2+1+2x}=\sqrt{x^2+2x+1}=\pm (x+1)$ . In fact, for any $k\in\mathbb{Z}$ , $k^2(x^2+1)$ meets this criteria. Are there any other functions with this property?","['functions', 'derivatives', 'polynomials']"
4297124,"For a continuous function going through $(-1,0),(1,0),(2,3)$, show there are two distinct fixed points","Let $f : \mathbb{R}→\mathbb{R}$ be a continuous function such that $f(−1) = f(1) = 0, f(2) = 3$ . Show that there are distinct $a,b \in\mathbb{R}$ such that $f(a) = a$ and $f(b) = b$ . I'm pretty sure I use the Intermediate Value Theorem here, and to ensure $a \not = b$ , I think I would divide this problem up into two intervals $ [-1,1]$ and $[1,2]$ . But I have no idea how to do the proof.","['continuity', 'functions', 'real-analysis']"
4297155,Derivative of the Wronskian,"Consider a non-autonomous linear system of ode's: $X'=A(t)X$ , $X:\mathbb{R}\rightarrow\mathbb{R}^n$ . Let $B(t)$ be a fundamental matrix solution $\dot{B}=A(t)B$ of the system and $W(t):=\det B(t)$ the Wronskian. Show that $$\dot{W}=tr(A(t))W.$$ My idea: I started by induction. For $n=2$ we have $B=\left(\begin{array}{cc}
x_{1} & y_{1}\\
x_{2} & y_{2}
\end{array}\right)$ and $$W	=\det B	=x_{1}y_{2}-y_{1}x_{2}.$$ Therefore, $$
\begin{eqnarray}
\dot{W}	&=&\dot{x_{1}}y_{2}+x_{1}\dot{y_{2}}-\dot{y_{1}}x_{2}-y_{1}\dot{x_{2}}\\
	&=&\dot{x_{1}}y_{2}-\dot{y_{1}}x_{2}+x_{1}\dot{y_{2}}-y_{1}\dot{x_{2}}\\
	&=&\det\left(\begin{array}{cc}
\dot{x_{1}} & x_{2}\\
\dot{y_{1}} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & \dot{x_{2}}\\
y_{1} & \dot{y_{2}}
\end{array}\right)\\
	&=&\det\left(\begin{array}{cc}
a_{11}x_{1}+a_{12}x_{2} & x_{2}\\
a_{11}y_{1}+a_{12}y_{2} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{21}x_{1}+a_{22}x_{2}\\
y_{1} & a_{21}y_{1}+a_{22}y_{2}
\end{array}\right)\\
	&=&\det\left(\begin{array}{cc}
a_{11}x_{1} & x_{2}\\
a_{11}y_{1} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
a_{12}x_{2} & x_{2}\\
a_{12}y_{2} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{21}x_{1}\\
y_{1} & a_{21}y_{1}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{22}x_{2}\\
y_{1} & a_{22}y_{2}
\end{array}\right)\\
	&=&\det\left(\begin{array}{cc}
a_{11}x_{1} & x_{2}\\
a_{11}y_{1} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{22}x_{2}\\
y_{1} & a_{22}y_{2}
\end{array}\right)\\
	&=&a_{11}\det\left(\begin{array}{cc}
x_{1} & x_{2}\\
y_{1} & y_{2}
\end{array}\right)+a_{22}\det\left(\begin{array}{cc}
x_{1} & x_{2}\\
y_{1} & y_{2}
\end{array}\right)\\
	&=&(a_{11}+a_{22})\det\left(\begin{array}{cc}
x_{1} & x_{2}\\
y_{1} & y_{2}
\end{array}\right)\\
	&=&tr(A)W.
\end{eqnarray}$$ Now I suppose that it is valid for $n>2$ and consider taking $n+1$ and the formula for the determinant $$W=\sum_{k=1}^{n+1}(-1)^{i+k}b_{ik}\det B_{ik}$$ where $B_{ik}$ is the $ik$ -minor of $B$ . Up to here I haven't been able to advance. Any suggestions?","['wronskian', 'ordinary-differential-equations']"
4297371,When does a finite group $G$ have an element of order equal to its exponent?,"The following question came up to my mind while I was playing with the exponent. There is no other motivation except plain curiosity: Question. If $G$ is a finite group, denote by $e(G)$ its exponent. Can we characterize finite groups $G$ which have an element of order $e(G)$ ? Examples and non examples. An abelian group $G$ has an element of order $e(G)$ A $p$ -group has an element of order $e(G)$ More generally (as pointed out by @Brauer Suzuki), a finite nilpotent group $G$ has an element of order $e(G)$ A non cyclic group $G$ whose Sylow subgroups are cyclic (eg $D_n$ for $n$ odd) has no element of order $e(G)$ . Indeed, in this case, $e(G)=\vert G\vert$ . For any $e\geq 1$ and any finite group $H$ such that $e(H)\mid e,$ $G=H\times \mathbb{Z}/e\mathbb{Z}$ has an element of order $e(G)$ Finite groups $G$ satisfying the required property are not necessarily isomorphic to $H\times \mathbb{Z}/e(G)\mathbb{Z}$ for  some suitable $H$ , since the Heisenberg group of order $p^3$ is a counterexample. Because of the last three items, I do not expect the answer to be easy (it is plausible that there is no satisfactory answer), but I am not an expert in group theory, so maybe there is a nice  characterization in terms of linear representations, for example ? or in terms of derived subgroup ?","['group-theory', 'finite-groups']"
4297393,"Fourier in $\mathbb{Z}^d$ and use of Pigeonhole principle (Prop. $1.1.13$, HOFA by T. Tao)","We use the following notation: $$[N]:=\{1,2,3,\cdots,N\}$$ $$\mathbb{E}_{n\in [N]}f(x) : = \frac{1}{N}\sum_{n \in [N]}f(x)$$ This doubt is from the book titled Higher Order Fourier Analysis by T. Tao. The doubt is from the proof of proposition 1.1.13. The book states that: We have that $$\left \vert \mathbb{E}_{n\in [N] } f(x_n)\right \vert >\delta.$$ Now, using the kernel bounds $$\int_{\mathbb{T}^d}K_R=1$$ where $K_R$ is the Fejer kernel $$K_R(x_1,\cdots,x_d):=\prod_{j=1}^{d}\frac{1}R\left(\frac{\sin(\pi R x_j)}{sin(\pi x_j)} \right)^2$$ and \begin{equation}  
\tag{1}|K_R(x)|\ll_d \prod_{j=1}^{d}R(1+R ||x_j||_{\mathbb{T}})^{-2},\end{equation} (where $f\ll_d g$ means that $f\leq C_d g$ where the constant $C$ depends on $d$ and $||\cdot||$ means the nearest distance to an integer ) and the Lipschitz nature of $f$ , we see that \begin{equation}\tag{2}F_Rf(x) = f(x) + O_d(1/R).\end{equation} Thus, if we choose $R$ to be a sufficiently small multiple of $\frac{1}{\delta}$ , we get $$\Big\vert\mathbb{E}_{n \in [N]}F_Rf(x_n) \Big\vert \gg \delta$$ and thus by pigeonhole principle we have \begin{equation}\tag{3} \Big\vert\mathbb{E}_{n \in [N]}e(k\cdot x_n) \Big\vert \gg_d \delta^{O_d(1)}\end{equation} for some non-zero $k$ of magnitude $|k| \ll_d \delta^{-O_d(1)} .$ i) How does $(1)$ and $(2)$ follow? ii) How does $(3)$ follow from pigeonhole principle and the fact that $\Big\vert\mathbb{E}_{n \in [N]}F_Rf(x_n) \Big\vert \gg \delta$ ? My idea is the following: $\Big\vert\mathbb{E}_{n \in [N]}F_Rf(x_n) \Big\vert \gg \delta \implies \Big\vert\mathbb{E}_{n \in [N]}\sum_{k \in \mathbb{Z}^d }m_R(k)\hat{f}(k)e(k \cdot x) \Big\vert \gg \delta $ Now, using the trivial bound $\hat{f}(k) = O(1)$ and $\hat{f}(0) =0$ ... But how do we get $\delta$ raised to some power $(O_d(1))$ and how to use pigeonhole principle?","['pigeonhole-principle', 'additive-combinatorics', 'fourier-analysis', 'discrete-mathematics']"
4297429,convergence $\sum_{n=1}^{\infty}\frac{1}{n}\sin\left(\frac{\pi}{n^{2}}\right)$,"I'm checking convergence of the series $\sum_{n=1}^{\infty}\frac{1}{n}\sin\left(\frac{\pi}{n^{2}}\right)$ using the integral test. I calculated the integral $\int_{1}^{+\infty}\frac{1}{x}\sin\left(\frac{\pi}{x^{2}}\right)dx$ using substitution $u=\frac{\pi}{x^{2}}$ , I got: $\frac{1}{2}\int_{0}^{\pi}\frac{\sin u}{u}du$ but I don't know what to do next thanks for any help, and sorry if I have English mistakes.","['integration', 'sequences-and-series']"
4297479,"Growth/decay rate of $a_1 = \frac{5}{2}, a_{n + 1} = \frac{1}{5}(a_n^2 + 6)$","this might be a vague question but I am interested in the recurrence relation defined by $a_1 = \frac{5}{2}$ and $a_{n + 1} = \frac{1}{5}(a_n^2 + 6)$ . Some properties I have proven include that $2 < a_n \leq \frac{5}{2}$ is strictly decreasing converges to $2$ . The next thing I did was looking at the difference $r_i = a_i - 2$ , for which I found to 0.5, 0.45, 0.4005, 0.35248, 0.3068325, 0.2642952, 0.2254066, 0.1904869, 0.1596466, 0.1328146 . Clearly there is some ""near-""exponential decay going on here. One more result I have proven is that $\frac{8}{10}r_i < r_{i + 1}\leq \frac{9}{10}r_i$ . However I am unable to show any further results about the error term $r_i$ - obviously it tends to $0$ exponentially with $0.8 < r \leq 0.9$ but can we do better? So my question is, is there more precise bounds for $a_{i + 1} - 2$ , whether by relating to $a_i - 2$ or in general? (Also note that the original quadratic recurrence doesn't seem to be solvable by existing techniques.)","['algebra-precalculus', 'sequences-and-series']"
4297536,Showing that $\arctan(\cos(\alpha)*\tan(x)) = \arccos(\dfrac{\cos(x)}{\cos(\arcsin(\sin(\alpha)*\sin(x)))})$,"In trying to calculate an arc length of a right spherical triangle, I reached this $\arccos$ expression. However, I see the expression given in the manuals for the same problem is much simpler. checking against several values of $\alpha$ and $x$ (Let them both be between $0$ and $\pi/2$ ) shows those expressions are indeed equivalent. $$\arctan(\cos(\alpha)*\tan(x)) = \arccos(\dfrac{\cos(x)}{\cos(\arcsin(\sin(\alpha)*\sin(x)))})$$ I tried to prove this equivalence using some trigonometric identities, but I'm quite far it seems.",['trigonometry']
4297569,In how many ways can you sit 5 people in a row of 20 seats if no 2 can sit together?,"I've seen the simpler problem of just sitting 2 people in non consecutive seats. In that case, I would subtract from the total number of ways to sit the 2 persons the number of ways of sitting them together. In this harder version of the problem,I've though of the same thing, but now considering the case were 2, 3, 4 or 5 sit together.
But that seems to count duplicate cases.","['combinatorics', 'discrete-mathematics']"
4297600,How do I solve the first order nonlinear ODE? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $$\dfrac{dy}{dx} = \dfrac{(1+ky-y/x)}{(1-y^2)}$$ $$y(0) = 0$$ I have tried using change of variables such as $x=y^2z$ , or $x=yz$ , however, I am still not able to separate the equation. Any thoughts on whether i should consider a different change of variable?",['ordinary-differential-equations']
4297639,Why is a linear system of equations considered consistent when a solution contains 0/0?,"Suppose I have a system of equations as such, with $\alpha$ and $\beta$ being constants: $$
x_1+5x_2+x_3=1\\
x_1+6x_2-x_3=1\\
2x_1+\alpha x_2-6x_3=\beta
$$ Suppose I want to calculate for which values of $\alpha$ and $\beta$ my system is consistent. I can do that by making a coëfficiënt matrix and using Gaussian elimination to derive the solution space, it being: $$
x_1=1-\frac{11\beta -22}{2\alpha-28}\\
x_2=\frac{\beta -2}{\alpha -14}\\
x_3=\frac{\beta -2}{2\alpha -28}
$$ Then I can see that for the value $\alpha=14$ the denominator = 0, which is not possible. So, I can conclude that the system is only consistent for $\alpha\neq14$ .
This is clear to me. However, my answer sheet says that $\alpha=14$ and $\beta=2$ is also an answer. Plugging these values into $x_1$ , $x_2$ and $x_3$ gives these answers: $$
x_1=1-\frac{0}{0}\\
x_2=\frac{0}{0}\\
x_3=\frac{0}{0}
$$ I thought $\frac{0}{0}$ was supposed to be undefined? How can it be then that using these values for $\alpha$ and $\beta$ , we get a consistent system? To try to figure it out, I checked the answer of $A\overrightarrow{x}=\overrightarrow{b}$ , which gives me this (I think): $$
\begin{bmatrix}1 & 5 & 1\\1 & 6 & -1\\2 & \alpha & -6\end{bmatrix}
\begin{bmatrix}1-\frac{0}{0}\\\frac{0}{0}\\\frac{0}{0}\end{bmatrix}
= 
\begin{bmatrix}1-\frac{0}{0}+5(1-\frac{0}{0})+1-\frac{0}{0}\\\frac{0}{0}+6\frac{0}{0}-\frac{0}{0}\\2\frac{0}{0}+\alpha\frac{0}{0}-6\frac{0}{0}\end{bmatrix}
$$ Which in turn implies that the equation below must be true, since the answer sheet says that the system is consistent: $$
\begin{bmatrix}1-\frac{0}{0}+5(1-\frac{0}{0})+1-\frac{0}{0}\\\frac{0}{0}+6\frac{0}{0}-\frac{0}{0}\\2\frac{0}{0}+\alpha\frac{0}{0}-6\frac{0}{0}\end{bmatrix}
=
\begin{bmatrix}1\\1\\\beta\end{bmatrix}
$$ If I substitute $\frac{0}{0}$ for something else (lets say $\frac{0}{0}=p$ ), I get this: $$
\begin{bmatrix}1-p+5(1-p)+1-p\\p+6p-p\\2p+\alpha p-6p\end{bmatrix}
=
\begin{bmatrix}7-7p\\6p\\\alpha p-4p\end{bmatrix}
=
\begin{bmatrix}1\\1\\\beta\end{bmatrix}
$$ I get this equation, which when looking at the first 2 components of the vectors already doesn't make any sense, since for $7-7p=1$ to make sense, $p$ would have to be $\frac{6}{7}$ . But for $6p=1$ , $p$ would have to be $\frac{1}{6}$ .",['linear-algebra']
4297651,Evaluate the integral : $\int_0^\infty (-1)^{ \lfloor x \sin x \rfloor } dx$,"How to evaluate the following integral? $$ \int_0^\infty (-1)^{ \lfloor x \sin x  \rfloor } dx$$ I have no idea how to calculate this improper integral. Maybe I have to use some property of floor functions to reduce $ (-1)^{ \lfloor x \sin x  \rfloor } $ to something simpler I can work with, but I don't know what property. Thanks in advance for help! Edit: I reached the following, For every $ k \in \mathbb{Z_+} $ we have that $ \int_0^{\pi \cdot (k+1)} (-1)^{ \lfloor x \sin x  \rfloor } dx = \sum_{m=0}^k \int_{\pi \cdot m}^{ \pi \cdot (m+1)} (-1)^{ \lfloor x \sin x  \rfloor } =  \{ x = t + \pi \cdot m , dx = dt \} = \sum_{m=0}^k \int_{0}^{ \pi} (-1)^{ \lfloor ( t+\pi \cdot m)\cdot( \sin(t+\pi \cdot m) )  \rfloor } dt = \sum_{m=0}^k \int_{0}^{ \pi} (-1)^{ \lfloor ( t+\pi \cdot m)\cdot( \sin(\pi \cdot m )\cos(t) + \sin(t)\cos(\pi \cdot m)  )  \rfloor } dt = \sum_{m=0}^k \int_{0}^{ \pi} (-1)^{ \lfloor ( t+\pi \cdot m)\cdot  (-1)^m \cdot \sin(t)   \rfloor } dt $ . If I can get a formula for the last integral from $ 0 $ to $ \pi $ I'll have an answer whether the integral from $ 0 $ to $ \infty $ converges or not.","['integration', 'calculus', 'ceiling-and-floor-functions']"
4297669,Proof of Faà di Bruno formula,"By induction, I want to prove the Faà di Bruno formula for $n$ -times differentiable $f,g$ \begin{align*}
        D^n(f\circ g) = \sum_{(k_1,\ldots,k_n)\in T_n} \frac{n!}{k_1!\cdots k_n!} \Big(\big(D^{k_1+\ldots+k_n} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^n \bigg( \frac{D^m g}{m!}\bigg)^{k_m},
    \end{align*} where $T_n := \big\{k\in \mathbb{N}_0^n : \sum_{j=1}^n j k_j = n\big\}$ . Induction step: \begin{align*}
            D^n (f\circ g) &= D D^{n-1} (f\circ g) \\
            &= D \Bigg( \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\
            &= \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} D\Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\
            &= \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}+1} f\big) \circ g\Big) Dg \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m} \\
            &\hspace{0.5cm}+ \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\
            &= \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}+1} f\big) \circ g\Big) Dg \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg) \\
            &\hspace{0.5cm}+ \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg(\Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg)\\
            &= \Bigg(\sum_{(k_1,\ldots,k_{n})\in (T_{n-1},0)} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+1+\ldots+k_{n-1}} f\big) \circ g\Big) \bigg(\frac{D^1g}{1!}\bigg)^{k_1+1} \prod_{\substack{m=2\\k_m\geq 1}}^{n} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg) \\
            &\hspace{0.5cm}+ \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg(\Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg)\\
        \end{align*} I don't know how to split $T_n$ so that I arrive at the correct sum, has anybody an idea?","['derivatives', 'summation', 'combinatorics']"
4297764,Spherical cap area is $\pi r^2$. But why?,"(If you're surprised by the title — $r$ is not what you (perhaps) think it is : ) Let $x$ be a point on a sphere $S$ and let $U$ be some sphere with center $x$ that intersects $S$ . Claim¹. The spherical cap cut out from $S$ and the circle cut out from $T_xS$ (tangent plane to $S$ at $x$ ) have the same area. In other words, the area of a spherical cap is $\pi r^2$ where $r$ is the distance from its center $x$ to its boundary. (So we have a very simple formula for circle area in spherical geometry. But it's somewhat strange: $r$ is neither spherical nor Euclidean radius of the circle!) Question. How to prove this geometrically? It can be proved by direct computation² but surely there should be… some explanation why this map from a sphere to a plane is area-preserving, perhaps? One interesting special case is $r=2R$ (where $R$ is the radius of $S$ ). The 'cap' in this case is the whole sphere — and $\pi r^2=4\pi R^2$ . So a (good) answer to my question would give (yet another) explanation of sphere's surface area formula. ¹ I learned this from A. Akopyan. ² For example: the height of the cap is $r^2/2R$ (because $h/r=(r/2)/R$ ), so by Archimedes' hatbox lemma the area is $2\pi R\cdot r^2/2R=\pi r^2$ .","['area', 'geometry', 'spherical-geometry']"
4297780,Show that $\mathbb E[\exp(-\lambda(T_{a}\land T_{b}))]=\cosh(\frac{a+b}{2}\sqrt{2\lambda})/\cosh(\frac{a-b}{2}\sqrt{2\lambda})$,"Let $a < 0 < b$ and further for a standard Brownian motion $(B_{t})$ define the stopping times $T_{x}:=\inf\{t \geq 0: B_{t}=x\}$ . Show that: $$\mathbb E[\exp(-\lambda(T_{a}\land T_{b}))]=\cosh(\frac{a+b}{2}\sqrt{2\lambda})/\cosh(\frac{a-b}{2}\sqrt{2\lambda})$$ My attempt: I have already shown that $\mathbb E[\exp(-\lambda T_{a})]=\exp(-a\sqrt{2\lambda}) \; (*)$ through the Optional sampling theorem as well as the choice of martingale $\left(\exp(\alpha B_{t}-\frac{\alpha^{2}}{2}t)\right)_{t\geq 0}$ , i.e. choosing the martingale $\left(\exp(\sqrt{2\lambda} B_{t}-\lambda t)\right)_{t\geq 0}$ , we obtain $(*)$ . Now I am supposing we need to find a suitable exponential martingale again, but I am struggling to find one, any suggestions? Additional question: How does the knowledge of $$E[\exp(-\lambda(T_{a}\land T_{b}))]=\cosh(\frac{a+b}{2}\sqrt{2\lambda})/\cosh(\frac{a-b}{2}\sqrt{2\lambda})$$ allow us to deduce $\mathbb E[T_{a}\land T_{b}]=b\lvert a \rvert$ ? I cannot see it.","['stochastic-calculus', 'martingales', 'stopping-times', 'probability-theory', 'probability']"
4297899,What is the number of non-increasing 4 digit numbers?,"This problem, though quite simple, has stumped my teenage mind. How many 4-digit numbers are there whose digits are non-increasing? This seemed quite simple at first, meaning I have the digits [9 8 7 6 5 4 3 2 1 0], and I just pick four numbers from that which gives me the answer of 10C4, 210, but this fails to include the fact that a possible solution could be 9999, or 9988, or 6332, a number with repeating digits. How would I go about including this in an equation? What would that equation look like? I presume the solution would be something relating to the casework method, but I'm not very familiar with it in the first place.","['combinatorics-on-words', 'combinatorics']"
4297923,Reasoning behind choosing appropriate decomposition when solving linear equation.,"Now I am reading the book ""Introduction to linear algebra"" by Gilbert Strang. I understand all the technical details regarding LU, QR and SVD decompositions, but I get completely confused when it comes to choosing between them for a particular case. I also know the following time complexities: LU decomposition: $\mathcal O\left(n^3\right)$ for square matrix of size $n \times n$ ; QR decomposition: $\mathcal O\left(mn^3\right)$ for rectangular matrix of size $m \times n$ ; SVD decomposition: $\mathcal O\left(n^3\right)$ for square matrix of size $n \times n$ . Also, time complexity for solving $Ax = b$ as $x = A^{-1}b$ is $\mathcal O\left(n^2\right)$ if we already know $A^{-1}$ . But if you already have your matrices $L$ and $U$ , time complexity for solving $LUx = b$ is the same. So, as I know, in this case using LU decomposition is only considered better for numerical reasons. QR decomposition doesn't speed up the process either. QR decomposition can make calculation of least squares solution simpler, since the equation becomes $\hat x = R^{-1}Q^{\mathrm T}b$ . But taking into account the fact you first need to calculate QR decomposition itself and only after that apply least squares, then this QR decomposition doesn't speed up anything. I also know it's a good idea to use LU or QR, when you always have the same matrix $A$ and different $b$ 's. But not considering this use case, are all these decompositions used only for numerical stability? Because I don't actually see how they can speed up calculations. But it seemed to me those are just little pieces of a big picture, since I am not sure what decomposition to choose when it comes to a real problem. I encountered a lot of related questions on this site, but they all seemed to ask about some particular detail regarding some particular decomposition whereas I'm asking about all them at once. Could someone make it clear when one should use each decomposition and why? Some examples of use cases would be highly appreciated.","['linear-algebra', 'numerical-linear-algebra', 'optimization', 'computational-complexity', 'matrix-decomposition']"
4297949,Implementing binary logistic regression from scratch,"Background knowledge: To train a logistic regression model for a classification problem with two classes (called class $0$ and class $1$ ), we are given a training dataset consisting of feature vectors $x_1, x_2, \ldots, x_N \in \mathbb R^d$ and corresponding target values $y_1, y_2, \ldots, y_N \in \{0,1\}$ . Our goal is to find numbers $\beta_0, \beta_1, \ldots, \beta_d \in \mathbb R$ such that $$
\tag{1} y_i \approx \sigma(\beta_0 + \beta_1 x_{i1} + \cdots + \beta_d x_{id}) \quad \text{for } i = 1, \ldots, N.
$$ Here $x_{i1}, \ldots, x_{id}$ are the components of the feature vector $x_i$ , and $\sigma:\mathbb R \to \mathbb R$ is the sigmoid function (also called ""logistic function"") defined by $$
\sigma(u) = \frac{e^u}{1 + e^u} \quad \text{for all } u \in \mathbb R.
$$ The sigmoid function is useful in machine learning because it converts a real number into a probability (that is, a number between $0$ and $1$ ).
Equation (1) can be expressed more concisely using vector notation: we hope that $$
y_i \approx \sigma(\hat x_i^T \beta) \quad \text{for } i = 1, \ldots N
$$ where $$
\hat x_i = \begin{bmatrix} 1 \\ x_{i1} \\ \vdots \\ x_{id} \end{bmatrix} \quad \text{and} \quad \beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_d \end{bmatrix}.
$$ (The vector $\hat x_i$ is called an ""augmented"" feature vector. It's the vector you get by prepending a $1$ to $x_i$ .)
I think of $\sigma(\hat x_i^T \beta)$ as being a ""predicted probability"" which tells you how strongly our model believes that example $i$ belongs to class $1$ . And $y_i$ is
a ""ground truth"" probability which reflects certainty about whether or not example $i$ belongs to class $1$ . We will use the binary cross-entropy loss function $$
\ell(p,q) = -p \log(q) - (1 - p) \log(1 - q)
$$ to measure how well a predicted probability $q \in (0,1)$ agrees with a ground truth probability $p \in [0,1]$ . Here $\log$ denotes the natural logarithm. The vector $\beta$ will be chosen to minimize the average cross-entropy $$
\tag{2} L(\beta) = \frac{1}{N} \sum_{i=1}^N \ell(y_i, \sigma(\hat x_i^T \beta)).
$$ We can minimize $L(\beta)$ using an optimization algorithm such as gradient descent, stochastic gradient descent, or Newton's method. In order to use such methods, we need to know how to compute the gradient of $L$ . For Newton's method, we also need to know how to compute the Hessian of $L$ . Question: How can we compute the gradient and the Hessian of the average cross-entropy function $L(\beta)$ defined in equation (2)? The goal is to compute the gradient and the Hessian of $L$ with finesse , making good use of vector notation. (I'll post an answer below.)","['statistics', 'machine-learning', 'multivariable-calculus', 'logistic-regression', 'optimization']"
4297952,Combinatorics and set theory question,"Problem: Let $A_n=\{ 1,2,...,n \}$ .
We define the ordered triplet $(A,B,C)$ as good if $A \cup B \cup C = A_n$ and $\operatorname{card}(A \cap B \cap C)=2$ (so the intersection has exactly $2$ elements). Let $p_n$ be the number of such interesting pairs. Is there a formula for $p_n$ , and if so, is the fraction $\frac{p_{n+1}}{p_n}$ constant? Question: I have created a very simple backtrack algorithm that calculated $p_3=18$ , $p_4=216$ and $p_5=2160$ . I don't see any obvious formula for this, as it may be an arithmetic progression.
It is clear that the number of possbible intersections $A \cap B \cap C$ is $n(n-1)/2$ . So the question remains in how many possible ways can we arrange such sets such that they will all include at least once all the elements. Can someone shed some light on what theoretical notion I am forgetting about, and give a hint for the solution? Any help would be appreciated.","['elementary-set-theory', 'combinatorics']"
4297954,"Solving $(x^2 +1)y''+2xy=0, y(0)=1, y'(0)=1$ by power series","I have to resolve this differential equation: \begin{eqnarray*}
(x^2 +1)y''+2xy=0, \hspace{1cm}y(0)=1, y'(0)=1
\end{eqnarray*} by power series. So I know that: \begin{eqnarray*}
y&=&\sum_{n=0}^{\infty}c_{n}x^{n}\\
y'&=&\sum_{n=1}^{\infty}nc_{n}x^{n-1}\\
y''&=& \sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}
\end{eqnarray*} Then \begin{eqnarray*}
(x^2 +1)\sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}+2x\sum_{n=0}^{\infty}c_{n}x^{n}&=&0\\
 \sum_{n=2}^{\infty}n(n-1)c_{n}x^{n}+\sum_{n=2}^{\infty}n(n-1)c_{n}x^{n-2}+\sum_{n=0}^{\infty}2c_{n}x^{n+1}&=&0\\
\end{eqnarray*} So I do the substitution $k=n-2$ and $k=n+1$ and I have \begin{eqnarray*}
 \sum_{k=2}^{\infty}k(k-1)c_{k}x^{k}+\sum_{k=0}^{\infty}(k+2)(k+1)c_{k+2}x^{k}+\sum_{k=1}^{\infty}2c_{k-1}x^{k}&=&0\\
2c_{2}+6c_{3}x+2c_{0}x+\sum_{k=2}^{\infty}[k(k-1)c_{k}+(k+2)(k+1)c_{k+2}+2c_{k-1}]x^{k}&=&0
\end{eqnarray*} But in general: \begin{eqnarray*}
 k(k-1)c_{k}+(k+2)(k+1)c_{k+2}+2c_{k-1}=0
\end{eqnarray*} And I don't know how can I continue, Cause I can't find $c_{1}$ and in general I don't know how can I write $c_{k}$ . I'm stuck. Do you know how can I continue? Can you give some hint to continue? Thank you.","['power-series', 'ordinary-differential-equations']"
4297960,Obscure passage in the calculus of a limit,"Here is the multivariable limit with $\lambda\in\mathbb R$ $$\lim_{(x,y)\to (0,0)\\(x,y)\neq(0,0)} \frac{1-\cos(x^{3}y^{2\lambda})}{(x^{2}+y^{2})^{2\lambda+1}}$$ I'm studying 2 different cases: $\lambda\ge0$ and $\lambda < 0$ . Let $\lambda$ be strictly negative: $(x^{2}+y^{2})^{2\lambda+1}$ is evaluated to a negative power if $\lambda < -\frac{1}{2}$ . So the limit is equal to $0$ . But how can I study the subcase $-\frac{1}{2}\le\lambda<0$ ? If I exclude the the direction $y=0$ (x-axis) I find that the limit exist and is equal to zero but how about the x-axis? The limit doesn't exist? Or the domain is not defined in $y=0$ for $-\frac{1}{2}\le\lambda<0$ ? I don't understand this conceptual passage. Thanks in advice!","['multivariable-calculus', 'limits', 'calculus', 'analysis']"
4297975,Prove that $n^2-1$ is divisible by 8 for any odd integers n.,"Below is my proof and I am confused about a few points. I am not sure the final lines are correct as I know that showing 2,4 are factors of $4k^2 + 4k$ is enough to prove that it is divisible by 8 and I have looked at some other examples. In an example of 30, both 3 and 6 are factors of 30 but 30 is not divisible by 18. But I am stuck on how to modify this proof to be complete. To prove this statement, I intend to use direct proof. Since n has to be an odd integer as prescribed in the statement, by the definition of odd numbers, $(2k+1)^2-1$ must be divisible by 8 where $n = 2k+1$ for some integer k. Next, we expand $(2k+1)^2-1$ to be $4k^2 + 4k + 1 - 1$ , which simplifies to $4k^2 + 4k$ . First, we use the distributive property to get the following $4(k^2 + k)$ . We let $m = k^2 + k$ and therefore $4(k^2 + k)$ = $4m$ . Hence, we know that $4k^2 + 4k$ must be divisible by 4. Then, we use the distributive property to factor 4k from the expression to get $4k(k + 1)$ . By the definition of even and odd number, if k is odd then k+1 must be even and if k is even then k+1 is odd. As an integer is Since 2 and 4 are common factors of $4k^2 + 4k$ , which means 8 must also be a factor of $4k^2 + 4k$ . $n^2-1$ is therefore divisible by 8 where $n = 2k+1$ for some integer k. Therefore, we have proven that $n^2-1$ is divisible by 8 for any odd integers n.",['discrete-mathematics']
4297978,"If $f:M\to N$ is locally injective in the connected metric space $M$, and $g: N\to M$ such that $f(g(x))=x$ then $f$ is a homeomorphism",Let $f: M\to N$ continuous and locally injective on the connected metric space $M$ . ( $N$ also is a metric space). If there is a continuous function $g: N\to M$ such that $f(g(x))=x$ for all $x\in N$ then $f$ is a homeomorphism between $M$ and $N$ . I've tried a lot of things to solve this. For example i have tried to show that $f$ is locally a homeomorphism and use the connectedness of $M$ to extend this for a global homeomorphism. Any hint will be appreciated!,"['general-topology', 'metric-spaces']"
4298011,"Stack of Dices - Number of ""visible"" sides","i came across a ""math-problem"" for students in middleschool:
A stack of dices is placed on top of each other and the task is to find a term that describes the number of sides that are not covered by the table or a subsequent dice. I am not satisfied with the solution given in the textbooks and my school days are (unfortunately) too long ago to ask my old maths teacher, so I am trying my luck here ;) I have linked the problem here (it is in German, but I am only interested in the visual representation). The solution suggested is 4x+1=""number of sides"". What bothers me is that it doesn't include the case where there is no cube. So if x is 0, the result of the number of sides should also be 0. I have created a table of values with my maths skills and entered it into wolframAlpha , but am failing to come up with a correct function that ""gives"" ""my"" solution. I hope I have expressed myself clearly and that someone can help me. Many thanks in advance!",['functions']
4298044,Find $\inf$ and $\sup$ of $a_n = \frac{a_{n-1} + a_{n-2}}{a_{n-3}}$,"Sequence $a_n$ is defined in the following way: $a_1 = a_2 = a_3 = 1$ and for $n > 3$ : $a_n = \frac{a_{n-1} + a_{n-2}}{a_{n-3}}$ . Find $\inf$ and $\sup$ of $A = \{a_n  | n \in  \mathbb{N}\}$ . Edit : turns out finding out $\sup$ and $\inf$ is a hard task not only for me. I welcome any interesting findings about this sequence in the comments/answers. For examples improving obvious lower bound or providing an upper bound. I've wrote a script in Python and it seems that $\inf A \approx 0.81722$ and $\sup A \approx 5.13748$ . Also, the sequence itself behaves a bit like a sin function. I would like to hear ideas on how to find sup and inf. (I struggle even to show that $a_n$ is bounded from above). Graph of $a_n$ values for $n=1,2,...,100$ :","['supremum-and-infimum', 'recurrence-relations', 'sequences-and-series']"
4298064,Ways to put $50$ balls into $6$ distinct urns with an odd number in each urn?,"Say there are $50$ identical balls. We have $6$ different containers (note that these aren't named or anything, simply just distinct). How many ways can we put these balls into the urns so that each urn has an odd number? Okay. So, you have $23$ odd numbers from $1-50$ and six different urns. This leads you to say $\binom{23}{6}$ , though that will not work due to the fact that these numbers you're selecting have a property: They must add up to $50$ . How would I go about adding this property to my equation? How would I make an equation based on this? What kind of formula/theory is this question asking for? What am I missing in my equation or thought process?","['algebra-precalculus', 'combinatorics', 'problem-solving']"
4298075,Continuity of minimum function on topological spaces,"The question I have is the following: Suppose $X,Y$ are compact Hausdorff topological spaces and $f: X \times Y \longrightarrow \mathbb{R}$ is continuous. Define $g: X \longrightarrow \mathbb{R}$ by $g(x) = \min_{y \in Y} f(x,y)$ .  Show that $g$ is continuous on $X$ using basic definitions of continuity.  I hypothesized using the closed graph theorem ( $g$ is a closed map) but the prompt wants me to use basic characterizations of continuity. My basic approach was to let $(x_n)_{n=1}^{\infty} \subset X$ converge to $x \in X$ .  Then for each $n \in \mathbb{N}$ , there exists a $y_n \in Y$ corresponding to $\text{argmin}_{y \in Y} f(x_n,y)$ .  Consider the sequence $(x_n,y_n) \subset X \times Y$ .  Since $Y$ is compact, we can drop to a subsequence $(x_{n_k},y_{n_k})_{k=1}^{\infty}$ such that $(x_{n_k},y_{n_k}) \longrightarrow (x,y)$ . This is where I get stuck.  I don't even know if $g(x_{n_k})=f(x_{n_k},y_{n_y}) \longrightarrow f(x,y) = g(x)$ since $y$ need not be a minimizing value of $f$ . Even if it was, I will have only shown that the image of a specific subsequence under $g$ converges. Any help is appreciated.","['continuity', 'general-topology', 'functions', 'convergence-divergence']"
4298084,Upper bound on the expectation of the min max of gaussian random variables,"Define a random $n \times m$ matrix X , where each entry is independent and follows a standard gaussian distribution, i.e., $X_{ij} \sim N(0,1)$ , i.i.d., $\forall$ $i,j$ . My objective is to find an upper bound (as a function of m and n) for $$\mathbb{E}\left[\min_{1 \leq j \leq m} \max_{1 \leq i \leq n}X_{ij} \right ]$$ I know that the expectation of the maximum has an upper bound of order $\sqrt{\log n}$ , and in numerical simulations, I can see that when $m=n$ this expectation diverges to infinity as well, but I am struggling to find a sharp upper bound.","['statistics', 'normal-distribution', 'expected-value', 'order-statistics', 'probability']"
4298153,"Let $X$ be a random variable with probability function $f(x)=\lambda e^{-\lambda x} \text{ if } x>0$ ($\lambda >1$ constant), and $0$ otherwise.","Let $X$ be a random variable with probability function $\operatorname{f}\left(x\right)=\lambda{\rm e}^{-\lambda x}$ if $x>0$ and $\lambda >1$ Is a $constant$ . $0$ otherwise. Calculate the expected value of the variable $Y=e^X$ by first finding
the density function of $Y$ and applying the elementary definition of
expectation. As a second method use the unconscious statistician's
theorem. Attempt I have already tried, and according to my calculations, the density function of $Y$ is $$g(y)=\frac{1}{y}\lambda  e^{-\lambda (\frac{1}{y})} \text{ if } y>1 (\lambda >1 \text{ constant}),$$ and $0$ otherwise. Are my calculations correct? Therefore, the expected value is $$E(Y)=\int_{1}^{\infty}\, y\cdot g(y)\, dy=\int_{1}^{\infty}\, \lambda  e^{-\lambda (\frac{1}{y})}\, dy$$ How do I calculate this integral?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4298162,Is the function $f : \mathbb C^{*} \rightarrow \mathbb C^{*} $ given by $f(z)=ze^{z}$ a closed map?,"I came across a problem where I was asked to find that, the same map is closed or not when seen as from $\mathbb C^{*}$ to $\mathbb C$ . So i solved it by showing that image of real line is not closed in the later. After then I was thinking about the closeness of the same function with co-domain non-zero complex number, but I wasn't able to do it as in the last case zero was playing a crucial role. Any hint please?","['complex-analysis', 'functional-analysis', 'analysis', 'real-analysis']"
4298242,How can I find the formula for the integral $ \int_{0}^{\infty} \frac{x^{n}\left(e^{3 x}-e^{x}\right)}{\left(e^{x}-1\right)^{4}} d x ? $ where $n>2$.,"Couple of days ago, I was asked to find the exact value of the definite integral $$\displaystyle \int_{0}^{\infty} \frac{x^{5}\left(e^{3 x}-e^{x}\right)}{\left(e^{x}-1\right)^{4}} d x ,\tag*{} $$ After finding its exact value, I noted that the proof can be generalized to $n$ instead of 5. Now I am going to share my proof with you all. $$\displaystyle I(n)=\int_{0}^{\infty} \frac{x^{n}\left(e^{3 x}-e^{x}\right)}{\left(e^{x}-1\right)^{4}} d x,$$ where $n\geq 3.$ Simplifying and splitting the integrand into 2 simpler one yields $\displaystyle I(n)=\int_{0}^{\infty} \frac{x^{n} e^{x}\left(e^{x}+1\right)}{\left(e^{x}-1\right)^{3}} d x=\underbrace{\int_{0}^{\infty} \frac{x^{n} e^{x}}{\left(e^{x}-1\right)^{2}} d x}_{J} +2\underbrace{\int_{0}^{\infty} \frac{x^{n} e^{x}}{\left(e^{x}-1\right)^{3}} d x}_{K} \tag*{} $ Using integration by parts and geometric series, we evaluate the integral $J.$ $ \displaystyle \begin{aligned}J &=-\int_{0}^{\infty} x^{n} d\left(\frac{1}{e^{x}-1}\right) \\&=-\left[\frac{x^{n}}{e^{x}-1}\right]_{0}^{\infty}+n\int_{0}^{\infty} \frac{x^{n-1}}{e^{x}-1} d x \\&=n\int_{0}^{\infty} \frac{x^{n-1} e^{-x}}{1-e^{-x}} d x \\&=n\int_{0}^{\infty} x^{n-1} e^{-x}\left(\sum_{k=0}^{\infty} e^{-k x}\right) d x\\&=n\sum_{k=0}^{\infty} \int_{0}^{\infty} x^{n-1} e^{-(k+1) x} d x\end{aligned} \tag*{}$ Similarly for the integral $K$ , we have $\displaystyle \begin{aligned}K &=\int_{0}^{\infty} \frac{x^{n} e^{x}}{\left(e^{x}-1\right)^{3}} d x \\&=-\frac{1}{2} \int_{0}^{\infty} x^{n} d\left(\frac{1}{e^{x}-1}\right)^{2} \\&=\frac{n}{2} \int_{0}^{\infty} \frac{x^{n-1}}{\left(e^{x}-1\right)^{2}} d x \\&=\frac{n}{2} \int_{0}^{\infty} \frac{x^{n-1} e^{-2 x}}{\left(1-e^{-x}\right)^{2}} d x\end{aligned} \tag*{} $ To deal with the last integral, we do need an infinite geometric series $\displaystyle \frac{1}{1-y}=\sum_{k=0}^{\infty} y^{k} \text { for }|y|<1.\tag*{} $ Now we differentiate both sides w.r.t. $y$ and obtain $\displaystyle \frac{1}{(1-y)^{2}}=\sum_{k=0}^{\infty} k y^{k-1} \tag*{} $ Replacing $y$ by $e^{-x}$ yields $ \begin{aligned}K &=\frac{n}{2} \int_{0}^{\infty} x^{n-1} e^{-2 x} \sum_{k=0}^{\infty} k e^{-(k-1) x} d x \\&=\frac{n}{2} \sum_{k=0}^{\infty} k\int_{0}^{\infty} x^{n-1} e^{-(k+1) x} d x\end{aligned}\tag*{} $ Grouping them together, we can conclude that $ \displaystyle \begin{aligned}I(n) &=J+2 K \\&=n\left(\sum_{k=0}^{\infty} \int_{0}^{\infty} x^{n-1} e^{-(k+1) x} d x+\sum_{k=0}^{\infty} k \int_{0}^{\infty} x^{k} e^{-(k+1) x} d x\right)\\&=n \sum_{k=0}^{\infty}(k+1) \int_{0}^{\infty} x^{n-1} e^{-(k+1) x} d x  \\&=n\sum_{k=0}^{\infty}(k+1) \cdot \frac{(n-1)!}{(k+1)^{n}} \quad \text {Via IBP repeatedly}\\&=n! \sum_{k=1}^{\infty} \frac{1}{k^{n-1}}\end{aligned} \tag*{} $ I finally succeed to find a beautiful formula for the integral $I(n)$ with $n\geq 3$ , $$\boxed{\int_{0}^{\infty} \frac{x^{n}\left(e^{3 x}-e^{x}\right)}{\left(e^{x}-1\right)^{4}} d x =n! \zeta (n-1) }$$ :|D Wish you enjoy my proof! Your suggestions, comments and alternate methods are warmly welcome!","['integration', 'sequences-and-series']"
4298254,How to efficiently compute this Pffafian of curvature form,"Though one can verify this $$Pf(\Omega) = \frac{1}{8}(\vert \text{Rm} \vert^2 - 4 \vert \text{Ric}\vert^2 + R^2)$$ for a 4-dim manifold by listing out all the curvature components mechanically (I have to ask my computer to do so), is there a clever way to compute this quantity? Thanks",['differential-geometry']
4298269,Find the polar equation of the loci of $\Im(z-1+\frac{4}{z})=0$,"Question: Find the polar equation of the loci $$\Im(z-1+\frac{4}{z})=0$$ Where $\Im()$ means the imaginary part of the resulting complex number. My Workings: Let $z = x+iy$ So that $$\Im(x+iy-1+\frac{4}{x+iy})=0$$ Therefore $$\Im((x-1)+iy+\frac{4(x-iy)}{x^2+y^2})=0$$ Therefore $$\Im(x-1+\frac{4x}{x^2+y^2} + i(y-\frac{4y}{x^2+y^2}))=0$$ This implies $$ y-\frac{4y}{x^2+y^2} = 0 $$ Now, I am stuck and don't know what to do.","['algebra-precalculus', 'complex-numbers']"
4298298,Is it possible to paint a $19\times19$ board so that every $10\times10$ square has a different number of colored squares?,"Is it possible to paint a $19\times19$ board so that every $10\times10$ square has a different number of colored squares? I'm not quite sure how to definitely prove this. What I've done so far is figure out that there's a $100$ different $10\times10$ squares in a $19\times19$ board. This means that we'll have one $10\times10$ square for every number from $1$ to $100$ or $0$ to $99$ . I've thought about looking at the square with $0$ or $1$ colored squares and seeing what it'll mean for the other squares. I know that the $0$ and $99$ or $1$ and $100$ squares must share exactly one square. If they share anymore it'll be impossible. Not sure where to go from here, any help is appreciated.",['combinatorics']
4298358,Triple Integral - My answer seems too large,"Consider $R=[(x,y,z)\in \mathbb{R}^3 | 1 \leq x^2 + y^2 \leq 16, 0 \leq z \leq y+4]$ . Then calculate the integral $$I=\int_R(x-y)dV$$ I thought about the region and found that as $€ \leq x^2 + y^2 \leq 16,$ we have $x \in [1,4], y \in [1,4]$ . Then we can have $z \in [0,8]$ I then changed the integral to $\int_1^4 \int_1^{16-x^2}\int_0^{y+4}(x-y)dzdydx$ , which I found to be $\frac{93461}{70} \approx 1335.16$ However, if I was to make the region larger, say taking the  circle $x^2+y^2=16 \ (a=4)$ and making it into a spheroid with $z=8 \ (c=4)$ , then I would get volume $\frac{4}{3} \pi 4^24=\frac{256}{3}\pi \approx 268.08$ Hence I know that my answer is far too large, so I have probably found the incorrect limits of integration?","['definite-integrals', '3d', 'geometry', 'multivariable-calculus', 'multiple-integral']"
4298418,"Show that for $n\geq 4$ there is a partition $\{1,2,...,n\}=A\cup B$ such that $\{a_i+a_j:i<j\}=\{b_i+b_j:i<j\}$ if and only if $n$ is a power of 2.","If there is such a partition $A\cup B$ for a given $n$ then $A'=A\cup (n+B)$ and $B'=B\cup (n+A)$ does the trick for $2n$ . Beginning with $A=\{1,4\}$ and $B=\{2,3\}$ we get such partitions for any power of 2, which I strongly believe (but can't prove!) are unique. As for the converse, I have no idea on how to proceed. Many thanks in advance for any help on this problem.",['combinatorics']
4298423,conformal map from disc to funny domain,"I am working on numerical experiment about the Riemann mapping theorem.
In order to test my program, I would like to have some test function on highly non-symmetric and non convex shape, funnier than this one : Ideally, something like the standard test domain, like the rabbit: But I need an exact conformal map like a rational function: $$\left(\sum_{0}^n a_k z^k\right)/\left(\sum_{0}^m b_k z^k\right), $$ or at least something I can give to python. Of course for the rabbit like domain, I am not expected to easily get something  perfectly resembling with a rational function, but at least something that is roughly comparable. Does anyone knows where I can find this can kind of conformal parametrization? I have google many key words without any good result.
Thank in advance! Edit:
Thanks to Matt E., I get the desired shape, an I am also able to interpolate, but the code for the interpolation using Bezier curves, so the coefficients correspond to splines functions. How can get some trigonometric ones with python? Then I have to hope that the harmonic extension is bijective in the interior of the disc.","['complex-analysis', 'conformal-geometry']"
4298553,Expectation - Function using normalising constant and indicator function,"$f:\mathbb{R}^2\rightarrow \mathbb{R}^2, by \ f(x,y)=(x^2+y^2,x+y)$ Let $B = f^{-1}([1,4]\ \times \ [-4, \infty)\ \times \ (-2\sqrt2,2\sqrt2))$ Suppose $(X,Y)$ is a r.v. taking values in $B$ with $pdf:f(x,y)=Ce^{-\frac{x^2+y^2}{2}}I_B(x,y)$ - where C is a normalising constant. Find $\mathbb{E}(X), \mathbb{E}^7, \mathbb{E}(|XY|(X^2+Y^2)^{-1})$ I do not understand how to approach this question. I know that for finding the expectation from a random variable, you use $\mathbb{E}(X) = \int_{_-\infty}^\infty xf_X(x)dx$ So would I need to find the marginal distribution of $X$ And find then find it’s expectation? Do I need to find the value of the normalising constant $C$ ? How do I approach expectation when an indicator variable is involved? Do I find the probability of the indicator variable equalling 1 (and hence equalling 0)? Sorry lots of questions, but this problem seems to be more difficult than others I have attempted before.","['probability-distributions', 'expected-value', 'continuity', 'functions', 'probability']"
4298570,"If $f^{-1}(c)$ is closed $\forall c\in\Bbb R$ and if for each $c\in\Bbb R$ between $f(x)$ & $f(y)$ there is $z\in[x,y]$ s.t. $f(z)=c,f$ is continuous.","For $x,y\in\Bbb R^n,$ define $[x,y]:=\{(1-\lambda)x+\lambda y\}.$ Let $f:\Bbb R^n\to\Bbb R.$ Prove the following statement: If $f^{-1}(c)$ is closed $\forall c\in\Bbb R$ and if for each $c\in\Bbb R$ between $f(x)$ and $f(y)$ there is $z\in[x,y]$ such that $f(z)=c,$ then $f$ is continuous. My thoughts: Let's write $z_1=z\in[x,y]$ as $z_1=(1-\lambda_1)x+\lambda_1y.$ If the statement holds for any $c_1=c$ between $f(x)$ and $f(y),$ then it should also hold for any $c_2$ between $f(x)$ and $c_1$ or $c_1$ and $f(y)$ . WLOG, assume it's the former, so , for that $c_2,$ there is some $z_2\in[x,z_1]$ such that $f(z_2)=c_2$ . Then, $$\begin{aligned}z_2&=(1-\lambda_2)x+\lambda_2z_1\\&=(1-\lambda_2)x+\lambda_2((1-\lambda_1)x+\lambda_1y)\\&=(1-\lambda_2+\lambda_2(1-\lambda_1))x+\lambda_1\lambda_2y\\&=(1-\lambda_1\lambda_2)x+\lambda_1\lambda_2y.\end{aligned}$$ Denote $\overline\lambda_2=\lambda_1\lambda_1\le\lambda_1.$ Then, we can repeat the procedure to approach $x$ arbitrarily close as $\lim\limits_{k\to\infty}\overline\lambda_k=0$ and obtain a somewhat recursive sequence $(z_k)_{k\in\Bbb N}, z_{k+1}\in[x,z_k]$ with $f(z_{k+1})\in\Bbb R$ between $f(x)$ and $c_k.$ We can, then, choose different lines in $\Bbb R^n$ through $x$ and $y'$ s on all sides. I know that a function having the same limit at a point in all directions is insufficient and that the limit should be uniform and combine it with Heine characterization of limit of the function at a point. I guess I should use the fact $f^{-1}f((x))$ is closed here, but my attempt seems wavy and too informal. How should I proceed?","['continuity', 'general-topology', 'functions', 'real-analysis']"
4298571,Show that $\cos(2^n)$ diverges,"Problem: I have sequence defined as follows: $$
a_n = \cos(2^n)
$$ I need to show that it diverges. My progress: I tried common method here supposing that this sequence has limit $\lim_{n\to\infty}\cos(2^n) = a$ and then trying to get to contradiction with $\lim_{n\to\infty}(\cos(2^{n+1})-\cos(2^n))=0$ . But it led me nowhere. How can I show that this sequence diverges?","['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
4298588,Dominated convergence theorem for Banach space,"I'm trying to prove dominated convergence theorem for Banach space. Could you verify if my proof is fine or contains some subtle mistakes? Let $(f_n)$ be a sequence in $\mathcal L_0 (X, \mu, E)$ . Suppose that there exists $g \in \mathcal L_1 (X, \mu, E)$ such that $|f_n| \le |g|$ $\mu$ -a.e. for all $n$ . Suppose also that, for some $f \in E^X$ , $f_n \to f$ $\mu$ -a.e. Then $f_n,f \in \mathcal L_1 (X, \mu, E)$ for all $n$ , $f_n \to f$ in $\mathcal L_1 (X, \mu, E)$ , and $\int_X f_n \mathrm d \mu \to \int_X f \mathrm d \mu$ in $E$ . First, we recall related definitions. Let $(X, \mathcal A, \mu)$ be a $\sigma$ -finite measure space and $(E, | \cdot |)$ a Banach space. A function $f \in E^{X}$ is called $\mu$ - simple if $f = \sum_{k=1}^n e_k 1_{A_k}$ where $0 \neq e_k \in E$ and $(A_k)_{k=1}^n$ is a finite sequence of pairwise disjoint sets with finite measure in $\mathcal A$ . The integral of such $f$ w.r.t. $\mu$ is defined by $\int_{X} f \mathrm d \mu := \sum_{k=1}^n e_k \mu(A_k)$ . Let $\mathcal S (X, \mu, E)$ be the space of such $\mu$ -simple functions. We define $\| \cdot \|_1 : \mathcal S (X, \mu, E) \to \mathbb R$ by $f \mapsto \int_{X} |f| \mathrm d \mu$ . Then $\| \cdot \|_1$ is a semi-norm on $\mathcal S (X, \mu, E)$ . The notion of Cauchy sequence is thus applicable for $(\mathcal S (X, \mu, E), \| \cdot \|_1)$ . A function $f \in E^{X}$ is called $\mu$ - measurable if $f$ is a $\mu$ -a.e. limit of a sequence $(f_n)$ in $\mathcal S (X, \mu, E)$ .  Let $\mathcal L_0 (X, \mu, E)$ be the space of such $\mu$ -measurable functions. A function $f \in E^{X}$ is called $\mu$ - integrable if $f$ is $\mu$ -a.e. limit of a Cauchy sequence $(f_n)$ in $\mathcal S (X, \mu, E)$ . The integral of such $f$ w.r.t. $\mu$ is defined by $\int_{X} f \mathrm d \mu := \lim_n \int_{X} f_n \mathrm d \mu$ . We also define the semi-norm $\| f\|_1 :=  \int_{X} |f| \mathrm d \mu$ . Let $\mathcal L_1 (X, \mu, E)$ be the space of such $\mu$ -integrable functions. Then $\big (\mathcal L_1 (X, \mu, E), \| \cdot \|_1 \big )$ is complete. Also, $\mathcal S (X, \mu, E)$ is dense in $\mathcal L_1 (X, \mu, E)$ w.r.t. $\| \cdot \|_1$ . Proof: First, we assume $f_n \in \mathcal L_1 (X, \mu, E)$ for all $n$ . Let $0 \le g_n := \sup_{i,j \ge n} |f_i - f_j| \le 2 |g|$ . Then $(g_n)$ is a non-increasing sequence in $\mathcal L_0 (X, \mu, \mathbb R^+)$ that converges to $0$ $\mu$ -a.e. By ""reverse"" Fatou's lemma, $$0 \le \limsup_n \int_X g_n \mathrm d \mu \le \int_X \limsup_n g_n \mathrm d \mu = 0.$$ It follows that $\int_X g_n \mathrm d \mu \to 0$ in $\mathbb R^+$ . For all $i, j \ge n$ , $$\|f_i -f_j \|_1 = \int_X |f_i - f_j| \mathrm d \mu \le \int_X  g_{n} \mathrm d \mu.$$ Hence $(f_n)$ is a Cauchy sequence in $\mathcal L_1 (X, \mu, E)$ which is complete. Then $f_n$ converges to some $\hat f$ in $\mathcal L_1 (X, \mu, E)$ . Second, we will show that there is a subsequence $(f_{\varphi(\ell)})$ of $(f_n)$ such that $f_{\varphi(\ell)} \to \hat f$ $\mu$ -a.e as $\ell \to \infty$ . It suffices to assume $\hat f =0$ , because, if $\hat f \neq 0$ , we can consider the sequence $(f_n - \hat f)$ . There is a subsequence $\varphi$ of $(n)$ such that $\|f_i-f_j\|_1 \le 2^{-2\ell}$ for $i,j \ge \varphi(\ell)$ . Let $h_\ell :=f_{\varphi(\ell)}$ . Then $\|h_\ell - h_m\|_1 \le 2^{-2\ell}$ for $m \ge \ell$ . Take the limit $m \to \infty$ , we get $\|h_\ell\|_1 \le 2^{-2\ell}$ . Let $B_\ell := \{ x\in X \mid |h_\ell (x)| \ge 2^{-\ell} \}, A_n := \cup_{\ell \ge n} B_\ell$ , and $A := \cap A_n$ . Then $\mu(B_\ell) \le 2^{-\ell}$ for $\ell \in \mathbb N$ , $\mu(A_n) \le 2^{-n+1}$ for $n \in \mathbb N$ , and $\mu(A) =0$ . Then $(h_\ell)$ converges to $0$ uniformly on each $A_n^c$ and pointwise on $A^c$ . On the other hand, $f_n \to f$ $\mu$ -a.e., so does $(f_{\varphi(\ell)})$ . Hence $f = \hat f$ $\mu$ -a.e. and thus $f_n$ converges to $f$ in $\mathcal L_1 (X, \mu, E)$ . Next we have $$\left |\int_X f_n \mathrm d \mu - \int_X f \mathrm d \mu \right | = \left | \int_X (f_n - f) \mathrm d \mu \right | \le \int_X |f_n - f| \mathrm d \mu = \|f_n -f\|_1 \to 0.$$ Next we're going to prove $f_n \in \mathcal L_1 (X, \mu, E)$ . It suffices to prove for $n=0$ . Let $\left(\varphi_{n}\right)$ be a sequence in $\mathcal{S}(X, \mu, E)$ such that $\varphi_{n} \to f_0$ $\mu$ -a.e. Let $A_{n}:= \{x \in X \mid \left|\varphi_{n}(x)\right| \le |g(x)| \}$ and $g_{n} := 1_{A_{n}} \varphi_{n}$ for $n \in \mathbb N$ . Then $\left(g_{n}\right)$ is a sequence in $\mathcal{S}(X, \mu, E) \subseteq \mathcal{L}_1(X, \mu, E)$ that converges $\mu$ -a.e. to $f_0$ . Because $\left|g_{n}\right| \le g$ for $n \in \mathbb{N}$ , the claim follows from result proved above. This completes the proof. Update: I have found that the proof of $f_n \in \mathcal L_1 (X, \mu, E)$ is more subtle than I thought. Below is a proper treatment. Next we're going to prove $f_n \in \mathcal L_1 (X, \mu, E)$ . It suffices to prove for $n=0$ . Let $\left(\varphi_{n}\right)$ be a sequence in $\mathcal{S}(X, \mu, E)$ such that $\varphi_{n} \to f_0$ $\mu$ -a.e. Let $A_{n}:= \{x \in X \mid \left|\varphi_{n}(x)\right| \le 2 |g(x)| \}$ and $g_{n} := 1_{A_{n}} \varphi_{n}$ for $n \in \mathbb N$ . Then $\left(g_{n}\right)$ is a sequence in $\mathcal{S}(X, \mu, E) \subseteq \mathcal{L}_1(X, \mu, E)$ . Let's prove that $(g_n)$ converges $\mu$ -a.e. to $f_0$ . Let $A$ be a null set such that $\varphi_n (x) \to f_0 (x)$ for all $x \in A^c$ . Let $B$ be a null set such that $|f_0 (x)| \le |g(x)|$ for $x \in B^c$ . Also, $C := A \cup B$ and $D := \{x \in X | g (x) \neq 0 \}$ . Then $C$ is also a null set. For each $x \in C^c \cap D$ , there is $N \in \mathbb N$ such that $|\varphi_n(x) - f_0 (x)| \le |g(x)|$ and thus $|\varphi_n(x)| \le |f_0 (x)| + |g(x)| \le 2 |g(x)|$ and thus $x \in A_n$ for $n \ge N$ . This means $g_n (x) = \varphi_n (x) \to f_0(x)$ for $x \in C^c \cap D$ . For $x \in C^c \cap D^c$ , $g(x) = f_0(x) = 0$ and thus $|g_n (x) - f_0(x)| = |g_n(x)| = 1_{A_{n}} |\varphi_{n}| \le |\varphi_n(x)| \to 0$ and thus $g_n (x) \to f_0(x)$ . Hence $g_n \to f_0$ on $C^c$ . Because $\left|g_{n}\right| \le 2|g|$ for $n \in \mathbb{N}$ , the claim follows from result proved above. This completes the proof.","['banach-spaces', 'measure-theory', 'solution-verification', 'functional-analysis']"
4298589,"Use Green's theorem to calculate $\int_{\gamma}y\,dx+x^2dy$","Use greens theorem to calculate $\int_{\gamma}y\,dx+x^2dy$ where $\gamma$ is the following closed path:
(a) The circle given by $g(t) = (\cos(t), \sin(t)), 0 \le t \le 2\pi$ . What I have tried: Using the following $$\int_\gamma P\,dx+Q\,dy = \int_D\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y} \right)$$ I have that $$\frac{\partial (x^2)}{\partial x}-\frac{\partial (y)}{\partial y} = 2x-1$$ Replacing $x = \cos(t)$ I then have: $$\int_0^{2\pi}(2\cos(t)-1)\,dt=-2\pi$$ But the answer in my book show $-\pi$ , so I have tried replacing $x = \cos(t), y = \sin(t), dx = -\sin(t), dy = \cos(t)$ in the integral but that gives me $\pi$ . Have I made a mistake?",['multivariable-calculus']
4298592,Locally closed embedding whose image is closed is a closed embedding?,"Let $\pi: X \to Y$ be a locally closed embedding of schemes. Then we can realize $X$ as a closed subscheme of an open subscheme $U$ of $Y$ , and $\pi$ factors as $$X \to U \to Y$$ where $X \to U$ is a closed embedding and $U \to Y$ is an open embedding. We want to show that $\pi$ is a closed embedding. Therefore, we need to show that for any open affine subset $\operatorname{Spec} B$ of $Y$ , the preimage $\pi^{-1}(\operatorname{Spec} B)=X \cap \operatorname{Spec} B$ is affine, say $\operatorname{Spec} A$ , and the induced map of sections $B \to A$ is surjective. The problem I am seeing is that the open embedding $U \to Y$ need not be affine, i.e., $U \cap \operatorname{Spec} B$ need not be affine, and therefore $X \cap U \cap \operatorname{Spec} B= X\cap \operatorname{Spec} B$ need not be affine; but we need it to be.",['algebraic-geometry']
4298614,"$P$ poset: $ J \subseteq P$ is join-dense iff $\forall x \in P$, $\,\,\, x = (\downarrow x \cap J) $","Let $\mathrm{P}$ be a poset: $ \mathrm{J} \subseteq \mathrm{P}$ is join-dense in $\mathrm{P} \iff \forall x\in \mathrm{P}$ we have $x =\bigvee (\downarrow x\cap \mathrm{J})$ about this implication "" $\Rightarrow$ "", since $\mathrm{J}$ is join-dense in $\mathrm{P}$ , $\exists\,\, \mathrm{S} \subseteq\mathrm{J}$ s.t. $x = \lor \mathrm{S}$ , so I am trying so show that $\mathrm{S} = \downarrow x\cap\mathrm{J}$ : $\,\,$ let $b\in\mathrm{S} \Rightarrow$ $b \le \lor\mathrm{S} = x$ , so $b \in \downarrow x \Rightarrow b\in \downarrow x\cap\mathrm{J}$ . $\,\,\,\,$ Now let $c \in \mathrm{I}\cap\mathrm{J}$ $\Rightarrow c \le x = \lor \mathrm{S}$ $\,\,\,\,(...)$ any idea how to complete it? Or maybe there is a different path to follow? The implication $""\Leftarrow""$ is easy because of the assumption, by putting $T := \downarrow x \cap J \subseteq J$ , then $\forall x \in P \,\, \exists \,\, T \subseteq J$ s.t. $\,\, x= \bigvee T$ , that is the definition of join-dense set.","['lattice-orders', 'order-theory', 'combinatorics', 'discrete-mathematics']"
4298634,Quibbles on the definition of the Lebesgue integral via simple functions; may we use the infimum of upper bounds instead?,"I am new to measure theory. This is my understanding of the Lebesgue integral for positive measurable functions: Let $f:X\to\Bbb R^+$ be a positive Borel measurable function. Take the image $f(X)$ and partition it into $P=\{y_0\lt y_1\lt y_2\lt y_3\lt\cdots\lt y_N\}$ for some finite $N$ , of course such that $y\in f(X)\iff \exists n:y\in[y_{n+1},y_n]$ , and define $A_n=f^{-1}([y_{n+1},y_n])$ for $n\in\{0,1,\cdots,N-1\}$ . Then define the simple function $h_P(x)=\sum_{n=0}^{N-1}y_n\cdot\chi_{A_n}$ , where $\chi_{A_n}$ is the characteristic function of $A_n$ . It is easily shown that $h_P$ is also a Borel measurable function from $X\to\Bbb R^+$ , and by construction $h(x)\le f(x)$ for all $x\in X$ . By the properties of the Lebesgue integral for simple functions, the natural generalisation of the integral to arbitrary $f$ keeps the monotonicity property. As the supremum will always exist if you allow $\infty$ , then the definition: $$\int_X f(x)\,\mathrm{d}\mu=\sup_{n\in\Bbb N}\left\{\int_X h_P(x)\,\mathrm{d}\mu:P\text{ is a partition, order $n$, of $f^{-1}(X)$}\right\}$$ Is well-defined and natural. This is my own phrasing of what I've learned so far. I am deeply curious to know if the following definition that I just thought of is valid: Call the integral defined as above by $L(f)$ . Redefine $h_P$ as $\sum_{n=1}^Ny_n\cdot\chi_{A_{n-1}}$ , so that $f(x)\le h_P(x)$ . With this $h_P$ , define: $$U(f)=\inf_{n\in\Bbb N}\left\{\int_X h_P(x)\,\mathrm{d}\mu:P\text{ is a partition, order $n$, of $f^{-1}(X)$}\right\}$$ In the same spirit as the definition of the upper and lower Darboux integrals. My question: Is $U(f)=L(f)=\int_Xf(x)\,\mathrm{d}\mu$ always? If so, can I safely not care whether I choose the infimum of upper bound simple integrals or the supremum of lower bound simple functions? If not, why, and when?","['measure-theory', 'definition', 'lebesgue-integral']"
4298639,Binomial rule and repeatedly differentiating,"Exercise from book: Show that for all positive integers n $$
\sum_{k=0}^{n}(-1)^{k}\left(\begin{array}{l}
n \\
k
\end{array}\right) k^{j}= \begin{cases}0, & j=0,1, \ldots, n-1 \\
(-1)^{n} n ! j=n\end{cases}
$$ and hint from author: Hint. Expand $(1-x)^{n}$ by the binomial rule. Repeatedly differentiate, but with a twist. My question: what does mean differentiate with a twist??","['binomial-theorem', 'derivatives', 'real-analysis']"
4298654,how to get geometric median in 1D by solving an optimization problem?,"I have a set of numbers $\{x_1,x_2,\cdots\}$ . The median could be obtained by sorting, but I want to get via solving an optimization problem. I know that the media minimizes $$\sum_i \left| {{x_i} - \mu } \right|$$ However, when solving the optimization problem via a solver, I get a discrepancy between the true median and obtained median. what could go wrong ? P.S. the median could be thought of as the point that if placed between the points, the distance between it and all points is the smallest.","['optimization', 'calculus', 'geometry', 'median']"
4298713,Questions about Rudin's rank theorem,"I am trying to understand the rank theorem in Rudin's Principles of Mathematical Analysis. The theorem states: Theorem Suppose $m,n,r$ are nonnegative integers, $m\ge r, n\ge r$ , $F$ is a $C^1$ mapping of an open set $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$ , and $F'(x)$ has rank $r$ for every $x\in E$ . Fix $a\in E$ , put $A = F'(a)$ , let $Y_1$ be the range of $A$ , and let $P$ be a projection in $\mathbb{R}^m$ whose range is $Y_1$ and let $Y_2$ be the kernel of $P$ . Then there are open sets $U$ and $V$ in $\mathbb{R}^n$ , with $a\in U\subset E$ , and there is a 1-1 $C^1$ mapping $H$ of $V$ onto $U$ (whose inverse is also of $C^1$ ) such that $$F(H(x)) = Ax+\phi(Ax)\;\;\;\;(x\in V)$$ where $\phi$ is a $C^1$ mapping of the open set $A(V)\subset Y_1$ into $Y_2$ . I have quite a few problems trying to understand this theorem, but perhaps mainly this: What kind of projection does Rudin have in mind? All he says about the projection is that its range is $Y_1$ . Is this the projection of the entire $\mathbb{R}^m$ and the $\text{im}(P)=Y_1$ ? Can you choose any projection with such range? Does the choice of such projection affect the null space of P? What does the equation $F(H(x)) = Ax + \phi(Ax)$ tell us intuitively? Why are we mapping with $\phi$ from the range of $P$ to its nullspace?","['multivariable-calculus', 'inverse-function-theorem', 'implicit-function-theorem', 'real-analysis']"
4298747,"What is $E\left[T_n(X_1,\ldots,X_n)\mid X_{(1)},\ldots,X_{(n)}\right]$ if $X_1,\ldots,X_n$ are i.i.d random variables?","Consider i.i.d random variables $X_1,X_2,\ldots,X_n$ with an absolutely continuous distribution. Suppose $\boldsymbol Y=(X_{(1)},X_{(2)},\ldots,X_{(n)})$ is the full set of order statistics corresponding to $\boldsymbol X=(X_1,X_2,\ldots,X_n)$ . If $\mathcal P_n$ is the set of $n!$ permutations of $(1,2,\ldots,n)$ , then the conditional distribution of $\boldsymbol X$ given $\boldsymbol Y$ is known to be uniform over $\mathcal P_n$ . So if $T_n(\boldsymbol X)$ is any function of $\boldsymbol X$ , we can say $$E\left[T_n(\boldsymbol X)\mid \boldsymbol Y\right]=\frac1{n!}\sum_{(i_1,i_2,\ldots,i_n)\in \mathcal P_n}T_n(X_{i_1},X_{i_2},\ldots,X_{i_n}) \tag{$\star$}$$ But does $(\star)$ hold in much more generality, i.e. without the absolute continuity assumption? If not, what can we say about $E\left[T_n(\boldsymbol X)\mid \boldsymbol Y\right]$ if we only know that $X_1,\ldots,X_n$ are i.i.d random variables? We can assume $T_n$ has finite mean and variance.","['conditional-expectation', 'probability-theory', 'order-statistics']"
4298827,Differential equation for Brownian motion,"I was trying to solve the following differential equation: $$ \frac{dv}{dt} + \frac{\gamma}{m}v + \omega_0^2x = \frac{1}{m} \psi(t)$$ where: $v=\frac{dx}{dt}$ and $x=x(t)$ . But I cant really advance. The solution for it looks like this: $$ v(t)=v_0 e^{- \Gamma t}C(t) - \frac{\omega_0^2}{\Delta}x_0e^{- \Gamma t} \sinh(\Delta t)+ \frac{1}{m} \int_0^tdt' \psi(t')e^{- \Gamma (t-t')}C(t-t') \hspace{1cm}(1)$$ where: $C(t)= \cosh(\Delta t )- \frac{\Gamma}{\Delta}\sinh(\Delta t)$ , $\Gamma = \frac{\gamma}{m}$ and $\Delta = \sqrt{\Gamma^2 - \omega_0^2}$ . I tried the annihilation polynomial (by, first, assuming the homogeneous equation): $$ P(\lambda) = \lambda^2+ \frac{\gamma}{m} \lambda + \omega_0^2 =0$$ which lead me to: $$ \lambda_{\pm} = - \frac{\Gamma}{2} \pm \sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  }$$ So, the homogeneous solution would be: $$ x(t) = c_1e^{ - \frac{ \Gamma}{2}t} \cos \left(\sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  } t \right) + c_2e^{ - \frac{ \Gamma}{2}t} \sin \left(\sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  } \right)$$ I dont know where the integration comes from, in the last term of $(1)$ on the RHS, since I would have to apply a derivative to this solution I obtained, in order to get the expression for the velocity $v(t)$ . I still need to find the particular solution, and I dont know how to find it either. How can I obtain $(1)$ ?","['physics', 'ordinary-differential-equations']"
4298863,"Is it rigorous to write $\frac{dy}{dx}$ with ""$y=f(x)$"" instead of $\frac{df(x)}{dx}$ ? Can ""dependent variables"" be defined mathematically?","In my understanding, to make a rigorous use of the Leibniz notation, one must write either $\frac{df(x)}{dx}$ $ \ \ \ $ i.e. $ \ \ $ $f'$ $ \ \ $ ( which denotes a function ) or $\frac{df(x)}{dx}(a)$ $ \ \ $ or $ \ \ $ $\frac{df(x)}{dx}|_{x=a}$ $ \ \ $ i.e. $ \ $ $f'(a)$ $ \ \ $ (which denotes a value) (where $f$ is a function that is differentiable at $a$ , and $x$ is just a placeholder/a bound variable) Thus, I suppose that writing $\frac{df}{dx}$ for $f'$ or $\frac{df}{dx}|_{x=a}$ for $f'(a)$ is merely a common abuse of notation, since the $\frac d{dx}$ must be followed by a literal expression dependant on $x$ , not by a function. ( See also this question ) What disturbs me is that I sometimes see the notation $\frac{dy}{dx}$ for $\frac{df(x)}{dx}$ where $y=f(x)$ is a ""dependent variable"". How can $y$ have any mathematical meaning ? It's seemingly neither a function (because it equals $f$ evaluated at $x$ ) nor a constant number (because it depends on $x$ ). So it is apparently a weird mathematical object linked by convention to a variable called $x$ . (I think this illustrates one of the big problems with Leibniz notation: it requires assigning fixed letters to the variables of a function, which is bogus since a function should be independent of the name given to its argument. The same problem occurs with the Leibniz notation for partial derivatives: if $f$ is a function $\mathbb{R}^2 \rightarrow \mathbb{R}$ , then unlike the unambiguous notation $\partial_1f$ (for the partial derivative w.r.t. the first argument), the Leibniz notation $\frac{\partial f}{\partial \, r}$ presupposes that the first variable of the function will always be denoted by $r$ ) So is there a rigorous way to define a « dependent variable », or is this just pseudo-mathematical quirkiness ? Edit: What is driving my question is that I have the impression that Leibniz's notation consistently treats everything as variables dependent on each other rather than as functions and arguments. As in the chain rule $\frac{dy}{dx}=\frac{dy}{dg}\frac{dg}{dx}$ that acts as if $y$ depends on $g$ even though $g$ is a function. I would like to know if there is a there is a purely mathematical aspect behind it, perhaps related to something in higher math like maybe manifolds. (I don't know what manifolds are, I don't even necessarily want to try to understand the mathematical definition of a ""dependent variable"", I would just like to know if this rigorous mathematical aspect exists or not). Edit: This thread asks questions similar to mine, but I haven't found satisfactory answers on it.","['notation', 'calculus', 'derivatives', 'analysis']"
4298951,Prove that the sequence $(a_n)$ is Cauchy and find the limit.,"Let us define a sequence $(a_n)$ as follows: $$a_1 = 1, a_2 = 2 \text{ and } a_{n} = \frac14 a_{n-2} + \frac34 a_{n-1}$$ Prove that the sequence $(a_n)$ is Cauchy and find the limit. I have proved that the sequence $(a_n)$ is Cauchy. But unable to find the limit. I have observed that the sequence $(a_n)$ is decreasing for $n \ge 2$ .","['limits', 'calculus', 'cauchy-sequences', 'real-analysis']"
4298956,"How to find *all* roots of arbitrarily high degree polynomials (in particular, characteristic polynomials)?","In eigenvalue characteristic equation computations of large matrices, it is often necessary to find all the the roots of this polynomial. I understand that there is no general explicit solution for irrational roots of polynomials of degree higher than 5. We might thus use numerical approximations, such as Newton's method, to find a root for the polynomial equation. My question is, for very high degree polynomials, eg. characteristic equation of degree 100, how can we make systematic/algorithmic computation to find all the 100 roots to this polynomial? Is there a consistent method that will always find all the roots to this eigenvalue characteristic equation? Thank you!","['eigenvalues-eigenvectors', 'roots', 'matrices', 'linear-algebra', 'polynomials']"
4299029,"Suppose $X \sim \chi^2(n)$. How can I prove that there exist $Z_i \sim N(0,1)$ independent such that $X = \sum_{i=1}^n Z_i^2$?","In general, knowing the distribution of a random variable $X$ is not the same as knowing the explicit mapping from $\Omega$ to $\mathbb{R}$ . Suppose then that $X \sim \chi^2(n)$ . I know that the distribution of $X$ is the same as the distribution of a sum of squares of independent standard normals, but how can I show that there exist $Z_1, \cdots, Z_n$ such that $X = \sum_{i=1}^n Z_i^2$ ? Don't I have to know something about the underlying probability space to make this claim?","['probability-distributions', 'probability-theory']"
4299037,"Find all $(x,y)\in\mathbb Z$ s.t. $x^2(x^2+1)=21^y-1$","$Q.$ Find all $(x,y)\in\mathbb Z^2$ s.t. $$x^2(x^2+1)=21^y-1$$ I tried to simplify it, $$x^4+x^2+(1-21^y)=0$$ It's just a quadratic equation , so I solved for $x^2$ $$x^2=\frac{-1\pm\sqrt{1-4(1-21^y)}}{2}$$ as $x^2\in\mathbb Z^+$ , $\Delta=a^2$ where $a\in\mathbb Z^+$ So $$1-4(1-21^y)=a^2\implies\underbrace{4(21^y-1)}_{\mathbb E^+}=(a+1)(a-1)\tag1$$ Which means either $(a+1)\in\mathbb E^+$ or $(a-1)\in\mathbb E^+$ , but this doesn't matter as both $(a+1),(a-1)\in\mathbb E^+$ . This is because difference of two consecutive Even number is $2$ . So let $(a-1)=2p$ , then $(a+1)=2(p+1)$ . Using this value in $eq^n(1)$ $$4(21^y-1)=4p(p+1)\implies\underbrace{4\cdot21^y-3}_{\Delta}=(2p+1)^2$$ Hence $a\in\mathbb O^+$ Now $$x^2=\begin{cases}\frac{-1+(2p+1)}{2}=p\\\frac{-1-(2p+1)}{2}=-(p+1)\end{cases}$$ But I'm not able to solve it further, any hint will be appreciated.","['number-theory', 'functions']"
4299066,Stuck on a probability question about pairs of socks,"The question is as follows: You have 10 pairs of socks (i.e., 20 socks in total) with each pair in a different color.  You put all the socks into the washing machine but it “eats” four of the 20 socks at random. What  is  the  expected  number  of  complete  pairs  left  in  the  washing machine? I approached this question like this: There are 3 possible outcomes: 8 pairs of socks survive 7 pairs of socks survive 6 pairs of socks survive I then tried to find the probability of each of these events occurring. The ways that 8 pairs of socks survive is: Eating 2 different colour socks then eating their pairs Eating first sock, then eating it's pair, Eating the second sock, then eating its pair P(8 pairs) = $1*\frac{18}{19}*\frac{2}{18}*\frac{1}{17}\\+1*\frac{1}{19}*1*\frac{1}{17}$ The ways that 7 pairs of socks survive is: Eating three socks of different colours, then eating any one of their pairs Eating two socks of different colours, then eating any one of their pairs, then drawing another sock of a different colour Eating any sock, eating another sock of the same colour, then eating two other socks of different colours P(7 socks)= $1*\frac{18}{19}*\frac{16}{18}*\frac{3}{17}\\+1*\frac{18}{19}*\frac{2}{18}*\frac{16}{17}\\+1*\frac{1}{19}*1*\frac{16}{17}$ The ways that 6 pairs of socks survive is: Eating 4 socks of different colours P(6 pairs)= $1*\frac{18}{19}*\frac{16}{18}*\frac{14}{17}$ I am confident that my solution is incorrect. First of all, the probabilities sum to $\frac{2906}{2907}$ when it should sum to 1. Secondly, I simulated the process in the question on my computer and the probabilities that I obtained were: P(8 pairs)= $0.0269986$ P(7 pairs)= $0.4319539$ P(6 pairs)= $0.5410475$ Compared to my answers which were: P(8 pairs)= $0.0093$ P(7 pairs)= $0.2972$ P(6 pairs)= $0.6935$ Would there be another simpler way of solving the question?","['combinatorics', 'probability']"
4299135,Mean value of $\Omega(n)$ is $\log\log n$,"This question is from my number theory assignment and I was unable to solve it. I have been following Dekonick and Luca. Let $\Omega(n)$ be the number of prime divisors of n, counted with multiplicity ie the number of prime powers dividing n, for instance $\Omega(12) = 3$ . Show that mean value of $\Omega(n)$ is $\log\log n$ : $\frac{1}{N}\sum_{n\leq N} \Omega(n) = \log\log N+O(1)$ . Attempt: I thought of using Abel's Identity : $\sum_{n\leq N} f(n) = f(N)[N]- \int_{1}^x [t] f'(t)\,dt$ . Here $f(n) = f(p_1^{x_1} \cdots p_r^{x_r}) = x_1 + \dots + x_r$ So, I got $\sum_{n\leq N} f(n) = (x_1 + \dots + x_r)[N]- \int_{1}^{x} [t] (x_1+\dots+x_r) \log t\,dt$ . Dividing by $N$ , $((x_1 + \dots + x_r)[N])/N =O(1)$ , and the other term after simplifying I got as $$(x_1+\dots+x_r)(\log N -1-(N\log N)/2+N/4+1/N(3/4))$$ which is not equal to what has to be proved. So, please help.","['analytic-number-theory', 'number-theory']"
4299137,Is this a good way to differentiate between evaluation of a function at a specific point and multiplication.,"A similar question has been asked but the answer states that it should be obvious from context, which is not satisfactory for my situation.
I have a function $l_x(\theta) = (x-1)\log(1-\theta)+\log \theta$ and wish to check that the second derivative at $\theta = 1/x$ is negative. \begin{align}
\frac{d}{d\theta} \frac{dl_x(\theta)}{d\theta}\left(\frac{1}{x}\right) &= \left(\frac{1-x}{(1-\theta)^2}-\frac{1}{\theta^2}\right)\left(\frac{1}{x}\right) \\\\
        &= \frac{x^2}{1-x}-x^2 < 0 \iff x \notin [0, 1]
\end{align} However, the second step does not clearly differentiate between evaluation at the point $\theta = 1/x$ and multiplication by $1/x$ . In fact, if I saw just that equation I would be convinced we were talking about multiplication. The only reason this can be seen is because the entire derivation of the second derivative has been omitted so that we can clearly see that the previous step was evaluating a derivative. Looking just at the line, one would think that the correct continuation would be \begin{align}
\frac{d}{d\theta} \frac{dl_x(\theta)}{d\theta}\left(\frac{1}{x}\right) &= \left(\frac{1-x}{(1-\theta)^2}-\frac{1}{\theta^2}\right)\left(\frac{1}{x}\right) \\\\
        &= \frac{1-x}{x(1-\theta)^2}-\frac{1}{x\theta^2} < 0
\end{align} which is entirely wrong. In order to make the difference clear, I have introduced in my work the notation $@$ to mean specifically evaluation at \begin{align}
\frac{d}{d\theta} \frac{dl_x(\theta)}{d\theta}@\left(\frac{1}{x}\right) &= \left(\frac{1-x}{(1-\theta)^2}-\frac{1}{\theta^2}\right)@\left(\frac{1}{x}\right) \\\\
        &= \frac{x^2}{1-x}-x^2 < 0 \iff x \notin [0, 1]
\end{align} However, I fear that this symbol might already have a different meaning in mathematics. (for one, in the Python programming language, @ means matrix multiplication). Is the notation $@$ for function evaluation unambiguous and are there better alternatives avaiable?","['notation', 'functions']"
4299150,Compute $\lim _{x\to 0}\left(\frac{\sqrt[3]{x}}{x}\right)$,"I want to compute $\displaystyle \lim _{x\to 0}\left(\frac{\sqrt[3]{x}}{x}\right)$ $\displaystyle \lim _{x\to 0^{+}}\left(\frac{\sqrt[3]{x}}{x}\right)$ \begin{align*}
 \lim _{x\to 0^{+}}\left(\frac{\sqrt[3]{x}}{x}\right)&= \lim _{x\to 0^{+}}\left(\frac{\sqrt[3]{x}}{\sqrt[3]{x}^{3}}\right)\\
&=\lim _{x\to 0^{+}}\left(\sqrt[3]{\frac{x}{x^3}}\right)\\
&=\lim _{x\to 0^{+}}\left(\sqrt[3]{\frac{1}{x^2}}\right)\\
&=\lim _{x\to 0^{+}}\left(\sqrt[3]{\frac{1}{(0^{+})^2}}\right)=+\infty\\
\lim _{x\to 0^{+}}\left(\frac{\sqrt[3]{x}}{x}\right)&=+\infty
\end{align*} $\displaystyle \lim _{x\to 0^{-}}\left(\frac{\sqrt[3]{x}}{x}\right)$ $x\to 0^{-}\implies x<0 \implies (-x)>0\implies (-x)^{3}>0 \implies (-x)=\sqrt[3]{(-x)^3}   $ \begin{align*}
 \lim _{x\to 0^{-}}\left(\frac{\sqrt[3]{x}}{x}\right)&=  \lim _{x\to 0^{-}}\left(\frac{-\sqrt[3]{x}}{-x}\right)\\
&=\lim _{x\to 0^{-}}\left(\frac{-\sqrt[3]{x}}{\sqrt[3]{(-x)^3}}\right)\\
\end{align*} I'm stuck here; please correct me if am wrong
Thanks in advance","['limits', 'calculus', 'limits-without-lhopital', 'real-analysis']"
4299225,Proving whether a complex function is analytical help!,"I am currently struggling to prove whether a complex function is analytical. I understand that I must employ the Cauchy-Riemann relations to do this. However, the answer I get is one that I can't reconcile with what I am told the answer is. This is the function; $f(x+iy) = |x^2 - y^2| + 2i|xy|$ With this, I believe that I need to do Cauchy-Riemann with four different functions, namely, $f(x+iy) = x^2 - y^2 + 2ixy$ $f(x+iy) = y^2 - x^2 - 2ixy$ $f(x+iy) = x^2 - y^2 - 2ixy$ $f(x+iy) = y^2 - x^2 + 2ixy$ Which makes me arrive at; $u_x = 2x = v_y = 2x$ , $u_y = -2y = -v_x = -2y$ $u_x = -2x = v_y = -2x$ , $u_y = 2y = -v_x = 2y$ $u_x = 2x = v_y = -2x$ , $u_y = -2y = -v_x = 2y$ $u_x = -2x = v_y = 2x$ , $u_y = 2y = -v_x = -2y$ respectively. Now, I think the answer is that the function is only analytical at x = 0, as I think that is the way only all of these equations can be solved simultaneously. But the prescribed solution is not this and I can't really see why. Thanks :D","['cauchy-riemann-equations', 'analyticity', 'complex-analysis', 'derivatives', 'analytic-functions']"
