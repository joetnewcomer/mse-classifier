question_id,title,body,tags
1778738,Why do I keep choosing the wrong probability rule?,"I was wondering if someone could help clarify probability rules, although I think I understand them, whenever I have a straight forward and/or probability question, I seem to want to do a permutation or combination. For example...using the numbers 1-7 form a 4 digit number without repeats. When I saw this I immediately wanted to do a permutation of 7P4, only to find out that it is 7*6*5*4. I hope this isn't too simple or vague a question, thanks in advance.",['combinatorics']
1778769,Find the range of the function $y = \sqrt{x^2 + 1}− x$?,"I have a function $y = \sqrt{x^2 + 1} − x$, where the Domain is $(−\infty,+\infty)$. Explanation for the domain I need to make sure the domain of the function does not include values of $x$
that will make the square root negative. This means that I need: $x^2+1\ge0$ the discriminant is negative and $x \in\mathbb R$. In fact, $x^2\ge 0$,  $\forall x\in\mathbb R$ and $1 > 0$. I wish, if it's possible, to explain the value of the range with an algebraic demonstration. I am using an example to solve the function for $x$ using $y$ as parameter: $\sqrt{x^2 + 1} − x = y$ $\sqrt{x^2 + 1} = y + x$ irrational equation, therefore: $$
\begin{cases}
& y + x >= 0\\
& x^2 + 1 =(y+x)^2
\end{cases}
$$ $$
\begin{cases}
& x >= -y\\
& x^2 + 1 =y^2 + x^2 + 2yx
\end{cases}
$$ $$
\begin{cases}
& x >= -y\\
& 1 =y^2 + 2yx
\end{cases}
$$ $$
\begin{cases}
& x >= -y\\
& x = \frac{1 - y^2}{2y}
\end{cases}
$$ now I need to find for which values is $x >= -y$, therefore: $ \frac{1 - y^2}{2y} < 0$ solving the numerator and denominator numerator $1 - y^2 > 0$ $y^2 -1 < 0$ solve for $-1 < y < +1$ denominator $2y > 0$ solve for $y > 0$ combine the tow solutions the inequality has occurred when $-1 < y < 0$ or $y> 1$",['functions']
1778783,Partial fraction decompostion,"Solve the partial fraction. Starting out with... $${x^2+1\over x^3-1}={x^2+1\over (x-1)(x^2+x+1)}$$ Then the partial fraction formula part of $\displaystyle {A\over x-1}+{Bx+C\over x^2+x+1}$. Solving with the ""shortcut"" method of substitution, here is what came up. Multiplying the partial fraction part by the problem, $${x^2+1\over x^3-1}={x^2+1\over (x-1)(x^2+x+1)}=
{A\over x-1}+{Bx+C\over x^2+x+1}
$$ is $$x^2+1=A(x^2+x+1)+B(Bx+C)(x-1)$$ Then letting $x=1$ then $2=3A$, $~~A=\frac23$ $x=0$ then $1=A-C$, then $C=-\frac{1}3$ $x=-1$ then 
$$\eqalign{2&=A(-2(-B)+C)=A+2B-2C=\frac23+2B-2\left(-\frac13\right)=\frac43+2B\cr
2-\frac43&=2B\cr}
$$
$B=  \frac13$ Then factoring, the answer is $\displaystyle{1\over3}\left({2\over x-1}+{x-1\over x^2+x+1}\right)$. Is there any possible quicker way, or is this the fastest?","['algebra-precalculus', 'partial-fractions']"
1778833,Is there an adjoint functor to the contravariant hom functor in the category of A-modules.,"I should start by saying that I don't know any category theory. However, I am reading Atiyah-MacDonald and have just learned that in the category of A-modules (where here A is a commutative unital ring) the functor $\,H=$ Hom$(N,\cdot\,)$ is the right adjoint of the functor $\,T=\cdot\,\otimes_A N;$ that is: Hom$(T(M),P)\cong$ Hom$(M,H(P))$ for all $A$-modules $M$ and $P.$ This leaves me with two questions: 1) What about tensoring on the left? I guess this is just the same since there is a canonical isomorphism between $N\otimes_A M$ and $M\otimes_A N$...? 2) Is there an adjoint for the contravariant hom functor Hom$(\cdot\,,N)?$ Many thanks!","['category-theory', 'abstract-algebra', 'commutative-algebra']"
1778838,How to get explicit solution of the fractional minimization problem?,"I need to minimize $f(x) = \frac{x^tQx}{a^tx-b}$, $Q$ is positive definite, $a,b$ are constant vector. But when I take gradient and set it equal to $0$, it's hard to get an explicit expression for $x$. Anyone knows how to solve for x explicitly?","['optimization', 'convex-analysis', 'calculus', 'multivariable-calculus', 'convex-optimization']"
1778872,Use cdf to find expectation,"I have a cdf for a $\mathbf {discrete}$ random variable, $X$, $$F_X(x)=1-(1-p)^{xn}$$ where $p\in(0,1)$, $n\in\mathbb N$, $x\in\mathbb N$ My thought is to use $$E[X]=\sum_{x=0}^\infty (1-F_X(x))=\frac {1}{1-(1-p)^n}$$ the final answer correct but I'm not sure about my work. Does the expected value equal to this in the discrete case? If it does, why is that? And is writing the sum starting from $x=0$ correct and why? Or is there any other way to get the expected value using cdf? On the other hand, does the equation of expected value and cdf correct in the continuous case by changing the sum into integral?","['expectation', 'density-function', 'probability-distributions', 'statistics', 'order-statistics']"
1778874,"Distribution of $R^2 = X^2 +Y^2$ where (X,Y) is a point on the unit circle","So I have a point $(X,Y)$ chosen from the unit disk with uniform distribution. And I'm attempting to find the distribution of $R^2$, where $R$ is the distance from the point to the origin. Now obviously the joint distribution: $f_{X,Y}(x,y)=\frac{1}{\pi}, \forall (x,y)\in C$ where $C=\{(x,y)\in\mathbb{R}:x^2+y^2\le1\}$ Now, I attempted to take the marginal distributions, square them, and then add them back, which gave the following: $F_X(x)=F_Y(x)=\frac{2}\pi\sqrt{1-x^2}, \forall x\in [-1,1]$ Applying the transformation, $g:[-1,1]\to\mathbb{R}, x\mapsto x^2$ \begin{align}
f_{X^2}(x) & =f_{Y^2}(x)=f_X(\sqrt{x}) \left|\frac{1}{2}x^{-1/2}\right| +  
\space f_X(-\sqrt{x}) \left| -\frac{1}{2}x^{-1/2}\right| \\[10pt]
& =\frac{2}\pi\sqrt{\frac{1-x^2}{x}}, \forall x\in[0,1]
\end{align} But then I realised I couldn't just add them as $X$ and $Y$ are dependent and as such so would $X^2$ and $Y^2$. So I assume I have to apply the transformation, $h:[-1,1]\times[-1,1] \to \mathbb{R}^2, (x,y)\mapsto(x^2,y^2)$ Which would entail me finding the Jacobian and inverse, etc. which I am fully able to do. However, $h$ is clearly not 1:1, and although I could get around that in the single variable case, I'm not sure what the anologue is for multy variables. So if anyone has any ideas on where to go from here, any assistance would be appreciated.","['probability', 'transformation', 'probability-distributions']"
1778886,Suppose that $E[X^n] = 3n$. Find $E[e^X]$...,"Suppose that $E[X^n] = 3n$. Find $E[e^X]$. Hint from my professor: $e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} +···$ Not quite sure how to solve this problem, wouldn't $e^x$ go on exponentially. Any help is really appreciated.","['statistics', 'probability', 'random-variables']"
1778920,Dissipation term in wave equation,"If we're given a string with mass density $\rho$ in units $\frac{M}{L^3}$ with constant cross-section $A$, tension $T$ in units $\frac{F}{L^2}$, and whose length is $L$; and then we assume that the vertical displacement is small, the slope of the string during displacement is also small (which probably means that we can approximate tension to be constant); then I think it would be correct to say that the governing equation in this case is $$u_{tt}=c^2u_{xx}$$ where $c^2 = \frac{T}{\rho}$. (Please correct me if I'm wrong). Now a dissipation term must be added, which should be directly proportional to the mass and velocity of the string. Would it be correct to specify the dissipation term as follows: $-b\underbrace{\rho A L}_\text{mass} (u_t)^2\frac{1}{u}$ (where $b$ is some dimensionless proportionality constant)? If this is correct, then the equation would become $$u_{tt}=c^2u_{xx}-bm(u_t)^2\frac{1}{u}$$ where $m$ is mass.","['physics', 'ordinary-differential-equations', 'mathematical-physics', 'partial-differential-equations']"
1778926,challenge problem: show this sequence is convergent.,"had this difficult question from a textbook, and I haven't been able to figure out the solution. say we have a sequence of bounded real numbers $a_n$ such that $2a_n \leq a_{n-1} + a_{n+1} \forall n\in\mathbb{N}$. Show that this sequence converges. What ive tried: I did some algebra on it then tried to use the cauchy criterion, since we don't know the limit, but we know the relation between terms. also tried moving things around and reindexing and using boundedness. but havent been able to come up with anything there either. since bounded and monotone converges. any help would be appreciated","['real-analysis', 'cauchy-sequences', 'sequences-and-series']"
1778930,Find $\lim_{n\to\infty}1+\sqrt[2]{2+\sqrt[3]{3+\sqrt[4]{4+\ldots+\sqrt[n]{n}}}}$,"$$\lim_{n\to\infty}1+\sqrt[2]{2+\sqrt[3]{3+\sqrt[4]{4+\ldots+\sqrt[n]{n}}}}$$ I have no idea about this. The equation can be written in its recursive form as: $$f(n) = g(1,n)$$ Where $$g(x,n) = [x\impliedby n]\cdot (x+ g(x+1,n))^{\frac 1x}+[x=n]\cdot (n)^{\frac 1n}$$ Of course, [] is the indicator function representing of piece wise notation.","['nested-radicals', 'calculus', 'limits']"
1778944,What is the intuition behind covering spaces?,"I've come to study this definition and become interested on the intuition behind it mainly because of the study of spinors, motivated by Quantum Mechanics. The definition of covering space is as follows: Let $X$ be a topological space. A covering space of $X$ is a topological space $C$ together with a continuous surjective map $p: C\to X$ such that for each $x\in X$ there is an open neighborhood $U$ of $x$ such that $p^{-1}(U)$ is the disjoint union of open sets which are homeomorphic to $U$ through $p$. The definition means that for each $x\in X$ there's $U\subset X$ which is open with $x\in U$ which that there are $\{V_{\alpha}\subset C : \alpha \in A\}$, with all of the $V_\alpha$ open, satisfying $V_{\alpha}\cap V_{\beta}=\emptyset$ when $\alpha\neq \beta$ and $p | V_\alpha$ being one homeomorphism between $V_\alpha$ and $U$ with $$p^{-1}(U)=\bigcup_{\alpha \in A}V_\alpha.$$ Now, although the definition is fine, I want to get some intuition about it. When we define covering spaces, what is the intuitive thing which we are really turning rigorous with a precise definition? What is really the idea behind this definition? I really couldn't get a nice intuition regarding this. Also, what is the importance of this definition, in the sense of when do we expect to see this concept becoming useful?","['intuition', 'general-topology', 'covering-spaces', 'definition']"
1778949,Every finite group is isomorphic to the Galois group of some polynomial,"I was reading through chapter 14 of Dummit and Foote just now and I came across the sentence  ""It is an open problem to determine whether every finite group appears as the Galois group for some polynomial over $\mathbb{Q}$ "" and I was wondering 2 things: Is it known whether each finite group is the Galois group of some polynomial (not necessarily over $\mathbb{Q}$ )? Why is this problem so hard? (I know this question may defy a simple answer, but from the very naive perspective of someone who knows very little--me--it isn't very clear why this should be too much harder than the abelian case considering what is known about the structure of finite groups in general) thanks :)","['galois-theory', 'group-theory']"
1778950,"Find $P(Y=5)$ for $Y=X_1+X_2+X_3$, where $X_i$ are mutually indpt Poisson R.V","In this problem, I am told that $X_1,X_2,X_3$ are mutually independent Poisson random variables with means $2,1,4$ respectively. I am also told to find the moment generating function for $Y=X_1+X_2+X_3$, which I found to be $M(t)=e^{7(e^t-1)}$. My question is how do I find $P(Y=5)$?. Is there a connection between $P(Y=5)$ and $M(t)$? Attempt at a solution: Since $X_1,X_2,X_3$ are mutually indpt. Poisson R.V. with mean $2,1,4$, we know that $$f(X_1)=\frac{e^{-2}2^x}{x!},$$ $$f(X_2)=\frac{e^{-1}1^x}{x!}$$ and $$f(X_3)=\frac{e^{-4}4^x}{x!}$$, which imply that $f(Y)=f(X_1)f(X_2)f(X_3)$ (is this true?I think it is bc of independence) Hence, after computations $$f(Y)=\frac{e^{-7}2^{3x}}{x!x!x!}.$$ To find $P(Y=5)$, do I just plug in $x=5$ into $f(Y)$? or do I take the summation of $f(Y)$ for $x=0$ all the way up to $x=5$ or neither? It's been a while since I have seen this material. Any help will be appreciated. Thanks!",['statistics']
1778970,Why is the square root of a number not plus or minus? [duplicate],"This question already has answers here : Square roots -- positive and negative (5 answers) Closed 8 years ago . For example, $\sqrt{4}$. I've asked a bunch of people and I get mixed answers all the time, as to whether it is $-2$ and $+2$ or just $+2$. How about if there's a negative in front of the square root sign, for example, $-\sqrt{4}$? Would that still be plus or minus or just minus?","['algebra-precalculus', 'radicals']"
1778990,Find a conformal mapping to map the intersection of the disks $|z|<1$ and $|z-1|<1$ to the unit disk $|w|<1$.,"My thinking is to use a composition of mappings to map 1) the given intersection to a sector of the complex plane enclosed by two rays from the origin, 2) the sector to the upper half plane, and 3) the upper half-plane to the unit disk. To achieve 1), I used 
$$
s(z) = \frac{z(1-i\sqrt{3})+i\sqrt{3} + 1}{z(1+i\sqrt{3})-i\sqrt{3} + 1}
$$
which maps the area conformally to all complex numbers $w$ such that $0 \leq \arg(w) \leq \frac{2\pi}{3}$. For 2),
$$
f(z) = z^{2/3}
$$
to map to the upper half imaginary plane. To get 3), I used
$$
g(z) = \frac{z-i}{z+i}
$$
to map the upper half imaginary plane to the unit disk. I then claim that
$$
T := (g \circ f \circ s)
$$
gives us the desired mapping. Firstly, does this look correct to others? Secondly, is there a faster way to achieve this without using a composition of 3 mappings?",['complex-analysis']
1779045,counting probability with multiple cases,"There are four different colors of paint one can use for four different houses. If one color can be used up to three times, how many total possibilities are there? I approached the problem by splitting it into cases: 1) each color used once 2) one color used twice 3) two colors used twice 4) one color used thrice but i dont know how to move on from here or if im right","['combinatorics', 'probability', 'probability-distributions']"
1779090,Let $B$ be an open subset of $C$ and $\partial B $ denotes boundary of $B$,"Let $B$ be an open subset of $C$ and $\partial B$ denote the boundary of $B$.Which of the following are correct? (a) For every entire function $f$ ,we have $\partial(f(B)) \subset f(\partial B)$ (b) For every entire function $f$ and a bounded open set $B$ , we have       $\partial(f(B)) \subset f(\partial B)$ (c) For every function $f$ , we have $\partial(f(B)) = f(\partial B)$ (d) There exist an unbounded open subset $B$ of $C$ and an entire function $f$ such that $\partial(f(B)) \subset f(\partial B)$ From where I start, I can't understand.Please help something. Thank you for your time.",['complex-analysis']
1779105,Maximum value of $\sin A+\sin B+\sin C$?,What is the maximum value of $\sin A+\sin B+\sin C$ in a triangle $ABC$. My book says its $3\sqrt3/2$ but I have no idea how to prove it. I can see that if $A=B=C=\frac\pi3$ then I get $\sin A+\sin B+\sin C=\frac{3\sqrt3}2$. And also according to WolframAlpha maximum is attained for $a=b=c$. But this does not give me any idea for the proof. Can anyone help?,"['inequality', 'optimization', 'trigonometry', 'triangles', 'geometry']"
1779118,Relationship between two-equation constrained optimization and one-equation version,"I am learning about the Lagrange multiplier. Here's what I understand so far. Suppose a point $P$ is a minimizer of $f(x)$ subject to $g(x)=0$. Then any movement along that level-curve of $g$ must leave $f$ unaffected, because otherwise (assuming $f$ smooth or something) there would be some point higher than that at $P$. So the level-curve of $g$ must be perpendicular to the gradient of $f$, denoted $\nabla f$. I think the gradient of $g$ is perpendicular to the level curve of $g$ for a similar reason. So the gradients of both $g$ and $f$ are perpendicular to the level curve of $g$. Therefore we can express this situation as $$\nabla f(p) = -\lambda \nabla g(P)$$
with a conventional minus sign, and also keep in mind that $$g(P) = 0.$$ These constitute our two equations for minimizing $f$ subject to $g(x)=0$. I think this should be enough information to just solve it from here. But I often see a single-equation formulation written something like 
$$L(x,\lambda) = f(x) - \lambda g(x).$$ Questions (or clusters of questions): What is the relationship between these two formulations? It seems like the first is just some derivative of the second? How would we arrive at the single-equation formulation? What is the point of the second if the first (i.e. the pair of equations) suffice to solve it? In an applied problem would we be trying to get from the first to the second? It seems like the first is a manageable problem, and I don't really understand where the second comes from.","['derivatives', 'optimization', 'partial-derivative', 'constraints', 'lagrange-multiplier']"
1779143,Integrate $\sin^2(nx)\csc^2(x)dx$,"I need help to integrate $$\int{\sin^2(nx)\csc^2(x)dx}$$ 
Integral of each part is easy $$\int{\sin^2(nx)}dx=\frac{x}{2}+\frac{\sin(2nx)}{4n}+C$$ $$\int{\csc^2(x)}dx=-\cot(x)+C$$
I then tried to use the integral by parts but failed to make it work. Also tried Walfram calculator but it returned a very complicated expression that's not what I wanted. Can you help to resolve this indefinite integration? Definite integral $$\int_{2\pi}^{N\pi}{\sin^2(nx)\csc^2(x)}dx$$ where $N$ is big natural number, is also helpful.","['indefinite-integrals', 'definite-integrals', 'integration', 'trigonometry']"
1779194,"What is ""Bra"" and ""Ket"" notation and how does it relate to Hilbert spaces?","This is my first semester of quantum mechanics and higher mathematics and I am completely lost. I have tried to find help at my university, browsed similar questions on this site, looked at my textbook (Griffiths) and read countless of pdf's on the web but for some reason I am just not getting it. Can someone explain to me, in the simplest terms possible, what this ""Bra"" and ""Ket"" (Dirac) notation is, why it is so important in quantum mechanics and how it relates to Hilbert spaces? I would be infinitely grateful for an explanation that would actually help me understand this. Edit 1: I want to thank everyone for the amazing answers I have received so far. Unfortunately I am still on the road and unable to properly read some of the replies on my phone. When I get home I will read and respond to all of the replies and accept an answer. Edit 2: I just got home and had a chance to read and re-read all of the answers. I want to thank everyone again for the amazing help over the past few days. All individual answers were great. However, the combination of all answers is what really helped me understand bra-ket notation. For that reason I cannot really single out and accept a ""best answer"". Since I have to accept an answer, I will use a random number generator and accept a random answer. For anyone returning to this question at a later time: Please read all the answers! All of them are amazing.","['notation', 'hilbert-spaces', 'mathematical-physics', 'functional-analysis', 'linear-algebra']"
1779197,How to Solve the following Cauchy Problem?,"I want to find $x,y: \mathbb{R} \rightarrow \mathbb{R}$ such that: $x'(t)= -3x(t)+4y(t)$ $y'(t) = -x(t) + y(t)$ with the initial conditions $x(0) = y(0) = 1$ $\begin{bmatrix}
         x' \\
         y'
      \end{bmatrix} =  \begin{bmatrix}
         -3 & 4  \\
         -1 & 1
      \end{bmatrix} \times  \begin{bmatrix}
         x \\
         y
      \end{bmatrix}  $ Let the $A=   \begin{bmatrix}
         -3 & 4  \\
         -1 & 1
      \end{bmatrix}$ $det(A-\lambda I) = (1+\lambda)^2 = 0 \implies \lambda_1 = \lambda_2 = -1$ The eigenvectors are $V_{\lambda_1} = V_{\lambda_2} =  \begin{bmatrix}
         1  \\
         1
      \end{bmatrix}$ From there I am stuck and I don't know what to do","['real-analysis', 'ordinary-differential-equations', 'linear-algebra']"
1779221,How to remember sum to product and product to sum trigonometric formulas?,"They are: \begin{align}
\cos(a)\cos(b)&=\frac{1}{2}\Big(\cos(a+b)+\cos(a-b)\Big) \\[2ex]
\sin(a)\sin(b)&=\frac{1}{2}\Big(\cos(a-b)-\cos(a+b)\Big) \\[2ex]
\sin(a)\cos(b)&=\frac{1}{2}\Big(\sin(a+b)+\sin(a-b)\Big) \\[2ex]
\cos(a)\sin(b)&=\frac{1}{2}\Big(\sin(a+b)-\sin(a-b)\Big) \\[2ex]
\cos(a)+\cos(b)&=2\cos\left(\frac{a+b}{2}\right)\cos\left(\frac{a-b}{2}\right) \\[3ex]
\cos(a)-\cos(b)&=-2\sin\left(\frac{a+b}{2}\right)\sin\left(\frac{a-b}{2}\right) \\[3ex]
\sin(a)+\sin(b)&=2\sin\left(\frac{a+b}{2}\right)\cos\left(\frac{a-b}{2}\right) \\[3ex]
\sin(a)-\sin(b)&=2\cos\left(\frac{a+b}{2}\right)\sin\left(\frac{a-b}{2}\right)
\end{align} I have found nice mnemonics that helped me to remember the reduction formulae and others but I can't find a simple relationship between the formulas above. Can you help?","['algebra-precalculus', 'mnemonic', 'trigonometry']"
1779236,What is the notation for the empty matrix?,"My question: What is a notation for an empty 0x0 matrix (i.e. the matrix for the only linear map $f:\{0\}\to\{0\}$)? Is it written $()$? How can I distinguish the 0x0 matrix with for example the 0x3 matrix or the 3x0 matrix? What I have already found out: Concerning the section “Empty matrices” of the Wikipedia article “Matrix (mathematics)” there “is no common notation for empty matrices”. But unfortunately I haven't found any notation so far... Notes: I am looking for a notation which is used in a textbook. I am not interested in how empty matrices can be created in CAS like Matlab, Mathematica, etc. Reason for this question: In our course we had the task to draw all graphs with three vertices and to state the incidence matrix for each drawn graph. Thus, for the empty graph I have to state a 0x3 matrix, but I didn't know the right notation for it...","['matrices', 'notation', 'linear-algebra']"
1779281,"Find maximum value of$ |z_1 -z_2 |^2 + |z_2 -z_3 |^2 + |z_3 -z_1 |^2 $ if $|z_1 | = 2, |z_2 | = 3, |z_3 | = 4 $ [duplicate]","This question already has answers here : Given complex $|z_{1}| = 2\;\;,|z_{2}| = 3\;\;, |z_{3}| = 4\;\;$ : when and what is $\max$ of $|z_{1}-z_{2}|^2+|z_{2}-z_{3}|^2+|z_{3}-z_{1}|^2$ (3 answers) Closed 5 years ago . i know $ |z_1 -z_2 |^2 =  |z_1 |^2 + |z_2 |^2 - 2|z_1 | |z_2 | \cos \alpha $ where $ \alpha $ is angle between $z_1$  and $z_2 $. Similarly i get $ |z_1 -z_2 |^2 + |z_2 -z_3 |^2 + |z_3 -z_1 |^2 = 2(4+9+16) - 2(6 \cos \alpha+8 \cos \beta+12\cos \gamma)$ What will be minimum value of $ 6 \cos \alpha+8 \cos \beta+12\cos \gamma $ if  $ z_1  , z_2, z_3   $ lie on the 3 given circles","['trigonometry', 'complex-numbers']"
1779287,Definition of Cylindrical Brownian Motion and Spatial Correlation,"From Gawarecki and Mandrekar, Stochastic Differential Equations in Infinite Dimensions : We call a family $\{ W_t \}_{t\geq 0}$ defined on a filtered probability space $(\Omega, \mathcal{F}, \{\mathcal{F}_t\}_{t \geq 0}, P)$ a cylindrical Wiener process in a Hilbert space $K$ if: For an arbitrary $t\geq0$, the mapping $W_t:K \rightarrow L^2(\Omega, \mathcal{F}, P)$ is linear. For an arbitrary $k \in K$, $W_t(k)$ is an $\mathcal{F}_t$-Brownian motion. For arbitrary $k, k' \in K$ and $t \geq 0$, $E(W_t(k)W_t(k')) = t \langle k, k'\rangle_K$. Exercise 2.2 Show that $E(W_t(k)W_s(k')) = (t \wedge s) \langle k, k' \rangle_K$ and conclude that $W_t(f_j), j=1, 2, ...$, where $\{f_j\}_{j=1}^\infty$ is an orthonormal basis in $K$, are independent Brownian motions. I am new to functional analysis and don't see how to start this exercise. I can see that the third point is useful, but how does one begin? My thought was the following (assuming $t \leq s$: $$
\begin{align*}
E(W_t(k)W_s(k')) &= E(E(W_t(k)W_s(k')|\mathcal{F}_t))\\
&= E(W_t(k)E(W_s(k')|\mathcal{F}))\\
&= E(W_t(k)W_t(k')\\
&= t\langle k, k' \rangle_K
\end{align*}
$$
by LIE, point two, and point three, respectively. The opposite holds for $s \leq t$. Is this the right approach? Then the independence follows from orthogonality.","['stochastic-processes', 'probability-theory', 'functional-analysis', 'brownian-motion', 'stochastic-calculus']"
1779299,A combinatorial task I just can't solve,"Suppose you have $7$ apples, $3$ banana, $5$ lemons. How many options to form $3$ equal in size baskets ($5$ fruits in each) are exist? At first I wrote: 
$\displaystyle \frac{15!}{7!3!5!} $
But its definitely not true because I do not care about permutations of apples in one $5$-sized basket. So the real value in divider is smaller than $7!$ (the same logic for other fruits). Textbook says that the answer is $\displaystyle\frac{15!}{2^93^35^3}$ The method of getting of numerator is clear but divider...","['generating-functions', 'combinatorics', 'discrete-mathematics']"
1779328,Prove that $\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$,Using $\text{n}^{\text{th}}$ root of unity $$\large\left(e^{\frac{2ki\pi}{n}}\right)^{n} = 1$$ Prove that $$\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$$,"['products', 'roots-of-unity', 'trigonometry', 'complex-numbers']"
1779348,"If $\dim(V) $ is infinite, show that $V\oplus V$ is isomorphic to $V$","For a vector space $V$ of infinite dimension, to show that $V\oplus V$ is isomorphic to $V$ is to show that there exists an invertible linear transformation between $V \oplus V $ and $V$. Every vector space have a basis.
If $B$ is the infinite set of basis for $V$, then the set $B\oplus0 \cup 0\oplus B$ is the basis for $V \oplus V$.  Using axiom of choice the cardinality $|B\oplus0 \cup 0\oplus B|= |B\oplus0|+|0 \oplus B|=|B|+ |B| = \max\{|B|,|B|\}= |B|$ and then I was trying to argue that this implies that there is an isomorphism between the bases of $V\oplus V$ and $V$, so they are isomorphic. But I'm not sure how $|B|+|B|=\max\{|B|,|B|\}= |B|$ is true by the axiom of choice. How could we show that if $V$ is infinite dimensional vector space, then $V\oplus V \cong V$?","['abstract-algebra', 'linear-algebra', 'vector-spaces']"
1779361,How to prove $\cos^4x - \sin^4x - \cos^2x + \sin^2x$ is always $0$?,"So I have a small problem here where I have to prove the following : $$\cos^4x - \sin^4x - \cos^2x + \sin^2x = 0 $$ I know that the 2nd part is always $1$, so I need to prove that the first part also equals $1$. So how should I prove it ? Edit : Sorry, the equation itself was wrong. I've edited it.",['trigonometry']
1779371,Compute $\sum_{k=0}^{180} \cos^22k^\circ $,I'm trying to compute the following: $$\sum_{k=0}^{180} \cos^22k^\circ $$,['trigonometry']
1779383,$Df(x_0)$ is nonzero $\Rightarrow $invertible. What about the reverse?,"Problem says: Assume that $f:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}$
  is smooth and C-R
  equations $$\frac{\partial f_{1}}{\partial x}	=	\frac{\partial f_{2}}{\partial y}
$$ $$\frac{\partial f_{1}}{\partial y}	=	-\frac{\partial f_{2}}{\partial x}$$ holds. Suppose that $f$
  is locally invertible at $x_{0}$
 . Show that $Df(x_{0})\neq0$
 . Find a counter-example when you don't have C-R
  equations. . Clearly, by inverse function theorem, $\Rightarrow $ direction holds. But showing that the other direction hold under C-R equation seems to be hard since the theorem (implicit, inverse function theorem) only says about ($\Rightarrow $) direction. How could I show that the reverse holds?","['multivariable-calculus', 'real-analysis']"
1779414,2D parametric equation for an arc between two points with a start angle,"What's a parametric equation (eg. $(x,y)=(\cos(t \cdot 2\pi),\sin(t \cdot 2\pi)$ plots a circle where $t$ is the 'time' along the circle) that draws an arc between the two points $(x_0,y_0)$ and $(x_1,y_1)$ with a start angle $a$. Also, can I calculate what the end angle is going to be? I drew a picture to show you what I mean. Thank you :) The picture","['parametric', 'angle', 'trigonometry', 'geometry']"
1779418,Taylor series third order approximation,"There has been this question that had been bothering me for a while and I could not find a satisfying answer on the internet or any of the books even though it is not very complex. i) Its because if I have to find a third order polynomium approximation using taylor series for a 2 variable function, then is it correct to write that the third term will look something like this, $$ ... + \frac{1}{3!}[f_{xxx}(x_0,y_0)(x-a)^3 + 6f_{xxy}(x_0,y_0)(x-a)(y-b)+f_{yyy}(x_0,y_0)(y-b)^3] + ....   $$ I was a bit unsure about the middle part. ii) About the hessian matrix, how would I write a hessian matrix if I have to make one for a third order like the one above. I know that for second order it looks like, $$H_f(x,y) = \left(\begin{array}{cccc}
f_{xx} & f_{xy} \\
f_{yx} & f_{yy}
\end{array}\right)$$ Thank You :)","['multivariable-calculus', 'hessian-matrix', 'taylor-expansion']"
1779458,is $y = \sqrt{x^2 + 1}− x$ a injective (one-to-one) function?,"I have a function  $y = \sqrt{x^2 + 1}− x$ and I need to prove if it's a Injective function (one-to-one).  The function f is injective if and only if for all a and b in A, if f(a) = f(b), then a = b $\sqrt{a^2 + 1} − a = \sqrt{b^2 + 1} − b$ $\sqrt{a^2 + 1} + b = \sqrt{b^2 + 1} + a$ $(\sqrt{a^2 + 1} + b)^2 = (\sqrt{b^2 + 1} + a)^2$ $a^2 + 1 + b^2 + 2b\sqrt{a^2 + 1} = b^2 + 1 + a^2 + 2a\sqrt{b^2 + 1}$ $2b\sqrt{a^2 + 1} = 2a\sqrt{b^2 + 1}$ $b\sqrt{a^2 + 1} = a\sqrt{b^2 + 1}$ $(b\sqrt{a^2 + 1})^2 = (a\sqrt{b^2 + 1})^2$ $b^2(a^2 + 1) = a^2(b^2 + 1)$ $b^2a^2 + b^2 = a^2b^2 + a^2$ $b^2 = a^2$ $\sqrt{b^2} = \sqrt{a^2}$ $b = \pm a$ then the function is not injective because $b$ in not equal to $a$, but using online solver the function is injective.",['functions']
1779481,Why does existence of directional derivatives not imply differentiability?,"In my notes I have:
$$Df\big|_{\mathbf{a}}(\mathbf{h})=\lim_{t\to 0}\frac{f(\mathbf{a}+t\mathbf{h})-f(\mathbf{a})}{t}$$
It says that even if this limit exists for all $\mathbf{h}$, we do not necessarily have differentiability. Is this because the $Df\big|_{\mathbf{a}}$ we get from this is not necessarily linear, or even continuous, in $\mathbf{h}?$ Can I say that if the result map is linear & continuous then we have differentiability? I ask because I am doing a problem which asks to check differentiability of some functions $f:\mathbb{R}^2\to\mathbb{R}$. One of them is: $$f(x,y)=xy\left(\frac{x^4-y^4}{x^4+y^4}\right)\qquad f(0,0)=0$$
which I thought was differentiable at $\mathbf{0}$ since $Df\big|_{\mathbf{0}}(\mathbf{h})=0$ for all $\mathbf{h}$, but I am not sure this is enough.","['derivatives', 'real-analysis', 'limits', 'proof-verification', 'multivariable-calculus']"
1779516,Is it always safe to assume that a integral is zero if it has equal bounds?,"I'm still a ""newbie"" on mathematical analysis and I stumbled upon this integral.
This is my solution: $$\int_0^{2\pi}{\frac{x\cos x}{1+\sin^2x}dx}$$ Now I substitued with $t=\sin x$
$$=\int_0^0{\frac{a\sin x}{1+t^2}dt} = 0$$ But I found that the integral $\displaystyle\int_0^\pi{\frac{\cos x}{\sin^2 x}dx}$ which I solved by the same substitution is not $0$ but it is indeterminate. Not sure if this is right.","['integration', 'calculus', 'analysis']"
1779544,Prove $\frac{1}{\log_2(\pi)}+\frac{1}{\log_3(\pi)}+\frac{1}{\log_5(\pi)}<3$,Prove: $$\frac{1}{\log_2(\pi)}+\frac{1}{\log_3(\pi)}+\frac{1}{\log_5(\pi)}<3.$$ I don't have any good idea for this problem. I have tried AM-GM inequality but didn't see the solution.,['algebra-precalculus']
1779583,"If $a$ is not divisible by $7$, then $a^3 - 1$ or $a^3 + 1$ is divisible by $7$","Determine is, in general, true or false. Recall that a universal statement is true if it is true for all possible cases while it is false if there is even one counterexample. Be prepared to prove that your answer is correct by supplying a proof or counterexample, whichever is appropriate. If $a$ is not divisible by $7$, then $a^3 - 1$ or $a^3 + 1$ is divisible by $7$. Solution: According to Fermat's Little Theorem, we know that if $p$ is prime and $a$ is an integer and $p \nmid a$, then $a^{p - 1} \equiv 1 \mbox{ (mod $p$)}$.
Hence, using the above theorem we get that
$$
\begin{array}{rcl}
a^{7-1} & \equiv & 1 \mbox{ (mod 7)}\\
a^6 & \equiv & 1 \mbox{ (mod 7)}\\
 a^6  -1 & = & 7k \mbox{ where $k$ is an integer}\\
(a^3-1)(a^3+1) & = & 7k
 \end{array}
$$
Since $k$ is integer
$(a^3-1)$ or $(a^3+1)$ is divisible by $7$. Could you check it for me please is it correct or not?","['number-theory', 'discrete-mathematics', 'elementary-number-theory']"
1779607,How to prove this geometry inequality (1) with $2(DF+EF)\ge BC$,"There is a picture which hope to illustrate the configuration: $\triangle ABC$ is such that $\angle A=\dfrac{2\pi}{3}$, and $F$ is the midpoint of $BC$, and $D,E$ lie on $AB,AC$ respectively, such that $DE ||BC$.
Show that:
$$2(DF+EF)\ge BC$$ maybe use cosine  theorem,let $\dfrac{BD}{AB}=k,AB=c,BC=a,AC=b$, and in $\Delta ABC$ we use cosine theorem we have
$$a^2=b^2+c^2+bc\tag{1}$$
and $$DF^2=BD^2+BF^2-2BD\cdot BF\cos{\angle B}
=(kc)^2+\dfrac{a^2}{4}-2kc\cdot\dfrac{a}{2}\dfrac{a^2+c^2-b^2}{2ac}=k^2c^2+\dfrac{a^2}{4}-\dfrac{k(a^2+c^2-b^2)}{2}$$
the same as
$$EF^2=\dfrac{a^2}{4}+(kb)^2-\dfrac{k(a^2+b^2-c^2)}{2}$$
it is equivalent $$\sqrt{4k^2c^2+a^2-2k(a^2+c^2-b^2)}+\sqrt{4k^2b^2+a^2-2k(a^2+b^2-c^2)}\ge a,0\le k\le 1\tag{2}$$ I have tried some inequality (AM-GM,Cauchy-Schwarz,$\cdots$) to prove $(2)$, but didn't see the solution.","['geometric-inequalities', 'geometry']"
1779615,How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$?,"I can show that the following limit exists but
I am having difficulties to find it. It is
$$\lim_{n\to \infty} \sum_{k=1}^n \frac{k^n}{n^n}$$
Can someone please help me?","['sequences-and-series', 'calculus']"
1779636,What is implied by $f \circ g = g \circ f$?,"For any two functions $f(x)$ and $g(x)$ we are given $f \circ g = g \circ f$. What does this imply? I found that $f(x) = g(x)$, $f(x) = g^{-1}(x)$ and $ f(x) = x \ (\neq g(x))$ are some of the solutions. However, are they the only functions satisfying this? If so, how can we prove it? Clarification : $f \circ g$ denotes the composition of $f$ and $g$, i.e, $f(g(x))$","['algebra-precalculus', 'calculus', 'functions']"
1779647,Why is the Galois group for a cyclotomic extension isomorphic to $Z_n^{\times}$ and not to $Z_{n-1}$?,"I'm struggling to understand this.  Let $\mathbb{Q}(\xi)$ be the splitting field for $x^n - 1$, so here $\xi$ is the primitive n-th root of unity.  If I consider the possible automorphisms of this field, I will get: $$\iota \textrm{ (the identity)}, \qquad \textrm{and} \qquad \sigma_k: \xi \mapsto \xi^k, \quad2 \leq k< n.$$ I will have $n-1$ elements, same as $Z_{n-1}$.  How can it be shown that this is actually $Z_n^{\times}$ instead?  What am I missing?","['abstract-algebra', 'galois-theory', 'extension-field', 'field-theory']"
1779653,"Proving limit using the definition , Is my approach correct?","I have the following problem: Prove that $$\lim_{n\to \infty} \frac {n^2}{n^2-10n-100}=1$$ using the definition. My approach to the problem was finding $f(n)-1$ which has to be smaller than $\epsilon$. The value of that was $\frac {10n+100}{n^2-10n-100}$, so I said $10n+100<10n$ and $n^2-10n-100 < n^2$, so I find that $n < \frac {10}\epsilon$ : The approach shown by my instructor was really complicated, so I want to know if this one is correct. Thank you","['sequences-and-series', 'limits']"
1779709,Combinatorics - Number of Paths in a Grid with a Hole,"Given a $12\times12$ grid with a hole of $4\times4$ in its middle, how many short paths (right or up only) are there from $(0,0)$ to $(12,12)$. I tried using inclusion-exclusion by counting the number of paths that go through at least one point inside the hole, then count the paths that go through at least 2 points in the hole etc. but so far I got lost. Would appreciate any (perhaps simpler) suggestions to try solve this. Thanks","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
1779756,Why is (one of) the criteria for a Noetherian module not implied by Zorn's lemma?,"One of the equivalent definitions of a Noetherian $R$-module is Every nonempty collection of $R$-submodules has a maximal element under inclusion so, what I'm wondering is, why is this not an immediate application of Zorn's lemma that applies to any module? Our $R$-submodules have a natural partial order under inclusion, and they are bounded above by $M$ because they all must be contained in $M$. Of course my logic must be wrong somewhere because not every module is Noetherian. What am I missing?","['abstract-algebra', 'modules']"
1779780,Find the derivative of $\sqrt[n]{x}$ using the formal definition of a derivative,"Given $\sqrt[n]{x}$, prove using the formal definition of a derivative that : $$\frac{d}{dx} (\sqrt[n]{x}) = \frac{x^{\frac{1-n}{n}}}{n}$$ Now this would be ridiculously easy to show using the Power Rule, but alas, that is not the goal of this question. Using the formal definition of a limit we get : \begin{equation} 
\begin{split}
f'(x) & = \lim_{h \ \to \  0} \frac{f(x+h)-f(x)}{h}  \\
 & = \lim_{h \ \to \  0} \frac{\sqrt[n]{x+h}-\sqrt[n]{x}}{h} \\
 & = \lim_{h \ \to \  0} \frac{(x+h)^{\frac{1}{n}}-(x)^{\frac{1}{n}}}{h}
\end{split}
\end{equation} But it is unclear to me how to proceed next, essentially all we need to do to get this limit into a determinate form (it currently is in an indeterminate form) is to factor out a $h$ in the numerator, but there doesn't seem to be an obvious way to do so. What algebraic technique , would you use to factor out a $h$ in the numerator in this case? For $n=2$, you could easily multiply the fraction by the conjugate to get the limit into a determinate form, and for $n=3$, you could do the same with the help of a few identities, but how would you go about this for the general case, as stated in the example I've given above. This question is the general $n^{th}$ case of finding the derivative using the formal definition, for functions such as $f(x) = \sqrt{x}$, $f(x) = \sqrt[3]{x}$ and so forth, and is aimed at finding the best algebraic technique to manipulate the limit to get it into a determinate form.","['derivatives', 'real-analysis', 'calculus', 'limits']"
1779783,"Are closed points of a scheme $\frac{X}{k}$ the same $\overline{k}$-points, modulo Galois group action","Let $k$ be a field, and $X$ a scheme locally of finite type over $k$. Let $\overline{k}$ be the algebraic closure of $k$. Is it true that the set of closed points of $X$ is in bijection with $$\frac{X(\overline{k})}{Gal(\frac{\overline{k}}{ k})}\quad ?$$ I think so, but I don't recall seeing this anywhere before, so either it's not true or I wasn't paying attention.
The way the (alleged) correspondence goes is that given a closed point $x \in X$, Zariski Lemma says that $\frac{\kappa(k)}{k}$ is a finite extension, and hence $\kappa(x)$ can be embedded in $\overline{k}$ in $Aut(\frac{\kappa(x)}{k})$ ways, and so that's why I am quotienting out by the Galois action. Then the composite $O_{X, x} \to \kappa(k) \to \overline{k}$ gives rise to a $\overline{k}$-valued point of $X$.",['algebraic-geometry']
1779787,Discounted price process in Black-Scholes model is a martingale with respect to Q.,"I have been presented a proof that the discounted price process in the Black and Scholes formula is a martingale, but there is something important omitted, and I am not able to fill in the gap. I will present the presentation of the proof I am given here: start of proof: The price process is modelled by $dS_t = \mu dt+\sigma dB_t$, $\mu, \sigma$ constant. We also have a constant interest rate r, and we define $\tilde{S}_t = e^{-rt}S_t $. Using the Ito's lemma on $e^{-rt}S_t$, we find that $d\tilde{S}_t =\tilde{S}_t(\mu-r)dt+\tilde{S}_t\sigma d B_t$ Then we have this version of Girsanovs theorem: If $X_t$ is a stochastic process where: $E[\exp(\int_o^
tX_t^2/2)]<\infty, \forall t.$ And B is a Brownian motion under an
  original probability space $(\Omega, \mathcal{F},P)$. Then
  $\tilde{B}_t=B_t-\int_o^tX_s ds$ is a brownian motion on the space
  $(\Omega, \mathcal{F},Q)$, where $Q(A)=E_P[1_A \exp(\int_0^tX_t
dB_t-1/2\int_0^tX_t^2ds)]$. We then define $X_t = (r-\mu)/\sigma$. It satisfies the novikov condition because it is a constant, and hence. $\tilde{B}_t=B_t-(\mu-r)/\sigma \cdot t$ is a Brownian motion under Q. But now comes by problem, in order to finish the proof they say that: since we have $\tilde{S}_t=\tilde{S}_0+\int_0^t\tilde{S}_s(\mu-r)ds+\int_0^t\tilde{S}_s\sigma dB_S=\tilde{S}_0+\int_0^t\tilde{S}_s\sigma(\frac{\mu-r}{\sigma}ds+dB_s)=\tilde{S}_0+\int_0^t\tilde{S}_S\sigma d\tilde{B}_s$. And this is a martingale under Q.$\square$ My problems are these: I agree that $\tilde{S}_0+\int_0^t\tilde{S}\sigma d\tilde{B}_t$. is a martingale under Q. But how do we have that $\tilde{S}_0+\int_0^t\tilde{S}_s\sigma(\frac{\mu-r}{\sigma}ds+dB_s)=\tilde{S}_0+\int_0^t\tilde{S}_S\sigma d\tilde{B}_s$. I do see that it is logical, and where it comes from. But we can't just play around with differentials like that? I see where it comes from if we can manipulate differentials in the formula like this, but we can't just do algebra on differentials? If we in some way are able to deduce that $\tilde{S}_0+\int_0^t\tilde{S}_s\sigma(\frac{\mu-r}{\sigma}ds+dB_s)=\tilde{S}_0+\int_0^t\tilde{S}_S\sigma d\tilde{B}_s$ (from point 1), we still can't know that the integral considered as a r.v. will be the same under the measure Q, because in the definition of the stochastic integral when we defined it, it depended very much on what probability-space we started with(it realies heavily on cauchy and convergence of random variables). So how do we know that the integral $\int_0^t\tilde{S}_S\sigma d\tilde{B}_s$ is the same considered in both $(\Omega, \mathcal{F},P)$ and $(\Omega, \mathcal{F},Q)$? Is $\int_0^t\tilde{S}_s\sigma d\tilde{B}_s[P]=\int_0^t\tilde{S}_s\sigma d\tilde{B}_s[Q]?$ Can you guys please help me? Do you see how to make the problem complete? In summery the problem is this: How do we see that 
$\tilde{S}_0+\int_0^t\tilde{S}_s(\mu-r)ds+\int_0^t\tilde{S}_s\sigma dB_s=\tilde{S}_0+\int_0^t\tilde{S}_S\sigma d\tilde{B}_s$.
When the first stochastic integral is created using $(\Omega,\mathcal{F},P)$, and the second stochastic integral is created using $(\Omega, \mathcal{F},Q)$?","['stochastic-processes', 'probability-theory', 'stochastic-integrals', 'stochastic-analysis', 'stochastic-calculus']"
1779831,Why isn't Dominated Convergence Theorem taught in intro analysis,In a course based off a book like Rudin's Principles of Mathematical Analysis that does non-measure theoretic analysis why isn't dominated convergence taught? It would be useful since continuous functions are Lebesgue measurable. Is there not a way to prove a non-measure theoretic version of this statement?,['real-analysis']
1779895,"Interval notation: infinity, -infinity in closed interval","I was watching a video stream a little bit ago and noticed on an equation without context that had the interval $\left[{-\infty, \infty}\right]$. This was preculiar to me as I've never seen the interval for $\mathbb{R}$ expressed this way before, however, I do vaguely remember hearing something about this a long time ago and its use in the construction of certain uncountably infinite sets in axiomatic set theory, but I'm not sure. Is there any use in writing ${-\infty, \infty}$ in a closed interval? If so, what would its use be?","['infinity', 'notation', 'elementary-set-theory']"
1779912,If $3^x +3^y +3^z=9^{13}$.Find value of $x+y+z$,Problem: If $3^x +3^y +3^z=9^{13}$.Find value of $x+y+z$. Solution: $3^x +3^y +3^z=9^{13}$ $3^x +3^y +3^z=3^{26}$ I am unable to continue from here. Any assistance is appreciated. Edited $9^{13} =3^{26}$ $=3^{25} (3)$ $=3^{25} (1+1+1)$ $=3^{25} + 3^{25} + 3^{25}$ So $x+y+z =75$,['algebra-precalculus']
1779935,Find $\lim_{y\to 1^-} \frac{\sqrt {1-y^2}}{y-1}$ without L'Hopital.,I need help understanding this limit question. Wolfram states the answer as −∞. We aren't allowed to use L'Hopital's. A step by step solution would be greatly appreciated. $$\lim_{y\to 1^-} \frac{\sqrt {1-y^2}}{y-1} = −∞$$,"['limits-without-lhopital', 'calculus', 'limits']"
1779975,Solve this logarithmic equation.,"$2\log_6(x^{1/2}+x^{1/4}) = \log_4 x$ I don't have any good ideas how to start. Im stuck at this easy question, I don't know what is going on.",['algebra-precalculus']
1780006,Counting numbers of fruit baskets,"Suppose you have $10$ apples, $12$ bananas, and $8$ peaches, and you want to divide them into $3$ baskets containing $10$ fruit each. In how many ways can you do this, if the fruit of each type is indistinguishable but the baskets are numbered? (This question is a modification of A combinatorial task I just can't solve ,  and I couldn't think of a simple approach to solve it other than trying to list all the possible ways of distributing the bananas in the baskets, and then the apples.","['generating-functions', 'combinatorics', 'discrete-mathematics']"
1780026,A conjectured asymptotic expansion of a function related to the sine and cosine integrals,"Recall the definitions of the sine and cosine integrals:$$\operatorname{Si}(x)=\int_0^x\frac{\sin t}t dt,\quad\operatorname{si}(x)=-\int_x^\infty\frac{\sin t}t dt=\operatorname{Si}(x)-\frac\pi2,\tag1$$
$$\operatorname{Ci}(x)=-\int_x^\infty\frac{\cos t}t dt.\tag2$$
In this question, we are only interested in positive real values of the argument: $x\in\mathbb R^+$. We prefer using $\operatorname{si}(z)$ rather than $\operatorname{Si}(z)$, because the former decays on infinity in a manner similar to $\operatorname{Ci}(z)$. We will also need the definition of the exponential integral (of a complex argument):
$$\operatorname{Ei}(z)=\int_{-z}^\infty\frac{e^{-t}}t dt.\tag3$$
Note that
$$\Re\operatorname{Ei}(ix)=\operatorname{Ci}(x),\quad\Im\operatorname{Ei}(ix)=\operatorname{si}(x)+\frac\pi2.\tag4$$
We want to factor $\operatorname{si}(z)$ and $\operatorname{Ci}(z)$ into an amplitude $A(x)$ and a phase $\Phi(x)$, both of which are continuous monotonic functions. We follow an approach from this paper , where it is applied to Bessel functions. Let
$$\operatorname{si}(x)=A(x)\cdot\sin\Phi(x),\quad\operatorname{Ci}(x)=A(x)\cdot\cos\Phi(x),\tag5$$
where
$$A(x)=\Big|\!\operatorname{Ei}(ix)-i\pi\Big|=\sqrt{\operatorname{si}^2(x)+\operatorname{Ci}^2(x)},\tag6$$
$$\Phi(x)=\arg\left((\operatorname{Ei}(ix)-i\pi)\,e^{-ix}\right) + x=\arctan\frac{\operatorname{si}(x)}{\operatorname{Ci}(x)}\color{gray}{+2\pi n}.\tag7$$
On the right of $(7)$, $n$ is an integer depending on $x$. This last term is present to compensate for jump discontinuitues introduced by $\arctan$ and to make the phase $\Phi(x)$ continuous monotone function (the former representation using $\arg$ is already continuous as it stands). It is convenient to consider the derivative of the phase $\Phi'(x)$ rather than the phase itself, because this way we do not need to care about jump discontinuities, the closed form looks more manageable, and coefficients discussed below will assume simpler integer form:
$$\Phi'(x)=\frac{\operatorname{Ci}(x)\cdot\sin x-\operatorname{si}(x)\cdot\cos x}{\left(\operatorname{si}^2(x)+\operatorname{Ci}^2(x)\right)\cdot x}.\tag8$$ I'm interested in the asymptotic expansion of $\Phi'(x)$ for $x\to\infty$. Numerical evidence based on Wynn's epsilon method strongly suggests the following expansion:
$$\Phi'(x)\sim1+\frac1{x^2}-\frac{13}{x^4}+\frac{461}{x^6}-\frac{29093}{x^8}+\frac{2829325}{x^{10}}-\frac{392743957}{x^{12}}+O\left(\frac1{x^{14}}\right).\tag9$$
In fact, I have obtained conjectured coefficients up to a much higher order. You can find them here . There are some additional evidence that these coefficients are not just random artifacts of round-off errors, despite being computed by approximate numeric methods. They are stable across different numeric algorithms, appear to be exact integers to a very high precision, and follow certain patterns: alternating signs, periodic modulo some integers, etc. But so far these all are just empirical results. I am looking for a proof that the asymptotic expansion $(9)$ really holds, and my conjectured values of its coefficients are indeed correct. Also, I am looking for some general formula or recurrence relation for these coefficients. Related questions: (1) , (2) .","['asymptotics', 'calculus', 'special-functions', 'sequences-and-series', 'analysis']"
1780031,Three circles in a rectangular box. What is the largests radius?,"I keep three circular medallions in a rectangular box in which they
  just fit with each one touching the other two. The smallest one has
  radius $4 \, cm$ and touches one side of the box, the middle sized one has
  radius $9 \, cm$ and touches two sides of the box and the largest one
  touches three sides of the box. What is the radius of the largest one? In this case circle $A$ has radius $4 \, cm$ and circle $B$ has radius $9 \,cm$. I have tried to look at the problem from different points of view as well as by investigating for a pattern, however the answer is not visibly yielded this way. I do not know what implications the circles touching each other has on the answer, but this must be a key observation to understand. Otherwise, I have tried to link the circle centres together forming a triangle, however this seems to lead nowhere. However I may be wrong. Thank you for the help.",['geometry']
1780039,Integral involving logarithm: $\int_0^\infty \frac{ \ln x}{(x+a)(x+b)} dx$,"How to solve the following integral
$$\int_{0}^{\infty} \frac{ \ln x}{(x+a)(x+b)} dx,$$
where $a,b>0$ and $a \neq b$. I was looking for some kind of substitution. However, I don't see an obvious one. Thanks!","['logarithms', 'improper-integrals', 'integration', 'calculus']"
1780067,Taking the derivative of a set function,"Lets say $f(A)$ is a set function $A \to \mathbb{R}$ for a collection of sets $\mathcal{A}$. Is is possible to differentiate such a function, and if so, what theory is used to give this notion a rigorous foundation? Or maybe, is it only possible if $\mathcal{A}$ is a $\sigma-$field with measure $|\mu(A)|<\infty\;\forall A\in \mathcal{A}$. Here are my attempts and thoughts about this Let $\mathcal{A}$ be the collection of sets upon which $f(A)$ is defined. To make this easier, assume that $\mathcal{A}$ is a $\sigma$-Field. If I define a decreasing sequence of sets $A_i \in \mathcal{A}: A_1 \supset A_2 \supset A_3 ... \supset A$ so that $\lim\limits_{n \to \infty} A_i = A$, then I have something like the univariate $x\to \infty$. However, the problem is that $A \in \mathcal{A}$ need not define a metric space, so I can't just assign an absolute distance to each set in the decreasing sequence, right? If I could, that would essentially just translate this to univariate calculus. I think if I define a measure over the field $\mathcal{A}$ I could also make this easier, but that would be the Radon-Nikodym derivative (unless that is the derivative of a set function). I thought maybe the following could work too. Let's define distance between two sets $A,B$ in the domain of $f(\cdot)$ as: $$d(A,B) := \left|\sup_{x \in A, \;y \in B\cap A^c } |f(x)-f(y)| - \sup_{x \in B, \;y \in A\cap B^c } |f(x)-f(y)|\right|$$ With this, I thought we could define a derivative from this as: $$f'(A) := \lim_{n\to \infty} \frac{f(A_{i+1})-f(A_{i})}{d(A_{i+1},A_{i})}$$","['derivatives', 'real-analysis', 'calculus']"
1780070,Necessary to find an estimator's probability distribution before calculating its expectation?,"Where $X_{1}, X_{2}, \dots X_{n}$ is an iid distribution with pdf given by: \begin{cases}
\frac{1}{\theta}x^{1-\theta} \qquad &\text{If $0 \leq x \leq 1$} \\[5 pt]
0 \qquad &Otherwise
\end{cases} It can be shown using the M.L.E for estimator inference that $\theta$ is given by:
$$\theta = \frac{-1}{n} \sum_{i = 1}^{n} \ln(X_{i})$$ To take the expectation of this estimator, what's the next step? I believe that I need to find the underlying distribution for $\theta$, correct? I ask that question, only because a solution does not find this underlying distribution (or else is not explicit): And what if I find $\theta$ to be $(1 - \bar{X})/\bar{X}$? In that case, how would I find the underlying pmf of $\theta$?","['parameter-estimation', 'statistics', 'statistical-inference']"
1780220,Relation between chain rule and implicit differentiation derivation in multi variable calculus,"So my question is on the derivation of the implicit differentiation (taken from here ). The general chain rule, from here , it says that if we have a function $z$ of $n$ variables, $x_1, x_2,\ldots,x_n$ and each of these variables are in turn a function of $m$ variables, $t_1, t_2,\ldots, t_m$.  Then for any $t_i, i=1, 2, \ldots, m$ we have (1) 
$$
\frac{\partial z}{\partial t_i}=\frac{\partial z}{\partial x_1}\frac{\partial x_1}{\partial t_i}+\frac{\partial z}{\partial x_2}\frac{\partial x_2}{\partial t_i}+\cdots+\frac{\partial z}{\partial x_n}\frac{\partial x_n}{\partial t_i}$$ My question is how does did the differentiation of $F$ with respect to $x$ come up as it did.  How did it come up as (2) $$\frac{\partial F}{\partial x}\frac{\partial x}{\partial x}+\frac{\partial F}{\partial y}\frac{\partial y}{\partial x}+\cdots+\frac{\partial F}{\partial z}\frac{\partial z}{\partial x}=0$$
If I match (2) with (1), then it seems that the left hand side is $\frac{\partial F}{\partial x}$ because $F=z, x=x_1, y=x_2, z=x_3$.  As far as the initial condition of $F$ having three variables $(x, y, z)$ that in turn are each supposed to be functions of more variables, can it be assumed that $x$ and $y$ are constants; this would be mean effectively that $x=g(t_1, t_2, t_3)=x$ of some function g and $y=y(t_1, t_2, t_3)=y$.  So is the left hand side of (2) $\frac{\partial F}{\partial x}$ or some different notation.  What does the differentiation of $F$ with respect to $x$ on the left hand side look like in symbols then?","['multivariable-calculus', 'implicit-differentiation', 'proof-explanation']"
1780222,Determinant of Matrix with uncomputable values.,"Calculate the determinant of the matrix
  $$
\begin{pmatrix}
  10^{10} & 10^{10^{10}} & 11^{11^{11}} & 1 & 0 \\
  2^{2^2} & 3^{3^3} & 7^{7^7} & 0 & 1 \\
  11 & 17 & 12 & 2 & 7 \\
  2 & 3 & 5 & 1 & 1 \\
  9 & 14 & 7 & 1 & 6 \\
\end{pmatrix}
$$ My wonderful Russian Professor put this one up on the board. Obviously it's not something one can put in Wolfram Alpha. I can't see any obvious linear dependencies between rows or columns, I've tried assigning variables to the big values (just for ease of notation) and doing row operations, transposing to get it into upper triangular it all still ends in a mess. Now, I know our Prof HATES computation and is terrible at it, so he wouldn't write this up unless there is some structural simplicity I can't see...?","['linear-algebra', 'determinant']"
1780233,Confusion with Elimination of Indeterminacy,"I am referring to, say, Theorem 3 in Shafarevich Basic Algebraic Geometry 1, Chapter IV, subsection 3.3. or Beauville Complex Algebraic Surfaces Theorem II.7. The statement is: Let $\phi: S \dashrightarrow X$ be a rational map from a surface to a projective variety. Then there exists a surface $S'$, a morphism $\eta:S' \to S$ which is a composite of a finite number of blow-ups, and a morphism $f:S' \to X$ such that $f=\phi \circ \eta$. Q1. I don't know where my understanding goes wrong: I use definition that regular map means defined everywhere. So say $p\in S$ is where the map $\phi$ is undefined and say $\eta$ is just one blow-up of $p \in S$. Let $E$ be the exceptional divisor. How can $f:S' \to X$ be regular if $f(E)=(\phi \circ \eta)(E)=\phi(p)=$undefined? In particular, I would like to understand an explicit example as in Shafarevich, bottom of page 255: Q2. Let $\phi: \mathbb{A}^2 \dashrightarrow \mathbb{P}^1$ by given by $\phi(z_0,z_1)=[z_0:z_1]$, which is not regular at $P=(0,0)$. Then it says that on the blow-up a particular chart is defined by equations $z_0=u$ and $z_1=uv$ and so (?) $f(z_0,z_1)=[1:v]$. Following Mohan's advice, I am still lost. For example, let 
\begin{equation}
Bl_0\mathbb{A}^2= \{ (z_0,z_1),[w_0:w_1] \vert z_0w_1=z_1w_0 \} \text { and }[z_0:z_1]=[w_0:w_1] \text{ when } z_0, z_1 \text{are not both zero}
\end{equation}
We have Which works great away from the origin, but I still don't understand how the composition of maps works on the origin of $\mathbb{A}^2$. I've been told that on the affine chart of $Bl_0\mathbb{A}^2$ where $w_1 \neq0$ the map $\phi \circ \eta$ does this: 
\begin{equation}
(z_0,z_1),[w_0:w_1] \mapsto [\frac{w_0}{w_1}z_1:z_2]=[w_0z_1:w_1z_1]=^{??}[w_0:w_1]
\end{equation}
Obviously, I don't understand why can we cancel out $z_1$ if it definitely could be zero.",['algebraic-geometry']
1780251,Solve: $\int\frac{1}{3x}dx$,Solve: $$\int\frac{1}{3x}dx$$ The answer in the back of the book is: $$\frac{1}{3}\log_e|x|+c$$ I get this result when I remove the constant $\frac{1}{3}$ from the integral at the start and substitute u for x at the beginning. Also by differentiating the result I can return to the initial integral. But if at the beginning I do not remove the constant $\frac{1}{3}$from the integral and substitute $u$ for $3x$ I get a different result. $$\frac{1}{3} \log_e |3x| +c$$ Differentiating this does not bring me back to the initial integral. What is wrong with substituting $u$ for $3x$ here?,"['derivatives', 'integration', 'calculus']"
1780253,"Probability that for two random points in unit ball, one is closer to the center than to the other point","If I have two points $p_1, p_2$ uniformly randomly selected in the unit ball, how can I calculate the probability that one of them is closer to the center of the ball than the distance between the two points? I know how to calculate the distribution of the distance between two random points in the ball, same for one point from the center, but I'm not sure how to use the two distributions to get what I'm after.","['probability', 'geometric-probability', 'geometry']"
1780265,Determining the number of balls in a box,"I'm facing problems solving this question and I'd like some help: A box contains n balls, where just 2 are white and the rest are red. A random sample of 4 balls is drawn without replacement. It's known that the probability of the 2 white balls are in the sample is 6 times higher than the probability that no white balls are in the sample. Calculate n . I did like that: $$6* (\frac{2}{n} *\frac{1}{n-1} * \frac{n-2}{n-2} * \frac{n-3}{n-3}) = (\frac{n-2}{n} *\frac{n-3}{n-1} * \frac{n-4}{n-2} * \frac{n-5}{n-3}) => n = 8 $$ But, according to the answer of this question, n = 6. I trying to find my mistake. Can someone help me?",['probability']
1780276,Book about intuition behind Lebesgue measure,"I recently completed a course in Real analysis covering Lebesgue and Borel measure, Fourier series, $L^p$ spaces and such. I can solve problems in these topics but am afraid that I do not truly understand these concepts. I have read about intuition of bits and pieces here on MSE. But are there any books on intuition behind such concepts (or atleast measure theory). The flavor I am looking for is similar to the one found in ""Visual Complex Analysis"" by Needham. It does not have to be rigorous as long as it includes intuition/physical interpretation of the idea.","['big-list', 'reference-request', 'lebesgue-measure', 'book-recommendation', 'measure-theory']"
1780280,"Given bijection between $\mathbb{N}$ and $A$ and $B$, find bijection from $\mathbb{N}$ to $A \cup B$","Let $A$ and $B$ be two countable sets and consider that $f$ is a bijection from $\mathbb{N}$ to $A$ and $g$ is a bijection from $\mathbb{N}$ to $B$. I have to find a bijection from $\mathbb{N}$ to $A \cup B$ involving both $f$ and $g$. I thought of split  $\mathbb{N}$ in even and odd numbers using $f(\frac{n}{2})$ if $n$ is even and $g(\frac{n-1}{2})$ if $n$ is odd, but it doesn't work because I can just prove the onto property, I can't guarantee it will be one-to-one because $A \cap B$ may be non empty (for instance, it might happen that $f(2)$ would be equal $g(5)$...) In general, all the strategies I've tried to construct such bijection have failed because $A \cap B$ may be non empty. How can I work around this?","['elementary-set-theory', 'functions']"
1780331,Integrate $\int_{0}^{2\pi}\log|1-e^{i\theta}| d\theta$,"Here is exercise 11, chapter 3 from Stein & Shakarchi's Complex Analysis II: Show that if $|a|<1$, then: $$\int_{0}^{2\pi}\log|1-ae^{i\theta}|\,d\theta = 0$$ Then, prove that the above result remains true if we assume only that $|a|\leq 1.$ ($a\in \mathbb{C}$) I've already proved it for $|a|<1$, but couldn't make it for $|a|=1$, which I thought would be the easy part. But the only thing I could come up with was that, WLOG, it is enough to prove that: $$\int_{0}^{2\pi}\log|1-e^{i\theta}|\,d\theta = 0$$ Tried to look for some kind of symmetry, but could't find it. Any ideas? Thanks!","['complex-analysis', 'complex-integration']"
1780336,"How to prove that if $f$ is continuous a.e., then it is measurable.","Definition of simple function $f$ is said to be a simple function if $f$ can be written as
$$f(\mathbf{x}) = \sum_{k=1}^{N} a_{_k} \chi_{_{E_k}}(\mathbf{x})$$
where $\{a_{_k}\}$ are distinct values for $k=1, 2, \cdots, N$ and
$\chi_{_{E_k}}(\mathbf{x})=\cases{1&if $\mathbf{x}\in E_k$\\0&if
$\mathbf{x}\notin E_k$}$. Theorem $(4.12)$ If $\{f_k\}$, is a sequence of measurable functions, then
$\displaystyle\limsup_{k\to\infty}f_k$ and
$\displaystyle\liminf_{k\to\infty}f_k$ are measurable. In particular
if $\displaystyle\lim_{k\to\infty}f_k$ exists a.e., it is
measurable. Theorem $(4.13)$ Every function $f$ can be written as the limit of a sequence
$\{f_k\}$ of simple functions. Problem If $f(x)$, $x\in\mathbb{R}^1$, is continuous at almost every point of an interval $[a, b]$, show that $f$ is measurable on $[a, b]$. Generalize this to functions defined in $\mathbb{R}^n$. [For a constructive proof, use the subintervals of a sequence of partitions to define a sequence of simple measurable functions converging to $f$ a.e. in $[a, b]$. Use $(4.12)$.] By the theorem $(4.13)$, we can choose $\{f_k\}$, which are simple functions defined on $[a, b]$ and approaching to $f$. That is, choose $\{f_k\}$ such that, for $k=1, 2, \cdots$, 
$$f_k=\sum_{i=1}^{N} a_{_i}^{(k)} \chi^{(k)}_{_{E_i}} \quad \text{and} \quad \displaystyle\lim_{k\to\infty}f_k=f$$ Since $f$ is continuous a.e., $f_k$ are measurable for all $k\in\mathbb{N}$. Then, by the theorem $(4.12)$, $f$ is measurable. Q1) Why are $f_1, f_2, \cdots, f_N$ all measurable? Q2) Morever, if $f_k$ are all measurable, then how can I ensure that $\displaystyle \lim_{k\to\infty}f_k$ exists? Is it ensured by theorem $(4.13)$? If there is any advice or other proofs, please let me know them. Thank you.","['simple-functions', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
1780348,Multiplication of unitary matrices to make symmetric off-diagonal elements zero,"Context Starting with a unitary matrix $U$ of size $m \times m$, I have read of a way to obtain a diagonal matrix by sequentially multiplying $U$ from the right by unitary matrices $V$ of a certain form. Each of these matrices is the identity matrix, except that the elements $V_{ab}$, $V_{ba}$, $V_{aa}$, $V_{bb}$ are replaced by elements of a $2\times2$ unitary matrix, chosen such that the target element $U^{\mathrm{new}}_{ab}$ of the output matrix $U^{\mathrm{new}}=U^{\mathrm{old}}V$ becomes zero. For the diagonalization, the sequential multiplications are performed to make all off-diagonal elements of each row zero, starting with the last. When a row has been completed in this way, the off-diagonal elements of the corresponding column have automatically become zero as well. Problem My question is why the off-diagonal elements of the corresponding column become zero if the off-diagonal elements of a row are made zero in this way? Example As a $3\times 3$ example, starting with this unitary matrix
$$U=\left(
        \begin{matrix}
        0.5000+0.0000i & 0.5000+0.5000i & -0.5000+0.0000i \\
        0.0000-0.5774i & 0.0000+0.5774i & 0.5774+0.0000i \\
        0.0000+0.6455i & 0.3873+0.1291i & 0.5164+0.3873i \\
        \end{matrix}\right)
$$ the first steps that lead to a diagonalized last row and column would be achieved with the following matrices
$$V1=\left(
        \begin{matrix}
        1.0000+0.0000i & 0.0000+0.0000i & 0.0000+0.0000i \\
        0.0000+0.0000i & -0.8452+0.0000i & 0.5345+0.0000i \\
        0.0000+0.0000i & 0.5071-0.1690i & 0.8018-0.2673i \\
        \end{matrix}\right)
$$
$$V2=\left(
        \begin{matrix}
        -0.7638+0.0000i & 0.0000+0.0000i & 0.6455+0.0000i \\
        0.0000+0.0000i & 1.0000+0.0000i & 0.0000+0.0000i \\
        0.2041+0.6124i & 0.0000+0.0000i & 0.2415+0.7246i \\
        \end{matrix}\right).
$$ The results of the first two multiplication steps are:
$$U\times V1=\left(
        \begin{matrix}
        0.5000+0.0000i & -0.6761-0.3381i & -0.1336+0.4009i \\
        0.0000-0.5774i & 0.2928-0.5855i & 0.4629+0.1543i \\
        0.0000+0.6455i & 0.0000+0.0000i & 0.7246+0.2415i \\
        \end{matrix}\right)
$$
$$U\times V1\times V2=\left(
        \begin{matrix}
        -0.6547+0.0000i & -0.6761-0.3381i & 0.0000+0.0000i \\
        0.0000+0.7559i & 0.2928-0.5855i & 0.0000+0.0000i \\
        0.0000+0.0000i & 0.0000+0.0000i & 0.0000+1.0000i \\
        \end{matrix}\right),
$$
where as expected the last row and column have zeros as off-diagonal entries. Known property Since $U^{\mathrm{old}}$ and $V$ are both unitary, so is their product $U^{\mathrm{new}}$. However, based on that alone I do not see why the effect on the columns happens. If anyone knows an explanation, it would be a great help!","['matrices', 'linear-algebra', 'complex-numbers']"
1780395,Integral of $\frac{\sqrt {x}}{x^2+x}$,"I'm trying to find $$\int_{1/3}^3 \frac{\sqrt{x}}{x^2+x} dx.$$ I used a $u$ substitution where $u = \sqrt{x}$ to get $$2 \int_\sqrt{1/3}^\sqrt{3} \frac{u}{u^4+u^2} du.$$ Substituting $u = \tan v$, I get
$$2 \int \cot v dv
  = \left. \ln |\sin v|\right|_{\arctan \sqrt{1/3}}^{\arctan \sqrt 3}
  = 2\ln \left( \frac{\sqrt{3}}{2}\right)-2\ln(1/2),
$$ which is wrong. Where did I go wrong?","['integration', 'trigonometry', 'calculus']"
1780437,Eigenvalues of matrices of order $n$,"How to find eigenvalues of following matrices of order $n$? $$A=\begin{bmatrix} 
1 & 1 & 1 & 1 & \cdots & 1 \\
1 & 0 & 1 & 1 & \cdots & 1 \\
1 & 1 & 0 & 1 & \cdots & 1 \\
1 & 1 & 1 & 0 & \cdots & 1 \\
\vdots & \vdots & \vdots & \vdots &\ddots & \vdots \\
1 & 1 & 1 & 1 & \cdots & 0 \\
\end{bmatrix}_n$$ $$B=\begin{bmatrix} 
-1 & 1 & 1 & 1 & \cdots & 1 \\
1 & 0 & 1 & 1 & \cdots & 1 \\
1 & 1 & 0 & 1 & \cdots & 1 \\
1 & 1 & 1 & 0 & \cdots & 1 \\
\vdots & \vdots & \vdots & \vdots &\ddots & \vdots \\
1 & 1 & 1 & 1 & \cdots & 0 \\
\end{bmatrix}_n$$ I had apply matrix calculator to find eigenvalues and I found that exactly $n-2$ eigenvalues of both matrices are $-1$,but I was not able to find rest of the eigenvalues.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
1780449,How to integrate $\int_{0}^{1}\text{log}(\text{sin}(\pi x))\text{d}x$ using complex analysis,"Here is exercise 9, chapter 3 from Stein & Shakarchi's Complex Analysis II: Show that:
$$\int_{0}^{1}\text{log}(\text{sin}(\pi x))\text{d}x=-\text{log(2)}$$ 
[Hint: use a contour through the set $\{ri\,|\,\,r\geq0\} \cup\{r\,\,|\,\, r\in[0, 1]\}\cup\{1+ri\,\,|\,\,r\geq0\}$] That contour doesn't seem to make sense to me for this integral in particular. I really have no clue for this one. Any ideas?","['complex-analysis', 'complex-integration']"
1780458,Null set vs Measure zero set,"In the context of Lebesgue measure on $\mathbb{R}^n$, is null set the same thing as a set of measure zero? I understand that null set implies measure zero, not sure about the other direction. Update: By null set I mean https://en.m.wikipedia.org/wiki/Null_set a set that can be covered by a countable union of intervals(balls)  of arbitrarily small total length. To be precise: Is a set with measure zero coverable by countable union of balls of arbitrarily small total length?","['real-analysis', 'lebesgue-measure']"
1780463,show that number theory inequality $a_{1}+a_{2}+\cdots+a_{k}<\frac{5}{2}n$,"Let $n,a_{1},a_{2},\cdots,a_{k}$ be postive integers and at least  greater than $1$,and such
$$(a_{1})!\cdot(a_{2})!\cdots(a_{k})!|n!$$ show that
$$a_{1}+a_{2}+\cdots+a_{k}<\dfrac{5}{2}n$$ I have know prove $k=2$ see Erdos 1968problem But unfortunately I am looking for much smaller bound   Any idea would be helpful. I think use 
$$a_{1}+a_{2}+\cdots+a_{k}-s_{2}(a_{1})-s_{2}(a_{2})-\cdots-s_{k}(a_{k})<n-s_{2}(n)<n$$
where $s_{p}(n)$ is the sum of the digits of $n$ when
written in base $p$ Add it . the constant $\dfrac{5}{2}$ is best?",['number-theory']
1780468,Five people have applied for three different positions in a store. In how many ways can the positions be filled?,"Five people have applied for three different positions in a store. If each person is qualified for each position, in how many ways can the positions be filled? Can someone tell me if I have to use permutations $\mathrm P(5, 3)$ or combinations $\mathrm C(5, 3)$ and why? Thanks for any help!","['permutations', 'combinatorics', 'combinations']"
1780488,Equivalent characterizations of the dual norm on finite dimensional vector spaces,"In their book on Convex Optimization, Boyd and Vandenberghe state that given a norm, $||\cdot||$, defined on $\mathbb{R}^n$, the dual norm is defined as $$||z||_*= \sup \{ z^Tx : ||x|| \leq 1 \}$$ In other places, I have encountered an equivalent characterization of the dual norm as $$||z||_*= \sup_{x \neq 0} \displaystyle\frac{z^Tx}{||x||}$$
I don't actually see how these two things are equivalent, even though this is said to be a simple one-liner. In particular, what's confusing to me is that I would have argued the following, even though this seems to not be correct:  By norm homogeneity, the set we're taking the supremum over is invariant to dilations.  That is, for any $\alpha >0$, we have $$\displaystyle\frac{z^T(\alpha x)}{||\alpha x||} = \displaystyle\frac{\alpha (z^Tx)}{ |\alpha|  ||x||} = \displaystyle\frac{z^Tx}{||x||}$$ and therefore we may find the supremum of the set by merely considering the $x$ values with some constant norm, for example, the unit norm:  $$||z||_*= \sup_{x \neq 0} \displaystyle\frac{z^Tx}{||x||} = \sup_{x : ||x||=1} \displaystyle\frac{z^Tx}{||x||} = \sup \{ z^T x : ||x|| = 1\}$$. Why is this logic incorrect?","['normed-spaces', 'convex-optimization', 'linear-algebra']"
1780497,Algebraic number fields in which all rational primes are inert,Is there an algebraic number field $F\supsetneq\mathbb{Q}$ such that all rational primes are inert in $\mathcal{O}_F$?,"['algebraic-number-theory', 'number-theory', 'ramification', 'prime-numbers', 'field-theory']"
1780501,What is the expected number of steps in a random walk from leaf to leaf in a full binary tree?,"Let $h \geq 2$ be a natural number. Consider a complete binary tree of height $h$. Say we take a random walk starting from the ""leftmost"" leaf. What is the expected number of steps before the ""rightmost"" leaf is visited? Is there a known closed form for this? If not, are there any lower or upper bounds? As an example, let $h = 3$. Then we are looking at the above tree and asking what the expected number of steps to walk from node 4 to node 7 is. What I've found: Denote $n = 2^h - 1$ to be the number of nodes in the tree. This problem can be viewed as a system of linear equations. Let $x_i$ denote the expected number of steps to walk from node $i$ to node $n$. We are trying to find $x_{(n+1)/2}$ in the system
$$
\begin{cases}
  x_1 = 1 + \frac{1}{2}x_2 + \frac{1}{2}x_3 \\
  x_i = 1 + \frac{1}{3}x_{\lfloor i/2 \rfloor} + \frac{1}{3}x_{2i} + \frac{1}{3}x_{2i+1} & \text{for } 1 < i < \frac{n+1}{2} \\
  x_i = 1 + x_{\lfloor i/2 \rfloor} & \text{for } \frac{n+1}{2} \leq i < n \\
  x_n = 0.
\end{cases}
$$ I wrote a Python program to solve this system. I found the following values:
$$
\begin{array}{c|c}
h & \text{expected number of steps} \\
\hline 2 & 4 \\
\hline 3 & 24 \\
\hline 4 & 84 \\
\hline 5 & 240 \\
\hline 6 & 620 \\
\hline 7 & 1512 \\
\hline 8 & 3556 \\
\hline 9 & 8160 \\
\hline 10 & 18396
\end{array}
$$
I plugged these values into OEIS, but there was no matching sequence. Although I do not know how to solve for an exact value, I can provide a lower bound of $2^{h+1} - 3h - 1$. Note that a walk from the leftmost leaf to the rightmost leaf must pass through the root. Therefore, we can lower bound the desired value by the expected number of steps before the root is visited in a random walk starting from the leftmost leaf. Then we are just asking how many steps it takes to randomly walk from a node at level $h$ to the root node at level $1$. At level $h$, you always step to level $h-1$. At level $i$ where $1 < i < h$, you have a $\frac{2}{3}$ chance of stepping to a child at level $i+1$ and a $\frac{1}{3}$ chance of stepping to the parent at level $i-1$. Let $x_i$ denote the expected number of steps to randomly walk from level $h-i+1$ to level 1. We are then trying to find $x_1$ in the system of equations
$$
\begin{cases}
x_h = 0\\
x_i = 1 + \frac{2}{3}x_{i-1} + \frac{1}{3}x_{i+1} & \text{for } 1 < i < h\\
x_1 = 1 + x_2.
\end{cases}
$$
I claim that $x_{i-1} = x_i + 2^i - 3$ for $1 < i \leq n$. Proceed by induction. As the case case, we have $x_1 = x_2 + 1 = x_2 + 2^2 - 3$. For the induction hypothesis, suppose that $x_{k-1} = x_k + 2^k - 3$. Then for our induction step, we have
$$
\begin{align*}
  x_k &= 1 + \frac{2}{3}x_{k-1} + \frac{1}{3}x_{k+1} \\
      &= 1 + \frac{2}{3}\left(x_k + 2^k - 3\right) + \frac{1}{3}x_{k+1}
\end{align*}
$$
which rearranges to $x_k = x_{k+1} + 2^{k+1} - 3$, proving the claim. Then we have
$$
x_1 = \sum_{i=2}^h \left( 2^i - 3 \right) = 2^{h+1} - 3h - 1.
$$ Hence the expected number of steps to reach the root from a leaf is $2^{h+1} - 3h - 1$. This gives our lower bound. I do not know where to go from here. Looking at the numbers, I suspect one might be able to prove a $\Omega(h2^h)$ lower bound.","['random-walk', 'markov-chains', 'probability']"
1780506,Positive maps and $*$-homomorphisms,"If $\varphi:A \to B$ is a linear map between $C^*$-algebras, it is said to be positive if it sends positive elements in $A$ to positive elements in $B$. We know that every $*$-homomorphism is positive. But how about the converse? Does every positive map between $C^*$-algebras necessarily a $*$-homomorphism? Thanks in advance!","['functional-analysis', 'c-star-algebras']"
1780510,Can we use Rouche's theorem for $z^5 + 3z^4 + 6$ on $|z| = 3$?,"This is from an exam. How many zeroes of $f(z) = z^5 + 3z^4 + 6$ are within $|z| = 3$? The test is for a class based on Complex Variables and their applications, so of course Rouche's theorem is what I went for. However, it seemed very difficult to get this function into a form that we could work with Rouche's theorem. After thinking about the question afterwards, here's the best I could do to muster up an answer: Use Rouche's theorem on $|z| = 2$. We get $4$ zeroes within this contour. Furthermore, $f(z)$ has a real root since it's of odd degree. Hence, this root must be of the form $-r$ with $r > 0$. At $r = 3$, we have $f(-3) = 6$ and the function is decreasing as $r$ increases, hence this zero is larger than three. So there are $\boxed{4}$ zeroes in total. Is there an easier, more direct way to get this result? I suspect there is since the other version of the exam had $f(z) = z^4 + 3z^3 + 6$ which is also hard to use Rouche's theorem on and cannot be solved in this manner.",['complex-analysis']
1780516,What is the limit of $\lim_{n\to \infty}\frac{1-2+3-4+\cdots+(-2n)}{\sqrt{n^2+1}}$?,"Find the $$\lim_{n\to \infty}\frac{1-2+3-4+\cdots+(-2n)}{\sqrt{n^2+1}}.$$ I thought to apply squeeze theorem so 
$$\frac{1-2+3-4+\cdots+(-2n)}{\sqrt{n^2+1}}\leq \frac{1+2+3+4+\cdots+(2n)}{\sqrt{n^2+1}}=\frac{n(2n+1)}{\sqrt{n^2+1}}$$ But I am unable to find a lower bound though. How can I do this?","['sequences-and-series', 'limits']"
1780531,What does it mean to integrate a vector function?,What is the meaning of a vector function and what is the geometric interpretation of integrating such a function?,"['multivariable-calculus', 'calculus', 'vector-analysis']"
1780533,Recursive relation between conditional expectations,"Let $\epsilon _n$ and $\eta _n$ iid random variables (and the sequences are independent of each other) such that $\epsilon _n \sim \mathcal{N}(0, \sigma ^2)$ and $\eta _n \sim \mathcal{N}(0, \delta ^2)$. Let $X_0=0$, $X_{n+1}=a_nX_n+\epsilon_{n+1}$ and $Y_n=cX_n+\eta_n$ where $c$ and $a_n$ are positive constants. Put $\widehat{X_{n/n}}=E(X_n|Y_0,\ldots ,Y_n$) and $\widehat{X_{n/n-1}}=E(X_n|Y_0,\ldots ,Y_{n-1}$). I want to show that 
$$\widehat{X_{n/n}}=\widehat{X_{n/n-1}}+\frac{E(X_nZ_n)}{E(Z_n^2)}Z_n $$ where $Z_n=Y_n-c\widehat{X_{n/n-1}}$, but my attempts didn't suceed.","['probability-theory', 'conditional-expectation', 'probability']"
1780580,What is wrong in this proof: That $\mathbb{R}$ has measure zero,"Consider $\mathbb{Q}$ which is countable, we may enumerate $\mathbb{Q}=\{q_1, q_2, \dots\}$. For each rational number $q_k$, cover it by an open interval $I_k$ centered at $q_k$ with radius $\epsilon/2^k$. The total length of the intervals is a geometric progression that sums up to $\epsilon$. Each real number is arbitrarily close to a rational number since $\mathbb{Q}$ is dense in $\mathbb{R}$. Thus, each real number is in one of the open intervals. Thus the entire real line is covered by the union of the $I_k$, thus $\mathbb{R}$ is a null set with measure zero. Clearly there is something wrong in the above proof, however I am not sure where is it? Thanks for any help.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1780630,can a convex polygon have only one boundary point at locally maximum distance from its centroid?,"It's easy to see that given any convex polygon P and any point c in its interior, there is at least one point m on the boundary of P at locally maximum distance from c: simply choose m to be a vertex at maximum distance from c. The following picture shows an example of P and c such that P has only one boundary point m at locally maximum distance from c. Which of the following statements are true?  I'm looking for a simple proof or counterexample for each. (S 02 ) Every convex polygon P has at least two boundary points at locally maximum distance from the centroid c 0 of its vertices. (S 12 ) Every convex polygon P has at least two boundary points at locally maximum distance from the centroid c 1 of its boundary. (S 22 ) Every convex polygon P has at least two boundary points at locally maximum distance from the centroid c 2 of its area. These are all cases of the more general parametrized statement, for $0 \leq$ k $\leq$ n: (S kn ) Every convex n-dimensional polytope P has at least two boundary points at locally maximum distance from the centroid c k of its k-skeleton.","['polygons', 'polyhedra', 'geometry', 'centroid', 'polytopes']"
1780648,Is it almost impossible for the values of continuous random variables to lie on a plane?,"Let $X: \Omega \to \mathbb{R}^d$ be a random variable with density $f$ (the pushforward measure on $\mathbb{R}^d$ is absolutely continuous). Let $x_1, \dots, x_{d}$ be the set of values of $d$ i.i.d. copies of $X$. Do we have that $$\mathbb{P} \left( x_1, \dots, x_{d} \text{ are coplanar} \right)=0,$$
where ""plane"" is read as $a+U$, with $a \in \mathbb{R}^d$ and $U$ a $k \leq d-1$ dimensional subspace of $\mathbb{R}^d$? Moreover, could we more generally say that
$$\mathbb{P} \left( x_1, \dots, x_{d} \in M\right)=0,$$
where $M$ is any smooth manifold uniquely defined by $d-1$ points (a lower dimensional sphere, say). This seems super obvious, but I'm not sure how to prove it. I would like to write something like ""Consider $x_1, \dots,x_{d-1} \in \mathbb{R}^d$. We would now need $x_d \in M$, with $M$ specified by $x_1, \dots, x_{d-1}$. Since $M$ is of measure zero (Hausdorff dimension $< d$) and $X$ is absolutely continuous $\mathbb{P}(x_d \in M ) = 0$"". Now, for me, the problem with the above thing is that we are implicitly conditioning on a zero probability event ($M$ being a specific manifold). You can't simply partition $\Omega$ to $A_M$ by events $A_M = ""\text{the manifold specified by the first observations is } M$ and write
$$\mathbb{P} \left( x_1, \dots, x_{d} \in M \right) = \sum \limits_{M^* \in \mathcal{M}} \mathbb{P} \left( x_1, \dots, x_{d} \in M \middle| M= M^* \right) = 0.$$
I'm pretty sure there is a well known way to deal with these zero-probability events rigorously or avoid dealing with them directly alltogether.","['probability-theory', 'probability']"
1780677,Special Gamma function integral,"I'm trying to evaluate this integral $$\int_{0}^{1} \sin (\pi x)\ln (\Gamma (x)) dx$$
and I got to the point, when I need to find 
$\displaystyle \int_{0}^{\pi } \sin (x)\ln (\sin (x)) dx$
but everything I tried just failed,or either I was not able to put in the borders . Could you please help me.
Thanks","['special-functions', 'integration', 'definite-integrals', 'gamma-function']"
1780688,Unusual integral notation,"When I was learning analysis, I often wondered why I couldn't seem to find anything like $$\iint f(x) (dx)^2$$ in a standard calculus text, and concluded that it should be meaningless – even though, since we can differentiate functions multiple times, it would make sense that we can also integrate them repeatedly. But then I stumbled upon this blog entry by the creator of Mathematica , showing that Leibniz had similar notation in mind when he was developing the calculus, and found out about the differintegral operator, using which the above expression looks like $D^{-2}[f(x)]$. My question is, why don't we see this notation that often in basic analysis courses? What is the graphical meaning of such an expression – i.e. how would its behavior affect the shape of $f(x)$? And how would one solve it? Is there even a definite analog of it, and if so what is its geometrical meaning?","['integration', 'notation', 'calculus']"
1780705,How can I show $1-\frac{1}{x}+x^{1-\frac{1}{x}}<x$ for real $x>1$?,"Denote $$f(x):=1-\frac{1}{x}+x^{1-\frac{1}{x}}$$ How can I prove that $f(x)<x$ holds for every real $x>1$ ? Wolfram gives the taylor series $$f(x)=1+(x-1)-\frac{1}{2}(x-1)^3+\frac{4}{3}(x-1)^4-\frac{31}{12}(x-1)^5+O((x-1)^6)$$ But I would like to have a proof without the taylor series because it is difficult to estimate the remainder theorem by hand. The Lambert-W function or logarithming $x^{1-\frac{1}{x}}$ might help, but I did not succeed with either of these methods.","['real-analysis', 'inequality', 'functions']"
1780707,"Can $\cos (2\pi/7)$ be written as $p+\sqrt{q}+\sqrt[3]{r}, p,q,r\in \mathbb{Q}$?","Is it possible to find $p,q,r \in \mathbb{Q}$ such that $$\cos \frac{2\pi}{7}=p+\sqrt{q}+\sqrt[3]{r}.$$ Assume we can find such $p,q,r$, then $\mathbb{Q}(\cos \frac{2\pi}{7}) \subseteq \mathbb{Q}(\sqrt{q},\sqrt[3]{r})$. I can show that $[\mathbb{Q}(\sqrt{q},\sqrt[3]{r}): \mathbb{Q}]=6$.(WLOG assuming $q$ and $r$ are square free and cube free resp.) I also know that $\mathbb{Q}(\cos \frac{2\pi}{7})/\mathbb{Q}$ is a cyclic extension (because $2\cos \frac{2\pi}{7}=\zeta_7+\zeta_7^{-1}$ where $\zeta_7$ is the primitive 7th root of unity and $\mathbb{Q}(\zeta_7)/\mathbb{Q}$ is a cyclic extension). But then I don't know how I can proceed, please help.","['abstract-algebra', 'galois-theory', 'field-theory']"
1780727,Find the number of solutions of $\sin x+2 \sin 2x- \sin 3x=3$,"In $(0 \:\:\pi)$Find the number of solutions of $$\sin x+2 \sin 2x- \sin 3x=3$$ The equation can be written as $$\sin x+4 \sin x \cos x=3+\sin 3x$$ i.e. $$\sin x(1+4\cos x)=3+\sin 3x$$ i.e., $$\sin x(1+4\cos x)=3+\sin x(3-4\sin^2 x)=3+\sin x(4\cos ^2x-1)$$  so $$\sin x(4\cos^2 x-4\cos x-2)=-3$$ Any hint from here?","['trigonometry', 'trigonometric-series', 'inverse-function']"
1780734,Sequence of funtions $f_n = n(f(x+ \frac{1}{n})-f(x))$ for the continous differentiable function $f$ on $\mathbb R$,"Let  $f$ be a continous differentiable function on $\mathbb R$. Let $f_n$ be a sequence of functions $f_n = n(f(x+ \frac{1}{n})-f(x))$. Then (a) $f_n$ converges uniformly on $\mathbb R$ (b) $f_n$ converges on $\mathbb R$, but not necessarily uniformly. (c) $f_n$ converges to the derivative of $f$ uniformly on$[0,1]$ (d) there is no guarantee that $f_n$ converges on any open interval. We know that $f'(x) =\lim_{h \to 0} \frac{f(x+h) - f(x)}{h} = \lim_{n \to\infty} \frac{f(x+\frac{1}{n}) - f(x)}{\frac{1}{n}}=\lim_{n \to \infty} f_n(x)$. Thus $f_n$ converges pointwise to $f'$ Please tell me how to check the uniform convergence. Any help would be appreciated. Thank you.","['derivatives', 'real-analysis', 'sequences-and-series', 'uniform-convergence']"
1780739,"prove that a non constant periodic, continuous function has a ""smallest period""","Let $\ f:\mathbb{R}\to\mathbb{R} \ $ be a non constant, continuous and periodic function. Prove that $f$ has smallest/minimum period. The definition of period that I work with is: $p$ is a period of a function $f$ if $\ p\gt0 \ \land \ \forall x\in\mathbb{R}:f(x+p)=f(x) $.","['continuity', 'uniform-continuity', 'calculus']"
1780777,How do I calculate this limit when two terms tend to infinity at similar rates,"In a particular problem that I am currently trying to solve, I have the following expression (this is not the entire expression, I have included only the terms involving $a_1$ and $b_1$), $\lim_{(a_1,b_1)\to (\infty,\infty)} \frac{(a_1+a_2+p)!}{(a_1 + p)!}\frac{(b_1+q)^{a_1 +p}}{(b_1+b_2+q)^{a_1+a_2+p}}$ [equation 1] $a_1$ and $b_1$ tend to infinity such that $a_1/b_1 \rightarrow c_0$, i.e., the ratio of $a_1$ and $b_1$ tends to a fixed value. (It is known that $a_2$, $p$ and $q$ do not tend to infinity) This is what I did so far. I used Stirling's approximation for the factorials, the limit reduces to, $\lim_{(a_1,b_1)\to (\infty,\infty)} \left[\sqrt{\frac{a_1+a_2+p}{a_1 +p}}\left(\frac{a_1+a_2+p}{b_1+b_2+q}\right)^{a_1+a_2+p} \left(\frac{b_1+q}{a_1+p}\right)^{a_1+p} e^{-a_2}\right]$ I then, made these approximations, $(a_1+a_2+p) \rightarrow a_1$, $(b_1+b_2+q) \rightarrow b_1$, $(a_1+p) \rightarrow a_1$ and $(b_1+q) \rightarrow b_1$. With these approximations, the limit is calculated as, $\lim_{(a_1,b_1)\to (\infty,\infty)} \left(\frac{a_1}{b_1}\right)^{a_2}e^{-a_2} = c_0^{a_2}e^{-a_2}$ [equation 2] I want to know if this is a correct way to approximating the limit. I doubt my solution because the simulation results in values that can be obtained if I approximate [equation 1] as $c_0^{a_2}e^{-c_0}$ . This differs from [equation 2] in only one term but creates a huge difference. Is there any problem in my the way I approximate  the limit? How do I solve this? Any help is appreciated. Thanks in advance","['infinity', 'approximation', 'limits']"
1780790,Relative entropy (KL divergence) of sum of random variables,"Suppose we have two independent random variables, $X$ and $Y$, with different probability distributions.
What is the relative entropy between pdf of $X$ and $X+Y$, i.e.
$$D(P_X||P_{X+Y})$$
assume all support conditions are met.
I know in general pdf of $X+Y$ is convolution of pdf of $X$ and $Y$, but is there 
an easier way to calculate the relative entropy or at least simplify it?","['information-theory', 'probability-theory', 'probability']"
1780800,"Find functions $f:\mathbb{R}\to[0,1]$ such that: $\lim_{x \to +\infty} \frac{f(x)^2}{f(2 x)}=1$","Find functions $f:\mathbb{R}\to[0,1]$ that satisfy: $$\lim_{x \to +\infty} \frac{f(x)^2}{f(2 x)}=1,$$
$$f'(x)\leq 0 \, \forall x,$$
and
$$\lim_{x \to +\infty} f(x)=0.$$
$$\lim_{x \to -\infty} f(x)=1.$$
I tried to find one without success.","['functional-equations', 'calculus', 'limits']"
1780812,Is there a mathematical notation of indexing a matrix?,"Do matrices in linear algebra support an operation of indexing them analogous to array indexing? For example: $$
A =
\left [\begin{array}{cc}
    1 & 2 \\
    3 & 4
\end{array}\right]
$$ In C, a fixed 2 by 2 array in 32 bit integer space could be described as: int32_t A[][2] = {
    {1, 2}, 
    {3, 4}
}; and allows operations like: A[0][0] = 2; Is there a universally accepted equivalent to such an operation in linear algebra? I understand that a Matrix is not a C Array equivalent, but I am curious whether such an operation is supported.","['notation', 'linear-algebra']"
