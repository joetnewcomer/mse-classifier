question_id,title,body,tags
3627818,Proof request: a collection of sliced squares of size 1 to n can always form a nontrivial rectangle,"I'm an active member and challenge writer on Code Golf SE . Here is a challenge of mine, titled Make a rectangle from a collection of (sliced) squares : Task There is a famous formula on the sum of first $n$ squares: $$ 1^2 + 2^2 + \dots + n^2 = \frac{n(n+1)(2n+1)}{6} $$ It is known that this number is composite for any $n \ge 3$ . Now, imagine a collection of row tiles (a tile of shape $1 \times k$ with the number $k$ written on each cell), and you have 1 copy of
  size-1 tile, 2 copies of size-2 tiles, ... and $n$ copies of
  size- $n$ tiles. [1]  [2 2]  [2 2]  [3 3 3]  [3 3 3]  [3 3 3] ... Then arrange them into a rectangle whose width and height are both $\ge 2$ . You can place each tile horizontally or vertically. +-----+---+-+-+
|3 3 3|2 2|1|2|
+-----+---+-+ |
|3 3 3|3 3 3|2|
+-----+-----+-+ Output such a matrix if it exists. [...] I added a conjecture on the task: I believe there exists a solution for any $n \ge 3$ . Then an answerer came up with a constructive proof that it is possible to form a nontrivial rectangle using only horizontal tiles : It's an incremental construction. Consider $n \bmod 6$ , we can have
  these values for height and width of rectangles: $n/6\times (n+1)(2n+1)~(n\bmod 6=0)$ $(2n+1)/3\times n(n+1)/2~(n\bmod 6=1)$ $(n+1)/3\times n(2n+1)/2~(n\bmod 6=2)$ $n/3\times (n+1)(2n+1)/2~(n\bmod 6=3)$ $(2n+1)/3\times (n+1)n/2~(n\bmod 6=4)$ $(n+1)/6\times n(2n+1)~(n\bmod 6=5)$ (dimensions might be $1$ for $n\leq 6$ so these small cases are
  handled manually) So the main idea of my construction is: We construct rectangles with the heights and widths as in above list. If $n\bmod 3 \neq 1$ , construct the solution for $n-6$ recursively, add in $n-5,n-4\cdots n$ . The height of the rectangle
  will only increase 1 or 2. If $n\bmod 3=1$ , construct the solution for $n-3$ recursively and add in $n-2,n-1,n$ . The height of the rectangle will only
  increase 2. We first carefully assign the new numbers to the added columns, and then put rest of the numbers into one or two added rows. The rest of the job is some careful casework to pick the numbers. [...] While this proof solves the question at hand, it isn't as elegant as we expected. So here is the question: Can we find a more mathematical (and less case analysis based) proof that such a solution exists for any $n \ge 3$ ? It doesn't need to be restricted to horizontal tiles. Edit: As the question isn't getting any sign of progress, I'm also open to any proof (not necessarily elegant) that is distinct from the one above.","['alternative-proof', 'recreational-mathematics', 'combinatorics', 'discrete-mathematics']"
3627837,Proof of $k$th derivatives always being integers,Consider the function $f(x)$ defined such that $$f(x)=\frac{x^n(1-x)^n}{n!}$$ Then prove that the $k$ th derivatives $f^{(k)}(0)$ and $f^{(k)}(1)$ are always integers. Here $n$ and $k$ are integers and $n\ge1$ and $k\ge 0$ I used binomial theorem to prove that the result holds true for $x=0$ for all $k\le n$ which was trivial. But I'm not able to prove it for the general case as such. I also tried using induction..but that didn't work as well. Any research I do to find the answer online leads me to the proof of the irrationality of $\pi$ or $e^n$ where the above statement is taken as a starting point without the proof. Thanks for any answers!!,"['number-theory', 'calculus', 'derivatives', 'elementary-number-theory']"
3627847,"$f$ is integrable on $[a, b]$ and $F(x) = \int_a^x f(t) \, dt$. If $F$ is differentiable at $x_0$ is it always true that $F'(x_0) = f(x_0)$?","Problem. Always true or sometimes false: If $f$ is Riemann integrable on $[a, b]$ (not necessarily continuous) and $F(x) \int_a^x f(t) \, dt$ is differentiable at $x_0 ∈ [a, b]$ then $F'(x_0) = f(x_0)$ ? Full disclosure: this question appeared on an open book exam for my analysis class. The exam is now over – I can no longer submit answers – so this question is purely for my interest. Also please note that this function $f$ is not necessarily continuous everywhere on $[a, b]$ , so it does not satisfy all the conditions of the fundamental theorem of calculus. Please find my work on the problem below: Obviously if $f$ is everywhere continuous on $[a, b]$ then the statement holds, so we can suppose $f$ is not continuous everywhere on $[a, b]$ . I know that a function is Riemann integrable on $[a, b]$ if and only if it is continuous almost everywhere on $[a, b]$ . That is, the set of points where it is not continuous is a set of measure zero. So the set $U$ of points where $f$ is not continuous is a set of measure zero. 
Also I have the following result from class, which is stronger than the fundamental theorem of calculus. Lemma. Let $f$ be integrable on $[a, b]$ and let $c ∈ [a, b]$ . Suppose $f$ is continuous at $x_0 ∈ [a, b]$ . Let $F(x) = \int_c^x f(t) \, dt$ . Then $$F'(x_0) = f(x_0).$$ So the statement given in the title certainly holds at every point where $f$ is continuous. That is, $F'(x_0) = f(x_0)$ at every point in $x_0 \in U$ . Now, the question that remains as far as I can see, since the statement we are considering includes the assumption that $F$ is differentiable at $x_0$ , is whether $F$ can be differentiable at $x_0$ while $f$ is not continuous at $x_0$ . So we really just need to consider the case where $f$ is not continuous at $x_0$ . This is where I am stuck. I tried to proceed by classifying the possible discontinuities at $x_0$ . The fact that $f$ is integrable means $f$ is bounded, so it definitely does not have an essential discontinuity at $x_0$ . But a priori it may have a jump discontinuity or a removable discontinuity at $x_0$ . I think that if $f$ has a jump discontinuity at $x_0$ then $F$ will not be differentiable at $x_0$ , although I can’t prove it. As for a removable discontinuity, I think the effect of this would be that $F'(x_0) \neq f(x_0)$ , although I also cannot prove it. I also tried the following to prove the statement to be true: The fact that the set $U$ of points where $f$ is discontinuous is of measure zero also means that $U$ is dense in $[a, b]$ . So every subinterval of $[a, b]$ contains points in $U$ . This means we can choose a sequence $x_n \to x_0$ with $x_n \neq x_0$ and $x_n ∈ U$ for all $n$ . So then since $x_n ∈ U$ it follows by the lemma that $F'(x_n) = f(x_n)$ for all $n$ . Thus, $$\lim_{n \to \infty} F'(x_n) = \lim_{n \to \infty} f(x_n).$$ But this gets us nowhere since we don't know if $f$ or $F'$ is continuous at $x_0$ That's all the information I have on the problem. Thank you for any assistance.","['integration', 'calculus', 'riemann-integration', 'real-analysis']"
3627862,$Q=(P+\frac12I)$ is invertible where $P$ is a square matrix with integral entries,"Let $P $ be $n×n$ matrix with integral entries and $Q=P+\frac12I$ where I denote the $n×n$ identity matrix . Then $Q$ is invertible. My attempt to prove: $Q$ is invertible iff $0$ is not an eigen value. If possible, let $0$ be eigen value. Then $\exists$ $v\in R^n$ s.t $Qv=0$ Then $Pv=-\frac12v$ . So $-\frac12$ is an eigen value of $P$ with eigen vector $v$ . So $(2x+1)$ is a divisor of the characteristics polynomail $p(x)$ (say) of $P$ which is of course a monic polynomial with integer coefficients. So $\exists g(x)$ s.t $(2x+1)g(x)=p(x)$ . I am not sure how to proceed from here because $g(x)$ may not itself be monic but the product is monic. . This may be  elementary question but please guide me if I am wrong or give a better proof.A lot of thanks for your time!!","['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
3627917,"Where is my mistake in finding this lim ? $\lim\limits_{(x,y)\to (0,0)}\frac{|y|}{x^{2}}~e ^{-\frac{|y|}{x^{2}}}$","Im going to find the mistake in this two solutions : Question $\to $ find : $$\Omega =\lim\limits_{(x,y)\to (0,0)}\frac{|y|}{x^{2}}~e ^{-\frac{|y|}{x^{2}}}$$ The first suggested solution Let prove does not exist lim : $•\color{red}{y=x^{2}}$ then : $$\Omega =\lim\limits_{(x,x^{2})\to (0,0)}\frac{|x^{2}|}{x^{2}}~e ^{-\frac{|x^{2}|}{x^{2}}}$$ $$=\lim\limits_{(x,x^{2})\to (0,0)}e ^{-\frac{x^{2}}{x^{2}}}=\color{green}{\frac{1}{e}}$$ $•\color{red}{y=x}$ then : $$\Omega =\lim\limits_{(x,x)\to (0,0)}\frac{|x|}{x^{2}}~e ^{-\frac{|x|}{x^{2}}}$$ $$\Omega =\lim\limits_{(x,y)\to (0,0)}\frac{1}{|x|}~e ^{-\frac{1}{|x|}}$$ $$=\lim\limits_{t\to +\infty}te^{-t}=\color{green}{0}$$ This mean that : does not exist lim! The second suggested solution Using the polar coordinates, we find: $$x=r\cos \theta , y=r\sin \theta $$ So : $$\Omega =\lim\limits_{r\to 0}\frac{|\sin \theta |}{r\cos^{2} \theta }e^{-\frac{|\sin \theta |}{r\cos^{2} \theta }}$$ $$=\lim\limits_{t\to +\infty}te^{-t}=\color{red}{0}$$ I am waiting for your explanation, comments and advice, I will be happy if i see other ways! Thanks!","['multivariable-calculus', 'limits', 'calculus']"
3627942,Definition Explanation for differential cryptanalysis,"I am reading Differential Attack from Stinson-Cryptography: Theory and Practice on a toy example of S-box(block Cipher) I am mainly confused in the following definition Definition 3.1 : Let $\pi_S:\{0,1\}^m\rightarrow \{0,1\}^n$ be a S-box. Consider an (ordered) pair of bitstrings of length $m$ , say $(x,x^*)$ . We say that input XOR  of the S-box is $x\oplus x^*$ and the output XOR is $\pi_S(x)\oplus\pi_S(x^*)$ . Now, the point of confusion is how can the output of the S-box be separated as $\pi_S(x)\oplus\pi_S(x^*)$ when the input is $x\oplus x^*$ ? For eg: We choose the two plaintexts $x_1$ and $x_2$ . Then we have $x_{12}=x_1\oplus x_2$ Now, we have round $1$ key $K^1$ , we get $^1u^1=x_1\oplus K^1$ and $^2u^1=x_2\oplus K^1$ Then, we apply $\pi_S$ on the above, we get $^1v^1=\pi_S(^1u^1)=\pi_S(x_1\oplus K^1)$ and $^2v^1=\pi_S(^2v^1)=\pi_S(x_2\oplus K^1)$ Therefore, $^1v^1\oplus\enspace ^2v^1=\pi_S(^1u^1)\oplus \pi_S(^2v^1)=\pi_S(x_1\oplus K^1)\oplus \pi_S(x_2\oplus K^1)$ Now, I don't understand that How, $^1v^1\oplus\text{ }^2v^1=\pi_S(x_1)\oplus\pi_S(x_2)$ ? Also, I don't know whether I got the definition or not?","['cryptography', 'discrete-mathematics']"
3627994,"Prove that $f(x)=\sum_{n=1}^\infty \max(0, 1-2^n|x-n|)$ is square integrable on $\mathbb{R}$.","In this post , @Post No Bulls claims that $$
f(x)=\sum_{n=1}^\infty \max(0, 1-2^n|x-n|)
$$ is square integrable on $\mathbb R$ but $\limsup_{x\to\infty} f(x)=1$ , and $\lim_{x\to\infty} f(x)$ does not exist. How to prove that $f$ is square integrable on $\mathbb{R}?$","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis', 'calculus']"
3628025,"Prove there is a formula $\phi(x,z)$ of VC dimension $\kappa$ and dual VC dimension $2^\kappa$","I first recall some definition since the notation can differ somewhat. Consider a formula $\phi(x,z)$ defining a binary relation $\{(u,v)\in U\times V|\quad \phi(u,v)\}$ . $\phi$ defines subsets of $U$ as $\phi(U,b)=\{a\in U|\quad \phi(a,b)\}$ . The corresponding collection of defined subsets is $\phi(U,b)_{b\in V}\subset\wp(U)$ Now, we say $\phi$ has VC dimension $\kappa$ if $\kappa$ is the maximum cardinality of $A\subset U$ such that $\phi$ shatters $A$ i.e. $\phi(A,b)_{b\in V}=\wp(A)$ The dual dimension is just the dimension of the dual formula $\phi^*(z,x):=\phi(x,z)$ Now I am asked to find a formula as in the title. The only way I know to produce a formula (actually a set-system, in an equivalent fashion) of arbitrary $\kappa$ VC dimension is to consider: $$\Phi=U^{\leq\kappa}:=\{A\subset U|\quad |A|\leq\kappa\}\subset\wp(U)$$ Now this is equivalent to the binary relation defined by $$\phi(u,B)\Leftrightarrow u\in B \quad\quad on\ \ U\times\Phi$$ So my try is to prove that $\phi^*(B,u)$ has VC dimension $2^k$ . To asess that it has dimension $\geq2^\kappa$ I would try by proving that since there must be $A\subset U\ \ |A|=\kappa$ shattered by $\phi$ then $\phi^*$ shatters $\wp(A)\subset \phi\ \ |\wp(A)|=2^\kappa$ . Anyhow I am quite stuck on this. I think that similarly one can prove that $\phi^*$ has VC dimension $\leq 2^\kappa$ Can someone help?","['computational-geometry', 'model-theory', 'logic', 'machine-learning', 'combinatorics']"
3628035,I need to prove that A is Lebesgue-measurable,"I'm struggling with this measure theory problem. It says: Let $B$ be a Lebesgue-measurable set, with $m(B)<\infty$ and $A\subset B$ . Prove that $A$ is Lebesgue-measurable if and only if $m(B)=m$ * $(A)+m$ * $(B\backslash A)$ . ( $m$ is the measure application
and $m$ * $ $ the exterior measure application in $\Bbb R$ ). The implication $A$ is Lebesgue-measurable $\longrightarrow$ $m(B)=m$ * $(A)+m$ * $(B\backslash A)$ is easy, because if $A$ is Lebesgue-measurable, then $m$ * $(F)=m$ * $(F\cap A)+m$ * $(F\backslash A), \forall F\subseteq \Bbb R$ . I have problems with the reverse implication. The problem gives a hint: ""you can use that, if $A\subseteq \Bbb R, m$ * $(A)<\infty$ , then $\exists B$ Lebesgue-measurable with $A\subseteq B$ that verifies $m(B)=m$ * $(A)$ "".","['measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
3628133,Can the exposed area of a 3D shape ever be less than the side with minimum area?,"If you have a 1 m^3 cube centered on the origin (0,0,0) the exposed area on the XY plane is 1 m^2. If you rotate that cube 45 deg then the exposed area becomes $\sqrt{2}$ (larger than the surface area one side). It made me wonder - is it possible to have a shape where the exposed area can ever be smaller than the side of that shape with the smallest surface area? Cheers","['area', 'geometry', 'rotations']"
3628221,Zeros of the Jacobi Theta function,"How do you obtain all the zeros in $z$ of the Jacobi Theta function $$\vartheta(z) = \sum_{n} e^{\pi i n^2 \tau + 2\pi i n z} \, ?$$ Probably the easiest way is to just read them of the Jacobi-Triple product, but I'm pretty sure they can also be derived from the series representation. The zeros are $$z=a\tau + b + \frac{\tau + 1}{2} \, ,$$ where $a,b \in {\mathbb Z}$ , of which I lack to find the term $a\tau$ . Since $\vartheta(z+1)=\vartheta(z)$ , it is periodic with perdiod $1$ in $z$ . So any zero $z_0$ will lead to a zero $b+z_0$ for any integer $b$ . It can be seen that $z_0=\frac{\tau+1}{2}$ is a zero since $$\vartheta(z_0) = \sum_{n} e^{\pi i n^2 \tau + \pi i n (\tau+1) } \stackrel{n\rightarrow -n-1}{=} \sum_{n} e^{\pi i n^2 \tau + 2\pi i n \tau + \pi i\tau - \pi i (n+1)(\tau+1)} \\
= -\sum_{n} e^{\pi i n^2 \tau + \pi i n \tau - \pi i n} = -\sum_{n} e^{\pi i n^2 \tau + \pi i n (\tau + 1)} = - \vartheta(z_0) \, .$$ $\vartheta(z)$ has a period of $2$ in $\tau$ , but that doesn't help to obtain the term $a\tau$ . Any idea?","['complex-analysis', 'theta-functions', 'analytic-number-theory']"
3628282,Standardization of coset tables in GAP,"I struggle to understand the notion of a standard coset table. In particular, I do not understand what the GAP function 'StandardizeTable' really does. My guess is that it relabels the cosets such that the resulting coset table is standard. My question is: Can it happen that, during this standardization procedure, the coset $1$ (which should correspond to the given subgroup) is relabeled to something else? Or is the coset $1$ always fixed?","['gap', 'group-theory', 'computer-algebra-systems']"
3628351,Verify triginometric result of cubic equation $x^3-x^2-p^2x+p^2=0$,"Consider the following cubic function, $$f(x):=(x+p)(x-p)(x-1)=x^3-x^2-p^2x+p^2$$ where $p\in(0,1)$ is a fixed parameter.
Then the sum of the absolute value of the three roots is $$S_1:=1+2p$$ On the other hand, if I use the trigonometric root formula to calculate the absolute sum, I obtained $$ S_2:=\frac{4}{3} \sqrt{1+3p^2}\, \cos \Bigg(\frac{1}{3}\arccos\Bigg(\frac{1-9p^2}{(1+3p^2)^{\frac{3}{2}}}\Bigg)-\frac{\pi}{3}\Bigg)+\frac{1}{3}$$ Then, obviously, the $\textbf{following is true}$ $$S_1=S_2 \quad \forall p\in(0,1)$$ $\textbf{Question:}$ Is there a way to verify $S_1=S_2$ by direct calculation? (I've tried with the Taylor series of the $\arccos(x)$ but I've not succeded.)","['cubics', 'trigonometry', 'real-analysis']"
3628360,"Any element $g$ of $GL(2,p)$ of order $p$, $p$ prime, is conjugate to $\begin{bmatrix}1&1\\0&1\end{bmatrix}$","Any element $g$ of $GL(2,p)$ of order $p$ , $p$ prime, is conjugate to $\begin{bmatrix}1&1\\0&1\end{bmatrix}.$ I showed that $\langle g\rangle $ acts on the set $X$ of vectors with entries in $ F_p$ and hence that $g$ fixes some non-zero element of $X$ (By Orbit-Stabiliser, since $|X| = p^2$ and $|\langle g\rangle|=p$ ). In an exercise, I am then asked to deduce from this the statement above, which I am stuck on.","['matrices', 'group-theory', 'finite-fields']"
3628366,"Possible significant error in proof of the spectral theorem, Brian C Hall, Quantum Theory for Mathematicians","Note/Edit: Read the below paragraphs for context. I think I found a counterexample, though I don't have the energy to work through it now. Suppose $\mathcal{H} = L^2([0,1],m) \oplus L^2([0,1],m)$ where $m$ denotes Lebesgue measure. Consider the ""position"" operator $X$ given by $Xf(x) = xf(x)$ and consider the operator $A$ given by $Af = 1_{[1/2,1]}f$ , each operator is a bounded self-adjoint operator on $L^2([0,1],m)$ . Then define the bounded self-adjoint operator $X \oplus A$ on $\mathcal{H}$ in the obvious way: $X \oplus A (f,g) = (Xf,Ag)$ . You can look at Hall's construction of the $(W_j, \psi_j)$ , but you can convince yourself that we could take $W_1 = L^2([0,1],m) \times \{0\}$ and have $\psi_1 = (1,0)$ (standard techniques using density of polynomials will then convince you that the closed span of the $(X \oplus A)^n \psi_1$ is $W_1$ ). Then note that $1 \in \sigma(X \oplus A|_{W_1})$ , $1_{\{1\}}(X \oplus A) \neq 0$ (as $(0, 1_{[1/2,1]} g)$ is an eigenvector of $X \oplus A$ with eigenvalue $1$ for all $g \in L^2([1/2,1],m)$ ), hence $\mu(\{1\}) \neq 0$ but $\mu_{\psi_1}(\{1\}) = 0$ , which contradicts surjectivity as I noted in the 3rd to last paragraph. Let me know if you think this works or doesn't. If if does then I guess there's a serious problem with the proof in the book. So I am in the middle of reading through B.C. Hall's proof of the spectral theorem for bounded self-adjoint operators on separable Hilbert spaces, so this question is a little hard to state given the amount of context. Let $A$ a self-adjoint bounded operator on a separable Hilbert space $\mathcal{H}$ . Suppose $\{W_j, \psi_j\}$ is a (possibly finite) sequence of pairwise orthogonal subspaces of $\mathcal{H}$ s.t. they are invariant under $A$ , for fixed $j$ the span of $A^n \psi_j$ is dense in $W_j$ , and $\mathcal{H} = \bigoplus_j W_j$ (the orthogonal direct sum). Let $A_j = A|_{W_j}$ . Let $\mu^A$ , $\mu^{A_j}$ denote the projection valued measure on the Borel $\sigma$ -algebra on $\sigma(A), \sigma(A_j)$ resp. (where $\sigma(B)$ is the spectrum of $B$ ). Let $\mu_{\psi_j}$ be the positive measure given by $\mu_{\psi_j}(E) = (\psi_j, \mu^{A_j}(E) \psi_j)$ . Let $\mu$ be a positive measure on $\mathcal{B}_{\sigma(A)}$ s.t. $\mu(E) =0 \iff \mu^A(E) = 0$ . Then note that $\mu^{A_j}(E) = 1_E(A_j) = 1_E(A|_{W_j}) = 1_E(A)|_{W_j}$ (where we are referring to the functional calculus induced by the respective operators, and the last equality follows from a lemma). Thus $\mu(E) = 0$ implies $0=\mu^A(E)= 1_E(A)$ , hence $\mu^{A_j}(E) = 1_E(A_j) = 1_E(A)|_{W_j} = 0$ which implies $\mu_{\psi_j}(E) = 0$ . So $\mu_{\psi_j}$ is absolutely continuous w/r/t $\mu$ on $\mathcal{B}_{\sigma(A_j)}$ . Let $\rho_j$ denote its Radon-Nikodym derivative. Finally we get to the part I'm stuck at. The author now claims ""one can easily see that"" the map $f \mapsto \rho_j^{1/2}f$ is unitary from $L^2(\sigma(A_j), \mu_{\psi_j}) \to L^2(\sigma(A_j), \mu)$ . It is easy to verify it is norm preserving but why is it surjective? Note that we only showed that $\mu_{\psi_j}$ was absolutely continuous w/r/t $\mu$ . I claim that we need the converse to hold. Suppose we had the converse. Then $\rho_j$ is a.e. (both $\mu$ and $\mu_{\psi_j}$ since these agree on null sets) to a function that is everywhere nonzero, so we can WLOG suppose that $\rho_j$ is everywhere nonzero. Then we note that $\int_{\sigma(A_j)} |f \rho_j^{-1/2}|^2 d\mu_{\psi_j} = \int_{\sigma(A_j)} |f|^2 \rho_j^{-1} \rho_j d\mu = \int_{\sigma(A_j)} |f|^2 d\mu$ . So if $f \in L^2(\sigma(A_j), \mu)$ , then $f\rho_j^{-1/2} \in L^2(\sigma(A_j), \mu_{\psi_j})$ . So the map is onto. On the other hand if we have that the converse doesn't hold, by considering a set that is $\mu_{\psi_j}$ null but not $\mu$ null ( $\mu$ can be taken to be a finite measure, we can WLOG suppose this set has finite measure), and considering the indicator function on this set, we can see that no function in $L^2(\sigma(A_j), \mu_{\psi_j})$ will map onto it. So why does this converse hold (to me it doesn't seem like it actually does so it feels like there is a near fatal error in this proof)? I tried constructing examples, but they got confusing fast. Sorry for the wall of text, the question is really deep into a long proof but I'm totally stuck and any help would be greatly appreciated.","['real-analysis', 'hilbert-spaces', 'functional-analysis', 'quantum-mechanics', 'spectral-theory']"
3628457,Limit Points of a sequence on the Circle Group,"I'm reading ch 7 of Nadkarni's book Spectral Theory of Dynamical Systems and I've come across this statement in a proof I'm currently trying to understand: If $z\in S^1$ (where $S^1$ denotes the Circle Group) and $z^{n_k}$ , with $k\in\mathbb{N}$ , has only a finite set of limit points of the form $e^{2\pi i \frac{p}{q}}$ (where $p$ and $q$ are integers and $q>0$ ), then for some integer $p_0$ , $z^{-p_0n_k}\rightarrow 1$ as $k\rightarrow\infty$ . I don't really understand, why this holds and would very much appreciate any input on this.","['limits', 'general-topology', 'topological-groups', 'intuition']"
3628488,Why is a DG-enhancement of the derived bounded category of coherent sheaves an enhancement?,"In order to make mirror symmetry more compatible with homological machinery, I understand it is common to give the derived bounded category on a variety a ""DG-enhancement"" by keeping around the data of an affine Cech cover. This is the perspective taken in ""Dirichlet Branes and Mirror Symmetry,"" section 8.2.1, page 586. Fix an affine cover $\mathcal{U}$ on $X$ a nonsingular variety. The objects of $D^b_\infty(X)$ are bounded complexes of locally free sheaves (shouldn't this be coherent, not locally free?). We define for each $q$ a complex of degree- $d$ morphisms $$
\mathcal{H}om^q(\mathcal{E}^\bullet, \mathcal{F}^\bullet) = \bigoplus_m \mathcal{H}om(\mathcal{E}^m, \mathcal{F}^{m+q}),
$$ (sheaf homs are over $\mathcal{O}_X$ ), which has a natural differential in increasing index. Combining this with the data of the Cech cover gives us a double complex, with Cech boundaries in one direction and the (degree- $q$ to degree- $(q+1)$ ) boundary in the other. We define the homset in our category to be the total complex of this double complex in the usual way: $$
\operatorname{Hom}^n_{D^b_\infty(X)}(\mathcal{E}^\bullet, \mathcal{F}^\bullet) := \bigoplus_{p+q = n} \check{C}^p(\mathcal{U}, \mathcal{H}om^q(\mathcal{E}^\bullet, \mathcal{F}^\bullet))
$$ My question: the text asserts that if we take the cohomology of this category (i.e. cohomology of each its morphism complexes), we should recover the usual derived category $D^b(X)$ . Is this obvious? There is an underlying spectral sequence computation, but I don't see why it simplifies.","['homological-algebra', 'spectral-sequences', 'algebraic-geometry', 'mirror-symmetry']"
3628534,Efficiently solving a 2D affine transformation,"For an affine transformation in two dimensions defined as follows: $$ 
p_i'=\mathbf{A}p_i \Leftrightarrow \\
\left[
\begin{matrix}
x_i' \\ y_i'
\end{matrix}
\right]
=
\left[
\begin{matrix}
a & b & e \\
c & d & f
\end{matrix}
\right]
\left[
\begin{matrix}
x_i \\ y_i \\ 1
\end{matrix}
\right]
$$ Where $(x_i,y_i), (x_i',y_i')$ are corresponding points, how can I find the parameters $\mathbf A$ efficiently? Rewriting this as a system of linear equations, given three points (six knowns, six unknowns): $$
\textbf{P}\alpha=\textbf{P}' \Leftrightarrow \\
\left[
\begin{matrix}
x_0 & y_0 & 0 & 0 & 1 & 0 \\
0 & 0 & x_0 & y_0 & 0 & 1 \\
x_1 & y_1 & 0 & 0 & 1 & 0 \\
0 & 0 & x_1 & y_1 & 0 & 1 \\
x_2 & y_2 & 0 & 0 & 1 & 0 \\
0 & 0 & x_2 & y_2 & 0 & 1 \\
\end{matrix}
\right]
\left[
\begin{matrix}
a \\ b \\ c \\
d \\ e \\ f
\end{matrix}
\right]
=
\left[
\begin{matrix}
x_0' \\ y_0' \\x_1' \\ y_1' \\x_2' \\ y_2'
\end{matrix}
\right]
$$ Allows the use of an LU decomposition, which can be computed in $O(M(n))$ time, where $M(n)$ is the time to multiply two n×n matrices (according to 1 ). Can the specific structure of the $\mathbf P$ matrix be exploited to utilize the Gaussian elimination to reach the reduced row echelon form (thus solving the system) more efficiently? Is there a way to symbolically derive the required operations? By hand seems rather cumbersome Thanks","['matrices', 'numerical-linear-algebra', 'linear-algebra', 'geometry']"
3628546,Conditional Expected value of the number of coin tosses until two heads are landed.,"Question: We toss a coin until we get 2 consecutive heads, but the probability, $p$ , of the coin landing heads is beta distributed with parameters $p \sim \beta(6, 8)$ . What is the expected number of flips until two heads show consecutively? So, previously I have done a very similar problem that is essentially the same problem except that the probability of the coin showing heads is simply $\frac{1}{2}$ , not beta distributed. In this simple case, the way that we can find the expected number of flips until 2 consecutive heads is the following: Let $X$ be the number of coin flips until 2 consecutive heads land. Let $H_{i}$ be the event of landing heads on the $i^{th}$ toss, and same for $T_{i}$ being tails. Then $E(X)$ may be conditioned on the first and second tosses. $$ E(X) = E(X | H_1) P(H_1) + E(X|T_1)P(T_1)$$ by law of total expectation. Now, the probabilities in this equation are easy, both are $\frac{1}{2}$ . Then, we can write $E(X | T_1) = 1 + E(X)$ because landing tails on the first toss is essentially like wasting that toss and starting over. Then, $$ E(X | H_1) = E(X|H_1, H_2)P(H_2) + E(X|H_1, T_2)P(T_2) $$ where we then condition on the second toss. The probabilities are again the same, and we can write all of the conditional expectations in terms of $E(X)$ , where $E(X|H_1, H_2) = 2$ and $E(X|H_1, T_2) = 2 + E(X)$ . We can then solve for $E(X)$ plugging into the original equation. However, in this case, we do not have that these probabilities are so simple, and rather it seems to me that we must condition $X$ on the random variable $P$ for the probability. My confusion and main concern for this is that I still intuitively think that we should still set the problem up and solve it in a very similar way to the simple case in that we need to condition $X$ on the first and second tosses. I am really confused on how to condition $X$ on both the probability and the tosses at the same time and how to express this in the forms of conditional expectations.","['conditional-probability', 'probability-distributions', 'conditional-expectation', 'expected-value', 'probability-theory']"
3628601,Is it valid to use operations on both sides before inequality is proven?,"As part of a bigger proof, I am trying to prove the inequality: $$\frac{ab}{a^2 + b^2}< \frac{1}{2}$$ Is the following proving method correct? $$\frac{ab}{(a^2)+(b^2)} <\frac{1}{2}$$ $$2ab < (a^2 + b^2)$$ $$0 <(a-b)^2$$ $a \neq b$ thus $a - b \neq 0$ and every number squared is not negative thus $(a-b)^2$ is positive thus bigger than zero My concern with this proof is that I am not sure if I am allowed to do operations on both sides before inequality is proven.","['algebra-precalculus', 'proof-writing', 'solution-verification', 'inequality']"
3628712,Finding solution set of $\frac{1}{\log_4 \left(\frac{x+1}{x+2}\right)}<\frac{1}{\log_4(x+3)}$ without using derivatives,"$$\frac{1}{\log_4\left(\frac{x+1}{x+2}\right)}\lt \frac{1}{\log_4(x+3)}$$ This inequality can be solved by using the monotonicity of $f(x)$ on $x\in(-1 ,\infty)$ where $f(x)=\frac{1}{\log_4\left(\frac{x+1}{x+2}\right)}-\frac{1}{\log_4(x+3)}$ and using the value of the function at any random point. Using this, the solution set comes out to be $(-1,\infty)$ . Can the inequality be solved more simply and without using methods of brute force? Any hints are appreciated. Thanks","['inequality', 'logarithms', 'proof-writing', 'functions', 'algebra-precalculus']"
3628823,Is it correct to consider the integral as the continuous equivalent of summation?,"I'm asking this because in many applications where there is a discrete case of some calculation (that includes a summation) and another continuous case, in the continuous one we simply swap the summation sign for an integral. Note: for the sake of disambiguation, I'm not referring to the Riemann Sum.","['integration', 'calculus', 'summation']"
3628828,Does there exist a sequence $\{a_n\}_{n \ge 0}$ of nonnegative reals such that $ \sum_{j \ge 0} a_{nj} = \frac{1}{n}$ holds for all naturals $n$?,Does there exist a sequence $\{a_n\}_{n \ge 0}$ of nonnegative reals such that $$ \sum_{j \ge 0} a_{nj} = \dfrac{1}{n}$$ holds for all naturals $n$ ? My progress: I could show that $a_n\le \frac{1}{2n}$ . I am not sure if this is even useful.,['sequences-and-series']
3628865,Mistake in Spivak's definition of a consistent orientation on a manifold,"I think Spivak may have made a mistake when defining the notion of a consistent orientation on a manifold. I've included the relevant section of Calculus on Manifolds below. I should mention that when Spivak mentions a manifold, they are referring to an embedded submanifold in $\mathbb{R}^n$ . It is often necessary to choose an orientation $\mu_x$ for each tangent space $M_x$ of a manifold $M$ . Such choices are called consistent provided that for every coordinate system $f\colon W\to\mathbb{R}^n$ and $a,b\in W$ the relation $$[f_*((e_1)_a),\ldots,f_*((e_k)_a)]=\mu_{f(a)}$$ holds if and only if $$[f_*((e_1)_b),\ldots,f_*((e_k)_b)]=\mu_{f(b)}.$$ I think this definition of consistent orientation is problematic when $W$ is not a connected set. For instance, consider $$M=\{(x,y)\in\mathbb{R}^2:y=0\}.$$ Then $M$ is a $1$ -manifold in $\mathbb{R}^2$ . However under Spivak's definitions, $M$ is non-orientable. Indeed, suppose for the sake of contradiction that $\mu$ is a consistent orientation on $M$ . Define $W=(-1,1)\cup (2,4)\subset\mathbb{R}$ and the coordinate systems $f,g\colon W\to\mathbb{R}^2$ by $$f(x)=(x,0)\qquad\text{and}\qquad g(x)=\begin{cases}(-x,0), &\text{if }x\in(-1,1);\\(x,0),&\text{if }x\in(2,4).\end{cases}$$ Finally, set $a=0$ and $b=3$ . Then $f(a)=g(a)=(0,0)$ and $f(b)=g(b)=(3,0)$ . According to Spivak's definition, we must have $[f_*((e_1)_a)]=\mu_{(0,0)}$ if and only if $[f_*((e_1)_b)]=\mu_{(3,0)}$ . Similarly, $[g_*((e_1)_a)]=\mu_{(0,0)}$ if and only if $[g_*((e_1)_b)]=\mu_{(3,0)}$ . This is impossible, since $$[f_*((e_1)_a)]=-[g_*((e_1)_a)]\qquad\text{but}\qquad [f_*((e_1)_b)]=[g_*((e_1)_b)].$$ I think a similar construction shows that every manifold is non-orientable with Spivak's definition. How do we fix this definition? Does it suffice to insist that $W$ be connected?","['manifolds', 'orientation', 'differential-geometry']"
3628963,Do there exist inner product spaces for families of real valued functions other than weighted integrals?,"In transform theory we join linear algebra with analysis by defining scalar products for real valued functions with weighted integrals of products, for example: $$\langle f,g\rangle_w = \int_{-\infty}^{\infty} w(t)f(t)g(t)dt$$ And in multiple dimensions as multivariate integrals: $$\langle f,g\rangle_w = \int\cdots\int_{-\infty}^{\infty} w(t_1,\cdots,t_k)f(t_1,\cdots,t_k)g(t_1,\cdots,t_k)dt_1 \cdots dt_k$$ To my question, does there exist other ways to define inner products for families of real valued functions?","['inner-products', 'reference-request', 'real-analysis', 'linear-algebra', 'soft-question']"
3628965,A Closed Interval is a Closed Set (Proof Verification),"I'm practicing my proof-writing and was hoping you could let me know if this proof looks good. I would like to know if the proof is incorrect if there are parts that are overly wordy/complicated, or if I'm missing some element of proof that is helpful to see, if not strictly necessary. The Prompt (from here): Show that closed intervals in $\mathbb R^1$ - sets of the form $\{x:a\leq x\leq b\}$ for fixed numbers $a$ and $b$ - are closed sets. My Proof: Suppose $I = \{x:a\leq x\leq b\}$ and is thus a closed interval in $\mathbb R^1$ . Then $I^c =\{y\in\mathbb R^1\mid y<a \text{ or }y>b\}$ which we will split into two sets $I_a^c= \{y\in\mathbb R^1\mid y<a\}$ and $I_b^c= \{y\in\mathbb R^1\mid y>b\}$ so that $I_a^c \cup I_b^c=I^c.$ Let $y$ be an arbitrary element of $I_a^c$ and define $e=a-y.$ Now define open ball $B_e(y) = \{z\in\mathbb R^1\mid |z-y| < e\}$ . For $z \in B_e(y)$ we can see: \begin{align*}
|z-y| &< e\\
|z-y| &< a-y\\
z-y &< a-y\\
z &< a\\
\end{align*} So we can see that all elements of $B_e(y)$ are included in $I_a^c$ for any value of $y$ , so $I_a^c$ is open. A similar argument (with $e=y-b$ ) shows that $I_b^c$ is open as well. Thus, since the union of two open sets is also open, $I^c$ is open. Since complement of $I$ is open, $I$ is a closed set.","['elementary-set-theory', 'general-topology', 'solution-verification']"
3628991,Is the union of an arbitrary collection of topological spaces a topological space?,"Let me first give a definition. By a separation of a topological space $X$ , I mean a pair $U, V$ of disjoint non-empty subset of $X$ whose union is $X$ . My question revolves around this well-known theorem of connectedness in topology. Let me quote the theorem word-to-word from Munkres'. Theorem $1$ : The union of a collection of connected subspaces of $X$ that have a point in common is connected. What is bothering me is for a set, say, $S$ to be connected, $S$ has to first be a topological space. The statement of this theorem is implicitly asserting that the union of a collection of connected subspaces of $X$ is itself a topological space. At first I thought I misinterpreted this incorrectly. But then in the proof of Theorem $1$ above, Munkres wrote: We (want to) prove that the space $Y = \bigcup A_\alpha$ is connected. Suppose that $Y = C \cup D$ is a separation of $Y$ . The point: To have a separation $Y$ needs to be a topological space. So, it must for sure implicitly asserting that $\bigcup A_\alpha$ is a topological space. I would like to ask whether this is a true statement until I stumbled upon this Wikipedia page which says: yes it is a topological space. But then, reading the content of that page, I realized that that is way beyond the current scope of my topological adventure. So I would like to ask, are there gaps of my topological knowledge which I overlooked or is this a fact that needs to be taken just with faith for now?",['general-topology']
3628998,"Is $g(\eta)$, as defined in the question, a Dirac delta function?","Section 1: Maths Question (TL;DR version) In the course of trying to solve a physics problem (ref. Section 2), I encountered a mathematical question. To make my post brief, I'll write only the maths question here that needs to be addressed: \begin{align} &f(r,\eta)= -\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}} &\text{where, }0\leq r \leq \infty \text{ & }-1 \leq \eta \leq 1\end{align} When one plots $f$ as a function of $r$ for various values of $\eta$ , one observes that $f$ is continuous at $r=R$ for all values of $\eta$ except $\eta=1$ . In the case of $\eta=1$ , $f$ diverges to $+\infty$ and $-\infty$ on the left and right sides of $r=R$ respectively $\left(\because f(r,1)=-\frac{r-R}{|r-R|^3}\right)$ . This implies the following, \begin{align}g(\eta) \equiv \lim_{r \to R+}f(r,\eta)-\lim_{r \to R-}f(r,\eta) \; &\text{is zero for }\eta \neq 1 \\ & \text{ blows up for }\eta=1 \end{align} This is similar to how a Dirac delta function behaves (blows up at one point and zero everywhere else). A stronger motivation for why I believe it might be a Dirac delta function is given in the next section. Question : Is $g(\eta)$ as defined above, a Dirac delta function in $\eta$ (up to some scale factor)? Section 2: Physics Problem The physics problem setup is a general spherical surface charge distribution $\sigma(\theta,\phi)$ of radius $R$ . It is known that the component of the electric field, $\mathbf{E}=-\nabla\Phi$ , that is normal to the spherical surface is discontinuous. i.e., $$\lim_{r \to R+}\partial_r \Phi(r,\theta,\phi)-\lim_{r \to R-}\partial_r\Phi(r,\theta,\phi)=-\frac{\sigma(\theta,\phi)}{\epsilon_0} \tag{1; eq. 2.31 in [1]}$$ The above result is commonly proved by applying Gauss' law to an infinitesimal Gaussian ""pill-box"" covering the region of interest. However, I wish to prove the above result (eq. 1) by only using the following Green's function solution for the electric potential (eq. 2). \begin{align}&\Phi(\mathbf{r}) =\frac{1}{4\pi \epsilon_0}\int \frac{\rho(\mathbf{r}')}{|\mathbf{r}-\mathbf{r}'|}d^3\mathbf{r}' &\rho(\mathbf{r})=\sigma(\theta,\phi)\delta(r-R) \tag{2}\\ 
\Rightarrow \;&\Phi(\mathbf{r})=\frac{1}{4\pi \epsilon_0}\int\frac{\sigma(\theta',\phi')}{|r \hat{r}-R\hat{r}'|}R^2\sin\theta' d\theta' d\phi' &\text{where, }\hat{r}=\hat{r}(\theta,\phi)  \text{ & }\hat{r}'=\hat{r}(\theta\,',\phi') \end{align} Using $|r \hat{r}-R\hat{r}'|=\sqrt{r^2+R^2-2rR\hat{r}\cdot\hat{r}'}$ , we have, \begin{align}\partial_r \Phi=-\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')\frac{r-R\hat{r}\cdot\hat{r}'}{|r \hat{r}-R\hat{r}'|^3}R^2\sin\theta' d\theta' d\phi'  \end{align} I encountered a question in the course of my attempt to prove eq. 1. I'll describe it below. \begin{align}&\partial_r \Phi=-\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}}R^2\sin\theta' d\theta' d\phi'  &\text{where, }\eta \equiv \hat{r}\cdot\hat{r}' \end{align} $$\lim_{r \to R+}\partial_r \Phi-\lim_{r \to R-}\partial_r\Phi =\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')(\lim_{r \to R+}f-\lim_{r \to R-}f)R^2\sin\theta' d\theta' d\phi'\tag{3}$$ $$\text{where, }f(r,\eta)\equiv -\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}} $$ When one plots this function $f$ online as a function of $r$ for various values of $\eta$ , one observes that $f$ is continuous at $r=R$ for all values of $\eta$ ( $\eta \in [-1,1]$ ) except $\eta=1$ . For $\eta=1$ , the function $f$ diverges to $+ \infty$ and $- \infty$ on the left and right  sides of $r=R$ respectively $\left(\because f(r,1)=-\frac{r-R}{|r-R|^3}\right)$ . This implies the following, \begin{align}g(\eta) \equiv \lim_{r \to R+}f(r,\eta)-\lim_{r \to R-}f(r,\eta) \; &\text{is zero for }\eta \neq 1 \tag{4}\\ & \text{ blows up for }\eta=1 \text{ ($\eta=1$ $\Leftrightarrow$ $\theta'=\theta$ and $\phi'=\phi$)}\end{align} This looks promising because the above behavior is similar to a Dirac delta function (blows up at one point and zero everywhere else). The discontinuity in the electric field at $(\theta,\phi)$ is only ""aware"" of the value of the surface charge density $\sigma$ at $(\theta,\phi)$ (ref. eq. 1) and hence, I believe I need a Dirac delta function in the integral in eq. 3 to get the $\sigma$ out of the integral. Question: Is $g(\eta)$ as defined in eq. 4, a Dirac delta function (up to some scale factor $\#$ )? That is, $$\text{Is }g= (\#)\; \delta(\theta'-\theta)\delta(\phi'-\phi)?$$ I'd really appreciate any insight that addresses my problem. References $[1]$ Griffiths, Introduction to Electrodynamics (3rd ed.)","['physics', 'multivariable-calculus', 'dirac-delta']"
3629021,Why does drawing the midpoint between some arbitrary point A and every point on some arbitrary curve create the same arbitrary curve?,"Take any curve at all, and select an arbitrary point A. Now draw the midpoint between A and every point of the curve. I conjecture that you will end up with a curve that is a translated and scaled version of the original curve. Why? What's the scaling factor and where is the translation exactly? Is my conjecture even true? It seems like some classical problem the Greeks have solved, but I couldn't find anything online and am stuck on it myself. Any help is appreciated. EDIT: This can also be generalized such that you don't take the the midpoint, but some point that divides the line in a given ratio. The conjecture still seems to hold.",['geometry']
3629045,Prove that if $m^p+n^p\equiv0\pmod p$ then $m^p+n^p\equiv0\pmod {p^2}$ where $p$ is an odd prime number.,"I am reading Burton W. Jones' The Theory of Numbers and have gotten stuck on this issue on page 58, section 2.5. I have used the theorem: $$\textrm{if}\ p\ \text{is a prime number, then}\ a^p≡a \pmod p$$ With which: $$m^p+n^p≡m+n \pmod p$$ and the Lemma: $$\binom{p}{k}≡0\pmod p$$ if $0<k<p$ .","['number-theory', 'elementary-number-theory']"
3629155,Existence of Dual for an Infinite Linear Program,"I am concerned with proving the existence of the dual of an infinite linear program. In addition to the writings of Rockafellar, Luenberger, and Boyd & Vandenberghe on: subdifferentials, Legendre-Fenchel transforms and the convex conjugate of a function, indicator functions, support functions/hyperplanes etc, I have consulted the following resources that seem to address this problem directly: http://web.mit.edu/mitter/www/publications/113_convex_optimization_RALC.pdf (Chapter 3) https://sites.math.washington.edu/~rtr/papers/rtr054-ConjugateDuality.pdf (Theorem 11 on page 34) While I am able to follow the development of the duality formalism developed therein, I am having trouble explicitly proving the existence of a dual problem for my infinite linear program in $\ell_1$ . I know the dual problem ought to be in $\ell_\infty$ , if it exists, but I am unsure how to proceed with an explicit formulation. Rockafellar's Conjugate Duality SIAM Publication only requires continuity of the optimal value function in the infinite dimensional case. My objective function $f:\ell_1\to\mathbb{R}$ , which I seek to minimize, in the variable $x=\{x_{ij}\}\in \ell_1$ , with $\{a_{ij}\}\in\ell_1$ being parameters, is: $$ f(x) =  \sum_{i=0}^{\infty}\sum_{j=0}^{\infty}a_{ij}x_{ij} $$ The constraints for my problem are all equality constraints. Rockafellar's discussion of existence, especially in Theorem 11, didn't seem concerned with the constraints, so I haven't considered them for proving existence of the dual problem. My question is as follows: Is it necessary to frame my objective function as Rockafellar does on page 18? That is to say: specifying a representation $f(x) = F(x,u) \quad u\in U, x\in X$ , where $X$ and $U$ are Banach spaces, and $u=0$ , which I believe would be the case to relate this representation to my objective function. Then, setting $\varphi (u) = \inf_{x\in X} F(x,u)$ , and attempting to show the dual exists via Theorem 11? If not, what would be the correct way of framing my objective function to show the dual exists? Please let me know if you need more information, or if I am misunderstanding anything, since I simply just want to show the dual exists for this problem. I have been using this site for years, but have recently joined and this is my first question, and I wasn't sure if it should be split up into separate questions or anything like that, so I hope it provides enough context and is answerable :)","['convex-optimization', 'lp-spaces', 'functional-analysis', 'optimization', 'convex-analysis']"
3629175,"Why is the open Set $(0,1)$ equivalent to the closed set $[0,1]$?","I understand the proof based on ordering $[0,1]$ into a set $A$ of distinct points that include $0$ and $1$ and then showing the one-to-one equivalence to $(0,1)$ , but what I can't get my head around is that from a non-mathematical view, it is clear that $[0,1]$ contains precisely $2$ more elements than $(0,1)$ ? I expect it has something to do with cardinality, and I get the basic concepts of equivalence and cardinality, but still trying to wrap my head around this question. I’m comfortable with the equivalence of $[0,1]$ to $[0,2]$ for example. This question is very similar but slightly different to this one: Are all infinities equal? I am specifically looking at the equivalence between open and closed sets. For example, if I map every element $x\in (0,1)$ to every element $y \in[0,1]$ such that $x = y$ then I will be left with two extra elements in $[0,1]$ . To me, this breaks one-to-one correspondence in how its typically interpreted, however I can see that it does not actually break the definition if we follow it to the strict letter since the idea of leftover elements does not factor into the definition: https://en.m.wikipedia.org/wiki/Bijection I’m definitely in the wrong here but I’m trying to understand the intuition.","['elementary-set-theory', 'real-numbers', 'analysis']"
3629220,Is my interpretation of why $\|\cdot\|_p$ becomes a norm on $L^p$ correct?,"I'm new to measure theory, $L^p$ spaces, etc. I was wondering if my interpretation of why $\|\cdot\|_p$ becomes a norm on $L^p$ is correct. Consider a measure space $(X,\mathcal{A},\mu)$ , and $\mathcal{L}^p = \{f : X \to \mathbb{C}, f \ \text{is measurable}, \|f\|_p  < \infty \}$ , where $\|f\|_p = \left( \int \lvert f \rvert^p \mathrm{d}\mu \right)^{1/p}$ , $p \in [1,\infty)$ is a seminorm on $\mathcal{L}^p$ . Define an equivalence relation on $\mathcal{L}^p$ as $f \sim g \iff f = g, \ \mu \ \text{a.e.}$ Let $L^p$ be the set of equivalence classes. Then $(L^p, \|\cdot\|_p)$ becomes a normed space. My understanding is that $\|\cdot\|_p$ starts out as a seminorm because it can be $0$ if $f$ is $0 \ \mu$ -a.e. For $\|\cdot\|_p$ to be a norm, $\|f\|_p = 0 \iff f = 0$ . This property is achieved when restricting $\|\cdot\|_p$ to $L^p$ , since if a function $f$ is $0 \ \mu$ -a.e., it is in the same equivalence class as $0$ , hence $f$ and $0$ are ""treated as the same thing"". Is this a correct interpretation?","['measure-theory', 'lp-spaces']"
3629226,Conditional Expectation of joint pdf,"I have a question about this joint distribution $f(x,y)=4\exp(-2y^2)$ $0<x<y$ and $0<y<\infty$ $Z=1$ if $|X-Y|>2$ $Z=0$ otherwise I need to find $E[Z|Y]$ Here's what I've done so far $E[Z|Y]$ = $P[|X-Y|>2|Y]$ = $P[|X-Y|>2|Y=y]$ = $P[2>X-Y>-2|Y=y]$ Now I'm stuck because I am not able to find the pdf of X.","['conditional-probability', 'statistics', 'conditional-expectation', 'probability']"
3629328,How can $SO(4) \cong SO(3) \times S^3$ if the fiber bundle $SO(3) \rightarrow SO(4) \rightarrow S^3$ does not have a global section?,"I am under the impression that if I had a product $X = A \times B$ the induced fibration $A \rightarrow X \rightarrow B$ would have a global section, namely $\sigma: B \rightarrow X$ taking $b$ to $(b, a_0)$ for any $a_0 \in A$ . Furthermore, I have seen (e.g. here ) that $SO(4) \cong SO(3) \times S^3$ as topological spaces. However, it seems that the fiber bundle $SO(3) \rightarrow SO(4) \rightarrow S^3$ doesn't have a global section (see e.g. here ; the second answer shows that there are local sections, which is sort of where I got this impression from). I am not sure what to make of this contradiction. My best guess at what's going on is that the ""canonical"" fiber bundle in the question linked above doesn't have a section, though one could write down a separate, perhaps less natural bundle $SO(3) \rightarrow SO(4) \rightarrow S^3$ that does have a section. In this case, what is this bundle, and what is the section? I am interested in using a global section to show that $SO(4)$ is homeomorphic to $SO(3) \times S^3$ . EDIT: It just occurs to me that perhaps in the general case the fiber bundle $SO(n) \rightarrow SO(n+1) \rightarrow S^n$ does not have a section (which would be consistent with the fact that in general the product does not seem to hold), but when $n = 3$ it happens to. In this case, can someone write down an explicit section $S^3 \rightarrow SO(4)$ ?","['general-topology', 'algebraic-topology']"
3629377,Can every element of a group be written as the product of two non-identity elements of the group?,"By part of the definition, two elements in a group can be put together with the group operation to obtain a third element that is also an element of the group. However, I am wondering if the converse is also true. So the new statement would be: For every element in the group, it can be written as the result of two non-identity elements of the group using the group operation. So here we are not considering the element itself with the identity. Is there any counterexample? Thanks.","['group-theory', 'abstract-algebra']"
3629383,"Let $H$ be a subgroup of $G$, and suppose that $G$ acts by multiplication over the set $X:=G/H$ of the left-hand side classes of $H$ over $G$.","This action is transitive. Now let be $yH \in X$ . What is the kernel of this action? I'm afraid that my answer isn't right. Could you check if the KERNEL is exactly this? MY ANSWER: Let $A:G \times X \rightarrow [G:H]=X$ be the action mentioned and $\lambda:G\rightarrow \operatorname{Sym}(G)$ (this homomorphism exists because of the permutational representation).
So, we have $
\begin{align*}
\operatorname{ker}(A)&=\{(g,yH) \in A: A(g, yH)=gyH=yH, \forall (g, yH)\}\\
&=\{g \in G: gyH=yH, \forall g \in G\}\\
&=yHy^{-1}, \forall g \in G\; \text{the reason of this step is going to be explained below**}\\
&=\displaystyle\bigcap_{g \in G} yHy^{-1}
\end{align*}
$ Explanation about **: $
\begin{align*}
\operatorname{Stab}(yH)&=\{g \in G: gyH=yH\}\\
&=\{g \in G: y^{-1} gyH=H\}\\
&=\{g \in G: y^{-1}gy \in H\}\\
&=\{g \in G: g \in yHy^{-1}\}\\
&=yHy^{-1}
\end{align*}
$","['permutations', 'group-theory', 'group-actions']"
3629388,"On Ramanujan's approximation, $n!\sim \sqrt{\pi}\big(\frac ne\big)^n\sqrt [6]{(2n)^3+(2n)^2+n+\frac 1{30}}$","Over here I discovered that Ramanujan gave the following factorial approximation, better than Stirling's formula: $$n!\sim \sqrt{\pi}\left(\frac ne\right)^n\sqrt [6]{(2n)^3+(2n)^2+n+\frac 1{30}}$$ such that the error term decreases rapidly as $n\to \infty$ . In other words, $$\lim_{n\to\infty}\cfrac{n!}{\sqrt{\pi}\left(\frac ne\right)^n\sqrt [6]{8n^3+4n^2+n+\frac 1{30}}}=1$$ Just to add, Stirling's formula is: $$n!\sim \sqrt{2\pi n}\left(\frac ne\right)^n$$ so somehow Ramanujan was able to turn $2n$ into $\sqrt [3]{8n^3+4n^2+n+\frac 1{30}}$ . Notice that $2n=\sqrt [3]{8n^3}$ so the important expression is $4n^2+n+\frac 1{30}$ . Does anybody know how he got this result? Or is this another one of his mysterious results...","['approximation', 'proof-writing', 'asymptotics', 'limits', 'radicals']"
3629394,"Prove that if a graph contains only odd cycles, there must exist a vertex with degree less than 3.","The graph need not be composed solely of a cycle, but every cycle in the graph need be an odd length cycle. I tried a contrapositive proof, trying to prove that if all vertices have degree greater than or equal to 3 then a graph does not contain any odd cycles but I didn't get very far with that.","['graph-theory', 'discrete-mathematics']"
3629436,"(Proof-writing)Prove $ \overline A \cup B=U \rightarrow A \subseteq B$,where A,B are sets in universe U.","My proof: $$ \begin{array} \\ \text{1.} & \overline A \cup B =U & premise \\ 2. & x \in U \leftrightarrow(x \in A) \land x \in (\overline A \cup B) & premise \\3. &x \in A &from \ (2) \\ 4. & x \in (\overline A \cup B) \equiv \lnot(x \in A) \lor (x \in B) \equiv x \in A \rightarrow x \in B & from \ (2) \\ 5. & x \in B & mp \ from \ (3),(4) \\ 6. & A \subseteq B &from \ (3),(4),(5) \\\hline \therefore &\overline A \cup B=U \rightarrow A \subseteq B \end{array}  $$ Any critique would be helpful!","['elementary-set-theory', 'proof-writing']"
3629474,Average angle between two randomly chosen vectors in a unit square,"Consider two randomly chosen vectors $(a,b)$ and $(c,d)$ within the unit square, where $a, b, c,$ and $d$ are chosen uniformly from $[0,1]$ . What is the expected angle between the vectors? Here's what I have so far. The angle between any two positive vectors $<a,b>$ and $<c, d>$ is $\arccos{\frac{ac + bd}{\sqrt{(a^2 + b^2)(c^2 + d^2)}}}$ . We just need to find the average value of this function over $a, b, c,d$ in range $[0,1]$ . This is equivalent to the quadrupal integral $$\iiiint_V \arccos{\frac{ac + bd}{\sqrt{(a^2 + b^2)(c^2 + d^2)}}} \,da\,db\,dc\,dd$$ . I can't find any way to compute this. I entered this into Mathematica and it wasn't able to output even a decimal approximation. I tried all the usual substitions to reduce this into a double integral but none of them worked. Converting to polar coordinates didn't work out nicely either. For one, the bounds become harder to work with and the jacobian is pretty nasty, so converting doesn't seem like it would help me. Probably the difficult part is working with the $\arccos$ function. I have no idea how to deal with it. Is it possible that there is an analytic solution to this integral? Even if someone found a numerical approximation, it would help me out.","['integration', 'geometric-probability', 'multivariable-calculus', 'calculus', 'multiple-integral']"
3629484,Prove that all ideals in $\mathbb{Z}[x]$ are generated by two elements.,"I was trying to prove that $\mathbb{Z}[x]$ is noetherian, so every ideal in $\mathbb{Z}[x]$ is finitely generated. I feel that all ideals in $\mathbb{Z}[x]$ are essentially generated by two elements - a polynomial and the smallest integer belonging to the ideal. Let $a(x) \in I$ , where $I$ is an ideal in $\mathbb{Z}[x]$ , be a polynomial whose degree is the least. Let $b(x)$ be another polynomial whose degree is more than $a(x)$ then $r(x)=a(x)-b(x)q(x) \in I$ becomes the polynomial of the smallest degree (we first assume that $r(x)$ is a non constant polynomial). So $r(x)$ has to be zero. If $r(x)$ is a constant in $\mathbb{Z}$ and  let $r $ be the least positive integer in $\mathbb{Z}[x]$ . If $r(x) \in (r)$ then we are done, or let $d=(r(x),r)$ then I will be generated by $(a(x),d)$ . What I think is that I am going wrong in the last paragraph. Can someone point out my mistake.","['noetherian', 'abstract-algebra', 'solution-verification', 'polynomials', 'ideals']"
3629551,Compute Curl Integral of Vector Field and Ellipse,"Consider a vector field : ${\bf F}=P(x){\bf i}+ Q(y){\bf j}$ , for some functions : $P(x), Q(y)$ which have continuous partial derivatives everywhere. Let: $C$ stand for the ellipse : $\{ 4x^2+9y^2=1\}$ . Then : $\int_C {\bf F}\cdot d{\bf r}$ equals: I can't determine an answer with this data but the options are: $P(\frac{1}{2}) - Q(\frac{1}{2}), 0, P(\frac{1}{2}),P(\frac{1}{2}) + Q(\frac{1}{2})$ Which is the right answer?",['multivariable-calculus']
3629557,How can the equation $Ax=b$ represent a polyhedron?,"I can understand how inequalities can be used to define a polyhedron, for example, each plane in a 3d setting would be one face and putting all the planes together we would get a closed body with the points in the inner volume satisfying all the inequalities. But when it comes to equations I cannot understand how you can replicate the above behaviour. The intersection of inequalities would give me a volume, but intersection of two equations of planes would give me a line or a point. Most texts seem to have a standard form for polyhedrons using equations like $Ax = b$ with $x\geq 0$ .","['polyhedra', 'discrete-geometry', 'polytopes', '3d', 'geometry']"
3629697,Reference on periodic Besov spaces,"I am looking for a reference for the construction and the study of the main properties of Besov spaces on the torus (i.e. $\mathrm{B}_{p,q}^s(\mathbb{T}^d))$ . Indeed, I know the classical $\mathrm{B}_{p,q}^s(\mathbb{R}^d)$ on the whole space and I would like to know if the theory is the same when we have periodicity. This is mainly a bibliographic question since I think the construction is quite the same, replacing Fourier transform by Fourier series (but I wonder if we could easily 'deduce' the periodic case from the euclidean case). Thank you !","['harmonic-analysis', 'besov-space', 'functional-analysis', 'reference-request']"
3629698,Proof of the derivative of a matrix exponential $\frac d {dt} e^{tA} = Ae^{tA}$,"I have the proposition in my book that $$\frac d {dt} e^{tA} = Ae^{tA}$$ The proof provided is somewhat terse. I think I've proved it using games with indices, but the book's preferred proof uses the defn of the derivative.
In particular, I'm annoyed by not understanding this line. $$\frac d {dt} e^{tA}=e^{tA} \lim_{h\rightarrow 0} \frac {e^{hA} -I}{h}$$ (we're fine and good) $$= e^{tA}A$$ (huh?) 
This would imply that the numerator of the limit is equal to $hA$ , which I am uncertain of how to prove. I have tried expanding $e^{hA}$ according to the defn of the matrix exponential, but see no further steps.","['matrices', 'matrix-calculus', 'derivatives', 'matrix-exponential']"
3629733,how to construct an equilateral triangle whose one vertex is given and other two vertices lie on two parallel lines,a vertex  and two parallel lines are given. construct an equilateral triangle whose other vertices lie on two parallel lines this is a question from the app euclidia. i tried to make equilateral triangle but one vertex is not touching the parallel line. please provide a solutionand how the construction works,"['euclidean-geometry', 'geometric-construction', 'geometry', 'triangles', 'plane-geometry']"
3629743,Generators for the ideal of entire functions vanishing on $\mathbb Z\times\mathbb Z\subset \mathbb C\times\mathbb C$,"For $n=1,2$ , let $R_n$ be the ring of entire complex functions in $n$ complex variables.  Let $I$ be the ideal $R_2$ of functions $f$ such that $f(\mathbb Z\times\mathbb Z)=\{0\}$ . Is $I$ generated by the two functions $(x,y)\mapsto\sin(\pi x)$ and $(x,y)\mapsto\sin(\pi y)$ ? The reason why I am asking this is the analogous case in $R_1$ : If $J$ is the  ideal $R_1$ of functions $f$ such that $f(\mathbb Z)=\{0\}$ then, for $f\in J$ , $\frac{f(x)}{\sin(\pi x)}$ seems to be an entire function.  This implies $J=\sin(\pi x)R_1$ .","['complex-analysis', 'analytic-geometry', 'several-complex-variables', 'ideals']"
3629771,Proof that a minimum problem has no solution,"Please I have an exam in a few days, can you help me with the following exercise? Let $A=\{x\in\mathbb{R}^2: 1<|x|<2\}$ and $M\geqslant 0$ . On the set $\mathcal{A}_{M}=\{u\in C(\bar{A})\cap C^1(A):u=0 \text{ on } |x| \text{ and } u=M \text{ on } |x|=1\}$ consider the area functional: \begin{equation*}
F:\mathcal{A}_{M}\to[0,\infty]
\end{equation*} \begin{equation*}
F(u)=\int_A\sqrt{1+|\nabla u(x)|^{2}} \, dx
\end{equation*} I have to prove the following statement: 3) There exists $M_0$ such that for $M>M_0$ the minimum problem has no solution in the class $\mathcal{A}_M$ Solution: I've computed the Euler Lagrange equation in the strong form for the functional: \begin{equation*}
F(\phi)= 2\pi\int_1^2 \sqrt{1+\phi'(r)}rdr
\end{equation*} Obtained from the previous one using the fact that $u$ is radial and computing a change of variables with polar coordinates. The explicit solution of the strong form of the Euler Lagrange equation is: \begin{equation*}
\phi(r)=\phi(1)-\int_1^r \dfrac{C}{\sqrt{t^2-C^2}}=M-\log\bigg({\dfrac{x+\sqrt{x^2-C^2}}{1+\sqrt{1-C^2}}}\bigg)
\end{equation*} $C$ is a real constant. Can someone help me to find an appropriate concusion of this proof?","['functional-calculus', 'functional-analysis', 'analysis', 'calculus-of-variations']"
3629848,Proving that $\frac{\pi^{3}}{32}=1-\sum_{k=1}^{\infty}\frac{2k(2k+1)\zeta(2k+2)}{4^{2k+2}}$,"After numerical analysis it seems that $$
\frac{\pi^{3}}{32}=1-\sum_{k=1}^{\infty}\frac{2k(2k+1)\zeta(2k+2)}{4^{2k+2}}
$$ Could someone prove the validity of such identity?","['riemann-zeta', 'constants', 'pi', 'sequences-and-series']"
3629928,Empty set clarification,"Is $∅ ∈ \{\{∅\}\}$ true or false?
I think it is false and my reasoning is this, ∅ is an element of a set of subset ∅ Since ∅ is an element of the set, it is therefore not an element of the subset inside the set . Am I right? Is ∅ ⊄ {∅,1,2} true or false?
I think it is true and my reasoning is this, ∅ is not a subset of set ∅, 1, 2 ∅ is an empty set therefore it is a subset of {∅, 1, 2} Lastly, is it right to say that in any power set of X, ∅ will always either be ∈ or ⊆ of power set X?","['elementary-set-theory', 'discrete-mathematics']"
3629957,Choosing representatives with spatial separation,"There are $n$ sets of $k$ points in the 2-dimensional plane. Following the recent social distancing instructions, the distance between each two points in the same set is at least 2. We would like to choose a single representative point from each set, such that the distance between each two representatives is at least 2. What is the smallest $k$ (as a function of $n$ ) for which this is always possible? For $n=2$ , I am quite sure that the answer is 3. $k=2$ is insufficient, as shown below: The distance between the green points is 2 and the distance between the blue points is 2, but the distance between each pair of representatives is only $\sqrt{2}$ . I do not have a proof that $k=3$ is sufficient, but in all configurations I tried, I ended up with a situation as shown below: If the distance between every two green points is 2, and the distance between every blue and green point is less than 2, then the blue points must be inside a very small region, and then there must be some blue points with a distance of less than 2. So my question is: given $n$ (the number of sets), what is the minimal $k$ (the number of points in each set) such that there always exist representatives with a sufficient separation?","['discrete-geometry', 'combinatorial-geometry', 'geometry']"
3630019,Does a concave increasing function that goes through the origin always have a diminishing elasticity?,"Consider a twice continuously differentiable function $f: \mathbb{R}_+^0 \rightarrow \mathbb{R}$ with $f(0)=0$ , $f'>0$ , $f''<0$ . Does that function feature non-increasing elasticity, i.e. $$\frac{ \partial \frac{f'(x) x}{f(x)}}{\partial x} \leq 0\quad \forall x?$$ I am unable to prove this simple point, but also unable to find a counter example. EDIT: I have made some progress, but it is not yet a proof. Take any $x>0$ and calculate the derivative on the LHS of the statement. The statement is true if (and only if) $$ f'(x) x - f(x) \geq \frac{f''(x) f(x) x}{f'(x)}$$ Using a second order Taylor expansion we obtain $$0=f(0)=f(x) - f'(x)x + 1/2 f''(x)x^2 - O(x^3).$$ Rearranging and plugging into the inequality we get $$ f''(x)x^2/2 - O(x^3) \geq \frac{f''(x) f(x) x}{f'(x)}$$ Which (at least in a neighborhood of 0 or if $-O(x^3)\geq 0$ ) has a sufficient condition in $$ f'(x) \leq 2\frac{f(x)}{x}.$$ Since $f(x)/x \geq f'(x)$ (because $f$ is concave, increasing and goes through the origin) that is true. Yet we don't know anything about what happens if $O(x^3) >0$ and we are far away from $x=0$ .","['derivatives', 'examples-counterexamples', 'real-analysis']"
3630041,Deformation of extensions,"I am trying to work on a deformation theory problem, but I don't have much experience in it, so any reference or insight would be much appreciated. Given two coherent sheaves $F$ and $G$ on a projective variety $X$ , we can consider an extension $E$ of these two sheaves $0\to F \to E \to G \to 0$ . Assume we have two deformations $F'\to F$ and $G' \to G$ (here I am treating deformations as in Hartshorne's book Deformation Theory). I was wondering if there is a way to induce a deformation of $E$ in a ""natural"" way. For example, if there exists a extension in $Ext^1_{X'}(G',F')$ compatible with the deformation maps, essentially, saying that there exists a surjective map $Ext^1_{X'}(G',F')\to Ext^1_{X}(G,F)\to 0$ . The above result is general in some sense, and I didn't find any reference in this direction. I was trying to apply this in the particular case where $F$ and $G$ are ideal sheaves of points in the $\mathbb{C}$ -projective space $\mathbb{P}^3$ , and we can even assume that the deformation $G$ is the trivial one, like $G'$ is just the pullback of $G$ . Thanks.","['algebraic-geometry', 'deformation-theory', 'coherent-sheaves']"
3630092,Relative Topology in $\mathbb{R}^n$,"I'm studying relative topology for the first time and I've come across a proposition which states if $V \subset \mathbb{R}^n$ is non empty and $A \subseteq V$ $$ \partial_v A = \partial A \cap V.$$ The proposition is clearly false if you take $A=V$ and $V$ closed then, $\partial_v V \subseteq \text{cl}_V(V - V)=\emptyset.$ Whereas, $\partial V$ is certainly not empty. The following is a proof on someone elses question Boundary of set on relative topology in $R^n$ . This doesn't work either, I've proven it for the case that $V$ is open, is that the only case that it holds for all $A \subset V$ (I know it is for $A \subseteq V$ )?","['multivariable-calculus', 'general-topology', 'real-analysis']"
3630101,What's the dimension of $K^S$ for arbitrary $S$?,"Let $K$ be a field, $S$ an arbitrary set, and $K^S$ denote the vector space of functions from $S$ to $K$ . What is the dimension of this space? By dimension, I mean the cardinality of a Hamel basis. I think the answer can only depend on $|S|$ and $|K|$ . I am also assuming the axiom of choice, thus the question makes sense.","['hamel-basis', 'cardinals', 'linear-algebra', 'functional-analysis']"
3630156,"Monotonicity of $f(x)=\max\left\{\frac{|x-y|}{x+y+1}:0\le y\le1\right\}$ for $x\in[0,1]$","Let the function $f:[0,1] \rightarrow \mathbb{R}$ be defined as $$f(x)=\max\left\{\frac{|x-y|}{x+y+1}:0\le y\le1\right\}\ \ \text{ for }\ \ 0 \le x \le 1\,,$$ then which of the following statements is correct? (A) $f$ is strictly increasing on $\left[0,\frac{1}{2}\right]$ and strictly decreasing on $\left[\frac{1}{2},1\right]$ (B) $f$ is strictly decreasing on $\left[0,\frac{1}{2}\right]$ and strictly increasing on $\left[\frac{1}{2},1\right]$ (C) $f$ is strictly increasing on $\left[0,\frac{\sqrt3-1}{2}\right]$ and strictly decreasing on $\left[\frac{\sqrt3-1}{2},1\right]$ (D) $f$ is strictly decreasing on $\left[0,\frac{\sqrt3-1}{2}\right]$ and strictly increasing on $\left[\frac{\sqrt3-1}{2},1\right]$ Apparently, this question was asked in an exam for high school students and shouldn't involve multi-variable calculus. I tried differentiation, trigonometric substitution, and modulus inequalities but nothing seems to be working. My question is how to solve it without multi-variable calculus?","['monotone-functions', 'maxima-minima', 'calculus', 'functions', 'algebra-precalculus']"
3630192,How can we evaluate this integral from the 2020 UC Berkeley Integration Bee?,"$$I = \int_1^2 \left(e^{1-\frac{1}{(x-1)^2}} +1\right) + \left(1 +\frac{1}{\sqrt{1- \log(x-1)} }\right)\mathrm{d}x$$ . From what I have read it has to do with forming a triangle, but I do not know where this comes from. If someone can please explain the evaluation of this it would be greatly appreciated. Thanks!","['integration', 'contest-math', 'real-analysis']"
3630204,bug on the box problem,"A bug walks on the surface of a box ( $L=B=1$ , $H=2$ ), starting at a corner, $A$ . You want to feed the bug but you also want the bug to walk the longest distance.  The bug takes the shortest path possible.  Where do you put the food to make it walk the longest? The box floats in the air and the bug can walk on any face of box it wants.
Hint: its not $B$ Refer the image","['puzzle', 'geometry']"
3630211,Tangent space of a union of closed affine varieties,"Suppose $X_{1}, X_{2} \subseteq \mathbb{A}^{n}$ are closed affine subvarieties such that $p:=(0,\dots,0) \in X_{1}\cap X_{2}$ . Define $X := X_{1} \cup X_{2}$ . I have shown the following inclusions for the tangent spaces: $T_{p}X_{1} + T_{p}X_{2} \subseteq T_{p}X$ . However, I have been asked to give an example where the above inclusion is $\textbf{strict}$ . I have tried to mess around with a few examples that I can visualise (i.e. $n \leq 3$ ), but I can only seem to think of varieties that ""intersect nicely"". I have not been able to come up with an example, or indeed develop an intuition as to why the above inclusion can be strict. Any help/hints would be appreciated!","['affine-varieties', 'algebraic-geometry', 'tangent-spaces']"
3630259,Integral matrices commute modulo $p^k$,"Given an $n \times n$ integral matrix $A \in M_n(\mathbb{Z})$ , denote by $A_k \in M_n(\mathbb{Z}/p^k \mathbb{Z})$ its reduction modulo $p^k$ . Now let $A, B \in M_n(\mathbb{Z})$ be be such that $A_k, B_k$ commute in $M_n(\mathbb{Z}/p^k\mathbb{Z})$ . What can we say about $A$ and $B$ ? I am looking for a statement of the form: there exist $A', B' \in M_n(\mathbb{Z})$ such that $A_k = A'_k, B_k = B'_k$ and $A'$ commutes with $B'$ . Or maybe this is too much to ask but we can still find commuting $A', B'$ such that $A_l = A'_l, B_l = B'_l$ , where $l$ is some function of $k$ ? What if we further require that $A_k, B_k \in GL_n(\mathbb{Z}/p^k\mathbb{Z})$ ? The question of whether almost-commuting matrices are close to commuting matrices has an enormous literature, but in everything I could find the matrices are complex and the ""almost"" and ""close"" is formalized in terms of the operator, Hilbert-Schmidt or Frobenius norm. Here I am asking the same question, but for integral matrices, where the ""almost"" and ""closed"" are formalized via equality modulo $p^k$ .","['modular-arithmetic', 'matrices', 'abstract-algebra', 'linear-algebra', 'matrix-equations']"
3630288,Solve $\lfloor{\sin x}\rfloor+\lfloor{\cos x}\rfloor=2^{1-|\sin x|}$,"Solve $$\lfloor{\sin x}\rfloor+\lfloor{\cos x}\rfloor=2^{1-|\sin x|}$$ Now, what I did notice is that $\lfloor{\sin x}\rfloor$ can only have three different values, and the same goes for $\lfloor{\cos x}\rfloor$ . Moreover, if either is $1$ then the other is $0$ , and $|\sin x|$ can only have one value. I am still not sure what are the solutions. Any help will be appreciated!","['trigonometry', 'functions', 'ceiling-and-floor-functions']"
3630291,Asymptotic behaviour of recurrence relation,"I have the following recurrence relation $$u_0=1, u_1=5$$ $$n^3u_n-(34n^3-51n^2+27n-5)u_{n-1}+(n-1)^3u_{n-2}=0, \forall n \geq 2$$ for which I know that $$b_n=\sum_{k=0}^n \binom{n}{k}^2\binom{n+k}{k}^2$$ is the solution. How can I prove that there exists a positive constant $A$ such that $b_n \sim A \alpha^n n^{-3/2}$ where $\alpha$ is the greatest root of the polynomial $x^2-34x+1$ ?","['combinatorics', 'recurrence-relations', 'asymptotics']"
3630336,"Probability of $X > Y$ given that $X, Y$ are i.i.d. continuous r.v.s","Can someone please explain why is $P(X > Y) = \frac{1}{2}$ , when $X, Y$ are i.i.d. continuous random variables? I have seen people use the symmetry argument to justify this answer. The argument goes as follows: there are two ways of arranging two numbers $x$ and $y$ , and out of these arrangements only one has $x > y$ . So, the probability using symmetry is $0.5$ . I don't understand this conclusion. Doesn't this argument make an assumption that the values of $X$ and $Y$ that are drawn are not equal? To take a more concrete example, if we assume $X, Y$ are standard normal, wouldn't the sample space be divided into three events: $X > Y, X < Y, X = Y$ ? Based on this, we can say that $X > Y$ and $X < Y$ must be equal using symmetry, and let's call this value $\alpha$ . So, $2\alpha + P(X=Y) = 1$ . Clearly, $\alpha < 0.5$ , contrary to the first argument. So, which is the correct argument and why? Edit: based on the comments I am adding the calculations for $X=Y$ in case of normal distributions. Can someone point out the mistake? Thanks! $P(X=Y) = \int_{-\infty}^{\infty} P(X=Y|Y=y) P(Y=y) dy$ $P(X=Y) = \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{-y^{2}} dy$ $P(X=Y) = \frac{1}{2\sqrt\pi}$ . Intuitively, the symmetry argument feels like an approximation (probably a very good one). Imagine we have a bivariate normal distribution, which is formed using $X$ and $Y$ . $P(X>Y)$ represents the region below the line $X=Y$ (in the 1st quadrant). Similarly, we can argue about the values in the other 3 quadrants. By geometry, the area of line is zero (because the line has no width?), and hence you arrive at the 1D analogy that the probability it takes a specific value is zero. Still not sure though why it doesn't show up in the calculations above. Edit: I realize the mistake I made in the above calculations. Going from step 1 to step 2, when I replaced $P(X=Y|Y=y)$ with $f_{X}(y)$ , this is wrong. As pointed out in the comments and answer below, this must be equal to zero. I confused (/abused) the notation for the discrete and continuous cases. Thanks everyone for an interesting discussion.","['probability-distributions', 'normal-distribution', 'probability']"
3630357,Determine hollow marble with least number of weighings,"We have 135 boxes each containing 100 marbles which look identical and weigh 10 grams, with the exception of one box, which contains hollow marbles of 9 grams each.
  By using a scale that can weight up to 999 grams, what is the least number of weightings required to determine the box with the hollow marbles? Well, I know the method of numbering the boxes and then taking 1 marble from box 1, 2 from box 2 etc and then compare the result with $\sum_{k=1}^{135} k*10$ ,
but this is 91800, so we would roughly need 100 weighings!
Maybe we can split the 135 boxes into two groups, 68+67, then from the first group weigh one of each, to see if the scale displays 680 or 679, then continue with either group which contains the hollow marble and do the same? 2nd weighing: 34 or 33 3rd: 17 or 17 And then we can apply the 1+2+3… method to determine in which of the two groups 9 or 8 are the hollow marbles. But this way we will need 5 weighings, which I doubt is the optimum method. Any help please?","['puzzle', 'combinatorics']"
3630370,"Prove from first principles that if $x_n \rightarrow x$ as $n \rightarrow \infty$, then $|x_n| \rightarrow |x|$.","I'm having trouble proving the limit of the above statement using first principles. Here is what I got so far, any tips you might have on continuing from here, would be much appreciated! We are given $x_n \rightarrow x$ as $n \rightarrow \infty$ . $$n \geq N \rightarrow \big||x_n|-|x|\big| < \epsilon $$ Proving $ \big||x_n|-|x|\big| \leq|x_n - x|$ . $$ \big||x_n|-|x|\big| \leq |x_n-x| \iff\big||x_n|-|x|\big|^2 \leq|x_n-x|^2$$ $$\iff x_n^2 - 2|x_nx|+x^2 \leq  x_n^2 - 2x_nx + x^2 \iff x_nx \leq |x_nx| $$ Which is always true. Therefore... $$\big||x_n|-|x|\big| \leq |x_n - x| < \epsilon$$ I'm not sure what to do after this as I have not practised much with a generalised sequences.","['absolute-value', 'real-analysis', 'solution-verification', 'sequences-and-series', 'limits']"
3630414,Find another expression for the formula,"Let there be a collection of sets $\{A_{1},...,A_{m}\}$ , where $A_{i}\subseteq\{1,2,3,...,n\}$ for every $i$ . Also let $\beta(J)=\begin{cases} 1 &\text{when }  |\bigcup_{i\in J}A_{i}|\ge s\\0 &\text{otherwise } \end{cases}$ Where $s$ is some natural number. Find a better formula for the following expression $$\sum_{i=1}^{m}(-1)^{i-1}\sum_{I\subseteq [m], |I|=i}\beta(I)$$ Thank you in advance","['inclusion-exclusion', 'combinatorics']"
3630441,Counting the number of triangles inside $3-4-5-$triangle [Found in Arabic Math book: الرياضيات | هندسة الإحداثيات | الإحصاء],"While reading a pdf Arabic math book, counting chapter, I found this question: It says: The points $(0,0),(0,3),(4,0)$ are jointed to each other. Also, the
points: $(0,1),(0,2),(0.8,2.4),(1,0),(1.6,1.8),(2,0),(2.4,1.2),(3,0),(3.2,0.6)$ are jointed to each other and to the vertices of the $3-4-5-$ triangle.
What is the total number of triangle? (Note: All triangles must be
considered). I tried to use simple formulas of counting triangles in simple shapes, like the big triangle is divided by joining a straight line from a vertex to the opposite side, we just count the number of bases on the divided side, we apply the formula $N=n(n+1)/2$ . Also for adjacent equilateral triangles we can use the formula $N=n(n+2)(2n+2)/8$ and then we round down, where $n$ is the number triangles on one side of the big one, .... and some other simple shapes. I tried to combine some of the together, but noway. What I knew about the given points is to make fixed total number of triangles. Moving a point slightly may change the answer. THERE ARE SMALL TRIANGLES! But this one is so complected, and without calculation, I think the total number of triangles is so large number. Maybe it is okay to keep the answer in a form containing factorials or $^aC_b$ or or $^aP_b$ such forms. I am not sure how to begin. If the vertices of the triangles that to be counted lie on the boundaries of the $3-4-5-$ triangle, then this is: $$^{12}C_3-^6C_3-^5C_3-^4C_3=186$$ But this is not the case, the required is to find the total number of possible triangles in the figure. Note: listing the coordinates implies an interest in the tiny triangles. Also, note that: because of these particular given coordinates, we have some intersection points of $3$ lines, and some of only $2$ lines, resulting some tiny triangles to be considered. EDIT: Here is a big figure, I used desmos to make it: Any help would be really appreciated. THANKS! I am still looking for a purely mathematical way to solve this problem, not by software. Noting that the coordinates are mentioned because they may for a very tiny triangles to be considered in counting. Also with those coordinates, three lines may meet at a single point, in that case we need to carful not to count a triangle. See the following image, taken from an answer to this question, by Mathematica. It may give you a better idea of what the question is.","['coordinate-systems', 'combinations', 'combinatorics', 'geometry']"
3630446,Energy method in the Wave equation,"Let $U\in\mathbb{R}^n$ be open, bounded, and connected, with a smooth boundary $\partial U$ . Suppose that $u=u(x,t)$ is a smooth solution of the initial-boundary-value problem. $$
\left\{ 
\begin{aligned}
u_{tt} - \Delta u + u^3 &=0\quad~~~~~~ \text{in}~ U\times (0,T]\\
u&=0\quad~~~~~~\text{on}~\partial U \times [0,T]\\
u(x,0)=0,\quad u_t(x,0)&=h(x)\quad~\text{in}~U\times\{t=0\}
\end{aligned}
\right.
$$ Show that for each $t>0$ , $$
\int\limits_{U} \frac{1}{2}\left[u_{t}(x, t)\right]^{2}+\frac{1}{2}|D u(x, t)|^{2}\, d x \leq \int\limits_{U} \frac{1}{2}[h(x)]^{2}\, d x.
$$ My Attempt: Suppose $E(t)=\int\limits_{U} \frac{1}{2}\left[u_{t}(x, t)\right]^{2}+\frac{1}{2}|D u(x, t)|^{2}\, d x$ . Then by using the Greens Identity we can get it to $\frac{d}{dt}E(t)=\int\limits_{U}u_t(u_{tt}-\Delta u)dx=-\int\limits_{U}u_tu^3$ . I was trying to show that this derivative is negative. so that $E(t)$ is decreasing. Hence $E(t)\leq E(0)$ that gives the answer. But I don't see how should I prove that the derivative is negative. Am I doing something wrong?","['multivariable-calculus', 'partial-differential-equations']"
3630494,Derivative parameric equation,"I would like to compute the derivative of the following parametric equations w.r.t $a$ and $b$ : $x=a~ \text{cos}(t)$ and $y= b~ \text{sin}(t)$ with $t \in [0, b]$ . Derivative w.r.t $a$ are easy to compute : $d_a x = \text{cos}(t)$ , $d_a y = 0$ with $t \in [0, b]$ . However, the ones w.r.t $b$ are somewhat not intuitive since $t$ depends on $b$ . For example, if $t=b$ then $d_b x =-ab \text{sin}(b)$ I would appreciate if someone has insight on how to compute these derivatives. Thank you. Reda E.","['parametric', 'derivatives']"
3630582,What can the range of a measure be?,"Given a measure space $(X,\mathcal{A},\mu)$ , what can the range of the measure, $\mu[\mathcal{A}]$ , look like? Clearly it can't be an arbitrary subset of $[0,\infty]$ as we know $0\in \mu[\mathcal{A}]$ . We also know $\mu[\mathcal{A}]$ has a maximal element ( $\mu(X)$ ). A bit less trivially, it must also satisfy the following for any $x,y\in \mu[\mathcal{A}]$ : $\exists z\ \{z,x-z,y-z,x+y-z,x+y-2z\}\subseteq\mu[\mathcal{A}]$ ( $z$ corresponds to the measure of the intersection of the sets $x$ and $y$ correspond to). This for instance tells us $\mu[\mathcal{A}]\ne \{0,1,3\}$ . Additionally, a fact that as far as I can tell is independent of the above comes from measuring the complement of a set:
if $x\in\mu[\mathcal{A}]$ then $M-x\in\mu[\mathcal{A}]$ , where $M$ is the unique maximal element of $\mu[\mathcal{A}]$ corresponding to $\mu(X)$ . Is any complete characterization known? edit: for example, at first I thought it might have to be closed, as every natural measure I could think had closed range. But the range of measure on $\mathbb{N}$ generated by $\mu(\{0\})=0.9$ , $\mu(\{1\})=0.99$ , $\mu(\{2\})=0.999$ ... has a sequence of elements approaching 1, but does not contain 1 as any singleton set has measure less than 1 and any two-or-more element set has measure greater than 1.",['measure-theory']
3630583,What does the index-set mean?,"I encountered this definition of a set from an introductory abstract algebra textbook, I'm confused on what “index-set” means it says: ""We say that a set $I$ is an “Index-set” for a collection of sets $\mathcal{A}$ if for any $\mathcal{a}\in{I}$ , there exists a set $A_{\mathcal{a}}\in{\mathcal{A}}$ and $\mathcal{A}=\{A_{\mathcal{a}}$ | $\mathcal{a}\in{I}\}$ . $(I)$ can be any non-empty set, finite or infinite”. From my own understanding $A_{\mathcal{a}}$ acts like, an small-set of ( $\mathcal{a}$ ) in a bigger set $\mathcal{A}$ . Am i right?","['elementary-set-theory', 'abstract-algebra']"
3630603,Triple Integral to find volume of solid [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Can someone tell me what the bounds of the triple integral would be? I am confused as to how to start the problem. **The previous integral I had was wrong so I am editing the post.
I now think the integral would go from 0 to 2 for the outermost integral dx, then 0 to 2 for dy, and 0 to square root of y for dz the innermost integral.","['integration', 'multivariable-calculus', 'volume']"
3630614,Find the value of $a$ where $a^x = \frac{\log(x)}{\log(a)}$ has only one solution,I have those two functions $f(x) = a^x$ $g(x) = \frac{\log(x)}{\log(a)}$ Where $g(x)$ is symmetric to $f(x)$ w.r.t. the $y=x$ axis. With $a = 1.3$ we have two solutions to $f(x) = g(x)$ : With a = 1.5 we have no solutions to $f(x) = g(x)$ : So there is a value $a$ (between 1.3 and 1.5) where $f(x) = g(x)$ admit one and only one solution . Graphically I've noticed that those value should be close to 1.4446 but I cannot find a more precise answer. I don't think that it is possible to isolate $x$ in the equation $f(x) = g(x)$ so how could I determine this value ?,"['limits', 'exponential-function']"
3630624,Evaluating $\sum_{r=1}^{3n-1}\dfrac{(-1)^{r-1}\cdot r}{\binom{3n}r}$,"$$\sum_{r = 1}^{3n-1}\left(-1\right)^{r - 1}\,\,\dfrac{r}{{3n \choose r}},\quad n \in 2k,\ k\in \mathbb{Z^+}$$ Answer given (much simpler than expected) $\dfrac{3n}{3n+2}$ I tried adding and subtracting 1 to $r$ so could use $\dfrac{\binom{n}r}{r+1}=\dfrac{\binom{n+1}{r+1}}{n+1}$ , but didn't prove to be useful. I know summation and double summation of binomial coefficients to quite to good extent. If you could help...","['summation', 'binomial-coefficients', 'linear-algebra', 'binomial-theorem']"
3630630,I don't understand why we can just extend the domain of this function,"The problem : Let $$g(x)= \left\{
\begin{matrix}
x & , \, 0 \leq x \leq 1/2\\
1-x& , \, 1/2 \leq x \leq 1
\end{matrix} \right .$$ Prove that $$S_n(x)=\sum_{k=0}^{n} \frac{g(2^kx)}{2^k}$$ converges uniformly to a continous function $f$ . My solution : Notice that $g(x)$ exist only when $x\in [0,1]$ . Since $2^n \to \infty$ when $n \to \infty$ , if $x>0$ then $2^nx >1$ for sufficiently large $n$ , therefore for $x=0, \ \{S_n(x)\}$ converges to $f(x)=0$ , and for any other value of $x, \ S_n(x)$ doesn't exist when $n\to \infty$ , so $f$ is undefined. Since the uniform converges definition is true for all $x\in\{0\}$ and since $x=0$ is not an accumulation point of $f, \ \{S_n(x)\}$ converges uniformly to $f$ , which is continous. Teacher's solution : Extend the domain of $g$ such that $g(x)=g(x+1)$ , then $g(x)\leq 1/2, \ \forall x\in \mathbb{R}$ , so $$\frac{g(2^nx)}{2^n} < \left(\frac{1}{2}\right)^n$$ By Weiertrass' M Criterion, $\{S_n(x)\}$ converges uniformly to a function $f$ over $\mathbb{R}$ and since every $g(2^nx)/2^n$ is continous, so is $f$ Now, I don't understand why extending the domain of $g$ is valid here, isn't it like adding an extra condition that the problem doens't give?","['calculus', 'uniform-convergence', 'sequences-and-series']"
3630647,A possible proof that the affine line with double origin is not affine,"I'm trying to prove that the affine line with double origin is not an affine scheme. Consider $X_1:=\text{Spec}(k[x])$ and the open subset $U_1:=X_1\setminus\{(x)\}$ . In particular $\mathcal{O}_{X_1}(U_1)=k[x]_x$ . Similarly, $X_2:=\text{Spec}(k[y])$ , $U_2:=X_2\setminus\{(y)\}$ , so that $\mathcal{O}_{X_2}(U_2)=k[y]_y$ . The ring isomorphism $k[x]_x\to k[y]_y$ induces an isomorphism $\varphi:U_1\stackrel{\sim}{\to} U_2$ . We define the line with double origin as the scheme $$X=X_1\sqcup_\varphi X_2,$$ i.e. the gluing of $X_1$ and $X_2$ along $U_1\simeq U_2$ via $\varphi$ . Here is my strategy: if we remove one of the origins from $X$ , say $(x)$ , we get the usual line $\text{Spec}(k[y])$ , whose set of global sections is $k[y]$ . On the other hand, if $X$ is affine, i.e. $X\simeq \text{Spec}(A)$ for some ring $A$ , then the closed point $(x)$ corresponds to a maximal ideal $\mathfrak{m}\subset A$ . Hence the removal of $(x)$ gives the scheme $\text{Spec}(A)\setminus\{\mathfrak{m}\}$ , which should look like a line without an origin. I hope that I can prove that $\mathcal{O}_{\text{Spec}(A)}(\text{Spec}(A)\setminus\{\mathfrak{m}\})\not\simeq k[y]$ , but I don't know how to do it.","['affine-schemes', 'algebraic-geometry', 'schemes']"
3630663,Zero Divisors and Associated Primes of the zero ideal in a Noetherian ring,"I have the following question: Let $P_1, \dots, P_k$ be the associated prime ideals of the zero ideal in the Noetherian ring $R$ . Show that $P_1 \cup \dots \cup P_k$ is the set of zero divisors in $R$ . I'm denoting the zero ideal $(0)$ , and the set of zero divisors $Z_R$ . Let $(0) = Q_1 \cap \dots \cap Q_n$ be a primary decomposition, so that $P_i$ is the associated prime for $Q_i$ , i.e. $P_i = \mbox{Rad}(Q_i)$ . Clearly, we want to show both inclusions. First, let $x \in Z_R$ , then $xy = 0$ for some $y \in R$ . Then, $xy \in (0) \implies xy \in Q_1 \cap \cdots \cap Q_k \implies xy \in P_1 \cap  \cdots \cap P_k$ . Since each $P_i$ is prime, $xy \in P_i \implies x \in P_i$ or $y \in P_i$ . However, this is where I get stuck, since I need explicitly that $x \in P_i$ . For the other inclusion, I'm not quite sure where to start. If we let $z \in P_1 \cup \cdots \cup P_k$ , then $z \in P_i$ for some $i$ . Then, $z^m \in Q_i$ for some power $m$ . But I don't know how to proceed from here. Any help would be appreciated.","['abstract-algebra', 'maximal-and-prime-ideals', 'commutative-algebra']"
3630676,Is this proof of Markov's inequality correct?,"Markov's inequality states: Let $X$ be a non-negative random variable and suppose that $\mathbb{E}(X)$ exists. For any $t > 0$ : $$\mathbb{P}(X > t) \leq \frac{\mathbb{E}(X)}{t}$$ My text contains the following proof: Since $X > 0$ , $$ \begin{align} \mathbb{E}(X) &= \int_0^{\infty} xf_X(x)dx \\ &=
\int_0^{t} xf_X(x)dx + \int_t^{\infty} xf_X(x)dx \\ &\geq
\int_t^{\infty} xf_X(x)dx \\ &\geq t\int_t^{\infty} f_X(x)dx \\ &=
t\mathbb{P}(X > t) \end{align} $$ My concern is the step where we remove $x$ from the integral. I think the assumption is that since we know $x$ is non-negative, that removing multiplication by $x$ can only make things smaller. However, for $0 < x < 1$ removing multiplication by $x$ should actually make them bigger. If $f_X$ only has density in that range, then I don't think you can say that $\int_t^{\infty} xf_X(x)dx \geq \int_t^{\infty} f_X(x)dx$ . Unless somehow multiplying by $t$ at the same time gets rid of this problem?","['statistics', 'solution-verification', 'probability', 'inequality']"
3630758,Begging the question in a proof,"I've found in my math textbook a proof of De Morgan's law but I think it's begging the question. Here's the proof: $$ \begin{aligned} x\in(A\cap B)^c&\iff x\notin A\cap B \\
&\iff x\notin A \text{ or } x\notin B \\
&\iff x\in A^c \text{ or } x\in B^c \\
&\iff x\in A^c\cup B^c
\end{aligned}$$ Thus: $(A\cap B)^c=A^c\cup B^c$ Am I right? If not, why?","['elementary-set-theory', 'logic']"
3630761,Is there a name for the matrix operation $ABA^t$,"I know in group theory, the operation $ABA^{-1}$ , i.e. the element A multiplied by the element B, and then multiplied by the inverse of A, is called conjugation. When dealing with matrices, there is a similar operation that happens frequently: $ABA^{t}$ , where instead of multiplying by the inverse of A, you multiply by the transpose of A. Does this operation or the resulting matrix have a name?","['matrices', 'abstract-algebra', 'linear-algebra']"
3630804,Recurrence Relation: Hᵥ = 2ᵛ - Hᵥ₋₁,"This problem has been giving me a headache for days. My teacher has taught us the plug-and-chug method of working these problems out and eventually finding a closed form.
However the "" - Hᵥ₋₁ "" is really throwing me off. My Plug and chug comes out like this: H₀ = 1 ; H₁ = 2¹ - 1 = 1 ; H₂ = 2² - (2¹ - 1) = 2² - 2¹ + 1 = 3 ; H₃ = 2³ - (2² - 2¹ + 1) = 2³ - 2² + 2¹ - 1 = 5 ...... I've worked out and been staring at this equation for hours: Hᵥ = 2ᵛ - 2^(v-1) + 2^(v-2) - 2^(v-3) + ... + ((-1)^(v-1))2 + (-1)ᵛ I know the formula that 1 + r + r² + r³ + ... + rᵛ = (r^(v+1) - 1)/(r - 1) and I've tried to manipulate it to fit my problem but I just cannot figure it out. Please someone help. Anything is appreciated.","['recurrence-relations', 'closed-form', 'discrete-mathematics', 'recursion']"
3630820,Are there any tricks to find an orthonormal basis for polynomials by hand?,"I am taking a Numerical Analysis class. On a previous exam, there is a question which involved finding an orthonormal basis for $\mathbb{P}_2$ w.r.t the inner product $(u, v) = \int_0^2 u(t)v(t) dt.$ I did this with Gram-Schmidt, and it took me about $15-20$ minutes (and this was only half a question!). Plus I of course made arithmetic mistakes, and I tired myself out working at $100 \%$ speed. Along the way, I had to solve integrals such as $$\int_0^2 \left((x-\frac 34)^2 - \frac {21}{16} \right)^2 dx$$ which take long and are very prone to mistakes. The only thing I could think of was to make a u substitution, $u = x-3/4$ . I am wondering: I applied Gram-Schmidt to $\{1, x, x^2 \}$ ; is this the way to go, or is there a better way? (I have seen the 3-term reccurence, but it seems like a lot of memorization, and seems to involve a lot of integrals too). Also, when I do Gram-Schmidt, I orthogonalize, and then normalize everything at the end. Is it better to normalize along the way? Lastly, if Gram-Schmidt is the way to go, would you have any advice on how to quickly evaluate these types of integrals that come out of Gram-Schmidt? (integrals of polynomials of low degree, often squared). Thank you very much, I greatly appreciate any help.","['integration', 'real-analysis', 'calculus', 'linear-algebra', 'numerical-methods']"
3630828,Relation of row sums to largest eigenvalue,"I know that the largest eigenvalue of a graph is bounded between the minimal and maximal row sum of the matrix. If I have a $0-1$ symetric matrix (an adjacency matrix) and I know $k$ of the rows have at least $k$ zeros in them (more specifically, I know $k$ is the maximum size of an all zeros principal sub-matrix of the adjacency matrix), is there anything else I can tell about the largest eigenvalue (or others eigenvalues)? Is there anything I can tell about the eigenvalues which implies I have that kind of principal sub-matrix? Thanks!","['matrices', 'linear-algebra', 'spectral-graph-theory', 'eigenvalues-eigenvectors']"
3630842,"Let $T,U:V\to W$ be linear transformations. Prove that if $W$ is finite-dimensional, then $\text{rank}(T+U)\leq\text{rank}(T) + \text{rank}(U)$.","Let $T,U:V\to W$ be linear transformations. (a) Prove that $R(T+U)\subseteq R(T) + R(U)$ . (b) Prove that if $W$ is finite-dimensional, then $\text{rank}(T+U)\leq\text{rank}(T) + \text{rank}(U)$ . MY ATTEMPT (a) If $w\in R(T+U)$ , there exists $v\in V$ such that $w = (T+U)v = T(v) + U(v)$ . But $T(v)\in R(T)$ and $U(v)\in R(U)$ . Thus $w\in R(T) + R(U)$ , from whence we conclude that $R(T+U)\subseteq R(T)+R(U)$ . (b) Since $R(T + U)$ is a linear subspace of $R(T)+R(U)$ , we have that \begin{align*}
\dim R(T+U) & \leq \dim(R(T) + R(U)) = \dim R(T) + \dim R(U) - \dim(R(T)\cap R(U))\\\\
& \leq \dim R(T) + \dim R(U)
\end{align*} Could someone verify if I am not doing any conceptual mistake?","['matrix-rank', 'solution-verification', 'linear-algebra']"
3630867,"Commuting second-order partial derivatives at some point, but exactly one of them is continuous at the point","Question. Does there exist a function $f(x,y)$ and a point $(x_0,y_0)$ such that: Both $f_{xy}$ and $f_{yx}$ exist in a neighbourhood of $(x_0,y_0)$ . $f_{xy}(x_0, y_0)=f_{yx}(x_0, y_0)$ . $f_{xy}$ is continuous at $(x_0,y_0)$ . $f_{yx}$ is not continuous at $(x_0,y_0)$ . My thought. Since there are functions $g(x,y)$ with $g_{xy}(0,0)=a\ne b= g_{yx}(0,0)$ (e.g. see this post ), a possible way would be to construct a sequence of disjoint balls $B_n$ centered at $(1/n,1/n)$ with radius $1/(n+1)^3$ , then smoothly restrict $g(x-1/n, y-1/n)$ in $B_n$ to get a function $$h_{n}(x,y)=\begin{cases} g(x-1/n,y-1/n) & \text{in a neighbourhood of}~ (1/n,1/n) \\ 0, &\text{outside}~B_n\end{cases}$$ and finally let $f(x,y)=\sum_{n\geq 1} h_n(x,y)$ . In this way $$f_{xy}(1/n,1/n)=(h_n)_{xy}(1/n,1/n)\to a \quad\text{and}\quad f_{yx}(1/n,1/n)=(h_n)_{yx}(1/n,1/n)\to b $$ and the requirement 4 is met so long as $f_{xy}(0,0)=f_{yx}(0,0)=a$ . But as to fulfill requirement 3, I cannot prove $f_{xy}(x_n,y_n)\to a$ when $(x_n,y_n)$ tends to $(0,0)$ arbitrarily, not just taking on points $(1/n,1/n)$ .","['partial-derivative', 'real-analysis']"
3630871,Remembering Riemann-Roch,"Embarrassingly, I've always struggled to remember the form of the Riemann-Roch theorem for curves. Does anyone have any intuition to share about how to remember the some of the terms in the formula? Recall that for $C$ a Riemann surface and $D$ a divisor on $C$ , the Riemann-Roch theorem says that: \begin{equation}
h^0(D) - h^0(K-D) = \mathrm{deg}(D) + 1  - g
\end{equation} where $K$ is the canonical divisor on $C$ . I'm happy with the interpretation of the terms on the left hand side (it's some kind of Euler characteristic), but is anyone able to give an informal explanation for the quantity on the right hand side? Why $\mathrm{deg}(D) + 1  - g$ ? In particular, why should I expect the left hand side to grow like $\mathrm{deg}(D)$ , with a correction of $1-g$ ? I understand that there's some very classical way to think about this, but I've never seen it explained anywhere.","['algebraic-curves', 'riemann-surfaces', 'complex-geometry', 'algebraic-geometry', 'line-bundles']"
3630884,$\int\limits_{0}^tu_tu^3d\tau=\frac{1}{4}u^4$,"I came across this statement(which I don't understand) when reading a solution to a different problem. I think I'm missing a small point here but I don't see it. Can you please help me with an explanation. Let $U\in\mathbb{R}^n$ be open. Suppose that $u=u(x,t)$ is a differentiable function from $U\times [0,\infty)\to\mathbb{R}$ with $u(x,0)=0$ Then $\int\limits_{0}^tu_tu^3d\tau=\frac{1}{4}u^4$ where $u_t$ is the derivative with respect to $t\in[0,\infty)$","['multivariable-calculus', 'calculus']"
3630892,Stokes Theorem: manifolds vs. chains,"So, reading both Baby Rudin (Principles of Mathematical Analysis) and Munkres (Analysis on Manifolds), I start to wonder the difference between this two approaches. In Rudin's, he defines the Stokes Theorem in chains: Let $\Psi$ a $k$ -chain of class $C''$ in an open set $V \subset \mathbb{R}^n$ and $\omega$ a $(k - 1)$ -form of class $C'$ in $V$ . Then: $$\int_{\Psi} d\omega = \int_{\partial \Psi} \omega.$$ While in Munkres, the definition is on a oriented manifold: Let $k>1$ and let $M$ be a compact oriented $k$ -manifold in $\mathbb{R}^n$ ; give $\partial M$ the induced orientation if $\partial M \neq \emptyset$ .
Let $\omega$ be a $(k - 1)$ -form defined in an open set of $\mathbb{R}^n$ containing $M$ . Then, if $\partial M = \emptyset$ , $$\int_{\partial M} \omega = 0.$$ If $\partial M\neq\emptyset$ , $$\int_M d\omega = \int_{\partial M} \omega.$$ I don't have the mathematical maturity yet to understand both statements completely (I have a litle knowledge of what a manifold and a differential form is, but I don't know very well what a chain happens to be), but, in case of future studies, what is the difference between these? Which one is more general? Searching about integration on chains, I came across some definitions of algebraic topology, such as homology and simplexes. What is the relation between the Stokes theorem and these concepts? Sorry about the long post and such ""disjoint"" questions, but this different approaches poked up my curiosity. Thanks in advance!","['analysis', 'stokes-theorem', 'manifolds', 'differential-forms', 'differential-geometry']"
3630920,Smooth morphism of relative dimension $>0$ and fiber bundle,"Let $f:X\to Y$ be a smooth morphisms of relative dimension $>0$ of two smooth (affine) varieties over $\mathbb{C}$ . I wonder if the corresponding holomorphic map of the corresponding complex manifolds can be a fiber bundle, i.e. the holomorphic map $f_{an}:X_{an}\to Y_{an}$ is a fiber bundle. I know $f$ is a submersion thus $f_{an}$ is a submersion too. But by an exercise of Hartshorne, a proper map between affine varieties is finite. In my case, $f,f_{an}$ are not finite, which implies $f$ is not proper, so it seems (I am not very sure) $f_{an}$ is not proper. But by Ehresmann's lemma: a proper surjective submersion is a locally trivial fibration. I wonder if it would make $f_{an}:X_{an}\to Y_{an}$ not a fiber bundle?","['affine-varieties', 'algebraic-geometry']"
3630952,Simplifying $\frac{ \cos^2 x - \sin^2 x }{\sin{2x}}$,Please consider the following problem and my answer to it: Problem: Simplify the following expression: $$ \frac{ \cos^2 x - \sin^2 x }{\sin{2x}} $$ Answer: $$ \frac{ \cos^2 x - \sin^2 x }{\sin{2x}} = \frac{ \cos^2 x - \sin^2 x }{ 2 \sin x \cos x} $$ $$ \frac{ \cos^2 x - \sin^2 x }{\sin{2x}} = \frac{\cos x}{2 \sin x} - \frac{\sin x}{2 \cos x} $$ The result is not much simpler than what I started with. Is there an additional simplification that can be made? Am I missing something?,['trigonometry']
3630967,Is the Gaussian density Lipschitz continuous?,"More precisely, define $\phi(x) = \frac1{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ . Does there exists a constant $L$ such that $$|\phi(x)-\phi(y)|\le L|x-y|,$$ for all $x,y\in \mathbb{R}$ . If yes, what is the minimal $L$ ?","['calculus', 'lipschitz-functions', 'probability', 'real-analysis']"
3631020,A problem about Riesz's Lemma,"I am thinking in this problem: If $E$ is a normed space and $M\subseteq E$ is a subspace of finite dimension, prove that for all $x\in E-M$ there exists $m_0\in M$ such that $d(x,M)=\|x-m_0\|$ . I am trying to apply Riesz's Theorem because $M$ is a closed space (finite dimensional), but I don't know how....","['functional-analysis', 'metric-spaces']"
3631046,Proof of Supermartingale Convergence Theorem,"There is a supermartingale convergence theorem which is often cited in texts which use Stochastic Approximation Theory and Reinforcement Learning, the theorem is: ""Let $Y_t, X_t, Z_t, t = 1,2,3,....$ be three sequences of random variables and let $\mathcal{F_t}$ be sets of random variables such that $\mathcal{F_t} \subset \mathcal{F_{t+1}}$ for all t, suppose that: (a) The random variables $Y_t, X_t, Z_t$ are nonnegative and are functions of the random variables in $\mathcal{F}_t$ (b) For each $t$ we have $E[Y_{t+1}|\mathcal{F_t}] \leq Y_t - X_t +Z_t$ (c) $\sum_{t=0}^\infty Z_t \lt \infty$ Then: $\sum_{t=0}^{\infty}X_t \lt \infty $ and there exists a nonnegative random variable $Y$ such that $Y_t \rightarrow Y$ with probability 1.
"" The problem is, I can't find any proofs of this anywhere. Most texts on probability theory prove the standard Martingale Convergence Theorem but I feel this is a big step from that. Can anyone direct me to a source which proves the above or write out a proof from the standard convergence theorem?","['stochastic-processes', 'markov-process', 'martingales', 'convergence-divergence', 'probability-theory']"
3631102,Periodic points of hyperbolic toral automorphism are dense on the torus,"I am having a difficult time following the proof of Proposition 4.2 (see image below) from Devaney's An Introduction to Chaotic Dynamical Systems (2e) on p. 192. Now, from topology I know that a subset $A \subseteq X$ of a topological space $(X, \tau)$ is dense in $X$ iff. the closure $\bar{A}$ of $A$ is equal the space $X$ , i.e. $\bar{A} = X$ .  And the closure is the set of all contact\adherent points and the closure can also be expressed as the union of all points in $A$ and the set of all limit points of $A$ (the derived set). But even knowing this I cannot follow the proof below at all. What I particularly do no understand: (i) We have to prove $\overline{Per(L_{A})} = T$ . In other words, $ \overline{Per(L_{A})} \subseteq T$ and $ T \subseteq \overline{Per(L_{a})}$ , right? So it seems that the proof does $ T \subseteq \overline{Per(L_{a})}$ but not $ \overline{Per(L_{a})} \subseteq T$ . Why? (ii) So in showing $ T \subseteq \overline{Per(L_{a})}$ , the proof assumes $p \in T$ and then it shows $p \in \overline{Per(L_{a})}$ i.e. that $p$ is an adherent point of $Per(L_{a})$ . How is this then equivalent to showing that $p$ is a periodic point of $L_{a}$ ? Also why do we choose $p \in T$ to have rational coordinates? And why is the phrase in the proof ""Such points are clearly dense in T, for we may take $k$ arbitrarily large""? Is it because $\mathbb{Q}$ is dense in $\mathbb{R}$ and then $\mathbb{Q} \times \mathbb{Q}$ is dense in $\mathbb{R} \times \mathbb{R}$ ? (iii) And then I am completely lost on the last paragraph of the proof where it is shown that $p$ is actually periodic with period less than or equal to $k^{2}$ . For clarification of the notation used in the proposition. $L_{A}$ is the hyperbolic toral automorphism defined by: Let $L(x) = A \cdot x$ where $A$ is a $2 \times 2$ matrix satisfying (i) All entries are integers; (ii) $\det(A) = \pm 1$ ; $A$ is hyperbolic, meaning that none of its eigenvalues have absolute value one. The map induced on $T$ by $A$ is called a hyperbolic toral automorphism and is denoted by $L_{A}$ . The $2$ -torus $T$ is defined setting $T$ as the set of all equivalence classes of all points in the plane whose coordinates differ by integers. Formally, let $T$ be the set of all equivalence classes under the equivalence relation $\sim \subseteq \mathbb{R}^{2} \times \mathbb{R}^{2}$ defined by $(x, y) \sim (x', y')$ if and only if $x - x'$ and $y - y'$ are integers.","['general-topology', 'dynamical-systems', 'real-analysis']"
3631105,Intuitive/combinatorial explanation of Delannoy summand,"The $n$ th central Delannoy number $D_n$ is the number of lattice walks from $(0,0)$ to $(n,n)$ taking only steps up, right, and northeast, to neighbor lattice points. From Wikipedia: $$D_n=\sum_{k=0}^n{n \choose k}{{n+k} \choose k}.$$ I am wondering if there is an intuitive/combinatorial explanation as to why we are summing ${n \choose k}{{n+k} \choose k}$ . That is, is there an intuition behind what it is counting?","['binomial-coefficients', 'combinatorics', 'intuition']"
3631118,Shortest distance of a tetrahedron,"Show that the shortest distance between the opposite edges of a regular tetrahedron is $ \frac{1}{2} 
\sqrt{2} \alpha $ when the length of each side is $ \alpha $ . What I've done so far: Take the vertices to be $ O=(0,0,0), A =(\alpha,0,0), B=(\frac{\alpha}{2},\alpha\frac{\sqrt3}{2},0). $ Take $ \hat i $ in the direction of $ OA $ , $ OBA\ $ lying in the $ Oxy $ plane with $ \hat{j} $ in the direction of $ Oy $ and $ \hat{k} $ perpendicular to $ Oxy $ plane. Then the other vertex will have coordinates $ C=(\frac{\alpha}{2}, \frac{\alpha}{2 \sqrt{3}},\alpha) $ . The vector equation of $ BC= \frac{\alpha}{2}\hat{i}+\frac{\alpha \sqrt{3}}{2} \hat{j}+\lambda (-\frac{\sqrt{3}\alpha}{2} \hat{j} + \alpha\hat{k}). $ $ OA = \alpha \hat{i}. $ $ OA \land BC = -\alpha^2\hat{j}+ \frac{\alpha^2}{2 \sqrt{3}} \hat{j}. $ Taking the dot product of $ \frac{\alpha}{2}\hat{i}+ \frac{\alpha \sqrt{3}}{2} \hat{j} $ and this vector would not give the required answer ( $ \land $ denotes the cross product. ) Please help. Thank you in advance.","['vectors', 'geometry']"
3631244,Do sequences of full conditional probabilities converge?,"Setup. Let $\Omega$ be a set, and let $\mathcal F$ be its powerset. A full conditional probability is a function $P(\cdot \mid \cdot): \mathcal F \times \mathcal F \setminus \emptyset \to [0,1]$ that satisfies: (1) For all non-empty $F \in \mathcal F$ , $P(\cdot \mid F)$ is a finitely additive probability measure on $(\Omega, \mathcal F)$ ; (2) For all non-empty $F \in \mathcal F$ , $P(F\mid F)=1$ ; (3) For all $A,B \in \mathcal F$ and all non-empty $C \in \mathcal F$ for which $B \cap C$ is also non-empty, $$P(A \cap B \mid C) = P(B \mid C)P(A \mid B \cap C)\tag{1}.$$ If $P$ is a full conditional probability, we write $P(F)$ instead of $P(F \mid \Omega)$ . Note that if $P(B)>0$ , then $P(A \mid B) = P(A \cap B)/P(B)$ . This is obtained from (1) with $C = \Omega$ . In other words, full conditional probabilities extend the usual notion of conditional probability given an event to allow for conditioning on events with probability zero. Question. Let $F_1 \supset F_2 \supset ...$ be a decreasing sequence of non-empty subsets of $\Omega$ . Is it the case that $P(A \mid F_n)$ converges for all $A \in \mathcal F$ ? Observations. Let $F = \bigcap_n F_n$ . If $P(F)> 0$ , so that $P(A \mid F_n) = P(A \cap F_n)/P(F_n)$ , then $P(A \mid F_n)$ converges. If $P(\cdot \mid \Omega)$ is countably (and not merely finitely) additive, then the limit is equal to $P(A \mid F)$ . Without countable additivity, however, $P(A \cap F_n)$ might not converge to $P(A \cap F)$ , and $P(F_n)$ might not converge to $P(F)$ (they converge to something , though, because they are both bounded and non-increasing).","['conditional-probability', 'measure-theory', 'probability-theory']"
3631254,Covariant derivative of a metric determinant,"It is a well-known fact that the covariant derivative of a metric is zero. In a textbook, I found that the covariant derivative of a metric determinant is also zero.
I know $$
g_{\alpha \beta;\sigma}=0
$$ So, $g=\det g_{\alpha\beta}$ is a metric determinant. $g_{;\sigma}$ is a covariant derivative of a metric determinant which is equal to an ordinary derivative of $g$ . $$
g_{;\sigma}=g_{,\sigma}=g g^{\alpha\beta}g_{\alpha\beta,\sigma}
$$ My question is why this must be zero?","['tensors', 'metric-spaces', 'differential-geometry']"
3631295,Is There a Basis for a Smooth Vector Fields on the 2-Sphere?,"I've been watching Frederic Schuller's course on ""The Mathematics and Physics of Gravity and Light"" and at a moment during Lecture 6 he claims there is no basis for $\Gamma(TS^2)$ (the space of smooth sections $\sigma \colon S^2 \to TS^2$ ) when considered as a $C^\infty(S^2)$ -module and gives an example through the claim that since every vector field must vanish somewhere, you can't get a basis. During his example, he uses a vector field which vanishes at points $(\pm 1, 0, 0)$ and $(0, 0, \pm 1)$ and says that since on, say, $(0, 0, \pm 1)$ you have only one nonvanishing vector field, and thus a linear combination can't point towards a different direction. My first question is: adding a new vector field which vanishes at $(0, \pm 1, 0)$ wouldn't solve the problem? At every point you have at least two non-parallel vectors, so it seems to me it would work, at least in principle. Furthermore, every vector space admits a basis, so $\Gamma(TM)$ (for some smooth manifold $M$ ) when considered as a real vector space has a basis. Since every real number can be regarded as a constant (and hence smooth) function, wouldn't it allow us to obtain a basis for $\Gamma(TM)$ when considered as a $C^\infty(M)$ -module? I believe it is worth mentioning I do not have much background in Differential Geometry nor Module Theory.","['vector-fields', 'modules', 'differential-geometry']"
