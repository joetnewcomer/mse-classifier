question_id,title,body,tags
4839081,More on an unusual cubic representation problem,"Amateur here. I'm trying to understand how the smallest positive integer solutions (which are eighty digit numbers) for $$\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}=4$$ are calculated. This equation can be expanded out to the elliptic curve $$a^{3}+b^{3}+c^{3}-3\left(a^{2}b+a^{2}c+ab^{2}+ac^{2}+b^{2}c+bc^{2}\right)-5abc=0,$$ which can be transformed to the Weierstrass form $$y^{2}=x^{3}+109x^{2}+224x.$$ This problem for any non-zero integer $N$ (not just $N=4$ ) is presented in the paper An unusual cubic representation problem by Bremner and Macleod (B&M) here . Pitched more at my level, there is also a nice, less technical discussion for $N=4$ given by Alon Amit in his Quora answer here . On page 32, B&M give a generator for the above Weierstrass form as $G=\left(x,y\right)=\left(-4,28\right)$ ,which corresponds to $\left(a,b,c\right)=\left(11,4,-1\right)$ . They don't explain how they choose this $G$ . They then go to say: “The point $9G$ is the smallest multiple of $G$ that corresponds to a positive solution (in which $a,b,c\sim10^{80}$ ).” Now, I can understand most of this problem. I've derived the Weierstrass form from the above cubic. I've derived the equations linking $a,b,c$ and $x,y$ (page 30, B&M). Using the chord and tangent technique, I've calculated the first few values of $nG$ and, when that got too complicated, used SageMath to compute $9G$ and hence the desired positive $a,b,c$ solution. So far, so good. What I can't see is why 9G is the smallest possible solution. So my question is, how to show that there isn't another generator that gives a smaller positive solution? In general terms, Amit answers my question when he writes: “As we continue to add the point $P$ to itself, the denominators just keep growing. It is not quite easy to prove that, as there's always a possibility of some cancellation, but the theory of heights on an elliptic curve allows us to show that those astronomical numbers are, indeed, the simplest solutions to the question.” I've looked up heights online, found that $h\left(nP\right)$ is roughly $n^{2}$ times a constant, but cannot relate that to why $G=\left(x,y\right)=\left(-4,28\right)$ is the smallest generator to give the desired solution.","['number-theory', 'elliptic-curves']"
4839154,Maximum of Ornstein–Uhlenbeck process,"Suppose $X_t = e^{-t}W_{e^{2t}}$ is an Ornstein–Uhlenbeck process ( $W_t$ is a Wiener process). I'd like to show that for all $ 0 < a < b $ , $$
\mathbb{P}\Big( \max_{0 \leq t \leq a} X_t = \max_{0 \leq t \leq b} X_t\Big) = \frac{a}{b}.
$$ I know I need to use the stationarity and Markov property of the Ornstein–Uhlenbeck process, but I'm not sure how to. Any help would be much appreciated.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4839248,Proof that a certain set is not nowhere dense,"Question: Prove that the set $N=\{f\in C([0,1])|f(0)=0\}$ is a nowhere dense subset of $(C([0,1]),d_{\infty})$ , but not a nowhere dense subset of $(C([0,1]),d_{1})$ , where $d_{\infty}(f,g)=sup_{x\in[0,1]}|f(x)-g(x)|$ and $d_{1}(f,g)=\int_{0}^{1}|f(x)-g(x)|dx$ . My try: I tried my best but cannot prove the second part . The first part of the proof can be found in my lecture notes, which is the following: Proof: Let $f\in C([0,1])$ and $r>0$ . We can clearly choose $a\in\mathbb{R}$ and $\delta>0$ such that $(a-\delta,a+\delta)\subseteq (f(0)-\frac{r}{2},f(0)+\frac{r}{2})\backslash\{0\}$ . Note that $|a-f(0)|\leq\frac{r}{2}$ and $\delta \leq \frac{r}{2}$ . Let $g\in C([0,1])$ denote the continuous function defined by $g=f+a-f(0)$ . We claim that $B(g,\delta)\subseteq B(f,r)$ . Indeed, choose a $h\in B(g,\delta)$ , then $d_{\infty}(f,h)\leq d_{\infty}(f,g)+d_{\infty}(g,h)\leq |a-f(0)|+\delta\leq \frac{r}{2}+\frac{r}{2}=r$ . That is, $h\in B(f,r)$ . Next, we claim that $B(g,\delta)\subseteq C([0,1])\backslash N$ . Indeed, choose a $h\in B(g,\delta)$ , then $|h(0)-a|\leq|h(0)-g(0)|+|g(0)-a|\leq d_{\infty}(h,g)+0\leq\delta$ , hence $h(0)\in (a-\delta,a+\delta)\subseteq (f(0)-\frac{r}{2},f(0)+\frac{r}{2})\backslash\{0\}$ . This means $h(0)\neq 0$ and thus $h\notin N$ .","['general-topology', 'functional-analysis', 'real-analysis']"
4839264,Expected difference between the largest and second largest observations in a sample of i.i.d. normal variables,"Let $X_1,\dots,X_n$ be an i.i.d. sample from the standard normal distribution. Let \begin{align}
\mu_n = \mathbb{E}[X_{(n)} - X_{(n-1)}], 
\end{align} be the expected difference between the largest and second largest observations. Question : Can we show that $\mu_n$ is decreasing in $n$ ? In this answer it is shown that $\mu_n$ is asymptotically decreasing, i.e., it is decreasing for large enough $n$ , and in this answer it is argued numerically that $\mu_n$ is also decreasing for finite $n$ . A good first step is to use a known order statistics formula (see e.g. this answer ) to write \begin{align}
\mu_n = n\int_{-\infty}^{\infty}\Phi^{n-1}(x)(1-\Phi(x))dx,
\end{align} where $\Phi$ is the standard normal CDF. However, I am unable to show from this that $\mu_n$ is decreasing. Addition: The intuition provided in a previously linked answer is that $\mu_n$ is decreasing if the $X_i$ 's come from a thin-tailed distributions like the normal distribution, and that $\mu_n$ is constant or even increasing if the $X_i$ 's come from a more heavy-tailed distribution (in particular, it is easy to show that $\mu_n$ is constant if the $X_i$ 's come from the exponential distribution). Perhaps this intuition can help in showing that $\mu_n$ is decreasing for the normal distribution.","['statistics', 'stochastic-processes', 'order-statistics', 'probability-theory', 'probability']"
4839301,Integer partitions appearing in the derivatives of $f(x) \log\left(f(x)\right)$,"This topic has probably been already investigated before, if not, it suggests an interesting connection between calculus and number theory , in particular integer partitions . Consider the function defined as $F(x) = f(x)\log\left(f(x)\right)$ , where $\log$ is the natural logarithm. Now, take a look at its derivatives: $$
\begin{align}
 F^{(1)}&=f'\log f+f' \\
 F^{(2)}&=f''\log f+f''+\frac{\left(f'\right)^2}{f} \\ 
 F^{(3)}&=f^{(3)}\log f+f^{(3)}-\frac{\left(f'\right)^3}{f^2}+3\frac{f'f''}{f} \\
 F^{(4)}&=f^{(4)}\log f+f^{(4)}+2\frac{\left(f'\right)^4} 
          {f^3}-6\frac{\left(f'\right)^2f''}{f^2}+3\frac{\left(f''\right)^2} 
          {f}+4\frac{f^{(3)}f'}{f} \\
 F^{(5)}&=f^{(5)}\log f+f^{(5)}-6\frac{\left(f'\right)^5}{f^4}+5\frac{f^{(4)}f'}{f}+10\frac{f^{(3)}f''}{f}-10\frac{f^{(3)}\left(f'\right)^2}{f^2}+20\frac{\left(f'\right)^3f''}{f^3}-15\frac{f'\left(f''\right)^2}{f^2} \\
\vdots
\end{align}
$$ As you can see, if we ignore the first term, where the logarithm appears, and the denominators of the fractions, for example by supposing there is an $x_0$ s.t. $f(x_0)=1$ , then it seems that $F^{(n)}(x_0)$ has terms corresponding exactly to the partitions of $n$ .
To see this, just sum up the orders of the derivatives that appear in each term with multiplicity given by their exponent, it will always result in exactly the order of the derivative you are looking at. Moreover, this ""correspondence"" appears to be a ""bijection"", in the sense that $$\text{If $(\lambda_1,\lambda_2,\dots,\lambda_k)$ is a partition of $n$, then it appears in the expansion of $F^{(n)}(x_0)$} $$ All of these are still conjectures. I was only able to check this up to $n=10$ since Wolfram won't go any further in calculating the derivatives, but it seems true up to that point. If the result holds, then, by referring with $p(n)=$ to the partition function, in other words the number of distinct partitions of $n$ $\left| F(x) \right|$ to the ""number of terms"" in the expansion of $F$ (not very clear as a definition but it's the best I have) Then the following should hold: $$p(n) = \left|F^{(n)}(x_0)\right|$$ Now, to make it more rigorous, is the following conjecture true? If $F(x)=f(x)\log\left(f(x)\right)$ , and $f(x_0)=1$ , then $$ F^{(n)}(x_0) = \sum_{(\lambda_1,\dots,\lambda_k) \text{partition of $n$}} C(\lambda_1,\dots,\lambda_k)\cdot f^{(\lambda_1)}(x_0)\cdot\dots\cdot f^{(\lambda_k)}(x_0) $$ where the coefficients $C(\lambda_1,\dots,\lambda_k)$ are non-zero integers. I believe it is true, but can't see a way to prove it. Either there is something very obvious about how the terms in every derivative generate from the terms in the previous one and I'm missing it, or there is something more interesting going on. Now, let us focus on the coefficients $C(\lambda_1,\dots,\lambda_k)$ , since they follow many non-trivial patterns. In a way, $C(\lambda_1,\dots,\lambda_k)$ can be interpreted as some kind of ""weight"" of the partition $(\lambda_1,\dots,\lambda_k)$ . Unfortunately their value seems to be far from obvious, altough in some cases it seems to follow some patterns that I will show, but first, some notation: I will denote the partition $n=\underbrace{a+\dots+a}_{c_a}+\underbrace{b+\dots+b}_{c_b}+\dots+\underbrace{m+\dots+m}_{c_m}$ as $(a\cdot c_a,b\cdot c_b,\dots,m\cdot c_m)$ Now, for a fixed $n\in\mathbb{N}$ , some of the regularities that I spotted in the weights are the following: $C(n)=1$ $C(1\cdot(n-k),k)=\frac{(-1)^{n-k-1}n!}{(n-k)k!}$ for $k$ up to $n-1$ $C(n-k,k)=\binom{n}{k}$ EXCEPT when $n-k=n/2$ , in that case this coefficient fails if $n=2k$ , then $C(k\cdot2)=\frac12 \frac{(2k)!}{(k!)^2}$ if $n=3k$ , then $C(k\cdot3)=-\frac16 \frac{(3k)!}{(k!)^3}$ if $n=\alpha\cdot k$ , $\alpha\ge 4$ , then $C(k\cdot\alpha)$ appears to be of the form $\frac{1}{c_{\alpha}}\frac{(\alpha k)!}{(k!)^{\alpha}}$ for some $c_{\alpha}\in\mathbb{N}$ As you can see, this are all pretty ""easy"" partitions, in the sense that they either have only two components or most of the components are the same. I couldn't find any regularity for more complicated partitions, and part of the reason was the number of partitions of $n$ grows exponentially as $\sqrt n$ , so there are many cases to check. However, I am interested in knowing if some kind of closed expression for $C(\lambda_1,\dots,\lambda_k)$ exists, so the second question is: Given a partition of $n$ of the form $(\lambda_1,\dots,\lambda_k)$ , is there a way to determine the value of its ""weight"" $C(\lambda_1,\dots,\lambda_k)$ , defined as the coefficient of the partition in the expansion of $F^{(n)}(x_0)$ ? As an interesting final remark, notice that, if we set $f(x)=e^x$ , and $x_0=0$ , then, expanding $xe^x$ as a Taylor series, and assuming the conjecture holds true, we have that, for all $n\in\mathbb{N}$ , $$\sum_{(\lambda_1,\dots,\lambda_k) \text{partition of $n$}} C(\lambda_1,\dots,\lambda_k) = n $$ which I think is quite nice.","['number-theory', 'real-analysis', 'calculus', 'sequences-and-series', 'derivatives']"
4839327,Can anyone come up with a method for integrating product of logarithms?,"So recently I’ve been studying integrals of products of logarithms $$\int \prod_{n=1}^N \log(x+a_n) dx$$ Can anyone come up with general method for dealing with such integral?
Let's start simple, with N = 2. $$ \int \frac{\log(z+m)}{z} dz = \int \frac{\log(1-u)+\log m}{u} du = -Li_2(\frac{-z}{m}) + \log m\log z + C$$ Then we apply integration by parts to $\int \log(x+a)\log(x+b) dx$ , setting $u = \log(x+b)$ and $v = (x+a)\log(x+a)-(x+b)$ . Well, $x+a$ could be written as $(x+b) + (a-b)$ . Yielding $$\int \log(x+a)\log(x+b) dx = (x+a)\log(x+a)\log(x+b) - (x+a)\log(x+a) - (x+b)\log(x+b) + 2x + (b-a)\int\frac{\log(x+a)}{x+b}dx$$ $$=(a-b)Li_2(\frac{x+b}{b-a}) +(x+a)\log(x+a)\log(x+b) - (x+a)\log(x+a) $$ $$-(x+(a-b)\log(a-b)+b)\log(x+b) + 2x + C$$ The case $N = 3$ is a lot more complicated. I wasn't able to come up with a solution until recently. WolframAlpha was able to give solution but cannot provide any human readable derivation to its solution. My solution to $N=3$ goes like this.
We apply integration by parts to $\int\frac{log(z)log(z+m)}{z}dz$ setting $u = logz$ and $v = -Li_2(\frac{-z}{m}) + logmlogz$ . Yielding $$\int\frac{log(z)log(z+m)}{z}dz = Li_3(\frac{-z}{m}) - logz Li_2(\frac{-z}{m}) + \frac{1}{2} logmlog^2z + C$$ Now, we apply integration by parts to $\int\frac{log^2z}{z+m}$ setting $u = log^2z$ and $v=log(z+m)$ , hence we get $$\int\frac{log^2z}{z+m} dz = log^2zlog(z+m) - 2\int\frac{logzlog(z+m)}{z} dz = $$ $$-2Li_3(\frac{-z}{m}) + 2logz Li_2(\frac{-z}{m}) + log^2zlog(z+m) - logmlog^2z+ C$$ To keep things neat from here, $$\int log(x+a)log(x+b) dx = A(x, a, b) + C$$ $$\int\frac{log(z)log(z+m)}{z}dz = B(z, m) + C $$ $$\int\frac{log^2z}{z+m} dz = C(z, m) + C$$ We perform a substitution, $z = (y+p)/(y+q)$ . $dz = (q-p)/(y+q)^2 dy$ $$\frac{dz}{z+m} = \frac{\frac{q-p}{(y+q)^2}}{\frac{(1+m)y+(mq+p)}{y+q}} dy = \frac{qdy}{(y)(y+q)} = (\frac{1}{y}-\frac{1}{y+q})dy$$ let's set $mq+p = 0 => m = \frac{-p}{q}$ $$\int(log^2(y+p)-2log(y+p)log(y+q)+log^2(y+q))(\frac{1}{y}-\frac{1}{y+q})dy = C(\frac{y+p}{y+q}, \frac{-p}{q})+C$$ $$\int\frac{log(y+p)log(y+q)}{y}dy = D(y, p, q) + C = \frac{1}{2}(-C(\frac{y+p}{y+q}, \frac{-p}{q})+C(y+p, -p) - C(y+p, q-p) + C(y+q, -q) + \frac{log^3(y+q)}{3})+B(y+q, p-q)+C$$ Finally, we apply integration by parts to $\int log(x+a)log(x+b)log(x+c)$ setting $u=log(x+a)log(x+b)log(x+c)$ and $v=x+c$ $$\int log(x+a)log(x+b)log(x+c)dx = (a-c)D(x+a, b-a, c-a) + (b-c)D(x+b, a-b, c-b) - A(x, a, b) - A(x,a, c) - A(x, b, c) + (x+c)log(x+a)log(x+b)log(x+c)+ C$$ This can be generalized to a method that reduces $\int\frac{\prod_{n=1}^N log(x+a_n)}{x}dx$ to a bunch of integrals of the form $\int\frac{log^2(y+b)\prod_{n=1}^{N-2}log(y+b_n)}{y}dy$ $$\frac{log(y+p)log(y+q)}{y} = \frac{1}{2}(log^2(y+p)+log^2(y+q) - log^2(\frac{y+p}{y+q}))(\frac{1}{y} - \frac{1}{y+q}) + \frac{log(y+p)log(y+q)}{y+q} = $$ $$\frac{q}{2}\frac{log(\frac{y+p}{y+q})}{y(y+q)}+\frac{1}{2}(log^2(y+p)+log^2(y+q))(\frac{1}{y}-\frac{1}{y+q})+\frac{log(y+p)log(y+q)}{y+q}$$ We split $\int\frac{f(y)log(y+p)log(y+q)}{y}dy$ using this. $$\int\frac{f(y)log^2(\frac{y+p}{y+q})}{y(y+q)}dy = \int\frac{f(y)log^2(\frac{y+p}{y+q})}{\frac{y}{y+q}}*\frac{dy}{(y+q)^2} = \frac{1}{q}\int_{z=\frac{y+p}{y+q}}\frac{f(\frac{-qz+p}{z-1})log^2z}{z-\frac{p}{q}}dz$$ Substitute $z = \frac{y+p}{y+q} = 1+\frac{p-q}{y+q} => y = \frac{-qz+p}{z-1}$ . $dz = \frac{q-p}{(y+q)^2}dy$ $$\frac{1}{\frac{y}{y+q}}*\frac{dy}{(y+q)^2}=\frac{\frac{1}{q}}{\frac{(q-p)y}{q(y+q)}}*\frac{(q-p)dy}{(y+q)^2} = \frac{\frac{1}{q}}{\frac{y+p}{y+q}-\frac{p}{q}} dz = \frac{1}{q} \frac{1}{z-\frac{p}{q}} dz$$ We apply integration by parts to $\int\frac{f(y)log(y+p)log(y+q)}{y+q}dy$ setting $u = f(y)log(y+p)$ and $v = \frac{1}{2} log^2(y+q)$ . We have $$\int\frac{f(y)log(y+p)log(y+q)}{y+q} dy = \frac{1}{2}(f(y)log(y+p)log^2(y+q) - \int f'(y) log(y+p) log^2(y+q)dy-\int\frac{f(y)log^2(y+q)}{y+p})$$ $$\int\frac{f(y)log(y+p)log(y+q)}{y}dy = \frac{1}{2} (-\int_{z=\frac{y+p}{y+q}} \frac{f(\frac{-qz+p}{z-1})log^2z}{z-\frac{p}{q}}dz + \int(log^2(y+p)-log^2(y+q))(\frac{1}{y}-\frac{1}{y+q})dy+$$ $$f(y)log(y+p)log^2(y+q) - \int f'(y) log(y+p) log^2(y+q)dy-\int\frac{f(y)log^2(y+q)}{y+p})$$ Setting $f(y) = log(y+r)$ allows us decompose $\int\frac{log(y+p)log(y+q)log(y+r)}{y}dy$ into  bunch of integrals of the form $\int\frac{log(w+P)log^2(w+Q)}{w}dw$ . Solving this integral will let us solve the case $N = 4$ . For the case $N=4$ , WolframAlpha fails to give any answer. I am also stuck when it comes to the case $N=4$","['indefinite-integrals', 'calculus', 'computer-algebra-systems']"
4839337,Properties of $f(f(x))=g(x)$,"With reference to this Youtube video, I was trying to find out what important information we can gain from the equation $f(f(x))=g(x)$ [Assume all functions to be real] One particularly interesting one that I found was this: Consider $\text{Fix}(g)$ to be the set of all real numbers invariant under the map $g$ (which is to say that $\text{Fix}(g)=\{x:x\in\mathbb{R},g(x)=x\}$ ). Now, $$\begin{eqnarray}
&&f(f(x))=g(x)\\
&\implies& f(f(f(x)))=f(g(x))\\
&\implies&g(f(x))=f(g(x))
\end{eqnarray}$$ This implies compositional commutativity of the 2 functions. Now for any arbitrary $x_0$ in $\text{Fix}(g)$ : $$\begin{eqnarray}
&&g(f(x_0))=f(g(x_0))\\
&\implies& g(f(x_0))=f(x_0)
\end{eqnarray}$$ So, $f(x_0)$ must also be in $\text{Fix}(g)$ , as it is invariant under $g$ . So, we can conclude that if $x_0$ is invariant under $g$ , then $f(x_0)$ will also be invariant under $g$ Similarly, we can also show that if $x_0$ is invariant under $f$ , then $g(x_0)$ is also invariant under $f$ . [This is particularly useful when $\text{Fix}(g)$ has only one element] Now my question is: What are the other useful pieces of information that we can extract from $f(f(x))=g(x)$ ? I know this is a little broad-ended, but I am looking for all possible perspectives to inspect this problem. I am intending for the answers and comments to this question to be a compendium of everything that we can extract from the relation.","['functional-equations', 'functions', 'function-and-relation-composition']"
4839361,Is there any use of finite metrics on $\mathbb{R}$ or extended real numbers $\bar{\mathbb{R}}$?,"So in my grad level Real Analysis course based on Mesaure Theory and Lebesgue Integration, our professor added a note stating that extended real numbers $\bar{\mathbb{R}}$ can be used with a finite metric by defining \begin{equation} \rho (x, y) = |\arctan x - \arctan y| \text{ on }\mathbb{R}\end{equation} and extended with the values \begin{equation} f(\infty) := \lim_{x\rightarrow \infty}\arctan(x) = \pi/2 \end{equation} \begin{equation} f(-\infty) := \lim_{x\rightarrow -\infty}\arctan(x) = -\pi/2 \end{equation} which results \begin{equation} \rho (x,y) = |f(x) - f(y)| \text{ on }\mathbb{\bar{\mathbb{R}}} \end{equation} Here $\rho$ is a metric on $\mathbb{R}$ and $\bar{\mathbb{R}}$ since the extended $f$ is still injective and it is bounded by $\pi$ in absolute value. So both versions are examples of finite metrics. This was a basic practice in Metric Spaces class of course. My question is whether or not considering $\mathbb{R}$ and $\bar{\mathbb{R}}$ gives us any useful theorems or tools. I presume that it could be related to integration which would make lots of integrals finite and integrable considering measure spaces produced by such metrics $\rho$ or functions $f$ .","['integration', 'measure-theory', 'metric-spaces', 'real-analysis']"
4839371,Finding $\int_0^{\infty} e^{-x^2} (\mathrm{ln}(ax))^2dx$ [duplicate],"This question already has an answer here : Integrating:$\int\limits_0 ^ {\infty}e^{-x^2}\ln(x)dx $ (1 answer) Closed 6 months ago . This is my first question on MSE, so please overlook any mistakes I may have unknowingly committed. I was attempting to find the value of this integral $$ \int_0^\infty e^{-x^2} (\mathrm{ln}x)^2 dx$$ So I tried using Feynman's trick to solve this problem by defining a function $$I(a) = \int_0^\infty e^{-x^2} (\mathrm{ln}(ax))^2\mathrm{d}x$$ Then $$I'(a) = \int_0^\infty e^{-x^2} 2(\mathrm{ln}(ax))\cdot \frac{1}{ax} \cdot x \mathrm{d}x = \frac{2}{a}\cdot \int_0^\infty e^{-x^2}\mathrm{ln}(ax) \mathrm{d}x $$ $$I''(a) = \frac{2}{a^2}\cdot \int_0^\infty e^{-x^2}dx -\frac{2}{a^2}\cdot \int_0^{\infty}e^{-x^2} (\mathrm{ln}(ax))\mathrm{d}x$$ Observing that $$\int_0^\infty e^{-x^2} \mathrm{d}x = \frac{\sqrt{\pi}}{2}$$ We can rewrite $I''(a)$ as $$I''(a) = \frac{\sqrt{\pi}}{a^2} - \frac{I'(a)}{a}$$ Or, $$I''(a) + \frac{I'(a)}{a} = \frac{\sqrt{\pi}}{a^2}$$ Solving this differential equation, I got: $$I(a) = \frac{\sqrt{\pi}}{2} (\ln(a))^2 + c_1\ln(a) + c_2$$ But I am unable to determine what $c_1$ and $c_2$ should be, since we don't know the values of the initial parameters. Please help me determine the exact values of $c_1$ and $c_2$ specifically. I would greatly appreciate your help in this regard. Thank you very much.","['integration', 'calculus', 'definite-integrals', 'ordinary-differential-equations']"
4839388,Proving $a_n = \frac{(-1)^n n}{n+1}$ has no limit,"I want to prove the sequence $$a_n = \frac{(-1)^n\cdot n}{n+1}$$ has no limit as $\,n\to+\infty$ . I tried to work with inequalities to get that $\,|a_n-L|\geqslant\varepsilon\,$ and the way I tried to solve it was to divide into cases: $\,L\geqslant0\,$ or $\,L<0\,.$ When I did the case with $\,L\geqslant0\,,\,$ I chose an even $\,n\,$ and I found there is $\,\varepsilon=L\,$ for which this is true, but I don't know if I am allowed to represent $\,\varepsilon\,$ with $\,L\,$ or should I represent it as a number.
And I would also be glad if someone could show me the way to solve this because my prof hasn't shown us this although he gave us to do that.","['limits', 'calculus', 'epsilon-delta', 'sequences-and-series']"
4839467,Show that there is no sequence with infinitely many zero's that converges to a non-zero number.,"I've already finished what I think is a valid proof by contradiction, but I would like to know if it's correct. Let $(a_n)$ be a sequence with infinetly many $0$ s, such that $\lim(a_n) = a \ , a\neq0$ . Then $\forall\epsilon>0,\exists N\in \mathbb{N}$ such that $|a_n-a| < \epsilon \ ,\forall n>N$ . Let's choose an epsilon such that $0<\epsilon<|a|$ . Knowing that $(a_n)$ has infinitely many $0$ s, then $\exists n>N$ such that $a_n=0$ . That way, for that particular $n$ , we have that $|0-a|=|a|<|a|$ , which is absurd.We can now conclude that only one of the following happens: $(a_n)$ doesn't converge to a non-zero number or $(a_n)$ has a finite number of $0$ s.
Is this proof correct? Before getting to this result I was trying to achieve that $\forall n > N$ we necessarily have that $a_n > 0 \ \lor \ a_n < 0$ , but I was unable to do it. Is it possible to achieve something like this?","['solution-verification', 'sequences-and-series', 'real-analysis']"
4839471,Brezis' exercise 8.31.2: if $\int_I f=0$ then $\|u\|_{L^2(I)} \leq \frac{1}{(1+\pi^2)} \|f\|_{L^2(I)}$,"Let $I$ be the open interval $(0, 1)$ .
I am trying to solve a problem in Brezis' Functional Analysis Exercise 8.31 Consider the Sturm-Liouville operator $A u=-u^{\prime \prime}+u$ on $I$ with Neumann boundary condition $u' (0) = u' (1)=0$ . Compute the eigenvalues of $A$ and the corresponding eigenfunctions. Given $f \in L^2 (I)$ with $\int_I f=0$ , let $u$ be the weak solution of $$
(1) \quad
\begin{cases}
-u'' + u = f \quad \text {on} \quad I, \\
u'(0)=u'(1)=0.
\end{cases}
$$ Prove that $$
\|u\|_{L^2(I)} \leq \frac{1}{(1+\pi^2)} \|f\|_{L^2(I)} .
$$ I am trying to solve question (2.). In below attempt, I get the inequality with a bigger constant and have not utilized $\int_I f=0$ . Could you elaborate on how to obtain the desired constant $\frac{1}{\left(1+\pi^2\right)}$ ? It follows from (1) that $$
\int_I [ -u''v + uv ] = \int_I f v,
\quad \forall v \in H^2(I),
$$ which (by integration by parts) implies $$
\int_I [ u'v' + uv ] = \int_I f v,
\quad \forall v \in H^2(I),
$$ Substituting $v=u$ , we get $$
\int_I |u|^2 \le \int_I fu,
$$ which (by Cauchy-Schwarz inequality) implies the desired inequality.","['sobolev-spaces', 'functional-analysis', 'ordinary-differential-equations', 'sturm-liouville']"
4839476,Show that $\sqrt{n}(X_n - \mu) \xrightarrow{D} X \implies X_n \xrightarrow{p} \mu$,"This is what I've done so far: $F_{\sqrt{n}(X_n - \mu)}(t) \xrightarrow[\text{n $\rightarrow \infty$ }]{} F_{X}(t)$ for every $t \in R$ where $F_X$ is continuous. Thus, $P(X_n - \mu \le \dfrac{t}{\sqrt{n}}) \xrightarrow[\text{n $\rightarrow \infty$ }]{} P(X \le t) \iff 1 - P(X_n - \mu \ge \dfrac{t}{\sqrt{n}}) \xrightarrow[\text{n $\rightarrow \infty$ }]{} P(X \le t)$ $\iff 1 - P(X_n - \mu \ge \dfrac{-t}{\sqrt{n}}) + P(\lvert X_n - \mu \rvert \ge \dfrac{t}{\sqrt{n}}) \xrightarrow[\text{n $\rightarrow \infty$ }]{} P(X \le t)$ If I somehow manage to prove that $P(X_n - \mu \le \dfrac{-t}{\sqrt{n}}) \xrightarrow[\text{n $\rightarrow \infty$ }]{} P(X \le t)$ , then $X_n \xrightarrow{p} \mu$ , no? This idea doesn't seem too farfetched to me, given that $P(X_n - \mu \le \dfrac{t}{\sqrt{n}}) \xrightarrow[\text{n $\rightarrow \infty$ }]{} P(X \le t)$ . However, with little  information about $F_{X_n - \mu}$ and its coninuity, I'm unsure of how to finish the proof with this approach.
I'd appreciate any help I could get.","['convergence-divergence', 'probability-theory']"
4839499,How to express a Gaussian as a series of exponential? $\displaystyle e^{-x^2}=\sum_{n=1}^{\infty}c_n e^{-nx}$,"Context I would like to express the Gaussian function as a series of exponentials: $$e^{-x^2}=\sum_{n=1}^{\infty}c_ne^{-n|x|}\qquad\forall x\in\mathbb{R}$$ For simplicity (the absolute value is added later), I consider only the positive part: $$e^{-x^2}=\sum_{n=1}^{\infty}c_ne^{-nx}\qquad\forall x\geq0$$ My work To do this I looked for the orthonormal basis of $\{e^{-nx}\}_{n=1}^{\infty}$ on the interval $[0,\infty)$ so I used Gram-Schmidt process and I obtained the following orthonormal basis: $$\color{blue}{(b_n(x))=\left\lbrace\sqrt{2\left(n+1\right)}\cdot\frac{P_{n+1}\left(2e^{-x}-1\right)+P_n\left(2e^{-x}-1\right)}{2}\right\rbrace_{n=0}^{\infty}}$$ Where $P_n(x)$ is the Legendre polynomial Question The final formula is as follows: $$e^{-x^2}=\sum_{n=1}^{\infty}\left(\int_0^{\infty}b_n(x)e^{-x^2}\mathrm{d}x\right)b_n(x)\qquad\text{for }x\geq0$$ Which is mathematically correct, but I would like it to be expressed in terms of the sum of exponentials (since $b_n(x)$ depends on Legebre polynomials which are a sum of terms) The value of the integral is not important (for now it's just a number, I will solve it separately later), for now I indicate it with $I_n$ , I would like some help to make this transformation: $$\color{blue}{\sum_{n=1}^{\infty}I_n\cdot \sqrt{2\left(n+1\right)}\cdot\frac{P_{n+1}\left(2e^{-x}-1\right)+P_n\left(2e^{-x}-1\right)}{2}\quad \mapsto\quad \sum_{n=1}^{\infty}c_n e^{-nx}}$$ I had also tried to do this transformation initially: $$e^{-x^2}=\sum_{n=1}^{\infty}c_n e^{-nx}\quad\overset{x=\ln(z)}{\mapsto}\quad e^{-\ln(z)^2}=\sum_{n=1}^{\infty}c_n z^n$$ So essentially the series of $e^{-\ln(z)^2}$ at $z=0$ had to be calculated, but this is not possible, $\color{red}{\text{does}}$ $\color{red}{\text{this}}$ $\color{red}{\text{mean}}$ $\color{red}{\text{that}}$ $\color{red}{\text{the}}$ $\color{red}{\text{initial}}$ $\color{red}{\text{problem}}$ $\color{red}{\text{cannot}}$ $\color{red}{\text{be}}$ $\color{red}{\text{solved}}$ $\color{red}{\text{either?}}$ Update: value of the integral $$\color{green}{\frac{P_{n+1}\left(2z-1\right)+P_n\left(2z-1\right)}{2}=\sum_{k=0}^{n}\left(-1\right)^{n-k}\binom{n}{k}\binom{n+k+1}{n}z^{k+1}}$$ $$\frac{P_{n+1}\left(2e^{-z}-1\right)+P_n\left(2e^{-z}-1\right)}{2}=\binom{2n+1}{n}e^{-(n+1)x}{}_2F_1\left(\left.{-n-1,-n\atop -2n-1}\right|e^z\right)$$ $$\color{blue}{\int_{0}^{\infty}b_n(x)e^{-x^2}\mathrm{d}x=\sqrt{\frac{\pi\left(n+1\right)}{2}}\sum_{k=0}^{n}\left(-1\right)^{n-k}\binom{n}{k}\binom{n+k+1}{n}\ e^{\frac{\left(k+1\right)^{2}}{4}}\text{erfc}\left(\frac{k+1}{2}\right)}$$ I did a Desmos graph if someone want to ""play"" with it. (Don't put $T\geq 9$ because then it calculates badly since it has to take into account terms like $e^{50}\approx 5.18\cdot 10^{21}$ )","['gram-schmidt', 'legendre-polynomials', 'normal-distribution', 'sequences-and-series']"
4839503,Zero sets in pseudocompact spaces,"Let $X$ be a Tychonoff pseudocompact space and $Z$ be a zero-set in $X$ (i.e., there is a continuous function $f:X \to \mathbb{R}$ such that $x\in Z$ if and only if $f(x)=0$ ). It is known that these conditions imply that $Z$ is a weakly pseudocompact space (that is, $Z$ is $G_\delta$ dense in some compactification of it). Is it true that $Z$ must actually be pseudocompact? Since the result is usually stated as ""zero-sets of pseudocompact spaces are weakly pseudocompact"" my guess is that there must be a counterexample, but I've not been able to find it.",['general-topology']
4839520,Angle between tangential line and centerline of eccentrically spinning circles,"Pictured in Figure 1, there are two equally sized, eccentrically mounted circles with radius r=25 , turning around centerpoints with distance a=100 with eccentricity e=10 . The numerical values are given for clarity only. The line at the top connects both circles tangentially. The circles spin around their centers with a phase difference of 180°, meaning the lines connecting the circles' centers to the centers of rotation are always parallel, but point in opposite directions. There are (objectively poorly drawn) angles α and β , with α being the angle between the horizontal and tangential line. I'm now trying to find an equation for α as a function of β . While I expected this to be sinusoidal, I let a CAD program do some computations and according to Figure 2, it seems just about not to be. I'd appreciate any help!","['circles', 'geometry']"
4839521,Calculate the Lie derivative using definition,"Definition: If $X$ and $Y$ are vector fields on a smooth manifold $M$ , the Lie derivative of $Y$ in the direction of $X$ is the vector field $$
\mathcal{L}_X Y := \frac{d}{dt}\bigg\vert_{t=0}(F^X_{-t})_\ast Y.
$$ Here $F^X$ denotes the flow of $X$ . Using this definition, calculate the Lie derivative $\mathcal{L}_X Y$ where $X = \frac{\partial}{\partial x}$ and $Y = x \frac{\partial}{\partial y}$ (here $x,y$ are the standard coordinates on $\Bbb{R}^2$ ). My attempt: A previous exercise asked to calculate this via the Lie bracket, to which I found that $[X,Y] = \frac{\partial}{\partial y}$ , so I know the Lie derivative must equal this as well. I calculated the flow of $X$ as being $F^X: \Bbb{R}^2 \times \Bbb{R} \to \Bbb{R}: ((x,y),t) \mapsto (x+t,y)$ . Hence at a time $-t$ we get $F^X_{-t} = (x-t,y)$ . But when I calculate the derivative of this map, I just get that the derivative, $(F^X_{-t})_\ast = \operatorname{Id}$ . But this is independent of $t$ , so that would mean $\mathcal{L}_X Y = 0$ , which is not the correct answer. Where did I go wrong here? Is the flow correct? What about the derivative? Am I interpreting the notation incorrectly? Thanks in advance!","['vector-fields', 'lie-derivative', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
4839542,How to prove transitivity on proper subsets?,"I'm new to set theory, and recently I came across with a problem: Prove transitivity on proper subsets, that is if $A \subset B$ and $B \subset C$ , then $A \subset C$ . From what I've learned so far, proper subset, $A \subset B$ iff $(\forall x)(x \in A \Rightarrow x \in B) \land A \neq B$ . In other words, if $A \subseteq B$ and $A \neq B$ , then $A \subset B$ . From what's given, $(\forall x)(x \in A \Rightarrow x \in B)$ and $(\forall x)(x \in B \Rightarrow x \in C)$ , I can prove $(\forall x)(x \in A \Rightarrow x \in C)$ with specialization, transitivity, and generalization. Thus $A \subseteq C$ is the conclusion. Only if I can prove $A \neq C$ from $A \neq B$ and $B \neq C$ , together with $A \subseteq C$ , I can prove $A \subset C$ . $A = B$ iff $(\forall x)(x \in A \Leftrightarrow x \in B)$ (axiom of extent), thus $A \neq B$ iff $\neg (\forall x)(x \in A \Leftrightarrow x \in B)$ , iff $(\exists x)(\neg (x \in A \Leftrightarrow x \in B))$ , iff $$(\exists x)((x \in A \land x \notin B) \lor (x \notin A \land x \in B)).$$ So this is where I got stuck, that is, given $$(\exists x)((x \in A \land x \notin B) \lor (x \notin A \land x \in B))$$ and $$(\exists x)((x \in B \land x \notin C) \lor (x \notin B \land x \in C)),$$ how to prove that $$(\exists x)((x \in A \land x \notin C) \lor (x \notin A \land x \in C))?$$ Sorry if this is a really stupid question and thank in advance for any help! :)","['elementary-set-theory', 'proof-writing']"
4839566,Deformation and algebraic equivalence relation,"I found the algebraic equivalence relation is closely related to deformation, the most intuitive description that I found is in Griffiths' Topics in Transcendental Algebraic Geometric Chapter 1 which is defined as follows: (Definition 1) Let $X$ be a projective (algebraic) variety. Two cycles $Z_1$ and $Z_2$ on $\mathrm{X}$ are algebraically equivalent if, roughly speaking, one can be deformed into the other via an algebraic family of cycles on $X$ . To be more precise, there is an algebraic variety $S$ and an algebraic cycle $T$ in $S \times X$ such that $Z_1$ and $Z_2$ are the restrictions of $T$ to two fibers of the projection $\pi: S \times X \rightarrow S$ . The picture shows as follows In the case of divisor, there is another definition as follows (Definition 2) Let $X$ be projective (algebraic) variety, assume $D,D'$ be two Cartier divisor, we say $D\sim D'$ being algebraic equivalence if the linear bundle $D-D'\in \operatorname{Pic}^0(X)$ . If we mod out it we get $$\text{NS}(X) = \operatorname{Pic}(X)/\operatorname{Pic}^0(X)$$ The question is: Are these two definitions the same? Easy to see from the second one that algebraic equivalence relation is also numerical equivalence. If we can prove definition 1 = definition 2, then it will imply that the intersection number is deformation invariance, correct? (see related question: https://math.stackexchange.com/a/2679928/360262 )","['complex-geometry', 'algebraic-geometry', 'birational-geometry']"
4839577,"How Can We Study the Possibility of Uniform ""Distribuitons"" on Countable Sets Like $\mathbb{N}$ or $\mathbb{Q}$?","I wonder if there exist ways to deal with infinitely coutable sets with a probability in the  ""uniform"" sense. In the basic Probability Theory, as far as I know, we usually study two kinds of probability most of the time that is compatible with our sense of probability: 1- First one is the discrete probability which uses convergent sums and mass functions defined on a finite outcome space $\Omega$ and finite $\sigma$ -algebra $\mathcal{F} \subset 2^{\Omega}$ such that $\mathbb{P}(\Omega) = 1$ . We consider $\sigma$ -algebras to generalize the concept but things are simpler here and all can be induced to positive sums and series. In this version, uniform distributions corresponds to the case that the mass function is constant and $f({x}) = 1/n$ for each outcome $x\in\Omega$ where $|\Omega| = n$ where $\mathcal{F}$ is just the entire power set $2^{\Omega}$ since it should contain all singletons and all sets are finite. Then, for any event $E\in \mathcal{F} = 2^{\Omega}$ where $|F| = k \leq n$ , it produces $f(E) = k/n$ by finite additivity of the (probability) measure. Other than the uniform distribution, we can consider any countable $\Omega$ where $\mathbb{P}(\Omega) = 1$ is still satisfied which means that any non-negative function $f$ satisfying $\sum_{x\in \Omega} \mathbb{P}({x}) = 1$ can be considered as a mass function. Then, the sense for uniform distribution does not work here as a probability mass function because countable infinite set causes the mass function $f(x)$ to be zero if $E$ is finite or undefined caused by indeterminate quotient $\infty/\infty$ if $E$ is infinite. 2- However, there is the second kind of the theory, the continuous probability on, for example, real numbers $\mathbb{R}$ using Lebesgue Integration . In this case, probability is not considered as the sum of singleton outcomes, i.e. $\sum f(x)$ , but the integral of the density function of an event $E\in \mathcal{F}\subset 2^{\Omega}$ as the integral $\int_E fd\lambda$ in general. So we can use the similar set-up here where the only change is to use integration instead of sums and series. Now the sense for uniform density here is just a constant function again: Given a bounded measurable set E which is not null, e. g. intervals. Then $c := m(E) \in (0, \infty)$ and we can use the uniform distribution on this bounded set as $f(x) = 1/c > 0$ . Then we simply have the total probability as the integral $\int_E f dm = (1/c)c = 1$ which makes it a distribution for real. Unfortunately, there are things we are missing here, too : None of the null sets are covered by the continous probability as well as sets with measure $\infty$ : The latter is similar to the problem we encountered in discrete case since it causes the uniform distribution on the set to be 0 and then the probibility, which means the integral of the zero distribution here, becomes 0, not 1 . On the other hand the former is caused by the fact that integrals over null sets are zero automatically, not 1 . Thus no function $f$ can be a density function for a null set like countable infinite sets $\mathbb{N}$ or $\mathbb{Q}$ if they are null in the given measure, for example Lebesgue Measure or Borel measure . Finally, I think that using a measure where the set $\mathbb{N}$ is
not null (consider a measure where every natural number is an atom that of equal measure) coincides with the discrete probability with some editing since the integration
on $\mathbb{N}$ becomes the limit of the finite sum by the definition
of the integral via simple functions. QUESTION : We have the sense that choosing a random natural number should be considered in some sort of ""uniform"" way . How can we describe that uniformity and study it in a natural sense? Which tools do we have to examine them? After my surfing on that topic I realized that the discrete way cannot handle some of infinite quantities includig uniform ones and continous way cannot handle null-sets including countable ones. So I believe that there should be a middle-way or a completely different approach to study (infinitely) countable uniformity.","['statistics', 'uniform-distribution', 'conditional-probability', 'probability-theory', 'probability']"
4839618,If $P= \tan(3^{n+1}\theta) - \tan\theta$ and $ Q=\sum_{r=0}^n\frac{\sin(3^r\theta)}{\cos(3^{r+1}\theta)} $. Prove that $P=2Q$,"I have been able to prove the above relation using the fact that $\tan{x}-\tan{y}$ = $\frac{sin(x-y)}{cosx cosy}$ . In my search for alternative solutions, I tried using the complex definition of sin and cos as in my mind it felt like a great start to generate a telescoping series. However, I was not able to simplify the expression when I took $sinx$ as $\frac{e^{ix}-e^{-ix}}{2i}$ and $cos3x$ as $\frac{e^{3ix}+e^{-3ix}}{2}$ . Please help me with how to prove this relation using complex numbers","['trigonometry', 'complex-numbers']"
4839642,Modeling Nanotubes Geometry,"In various references, we see the construction of unit cells of carbon nanotubes (CNTs) from chiral and translational vectors. The chiral vector is given as: $$\vec C_h = n\vec a_1 + m\vec a_2$$ While the translational vector is given by: $$\vec T = t_1\vec a_1 + t_2\vec a_2$$ Clearly, $\vec C_h$ and $\vec T$ must be orthogonal, hence $\vec C_h \cdot \vec T = 0$ . In the literature, it always appears that $t_1 = \displaystyle\frac{2m+n}{d_R}$ and $t_2 = -\displaystyle\frac{2n+m}{d_R}$ , where $d_R=\mathrm{gcd}(2m+n,2n+m)$ . My questions are, how to deduce this? And how does this relate to the fact that the translational vector can determine the size of the unit cell? I managed to replicate this idea for a hexagonal lattice, similar to graphene. For a rectangular lattice, I successfully reproduced the chiral vector, but I haven't yet managed to understand or reproduce the translational vector (not even for CNTs, actually).","['gcd-and-lcm', 'integer-lattices', 'geometry', 'physics', 'mathematical-physics']"
4839649,"When does $(a+b\sqrt n)^3+(a-b\sqrt n)^3=c^3$ have integer solutions $(a,b,c,n)$?","From this post Where Fermat's Last Theorem fails , we find the nice, $$(18+17\sqrt2)^3+(18-17\sqrt2)^3=42^3$$ Using this initial solution, an infinite more can be generated using P. Tait's identity, $$\big(x(y^3 + z^3)\big)^3 + \big(y(-x^3 - z^3)\big)^3  = \big(z(x^3 - y^3)\big)^3$$ which is true if $x^3+y^3=z^3.\,$ For example, the first leads to a second, $$(707472 + 276119 \sqrt{2})^3 + (707472 - 276119 \sqrt{2})^3 = 1106700^3$$ and so on infinitely. But it is also the case that, $$(2+\sqrt{-2})^3+(2-\sqrt{-2})^3=(-2)^3$$ $$(121+23\sqrt{-11})^3+(121-23\sqrt{-11})^3=(-88)^3$$ all of which are unique factorization domains UFD ${\bf Q}(\sqrt n)$ . So at first I thought it was peculiar to UFDs, but expanding, $$(a+b\sqrt n)^3+(a-b\sqrt n)^3=c^3$$ we get the simpler, $$2 a^3 + 6 a b^2 n = c^3\tag{eq.1}$$ After I tested various $n$ , it didn't seem to be limited to UFDs. For example, $$(256+11\sqrt{-41})^3+(256-11\sqrt{-41})^3=296^3$$ Question : Can $2 a^3 + 6 a b^2 n = c^3$ (labeled as eq.1) be turned into an elliptic curve similar to this post so we can easily find $n$ for which eq.1 is solvable? If not, what are the $-100 < n < 100$ such that $(1)$ has a solution? (Is it in the OEIS?) P.S. We avoid square $n$ , since by FLT $x^3+y^3=z^3$ , it just yields trivial solutions $xyz = 0$ . Or if $n=-3m^2$ , then $(a,b,c) = (3m,\,1,\,0)$ so we avoid $abc =0$ as well.","['computational-mathematics', 'number-theory', 'elliptic-curves', 'diophantine-equations']"
4839665,A challenging Integral Involving Logarithmic and Trigonometric Functions,"Question: How to evaluate $$\frac{1}{2\sqrt{2}} \int_{0}^{\frac{\pi}{2}} \frac{\log(1 + \tan y)}{(\cos y + \sqrt{2} \sin(y + \frac{\pi}{4})) \sqrt{1 + \sqrt{2} \sin(2y + \frac{\pi}{4})}} \, dy = G$$ My attempt Recall that $$\sin\left(x + \frac{\pi}{4}\right) = \frac{1}{\sqrt{2}} (\cos x + \sin x) \text{ and } \cos(2x) = 2\cos^2 x - 1$$ $$J = \frac{1}{2\sqrt{2}} \int_{0}^{\frac{\pi}{2}} \frac{\log(1 + \tan x)}{(\cos x + \sqrt{2} \sin(x + \frac{\pi}{4})) \sqrt{1 + \sqrt{2} \sin(2x + \frac{\pi}{4})}} \, dx$$ $$= \frac{1}{2\sqrt{2}} \int_{0}^{\frac{\pi}{2}} \frac{\log(1 + \tan y)}{(2\cos y + \sin y) \sqrt{2\cos^2 y + 2\sin y \cos y}} \, dy$$ . $$= \frac{1}{4} \int_{0}^{\frac{\pi}{2}} \frac{\log(1 + \tan y)}{(2 + \tan y) \sqrt{1 + \tan y}} \cdot \frac{1}{\cos^2 y} \, dy$$","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'closed-form']"
4839673,Four rational Pythagorean triples in a square,"Suppose we have a unit square $ABCD$ . If we have a primitive Pythagorean triple $(a, b, c)$ , let $CD = 1$ and hence $ED = a/b$ . It also follows that $EC$ is rational of course. In fact, using the similarity of $\Delta AFE$ and $\Delta ECD$ , all of the sides will also be rational numbers except for $FC$ . After some work, $FC^2 = 1^2 + \left(1 - \frac{a}{b} + \frac{a^2}{b^2} \right)^2$ $ = \frac{b^4 + (a^2 - ab + b^2)^2}{b^4}$ . Thus we need $(b^2)^2 + (a^2 - ab + b^2)^2 = k^2$ for some positive integer $k$ for $FC$ to be rational: Using Euclid's formula for Pythagorean triples, one option is that $p^2 - q^2 = b^2; \ 2pq = (a^2 - ab + b^2); \ p^2 + q^2 = k$ , where integers $p > q > 0$ and $p, q$ are of opposite parity (otherwise the triple will not be primitive). Wolfram Alpha then uses the quadratic formula to tell me that $a = \frac{1}{2} \left(b \pm \sqrt{-3p^2 + 8pq + 3q^2} \right)$ , which simplifies to $a = \frac{1}{2} (b \pm \sqrt{(3q - p)(3p + q)})$ . This tells us that $3q > p$ or that $p/3 < q < p$ . Of course, the other option is that $p^2 - q^2 = a^2 - ab + b^2, 2pq = b^2$ . This gives $a = \frac{1}{2} \left(b \pm \sqrt{2(2p+q)(p-2q)}\right)$ and hence $p > 2q$ or that $p/2 > q > 0$ . So can we find a configuration where $FC$ is rational, or where $(b^2)^2 + (a^2 - ab + b^2)^2 = k^2$ for some positive integer triple $(a, b, k)$ ?","['pythagorean-triples', 'geometry', 'diophantine-equations']"
4839679,Generation of a cardioid as the locus of intersection point of two tangents to two intersecting circles,"About 8 months ago I came up with this nice feature using GeoGebra but I couldn't prove it, any help would be appreciated We have two intersecting circles and we drew the moving line passing through one of the two intersection points, then we drew the tangents to the two circles at the ends of its intersection with the two circles and set the red point that represents the point of intersection of the two tangents. What I arrived at is that the traces of the red point will create a cardiac curve whose head is the other point of intersection of the two circles and touches both circles.","['euclidean-geometry', 'locus', 'geometry']"
4839681,Cut a unit stick at $n-1$ random points. Expectation of product of fragment lengths is $\prod\limits_{k=n}^{2n-1}\frac1k$. Why?,"On a straight stick of length $1$ , choose $n-1$ independent uniformly random points. Cut the stick at those points, yielding $n$ fragments. Let $\mathbb{E}_n$ be the expectation of the product of lengths of the fragments. Simulations suggest that $\mathbb{E}_n=\prod\limits_{k=n}^{2n-1}\frac1k$ . For example, $\mathbb{E}_5=\frac{1}{5\cdot6\cdot7\cdot8\cdot9}$ . This is an elegant result, so I suspect there is an intuitive explanation, but it eludes me. Is there an intuitive explanation for $\mathbb{E}_n=\prod\limits_{k=n}^{2n-1}\frac1k$ ? My thoughts: I have only been able to prove the result for $n=2$ and $n=3$ , and my proofs are not intuitive. $\color{red}{n=2}$ : Hold the stick horizontally. Let the distance of the random point from the left end be $x$ . $$\mathbb{E}_2=\int_0^1x(1-x)\mathrm dx=\frac16=\prod\limits_{k=2}^{2(2)-1}\frac1k$$ $\color{red}{n=3}$ : Hold the stick horizontally. Let the distance of the 1st chosen point from the left end be $x$ . Let the distance of the 2nd chosen point from the left end be $y$ . (We make no assumption about which of $x$ and $y$ is greater.) For a fixed $x$ value, the expectation of the product of fragment lengths is $$P(y<x)\times\int_0^xy(x-y)(1-x)\mathrm dy+P(y>x)\times\int_x^1x(y-x)(1-y)\mathrm dy$$ where $P(y<x)=x$ and $P(y>x)=1-x$ . Then we integrate this expectation from $x=0$ to $x=1$ : $$\mathbb{E}_3=\int_0^1\left(x\int_0^xy(x-y)(1-x)\mathrm dy+(1-x)\int_x^1x(y-x)(1-y)\mathrm dy\right)\mathrm dx=\frac{1}{60}=\prod\limits_{k=3}^{2(3)-1}\frac1k$$ Context: I recently answered a question involving products of lengths of random line segments. That made me think of this question.","['geometric-probability', 'geometry', 'expected-value', 'intuition', 'products']"
4839695,Property for Conjugate diameters in an ellipse [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question About six months ago I came up with a nice property for Conjugate diameters in an ellipse but couldn't prove it If anyone can prove this, please do so If the feature is already discovered, please point to a reference mentioning it","['conic-sections', 'geometry']"
4839717,Generalized Coxeter's integral $\displaystyle \int_0^{\frac{\pi}{2}} \arccos \left(\frac{a\cos(x)}{1+b\cos(x)}\right) dx$,"Coxeter's integral is a well-known definite integral given by: $$I = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{\cos(x)}{1+2\cos(x)}\right)dx$$ I am well aware of the 11-page excruciatingly long procedure used to evaluate this integral by Paul Nahin. This integral made me wonder whether we can find an expression for a generalized integral (similar to that of Coxeter's) of the form: $$I(a,b) = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{a\cos(x)}{1+b\cos(x)}\right) dx$$ For simplicity, considering the cases for $a=1$ , we have: $$I(b) = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{\cos(x)}{1+b\cos(x)}\right) dx$$ I begin simplifying this as Nahin did - using the fact that $$\arccos x = 2 \arccos \left(\sqrt{\frac{1+x}{2}}\right)$$ We can write the integral as $$I(b) = \int_0^{\frac{\pi}{2}} \arccos \left(\sqrt{\left(\frac{1+(b+1)\cos(x)}{2 + 2b\cos(x)}\right)}\right) dx$$ $$ = \int_0^{\frac{\pi}{2}} \arctan \sqrt{\left(\frac{1+(b-1)\cos(x)}{1+(b+1)\cos(x)}\right)} dx$$ I have tried two approaches with this integral. The first approach was the same as followed by Nahin for evaluating Coxeter's integral. It turned out to be a dead end and I could proceed no further with that procedure. The second approach was to substitute $t = \tan(\frac{x}{2})$ and simplify. I obtained $$I(b) = 4 \int_0^1 \arctan \sqrt{\left(\frac{b-(b-2)t^2}{(b+2)-bt^2}\right)} \frac{dt}{t^2+1}$$ Then recalling that $$\arctan(b) = b \cdot \int_0^1 \frac{1}{1+b^2u^2} du$$ I rewrote the original integral into a double integral given by: $$I(b) = 4 \int_0^1 \int_0^1 \frac{\sqrt{(b-(b-2)t^2)((b+2)-bt^2)}}{(t^2+1)\cdot \left[(b+2-bt^2) +(b-(b-2)t^2)\cdot u^2 \right]} dt du$$ I am stuck here. I am not sure, but I have a hunch that we will have to substitute something like $$bt^2 = (b+2)sin^2v$$ Followed by another trigonometric substitution $$\tan v = t$$ Can anyone help me simplify this integral further, or suggest a better approach to find $I(b)$ , and if possible - $I(a,b)$ ? Any help is appreciated. Thanks for reading!","['integration', 'calculus', 'definite-integrals']"
4839872,A uniform (non-asymptotic) upper bound for Hermite polynomials in the complex plane,"In a paper dating back to 1990, Eijndhoven and Meyers [1] mention the following ""elementary"" upper bound for Hermite polynomials on the whole complex plane: $$ \forall z \in \mathbb{C}, \forall n\in\mathbb{N},\;\; |H_n(z)| \leq 2^{\frac{n}{2}} \,\sqrt{n!}\; e^{\sqrt{2n}|z|}$$ They did not give the proof of this very useful inequality. They only gave a hint, which was to use the well-known, explicit polynomial sum -- a nice piece of advice that left me stranded. Strange as it may seem, I have not found it anywhere in reference books (Bateman, NIST, Gradshtein, Prudnikov and alii etc.).
The inequality plays an important part in the other proofs of the paper. Any idea of how to  cleanly derive it? I have a partial proof by induction that unfortunately only works in the complex plane outside of the disk of radius $\sqrt{\frac{n+1}{2}}$ , what I am looking for is a proof valid in the whole complex plane. Also, there exist better bounds on the real and imaginary lines, so partial answers limited to these lines will not do. (Historical edit) For low to moderate values of $n$ , this ""elementary"" upper bound is way better than it looks, at least for high real parts and low imaginary parts. The best uniform inequality in the complex plane  I know of was given by P. Rusev in a paper of the Bulgarian academy of sciences [2]: $$|H_n(z)| \leq (2e/\pi)^\frac{1}{4}(\Gamma(2n+1))^\frac{1}{2} (2n+1)^{-n/2-1/4} e^\frac{n}{2} e^{x^2}\cosh((2n+1)^\frac{1}{2}y), \text { where } z=x+iy$$ It is surprising to see that this ""non elementary"" estimate is in $e^{x^2}$ while the elementary one is only (asymptotically for $y$ fixed) in $e^{|x|}$ . Rusev's bound is not much better for high imaginary parts and low real parts either. It may be better for higher values of $n$ and bounded $x$ . Apparently the author did not know of Eijndhoven and Meyers' formula given 10 years earlier. [1] Eijndhoven,S.J.L.  and Meyers, J.L.H. ""New Orthogonality Relations for the Hermite Polynomials and Related Hilbert Spaces"", JOURNAL OF MATHEMATICAL
ANALYSIS AND APPLICATIONS 146, 89-98 (1990), eq. 1.2 p. 90 [2] P. Rusev ""An Inequality for Hermite's Polynomials in the Complex Plane"", Dokladi na b'lgarckata akademia na naukite/Comptes rendus de l'Académie bulgare des Sciences, 53,10 (2000).","['complex-analysis', 'orthogonal-polynomials', 'hermite-polynomials', 'upper-lower-bounds']"
4839922,"For a linear mapping to be a bijection, is it necessary to include the dimension of the domain and co-domain?","Suppose I have an operator $U$ which maps elements from $\mathcal{O}$ to $\mathcal{D}$ . To show that it is bijective, $U$ must be: 1)surjective (onto) 2)injective (one-to-one) However, in some places, I have seen as an additional requirement that the dimension of $\mathcal{O}$ and $\mathcal{D}$ must be equal: 3) $\mathcal{O}_d$ = $\mathcal{D}_d$ Is 3) necessary to include, or do all mappings that satisfy 1) and 2) automatically satisfy 3)?","['functions', 'linear-algebra', 'linear-transformations']"
4839939,The value of $\lim_{h\to 0}\frac{\sin(a+3h)-3\sin(a+2h)+3\sin(a+h)-\sin a}{h^3}$ is?,"Here was my approach: I grouped the terms $\frac{\sin(a+3h)-\sin a}{h^3} $ and $\frac{3\sin(a+h)-3\sin(a+2h)}{h^3}$ . By using first principle and got $\frac{3\cos a-3\cos(a+h)}{h^2}$ . However, now when I expand $\cos(a+h)$ using the identity, my final answer isn't quite right. The answer is supposed to be $-\cos a$ . I cannot understand where I went wrong so please help me with this.","['limits', 'limits-without-lhopital']"
4839962,"Find $\lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt$ if $g_n \nearrow g$ and $g^+ \in L^1[0,1]$","Let $g_n: [0,1]\to \Bbb R$ be measurable for all $n\in \Bbb N$ , and $g_n \nearrow g: [0,1]\to \Bbb R$ . Let $g^+ = \max\{0,g\}$ . Assume $g^+ \in L^1[0,1]$ . Evaluate $$\lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt.$$ My work. My guess is that the limit approaches $\int_0^1 g(t)\, dt$ . However, the correct limit (this is a quals problem) seems to be $\int_0^1 g^+(t)\, dt$ . I have shown my work below. As $g_n \nearrow g$ , $g$ is measurable. $g^+$ is also measurable. We have $$\left| \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt - \int_0^1 g(t)\, dt \right| \le \left|\frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt  - \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  \right| + \left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right|.$$ We can control the second term using Lebesgue's DCT. We have $$\frac{\ln (1+e^{ng(t)})}{n} \le \frac{\log 2}{n} + g(t) \le 1 + g^{+}(t) \in L^1[0,1]$$ implying $$\lim_{n\to\infty}\left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right| = 0$$ by DCT. As a result, for some $N_1 \in \Bbb N$ , we have $$\left| \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt  -\int_0^1 g(t)\, dt \right| < \frac \varepsilon 2$$ for all $n\ge N_1$ . The first term is $$\left|\frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt  - \frac 1n \int_0^1\ln (1+e^{ng(t)})\, dt \right| = \left| \frac 1n \int_0^1\ln\left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}}\right)\, dt\right|$$ As $g_n \nearrow g$ , we get $$\ln \left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}} \right)\xrightarrow{n\to\infty} 0.$$ So, for some $N_2 \in \Bbb N$ , $$\left|\ln \left(\frac{1+e^{ng_n(t)}}{1 + e^{ng(t)}} \right)\right| < \frac{\varepsilon}{2}$$ for all $n\ge N_2$ . Hence, for $n\ge N:= \max\{N_1,N_2\}$ , we have $$\left| \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt - \int_0^1 g(t)\, dt \right| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2N} < \varepsilon.$$ Thus, $$\lim_{n\to\infty} \frac 1n \int_0^1\ln (1+e^{ng_n(t)})\, dt = \int_0^1 g(t)\, dt.$$ I'd like to know where I'm going wrong, and how to justify the correct limit $\int_0^1 g^+(t)\, dt$ . I guess my estimate of the first term might be incorrect. Thanks!","['measure-theory', 'solution-verification', 'analysis', 'real-analysis']"
4839965,Solution of a meme integral: $\int \frac{x \sin(x)}{1+\cos(x)^2}\mathrm{d}x$,"Context A few days ago I saw a meme published on a mathematics page in which they joked about the fact that $$\int\frac{x\sin(x)}{1+\cos(x)^2}\mathrm{d}x$$ was very long (and they put a screen shot of the result obtained using Wolfram) (I avoid posting the whole screen because it is actually very very long, just open this link .) My work However, out of curiosity I tried to do it and got the following result: $${\color{blue}{\int_{0}^{x}\frac{t\sin(t)}{1+\cos(t)^2}\mathrm{d}t=-x\operatorname{tan^{-1}}(\cos(x))-2\sum_{n=1}^{\infty}(-1)^n\frac{(\sqrt{2}-1)^{2n-1}}{(2n-1)^2}\sin((2n-1)x)}}$$ Which is ridiculously shorter than the whole result Wolfram gives haha Question I would like to express the last series in terms of known functions, but I can't (I'm pretty sure it's the imaginary part of the dilogarithm, but I can't bring it back), could anyone help me? I'll leave a link for Desmos if anyone wants to work on it.","['integration', 'definite-integrals', 'calculus', 'polylogarithm', 'sequences-and-series']"
4840008,The asymptotic $\prod_{k=1}^n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t\sim A/(Bn)^n$,"Let $P_f(n)=\prod_{k=1}^n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t$ be the product of integral parts of a function $f$ . For simplicity, assume $f$ is smooth and positive-valued on $(0,1)$ . I noticed some limits in this question and this question can be rephrased as asymptotics: $\displaystyle P_f(n)\sim \frac{2\cos(\frac{\pi}{2\sqrt{3}})}{(en)^n}$ for $f(t)=\sqrt{t(1-t)}$ $\displaystyle P_f(n)\sim \frac{4\cosh^2(\frac{\pi}{2\sqrt{3}})}{(2n)^n}$ for $f(t)=1-\cos(2\pi t)$ $\displaystyle P_f(n)\sim\frac{2}{(e^2n)^n}$ for $f(t)=t(1-t)$ $\displaystyle P_f(n)\sim\frac{2\cosh(\frac{\pi}{2\sqrt{3}})}{(e^2n)^n}$ for $f(t)=t^2$ All of the asymptotics are of the form $A/(Bn)^n$ for some constants $A$ and $B$ . Taking logs, dividing by $n$ , and approximating $n\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t\approx f(\frac{2k-1}{2n})$ , we can say $-\ln B$ is approximated by a midpoint Riemann sum yielding, potentially, $$ B=\exp\Big(-\int_0^1\ln f(t)\mathrm{d}t\Big)=\prod_0^1 f(t)^{-\mathrm{d}t}, $$ a product integral . This agrees with the four examples above. But how do we find $A$ ? With the ol' adding-and-subtracting trick, I get $\ln A=X+Y$ where $X$ and $Y$ are given by $$ X=\lim_{n\to\infty} n\Big[-\int_0^1\ln f(t)\mathrm{d}t+\sum_{k=1}^n\frac{1}{n}\ln f\big(\frac{2k-1}{2n}\big)\Big] $$ $$ Y=\lim_{n\to\infty} \sum_{k=1}^n \ln\Big(\frac{\int_{(k-1)/n}^{k/n}f(t)\mathrm{d}t}{\frac{1}{n}f(\frac{2k-1}{2n})}\Big) $$ The former seems to be in want of an Euler-Maclaurin formula for product integrals. The latter I assume needs to use Newton-Mercator for $\ln(1+u)$ , but if $f\to 0$ as $t\to0$ or $t\to1$ then I'm not sure $u\to0$ uniformly as $n\to\infty$ . Also possible is expanding $f(t)$ as a Taylor series around $\frac{2k-1}{2n}$ in each integral but I think this will hit the same wall. Ideally it may be possible to find a complete asymptotic series for $\ln P_f(n)$ as $n\to\infty$ with coefficients expressed in terms of $f$ , or at least an algorithm to generate such a series.","['integration', 'asymptotics', 'real-analysis', 'euler-maclaurin', 'numerical-methods']"
4840012,Prove that $\lim_{N \to +\infty}\sum^N_{n=1}{|\sin n^\alpha| \over n^\beta} = +\infty$,"I want to prove that $$\lim_{N \to +\infty}\sum^N_{n=1}{|\sin n^\alpha| \over n^\beta} = +\infty \quad \forall\alpha \gt 0, \; \forall \beta \le 1, \; \alpha, \beta \in \Bbb R.$$ In other words, I want to show that, under the specified restrictions on the parameters, the series diverges to $+\infty$ and is not oscillating or irregular, for instance. I have verified that the series indeed does this by looking at its graph for various parameter choices. I observed that, $\forall \alpha \gt 0$ : $0 \lt \beta \le 1 \implies \lim_{n \to +\infty}a_n=0$ , so the necessary condition for convergence is met. To prove positive divergence, I tried studying the associated improper integral: $$\int^{+\infty}_1{|\sin x^\alpha| \over x^\beta}\ dx = \int^{\sqrt[\alpha]{\pi}}_1{|\sin x^\alpha| \over x^\beta}\ dx + \sum^{+\infty}_{k=1} \int^{\sqrt[\alpha]{(k+1)\pi}}_{\sqrt[\alpha]{k\pi}}{|\sin x^\alpha| \over x^\beta}\ dx.$$ The integrand is $\ge 0$ within the intervals considered, so the integral on the left diverges to $+\infty$ . Since the integral test only works if the sequence is monotone decreasing, I'm not sure I can apply this result to show that also $\sum^{+\infty}_{n=1}a_n = {+\infty}$ ; $\beta \le 0 \implies \nexists \lim_{n \to +\infty}a_n$ , so the series is not convergent. Still, I want to show that the stronger statement above is true. An idea that occurred to me, but that I wasn't able to formalize, was that $n \in \Bbb N \implies n^\alpha \neq \pi \; \forall \alpha \gt 0$ , since $\pi$ is a transcendental number and as such cannot be construed out of any combination of real numbers, which in turn implies that $|\sin{n^\alpha}| \gt 0 \; \forall \alpha \gt 0.$ (However, I do not have the knowledge necessary to back up this claim.) Wouldn't this imply that $a_n$ is bounded below by some constant $k \gt 0$ , meaning that the series would diverge to $+\infty$ ? If anybody could help shed some light, I'd much appreciate it.","['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
4840103,A nice finite induction problem: $\sqrt{2 \sqrt{3 \sqrt{4 \ldots \sqrt{n}}}}<3.$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question for all $n\in\mathbb{N}$ , $$\sqrt{2 \sqrt{3 \sqrt{4 \ldots \sqrt{n}}}}<3.$$ Any help proving this problem by finite induction. I tried to use $\sqrt{2}\cdot\sqrt[4]{3}\cdot\sqrt[8]{4}\cdots\sqrt[2^{n-1}]{n}<3$ as a hypothesis, but I was not successful. I'm trying to resolve this issue in my article, which brings together questions like this.","['induction', 'discrete-mathematics']"
4840113,"Does there exist a polynomial $f(x, y, z)$ which is positive iff |x|, |y|, |z| are sides of a triangle?","The following problem appeared in the Kömal (Hungarian mathematical journal) in year 1997. Official Source: Problem N 145 at the end of the page here. Problem. Does there exist a polynomial $f(x, y, z)$ of real coefficients such that $f(x,y,z)$ is positive if and only if $|x|, |y|, |z|$ are sides of a triangle? The given inequalities $|x|+|y|>|z|$ , $|y|+|z|>|x|$ , and $|x|+|z|>|y|$ impose an interesting geometric region in $\mathbb{R}^3$ : I am guessing that the question has a negative answer -- no such polynomial should exist. Any thoughts / comments / hints are appreciated!","['contest-math', 'polyhedra', 'polynomials', 'real-analysis']"
4840134,Is $\frac{(p-1)^p+1}{p^2}$ square-free?,"Is $\frac{(p-1)^p+1}{p^2}$ squarefree for all primes $p \geq 7$ ? I did some small testing and it seems to hold up to $p \leq 47$ . Also, note that the above expression is indeed an integer by binomial expansion.","['number-theory', 'divisibility', 'prime-numbers']"
4840135,Why is a periodic subgroup of a residually p-group a p-group?,"This might be a stupid question since I don't have much knowledge in group theory, but I've been struggling on this for a bit of time ; I am currently reading Wehrfritz's book on linear groups, and during the proof for the corollary 4.8, Wehrfritz states that "" [...] $H$ is residually a $p$ -group. Then every periodic subgroup of $H$ is a p-group"" There seems to be no additionnal hypothesis.
Correct me if I'm mistaken, but since the order of the image of an element by a morphism is lower than the original order of the element, couldn't we imagine some element of order $p^mq$ (so which is of torsion) in $H$ such that the image by the morphism given by the fact that $H$ is residually a $p$ -group is of order $p^n, n \leq m$ as expected ? Then we could have a periodic residually $p$ -group which isn't a $p$ -group ?","['group-theory', 'abstract-algebra', 'p-groups']"
4840143,"Equivalence relations, formal languages, and hypercube walks - what did I unleash on my students?","I teach a class in discrete mathematics and formal language theory. On my most recent final exam, I asked a question that involved a crossover between equivalence relations and formal languages. Here's the setup. Recall that an equivalence relation over a set $A$ is a set $R \subseteq A^2$ with these three properties: $$\begin{aligned}\text{(I)}&\quad \forall x \in A. (x, x) \in R \\ \text{(II)}&\quad \forall x \in A. \forall y \in A. ((x, y) \in R \to (y, x) \in R) \\ \text{(III)}&\quad \forall x \in A. \forall y \in A. \forall z \in A. ((x, y) \in R \land (y, z) \in R \to (x, z) \in R)\end{aligned}$$ Based on this, here's a new definition. An equivalence language over an alphabet $\Sigma$ is a language $L \subseteq \Sigma^\star$ with the following three properties: $$\begin{aligned}\text{(I)}&\quad \forall x \in \Sigma^\star. xx \in L \\ \text{(II)}&\quad \forall x \in \Sigma^\star. \forall y \in \Sigma^\star. (xy \in L \to yx \in L) \\ \text{(III)}&\quad \forall x \in \Sigma^\star. \forall y \in \Sigma^\star. \forall z \in \Sigma^\star. (xy \in L \land yz \in L \to xz \in L)\end{aligned}$$ Essentially, this is what you get if you translate $(x, y) \in R$ to $xy \in L$ , hence the name ""equivalence language."" On the exam itself I asked a question that isn't relevant here (it was basically a standard equivalence relation proof translated into formal language theory). After the exam, though, I started wondering about what sorts of languages these equivalence languages happen to be. What do they look like? What properties do they have? To begin with, it's not too hard to show the following. Every equivalence language $L$ contains the empty string $\varepsilon$ (apply property (I) to $\varepsilon$ ). Every equivalence language $L$ is closed under concatenation. Specifically, if $x \in L$ and $y \in L\text,$ then because $x\varepsilon \in L$ an $\varepsilon y \in L$ by property (III) we have $xy \in L\text.$ Restating (1) and (2), every equivalence language $L$ over $\Sigma$ is a submonoid of the free monoid $\Sigma^\star\text.$ Suppose we let $ER(\Sigma)$ denote the class of all equivalence languages. Then $ER(\Sigma)$ is closed under infinite union. In particular, the language $\bigcap ER(\Sigma)$ is itself an equivalence language, and is the minimal equivalence language over $\Sigma$ from an order-theoretic perspective. This last property is the source of my ultimate question, which is the following: Question (v1): What is $\bigcap ER(\Sigma)$ ? I have a limited answer to this question in the specific case where $\Sigma$ is small. Let $\Sigma$ be a language and let $Z(\Sigma) = \{ w \in \Sigma^\star \ | \ \text{every character in } \Sigma \text{ appears an even number of times in } w \}\text.$ So, for example, $\mathtt{ababcc} \in Z(\{\mathtt{a}, \mathtt{b}, \mathtt{c})$ . It intuitively makes sense that strings of this form will be in $\bigcap ER(\Sigma)$ because property (I) immediately gives us a bunch of strings that have even character counts. My conjecture is the following: Conjecture: $\bigcap ER(\Sigma) = Z(\Sigma)$ . I'm able to prove the following: Theorem: If $|\Sigma| \le 2\text,$ then $\bigcap ER(\Sigma) = Z(\Sigma)\text.$ Proving this isn't too hard. First show that the language $Z(\Sigma)$ is an equivalence language, then show that every element of $Z(\Sigma)$ belongs to every equivalence language. In the case where $|\Sigma| = 1$ this isn't too tricky to do. I thought it would be informative to include my proof for the case of $|\Sigma| = 2$ here, because it relies on a point that I'll need later on in this question. Proof for $|\Sigma| = 2$ : First check that indeed $Z(\Sigma)$ satisfies the three rules for equivalence languages (it does; this proof is independent of $|\Sigma|$ so I'll skip it here). Next we need to show that if $L$ is an equivalence language, then $Z(\Sigma) \subseteq L\text.$ So choose some arbitrary equivalence language $L$ . We're assuming $|\Sigma| = 2\text,$ and assume WLOG that $\Sigma = \{\mathtt{a}, \mathtt{b}\}$ . We need to show that $Z(\Sigma) \subseteq L\text,$ and we'll do that by induction on the length of the strings in $Z(\Sigma)$ . First, our base cases: strings of lengths 0, 2, and 4. Note that $\varepsilon \in Z(\Sigma)$ and that $\varepsilon \in L$ as well (as described above). Next note that $\mathtt{aa}$ and $\mathtt{bb}$ are the only length-two strings in $Z(\Sigma)$ , and each belongs to $L$ by property (I) of equivalence languages applied to $\mathtt{a}$ and $\mathtt{b}$ . There are eight strings of length four in $Z(\Sigma)$ : $\mathtt{aaaa}$ , $\mathtt{aabb}$ , $\mathtt{abba}$ , $\mathtt{baab}$ , $\mathtt{bbaa}$ , $\mathtt{abab}$ , $\mathtt{baba}$ , and $\mathtt{bbbb}$ . Of these, $\mathtt{aaaa}$ , $\mathtt{bbbb}$ , $\mathtt{abab}$ , and $\mathtt{baba}$ can be shown to be in $L$ via property (I) of equivalence languages. $\mathtt{aabb}$ and $\mathtt{bbaa}$ are in $L$ because $\mathtt{aa}$ and $\mathtt{bb}$ are in $L$ and $L$ is closed under concatenation. $\mathtt{baab}$ and $\mathtt{abba}$ can be shown to be in $L$ because $\mathtt{aabb}$ is in $L$ and these two strings are rotations of that string, which by property (II) are in $L$ . For our inductive step suppose that all strings of lengths $k$ and $k+2$ in $Z(\Sigma)$ are in $L$ . (We only need to worry about even lengths, so there are no strings of length $k+1$ in $L$ .) We'll show that all strings of length $k+4$ are in $L$ . Pick any string $w \in Z(\Sigma)$ of where $|w| = k+4$ . We consider two cases. Case 1: $w$ starts or ends with $\mathtt{aa}$ or $\mathtt{bb}$ . Assume WLOG that $w = \mathtt{aa}z$ for some string $z$ . We know $|w| = k+4$ , so this means $|z| = k+2$ . We can check that $z$ has an even number of copies of each character ( $w$ has an even number of copies of each character and we just dropped $\mathtt{aa}$ ), so $z \in Z(\Sigma)$ . Therefore by our IH we know $z \in L$ . We also know $\mathtt{aa} \in L$ from our base case, and since equivalence languages are closed under concatenation we know $\mathtt{aa}z = w \in L\text.$ Case 2: $w$ doesn't start or end with $\mathtt{aa}$ or $\mathtt{bb}$ . Then $w$ both starts with either $\mathtt{ab}$ or $\mathtt{ba}$ and ends with either $\mathtt{ab}$ and ends with $\mathtt{ba}$ . Assume WLOG that $w = \mathtt{ab}z\mathtt{ab}$ for some string $z$ of length $k$ . As above, $z \in Z(\Sigma)$ , so by our IH $z \in L$ . We also know that $\mathtt{abab} \in L$ from our base case, so $\mathtt{abab}z \in L$ as well. Applying Property (II) of equivalence languages to this string gives $\mathtt{ab}z\mathtt{ab} = w \in L\text.$ This means we always have $w \in L\text,$ as required. $\blacksquare$ Notice that the logic in the inductive step is heavily specific to the fact that we're working with a two-character alphabet, where it's possible to enumerate all possible ways that a string can start or end. But I don't see a nice way to generalize this to work with larger alphabets. While I'm conjecturing that $\bigcap ER(\Sigma) = Z(\Sigma)$ , I'm not fully convinced this is the case. For example, I don't see how to get $\mathtt{abcacb} \in Z(\Sigma)$ using the rules for equivalence languages plus the other properties listed above. All this is to say that a refined version of my question is as follows: Question (v2): Is the conjecture that $\bigcap ER(\Sigma) = Z(\Sigma)$ true? And now the plot thickens. While thinking about how I might prove this, I realized that there's a close connection between equivalence languages, $Z(\Sigma)$ , and walks in a hypercube graph. Specifically, given an alphabet $\Sigma$ where $|\Sigma| = n\text,$ think of an $n$ -dimensional hypercube and assign each axis a unique character from $\Sigma$ . We can interpret a string over $\Sigma$ as a series of instructions about how to walk around the hypercube. Each character means ""the next step is to walk on the edge along the axis associated with this character."" Every string in $Z(\Sigma)$ therefore corresponds to a walk in a hypercube that ends where it begins (e.g. a closed walk), since if there's an even number of copies of each character we ""undo"" every step we take. We can also check that the three rules for equivalence languages, applied to strings that are closed walks in a hypercube, preserve this property. Specifically: Property (I) says that any series of directions, repeated twice, returns home. Property (II) says that cyclically shifting a closed walk leaves a closed walk. Property (III) says that if one closed walk ends with the same sequence that a second closed walk starts with, we can ""cancel out"" the matching parts and glue the remaining steps together to get a closed walk. It's clear that these rules will give rise to closed walks and only closed walks, but it's not clear that these rules are sufficient to generate all possible closed walks on a hypercube. This connection between equivalence languages, $Z(\Sigma)$ , and hypercube walks gives rise to the final version of my question: Question (v3): What walks on a hypercube do you get if you begin with walks generated by property (I) and then repeatedly apply rules (II) and (III)? I'm curious for an answer to any version of the question. All of this is to say - I accidentally invented something much more nuanced than I anticipated when writing my final exam, and I'd like some clarity about what exactly I just unleashed on my students. :-)","['graph-theory', 'equivalence-relations', 'formal-languages', 'discrete-mathematics']"
4840165,Operational Calculus for Homogeneous ODE,"I watched a Youtube vedio on operational calculus which is really interesting. There is an example on solving Homogeneous ODE which I transcribe below: $$
\begin{align*}
& y-\frac{dy}{dx}=x^3 \\
\Rightarrow\quad& y-\mathcal{D}y=x^3 \\
\Rightarrow\quad& (1-\mathcal{D})y=x^3 \\
\Rightarrow\quad& (1-\mathcal{D})y=x^3 + 0 \\
\Rightarrow\quad& y=(1-\mathcal{D})^{-1}x^3+(1-\mathcal{D})^{-1}0 \\
\Rightarrow\quad& y=(1+\mathcal{D}+\mathcal{D}^2+\mathcal{D}^3+\cdots)x^3+(1-\mathcal{D})^{-1}0 \\
\Rightarrow\quad& y= x^3+3x^2+6x+6+(1-\mathcal{D})^{-1}0 \\
\Rightarrow\quad& y= x^3+3x^2+6x+6+ce^x
\end{align*} 
$$ However I don't understand the last step of the above (on 10:54 of the vedio ), which is: $$(1-\mathcal{D})^{-1}0=ce^x$$ Could any one help to explain? Many thanks. Edit 1: To clarify my question, I understand that it is easy to verify $(1-\mathcal{D})e^x=0$ . But let's say I don't want to guess the solution, but would like to follow a similar spirit of the operational calculus, namely using the inverse of the operator. Obviously, below argument doesn't make sense: $$(1-\mathcal{D})^{-1}0=(1+\mathcal{D}+\mathcal{D}^2+\mathcal{D}^3+\cdots)0=ce^x$$","['operator-theory', 'ordinary-differential-equations']"
4840169,Does Newton's method converge for all polynomials?,"I made an implementation of Newton's method (starting at $x=\pi$ ) for polynomials in the programming language Uiua ( here it is if you are interested, but be warned that Uiua looks weird). It works fine for most polynomials but there are a few that it gets stuck on. I tried [12 ¯9 ¯21 37 ¯41 ¯10 31 49] , (a degree 7 polynomial with coefficients listed from lowest degree to highest) and the execution time ran out. Inspecting further, it seems to be oscillating somewhat randomly in the range $[0.3,0.7]$ , even after 500+ iterations, when the actual answer is $-0.568$ . Another online implementation also fails to converge when starting at $x=\pi$ . My question is whether Newton's method should be working in this case or if I should expect to find there is some problem in my code or in the way that Uiua handles floats. Does Newton's method always converge for polynomials?","['calculus', 'convergence-divergence', 'polynomials', 'roots']"
4840198,"Showing $c^2+a^2= \csc^2\theta$, given $a^2+b^2+2ab \cos\theta=1$, $c^2+d^2+2cd \cos\theta=1$, $(ac+bd)+(ad+bc)\cos\theta=0$","Given that $$\begin{align}
a^2+b^2+2ab \cos\theta &=1 \\
c^2+d^2+2cd \cos\theta &=1 \\
(ac+bd)+(ad+bc)\cos\theta &=0
\end{align}$$ Prove that $c^2+a^2 = \csc^2\theta$ I tried to solve this question by separating $\cos\theta$ in all the three equations, hoping to square it and then obtain $\csc^2\theta$ . Got stuck. Then I tried extracting $\cos\theta$ from the third equation, squaring it and finding $\csc^2\theta$ and somehow manipulating the first two equations to arrive at $c^2+a^2$ . That failed too. The equations somewhat resemble the cosine rule, but I just can't figure out how to manipulate them with so many variables involved. Please enlighten me.","['trigonometry', 'systems-of-equations']"
4840205,Induced connection 1-form on orthogonal frame bundle by the Levi-Civita connection on tangent bundle,"I recently posted a similar question but I have deleted that one and replaced it with this one, which is hopefully more focused. On page 317 of this paper by Pu-Young Kim and Joon-Sik Park they say It is well known that, in orthonormal frame bundle $O(M)$ of a Riemannian manifold $(M,g)$ , the connection form defined by the
Levi-Civita connection becomes a connection of principal fibre bundle $O(M)$ . I could not find a reference for this well known result so I am trying to figure it out on my own. Suppose $M$ has dimension $n$ . The Levi-Civita connection is a connection on $TM$ , and $TM$ can be viewed as an associated vector bundle whose fiber is isomorphic to $\mathbb{R}^n$ with structure group $O(n, \mathbb{R})$ . I am also familiar with Ehresmann connections on principal bundles, and in this case the principal bundle of interest is the $O(M)$ frame bundle. I am having trouble connecting the Levi-Civita connection to the associated vector bundle and then connecting that to a connection on the $O(M)$ bundle. So far what I've thought of is that we know every vector bundle has a metric connection, so if we can somehow introduce a torsion-free condition to the associated vector bundle then we will have a Levi-Civita connection which we can hopefully show is unique. But what does this have to do with the Levi-Civita connection on $TM$ ? And how does all of this induce a Ehresmann connection on the frame bundle $O(M)$ ?","['principal-bundles', 'connections', 'vector-bundles', 'differential-geometry']"
4840279,Are the nonmaximal point in $\operatorname{Spec} A$ unimportant in some situations?,"(I don't know the best way to set up the context of this question, so I am just going to say whatever I heard people keep saying, according to my poor memory.) Let $k$ be an algebraically closed field. An affine $k$ -variety is the space $\operatorname{Spec} A$ for some finitely generated integral domain $A$ over $k$ . (Different people have different conventions for affine variety, so let's not pay too much attention to that.) In this context, we often ignore points in $\operatorname{Spec} A$ that are not maximal ideals. For example, this means we can identify $\operatorname{Spec} k[x_1, \cdots, x_n]$ with points in $k^n$ by the Hilbert Nullstellensatz and $\operatorname{Spec} k[x_1^{\pm 1}, \cdots, x_n^{\pm 1}]$ with the algebraic torus $(k^\times)^n$ . Honestly, I am not confident that we can always do that, but yet from my experience of hearing other people's talks, this seems to be quite a common thing to do/assume. Intuitively, every ideal is included in some maximal ideal, so this ignorance kinda makes sense, but I never recall Vakil doing that in his FOAG (maybe he does but I just forgot). I guess my (rather broad) question for today is that: can we always do that? If not, then why do people keep ignoring nonmaximal ideals in certain settings (and in which settings are this ignorance permissible)? When we write proofs, how do we even make this precise, because setwise you really can't identify $\operatorname{Spec} k[x_1, \cdots, x_n]$ with $k^n$ , as $k[x_1, \cdots, x_n]$ do contains nonmaximal ideals?",['algebraic-geometry']
4840303,"If $(a,b,c)$ are the sides of a triangle and $x \ge 1$, what is the probability that $a+b > cx$?","I am trying to generalize the triangle inequality in probabilistic terms. Assume that the vertices are uniformly distributed around the circumference of a fixed circle. In this question it was proved that if $(a,b,c)$ are the sides of a triangle than the probability that geometric mean of any two sides is greater than the third side i.e. $P\left(\sqrt{ab} > c\right) = \frac{2}{5}$ . Since $a+b > 2\sqrt{ab}$ , it implies that probability that $P(a+b > 2c) > \frac{2}{5}$ . In general, we can ask: Question : If $(a,b,c)$ are the sides of a triangle and $x \ge 1$ , what is the probability that $a+b > cx$ ? Equivalence with the Basel's Problem : The series in the accepted answer is actually the Lengendre Chi function $\chi_2\left(\frac1{x}\right)$ ; hence $$
P(a+b > cx) = \frac{8}{\pi^2}\chi_2\left(\frac1{x}\right)
$$ Taking $x = 1$ and applying the fact that the sum of the reciprocal of odd squares is $3/4$ -th the sum over the reciprocal of the squares of natural numbers we find that the probability that the sum of any two sides of a triangle is greater than than the third side is $\displaystyle \frac{6\zeta(2)}{\pi^2}$ . Since this probability must be $1$ and the proof does not require the value of $\zeta(2)$ to be known in advance, it unexpectedly shows that: The triangle inequality equivalent to the Basel Problem $$ 1 + \frac{1}{2^2} +
\frac{1}{3^2} + \cdots = \frac{\pi^2}{6} $$ I have changed the title to reflect this remarkable connection. Related question : If $(a,b,c)$ are the sides of a triangle, is it true that probability that $a+b > c^{\frac{3}{c}}$ is $\zeta(2)-1$ ?","['integration', 'euclidean-geometry', 'probability-distributions', 'geometry', 'probability']"
4840309,$f(S)=S^{-1}$ is continuous where $S:E\rightarrow E$ is an isomorphism between Banach spaces.,"Let $E$ be a Banach Space and $f:Iso(E,E)\rightarrow Iso(E,E)$ be given by $f(S)=S^{-1}$ . Prove that $f$ is continuos. Hint: $f(I-T)=\sum_{n=0}^\infty T^n$ if $\lVert T\rVert<1$ . I was able to solve this without the hint as follows: $$\lVert S^{-1}-S_o^{-1}\rVert=\lVert S^{-1}S_oS_o^{-1}-S_o^{-1} \rVert\leq \lVert S^{-1}S_o-I \rVert \lVert S_o^{-1} \rVert \leq  \lVert S^{-1}S_o-S^{-1} S \rVert \lVert S_o^{-1} \rVert\Rightarrow $$ $$\lVert  S^{-1}-S_o^{-1}\rVert\leq \lVert S^{-1}\rVert\lVert S_o^{-1}\rVert\lVert S-S_o\rVert$$ So we need to find a nice bound for $\lVert S^{-1} \rVert$ . At first I noticed: $$\lVert S^{-1} \rVert=\sup_{x\not=0}\frac{\lVert S^{-1}(x)\rVert}{\lVert x\rVert}=\sup_{v\not=0}\frac{\lVert S^{-1}(S(v))\rVert}{\lVert S(v)\rVert}=\sup_{v\not=0}\frac{\lVert v\rVert}{\lVert S(v)\rVert}=$$ $$\left(\inf_{v\not=0}\frac{\lVert S(v)\rVert}{\lVert v\rVert}\right)^{-1}=\frac{1}{\inf_{\lVert v\rVert =1}\lVert S(v)\rVert}$$ It is straightforward to see that for $\lVert v \rVert=1$ : $$\lVert S (v) \rVert\geq \lVert S_o(v)-(S_o-S) (v) \rVert \geq\lVert S_o(v) \rVert-\lVert S-S_o\rVert\Rightarrow$$ $$ \inf_{\lVert v\rVert=1}\lVert S (v) \rVert \geq \inf_{\lVert v\rVert=1}\lVert S_o (v) \rVert - \lVert S-S_o\rVert\Rightarrow$$ $$ \frac{1}{\lVert S^{-1} \rVert} \geq \frac{1}{\lVert S_o^{-1}\rVert} - \lVert S-S_o\rVert\Rightarrow\lVert S^{-1}\rVert\leq \frac{\lVert S_{o}^{-1}\rVert}{1-\lVert S_o-S \rVert\lVert S_o^{-1}\rVert}$$ Combining this bound for $S$ with our initial inequality yields: $$\rVert S^{-1}- S_o^{-1} \rVert \leq  \lVert S_o- S \rVert\lVert  S^{-1}\rVert \lVert S_o^{-1} \rVert\leq \frac{\lVert S_{o}^{-1}\rVert^2\lVert S-S_o \rVert}{1-\lVert S_o-S \rVert\lVert S_o^{-1}\rVert}$$ And from this it is clear that if $S\in Iso(E,E)$ and $\rVert S- S_o \rVert\leq \delta(\varepsilon)\Rightarrow \rVert S^{-1}- S_o^{-1} \rVert\leq\varepsilon$ . I wonder how the hint comes into play and if my proof avoiding the hint is correct.","['alternative-proof', 'solution-verification', 'functional-analysis']"
4840310,Vague convergence of dirac measure,"Task . Let $x_n$ be a series in $\mathbb R^d$ . When does $\mu_n = \delta_{x_n}$ converge a) vaguely and when does it converge in b) variation distance? Regarding a) . We want to have $$\int f \, d \mu_n = f(x_n) \rightarrow \int f \, d \mu$$ for all $f: \mathbb R^d \rightarrow \mathbb R$ continuous with a compact support, where $\mu$ is some probability measure. But I don't really know how to continue from here. My conjecture is that such a $\mu$ exists if and only if $x_n$ converges against some $x$ . The direction "" $\Leftarrow$ "" is trivial because $f$ is continuous. But how about the other direction? Regarding b) . So I am not certain if I am allowed to use the formula from our lecture ""for the discrete case"" or not. The dirac measure is discrete but how about the convergence measure $\mu$ ? However, if I am allowed to use that formula, the following is my approach. We want: $$d_{\text{TV}}(\mu_n,\mu)= \frac 1 2 \sum_{x \in \mathbb R^d} |\mu_n(\{x\})-\mu(\{x\})| \rightarrow 0$$ Which we can write as: $$|1-\mu(\{x_n\})| + \sum_{x \in \mathbb R^d, x\neq x_n}|\mu(\{x\})| \rightarrow 0$$ Sice each term in this is $\geq 0$ , each of them must converge to $0$ . But that means that $\mu$ is 1 on ${x_n}$ and $0$ everywhere else, meaning it is a dirac measure of some $x$ with $x_n \rightarrow x$ uniformly.","['statistics', 'analysis', 'real-analysis', 'convergence-divergence', 'probability-theory']"
4840356,"Calculate $\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}}),$ for Brownian motion.","Calculate the limit $$\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}}),$$ where $B_t$ - standard Brownian motion. $\lim_{t\to \infty} \mathbb E(1-e^{-e^{B_t}})=1-\lim_{t\to \infty} \mathbb E(e^{-e^{B_t}})$ . I think that next we should use the property of Brownian motion. Perhaps it is useful to know that $B_t \sim \mathcal N(0,t)$ or that $B_t \sim \sqrt t W_0$ . However I have a problem to find this limit.","['limits', 'brownian-motion', 'stochastic-processes']"
4840372,Calculate $\mathbb EB_1B_2^2 B_3^3$ for Brownian motion,"Calculate $\mathbb EB_1B_2^2 B_3^3$ cleverly (expressing the result, e.g. in terms of $\mathcal N(0,1)$ moments), where $B_t$ is standard Brownian motion. I know that in tasks like this, it's often useful to use $W_s = W_t + (W_s - W_t)$ (like for example here ). However, I don't know if my way is correct: $$\begin{align}\mathbb E(B_1B_2^2B_3^3) &= \mathbb E(B_1B_3)\mathbb E(B_2^2)\mathbb E(B_3^2) \\ &= \mathbb E \left[ -\frac 12 (B_1-B_3)^2 + \frac 12 (B_1^2+B_3^2) \right] \cdot \mathcal N(0,2)\cdot \mathcal N(0,3) \\ &= \left[ -\frac 12 \mathbb E(B_1-B_3)^2 + \frac 12 \mathbb E (B_1^2)+\frac 12 \mathbb E(B_3^2) \right] \cdot \mathcal N(0,2)\cdot \mathcal N(0,3) \\ &= 
 \left[ - \frac 12 \mathcal N(0,3-1)+\frac 12 \mathcal N(0,1)+ \frac 12 \mathcal N(0,3) \right) \cdot \mathcal N(0,2)\cdot \mathcal N(0,3)\end{align}$$","['stochastic-processes', 'brownian-motion', 'probability']"
4840383,Enemy of an enemy relation,"I'm going through the book ""How to prove it"" by Velleman and I have come across the following question: ""Let E = {(p, q) ∈ P × P | the person p is an enemy of the person q}, and F = {(p, q) ∈ P × P | the person p is a friend of the person q}, where P is the set of all people. What does the saying ""an enemy of one’s enemy is one’s friend"" mean about the relations E and F?"" In attempting to solve this problem I let E be the set {(J, M), (B, M)}. Where J, M and B are people named John, Bill and Mark. I assume this would then mean that E -1 ∘ E = F. But the answer key gives a different solution: E ∘ E ⊆ F (answer key) While I agree that a subset of F is correct (one can have friends that are not enemies of enemies). I find the choice of E ∘ E peculiar, after some further reasoning this only seems to be true if both the pairs (J, M) and (M, J) are included. But it's certainly possible to be one's enemy without that being the opposite way around unless I am misunderstanding something here? Any input is appreciated.","['elementary-set-theory', 'relations', 'function-and-relation-composition']"
4840403,Prove that $f(x) = \|x\|^2$ is smooth using the definition of total derivative,"Consider the function $f : \Bbb R^n \to \Bbb R$ given by $f(x) = \|x\|^2$ ,
for $x \in \Bbb R^n$ . Here $\|.\|$ is the Euclidean norm on $\Bbb R^n$ . Answer the following questions from the definition of the (total) derivative, i.e. without using partial derivatives. (a) Prove that $f$ is differentiable and calculate its derivative. (b) Prove that $Df$ is differentiable and calculate its derivative. (Hint:
identify $Lin(\Bbb R^n; \Bbb R)$ with $\Bbb R^n$ via the inner product.) (c) Prove that $f$ is smooth My attempt: (a) Using the definition of total derivative $f(a+v)=\|a+v\|^2=\|a\|^2+\|v\|^2 + 2a^Tv=f(a)+\|v\|^2 + 2a^Tv$ $\lim_{v\to  0}\frac{|f(a+v)-f(a)-Lv|}{\|v\|}=\lim_{v\to  0}\frac{|\|v\|^2 + 2a^Tv-Lv|}{\|v\|}=0 $ if I take $L=2a^T$ ,which is a linear map $L(x)=2a^Tx, x \in \Bbb R^n$ . So $f$ is differentiable at $a$ , and therefore differentiable (meaning at any point) and the derivative is $Df(a)=2a^T$ i.e. $Df(a)(h)=2a^Th, h \in \Bbb R^n$ (b)
I think they should ask about the derivative of Df(a) and not about the derivative of Df which is another thing (the map that assignes to each point the linear map called derivative at that point. the derivative of that map should be 0 since it is always the same map for every point) So let $g:=Df(a)=2a^T$ I want to calculate the derivative of $g $ at some point $\tilde a\in\Bbb R^n$ ie $Dg(\tilde a)$ , for this I need a $v\in\Bbb R^n $ in the definition $\lim_{v\to  0}\frac{|g(\tilde  a +v)-g(\tilde a)-Lv|}{\|v\|}$ $=\lim_{v\to  0}\frac{|Df(a)(\tilde a+v)-Df(a)(\tilde a)-Lv|}{\|v\|}$ $=\lim_{v\to  0}\frac{|Df(a)(\tilde a)+Df(a)(v)-Df(a)(\tilde a)-Lv|}{\|v\|}$ $=\lim_{v\to  0}\frac{|Df(a)( v)-Lv|}{\|v\|}$ if I take $ L=Df(a)$ ,which is a linear map $L(h)=Df(a)h, x \in \Bbb R^n$ I get that $D^2f(\tilde a)=D(Df(a))(\tilde a)=D(g)(\tilde a)=L=Df(a) \tag {*} $ My questions are (1) I think (a) is OK. What about (b) ? I am specially unsure about the correctness of the notation at (*).Was I right to fix the question? Should they have asked about the derivative of Df(a) ? I thought so because that is what I plugged in in the definition of total derivative at (b) in order to get the correct answer as stated above. This was my earlier attempt: In (a) I plugged in $f $ in the definition, and got the right answer In (b) I was tempted to blindly do the same and plug in $g=Df$ ,because they are asking to assess the differentiability of $Df$ but that was resulting in a absurd: In the denominator I was getting $|g(\tilde a+v)-g(\tilde a)-Lv|=|Df(\tilde a+v)-Df(\tilde a)-Lv|$ (and usign that $Df(a)=2a^T$ :) $=|2 (\tilde a+v)^T-2\tilde a^T-Lv|=|2v^T-Lv|$ which is absurd This is what the definition yields if I apply it verbatim. Therefore I changed the question and though that it should $Df(a)$ instead of $Df$ Moreover I am not sure I am using the hint here, am I? (2) How do I prove (c) ? I am unsure how to do this since I am not supposed to use partial derivatives. The definitions I am supposed to use are those in Appendix C of Lee's introduction to smooth manifolds, where smooth was  defined with partial derivatives as seen below: definition of smooth function: definition of total derivative: Edit: Rewriting (b) following the suggestions: $g=Df$ $|g(\tilde a+v)-g(\tilde a)-Lv|=|Df(\tilde a+v)-Df(\tilde a)-Lv|$ (and usign that $Df(a)(u)=2a^Tu=<a,u>$ but here I am unsure how to write Df(a)=? I can only thik of 2a^T, but I was advised to use Df(a)(u)=<a,u> so that I don't have to worry about the transpose but I am not sure how I can write Df(a) using <,> and not writting u, maybe $Df(a)=<a,\cdot>$ ) $=|2 <\tilde a+v,\cdot>-2<\tilde a,\cdot>-Lv|=|2<v,\cdot>-Lv|$ then $\lim_{v\to 0}\frac{|2<v,\cdot>-Lv|}{\|v\|}=0$ if $Lv=2<v,\cdot>$ which is a linear map $v \mapsto 2<v,\cdot> $ , But I was expecting to find the constant map Lv=2 instead,... how do I fix this?","['multivariable-calculus', 'linear-algebra', 'smooth-manifolds', 'differential-geometry']"
4840418,"Prerequisites to begin Beauville's ""Complex Algebraic Surfaces""","I'm going to read through the first three chapters of Arnaud Beauville's Complex Algebraic Surfaces . My background in algebraic geometry only consists of Gathmann's algebraic geometry notes (the part before schemes are introduced), but the point of view is quite different: Gathmann is more on the algebraic side, oriented towards schemes; instead Beauville has a more classical point of view, coming from complex geometry. For example, I don't know about complex manifolds, divisors, singularities, which seem to be prerequisites for Beauville's book. Is there any introduction to complex geometry / complex surfaces that you would recommend, in order to move on Beauville's book later? Thank you","['complex-geometry', 'algebraic-geometry', 'book-recommendation', 'reference-request']"
4840428,"Does there exist an entire function such that it transforms the real axis in the imaginary axis, and the imaginary axis into the parabola $y=x^2$","I have been trying to solve this question for my complex analysis class, its from another year exam. I attempted to use the Cauchy-Riemann equations to prove that there is non, but having $f(x+iy)=u(x,y)+iv(x,y)$ results in having either $x=0$ or $y=0$ in my inputs of the function. I end up having that: $u(x,0)=0$ $v(x,0)$ is a surjective function $u(0,y)^2=v(0,y)$ $u(0,0)=v(0,0)=0$ as it is the intersection of the images Then I attempted to prove that either $u$ or $v$ are not harmonic functions, but that led me nowhere for the same reason. After trying multiple things I reach nowhere. NOTE: An entire function is a holomorphic function in all the points of the complex plane.","['complex-analysis', 'analysis']"
4840464,solution for second order differential equation,"Please support me on this problem. I have tried so hard but couldn't get the answer $$
yy'' + yy' + 1 =0
$$ Thank you very much. The original equation can be simplified to $$y''+y'+\frac{1}{y}=0\tag{1}$$ using $u=\frac{dy}{dx}$ and $y''=\frac{du}{dx}=u\frac{du}{dy}.$ Then Eqn. $(1)$ : $$u\frac{du}{dy}+u+\frac{1}{y}=0.\tag{2}$$ Eqn. $(2)$ can be reduced to $\frac{du}{dy}=-1-\frac{1}{uy}$ , then $$du=-dy- \frac{dy}{uy}.\tag{3}$$ Integrate both side of eqn. $(3)$ : $u=-y- \int \frac{dy}{uy}.$ I am stuck right here. Anyone can help me. Thank you.",['ordinary-differential-equations']
4840467,"Coupon Collector ""Paradox""","Example Scenario: Imagine a Player has an unfair die with sides: $\{A,B,C,D,E,F\}$ The probability of each side $p_i = \{1/12,1/6,1/4,1/12,1/6,1/4\}$ The player's goal is to obtain at least one of each $S = \{A,B,C\}$ Questions: The Player wishes to answer 3 questions about the game. What is the Expected Number of die rolls until obtaining the 1st successful roll? What is the Expected Number of desired sides seen at least once in $n$ rolls? What is the Expected Number of die rolls to complete the collection? My Solutions to the Questions: Using a Geometric Distribution approach we have that the expected waiting time until the 1st success is $$E[x] = 1/p$$ where $$p = \sum_{i\in S} p_i$$ therefor $$p = 1/12 + 1/6 + 1/4 = 1/2$$ and $$E[x] = 1/(1/2) = 2$$ I believe this solution can be obtained from the the inclusion-exclusion principle: $$E[x] = \sum_{i\in S} 1-(1-p_i)^n$$ thus for $$n = 2$$ $$E[x] = 1-(1-1/12)^2 + 1-(1-1/6)^2 + 1-(1-1/4)^2 \approx 0.902777778$$ This answer comes from Ross; Introduction to Probability Models as the solution to the Unequal Coupon Collector Problem: $$\int_0^\infty1-(1-e^\frac{-x}{12})(1-e^\frac{-x}{6})(1-e^\frac{-x}{4})dx = 14.6$$ My ""Paradox"": I am not 100% confident in the solution to question 2. I do not know how to prove that formula is correct, but feels correct to me. However, the confusing thing to me is that under the $n = 2$ scenario why does the result not equal 1.0? It feels to me like the result for that question should either equal 1.0 to match the solution to question 1 since they feel like the same question just from two different view points. Or if the result was slightly higher than 1.0 I could also reason to my self that the excess is attributable to the scenarios where you obtain 2 successes in 2 rolls. But with the solution of $\approx 0.9$ I do not have an intuitive understanding of why that is true. So I am asking if my solution to question 2 is correct. And if it is, is there an intuitive explanation for why the solution to question 2 for when n = solution from question 1 the result is not 1.0?","['expected-value', 'statistics', 'paradoxes', 'coupon-collector']"
4840471,Theorems about periodic functions,"I'm trying to prove something about periodic functions and I'd need someone to tell me if what I wrote is right! If $f$ is a periodic function with fundamental period $\tau$ . Then, all periods of $f$ are  in the form $k\tau$ , with $k$ in $\mathbb{Z}$ Obviously, if $\tau$ is the fundamental period of $f$ , $k\tau$ is too: \begin{equation*}
	f(x+k\tau)=f(x+\underbrace{\tau+\tau+\dots+\tau}_{k \text{ times}})=f(x+\underbrace{\tau+\tau+\dots+\tau}_{k-1 \text{ times}})=\dots=f(x+\tau)=f(x)
\end{equation*} Viceversa, let's suppose that a period different from $k\tau$ exists, and let's call it $\beta>\tau$ .
Hypothesis tell us that $\frac{\beta}{\tau}\neq k$ , and so $\frac{\beta}{\tau}$ is not an integer. We have  two possibilities: $\frac{\beta}{\tau}$ is rational, but not integer; $\frac{\beta}{\tau}$ is irrational. In the first case, it could be: $\beta$ and $\tau$ are two irrational numbers that gives a rational when divided: $\beta=s\tau$ with $s\in\mathbb{Q}\verb|\| \mathbb{Z} $ $\beta$ and $\tau$ are two integers without factors in common. Let's analyze case by case If $\beta=s\tau$ with $s$ like I said, $\beta$ is not a period. In fact, even if $\tau$ is a period of $f$ , $\beta=s\tau$ isn't anymore. We can consider $\sin(\pi+2\pi)=\sin(\pi)=0\neq\sin(\pi+\frac{1}{3}2\pi)$ for a counterexample. If $\beta$ and $\tau$ are two integers without factors in common, we cand divide $\beta$ by $\tau$ , finding the quotient $q$ and the remainder $r$ so that: \begin{equation*}
			\beta=q\cdot\tau+r
		\end{equation*} with $0<r<\tau$ .
Now, for hypothesis $\beta$ is a period, and so: \begin{equation*}
		f(x)=f(x+\beta)=f(x+q\cdot\tau+r)=f(x+r)
\end{equation*} That means that $r$ too is a period, but this is absurd because $\tau$ is the minimum  period and $r<\tau$ . If $\frac{\beta}{\tau}=t$ with $t$ irrational, $\beta=\tau t$ isn't a period anymore (it's possible to create a counterexample like done before).
So, if $\beta\neq k\tau$ , $\beta$ isn't a period, therefore $\beta=k\tau$ , with $k\in\mathbb{Z}$ Now let's use this result to show that:
If $f$ and $g$ are periodic functions with fundamental period $s$ and $t$ respectively, then if: $i)$ $\frac{s}{t}$ is a rational number $\neq1$ , $f+g$ , $fg$ , $f/g$ are periodic functions with period $mcm(s,t)$ . $ii)$ $s=t$ , $f+g$ , $fg$ , $f/g$ are periodic functions and their period is $\leq s=t$ ; $iii)$ $\frac{s}{t}$ is irrational, $f+g$ , $fg$ , $f/g$ are not periodic functions. Here we extend the notion of $mcm$ to real number as it follow: \begin{equation*}
		z=mcm(\alpha,\beta) \iff \exists m,n \in \mathbb{Z} : \begin{cases}
			\alpha=m\cdot z\\
			\beta=n\cdot z
		\end{cases}
		\end{equation*} $i)$ If $\frac{s}{t}=k$ is rational,  we have $\frac{s}{t}=\frac{m}{n} \implies sn=mt$ , with $s$ and $m$ integers. $mcm(s,t)=sn=mt$ . Let's see if $sn$ is a period for $f+g$ , $fg$ e $f/g$ .
We have: \begin{equation*}
	\begin{aligned}
		&(f+g)(x+m)=f(x+m)+g(x+m)=f(x+n\cdot kt)+g(x+l\cdot 	t)=f(x)+g(x)=(f+g)(x)\\
		&(fg)(x+sn)=f(x+sn)g(x+sn)=f(x+sn)g(x+mt)=f(x)g(x) \\
	    &\frac{f(x+sn)}{g(x+mt)}=\frac{f(x)}{g(x)}
	\end{aligned}
\end{equation*} So $sn=mt$ is a period for $f+g$ , $fg$ , $f/g$ .
We have to show that $sn=mt$ is the minimum of the positive periods of $f$ :
suppose that $\alpha$ is the period of $f$ , so $\alpha\leq sn$ .
By the precedent proposition, we know that $sn$ is in the form $\alpha k_1$ , and so: \begin{equation*}
				k_1=\frac{sn}{\alpha}
			\end{equation*} $k_1$ is an integer, so $\frac{sn}{\alpha}$ must be an  integer.
That happens if $sn=mt=\alpha$ (in this case,we conclude), or $\frac{s}{\alpha}$ is integer: $s=k_2\cdot \alpha$ . But that means that $s$ is a period for $f+g, fg, f/g$ .
We can repeat the same with $mt$ , and we would get that $t$ is a period for $f+g, fg, f/g$ . However, since $s=kt$ ; if $k$ is rational not integer, that isn't true (in fact $s$ wouldn't be a period for $g$ ), if $k$ is an integer, $t$ can't be a period for $f$ (otherwise it would be a positive period less than the fundamental period),and so it can't be a period for $f+g, fg, f/g$ too. That means that the only possibility is $sn=mt=\alpha$ . $ii)$ In this case it's obvious that $s=t$ is a period for  the functions $f+g$ , $fg$ , $f/g$ . However, we don't manage to say much on he period of these functions.
The only thing we can say is that if $\alpha$ is \textbf{the} fundamental period, it is the minimum of the positive periods, and so it surely will be $\alpha \leq s=t$ $iii)$ If $\frac{s}{t}$ is irrational, then we can't find integers $m$ , $n$ so that $ms=nt$ . Anyway, suppose that $f+g$ $fg$ $f/g$ are periodics with fundamental period $c$ . $c$ can't be, at the same time, period of $f$ and period of $g$ , because such number doesn't exist. If $c$ was a period for $f$ ( $c=kt$ ), we would have: \begin{equation*}
	f(x+kt)+g(x+kt)=f(x)+g(x+kt)\neq f(x)+g(x)
\end{equation*} And so $kt$ is not the period of $f+g$ ,same thing if $c$ was a period for $g$ .
But that means that $\exists x\in X : f(x+c)\neq f(x)$ and $g(x+c)\neq g(x)$ and so $f(x)+g(x) \neq f(x+c)+g(x+c)$ . Therefore, $c$ is not a period for $f+g$ , contraddiction: $f+g$ is not periodic.
This can be done in the same way for the function $fg$ $f/g$ , and we conclude.","['periodic-functions', 'real-analysis', 'calculus', 'functions', 'solution-verification']"
4840476,Most general result for intersecting a family of structures such as ideals and topologies to produce a new structure,"I sometimes encounter a kind of repetitive proof in which you take an arbitrary family of some kind of structure and show that its intersection also has this structure. The proof goes something like this. Fix a ring $R$ . Let $F$ be a nonempty family of ideals in $R$ . Consider $\cap F$ . We now verify that $\cap F$ is an ideal by running back to the definition. $0$ is in $\cap F$ . If $a$ is in $\cap F$ , then $a$ is in every $I \in F$ and thus $ra$ is in $\cap F$ for each ring element $r$ . If $a-b$ is in $\cap F$ , then $a$ and $b$ are in every $I \in F$ and thus $a-b$ is in every $I$ and thus $a-b$ is in $\cap F$ . The proof that the intersection of topologies (sets of open sets satisfying some rules) over a set of points $X$ is again a topology is also quite similar. Fix a set of points $X$ . Let $F$ be a nonempty family of topologies on $X$ , i.e. $(X, \tau)$ is a topological space for each $\tau$ in $F$ . $\varnothing$ is in $\cap F$ . $\varnothing^c$ is in $\cap F$ . Let $W$ be an arbitrary subset of $\cap F$ . $W$ is a subset of each $\tau$ in $F$ . Thus $\cup W$ is a subset of each $\tau$ in $F$ . Thus $\cup W$ is in $\cap F$ . Let $a$ and $b$ be elements of $\cap F$ . Thus $a$ and $b$ are elements of each $\tau$ in $F$ . Thus $a \cap b$ is an element of each $\tau$ in $F$ . Thus $a \cap b$ is an element of $\cap F$ . I'm curious just how much we can generalize this. Here's what I've come up with. Let $U$ be a nonempty set. Let $\lambda$ be a nonzero ordinal. Let $\text{Seq}[\bigcirc]$ denote $\lambda \to \bigcirc$ . Let $\mathcal{F}$ be a family of functions each of which has the form $\text{Seq}[U] \to 2^U$ . Let $f(x)$ be defined to be $\varnothing$ if $f$ is a function in $\mathcal{F}$ and $x$ is outside the domain of $x$ . If $X$ is a set, say that $X$ is closed under a function $f : \text{Seq}[U] \to 2^U$ if and only if it holds for all sequences $\vec{v}$ that $f(\vec{v})$ is a subset of $X$ . Say that $X$ is closed under a family of functions $\mathcal{F}$ precisely when $X$ is closed under $f$ for each $f$ in $\mathcal{F}$ . Lemma: Let $R$ be a set. Let $\mathcal{G}$ be a family of functions. Let $W$ be a nonempty family of subsets of $R$ . Suppose each $V$ in $W$ is closed under $\mathcal{G}$ . Then, $\cap V$ is closed under $\mathcal{G}$ as well. Proof. If $\cap W$ is empty, then we're done. Assume that it's not empty. Choose a function $f$ in $\mathcal{G}$ arbitrarily. Choose a sequence $\vec{v}$ with elements among $\cap W$ arbitrarily. $\vec{v}$ is in every $V$ in $W$ . $f(\vec{v})$ is in every $V$ in $W$ . $f(\vec{v})$ is in $\cap W$ . Since $f$ was chosen arbitrarily, we're done. Here's how you prove the ideal case and the topology case using this lemma. As a technical detail, a nullary operation ignores all of its arguments, a unary operation ignores all but its first argument, a binary operation ignores all but its first two arguments, and so on. Functions that are well-defined return a singleton set containing only their result. That's how you shoehorn something like $+$ into $\text{Seq}[U] \to 2^U$ . For ideals, choose as our set of functions $0, +, -$ from the ring $R$ that all of our ideals are ideals of, as well as multiplication by $r$ for each element of $R$ . For a family of topologies on a topological space $X$ , set $\lambda = \min(|X|, \omega)$ . Our operations are the nullary operations $\varnothing$ and $\varnothing^c$ , the binary operation $\cap$ , and arbitrary unions on sequences $\cup(\vec{v})$ . We can also prove that the intersection of a family of subfields is a subfield by choosing as our set of functions, $0, +, -, *$ and the unary partial function $\bigcirc^{-1}$ .",['abstract-algebra']
4840493,Convergent Improper Integral,"I am analyzing the following integral $$
\int_0^{+\infty}\frac{\log(x)}{(x-a)(x+b)}dx
$$ with $a,b>0$ . Playing a little with the values I concluded that this integral is only convergent when $a=1$ and $b>0$ but I don't know how to prove it. Any ideas? Or maybe my trial-and-error attempts have failed?","['calculus', 'improper-integrals', 'analysis']"
4840535,A closed form for integrals of the type $\Gamma\left(\sigma+it\right)$?,"Numerical evidence strongly suggests that: $$\int_{-\infty}^{\infty}\Gamma\left(\sigma+it\right) \,\mathrm{d}t = 2\cdot\frac{\pi}{\mathrm{e}} \qquad \sigma \in \mathbb{R}, \sigma > 0$$ and $$\int_{-\infty}^{\infty}\Gamma\left(\sigma+it\right) \,\mathrm{d}t = 2\cdot (1-\frac11\mathrm{e})\cdot\frac{\pi}{\mathrm{e}} \qquad \sigma \in \mathbb{R}, -1 < \sigma < 0$$ and $$\int_{-\infty}^{\infty}\Gamma\left(\sigma+it\right) \,\mathrm{d}t = 2\cdot \left(1-0\,\mathrm{e}\right)\cdot\frac{\pi}{\mathrm{e}} \qquad \sigma \in \mathbb{R}, -2 < \sigma < -1$$ and $$\int_{-\infty}^{\infty}\Gamma\left(\sigma+it\right) \,\mathrm{d}t = 2\cdot \left(1-\frac12\,\mathrm{e}\right)\cdot\frac{\pi}{\mathrm{e}} \qquad \sigma \in \mathbb{R}, -3 < \sigma < -2$$ and $$\int_{-\infty}^{\infty}\Gamma\left(\sigma+it\right) \,\mathrm{d}t = 2\cdot \left(1-\frac13\,\mathrm{e}\right)\cdot\frac{\pi}{\mathrm{e}} \qquad \sigma \in \mathbb{R}, -4 < \sigma < -3$$ and $$\int_{-\infty}^{\infty}\Gamma\left(\sigma+it\right) \,\mathrm{d}t = 2\cdot \left(1-\frac38\,\mathrm{e}\right)\cdot\frac{\pi}{\mathrm{e}} \qquad \sigma \in \mathbb{R}, -5 < \sigma < -4$$ where the rational factors before $\mathrm{e}$ seem to be OEIS: A053557 , i.e.: $$\int_{-\infty}^{\infty}\Gamma\left(\sigma+it\right) \,\mathrm{d}t = 2\cdot \left(1-\mathrm{e}\sum_{k=0}^n \frac{(-1)^k}{k!}\,\right)\cdot\frac{\pi}{\mathrm{e}} \qquad \sigma \in \mathbb{R}, -(n+1) < \sigma < -n, n \in \mathbb{N}$$ I expect this is already known in the math literature, however would like to understand how this could be derived. Tried many sources on the web e.g. the Wiki and Wolfram pages and also OEIS: A061382 however without success. Does anyone know how this is done or would be able to provide a link to a proof? P.S.:
A well-known and maybe related result is: $$\int_{-\infty}^{\infty}\frac{\cos(x)}{(x^2+1)} \,\mathrm{d}x = 2\cdot\frac{\pi}{\mathrm{e}}$$","['integration', 'closed-form', 'gamma-function']"
4840597,Radon-Nikodym Derivative for paths of stochastic process,"I am currently trying to understand this paper https://arxiv.org/pdf/1505.07612.pdf and have problems understanding lemma 3.1. We have a stochastic Process given by \begin{equation}
         dX_t=rh_t(X) 1_{ \{\theta \leq t\} }dt+\sigma \sqrt{h_t(X)}dW_t   
         \end{equation} where $\theta$ is a Random Variable and $W$ a Brownian motion.
Let \begin{equation}
\begin{split}
\mu_{u,t}(A)&=P\left(X|_{[0,t]}\in A|\theta = u \right)& \quad \mu_{t}(A)&=P\left(X|_{[0,t]}\in A\right)
\end{split}
\end{equation} for $A\in \mathcal{B}\left( C\left([0,t],\mathbb{R}\right)\right)$ .
Then the lemma states that the Radon-Nikodym Derivative of $\mu_{u,t}$ w.r.t $\mu_{t,t}$ is given by \begin{equation}
\frac{d\mu_{u,t}}{d\mu_{t,t}}=\exp\left(\int_{u}^{t}\frac{r}{\sigma^2}dX_s-\frac{1}{2}\int_{u}^{t}\frac{r^2}{\sigma^2}h_s\left(X\right)ds\right)
\end{equation} The lemma therefore says: \begin{equation}
\mu_{u,t}(A)=\int_{A}\frac{d\mu_{u,t}}{d\mu_{t,t}}(z)d\mu_{t,t}(z)
\end{equation} Right ?
My Problem with this is that $\frac{d\mu_{u,t}}{d\mu_{t,t}}$ is a random variable i.e. $\frac{d\mu_{u,t}}{d\mu_{t,t}}:\Omega \rightarrow \mathbb{R}$ . But $\mu_{u,t}$ and $\mu_{t,t}$ are measures on $\mathcal{B}\left( C\left([0,t],\mathbb{R}\right)\right)$ .
So they are acting on different spaces. How does this then make sense ? I have already read the proof, but I don't understand the last step where this happens. I would be very grateful if someone could clear up my confusion.","['stochastic-calculus', 'stochastic-processes', 'probability-theory', 'probability', 'radon-nikodym']"
4840646,What is diameter and growth of group generated by two cyclic permutations having only two common elements (rotation group of two-circle puzzle)?,"Consider subgroup of S_n generated by two cyclic permutations of the form  described below. (Roughly speaking cycles having only two common elements). Question: what is known about its growth and diameter ? Growth seems to be about 2.7 - see computational experiments here: https://www.kaggle.com/competitions/santa-2023/discussion/464953 Remark: Grwoth and diameter are quite related - since probably one can estimate number of elements combinatorily and afterwards diameter can be estimated as about: log_growth(group-size). The group is described like that. Let us give the examples first. It is generated by by two cyclic permutations denoted ""l"" and ""r"". n = 10
l (0 1 2 3 4 5)
r (0 6 7 2 8 9)

n = 12
l (0 1 2 3 4 5 6)
r (0 7 8 9 2 10 11)

n = 22
l (0 1 2 3 4 5 6 7 8 9 10 11)
r (0 12 13 14 15 16 17 18 3 19 20 21)

n = 40
l (0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20)
r (0 21 22 23 24 25 26 27 28 29 30 31 32 33 6 34 35 36 37 38 39) I.e. first is cycling moving 0...k, second is k+1...n with two intersecting positions. That group can be thought as symmetry group of the puzzle consisting of two circles: https://www.kaggle.com/code/marksix/visualize-allowed-moves Or the similar subgroup appears when we make rotation of the Rubik's cube around two facets: https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fimq4s00bgx1a1.gif%3Fformat%3Dmp4%26s%3Dabea9b805f838832ba835c36eeefd2e73fd2eae1","['permutations', 'group-theory', 'rubiks-cube']"
4840678,Sum of two values in the range of $\sigma_1(n)$,"$\mathcal{Q}$ : Is it true that for $n>3$ , there exists $u$ and $v$ in $\mathbb N$ such that $$n=\sigma_1(u)+\sigma_1(v),$$ where $\sigma_1(k)$ is the sum of the positive integer divisors of $k$ for $k\in \mathbb N$ ? I assume this is true and may seem to be connected with Goldbach conjecture but clearly should be a lot easier. For instance, if $n\ge 8$ is even, then assuming that Goldbach conjecture is true, $$n-2=p+q\implies\ n=p+1+q+1=\sigma(p)+\sigma(q)$$ for some primes $p$ and $q$ . So, the answer to $\mathcal Q$ would be affirmitive for even $n$ if we assume Goldbach. There seems to be no simple way to do something similar for $n$ odd. The numbers in the range of $\sigma_1(n)$ less than $100$ are $$A=\{ 1, 3, 4, 6, 7, 8, 12, 13, 14, 15, 18, 20, 24, 28, 30, 31, 32, 36, 
38, 39, 40, 42, 44, 48, 54, 56, 57, 60, 62, 63, 68, 72, 74, 78, 80, 
84, 90, 91, 93, 96, 98\}.$$ The list of $a+b$ with $a,b\in A$ , and then sorted is $$2, 4, 4, 5, 5, 6, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10, 
11, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 14, 15, 
15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 
18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,... 
20, 20$$ which makes the conjecture that $\mathcal Q$ is true very plausible. I wonder if this is already known. This is a question asked by Sourav Mandal on Research Gate . As one can read below, it is likely that if $n\ge 7$ , one can find $u$ a prime.","['number-theory', 'goldbachs-conjecture']"
4840715,"""Interesting"" pattern when plotting ratio between natural numbers and the sum of their closest factors","I was playing around with graphing numbers recently and an interesting pattern emerged. I was plotting them on a cartesian plane with points $(x, y)$ where $x$ is each of the natural numbers, and $y$ is the ratio of the number itself divided by the sum of its closest pair of factors. For example, the point where $x = 12$ would be $(12, \frac{12}{7})$ . Below is the graph I got for the first 1000 numbers. Also, what is interesting is that I thought since the y-coordinate is a ratio it wouldn't get significantly larger as x tends towards larger numbers but it doesn't seem so. image here because stackexchange doesn't let me embed images Is this supposed to happen? Am I just being stupid?","['number-theory', 'prime-numbers']"
4840718,Simplify/re-express a potential expression for the inverse of $\frac{\sinh(x)}x$,"$\DeclareMathOperator\M M \DeclareMathOperator\csch {csch}$ An attempt to invert the sinhc function $\frac{\sinh(x)}x$ uses Mellin inversion . Define $f(x)$ as the  inverse of $y=x\csch(x),0<y\le1$ and $0$ otherwise, graphed here . Therefore, $f(x)$ ’s Mellin transform is: $$\M_t(f(t))=\int_0^1t^{s-1}f(t)dt=\int_0^\infty t(t\csch(t))^{s-1} d(t\csch(t))=-\frac1s\int_0^\infty (t\csch(t))^sdt$$ after substituting $f(t)\to t$ as well as integration by parts on $t$ and $(t\csch(t))^{s-1} d(t\csch(t))$ with $\left.\frac ts(t\csch(t))^s\right|_0^\infty=0$ . The final integral appears in: Compute integral of general form $ \int_0^\infty \left(\frac{x}{\sinh x}\right)^n d x $ We apply binomial series, converging on $e^x>1\iff 0<x$ , to get: $$\M_t(f(t))=-\frac{2^s}s\int_0^\infty\left(\frac t{e^t-e^{-t}}\right)^sdt=-\frac{2^s}s\sum_{n=0}^\infty\binom{-s}n (-1)^n\int_0^\infty t^s e^{-(2n+s)t}dt=-2^s\Gamma(s)\sum_{n=0}^\infty\binom{-s}n \frac{(-1)^n}{(2n+s)^{s+1}}$$ Expanding/simplifying the binomial and applying the inverse Mellin transform finally gives: $$\boxed{\operatorname{sinhc}^{-1}(x)=\pm\frac1{2\pi i}\int_{c-i\infty}^{c+i\infty}(2x)^sg(s)ds;g(s)=2^{-s}\M_t(f(t))(s)=\sum_{n=0}^\infty\frac{\Gamma(n+s)}{(2n+s)^{s+1}n!},x\ge1}$$ shown here: The result is accurate to $2$ decimal places if one truncates the integral/sum at $\pm130$ . The boxed result is an integral of a sum which looks a bit cumbersome to use. However, it could possibly be a  sum of a sum if Ramanujan master theorem , which also uses the Mellin transform, is applied. Another idea is a closed form for $\M_t(f(t))$ . Both would make the boxed result cleaner. If the boxed result it correct, what is a cleaner expression for the inverse of $\frac{\sinh(x)}x$ possibly using the Ramanujan master theorem or getting a closed form for $\M_t(f(t))$ ?","['ramanujan-master-theorem', 'inverse-function', 'analysis', 'closed-form', 'mellin-transform']"
4840723,"Can we find $f: [0,1] \to [0, \infty) \notin L^{\frac{q}{q-1}}[0,1]$ such that $\int_A f \le |A|^{1/q}$ for some $q \ge 2$ and all $A \subset [0,1]$?","Let $f: [0,1] \to [0, \infty)$ be a measurable function satisfying $$\int_A f \le |A|^{1/q}$$ for some $q \ge 2$ and all measurable subsets $A \subset [0,1]$ . Show that $f\in L^p[0,1]$ for all $1 < p < \frac{q}{q-1}$ . Is $f$ necessarily in $L^{\frac{q}{q-1}}[0,1]$ ? I solved the first part. Apply the given inequality to the sets $A_t := \{f > t\}$ for $t\in \Bbb R$ , to get $$t |A_t| \le \int_{A_t} f \le |A_t|^{1/q},$$ i.e., $$|A_t| \le t^{\frac{q}{1-q}}.$$ Using $$\int_0^1 f^p = p\int_0^\infty t^{p-1} |A_t| \, dt$$ we get $$\int_0^1 f^p \le p + \frac{p}{\frac{q}{q-1} - p} < \infty$$ for $1 < p < \frac{q}{q-1}$ . Could someone help me produce a measurable function $f: [0,1] \to [0, \infty) \notin L^{\frac{q}{q-1}}[0,1]$ satisfying $$\int_A f \le |A|^{1/q}$$ for some $q \ge 2$ and all measurable subsets $A \subset [0,1]$ ?","['measure-theory', 'weak-lp-spaces', 'analysis', 'real-analysis', 'lp-spaces']"
4840762,Show that $\dfrac{1}{e^x-1} = \dfrac{1}{x} - \dfrac{1}{2} + 2x \sum_{n=1}^{\infty} \dfrac{1}{(4n^2 \pi^2 +x^2)}$.,In the fifth method of deriving the functional equation for Riemann's zeta function in Titchmarsh's book the following 'expansion' is assumed without proof : $$\dfrac{1}{e^x-1} = \dfrac{1}{x} - \dfrac{1}{2} + 2x \sum_{n=1}^{\infty} \dfrac{1}{4n^2 \pi^2 +x^2}.$$ I have never encountered this type of expansion and also WolframAlpha does not give a clue for the identity. How this equality is derived? I have a second question regarding the integral just below this identity in the book How to compute the integral $\int_0^{\infty} \dfrac{x^s}{4n^2 \pi^2 +x^2} dx$ ? Again using WolframAlpha I couldn't get any hint.,"['proof-explanation', 'improper-integrals', 'sequences-and-series']"
4840781,Show that for $0<b<1<a$: $\mathbb P(\tau_a < \tau_b)=\frac{1-b^4}{a^4-b^4}$,"$(X_t)_{t \ge 0}$ is a process adapted to $(\mathcal F_t)_{t \ge 0}$ with continuous trajectories and such that $X_0=1, X_t$ is non-negative, $X_t ^4$ is a martingale with respect to $(\mathcal F_t)$ and $\limsup _{t \to \infty} X_t = +\infty$ . Let for $a>0$ : $$\tau_a=\inf \{ t>0: X_t =a\}.$$ Show that for $0<b<1<a$ $$\mathbb P(\tau_a < \tau_b)=\frac{1-b^4}{a^4-b^4}.$$ I have absolutely no idea for this task. $$\mathbb P(\tau_a < \tau_b)=\mathbb P(\inf \{ t>0: X_t =a\}-\inf \{ t>0: X_t =b\}<0)$$ I think you need to make clever use of the stopping time properties and the facts about $X_t$ given in the problem, but I didn't get anything sensible.","['stochastic-analysis', 'stochastic-processes', 'martingales', 'stopping-times', 'probability']"
4840833,Symmetry of a vector field with Roger Brockett example,"It is me again. I ask you to say if I infringing any SO rule, like those related to good questions and so. My supervisor helps me learn about symmetry in geometry. His question for me is clear: we can find the symmetries of a vector field $a$ by finding a connection $\nabla$ for such the Lie derivative $L_a \nabla$ to be 0. He suspects the result may be related to the controllability of a dynamical system in a topological space. Since finding a symmetry connection is not trivial, he suggested pursuing the following direction: we can find the 1-form $\omega$ to satisfy $L_a \omega = 0$ . Great! I like reading broad theoretic textbooks, but I still need an example. Thus, he gave me this example: Let us take the Brockett integrator $(u^1, u^2, x^2 \, u^1 - x^1 u^2)$ as vector field $a \in T_x\mathbb{R}^3$ . We define an extended vector field $\bar{a}$ as extension $(a, 0, 0)$ . My first solution guess was to use Cartan's formula for the Lie derivative of a $n$ -form $L_a \omega$ , given by expression $\iota_a(d\omega)+d(\iota_a \, \omega)$ and expanded below, in Einstein's notation. $$[(\partial_j a^i) \alpha_i + a^i (\partial_i \alpha_j)] dx^j$$ I was able to only find the 2 1-forms $d u^1$ and $d u^2$ . Since the Brockett integrator is a transformation of unicycle $(v \cos(\theta), v \sin(\theta), \omega)$ , such that input variables $v$ and $\omega$ are respectively linear and angular speeds, I think I can use the translation and rotation symmetries somehow, but as I said, I am not an expert, just a curious student. Both state and input maps from Brockett integrator to unicycle are below: $$\begin{bmatrix} x^1 \\ x^2 \\ x^3 \end{bmatrix} = \begin{bmatrix} \theta \\ x \cos{(\theta)} + y \sin{(\theta)} \\ x \sin{(\theta)} - y \cos{(\theta)} \end{bmatrix}$$ $$\begin{bmatrix} u^1 \\ u^2 \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 1 & x \sin{(\theta)} - y \cos{(\theta)} \end{bmatrix} \begin{bmatrix} v \\ \omega_{\theta} \end{bmatrix}$$ $\mathbf{Question}$ : how can I use translation and rotation symmetry with new coordinates $(x, y, \theta, v, \omega)$ for calculating 1-form $\bar{\omega}$ on partial differential equation $L_\bar{a} \bar{\omega} = 0$ , for vector field $\bar{a}=(v \cos{\theta}, v \sin{\theta}, \omega_{\theta}, 0, 0)$ ? The next step is coordinate transformation $(x^1, x^2, x^3, u^1, u^2) =  \psi(x, y, \theta, v, \omega_\theta)$ and its inverse $\psi^{-1}$ .Additionally, how can I find the symmetry connection coefficients for connection $\nabla$ given 1-form $\omega$ ?","['geometry', 'dynamical-systems']"
4840852,Only injectivity sufficient for isomorphism between two groups according to Herstein?,"I just re-read Herstein's popular text Topics in Algebra , where he says that an isomorphsim between two groups is defined as a homomorphism between those two groups with the added property of injectivity. However, the very next definition shows that two groups are said to be isomorphic iff there exists an isomorphism from one of the groups onto the other. And, many standard perception and texts say that isomorphism is always bijective. So, what is this thing all about? Is injectivity sufficient for most properties of isomorphism, like, the isomorphism/homomorphism theorems? Any hints? Thanks beforehand.","['definition', 'abstract-algebra', 'group-theory', 'algebra-precalculus', 'terminology']"
4840866,"If $A,B.C$ are matrices and $ABC=X$ is it possible to find $AC$?","Let $A,B,C$ be matrices. If $A=BC$ then $C= B^{-1}A$ (assuming $B$ is invertible) But what if $A,B,C$ are matrices  and if $ABC=X$ and $B$ is $n\times n$ invertible matrix, $A$ is an $m\times n$ matrix and $C$ is a $n \times m$ matrix is it possible to find $AC$ by Knowing $B$ and $X$ ? If it is possible, how to find it ? If it was not always possible, what are the necessary conditions on $A,B,C,X$ so that finding $AC$ becomes possible?","['matrices', 'linear-algebra']"
4840933,"Conjecture: If $A,B,C$ are random points on a sphere, then $E\left(\frac{\text{Area}_{\triangle ABC}}{\text{Area}_{\bigcirc ABC}}\right)=\frac14$.","On (not in) a sphere, choose three independent uniformly random points $A,B,C$ . Is the following conjecture true: The expectation of the ratio of the area of (planar) $\triangle ABC$ to the area of the (planar) disk with $A,B,C$ on its perimeter, is $1/4$ . My conjecture is based on experimentation: using sphere point picking , I ran $1.5\times10^7$ trials and the average ratio was about $0.2500075$ . Since $1/4$ is a simple ratio, I am wondering if there is an intuitive explanation (here is a recent example ), but any proof is welcome. Remarks: Assume the sphere is a unit sphere. The expected area of the disk is $\frac{4\pi}{5}$ ( proof ). The expected area of the triangle seems to be $\frac{\pi}{5}$ based on experimentation, but I don't have a proof (Wolfram has a page called "" Sphere triangle picking "" but it's empty). Curiously, the expectation of the ratio seems to equal the ratio of the expectations. The probability that the ratio is greater than $1/4$ is about $0.534$ . The standard deviation of the ratio is about $0.10667$ (could it be $8/75$ ?). The probability density function of the ratio looks roughly like this: The expectation of the reciprocal ratio (i.e. the ratio of the area of the disk to the area of the triangle) seems to be about $6.6$ , but this ratio flucuates a lot and I'm not sure its expectation even exists. My attempt: I tried to adopt the techniques in the proof that the area of the disk is $\frac{4\pi}{5}$ (with a unit sphere), but I don't know how to express the area of the triangle in a usable way. I tried to find a more intuitive explanation, possibly using symmetry, but failed. (In the $2D$ case, the expected ratio of areas is $\frac{3}{2\pi^2}\approx 0.15198$ ( proof ), which is not an intuition-friendly number, so it seems that an intuitive explanation would apply to the $3D$ case but not the $2D$ case.) Context: This question was inspired by a question about a random triangle inscribed in a circle.","['conjectures', 'geometric-probability', 'geometry', 'expected-value', 'intuition']"
4840940,Finding orthogonal polynomials,"I have the vector space $C^0([0,1],ℝ)$ of continious functions from 0 to 1, with the inner product $$ \langle f , g \rangle =  \int_{0}^{1} f(x)g(x) \,dx	$$ I have to find all the first degree polynomials $g(x)=ax+b$ that are orthogonal on $f(x)=6x$ , which means that $\langle f , g \rangle = 0$ . I have solved it like this $$\langle f,g\rangle = \int_{0}^{1} f(x)g(x) \,dx = \int_{0}^{1} (6x)(ax+b) \,dx = 6a \int_{0}^{1} x^2 \,dx   + 6b\int_{0}^{1} x \,dx  $$ $$
= 6a \left[ \frac{x^3}{3} \right]^1_0 + 6b\left[ \frac{1}{2}\right]^1_0 = 6a \cdot \frac{1}{3} + 3b = 2a+3b
$$ $$ 2a+3b = 0$$ $$ a =- \frac{3b}{2}$$ Therefore $g(x)=ax+b$ $$g(x) =- \frac{3b}{2}x+b = b(-\frac{3}{2}x+1)$$ Is this solution correct?","['linear-algebra', 'linear-transformations']"
4840980,Prove that if characteristic function is absolutely integrable then corresponding density function will be uniformly continuous.,"So the point is that you obtain density function through the characteristic function with the inversion formulas. That's the part of the task. Then it supposed to be uniformly continuous. That is, if $\phi(t)$ is absolutely inegrable, show that $\frac{1}{2\pi}\int \phi(t) e^{-itx} dt$ is unformly continuous. I have got the exercise in my book, I’ve tried and it led to nothing as follows: $|p_\xi(x)-p_\xi(y)| \leq \frac{1}{
2\pi}|x -y|\int |t \phi_\xi(t)| dt$ .
If the last integral was without $t$ then it would be it. But it contains $t$ and may not converge since that.
I’ve been stuck on this for a few days, please, help.","['characteristic-functions', 'uniform-continuity', 'probability-theory']"
4841035,Looking for another explanation for the Gambler's Fallacy [duplicate],"This question already has answers here : Past coin tosses affect the latest one if you know about them? [duplicate] (4 answers) Closed 6 months ago . I'm a laymen, and the Gambler's fallacy has bothered me for decades. I'm looking for someone to help me look at it from another perspective. Here's why it confuses me: Of course, if you were to flip a coin 100 times, none of those results should be affecting the 101th flip. The 101th flip is 50/50 Statistically, if you flipped a coin 100 times, you would expect around 50 heads and 50 tails. So here's the question... why is it if you flipped 99 coins: 50 Heads and 49 Tails, why are we allowed to say that the 100th coin is still 50/50?
Or put in another way, if we flipped 99 coins, and they were all 99 heads, why are we not allowed to say that the 100th coin is going to likely be a tails? I logically understand that it will always be a 50/50 chance, but where I get confused is when we introduce a set of coin flips (like say we are going to flip 100 coins total), my mind wants to believe that the flips will be distributed 50/50 across the set. I dug around this stackexchange and did find this answer which sort of helps: https://math.stackexchange.com/a/845405/1276792 His third point i think is the closest I've been to understanding it, that there is no expected difference between the number of heads and tails... but it's still hard for me to reconcile the fact that the 100th toss is 50/50 still. Maybe just a rephrase would help me Mods feel free to close this question if it is not in the correct spirit of the community","['statistics', 'probability']"
4841038,Evaluating a double integral involving a biquadratic with variable coefficients,"Please keep in mind that I only have an intermediate level proficiency in Calculus-II . I am relatively new to the concept of double integration, though I have solved some problems in this topic. I have tried a lot, but have failed in simplifying this integral $$\int_{u=0}^1 \int_{t=0}^{\infty} \frac{2 \sqrt{115} \cdot (27t^2+11)}{\left[(729+1035u^2)t^4+(594+2070u^2)t^2 + (121+1035u^2)\right]} dt du$$ I know that in general, to deal with integrals of the type $$\int \frac{ax^2+b}{x^4+px^2+q} dx$$ We divide both numerator and denominator by $x^2$ and try to find suitable constants $c,d$ such that: $$ a + \frac{b}{x^2} = c \left(1 + \frac{\sqrt{q}}{x^2}\right) + d \left(1 - \frac{\sqrt{q}}{x^2}\right)$$ But after I take $(729+1035u^2)$ common to make the integrand in such a form, I am not able to re-integrate the expression I obtain with respect to $u$ . The integrals look terrible and certainly unsolvable by me. WolframAlpha suggests that the answer of the integral may be $\frac{\pi^2}{9}$ I would like to know of any better methods to solve this double integral. Thanks in advance! Have a nice day!","['multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4841060,Entire function is a polynomial,"Let $f = \sum_{n =0}^{\infty} a_nz^n$ be an entire function such that $\forall w \in \mathbb{C}, f(z) = w$ admits a finite number of solutions. I want to prove that f is a polynomial. So far, I know that $f(z) - w$ admits a finite number of roots, by fixing $w$ I wanted to directly show that it forces $f$ to be a polynomial of maximum degree $n$ (with $n$ being the number of roots). I don't know how to proceed and any help would be welcome.",['complex-analysis']
4841086,Permutation of cartesian product,"$$
\DeclareMathOperator{\sgn}{sgn} |X|=m,\, |Y|=n,\, \sigma \in S_X, \, \tau \in S_Y. 
\\\xi \in S_{X \times Y},\, \xi(x, \ y) = (\sigma(x),\, \tau(y)),\, x \in X,\, y \in Y
$$ I need to find $\sgn \xi$ , if $\sgn \sigma$ and $\sgn \tau$ are specified. I need to find lengths of independent cycles in $\xi$ expansion, if lengths $k_1,\dots,k_s$ and $l_1,\dots,l_t$ of independent cycles in $\sigma$ and $\tau$ expansions are specified. I solved first point of the task in the following way: let $r$ be number of transpositions in $\sigma$ and $q$ number of transpositions in $\tau$ , $r_i$ and $q_j$ are specific transpositions in $\sigma$ and $\tau$ respectively. $$
\xi = \bigl((x),r_1\bigr), \dots, \bigl((x),r_n\bigr) \bigl((q_1),(y)\bigr), \dots, \bigl((q_m),(y)\bigr); 
\quad x = 1,\dots,m;\; y = 1,\dots,n. 
$$ So, total number of transpositions is $m \cdot r + n \cdot q$ . It means that $\sgn \xi = (\sgn \sigma)^m \cdot (\sgn \tau)^n$ . Second point of the task seems unclear to me. I don't understand how independent cycles in $\xi$ expansion look like. I tried to use the second point of task for finding another solution for the first one: $\sgn \xi = (-1)^{mn - \sum_{i=1}^s \sum_{j=1}^t gcd(k_i,l_j)}$ But the answer is $\sgn \xi = (\sgn \sigma)^n \cdot (\sgn \tau)^m$ .
It means that $\sgn \xi = (-1)^{2mn - sn - tm}$ .So, I need to prove that $mn - \sum_{i=1}^s \sum_{j=1}^t gcd(k_i,l_j)  \equiv - sn - tm(mod \ 2)$ . I don't really know how to do that.","['permutations', 'group-theory', 'abstract-algebra', 'combinatorics']"
4841101,Is it possible to define a space and/or a distance function such that there is always more than 1 shortest path between any 2 points?,"I am in my second semester of university in maths and physics and thought of a question I am unable to answer.
I asked my analysis teacher of the last semester if it was possible to define a space and/or a distance function such that there is always more than 1 shortest path between any 2 points. She thought about it and then she told me she didn't know. Firstly, I thought that maybe it would be possible if you could define the distance between two points in Q'XQ U QXQ' only by going through points (a,b) such that either a is rationnal and b irrational or a irrational and b rationnal, but she told me that there would be no way to connect the points if we use only those points. ({(a,b)|a is rational and b is irrational or a is irational and b is rational}) However, I know that it is possible in special cases. A very simple example of this is that any 2 opposite points on a sphere has infinitely many shortest path between them. However, if the points are not opposite, then there is only one shortest path between them. It is possible to create as many shortest paths as wanted between some 2 points on a surface, for example you can take the shape of an american football ball, put point 1 on one edge and point 2 on the other edge, and dig into the ball so that there are 2 ways to get from one edge to another. You can repeat this processus to create as many paths as you want between the 2 points on the edges. These are only special cases though, and my question remain unanswered. So I thought about it and found a possible candidate. Could the distance calculated as the lenght of the circle arc between 2 points work? For example, in R^2, you would get two shortest paths from one point to another. I believe it respects the usual definition of distance: It's always positive, the distance is equal to zero iff it is the distance from point a to point a, it is symmetric, d(a,b)=d(b,a) and the triangular inequality should be sastified. The problem I encountered though, is that to create this circle you would need to first find the middle point between the 2 points and then put each point that will form the path from point 1 to point 2 at an equal distance to the middle point, and I believe it does not make sense as we are using normal euclidian distance. I know this question lacks a lot of rigor and I don't even know if it makes sense asking it. For example I would need to define what I mean by a path. I believe requiring continuity would make sense but I'm not sure about the rest. I would be very grateful if you had any documentation related to this or the answer to my question. Thank you. Here is a picture of what I meant","['path-connected', 'geometry', 'metric-spaces']"
4841229,How often is a tensor product of two irreps of a finite group still irreducible?,"All representations are considered over the complex numbers. Let $ G $ be a finite group. Then Any 1-dimensional character $\otimes$ irreducible character is irreducible . But what if we have two irreducible characters both of degree greater than 1? How often will it be the case that the tensor product of two irreps of degree $ \geq 2 $ will still be irreducible? For some groups there are no irrep triples $ \chi_1 \chi_2=\chi_3  $ with $ dim(\chi_1), dim(\chi_2) \geq 2 $ . For example, $ A_5 $ only has irreps of degrees $ 1,3,3,4,5 $ and $ 3^2>5 $ so no such triple exist. But triples like this certainly aren't impossible. A generic construction of some example triples is given in Representations irreducible with respect to the tensor product . However those examples take a tensor product of two representations, neither of which are faithful. For an example using irreps which are not all faithless, take $ G=2.A_5 $ then we have
examples like $ \pi_{2,1} \otimes \pi_{2,2}= \pi_{4,1} $ and $ \pi_{2,1} \otimes \pi_{3,2}= \pi_{6} $ and $ \pi_{2,2} \otimes \pi_{3,1} = \pi_{6} $ where the first index denote the dimension of the irrep, and if there are multiple irreps of the same dimension then the second index represents the order it is listed by GAP in the command CharacterTable(""2.A5""). For example $ \pi_{4,1} $ denotes the first 4d irrep listed by GAP (the non-faithful one). Can we say anything about which finite groups admit irrep triples like this? For example $ A_9 $ and the monster group have such triples. Certainly if $ G $ has character triples like this, then any group with $ G $ as a quotient also has character triples like this, so the question is most interesting when at least one of the characters in the triple is faithful.","['gap', 'characters', 'representation-theory', 'finite-groups', 'group-theory']"
4841265,What is $\int\limits_{0}^{\infty}\frac{\sin(x^n)}{x^2+1}dx$?,"Let $I_n=\int\limits_{0}^{\infty}\frac{\sin(x^n)}{x^2+1}dx$ Wolfram Alpha gives an exact value for $n=-4,-3,-2,-1,0,1,2,3,4,6,8$ and interestingly enough, it seems $I_n=I_{-n}$ . $$I_0=\frac{\pi}{2}\sin(1)$$ $$I_1=I_{-1}=\frac{\operatorname{Ei}(1)-e^2\operatorname{Ei}(-1)}{2e}$$ $$I_2=I_{-2}=\frac{\pi}{2}\left(\sin(1)\left(\operatorname{C}\left(\sqrt{\frac{2}{\pi}}\right)+\operatorname{S}\left(\sqrt{\frac{2}{\pi}}\right)\right)+\cos(1)\left(\operatorname{C}\left(\sqrt{\frac{2}{\pi}}\right)-\operatorname{S}\left(\sqrt{\frac{2}{\pi}}\right)\right)\right)$$ And so on... $$I_{\frac{1}{2}}=\frac{\pi}{e^\frac{1}{\sqrt{2}}}\sin\left(\frac{1}{\sqrt{2}}\right)$$ So my question is: For any $n$ , what is $\int\limits_{0}^{\infty}\frac{\sin(x^n)}{x^2+1}dx$ ?","['integration', 'improper-integrals', 'definite-integrals']"
4841276,"Are there rational solutions $r,s \in \mathbb{Q}$ to the equation $\tan^2(\pi r) + \tan^2(\pi s) = 1$","I am seeking to understand the structure of solutions to the diophantine equation $$\tan^2(\pi r) + \tan^2(\pi s) = 1.$$ I am conjecturing that there are no rational solutions $r, s \in \mathbb{Q}$ to the equation satisfying both $\tan(\pi r) >0$ and $\tan(\pi s) >0$ . (This is so we exclude ""trivial"" solutions such as $r = 1$ and $s = \frac{1}{4}$ , which imply $\tan(\pi r) = 0$ and $\tan(\pi s) = 1$ ). On the other hand, if there are ""non-trivial"" rational solutions, can the form of the solutions be characterized? I have been unable to make progress on this, so thought I'd ask for any thoughts. Thank you!","['rationality-testing', 'trigonometry', 'diophantine-equations']"
4841312,Coincidence of numbers of solutions of matrix equations $A^2=B^2$ and $AB=0$ of size 2 over finite fields,"Let ring $R:=M_2(\mathbb F_p)$ , where prime $p\ne 2$ . There are two interesting equations $A^2=B^2$ and $(A+B)(A-B)=0$ , and the numbers of solution pairs $(A,B)\in R^2$ of the equations are denoted $S_1(p),S_2(p)$ . The fun fact is that, I used a computer search and found $S_1(p)=S_2(p)$ for very small $p$ -s. Actually: $$
S_1(3)=S_2(3)=417,\ S_1(5)=S_2(5)=4705,\ S_1(7)=S_2(7)=23233,\ S_1(11)=S_2(11)=202081. 
$$ And if we set $R:=M_2(\mathbb Z/p\mathbb Z)$ where $p$ may be a general odd number , we even have $S_1(9)=S_2(9)=123201$ . So $S_1=S_2$ seems to be true in some more general settings. But I can't find any patterns in these numbers $S_i$ by just looking at them or using an OEIS check. Let's name the equations (1) and (2) respectively. For (1), I've tried to use $A=X+Y,B=X-Y$ to rewrite it as follows, $$ (X+Y)^2=(X-Y)^2 \iff 2(XY+YX)=0, $$ since $R$ is not commutative. And by $p$ is odd, we eventually get $ab+ba=0$ . And for (2), a similar argument reduce it to $XY=0$ . But I still can't see how can one relate a solution of $XY+YX=0$ to a solution of $XY=0$ . Is there any magic behind this? Also, there would be no similar results for $M_3(\mathbb F_3)$ , in that case $S_1=221157$ and $S_2=496341$ . Update The above is the original problem. By a careful case-by-case discussion over the trace and rank of one of the matrix $X$ or $Y$ , as suggested by the answer or the comment, we find the two equations $XY+YX=0$ and $XY=0$ both have $p^5+3p^4-2p^3-2p^2+p$ solutions. But there are similar and more shocking phenomena: for a positive integer $n>0$ , the following equation $$
f_n(X,Y)=XY-YX-X^n=0
$$ always seems to have $2p^4-p^2$ solutions no matter what $n$ is, coprime to $p$ or not. This sequence is OEIS $\mathsf A2593$ . But there seems to be nothing related to our problem on that link page. Another Update Thanks to @user1551. For the new equation $f_n(X,Y)=0$ when $n\ge 2$ , there is now some shortcut to see the result by linear algebra.
By the fact that $[X,Y]$ commutes with $X$ (since $[X,Y]=X^n$ ), Jacobson's Lemma implies that $[X,Y]$ is nilpotent and $X$ is nilpotent too under our field characteristic condition. Because $X\in R$ is a $2\times 2$ nilpotent matrix, we have $X^n=0$ when $n\ge 2$ . So solution sets for $n\ge 2$ are the same since $f_n(X,Y)=0$ iff $[X,Y]=0$ and $X$ is nilpotent. When $n=1$ , we know now $[X,Y]=X$ is nilpotent. Notice that this equation has at least one solution $Y_0$ for each given nilpotent $X$ (consider $X$ to be a Jordan form and construct $Y$ directly). After $Y\mapsto Y+Y_0$ , equation $[X,Y_0+Y]=X$ becomes $[X,Y]=0$ . This build a correspondence between the $n=1$ case and the $n\ge 2$ case.","['number-theory', 'finite-fields', 'matrix-equations', 'linear-algebra']"
4841320,Why does it seem like that a tangent line of an odd-degree polynomial function crosses the curve at more than one point?,"This question has bothered me for a long time. I know that a tangent line only crosses a curve at one specific point. However, consider this: Let $f(x)=x^3$ The derivative of this function is $f^\prime(x)=3x^2$ Suppose that I want to know the equation of the tangent line at $x=1$ . To determine the slope, I just need to plug $x=1$ into $f^\prime(x)$ : $f^\prime(1)=3(1)^2=3$ Therefore, the slope at $x=1$ of $f(x)$ is $3$ . Using the slope formula, I get $(y-1)=3(x-1)$ Solving for $y$ , I get $y=3x-2$ Therefore, the tangent line equation of the cubic curve at $x=1$ is $y=3x-2$ I would think that the tangent line will only intersects the cubic curve at a single point, but when I typed this into the Desmos Calculator , it intersects on the curve at two points $(1,1)$ and $(-2,-8)$ . This is actually different for a quadratic function. Consider this: Let $f(x)=x^2$ Then $f^\prime(x)=2x$ Suppose that I want to find the slope of the function at $x=1$ : $f^\prime(1)=2(1)=2$ The equation of the tangent line at the point when $x=1$ is $(y-1)=2(x-1)$ $y=2x-1$ When I typed this into the Desmos Calculator , it shows me that the only solution to this pair of system of equation is $(1,1)$ Why did this happen? Am I misunderstanding something? Thank you for your time.","['calculus', 'polynomials', 'tangent-line']"
4841342,Explicit formula for the principal connection 1-form induced by a Cartan connection,"Let $P \subseteq G$ be a closed Lie subgroup. Suppose that a principal $P$ -bundle $\mathcal{P} \to M$ is equipped with a Cartan connection $\omega: T\mathcal{P} \to \mathfrak{g}$ . Then the extended principal $G$ -bundle $\mathcal{P} \times_P G \to M$ admits a principal connection $\overline{\omega}$ . However, I cannot find a nice explicit formula given in Sharpe's or Cap & Slovak's book for this principal connection $1$ -form. Note that the tangent bundle of the extended principal $G$ -bundle is $$T(\mathcal{P} \times_P G) \cong T\mathcal{P} \times_{TP} TG,$$ where $TP$ is the tangent group of $P$ . For those unaware of this term, note that the tangent bundle of a Lie group is again a Lie group when we apply the tangent functor to the multiplication and inversion operations (since the tangent functor preserves limits). Thus, for some tangent vector $[\gamma, \tau] \in T\mathcal{P} \times_{TP} TG$ , I would naively guess that the formula should be something like $$\overline{\omega} [\gamma, \tau] = \omega(\gamma) + \omega_G(\tau) $$ where $\omega_G$ is the Maurer-Cartan form for $G$ . However, while I have a formula for how the Maurer-Cartan form behaves with respect to the action of the tangent group $TP$ , I don't see how to get a similar formula for the Cartan connection with respect to the action of $TP$ .","['principal-bundles', 'connections', 'cartan-geometry', 'differential-geometry']"
4841344,What Is the Significance of Geometric Construction?,"So, when I was in high school, I learnt several geometric drawings such as bisecting line segments or angles, to constructing a square whose area is the sum of two other squares etc. using a ruler and a compass. Later, at a more mature level of my mathematics journey, I learnt these problems puzzled us since classical Greek era, and they constitute of the whole branch of Geometric construction. But my question is, why? In particular, the rules of the game (you can only use a straightedge and a compass, no other tool) as well as some of the problems seem rather arbitrary, as well as rather primitive. I am not necessarily asking practical applications, but do these methods or knowing the drawing techniques reveal anything particularly interesting, about structure of real numbers or topology of 2D surface etc.? The problems seem useful to build intuitions of high school students, but beyond that, is there anything deeper that warrants a name ( Geometric Construction ) for this whole branch of mathematics, and something that mathematicians find interesting?","['euclidean-geometry', 'math-history', 'geometry', 'geometric-construction']"
4841357,Can the vertices of three squares erected on the edges of a non-isosceles triangle lie on a conic?,"I came up with this guess about three years ago, but I wrote it down for the first time about two months ago, and I don’t know how it can be proven, nor whether it was previously discovered or not. Whoever can help, please do so, thank you. It is impossible for the six points to lie on a conic section if the triangle is not isosceles","['euclidean-geometry', 'conic-sections', 'geometry']"
4841397,Calculate the numbers obtained by a die,"Original question: ""A fair die is rolled 150 times. Let X denote the number of sixes obtained. Use an approximation to find: a)P(X>24)
b)P(20≤X≤25) A.0.534 B.0.107 C.0.025 D.0.391"" I tried to use the binomial distribution transferring to normal distribution with 25 as mean and 125/6 as variance(steps shown below), but the answer didn't suit to the answer given by calculator which is about 0.534 and 0.441. I'm not pretty sure if I used the corret method. So it would be very helpful if there's any methoed I don't know can solve it. Edit 1:I got 0.534 and 0.441 by input these to calculator $$1-\sum_{x=0}^{24} \binom{150}{x} \left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{150-x}$$ and the output is 0.53396149. $$\sum_{x=20}^{25} \binom{150}{x} \left(\frac{1}{6}\right)^x \left(\frac{5}{6}\right)^{150-x}$$ and the ouput is 0.4415964366. However, there's no option about 0.587 or 0.441. But I have no idea what's the problem, especially for 0.441, because I'm using the same formula on both, but one of them is correct and another one is not.","['statistics', 'probability']"
4841408,When does the derivative $f'(0)$ of $f(x)= (x^p)^{1/q}$ exist?,"Let $c=p/q$ where $p,q \in \mathbb Z$ are coprime and $q$ is odd. Define $f:\mathbb R \to \mathbb R$ by $f(x)=x^c=(x^p)^{1/q}$ . Question: When does $f^\prime(0)$ exist? What I've tried (and where I'm stuck): If $c<0$ , then $0^c$ is undefined. So, we must have $c\ge 0$ . If $c=0$ , then $f(x)=1$ for all $x$ and $f^\prime(0)=0$ . (Trivial case.) If $c=1$ , then $f(x)=x$ for all $x$ and $f^\prime(0)=1$ . (Trivial case.) If $0<c<1$ , then it seems that $f^\prime(0)$ is undefined. Is this true and how do I prove this? If $c>1$ , then it seems that $f^\prime(0)=0$ . Is this true and how do I prove this?","['calculus', 'analysis']"
4841480,"Cardinality of the set $\{f: S\times S\to S \: | \: f \text{ is onto and } f(a,b)=f(b,a)\geq a\ \ \forall (a,b)\in S \times S\}$, with $S=\{1,2,3,4\}$","I approached it this way. There would be $16$ elements in $S×S$ . Case $1$ : $(4,1),(1,4),(4,2),(2,4),(4,3),(3,4)$ and $(4,4)$ will have only $4$ as the choice for their image in $S$ Case $2$ : $(3,1),(1,3),(3,2),(2,3)$ and $(3,3)$ can have $3$ and $4$ as the choice for their image in $S$ Case $3$ : $(2,1),(1,2)$ and $(2,2)$ can have $2$ , $3$ and $4$ as the choice for their image in $S$ and Case $4$ : $(1,1)$ can have all of $1$ , $2$ , $3$ and $4$ as its image but it will have to take the image $1$ to make the function onto. Similarly any one function in Cases $2$ and $3$ will have to take $3$ and $2$ as their images respectively for function to become onto. Therefore Total Functions should be= $2$ × $2$ × $2$ × $2$ × $3$ × $3$ = $144$ . But the solutions I saw to this problem wrote total functions as $72$ . Also I am not able to reach the value of required functions from here. So please explain in elementary terms where I went wrong and how to proceed after this. PS: The answer to the question is $37$ .","['elementary-set-theory', 'functions']"
4841495,"$f$ and $h\circ f$ are linear and $f$ is surjective, is $h$ linear?","I have a very easy question but I can't find the solution. Let $V,U,W$ be three $\mathbb{R}$ -vector spaces and let $f: V \rightarrow U$ be a surjective linear map and $g: V \rightarrow W$ a linear map. Now, define $h : U \rightarrow W$ such that $h \circ f =g$ . I want to know if $h$ is linear or not. Let $u_1,u_2 \in U$ such that $u_1= f(v_1)$ and $u_2= f(v_2)$ and let $c \in \mathbb{R}$ .
Then, \begin{equation}
\begin{aligned}
h(cu_1+u_2) &= h(cf(v_1)+f(v_2)) \\
&= h(f(cv_1+v_2)) \\
&=g(cv_1+v_2) \\
&=cg(v_1)+g(v_2) \\
&= c h(f(v_1))+h(f(v_2)) \\
&= c h(u_1) +h(u_2)
\end{aligned}
\end{equation} Then, I want to conclude than $h : U \rightarrow W$ is linear. But now, take $f$ and $g$ such that $\text{ker} (f) \neq \text{ker} (g)$ (and suppose the kernel are not reduce to $0$ ). Then, there exists $v \in V$ such that $v \in \text{ker} (f)$ and $v \not\in \text{ker} (g)$ and so $$h(0) = h(f(v))=g(v) \neq 0$$ and so $h$ is not linear ... Can you explain to me what the problem is with my reasoning? My first calculation tells me that in any case $h$ is linear but the second says that when $\text{ker} (f) \neq \text{ker} (g)$ then $h$ is not linear ...",['linear-algebra']
4841508,Question About Collecting Power of $x^n$ in Taylor Expansion of $e^{\sin(x)}$.,"Theorem 1 For real numbers $\left\{a_{j,k}\right\}_{j,k\ge 1},$ if $$\sum_{j=1}^\infty \sum_{k=1}^\infty \left|a_{j,k}\right|<\infty$$ then for any bijection $\sigma : \mathbb{N} \to \mathbb{N}^2$ we have that the sums $$\sum_{j=1}^\infty \sum_{k=1}^\infty a_{j,k}
= \sum_{k=1}^\infty \sum_{j=1}^\infty a_{j,k} 
= \sum_{n=1}^\infty a_{\sigma(n)}$$ all converge and are equal to one another.
Now in some post I saw $$e^y=1+y+\frac12y^2+\frac16y^3+\frac1{24}y^4+\cdots\tag1$$ and that $$\sin(x)=x-\frac16x^3+\cdots\tag2$$ So, in the RHS of $(1)$ replace $y$ with the RHS of $(2)$ .
i.e $$e^{\sin(x)}=1+\left(x-\frac16x^3+\cdots\right)+\frac12 \left(x-\frac16x^3+\cdots\right)^2+\frac16 \left(x-\frac16x^3+\cdots\right)^3+\cdots+$$ Now since series of $\sin(x)$ is absolutely convergence things like $\left(x-\frac16x^3+\cdots\right)^2$ can be computed in any order. so we can collect power of $x$ in $\left(x-\frac16x^3+\cdots\right)^2$ .
My question is about why we can do this on $$e^{\sin(x)}=1+\left(x-\frac16x^3+\cdots\right)+\frac12 \left(x-\frac16x^3+\cdots\right)^2+\frac16 \left(x-\frac16x^3+\cdots\right)^3+\cdots+$$ if  we have $$|1|+\left(|x|+|-\frac16x^3|+\cdots\right)+\frac12 \left(|x|+ |-\frac16x^3|+\cdots\right)^2+\cdots+\tag{3}$$ is finite then we can use above Theorem 1 and collect power of $x^n$ . So my question is sum in $(3)$ is indeed finite . Or there is something different thing that we are using there. Edit:
Let $f(x)=\sum a_n x^n$ and $g(x)=\sum b_n x^n$ . assume Radius of convergence of $\sum a_n x^n$ is enough for composition then we have $$f(g(x))=a_0+a_1 \left(\sum b_n x^n \right)^1+a_2 \left(\sum b_n x^n \right)^2 +\cdots$$ Now my question is about rearrange of this series. If it is always true that $$|a_0|+a_1 \left(\sum |b_n x^n| \right)^1+a_2 \left(\sum |b_n x^n |\right)^2 +\cdots$$ Is finite then we can use theorem and it answers my question. If not then why we can rearrange?","['analysis', 'real-analysis', 'taylor-expansion', 'sequences-and-series', 'power-series']"
4841554,Convergence of subsequence defined by geometric sequence,"Let $\{x_n\}$ be a real sequence. Suppose that for any geometric sequence $\{a_n\}$ with $a_1>1$ and the common ratio greater than $1$ , $\lim\limits_{n\rightarrow+\infty}x_{[a_n]}=l$ , where $[\cdot]$ is the floor function. Show that $\{x_n\}$ converges to $l$ . Here, floor function $[x]$ is the greatest integer less than or equal to $x$ . If the condition “geometric sequence” is replaced by ""arithmetic sequence"", then the problem is easy to solve because finite many arithmetic sequences will cover all indices; for example, we may take $a_1$ and the common difference $d$ to be arbitrary positive integers, and then $d$ subsequences will cover the whole sequence. An typical example is the even subsequence and the odd subsequence. However, is this idea still applied to “geometric sequence”? If we only consider all index sequences $\{a_n\}$ within integers, it will certainly fail because any prime integer must be the first term of some sequence. Hence, we have to consider the case when the first term $a_1$ or the common ratio is not an integer. Maybe it is related to elementary number theory, isn't it?","['elementary-number-theory', 'analysis', 'real-analysis', 'calculus', 'sequences-and-series']"
4841568,The Stratonovich operator application,"I am self-studying https://sayanmuk.github.io/StochasticAnalysisManifolds.pdf and I am struggling with the proof of the following proposition: Proposition 2.4.2: Let $\theta$ be a 1-form on $M$ and $X$ the solution of the equation $dX_t = V_\alpha(X_t) \circ dZ_\alpha$ . Then, $$
\int_{X[0,t]} \theta = \int_{0}^{t} \theta(V_\alpha)(X_s) \circ dZ_\alpha
$$ Proof: From Lemma 2.3.8, we have $dW_t = U_t^{-1} V_{\alpha}(X_t) \circ dZ_{\alpha}$ . Hence, the differential of the line integral is: $$\widetilde{\theta}(U_t) \circ dW_t = \left\langle \widetilde{\theta}(U_t), U_t^{-1} V_{\alpha}(X_t) \right\rangle \circ dZ_{\alpha} = \theta(V_{\alpha})(X_t) \circ dZ_{\alpha}
$$ My problem is understanding how the Stratonovich integral was applied, or more specifically the operator $\circ$ . By definition, the Stratonovich integral relates with the Ito integral in the following way: $X_t = X_0 + \int_{0}^{t} V_\alpha(X_s) \, dZ_\alpha(s) + \frac{1}{2} \int_{0}^{t} \nabla_{V_\beta} V_\alpha(X_s) \, d\langle Z_\alpha \, dZ_\beta\rangle_s$ . I could see the second term with the covariation is very similar apart from the differential to $\langle\tilde{\theta} (U_t), U^{-1}_t V_\alpha(X_t) \rangle$ , but even so it needs to be differentiated. The first component I do not know how it would disappear. This is just a summary of my handwritten attempts. Question : Can someone explain me this first step in the proof?","['stochastic-analysis', 'differential-geometry']"
4841584,Showing $ \tan\frac{A+B}{2}= \frac{\sin A+\sin B}{\cos A+\cos B}$,"Problem Statement When helping a high school student, I came across this problem: show that $$
\tan\left(\frac{A+B}{2}\right)=
\frac{
\sin\left(A\right)+\sin\left(B\right)
}{
\cos\left(A\right)+\cos\left(B\right)
}
$$ My Solution 1 Define $X=\frac{A+B}{2}$ and $Y=\frac{A-B}{2}$ . We have $$
\begin{aligned}
\sin\left(A\right)&=\sin\left(X+Y\right)\\
&=\sin\left(X\right)\cos\left(Y\right)+\sin\left(Y\right)\cos\left(X\right)
\\
\\
\sin\left(B\right)&=\sin\left(X-Y\right)\\
&=\sin\left(X\right)\cos\left(Y\right)-\sin\left(Y\right)\cos\left(X\right)
\\
\\
\cos\left(A\right)&=\cos\left(X+Y\right)\\
&=\cos\left(X\right)\cos\left(Y\right)-\sin\left(X\right)\sin\left(X\right)
\\
\\
\cos\left(B\right)&=\cos\left(X-Y\right)\\
&=\cos\left(X\right)\cos\left(Y\right)+\sin\left(X\right)\sin\left(X\right)
\end{aligned}
$$ Substitute these to the RHS then we have $$
\frac{
\sin\left(A\right)+\sin\left(B\right)
}{
\cos\left(A\right)+\cos\left(B\right)
}
=
\frac{\sin\left(X\right)}{\cos\left(X\right)}=\tan\left(\frac{A+B}{2}\right)
$$ My Solution 2 Define $O$ as origin of the cartesian plane. Consider two unit vectors $$
\begin{aligned}
\overrightarrow{OP}&=\begin{bmatrix}\cos\left(A\right)&\sin\left(A\right)\end{bmatrix}^{\top}
\\
\overrightarrow{OQ}&=\begin{bmatrix}\cos\left(B\right)&\sin\left(B\right)\end{bmatrix}^{\top}
\end{aligned}
$$ The two vectors form angles $A$ and $B$ respectively with the positive $x$ axis. The following vector bisects the angle $\angle POQ$ : $$
\overrightarrow{OP}+\overrightarrow{OQ}
=
\begin{bmatrix}\cos\left(A\right)+\cos\left(B\right)&\sin\left(A\right)+\sin\left(B\right)\end{bmatrix}^{\top}
$$ Therefore, we have $$
\tan\left(\frac{A+B}{2}\right)=
\frac{
\sin\left(A\right)+\sin\left(B\right)
}{
\cos\left(A\right)+\cos\left(B\right)
}
$$ My Solution 3 (new) Using a unit circle, the following picture illustrate the proof: Remarks I find the second solution simpler and wish to ask others if there are other elementary alternative solutions.","['trigonometry', 'solution-verification']"
4841585,Is there a theorem which provides conditions under which a power series satisfies the reciprocal root sum law?,"This  paper discusses how to prove that $\sum\limits_{n=1}^\infty \frac{1}{n^2}=\frac{\pi^2}{6} $ . The first proof on this paper is Euler's original proof: $$\frac{\sin(\sqrt x)}{\sqrt x} = 1- \frac{x}{3!}+ \frac{x^2}{5!}- \frac{x^3}{7!}+\dots$$ The roots of $\frac{\sin(\sqrt x)}{\sqrt x}$ n are the numbers $\pi^2, 4\pi^2, 9\pi^2, 16\pi^2, \dots$ Now Euler knew that adding up the reciprocals of all the roots of a polynomial results in the negative
of the ratio of the linear coefficient to the constant coefficient. In symbols, if $$(x − r_1)(x − r_2)···(x − r_n) = x^{n} + a_{n−1}x^{n−1} + \dots + a_1x + a_0$$ then $$\sum_{k=1}^n \frac{1}{r_k}= \frac{-a_1}{a_0}$$ Assuming that the same law must hold for a power series expansion, he applied it to $\frac{\sin(\sqrt x)}{\sqrt x}$ ,
concluding that $$\frac{1}{6}=\sum\limits_{n=1}^\infty \frac{1}{(\pi n)^2}$$ Why is this not considered a valid proof today? The problem is that power series are not polynomials,
and do not share all the properties of polynomials. The next page the author wrote this : $$\frac{1}{1 − x}=1+ x + x^2 + x^3 + \dots$$ holds for all $x$ of absolute value less than $1$ . Now consider the function $g(x)=2 − 1/(1 − x)$ . Clearly, $g$ has a single root, $1/2$ . The power series expansion for $g(x$ ) is $ 1−x−x^2−x^3−···$ , so $a_0 = 1$ and $a_1 = −1$ .
The sum of the reciprocal roots does not equal the ratio $−a_1/a_0$ . While this example shows that the
reciprocal root sum law cannot be applied blindly to all power series, it does not imply that the law
never holds. Indeed, the law must hold for the function $\frac{\sin(\sqrt x)}{\sqrt x}$ because we have independent
proofs of Euler’s result. Notice the differences between $\frac{\sin(\sqrt x)}{\sqrt x}$ and the $g$ of the counterexample. The
function $\frac{\sin(\sqrt x)}{\sqrt x}$ has an infinite number of roots, where $g$ has but one. And $\frac{\sin(\sqrt x)}{\sqrt x}$ has a power series that converges
for all $x$ , where the series for $g$ only converges for $−1 <x< 1$ . $\color{red}{\text{Is there a theorem which provides
conditions}}$ $ \color{red}{\text{under which a power series satisfies the reciprocal root sum law? I don’t know.}}$ Now I tried to search for a this theorem and I couldn't find any thing. Is there is a theorem like this somewhere ? Is there a proof for a  under which a power series satisfies the reciprocal root sum law?","['summation', 'analysis', 'reference-request', 'real-analysis', 'sequences-and-series']"
4841591,Reverse L'Hospital rule under certain conditions,"Let $p$ be a positive real number, and $f:\mathbb{R}^+\rightarrow\mathbb{R}$ a differentiable function. Suppose that $f'(x)$ is monotonically increasing. Show that $\lim\limits_{x\rightarrow+\infty}\frac{f(x)}{x^p}=l$ if and only if $\lim\limits_{x\rightarrow+\infty}\frac{f'(x)}{px^{p-1}}=l$ . The direction "" $\Leftarrow$ "" follows from L'Hospital rule. By the way, many textbooks only prove the L'Hopsital rule for $\frac{0}{0}$ case, but just say that the proof for $\frac{\infty}{\infty}$ case is similar. I do not think that the proof for $\frac{\infty}{\infty}$ case is as easy as that for $\frac{0}{0}$ case. Come back to our question, how to prove "" $\Rightarrow$ ""?","['analysis', 'real-analysis', 'calculus', 'limits', 'derivatives']"
4841631,Sequence of polynomials $\{P_n\}$ of bounded degree such that $\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x$ converges for all test functions $\varphi$,"I was wondering that: Suppose that $\{P_n\}$ is a sequence of polynomials with degree $\le p$ such that for every test function $\varphi\in C^\infty_c(\mathbb{R}^d)$ , the sequence of integrals $\left\{\displaystyle\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x\right\}$ converges. Must the coefficients of $\{P_n\}$ of each degree converge? Here $C^\infty_c(\mathbb{R}^d)$ is the space of smooth functions over $\mathbb{R}^d$ with compact support. I would like to find a function $\varphi$ with compact support such that, for a given multiindex $|\alpha_0|\le p$ , we have $$
\int_{\mathbb{R}^d}x^{\alpha_0}\varphi(x){\rm d}x\neq 0,\quad\int_{\mathbb{R}^d}x^\alpha\varphi(x){\rm d}x=0,\quad\forall|\alpha|\le p,\alpha\neq\alpha_0,
$$ then plugging this function into the integral shows that the $\alpha$ -coefficients of $\{P_n\}$ converge. Could you please give some strategies to construct such $\varphi$ if it does exist, or is there any more elegant way to think about this question? Edit: The following result would be applicable. (Banach-Steinhaus theorem of distributions) Suppose that $\{T_n\}$ is a sequence of distributions over an open set $\Omega\subset\mathbb{R}^d$ , and that $\langle T_n,\varphi\rangle$ converges for every $\varphi\in C^\infty_c(\Omega)$ , then $\{T_n\}$ converges in $D'(\Omega)$ (the space of distributions). So we know that $\{P_n\}$ converges to some distribution $T$ . Moreover, $$
0=\partial^{\alpha}P_n\to\partial^{\alpha}T,\quad\forall|\alpha|=p+1,
$$ so $\partial^{\alpha}T=0$ for every $|\alpha|=p+1$ . It is standard (can be shown by induction on $p$ ) that $T$ is itself defined by a polynomial $P$ with degree $\le p$ . By subtracting $P$ we can suppose that $$
\lim_{n\to\infty}\displaystyle\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x=0,\quad\forall\varphi\in C^\infty_c(\mathbb{R}^d),
$$ and I want to show that the coefficients of $\{P_n\}$ of each degree converge to $0$ .","['distribution-theory', 'sequences-and-series', 'polynomials', 'real-analysis']"
4841651,A definite integral over the unit sphere,"Is there a closed form for the following definite integral over the unit sphere? $$I = \int_0^{\pi}d\theta \sin\theta \int_0^{2\pi}d\phi |x (\sin\theta\cos\phi)^2 +(1-x)(\sin\theta\sin\phi)^2 - (\cos\theta)^2| $$ where $x\in[0,2]$ . This came up in a physics problem. Mahthematica (surprisingly) returned the result $I = 4 \pi  \sqrt{\frac{x}{(x+1)^3}}$ which really only agrees with the numerical integration at $x=\frac{1}{2}$ , so it is most likely wrong. Since the function inside the absolute value signs is just a weighted sum of squares of the coordinates of the unit directional vector, I thought that it might be dealt with by going to some sort of ellipsoidal coordinate system. But so far I have not found a way to make it work. Update 1 : Thanks to the comment from @SangchulLee , I was able to proceed a bit further to the following intermediate result. First, we substitute $$u(\phi)=x\cos^2\phi+(1-x)\sin^2\phi$$ Then, the integral reduces to $$ I = \int_0^{2\pi}d\phi\int_{-1}^1dt | (1-t^2) u(\phi) - t^2 | = \frac{2}{3} \int_0^{2\pi}d\phi \left[ 1-2u(\phi) + 4u(\phi)\sqrt{\frac{u(\phi)}{1+u(\phi)}}\Theta(u(\phi)) \right] $$ where $\Theta$ is the Heaviside theta function. The integral of the first two summands yields zero, so we obtain $$ I = \frac{8}{3} \int_0^{2\pi}d\phi \sqrt{\frac{u(\phi)^3}{1+u(\phi)}}\Theta(u(\phi))$$ Thus, for some special values of $x=0,1,\frac{1}{2}$ , the integral can be easily performed. However, I am not able to go further from here on.","['integration', 'multivariable-calculus', 'definite-integrals']"
