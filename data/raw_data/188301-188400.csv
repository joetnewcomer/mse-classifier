question_id,title,body,tags
3509979,"Number of functions $f:\{1,...,n\}\to\{1,...,n\}$ that have $|f^{-1}(\{i\})|=i$ for some $i$","Let $S=\{1,...,n\}$ , I am looking at number of functions functions $f:S\to S$ such that there exists $i \in S$ such that $$|f^{-1}(\{i\})|=i$$ I am guessing I am supposed to use PIE (Principle of Inclusion and Exclusion) I let $X=\{f:S\to S\}$ .
My attempt is to define $A_i=\{f\in X;|f^{-1}(\{i\})|=i\}$ . It can be computed that $|A_i|=\binom{n}{i}(n-1)^{n-i}$ . Similarly, $|A_i\cap A_j|=\binom{n}{i}\binom{n-i}{j}(n-2)^{n-i-j}$ . but I am not sure how to proceed.","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
3510025,Limit of alternating sum of fractional parts,"Find $\newcommand{\pars}[1]{\left\{ \frac{n}{#1} \right\}}$ $$\lim_{n\to\infty}\dfrac{1}{n} \left( \pars{1} - \pars{2} + ... + (-1)^{n+1} \pars{n} \right),$$ where $\left\{ x \right\} $ denotes the fractional part of $x$ . My guess is that the limit is equal to 0; I tried finding some asymptotics for the fractional part sum, by looking for example at ways to bound $\pars{k} - \pars{k+1}$ ; My intuition is that this difference is rather small(perhaps less than $\frac{n}{k(k+1)}$ ) and that it is big enough to be relevant only when one of them is zero, meaning that $k$ or $k+1$ divides $n$ . This would lead me to conjecture that it grows at most as $O(\sqrt{n})$ , which would make the limit zero, but I have not been able to make this rigorous. Another idea I had would be to look at the sum with odd denominator and the sum with even denominators and show that they must be ""rather"" close; this seems pretty intuitive but the fractional part is very chaotic and I have not been able to get any bounds. Any ideas/tips would be appreciated!","['limits', 'sequences-and-series', 'fractional-part', 'real-analysis']"
3510069,"Does a symmetric expression in $x,y$ take optimum value only if $x=y$?","Suppose $f(x,y)$ is a symmetric expression in $x,y$ .Suppose $f$ has a maximum or minimum.Then does it occur at a point where $x=y$ ?Can it happen that $f$ is optimum although $x\neq y$ ?
I have used this result many times this thing but I want to know rigorously if the result is always true.Can someone help me?I think due to symmetry this should always happen.","['optimization', 'multivariable-calculus', 'functions', 'real-analysis']"
3510102,probability for a given randomized recurrence relation,"Q: There are two numerical sequence $X_n$ and $Y_n $ . $X_n$ takes random integer from one to six for a given n. $Y_1=X_1$ , and $$Y_n=X_n+\dfrac{1}{Y_{n-1}}$$ . Find the probability $P$ that $$\dfrac{1+\sqrt3}{2}\leq Y_n \leq 1+ \sqrt3$$ when n is very large. I can calculate the limit if $X_n$ is constant. But have no idea how to deal with the random number. Thanks in advance.","['random', 'recurrence-relations', 'probability-theory', 'probability']"
3510156,Solve the Diophantine Equation $x^2 + 7 = y^5$.,"This is a duplicate question of Find integers solutions of $x^2+7=y^5$ , however there was no full answer. The solutions $(\pm5, 2)$ and $(\pm 181, 8)$ have been found. The usual strategy for such a question is to work inside the ring of integers of $\mathbb{Q}(\sqrt{-7})$ , which is $\mathcal{O} = \mathbb{Z}[ \frac{1+\sqrt{-7}}{2}]$ . It turns out that this is a unique factorisation domain (which one can figure out by calculating its Class group). So it is natural to factor the equation as $(x - \sqrt{-7})(x+\sqrt{-7}) = y^5$ . If we assume that $x-\sqrt{-7}$ and $x+\sqrt{-7}$ are coprime, we find that $x+\sqrt{-7} = \beta^5$ for a certain $\beta = a + b\frac{1+\sqrt{-7}}{2}\in \mathbb{Z}[\frac{1+\sqrt{-7}}{2}]$ . Writing $c= 2a+b$ and expanding the fifth power, this gives the system of equations $$ c^5 -70 c^3 b^2 + 245 c^4 b = 32 x, $$ $$ 5 c^4 b -70 c^2 b^3 + 49 b^5 = 32. $$ Now with enough patience, one can show that this system has no solutions with $b \equiv c \pmod{2}$ . However this contradicts the solutions that we have found. And indeed there's no reason for $x \pm \sqrt{-7}$ to be coprime when $x$ is odd. What is the approach to solve the remaining case of this diophantine equation? One approach that I have tried is that the coprime condition holds inside the ring $\cal{O}[\frac{1}{2}]$ . This gives the equation $x + \sqrt{-7} = (a+b\sqrt{-7})^5$ with $a,b \in \mathbb{Z}[\frac{1}{2}]$ , which I am unable to solve.","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'diophantine-equations']"
3510226,How does using limits to find the derivative of a function avoid dividing by $0$?,"What is wrong with this argument that differentiation using the First Principle leads to division by $0$ : $$
f'(x)=\lim_\limits{h \to 0} \frac{f(x+h)-f(x)}{h} \\
$$ Using the quotient limit law: $$
\lim_\limits{h \to 0} \frac{f(x+h)-f(x)}{h}=\frac{\lim_\limits{h \to 0}f(x+h)-f(x)}{\lim_\limits{h \to 0}h}
$$ $$
\lim_\limits{h \to 0}h = 0
$$ Therefore, there the top half of the fraction is divided by $0$ . Here is my reasoning for why $\lim_\limits{h \to 0}h = 0$ : As $h$ approaches $0$ , its value becomes smaller (and will become smaller than any number strictly greater than $0$ ). For example, you cannot evaluate the limit as equalling $0.001$ , because at some point $h$ will be lower than this. $0$ is the largest number that does not have this problem. Therefore, the limit expression is equal to $0$ . Thank you for reading.","['calculus', 'derivatives', 'intuition']"
3510227,Two ways of interpreting matrix-vector multiplication?,"I have been trying to get an intuitive grasp of the matrix-vector multiplication operation. So far, I've consumed both 3Blue1Brown's videos on this topic as well as studied Gilbert Strang's textbook chapters relevant to matrix-vector multiplication. Both seem to offer two very different ways of intuitively looking at the product. 3Blue1Brown interprets the matrix as transforming the vector by performing operations such as ""stretching"", ""shrinking"", ""rotating"", etc. In other words, the thing being transformed is the vector. Strang, on the other hand, interprets this operation as the columns of the matrix being added together with the elements of the vector acting as coefficients. In other words, the things being transformed are the columns of the matrix. I see these as two very different interpretations of the same thing. Is there some way to reconcile these two things? Am I missing something painfully obvious?","['linear-algebra', 'intuition']"
3510250,How to calculate using derivative definition,"I am trying to find the derivative of $f$ at $x=1$ when $y=\sqrt{3x+1}$ using only the following derivative definition: $$f^\prime(1)=\lim_{x\to1}\frac{f(x)−f(1)}{x−1}$$ I think I need to get rid of the radical first, but I simply cannot find the right way. I am relearning math after many years, so please bear with me.","['radicals', 'derivatives']"
3510262,Solving $\frac{d \xi(t)}{dt} = 1 + (\xi(t))^2$,"How should I go about solving $\frac{d \xi(t)}{dt} = 1 + (\xi(t))^2$ . So far I have noticed it has the form of a Riccati equation, which tells us that a solution is of the form $\xi(t) = \frac{u'}{u}$ where $u$ solves $$ u'' - (q_1 + \frac{q_2'}{q_2})u' + q_2q_0 u = 0,$$ (using the notation of the wiki ) which gives us $$u'' + xu =0.$$ But this causes me problems to solve. (This question comes about whilst trying to solve the PDE $$(1+x^2)u_x + u_y =0, \quad u(0,y) = g(y) $$ by the method of characteristics, $\xi(t)$ is intended to be the characteristic line.)","['partial-differential-equations', 'characteristics', 'ordinary-differential-equations', 'real-analysis']"
3510270,"Find $f \in C^1$ when $\forall x\in\mathbb{R}\;h\in\mathbb{R}\;\; f(x) \geq x+1,\: f(x+h) \geq f(x)f(h)$","$$\forall x\in\mathbb{R}\;h\in\mathbb{R}\;\; f(x) \geq x+1,\: f(x+h) \geq f(x)f(h)$$ $f(x)$ is defined and differentiable in $\mathbb R$ I assume that $f(x)=e^x$ , but I can't prove why...","['functional-equations', 'functional-inequalities', 'real-analysis', 'functions', 'inequality']"
3510272,Why should the trace of a 3d rotation matrix have these properties?,"On the Wikipedia article about Rotation Matrices ( https://en.wikipedia.org/wiki/Rotation_matrix#Determining_the_angle ), the article states that the trace of the matrix will be equal to 1 + 2 cos(theta), where the theta represents the angle of the rotation in axis/angle form. How is this property found?  There doesn't appear to be any derivation on the site, and I can't see any reason why it might be the case.","['trace', 'matrices', 'orthogonal-matrices', 'linear-algebra', 'rotations']"
3510322,"Can $\Phi :C_0^\infty(\Omega) \rightarrow \mathbb{R}, f \mapsto f(0)$ be extended to $H_0^0(\Omega)$ or $H_0^1(\Omega)$ and find function","Let $\Omega=(-1,1)$ and consider the functional $\Phi :C_0^\infty(\Omega) \rightarrow \mathbb{R}, f \mapsto f(0)$ . Can this functional be extended to a continous linear functional on $H_0^0(\Omega)$ or $H_0^1(\Omega)$ ? Show that there is a unique function $u \in H_0^1(\Omega)$ , such that $$\int_{-1}^1 u'f' dx = f(0) \; \operatorname{for all} \; f \in C_0^\infty(\Omega)$$ and find the function $u$ . For the first part I need your help. I know that the solution has to be that $\Phi$ can be extended to $H_0^1(\Omega)$ , otherwise the second part of the task wouldn't make sense, but how can I show that? For the second part I have the following approach: The term $\int_{-1}^1 u'f' dx$ is one part of the usual scalarproduct on the Sobolev space $H_0^1(\Omega)$ given by $$ (u,v)_{H^1_0(\Omega)=} = \int_\Omega u(x)v(x) dx + \int_\Omega u'(x)v'(x) dx$$ Assume that we have extended $\Phi: H_0^1(\Omega) \rightarrow \mathbb{R}$ . Since $H_0^1(\Omega)$ is a Hilbert space and $\Phi$ is a bounded linear functional, the Riesz Representation Theorem can be applied and yields that there exists an unique $u \in H_0^1(\Omega)$ such that $$ (u,f)_{H_0^1(\Omega)} = \Phi(f) = f(0) \; \operatorname{for all} f \in H_0^1(\Omega)$$ To get the desired result, $\int_\Omega u(x)f(x) dx$ has to vanish in the scalarproduct $(u,f)_{H_0^1(\Omega)}$ . Do you have an argument for that? Concerning the construction of $u$ , set $u(x)=
\begin{cases}
0.5(1-x) &\text{ for } x>0\\
0.5(1+x) &\text{ for } x\leq 0
\end{cases}$ , $\;\;$ so $
u'(x)=
\begin{cases}
-0.5 &\text{ for } x>0 \\
0.5 &\text{ for } x\leq 0
\end{cases}
$ and $$\int_{-1}^1 u'(x)f'(x) dx = 0.5 \int_{-1}^0 f'(x) dx + -0.5\int_{0}^1 f'(x) dx \\
= 0.5[f(0)-f(-1)] - 0.5[f(1)-f(0)] = f(0) - 0.5[f(-1)+f(1)] = f(0),$$ where $f(-1) = f(1) = 0$ , because $-1$ and $1$ are the boundary points of the support and f is continuous.","['hilbert-spaces', 'functions', 'sobolev-spaces', 'functional-analysis']"
3510330,rolling dice - win if the sum of rolls is exactly $n$,"This question was asked during my interview: Suppose you have a fair dice (6 faces as usual). You can pick a positive integer $n$ . Then you can repeatedly roll a dice until the sum of the rolls exceeds or equals to $n$ . If the sum is exactly $n$ , it is a win. Otherwise, you lose. Find $n$ that maximizes your chance of winning. Let $P(n)$ be probability of win when the value is $n$ . Then, $$P(n) = \sum_{i=1}^6 \frac{1}{6}P(n-i)$$ However, solving this recurrence seems complicated and tedious. Is there an easier way to solve this?",['probability']
3510334,Proving diagonal principle for nets,"Problem 11D in Willard's General topology textbook states that: Diagonal principle) If $(x_i)$ converges to $x$ and, for each $i\in I$ , a net $(x^i_j)_{j\in J_i}$ converges to $x_i$ , then there is a diagonal net converging to $x$ ; i.e., the net $(x^i_j)_{i\in I,\,j\in J_i}$ , ordered lexicographically by $I$ , then by $J_i$ , has a subnet which converges to $x$ . How do you prove this fact ? Here's my progress: Firstly I found the wording of the problem to be a bit tad too confusing -- here's what I understand: So we have a directed set $D$ , and a net $N$ on $X$ , and for each $x_\lambda \in N$ , there's a directed set $D_\lambda$ and $N_\lambda$ on $X$ such that it converges to $x_\lambda$ . Define a new directed set $D' = \{ (i, j_i) | i \in D, j_i \in D_i \}$ such that $(\lambda_1, \mu_{\lambda_1}) \leq (\lambda_2, \mu_{\lambda_2})$ iff $\lambda_1 \leq \lambda_2$ and $\mu_{\lambda_1} \leq \mu_{\lambda_2}$ and the new net $N' = \cup_{x_\lambda \in N} N_\lambda$ such that if $i \in D$ corresponds to $X_i \in X$ , and $j_i \in D_i$ corresponds to $X_{j_i} \in X$ , then $(i,j_i) \in D'$ corresponds to $X_{j_i}$ . We wish to find a subnet of $N'$ such that it converge to $x$ (the original net $N$ converges to $x$ ) Let $\mathfrak{U}$ be the collection of open sets around $x$ . Define a function $f: N \mapsto P(\mathfrak{U})$ as follows: $f(x_\lambda) = \{ U \in \mathfrak{U} | x \in U, \lambda \leq \lambda' \Rightarrow x_{\lambda'} \in U \}$ . On the other hand, each $U \in f(x_\lambda)$ is an open set containing $x_\lambda$ too, so by the convergence of $N_\lambda$ , we can define a function $g_\lambda: f(x_\lambda) \mapsto D_\lambda$ such that if $g_\lambda(U) \leq \mu$ , then $x_\mu \in U$ . Now it would have been nice to define $\displaystyle h_\lambda > g_{\lambda}(U) $ for all $U \in f(x_\lambda)$ , and let $D'' = (\lambda, h_\lambda)$ and take $N''$ to be correspodning subnet induced by $D''$ . But the problem here is that $h_\lambda$ may note be defined when $|f(x_\lambda)|$ is infinite, and this is where I'm stuck.","['general-topology', 'nets', 'convergence-divergence']"
3510411,Are there function with one local extremum but no global extrema?,"Is it possible to find function $f : \mathbb R^2 \to \mathbb R$ such that $f$ is $C^2$ and has one, and only one local extremum but no global extremum. I tried to make up an example with this function $f : (x,y) \mapsto e^{-(x^2+y^2)}+2x+2y$ whose graph looks like this : I tried to show that $f$ has only one global maximum by showing there exists only one point where $\nabla f(x,y) = 0$ : I obviously started by calculating, for $x, y \in \mathbb R$ , $\nabla f (x,y)$ : $$\nabla f (x,y) = \begin{bmatrix} -2xe^{-(x^2+y^2)}+2\\ -2ye^{-(x^2+y^2) }+2 \end{bmatrix}$$ I need to show that the $\nabla f(x,y) = 0$ has only one solution and that it is a local extremum. Any help would be apreciated!","['multivariable-calculus', 'calculus']"
3510459,"Derive ideal form of Bézout's Identity: $a\Bbb Z + b\Bbb Z = \gcd(a,b)\Bbb Z$","Generalized statement of Bézout's Lemma. Let $a, b$ be integers. Then $\{ax + by : x, y \in \mathbb{Z}\}$ , that is, the numbers generated by $a$ and $b$ , is equal to the multiples of $\text{gcd}(a, b)$ . Bézout's Identity. If $d = \text{gcd}(a, b)$ , then $ax + by = d$ for some integers $x$ and $y$ . How does (2) imply (1)? If this is considered ""obvious,"" I cannot see it and would benefit from the logic of the derivation being explained to me as if I were a middle schooler.","['number-theory', 'gcd-and-lcm', 'proof-explanation', 'elementary-number-theory', 'ideals']"
3510489,"Expectation of $Y$ when $X,Y$ are jointly distributed.","Suppose $X,Y$ are jointly distributed continuous random variables with probability density function $f_{X,Y}(x,y)$ . I know that in order to recover the marginal distribution of one of the random variables, say $Y$ , we can compute $$f_{Y}(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dx .$$ My question is about computing $E[Y]$ when starting from the above situation. Considering the fact that the definition of the expectation is $$E[Y] := \int_{-\infty}^{\infty} y \cdot f_{Y}(y) \, dy, $$ My approach is then to compute the expectation as $$E[Y] = \int_{-\infty}^{\infty} y \cdot f_Y(y) \, dy = \int_{-\infty}^{\infty} y \left[ \int_{\infty}^{\infty} f_{X,Y}(x,y) \, dx \,\right] dy = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X,Y}(x,y) \, dx \, dy. $$ However, I regularly see solutions that will compute it as $$E[Y] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \cdot f_{X,Y}(x,y) \, dy \, dx. $$ I am familiar with the concept of changing the order of integration, but this seems like more than that. It is no longer clear to me why this still fits the definition of the expected value, because I don't see how we are recovering the marginal pdf of $Y$ and then integrating it against $y$ to arrive at the expectation. I have asked several friends who also solved problems this way why it is valid and none of them seem to have an answer and they just say ""why wouldn't you be able to calculate it this way?"". So either I am crazy or they are unconscious statisticians. On that note, I did notice on a formula page in a text book it has an identity: $$E[g(X,Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) \cdot f_{X,Y}(x,y) \, dy \, dx.$$ In the above I agree the order of integration wont matter (by setting X = Y and vice versa). I suppose in this result if you take the function $g(X,Y) = Y$ it will permit the computation that I am hesitant about. Is this how we know we can do that? Or is it simpler than that and I am just being crazy? As a concrete example, here is a specific problem where the solution provided uses the method I am hesitant about. Let $X$ and $Y$ denote the values of two stocks at the end of a five-year period. $X$ is uniformly distributed on the interval $(0,12)$ . Given $X = x$ , $Y$ is uniformly distributed on the interval $(0,x)$ . Find $E[Y]$ . Please remember, my question is not how to solve this problem. It is why a specific method works. My method of solving this would be to first discover that the support of $(X,Y)$ is $0 < y < x < 12$ . Then since $f_{Y|X}(y|x) = x^{-1}$ and $f_{X}(x) = 12^{-1}$ we can deduce that $f_{X,Y}(x,y) = (12x)^{-1}$ . Then compute $$f_{Y}(y) = \int_{y}^{12} (12x)^{-1} \, dx = (1/12)[\ln(12) - \ln(y)].$$ Then use this to compute $$E[Y] = \int_{-\infty}^{\infty} y \cdot f_{Y}(y) \, dy = \int_{0}^{12} y \cdot (1/12)[\ln(12) - \ln(y)] \, dy = 3.$$ Computing the last integral was... 'do-able' for a well-practiced integrater, but it was not ideal. The solution posted was the following: $$E[Y] = \int_{0}^{12} \int_{0}^{x} (y/12x) \, dy \, dx = 3.$$ The above integral is much easier to solve, so once I understand this is a valid way to compute the expectation I will happily add this to my tool belt for solving problems. But again, I don't see how it fits the definition of expectation, because I don't see how it is recovering the marginal distribution for $Y$ . Unless, doing it this way is using the identity that I mention in the middle block of text. So why is the other method valid?","['statistics', 'probability-distributions', 'calculus', 'probability-theory', 'probability']"
3510494,Do eigenvalues depend smoothly on the matrix elements of a diagonalizable matrix?,Suppose I have a matrix $M(t)$ whose matrix elements depend smoothly on a real parameter $t$ . I also know that this matrix is diagonalizable for the $t$ s I'm interested in. Can I say that its eigenvalues depend smoothly on $t$ ? Thanks very much!,"['eigenvalues-eigenvectors', 'real-analysis', 'matrices', 'linear-algebra', 'matrix-analysis']"
3510534,A data processing inequality for a non-$f$ divergence?,"Consider two probability distributions $P$ and $Q$ on some space $\mathcal X$ . Given an convex function with $f(1) = 0$ , the $f$ -divergence from $P$ to $Q$ is defined by $$
D_f(P \| Q) = \int f\left(\frac{dP}{dQ}\right) \ dQ.
$$ A well-known property of $f$ -divergences is the data processing inequality $D_f(P \| Q) \ge D_f(T^{-1} P \| T^{-1} Q)$ where $T^{-1}$ is a measurable mapping from $\mathcal X$ to another space $\mathcal Y$ . The intuition being that you can't make two distributions easier to distinguish by applying an a-priori known transformation to both. Given this background, I am interested in the following divergence $$
V(P \| Q) = \int \left(\log \frac{dP}{dQ}\right)^2 \ dQ.
$$ This is not an $f$ -divergence, and it is possible to show that the data processing inequality fails (I randomly searched over $\mathcal X = \{1,2,3\}$ and $\mathcal Y = \{1,2\}$ and found a counterexample). My question is whether a weaker version of the data processing inequality holds. For example, could we find a $K$ such that $V(T^{-1} P \| T^{-1} Q) \le K \, V(P \| Q)$ ? Or a function $\phi(x)$ such that $$
V(T^{-1} P \| T^{-1} Q) \le \phi\{V(P \| Q)\} \, V(P \| Q)
$$ where $\phi(x)$ is bounded near $0$ ? Or is there an obstruction to this type of result?","['statistics', 'probability-theory', 'information-theory']"
3510576,Does any pseudo-Riemannian metric admit a metric-compatible linear connection with vanishing curvature?,"It is well-known that a pseudo-Riemannian manifold $(M,g)$ admits a (unique) torsion-free linear connection $\stackrel{\circ}{\nabla}$ that is compatible with the given metric, i.e. $\stackrel{\circ}{\nabla} g = 0$ . Is it true that there also exists a curvature-free metric-compatible connection $\nabla$ ? Or under what circumstances? Edit: About my conjecture below: In the refined case of a parallelizable pseudo-Riemannian manifold, does there always exist a curvature-free metric-compatible linear connection? Take as an counterexample the maximally-extended Schwarzschild spacetime $(M,g)$ . It is parallelizable as the underlying manifold is homeomorphic to $S^2 \times \mathbb R^2$ . Suppose there exists a metric-compatible curvature-free connection $\nabla$ . Since $M$ is simply-connected, the holonomy group $\operatorname{Hol}^{\nabla}(p)$ vanishes for any point $p \in M$ . We thus can construct a global orthonormal frame using the parallel transport of $\nabla$ . However, no such global orthonormal frame exists. A contradiction.","['riemannian-geometry', 'differential-geometry']"
3510619,Proof verification: If $A \in \mathcal{M}_n$ commutes with every $B$ then $A = \lambda I$,"I'm doing an exercise where I have a similar statement concerning linear transformations. More specifically: $\forall \sigma \in \mathcal{L} (V) (\tau \sigma = \sigma \tau) \implies \tau = a \iota$ , where $\iota$ is the identity operator. Since every linear transformation with respect to a basis is equivalent to a matrix, I thought I could instead prove the statement in the title, that is, a square matrix which commutes with every other square matrix of same size must be a scalar multiple of $I$ . I tried constructing matrices to assist me in the proof, so I wanted to be sure if the proof is actually valid. The proof starts below: Let $M_c (l, k) \in \mathcal{M}_n (F)$ , where $n \geq 2$ be a matrix where $m_{l, l} = c$ , $m_{l, k} = -c$ , and for any other index $m_{i, j} = 0$ . Also, $l \neq k$ and $c \neq 0$ . Summarizing, the $n$ th entry in the main diagonal has value $c$ , and another entry in the same row has value $-c$ , and all other entries have value $0$ . We intend to prove that any matrix $A \in \mathcal{M}_n$ which commutes with $M_a (l, k)$ for every $1 \leq l, k \leq n$ is a diagonal matrix. For the sake of readability, the matrix will be written only as $M$ . Since both commute, for any $l$ and $k$ , we have: \begin{equation}
			\begin{split}
				[B M]_{l, l} = \sum^n_{i = 1} b_{l, i} m_{i, l} = b_{l, l} m_{l, l} = c b_{l, l} \\
				= [M B]_{l, l} = \sum^n_{i = 1} m_{l, i} b_{i, l} = b_{l, l} m_{l, l} + b_{k, l} m_{l, k} = c b_{l, l} + (- c b_{k, l}) \; .
\end{split}
\end{equation} We have that $- c b_{k, l} = 0$ . Since $a \neq 0$ , the same holds for its additive inverse. This implies that $b_{k, l} = 0$ for arbitrary $k$ and $l$ where $k \neq l$ . We can choose every $k$ and $l$ between $1$ and $n$ and arrive at the same conclusion for these indexes, so we have that $b_{i, j} = 0$ for $i \neq j$ , therefore $B$ must be a diagonal matrix, with all entries outside the main diagonal equal to $0$ . Now, we pick some matrix $C \in \mathcal{M}_n$ where $c_{l, k} \neq 0$ for $l \neq k$ . Since $A$ commutes with $C$ : \begin{equation}
\begin{split}
		[A C]_{l, k} = \sum^n_{i = 1} a_{l, i} c_{i, k} = a_{l, l} c_{l, k} \\
		= [C A]_{k, j} = \sum^n_{i = 1} c_{l, i} a_{i, k} = a_{k, k} c_{l, k} \; .
\end{split}
\end{equation} Since $c_{l, k} \neq 0$ , this implies any two entries in the diagonal are equal, and since $l$ and $k$ are arbitrary, this applies for all entries. Since $A$ is also diagonal, we conclude that $A$ must be the multiple of an identity matrix. I just wanted to be sure that I didn't make any mistakes and that these conclusions are enough to prove the statement. Is the proof correct? Thanks in advance!","['matrices', 'solution-verification', 'linear-algebra']"
3510715,Bayesian hypothesis testing and posterior distribution,"Let $X$ be a random variable with a probability density $f(\cdot;\theta)$ where $\theta \in \Theta \subset \mathbb R$ is an unknown parameter. Suppose that we have a prior density $\pi(\theta)$ , with associated random variable for the parameter $\tilde\Theta$ . Suppose that we have a random sample $X_1, \dots, X_n$ from $X$ . The interest is in the hypothesis testing problem $$ H_0: \theta \in \Theta_0 \quad\text{vs.}\quad H_1: \theta \in \Theta_1,$$ where $\Theta_1 = \Theta \setminus \Theta_0$ . Now, according to my course notes, we should proceed as follows. We determine the ratio of the posterior probabilities $$ \frac{P(\tilde\Theta \in \Theta_0 \mid X_1,\dots,X_n)}{P(\tilde\Theta \in \Theta_1 \mid X_1,\dots,X_n)}
= \frac{P(\tilde\Theta \in \Theta_0) \int_{\Theta_0} \prod_{i=1}^n f(X_i;\theta) \pi(\theta) \,d\theta}{P(\tilde\Theta \in \Theta_1) \int_{\Theta_1} \prod_{i=1}^n f(X_i;\theta) \pi(\theta) \,d\theta}.$$ So we have $$ \frac{P(\tilde\Theta \in \Theta_0 \mid X_1,\dots,X_n)}{P(\tilde\Theta \in \Theta_1 \mid X_1,\dots,X_n)} = \frac{
\int_{\Theta_0} \prod_{i=1}^n f(X_i;\theta) \pi(\theta) \,d\theta}{ \int_{\Theta_1} \prod_{i=1}^n f(X_i;\theta) \pi(\theta) \,d\theta} \times \frac{\int_{\Theta_0} \pi(\theta)}{1 - \int_{\Theta_0} \pi(\theta)}.$$ However, we also have that the posterior density is given by $$f_{\tilde \Theta \mid X_1,\dots,X_n}(\theta\mid x_1,\dots,x_n) = \frac{\prod_{i=1}^n f(X_i;\theta) \pi(\theta)}{\int_{\Theta}\prod_{i=1}^n f(X_i;\theta) \pi(\theta)\,d\theta}.$$ So it seems to me that the factor $\frac{
\int_{\Theta_0} \prod_{i=1}^n f(X_i;\theta) \pi(\theta) \,d\theta}{ \int_{\Theta_1} \prod_{i=1}^n f(X_i;\theta) \pi(\theta) \,d\theta}$ is already the ratio of the posterior probabilities (since it is just the posterior distribution integrated over $\Theta_0$ and $\Theta_1$ ). Where is the mistake in my reasoning?","['statistical-inference', 'statistics', 'bayesian', 'hypothesis-testing']"
3510727,Strenghtening of Hoeffding inequality on finite range interval,"Let $X_1,\ldots,X_n$ be independent random variables with values in $[0,1]$ . Let $S_n=\sum_{k=1}^n X_k$ and $m=E(S_n)$ . Prove that for $t\in [m,n)$ , $$P(S_n\geq t)\leq \left(\frac mt\right)^{t} \left(\frac {n-m}{n-t}\right)^{n-t}$$ This is Exercise 2.4 in Devroye's Combinatorial Methods in Density Estimation . For $t=m$ the inequality is trivial: $P(S_n\geq m)\leq 1$ , while for $t=n$ the inequality still makes sense (since $\lim_{x\to 0} x\log x = 0$ ) and yields $\displaystyle P(S_n\geq n)\leq \exp\left(n\log\left(\frac mn \right) \right)$ . The usual Hoeffding bound yields $\displaystyle P(S_n\geq n)\leq\exp\left(-\frac{2(n-m)^2}n \right)$ . Since for $n$ large enough we have $n^2\log\left(\frac nm \right)\geq 2(n-m)^2$ , the proposed concentration bound is tighter than Hoeffding's. This is surely because $t$ is constrained to be $\leq n$ . I suspect that Chernoff bound should be used and somehow tailored to account for the additional constraint on $t$ .","['inequality', 'concentration-of-measure', 'probability-theory']"
3510852,Calculating the normal vector to a space curve to construct a 3D plot,"In trying to follow the calculation of the normal curvature of a curve $C$ at a point $P,$ using Geogebra and a somewhat random example, I got stuck. Here is what I did: The surface $S$ was set up as with domain boundaries for $-1<x<1$ and $-1<y<1$ as: $$f(x,y)=-x^2+\cos(x)+\cos(y)$$ The space curve $C\in \mathbb R^3$ was parametrized by $t$ with $-1<t<1$ as: $$C(t)=(t,t^2,f(x,y))$$ With $x=t$ and $y=t^2.$ The normal vector to the surface $\vec N$ was calculated as: $$\vec N(t)=\left (-\frac{\partial f}{\partial x}\frac{\partial x}{\partial t} ,-\frac{\partial f}{\partial y}\frac{\partial y}{\partial t},1\right)=\left(2t+\sin(t),\sin(t^2),1\right)$$ The tangent vector at point $P$ ( $\vec T\in T_PS)$ was calculated as $$\vec T(t) =\frac{d}{d t}C(t) =\left(1,2t,-2t-\sin(t)-2t\sin(t^2)\right)$$ This seemed to result in a plausible plot: However, the calculation of the normal vector to $C$ at $P$ was not so happy: $$\vec n=\frac{\vec T'}{\vert T\vert}=\frac{(0,2,-2\sin(t^2)-4t^2\cos(t^2)-2-\cos(t))}{\vert T\vert}$$ which would yield something like this (black arrow): clearly not orthogonal to $\vec T.$ Where did I go wrong in this calculation of the normal vector of $C$ at $P$ ? I would like to calculate this normal vector to the curve by differentiation; however, the only way I have been able to produce some plausible plot is by first calculating the binormal vector : $$\vec B=\frac{T\wedge T'}{|T\wedge T'|}$$ to later do the cross product of $\vec T$ with $\vec B:$","['multivariable-calculus', 'differential-geometry']"
3510853,Is each mixed partial derivatives at one point independent of the order of differentiation ？,"$\left(\textit{A.C.CLAIRAUT}\right)$ Suppose that $U$ is an open connected set in $\mathbf{R}^n,$ that $x_0\in U,$ and that $f：U\rightarrow \mathbf{R}.$ Its mixed second partial derivatives $f_{ji},f_{ij} (1\leq i<j \leq n) $ exist on $U$ . If $f_{ji},f_{ij}$ are continuous at $x=x_0$ , then $f_{ji}(x_0)=f_{ij}(x_0).$ By the Clairaut's theorem,the following proposition can be easily proved applying the induction method. $\textbf{Proposition}$ $U$ is an open connected set in $\mathbf{R}^n.$ If $f：U\rightarrow \mathbf{R}$ all of whose partial derivatives up to $k$ are $\underline{\text{ defined and continuous in } U }$$(i.e.f\in C^{k}(U))$ , then for any fixed $r (2\leq r\leq k),$ the value $\partial_{i_1\cdots i_r}f(x)$ of the partial derivative remains the same for any permutation of the  indices $i_1\cdots i_r (1\leq i_{1}，\cdots，i_{r}\leq n).$ $\textbf{My Question:}$ Now we consider an elementary question,slightly modify the proposition'condition：replacing by "" If $f：U\rightarrow \mathbf{R}$ all of whose partial derivatives up to order $k$ are $\underline{\text{defined in } U\text{ and continuous at } x_{0}\in U.}$ "" Whether we also get that the value of $\partial_{i_1\cdots i_r}f(x)$ at $x=x_{0}$ is independent of the order $i_1\cdots i_r  (1\leq i_{1}，\cdots，i_{r}\leq n)$ , for any fixed $r (2\leq r\leq k)$ ？When $k>2$ ,the conclusion will not holds (I think so).But I need some counterexamples to verify！","['multivariable-calculus', 'calculus', 'real-analysis']"
3510872,Calculating the Jacobian of a transformation,"I have the functions $y_i = \frac{S_i}{S_n}, \ i=1,...,n-1$ and $y_n=S_n$ , where $S_m = \sum_{k=1}^m x_k$ . Now I found the inverse functions to be \begin{align} 
x_1&=y_1y_n, \\
 x_2&=y_2y_n-y_1y_n, \\
 &\ \, \vdots 
\\ x_{n-1}&=y_{n-1}y_n-y_{n-2}y_n, \\ x_n&=y_n(1-y_{n-1}).\end{align} I need to get the Jacobian of these inverse functions but I cannot get the right answer, which my textbook says is supposed to be $y_n^{n-1}$ ?","['jacobian', 'multivariable-calculus']"
3510947,Question about domain of composition of functions?,"I am tutoring a student in precalculus and there was a question that I couldn't quite answer. I know that the domain of $f \circ g$ is the domain of $g$ , minus the points $z$ where $g(z)$ is not in the domain of $f$ . Now, in precalculus we often have functions such as $f(x) = \dfrac {1}{x+2}$ and $g(x) = 1/x$ . According to what I said above, the domain for $f \circ g$ is $\mathbb{R} - \{0, -\frac 12 \}$ . But if we just compute the formula for $f\circ g$ directly, we get $\dfrac {x}{2x+1}$ . Now, this function also gives us the restriction $x \not = -\frac 12$ . So it seems that, we still have to start with the domain of $g$ , but then instead of finding the points $z$ where $g(z)$ is not in the domain of $f$ , it seems like we can directly compute $f \circ g$ and compute the domain directly from there (after simplification). In fact, my tutee's professor even told the students to write this ""fact"" down. Is this really true, or can it fail? I really think it should fail in some cases, because I really don't see how you would ever prove it; ""finding the domain of $f \circ g$ after simplification"" is just a very ambiguous concept, so I can't even see where you would start to prove such a thing.","['elementary-set-theory', 'algebra-precalculus', 'functions']"
3510979,Are night and day lengths over a year equal everywhere on earth?,"As you all know, in winter the nights are longer and there is nearly no sun, but in the summer there are really long days. If you look at different places over the world, you find strange things like a 3 month day in summer with 3 months of darkness in the winter when you keep going north or south enough. The question I asked myself was: Question: Is the overall length of day equal to the overall length of night over the time span of one year (thus half of the year for both) everywhere on earth? I tried to solve this analytically, but looking at complex trigonometric function that include the earth rotating around its tilted axis around sun lead to nowhere. However, I noticed that when looking at mountains / high buildings, the overall day length becomes longer, thus this question is meant for a spherical earth with no altitude differences .","['calculus', 'trigonometry']"
3510982,Doubt in understanding Space $\mathscr D(\Omega)$,"I was reading about distributions from Rudin. I had 2 doubts in understanding space $\mathscr D(\Omega)$ . Here is the relevant section: 6.2 The space $\mathscr{D}(\Omega)$ Consider a nonempty open set $\Omega \subset R^{n}$ . For each compact $K \subset \Omega$ , the Fréchet space $\mathscr{D}_{K}$ was described in Section 1.46. The union of the spaces $\mathscr{D}_{K}$ , as $K$ ranges over all compact subsets of $\Omega$ , is the test function space $\mathscr{D}(\Omega)$ . It is clear that $\mathscr{D}(\Omega)$ is a vector space, with respect to the usual definitions of addition and scalar multiplication of complex functions. Explicitly, $\phi \in \mathscr{D}(\Omega)$ if and only if $\phi \in C^{\infty}(\Omega)$ and the support of $\phi$ is a compact subset of $\Omega$ .
Let us introduce the norms $$
\|\phi\|_{N}=\max \left\{\left|D^{\alpha} \phi(x)\right|: x \in \Omega,|\alpha| \leq N\right\}\tag1
$$ for $\phi \in \mathscr{D}(\Omega)$ and $N=0,1,2, \ldots$ ; see Section $1.46$ for the notations $D^{\alpha}$ and $|\alpha|$ . The restrictions of these norms to any fixed $\mathscr{D}_{K} \subset \mathscr{D}(\Omega)$ induce the same topology on $\mathscr{D}_{K}$ as do the seminorms $p_{N}$ of Section $1.46$ . To see this, note that to each $K$ corresponds an integer $N_{0}$ such that $K \subset K_{N}$ for all $N \geq N_{0}$ . For these $N,\|\phi\|_{N}=p_{N}(\phi)$ if $\phi \in \mathscr{D}_{K} .$ Since $$
\|\phi\|_{N} \leq\|\phi\|_{N+1} \quad \text { and } \quad p_{N}(\phi) \leq p_{N+1}(\phi)\tag2
$$ the topologies induced by either sequence of seminorms are unchanged if we let $N$ start at $N_{0}$ rather than at $1 .$ These two topologies of $\mathscr{D}_{K}$ coincide therefore; a local base is formed by the sets $$
V_{N}=\left\{\phi \in \mathscr{D}_{K}:\|\phi\|_{N}<\frac{1}{N}\right\} \quad(N=1,2,3, \ldots)\tag3
$$ The same norms (1) can be used to define a locally convex metrizable topology on $\mathscr{D}(\Omega)$ ; see Theorem $1.37$ and $(b)$ of Section $1.38$ . However, this topology has the disadvantage of not being complete. For example, take $n=1, \Omega=R$ , pick $\phi \in \mathscr{D}(R)$ with support in $[0,1], \phi>0$ in $(0,1)$ , and define $$
\psi_{m}(x)=\phi(x-1)+\frac{1}{2} \phi(x-2)+\cdots+\frac{1}{m} \phi(x-m)
$$ Then $\left\{\psi_{m}\right\}$ is a Cauchy sequence in the suggested topology of $\mathscr{D}(R)$ , but $\lim \psi_{m}$ does not have compact support, hence is not in $\mathscr{D}(R)$ . (Trascribed from screenshots 1 , 2 , 3 .) Doubts: Why are the topologies on $\mathscr D(\Omega)$ and $\mathscr D_k $ the same? Why is $\{\psi_m\}$ a Cauchy sequnce but its limit doesn't have compact support? I am studying functional analysis on my own with only the help of Math Stackexchange. Any help will be appreciated.","['proof-explanation', 'distribution-theory', 'real-analysis', 'functional-analysis', 'general-topology']"
3511027,How can I “tighten” the curvature of an exponential function to satisfy three given points?,"I'm trying to map a range: $$0 \le x \le 1$$ non-linearly to another range: $$0.01 \le y \le 4$$ where $x = 0.5$ must map to $y = 1$ .  Essentially, I have three known points: x  0.00  0.50  1.00
y  0.01  1.00  4.00 and $y$ cannot ever be $\lt 0.01$ (I mention this explicitly because I have also toyed with quadratics in an attempt to solve this problem). My distant high-school maths memories tell me that I need an exponential function to achieve this.  To that end, I followed the instructions in $\infty+1$ 's YouTube video, Write an Exponential Equation Given 3 Points , and worked from $y = ar^x + c$ to arrive at this very-close solution: $$ y = \frac{13167}{6700}\cdot\left(\frac{100}{33}\right)^{x}-\frac{131}{67} $$ This solution satisfies $(0,0.01)$ and $(1,4)$ , but not $(0.5,1)$ .  I guess I need to somehow “tighten” the curve such that the $y$ values for $0 \le x \le 0.5$ are lower without affecting point $(1,4)$ , but I'm at a loss as to how I might do this.  I guess I have chosen the wrong exponential equation to start from. Should I be starting from another exponential equation, and should I be able to solve this with my reasonably basic maths skills of algebra and simultaneous equations (and not enough understanding to know where to start with terms like regression , monotone , and differentiable as found in answers to similar questions)?",['functions']
3511048,Finding the area enclosed by the locus of centroids of equilateral triangles inscribed in an ellipse.,"Let a and b be the lengths of the semi-major and semi-minor axes of an ellipse, respectively. How to find the area enclosed by the locus of the centroids of equilateral triangles inscribed in the ellipse. Answer.
 How to answer  this question using calculus,trigonometry?","['self-learning', 'area', 'conic-sections', 'calculus', 'trigonometry']"
3511064,Stirling Numbers of the Second Kind Proof,"Prove that \begin{align*}
    \sum_{n=1}^\infty S(n,n-2)x^n=\dfrac{x^3(1+2x)}{(1-x)^5}
\end{align*} My guess is that I have to take the LHS and simply it, as well as take the RHS and simplify it, but not sure how to exactly do that. Any help, tips, or a fully worked out solution would be appreciated!","['combinatorics', 'stirling-numbers', 'discrete-mathematics']"
3511070,proposition logic problem,"below is proposition ""When A holds,  if B holds C holds, if B does not hold C also does not holds.  Regardless of B,C  if A does not hold, then D does not hold. "" Write the above sentence with a propositional formula. $A \implies (B \implies  C)$ $A \implies ( \sim B \implies  \sim C)$ $\sim A \implies  \sim D$ Suppose that the sentence with proposition above is holds.  At this time, determine the truth of ""If both C and D hold, then B holds."" is this mean i have to find : $(A \implies  (B \implies C) \land (A \implies  ( \sim B \implies  \sim C)) \land (\sim A \implies  \sim D)) \implies  ((C \land D) \implies  B)$ is tautology or not? I draw the table , but there is false so it means the proposition does not holds?","['propositional-calculus', 'solution-verification', 'logic', 'discrete-mathematics']"
3511075,Do arbitrary union and arbitrary intersection operations commute?,"Let $X$ be a set, and $\mathcal{S}$ a set of subsets of $X$ . Let $\sigma(\mathcal{S})$ be the set of all arbitrary unions of the elements of $\mathcal{S}$ , and $\tau(\sigma(\mathcal{S}))$ the set of all arbitrary intersections (empty intersection being $X$ ) of the elements of $\sigma(\mathcal{S})$ . Similarly, let $\tau(\mathcal{S})$ be the set of all arbitrary intersections of the elements of $\mathcal{S}$ , and $\sigma(\tau(\mathcal{S}))$ the set of all arbitrary unions of the elements of $\tau(\mathcal{S})$ . Then do we have $$\tau(\sigma(\mathcal{S}))=\sigma(\tau(\mathcal{S})) \ ?$$ A couple of observations: If $\sigma$ and $\tau$ are finite union and finite intersection operations, respectively, then the equality seems true. If only $\tau$ is finite intersection operation, then the equality fails.","['elementary-set-theory', 'general-topology']"
3511088,continuous function on $\ell^1$ without maximum on the closed unit ball,"Let $$ \ell^1(\mathbb{R}) = \{ x=(x_k)_{k=1}^\infty : x_k \in \mathbb{R}, ||x||=\sum_{k=1}^\infty |x_k| < \infty \}$$ and let $$B=\{x \in \ell^1: ||x||\leq1\}$$ be the unit ball in $\ell^1$ . Is there a continuous function on $\ell^1$ that doesn't take its maximum on $B$ ? I know that a continuous function on compact sets takes it maximum and minimum, but $B$ here isn't compact anymore since $\ell^1$ is infinite dimensional.","['hilbert-spaces', 'continuity', 'functions', 'functional-analysis']"
3511159,A question about tightness and almost sure convergence,"Let us consider $\{Z_n\}_{n\in\mathbb{N}}$ a sequence of independent Bernoulli random variables of parameter $1/n$ . So $P(Z_n=1)=1/n$ for all $n$ . Let $X_n=Z_1+\dots+ Z_n$ . Then $X_1\le X_2\le\dots$ is a nondecreasing sequence of random variables, so it admits an almost sure limit. I claim that this limit is $\infty$ . Indeed, we consider the independent events $A_n=\{Z_n=1\}$ . Since $$\sum_n P(A_n)=\sum_n 1/n=\infty,$$ by Borel Cantelli lemma, $P(\limsup A_n)=1$ , so for almost every $\omega\in\Omega$ (our possibility space), there exist infinite indices $n$ for which $Z_n(\omega)=1$ , and so $\lim_n X_n(\omega)=\infty$ almost surely. On the other hand, every $X_n$ is a nonnegative fandom variable with finite mean 1, so we can use Markov inequality to deduce that $\{X_n\}$ is a tight sequence: $$P(|X_n|>M)<1/M$$ for every $M$ . By Prokhorov theorem, we can find a subsequence converging in distribution to some real value, and this contradicts the almost sure convergence to $\infty$ . What am I missing?",['probability-theory']
3511175,Representation over a finite field,"Are there any two inequivalent and irreducible $F$ -representations of a finite group $G$ (where $F$ is a field of positive characteristic) having the same characters? I can surely find an example in which the representations are not both irreducible, but I thought it would be nice to find an example in which both are irreducible.","['representation-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
3511223,How do I rotate a square around a circle?,"I have a circle of radius r a square of length l The centre of the square is currently rotating around the circle in a path described by a circle of radius $(r + \frac{l}{2})$ However, the square overlaps the circle at e.g. 45°. I do not want the square to overlap with the circle at all, and still smoothly move around the circle. The square must not rotate, and should remain in contact with the circle i.e. the distance of the square from the circle should fluctuate as the square moves around the circle. Is there a formula or algorithm for the path of the square? [edit] Thanks for all your help! I've implemented the movement based on Yves solution with help from Izaak's source code : I drew a diagram to help me visualise the movement as well (the square moves along the red track):",['geometry']
3511324,Extraction of elements from a box,"I have a box with $n$ distinct elements and i need to do $n$ extractions with reposition. Let $N$ be the number of different elements that i found through the process I should try to find ${P}[N=k]$ for $k$ ranging between $1$ and $n$ . Here is my work. Let's take $\Omega$ ={ $(x_{1},...,x_{n})\in R^{n}; x_{i}\in
{1,...,n}$ }. I have that $\#\Omega$ = $n^{n}$ . 
Now I have problem calculating the cardinality of string with exactly $N$ elements. I need this as I would like to express my probability as a quotient of favorable cases over possible cases.
I guess that I should start with $\binom{n}{N}$ choosing the different collection of N elements that appear. Now i have to calculate how many different string of n elements I can get knowing that N and only N elements show up. I was trying with something such as counting surjective functions from $n\to N$ . To so i tried do choose N elements from n, permutating them (it should show all the possible function) and letting the n-N elements range randomly. The problem is that my reasoning is wrong as i am clearly counting multiple times the same string, any help? Thanks.","['combinatorics', 'probability']"
3511335,"Linear differential equations, integrating factor","Solve the following differential equation: $$ dr+(2r \cot\theta+\sin 2\theta)d\theta=0$$ I have tried like this: $$ \frac{dr}{d\theta}+2r\cot\theta=-\sin{2\theta}$$ \begin{align}
I.F. &=e^{\int{2\cot\theta d\theta}}\\
& =e^{-2\log\sin\theta}\\
& =\frac 1{\sin^2\theta}\\
\end{align} $$\therefore\frac r{\sin^2\theta}=-\int\frac{\sin 2\theta d\theta}{\sin^2\theta}\\
\implies \frac r{\sin^2\theta}=-2\int{\cot\theta d\theta}\\
\implies \frac r{\sin^2\theta}=2\log\sin\theta+c $$ But in my book the answer is: $$2r{\sin^2\theta}+{\sin^4\theta}=c$$ Please check out which is correct..","['calculus', 'ordinary-differential-equations']"
3511393,Pugh's definition of 'totally disconnected space',"In Pugh's text Real Mathematical Analysis page 105, he defines a 'totally disconnected space' as follows: A metric space $M$ is totally disconnected if each point $p ∈ M$ has arbitrarily small clopen neighborhoods. 
  That is, given $\epsilon > 0, p ∈ M$ , there exists a clopen set $U$ such that $p ∈ U ⊂ M_{\epsilon}(p)$ Edit: $M_{\epsilon}(p)$ means 'an open ball of radius $\epsilon$ around p' The usual definition of a totally disconnected space is one where the singletons are the only connected subspaces. I can see how Pugh's definition implies the usual one, but not the other way around. There may be a contrived example where in fact the clopen subsets are ugly enough that they allow for total disconnectedness yet there aren't any clopen sets contained in a ball of radius $\epsilon$ . I can't find a counterexample either since all the metric spaces I've worked with thus far are 'nice'. So my question is - with regards to metric spaces, are these definitions equivalent?","['general-topology', 'real-analysis']"
3511512,A continuous analogy of an application of the Markov inequality together with the Borel-Cantelli lemma that implies a.s. convergence,"It is well known, due to the Markov inequality and the Borel-Cantelli lemma, that for a sequence of random variables $(X_n)_{n\in \mathbb{N}}$ that converge to $0$ in probability, from $\sum E(|X_n|) < + \infty$ one can deduce that the convergence also happens almost surely. Question: I am wondering if the same analogy can be made with a stochastic process $(Y_t)_{t \in \mathbb R^+}$ (possibly with continuous paths) converging to $0$ in probability, for example under the assumptions: 1) $\sum E(|Y_{n}|) < +\infty  $ , and 2) $\mathbb{R}^+\ni t \mapsto E(|Y_t|) $ is decreasing. If not, is there any other assumption under which we could draw such a conclusion (i.e. that $Y_t \to 0$ a.s.)? In the related question Is there a continuous version of the Borel-Cantelli lemma? the answer provided is not sufficient to conclude an a.s. convergence.","['stochastic-processes', 'probability-theory', 'probability', 'real-analysis']"
3511541,What is the solution to the following Lagrange system?,"I have to maximize the function $$u(x_1, x_2) = x_1^{1/2} + x_2^{1/2}$$ in the set $$\{(x_1,x_2) \in \mathbb R^2 \mid x_1,x_2\geq0, (x_1p_1+x_2p_2) \leq N\},$$ where $p_1, p_2>0$ and $N>0$ . The Lagrange system is : $1/2x_1^{-1/2}+\lambda_1-\lambda_3p_1 =0$ $1/2x_2^{-1/2}+\lambda_2-\lambda_3p_1 =0$ $\lambda_1(-x_1)=0$ $\lambda_2(-x_2)=0$ $\lambda_3(x_1p_1+x_2p_2-N)=0$ $-x_1\leq 0$ $-x_2\leq 0$ $x_1p_1+x_2p_2 \leq N$ My professor claims that the only solution of the system is: $x_1=\frac{p_2}{p_1^2+p_1p_2}N$ and $x_2=\frac{p_1}{p_1+p_2}N$ . But I can not get that solution fro  the system. Can you help me please?","['multivariable-calculus', 'calculus', 'maxima-minima', 'optimization']"
3511566,Total order on irrational numbers,"I am trying to prove that the set of irrational numbers $\mathbb{I}$ is totally ordered. I almost completed the proof but stuck at the very end... Some theory. The set of irrational numbers $\mathbb{I}$ consist of all possible $A)(A'$ cuts that can be constructed on rational numbers $\mathbb{Q}$ . The $A)(A'$ means there is no biggest element in $A$ class and no lowest element in $A'$ class. Now we define the equal and bigger relations between two arbitrary irrational numbers $\alpha = A)(A'$ and $\beta = B)(B'$ . $$ \alpha = \beta :\Leftrightarrow (A=B) \land (A' = B') \qquad \quad \alpha > \beta :\Leftrightarrow B\subset A $$ So now I need to prove that the set of irrational numbers is totally ordered: $$ \boxed{\forall \alpha, \beta \in \mathbb{I} : \ (\alpha = \beta) \lor \Big( (\alpha > \beta) \dot{\lor} (\beta > \alpha)\Big)} $$","['proof-explanation', 'irrational-numbers', 'order-theory', 'solution-verification', 'elementary-set-theory']"
3511647,A geometric interpretation (angles) of mathematical expectation and covariance.,"I have a habit of understanding everything geometrically. In this case, the victim is probability theory. I will assume that we are working on $L^2 (\mathcal{F})$ , the space of integrable  random variables $X^2$ , a Hilbert Space. Here we have a well defined inner product given by: $$<X,Y> = E[XY]$$ Quickly, the inner product is directly related to angles. To be more specific, we have the following formula: $$\cos (angle(X,Y)) = \frac{<X,Y>}{|X||Y|}$$ Decreasing the length of the vectors does not change the angle. So I will assume for now that I am working with unit vectors, so that: $$angle(X,Y) = \cos^{-1} (<X,Y>)$$ Thus we can assume that the internal product measures angles. All right, given all of this, I have some questions I can interpret $E(X)= <X,1>$ as the angle between $X$ and $1$ , a constant random variable. But I have a hard time relating this geometric interpretation (using angles) to the statistical intuition we learned in school. The case of covariance is similar. We know that the covariance between X and Y tells us about or indicates the relationship of two variables whenever one variable changes. However, mathematically, $$Cov (X, Y) = Cov (X - \mu_X ,Y) = E[(X- \mu_X)Y] = Cov(X - \mu_X,Y) = angle(\bar{X},Y)$$ Where $\bar{X} = X - \mu_X$ . How can I link the $angle(\bar{X},Y)$ to the classical interpretation of the covariance? Something more dramatic is the case of variance. we know that variance measures the dispersion of possible outcomes of the random variable $X$ . Assuming that $\mu_X = 0$ , we have: $$Var(X) = Cov(X,X) = E[X^2] = <X,X> = |X|^2$$ That is, the notion of dispersion is linked to the notion of norm. Any light on 
  this?","['probability-theory', 'geometric-probability']"
3511707,Tensor calculus - curve definition doesn't quite make sense,"This is probably a rather stupid question, and I'm probably just looking at this wrong, but I can't get this definition to make sense. I'm reading Tensor Calculus by Synge and Schild, and it notes A curve is defined as the totality of points given by the equations $$ x^r = f^r(u)    (r = 1,2,..., N)$$ Here $u$ is a parameter and $f^r$ are $N$ functions. Earlier it introduced superscripts (as opposed to subscripts) as a way to notate numerical labels, so this a series of $N$ equations. I just don't understand how this produces a ""curve"".","['curves', 'functions', 'geometry', 'tensors']"
3511736,On free abelian groups,"I'm learning about the concept of a free abelian group. First question: nowhere it is stated that these groups cannot be finite, but the definition seems to imply it. Is this true? Second question: an isomorphism to a free abelian group $A = \left< S \right> = \left< \{s_i\}_{i=1}^n \right>$ is given as \begin{eqnarray*}
&ℤ^n &\tilde\longrightarrow A \\
&(c_i)_{i=1}^n &\mapsto \sum_{s ∈ S} c_ss
\end{eqnarray*} Then a claim is made that I don't understand: ""The induced isomorphism $A/2A \cong (ℤ/2ℤ)^n$ now shows that $A/2A$ is of order $2^n$ , from which we conclude that the rank of $A$ does not depend on the choice of a basis for $A$ "" What is this induced isomorphism? Is it induced by the (first) Isomorphism Theorem? What exactly is $A/2A$ , and why are we looking at it? How is the final conclusion drawn?","['free-abelian-group', 'combinatorial-group-theory', 'free-groups', 'group-theory', 'abelian-groups']"
3511771,Covering of $B^{n}$,"Suppose we have $B^{n} = \{0,1\}^{n}$ .We want to find for what $n$ there is exist $B_{1}(q_{1}) \dots B_{1}(q_m)$ : $B_{1}(q_i) \cap B_1(q_j) = \emptyset$ and $\bigcup B(q_i) = B^n$ , where $B_{1}(q) - $ is sphere with unit radius. I think the answer is $2^k$ and $2^{k} - 1$ . But does there any other solutions? My idea based on Hammings codes and $B^{n} = B^{n-1} \times B$ .","['boolean-algebra', 'coding-theory', 'discrete-mathematics']"
3511773,Maximum number of students such that they all have at least 3 out of 6 different answers.,"A teacher made a test for his mathematics class with 6 true or false questions. When he received the tests, he noticed that any pair of students had at least three diferent answers. Since all the students answered every question, what's the maximum number of students of the class? Here is my take. $2^6=64$ diferent tests. For one of those tests, there are $^6C_3+^6C_4+^6C_5+^6C_6$ =42 tests which have 3 or more diferent questions from it. $42/63 = 2/3$ , and if the ratio stands for successive pairs of tests, ${3/2}^{10}<63<{3/2}^{11}$ then 10 students. This argument is clearly fallacious and unfounded, but I found no better.","['graph-theory', 'coding-theory', 'combinatorics']"
3511809,is it possible to divide $9\times 11$ rectangle into one $1\times 3$ rectangle and N tetrominoes?,"There is a rectangle of size $9\times 11$ . Is it possible to divide it using 1 tromino and $N$ tetrominoes? I have tried a lot and it seems that this is not possible, thus I want to prove it is not possible. Such a simple method as counting the number of squares, count
add/even contradictions - seems to not work. Coloring method. Maybe this works, but then we have not found the
right coloring. What I mean - I read that sometimes such problems can be proved by coloring rectangle squares using some specific pattern and this could give some contradiction usually with something odd/even. We tried a lot of coloring patterns (the simplest one was coloring rectangle squares black and white as chessboard, also coloring first line black, second one line white etc., also tried other coloring patterns). We have not found any contradictions that help us to prove that this is not possible. It would be nice if someone suggests any ideas, how to proceed with such type of problems. P.S. I tried this to solve together with my 5th grader and now I am so much into it that I really want to solve it","['contest-math', 'combinatorics', 'tiling', 'discrete-mathematics']"
3511828,Probability you run out of white balls first?,"I saw this question online (interview brainteaser prep), and can't get my head around it. You have $r$ red balls, $w$ white balls in a bag. If you keep drawing
  balls out of the bag until the bag only contains balls of a single
  color (i.e you run out of a color) what is the probability you run out
  of white balls first? (in terms of $r$ and $w$ ). Supposedly the answer is just $r/(w+r)$ , but I have no idea how this is arrived at. Any guidance? Thanks",['probability']
3511847,"Can a function $f:\mathbb{R} \rightarrow \mathbb{R}$ be ""infinitely steep"" on a set with non-zero Lebesgue outer measure?","By being ""infinitely steep"" on a set I mean that for each point $x$ in the set we have $\sup\limits_{\delta>0}\text{ }\inf\limits_{y\in(x-\delta,\text{ }x+\delta)\backslash\{x\}}\frac{f(y)-f(x)}{y-x}=\infty$ . As a remark, the Cantor function is infinitely steep on the Cantor-Set which is uncountable, but of course has Lebesgue measure $0$ .",['real-analysis']
3511879,Is it possible to construct an $L^1$ dominating random variable for a non-negative uniformly integrable martingale?,"Consider a non-negative martingale $(M_n)_{n \in \mathbb{N}}$ that is also uniformly integrable. Does there exist $Y \in L^1$ such that $$|M_n| \leq Y \ \ \text{for all } n \in \mathbb{N}?$$ I haven't been able to make much progress on this problem for the past few days, besides the fact that we can note the existence of an $L^1$ limit $M_{\infty}$ for the martingale (from the uniform integrability). I tried to construct a dominating sequence $Y_n$ based on a running maximum of $M_n$ and acquire a $Y$ via a convergence (to the martingale limit, so our candidate $Y = M_{\infty}$ ) in the spirit of the generalized DCT but it didn't seem to work. Prima facie, it looks like a partial converse to what David Williams, in Probability with Martingales , calls Hunt's Lemma: Suppose that $(X_n)$ is a sequence of random variables such that $X := \lim_n X_n$ exists, almost surely and that $(X_n)$ is dominated by non-negative $Y \in L^1$ . For any filtration $\{\mathcal{F}_n\}$ , we can show that $E(X_n| \mathcal{F}_n) \to E(X|\mathcal{F}_{\infty})$ . Note: NOT a homework problem.","['measure-theory', 'stochastic-processes', 'martingales', 'probability-theory', 'probability']"
3511897,How to solve $\sin 2x \sin x+(\cos x)^2 = \sin 5x \sin 4x+(\cos 4x)^2$?,"How to solve $\sin 2x \sin x+(\cos x)^2 = \sin 5x \sin 4x+(\cos 4x)^2$ ? \begin{align*}
2\cos x(\sin x)^2+(\cos x)^2 & = \frac{1}{2}(\cos x- \cos 9x) +(\cos 4x)^2\\
4\cos x(1-(\cos x)^2)+2(\cos x)^2 & = \cos x + \cos 9x +(2(\cos 4x)^2-1)+1\\
\cos x(4(1-(\cos x)^2 + 2\cos x -1) & = \cos 9x + \cos 8x + 1
\end{align*} I don't see any way to get rid of $8x$ and $9x$ as arguments to have same angles from there",['trigonometry']
3511898,Decomposition of Unmeasurable Sets into Measurable and Purely Unmeasurable Sets,"Below is a proof I wrote up that unmeasurable sets can be written as a union of a measurable set and a ""purely unmeasurable"" set. Is this result well-known (it is not very advanced, obviously), and does it hold more generally than given here? Let $\mu^*$ be an outer measure on a set $X$ .
If $\mathcal{M} \subset \mathcal{P}(X)$ is the set of $\mu^*$ -measurable subsets of $X,$ then $(X,\mathcal{M},\mu)$ is a measure space, where $\mu = \mu^*|_{\mathcal{M}}$ .
We say $\mu^*$ is $\sigma$ -finite if $(X,\mathcal{M}, \mu)$ is a $\sigma$ -finite
measure space,
i.e. $X = \cup_{j=1}^\infty X_j,$ where $\forall j \in \mathbb{Z}_{>0}$ , $X_j \in \mathcal{M}$ and $\mu^*(X_j) < \infty.$ Definition 1.  A subset $Y \subset X$ is called purely unmeasurable
if $Y$ is not measurable, and
for every $Z \subset Y,$ \begin{equation*}
Z\ \text{measurable} \ \implies \  \mu^*(Z) = 0.
\end{equation*} Proposition 1. Let $\mu^*$ be a $\sigma$ -finite outer measure on $X$ .
If $Y \subset X$ is not measurable, then $Y$ can be written $Y = A \cup B,$ where $A$ is purely unmeasurable, $B$ is measurable,
and $A \cap B = \emptyset.$ \
\textit{Proof.} First, suppose $\mu^*$ is actually finite. We give an algorithm for forming $A$ and $B.$ This algorithm can, in principle, be applied whether or not $Y$ is not measurable.
Initialize $A= Y,$ $B = \emptyset.$ For $\epsilon>0,$ define $$\mathcal{A}_\epsilon = \mathcal{M} \cap \mathcal{P}(A) \cap (\mu^*)^{-1}([\epsilon, \infty]) = \mu^{-1}([\epsilon, \infty]) \cap \mathcal{P}(A).$$ Then the algorithm proceeds as follows. For $n = 1, 2, 3, ...$ ,
until $\mathcal{A}_{1/n} = \emptyset,$ choose $E \in \mathcal{A}_{1/n}$ and re-define $A := A \setminus E,$ $B = B \cup E.$ The $n$ th such ``until"" loop lasts at-most $\lfloor{n\mu^*(X)}\rfloor$ steps, since $\mu^*(B)$ increases by
at least $1/n$ with each iteration; the algorithm therefore calls for at most countably-many iterations.
Any $B$ which results from this process is a countable union of measurable sets, and is hence measurable. Since $A = Y \setminus B,$ and $B$ is measurable, if $Y$ is not measurable, then $A$ is not measurable.
Let $E$ be a measurable subset of $A$ (at the end of the algorithm). 
If $\mu^*(E)>0,$ then there is an $N \in \mathbb{Z}_{>0}$ so large that $\mu^*(E) > 1/N.$ But then, at the start of the $N$ th iteration of the ``for"" loop, we had $E \in \mathcal{A}_{1/N}.$ The algorithm could not proceed to the $(N+1)$ th iteration, and thus could not have been completed,
without the removal of all or part of $E$ from $A.$ In view of this contradiction,
we see that there are two possibilities: if $Y$ was measurable to begin with, then $\mu^*(A) = 0.$ If $Y$ is unmeasurable, then $A$ is \emph{purely} unmeasurable. Now assume that $\mu^*$ is $\sigma$ -finite,
and write $X = \cup_{j=1}^\infty X_j,$ where $\mu^*(X_j) < \infty$ , and for $i \neq j, X_i \cap X_j = \emptyset.$ For $n = 1,2,3,...$ , let $X_n \cap Y = A_n \cup B_n$ be a decomposition given by the above algorithm. Note that,
even though $Y$ is not measurable, it may happen that $X_n \cap Y$ is measurable.
This is harmless: in the case that $X_n \cap Y$ is measurable, $A_n$ is a measurable set with $\mu^*(A_n) = 0.$ We define $B = \cup_{n=1}^\infty B_n$ and $A = Y \setminus B.$ Since $B$ is a countable union of measurable sets, it's measurable;
since $Y$ is not measurable and $A = Y \setminus B,$ $A$ is also not measurable.
Let $E$ be a measurable subset of $A.$ Since $E \cap X_n$ is a measurable subset of $A_n,$ $\mu^*(E \cap X_n) = 0.$ Thus by subadditivity, $\mu^*(E) \leq \sum_{n=1}^\infty \mu^*(E \cap X_n) = 0.$ Hence $A$ is purely unmeasurable. Edit:
Does anyone know whether this result holds for outer measures which are not sigma-finite (in the sense that the induced measure is not sigma finite)?","['measure-theory', 'real-analysis']"
3511906,"$G$ abelian $\{ a^ib^j : a^m = b^n = 1, a^2=b^3 \}$, prove that $G=\langle c:c^k=1 \rangle$","Let G be an abelian group $\{ a^i b^j : a^m = b^n = 1, a^2=b^3 \}$ . Prove that $G=\langle c:c^k=1 \rangle$ with $k=\gcd(3m,2n)$ . Hint: consider the element $ab^{-1}$ . This should be easy but I'm having a hard time with it. I think even with the hint that $ab^{-1}$ is supposed to be the generator $c$ . So that point it to prove that all the powers of $c$ represent every element in the group. My first idea was to notice that $a^{2m}b^{3n}=(ab)^h=1$ where $h=lcm(2m,3n)$ . So $(ab)^h$ could be a generator, but I am not sure if all the elements of $G$ are represented, and it seems it does not use the hint. I could use some help on this. Thanks. OK so with the (other) hint, $(ab^{-1})^2=b$ and $(ab^{-1})^3=a$ , so $(ab^{-1})^{2n}=b^n=1$ and $(ab^{-1})^{3m}=a^m=1$ , that is how we get the $3m$ and $2n$ . $k=\gcd(3m, 2n)$ , and I find then there exists 2 integers, $h$ and $g$ such that $kh=3m \implies k=\frac{3m}{h}$ and $kg=2n \implies k=\frac{2n}{g}$ . However still stuck here.","['combinatorial-group-theory', 'group-theory', 'cyclic-groups', 'abelian-groups']"
3511964,Lax-Milgram theorem proof in Brezis book.,"I am reading Brezis book ""FA, Sobolev Sp. and PDEs"" and I am working through the proof of Stampacchia theorem (5.6 page 138 2010 edition) and I am particulary interested in Lax-Milgram theorem (which is given as a Corollary 5.8) . Lax-Milgram theorem is proven as a corollary of Stampacchia theorem invoking corollary 5.4 , which states Suppose $W$ is a closed linear subspace of $H.$ For $x\in H,$ $y=P_Kx$ is characterized by the property that for all $\omega \in W$ $$ y\in W
\ \text{and} \ \langle x-y, \omega \rangle =0.$$ How does one prove Lax Milgram from this? My explanation What I would say is that taking $H=K$ we argue as in Stampacchia theorem (keeping in mind that for $H$ the above argument applies with $H=W$ )
  to obtain the unique $u \in H$ such that for all $v \in H$ $$a(u,v-u+u)=\ell (v-u+u),$$ where we have an equality ( $= 0$ ) instead of an inequality ( $\leq 0$ ) precisely because we have so in the corollary 5.4, and the same goes about the $+u$ factor. And then when $a$ is symmetric the Stampacchia's argument completely carries over without any change so that the minimizing function is the same but we minimize it over $H$ instead of $K.$ Is it correct? I would like to make it more precise, any suggestions? Note I found this recent question which asks the same thing but has received answers that are not very to the point, since they just give standard proofs of Lax-Milgram without any reference to Stampacchia's theorem, which is not what the other (and my own) question was about at all. So I ask hoping to receive a more on-topic answer.","['real-analysis', 'hilbert-spaces', 'solution-verification', 'functional-analysis', 'partial-differential-equations']"
3511995,What is the notation $f'(x^+) $ and $f'(x^-)$?,Differentiability of a function at a point $x$ is confirmed when $\lim_{h\to0} \frac{f(x+h)-f(x)}h$ exists and is finite. But in some textbooks it is noted that a function is differentiable if $f'(x^+)$ and $f'(x^-)$ are equal and finite. I'm confused how this notation links with the above method of derivative by first principle.,"['calculus', 'functions', 'slope', 'limits', 'derivatives']"
3512025,"Soft question: Preference between ""Isotropy subgroup"" and ""Stabiliser subgroup""?","Given an action of a group $G$ on a set $X$ , one can define the stabiliser subgroup of an element $x\in X$ as $$
S_G(x):=\{g\in G:gx=x\}.
$$ The stabiliser subgroup is also referred to as the isotropy subgroup in many textbooks and papers. To me, the term `stabiliser' makes more sense. I was curious as to whether one of the terms has a higher preference in certain literature as compared to the other.","['group-theory', 'group-actions', 'soft-question', 'lie-groups', 'terminology']"
3512097,"Number of strings of $\{0,1,2\}$ : the longest substrings of $1$ is odd-length.","Consider $A^* = \{0,1,2\}^*$ . We want to know how many string of length $n$ in this alphabet following such property : all longest substrings of 1-s has odd length (let it be $a_n$ ). I know that it can be founded by regular-sequence, but I don't understand how to generate it. Essential edits: We don't consider zero numbers of $1s$ . Longest means maximal, i.e. $111011$ , $1110111$ are valid, $11101111$ invalid. So I've tried to consider some extra cases : $a_1 = 1,a_2 = 4(10,12,01,21),a_3 = 15,a_4 = 48$ . The OEIS gives me that it should be $a_n = n Pell(n)$ . Maybe it's possible to show it somehow?","['combinatorics', 'discrete-mathematics', 'oeis']"
3512161,Find summation definition of a certain function,"Consider a tuple $T$ to be an object (list of numbers) similar to a row vector of $n$ elements which are real numbers: $$T=(x_{1}, x_{2},\ldots, x_{n}); n\in \Bbb N, x_{i}\in \Bbb R$$ A function $F$ can be defined from the tuples $T$ to $\Bbb R$ which takes the alternating sum of all possible products of $2$ elements. For example, for $n=3$ : $$F(T) = x_{1}x_{2}-x_{2}x_{3}+x_{3}x_{1}$$ and for $n=4$ $$F(T)=x_{1}x_{2}-x_{2}x_{3}+x_{3}x_{4}-x_{1}x_{3}+x_{2}x_{4}-x_{1}x_{4}$$ It is pretty obvious that the number of terms is $n \choose 2$ and if $n \choose 2$ is even, there are as much positive terms as there are negatives.
I came up with a faulty general definition that doesn't seem to get the signs properly: $$\sum^{n-1}_{k=1}\sum^{n-k}_{j=1}\left[(-1)^{j+1}*x_{k}*x_{k+j}\right]$$ This doesn't work because the signs on some of the elements are flipped. You are supposed to place the signs in that manner: you first take all products of neighboring elements and multiply by $-1$ each time, then the elements distance $1$ apart , then distance $2$ apart and so on. I am asking to correct me.","['summation', 'functions']"
3512177,Why is $(1+\frac{1}{n})^n < (1+\frac{1}{m})^{m+1}$?,"Why is it that $$ 
\left(1+\frac{1}{n}\right)^n < \left(1+\frac{1}{m}\right)^{m+1},
$$ for any natural numbers $m, n$ ? I have tried expanding using the binomial series and splitting into cases. I understand why it is trivially true when $m=n$ but I am not sure if there is a rigorous proof for other cases?","['inequality', 'real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence']"
3512209,Cartesian coordinates and Linear Transformation,Equation 3.1(a) is a linear transformation but what is the meaning of 3.1(b) and 3.1(c)? Why should it satisfy these conditions?,"['matrices', 'coordinate-systems', 'linear-transformations']"
3512240,Records and covers,"Juan is challenging his friend Thomas with the following: Juan has 5 LP vinyl records, each of them of a different band, and asks Thomas to match them with its respective band name, by showing him only the cover photo, not the names etc. Thomas will make a first guess and Juan will reveal him the number of correct matchings but without telling him which ones are correct. If not all his matchings are correct, he has the right to do a second guess, by changing as many as he wants. If he correctly guesses all 5 in these two attempts, then Juan will buy him a nice dinner. What is the probability for Thomas to win the dinner, in each of the below 3 cases: 1) He can recognize the 3 covers but not the other 2. 2) He can recognize the 2 covers but not the other 3. 3) He knows the matching of 2 covers with 2 of the bands but not which is which. Same also for 2 more covers. For example, he knows that cover1 and cover2 correspond to Band1 and Band2 but not which is which, and also he knows that cover3 and cover4 correspond to Band3 and Band4 but not which is which. I can easily tell that the requested probability for 1) is 100%: If he knows 123 and he guesses 45, Juan will reply him “3” as the correct number of matchings. Since Thomas knows 123 are correct, he will say 12354 and this will be correct. For number 3): Thomas must guess 12 or 21, 34 or 43 and he can deduce 5. 
We have 4 cases but with the 2nd attempt, in essence we have 6: Suppose the correct choices are 21345: If he says 21345, Juan replies “5” and we are done. If he says 12435, Juan replies “1” and we are also done because Thomas knows it is 21345. If he says 12345, Juan replies “3”. Thomas doesn’t know which are the correct two (he only knows 5). Therefore in the second attempt, he can either say 21345, (correct) or 12435 which is wrong. Same also for 21435: In the second attempt he can either say 21345 (correct) or 12435 (wrong). So the overall probability is 4/6? Is this correct? Can you also help me out with the 2nd case? I started examining the cases but it’s very confusing! Many thanks!",['probability']
3512315,Smoothness of fibers vs smoothness of total space,"Let $f:X \to Y= \mathbb P^n$ be a flat morphism. We define condition $S_k$ for such morphisms whose total space over every $k$ -dimensional linear space is smooth, namely: $$f\in S_k \text{ if for every linear space $\mathbb P^k \subset \mathbb P^n$, $f^{-1}(\mathbb P^k)$ is smooth.}$$ I want to know if the following is true: $S_i \subsetneqq S_j$ if $i<j$ . The inclusion $S_0\subset S_n$ is the well-known fact, so this can be seen as an enhancement of it. The strict inclusion is related to one of my previous question . Thanks in advance.","['complex-geometry', 'algebraic-geometry', 'intersection-theory']"
3512341,"Why is it that given $E=\{ 1, \{2,3\}, \{2,4\} \}$ we have $1\in E$ but $2\not\in E$?","I am reading ""Book of proof"" of Richard Hammard and it says that given the set $E=\{ 1, \{2,3\}, \{2,4\} \}$ , we have that $2\not\in E$ . I do not understand why is it so. I understand that the set $E$ is a collection of the elements "" $1$ "", "" $\{2,3\}$ "" and "" $\{2,4\}$ "". But I do not understand why I could not go further and deduce that since $2\in\{2,3\}$ and $\{2,3\}$ is an element of $E$ then $2$ must be an element of $E$ . Sorry if this is a very basic question but I do not even seem to know how to ask for this clarification.",['elementary-set-theory']
3512389,Finding the tangent planes to a torus parallel to $3x+4y-5z=20$.,"I've been tasked with finding all the points on the torus $S$ given by $$
\left(6-\sqrt{x^2+y^2}\right)^2+z^2=2
$$ at which the tangent plane is parallel to the plane $P$ given by $3x+4y-5z=20$ (which is, itself, tangential to $S$ at $(3,4,1)$ ). The method that I have employed gives me another point, but since $S$ is a torus, and $P$ is not parallel to any of $x=0$ , $y=0$ or $z=0$ , I know there must be at least two more. The method that I have employed is as follows. Let $f(x,y,z)=\left(6-\sqrt{x^2+y^2}\right)^2+z^2$ , then we know that a vector normal to $S$ at $(x_0,y_0,z_0)$ is given by $\nabla f(x_0,y_0,z_0)$ . We use this fact, in conjunction with the fact that for two planes to be parallel, their normal vectors must also be parallel, to arrive at the condition $$
\nabla f(x_0,y_0,z_0)=\lambda\begin{pmatrix}3\\4\\-5\end{pmatrix}
$$ for some scalar, $\lambda\in\mathbb{R}$ . We compare the components of these vectors to obtain expressions for $z$ and $x$ in terms of $y$ , and then substitute these expressions into the equation for $S$ to find that $y=4$ or $y=28/5$ . Since we already have a point at which $y=4$ , we discard this value, and hence we have that another tangent to $S$ which is parallel to $P$ is given by $5x/5+8y/5-2z=16$ . Graphically, these are the two tangent planes in question However, as we can see, there is an entire other half of the torus, one which I'm confident has two more points at which the tangent planes are parallel to $P$ . How come I have missed these points? Any help is appreciated.","['multivariable-calculus', 'calculus']"
3512432,What's the matter of Cantor's diagonal argument in binary system？,"In binary system, Cantor's diagonal argument lost its effect.( https://zhuanlan.zhihu.com/p/20197130)(Sorry for the Chinese needed to translating)
What can we say about it? How can we improve it？ In binary system,if we construct like this:
r = 0.d1d2d3…
di = 1 - dii Then we fail. For an example:
r1 = 0.1
r2= 0. 001
r3 = 0.0001
r4 = 0.00001
… The thing we diagonally inverted constructed is:
r = 0.0111… But in this way,it just is r1. And it seems other ways of diagonal construction fails too. (Sorry for my be utterly ignorant of things like Latex)","['elementary-set-theory', 'binary', 'logic']"
3512521,Proving a symmetric Cauchy matrix is positive semidefinite,"Let $x_1, x_2,\dots, x_n$ be positive real numbers. Let $A$ be the $n\times n$ matrix whose $i,j^\text{th}$ entry is $$a_{ij}=\frac{1}{x_i+x_j}.$$ This is a Cauchy matrix . I am trying to show that this matrix is positive semi-definite. I have been given the following hint: Consider the matrix $T=(t^{x_i+x_j})$ where $t>0$ . Use the fact that $T$ is positive semi-definite and that $$\frac{1}{x_i+x_j}=\int_0^1t^{x_i+x_j-1}dt.$$ I have managed to show that $T$ is positive semi-definite but I don't understand where to go from there or how to use the rest of the hint. I would like another way to do this, preferably without involving integrals Thank you.","['matrices', 'cauchy-matrices', 'linear-algebra', 'positive-semidefinite']"
3512583,Does the Dominated Convergence Theorem Hold Almost Surely.,"This question is about the dominated convergence theorem for a stochastic process. Let $X_t$ be a sequence of stochastic process on some probability space $(\Omega,\mathcal{F}_t,\mathbb{P})$ . For each $\omega$ assume $X_\cdot(\omega)$ is continuous on $[0,T]$ (i.e the realisations of the stochastic process are continuous paths). Say $X_t$ take values in $\mathbb{R}^d$ . Let $f^n$ be a sequence of functions $f^n:\mathbb{R}^d \to \mathbb{R}$ converging point wise to some function $f$ . Consider an integral $$\lim_{n\to \infty}\int_0^T f^n(X_s)\,ds $$ Assume that for almost every $\omega$ there exists a function $g^\omega$ such that $f^n(X_s(\omega))\leq g^\omega(s)$ , where $g^\omega$ is integrable, i.e $\int_0^T g^\omega(s)ds<\infty .$ Can one apply the dominated convergence theorem to say $$\lim_{n\to \infty}\int_0^T f^n(X_s)\,ds=\int_0^T\lim_{n\to \infty} f^n(X_s)\,ds \text{?} ~~~\text{almost surely}$$ What would be the case if there was a $g$ (independent of $\omega$ ) which did the dominating job for a.e $\omega$ .","['measure-theory', 'lebesgue-measure', 'analysis', 'stochastic-processes', 'almost-everywhere']"
3512723,Proof that $a^2 + b^2 + ab$ and $a^2 + b^2 - ab$ can not both be perfect squares,"If $a$ , $b$ are co-prime integers, how can I prove that $a^2 + b^2 + ab$ and $a^2 + b^2 - ab$ cannot be perfect squares? I know that perfect squares should be capable of expression in the form $a^2 + b^2 + 2ab$ and $a^2 + b^2 - 2ab$ , but it is not immediately clear to me that there might not be other integers $x$ , $y$ that would work even if $a$ , $b$ do not. Thanks if anybody can help. I should have said that $a$ , $b$ are distinct integers and are both positive so that they could not both be equal to 1. Please note that the premise of this question has now been disproved with a straightforward counter example, which is all the answer I require. Thanks to everyone who has taken the trouble to contribute. I am grateful for the help but won't be continuing to monitor further responses.","['number-theory', 'square-numbers']"
3512728,Sum of endomorphisms that is not an endomorphism,"Consider a group $(G,+)$ and the structure $\operatorname{End}(G)$ of all endomorphisms of $G$ . We know that if $G$ is abelian, then $\operatorname{End}(G)$ forms (typically non-commutative) unitary ring together with pointwise addition and function composition. The reason we want $G$ to be commutative is that sum of endomorphisms may fail to be an endomorphism. (here sum means the $+$ group operation of $G$ ). Here is the proof that sum of two endomorphism under pointwise commutative addition is an endomorphism again (maybe it will help someone to find the counterexample). Let $(G,+)$ be the abelian group and $(\operatorname{End}(G),\oplus)$ be the additive subgroup of its endomorphism ring. Let $\varphi,\psi$ be two endomorphisms, then \begin{align}
(\varphi\oplus\psi)(x+y) &= \varphi(x+y)+\psi(x+y) =\\&= \varphi(x)+\varphi(y)+\psi(x)+\psi(y) =\\&=\varphi(x)+\psi(x)+\varphi(y)+\psi(y) =\\&=
(\varphi \oplus \psi)(x) + (\varphi\oplus\psi)(y)
\end{align} so $\varphi\oplus\psi$ is an endomorphism. Is there an example of (necessarily non-commutative) group $G$ , s.t. $\operatorname{End}(G)$ fails to be a ring exactly due to the closure of endomorphisms under non-commutative pointwise addition?","['examples-counterexamples', 'ring-theory', 'abstract-algebra', 'noncommutative-algebra', 'group-theory']"
3512732,Finite Difference Method for the second order ODE $y''=\frac{y}{y+1}$,"I would like to solve a non-linear second order ODE for $y \in (0,1)$ using a numerical method, preferably finite differences. The ODE is $$
\frac{d^2 y}{d x^2} = \frac{y}{y+1}, \quad y(0)=\alpha, \ y(1)=\beta.
$$ I understand that a relaxation method can be used. I have worked out the iteration process to be $$y_i =  \frac{1}{2}(y_{i-1}+y_{i+1}) + \frac{1}{h^2}y_i - y_i(y_{i-1}-2y_i+y_{i+1}),$$ for a mesh step $h$ , but I'm unsure as to whether this is an efficient or incorrect method for this particular problem. A solution need not follow this particular method, any will do!","['ordinary-differential-equations', 'finite-difference-methods', 'numerical-calculus', 'boundary-value-problem', 'numerical-methods']"
3512806,Partial derivative with variables where some are dependent on each other,"Say I have a function $f(x,y,z,w)$ . Let $x$ and $y$ each be a function of $z$ and $w$ so $x=x(z,w)$ and $y=y(z,w)$ . Then we have a function of the form $$f(x(z,w),y(z,w),z,w)$$ Using the chain rule for functions of multiple variables I get $$\frac{\partial f}{\partial z} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial z}+ \frac{\partial f}{\partial y}\frac{\partial y}{\partial z}+ \frac{\partial f}{\partial z}\frac{\partial z}{\partial z}+ \frac{\partial f}{\partial w}\frac{\partial w}{\partial z}$$ As $w$ is not dependent on $z$ then $\frac{\partial w}{\partial z}=0$ $$\frac{\partial f}{\partial z} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial z}+ \frac{\partial f}{\partial y}\frac{\partial y}{\partial z}+ \frac{\partial f}{\partial z}1+ \frac{\partial f}{\partial w}0$$ $$\frac{\partial f}{\partial z} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial z}+ \frac{\partial f}{\partial y}\frac{\partial y}{\partial z}+ \frac{\partial f}{\partial z}$$ This means $\frac{\partial x}{\partial z}=0$ and $\frac{\partial y}{\partial z}=0$ but this is not necessarily true because $x(z,w)$ and $y(z,w)$ are dependent on $z$ . I am unsure where this contradiction is coming from. I think I must be applying the chain rule incorrectly. I am also unsure when partially differentiating with respect to $z$ whether to treat just $w$ as a constant or $x,y$ and $w$ as constants. I found similar problems: Partial derivative of a two variables function, one of which dependent on the other Partial Derivatives - constants However these discussed functions that can be written in terms of one variable whereas the function I am confused with can be written in terms of two variables at the least ( $z$ and $w$ ). Please explain what I am doing wrong and thank you for any help!","['partial-derivative', 'multivariable-calculus', 'derivatives', 'partial-differential-equations']"
3512940,Once a mathematical theorem is proven true like the Halting problem can it ever be disproven?,"Just curious about this article I read today in the Google News. I am not a mathematician but enjoy the history of mathematics and the article seems to suggest the Halting problem has been disproven. I always thought once a theorem is proven it would never be disproven but again I am not an expert. The article is the following: https://gizmodo.com/remarkable-mathematical-proof-describes-how-to-solve-se-1841003769 I do not know what the rules are for allowing me to enter a link so maybe I will write the part of the article in quotes to illustrate the point as follows: Computer scientists are buzzing about a new mathematical proof that proposes a quantum-entangled system sort of like the one described above. It seems to disprove a 44-year-old conjecture and details a theoretical machine capable of solving the famous halting problem, which says a computer cannot determine whether it will ever be able to solve a problem it’s currently trying to solve. The 150-page proof, titled simply “MIP*=RE,” deals in the esoteric subject of computational complexity. If it holds under scrutiny, it demonstrates a profound connection between quantum physics, computation, and mathematics. It shows that a theoretical class of computing devices—a verifier interrogating the quantum-entangled oracles—can check some of the most complex computer problems imaginable. The last paragraph is beyond my understanding with the level of math that I have but what disturbes me is that I always believed once a proof was shown to be true it could not be disproven. The halting problem is related to Godel's incompeteness theorem and I know Godel's Theorem has also been proven to be true. I thought perhaps someone who is expert could comment on this. Thank you.","['computer-science', 'logic', 'discrete-mathematics', 'quantum-computation', 'computability']"
3512961,Divide the positive integers into infinite sets,"I have a great problem from a past olympiad. Divide the set of positive integers into two infinite sets, such that in each set the sum of any seven numbers is in the same set. Find all such partitions! I need some help. Of course, I have found the trivial ones, such as even and odd numbers, but how to find them all? Thanks!","['contest-math', 'pigeonhole-principle', 'combinatorics']"
3512997,smooth structure on a smooth manifold [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Why there is a unique smooth structure (choice of a smooth atlas) up to diffeomorphism on the real line? Can someone please send a proof, thanks.","['manifolds', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
3513023,Conditional Variance for Bivariate Normal Random Variables is Constant,"Below is a problem I just did. My question for MSE is not how to solve it - but I provide it to illustrate what exactly I am asking. Suppose X,Y are bivariate normal random variables with $E[X] = 40$ , $\mathrm{Var}(X) = 76$ , $E[Y] = 30$ , $\mathrm{Var}(Y) = 32$ , and $\mathrm{Var}(X | Y = 28.5) = 57.$ Calculate $\mathrm{Var}(Y | X = 25)$ . Although I know very little about bivariate random variables, I was able to solve this problem because I have a formula: $$\mathrm{Var}(Y | X = x) = \sigma_{Y}^2(1 - \rho^2).$$ I am not certain, but based on convention I assume $\rho$ = $\rho_{X,Y}$ = $\frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}$ . Looking at the information given and my formula, I saw I could use the second formula to solve for $\rho$ , and then re-use the formula to find the desired value. This is when I realized - the question in no way depends on the values of the conditioning variables ( $Y = 28.5, X=25)$ . This seemed strange to me. Keep in mind, my solution is just number crunching for me, I don't have a lot of background knowledge to provide intuition. Can someone explain to me how this is intuitive that the function $f(x) =
 \mathrm{Var}(Y | X = x)$ is a constant function? In my head when I picture a bivariate normal distribution I see what looks like an ant-hill centered over (0,0) in $\mathbb{R}^2$ (yes, technically I'm picturing a standard-bivariate normal). But then if I consider the cross sections cut out by fixing values of $X$ , it seems the ones closer to the origin have a bigger hump - thus less variance? Is each cross section for different values of $X$ actually just like.. a scaling of the others? Thus variance stays fixed? Was this intentional in the construction of bivariate normals?","['statistics', 'conditional-probability', 'normal-distribution', 'probability-theory', 'probability']"
3513027,"How I can prove that $T$ is well defined and continuous such that we have :$\|T\|\leq \|g\|_q,$ $T(f)=\int_\Omega f\overline{g}\,d\mu$?","Exercice: let $(\Omega,\mathcal{A},\mu)$ be a measurable space, $p\in[1,+\infty[$ and let $g\in L^q(\Omega)$ with $q$ is the conjugate exponent of $p,$ let $T$ such that $T:L^p(\Omega)\to\mathbb{C}$ defined by $T(f)=\int_\Omega f\overline{g}\,d\mu$ , I want to prove that $T$ is well defined and continious such that we have : $\|T\|\leq \|g\|_q$ ? My Attempt: I have tried to reformualte the Holder inequality like this : \begin{eqnarray*}
\int_{\Omega}f\bar{g} \,d\mu & \leq & \left(\int_\Omega f^p \, d\mu\right)^{1/p} \left(\int_\Omega \bar{g}^q \, d\mu\right)^{1/q}\\
&\leq&\|f\|_p\|\bar{g}\|_q
\end{eqnarray*} Now $\|f\|_p\|\bar{g}\|_q$ is finite implies that $T$ is defined , for continuity I have used the fact that $T$ is measurable implies that is continious , But my problem how I can show that $\|T\|\leq \|g\|_q$ , I have used the assumption $1\leq p <q < \infty$ implies $\ell^p\subset \ell^q$ which prove the result , But am not care about this way?","['lp-spaces', 'lebesgue-integral', 'functional-analysis']"
3513047,Solve inverse trigonometric equation $\frac{\pi}{6}=\tan^{-1} \frac{11}{x} -\tan^{-1} \frac{1}{x}$,How do I go about solving for $x$ when I have: $\frac{\pi}{6}=\tan^{-1} \left( \frac{11}{x} \right)-\tan^{-1}\left( \frac{1}{x} \right)$ .,"['algebra-precalculus', 'trigonometry']"
3513089,Unexpected appearances of $\pi^2 /~6$.,"""The number $\frac 16 \pi^2$ turns up surprisingly often and frequently in unexpected places."" - Julian Havil, Gamma: Exploring Euler's Constant . It is well-known, especially in 'pop math,' that $$\zeta(2)=\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\cdots = \frac{\pi^2}{6}.$$ Euler's proof of which is nice. I would like to know where else this constant appears non-trivially. This is a bit broad, so here are the specifics of my question: We can fiddle with the zeta function at arbitrary even integer values to eek out a $\zeta(2)$ . I would consider these 'appearances' of $\frac 16 \pi^2$ to be redundant and ask that they not be mentioned unless you have some wickedly compelling reason to include it. By 'non-trivially,' I mean that I do not want converging series, integrals, etc. where it is obvious that $c\pi$ or $c\pi^2$ with $c \in \mathbb{Q}$ can simply be 'factored out' in some way such that it looks like $c\pi^2$ was included after-the-fact so that said series, integral, etc. would equal $\frac 16 \pi^2$ . For instance, $\sum \frac{\pi^2}{6\cdot2^n} = \frac 16 \pi^2$ , but clearly the appearance of $\frac 16\pi^2$ here is contrived. (But, if you have an answer that seems very interesting but you're unsure if it fits the 'non-trivial' bill, keep in mind that nobody will actually stop you from posting it.) I hope this is specific enough. This was my attempt at formally saying 'I want to see all the interesting ways we can make $\frac 16 \pi^2$ .' With all that being said, I will give my favorite example as an answer below! : $)$ There used to be a chunk of text explaining why this question should be reopened here. It was reopened, so I removed it.","['integration', 'big-list', 'pi', 'sequences-and-series', 'riemann-zeta']"
3513117,Using the complex definition of $\sin$ to solve an integral,"Is it possible to use the complex definition of trigonometric identities to simplify integrals? For instance, the integral: $$\int e^{-at}\cos(bt) \, dt$$ Would it be appropriate to use the definition of complex sine and then convert it back after the integration, versus using integration by parts?","['integration', 'complex-numbers']"
3513166,Group $G$ such that $[G : Z(G)] = 4$,"Let $G$ be a group, let $Z(G)$ be the center of $G$ , and suppose that $[G : Z(G)] = 4$ . (a) Prove that $x^2 \in Z(G)$ for every $x \in G$ . (b) Prove that $Z(G)$ contains an element of order two. Here are my thoughts so far: Recall that $Z(G)$ is a normal subgroup of $G$ . Thus, $G/Z(G)$ has a group structure. By the fact that $[G : Z(G)] = 4$ , we have that $|G/Z(G)| = 4$ . This gives that $G/Z(G)$ is either isomorphic to $\mathbb{Z}/4\mathbb{Z}$ or $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ . But, $G/Z(G)$ cannot be isomorphic to $\mathbb{Z}/4\mathbb{Z}$ ; this would mean that $G/Z(G)$ is cyclic $\Rightarrow$ $G$ abelian $\Rightarrow$ $G = Z(G)$ , which contradicts the index of $Z(G)$ in $G$ being equal to $4$ . Thus, so far I know that $G/Z(G)$ is isomorphic to $\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ . I also know that $G/Z(G) \cong Inn(G)$ , the inner automorphism group of $G$ . How can I use these facts to  prove the desired facts ? Here's one attempt to use that $G/Z(G) \cong Inn(G)$ for part (b). If this is the case, we have that $Inn(G) \cong \mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ $\Rightarrow$ $Inn(G)$ necessarily contains an element of order $2$ . Thus, the map $\phi: G \longrightarrow G : h \longmapsto ghg^{-1}$ for some $g \in G$ , where $\phi$ is not the identity map, has order $2$ $\Rightarrow$ $g^2hg^{-2} = h$ $\Rightarrow$ $g^2h = hg^{2}$ . It looks like I have a commutativity relation here -- how can I relate this to $Z(G)$ having an element of order $2$ ? For part (a), is the right idea to look at the order of the cosets $G/Z(G)$ ? I suppose all cosets that aren't the identity are of order $2$ , since $G/Z(G) \cong \mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z}$ . Is this the right route to take? Thanks for all of your help. (=","['normal-subgroups', 'group-theory', 'abstract-algebra']"
3513168,Suppose we draw two cards without replacement out of a standard deck of 52 cards,"Suppose we draw two cards without replacement out of a standard deck of 52 cards, while each time a card is drawn randomly with the (remaining) cards well-shuffled. Let A be the event that the first card is an Ace. and B be the event that the second card is a spade. Find out if A and B are independent. My attempt: 
Intuitively, of course they are not independent because P(B|A)=12/51 if A is the Ace of Spades, but P(B|A)=13/51 if A is not the Ace of Spades. But, teacher wants us to show it systematically, i.e. show $P(A)*P(B)$ is not equal to $P(A)intersectP(B)$ P(A)=1/13 P(B)=51/204 by Total probability theorem How do I find P(AnB)? I know the formula, but P(B|A) takes on two values depending on A.... which is where I am confused.",['probability']
3513195,Can separate manifold regions have the same coordinates?,"Despite referring to physics, this question is purely mathematical on geometry and topology of the given pseudo-Riemannian manifold. Case A As a starting point, consider the Schwarzschild solution extended to the interior of the horizon. Here we have two regions, external and internal with known different properties. We can describe the global spacetime (disregarding the maximal Kruskal extension) by two regions of the Schwarzschild coordinate chart, internal and external. (In this question, I am not concerned with the horizon boundary.) Case B Now consider a thin, hollow, massive shell of dust collapsing under a spherical symmetry to its Schwarzschild radius. As known, the spacetime outside the shell is Schwarzschild while the spacetime inside is time dilated Minkowski. We can describe this spacetime by two regions of a chart in the Schwarzschild coordinates. So far, so good. Problem A trouble begins in the frame of the collapsing shell where the proper time to the Schwarzschild radius is finite. There we have two logical options: The proper time of the shell ends at the horizon. In this case, the above mentioned coordinate chart (Case B) covers the entire global spacetime manifold (short of the horizon boundary perhaps). The shell in its frame continues through the horizon. Then, per the Penrose Singularity Theorem, a singularity must form on the inside. In this case the spacetime there should look somewhat similar to the Schwarzschild extension (Case A). In this case we end up with two spacetime regions on the inside: one is time dilated Minkowski (before crossing the Schwarzschild radius in the shell proper time) and the other is similar to extended Schwarzschild with a singularity (after the crossing). They seem to overlap over the same set of the Schwarzschild coordinates: for any $t,r,\phi,\theta$ there exist two different events in these spacetimes. Question Can two different regions of the same global spacetime manifold share the same coordinates? Sorry for the lack of rigor, I am not a mathematician. If any clarification or additional conditions are required, I would be happy to provide. Please do not hesitate to ask. Thanks for your expert insight! EDIT - Based on comments, below is a clarification on the terminology used in the question. Nothing new here, just some optional background for clarity. Schwarzschild The Schwarzschild spacetime is a pseudo-Riemanian manifold defined by the following metric in polar coordinates: $$ -{d \tau}^{2} = -\left(1 - \frac{r_\mathrm{s}}{r} \right) \,dt^2 + \left(1-\frac{r_\mathrm{s}}{r}\right)^{-1} \,dr^2 + r^2 d\Theta^2 $$ Where $d\Theta^2$ denotes the spherical metric induced by the Euclidean on a two sphere, i.e. $$ d\Theta^2 = d\theta^2 + \sin^2\theta \, d\varphi^2\;\;\; \text{and} \;\;\; r=\sqrt{x_1^2 + x_2^2 + x_3^2} $$ Here $r=r_s$ is a sphere of the event horizon , a coordinate singularity where the temporal part of the metric is zero while the spatial part radially diverges. The Schwarzschild metric accurately describes the gravitational field outside an uncharged non-rotating spherical object, such as a planet, star, black hole (or a hollow spherical shell in this question). By extending this metric through the horizon, we notice that $t$ becomes spacelike while $r$ becomes timelike on the inside. This extension is the mainstream interpretation of the spacetime geometry inside an uncharged non-rotating black hole. It is easy to see that a timeslice inside the horizon in the Schwarzschild coordinates is a spherinder rapidly shrnking in time $r$ to its axis (along $t$ ) called the Schwarzschild singularity, an infinite line $(r=0,-\infty<t<+\infty)$ removed from the spacetime manifold: Is the Schwarzschild singularity stretched in space as a straight line? In this question, in the Case A, the Schwarzschild metric applies both outside and inside the horizon; in the Case B, this metric applies only outside of the massive spherical shell. Minkowski The Minkowski spacetime in a hollow massive shell is a flat pseudo-Euclidean manifold defined by the following metric (where $t\equiv x_0$ ): $$ -{d \tau}^{2} = -H\, dx_0^2 + dx_1^2 + dx_2 + dx_3^2 $$ or in polar coordinates: $$ -{d\tau}^2=-H\,dt^2+dr^2+r^2d\Theta^2 $$ See Weinberg, ""Gravitation and Cosmology"", p. 337 where $H$ is denoted as $f(t)$ . Here $H$ defines the time dilation (squared) and can be renormalized to unity in the coordinates inside the shell, but not in the Schwarzschild coordinates, because $dt$ must be continuous through the shell (the same time dilation inside as at the shell): $$ H=1-\dfrac{r_s}{R} $$ where $R$ represents the radius of the massive shell, so the time dilation is the same anywhere inside the shell, at any radial coordinate $r$ . See: On a Common Misunderstanding of the Birkhoff Theorem where $H$ is denoted as $h(t)$ . In this question, this metric applies to the Case B inside the massive spherical shell while the shell is larger than the horizon $r>r_s$ (which is forever in the Schwarzschild coordinates).","['metric-spaces', 'differential-geometry']"
3513334,"Distribution of number of heads, when we keep tossing a coin until we have 4 tails","We toss a fair coin until we've tossed tails exactly 4 times. Let $X$ be the number of tossed heads. What is the distribution of $X$ ? My attempt: We keep tossing the coin until we have registered 4 tails. Suppose that we needed $n$ tosses. The last toss has to be tails, and exactly three of the preceding tosses had to be tails as well: $$ P(\text{4 tails}|n \text{ tosses})=\frac12\cdot \binom{n-1}{3}\left(\frac 12\right)^3\left(\frac 12\right)^{n-4}=\frac{(n-1)(n-2)(n-3)}{6\cdot 2^n}.$$ This is also the probability of tossing $n-4$ heads, knowing that we needed $n$ tosses to stop the game. Is this the answer to the question? How do I find 'the distribution' of $X$ ? Thanks.","['statistics', 'solution-verification', 'probability']"
3513371,Handling empty sets in relations,"Let A consist of all 8 subsets of the set $\{1, 2, 3\}$ , that is A = $\{ \phi , \{1\}, \{2\}, \{3\}, ...\}$ The relation $R$ on $A$ is defined by: $aRb$ iff some element of $a$ is larger than some element of $b$ . My question is, is $(\phi,\{1\})$ an element of $R$ ? My answer is no, because there simply is no element of $\phi$ . Similarly, $(\{1\}, \phi)$ is not in $R$ . Is my thinking correct?","['elementary-set-theory', 'relations', 'logic', 'discrete-mathematics']"
3513407,$G$-bundles over $S^2$,"In light of the fact that the only homogeneous (under a finite--dimensional Lie group action) $S^2$ -bundle over $S^2$ is the trivial one. I would like to know if this fact is more general. i.e., is any homogeneous fiber bundle over $S^2$ and with a fiber that is  connected and simply-connected is trivial? I did a google search and I found the following in Encyclopedic dictionary of math. p. 572 , the following: ""The set of the equivalence classes of principal $G$ -bundles or $G$ -bundles with fiber $F$ over the base $S^n$ is one-to-one correspondence with the set $\pi_{n-1}(G)/\pi_0(G)$ of equivalence classes under the operation  of $G$ on $\pi_{n-1}(G)$ ..."" So for example in the (principal) Hopf-fibration $$SO(2)\hookrightarrow SO(3) \to SO(3)/SO(2)=S^2$$ we have $\pi_1(SO(2))=\mathbb Z$ , so there are infinitely many different $S^1$ -bundles over $S^2$ . However, I didn't find an ""actual""  reference for the result claimed in the shaded box in the case of $G$ -homogeneous bundles? But in case it is true, shouldn't be a condition on the effectivity of the group action as we can take our group to be simply connected (by going up to the universal covering)? I have a hunch about the following situation: Let $J/H\hookrightarrow G/H\to G/J$ be a fiber bundle consisting of a connected (finite--dimensional) Lie group $G$ and closed connected subgroups $H\subset J$ with base $$G/J\;\cong\; \mathrm{SU}(2)/S^1\;\cong \;S^2$$ and a compact fiber $$J/H\;\cong\;\mathrm{SU}(n)/T$$ where $T$ is a maximal torus in $\mathrm{SU}(n)$ . In this case, the following can be shown: (1) the total space $G/H$ can be given a structure of a (generalized)
flag variety. Thus, $G/H\cong \mathrm{SU}(m)/T_m$ . (2) the $\mathrm{SU}(2)$ acting in the base is a normal subgroup of $G$ . Since maximal tori are conjugate  to each other, then I can pick any maximal torus $T_m$ such that $$G/H \;\cong\; \mathrm{SU}(m)/ T_m\; \cong \; \Big(\mathrm{SU}(n)\times \mathrm{SU}(2)\Big)\Big/\Big(T\times S^1\Big)\; \cong \; \mathrm{SU}(n)/T \;\times \mathrm{SU}(2)/S^1$$ But even if my hunch is right, a general fact would be more interesting! My question : Let $G$ be a connected and simply-connected finite dimensional Lie group and $H\subset J$ be closed subgroups of $G$ such that $J$ is connected  and $J/H$ is simply-connected. Is the following fibration trivial? $$J/H\hookrightarrow G/H\to G/J\cong S^2$$","['algebraic-topology', 'fiber-bundles', 'topological-groups', 'lie-groups', 'differential-geometry']"
3513485,Testing whether polynomial is in algebra of given collection of polynomials,"A collection $\Sigma$ of polynomials is an algebra if: $\lambda f + \eta g \in \Sigma$ for any $f,g \in \Sigma,
    \lambda,\eta \in \mathbb{R}$ $f,g \in \Sigma$ implies $fg \in \Sigma$ . $1 \in \Sigma$ We say that $P$ is in the
    algebra of $\{P_1,\dots,P_n\}$ if $P$ is in the smallest algebra
    containing $P_1,\dots,P_n$ . I was wondering if there was a way, on any computer math software, to check whether a given $P$ is in the algebra of a given collection $P_1,\dots,P_n$ . I know Mathematica can check if $P$ is in the ideal generated by $P_1,\dots,P_n$ , but I don't know about algebras, or about any software besides Mathematica (which I barely know). Example : Take $n \ge 1$ , and let $P_1 = x_1+\dots+x_n, P_2 = x_1^2+\dots+x_n^2,\dots P_n = x_1^n+\dots+x_n^n$ . Then all $n$ of the following symmetric functions are in the algebra generated by $P_1,\dots,P_n$ : $$x_1+\dots+x_n$$ $$x_1x_2+\dots+x_{n-1}x_n$$ $$x_1x_2x_3+\dots+x_{n-2}x_{n-1}x_n$$ $$\dots$$ $$x_1\dots x_n$$ I'd appreciate any help.","['abstract-algebra', 'math-software']"
3513560,How to finish the derivative calculation?,"I know that this can be calculated as: $x^n=n \cdot x^{n-1}$ but I need to find a solution as a limit: $f(x)=\sqrt[3]{x} ;f^{'}(x)=\lim _{\Delta x\to 
0}\frac{\sqrt[3]{x_0+\Delta x} -\sqrt[3]{x_0}}{\Delta x}=\lim 
_{\Delta x\to 0}\frac{(\sqrt[3]{x_0+\Delta x} -\sqrt[3]{x_0} 
)(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}{\Delta 
x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=\lim _{\Delta x\to 
0}\frac{(\sqrt[3]{x_0+\Delta x})^{2}-(\sqrt[3]{x_0})^{2}}{\Delta 
x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=\lim _{\Delta x\to 
0}\frac{(\sqrt[3]{x_0+\Delta x})^{2}-(\sqrt[3]{x_0})^{2}}{\Delta 
x(\sqrt[3]{x_0+\Delta x} +\sqrt[3]{x_0} )}=...$ I don't know how to calculate further due to: $(\sqrt[3]{x_{0}+\Delta x})^{2}-(\sqrt[3]{x_{0}})^{2}$ . If instead it would be: $(x_{0}+\Delta x)^{2}-(x_{0})^{2}$ , then I would write $(x_{0}+\Delta x)^{2}$ as: $(x_{0})^{2}+2 \cdot x_{0} \cdot \Delta x +(\Delta x)^{2}$ , but i don't know how to be with cube root ?",['derivatives']
3513584,Integral operators on $L^2$ and the Schwartz kernel theorem,"I would like to better understand the interplay between bounded operators on $L^2$ and operators that are given by distributional Schwartz kernels. As someone who is usually more interested in $L^2$ and Sobolev spaces, I'm aware that many useful bounded operators on $L^2$ can be represented by their Schwartz kernels. However, it isn't clear to me what the relation between these two sets of operators is, perhaps because (it seems to me) discussions of distribution theory usually take place away from the Hilbert space setting. In particular, I haven't been able to find explicit answers to the following questions: Question 1: Can a bounded operator $T:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ always be represented by a distributional Schwartz kernel? Question 2: If not, is there an easy example of $T$ that cannot be represented in this way? I would also appreciate any suggestions for references that address these or similarly minded concerns.","['operator-theory', 'sobolev-spaces', 'functional-analysis', 'distribution-theory']"
3513683,use an inversion circle to prove an identity for the arbelos and magic twin circles,"In the figure, we assume that everything is ""how it looks.""  For instance, everything that looks tangent, is tangent, all the points that appears to be centers of the circles, are centers, and the line segment shown is vertical. Also, assume circles $D$ and $G$ are orthogonal. Let $r_G$ be the radius of circle $G$ , let $r_B$ be the radius of circle $B$ , and let $r_C$ be the radius of circle $C$ . Problem: We need to use circle $D$ as an inversion circle to prove that $$
r_G=\frac{r_Br_C}{r_B+r_C}.
$$ I asked about this problem a few days ago and it was answered here using the Pythagorean Theorem, but without using the inversion circle.  I was hoping that would give me an insight to using inversion, but sadly it did not. Some observations. We know that $\odot G$ is invariant under inversion since it's orthogonal to the inversion circle. If we can prove that $\odot D$ passes through $E$ , that would be sufficient for me to prove the rest. Any ideas would be much appreciated.  Thanks!","['euclidean-geometry', 'circles', 'geometry']"
3513720,Birthday problem-Probability exactly $2$ triples and $4$ pairs if $20$ people in room,"Say there are 20 people in a room. What is the probability there are exactly 2 triples and 4 pairs. Is my answer shown below correct?
Assume 365 days in the year. $P= \dfrac{\binom{365}{2}\binom{363}{4}\binom{20}{3}\binom{17}{3}\binom{14}{2}\binom{12}{2}\binom{10}{2}\binom{8}{2} \cdot 359 \cdot 358 \cdot 357 \cdot 356 \cdot 355 \cdot 354}{365^{20}}$ ? Term $365C2$ chooses the $2$ birthdays for the $2$ triples. Each triple has a different birthday. Term $363C4$ chooses the $4$ birthdays for the $4$ pairs. Each pair has different birthdays. Term $20C3$ selects the $3$ people for the first triple and $17C3$ the $3$ people for the second triple. Term $14C2$ picks the $2$ people for first pair, $12C2$ for second pair, $10C2$ the third pair, and finally $8C2$ for the fourth pair. The term $(359 \cdot 358 \cdot 357 \cdot 356 \cdot 355 \cdot 354)$ is the birthdays for the remaining $6$ people, which do not match. I start with $359$ because $6$ birthdays have taken by the $2$ triples and the $4$ pairs.
All this is then divided by the total number of possible birthday selections $365^{20}$ . I am wondering if the selection of the people for the $2$ triples should be $20C6$ instead of $20C3 \cdot 17C3$ as I show. I believe my method is the correct one. Please let me know.","['combinatorics', 'birthday', 'probability']"
3513730,"Show that $(U+X)Y$ is uniformly distributed over $[-n,n]$","$X,U$ and $Y$ are random variables for which: $$ \forall i\in\{0,1,\dots,n-1\}: P(X=i)=\frac 1n,$$ $$ P(Y=1)=P(Y=-1)=\frac 12,$$ $$ U =_d U[0,1].$$ Show that $(U+X)Y = _d U[-n,n]$ if $U,X$ and $Y$ are mutually independent. My attempt: We note that $P\left( (U+X)Y\le z\right) = \frac 12\left(P(U+X\le z)+P(U+X\ge -z \right)$ . We determine the distribution of $T=U+X$ : $f_{T,X}(t,x)=f_U(t-x)f_X(x)$ (using $U\perp\!\!\!\perp X$ ). Since $X$ is discrete, we get the marginal distribution of $T$ by summing over all possible values that $X$ can take: $$ f_{U+X}(t)=\sum_{x=0}^{n-1} f_U(t-x)P(X=x)=\frac 1n\sum_{x=0}^{n-1}f_U(t-x).$$ Inspired by the claim, we can note that $f_{U}(t-x)=1$ for $-n\le t \le n$ . Therefore $$ f_{U+X}(t)=1\cdot I(-n\le t\le n).$$ Then, $$ P(U+X\le z)=\int_{-n}^zdt=z+n =P(U+X\ge -z).$$ We get $P((U+X)Y\le z)=F_{(U+X)Y}(z)=z+n$ , $-n\le z\le n$ . This does not correspond to the CDF of a uniformly distributed RV on $[-n,n]$ . Where is my mistake?","['statistics', 'probability-distributions', 'solution-verification']"
3513736,How to determine all $2 \times 2$ normal matrices?,"Determine all $2 \times 2$ normal matrices. In particular, how would I show that there are normal matrices which are neither unitary, Hermitian, skew-Hermitian, symmetric, nor skew-symmetric. The only thing I do know is $AA^* = A^*A$ , but I'm not sure how to proceed. I tried writing in $a,b,c,d$ as entries of $2 \times 2$ matrix, but it seemed to lead nowhere.","['matrices', 'hermitian-matrices', 'linear-algebra']"
3513798,No bijective continuous function from upper half plane to $\mathbb{C}$,"If we take $H := \{z \in \mathbb{C}\ : im(z) \geq 0\}$ . Is there a continuous bijective function from $H$ to $\mathbb{C}$ ? $H$ is not isomorphic to $\mathbb{C}$ , because we include the real line in $H$ . So there can't be a bijective continuous map with an inverse that is also continuous. But I can't seem to find a map $H \longrightarrow \mathbb{C}$ that would be continuous and bijective at all. I'm wondering if it's the same situation as in the real numbers, where two connected $X,Y \subset \mathbb{R}$ are already homeomorphic if there's a continuous bijection from $X \longrightarrow Y$ , and there's no necessity to check that the inverse is continuous. But to go back to the example with $H$ and $\mathbb{C}$ . It could be that they're not isomorphic, but that there is a continuous bijection from $H$ to $\mathbb{C}$ , but the inverse wouldn't be continuous. So that's why I'm wondering if there is a continuous bijection from $H$ to $\mathbb{C}$ at all.","['continuity', 'general-topology']"
3513841,When do two conic sections intersect?,"Given two conics $$ax^2+bxy+cy^2+dx+ey+f=0$$ and $$Ax^2+Bxy+Cy^2+Dx+Ey+F=0$$ I want to find the conditions that they have a common intersection point. Unfortunately I think the answer may involve enormous polynomials of degree $>12$ . (For example the Reduce function in Mathematica simply gives up!) To make it 'simpler' I consider writing these as vector formula with $X=(1,x,y)$ then they can be written $P^{ij}X_iX_j=0$ and $Q^{ij}X_iX_j=0$ . Then the solution would only depend on the pair of $3\times3$ matrices $P$ and $Q$ . I have searched everywhere and can't find a solution so I expect this is a very hard question that may even be unsolved. Is there any known solution? Particularly a 'neat' solution in terms of matrices would be nice!","['algebraic-geometry', 'conic-sections', 'real-algebraic-geometry']"
3513887,"Let $(G,\cdot)$ be a group of order $2n$ with $n$ elements of order $2$. Prove $n$ is odd and $G$ has an abelian subgroup of order $n$.","Let $(G,\cdot)$ be a group of order $2n$ which has exactly $n$ elements of order $2$ . Prove that $n$ is odd and that $G$ has an abelian subgroup of order $n$ . For the first part, the elements whose order is greater than $2$ can be grouped in pairs of the form $\{x,x^{-1}\}$ . Now it easily follows that $n=\text{even}-\text{odd}=\text{odd}$ . For the second part, I though about considering the set $H=\{x_1,x_2,\dots ,x_n\}$ where $x_i$ is an element of order $2$ $\forall i=\overline{1,n}$ . I observed that for $i\neq j$ we have that $x_i x_j \in G\setminus H$ , because otherwise $\{e,x_i,x_j, x_i x_j\}$ would be a subgroup of order $4$ of $G$ , which would contradict Lagrange's theorem. I didn't know how to proceed from here.","['abelian-groups', 'group-theory', 'abstract-algebra', 'finite-groups']"
3513905,Computing $\int_0^1\frac{\ln(1-x^2)}{x}\operatorname{Li}_2\left(\frac{1-x}{2}\right)\ dx$,"How to evaluate $$I=\int_0^1\frac{\ln(1-x^2)}{x}\operatorname{Li}_2\left(\frac{1-x}{2}\right)\ dx\ ?$$ This integral was mentioned by @nospoon in the comments of this problem . What I tried is integration by parts which gives $$I=\frac12\int_0^1\frac{\operatorname{Li}_2(x^2)}{1-x}\ln\left(\frac{1+x}{2}\right)\ dx$$ Now if we use the following identity that can be found on page $95$ Eq $(4)$ of this paper $$\sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)x^n=-\ln(2)+\sum_{n=1}^\infty(-1)^n(\overline{H}_n-\ln2)x^n=\frac{\ln\left(\frac{1+x}{2}\right)}{1-x}$$ and multiply both sides by $\large \frac{\operatorname{Li}_2(x^2)}{x}$ then $\int_0^1$ ,  we obtain $$\int_0^1\frac{\operatorname{Li}_2(x^2)}{x(1-x)}\ln\left(\frac{1+x}{2}\right)\ dx=-\frac12\ln2\zeta(3)+\sum_{n=1}^\infty(-1)^n(\overline{H}_n-\ln2)\int_0^1 x^{n-1}\operatorname{Li}_2(x^2)\ dx$$ where $$\int_0^1 x^{n-1}\operatorname{Li}_2(x^2)\ dx\overset{x^2\to x}{=}\frac12\int_0^1 x^{\frac n2-1}\operatorname{Li}_2(x)\ dx=\frac12\left(\frac{2\zeta(2)}{n}-\frac{4H_{n/2}}{n^2}\right)$$ so $$\int_0^1\frac{\operatorname{Li}_2(x^2)}{x(1-x)}\ln\left(\frac{1+x}{2}\right)\ dx=-\frac12\ln2\zeta(3)+\sum_{n=1}^\infty(-1)^n(\overline{H}_n-\ln2)\left(\frac{\zeta(2)}{n}-\frac{2H_{n/2}}{n^2}\right)$$ and since $$\int_0^1\frac{\operatorname{Li}_2(x^2)}{x(1-x)}\ln\left(\frac{1+x}{2}\right)\ dx=\int_0^1\frac{\operatorname{Li}_2(x^2)}{x}\ln\left(\frac{1+x}{2}\right)\ dx+2I$$ therefore $$I=-\frac14\ln2\zeta(3)+\frac12\color{blue}{\sum_{n=1}^\infty(-1)^n(\overline{H}_n-\ln2)\left(\frac{\zeta(2)}{n}-\frac{2H_{n/2}}{n^2}\right)}-\frac12\underbrace{\int_0^1\frac{\operatorname{Li}_2(x^2)}{x}\ln\left(\frac{1+x}{2}\right)\ dx}_{\text{manageable}}$$ any idea how to evalute the blue sum? I think I made it more complicated. Any other ideas? Thank you.","['integration', 'harmonic-numbers', 'calculus', 'closed-form', 'sequences-and-series']"
3513917,Inconsequent use of coordinates in function evaluation and differentiation.,"Assume $X = (0,\infty)$ , $Y = \mathbb{R}$ and that $f : X \to Y$ is a function with some rule, for example $$
f(x) = ax; \qquad a\in \mathbb{R} := A.
$$ These symbols provide the following inference: ( $i$ ) the point $x$ in the domain of $f$ is associated to the point $ax \in Y$ by the relation $f$ . ( $ii$ ) the point $x$ in the domain of $f$ is associated to the point $$
\lim_{t \to 0} \frac{1}{t} \left[ a(x + t) - a(x) \right] \in Y 
$$ by the relation $\partial f/\partial x$ . Two concerns: I have seen authors switch between the notation $f(x)$ and $f(b)$ carelessly. That is no problem for inference provided above, because in the first instance I know that I should associate $x \mapsto ax$ and in the second $b \mapsto ab$ .  However, some authors seem to switch between « $f(x)$ » and « $f(a)$ » as to claim that «the symbol $x$ appears in the rule $f$ » and «the symbol $a$ appears in the rule $f$ » respectively. Every time this happens, I wonder if the picture in ( $i$ ) and ( $ii$ ) is indeed correct and/or if it is reconcilable with that meaning. However, this dilemma becomes vivid if you consider the derivative: Suppose (to my horror) that an author wrote $\partial f/ \partial b = 0$ and I wanted to give meaning to this expression. It seems to me that the formula for the derivative no longer applies because we were never told how to define the limit (the derivative) for any other coordinate than $x$ (the coordinate on the domain of $f$ ). At best, I can give meaning to $\partial f/ \partial a$ if I redefine the function $f : A \to Y$ with a different domain by $f(a) = ax$ because the symbol $a$ appears in the rule $f$ above. However, I would not know how to define the limit with respect to the symbol $b$ , because it does not appear in the product $ax$ . If the view in ( $i$ ) and ( $ii$ ) is indeed correct, can you explain what happens when authors write $f(a)$ «seemingly to say that $f$ contains the symbol $a$ » and $\partial f/\partial b = 0$ «seemingly to say that $f$ does not contain the symbol $b$ » such that these notions can be reconciled? If ( $i$ ) or ( $ii$ ) is false, please let me know what is correct.","['smooth-manifolds', 'functions', 'manifolds', 'derivatives', 'differential-geometry']"
