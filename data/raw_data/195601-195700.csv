question_id,title,body,tags
3765778,Linearized system for $ \begin{cases} \frac{d}{dt} x_1 = -x_1 + x_2 \\ \frac{d}{dt} x_2 = x_1 - x_2^3 \end{cases} $ is not resting at rest point?,"Assume there is the dynamical system $$
\begin{align}
\frac{d}{dt} x_1 &= -x_1 + x_2 \\
\frac{d}{dt} x_2 &=  x_1 - x_2^3
\end{align}
$$ The system is at rest at the point $(x_1, x_2) = (1, 1)$ and the point is stable. At this point of course $$
\begin{align}
\frac{d}{dt} x_1 &= 0 \\
\frac{d}{dt} x_2 &= 0
\end{align}
$$ I want to investigate the rest point more and so I use the linear model from the Taylor series at the rest point: $$
\frac{d}{dt}x = \begin{pmatrix} -1 & 1 \\ 1 & -3 \end{pmatrix}x
$$ I want to simulate both nonlinear and linear model. But something is strange. At the rest point I have: $$
\frac{d}{dt}x = \begin{pmatrix} -1 & 1 \\ 1 & -3 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ -2 \end{pmatrix}
$$ So although the nonlinear model is at rest at $(1, 1)$ the linear model is not at rest there! So when I simulate both systems they are very different even at the start and even if the start point is very near to the rest point. Look: The red $x_2$ trajectory is going even in the wrong direction at the start. What is the cause of the problem? Shouldn't the linear system approximate the nonlinear system at least when it starts near the rest point?","['linearization', 'ordinary-differential-equations', 'control-theory', 'numerical-methods', 'dynamical-systems']"
3765785,"Is it possible to construct a continuous and bijective map from $\mathbb{R}^n$ to $[0,1]$?","Let $U$ be a non-trivial finite-dimensional vector space over $\mathbb R.$ I am trying to use a bijective and continuous map $f: U \to [0,1]$ and $d(x,y)=|f(x)-f(y)|$ to prove that there exist a metric on $U$ that makes $U$ compact. However, I couldn't find such continuous and bijective map: $f:U\to[0,1] \text{ (or $[0,1]^n$).}$ Is there any example? Or is there any other way to prove there exists a metric on $U$ that makes $U$ compact? Edited: Thank you for all of your comments. I just started to learn compactness these days so I am not very good at some of the concepts. Now I understand that there is no need to construct a continuous map to prove the compactness. I also know that there does not exist a norm on U which makes U compact. My question is: how to prove there does exist a metric on U which makes U compact?","['general-topology', 'vector-spaces', 'compactness', 'metric-spaces']"
3765820,Let $\lambda$ be a real eigenvalue of matrix $AB$. Prove that $|\lambda| > 1$.,"Let $A$ and $B$ be real symmetric matrices with all eigenvalues strictly greater than 1. Let $\lambda$ be a real eigenvalue of matrix $AB$ . Prove that $|\lambda| > 1$ . My solution: Let $a$ and $b$ be eigenvalues of $A$ and $B$ corresponding the eigenvectors $y$ and $x$ , respectively. Looking at the following dot product: $$\langle ABx,y \rangle = \langle Bx,A^Ty \rangle=\langle Bx,Ay \rangle = \langle bx,ay \rangle=ab\langle x,y \rangle=\langle abx,y \rangle$$ we get $$(AB)x=(ab)x$$ Therefore, $\lambda := ab$ is an eigenvalue of $AB$ . Since $a>1$ and $b>1$ , it follows that $\lambda > 1$ However, it doesn't seem ok as the problem was actually asking to prove $|\lambda|>1$ . Indeed, $\lambda > 1 \implies |\lambda|>1$ , but then the problem wouldn't write $|\lambda|$ in my opinion. The given solution: The transforms given by $A$ and $B$ strictly increase the length of every nonzero vector, this
can be seen easily on a basis where the matrix is diagonal with entries greater than $1$ in the diagonal.
Hence their product $AB$ also strictly increases the length of any nonzero vector, and therefore its real
eigenvalues are all greater than $1$ or less than $-1$ . Any help is appreciated.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3765826,Formulas for the Spinor Representation Product Decompositions $2^{[\frac{N-1}{2}]} \otimes 2^{[\frac{N-1}{2}]}=?$ and ...,"We know that given the dimension $N$ , we can construct the corresponding spinors for the $Spin(N)$ group (which has $Spin(N)/\mathbb{Z}_2=SO(N)$ so $Spin(N)$ is a double cocver of the spatial rotational group $SO(N)$ ). Now, for a general $N$ , I believe there is a $$2^{[\frac{N-1}{2}]}$$ -dimensional irreducible Spinor representation (irrep) of the $Spin(N)$ group. Here $[\frac{N-1}{2}]$ is the step function takes the maximum integer value but smaller than $\frac{N-1}{2}$ . For example, $N=2n$ even, we have $$2^{[\frac{N-1}{2}]}=2^{[\frac{2n-1}{2}]}=2^{{n}-1}=2^{\frac{N}{2}-1}.
$$ For example, $N=2n+1$ odd, we have $$2^{[\frac{N-1}{2}]}=2^{n}=2^{\frac{N-1}{2}}.
$$ My question is that are there general simple formulas that can decompose the product of the spinor representations $2^{[\frac{N-1}{2}]}$ and their complex conjugation representations $\overline{2^{[\frac{N-1}{2}]}}$ , say for the even $N$ : $$
2^{[\frac{N-1}{2}]} \otimes 2^{[\frac{N-1}{2}]} = N  \oplus \dots  ?
$$ $$
2^{[\frac{N-1}{2}]} \otimes \overline{2^{[\frac{N-1}{2}]}} = 1 \oplus \frac{N(N-1)}{2}  \oplus \dots?
$$ For the odd $N$ : $$
2^{[\frac{N-1}{2}]} \otimes 2^{[\frac{N-1}{2}]} =
 1 \oplus N  \oplus \frac{N(N-1)}{2}  \oplus  \dots  ?
$$ For example, when $N=3$ , we have: $$
2 \otimes 2 =2 \otimes  \overline{2} = 1 \oplus 3.
$$ When $N=4$ , we have: $$
2 \otimes 2 = 4.
$$ $$
2 \otimes  \overline{2} = 1 \oplus 3.
$$ For example, when $N=5$ , $$
4 \otimes 4 = 1 \oplus 5 \oplus 10.
$$ $N=6$ , we have: $$
4 \otimes 4 = 6 \oplus 10.
$$ $$
4 \otimes \overline{4} = 1 \oplus 15.
$$ $N=7$ : $$
8 \otimes 8 = 1 \oplus 7 \oplus 21 \oplus 35.
$$ Do we have a formula for general $N$ ? Ref/s are welcome!","['spin-geometry', 'representation-theory', 'group-theory', 'lie-groups', 'differential-geometry']"
3765836,Is it true that if $P(\int_0^T f^2(s) ds<\infty)=1$ then the exponential defines a density?,Let $f(t)$ be a progressively measurable process wrt Brownian motion $B(t)$ so that $$P\left(\int_0^Tf^2(s)ds<\infty\right)=1$$ Is it true then that the exponential $$\exp\left(\int_0^T f(s)dB(s)-\frac12\int_0^Tf^2(s)ds\right)$$ defines a density on Wiener space? I know the Novikov condition $$E\left[\exp\left(\int_0^Tf^2(s)ds\right)\right]<\infty$$ implies that the exponential defines a density. But what is you just have a.s. $L^2$ ?,"['stochastic-analysis', 'stochastic-processes', 'stochastic-calculus', 'probability']"
3765845,"Find the smallest eigenvalue of $G=[ \exp(-(x_i-x_j )^2]_{i,j}$ for ${\bf x}=[x_1,\dots,x_n]$","Consider a sequence $\{x_1,...,x_n \}$ such that $b=\max_i  |x_i|$ and $d_{\min}=\min_{ij: i \neq j} |x_i-x_j|$ .  We assume that $b<\infty$ and $d_{\min}>0$ . Can we find a non-trivial lower bound on the smallest eigenvalue of $$G=[ \exp(-(x_i-x_j )^2)]_{i=1..n,j=1..n}$$ We want this lower bound to depend on some property of this sequence. I was thinking of writing it as \begin{align}
u^T G u =\sum_i \sum_j  u_i u_j   \exp(-(x_i-x_j )^2)
\end{align} and showing a lower bound that holds for all $(u_i,u_j)$ . We have the following bounds on each entry $$\exp(-d_{\min}^2) \ge \exp(-(x_i-x_j )^2) \ge  \exp(-4 b^2).$$ However, I don't know how to combine these two steps. Note that we know that $G$ is positive definite. This follows since $\exp(-t^2)$ is a positive definite kernel.","['linear-algebra', 'positive-definite', 'eigenvalues-eigenvectors']"
3765877,Does $\int_1^\infty\frac{f_ng_n}{f_n^2+g_n}dx$ go to $0$ under these conditions of $f_n$ and $g_n$,"Question: Let $f_n,g_n:[1,\infty)\rightarrow (0,\infty)$ be two sequences of measurable functions such that $|g_n(x)|\leq\frac{1}{x^3} \forall x\geq1$ and $f_n\rightarrow 0$ pointwise almost everywhere.  Is it always true that $\int_1^\infty\frac{f_ng_n}{f_n^2+g_n}dx\rightarrow0$ ? My thoughts: I was thinking that some sort of dominated convergence theorem will need to be used here since $|g_n(x)|$ is bounded by an integrable function $\frac{1}{x^3} \forall x\geq1$ .  The pointwise a.e. convergence of $f_n$ to $0$ seems like it would be easy to play with, but I'm just not sure how to deal with these functions as they are in the integrand.  Any help, suggestions, etc. are appreciated!  Thank you!","['measure-theory', 'measurable-functions', 'real-analysis']"
3765888,"Why is it called the ""sampling distribution of the mean""?","Is there a good (or even a bad) reason why it's called the ""sampling distribution of the mean"" and not the ""distribution of the sample mean""? If we take multiple samples all of the same size, $n$ , we get a distribution of sample means , $\bar{X}$ . If I get this right, this is called the ""sampling distribution of the mean"". But that seems like an overly confusing name. I can be fussy about names of things. But then sometimes there's a good reason for a ""bad"" name. So why did we give a distribution of sample means this unwieldy name? Is it wrong to call it a ""distribution of sample means""?","['descriptive-statistics', 'statistics', 'terminology']"
3765928,Is an automorphism a function or a group?,this is my first time on the Stack Exchange so I apologize if I went about asking this improperly! I was working on a problem set that asked to compute Aut(S3). In class we've been thinking about automorphisms as a function that maps elements of one group to another. However it seems like an automorphism can be better thought of as a subgroup of S3. Could someone provide clarity about what it means to compute/find Aut(S3)?,"['automorphism-group', 'group-theory', 'abstract-algebra']"
3765947,Is there a function that grows slower than the factorial function and whose derivative grows faster than the function itself? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is there a function that grows slower than the factorial function and whose derivative grows faster than the function itself? In symbols,
is there a function $f(x)$ that satisfies $f(x) = o(x!)$ and $\lim_{x \to \infty} f(x)/f'(x) = 0$ ?","['factorial', 'calculus', 'functions', 'limits', 'derivatives']"
3765999,Is the determinant a tensor?,"I was reading Schutz's book on General Relativity. In it, he says that a(n) $M \choose N$ tensor is a linear function of $M$ one-forms and $N$ vectors into the real numbers. So does that mean the determinant of an $n \times n$ matrix is a $0 \choose n$ tensor because it is a function that maps the $n$ column vectors of the matrix to a real number (the value of the determinant)? But then, the determinant also maps the $n$ column vectors of the matrix to the same real number (the value of the determinant). So would the tensor representation of the determinant be different if you choose the map for the column vectors than the map for the row vectors?","['determinant', 'linear-algebra', 'tensor-rank', 'tensors']"
3766002,"How do you take the derivative $\frac{d}{dx} \int_a^x f(x,t) dt$?","How does one take the derivative for the function $g(x) = \int_a^xf(x,t)\ dt$ ? $$ \frac{d}{dx}g(x) = \frac{d}{dx}\int_a^xf(x,t)\ dt $$ For example, how would one find $\frac{d}{dx}\int_0^x x + t \ dt$ ? If $f=f(t)$ , then I know I can just use the fundamental theorem, but here $f=f(x,t)$ .","['integration', 'calculus', 'derivatives']"
3766020,Prove that $\lim \limits_{T\to \infty} \frac{1}{T} \int_{-T/2}^{T/2} \cos(\omega t + \theta)dt = 0$,"How to prove that $L=0$ if $$L = \lim \limits_{T\to \infty} \frac{1}{T}\int\limits_{-T/2}^{T/2} \cos(\omega t + \theta)\, dt.$$ I want to say the numerator is equal to zero as $T$ approaches infinity because integrating a period function over an infinite range is zero, however, the denominator also grows to infinity. Is that enough to prove that $L=0$ ?","['integration', 'limits', 'calculus']"
3766042,Solving $ 2(2^x- 1) x^2 + (2^{x^2}-2)x = 2^{x+1} -2$,"I was doing the problem Find all real solutions for $x$ in: $$ 2(2^x- 1) x^2 + (2^{x^2}-2)x = 2^{x+1} -2$$ There was a hint, to prove that $2^{x} - 1$ has the same sign as $x$ , although with basic math, if $2^0 - 1$ = 0, and x in this case = 0, The two expressions will have the same sign, so I am just puzzled on where to go next with this problem, any help would be welcome! Messing around with basic values, I got $\pm 1, 0$ as the answers, although I have not checked for extra solutions, and do not know how to prove these, as it was just basic guessing and checking with the three most basic numbers for solving equations.","['algebra-precalculus', 'exponential-function', 'roots']"
3766043,Do Approximate Eigenvalues Imply Approximate Eigenvectors?,"My apologies in advance if this has already been asked somewhere. Suppose I have two real symmetric matrices $A$ and $B$ in $\mathbb{R}^{d \times d}$ for which $\lVert A - B \rVert_{op} \le \varepsilon$ . Further, call the eigenvalue-eigenvector pairs for $A$ and $B$ as $(\lambda_i, u_i)$ and $(\tau_i, v_i)$ , for all $i \in [d]$ , and suppose that $\lVert u_i \rVert_2 = \lVert v_i \rVert_2 = 1$ for all $i \in [d]$ . My question is: under what condition can we say something interesting about $\lVert u_i - v_i \rVert_2$ ? So far, I've tried using the following facts. For all $i$ , $\lvert \lambda_i - \tau_i \rvert \le \varepsilon$ . If $\lvert \lambda_i - \tau_i \rvert \le \varepsilon$ , then we can write $\lVert Bu_i - \lambda_i u_i \rVert \le \varepsilon$ (the reason I thought this might be useful is that it shows that the eigenvalue-eigenvector pairs for $A$ are almost eigenvalue-eigenvector pairs for $B$ , in some sense) I'm not sure where to go from here, or if I should be looking someplace else entirely. Thank you in advance for the help!","['symmetric-matrices', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'perturbation-theory']"
3766092,Alternative rank test for controllability (purely linear-algebra problem),"Given $A \in \mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$ , show that $$ \text{rank} \begin{bmatrix} (sI-A) \quad b \end{bmatrix} = n \quad \forall s \in \mathbb{C}
\quad 
\text{(the PBH criterion for controllability)}
$$ if and only if the only $n\times n$ matrix $X$ such that $AX=XA$ and $Xb\equiv0$ is $X \equiv 0$ . I can only go as far as follows (for the $\Rightarrow$ direction): Suppose $q$ is a left eigenvector of $A$ , i.e., $qA=\lambda q\:$ for some $\lambda$ . From the PBH rank test, there is no left eigenvector of $A$ that is orthogonal to $b$ , i.e., $qb\equiv0$ . Now, given an $X$ such that $AX=XA$ and $Xb\equiv0$ . Left multiply them by $q$ gives $qAX=\lambda qX=qXA$ and $qXb\equiv0$ , which shows that $qX$ is also a left eigenvector of $A$ with the same eigenvalue and $(qX)b\equiv0$ . So by the PBH rank test, we must have $qX\equiv0$ . This holds for all left eigenvectors of $A$ . If $A$ is diagonalizable, then we can find a set of left eigenvectors that forms a basis for $\mathbb{R}^n$ , and thus $X\equiv0$ . However, A may not be diagonalizable.","['matrix-rank', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'linear-control']"
3766100,Invariant $SU(3)$ subgroup for ${\bf 8}$ in ${\bf 3}^* \otimes {\bf 3} ={\bf 1} \oplus {\bf 8}$,"This question concerns finding an invariant subgroup of a total group $G$ . Warm-Up Toy Example (which I already solved) : Let us take $G=SU(2)$ as a special unitary group. Let us take $$u \text{ as } 
{\bf 2} \text{ of } G=SU(2)$$ has the fundamental representation ${\bf 2}$ (two complex components) of $G=SU(2)$ . There is also the complex conjugate representation $u^*$ , but in this case, ${\bf 2}^*={\bf 2}$ still. Now we can combine two fundamental representations to form $$
{\bf 2} \otimes {\bf 2} = {\bf 2}^*
\otimes {\bf 2} ={\bf 1} \oplus {\bf 3}.
$$ Where $$
{\bf 2} \otimes {\bf 2} = {\bf 2}^*
\otimes {\bf 2} =u^{*T} (n^j \sigma^j) u.
$$ Again $*$ is complex conjugation, and $T$ is the transpose.
with $\sigma^j$ of $j=0,1,2,3$ and https://en.wikipedia.org/wiki/Pauli_matrices $$
\sigma^0=\begin{pmatrix}
      1&0\\
      0&1
    \end{pmatrix},
\sigma^1=\begin{pmatrix}
      0&1\\
      1&0
    \end{pmatrix},
\sigma^2=\begin{pmatrix}
      0&-i\\
      i&0
    \end{pmatrix},
\sigma^3=\begin{pmatrix}
      1 &0\\
      0 &-1
    \end{pmatrix}.
$$ So for a fix $j$ , $u^{*T} (n^j \sigma^j) u$ outputs a scalar number. Here we can define a scalar (for some coefficient $n^0$ ) $$
{\bf 1} \text{ as } V^0 := u^{*T} (n^0 \sigma^0) u
$$ and a 3-vector  (for some coefficient $n^1,n^2,n^3$ ) $$
{\bf 3} \text{ as } V^j := \Big( u^{*T} (n^1 \sigma^1) u, \quad u^{*T} (n^2 \sigma^2) u, \quad u^{*T} (n^3 \sigma^3) u\Big)
$$ (warm-up 1) If we choose to look at ${\bf 1}$ as $V^0$ with a fixed $n^0$ coefficient, we can ask: What are the subgroup of $SU(2)$ which acts on $u \mapsto u'=\exp(i \theta^a \sigma^a) u$ makes the ${\bf 1}$ as $V^0:= u^{*T} (n^0 \sigma^0) u$ invariant? Answer : The full $SU(2)$ makes $V^0$ invariant. Since under $SU(2)$ transformation: $$V^0:= u^{*T} (n^0 \sigma^0) u  \mapsto
{V^0}'=
{u'}^{*T} (n^0 ) u' = u^{*T} \exp(-i \theta^a \sigma^a) 
(n^0 \sigma^0) \exp(i \theta^a \sigma^a) u 
=
u^{*T}  
(n^0 ) u = V^0$$ for arbitrary $\theta^j$ , thus the full $SU(2)$ .
In short, there is a fibration structure: $$
\text{stablizer $\hookrightarrow$ total $G$ $\to$ orbit}.
$$ From the $SU(2)$ view: $$
SU(2)\hookrightarrow SU(2) \to pt
$$ (warm-up 2) If we choose to look at ${\bf 3}$ as $V^j$ with a fixed coefficient $n^1,n^2,n^3$ coefficient, we can ask: What are the subgroup of $SU(2)$ which acts on $u \mapsto u'=\exp(i \theta^a \sigma^a) u$ makes the ${\bf 3}$ as $V^j := \big(u^{*T} (n^j \sigma^j) u\big)$ with $j=1,2,3$ invariant? Answer : Only the $U(1)$ subgroup of $SU(2)$ makes $V^j$ invariant. Since under $SU(2)$ transformation: $$V^j:= u^{*T} (n^j \sigma^j) u  \mapsto
{V^j}'=
{u'}^{*T} (n^j \sigma^j) u' = u^{*T} \exp(-i \theta^a \sigma^a) 
(n^j \sigma^j) \exp(i \theta^a \sigma^a) u. 
$$ The $V^j$ is a 3-vector on an $S^2$ sphere. while the $SU(2)$ acts on $u$ becomes effectively as $SO(3)$ acts on $V^j$ on the $S^2$ sphere. With a fixed coefficient $n^1,n^2,n^3$ coefficient,
only when the $SO(3)$ subgroup that makes the $V^j$ 3-vector on an $S^2$ sphere invariant would be the desired subgroup, which is the $U(1)$ subgroup. In short, there is a fibration structure: $$
\text{stablizer $\hookrightarrow$ total $G$ $\to$ orbit}.
$$ From the $SO(3)$ view: $$
U(1)\hookrightarrow SO(3) \to S^2
$$ From the $SU(2)$ view: $$
Spin(2)\hookrightarrow SU(2) \to S^2
$$ such that $Spin(2)/\mathbb{Z}_2=U(1)$ . Serious Puzzle : Let us take $G=SU(3)$ as a special unitary group. Let us take $$u \text{ as } 
{\bf 3} \text{ of } G=SU(3)$$ has the fundamental representation ${\bf 3}$ (three complex components) of $G=SU(3)$ . There is also the complex conjugate representation $u^*$ , but in this case, ${\bf 3}^*$ which is distinct from ${\bf 3}$ . Now we can combine two fundamental representations to form $$
{\bf 3}^* \otimes {\bf 3} ={\bf 1} \oplus {\bf 8}.
$$ We can take $$
{\bf 3}^*
\otimes {\bf 3} =u^{*T} (n^j \lambda_j) u.
$$ with the https://en.wikipedia.org/wiki/Gell-Mann_matrices : $\lambda_0 = \begin{pmatrix}   1 & 0 & 0 \\  0 &1  & 0 \\ 0 & 0 & 1 \end{pmatrix}$ , $\lambda_1 = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ , $\lambda_2 = \begin{pmatrix} 0 & -i & 0 \\ i & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ , $\lambda_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ , $\lambda_4 = \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix}$ , $\lambda_5 = \begin{pmatrix} 0 & 0 & -i \\ 0 & 0 & 0 \\ i & 0 & 0 \end{pmatrix}$ , $\lambda_6 = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix}$ , $\lambda_7 = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & -i \\ 0 & i & 0 \end{pmatrix}$ , $\lambda_8 = \frac{1}{\sqrt{3}} \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -2 \end{pmatrix}$ with $j=1,2,3, \dots, 8$ (warm-up 3) If we choose to look at ${\bf 1}$ as $V^0$ with a fixed $n^0$ coefficient, we can ask: What are the subgroup of $SU(3)$ which acts on $u \mapsto u'=\exp(i \theta^a \lambda_a) u$ makes the ${\bf 1}$ as $V^0:= u^{*T} (n^0 \lambda_0) u$ invariant? The answer shall be the full $SU(3)$ . (FINALY NOW Puzzle)If we choose to look at ${\bf 8}$ as $V^j$ with a fixed coefficient $n^1,n^2,n^3,\dots, n^8$ coefficient, we can ask: What are the subgroup of $SU(3)$ which acts on $u \mapsto u'=\exp(i \theta^a \lambda_a) u$ makes the ${\bf 8}$ as $V^j := \big(u^{*T} (n^j \lambda_j) u\big)$ with $j=1,2,3, \dots, 8$ invariant?","['representation-theory', 'abstract-algebra', 'invariant-theory', 'lie-groups', 'differential-geometry']"
3766125,What is the algebraic interpretation of a contracted product?,"Suppose you have a linear reductive group $G$ acting on an algebraic variety $X$ . Let $P\leq G$ be an algebraic subgroup and let $Y \subseteq X$ be a closed subvariety, invariant under $P$ . Then a number of texts define the construction of the so called ""contracted"" or ""twisted"" product give by $G \times^P Y := G \times Y /\sim \quad \quad$ where $(gp, y) \sim (g, py)$ for all $p \in P$ . My question is this: What is the algebraic interpretation of this geometric space and how should I be thinking about this scheme? In the case that it is affine, is there simple description of its coordinate ring? Also does anyone have any references for a good exposition of these concepts? If there were a more categorical description of this object that would also be amazing (i.e. a definition given by diagrams etc.). Thanks! Greatly Appreciated!","['algebraic-geometry', 'representation-theory', 'category-theory', 'sheaf-theory']"
3766129,Alternative formulations of the natural numbers,"Do there exist other ways to create the natural numbers, other than the definition given by $$0=\emptyset \\
x^+ = x \cup \{x\}$$ For example, one could also define the succesor operation as $$x^+ = \{x\}$$ Would both formulations lead to a set with the same basic properties? Its obvious that at least some properties would be lost, for example, the $\subset$ would no longer order the set, but its also clear that this set could still be ordered. In essence, my question is this: Is the typical definition of the natural numbers the only valid way to define the natural numbers in ZF? And if not, why do we choose that particular formulation of the natural numbers (i.e. what unique properties does it have)?","['elementary-set-theory', 'natural-numbers']"
3766146,Show that $\lim\limits_{N\rightarrow\infty}\sum\limits_{n=1}^N\frac{1}{N+n}=\int\limits_1^2 \frac{dx}{x}=\ln(2)$,"Show that: $$\lim\limits_{N\rightarrow\infty}\sum\limits_{n=1}^N\frac{1}{N+n}=\int\limits_1^2 \frac{dx}{x}=\ln(2)$$ My attempt: We build a Riemann sum with: $1=x_0<x_1<...<x_{N-1}<x_N=2$ $x_n:=\frac{n}{N}+1,\,\,\,n\in\mathbb{N}_0$ That gives us: $$\sum\limits_{n=1}^N(x_n-x_{n-1})\frac{1}{x_n}=\sum\limits_{n=1}^N \left(\frac{n}{N}+1-\left(\frac{n-1}{N}+1\right)\right)\frac{1}{\frac{n}{N}+1}=\sum\limits_{n=1}^N \frac{1}{N}\frac{N}{N+n}=\sum\limits_{n=1}^N\frac{1}{N+n}$$ We know from the definition, that: $$\lim\limits_{N\rightarrow\infty}\sum\limits_{n=1}^N\frac{1}{N+n}=\lim\limits_{N\rightarrow\infty}\sum\limits_{n=1}^N(x_n-x_{n-1})\frac{1}{x_n}=\int\limits_1^2 \frac{dx}{x}$$ Now we show that, $$\int\limits_1^2 \frac{dx}{x}=\ln(2)$$ First we choose another Rieman sum with: $1=x_0<x_1<...<x_{N-1}<x_N=2$ $x_n:=2^{\frac{n}{N}},\,\,\,n\in\mathbb{N}_0$ We get: $$\sum\limits_{n=1}^N(x_n-x_{n-1})\frac{1}{x_n}=\sum\limits_{n=1}^N\left(2^{\frac{n}{N}}-2^{\frac{n-1}{N}}\right)\frac{1}{2^{\frac{n-1}{N}}}=\sum\limits_{n=1}^N 2^{\frac{1}{N}}-1=N\left(2^{\frac{1}{N}}-1\right)$$ Since we know that (with $x \in \mathbb{R})$ : $$\lim\limits_{x\rightarrow0}\frac{2^x-1}{x}=\ln(2)\Longrightarrow \lim\limits_{x\rightarrow \infty}x(2^{\frac{1}{x}}-1)=\ln(2)\Longrightarrow \lim\limits_{N\rightarrow \infty}N(2^{\frac{1}{N}}-1)=\ln(2)$$ We get: $$\ln(2)=\lim\limits_{N\rightarrow \infty}N(2^{\frac{1}{N}}-1)=\lim\limits_{N\rightarrow \infty}\sum\limits_{n=1}^N\left(2^{\frac{n}{N}}-2^{\frac{n-1}{N}}\right)\frac{1}{2^{\frac{n-1}{N}}}=\int\limits_1^2 \frac{dx}{x}=\lim\limits_{N\rightarrow\infty}\sum\limits_{n=1}^N\frac{1}{N+n}$$ $\Box$ Hey it would be great, if someone could check my reasoning (if its correct) and give me feedback and tips :)","['integration', 'limits', 'solution-verification', 'riemann-sum']"
3766148,Guided Integration Question,"Part 1 is easy, but struggling to see how it helps with Part 2. Part 1 Find $\frac{dy}{dx}$ if $y=xe^{-x}$ . Part 2 Hence show that $\int xe^{-x}dx=-xe^{-x}-e^{-x}+c$ .","['integration', 'derivatives']"
3766150,"Simple proof of: if $ax\equiv ay \pmod{m}$, and $\gcd(a,m)=1$, then $x\equiv y$","I'm working on a proof of: ""if $ax\equiv ay \pmod{m}$ , and $\gcd(a,m)=1$ , then $x\equiv y\pmod{m}$ "". Here's what I have so far: Suppose $ax\equiv ay\pmod{m}$ , and $\gcd(a,m)=1$ By definition, $ax = ay + mp$ for some $p\in\mathbb{Z}$ By definition, $ay = ax + mr$ for some $r\in\mathbb{Z}$ By Bezout's identity, it must be that $\gcd(a,m) = ax$ Similarly, it must be that $\gcd(a,m) = ay$ Therefore, $ax = ay$ Obviously, $x=y$ Q.E.D. Is this ok?","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3766154,Residue Theorem Integral,"Studying for qualifying exams, I came across the following problem: using complex analysis, compute $$\int_{-\infty}^{\infty}\frac{x^2\sin(\pi x)}{x^3-1}dx
$$ I decided to use the integrand $f(z)=\frac{z^2e^{i\pi z}}{z^3-1}$ (the goal is to take the imaginary part in the end), and I found that $f$ has a removable singularity at $1$ . Now, it seems to me that $|f(Re^{i\theta})|=O(e^R/R)$ , so a semicircle countour won't work. I also tried to use a rectangular contour (with height $2\pi$ ) and the sides do vanish as $R\to\infty$ , but since there are quadratic terms in the integrand, I am not able to get a simple result and conclude via the residue theorem. Any ideas? Edit: With help from the comment, I think I have a solution. Taking $f(z)$ as before, we see that $f(z)=e^{i\pi z}\cdot g(z)$ , where $g(z)=\frac{z^2}{z^3-1}$ , and since $|g(Re^{i\theta})|\leq \frac{C}{R}$ , we invoke Jordan's lemma to say that $\lim_{R\to\infty}\int_{\Gamma_R}f(z)=0$ , where $\Gamma_R$ is the upper semi-circle of radius $R$ centered at $0$ . Therefore, we get: $$\int_{-\infty}^{\infty}\frac{xe^{i\pi x}}{x^3-1}=2\pi i Res_{e^{2\pi i/3}}f
$$ Now, $$Res_{e^{2\pi i/3}}f=\lim_{z\to e^{2\pi i/3}}\frac{ze^{i\pi z}}{(z-1)(z-e^{4i\pi /3})}=\frac{e^{2\pi i/3}exp\{i\pi e^{2\pi i/3}\}}{(e^{2\pi i/3}-1)(e^{2\pi i/3}-e^{4\pi i/3})}
$$ This will simplify (using the fact that the roots of unity sum to $0$ ) to $exp\{i\pi( e^{2\pi i/3}-2/3)\}$ Now, $Im\{2\pi i \exp\{i\pi( e^{2\pi i/3}-2/3)\}\}=2\pi e^{-\pi \sin(2\pi/3)}\cos[\cos(2\pi/3)-2/3]$ . I don't think this should be the answer, but I am unable to figure out where I went wrong.","['complex-analysis', 'residue-calculus']"
3766220,What is exponential map in differential geometry,"Is $\exp_{q}(v)$ a projection of point $q$ to some point $q'$ along the geodesic whose tangent (right?) at $q$ is the vector $v$ ? And so $\exp_{q}(v)$ is the projection of point $q$ to some point along the geodesic between $q$ and $q'$ ? (Another post gives an explanation: Riemannian geometry: ...Why is it called 'Exponential' map? So now I'm wondering how we know where $q$ exactly falls on the geodesic after it travels for a unit amount of time. Does it uniquely depend on $p, v, M$ only, is it affected by any other parameters as well, or is it arbitrarily set to any point in the geodesic?) The reason that it is called exponential map seems to be that the function satisfy that two images' multiplication $\exp_{q}(v_1)\exp_{q}(v_2)$ equals the image of the two independent variables' addition (to some degree)? But that simply means a exponential map is sort of (inexact) homomorphism. Is there any other reasons for this naming? (To make things clearer, what's said above is about exponential maps of manifolds, and what's said below is mainly about exponential maps of Lie groups. And I somehow 'apply' the theory of exponential maps of Lie group to exponential maps of Riemann manifold (for I thought they were 'consistent' with each other). What I tried to do by experimenting with these concepts and notations is not only to understand each of the two exponential maps, but to connect the two concepts, to make them consistent, or to find the relation or similarity between the two concepts. Now it seems I should try to look at the difference between the two concepts as well.) It seems that, according to p.388 of Spivak's Diff Geom, $\exp_{q}(v_1)\exp_{q}(v_2)=\exp_{q}((v_1+v_2)+[v_1, v_2]+...)$ , where $[\ ,\ ]$ is a bilinear function in Lie algebra (I don't know exactly what Lie algebra is, but I guess for tangent vectors $v_1, v_2$ it is (or can be) inner product, or perhaps more generally, a 2-tensor product (mapping two vectors to a number) (length) times a unit vector (direction)). It seems $[v_1, v_2]$ 'measures' the difference between $\exp_{q}(v_1)\exp_{q}(v_2)$ and $\exp_{q}(v_1+v_2)$ to the first order, so I guess it plays a role similar to one that first order derivative $/1!$ plays in function's expansion into power series. With such comparison of $[v_1, v_2]$ and 2-tensor product, and of $[v_1, v_2]$ and first order derivatives, perhaps $\exp_{q}(v_1)\exp_{q}(v_2)=\exp_{q}((v_1+v_2)+[v_1, v_2]+ T_3\cdot e_3+T_4\cdot e_4+...)$ , where $T_i$ is $i$ -tensor product (length) times a unit vector $e_i$ (direction) and where $T_i$ is similar to $i$ th derivatives $/i!$ and measures the difference to the $i$ th order. (According to the wiki articles https://en.wikipedia.org/wiki/Exponential_map_(Lie_theory) mentioned in the answers to the above post, it seems $\exp_{q}(v))$ does have an power series expansion quite similar to that of $e^x$ , and possibly $T_i\cdot e_i$ can, in some cases, written as an extension of $[\ , \ ]$ , e.g. $[v_1,[v_1,v_2]]$ so that $T_i$ is $i$ -tensor product but remains a function of two variables $v_1,v_2$ .) Besides, if so we have $\exp_{q}(tv_1)\exp_{q}(tv_2)=\exp_{q}(t(v_1+v_2)+t^2[v_1, v_2]+ t^3T_3\cdot e_3+t^4T_4\cdot e_4+...)$ . I'm not sure if my understanding is roughly correct.","['riemannian-geometry', 'differential-geometry']"
3766226,Question on cardinality of sets,"Let $a=\bigcup_{i\in I_a}a_i$ and $b=\bigcup_{j\in I_b}b_j$ where $I_a$ is the index set of $a$ and $I_b$ is the index set of $b$ , such that $\forall i\in I_a:a_i$ is a finite set $\forall j\in I_b:b_j$ is a finite set $a_k\cap a_m= b_l\cap b_n=\emptyset$ if $k\neq m$ and $l\neq n$ $\left |a_i \right |\ge\left |b_j \right |$ for every $i\in I_a$ and $j\in I_b$ $\left |I_a \right |>\left |I_b \right |$ . Is $\left |a \right |>\left |b \right|$ ?",['elementary-set-theory']
3766305,"Simple proof there is no continuous bijection from $\mathbb{R}^n$ onto $[0,1]^m$","This is inspired by this question . Using invariance of domain or some such theorem, it is easy to prove there is no continuous bijection $f:\mathbb{R}^n\to [0,1]^m$ for any $n,m\in\mathbb{N}$ . Otherwise, composing it with the inclusion in $\mathbb{R}^m$ , we would get that $[0,1]^m$ must be open in $\mathbb{R}^m$ . However, I imagine there must be a much simpler proof using simpler properties of the cube and the Euclidean space. For instance, it's very easy to prove this for $m=1$ and any $n$ , as in the question I linked to. Any ideas? EDIT . Two answers have been so far suggested, but Kevin's is incomplete, and I believe the other is incorrect.","['alternative-proof', 'continuity', 'general-topology']"
3766349,Local Galois group action on étale cohomology,"Consider a scheme $X$ over $\mathbb{Q}$ . By the base change theorem for the étale cohomology, we have $$H^i_{ét}(X_{\bar{\mathbb{Q}}},\mathbb{Z}/\ell^n)\cong H^i_{ét}(X_{\mathbb{C}_p},\mathbb{Z}/\ell^n)$$ in a natural way and thus passing to the limit over $n$ , we get an isomorphism of $\mathbb{Z}_\ell$ -modues $$H^i_{ét}(X_{\bar{\mathbb{Q}}},\mathbb{Z}_\ell)\cong H^i_{ét}(X_{\mathbb{C}_p},\mathbb{Z}_\ell)$$ which yields after tensoring to $$(*) \quad H^i_{ét}(X_{\bar{\mathbb{Q}}},\mathbb{Q}_\ell)\cong H^i_{ét}(X_{\mathbb{C}_p},\mathbb{Q}_\ell).$$ On the vector space on the left hand side, there is a natural action of the absolute galois group $G_{\mathbb{Q}}$ , where as on the righthand side the local absolute galois group acts, i.e. $G_{\mathbb{Q}_p}$ . Since we have inclusion $G_{\mathbb{Q}_p}\rightarrow G_{\mathbb{Q}}$ , it seems natural to me that $G_{\mathbb{Q}_p}$ acts on $H^i_{ét}(X_{\bar{\mathbb{Q}}},\mathbb{Q}_\ell)$ . Does $G_{\mathbb{Q}}$ act on $H^i_{ét}(X_{\mathbb{C}_p},\mathbb{Q}_\ell)$ making $(*)$ an isomorphism of $G_{\mathbb{Q}}$ -modules compatible with the $G_{\mathbb{Q}_p}$ -action?","['etale-cohomology', 'algebraic-number-theory', 'algebraic-geometry', 'galois-representations']"
3766352,The importance and applications of order of a group?,"Recently, I'm exposed to some exercises and theorems concerning order of a group. For example, If an abelian group has subgroups of orders $m$ and $n$, respectively, then it has a subgroup whose order is $\operatorname{lcm}(m,n)$. A simple proof of Sylow theorem for abelian groups If a finite group $G$ of order $n$ has at most one subgroup of each order $d|n$, then $G$ is cyclic IMHO, the classic result of this kind is Sylow theorems that appear in most standard textbooks about abstract algebra. As such, I would like to ask about the importance of order of a group in abstract algebra and its applications in other branches of mathemactics. Thank you for your elaboration!","['group-theory', 'abstract-algebra', 'sylow-theory']"
3766465,Solve $\lfloor \ln x \rfloor \gt \ln \lfloor x\rfloor$,"The question requires finding all real values of $x$ for which $$\lfloor \ln x\rfloor \gt \ln\lfloor x\rfloor $$ To start off, one could note that $$\lfloor \ln x \rfloor =\begin{cases} 0,& x\in[1,e) \\ 1,& x\in[e,e^2) \\ 2, &x\in [e^2,e^3) \\ 3,& x\in [e^3,e^4) \\ \vdots \end{cases}$$ and $$\ln\lfloor x\rfloor =\begin{cases} 0, &x\in[1,2) \\ \ln 2, &x\in [2,3) \\ \ln 3,& x\in[3,4) \\ \ln 4,& x\in[4,5) \\ \vdots \end{cases}$$ Although, from here it is not exactly clear to me how I can proceed. It appeas to me that there are infinitely many intervals of $x$ for which this inequality is true, but how can I find a generalized form of such an interval? E.g. something of the form $x\in \big(f(k), g(k)\big)$ for $k\in\mathbb N$ ?","['algebra-precalculus', 'ceiling-and-floor-functions', 'logarithms']"
3766485,Remainder when $^{40}C_{12}$ is divided by $7$.,"I was solving a binomial summation problem, and I got $^{40}C_{12}$ as the answer. Now, the question demands to find the remainder when it is divided by $7$ . $40!$ can be divided $5$ times by $7$ (using a famous GIF trick.) Unfortunately, $28!$ and $12!$ can be divided $4$ and $1$ time respectively by $7$ , and hence the answer is clearly not zero. I am hence unable to find a way to deduce the remainder (without using a calculator to calculate $^{40}C_{12}$ ).","['number-theory', 'binomial-theorem', 'divisibility']"
3766523,"Describing the set $\{n\in \Bbb N\lvert (n>1)\wedge (\forall x,y\in \Bbb N)[(xy=n)\implies (x=1\lor y=1)]\}$","This set appears in an exercise of the course Introduction to Mathematical Thinking , by Dr.Keith Devlin $A$ = $\{n\in \Bbb N\lvert (n>1)\wedge (\forall x,y\in \Bbb N)[(xy=n)\implies (x=1\lor y=1)]\}$ Although this is the very first question of the exercise on a refresher to Set Theory, I cannot process the logical syntax as it is. I'll break down my approach to understanding this set: The domain of discourse is the set of natural numbers(the convention followed in the course is to exclude $0$ from $\Bbb N$ ). The set $A$ , first and foremost, requires $n$ to be greater than $1$ . Next, $A$ requires $n$ to satisfy the implication $(xy=n)\implies (x=1\lor y=1)$ for every $x, y$ chosen from $\Bbb N$ (this is precisely where I feel I might be wrong). Clearly, in the event of both $x,y$ being greater than 1 the entire implication will be falsified. Thus, there does not exist any $n\in \Bbb N$ satisfying the conditions required by $A$ , which means $A$ is an empty set. I'd like to know which part of my reasoning is incorrect(if any at all). I believe such case-by-case splitting may not be necessary for trivial examples like this one (at least for math students and professionals), but I'm a layman and would hence appreciate such an approach.","['elementary-set-theory', 'solution-verification', 'predicate-logic']"
3766528,Find equation of line(s) bisecting perimeter and area of triangle formed by $xy$-axes and $6x+8y=48$,"The line $6x+8y=48$ intersects the $x$ -axis in point $A$ and the $y$ -axis in point $B$ . A line $L$ bisects the area and the perimeter of the triangle $OAB$ ,where $O$ is origin. Find possible equation(s) of $L$ . My Attempt I feel there can be three lines but how to proceed.","['trigonometry', 'algebra-precalculus', 'geometry', 'analytic-geometry']"
3766532,Prove the sum of $\sqrt{3} + \sqrt[3]{4}$ is Irrational number,"I really don't know how to prove it.
I can prove that $\sqrt[3]{4}$ is irrational and prove that $\sqrt{3}$ is irrational.
but as we know , sum of 2 irrational can be irrational or rational ( $\sqrt{2} + -\sqrt{2} =$ rational).
so I tried to prove that the sum of 2 different positive irrational numbers is always irrational but also failed. anyone know?",['discrete-mathematics']
3766578,How to simplify expressions with del (or nabla) in them?,"I always find it difficult to simplify expressions or open brackets in expressions that have a 'Del' (or 'Nabla') in them. For example, how would one go about simplifying this expression?: $$\nabla\boldsymbol{\cdot}(\phi\nabla\psi)$$ ( $\phi$ and $\psi$ are both scalar fields) I need it to become: $$[\phi\nabla^2\psi + (\nabla\phi)\boldsymbol{\cdot}(\nabla\psi)]$$ I would also love to know how to simplify those standard equations mentioned in Griffiths (for example - the expansion of the 'curl of the curl' of a vector field) The only method I know is to find out every single term in the expression (in terms of $a_x$ , $a_y$ etc.) and then cancel out the terms and then find patterns and regroup the terms in the remaining expression Is there a faster way to approach these 'simplify' (or 'expand') problems? Maybe there are some tricks or formulas that I am unaware of (maybe something analogous to the uv-rule for differentiating the product of two functions in simple calculus) $$\frac{d}{dx}[u(x)v(x)] = u'(x)v(x) + u(x)v'(x)$$ I understand that the uv-rule seems to work on my original expression. But I would still love some sort of formalization. The problem I have is that, in simple calculus, multiplying two functions does not have two meanings. With Nabla however, I have two choices - Dot product and Cross product . And I also have three choices for differentiation - Gradient, Divergence and Curl To explain my concern better, try answering what would have been the simplification if the original expression was - $$\nabla \times (\phi\nabla\psi)$$ or maybe $$\nabla(v\boldsymbol{\cdot}\nabla\psi)$$ where $v$ is a vector-field For the analogy, these three questions become the same question - "" Differentiation of something multiplied by the differential of something else ""","['divergence-operator', 'multivariable-calculus', 'vector-analysis']"
3766581,Recursive on Gray Code,"I have been curious since working on gray code that how can I write recursive function to calculate the integer that appear in position m of n-bit (as show in the link that I provide) like 3-bit Gray Code at position 5 is 7. https://commons.wikimedia.org/wiki/File:Binary-reflected_Gray_code_construction.svg Figure: The Reflect and Prefix Method. General Information Gray Code is an ordering of the binary numeral system such that two successive values differ in only one bit (binary digit). For example, gray code of 3 digits (and its decimal) are listed as follows: 000 = 0,
001 = 1,
011 = 3,
010 = 2, and vice versa which is simple binary number.","['gray-code', 'discrete-mathematics']"
3766592,Writing out a Maple-integrated Function does not work as intended,"I think I have a basic problem with how Maple handles functions. Here's what I do: I used the Function KummerM ( https://de.maplesoft.com/support/help/Maple/view.aspx?path=Kummer ) without any problems, but for specific reasons I need to use the integral representation: $$
M(a, b, z) =  \frac{\Gamma(b)}{\Gamma(a)\Gamma(b-a)} \int_0^1 e^{zu} u^{a-1} (1-u)^{b-a-1}\, du ~~~~~ (1)
$$ It turns out that Maple doesn't do what I want: > evalb(GAMMA(b)*(int(u^(a-1)*(1-u)^(b-a-1)*e^(z*u), u = 0 .. 1))/(GAMMA(a)*GAMMA(b-a)) = KummerM(a, b, z));
                             false

KummerMint := (a, b, z) ->  GAMMA(b)*(int(u^(a-1)*(1-u)^(b-a-1)*e^(z*u), u = 0 .. 1))/(GAMMA(a)*GAMMA(b-a));


> evalb(KummerMint(1, 2, 1) = Malt(1, 2, 1));
                             false Meaning that $(1)$ is not true :-/ Does anyone see the error? Much appreciated! bm","['integration', 'functions', 'maple', 'gamma-function']"
3766601,Proof Check: Multiplication operator on $L^{p}(\mathbb R^{d})$ is closed,"Let $m : \mathbb R^{d} \to \mathbb K$ measurable and $A: \operatorname{dom}(A) \to L^{p}(\mathbb R^{d})$ such that $Af(x)=m(x)f(x)$ and $\operatorname{dom}(A):=\{f \in L^{p}(\mathbb R^{d}):mf \in L^{p}(\mathbb R^{d})\}$ . Show that $A$ is a closed operator. My proof: Let $(f_{n})_{n \in \mathbb N}\subseteq \operatorname{dom}(A)$ with $f_{n} \xrightarrow{L^{p}} f$ and $Af_{n} \xrightarrow{ L^{p}} z$ . Then by $L^{p}$ convergence there exists a measurable $A_{k}$ and a subsequence $(f_{n(k)})_{k\in \mathbb N}$ such that $\mu(A_{k})=0$ and $f_{n(k)}(x)\xrightarrow{k \to \infty} f(x),\; \forall x \in A_{k}^{c}$ . Now we want to show that $mf=z$ a.e., thus consider $$\lvert \lvert mf-z\rvert\rvert_{p}^{p}\\=\int\limits_{\mathbb R^{d}}\lvert m(x)f(x)-z(x)\rvert^{p}dx\\=\int\limits_{A_{k}^{c}}\lvert m(x)f(x)-z(x)\rvert^{p}dx\\=\int\limits_{A_{k}^{c}}\lim\limits_{k\to \infty}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\ =\int\limits_{A_{k}^{c}}\liminf\limits_{k\to \infty}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\ \leq \liminf\limits_{k\to \infty} \int\limits_{A_{k}^{c}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx\\\leq \liminf\limits_{k\to \infty} \int\limits_{\mathbb R^{d}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx \\ =\lim\limits_{k\to \infty} \int\limits_{\mathbb R^{d}}\lvert m(x)f_{n(k)}(x)-z(x)\rvert^{p}dx=0.$$ Thus $mf = z$ a.e. and since $z \in L^{p}$ , it follows that $f \in \operatorname{dom}(T)$ and hence $A$ is closed. The critical part of my proof was to use Fatou's Lemma. Are there any other alternative ways to prove this and is my proof at all correct?","['integration', 'measure-theory', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3766632,Another extension of mutual information to multiple variables,"The mutual information can be expressed as $$
I(X;Y) = H(X) + H(Y) - H(X, Y)
$$ And now I encounter the following expression, which seems to be an extension of mutual information: $$
F(X_1;\cdots;X_N) = \sum_{i=1}^N H(X_i) - H(X_1, X_2, \cdots, X_N)
$$ However, I know the definition of multivariate mutual information, where $$
I(X_1;\cdots;X_N) = -\sum _{T\subseteq \{X_1,\ldots ,X_N\}}(-1)^{|T|}H(T)
$$ I have two questions, Why $I$ is the commonly used extension of mutual information, rather than $F$ ?
From my view, $F$ is more like an ""information gain"" because $F$ is non-negative.  And $I$ can be positive and negative, confusing me what it stands for. Is there any interpretation of $F$ ? Or any studies about properties? I didn't have much knowledge about information theory, and I appreciate any kindly help.","['statistics', 'entropy', 'probability', 'information-theory']"
3766654,"Determine whether or not the following structure $(P,S,1)$ is a Peano System","First this is how the book define as a Peano System. By a Peano System we mean a set $P$ , a particular element $1$ in $P$ , and a singulary operation $S$ on $P$ such that the following axioms are satisfied. (P1) $1$ is not the successor $S(x)$ of any object $x$ in $P$ . In Symbols: $$(\forall x)(S(x) \neq 1)$$ (P2) Different objects in $P$ have different successors. This can be formulated as follows: $$(\forall x)(\forall y)(x \neq y \Rightarrow S(x) \neq S(y))$$ (P3) Principle of mathematical induction: Any subset of $P$ containing $1$ and closed under $S$ must be identical with $P$ . This can be symbolically rendered as follows: $$(\forall B)([B \subseteq P \land 1 \in B \land (\forall x)(x \in B \Rightarrow S(x) \in B)] \Rightarrow P=B)$$ Such a Peano System will be denoted by the ordered triple (P,S,1): $P$ is called underlying set , $S$ the successor operation , and $1$ the distinguished element . Now the question: Determine wheter or not the following structure (P,S,1) are Peano Systems. a. $P$ is the set of all integers greater than $9$ , "" $1$ "" stands for the integer $10$ . $S(u)=u+1$ for any $u$ in $P$ . My atempt: $\checkmark$ (P1): Its clear that for any $x$ in $P$ we have that $x>9$ thus $(\forall x)(x+1 \neq 10)$ holds true $\checkmark$ (P2): For any $x$ and $y$ in $P$ we have that $x \neq y \Rightarrow x+1 \neq y+1$ ? (P3): begin edit 1 First I assume some subset $B$ of $P$ , such $10 \in B$ and $B$ is closed under $S$ , from this we have that $(\forall x)(x \in B \Rightarrow S(x) \in B)$ But as $10 \in B$ we have that $S(10) \in B$ also, and then $S(S(10)) \in B$ too then $S(S(S(10))) \in B$ ..., from this I think its possible to ""see"" that every integer greater than 9 are in $B$ also. But I think its possible to state this in a better way. For every integer $x$ greater than $9$ which is different from $10$ , we have that $(\exists !y)(x = S(y))$ , thus if $y \neq 10$ , we will have that $(\exists !z)(y = S(z))$ and the same proccess can be repeated until we reach the application of $S$ to $10$ , let $z=10$ , thus we will have $x = S(S(10))$ which in fact is the integer 12, thus we needed to apply $S$ , $x-10$ times to obtain the integer $x$ , then if some $x \in P$ , we know that applying $S$ in $10$ , $x-10$ times will be the same integer, and therefore $x \in B$ because $B$ is closed under $S$ , thus $P \subseteq B$ . From this we have that $P=B$ thus (P3) holds true from $(P,S,1)$ end edit 1 I dont know what is needed to prove or conclude that the (P3) holds true $(P,S,1)$ , in some other exercises its clear that the system dont hold true for (P1) or (P2) and therefore is not a Peano System, but when (P1) and (P2) holds true I dont know how to continue. Thus I need help to understand how (P3) can be proved, I have looked at other questions concerning Peano System and Mathematical Induction axiom, some people says this cannot proved, If it is correct I should just assume (P3) holds true?","['elementary-set-theory', 'solution-verification', 'natural-numbers', 'peano-axioms']"
3766660,Tangent bundle of the incidence variety and its Chern class,"I am trying to learn about Thom polynomials and I often find arguments in the literature that just make no real sense to me. Maybe it is due to my lack of knowledge in $K$ - theory. I apologize for the lengthy post, but maybe someone knows these things well. Let $P := \mathbb{C}P^n$ , $N := \mathbb{C}P^{n*}$ and set $X := P\times N$ . Denote by $p_1:X\rightarrow P$ and $p_2: X\rightarrow N$ the natural projections. Let $V = \mathbb{V}(F)\subset P$ be a hypersurface of degree $d$ and $H = \{(x,a^*)\in P\times N:\text{ }a^*(x) = 0\}$ be the incidence variety of points and lines in $P$ . In fact, $H$ can be realized as the zero-set (zero-scheme) of a section of the line bundle $p_1^*\mathcal{O}_P(1)\otimes p_2^*\mathcal{O}_N(1)$ over $X$ . Also, the polynomial $F$ defines a section of the line bundle $p_1^*\mathcal{O}_P(d)$ over $X$ . Hence, there is an induced section of the rank $2$ vector bundle $E =p_1^*\mathcal{O}_P(d)\oplus (p_1^*\mathcal{O}_P(1)\otimes p_2^*\mathcal{O}_N(1))$ over $X$ . Its zero-set (zero-scheme) is the subvariety $M :=\{ (x,a^*)\in V\times N:\text{ }a^*(x) =0\}$ , i.e. the incidence variety of points in $V$ and lines in $P$ . Define $f:M\rightarrow N$ by composing $p_2:X\rightarrow N$ with the inclusion $i:M\hookrightarrow X$ , i.e. $f = p_2\circ i$ . I am interested in computing the total Chern class $c(f^*TN-TM)$ , where $f^*TM-TN$ is the virtual bundle living in the $K$ - group of $M$ . The claim is: $$TM = i^*(TX-E)\text{ and hence } f^{*}TN - TM = i^{*}(E - p_1^*TP).$$ It appears to me that these are just standard calculations in $K$ - groups but I don't get it.","['k-theory', 'algebraic-geometry', 'topological-k-theory', 'intersection-theory', 'algebraic-topology']"
3766683,How can someone reject a math result if everything has to be proved?,"I'm reading a book on axiomatic set theory, classic Set Theory: For Guided Independent Study, and at the beginning of chapter 4 it says: So far in this book we have given the impression that sets are needed to help explain the important number systems on which so much of mathematics (and the science that exploits mathematics) is based. Dedekind's construction of the real numbers, along with the associated axioms for the reals, completes the process of putting the calculus (and much more) on a rigorous footing. and then it says: It is important to realize that there are schools of mathematics that would reject 'standard' real analysis and, along with it, Dedekind's work. How is it possible that ""schools of mathematics"" reject standard real analysis and Dedekind's work? I don't know if I'm misinterpreting things but, how can people reject a whole branch of mathematics if everything has to be proved to be called a theorem and cannot be disproved unless a logical mistake is found? I've even watched this video in the past: https://www.youtube.com/watch?reload=9&v=jlnBo3APRlU and this guy, who's supposed to be a teacher, says that real numbers don't exist and that they are only rational numbers. I don't know if this is a related problem but how is this possible?","['foundations', 'logic', 'real-analysis', 'philosophy', 'set-theory']"
3766719,"Show total differentiability of $\phi: \mathbb R^n \to \mathbb R^n, x \mapsto \varphi(\lVert x\rVert_2) x$ where $\varphi$ is differentiable","Let $\varphi$ be differentiable. Show that $$\phi: \mathbb R^n \to \mathbb R^n, x \mapsto \varphi(\lVert x\rVert_2) x$$ is (total) differentiable where $x \neq 0$ . How can I show this? I know that $\varphi(\lVert x \rVert_2)$ is differentiable by the chain rule but I don't know any ""multidimensional product rule"". How can I show differentiability instead?","['derivatives', 'chain-rule']"
3766739,Remainder of $15^{81}$ divided by $13$ without using Fermat's Little theorem.,"I was requested to find the congruence of $15^{81}\mod{13}$ without using Fermat's theorem (since that is covered in the chapter that follows this exercise). Of course I know that by property $15^{81} \equiv 2^{81} \pmod{13}$ , but how could I find what is the congruence of $2^{81}$ without using Fermat? Needless it is to say that an exhaustive iterative method would be extremely long.","['elementary-number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3766767,Is there a simple-ish function for modeling seasonal changes to day/night duration and height of the sun?,"I'm a hobbyist programmer, and not much of a mathematician. I'm trying to model something like the seasonal change in day length. There are two other questions here that are very similar to mine, and I posted a bounty for one of them, but the answers are over my head, and I don't think I can adapt them to what I'm doing. I was thinking more something like a sine-ish function, and hoping for some easier math. Perhaps if I show my specific case, the answers can be narrowed and simplified. What I've been able to come up with is a function getSunHeight(x, cycleDuration, dayToNightRatio) . (It's not for Earth; I'm experimenting with different values in a simulation, so a 24-hour cycle isn't a given.) In mathematical terms, getSunHeight is calculated as follows. Let $d_{\text{cycle}}$ denote the duration of a full cycle and $r_\text{day-to-night}$ denote the ratio of day to night. Let $$d_\text{daylight} = d_\text{cycle} \times r_\text{day-to-night}$$ and $$d_\text{darkness}= d_\text{cycle} - d_\text{daylight}$$ Then the sun height is $$y(x)=\left\{
\begin{array}{lcl}
\sin\left(\frac{\pi x}{d_\text{daylight}}\right) & : & 0\le x\le d_\text{daylight}\\
\sin\left(\frac{\pi\left(x-d_\text{cycle}\right)}{d_\text{darkness}}\right) & : & d_\text{daylight} < x \le d_\text{cycle}
\end{array}
\right.$$ So $y=\operatorname{getSunHeight}(x, 10, 0.2)$ gives me a graph like this: Is there some way to get rid of the hard angle at $x=d_\text{daylight}$ (i.e. daylightDuration )? It's not a problem if the shape of the curve changes slightly; in fact it might be better, more realistic. Also, I'm not trying for a general case where I specify the latitude. I'm looking for something that assumes I'm at a fixed latitude. Further, although I'm trying to model a change in the period, I'm not particularly attached to that approach. It was suggested that I try to vary the height of the sun and keep the period the same. After lots of experimentation on Desmos, I'm still at a loss. I've been experimenting with averaging the slopes at that discontinuity, and using that average somewhere in the equation, but I haven't been able to make any headway. News: With inspiration from the comments, I've finally realized that I need to think about the entire winter/summer cycle, not just one day/night cycle. I think I almost have it solved: Let $d_{\text{annualCycle}}$ denote the duration of a full summer/winter cycle, expressed in full day/night cycles Let $d_{\text{diurnalCycle}}$ denote the duration of a full day/night cycle Let $d_{\text{daylight}}$ denote the duration of daylight for one day/night cycle Let $d_{\text{darkness}}$ denote the duration of darkness for one day/night cycle Let $r_{\text{day-to-night}}$ denote $d_{\text{daylight}}:d_{\text{diurnalCycle}}$ at the first solstice! At the second solstice, the ratio is 1 - $r_{\text{day-to-night}}$ , and at the equinoxes, the day/night ratio is 1:1 (d'oh!) Finally, rather than thinking of the sun's height , with all that angle stuff, I'll think of the function as a kind of temperature reading. So with a function y = getTemperature(x, $d_{\text{diurnalCycle}}$ , $d_{\text{annualCycle}}$ , $r_{\text{day-to-night}}$ ) I've come up with this: Let yearFullDuration = $d_{\text{annualCycle}} \ x \ d_{\text{diurnalCycle}}$ Let $r_{\text{night-to-day}} = 1 - r_{\text{day-to-night}}$ Let $c=\left(r_{\text{night-to-day}}-r_{\text{day-to-night}}\right)\sin\left(\frac{2\pi r_{\text{night-to-day}}}{d_{\text{diurnalCycle}} r_{\text{day-to-night}}}\right)+r_{\text{night-to-day}}$ $y = \sin\left(\frac{2\pi xd_{\text{diurnalCycle}}}{\text{yearFullDuration}}\right) + \sin\left(\frac{1.3 cxr_{\text{night-to-day}}}{\text{yearFullDuration}}\right)$ It gives me a graph like the following. As you can see, the zeros don't land quite where they're supposed to. I put in a fudge factor of 1.3, which is incredibly unsatisfying, but I haven't yet figured out how to the crossings right. More News: Again, with much inspiration and help from the comments, I've figured out the easier case of just adding the seasonal sine to the diurnal sine. The thing that was eluding me--the reason for the fudge factor of 1.3--was the need to square one of the ratios in the seasonal sine: Let $d_{\text{diurnal}}$ denote the duration of one day/night cycle Let $d_{\text{annual}}$ denote the number of full diurnal cycles in one summer/winter cycle Let $d_{\text{full-year}}=d_{\text{annual}}*d_{\text{diurnal}}$ Let $r_{s}$ denote the ratio of daylight duration to $d_{diurnal}$ at the summer (first) solstice Let $f_{a}=\sin\left(\frac{2xr_{s}^{2}}{d_{\text{full-year}}}\right)$ -- the annual curve Let $f_{d}=\sin\left(\frac{2\pi xd_{\text{diurnal}}}{d_{\text{full-year}}}\right)$ -- the diurnal curve And finally $y=\frac{1}2\sin\left(f_{a}+f_{d}\right)$ The graph comes out looking like one might expect if one were more math-oriented. I'm still very curious to see whether there's a way to smoothly vary the daylight/darkness ratio as the seasons progress (my original idea, extended over the course of a year rather than just one day). I've been all over that one and not made any progress.","['continuity', 'trigonometry', 'functions', 'mathematical-modeling']"
3766779,"What is the smallest integer $n>1$ for which the mean of the square numbers $1^2,2^2 \dots,n^2 $ is a perfect square?","What is the smallest integer $n>1$ for which the mean of the square numbers $1^2,2^2 \dots,n^2 $ is a perfect square? Initially, this seemed like one could work it out with $AM-GM$ , but it doesn't seem so. From $AM-GM$ one gets that $$\frac{1^2+2^2+ \dots+n^2}{n} \geqslant \sqrt[\leftroot{-1}\uproot{2}n]{1^2\cdot2^2\dots\cdot n^2}
$$ is this of any help here? Remark. Thanks to Favst , the source of the problem is Problem 1 of 1994 British Mathematical Olympiad Round 2","['contest-math', 'number-theory', 'pell-type-equations', 'elementary-number-theory']"
3766874,"If $A$ is a matrix such that $A^T = A^2$, what are eigenvalues of $A$?","If $A$ is a matrix such that $A^T = A^2$ , what are eigenvalues of $A$ ? Now I read somewhere that changing the matrix by taking a transpose does not change the characteristic polynomial. So it is safe to say that the annihilating polynomial in this case is $x^2 - x$ . If this is the case then I think the answer is quite easy. But if there is some caveat I am missing then I am lost as to how to approach this problem.","['eigenvalues-eigenvectors', 'matrices', 'minimal-polynomials', 'linear-algebra', 'transpose']"
3766878,Why does setting y=x work in this proof?,"The problem in question asks you to show that the non-negativity of a metric follows from the 2nd, 3rd, and 4th metric space axioms i.e. $\textbf{M2}$ $d(x,y) = 0$ iff $x=y$ $\textbf{M3}$ $d(x,y) = d(y,x)$ $\textbf{M4}$ $d(x,y) \leq d(x,z) + d(z,y)$ My (kind of longwinded) proof was the following: We prove by showing that all possible distances on the arbitrary points $x,y,z$ are non-negative. That $d(x,x), d(y,y), d(z,z)$ are non negative is given by $\textbf{M2}$ . For the remainder, let $x=y$ , then using $\textbf{M2}$ , $\textbf{M3}$ , $\textbf{M4}$ we have \begin{equation}
\begin{split}
    d(x,y)=0 & \leq d(x,z) + d(z,y) \\
             & \leq d(x,z) + d(z,x) \\
             & \leq d(x,z) + d(x,z) \\
             & \leq 2d(x,z)
\end{split}
\end{equation} dividing both sides by 2 gives \begin{equation}
    0 \leq d(x,z)
\end{equation} Thus by $\textbf{M3}$ and using the fact that $y=x$ we have that $d(x,z)$ , $d(z,x)$ , $d(y,z)$ , and $d(z,y)$ are all non-negative. QED. I've checked some answers setting and y=x in $\textbf{M4}$ seems to be the right strategy. My question is why does setting y=x work in this case? Doesn't this only prove for the case in which y=x? What about when y doesn't equal x? I've a seen similar strategies used in proofs before and I never quite understood how prooving for a limited set of cases prooved all cases. Many thanks.",['functional-analysis']
3766890,Fundamental group of Klein Bottle,"It is well know that the fundamental group of the Klein Bottle $G$ is defined by $$G=BS(1,-1)=\langle a,b: bab^{-1}=a^{-1}\rangle.$$ I know, for example  that $BS(1,2)$ can be defined as the group $$BS(1,2)=\langle A,B\rangle $$ where $$A=\left(
\begin{array}{cc}
 1 & 1 \\
 0 & 1 \\ 
\end{array}
\right), B=\left(
\begin{array}{cc}
 2 & 0 \\
 0 & 1 \\
\end{array}
\right).$$ These matrices satisfy the equation $BAB^{-1}=A^{2}$ and are free : there is not an integer $k$ such that $A^{k}=I$ or $B^{k}=I$ . This implies that we obtain an ""explicit description"" of $BS(1,2)$ as the group generated by $A$ and $B$ . I know that the matrices $$a=\left(
\begin{array}{cc}
 1 & 1 \\
 0 & 1 \\ 
\end{array}
\right), b=\left(
\begin{array}{cc}
 -1 & 0 \\
 0 & 1 \\
\end{array}
\right)$$ satisfy the relation $bab^{-1}=a^{-1}$ but $b^{2}=I$ . This implies that $BS(1,-1)$ is not generated by $a$ and $b$ . My question is: is there an ""explicit description"" for $G=BS(1,-1)$ with matrices or maybe another couple of objects?","['klein-bottle', 'fundamental-groups', 'abstract-algebra', 'group-theory', 'algebraic-topology']"
3766904,Wasserstein gradient flow and continuity equation,"As I was reading the note https://www-dimat.unipv.it/savare/Ravello2010/ravelloC.pdf (page 9), I was not sure why the last equality holds. It seems to me that the last expression can be obtained by integration by parts. However, I don't know if there is any boundary conditions assumed implicitly. Any help will be greatly appreciated!","['gradient-flows', 'calculus-of-variations', 'optimal-transport', 'functional-analysis', 'partial-differential-equations']"
3766930,How can I prove $A − A(A + B)^{−1}A = B − B(A + B)^{−1}B$ for matrices $A$ and $B$?,The matrix cookbook (page 16) offers this amazing result: $$A − A(A + B)^{−1}A = B − B(A + B)^{−1}B$$ This seems to be too unbelievable to be true and I can't seem to prove it. Can anyone verify this equation/offer proof?,"['matrices', 'matrix-equations']"
3766951,"How to prove that $(a^m)^n=a^{mn}$ where $a,m,n$ are real numbers and a>0?","I know how to prove the equality when $m$ is a rational number and $n$ is an integer, but do not know how to go about proving this for real numbers. On a semi-related note, I was trying to prove this when both $m$ and $n$ are rational, and found out that I have to prove that $(\frac{1}{z})^{\frac{1}{y}}$ = $\frac{1}{z^{\frac{1}{y}}}$ . Does this need to be proven or can I accept it as a definition?","['real-numbers', 'exponentiation', 'real-analysis', 'algebra-precalculus', 'exponential-function']"
3766978,"If $|{z}|=\max\big\{|{z}+2|,|{z}-2|\big\}$, then which is true: $|z\pm \bar{z}|=2$ or $|z\pm \bar{z}|=1/2$?","If $|{z}|=\max\big\{|{z}+2|,|{z}-2|\big\}$ , then (a) $|{z}-\bar{z}|=1 / 2$ . (b) $|{z}+\overline{{z}}|={2}$ . (c) $|{z}+\overline{{z}}|=1 / 2$ . $({d})|{z}-\overline{{z}}|={2}$ . My approach $|z|=|z+2|$ $\Rightarrow {z\overline{z}}=({z}+2)(\overline{{z}}+2)$ $\Rightarrow {z}+{\overline{z}}=-2 \Rightarrow|{z}+{\overline{z}}|=2$ $|z|=|z-2| \Rightarrow z \bar{z}=(z-2)(\bar{z}-2)$ $\Rightarrow {z}+\overline{{z}}=2 \Rightarrow|{z}+\overline{{z}}|=2$ I guess it is right. My question is how can I  come to solution using any graphical approach! for ref :- https://www.desmos.com/calculator/nnnnairelh","['complex-analysis', 'solution-verification', 'absolute-value', 'complex-numbers']"
3766998,Is there an explicit solution to $a^x+b^x=1$?,"Is there an explicit solution to $a^x+b^x=1$ ? Where $a, b \in [0, 1]$ and $a+b \le 1$ . I've been playing around with this equation, but I can't seem to make any progress in solving it.  I tried it in Wolfram for some hints, but nothing showed up. Should I assume there isn't an explicit solution to this?",['real-analysis']
3767054,"Solve the initial value problem: $\frac{dy}{dx} = e^{x+y}$, given $y(0)=0$. [duplicate]","This question already has answers here : Find the general solution of $y'= a^{x + y}$ where y is the function (2 answers) Closed 3 years ago . I have attempted the question several times so far, and I have always reached the same answer that differs from the solution, any advice would help greatly! My attempt $$\frac{dy}{e^y} = e^x dx$$ Taking the integral, I got $$-e^{-y} = e^x + C.$$ Solving for $y$ , I got $$y=-\ln(C-e^x).$$ After subbing in $y(0)=0$ , I got $$0=-\ln(C-1)$$ and solving for $C$ , I got $$C=2.$$ Thus, I got $$y=-\ln(2-e^x).$$ However, the solutions have $y=-\ln(1-e^x)$ as the answer. Have I done something wrong? Thanks in advance!","['initial-value-problems', 'ordinary-differential-equations']"
3767067,Kernels of commuting linear operators on infinite dimensional vector space,"If $S$ and $T$ are commuting operators on an infinite dimensional vector space $V$ , it is in general true that $$\ker S + \ker T \subseteq \ker(ST),$$ but in general equality does not hold. A simple example is given by $S = T = \frac{d}{dx}$ on $C^\infty(\mathbb{R})$ . I am looking for conditions on $S$ and $T$ that will give equality in the above equation, ie: $$\ker S + \ker T = \ker (ST)$$ Writing $\ker T^\infty$ for $\cup_n \ker T^n$ , I am currently trying to show that the conditions $\mathrm{im} S = \mathrm{im} T = V$ , $\ker S^\infty \cap \ker T^\infty = \{ 0 \}$ , $\dim \ker S < \infty$ and $\dim \ker T < \infty$ , $ST = TS$ imply that $\ker S + \ker T = \ker(ST)$ . I think the second condition can be weakened to $\ker S^2 \cap \ker T^2 = \{ 0 \}$ , but I have this stronger condition for some operators I am interested in. Any help would be appreciated, thanks. -edit- I am not confident that all these conditions are necessary.",['linear-algebra']
3767111,Relations between two definitions of Lie algebra,"This post follows from another post What is exponential map in differential geometry about two kinds of exponential maps (of Riemannian groups and of Lie groups, separately) and Lie algebra. It is inspired by discussions following the answer, which are not repeated here. It’s said there are two definitions of Lie algebra (tangent space, left invariant vector field). (Edited to add:) (The question is originally stated as ‘ Relations between two two definitions of exponential maps’, that’s something I’m also interested in, I may put another post for that if necessary.) (Edited to add:) By far I guess Lie algebra is a bit like a collection $G$ of left invariant (well behaved) vector field such that from a vector at a point we can infer or generate vectors at all other points, (i.e. a well behaved vector field), for these vectors are somehow the same or homogeneous; the homogeneity and generalizability is what the invariant means. [It's, as explained below, invariant of vector fields $X$ or phase space... w.r.t. the operation $+$ of Lie group. e.g. $X_{p+q} = X_q$ for all $p, q$ in the Lie group.](Probably right invariant works too) So there is a one one correspondence between a left invariant vector field in $G$ and a vector in a tangent space $T_qM$ (it seems, according to some other posts, $q$ can be any point and we prefer identity for it’s convenient.) and so $G$ of these vector fields and $T_qM$ are isomorphic or have at least some kind of one one correspondence and so the two definitions are consistent. The definition of Lie algebra also includes the consideration of commutability of two left invariant vector fields. For that purpose we define an unusual multiplication [,]. Why we particularly need to take care of that commutability? I guess it’s for the the expansion of log (exp(X)exp(Y)), as mentioned in the comment of the origin post. (Btw, in the tangent space definition do we need to consider commutability?) Why we do such expansion? It’s because the idea of exponential maps of Lie groups originates from exponent of matrix? In a word, the left invariant definition seems to justify the tangent space definition (I guess there is a related proof) and if we consider tangent space at all points and carefully pick up a vector of invariant property (like of certain length and direction) from each tangent space we may well visualize ANY left invariant vector field. And it is isomorphic to a vector of tangent space at ONE point. (The following continues discussion, in comments on an answer, on notations in Lie group) About notations, using Lie group $M$ as an example, $\ell_q:M\to M$ (or in Spivak's notation, $L_a$ ) is adding a point $q$ to any point in $M$ (such addition is possible since we impose a Lie group structure to a manifold ), while $\ell_{q*} $ (or $\ L_{a*}$ ) is the derived operation for the tangent space of Lie group $M$ (NOT the Lie group itself) at a point $q$ , e.g. $T_pM$ or $M_p$ (it confuses me since the two denote the same thing), adding q to p (NOT adding elements in tangent space) to get tangent space $T_{q+p}M$ . Using Lie group $SO(2)$ (~ $S^1$ ) as an example $\ell_A:SO(2)\to SO(2)$ is multiplying a matrix $A$ to any matrix in $SO(2)$ , while $\ell_{A*}$ is the derived operation for the tangent space of Lie group $SO(2)$ at a point $p$ , e.g. $T_pS^1$ , adding q to p to get tangent space $T_{q+p}S^1$ . Left invariant means a vector field (or a collection of vector fields, or all tangent vectors at all points or in phsical context the phase space, or in symplectic geometry and the Hamiltonian mechanics (which I know little) the similar pair of position and velocity), each element of it for any 'distance' (any element in Lie group) being transferred or moving to another point and we still get the same vector field (or vector fields, or phase space...). (Complement: considering Lie derivative of a vector field, this seems to somehow the same as saying $L_XX=0$ , which in terms of Lie algebra, just $[X,X]=0$ in the definition; by seeing [ , ] as 'derivative' it seems the meaning is clearer. Put that view in the context of matrix Lie group, e.g. $SO(2)$ where $[A, A]=0, [A, B]=0$ , it's like saying the two vector fields corresponding to two tangent vectors at a same point differentiated against themselves and, sometimes, even against each other equals zero.) And Lie group basically enables us to to interpret a point at a manifold as a distance, similar to that we can treat a vector (position) in Euclidean space as a displacement (by setting the 'original point' $O$ , which 'becomes' in Lie group the unit $e$ ). With Lie group we 'geometrify' the non-geometrical objects like a matrix set, and 'numerify' the non numerical objects like a manifold. And exponential maps basically links (though not necessarily one one) a tangent vector to a point at a manifold (geometrical manifolds like surface or more abstract manifold like a matrix set, the two corresponding to the two kinds of exponential maps I guess) interpreted as a 'distance'/displacement. With exponential maps we link tangent space (a vector space) to the manifold (now made a Lie group). But here comes another question, which I states in another post: why we need to, with exponential maps, make a link between a tangent space and the manifold?","['lie-algebras', 'lie-groups', 'differential-geometry']"
3767119,How does the process of simplifying imaginary numbers actually work?,"Sorry in advance if this is a really stupid question In class I've been told that $$\sqrt{-25} = 5j $$ Converting $\sqrt{-25} $ into $5j$ is straightforward for me, but I don't understand how it works Doesn't the property of $\sqrt{xy} = \sqrt{x}\sqrt{y} $ only hold true for positive real number values of x and y, where $i^2$ is defined to be negative 1? In the case of $$\sqrt{-25} = 5j $$ Would we treat j= $\sqrt{-1}$ as a positive real number in order to ""break"" the root using the elementary algebra associated with roots? I don't really understand how this works algebraically.","['elementary-number-theory', 'algebra-precalculus', 'education', 'terminology']"
3767136,L'hopital rule fails with limits to infinity?,"$$  \lim_{n \to \infty} \frac{1 +cn^2}{(2n+3 + 2 \sin n)^2} = ? $$ if I  factor the $n^2$ out of denominator, $$ \lim_{n \to \infty} \frac{ 1 + cn^2}{ n^2 ( 2 + 3n^{-1} + 2 \frac{ \sin n}{n} )^2}$$ And take limit directly, I get the answer as $$ \frac{c}{4}$$ However, If I apply l'hopital rule, Iget $$ \lim_{ n \to \infty} \frac{ 2cn}{2 (2n + 3 + 2 \sin n)( 2 + 2 \cos n)} $$ However this new limit gives a different value than original according to wolfram.. and neither am I able to compute it by hand, what am I missing? Some people say of limit existing and not existing, but then suppose $$ \lim_{x \to 0} \frac{1}{x} = \infty$$ Does this limit exist? how do you define a limit to be existing as in what is sufficent condition for it","['limits', 'calculus']"
3767159,"How should one understand the ""indefinite integral"" notation $\int f(x)\;dx$ in calculus?","In calculus, it is said that $$
\int f(x)\; dx=F(x)\quad\text{means}\quad  F'(x)=f(x)\tag{1}
$$ where $F$ is a differentiable function on some open integral $I$ . But the mean value theorem implies that any differentiable function $G:I\to \mathbb{R}$ with the property $G'(x)=f(x)$ on $I$ can be determined only up to a constant. Since the object on the right of the first equality of (1) is not unique, we cannot use (1) as a definition for the symbol $\int f(x)\;dx$ . Formulas for antiderivatives are usually written in the form of $\displaystyle \int f(x)\;dx=F(x)+C$ . For example, $$
\int \cos x\;dx = \sin x+C\;\tag{2}
$$ where $C$ is some ""arbitrary"" constant. One cannot define an object with an ""arbitrary"" constant. It is OK to think about (2) as a set identity: $$
\int \cos x\; dx = \{g:\mathbb{R}\to\mathbb{R}\mid g(x)=\sin x+C,\; C\in\mathbb{R}\}. \tag{3}
$$ So sometimes, people say that $\int f(x)\;dx$ really means a family of functions. But interpreting it this way, one runs into trouble of writing something like $$
\int (2x+\cos x) \; dx = \int 2x\;dx+\int \cos x\; dx = \{x^2+\sin x+C:C\in\mathbb{R}\}\;\tag{4}
$$ where one is basically doing the addition of two sets in the middle, which is not defined. So how should one understand the ""indefinite integral"" notation $\int f(x)\;dx$ ? In particular, what kind of mathematical objects is that?","['integration', 'calculus', 'notation']"
3767178,How to evaluate the following limit: $\lim_{x\to 0}\frac{12^x-4^x}{9^x-3^x}$?,"How can I compute this limit $$\lim_{x\to 0}\dfrac{12^x-4^x}{9^x-3^x}\text{?}$$ My solution is here: $$\lim_{x\to 0}\dfrac{12^x-4^x}{9^x-3^x}=\dfrac{1-1}{1-1} = \dfrac{0}{0}$$ I used L'H $\hat{\mathrm{o}}$ pital's rule: \begin{align*}
\lim_{x\to 0}\dfrac{12^x\ln12-4^x\ln4}{9^x\ln9-3^x\ln3}&=\dfrac{\ln12-\ln4}{\ln9-\ln3}
\\ &=\dfrac{\ln(12/4)}{\ln(9/3)}
\\ &=\dfrac{\ln(3)}{\ln(3)}
\\ &=1
\end{align*} My answer comes out to be $1$ . Can I evaluate this limit without L'H $\hat{\mathrm{o}}$ pital's rule? Thanks.","['limits', 'calculus', 'limits-without-lhopital']"
3767198,Is my proof for $f$ is convex iff $f'$ is monotonically increasing correct?,"I am following up on my previous question . My previous attempt for the proof was wildly incorrect (my question was how that proof was exactly my old proof was incorrect) and I have now come up with a new proof. I have to prove: Let $f:(a, b) \to R^1$ be differentiable. Prove that $f$ is convex iff $f'$ is monotonically increasing. What I have for the proof: ( $\Rightarrow$ ) Assume $f$ is convex in $(a, b)$ . Let $a<s<t<u<b$ . By Exercise 23 in Chapter 4, \begin{align}\tag{14.1}
            \frac{f(t)-f(s)}{t-s} \le \frac{f(u)-f(s)}{u-s} \le \frac{f(u)-f(t)}{u-t} 
\end{align} Since $f$ is differentiable on $(a,b)$ , both $ f'(s) = \lim_{t  \to s} \frac{f(t)-f(s)}{t-s}$ and $f'(t)=\lim_{u  \to t} \frac{f(u)-f(t)}{u-t}$ exist. However, applying the Order Limit Theorem on (14.1) gives \begin{align*}
        \lim_{t  \to s} \frac{f(t)-f(s)}{t-s} \le \lim_{u  \to t} \frac{f(u)-f(t)}{u-t} \implies f'(s) \le f'(t)
    \end{align*} which shows that $f'$ is monotonically increasing in $(a, b)$ . ( $\Leftarrow$ ) Assume $f'$ is monotonically increasing in $(a, b)$ and $a<x<y<b$ . Fix $0 < \lambda< 1$ . By Exercise 23 in Chapter 4, we must show that \begin{equation}\tag{14.0}
        f(\lambda x + (1- \lambda)y) \le \lambda f(x) + (1-\lambda) f(y)
    \end{equation} Denote $z=\lambda x+ (1-\lambda)y$ .Then, $z=\lambda(x-y)+y$ which implies that $\lambda=\frac{z-y}{x-y}$ . Since $\lambda>0, z-y>x-y \implies z>x$ . Also, $1-\lambda=\frac{x-y-z+y}{x-y} = \frac{x-z}{x-y}$ . Since $\lambda<1, x-z>x-y \implies z < y$ . Thus, $x<z<y$ . Then, (14.0) can be simplified as: \begin{align*}
        f(z) &\le f(y) + \lambda f(x) - \lambda f(y) \\
        \lambda f(z) - \lambda f(x) &\le f(y) - f(z) - \lambda f(y) + \lambda f(z) \\
         \lambda[f(z)-f(x)] &\le (1-\lambda)[f(y)-f(z)]
    \end{align*} Thus, since $\lambda = \frac{y-z}{y-x}$ and $1-\lambda = \frac{z-x}{y-x}$ , it suffices to show that \begin{equation}\tag{14.2}
         \frac{f(z)-f(x)}{z-x} \le \frac{f(y)-f(z)}{y-z}
    \end{equation} Now, as we take $z\to x$ on the left of (14.2) and $y\to z$ on the right of (14.2), then we have $f'(x)\le f'(z)$ , which holds since $x<z$ and $f'$ is monotonically increasing. Exercise 23 in Chapter 4 in Rudin : A real-valued function $f$ defined in $(a, b)$ is said to be convex if $$ f \left( \lambda x + (1- \lambda) y \right) \leq \lambda f(x) + (1-\lambda) f(y)$$ whenever $a < x < b$ , $a < y < b$ , $0 < \lambda < 1$ . Prove that every convex function is continuous. Hint: If $f$ is convex in $(a, b)$ and if $a < s < t < u < b$ , show that $$ \frac{ f(t)-f(s)}{t-s} \leq \frac{ f(u)-f(s)}{u-s} \leq \frac{ f(u)-f(t)}{u-t}.$$ Can someone please read over my proof and see if there is something that I did incorrectly? Also, specifically, is my usage of the Order Limit Theorem correct and is the argument right below (14.2) correct?","['general-topology', 'solution-verification', 'real-analysis']"
3767208,Smoothness of characteristic function at 0 is related to the decay of the measure at infinity.,"Let $\varphi$ be the ch. f of $\mu$ . After proving the inequality $$\mu \left\{ x : |x|>\frac{2}{u}\right\}\leq \frac{1}{u}\int_{-u}^u(1-\varphi(t))\,dt$$ the text im reading states ""this shows that the smoothness of the characteristic function at $0$ is related to the decay of the measure at $\infty$ ."" I can see that the left hand side refers to the decay of $\mu$ at infinity, but I don't understand exactly what is the relationship between the right hand side and the ""smoothness"" of $\varphi$ . I guess my question might be: how is smoothness measured exactly and how does that measure of smoothness relate to the right hand side of the inequality above?","['characteristic-functions', 'measure-theory', 'probability-theory']"
3767210,Directly Calculating Birthday Paradox Probabilites,"I am trying to calculate the probability of at least 2 people sharing a birthday in a group of 4 people. I understand that calculating it as 1-P(no shared birthdays) is simpler, but I would like to understand the counting method by doing it directly. My attempt for $n=4$ is P = P(2 people) + P(3 people) + P(4 people) = $\frac{1}{365}\binom{4}{2}+\frac{1}{365^2}\binom{4}{3}+\frac{1}{365^3}\binom{4}{4}=0.0164$ ... but this does not match up with P = $1-\frac{364}{365}\frac{363}{365}\frac{362}{365}=0.163...$ What am I doing wrong in the direct calculation?","['combinatorics', 'birthday', 'probability']"
3767266,Proving $\frac1{2\pi} \int_0^{2\pi} \frac{R^2-r^2}{R^2-2Rr\cos\theta+r^2} d\theta =1$ by integrating $\frac{R+z}{z(R-z)}$ without residue theorem.,"I was given the function: $$ \frac{R+z}{z(R-z)} $$ And I was asked to integrate it around a closed contour to prove: $$\frac1{2\pi} \int_0^{2\pi} \frac{R^2-r^2}{R^2-2Rr\cos\theta+r^2} d\theta =1$$ I've seen people get a proof quite easily by using the residue theorem, but I have not studied it yet so I am not supposed to do it. My attempt: Let $\gamma = re^{it}$ , $$\int_\gamma f dz = \int_\gamma \frac1z + \frac2{R-z} dz$$ $$\Rightarrow \int_\gamma f dz = \int_0^{2\pi} \frac{ire^{it}}{re^{it}}dt + \int_0^{2\pi} \frac{2ire^{it}}{R-re^{it}}dt$$ $$ = 2\pi i + \int_0^{2\pi} \frac{2Rr\cos t + 2r}{R^2+2Rr\cos t + r^2} dt$$ But I don't know what else should I do. Any ideas? Edit: Sorry I had a typo, the function to integrate was $ \frac{R+z}{z(R-z)} $ and not $ \frac{R-z}{z(R-z)} $","['complex-analysis', 'complex-integration']"
3767285,What is the value of $1 -\omega^h + \omega^{2h} -...+(-1)^{n-1} \omega^{(n-1)h}$ when $\omega$ is a root of unity?,"I'm reading Ahlfors' complex analysis book. One of the problems in the book says as follows What is the value of $1 -\omega^h + \omega^{2h} -...+(-1)^{n-1} \omega^{(n-1)h}$ ? where $h$ is some integer and $ \omega = \cos\left(\frac{2\pi}{n}\right) + i \sin \left(\frac{2 \pi}{n}\right)$ , for some fixed $n \in \mathbb{N}$ , is one of the $n$ -th roots of unity. The first thing I noticed is that I could write the series in terms of $-\omega^h$ as $$
1 +\left(-\omega^h\right) + \left(-\omega^h\right)^2 +...+\left(-\omega^h\right)^{n-1}
$$ Inspired by this, I separated the problem into 2 cases If $h$ is an integer of the form $ h = \frac{n(2k+1)}{2}$ for some $k \in \mathbb{Z}$ , then I get the following $$
-\omega^h = -\cos\left(\frac{2\pi}{n}h\right) - i \sin \left(\frac{2 \pi}{n}h\right)= -\cos\left(\pi + 2\pi k\right) - i \sin \left(\pi + 2\pi k\right) = 1
$$ which means the sum evaluates to $\sum_{j=0}^{n-1} 1 = n$ . If $-\omega^h \neq 1$ , then using the fact that the sum in question is a sum of the first $n$ terms in a geometric series, I can write $$
1 -\omega^h + \omega^{2h} -...+(-1)^n \omega^{(n-1)h} = \frac{1 - \left(-\omega^h\right)^n}{1 - \left(-\omega^h\right)} = \frac{1 - (-1)^n\omega^{nh}}{1 +\omega^h} 
$$ and since $h$ is an integer, I see that $$
\omega^{nh} = \cos\left(\frac{2\pi}{n}nh\right) + i \sin \left(\frac{2 \pi}{n}nh\right) = \cos\left(2\pi h\right) + i \sin \left(2\pi h\right) = 1
$$ which means the sum simplifies to $\frac{1 - (-1)^n}{1 +\omega^h}$ . From here I see that if $n$ is odd the sum will become $0$ because of the numerator, but for the case of $n$ being an even number, I don't see a way to simplify $\frac{2}{1 +\omega^h}$ more than it already is. Is my solution correct? And if so, is this as simplified as I can write the solution, or can it still be simplified further? Thank you very much!","['complex-analysis', 'solution-verification', 'roots-of-unity', 'complex-numbers']"
3767286,"Is the set $\{x \in \mathbb R^n : d(x, M) = c\}$ a smooth manifold for a small constant $c$ when $M$ is a smooth manifold embedded in $\mathbb R^n$?","I am a beginner of differential geometry. Let $M$ be a smooth manifold embedded in $\mathbb R^n$ and consider the
subset $$
      S = \{x \in \mathbb R^n : d(x, M) = c \},
$$ where $d(x, M)$ denotes the distance between a point $x$ and the manifold $M$ . I would like to know if $S$ is a smooth (at least $C^1$ ) manifold for sufficiently small $c > 0$ . In one of the simplest cases, that $M$ is a (finite) line segment in $\mathbb R^3$ , I think $S$ is the union of a cylinder of radius $c$ with its center line $M$ and two half spheres attached to the both ends of the cylinder, and $S$ is smooth.",['differential-geometry']
3767295,"Cartan Differentiable calculus. Show $g(x,y)= \frac{f(x)-f(y)}{x-y}$ is differentiable at $(x_{0},x_{0})$","I'm doing problem 8 of Cartan Differentiable Calculus book. The problem says as follow: Let $f$ assume its values in a Banach space $E$ , an let it be of class $\mathcal{C}^1$ in an open interval $I$ . Put $
\begin{cases}
g(x,y)&= \frac{f(x)-f(y)}{x-y} ~ \text{ if } x \neq y\\
g(x,x)&= f'(x)
\end{cases}
$ If $f''(x_{0})$ exists at $x_{0} \in I$ show that $g$ is differentiable in $(x_{0},x_{0})$ So, I think that we should have $Dg(x_{0},x_{0})[h_{1},h_{2}] = \frac{f''(x_{0})}{2}(h_{1}-h_{2})$ but I've manage nothing more. The version I have says the following hint: Apply the mean value theorem to the function $
f(x)=xf'(x_{0})- \frac{(x-x_{0})^2}{2}f''(x_{0})
$ I think there is a typo and it should be $h(x)=...$ Either way I would gladly appreciate any help. Edit: I already proved that g is continous in $I \times I$ and its $\mathcal{C}^1$ in $I \times I \backslash \cup_{x \in I} \{(x,x)\}$","['differential', 'banach-spaces', 'derivatives']"
3767308,Does a sequence of $d$-SOS polynomials converge to a polynomial that is also $d$-SOS?,"Let $\mathbb{R}[X]_{\leq 2d}$ denote the real vector space of polynomials of degree at most $2d$ in the coordinate ring $\mathbb{R}[X]$ of variety $X$ . Definition: A polynomial $f$ is $d$ -SOS if there exist $g_{1}, \dots, g_{k} \in \mathbb{R}[X]_{d}$ such that $$f = g^{2}_{1} + \cdots + g^{2}_{k}$$ Question: Suppose that you have a finite set $X\subset \mathbb{R}^{n}$ and a sequence $\{f_{i}\}_{i\in \mathbb{N}}$ of $d$ -SOS polynomials in $\mathbb{R}[X]_{\leq 2d}$ ,  then is $$f := \lim_{i \in \mathbb{N}} f_{i}$$ also $d$ -SOS? For me, it's clear that any non-negative polynomial in $X$ , as it is finite, is sum of squares (SOS), but for me it is not clear why the limit has degree at most $2d$ . Is it true? Why?","['algebraic-geometry', 'polynomials', 'real-algebraic-geometry', 'sum-of-squares-method', 'optimization']"
3767335,How to evaluate sequence of operations on an object?,"Example 1 Rational tangle dance mentioned here with operations: T(tangle) R(rotate) For example, sequence of operations $TTRTT$ is considered as $T^2 \cdot R \cdot T^2$ but not $2T + R + 2T$ mathematically. why? Example 2 Symmetries of polygon shown here , with operations: R(rotation) F(Flip) For example, $RFFRFFF$ is considered as $R \cdot F^2 \cdot R \cdot F^3$ but not $R +2F + R + 3F$ mathematically, why? Example 3 Flipping a coin The number of possible outcomes of each coin flip is 2 (either heads or tails.) So the probability of either a heads or a tails is 1/2. This make sense to me. But, the number of possible outcomes of several independent events is the product of the number of possible outcomes of each event individually. Yet to understand. The number of combinations that 3 coin flips will give 2 x 2 x 2 = 8 but not 2 + 2 + 2 = 6, why? Am assuming, example 1 & 2 are from non-arithmetic world and example 3 is  from arithmetic world(Number theory). How addition operation is different from multiplication in arithmetic & non-arithmetic world?","['number-theory', 'abstract-algebra', 'discrete-mathematics', 'arithmetic', 'probability']"
3767356,How do I understand this direct limit?,"As in Hartshorne page 72, we defined the morphism between locally ringed spaces, say $(f,f^{\sharp})$ is a morphism between $X$ and $Y$ . Then we have, for all $P\in X$ , an induced homomorphism between local rings, $f^{\sharp}_P:O_{Y,f(P)}\rightarrow O_{X,P}$ . To define this induced homomorphism, we note that for all open neighborhoods, $V$ , of $f(P)$ , $f^{-1}(V)$ is an open neighborhood of $P$ . Then $f^{\sharp}$ defines a homomorphism, $f^{\sharp}: O_Y(V) \rightarrow O_X(f^{-1}(V))$ . Then by taking direct limit over all such $V$ , we have a ring homomorphism, $O_{Y,f(P)} \rightarrow \varinjlim O_X(f^{-1}(V))$ . Now, my question is how do I understand this direct limit? Suppose $X=\operatorname{Spec}A$ and $Y=\operatorname{Spec} B$ and the morphism is induced by ring homomorphism $\phi:B \rightarrow A$ , is it true that the direct limit above is $A\otimes_{B}B_{\phi^{-1}(P)}$ ? If it is true how to prove it? A complete proof will be greatly appreciated.","['algebraic-geometry', 'category-theory', 'commutative-algebra']"
3767405,What does distance of a point from line being negative signify?,"When we take distance from the line, we take $$ d = \frac{ Ax_o + By_o + C}{ \sqrt{A^2 +B^2}}$$ usually with a modulus on top, now my question is if I evaluate this distance as negative what does it mean? Can I decide on which half-plane a point using this?",['linear-algebra']
3767420,How to check if $\phi(n)$ is a perfect square?,"How to check if $\phi(n)$ is a perfect square? Here, $\phi$ is Euler's totient function.  That is, $\phi(n)$ counts the positive integers up to a given integer $n$ that are relatively prime to $n$ for example if $n = 34$ , $\phi(34)=16=4\times 4$ . Can we check this without calculating $\phi(n)$ ?","['number-theory', 'totient-function', 'elementary-number-theory', 'sequences-and-series']"
3767452,"Solving the system $\cos x+\cos y+\cos z=\frac32\sqrt3$, $\sin x+\sin y+\sin z=\frac32$","Suppose we have $$\begin{align}
\cos x + \cos y + \cos z  &= \frac{3}{2}\sqrt{3} \\[4pt]
\sin x + \sin y + \sin z  &= \frac{3}{2}
\end{align}$$ How can we solve for $x$ , $y$ and $z$ ? According to Wolfram Alpha, the values of $x, y, z$ must be the same i.e. $\pi/6$ modulo $2\pi$ . How do we solve the equations analytically? What I am able to prove . I am able to show that two out of three variables $x,y, z$ must be equal. This I can do by reformulating the problem as ""maximize $\sin x$ subject to the above constraints."" and doing Lagrange optimization. I am sure there must be a simpler way. Problem source: From CMI Entrance 2010 paper","['trigonometry', 'complex-numbers']"
3767533,Proof of Deuring's Correspondence,"Let $E$ be a supersingular elliptic curve over $\overline{\mathbb{F}_q}$ where $q=p^n$ and $p$ is prime. Then $B:=\text{End}(E) \otimes \mathbb{Q}$ is a unique quaternion algebra over $\mathbb{Q}$ ramified exactly at $p$ and $\infty$ , and $\text{End}(E)$ is a maximal order in $B$ . I want the proof of the following statement: For every maximal order $O' \subseteq B$ , there exists $E'$ such that $O' \simeq \text{End}(E')$ . This is given in Voight 42.2.21 (p.790) : For every isogeny $\phi: E \rightarrow E'$ , there exists a left $O$ -ideal $I$ and an isomorphism $\rho:E_I \rightarrow E'$ such that $\phi=\rho \phi_I$ . Moreover, for every maximal order $O' \subseteq B$ , there exists $E'$ such that $O' \simeq \text{End}(E')$ . Here $E$ is supersingular and $O:=\text{End}(E)$ . Definition of $E_I=E/E[I]$ is given in 42.2.1. The proof just says use a connecting ideal between orders to prove the second statement, but I don't get how we can introduce the connecting ideal here. I suppose he means connecting ideal between $O:=\text{End}(E)$ and $O'$ , but how do we know that they are connected in the first place? According to his definition two orders are connected if and only if they are locally isomorphic, i.e. $O_\mathfrak{p} \simeq O'_\mathfrak{p}$ for all primes $\mathfrak{p}$ , and I don't think this can happen if we just choose a random maximal order $O' \subseteq B$ . Can anyone give me a full proof of this?","['number-theory', 'modular-forms', 'elliptic-curves']"
3767558,Axis of reflection,Is there any way to get the equation of axis of reflection given two intersecting lines without sketching? Example question: The image of the line $p:\; y-2x=3$ is the line $q:\;2y-x=9$ . Find the equation of axis of reflection?,"['euclidean-geometry', 'analytic-geometry', 'reflection', 'geometry', 'transformation']"
3767564,An area preserving diffeomorphism between a disk and an ellipse,"This is a self-answered question. I post it here since (embarrassingly) it took me some time to realize that the solution is obvious. Let $D \subseteq \mathbb R^2$ be the closed unit disk and let $E$ be an ellipse with the same area, i.e.  with minor and major axes of lengths $a<b$ and $ab=1$ . $$
E=\{(x,y) \, | \, \frac{x^2}{a^2} + \frac{y^2}{b^2} \le 1 \}
$$ Question: Can we construct explicitly an area preserving diffeomorphism $f:D \to E$ ? (i.e. $Jf=1$ identically on $D$ ).","['euclidean-geometry', 'area', 'riemannian-geometry', 'multivariable-calculus', 'differential-geometry']"
3767568,"Finding $\frac{\cot\gamma}{\cot \alpha+\cot\beta}$, given $a^2+b^2=2019c^2$","This is a question that appeared in the $2018$ Southeast Asian Mathematical Olympiad: In a triangle with sides $a,b,c$ opposite angles $\alpha,\beta,\gamma$ , it is known that $$a^2+b^2=2019c^2$$ Find $$\frac{\cot\gamma}{\cot\alpha+\cot\beta} $$ Well, by the Sine Law we have $$\sin^2\alpha+\sin^2\beta=2019\sin^2\gamma$$ and by the Cosine Law, $$\cos\gamma=\frac{a^2+b^2-c^2}{2ab} = \frac{1009c^2}{ab}$$ I’m stuck here. I tried to convert everything in our target expression to sines and cosines, but that makes the expression more complicated. I guess we can use the fact that $\cot\gamma=-\cot(\alpha+\beta)$ . How can you tackle this question? (also, apparently there are no worked solutions online)","['contest-math', 'algebra-precalculus', 'triangles', 'trigonometry']"
3767627,Help in finding $\int \frac{x+x\sin x+e^x \cos x}{e^x+x\cos x-e^{x} \sin x} dx$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I want to find $$\int \frac{x+x\sin x+e^x \cos x}{e^x+x\cos x-e^{x} \sin x} dx.$$ But since algebraic, exponential and trigonometric functions are involved I am not able to solve it. Please help in finding it by hand.",['integration']
3767629,Do infinitely many points on earth have the same temperature as their antipodal?,"Let $X=S^2$ be the unit sphere in $\mathbb{R}^3$ and $T:X\rightarrow \mathbb{R}$ be a continuous function. My topology textbook claims that the set $A=\{x \in X\ |\ T(x)=T(-x)\}$ has an infinite number of elements. The fact that $A$ is non empty is clear to me as a consequence of the intermediate-value theorem, since $$f:X\rightarrow \mathbb{R},\ x \mapsto T(x)-T(-x)$$ is continuous, X is connected and $f(X)$ contains a non-positive and a non-negative real number. What's way less clear is how there can't be a finite number of points in $A$ . My intuition is that there must be a (non-trivial) curve on the sphere that contains the antipodal of each of its points, but I really don't know how to show it, if that's even true.",['general-topology']
3767651,"Distance between two lines $L_1:\> x+y+z=6,\> x-2z=-5$ and $L_2:\> x+2y=3,\> y+2z=3 $","Find the distance between the two lines defined by : $$\mathbb L_{1}=
  \begin{cases}
                                   x+y+z=6 & \\
                                   x-2z=-5 &  \\ 
  \end{cases}$$ $$\mathbb L_{2}=
  \begin{cases}
                                   x+2y=3 & \\
                                   y+2z=3 &  \\ 
  \end{cases}$$ I know that if we have two lines : $\mathbb L_{1}=P_1+tv_1$ and $\mathbb L_{2}=P_2+tv_2$ ,then the distance is given by : $$d(\mathbb L_{1},\mathbb L_{2})=\frac{\left|\left(P_{2}-P_{1}\right)\cdot\left(v_{1}\times v_{2}\right)\right|}{\left|v_{1}\times v_{2}\right|}$$ But the problem is that thegiven equations are not in the mentioned form,and I 'm not sure even if they are line (the equations seems to be plane). So how to start?","['analytic-geometry', 'linear-algebra', 'vectors', '3d']"
3767713,"Existence of limit for sequence $x_n=\frac12\left(x_{n-1}+\frac8{x_{n-2}}\right)$ with initial values $x_0=5,x_1=10$","Let $x_0=5,x_1=10,$ and for all integers $n\ge2$ let $x_n=\frac12\left(x_{n-1}+\frac8{x_{n-2}}\right).$ By induction, we have $\forall m\in\mathbb Z_{\ge0}\enspace x_m>0,$ so we can avoid division by $0$ and the sequence is well-defined. According to a Math GRE practice problem, the limit exists. How can we prove that? Note that, if we assume the limit exists, then we can show it equals $\sqrt8,$ but finding the value of the limit is not my goal here. My work: We can compute $x_2=5.8,x_3=3.3,$ which are strictly between $4/3$ and $6,$ and then, assuming an inductive hypothesis, for all integers $n\ge4$ we have $4/3<x_{n-1}<6$ and $4/3<8/x_{n-2}<6,$ so that $4/3<x_n<6.$ We can probably compute more values of $x_n$ to get tighter bounds, but I don't see how to actually show convergence.","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
3767725,"Is a uniform distribution on a sphere always a norm-scaled normal $\mathcal{N}(0,I_d)$ distribution?","Let $U \sim Unif(S^{d-1}).$ I was wondering if it's true that, and if yes, how could we prove that: $U = \frac{Z}{\|Z\|}$ where $Z \sim \mathcal{N}(0, I_d), $ i.e. a uniform distribution on a sphere is always a norm scaled distribution? This is kind of like ""Polar decomposition in probability."" If yes, one would've to just construct $Z.$ To do so, I'd try to use the fact (the ""converse statement"") that for any $W \sim \mathcal{N}(0, I_d), \frac{W}{\|W\|}\sim Unif(S^{d-1})$ and that ${\|W\|}\sim \chi_d,$ a chi distribution with $d$ degrees of freedom. We'd also use that: $\|W\|$ and $\frac{W}{\|W\|}$ are independent (see Vershynin , Exercise 3.3.7, P.53). More precisely, I'd define $Z:= N U,$ where $N\sim \chi_d, U \sim Unif(S^{d-1}),$ and enforce the condition that $N, U$ are independent. If my ideas are so far correct, it remains to show that: 1) $NU \sim \mathcal{N}(0,I_d)$ and 2) $N =\|NU\|= \|Z\|.$ The second one is obvious, since $\|U\|=1 \implies \|NU\| = N.$ Proving 1) might be a real pain I think, given the complicated PDF of the chi random variable $N$ defined above. So how would we circumvent that problem? Should we some kind of rotational symmetry argument? Of course, if my idea were wrong, then we won't pursue this route. P.S. Just a comment: above, we're trying to construct a normal distribution, given a uniform one. The following might be related but I didn't find much information on this online, but perhaps a high dimensional version of Box-muller transform is something that'd also transform a uniform distribution into a normal distribution, except in the case, the uniform distribution has to be on an open unit cube, instead of a sphere, unlike the question, which'll make it simpler, as the co-ordinates would be independent in this case, unlike the question above.","['geometric-probability', 'probability-distributions', 'probability-theory', 'probability', 'random-variables']"
3767764,Trouble on Factorizing,"I have trouble factoring $ a^2+ab+b^2 $ .
It can be done easily using $\omega$ , which is a complex cube root of unity with non-zero imaginary part- $$ a^2+ab+b^2 = a^2 - \omega ab - {\omega}^2ab+{\omega}^3b^2 = a(a-\omega b) -\omega ^2b(a-\omega b) = (a-\omega ^2b)(a-\omega b) $$ But the following is also a way to factor- $$ a^2+ab+b^2 = (a+b)^2-ab = (a+b-\sqrt {ab})(a+b+\sqrt {ab}) $$ But why is it that this factorization not so much popular while the one with complex factors is more popular? Can anybody explain me with an example? What I think is it is probably because $\sqrt{ab}$ can't be real all the time.
Thanks!","['complex-analysis', 'factoring']"
3767796,"Compute $\iint (x+y)\,dx\, dy$ with circle constraint $x^{2}+y^{2}=x+y$","I have a double integral: $$\iint (x+y)\,dx\, dy$$ with circle constraint: $$x^{2}+y^{2}=x+y$$ I tried to calculate it with transition to polar coordinates: $$x^{2}+y^{2}=x+y$$ $$\left(x-\frac{1}{2}\right)^{2}+\left(y-\frac{1}{2}\right)^{2}=\frac{1}{2}$$ In polar coordinates: $$r^{2}(\cos(\varphi))^{2} + r^{2}(\sin(\varphi))^{2} = r\cos(\varphi) + r\sin(\varphi)$$ $$r = \cos(\varphi) + \sin(\varphi)$$ Graph looks like this: But i don't understand how to find polar radius change interval here. If i separate circle into two, for first half circle for example it will go from $\textbf{some point}$ to $\frac{\pi}{2}$ . I don't understand how to find that $\textbf{some point}$ , cause it starts from point ( $\frac{1}{2}-\frac{1}{\sqrt{2}} = -0.2071$ ).","['integration', 'multivariable-calculus', 'multiple-integral', 'polar-coordinates']"
3767810,Ellipse on complex plane,"I have found that the curve $z(x)$ on the complex plane with $$
z=\frac{ax+b}{x^2+1}
$$ at real $x$ spanning from $-\infty$ to $+\infty$ looks very similar to an ellipse at any complex parameters $a,b$ (see the example on the picture). Is there any easy proof that this is indeed the ellipse?","['conic-sections', 'geometry', 'complex-numbers']"
3767819,What is the relationship between the Leibniz integral rule and the dominated convergence theorem?,"The Leibniz integral rule states $$
{\displaystyle {\frac {d}{dx}}\left(\int _{a}^{b}f(x,t)\,dt\right)=\int _{a}^{b}{\frac {\partial }{\partial x}}f(x,t)\,dt}
$$ when the integral bounds are not a function of $x$ , i.e. the variable we take the derivative with respect to. An expectation of a continuous random variable or with respect to a continuous distribution is defined as an integral. More precisely, let $X$ be a continuous r.v. and $p(x)$ be its parametrized (by $\theta$ ) density, then we have $$
\mathbb{E}_{p_{\theta}(x)}\left[ f(x) \right] = \int p_{\theta}(x) f(x) dx
$$ In certain cases, you need to take the derivative of an expectation with respect to the parameters $\theta$ (e.g. this is common in certain machine learning problems) $$\frac{d}{d \theta}\mathbb{E}_{p_{\theta}(x)}\left[ f(x) \right]$$ So, some people, in certain papers, seem to apply the Leibniz integral rule to get \begin{align}
\frac{d}{d \theta}\mathbb{E}_{p_{\theta}(x)}\left[ f(x) \right] 
& \stackrel{?}{=}
\mathbb{E}_{p_{\theta}(x)}\left[  \frac{d}{d \theta} f(x) \right] \\
& \stackrel{?}{=}
\int \frac{d}{d \theta} \left[ f(x)  p_{\theta}(x) \right] dx \\
\end{align} Some papers that say that we can bring the derivative inside the expectation because of the dominated convergence theorem , which I am not familiar with, so I would like someone to clarify me the relationship between the Leibniz integral rule above and the dominated convergence (specifically, in the context of taking derivatives of expectations, i.e. probability theory and statistics). Is the DCT just the way to prove the Leibniz integral rule? If that's true, can you show that? Moreover, if you see above, I have $\mathbb{E}_{p_{\theta}(x)}\left[  \frac{d}{d \theta} f(x) \right] \stackrel{?}{=} \int \frac{d}{d \theta} \left[ f(x)  p_{\theta}(x) \right] dx$ , however, $p_{\theta}(x) \frac{d}{d \theta} f(x)  \neq \frac{d}{d \theta} \left[ f(x)  p_{\theta}(x) \right]$ , so I suspect I've done something wrong or that the DCT and Leibniz integral rule are not applicable to the same contexts, i.e. maybe the Leibniz integral rule is not directly applicable to expectations because they involve random variables and densities?","['integration', 'proof-explanation', 'expected-value', 'calculus', 'probability-theory']"
3767824,Prove that $f(x) \leq K \cdot\exp(L\cdot \int_a^x g(t)dt)$,"Suppose that $f, g$ are non negative continuous functions in $[a, b]$ , and $K, L$ positives constants such that $$
f(x) \leq K + L \int_a^x f(t)g(t) dt,\quad \forall x \in [a, b] .
$$ Prove that $$
f(x) \leq K\exp\left(L\int_a^x g(t)dt\right).
$$ I tried to use the continous proprierty by applying the first inequality for $x = a$ For $x = a,$ $$
f(a) \leq K \implies \exists I \subset [a, b]\ \text{such that}\  f(x) \leq 2K\ \forall x\in I
$$ However I couldn't go much further.","['inequality', 'ordinary-differential-equations', 'real-analysis']"
3767834,How can I find the smallest set of groups of $n$ elements such that every element is in the same group as every other at least once?,"Background: I'm working on a King of the Hill challenge for Programming Puzzles & Code Golf , and I've run into a problem with how I'm creating the individual matchups (groups of 4 entries). Currently, I'm simply generating all combinations of 4 elements (in the combinatoric sense) of entries, but that gets really big really quickly: $$x \choose 4$$ (from Wolfram Alpha ) Therefore, my question is: How can I choose subsets (of length $n$) of a set such that each element of that set appears in the same subset as every other element at least once, while maintaining a minimal amount of subsets? I can figure this out for $n = 2$, since the solution is quite intuitive: simply combine the first element with every other element, combine the second element with every other element that comes after it, etc. For a set of length $L$, this yields: $$\sum_{x=1}^{L-1}(L - x)$$ (from Wolfram Alpha ) which is much smaller and more manageable. However, I still have not been able to figure out: How can I generalize this to any $n$?","['elementary-set-theory', 'combinatorics']"
3767844,Show $\mathbb{P}[X-m>\alpha]\leq \frac{\sigma^2}{\sigma^2+\alpha^2}$,"I found this problem in an old statistics book: Suppose $X$ is a square integrable random variable  with mean $m$ and variance $\sigma^2$ . For any $\alpha>0$ , show $$
\mathbb{P}[X-m>\alpha]\leq\frac{\sigma^2}{\sigma^2 +\alpha^2}
$$ At first I thought that the inequality results from  direct application of Markov-Chebyshev's inequality, but when I actually tried it I realized it was not so. Does anybody know about this inequality and how to obtain it?","['statistics', 'probability-theory', 'probability']"
3767861,congruent matrices,"Show that in $M_3(\mathbb{Z}_7), \begin{pmatrix}3 & 0 & 0 \\
0 & 3 & 0\\
0 & 0 & 0\end{pmatrix} \cong \begin{pmatrix}1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix}$ but $\begin{pmatrix}3 & 0 & 0 \\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix} \not\cong \begin{pmatrix}1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix}.$ I know that to show that two matrices $A$ and $B$ are congruent, it suffices to show that for some invertible matrix $P, A = P^T B P$ . However, I am unable to find such an invertible matrix, so I was wondering if it was easier to arrive at a contradiction if I assume that no such matrix exists? To show that the given two matrices are not congruent, one way (though obviously not very generalizable) is to show that no matrix $P = \begin{pmatrix}a & b & c\\
d & e & f\\
g & h & i\end{pmatrix} \in M_3(\mathbb{Z}_7)$ can satisfy that $P^T AP = B,$ where $A = \begin{pmatrix}3 & 0 & 0 \\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix}$ and $B = \begin{pmatrix}1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0\end{pmatrix}$ (in this case this works, but it may be harder to show this if such a matrix exists, but it just isn't invertible). One can do so by manipulating a system of equations and showing that there is never a solution (e.g. one can fix a variable $d$ and consider when $d\in \{0,1,2,3,4,5,6\}$ ). So I was wondering if there was an easier approach for this?","['matrices', 'transpose', 'linear-algebra', 'modular-arithmetic']"
3767907,Challenging problem: Find $a$ where $\int_0^\infty \frac{\cos(ax)\ln(1+x^2)}{\sqrt{1+x^2}}dx=0$.,"What is the value of $a\in\mathbb{R}$ that makes the following integral true $$\int_0^\infty \frac{\cos(ax)\ln(1+x^2)}{\sqrt{1+x^2}}dx=0\,?$$ This question was proposed by my friend Khalef Ruhemi and I have no idea how to approach it but all I tried is setting $x=\tan\theta$ and I don't know how to continue after that. Also I noticed that the integrand is an even function and again I don't know how to make use of this fact. Any help would be much appreciated.","['integration', 'calculus', 'improper-integrals', 'real-analysis']"
3767932,Proof $\exists\alpha$ s.t. $P(X>\alpha)>0$ if $P(X>0)>0$,"For probability triple $(\mathbb{R}, \mathcal{B}(\mathbb{R}), \mu)$ prove that for a random variable $X$ , if $\mu(X>0)>0$ , there must be $\alpha>0$ s.t. $\mu(X>\alpha)>0$ . So if $X$ is a random variable with that property, it means that $\exists $ event $ A \in \mathcal{F}$ and interval $B=(0, \infty)$ s.t. $$
A:\{\omega \in \mathbb{R}: X^{-1}(B)= A \}, \mu(A)=m>0
$$ Since $A$ is an interval, we can set its upper and lower bounds as $\beta_1, \beta_2$ . Since $A \in \mathcal {F}$ , we can certainly find number $\alpha^{-1}$ such that there exist two disjoint intervals $A_1 \cup A_2=A$ with the same measure: $$
A=A_1 \cup A_2, A_1 = [\beta_1, \alpha^{-1}], A_2 = [\alpha^{-1}, \beta_2], \mu(A_1)=\mu(A_2)=\frac{m}{2}
$$ Obvisouly $\alpha^{-1} \in A$ , and, since $A$ is a preimage of $B,  \alpha^{-1} = X^{-1}(\{\alpha\})$ , and $\alpha \in B$ . Therefore $$
A_2 = \{\omega:X^{-1}(\alpha, \infty)\}
$$ and $\mu(X>\alpha) = \mu(A_2)=\frac{m}{2}>0$ . I think this is correct, but the hint for the problem is to use the continuity of probabilities, which I didn't.","['measure-theory', 'solution-verification', 'lebesgue-measure', 'probability-theory']"
3767945,Tic-tac-toe with one mark type,"In an $a\times b$ board, two players take turns putting a mark on an empty square. Whoever gets $c\leq \max(a,b)$ consecutive marks horizontally, vertically, or diagonally first wins. (Someone must win because we use only one mark type.) For each triple $(a,b,c)$ , who has a winning strategy? For $a=b=c=3$ (tic-tac-toe size), the first player can win by first going on the middle square and winning in the next turn. A generalization is that for $a,b,c$ are all odd, the first player can go on the middle square, then reflect the second player's move across the middle square. (He also needs to keep his eyes open in case the second player marks the $(c-1)$ st square of a $c$ -in-a-row, so that he can win immediately.) In the one-dimensional case ( $a=1$ ), this may well be a known game, but I also cannot find a reference.","['combinatorics', 'combinatorial-game-theory']"
3767957,Normal endomorphism on a group,"I was learning the Krull-Schmidt theory and came across this concept and just can't understand what's it all about. A group endomorphism $f\colon G\to G$ is called normal iff $f(aba^{-1})=af(b)a^{-1}$ for all $a,b\in G$ . It's true that $H$ is a normal subgroup of $G$ implies $f(H)$ is a normal subgroup of $G$ , given that $f$ is a normal endomorphism on $G$ . Is the converse true? E.g. Is it true that ""an endomorphism $f$ on group $G$ images every normal subgroup of $G$ to a normal subgroup"" implies "" $f$ is a normal endomorphism""? If it's not true, some other way to understand this definition would be appreciated(what does it have to do with normality?).","['normal-subgroups', 'group-theory', 'abstract-algebra']"
3768021,Preservation of convergence in measure by absolutely continuous measures,"In a paper on Risk theory that I am reading, it is stated that, unlike convergence in $L_p$ , $1\leq p<\infty$ , convergence in measure is preserved within a collection of probability measures that are absolutely continuous. That is, Suppose $\mu$ and $\nu$ are probability measures on a measurable space $(\Omega,\mathcal{F})$ and $\nu\ll \mu$ . If  the sequence $X_n$ of random variables converges to $X$ in $\mu$ -measure,   then $X_n$ converges to $X$ in $\nu$ -measure. This seems to be an easy enough problem, but I don't have a clear idea of how to start. I would appreciate any hints.","['measure-theory', 'convergence-divergence', 'probability-theory']"
3768036,Example of absolutely continuous function $f$ with $\sqrt{f}$ not absolutely continuous,"I'm looking for an example of a function $f$ that is absolutely continuous, but $\sqrt{f}$ is not absolutely continuous. I've been playing around with the Cantor-Lebesgue function, but I feel like there should be something simpler.","['continuity', 'measure-theory', 'real-analysis']"
3768042,An illusionist and their assistant are about to perform the following magic trick,"Let $k$ be a positive integer. A spectator is given $n=k!+k−1$ balls numbered $1,2,\dotsc,n$ . Unseen by the illusionist, the spectator arranges the balls into a sequence as they see fit. The assistant studies the sequence, chooses some block of $k$ consecutive balls, and covers them under their scarf. Then the illusionist looks at the newly obscured sequence and guesses the precise order of the $k$ balls they do not see. Devise a strategy for the illusionist and the assistant to follow so that the trick always works. (The strategy needs to be constructed explicitly. For instance, it should be possible to implement the strategy, as described by the solver, in the form of a computer program that takes $k$ and the obscured sequence as input and then runs in time polynomial in $n$ . A mere proof that an appropriate strategy exists does not qualify as a complete solution.) Source: Komal, October 2019, problem A $760$ . Proposed by Nikolai Beluhov, Bulgaria, and Palmer Mebane, USA I can prove that such a strategy must exist: We have a set $A$ of all permutations (what assistant sees) and a set $B$ of all possible positions of a scarf (mark it $0$ ) and remaining numbers (what the illusionist sees). We connect each $a$ in $A$ with $b$ in $B$ if a sequence $b$ without $0$ matches with some consecutive subsequence in $a$ .  Then each $a$ has degree $n-k+1$ and each $b$ has degree $k!$ . Now take an arbitrary subset $X$ in $A$ and let $E$ be a set of all edges from $X$ , and $E'$ set of all edges from $N(X)$ (the set of all neighbours of vertices in $X$ ). Then we have $E\subseteq E'$ and so $|E|\leq |E'|$ . Now $|E|= (n-k+1)|X|$ and $|E'| = k!|N(X)|$ , so we have $$ (n-k+1)|X| \leq k!|N(X)|\implies |X|\leq |N(X)|.$$ By Hall marriage theorem there exists a perfect matching between $A$ and $B$ ... ...but I can not find one explicitly. Any idea? Update: 2020. 12. 20. https://artofproblemsolving.com/community/c6t309f6h2338577_the_magic_trick https://dgrozev.wordpress.com/2020/11/14/magic-recovery-a-komal-problem-about-magic/","['contest-math', 'puzzle', 'combinatorics', 'discrete-mathematics', 'coding-theory']"
3768081,Intersection of open affines can be covered by open sets distinguished in *both*affines,"Suppose $X$ is an arbitrary scheme and $U \cong \operatorname{Spec} A$ and $V \cong \operatorname{Spec} B$ are affine upon subsets of $X$.  It's not true in general that $U \cap V$ is affine, so if we want to prove basic results about general schemes (e.g. the stuff in section II.3 of Hartshorne), we need some technical means of understanding what goes on on the intersections of open affines. I've heard that this can be done by covering $U \cap V$ with open sets which are distinguished in both $U$ and $V$, i.e. sets $W \subseteq U \cap V$ with $W \cong \operatorname{Spec} A_f \cong \operatorname{Spec} B_g$ for some $f \in A$ and $g \in B$.  I'm having trouble actually proving this. So, questions: Is this actually true? If so, how are these sets constructed?","['algebraic-geometry', 'schemes']"
3768086,$f(X_n)$ converges in probability to $f(X)$ implies $X_n$ converges in probability to $X$,"Show that $(X_n)_n$ converges in probability to $X$ if and only if for every continuous function $f$ with compact support, $f(X_n)$ converges in probability to $f(X).$ $\implies$ is very easy, the problem is with the converse. Any suggestions to begin?","['measure-theory', 'probability-theory']"
3768087,One number is removed from the set of integers from $1$ to $n.$ The average of the remaining numbers is $163/4.$ Which integer was removed?,"One number is removed from the set of integers from $1$ to $n.$ The average of the remaining numbers is $\dfrac{163}4$ .  Which integer was removed? Source. British Mathematical Olympiad 2010/11, Round 1, Problem 1 I was hoping if someone could spot the flaw in my working for this question. Attempt. I began by letting the integer that was removed be $x$ . Then: $$\frac{1 + 2 + \cdots + (x-1) + (x+1) +\cdots + n} {n-1} = \frac{163}{4}$$ There is two arithmetic sums in the denominator, the first from 1 to $x$ and the second from $x+1$ to $n$ . These are equal to $\frac{x(x-1)}{2}$ and $\frac{(n-x)(n+x+1)}{2}$ , and subbing in to first equation this gives: $$\frac{x(x-1) + (n-x)(n+x+1)}{2(n-1)} = \frac{163}{4}$$ which reduces to: $$\frac{n^2 + n - 2x}{2(n-1)} = \frac {163}{4}$$ And then: $$2(n^2 + n -2x) = 163(n-1)$$ At first I thought you could consider factors, as 163 was prime then: $n-1 = 2$ giving $n = 3$ and $n^2 + n - 2x = 163$ , which using $n=3$ gives $x= -75.5$ which isn't our positive integer. I then tried considering a quadratic in $n$ and using the discriminant but again that just looked to give a negative value of $x.$ I would be grateful for any help","['contest-math', 'elementary-number-theory', 'diophantine-equations', 'average', 'algebra-precalculus']"
