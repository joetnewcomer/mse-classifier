question_id,title,body,tags
2719590,Find all solutions to the congruence $x^3\equiv 1 {\pmod{77}}$,"Find all solutions to the congruence $x^3\equiv 1 \pmod{77}$ I was able to do this for $x^2 \equiv 1\pmod{77}$ since $x$ would either be $1$ or $-1$, in each of $\bmod 7$ and $\bmod 11$. I don't know what $x$ would be in this case. I can do the Chinese remainder theorem to find the final answers.","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
2719598,"given the Heat kernel corresponding to $e^{-t\Delta_q}$, find the kernel of the smoothing operator $Fe^{-t\Delta_q}$","In the proof of the Lefschetz formula for smooth maps $\phi\colon M \to M$ via the Heat Kernel (See Roe Elliptic Operators, Topology and asymptotic methods chapter 10) it's done the following observation: Let $\phi \colon M \to M$ a smooth map ($M$ closed smooth oriented), let $(S,d)$ be a Dirac complex, let $\zeta \colon \phi^*S \to S$ a smooth bundle map, together they define a geometric endomorpism $$ F:=\zeta\phi^* \colon C^{\infty}(S)\to C^{\infty}(S)$$ the smooth sections of my bundle $S$. Given now a smoothing operator $$P\colon L^2(S)\to C^{\infty}(S)$$ where $L^2(S)$ are the $L^2$-sections of $S$, it's claimed that $$FP\colon L^2(S)\to C^{\infty}(S)$$ is again a smoothing operator What I tried: let $k\ \colon M\times M \to S\boxtimes S^*$ be the smooth kernel of $P$ (think of $k(m_1,m_2)\in \hom(S_{m_2},S_{m_1})$), then for any section $s\in L^2(S)$, we have $$ FP(s)(m_1)=F\int_M k(m_1,m_2)s(m_2)vol(m_2)$$ Now upon carefully looking at what $F=\zeta\phi^*$ does on a section $s\colon M \to M$, We see that 
$$ FP(s)(m_1)=\zeta P(s)(\phi(m_1))=\zeta\circ \int_M k(\phi(m_1),m_2)s(m_2)vol(m_2)$$ I would like to move my bundle map $\zeta$ inside the integral . If I'm allowed to do that I think I'm able to prove (following what's done by Roe at page $135$) that we still have a smooth kernel. Maybe using partition of unity (we can take a finite cover subordinate to the trivialising cover) we can reduce everything to (a finite sum of) integrals of vector valued functions over opens $U_i$, and then $\zeta$ would be a finite matrix whose entries depend on $m_1$) which should commute with the integral. Can someone provide some feedback or ideas on how to do that? (Roe ""does"" it for the heat kernel at page 135 without any justification) Some context (especially to understand more the notation) can be found here","['functional-analysis', 'smooth-manifolds', 'differential-geometry', 'operator-theory']"
2719600,"Confusion with regards to the phrase ""exactly one of the events occurs""","Suppose we have events $A$ and $B$. We want to write the probability that exactly one of the events $A,B$ occurs in terms of $P(A),P(B)$ and $P(A \cap B)$ only My thought: Since I want only one occurring, $A$ or $B$, we must find $P(A \cup B)$ which equals $P(A) + P(B) - P(A \cap B)$.. However, on my answer sheet it says the answer is $P(A) + P(B) - 2P(A \cap B )$. Am I missing something?",['probability']
2719614,"""$f^\prime (c) = \lim_{n \to \infty} f^\prime (x_n)$"" does not imply the continuity.","Question: suppose $f : [a,b] \rightarrow R$ is a differentiable and $c \in [a,b]$. Then show there exists a sequence $\{x_n\}$ converging to $c$, $x_n \not = c$ for all $n$, such that $f^\prime (c) = \lim_{n \to \infty} f^\prime (x_n)$. Do note this does not imply that $f^\prime$ is continuous (why?). Attempt: Let $x_n \in [a,b]$ Then, $f(x_n)$ is differentiable.  Then, $f^\prime (c) = f^\prime(\lim_{n \to \infty}x_n)=\lim_{n \to \infty}f^\prime(x_n)$. I don't understand why this does not imply continuity because if $f(c)=\lim_{n \to \infty}f(x_n)$, $f$ is continuous at $c$. Isn't it the case for the derivative function? Thank you in advance. Edit: I agree that my attempt above assumes that $f^\prime$ is continuous, which might not be true. So, could you give me some hint to solve this problem without using that assumption? Let $x_n \in [a,b]$. Then, $f(x_n)$ is differentiable. Then, $\lim_{x_n \to c} \frac {f(x_n)-f(c)}{x_n-c}=f^\prime (c)$.I don't know how to proceed from here.","['derivatives', 'real-analysis']"
2719618,Proof of Zeta Function product formula,"In my Complex Analysis classes I was studying the Riemann's zeta function. At some point my teacher was able to demonstrate the product formula in this way: ""Suppose M and N are positive integers with M>N. Observe now that, by the fundamental theorem of arithmetc, any positive integer $ n \le N$ can be written uniquely as a product of primes, and that each prime that occurs in the product must be less than or equal to N and repeated less than M times. Therefore:"" $$\zeta(s)\le \prod_{p \le N} (1+p^{-s}+p^{-2s}+...+p^{-Ms}) \mbox{    
       (1)}$$, p primes. Letting N tend to infinity, we have: $$\zeta(s)\le \prod_{p}\frac{1}{1-p^{-s}}$$
And then, using a similar argument with the fundamental theorem of arithmetic: $$\prod_{p \le N} (1+p^{-s}+p^{-2s}+...+p^{-Ms}) \le \zeta(s) \mbox{   
  (2)}$$
And conclude that: $$\zeta(s)=\prod_{s} (\frac{1}{1-p^{-s}})$$ Assuming $s$ real But I'm not understanding how to get the inequalities (1) and (2). Can someone explain it to me?","['complex-analysis', 'riemann-zeta', 'prime-numbers']"
2719621,Is there a short notation for function composition?,"So I think that if there exists two functions $f_1 \colon X \to Y$ and $f_2 \colon Y \to Z$ you can notate their composition using $(f_1 \circ f_2)(x) = f_3(x)$ right? If I have many functions that I need to compose, is there some shorter notation I can use such as $f_3 \colon X \to Y \to Z$ or something? The first example can get tedious when there are a lot of functions/mappings. I already thought of putting all the sets in a some auxiliary set $V = (X_1,...X_n)$ and then defining $f_i \colon X_i \to X_{i+1}$ with $0 < i \leq n$ so I can notate composition like so $(f_1 \circ...\circ f_n)(x)$. But some functions are partial so I need to explicitly use the partial function arrow with a bar sometimes in this format $f_3 \colon X \to Y \to Z$. Would have demonstrated the bar arrow here but needs extra package installed. Edit: In the context of the paper I'm writing, I'm explaining the specific ways in which the functions work with words because its computer science work that is hard to express succinctly with maths notation, but I would still like to show the relationship between the these functions using function notation. I assumed this was the best way to show this relationship. I'm open to the suggestion of alternative notations though.","['elementary-set-theory', 'function-and-relation-composition', 'notation', 'functions']"
2719622,Why do people study algebraic extension?,"Yesterday, I learned Kronecker’s theorem and a finite extension. And now I’m studying the next chapter, Algebraic extension. I think the next theorem shows how important algebraic extension is Let $E$ be an extension field of a field $F$. Let $α$ be an element of $E$. If $α$ is algebraic over $F$, there exists a unique monic irreducible polynomial $p(x)$ in $F[x]$ such that $p(α)=0$ in $E$. But I’m wondering there is another important reason why we should study algebraic extensions. Thanks for your help.","['abstract-algebra', 'extension-field', 'field-theory']"
2719673,Is there a way to sum $\sum_{n=1}^{\infty}\left[ \frac{y}{n} - \tan^{-1}\left( \frac{y}{n} \right)\right]$,"I'm interested in computing the sum for $y>0$:
$$
\sum_{n=1}^{\infty}\left[ \tan^{-1}\left( \frac{y}{n} \right) - \frac{y}{n} \right]
$$ In formula (42.1.6) of Hansen's A Table of Series and Products, there is a formula:
$$
\sum_{n=-\infty\\(n\neq0)}^{\infty}\left[ \tan^{-1}\left( \frac{y}{n+x} \right) - \frac{y}{n} \right] \ = \ \tan^{-1}\big( \tanh(\pi y) \cot(\pi x) \big) - \tan^{-1}\left( \frac{y}{x} \right)
$$ In taking $x \to 0^{+}$, the above formula becomes the following:
$$
\sum_{n=-\infty\\(n\neq0)}^{\infty}\left[ \tan^{-1}\left( \frac{y}{n} \right) - \frac{y}{n} \right] \ = \ 0
$$ Which simplifies to $\sum_{n=1}^{\infty}\left[ -\tan^{-1}\left( \frac{y}{n} \right) + \frac{y}{n} \right] + \sum_{n=1}^{\infty}\left[ \tan^{-1}\left( \frac{y}{n} \right) - \frac{y}{n} \right] = 0$, which is obvious. Is there a way to sum this series?","['summation', 'sequences-and-series']"
2719675,Is this proof that every strictly increasing $f\colon\mathbb{R\to R}$ is injective flawed?,"The problem is as follows: A function $f:\mathbb{R}\to\mathbb{R}$ is called strictly increasing if $$\forall x,y\in\mathbf{R},\ x<y\implies f(x)<f(y).$$ Show that any strictly increasing function is injective. The provided solution is as follows: $\ \ $ $\text{S}\scriptstyle{\text{OLUTION}}:$ Suppose that $x_1,x_2\in\mathbb{R}$ are such that $f(x_1)=f(x_2)$. Then it is not true that $x_1<x_2$ (for then $f(x_1)<f(x_2)$) and so $x_1\geq x_2$. Similarly $x_2\geq x_1$, and so $x_1=x_2$. Thus $f$ is injective. I've been pondering over this proof, and it seems to me that it might be logically incorrect. In the definition of a strictly increasing function, we have that $x < y \implies f(x) < f(y)$. So we have that $A \implies B$, where $A = x < y$ and $B = f(x) < f(y)$. So it is a one-way implication . Then, in the solution proof, it begins by assuming that $f(x_1) = f(x_2)$. But it then claims that this implies that it is not true that $x_1 < x_2$, since we would otherwise have had $f(x_1) < f(x_2)$. However , as I just stated, the definition for a strictly increasing function is provided as $x < y \implies f(x) < f(y)$ -- a one-way implication from $x < y$ to $f(x) < f(y)$. But isn't the solution proof attempting to use $f(x) < f(y) \implies x < y$ instead, since it is using the fact that we have $f(x_1) = f(x_2)$ to claim that it is then not true that $x_1 < x_2$? So, in order for this solution proof to be correct, wouldn't the definition provided for a strictly increasing function have to also include the reverse one-way implication, so that we would have $x < y \iff f(x) < f(y)$? I would greatly appreciate it if people could please take the time to clarify this. If I'm misunderstanding anything, I would appreciate an explanation so that I may understand where my misunderstanding lies.","['real-analysis', 'functions', 'proof-verification']"
2719736,Finding canonical coordinates for a given symplectic form in $\Bbb S^2$.,"Consider in $\Bbb S^2$ the symplectic form $\omega \in \Omega^2(\Bbb S^2)$ given by $\omega_p(v,w) = \langle p, v \times w\rangle$ (that is, the usual area form). If $f \in \mathcal{C}^\infty(\Bbb S^2)$ is positive, then $\omega^f \doteq f\omega$ is also a symplectic form. I would guess that there is a smart way to find coordinates $(\theta,\varphi)$ in $\Bbb S^2$ so that $\omega^f = {\rm d}\theta \wedge {\rm d}\varphi$, but I'm a bit at a loss of how to do that. Attempt: (?) For $f = 1$, $\omega^f = \omega$ is the usual area form, and $x = \sqrt{1-z^2} \cos \theta$, $y = \sqrt{1-z^2}\sin \theta$ does the job . So I tried calling $$\begin{cases} x(\theta,\varphi) \doteq g(\varphi) \cos \theta \quad{\rm and} \\ y(\theta, \varphi) = g(\varphi) \sin \theta, \end{cases}$$with no restriction on $z = z(\theta,\varphi)$, a priori. The condition $g(\varphi)^2 + z(\theta,\varphi)^2 = 1$ gives $\partial z/\partial \theta = 0$. With this 
I painstakingly computed the pull-back by expressing ${\rm d}x, {\rm d}y$ and ${\rm d}z$ in terms of ${\rm d}\theta$ and ${\rm d}\varphi$, and substituting in $$\omega^f = fx\,{\rm d}y \wedge {\rm d}z + fy\,{\rm d}z\wedge{\rm d}x + fz\,{\rm d}x \wedge {\rm d}y,$$and got the relation $$f g^2 \frac{\partial z}{\partial \varphi} - fz gg' = 1.$$Since the condition $g(\varphi)^2 + z(\theta,\varphi)^2 = 1$ also gives $gg' + z \partial z/\partial \varphi = 0$, that simplifies to $$f\frac{\partial z}{\partial \varphi} = 1.$$ This smells bad. Is my attempt too wrong? What is the intelligent way to solve the problem? It is exercise 5.1.1 here in case you're too curious. I'm in that stage of learning that you go around scavenging for nice exercises.","['symplectic-geometry', 'differential-forms', 'differential-geometry', 'partial-differential-equations']"
2719738,Show that $ f^2 − g^2 = C $ for some real constant C,"Suppose $f$ and $g$ are continuously differentiable functions such that $f(x) = g'(x)$ and $g(x) = f'(x)$ and that any product of $f, f', g$ and $g'$ is commutative for all $x ∈ R$. Show that $ f^2 − g^2 = C $ for some real constant C I have actually no clue how to solve this, and would be really greatfull for all the help i can get","['derivatives', 'calculus', 'functions']"
2719747,"Evaluating $\int_{-\pi}^{\pi}\frac{1}{ai+b\cos(x)}dx$ for real $a,b$","I want to do following integrals
\begin{align}
\int_{-\pi}^{\pi}\frac{1}{ai+b\cos(x)}dx
\end{align}
where $a,b$ are real Frist my trial was using the ideas of complex analysis, but here I don't know whether the poles are inside $|z|<1$ or not. (Since i didn't fix the magnitude of a and b) any ideas? To this integral be finite, do i have to restrict the magnitude of $|a|$ and $|b|$ (i.e. |a|<|b|) For the simple case, via mathematica i can obtain some results, for example setting a=1 or b=1 case. I want to know how to calculate such integrals. For $a+b\cos(\theta)$ case, introducting complex variables or parametrizing cos(x) into functions of $tan^2(x/2)$ i can do the integral without any problem, but i want to do it in more general","['complex-analysis', 'improper-integrals', 'integration', 'complex-integration']"
2719755,How good can you approximate a continuous distribution by replacing trailing digits with uniformly random digits?,Let's say I'm in the business of sampling from the standard normal distribution. And let's ignore floating point issues and other numerical inaccuracies for the moment. Say I want to be lazy and instead of drawing an exact sample from the normal distribution I only get the first 3 digits beyond the decimal point right. After that I just add uniformly random digits. It seems to me that the random number I'm sampling in this fashion will still be fairly close to normally distributed. But in what sense? What kind of formal statement could one make here?,"['probability', 'probability-distributions']"
2719791,"If $f$ preserves orientation locally, then $f$ preserves orientation globally","Suppose $f:X\to Y$ is a diffeomorphism of connected oriented manifolds
  with boundary. Show that if $df_x: T_x(X)\to T_{f(x)}(Y)$ preserves
  orientation at one point, then $f$ preserves orientation globally. What I can see is that since $f$ is a diffeomorphism, each $df_x$ is an isomorphism. But the concept of orientation preserving linear map remains not so clear to me (see this question ). Is the fact that $df_x$ preserves orientation amounts to saying that $\det df_x > 0$? After that, am I supposed to use some kind of continuity of $x\mapsto df_x$?","['real-analysis', 'differential-topology', 'manifolds', 'differential-geometry', 'linear-algebra']"
2719817,Proof of the Central Limit Theorem using moment generating functions,"Below is a method of proving the Central Limit Theorem using moment generating functions. Let $$X_{1},X_{2},...,X_{n}$$ be a sequence of i.i.d. random variables with expected value and variance $$E(X_{i}) = \mu < \infty,  Var(X_{i})=\sigma ^{2}< \infty.$$ Now let $$Z_{n}=\frac{\overline{X}-\mu }{\frac{\sigma }{\sqrt{n}}} = \frac{X_{1}+X_{2}+...+X_{n}-n\mu }{\sigma \sqrt{n}}.$$ We want to show that $$\lim_{n \to \infty} M_{Z_{n}}(t)=e^{\frac{t^{2}}{2}}$$ where $M_{X}(t)$ is the moment generating function over some finite interval.  In order to prove this, we can define a new random variable, $Y_{i}$, which is the normalized version of $X_{i}$.  Thus, $$Y_{i}=\frac{X_{i}-\mu }{\sigma }.$$ Then, we can say that $Y_{i}$ is i.i.d. with expected value and variance $$E(X_{i}) =0,  Var(X_{i})=1.$$ Using this information, we have $$Z_{n}=\frac{\overline{Y}-\mu }{\frac{\sigma }{\sqrt{n}}} = \frac{Y_{1}+Y_{2}+...+Y_{n} }{\sqrt{n}}.$$ Finding the moment generating function gives $$M_{Z_{n}}(t)=E[e^{t\frac{Y_{1}+Y_{2}+...+Y_{n} }{\sqrt{n}}}] =E[e^{t\frac{Y_{1}}{\sqrt{n}}}]\cdot E[e^{t\frac{Y_{2}}{\sqrt{n}}}]\cdot ...\cdot E[e^{t\frac{Y_{n}}{\sqrt{n}}}]= M_{Y_{1}}(\frac{t}{\sqrt{n}})^{n}.$$ Lastly, $$\lim_{n \to \infty} M_{Z_{n}}(t)=\lim_{n \to \infty} M_{Y_{1}}(\frac{t}{\sqrt{n}})^{n} = e^{\frac{t^{2}}{2}}.$$ This concludes the proof.  However, how does one show analytically that this final limit does indeed equal $$e^{\frac{t^{2}}{2}}?$$","['statistics', 'central-limit-theorem', 'moment-generating-functions', 'limits']"
2719854,What connects Fibonacci and Lucas numbers?,"The Fibonacci numbers $F_n$ and Lucas numbers $L_n$ both satisfy the recurrence relation $x_n=x_{n-1}+x_{n-2}$. The Fibonacci numbers start $0,1$ while the Lucas numbers start $2,1$. I would like to know about the theory underpinning them, in particular, the analogies with differential equations ($\sin$ and $\cos$) and how it works for for higher order (linear, homogeneous, constant-coefficient) difference equations. I have many questions Are the two sequences linearly independent? Orthogonal? Given $F_0=0$ and $F_1=1$ does this define $L_n$? How? $\sin$ and $\cos$ have $0,1$ and $1,0$ as $f(0)$ and $f'(0)$, respectively. Can we view $F_n$ and $L_n$ similarly? On the page: http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/lucasNbs.html they say that (2,1) is chosen because they are the two ""simplest"" (positive) numbers which don't produce the Fibonacci numbers (or shifted versions of them). Also, they seem to exclude (0,2) which produces the Fibonacci numbers. (I need to to read the following page http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/fibGen.html#gstart to work out what ""simplest"" really means and why the choice (0,1) and (2,1) give rise to so many nice properties.)","['recurrence-relations', 'fibonacci-numbers', 'ordinary-differential-equations']"
2719868,Operator norm is equal to max eigenvalue,"Take a matrix $A \in M_{2 \times 2}(\mathbb{R})$ and consider the norm $\vert\vert A\vert\vert = \sup\limits_{x \in \mathbb{R}^2} \frac{ \vert\vert Ax\vert\vert}{\vert\vert x \vert\vert} = \sup\limits_{x, \vert\vert x \vert\vert = 1} \vert\vert Ax\vert\vert$ . I am unable to see that the norm must be less than or equal to the maximum eigenvalue of $A$ : $\vert\vert A\vert\vert \le \max\limits_{\lambda \in \sigma(A)} \lambda$ and I am also unable of characterizing the type of $A$ such that: $\vert\vert A\vert\vert = \max\limits_{\lambda \in \sigma(A)} \lambda$","['matrices', 'normed-spaces', 'linear-algebra']"
2719890,"Is there a different approach to evaluate $\int \ln(x)\,\mathrm{d}x?$","The usual method of evaluating $\int \ln(x)\,\mathrm{d}x$ requires you to rewrite it as 
$$
\int \ln(x) \cdot \color{brown}1\,\mathrm{d}x
$$
and apply integration by parts. Letting $u=\ln(x)$ and $\mathrm{d}v=\color{brown}1 \,\mathrm{d}x$, we get that $\mathrm{d}u=\frac{1}{x} \,\mathrm{d}x$ and $v=\int \color{brown}1 \,\mathrm{d}x = x$, so our integral becomes \begin{align}
\int \ln(x) \cdot \color{brown}1\,\mathrm{d}x
&=uv-\int v\,\mathrm{d}u
\\[0.5em]
&=x\ln(x)-\int x\cdot\frac{1}{x}\,\mathrm{d}x
\\[0.5em]
&=x\ln(x)-\int \mathrm{d}x
\\[0.5em]
&=x\ln(x)-x+c
\end{align} After looking through many calculus books, this is the only method I've found to integrate $\ln(x)$. Are there any other methods one could use to integrate the function $\ln(x)?$","['logarithms', 'indefinite-integrals', 'integration', 'calculus']"
2719901,How to extend an orthonormal set to a basis on a Hilbert space?,"I've seen, during some proofs, in many texts an argument as the following: Consider $x\in H,$ $x\neq 0,$ $H$ a complex Hilbert space. The orthonormal set $\{\frac{x}{||x||}\}$ can be extended to an orthonormal basis of $H.$ Another kind of is: if $\{e_{1},\ldots,e_{n}\}$ is an orthonormal set, then such set can be extended to an orthonormal basis. I know that every Hilbert space has orthonormal basis. Even more, if $H$ is separable, then every orthonormal basis has to be numerable. I was thinking, in the last case, if we have an independent set, we can use Gram-Schmidt process to get the desire basis, but what about in the above cases?
I begin to believe in the use of Zorn's lemma, but I'm not sure. Any kind of help is thanked in advanced.","['functional-analysis', 'hilbert-spaces']"
2719936,"Proving that a plane triangulation is 3-colorable iff it's even, using faces?","My textbook gave the theorem that a plane triangulation is 3-vertex-colorable if and only if all its vertices were of even degree. It offered a proof using that the graph is 2-face-colorable, which I do not understand. The book basically states that all the faces that were colored red would be labelled a, b, c clockwise, and all the faces colored blue would be labelled a, b, c counterclockwise, and that ""this vertex coloring can be extended to the whole graph"", thus proving the theorem. I feel like I'm missing something because this doesn't really seem to be a proof so much as an example. Is there some theorem regarding ""oriented"" labels, or some name on this concept I can read up more on?","['combinatorics', 'graph-theory', 'coloring']"
2719953,How to prove the complement $P^\perp$ of a projection matrix $P$ have relation $I-P=P^\perp$,"I want to know how to prove that for a projection matrix $P$ and its complement matrix $P^\perp$. We have
$$I-P=P^\perp$$ I do know the intuition that $P$ and $P^\perp$ project a vector into two different subspaces. But can we prove it in a algebra way? P can be represented by $$P=U_1U_1^T;P^\perp=U_2U_2^T$$, in which $$\begin{bmatrix}U_1&U_2\end{bmatrix}$$ is an orthogonal matrix.","['matrices', 'orthogonal-matrices', 'projection-matrices', 'linear-algebra']"
2720021,differentiability of characteristic function - additional question,"In Wikipedia, we have the theorem about characteristic functions: Provided the n'th moment exist, the characteristic function can be differentiated n times and: $$ E[X^n] = i^{-n} \phi_X^{(n)}(0) = i^{-n} \left[ \frac{d^n}{dt^n} \phi_X(t) \right]_{t=0}.$$ My additonal question is: Is it possible to conclude the opposite, with some restrictions about $X$?
If $X$ is a non-negative random variable, can we conclude: $\phi_X$ is differentiabel in $0$ $\Rightarrow$ The first moment exist? I have a result which states:
Let $(X_n)_{n\in\mathbb{N}}$ be a sequence of identically distributed random variables on $(\Omega, F, P)$, and assume $E[X_1^-]<\infty$ and $E[X_1^+]=\infty$. Then for every $M \in (0, \infty)$ we have, that: $$\text{min}\{X_1,M\}\in \mathcal{L}^1(P) \text{ and } \frac{1}{n}\sum_{k=1}^{n} \text{min}\{X_k,M\} \geq E[\text{min}\{X_1,M\}]$$ I tried to use this, but haven't gotten far.","['characteristic-functions', 'probability-theory', 'measure-theory']"
2720042,Dimension as a real manifold vs dimension as a real variety.,"Let $X$ be a smooth $\mathbb{R}$ irreducible algebraic set, where the closed sets zero sets in $\mathbb{R}^n$ of real polynomials. I am trying to understand why dimension of $X$ as a manifold equals 
the dimension defined topologically (given as the length of a maximal chain of irreducible algebraic sets). Any comments are appreciated. Thank you.","['differential-geometry', 'algebraic-geometry']"
2720088,Find the integral $\int \frac{11x-18}{x^{11} \sqrt[6]{x^{12}-2x+3}}dx$,"$$\int \frac{11x-18}{x^{11} \sqrt[6]{x^{12}-2x+3}}dx$$ I am trying to solve using $$x^{12}-2x+3=t \implies  (12x^{11}-2)dx=dt$$ I am facing the problems, can anyone help me?","['integration', 'calculus']"
2720108,L2 norm of a function w.r.t. its gradient,"I was wondering whether it is possible to generalise the statement of this post , i.e. I would like to show something like $$\|f\|^2_{L^2(\Omega)}\leq C \|\nabla f\|^2_{L^2(\Omega)},$$ where $\Omega=[0,1]^n$ and $f:\Omega\to\mathbb{R}$ is a suitable function with $f=0$ on the vertices of $\Omega$ (and $C$ does not depend of $f$ ). Although this seems to be a standard question I was not able to find anything on this for $n\geq2$ . My approach was the following (given that everything is well-defined). By $f(0)=0$ we know that: $$f(x)=\int^1_0\langle\nabla f(hx),x\rangle dh=\int^{|x|}_0\langle\nabla f(uw),w\rangle du,$$ where $w=x/|x|$ . If I am not mistaken, using polar coordinates (cf. this post ) should give the identity $$\|f\|^2_{L^2(\Omega)}=\int^{\rho_n}_0\int_{A_n}f(rw)^2dwr^{n-1}dr$$ with $A_n=S^{n-1}\cap[0,1]^n$ , $S^{n-1}$ sphere and $\rho_n=\sqrt{n}$ being the ""diagonal"" (or diameter) of $\Omega$ . Next I would like to combine the two preceding equalities to obtain the desired inequality. This should give (if I did not mix anything up with the gradient) the equation $$\|f\|^2_{L^2(\Omega)}=\int^{\rho_n}_0\int_{A_n}\Big(\int^r_0\langle\nabla f(uw),w\rangle du\Big)^2dwr^{n-1}dr.$$ Cauchy-Schwarz now yields $$\|f\|^2_{L^2(\Omega)}\leq\int^{\rho_n}_0\int_{A_n}\int^r_0|\nabla f(uw)|^2dudwr^ndr$$ but I do not see how I could end up with $\|\nabla f\|^2_{L^2(\Omega)}$ since I would need the term $u^{n-1}$ to pop-up somewhere. Any ideas are appreciated especially for the special case $n=2$ . In particular, I would like to consider Sobolev functions with $s\in(1,2)$ to obtain $$\|f\|^2_{L^2(\Omega)}\leq C \|\nabla f\|^2_{L^2(\Omega)}\leq C' \sum_{|\alpha|=1}\| f^{(\alpha)}\|^2_{H^{s-1}(\Omega)},$$ such that $C'$ does not depend on $f$ and I think that I already showed that the second inequality holds.","['normed-spaces', 'functional-analysis', 'multivariable-calculus', 'measure-theory', 'sobolev-spaces']"
2720122,"Does the curve $(t^2, t^5)$ have a tangent at the origin?","My question is whether or not the curve $x(t) = t^{2}, \ y(t) = t^{5}$ has a tangent at $(x, y) = (0, 0)$. I don't really know what to do if both $dy = dx = 0$, so I tried to take the limit $\lim_{t \to 0} \frac{dy}{dx} = \lim_{t \to 0}\frac{5t^{4}}{2t}$ which is zero. But does this mean the curve has a tangent, with slope $0$? My book is very unclear with this situation. Thanks!","['multivariable-calculus', 'tangent-line', 'derivatives']"
2720132,Can someone explain if this is a different function?,"I have a function which looks like: $$\frac{x+10}{(x+10) (x - 9) (x - 5)}$$ The domain of the function for input x is any number except -10, 9, and 5, because it would be dividing by zero. The thing I don't quite get is that if I were to cancel out the (x+10)'s $$\frac{1}{(x-9)(x-5)}$$ I'm told the function has changed. I know that now the domain includes -10, when before it didn't, but this seems a bit paradoxical to me because If I were to do: $$\frac{x+10}{(x+10) (x - 9) (x - 5)} = a$$ then $$\frac{1}{(x - 9) (x - 5)} also = a$$ Is there a way a someone could explain this to me because it seems a bit paradoxical. The previous two equations are equal, but the functions are not? Is it the same function but just with a different domain or is it a completely different function?","['fractions', 'functions']"
2720188,"Showing that the sequence $\Big(\frac{e^{int}}{\sqrt{2\pi}}\Big)_{n=1}^{\infty}$ is an orthonormal basis for $L^2 ((-\pi, \pi))$","If we take the inner product $\langle f, g \rangle = \displaystyle\int_{-\pi}^{\pi} f(t) \overline{g(t)} dt$ on $L^2 ((-\pi, \pi))$, which allows functions to $\mathbb{C}$, then it's not hard to check that $\Big(\frac{e^{int}}{\sqrt{2\pi}}\Big)_{n=1}^{\infty}$ is an orthonormal sequence in the space, but I'm having trouble showing it to be an orthonormal basis -- i.e. a complete orthonormal sequence. I tried using the characterisation of complete orthonormal sequences telling us that for any $f \in L^2 ((-\pi, \pi))$, $$\displaystyle\sum_{n=1}^{\infty}|\langle f, e_n \rangle|^2 = ||f||^2,$$ where we define $e_n =\frac{e^{int}}{\sqrt{2\pi}},$ but the products on the left were difficult because they're not as easily transformable into ordinary complex line integrals as $\langle e_m, e_n \rangle$ is . Trying to use the definition -- where $\langle f, e_n \rangle = 0$ $\forall n \in \mathbb{N}$ $\implies f=0$ -- but that presents the same difficulty. Is there a simple way of proving the completeness of this sequence?","['functional-analysis', 'orthonormal', 'lp-spaces', 'inner-products']"
2720200,How to prove the real points of affine variety defined over the reals is a differentiable manifold?,"Let $V$ be an affine algebraic set defined as a zero set of real polynomials. Then $V \cap \mathbb{R}^n \backslash V^*$ defines a differentiable manifold according to Wikipedia https://en.wikipedia.org/wiki/Differentiable_manifold . Here $V^*$ is the set of singular points. 
I have been taking this fact for granted, but realized not sure how to actually prove it. I would greatly appreciate comments and references. PS I would greatly appreciate a reference for this also!","['reference-request', 'differential-geometry', 'algebraic-geometry']"
2720218,Sum of reciprocals of odd numbers that add up to $2$,"It is well known that certain sums of reciprocals add up to $2$: the reciprocals of powers of $2$: $$
{1 \over 1}+{1 \over 2}+{1 \over 4}+{1 \over 8}+{1 \over 16}+{1 \over 32}+\cdots =2,
$$ the reciprocals of the triangular numbers: $$
{1 \over 1}+{1 \over 3}+{1 \over 6}+{1 \over 10}+{1 \over 15}+{1 \over 21}+\cdots =2,
$$ $\ldots$ Are there any sums such that $\sum_{i=0}^{\infty} s_i^{-1}=2$ for which all the $s_i$ are all distinct odd positive integers? Similarly, are there any sums such that $\exists k\in\mathbb{Z}, k>0$ such that $\sum_{i=0}^{k} s_i^{-1}=2$ for which all the $s_i$ are all distinct odd positive integers?","['sequences-and-series', 'elementary-number-theory']"
2720250,Domain of definition of function $\frac{1}{\tan x}$,"We consider the real valued function $$f: x \mapsto \frac{1}{\tan x}$$ we know that the domain of definition of $x \mapsto \tan x$ is  $D_1=\mathbb{R}-\{\frac{\pi}{2}+k\pi, ~k\in \mathbb{Z}\}$,  we have $\frac{1}{\tan x}=\cot x$ moreover the domain of definition of $x \mapsto \cot x$ is  $D_2=\mathbb{R}-\{k\pi, ~k\in \mathbb{Z}\}$ so now when we want to determine the domain of definition of $f$ do we have to consider only $D_2$ of $\cot x$ since $f(x)=\cot x$ or $D_1\cap D_2$ since $f(x)=\frac{1}{\tan x}= f_1\circ f_2 (x)$ where $f_1= \frac{1}{x}$ and $f_2(x)=\tan x$?
Thanks","['algebra-precalculus', 'trigonometry', 'calculus']"
2720284,"Short exact sequence of functors which splits, but not naturally (revisited, motivated by the Universal Coefficient Theorem)","I already took a look at this and this post, but I still don't really understand what the definition of a naturally split sequence is (which is mentioned in the Universal Coefficient Theorem ). I tried to figure it out by myself, but it seems like I found something contradictory in my thoughts. Let me elaborate these: Let $A,B,C: R\text{-MOD} \to R\text{-MOD}$ be covariant functors such that
$ 0 \xrightarrow{} A \xrightarrow{\eta} B \xrightarrow{\psi} C \xrightarrow{} 0 $ forms a natural short exact sequence and a (not necessarily naturally) split exact sequence . Furthermore, let $f:X\to Y$ be an $R$-module homomorphism. We can take a look at the diagram $$
\newcommand{\ra}[1]{\kern-1.5ex\xrightarrow{\ \ #1\ \ }\phantom{}\kern-1.5ex}
\newcommand{\ras}[1]{\kern-1.5ex\xrightarrow{\ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex}
\newcommand{\da}[1]{\bigg\downarrow\raise.5ex\rlap{\scriptstyle#1}}
\newcommand{\ua}[1]{\bigg\uparrow\raise.5ex\rlap{\scriptstyle#1}}
\newcommand{\id}{\operatorname{id}}
\begin{array}{c}
0 & \ra{} & A(X) & \ra{i_X} & A(X) \oplus C(X) & \ra{p_X} & C(X) & \ra{} & 0 \\
& & \ua{\id_{A(X)}} & & \ua{\theta_X} & & \ua{\id_{C(X)}} \\
0 & \ra{} & A(X) & \ra{\eta_X} & B(X) & \ra{\psi_X} & C(X) & \ra{} & 0 \\
& & \da{A(f)} & & \da{B(f)} & & \da{C(f)} &  \\
0 & \ras{} & A(Y) & \ras{\eta_Y} & B(Y) & \ras{\psi_Y} & C(Y) & \ras{} & 0 \\
& & \da{\id_{A(Y)}} & & \da{\theta_Y} & & \da{\id_{C(Y)}} \\
0 & \ra{} & A(Y) & \ra{i_Y} & A(Y) \oplus C(Y) & \ra{p_Y} & C(Y) & \ra{} & 0 \\
\end{array}
$$ where $i$ denotes the natural embedding and $p$ denotes the natural projection. The upper and lower part of this diagram commutes because of 2. and the middle part is commutative because of 1. Hence, the whole diagram commutes. Since this diagram commutes, we get a commutative diagram $$
\newcommand{\ra}[1]{\kern-1.5ex\xrightarrow{\ \ #1\ \ }\phantom{}\kern-1.5ex}
\newcommand{\ras}[1]{\kern-1.5ex\xrightarrow{\ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex}
\newcommand{\da}[1]{\bigg\downarrow\raise.5ex\rlap{\scriptstyle#1}}
\newcommand{\ua}[1]{\bigg\uparrow\raise.5ex\rlap{\scriptstyle#1}}
\newcommand{\id}{\operatorname{id}}
\begin{array}{c}
 A(X) & \ra{i_X} & A(X) \oplus C(X) & \ra{p_X} & C(X)   \\
 \da{A(f)} & & \da{\theta_Y \circ B(f) \circ \theta_X^{-1}} & & \da{C(f)} \\
 A(Y) & \ra{i_Y} & A(Y) \oplus C(Y) & \ra{p_Y} & C(Y)   \\
\end{array}.
$$ One can show that such a commutative diagram must lead to $\theta_Y \circ B(f) \circ \theta_X^{-1} = A(f) \oplus C(f)$, so $\theta_Y \circ B(f)  = \left(A(f) \oplus C(f)\right) \circ \theta_X$. But this shows that $\theta: B \to A \oplus C$ is a natural isomorphism which is the definition of naturally split, isn't it? But this seems to be wrong since the Universal Coefficient Theorem states that there is a split exact sequence which does not split naturally. Could you explain me what's wrong with my reasoning? Thank you.","['algebraic-topology', 'abstract-algebra', 'category-theory', 'homological-algebra']"
2720287,"Contour integration problem, proving complex function is bounded","I've got a complex function:
$$f(z)=\frac{\pi\cot{\pi z}}{z^2}$$
I want to integrate it around a contour $\Gamma_N$ such that the poles at $$-N,-N+1,-N+2\cdots-1,0,1,\cdots N-2, N-1, N$$ are contained in the contour, so i choose a circle with radius $N+\varepsilon$ where $N\in\Bbb{N}, \varepsilon \in (0,1)$ My goal is to show that the integral for $N\to\infty$
$$\oint_{\Gamma_N}f(z)dz=0$$
In fact, it must equal to zero because by residue theorem
$$\oint_{\Gamma_N}f(z)dz=\operatorname{Res}(f,0)+\sum_{n=-N,n\neq0}^Nn^{-2}$$
the residuum at $0$ is equal to $-\pi^2/3$ and for $N\to\infty$ this solves the basel problem. Returning to the contour integral:
$$\oint_{\Gamma_N}f(z)dz=\int_0^{2\pi}f((N+\varepsilon)e^{i\varphi})d(N+\varepsilon)e^{i\varphi}=$$
$$=\int_0^{2\pi}\frac{\pi\cot({\pi(N+\varepsilon)e^{i\varphi})}}{(N+\varepsilon)e^{i\varphi}}id\varphi$$
Now i would prove that the limit as $N\to\infty$ of the integrand goes to zero, namely:
$$\mathcal L= \lim_{N\to\infty}\frac{\cot{(\pi(N+\varepsilon))}}{(N+\varepsilon)}=\lim_{N\to\infty}\frac{\cos{(\pi(N+\varepsilon))}}{(N+\varepsilon)\sin{(\pi(N+\varepsilon)})}$$
and the argument is: $$\lim_{N\to\infty}\frac{\cos{(N+\varepsilon)}} {N+\varepsilon}=0$$ by squeeze theorem and $$\sin(\pi(N+\varepsilon))$$ is never equal to zero, because $$(N+\varepsilon)\notin\Bbb{N}$$ This would prove that integrand is zero therefore the contour integral is equal to zero. My question is, is my argument correct? Thanks for any advice.","['complex-analysis', 'contour-integration', 'upper-lower-bounds', 'limits']"
2720289,natural manifold structure on the cotangent bundle,"If $M$ is a differential manifold with an atlas $\{(U,\varphi)\}$ and $T^*M$ is the cotangent bundle. Then what is the natural atlas on $T^*M$? I would be inclined to say something like this: $\{(V,\psi)\}$ where $V:=U\times \mathbb{R}^n$ and $\psi:(u,f)\mapsto (\varphi(u),f(\varphi^{-1}(e_1)),\cdots,f(\varphi^{-1}(e_n))$","['tangent-bundle', 'differential-geometry']"
2720291,Quasiregularity almost everywhere (removability),"The three equivalent definitions of quasiregular mapping that I am using are these ones: Let $U\subset\mathbb{C}$ be an open set and $K < \infty$. Then: A mapping $g:U\to\mathbb{C}$ is $K$-quasiregular if and only if $g = f\circ\phi$ for some $K$-quasiconformal map $\phi:U\to\phi(U)$ and for  some holomorphic map $f:\phi(U) \to g(U)$. A continuous mapping $g:U\to\mathbb{C}$ is $K$-quasiregular if and only if $g$ is locally $K$-quasiconformal except at a discrete set of
  points. A mapping $g:U\to \mathbb{C}$ is $K$-quasiregular if and only if for every $z\in U$ there exist neighbourhoods of $z$ and $g(z)$
  denoted by $N_z$ and $N_{g(z)}$ respectively, a $K$-quasiconformal
  mapping $\psi:N_z\to \mathbb{D}$ and a conformal mapping
  $\varphi:N_{f(z)}\to\mathbb{D}$ such that $(\varphi\circ g\circ
 \psi^{-1})(z) = z^d$, for some $d\geq 1$. I am looking for some result or proof to deal with the following: I have a continuous mapping $g:\mathbb{C}\to\mathbb{C}$ that is quasiregular on the whole plane except in two Jordan curves. How can I ensure that $g$ is quasiregular on the whole plane?","['quasiconformal-maps', 'complex-analysis', 'complex-dynamics', 'holomorphic-functions']"
2720327,"When $xyz=1$ why is $x+y+z\geq3$? ($x,y,z>0$)","How can I prove the statement below without using the Inequality of arithmetic and geometric means? $\forall x,y,z \in F  $ (F is an ordered field) $(x,y,z>0)$ $xyz=1 \implies x+y+z\geqslant3$ For the case where $x,y,z = 1$ it's easy to understand, but I fail to grasp how many more cases of $x,y,z$ ,I have to prove that $x+y+z\geqslant3$ work's for.","['inequality', 'a.m.-g.m.-inequality', 'abstract-algebra', 'calculus', 'ordered-fields']"
2720391,What is the dimension of the set generated by $z \mapsto z + 1$ and $z \mapsto \frac{z}{1+z}$?,"Let's say I have subgroup of $SL_2(\mathbb{Z})$ generated by two elements, I wanted to compute the limit set.  So I wrote down two matrices: $$
A =  \left( \begin{array}{cc} 1 & 1 \\ 0 & 1 \end{array} \right) ,\;
B =  \left( \begin{array}{cc} 1 & 0 \\ 1 & 1 \end{array} \right)
$$
These can be written as two maps of fractional linear transformations or Möbius transformations:
$$  z \mapsto z + 1\;\text{ and }z\mapsto  \frac{z}{1+z}$$
The plots of the orbits of these sets was surprising.  I represented a fraction $\frac{a}{b} = [a:b] \in \mathbb{R}P^2$ as a line in the Euclidean plane . What are the basic properties of this fractal?  E.g. it's dimension or that it's not connected . a closer look: an even closer look:","['mobius-transformation', 'fractals', 'dimension-theory-analysis', 'general-topology', 'fractions']"
2720393,Suppose the function $\sqrt{x}$ is continuous at any $c>0$. Show that this function is differentiable at any $c>0$ and find $f'(c)$.,Suppose the function $\sqrt{x}$ is continuous at any $c>0$. Show that this function is differentiable at any $c>0$ and find $f'(c)$. I know that if a function is continuous any any $c>0$ then for any sequence $x_n \rightarrow c$ we have that $f(x_n) \rightarrow f(c)$ Now I want to show that $\frac{f(x_n)-f(c)}{x_n-c} \rightarrow f'(c)$ So suppose that the function is continuous then let $x_n \rightarrow c$ then we know that $f(x_n) \rightarrow f(c)$ now I can't see how to connect this to what I want to show. Can I go with trying to evaluate what the limit will be? such as $\lim_{x \rightarrow c} \frac{\sqrt(x_n)-\sqrt(c)}{x_n-c}$ then multiplying by the conjugate $\sqrt(x_n) + \sqrt(c)$ I end up with $\lim_{x \rightarrow c} \frac{1}{\sqrt{x_n}+\sqrt(c)}=\frac{1}{2\sqrt(c)}$,"['derivatives', 'continuity']"
2720395,A formula for the Riemann zeta function at all positive integers,"The function $f$ below claims to approximate the Riemann zeta function $\zeta(s)=\sum_{n=1}^{\infty} \frac{1}{n^s}$ at all positive integers $s>1$: $$
f(s) = \frac{\pi^s}{\left\lfloor((2^s - 1)\frac{\pi^s}{2^s}\right\rfloor-1} \approx\zeta(s),
$$ where $\lfloor x \rfloor$ represents the greatest integer small than or equal to $x$. Indeed, tabulating the first few values gives: \begin{array}{|c|c|c|}
\hline
s & f(s) & |\zeta(s) - f(s)| \\ \hline
2 &  \frac{\pi^2}{6} & 0 \\ \hline
3 &  \frac{\pi^3}{26} & 0.00950\ldots \\ \hline
4 &  \frac{\pi^4}{90} & 0 \\ \hline
5 &  \frac{\pi^5}{295} & 0.00042\ldots \\ \hline
6 &  \frac{\pi^6}{945} & 0 \\ \hline
7 &  \frac{\pi^7}{2995} & 0.00009\ldots \\ \hline
8 & \frac{\pi^8}{9450} & 0 \\ \hline
9 & \frac{\pi^9}{29749} & 0.000011\ldots \\ \hline
\end{array} I thought there was no function that even came close to ""unifying"" the values of $\zeta(s)$ at odd and even integers. What is the idea behind the construction of $f$ that allows such behaviour?","['number-theory', 'riemann-zeta']"
2720415,"Roll two dice, find the pmf of X if X is the difference between largest and smallest number","I have another problem very much like the one I recently asked about in the thread Problem: Roll two dice and find the pmf of X . I'm trying to solve it using similar techniques but with no luck. ""Roll two dice and find the pmf of X if X is the difference between the largest and the smallest numbers."" I introduce the random variables $X_1=\{1,2,...,6\}$ and $X_2=\{1,2,...,6\}$
and I am trying to find $P(\max(X_1,X_2)-\min(X_1,X_2) = k) $ for $k=\{0,1,...,5\}$. (This is how i interpret the question. It would make sense to talk about the absoulte value between the two numbers, but since they explicity state in the problem the largest minus the smallest i figured this was the way to go.) I note that $$P(\max(X_1,X_2)-\min(X_1,X_2) = k) = \\=P(\max(X_1,X_2)-\min(X_1,X_2) \leqq k) - P(\max(X_1,X_2)-\min(X_1,X_2) < k) =\\= P(\max(X_1,X_2)-\min(X_1,X_2) \leqq k) - P(\max(X_1,X_2)-\min(X_1,X_2) \leqq k-1)$$ So now I try to evaluate the expression $P(\max(X_1,X_2)-\min(X_1,X_2) \leqq k)$. By rearranging I get that $P(\max(X_1,X_2)-\min(X_1,X_2) \leqq k) = P(\max(X_1,X_2) \leqq k+\min(X_1,X_2))$ and since $\{\max(X_1,X_2) \leqq k\}$ = $\{X_1\leqq k\} \cap \{ X_2 \leqq k\}$ the expression above equals $$P(\max(X_1,X_2) \leqq k+\min(X_1,X_2)) = P[(X_1\leqq k + \min(X_1,X_2)) \cap (X_2 \leqq k + \min(X_1,X_2))] = P(X_1\leqq k + \min(X_1,X_2))P(X_2\leqq k + \min(X_1,X_2)) = [P(X_1\leqq k + \min(X_1,X_2))]^2$$ Now evaluating $P(X_1\leqq k + \min(X_1,X_2)) = P(\min(X_1,X_2)\geqq X_1 -k)$ which is, by the same reasoning as above, $P[(X_1\geqq X_1-k) \cap (X_2\geqq X_1-k)] = P[(k\geqq 0) \cap (X_2\geqq X_1-k)] = P(X_2 \geqq X_1-k) = P(X_1 -X_2 \leqq k)$. Here I can now try with a few values on $k$ and hope that a pattern emerges. $$k=0: P(X_1 -X_2 \leqq 0) = \frac{6}{36}$$ $$k=1: P(X_1 -X_2 \leqq 1) = \frac{12}{36}$$ $$k=2: P(X_1 -X_2 \leqq 2) = \frac{16}{36}$$ and so on. I cant seem to find a pattern however, and it makes med doubt that the method I used is the right one. There is probably some even more simpler solution to this problem, but even if that is the case I wonder if my method is right and if so how to proceed from here? Thanks!","['discrete-mathematics', 'probability-theory', 'statistics', 'probability', 'random-variables']"
2720456,Open covers for $\mathbb{R}$,"Does it seem plausible to conjecture that given an enumeration of the rational numbers $\{ q_1, q_2, q_3, \ldots \}$ and a convergent sequence $\{ \epsilon_n \}_{n \in \mathbb{Z^+}} \subseteq \mathbb{R^+}$ such that $\lim \epsilon_n = 0$ but $\sum_{n \in \mathbb{Z^+}} \epsilon_n = +\infty$, the set of open intervals $\{ (q_n - \epsilon_n, q_n + \epsilon_n) \mid n \in \mathbb{Z^+} \}$ covers $\mathbb{R}$? For instance, can $\mathbb{R}$ be covered by $\{ (q_n - \frac{1}{n}, q_n + \frac{1}{n}) \mid n \in \mathbb{Z^+} \}$? It looks like if we allow $\lim \epsilon_n \neq 0$, the conjecture might work...",['real-analysis']
2720460,How does $\iint_{\mathbf{ℝ^2}}\frac{x}{1+x^2+y^2}dxdy$ diverge?,"I have a question regarding a improper double integral which will diverge but I cannot seem to understand how to reach that conclusion. The integral is the following: $$\iint_{\mathbf{ℝ^2}}\frac{x}{1+x^2+y^2}dxdy$$ I can see that $f(x,y) \geq 0 \space \forall x\geq0$ and $f(x,y)\leq 0 \space \forall x\leq 0$ and therefore I split $\mathbf{ℝ}$ into to parts such that: $ℝ=\Omega_1\cap \Omega_2 = \{(x,y)\in ℝ^2 \mid x\geq0\} \cap \{(x,y)\inℝ^2 \mid x\leq0\}$ and now we integrate over both $\Omega_1$ and $\Omega_2$. However, when I attempt this using polar coordinates I find that both the integrals have the final form of $ \space""0 \cdot\infty ""$. How can I conclude that the integral diverges from this? Or am I doing something completely wrong?","['multivariable-calculus', 'improper-integrals', 'calculus']"
2720467,Determinant of a matrix and linear independence (explanation needed),"It is written on Wikipedia that: $n$ vectors in $\mathbb R^n$ are linearly independent if and only if the determinant of the matrix formed by taking the vectors as its columns is non-zero Can someone explain this to me? You do not have to give a complete proof, just in simple terms explain what the determinant of that matrix has to do with linear independence? And why it has to be non-zero? And are vectors allowed to be rows instead of columns in that matrix?","['matrices', 'linear-algebra', 'determinant']"
2720471,Integral of $\frac{1}{\sqrt{1+x^4}}$,"How do you compute $$\int_1^\infty \frac{1}{\sqrt{1+x^4}}\mathrm dx?$$
I thought of series expansion but the sum is too complicated.
Any help appreciated","['integration', 'calculus']"
2720480,Does a circle minimize mean squared curvature in the plane?,"Suppose we have a non-intersecting closed curve in the plane of fixed length 1 with continuous second derivative. Its mean squared curvature is
$$\langle \kappa \rangle = \int_C |\kappa|^2 ds = \int_0^1 \left|\frac{d^2 \textbf{x}(s)}{ds^2}\right|^2 ds,$$
where the curve is parameterized by arclength. Does the circle with circumference 1 minimize this quantity over all twice continuously differentiable closed curves?","['optimization', 'curvature', 'geometry']"
2720518,A curious property of an acute triangle,"Many years back in high school I happened to stumble upon the following property that seems to hold for any acute triangle: $CD$ and $BE$ are altitudes, the arcs are semicircles with diameters $AB$ and $AC$ respectively. The property is that $AF = AG$ Proof: Let $H$ be the midpoint of $AB$ (and the centre of the respective semicircle)
$$AG^2 = AD^2 + GD^2 = \left(AC\cdot \cos\angle A\right)^2 + GD^2$$
Since $HG = AH = \frac{AB}{2}$ is the radius of the semicircle
$$GD^2 = HG^2 - HD^2 = \left(\frac{AB}{2}\right)^2 - \left(\frac{AB}{2} - AC\cdot\cos\angle A\right)^2 = \\ = AB\cdot AC\cdot \cos\angle A - \left(AC\cdot\cos\angle A\right)^2$$ which gives
$$AG^2 = AB\cdot AC\cdot \cos\angle A$$ Analogously ($I$ is the midpoint of $AC$)
$$AF^2 = AE^2 + FE^2 = \left(AB\cdot \cos\angle A\right)^2 + FE^2$$
$$FE^2 = FI^2 - EI^2 = \left(\frac{AC}{2}\right)^2 - \left(AB\cdot \cos\angle A - \frac{AC}{2}\right)^2 = \\ = AC\cdot AB\cdot \cos\angle A - \left(AB\cdot \cos\angle A\right)^2$$
which finally gives
$$AF^2 = AC\cdot AB\cdot \cos\angle A$$ Questions: Is this a known property? Is there a better more elegant proof?","['triangles', 'geometry']"
2720550,Third degree Taylor series of $f(x) = e^x \cos{x} $,"Suppose you have the function: $$f(x) = e^x  \cos{x} $$
and you need to find the 3rd degree Taylor Series representation. The way I have been taught to do this is to express each separate function as a power series and multiply as necessary for the 3rd degree. For example for 
$$ \cos x =\sum_{n=0}^\infty (-1)^n\frac{  x^{2n}}{(2n)!} = 1-\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots  \text{ and }  e^x =\sum_{n=0}^\infty \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!}+\cdots $$ multiply the terms on the right of each until you get the 3rd degree. Logically, I am happy. However, I have not seen a theorem or any rule that says you can just multiply series in this way. Doing it this way, is there a guarantee that I will always get the power series representation of $f(x)$? Additionally, if instead of multiplying, functions were being added? Would the above hold true - take the series of each function and add up the necessary terms?","['power-series', 'real-analysis', 'taylor-expansion', 'sequences-and-series']"
2720597,Calculate $\lim _{n\to \infty }\left(\frac{e^nn!}{n^n}\right)$ [duplicate],"This question already has answers here : Limit $c^n n!/n^n$ as $n$ goes to infinity (4 answers) Closed 6 years ago . $$\lim _{n\to \infty }\left(\frac{e^nn!}{n^n}\right)$$ According to wolfram, the limit is $\infty$. Both ratio and Cauchy tests yield $1$, so don't don't help much. I tried to bound the sequence from below, as follows $$\frac{e^n}{n^n}n!\ge \frac{e^n}{n^n}\left(\frac{n}{2}\right)^{\frac{n}{2}}$$ but then I get $$\lim _{n\to \infty }\left(\frac{e^n}{n^n}\left(\frac{n}{2}\right)^{\frac{n}{2}}\right)=0$$","['factorial', 'exponential-function', 'sequences-and-series', 'limits']"
2720620,Trigonometric series sum involving tangents,"$$\frac{1}{4}\tan\bigg(\frac{\pi}{8}\bigg)+\frac{1}{8}\tan\bigg(\frac{\pi}{16}\bigg)+\frac{1}{16}\tan\bigg(\frac{\pi}{32}\bigg)+\cdots\cdots \infty$$ Try: $$\cos x\cdot \cos(x/2)\cdot\cos(x/2^2)\cdots\cdots \cos(x/2^{n-1})=\frac{1}{2^{n-1}}\frac{\cos(x)}{\sin(x/2^{n-1})}$$ Could some help me how to solve ahead , thanks",['trigonometry']
2720625,"How to calculate tangent of function, which has to cross certain value on x axis","For example, let's take a simple function $f(x) = x^2-2$. How to get a tangent, which has to cross given value at x axis, for example $x = 2$, like that: Thank you very much for any help.","['derivatives', 'tangent-line', 'calculus', 'functions']"
2720626,"Closed form of the power series of $\sum_{n=0}^{\infty} \sin(n)\,x^n$ (Mathematica)","It's not hard to show that the radius of convergence of the series $\sum_{n=0}^{\infty} \sin(n)\,x^n$ is $1$, however I'm at a loss for finding a closed form of the series... although Mathematica seems to have no trouble whatsoever: $$
\sum_{n=0}^{\infty} \sin(n)\,x^n =  \frac{-ix(e^{2i}-1)}{2(x-e^{i})(e^{i}x-1)} \\
$$
$$
= \frac{x \sin(1)}{1 + x^2 - 2 x \cos(1)}
$$ which oddly enough is real-valued, after simplifying a bit with Mathematica. How would one go about finding this? I couldn't believe that Mathematica had such an easy time with the series, let alone that there was an actual closed form.","['complex-analysis', 'taylor-expansion', 'sequences-and-series', 'power-series']"
2720692,Integrate sin(nx)sin(mx) from 0 to 2$\pi$ using residuals,"I need to use the residues integration method to calculate the following integral: $\int_0^{2\pi} \sin(nx)\sin(mx)dx$ where m and n are positive whole numbers. I know that I need to transform the sinus into its exponential form then substitute for $z=e^{ix}$ and do a change of variable in the integral, then use the theorem saying that the integral along a closed curve is $2\pi i$ times the sum of the residues of all the singularities inside the curve. However, I do not manage to get the right answer, and for the case m=n I get $\pi/2$ instead of $\pi$ .","['residue-calculus', 'complex-integration', 'complex-analysis', 'integration', 'contour-integration']"
2720694,Second variation corresponding to the functional,"I am facing difficulty to calculate the second variation to the following functional. Define $J: W_{0}^{1,p}(\Omega)\to\mathbb{R}$ by
$J(u)=\frac{1}{p}\int_{\Omega}|\nabla u|^p\,dx$ where $p>1$. I am able to calculate the first variation as follows: 
$J'(u)\phi=\int_{\Omega}\,|\nabla u|^{p-2}\nabla u\cdot\nabla\phi\,dx$
which I have got by using the functional $E:\mathbb{R}\to\mathbb{R}$ defined by $E(t)=J(u+t\phi)$. But I am unable to calculate the second variation. Any type of help is very much appreciated. Thanks.","['harmonic-functions', 'regularity-theory-of-pdes', 'fractional-sobolev-spaces', 'multivariable-calculus', 'sobolev-spaces']"
2720709,True value of $\int_0^{\pi/2}\frac{dx}{(1-2\sin^2 x)\sqrt{1-4\sin^2 x}}$,"During the generation of test values I stumbled over this question What is the true value of the complete elliptic integral of the 3rd kind
  $$I=\int_0^{\pi/2}\frac{dx}{(1-2\sin^2 x)\sqrt{1-4\sin^2 x}}?$$ This is a special case of the $\Pi$ integral (see e.g. https://dlmf.nist.gov/19.2.E7 ) and the Cauchy principal value has to be taken. Here are my experiments with some well-known resources: With Wolfram/Mathematica style elliptic integrals (using parameters) the value should be
$$I=\Pi(2|4)=\mathtt{EllipticPi}[2,4] \approx -0.36396057469 - 0.53912891187i$$ With Maple style integrals (using modulus) it should be
$$I=\Pi(2,2)=\mathtt{EllipticPi}(2,2) \approx 1.20683575210 - 0.53912891187 i$$
(Note  that the imaginary parts are the same). In order to resolve the different results, I asked Wolfram Alpha to evaluate the integral using int(1/((1-2*sin(x)^2)*sqrt(1-4*sin(x)^2)), x=0..Pi/2) with 30 digits as input and got the very disturbing different (!?) results Computation result:    1.05574680871 - 3.53800841627 i 
Decimal approximation: 1.11534422571 + 1.91703566486 i Pari/GP 2.9.4 gives another numerical integration result ? intnum(x=0,Pi/2,1/((1-2*sin(x)^2)*sqrt(1-4*sin(x)^2)))
%1 = 1.046038054724273208736077496 + 4.797148009326248296 E27*I Edit (May 2018): I did not expect an answer after two month, therefore I update
the question. Since I am interested in the real part only, I meanwhile use the following formulas: For $|k|>1, \nu \ne k^2$ the following formula (derived from http://functions.wolfram.com/08.06.04.0029.01 with $x=\pi/2$):
$$
\Re \Pi(\nu,k) = \frac{1}{k}\; \Pi\left(\frac{\nu}{k^2} , \frac{1}{k}\right)
- \Re \left(\frac{\pi}{2} \; \sqrt{\frac{\nu}{(\nu - 1)(k^2 - \nu)}}\;\;\right)
$$
(the second term is zero for $k^2 < \nu$), 
and for $\nu=k^2 > 1$ the result is (c.f. http://functions.wolfram.com/08.03.03.0003.01 ):
$$
\Pi(k^2,k)\ = \frac{E(k)}{1-k^2}\;\cdot
$$","['special-functions', 'integration', 'elliptic-integrals']"
2720722,How many different terms are there in the expansion of $(x_1 + x_2 + \cdots + x_m)^n$ after all terms with identical sets of exponents are added?,I just can't understand how to go about it!,"['combinatorics', 'discrete-mathematics']"
2720736,Series solution to $e^xy''+xy=0$,"Question Find the first two non-zero terms of each of two power series solution about the origin of $$e^xy''+xy=0$$ Attempt Let $$y= \sum_{n=0}^\infty a_nx^n$$
Substituting $y$ in the given equation gives.
$$e^xy''+xy = \sum_{n=0}^\infty \frac{x^n}{n!}\sum_{n=0}^\infty (n+2)(n+1)a_{n+2}x^n +\sum_{n=0}^\infty a_nx^{n+1}=0$$ Setting the coefficients to $0$ gives us 
$$a_2 = 0;~~~ a_3 = -\frac{1}{6}a_0;~~~ a_4 = \frac{1}{12}\left(a_0-a_1\right)$$ Substituting it back into $y$ gives us 
$$y = a_0 +a_1x-\frac{1}{6}a_0 x^3  +\frac{1}{12}\left(a_0-a_1\right) x^4$$ However the provided solution is 
$$y = 1 -\frac{1}{6} x^3  +\frac{1}{12} x^4+...$$ $$y = x  +\frac{1}{12} x^4+...$$ Why are we supposed to assume $(a_0,a_1) = (1,0)$ for one solution and $(a_0,a_1) = (0,1)$? Is it because the solution set is a vector space with the basis $ (1,0), (0,1)$? Moreover, would such assumption (that the solution set is a vector space and hence can be written as linear combination of $y_1$ and $y_2$) be feasible if there was a value of $n$ for which $a_n$ was non-linear in $a_0$ or $a_1$ or at the very least, was $ca_0a_1$? (Since the coefficient would no longer remain and hence would not be a vector space",['ordinary-differential-equations']
2720749,"""Strange"" isomorphism $\mathbb C^*/C_n\simeq \mathbb C^*$","Prove the ""strange"" isomorphism: $\mathbb C^*/C_n\simeq \mathbb C^*$. My solution: 
The map  $f:x\mapsto x^n$ is homomorphism since $f(x)f(y)=x^n y^n=(xy)^n=f(xy)$,
$\mathrm{ker}\,f=C_n$, $\mathrm{im}\,f=\mathbb C^*$. Therefore, by the homomorphism theorem: $\mathbb C^*/C_n\simeq \mathbb C^*$. Here $C_n$ is the root of unity.
Could someone prove it and correct if it's needed.","['group-theory', 'proof-verification']"
2720799,Property of a recursive integer sequence,"Loosely related to this question , I encountered a recursive sequence of integers $(b(j,n))_{j\in\mathbb Z,n\in\mathbb N}$ given by $$
 b(0,1)=-1\qquad b(j,n)=0\text{ if }j<0\text{ or }j\geq n\\
 b(j,n+1)=b(j,n)(2j-n)+b(j-1,n)(2j-3n-1)\text{ for all }n\in\mathbb N, j\in\lbrace 0,\ldots,n\rbrace.\tag{1}
$$ Note that the non-vanishing terms of $(b(j,n))_{j\in\mathbb Z,n\in\mathbb N}$ form a triangle
$$
\begin{matrix}
&\underline{j=0}&\underline{j=1}&\ldots&&\\
n=1\,|&-1	&&&&\\
n=2\,|&\hphantom{-}1	&\hphantom{-}2&&&\\
\vdots&-2	&-5&-6&&\\
&\hphantom{-}6	&\hphantom{-}21	&\hphantom{-}24	&\hphantom{-}24&\\
&-24	&-108	&-189	&-120	&-120
\end{matrix}
$$ Now explicit calculations suggest that one can attach polynomial weights in $j$ and $n$ to the $b(j,n)$ such that the sum over any row vanishes . More precisely: Conjecture. For any $n\in\mathbb N$
  $$
\sum_{j=0}^{n-1} b(j,n)\big( 32j^3-32(2n-1)j^2 +2(22n^2-30n+13)j-(n-1)(2n-1)(5n-6) \big)=0
$$ I believe this to be true, but so far I was not able to prove it. The problem with induction here is that when using (1) in the induction step, the weight gets $j^4$-terms which then can't be directly connected to (2) anymore. The obvious way would be trying to find a closed form for the $b(j,n)$ but that seems quite difficult and I was hoping there would be an easier, ""intrinsic"" (only using the recursion or other properties of the sequence) way of proving this. I'm aware of the generating functions ansatz but as the ""weights"" in (1) depend on $j,n$ this seems to not work directly either. Thanks in advance for any answer or comment! Edit: just to vizualize the problem, the first non-vanishing elements of the sequence in question $\scriptstyle\big(b(j,n)(32j^3-32(2n-1)j^2 +2(22n^2-30n+13)j-(n-1)(2n-1)(5n-6))\big)_{j\in\mathbb Z,n\in\mathbb N}$ are given by $$
\begin{matrix}
&\underline{j=0}&\underline{j=1}&\ldots&&\\
n=1\,|&0	&&&&\\
n=2\,|&-12&\hphantom{-}12&&&\\
\vdots&\hphantom{-}180	&-120&	-60&&\\
&-1764	&\hphantom{-}84	&\hphantom{-}1104	&\hphantom{-}576&\\
&\hphantom{-}16416	&\hphantom{-}12312	&-13608	&-7920	&-7200
\end{matrix}
$$ Now it is evident that any row here sums up to 0.","['real-analysis', 'recursion', 'sequences-and-series', 'analysis']"
2720878,Polynomials and partitions,"There is a question I have based on the fact:
If you take a quadratic polynomial with integer coefficients, take the set $\{1,2,3,4,5,6,7,8\}$, make a partition $A=\{1,4,6,7\}$, $B=\{2,3,5,8\}$, and then evaluate the polynomial with the elements of each set you got an equality. If now with a cubic polynomial and the set $\{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16\}$, how to pick a suitable partition that accomplish the same fact? I mean is there a general formula for bigger degree?","['permutations', 'abstract-algebra', 'combinatorics', 'polynomials']"
2720880,Does the multiplication operator $T_f$ given by $T_f\phi = f\phi$ have a discrete spectrum? Can a multiplication operator have continuous spectrum?,"Consider the multiplication operator $T_f:L^2(\mathbb{R}) \to L^2(\mathbb{R})$ given by $$
\phi(x) \mapsto (T_f\phi)(x) = f(x)\phi(x),
$$
where
$$
f(x) =
\begin{cases}
1, \quad x \ge 0, \\
0, \quad x < 0.
\end{cases}
$$ So I checked and found that the spectrum of this operator is $\sigma(T_f) = \sigma_p(T_f) = \{0,1\}$. That is, $(T_f-\lambda I)$ is not injective when $\lambda = 0,1$, and hence the spectrum is simply the point spectrum. Now I have read that the point spectrum $\sigma_p$ has a subset $\sigma_d$ called the discrete spectrum. Are $0$ and $1$ in the discrete spectrum $\sigma_d$ or are they in $\sigma_p\setminus \sigma_d$? One of the conditions for $\lambda$ to be in $\sigma_d$ is that 'the root subspace $L_\lambda(A)$ corresponding to $\lambda$ is finite-dimensional. It seems to me that the eigenfunctions corresponding to, for example, $\lambda = 0$ form an infinite dimensional subspace. I.e. $\phi(x) \in \mathbb{R}$ for $x <0$ and $\phi(x) = 0$ for $x \ge 0$. So this means that $0$ and $1$ are not in the discrete spectrum? Is it possible to construct a simple explicit example of a multiplication operator that has a continuous spectrum? In this case $(T_f-\lambda I)$ should be injective but only have a dense image (i.e. not the whole space). Can a multiplication operator have this type of spectrum?","['functional-analysis', 'spectral-theory', 'operator-theory']"
2720882,derivative of square of dot product,"I've got: $\frac{\partial}{\partial \textbf{w}} \bigg( \textbf{x}^T\textbf{w}\bigg)^2$ when taking the derivative, does this result in $2 (\underbrace{\textbf{x} ~\textbf{x}^T}_{\Sigma_x}) \textbf{w}^T$ or in the dot product of the $\textbf{x}$s $2 (\underbrace{\textbf{x}^T ~\textbf{x}}_{scalar}) \textbf{w}^T$","['derivatives', 'vectors']"
2720901,Vitali Covering Lemma Proof,"Why may we assume that each interval in $\mathcal{F}$ is contained in $\mathcal{O}$? What warrants this reduction? Why is statement (4) true? If $x \in E - \bigcup_{k=1}^n I_k$, then $x \in E$ and $x \notin I_k$ for every $k=1,...,n$. Given some $\epsilon > 0$, there exists $I \in \mathcal{F}$ containing $x$ with $\ell (I) < \epsilon$. I tried choosing $\epsilon > 0$ small enough so that it $I$ wouldn't intersect any of the $I_k$, thereby showing $I \in \mathcal{F}_n$; but it wasn't clear to me how to do this. Indeed, it doesn't seem possible...[Note: the errata sheet for Royden-Fitpatrick's Real Analysis says that $\infty$ should be replaced by $n$] Where are we getting all of the disjoint collections? What guarantees they exist? E.g., ""Suppose $n$ is a natural number and the finite disjoint subcollection $\{I_k\}_{k=1}^n$ has been chosen."" How has this been chosen? It seems that we've chosen them out of thin air. I could keep going on. At this point I'm pretty lost given the sheer number of choices he has made and will make (e.g., why can we choose $\ell(I_{n+1}) > s_n/2$?). It isn't terribly clear how $\{I_k\}_{k=1}^n$ is obtained and how the rest are obtained. EDIT: I with the help of Tony S.F., I have been able to resolve the 1st and 2nd parts of my question. For the 1st, given $x \in E \subseteq \mathcal{O}$, there is some $r > 0$ such that $B(x,r) \subseteq \mathcal{O}$. Given $\frac{r}{2} > 0$, there is a compact interval $I = [a,b] \in \mathcal{F}$ with $\ell (I) < \frac{r}{2}$ such that $x \in I$. Hence $b-a < \frac{r}{2}$ or $b < \frac{r}{2} + a$, and $a \le x \le b$. From these we get $a \le x < a + \frac{r}{2}$ or $|x-a| < \frac{r}{2}$. Hence, if $y \in I = [a,b]$, then \begin{align}
|x-y| &= |(x-a) + (a-y)| \\
&\le |x-a| + |y-a| \\ 
&< \frac{r}{2} + \frac{r}{2} = r, \\
\end{align}
and therefore $y \in B(x,r)$, from which it follows $I \subseteq B(x,r) \subseteq \mathcal{O}$. Now for the second part. Let $\{I_k\}_{k=1}^n \subseteq \mathcal{F}$ and let $x \in E- \bigcup_{k=1}^n$. Then $x \notin I_k$ for every $k$, and, as the $I_k$ are closed, there exists $r_k > 0$ for which $B(x,r_k) \cap I_k = \emptyset$. Letting $r = \frac{1}{2} \min\{r_1,...,r_n\}$, there exists $I_r$ with $\ell(I_r) < r \le \frac{r_k}{2}$ for each $k$ such that $x \in I_r$. But, as we saw above, this means $I_r \subseteq B(x,r_k)$ for each $k$ and therefore $I_r \cap I_k = \emptyset$ for each $k$. Hence $x \in I_r \subseteq \bigcup_{I \in \mathcal{F}_n} I$. Point 3 is still giving me trouble. I still don't understand this: ""Suppose $n$ is a natural number and the finite disjoint subcollection $\{I_k\}_{k=1}^n$ has been chosen."" What justifies this supposition? Granting that for a moment, I think I now see how $I_{n+1}$ is chosen. Since $s_n := \sup \{\ell (I) \mid I \in \mathcal{F}_n\}$ is finite, given $s_n/2 > 0$, there exists $I \in \mathcal{F}_n$ for which $s_n < \ell(I) + s_n/2$ or $\ell(I) > s_n/2$. Let $I_{n+1}$ equal this particular $I \in \mathcal{F}_n$. The only thing I don't see is why $\ell (I_{n+1}) > \ell(I)/2$ for every $I \in \mathcal{F}$. Certainly $\ell (I_{n+1}) > \ell(I)/2$ for every $I \in \mathcal{F}_n$ is true, because by definition $\ell(I_{n+1}) > s_n/2 \ge \ell(I)/2$ for every $I \in \mathcal{F}_n$. The last thing giving me trouble is how $I \cap I_k = \emptyset$ for every $k$ implies $\ell(I_k) > \ell(I)/2$ for every $k$.","['real-analysis', 'measure-theory', 'proof-explanation']"
2720984,"Counterexamples to $\{f_n\} \subseteq L^1(\mu)$, $f_n \to f$ uniformly, $\mu(X) = \infty$ then $f \in L^1(\mu)$","When $\mu(X) < \infty$ $\{f_n\} \subseteq L^1(\mu)$, $f_n \to f$  uniformly, then $f \in L^1(\mu)$ is a true statement by If $\mu(X)<\infty$ then $f \in L^1(\mu)$ and $\int f_n \to \int f$ - An exercise question What if $\mu(X) = \infty$? My first thought was to let $X = (0, \infty)$ under the Lebesgue measure, define $f_n = (\frac{1}{x})^{1 + 1/n}$ and argue that while for all $n$, $\int |f_n| < \infty$, $\int |\frac{1}{x}| = \infty$. However, it is not the case that $f_n \to f$ uniformly. Can someone propose a counter-example where $X = \mathbb{R}$ and the Lebesgue measure is used?","['real-analysis', 'measure-theory']"
2720986,On the equation involving the sum of divisors function $\sigma(105n+\sigma(n))=108\sigma(n)$,"I am curious about the solutions of the following equation involving the sum of divisors function $\sigma(m)=\sum_{d\mid m}d$ $$\sigma(105n+\sigma(n))=108\sigma(n).\tag{1}$$
It is obvious, since $107$ isn't a Mersenne prime, that every even perfect number is a solution of our equation $(1)$. And if we presume that there exist some odd perfect numbers coprime with the prime $107$, these should be also solutions of our equation. Computational fact. Our sequence starts as $$6,28,402,496,1512,1710,1876,7980,8128,15012,29967,30267,\ldots$$
that you can see using Sage Cell Server (choose GP as language) with this code for (i = 1, 1000000,if(sigma(105*i+sigma(i))==108*sigma(i),print(i))) Question. Is it possible to prove that the equation
  $$\sigma(105n+\sigma(n))=108\sigma(n)$$
   has infinitely many solutions? What work can be done*? Many thanks. *Since I think that this is a difficult question (this kind of equations involving arithmetic functions are difficult, I should to accept an answer showing what work can be done , in the case that a full answer isn't feasible). Thus I am asking if you can to show an infinitude of solutions, or a compelling argument about why we can build such sequence.","['divisor-sum', 'sequences-and-series', 'elementary-number-theory']"
2720998,What's the relationship between free variables and nullspaces?,"I know what a free variable and a basic variable is.
I understand free variables show up because there is a lack of a pivot.
I understand a pivot represents a solution in a subspace that's not a nullspace. So my question is do free variables represent a solution in a nullspace? Is there some connection between that and a reduced row echelon form that would be easy to understand?",['linear-algebra']
2721046,"A question on non-principalness of ideal $\langle 3, 1 + \sqrt{223} \rangle \subset \textbf Z[\sqrt{223}]$","Consider the ideal $I = \langle 3, 1 + \sqrt{223} \rangle \subset \textbf Z[\sqrt{223}]$. Q: How do I show $I$ is non-principal? This is related to an exercise in Number Fields by Daniel Marcus. I do not think there should be any advanced machinery showing up. I tried the following. Consider $I = \langle a \rangle$ for some $a \in \textbf Z[\sqrt{223}]$. The ideal norm gives $3$ for both. Then $x^2 - 223y^2 = -3$ has a solution by $a$. Quadratic residue by modding out $223$ or $3$ does not help. I am aware there is the following links. However $-3$ is never showing up in a trivial way. Show that $\mathbb{Z}[\sqrt{223}]$ has three ideal classes. Or by continued partial fractions. (This is not used in Marcus's Number Fields for sure. I guess there should be a much naïve way of solving this.) How to prove there are no solutions to $a^2 - 223 b^2 = -3$. Or A diophantine equation","['number-theory', 'abstract-algebra', 'diophantine-equations', 'algebraic-number-theory']"
2721051,Show that $\lim_{z\to z_0}|f(z)|=\infty$ if and only if $z_0$ is a pole,"Im stuck with this exercise Let $f$ holomorphic on $U\setminus\{z_0\}$ where $U$ is open. Show that $\lim_{z\to z_0}|f(z)|=\infty$ if and only if $z_0$ is a pole. One direction is easy to show, Im stuck trying to show that $$\lim_{z\to z_0}|f(z)|=\infty\implies z_0\text{ is a pole}$$ In particular Im having a hard time to figure how to show that if $\lim_{z\to z_0}|f(z)|=\infty$ then $z_0$ cannot be an essential singularity. My work so far: if $\lim_{z\to z_0}|f(z)|=\infty$ then $f$ have a non-removable singularity at $z_0$ . If $z_0$ would be an essential singularity then I know that (this is my definition of essential singularity) the principal part of the Laurent expansion of $f$ around $z_0$ have infinitely many non-zero coefficients, that is $$\lim_{z\to z_0}|f(z)|=\lim_{z\to z_0}\left|\sum_{k=1}^\infty c_{-k}(z-z_0)^{-k}\right|\tag1$$ where there are infinitely many $c_{-k}\neq 0$ . Then to show that the limit of $(1)$ doesnt exists is enough to show that there is some $\theta\in[0,2\pi)$ such that $$\lim_{r\to\infty}\left|\sum_{k=1}^\infty c_{-k}e^{ik\theta}r^k\right|<\infty\tag2$$ That is: if there is some linear path $z\to z_0$ such that $\lim_{z\to z_0}|f(z)|<\infty$ we are done. However Im not sure if this approach is useful or not. In any case I get stuck here, I dont have a clue about how to show that $z_0$ cannot be an essential singularity if $\lim_{z\to z_0}|f(z)|=\infty$ . Some help will be appreciated, thank you. EDIT: I think I see a path for a solution. We can write $\omega_k(\alpha_k)^k$ for $\omega_k\in\mathrm S^1$ and $\alpha_k\in(0,\infty)$ such that $\lim \alpha_k=\infty$ instead of $c_{-k}e^{ik\theta}r^k$ (because $\lim c_{-k}= 0$ ) what give us great freedom to choose suitable sequences $(\omega_k)$ and $(\alpha_k)$ to try to prove $(2)$ . Indeed, if Im not wrong, choosing $\omega_k=(-1)^k$ we are done.","['complex-analysis', 'singularity', 'holomorphic-functions']"
2721106,Why this function well defined?,"Let $M^{n-1} \subset \mathbb{R}^n$ be a smooth compact manifold, $\Lambda$ = $\{U_i\}_{i \in I}$ be an open cover of $\mathbb{R}^n$. Suppose that for every $U_i$ there is a smooth bounded function $f_i : U_i \rightarrow [-1,1]$, such that, for every $i \neq j$, occurs only one of the following conditions: $f_i(x) = f_j(x)$, $\forall$ $x$ $\in$ $U_i \cap U_j,$ $f_i(x) = -f_j(x)$, $\forall$ $x$ $\in$ $U_i \cap U_j.$ Besides that $f_i(x) = 0$ $\Leftrightarrow$ $x \in M\cap U_i$. For $x_0$ $\in$ $\mathbb{R}^n \setminus \{0\}$, we will define a function $f_{x_0}:\mathbb{R}^n \rightarrow [-1,1]$ as following: Consider $x$ $\in$ $\mathbb{R}^n$, let $\gamma_x:[0,1] \to \mathbb{R}^n$ be a continious path linking $x_0$ to $x$. Note that we can subdivide $[0,1]$ in $0=t_0\leq t_1 \leq t_2 \leq \ldots \leq t_n =1$, in such way that $\gamma_x([t_i,t_{i+1}]) \subset U_{j_i}$, for some $U_{j_i}$ $\in$ $\Lambda$. Suppose, without loss of generality, that $U_{j_i} = U_i$, for every $i$ $\in$ $\{0,1,...,n\}$. For every $i$ $\in$ $\{0,...,n\}$ there is $\xi_i$ $\in$ $U_{i-1} \cap U_{i}$, such that, $f_{i}(\xi_i) \neq 0.$ Defining $g_i:U_i \rightarrow [-1,1]$ recursively as $g_{0}(x)$ $=$ $f_{0}(x)$, $g_i (x)= \frac{f_i(\xi_i)}{g_{i-1}(\xi_{i})} \cdot f_i(x)$,   $\forall$ $i$ $\in\{1,...,m\}$. Finally we say that $f_{x_0}(x) = g_n(x)$, for $x \in U_n$. My Doubt: Why $f_{x_0}$ does not depends on the path $\gamma_x$ used to define $f_{x_0} (x)$? I think the function $f$ is well defined because $\mathbb{R}^{n}$ is a  simply connected, but I don't know how to demonstrate that the construction of the value  $f_{x_0}(x)$ is invariant under homotopy.","['real-analysis', 'smooth-manifolds', 'differential-geometry', 'homotopy-theory']"
2721148,"Prove $E[f(X,Y)\mid Y]=E[f(X,Y)\mid Y,Z]$ if $X$ is independent of $Y$ and $Z$","I would like to prove the following claim (which I think is true): Suppose $\mathcal{F}_1 \subset \mathcal{F}_2$ are two $\sigma$-fields, $X,Y$ are random variables, and $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is a measurable function. If $X$ is independent of $\mathcal{F}_2$, $Y\in\mathcal{F}_1$, then $E[f(X,Y)\mid \mathcal{F}_1] = E[f(X,Y)\mid \mathcal{F}_2]$. Intuitively, this is just saying $E[f(X,Y)\mid Y]=E[f(X,Y)\mid Y,Z]$ if $X$ is independent of the pair $(Y,Z)$, but I don't know how to prove the above claim rigorously. Thanks in advance!","['independence', 'probability-theory', 'conditional-expectation']"
2721172,Are the spaces $\mathbb{H}^n$ and $\overline{\mathbb{R}}^n_+$ homeomorphic?,"Let $\mathbb{H}^n = \{(x^1, \dots , x^n) \in \mathbb{R}^n : x^n \ge 0\}$, and $\overline{\mathbb{R}}^n_+ = \{(x^1, \dots , x^n) \in \mathbb{R}^n : x^1 \ge 0, \dots, x^n \ge 0\}$. Endow each set with the subspace topology it inherits from $\mathbb{R}^n$. Exercise 16.18 on page 415 of Lee's Introduction to Smooth Manifolds (2nd edition) asks us to show that $\mathbb{H}^n$ and $\overline{\mathbb{R}}^n_+$ are homeomorphic. However, it seems to me the spaces are not homeomorphic, as shown by the following argument. Suppose $f:  \overline{\mathbb{R}}^n_+ \to \mathbb{H}^n$ is a homeomorphism. Set $f(0, \dots , 0) = (a^1, \dots, a^n)$. Then $$A = f(\overline{\mathbb{R}}^n_+ \setminus \{(0, \dots, 0)\}) = f((\infty,0) \times \cdots \times (\infty,0)) = \\
 ((\infty, a^1) \cup (a^1, - \infty)) \times \cdots \times ((\infty, a^n) \cup (a^n, 0]),$$
where, if $a^n= 0$, then in the last factor we discard $(a^n, 0]$. The set $A$ is connected as it is the continuous image of the connected set $(\infty, 0) \times (\infty,0)$. But at the same time we see that $A$ is disconnected as it is the union of the disjoint relatively open sets 
$(\infty, a^1) \times \cdots \times (\infty, a^n)$ and $ (a^1, - \infty) \times \cdots \times (a^n, 0]$ (or $(\infty, a^1) \times \cdots \times (\infty, 0)$ and $(a^1, - \infty) \times \cdots \times (\infty, 0)$ in case $a^n = 0$). Is there a  mistake in my argument or something I am missing? Any comments are greatly appreciated.","['general-topology', 'smooth-manifolds', 'connectedness']"
2721176,proof of Taylor's theorem,"I am struggling to understand this proof. At the near last part, I don't understand how the author derive this equation $g^{(n+1)}(s)=f^{(n+1)}(s)-(n+1)!M_{x,x_0}$. I think it should be $g^{(n+1)}(s)=f^{(n+1)}(s)-(P^{x_0}_n)^{(n+1)}(s)-M_{x,x_0}(s-x_0)^{(n+1)}$. Could you help me understand this part? Thank you in advance!","['derivatives', 'real-analysis']"
2721209,"Why can Lebesgue integral be defined only with supremum, but Riemanns needs both sup. and inf.?","As stated above, why can Lebesgue integral be defined only with supremum
$$
 \int_E f \, d\mu := \sup\left\{ \int_E s \, d\mu : 0 \le s \le f, s \text{ simple } \right\}.
$$ but the Riemann integral needs to be expressed in terms of both the sup and inf? What is the key difference? It seems important to me, but everywhere, I jut see the definition, without an explanation, why it is defined like this. Please help me understand, I can not find an explanation anywhere.","['lebesgue-measure', 'lebesgue-integral', 'measure-theory', 'riemann-integration']"
2721213,How to better prove this statement? Let G be a graph with $n$ vertices and $k > {n-1 \choose 2}$ edges. Then G is connected,"I tried doing it by contrapositive. I'm here to ask two things 1) Whether this proof is correct 2) If there's a better, more direct way of proving this using graph theory Here's how I did it: Let G be a non-connected graph with n vertices. Then G has at most ${n-1 \choose 2}$ edges Proof: If G isn't connected, it has two subgraph components F and H, such that $ \#V_F = m$ and $\#V_H = n - m$, $ 1 \le m \le n-1$ $\#E_F + \#E_H = \#E_G$ and also $\#E_F \le \#E_{K_m}$ and $\#E_H \le \#E_{K_{n-m}}$ So from this follows that $ \#E_G = \#E_F + \#E_H \le \#E_{K_m} + \#E_{K_{n-m}} = {m \choose 2 } + {n-m \choose 2}$ So I'll define $f(m) = {m \choose 2 } + {n-m \choose 2}$ and I want to prove that $ \Delta f(m) \le 0$, for $1 \le m \le \lfloor \frac {n-1}{2} \rfloor$ $\Delta f(m) \ge 0$, for $\lceil \frac{n-1}{2} \rceil \le m \le n-1$ $f(m) = f(n-1) = {n-1 \choose 2}$ This would prove f(m) has its maximum value at m=1. And then the maximum amount of edges in G is f(1). I'm not writing the proof for these three last statements, but it's easy to prove knowing that $\Delta f(m) = 2m+1-n$ Thanks in advance!","['graph-theory', 'discrete-mathematics']"
2721232,Independence and Conditional Expectations,"I have a simple question. It is known that, if X and Y are independent, then $\mathbb{E}[X | Y] = E[X]$ where X and Y are just random variables. However, is the converse true? i.e. Given that $\mathbb{E}[X | Y] = E[X]$, can I assume they are independent? Thanks","['statistics', 'conditional-expectation', 'probability']"
2721309,"A concurrency involving that the feet of an altitude, the centroid and a point on the circumcircle","Let $ABC$ be an acute angled triangle and suppose $X$ is a point on the circumcircle of $\Delta ABC$ with $AX||BC$ and $X\neq A$. Denote by $G$ the centroid of triangle $ABC$, and by $K$ the foot of the altitude from $A$ to $BC$. Prove that $K,G,X$ are collinear. I tried to apply Menelaus theorem but couldn't find a triangle to apply. I found a homothety centred at $G$ which maps the medial triangle to the main triangle. I guess the homothety maps $K$ to $X$ i.e. $K$ a point on the circumcircle of the medial triangle but i failed to proof this. Please help me.","['euclidean-geometry', 'vectors', 'geometry']"
2721363,Constructing symplectic tubular neighborhood of compact submanifold,"I'm studying the proof of the Darboux-Weinstein theorem, but I'm very confused about a step. Let $M$ be a smooth manifold, $Q\subseteq M$ be a compact submanifold, and $\omega_0, \omega_1 \in \Omega^2(M)$ be two symplectic forms on $M$ such that $\omega_0|_Q = \omega_1|_Q$, in the sense that for all $q \in Q$, the bilinear maps $(\omega_0)_q,(\omega_1)_q\colon T_qM \times T_qM \to \Bbb R$ are equal. For all $t \in [0,1]$, define $\omega_t \doteq \omega_0 + t(\omega_1-\omega_0)$. I want to construct a neighbourhood $N_0$ of $Q$ such that all the $\omega_t$ are symplectic along $N_0$. Note that $\omega_t$ is non-degenerate along $Q$ because of the restriction hypothesis. I know we must use compactness of both $Q$ and $[0,1]$, but I'm messing up the order. Attempt: given $t \in [0,1]$, for all $q \in Q$ there is an open set $U_{t,q}$ around $q$ such that $(\omega_t)_p$ is non-degenerate for all $p \in U_{t,q}$. Then $\{U_{t,q}\}_{q \in Q}$ is an open cover for $Q$ and we get $q_1,\ldots,q_k \in Q$ with $$Q \subseteq U_{t,q_1}\cup \cdots \cup U_{t,q_k},$$for all $t \in [0,1]$. I'd like to take the intersection in the right side, but the result will remain open only if we consider a finite amount of $t$'s. Fixed one of these points $q_i$, for all $t \in [0,1]$ we get an open interval $I_{t,q_i}$ around $t$ such that $(\omega_s)_{q_i}$ is non-degenerate for all $s \in I_{t,q_i}$. The $\{I_{t,q_i}\}$ is an open
cover of $[0,1]$ and we get $t_{1,i}, \ldots, t_{r_i,i} \in [0,1]$ such that $$[0,1] \subseteq I_{t_{1,i},q_i}\cup \cdots \cup I_{t_{r_i,i},q_i}$$for all $1 \leq i \leq k$. Intersecting we get $$[0,1]\subseteq \bigcap_{i=1}^n(I_{t_{1,i},q_i}\cup \cdots \cup I_{t_{r_i,i},q_i}).$$Now, for good or worse we have a finite quantity of $t$'s. Intuition says that putting $N_0$ as the intersection of $U_{t,q_1}\cup \cdots \cup U_{t,q_k}$ with $t$ running over these values should work. But this does not seem uniform. I can only control non-degeneracy using continuity on each variable separately. If I actually can control both at the same time, I need to see a proof. I am not interested in other references or constructions of tubular neighborhoods in general. I want to fix this argument. Thanks.","['differential-topology', 'symplectic-geometry', 'compactness', 'general-topology', 'differential-geometry']"
2721367,Dual basis with non-degenerate bilinear form,"Let $V$ be $n$-dimensional vector space and $\{x_1,\cdots,x_n\}$ a basis. Let $\beta:V\times V\rightarrow F$ be a non-degenerate (symmetric) bilinear form. This implies that there exists a dual basis $\{y_1,\cdots, y_n\}$ of $V$ w.r.t. $\beta$, i.e. 
$$\beta(x_i,y_j)=\delta_{ij}.$$
Can we write expression for basis elements $y_i$'s in terms of $x_i$'s and matrix of $\beta$ w.r.t. basis $\{x_1,\cdots, x_n\}$? This is an obvious computational question and may be very trivial, but I didn't find in any book mentioning about computation of dual basis. For example, as a concrete example, let $V$ be the space of column vectors of length $n$ over field $\mathbb{R}$. Consider dot product on this column space.  Then given any invertible $n\times n$ matrix $A$ over $\mathbb{R}$, its $n$ columns define a basis of $V$; what is dual basis w.r.t. dot product? Perhaps it is columns of $(A^{-1})^t$, am I right? Then next question comes more general, which I put above.","['matrices', 'bilinear-form', 'linear-algebra']"
2721368,"Empty set, subsets, and vacuous truths","So I was trying to prove that a null set is a subset of any set. First, to define when $A$ is a subset of $B$: $$A \subseteq B \iff \forall x(x \in A \implies x \in B)$$ (At least I think that's right?) So then consider the empty set $\emptyset$ for which $\forall x(x \not\in \emptyset)$ is true. I tried to prove that the empty set is a subset of any other set, or: $$\emptyset \subseteq B \iff \forall x(x \in \emptyset \implies x \in B) \vdash \text{T}$$ To me this seemed true because it was ""vacuously true""... somehow. Like it makes sense to call it vacuously true that ""all $0$ items in $\emptyset$ can be found in $B$, yep!"" but that isn't satisfying to me, how do I ""prove"" this is the case? Is $x \in \emptyset$  a false... statement? A false predicate? Something else? Something that results in false so that the implication itself is true. Is this a $(\text{F}\implies \text{F}) \vdash \text{T}$ thing? Or is it the $\forall$ that makes it false somehow? What if I had said $\exists x \in \emptyset$, this feels like it would certainly be false but again I can't prove why, it's just an intuition. Can anyone clarify the correct definitions / implications and why they're true or false or what have you?","['predicate-logic', 'logic', 'elementary-set-theory', 'definition']"
2721509,Rudin Theorem 6.10 doubt,"This question is about Riemann Stieltjes integral: Here is definition: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write 
  $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$
  It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put 
  $$ 
\begin{align}
U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\
L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i, 
\end{align}
$$
  where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define 
  $$
\begin{align}
\tag{5} \overline{\int_a^b} f d \alpha = \inf U(P, f, \alpha), \\
\tag{6} \underline{\int_a^b} f d \alpha = \sup L(P, f, \alpha), \\\,
\end{align}
$$
  the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by 
  $$ \tag{7} \int_a^b f d \alpha $$
  or sometimes by 
  $$ \tag{8} \int_a^b f(x) d \alpha(x). $$
  This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. This is Rudin PMA's Th 6.10: Suppose $f$ is bounded on $[a, b]$, $f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$. Proof : Let $\varepsilon > 0$ be given. Put $M = \sup \left\vert f(x) \right\vert$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[ u_j, v_j \right] \subset [a, b]$ such that the sum of the corresponding differences $\alpha\left(v_j\right) - \alpha \left( u_j \right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap (a, b)$ lies in the interior of some $\left[ u_j, v_j \right]$. Remove the segments $\left( u_j, v_j \right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta > 0$ such that $\left\vert f(s) - f(t) \right\vert < \varepsilon$ if $s \in K$, $t \in K$, $\left\vert s-t \right\vert < \delta$. Now form a partition $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ of $[a, b]$, as follows: Each $u_j$ occurs in $P$. Each $v_j$ occurs in $P$. No point of any segment $\left( u_j, v_j \right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_j$, then $\Delta \alpha_i < \delta$. Note that $M_i - m_i \leq 2M$ for every $i$, and that $M_i - m_i \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_j$. Hence, as in the proof of Theorem 6.8, 
  $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon.$$
  Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$. [Code borrowed from here .] My question is: What I can't understand is: Why do we treat $u_j$'s differently? How do we get $U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon$ in last line?","['real-analysis', 'riemann-integration']"
2721511,Ratio of circumradius to inradius of a regular tetrahedron,"The circumradius of a set in $\mathbb{R}^n$ is the radius of the smallest sphere enclosing the set.  Similarly, the inradius is the radius of the largest sphere fitting inside a set. Question: What is the ratio $\frac{\text{circumradius}}{\text{inradius}}$ for a regular tetrahedron? This is one of my all time favorite questions because it admits at least three very different solutions, two of which fit anyone's definition of ""elegant"".  I'm posting my favorite solution, but I am keen to see what solutions others can come up with.","['simplex', 'big-list', 'euclidean-geometry', 'geometry']"
2721648,"Proving $\| x + y \| = \| x \| + \| y \|$ implies $\|ax + by\| = \|a x \| + \|by\|$ for any $a,b \geq 0$","Consider $ (R^p, \| \cdot \|)$ a norm space and let $x$ and $y$ be vectors such that $\| x + y \| = \| x \| + \| y \|$. Now show that for any $\alpha$, $\beta$ $\geq 0$, $\|\alpha x + \beta y\| = \|\alpha x \| + \|\beta  y\|$. This is with regard to general norms, i.e. $$
||\bar{x}|| \geq 0 
$$
$$
||\alpha\bar{x}|| = |\alpha | ||\bar{x}|| 
$$
$$
||\bar{x} + \bar{y} || \leq ||\bar x|| + ||\bar y|| 
$$
$$
||\bar{x}||= 0 \Leftrightarrow \bar x = 0
$$ With $|| \cdot ||_2$ the proof of this is simple, it implies colinearity of the vectors. For a general norm, I think it implies a somewhat weaker notion of ""colinearity"", in the hand-wavey sense. This is a homework question and I don't really want the answer---that will not be of much help. My assessment of the problem is to solve it by making strict use of the four properties of a norm that I've listed above but I am missing some key insight that tells me my maths toolbox is lacking. I would appreciate some problem solving guidance that will eventually lead me to figure this out.","['real-analysis', 'analysis']"
2721705,Find the value of $\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\frac \pi3 - x)}$,"The question from my textbook requires us to find the value of the below limit: $$\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\pi/3 - x)}$$ On using the trigonometric identity $\cos2x = 1-2\sin^2x$ , the above limit reduces to: $$\lim \limits_{x \to \pi /3} \frac{\lvert \sin3x \rvert}{\pi/3 - x}$$ On substituting $\frac \pi3 - x$ by $y$ the above limit becomes: $$\lim \limits_{y \to 0} \frac{\lvert \sin3y \rvert}y$$ In this case, the left hand and right hand limits are unequal, being equal to $-3$ and $3$ respectively. Thus, $$\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\pi/3 - x)} \text{ does not exist.}$$ But my textbook gives the answer as $3$ . It was apparently because it considered $(1-\cos6x)^{1/2}$ as equal to $\sin 3x$ and not as equal to $\lvert \sin3x \rvert$ . But since $x \to \pi/3$ , shouldn't $(\sin^2 3x)^{1/2}$ equal $\lvert \sin3x \rvert$ ? Is my argument incorrect? Any help would be appreciated.","['proof-verification', 'limits']"
2721751,Moore-Penrose pseudoinverse of the multiplication operator,"Let $E=L^2(X,\mu)$ and $g\in L^{\infty}(X)$ be such that $g(x)\geq 0$ for all $x\in X$. Consider the multiplication operator $M_g(f)=fg$ for all $f\in L^2(X,\mu)$. Since $g(x)\geq 0$ for all $x\in X$, then $M_g\geq0$ (i.e. $M_g$ is self-adjoint and $\langle M_g(f),f\rangle\geq 0$ for all $f\in L^2(X,\mu)$. I want to calculate the Moore-Penrose pseudoinverse of $M_g$ (for more details see 2 ) denoted $M_g^{+}$.","['functional-analysis', 'linear-algebra', 'operator-theory']"
2721753,Multiplicative group $\mathbb{G}_m$ is connected,"I wanted to show the elementary fact mentionned in the title. Context: Let $\mathbb{k}$ be an algebrically closed field of caracteristic 0. (I know that I don't need that much hypothesis, but these are the general hypothesis I'm working on). Then the multiplicative group $\mathbb{G}_m \simeq \mathbb{k} \backslash \{0\}$ is a connected algebraic group. I Know for sure that $\mathbb{G}_m$ is an affine standard set (i.e. $\mathbb{G}_m \simeq D(P) = \mathbb{k} \backslash V(P)$) where $P$ is the polynomial $P(T) = T$. I have the intuition that the proof of the connectedness lies in here, but I don't know how to explicit it. Maybe a general fact that an affine standard open set is irreductible? Could you enlight me please? Thanks in advance. K. Y.","['algebraic-groups', 'algebraic-geometry']"
2721754,Integral identity using the transformation formula,"Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be integrable. I want to show that
$$ \int_\mathbb{R}f(x)\,\mathrm{d}\lambda(x) = \int_\mathbb{R} f\left(x-\frac{1}{x}\right)\,\mathrm{d}\lambda(x).$$
I tried to use the transformation formula, but I did not get the identity. I would appreciate any hints.","['real-analysis', 'integration']"
2721785,Problem of quantifiers and predicate.,"I was reading the book ""Advanced Calculus"" by Loomis and Sternberg and I was following everything perfectly until I came across a paragraph which I have attached here. My first confusion is that I am unable to understand what the authors are trying to say in this paragraph about hanging quantifiers and how does the first ambiguous statement become true and second become false?. My second confusion arises when I came across two statements: $$'[(\forall x)(x<y)](\exists y)'$$
and $$'(\forall x)[(x<y)(\exists y)]'$$
My problem is that a quantifier has been used after the predicate $(x<y)$ in both the statements. How is this possible? And how does first is false and second is true? I have never seen such a way of writing logical statement. My current understanding: I understand that the order of quantifiers are important. ""Every $x$ is less than some $y$"" is False . ""Some $y$ is greater than every $x$"" is False .","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
2721833,Is it possible to write this operation as a (series of) matrix operation(s),"Consider the symmetric matrix $T\in \mathbb{R}^{n\times n}$. For the sake of simplicty I will use the example for $n=2$. I was wondering if we have the full matrix as
$$ T = \begin{pmatrix} a & c\\ c& b\end{pmatrix},$$
it is possible to construct this from a vector $t\in\mathbb{R}^{\frac{1}{2}n(n+1)}$, such that 
$$ t = \begin{pmatrix} a\\b\\c\end{pmatrix}.\quad (n=2)$$
I was thinking that this might be possible by writing it as some kind of strange permutation in the form of
$$AtB=T,$$ where $A\in \mathbb{R}^{n\times \frac{1}{2}n(n+1)}$ and $B\in \mathbb{R}^{1\times n}$. However, working this out component wise gives contradicting requirements for $a_{ij}$ ($a_{13}=0$ and $1$), so this seems impossible. $\textbf{Question:}$ Is there a way to construct $T$ when $t$ is given, which uses only matrix-vector operations?","['matrices', 'permutations', 'linear-algebra']"
2721855,Haar measure on orthogonal group,"I have a few question regarding the Haar measure on the orthogonal, or unitary, or indeed Lie, groups. Let me consider $SO(N)$. If I were to define a probability measure on $SO(N)$, I would proceed like this. Every such matrix is the exponential of a real antisymmetric matrix, $O=e^M$. The antisymmetry constraint is much simpler than the orthogonality constraint, and I could simply put a Gaussian measure on the off-diagonal elements of $M$, thereby inducing a probability measure on $SO(N)$. First question: Is the Haar measure on $SO(N)$ related at all to the above procedure? Also, I have seen in the literature the expression $d\mu(O)=O^TdO$ to indicate the Haar measure, with the argument that this is invariant under adjoint action since $O\mapsto AO$ leaves it invariant as $(AO)^Td(AO)=O^TA^TAdO=O^TdO$. Fine. But, second question: What is meant by $dO$ in the above notation? If $O=e^M$, then I suppose $dO=OdM$ and then $O^TdO=dM$. Does this make any sense?","['harmonic-analysis', 'differential-geometry', 'probability', 'group-theory']"
2721857,Fill the blank to make a true identity,"I need to make a true identity by filling the missing part here:
 $$C\setminus(A\triangle B) = (A\cap C)\triangle ...................$$ I used Venn diagrams to find a solution which led me to $(A\cap C)\triangle(C\setminus B)$, however when I tried to represent what it means for x to be an element of both sides of the solution. Both answer differs, or I missing something. Here is what I have done: For $x$ to be an element of $C\setminus(A\triangle B)$, means:
$$(x\in C\setminus x\in((A\setminus B)\cup(B\setminus A))) - \text{definition of }\triangle
\\
(x\in C\land\neg((x\in A\land x\notin B)\cup(x\in B\land x\notin A))) -\text{definition of }\setminus\\(x\in C\land\neg(x\in A\land x\notin B)\lor(x\in B\land x\notin A)) - \text{definition of }\cup\\(x\in C\land(\neg(x\in A\land x\notin B)\land \neg(x\in B\land x\notin A))) - \text{DeMorgan's law}\\(x\in C\land((x\notin A\lor x\in B)\land(x\notin B\lor x\in A)))-\text{DeMorgan's law}\\
\text{after associative law I end up  with }  x\in C\land(x\notin A\lor x\in B)\land(x\notin B\lor x\in A)$$ The right side $(A\cap C)\triangle(C\setminus B):$
$$((A\cap C)\setminus(C\setminus B))\cup((C\setminus B)\setminus(A\cap C)) \text{ means:}\\ ((x\in A\land x\in C)\setminus x\in(C\setminus B))\cup(x\in(C\setminus B)\setminus(x\in A\land x\in C))-\text{definition of }\cap\\((x\in A\land x\in C)\land\neg(x\in C\land x\notin B))\cup((x\in C\land x\notin B)\land\neg(x\in A\land x\in C))- \text{definition}\ \setminus\\((x\in A\land x\in C)\land\neg(x\in C\land x\notin B))\cup((x\in C\land x\notin B)\land\neg(x\in A\land x\in C))-\text{definition}\ \cup\\((x\in A\land x\in C)\land(x\notin C\lor x\in B))\lor((x\in C\land x\notin B)\land(x\notin A\lor x\notin C))-\text{DeMorgan's\ law}\\(x\in A\land (x\in C\land(x\notin C\lor x\in B))\lor((x\in C\land x\notin B)\land(x\notin A\lor x\notin C))-\text{associative law} \\(x\in A\land ((x\in C\land x\notin C)\lor (x\in C\land x\in B))\lor((x\in C\land x\notin B)\land(x\notin A\lor x\notin C))-\text{distributive law}\\(x\in A\land (x\in C\land x\in B))\lor((x\in C\land x\notin B)\land(x\notin A\lor x\notin C))-\text{contradiction law}\\(x\in A\land (x\in C\land x\in B))\lor( x\notin B\land(x\in C\land(x\notin A\lor x\notin C)))-\text{associative law}\\(x\in A\land (x\in C\land x\in B))\lor( x\notin B\land((x\in C\land x\notin A)\lor (x\in C\land x\notin C)))-\text{distributive law}\\(x\in A\land (x\in C\land x\in B))\lor( x\notin B\land(x\in C\land x\notin A))-\text{contradiction law}\\(x\in A\land x\in C\land x\in B)\lor( x\notin B\land x\in C\land x\notin A)-\text{associative law}\\x\in C\land ((x\in A\land x\in B)\lor( x\notin B\land x\notin A))$$ Sorry for being so verbose here. The question is are those results correct? If not - what I am missing? Thanks.","['propositional-calculus', 'logic', 'elementary-set-theory']"
2721867,"What is the closest point to $(0,-1/2)$ on the curve $x^2+2y=0$? Why can't I differentiate over $y$?","Let $L$ be the distance between $(x,y)$ on the curve $x^2+2y=0$ and $(0,-1/2)$.
Then $L^2=x^2+(y+\frac{1}{2})^2=-2y+(y+\frac{1}{2})^2$,
$\frac{dL^2}{dy}=-2+2(y+\frac{1}{2})=0$,
which gives $y=1/2$. But this is clearly incorrect. $y=1/2$ does not even lie on the curve! I know that if we express $y$ in terms of $x$, and then differentiate over $x$, we can get the correct answer $(0,0)$. But I don't figure out what's wrong when we differentiate over $y$.","['derivatives', 'optimization', 'calculus']"
2721933,Can you really integrate functions?,"Statements such as ""you can't really integrate functions, the only thing you can integrate are differential forms"" and ""if you really believe you know how to integrate functions tell me the integral of the function 1 over this blackboard"" (I read that one once on a stackexchange answer which I've been unable to track down) feel scary to me, since I would like to believe I know how to integrate functions. For instance, the Riemann integral of the function $1$ over the interval $(a,b)$ of the real numbers is $b-a$ ... do I really need a differential form for that? Is integration ""coordinate dependent"" in some way, even in $\mathbb{R}^N$? I am used to the nice change of variables formula: $$\int_{U} f = \int_{\phi^{-1}(U)}f\circ \phi ~|\det{D\phi}|$$ I don't really see how this is chart dependant. From what I could gather from questions such as this one , the issue seems to be that people 
 would rather have a function multiplied with another object that spits out the jacobian automatically than place it there by hand, however, I don't really see the problem with just shoving it there when one wishes to change coordinates. I am definitely missing some essential insight and I would love to have this clarified for me.","['coordinate-systems', 'differential-forms', 'integration', 'differential-geometry']"
2722012,Is there geometric Intuition for singular (2x2) matrices in space of (2x2) matrices?,"Consider the vector space of 2x2 matrices $M_{2x2}$ over the reals. There's a clear isomorphism from the space to $\mathbb{R}^4$. Take a singular matrix in this space, say $\left[\begin{array}{ccc} a & 0 \\ 0 & 0 \end{array}\right]$, $a \in \mathbb{R}$. Taking the set of all matrices of this form clearly forms a subspace, which can be thought of as a line in $\mathbb{R}^4$, yet the set of all singular matrices is not a subspace. My question is thus: in general, do sets of singular matrices of certain forms always have nice geometric intuitions like this? Furthermore, in this space of matrices, can we always form subspaces of singular matrices, and if so, are they always one or two dimensional? Clearly $\left[\begin{array}{ccc} a & b \\ 0 & 0 \end{array}\right]$, $a, b \in \mathbb{R}$ is a two-dimensional subspace consisting entirely of singular matrices in $M_{2x2}$, which can be interpreted as plane in $\mathbb{R}^4$, but $\left[\begin{array}{ccc} a & b \\ c & 0 \end{array}\right]$, $a, b, c \in \mathbb{R}$ is $not$ a subspace of singular matrices. Is there a general theory for subspaces of nxn singular matrices of certain forms in $M_{nxn}$?","['matrices', 'intuition', 'linear-algebra', 'vector-spaces']"
2722026,Locally convex inductive limit topology versus cofinal topology,"Let $V$ be a complex vector space and let $\{0\} \subset V_1 \subset V_2 \subset \dots \subset V$ be an increasing sequences of subspaces of $V$, whose union is $V$. Suppose that each $V_n$ is a locally convex Hausdorff topological vector space and that every inclusion $V_{n} \subset V_{n+ 1}$ is continuous. We can then endow $V$ with two topologies: The finest locally convex vector space topology, making all inclusions $V_n \subset V$ continuous (which can be shown to exist). The finest topology on $V$ which makes makes all inclusion maps $V_n \subset V$ continuous (a general, point-set topology construction that works for all topological spaces). Both of these topologies have universal properties characterizing continuous maps out of $V$: In the first case, this universal property holds within the category of locally convex spaces, while in the second, it holds in the category of all topological spaces. My questions are: Is there an example in which the second topology does even yield continuous vector space operations on $V$? Does there exist an example in which the sceond topology does yield continuous vector spaces operations on $V$ but the two topologies are nonetheless different?","['general-topology', 'locally-convex-spaces', 'universal-property']"
2722096,"$a^n+b^{n+2}=c^{n+1}$ (any solutions, other than this one?)","I just made up an equation $a^n+b^{n+2}=c^{n+1}$ and chosen an exponents $n$ and $n+2$ and $n+1$ to be right where they are so that we have one solution, that is, a solution $(1,2,3)$ for $n=1$. Are there any other solutions for $(a,b,c) \in \mathbb N^3$ and $n \in \mathbb N$?","['number-theory', 'diophantine-equations']"
2722144,Geometric measure theory on manifolds: suggested book,"I am a newcome to geometric measure theory, but I would like to learn it directly from the context of manifolds, without reference to an Euclidean ambient space. Do you recommend a book on this topic with this point of view?","['geometric-measure-theory', 'book-recommendation', 'measure-theory', 'geometry']"
2722164,Closed orbits of a geometrically defined dynamical system,"I'm sorry I can't post the image. I don't have enough reputation points to post the image.
Please have a look here : I am not sure of the answer but after a few examples, I think that the process repeats $\frac{360}{\angle(\text{line,X-axis})}$ . And perhaps it's possible for all angles like perhaps the irrational values of $\theta$ . I guess all the Rationals will lead to a repetition. Not sure though. For example,in the picture, we have begun with a point P on the X-axis and then continued till we reached P again. Here, the angle between line l and the x-axis is 60∘ and we see 6 steps required to reach back to P.","['puzzle', 'dynamical-systems', 'geometry']"
2722193,No canonical non-quadratic residue for primes $\equiv 1 \bmod 8$?,"Let $p$ be an odd prime number. If $p \equiv 3 \bmod 8$ or $p \equiv 5 \bmod 8$, $2$ is not a square mod $p$. If $p \equiv 3 \bmod 8$ or $p \equiv 7 \bmod 8$, $-1$ is not a square mod $p$. If $p \equiv 1 \bmod 8$, there seems to be no way to choose a single $n \in \mathbb{Z}$ such that $n$ is not a square mod $p$ regardless of the choice of $p$. Can one prove that this is not the case? What if we continue searching for residues mod 16? In fact, the more general question I would like to know the answer to, is the following: Can there be a finite set of natural numbers $S$ such that that for each prime number $p$, there exists an $n \in S$ such that $n$ is not a square mod $p$?","['number-theory', 'prime-numbers', 'quadratic-residues']"
2722202,"If $F_n$ is the Fibonacci sequence, show that $F_n < \left(\frac 74\right)^n$ for $n\geq 1.$","Recall that the Fibonacci sequence is defined by $F_0 = 0$, $F_1 = 1$, and $F_n = F_{n−1} + F_{n−2}$ for $n ≥ 2$. Prove that: $$\forall \,\, n ≥ 1 ,\,\, F_n < \left(\frac 74\right)^n$$ In this question I understand how to do the basis step.
In the induction step I know that you have to assume that n=k but I am having trouble figuring out on how to do that. Could someone please explain how to do this question.","['inequality', 'fibonacci-numbers', 'discrete-mathematics']"
2722230,prove that the product of primes in a given interval is less than or equal to binom,"Let $\pi(m, n)$ denote the set of prime numbers in the interval $[m,n]$. I need to show that - $$\prod_{p\in \pi(m+1, 2m)} p \leq \binom{2m}{m} $$ I tried a lot of ways for hours and the result didn't come.","['number-theory', 'combinatorics', 'prime-numbers']"
2722327,A limit involving the Thue–Morse sequence,"(it is a follow-up on my previous question ) Let's define $t_n$ by the recurrence $$t_0 = 1, \quad t_n = (-1)^n \, t_{\lfloor n/2\rfloor}.\tag1$$ It is easy to see that $|t_n|=1$ , and the signs follow the same pattern as  the Thue–Morse sequence : $$1,\,-1,\,-1,\,1,\,-1,\,1,\,1,\,-1,\,-1,\,1,\,1,\,-1,\,1,\,-1,\,-1,\,1,\,...\tag2$$ (see this question for an example of a non-recursive formula for $t_n$ ). Now, let: $$\mathcal{L}(z)=\lim_{n\to\infty}\,\sum_{k=1}^{2^n-1}t_k\,k^z.\tag3$$ I conjecture that the following propositions hold: $\color{gray}{\text{(a)}}$ The limit $\mathcal{L}(z)$ exists for all $z\in\mathbb C$ . $\color{gray}{\text{(b)}}$ $\mathcal{L}(z)$ is an entire function of $z$ . $\color{gray}{\text{(c)}}$ $\mathcal{L}(z)=0$ if and only if $z\in\mathbb Z^+$ . Can we prove these conjectures? Can we find a different representation of $\mathcal{L}(z)$ ? Does this function have any interesting properties? Update: The ""if"" part of the conjecture $\color{gray}{\text{(c)}}$ is certainly true . I am now pretty sure the ""only if"" part fails at some points on the imaginary axis (e.g. near $z\approx i\,4.53236...$ ; is it exactly $i\,\pi/\ln2$ ?). I am still wondering if there are any zeros except positive integers and off the imaginary axis. Update: I found a very relevant and interesting paper (preprint) by Giedrius Alkauskas, Dirichlet series associated with Thue–Morse sequence $^{[1]}$ $\!^{[2]}$ (be aware that the author claims $^{[3]}$ some results in it may be incorrect). Here is my attempt to plot the function for a range of real arguments:","['conjectures', 'limits', 'number-theory', 'complex-analysis', 'sequences-and-series']"
