question_id,title,body,tags
4384712,"Seeming gap in Hartshorne's Algebraic Geometry, proof of Proposition 1.10","The claim is that if $Y$ is a quasi-affine variety (an open subset of an affine variety), then dim $Y = $ dim $\overline{Y}$ . Here dimension is defined as the maximal length of an ascending chain of irreducible closed subsets, minus one. Hartshorne starts with the observation that dim $Y \le $ dim $\overline{Y}$ (a chain of irreducible closed subsets of $Y$ induces a chain of irreducible closed subsets of $\overline{Y}$ by taking closures). He then states that a maximal chain of such subsets of $Y$ induces a maximal chain of $\overline{Y}$ (this claim is not explicitly proven, but I think I have worked through the details). The chain of irreducible closed subsets of $Y$ is $Z_0 \subset Z_1 \subset... \subset Z_n$ , where $Z_0$ is a singleton and $n + 1$ is the maximal length of such a chain. Then $\mathfrak{m} = I(\overline{Z_0}) \subset I(\overline{Z_1}) \subset... \subset I(\overline{Z_n})$ is an ascending chain of prime ideals, where $\mathfrak{m}$ is a maximal ideal. Hartshorne concludes, by maximality of the chain of closed subsets of $\overline{Y}$ , that height $\mathfrak{m} = n$ . Here is where I find the gap. Just because the chain of closed subsets of $\overline{Y}$ is maximal, in the sense that it cannot be extended to a longer chain, does not obviously mean that it is of maximal length over all such chains. If it is not of maximal length we only know height $\mathfrak{m} \ge n$ . Am I correct that Hartshorne has failed to account for this, and is there a way to fill the gap?",['algebraic-geometry']
4384741,Extraction of pointwise convergenct subsequence using Arzela-Ascoli theorem,"Let $f_n:[a, b] \rightarrow \mathbb{R}$ be a sequence of continuous functions which is uniformly bounded i.e. $||f_n||_{L^{\infty}} \leq M <\infty$ and satisfies $f_n(a)=A$ for all $n\in \mathbb{N}.$ In addition for every $\epsilon >0,$ the function $f_n$ also satisfies the following equicontinuity estimate, $$|f_n(x)-f_n(y)| \leq c(\epsilon)|x-y| \quad \quad \quad \text{ for all } x,y\in [a+\epsilon, b],$$ with $c(\epsilon) \rightarrow \infty$ as $\epsilon \rightarrow 0^+.$ Now can we conclude that up to a subsequence $f_n \rightarrow f$ pointwise where $f \in C([0,b])$ and $f(a)=A?$ P.S.: If the answer is yes, a clean proof of the same would be appreciated.","['real-numbers', 'arzela-ascoli', 'sequence-of-function', 'analysis']"
4384783,Help on this integral $I=\int_0^1 \frac{x \arctan(x)}{1-x^2}\ln\left(\frac{2}{1+x^2}\right) dx$,"$$I=\int_0^1 \frac{x \arctan(x)}{1-x^2}\ln\left(\frac{2}{1+x^2}\right) dx$$ Here is my attempt $\frac{x}{1-x^2}dx=-\frac{1}{2}d\ln(1-x^2)$ , integration by part, we got $$I=-\frac{1}{2}P-Q$$ Where $P=\int_0^1 \frac{\ln(1-x^2)\ln\left(\frac{1+x^2}{2}\right)}{1+x^2}dx$ , and $Q=\int_0^1 \frac{x\cdot\arctan(x)\cdot\ln(1-x^2)}{1+x^2}dx$ How to proceed then? or should I switch to other ways?","['integration', 'calculus', 'definite-integrals']"
4384827,Comparison of Fourier series and Laplace transform solutions to mass-spring system with Dirac-comb excitation,"I'm trying to solve the differential equation $$y''+y=\sum_{-\infty}^{\infty}\delta(x-n\pi)\qquad n \in \mathbb{Z}\tag{1}$$ with initial conditions $$y(0)=y'(0)=0\tag{2}$$ using both Laplace transform and Fourier series. I would like to see that the two methods give the same steady state time domain response. I know that the solution to this is the half-wave rectified sine, and I can get that using the Laplace transform. But I'm stuck when solving it using Fourier-series. As far as I know, the Fourier-series of the Dirac-comb with period $\pi$ is $$\sum_{-\infty}^{\infty}\delta(x-n\pi)=\dfrac{1}{\pi}+\dfrac{2}{\pi}\sum_{n=1}^\infty\, \cos(2nx).\tag{3}$$ I solved the ODE by separating this sum into $f_0(x)=\dfrac{1}{\pi}$ and then $f_n(x)=\dfrac{2}{\pi} \cos(2nx)$ , found the sum of the particular integral and the homogeneous solution for those, put them into a sum and got $$y(x)=\dfrac{1}{\pi}+\dfrac{2}{\pi}\sum_{n=1}^\infty \dfrac{1}{1-4n^2} \cos(2nx).\tag{4}$$ But the Fourier-series of the half wave rectified sine should be $$y(x)=\dfrac{1}{\pi}+\mathbf{\dfrac{1}{2}}\mathbf{\sin(x)}+\dfrac{2}{\pi}\sum_{n=1}^\infty \dfrac{1}{1-4n^2} \cos(2nx).\tag{5}$$ so I'm missing the sine term from my answer and I just can't see where it comes into the game when I solve the ODE the way I did. Any help with this would be much appreciated as I've been unable to resolve this despite many attempts.","['fourier-series', 'laplace-transform', 'ordinary-differential-equations', 'dirac-delta']"
4384833,"Birth processes, X-Bacterias in a petri dish abide to the following rules...","Question X-Bacterias in a petri dish abide to the following rules: each bacteria evolves identically and independently from the others. each bacteria is replaced by four new bacterias after a random time with $e(\beta)$ -distribution,
where $\beta = 1$ hours $^{−1}$ . Denote $X_t$ by the number of bacterias at time t and assume that $X_0 = n$ , where $n \ge 1$ . a) Show that $\mathbb E[X_t \mathbb{1}_{T_1\le t}]=e^{-\beta t}\int_{0}^{t}4\beta e^{\beta s}\mathbb E[X_s]ds$ b) Deduce that $\mathbb E[X_t]=e^{-\beta t}\int_{0}^{t}4\beta e^{\beta s}\mathbb E[X_s]ds +e^{-\beta t}$ c) Show that $\mathbb E[X_t]=e^{3\beta t}$ My attempt's a)
I am unsure how to deal with the indicator. First instinct is to say $\mathbb E[X_t\mathbb{1}_{T_1\le t}]=\mathbb E[X_t |T_1\le t]$ , which I believe is incorrect, even still, then using conditional expectation on this is tricky. b)
Feel like this requires part a) so haven't attempted this as of yet. c)
As for this, assuming I have proved b), Let $\mu(t):=E[X_t]=e^{-\beta t}\int_{0}^{t}4\beta e^{\beta s}\mathbb E[X_s]ds +e^{-\beta t}$ Now computing the following, using Leibniz's integral rule: $\mu'(t)=\frac{d}{dt}\{\int_{0}^{t}4\beta e^{\beta (s-t)}\mu(s)ds+e^{-\beta t}\}$ , we get... $\mu'(t)=\frac{d}{dt}(t)\cdot4[\beta e^{\beta (s-t)}\mu(s)]_{s=t}-0+\int_{0}^{t}\partial_t\{4\beta e^{\beta (s-t)}\mu(s)\}ds-\beta e^{\beta s}=4\beta\mu(t)-\beta\int_{0}^{t}4\beta e^{\beta (s-t)}\mu(s)ds-\beta e^{\beta s}=4\beta\mu(t)-\beta[\int_{0}^{t}4\beta e^{\beta (s-t)}\mu(s)ds+e^{-\beta t}]=4\beta\mu(t)-\beta\mu(t)=3\beta\mu(t)$ Hence, we have the following differential equation: $\mu'(t)=3\beta\mu(t)$ $\therefore \mu(t)=\mathbb E[X_t]=e^{3\beta t}$ Comments Any hints with a) would be greatly appreciated, I feel like I am missing some crucial steps and haven't made much progress over the past few hours. Moreover, some alternative solutions would be interesting to see also:)","['birth-death-process', 'ordinary-differential-equations', 'expected-value', 'solution-verification', 'probability']"
4384876,Direct limits of topological vector spaces,"It is sometimes useful in functional analysis to take direct limits of function spaces. For instance, the space $\mathcal{D}(\mathbf{R}^d)$ of test functions is the direct limit of the family of function spaces $C_c^\infty(K)$ in the category of locally convex topological vector spaces, i.e. such that a convex subset of $\mathcal{D}(\mathbf{R}^d)$ is open if and only if it's intersection with $C_c^\infty(K)$ is open for each compact set $K \subset \mathbf{R}^d$ . We can also consider the direct limit in the family of topological vector spaces. In this sense, any (not necessarily convex) subset of $\mathcal{D}(\mathbf{R}^d)$ will be open if it's intersection with each $C_c^\infty(K)$ is open. Are these topologies different? If so, what is a set which is open in the latter topology, but not the former. If these two topologies are the same, are there other examples of direct limits where the locally convex limit differs from the general limit?","['functional-analysis', 'distribution-theory']"
4384883,Grothendieck's Galois theory implies Galois theory?,"Szamuely's book Galois groups and fundamental groups formulates several variants of the main theorem of Galois theory. This is the usual formulation (dual isomorphism of posets between intermediate fields and subgroups). Then there is also Grothendieck's version (dual equivalence of categories between finite étale algebras and actions of the Galois group). Question: Does Grothendieck's version imply the usual formulation of the main theorem of Galois theory? If yes, why? This question seems to be a very natural one - but as far as I can see it is not discussed in the book (am I missing something?). There are also two versions of the classification of covering spaces: firstly, as a bijection between subgroups and covers; secondly, as an equivalence of categories between covers of $X$ and $\pi_1(X,x)$ -sets. Same question: does the equivalence of categories imply the bijection?","['galois-theory', 'fundamental-groups', 'algebraic-geometry', 'covering-spaces']"
4384887,Finite entropy and finite moments,"This question is about when a discrete distribution has finite entropy because it has finite moments. Let $X$ be a discrete random variable (which can be positive, negative, and/or zero unless stated otherwise), and let— $$H(X) = \sum_n -\mathbb{P}[X=n] \ln(\mathbb{P}[X=n]),$$ be the Shannon entropy of $X$ . It is known that: If $X$ is integer-valued and has a finite variance, then $H(X)$ is finite (Massey 1988). If $X$ is integer-valued, is 0 or greater, and has a finite mean, then $H(X)$ is finite (Rioul 2022). It is also known that a discrete distribution can have infinite Shannon entropy even if it's positive-valued (one example is some members of the zeta Dirichlet distribution); see also Devroye and Gravel 2020. However, I believe that this is so because the infinite-entropy zeta Dirichlet distributions (I think) have an infinite $z$ th moment for any real $z>0$ . Moreover, I believe that the Cauchy distribution has a finite entropy even though its mean is infinite because it does have a finite $z$ th moment for some $z$ in $(0, 1)$ , in fact for every $z$ in $(0, 1)$ . However, several questions remain on the relationship between finite entropy and finite moments. Questions If $X$ is supported on the whole set of integers and is such that $\mathbb{E}[X^2]$ is infinite but $\mathbb{E}[X^z]$ is finite for some $z$ in $[1, 2)$ , then is $H(X)$ finite? If not, under what additional conditions is $H(X)$ finite? If $X$ is supported on the integers or some subset thereof, such that $\mathbb{E}[X]$ is infinite but $\mathbb{E}[X^z]$ is finite for every $z$ in $(0, 1)$ , then is $H(X)$ finite? If not, under what additional conditions is $H(X)$ finite? If $X$ is supported on the integers or some subset thereof, such that $\mathbb{E}[X]$ is infinite but $\mathbb{E}[X^z]$ is finite for some $z$ in $(0, 1)$ , then is $H(X)$ finite? If not, under what additional conditions is $H(X)$ finite? If $X$ is supported on the integers or some subset thereof, such that $\mathbb{E}[X^z]$ is infinite for every real $z > 0$ , then is $H(X)$ infinite? Motivation My motivation is to characterize the discrete distributions with finite entropy, since only finite-entropy distributions can be sampled in finite time on average (Knuth and Yao 1976). References O. Rioul, ""Variations on a Theme by Massey,"" in IEEE Transactions on Information Theory, doi: 10.1109/TIT.2022.3141264. Massey, J.L., ""On the entropy of integer-valued random variables"", 1988. Devroye, L., Gravel, C., "" Random variate generation using only finitely many unbiased, independently and identically distributed random bits "", arXiv:1502.02539v6  [cs.IT], 2020. Knuth, Donald E. and Andrew Chi-Chih Yao. ""The complexity of nonuniform random number generation"", in Algorithms and Complexity: New Directions and Recent Results , 1976.","['statistics', 'entropy', 'probability']"
4384904,Geometric growth rate and Kelly's Criterion question,"In the Wikipedia page about Kelly Criterion, the author calculated the expected wealth after N bets as $$W * (1+g)^N$$ where $W$ is the initial wealth, and $g$ is the expected geometric growth rate. For example, with \$25 starting wealth, a 60% chance of winning/losing the whatever you wager, if our strategy is to bet 20% of current wealth, then the article says that $1+g = (1+0.2*1)^{0.6}(1-0.2*1)^{0.4} = 1.02034$ and expected wealth at round N is $W_N = 25*(1.02034)^N$ But if I look at it from another perspective: the expected wealth after 1 round is $0.6(1.2W) + 0.4(0.8W) = 1.04W$ . So shouldn't $W_N$ be $25*(1.04)^N$ instead? That is, there should only be one answer for expected wealth at round N, given our paramters p,q,f (here f is known so Kelly Criterion doesn't really come into play). However, the Wiki page has a completely different computation than what I have.","['mathematical-modeling', 'gambling', 'finance', 'economics', 'probability']"
4384963,Convergence in Law for Sum of gamma variables,"Let $X_1,X_2,...,X_n \overset{\text{iid}}{\sim} \text{gamma}(\alpha , \text{scale}=\theta )$ .
Let $Y_n := \sum_{i=1}^n X_i$ . I have found that for each positive integer $n$ , $Y_n \sim \text{gamma}(n\alpha ,\beta )$ (using moment generating functions). I am now asked to find two constants $c_1$ and $c_2$ so that $$\sqrt{n} \left ( Y_n - c_1 \right ) \overset{L}{\longrightarrow} \text{N}(0,c_2) .$$ My first thought is to quote central limit theorem, but that is only defined with sample means. I also attempted to use moment generating functions, but I had trouble with the $\sqrt{n}$ affecting when the moment generating functions exist finitely. If anyone can give a suggestion, that would be very helpful.","['statistical-inference', 'statistics']"
4384985,Proving an inequality regarding the tenth moment,"Let $Y$ be a random variable with $\mathbb{E}(Y \mid X) = X$ , where $X \sim \mathrm{U}[0,1]$ . Prove that $\mathbb{E}(Y^{10}) \geq \frac{1}{11}$ . I tried using Jensen's inequality and found out that $\mathbb{E}(Y^{10}) \geq \frac{1}{2^{10}}$ , but it's too rough. Would thank you for any suggestions. Based on suggestions from comments, I reproduce my Jensen's-inequality argument below: $X\sim\mathrm{U}[0,1]$ is a r.v. uniformly distributed in $[0,1]$ . By the law of total expectation, $$\mathbb{E}(Y)=\mathbb{E}(\mathbb{E}(Y\mid X))=\mathbb{E}(X)=\frac{1}{2}$$ $h(x)=x^{10}$ is convex, so by Jensen's inequality, $\mathbb{E}(h(X))\geq h(\mathbb{E}(X))$ , that is $$\mathbb{E}(Y^{10})≥(\mathbb{E}(Y))^{10}=\frac{1}{210}$$","['uniform-distribution', 'conditional-expectation', 'inequality', 'probability-theory', 'probability']"
4384988,Generalization of Poincare Lemma for vector fields,"Poincaré Lemma implies that for any vector field $A\in \mathbb{R}^3$ satisfying $\nabla \cdot A = \text{div }A = 0$ ,  we can express locally this vector field as $A = \nabla \times B = \text{rot }B$ , where $B$ is another vector field. Can we say something similar with respect to a symmetric 2-tensor field $\Sigma$ that satisfies $\nabla\cdot\Sigma = 0$ ? Note: $\nabla\cdot\Sigma = \sum_{k=1}^3 \cfrac{\partial \Sigma_{ik}}{\partial x^k}$","['vector-fields', 'partial-differential-equations', 'vector-analysis', 'differential-geometry']"
4385049,Lengths of Altitudes in incircle and excircle,"Let $ABC$ be a triangle. Define $(I)$ as the incircle. Let the incircle touch $AB,BC,CA$ at $A',B',C'.$ Define $E',D',F'$ as the feet of the altitude from $A',B',C'$ on $B'C',A'C',A'B'$ respectively.Denote $I_A$ as the A excentre, $X,Y,Z$ as the touch points. Consider triangles $A'B'C'$ and $XYZ.$ Find the length of the altitudes and simplify it. Let $G$ be the feet of the altitude from $X$ to $YZ.$ Then $XG=XY\times \sin(XYG)=XY\times \sin(B/2).$ And $$\frac{XY}{\sin(c/2)}=\frac{s-c}{\sin(c)}\implies XG=\frac{s-c}{\sin(c)}\times\sin(c/2)\times \sin(B/2). $$ Also $XYCI_A$ is cyclic. So we can use sin rule. We also have $EA'=A'C'\times \sin(90-C/2).$ $$\frac{A'C'}{\sin(B)}=\frac{s-c}{\sin(90-B/2)}\implies EA'=\frac{s-c}{\sin(90-B/2)}\times \sin(B)\times \sin(90-C/2)$$ And we can proceed to find others? Can someone find the other altitudes length in this way? Can we simplify them even more? We have the following progress: $$A'E = 2\left(s-b\right)\sin\left(\dfrac{\widehat{B}}{2}\right)\cos\left(\dfrac{\widehat{C}}{2}\right)$$ $$B'D=2(s-c)\sin(C/2)\cos(A/2)$$ $$B'D=2(s-a)\sin(A/2)\cos(B/2)$$ Define altitude from $X,Y,Z$ be $G,F,H.$ Then $$XG=2(s-b)\sin(B/2)\cos(C/2)$$ $$ZH=2(s-c)\cos(B/2)cos(A/2)$$","['euclidean-geometry', 'triangles', 'geometry', 'plane-geometry']"
4385061,"If the range of $y = f(x)$ is $-1\leq y\leq 2$, what is the range of $y = 1/f(x)$","If the range of $y = f(x)$ is $-1\leq y\leq 2$ , what is the range of $y = 1/f(x)$ Could someone explain why is it not $-1\leq y\leq 1/2$ ?",['functions']
4385071,Ask a question about probability calculation in statistics,"Suppose that the time to death X has an exponential distribution with hazard rate $\lambda$ and that the right-censoring time C is exponential with hazard rate $\theta$ . Let T = min(X,C) and $\delta$ = 1 if X $\leq$ C; 0,if X > C. Assume that X and C are independent. Show that $\delta$ and T are independent. My attempt: We only need to show that $P(\delta=1, T>t)= P(\delta=1) P(T>t)$ , for any non-negative t, and $P(\delta=0, T>t)= P(\delta=0) P(T>t)$ . $P(\delta=1, T>t)= P(X \leq C, T>t)= P(X \leq C, \min(X,C)>t) = P(X \leq C, X>t,C>t)= P(X \leq C, X>t | C>t) P(C>t)$ , The second term is easy to find since C is exponential distributed. I find $P(C>t) = e^{- \theta t}$ . I am stuck with the first term. $P(X \leq C, X>t | C>t) = \int_{s=t}^\infty P(X \leq C, X>t | C=S) ds = \int_{s=t}^\infty P(X \leq S, X>t | C=S) ds = \int_{s=t}^\infty P(X \leq S, X>t) ds$ since X and C are independent. Then I find $P(X \leq S, X>t) = e^{- \lambda t} - e^{- \lambda s}$ . Then I plug this value back, I get $\int_{s=t}^\infty (e^{- \lambda t} - e^{- \lambda s}) ds$ , which the first term gives me infinity, which is wrong.","['integration', 'statistics']"
4385079,Is it always possible to extend continuous functions defined on a *closed* subset of a locally compact Hausdorff space?,"In the following lemma the authors used Tietze's extension to get $f_1$ and $g_1$ . I know this version of Tietze , but it requires the subset to be compact not merely closed , i.e., continuous functions defined on compact subsets of a LCH space can be extended upto the whole space. But in the above Lemma 1.3 the subset $X_0$ was only assumed to be closed ; then how did the authors use the Tietze theorem? Question . Suppose $X_0$ is a closed subset of a locally compact Hausdorff space $X$ , and $f\in C_0(X_0)$ . Is it always possible to extend $f$ to whole of $X$ ? EDIT The lemma above is in this pdf (Lemma 1.3).",['general-topology']
4385088,Let $f:\mathbb N\to \mathbb N\;$ be a strictly increasing function s.t. $\;f\bigl(f(n)\bigr)=3n$ for all $n$. Find $f(2001)$ via a specific approach,"Let $f:\mathbb N\to \mathbb N\;$ be a strictly increasing function such that $\;f\bigl(f(n)\bigr)=3n\;$ , for all natural numbers $n$ . Find $f(2001)$ . Approach I want to follow: Replace $n$ by $f(n)$ in $\;f\bigl(f(n)\bigr)=3n,\;$ for $m$ times. Therefore $$\underbrace{f\circ f\circ f\circ \dots\circ f}_\text{m+2 times}(n)=3\cdot \underbrace{f\circ f\circ f\circ \dots\circ f}_\text{m times}(n).\tag 1\label 1$$ Let $\;a_0=n\;$ for some fixed $\;n\;$ and $\; a_{m+1}=f(a_m)\;$ $\forall \; m\ge 0$ . From Equation \eqref{1} $$a_{m+2}=3\cdot a_{m}.$$ Its characteristic equation is $$x^{m+2}-3\cdot x^m=0, \;x\ne0$$ or $$x^2-3=0$$ $$\implies x=\pm \sqrt3.$$ $$\implies a_m=A(\sqrt{3})^m\;+\;B(-\sqrt{3})^m\;\forall \; m\ge 0.$$ And $A$ must be greater than $B$ otherwise $a_{m}$ will tend to $-\infty$ when $m$ is odd. Now I am stuck. How can I proceed further using this method? Or We can't solve this problem using my method? I know other methods to solve this problem. This problem is same as ""https://math.stackexchange.com/questions/1410635/if-f-bbbn-to-bbbn-is-strictly-increasing-and-ffn-3n-find-f200"", but with a different method.","['functional-equations', 'recurrence-relations', 'recursion', 'functions', 'algebra-precalculus']"
4385102,Area of shaded region in a square,"My approach The area of shaded region = 8×the area of Given that the points of the shaded region are closer to the center than the boundary of the square. Let's talk about the boundary of the shaded region The boundary of the shaded region therefore must be the locus of all the points whose distance from the center of the square = distance from the boundary. Let's find the locus of the boundary of the shaded region From the second figure ${\sqrt {h^2+k^2} } = {\sqrt {(h-h)^2 + (k-{a\over 2})^2 }}$ This simplifies to be: $k = {a^2 - 4h^2\over 4a}$ $y = {a^2 - 4x^2\over 4a}$ Also the curve intersects the line( the hypotenuse of the triangle) y= x For point of intersection : $ {a^2 - 4x^2\over 4a} = x $ $4x^2 +4ax - a^2 = 0$ $x = {-4a ± \sqrt{16a^2 - 4×4×(-a^2)}\over 8}$ $x = a{(\sqrt{2} ± 1)\over 2}$ Solution 1: $x = a{(\sqrt{2} + 1)\over 2}$ Solution 2: $x = a{(\sqrt{2} - 1)\over 2}$ Solution 1 can be discarded as $ x = a{(\sqrt{2} + 1)\over 2} ≈ 1.207106 a > {a\over 2} $ Solution 2: $ a{(\sqrt{2} - 1)\over 2} ≈ 0.2071 a < {a\over 2} $ from 0 to $a{(\sqrt{2} - 1)\over 2}$ :
The curve (boundary of shaded region)  lies above the line $ y=x$ so $dA = ({a^2\over 4a} - {4x^2\over 4a} - x)dx $ $dA = ({a\over 4} - {x^2\over a} - x)dx $ $\int_{0}^{A} dA = \int_{0}^{a{(\sqrt{2} - 1)\over 2}} ({a^2\over 4a} - {4x^2\over 4a} - x)dx $ A = $({a\over 4}x - {x^3\over 3a} - {x^2\over 2})]_{0}^{{a{(\sqrt{2} - 1)\over 2}}}$ $ A = {a^2\over 8}(\sqrt{2}-1) - {a^2\over 24}(\sqrt{2}-1)^3 - {a^2\over 8}
(\sqrt{2}-1)^2 $ $ = {a^2\over 8}(\sqrt{2}-1) \Biggl( 1 - {(\sqrt{2}-1)^2\over 3} - (\sqrt{2} -1)\Biggr)$ This simplifies to be equal to ${a^2\over 8}(\sqrt{2}-1)({3+5\sqrt{2}\over 3})$ $= {(7- 2\sqrt{2})\over 8×3}a^2$ Area of shaded figure = 8A $A_{total}$ = ${(7- 2\sqrt{2})\over 3}a^2$ But the answer is : ${4\sqrt{2}-5\over 3}a^2 $ I don't know where I got it wrong, and also I have re calculated this and the result is same. Did I miss something important or calculated wrongly Any help of hint or suggestion or worked out solution would be appreciated.","['integration', 'area', 'locus', 'calculus', 'algebra-precalculus']"
4385108,Limit of flipping coins with $\limsup_{n\rightarrow\infty}\frac{N_n}{\log n}=\frac{1}{\log 2}$,"We flip a fair coin repeatedly and independently. Let $N_n$ be the number of consecutive heads beginning from the $n^{th}$ flip. (For example, $N_n =0$ if the $n^{th}$ flip is a tail, and $N_n =2$ if the $n^{th}$ and $(n+1)^{th}$ flips are heads but the $(n + 2)^{th}$ flip is a tail). Show that $$\limsup_{n\rightarrow\infty}\frac{N_n}{\log n}=\frac{1}{\log 2} \ \ \text{a.s.}.$$ $$$$ I suppose that problem like this is to use Borel-Cantelli two times to give an upper and lower bound of LHS, it's easy to obtain that $\mathbb{P}[\limsup_{n\rightarrow\infty}\frac{N_n}{\log n}\ge\frac{1+\epsilon}{\log 2}]=0$ using Borel-Cantelli, but I cannot obtain the other similar inequality since $N_n$ s are not independent. What should I do? Thanks! Edited: Question was changed to prove the almost sure result. Edited 2: I have an idea, that is if we can prove that $\mathbb{P}[\sup_{1\le k\le n}\frac{N_k}{\log k}\ge\frac{1}{\log 2}]$ is strictly larger than $0$ , then we can (?) apply the Kolmogorov's 0-1 law to obtain that it must be $1$ when $n\rightarrow \infty$ ? I'm not quite sure if this works······","['limsup-and-liminf', 'analysis', 'limits', 'probability-theory', 'probability']"
4385111,Prove that $ABCD$ is a parallelogram.,"Let $E , G , F , H$ be the midpoints of sides $AB , BC , CD , DA ,$ respectively of quadrilateral $ABCD.$ The common points of segments $AF , BH,CE,DG $ divide
each of into three parts, as shown in the figure. Given that $$\frac{LK}{GD}=\frac{LI}{AF}=\frac{IJ}{BH}=\frac{JK}{CE}=k$$ Prove that $ABCD$ is a parallelogram. Note that $EFGH$ is a parallelogram. And $Ar[ABCD]=2Ar[EFGH].$ And define $M=AC\cap BD.$ Note that $$2k^2\times Ar[MIJ]=2 Ar[MHB]=2Ar[MEB]=Ar[MAB].$$ Hence, we have $2k^2\times Ar[IJKL]=Ar[ABCD]\implies 2k^2\times Ar[IJKL]=Ar[EFGH].$ To prove that $ABCD$ is a paralleogram. Areas can help. It is enough to show that $Ar[MAB]=Ar[MDC]$ or show $Ar[MLK]=Ar[MIJ].$ Can we also show $IJKL$ parallelogram? It would help us.","['contest-math', 'euclidean-geometry', 'quadrilateral', 'geometry', 'plane-geometry']"
4385116,"How to evaluate the integral $\int_{0}^{\frac{\pi}{2}}\sqrt[n]{\tan\theta} d \theta$, where $n\geq 2$?","In my post , I started to investigate the integral $\int_0^{\frac{\pi}{2}} \sqrt{\tan \theta} d \theta$ and then $\int \sqrt[3]{\tan \theta} d \theta$ in post . After encountering the Beta Functions, I want to try to apply it to the integral. $$
\begin{aligned}
\int_0^{\frac{\pi}{2}} \sqrt{\tan \theta} d \theta &=\int_{0}^{\frac{\pi}{2}} \sin ^{\frac{1}{2}} \theta \cos ^{-\frac{1}{2}} \theta d \theta \\
&=\int_{0}^{\frac{\pi}{2}} \sin ^{2\left(\frac{3}{4}\right)-1} \theta \cos ^{2\left(\frac{1}{4}\right)-1} \theta d \theta \\
&=\frac{1}{2} B\left(\frac{3}{4}, \frac{1}{4}\right) \\&=\frac{\pi}{\sqrt2} 
\end{aligned}
$$ I then go further to $$
I_n=\int_{0}^{\frac{\pi}{2} }\sqrt[n]{\tan \theta} d\theta.
$$ Similarly $$
\begin{aligned}
I_n&=\int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{n}  } \theta d \theta \\
&=\int_{0}^{\frac{\pi}{2}} \sin ^{\frac{1}{n} } \theta \cos ^{-\frac{1}{n} } \theta d \theta \\
&= \int_{0}^{\frac{\pi}{2}} \sin ^{2\left(\frac{n+1}{2 n}\right)-1} \theta \cos ^{2\left(\frac{n-1}{2 n}\right)-1} d \theta \\
&=\frac{1}{2} B\left(\frac{n+1}{2 n}, \frac{n-1}{2 n}\right)
\end{aligned}
$$ Applying the theorem $$
B(x, 1-x)=\pi \csc (\pi x), \textrm{ where } x\notin Z
$$ gives $$
\boxed{\int_{0}^{\frac{\pi}{2}} \sqrt[n]{\tan \theta} d \theta =\frac{\pi}{2} \csc \left(\frac{n+1}{2 n} \pi\right)=\frac{\pi}{2} \sec \left(\frac{\pi}{2 n}\right)}
$$ which is unexpectedly beautiful and decent. Furthermore Replacing $\frac{1}{n}$ by $a$ yields $$\boxed{
\int_{0}^{\frac{\pi}{2}} \tan ^{a} \theta d \theta =\frac{\pi}{2} \csc \left(\frac{a+1}{2} \pi\right)=\frac{\pi}{2} \sec \frac{a \pi}{2}}
$$ For example, $$
\int_{0}^{\frac{\pi}{2}} \sqrt[3]{\tan \theta} d \theta =I\left(\frac{1}{3}\right) =\frac{\pi}{2} \sec \left(\frac{ \pi}{6}\right)=\frac{\pi}{\sqrt{3}} $$ $$\int_{0}^{\frac{\pi}{2}} \sqrt[6]{\tan \theta} d \theta =I\left(\frac{1}{6} \right)=\frac{\pi}{2} \sec\left(\frac{\pi}{12}\right)=\pi \sqrt{2-\sqrt{3}}$$ $$
\int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{e}}\theta d\theta =\frac{\pi}{2} \sec \frac{\pi}{2 e}
$$ $$
\int_{0}^{\frac{\pi}{2}} \tan ^{\frac{1}{\pi}} \theta d \theta=\frac{\pi}{2} \sec \frac{1}{2}
$$ checked by Wolframalpha. My question : Is there a method without using Beta Functions?","['integration', 'beta-function', 'calculus', 'definite-integrals']"
4385121,"$x_{n+1}=\log(1+x_n)$, then how can I solve $\lim{nx_n}$ within high-school level?","Given $x_{n+1}=\log(1+x_n)$ , I know $x_n\to0$ because if $\lim x_n=\alpha$ , then $\alpha=\log(1+\alpha)$ . And using Stolz-Cesaro Theorem, then $\lim nx_n=\lim\frac{x_nx_{n+1}}{x_n-x_{n+1}}$ , and it can be changed as $\lim_{t\to0}\frac{t\log(1+t)}{t-\log(1+t)}$ . It is not difficult to see that this value is 2.(Using L'Hopital, Talyor... etc.) But, how about high-school level? I don't know any other way not to use the Stolz-Cesaro theorem. Please help me if you know how to solve this in high school level.","['limits', 'calculus', 'limits-without-lhopital']"
4385133,Measure theoretic probability book for self-study [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 2 years ago . Improve this question Background: Heading into my final year of undergraduate and I'm taking a class on Measure-theoretic probability theory. I have taken introductory probability and statistics classes, as well as Real Analysis. What I'm looking for: I learn way better from books than lectures so I'm looking for a self-study book. When I was taking Real Analysis I read Understanding Analysis by Stephen Abbott and I really loved it. So I'm looking for a text that is similar in style to this, i.e adequate for self-study, explains things well with diagrams, has a decent number of exercises (and preferably solutions) and is not simply a ""reference text"". So far I've found: Probability and Measure by Billingsley Measure, Integral and Probability by Capinski Probability Theory and Examples by Durrett Foundations of Modern Probability by Kallenberg Probability Theory: A Comprehensive Course by Klenke A User's Guide to Measure Theoretic Probability by Pollard Probability and Measure Theory by Ash A First Look at Rigorous Probability Theory by Rosenthal Measures, Integrals and Martingales by Schilling Real Analysis: Theory of Measure and Integration by Yeh Obviously, a ton of choices here and I can't read all of them. Would appreciate any suggestions on the above texts or perhaps another one, thank you!","['measure-theory', 'probability-theory', 'reference-request']"
4385141,"If $f(g(x)) = g(f(x))$ $\forall x \in [0, 1]$.Prove that exist $x_0\in [0,1]$ such that $f(x_0) = g(x_0)$ [duplicate]","This question already has an answer here : Commuting functions on the closed interval have the same value somewhere (1 answer) Closed 2 years ago . Assume that $f, g: [0, 1] \rightarrow [0,1]$ are continous functions satisfying $$f(g(x)) = g(f(x))$$ $\forall x \in [0, 1]$ . Prove that exist $x_0\in [0,1]$ such that $f(x_0) = g(x_0)$ . My Attempt:
I tried using intermediate value property theorem for continuous function but couldn't find any way out","['continuity', 'calculus', 'functions', 'real-analysis']"
4385184,$\sin 12°=\ldots$ without a scientific calculator,"I would like to find the $$\color{red}{\huge\sin 12°}$$ without a scientific calculator and Maclaurin formula because my students of an high school don't know this approach. I have thought starting from $\sin 18°=(\sqrt 5-1)/4$ and the cosine $\cos 18°=(\sqrt{10}+2\sqrt 5)/4$ (from the geometry we know that the side of the regular decagon is the golden section of the radius). Being $\sin (2\alpha)=2\sin(\alpha)\cos(\alpha)$ I know $\sin 36°$ , but for the $\sin 12°$ I have thought $$\cos3α=\cos(2α+α)=\cos2α\cosα−\sin2α\sinα=$$ $$\cosα⋅(2\cos2α−1)−2\sin2α\cosα$$ $$=2\cos3α−\cosα−2\cosα(1−\cos2α)=$$ $$2\cos3α−\cosα−2\cosα+2\cos3α=4\cos3α−3\cosα$$ Considering the formulas of trisection of an angle I have: $$\boxed{\cosα=\frac 43\cos3α −3\cos \frac\alpha3}$$ We could put $\cos(α/3)=z$ and thus I will have the equation $$4z^3-3z-\cosα=0$$ It is then a matter of solving that third degree equation, of which there is a solution method (which reminds me of the famous querelle between Tartaglia and Cardano...). My problem it is this: You want to build a platform like the one in the figure to overcome a height difference $h$ of 15 cm. The inclination must be 12°. What is the length $\ell$ of the platform? (the link is https://invalsi.zanichelli.it/taoDelivery/DeliveryServer/runDeliveryExecution?deliveryExecution=kve_de_https%3A%2F%2Finvalsi.zanichelli.it%2Ffirst.rdf%23i164517281905829660045946 ) My students have not a scientific calculator during the test. How can I solve the problem simply?","['trigonometry', 'soft-question', 'education']"
4385195,Visual Illustration of a Geometric Theorem,"I am reading about the following theorem called the ""Honeycomb Conjecture"" ( https://arxiv.org/pdf/math/9906042.pdf ): This is a very famous theorem that proved that out of all possible tiling arrangements, ""a hexagonal tiling has the smallest possible perimeter to the area ratio"". This ratio is roughly 1.86. I am trying to understand how this above theorem describes the problem of the Honeycomb Conjecture. Here is what I have understood so far: A locally connected graph is a graph in which any node can only have a finite number of edges. A hexagonal grid (tiling) can be interpreted as a ""locally finite graph"" - each vertex of a hexagon can be a node, and each length of a hexagon can be an edge. ""B"" is a disk with radius ""r"". ""C"" represents a ""plane"" (in R-2) that contains an infinite number of disks ""B"" But beyond this, I am really struggling to understand the above theorem. For instance, below I have drawn an empty plane ""C"" and the same plane ""C"" containing hexagonal tilings (in R-2): Next, I have shown the plane ""C"" containing an ""infinite"" number of bounded discs ""B"" with radius ""r"" (as well as the plane ""C"" containing infinite bounded discs ""B"" with radius ""r"" containing hexagonal tilings): But I am having trouble connecting everything together. How could we draw some pictures that illustrate this theorem? Can someone please help me show how ""C"", ""B"" and ""r"" would look like in some drawing/visual diagram? How would we illustrate the unions and intersections of these components? Would it be possible to show how this drawing/visual diagram changes as ""r"" becomes bigger and bigger? Could we also show (12^0.25) in this drawing/visual diagram? Thank you!","['graph-theory', 'limits', 'circles', 'geometry']"
4385212,The outer measure of a closed interval is its length without Heine-Borel Theorem.,"I tried to prove that the outer measure of a closed interval is its length without Heine-Borel Theorem. Proving the first part $m^\star ([a,b])\leq a-b$ is easy and does not require Heine-Borel Theorem. Proving the other part  is summarized as follows: (1)-Clearly we have $[a,b]\subset(a-\epsilon ,b+\epsilon ) $ for any scalar $\epsilon>0$ . (2)- $a$ and $b$ are interior points of  any open coverng since $[a,b]$ $\subset$ $\cup_{k=1}^\infty I_k$ . (3)- The covering $\cup_{k=1}^\infty I_k$ is open. Hence there exists $\epsilon_1>0$ and $\epsilon_2>0$ such that  the intervals $(a-\epsilon_1,a+\epsilon_1)$ and $(b-\epsilon_2,b+\epsilon_2)\subset  \cup_{k=1}^\infty I_k$ . (4)- Steps (1), (2), and (3) imply that    there exists $\epsilon=min\{\epsilon_1,\epsilon_2\}$ such that $(a-\epsilon ,b+\epsilon )\subset  \cup_{k=1}^\infty I_k$ for every open covering of $[a,b]$ , i.e. for any open covering $\cup_{k=1}^\infty I_k$ we have an open cover (not subcover) that contains $[a,b]$ . In other words, for any open cover, there exists $\epsilon>0$ such that $(a-\epsilon ,b+\epsilon )$ is an open cover of $[a,b]$ with the following property: $$[a,b]\subset(a-\epsilon ,b+\epsilon ) \subset  \cup_{k=1}^\infty I_k$$ I know that the interval $(a-\epsilon ,b+\epsilon )$ is not a subcover but it is an open cover that is tighter than the open cover itself. We can take the infimum over $\epsilon$ which is the same as taking the infimum overall covering. $\textbf{EDIT: We have}$ $b-a\leq l((a-\epsilon,b-\epsilon)\leq\sum_kl(I_k))$ , $\textbf{which implies:}$ $$b-a= \inf_{\epsilon>0}\{ b -a+2\epsilon \}\leq \inf\bigg\{\sum_{k=0}^\infty l(I_k): [a,b]\subset\cup_{k=0}^\infty I_k\bigg\}= m^\star ([a,b])$$ which completes the proof. Note that I did not use the subcovering property. I used the fact that every open covering of a closed interval contains an open interval (which is not a subcover) that covers the closed set. Is there any flaw in my proof?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4385228,"Prob. 7, Sec. 4.3, in Kreyszig's Functional Analysis Book: Existence of a particular kind of a bounded linear functional on a Hilbert space","Here is Prob. 7, Sec. 4.3, in the book Introductory Functional Analysis With Applications by Erwine Kreyszig: Give another proof of Theorem 4.3-3 in the case of a Hilbert space. And, here is Theorem 4.3-3 in Kreyszig: Let $X$ be a normed space and let $x_0 \neq 0$ be any element of $X$ . Then there exists a bounded linear functional $\tilde{f}$ on $X$ such that $$
\left\lVert \tilde{f} \right\rVert = 1, \qquad \tilde{f} \left( x_0 \right) = \left\lVert x_0 \right\rVert.
$$ My Attempt: Let $x_0$ be a non-zero point of the Hilbert space $X$ , and let $$ e_0 := \frac{1}{\left\lVert x_0 \right\rVert} x_0. $$ Then we have $$
\left\lVert e_0 \right\rVert = 1.
$$ Now the functional $\tilde{f}$ on $X$ defined by $$
\tilde{f} (x) := \left\langle x, e_0 \right\rangle \qquad \mbox{for all } x \in X
$$ is linear and bounded and has norm $$
\left\lVert \tilde{f} \right\rVert = \left\lVert e_0 \right\rVert = 1.
$$ Moreover, $$
\begin{align} 
\tilde{f} \left( x_0 \right) &= \left\langle x_0, e_0 \right\rangle \\
&= \left\langle x_0, \frac{1}{ \left\lVert x_0 \right\rVert } x_0 \right\rangle \\
&= \frac{1}{\left\lVert x_0 \right\rVert } \left\langle x_0, x_0 \right\rangle \\
&= \frac{1}{\left\lVert x_0 \right\rVert } \left\lVert x_0 \right\rVert^2 \\
&= \left\lVert x_0 \right\rVert,
\end{align}
$$ as required. Is this proof satisfactory enough? Or, is it lacking in some respect?","['analysis', 'real-analysis', 'hilbert-spaces', 'solution-verification', 'functional-analysis']"
4385255,Young's and Peter-Paul's inequalities,"Following the idea from Jarchow 1981, pp. 47–55, let's retell the whole story in case some wishes to be complete. Let $1<p<\infty$ and $q$ such that $p+q=pq$ , or $$
\frac{1}{p}+\frac{1}{q}=1
$$ Let a real-valued function $f$ of the positive real number $t>0$ be defined as $$
f(t):=\frac{t^{p} a^{p}}{p}+\frac{t^{-q}b^{q}}{q}\label{1}\tag{1}
$$ By setting the derivative over $t$ equal to zero, it can be found that $$
t=(a^{-p}b^q)^{\frac{1}{p+q}}
$$ Plugging it into \eqref{1}, then the minimum of the function $f(t)$ can be found $$
\begin{align}
f(t)&=\frac{t^{p}a^{p}}{p}+\frac{t^{-q}b^{q}}{q}\\
&\ge\frac{(a^{-p}b^q)^{\frac{p}{p+q}}a^{p}}{p}+\frac{(a^{-p}b^q)^{\frac{-q}{p+q}}b^{q}}{q}\\
&=\frac{a^{\frac{pq}{p+q}}b}{p}+\frac{ab^{\frac{pq}{p+q}}}{q}=ab\left(\frac{1}{p}+\frac{1}{q}\right)=ab.
\end{align}
$$ This is all right until I need to find what relation can be built between $t$ and $\varepsilon$ in the so-called ""Peter–Paul"" inequality \begin{align}
ab\le \frac{a^2}{2\varepsilon}+\frac{\varepsilon b^2}{2}
\end{align} since I will not be able to cancel the auxiliary parameter $t$ after using AM-GM. What on earth $p$ must be equal to $q$ ?",['real-analysis']
4385292,Monty Hall Change in Problem Suggestion,"So, Let's say we have three doors and three guests in our show. A car is behind one door. The other two doors have goats . Please note.. I am not asking about the original Monty Hall problem here. This is another problem (but eventually it might be equivalent to the original one). Each guest picks a different door. and the host opens one losing door and says goodbye to one contestant.Then He asks each of the remaining guests if they would like to switch. Q1 What is winning probability of each of the remaining guests if they decide to switch? Q2 If both have to switch doesn't that mean that the computed probabilities are useless? NOTE I am aware that we have two events which aren't disjoint here Event A : Car is behind the losing door or first remaining guest's choice Event B : Car is behind the losing door or second remaining guest's choice and each has 2/3 chance to win. So, the sum here isn't 1 So, eventually.. If the answer is to Switch or NOT , doesn't it mean that we have same probability for the remaining doors.
Thanks.. please give time for discussion and don't down vote for the question if it's against your mathematical beliefs. I am suggesting a new problem here, so be open to other's ideas :)","['conditional-probability', 'monty-hall', 'probability-theory', 'probability']"
4385306,Proving an inequality using a version of Markov's inequality,"Let $X$ be a nonnegative random variable with $\mathbb{E}(X) = 1$ and $\Pr(X \geq 2) \geq 1/3$ . Prove that $\Pr(X \geq 10) \leq 1/24$ . The hint suggests generalizing the proof of Markov's inequality. I tried but didn't manage to prove the above. Would appreciate your help. Edit: I add the proofs of Markov's inequality I learned: Proof I : Assume that $X$ is continuous, and let $t > 0$ . Then $$\mathbb{E}(X) = \int_0^\infty x f_X(x) \,\mathrm{d}x = \int_0^t x f_X(x) \,\mathrm{d}x + \int_t^\infty x f_{X}(x) \,\mathrm{d}x \geq t\Pr(X \geq t)$$ since $\int_0^t x f_X(x) \,\mathrm{d}x \geq 0$ and $\int_t^\infty x f_X(x) \geq \int_t^\infty t f_X(x) \,\mathrm{d}x = t\int_t^\infty f_X(x) \,\mathrm{d}x = t\Pr(X \geq t)$ . Hence $$\Pr(X \geq t) \leq \frac{\mathbb{E}(X)}{t}.$$ A similar proof works for a discrete $X$ . Proof II : Let $t > 0$ . Let $I_{X \geq t}$ be the indicator of the event $\{X \geq t\}$ (for any event $E$ , $I_E(\omega) = 1$ for $\omega \in E$ and $I_E(\omega) = 0$ for $\omega \notin E$ ). Note that $tI_{X \geq t} \leq X$ . For every $\omega \in \Omega$ , if $X(\omega) \geq t$ then $tI_{X \geq t}(\omega) = t \cdot 1 = t \leq X(\omega)$ , and else if $X(\omega) < t$ then $tI_{X \geq t}(\omega) = t \cdot 0 = 0 \leq X(\omega)$ since $X$ is nonnegative. Since the expectation is a monotonically increasing function, $$t\Pr(X \geq t) = t\mathbb{E}(I_{X \geq t}) = \mathbb{E}(tI_{X \geq t}) \leq \mathbb{E}(X)$$ and hence $\Pr(X \geq t) \leq \frac{\mathbb{E}(X)}{t}$ .","['expected-value', 'inequality', 'probability-theory', 'probability']"
4385329,"Prove $S_4$ has no normal subgroup of order 8, or of order 3?","Prove that $S_4$ does not have a normal subgroup of order 8, nor a normal subgroup of order 3. (Dummit/Foote 3.2.14) Proof: Suppose $A$ and $B$ are normal subgroups of $S_4$ with orders 8 and 3, respectively. (i) Since $A \trianglelefteq S_4$ then for any subgroup $X\leq S_4$ we have $AX \leq S_4$ . Let $X= \{1,x\}$ such that $|x|=2$ and $x\notin A$ . Since $|A|=8$ , and $S_4$ contains more than 8 elements of order 2, then $x$ exists. By definition we therefore have $A\cap X = \{1\}$ and hence their product $AX$ has order $$|AX|=\frac{|A||X|}{|A\cap X|}=\frac{8\cdot 2}{1}=16,$$ however by LaGrange's Theorem, if $AX\leq S_4$ then $|AX|$ divides $|S_4|$ , but 16 does not divide 24, so $A$ cannot be a normal subgroup. (ii) Similar to part (i), since $B \trianglelefteq S_4$ then for any subgroup $Y\leq S_4$ we will have $BY\leq S_4$ . Let $Y= \{1, y, y^2\}$ such that $|y|=|y^2|=3$ and $y, y^2 \notin B$ . Once again, this is possible since $S_4$ contains more than 3 elements of order 3. Then we will have, $$|BY|=\frac{|B||Y|}{|B\cap Y|}=\frac{3\cdot 3}{1}=9,$$ but since 9 does not divide 24 therefore $BY$ is not a subgroup of $S_4$ , so $B$ is not a normal subgroup. Is this correct?","['normal-subgroups', 'group-theory', 'abstract-algebra', 'solution-verification']"
4385371,There is no cover from $\mathbb{R}^3$ to $SO(3)$,"I'm working with dynamical system defined on $SO(3)$ and have to deal with the issue of ""gimbal lock"" (or similar) in all coordinate charts that I've found so far. It seems that this is a necessary issue, however I'm at a loss finding a convincing argument why it always has to occur. Ideally I would be able to find a surjective (and likely not bijective) differentiable map from $\mathbb{R}^3$ to $SO(3)$ . What is the reason which prohibits such map to exist? Edits as result of comments: Suppose I find $f : \mathbb{R}^3 \to SO(3)$ sufficiently nice. Then I can pullback vector field describing dynamics from $SO(3)$ to $\mathbb{R}^3$ and study it (numerically) in an easier topology without loss of generality. Demanding that $f$ is differentiable covering is certainly sufficient but likely not necessary.","['general-topology', 'differential-topology', 'differential-geometry']"
4385391,Is an immersion an embedding almost everywhere?,"Let $M, N$ be smooth manifolds, and let $F \colon M \to N$ be a smooth immersion. I know that $F$ is a local embedding, but is it also an embedding almost everywhere ? In other words, does there exists a set of measure zero $X \subset M$ such that $F$ restricted to $M \setminus X$ is a smooth embedding?","['measure-theory', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
4385401,Proof that this isn't a complete metric space.,"Given $d:\mathbb{R}\times\mathbb{R},d(x,y):=|e^{-x}-e^{-y}|$ , $(\mathbb{R},d)$ is a, metric space (we don't have to prove this). Show that $(\mathbb{R},d)$ is not complete.
So the idea for my proof was:
Let $x_n:=n, n\in\mathbb{N}$ be a sequence of natural numbers. This sequence is a Cauchy-Sequence because let $\epsilon=N,N<n$ $|e^{-n}-e^{-m}|=e^{-n}-e^{-m}<e^{-n}<e^{-N}=\epsilon$ so $\forall\epsilon>0\exists N_{\epsilon}\in\mathbb{N}\forall n,m>N_{\epsilon}:d(x_n,x_m)<\epsilon$ . The second part is to show that $x_n$ doesn't converge in $(\mathbb{R},d)$ : Take a look at $e^{-n}\to 0$ as $n\to\infty$ which means for $\epsilon>0$ , $|e^{-n}|<\epsilon\Leftrightarrow |e^{-n}-0|<\epsilon$ but $\forall x\in\mathbb{R}:e^{-x}\neq 0$ which means that $x_n$ doesn't converge in this metric space. In conclusion $x_n$ is a Cauchy-Sequence that doesn't converge in $(\mathbb{R},d)$ which means that $(\mathbb{R},d)$ isn't a complete metric space. I know that my proof is very poorly written but I'm not a native english speaker and therefore don't usually write proofs in english. Is this example right though and is this proof valid?","['sequences-and-series', 'metric-spaces', 'real-analysis']"
4385402,Generalized integrals for Bessel Moments $\int_{0}^{\infty} x^4K_0(x)K_1(x)^3 \ln(xK_1(x))^2\text{d}x=\frac{1}{32}$,"Let $I_\nu(x)$ be the modified Bessel functions of first kind with order $\text{ }\nu$ , $K_\nu(x)$ be the modified Bessel functions of second kind with order $\text{ }\nu$ . Prerequisite Information: The integral $$\int_{0}^{\infty} x^4K_0(x)K_1(x)^3
\ln(xK_1(x))^2\text{d}x=\frac{1}{32}$$ can be shown as follows: Note that $$\frac{\text{d}}{\mathrm{d}x}\left ( -\frac{x^\alpha}{\alpha}
K_1(x)^\alpha  \right ) 
=x^\alpha K_0(x)K_1(x)^{\alpha-1}.$$ Therefore, $$\int_{0}^{\infty}x^\alpha K_0(x)K_1(x)^{\alpha-1}\text{d}x
=\frac{1}{\alpha},\qquad{\Re(\alpha)>0} .$$ And the equality immediately follows by differentiating the expression. There are also some integral identities involving Bessel functions , but not (quite) trivial. These integrals had studied in arXiv:0801.0891 . For example, $$\begin{aligned}
&\int_{0}^{\infty}K_0(x)^3\text{d}x=\frac{3\Gamma\left ( \frac{1}{3}  \right )^6 }{32\pi\cdot2^{2/3}}  ,\\
&\int_{0}^{\infty}xK_0(x)^4\text{d}x=\frac{7}{8}\zeta(3) ,\\
&\int_{0}^{\infty}xI_0(x)K_0(x)^2\text{d}x= \frac{\pi}{3\sqrt{3} },\\
&\int_{0}^{\infty}xI_0(x)K_0(x)^3\text{d}x=\frac{\pi^2}{16} .
\end{aligned}$$ In this paper, the authors determine some relations among the moments. For example, $$
\int_{0}^{\infty}K_0(x)^4\text{d}x
=\pi^2\int_{0}^{\infty}K_0(x)^2I_0(x)^2\mathrm{d}x.
$$ These relations can be generalized in many ways. Using contour integration , we conclude that $$
\int_{0}^{\infty} 
x^3 K_0(x)^5I_0(x)\left ( \pi^2I_0(x)^2-K_0(x)^2 \right ) 
\text{d}x=\frac{\pi^4}{128}.
$$ (Only one example.) Moreover, $$
\int_{0}^{\infty} 
x^{2k+1} K_0(x)^5I_0(x)\left ( \pi^2I_0(x)^2-K_0(x)^2 \right ) 
\text{d}x=
\begin{cases}
  0 & k=0, \\
 a_k\cdot\pi^4 & k\in\mathbb{Z}^{+}.
\end{cases}
$$ Where $a_k$ is always a rational number.
And we are able to compute $$
\int_{0}^{\infty} 
xI_0(\alpha x) K_0(x)^5I_0(x)\left ( \pi^2I_0(x)^2-K_0(x)^2 \right ) 
\text{d}x
$$ by expanding the $I_0(\alpha x)$ into Maclaurin series .
Another simple identity is given by $$
\int_{0}^{\infty}x^7K_0(x)K_1(x)^2K_2(x)\text{d}x
=\frac{1}{3}.
$$ Problem: I am trying to find more results but failed. Can we find the closed-forms of other moments such as $\int_{0}^{\infty}K_0(x)^5\text{d}x,
\int_{0}^{\infty}K_0(x)I_0(x)J_0(x)Y_0(x)\text{d}x$ ? Any idea would be much appreciated. Maybe interests: Two integrals (both are easy to check): $$\begin{aligned}
&\int_{0}^{\infty} \frac{x^2}{\alpha^2+x^2}K_0(x)^2\text{d}x
=\frac{\pi^2}{4}-\frac{\pi^3}{8}\alpha \left ( J_0(\alpha)^2+Y_0(\alpha)^2 \right ),
\\ 
&\int_{0}^{\infty}K_0(x)^2\cos(\alpha x)\mathrm{d}x
=\frac{\pi}{\sqrt{4+\alpha^2} }K\left ( \frac{\alpha}{\sqrt{4+\alpha^2} }  \right ). 
\end{aligned}$$ Where $K(x)=\frac\pi2{}_2F_1\left(\frac12,\frac12;1;x^2\right)$ and ${}_2F_1$ is Gauss hypergeometric function .","['integration', 'definite-integrals', 'contour-integration', 'closed-form', 'bessel-functions']"
4385416,Alternative way to solve a limit problem,"$$
\lim _{n \rightarrow \infty} \frac{1}{1+n^{2}}+\frac{2}{2+n^{2}}+\cdots+\frac{n}{n+n^{2}}
$$ I want to find the limit of this infinite series which I found in a book. The answer is $1/2$ . The solution to this limit was given by Sandwich/Squeeze Theorem, which was basically that the above function lies between: $$
\frac{1}{n+n^{2}}+\frac{2}{n+n^{2}}+\frac{3}{n+n^{2}}+\cdots+\frac{n}{n+n^{2}}
$$ And, $$
\frac{1}{1+n^{2}}+\frac{2}{1+n^{2}}+\cdots+\frac{n}{1+n^{2}}
$$ series and the limit of both of these series tend to $1/2$ as $n \to \infty$ . I fully understood the solution, but I find that this isn't something that naturally/intuitively comes to your mind. I mean we need to find two different series by trial and error, both of which need to converge to a single number. Is there any different solution to this limit problem, like dividing by powers of n, or maybe telescoping sums?","['limits', 'calculus', 'telescopic-series', 'sequences-and-series']"
4385468,"Does there exist any $p >0$ such that $\frac{1}{n^p \sin(n)} \to 0 \;,n\to+\infty$?","Does there exist any $p >0$ such that \begin{equation*}
    \frac{1}{n^p \sin(n)} \to 0 \;,n\to+\infty \;?
\end{equation*} If there is one, what's the infimum of those $p$ ? Is it also a minimum? I started wondering about it since the set of limit points of $\big(\sin(n)\big)_{n \in \mathbb{N}}$ is the whole interval $[-1,1]$ , so it seems interesting to quantify how fast this sequence clusters around $0$ .","['limits', 'sequences-and-series', 'elementary-number-theory', 'real-analysis']"
4385510,What is the solution to this vector differential equation?,"What is the solution to the matrix differential equation: $$ \frac{dx}{dt} = \boldsymbol{A}x(t) + \boldsymbol{B}u$$ Where, $ x(0) = x_0 $ , $A_{n \times n}$ is a square matrix and $B_{n\times1}$ is $n\times1$ matrix Assuming u is constant for the time interval $[0, T]$ Also please refer me to a resource where I can learn to solve this types of equations, because solving the equations were not covered in my linear algebra course.","['matrices', 'matrix-calculus', 'linear-algebra', 'ordinary-differential-equations']"
4385516,Markov Chains and Conditional Probability: Easy rat in the maze problem,"Why is the following solution to this problem incorrect, A rat is trapped in a maze with three doors and some hidden cheese. If the rat takes door one, he will wander around the maze for 2 minutes and return to where he started. If he takes door two, he will wander around the maze for 3 minutes and return to where he started. If he takes door three, he will ind the cheese after 1 minute. If the rat returns to where he started he immediately picks a door to pass through. The rat picks each door uniformly at random. How long, on average, will the rat wander before finding the cheese? Construct a graph with three vertices: X, Y, Z, each of which
represents a door in the original problem. Add edges between every vertex, including self-loops, each of which has a weight of 1/3. Take the stationary distribution to be (1/3, 1/3, 1/3). It is unique because the probability transition question for the graph in (1) is regular (i.e., its limit is just the same matrix with all entries equal to 1/3) Compute the expected time as 2 1/3 + 3 1/3 + 1*1/3 = 2 The correct answer is 6. I know that the set-up is completely wrong and that I should use conditional probability, but is there a way to do it like this? The main issue is that we don't know the total time spent in the maze (this is kind of what we need to find) so I can see that it doesn't really make sense to take this approach.","['markov-chains', 'discrete-mathematics', 'probability']"
4385521,"X,Y independent and normally distributed. What is the expected length and angle of the vector (X,Y)?","Let $X, Y$ be independent, normally distributed random variables with equal variances and means $\mu_X, \mu_Y$ . What is the expected length of the vector $(X,Y)$ and what is the expected angle with respect to the $x$ -axis? If the means are $0$ , then the length is Rayleigh distributed and the angle is uniformly distributed. But what if the means are not $0$ ? Is there a way to calculate the integrals to get their mean and variance (or other properties)?","['statistics', 'probability-distributions', 'probability']"
4385558,Large charts on smooth manifolds,"I suddenly found out that in all the examples coming to my mind a (compact) smooth manifold has a ""large"" coordinate chart whose complement has positive codimension. In other word, is it true that for any smooth compact manifold $M$ there exists a closed (probably singular) submanifold $N$ of positive codimension such that $M\setminus N$ is diffeomorphic to $\mathbb{R}^n$ ? And the same question about topological manifolds with ""diffeomorphic"" replaced by ""homeomorphic"" and the complement of positive codimension replaced by some appropriate notion of ""small"" subset.","['geometric-topology', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
4385568,"Fiber bundles, hyperbolic surfaces and Thurston geometries","Let $ M $ be a 3-manifold which is the total space of a fiber bundle (with connected nontrivial fiber and base) with either the fiber or base a hyperbolic surface. Then does $ M $ always admit a Thurston geometry? The fundamental group of a hyperbolic surface is not virtually solvable. Therefore any bundle with a hyperbolic surface as base or fiber cannot have a virtually solvable fundamental group. So it can only admit $ \tilde{SL_2}, H^2 \times E^1 $ or $ H^3 $ geometry. Bearing this in mind here are some examples of bundles with these three geometries: $ \tilde{SL_2} $ : The unit tangent bundle of any hyperbolic surface. $ H^2 \times E^1 $ : The mapping torus of any periodic (finite order) mapping class of a hyperbolic surface. Including the trivial product of a circle with any hyperbolic surface. $ H^3 $ : The mapping torus of any pseudo-Anosov mapping class of a hyperbolic surface. Another thing to note: The fundamental group of a circle bundle always contains a normal copy of $ \mathbb{Z} $ by LES homotopy. Thus a circle bundle over a hyperbolic surface $$
 S^1 \to M \to \Sigma_g 
$$ has a fundamental group which contains a normal $ \mathbb{Z} $ subgroup. Therefore, a circle bundle over a hyperbolic surface can only admit $ \tilde{SL_2} $ or $ H^2 \times E^1 $ geometry (such examples of circle bundles are given above with the unit tangent bundle admitting $ \tilde{SL_2} $ geometry and the trivial circle bundle admitting $ H^2 \times E^1 $ geometry).","['riemannian-geometry', 'fiber-bundles', 'smooth-manifolds', 'geometric-topology', 'differential-geometry']"
4385574,Drag exerted on condensing raindrop,"Newton's second law of motion $F = ma$ = $m{dv\over dt}$ can be written in the form $F = {d\over dt}(mv)$ in terms of the momentum $mv$ of a particle of mass $m$ and velocity $v$ , and remains valid in this form even if $m$ is not constant, as assumed so far. Suppose a spherical raindrop falls through air saturated with water vapor, and assume that by condensation the mass of the raindrop increases at a rate proportional to its surface area, with $c$ the constant of proportionality. If the initial radius and velocity of the raindrop are both zero, show that the drag exerted by the condensation of the water vapor has the effect of making the raindrop fall with acceleration ${1\over 4}g$ . Hint: Show that ${d\over dr}(r^3v) = ({\delta \over c})r^3g$ , where $r$ is the radius of the raindrop and $\delta$ is its density. This is a question from a math textbook from the chapter integration and differential equations My approach Given that, ${dm\over dt} \propto 4\pi r^2$ ${dm\over dt} = c\cdot 4\pi r^2\cdot \delta \cdot{dr\over dt}$ since mass = density × volume $m = {4\over 3}\pi r^3\cdot\delta$ I can't write the equation for $v$ . Also I am totally stucked at this step. Plus I can't reach the hint given in question. Any kind of help or suggestion or worked out steps would be appreciated.","['integration', 'physics', 'calculus', 'ordinary-differential-equations']"
4385652,$ \tan{\left\lfloor{x}\right\rfloor}$ self-similar?,"As you can see here, there is plenty of what appears to be self-similarity within the graph of $ \tan{\left\lfloor{x}\right\rfloor}$ . Every $7\pi$ interval along the x-axis there appears to be a repeating backwards tangent graph made from the integer flooring, but one can also see larger forwards tangent graphs. What's the explanation behind all of this?","['desmos', 'ceiling-and-floor-functions', 'graphing-functions', 'trigonometry', 'fractals']"
4385653,Showing $\mathbb{E}[\exp(\lambda X) | G] \leq \exp\left(\frac{\lambda^{2}}{2}\right)$ for a random variable $X$ and sub-sigma-algebra $\mathcal G$,"Can anyone help me finish off a proof that, given $\mathcal G$ a sub- $\sigma$ -algebra of $\mathcal F$ , $\lambda > 0$ , and $X$ a random variable with $\mathbb{E}[X | \mathcal G] = 0, \mathbb{P}[|X| \leq 1] = 1$ , $$\mathbb{E}\left[\exp(\lambda X) | \mathcal G\right] \leq \exp\left(\frac{1}{2}\lambda^{2}\right)?$$ I've tried to use the fact that $$\exp(\lambda x) \leq \frac{1-x}{2} \exp(-\lambda) + \frac{1+x}{2} \exp(\lambda)$$ for $x \in X$ , since the exponential is convex, but I can't figure how to reduce it from here. It also is a bit of a particular challenge since I don't think I can necessarily assume the RV is normal given the wording of the question.","['conditional-expectation', 'moment-generating-functions', 'martingales', 'convex-analysis', 'probability-theory']"
4385668,Why does differentiation of a tensor increase its rank?,"There is a statement in both Wald and Carroll's GR texts that, in short, state that the derivative of a $(k,l)$ -tensor is a $(k,l+1)$ -tensor. In both places this as stated as though it should be obvious, but I am having trouble developing an intuition for this. Could someone explain this? I am most comfortable thinking about tensors with the ""slot machine"" intuition, so if it could be explained in terms of ""why the derivative increases the number of vector slots"" that would be appreciated.","['general-relativity', 'tensors', 'derivatives', 'tensor-rank']"
4385676,"Let $Y_n$ be i.i.d with $EY_n = 1, P(Y_n = 1) < 1, X_n = \prod_{k=1}^n Y_k$. Show martingale convergence of $X_n \to 0$ a.s.",Let $Y_n$ be a sequence of non-negative i.i.d random variables with $EY_n = 1$ and $P(Y_n = 1) < 1$ . Consider the martingale process formed by $X_n = \prod_{k=1}^n Y_k$ . Use the martingale convergence theorem to show that $X_n \to 0$ almost surely. I see that the Martingale convergence theorem says that $X_n \to X$ almost surely with $E \lvert X \rvert < \infty$ . I don't see how to reach the conclusion that $X = 0$ or $X_n \to 0$ . I see we can prove that $E \lvert X_n \rvert < \infty$ and that $X_n$ is uniformly integrable and $X_n \to X$ in $L^1$ . And that $X_n = E(X \mid \mathcal{F}_n)$ .,"['stochastic-processes', 'convergence-divergence', 'probability-theory', 'martingales']"
4385735,How to solve for the total derivative with respect to a parameter for a system of two embedded functions?,"My problem has two embedded functions: $$E = F(x, P)$$ $$P = f(x, E)$$ where $x$ is a parameter. I want to sign the total derivative of $E$ wrt to $x$ , i.e. $dE/dx$ . Can this be solved? If so, is it correct to simply proceed as follows? $$dE = F_1dx + F_2dP$$ $$dP = f_1dx +f_2dE $$ where $F_1$ is the partial derivative of $F$ wrt the first argument $x$ , and $F_2$ is the partial derivative wrt a change in $P$ . Similarly for $f_1$ and $f_2$ . Substituting: $$dE = F_1dx + F_2(f_1dx +f_2dE)$$ $$dE(1 - F_2f2) = (F_1 + F_2f_1)dx $$ $$dE/dx =  (F_1 + F_2f_2)/(1 - F_2f_1)$$ I have priors on all the partial derivatives: $F_2 < 0$ and $f_1 > 0$ so the denominator is positive. The numerator is negative, so if above calculations are legit $dE/dx < 0$ . Thanks.","['calculus', 'derivatives']"
4385741,Derivation of E(X) for a geometric distribution,"I'm having trouble following the derivation of the expected value for the geometric distribution. I've reached: $$\sum_{k=1}^{\infty}
    p \cdot k(1-p)^{k-1} = 
    p(1 + 2(1-p) + 3(1-p)^2 + ...)
$$ The next step rewrites $k$ as an infinite sum of ascending values, that is: $$=p( \sum_{k=1}^{\infty}\ (1-p)^{k-1} + \sum_{k=2}^{\infty}\ (1-p)^{k-1} + \sum_{k=3}^{\infty}\ (1-p)^{k-1} + ...)$$ Why exactly is it that we can represent $k$ as this infinite sum?","['probability-distributions', 'probability', 'sequences-and-series']"
4385742,How close can polynomials be in magnitude?,"If $p$ and $q$ are polynomials and $|p(t)|=|q(t)|$ on the interval $[0,1]$ , then in fact $p(t)=\omega q(t)$ for some constant phase $\omega$ . I am curious about quantitative strengthenings of this fact.  For example, suppose that $p$ and $q$ are polynomials (with possibly complex coefficients) of degree $d$ and $$
\max_{t\in[0,1]}
||p(t)|-|q(t)|| \leq \delta.
$$ When it is possible to conclude that there exist a phase $\omega\in\mathbb{C}$ , $|\omega|=1$ , such that $$
\max_{t\in[0,1]}
|p(t) - \omega q(t)| 
$$ is small? For example, if $q=1$ and $p_d$ is the Taylor approximation to $e^{it/2}$ given by $$
p_d(t) = \sum_{j=0}^d \frac{1}{j!}(it/2)^j,
$$ then $$
\max_{t\in[0,1]} |p_d(t) - e^{it/2}|
\leq C \frac{2^{-d}}{d!},
$$ which means that $||p_d(t)|-1|$ is very small on the interval $[0,1]$ , but $p_d$ is also not close to any constant phase. I suspect for example that this is close to the best possible, meaning that for example if $p$ is a polynomial of degree $d$ and $$
\max_{t\in[0,1]} ||p(t)|-1| \leq \delta (Cd)^{-d}
$$ for some large $C$ , then it follows that $|p(t)-\omega|$ can be bounded by $\delta$ somehow.  I am curious what bounds are available to this effect.","['polynomials', 'real-analysis']"
4385792,Collision of Ball in Triangle,"A ball with position and velocity $(P_0,V_0)$ is in a triangle. Which side of the triangle it will hit? Calculations : The ball's motion is $$L( t) = P_0+t*V_0 = \ ( V_{0_{x}} t+P_{0_{x}} ,V_{0_{y}} t+P_{0_{y}}) \tag{1}$$ The sides of the triangle are given by $A_1x+B_1y=C_1$ , and we can represent the other two sides similarly. Substituting for $x$ and $y$ in $(1)$ , we have: $$A_1x+B_1y=C_1 \rightarrow A_{1}( V_{0_{x}} t_1+P_{0_{x}}) +B_{1}( V_{0_{y}} t_1+P_{0_{y}}) =C_{1} \tag{5}$$ $$t_{1} =\frac{C_{1} -A_{1} P_{0_{x}} +B_{1} P_{0_{y}}}{A_{1} V_{0_{x}} +B_{1} V_{0_{y}}} \tag{6}$$ Problem : When I compute my solution in Desmos, my formulas produce the wrong minimum time. For instance, even though the ball would clearly hit the Wall 2 (Blue), the minimum time is $t_3$ ! I've tried many things, but to no avail. Any help is greatly appreciated. Link to Desmos Computation: Here","['parametric', 'geometry']"
4385815,"Find some function $f$ such that $ f(2,3) < f(3,2) < f(2,4) < f(4,2) < f(3,4) < f(4,3) < ... < f(13,14) < f(14,13) $","For context, what this question is essentially asking is if there is some simple function by which the following properties emerge: $$ 1. - \forall a,b\in\mathbb{Z},\; (a > b) \iff (f(a,b) > f(b,a)) $$ $$ 2. - \forall a,b,c\in\mathbb{Z},\; (a > c) \iff (f(a,b) > f(c,b)) $$ $$ 3. - \forall a,b,c\in\mathbb{Z},\; (b > c) \iff (f(a,b) > f(a,c)) $$ $$ 4. - \forall a,b,c,d\in\mathbb{Z},\; (\max(a,b) > \max(c,d)) \implies (f(a,b) > f(c,d)) $$ If this is impossible or too difficult, then I would still be perfectly happy with a simple function (preferably a linear function with integer coefficients) by which the following arises: $$ f(2,3) < f(3,2) < f(2,4) < f(4,2) < f(3,4) < f(4,3) < ... < f(13,14) < f(14,13) $$ Edit: It doesn't have to be a linear or even an elementary function, although it would be nice.",['functions']
4385821,"Let f be a continuous real valued function on the compact interval [a,b]. Given ϵ>0, show that there is a polynomial p such that: |p(x)−f(x)|<ϵ","Let $f$ be a continuous real valued function on the compact interval $[a,b]$ . Given $\epsilon > 0$ , show that there is a polynomial $p$ such that: $p(a)=f(a)$ , $p'(a)=0$ and $|p(x) - f(x)| < \epsilon$ This is a question I came across in Pugh's Real Mathematical Analysis. Is this not a simple application of the Weierstrass Approximation Theorem and the fact that the set of polynomials is dense in $C^0([a, b], \mathbb{R})$ . Density means that for each $f \in C^0$ and each $\epsilon > 0$ there is a polynomial function $p(x)$ such that for all $x \in [a, b]$ , $|f(x) − p(x)| < \epsilon$ . So the third property being asked is immediately satisfied. Is this correct? But how does this imply that $p(a)=f(a)$ and $p'(a)=0$ ? Am I misreading the question or not understanding it at all? Any tips or clarifications would be greatly appreciated. $\mathbf{Edit:} \mathbf{ Attempted} \mathbf{ Solution}$ $f(x)$ is a continuous function on the compact interval $[a,b]$ . By the Weierstrass Approximation Theorem we can find a polynomial $q(x)$ such that: $|f(x)-q(x)|<\epsilon/2$ So, let $p(x)=q(x)-q(a) +f(a)$ , then $p(a)=f(a)$ Given $\epsilon > 0$ , define $q(x) = (\frac{b-a}{n}$ ) $(\frac{x-a}{b-a})^n$ for large $n$ Then, $q'(x)=(\frac{x-a}{b-a})^{n-1}$ so $q'(a)= 0$ and thus $p'(a)=0$ Finally, $|p(x) - f(x)| < |q(x) - f(x)| + |q(a)-f(a)|$ $|p(x) - f(x)| < \epsilon/2 + \epsilon/2$ $|p(x) - f(x)| < \epsilon$ Is this a reasonable solution? Thank you kindly.","['weierstrass-approximation', 'metric-spaces', 'real-analysis', 'solution-verification', 'general-topology']"
4385830,Effect of conditioning on quantiles,"Cross posted on Cross Validated https://stats.stackexchange.com/questions/565661/effect-of-conditioning-on-quantiles Suppose that we have three continuous, independent, non-negative random variables $X,Y,Z$ . Fix $q\in(0,1)$ and suppose that $$q=\mathbb{P}(X+Y\leqslant \tau)=\mathbb{P}(X+Y+Z\leqslant \pi).$$ For $\mu>0$ such that $\mathbb{P}(X\leqslant \mu)>0$ , is it true that $$\mathbb{P}(X+Y\leqslant \tau|X\leqslant \mu) \geqslant \mathbb{P}(X+Y+Z\leqslant \pi|X\leqslant \mu)?$$ In other words, what effect does replacing the unconditional sums $X+Y, X+Y+Z$ with $(X+Y)|(X\leqslant \mu)$ , $(X+Y+Z)|(X\leqslant \mu)$ have on the quantiles?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4385834,Finding proper intercepts of a square root function,"Apologies for not knowing how to insert the proper math symbols - new to the forum. The problem asks to find the $x$ and $y$ intercepts of $y=\sqrt{x+49}$ . Solving easily yields the $x$ -intercept at $-49$ , but setting $x$ to zero yields $y$ -intercepts of $+7$ and $-7$ . Of course, graphing the function clearly shows that the $-7$ value is NOT in this function, as the graph begins at $(-49,0)$ and continues on a positive slope, crossing the y-axis at $+7$ , and continuing to infinity. My issue is the software being used is forcing me to write the y-intercept at $+7$ and $-7$ . It is ALEKS, if anyone is familiar with the program. Is this an AI glitch, or am I off in my assertion?",['algebra-precalculus']
4385911,"""Playing"" a Combinatorial Math Game I Invented","Recently, I thought of the following ""game"" that I would like to frame as a combinatorial optimization problem: Assume there are 5 Baskets. The first basket has 5 discrete objects (e.g. apples), the second basket has 3 discrete objects (e.g. oranges), the third basket has 1 discrete object (e.g. watermelon), and the fourth and fifth baskets have 11 kilograms of some continuous object (e.g. coffee and rice - not the right analogy, but bear with me) Assume that there exists some discrete and non-differentiable (black box) function"" which assigns a cost to combinations of objects from different baskets. For example : A1,A2, B2, D = (1 : 2.5), E = (6.1 : 7, 8.1 : 9.2) might have a cost of ""7.1"" and A1,A3, B1, C = C1 D = (1 - 8), E = (5.1 - 5.5) might have a cost of ""8.773"". For this ""game"", here are the rules: There are 3 players. The first player chooses some items from some of these 5 baskets. If the first player wanted, the first player could pick everything and leave the other 2 players with nothing. The first player also has the choice of ignoring baskets if he want to (e.g. A1, A2, C1, D = (1 : 3.1) ) From the remaining items, the second player chooses some combination of items. The second player also has the option of selecting all the remaining items and leaving the third player with nothing. Finally, the third player chooses items from the remainder (at the end, some items can remain unchosen by all 3 players). Once each player has made their selections, the ""function"" assigns a cost to each of their selections. This function has the general form of : f(selection A = a, B = b, C = c, D = d, E = e) = Cost Let's say for the purpose of this game, the cost function isn't  ""linear"". For example f(A = A1, B = 0, C = 0, D = 0, E = 0) = 3.1 , f(A = A2 , B = 0, C = 0, D = 0, E = 0) = 1.6 , f(A = (A1, A2) , B = 0, C = 0, D = 0, E = 0 ) = 0.89. The goal of the game is for: Objective: All 3 players have to try and ensure that the total summed cost of the group's selection is as close to 30 as possible (a cost of exactly 30 automatically results in a win), but any cost over 30 automatically results in a loss. Either all 3 players win together or lose together. Constraint: Each player should at least have 1 Red Square in their selection - this will ensure none of the players can finish the game with an empty basket. Thus - what is the optimal selection that each player should make such that the group's objective is maximized and the constraint is met? My Question: To me, this ""game"" seems to be some variant of the ""Knapsack Optimization Problem"" or a ""Assignment/Resource Allocation Problem"". In particular, this seems to be a Discrete Combinatorial Gradient-Free Optimization Problem (an Mixed Integer Programming Problem). Suppose these these 3 players can play this game over and over while they study ""how their selections influence the overall cost"" and ""which selections result in the cost function being closer to the desired value"". For instance: Round 1: Player_1_Cost = 12.1, Player_2_Cost = 8.5, Player_3_Cost = 19.11. Total Cost = 30 - 39.7 = - 9 .7 Round 2: Player_1_Cost = 1.5, Player_2_Cost = 0.5, Player_3_Cost = 0. Total Cost = 30 - 2 = + 28 Round 1000: Player_1_Cost = 9.5, Player_2_Cost = 7, Player_3_Cost = 8 . Total Cost = 30 - 24.5 = + 5.5 In this case, the players would keep playing the game until they start to ""learn"" which selections will result in the ""Total Cost"" being closest to 0. At first, the players might simply pick random selections and observe the ""Total Cost."" Later, they might use a more sophisticated approach such as ""Evolutionary Algorithms and Metaheuristics"" to ""strategically combine"" successful selections from the past and gradually progress towards more optimal selections: Although I know very little about Game Theory, I think that we might be able to view a ""progression and evolution"" of selection strategies as the number of rounds increases. For instance, Player 1 always chooses first - at first, Player 1 might behave very erratically and leave Player 2 and Player 3 in a position that will make it impossible for the group to win the game; but as all 3 players continue to play the game and try to ""reverse engineer"" the nature of the Cost Function by studying the impact of including/removing certain items on the Total Cost; they might be able to learn how to strategically cooperate with each other to win the game as a group. Can someone please tell me if this ""game"" that I have created can be interpreted as a (Discrete Combinatorial) Optimization Problem? Does this problem that I have created correspond to some a pre-existing type of Optimization Problem? Could certain types of Optimization Algorithms (e.g. Evolutionary Algorithms) make progress in this game and slowly start to ""learn"" optimal selections? Thanks!","['optimization', 'combinatorics', 'discrete-mathematics', 'linear-programming']"
4385963,Question about a solution of $y'(t) \le Ky(t)$ for $t \ge 0$ implies $y(t) \le y(0)e^{Kt}$,"There is a step in the following solution that I am not sure it is correct, but maybe I am missing something. The problem is: Let $K$ be a real constant. Suppose that $y(t)$ is a positive differentiable function satisfying $y'(t) \le Ky(t)$ for $t \ge 0$ . Prove that $y(t) \le e^{Kt}y(0)$ for $t \ge 0$ . And the solution is: From the given inequality we get $0 \ge e^{-Kt}y'(t)-Ke^{-Kt}y(t)=\frac{d}{dt}\left(e^{-Kt}y(t)\right)$ for $t \ge 0$ ; integrating from $0$ to $t$ we find that, for $t \ge 0$ , $e^{-Kt}y(t)-y(0) \le 0$ from which the desired inequality follows. I am not sure about the step where the author integrates from $0$ to $t$ : in the hypotheses we have, we know that $y$ is differentiable and, since $e^{-Kt}$ is indefinitely differentiable, so $y(t)e^{-Kt}$ is at most differentiable in this context. So $\frac{d}{dt}y(t)e^{-Kt}$ is not  continuous, and $\frac{d}{dt}y(t)e^{-Kt}$ is not monotonic as far as we know, hence there is no theorem (as far as I know) that guarantees the integrability of $\frac{d}{dt}y(t)e^{-Kt}$ in the interval $[0,t]$ . Moreover, where the hypothesis that $y$ is positive is used? My solution is similar, but after getting to $\frac{d}{dt}\left(e^{-Kt}y(t)\right) \le 0$ I proceed by saying that so $y(t)e^{-Kt}$ is decreasing for $t \ge 0$ and so $y(t)e^{-Kt} \le y(0)e^{-K \cdot 0}=y(0)$ , so $y(t) \le e^{Kt}y(0)$ . Is my solution correct? I am not using that $y$ is positive as well.","['inequality', 'derivatives', 'real-analysis']"
4385967,Necessary and sufficient conditions for existence of solutions to $\Delta \phi = f$ on torus,"Let $\mathcal{T}$ be a torus with Riemannian metric. Consider the sourced Laplace equation on $\mathcal{T}$ : \begin{align}
\tag{1}
\Delta \phi = f.
\end{align} I'd like to know necessary and sufficient conditions on $g$ which guarantee the existence of a solution for $\phi$ . One such condition is easy to spot: integrating both sides of (1) gives \begin{align}
\tag{2}
\int \text{vol }f   = 0.
\end{align} I'd previously always thought that (2) was both necessary and sufficient, but now I'm not so sure. The reason for my doubt is that under a conformal rescaling of the metric $g'_{\mu\nu}=e^{2\Omega}g_{\mu\nu}$ , the Ricci scalar changes as $$\tag{3}\sqrt{g'}R' = \sqrt{g}(R-2\Delta \Omega).$$ By Gauss-Bonnet, $\int\text{vol }R=0$ , so if (2) was sufficient for existence, we'd always be able to find $\Omega$ such that the RHS of (3) vanishes. Hence we'd find every torus is conformally flat, which is not true (the moduli space of the torus has dimension 1). Therefore my guess is that there must be further conditions on the source function, on top of (2), which are necessary for solutions to exist to the sourced Laplace equation. What are these conditions?","['partial-differential-equations', 'harmonic-functions', 'differential-geometry']"
4385989,Prove that the sum of Cauchy distribution random variables divide by a infinite sequence convergence,"Let $X_1,...,X_n$ are independent identical random variables of standard Cauchy distribution (i.e. the density is $\frac{1}{\pi(1+x^2)}$ ). Let $n \to \infty$ and consider the random variable $S_n=X_1+,...,+X_n$ . Prove: $\frac{S_n}{a_n}\rightarrow0$ a.e. iff $\sum \frac{1}{a_n}<\infty$ . Try to give the necessary and sufficient conditions for which $$
\frac{\max(X_1,...,X_n)}{a_n}\to 0 \quad\text{a.e.}
$$ My ideas so far : For problem 1, given $\sum \frac{1}{a_n}<\infty$ , I would like to prove that $\frac{S_n}{a_n}\rightarrow0$ : by Kroencker lemma, I just need to prove that $\sum\frac{X_n}{a_n}<\infty.$ Then I do it by applying Kolmogorov three series theorem: thus I can easily prove the convergence of $\sum\frac{X_n}{a_n}$ . But how to prove the converse? What can I deduce from $\frac{S_n}{a_n}\rightarrow0$ ? Maybe I should try to use characteristic function? For problem 2, I think maybe there is some way to convert $\frac{\max(X_1,...,X_n)}{a_n}$ to a similar form as in problem 1: am I right? Thanks in advance for any tips or help in general.","['probability-theory', 'probability', 'sequences-and-series']"
4385990,Solving recursive formula dependent on a periodic function,"Say we have some initial values $a_1, a_2, \dots, a_k$ and a recursive formula $a_n = f(n) + c_{1}a_{n-1} + c_{2}a_{n-2} + \dots + c_{k}a_{n-k}$ Where $f(n)$ is periodic and $a_n,f(n),c_i \in \mathbb{N}$ . For example let $a_1 = 1$ and $a_n = f(n) + 2a_{n-1}$ where $f(n) = 1,2,1,2 \dots$ How can we generally solve this kind of formula? I was initially thinking to solve the linear recurrence relation for each value of $f(n)$ and then merge the subsequences, but not sure how this should be done. The main issue is how to describe the extra shifting done each time we get a higher value for $f(n)$ In the example we can solve $a_{n,1} = 1 + 2a_{n-1,1}$ and get $a_{n,1} = 2^n - 1$ And solve $a_{n,2} = 2 + 2a_{n-1,2}$ and get $a_{n,2} = 3\cdot2^{n-1} - 2$ Though I am not sure how to merge them to get $a_n$ which first values are $a_n= 1, 3, 8, 17, 36,..$ given by multiplying by $2$ and adding $1$ or $2$ alternately.","['periodic-functions', 'combinatorics', 'recurrence-relations']"
4386054,A Geometric Construction problem that's got me stumped since 1975,"Hello, I'm a retired civil engineer from Greece. Ever since I was a student I have really liked maths. I had to solve this problem during my first year at NTUA, back in 1975! I couldn't solve it then and my question remains: what is the solution? The problem is as follows: Let there be two lines (e1) and (e2) and a point A beyond them. Both lines and the point A are on the same plane. How can we construct a line (x1) starting from A which intersects the line (e1) on point B and the line (e2) on point D and similarly a line (x2) starting from A which intersects the line (e1) on point C and the line (e2) on point E so that BC = a cm and DE = b cm, where the lengths a,b are given. TIA","['geometry', 'geometric-construction']"
4386105,Alternative concepts for tangent spaces of smooth manifolds and derivatives of smooth maps,"The derivative of a smooth  map $f : U \to V$ between open subsets $U \subset \mathbb R^m, V \subset \mathbb R^n$ at $p \in U$ is a linear map $df_p : \mathbb R^m \to \mathbb R^n$ which is charcterized by the well-known property $\lim_{h \to 0} \dfrac{\lVert (f(p +h) - (f(p) + df_p(h))\rVert}{\lVert h \rVert} = 0$ . This concept relies on the linear structures of domain and range of $f$ which are given locally around $p$ and $f(p)$ . For a smooth  map $f : M \to N$ from an $m$ -manifold $M$ to an $n$ -manifold $N$ this not work because we do not have (in general) canonical linear structures around $p$ and $f(p)$ . We can of course choose charts $\phi : U \to U'$ on $M$ around $p$ and $\psi : V \to V'$ on $N$ around $f(p)$ and get a ""localized derivative"" $$d_{(\phi,\psi)} f_p  = d(\psi \circ f \circ \phi^{-1})_{\phi(p)} .$$ But this depends on the choice of local Euclidean coordinates around $p$ and $f(p)$ so that $d_{(\phi,\psi)} f_p$ does not seem to be a good concept of a derivative of $f$ at $p$ . Indeed the derivative of $f$ at $p$ is usually understood as a linear map $df_p : T_pM \to T_{f(p)}N$ between tangent spaces. The tangent space $T_pM$ is defined either as the set of derivations $C^\infty(M) \to \mathbb R$ at $p$ or as the set of ""derivatives $u'(0)$ of curves $u : J \to M$ through $p$ "". Such $u$ is a smooth map defined on an open interval $J \subset \mathbb  R$ with $0 \in J$ and $u(0) = p$ and the derivative (whatever its definition may be) is taken at $t =0$ . The approach via derivations is very abstract; in my opinion the approach via curves is more intuitive and better for motivational purposes. However, it involves a ""preliminary"" concept of derivative using the following fact: Although we do not yet have an interpretation of $u'(0)$ we can at least say what it means that two curves $u,v$ have the same derivative at $0$ : This can be characterized by the requirement that $(\phi \circ u)'(0) = (\phi \circ v)'(0)$ for all charts $\phi : U \to U'$ around $p$ . Note that this equation holds for some chart, then it holds for all charts. Having the same derivative at $0$ is an equivalence relation for curves through $p$ and one defines $u'(0) = [u]$ = equivalence class of $u$ . In my eyes this is a strange interpretation of ""derivative"", but formally it does make sense. A similar phenomenon occurs in the context of the cotangent space $T^*_pM$ . One approach is to define it as the set of equivalence classes of maps $f \in C^\infty(M)$ with respect to the equivalence relation of having the same derivative at $p$ which is defined via chart $\phi : U \to U'$ around $p$ by considering $d(f \circ \psi^{-1})_{\psi^{-1}(p)}$ . Some authors write $df_p = [f]$ (e.g. Hitchin p. 17 ; see also Hitchin's definition of tangent space and tangent vectors ). My questions: The same thing could be done for smooth maps $f : M \to N$ by considering equivalence classes of smooth maps $M \to N$ with respect to the equivalence relation of having the same derivative at $p$ , the latter being defined via charts. That is, $df_p = [f]$ . Does this occur anywhere in the literature and does it have any use? Is there an alternative approach to define $T_pM$ (and also $T^*_pM$ ) not in terms of equivalence classes of curves, but in a more persuavive form? A vague idea would be that the set of derivatives of curves through a point $p$ of an open subset of $\mathbb R^m$ is nothing else than $\mathbb R^m$ itself. So why not take $T_pM = (\mathbb R^m,\phi)$ , where $\phi : U \to U'$ is a fixed chart around $p$ ? This would involve the axiom of choice to assign charts $\phi$ to points $p$ , but isn't it okay?","['tangent-spaces', 'reference-request', 'multivariable-calculus', 'derivatives', 'differential-geometry']"
4386107,What is the common $t$-test for $H_0: \mu = \mu_0 \ \text{ and } \ H_A:\mu > \mu_0$?,"The following remark came up in my lecture: Let $X_1,X_2,\ldots,X_n$ be normally distributed with known variance $\sigma^2 > 0$ . For testing $$H_0: \mu = \mu_0 \ \text{ and } \ H_A:\mu > \mu_0.$$ we use the ""common level- $\alpha$ -test"". (If I am not mistaken my teachter meant the $t$ -test with this.) In the lecture we only did tests where the alternative hypothesis was the complement of the hypothesis, so I do not understand what the ""common level- $\alpha$ -test"" is supposed to be. Could you please explain this to me? To clearify: My problem here is that I do not understand how to formulate a test where $H_A$ is not complementary to $H_0$ .","['statistics', 'hypothesis-testing']"
4386111,n-rooks n-colors problem,"The problem goes as follows: $n$ colors are used to color the squares in a $n\times m$ chess board so that each color is used exactly $m$ times. Can you always place $n$ rooks on the board, so that the rooks don't attack each other and there is exactly one rook for every color? Visual example for $8\times 8$ : What are the values of $n$ and $m$ where this is true? Particularly, what happens when $n=m$ ? Does this problem have a name? I encountered this problem here , but the comments were not helpful in my googling attempts. Obviously, if $n>m$ , it's impossible to place $n$ rooks, without them threatening each other. And also if the statement is true for some $(n,m)$ it's also true for $(n,m+1)$ . Here are some colorings that are impossible to satisfy $$n=m=2$$ \begin{matrix}
1 & 2 \\
2 & 1 \\
\end{matrix} $$n=m=3$$ \begin{matrix}
2 & 3 & 1 \\
1 & 2 & 3 \\
2 & 3 & 1 \\
\end{matrix} $$n=m=4$$ \begin{matrix}
4 & 4 & 1 & 3 \\
2 & 3 & 4 & 1 \\
2 & 4 & 1 & 2 \\
3 & 3 & 2 & 1 \\
\end{matrix} $$n=m=5$$ \begin{matrix}
3 & 4 & 5 & 3 & 4 \\
1 & 2 & 4 & 1 & 2 \\
3 & 4 & 2 & 2 & 4 \\
1 & 5 & 2 & 1 & 5 \\
5 & 3 & 1 & 5 & 3 \\
\end{matrix}","['latin-square', 'discrete-mathematics']"
4386127,How can I make this 1st order ODE separable?,"$$ y (2+3xy) \,{\rm d} x = x (2-3xy) \,{\rm d} y $$ I tried using the substitution $y=\frac{v}{x}$ , but that didn't get me far. Then I tried using the substitution $y=vx$ , but that didn't work either. Any help, please?","['substitution', 'ordinary-differential-equations']"
4386158,Does the Nash embedding theorem extend to pseudo-Riemannian manifolds?,"Does the Nash embedding theorem extend to pseudo-Riemannian manifolds? More precisely, can any ( $\mathcal{C}^k$ for some specified $k$ ) pseudo-Riemannian manifold be isometrically embedded into some pseudo-Euclidean space ? If so, can the dimension of the pseudo-Euclidean space be bounded by some absolute function of the manifold's dimension? More strongly, can the positive and negative indices in the pseudo-Euclidean space be separately bounded by some absolute function of the pseudo-Riemannian manifold's metric signature?","['semi-riemannian-geometry', 'differential-geometry']"
4386251,"Best-fitting function subspaces in $L^2[-1,1]$","I recently came cross a question related to best-fitting function subspaces as follow. Let $L^2[-1,1]$ be the Hilbert space of real valued square integrable functions on $[-1,1]$ equipped with the norm $\|f\|$ = $\sqrt{\int_{{-1}}^1 {|f(x)|}^2 dx}$ . Obviously, $1,x,x^2,\cdots,x^{n-1}$ are all in $L^2[-1,1]$ for any positive integer $n$ . My question is that for any fixed positive integer $k$ ( $k<n$ ), are there $k$ functions $h_1,h_2,\cdots,h_k$ in $L^2[-1,1]$ that minimize \begin{equation}
{\rm dist}\left({\rm span}(h_1,\cdots,h_k);1,x,\cdots,x^{n-1} \right) :=
\inf_{\alpha_{ij} \in \mathbb{R} :0\leq i \leq n-1, 1\leq j \leq k} \sum_{i=0}^{n-1} \left\| x^i - \sum_{j=1}^k \alpha_{ij} h_j \right\|^2.
\end{equation} If there are $k$ such functions, what are their expressions? Indeed, the question is equivalent to find a $k$ -dimension best-fitting function subspaces of $1,x,x^2,\cdots,x^{n-1}$ in $L^2[-1,1]$ . I feel like this question might be related to principal component analysis(PCA),
but I don't know how to generalize PCA to $L^2[-1,1]$ . Does anyone have a reference or a solution which answers this question? Thanks in advance.","['approximation-theory', 'analysis', 'real-analysis', 'hilbert-spaces', 'functional-analysis']"
4386377,"LR-test for a Pareto$(1,\beta)$-distribution","Let $X_1,\ldots,X_n$ be a sample from a Pareto $(1,\beta)$ -distribution with density $$f(x \mid \beta) = \begin{cases}
\beta x^{-(\beta+1)} & x \ge 1 \\
0 & \, \text{else}
\end{cases}$$ We test for a given $\gamma \in \mathbb{R}$ if $H_0: \beta \ge \gamma, H_A: \beta < \gamma$ . On which statitistic does the LR-test for $\beta$ depennd upon and what is its area of acceptence? So far I have computed the MLE for $\beta$ : $L_n(\beta \mid X) = \prod_{i=1}^n \beta x_i^{-(\beta+1)} = \beta^n\prod_{i=1}^n x_i^{-\beta+1}$ The log-likelihood is thus: $l(\beta \mid X) = n \log(\beta) - (\beta+1) \sum_{i=1}^n \log(x_i)$ $l^\prime (\beta \mid X) = \frac{n}{\beta} - \sum_{i=1}^n \log(x_i)$ Therefore the MLE $\hat{\beta}$ for $\beta$ is given by $$\hat{\beta} = \frac{n}{\sum_{i=1}^n \log(x_i)}$$ I know that the LR test is given by $$\frac{L(\hat{\beta_0} \mid X)}{L(\hat{\beta} \mid X)}$$ , however, I do not see what I should do now, I mean I see no way to simplify this quotient. Could you please give me a hint?","['statistics', 'hypothesis-testing']"
4386400,Intuition and Proof of Confidence Interval for Sample Variation,"Let $X_1, ..., X_n$ be a random sample from a normal distribution with an unknown $\mu$ and unknown $\sigma$ . The sample mean of this sample is $\bar{X}$ . The following graph shows the density curve of the sampling distribution of the sample mean: The yellow shaded region represents where 95% of random sample means would fall and has an interval of $(\mu - L, \mu + L)$ . Since there is a 95% chance that any random sample mean would fall in that interval, we can also say with 95% confidence that a given sample mean would contain the population mean within its confidence interval (CI). Finding the CI then for a sample mean simply requires centering the interval around the sample mean: $$(\mu - L + (\bar{X} - \mu), \mu + L + (\bar{X} - \mu)) = (\bar{X} - L, \bar{X} + L)$$ where $L = t_{\alpha/2} * \frac{s}{\sqrt(n)}$ . Unfortunately, this approach seems to fall apart when I try to apply it to finding the confidence interval for the sample variance. The sample variance has a sampling distribution of $\chi^2$ with $n - 1$ degrees of freedom. We can convert a sample variance to its corresponding $\chi^2$ value with the following formula: $$\chi^2 = \frac{(n - 1)s^2}{\sigma^2}$$ Here is a plot of a chi-square distribution with $df = 4$ . It can represent the distribution of the ""chi-transformed"" sample variances. The red line is the mean of the distribution, the blue lines represent the $\chi^2$ values below which and above which lie 5% of the data. And the dotted green line represents the ""chi-transformed"" value of our theoretical sample variance. I can no longer apply my ""shift the interval"" strategy because the distribution is skewed and also cannot go below 0. How do I intuit (mathematically and visually) the 90% confidence interval of the sample variance?","['statistics', 'confidence-interval', 'variance', 'probability-distributions', 'probability']"
4386402,Can two distinct $L^2$ functions have the same $L^2$ derivative?,"Take $f' = g' \in L^2$ for $f, g \in L^2 (\mathbb R)$ . It seems that taking the Fourier transform gives $f = g$ except on a set of measure zero (using the fact that $\hat{f'}(x) = ix \hat f(x)$ ) - does this make sense? I think it does because you can't translate a function up and maintain its $L^2$ integrability, but I was a bit confused because it seems unintuitive.","['fourier-analysis', 'functional-analysis', 'real-analysis']"
4386479,"Is the ""unit sphere"" in $\mathbb{R}^\omega$ metrizable?","Let $\mathbb{R}^\omega$ be the countably infinite product of $\mathbb{R}$ with itself in the product topology. $\mathbb{R}^\omega$ is metrizable but the metric doesn't arise from a norm, so a natural analogue to a unit sphere is the quotient of $\mathbb{R}^\omega - \{0\}$ under the action of $(0, \infty)$ (that is, we say two elements of $\mathbb{R}^\omega$ are equivalent if they are positive multiples of one another). Denote the quotient space $\mathbb{R}^\omega - \{0\} \: / \: \mathbb{R}^+$ . Is $\mathbb{R}^\omega - \{0\} \: / \: \mathbb{R}^+$ itself metrizable? So far I've shown the quotient map $q: \mathbb{R}^\omega \rightarrow \mathbb{R}^\omega - \{0\} \: / \: \mathbb{R}^+$ is open. Since $\mathbb{R}^\omega$ is second countable, the quotient space is also second countable, so I was hoping it would be easy to prove the quotient space is regular and then apply the Urysohn Metrization Theorem. Unfortunately I don't think the quotient map is closed, and showing regularity by hand seems a bit difficult, so I'm stuck. Any help or advice would be appreciated, and let me know if more details would be helpful.","['metrizability', 'general-topology', 'metric-spaces', 'quotient-spaces']"
4386507,Why is this element invertible?,"I'm reading through C* Algebras by Murphy, and the following theorem and proof is presented. Theorem: suppose that $a,b \in A^{+}$ (are positive elements) of a C* algebra A.  Then $a \leq b$ implies $a^{\frac{1}{2}} \leq b^{\frac{1}{2}}$ . Proof: we may suppose wlog that A is unital.  If we show that $a^2 \leq b^2$ implies $a \leq b$ then we're done. Let $t>0$ and Let $c,d $ be the real and imaginary hermitian parts of $(tI+b+a)(tI+b-a)$ for $I$ the unit of $A$ . It can be verified that $c \geq t^2I$ implying that $c$ is invertible and positive. Then $I+ic^{-\frac{1}{2}}dc^{-\frac{1}{2}}$ is invertible implying that $c+id$ is invertable. Hence $(tI+b-a)$ is left invertable and hence invertible since it's hermitian. Hence $t \notin \sigma(b-a)$ , implying that $a \leq b$ . My question, why is $I+ic^{-\frac{1}{2}}dc^{-\frac{1}{2}}$ invertible? I don't see how this follows from $c$ being positive and invertible whatsoever. If someone could explain that'd be great.","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
4386528,Interesting closed form for $\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta$,"Some time ago I used a formal approach to derive the following identity: $$\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta=\frac{3^{\frac{1}{12}}\pi\sqrt{2}}{AGM(1+\sqrt{3},\sqrt{8})}\tag{1}$$ where $AGM$ is the arithmetic-geometric mean . Wolfram Alpha does not tell me whether this is correct, but it does appear to be accurate to many decimal places. I have three questions: Can anyone verify whether $(1)$ is in fact correct? Is there a way of generalizing $(1)$ to integrals of the form $\int_0^{\frac{\pi}{2}}\left(a+\sin^2{\theta}\right)^{-\frac{1}{3}}\;d\theta$ or is this integral more special? My derivation (see below) appears to only work for $a=\frac{1}{3}$. There is a superficial similarity between $(1)$ and elliptic integrals (e.g. the $AGM$ evaluation ); is there a way to transform this integral into an elliptic integral that I have missed, or is it merely a coincidence that an integral of this form is the reciprocal of an $AGM$? Derivation : I have put this here in case it helps to see where I am coming from; I apologize for its length. I began by using a multiple integration trick of squaring the integral and converting to polar coordinates to evaluate $\int_0^\infty e^{-x^6}dx=\frac{1}{6}\Gamma(\frac{1}{6})$ as follows: $$\left[\int_0^\infty e^{-x^6}\;dx\right]^2=\int_0^\infty\int_0^{\frac{\pi}{2}}re^{-r^6(\cos^6\theta\;+\;\sin^6\theta)}\;d\theta\;dx={\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{3r^6\cos^2\theta\sin^2\theta}\;d\theta\;dx}$$ $$=\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{\frac{3r^6}{4}\sin^22\theta}\;d\theta\;dx={\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{\frac{3r^6}{4}\cos^2\theta}\;d\theta\;dx}$$ I then made use of the following formula (see here ): $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}e^{4x\cos^2\theta}\;d\theta\tag{2}$$ Using $(2)$ and formally interchanging integration and summation we get: $$\frac{\Gamma(\frac{1}{6})^2}{36}=\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{4\left(\frac{3r^6}{16}\right)\cos^2\theta}\;d\theta\;dx=\frac{\pi}{2}\int_0^\infty re^{-r^6}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3r^6}{16}\right)^n\;dx$$ $$=\frac{\pi}{2}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3}{16}\right)^n \int_0^\infty r^{6n+1}e^{-r^6}\;dx=\frac{\pi}{12}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3}{16}\right)^n \Gamma\left(n+\frac{1}{3}\right)$$ I then used Laplace transform identities and $(2)$, freely interchanging integrals and sums, to write: $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\frac{\Gamma\left(n+\frac{1}{3}\right)}{s^{n+\frac{1}{3}}}=L\left[\sum_{n=0}^\infty \frac{(2n)!}{(n!)^3}t^{n-\frac{2}{3}}\right](s)={\frac{2}{\pi}L\left[t^{-\frac{2}{3}}\int_0^\frac{\pi}{2}e^{4t\cos^2\theta}\;d\theta\right](s)}={\frac{2}{\pi}\int_0^\frac{\pi}{2}L\left[t^{-\frac{2}{3}}e^{4t\cos^2\theta}\right](s)\;d\theta}={\frac{2}{\pi}\int_0^\frac{\pi}{2}\frac{\Gamma(\frac{1}{3})}{(s-4\cos^2\theta)^{\frac{1}{3}}}\;d\theta}$$ Accordingly, since $\frac{4}{3}-\cos^2\theta=\frac{1}{3}+\sin^2{\theta}$ we can deduce that: $$\frac{\Gamma(\frac{1}{6})^2}{36}=\frac{\Gamma(\frac{1}{3})}{6}\left(\frac{4}{3}\right)^\frac{1}{3}\int_0^\frac{\pi}{2}\frac{1}{(\frac{1}{3}+\sin^2\theta)^{\frac{1}{3}}}\;d\theta$$ Reflection and duplication give $\Gamma(\frac{1}{6})=2^{-\frac{1}{3}}\sqrt{\frac{3}{\pi}}\Gamma(\frac{1}{3})^2$ and hence we have the following identity: $$\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta=\frac{3^\frac{1}{3}\Gamma(\frac{1}{3})^3}{2^\frac{7}{3}\pi}\tag{3}$$ while $(1)$ may be obtained by using the following identity (see here ): $$\Gamma\left(\frac{1}{6}\right)=\frac{2^\frac{14}{9}3^\frac{1}{3}\pi^\frac{5}{6}}{AGM(1+\sqrt{3},\sqrt{8})^\frac{2}{3}}$$ This completes the derivation; I cannot see how a method like this (especially with the conversion to polar coordinates) could be used to give results more general than $(1)$ and $(3)$.","['definite-integrals', 'gamma-function', 'calculus', 'closed-form', 'elliptic-integrals']"
4386624,"necessary and sufficient conditions of $\frac{X_n}{a_n}\rightarrow0, \frac{S_n}{a_n}\rightarrow0$ and $\frac{max\{X_1,...,X_n\}}{a_n}\rightarrow0$.","$X_1,...,X_n$ are independent identical random variables of standard normal distribution, $S_n=X_1+...+X_n$ , $a_n \uparrow \infty$ , try to give the necessary and sufficient conditions of (1) $\frac{X_n}{a_n}\rightarrow0$ a.e. (2) $\frac{S_n}{a_n}\rightarrow0$ a.e. (3) $\frac{\max\{X_1,...,X_n\}}{a_n}\rightarrow0$ a.e. My ideas so far:
(1) since $\frac{X_n}{a_n}\rightarrow0$ is equivalent to $|\frac{X_n}{a_n}|\rightarrow0$ , then for each $\epsilon>0$ , I tried to proove $p\left(\limsup|\frac{X_n}{a_n}| \geq \epsilon\right)=0$ . By Borel 0-1 lemma, this is equivalent to proof $\sum p(|\frac{X_n}{a_n}| \geq \epsilon) < \infty$ . Since $a_n$ is non-negative to some extent, \begin{equation}
\sum p\left(|\frac{X_n}{a_n}| \geq \epsilon\right) =\sum p(|X_n| \geq \epsilon a_n)=2\int_{\epsilon a_n}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=2\sum(1-F(\epsilon a_n))\
\end{equation} Since for normal distribution $1-F(x)=p(X>x)$ ~ $\frac{1}{2\pi x}e^{-\frac{x^2}{2}}$ as $x \rightarrow \infty$ . Therefore the summation above convergence iff \begin{equation}
\sum \frac{1}{a_n}e^{-\frac{{\epsilon} a_n^2}{2}} < \infty\
\end{equation} The proof of (2) and (3) seems more difficult, since $\{\frac{S_n}{a_n}\}$ are not independent any more, so I cannot use Borel 0-1 lemma to give a sufficient and necessary condition like problem(1). The results are weired, and I am doubt whether my ideas are right? Thanks in advance for any tips or help in general.","['probability-theory', 'probability-distributions', 'almost-everywhere', 'pointwise-convergence', 'probability']"
4386625,Dominance Poisson/Binomial in convex order,"Given two r.v. $X,Y$ (with well-defined expectations) such that $\mathbb{E}[X]=\mathbb{E}[Y]$ , we say that $X$ is dominated by $Y$ in the convex order , denoted $X \leq_{\rm cx} Y$ , if $$
\mathbb{E}[f(X)] \leq \mathbb{E}[f(Y)]
$$ for every convex function $f\colon\mathbb{R}\to\mathbb{R}$ , provided the expectations exist. Given $n\geq 1$ , $p\in[0,1]$ , let $\lambda := np$ , and $X,Y$ be distributed as $\textrm{Binomial}(n,p)$ and $\textrm{Poisson}(\lambda)$ , respectively. Is it true that $X \leq_{\rm cx} Y$ ? If so, what would be a good reference or proof for this fact? This seems to follow from Theorem 2.2 of [1], but I may be misreading -- and, in any case, this result is much more general, it would fill like overkill to use it. [1] Negative dependence and stochastic orderings, Fraser Daly (2015). https://arxiv.org/abs/1504.06493","['inequality', 'probability-distributions', 'probability-theory']"
4386736,Showing that a specific function is decreasing in one of its arguments,"Consider the function: $$\lambda(a,b,N) = \frac{N - \frac{a}{1-a^2}(3-a^N)(1-a^N)}{N - \frac{b}{1-b^2}(3-b^N)(1-b^N)},$$ where $N \in \mathbb{N}$ , and $a,b \in [0,1]$ . I want to show that, if $a<b$ , then $\lambda(a,b,N+1) - \lambda(a,b,N)<0$ for any $N$ . I'm >99.9% sure that this is true (based on evaluating this function on a grid of values for $a$ , $b$ , and $N$ ), but I'm finding very hard to prove that analytically.","['algebra-precalculus', 'functions']"
4386761,"if $f,\phi$ are continuous such that $\lim_{x\to\infty}\phi(x)-x=\infty, \ \phi(x)=x$ finitely many times, and $f\circ \phi=f$, show $f$ is constant","Question: Let $\phi:\mathbb{R}\to\mathbb{R}$ be continuous, satisfying: $\lim\limits_{x\to+\infty}(\phi(x)-x)=+\infty$ ; $\{x\in\mathbb{R}|\phi(x)=x\}$ is a non-empty,finite set. Prove that if $f:\mathbb{R}\to\mathbb{R}$ is continuous and $f\circ\phi=f$ ,then $f$ is constant. Attempt: Set $x_0=\sup\{x\in\mathbb{R}|\phi(x)=x\}$ ,then $\phi(x_0)=x_0,\forall x>x_0,\phi(x)>x$ . Let $f(x_0)=c.$ Suppose there exists $x_1>x_0$ , such that $f(x_1)=d\neq c$ . Since $f\circ\phi=f$ , we have by induction $f\circ\phi^n=f$ . Set $x_{n}=\phi^{n}(x_1)$ , then $$x_{n+1}=\phi(x_n)>x_n,$$ $$x_{n+1}-x_n=\phi(x_n)-x_n\implies x_n\to+\infty,x_{n+1}-x_n\to+\infty,$$ so $$\{f(x_n)=f(x_1)=d\}_{n=1}^{+\infty}$$ is a subsequence of $\{f(x)\}$ goes to $d$ when $x\to+\infty$ . Since $f$ is continuous,this is true for any value between $c,d$ (domain in $(x_0,x_1)$ ). I wonder if this could lead to a contradiction.","['functions', 'analysis', 'real-analysis']"
4386770,Prove that ratio is $k=2/5$,"Let $E , G , F , H$ be the midpoints of sides $AB , BC , CD , DA ,$ respectively of quadrilateral $ABCD.$ The common points of segments $AF , BH,CE,DG $ divide
each of into three parts, as shown in the figure.
Given that $$\frac{LK}{GD}=\frac{LI}{AF}=\frac{IJ}{BH}=\frac{JK}{CE}=k$$ Find $K.$ And also construct the $ABCD$ such that $ABCD$ is a parallelogram and still satisfies the condition. Additionally, prove that $LIJK$ is a trapezoid. The answer is $2/5.$ And I want an elementary proof. The question's subpart was asked here and We think a separate new question should be there to discuss about the elementary method. Note that $EFGH$ is a parallelogram. And $Ar[ABCD]=2Ar[EFGH].$ And define $M=AC\cap BD.$ Note that $$2k^2\times Ar[MIJ]=2 Ar[MHB]=2Ar[MEB]=Ar[MAB].$$ Hence, we have $2k^2\times Ar[IJKL]=Ar[ABCD]\implies 2k^2\times Ar[IJKL]=Ar[EFGH].$","['quadrilateral', 'euclidean-geometry', 'geometry', 'plane-geometry']"
4386791,Finding the radius of incircle of a triangle,"The question is - AB and AC are the circular arcs of center O' and O of radius 5 units respectively. OO' = 6 unit. In $\Delta$ ABC , AB = AC and a circle is inscribed in the triangle. The radius of the circle can be written as $\frac{a}{\sqrt b +1}$ where a, b are integers, a+b=? My try: At first, AO = 5 AO' = 5 OC = 5 O'B = 5 OO' = 6 OB = OO' - O'B = 6-5 = 1 O'C = 1 BC = OO' - (OB+O'C) = 6-2 = 4 So, BC = 4, In $\Delta$ AOO' ,(using Law of cosines) AO $^2$ = AO' $^2$ + OO' $^2$ - 2AO' $*$ OO' $*$$\cos$$\angle$ AO'O So $\angle$ AO'O = 53.13° $\Delta$ AO'B is Isosceles , i know AO' = 5, BO' = 5 and $\angle$ AO'B = 53.13° So base AB = 2×AO'× $\sin$ (0.5× $\angle$ AO'B) = 4.4721. So AC = 4.4721 Now i know the length of AB, AC and BC, and i can find the radius of incircle of $\Delta$ ABC so the radius= $\frac{\sqrt{s(s-AB)(s-AC)(s-BC)}}{s}$ $s = (AB+AC+BC)/2 = (4.4721+4.4721+4)/2 = 6.4721.$ Also ${\sqrt{s(s-AB)(s-AC)(s-BC)}} = 8$ but the value of s is 6.4721, how can I express $\frac{8}{6.4721} $ as $\frac{a}{\sqrt b +1}$ ??","['trigonometry', 'geometry']"
4386821,Functional Poincaré Lemma for second-order PDE $\Delta u(x) + F(\nabla u(x))=0$?,"Suppose I can write a second-order elliptic PDE for an unknown function $u:\mathbb R^n\to\mathbb R$ in the form $$\Delta u(x) + F(\nabla u(x))=0\qquad\forall x\in\mathbb R^n.$$ Under what conditions on the (potentially nonlinear) function $F:\mathbb R^n\to \mathbb R$ does this PDE correspond to the Euler-Lagrange equation of some functional acting on $u$ ? For example, if $F(v)\equiv0$ then the resulting PDE $\Delta u=0$ (Laplace's Equation) is the Euler-Lagrange equation corresponding to the Dirichlet energy $u\mapsto \int \|\nabla u(x)\|_2^2\,d\mathrm{Vol}(x)$ . More generically, I'm looking for a functional version of Poincaré's Lemma relevant to second-order PDE.  Posts like this one and this one are relevant, but I had trouble translating them into the case I'm interested in.","['calculus-of-variations', 'classical-mechanics', 'euler-lagrange-equation', 'functional-analysis', 'partial-differential-equations']"
4386866,Proving $48\sum\limits_{n\ge1}{e^{2n}(1+e^{4n})\over(1-e^{4n})^2}=24\pi^2\sum\limits_{n\ge1}{e^{\pi^2n}(1+e^{2\pi^2n})\over(1-e^{2\pi^2n})^2}+\pi^2-2$,"I am looking for a direct proof of the identity $$2\sum_{n\ge1}\frac{e^{2n}(1+e^{4n})}{(1-e^{4n})^2}=\pi^2\sum_{n\ge1}\frac{e^{\pi^2n}(1+e^{2\pi^2n})}{(1-e^{2\pi^2n})^2}+\frac{\pi^2-2}{24}\tag1$$ which can be shown by evaluating in two ways \begin{align}\sum_{k\ge1}\frac{(-1)^k}{\sinh^2k}&=\sum_{k\ge1}\frac2{\sinh^22k}-\sum_{k\ge1}\frac1{\sinh^2k}\tag{first method}\\&=-\pi^2\sum_{k\ge1}\frac{\cosh\pi^2k}{\sinh^2\pi^2k}-\frac{\pi^2-2}{12}\tag{second method}\end{align} The first equality follows directly from the definition, and Mathematica evaluates it to $$\frac12\psi_{e^2}^{(1)}(1)+\frac12\psi_{e^2}^{(1)}\left(1-{i\pi\over2}\right)-\psi_e^{(1)}(1)-\psi_e^{(1)}(1-i\pi)=-4\sum_{n\ge1}\frac{e^{2n}(1+e^{4n})}{(1-e^{4n})^2}$$ where $\displaystyle\psi_q^{(1)}(z)=\log q+\log^2q\sum_{n\ge0}\frac{q^{n+z}}{(1-q^{n+z})^2}$ is the first derivative of the $q$ -digamma function. The second equality follows by substituting $z\mapsto iz$ in the Mittag-Leffler expansion of $\csc^2z$ and interchanging the order of summation. Now $$\sum_{k\ge1}\frac{\cosh\pi^2k}{\sinh^2\pi^2k}=\frac1{\pi^4}\psi_{e^{\pi^2}}^{(1)}(1)-\frac1{\pi^4}\psi_{e^{\pi^2}}^{(1)}(1-i\pi)=2\sum_{n\ge1}\frac{e^{\pi^2n}(1+e^{2\pi^2n})}{(1-e^{2\pi^2n})^2}$$ so equating the two series gives us $(1)$ . But can $(1)$ be proved directly using the theory of theta functions or otherwise?","['modular-forms', 'special-functions', 'sequences-and-series']"
4386881,Connection on a principal $S^1$ bundle,"Let $\pi:M\to B$ be a principal $S^1$ -bundle over a symplectic manifold $(B,\omega)$ . Is it always possible to construct a vector field $R\in \mathfrak{X}(M)$ such that the $S^1$ action on $M$ is generated by the flow of $R$ ? If yes, how do you prove it? Given the existence of this vector field $R$ , is it true that a principal connection on this bundle is just a 1-form $\alpha\in \Omega^1M$ satisfying $\mathcal L_R\alpha=0$ and $\alpha(R)=1$ ? Is it equivalent to the usual conditions of equivariant and normalization?","['principal-bundles', 'connections', 'differential-geometry']"
4387020,existence of $99$ lines in $\mathbb{R}^2$ passing through $100^2$ boxes,"Problem 5 from ""Bernoulli Trials Problems for 2017"" link Prove or disprove the following: there exist $99$ lines in $\mathbb{R}^2$ so that for all $k,l \in \{1,2,\cdots, 100\}$ , one of the lines passes through the interior of the square with vertices at $(k,l), (k-1, l), (k-1, l-1),$ and $(k, l-1)$ . I'm not sure which lines to choose; the statement may be true. It seems that the lines given by the equations $2x+4y = 3+6k$ for $1\leq k\leq 98$ skip a bunch of squares (they have slope $-1/2$ and intercepts at $y = \frac{3+6k}4$ for each $k$ ), so I might have to choose the lines differently. There are $10^4$ squares that the $99$ lines need to pass through, so on average each line should pass through over $100$ squares. So if the statement is false, perhaps the Pigeonhole argument might be useful? Of course, one could just sketch out the lines in desmos, but that doesn't really demonstrate why the lines satisfy the given property. Note: Ideally I'd want a formal proof of why the answer is correct (if you change $n$ to $100$ and the last equation to $2(n-1) x - 2(n-2) = y$ ). I think there should be one that isn't too tedious and likely uses induction.","['contest-math', 'combinatorial-geometry', 'combinatorics', 'geometry']"
4387024,Asymptotic normality of estimator of uniform's distribution parameter,"We have $X_1, ..., X_n \sim U[0, \theta]$ and estimator $\phi^*(X_{[n]}) = X_{(n)}$ . $X_{(n)}$ here stands for $\max_iX_i$ . I need to made this estimator unbiased and check if it is asymptotic normal. Unbiasing is easy: we need to find expectation of $\phi$ . So $\mathbb{E}\phi = \frac{n}{n+1}\theta$ . Bias is $b(\phi^*, \theta) = \mathbb{E}X_{(n)} - \theta = -\frac{\theta}{n + 1}$ . So, unbiased estimator is $\tilde{\phi^*}$ = $\frac{n+1}{n}X_{(n)}$ . And now I need to check whether this unbiased estimator is asymptotic normal , i.e. $\sqrt{n}(\frac{n+1}{n}X_{(n)} - \theta) \to \mathcal{N}(0, \sigma^2(\theta) )$ . How can I do that? Do I need to use central limit theorem?","['statistics', 'probability-limit-theorems', 'central-limit-theorem', 'probability-theory', 'probability']"
4387025,nine points on a sphere,"Show with proof that there exist $9$ points on the unit sphere (centred at the origin) so that each of the $9$ points has exactly $4$ equidistinct nearest neighbours. I found a solution to this problem online, which is shown below, but I would like to know how to come up with this solution. Place $3$ of the points around the circle at positions $(1,0,0),(-1/2, \pm \sqrt{3}/2, 0)$ . Choose $c$ with $0 < c < 1$ and set $ r = \sqrt{1-c^2}$ and place three points along each of the two circles at positions $(-r, 0, \pm c)$ and $(1/2 r, \pm \sqrt{3}/2 r, \pm c)$ . Choose $c=\sqrt{5}/3$ to yield the required points. Then one can check that the distance between nearest neighbours is $2/\sqrt{3}$ (isn't there a way better than a tedious brute-force check)? How did the answerer know to choose those points and how did they know to choose $c=\sqrt{5}/3$ ?","['solid-geometry', 'algebra-precalculus', 'geometry']"
4387032,Is ths set of vector fields on a manifold a ring?,"I´m studying differential geometry and we know that if $T$ is the set of smooth vector fields defined on a manifold $M$ , then we can see that $T$ is a $R-$ module with $R$ the set of smooth real-valued functions defined on $M$ . However, Can we give to $T$ a ring structure with the ""multiplication"" defined as $VW(f)=V(W(f))$ por all $f \in R$ ? Is it true that, if $V$ and $W$ are vector fields defined on a neighborhood $U$ , then $VW$ is defined on $U$ ? I cannot see any counterexample for this. Is true in general or not? Greetings!","['vector-fields', 'riemannian-geometry', 'differential-geometry']"
4387121,Stuck on simplifying expressions involving trig and inverse trig functions,"TL; DR Using Mathcad and Wolfram I can see that $$\sqrt{7}\cos\frac{\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)}{3}=2.5$$ The decimal value seems to be exact because Mathcad displays it like that with the highest possible accuracy (17 significant digits), and so does Wolfram, as far as I can tell. How do I simplify that and arrive at the nice value of $2.5$ ? The Context I was solving this cubic equation (Cardano's formula and all) $$y^3-\frac{7}{3}y+\frac{20}{27}=0$$ Here $p=-\frac{7}{3}$ and $q=\frac{20}{27}$ . This particular equation has the roots $-\frac{5}{3}$ , $\frac{1}{3}$ and $\frac{4}{3}$ , so I know the result I'm supposed to get but I'm trying to walk through the steps and verify this result myself. I end up with two complex numbers, $$z_1=-\frac{10}{27}+\frac{\sqrt{3}}{3}i$$ $$z_2=-\frac{10}{27}-\frac{\sqrt{3}}{3}i$$ whose modulus is $r=|z_1|=|z_2|=\frac{7\sqrt{7}}{27}$ and whose arguments are $\varphi_1=\pi-\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)$ and $\varphi_2=\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)-\pi$ . These complex numbers are the result of calculating $$z_{1,2}=-\frac{q}{2}\pm\sqrt{\frac{q^2}{4}+\frac{p^3}{27}}$$ The next step is to obtain the solutions from $$y=\sqrt[3]{z_1}+\sqrt[3]{z_2}$$ knowing that we are taking the complex root of third degree here and so $\sqrt[3]{z_1}$ and $\sqrt[3]{z_2}$ will each give a set of three values (say, $\alpha_i$ and $\beta_i$ , where $i,j=\{0,1,2\}$ ). For the values for which the condition $\alpha_i\beta_j=-\frac{p}{3}$ holds, I will calculate sums $\alpha+\beta$ and those will be the solutions to the original equation. So I need to calculate the complex roots $\alpha_1,\alpha_2,\alpha_3$ and $\beta_1,\beta_2,\beta_3$ and then calculate the three sums $\alpha+\beta$ whose addends satisfy $\alpha\beta=-\frac{p}{3}$ . The Actual Problem I'm not going to list all the calculations, just the first one, since the same problem happens with all the others. Taking the (complex) cube root of $z$ : $$\sqrt[3]{z}=\sqrt[3]{r}\left(\cos\left(\frac{\varphi+2\pi k}{3}\right)+i\sin\left(\frac{\varphi+2\pi k}{3}\right)\right)$$ ( $k=0,1,2$ but I'm only showing the case $k=0$ here.) I found out with the help of Mathcad and Wolfram that one of the pairs of $\alpha$ , $\beta$ to satisfy $\alpha\beta=-\frac{p}{3}$ is actually $\alpha_0$ and $\beta_0$ , the ""first"" cube root values (with $k=0$ ) of the numbers $z_1$ and $z_2$ above. So I get $$\alpha_0=\frac{\sqrt{7}}{3}\left(\cos\frac{\varphi_1}{3}+i\sin\frac{\varphi_1}{3}\right)$$ $$\beta_0=\frac{\sqrt{7}}{3}\left(\cos\frac{\varphi_2}{3}+i\sin\frac{\varphi_2}{3}\right)$$ What do I substitute for $\varphi_1$ and $\varphi_2$ ? Yes, the values I listed above: $\varphi_1=\pi-\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)$ and $\varphi_2=\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)-\pi$ . Let me denote $\delta:=\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)$ . So I get $$\alpha_0+\beta_0=\frac{\sqrt{7}}{3}\left(\cos\left(\frac{\pi}{3}-\frac{\delta}{3}\right)+i\sin\left(\frac{\pi}{3}-\frac{\delta}{3}\right)+\cos\left(\frac{\delta}{3}-\frac{\pi}{3}\right)+i\sin\left(\frac{\delta}{3}-\frac{\pi}{3}\right)\right)$$ Since $\cos(-x)=\cos x$ and $\sin(-x)=-\sin x$ , the following are true: $$\cos(-x)+\cos x=2\cos x$$ $$\sin(-x)+\sin x=0$$ So it's possible to simplify the expression $\alpha_0+\beta_0$ : $$\alpha_0+\beta_0=\frac{\sqrt{7}}{3}\cdot2\cos\left(\frac{\pi}{3}-\frac{\delta}{3}\right)$$ And since $\cos(x\pm y)=\cos x\cos y\mp\sin x\sin y$ , $$\cos\left(\frac{\pi}{3}-\frac{\delta}{3}\right)=\cos\frac{\pi}{3}\cos\frac{\delta}{3}+\sin\frac{\pi}{3}\sin\frac{\delta}{3}=\frac{1}{2}\cos\frac{\delta}{3}+\frac{\sqrt{3}}{2}\sin\frac{\delta}{3}$$ and $$\alpha_0+\beta_0=\frac{\sqrt{7}}{3}\cos\frac{\delta}{3}+\frac{\sqrt{7}}{3}\cdot\sqrt{3}\sin\frac{\delta}{3}$$ Finally I substitute back $\delta$ : $$\alpha_0+\beta_0=\frac{\sqrt{7}}{3}\cos\frac{\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)}{3}+\frac{\sqrt{7}}{3}\cdot\sqrt{3}\sin\frac{\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)}{3}$$ The Question $\alpha_0+\beta_0$ is indeed one of the three real solutions to the original equation as both Mathcad and Wolfram agree on the value $\alpha_0+\beta_0=\frac{4}{3}$ . I can even chop up this sum into smaller pieces, and see that e. g. $$\sqrt{7}\cos\frac{\tan^{-1}\left(\frac{9\sqrt{3}}{10}\right)}{3}=2.5$$","['cubics', 'trigonometry', 'inverse-function', 'complex-numbers']"
4387128,Sample Pearson correlation coefficient,"Given paired data $\left\{(x_{1},y_{1}),\ldots ,(x_{n},y_{n})\right\}$ consisting of $n$ iid pairs ( $x_i$ and $y_i$ are indenpendent), $r_{xy}$ is defined as: $$ r_{xy}={\frac {\sum _{i=1}^{n}(x_{i}-{\bar {x}})(y_{i}-{\bar {y}})}{{\sqrt {\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}}}{\sqrt {\sum _{i=1}^{n}(y_{i}-{\bar {y}})^{2}}}}}$$ where $Ex_1=\mu_1$ , $Ey_1=\mu_2$ , $Var [x_1]=\sigma_1^2$ , $Var [y_1]=\sigma_2^2$ . What is the limiting distribution of $\frac{\sqrt{n} \, r_{xy}}{\sqrt{1-r_{xy}^2}}$ .","['statistical-inference', 'statistics', 'probability-limit-theorems', 'analysis', 'probability']"
4387130,Is there a closed set which doesn't have a minimal distance to a point?,"I am currently studying functional analysis and in the lecture we had the following theorem: Let $X$ be a reflexive normed space. Let $A \subset X$ be a convex, closed non-empty subset and let $x_0 \notin A$ be a point. Then there is an $x \in A$ such that $$||x_0 - x||_X=\text{dist}(x_0,A) = \text{inf}_{y \in A} \{||x_0 - y||\}$$ So there is actually a point in $A$ which has minimum distance to $x_0$ . Now I wonder, what would be a counterexample of this statement, if we exclude the convexity? Or the reflexivity? I already am pretty sure that there only can be such a counter example in an infinite dimensional vectorspace, since in a finite dimensional space, we can look at $\mathbb{R}^n$ (because all normes are equivalent) and can define a sequence $x_n$ in $X$ such that $||x_0 - x_n|| \rightarrow \text{dist}(x_0, A)$ for $n \rightarrow \infty$ . The alomost every element in the sequence is in a ball of radius $\text{dist}(x_0, A) +1$ around $x_0$ , so the sequence is bounded and therefore admits a convergent subsequence which converges in $A$ , since it is closed. If I didn't make a mistake, closedness should therefore be sufficiant in finite dimensions, but what about infinite? I am thinking for a while now and I just can't come up with a example.. Looking forward to your replies! Hannes",['functional-analysis']
4387158,Create Absolute Value Equation from 2 lines,"I am trying to convert these 2 lines for an absolute value equation: $$y=\frac{10}{10-a}(x-a)$$ $$y=\frac{10}{a}(x-a)$$ $a$ is a parameter that has bounds $(0, 10)$ . The absolute value equation should use the first equation for positive y values and second equation for negative y values Here's a desmos link so you can easily see it: https://www.desmos.com/calculator/itxtp5eyij The reason for this is I am trying to calculate priority for tasks. $a$ is the amount of the task already completed on a range of 0 to 10, and $x$ is the time elapsed. ( $x=10$ means all the time has elapsed) The $y$ value would be the priority of the task. For example, if $a=10$ and $x=0$ it means that the task has been completed as soon as it started. Priority would start at -10 and increase over time until it hits zero. Similarly, if $a$ remains 0, as $x$ approaches 10, the priority would also increase to 10 at the same rate Any help converting this to an absolute value would be appreciated, although the main goal is to just have a concise code implementation for this (it's for an app)","['limits', 'trigonometry', 'absolute-value', 'rotations']"
4387159,"Is there a simple characterization of compact subspaces of the ""unit sphere"" in $\mathbb{R}^\omega$?","Let $\mathbb{R}^\omega$ be the countably infinite product of $\mathbb{R}$ with itself in the product topology. $\mathbb{R}^\omega$ is metrizable but the metric doesn't arise from a norm. A natural analogue to a unit sphere is the quotient of $\mathbb{R}^\omega - \{0\}$ under the action of $(0, \infty)$ (that is, we say two elements of $\mathbb{R}^\omega$ are equivalent if and only if they are positive multiples of one another). Denote the quotient space by $S$ . In this answer we see that $S$ is not metrizable, but I'm trying to figure out if compactness can still be characterized nicely in $S$ . I've started by characterizing compactness in $\mathbb{R}^\omega$ : a subspace $K$ is compact if and only if it is closed and contained in some set of form $\Pi_{i=1}^\infty [a_i, b_i]$ where for all $i$ $a_i$ and $b_i$ are real numbers such that $a_i \leq b_i$ . This in turn gives us a lot of sets in $S$ that are compact, but I'm looking for necessary and sufficient conditions about as simple as those found for $\mathbb{R}^\omega$ . For example, "" $K \subseteq S$ is compact if and only if it is the image under the quotient map of a compact set in $\mathbb{R}^\omega$ "" would be very nice. Any answers, advice, or comments are appreciated!","['quotient-spaces', 'general-topology', 'compactness']"
4387200,"Is there a relationship between $\sum _{n=1}^{\infty }\left({\frac {1}{2}}\right)^{n} = 1$ and $\int_{1}^{\infty} \frac{1}{x^2} \,dx = 1$?","A classic example of an infinite series that converges is: ${\displaystyle {\frac {1}{2}}+{\frac {1}{4}}+{\frac {1}{8}}+{\frac {1}{16}}+\cdots =\sum _{n=1}^{\infty }\left({\frac {1}{2}}\right)^{n}=1.}$ A classic example of an infinite integral that converges is: $\displaystyle\int_{1}^{\infty} \frac{1}{x^2} \,dx = 1.$ They feel very similar! But not quite the same. Is there a way to think about one in terms of the other? I ask partly because I want to borrow the nice geometric illustrations that the former converges (like this , or similarly for other geometric series ) to show the latter converging. ( Related question about illustrating the geometry of $\frac{1}{x}$ vs. $\frac{1}{x^2}$ .)","['integration', 'visualization', 'calculus', 'convergence-divergence', 'geometric-series']"
4387216,"$f\in C^2((0,1)),\lim_{x\to 1^{-}}f(x)=0,\exists C\forall x\in (0,1)$,$(1-x)^2|f''(x)|\leqslant C$,then $\lim_{x\to 1^{-}} (1-x)f'(x)=0$ [duplicate]","This question already has answers here : Prove or disprove that if $\lim\limits_{x\to0^+}f(x)=0$ and $|x^2f''(x)|\leq c$ then $\lim\limits_{x\to0^+}xf'(x)=0$ (2 answers) Closed 2 years ago . Question : Suppose that $f\in C^2((0,1))$ , $\lim\limits_{x\to 1^{-}}f(x)=0$ . Assume that there exists a constant $C>0$ such that $\forall x\in (0,1)$ , $(1-x)^2|f''(x)|\leqslant C$ . Prove that $\lim_\limits{x\to 1^{-}} (1-x)f'(x)=0$ . Attempt : I've tried to use Taylor expansion and got $0=f(x)+f'(x)(1-x)+\frac{f''(c)}{2}(1-x)^2,c\in (x,1).$ But that's not enough.","['functions', 'analysis', 'real-analysis']"
4387258,Can you solve any mathematical function?,"For any finite mathemathical function (consisting of addition, subtraction, division, multiplication, exponentiation, trigonometry) can you find $x$ in $f(x) = y$ where $y$ is a number you want? Is it proven that any function is solvable or not solvable? Can a computer solve a function that's millions of characters long? I tried to solve a large function in maxima but it got in an infinite loop.","['elementary-functions', 'functions', 'closed-form', 'inverse-function']"
4387262,What is the kernel of this map $\Phi: F_2 \to \mathbb{Z}_2 \oplus \mathbb{Z}_3$?,"What is the kernel $K\leq F_2 = \langle a,b \rangle$ of this map $\Phi: F_2 \to \mathbb{Z}_2 \oplus \mathbb{Z}_3$ given by $a \mapsto (1+2\mathbb{Z},0+3\mathbb{Z})$ and $b\mapsto (0+2\mathbb{Z}, 1+3\mathbb{Z})$ ? Here is my thought process so far:
Since $K$ is the kernel, it will be all words on $F_2$ such that their image under $\Phi$ is the identity $e$ , which in this case would be $(0+2\mathbb{Z}, 0+3\mathbb{Z})$ . I see that clearly $a+a =e$ , as well as $4a$ or $n\cdot a$ for $n$ a multiple of 2. Similarly, $b+b+b=e$ , and so on for multiples of 3. Any combination of these would also obtain $e$ , so my initial thought is that $K$ is generated by $a^2$ and $b^3$ . For instance, $\Phi(a^4b^9)=e+e=e$ . Would this be the correct way of stating it?","['direct-sum', 'finitely-generated', 'abstract-algebra', 'free-groups', 'group-theory']"
4387342,What is the area of the triangle here?,"We are given that the angle of $BAD$ is $2\alpha$ and the angle of $DAC$ is $\alpha$ . $|AC| = 10$ , $|BD| = 6$ , $|DC| = 5$ units. Find the area of the triangle $ABC$ . The answer would be $33$ . We need to show that if we drop an altitude from $A$ to $BC$ at point $E$ , $|AE| = 6, |EC| = 8$ . Somehow $\triangle AEC$ becomes $6$ - $8$ - $10$ triangle. Since $|AE| = 6$ and $|BC|=11$ , the area becomes $33$ . But how can we prove that?","['euclidean-geometry', 'area', 'geometry', 'triangles', 'plane-geometry']"
4387346,Finding isomorphism between galois field $ {\rm GF}(2^8)$ and polynomial ring of galois field ${\rm GF}(2^4)[X] / F[X]$,"How to find an isomorphism between finite Galois field and polynomial ring of Galois field? For example, Let $F_1 = GF(2^8)$ where polynomial of $GF(2^8)$ is $a^8 + a^4 + a^3 + a + 1$ , $F_2 = GF(2^4)[X] / (X^2 + b^2X + b^2)$ with polynomial of $GF(2^4)$ is $b^4 + b + 1$ . $(a^8 + a^4 + a^3 + a + 1)$ , $(b^4 + b + 1)$ , $(X^2 + b^2X + b^2)$ are all irreducible polynomial. So cardinality of $F_1, F_2$ are the same which implies the existance of an isomorphism $\phi : F_1 \rightarrow F_2$ . But I don't know how to find a specific isomorphism. I even don't know what keywords to search, so I would appreciate your help. (Moreover, above example is for the simplicity. The problem I'm currently facing is larger field such as an isomophism between $GF(2^{512}), GF(2^{128})/ f(X)$ where $f(X)$ is irreducible 4th degree polynomial. So I need a generic solution, not only for the above example.)","['field-theory', 'group-theory', 'finite-fields', 'polynomial-rings']"
4387398,Proportion of 2-generated $p$-groups of a given order,"For a prime $p$ and positive integer $n$ , let $N(p^n)$ be the number of isomorphism classes of groups of order $p^n$ , and let $T(p^n)$ be the number of isomorphism classes of groups of order $p^n$ which are 2-generated. What can we say about the ratio $T(p^n)/N(p^n)$ ?  Does it always have a limiting value of 1 as $n\rightarrow\infty$ , in particular?","['group-theory', 'finite-groups', 'p-groups']"
4387408,Lie Derivative on Vector Bundles,"I (a physicist) am trying to understand more about the foundations of differential geometry. I am having some trouble disentangling the difference between the Lie derivative and the covaraint derivative. Let $E$ be a bundle over $M$ . If we wish to compare an object $W_q$ in the fiber $F_q$ over $q$ to an object $W_p$ in the fiber $F_p$ over $p$ , we require an isomorphism from $E_q$ to $E_p$ . This could be provided by the flow generated by a vector field $X$ , such that $q = \phi_t(p)$ and $p = \phi_t^{-1}(q) = \phi_{-t}(q)$ . Provided we can find $$
(\phi_{-t})_*: \pi^{-1}[q] \to \pi^{-1}[p],
$$ then $$
(\phi_{-t})_* W_{\phi_t(p)} \in \pi^{-1}[p]
$$ is, for small $t$ , a curve in $\pi^{-1}[p]$ . We can differentiate this curve at $t=0$ , giving us an object in $T_p(\pi^{-1}[p]) = T_p W_p$ . This is intuitvely the Lie derivative for the bundle $E$ . It sounds to me like the Lie derivative then lives in the vertical subspace tangent to a given fiber. However, when considering the connection on a vector bundle, we also use such constructions when dealing with the lifts of curves and parallel transport. Actually, the vertical subspace in this regard has the property that $$
\pi_* V_p = 0.
$$ But since the Lie derivative of two vector fields is again another vector field, I don't see how this property can be reconciled with the Lie derivative living in the vertical subspace over a fiber. I am wondering whether the condition applies only after we define a connection and parallel transport, and isn't generally true, or whether I am mistaken about where the Lie derivative ""lives"". However, if we do have a connection, can the Lie derivative still be defined? Reference used: https://cefns.nau.edu/~schulz/lieder.pdf","['vector-bundles', 'lie-derivative', 'differential-geometry']"
