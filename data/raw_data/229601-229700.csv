question_id,title,body,tags
4757453,Terminology for Combining (Intersecting?) Partitions,"What's the term for making a new partition from two existing partitions of a space? For example, if you have a partition $X = \{A, B\}$ and $Y = \{C, D\}$ , and you form the partition $\{AC, AD, BC, BD\}$ , what do you call it? (Example: on the interval $[0, 4]$ , $X = \{[0, 2), [2, 3]\}$ and $Y = \{[0, 1), [1, 3]\}$ , so the new partition is $\{[0, 1), [1, 2), [2, 3]\}$ .) For example, do you call it ""the partition formed through intersecting $X$ and $Y$ ""? Or ""the partition formed through crossing $X$ and $Y$ ""? Thank you!","['elementary-set-theory', 'set-partition', 'terminology']"
4757478,Prove an entire function is constant on complex plane,"Let $f(z)$ , $F(z)$ be two analytic functions on $\Bbb C$ satisfies $f(z)=F(\overline{f(z)})$ . Here $\overline{f(z)}$ is the complex conjugate of $f(z)$ . Prove that $f(z)$ is constant on $\Bbb C$ . I tried to use Cauchy-Riemann equations but I didn't work, and I don't know what else I can do. Any help would be appreciated.","['complex-analysis', 'entire-functions', 'cauchy-riemann-equations', 'analytic-functions']"
4757521,An urn with r red balls and b blue balls and a special way of taking the balls out until a single color is left,"This is problem 2.13 b) in David Stirzaker's book: ""Elementary probability"". The urn contains r red balls and b blue balls and we are to take the balls out until a single color is left in the urn. The special way of taking the balls is as follows: We are to follow a process where we first grab a ball and take it out of the urn, register its color and then we randomly take balls from the urn until we take out one that is of different color. That ball of different color is RETURNED to the urn and we are to repeat the process until a color is left. Do note that when we start a new process, the first ball that we take will always be pulled out of the urn, so it is well defined to talk about the probability that a color is left (after a finite amount of steps, a color will be singled out in the urn). Now we are asked to find the probability that the color left in the urn is red. My main approach was to use double induction on the number of red and number of blue balls. One gets that when fixing the number of red balls to 1 and using induction to generalize the number of blue balls, the answer is 1/2. This is also true for the case of r=2: by induction on b we get 1/2. I got stuck on the inductive step when we generalize the number of red balls: using conditional probability a complicated sum is formed and I am not sure how to get 1/2 as the answer. Perhaps there is an easier way so I hope someone can share some ideas or even give the whole solution.","['induction', 'probability']"
4757532,$\angle AFI =90$,"Let triangle $ABC$ inscribed $(O)$ and $(I)$ inscribed triangle $ABC$ . $AH,ID$ are perpendicular to $BC$ . $AI$ cuts $(O)$ at $E$ . $DE$ cuts $(O)$ at $F$ . $BC$ cuts $AF$ at $K$ . a) Prove that $A,I,K,H$ lie on one circle. b) $EH$ cuts $(O)$ at $L$ . $FL$ cuts $BC$ at $J$ . Prove that tangent of $(O)$ from $F$ passes mid-point of $JK$ Here is what I try: Because $A,I,K,H$ lie on one circle so I want to prove that $FIDK$ is cyclic. So it lead to prove $\angle AFI =90$ which is my problem. I need this a lots for my next day, thanks you for all your helps.","['triangles', 'circles', 'geometry']"
4757557,"$\frac{dy}{dx}=\frac{7}{3}y^{\frac{4}{7}},y(x_0\neq0)=0$ has a unique solution?","$$\frac{dy}{dx}=\frac{7}{3}y^{\frac{4}{7}},\quad y(x_0\neq0)=0$$ is simply solved with $$\int y^{-\frac{4}{7}}dy=\frac{7}{3}\int dx\implies y(x)=(x-x_0)^{\frac{7}{3}}.$$ My introductory textbook says this ODE has a unique solution. But as far as I can see, in addition to the one displayed above, also $y(x)=0$ solves the same ODE. What do I not understand about ""unique solution"" here, or how am I mistaken?",['ordinary-differential-equations']
4757580,Reordering proof of $p$-adic logarithm fundamental property,"I am reading a book on $p$ -adic analysis and am currently stuck at the proof of the following theorem: Theorem The $p$ -adic logarithm satisfies the fundamental property $$\ln_p(xy) = \ln_p(x) + \ln_p(y)$$ As a reminder: the $p$ -adic logarithm is defined as $$\ln_p(x) = \sum_{n = 1}^\infty (-1)^{n + 1} \frac{(x-1)^n}n$$ The proof they give is the following: The following identity $$\log(1 + X) + \log(1 + Y) - \log(1 + X + Y + XY) = 0$$ holds for formal power series. One can check directly (by expanding and reordering terms) that all coefficients of the resulting series reduce to zero. Where $\log(1 + X)$ is defined as $$\log(1 + X) = \sum_{n = 1}^\infty (-1)^{n + 1} \frac{X^n}n$$ I understand that you must be able to rearrange the above equation/series to get zero, as the identity can be proven through other ways. However, I'm a bit confused as to what specific reordering they are referring to here. I've tried developing the partial sums till $n = 4$ but wasn't able to find anything interesting.","['p-adic-number-theory', 'analysis', 'sequences-and-series']"
4757606,Why does deg(arcsin(sin(rad(degree)))) not produce degree?,"I have an angle in degrees and I want it to be encoded between $-1, ..., 1$ to make it easier for using in a neural network. I thought it might be a good idea to first convert the angle into radians and then take the sin of it. To get the angle back in degrees, first use arcsin and then convert to degrees. In python (using NumPy math functions ) this would look something like this: np.rad2deg(np.arcsin(np.sin(np.deg2rad(264.0)))) but instead of 264 this produces -84 : why is that?","['python', 'rounding-error', 'trigonometry', 'inverse-function']"
4757663,Does Amann's Theorem 1.4 about $\mu$-measurability extend to metrizable topological groups?,"Let $(X, \mathcal A, \mu)$ be a complete $\sigma$ -finite measure space and $(E, | \cdot |)$ a Banach space. Let $f:X \to E$ . We recall some definitions at page 62 of Amann's Analysis III . $f$ is called $\mu$ -simple if $f = \sum_{k=1}^n e_k 1_{A_k}$ where $e_k \in E \setminus \{0_E\}$ and $(A_k)_{k=1}^n$ is a finite sequence of pairwise disjoint sets with finite measures in $\mathcal A$ . Let $\mathcal S (X, \mu, E)$ be the space of such $\mu$ -simple functions. $f$ is called $\mu$ -measurable if $f$ is a $\mu$ -a.e. limit of a sequence $(f_n)$ in $\mathcal S (X, \mu, E)$ . $f$ is called $\mathcal A$ -measurable if $f^{-1}(O) \in \mathcal A$ for all open sets $O \subseteq E$ . $f$ is called $\mu$ -almost separable valued if there is a null set $A$ such that $f(A^c)$ is separable. I'm reading the proof of Theorem 1.4 at page 65 in the same book. Theorem 1.4 A function $f:X \to E$ is $\mu$ -measurable if and only if $f$ is $\mathcal A$ -measurable and $\mu$ -almost separable valued. "" $\Longrightarrow$ "" Let $O \subseteq E$ be an open set, $A \in \mathcal A$ a null set, and $(f_n)$ a sequence of $\mu$ -simple functions that converges to $f$ on $A^c$ . We want to prove $f^{-1} (O) \in \mathcal A$ . Let $A_1 :=  A \bigcap f^{-1} (O)$ and $A_2 := A^c \bigcap f^{-1} (O)$ . Because $\mu$ is complete and $A_1 \subseteq A$ , we get $A_1 \in \mathcal A$ . Notice that $x \in A_2$ IFF almost all terms $f_n(x)$ belong to $O$ and $(f_n(x))_n$ converges to some point not on the boundary $\partial O$ . Let $O_m := \{x \in X \mid d(x, \partial O)> 1/m\}$ . Because $d$ is continuous, $O_n$ is open. Also, $$A_2 = A^c \bigcap \left [ \bigcup_{N \in \mathbb N} \bigcap_{n \ge N} f^{-1}_n (O) \right ] \bigcap \left [ \bigcup_{m \in \mathbb N^*} \bigcap_{N \in \mathbb N} \bigcup_{n \ge N} f_n^{-1} (O_m) \right ].$$ Clearly, $A_2 \in \mathcal A$ and thus $f^{-1}(O) = A_1 \cup A_2 \in \mathcal A$ . This means $f$ is $\mathcal A$ -measurable. Let $F := \bigcup_n f_n (A^c)$ . Then $F$ is countable and thus $\overline F$ is separable. Hence $f(A^c) \subseteq \overline F$ is separable and thus $f$ is $\mu$ -almost separable valued. "" $\Longleftarrow$ "" Let's first assume that $\mu$ is finite . It follows from $f$ is $\mu$ -almost separable valued that there is a null set $A \in \mathcal A$ such that $f(A^c)$ is separable. Let $E = \{ e_m \mid m \in \mathbb N\}$ be a countable dense subset of $f(A^c)$ . For $n \in \mathbb N^*$ , let $\mathcal U_n := \{\mathbb B(e_m, 1/n) \mid m \in \mathbb N\}$ be the collection of open balls with center in $E$ and radii $1/n$ . Clearly, $\mathcal U_n$ is an open cover of $f(A^c)$ , i.e., each $x \in A^c$ belongs to at least one ball in $\mathcal U_n$ . Let $A_{m, n} := f^{-1} (\mathbb B(e_m, 1/n))$ . It follows from $f$ is $\mathcal A$ -measurable that $A_{m, n} \in \mathcal A$ . Also, $\mu (A_{m, n}) \le \mu(X) < \infty$ . For each $n \in \mathbb N^*$ , let $\varphi(n)$ be the least $k$ such that $$\mu \left [ \bigg ( \bigcup_{m = 0}^k A_{m, n} \bigg )^c \right ] \le \frac{1}{2^n}.$$ The existence of such $\varphi(n)$ is guaranteed by the continuity from below and the finiteness of $\mu$ . Let $$B_n := \bigg ( \bigcup_{m = 0}^{\varphi(n)} A_{m, n} \bigg )^c.$$ Then $\mu(B_n) \le 1/2^n$ . For each $n \in \mathbb N^*$ , we define $f_n \in E^X$ by $$f_n (x) := \begin{cases}
e_k &\text{if}  \quad x \in A_{k, n} \setminus \bigcup_{m = 0}^{k-1} A_{m, n}, \quad k=0, \ldots,\varphi(n) \\
0   &\text{otherwise}.
\end{cases}$$ In this way, we assign each $x \in B_n^c$ to a center of an open ball for which $x$ belongs to its pre-image. Then $f_n \in \mathcal S(X, \mu, E)$ and $|f_n(x) - f(x)| <1/n$ for all $x \in B_n^c$ and $n \in \mathbb N^*$ . Notice that $\mu(B_n) \searrow 0$ , but not $B_n \searrow \bigcap_n B_n$ . We make a tweak by defining $C_n := \bigcup_{k=n}^\infty B_k$ for $n \in \mathbb N^*$ . Then $$\mu(C_n) \le \sum_{k=n}^\infty \mu(B_k) = \sum_{k=n}^\infty \frac{1}{2^k} = \frac{1}{2^{n-1}}, \quad n \in \mathbb N^*.$$ Let $C := \bigcap_n C_n$ . Then $C_n \searrow C$ and $\mu(C_n) \searrow 0$ . We define $g_n \in E^X$ by $$g_n := f_n 1_{C_n^c}, \quad n \in \mathbb N^*.$$ It's straightforward that $g_n \in \mathcal S(X, \mu, E)$ . For each $x \in C^c$ , there is $n_x \in \mathbb N^*$ such that $x \in C_n^c$ for all $n \ge n_x$ . This means $|g_n(x) - f(x)| <1/n$ for all $n \ge n_x$ . Hence $g_n \to f$ on $C^c$ . In case $\mu$ is $\sigma$ -finite . Then there is a sequence of pairwise disjoint sets with finite measure $(X_m)$ such that $\bigcup X_m = X$ . Let $\mathcal A_m := \{A \cap X_m \mid A \in \mathcal A\}$ and $\mu_m := \mu_{\restriction \mathcal A_m}$ the induced $\sigma$ -algebra and induced measure on $X_m$ respectively. By above result, for each $m$ , there is a null set $C_m \in \mathcal A_m$ and a sequence $(g_{m,n})_n$ in $\mathcal S(X_m, \mu_m, E)$ such that $g_{m,n} \xrightarrow{n \to \infty} f 1_{X_m}$ on $C_m^c$ . Let $C := \bigcup_m C_m$ . Then $C \in \mathcal A$ is a null set. We define $g_n \in E^X$ by $$g_n (x) := \begin{cases}
g_{m,n}(x) &\text{if}  \quad x \in X_m \\
0   &\text{otherwise}
\end{cases}, \quad n \in \mathbb N^*.$$ Then $g_n \in \mathcal S(X, \mu, E)$ and $g_n \xrightarrow{n \to \infty} f$ on $C^c$ . This completes the proof. My understanding The proof and all related definitions only use the metric structure of $E$ and the fact that there is an identity element $0_E$ , not the vector space structure nor the completeness of $E$ . It seems to me above theorem and all definitions extend naturally to the case $E$ is a metrizable topological group. Could you confirm if my understanding is fine?","['banach-spaces', 'measure-theory', 'topological-groups', 'measurable-functions', 'functional-analysis']"
4757665,Does $\hat{\theta} - \theta \overset{p}{\rightarrow} 0$ imply $\sqrt{n}(\hat{\theta} -\theta) \overset{p}{\rightarrow} 0$? Or vice versa?,"Suppose I have a sample of $n$ observations, and $\hat{\theta}$ is an estimator for the parameter $\theta$ (which is a constant). From the following two convergence in probability statements: $\hat{\theta} - \theta \overset{p}{\rightarrow} 0$ $\sqrt{n}(\hat{\theta} -\theta) \overset{p}{\rightarrow} 0$ Does (1) imply (2)? The other way around? Or (1) $\Leftrightarrow$ (2)?","['statistics', 'probability']"
4757671,Calculator doesn't show the steps to finding the inverse of this function.,"I was recently given the function $\sqrt{x}/(x-1)$ and was asked to evaluate $f^{-1}(1)$ find the inverse and draw the graph. Firstly, in order for a function to be invertible, it must be bijective, meaning it has a one-to-one mapping and its range is the same as its co-domain. So based on our definition, Should the inverse not be undefined as well at that point. When searching on a calculator, I either get the result $y=(1+2x^2\pm\sqrt{4x^2+1})/(2x^2)$ or an error telling me it is not possible Can someone please run me through on how one can achieve this inverse? My guess is that we must use the quadratic formula however I have already tried. The furthest I have gotten is $\sqrt{y}/(y-1) = x$ $y/(y-1)^2 = x^2$ $(y-1)^2/y = 1/x^2$ $(y^2-2y+1)/y = 1/x^2$ $y-2+1/y = 1/x^2$ $y+1/y = (1+2x^2)/x^2$ I can't seem to get any further than this for some reason. Also if someone could explain how our definition of an invertible function still holds this would be a literal contradiction of the properties of an invertible function. I can't seem to grasp why this is possible, unless my definition for an invertible function is wrong. Lastly I don't understand how $\pm$ is in the function, does this imply there is more than one output for each specific input, and if so I thought this was also contradictory to the definition of a function.","['functions', 'inverse', 'inverse-function']"
4757677,Topology of convergence in measure is not compatible with the vector space structure of measurable functions,"Below we use Bochner measurability and Bochner integral . Let $(X, \mathcal A, \mu)$ and $(Y, \mathcal B, \nu)$ be complete $\sigma$ -finite measure spaces, $(E, | \cdot |)$ a Banach space, $S (X)$ the space of $\mu$ -simple functions from $X$ to $E$ , $L^0 (X)$ the space of $\mu$ -measurable functions from $X$ to $E$ , $L^1 (X)$ the space of $\mu$ -integrable functions from $X$ to $E$ , $Z := X \times Y$ , $\mathcal C$ the product $\sigma$ -algebra of $\mathcal A$ and $\mathcal B$ , $\lambda$ the product measure of $\mu$ and $\nu$ , $(Z, \overline{\mathcal C}, \overline{\lambda})$ the completion of $(Z, \mathcal C, \lambda)$ . For $\delta >0$ and $f,g \in L^0 (Y)$ , we write $$
\begin{align*}
\{|f - g| > \delta\} &:= \{y \in Y : |f (y) - g(y)| > \delta\}, \\
\nu (|f - g| > \delta) &:= \nu (\{|f - g| > \delta\}).
\end{align*}
$$ For $f, g \in L^0 (Y)$ , we define $$
\hat \rho_Y (f, g) := \inf_{\delta >0} \{ \nu (|f - g| > \delta)  +\delta \}.
$$ Then $\hat \rho_Y$ is an extended pseudometric on $L^0 (Y)$ . For $f_n, f \in L^0(Y)$ , we have $\hat \rho_Y (f_n, f) \to 0$ IFF $f_n \to f$ in measure . My understanding It seems to me $(L^0 (Y), \hat \rho_Y)$ is not necessarily a metric linear space because scalar multiplication is not necessarily continuous in the topology induced by $\hat \rho_Y$ . Take $E=Y = \mathbb R$ and $\nu$ its Lebesgue measure. Let $f_n := 1$ and $\lambda_n = 1-\frac{1}{n}$ . Then $|\lambda_n -1| \to 0$ and $\hat \rho_Y(f_n, 1) =0$ . However, $$
\hat \rho_Y (\lambda_n f_n, 1) = \hat \rho_Y \bigg (\frac{1}{n}, 0 \bigg) = +\infty.
$$ Could you confirm if my above understanding is fine?","['measurable-functions', 'measure-theory', 'topological-vector-spaces', 'functional-analysis']"
4757692,Special way to solve systems of ODEs,"If we have a system of two  differential equations and it can be written in the form: $$\frac{dx}{f_1(x,y,t)}=\frac{dy}{f_2(x,y,t)}=\frac{dt}{f_3(x,y,t)}=\gamma$$ Then we can conclude that: $$\frac{\alpha_1dx+\alpha_2dy+\alpha_3dt}{\alpha_1f_1(x,y,t)+\alpha_2f_2(x,y,t)+\alpha_3f_3(x,y,t)}=\gamma$$ Now if we choose the correct $\alpha_1,\alpha_2,\alpha_3$ , Then we can get a  solution. I don't understand if there is an easy way to get the correct coefficients of it is purely a guesswork. Here's an example: $$\frac{dx}{1+\sqrt{z-x-y}}=\frac{dy}{1}=\frac{dz}{2}=\gamma \Rightarrow \frac{\alpha_1dx+\alpha_2dy+\alpha_3dz}{\alpha_1(1+\sqrt{z-x-y})+\alpha_2+2\alpha_3}=\gamma$$ Then one easy tripple of coefficients is: $$\alpha_1=0 ;\\ \alpha_2=2 ;\\ \alpha_3=-1$$ Then after substitution we get: $$2dy-dz=d(2y-z)=0 \Rightarrow 2y-z=C_1$$ To get the full solution of the system, we need one more tripple of coefficients but I don't know what logic to use to find the most appropriate one. Can you help not only with this particular example but with this method as a whole? Also if you know how is it called because I can't even find it online to educate myself.","['systems-of-equations', 'ordinary-differential-equations']"
4757696,Is the tangent space of a Riemannian manifold a local approximation of the manifold?,"Let $(M,g)$ be a Riemannian manifold, $p \in M$ . You will often hear people say "" $(T_pM, g(p))$ is the infinitesimal geometry of $M$ at $p$ "". I would like to upgrade `infinitesimal' to 'local' in some suitably weak sense (if possible). Of course, $M$ will not be flat in general so we can't hope for an open subset $U \subset M$ to be isometric to a subset of $T_pM$ . I would like the following weaker statement to be true: For $K$ arbitrarily close to 1, there are open subsets $p \in U_K \subset M$ and $V_K \subset T_pM$ and a $K$ -bilipschitz homeomorphism $f:U_K \to V_K$ . I have a vague sense that this should follow from the continuity of the metric. I've tried to choose a coordinate chart $U \subset \mathbb{R}^n$ small enough so that the two metrics (ie $g$ and $g(p)$ ) are nearly identical, but I don't see how to use this information to bound $\frac{d_{g}(x,y)}{d_{g(p)}(x,y)}$ . At this stage I am not even sure if the statement is true.","['lipschitz-functions', 'geometry', 'riemannian-geometry', 'metric-spaces']"
4757741,How can you prove or disprove that half of all natural numbers can be expressed as the sum of three squares minus the product of their roots?,"I do not have much formal mathematical education beyond high school, but I do like watching math YouTube videos. I recently read a comment on a video asking if anyone could prove that there are no positive integer solutions to the equation a 2 + b 2 + c 2 - abc = 1. It took me a while, but I found a proof, and I then used a similar method to prove a more general claim. For any given positive integer n , there exists a set of three positive integers a , b , and c satisfying the equation a 2 + b 2 + c 2 - abc = n if and only if there exists such a set where a 2 + b 2 ≤ n. For example, if there are no positive integer solutions to the equation a 2 + b 2 + c 2 - abc = 6 where a 2 + b 2 ≤ 6, then there are no positive integer solutions to that equation at all. My proof is too long to paste into this post, so here is a link to a PDF of it: https://www.dropbox.com/scl/fi/9wsm5b4zsz9xpc0r7p0rk/naturalnumberproblem.pdf?rlkey=0gwqktvfu9215fskihlt2fvu3&dl=0 Since there are only finitely many pairs of positive integers who squares sum to a value less than or equal to a given number, you only need to check finitely many cases to determine whether there is a solution for a given n . For example, for n = 6, you only need to check {a = 1, b = 1} and {a = 1, b = 2}. Solving for c shows there are no positive integer solutions. After discovering this, I wrote a program to that checked whether there were solutions for each n from 1 to 100. I was hoping to find a pattern in the numbers that have or don't have solutions, but I didn't find much. However, I did notice that about half of the numbers had solutions. That intrigued me, so I increased the range of my search to see if this percentage held. Here are the results: From n = 100 to 999, the percentage of numbers with solutions stays between 49.3% and 52.5%. From n = 100 to 9999, the percentage of numbers with solutions stays between 49.8% and 50.4%. From n = 10000 to 99999, the percentage of numbers with solution stays between 50.1% and 50.7% My conjecture is that this percentage stays around 50% and that the deviations get smaller and smaller over time. I assume there is some way to state this more formally using limits. Does anyone know if what I found has already been discovered and if my conjecture has already proven or disproven? What methods would you even use to do this? As I said, I don't have much formal mathematical education. The math used in the proof linked above is fairly simple. Nothing more 'advanced' than modular arithmetic and proofs by contradiction. My intuition is that proving my conjecture would rely on math I've never even heard of, but maybe there is some (relatively) simple proof out there. I can prove that ~41.6% of all numbers will not have solutions. (I am speaking loosely. I know this needs to be stated more formally with limits.) This is because there are no solutions to the equation when n is congruent to 3 mod 4, 3 mod 9, or 6 mod 9. This was verified by a computer trying every possible solution working from mod 2 to mod 800. Let f(n) count the numbers less than or equal to n that are congruent to 3 mod 4, 3 mod 6, or 3 mod 9. There are no numbers that are congruent to both 3 mod 9 and 6 mod 9. The only numbers that are congruent to both 3 mod 4 and 3 mod 9 are those that are congruent to 3 mod 36, and the only one congruent to both 3 mod 4 and 6 mod 9 are those that congruent to 15 mod 36. Therefore, the limit of f(n)/n as n approaches infinity is equal to 1/4 + 1/9 + 1/9 - 1/36 - 1/36 = 5/12 = ~41.6% One more thing I know (proof in that PDF) is that for all n except n = 2 , there are either no positive integer solutions to the equation a 2 + b 2 + c 2 - abc = n or there are infinitely many such solutions. For n = 2, there is only one solution {a = b = c = 1}. I doubt that is relevant to the conjecture, but it does not hurt to mention it. One last thing: I'm tagging this as a number theory question and as a question about Diophantine equations. Let me know if that is incorrect. EDIT #1:
I want to add that I looked up the first dozen n with solutions in the OEIS and didn't find anything. Here's a list of all n with solutions up to 100 in case it helps: 2, 4, 5, 8, 10, 13, 14, 17, 18, 20, 22, 25, 26, 28, 29, 32, 34, 37, 38, 40, 41, 44, 45, 49, 50, 52, 53, 54, 58, 61, 62, 64, 65, 68, 70, 72, 73, 74, 76, 77, 80, 82, 85, 88, 89, 90, 92, 94, 97, 98, 100 EDIT #2:
My program just finished checking up to n = 1,000,000. From n = 100,000 to 1,000,000 the percentage of numbers with solutions stays between 50.7% and 51.6%. It's getting further from 50%, so I'm starting to doubt my conjecture is true. If anyone wants to run the program themselves, here's a link to txt version of it: https://www.dropbox.com/scl/fi/kb2p277sa6d29ieeedigk/mathprogram.txt?rlkey=sr5su9zny13y75qg0b5k1v0g4&dl=0 It's written in Python. I don't have any formal programming education either. Just started teaching myself Python a few weeks ago, so I apologize if it's confusing or inefficient code. Also, the program will not make much sense unless you've read the section in the linked proof called ""A Note on Searching For Satisfactory Triplets."" EDIT 3: There was a typo in my post. I said there are no solutions when N is 3 mod 6. I meant to write 6 mod 9. Sorry! EDIT 4: There were also a couple typos in my PDF where I used < when I meant to write ≤, but I have corrected them. (Unfortunately, this somehow created some new typos. It must be an issue with the conversion from .doc to .pdf but I don't think it effects the clarity much.) EDIT 5: Some errors in the proof were fixed, and a new link has been created. This one should be viewable by anyone. EDIT 6: Refined the approximation of how many numbers (less than N for sufficiently large N) don't have solutions from 40% to 41.6%.","['number-theory', 'conjectures', 'elementary-number-theory', 'diophantine-equations']"
4757754,"Does $f(z)= \frac{z^5}{|z|^4} , \forall z \neq 0, ; f(0)=0$ satisfy Cauchy Riemann Equations?","In my book it is given that $$f(z)=\frac{z^5}{|z|^4} ,\forall z \in \mathbb{C}-\{0\}; f(0)=0$$ Satisfy Cauchy Riemann Equations at $z=0$ but not differentiable at $z=0$ . But while checking it does not satisfy the Cauchy Riemann Equations itself... Because $$f_x(0)=\lim_{h \rightarrow 0}\frac{f(h)-f(0)}{h}$$ $$=\lim_{h\rightarrow 0}\frac{\frac{h^5}{h^4}}{h}$$ $$=1$$ Therefore $f_x(0)=1$ Similarly proceeding for $f_y(0)$ We get $$f_y(0)=\lim_{ih \rightarrow 0} \frac{f(ih)-f(0)}{ih}$$ $$ = \frac{\frac{(ih)^5}{h^4}}{ih}$$ $$ = 1$$ therefore we got $$f_x(0)=f_y(0)=1$$ But by the Cauchy Reimann equations $f_x(z)=-i f_y(z)$ and hence the CR equations isn't satisfied... Am I doing this correctly? Is there any other way to do this? EDIT:- My mistake...It satisfies CR equations at z=0.","['complex-analysis', 'limits', 'derivatives', 'real-analysis']"
4757833,Covariant derivative of a rotationally symmetric metric,"I'm trying to follow a calculation of the curvature tensor of a rotationally symmetric metric, and there's this step I can't justify to myself. The discussion can be found on Petersen's ""Riemannian Geometry"", section 4.2.3. The setting is as follows. We have a metric $$g =dr^2 +\rho(r)^2ds^2_{n-1} $$ and we set $$ g_r = \rho^2 ds^2_{n-1} $$ We've proved that $$ Hess(r)=\frac{\partial_r \rho}{\rho} g_r $$ The author then calculates as follows: $$ 
\begin{align} 
 \nabla_{\partial_r}Hess(r) 
    &= \nabla_{\partial_r}(\frac{\partial_r \rho}{\rho} g_r) \\
    &= \nabla_{\partial_r} (\frac{\partial_r \rho}{\rho}) g_r + \frac{\partial_r \rho}{\rho} \nabla_{\partial_r} (g_r)\\
 \# &= \frac{(\partial_r^2 \rho)\rho - (\partial_r \rho)^2}{\rho^2}g_r \\
    &= \frac{(\partial_r^2 \rho)}{\rho}g_r -Hess^2(r)
\end{align}
$$ I've marked the step I'm confused about with a #. I think it implies $\nabla_{\partial_r} g_r =0$ , but when calculating, I have: $$
\nabla_{\partial_r} (g_r) = \nabla_{\partial_r}(\rho^2 ds^2_{n-1} ) = 2\rho \partial_r (\rho)ds^2_{n-1}
$$ What did I get wrong?","['riemannian-geometry', 'differential-geometry']"
4757836,On $\zeta(5)$ and the closed form of $\sum_{n= 1}^\infty \frac{1}{n^5(e^{2\pi n}\pm1)}$?,"Consider the closed-form of the sum, $$\sum_{n= 1}^\infty \frac{1}{n^p(e^{2\pi n}\pm1)} = \; ??$$ for $\color{blue}{p=4m+1}$ . (Since closed-forms are known for $p=4m+3.$ ) For $p=1$ , we have, \begin{align}
\sum_{n= 1}^\infty \frac{1}{n(e^{2\pi n}-1)} &=+\frac {3\log(\pi)}4-\log\Big(\Gamma\big(\tfrac 14\big)\Big)-\frac {\pi}{12}+\log(2)\\
\sum_{n= 1}^\infty \frac{1}{n(e^{2\pi n}+1)} &=-\frac{ 3\log(\pi)}4+\log\Big(\Gamma\big(\tfrac 14\big)\Big)\,+\,\frac {\pi}{4}-\frac{7\log(2)}4
\end{align} and the related, $$\log 2 = -\frac{4}{3}\sum_{n= 1}^\infty \frac{1}{n(e^{2\pi n}-1)} -\frac{4}{3}\sum_{n= 1}^\infty \frac{1}{n(e^{2\pi n}+1)}+\frac{2}{9}\pi$$ $$\zeta(5) = -\frac{72}{35}\sum_{n= 1}^\infty \frac{1}{n^5(e^{2\pi n}-1)} -\frac{2}{35}\sum_{n= 1}^\infty \frac{1}{n^5(e^{2\pi n}+1)}+\frac{1}{294}\pi^5$$ $$\zeta(9) = -\frac{992}{495}\sum_{n= 1}^\infty \frac{1}{n^9(e^{2\pi n}-1)} -\frac{2}{495}\sum_{n= 1}^\infty \frac{1}{n^9(e^{2\pi n}+1)}+\frac{125}{3704778}\pi^9$$ and so on, with the $\zeta(4m+1)$ formulas mentioned by Plouffe and Arndt in the 1998 article, ""Identities Inspired from Ramanujan’s Notebooks"" . Note that by splitting the formula for $\log 2$ , we get closed-forms for $\large{\sum_{n= 1}^\infty \frac{1}{n(e^{2\pi n}\pm1)}}$ . Question: Likewise, can we split the formula for $\zeta(5)$ and get a closed-form for, $$\sum_{n= 1}^\infty \frac{1}{n^5(e^{2\pi n}\pm1)} =\; ??$$ and other $p=4m+1,$ without expressing one case in terms of the other? Or without using other summations like, \begin{align}
\zeta(5) &= \frac{64}{5}\sum_{n= 1}^\infty \frac{1}{n^5(e^{\pi n}-1)}-\frac{74}{5}\sum_{n= 1}^\infty \frac{1}{n^5(e^{2\pi n}-1)}+\frac{1}{630}\pi^5\\[4pt]
 \zeta(5) &= \frac{256}{125}\sum_{n= 1}^\infty \frac{1}{n^5(e^{\pi n}+1)}-\frac{6}{125}\sum_{n= 1}^\infty \frac{1}{n^5(e^{2\pi n}+1)}+\frac{7}{2250}\pi^5
\end{align} and similar variations? P.S. In contrast, the concise closed-forms for $p=4m+3$ are already known for the negative case, examples in this post .","['summation', 'special-functions', 'closed-form', 'sequences-and-series', 'riemann-zeta']"
4757861,Adjoint of the derived group,"Firstly, let $G$ be a nice linear algebraic group (for instance connected reductive group) over $\mathbb{Q}$ . I shall first define the two other groups I require $\textbf{Definition:}$ If $Z(G)$ is the center of the group $G$ , then we define the adjoint group of $G$ , denoted by $G^{\operatorname{ad}}$ , to be the quotient $G/Z(G)$ . $\textbf{Definitoin:}$ The derived subgroup of $G$ , denoted by $G^{\operatorname{der}}$ , is the intersection of all normal subgroups $N$ of $G$ such that $G/N$ is commutative. Is $G^{\operatorname{ad}}=(G^{\operatorname{der}})^{\operatorname{ad}}$ ? I ask this because it seems to be true for $G=\operatorname{GL}_n$ , where $G^{\operatorname{der}}=\operatorname{SL}_n$ and $(G^{\operatorname{der}})^{\operatorname{ad}}\cong G^{\operatorname{ad}}\cong \operatorname{PGL}_n$ and also for $G=\operatorname{GSp}_{2n}$ , where $G^{\operatorname{der}}=\operatorname{Sp}_{2n}$ and $(G^{\operatorname{der}})^{\operatorname{ad}}\cong G^{\operatorname{ad}}$ . My question comes because I am trying to understand Shimura varities. If $(G,X)$ is Shimura datum, and $X^+$ is a connected component of $X$ , then $(G^{\operatorname{der}},X^+)$ is a connected Shimura datum. This clearly requires the two of them to have the same adjoint group, but I am not sure why this is true. A proof or any reference of one, will be a great help. If in fact the answer is 'no', then I might need to find some other reason why a connected component of a Shimura datum is a connected Shimura datum.","['algebraic-groups', 'reductive-groups', 'number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4757863,What is extension closure in triangulated categories?,"The term extension closure appears in some papers constructing t-structures on triangulated categories, for example in Section 1.2 of Bayer, Arend; Macrì, Emanuele; Toda, Yukinobu , Bridgeland stability conditions on threefolds. I Bogomolov-Gieseker type inequalities , J. Algebr. Geom. 23, No. 1, 117-163 (2014). ZBL1306.14005 . The text is roughly as follows. Let $X$ be a smooth projective $\mathbb{C}$ -variety. Let $D^b(X)$ denote the derived category of bounded complexes of coherent sheaves. Let $\mathrm{Coh}(X)\subseteq D^b(X)$ denote the heart of the natural t-structure. Given a torsion pair $\mathcal{T},\mathcal{F}\subseteq\mathrm{Coh}(X)$ , we want to consider a tilting heart $$\langle \mathcal{F}[1],\mathcal{T}\rangle\subseteq D^b(X)$$ which is called the extension closure . Here are my questions What is $\langle \mathcal{F}[1],\mathcal{T}\rangle$ ? What are the objects? What are extensions in $D^b(X)$ ? I know short exact sequences in $D^b(X)$ split and thus I do not believe extensions here mean short exact sequences as for abelian categories. Does the order matter? Does $\langle \mathcal{T},\mathcal{F}[1]\rangle=\langle \mathcal{F}[1],\mathcal{T}\rangle$ ?","['derived-categories', 'algebraic-geometry', 'triangulated-categories']"
4757878,Tough integrals: $\int_0^1\frac{\log x\arctan\left(\frac{\log x}{2\pi}\right)}{1+x^2}dx$,"Here are two integrals: $$1:\hspace{2cm} \int_0^1\frac{\log x\arctan\left(\frac{\log x}{2\pi}\right)}{1+x^2}dx=\frac{\pi^2}{4}\log\left(\frac{192\pi^4}{\Gamma\left(\frac14\right)^8}\right)+\frac{\pi C}{2}  $$ $$2:\hspace{1.7cm}\int_0^1\frac{\log x\arctan\left(\frac{\pi}{2\log x}\right)}{1+x^2}dx=\frac{\pi C}{2}+\frac{\pi^2}{8}-\frac{\pi^2}{24}\log\left(\frac{A^{36}}{2\pi^3}\right) $$ where $C$ is Catalan's constant and $A$ is Glaisher's constant. My proof for both of them uses Feynman's trick on $$G(a)=\int_0^1\frac{\log x\arctan\left(\frac{a}{\log x}\right)}{1+x^2}dx $$ but it requires to know some values of $\psi^{-2}(x)$ , which I had to look up on Wikipedia . So, given the ""nice"" closed forms, I hope there is a way to evaluate them without using Feynman technique. For instance in $(1)$ I tried to expand the $\arctan$ as a series, and then use the fact that $$\int_0^1\frac{\log^{2n}(x)}{1+x^2}dx=\frac{(-1)^n}{2}\left(\frac{\pi}{2}\right)^{2n+1}E_{2n} $$ to get a series with Euler numbers. But this is wrong, since $\left|\frac{\log x}{2\pi}\right|>1$ for $0<x<1$ , and so the series is not convergent. In contrast, if one tries the same approach on $(2)$ , where the series is convergent due to $\left|\frac{\pi}{2\log x}\right|<1$ , one gets integrals of the form $$\int_0^1\frac{\log^{-2n}(x)}{1+x^2}dx$$ which are not convergent at all. So how would you do it?","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'sequences-and-series']"
4757891,A question about convergence of integrals with respect to levy measures,"Suppose $g(x):\mathbb R^p\to \mathbb R$ is a bounded continuous function such that: \begin{equation}\label{8.3}
    \quad g(x)=1+o(|x|),\quad  \hbox{ as }  |x| \rightarrow 0 \quad \hbox{  and } \quad g(x)=O(1 /|x|),\quad  \hbox{ as }  |x| \rightarrow \infty.
\end{equation} i.e.: \begin{equation}\label{8.4}
\lim_{x \to 0} \frac{g(x)-1}{|x|} =0 \quad \hbox{  and } \quad |g(x)| \leq M \frac{1}{|x|},\, \forall\, x \geq x_0.
\end{equation} Let $(\mu_n)_{n \in \mathbb{N}}$ and $\mu$ be p-dimensional Levy measures such that: \begin{equation}\label{L}\tag{L}
\mu_n(E) \longrightarrow \mu(E),\quad \hbox{ as }n \to \infty\quad\left( \forall E \in \mathcal C_\mu, \hbox{class of continuity sets, and }\,0 \notin \bar E.\right)
\end{equation} Remember that $\mu$ is a Levy measure if $\int_{|x|\leq \epsilon} |x|^2 d \mu < 0$ and $\int_{|x|> \epsilon} d\mu< \infty$ , for all $\epsilon >0$ ( There are other equivalent definitions). Question Suppose \begin{equation}\label{H}\tag{H}
\int_{\mathbb R^p} |x|^2d\mu_n \leq C< \infty, \quad \forall\, n
\end{equation} (Given (\ref{L}), it is strightforward to show that (\ref{H}) implies $\int_{\mathbb R^p} |x|^2d\mu \leq C$ ). So, assuming (\ref{L}) and (\ref{H}), how to show that the following convergence: \begin{equation}\label{abc}\tag{I}
\int_{\mathbb R^p} x[1-g(x)] d\mu_n \longrightarrow \int_{\mathbb R^p} x[1-g(x)] d\mu < \infty, \quad (n \to \infty)
\end{equation} Remark If (\ref{H}) is not true, we have a counterexample. Suppose $p=1$ , $g(x)=\frac{1}{1+|x|^2}$ . Take $d\mu_n(x)=\frac{1}{x^2}\mathbf{1}_{(0,n)}dx$ and $d\mu(x)=\frac{1}{x^2}\mathbf{1}_{(0,\infty)}dx$ . Note that for any $\epsilon>0$ : $$\int_{|x|>\epsilon}d\mu_n = \int_{\epsilon}^n\frac{1}{x^2}dx=\frac{1}{\epsilon}- \frac{1}{n} \longrightarrow \frac{1}{\epsilon} = \int_{|x|>\epsilon} d\mu(x), \quad(n \to \infty)$$ I think that this is sufficient to prove (\ref{L}).
Now, note that $x[1-g(x)]= \frac{x^3}{1+x^2}$ . Thus, as $n \to \infty$ : $$\int_{\mathbb R} x[1-g(x)] d\mu_n =  \int_{0}^n \frac{x}{1+x^2} dx= \frac{1}{2}\ln(1+n^2)\longrightarrow \infty = \int_{\mathbb R} x[1-g(x)] d\mu$$ Help! (The criteria for choosing to give the bounty will be for whoever answers the correct answer the fastest.)","['measure-theory', 'convergence-divergence', 'probability-theory', 'probability']"
4757910,Can anyone help to perform this heinous integral? (Peskin & Schroeder's Quantum field theory (4.76)-(4.78)) (Including my own trial ),"I am reading the Peskin & Schroeder's An introduction to Quantum field theory, p.105~p.106 (Construction of a cross-section from the invariant matrix element (p.104) ) and stuck at understanding some integration. I think that I never seen a pattern like this. EDIT : I edited this post to try to answer this question on my own. I wish this edited post will be helpful to readers. (And if there is anything that needs to be corrected, then it will be appreciate to inform me.) First, we are given (their book, p.105, (4.76):) $$ d\sigma =(\prod_f\frac{d^3p_f}{(2\pi)^3}\frac{1}{2E_f}) \int d^2b(\prod_{i=A,B}\int \frac{d^3k_i}{(2\pi)^3}\frac{\phi_i(\vec{k}_i)}{\sqrt{2E_i}}\int \frac{d^3 \bar{k}_i}{(2\pi)^3}\frac{\phi_i^{*}(\bar{\vec{k}}_i)}{\sqrt{2\bar{E}_i}})$$ $$\times e^{\vec{b} \cdot (\bar{\vec{k}_B} - \vec{k}_B)} ( _{out}\langle \{\vec{p}_f\}|\{\vec{k}_i|\}\rangle_{in})( _{out}\langle \{\vec{p}_f\}|\{\bar{\vec{k}}_i|\rangle_{in})^*  \tag{4.76}.  $$ And the goal is an expression ( p. 106, (4.78) ) : $$ d\sigma = (\prod_f\frac{d^3p_f}{(2\pi)^3}\frac{1}{2E_f})\frac{|\mathcal{M}(p_A, p_B \to \{p_f\})|^2}{2E_A 2E_B|v_A-v_B|}  \tag{4.78} \int \frac{d^3k_A}{(2 \pi)^3}\int\frac{d^3k_B}{(2\pi)^3} $$ $$\times |\phi_A(\vec{k}_A)|^2|\phi_B(\vec{k}_B)|^2 (2\pi)^4 \delta^{(4)}(k_A + k_B - \Sigma p_f).$$ Through p.105~p.106 he derives this formula but I don't quite understand his argument in part. For example, ""..We can use the scond of these delta functions, together with the $\delta^{(2)}(k_B^{\perp}-\bar{k}_B^{\perp})$ , to perform all six of the $\bar{k}$ integrals in $(4.76)$ . Of the six (?) integrals, only those over $\bar{k}_A^z$ and $\bar{k}_B^z$ require some work. ( his book p.105 ).."" and ""Now recall that the initial wavepackets are localized in momentum space, centered on $\vec{p}_A$ and $\vec{p}_B$ . This means that we can evaluate all factors that are smooth functions of $\vec{k}_A$ and $\vec{k}_B$ at $\vec{p}_A$ and $\vec{p}_B$ , pulling them outside except the remaining delta function ( his book p.106, first paragraph ).. "" Second, meanwhile, I found next note (Introduction to quantum field theory by Nastase): https://professores.ift.unesp.br/ricardo.matheus/files/courses/2014tqc1/QFT1notes.pdf In p.176 ~ p.177 of his note, he notes that ( 19.35 , 19.36 ) $$ \int d^2\vec{b} e^{i\vec{b} \cdot (\vec{\bar{k}}_B-\vec{k}_B)} = (2\pi)^2 \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) $$ $$ ( _{out}\langle \{\vec{p}_f\}|\{\vec{k}_i|\}\rangle_{in}) = i \mathcal{M} (2 \pi)^4 \delta^{(4)}(\Sigma k_i - \Sigma p_f)$$ $$( _{out}\langle \{\vec{p}_f\}|\{\bar{\vec{k}}_i|\rangle_{in})^* = -i \mathcal{M}^* (2 \pi)^4 \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \tag{19.35} $$ Q. What's the definition of $k_B^{\perp}$ ( $\bar{k}_B^{\perp}$ )? EDIT : I think that $k_B^{\perp} := (k_B^{x},k_B^{y})$ , $\bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y}))$ . Accepting this, we can show the first equality in (19.35) ( ; $d^2 b = d b^x d b^y$ ? ). And ( using $\bar{k}_A = ( \bar{E}_A , \bar{k}_A^{x} , \bar{k}_A^{y} , \bar{k}_A^{z} )$ , $\bar{k}_B = ( \bar{E}_B , \bar{k}_B^{x} , \bar{k}_B^{y} , \bar{k}_A^{z} )$ and $p_f = (E_f, p_f^{x}, p_f^{y}, p_f^{z})$ ) $$\int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) $$ $$\stackrel{?}{=}  (k_i^{\perp} = \bar{k}_i^{\perp}) \times \int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f) $$ $$ = \int d \bar{k}_A^z \delta ( \sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A}$$ $$\stackrel{?}{=} \frac{1}{|\frac{\bar{k}^z_A}{\bar{E}_A} -\frac{\bar{k}_B^z}{\bar{E}_B} |} = \frac{1}{|v_A -v_B|}  \tag{19.36}$$ Putting everything together, we find the above (4.78). Q. My question is, why the equalities in (19.36) marked by question symbol  are true? Can we prove these more concretely? I think that this integral is one of the hardest one I've ever seen, because partially I don't know what is exact definition for $k_B^{\perp}$ ( or $\bar{k}_B^{\perp}$ ), $\bar{k}_A^z$ ( or $\bar{k}_B^z$ , $p_f^z$ ). EDIT : So far, I think that $k_B^{\perp} := (k_B^{x},k_B^{y})$ , $\bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y}))$ . Correct? For the first equality in (19.36) , perhaps, does next formula holds true? $$ \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) = 1 \operatorname{or} ( k_i^{\perp}= \bar{k}_i^{\perp})$$ ?
If so, how? If this is true, then we maybe drive the first equiality in (19.36) (?) So far, I don't understand what the strange notation $(k_i^{\perp} = \bar{k}_i^{\perp})$ exactly means. Here, it seems that the definition of $k_B^{\perp}$ and $\bar{k}_B^{\perp}$ plays key role. EDIT : I think that the strange notation $(k_i^{\perp} = \bar{k}_i^{\perp})$ in (19.36) is negligible. For the first equality in (19.36),
I think that we may use $$\int_{-\infty}^{\infty} \delta(x_i-x)dx_i = 1 $$ Note that $$\int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =\int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f) $$ $$\times \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) $$ So, if $$ \int d \bar{k}_A^{x} \int d \bar{k}_A^{y} \int d \bar{k}_B^x \int d \bar{k}_B^{y} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =1 $$ ,
we obtain $$\int d^3\bar{k}_A \int d^3 \bar{k}_B \delta^{(4)}(\Sigma \bar{k}_i - \Sigma p_f) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) =\int d \bar{k}^z_A d\bar{k}_B^{z} \delta(\bar{k}_A^{z} + \bar{k}_B^{z} - \Sigma p_f^{z}) \delta ( \bar{E}_A + \bar{E}_B - \Sigma E_f) $$ , which is the first equality in the (19.36) above.
But I think that is true since, for example, $\delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp}) = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y})$ ( C.f. Here we use $k_B^{\perp} := (k_B^{x},k_B^{y})$ , $\bar{k}_B^{\perp}:= (\bar{k}_B^{x}, \bar{k}_B^{y})) $ is independent from variable $\bar{k}_A^{x}$ , this can be pulled outside the $\bar{k}_A^{x}$ -integral ; i.e., $$ \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta^{(2)}(k_B^{\perp} - \bar{k}_B^{\perp})$$ $$=  \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) $$ $$ = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) \int d \bar{k}_A^{x} \delta ( \bar{k}_A^{x} + \bar{k}_B^{x} - \Sigma p_f^{x}) $$ $$ = \delta ( \bar{k}_A^{y} + \bar{k}_B^{y}-\Sigma p_f^{y} ) \delta(k_B^{x} - \bar{k}_B^{x}) \delta(k_B^{y} - \bar{k}_B^{y}) \times 1$$ And so on.. This arguemnt really works? And for the final equality in (19.36) , my frist attempt is, I want to use that $$ \delta[f(x)] = \Sigma_{i} \frac{\delta(x-x_i)}{|f'(x_i)|}$$ if $f(x_i)=0, f'(x_i) \neq 0$ ; (C.f. Boas, mathematical methods in the physical sciences, p456)
From this, we can show that $$ \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|}$$ (True?)
And, let $$f(\bar{k}_A^{z}) := (\sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A} $$ Note that (by simply taking derivative) $$ \frac{d}{d \bar{k}_A^{z}} f(\bar{k}_A^z) = \frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}  $$ Then from this, how can we deduce the above final equality in (19.36) using $$ \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|}$$ ?
What will be roots $x_i$ of $f( \bar{k}_A^{z})$ such that $f'(x_i) \neq 0$ ? EDIT : For this issue, let's look (19.36) more closely. First, $\int d \bar{k}_A^z \delta ( \sqrt{\bar{k}_A^2 + m_A^2}+\sqrt{\bar{k}_B^2 + m_B^2} - \Sigma E_f ) |_{\bar{k}^z_B = \Sigma p_f^z - \bar{k}^z_A}$ is a 'number', which can be calculated by $ \int \delta[f(x)] dx = \Sigma_{i=1} \frac{1}{|f'(x_i)|}$ . Second, on the other side, $\frac{1}{|\frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}|} = \frac{1}{|\frac{d}{d \bar{k}_A^{z}} f(\bar{k}_A^z)|}$ is a 'function', which depends on variable $\bar{k}_A^{z}$ . Where does this discrepancy occurs? I guess that, the notation $\frac{1}{|\frac{\bar{k}_A^{z}}{\bar{E}_A}- \frac{\bar{k}_B^{z}}{\bar{E}_B}|} $ means an 'implicit' notation
indicating for $\Sigma_{i=1} \frac{1}{|f'(x_i)|}$ . True?","['integration', 'quantum-field-theory']"
4757913,"Why do complex analysis and complex geometry exhibit a sort of ""arithmetic"" behavior?","That is going to be a rather vague question, but as I stated in the title, why do complex analysis and complex geometry seem to exhibit a sort of ""arithmetic"" behavior, contrary to real analysis and real differential geometry? I am referring to facts like complex tori having as moduli space $\mathbb H/PSL(2, \mathbb{Z})$ while real tori are all diffeomorphic, or the relation between complex analysis and number theory in general. Is there a deeper reason for that? After all, at a first glance real and complex analysis are built on the derivative, which is constructed in the same way in $\mathbb{R}$ and $\mathbb{C}$ , only relying on completeness as metric spaces and field operations (as far as I know). Only difference (although a relevant one) being that one of them is algebraically closed, while the other one is not; again, as far as I know. Apologies again for this not-so-well-asked question.","['number-theory', 'complex-geometry', 'complex-analysis', 'abstract-algebra', 'differential-geometry']"
4757928,"If $X\in\mathscr L^5$, then $X\in\mathscr L^1$.","I have a random variable $ X $ . Im suppossed to prove or disprove if $ X \in {\mathscr L}^5 $ , then $ X \in {\mathscr L}^1$ .
However, I dont know what $\mathscr L $ is suppossed to mean. I dont have the script for the course so I dont know how it was defined. However, I know that Lebesgue integration was covered in the course if thats helpful. Edit: As sugessted in the comments, I tried using the Hölder inequality, where I get something like this: $$E(|X^2|) \ \le \ E(|X|^p)^{\frac{1}{p}} \ E(|X|^q)^{\frac{1}{q}} $$ I tried $p=2$ and $p=5$ in hopes of getting rid of the $E(|X^2|)$ or trying to sneak the 5th power of $X$ in, but it didnt work. For $p=2$ , $$
E(|X^2|) \ \le \ E(|X|^2)^{\frac{1}{2}} \ E(|X|^2)^{\frac{1}{2}}=E(|X|^2)
$$ I get a very trivial result and for $p=5$ , I get the following: $$
E(|X^2|) \ \le \ E(|X|^5)^{\frac{1}{5}} \ E(|X|^{\frac{5}{4}})^{\frac{4}{5}}
$$ And from here I intended to show that $E(X^5) \le E(X^2) \lt \infty$ but I cant rearrange the equation in that way.","['probability-theory', 'probability', 'random-variables']"
4757939,"Let $\{a_n\}$ satisfy $a_1=1, a_{n+1}=\sin(a_n)$, find $\lim_{n\to\infty}\frac{\log(a_n)}{\log(n)}$.","This is question 24 of a Brazilian Olympiad multiple-choice test (for undergraduate students) that you need to pass in order to qualify for the main national math Olympiad. The problem is as follows: Let $\{a_n\}$ be a sequence of real numbers recursively defined by $a_1=1$ and $a_{n+1}=\sin(a_n)$ . Find the value of $$\lim_{n\to\infty}\frac{\log(a_n)}{\log(n)}.$$ The answer choices are A) 0 B) $-\frac{1}{2}$ C) $-1$ D) 1 E) $-\infty$ . I tried using $\sin(x) > x - \frac{x^3}{6}$ to simplify the nested sine functions, but that didn't work. I also attempted to find the Taylor expansion of the nested sine function, but I could only get up to the $x^3$ term and the other terms were too difficult. My feeling is that since $a_n \to 0$ and $\log$ approaches $-\infty$ quickly while $\log(n)$ approaches $\infty$ very slowly, the answer will be E. It's worth noting that this exam has 25 problems to be completed in 3 hours, and while there are many non-trivial problems, they shouldn't be too difficult.","['contest-math', 'recreational-mathematics', 'sequences-and-series']"
4757966,Preimage of a specific size,"My kid has this problem in a discrete-math course: How many functions $f:\{1,2,3,4\}\to\{1,2,3,4\}$ have the property that, for all $i$ , the inverse image of $i$ does not have size $i$ (that is, $\left|f^{-1}[\{i\}]\right|\ne i$ )? One can of course list all of them, but I suspect there's a nicer way that I'm not seeing.","['functions', 'combinatorics']"
4757995,Solution to differential equation $(x - y/y')^2 (1 + (y')^2) = 1$,"This differential equation showed up in a geometry problem $$ \left(x - \frac{y}{y'}\right)^2 \left(1 + \left(y'\right)^2\right) = 1 $$ I figured out by trial and error that $y(x) = \left( 1 - x^{2/3} \right)^{3/2}$ , the graph of an astroid, is a solution, but I'd like to see a way how this can be solved in a more proper manner, rather than just by getting lucky, and if multiple solutions exist.",['ordinary-differential-equations']
4758033,Probability that each person will get someone else's hat or/and coat.,"Each of the $5$ people leaves a coat and a hat in the cloakroom. The absent-minded cloakroom attendant gives each person a random coat and a random hat. Calculate the probability of the event that each of the $5$ people gets someone else's coat or someone else's hat. To count the good cases, I thought to subtract from all possibilities those where at least one person gets both their own coat and their own hat. I used the principle of inclusion and exclusion. Initially, we assume that one person has their things, so we have $|A_i| = \binom{5}{1} \cdot 4! \cdot 4!$ , where we choose the person with $\binom{5}{1}$ , and we assign the remaining hats and coats with $4! \cdot 4!$ . For two people it is $\binom{5}{2} \cdot 3! \cdot 3!$ , etc. So my solution is $$\frac{1}{n! \cdot n!}\sum_{i=0}^n (-1)^i \color{blue}{\binom{n}{i}} ((n-i)!)^2 = \frac{11844}{14400} \approx 0.82$$ And in the answers, there is $$\frac{1}{n! \cdot n!}\sum_{i=0}^n (-1)^i ((n-i)!)^2 = \frac{13856}{14400} \approx 0.96$$ Why don't we want to choose the person who will get their things?","['discrete-mathematics', 'combinatorics', 'probability']"
4758051,If $ \prod_{k=0}^n (x+\frac{1}{2^k}) = \sum_{k=0} ^n a_kx^k $find $\lim_{x\to \infty}\frac{a_{n-2}}{a_{n-4}} = ?$,"If $$ \prod_{k=1}^n (x+\frac{1}{2^k}) = \sum_{k=0} ^n a_kx^k 
$$ then find $$\lim_{n\to \infty}\frac{a_{n-2}}{a_{n-4}} = ?$$ I’m able to find $a_{n-2}$ easily but finding $a_{n-4}$ is quite difficult for me. I think involving binomial theorem or principle of inclusion and exclusion may work but still I’m not able to proceed with that.","['limits', 'binomial-coefficients', 'sequences-and-series']"
4758122,Question on equality in ODEs,"I've been doing some reading in differential geometry and it's caused me some confusion over the objects involved in differential equations. Consider $f:\mathbb{R}\rightarrow\mathbb{R}$ and a first order ODE $$\frac{df}{dt}\Bigg|_t=g(f,t)$$ The R.H.S evaluates on $\mathbb{R}\rightarrow\mathbb{R}$ while the L.H.S $\frac{df}{dt}\Big|_t=f_*(\frac{d}{dt}\Big|_t)$ evaluates on $\mathbb{R}\rightarrow T\mathbb{R}$ so it seems to me that we stating that unlike objects are equal. It seems to me that on the L.H.S we are really interested in the 'natural' map $v(t)\frac{d}{dt}\Big|_t\mapsto v(t)$ , is this correct and should it be made explicit? (and without this is a diffeomorphism more appropriate than an equality?) It's worth mentioning that the partial derivative of $f:M\rightarrow\mathbb{R}$ with coordinate system $x$ is defined by a in terms of the directional derivative $$\frac{\partial f}{\partial x^i}\Bigg|_p := D_i(f\circ x^{-1})(x(p))$$ and perhaps this is from where my confusion is arising. If I plug this definition into the above problem I get $\frac{df}{dt}\Big|_t=f'(t)$ which appears to resolve my issue, but I've chosen to reject this answer! This definition appears to be useful in that it tells explains how to compute the value of the tangent vector, but still leaves me befuddled with respect to the objects at play. Is there something else that I am missing? Anything to alleviate my confusion will be much appreciated. Note: The notation that I'm using (should be) consistent with that used by Spivak. Edit 1: From the answers I've received it seems that we do make this identification is between $\mathbb{R}$ and $T\mathbb{R}$ implicitly. Mathematical 'Identifications' such as these feel very 'algebraic' to me - which leads me to a second (and less well formed) question: Is there a nice way to describe this using some algebraic theory of morphisms?","['ordinary-differential-equations', 'abstract-algebra', 'partial-differential-equations', 'derivatives', 'differential-geometry']"
4758129,Riff on Sequential Criterion for Continuity,"A function $f:X\rightarrow\mathbb{R}$ on a metric space $(X,d)$ is continuous at $p\in X$ if and only if whenever we have a sequence $\{x_n\}\subseteq X\setminus\{p\}$ such that $x_n\rightarrow p$ , $f(x_n)\rightarrow f(p)$ . For my own curiosity, I asked myself whether a function $f:[a,b]\rightarrow\mathbb{R}$ is continuous provided it satisfies the following weaker condition: whenever we have a sequence $\{x_n\}\subseteq ([a,b]\setminus\{p\})\cap\mathbb{Q}$ such that $x_n\rightarrow p$ , $f(x_n)\rightarrow f(p)$ . After playing with examples like Thomas’s function and it’s variants, I have been unable to find a counterexample as the point $p$ is not required to be a rational number. At the same time, when I try to prove $f$ is continuous, I am unable to do so. Does anyone have any hints?","['continuity', 'sequences-and-series', 'metric-spaces', 'real-analysis']"
4758132,Probabilistic Kleene's recursión theorem,"Kleene's recursion theorem states that for every total computable function $f$ there is an index $e$ such that $$\phi_{f(e)} = \phi_e,$$ where $\phi_n$ is a valid enumeration of the partial computable functions. Call such an $e$ a fixed point of $f$ . In Cutland's Computability , a question is proposed: would a chimpanzee sending each $n$ to a random $f(n)$ leave some $e$ fixed? Although the question is mainly philosophical, there is a purely mathematical version: If a random total function $f : \mathbb N \to \mathbb N$ is chosen, what is the probability that $f$ does not have fixed points in the above sense? I conjecture that the measure of that set is going to be zero, so the particular probability measure used is not going to be important. My reasons for the conjecture . I find it rather unlikely that computable functions all share a property that few non-computable functions share (except for properties that clearly have to do with $f$ being computable). In fact, all the functions in Turing degrees below $\emptyset'$ are known to have this property, not only computable functions. (Edit: only the recursively enumerable degrees below it) Nevertheless, it also appears that, for every $e$ , almost no function fixes $e$ , and so many, many functions won't have fixed points. This is counter evidence for my conjecture. This question is probably simple, but I don't have much background in measure theory.","['measure-theory', 'fixed-points', 'probability', 'computability']"
4758140,A probability problem about picking balls - and somehow related to solar panels,"CONTEXT Hello everyone! I decided to make this post for a friend of mine, who is currently working on energy-related subjects. She had been asked to simulate how solar panels would react when being randomly damaged. She was able to complete this task, but wanted to make sure there was no mistake in her code, so she tried to compute the theorical probabilistic results. So, she shared her work with me and a group of friends by simplifying all the complicated physics that was hidden, leaving us with something that looked like a very classic school probability problem. The thing is, we all kinda suck at math today (we're finishing our engineering studies, so yeah, long time since we last used our brain for math stuff). This is where I need your help; there isn't any knowledge related to solar panels needed to help her, and we were hoping some of you could give an answer that would help computing the theorical probabilities. Here is the problem: THE PROBLEM We have a bag containing $r$ red balls, $g$ green balls and $b$ blue balls. We pick a ball at random in the bag, draw a line on it, and put it back in the bag. When a ball has a fixed number of $n$ lines drawned on it, we don't put it back in the bag but place it in a separated box. We're repeating this process until eventually the bag gets emptied, and all the balls are in our box. Question: On average, how many picks do we have to make to have at least one ball of each color in our box? DISCUSSION It is interesting to note that this is actually a finite problem, and that no matter how we pick the balls it will take exactly $n(b+r+g)$ picks to have our box filled with all the balls. The pigeonhole principle also helps to grant a (slightly) better upper boundary for the average we're looking for. Let's suppose without any loss of generality that $g=\min \{r,g,b\}$ ; then we can be sure we have at least one ball of each color after $n(r+b)+g(n-1)+1$ picks, which represents the worst case scenario: we take out all red and blue balls, then draw $n-1$ lines on every green ball before finally taking out one of the green balls in the next pick. Sadly, no one in our group was able to find anything better than an upper bound. We had much trouble finding out what the right modelisation of random variables was, and could not handle the fact that the number of balls in the bag randomly change overtime. So, does anyone have an idea how to solve this problem? We're actually trying to solve for precise values in our context, with $b=r=20$ and $g=n=10$ ; but it would be great if we could find a solution for the general case!",['probability']
4758159,Does the Mobius Strip have an homogenous embedding?,"So in this question I’m trying to do two things at once. 1. Define what a “homogenous embedding” is by describing what it is like and then furthermore ask if such an embedding exists for the mobius strip. We start with the first part. A manifold is homogenous, loosely, if every point is like every other point in a “rigid” sense ie there are isometries of the manifold sending one point to the other. Examples of this are the following: The real line, or any line in $\mathbb{R}^n$ . any point can be mapped to any other point by shifting the line. So all points are kind of the same. The perimeter of a circle embedded in $\mathbb{R}^2$ , any point can be mapped to any other point by rotation so again all points are kind of the same. The surface of a sphere (which naturally embeds in $\mathbb{R}^3$ or higher, any point can clearly be rotated to any other point. The surface of an infinite cylinder embedded in $\mathbb{R}^3$ . We can translate or rotate any point to any other (notice with a finite cylinder there is a clear difference between boundary points and interior points so this really HAS to be infinite and can be viewed as $S_1 \times R$ where the homogenous group actions from those factors gives rise to the homogenous group action for the whole space) A torus in $\mathbb{R}^4$ this one is a little tricky to see. In $\mathbb{R}^3$ the torus isn’t homogenous because most obvious the inner perimeter has a different length than the outer perimeter. In $\mathbb{R}^4$ it becomes possible to draw a torus so that all perimeters are identical in length and so we can really, say all points are identical with a natural action of $S_1 \times S_1$ acting on them. So now we get to the meat of the question. What about the mobius strip? Is there a way to embed an infinite mobius strip in $\mathbb{R}^4$ or higher so that it doesn’t self intersect and really is homogenous? IE every point is basically identical, meaning for any pair of points there is a function from the manifold to itself that preserves all relative distances and maps one point to the other? It’s not clear to me if this is possible but I do feel optimistic.","['homogeneous-spaces', 'isometry', 'general-topology', 'lie-groups', 'differential-geometry']"
4758227,How to deduce the monotonicity of a function from another function?,"I have the following task and I am a bit confused by the solution of it. In the official solution it's stated that only the first assumption is correct. But in my approach I am always coming to the end that the first and the third assumptions are correct. I would be happy if someone could verify if my solution is correct and there is a mistake in the solutions or to explain where I am thinking wrong. Task The function $f:\mathbb{R}\rightarrow\mathbb{R}\setminus\{0\}$ is a
strict monotonic growing function. Let $g:\mathbb{R}\rightarrow\mathbb{R}$ , $g(x)=\frac{1}{f(x)}$ . Which of the following is correct? If $f$ doesn't take on positive function values, $g$ is strictly monotonically decreasing. If $f$ doesn't take on positive function values, $g$ is strictly monotonically increasing. If $f$ only takes on function values which are bigger than $-1$ , $g$ is strictly monotonically decreasing. If $f$ only takes on function values which are less than $1$ , $g$ is strictly monotonically increasing. My Solution First we check what happens when $f$ doesn't take on positive function values. That means that for every $x\in\mathbb{R}$ , $f(x)<0$ holds. We know that if we have $x,y\in\mathbb{R}$ with $x<y$ then $f(x)<f(y)$ . In this case the function $f$ looks like this $f:\mathbb{R}\rightarrow\mathbb(-\infty,0)$ . Where $(-\infty,0)$ is an interval. Then let $h$ be the following function $h:(-\infty,0)\rightarrow\mathbb{R}$ , $h(x)=\frac{1}{x}$ . We can see that $h$ is strictly monotonic decreasing. If we have $x,y\in(-\infty,0)$ with $x<y$ then we can see that $f(x)>f(y)$ holds. Now the following holds: $(h\circ f)(x)=h(f(x))=\frac{1}{f(x)}=g(x)$ . Because the function composition of one strictly monotonic increasing and one strictly decreasing function is strictly decreasing again, we can say that the first assumption is a correct one and the second one is wrong. From this reasoning we can also deduce that the fourth assumption is wrong. Now we are looking at the third assumption. Where we look at the case where $f(x)>0$ . Here our function $f$ looks like the following $f:\mathbb{R}\rightarrow(0,\infty)$ . Then we let $p$ be the following function $p:(0,\infty)\rightarrow\mathbb{R}$ , $p(x)=\frac{1}{x}$ . If we have $x,y\in(0,\infty)$ with $x<y$ we have $p(x)>p(y)$ and thus we have a monotonic decreasing function again. Now the following holds again $(p\circ f)(x)=p(f(x))=\frac{1}{f(x)}=g(x)$ . And because the function composition of one strictly monotonic increasing and one strictly decreasing function is strictly decreasing again. Thus the third assumption is correct as well. So where is my error, or is the solution incorrect?","['calculus', 'functions', 'solution-verification', 'algebra-precalculus']"
4758266,Prove that $\sin(x + y + z) \ge \frac45$ for reals with $\sum\limits_{\mathrm{cyc}} \sin x = 2$ and $\sum\limits_{\mathrm{cyc}} \cos x = \frac{11}{5}$,"Problem . Let $x, y, z$ be real numbers
with $\sin x + \sin y + \sin z = 2$ and $\cos x + \cos y + \cos z = \frac{11}{5}$ .
Prove that $$\sin(x + y + z) \ge \frac45.$$ This question is related to these two questions Q1 , Q2 . The result can be verified by Mathematica . In detail, denote $c_1 = \cos x, c_2 = \cos y, c_3 = \cos z, s_1 = \sin x, s_2 = \sin y, s_3 = \sin z$ . We need to prove that $s_1c_2c_3 - s_1s_2s_3 + c_1s_2c_3 + c_1c_2s_3 \ge \frac45$ given that $c_1^2 + s_1^2 = c_2^2 + s_2^2 = c_3^2 + s_3^2 = 1$ and $s_1 + s_2 + s_3 = 2$ and $c_1 + c_2 + c_3 = \frac{11}{5}$ . Mathematica says it is true. Equality holds if e.g. $\cos x = \frac45, \cos y = \frac45, \cos z = \frac35, \sin x = \frac35, \sin y = \frac35, \sin z = \frac45$ . Is there a nice/simple proof?","['inequality', 'maxima-minima', 'trigonometry', 'optimization', 'algebra-precalculus']"
4758272,Keep rolling two dice until the cumulative sum hits 1000,"Suppose I roll two dice until the cumulative sum hits 1000 (inclusive). What's the most likely last roll? That is, suppose $X$ is the sum of the two dice during the first roll that makes the cumulative sum go over 1000. What's its mode? I'm struggling to get the answer. I know the most likely sum from a roll of two dice is $7$ . I also know how to compute the expected value of the last roll if I were rolling just one die. However, I'm stuck on computing the mode of rolling tow dice. Can someone help?","['dice', 'probability']"
4758281,Solving $\cos\phi\cos(2\theta - \phi)+\sin(\theta - \phi)\sin(\theta + \phi)=0$ for $\theta$,Let $$ d = \frac{2v_i \cos(\theta)\sin(\theta - \phi)}{g\cos^2\phi} $$ Now I have to equate $\frac{d}{d\theta}$ to $0$ and find $\theta$ . $$\frac{(2(v_i)^2\sin{\theta}\sin(\theta - \phi)+2(v_i)^2\cos{\theta}\cos(\theta - \phi))g\cos^2\theta+4(v_i)^2\cos{\theta}\sin(\theta - \phi)g\cos{\theta}\sin\phi}{g^2\cos^4\phi}=0$$ Then I get to the following expression: $$-\cos{\phi}\sin{\theta}\sin({\theta - \phi})+\cos{\theta}\cos(\theta - \phi)\cos\phi+2\cos{\theta}\sin(\theta - \phi)\sin \theta=0$$ $$\cos\phi\cos(2\theta - \phi)+\sin(\theta - \phi)\sin(\theta + \phi)=0$$ I need to get to $\theta = \frac{\pi}{4}-\frac{\phi}{2}$ but I don't know how to continue. I tried different ways but I never get to the desired solution.,"['algebra-precalculus', 'trigonometry']"
4758289,Question about axioms of topology [duplicate],"This question already has answers here : Why do we require a topological space to be closed under finite intersection? (7 answers) Closed 10 months ago . In the three axioms of topology on $X$ , denoted by $\tau_X$ , which are, $\emptyset \in \tau_X$ , and $X \in \tau_X$ for elements(open sets) $u_1, u_2, ... \in$ $\tau_X$ , $$\bigcup^{\infty}_{i = 1}u_i \in \tau_X$$ for elements(open sets) $u_1, u_2,... \in \tau_X$ , $$\bigcap^{n}_{i=1}u_i \in \tau_X$$ What I want to ask is that why the third axiom of topology cannot be ""the intersection of infinite number of elements are in topology class"", is this due to some properties we want for topology, can somebody tell me why and if so, what will happen?","['general-topology', 'axioms']"
4758292,A map between suspensions which is a homology isomorphism,"Problem: $Y \subset X$ be a subcomplex of a C. W. complex. Suppose that there is a retraction $r: X \to Y$ .
Prove that there is a map from $\Sigma X \to \Sigma Y \vee \Sigma(X/Y)$ which is a homology isomorphism. Taking the example of $X$ being the annulus and $Y$ the inner circle I get that the given spaces of suspensions are homotopy equivalent. In general I think we need to show that $\Sigma X $ and $ \Sigma Y \vee \Sigma(X/Y)$ are homotopy equivalent. A homotopy equivalence, say $f$ , will clearly give isomorphisms on all the homology groups.
But I do not know how to show the homotopy equivalence. In fact it is not necessary I think since the question only asks for homology isomorphisms.","['homotopy-theory', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
4758322,"On the integral $\int_0^1 \frac{\ln(2-x)}{1+x^2} \, \mathrm{d}x$","When I found closed form of integrals $1$ and $2$ \begin{align}
\int_0^1 \frac{\ln(1+x)}{1+x^2} \, \mathrm{d}x  \tag{1}\\
\int_0^1 \frac{\ln(1-x)}{1+x^2} \, \mathrm{d}x  \tag{2}\\
\end{align} I thought why not generalize this for any $ b \in (-\infty, 1]$ like this $$ \mathcal{I}(b) = \int_0^1 \frac{\ln(1-bx)}{1+x^2} \, \mathrm{d}x  \tag{3}\\$$ Using differentiation under integral sign I got that, $$ \mathcal{I}'(b) = \frac{1}{b^2+1} \bigg[ -\ln(1-b) + \frac{1}{2}\ln(2) - b\frac{\pi}{4} \bigg]  
 \tag{4}$$ Now after integrating [ $4$ ] from $0$ to $y$ we get, $$ \mathcal{I}(y)= -\int_0^y \frac{\ln(1-x)}{x^2+1} \, \mathrm{d}x + \frac{1}{2} \ln(2) \arctan(y) - \frac{\pi}{8}\ln(y^2+1) \tag{5} $$ I wanted to find closed form of [3] at $b=1/2$ , $$ \mathcal{I}(1/2) = \int_0^1 \frac{\ln(2-x) - \ln(2)}{1+x^2} \, \mathrm{d}x $$ from [5] I got, $$ \mathcal{I}(1/2)= -\underbrace{\int_0^{1/2} \frac{\ln(1-x)}{x^2+1} \, \mathrm{d}x}_{\mathcal{J}} + \frac{1}{2} \ln(2) \arctan\left( \frac{1}{2}\right) - \frac{\pi}{8}\ln\left( \frac{5}{4}\right) \tag{6} $$ Now I have been struggling with $\mathcal{J}$ . I have no idea how to find its closed form. I tried by substituting $x \to x/2$ then I got, $$ \mathcal{J} = \int_0^1 \frac{2\ln(2-x)}{x^2+4} \, \mathrm{d}x $$ Now I am totally stuck at this one.... Series expansion is making it more difficult to evaluate. I search on Approach Zero but couldn't find which can help me. Thank you very much!","['integration', 'definite-integrals', 'special-functions', 'real-analysis']"
4758374,"Question from Katok and Hasselblat (page 186, proposition 5.1.5)","On page 186, the authors are discussing that if $f:M\rightarrow M$ is a non-invertible smooth map on a manifold $M$ , $\Omega$ is a volume form and $\rho:M\rightarrow \mathbb{R}^+$ is a density so that the measure $\mu(A) := \int_A \rho.\Omega$ defines an $f$ -invariant measure, then $$
\rho(x) = \sum_{y \in f^{-1}(x)} \dfrac{\rho(y)}{\mathrm{Jac}(f)(y)}.
$$ I was wondering why this should be true. I was guessing that probably you can still define something like a measure theoretic version of the pull back of the volume form for non-invertible smooth maps. Indeed, if $M$ is compact, almost every point has finitely many pre-images by Sard's and Regular value thoerems. Therefore, via summing up sum stuff over probably finitely many pre-images we should be able to get what we want via a few applications of the change of variables theorem. However, I wasn't able to define this pull back for some reason. Either if this is or is not the strategy, why is this formula correct?","['measure-theory', 'ergodic-theory', 'dynamical-systems', 'differential-geometry']"
4758398,$A \subset S$ and $B \subset S$ are negatively correlated,"Let $s_1,...,s_m$ be i.i.d (independent and identically distributed) random variables, distributed on the set
{1,2, ..., n}
and consider the (random) set $S = \{s_1, ..., s_m\}$ . For any given disjoint subsets $A,B \subset {1,..., n}$ Prove that $\mathbb{P}(A \cup B \subset S) ≤ \mathbb{P}(A \subset S)\mathbb{P}(B \subset S)$ that is, the events ${A \subset S}$ and ${B \subset S}$ are negatively correlated. This looks like Harris's inequality (or the more general FKG inequality), but I couldn't find a way to apply it. We may assume $A = \{1,2,...,|A|\}, B = \{n-|B|+1, ..., n\}$ as they are disjoint.
Now, I tried applying the FKG to the lattice $\{1,...,n\}^m$ with usual lexicographical order, but it doesn't seem to work since the event ${A \subset S}$ is not monotonic. Do you have a suggestion, or perhaps a hint or a solution? I attached a link to the FKG inequality from Wikipedia: https://en.wikipedia.org/wiki/FKG_inequality Thank you in advance!","['combinatorics', 'probability-theory']"
4758414,Complex integration tricks for evaluating real integrals,"I was doing some homework and I stumbled upon this integral: $$
\int_{-\infty}^{\infty}dx \, \frac{\cos(x)-1+\frac{x^2}{2}}{x^4}
$$ My first thoughts were: at $\pm\infty$ it should converge, since the order of the denominator is bigger. expanding in Taylor's series of the argument at $0$ should be: $\frac{1}{4!}+O(x^6)$ . This means that the integral also converges near $0$ , and the function is continuous (so there is an analytic continuation in $0$ ). Im a little bit stuck because if the function is analytical its integral should be $0$ ?
But this is not the case in the solution.
Also, when adding two arcs (one at infinity and one around $0$ ) to close the loop, no other non-analytic points exists, so the residue theorem also says that the integral is $0$ .
What am I doing wrong? Thanks in advance for any help.","['integration', 'complex-analysis']"
4758436,"Redundancy with basis vectors for $T_pM^\mathbb C \cong T_pM^{1,0} \oplus T_pM^{0,1}$","I've found out that the primary motivation for complexifying the tangent bundle is to enable its decomposition into the eigenspaces $\pm i$ of $J$ . This decomposition facilitates the splitting of the bundle into holomorphic and antiholomorphic components. For the complexified tangent space $T_pM^\mathbb C$ , we have a complex basis given by $\{\partial/\partial x_1,\dots, \partial/\partial x_n, \partial/\partial y_1,\dots, \partial/\partial y_n\}$ . Given that $T_pM^\mathbb C \cong T_pM^{1,0} \oplus T_pM^{0,1}$ , the isomorphisms can be represented as: \begin{align*}
    T_pM &\to T_pM^{1,0}, \ v \mapsto \frac{1}{2}(v-iJv) \\
    T_pM &\to T_pM^{0,1}, \ v \mapsto \frac{1}{2}(v+iJv).
\end{align*} Next I wanted to show that under these isomorphisms we can map the basis $$\{\partial/\partial x_1,\dots, \partial/\partial x_n, \partial/\partial x_y,\dots, \partial/\partial y_n\}$$ to the basis $$\{\partial/\partial z_1,\dots, \partial/\partial z_n, \partial/\partial \bar{z}_1,\dots, \partial/\partial \bar{z}_n\}$$ but I get some redundancy. For the first map $T_pM \to T_pM^{1,0}, \ v \mapsto \frac{1}{2}(v-iJv)$ we have \begin{align*}
    \frac{\partial}{\partial x_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial x_j} - iJ\left(\frac{\partial}{\partial x_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial x_j} - i\frac{\partial}{\partial y_j}\right) = \frac{\partial}{\partial z_j} \\
    \frac{\partial}{\partial y_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial y_j} - iJ\left(\frac{\partial}{\partial y_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial y_j} + i\frac{\partial}{\partial x_j}\right)
\end{align*} and here $\frac{\partial}{\partial y_j}$ doesn't map to either $\frac{\partial}{\partial z_j}$ or $\frac{\partial}{\partial \bar{z}_j}$ . Similarly for the map $T_pM \to T_pM^{0,1}, \ v \mapsto \frac{1}{2}(v+iJv)$ we have \begin{align*}
    \frac{\partial}{\partial x_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial x_j} + iJ\left(\frac{\partial}{\partial x_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial x_j} + i\frac{\partial}{\partial y_j}\right) = \frac{\partial}{\partial \bar{z}_j} \\
    \frac{\partial}{\partial y_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial y_j} + iJ\left(\frac{\partial}{\partial y_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial y_j} - i\frac{\partial}{\partial x_j}\right)
\end{align*} but again $\frac{\partial}{\partial y_j}$ doesn't map to a corresponding basis element. Could anyone help me out why? Edit : Does the following work out properly with the differences regarding complex and real bases? Since $T_pM^\mathbb C \cong T_pM^{1,0} \oplus T_pM^{0,1}$ we can use the isomorphisms \begin{align*}
    T_pM &\to T_pM^{1,0}, \ v \mapsto \frac{1}{2}(v-iJv) \\
    T_pM &\to T_pM^{0,1}, \ v \mapsto \frac{1}{2}(v+iJv)
\end{align*} to send the real basis $\left\{ \frac{\partial}{\partial x_1}, \dots, \frac{\partial}{\partial x_n}, \frac{\partial}{\partial y_1}, \dots, \frac{\partial}{\partial y_n} \right\}$ of $T_pM$ to \begin{align*}
    \frac{\partial}{\partial x_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial x_j} - iJ\left(\frac{\partial}{\partial x_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial x_j} - i\frac{\partial}{\partial y_j}\right) = \frac{\partial}{\partial z_j} \\
    \frac{\partial}{\partial y_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial y_j} - iJ\left(\frac{\partial}{\partial y_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial y_j} + i\frac{\partial}{\partial x_j}\right) = i\frac{\partial}{\partial z_j}
\end{align*} giving $T_pM^{1,0}$ a real basis $\left\{ \frac{\partial}{\partial z_1}, \dots, \frac{\partial}{\partial z_n}, i\frac{\partial}{\partial z_1}, \dots, i\frac{\partial}{\partial z_n} \right\}$ and a complex basis $\left\{ \frac{\partial}{\partial z_1}, \dots, \frac{\partial}{\partial z_n}\right\}$ . In a similar manner we send the real basis of $T_pM$ to $T_pM^{0,1}$ by \begin{align*}
    \frac{\partial}{\partial x_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial x_j} + iJ\left(\frac{\partial}{\partial x_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial x_j} + i\frac{\partial}{\partial y_j}\right) = \frac{\partial}{\partial \bar{z}_j} \\
    \frac{\partial}{\partial y_j} &\mapsto \frac{1}{2}\left(\frac{\partial}{\partial y_j} + iJ\left(\frac{\partial}{\partial y_j}\right)\right) = \frac{1}{2}\left(\frac{\partial}{\partial y_j} - i\frac{\partial}{\partial x_j}\right) = -i\frac{\partial}{\partial \bar{z}_j}
\end{align*} which gives $T_pM^{0,1}$ a real basis $\left\{ \frac{\partial}{\partial z_1}, \dots, \frac{\partial}{\partial z_n}, -i\frac{\partial}{\partial \bar{z}_1}, \dots, -i\frac{\partial}{\partial \bar{z}_n} \right\}$ and a complex basis $\left\{ \frac{\partial}{\partial \bar{z}_1}, \dots, \frac{\partial}{\partial \bar{z}_n}\right\}$ .","['complex-manifolds', 'differential-geometry']"
4758475,Galois correspondence for non-necessary étale cover of normal integral schemes,"I am struggling at finding a proof for some fact I saw in a paper.
I call a variety any integral separated scheme of finite type over a field $k$ of char 0, and a cover of varieties is simply a finite surjective morphism (not necessarily etale).
For a cover of normal varieties $\pi: Y\to X$ , I know $Aut_X(Y)$ is isomorphic to $Aut_{k(X)}(k(Y))^{op}$ and then we call $\pi$ Galois if $\#Aut_X(Y)=deg(\pi)$ or equivalently if $k(Y)/k(X)$ is Galois. This is the same definition I found in Milne's notes on Etale cohomology for ""generically Galois"". The paper now tells that for a Galois cover between normal varieties $\pi:Y\to X$ , we have a bijection between subgroups of $Aut_{k(X)}(k(Y))$ and subcovers $Y\to Z \to X$ give by $$H\mapsto [Y\to Y/H\to X] $$ where $Y/H$ is the normalization of $X$ in $k(Y)^H$ . However there is no clear proof of this fact, the paper only mentioning that it is true by Galois theory on fields and a version of Zariski's main theorem for normal varieties si I tried to show it myself. From a subcover $Y\to Z \to X$ , I denote $H$ the subgroup of $Aut_{k(X)}(k(Y))$ such that $k(Z)=k(Y)^H$ . Then I basically want to show $Z$ and $Y/H$ are isomorphic. I know they are birational since they have the same function field so let $f:Z\to Y/H$ be a birational map. I assume the version of Zariski's main theorem the paper is considering is similar to this statement in Liu's book. I don't understand neither why $f$ should be quasi-finite nor why having $f$ being an open immersion concludes $f$ is an isomorphism. Another attempt of myself is taking $U$ an affine open subset of $X$ and then checking $V1,V2,W$ its inverse images in $Z,Y/H$ and $Y$ .
That should give me extensions of rings and then by normality, I assume $B_1$ and $B_2$ should both be the integral closure of $A$ in $k(Y)^H=k(Z)\subset k(Y)$ so I could conclude $Z=Y/H$ . The thing is I do not know how to generalize to non-affine schemes and I don't think Zariski's main theorem is involved in this way. Thanks for your attention, any hint accepted!","['galois-theory', 'algebraic-geometry', 'schemes']"
4758486,Counterexample to Egorov for functions valued in non-separable metric space,"A general form of Egorov (e.g. https://www.ime.usp.br/~glaucio/mat6704/textos/GMTLecureNotes.pdf ) states that: Egorov Theorem : Let $\mu$ be an outer measure on the set $X$ and $(Y,d)$ a separable metric space. Consider $A\subseteq X$ with $\mu(A)<\infty$ and a sequence of measurable functions $f_n:X\to Y$ in the sense that $f_n^{-1}(U)$ is $\mu$ -measurable in $X$ for every open $U$ in $Y$ . If $\{f_n\}$ converges pointwise $\mu$ -a.e. on $A$ to a $\mu$ -measurable function $f:\mathrm{dom}(f)\subseteq X\to Y$ , then $\{f_n\}$ converges almost uniformly to $f$ on $A$ in the sense that for any $\epsilon>0$ , there is a $\mu$ -measurable set $B$ such that $\mu(A-B)<\epsilon$ and $\{f_n\}$ converges uniformly to $f$ on $B$ . Here, a set $A\subseteq X$ is called $\mu$ -measurable if it satisfies the Carathéodory condition: $\mu(T)=\mu(T\cap A) + \mu(T-A)$ for any $T\subseteq X$ . My question : is there a counterexample to Egorov Theorem without separability? That is, is there a sequence of $\mu$ -a.e. convergent functions valued in a non-separable metric space $Y$ that does not converge almost uniformly? The proof of Egorov Theorem goes the usual way by (I) defining $$C_{i,j}:= \bigcup_{n\geq j} \{x\in X: d(f_n(x),f(x))>1/i\},$$ (II) using $\mu$ -a.e. convergence to show that for each $i$ the intersection $\bigcap_{j} C_{i,j}$ has zero measure, and then (III) using $\mu(A)<\infty$ and continuity of measure to choose $J(i)$ so large for each $i$ that the exceptional set $\bigcup_i C_{i,J(i)}$ has small measure. Separability condition on $Y$ is used only in step (I) to show that the sets $C_{i,j}$ are $\mu$ -measurable. Indeed, a separable metric space $(Y,d)$ has a countable basis for its metric topology so that the Borel $\sigma$ -algebra $\mathcal{B}(Y\times Y)$ (generated by open sets in product topology on $Y\times Y$ ) coincides with the product $\sigma$ -algebra $\mathcal{B}(Y)\otimes \mathcal{B}(Y)$ . Then, the pre-image of open set $(1/i,\infty)$ in $\mathbb{R}$ under continuous map $(x,y)\mapsto d(x,y)$ is open and measurable wrt product $\sigma$ -algebra $\mathcal{B}(Y)\otimes \mathcal{B}(Y)$ , so that its pre-image $C_{i,j}$ under measurable map $x\mapsto (f_n(x),f(x))$ is measurable. I have found a counterexample to this technique , if $(Y,d)$ is not separable. For this, consider $Y$ a set with cardinality strictly larger than continuum, and $d$ the discrete metric. Then it can be shown that the diagonal $\Delta \in \mathcal{B}(Y\times Y)- \mathcal{B}(Y)\otimes \mathcal{B}(Y)$ ; this is known as the Nedoma's paradox cf. https://www.drmaciver.com/2006/04/journal-of-obscure-results-1-nedomas-pathology/ . Now put $X=Y\times Y$ and $f,g:Y\times Y\to Y$ projections onto the first and second coordinates respectively. By construction, $f,g$ are measurable, but the set $\Delta=\{(x,y)\in Y\times Y: d(f(x,y),g(x,y))<1\}$ is not measurable. It seems more difficult to construct a counterexample to the Egorov statement... Any help is appreciated :)","['measure-theory', 'examples-counterexamples', 'measurable-sets', 'measurable-functions', 'uniform-convergence']"
4758504,"Does locally compact, locally connected, connected metrizable space admit a metric with connected balls?","If $X$ is a locally compact, locally connected, connected metrizable space, does that imply that there must be a metric $d$ on $X$ such that $B(x, r) = \{y\in X : d(x, y) < r\}$ is connected for all $x\in X, r > 0$ ? Note that if $X$ admits such metric then it's necessarily locally connected and connected. If $X$ has at least two points, then we can always find a metric on $X$ with a disconnected open ball by choosing non-empty disjoint open $U, V\subseteq X$ , $x\in U\cup V$ and a metric $d$ on $X$ such that $U\cup V$ is open unit ball around $x$ . See here . Edit: Added the assumption of local compactness.","['connectedness', 'compactness', 'metric-spaces', 'general-topology', 'locally-connected']"
4758505,Complicated change of variables formula with CDF of normal distribution instead of diffeomorphism,"I have a parameter $\theta\in[0, 10]^4$ and a variable $z\in\mathbb{R}^m$ . Consider the following integral $$
I(\theta, z) := \int F(\theta, z) \mathbb{I}(\|f(\theta, z)\| \leq 1) p(\theta, z) dz d\theta
$$ Is it possible to rewrite it using the variables $(\vartheta, z)$ where $\theta = G(\vartheta)$ and $$
G(\vartheta) = 10\cdot (\Phi(\vartheta_1), \Phi(\vartheta_2), \Phi(\vartheta_3), \Phi(\vartheta_4))
$$ with $\Phi$ being the CDF of a standard normal distribution. Attempt Informally, one could go ahead and simply replace $\theta$ with $G(\vartheta)$ and multiply the integrand by $|J_G(\vartheta)|$ , the absolute determinant of the Jacobian of $G$ $$
I(\theta, z) = \int F(G(\vartheta), z) \mathbb{I}(\|f(G(\vartheta), z)\| \leq 1) p(G(\vartheta), z) |J_G(\vartheta)| dz d\vartheta =: I(\vartheta, z)
$$ However, this only works informally. The substitution I have just done is not correct according to the Change of Variables formula (e.g. see Billingsley). This is because the CDF of a normal distribution is not a diffeomorphism: it has no closed-form inverse.","['integration', 'calculus', 'change-of-variable', 'measure-theory']"
4758508,Visualizing the trig identity $1+\tan A\tan\frac12A = \sec A$,"Can someone help me visualize this trigonometry Identity Prove that : $$1+\tan A\tan \frac12A = \sec A$$ I got the answer by manipulating the lhs substitution lots of mundane stuff but without a pen and paper and say a magical ability to never be able to visualize text in my mind, I wouldn't have been able to visualize this geometrically and understand how $\sec A$ is achieved. Any suggestions ?","['trigonometry', 'visualization', 'intuition']"
4758516,Show that a nonlocal operator with symmetric kernel is well defined,"Assume that $J:\mathbb{R}^N\backslash \{0\}  \to (0, \infty)$ is a symmetric function; that is, $J(x)=J(-x)$ for any $x \in\mathbb{R}^N$ . Moreover, we assume that
there is a constant $J_0 > 0$ and $0 < s < 1$ such that \begin{equation}\label{CondJ1}  
J_0 \leqslant J(x)|x|^{N + 2s},
\end{equation} almost everywere in $\mathbb{R}^N$ .
We also assume that \begin{equation}\label{CondJ2}  
Jm \in L^1(\mathbb{R}^N),
\end{equation} where $m(x) = \min\{1, |x|^2\}$ .
Now, consider the nonlocal operador $$\mathcal{L}_Ju(x)= 2\int_{\mathbb{R}^N} J(x - y)(u(x)-u(y))dy.$$ I need to show that $\mathcal{L}_J u \in L^2(\mathbb{R}^N)$ if $u \in C^{\infty}_0(\mathbb{R}^N)$ .This indeed should happen, as mentioned on page 4 of the paper: https://arxiv.org/pdf/1612.05696.pdf . Attempt: If $u \in C^{\infty}_0(\mathbb{R}^N)$ , then $|u(x) - u(y)| \leqslant C|x - y|$ , for some $C > 0$ and $|u(x) - u(y)| \leqslant 2\|u\|_{L^{\infty}(\Omega)}$ . Thus $$|u(x) - u(y)| \leqslant K\sqrt{m(x - y)},$$ for some $K > 0$ . Consequently \begin{split}
    \int_{\mathbb{R}^N}\Big|2\int_{\mathbb{R}^N}J(x - y)(u(x) - u(y))dy\Big|^2dx &\leqslant 4K^2\int_{\mathbb{R}^N}\Big|\int_{\mathbb{R}^N}J(x - y)\sqrt{m(x - y)}dy\Big|^2dx.\\
\end{split} But I only know that $Jm \in L^1(\mathbb{R}^N)$ .","['multivariable-calculus', 'smooth-functions', 'functional-analysis', 'fractional-sobolev-spaces']"
4758530,Max and min of a three variable function over a set,"I need help for the following exercise, because I am not sure of myself.
Consider the function $$f(x, y, z) = -(xy + zy)^2 + x - z$$ and the set $$A = \{ (x, y, z) \in \mathbb{R}^3; xy + zy + x - z = 0 \}$$ Does $g(t) = f(e^t, 1, e^t)$ have absolute max/min as $t$ changes in $\mathbb{R}$ ? Does $f(x, y, z)$ have absolute max/min over $A$ ? If yes, find it/them. My Attempts So, for what concerns the first request, I just calculated $$g(t) = -4e^{2t}$$ And this is a monotone decreasing function over $\mathbb{R}$ hence it has no max/min (absolute or relative). For what concerns the second request, I'm a bit perplexed. First of all, I don't know how to show if $A$ is bounded and or closed (not asked, but I always want to do this part). So I have no clue if I could use Weierstrass theorem, for not knowing if $A$ is compact cannot make me to proceed. But alas, in spite of that, I went on. I wrote the set $A$ in function of $z$ as $z = \frac{x(1+y)}{1-y}$ and I thence wrote $f$ restricted to that $$f\bigg|_z = -\frac{2xy}{1-y}\left(1 + \frac{2xy}{1-y}\right)$$ At this point I calculated the gradient (tedious): $$\nabla f = \left(-\frac{4 x y^2}{(1-y)^2}-\frac{2 y \left(\frac{2 x y}{1-y}+1\right)}{1-y},-\frac{2 x \left(\frac{2 x y}{1-y}+1\right)}{1-y}-\frac{2 x y \left(\frac{2 x y}{1-y}+1\right)}{(1-y)^2}-\frac{2 x y \left(\frac{2 x y}{(1-y)^2}+\frac{2 x}{1-y}\right)}{1-y}\right)$$ Now, I found only two solutions: $x = y = 0$ and $y = \frac{1}{1-4x}$ (with $x \neq 0$ ). At this point I don't know if I should proceed with the Hessian study, or not.
Maybe there is a more conveniente way to attack this problem. I'm just puzzled about the set $A$ , I cannot figure it out in terms of what it does represent. Thank you so much!","['analysis', 'real-analysis', 'maxima-minima', 'multivariable-calculus', 'optimization']"
4758531,FTC II Type Question,"Suppose we define the function G(x) = $\int_{0}^{x^2} [y^2 \int_{0}^{y} f(t)dt] dy$ , and let's name $\phi(y) = y^2 \int_{0}^{y} f(t)dt$ . Then G'(x) = $\phi(x^2)*2x = 2x^5 \int_{0}^{x^2} f(t)dt$ . Going further, $\phi ' (x^2) = \frac{d}{dx} \left( x^4 \int_{0}^{x^2} f(t)dt \right) = 4x^3 \cdot \phi (x^2) + x^4\cdot f(x^2) \cdot 2x$ . Thus $G''(x) = \phi ' (x^2) \cdot 2x + 2 \phi (x^2) = 2x ( 4x^3 \cdot \phi (x^2) + 2x^5 \cdot f(x^2) ) + 2\phi (x^2).$ Is my math correct here? Did I apply the FTCII correctly?","['calculus', 'solution-verification', 'derivatives', 'chain-rule']"
4758535,"What does ""the converse"" mean in this theorem about convergent series in metrizable spaces?","In Munkres' Topology, Theorem 21.3 says Let $f:X \to Y$ . If the function, $f$ is continuous, then for every convergent series $x_n \to x$ in $X$ , the sequence $f(x_n)$ converges to $f(x)$ . The converse holds if $X$ is metrizable. I don't understand what they mean by ""the converse"" here. It certainly can't mean ""if $f(x_n)$ converges to $f(x)$ then $x_n \to x$ "" since this is patently false (e.g. if $f$ is constant or periodic). I think what it means is something like: Let $y_n \to y$ where the $y_n$ and $y$ are in the image of $f$ . Then, there exists points $x_n$ and $x$ in $X$ such that $x_n \to x$ , $f(x_n) = y_n$ , and $f(y) = y$ . If this is right, I don't understand the proof though. The book proves that $f(\overline{A}) \subset \overline{f(A)}$ for $A \subset X$ but I don't see how that implies my statement above.","['proof-explanation', 'general-topology']"
4758543,Function Vector Space and operations defined on it,"How is $(af)(x)$ different from $a(f(x))$ , or how is $(f+g)(x)$ different from $f(x)+g(x)$ ? Intuitively, I understand that those relations need to be equal (because of the definition of vector sum and vector scaling), but I can't visually imagine with the functions from Calculus I. Ex: if I consider $f(x)=ax^2$ and $g(x)=bx^2$ , then $f(x)+g(x)=ax^2 + bx^2$ . If I consider $(f+g)(x)$ then I can't intuitively think of anything different from $ax^2 + bx^2$ . Should I think of the latter as $(a+b)x^2$ ? In this case how would you think of other functions like exp or ln: $f(x)=e^x$ , $g(x)=ax^2$ ; in this last example I ultimately can't think of anything different from $e^x + ax^2$ . Thx in advance","['functions', 'vector-spaces']"
4758563,Is finding the area of this rectangle impossible?,"One of my students gave this problem and I am feeling quite ashamed that I could not find an answer. It asks for the area of the pink rectangle and it says that the triangle ABC is a right angle triangle. I have tried using triangle similarity to find a system of equations, but I could not find a solution. I would venture that it has infinite solutions, given the informations. But I fell that I am missing something really obvious. The only assumptions are: AB = 12
EC = 7
ABC is a right angled triangle
The pink shape is a rectangle","['euclidean-geometry', 'triangles', 'geometry']"
4758597,Find the number of $m$-digit numbers whose sum of digits is $12$,"The question is straight forward. Here is my attempt but I am not sure if it is correct. $$x_1+x_2+...+x_m=12 \; \ldots(1), \quad x_1\ge1, \quad 0\le x_i \le9$$ Number of total solutions is $\binom{m+10}{11}$ $A_i$ - set of numbers where $x_i\ge 10$ ; $|A_i|=\binom{m+1}{2}$ Final result: $\binom{m+10}{11} - m\binom{m+1}{2}$ So to sum it all up, I calculated number of total solution of the equation $(1)$ , and subtracted the number of solutions if $x_i\ge10$ . Edit: I have taken into account the $x_1\ge1$ term. This is what I get now, but I'm still missing something","['combinatorics', 'discrete-mathematics']"
4758606,"$\sum_{n=1}^\infty 2^{-n} f_n$ has unbounded variation on every $[a,b] \subset [0,1]$ with $a < b$","Consider $n\in \Bbb N$ , and define a piecewise linear function $f_n: [0,1] \to \Bbb R$ so that the graph of $f_n$ contains the points $\{(k \cdot 2^{-n}, (-1)^k)\}_{0\le k\le 2^n}$ . Define $f: =\sum_{n=1}^\infty 2^{-n} f_n$ . Since $|f_n| \le 1$ and $\sum_{n\ge 1} 2^{-n} < \infty$ , $f$ is continuous. How would you show that $f$ has unbounded variation on every closed subinterval $[a,b]$ of $[0,1]$ with $a < b$ ? The points $\{k \cdot 2^{-n}\}_{0\le k\le 2^n}$ form a partition of $[0,1]$ for every $n\ge 1$ , so it seems a natural choice for a partition of $[a,b]$ to consider would consist of $\{a,b\}$ and points from $\{k \cdot 2^{-n}\}_{0\le k\le 2^n}$ that fall in the interval $[a,b]$ . P.S. This is what $f_2$ looks like: The plots of $\{f_n\}$ will just have more ""oscillations"" as $n$ increases. Edit: The current answer looks good. However, is there a way to avoid using differentiability a.e. of functions of bounded variation - perhaps directly computing the variation?","['bounded-variation', 'analysis', 'real-analysis']"
4758612,"Prove that if $A,B\subseteq\mathbb R$ are non-empty and bounded from above, and $A+B=A$, it follows that $B=\{0\}$. Seems Wrong?","A question from a basic textbook on real analysis: Let $A,B\subseteq\mathbb R$ be non-empty and bounded from above. Prove that if $A+B = \{a+b : a\in A,\,b\in B\} =A$ , it follows that $B = \{0\}$ . That seems wrong to me. For example, consider A = B = $\mathbb R_{\le0}$ . $A+B$ is therefore $\mathbb R_{\le0}$ as well. Am I missing something here?",['real-analysis']
4758663,Tricky exponential elliptical integral,"I've run into this tricky exponential integral while trying to calculate the steady state heat distribution of a moving toroidal heat source. A simple form of it is: $$\int_0^{2π} \frac{e^{-\sqrt{1-a\cos \theta}}}{\sqrt{1-a\cos \theta}}\, d\theta$$ Where $|a| < 1$ I've thrown many tricks at it, but each seems to balloon in complexity. Any ideas? Maybe a Fourier transform can help, considering the offset axisymmetry that created this problem. I have seen some semi-related literature involving toroidal expansions. This integral emerged from attempting to revolve a point source field of the form $e^{-r}/r$ into a ring.","['integration', 'calculus', 'definite-integrals', 'trigonometry']"
4758677,An atypical special harmonic series,"The following atypical harmonic series was recently proposed by C.I. Valean , $$ \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{n^2}$$ $$=\frac{13}{12}\log^4(2)-\frac{35}{4}\log^2(2)\zeta(2)+\frac{91}{4}\log(2)\zeta(3)-\frac{665}{16}\zeta(4)+6 \log(2)\pi G -8 G^2$$ $$+12 \pi \Im\biggr\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr\}+26 \operatorname{Li}_4\left(\frac{1}{2}\right),$$ where $\zeta$ is the Riemann zeta function, $G$ represents the Catalan's constant, and $\operatorname{Li}_n$ denotes the Polylogarithm. His solution starts with using the well-known property with harmonic numbers and skew-harmonic numbers, that is $\overline{H}_{2n}=H_{2n}-H_n$ , or $H_n=H_{2n}-\overline{H}_{2n}$ , which leads to \begin{equation*}
 \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{n^2}=\sum_{n=1}^{\infty}\frac{(H_{2n}-\overline{H}_{2n}) H_{4n}}{n^2}=4\sum_{n=1}^{\infty} \frac{H_{2n} H_{4n}}{(2n)^2}-4\sum_{n=1}^{\infty} \frac{\overline{H}_{2n} H_{4n}}{(2n)^2}
\end{equation*} \begin{equation*}
=2\sum_{n=1}^{\infty} \frac{H_n H_{2n}}{n^2}-2\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_n H_{2n}}{n^2}-2\sum_{n=1}^{\infty} \frac{\overline{H}_n H_{2n}}{n^2}+2\sum_{n=1}^{\infty} (-1)^{n-1}\frac{\overline{H}_n H_{2n}}{n^2},
\end{equation*} where the series $$\sum _{n=1}^{\infty} \frac{\overline{H}_nH_{2n}}{n^2}=\frac{507}{64}\zeta(4)-\frac{7}{4}\log(2)\zeta(3)+\frac{5}{4}\log^2(2)\zeta(2)-\frac{7}{48}\log^4(2)-2\pi \Im\biggr \{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr \}-\frac{7}{2}\operatorname{Li}_4\left(\frac{1}{2}\right)$$ and $$ \sum_{n=1}^{\infty} (-1)^{n-1}\frac{\overline{H}_n H_{2n}}{n^2}=\frac{5}{48}\log^4(2)-\frac{5}{8}\log^2(2)\zeta(2)+\frac{7}{2}\log(2)\zeta(3)-\frac{77}{32}\zeta(4)+\log(2)\pi G-2 G^2+\frac{5}{2}\operatorname{Li}_4\left(\frac{1}{2}\right)$$ are found in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023) , Chapter $4$ , Section $4.57$ , p. $454$ , then $$ \sum_{n=1}^{\infty}\frac{H_n H_{2n}}{n^2}=\frac{13}{8}\zeta(4)+\frac{7}{2}\log(2)\zeta(3)-\log^2(2)\zeta(2)+\frac{1}{6}\log^4(2)+4 \operatorname{Li}_4\left(\frac{1}{2}\right)$$ is given in the same book, Chapter $4$ , Section $4.23$ , p. $428$ , and finally $$ \sum _{n=1}^{\infty} (-1)^{n-1}  \frac{H_n H_{2n}}{n^2}=2G^2-2\log(2)\pi G-\frac{1}{8}\log^4(2)-\frac{21}{8}\log(2)\zeta(3)+\frac{3}{2}\log^2(2)\zeta(2)+\frac{773}{64}\zeta(4)-4\pi \Im\biggr \{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr \}-3\operatorname{Li}_4\left(\frac{1}{2}\right),$$ presented in the same reference, Chapter $4$ , Section $4.55$ , p. $453$ , thus giving the desired result. Questions: $1)$ . Are such results known in the literature, or similar ones like, say, $\displaystyle \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{(2n-1)^2}$ ? $2).$ I would love to see very different ideas, and strategies leading to the desired result, and at the same time remaining in the realm of simple real methods. So, what other ways would we like to explore? (there is absolutely no hurry)","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'sequences-and-series']"
4758709,"Sum of even binomial coefficients modulo $p$, without complex numbers","Let $p$ be a prime where $-1$ is not a quadratic residue, (no solutions to $m^2 = -1$ in $p$ ). I want to find an easily computable expression for $$\sum_{k=0}^n {n \choose 2k} (-x)^k$$ modulo $p$ . With one caveat, all the computations must be done over the integers modulo $p$ , no complex numbers. That is, I'm ruling out the standard trick where you take $(1+xi)^n + (1-xi)^n$ to extract even terms. I'm looking for any other alternate expressions for this sum. The final expression should not have a sum over $n$ terms and not involve any complex numbers implicitly or explicitly. For example, Chebyshev functions are ruled out, as they implicitly involve a complex power. Other than that any interesting expressions for the sum over the integers or the integers modulo $p$ are allowed.
Can anyone do it? $\textbf{Rephrase}$ Any way of writing the real part of $(1+xi)^n$ modulo $p$ , as $f(x,n)$ for some $f$ that only involves real numbers in the computation and no sum over $n$ . For example, $f(x,n)$ can not be a polynomial in $x^n$ because $f(x,n)$ would be periodic modulo $p-1$ in $n$ , while $(1+xi)^n$ would be periodic modulo $p^2-1$ (and not $p-1$ ). So the function $f$ would have to be quite intricate.","['number-theory', 'binomial-coefficients', 'combinatorics', 'modular-arithmetic']"
4758711,How much geometry should I know for the AMC 12,"I have been reading ""Geometry Revisited"" recently in prep for the amc 12, and just out of my own curiosity. It may seem very ambitious, but I'm hoping to at least get into AIME and possibly into USAMO. The last 3 chapters of ""Geometry Revisited"" cover more advanced transformations, inversive geometry, and projective geometry, and I wasn't sure if those were relevant to the AMC 12. ""Introduction to Geometry"" seems to be a lot more advanced, so I haven't read it yet, but I was wondering if there was any specific part or chapter of it I should read. Sorry in advanced if I did anything wrong in my post, as this is my first time using Math Stack Exchange","['contest-math', 'euclidean-geometry', 'geometry']"
4758741,Find a subset of a collection of vectors that sums to $0$,"Consider following practice problem on a handout from a previous year's Putnam seminar at MIT ( https://ocw.mit.edu/courses/18-a34-mathematical-problem-solving-putnam-seminar-fall-2018/ ). (Problem 100). Let $M_0$ be the $2^n \times n$ matrix whose rows consist of all $2^n$ distinct vectors of $\pm 1$ ’s (in some order) of length $n$ . Suppose that an arbitrary subset of the entries is changed to $0$ ’s. Show that some nonempty subset of the rows of the resulting matrix $M$ sums to the vector $[0, 0, . . . , 0]$ . Here are some observations I have made so far. Bounding the number of zeros . Suppose for a contradiction that there is some resulting matrix $M$ that has no nonempty subset of rows that sums to the zero vector. In the original matrix $M_0$ , pair each row with its negation, yielding $2^{n-1}$ pairs. For each associated pair of the matrix $M$ , there exists some column $j$ such that exactly one vector in the pair has $j$ -th entry equal to $0$ ; otherwise that pair would sum to zero. It follows that at least $2^{n-1}$ of the entries of $M$ are zero, and at least $2^{n-1}$ of the entries of $M$ are nonzero. Linear algebra . In vector notation, we want to show that there exists a vector $x \in \{0,1\}^{2^n}$ such that $x^T M = 0$ . The matrix $M$ has rank at most $n$ , so the set of all real $2^n$ -vectors $x$ satisfying this equation is a linear subspace with dimension at least $2^n - n$ . Perhaps we could control the coefficients of $x$ by considering an augmented equation $x^T [ M ~M'] = 0$ for some $2^n \times p$ matrix $M'$ that enforces relationships among the coefficients of $x$ . Alternatively, if we reduce everything modulo 2, then the set of vectors $x \in \mathbb{Z}_2$ satisfying $x^T M  = 0 \operatorname{mod} 2$ is also a linear subspace of dimension at least $2^n - n$ . In particular, it follows that there exists a nonempty subset of the rows of $M$ that sums to a vector in $(2 \mathbb{Z})^n$ . Parity . Calculations for small examples suggest that there is an odd number of zero-sum row subsets. This is clearly true in the extreme cases of $M = M_0$ and $M = 0$ . It would be sufficient to show that each time an entry is changed to $0$ , the parity of the number of nonempty zero-sum subsets is unchanged. In order to count these zero-sum subsets, it is natural to consider the (formal) function $$
f(z_1, \ldots, z_n) = \prod_{i =1}^{2^n} \bigl( 1 + z_1^{m_{i1}} \cdots z_n^{m_{in}} \bigr) = 1 + \sum_{A} \prod_{j=1}^{n} z_j^{ \sum_{i \in A} m_{ij} },
$$ where the sum is taken over all nonempty subsets $A$ of $\{1,\ldots, 2^n\}$ . Induction. Write each row $i$ of $M$ as $r_i = (r_i^1, r_i^{-1})$ where $r_i^{-1}$ is an $(n-1)$ -vector. The result follows by induction if we can find disjoint nonempty subsets $I_1, \ldots, I_{2^{n-1}}$ of the rows of the matrix (many of which may be singletons) such that (a) $\sum_{i \in I_k} r_i^1 = 0$ for each $k$ ; and (b) the $2^{n-1} \times (n-1)$ matrix with rows $R_k = \sum_{i \in I_k} r_i^{-1}$ for $k = 1, \ldots, 2^{n-1}$ takes the right form to apply the inductive hypothesis. Any ideas would be appreciated!","['contest-math', 'combinatorics']"
4758769,"Any other methods for evaluating $\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx$","I evaluated the following integral: $$I:=\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx$$ I would like to see any alternate solutions, here is my work. Using: $$\frac{\arctan(x)}{x}\equiv\int_{0}^{1}\frac{1}{1+x^2y^2}\,dy$$ It is trivial to show that: $$\frac{\arctan(\cos(x))}{\cos(x)}\equiv\int_{0}^{1}\frac{\sec^2(x)}{\tan^2(x)+1+y^2}\,dy$$ Therefore: $$I=\int_{0}^{\frac{\pi}{4}}\int_{0}^{1}\frac{\sec^2(x)}{\tan^2(x)+1+y^2}\,dy\,dx$$ Let $$\tan(x)\longrightarrow{x}$$ $$I=\int\int_{[0,1]^2}\frac{1}{1+x^2+y^2}\,dy\,dx$$ $$=\int_{0}^{\frac{\pi}{4}}\int_{0}^{\sec(\theta)}\frac{r}{1+r^2}\,dr\,d\theta+\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\int_{0}^{\csc(\theta)}\frac{r}{1+r^2}\,dr\,d\theta$$ $$=\frac{1}{2}\left[\int_{0}^{\frac{\pi}{4}}\log\left(2+\tan^2(\theta)\right)\,d\theta+\int_{\frac{\pi}{4}}^{\frac{\pi}{2}}\log\left(2+\cot^2(\theta)\right)\,d\theta\right]$$ For the rightmost integral let $$\theta\longrightarrow{\frac{\pi}{2}-\theta}$$ To get: $$I=\int_{0}^{\frac{\pi}{4}}\log\left(2+\tan^2(\theta)\right)\,d\theta$$ Now let $$\tan(\theta)=x$$ Which yields: $$I=\int_{0}^{1}\frac{\log\left(2+x^2\right)}{1+x^2}\,dx$$ Define $I(t)\forall t\in[0,1]$ to be: $$I(t):=\int_{0}^{1}\frac{\log\left(1+t(1+x^2)\right)}{1+x^2}\,dx$$ We can see that: $$I(0)=0$$ $$I(1)=I$$ And so it follows from the F.T.C. That: $$I=\int_{0}^{1}I’(t)\,dt$$ And so to follows that: $$I’(t)=\int_{0}^{1}\frac{1}{tx^2+1+t}\,dx$$ $$=\frac{\arctan\left(\sqrt{\frac{t}{1+t}}\right)}{\sqrt{t(1+t)}}$$ And so we have that: $$I=\int_{0}^{1}\frac{\arctan\left(\sqrt{\frac{t}{1+t}}\right)}{\sqrt{t(1+t)}}$$ Let $$\sqrt{\frac{t}{1+t}}=x$$ To get: $$I=2\int_{0}^{\frac{1}{\sqrt{2}}}\frac{\arctan(x)}{1-x^2}\,dx$$ Let $$x\longrightarrow{\frac{1-x}{1+x}}$$ $$I=\int_{3-2\sqrt{2}}^{1}\frac{\frac{\pi}{4}-\arctan(x)}{x}\,dx$$ $$=\frac{\pi}{4}\int_{3-2\sqrt{2}}^{1}\frac{1}{x}\,dx+\int_{0}^{3-2\sqrt{2}}\frac{\arctan(x)}{x}\,dx-\int_{0}^{1}\frac{\arctan(x)}{x}\,dx$$ $$=\frac{\pi}{2}\log\left(1+\sqrt{2}\right)+Ti_{2}\left(3-2\sqrt{2}\right)-G$$ And so we have the result: $$\int_{0}^{\frac{\pi}{4}}\frac{\arctan(\cos(x))}{\cos(x)}\,dx=\frac{\pi}{2}\log\left(1+\sqrt{2}\right)+Ti_{2}\left(3-2\sqrt{2}\right)-G$$ Where: $$G:=\sum_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)^2}$$ Is Catalan’s constant. And $$Ti_{2}(x):=\int_{0}^{x}\frac{\arctan(t)}{t}\,dt$$ Is the inverse tangent integral function. As I mentioned in the beginning of my post. Is there any other unique methods for the evaluation of this integral?","['integration', 'definite-integrals', 'special-functions', 'real-analysis', 'alternative-proof']"
4758772,Compute $\prod^{100}_{n=1} \left(5-4 \cos \left(\frac{\pi (2n-1)}{100} \right) \right)$.,"Question: Compute the value of the product $$\prod^{100}_{n=1} \left(5-4 \cos \left(\frac{\pi (2n-1)}{100} \right) \right)$$ I began by considering $$a_k=\prod^{k}_{n=1} \left(5-4 \cos \left(\frac{\pi (2n-1)}{k} \right) \right)$$ for smaller values of $k$ . $a_1=3$ $a_2=25$ $a_3=81$ $a_4=289$ $a_5=1089$ This led me to conjecture that $a_k=(2^k+1)^2$ . However, I am unsure how to go about proving this. I first began by rewriting $$a_k=4^k \cdot \prod^{k}_{n=1} \left(\frac{5}{4}-\cos \left(\frac{\pi (2n-1)}{k} \right) \right)$$ I then thought of interpreting this as $P(5/4)$ , where $$P(x)=\prod^{k}_{n=1} \left(x-\cos \left(\frac{\pi (2n-1)}{k} \right) \right)$$ Initially, I thought this looked similar to De Moivre's theorem, since $P(x)$ is equal to the product of $\Re \left(x-e^{\frac{i\pi(2n-1)}{k}} \right)$ . However, when multiplying imaginary terms by each other, the result is real, and they don't seem to cancel nicely.","['contest-math', 'algebra-precalculus', 'trigonometry']"
4758831,"Asymptotic distribution of the time until collision of Brownian motions started at $-a$, $0$, and $a$","Suppose I have three independent Brownian motions started at positions $-a$ , $0$ , and $a$ with $a>0$ . What is the asymptotic distribution of the time $\tau$ of the first collision between any pair of these Brownian motions? In particular, what is the suppression in time: can $\mathbb{P}(\tau > t)$ be upper-bounded by a decaying exponential in $t$ , like $\mathbb{P}(\tau > t) \leq c_1 e^{-c_2 \frac{t}{a^2}}$ for an appropriate pair of constants $c_1, c_2 >0$ ? Here are my thoughts. First, the first collision will necessarily involve the Brownian motion started at $0$ and either of the motions started at $a$ or $-a$ . As a first approximation, consider the case where the walkers at $a$ and $-a$ are stationary. The collision time then corresponds to the distribution of the first exit time of a Brownian motion, which obeys $\mathbb{P}(\tau_{\rm exit} >t ) \leq c_1 e^{-c_2 \frac{t}{a^2}}$ for an appropriate pair of constants $c_1$ and $c_2$ , as noted in an answer by Chris Jangjigian to another question. I suspect that the suppression in time in our original question at the top is actually weaker (has a heavier tail) so that the answer to my question is no. My reason for this is that there will be rare events where the Brownian motion started at $-a$ drifts even more negative while the Brownian motion started at $a$ drifts even more positive for an extended period of time. Though these trajectories may be rare, I suspect they are sufficient to change the tail properties of $\mathbb{P}(\tau > t)$ at large $t$ . My motivation for this question came from a comment from kodlu on another question . There, I considered the case of simple symmetric random walks on the integer line started at all integer multiples of $a$ , and I was curious about the tail bounds of the first collision time of the walk started at $0$ with any other walk. While the questions are similar, I actually suspect the answers will be different, and that the case with three walkers may have heavier tails than the case with an infinite number of walkers. My rough reason is that it is harder to imagine the kinds of rare events that could enhance the tail probability in the case of an infinite number of walkers.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'asymptotics']"
4758836,"If $\dfrac{a}{b}=\dfrac{c}{d}$, show that $\dfrac{a^2-c^2}{b^2-d^2}=\dfrac{(a+2c)(a+3c)}{(b+2d)(b+3d)}$ [duplicate]","This question already has answers here : A property of proportions: if $a/b=c/d$, then $(ma+nb)/(pa+qb)$ is equal to $ (mc+nd)/(pc+qd)$ (2 answers) Closed 9 months ago . If $\dfrac{a}{b}=\dfrac{c}{d}$ , show that $\dfrac{a^2-c^2}{b^2-d^2}=\dfrac{(a+2c)(a+3c)}{(b+2d)(b+3d)}$ $\dfrac{a}{b}=\dfrac{c}{d}=k$ $\Rightarrow a=bk, c=dk$ $\dfrac{(bk)^2-(dk)^2}{b^2-d^2}=\dfrac{(bk+2dk)(bk+3dk)}{(b+2d)(b+3d)}$ $[(bk)^2-(dk)^2][(b+2d)(b+3d)]=(b^2-d^2)[(bk+2dk)(bk+3dk)]$ $k^2(b^2-d^2)(b^2+5bd+6d^2)=(b^2-d^2)(k^2)(b+2d)(b+3d)$ $k^2(b^2-d^2)(b+2d)(b+3d)=(b^2-d^2)(k^2)(b+2d)(b+3d)$ $1=1$ It looks correct to me, but I'm a bit confused what the end result $1=1$ means since it looks so trivial and pointless. Does it mean I've proven $\dfrac{a^2-c^2}{b^2-d^2}=\dfrac{(a+2c)(a+3c)}{(b+2d)(b+3d)}$ ? Thanks.","['elementary-number-theory', 'algebra-precalculus']"
4758849,Curved space described by inverse matrix of another curved space,"I was introduced to geometry of curved space in General Relativity but now I am more interested in learning more about it in general.The tensors which  describe a curved 3D space can be represented in the form of a matrix A.The eleements of the main diagonal of $A$ give the distance conversion of each dimension and $A[i,j] , j \ne i$ give the skew angle between corresponding dimensions.Suppose we get the inverse of A $A^{-1}$ .Which is the relationship between the curved space described by $A$ and $A^{-1}$ ?",['differential-geometry']
4758853,Trouble in Proving the Sequential Criterion for Limits at Infinity,"Let $A\subseteq R,$ let $f : A\to R,$ and suppose that $(a,\infty)\subseteq A$ for some $a \in R.$ Then the following statements are equivalent: (i) $L=\lim_{x\to \infty}f(x)$ (ii) For every sequence $(x_n)$ in $A \cap (a,\infty)=(a,\infty)$ such that $\lim x_n=\infty$ , the sequence $f(x_n)$ converges to $L.$ I tried proving this in the following way: First we try to show, $(i)\implies (ii).$ If, $\lim_{x\to\infty}f(x)=L$ then this means, For every $\epsilon\gt 0$ there exists a $K>a$ such that if $x\geq K,$ we have $|f(x)-L|\lt \epsilon.$ Now, if $(x_n)$ is a sequence in $A \cap (a,\infty)=(a,\infty)$ such that $\lim x_n=\infty$ then, $\exists M\in\Bbb N$ such that $x\gt K(\gt a),$ when $n\geq M.$ But if, $x_n>K$ we have, $|f(x_n)-L|\lt \epsilon.$ To sum, we have shown that, if $n\geq M$ , then, $|f(x_n)-L|\lt \epsilon.$ But $\epsilon \gt 0$ and $(x_n)$ was arbitrary and this implies, that for every sequence $(x_n)$ in $A \cap (a,\infty)=(a,\infty)$ such that $\lim x_n=\infty$ , the sequence $f(x_n)$ converges to $L.$ However, while to show the converse, I think I make my argument faulty: Next, we try to show, that $(ii)\implies (i).$ If for every sequence $(x_n)$ in $A \cap (a,\infty)=(a,\infty)$ such that $\lim x_n=\infty$ , the sequence $f(x_n)$ converges to $L,$ then we fix,  an arbitrary $\epsilon\gt 0.$ Now, we denote, each sequence as $X_i=(x_n^{(i)})$ , where $i\in \Bbb N$ and $X_i\in A \cap (a,\infty)=(a,\infty)$ . If $\lim X_i=\infty$ for a particular $i\in Bbb N$ then $\exists M_i\in \Bbb N$ such that, $|f(x_n^{(i)}-L|\lt \epsilon$ for all $n\geq M.$ For each, $X_i$ we obtain, an $M_i$ and if, $M=\max(M_i)_{i\in \Bbb N},$ we may take, $x\geq M$ and then, $|f(x)-L|\lt\epsilon.$ Hence, we claim that, $L=\lim_{x\to \infty}f(x).$ I doubt my proof about the converse statement as, $\max\{\text{infinite set}\}$ has no meaning at all. We can take $\max$ element in a finite set and not in an infinite one. Secondly, in the part, where I wrote, "" $x\geq M$ and then, $|f(x)-L|\lt\epsilon$ "" might be faulty as well. This is because, if $x\geq M$ then, $|f(x)-L|\lt\epsilon$ will be true, only if, $x$ is in some properly diverging sequence $X_i$ tending to $\infty.$ But, how can we gurantee, that $x$ will be in such a sequence $X_i$ just like that? For these two reasons, I feel my argument breaks down. All in all, I don't understand how to show, $(ii)\implies (i)$ ?","['real-analysis', 'functions', 'sequences-and-series', 'limits', 'convergence-divergence']"
4758866,Doubt with a proof in Convergence theorem,"Theorem : If $c \in (0,1) $ , then $\lim_{n \rightarrow \infty} c^{n}=0$ . Proof : If $ 0 < c < 1 $ , we claim that $0 < c^{n + 1} < c^{n} < 1 $ . This can be easily shown by using Induction. Thus we have that $\{c^{n}\}$ is a monotonic decreasing sequence and bounded below. Thus $\{c^{n}\}$ is convergent. let $L = \lim_{n \rightarrow \infty} c^{n}$ . We need to show that $L = 0$ . Let $\epsilon > 0.$ Then $\exists N\ \in ℕ $ such that $ \forall n ≥ N$ , we have : $ \\ \: |c^{n} - L| < (1 - c)\dfrac{\epsilon}{2} $ . (I got this part). *Therefore, $\ |1 - c||L| = |Lc - L| = |L - c^{N + 1} + c^{N + 1} - cL| ≤ |L - c^{N + 1}| + c|c^{M} - L| < (1 - c)\dfrac{\epsilon}{2} + c(1 - c)\dfrac{\epsilon}{2} < (1 - c)\epsilon $ Thus, $\forall 
 \epsilon > 0, |L|< \epsilon \implies L = 0.$ $\blacksquare$ *I want to ask that how did we get this? Is it simply by manipulating.i.e, $|c^{n} - L| < |c - L| < |Lc - L|$ But then how did he came up with this? I know that if $|1 - c||L| < |1 - c|\epsilon$ then also, $|c^{n} - L| < |1 - c|\epsilon$ but we already chose a epsilon for $|c^{n} - L|$ , so I really do not get this part. Can somebody intuitively explain this to me like how does this proof actually works? Thanks a lot!","['limits', 'proof-explanation', 'real-analysis']"
4758875,Establishing Analyticity of Function f,"G is a domain in the complex plane. Consider the function $f: G \rightarrow \mathbb{C}$ , which is both complex and continuous within G. Additionally, we have the relation $e^{f(z)} = z$ . I know that inorder to show that f is analytic I need to show the following: 1. Show that G is an open set (This is inherently true as G is a domain). 2. Confirm that f is differentiable within G. It's worth noting that the function $e^x$ exhibits both continuity and differentiability within G. Given that f(z) is continuous across G, we can deduce the continuity of $e^{f(z)}$ within G. I'm seeking your assistance in establishing a connection between these points to demonstrate that f is analytic.",['complex-analysis']
4758886,Method to obtain values of Gauss hypergeometric function.,"Context: Reading this interesting paper: ( https://arxiv.org/pdf/1607.04742.pdf ) (""Special values of Gauss’s hypergeometric series derived from
Appell’s series F1 with closed forms"" by Akihito Ebisu).
We find that the author obtains a lot of values of Gauss Hypergeometric function with a new approach.
In particular we have: $$\sum_{n=0}^{\infty}\frac{(3n)!(2n)!(-1)^n}{n!^5}\left(\frac{1}{8640} \right)^n=\frac{9\cdot 5^{2/3}\cdot\Gamma(1/3)^6}{100\pi^4},\tag{1}$$ $$\sum_{n=0}^{\infty}\frac{(4n)!(-1)^n}{n!^4}\left(\frac{1}{6635520} \right)^n=\frac{3\cdot 5^{1/4}\cdot\Gamma(1/4)^4}{25 \pi^3}.\tag{2}$$ Guillera obtains values of Gauss Hypergeometric function with the WZ method: ( https://arxiv.org/pdf/2001.08104.pdf ), and then with Clausen's formula deduces some Ramanujan series for $1/\pi$ . Updated Level 3: From the comments of Tito Piezas III we are going to give the transformation that allows us to obtain (1) using Ebisu's values.
We take this version of Clausen's formula: $$\left(\sum_{n=0}^{\infty}\frac{f(a)f(b)}{f(2b)n!}x^n\right)^2=(1-x)^{-a}\sum_{n=0}^{\infty}\frac{f(a)f(b)f(2b-a)}{f(b+1/2)f(2b)n!}\left(\frac{x^2}{4(x-1)} \right)^n\tag{3}.$$ Where $f(x)=\frac{\Gamma{(x+n)}}{\Gamma{(x)}}=(x)_{n},$ is the Pochhammer symbol. Setting $a=1/3$ and $b=1/2$ in $(3)$ we have: $$_2F_1(\frac{1}{3},\frac{1}{2};1;x)^2=\frac{1}{(1-x)^{1/3}}\sum_{n=0}^{\infty}\frac{(2n)!(3n)!}{n!^5}\left(\frac{x^2}{432(x-1)}\right)^{n}\tag{4}.$$ Now using value $(E′′.1)$ (page 11 from Ebisu's paper) and setting in $(4)$ $x=\frac{1}{5}$ we have $(1)$ which is the second formula of level 3 listed by Tito. Concerning the second series of level 3 we find that solutions to $\frac{x^2}{432(x-1)}=-\frac{1}{8640}$ are $x=1/5$ and $x=-1/4$ so setting in $(4)$ $x=-1/4$ gives also: $$_2F_1(\frac{1}{3},\frac{1}{2};1;-\frac{1}{4})=\frac{3\cdot 20^{1/6}\Gamma{(\frac{1}{3})}^3}{10\pi^2}\tag{5}.$$ Concerning the third series of level 3 we find that solutions to $\frac{x^2}{432(x-1)}=-\frac{1}{326592}$ are $x=1/28$ and $x=-1/27$ so setting in $(4)$ $x=1/28$ gives: $$_2F_1(\frac{1}{3},\frac{1}{2};1;\frac{1}{28})=\frac{3\cdot 14^{1/3}\Gamma{(\frac{1}{3})}^3}{14\pi^2}\tag{6}.$$ Setting in $(4)$ $x=-1/27$ gives also: $$_2F_1(\frac{1}{3},\frac{1}{2};1;-\frac{1}{27})=\frac{9\cdot 2^{2/3}\Gamma{(\frac{1}{3})}^3}{28\pi^2}\tag{7}.$$ Updated Level 2: To obtain $(2)$ there is not explicit value listed in Ebisu's paper but using formula (B'.1) from page 8 and setting $a=1/4$ implies: $$_2F_1(\frac{1}{4},\frac{1}{2};1;-\frac{1}{80})=\frac{20^{1/4}\Gamma{(1/4)}^2}{5\pi^{3/2}}.\tag{8}$$ Now using $(3)$ and setting $a=1/4$ and $b=1/2$ we have: $$_2F_1(\frac{1}{4},\frac{1}{2};1;x)^2=\frac{1}{(1-x)^{1/4}}\sum_{n=0}^{\infty}\frac{(4n)!}{n!^4}\left(\frac{x^2}{1024(x-1)}\right)^{n}.\tag{9}$$ Then using $(8)$ and $(9)$ we recover $(2)$ which is the second series of level 2 given by Tito.
Also solutions to $\frac{x^2}{1024(x-1)}=-\frac{1}{6635520}$ are $x=-1/80$ and $x=1/81$ so putting $x=1/81$ in $(9)$ we get also: $$_2F_1(\frac{1}{4},\frac{1}{2};1;\frac{1}{81})=\frac{3\sqrt{2}\Gamma{(1/4)}^2}{10\pi^{3/2}}.\tag{10}$$ Concerning the first series of level 2 given by Tito we see that solutions to: $\frac{x^2}{1024(x-1)}=-\frac{1}{12288}$ are $x=-1/3$ and $x=1/4$ so using $(9)$ when $x=-1/3$ gives: $$_2F_1(\frac{1}{4},\frac{1}{2};1;-\frac{1}{3})=\frac{\sqrt{6}\Gamma{(1/4)}^2}{6\pi^{3/2}}.\tag{11}$$ And setting $x=1/4$ also: $$_2F_1(\frac{1}{4},\frac{1}{2};1;\frac{1}{4})=\frac{3^{1/4}\Gamma{(1/4)}^2}{3\pi^{3/2}}.\tag{12}$$ Updated Level 2': Also: $$_2F_1(\frac{1}{4},\frac{1}{2};1;-\frac{32}{49})=\frac{28^{1/4}\Gamma{(1/7)}\Gamma{(2/7)}\Gamma{(4/7)}}{8\pi^2}\tag{13},$$ and $$_2F_1(\frac{1}{4},\frac{1}{2};1;\frac{32}{81})=\frac{3\cdot 1372^{1/4}\Gamma{(1/7)}\Gamma{(2/7)}\Gamma{(4/7)}}{56\pi^2}\tag{14}.$$ So the $_3F_2$ series: $$\sum_{n=0}^{\infty}\frac{(-1)^n(4n)!}{63^{2n}n!^{4}}=\frac{3\cdot\Gamma{(1/7)}^2\Gamma{(2/7)}^2\Gamma{(4/7)}^2}{32\pi^{4}}\tag{15}.$$ With some effor one can shows the Ramanujan's series: $$\sum_{n=0}^{\infty}\frac{(-1)^n(65n+8)(4n)!}{63^{2n}n!^{4}}=\frac{9\sqrt{7}}{\pi}.\tag{16}$$ Question: So Akihito's method is less opaque than WZ method. Do you think that we can use his method and prove all the rational series known for $1/\pi$ ? Reflections: This question can be misunderstood as to what the WZ method refers.
I think this method is revolutionary and brilliant because it's ""trivial"" except for the fact when Carlson's theorem is needed. Also as Tito Piezas III has pointed, Jesús Guillera has proved series for $1/\pi^2$ using it and no other proofs are known till today. This method goes far beyond and deals with any hypergeometric series so you can see how magnificent it is.","['closed-form', 'hypergeometric-function', 'sequences-and-series']"
4758920,Help understanding the proof of Konig’s minimax theorem,"I am reading the appendix of  A Course on Large Deviations with an
Introduction to Gibbs Measures, by Rassoul-Agha.  Consider the following section I am having trouble with the highlighted parts About the first one: I don't get how they used the result of exercise A.13 to argue that we can choose $(t_1,....,t_k)\in M_k$ such that $\sum_{i=1}^k t_ih(x_i)\in K$ . I found the original paper the author used to rewrite this proof here https://www.researchgate.net/publication/226962484_A_simple_proof_for_Konig's_minimax_theorem , maybe that can help how is he using that so called density of $M_k$ About the second one I don't get how the previous arguments lead to conclude that there exists $p\in K^\circ$ (why does it have to be in the interior?) such that $h_1(\mathscr X)\subseteq \prod_{j=1}^n(-\infty, p_j].$ (how do I get this inclusion?) Again I don't get how they used the result of exercise A.13 to argue the existence of $t\in M_n$ close enough to $\delta$ such that the hyperplane $\sum_{j=1}^nt_ju_j=0 $ (where did this come from?) separates $h_1(\mathscr X)$ and $K$ . and why does that imply $\sum_{j=1}^nt_jf(x,y_j)\le c_1$ . While this last equation means is  that $h_1(\mathscr X)$ is to one side of the plane, but why isn't it on the other side? Could you help me understand these points? Quoted properties:","['analysis', 'real-analysis', 'functional-analysis', 'general-topology', 'convex-analysis']"
4758935,Smooth tiling of the plane,"For my master thesis, I solved a PDE under the assumption of the domain being smooth and small. I wanted to patch these domains and solutions somehow together, hoping that I can get a global result. With that in mind, I asked myself, whether one can tile the plane using bounded domains with smooth boundary. By tiling, I mean a family of closed sets covering the entire plane, such that the interiours are pairwise disjoint.
My reasonable assumptions on the sets are, that they should be closed domains, with a uniform bound on the diameter, having a smooth boundary and the family needs to be locally finite. It seems impossible to me, if one restricts to locally finite tilings, since when two adjacent boundaries separate, they always leave a cusp, which can’t be filled by a smooth and locally finite tiling. Does anyone have a clue if such a smooth tilings can exist or if they have a name? Maybe I missed something, but I think the cusps are inevitable. Edit 1: here are the reasons for my assumptions:
If one removes one of the assumptions (smooth boundary, uniformly bounded and locally finite) without smooth boundary, one can take for example a square grid tiling the plane without boundedness, one could take two half planes. Without a uniform bound on the diameter, one could take concentric rings which tile the plane. without the local finiteness, one could use a Vitali-like covering using balls. Edit 2:
My sketch of the proof, that it is impossible to tile the plane with these sets goes as follows. Assume such a tiling exists. If two adjacent sets touch, then the touching boundaries either seperate at some point, or the sets share a connected component of their boundaries. If they seperate, one sees by local graph description of the sets, that he have a cusp, which can’t be filled by a single smoothly bounded set (maybe by a sequence of domains,but that is against the local finiteness). Therefore,  sets that share a point at their boundary must actually share the entire connected component of the boundary. For each set $A$ , there is some set $B$ that touches the most outer boundary enclosing the set $A$ and must therefore share this outermost connected component. This makes it, so that $B$ has strictly larger diameter, since $A$ is surrounded by $B$ . This process continues for the outermost boundary of $B$ and if we use the local finiteness, one can continue arbitrary often to find that the sets have unbounded diameter. This is a contradiction to the hypothesis, that such a tiling exists.","['euclidean-geometry', 'partial-differential-equations', 'tiling', 'differential-geometry']"
4758939,"The integral solved $\int x^{\ln(x)-1}\ln(x)\,\mathrm dx$","In the past time I was trying to find a common answer for $\displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx$ , I actually answered the question which pattern is as follows. $\begin{align*} \displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx&=\int x^{\ln(x)}x^{-1}\ln(x)\,\mathrm dx \tag*{$\alpha^{m+n}=\alpha^m\alpha^n$} \\ &=\int \frac{x^{\ln(x)}\ln(x)}{x}\,\mathrm dx\\ &=\int\frac{\left( \left( e^{\ln(x)} \right)^{\ln(x)} \right)\ln(x)}{x}\,\mathrm dx\\ &=\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx \end{align*}\tag*{1}$ Use the Integration by substitution
Let $\begin{aligned} u&=\ln(x) \\\dfrac{\mathrm{d}u}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(x) \right] \\ \mathrm du &=\frac{1}{x}\,\mathrm dx \iff\mathrm dx=x\,\mathrm du \end{aligned}\tag*{2}$ $\text{We obtain $\displaystyle\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx =\int ue^{u^2}\,\mathrm du$}\tag*{3}$ Apply the integration by substitution twice and this time I would use $\alpha$ since I have used $u$ previously. $\begin{aligned} \alpha&=u^2\\\dfrac{\mathrm d\alpha}{\mathrm du} &=\dfrac{\mathrm d}{\mathrm du}\left[ u^2 \right] \\ \mathrm d\alpha&= 2u\,\mathrm du\implies\mathrm du=\frac{1}{2u}\,\mathrm d\alpha \end{aligned}\tag*{4}$ Hence $\begin{aligned} \displaystyle\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx =\int ue^{u^2}\,\mathrm du\implies\int ue^{u^2}\,\mathrm du&= \frac12\int e^{\alpha}\,\mathrm d\alpha \\ &=\frac{1}{2}e^{\alpha}+C\\ &=\frac{e^{u^2}}{2}+C\qquad \alpha\implies u\\ &=\frac{e^{{\ln^2(x)}}}{2}+C \qquad u\implies x \\&=\frac{e^{\ln(x)\ln(x)}}{2}+C\\ &=\frac{\left( e^{\ln(x)} \right)^{\ln(x)}}{2}+C\\&= \boxed{\frac{x^{\ln(x)}}{2}+C} \end{aligned}\tag*{5}$ Use logarithmic differentiation to tell that $\frac{e^{{\ln^2(x)}}}{2}+C$ is correct. Let $y=\frac{e^{{\ln^2(x)}}}{2}$ . $\begin{aligned} y&=e^{\ln^2(x)}\\ \ln(y) &=\ln\left( e^{\ln^2(x)} \right) \\ \ln(y) &=\ln^2(x)\ln(e)\\ \ln(y) &=\ln^2(x)\\\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(y) \right] &=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln^2(x) \right]\\ \dfrac{\mathrm{d}}{\mathrm{d}y}\left[ \ln(y) \right]\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}\alpha}\left[\alpha^2\right]\dfrac{\mathrm{d}}{\mathrm{d}x}[\ln(x)]\\ \frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=2\alpha\left( \frac{1}{x} \right)\\\frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2\ln(x)}{x}\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=y\left( \frac{2\ln(x)}{x} \right)\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2e^{\ln^2(x)}\ln(x)}{x} \end{aligned}\tag*{6}$ Since we have the $\frac{e^{\ln^2(x)}}{2}$ , then we can say that like this $\begin{aligned}\displaystyle y=\frac{e^{\ln^2(x)}}{2}&\implies\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \frac{e^{\ln^2(x)}}{2} \right]\\&\implies\frac12\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ e^{\ln^2(x)} \right]\\&\implies\frac{1}{2}\left( \frac{2e^{\ln^2(x)}\ln(x)}{x} \right)\\& \implies\frac{e^{\ln^2(x)}\ln(x)}{x} \\ &\implies\frac{e^{\ln(x)\ln(x)}\ln(x)}{x}\\ &\implies\frac{\left( e^{\ln(x)} \right)^{\ln(x))}\ln(x)}{x}\\&\implies\frac{x^{\ln(x)}\ln(x)}{x}\\ &\implies x^{\ln(x)-1}\ln(x)\end{aligned}\tag*{7}$ The second version is by substituting $u=x^{\ln(x)}$ . $\displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx \tag*{8}$ Let $\begin{aligned} u&=x^{\ln(x)}\\ \frac{\mathrm du}{\mathrm dx}&=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ x^{\ln(x)} \right]^* \\\mathrm du &= 2x^{\ln(x)-1}\ln(x)\,\mathrm dx \implies\frac{\mathrm{d}u}{2x^{\ln(x)-1}\ln(x)}=\,\mathrm dx \end{aligned}\tag*{9}$ Then $\begin{aligned} \displaystyle \int x^{\ln(x)-1}\ln(x)\,\mathrm dx&=\int x^{\ln(x)-1}\ln(x)\cdot\frac{1}{2x^{\ln(x)-1}\ln(x)}\,\mathrm du\\ &=\frac12\int 1\,\mathrm du\\ &=\frac12[u]\\ &=\frac{x^{\ln(x)}}{2}+C \end{aligned}\tag*{10}$ *Proof $\frac{\mathrm{d}}{\mathrm{d}x}\left[ x^{\ln(x)} \right]=2x^{\ln(x)-1}\ln(x)$ To proof this let $y=x^{\ln(x)}$ $\begin{aligned} y&=x^{\ln(x)} \\ \ln(y)&=\ln\left( x^{\ln(x)} \right)\\ \ln(y)&=\ln(x)\ln(x)\\ \ln(y)&=\ln^2(x)\\\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(y) \right] &=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln^2(x) \right]\\ \dfrac{\mathrm{d}}{\mathrm{d}y}\left[ \ln(y) \right]\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}\alpha}\left[\alpha^2\right]\dfrac{\mathrm{d}}{\mathrm{d}x}[\ln(x)]\\ \frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=2\alpha\left( \frac{1}{x} \right)\\\frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2\ln(x)}{x}\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=y\left( \frac{2\ln(x)}{x} \right)\\&=\frac{2x^{\ln(x)-1}\ln(x)}{x}\\\dfrac{\mathrm{d}y}{\mathrm{d}x} &=2x^{\ln(x)-1}\ln(x)\end{aligned}\tag*{11}$ I tried asking various sources including AI sites whether there is a match between the functions $e^{\ln^2(x)}$ and $x^{\ln(x)}$ because the derivatives are the same and I checked the results my way of doing this with the help of Integral Calculator and looks as follows. $\displaystyle \int x^{\ln(x)-1}\ln(x)\,\mathrm dx$ "" /> $u= \ln(x)$ )"" /> $u= x^{\ln(x)}$ ) and the antiderivative results via maxima"" /> I also prove that what I'm doing is correct by differentiating the function for the result of the integral in the first way as shown in (5) using logarithmic differentiation as I did in (6) and how I get $x^{\ln (x)}$ from $e^{\ln^2(x)}$ I separate the $\ln^2(x)$ into $\ln(x)\ln(x)$ then because I use exponential rule that $\left(\alpha^{m}\right)^n =\alpha^{mn}$ and $e^{\ln(x)}= x$ because $e^{\ln(\alpha )}=1^{\alpha}=\alpha$ . So do you think what I'm doing is right?","['logarithms', 'calculus', 'solution-verification', 'indefinite-integrals', 'derivatives']"
4758943,Bounds for vertical integrals and Mellin transforms,"I am trying to bound Mellin transforms. They look like $$I(x) = \int_{(c)} f(w) \frac{x^w}{w} dw$$ where the integral is on the vertical line of abscissa $c$ . Let's say that $f$ has vertical growth controlled by $$f(w) \ll (1+\Im(w))^k$$ for a certain real number $k$ . How can I bound $I(x)$ in terms of $x$ , in general? I tried with some explicitly truncated versions of the integral, but I keep stuck with the remaining pieces of the integral...","['integration', 'number-theory', 'mellin-transform', 'analytic-number-theory']"
4758968,20 two colored balls into 4 bins,"We have $16$ red identical balls and $4$ black identical balls. We have $4$ bins and put all balls randomly into the bins such that each bin contains $5$ balls at the end. Each arrangement has the same probability. How many ways are there to distribute the balls into the bins? What is the probability that all black balls are in different bins? My idea was to realize that the red balls have no influence, so the number of all arrangements is simply ${4+4-1\choose 4}$ . As all black balls are identical there is only one way that all black balls are in different bins, hence the probability is $\frac{1}{{7\choose 4}}$ . But this Sound too easy... what do I miss? Or is it correct?","['combinatorics', 'balls-in-bins', 'probability']"
4758983,Find the value of $\dfrac{\cos(\pi/4)\cos(\pi/6)\cos(\pi/8)\cos(\pi/10)\cos(\pi/12)\cdots}{\cos(\pi/3)\cos(\pi/5)\cos(\pi/7)\cos(\pi/9)\cdots}$,I manually calculated the values and found that the resulting answer was very close to $1.57\simeq \dfrac{\pi}{2}$ $$\dfrac{\cos(\pi/4)\cos(\pi/6)\cos(\pi/8)\cos(\pi/10)\cos(\pi/12)\cdots}{\cos(\pi/3)\cos(\pi/5)\cos(\pi/7)\cos(\pi/9)\cdots}=>\dfrac{\sin(\pi x/4)}{\sin(\pi x/6)}.\dfrac{\sin(\pi x/3)}{\sin( 3\pi x/10)}.\cdots$$ I assigned $x$ inside each term in order to separate and apply L' Hopital's rule for each term and somehow transform it into the Wallis product. But that may not be correct usage of the rule. I do not know how to proceed further. I would appreciate any solutions either disproving or proving the assumed result of $\dfrac{\pi}{2}$,"['trigonometric-series', 'trigonometry', 'sequences-and-series']"
4759007,"Homology in a picture? (Is this picture just metaphorical, or a rigorous example that can be formalized?)","A post-doc colleague showed me this picture and said: going from the diagram No.2 to No.3 and to No.4 is taking the homology. I did not quite understand this comment. For me, if I take simplicial homology as an example, homology is setting up a simplicial chain complex and then forming the quotient groups of cycles modulo boundaries. I can see that in a certain way the diagrams 2-4 are related to the idea of forming the quotient, or I should say they show ""modding out"" the boundaries in some sense. My question, are these diagrams actually more metaphorical (a good visual to support memory), or are they in fact quite rigorous examples that could easily be formalized? I have tried but could not rigorously formalize what I see in the diagrams. Any help would be greatly appreciated!","['homological-algebra', 'winding-number', 'group-theory', 'algebraic-topology', 'simplicial-complex']"
4759010,Simplify a summation in the solution of $\displaystyle\int_{0}^{\infty}e^{-cx}x^{n}\arctan(ax)\mathrm{d}x$,"Context I calculated this integral: $$\begin{array}{l}
\displaystyle\int_{0}^{\infty}e^{-cx}x^{n}\arctan(ax)\mathrm{d}x=\\
\displaystyle\frac{n!}{c^{n+1}}\left\lbrace\sum_{k=0}^{n}\left[\text{Ci}\left(b\right)\sin\left(b-\frac{k\pi}{2}\right)+\left(\frac{\pi}{2}-\text{Si}\left(b\right)\right)\cos\left(b-\frac{k\pi}{2}\right)\right]\frac{b^{k}}{k!}\right.\\
\displaystyle\left.\quad\qquad+\sum_{k=1}^{n}\frac{1}{k}\sum_{j=1}^{k}\sum_{l=0}^{n-k}\frac{\cos\left(\frac{\pi\left(l-j\right)}{2}\right)}{\left(j-1\right)!l!}b^{j+l-1}\right\rbrace\end{array}$$ Where: $b:=\dfrac{c}{a}$ $\text{Si}(z)$ is the sine integral function $\text{Ci}(z)$ is the cosine integral function Question I need help simplifying the last two summations: $$\sum_{j=1}^{k}\sum_{l=0}^{n-k}\frac{\cos\left(\frac{\pi\left(l-j\right)}{2}\right)}{\left(j-1\right)!l!}b^{j+l-1}$$ I can't find the transformation that allows me to calculate the sum only on the even "" $l-j$ "" given that: $$\cos\left(\frac{\pi\left(l-j\right)}{2}\right)=\begin{cases}(-1)^{l-j}&\text{if }l-j\text{ is even}\\
0&\text{if }l-j\text{ is odd}\end{cases}$$ My idea I noticed that the polynomial is odd, so technically it is possible to substitute $l-j=2s$ where $1\leq s\leq \left\lfloor\dfrac{n}{2}\right\rfloor$ ? Thanks in advance for any suggestions","['summation', 'definite-integrals', 'real-analysis', 'calculus', 'trigonometry']"
4759025,Linear Algebra Done Right Exercise 6.B.16,"I am trying to solve Exercise 16 from Section 6.B of the third edition of Linear Algebra Done Right by Axler. Suppose $ \mathbf{F} = \mathbf{C}, V $ is finite-dimensional, $ T \in \mathcal{L}(V) $ , all the eigenvalues of $ T $ have absolute value less than $1$ , and $ \epsilon > 0 $ . Prove that there exists a positive integer $ m $ such that $ \lVert T^m v \rVert \leq \epsilon \lVert v \rVert $ for every $ v \in V $ . ( $V$ is an inner product space.) I think this is a quick corollary of the implication $\rho(T) < 1 \implies \lim_{m \to \infty} \lVert T^m \rVert = 0 $ . However, at this point in the textbook we have discussed neither spectral radius nor operator norm; for this reason I think Mr. Axler had another approach in mind. The title of Section 6.B is 'Orthonormal Bases', so I tried to look for a solution which only uses basic properties of orthonormal bases. Since $V$ is a finite-dimensional complex vector space, Schur decomposition (Theorem 6.38 in the text) ensures there is an orthonormal basis of $V$ with respect to which $T$ has an upper-triangular matrix. I managed to prove the result in the special case where this matrix is diagonal and also in the special case where $\dim V = 2$ , but I haven't been able to prove the general case. Can anyone see an 'orthonormal basis approach'? I did find this post . Unfortunately, I think there is a mistake in that answer. At least, I can't see how the equality $$
\sum_{k=1}^n \langle T^j(v), e_k \rangle e_k = \sum_{k=1}^n a_k \lambda_k^j e_k
$$ follows from $\langle T^j(e_k), e_k \rangle = \lambda_k^j$ .","['linear-algebra', 'eigenvalues-eigenvectors']"
4759038,Derivatives of Matrix Functions of Different Dimensions,"Notation: For matrices $A,B\in\mathbb R^{n\times m}$ , we define the inner product $\langle A,B\rangle=\sum_{i,j}A_{ij}B_{ij}$ The basis vector $e_i$ is equal to $1$ at position $i$ and $0$ otherwise. Problem: Consider the function $f:\mathbb R^{n\times n}\times \mathbb R^{d\times n}\to \mathbb R$ such that $$f(X,Y)=\langle W, X-Y^\intercal Y\rangle.$$ My goal is to determine when the derivative of $f$ with respect to $X$ and $Y$ are equal to $0$ . From what I understand, we can take $\nabla_Xf=W$ . However, for $\nabla_Yf$ , we should consider the matrix $M$ labeled with $$M_{ij}=\frac{\partial f}{\partial Y_{ij}}=e_j^\intercal WY^\intercal e_i$$ (here $e_j\in\mathbb R^n$ , while $e_i\in\mathbb R^d$ ), meaning $M=WY^\intercal$ . The problem is that $M$ is of size $d\times n$ , so I'm not sure how to describe when the derivative of $f$ is equal to some matrix $Z$ . Attempt: I thought about trying to represent the problem in a way where everything is the same shape. I noticed \begin{align}
    \langle W,Y^\intercal Y\rangle&=\text{Trace}(W^\intercal Y^\intercal Y)\\
&=\text{Trace}(YW^\intercal Y^\intercal)\\
&=\langle WY^\intercal, Y^\intercal\rangle\\
&=\left\langle\begin{bmatrix}
0&Y^\intercal\\
Y&0
\end{bmatrix},
\begin{bmatrix}
0&WY^\intercal\\
YW^\intercal&0
\end{bmatrix}
\right\rangle,
\end{align} meaning we can rewrite $f$ as $$
f(X, Y)=\left\langle\begin{bmatrix}
    X&Y^\intercal\\
    Y&0
\end{bmatrix},
\begin{bmatrix}
    W&-WY^\intercal/2\\
    -YW^\intercal/2&0
\end{bmatrix}
\right\rangle
$$ so I'm thinking we can represent $\nabla_{X,Y}f$ as $$\begin{bmatrix}
W&-WY^\intercal/2\\
-YW^\intercal/2&0\end{bmatrix}.$$ If $Z=\begin{bmatrix}
Z_1&Z_2^\intercal\\
Z_2 & Z_3
\end{bmatrix}$ , Then the derivative is equal to $Z$ when $W=Z_1$ , $YW^\intercal=-2Z_2$ (meaning $Y^\intercal=-2W^+Z_2$ ?), and $Z_3=0$ . Does this make any sense? How is this usually done?","['derivatives', 'matrix-analysis', 'matrix-calculus', 'linear-algebra']"
4759100,Factoring a Sum of Cubes Without Prior Knowledge of the Formula,"Many solutions related to factoring a sum of cubes suppose knowledge of the sum of cubes formula, $$
a^3 + b^3 = (a+b)(a^2 - ab + b^2)
$$ If you did not possess prior knowledge of this formula, how would you go about factoring a sum of cubes? I understand that some experimentation with distributing polynomials could lead to the formula, which could then be used. I think there must be a way to factor a sum of cubes without invoking it directly. For instance, how would you find some polynomial $p(x)$ such that $$
p(x) \cdot \, (x+c) = x^3 + c^3
$$ Without first resorting to applying the sum of cubes formula?","['algebra-precalculus', 'factoring', 'polynomials']"
4759113,When are two semidirect products of two cyclic groups isomorphic,"A semidirect product of two cyclic groups $C_m$ and $C_n$ has the form $$
C_m \rtimes_k C_n 
= \langle x,y 
\mid x^m = y^n = 1,\, 
yxy^{-1} = x^k \rangle,
$$ for some $k^n \equiv 1\pmod m$ . Now, a question that seems quite elementary and yet to which I have not found a satisfactory answer is: what $k$ 's give isomorphic semidirect products? Clearly, if $\gcd(r,n)=1$ , then $C_m\rtimes_k C_n$ is isomorphic to $C_m\rtimes_{k^r} C_n$ via the mapping $(x,y)\mapsto(x,y^{r^{-1}\ \mathrm{mod}\ n})$ . Is the converse also true, that is, if $C_m\rtimes_k C_n$ is isomorphic to $C_m\rtimes_{k'} C_n$ , then $k'\equiv k^r\pmod m$ for some $\gcd(r,n)=1$ ? Any help/reference appreciated.","['semidirect-product', 'group-theory', 'abstract-algebra', 'finite-groups']"
4759138,Durrett Q4.8.3 on martingales and the Optional Stopping Theorem,"I'm working through this question from Durrett's probability textbook. Let $S_n = \xi_1+\cdots +\xi_n$ where the $\xi_i$ are independent with $E\xi_i = 0$ and var( $\xi_i) = \sigma^2$ . $S_n^2 - n\sigma^2$ is a martingale. Let $T = \min\{n : |S_n| > a\}$ . Use Theorem 4.8.2 to show $ET \geq a^2/\sigma^2$ . (Theorem 4.8.2 refers to the following result: If $X_n$ is a submartingale and $N$ is a stopping time such that $E|X_N| < \infty$ and $X_n 1_{\{N > n\}}$ is uniformly integrable, then $X_{N \wedge n}$ is also uniformly integrable and hence $EX_N \geq EX_0$ ) If I define $X_n = S_n^2 - n \sigma^2$ , then $X_n$ is a martingale and we have that $$EX_n1_{\{T > n\}} \leq ES_n^2 1_{\{T > n\}} \leq a^2$$ So, $X_n1_{\{T > n\}}$ is uniformly integrable. I'm having trouble with the other condition needed - that is, $E|X_T| < \infty$ . Since the final result is immediate if $ET = \infty$ , we can assume that $ET < \infty$ . $$ E|X_T| \leq ES_T^2 + \sigma^2 ET $$ So, it is enough to show that $ES_T^2 < \infty$ . Now, as if $T < \infty$ , $S_{T-1}^2 \leq a^2$ and if $T = \infty$ , $S_n^2 \leq a^2$ for all $n$ . $$ ES_N^2 \leq a^2 + E\xi_N^2 + a^2$$ This reduces to showing that $E\xi_N^2 < \infty$ , but I have not been able to show this.","['martingales', 'probability-theory']"
4759153,Existence of a simple convex polygon of specified angle measures,"Given $n \geq 3$ positive reals $\alpha_1, \dots, \alpha_n$ such that $\alpha_i < 180$ and $\alpha_1 + \dots + \alpha_n = 180(n - 2)$ , how do we show the existence of an $n$ -sided simple (convex) polygon with interior angles of measure $\alpha_1, \dots, \alpha_n$ ? This is a simpler version of this question , which considers non-convex polygons as well (but unfortunately has no posted answer). EDIT:
I have accepted Jim Ferry's answer, but for completeness I will fill in some details he left implicit. We can guarantee the tangency of each successive ray because we have the freedom to choose when the previous ray ends. If the point of tangency $P'$ determined by the final ray was not equal to our starting point $P$ (the point on the circle from where our first ray birthed), then the ray from $P$ in the opposite direction and the ray from $P'$ as a continuation of the previous ray would intersect at another point $M$ . Now, the $n$ -points on our failed polygon along with $M$ determine the vertices of an $n + 1$ sided polygon, and moreover, $n$ of its interior angles are equal to the $\alpha_i$ . But then, the interior angle $\alpha_{n + 1}$ at $M$ is equal to $180(n - 1) - (\alpha_1 + \dots + \alpha_n) = 180$ , a contradiction. The reason why the process doesn't ""loop too far around"" the circle is given by a similar argument as above. The motivation for centering the construction around a circle was to prevent our polygon from hitting itself.","['geometry', 'polygons']"
4759173,"How many $3$-element subsets of $\{1,2,3,...,19,20\}$ have product divisible by $4$?","Same question :- Where am I overcounting? How many $3$ element subsets of the set $\{1,2,3,...,19,20\}$ are there such that the product of the three numbers in the subset is divisible by $4$ ? My attempt:- I divided this into broadly 2 cases :- Case 1:- Subsets containing atleast 1 number of type 4k :- $4k, 4k, 4k$ = ${5 \choose 3}$ $4k, 4k, 4k+1$ = ${5 \choose 2}*{5 \choose 1}$ $4k,4k,4k+2$ = ${5 \choose 2}*{5 \choose 1}$ $4k,4k,4k+3$ = ${5 \choose 2}*{5 \choose 1}$ $4k,4k+1,4k+1$ = ${5 \choose 1}*{5 \choose 2}$ $4k,4k+1,4k+2$ = ${5 \choose 1}*{5 \choose 1}*{5 \choose 1}$ $4k,4k+1,4k+3$ = ${5 \choose 1}*{5 \choose 1}*{5 \choose 1}$ $4k,4k+2,4k+2$ = ${5 \choose 1}*{5 \choose 2}$ $4k,4k+2,4k+3$ = ${5 \choose 1}*{5 \choose 1}*{5 \choose 1}$ Case 2:- without any 4k type of number $4k+2, 4k+2, 4k+2$ = ${5 \choose 3}$ I cant figure out what all cases am I missing ?  I am getting 685 cases however total cases are 795","['contest-math', 'combinatorics']"
4759181,"If an infinite group $G$ is generated by two elements $a,b$ such that $a^n=b^n=e$, must $x^n=e$ have infinitely many solutions?","Suppose that $G$ is an infinite group, $a,b\in G$ satisfy $G=\langle a,b\rangle$ and $a^n=b^n=e$ for some $n\in\mathbb{N}^*$ , where $e$ is the identity of $G$ . Must $G$ contain infinitely many $x$ such that $x^n=e$ ? The result is almost obvious for $n=2$ : we have $((ab)^ra)^2=(b(ab)^s)^2=e$ for all $r,s\in\mathbb{N}$ . If $x^2=e$ has only finitely many solutions, then $(ab)^ra=(ab)^{r'}a$ for some $r\neq r'$ , or $b(ab)^s=b(ab)^{s'}$ for some $s\neq s'$ , or $(ab)^ra=b(ab)^s$ for some $r,s$ . In either case, there is some $m$ such that $(ab)^m=e$ ( $e=r'-r$ in the first case, $s'-s$ in the second case and $r+s+1$ in the third case), and it turns out that $G$ is a subgroup of $D_{2m}$ , the dihedral group of order $2m$ , which is a finite group. What can be said for general $n$ ? Any help appreciated. Edit: Since the result has been proven to be true, there is a consequence that may appear to be interesting: suppose that every finite subgroup of $G$ is cyclic, then for every $n\in\mathbb{N}^*$ , $x^n=e$ has either at most $n$ solutions or infinitely many solutions. Prove: Fix $n$ , suppose that $x^n=1$ has only finitely many solutions. Let $S$ be the set of solutions. If there exists $a,b\in S$ such that $\langle a,b\rangle$ is infinite, then $\langle a,b\rangle$ contains infinitely many $x$ such that $x^n=e$ , a contradiction; so the subgroup generated by every two elements in $S$ is finite (and thus cyclic), which means that every two elements in $S$ commute, so $\langle S\rangle$ is finite (and thus cyclic, actually equal to $S$ ), which means that $x^n=1$ has at most $n$ solutions. Of course the converse is not true, as shown by the example $D_\infty\times C_{2m}$ , where $D_\infty$ is the infinite dihedral group ( $x^n=1$ has at most $n$ solutions for odd $n$ and infinitely many solutions for even $n$ , but the group contains infinitely many subgroups isomorphic to $C_2\times C_{2m}$ which is not cyclic). Edit 2: The result above is already contained by the comment of Mikko Korhonen to the answer.","['group-theory', 'abstract-algebra', 'infinite-groups']"
4759184,Minimum Number of Expected Weighings,"We all know the riddle where given $9$ coins, all being of equal weight except for one, the counterfeit, you have to show that the counterfeit coin can always be found in two weighings. I wanted to generalize the problem and find the minimum number of weighings that guarantees that you find a counterfeit coin from $n$ coins and got that it was $\lceil \log_3 n\rceil$ . I was wondering what would the optimal strategy be for minimizing the expected number of weighings instead. My approach so far was this. Let $X(n)$ be the random variable that gives the number of weighings used using the optimal strategy. Then the first weighing splits the coins into three groups of sizes $j$ , $j$ , and $k$ . Then if the coin is in the first two groups, $X(n) = X(j) + 1$ . Else, $X(n) = 1 + X(k)$ . So $$E(X(n)) = 2(j/n)E(X(j)) + (1-2j/n)E(X(n-2j)) + 1$$ where $E(X(1)) = 0$ and $E(X(2)) = 1$ . I kind of got stumped after this. I wasn't sure what you could do from here to find an optimal strategy. In the original version of the problem, I solved it by getting an intuition for what the optimal strategy is and then proving that it's optimal using induction. Here though I'm not really sure if there is a pattern in where the split occurs or even if there's an explicit formula for the expected number of weighings using the optimal strategy. Edit: (8/27/23) I made a python script for finding the expected number of weighings using the optimal strategy: import math

import matplotlib.pyplot as plt

def expect(n):

    expectation = [0, 1]

    for i in range(2, n + 1):

        mini = float(""inf"")

        for j in range(1, i//2 + 1):

            mini = min((2*j/i)*expectation[j - 1] + (1-2*j/i)*expectation[i - 2*j - 1] + 1, mini)

        expectation.append(mini)

    return expectation[-1]



numCoins = list(range(1, 1000, 10))

expectations = [expect(i) for i in numCoins]

plt.scatter(numCoins, expectations)

plt.plot(numCoins, [math.ceil(math.log(i)/math.log(3)) for i in numCoins])

plt.xlabel(""Number of Coins"")
plt.ylabel(""Average Number of Weighings"")
plt.show() And here's a scatter plot of the average minimum of weighings against the number of coins with the solid line being the number of weighings needed to guarantee that you find the counterfeit coin.","['expected-value', 'discrete-mathematics', 'recurrence-relations', 'probability']"
4759186,Generalization of Dirac delta identity,"The Dirac delta distribution obeys the following identity in $\mathbb{R}$ $$\lim_{\epsilon\to 0}\dfrac{1}{\pi}\dfrac{\epsilon}{\epsilon^2+x^2}=\delta(x)\tag1.$$ I know how to prove this using the Sokhotski–Plemelj theorem, namely $$\lim_{\epsilon\to 0}\dfrac{1}{x\pm i\epsilon}=\mp i\pi\delta(x)+{\cal P}\dfrac{1}{x},\tag2$$ where $\cal P$ means the principal value. Indeed subtracting the formula for the two signs it follows that $$\lim_{\epsilon \to 0}\dfrac{1}{\pi}\left[\dfrac{\epsilon}{x^2+\epsilon^2}\right]=\dfrac{1}{2\pi i}\lim_{\epsilon \to 0}\left[\dfrac{1}{x-i\epsilon}-\dfrac{1}{x+i\epsilon}\right]=\delta(x)\tag{3}.$$ All that said, I'm interested in a generalization. I want to understand the small $\epsilon$ expansion of $$\left(\dfrac{\epsilon}{\epsilon^2+\|x\|^2}\right)^a\tag{4}$$ where $a\in \mathbb{C}$ and $x\in \mathbb{R}^n$ . For example this often appears in some Physics papers when discussing the wave equation in hyperbolic spaces. It is often claimed that one has an expansion of the form $$\left(\dfrac{\epsilon}{\epsilon^2+\|x\|^2}\right)^a=\pi^{n/2}\dfrac{\Gamma\left(a-\frac{n}{2}\right)}{\Gamma(a)}\epsilon^{n-a}\delta^{(n)}(x)+\dfrac{\epsilon^a}{\|x\|^{2a}}+\cdots\tag{5}$$ where the dots denote subleading terms in the small $\epsilon$ expansion. Setting $a=1$ and $n=1$ and taking $\epsilon\to 0$ we recover (1). I would like to understand how to prove (5) and how to identify the order of the small $\epsilon$ corrections. I also want to understand whether some restriction on $a$ has to be made, another reason I want to fully understand the proof of this result. How can this be shown?","['analysis', 'distribution-theory', 'calculus', 'limits', 'mathematical-physics']"
4759249,"Let $\mathcal{B}_0(X, Y)$ is a Banach space. Does this imply that $Y$ is a Banach space?","Consider $$\mathcal{B}_0(X, Y) =\\{T\in\mathcal{B}(X,Y): \overline{T(B_X[0,1])}\subset Y \text{compact}\\}$$ where $B_X[0, 1]=\{x\in X: \|x\|\le 1\}$ Claim: $\mathcal{B}_0(X, Y)$ is a Banach space ( or closed subspace of $(\mathcal{B}(X, Y), \|•\|_{op}) $ iff $Y$ is Banach. If $Y$ is a Banach space then it can be proved easily that the operator limit of a sequence of compact operator is compact. I guess the other implication is not true! Let $\mathcal{B}_0(X, Y)$ is a Banach space. Does this imply that $Y$ is a Banach space? Can we characterize all normed linear spaces $X$ such that $\mathcal{B}_0(X)=\mathcal{B}_0(X,X) $ is closed?","['banach-spaces', 'operator-theory', 'banach-algebras', 'analysis', 'functional-analysis']"
4759292,Are closed group of $GL(n)$ always determined by a set of tensors?,"Let $\mathcal{T}$ be a set of tensors in $\mathbb{R}^n$ , and let $G_{\mathcal{T}}$ be a subgroup of $GL(n)$ defined as $$
G_{\mathcal{T}} = \{ g \in GL(n) \mid g \cdot T = T, \, \forall T \in \mathcal{T} \},
$$ where "" $\cdot$ "" denotes the appropriate tensor transformation rule under the action of $g \in GL(n)$ .
I know $G_{\mathcal{T}}$ is a closed subgroup of $GL(n)$ (right?). Now, given a closed subgroup $G$ of $GL(n)$ , can we find a set of tensors $\mathcal{T}$ such that $G=G_{\mathcal{T}}$ ?","['group-theory', 'tensors', 'linear-algebra', 'lie-groups']"
4759296,Local dimension at a point and dimension of irreducible components passing through that point,"I'm using this definition for the (Krull) dimension of a topological space $X$ and (Krull) dimension at a point $x\in X$ . In general, given a topological space $X$ , one always has $$
\dim X=\max\{\dim T\mid T\subset X\text{ is an irreducible component}\}.
$$ If $X$ is a scheme locally of finite type over a field, one has $$
\dim_x X=\max\{\dim T\mid T\subset X\text{ is an irreducible component passing through }x\}.
$$ (See 0A21 (5).) Now, if $X$ is just some arbitrary topological space, is the last formula true? My guess is that it is not, but I don't know right now what a counterexample might look like. Besides, can we show that one quantity is always bounded by the other one? Or are there examples of both types of behaviors? It seems one cannot obtain a bound: let $T\subset X$ be an irreducible component passing through $x$ , and let $U\subset X$ be an open neighborhood of $x$ . Then $\dim T$ and $\dim U$ are both greater or equal that $\dim T\cap U$ , but I don't see a way of comparing the values $\dim T$ with $\dim U$ .","['krull-dimension', 'general-topology', 'algebraic-geometry']"
4759328,Non-decimal base conversion algorithm,"I'm looking for a general method of radix conversion, that might be performed without using decimal system at all. Given a number $x$ in base $b_1$ , and I want to convert it to the base $b_2$ , where $x, b_1, b_2$ are positive integers and $b_1, b2 \ne 10$ . I know the question is very basic but unfortunately, all I have encountered till now were cases where one of the bases is 10, or the conversion method includes decimal system as a step. Also, I found some shortcut methods to convert between certain systems (where both bases are powers of $2$ ), but is there a general method? Do I miss something and there's a way to achieve it using Euclidean division or Horner's scheme?",['discrete-mathematics']
4759348,Is the following operator well-defined?,"Let $u \in L^2(\Omega \times (0,T))$ where $\Omega \subset \mathbb{R}^n$ is an open bounded set and $T>0$ . Consider the operator $$F(u)=\begin{cases} u \quad \text{ if } \vert u(x,t) \vert \leq k \text{ for almost every } (x,t)\in \Omega \times (0,T)\\
k \quad \text{ if } u(x,t) > k \text{ for almost every } (x,t)\in \Omega \times (0,T)\\
-k \quad \text{ if } u(x,t)<-k \text{ for almost every } (x,t)\in \Omega \times (0,T)
\end{cases}$$ Is $F$ well-defined for $u \in L^2(\Omega \times (0,T))$ ? Here is my approach: First, I had doubts that the regularity $L^2(\Omega \times (0,T))$ is not enough, since we cannot talk about pointwise values of $u$ in this case. But then again, I have tried to use the fact that since $u \in L^2(\Omega \times (0,T))$ then it is finite almost everywhere in $\Omega \times (0,T)$ and therefore it must satisfy one of the three cases indicated in the definition of $F$ and hence $F(u)$ is well-defined.","['lebesgue-measure', 'operator-theory', 'lebesgue-integral', 'functional-analysis', 'bochner-spaces']"
4759355,"$\forall k\in\mathbb{Z},\sum_{i=1}^n\lambda_i^k=(\sum_{i=1}^n\lambda_i)^k$ implies at most one number of $\{\lambda_i\}$ doesn't equal $0$?","Given $\{\lambda_i:\lambda_i\in\mathbb{R},1\leq i\leq n\}$ . If $$\forall k\in\mathbb{Z^+},\sum_{i=1}^n\lambda_i^k=\left(\sum_{i=1}^n\lambda_i\right)^k,$$ does this imply at most one number of $\{\lambda_i\}$ doesn't equal $0$ ? I attempted to use the Newton's identity and proved the case for $n=2$ and $n=3$ , however for $n=4$ it becomes complicated. When $n=4$ , it is (use $a,b,c,d$ for simplicity): $$4abcd=(abc+acd+bcd+abd)(a+b+c+d)-(ab+ac+ad+...)(a^2+b^2+c^2+d^2).$$ Thanks for the answer. I have one more question: can this be extended to $\{\lambda_i\}\in\mathbb{C}$ ?","['algebra-precalculus', 'linear-algebra', 'polynomials']"
4759358,"Permutation question on arrangement of 6 objects, grouped in pairs of 2, in 2 rows with 3 spaces in each row.","Q. Three couples sit for a photograph in 2 rows of three people each such that no couple is sitting in
the same row next to each other or in the same column one behind the order. How many
arrangements are possible? _ _ _ _ _ _ I used inclusion-exclusion for this. Let couples be $A_1B_1$ , $A_2B_2$ , $A_3B_3$ . Let the sets be defined as - $S_1$ = Cases in which at least 1 couple is in the same row, directly adjacent each other. $S_2$ = Cases in which at least 1 couple is in the same column. Total cases = $6!$ $n(S_1)$ = $4 \times 3 \times 2 \times 4!$ . Here 4 = Choosing of 2 horizontally-adjacent places, 3 = Three pairs of couples, 2 = Permutations of the horizontally-adjacent couple, 4! = No. of ways to arrange remaining 2 couples. $n(S_2)$ = $3 \times 3 \times 2 \times 4!$ . Here 3 = Choosing of 2 vertically-adjacent places, 3 = Three pairs of couples, 2 = Permutations of the vertically-adjacent couple, 4! = No. of ways to arrange remaining 2 couples. $n(S_1 ∩ S_2)$ = $3C2 \times 4 \times 1 \times 2! \times 2! \times 2!$ . Here $3C2$ = Choosing of two pairs of couples, 4 = Choosing of 2 horizontally-adjacent places, 1 = Forced selection of vertically-adjacent place, $2!,2!,2!$ = Permutations of horizontally-adjacent, vertically-adjacent, remaining two respectively. However it seems like $S_1∪ S_2$ is coming out to be greater than the universal set(total cases). I'm guessing I made some error in counting, or maybe in taking the sets. Help.","['permutations', 'inclusion-exclusion', 'combinatorics']"
4759432,Find topologies that characterise smoothness,"Is there a family of topologies $\tau_i$ on $\mathbb R$ such that for every map $f\colon \mathbb R \to \mathbb R$ we have $$
f \text{ is continuous w.r.t. }\tau_i \forall i \iff f \text{ is smooth?}
$$ By smooth we mean infinitely often differentiable here. Related is this article , proving that no topologies on $\mathbb R$ can characterize differentiability as continuity. However maybe if we consider a whole family of topologies?","['continuity', 'general-topology', 'smooth-functions', 'real-analysis']"
4759455,Pairwise intersecting sets have nonempty intersection,"Definition: Call a set of sets $\mathcal S$ cooperative when for every $\mathcal S'\subseteq\mathcal S$ , if every two elements in $\mathcal S'$ have nonempty intersection (so $\forall X,Y\in\mathcal S'. X\cap Y\neq\varnothing$ ) then $\mathcal S'$ has nonempty intersection (so $\bigcap \mathcal S'=\varnothing$ ). Intuitively, think of $\mathcal S$ as a set of agents (so an agent is an element $X\in\mathcal S$ ), and imagine that a set of agents can cooperate precisely when its intersection is nonempty.  Then $\mathcal S$ is cooperative when a group of agents $\mathcal S'\subseteq \mathcal S$ can cooperate, if and only if its members can cooperate pairwise. Note that is not required here that $\bigcap\mathcal S\neq\varnothing$ , since it need not be the case that every two elements in $\mathcal S$ has nonempty intersection. Examples: $\mathcal S=\bigl\{\{0\},\{1\},\{2\}\bigr\}$ is trivially cooperative, because no distinct elements intersect. $\mathcal S=\bigl\{\{0\},\{0,1\},\{0,2\}\bigr\}$ is trivially cooperative, because $\bigcap\mathcal S=\{0\}$ . $\mathcal S=\bigl\{\{0\},\{0,1\},\{0,2\},\{4\}\bigr\}$ is cooperative (even though $\bigcap\mathcal S=\varnothing$ ). $\mathcal S=\bigl\{ \{0,1\}, \{1,2\}, \{2,0\} \}$ is not cooperative, because if we take $\mathcal S'=\mathcal S$ then every pair of elements in $\mathcal S'$ intersects, but $\bigcap\mathcal S'=\varnothing$ . Has a theory of cooperative sets been explored, and if so where? Thank you. (This question is not identical to, but seems related to, Prove that the intersection of all the sets is nonempty. )","['elementary-set-theory', 'combinatorics', 'problem-solving']"
