question_id,title,body,tags
3158212,Notation of Probability space,"Assume we have red and a black cube (normal cubes with 6 sides). We roll these two dice. Is it true that the sample space is $$\Omega = \left\{ (r,s) \mid r \in \{ 1, \dots, 6\}, s \in \{ 1, \dots, 6\}  \right\}?$$ I'm now interested in the event $A =$ 'The red cube shows an even number' for example. Can I define the set $A$ like $$A = \{(r,s) | r \in \{2,4,6\}, s \in \{ 1 \dots 6\} \}?$$ How can I define $A$ 's probability density function and  probability measure for this probability space?","['notation', 'probability-theory', 'probability']"
3158219,Cohomology of union of quadric surfaces in $\mathbb{C}P^3$,"It is known that a degree 4 elliptic curve $E\subset \mathbb{C}P^3$ is the complete intersection of two irreducible quadric surfaces $E=Q_1 \cap Q_2.$ Can one compute the (co)homology groups (over $\mathbb{Q}$ coefficients) of the union of these quadric surfaces $H^*(Q_1 \cup Q_2)?$ E.g. by looking at the Mayer–Vietoris sequence, one can get that $H^0=\mathbb{Q},H^1=0,H^4=\mathbb{Q}^2,$ but two results $$H^2=\mathbb{Q}^6, H^3=\mathbb{Q}$$ and $$H^2=\mathbb{Q}^5, H^3=0$$ both fit in it (at least algebraically). Is there a way to find out which one is correct?","['quadrics', 'algebraic-geometry', 'homology-cohomology', 'algebraic-topology']"
3158229,can a Linear operator on infinite dimensional vector space have infinite eigenvalues?,"I know that for a finite dimensional vector space, the number of eigenvalues is at most the dimension of the vector space. Is there an example of an infinite dimensional vector space and a linear operator such that the linear operator allows infinite eigenvalues? I cannot think of such an example, but I was wondering if it is possible to construct one since I was also unable to prove why the number of eigenvalues would necessarily be finite in such a case.","['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
3158242,Probability that THHT occurs in a sequence of 10 coin tosses,"Assume we have a fair coin, and we throw the coin $10$ times in a row. I want to calculate the probability that the sequence 'tail, head, head, tail' occurs. So I think I can interpret this event as a binary number with $10$ digits. So $1$ means tail, $0$ means head. Therefore we have $2^{10} = 1024$ different outcomes of the $10$ throws. The sequence  'tail, head, head, tail' can start at $7$ different positions and so there are $7\cdot2^6 = 448$ different outcomes of the $10$ throws with the sequence 'tail, head, head, tail'. So the probability would be $\frac{448}{1024} = 0.4375$ . But I have a feeling there's something wrong?","['combinatorics', 'probability']"
3158262,Proving existence of integer solution in $x^2y^3+z^2=6$,"I've been working with this curve lately, but I haven't been able to solve this case yet. $$x^2y^3+z^2=6$$ I achived an algorithm to check for solutions depending on the values of $z$ and got that no solution exist when $|z|\leq 10^5$ . I could try augmenting the size of the search, but since the algorithm runs in $O(n^2)$ the amount of time it would take would jump to 1 hour just to check $|z|\leq 10^6$ and $5$ days for $|z|\leq 10^7$ . Right now I know that if a solution exist, then $x,z \equiv 1 \mod{2}$ and that $y \equiv 1 \mod{4}$ and that $2 \not | z,x,y $ and $3 \not | z,x,y$ , also $(xy,z)=1$ . The question is not to find the values of $x,y$ or $z$ , but to prove whether a solution over integers exist or not. Any help, hints or solutions would be appreciated. EDIT: I think that with python and multiprocessing I've check that for $|z|\leq 10^8$ there doesn't exist a solution. But I feel that I've coded something wrong since the program run much faster than it should have.","['number-theory', 'real-analysis']"
3158301,Are all finite groups Lie groups?,Is it possible to find an isomorphism from any finite group to a Lie group (which has manifold dimension 0 and equipped with the discrete topology)?,"['group-theory', 'finite-groups', 'lie-groups', 'differential-geometry']"
3158328,Lipschitz continuous and Jacobian matrix,"Consider a function $f:\mathbb{R}^n\longrightarrow\mathbb{R}^m$ with partial derivatives everywhere so that the Jacobian matrix is well-defined. Let $L>0$ be a real number. Is it true that: $$|f(x)-f(y)|\leq L|x-y|,\forall x,y \Longleftrightarrow |J_f(x)|_2\leq L,\forall x$$ where $|\cdot|$ denotes the euclidean vector norm and $|\cdot|_2$ the spectral matrix norm.","['jacobian', 'multivariable-calculus', 'lipschitz-functions']"
3158361,Proving $f_n(z)=\frac{nz}{1+n^3z^2}$ converges uniformly,"Show that the sequence of functions $f_n(z)=\frac{nz}{1+n^3z^2}$ converges uniformly on the set $E=[1,\infty]$ . $\lim_{n\to\infty}\frac{nz}{1+n^3z^2}=0$ so it converges pointwise to 0. So I am going to check if it converges uniformly to $0$ . $|\frac{nz}{1+n^3z^2}-0|\leqslant |\frac{nz}{n^3z^2}|=|\frac{1}{n^2z}|\leqslant\frac{1}{n^2}\to 0$ as $n\to\infty$ since $|\frac{nz}{1+n^3z^2}-0|$ is majored by $\frac{1}{n^2}$ that does not depend on $z$ . I conclude the function converges uniformly to $0$ . Questions: Is this proof right? If not why? Which are the alternatives? Thanks in advance!","['complex-analysis', 'sequences-and-series']"
3158363,How to compute a primitive element for the splitting field of $x^3-2 \in \Bbb{Q}[x]$?,"Let $\alpha:=\sqrt[3]{2}\in\mathbb{R}$ and $\omega:=e^{2\pi i/3}\in\mathbb{C}$ . Then the splitting field for the polynomial $x^3-2\in\mathbb{Q}[x]$ is $$\mathbb{Q}(\alpha,\omega\alpha,\omega^2\alpha)=\mathbb{Q}(\alpha,\omega).$$ Since $\mathbb{Q}$ has characteristic zero we know from the Primitive Element Theorem that there exists some $\gamma\in\mathbb{Q}(\alpha,\omega)$ with $$\mathbb{Q}(\alpha,\omega)=\mathbb{Q}(\gamma).$$ Question: How can I find a specific example of such an element $\gamma$ ?","['separable-extension', 'field-theory', 'galois-theory', 'abstract-algebra', 'galois-extensions']"
3158369,Do the primes contain an infinite almost arithmetic progression?,"The primes contain finite arithmetic progressions of arbitrary length, but not an infinite arithmetic progression. Say we define an almost arithmetic progression to be a sequence $a_k$ , $k \geq 0$ , such that there exist $a,d$ such that $a_k = a+kd + O(\sqrt{k})$ . Do the primes contain an infinite almost arithmetic progression? (The definition is ad hoc and just made up out of curiosity. I wrote the “error term” $O(\sqrt{k})$ in analogy to the expectation of a random walk. An obvious generalization of the question is to replace this with some other “small” error term like $O(\ln k)$ or whatever.)","['number-theory', 'prime-numbers']"
3158393,What is the space $L^p(\mathbb R)/_\sim$ where $f\sim g$ $\iff$ $f$ and $g$ has the same distribution?,"Let $L^1(\mathbb R)$ the set of function that are Lebesgue integrable. Define for $f$ and $g$ the relation $$f\sim g\iff m\{f\leq x\}=m\{g\leq x\},$$ where $m$ is the Lebesgue measure. It's an equivalence relation. How this equivalence relation is interesting ? In the probability point of view, it looks to be the space of random variable having the same law. Is this space important ? Commonly used ? Does someone knows reference for such a space ? For example, a problem I see is the fact that $X:\Omega \to \mathbb R$ and $Y: \Omega '\to \mathbb R$ can be random variable on $(\Omega ,\mathcal F,\mathbb P)$ and $(\Omega ',\mathcal F',\mathbb P')$ (different probability space), but saying that $X$ and $Y$ are equivalent looks strange. So maybe, even if they are on the same probability space, at the end, $X\sim Y$ is not really relevant and doesn't give us interesting information. What do you think ?","['measure-theory', 'probability', 'real-analysis']"
3158417,Prove that $DD' \parallel EE'$.,$BB'$ and $CC'$ are altitude of $\triangle ABC$ . Point $D'$ is outside $\triangle ABC$ such that $D'B \perp AB$ at $B$ and $D'C \perp AC$ at $C$ . $AD \cap B'C' = \{E\}$ and $AD' \cap BC = \{F\}$ . Prove that $DD' \parallel EE'$ . I tried using intercept theorem $\left(\dfrac{AE}{AD} = \dfrac{AE'}{AD'}\right)$ but I don't know how.,"['triangles', 'circles', 'geometry']"
3158419,Infinite group with the order of abelian subgroup bounded,"In Isaac's Finite Group Theory Page 28, it states： There exist infinite groups in which the abelian subgroups have bounded order. I fail to construct such group. In fact, I'm only able to deduce that the order of every element is bounded by an constant, which makes me feel hard to construct an example. Hope for an answer!","['group-theory', 'abstract-algebra', 'abelian-groups']"
3158452,"Is there a bijective, monotonically increasing, strictly concave function from the reals, to the reals?","I can't come up with a single one. The range should be the whole of the reals. The best I have is $\log(x)$ but that's only on the positive real line. And there's $f(x) = x$ , but this is not strictly concave. And $-e^{-x}$ only maps to half of the real line. Any ideas?","['real-numbers', 'recreational-mathematics', 'functions', 'real-analysis']"
3158454,Questions about the functions of the type $f:\Bbb Z_{40} \rightarrow \Bbb Z_{60}$,"Given the function $f:\Bbb Z_{40} \rightarrow \Bbb Z_{60}$ , how many of them are there such that $f([0]_{40})=[0]_{60}$ and $f([1]_{40})=[1]_{60}$ ? How many of them are homomorphism of additive groups? I have no idea how to answer to these questions. Any help? For the first question, is it a good way set up a system to know when $x\equiv(0 \mod 60) $ and $x\equiv(1 \mod 60)$ ?","['group-homomorphism', 'group-theory', 'functions', 'discrete-mathematics']"
3158522,A tricky looking functional equation,"Could someone please help me with this problem in ODE's, to show that for a continuous function $f$ which satisfies $f(x) = f(x+1) = f(x+\sqrt{2})$ $f$ is constant? I tried showing derivative exists and is zero but nothing came of it. Perhaps something to do with $ 1,\sqrt{2} $ being a rational and irrational, respectively. Thanks for all help.","['functions', 'ordinary-differential-equations']"
3158525,"Mapping to or from elements of a set, when that set is an element","This is an embarrassingly stupid question, but a colleague and I disagree, and it is relevant to what we are trying to do. The question is, suppose you have a pair of sets $A = \{1, 2, 3\}$ and $B = \{4, 5, 6\}$ . Now if you have a set $C = \{A, B\}$ , i.e., $C = \{\{1, 2, 3\}, \{4, 5, 6\}\}$ , then the set $C$ definitely has two elements, and cannot meaningfully be thought of as having six elements...right? So if you have another set $D = \{a, b, c, d, e, f\}$ , there is no surjective function from $C$ to $D$ because $C$ has two elements and $D$ has six, even though $C$ consists of elements that, as their own creatures, so to speak, contain three elements each? I know this question is painfully dumb, so here's a question that's hopefully a little more interesting and clarifies why this is an issue: When you consider the set $C$ , can a function, correspondence, or relation to or from $C$ ever meaningfully reference the elements contained inside $A$ and $B$ ? I have been saying no, but I am open to being told I am wrong about this.",['elementary-set-theory']
3158533,Negative intersections between distinct curves: geometric picture?,"On a smooth projective complex surface, if $C$ and $C'$ are distinct irreducible curves then their intersection is non-negative, $C \cdot C' \geq 0$ . I am interested in cases where negative intersections occur because we drop either distinctness or irreducibility. One side of this I am reasonably familiar with: it is common to consider self-intersections $C^2$ of irreducible curves. Non-negative self-intersections $C^2 \geq 0 $ still often occur: in these cases the curve can be deformed into another in the same linear equivalence class, and one takes the transverse intersection of this deformed curve with the original. In cases where $C^2 < 0$ the geometric picture is that no such deformation is possible. However if instead of dropping distinctness we drop irreducibility but keep distinctness , one again finds examples of negative intersections, $C \cdot C ' < 0 $ . I am wondering what is the geometric picture in this case ?","['algebraic-geometry', 'intersection-theory']"
3158538,Natural logarithm with absolute value: Can I cancel the absolute value?,"I was calculating basic rational integrals and came up with this kind of problem. I have this expression: $$2\ln|x|$$ I can re-write it down like that: $$\ln{x^2}$$ and thus cancel the modulus. The question is, what about $\frac{1}{2}\ln|x|$ ? Should I write it down like this: $$\ln{\sqrt{x}}$$ or like this: $$\ln{|\sqrt{x}|}\,?$$","['algebra-precalculus', 'absolute-value', 'logarithms']"
3158546,"Showing there is a unique group table for $\{1, a,b,c\}$ such that there is no element of order $4$. [duplicate]","This question already has answers here : Cayley tables for two non-isomorphic groups of order 4. (2 answers) Closed 5 years ago . Assume $G = \{1, a,b,c\}$ is a group of order $4$ with identity $1.$ Assume also that $G$ has no elements of order $4$ . Show that there is a unique group table for $G$ . Also show that $G$ is abelian. If $G$ is abelian, then the group table matrix must be symmetric. How can I introduce a binary function and show it? I am new in this field, so I am not so familiar. I have proved many other exercises, but it is a little tough (for me). Can you please help? Edit: I know every element has order $\leq 3$ , but I do not understand how I will proceed.","['matrices', 'group-theory', 'abstract-algebra', 'abelian-groups']"
3158634,What's the cross product in 2 dimensions? [duplicate],"This question already has answers here : Is the vector cross product only defined for 3D? (7 answers) Closed 5 years ago . The math book i'm using states that the cross product for two vectors is defined over $R^3$ : $$u = (a,b,c)$$ $$v = (d,e,f)$$ is: $$u \times v = \begin{vmatrix}
\hat{i} & \hat{j} & \hat{k} \\
a & b & c \\
d & e & f \\
\end{vmatrix}
$$ and the direction of the resultant is determined by curling fingers from vector v to u with thumb pointing in direction of the cross product of u x v. Out of curiosity, what's the cross product if u and v are defined over $R^2$ instead of $R^3$ instead: $$u = (a,b)$$ $$v = (d,e)$$ Is there a ""degenerate"" case for the cross product of $R^2$ instead $R^3$ ?  like this is some type of 2x2 determinant instead? for instance if had a parameterization: $$\Phi(u,\ v) = (\ f(u),\ \ g(v)\ )$$ and needed to calculate in $R^2$ : $$
D = \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg|
$$ There are plenty of examples in the book for calculating the determinate D in $R^3$ but none at all for $R^2$ case. As in: $$
\iint_{V} f(x,y) dx\ dy = \iint_{Q} f(\Phi(u,v) \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg|
$$ $$
\Phi(u,v)=(2u \cos v,\ \ u \sin v)
$$","['multivariable-calculus', 'vectors']"
3158687,Limits and Infinite Integration by Parts,"It is well known that $$\int \frac{\sin(x)}{x} \,dx$$ cannot be expressed in terms of elementary functions. However, if we repeatedly use integration by parts, we seem to be able to at least approximate the integral through the formula $$\int f(x) \,dx \approx \sum_{n=1}^a \frac{(-1)^{n-1}\cdot f^{(n-1)}(x)\cdot x^n}{n!}$$ where $a \in \mathbb{N}$ . When plugging this in to a graphing calculator, it converges, but very slowly. It also tends to converge more quickly for functions that tend to $0$ as $x \to \infty$ .  My guess is that $$\int f(x) \,dx = \lim_{a\to\infty}\sum_{n=1}^a \frac{(-1)^{n-1}\cdot f^{(n-1)}(x)\cdot x^n}{n!}$$ at least on a certain interval, but I am uncertain where to look  to learn more about these series. Any ideas? Thanks!","['integration', 'calculus', 'sequences-and-series', 'real-analysis']"
3158695,"If $|f| < 1$, compute $\lim_{n\to \infty} \int \left ( \frac{f ^n}{1 + n |f|} \right )\, d\mu.$","Let $f: X \to \mathbb C$ be integrable in a measure space $(X, \mathfrak M, \mu)$ , i.e. $\int |f| \, d\mu < \infty$ . Suppose that $|f(x)| \leq 1$ for all $x \in X$ . How can one compute the limit $$ \lim_{n\to \infty} \int \left ( \frac{f ^n}{1 + n |f|} \right )\, d\mu \quad ? $$ My attempt: I want to find a Lebesgue integrable $g$ that dominates the sequence $f_n = \frac{f ^n}{1 + n |f|}$ and, then, I would conclude that $$ \lim_{n\to \infty} \int \left ( \frac{f ^n}{1 + n |f|} \right ) d\mu = \int \left ( \lim \frac{f ^n}{1 + n |f|} \right ) d\mu = 0, $$ since $|f| < 1$ implies $\lim \frac{f ^n}{1 + n |f|} = 0$ . My problem is in find such function $g$ , I can see that $|f_n| < 1$ for each $n$ , however the function $g = 1$ does not need to be Lebesgue integrable since $\mu(X)$ maybe $\infty$ . Help?","['integration', 'lebesgue-integral', 'real-analysis']"
3158750,"When is ""base changing morphisms of schemes"" surjective?","Suppose $X, Y$ are $S$ -schemes and $S'\to S$ is a morphism. Every $S$ -morphism $f:X\to Y$ gives rise to a $S'$ -morphism $f':X'\to Y'$ (where $f'$ , $X'$ , $Y'$ are the base changes). Under which assumptions is $Mor_S(X,Y)\to Mor_{S'}(X',Y')$ surjective? Is it possible to specify properties of $X$ and/or $Y$ and/or $S'\to S$ which guarantee surjectivity? (Sorry for the vague question. I don't have a particular setting in mind, any example is appreciated.)","['algebraic-geometry', 'schemes']"
3158768,Banach-Tarski nonparadox: Reassembling a ball into two balls with a total volume equal to the original volume,"The Banach-Tarski paradox states that a ball can be partitioned into finitely many pieces which can be rotated and translated into two balls identical to the original one. But can a ball be partitioned into finitely many pieces which can be rotated and translated to form two balls with non-zero radiuses such that the sum of their volumes is equal to the original volume? If yes, can this be done without choice?","['axiom-of-choice', 'measure-theory', 'set-theory']"
3158770,How to self study topology?,"I'm a first year undergraduate student and I'm a math major. Currently, I'm taking an intro to analysis class and a linear algebra class. However, I often feel constrained by what I do in class and feel like exploring topics in math beyond class. I'm intrigued by topology but haven't had any prior exposure to it. At this stage, considering that my knowledge of both analysis and linear algebra is fairly elementary, does it make sense to delve into higher-level topics like topology? What are the pre-requisites for introducing oneself to topology? And if you recommend that I go on and try to self-learn some topology, what are some resources I can/should use? In general, if not topology, at this stage, what beyond class can/should I do? Thanks!","['self-learning', 'general-topology', 'soft-question', 'education']"
3158835,Loop Space of $BU \times \mathbb{Z}$,"I have a question about an argument that occurred in the discussion about consequences of Bott periodicity in A Concise Course in Algebraic Topology by P. May on page 207. Here is the excerpt: Could anybody explain the ""little argument with $H$ -spaces"" which May has in mind to show that $\Omega^2(BU \times \mathbb{Z}) $ is equivalent to $(\Omega^2_0BU) \times \mathbb{Z}$ as $H$ -spaces? My considerations: As explained above the loop space ""sees"" only the component of the base point so $\Omega^2BU= \Omega^2_0BU$ . The problem reduces to two questions: Does $\Omega^2$ respect products like $\pi_k(-)$ ? And which role does the fact $\pi_2(BU) = \mathbb{Z}$ play? This is a statement about homotopy classes of loops $\Omega^2(BU)$ but in the considerations above we haven't passed to homotopy classes so here we would ""lose"" some information. Does anybody see the correct argument? Thanks in advance.","['loop-spaces', 'topological-k-theory', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
3158865,Calculating Probability of a given X range if X is a continuous random variable with a given PDF.,"This question was given to me as a review for an upcoming exam. Find $P(2 \leq X \leq 3)$ if $X$ is a continous random variable with pdf: $$f_X(x) =
 \begin{cases} 
      xe^{-x} & x\geq 0 \\
      0 & x \lt 0
   \end{cases}
$$ My work: $$P(2 \leq X \leq 3) = \int_{2}^{3}xe^{-x}dx$$ Integration by parts: $u=x,u'=1,v'=e^{-x},v=-e^{-x}$ $$\int_{2}^{3}xe^{-x} dx = -xe^{-x}\Big|^3_2-\int_{2}^{3}-e^{-x}dx\\
=\frac{-4}{e^3} + \frac{3}{e^2} \approx 0.2069$$ Did I miss anything or do something incorrectly? Also, I had a quick question regarding a different case, let's say problem was asking for $P(-1 \leq X \leq 3)$ instead, would we do this instead?: $$P(-1 \leq X \leq 3)= \int_{-1}^{0}0 dx + \int_0^3xe^{-x}dx$$ Obviously the left integral will be 0 in this case, I'm just wondering if we split up the integrals with addition if the range we are looking for is in two different domains of the PDF.","['statistics', 'probability']"
3158879,How to compute the gradient $\nabla_W \left( x^TW^{-T}W^{-1}x \right)$?,Calculate the following gradient $$\nabla_W \left( x^TW^{-T}W^{-1}x \right)$$ where $W$ is a $\mathbb{R}^{d×d}$ matrix and $x$ is a $\mathbb{R}^d$ vector. The result should be a $\mathbb{R}^{d×d}$ matrix. I wonder whether there is a clean and compact form of the result. I first tried to write this as $$2(W^{-1}x)\frac{d(W^{-1}x)}{dW}$$ but the latter one is a tensor (actually it should be $2 \sum_{i=1}^d(W^{-1}x)_i\frac{d(W^{-1}x)_i}{dW}$ but the next step is messy.,"['matrices', 'matrix-calculus', 'derivatives']"
3158895,Products and the axiom of choice,"Here: Universal property of the direct product, proof verification Matematleta noted in the comments, that the definition of the product uses the axiom of choice by default.
Why is that? The definition i am looking at is simply: For sets $X_1, X_2, \dotso, X_n$ is $\prod_{i= 1}^n X_i:=\{(x_1,\dotso, x_n)| x_1\in X_1,\dotso, x_n\in X_n\}$ Analogously for an arbitrary index set. Also I always wondered, why you need the axiom of choice to state, that the product of not empty sets is not empty.","['elementary-set-theory', 'definition']"
3158927,The probability of Bus A arriving before Bus B,"Bus A arrives at a random time between 2pm and 4pm, and Bus B arrives at a random time between 3pm and 5pm. What are the odds that Bus A arrives before Bus B? My understanding is that since Bus B cannot possibly arrive between 2 and 3, we can only talk about the time between 3 and 4 pm, when there is an equal probability for both buses arriving. But in this case, the probability of Bus A arriving before B is 50%. No? What am I missing here? Or I should look at the entire timeline, 2 pm - 5 pm? But then in this case, it is still 50%. Where is my thinking wrong?",['probability']
3158979,How can I minimize the real part of the roots of this function involving both $x$ and $e^x$ terms?,"The question I have a function $D(s) = s^2 + c s + k + K_d s e^{-s} + K_p e^{-s}.$ The values of $c$ and $k$ are fixed, but I can choose $K_d$ and $K_p$ . How do I choose these two values in order to minimize $\max \{ \operatorname{Re}(s) : D(s) = 0 \}$ ? Motivation I'm writing an autopilot program for a flight simulation video game. Given information about the current state of the aircraft, it attempts to calculate flight control inputs which will put it into a desired state. I'm using PID controllers to accomplish this. However, it's difficult to make these work in practice. The main reason is that aircraft are inherently oscillatory in several ways, and poorly chosen PID parameters exacerbate these oscillations. Attempting to find PID parameters which control the aircraft effectively without producing oscillations is very tedious. In order to try to calculate parameters automatically, I've decided to examine a ""toy"" control problem whose behavior is similar to the aircraft in the video game. Problem In the ""toy"" problem, the system is a damped harmonic oscillator. The oscillator is driven by a PD controller which attempts to drive the position of the oscillator to $0$ . However, the controller suffers a delay of $1$ second. The differential equation describing this system is $$f''(t) = - c f'(t) - k f(t) - K_d f'(t - 1) - K_p f(t - 1).$$ Here, the $c$ term represents the damping force and the $k$ term represents the restoring force. The $K_d$ and $K_p$ terms form the driving force; the $K_d$ term attempts to slow the motion of the system, and the $K_p$ term attempts to push the position of the system towards $0$ . The constants $c$ and $k$ cannot be changed, but we are able to select $K_d$ and $K_p$ . We can find the Laplace transform of $f$ : $$s^2 F(s) - s f(0) - f'(0) = -c (s F(s) - f(0)) - k F(s) - K_d (s e^{-s} F(s) - f(-1)) - K_p e^{-s} F(s)$$ $$s^2 F(s) + c s F(s) + k F(s) + K_d s e^{-s} F(s) + K_p e^{-s} F(s) = s f(0) + f'(0) + c f(0) + K_d f(-1)$$ $$F(s) = \frac{s f(0) + f'(0) + c f(0) + K_d f(-1)}{s^2 + c s + k + K_d s e^{-s} + K_p e^{-s}}$$ If I understand the Laplace transform correctly, the system converges whenever all of the poles of $F(s)$ have a negative real part; and it suffers from divergent oscillations whenever at least one of the poles of $F(s)$ has at least one positive real part. So, the behavior is determined by the rightmost root of $s^2 + c s + k + K_d s e^{-s} + K_p e^{-s}$ . If the real part of this root is negative, then the system will converge. Furthermore, the closer the real part is to negative infinity, the more quickly the system will converge. So, we want to choose $K_d$ and $K_p$ so as to make the real part of the rightmost root as small as possible. Hence, the question at the top of this post. My thoughts The equation $s^2 + c s + k + K_d s e^{-s} + K_p e^{-s} = 0$ doesn't look like it admits an elementary solution. I could probably find its roots using some type of numerical search; is this the best way? Even if I had a fast way to calculate the solutions to this equation, I'd then have to perform another search in order to find the one which minimizes the maximum real part. If I had to perform nested searches, then the whole process could get very slow. Based on playing around with the function in graphing calculators, it looks like it usually has three roots near the origin (not necessarily distinct). Does this function always have exactly three roots near the origin when $K_d$ and $K_p$ are not both zero? If a numerical search is the best way to go for both parts of the problem (locating the roots and minimizing them), maybe the best approach is going to be to use gradient descent in the outer loop to minimize the roots, and Newton's method in the inner loop to locate the roots.","['complex-analysis', 'optimization', 'control-theory']"
3159078,How to prove that $\frac{a-b}{\sqrt{1+a^2}\sqrt{1+b^2}} < \arctan{a}- \arctan{b}$ when $0<b<a$?,"$$\frac{a-b}{\sqrt{1+a^2}\cdot\sqrt{1+b^2}} < \arctan{a}- \arctan{b}$$ when $0<b<a$ This might relate to the mean value theorem, but I just can't prove it. This question was put on hold as off-topic, I couldn't understand the reason why people voted to close it so I add more details and try to re-open it. After I learned the course of the mean value theorem, my teacher asked us to prove that $$\frac{a-b}{\sqrt{1+a^2}\cdot\sqrt{1+b^2}} < \arctan{a}- \arctan{b} < a-b $$ I found by using the equation $\arctan a - \arctan b = \frac{1}{1+\xi^2}(a-b)$ I could easily prove that $$\frac{a-b}{1+a^2} < \arctan{a}- \arctan{b} < a-b $$ the right inequality related to several questions on StackExchange so I omitted it. I also tried to use the inequality $\frac{x}{1+x^2} < \arctan x < x$ combined with the equation $\arctan x - \arctan y = \arctan{\frac{x-y}{1+xy}}$ to prove this question but I failed again. So I went to check if this question is correctly written and my teacher said ""Yes, nothing wrong with it"". I found the comment of @YuDing is more useful than the answer of @AdamLatosiński so I didn't tick the answer and I also couldn't tick the comment because it's just a comment. The comment of @MartinR and the answer of @Matteo provided us a great perspective to solve the question. I ticked the answer for the reason that I couldn't tick a comment. Maybe because the answer is on a purely geometrical perspective so my question is off-topic? I hope OP could re-open this question because I really appreciate everyone's efforts here. Thank you all.","['calculus', 'algebra-precalculus']"
3159164,Minimal set of edges.,"There is given complete graph $K_{n}$ . Let $A(n,k)=$ minimal (in respect of its size) subset of edges of $K_{n}$ such every $k$ -clique has at least one edge in common with this set. Find formula for $|A(n,k)|$ I wrote down some examples such $|A(4,3)|=2$ But afterall i can not see the pattern. Thank you in advance for help.","['graph-theory', 'combinatorics']"
3159210,Intuition behind how the Cauchy-Schwarz inequality's proof was obtained,"I'm studying multivariable calculus. Usually, when I study, I go through a book until I find a theorem, and then try to prove it. I was unable to, so I read the proof, which is the following: Let $x, y \in \mathbb{R}^m, \alpha \in \mathbb{R}$ . Then $(x+\alpha
 y)\cdot(x+\alpha y) = \vert \vert x+\alpha y\vert\vert^2 \geq0$ .
 Using the properties a the inner product we get: $(x+\alpha y)\cdot(x+\alpha y) = x\cdot x+\alpha x\cdot y +
 \alpha y\cdot x + \alpha^2y\cdot y
 = \vert\vert x\vert\vert^2+2(x\cdot y)\alpha + \alpha^2\vert\vert y\vert\vert^2 \geq 0$ . That last inequality is true iff the discriminant of the polynomial with respect to $\alpha$ is less than or equal to 0. Therefore $\vert
 x\cdot y\vert - \vert \vert x\vert\vert^2\vert\vert y\vert\vert^2
 \leq 0$ , from which comes the Cauchy-Schwarz inequality. Q.E.D I can follow every step of the proof. I also get the intuition of why the inequality should be true. However, the proof seems ""empty"" to me. I don't understand what someone who wanted to prove this would do to find it. What's the intuition behind using $x+\alpha y$ ? The reason I ask this is because, after I read the proof, the way used to prove it was so beyond everything that I tried, that I am almost sure that I'd never be able to prove this on my own. How to deal with these kind of situations?","['multivariable-calculus', 'soft-question', 'intuition']"
3159219,Limit of geometric series sum when $r = 1$,"I'm currently learning the prove of sum of geometric series on Khan Academy. I understand the behaviour of the function when $|r| > 1$ , when $|r| < 1$ , when $r = 0$ and when $r = -1$ , but I am a bit confused by its behaviour when $r = 1$ . The narrator said that when $r = 1$ , the limit function is undefined because the denominator of the limit function would be $0$ , and the behaviour of the limit function is UNDEFINED, which I do understand. My confusion arises when I tried to substitute $r = 1$ into the original function for sum of geometric series, if $r = 1$ , then every term would equal to a, and the sum of the geometric series would approach infinity, so its behaviour is DEFINED. So when $r = 1$ , behaviour of sum function is DEFINED, but behaviour of limit function is UNDEFINED, but sum function also equal to limit function?! This is causing me so much confusion.","['limits', 'calculus', 'summation', 'sequences-and-series']"
3159233,"Is there a closed form for the recurrence $V_{n+1}={V_n+\Delta V\over 1+{V_n\cdot \Delta V/C^2}}$, for constants $\Delta V$ and $C$?","I was wondering if the following recurrence formula has a closed form: $$V_{n+1}={V_n+\Delta V\over 1+{V_n\cdot \Delta V\over C^2}}$$ where $\Delta V$ and $C$ are positive constants, $V_n$ is the velocity of the $n$ -the inertial frame and the primary velocity $V_0$ is given (take it $0$ if needed). Attempt This sequence obviously tends to $C$ (the speed-of-light supremum of speeds of observations), so I naturally tried to crack it using $$e_n=V_n-C$$ but I failed. Any idea is appreciated. Note The above rule determines the Relativistic Velocity-addition Formula where $V_n$ is supposed to be the velocity of the inertial frame $2$ that is moving with respect to us (inertial frame $1$ ) and $\Delta V$ is an increase in the speed of the moving object (or we can assume it as the relative speed of object in the inertial frame 2). My work basis is the Lorentz Transformation .","['special-relativity', 'recurrence-relations', 'sequences-and-series']"
3159242,Evaluate $\int^{2}_{0}\frac{\tan^{-1}(x)}{1+4x}\mathrm dx$,"Evaluate $\displaystyle \int^{2}_{0}\frac{\tan^{-1}(x)}{1+4x}\mathrm dx$ My effort: \begin{align*}
I(a)&=\int^{2}_{0}\frac{\tan^{-1}(ax)}{1+4x}\mathrm dx\\
I'(a) &= \int^{2}_{0}\frac{x}{(1+4x)(1+a^2x^2)}\mathrm dx\\
I'(a) &= \frac{1}{4}\int^{2}_{0}\frac{(1+4x)-1}{(1+4x)(1+a^2x^2)}dx\\
I'(a) &= \frac{1}{4a}\tan^{-1}(2)-\frac{1}{4}\int^{2}_{0}\frac{1}{(1+4x)(1+a^2x^2)}dx
\end{align*} Then how to proceed? Thank you.","['integration', 'calculus', 'definite-integrals']"
3159263,"Convergence of $\sqrt[k]{z+\sqrt[k]{z+\sqrt[k]{z+\cdots}}}$, where $z=(1+x)^k-(1+x)$","If one writes $$1+x=\sqrt{(1+x)^2}=\sqrt{1+2x+x^2}=\sqrt{x+x^2+(1+x)}$$ then one has a recursive definition of the function $1+x$ which can be used to write $1+x$ as the infinite nested radical: $$1+x=\sqrt{x+x^2+\sqrt{x+x^2+\sqrt{x+x^2+\sqrt{\cdot\cdot\cdot}}}}$$ But this definition relies on the fact that $$
1+x=\sqrt{(1+x)^2}
$$ which is only true for $x \ge-1$ . In general one could state that $$
1+x=\sqrt[n]{(1+x)^n}=\sqrt[n]{(1+x)^n-(1+x)+\sqrt[n]{(1+x)^n-(1+x)+\sqrt[n]{\cdots}}}
$$ But the RHS does not converge to $1+x$ for most values of $x\in\mathbb{C}$ . So, my question is, what is the actual closed form of the following function? For what values of $x\in\mathbb{C}$ does the following function converge? $$
\sqrt[k]{(1+x)^k-(1+x)+\sqrt[k]{(1+x)^k-(1+x)+\sqrt[k]{\cdots}}}
$$ If unclear the above nested radical can be defined by $\lim_{n\to\infty}a_n$ where $$
a_1=\sqrt[k]{(1+x)^k-(1+x)},\quad
a_n=\sqrt[k]{(1+x)^k-(1+x)+a_{n-1}}.$$","['nested-radicals', 'radicals', 'complex-analysis', 'functions', 'convergence-divergence']"
3159280,Find limit $\lim_{x \rightarrow 0^+} \frac{x}{x^x-1}$,Find $$\lim_{x \rightarrow 0^+}(x^{x-1}-x^{-1})^{-1}$$ my approach Firstly I should represent factor in more intuitive form $$\lim_{x \rightarrow 0^+}\frac{x}{x^x-1} $$ I know that $$\lim_{x \rightarrow 0^+} x^x = 1$$ so I suspect that I have expression of type $ \frac{0}{0} $ Ok. Now I am goind to find $$\lim_{x \rightarrow 0^+} \frac{f'(x)}{g'(x)}= \lim_{x \rightarrow 0^+}\frac{1}{(\ln (x) +1)\cdot x^x}$$ Ok but know I have no idea how I can deal with that because $ln x\rightarrow -\infty$ when $x\rightarrow 0^-$ and from other hand $x^x$ is going to $\infty$ and I can't use there Hospital rule again..,"['limits', 'real-analysis']"
3159300,Find $\lim\limits_{x\to 0}\left (\frac{1^x+2^x+3^x+\dots+n^x}{n} \right)^{\frac1x}$,"Consider the following expression. $$\lim\limits_{x\to 0} \left (\frac{1^x+2^x+3^x+\dots+n^x}{n} \right )^{\frac 1 x}$$ How to solve this? Let $y= \left (\frac {1^x+2^x+\cdots +n^x} {n} \right)^{1/x}$ I tried taking $\ln$ on both sides. We get that $$\ln(y)=\frac{1}{x}\ln \left (\frac {1^x+2^x+\cdots +n^x} {n} \right ).$$ Taking $\lim$ on both sides we get $$\ln(y)=\lim_{x\to 0}\frac{1}{x}\ln \left (\frac {1^x+2^x+\cdots +n^x} {n} \right ).$$ Now applying the LH rule, we get $$\ln(y)=\lim_{x\to 0}\frac{n}{1^x+2^x+\cdots +n^x}({1^x\ln(1)+\cdots +n^x\ln(n)})$$ Is this a right way to go?","['sequences-and-series', 'real-analysis']"
3159349,Does there always exist a Chebyshev center of three constant weight points in $\mathbb F_2^n$ which is equidistant?,"Given three distinct points $x_1,x_2,x_3$ in $\mathbb F_2^n$ (endowed with the Hamming metric $d(\cdot,\cdot)$ ) with the same (but arbitrary) Hamming weight, the Chebyshev radius of them is defined as $$\min_{y\in\mathbb F_2^n}\max_{i\in\{1,2,3\}}d(x_i,y),$$ i.e., the radius of the smallest ball containing all of $x_1,x_2,x_3$ .
The Chebyshev centers of $x_1,x_2,x_3$ is the set of $y$ 's which achieve the optimal value of the above optimization problem. Note that such $y$ is not necessarily unique. However, according to experiments, it seems that the set of Chebyshev centers always includes a $y$ such that $d(x_1,y)=d(x_2,y)=d(x_3,y)$ . Let's call such $y$ 's equidistant. Note that this is not true if we do not require constant weight. For instance, the Chebyshev center of $00,01,10$ is $00$ , which is not equidistant. And $00$ happens to be the unique center in this case. My question is: how can one prove or disprove the above conjecture? I ran a program for decent $n$ 's, such as $n=7,8,9,10$ and did not find counterexamples. I tried to prove it using the method of types, known in information theory, but failed. I also tried proof by contradiction but also failed. Ideally we want to get a contradiction with the constant weight assumption, which is the only useful information we have.","['discrete-geometry', 'coding-theory', 'combinatorics', 'extremal-combinatorics']"
3159350,Find Mistake: Independence of two Events,"Assume we have a black and a red cube with 6 sides. We definite two  events A = ""the black dice shows 5"", B = ""The product of the number of pips is a prime number"". We roll the dice. So $P[A] = \frac{1}{6}$ and $P[B] = \frac{1}{6}$ right ? Now i want to check, if  A and B are independent. So $P[A \cap B] = \frac{1}{36} = \frac{1}{6} * \frac{1}{6} = P[A]P[B]$ so A and B are independent. My instincts tell me the events are dependent. Where is my mistake ?","['independence', 'probability-theory', 'probability']"
3159363,If $B$ is nilpotent and $AB=BA$ then $\det(A+B) = \det(A)$ [duplicate],"This question already has answers here : If $\,A^k=0$ and $AB=BA$, then $\,\det(A+B)=\det B$ (3 answers) Closed 5 years ago . The following stumps me: Let $\mathbb K$ be a field. Let $A, B \in \mathbb K^{n \times n}$ where $B$ is nilpotent and commutes with $A$ , i.e., $A B = B A$ . Show that $$ \det(A+B)=\det(A) $$ I have no idea how to approach this I thought perhaps raise both sides to a power but nothing works.
Thanks for all help.","['matrices', 'nilpotence', 'determinant', 'linear-algebra']"
3159427,Why is a symmetric relation defined: $\forall x\forall y( xRy\implies yRx)$ and not $\forall x\forall y (xRy\iff yRx)$?,"Why is a symmetric relation defined by $\forall{x}\forall{y}(xRy \implies yRx)$ and not $\forall{x}\forall{y}(xRy \iff yRx)$ ? 
(I have only found a couple of sources that defines it with a biconditional) For example, according to Wolfram : A relation $R$ on a set $S$ is symmetric provided that for every $x$ and $y$ in $S$ we have $xRy \iff yRx$ . But the majority of books defines it the other way.
And I think I agree with the second definition. Because if we use the first definition with "" $\implies$ "", we know the truth table of the implication in particular $P \implies Q$ is true when $P$ is false and $Q$ is true. That means in the context of symmetric relation that $(x,y) \notin R \implies (y,x) \in R$ is true. And the example $A = \{1,2,3,4\}$ with relation $R = \{(2,1),(3,1),(4,1)\}$ satisfies the definition because $(x,y) \notin R \implies (y,x) \in R$ is true. And for me it's weird that this case is considered symmetric.
Or maybe I have a profound confusion with the concept.
 I would like that you guys help me clarify. *Sorry for my grammar I'm not a native english speaker.","['definition', 'relations', 'logic', 'discrete-mathematics']"
3159447,How does equalizer (category theory) characterize set theory equalizer?,"In the category Set, the equalizer of $f: A \to B$ is the (largest) set of elements $x \in A$ such that $f(x) = g(x)$ . But in category theory, this is generalized to a map $E \xrightarrow{e} A$ such that $f\circ e = g\circ e$ and such that the UMP holds. But looking back at the set example, can't you take a smaller subset than the largest and surely that would (the inclusion) be a map $e$ .  In other words, how does being an equalizer ensure that $E$ is indeed the ""largest"" object?","['functions', 'abstract-algebra', 'category-theory']"
3159473,Higher cup-1 product of coboundaries is also a coboundary?,"In the cohomology or the group cohomology theory, suppose $\mu_1$ and $\mu_2$ are coboundaries of arbitrary dimensions, $$
\mu_1=\delta \eta_1
$$ $$
\mu_2=\delta \eta_2
$$ where $\eta_1$ and $\eta_2$ are their lower dimensional split cochains. Could we prove that the higher cup 1 product is also a coboundary? $$
\mu_1 \cup_1 \mu_2=(\delta \eta_1)\cup_1 (\delta \eta_2)=\delta(\beta)?
$$ If so, how do we write this $\beta$ explicitly? Is a Higher cup-1 product of coboundaries also a coboundary?","['group-cohomology', 'general-topology', 'homology-cohomology', 'algebraic-topology', 'simplicial-complex']"
3159522,A box has $4$ red and $20$ white balls. A person takes $10$ balls. What is the probability that all or none of the red balls were taken?,"A box has $24$ balls, $4$ red and $20$ white. One person takes $10$ balls and the second the remaining $14$ . What is the probability that one of the two people picked up the $4$ red ones? I don't understand why is this correct. $$\frac{{20 \choose 6}+{20 \choose 10}}{24 \choose 10} $$","['combinatorics', 'probability']"
3159555,"$X_1, X_2, ..., X_n \sim Exp(\lambda)$, what's the joint distribution of $X_1, X_1+X_2, ..., X_1+X_2+...X_n$ and is it a uniform ordered distribution?","To elaborate on the title, here is the entire problem: Let $X_1, X_2, ..., X_n \thicksim Exp(\lambda)$ be an independent sample. What's the joint distribution of the sequence of $X_1, X_1 + X_2, ..., X_1 + X_2 + ... + X_{n-1}$ with the condition of $X_1 + X_2 + ... + ... + X_n = t$ ? And is this joint distribution equal to an $n-1$ member ordered uniform ( $Unif[0,t]$ ) sample's joint distribution, meaning that: If $Y_1, Y_2, ..., Y_{n-1} \thicksim Unif[0,t]$ independent sample, and we order them: $Y_1^*, Y_2^*, ..., Y_{n-1}^*$ , then are these equal: $$F_{X}(x_1,...,x_{n-1}) = \Bbb{P}(X_1 < x_1, X_1 + X_2 < x_2, ...,~~~ X_1 + X_2 + ... + X_{n-1} < x_{n-1} | X_1 + X_2 + ... + X_n = t) \stackrel{?}{=} \Bbb{P}(Y_1^* < x_1,  Y_2 < x_2, ..., Y_{n-1}^* < x_{n-1}) = F_{Y^*}(x_1,...,x_{n-1})$$ where $F_X$ is the joint distribution function of the $X_1, X_2, ...,X_1 + X_2 + ... + X_{n-1}$ sample with the condition of $\sum_{i=1}^n{X_i} = t$ and $F_{Y^*}$ is the joint distribution function of the $Y_1^*, Y_2^*, ..., Y_{n-1}^*$ sample. If so, prove it; if not, disprove it. The problem is...: ...that $X_1, X_1 + X_2, ..., X_1 + X_2 + ... + X_n$ aren't independent, so calculating the joint distribution function is hard, especially with a condition. Ordered samples also follow a Beta distribution, which is generally tough to deal with: $$\forall k \in \{1,...,n\}: \quad Y_k^* \thicksim \frac{1}{t}Beta(n,n-k+1)$$ Here is what I've tried so far: 1. Introduce new variables: $$A_1 = X_1 \\
A_2 = X_1 + X_2 \\
\vdots \\
A_n = X_1 + X_2 + \dots + X_n$$ This way, we can write up the $X$ 's like so: $$X_1 = A_1 \\
X_2 = A_2 - A_1 \\
X_3 = A_3 - A_2 \\
\vdots \\
X_n = A_n - A_{n-1}$$ We can also calculate the individual distributions of these $A$ 's: $$\forall k \in \{1,...,n\} \quad A_k \thicksim Exp\left(\frac{\lambda}{k}\right)$$ But this didn't lead me much further, since we still can't write up the joint distribution functions of $A$ 's or $X$ 's since they're not independent. 2. I tried thinking outside the box: $X_1 + X_2 + ... + X_k$ could mean the arrival time of a truck, and if they're from an exponential distribution, then their arrival times are expected to be uniform. However, expected value says very little about joint distribution, plus this wouldn't be a very mathematically appropriate proof. Can anyone lead me on the correct path?","['statistics', 'uniform-distribution', 'probability-distributions', 'exponential-distribution', 'probability-theory']"
3159565,"If series is not uniformly convergent, can we still integrate term by term?","We know that if $\sum a_nx^n$ converges uniformly, then we can integrate term by term. So this is just a sufficient condition, right? Does there exists a series not converging uniformly and still we can perform term by term integration? I am looking for some general results in this direction.","['analysis', 'real-analysis', 'calculus', 'functional-analysis', 'sequences-and-series']"
3159577,Conjecture about primes and the greatest common divisor,"Conjecture: Given $m,n\in\mathbb N^+$ , one odd and one even, there are two
  primes $p,q$ such that $|mp-nq|=\gcd(m,n)$ . I hope MSE can determine its validity. From time to time, when testing my growing math packages BigZ and Forthmath , I recognize some patterns which I can't prove or disprove (or even have the ambition to). I post them here with the hope that it will not annoy too much. I hope you can bear with it.","['number-theory', 'gcd-and-lcm', 'conjectures', 'prime-numbers']"
3159624,Probability of match in three elements choosing from a group,"One of my teachers asked all 26 ( $t$ ) of the students in our class to randomly choose 5 ( $k$ ) exercises from a website from a set of 20 ( $n$ ). He then said that no two students in the class should have 3 ( $s$ ) or more of the same exercises solved. I want to know the probability of two students in the class having 3 ( $s$ ) of the same exercises solved. I know if $s$ was equal to $k$ , it would be the same as the birthday problem with ( $n$ choose $k$ ) days and a room of $t$ , but what about the other case of $s<k$ ?","['birthday', 'probability']"
3159678,Does nudging an exact differential equation nudge or destroy the identity integrating factor?,"This question will be related to this one , if for no other reason because a positive answer to the latter would likely help to solve the former. Consider the differential equation $(y)\ dx + (x)\ dy = 0$ .  It is already exact, so we can think of the multiplicative identity, $u(x) = 1$ , as an integrating factor.  We can also observe by separating variables that $u(x) = \frac{1}{xy}$ is an integrating factor, yielding $(\frac{1}{x})\ dx + (\frac{1}{y})\ dy = 0$ .  From here, $u(x) = xy$ allows us to return to the original form, so we can toggle freely between the two, and both produce the same solution, $y = \frac{C}{x}$ . Now instead consider the inexact equation $(y)\ dx + (x + \epsilon\ x)\ dy = 0$ , for some small value of $\epsilon$ .  By separating variables, we find the integrating factor $u(x) = \frac{1}{(x + \epsilon x)y}$ , and the solution $y = \frac{C}{x^{\frac{1}{1 + \epsilon}}}$ , which respectively are very close to $u(x) = \frac{1}{xy}$ and $ = \frac{C}{x}$ from the previous problem.  However, the original inexact form has $\frac{\partial N}{\partial x} - \frac{\partial M}{\partial y} = \epsilon$ , which is very close to $0$ .  Does this imply (not only for this problem, but in general) that an integrating factor very close to $u(x) = 1$ exists, or is the nearby exact form destroyed entirely when we nudge $\frac{\partial N}{\partial x} - \frac{\partial M}{\partial y}$ even by a tiny amount?","['partial-derivative', 'integrating-factor', 'ordinary-differential-equations']"
3159688,Show that G has a conjugacy class of size 3.,"1)Let $G$ be a group of size $48$ with centre consisting of the identity element only. Show that $G$ has a conjugacy class of size $3$ . My attempt:
I know $C(g)$ centraliser of $g$ is a subgroup of $G$ , so its size must divide that of $G$ .
And that $|cl(g)|=|G|/|c(g)|$ , but how am I sure there exists a $g\in G$ such that $|c(g)|=16$ ? 2)Also say I had a group of size 48 and at least one conjugacy class of size 3 does that imply the centre only consists on the identity element? Any help on 2) would be appreciated thanks.",['group-theory']
3159695,$I = \int_0^k z^{m_1 - 1} \ln(1 + z) \left(\frac{m_1 z}{a} + \frac{m_2}{b} \right)^{-(m_1 + m_2)} \mathrm dz.$,"Question: How to find the closed-form solution for the given integral? $$I = \int_0^k z^{m_1 - 1} \ln(1 + z) \left(\dfrac{m_1 z}{a} + \dfrac{m_2}{b} \right)^{-(m_1 + m_2)} \mathrm dz,$$ where $k, a, b, m_1, m_2 \in \mathbb R_+$ . Any leads appreciated. Attempt : I find it difficult because of the limits. In case the upper limit is $\infty$ , the integral can be solved as \begin{align}
I < & \int_0^\infty z^{m_1 - 1} \ln(1 + z) \left(\dfrac{m_1 z}{a} + \dfrac{m_2}{b} \right)^{-(m_1 + m_2)} \mathrm dz \\
= & \left( \dfrac{b}{m_2}\right)^{m_1 + m_2}\int_0^\infty z^{m_{1} - 1} G_{2, 2}^{1, 2} \left( z  \left\vert \begin{smallmatrix} 1, & 1 \\[0.6em] 1, & 0\end{smallmatrix} \right.\right) G_{1, 1}^{1, 1} \left( \left. \dfrac{b m_1}{a m_2} z\right\vert \begin{smallmatrix} 1 - m_{1} - m_{2}\\[0.6em] 0\end{smallmatrix}\right) \mathrm dz \\
= & \left( \dfrac{b}{m_2}\right)^{m_1 + m_2} G_{3, 3}^{3, 2} \left( \left. \dfrac{b m_1}{a m_2} \right\vert \begin{smallmatrix} 1- m_{1} - m_{2}, & -m_{1}, & 1 - m_{1} \\[0.6em] 0, & -m_{1}, & -m_{1}\end{smallmatrix}\right),
\end{align} where the integration above is solved using [1 , eqns. (7, 10, 11, & 21)] and $G_{p, q}^{m, n}(\cdot)$ is Meijer's G-function. But this results in a (very) loose bound.","['integration', 'definite-integrals', 'special-functions']"
3159702,Who has more probability of winning the game?,"Alice and Bob play a coin tossing game. A fair coin (that is, a coin with equal probability of landing heads and tails) is tossed repeatedly until one of the following happens. $1.$ The coin lands ""tails-tails"" (that is, a tails is immediately followed by a tails) for the first time. In this case Alice wins. $2.$ The coin lands ""tails-heads"" (that is, a tails is immediately followed by a heads) for the first time. In this case Bob wins. Who has more probability of winning the game? My attempt $:$ Let $X$ be the random variable which counts the number of tosses required to obtain ""tails-tails"" for the first time and $Y$ be the random variable which counts the number of tosses required to obtain ""tails-heads"" for the first time. It is quite clear that if $\Bbb E(X) < \Bbb E(Y)$ then Alice has more probability of winning the game than Bob $;$ otherwise Bob has more probability of winning the game than Alice. Let $X_1$ be the event which denotes ""the first toss yields heads"", $X_2$ be the event which denotes ""tails in the first toss followed by heads in the second toss"", $X_3$ be the event which denotes ""tails in the first toss followed by tails in the second toss"". Then $X_1,X_2$ and $X_3$ are mutually exclusive and exhaustive events. Let $\Bbb E(X) = r.$ So we have $$\begin{align} r & = \Bbb E(X \mid X_1) \cdot \Bbb P(X_1) + \Bbb E(X \mid X_2) \cdot \Bbb P(X_2) + \Bbb E(X \mid X_3) \cdot \Bbb P(X_3). \\ & = \frac {1} {2} \cdot (r+1) + \frac {1} {4} \cdot (r+2)+ 2 \cdot \frac {1} {4}. \\ & = \frac {3r} {4} + \frac {3} {2}. \end{align}$$ $\implies \frac {r} {4} = \frac {3} {2}.$ So $\Bbb E(X) = r = 6.$ But I find difficulty to find $\Bbb E(Y).$ Would anybody please help me finding this? Thank you very much for your valuable time.",['probability']
3159742,Show that $\lim [(2n)^\frac{1}{n})] = 1$,"Prove that $\lim ((2n)^\frac{1}{n}) = 1$ . I have obtained the following: $$(2n)^\frac{1}{n} = 1 + k_{n}; n > 1$$ $$(2n) = (1 + k_{n})^n$$ By the Binomial Theorem: $$(1 + k_{n})^n = 1 + k_{n} + \frac{1}{2}n(n-1)k_{n}^2 + ...$$ So $$2n > \frac{1}{2}n(n-1)k_{n}^2$$ $$k_{n} < \frac{2}{\sqrt{n-1}}$$ However, I am not sure how to simplify this down to show $\frac{2}{\sqrt{n-1}} < \varepsilon, \forall \varepsilon > 0$ e.g. by invoking say $\frac{2}{\sqrt{n-1}} < \frac{1}{n} < \varepsilon$","['limits', 'binomial-theorem', 'sequences-and-series', 'real-analysis']"
3159747,Proving the closed-formed formula of a recursive expression,"I was given this exercise as a practice for discrete mathematics, among some others. The instruction is to provide a closed-form formula for the recursive formula shown below. $u_n =2 + \sum_{i=1}^s 2^t*u_i $ where $s=n-1$ and $t=n-2i$ . We are asked to prove the closed-formed formula we suggested by induction. I have encountered trouble in my efforts, since, though I can prove my formula is true for a base case, and then assume it is true for k , I can't seem to prove it's true for k+1 .
We are told (a) $u_n$ is true for all n>1, (b) $u_1=2$ By evauluation, $u_2= 2 + 1*2 = 4$ and $u_3 = 2+2^*u_1+\frac12*u_2=2+2*2+\frac12*4=6+2=8$ Being $u_1=2$ $u_2 = 4$ $u_3=8$ I suggested the formula $u_n=2^n$ , so that $$u_n =2 + \sum_{i=1}^s 2^t*u_i =2^n$$ As I said, I failed to prove this by induction. Is anybody out there able to point my error out, or show how can I prove this by induction?","['summation', 'closed-form', 'induction', 'discrete-mathematics']"
3159842,Geometric Interpretation of Automorphisms of Projective Bundles,"Let $\mathcal{E}$ be a rank three vector bundle on $\mathbb{P}^1$ . It splits as $\mathcal{E}\cong\mathcal{O}(a_1)\oplus\mathcal{O}(a_2)\oplus\mathcal{O}(a_3)$ . It's not hard to see that any automorphism of $\mathbb{P}(\mathcal{E})$ acts trivially on the Picard group, which is generated by the class $f$ of a fiber and the class $h$ of $\mathcal{O}_{\mathbb{P}(\mathcal{E})}(1)$ . In particular, because any automorphism takes fibers to fibers, we obtain a surjective map $\text{Aut}(\mathbb{P}(\mathcal{E}))\rightarrow \text{Aut}(\mathbb{P}^1)$ . We get an exact sequence $$
1\rightarrow G\rightarrow \text{Aut}(\mathbb{P}(\mathcal{E}))\rightarrow \text{Aut}(\mathbb{P}^1)\rightarrow 1
$$ We think of $G$ as automorphisms that fix each fiber as a set and leave $\mathbb{P}^1$ fixed. Algebraically, we can see $G=\mathbb{P}\text{Aut}(\mathcal{E})$ . We have $\text{Aut}(\mathcal{E})\subset \text{End}(\mathcal{E})=H^0(\mathbb{P}^1,\mathcal{E}\otimes \mathcal{E}^{\vee})$ . We can view an element of $H^0(\mathbb{P}^1,\mathcal{E}\otimes \mathcal{E}^{\vee})$ as a $3\times 3$ matrix with entries $a_{ij}\in H^0(\mathbb{P}^1,\mathcal{O}(a_i-a_j))$ . I'd like to get my hands on what these automorphisms do in a geometric sense. It may be instructive to consider a specific example. Let $\mathcal{E}=\mathcal{O}(3)\oplus\mathcal{O}(3)\oplus\mathcal{O}(4)$ . Our matrix is $\begin{pmatrix} 
a & b & 0 \\ 
c & d & 0 \\ 
l_1 & l_2 & e  
\end{pmatrix} $ where $a,b,c,d,e$ are scalars and $l_1, l_2\in H^0(\mathbb{P}^1,\mathcal{O}(1))$ . The condition that this matrix defines an automorphism is that the determinant doesn't vanish, which just means that $(ad-bc)e\neq 0$ . After projectivizing, we can set $e=1$ .  Intuitively, this matrix breaks up in to a few parts. The top left $2\times 2$ block seems to be a $GL_2$ . The bottom left part is a $H^0(\mathcal{O}(1))^{\oplus 2}=\mathbb{A}^4$ . How can I interpret these parts of the matrix geometrically? What types of automorphisms do they correspond to? One idea I have is the following. There is a natural divisor $\mathbb{P}(\mathcal{O}(3)\oplus\mathcal{O}(4))$ inside $\mathbb{P}(\mathcal{E})$ given by one of the obvious inclusions. Using the well-known Chow ring of $\mathbb{P}(\mathcal{E})$ , I was able to compute that this divisor lies in class $h-3f$ . Using the projection formula, we see that $h^0(\mathbb{P}(\mathcal{E}),\mathcal{O}(h-3f))=4$ . From here, it seems like the bottom left part of the matrix moves around divisors in the class $h-3f$ . Is this a reasonable interpretation? What about the $GL_2$ block? Perhaps it corresponds in some way to the divisor $\mathbb{P}(\mathcal{O}(3)\oplus\mathcal{O}(3))$ ?",['algebraic-geometry']
3159875,Gradient and Hessian of $g(x) = f(Ax + b)$,"Given scalar field $f : \Bbb R^m \to \Bbb R$ , matrix $A \in \Bbb R^{m \times n}$ and vector $b \in \Bbb R^m$ , find the gradient and the Hessian of the scalar field $g : \Bbb R^n \to \Bbb R$ defined by $g(x) := f(Ax + b)$ . I cannot find the expression for the derivative: $g'(x) = f'(Ax + b)*(Ax + b)'$ I believe the derivative $f'(Ax + b)$ is simply A*partial derivatives. But I do dot know how to proceed with the other terms. I know the expressions for gradient and Hessian, but I did never see it in matrix form.","['scalar-fields', 'multivariable-calculus', 'derivatives', 'hessian-matrix']"
3159936,Extension of Vector Field in the $\mathcal{C}^r$ topology.,"Let $M\subset \mathbb{R}^n$ be a compact smooth manifold embedded in $\mathbb{R}^n$ , we define $$\mathfrak{X}(M) := \{X: M \to \mathbb{R}^n;\ X\mbox{ is smooth and }\ X(p) \in T_p M \subset \mathbb{R}^n,\ \forall\ p \in M \}.$$ Choosing an atlas $\{(\varphi_i,U_i)\}_{i=1}^{n}$ , and compacts $K_i \subset U_i$ , such that $$\bigcup_{i=1}^n K_i = M,$$ we define the $\|\cdot \|_r$ norm as \begin{align*}\|\cdot\|_r :  \mathfrak{X}(M)&\to \mathbb{R}\\
X &\to \max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right)  \right\|\right\},
\end{align*} then we named $\mathfrak{X}^r(M)$ as the complete Banach space $(\mathfrak{X}(M),\|\cdot\|_r)$ (it is possible to prove that the topology of $\mathfrak{X}^r(M)$ does not depend on the selected atlas). My Question: Let $X \in \mathfrak{X}(M)$ and $Y$ be a smooth vector field on $M$ defined just in a compact $K \subset M$ such that $$\max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i\cap K)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right) - \text{d}^{j}\left( Y\circ\varphi_i \right) \right\|\right\}<\varepsilon,$$ is it possible extend $Y$ to a vector field $\tilde{Y}$ such that 1) $\left.\tilde{Y}\right|_{K} = Y$ , 2) $\|X-\tilde Y\|_r < A\cdot\varepsilon$ , where $A $ is a constant that depends only on the manifold $K$ ? The compact $K$ is a connected submanifold with boundary of $M$ , such that $\dim K = \dim M$ . Edit: I changed $\|X-\tilde Y\|_r < \varepsilon$ to $\|X-\tilde Y\|_r < A\cdot\varepsilon$ after Moishe Kohan's comment. My ideas First, I extend $Y$ by a smooth vector field $Z$ $\in \mathfrak{X}(M)$ , by the continuity of $Z$ , so there exists a neighborhood $U$ of $K$ , such that $$\max_{\substack{i\in\{1,...,n\} \\ j\in \{0,...,r\}}}\left\{\sup_{x \in \varphi^{-1}_i(K_i\cap U)}\left\| \text{d}^{j}\left( X\circ\varphi_i \right) - \text{d}^{j}\left( Z\circ\varphi_i \right) \right\|\right\}<\varepsilon,$$ and then choosing a partition of unity $ \{\phi_1, \phi_2\}$ subordinate to the cover $\{U,M\setminus K\}$ we can define $$\tilde{Y} = \phi_1 Z + \phi_2 X, $$ however I could not guarantee that $\|X - \tilde{Y}\|_r < \varepsilon$ , because I can not control de derivatives of $\phi_1$ and $\phi_2$ . Does anyone know how  should I proceed?","['analysis', 'real-analysis', 'manifolds', 'differential-topology', 'differential-geometry']"
3159961,What's the domain of $\frac{1}{\tan\left(x\right)}$?,"I'm asking this because I've seen contradictory answers in different textbooks. On one hand, $\frac{1}{\tan\left(x\right)}$ is undefined for $\tan(x)=0$ , but $\tan(x)$ itself is undefined for $x=\frac{\pi}{2}$ . So, in this view, the domain should be $\mathbb{R}\backslash \left\{\frac{\pi }{2}n,\:n\in \mathbb{Z}\right\}$ . On the other hand, $\frac{1}{\tan\left(x\right)}$ can also be written as $\frac{\cos\left(x\right)}{\sin\left(x\right)}$ , and this is only undefined when $\sin\left(x\right)=0$","['trigonometry', 'functions']"
3159971,Why $ \Im \frac{1}{1+e^{-s + i a }}=\frac{\sin(a)}{\cos(a)+\cosh(s)} $?,"Why this $$ \Im \frac{-2}{1+e^{-s + i a }} $$ equals to this expression: $$\\\  \frac{\sin(a)}{\cos(a)+\cosh(s)}
$$ I was trying to evaluate the Fourier transform of a hyperbolic function and my textbook and the other sources say this equality holds on. I only got to: $$ \Im \frac{e^{-ia}}{e^{-ia}+e^{-s}} $$ Well, generally the imaginary part of $e^{-ia}$ is $\sin(a)$ , but $e^{-s}$ is real. So I don't understand the $\cosh (s)$ part. I am just so confused. The fourier tranform has been done on this function: $$f(x) = \frac{\sinh(ax)}{\sinh(\pi x)}$$","['hyperbolic-functions', 'fourier-transform', 'complex-analysis', 'calculus', 'trigonometry']"
3160000,Proving a dynamical system can't have a node or a center.,"Given the differential equation $$\begin{cases}x'=P_n(x,y)\\y'=Q_n(x,y)\end{cases}$$ Where $P_n$ and $Q_n$ are homogeneous polynomials of order $n$ , I have shown that if the differential equation has an isolated critical point then it is the origin $(0,0)$ and it is unique. Now, how can I prove that if $n$ is even, then the origin can't either be a center of a node?","['ordinary-differential-equations', 'dynamical-systems']"
3160006,Confusions in Evans book regarding weak derivatives in Banach spaces,"I am studying PDE using Evans' book and I have two main confusions (probably stupid questions to experts) regarding weak derivatives in Banach spaces. First confusion: $\def\u{\mathbf u}$ $\def\v{\mathbf v}$ DEFINITION. Let $\u \in L^1(0, T; X)$ . We say $\v \in L^1(0, T; X)$ is the weak derivative of $\u$ , written $$\u'=\v,$$ provided $$\int_0^T \phi'(t) \u(t) \, dt = -\int_0^T \phi(t) \v(t) \, dt$$ for all scalar test functions $\phi \in C_c^\infty (0, T)$ . However, a subsequent theorem begins with an assumption of THEOREM $\mathbf 3$ (More calculus). Suppose $\u \in L^2(0, T; H_0^1(U))$ , with $\u' \in L^2(0, T; H^{-1}(U))$ . Now, the main confusion here is that $H^{-1}$ is only the dual space of $H^1_0$ , not a subset nor a superset of $H^1_0$ , whereas in the definition, both the function and its weak derivative take values in the same Banach space $X$ . Is there some kind of hidden identification of spaces? Second confusion: THEOREM $\mathbf 2$ (Calculus in an abstract space). Let $\u \in W^{1,p} (0, T; X)$ for some $1 \leq p \leq \infty$ . Then (i) $\u \in C([0,T]; X)$ (after possibly being redefined on a set of measure zero), and (ii) $\u(t) = \u(s) +\displaystyle\int_s^t \u'(\tau) \, d\tau$ for all $0 \leq s \leq t \leq T$ . (iii) Furthermore, we have the estimate $$\max_{0 \leq t \leq T} \|\u(t)\| \leq C\|\u\|_{W^{1,p} (0,T;X)},  \tag{7}$$ the constant $C$ depending only on $T$ . Proof. $1$ . Extend $\u$ to be $\mathbf 0$ on $(-\infty, 0)$ and $(T, \infty)$ , and then set $\u^\varepsilon = \eta_\varepsilon * \u$ , $\eta_\varepsilon$ denoting the usual mollifier on $\mathbb R^1$ . We check as in the proof of Theorem $1$ in $\S5.3.1$ that $\u^{\varepsilon'} = \eta_\varepsilon * \u'$ on $(\varepsilon, T-\varepsilon)$ . Then as $\varepsilon \to 0$ , $$\begin{cases} \u^\varepsilon \to \u & \text{in } L^p(0, T; X), \\ (\u^\varepsilon)' \to \u' & \text{in } L^p (0, T; X). \end{cases}  \tag{8}$$ Fixing $0<s<t<T$ , we compute $$\boxed{\u^\varepsilon (t) = \u^\varepsilon (s) +\int_s^t \u^\varepsilon {}' (\tau) \, d\tau.}$$ Some kind of ""fundamental theorem of calculus for Bochner integrals"" seems to be used here. Am I correct that the functions are $C^1$ after mollification, so some version of the ""fundamental theorem of calculus for Bochner integrals"" can be applied? Here, at least the weak derivative and the function belong to the same space, as in the definition. But this is not the case in the proof of the subsequent theorem. Dual functions suddenly appear as derivatives: Proof. $1$ . Extend $\u$ to the larger interval $[-\sigma, T+\sigma]$ for $\sigma>0$ , and define the regularizations $\u^\varepsilon = \eta_\varepsilon * \u$ , as in the earlier proof. Then for $\varepsilon$ , $\delta>0$ , $$\frac{d}{dt} \|\u^\varepsilon (t) -\u^\delta (t)\|_{L^2(U)}^2 = 2 \bigl(\u^{\varepsilon'} (t) -\u^{\delta'} (t), \u^\varepsilon (t) -\u^\delta (t)\bigr)_{L^2(U)}.$$ Thus \begin{align} \|\u^\varepsilon (t) -\u^\delta (t)\|_{L^2(U)}^2 &= \|\u^\varepsilon (s) -\u^\delta (s)\|_{L^2(U)}^2 \\ &{}+2\int_s^t \langle \u^{\varepsilon'} (\tau) -\u^{\delta'} (\tau), \u^\varepsilon (\tau) -\u^\delta (\tau)\rangle \, d\tau  \tag{11} \end{align} None of these two ""fundamental theorems of calculus"" are derived explicitly and I am really confused!","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
3160008,Hopf Bifurcation Theorem,"I have a 2D dynamical system of the form \begin{cases}
\dot{x}=f(x,y,K) \\[1ex]
\dot{y}=g(x,y,K)
\end{cases} where $K$ is a free parameter (later I can write the system here). I've found two Hopf bifurcations at approximately $K=0.69$ and $K=0.84$ . In between these two values, there is a clear stable limit cycle, and for $K<0.69$ and $K>0.84$ the oscilations die after some time $t$ . Furthermore, after computing the eigenvalues, I can see that the real part of the eigenvalue is negative for $K<0.69$ and $K>0.84$ , and it is positive for $0.69<K<0.81$ . With all this, I think I can say that there is a supercritical bifurcation at $K=0.69$ because there are stable oscillations for $K>0.69$ . As for $K=0.84$ , I don't think we can say anything unless we go to Hopf theorem in here , and compute the derivative of the real part of the eigenvalue and the $a$ value which I think is the 1st Lyapunov coefficient (although I have no idea how the complicated expression for this coefficient turns out to be equal to this...?) Now, the problem is that I've computed these two things at $K=0.69$ and $K=0.84$ and they are both positive in the two cases. So, according to the theorem this would mean that periodic solutions exist for $K<0.69$ and $K<0.84$ . The second case is correct, but the first is not. Moreover, the theorem states that the fixed point is stable for $K<0.69$ and $K<0.84$ , and unstable for $K>0.84$ , which is clearly not the case as it is easy to see from the signal of the real part of the eigenvalues! So, what's happening here? EDIT: The system is: \begin{cases}
\dot{x}=-\frac{8}{3}(-0.99e^{-0.01y}+1)(x^2-x)+Kxy\\[1ex]
\dot{y}=\frac{4}{x^{3/2}}\left(-y+1+\frac{3}{2}\frac{y}{x}(x-1)-0.1\frac{x-1}{y+1}-K\right)
\end{cases} and the nullclines+vector field for the case $K=0.69$ is: EDIT2: After discussing in the comments with Evgeny , I realized that I was evaluating the derivative of the real part of the egeinvalue incorrectly. Indeed, by simply analysing that it changes from negative to positive at $K=0.69$ and from positive to negative at $K=0.84$ , we can say that its derivative at these points is positive and negative, respectively. Furthermore, because the first Lyapunov coefficient is positive in both cases, we can conclude through Hopf theorem that there is a subcritical bifurcation at $K=0.69$ and a supercritical at $K=0.84$ . So this means that the stable periodic oscillations that I mentioned I can clearly see between $K=0.69$ and $K=0.84$ are due to the supercritical bifucartion at $K=0.84$ , and not the bifurcation at $K=0.69$ . EDIT3: I've just realized that the conclusions I've drawn in EDIT2 are not consistent with Hopf theorem! And so my question still remains: The real part of the eigenvalue is positive for $0.69<K<0.84$ and negative for $K<0.69$ and for $K>0.84$ . For $K<0.69$ I get this situation: which I can't understand if it's a no limit-cycle situation or an unstable situation. For $0.69<K<0.84$ I have this situation: which is clearly a stable limit cycle! Finally for $K>0.84$ I get a situation similar to $K<0.69$ . With all this, it seems that the first Lyapunoc coefficient should be negative! But when I do the calculation I get a positive one!","['bifurcation', 'limit-cycles', 'ordinary-differential-equations', 'dynamical-systems']"
3160013,Question about the proof of Second Isomorphism Theorem,"The Second Isomorphism Theorem:
Let $H$ be a subgroup of a group $G$ and $N$ a normal subgroup of $G$ . Then $$H/(H\cap N)\cong(HN)/N$$ There is the proof of Abstract Algebra Thomas by W. Judson: Define a map $\phi$ from $H$ to $HN/N$ by $H\mapsto hN$ . The map $\phi$ is onto, since any coset $hnN=hN$ is the image of $h$ in $H$ . We also know that $\phi$ is a homomorphism because $$\phi(hh')=hh'N=hNh'N=\phi(h)\phi(h')$$ By the First Isomorphism Theorem, the image of $\phi$ is isomorphic to $H/\ker\phi$ , that is $$HN/N=\phi(H)\cong H/\ker\phi$$ Since $$\ker\phi=\{h\in H:h\in N\}=H\cap N$$ $HN/N=\phi(H)\cong H/H\cap N$ My question: Is it necessary to prove that the map $\phi$ is onto? Can we only prove that $\phi$ is well defined and the image of $\phi$ is a subset of $HN/N$ ? And then we can use the First Isomorphism Theorem and continue the proof. Thank you.","['group-homomorphism', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
3160090,"The number of solutions of the equation $1 + \sin x\sin^2 {\frac{x}{2}} =0$ in $[-\pi,\pi]$ is...","The number of solutions of the equation $1 + \sin x\sin^2 {\frac{x}{2}} =0$ in $[-\pi,\pi]$ is... What I have tried... Since, $\sin \frac{x}{2} = \sqrt{\frac{1-\cos x}{2}}$ , $$ 1 + \sin x\cdot\frac{1-\cos x}{2} = 0$$ $$2+\sin x \cdot (1-\cos x) = 0$$ From here onwards I am not sure how to continue... P.S. The answer to this question is $0$ ( which means that the equation has no solution. Please explain how) Thank you!",['trigonometry']
3160109,Can I use Seifert-van Kampen theorem infinite times,"I know the definition of Seifert-van Kampen theorem for a topological space ""made"" with 2 parts. Is not difficult to see that if I use the theorem a finite number of times to calculate a the fundamental group of a topological space made from finite many parts, it is valid. But, can I use the theorem infinite times? For example, to show that te fundamental group of a orientable surface of infinite genus is isomorphic to the free group with infinite generators? Thanks.","['general-topology', 'fundamental-groups', 'algebraic-topology']"
3160130,Reaching upon $9=1$ while solving $x$ for $3\tan{(x-15^{\circ})}=\tan{(x+15^{\circ})}$,"$x$ for $3\tan{(x-15^{\circ})}=\tan{(x+15^{\circ})}$ Substituting $y=x+45^{\circ}$ , we get $$3\tan{(y-60^{\circ})}=\tan{(y-30^{\circ})}$$ $$3\frac{\tan y - \sqrt3}{1+\sqrt3\tan y}=\frac{\tan y - 1/\sqrt3}{1+1/\sqrt3\cdot\tan y}$$ $$3(\tan ^2 y-3)=3\tan ^2-1$$ $$9=1$$ The solution provided by the book $x=n\pi  + \pi/4$ fits, so why did i get $9=1$ ?","['proof-explanation', 'trigonometry']"
3160157,"By means of an example, show that $P(A) + P(B) = 1$ does not mean that $B$ is the complement of $A$","I'm in grade 10, and I've just started to learn about complementary events. I am rather perplexed with this question. Isn't this question kinda contradictory, since $P(A) + P(A') = 1$ ? This is what I got to: $P(A) + P(B) = 1$ $P(A) + P(A') = 1$ How could it be proven that $B$ isn't the complement of $A$ ? Help would be greatly appreciated.",['probability']
3160177,"Compute the worst case time complexity of the following algorithm, for i = 1 to n do for j = i to n^2 do print (i, j).","for i = 1 to n do 

    for j = i to n^2 do 

            print (i, j). So here is what I've got $\sum_{i=1}^n \ \sum_{j=i}^{n^{2}} \ $ $C\sum_{i=1}^n \ \sum_{j=i}^{n^{2}} 1\ $ $C\sum_{i=1}^n \ (n^{2}-i+1) $ $C\sum_{i=1}^n \ n^{2} \ - \sum_{i=1}^n \ i \ + \sum_{i=1}^n \ 1\  $ Which becomes $n^{3} + \dfrac{n(n+1)}{2} + n$ And since the term with the highest exponent dominates $O( n^3)$ Now I'm a complete beginner, and I came up with this solution by going over my notes. Was this the correct solution? If not how would I solve this problem?","['computational-complexity', 'discrete-mathematics', 'computer-science']"
3160249,Left and right adjoints of restriction in modules.,"Given a map $f:A\rightarrow B$ of rings, we get a functor $Res_f:\text{B-Mod}\rightarrow \text{A-Mod}$ , induced by our map $f$ . This has left adjoint $B\otimes_A \text{_}$ where the $B$ module structure is given by multiplication on the first factor. We also have a right adjoint $Hom_A(B,\text{_})$ , where we premultiply to give the $B$ module structure. If $B/A$ is free, then these functors are isomorphic, though it seems we need to pick a basis of $B/A$ to exhibit this. Are these functors still isomorphic if $B$ is locally free over $A$ ? Really, this is trying to understand the difference between the left and right adjoints of $f_\ast$ for a map of schemes, but I am more comfortable in the commutative algebra/modules setting, hence phrasing in this manner.","['adjoint-functors', 'algebraic-geometry', 'commutative-algebra', 'modules']"
3160269,What is the length of $x$ in this pentagon diagram?,"ABCDE is a regular pentagon. $\angle AFD = \angle EKC$ $|FH|=1$ cm; $|AH|=3$ cm What is $|DK|?$ I know that triangles $EFA$ and $DEK$ are similar and that $|EK|=4$ cm. Also because this is a regular pentagon each one of the interior angles are $108^o$ . Naming similar angles inside the pentagon, I tried to find an isosceles triangle, but I couldn't. I can't progress any further from here. How can I solve this problem?","['euclidean-geometry', 'geometry', 'polygons']"
3160340,Compute $\lim\limits_{x \to +\infty}\frac{\ln x}{ \int_0^x \frac{|\sin t|}{t}{\rm d}t}$,"Problem Compute $$\lim\limits_{x \to +\infty}\dfrac{\ln x}{\displaystyle \int_0^x \dfrac{|\sin t|}{t}{\rm d}t}.$$ Comment Maybe, we can solve it by L'Hospital's rule , but there still exists a difficulty here. Though $x \to +\infty$ implies $\ln x \to +\infty$ ,  we do not know the limit of the denominator. How to solve it?","['limits', 'calculus', 'definite-integrals', 'trigonometry']"
3160362,$\lim\limits_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3}$,"Find the limit, $$\lim_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3}$$ Where $[.]$ denotes the integral part of $.$ ? Efforts: If $x$ is integer, $$\lim_{n\to\infty}{[x]+[2^2x]+\dots +[n^2x]\over n^3}$$ $$\lim_{n\to\infty}{x(1+2^2+\dots +n^2)\over n^3}$$ We know $\sum\limits_{i=1}^ni^2=(n+1)(2n+1)n/6$ Therefore we get that limit is equal to $x/3$ . How to solve it for non integral values.
Thanks in advance.","['limits', 'real-analysis']"
3160389,The uniqueness of the continuation of the functional.,"I have a continuous linear functional $f_0 \in c_0^*$ where $$c_0 = \lbrace{x: (x_1, \dots, x_n, \dots)|\lim_{n\rightarrow\infty}x_n = 0\rbrace} \subset m = \lbrace{x: (x_1, \dots, x_n, \dots)|\sup_{n\in\mathbb{N}}|x_n| < \infty\rbrace}$$ My question: Prove that $f_0$ have a uniqueness continuation $f$ on space $m$ with respecting the norm, i.e $$\forall x \in c_0,\ f(x) = f_0(x)\quad \text{and}\quad ||f_0|| = ||f||.$$ The Hahn-Banach theorem guarantees the existence of such a continuation, but not its uniqueness. I tried to figure out some properties of the space conjugate to $c_0$ . I proved that $c_0^* \cong l^1$ . Having proved this, I saw how functionals on $c_0$ looks like. Then I proved that $(l^1)^* \cong m$ and saw how functionals on $l^1$ looks like. This is good, because in these spaces I cannot use the Riesz theorem, but, as I said, in the process of the proof I obtained the general form of functionals on these spaces. But further, I do not know what to do. How to use this information? Or maybe there are some tricky lemmas or theorems that give information about the uniqueness of the continuation? Could you tell me what to do next? Thanks in advance!","['lp-spaces', 'functional-analysis']"
3160405,Existence of Smooth paths,"How can we prove that there exists a smooth path between any pair of points in a connected smooth manifold?
I think we can do this locally by smooth charts, but I don't know how to glue these paths together to construct a smooth curve. If we construct these curves locally and patch them together then we have finite numbers of singularities along the curve. How can I smooth the curve?","['smooth-manifolds', 'differential-geometry']"
3160500,Finding a vector potential for a solenoidal vector field,"I have to find a vector potential for $F = -y \hat{i} + x \hat{j}$ This is what I have done: We know that, if $\nabla \cdot F = 0$ , we can construct the following: $$F= \nabla\times G$$ Where $G$ is the vector potential we want to find out. We know what F is, so it is just about doing the following: $$\frac{\partial G_3}{\partial y} - \frac{\partial G_2}{\partial z} = -y$$ $$\frac{\partial G_1}{\partial z} - \frac{\partial G_3}{\partial x} = x$$ Noting that the partial derivatives with respect to $z$ are zero in this case, we get: $$G = \frac{-x^2-y^2}{2}+C$$ Where $C$ is just the gradient of any scalar. I am given a whole list of possible vector potentials: Now I could use the most brute method: Trial and error with each possible vector potential given, using the equation: $$G_n = \frac{-x^2-y^2}{2}+C$$ Solving for $C$ and seeing whether it holds. This is pretty tedious; is there any brightest method? Thanks. EDIT $$\frac{\partial G_2}{\partial z} = y$$ $$\frac{\partial G_1}{\partial z} = x$$ $$\frac{\partial G_2}{\partial x} - \frac{\partial G_1}{\partial y} = 0$$ I get: $$G = <xz,yz,0>$$ Which indeed satisfies: $$F= \nabla\times G$$ But this option is not in the list... Now let's set $G_2 = 0$ : $$\frac{\partial G_3}{\partial y} = -y$$ $$\frac{\partial G_1}{\partial z} - \frac{\partial G_3}{\partial x} = x$$ $$\frac{\partial G_1}{\partial y} = 0$$ I get: $$G = <0,0,\frac{-x^2 - y^2}{2}>$$ Which indeed satisfies: $$F= \nabla\times G$$","['partial-derivative', 'vector-fields', 'ordinary-differential-equations']"
3160556,Is it possible to prove the derivative of sine geometrically without arc length?,"There are a great many ways to prove that the derivative of sine is cosine, some of them based on things like the Taylor series definition.  I’d like to prove it using only the right-triangle definition of trigonometric functions.  But all the proofs I’ve seen involve the notion of the arclength of a circular arc subtended by a given angle and/or the notion of the area of a sector subtended by a given angle. My question is, is it possible to prove that $\lim_{x\to0}\frac{\sin(x)}{x}=1$ and hence $\frac{d}{dx}\sin(x) =\cos(x)$ without invoking the notions of the arclength of a circular arc or the area of a sector?  After all, it is perfectly possible to define the measure of an angle without referring to arclength; it’s done in most modern axiomatizations of Euclidean geometry, like this one : Postulate $11$ . Angle Measurement Postulate. To every angle there corresponds a real number between $0^\circ$ and $180^\circ$ . Postulate $12$ . Angle Construction Postulate. Let $AB$ be a ray on the edge of the half-plane $H$ . For every $r$ between $0$ and $180$ there is exactly one ray $AP$ , with $P$ in $H$ such that $m \angle PAB = r$ . Postulate $13$ . Angle Addition Postulate. If $D$ is a point in the interior of $\angle BAC$ , then $m \angle BAC = m \angle BAD + m \angle DAC$ . Postulate $14$ . Supplement Postulate. If two angles form a linear pair, then they are supplementary. So using axioms like these, is it possible to prove that $\sin(x)\leq x \leq \tan(x)$ , which is what’s needed to prove that $\lim_{x\rightarrow 0}\frac{\sin(x)}{x}=1$ and hence $\frac{d}{dx}\sin(x) =\cos(x)$ ?","['euclidean-geometry', 'angle', 'geometry', 'calculus', 'limits']"
3160582,Extending the spectral theorem for bounded self adjoint operators to bounded normal operators,"I'm currently preparing for an exam in functional analysis, and I have a question about the extension of the spectral theorem for bounded self adjoint operators to bounded normal operators. Starting point is the spectral theorem for bounded self adjoint operators:
Let $T$ be a bounded self adjoint operator in an Hilbert space $X$ , then there exists a unique spectral measure $E : \Sigma_\mathbb{R} \rightarrow B(X)$ , which has compact support in $\mathbb{R}$ (Here $\Sigma_\mathbb{R}$ is the Borel- $\sigma$ -algebra on $\mathbb{R}$ and $B(X)$ is the set of all bounded and linear operators in $X$ ) and $T = \int\limits_{\mathbb{R}}\lambda dE_\lambda$ .
Moreover the mapping $f \rightarrow f(T) := \int\limits_{\mathbb{R}} f(\lambda) dE_\lambda$ , for bounded and measurable functions $f$ , satisfies the conditions of the (unique) measurable functional calculus. If a normal operator $T \in B(X)$ is given, one can define the Operators: $S_1 := \frac{1}{2} \left( T+T^{\ast} \right)$ and $S_2 := \frac{1}{2i} \left( T-T^{\ast} \right)$ .
Then we get that $T = S_1 + i S_2$ and that $S_1$ and $S_2$ are self adjoint.
Then by the spectral theorem for self adjoint operators there exist two spectral measures $E^1$ and $E^2$ . Since $T$ is normal, $S_1$ and $S_2$ commute, and therefore the spectral measures $E^1$ and $E^2$ . Then there exists a unique spectral measure $E : \Sigma_{\mathbb{R}^2} \rightarrow B(X)$ such that for all $A, B \in \Sigma_\mathbb{R}$ we have that $E(A \times B) = E^1(A)E^2(B)$ . (See: Schmüdgen - Thm. 4.10) By identifying $\mathbb{R}^2$ with $\mathbb{C}$ one gets a unique specral measure $E : \Sigma_\mathbb{C} \rightarrow B(X)$ and is able to define integrals with respect to this spectral measure in the natural way: First for step functions and then for bounded measurable functions by approximation. Now I have to show that $E$ has the same properties as the spectral measure for self adjoint operators, i.e.: $T = \int\limits_{\mathbb{C}} z dE_z$ and the mapping $f \rightarrow f(T) := \int\limits_{\mathbb{C}} f(z) dE_z$ , for bounded and measurable functions $f$ , satisfies the conditions of the (unique) measurable functional calculus. My question now is: is there any other way to show that, beside re-do the proof of the spectral theorem for self adjoint operators? It's not that much work, once one has the proof of the self adjoint case. I'm just curious if there's an more elegant way ... Thanks in advance, GordonFreeman","['spectral-theory', 'functional-analysis']"
3160625,Marcus Number Fields chapter 5 exercise 8,"The exercise is the following: one should show that $R=\mathbb{Z}[\sqrt{223}]$ has ideal class group which is a cyclic group of order $3$ . I tried to follow the standard path which Marcus uses to determine other ideal class groups: firstly, I calculated $\lambda$ which turns out to be $\sqrt{223}$ . Having that, one reduces to check what happens to prime in $R$ which are above $2,3,5,7,11,13$ . I've checked that the prime above $2,5,7,11,13$ are trivial in the ideal class group and using Kummer theorem I've found that $$3R=(3,1+\sqrt{223})(3,1-\sqrt{223}) .$$ Let's call this two primes respectively $p,q$ . One can check that $14-\sqrt{223} \in p $ and $14-\sqrt{223} \not \in q$ , while the nrm of this element is $-27$ , so that one got $p^3=(14-\sqrt{223})=e$ in the ideal class group. With a similar reasoning, one got $q^3=e$ in the ideal class group. Now, if we manage to demonstrate that $p,q$ are not principal we are done. If one of this ideal were principal, the element which generates it should have norm $\pm 3$ . So , we are reduced to show the following :there are no solutions in $\mathbb{Z}$ of $a^2-223b^2=\pm3$ . The one with the $+$ is showed to be impossible using reciprocity, while I've got no idea to end for the other equation.","['number-theory', 'algebraic-number-theory', 'principal-ideal-domains']"
3160658,Proving $f(x)=|x|$ is onto,"I've been working on proving that this is a onto function: $f$ : $\mathbb R$ $\to$ $\mathbb R^{\geq0}$ is defined by $f(x)=|x|$ My proof so far: Let $y\in\mathbb R$ . Rough work: $|x|=y \Rightarrow \sqrt {x^2}=y \Rightarrow n^2=y^2 \Rightarrow \pm x=\pm y$ Suppose $f(\pm y)=|\pm y|=y$ . I know that this function is definitely onto given the co-domain of $\mathbb R^{\geq0}$ , but I feel like my proof is flawed. Am I supposed to individually account for the $-x$ and the $+x$ from $\pm x=\pm y$ when trying to solve $f(x) = y$ ? Thanks!","['elementary-set-theory', 'proof-verification']"
3160691,an interesting game,"Alice, Bob and Cindy are playing a game of a circle. Firstly, Alice starts by drawing a point around the circle. Subsequently, being aware of Alice's decision Bob makes his move. Finally, Cindy puts a point around the circle being aware of Alice's and Bob's decisions. After all players fix their positions a point X drawn around the circle randomly. The winner of the game is the one whose position is the closest to the point X. Question: How should Bob make his choice in order to maximize the probability of winning?","['random', 'probability']"
3160696,"Prove that $BC$, $B_1C_1$, $B_2C_2$ are concurrent.","Consider altitude $AH$ of $\Delta ABC$ . $B_1$ and $B_2$ are points on side $AB$ such that $HB_1 \perp AB$ and $HB_2 \parallel AC$ . $C_1$ and $C_2$ are points on side $AC$ such that $HC_1 \perp AC$ and $HC_2 \parallel AB$ . Prove that $BC$ , $B_1C_1$ , $B_2C_2$ are concurrent. I tried letting $B_1C_1 \cap B_2C_2 = \{A'\}$ and tried to prove that $\widehat{AA'B} = \widehat{AA'C}$ but I don't see the light at the end of the tunnel.","['euclidean-geometry', 'geometric-transformation', 'geometry']"
3160748,What differential equation corresponds to this vector field?,"Here is a vector field: $$ \vec F(x,y)=\{\sin(x),\sin(y)\}, $$ where $x,y \in (0,\pi).$ How do you find the differential equation, that when solved gives the integral curves for this vector field? I made this plot on WolframAlpha:","['vector-fields', 'multivariable-calculus', 'ordinary-differential-equations']"
3160750,"The orthogonal complement of the orthogonal complement from ""Linear Algebra Done Right""","The following content is from ""Linear Algebra Done Right"" by Sheldon Axler Corollary: Suppose $U$ is a finite-dimensional subspace of $V$ . Then $$U = (U ^\perp)^\perp.$$ We need to prove the following: i) $U \subseteq (U^\perp)^\perp$ and 
   ii) $(U^\perp)^\perp \subseteq U$ Proof i): Supposer $u \in U$ . Then $\langle u,v \rangle = 0$ for $v \in U^\perp$ . Since all vector $u$ is orthogonal to $v$ . Then $u \in U$ . Hence $u \in (U ^\perp)^\perp$ I do understand the proof for i) as stated in the above. But I'm confused about the proof in ii) ii) Let $u \in (U ^\perp)^\perp$ From Proposition 2, we have that $V = U \bigoplus U^\perp$ . and so $u$ can be written as $u=v+w$ where $v \in U$ and $w \in U^\perp$ . But $u - v = w$ , so $u - v \in U^\perp$ . Now we already have that $u \in (U ^\perp)^\perp$ and $v \in (U ^\perp)^\perp$ and $u - v \in (U ^\perp)^\perp$ . Therfore, $u - v \in U ^\perp \cap (U^\perp)^\perp$ . ***Here is where I don't understand! Why we can let $u = v + w$ , but since $u \in (U ^\perp)^\perp$ ? and Why does $u - v \in U ^\perp \cap (U^\perp)^\perp$ ?","['orthogonality', 'abstract-algebra', 'linear-algebra', 'proof-verification']"
3160753,Markov chains. If $X_0=0$ then the probability that $X_n\ge1$ for all $n\ge 1$ is $\frac{6}{\pi^2}$,"Let $(X_n)_{n\ge0}$ be a markov chain on $\{0,1,...\}$ with transition probabilities given by $p_{01}=1,p_{i,i+1}+p_{i,i-1}=1, p_{i,i+1}=\Big(\frac{i+1}{i}\Big)^2p_{i,i-1}, i\ge1$ I need to show that if $X_0=0$ then the probability that $X_n\ge1$ for all $n\ge 1$ is $\frac{6}{\pi^2}$ I'm looking for tips on how to solve this task.","['stochastic-processes', 'measure-theory', 'markov-process', 'markov-chains']"
3160811,How to decide convergence of Integrals,"I got this doubt while evaluating the integrals: $$I=\int_{0}^{\frac{\pi}{2}}\ln(\sin x)\sin xdx$$ and $$J=\int_{0}^{\frac{\pi}{4}}\csc xdx$$ Now even though the integrand $f(x)=\ln(\sin x)\sin x$ is not defined at $x=0$ which is the lower limit, still it has a finite answer. But integrand in $J$ is not defined at $x=0$ and integral is infinite. So how to identify without explicitly evaluating?","['integration', 'algebra-precalculus', 'convergence-divergence']"
3160823,An almost complex structure on $M$ is equivalent to a reduction of the structure group of the tangent bundle,"Let $M$ be an $2n$ -dimensional manifold. Let $\mathcal{F}_{\mathrm{GL}(2n, \mathbb{R})}$ be the frame bundle over $M$ . Consider the subgroup $\mathrm{GL}(n, \mathbb{C})\subset\mathrm{GL}(2n, \mathbb{R})$ . What I'm trying to prove is: If $M$ has an almost complex structure $J:TM\rightarrow TM$ then there is a reduction of the structure group $\mathrm{GL}(2n, \mathbb{R})$ of $\mathcal{F}_{\mathrm{GL}(2n, \mathbb{R})}$ to $\mathrm{GL}(n, \mathbb{C})$ . The reciprocal is also true, and I was able to prove it. But I'm stuck on this direction. Does anyone have a suggestion?","['principal-bundles', 'complex-manifolds', 'almost-complex', 'differential-geometry']"
3160834,"What powers of $p$, where $p$ is prime, divide $\binom {p^{\alpha} m}{p^{\alpha}}$?","My question is about a short combinatorial discussion in Herstein's Topics in Algebra, on page 92, preceding one of his three proofs of Sylow's Theorem. I will cite the important part: If $p^r | m$ but $p^{r+1} \nmid m$ , consider $$\binom {p^\alpha m}{p^{\alpha}} = \frac{(p^{\alpha}m)!}{(p^{\alpha})!(p^{\alpha}m-p^{\alpha})!} = \frac {p^{\alpha}m(p^{\alpha}m-1) \dots (p^{\alpha}m-i) \dots (p^{\alpha}m-p^{\alpha} + 1)} 
{p^{\alpha}(p^{\alpha}-1) \dots (p^{\alpha}-i) \dots (p^{\alpha}-p^{\alpha} + 1)}.$$ The question is, What power of p divides $\binom {p^{\alpha} m}{p^{\alpha}}$ ? Looking at this number, written out as we have written it out, one can see that except for the term $m$ in the numerator, the powers of $p$ dividing $(p^{\alpha}m-i$$)$ is the same as that dividing $p^{\alpha}-i$ , so all powers of $p$ cancel out except the power which divides $m$ . Thus $p^r | \binom {p^{\alpha} m}{p^{\alpha}}$ but $p^{r+1} \nmid \binom {p^{\alpha} m}{p^{\alpha}}$ . My understanding is as follows. $\binom {p^\alpha m}{p^{\alpha}} = m *\frac{\binom {p^\alpha m}{p^{\alpha}}}{m}$ , consider the unique factorization of both factors. The right will not contain any nonzero powers of $p$ , and so the power of $p$ dividing our binomial is the largest power in the unique factorization of $m$ . But there are two gaps in my understanding: why is the second factor neccesarily an integer, and why does the second factor not contain powers of $p$ ? Herstein explains this second question, but I don't get his argument.","['abstract-algebra', 'combinatorics']"
3160839,"Is $\pi: \mathcal{C}^\infty (M,N) \to \mathcal{C}^\infty (S,N)$, $\pi(f) = \left. f\right|_{S}$ a quocient map in the $\mathcal C^1$ topology?","Let $M, N$ be smooth connected manifolds (without boundary), where $M$ is a compact manifold, so we can put a topology in the space $\mathcal C^\infty(M, N)$ using $\mathcal{C}^1$ Whitney Topology . Now, consider $S\subset M$ a compact submanifold of $M$ with boundary such that $\text{dim}S=\text{dim}M$ , using the same process we can put a topology in $\mathcal C^\infty(S,N)$ using the $\mathcal{C}^1$ Whitney Topology. There is a natural continous projection of $\mathcal C^\infty(M, N)$ on $\mathcal C^\infty(S,N)$ , definided by \begin{align*}
\pi: \mathcal C^\infty(M, N) &\to \mathcal C^\infty(S,N)\\
f&\mapsto \left.f\right|_{S}.
\end{align*} My Question: Is $\pi$ an open map or at least a quocient map ?","['differential-topology', 'functions']"
3160851,"Groups, Rings and Fields.","I am asking for the analogy behind these structures names. Why is a ""field"" called a field? Is there an analogy between a usual ring (finger ring) and a mathematical ring?","['ring-theory', 'group-theory', 'abstract-algebra', 'linear-algebra']"
3160875,Picking a Lyapunov function that is dependent on $\dot{x}$,"I have a system $\dot{x}=f(x)$ . Is it a good idea if I pick a Lyapunov function V that is dependent on $\dot{x}$ . So that $V=V(x,\dot{x})$ ? One of the conditions for stability is that the origin is stable, that is $x=0$ implies $V=0$ . But this won't be the case if V is dependent on $\dot{x}$ as well. Am I missing something or is it just a bad idea to pick a V dependent on $\dot{x}$ .","['lyapunov-functions', 'multivariable-calculus', 'control-theory', 'dynamical-systems']"
3160919,Vector Bundle Transition Functions as Cech Cocycles,"I am trying to understand the fact that vector bundles of rank $r$ over a space $X$ are classified by the Cech cohomology group $\check{H}^{1}\big(X, GL_{r}(\mathcal{O}_{X})\big)$ .  I believe this should work in any of the usual categories, so I wont specify smooth, holomorphic, etc.  I understand broadly how this goes, but there are a few key details tripping me up. So if we have a Cech 1-cocycle $g = \{g_{\alpha \beta}\} \in \check{H}^{1}\big(X, GL_{r}(\mathcal{O}_{X})\big)$ with respect to some open cover $\{U_{\alpha}\}$ , then we know: $$(dg)_{\alpha \beta \gamma} = g_{\beta \gamma} \, g_{\alpha \gamma}^{-1} \, g_{\alpha \beta} =1$$ where we write everything multiplicatively, since that's the group operation on sections of $GL_{r}(\mathcal{O}_{X})$ .  So this equation above is obviously the cocycle condition satisfied by vector bundle transition functions.  But for bundles, we also require that $g_{\alpha \alpha} =1$ .  Is this latter condition true in general for Cech cohomology, or is it somehow an extra requirement in this case? My second confusion is the statement that isomorphic bundles define cohomologous cocycles.  If we have a 0-cochain $\lambda = \{\lambda_{\alpha}\} \in \mathcal{C}^{0}(GL_{r}(\mathcal{O}_{X}))$ , then applying the differential we get $$(d\lambda)_{\alpha \beta} = \lambda_{\beta} \, \lambda_{\alpha}^{-1}$$ So I would be inclined to say that the condition that two 1-cocycles $\{g\}$ and $\{g'\}$ are cohomologous is $$g_{\alpha \beta} \, (g_{\alpha \beta}')^{-1} = \lambda_{\beta} \, \lambda_{\alpha}^{-1}.$$ However, I know that two bundles are equivalent when their transition functions satisfy $$g_{\alpha \beta} = \lambda_{\alpha} g_{\alpha \beta}' \lambda_{\beta}^{-1}$$ and things are clearly in the wrong order (for all ranks larger than 1) to be compatible with the previous equation.  So where are the flaws in my understanding?","['complex-geometry', 'vector-bundles', 'sheaf-cohomology', 'differential-geometry']"
3160924,"$y'' + y' -2y = x^2$, find $A, B$, & and $C$ such that $y = Ax^2+Bx+C$ satisfies this equation.","I am doing an extra credit problem for college. I don't expect anyone to solve it for me, but I would really appreciate being given some hints. The problem: $y'' + y' -2y = x^2$ , find $A, B$ , & and $C$ such that the function $y = Ax^2+Bx+C$ satisfies this equation. I understand how to find derivatives if I know the function, but this is stumping me.","['calculus', 'derivatives', 'ordinary-differential-equations']"
3160925,Solving $\sin(x)-\cos(x)=1$,"Solving $$\sin(x)-\cos(x)=1$$ for $x$ . I used Pythagoras' Theoream and $$C\sin(x+\alpha)=A\sin(x)+B\cos(x)$$ where $A=1$ and $B=-1$ , and I obtained $$C=\sqrt{2}$$ $$\alpha = \dfrac{\pi}{4}$$ and substituted where, $$\sqrt{2}\sin(x+\dfrac{\pi}{4})=1$$ but somehow I think there is something wrong with my calculation, because in Wolfram it is $$-\sqrt{2} \sin(\dfrac{\pi}{4}-x)=1$$ and I don't understand why do I get a different solution, I did everything correct algebraically.",['trigonometry']
3160946,Why we can't differentiate both sides of a polynomial equation? [duplicate],"This question already has answers here : Is differentiating on both sides of an equation allowed? [duplicate] (9 answers) When is differentiating an equation valid? (2 answers) It is possible to apply a derivative to both sides of a given equation and maintain the equivalence of both sides? [duplicate] (5 answers) Closed 5 years ago . Suppose we had the equation below and we are going to differentiate it both sides: \begin{align}
&2x^2-x=1\\
&4x-1=0\\
&4=0
\end{align} This problem doesn't seems to happens with other equation like $\ln x =1$ or $\sin x = 0$ , we can keep differentiating these two without getting "" $4=0$ "", for example. This why I asked about polynomials. PS: I'm not trying to solve any of these equations by differentiating then. But differentiation or integration helps and solving equations? I remember that sometimes to solve trigonometry equtions like $\sin x = \cos x$ we had to square both side so we could use the identity $\sin^2x + \cos^2x =1$ . Even thought squaring appears to make it worse because we have a new root.","['calculus', 'derivatives', 'fake-proofs', 'real-analysis']"
3160970,Two-sided logarithm inequality,"I couldn't find a duplicate question, so I apologize if this has been asked before. I'm trying to show that $$ n - 1 < \left(\log \left( \frac{n}{n-1}\right)\right)^{-1} < n \tag{1} $$ I've verified this numerically, and it even seems to be the case that $$ \lim_{n \to \infty} \frac{1}{\log \left( n / (n - 1)\right)} = n - \frac{1}{2} $$ Again, I've only verified the two statements above numerically, and I'm having a hard time proving them. The inequality seems to make some intuitive sense since, if you consider a logarithm as counting the number of digits in base $e$ then $$ \log(n) - \log(n - 1) \sim \frac{1}{n} \tag{2} $$ However, (2) is only a hunch and I'm not sure how to formalize it. I'm wondering how do I prove the inequality (1)? . Hints are definitely welcome.","['limits', 'inequality', 'logarithms']"
3161040,Expected number of times of choosing a word out of a given vocabulary when words are grouped,"Two players (player C and player G) are playing a (modified) word guessing game. Both players share the same vocabulary $V$ and words in $V$ are grouped into $K$ bins, denoted as $b_1$ , $b_2$ , ..., $b_{K}$ . Furthermore, we know that $b_{i} \subset V$ and $\cup_{i=1}^{K} b_i = V$ . Note here we do not have $b_i \cap b_j = \emptyset$ for $i \neq j$ . The game protocol is described as follows: Player C uniformly chooses a word $w$ from the vocabulary $V$ . Player G does not know which word $w$ is. Player G chooses one bin and asks Player C whether his/her chosen word $w$ is in the bin. If it is, the game ends. Otherwise, Player G will choose another bin. Questions : What is the best bin choosing order and what is the expected number of times of choosing the bin, according to the best possible order? Example: Suppose we have a vocabulary consisting of ten words $V = \{w_1, w_2, ..., w_{10} \}$ and three bins $b_1 = \{w_1, w_2, ..., w_5\}$ , $b_2 = \{w_6, w_7 \}$ , and $b_3 = \{w_8, w_9, w_{10} \}$ . One possible bin choosing order is $b_1 \rightarrow b_3 \rightarrow b_2$ and the expected number of times of choosing the bin is $\frac{1}{2}*1 + \frac{1}{2}*\frac{3}{5}*2 + \frac{1}{2}*\frac{2}{5}*\frac{2}{2}*3 = 1.7$ . I suspect this is the best bin choosing order but how can we prove this result? Thanks. Note A related question (which has additional non-overlapping constraints on the bins) is asked in MO and in its comment, the user @DavidG.Stork gives a good answer (an intuitive proof of best ordering) for the case when those bins have no overlap.","['combinatorics', 'probability']"
