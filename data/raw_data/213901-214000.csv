question_id,title,body,tags
4331720,The asymptotic formula on Poisson Branching Model,"Consider the Poisson branching model with mean $c = 1$ and root Eve. For $n ≥ 3$ , let $A_n$ be the event where Eve has precisely two children, Dana and Fan, and that the total tree size $T = n$ . Let X be the size of the subtree with root Dana. For each $j \geq 1$ , find $\lim_{n \to \infty} \mathbb{P}(X=j|A_n)$ . Find the asymptotic formula for $\mathbb{P}(\frac{n}{3}<X<\frac{2n}{3})$ . $\mathbf{Hint:}$ Set $x=n\alpha$ and $y=n-1-x \sim n\beta$ with $\alpha+\beta = 1$ . Use the asymptotic equation $\mathbb{P}(T=k) = \frac{e^{-k}k^{k-1}}{k!} \sim \frac{1}{\sqrt{2\pi}}k^{-\frac{3}{2}}$ to estimate $\mathbb{P}(X=x)\mathbb{P}(Y=y)$ . The denominator $\mathbb{P}(X+Y = n-1)$ is dominated by $min(X,Y)$ being small. Perhaps surprisingly, the conditional distribution of $X$ is highly skewed to the corners. $\mathbf{Idea:}$ That's what I managed to do. $$\mathbb{P}(X=j|A_n)= \frac{\mathbb{P}(X=j \cap A_n)}{\mathbb{P}(A_n)}=\frac{\mathbb{P}(X=j \cap Z_1=2 \cap T=n)}{\mathbb{P}(Z_1=2 \cap T=n)}$$ Then $$\mathbb{P}(A_n)= \mathbb{P}(Z_1=2 \cap T=n)= \mathbb{P}(T=n|Z_1=2)\mathbb{P}(Z_1=2)$$ $$\mathbb{P}(X=j \cap Z_1=2 \cap T=n)= \mathbb{P}(X=j \cap T=n|Z_1=2)\mathbb{P}(Z_1=2)$$ We denote by $Y$ the size of the subtree with root Fan, then $$\mathbb{P}(X=j \cap T=n|Z_1=2) = \mathbb{P}(X=j \cap Y=n-1-j|Z_1=2)= \\ 
\mathbb{P}(X=j \cap Y=n-1-j)=\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j).$$ As the hint suggest we use the asymptotic equation to estimate $\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j)$ . Then $$\mathbb{P}(X=j) \sim \frac{1}{\sqrt{2\pi}}j^{-\frac{3}{2}} = \frac{1}{\sqrt{2\pi}}(n\alpha)^{-\frac{3}{2}}$$ and $$\mathbb{P}(Y=n-1-j) \sim \frac{1}{\sqrt{2\pi}}(n-1-j)^{-\frac{3}{2}} = \frac{1}{\sqrt{2\pi}}(n\beta)^{-\frac{3}{2}}$$ Combining them both we get: $$\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j) \sim \frac{1}{\sqrt{2\pi}}(n\alpha)^{-\frac{3}{2}}(n\beta)^{-\frac{3}{2}}=\frac{1}{\sqrt{2\pi}}n^{-3}(\alpha\beta)^{-\frac{3}{2}}$$ Finally we have that $$\mathbb{P}(A_n) =\mathbb{P}(T=n|Z_1=2)\mathbb{P}(Z_1=2)= \mathbb{P}(X+Y=n-1)\mathbb{P}(Z_1=2)$$ And we can approximate $\mathbb{P}(X+Y=n-1) \leq min(X,Y)$ .
Then we can conclude that $$\mathbb{P}(X=j|A_n)=\frac{\mathbb{P}(X=j \cap T=n|Z_1=2)}{\mathbb{P}(T=n|Z_1=2)} \geq \frac{\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j)}{min(X,Y)} \sim \frac{\frac{1}{\sqrt{2\pi}}n^{-3}(\alpha\beta)^{-\frac{3}{2}}}{min(X,Y)}$$ $\mathbf{Questions}$ I'm confused about this $min(X,Y)$ . I don't understand if I can conclude like this the first part of the exercise.
And I actually have no idea how to answer the second part (Find the asymptotic formula for $\mathbb{P}(\frac{n}{3}<X<\frac{2n}{3})$ ). Thank you all for the help!","['random-graphs', 'asymptotics', 'poisson-process', 'probability']"
4331764,Prove that $ \sum_{k=0}^n \frac{(-1)^k}{k!}\binom{n}{k}=e\int_0^\infty \frac{t^ne^{-t}}{n!} J_0(2\sqrt{t})\;\mathrm{d}t$ using only real analysis.,"Here $J_0$ is the Bessel function . Yesterday I ask this question on MSE. After some time some answers were given, only one (answer 2) being without mistakes. Then I tried to understand it.
This answer use complex analysis to prove the problem from the title: $\displaystyle \sum_{k=0}^n \frac{(-1)^k}{k!}\binom{n}{k}=e\int_0^\infty \frac{t^ne^{-t}}{n!} J_0(2\sqrt{t})\;\mathrm{d}t$ (this formula is a special case of https://dlmf.nist.gov/18.10.E9 , but here is not presented a proof) and the problem is that I don't know complex analysis . So, I ask for a solution to this problem using only real analysis. My approach. Denote LHS by $I_n$ and RHS by $\mathcal{J}_n$ . Then by using power series expansion for $J_0$ we obtain \begin{align*}
\mathcal{J}_n&=\frac{e}{n!}\int_0^\infty e^{-t}t^n \sum_{k=0}^\infty
\frac{(-1)^k}{k!^2}t^k\;\mathrm{d}t\\&=
\frac{e}{n!}\sum_{k=0}^\infty \int_0^\infty
               e^{-t}t^{n+k}\frac{(-1)^k}{k!^2}\;\mathrm{d}t\\
&=\frac{e}{n!}\sum_{k=0}^\infty
\frac{(-1)^k}{k!^2}\int_0^\infty e^{-t}t^{n+k}\;\mathrm{d}t
\\&=\frac{e}{n!}\sum_{k=0}^\infty\frac{(-1)^k}{k!^2}\Gamma(n+k+1)\\
&=\frac{e}{n!}\sum_{k=0}^\infty\frac{(-1)^k}{k!^2}(n+k)!\\&=
e\sum_{k=0}^\infty \frac{(-1)^k}{k!}\binom{n+k}{k}.
\end{align*} Remains to show that $e\sum\limits_{k=0}^\infty \dfrac{(-1)^k}{k!}\dbinom{n+k}{k}=I_n$ . Here I stopped. We have two problems: At the second equality we use Fubini's theorem without knowing that the integral converges. I cannot prove that $I_n=e\sum\limits_{k=0}^\infty \dfrac{(-1)^k}{k!}\dbinom{n+k}{k}$ (here my ideas are to use $e=\sum\limits_{n\geqslant 0}\frac1{n!}$ and Cauchy product of two series). EDIT. The answer to the question is in the now edited answer 2 here.","['integration', 'real-analysis', 'binomial-coefficients', 'sequences-and-series', 'bessel-functions']"
4331777,How do you check if a die is fair using the posterior probability density function?,"I want to understand how to check if one die is fair using the posterior probability density function, similarly to how a coin is checked in this manner here . To do this, I am assuming that the die has a multinomial distribution, giving the probability density function as: $$P=\frac{n!}{(n_1!)(n_2!)\cdots(n_x!)}p_1^{n_1}p_2^{n_2}\cdots p_x^{n_x}$$ where $n$ is the number of events, $n_x$ is the number of outcomes of event $x$ and $p_x$ is the probability of event $x$ happening. For the sake of the example, let's assume that we have a 3-sided die with letters $A$ , $B$ and $C$ respectively on each face and we want to check whether it is fair. Let's say that we have thrown the die 9 times and the outcomes were the following: Face   Count
A      2
B      4
C      3 Now, following this Wikipedia article we would need to create something along the lines of: $$f(a\mid A=2,B=4,C=3)=\frac{9!}{(2!)(4!)(3!)}a^2b^4c^3$$ where $a$ , $b$ and $c$ are the probabilities of the faces $A$ , $B$ and $C$ showing up respectively. When there are only two cases, this can be solved since the parameters $b$ and $c$ would be represented by $(1-a)$ . However, in this case I am stuck and do not know how to proceed.","['statistical-inference', 'statistics', 'bayesian', 'probability']"
4331791,Integers $n$ such that $i(i+1)(i+2) \cdots (i+n)$ is real or pure imaginary,"A couple of days ago I happened to come across [1], where the curious fact that $i(i-1)(i-2)(i-3)=-10$ appears ($i$ is the imaginary unit). This led me to the following question: Problem 1: Is $3$ the only positive integer value of $n$ such that $i(i-1)(i-2) \cdots (i-n)$ is a real number or a pure imaginary number? If not, can we describe all such integer values of $n$? Initial Analysis: We can view $i(i-1)(i-2) \cdots (i-n)$ as the result of applying a finite sequence of operations to $i,$ each of which involves a radial stretch from the origin and a rotation about the origin. Specifically, $i$ is moved radially outward by a factor of $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}$ and rotated counterclockwise by the angle $\arctan 1 + \arctan 2 + \cdots + \arctan n.$ Since we don't care about the magnitude of the result, only whether we land on the real axis or the imaginary axis, Problem 1 reduces to asking whether $3$ is the only positive integer value of $n$ such that $\arctan 1 + \arctan 2 + \cdots \arctan n$ is an integer multiple of $\frac{\pi}{4}.$ One can also check that $i(i+1)(i+2)(i+3) = -10.$ This is not a coincidence. Since $\arctan 1 + \arctan 2 + \arctan 3$ is an integer multiple of $\frac{\pi}{4},$ it follows that $\arctan (-1) + \arctan (-2) + \arctan (-3) = -\left(\arctan 1 + \arctan 2 + \arctan 3 \right)$ must also be an integer multiple of $\frac{\pi}{4}.$ Other than the products $(i-3)(i-2)(i-1)(i)$ and $(i)(i+1)(i+2)(i+3)$ (and two other products obtained by omitting the $i$ factor from these two), and products of the form $(i-k)(i-k+1) \cdots (i+k-1)(i+k)$ where $k$ can be any positive integer, I don't know any product of the form $(i+m)(i+m+1) \cdots (i+n)$ for integers $m$ and $n$ with $m < n$ that equals a real number or a pure imaginary number. Problem 2: Are $(i-3)(i-2)(i-1)(i)$ and $(i)(i+1)(i+2)(i+3)$ (and two other products obtained by omitting the the $i$ factor from these two), and products of the form $(i-k)(i-k+1) \cdots (i+k-1)(i+k)$ where $k$ can be any positive integer, the only pairs of integers $(m,n)$ with $m < n$ such that $(i+m)(i+m+1) \cdots (i+n)$ is a real number or a pure imaginary number? If not, can we describe all such pairs of integers $(m,n)$? Of course, it is easy to come up problems having a broader scope, such as replacing ""$i+m$"" with an arbitrary complex number whose real and imaginary parts are integers and/or using factors that increment the imaginary part by $1$ (or increment both the real and imaginary parts by $1$) and/or using factors that increment the real part (or the imaginary part, or both the real part and the imaginary part) by a constant integer amount, etc. I suspect the answers to these problems can be obtained from a careful analysis of Carl Størmer's 1890s results involving Machin-like formulas , but my knowledge of French and of this field of mathematics is rather poor. For those interested, I suggest looking at Størmer [2]. I also suspect there is a more direct way to solve Problem 1, and perhaps also Problem 2. Personally, I am only moderately interested in this issue, but I am posting it because I thought others in this group might find this something interesting to pursue. In particular, if there is not a fairly trivial way to solve Problem 1 and someone manages to find a solution that isn't extremely difficult, I suspect that such a solution would make for an interesting Math-Monthly type paper. [1] Charles-Ange Laisant (1841–1920), Remarque sur une équation différentielle linéaire [Remark on a linear differential equation], Bulletin de la Société mathématique de France 23 (1895), 62-63. [2] Fredrik Carl Mülertz Størmer [Störmer] (1874-1957), Sur l'application de la théorie des nombres entiers complexes à la solution en nombres rationnels $x_{1} \; x_{2} \; \dots \; x_{n} \; c_{1} \; c_{2} \; \dots \; c_{n}, \; k$ de l'équation : $c_{1} \text{arc tg}\, x_{1} + c_{2} \text{arc tg}\, x_{2} + \dots  . + c_{n} \text{arc tg}\, x_{n} = k\frac{\pi}{4},$ [On an application of the theory of complex integers to the solution in rational numbers $x_{1} \; x_{2} \; \dots \; x_{n} \; c_{1} \; c_{2} \; \dots \; c_{n}, \; k$ of the equation: $c_{1} \arctan x_{1} + c_{2} \arctan x_{2} + \dots . + c_{n} \arctan x_{n} = k\frac{\pi}{4}$], Archiv for Mathematik og Naturvidenskab 19 #3 (1896), 95 + 1 (errata) pages. UPDATE (30 December 2013): I have incorporated the comment benh made and I have made slight corrections to my Størmer paper citation, but otherwise I have left my original wording intact. I'm impressed with the variety of mathematical techniques brought up in the comments and solutions, especially the probabilistic analysis that Hagen von Eitzen gave. I'm choosing KenWSmith's answer because, more than anyone else, he is responsible for bringing to light a solution (to Problem 1): We observe that all products of the form $i(i+1)(i+2) \cdots (i+n)$ have the form $a+bi$ where both $a$ and $b$ are integers. Thus, if $a+bi$ is real or pure imaginary, we have $a=0$ or $b=0,$ and hence the modulus of $a+bi$ equals $b$ or $a,$ and hence the modulus of $a+bi$ will be an integer. On the other hand, the modulus of $a+bi$ also equals $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}.$ Thus, Problem 1 is equivalent to finding all positive integer values of $n$ such that $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}$ is an integer. Equivalently, find all positive integer values of $n$ such that $(1^2 + 1^2)(1^2 + 2^2) \cdots (1^2 + n^2)$ is the square of an integer. KenWSmith then posted this last version in mathoverflow: When is the product (1+1)(1+4)…(1+n^2) a perfect square? On the same day Lucia supplied an answer by linking to a preprint version of a 2008 paper by Javier Cilleruelo [Journal of Number Theory 128 #8 (August 2008), 2488-2491], which was written solely to answer the question of whether $n=3$ is the only positive integer such that $(1^2 + 1^2)(1^2 + 2^2) \cdots (1^2 + n^2)$ is the square of an integer, which was conjectured and ""partially verified"" in the 2-month earlier paper by Amdeberhan/Medina/Moll [Journal of Number Theory 128 #6 (June 2008), 1807-1846].","['number-theory', 'complex-numbers']"
4331813,eigenvalues of adjoint operator of a diagonizable complex matrix,"Let $A \in M_{n}(\mathbb{C})$ with eigenvalues $\lambda_{1}, \ldots, \lambda_{n}$ . Define a linear $\operatorname{map} \operatorname{ad}(A): M_{n}(\mathbb{C}) \longrightarrow M_{n}(\mathbb{C})$ by $\operatorname{ad}(A)(B)=A B-B A$ for $B \in M_{n}(\mathbb{C})$ . Determine the eigenvalues of $\operatorname{ad}(A)$ in terms of $\lambda_{1}, \ldots, \lambda_{n}$ . Here, we assume that $A$ is diagonizable. Previously, I worked on the following question: Let $W$ be the subspace of $M_2(\mathbb R)$ consisting of $2 \times 2$ matrices with trace $0$ . Let $V$ be the space of linear maps from $W$ to $W$ . Define a linear map $T : W \rightarrow V$ by $A \mapsto \operatorname{ad}(A)$ , where $\operatorname{ad}(A)(B) = AB - BA$ for $B \in W$ . Choose bases for $W$ and $V$ and compute the matrix of $T$ with respect to the bases. and solved it like this: First, we introduce a base for $W$ as follows: \begin{equation*}
\mathcal{B}_W= \bigg \{ \begin{pmatrix}
0 & 0 \\ 1 & 0
\end{pmatrix}, 
\begin{pmatrix}
0 & 1 \\ 0 & 0
\end{pmatrix}, 
\begin{pmatrix}
1 & 0 \\ 0 & -1
\end{pmatrix}
\bigg \}.
\end{equation*} Now, for $V$ we have the following set as a base with respect to $\mathcal{B}_W$ : \begin{equation*}
\begin{aligned}
    \mathcal{B}_V=\{T_{ij}:= &\text{The $3 \times 3$ matrix that the only nonzero element} \\  &\text{of it is the $ij$-th element, that equals $1$} \mid 1 \leq i, j \leq 3  \}
\end{aligned}
\end{equation*} Now, to compute the matrix of $T$ with respect to these bases, first note that for any $1 \leq i \leq 3$ , we have $\alpha_i . \alpha _i=0$ . So, without loss of generality, writing the matrix $A$ as $(\alpha_1, \alpha_2, \alpha_3)$ , and $B=(a\alpha_1, b \alpha_2, c\alpha_3)$ , we have: \begin{equation*}
\begin{aligned}
    T\alpha_1 (a, b, c) &= (2c, 0, -b), \\
    T\alpha_2 (a, b, c) &= (0, -2c, a), \\
    T\alpha_3 (a, b, c) &= (-2a, 2b, 0).
\end{aligned}
\end{equation*} Now, writing this in forms of matrices, we'll have: \begin{equation*}
\begin{aligned}
    T\alpha_1&= \begin{bmatrix}0 & 0 & 2 \\ 0 & 0 & 0 \\ 0 & -1 & 0 \end{bmatrix},\\
    T\alpha_2&= \begin{bmatrix}0 & 0 & 0 \\ 0 & 0 & -2 \\ 1 & 0 & 0 \end{bmatrix},\\
    T\alpha_3&= \begin{bmatrix}-2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \end{bmatrix}.\\
\end{aligned}
\end{equation*} So, the matrix of $T$ is going to be a $9 \times 3$ matrix as following: \begin{equation*}
    \begin{bmatrix} 
    0 & 0 & -2\\ 
    0 & 0 & 0 \\
    2 & 0 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 2 \\
    0 & -2 & 0 \\
    0 & 1 & 0 \\
    -1 & 0 & 0 \\
    0 & 0 & 0 \\
    \end{bmatrix}.
\end{equation*} Now, I think to solve the eigenvalues question, I should be able to use the second question I mentioned, however I don't know how and where to start. any help's appreciated.","['abstract-algebra', 'linear-algebra']"
4331950,"if $\lim_{k\to \infty} a_k=0$, with $a_k \geq 0$, then $\lim_{n\to \infty} \frac{1}{n}\sum_{k=0}^n a_k =0$?","I need help to prove the question or find a counterexample. I think that was true. Proof idea:
Since $\lim_{k\to \infty} a_k=0$ , given $\epsilon >0$ there is $k_0\in\mathbb{N}$ such that $a_k < \epsilon$ for all $k > k_0$ . Then for $n>k_0$ , $$
\frac{1}{n}\sum_{k=0}^n a_k =\frac{1}{n}\sum_{k=0}^{k_0} a_k+\frac{1}{n}\sum_{k=k_0+1}^n a_k <\frac{1}{n}\sum_{k=0}^{k_0-1} a_k+\frac{n-k_0}{n} \epsilon
$$ so $$
 \lim_{n\to \infty} \frac{1}{n}\sum_{k=0}^n a_k <\epsilon,
$$ and hence the proof is completed.","['limits', 'convergence-divergence', 'sequences-and-series']"
4331951,"If the limit function is differentiable, must its derivative equal the term by term derivative?","A well-known theorem states that if $\frac{d}{dx} a_n(x)$ is uniformly convergent then the limit $a_n(x)\rightarrow a(x)$ can be differentiated term by term, i.e. $\frac{d}{dx}a(x) = \lim_{n\rightarrow \infty}\frac{d}{dx}a_n(x)$ . Now suppose $f(x)=\lim_{n\rightarrow \infty} f_n(x)$ converges on $\mathbb{R}$ and is differentiable. Suppose $ f_n'(x)$ also converges pointwise everywhere. Without any uniform convergence assumptions, must we have $f'= \lim_{n\to\infty} f_n'(x)$ a.e.? I've seen an example here where they differ at one point. I think the answer is yes since pointwise convergence of continuous functions implies uniform convergence on a set of measure as large as we want, so they at least must agree on a large set. I tried to modify the proof given in Rudin but its not clear where we can swap ""uniform convergence"" for ""existence of $g$ derivative""","['measure-theory', 'real-analysis']"
4331959,"The limit $\lim_{n \to \infty} \int_0^1 f(\{n x\}) g(\{n/x\}) \, \mathrm{d} x$","This is problem 1.82 in Ovidiu Furdui's book 'Limits, Series, and Fractional Part Integrals'. Given $f,g \in \mathrm{C}([0,1])$ , let $$ a_n = \int \limits_0^1 f(\{n x\}) g\left(\left\{\frac{n}{x}\right\}\right) \, \mathrm{d} x \, , \, n \in \mathbb{N} \, , $$ where $\{\cdot\} = \cdot - \lfloor \cdot \rfloor$ is the fractional part function. We want to find the limit $\lim_{n \to \infty} a_n$ (if it exists). The known results (problems 1.70 and 1.71 in the same book) $$ \lim_{n \to \infty} \int \limits_0^1 f(x) g\left(\left\{\frac{n}{x}\right\}\right) \, \mathrm{d} x = \lim_{n \to \infty} \int \limits_0^1 f(\{n x\}) g(x) \, \mathrm{d} x = \int \limits_0^1 f(x) \, \mathrm{d} x \int \limits_0^1 g(x) \, \mathrm{d} x $$ suggest that the desired limit might also just be the product of the two integrals (which is true if either function is constant). We can write $$ a_n = \sum \limits_{k=0}^{n-1} \int \limits_{k/n}^{(k+1)/n} f(n x - k) g \left(\left\{\frac{n}{x}\right\}\right) \, \mathrm{d} x \overset{n x = k + u}{=} \int \limits_0^1 f(u) \frac{1}{n} \sum \limits_{k=0}^{n-1} g\left(\left\{\frac{n^2}{k + u}\right\}\right) \, \mathrm{d} u \, , \, n \in \mathbb{N} \, . $$ This shows that, by the dominated convergence theorem, the conjecture is true if $$ \lim_{n \to \infty} \frac{1}{n} \sum \limits_{k=0}^{n-1} g\left(\left\{\frac{n^2}{k + u}\right\}\right) = \int \limits_0^1 g(x) \, \mathrm{d} x $$ holds for almost every $u \in [0,1]$ . The left-hand side looks almost like a Riemann sum and the equation is reminiscent of the Riemann integral criterion for equidistributed sequences . However, the situation is slightly different and I do not know how to proceed from here, which is why I want to ask you: Can we complete this step of the proof? Is there another way to show that $\lim_{n \to \infty} a_n = \int_0^1 f(x) \, \mathrm{d} x \int_0^1 g(x) \, \mathrm{d} x$ ? Or is this conjecture in fact false in some cases?","['integration', 'real-analysis', 'fractional-part', 'limits', 'riemann-integration']"
4332087,"Find all positive integers $(a,b)$ such that $\displaystyle\frac{a^{2^{b}}+b^{2^{a}}+11}{2^{a}+2^{b}}$ is an integer","Find all positive integers $(a,b)$ such that $\displaystyle\frac{a^{2^{b}}+b^{2^{a}}+11}{2^{a}+2^{b}}$ is an
integer. The machine/code says that $a=2$ and $b=3$ are suitable up to symmetry. And I bet that these are the only solutions when $a≤b$ . If $a=0$ , then $1+2^b≤b+11$ implies $b≤3$ and the only solution is $b=1$ . In a similar way $a=1$ implies $b=0$ . Since the formula is symmetrical, we can assume $a,b>1$ . $a$ and $b$ cannot have the same parity, because the numerator will be odd and the denominator will be even, which is impossible. Suppose $a≥2$ is even and $b≥3$ is odd. The pair $(a,b)=(2,3)$ is a solution giving $\displaystyle\frac{a^{2^{b}}+b^{2^{a}}+11}{2^{a}+2^{b}}=29$ . For $a=2$ , there is no other solution for $b≤25$ . For $a=4$ and $a=6$ , there is no solution for $b≤25$ . But I don't think this reasoning will be enough for a complete proof...","['number-theory', 'integers', 'divisibility', 'elementary-number-theory']"
4332099,Clever proof for showing that if a graph $G$ is critically $k-$colorable then $\delta(G) \geq k - 1$,"While reading for my graph theory class, I came across a short - yet curious - proof for the following theorem: if a graph $G$ is critically $k-$ colorable then $\delta(G) \geq k - 1$ . Here is the proof to the claim: Suppose (for a contradiction) that $G$ is $k-$ critical and that $v \in V(G)$ satisfies $\text{deg}(G) < k-1$ . Then $G - v$ has a $(k-1)-$ coloring, and this coloring extends to a $k-1$ -coloring of $G$ . This yields a contradiction $._{\Box}$ Everything in this proof makes sense besides one particular item: in the second line, what does the author mean by extending a coloring from a subgraph of $G$ to the whole graph? In addition, why does $G-v$ have a $(k-1)$ -coloring that extends to all of $G$ (if that makes any sense)? I understand this may seem like an easy Google search but to be honest I can't find anything helpful and figured someone could provide some insight. Note that this is not a homework question but simply for going beyond what I am learning in class. Source: https://www.sfu.ca/~mdevos/notes/graph/445_colouring0.pdf","['graph-theory', 'solution-verification', 'coloring', 'discrete-mathematics']"
4332119,Conic chords projected by point $P$ form a quadrilateral whose vertex pairs are collinear with $P$,"Starting with a conic $c$ , a point $P$ not on $c$ , and points $D,E,F,G$ on $c$ , let $D',E',F',G'$ be the second points of intersection of the lines $PD,PE,PF,PG$ with $c$ . Let $$
\begin{align}
L &= DE\cdot FG \\
M &= D'E'\cdot F'G' \\
N &= FG\cdot F'G' \\
O &= DE\cdot D'E' \\
Q &= DE\cdot F'G' \\
R &= D'E'\cdot FG 
\end{align}
$$ It's easy to show that $O,N$ are on the polar of $P$ wrt $c$ .  But here's the question: show that the triples $P,Q,R$ and $P,L,M$ are respectively collinear. It's possible to find a projective transformation that takes $P$ to a point at infinity and $c$ to a circle.  Then the lines $DD',EE',\dots$ are parallel and it's easy to show the proposition.  But I'd like a proof that doesn't use that trick. The question arises from a simplification of 3D configuration, where $c$ is a sphere (or quadric) and two cones with apex $P$ cut $c$ in four planes.  These planes intersect in lines analogous to $L,M,\dots,R$ .  I'm hoping a projective proof in 2D will 'lift' to a proof in 3D.","['projective-geometry', 'conic-sections', 'geometry']"
4332151,Show that the full Euler-Lagrange equation of the Brachistochrone is $2y(x)y''(x)+y'(x)^2+1=0$,"This question I have is related to this previous question I asked regarding the Brachistochrone problem, so here are some background details from the previous link: If a point-like mass is rolling down a hill from a point $A$ to a point $B$ , the time it takes the mass to get from $A$ to $B$ depends on the profile of the hill. The Brachistochrone Curve is the profile that minimizes the time and we can find this profile using a Lagrangian extremization procedure. You can easily convince yourself that one can always find profiles which would make the time taken arbitrarily long, so if one finds a finite curve that extremizes the time taken, that curve should necessarily be a minimum. Before jumping to the core of this problem, let us first look at the ‘cycloid’ curve, which can be thought of as the curve a fixed point on a circle of radius $R$ draws as the circle ‘rolls’ along a straight line (see left of Fig. 4). With $\theta$ as the angle parameterizing the circle’s rotation, the coordinates on the curve are $$x=R(\theta-\sin\theta)\quad\text{and}\quad y=R(1-\cos\theta)$$ The derivatives along that curve are $x'(\theta) = R(1-\cos\theta)$ and $y'(\theta)=R \sin\theta$ and bearing in mind that $\cos\theta=1-y/R$ , ie $x'(\theta)=y$ , we have $$\left(x'(y)\right)^{-1}=y'(x)=\frac{\mathrm{d}y}{\mathrm{d}x}=\frac{y'(\theta)}{x'(\theta)}=\sqrt\frac{2R-y}{y}\tag{1}$$ Start of new question: Instead of parameterizing the trajectory as a function $x(y)$ we could have decided to parameterize the trajectory as a function $y(x)$ . Show that finding the trajectory $y(x)$ that leads to the quickest descent between the origin and the point B can also be formulated as a Lagrangian problem. Determine the relevant action, Lagrangian and coordinates. What are the resulting equations of motions. The author's solution (shown directly below) makes no attempt to derive the equation in the title (which is what I am trying to show): We have $$T=\int dt=\int\sqrt{\frac{\mathrm{d}x^2+\mathrm{d}y^2}{2gy(x)}}=\int_0^{x_B}\sqrt{\frac{y'(x)^2+1}{2gy(x)}}\mathrm{d}x$$ Now we can see the total time $T$ as a functional of the path $y(x)$ chosen to connect both points with fixed boundary conditions, the problem therefore reduces to one of Lagrangian mechanics where $T[y(x)]$ plays the role of the action, $L = \sqrt{\frac{y'(x)^2+1}{2gy(x)}}$ plays the role of the Lagrangian. In this language, the generalized coordinate is $y$ which is a function of $x$ (so $x$ plays the role of the ‘time’ $t$ is the standard Lagrangian formulation. The Euler Lagrange equations are $$\frac{\partial L}{\partial y(x)}-\frac{\mathrm{d}}{\mathrm{d}x}\frac{\partial L}{\partial y'(x)}=0\tag{A}$$ In the previous formulation, the Lagrangian did not depend on the coordinate itself (only on its derivative). This is no longer the case here as we see that this new Lagrangian depends both on $y(x)$ and $y' (x)$ . Note however that the Lagrangian does not depend explicitly on $x$ so the Hamiltonian is conserved in this formulation. Unsurprisingly, the number of conserved quantities s the same in both formulation. The full Euler-Lagrange equation is $$2y(x)y''(x)+y'(x)^2+1=0\tag{B}$$ I am trying to reach eqn $(\mathrm{B})$ .
So starting with the leftmost term of $(\mathrm{A})$ : $$\begin{align}\frac{\partial L}{\partial y(x)}&=\frac{\partial}{\partial y(x)}\left({\frac{y'(x)^2+1}{2gy(x)}}\right)^{1/2}\\&=\sqrt{\frac{y'(x)^2+1}{2g}}\frac{\partial}{\partial y(x)}\left(y(x)^{-1/2}\right)\\&=-\frac12y(x)^{-3/2}\sqrt{\frac{y'(x)^2+1}{2g}}\end{align}$$ Now moving on the right hand term of $(\mathrm{A})$ : $$\begin{align}\frac{\partial L}{\partial y'(x)}&=\frac{\partial}{\partial y'(x)}\left({\frac{y'(x)^2+1}{2gy(x)}}\right)^{1/2}\\&=\frac{1}{2}\times 2y'(x)\left(y'(x)^2+1\right)^{-1/2}\times \frac{1}{\sqrt{2gy(x)}}\\&=\frac{y'(x)}{\sqrt{2gy(x)}\sqrt{1+y'(x)^2}}\end{align}$$ and so $$\begin{align}-\frac{\mathrm{d}}{\mathrm{d}x}\frac{\partial L}{\partial y'(x)}&=-\frac{1}{\sqrt{2g}}\frac{\mathrm{d}}{\mathrm{d}x}\left(y'(x)\left(y(x)\left[1+y'(x)^2\right]\right)^{-1/2}\right)\\&=-\frac{1}{\sqrt{2g}}\left(y''(x)\left(y(x)\left[1+y'(x)^2\right]\right)^{-1/2}-\frac12y'(x)^{-3/2}y'(x)\left[1+y'(x)^2\right]^{-1/2}-\frac12\times 2y''(x)y'(x)y(x)^{-1/2}\left[1+y'(x)^2\right]^{-3/2}\right)\\&=-\frac{1}{\sqrt{2g}}\left(y''(x)\left(y(x)\left[1+y'(x)^2\right]\right)^{-1/2}-\frac12y'(x)^{-1/2}\left[1+y'(x)^2\right]^{-1/2}-y''(x)y'(x)y(x)^{-1/2}\left[1+y'(x)^2\right]^{-3/2}\right)\tag{C}\end{align}$$ Now putting it all together eqn $(\mathrm{A})$ implies $$\frac{\partial L}{\partial y(x)}-\frac{\mathrm{d}}{\mathrm{d}x}\frac{\partial L}{\partial y'(x)}=0$$ $$\implies-\frac12y(x)^{-3/2}\sqrt{\frac{y'(x)^2+1}{2g}}+\frac{1}{\sqrt{2g}}\left(y''(x)\left(y(x)\left[1+y'(x)^2\right]\right)^{-1/2}-\frac12y'(x)^{-1/2}\left[1+y'(x)^2\right]^{-1/2}-y''(x)y'(x)y(x)^{-1/2}\left[1+y'(x)^2\right]^{-3/2}\right)=0$$ I know this can be simplified a little, but I'm getting the impression that I have made a mistake or used the wrong approach (by using the triple product rule) to get $(\mathrm{C})$ for the $-\frac{\mathrm{d}}{\mathrm{d}x}\frac{\partial L}{\partial y'(x)}$ part of $(\mathrm{A})$ . Any hints or tips that will lead me closer to $2y(x)y''(x)+y'(x)^2+1=0\tag{B}$ please?","['multivariable-calculus', 'calculus', 'euler-lagrange-equation', 'calculus-of-variations']"
4332160,How to find the Fisher Information for $\overline{Y}$ where $Y_i := X_i+\mu$?,"Let $(X_i)_i$ be a sequence of i.i.d. RVs. Assume that the density $f_X$ is continuously differentiable, $supp(f_X) = \mathbb{R}$ and $xf_X(x) \rightarrow 0$ for $x \rightarrow \pm \infty$ . Further assume that $\mathbb{E}[X_i] = 0$ and $\mathbb{V}[X_i] < \infty$ for all $i$ . Now consider $Y_i := X_i + \mu$ for all $i$ . Consider $Y_i := X_i+\mu$ for $\mu \in \mathbb{R}$ and let $\overline{Y} := \frac{1}{n} \sum_{i=1}^n Y_i$ be an estimator for $\mu$ . Find a lower bound for $\mathbb{V}[\mu]$ . I recognise that the assumptions allow us to use the Cramer-Rao Lower Bound, so we just need to compute the Fisher Information $I(\mu)$ for $\overline{Y}$ , so $$I(\mu) = n \cdot \mathbb{E}\Bigg[ \bigg(\frac{\partial \ \log(f_\overline{Y}(y \mid \mu))}{\partial \ \mu}\bigg)^2\Bigg],$$ but I do not know how what the density function of $\overline{Y}$ is. By the transformation formula I see that $f_Y(y) = f_X(y-\mu)$ , but I do not understand how to go on from here. Could you please give me a hint?","['statistics', 'probability-theory']"
4332161,"Show that if $f_n\to f_1$ uniformly and $f_n\to f_2$ in $L^p$, then $f_1=f_2$ almost everywhere.","Let $(X,\Sigma _X,\mu )$ be a measurable space, $(f_n)_{n\in\mathbb{N}}$ a sequence of $L^p(X)$ with $p\in [1,\infty ]$ and $f_1,f_2:X\to \mathbb{R}$ two measurable functions. Show that if $f_n\to f_1$ uniformly and $f_n\to f_2$ in $L^p$ , then $f_1=f_2$ almost everywhere. I proved that $|f_1(x)-f_2(x)|\leq \limsup _{n\to\infty }|f_n(x)-f_2(x)|$ for all $x\in X$ . I also know that there's an increasing sequence $(k_n)_{n\in\mathbb{N}}$ of $\mathbb{N}$ such that $\lim_{n\to\infty }|f_{k_n}(x)-f_2(x)|=\limsup_{n\to\infty}|f_n(x)-f_2(x)|$ for all $x\in X$ . I was able to prove what I asked if we assume that $|f_{k_n}(x)-f_2(x)|\leq \limsup_{n\to\infty}|f_n(x)-f_2(x)|$ for all $n\in\mathbb{N}$ and $x\in X$ . However I don't know how to prove the last inequality. Thank you for your attention!","['measure-theory', 'analysis', 'real-analysis', 'lp-spaces', 'measurable-functions']"
4332167,In a semigroup does left identity and right inverse imply a group?,"Say we have a set $ G $ with the properties $ G $ is closed under some opperation $ \circ  $ $ G $ is associative under $ \circ  $ There exists an element $ e\in G $ such that for every $ g \in G $ we have \begin{align*}
  e\circ g=g .
\end{align*} For every $ g\in G $ there exists $ g^{-1}\in G $ such that \begin{align*}
  g\circ g^{-1}=e.
\end{align*} Is it a group? I have tried manipulating the expressions to get the standard definition of a group but to no success. I don't even know if it is true. I found this post Right identity and Right inverse implies a group , which shows something close, but not exactly what I want. Moreover, the top comment states In case you don't know: Right identity and Left inverse does not imply group. and I wonder if left identity and right inverse do not imply a group. If anybody could give me some guidance I would greatly appreciate it.","['group-theory', 'abstract-algebra', 'semigroups']"
4332176,The sequence $n\sin(\sqrt{4\pi^2n^2 +x^2})$ converges on compacts:,"I'm not sure about the idea behind the sequence $f_n(x)=n\sin(\sqrt{4\pi^2n^2 +x^2})$ being uniformly convergent on every compact of the form $[0,a]$ for $a>0$ . On $\mathbb{R}$ we can find a sequence such that $\sin$ alternates the sign, thus making it impossible for $f_n$ to have a limit. But the compact case is a little more tricky: let $K=[0,a]$ , then by Weierstrass Thm $\forall n\exists M_n\mid$ $|f_n(x)|<M_n$ , thus we wish for $M_n$ to be convergent. As we are using Weierstrass Thm, we can take $x_n\in K$ and $f_n(x_n)=M_n$ . But $K$ is compact in $\mathbb{R}$ : $\exists x_{n_k}\mid x_{n_k}\rightarrow\bar{x}\in K$ , thus $f_{n_k}(x_{n_k})=M_{n_k}\rightarrow\overline{M}\in\mathbb{R}$ , making $f_n$ uniformly convergent. The proof seems to make sense but on the other hand it doesn't depend on $f_n$ , so it sounds like I'm either missing something.","['real-analysis', 'solution-verification', 'uniform-convergence', 'sequences-and-series', 'limits']"
4332190,Metric space isometries and integration by substitution,"Let $X$ be a locally compact metric space. Assume that a commutative operation "" $+$ "" is defined on $X$ with $(X,+)$ forming a group. Assume further that for any $y\in X$ , $x\mapsto x+y$ is a bijective isometry. (We say $h:X\rightarrow X$ is an isometry iff for all $x_1,x_2 \in X$ , $d(x_1,x_2)=d(f(x_1),f(x_2))$ .) Let $\int_X \cdot \,dx$ denote the integral formed from the Haar measure on the locally compact topological group $(X,+)$ . What is the weakest possible additional assumption for it to be true that for all bijective isometries $f:X\rightarrow X$ : $$\int_X {g(x)\,dx}=\int_X {g(f(x))\,dx}$$ for all functions $g:X\rightarrow\mathbb{R}$ for which the integrals are defined. Notes: If $X=\mathbb{R}$ , then $f$ is a shift and/or reflection, and the claimed result holds by a trivial substitution. If $X=\mathbb{R}^n$ with the usual metric, then $f$ is a translation followed by a unitary transformation, so the result again goes through. It is not clear that it holds with non-standard metrics though. The result also holds for circles and spheres with the standard metric. The claimed result will not hold in general if $f$ is not an isometry. For example, take $X=[0, 1]$ , $g(x)=x$ and $f(x)=x^2$ . I would guess that the missing condition ensures that the metric in some sense agrees with the measure. For example, consider the Manhattan or max metrics on Euclidean space. They generate the same topology and thus (?) the same Haar measure, but it seems unlikely the integral condition holds for these metrics. New note: This question (just discovered) is clearly very closely related: Existence of isometry-invariant measures The only missing step is from the invariance of the measure to the invariance of the integral, but I guess this is trivial.","['integration', 'measure-theory', 'metric-spaces', 'change-of-variable', 'isometry']"
4332207,"How to show that $(-\infty,c)$ is measurable by caratheodory criterion?","Let, $m$ be the lebesgue outer measure. Let, $E \subset \mathbb{R}$ . $$m(E) \le m(E \cap (-\infty,c)) + m(E \cap [c,\infty)),$$ is true from definition of outer measure, but I'm having trouble showing the reverse inequality. Where, $$m(E) = \inf\{\sum^{\infty}_{j=1}|B_j|: E \subseteq \bigcup^{\infty}_{ j=1}B_j, B_j \textrm{ is a open interval} \}$$ Any help? Thanks.","['measure-theory', 'real-analysis']"
4332251,Coordinate-free definition of tangent space of differentiable manifold,"I am working on some notes about differentiable manifolds and I am trying to introduce the concept of tangent space at a point $P\in\mathbb{M}$ without making explicit reference to a coordinate patch. I consider the set $C(P)$ of all (smooth as necessary) curves $\gamma: I\in\mathbb{R}\to\mathbb{M}$ such that $P\in \gamma(I)$ . Since it is always possible to shift $I$ by a constant, I will assume that $\gamma(0)=P$ . On such set of curves, I introduce an equivalence $\sim$ as follows: $\gamma\sim\beta$ if $\forall f:\mathbb{M}\to\mathbb{R}$ (smooth as necessary) $$
\frac{d(f\circ \gamma)}{dt}|_{t=0}=\frac{d(f\circ \beta)}{dt}|_{t=0}.
$$ I define the tangent space as the quotient space $C(P)/\sim$ . Next, I need to show that $C(P)/\sim$ is a vector space. The zero element is the equivalent class of the constant curve $\gamma(t)=P$ . Let $\alpha$ be a real scalar, $\gamma'\in C(P)/\sim$ , and $\gamma$ a curve in the equivalence class $\gamma'$ . Then $\alpha\gamma'$ is the equivalence class that contains $\gamma(\alpha t)$ . So far, so good. The problem I am having is how to construct the equivalence class of the sum of two equivalence classes without falling back to considering the representation of the curves in a coordinate patch. In other words, how can I define the sum in a coordinate independent way? Thank you for your time","['tangent-spaces', 'smooth-manifolds', 'differential-geometry']"
4332287,Continuous on a set,I am trying to figure out what it means for a function to be continuous on a set. Everything I look up just gives me answers on continuity at a point. I guess I am confused as to what the difference is. Does it have anything to do with LHS and RHS limits?,"['continuity', 'functions']"
4332292,Show that $2\cos(\frac{\theta}{2})=\cos \theta$ iff $\theta = (4n+2)\pi \pm 2\phi$,"Q: Show that $2\cos(\frac{\theta}{2})=\cos \theta$ if and only if $\theta=(4n+2)\pi\pm 2\phi$ where $\phi$ is defined by $\cos(\phi)= \frac{1}{2}(\sqrt 3-1)\;$ , $0\le \phi\le \frac{1}{2}\pi$ , and $n$ is any integer Workings $2\cos(\frac{\theta}{2}) = \cos(\theta) \iff 2\cos(\frac{\theta}{2}) = \cos^2(\frac{\theta}{2}) - \sin^2(\frac{\theta}{2}) \iff 2\cos^2(\frac{\theta}{2})-2\cos(\frac{\theta}{2})-1=0$ So we have, $$ \cos(\frac{\theta}{2}) = \frac{1}{2} \pm \frac{\sqrt{3}}{2}$$ but, $\cos(\frac{\theta}{2}) \not = \frac{1}{2} +\frac{\sqrt{3}}{2}$ > 1 So $\cos(\frac{\theta}{2}) = \frac{1}{2} -\frac{\sqrt{3}}{2}$ $\therefore \frac{\theta}{2} = \pm \cos^{-1}(\frac{1-\sqrt{3}}{2})+2\pi n$ $\therefore \theta = \pm 2\cos^{-1}(\frac{1-\sqrt{3}}{2})+4\pi n$ I notice that the form is similar to what the question wants us to show. But it's not quite the same. Any idea where I've gone wrong?","['algebra-precalculus', 'trigonometry']"
4332298,Is this a known method for solving linear equations in 1 variable?,"I thought of a method to solve linear equations in one variable and I want to know if this is a well-known method or not. It's an elementary method, which makes me think it has probably been known for a thousand years, yet I can't find any information about it online. Given a linear equation with one variable, you test $x=0$ and $x=1$ as potential solutions, and if they are not, the differences between left and right sides of the equation can be used to determine the solution. Denote the error caused by $x=0$ as $E_{0}$ and the error caused by $x=1$ as $E_{1}$ . Then the solution to the equation is $\frac{E_{0}}{E_{0}-E_{1}}$ For example: Consider the equation $5(6x-3)+1=2(7-9x)+4$ . $x$ LHS RHS Error $0$ $-14$ $18$ $32$ $1$ $16$ $0$ $-16$ The solution is $x=\frac{32}{32-(-16)}=\frac{2}{3}$ . This works by considering $(0,32)$ and $(1,-16)$ as points on a line, and then finding the $x$ intercept of that line. This method works for special cases as well: The equation is true for any number if and only if both errors are $0$ The equation has no solutions if and only if the errors are equal (to something other than $0$ ) Thank you to anyone who can point me to any writing that already exists about this method.","['algebra-precalculus', 'math-history']"
4332319,"Let $G = \langle x, y \mid x^{7} = y^{3} = e,\; yxy^{−1} = x\rangle$. What is $|G|\,$?","Let $G = \langle x, y \mid x^{7} = y^{3} = e,\; yxy^{−1} = x\rangle$ . What is $|G|\,$ ? What I've done so far: $$yxy^{-1} = x \implies yx = xy.$$ The group $G$ is an abelian group. I can only think that $|G| = 21$ , because I don't know how else to proceed.","['group-presentation', 'combinatorial-group-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4332346,"$A=\left\{\sum_{k=1}^n a_ke^{kx}:a_k\in\mathbb R, n\in\mathbb N\right\}$ is dense in $C[a,b]$","I need to prove that $$A=\displaystyle\left\{\sum_{k=1}^n a_ke^{kx}:a_k\in\mathbb R, n\in\mathbb N\right\}$$ is dense in $C[a,b]$ . I had in mind to use the Stone–Weierstrass' theorem, I know that $A$ is an algebra that separates points, but we also need $A$ to contain non-null constants which is extremely complicated for me or at least I can't see the way. I have also tried to prove the density directly, but fall to prove that every polynomial can be uniformly approximated by elements of $A$ . Any ideas?","['functional-analysis', 'real-analysis']"
4332366,Calculate the circulation of the vector field alone a parameterized circle (Stoke's Theorem...?),"Find the circulation of the following vector field $\vec{F}(x, y, z) = \langle \sin(x^2+z)-2yz, 2xz + \sin(y^2+z), \sin(x^2+y^2)\rangle$ along the circle $\vec{r}(t)=\langle\cos(t), \sin(t), 1\rangle$ with $t\in [0, 2\pi]$ . I tried using Stoke's theorem to solve it, but I get a difficult integral trying it this way: $$\oint \vec{F}\cdot dr = \iint_S \nabla \times \vec{F} \cdot ds = \int_{0}^{2\pi} \vec{F}(\vec{r}(t))\cdot \vec{r}'(t) \,dt$$ What am I doing wrong?","['greens-theorem', 'multivariable-calculus', 'curl', 'stokes-theorem']"
4332388,Calculate the ratio based on the lengths of edges of triangle,"The problem My problem is, Let $\triangle ABC$ be an acute triangle, circumscribed in $(O)$ and has orthocenter $H$ . Let $HO$ intersect $(O)$ at $E$ and $F$ , as shown in the image. $AE$ cuts $BC$ at $K$ and $AF$ cuts $BC$ at $L$ . $AO$ cuts $BC$ at $T$ . Calculate the value of $\frac{TK}{TL}$ according to the side-lengths and angles of $\triangle ABC$ My approach is that I used the Anti-Steiner point and then use Menelaus Theorem. But that does not actually relates to $a,b,c$ (which are the side lengths of $BC$ , $CA$ , $AB$ , respectively). Any help is appreciated!","['euclidean-geometry', 'triangles', 'circles', 'geometry']"
4332419,"On the pointwise convergence of $\sum_{n\ge 1} \frac{\sin nx}{n}$ to the sawtooth function in $(-\pi,\pi)$","My question stems from Exercise $8$ , in Chapter $2$ of Stein and Shakarchi's Fourier Analysis . I have verified that $$\frac{1}{2i} \sum_{n\ne 0} \frac{e^{inx}}{n} = \sum_{n\ge 1} \frac{\sin nx}{n}$$ is the Fourier series of the $2\pi$ -periodic saw-tooth function defined by $f(0) = 0$ , and $$f(x) = \begin{cases} \frac{-\pi-x}{2} & -\pi < x < 0\\ \frac{\pi - x}{2} & 0 < x < \pi\end{cases}$$ The book says: Note that this function is not continuous. Show that nevertheless, the series
converges for every $x$ (by which we mean, as usual, that the symmetric partial
sums of the series converge). In particular, the value of the series at the origin,
namely $0$ , is the average of the values of $f(x)$ as $x$ approaches the origin from
the left and the right. By Dirichlet's test for convergence, I have shown that $\sum_{n\ge 1} \frac{\sin nx}{n}$ converges for all $x\in (-\pi,\pi) \setminus\{0\}$ . At $x = 0$ , $\sum_{n\ge 1} \frac{\sin nx}{n}$ clearly converges to $0$ . So, $\sum_{n\ge 1} \frac{\sin nx}{n}$ converges for all $x\in (-\pi,\pi)$ . Question: The series $\sum_{n\ge 1} \frac{\sin nx}{n}$ converges to $f$ , at $0$ . Does it also converge to $f$ at other points in $(-\pi,\pi)$ ? If yes, how can we show this with elementary methods ? Please note that this is only the second chapter of Stein and Shakarchi's Fourier Analysis , so the machinery we can use is limited . In particular, we can't use Dirichlet conditions , etc. Related questions: Post 1 . Edit(s) : Integrating the identity suggested by @mathcounterexamples.net from $0$ to $x$ , we get $$\sum_{n=1}^N \frac{\sin nx}{n} = -\frac{x}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx$$ I have trouble evaluating the last integral, and don't know how to proceed. For $0 < x < \pi$ , we want $$\left|\sum_{n=1}^N \frac{\sin nx}{n} + \frac{x-\pi}{2}\right| =\left|-\frac{\pi}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx\right| \xrightarrow{N\to\infty} 0$$ and for $-\pi < x < 0$ , we want $$\left|\sum_{n=1}^N \frac{\sin nx}{n} + \frac{x+\pi}{2}\right| =\left|\frac{\pi}{2} + \frac{\sin Nx}{N} + \int_0^x \sin Nx \cot\frac{x}{2}\, dx\right| \xrightarrow{N\to\infty} 0$$","['fourier-series', 'convergence-divergence', 'fourier-analysis', 'analysis']"
4332476,"For every square matrix $U$ there exist a diagonal matrix $E$ with $e_{i,i}=-1 , 1 $ such that $E U - I_n$ is non-singular","For every square matrix $U$ there exist a diagonal matrix $E$ with $e_{i,i} = \pm 1$ such that $E U - I_n$ is non-singular. If $U$ is unitary then $EU$ is also unitary Note that all computation is over coefficient modulo $3$ . I want to prove this statement, but have no idea how and where to start.","['matrices', 'linear-algebra']"
4332585,Solution to a system of ODE:s,"Consider a system of linear ODE:s: $$x'(t)=Ax(t)+C$$ Here $A$ and $C$ are coefficient matrices. If $C=0$ , the solution is $x(t)=\exp(Ax(t))x(0)$ , where $\exp$ represents the matrix exponential. Can we write a similar solution for a general $C$ ?","['systems-of-equations', 'ordinary-differential-equations']"
4332586,How to calculate curvature of the Earth and its effect on a 25-meter footprint?,"I am working on a problem that considers a remote sensing instrument hovering over the Earth with a 25 meter footprint. I'd like to see the effects of Earth's curvature on this footprint even if it's minuscule. The Earth curves about 8cm per 1 km or $0.00007858km/1km$ . I tried using this ratio and converted it to meters: $0.00007848km/1000m$ = $0.0000007848km$ of curvature per meter $0.00000007848m \cdot 25m$ $ = 0.000001962km$ of curvature in footprint. My advisor said this doesn't make sense to solve curvature with a linear solution since it does not behave linearly. I think he's referring to the fact that the 'footprint' of a laser shining onto the surface curves in both directions since it's a circle. It doesn't just curve in one dimension. What would've been the correct steps to approach this problem or represent it? It sounds like I would just add another dimension to it and scale it to something like the following: $T(x,y)=(0.000001962m)y^2 \cdot (0.000001962m)x^2$ if x and y are in meters.","['trigonometry', 'geometry']"
4332607,Partial Derivative of Hadamard Root (elements-wise square root),"Given $\mathbf{X} \in \mathbb{R}^{n \times 1}$ , $\mathbf{A} \in \mathbb{R}^{n \times n}$ , the function is $\mathbf{f}=\sqrt{\mathbf{A} \mathbf{X} \odot \mathbf{A} \mathbf{X}}$ , where $\sqrt{(\cdot)}$ is Hadamard root (elements-wise square root), and $(\cdot) \odot (\cdot)$ is the Hadamard Product . How to compute $\frac{\partial \mathbf{f}}{\partial \mathbf{X}}$ ? I have tried to follow this similar question and answer to solve my problem, but the function of that question is Frobenius inner product, which is different with mine so can not solve in that way. Thanks in advance!","['matrices', 'matrix-calculus', 'linear-algebra', 'partial-derivative', 'derivatives']"
4332659,Spectrum $\sigma(a)$ taken within the unitisation of a $C^*$-algebra,"My question relates to reconciling the definition of the spectrum $\sigma(a)$ of a point $a$ in a $C^*$ -algebra $A$ in both the unital case and the more general case using the unitisation, since I appear to have a mistake in my understanding. I outline below the definitions I am using and the problem I have arrived at. Each $C^*$ -algebra $A$ has a unitisation $\tilde{A}$ , which is defined to be the complex vector space $A\times\mathbb{C}$ with the multiplication $$(a,\lambda)(b,\mu):=(ab+\lambda b+\mu a,\lambda\mu),$$ the involution $(a,\lambda)^*:=(a^*,\overline{\lambda})$ and the norm $$\|(a,\lambda)\|:=\max\lbrace\|L_{a,\lambda}\|,|\lambda|\rbrace,$$ where $L_{a,\lambda}:A\to A$ is given by $L_{a,\lambda}(b):=ab+\lambda b$ . $A$ then sits inside $\tilde{A}$ as a maximal ideal. If $A$ is unital , we define its spectrum at $a$ to be $$\sigma(a):=\lbrace\lambda\in\mathbb{C}:\lambda 1_A-a\notin\text{Inv}(A)\rbrace.$$ Then, in general, we define $$\sigma_A(a):=\sigma_{\tilde{A}}((a,0))\equiv\lbrace\lambda\in\mathbb{C}:\lambda(0_A,1)-(a,0)\notin\text{Inv}(\tilde{A})\rbrace.$$ First of all, $\sigma(1_A)=\lbrace1\rbrace$ based on the definition for unital $C^*$ -algebras. Now suppose that $0\notin\sigma_{\tilde{A}}((1_A,0))$ . Then $(1_A,0)\in\text{Inv}(\tilde{A})$ so there exists $b\in A$ and $\mu\in\mathbb{C}$ for which $(1_A,0)(b,\mu)=(b+\mu1_A,0)=(0_A,1)$ , a contradiction, so $0\in\sigma_A(1_A)$ based on the second definition!? Where have I gone wrong?","['c-star-algebras', 'operator-algebras', 'banach-algebras', 'functional-analysis', 'spectral-theory']"
4332680,What does Heron's formula naturally deform?,"Fixing three real numbers $a,b,c>0$ determines a triangle with
side-lengths $a,b,c$ (if admissible). Therefore, the area of a
triangle is a function in $a,b,c$ . Due to the geometry of a
triangle, we know that the area is a symmetric function in $a,b,c$ . Indeed, Heron formula shows that $$area(a,b,c) = \frac{1}{4} \sqrt{[(a+b+c)] \cdot [(a+b-c)(a-b+c)(-a+b+c)]}$$ What's interesting is that the radicand is the product of two
symmetric polynomials. The fact that it's a product (and
therefore not as ""pure"") motivates me to look at the following
expression: $$f(a,b,c,d) = (a+b+c-d)(a+b-c+d)(a-b+c+d)(-a+b+c+d)$$ Notice that $f$ is a symmetric polynomial, with $f(a,b,c,0)$ recovering the ""impure"" product. Question : Does $f$ calculate anything? In other words, is
there any known combinatorial or geometrical meaning of $f$ ? I
will be glad to see if it deforms the area of a triangle in
some interesting way, but any interpretation is welcome.","['euclidean-geometry', 'symmetric-functions', 'combinatorics', 'symmetric-polynomials', 'symmetric-groups']"
4332692,Constructing an LR test for a concrete example and find its critical areas and power function,"I am working on the following exercise: We are given an observation of a discrete RV $X$ with PMF $f(x \mid \theta)$ and $\theta \in \{0,1,2\}$ as in the table below. Find the LR test for the hypothesis $H_0: \theta = 0$ and list all possible critical areas for such an LR test. From there take a level $\alpha$ -test for $\alpha = 0.15$ and find its power function $\beta(\theta)$ . $$\begin{pmatrix}
x &f(x \mid 0) &f(x \mid 1) &f(x \mid 2) \\
1 &3/4  &1/4 &1/3 \\
2 &1/8 &1/8 &1/3 \\
3 &1/8 &1/2 &1/6 \\
4 &0 &1/8 &1/6 \\
\end{pmatrix}$$ I am new to LR tests and can not quite see through its definition yet. Here is what I got so far: We defined the test statistic for the LR test as $$\lambda(X) := \frac{L(\hat{\theta_0} \mid X)}{L(\hat{\theta} \ \mid X)},$$ where $L$ is the likelihood function. If I am not mistaken we should have: $$\lambda(X) = \begin{cases}
        \frac{3/4}{3/4} = 1, & \text{for } X = 1 \\
        \frac{1/8}{1/3} = 3/8, & \text{for } X = 2\\
        \frac{1/8}{1/2} = 1/4, & \text{for } X = 3\\
        \frac{0}{1/6} = 0, & \text{for } X = 4.
        \end{cases}
 $$ For the LR tests we defined the critical area $K$ as $K := \{x \in \mathcal{X} \ \mid \lambda(x) < k\}$ , where $\mathcal{X}$ is the sample space. In the lecture we then said that we need to find some $k$ such that $\sup_{\theta \in \Theta_0} P_{\theta}(\lambda < k) \le \alpha$ , however, I do not see how I should do this here. Could you please help me?",['statistics']
4332722,Evaluating an improper integral using the Residue Theorem,"I have: \begin{equation}
\int_{-\infty}^{\infty}\frac{x^2}{(x^2+1)(x^2+9)}dx
\end{equation} and I want to solve it using a complex closed contour on C. I do the following: \begin{equation}
\int_{-\infty}^{\infty}\frac{z^2}{(z^2+1)(z^2+9)}dz
\end{equation} which clearly has 4 poles, $\pm i,\pm3i $ . The greatest radius of the clontour is thus $\rho=3$ , therefore I get the following integral form: \begin{equation}  
\int_{\gamma_3}\frac{z^2}{(z^2+1)(z^2+9)}dz=2\pi i Res(f;i,+3i)
\end{equation} which gives for i: \begin{equation}
Res(i)=\lim_{z\longrightarrow i}(z-i)\frac{z^2}{(z-i)(z+i)(z+3i)(z-3i)}dz=i/16
\end{equation} and for 3i: \begin{equation}
Res(3i)=\lim_{z\longrightarrow 3i}(z-3i)\frac{z^2}{(z-i)(z+i)(z+3i)(z-3i)}dz=-i/48
\end{equation} Plugging into the formula given above, I get: \begin{equation}  
\int_{\gamma_3}\frac{z^2}{(z^2+1)(z^2+9)}dz=2\pi i (i/16-i/48)=-\frac{\pi}{12}
\end{equation} But this is not correct, as the integral is negative.  Where is the error? Thanks!","['integration', 'complex-analysis', 'indefinite-integrals']"
4332724,Folland Theorem 3.22,"At the start of the proof for Theorem 3.22, Folland says that $dv=d\lambda + fdm$ implies $d|v|=d|\lambda| + |f|dm$ . I get why this is the case for positive and signed measures, but I'm not sure how to prove this for the complex measure case. Could anyone guide me in the right direction? This is my attempt based on Eric Towers', comment: $dv=d\lambda + fdm$ is such that $\lambda \perp m$ (ie. There exists a measurable set $E$ such that $\lambda(E)=m(E^c)=0$ ). We can decompose $v$ in this way: $\forall A$ measurable, $v(A)=v(A\cap E) + v(A\cap E^c)$ . Moreover, $v(A\cap E)=\int_{A\cap E}fdm$ and $v(A\cap E^c)=\lambda(A\cap E^c)$ . Note that $v_E(A)=v(A\cap E)$ and $v_{E^c}(A)=v(A\cap E^c)$ are measures. Since there exists a positive measure $u$ where $v<<u$ as in Folland chapter 3.3, we can represent $v=\int g du$ for some $g$ . Then $v(A\cap E)=\int_{A\cap E} g du=\int_{A\cap E}fdm=v_E(A)$ . Similarly, $v(A\cap E^c)=\int_{A\cap E^c} g du=\lambda(A\cap E^c)=\int_{A\cap E^c} \frac{d\lambda}{d|\lambda|}d|\lambda|=v_{E^c}(A)$ . Therefore, $|v|(A\cap E)=\int_{A\cap E}|g|du=|v_{E}|(A)=\int_{A\cap E}|f|dm$ and similarly, $|v|(A\cap E^c)=\int_{A\cap E^c} |g| du=\int_{A\cap E^c} |\frac{d\lambda}{d|\lambda|}|d|\lambda|=|v_{E^c}|(A)$ . But from Folland chapter 3.3, $|\frac{d\lambda}{|d\lambda|}|=1$ a.e. Thus, $|v|(A)=|v|(A\cap E) + |v|(A\cap E^c)=\int_{A\cap E^c}1d|\lambda| + \int_{A\cap E} |f|dm$ . Recalling from above that $\lambda(E)=m(E^c)=0$ , the result follows.","['measure-theory', 'radon-nikodym']"
4332771,How to find an example for a measure which is not continuous from above?,"I need to give an example of a measure $\mu$ and subsets $A_n$ s.t. $$A_1\supset A_2\supset ...$$ and $$\mu(\cap_{n=1}^\infty A_n)\neq \lim_{n\rightarrow \infty} \mu(A_n)$$ I hat the following in mind. Take $\mu$ to be the counting measure and $A_1=\mathbb{N}, A_2=\mathbb{N}\setminus\{1\},...$ then $$\mu(\cap_{n=1}^\infty A_n)=\mu(\emptyset)=0$$ but now I have some strugles to show the other part with the limes. could someone help me please? It would be nice if we could procede with this example since this was my own idea without using any internet.",['measure-theory']
4332774,"Matrix A and B (both mxn) have the same four spaces, proof this: (Is my reasoning ok?)","Suppose the m by n matrices A and B have the same four subspaces. If they are
both in row reduced echelon form, prove that F = G: $$A=
\begin{bmatrix}
I & F \\
0 & 0 \\
\end{bmatrix}
$$ $$
B=\begin{bmatrix}
I & G \\
0 & 0 \\
\end{bmatrix}
$$ My reasoning:
Because we know they have the same Nullspace, then \begin{array}{l}
\text{if $Ax=0$ and $Bx=0$ then }
\text{$Ax-Bx=0$}
\text{ then $(A-B)x=0$ because x $\ne$ 0}
\end{array} $$
\begin{bmatrix}
0 & F-G \\
0 & 0 \\
\end{bmatrix}=0
$$ So, $F-G=0$ then $F=G$ ¿What do you think? ¿Is this ok? ¿How would you do this?
Thank you.","['matrices', 'solution-verification', 'linear-algebra']"
4332807,function with two horizontal asymptotes and f'(0) = 0,"I am trying to find a smooth function that satisfies the properties $
\begin{align} 
\bullet &\qquad f'(0) = 0 \\
\bullet &\qquad \lim_{x\to\infty} f(x) = y_1  \\
\bullet &\qquad \lim_{x\to-\infty} f(x) = y_2
\end{align}
$ I had in mind something like the graph of $sech(x^2)$ with   different asymptotes on either side  ( sketch here ). Is it possible to do this without composing piecewise functions? edit: thanks for the responses! They made me realize I was not specific enough in what I was asking for. I need $f(x=0)$ to also be a global maximum if possible","['functions', 'asymptotics']"
4332853,Natural and Predictable Processes in Continuous Time (Reference/Proof Request),"In discrete time, it is quite easily shown that a process is predictable if and only if it is natural (as seen here for example).  However, in continuous time it is not nearly so clear (and certainly not clear to me).  We define a process $A$ , assumed to be of finite variation, as natural if and only if for every bounded RCLL martingale $M$ , we have $$E(M_tA_t) = E \int_0^t M_{s-} dA_s $$ which is analogous to the discrete time definition (of coure the integral makes sense as $A_s$ is finite variation). My question is: how does one prove that a process is natural if and only if it is predictable (i.e. measurable with respect to the sigma field generated by left continuous adapted processes)? I have never seen the proof of this fact in continuous time, and would really like to as a pathway to understand this note on the Doob Meyer decomposition theorem .  Any help would be massively appreciated, and I am sorry that I really have no idea where to start.","['stochastic-processes', 'probability-theory']"
4332869,Proving there exists a unique group structure such that there exists a homomorphism $A \to A/{\sim}$.,"Here is the context for this problem. Let $A$ be a group, $A' \subset A$ a subgroup, and define $\sim^{l}$ and $\sim^r$ , the left and right relations, respectively, on $A$ with respect to $A'$ by declaring $a \sim^{l} b$ if there exists $a' \in A'$ such that $b = a' a$ and $a \sim^{r} b$ if there exists $a'' \in A'$ such that $b = a a''$ . I proved that $A'$ is normal in $A$ if and only if $\sim^{l}$ and $\sim^{r}$ coincide. . If $A'$ is normal, then the left and right quotient sets are equal (though, as I understand, it is not the case that every element commutes; using more familar notation, if $H \leq G$ is a subgroup, then $gH = Hg$ for all $g \in G$ , but it is not the case that given $h \in H$ , $gh = hg$ for all $g \in G$ .) Finally, given a group $A$ , let $\pi: A \to A/{\sim}$ be the canonical projection $a \mapsto \overline{a}$ , where $\overline{a}$ is the coset containing $a$ . I am trying to prove the following theorem. If $A'$ is a normal subgroup of $A$ , the set $A/A'$ has a unique group structure for which the map $\pi: A \to A/A'$ is a group homomorphism. The hint suggests that I use the same idea as in the proof of the following theorem: Let $X$ be a set with an equivalence relation $\sim$ . Let $\phi: X \to Y$ be a map of sets such that $x_1 \sim x_2$ implies $\phi(x_1) = \phi(x_2)$ . Then there exists a uniquely defined map $\tilde{\phi}: X/{\sim} \to Y$ such that $\phi = \tilde{\phi} \circ \pi$ . I mention this result because the attempt that I have in mind does not proceed similarly to this proof. Here is what I had in mind. (Attempt) Let $A' \subset A$ be normal, $\pi: A \to A/{\sim}$ the canonical quotient map. Suppose that we have defined a group structure on $A/{\sim}$ such that $\pi$ is a group homomorphism. Let $\bullet$ denote the group operation on $A$ and $\star$ denote the group operation on $A/{\sim}$ . Then for any $a,b \in A$ , we have $$
\pi(a \bullet b) = \pi(a) \star \pi(b),
$$ so $$
\overline{a \bullet b} = \overline{a} \star \overline{b}. 
$$ So the group operation $\star$ must send the product of two cosets $\overline{a}$ and $\overline{b}$ to the coset of the product $a \bullet b$ , where $a \bullet b$ is some element of $A$ . I assumed that there is in fact such a group structure, but I believe I also need to show that this actually gives rise to a group. Closure. If $\overline{a}, \overline{b} \in A/{\sim}$ , then $\overline{a} \star \overline{b} = \overline{a \bullet b} \in A/{\sim}$ since $a \bullet b \in A$ . Having shown that this is in fact a binary operation, I'm going to drop explicit mention of $\star$ and $\bullet$ going forward. Associativity. Let $\overline{a}, \overline{b}, \overline{c} \in A/{\sim}$ . Then we have: $$
(\overline{a} \overline{b})(\overline{c}) = \overline{ab} \overline{c} = \overline{(ab)c} = \overline{a(bc)} = \overline{a} \overline{bc} = \overline{a} (\overline{b} \overline{c}),
$$ wherein we have used associativity in the group $A$ . Identity Let $e$ denote the identity in the group $A$ . I claim that the coset $\overline{e}$ is the identity element we seek for $A/{\sim}$ . Let $\overline{a} \in A/{\sim}$ . Then $$ 
\overline{a} \overline{e} = \overline{ae} = \overline{a}
$$ and $$
\overline{e} \overline{a} = \overline{ea} = \overline{a},
$$ so $\overline{e}$ is the desired identity element. Inverses. Given $\overline{a} \in A/{\sim}$ , $a \in A$ , so $a^{-1} \in A$ . Then $\overline{a^{-1}} \in A/{\sim}$ . I claim that $\overline{a^{-1}} = (\overline{a})^{-1}$ . We have: $$
\overline{a} \overline{a^{-1}} = \overline{aa^{-1}} = \overline{e}
$$ and $$
\overline{a^{-1}} \overline{a} = \overline{a^{-1} a} = \overline{e}.
$$ Therefore, $A/{\sim}$ with the proposed operation is a group. How does this proof look?","['normal-subgroups', 'group-theory', 'solution-verification']"
4332899,Geometric intuition for adjoint,"Let $V$ be a finite-dimensional inner product space, and let $T$ be a linear operator on $V$ . Then $T^*$ ( $T$ adjoint) is defined as the unique function such that $\langle T(x), y \rangle = \langle x, T^*(y) \rangle$ for all $x, y \in V$ . Furthermore, $T^*$ is linear. I know how to manipulate the adjoint algebraically, but I'm not sure how to interpret it geometrically. This has been asked before, but the previous questions did not suit my needs. Definition of adjoint operator (asking for intuition) I'm not asking about $T^*$ 's relation to $A$ 's conjugate transpose. Geometric intuition of adjoint I'm asking about intuition about $T^*$ , not $\text{Ker}(T^*)=(\text{Im}(T))^\perp$ . https://mathoverflow.net/q/6552 The answers to this question feel too complicated to me. https://mathoverflow.net/q/6573 I haven't yet learned about Hilbert spaces. Also, I don't know what $\langle \: |$ and $| \: \rangle$ are. https://mathoverflow.net/q/6567 biadjacency matrix?","['adjoint-operators', 'linear-algebra']"
4332947,"How do I solve $\sin x+x\cos x=0\,$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How do I solve $\sin x+x\cos x=0\,$ ? I've tried several different trigonometric identities and I'm aware it can be also written as $$\tan x=-x.$$ One of the answers is zero, but the other answers elude me. I've tried graphing $\tan x$ and $y=-x$ simultaneously as well, but I want to find the answer numerically.","['trigonometry', 'numerical-methods']"
4332950,Cellular homology computation,"I know the theory behind cellular homology, but I'm having troubles in the actual computation.
The cellular boundary formula states that: $$d(e^n_{\alpha})=\sum_{\beta}d_{\alpha \beta}e^{n-1}_{\beta}$$ Where, if $n>1$ : $$d_{\alpha\beta}=\deg(\varphi_{\alpha\beta})$$ $$\varphi_{\alpha\beta}: S_{\alpha}^{n-1}\xrightarrow{\text{attaching map}}X^{n-1}\xrightarrow{\pi}\frac{X^{n-1}}{X^{n-1}-e^{n-1}_{\beta}}\cong S_{\beta}^{n-1}$$ Where the last homeomorphism is defined by $\Phi_{\beta}:D^{n-1}_{\beta}\to X$ (the characteristic map of $e^{n-1}_{\beta}$ ). In fact we can restrict $\Phi_{\beta}$ in the codomain to $X^{n-1}$ and then $\Phi_{\beta}$ passes to the quotients(as a homeomorphism!): $$\tilde\Phi_{\beta}:\frac{D^{n-1}_{\beta}}{S^{n-2}_{\beta}}\to\frac{X^{n-1}}{X^{n-1}-e^{n-1}_{\beta}} $$ So: $$\tilde\Phi_{\beta}^{-1}:\frac{X^{n-1}}{X^{n-1}-e^{n-1}_{\beta}}\to\frac{D^{n-1}_{\beta}}{S^{n-2}_{\beta}}\cong S^{n-1}_{\beta} $$ Where the last homeomorphism is canonical. This is crystal clear! But when I read online examples of (non-trivial) computation, I just can't understand what's going on.
For example, sometimes I see the attaching map represented as a ""word"", and I don't understand what this representation means.
I didn't manage to find online any non-trivial examples of computation that actually shows in detail how to calculate these degrees rigorously.
(Maybe I just lack some intuition about the degree) Now I'll try to show how I would compute the homology the thorus. I'll choose the standard CW-decomposition of the thorus, so I have one $0$ -cell,two $1$ -cells, one $2$ -cell: $$0\leftarrow \mathbb{Z}\xleftarrow{d_1} \mathbb{Z}^2 \xleftarrow{d_2} \mathbb{Z}\leftarrow 0$$ Since there is only one $0$ -cell we have $d_1=0$ .
Now we compute $d_2$ .
Let $\color{red}{e^1_1}$ , $\color{blue}{e^1_2}$ be the two $1$ -cells The coefficient $d_{11}$ : $$d_{11}=\deg\left(\varphi_{11}:S_{1}^{1}\xrightarrow{\text{attaching map}}X^{1}\xrightarrow{\pi}\frac{X^{1}}{X^{1}-e^{1}_{\beta}}\cong S_{1}^{1}\right)$$ Intuitively the attaching map attachs the $2$ -disk along the $1$ -cells(intuitively we can divide the circumference in $4$ quarters; two opposite quarters are sent in the same $1$ -cell and the four junction points of the quarters are sent in the only $0$ -cell ).
Now since the canonical projection collapse the blue $1$ -cell and the $0$ -cell in one point, the map $\varphi_{11}:S^1_1\to S^1_1$ is: Now: $$\deg(\varphi_{11})=H_1(\varphi_{11})[\mathcal{C}]=[\varphi_{11}\circ \mathcal{C}]$$ Where $\mathcal{C}(x_0,x_1)=e^{2\pi ix_0}$ is the canonical generator of $H_1(S^1)$ . Now I don't know how to proceed. According to the results $\deg(\varphi_{11})$ should be $0$ , so $\varphi_{11}\circ \mathcal{C}$ should be a boundary. But I don't know if this is the right way. Thank you in advance!","['general-topology', 'homology-cohomology', 'cw-complexes', 'algebraic-topology']"
4332957,How to solve this nonlinear diff eq of celestial mechanics?,"$$(\dot{r})^2 = \frac{2 \mu}{r} + 2h$$ Where mu and h are constants. I have no idea how to solve it, maybe there is a trick I didn't know. The only thing that came in mind is to integrate $$\int \frac{dr}{\sqrt{\frac{2 \mu}{r} + 2h}} = \int dt$$ but I don't think this is really a solution, since I don't know too how to evaluate the integral in terms of elementary functions. So, any tips?","['indefinite-integrals', 'calculus', 'ordinary-differential-equations']"
4332982,Functions that satisfy $f(f(f(x))) = x$ for $f : \mathbb{R} \to \mathbb{R}$,"This is my first post on Math SE soo... I was reading over a thread a while ago that claims the only solution to $f(f(f(x))) = x$ for $f : \mathbb{R} \to \mathbb{R}$ is $f(x) = x$ , but.. I seemed to have found a counterexample: $$f(x) = \frac{1}{1-x}$$ Here, we have $$f(f(f(x))) = \frac{1}{1-\frac{1}{1-\frac{1}{1-x}}} = \frac{1}{1+\frac{1-x}{x}} = \frac{1}{\frac{1}{x}} = x$$ Well... looking over the original problem statement, would we say that my function satisfies $f : \mathbb{R} \to \mathbb{R}$ ? Or would it be something more like $f : \mathbb{R} \setminus \{1\} \to \mathbb{R} \setminus \{0\}$ ?",['functions']
4333012,Maximizing linear objective subject to quadratic equality constraint,"I've been trying to get through some practice questions on the Karush-Kuhn-Tucker (KKT) theorem but I can't seem to answer the following. Given $f, g : \mathbb{R}^2 \to \mathbb{R}$ defined by $f(x) := x_1 + x_2$ and $g(x) := x_1^2+3x_1x_2+3x_2^2-3$ , respectively, $$\begin{array}{ll} \underset{x \in \mathbb{R}^n}{\text{maximize}} & f(x)\\ \text{subject to} & g(x) = 0\end{array}$$ My attempt: $$\nabla f(x)=\begin{pmatrix}
1\\
1\\
\end{pmatrix}$$ $$\nabla g(x)=\begin{pmatrix}
2x_1+3x_2\\
3x_1+6x_2\\
\end{pmatrix}$$ and by complementary slackness $\lambda[x_1^2+3x_1x_2+3x_2^2-3]=0$ and $\lambda\geq0$ By first order conditions, I get $\lambda[2x_1+3x_2]=1$ and $\lambda[3x_1+6x_2]=1$ I checked WolframAlpha and the answer should be (3,-1) but I can't seem to figure out the right steps to solve this optimization.","['optimization', 'multivariable-calculus', 'qclp']"
4333021,Calculating integrals of the form $\int\frac{dx}{(1+x^{q})^\frac{p}{r}}$,"I am trying to understand how to calculate integrals of the form $\int\frac{dx}{(1+x^{q})^\frac{p}{r}}$ , where $p,q,r \in  \Bbb Z$ . I know how to calculate the integral $\int \frac{dx}{(1+x^2)^\frac{1}{2}}$ . However I cannot do it if $\int \frac{dx}{(1+x^3)^\frac{1}{2}}$ or $\int \frac{dx}{(1+x^5)^\frac{1}{2}}$ . Is there any method of evaluating closed from expressions of integrals of the form $\int\frac{dx}{(1+x^{q})^\frac{p}{r}}$ ? I have tried U-subbing using $x=z^{r/q} -1$ ,but it doesn't work.","['indefinite-integrals', 'calculus', 'analysis']"
4333062,An identity involving Catalan numbers and binomial coefficients.,"I stumbled upon the following identity $$\sum_{k=0}^n(-1)^kC_k\binom{k+2}{n-k}=0\qquad n\ge2$$ where $C_n$ is the $n$ th Catalan number. Any suggestions on how to prove it are welcome! This came up as a special case of a generating function for labeled binary trees. Actually I can directly prove the identity is zero by showing that certain trees don't exist, but I expect that seeing a direct proof will help me find nice closed formulae for other coefficients of the generating function.","['summation', 'catalan-numbers', 'combinatorics']"
4333086,Show that the Matrix mapping f defined as $f(A) = A^p$ is differentiable,"I've been asked to show that for a function $f:M_{n\times n}(\mathbb R)\longrightarrow M_{n\times n}(\mathbb R) $ that defined as $f(A) = A^p$ for all $2\leq p\in \mathbb N$ is differentiable. I've tried solving this using induction and showing that the partial derivatives are continues and then to conclude that f is differentiable. On the case of p = 2 is pretty simple to show that, but for the case of multiplying p matrices is more complex and I feel like i'm not on the right way.
Any suggestions or alternative ways of solving it? Edit: I found a different approach for this case, in addition to the solution mentioned below. I can define the function $h:M_{n\times n}(\mathbb R)\longrightarrow M_{n\times n}(\mathbb R)\times M_{n\times n}(\mathbb R)$ which is defined by $h(X) = (X,X^{p-1})$ and then define the bilnear map $g:M_{n\times n}(\mathbb R)\times M_{n\times n}(\mathbb R) \longrightarrow M_{n\times n}(\mathbb R)$ and now we get that $f=g\circ h$ . now $g$ is a bilnear map thus it's differentiable, and now by induction on $p$ we can solve the problem using the following formula: Derivative of the bilnear map $\beta:\mathbb R^m\times\mathbb R^n\longrightarrow\mathbb R^d$ is $D\beta(x_0,y_0)(h,k)=\beta (x_0,k)+\beta(h,y_0)$ And the chain rule.","['matrices', 'derivatives', 'linear-algebra', 'real-analysis']"
4333093,Prove $\frac{(q^n-1)(q^{n-1}-1)\dots (q^{n-r+1}-1)}{(q^r-1)(q^{r-1}-1)\dots(q-1)}=\sum_{\lambda\subseteq\Pi}q^{^{|\Pi\backslash\lambda|}}$,"Prove the combinatorial identity $$\frac{(q^n-1)(q^{n-1}-1)\dots (q^{n-r+1}-1)}{(q^r-1)(q^{r-1}-1)\dots(q-1)}=\sum_{\lambda\subseteq\Pi}q^{^{|\Pi\backslash\lambda|}},$$ where the summation is performed over all different Young diagrams $\lambda$ that fit into a rectangle $\Pi$ of size $r\times(n-r)$ , and the exponent $|\Pi\backslash\lambda|$ is equal to the number of cells in the complement of the diagram to a rectangle (the empty diagram $\lambda=\emptyset$ and the entire rectangle $\lambda=\Pi$ are also taken into account). The left side reminds me of formulas for the number of subspaces of a given dimension in an $n$ -dimensional space over a field of $q$ elements. However, I am not sure if this is how one should reason.","['combinatorics', 'geometry']"
4333147,Find SDE satisfied by transformation of solution to a different SDE,"Suppose that $X_t$ satisfies $dX_t = Y_t dt + Y_t dW_t$ , where $dY_t = Y_t dW_t$ . What can we say about the SDE that $\ln{(X_t)} + \frac{t}{2}$ solves? I'm not sure how to use the SDE that $X_t$ solves to find the SDE that $\ln{(X_t)} + \frac{t}{2}$ solves. In general, is there a way to find the SDE that the transformation of the process solves? I know that $dY_t = Y_t dW_t$ is a GBM with no drift, so that means $Y_t = Y_0 e^{-\frac{1}{2}t + W_t}$ . Then we have $dX_t = Y_0 e^{-\frac{1}{2}t + W_t} (dt + dW_t)$ . Is there any easy way to solve this? I'm not sure how to efficiently apply Ito's lemma in this case. Once a solution to $X_t$ is found, would we then apply that transformation and find the SDE it solves? Am I overcomplicating things? I'm not sure if there is a better way, and I'm still stuck trying to solve for $X_t$ . Any help is appreciated!","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory']"
4333152,"How many $4$-permutations are there of the set $\{A,B,C,D,E,F\}$ if whenever $A$ appears in the permutation, it is followed by $E$?","Case 1: when $A$ does not appear in the $4$ -permutation $5 \cdot 4 \cdot 3 \cdot 2 = 120$ Case 2: when $A$ does appear in the $4$ -permutation. Then $E$ should follow $A$ . Let $X$ stand for $AE$ , now we should consider $3$ -permutations, since $X$ occupies two spaces. Since $X$ stands for two elements, we consider the number of elements to be $5$ . $5 \cdot 4 \cdot 3 = 60$ Thus, number of permutations of the set $\{A,B,C,D,E,F\}$ where if whenever $A$ appears in the permutation, it is followed by $E$ is $120 + 60 = 180$ . Is my reasoning correct? EDIT Alternative reasoning for Case 2 when A does appear in the 4-permutation. Then E should follow A
Let X stand for AE, now we should consider 3-permutations, since X occupies two spaces Now X can be arranged in the following way X, *, * *, X, * *, *, X Thus we have $1 \cdot 4 \cdot 3 = 12$ $4 \cdot 1 \cdot 3 = 12$ $4 \cdot 3 \cdot 1 = 12$ So for case 2 the total number of permutations = $12 + 12 + 12 = 36$ Thus  Number of permutations of the set {A,B,C,D,E,F} if whenever A appears in the permutation, it is followed by E is = $120 + 36 = 156$ So which of my reasoning is correct? And why is the other one incorrect? Help greatly appritiated Thanks","['permutations', 'combinatorics', 'discrete-mathematics']"
4333184,Trigonometric related rates,"I need help with the relationship between the variables and the derivatives In the below question, I thought it would be something like $\tan(\theta)=\frac{y}{x}$ where $y=40t$ and $x=25-30t$ . And then, at the maximum, there would be no change in the angle so it would be zero. But when I differentiate, I get $0=\frac{40(25-30x)+30(40x)}{(25-30x)^2}$ If my intuition is correct, I'd want to say that the distance is closest when it's an isosceles triangle - that is, when $x=y$ and so I could find some other values. A navy vessel is located at $25$ km from the enemy shoreline where an enemy helicopter base is located. The vessel is moving toward the base at a constant speed of $v_p = 30$ km/h. At this time (referred to as $t = 0$ for convenience), its early warning radar detected that an enemy helicopter is located on the ground in the base and is taking off vertically with a constant speed $v_h = 40$ km/h (see figure). To determine the optimal time to fire a laser beam to shoot down the helicopter, the following information must be provided to the commander. (a) At what time will the distance between the vessel and the helicopter be shortest? What is this distance? Assume that the speeds and directions of both the vessel and helicopter are constant. (b) The laser is kept pointing directly at the helicopter at all times. At the time found in (a), what is the rate of change of the angle theta at which the laser is aimed (see figure)?","['related-rates', 'calculus', 'trigonometry']"
4333283,Can we define a norm on $\Bbb{R^\omega}$ in a basis free way?,"Let $\Bbb{R^\omega}=\{(x_n)_{n\in \mathbb{N}}: x_n \in \Bbb{R}\}$ .
Then, $(\Bbb{R^\omega}, +, \cdot) $ is a linear space.
I know , if $(x_n) $ are $p$ - summable, then we can define norm , $\ell_p$ -norm ( $1\le p<\infty $ ) on $\Bbb{R^\omega}$ . And if $(x_n) 's$ are bounded we can define supremum norm, $\ell_{\infty}$ on $\Bbb{R^\omega}$ . The best thing I can do for general $\Bbb{R^\omega}$ (no special assumption on sequences) is to define a metric on $\Bbb{R^\omega}$ by $$d(x, y) =\sum_{j\in\mathbb{N}}{(a_j)} \frac{|x_j -y_j|}{1+|x_j -y_j|}$$ where $(a_j) _{j\in\mathbb{N}}$ is any convergent series of positive reals. I can show that the metric isn't induced by a norm on $\Bbb{R^\omega}$ . But by checking a particular metric on $\Bbb{R^\omega}$ , doesn't gives us an opportunity to make sure that the linear space $\Bbb{R^\omega}$ is not a normed space. I also know that the existence of Hamel basis of a linear space implies the linear space is a normed space. Again to prove existence of Hamel basis we need Zorn's lemma, an equivalent version of AC. Question: Can we define a norm in a basis-free way on $\Bbb{R^\omega}$ to make it a normed space?","['normed-spaces', 'metric-spaces', 'functional-analysis', 'axiom-of-choice', 'set-theory']"
4333288,"Eliminate $t$ from $h=\frac{3t^2-4t+1}{t^2+1}, k=\frac{4-2t}{t^2+1}$","Eliminate the parameter $t$ from $$h=\frac{3t^2-4t+1}{t^2+1}$$ $$k=\frac{4-2t}{t^2+1}$$ This is not the actual question. The question I encountered was: Perpendicular is drawn from a fixed point $(3, 4)$ to a variable line which cuts positive x-axis at one unit distance from origin. Circle $S(x, y) = 0$ represents the locus of the foot of perpendicular drawn from point $(3, 4)$ to the variable line. If radius of $S(x, y) = 0$ is $\sqrt{λ_1}$ and length of tangent drawn to $S(x, y) = 0$ from origin is $\sqrt{λ_2}$ , then determine $\lambda_1$ and $\lambda_2$ . My attempt: I attempted the original question by assuming the variable line as $x+ty-1=0$ and calculating the coordinates of foot of perpendicular $(h,k)$ on it from $(3,4)$ . I got the coordinates $(h,k)$ as $$(h,k)\equiv \left(\frac{3t^2-4t+1}{t^2+1}, \frac{4-2t}{t^2+1}\right)$$ I am clueless after this point on how to eliminate $t$ . Solving for $t$ from either of the equations and substituting it in the other is very cubersome and not a very efficient method because I'm preparing for a competitive exam. I verified using WolframAlpha that the locus is a circle but am unable to obtain it in the standard form. I am looking for a relatively short method/trick for these type of locus. Thanks.","['analytic-geometry', 'algebra-precalculus', 'circles', 'conic-sections']"
4333316,"Number of 10 digit numbers having digits 0,1,2 so that no two 0s are consecutive.","Question : Find the number of 10 digit numbers having digits 0,1,2 so that no two 0s are consecutive. (here a number cannot begin with 0) And here's my attempt : Let $b_n$ denote the number of such numbers with length $n$ and ending with $1,2$ (here the leading digit cannot be $0$ ) and $a_n$ the number of numbers ending with $0$ .
Let $c_n=a_n+b_n$ , we want to find $c_{10}$ .
Note that $$a_n=b_{n-1}$$ since the second last digit of this thing should be a $1$ or a $2$ . Also, $b_n=b_{n-1}+a_{n-1}=c_{n-1}$ . It suffices to find $b_{11}$ but $b_n=b_{n-1}+b_{n-2}$ so it is easy to compute. Note: I am not looking for the answer, just to check if my approach is correct and if better approaches exist EDIT :  I guess this is a slightly less confusing way : denote by $a_n,b_n,c_n$ number of ""numbers"" of length $n$ ending with $0,1,2$ respectively, and $d_n=a_n+b_n+c_n$ . Clearly, $b_n=c_n$ and $a_n=b_{n-1}+c_{n-1}=2b_{n-1}$ . Further $b_n=c_n=a_{n-1}+b_{n-1}+c_{n-1}=d_{n-1}$ . But $b_n=2b_{n-2}+2b_{n-1}$ which is easy to compute","['contest-math', 'combinatorics', 'recurrence-relations']"
4333337,If $(3p_n\pm \sqrt{5p_n^2-4})\Delta/(2p_n)$ is an integer then $p_n$ is an odd-indexed Fibonacci number,"Suppose $1<p_1<\dots<p_n$ are pairwise relatively prime integers and let $\Delta=p_1\cdots p_n$ . In the proof of Lemma 4.8 of this paper ( https://arxiv.org/pdf/1606.08656.pdf ), there is the following paragraph. If $$D=\frac{3p_n\pm \sqrt{5p_n^2-4}}{2}\cdot \frac{\Delta}{p_n} $$ is an integer then $p_n$ is an odd-indexed Fibonacci number. In this case $\sqrt{5p_n^2-4} $ is an odd-indexed Lucas number and $D=p_1\cdots p_{n-1}F_{2m-1}$ . Note that $D=\frac{3p_n- \sqrt{5p_n^2-4}}{2p_n}\Delta<\frac{2}{3}\Delta$ . Actually the paper is about symplectic embeddings of rational homology balls into $\Bbb CP^2$ , and I can't understand most of this paragraph. I only know the definitions of Fibonacci and Lucas numbers. Why is $p_n$ an odd-indexed Fibonacci number $F_{2m+1}$ (assuming $D$ is an integer)? In this case why is $\sqrt{5p_n^2-4} $ an odd-indexed Lucas number? Also how do we have $D=p_1\cdots p_{n-1}F_{2m-1}$ ? In the last sentence how the sign $\pm$ in $D$ became $-$ ?","['lucas-numbers', 'number-theory', 'fibonacci-numbers', 'elementary-number-theory']"
4333358,Each chart of the canonical structure of a submanifold S is locally a submanifold chart,"I am bugged about some technical details in the following proof regarding submanifolds. I am reading a book that is not in english , so I'll try to translate here a couple of terms in the best possible way (privileged chart and submanifold chart)  but I am not sure thouse are the english names, in case  you know them, please tell me. Definition of submanifold Let $M$ be an n-manifold and $S$ a subspace of $M$ . $S$ is said a submanifold of dimension $d$ of $M$ is for every point $p$ of $M$ , there is a chart $(U,x)$ of $M$ such that $x(U\cap S)=x(U)\cap (\mathbb{R}^d\times \{0\}^{n-d})$ . In other words, if the points $q$ that fall in $U$ are characterized of the $n-d$ equations $x^{d+1},...,x^{n}(q)=0$ . Such a chart is called a privileged chart(with respect to S) and determins a chart $(U \cap S,x_{U \cap S})$ , called submanifold chart (if we identify $\mathbb{R}^d$ with $\mathbb{R}^d \times \{0\}^{n-d}$ )
If $(U,x), (V,y)$ are privileged charts, we have that $x(U\cap V \cap S)$ is an open set in $x(U\cap S)$ and so in $\mathbb{R}^d$ and $y_{V\cap S} \circ (x_{U\cap S})^{-1}=(y\circ x^{-1})_{x(U \cap V \cap S)}: x(U \cap V \cap S) \to y(U \cap V \cap S)$ is differentiable, so that the submanifold charts form an atlas and determine a d-dimensional differential structure over $S$ . With this structure, $S$ is a d-manifold. A d-submanifold is always thought of with this canonical or natural submanifold structure Proposition: Each chart of the canonical structure of a submanifold S is locally a submanifold chart. Proof. Let $p$ be a point in the domain of the chart $(V,y)$ of $S$ . By definition, ...(1) there exists a privileged chart $(U,x)$ of $M$ at $p$ that we can supposed such that $U \cap S \subset V$ and $ x(U)=A \times B$ with $A$ an open set at $0$ in $\mathbb{R}^d$ and B an open set at $0$ in $\mathbb{R}^{n-d}$ ...(2) By means of the chart $x$ , the canonical projection $\sigma : A\times B \to A\times \{0\}$ induces a sort of projection $\pi = x^{-1} \circ \sigma \circ x :U \to U \cap S$ which is differentiable ...(3) Thus we have that $(U,z)$ with $z=(y^1\circ \pi, ...,y^d\circ \pi,x^{d+1},...,x^n)$ with values in $y(U\cap S)\times B$ is a privileged chart and $z|_{U\cap S}=y|_{U\cap S}$ ...(4) My questions are: 1 They start using a definition of what?. If it is the definition of a d-submanifold,as I would expect, why aren't they using the definition above, that is $x(U\cap S)=x(U)\cap (\mathbb{R}^d\times \{0\}^{n-d})$ )? 2 Why can we suppose $U \cap S \subset V$ and why do we need that? and why do we take opens sets at $0$ necessarily ?) 3 Why is $\pi = x^{-1} \circ \sigma \circ x$ differentiable? My guess is that $\sigma $ is differentiable by definition of product topology (if that is applicable here), but still i would need to know that $x $ is differentiable for  the composition to be differentiable as well, but all I know is that $x$ is a homeomorphism, being $(U,x)$ a chart 4 Why is $z|_{U\cap S}=y|_{U\cap S}$ and why do we need to mention it ? I'm sorry for all these questions, but I have been around this for days and they are all related to this short proposition,so I don't think it made sense to make independent questions.... can someone please shed some light?","['manifolds', 'submanifold', 'differential-geometry']"
4333382,Index of pure braid group,"The index of a subgroup $H$ in a group $G$ is the number of left cosets of $H$ in G,  or equivalently, the number of right cosets of $H$ in $G$ . The index is denoted $(G:H).$ Let $S$ be a surface. If $S$ is neither the sphere nor the projective plane, then what is $(B_{k}(S):PB_{k}(S))?$ Here $B_{k}(S)$ and $PB_{k}(S)$ are braid group and pure braid group of $S.$ Does anybody have a reference of $(B_{k}(S):PB_{k}(S))?$","['group-theory', 'braid-groups']"
4333415,Why is conjugacy as an equivalence relation special in groups?,"The conjugacy transformation seems to be especially special in groups. Defining a conjugacy equivalence relation and splitting groups into their conjugacy classes is vital in classifying and decomposing groups. My question is, intuitively, why is the conjugacy equivalence relation special (as opposed to using some other notion of 'equivalence') for groups? My only current intuition is that, for matrix groups, matrices that differ only by a basis transformation do intuitively represent the same symmetry transformation (just in another basis). However this intuition seems to be tied to matrices, so I was wondering if there was a better, more fundamental, intuition?","['automorphism-group', 'abstract-algebra', 'group-theory', 'group-actions', 'soft-question']"
4333455,Complex conjugation and homeomorphisms,"According to this paper , Serre proved that there exists a pair $X, X'$ of smooth complex projective varieties, such that $X, X'$ are conjugate but not homeomorphic. Here, we say that $X, X'$ are conjugate if there exists a diagram $\require{AMScd}$ \begin{CD}
X @>{}>> X'\\
@VVV @VVV\\
\operatorname{Spec} \mathbb{C} @>{\sigma}>> \operatorname{Spec} \mathbb{C},
\end{CD} where $\sigma$ is complex conjugation. I'm completely confused by this statement: complex conjugation induces a $\mathbb{Z}/2$ action on $\mathbb{CP}^n$ by homeomorphisms. By assumption, $X \subset \mathbb{CP}^n$ , and the image of $X$ under this homeomorphism is precisely $X'$ (at least I think so...). Why does this not contradict Serre theorem?",['algebraic-geometry']
4333460,"Why are Aleph Cardinal Numbers ""strictly increasing""?","perhaps my title isn't very clear, I'll try to be more precise:
The definition I'm using is that (for the ""successor"" step of the recursion) $\aleph_{\alpha+1}$ is the smallest cardinal which is strictly greater that $\aleph_{\alpha}$ .
I take this to mean that $\aleph_{\alpha+1} = \inf \{$ cardinals $C$ such that there exists a one sided injection, but not a bijection between $C$ and $ \aleph_{\alpha} \}$ What I can't figure out is how to show that with this definition, $\aleph_{\alpha+1}$ indeed remains strictly greater than $\aleph_{\alpha}$ , ie there is no bijection between $\aleph_{\alpha}, \aleph_{\alpha+1}$ .
I know that there exists the generalized continuity hypothesis which would settle this question, but I'm thinking it is probably overkill and that I should be able to show this some other way, no?
If someone could help me with what is probably a simple reasoning that would be great.","['elementary-set-theory', 'cardinals']"
4333476,"Showing that the distribution of record times $(\tau_k)_{k\geq 1}$ doesn't depend on the distribution, $F$, of the records $X_i$","I read that it's possible to show that the distribution of a record time sequence doesn't depend on the distribution of the record sequence itself, but how would one do this? So $(X_i)$ is an iid sequence with common continuous distribution $F$ . Then $X_1$ is the first record, and $\tau_1$ the first record time. From here on, the $(k+1)$ st record time is $\tau_{k+1}$ , given by $\tau_{k+1}=\min\{i>\tau_k: X_i >M_{\tau_k}\}$ , for $k\geq1$ , with $X_{k+1}$ being the $(k+1)$ st record, and $M_{\tau_k}$ denoting the maximum of $X_{i-1}$ 's until time $\tau_k$ . It is supposedly then possible to show that the sequence $(\tau_{k})_{k\geq1}$ doesn't depend on $F$ , but I'm unsure how this can be done? I tried looking at the probability \begin{align*}
P(\tau_{k+1} \leq t) & = P(\min\{i>\tau_k: X_i >M_{\tau_k}\}\leq t)\\
& = P(i\leq t, i-1\leq t, \ldots),
\end{align*} for some $t$ , since if $i\leq t$ , then all 'previous' $i$ 's must necessarily also be less than or equal to $t$ , but I don't know if this is the way to go.","['probability-distributions', 'probability-theory', 'order-statistics', 'extreme-value-analysis']"
4333481,Probability that at least one triplet of points chosen will fall under the same straight line,"While I was observing some houseflies around, I wondered what is the probability that at least one triplet of them chosen will fall under the same straight line . I made some assumptions so as to remove ""infinity"" from the picture. Firstly, let $a$ denote the area occupied by one ""housefly"" (point) and $A$ denote the area of square plane considered. Total number of points (lattice points) is $\lfloor \frac{A}{a} \rfloor$ . Let $n$ denote number of points in one side of the square plane. $n = \sqrt{\lfloor \frac{A}{a} \rfloor}$ . I started from a very simple case. Let us consider a square with $3$ points on its sides. Let me assume that only $3$ points are chosen. For $3$ of them to fall on a straight line is equivalent to chosing $3$ points from any fixed straight line. Since there are $8$ straight lines containing $3$ points, so total number of favorable selections is ${3 \choose 3} . 8 = 8$ and total possible selections is $ {9 \choose 3} = 84$ . Therefore Probability $= 0.095$ .
Let us now consider a square with $4$ points on its sides. Let me assume that only $3$ points are chosen. Since there are $10$ straight lines containing $4$ points and $4$ straight lines containing $3$ points, so total number of favorable selections is ${4 \choose 3}.10 + {3 \choose 3}.4 = 44$ and total possible selections is ${16 \choose 3} = 560$ . Therefore probability $= 0.078$ . Let us now consider a square with $5$ points on its sides. Let me assume that only $3$ points are chosen. Since there are $12$ straight lines containing $5$ points, $4$ straight lines containing $4$ points and $4$ straight lines containing $3$ points, so total number of favorable selections is ${5 \choose 3}.12 + {4 \choose 3}.4 + {3 \choose 3}.4 = 140$ and total possible selections is ${25 \choose 3} = 2300$ . Therefore probability $= 0.0608$ . Generalizing this idea we get, $$P(n) = \frac{{n \choose 3}.(2n+2) + ({n-1 \choose 3}+{n-2 \choose 3}+...+{3 \choose 3}).4}{{n^2 \choose 3}}$$ or $$\boxed{P(n) = \frac{{n \choose 3}.(2n-2) + {n+1 \choose 4}.4}{{n^2 \choose 3}}}$$ where $P(n)$ denotes the probability that $3$ points chosen in a plane with lattice points $n^2$ will be collinear. Generalizing this idea to choosing $r$ points, probability that all $r$ points lie on the same straight line is $$P(n,r) = \frac{{n \choose r}.(2n-2) + {n+1 \choose r+1}.4}{{n^2 \choose r}}$$ The $P(n)$ vs $n$ graph is plotted below: Now, coming to the final part of the question, let $x$ denote the total number of points chosen. Total number of triplets is ${x \choose 3}$ . The chances of each triplet being collinear is given by $P(n)$ . Therefore using the concept of probability, it can be concluded that probability that at least one triplet of points chosen will fall under the same straight line $$\boxed{P'(n) = 1 - (1-P(n))^{x \choose 3}}$$ Here is a $P'(n)$ vs $x$ graph for $n = 100$ . The ""strange"" thing is that probability approaches $1$ for $x>30$ . Does that mean if we choose 30 points out of only $100^2$ lattice points we may not be able to make a 30-sided polygon? Am I correct in my deductions? N.B.: This question is different from this because I am not dealing with ""infinity"".","['solution-verification', 'probability']"
4333517,Hopf bifurcation computation fail,"For few days I am working on Hopf bifurcation of a system like this: $$
\frac{dx}{dt}=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\
\frac{dy}{dt}=\frac{1-y-cx^2 y }{1+x^2}
$$ above $\alpha$ is the bifurcation parameter and $a,\;b,\;c$ are real parameters.  I want to determine the parameter value that the Hopf bifurcation would occur. I have 2 sources (a thesis and an article focusing on another set of equations) for finding the parameters, I tried to recreate their results but I failed miserably.  Found steady state values as $x=x(\alpha),\;y=y(x)$ $$
0=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\
0=\frac{1-y-cx^2 y }{1+x^2}\\
$$ then for combining two equations from y $$
y=\frac{1}{\alpha}\frac{x^2+bx+1}{x^2  + a x }\\
y=\frac{1}{1+cx^2}
$$ and got an equation for $x$ and $\alpha$ only $$
{\alpha}=\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x }
$$ now stated bifurcation parameter in terms of the stated state. Then for $$
F(x,y)=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\
G(x,y)=\frac{1-y-cx^2 y }{1+x^2}
$$ wrote the characteristic equation and solve for lambda to find eigenvalues of the system. Meanwhile replaced $$
y\to\frac{1}{1+cx^2}\\
{\alpha}\to\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x }
$$ at the characteristic equation solution.
For finding Hopf bifurcation I look for the the values of $x$ making the eigenvalue of the Jacobian $0$ , such that by replacing those $x$ values can find the exact Hopf b. value from $$
{\alpha}=\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x }
$$ however it does give wrong answers. Can you trace something wrong here? (Let's say everything looks good can you please suggest a document where each step in the computation is clear so that I can go through and realize where i do wrong) Thank you for your time.","['nonlinear-system', 'nonlinear-analysis', 'ordinary-differential-equations']"
4333521,"Operator norm in Hilbert space, Schur criterion for infnite matrices","Let H be Hilbert with an orthonormal basis $(e_n)_n$ . Consider $A,B>0$ and two sequence $a_n>0, b_n>0$ such that $$\sum_{i=1}^\infty b_i(T e_i, e_j)\leq Aa_j \quad\text{and}\quad \sum_{j=1}^\infty a_j(T e_i, e_j)\leq Bb_i.$$ Show that $T$ extends to continuous linear operator and that $\|T\|\leq \sqrt{AB}$ Assume that $(Te_i, e_j)=\frac{1}{j+i-1}$ prove that $\|T\|\leq \pi.$ If $(Te_i, e_j)=\frac{1}{2^{j+i-1}}$ then compute $\|T\|.$ As explained in this question it is possible to get that, $$\|T\|\leq \Big( \sum_{j=1}^\infty \sum_{i=1}^\infty (Te_i, e_j)^2\Big)^{1/2}. $$ How do I move from here?","['hilbert-spaces', 'normed-spaces', 'orthonormal', 'functional-analysis']"
4333561,On convolution theorem and Fourier transform,"Wikipedia says: On locally compact abelian groups, a version of the convolution theorem holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The circle group $\mathrm S$ with the Lebesgue measure is an immediate example. For a fixed $g$ in $L^1({\mathrm{S}})$ , we have the following familiar operator acting on the Hilbert space $L^2(\mathrm{S})$ : $$ T{f}(x)=\frac {1}{2\pi }\int _{\mathrm {S} }{f}(y)g(x-y)\,dy $$ The operator $T$ is compact. Is there a way to prove this? Caveat . Sadly, I know nothing about group theory, but intuitively I suppose $L^2({\mathrm S})$ means if I have a function, say in $L^2([0, 2\pi])$ , the previous convolution makes sense only if I assume the function repeats itself periodically outside $[0, 2\pi]$ .","['operator-theory', 'convolution', 'fourier-transform', 'hilbert-spaces', 'functional-analysis']"
4333564,Complex functions whose derivative is itself,"If $f$ is a real function whose derivative is itself, then it is not difficult to see that it is of the form $f(x)=ce^{x}$ , $c \in \mathbb{R}$ .
(If $c=0$ , then $f=0$ ). Indeed, assume that $g(x)$ satisfies $g'(x)=g(x)$ .
Take $h(x):=g(x)e^{-x}$ .
Then by the Chain Rule we have: $h'(x)=g'(x)e^{-x}-g(x)e^{-x}$ .
Since $g(x)=g'(x)$ we obtain that $h'(x)=0$ .
Then $h(x)=c$ , for some $c \in \mathbb{R}$ .
Therefore, $c=h(x)=g(x)e^{-x}$ , so $g(x)=ce^{x}$ . Does the same proof (with adjustments) hold for complex functions, namely,
if $g'(z)=g(z)$ , then $g(z)=ce^{iz}$ , $c \in \mathbb{C}$ ? See this question. Thank you very much!","['complex-analysis', 'derivatives', 'ordinary-differential-equations']"
4333583,Does the sum of random variables sampled with/without substitution differ for large populations?,"We have a population of $N$ different balls. Half the balls are red, and half the balls are blue. We perform $N$ trials. In trials $i = 1,\cdots,N$ we pick a ball $B_i$ randomly. First, we pick the balls with replacement.  Then we pick the balls without replacement. Define the sum $S = \displaystyle\sum_{i=2}^N \sigma_i$ where for each trial $i \in (2,3,...N)$ $$\sigma_i = \ \begin{cases} 
      1 & \text{if } B_i\text{ has same color as } B_{i-1} \\
      0 & \text{otherwise} 
   \end{cases}
$$ If the balls are picked with replacement the probability mass function (pmf) for $\sigma_i$ is $g(\sigma_i) =  \begin{cases} 
      0.5 & \text{for } \sigma_i=1 \\
      0.5 & \text{for } \sigma_i=0 
   \end{cases} $ . Accordingly, the probability mass function for $S$ is described by the Binomial distribution $$
f(s)= \binom{N-1}{s}\times0.5^{s}\times0.5^{N-1-s}
$$ In the limit $N\rightarrow\infty$ , the binomial distribution becomes a Gaussian distribution. I have performed a few computer simulations where the balls are picked without replacement, and the pmf for $S$ converges to the same binomial/Gaussian distribution in the limit $N\rightarrow\infty$ . This convergence occurs even if $\sigma_i$ is no longer perfectly i.i.d (independent and identically distributed). Why does the sum $S$ converge to the same distribution whether the balls are picked with or without replacement? I suspect that I need a central limit theorem for weakly correlated variables. Note: I want proof, but all explanations are welcome. I suspect there exists a fundamental explanation obvious for somebody with more statistics experience than myself.","['statistics', 'central-limit-theorem', 'probability-distributions', 'law-of-large-numbers', 'probability']"
4333585,Turing's Symmetries,"In Alan Turing's classic paper on the Chemical Basis of Morphogenesis , the following bilateral and left-right symmetries are defined as follows: Might be an misunderstanding of English from my side, but I can't really visualize this non-parallel upright strokes and what makes it different from the bilateral symmetry. This is what I'm imagining Is that what he means? He goes further to define P- and F-symmetries, which is also a bit confusing with the 'coal scuttle' example: Any suggestions on where I could learn more about this? Are there more mathematically ""better"" ways of defining such types of symmetries?","['biology', 'symmetry', 'geometry']"
4333592,The Mapping Class Group of the Annulus,"I was reading ""A primer on Mapping Class Groups"" by B. Farb and D. Margalit and I'm stuck at a point in the proof of Mod $(A)\cong \mathbb{Z}$ where $A$ is the annulus. Proposition 2.4 $\operatorname{Mod}(A) \approx \mathbb{Z}$ . Proof. First we construct a $\operatorname{map} \rho: \operatorname{Mod}(A) \rightarrow \mathbb{Z}$ . Let $f \in \operatorname{Mod}(A)$ and let $\phi: A \rightarrow A$ be any homeomorphism representing $f$ . The universal cover of $A$ is the infinite strip $\widetilde{A} \approx \mathbb{R} \times[0,1]$ , and $\phi$ has a preferred lift $\widetilde{\phi}: \widetilde{A} \rightarrow \widetilde{A}$ fixing the origin. Let $\widetilde{\phi}_{1}: \mathbb{R} \rightarrow \mathbb{R}$ denote the restriction of $\widetilde{\phi}$ to $\mathbb{R} \times\{1\}$ , which is canonically identified with $\mathbb{R}$ . Since $\widetilde{\phi}_{1}$ is a lift to $\mathbb{R}$ of the identity map on one of the boundary components of $A$ , it is an integer translation. We define $\rho(f)$ to be $\widetilde{\phi}_{1}(0)$ . If we identify $\mathbb{Z}$ with the group of integer translations of $\mathbb{R}$ , then the map $\widetilde{\phi}_{1}$ itself is an element of $\mathbb{Z}$ , and we can write $\rho(f)=\widetilde{\phi}_{1} \in \mathbb{Z}$ . From this point of view, it is clear that $\rho$ is a homomorphism since compositions of maps of $A$ are sent to compositions of translations of $\mathbb{R}$ . We can give an equivalent definition of $\rho$ as follows. Let $\delta$ be an oriented simple proper arc that connects the two boundary components of $A$ . Given $f$ and $\phi$ as above, the concatenation $\phi(\delta) * \delta^{-1}$ is a loop based at $\delta(0)$ , and $\rho(f)$ equals $\left[\phi(\delta) * \delta^{-1}\right] \in \pi_{1}(A, \delta(0)) \approx \mathbb{Z}$ . Yet another equivalent way to define $\rho$ is to let $\widetilde{\delta}$ be the unique lift of $\delta$ to $\widetilde{A}$ based at the origin and to set $\rho(f)$ to be the endpoint of $\widetilde{\phi}(\widetilde{\delta})$ in $\mathbb{R} \times\{1\} \approx \mathbb{R}$ . We now show that $\rho$ is surjective. The linear transformation of $\mathbb{R}^{2}$ given by the matrix $$
M=\left(\begin{array}{ll}
1 & n \\
0 & 1
\end{array}\right)
$$ preserves $\mathbb{R} \times[0,1]$ and is equivariant with respect to the group of deck transformations. Thus the restriction of the linear map $M$ to $\mathbb{R} \times[0,1]$ descends to a homeomorphism $\phi$ of $A$ . The action of this homeomorphism on $\delta$ is depicted in Figure $2.5$ for the case $n=-1$ . It follows from the definition of $\rho$ that $\rho([\phi])=n$ . why $\rho$ is surjective ? why $\phi$ is homeomorphism ?  what is exactly homeomorphism $\phi$ of $A$ such that $\rho([\phi])=n$ ? why $\rho([\phi])=n$ ? also what is definition of preferred lift ? i think we have $T_{M}:\mathbb R\times [0,1]\to \mathbb R\times [0,1]$ (or  linear transformation of $\mathbb{R}^{2}$ given by the matrix $M$ ) is the covering map. If $x\in A$ , you can pick a $y\in T_{M}^{-1}(x)$ . Thanks to the equivariance property, $\phi(x) = T_{M}(My)=M^2y$ does not depend on the choice of $y$ . Local triviality gives you the continuity of $\phi$ . how we can show $\rho([\phi])=n$ ?","['mapping-class-group', 'general-topology', 'covering-spaces', 'group-theory']"
4333604,Lebesgue Integral Over Step Function,"Given the step function \begin{equation}
  g(x,y) =
    \begin{cases}
      \frac{1}{x^2} & 0<y<x<1\\
      -\frac{1}{y^2} & 0<x<y<1\\
      0 & \text{otherwise}
    \end{cases}       
\end{equation} I want to show that $$\int_{(0,1)}\int_{(0,1)} g(x,y) \, d\lambda(x)\,d\lambda(y) \neq \int_{(0,1)}\int_{(0,1)} g(x,y) \,d\lambda(y)\,d\lambda(x)$$ However, when trying to use the relation with Riemann integrability I end up with divergent integrals such as $$\int_0^x \int_0^1 \frac{1}{x^2} \, dx \, dy - \int_0^1 \int_0^y \frac{1}{y^2} \, dx \, dy$$ I also considered using limits such as following one, but again I end up with divergence. $$\lim_{k \rightarrow\infty} \int_{(\frac{1}{k},1)}\lim_{k \rightarrow\infty} \int_{(\frac{1}{k},1)}g(x,y) \, dx \, dy$$ I would really appreciate any help!","['measure-theory', 'lebesgue-measure', 'riemann-integration', 'measurable-functions']"
4333622,convergence of a series with logarithm,"I've been dealing with the convergence of the following series for a while: $$\sum_{n=1}^\infty\ln\sqrt[n]{1+\frac{x}{n}}$$ .
I've tried the Cauchy test and found it reduces to study the convergence of $$\ln\left({1+\frac{x}{2^n}}\right)$$ But now I don't know how to study this one, hope someone may help. Thanks
P.S According to the book it should converge for $x>-1$","['limits', 'calculus', 'sequences-and-series']"
4333709,In-center of a triangle,"In geometry the in-center is the point of intersection/concurrence of angle bisectors; If I have a triangle with three vertices $$A=(x_a,y_a), B=(x_b,y_b), C=(x_c,y_c),$$ known to be related by $$\left( \frac{ax_a + bx_b + cx_c}{a+b+c};\frac{ay_a + by_b + cy_c}{a+b+c} \right) \tag 1$$ how can I find/derive the above formula $(1)$ using the determinant of a matrix or by any other means?","['analytic-geometry', 'euclidean-geometry', 'trigonometry', 'determinant']"
4333724,Trapping Region for ODE System.,"I am working on the following problem, given the system of two differential equations $x′=2x+y−2x^3−3xy^2,$ $y′=−2x+4y−4y^3−2x^2y,$ So far, I have tackled similar problems by trying to find a trapping region that does not contain an equilibrium point in its interior. (Applying Poincare-Bendixson Theorem). Clearly $(0,0)$ is an equilibrium point. (It will need to be shown it is the only equilibrium point). Now using a Lyapunov function, I am trying to find a trapping region.","['limit-cycles', 'lyapunov-functions', 'ordinary-differential-equations', 'dynamical-systems']"
4333754,Joint distribution of diagonal of Wishart matrix,"If $S\sim \mathcal{W}_p(\Sigma, n)$ , what is the distribution of the diagonal part of $S$ , $\text{diag}(S)$ ? Principal submatrices of Wishart distributed matrices are also Wishart distributed, implying that each diagonal element is marginally gamma distributed. But what is the joint distribution of these diagonal terms? A refresher on the Wishart distribution: Let $X_i \sim \mathcal{N}_p(0, \Sigma)$ be a vector independently drawn from p-variate normal distribution with mean $0$ and covariance $\Sigma$ for $i\in\{1,\ldots,p\}$ .
Then $$
S \equiv \sum_{i=1}^{n}X_iX_i^T \sim \mathcal{W}_p(\Sigma, n)\,.
$$ I have been able to determine the joint moment generating function (MFG) of diag( $\Sigma$ ), and I will include the derivation here. I am a bit stuck at this point however, so feel free to skip to the bottom or ignore this work entirely if you think there is a better approach . The meat of my work so far: Let's denote the diagonal elements of $S$ as $\sigma \equiv \text{diag}(\Sigma)$ , with $\sigma_i = S_{ii}$ . Then we have that $$
\sigma_i = \left(\sum_{j=1}^{n}X_jX_j^T\right)_{ii} = \sum_{j=1}^{n}(X_j)_i^2\,.
$$ In vector notation, we have $$
\sigma = \sum_{i=1}^{n} X_i\odot X_i\,,
$$ where $\odot$ is the Hadamard (elementwise) product. Let's consider the MGF of $\sigma$ : $$
\phi_\sigma^n(t) \equiv \left\langle e^{t^T\sigma} \right\rangle_{p(\sigma)}
= \left\langle e^{t^T\left(\sum_{i=1}^nX_i\odot X_i\right)} \right\rangle_{p(X_1, \ldots, X_n)}
= \left\langle e^{t^T\left(X\odot X\right)} \right\rangle_{p(X)}^n\,,
$$ where the first equality is given by the law of the unconscious statistician and the second is given by the independence of $X_i$ and $X_j$ for $i \neq j$ . Considering just $$
\phi(t) \equiv \phi_\sigma^1(t) = \left\langle e^{t^T\left(X\odot X\right)} \right\rangle_{p(X)}\,,
$$ we have $$
\begin{align}
\phi(t) &= \int_{-\infty}^{\infty} dX \ p(X) e^{t^T\left(X\odot X\right)}
\\
&= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T\Sigma^{-1}X\right) \exp\left(t^T\left(X\odot X\right)\right)\,.
\end{align}
$$ If we let $D \equiv \begin{bmatrix}t_1 & &0\\ &\ddots& \\ 0&&t_p \end{bmatrix}$ , then we can write $t^T\left(X\odot X\right) = X^T D X$ : $$
\begin{align}
\phi(t) &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T\Sigma^{-1}X\right) \exp\left(X^T D X\right)
\\
&= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T(\Sigma^{-1} - 2D)X\right)\,.
\end{align}
$$ If $\Sigma$ is positive definite, we have a finite radius $\epsilon > 0$ such that $\Sigma^{-1} - 2D$ is positive definite for $|t_i| < \epsilon \ \forall i$ , and we can find a full-rank $p\times p$ matrix $B$ such that $B^TB = \Sigma^{-1} - 2D$ in this radius. Let's change variables, letting $Y \equiv BX$ . Including the Jacobian, we have $$
\begin{align}
\phi(t) &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T(\Sigma^{-1} - 2D)X\right) 
\\
&= |\Sigma|^{-1/2} |B|^{-1} \int_{-\infty}^{\infty} dY \ \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} Y^TY\right)
\\
&= |\Sigma|^{-1/2} |B|^{-1}
\\
&= |\Sigma|^{-1/2} |\Sigma^{-1} - 2D|^{-1/2}
\\
&= |I_p - 2\Sigma D|^{-1/2}\,.
\end{align}
$$ Finally, we have ( TLDR ): $$
\phi_\sigma^n(t) = |I_p - 2\Sigma D|^{-\frac{n}{2}}\,.
$$ This feels like a neat little result in of itself. I am fairly confident that this is correct, since it reduces to the MGF of the gamma distribution for $p=1$ , and it nicely reduces to the product of several gamma MFGs if $\Sigma$ is diagonal. However, I don't really know where to go from here. I can't really compute the inverse Laplace transform by inspection here, and I quickly get very lost when trying to do it explicitly. Is there a known result anybody can point me to? Is there a nice method of retrieving a joint density for $\sigma$ from this expression? Thank you very much for the help!","['probability-distributions', 'moment-generating-functions', 'random-matrices', 'probability-theory', 'probability']"
4333783,Does there exists any non trivial linear metric space in which every open ball is not convex?,"$\Bbb{R^\omega}=\{(x_n)_{n\in \mathbb{N}}: x_n \in \Bbb{R}\}$ $d(x, y) =\sum_{j\in\mathbb{N}}{(a_j)} \frac{|x_j -y_j|}{1+|x_j -y_j|}$ Then $(\Bbb{R^\omega}, d) $ is a metric space. I know in a normed space any ball is convex. And it is easy to prove. The space $(\Bbb{R^\omega}, d) $ is not a normed space, I mean no norm on $\Bbb{R^\omega} $ can induce the metric $d$ . So, I guess in that space, It may be possible to find an open ball which is not convex. My question :1) Can I pick any open ball to test whether it is convex or not? If no, then is there any linear metric space in which every open ball is not convex? Can we get an example of a linear metric space (not a normed space) in which every open ball is convex? For the last question can I take $(X, \|•\|)$ be any  normed space and then define a metric $d(x, y) =\sqrt\|x-y\|$ . I think it works. Isn't it? Here $d$ is not scaling equivalent, hence not induced by any norm. $B_{d}(x_0, r) =\{x\in X : \|x-x_0\|<r^2 \}=B_{\|•\|} (x_0, r^2) $ Hence, every open ball is convex.","['convex-analysis', 'functional-analysis', 'metric-spaces']"
4333790,Why is $\cos(\alpha+\beta) = \cos(\alpha)\cos(\beta)−\sin(\alpha)\sin(\beta)$?,I already posted a question about transformation matrices and rotation . But I'm not satisfied with the answer. They simply said Composition of functions corresponds to multiplication of matrices. I think I understand the concept but I'm still confused why exactly $$\cos(\alpha + \beta)=\cos(\alpha)\cos(\beta)−\sin(\alpha)\sin(\beta)$$ Is there a lemma or formula I have to use or does it simply derive from distributivity of matrix multiplication? I can't get my head around it.,"['matrices', 'linear-algebra', 'transformation', 'rotations']"
4333828,Algorithm to determine if a 3D ellipsoid is contained within another?,Can anyone point me to an algorithm for how to efficiently check if a 3D ellipsoid is contained within another one? We can assume their origins are collocated. I am dealing with covariance ellipsoids constructed from matrices.,"['optimization', 'conic-sections', 'geometry']"
4333866,Incircle Right Triangle Proof,"An incircle in the right triangle 𝐴𝐵𝐶. The common point 𝑇 of a circle and a hypotenuse divides the hypotenuse into two lines - the line 𝐴𝑇 and 𝑇𝐵. Prove that the product of the sizes of these lines is equal to the area of this triangle. So far i used formula for incircle radius (r=(a+b-c)/2) to get the radius. next I created a square inside the circle with side = r. Then subtracted side of that square from side ""a"" and ""b"". translated these to sides into side ""c"" which now equals c = (a-r)+(b-r). I used this to get Area of the triangle S = (a-r)(b-r). And that's where i got stuck. I think i need to further transform this where the proof is clear that S = (a-r)(b-r) here is another picture for visualization of what I've done so far thanks for any answers.","['triangles', 'circles', 'geometry']"
4333896,"What does ""almost everywhere invertible"" mean?","I'm reading the paper On differentiability in the Wasserstein space and well-posedness for Hamilton–Jacobi equations in which there is a paragraph There is a special non-commutative group related to the isometry $\sharp: \mathbb{H} / \sharp \rightarrow \mathcal{P}_{2}\left(\mathbb{R}^{d}\right)$ , namely the set $\mathcal{G}(\Omega)$ of Borel maps $S: \Omega \rightarrow \Omega$ (they lie in $\mathbb{H}$ ) that are almost everywhere invertible and have the same law as the identity map $\operatorname{id}$ . Here $\Omega$ is the ball of unit volume in $\mathbb{R}^{d}$ , centered at the origin. $\mathbb{H}:=L^{2}\left(\Omega, \mathrm d x, \mathbb{R}^{d}\right)$ . My naïve guess is that ""almost everywhere invertible"" means the Lebesgue measure of $\{\omega \in \Omega \mid \operatorname{card}(S^{-1}(\omega)) \le 1\}$ is $1$ . Could you elaborate on this notion?","['measure-theory', 'definition', 'inverse-function']"
4333934,Understanding dual of $\ell^{\infty}$,"Denote by $\ell^{\infty}$ the Banach space of bounded sequences (real or complex). By an elementary argument, the dual space of $\ell^{\infty}$ consists of all bounded finitely additive signed measures on $\mathcal{P}(\mathbb{N})$ ; see here or here . On the other hand, since $\ell^{\infty}$ is the same as $C(\beta\mathbb{N})$ where $\beta\mathbb{N}$ is the space of ultrafilters (or the Stone-Cech compactification of $\mathbb{N}$ ), by Riesz–Markov–Kakutani representation theorem the dual of $\ell^{\infty}=C(\beta\mathbb{N})$ consists of regular Borel measure on $\beta\mathbb{N}$ . My questions are: This means a finitely additive measure on $\mathbb{N}$ corresponds to a countably additive measure on $\beta\mathbb{N}$ . How can that be? How to understand this? Is there a direct discription of this correspondence? It seems the space of finitely additive measures is given the norm of total variation. Can we view it as a subspace of $B(\mathcal{P}(\mathbb{N}),\mathbb{R})$ (all bounded maps from $\mathcal{P}(\mathbb{N})$ to $\mathbb{R}$ ) and give it the sup norm? The obvious (countably additive) measures on $\beta\mathbb{N}$ are the point measures (either at some natural number or at some ultrafilter) or their countable weighted sum (I guess these already contain the space $\ell^1$ ). What are some non-trivial examples? Can we ""classify"" these measures, whatever that means?","['general-topology', 'functional-analysis', 'measure-theory']"
4333962,$\mu \otimes \nu(A \times B)=\mu(A)\nu(B)$,"Let $(\Omega,\mathcal{A},\mu)$ and $(X,\mathcal{C},\nu)$ be to measure spaces with s-finite measures (so $\mu(\Omega)>0$ and $\mu(X)>0$ ). Show that $\mu \otimes \nu(A \times B)=\mu(A)\nu(B)$ for $A\times B \in \mathcal{A} \times \mathcal{C}$ and that if they are $\Sigma-$ finite measures then $\mu \otimes \nu$ is uniquely determined by its values on $A\times B \in \mathcal{A} \times \mathcal{C}$ I know that they are finite it is simply that the measure is $<\infty$ . Further, it is s-finite if $\mu=\sum_{n=1}^{\infty}\mu_n$ where each $\mu_n$ is a finite measure. For it to be $\Sigma$ -finite if the underlying set is a countable union of measurable sets with finite measure which is of course a weaker condition than finite. I do not know how to proceed to solve the question with the definitions. For the uniqueness I was thinking about Fubini's theorem.","['set-theory', 'measure-theory', 'probability-theory', 'probability']"
4334964,Is $1 / \sinh (4x)$ equal to $\operatorname{csch} (4x)$?,"I do know that $\operatorname{csch}(x) = \dfrac 1 { \sinh(x)}$ , but I'm not sure if it applies to $x$ only. I don't know if it's applicable for $4x$ as well, or any other monomial.","['trigonometry', 'hyperbolic-functions']"
4334993,"Example/meaning of filtration on a group $(\mathbb{R},+)$","Serre, in his Lie algebras and Lie groups, gives the definition of a filtration $\omega$ on a group $G$ as a map $\omega:G\to R\cup\{+\infty\}$ such that $\omega(e)=+\infty$ where $e$ is the identity element; $\omega(x)>0$ for any $x\in G$ ; $\omega(xy^{-1})\geq \inf\{\omega(x),\omega(y)\}$ for all $x,y\in G$ ; $\omega(x^{-1}y^{-1}xy)\geq \omega(x)+\omega(y)$ for all $x,y\in G$ . Then I tried to do some examples on groups which are pretty simple. I started with $(\mathbb{R},+)$ , but, unfortunately, I was not able to come up with any non-trivial filtrations. I tried $\omega(x)=\frac{1}{|x|}$ , $\omega(x)=e^{\frac{1}{x}}$ , and $\omega(x)=|x|$ . So, are there any good sources to read about this? For example: How many different distinct filtration do we have for a given group $G$ ? What is geometric (or any) intuition for filtrations on $G$ ?","['group-theory', 'lie-algebras', 'filtrations']"
4335019,Beginner feedback on real analysis proof,"I am a bio student self-studying Abbott's Understanding Analysis and would love some feedback on one of my answers to an exercise. I have no experience writing proofs, and I'm used to plug-n-chug math taught by school, but I'm determined to get through this book as I find it fascinating. Thanks! Q: If $x ∈ (A ∩ B)^c$ , explain why $x ∈ A^c ∪ B^c$ . This shows that $(A ∩ B)^c ⊆ A^c ∪ B^c$ Pf: If $x \in A\cap B$ , then $x \in A,B$ and $x \in A\cup B$ . We can think of $A\cap B$ as the collection of elements in both $A$ and $B$ . The complement $(A\cap B)^c$ is therefore the set of elements not in $A$ and $B$ , elements can still originate from $A$ or $B$ , just not those in both. The set $(A\cap B)^c$ is equal to $A^c\cup B^c$ because $A^c$ is the set of elements not in A (but, again, can contain elements in $B$ ). But, if an element is in A and also in B, neither $A^c$ nor $B^c$ will contain that elements. Thus, $A^c\cup B^c$ can be thought of as the set of elements not in $A$ and $B$ , just as with $(A ∩ B)^c$ .","['elementary-set-theory', 'advice', 'real-analysis']"
4335022,Open and dense set in metric space,"My professor asked me the next question: If $A$ is an open subset of a metric space $X$ and $D=A\cup(A^c)^\circ$ , then $D$ is dense. Can you check my proof? I never used that $A$ is open. Lemma 1. If $U,V$ are subsets of a metric space, then $\overline{U\cup V}=\overline{U}\cup\overline{V}$ . Lemma 2. If $U$ is a subset of a metric space, then $$\overline{U^c}=(U^\circ)^c\mbox{ and }(U^c)^\circ=(\overline{U})^c$$ My proof (of the excercise): Since $D=A\cup (A^c)^\circ$ , by part 2 of Lemma 2 we have $D=A\cup(\overline{A})^c$ . Then, by Lemma 1, $\overline{D}=\overline{A}\cup\overline{(\overline{A})^c}$ . Now, from part 1 of Lemma 2, we have $D=\overline{A}\cup((\overline{A})^\circ)^c...(1)$ Now, since $(\overline{A})^\circ\subseteq\overline{A}$ , then $(\overline{A})^c\subseteq((\overline{A})^\circ)^c$ , so, taking union with $\overline{A}$ on both sides we have $X=\overline{A}\cup(\overline{A})^c\subseteq\overline{A}\cup((\overline{A})^\circ)^c$ . But the right hand side of the last inclusion is, by (1), $\overline{D}$ , so $X\subseteq\overline{D}$ and done. Do you think that my proof is ok?","['general-topology', 'metric-spaces']"
4335067,prove $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$ with algebra of sets,"A friend of mine had asked me to prove this statement with algebra of sets. So at first I provided him with this answer: Proof of $A\cap(B\cup C) = (A\cap B)\cup(A\cap C)$ but the problem is that this is not the kind of solution he wants,he wants this to be proved  using algebra of set but as I read in this link https://en.wikipedia.org/wiki/Algebra_of_sets this statement is a fundamental properties of set algebra and I'm not sure if we can prove this using set algebra. thanks in advance","['elementary-set-theory', 'proof-writing', 'discrete-mathematics']"
4335086,Is my work transforming $9(x+2)^2$ to $(3x+6)^2$ correct? What is this method called?,$$9(x+2)^2 = 3^2(x+2)^2=(3(x+2))^2=(3x+6)^2$$ I want to know if the use of brackets in this problem has been done correctly. What is this method called?,['algebra-precalculus']
4335136,Examine the uniform convergence of $\sum_{n=1}^{\infty} \frac{x}{(1+nx)(1+(n+1)x)}$,"The problem is stated as: Examine the uniform convergence of $\sum_{n=1}^{\infty} \frac{x}{(1+nx)(1+(n+1)x)}$ on the interval $[0,\infty)$ and $[a,\infty)$ for $a>0$ . My attempt: Case I: $[0,\infty)$ We form our partial sum $S_m(x) := \sum_{n=1}^{m} \frac{x}{(1+nx)(1+(n+1)x)}$ , hence we have that: $$ |S(x)-S_m(x)| = \sum_{n=m+1}^{\infty} \frac{x}{(1+nx)(1+(n+1)x)} $$ Picking $x_n = 1/n \in [0,1] \subset [0,\infty) \forall n \geq 1 $ , we have that the series can be rewritten as: $$ | S(x)-S_m(x)| = \sum_{n=m+1}^{\infty} \frac{1}{(2n)(2+1/n)} \geq \sum_{n=m+1}^{\infty} \frac{1}{4n+2}$$ which clearly diverges. Therefore, we have found a counter example of uniform convergence within this interval, since if we would have uniform convergence, no matter what x we pick, the series must converge uniformly. Case II: $[a,\infty)$ Notice that $$S(x) = \sum_{n=1}^{\infty} \frac{x}{(1+nx)(1+(n+1)x)} = \sum_{n=1}^{\infty} \left (\frac{1}{(1+2n)(1+nx)} - \frac{1}{(1+2n)(1+(n+1)x)} \right )$$ We can form our Majorant term by calculating the upper bound of the term in the series above and we have that: $$\left |\frac{1}{(1+2n)(1+nx)} - \frac{1}{(1+2n)(1+(n+1)x)} \right | \leq \left (\frac{1}{(1+2n)(1+na)} + \frac{1}{(1+2n)(1+(n+1)a)} \right )$$ Let $M_n := \left (\frac{1}{(1+2n)(1+na)} + \frac{1}{(1+2n)(1+(n+1)a)} \right )$ , and since both terms act asymptotically as $1/n^2$ , it becomes evident that the series of terms given by $M_n$ does converge. By Weierstrass M - test, we have uniform convergence within this interval. I hope you can give me some feedback on my solution. Maybe some tips on what to improve, and what steps went wrong. I'm really trying to get good at problems with uniform convergence, so any help would be appreciated! Thanks.","['sequence-of-function', 'sequences-and-series', 'uniform-convergence', 'real-analysis']"
4335192,Solving a nonhomogenous system of eqns with one eigenvalue,"I have the system: $\left[\begin{array}{@{}c@{}}
    x' \\
           y'
    \end{array} \right]= \left[\begin{array}{@{}c@{}}
    3&2 \\
    -2 & -1 
    \end{array} \right]\left[\begin{array}{@{}c@{}}
    x' \\
           y'
    \end{array} \right]+\left[\begin{array}{@{}c@{}}
    2e^{-t} \\
          e^{-t}
    \end{array} \right]$ Which I should solve using the fundamental matrix. So I start with obtaining the homogenous solution: I find the eigenvalues; \begin{pmatrix}
    3-\lambda&2 \\
    -2 & -1-\lambda 
\end{pmatrix} which gives the determinant: $\lambda^2-2\lambda+1=0$ . Thus $\lambda_1=1$ . Plugging that in the matrix in the original equation, I get that x=y. So a solution to the homogenous system would be: $y_h=e^{t}\left[\begin{array}{@{}c@{}}
    1 \\
          1
    \end{array} \right]$ Since there is no second solution to the determinant, I would ideally form the fundamental matrix: \begin{pmatrix}
 e^{t} & e^0 \\
    e^{t} & e^0 
\end{pmatrix} but this is to no avail. So how do I find the solution of this nonhomogenous system using the fundamental matrix with one eigenvalue? Thanks UPDATE: I set up the generalized eigenvector formula \begin{equation}
v_2(A-\lambda I)=v_2
\begin{pmatrix}
     3-\lambda&2 \\
    -2 & -1-\lambda 
\end{pmatrix}=v_1
\end{equation} \begin{equation}
v_2(A-\lambda I)=v_1=
\begin{vmatrix}
     3-\lambda&2 & | 1 \\
    -2 & -1-\lambda & |-1  
\end{vmatrix}
\end{equation} I now get as given by Moo, with Gaussian elimination, the matrix: \begin{equation}
\begin{vmatrix}
     1 &1 & | 1/2 \\
    0 & 0 & |0  
\end{vmatrix}
\end{equation} and have the second eigenvector: $e_2=e^{t}\left[\begin{array}{@{}c@{}}
    \frac{1}{2} \\
          0
    \end{array} \right]$ . So the homogeneous solution is: \begin{equation}
y_h=e^{\lambda_1 t}e_1+e^{\lambda_2t}e_2=e^{t}\left[\begin{array}{@{}c@{}}
    1 \\
          -1
    \end{array} \right]+e^{t}\left[\begin{array}{@{}c@{}}
    \frac{1}{2} \\
          0
    \end{array} \right]
\end{equation} At this stage, it remains to find the particular solution. We know that it must be in the form of: \begin{equation}
y_p=Ce^{-t}
\end{equation} and thus the general solution is: \begin{equation}
y_p=y_h+Ce^{-t}=e^{t}\left[\begin{array}{@{}c@{}}
    1 \\
          -1
    \end{array} \right]+e^{t}\left[\begin{array}{@{}c@{}}
    \frac{1}{2} \\
          0
    \end{array} \right]+Ce^{-t}
\end{equation} But can this be said?","['fundamental-solution', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4335210,Motivation of Stone–Čech compactification,"What are some uses of Stone–Čech compactification? What is the motivation for introducing this notion? Most textbooks on topology construct Stone–Čech compactification and prove that this construction satisfies a particular universal property (which essentially sais that it is the left adjoint to the inclusion functor from the category of compact Hausdorff to the category of topological spaces). But this seems to be quite isolated from the rest of (algebraic) topology. For instance, Munkres' book on topology has a short section on the Stone–Čech compactification, but after that section he doesn't use the Stone–Čech compactification anywhere else in the book, as far as I can see. Wikipedia gives one application of the Stone–Čech compactification: it can be used to construct the dual space of the bounded sequences of reals. I wonder: That's all??! Probably not! What's the real motivation and use of Stone–Čech compactification?","['general-topology', 'algebraic-topology']"
4335330,Exercise with derivation of rings,"Let $f:A\to B$ be an injective homomorphism of integral domains, where $A$ is integrally closed. Assume also that $ 
B$ and its integral closure, $\bar B$ , are finitely generated $A$ -modules. Show that if $\Omega_{B/A}=0$ , then $B=\bar B$ (the $A$ -module $\Omega_{B/A}$ is the universal derivation). I posted the whole text to give a context, but I'm actually interested in knowing how can I use that the universal derivation is zero in the case that $B$ is not an algebra over a field. I can't use Zariski tangent space and the related notions, but I don't have any ideas. Thanks for any hint","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4335350,Levi-Civita connection on parallel transported vectors,"Suppose $(M,g)$ is a Riemannian manifold, $\nabla$ the Levi-Civita connection and $\{e_i\}$ a basis of unit vector in in the tangent space $T_p M$ at $p\in M$ . Take now the geodesic $\gamma(t)$ verifing $\gamma(0)=p$ and $\dot{\gamma}(0)=e_0$ , and take a variation $\Gamma(s,t)$ of $\gamma(t)$ through geodesics. We define a frame on the variation in the following way: parallel transport $\{e_i\}$ along the transverse curve $\Gamma(s,0)$ , and then, for each $s$ , parallel transport along $\Gamma(s,t)$ . Let $\{e_i^{(s,t)}\}$ denote this frame on the point $\Gamma(s,t)$ . Now, I want to compute the covariant derivative of, for example, $\nabla_{e_1^{(0,t)}} e_1^{(0,t)}$ . Is it possible to relate this with $\nabla_{e_1^{(0,0)}} e_1^{(0,0)}$ ? What I tried: since the covarint derivative determines the covariant differentitation, and thanks to the way we have constructed the frame, we can write: $$
\nabla_{e_1^{(0,t)}} e_1^{(0,t)}=\lim\limits_{s\rightarrow 0}\frac{P_{(s,t)}^{(0,t}P_{(s,0)}^{(s,t)}P_{(0,0)}^{(s,0)}P_{(0,t)}^{(0,0)}e_1^{(0,t)}-e_1^{(0,t)}}{s}.
$$ This expression make me think of the relation between the Riemannian curvature tensor and the Levi-Civita connection, but since the limit is not taken on $t$ , I am confused. Any ideas?","['connections', 'riemannian-geometry', 'differential-geometry']"
4335390,Convergence of steepest descent dynamics under weak convexity assumption,"Let $f\colon \mathbb R^d \to \mathbb R$ be smooth and assume that $f$ is a convex function (its Hessian is positive semi-definite); $f$ is bounded from below, in particular $\inf_{z \in \mathbb R^d} f(z) > - \infty$ . Let $x(t)$ denote the solution to the differential equations $$
\dot x(t) = - \nabla f\bigl(x(t)\bigr), \qquad x(0) = x_0.
$$ Question: Are there classical results on gradient descent stating that $$
\lim_{t\to\infty}f\bigl(x(t)\bigr) = \inf_{z \in \mathbb R^d} f(z) =: I. \qquad \qquad (1)
$$ for all initial conditions $x_0 \in \mathbb R^d$ ?
Note that $f\bigl(x(t)\bigr)$ may converge even if $x(t)$ does not converge,
for example in the case $f(x) = e^{-x}$ . Proof of convergence when $f$ is coercive ,
that is to say when $f(z) \to \infty$ in the limit as $|z| \to \infty$ .
It is sufficient to show that, for all $\varepsilon > 0$ ,
the dynamics $x(t)$ reaches the set $A_{\varepsilon} := \{z: f(z) \leq I + \varepsilon \}$ eventually.
These sets are convex, and also bounded by coercivity.
Any minimizing sequence has a convergent subsequence by compactness of $A_{\varepsilon}$ ,
and so there exists a (possibly non-unique) minimizer $z_*$ of $f$ .
We denote $$
R_{\varepsilon} = \sup_{z \notin A_{\varepsilon}} \|z - z_*\| > 0.
$$ Let us fix $\varepsilon > 0$ and show that the norm $\|\nabla f\|$ is bounded from below by a strictly positive constant $C_{\varepsilon}$ uniformly for $z \notin A_{\varepsilon}$ .
Fix $z \notin A_\varepsilon$ and let $y$ denote the intersection between the boundary of $A_{\varepsilon}$ and the segment linking $z_*$ and $z$ .
Since $f(y) = I + \varepsilon$ ,
we have by convexity $$
    \langle \nabla f(y), z_* - y \rangle \leq f(z_*) - f(y) = - \varepsilon,
$$ Since $f$ is convex, it also holds that $$
    \langle \nabla f(z) - \nabla f(y), z - y \rangle \geq 0,
$$ and so $$
    \|\nabla f(z) \| \|z - y\| \geq \langle \nabla f(y), z - y \rangle
    = \frac{\|z-y\|}{\|y-z_*\|} \langle \nabla f(y), y - z_* \rangle
    \geq \frac{\|z-y\|}{\|y-z_*\|} \varepsilon \geq \|z-y\| \frac{\varepsilon}{R_{\varepsilon}}.
$$ Canceling $\|z-y\|$ on both sides,
we obtain the desired lower bound.
The conclusion then easily follows from the fact that $$
\frac{d}{dt} \bigl(f(x(t))\bigr) = \bigl\langle \nabla f\bigl(x(t)\bigr),  \dot x(t) \bigr\rangle = - \| \nabla f\bigl(x(t)\bigr) \|^2.
$$ Proof that (1) holds if $d = 1$ .
If $f$ attains its infimum the proof is not difficult,
so we suppose that $f$ does not have a minimizer.
In this case either $f'(z) > 0$ for all $z \in \mathbb R^d$ or $f'(z) < 0$ for all $z \in \mathbb R^d$ .
Without loss of generality, assume the latter case holds.
Then clearly $I = \lim_{z \to \infty} f(z)$ and $x(t)$ is a strictly increasing function of $t$ .
It is impossible that $x(t) \to x_*$ for some $x_* < \infty$ ,
because $\dot x$ can be bounded from below uniformly in a neighborhood of $x_*$ .
Consequently, it must hold that $x(t) \to \infty$ and so $\lim_{t \to \infty} f(x(t)) = I$ . Therefore , the case that is interesting is when $f$ is non-coercive with $d > 1$ . Then either $f$ attains its infimum or not, and I am particularly interested in the latter situation: $f$ does not have a minimizer.","['convex-optimization', 'optimization', 'gradient-flows', 'ordinary-differential-equations']"
4335396,How to find a function that minimize the following expectation,"Assume $ X $ is a continuous random variable which have a density function.
Assume $ \underline{Y} $ is a random vector which also have a density function.
And finally assume we have a joint density function of the random variable and the random vector. How can I find a function $\phi$ , such that $ \mathbb{E}\left[\left|X-\phi\left(\underline{Y}\right)\right||\underline{Y}\right] $ would be minimal? I'm not sure where to start. I do know that the asnwer should be that $\phi(\underline{Y})$ should be a median of $F_{X|\underline{Y}}$ , but Im not sure how to prove it. Any help would be appreciated. Thanks in advance.","['measure-theory', 'probability-theory']"
4335414,Estimate the coefficients of a polynomial against its maximum,"For $d, m \in\mathbb{N}$ fixed, let $P\equiv P(x) := \sum_{|\alpha|\leq m} c_\alpha\cdot x^\alpha$ be a real polynomial in $d$ variables of (total) degree $m$ . (I.e., the above sum ranges over all multiindices $\alpha=(i_1, \ldots, i_d)\in\mathbb{N}_0^{\times d}$ of length $|\alpha|\equiv i_1+\ldots + i_d$ less than $m$ .) I was wondering if it is somehow possible to estimate the maximum coefficient $\|c_\alpha\|_\infty := \max_{|\alpha|\leq m}|c_\alpha|$ of $P$ against its uniform norm $\|P\|_{\infty;K}:= \sup_{x\in K}|P(x)|$ , for $K$ some compact set in $\mathbb{R}^d$ (ideally something like the closed unit ball, but whatever compact set works). That is, does there exist a constant $\kappa \equiv \kappa(m,d,K)>0$ such that $$\tag{1}\|c_\alpha\|_\infty \ \leq \ \kappa\cdot \|P\|_{\infty; K} \qquad \text{ for each } \ P \ \text{ as above}?$$ Any references are welcome.","['real-analysis', 'complex-analysis', 'combinatorics', 'functional-analysis', 'polynomials']"
4335433,Lie algebra of the group of biholomorphisms,"Consider a complex manifold $X$ and $Aut(X)$ the Lie group of biholomorphisms of $X$ . I want to calculate the Lie algebra of this Lie groups so as to derive a result analogue in the holomorphic realm to that for real smooth manifolds, namely $$
\text{Lie}(\text{Diff}(M)) \simeq \mathfrak{X}(M)
$$ i.e. that infinitesimal diffeomorphisms are given by smooth vector fields. In the real case, one would take a 1-parameter family of diffeomorphisms $f_t$ close to the identity and argue that at each point, taking the derivative gives a tangent vector $\gamma'_p(0) = \frac{d}{dt}|_{t=0} f_t(p)$ , and after some calculations, deduce that this induces a smooth field of vectors over $M$ . For the holomorphic case, I suspect that the infinitesimal biholomorphisms of a complex manifold correspond to holomorphic vector fields , namely holomorphic sections of the complex vector bundle $T^{1,0}X \rightarrow X$ endowed with the natural holomorphic structure coming from $X$ . However, I don't know how to prove that the derivative $\gamma'_p(0)$ above is in a natural way a (1,0)-vector in $T\underline{X}_\mathbb{C}=T^{1,0}X \oplus T^{0,1}X$ (where $\underline{X}$ is the smooth manifold underlying $X$ ).","['complex-geometry', 'differential-geometry']"
4335440,Solution to $x'=x\sin(\frac{\pi}{x})$ is unique,"I want to prove that the only solution to the ODE $$x'=\begin{cases} x\sin(\frac{\pi}{x}) \text{ if } x \neq 0 \\ 0 \text{ else}\end{cases}$$ with $x(0) = 0$ is unique (so it is the constant solution $x=0$ ). Since $f(x) = x\sin(\frac{\pi}{x})$ is not locally Lipschitz at $0$ we cannot use any general result about uniqueness. I have tried to prove that if $x$ is a solution then $x^2$ is locally decreasing around $0$ (a trick that has worked other times ) but the oscillations of the term $\sin{\frac{\pi}{x}}$ make impossible reaching conclusions about sign. I have noticed that since $f$ is odd, any solution $x$ must be an even function, this is $x(-t) = x(t)$ . The equation does not seem to be solvable so more direct approach are out of hand I think. Can you give me any hint?","['ordinary-differential-equations', 'dynamical-systems']"
