question_id,title,body,tags
1999693,Why does Friedberg say that the role of the determinant is less central than in former times?,"I am taking a proof-based introductory course to Linear Algebra as an undergrad student of Mathematics and Computer Science. The author of my textbook (Friedberg's Linear Algebra , 4th Edition) says in the introduction to Chapter 4: The determinant, which has played a prominent role in the theory of linear algebra, is a special scalar-valued function defined on the set of square matrices. Although it still has a place in the study of linear algebra and its applications, its role is less central than in former times. He even sets up the chapter in such a way that you can skip going into detail and move on: For the reader who prefers to treat determinants lightly, Section 4.4 contains the essential properties that are needed in later chapters. Could anyone offer a didactic and simple explanation that refutes or asserts the author's statement?","['matrices', 'math-history', 'linear-algebra', 'determinant']"
1999705,Limit of the given sum: $f(x) = \lim_{n\to \infty} \sum_{r=1}^n 3^{r-1}\sin^3(x/(3^r))$,"$$f(x) = \lim_{n\to \infty}  \sum_{r=1}^n 3^{r-1}\sin^3(x/(3^r)) $$ 
I tried using the formula relating $\sin(3x)$ to $\sin^3(x)$ but got later stuck with a similar series who's sum I didn't know how to calculate","['sequences-and-series', 'limits']"
1999706,Find all $\alpha$ such that the series converges,Find all values of $\alpha$ such that series $$\sum^\infty_{n=1} \left( \frac{1}{n \cdot \sin(1/n)} - \cos\left(\frac{1}{n}\right) \right)^\alpha$$ converges. I used Maclaurin for $\sin$ and $\cos$ and got: $$a_n = \left( \frac{1}{1 - \dfrac{1}{3!n^2} + \ldots} - 1 + \frac{1}{2!n^2} - \frac{1}{4!n^4} + \ldots \right) ^ \alpha$$ Put it together in one fraction seems to be a hard thing to do.,"['trigonometry', 'sequences-and-series', 'convergence-divergence']"
1999777,Understanding torsors and semidirect products of groups,"I'm trying to understand the semi-direct product of groups from either a categorical or a geometric perspective and failing miserably. The four things I'm hoping will fit into a coherent picture are: Semidirect product as left adjoint ; Semidirect product in terms of torsors ; Semidirect product as a homotopy quotient ; Semidirect product in terms of subobjects of the product. I'll try to explain my struggles with each point. I started by trying to find a universal property and stumbled upon the first description, but I feel it did not help me even understand why semidirect products are interesting. Let $\mathsf{Grp^2}$ be the arrow category of the category of groups, and let $\mathsf{Grp}\circlearrowleft$ be the category of groups acting on groups: an object is a group action of a group $G$ on a group $H$ that respects all the operations, an arrow is an equivariant one. Such actions are in bijection with group homomorphisms $G\to \mathsf{Aut}(H)$. If these hold, we'll say $H$ is a $G$-group. The right adjoint is the forgetful functor $\mathsf{Grp^2}\to \mathsf{Grp}\circlearrowleft$ taking a group homomorphism to action by conjugation. The left adjoint, if I understand correctly, takes a $G$-group $\alpha:G\times H\to H$ to the inclusion $G\to H\rtimes G$. But why even care about $G$-groups? What is the semidirect product actually doing? Seeking intuition I hoped torsors might be related. Now, I don't know anything about torsors from the algebraic-geometry standpoint - for me a $G$-torsor is just a uniquely transitive $G$-set. John Baez's Torsors Made Easy and the linked answer made it clear torsors are somehow related to semidirect products, but I still don't get how, and still don't have a geometric picture of the semidirect product. What I understand of the answer is as follows: We start with an $H$-group $G$, and a $G$-torsor $S$. As a $G$-set, $S$ has $G$ as a group of autobijections. If we pick a unit in $S$ then we have a group isomorphism $G\cong S$ which makes $S$ into an $H$-group. This gives the set $S$ another group $H$ of autobijections. The semidirect product $G\rtimes H$ is the subgroup of $\mathsf{Sym}(S)$ generated by the subgroups $G,H\leq \mathsf{Sym}(S)$. But what is the geometry of this story? As the question itself mentions, the group $H$ seems to be keeping track of ""orientation"", while $G$ is doing the ""movement"". Does the semidirect product reflect the geometry of a torsor? I just don't get it. In Tao's Compactness and Contradiction I found a very useful page or two about torsors, with a brief discussion of the lamplighter group. The idea was to think of an element of $G$ as a ratio $\frac BA$ between elements in some convenient $G$-torsor of ""states"". This makes conjugation intuitive since $g\frac BA g^{-1}=\frac{gA}{gB}$ is the unique group element with initial state $gA$ and terminal state $gB$. I was very hopeful this intuitive approach might actually motivate the unpleasant multiplication of the semidirect product, but I haven't been able to make sense of that myself. Waiting until 1,2 make sense; Waiting until 1,2 make sense.","['intuition', 'abstract-algebra', 'category-theory', 'semidirect-product', 'group-theory']"
1999788,Weak convergence counter-example,"Given probability measures $\mu,\mu_n: \mathcal{B}(X)\to R$. Construct an example such that $$\langle f,\mu_n\rangle \rightarrow \langle f,\mu\rangle$$
for all convex function $f:X\to R$ but $\mu_n$ does not converge weakly to $\mu$. This question is from a problem in a Partially Observed Markov Decision Process. In there we can guarantee that starting from any initial prior $\pi$, for all cost function the ACOE has a bounded solution, which implies that $E[v(\pi_n)\mid\pi_0=\pi] \rightarrow C$. But the expected cost function $v(\pi)$ can only be a certain convex function of $\pi$ even if we allow all kind of cost function. So I wonder whether or not there's a counter-example saying that convex test function $f$ is not enough for weak convergence. The POMDP average cost problem is from this paper: http://epubs.siam.org/doi/abs/10.1137/0318028","['functional-analysis', 'probability-theory', 'weak-convergence']"
1999791,Inverse of a symmetric tridiagonal almost-Toeplitz matrix,"I'm trying to analytically find the inverse of the following $N \times N$ tridiagonal matrix: $$T = \begin{bmatrix}
1 & -c \\
-c & 2 & -c \\
& -c & 2 & \ddots \\
& & \ddots & \ddots & \ddots \\
& & & \ddots  & 2 & -c \\
& & & & -c & 2 & -c \\
& & & & & -c & 1
\end{bmatrix}$$ That is, all elements just off the main diagonal are $-c$, where $c \in (0, 1)$; the main diagonal has a one in the top-left and in the bottom-right, and twos in between; and everything else is a zero. This comes from a statistical model I'm using (a CAR spatial model for a path graph) where, by construction, $T^{-1}$ is a covariance matrix.  Thus I know $T^{-1}$ is symmetric and the elements of its main diagonal are all positive.  I'd happily settle for only getting formulas for those main diagonal elements. Copying from the Wikipedia page on tridiagonal matrices (and doing a little bit of work), the $i$th diagonal element of $T^{-1}$ is $(T^{-1})_{ii} = \theta_{i-1} \theta_{N-i} / \theta_N$, where $$\theta_i = a_i \theta_{i-1} - c^2 \theta_{i-2} \text{ for } i = 2,3,\dots,N$$ with initial conditions $\theta_0 = \theta_1 = 1$.  The $a_i$ are the main diagonal elements of $T$, so e.g. if $N = 5$ then $\{a_i\} = \{1,2,2,2,1\}$.  One way to answer my question would be to solve the above recurrence relation, i.e. find a closed-form for $\theta_i$ for any $N \ge 2$.  (I tried this using a generating function, but got stuck due to the pesky non-constant diagonal of $T$, i.e. the $a_i$.) In the following related question, the person who answered it used a very different, very involved approach: Inverse of a symmetric tridiagonal matrix. .  There, the matrix to be inverted had a constant main diagonal, so it was easier.","['recurrence-relations', 'tridiagonal-matrices', 'matrices', 'generating-functions', 'linear-algebra']"
1999796,Constructing a nearly 0 stochastic process or stopping times,"I'm trying to define a stochastic process $X_t$ with values on $\mathbb{R}$ which has the following properties, for the time interval $t \in [0,T]$: $X_t$ takes values in $[0,1]$ $m(\{t \in [0,T] : X_t>0\})\leq \epsilon$, for some prescribed $\epsilon\geq 0$, where $m$ is the Lebesgue measure on $\mathbb{R}$. $X_t$ is adapted to some other LÃ©vy process $Z_t$ which takes values in $\mathbb{R}^d$. Most importantly I am looking for the following Stopping condition to hold: Let $\tau_1,...$ and $r_1,...$ be the hitting times corresponding to when $X_t>0$ and when $X_t$ first becomes $0$ again.  I want $r_i -\tau_i$ to be small, always but stochastic. I am not sure how to rigorously construct such a process...  Could I just have a sequence of stopping times (this seems to make sense) but then how do I enforce the 2 and 4th conditions?","['stochastic-processes', 'probability-theory', 'stochastic-analysis', 'stopping-times']"
1999816,Dominated Convergence Theorem Exercise,"I am asked to find $$\lim_{n \to \infty} \int_0^\infty n^2e^{-nx} \tan^{-1} x \, dx.$$ Here is my attempt. Write $$\int_0^\infty n^2e^{-nx}\tan^{-1}x \, dx=\int_0^1 n^2e^{-nx} \tan^{-1} x \,dx + \int_1^\infty n^2e^{-nx}\tan^{-1} x \, dx$$
$$=\int_0^{n^2} e^{-\frac x n} \tan^{-1}\left(\frac x {n^2}\right) \, dx+\int_1^\infty n^2 e^{-nx} \tan^{-1}x \, dx.$$ Then note that $$\left| 1_{(0,n^2)}(x)e^{-x/n}\tan^{-1} \left(\frac x {n^2}\right) \right| \le \frac \pi 2$$ for all $x>0$ and all $n\ge 1$ and $$|n^2e^{-nx}\tan^{-1}x| \le \frac{\pi}{2}\frac 2 {x^2}$$ for all $x\in [1,\infty)$ and all $n\ge 1$. Thus the dominated convergence gives $${\lim_{n\to\infty} \int_0^\infty 1_{(0,n^2)}(x)e^{-x/n}\tan^{-1} \left(\frac x {n^2}\right) \, dx = 0}$$ and $$\lim_{n\to\infty} \int_1^\infty n^2e^{-nx} \tan^{-1}x\,dx=0,$$ and hence $$\lim_{n \to \infty}\int_0^\infty n^2e^{-nx}\tan^{-1}x\,dx=0.$$ Is this correct? EDIT: Unfortunately the above is not correct (see Dr. MV's comment). The correct justification is shown below (given by Sangchul Lee).
$$\int_0^\infty n^2e^{-nx} \tan^{-1} xdx=\int_0^\infty ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx.$$ Since $$|ne^{-x}\tan^{-1} (\frac{x}{n})|\le xe^{-x}$$ for all $x>0$ and all $n\ge1$ we deduce that  $$\lim_{n \to \infty} \int_0^\infty n^2e^{-nx} \tan^{-1} x \, dx=\lim_{n\to\infty}\int_0^\infty ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx=\int_0^\infty \lim_{n
\to\infty}ne^{-x} \tan^{-1} (\frac{x}{n}) \, dx=\int_0^\infty xe^{-x} \, dx=1.$$ The point is that $\tan^{-1}x\le x$ for all $x\ge0$, an inequality I had forgotten!","['real-analysis', 'measure-theory']"
1999819,"If $H_a $, $H_b$, $H_c$ are half turns, prove that $ H_aH_bH_c=H_cH_bH_a$","Prove: $H_aH_bH_c=H_cH_bH_a$ where $H_a, H_b$, and $H_c$ are half turns about a, b,and c. How do I show this? The only thing I know is that the product of two half turns is a translation.",['geometry']
1999823,Picard groups and fundamental groups of connected algebraic groups,"Recently, I'm reading V. L. Popov's paper ""Picard groups of homogeneous spaces of linear algebraic groups and one-dimensional homogeneous vector bundles"" V. L. Popov, 1974 , and I got confused about ""Theorem 6"" in that paper which says that ""Let $G$ be a connected linear algebraic group with radical $R$. Then $\mathrm{Pic}(G)$ is isomorphic to the fundamental group of the semisimple group $G/R$."" This theorem follows from ""Theorem 3"" and ""Theorem 4"" in that paper. However, this seems to contradict the following example. Consider $G = \mathrm{GL}(n,\mathbb{C})$, whose radical $R$ is isomorphic to $\mathbb{G}_{m}$. Then the homogeneous space is $G/R = \mathrm{PGL}(n,\mathbb{C})$. We know that $\mathrm{Pic}(\mathrm{GL}(n,\mathbb{C}))$ is $0$, but $\pi_{1}(\mathrm{PGL}(n,\mathbb{C}))=\mathbb{Z}/n\mathbb{Z}$. Can anyone explain what I've understood incorrectly?","['homogeneous-spaces', 'algebraic-groups', 'representation-theory', 'algebraic-geometry']"
1999837,"How to show that $\int [Y(x)\frac{dZ(x)}{dx} + Z(x)\frac{dY(x)}{dx}]\, dx = Y(x)Z(x) + C$?","How can I show that 
$$\int\left[Y(x)\frac{dZ(x)}{dx} + Z(x)\frac{dY(x)}{dx}\right]\, dx = Y(x)Z(x) + C$$ 
when it is known that 
$$\frac{d}{dt}[f(x)] = \frac{df}{dx}\cdot\frac{dx}{dt} \tag{1.}$$
and 
$$\frac{df(x)}{dx} = \left[\frac{dx(f)}{df}\right]^{-1} \tag{2.}$$ 
I have (of course) divided the two expressions like below to individually solve for the two integrals, but I can't continue from that (supposing that my first step is correct)
$$\int\left[Y(x)\frac{dZ(x)}{dx}\right]dx + \int\left[Z(x)\frac{dY(x)}{dx}\right] \, dx$$
This problem is the problem number 12. from the first Project PHYSNET module.","['integration', 'calculus', 'analysis']"
1999869,Evaluate $\int\frac1{1+x^n}dx$ for $n\in\mathbb R$,"I was wondering on how to evaluate the following indefinite integral for all $n\in\mathbb R$. $$\int\frac1{1+x^n}dx$$ It seems to be peculiar in that we have $$\begin{align}
\int\frac1{1+x^{-1}}dx&=x-\ln(x+1)+c\\
\int\frac1{1+x^0}dx&=\frac12x+c\\
\int\frac1{1+x^{1/2}}dx&=2\sqrt x-2\ln(1+\sqrt x)+c\\
\int\frac1{1+x^1}dx&=\ln(x+1)+c\\
\int\frac1{1+x^2}dx&=\arctan(x)+c\\
\int\frac1{1+x^3}dx&=\frac13\ln(1+x)-\frac2{3\sqrt3}\arctan\left(\sqrt{\frac43}\left(x-\frac12\right)\right)+c
\end{align}$$ Naturally, there appears to be some combination of $\ln$ and $\arctan$, though no simple formula arises to solve the general case. It is, however, easy to see that $$\int\frac1{1+x^{-n}}dx=\int1-\frac1{1+x^n}dx$$ So there is an easy enough connection between positive and negative $n$. Also, it is easy enough to find the series expansion, taking advantage of the above connection we just made to circumvent problems concerning convergence. $$\frac1{1+x^n}=1-x^n+x^{2n}-x^{3n}+\dots\forall\ |x|<1$$ $$\int\frac1{1+x^n}dx=c+x-\frac1{n+1}x^{n+1}+\frac1{2n+1}x^{2n+1}-\dots$$ $$=c+\sum_{k=0}^\infty\frac{(-1)^k}{kn+1}x^{kn+1}\ \forall\ |x|<1$$ Though this isn't very much along the lines of closed form. For $n=\frac ab$, where $a$ and $b$ are whole numbers, we can use the substitution $x=u^b$ to get $$\int\frac1{1+x^n}dx=\int\frac{bu^{b-1}}{1+u^a}du$$ though I'm unsure where that could lead. This reduces the integral down to $$\int\frac1{1+x^n}dx=b\int P(u)+\frac{u^{b-1-ak}}{1+u^a}du,\quad k\in\mathbb N$$ for some polynomial $P(u)$.  Though I'm still clueless as to how this can be advanced. How can I evaluate $\int\frac1{1+x^n}dx\ \forall\ n\in\mathbb R$ in closed form?  Can someone prove there at least exists some closed form solution for all $n\in\mathbb Q$ if the above is not possible?  If possible, use real numbers.","['indefinite-integrals', 'real-analysis', 'integration', 'sequences-and-series']"
1999880,Why is this statement 'There is a real number $x$ such that $x^2 < x$.' not true?,"Determine whether the statements are true or false. There is a real number $x$ such that $x^2 < x$. My obvious answer was the statement is true, take e.g. $x=0.5$ But the solution says otherwise: (Discrete Mathematics with Applications) This is strange, is the answer wrong because all I need to show is that there exists one real number for this given statement to be true. edit: added question screenshot: part b)","['logic', 'discrete-mathematics']"
1999881,Prove that $\sin (2 \arcsin x) = 2 x \sqrt{1 - x^2}$,"This is coming up in the computation of the integral of $\sqrt{1 - x^2}$. I have the result from Wolfram|Alpha, but I have no idea how to get there.",['trigonometry']
1999916,What is the position of the maximal value of this bell-shaped function?,"Consider the following function :
$$\tag{1}
f(v) = v^{\frac{d}{2}} \int_v^{\infty} u^{\alpha \,-\, \smash{\frac{d}{2}} \,-\, 1} \; e^{-\, \alpha \, u} \; du,
$$
where $d \le 6$ and $\alpha > 0$ are two positive constants (parameters).  Notice the lower limit of the integral : $v$ is a variable.  The plot of this function (for $0 \le v < \infty$) shows an almost bell-shaped curve, so it has a single maximal value. Now, I would like to find the position $v = v_0(\alpha, d)$ of the maximal value of this function.  I need an analytical expression for $v_0(\alpha, d)$, probably an approximation.
$$\tag{2}
v_0(\alpha, d) \approx \; ?
$$
From the graph of $f(v)$, I know that $v_0 \propto d$ (maybe with some exponent). Take note that the derivative of function (1), set to 0, give this relation :
$$\tag{3}
f(v_0) \equiv f_{\text{max}} = \frac{2}{d} \; v_0^{\alpha} \; e^{-\, \alpha \, v_0},
$$
where $v_0 \equiv v_0(\alpha, d)$ is the position of the max value of (1). Someone knows a method to find the function (2) ? EDIT : Function (1) describes the ""deformation"" of a black body luminosity caused by the expansion of space in a cosmology model.  From Wien's and Planck's laws, $\alpha$ should be around 3 (depending on the presence of gaz and dust).  $d$ describes the kind of fluid contained in the cosmological model.  We have $d = 3$ for a dust filled universe, and $d = 4$ for a radiation universe.  $d = 0$ for an empty universe with a cosmological constant.  Since $\frac{d}{2} + 1$ gives 2.5 for dust and 3 for radiation, I suspect that $\alpha$ should be close to $\frac{d}{2} + 1$ (while it is an independant parameter).  In this special case, it is easy to explicitely evaluate the integral in (1) and we get the special case
$$\tag{4}
v_0(d) = \frac{d}{d + 2}, \quad \text{if $\alpha = \frac{d}{2} + 1$}.
$$","['approximate-integration', 'functions', 'maxima-minima', 'integration', 'approximation']"
1999924,Solving $\sin z = i$,"I know that $$\sin z = \frac{e^{iz}-e^{-iz}}{2i}$$ so: $$\frac{e^{iz}-e^{-iz}}{2i} = i\implies e^{iz}-e^{-iz} = -2$$
but I can't take anything useful from here. How do I solve such equations? What about $\tan z = 1$? Is there any solutions?","['complex-analysis', 'calculus']"
2000019,Solving the differential equation $\frac{dy}{dx} = \frac{y^3}{2(xy^2-x^2)}$,"How can we solve the equation: $$\frac{dy}{dx} = \frac{y^3}{2(xy^2-x^2)}$$ I get the idea of dividing by $y^2$, But it doesn't become any more solvable (not homogenous).
$$\frac{dy}{dx} = \frac{y}{2(x-\frac{x^2}{y^2})}$$ Substituting $\frac{x}{y} = t$ causes even more complications. I get an idea of the question to convert into homogenous, but cant form the equation. Please give me a hint!","['derivatives', 'ordinary-differential-equations']"
2000075,Ring of Sets vs Ring in Universal Algebra,"I actually want to continue this post . I believe the naming convention in Mathematics is consistent, such that there are no clearly distinguish objects have the same name. However, I'm not sure how to relate the Ring of Sets and the Ring in Universal Algebra . The definition of Ring in Universal Algebra is $R$ is an abelian group under addition, meaning that: $(a + b) + c = a + (b + c)$ for all $a, b, c \in R$ ($+$ is associative). $a + b = b + a$ for all $a, b \in R$ ($+$ is commutative). There is an element $0 \in R$ such that $a + 0 = a$ for all $a \in R$ ($0$ is the additive identity). For each $a \in R$ there exists $âa \in R$ such that $a + (âa) = 0$ ($âa$ is the additive inverse of $a$). $R$ is a monoid under multiplication, meaning that: $(a Â· b) Â· c = a Â· (b Â· c)$ for all $a, b, c \in R$ ($Â·$ is associative). There is an element $1 \in R$ such that $a Â· 1 = a$ and $1 Â· a = a$ for all $a \in R$ ($1$ is the multiplicative identity). Multiplication is distributive with respect to addition: $a â (b + c) = (a Â· b) + (a Â· c)$ for all $a, b, c \in R$ (left distributivity). $(b + c) Â· a = (b Â· a) + (c Â· a)$ for all $a, b, c \in R$ (right distributivity). Next, the definition of Ring of Sets is $A,B\in\mathcal{R}$ implies $A\cap B\in \mathcal {R}$ and $A,B\in \mathcal {R}$ implies $A\cup B\in \mathcal {R}$. Then, I tried to relate between this two definition. Let assume that $\cap$ is the additive operator. Then, $\mathcal {R}$ must contain the additive identity and inverse for all $A \in \mathcal {R}$. The additive identity is the whole element set $S$, such that for all $A \in \mathcal {R}, A \subseteq S$. It can be easily seen that $A \cap S = A$, for all $A \in \mathcal {R}$. Nevertheless, pick $A \in \mathcal{R}$, such that $A \subset S$. Then, $A$ doesn't have the inverse, i.e. no $B \in \mathcal{R}$, such that $A \cap B = S$. Let assume that $\cup$ is the additive operator. The argument is similar to (1). I might be wrong, but what I'm trying to say is the structure of Ring of Sets isn't consistent with the structure of general Ring. Kindly need your explanation. Thank you.","['abstract-algebra', 'boolean-ring', 'universal-algebra', 'ring-theory', 'elementary-set-theory']"
2000110,"Is ""generalized"" singular homology/cohomology a thing? If not, why not?","From what I understand, the singular homology groups of a topological space are defined like so: Topological Particulars. There's a covariant functor $F : \mathbb{\Delta} \rightarrow \mathbf{Top}$ that assigns to each natural number $n$ the corresponding $n$-simplex. This yields a functor $$\mathbf{Top}(F-,-) : \Delta^{op} \times \mathbf{Top} \rightarrow \mathbf{Set}.$$ Hence to each topological space $X$, we can assign a simplicial set $\mathbf{Top}(F-,X) : \Delta^{op} \rightarrow \mathbf{Set}.$ General nonsense. We observe that every simplicial set induces a simplicial abelian group; that every simplicial abelian group induces a chain complex; and that chain complexes have homology and cohomology groups. Ergo, simplicial sets have homology/cohomology groups. Putting these together, we may speak of the homology and cohomology groups of a topological space $X$. However, the topological particulars don't seem too important. In fact, for any category $\mathbf{C}$ and any functor $F : \Delta \rightarrow \mathbf{C}$, there's a simplicial set $\mathbf{C}(F-,X)$ attached to each $X \in \mathbf{C}$, and therefore $X$ has homology and cohomology. For example, the underlying set functor $U : \mathbf{CMon} \rightarrow \mathbf{Set}$ has a left-adjoint $F : \mathbf{Set} \rightarrow \mathbf{CMon}$. But since $\Delta \subseteq \mathbf{Set}$ and $\mathbf{CMon} \subseteq \mathbf{Mon}$, this yields a functor $F : \Delta \rightarrow \mathbf{Mon}$. This should in turn allow us to attach homology and cohomology groups to each monoid $M$, by studying the simplicial set $\mathbf{Mon}(F-,M)$. Question. Is this a thing? If not, why not?","['abstract-algebra', 'homological-algebra', 'universal-algebra', 'soft-question', 'category-theory']"
2000114,If $|x+2|<2$ show that $|3x-2|<14$,If $|x+2|<2$ show that $|3x-2|<14$ solution so far: $|x+2|<2 \Leftrightarrow -2<x+2<2 \Leftrightarrow -4<x<0 \Leftrightarrow -12<3x<0 \Leftrightarrow -14<3x-2<-2$ This approach doesn't seem to be very helpful. Any thoughts of how to prove this?,"['algebra-precalculus', 'calculus']"
2000123,Show that $5\cdot10^n+10^{n-1}+3$ is divisible by 9,"Prove by induction the following: $5*10^n+10^{n-1}+3$ is divisible by 9 Base case: $n=1$ $5*10+10^{1-1}+3=5*10+10^0+3=50+1+3=54$ $9|54=6$ Inductive Hypothesis: If $k$ is a natural number such that $9|5*10^k+10^{k-1} +3$ Inductive step:
Show that $S_k$ is true $\Rightarrow$ $S_{k+1}$ is true $S_{k+1}$: $9|5*10^{k+1}+10^{k} +3$ $9|10(5*10^{k+1}+10^{k} +3)$ $9|5*10^{k+2}+10^{k+1} +10*3$ $9|5*(10^{k+1}*10^1)+(10^{k}*10^1) +(9+1)*3$ $9|5*(10^{k+1}*(9+1))+(10^{k}*10*(9+1)) +(9+1)*3$ $9|5(9*10^{k+1}+10^{k+1})+9*10^k+10^k+((9*3)+(1*3))$ This is were I am stuck for the last day try to figure out what move next would speed up the inductive proof as I have a feeling it can be finished up. Anyone help me see what I am unable to find.","['discrete-mathematics', 'induction', 'elementary-number-theory']"
2000124,"If $f$ is differentiable in $B(a)$ and $f(x) \leq f(a)$ for all $x$ in $B(a)$, then $\nabla f(a) = 0$","Assume $f$ is differentiable at each point of an n-ball $B(a)$. Prove that if $f(x) \leq f(a)$ for all $x$ in $B(a)$, then $\nabla {f(a)} = 0.$ I had my proof, but I'm not sure it is correct. Proof:
Since f is differentiable at each point of the n-ball B(a), meaning $$\lim_{h \to 0} \frac{f(a+hy)-f(a)}{h} = \nabla f(a) \cdot y$$
, where y is an arbitrary unit vector. From the mean value theorem, we know that $$\lim_{h \to 0} \frac{f(a+hy)-f(a-hy)}{h} = \nabla f(c) \cdot y$$
for some c where $||c|| < r$. Since $$\lim_{h \to 0} \frac{f(a+hy)-f(a)}{h} = \nabla f(c) \cdot y = - \lim_{h \to 0} \frac{f(a)-f(a-hy)}{h}$$ Since the RHS of the the first equation is 0, we have $\nabla f(a) = 0$. So, is there any mistake of any suggestion about the point that I can improve mathematically or about the way that I wrote ?","['derivatives', 'proof-verification', 'multivariable-calculus', 'proof-writing', 'vector-analysis']"
2000142,Does the set of inverse images of a generator of a sigma algebra generate the sigma algebra of inverse images of elements of that sigma algebra?,"Let $\Omega$ be a set, $(Y,B)$ be a measurable space, $f:\Omega \to Y$ and let $A=\{f^{-1}(b): b \in B\}$. If $\sigma(C)=B$, then is it necessary that $\sigma(D)=A$, where $D=\{f^{-1}(c):c\in C\}$? The context of the problem: I have a probability space $(\Omega,F,P)$. The sigma algebra $\sigma(X)$ generated by a random variable $X:\Omega\to \Bbb R$ (i.e. a Borel measurable function $\Omega\to \Bbb R$) is the set $\{X^{-1}(B): B \text{ is a Borel set}\}$. Two random variables $X,Y$ are called independent if $\sigma(X)$ and $\sigma(Y)$ are independent, where $S,S'\subset F$ are independent if $P(A\cap B)=P(A)P(B)$ for all $A\in S$ and $B\in S'$. I am trying to prove that two random variables $X,Y$ are independent if and only if for all $a,b\in \Bbb R$, $P(X\le a,Y\le b) = P(X\le a)P(Y\le b)$. I am stuck on the converse. The sets $\{X^{-1}((-\infty,a)): a\in \Bbb R\}$ and $\{Y^{-1}((-\infty,b)): b \in \Bbb R\}$ are $\pi$-systems. If we can prove that they are generators of $\sigma(X)$ and $\sigma (Y)$ (resp.), then we are done because of the following result: If $S,S'\subset F$ are independent $\pi$-systems, then $\sigma(S)$ and $\sigma(S')$ are also independent. Thanks for help.","['probability-theory', 'measure-theory']"
2000145,Suppose $f$ is entire and $|f(z)| \leq A + B |z|^{3/2}$. Show that $f$ is a linear polynomial.,"Suppose $f$ is entire and $|f(z)| \leq A + B |z|^{3/2}$ . Show that $f$ is a linear polynomial. My attempt: Since $f$ is entire, we know that it has a Taylor series expansion on the circle $|z| = R$ , that is $f(z) = \sum_{n = 0}^\infty a_n z^n$ . By using Cauchy's estimate, we can write $|f^n(0)| \leq \frac{n! M}{R^n}$ . Since this holds for any $R>0$ , it follows that for $n\geq 2$ , $|f^n(0)| \leq 0$ . This implies that $f^n(0) = 0$ for $n\geq 2$ . Hence $f$ is a polynomial of degree at most $1$ . Edit 1 : On the circle $|z| = R$ , $|f(z)|\leq A + B R^{3/2}$ . Thank you very much. Kindly suggest me if I am wrong? Is there any other apporoach to solve such kind of problems?",['complex-analysis']
2000268,"Rigorous definition of ""Average""","We usually tend to say the ""Average"" is whether ""Mean"", ""Median"" or ""Mode"" and in colloquial usage ""Average"" is always equivalent to ""Mean"". But my question is: Is there any precise rigorous definition of ""Average of a statistical population"" in statistics (regardless of our knowledge about mean, median or mode)?","['terminology', 'statistics', 'average', 'definition']"
2000289,"Steps for proving that a sequence converges, using the epsilon definition of convergence","I haven't been able to find any sources that clearly and methodically state the approach for proving the convergence of a sequence, using the epsilon definition of convergence. At best, I have been able to find vague, unjustified demonstrations. However, this does nothing to help me learn. I want to be able to generalise this method across all convergence problems that I encounter. I would like someone to state the steps and associated reasoning involved in proving that a sequence converges, using the epsilon definition of convergence. Please specify the reasoning behind each step of the methodology, to assist in justifying your calculations. I would like the 'why' and 'how' behind each step of such a proof. I have the sequence $ \{a_n\}_{n=1}^{\infty}$, where $a_n = \dfrac{(-1)^{n+1}}{n}$, $L = 0$. From what I have read, we want to prove that for any $\epsilon > 0$, there exists some $N > 0$, such that if $n > N$, $|a_n - L| < \epsilon$. However, as alluded to above, I do not fully appreciate or understand what this is saying. Thank you.","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'limits']"
2000294,Why arrival time in Poisson process has uniform distribution given that a single arrival occurred in an interval [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Consider a Poisson process. Given that a single arrival occurred in a given interval [0,t], why is the resulting distribution for the arrival time uniform?","['poisson-process', 'statistics', 'probability', 'poisson-distribution']"
2000308,Is there a ring whose spectrum is homeomorphic to $\Bbb C$ with the Zariski topology?,"Is there a commutative ring $R$ such that $\mathrm{Spec}(R)$ is homeomorphic to $\Bbb C$, both endowed with the Zariski topologies? In other words, is $\Bbb C$ a spectral space , when it is endowed with the Zariski (i.e. cofinite) topology? I know that $\mathrm{Spec}(\Bbb C[x])$ can be seen as $\Bbb C \cup \{(0)\}$, and that $\mathrm{SpecMax}(\Bbb C[x])$ is homeomorphic to $\Bbb C$. This question is closely related. We need to check the following conditions: $X$ is sober . Since we have the cofinite topology, I think this is verified. $X$ is compact. The compactness necessary condition seems to be verified . If $U,V\subseteq X$ are compact open sets, then $U\cap V$ is also compact. I had more trouble to check this property, and also the next one. The compact open subsets of $X$ form a basis for the topology of $X$. I tried also to read this , but it didn't answer my question. 
Anyway, if $\Bbb C$ happens to be a spectral space, what would be a corresponding ring $R$? Its Krull dimension has to be infinite. $\color{white}{\text{In some sense, we could try to change $\Bbb C[X]$ into $R$ by removing the fact that it is an integral domain...}}$ Any comment will be appreciated. Thank you!","['maximal-and-prime-ideals', 'zariski-topology', 'algebraic-geometry', 'commutative-algebra']"
2000333,"Is there a field $F$ which is isomorphic to $F(X,Y)$ but not to $F(X)$?","Is there a field $F$ such that $F \cong F(X,Y)$ as fields, but $F \not \cong F(X)$ as fields? I know only an example of a field $F$ such that $F$ isomorphic to $F(x,y)$ : this is something like $F=k(x_0,x_1,\dots)$. But in this case we have $F \cong F(x)$. This is related to this question on MO. However, it doesn't follow obviously from $R \not \cong R[x]$ that $\text{Frac}(R) \not \cong \text{Frac}(R)(X)$ (I'm not even sure that the given $R$ is an integral domain...). These questions could be relevant: (1) ; (2) . Thank you very much!","['abstract-algebra', 'examples-counterexamples', 'field-theory']"
2000344,4 equal figures which will fit together to form square,"A figure consists of 5 equal squares in the form of a cross. show how to divide it by two straight cuts into 4 equal figures which will fit together to form a square. i cut the figure through 2 perpendicular straight lines through center as shown.
is it correct?
is there any other way to solve this question?",['geometry']
2000354,Right Triangle: Hypotenuse and Side differ by 1,"So I have search to the best of my abilities but cannot find mathematically why this is true and if it is called something specific, the closest thing would be Pythagorean Triples but this is not the case although a $\,(3,4,5)$ , is a triple. I will do my best to explain and apologize in advance as math is not my strength. So in any right triangle I could find the following holds true. If $\,(a,b,c)\,$ and $\,b\,$ differs $\,-1\,$ from $\,c\,$ the other sid is the square root of the sum $\,a = \sqrt{b+c}$ . For example: $(3,4,5) a = \sqrt{4+5} = \sqrt{9} = 3 = \sqrt{9} = \sqrt{25-16} = \sqrt{5^2 - 4^2}$ $(a,17,18) a = \sqrt{17+18} = \sqrt{35} = \sqrt{324-289} = \sqrt{18^2 - 17^2}$ The only way I understand it is the difference between two squared numbers who base differs by $\,1\,$ will always be the sum of both numbers. So assuming $\,b\,$ is always $\,a+1,  b^2 - a^2 = a + b.\,$ Any explanation or pointing to where I can read about more would be much appreciated.","['recreational-mathematics', 'trigonometry', 'pythagorean-triples', 'geometry']"
2000378,Calculate $\lim\limits_{n \to \infty} \frac1n\cdot\log\left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)$ [duplicate],"This question already has answers here : Evaluating $\lim\limits_{n\to\infty}(a_1^n+\dots+a_k^n)^{1\over n}$ where $a_1 \ge \cdots\ge a_k \ge 0$ [duplicate] (2 answers) Closed 6 years ago . Calculate $L = \lim\limits_{n \to \infty} \frac1n\cdot\log\left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)$ I tried putting $\frac1n$ as a power of the logarithm and taking it out of the limit, so I got $$ L = \log\lim\limits_{n \to \infty} \left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)^\frac1n $$ At this point I thought of the fact that $\lim\limits_{n \to \infty} \sqrt[n]{a_1^n+a_2^n+\dots+a_k^n} = max\{a_1, a_2, \dots,a_k\}$ but this won't be of any use here, I guess. How can I calculate this limit, please?","['analysis', 'limits']"
2000414,Are all derivations of real-valued functions derivatives?,"To me it was obvious that all derivatives are derivations, since one can show that they are linear and satisfy the product rule. However, I was very surprised when I learned that every derivation of $C^{\infty}$ functions of a (smooth) manifold is essentially a directional derivative. The definition of a derivation is very simple and consists of two purely algebraic properties, while the definition of directional derivative should seemingly be inseparable from purely analytic notions like limits, Caucy/metric completeness, and the least upper bound property (although these notions may actually be purely topological, see here , although in any case they are not algebraic). However, the proof (see my community-wiki ""answer"" below) relies in a seemingly essential way on Taylor's theorem and its generalizations, and thus on the twice differentiability of the functions. This would seem to explain why calculus is taught in terms of derivatives and not derivations, in spite of the latter's greater simplicity, because to show that they are equivalent requires Taylor's theorem, which to understand requires a prior understanding of what differentiation is, and in any case is usually taught first in the second semester. Question: When we require only first differentiability and not second differentiability or continuous differentiability, is it still the case that all derivations are differential operators? Are there no derivations defined on the space of continuous real-valued functions? I.e., in every context where both derivatives and differentiation are defined, are derivations really exactly the same thing as derivatives? Or only when derivations are restricted to those functions for which Taylor's theorem holds? I have heard of there existing derivations for fields that are not even metrically complete, such as the finite fields $\mathbb{F}_p$ , and since the definition is really so much simpler than that of the derivative, I would be really surprised if they coincided exactly whenever both existed. Related questions: (1) (2) (3) Also I am not sure how to tag this question so please feel free to fix the tags.","['real-analysis', 'abstract-algebra', 'smooth-manifolds', 'differential-algebra', 'differential-geometry']"
2000421,How did early mathematicians make it without Set theory?,"It is said that Cauchy was a pioneer of rigour in calculus and a founder of complex analysis. Yet if baffles me as set theory was an invention of the 1870s, 20 years after the death of Cauchy. Currently the beginning of most concepts in mathematics begins with the concept of set. Furthermore the concept of groups whose foundations were laid by Galois and Abel were done so long before set theory. I hope there is a genral way to answer these questions 1) We define functions with a domain and range both being sets. But when Cauchy used the symbol 'f(x)', what did it really mean to him? As Cauchy was notorious for his rigorous approach, it is hard to believe that he may have just used the word function ambiguously with intuitive satisfaction. (If the following question makes the topic too broad I'd be more than happy to list it as a separate question. 2)To a certain extent I can even brush away the idea of functions before sets. But I simply cannot grasp how the concept of group was formulated without a set and I'm puzzled as to how Galois and Abel were independently able to frame methods to prove the unsolvability of the quintic (these days the proof makes generous use of set theory)without sets. In these days where N, Z, Q and R all sets, how did the early masters do what they did? How on earth was calculus made rigorous without the sets of different numbers?","['real-analysis', 'math-history', 'soft-question', 'elementary-set-theory', 'group-theory']"
2000477,Probability that each r-1 tuple is contained in at least one edge in a r-uniform hypergraph,"Let $n\ge r\ge 2$ and $p\in [0,1]$. Consider the random $r$-uniform hypergraph $H_r(n,p)$ on $n$ vertices, in which each $r$-tuple forms an edge with probability $p$ independently. Let $A$ be the event that each $(r-1)$-tuple is contained in at least one edge. For $\epsilon>0$, show that:
  $$\mathbb P(A)\rightarrow 
 \begin{cases} 
      0 & \text{if } p\leq (1-\epsilon)\frac{(r-1)\ln{n}}{n} \\
      1 & \text{if } p\geq (1+\epsilon)\frac{(r-1)\ln{n}}{n}
   \end{cases}$$
  as $n\rightarrow \infty$. Here is what I tried so far. For each $(r-1)$-tuple $t$, create a random variable $X_t$ which equals $0$ if $t$ is contained in some $r$-tuple and $1$ otherwise.
$\mathbb E(X_t)=\mathbb P(X_t=1)=(1-p)^{n-r+1}$, as there are $n-r+1$ such $r$-tuples containing $t$. Now $\displaystyle \mathbb P(A)=\mathbb P(\sum X_t=0)\ge 1-\sum_t P(X_t=1)=1-\binom{n}{r-1}(1-p)^{n-r+1}$. Now if $p\geq (1+\epsilon)\frac{(r-1)\ln{n}}{n}$, then clearly
$$\binom{n}{r-1}(1-p)^{n-r+1}\le \binom{n}{r-1}\left(1-(1+\epsilon)\frac{(r-1)\ln{n}}{n}\right)^{n-r+1} \rightarrow 0$$
by applying $\binom{n}{r-1}\le \left(\frac{en}{r-1}\right)^{r-1}$ and $1+x\le e^x$ for the second factor. So this gives the second case, however I don't know what to do about the first one.","['combinatorics', 'graph-theory', 'probability', 'discrete-mathematics']"
2000556,"Put $N$ identical balls into $m$ different buckets, each bucket has at least one ball, how many ways?","Suppose $N>m$, denote the number of ways to be $W(N,m)$ First method Take $m$ balls out of $N$, put one ball at each bucket. Then every ball of the left the $N-m$ balls can be freely put into $m$ bucket. Thus we have: $W(N,m)=m^{N-m}$. Second method When we are going to put $N$-th ball, we are facing two possibilities: the previous $N-1$ balls have already satisfies the condition we required, i.e. each of $m$ buckets has at least one ball. Therefore, we can put the $N$-th ball into any bucket. the previous $N-1$ balls have made $m-1$ buckets satisfies the condition, we are left with one empty bucket, the $N$-th ball must be put into that empty bucket. However, that empty bucket may be any one of the $m$ buckets. Therefore, we have the recursion formula: $$
W(N,m) = m W(N-1,m) + m W(N-1,m-1)
$$ It is obvious that the two methods are not identical, which one has flaws? I would like to know which part of the reasoning is wrong and I would also want to hear about the case when the balls are distinct.",['combinatorics']
2000595,How is a morphism different from a function,How is a morphism (from category theory) different from a function? Intuitive explanation + maths would be great,"['category-theory', 'morphism', 'functions']"
2000603,What is the total sum of the cardinalities of all subsets of a set?,"I'm having a hard time finding the pattern. Let's say we have a set $$S = \{1, 2, 3\}$$ The subsets are: $$P = \{ \{\}, \{1\}, \{2\}, \{3\}, \{1, 2\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\} \}$$ And the value I'm looking for, is the sum of the cardinalities of all of these subsets. That is, for this example, $$0+1+1+1+2+2+2+3=12$$ What's the formula for this value? I can sort of see a pattern, but I can't generalize it.","['combinatorics', 'summation', 'elementary-set-theory']"
2000628,"Is there a general formula for $I(m,n)$? [duplicate]","This question already has answers here : Closed form for $ \int_0^\infty {\frac{{{x^n}}}{{1 + {x^m}}}dx }$ (11 answers) Closed 7 years ago . Consider the integral $$I(m,n):=\int_0^{\infty} \frac{x^m}{x^n+1}\,\mathrm dx$$ For $m=0$, a general formula is $$I(0,n)=\frac{\frac{\pi}{n}}{\sin\left(\frac{\pi}{n}\right)}$$ Some other values are $$I(1,3)=\frac{2\pi}{3\sqrt{3}}$$ $$I(1,4)=\frac{\pi}{4}$$ $$I(2,4)=\frac{\pi}{2\sqrt{2}}$$ For natural $m,n$ the integral exists if and only if $n\ge m+2$. Is there a general formula for $I(m,n)$ with integers $m,n$ and $0\le m\le n-2$ ?","['integration', 'definite-integrals', 'calculus', 'closed-form']"
2000662,Does there exist a basis for the set of $2\times 2$ matrices such that all basis elements are invertible?,"As the title says, I'm wondering whether there exists a basis for the set of $2\times 2$ matrices (with entries from the real numbers) such that all basis elements are invertible. I have a gut feeling that it is false, but don't know how to prove it. I know that for a matrix to be invertible, it must be row equivalent to the identity matrix and I think I may be able to use this in the proof, but I don't know how. Thanks in advance for any help, Jack","['matrices', 'linear-algebra']"
2000668,Find the function given its Fourier series,"I am solving an exercise in which I'm asked to show that $$1=\frac{4}{\pi}\sum_{n=1}^\infty{\frac{\sin((2n-1)x)}{2n-1}}, 0<x<\pi$$ I am considering solving this exercise by finding the function given by this sum, but I am pretty sure there is a more elegant solution. Thanks!","['fourier-series', 'sequences-and-series']"
2000685,The closure of intersection of an open set with dense set is the closure of of the open set?,"It is actually problem 4.13 in Folland's Real Analysis. If $X$ is a topological space, $U$ is open in X, $A$ is dense in X, then $\overline{A\cap U} = \overline{U}$. This looks obvious since intersection of an open set with a dense set is dense in the open set. But Folland's book doesn't include the concepts of dense in a subset. It only mentions dense in topological space $X$. So how to approach this problem? Thanks!","['general-topology', 'real-analysis', 'analysis']"
2000704,Which functions of characteristic functions are characteristic functions?,"I have a basic question regarding characteristic functions: If $\phi(t)$ is a characteristic function of some random  variable $X$ , then is it necessary for $\sqrt{\phi(t)}$ $|\phi(t)|$ to be the characteristic function of some other random variable? I need some help to start .","['self-learning', 'probability-theory', 'characteristic-functions']"
2000712,Matrix given by $a_{ij} = 1/(i+j)$ is non-singular.,"What is a smart way to see that $$A\stackrel{\cdot}{=} \begin{bmatrix} 1/2 & 1/3 & \cdots & 1/(n+1) \\ 1/3 & 1/4 & \cdots & 1/(n+2) \\ \vdots & \vdots & \ddots & \vdots \\ 1/(n+1) & 1/(n+2) & \cdots & 1/(2n) \end{bmatrix} $$is non-singular? I computed $\det A$ for $n=1,2$ and $3$ but I failed to see the pattern. The context is as follows: in $\Bbb R[x] $ with inner product given by $\langle p(x),q(x)\rangle \stackrel{\cdot}{=}\int_0^1p(x)q(x)\,{\rm d}x$, I want to see that if $U\stackrel{\cdot}{=} \{p(x) \in \Bbb R[x]\mid p(0)=0\}$, then $U^{\perp} =\{0\}$ (and hence $U^{\perp\perp}=\Bbb R[x]\neq U$). I took $f(x) \in U^\perp$ and computed  $\langle f(x),x^k\rangle$ for $k\geq 1$. I obtained a homogeneous system for the coefficients of $f(x)$ which has $A $ as the matrix associated. So if $A$ is non-singular I'm done.","['inner-products', 'linear-algebra', 'determinant']"
2000768,For which values of positive integer k is it possible to divide the first 3k positive integers into three groups with the same sum?,"I'm on a GCSE-a level syllabus currently, and I can't seem to think of any algebraic equation that I could comprise to solve this (with the GCSE/early a level syllabus). The question in full is For which values of positive integer k is it possible to divide the first 3k positive integers into three groups with the same sum? (e.g. if k = 3, then the first 3k integers are 1,2,3,4,5,6,7,8,9. You can split these into 3 groups of 15, for example {{1,2,3,4,5},{7,8},{6,9}}. so it is possible for k=3) Any help would be appreciated. Thanks","['combinatorics', 'summation', 'integers']"
2000809,An example of a CameronâMartin space,"I am trying to compute an example for a CameronâMartin space, following an exercise in M. Hairer's notes on SPDEs. The problem is the following: consider $\mathcal{C}[0,1]$ endowed with Wiener measure. Prove that the Cameron Martin space (or ""reproducing kernel"" space) associated to this measure is $W^{1,1}.$ To see that an element in the range of the covariance operator has one weak derivative is not difficult. But my computations actually show that the weak derivative is bounded, not only square integrable. In fact let $\mu \in \mathcal{M}[0,1] = C[0,1]^*,$ then $$Q(\mu) (s)= \int_0^1s \wedge u \text{ } \mu(du) = \int_0^s\int_u^1 \mu(dv) du.$$ Now if we look at our weak derivative $\frac{d}{ds} Q(\mu) (s)= \int_s^1 \mu(dv)$ we see that this function is actually bounded, since $\mu$ is of bounded total variation. So the question is: what did I do wrong? Where have I missed the $L^2$ norm of the derivative?","['stochastic-analysis', 'probability-theory']"
2000829,Expected days to finish a box of cookies,Adam has a box containing 10 cookies. Each day he eats each cookie with a probability of $\frac12$. Calculate the expected number of days it takes Adam to complete the cookies. As a start we can set $X$ as the expected days it takes for Adam to finish eating the cookies. However I'm unable to progress further.,"['combinatorics', 'probability', 'discrete-mathematics']"
2000836,Absolute convergence of series $f_n(x)+g_n(x)$ implies convergence of series $f_n(x)$ and $g_n(x)$?,"Consider two function series $\sum_{n\geq 0} f_n(x)$ and $\sum_{n\geq 0} g_n(x)$. The following implication holds: $$\sum_{n\geq 0} f_n(x) \text{ converges absolutely and } \sum_{n\geq 0} g_n(x) \text{ converges absolutely} \\ \implies \sum_{n\geq 0} (f_n(x)+g_n(x)) \text{ converges absolutely}$$ But also the following proposition is true in general for another  function series $\sum_{n\geq 0} h_n(x)$ If $\sum_{n\geq 0} h_n(x)$ converges absolutely, then also every
  subseries converges. So in particular if $h_n(x)=f_n(x)+g_n(x)$, can I say the following? $$\sum_{n\geq 0} (f_n(x)+g_n(x)) \text{ converges absolutely} \\
\implies\sum_{n\geq 0} f_n(x) \text{ converges and } \sum_{n\geq 0} g_n(x) \text{ converges}  $$ If this is true, then is the convergence of $\sum_{n\geq 0} f_n(x)$ and $\sum_{n\geq 0} g_n(x)$ conditional, in general or is it absolute?","['power-series', 'real-analysis', 'sequences-and-series', 'functions']"
2000892,Does there exist a sequence on the unit sphere which converges to zero under compact operator?,"Let  $X$ be an infinite dimensional Banach space and $T: X \rightarrow Y$ be compact operator (between Banach spaces). Question: Does there always exist a sequence $x_n$ of norm $1$ such that $T(x_n)$ converges to zero? If we assume $X$ to be reflexive, this is the case: Choose a sequence on the unit sphere which converges weakly to zero. The compact operator $T$ then maps this sequence to a sequence converging to zero.","['functional-analysis', 'compact-operators']"
2000908,Solve for $x$ without using Lambert's $W$-Function.,"How would you solve $x=e^{\frac{1}{x}}$ for $x$ without using Lambert $W$-Function? WolframAlpha uses $W$, but I want to solve without using $W$. Thanks",['algebra-precalculus']
2000912,"Prove that the function f(x,y) is not differentiable","I have the following problem: Prove that the function: $f(x,y)=
\ \begin{cases} 
      \frac{x^3-x\cdot y^2}{x^2+y^2} & (x,y)\neq (0,0) \\
       \\0 & (x,y)=(0,0)
   \end{cases}
\\$ is continuous on $R^2$ and has its first order partial derivatives.
 everywhere on $R^2$, but $f$ is not differentiable at $(0,0)$ I know how to prove that it is continuous on $R^2$ and its partial derivatives exist at $(0,0)$ (I use limit definition of a derivative). But I do not know how to prove that this function is not differentiable.","['multivariable-calculus', 'calculus']"
2000917,Finding variance of Renewal process: a particular renewal equation,"This is probably well known in Renewal theory. But I didn't find any proof. Let $N(t)$ be the renewal process associated with $X_0=1$ and $X_1, X_2, \ldots$ iid $F$ with mean $\mu$ and variance $\sigma^2$ . Then $$\frac{\operatorname{Var}(N(t))}{t} \to \frac{\sigma^2}{\mu^3}$$ Let $E(N(t))=U(t)=1+\sum\limits_{n=1}^\infty F^{*(n)}(t)$ . After playing with $E(N(t))^2$ I got that $$E(N(t))^2=2U*U(t)-U(t)$$ I am stuck here. The problem is that known limit theorems do not help unless we have direct Riemann integrability and some other stuff.","['stochastic-processes', 'probability-theory', 'lebesgue-integral', 'stochastic-integrals']"
2000925,How do Linear Approximation and Tangent Planes equations generalize in Euclidean n-Dimension?,"I am reading out of Marsdens Vector Calculus and the text gives the same equation under two different headings, namely Linear or Affine Approximations and Tangent Plane to a Surface . The equation is the following at given points $(x_0, y_0)$:
$$z= f(x_0 ,y_0) +[\frac{\partial f}{\partial x} (x_0,y_0) ](x - x_0)+[\frac{\partial f}{\partial y} (x_0,y_0) ](y - y_0)$$ Question 1 Can we generalize this equation to an n-dimensional object which has a tangent (n-1) dimensional object at point $(x_1, x_2, \ldots x_{n-1})$?? Or is the case that this equation of the form $$z= f(x_1,x_2, \ldots x_{n-1}) +[\frac{\partial f}{\partial x_1} (x_1,x_2, \ldots x_{n-1} ) ](x-x_1)+ \cdots + [\frac{\partial f}{\partial x_{n-1}} (x_1,x_2, \ldots x_{n-1})](x- x_{n-1})$$
will represent the tangent plane of an n-dimensional object at point $(x_1, x_2, \ldots x_{n-1})$ in 
$\mathbb R^n$? Question2 :
Can someone prove either of these statements if they are true or suggest where I might find it? Question 3 : Is the equation of affine approximation and the tangent plane always the same equation as dimensions increase or does the affine approximation become expressed by a different equation than the tangent plane equation(or the tangent $n-1$ dimensional-object equation which ever is the case with the equation of this form)?? Thanks in advance, and I apologize for any errors in questions if there be, I'm working off a phone and it is incredibly frustrating.","['multivariable-calculus', 'affine-geometry', 'manifolds', 'dimensional-analysis', 'analysis']"
2000931,Help disproving the following statement.,"I know the following statement is false and I would like to know how to disprove the following statement: If $24|x^2$, then $24|x$. Is giving a counter example enough? Like this: Let $x = 12$, then $x^2 = 144$ So, $144/24 = 6$ but $12/24 = 0.5$. Hence the statement is false. Is this a right way of disproving this? Or do I need to write proof? Please help.","['number-theory', 'divisibility', 'discrete-mathematics']"
2000940,"Lipschitz function $f\colon A \to \mathbb{R}$, $A \subseteq \mathbb{R}$ measurable.","Suppose $A \subseteq \mathbb{R}$ is measurable and $f\colon A \to \mathbb{R}$ is Lipschitz on the set $A$, i.e there is some $K\ge 0$ such that $\lvert f(x)-f(y)\rvert \le K \lvert x-y\rvert$ for $x,y \in A$. I'm trying to prove that
$$
m^\ast(f(E)) \le K\,m^\ast(E)\textrm{ for every set }E \subseteq A.
$$ I've tried covering the set $E$ by intervals, yet the function $f$ may not have been defined outside the set $A$. Also, approaching the set $E$ from inside can encounter problem when $E$ is non-measurable.","['real-analysis', 'lebesgue-measure', 'measure-theory', 'lipschitz-functions', 'bounded-variation']"
2001016,Doubts on definition of the Lie derivative,"I have some difficulties to see why the definition of the lie derivative makes sense. I'm going to use the notations of Lee's book. Let $M$ manifold and $W$ a smooth vector field on $M$. Let $\eta$ the flow of another smooth vector field $V$ $$(L_VW)_p=d/dt(d(\eta_{-t})_{\eta_t(p)}(W_{\eta_t(p)})=\lim_t (d(\eta_{-t})_{\eta_t(p)}(W_{\eta_t(p)})-W_p)/t $$ where the derivative and the limit are computed in $0$ Now: 1) Is the second equality a definition ? 2) I don't understand what a derivative or a limit of these objects means. I would like to find a function $F$ depending on $t$ such that $(L_VW)_p=d/dt F(t)$ computed in $0$. My problem is that if I take $F(t)=d(\eta_{-t})_{\eta_t(p)}(W_{\eta_t(p)})$ I have a function with values not in $\mathbb R $ (I think they are in $T_pM$, right?) So how can I derive a such $F:\mathbb R\to T_pM$ ? Is this problem just that I do not know what a derivative in a topological vector space is or something like this? Same problem with the Lie derivative of a tensor field. P.S. Moreover, the symbol $d/dt$ in the definition of the Lie derivative should be interpreted as the velocity of a curve or as the limit of the difference quotient? Or, more in general, the velocity of a curve into $T_pM$ is actually equals to the limit of the different quotient?","['lie-derivative', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
2001041,Logarithm of the determinant of a positive definite matrix,"For positive definite $C=LL^T$, where $L$ is the lower triangular Cholesky factor of $C$, why is $\log(\det(C))=2\operatorname{trace}(\log(L))$? I know that if $\{\lambda_i\}$ are the eigenvalues of $C$, $\det(C)=\prod_i\lambda_i$, so that $\log(\det(C))=\sum\log(\lambda_i)$ but I'm not sure where to go from there.","['matrices', 'trace', 'positive-definite', 'determinant']"
2001045,Relation between the centralizer and the conjugacy class!,"I am trying to solve the following problem: (a) State a formula relating orders of centralizers and cardinalities of conjugacy classes in a finite group $G$. (b) Let $G$ be a finite group with a proper normal subgroup $N$ that is not contained in the center of $G$. Prove that $G$ has a proper subgroup $H$ with $|H|>|G|^{\frac{1}{2}}$. For part (a) I have: Let $x \in G$. Define $f: G/C_G(x) \to C_x$ by $f(gC_G(x))=gxg^{-1}$. Note that $f$ is well defined since if $gC_G(x)=hC_G(x)$ then $h^{-1}g C_G(x)=C_G(x)$ which means that $h^{-1}g \in C_G(x)$, so $h^{-1}gx=xh^{-1}g$ that is $gxg^{-1}=hxh^{-1}$, thus $f(g)=f(h).$ The same reasoning in opposite way shows that $f$ is injective. Moreover $f$ is trivially onto, then $f$ is a bijection. Therefore $|G/C_G(x)|=[G:C_G(x)]=\frac{|G|}{|C_G(x)|}=|C_x|.$ For part (b) I have. Since $N$ is proper and it is not contained in $Z(G)$, then let $x \in N-Z(G)$, then by part (a) and by taking $H=C_G(x)$ we have $H$ is a proper subgroup of $G$ as if $H=G$ we have $x \in Z(G).$ Moreover, by our formula in part (a) we have $|G|=|H||C_x|.$ Now I am trying to show that $|C_x|$ is strictly less than $|H|$, then we are done. I know that $|C_x|$is less then $|N|$ because since $N$ is normal to $G$ and $x \in N$ we have $gxg^{-1} \in N$ for all $g \in G$. Then I have $|C_x|<|N|$. Can anyone give me some hint to conclude that $|C_x|<|H|$? I would like to think a little bit more about it, so please dont give me the direct solution, just some hint for me give the next step and conclude the thing by myself. Let me know, also, if there is some mistake in my solution for part (a) and/or the solution for part (b) so far. Thank you so so much for your help!","['abstract-algebra', 'normal-subgroups', 'group-theory']"
2001078,"If $\sin \alpha +\cos \alpha =1.2$, then what is $\sin^3\alpha + \cos^3\alpha$?","If $\sin \alpha +\cos \alpha =1.2$, then what is $\sin^3\alpha + \cos^3\alpha$? All I know is that $\sin^{3}a+\cos^{3}a$ is equal to 
$$(\sin a + \cos a)(\sin^{2} a - \sin a \cos a + \cos^{2} a)= \dfrac{6}{5}(\sin^{2} a - \sin a \cos a + \cos^{2} a)$$  But now, I'm stuck.  Solutions are greatly appreciated.","['algebra-precalculus', 'trigonometry', 'calculus', 'systems-of-equations']"
2001115,Recover vector $x$ from rank-$1$ matrix $Q=xx^H$,"Let the matrix $Q \in\mathbb{C}^{n \times n}$ be known. It is also known that $Q=xx^H$ , where $x=[x_1,\ldots,x_n]^T$ and $x^H$ is its conjugate transpose. What is $x$ ? How to recover it?","['matrices', 'matrix-decomposition', 'rank-1-matrices', 'linear-algebra']"
2001147,Looking for a nice example for normal subgroups,"I am looking at normal subgroups at the moment and I came across a note that says that for a normal subgroup H of a group G, it is not always the case that for $a \in G$ and $h \in H$, we have $ha=ah$ for all a and h. However, it does imply that for each $h', \in H$, there is an $h'' \in H$ (where h' and h'' may be distinct) such that $h'a=ah''$. I think that I understand what this is saying but could I see a nice example of this? I greatly appreciate any help!","['abstract-algebra', 'normal-subgroups']"
2001153,Generalization of trace and associated determinant,"The standard relation between the trace and the determinant of matrices is presented in the MO-Q "" Cycling through the zeta garden "" where the log and exp functions allow one to jump between additive and multiplicative operations to relate the det and trace. The relations are easiest to understand with diagonalized matrices, i.e., in an eigenvector rep. The determinant and trace for a Fredholm kernel are defined analogously with the kernel of the Fredholm integral as the analog of a matrix and integration over a continuous variable as the analog of discrete matrix multiplications. Both the matrix and Fredholm operators have associated zeta functions, and the identities among the symmetric elementary, complete, and power polynomials, associated with the cycle index polynomials for the symmetric groups, certainly encompass these relations. What are some other generalizations of the trace and determinant (and their reciprocal characterizations)? (I'm particularly interested to learn if there are analogs in quandle algebra.) (Posted earlier on MO . A comment, but no other responses.)","['functional-analysis', 'special-functions', 'linear-algebra', 'operator-theory']"
2001225,f(x+s)=f(x)f(s) imply that f is an exponential function when f(0) = 1.,"I was reading a proof for a theorem in Erhan Cinlars intro to stochastic processes textbook that any equation with the following $f(x+s) = f(x)f(s)$, with $f(0)=1$. Then $f(x)=e^{(-ax)}$ for some $a$. I have been trying to prove this on my own. I came up with the following, but I didn't quite like the proof, and was wondering if someone could offer a proof utilizing Laplace transforms, or something of that flavor. Here are the following ways I have tried to show it. 1) $f'(t) =\lim_{h\to0} \frac{f(t+h) - f(t-h)}{h} = f(t) f'(0) = f(t)$ a Therefore by differential equations we get our desired result. 2) using laplace transform on $f(t+a) = f(t)f(a)$. First notice that $f(t)$ cannot be zero anywhere (assume its zero at $x$, then $f(t) = f(t+x-x) = f(t-x)f(x) = 0)$. Since it cannot be zero, we know $F(s)$ is not zero anywhere (integral of a strictly positive function is strictly positive function is strictly positive). Therefore, we have $F(S)e^{Sa} = F(S)f(a)$ which is equivalent to $0 = F(S) (e^{Sa}-f(a))$. Which is only true if $e^{Sa} = f(a)$ for all $S$, and a fixed. However, this is only true if $a = 0\dots$ So I am not sure what I am doing wrong here... I wanted to try and prove it using Laplace transforms. I would appreciate any help with understanding this.","['algebra-precalculus', 'applications', 'statistics', 'probability-distributions']"
2001252,"If $a^2 + b^2$ is a prime number $p$, with $p \equiv 1$ (mod $4$), then $a + bi$ is prime in the Gaussian integers $\mathbb{Z}[i]$","I know that there are a lot of questions similar to this, but the reason I'm confused is that it seems that you can prove this just knowing that $a^2 + b^2 = p$ and not use the fact that $p \equiv 1$ (mod $4$) by the following: Suppose $a + bi = xy$ ($x, y\in\mathbb{Z}[i]$). Then $N(x)N(y) = N(a + bi) = p$.
This implies that $N(x) = 1$ or $N(y) = 1$ which tells us that $a + bi$ is irreducible and therefore a prime in the ring of Gaussian integers.
Do I need to use the fact that $p$ is congruent to $1$? Help would be appreciated!","['abstract-algebra', 'algebraic-number-theory', 'gaussian-integers']"
2001269,Value of $\sin(\pi/18)$ by nested radicals,I recently asked a question about how to find value of $\sin(\pi/18)$ and I understand that there is no expression for $\sin(\pi/18)$ that uses the ordinary arithmetical operations. but I know that we have exact value by nested radicals. I want to know how we get this value ? http://mathworld.wolfram.com/TrigonometryAnglesPi18.html,"['nested-radicals', 'trigonometry']"
2001282,"Is the Hausdorff distance between $I=[0,1]$ and $B_n = \{0, \frac{1}{n}, \cdots, \frac{n-1}{n}, 1\}$ $0$ or $\frac{1}{2n}$?","In this post , it is claimed that the  Hausdorff distance $d_H$ between $I=[0,1]$ and $B_n = \{0, \frac{1}{n}, \cdots, \frac{n-1}{n}, 1\}$ equals zero. Is it not equal to $\frac{1}{2n}$ ? For example, if $n=1$, $B_1=\{0,1\}$, and since $(B_1)_{\frac{1}{2}}=I$, I believe the Hausdorff distance equals
$$
d_H(I,B_1)=\inf\{\varepsilon\;|\; I \subseteq (B_1)_\varepsilon,B_1\subseteq I_{\varepsilon} \} = \frac{1}{2}
$$ With the same logic, we can show that 
$$
d_H(I,B_n)=\frac{1}{2n} \rightarrow0
$$ Is this correct? Note : the notation $A_{\varepsilon}$ denotes the $\varepsilon$-thickening of the considered set $A$, that is, the set of points at a distance to any point of $A$ smaller than $\varepsilon\ge0$.","['hausdorff-measure', 'general-topology', 'metric-spaces', 'proof-verification']"
2001325,"Coefficients of a formal power series satisfying $\exp(f(z)) = 1 + f(q\,z)/q$","Let $(q;\,q)_n$ denote the $q$-Pochhammer symbol :
$$(q;\,q)_n = \prod_{k=1}^n (1 - q^k), \quad(q;\,q)_0 = 1.\tag1$$
Consider a formal power series in $z$:
$$f(z) = \sum_{n=1}^\infty \frac{(-1)^{n+1}P_n(q)}{n!\,(q;\,q)_{n-1}}z^n,\tag2$$
where $P_n(q)$ are some (yet unknown) polynomials in $q$: 
$$P_n(q) = \sum_{k=0}^{m} c_{n,k} \, q^k,\tag3$$
where $m=\binom{n-1}2 = \frac{(n-1)(n-2)}2$ and $c_{n,k}$ are some integer coefficients. Suppose the formal power series $f(z)$ satisfies the functional equation
$$\exp(f(z)) = 1 + f(q\,z)/q.\tag4$$
Expanding the left-hand side of $(4)$ in powers of $z$ using the exponential partial Bell polynomials , and comparing coefficients at corresponding powers of $z$ at both sides, we can obtain a system of equations, by solving which we can find the coefficients of the polynomials $P_n(q)$:
$$
\begin{align}
P_1(q) &= 1\\
P_2(q) &= 1\\
P_3(q) &= 2 + q\\
P_4(q) &= 6 + 6 q + 5 q^2 + q^3\\
P_5(q) &= 24 + 36 q + 46 q^2 + 40 q^3 + 24 q^4 + 9 q^5 + q^6\\
\dots
\end{align}\tag5
$$
This is a quite slow process, even when done on a computer. I computed the polynomials up to $n=27$ (they can be found here ) using a Mathematica program that can be found here . There are some patterns in the coefficients I computed (so far they are just conjectures):
$$
\begin{align}
c_{n,0} &= (n-1)!&\vphantom{\Huge|}\\
c_{n,1} &= \frac{(n-2)(n-1)!}2, &n\ge2\\
c_{n,2} &= \frac{(3n+8)(n-3)(n-1)!}{24}, &n\ge3\\
c_{n,3} &= \frac{(n^2 + 5 n - 34)\,n!}{48}, & n\ge4
\end{align}
\tag6
$$
and
$$
\begin{align}
c_{n,m} &= 1&\vphantom{\Huge|}\\
c_{n,m-1} &= \frac{(n+1)(n-2)}2, &n\ge2\\
c_{n,m-2} &= \frac{(3 n^3 - 5 n^2 + 6 n + 8)(n-3)}{24}, &n\ge3\\
c_{n,m-3} &= \frac{(n^4 - 10 n^3 + 43 n^2 - 74 n + 16) (n - 1) \, n}{48}, &n\ge4
\end{align}
\tag7
$$
where $m=\binom{n-1}2$. Other coefficients seem to follow more complicated patterns. We can also observe that
$$
\begin{align}
P_n(1) &= \frac{(n-1)!\,n!}{2^{n-1}}\\
P_{2n}(-1) &= \frac{(2n-1)!\,n!}{3^{n-1}}\\
P_{2n-1}(-1) &= \frac{(2n-1)!!\,(2n-2)!}{6^{n-1}},
\end{align}\tag8
$$
where $n!!$ denotes the double factorial . I am trying to find a more direct formula for the polynomials $P_n(q)$ or their coefficients $c_{n,k}$ (possibly, containing finite products and sums, but not requiring to solve equations).","['real-analysis', 'functional-equations', 'combinatorics', 'sequences-and-series', 'power-series']"
2001393,Segment by $a$ and $b$ and let $z$ be a complex number not in it. Show that $\frac{z-a}{z-b}$ isn't a real $\le 0$,"I need to show that, in a segment $ab$, when $z$ is a complex number outside it, the expression below is never a real $\le 0$. $$\frac{z-a}{z-b}$$ I think it has something to do with the Log function, as it's not analytic exactly in the real negative line with $0$ included. Later in this exercise it asks me to derivate $\log \frac{z-a}{z-b}$ and I did: $\log \frac{z-a}{z-b} = \log (z-a)-\log (z-b)$ then I took the derivative, but I need to know that the expression above is never on the negative real axis. Any ideas?","['derivatives', 'complex-analysis', 'complex-numbers', 'calculus']"
2001412,Why do we assume principal root for the notation $\sqrt{}$,"I'm wondering why when $n$ is even we always assume the positive root for $\sqrt[n]{}.$ For example, if we have $x = \sqrt{4}$ , we always assume $x = 2$ . Yet when we have $x^2 = 4,$ we write $x = \pm\sqrt{4} \Longrightarrow x = -2, 2$ . The problem is that if I take the 1st equation, and square both sides, I get $$x = \sqrt{4} \Longrightarrow x^2 = 4 \Longrightarrow x = \pm\sqrt{4} \Longrightarrow x = -2, 2$$ My teacher says that if you introduce the radical sign, use $\pm$ , but those two equations are the same under the rules about radicals she taught us, so the ""taking the positive root"" rule feels arbitrary to me. I get that people want to make the radical mean something without ambiguity, but making arbitrary rules, like taking just the positive answer, seems to confuse things and lead to inconsistency, like in the above. There are other examples, such as solving for $x$ , then substituting the original equation with the value of $x$ , and not getting it to work because you can only take the positive root. For things like the Pythagorean theorem, people like to say it's obvious to take the positive one only, but there are ways to represent the Pythagorean theorem without relying on human judgement to decipher the final answer.  Say you have a right triangle with legs $a=3$ , $b=4$ , and you want to find c.  So you do: $3^2 + 4^2 = c^2$ , $c > 0$ . and solve the systems of equations, just like any other: $c = \pm\sqrt{25} \Longrightarrow c = -5, 5$ . $c = -5, 5$ intersects $c > 0$ at $c = 5$ , so the answer is $c = 5$ .  I don't see why you would need to redefine square root: $\pm\sqrt{}$ to mean principal square root : $\sqrt{}$ to find the correct answer. Does the ""taking the positive root"" rule have something to do with imaginary numbers, or am I missing something? (I am not asking if $\sqrt{}$ means positive, I'm asking why.)","['algebra-precalculus', 'radicals', 'notation']"
2001438,Is the axiom of choice needed to prove $|G/H| \times |H| = |G|$ (Lagrange's Theorem)?,"Consider the following sequence of assertions, each of which implies the next. Nothing below has any topology, we just have sets and discrete groups. If $F \hookrightarrow  E \twoheadrightarrow B$ is fibre bundle of sets (this just means every fibre has the same cardinality), then $|E| = |F \times B|$ If $H \hookrightarrow E \twoheadrightarrow B$ is a principal bundle, then $|E| = |H \times B|$ If $H$ is a subgroup of $G$, then $|G| = |H \times G/H|$ If $H$ is a normal subgroup of $G$, then $|G| = |H \times G/H$| Question: Do all these assertions fail without the axiom of choice? Or, which of them hold? I actually only care about (3), but for some reason I thought adding these extra statements might somehow clarify things.","['set-theory', 'group-theory', 'axiom-of-choice', 'quotient-group']"
2001462,Is it possible that Fermat was NOT lying when he said he actually knew the proof for his last theorem?,"Fermat put forth his 'last theorem' in 1637 which says that No two positive integers greater than two satisfy the equation $a^n + b^n = c^n$ But Fermat claims that the margin in his notebook was not big enough to accommodate the large proof. Which simply made it a conjecture that was never to become a theorem until after 358 years of mathematicians attempting to prove it. The first successful proof was published in 1995 by Andrew Wiles who used modern methods such as the modularity theorem which is a thing of the 1950's and Ribet's theorem from 1985. My question is is there any way by which Fermat could have possibly known the proof for his theorem? Although Andrew Wiles used modern methods, there are many ways to prove a theorem. The answer probably is that Fermat could have never possibly proved it with the methods he had. (Euler could not provide a general proof either) Now Mathematics still was extremely advanced at times such as that of Fermat and Euler and the theorem even slipped through Gauss, Lagrange, Legendre, Riemann and Dirichlet (all leading proponents of Number Theory). What is it that was lacking that postponed the proof to after three centuries of masters?","['number-theory', 'math-history']"
2001481,Prove that two parts of a chord are equal.,"$O$ is the centre of the large circle $AB$ is a chord of the large circle $OB$ is a diameter of the small circle. Both circles touch at $B$ The small circle cuts the chord $AB$ at $X$ prove that $AX  =   XB$ Attempt I made $OB$ and $AO$ into lines which created an isosceles triangle (because $O$ is the midpoint). I think the next step is to make a line to bisect it, but how would I know where to draw the line? Thank heaps.",['geometry']
2001505,Quick way to check if a matrix is diagonalizable.,"Is there any quick way to check whether a matrix is diagonalizable or not? In exam if a question is asked like ""Which of the following matrix is diagonalizable?"" and four options are given then how can one check it quickly?
I hope my question makes sense.","['diagonalization', 'linear-algebra']"
2001509,Generalizing Ramanujan's $\pi$ formulas,"Ramanujan came up with this neat $\pi$ formula in Cambridge:
$$\frac 1\pi=\frac {2\sqrt2}{99^2}\sum_{k=0}^\infty\frac {(4k)!}{k!^4}\frac {26390k+1103}{396^{4k}}\tag{1}$$
I am simply amazed by this formula, and would like to know if there are other similar types of formulas like this. $(1)$ is interesting in the way that it is efficient, and yet beautiful. It is also closely related to $640320^3$ in some way. (I'm not too sure though) Question: Can $(1)$ be generalized and if so, what would be some examples?","['algebra-precalculus', 'sequences-and-series']"
2001514,Conway's proof of Goldstine's theorem.,"In proving Goldstine's theorem, Conway states the following. Suppose $B$ is the weak-star closure of $J(B_X)$ in $B_{X''}$, and assume there is some $x_0'' \in B_{X''}\smallsetminus B$. He claims Hahn-Banach implies there is some $x'\in X'$ and $\alpha,\varepsilon$ such that $$\langle x',x\rangle  < \alpha<\alpha+\varepsilon < \langle x',x_0''\rangle$$ for every $x\in B$. He asks the reader to figure out why this is true.  Could someone explain? His version of Hahn Banach is the usual one. I'm aware of certain geometric versions. In this case, $B$ is convex and weak-star closed, so it is convex and closed, and $\{x_0''\}$ is of course convex and compact, so I can separate them by a norm closed hyperplane, but I want a weak-star closed hyperplane (that is, I get some functional $\psi\in X'''$ that defines the hyperplane, but I want something in $X'$, right?)",['functional-analysis']
2001516,Prove that the product of two positive linear operators is positive if and only if they commute.,"Having problem in the following problems on positive forms: $1)$ Prove that the product of two positive linear operators is positive if and 
only if they commute. I am able to do one direction that if the product of two positive linear operators is positive then they commute. But unable to do the opposite direction. Let $T,S$ be two positive linear operators and they commute , i.e. $ST = TS$ . To show the product of two positive linear operators is positive we have to show that $\langle TS\alpha,\alpha\rangle > 0$ for any $\alpha \neq0$ and $(TS)^* = TS$ . I have shown the part $(TS)^* = TS$ . I need help to show that $\langle TS\alpha,\alpha\rangle > 0$ for any $\alpha \neq0$ . $2)$ Let $V$ be a finite-dimensional inner product space and $Î$ the orthogonal 
projection of $V$ onto some subspace. $(a)$ Prove that, for any positive number $c$ , the operator $cI + Î$ is positive. $(b)$ Express in terms of $Î$ a self-adjoint linear operator $Î¤$ such that $T^2 = I + E$ . In this I am able to do part $(a)$ but unable to the second part. Can anyone give me any lead to the problems?","['operator-theory', 'positive-definite', 'linear-algebra', 'inner-products']"
2001523,How to construct the set E invoving an almost constant function?,"Assume that $f$ is a function from $\mathbb{R}$ to $\mathbb{R}$, and for all $h\in \mathbb{R}$, the set $E_h=\{x:f(x+h)-f(x)\neq 0,x\in \mathbb{R}\}$ is a finite set which has no more than 2016 elements.  Prove that there exists a set $E$ which has no more than $1008$ elements, such that $f$ is a constant in $\mathbb{R}\backslash E$. To solve this problem, I think it needs a keen observation.  What I have thought is first to prove $f(\mathbb{R})$ have no more than 1008 elements, but it is hard for me.  How can I do this?","['real-analysis', 'functions']"
2001532,Easy criteria to determine isomorphism of fields?,"Let $K$ be a field and $f,g$ irreducible polynomials in $K[X]$, is there a nice iff condition for $K[X]/(f)\cong K[X]/(g)$? ($\cong$ denotes an isomorphism that is the identity on restriction to $K$). Thoughts: It is sufficient that they are $K^\times$ multiples of each other. I'd hoped this was necessary but it isn't as $\mathbb{Q}[X]/(X^2-2)\cong\mathbb{Q}(\sqrt2)=\mathbb{Q}(\sqrt2+1)\cong\mathbb{Q}[X]/(X^2-2X-1)$ with the middle two fields viewed as subfields of $\mathbb{C}$. It is necessary that they have the same degree. Please let me know if there are any other simple necessary conditions. Thanks!","['abstract-algebra', 'extension-field', 'field-theory']"
2001563,Help with tricky double integral,"Consider the region $R$ bounded by the circles
  $$x^{2}+y^{2}=Ax$$$$x^{2}+y^{2}=Bx$$$$x^{2}+y^{2}=Cy$$$$x^{2}+y^{2}=Dy$$
  where $B>A$ and $D>C$. Use the change of variables \begin{cases} u=\frac{x}{x^2+y^2}\\
 v=\frac{y}{x^2+y^2} \end{cases} to evaluate the integral $$\int\int_R  \frac{dxdy}{(x^2+y^2)^3}\quad
 (1)$$ My work so far : the Jacobian is
$$\begin{bmatrix}
\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y}\\ 
 \frac{\partial v}{\partial x}& \frac{\partial v}{\partial y}
\end{bmatrix}$$ With $\frac{\partial v}{\partial x}=\frac{\partial u}{\partial y}=\frac{-2xy}{(x^2+y^2)^2} $ and $\frac{\partial u}{\partial x}=\frac{y^2-x^2}{(x^2+y^2)^2}$
and $\frac{\partial v}{\partial y}=\frac{x^2-y^2}{(x^2+y^2)^2}$
: $$J(u,v)=\frac{\partial (x,y)}{\partial (u,v)}=\frac{-1}{(x^2+y^2)^2} \quad(2)$$ My first approach to get the limits is rearranging $x^{2}+y^{2}=Ax$ and hence $\frac{1}{A}=\frac{x}{x^2+y^2}$ hence $1/A$ is a limit and the rest is $1/B$, $1/C$ and $1/D$. My questions: How do I find the limits of integration? How do I express the integral is terms of $u$ and $v$ ? Any tips will be appreciated thanks!!","['multivariable-calculus', 'integration', 'limits']"
2001575,$(2x-5y+3)dx=(2x+4y-6)dy$,"This is not a new question. Sorry to revive it, but I cannot find another way because it seems that the original question disappeared (It's not me who posted the original question). Someone (I don't remember who) asked about solving the ODE :
$$(2x-5y+3)dx=(2x+4y-6)dy$$
Yesterday, I gave an answer to this question, but there was a mistake in it. At this time I had not enough time to make the correction and to rewrite it. So I deleted my answer. Today, as I wanted to post my corrected answer, I cannot find where the original question as gone. Since I suppose that the unusual method used in my answer would interest some readers, I post the problem as a new question and the answer will be immediately posted in the answer section. This procedure was suggested by Max in order to keeps it off the unanswered queue.","['ordinary-differential-equations', 'partial-differential-equations']"
2001600,Looking for a proof that if a normal subgroup of $Sym(\mathbb N)$ has an element of infinite order then it is the whole group,"I know that $Sym (\mathbb N)$ , the group of all permutations on $\mathbb N$ , has exactly two non-trivial normal subgroups and both of these subgroups are torsion , so it follows that if a normal subgroup of $Sym(\mathbb N)$ has an element of infinite order then it is the whole group . But I am looking for a simpler way to prove that ""if a normal subgroup of $Sym(\mathbb N)$ has an element of infinite order then it is the whole group "" without having to go through determining all the normal subgroups of $Sym(\mathbb N)$ . Is it possible ? I can get that if $N$ is a normal subgroup of $Sym(\mathbb N)$ containing an element of infinite order then $N$ contains an element of order $2$ with exactly countably infinitely many two-cycles and countably infinitely many fixed points . Is this fact any important in proving a simple way my required statement ? Please help . Thanks in advance","['normal-subgroups', 'infinite-groups', 'group-theory', 'symmetric-groups']"
2001632,"Let $f$ be a bijection $\mathbb{N} \rightarrow \mathbb{N}$ , prove there exist positive integers such that...","Let $f:\mathbb{N} \rightarrow \mathbb{N} $ be a bijection. Prove that there exist positive integers
$a < a + d < a + 2d$ such that $f(a) < f(a + d) < f(a + 2d).$","['permutations', 'discrete-mathematics']"
2001638,Can a countable number of intersections of subsets or their complements be the null set?,"Let $$A_i \subseteq X, \ i\in \mathbb{N}$$ be arbitrary subsets of X. Define $$\mathcal{F}= \{ \cap_{i = 1}^\infty C_i \mid C_i = A_i \text{ or } C_i = A_i^c \}$$ Is it true that $\emptyset \in \mathcal{F}$? I tried to construct counter examples for this but could not. EDIT: Another way to put the question is ""Is it possible to pick $A_i$ such that $\emptyset \notin \mathcal{F}$?"". I was not able to come up with counter-examples even considering the simple case of $X=\mathbb{R}$.",['elementary-set-theory']
2001708,Marginal Probability of joint distribution,"Consider random variables X,Y,Z with joint distribution x=1,....y-1,
y=2,3,....z-1;
z=3,4....
and 0< q <1 p+q=1
$$
Pr(X=x,Y=y,Z=z)=p^3  q^{z-3}
$$
What is the marginal probability of random variable X? I found $$Pr(X=x)=p  q^{x-1}$$ Does it correct ? Because Ä± can not show the following equality 
$$
\sum_{x}  Pr(X=x)=1
$$
 Can someone show me how to prove the above equality with my answer or your answers ?","['probability-theory', 'probability', 'statistics', 'probability-distributions']"
2001709,What is this integral $\int {\sqrt{\frac{x+1}{x}}}\:dx$?,I tried a lot of different approaches but found this problem very hard. So can you help me with this integral? $$\int {\sqrt{\frac{x+1}{x}}}\:dx$$ Thanks.,"['integration', 'calculus']"
2001720,How to show that $\sigma (X_n) \subset \sigma (X)$?,"Consider the random variable $X:(\Omega,F,\Bbb{P}) \to \Bbb{R}$ $$X_n(\omega)=n\mathbf 1_{F_n}(\omega)+\sum_{k=1}^{n2^n} \frac{k-1}{2^n} \mathbf1_{E_{n,k}} (\omega)$$
where
$$E_{n,k}=\left\{ \omega: \frac{k-1}{2^n} \le X(\omega) < \frac{k}{2^n} \right\},\,\, k=1,2,\dots,n2^n$$ and $F_n=\{ \omega:X(\omega) \ge n \}$. In here, I'm bit confused about the fact that $\sigma (X_n) \subset \sigma (X)$. How can I show that ? And how can we show that $X_n(\omega)$ converges to $X(\omega)$?","['probability-theory', 'random-variables']"
2001770,Measures converging weakly to Dirac delta function,"I have the following question: Let $\phi \in C_c^{\infty}([-1,1])$ be a smooth, compactly supported function with
integral $2$. Consider the sequence of measures on $[â1, 1]:$
$$\mu_n = n \phi(nx) dx$$ 
where $dx$ is Lebesgue measure. These measures converge in the weak-â topology.
What is the limit measure? Truth is, I'm not entirely sure what I have to show. I know these functions converge to the Dirac delta function. I also know that computing something like 
$$\lim_{x\to 0^+} \int_{\mathbb R} n \phi(nx) dx$$
is maybe the right thing to do to show this... I just can't put these together however. Could someone provide the first step or two just to start off, and I can take it from there? Thanks very much in advance.","['functional-analysis', 'real-analysis', 'distribution-theory', 'weak-convergence']"
2001778,"Proving $\int_0^1\frac{\vert f(x)\rvert^2}{x^2}\,\mathrm dx\le4\int_0^1{\vert f'(x)\rvert^2}\,\mathrm dx$ when $f\in\mathcal C^1([0,1])$ and $f(0)=0.$","Let us assume $f \in \mathcal{C}^1([0,1])$ and $f(0)=0.$ Prove that $$\int_{0}^{1} \frac{\vert f(x) \rvert^2}{x^2}dx \le 4 \int_{0}^{1} {\vert f'(x) \rvert^2}dx.$$ By integrating by parts I obtained the following $$\int_{0}^{1} \frac{\vert f(x) \rvert^2}{x^2}dx = -\frac{1}{x} \lvert f(x) \rvert^2\Big|_0^1+2\int_{0}^{1} \frac{f(x)|f'(x)|}{x |f(x)|} \le 2\int_{0}^{1} \frac{|f'(x)|}{x } $$ but I'm not sure of the result and its usefulness, it's easy Calculus, but I can't go on. Any suggestions?","['calculus', 'analysis']"
2001780,"Show that $\mathbb{E}\left(\bar{X}_{n}\mid X_{(1)},X_{(n)}\right) = \frac{X_{(1)}+X_{(n)}}{2}$","Let $X_{1},\ldots,X_{n}$ be i.i.d. $U[\alpha,\beta]$ r.v.s., and let $X_{(1)}$ denote the $\min$, and $X_{(n)}$ the $\max$. Show that
  $$
\mathbb{E}\left(\overline{X}_{n}\mid X_{(1)},X_{(n)}\right) = \frac{X_{(1)}+X_{(n)}}{2}.
$$ I know that $\displaystyle\mathbb{E}\left(\overline{X}_{n}\mid X_{(1)},X_{(n)}\right)=\mathbb{E}\left({X}_{1}\mid X_{(1)},X_{(n)}\right)$ but not much more.","['uniform-distribution', 'probability-theory', 'conditional-expectation']"
2001806,Prove: $\frac{1}{11\sqrt{2}} \leq \int_0^1 \frac{x^{10}}{\sqrt{1+x}}dx \leq \frac{1}{11}$,"Prove: $\frac{1}{11\sqrt{2}} \leq \int_0^1 \frac{x^{10}}{\sqrt{1+x}}dx \leq \frac{1}{11}$ Hint: Use the (weighted) Mean Value Theorem for Integrals. The MVT for Integrals: Suppose that $u$ is continuous and $v$ is integrable and nonnegative on $[a,b]$ Then $\int_a^b u(x)v(x)dx=u(c)\int_b^a v(x)dx$ for some $c$ in $[a,b]$. I plan on using $u(x)$ as $x^{10}$ as it is continuous and $v(x)$ as $\frac{1}{\sqrt{1+x}}$ as it is integrable on [0,1]. I'm not sure how to go from there and find $c$.","['continuity', 'real-analysis', 'integration', 'calculus']"
2001835,Moment generating function of exponential family,"How does one show, that the moment generating function of $T=(T_1, \ldots, T_k)$ of an natural parameterized $k-$ dimensional exponential family has the following form: $$ \psi(t) = E[exp(t^{\top}T)] = \dfrac{C(\theta_1, \ldots, \theta_k)}{C(\theta_1+t_1, \ldots, \theta_k+t_k)}$$ for $t=(t_1, \ldots, t_k) \in \mathbb{R}^k$.","['probability-theory', 'moment-generating-functions', 'statistics']"
2001836,Why are metrics defined as functions in $\mathbb{R}^{+}$?,"A metric on a set $S$ is a function $d: S^2 \to \mathbb{R}^{+}$ that is symmetric, sub-additive, non-negative, and takes $(x,y)$ to $0$ iff $x=y$. My question is: what makes $\mathbb{R}^{+}$ so special that metrics are universally defined in terms of it? Why don't we use some other totally-ordered set with a least element $m$ and an abelian operator $+$ which preserves order and under which $m$ is neutral? To take it one step further, why aren't metrics defined, more generally, to map elements of $S^2$ to any totally-ordered set that satisfies these conditions?","['abstract-algebra', 'general-topology', 'metric-spaces']"
2001890,Relation between Hilbert's hotel and Cantor's proof of the uncountability of the continuum,"I am reading the wikipedia page about Hilbert's Grand hotel . There it says: Hilbert's paradox is a veridical paradox: it leads to a counter-intuitive result that is provably true. The statements ""there is a guest to every room"" and ""no more guests can be accommodated"" are not equivalent when there are infinitely many rooms. An analogous situation is presented in Cantor's diagonal proof . Now I wonder what Hilbert's Grand hotel has to do with Cantor's diagonal proof, since Cantor's diagonal proof is concerned with showing that the continuum has bigger cardinality than the natural numbers, but Hilbert's hotel seems to be about showing that certain countable sets are equinumerous. Could you clarify?","['intuition', 'soft-question', 'elementary-set-theory']"
2001893,How to justify Einstein notation manipulations without explicitly writing sums?,"In calculating the expression for the coordinates of the Lie Bracket of two vector fields, one has to ""interchange the roles of the dummy indices $i$ and $j$ in the second term"" (p.187, Lee Introduction to Smooth Manifolds ) i.e. justify the following equality: $$X^j \frac{\partial Y^i}{\partial x\ ^j} \frac{\partial f}{\partial x^i} - Y^i \frac{\partial X^j}{\partial x^i}\frac{\partial f}{\partial x\ ^j} \overset{?}{=} X^j \frac{\partial Y^i}{\partial x\ ^j}\frac{\partial f}{\partial x^i} - Y^j\frac{\partial X^i}{\partial x\ ^j}\frac{\partial f}{\partial x^i}. $$ Now writing out the sums explicitly this is fairly easy to do: $$\sum_i\sum_j \left[X^j \frac{\partial Y^i}{\partial x\ ^j}\frac{\partial f}{\partial x^i} - Y^i \frac{\partial X^j}{\partial x^i}\frac{\partial f}{\partial x^j} \right] = \sum_i\sum_j\left[X^j \frac{\partial Y^i}{\partial x\ ^j}\frac{\partial f}{\partial x^i} \right] - \sum_i\sum_j\left[ Y^i \frac{\partial X^j}{\partial x^i}\frac{\partial f}{\partial x^j} \right] \\ = \sum_i\sum_j\left[X^j \frac{\partial Y^i}{\partial x\ ^j}\frac{\partial f}{\partial x^i} \right] - \sum_j\sum_i \left[ Y^j \frac{\partial X^i}{\partial x^j}\frac{\partial f}{\partial x^i} \right] = \sum_i\sum_j\left[X^j \frac{\partial Y^i}{\partial x\ ^j}\frac{\partial f}{\partial x^i} \right] - \sum_i\sum_j \left[ Y^j \frac{\partial X^i}{\partial x^j}\frac{\partial f}{\partial x^i} \right] \\ = \sum_i\sum_j \left[X^j \frac{\partial Y^i}{\partial x\ ^j}\frac{\partial f}{\partial x^i} - Y^j \frac{\partial X^i}{\partial x^j}\frac{\partial f}{\partial x^i} \right] .$$ However, writing out all of these sums is fairly laborous and defeats the purpose of using Einstein notation in the first place. Question: Is there a list somewhere of allowed manipulations using Einstein notation? I would like to use such a list to rigorously justify manipulations like the above using Einstein notation in the future with a clean conscience. I could probably supply the proofs writing out the sums explicitly myself, so the list of allowed manipulations doesn't need to come with proofs for all of the rules. Note: This is related to a previous question of mine , where I asked (essentially) whether and if so which and how many manipulations using Einstein notation require finiteness of the index sets in order to be justified. Note that the above calculation is another example where the finiteness of the indexing sets is appealed to implicitly in order to justify switching the order of summation in the second-to-last step (the third-to-last step consists simply of renaming variables).","['reference-request', 'notation', 'differential-geometry']"
2001898,$f''+f \ge 0$ implies $f(x)+f(x+\pi) \ge 0$,"Let $f: \mathbb{R} \to \mathbb{R}$ be a function of class $C^2$ satisfying $f''(t)+f(t) \ge 0$ for all $t \in \mathbb{R}$. Show that $f(t)+f(t+\pi) \ge 0$. What I did: Set $f''(t)+f(t)=g(t)$. This is an LDE of order 2, and we denote this equation by (E), and the corresponding homegoneous equation by (H). and $f$ is of the form $A\cos(t)+B\sin(t)+y_0(t)$ where $y_0(t)$ is a particular solution of (E), $A,B$ are constants. The trigonometric part cancels in the evaluation of $f(t)+f(t+\pi)$, so the problem boils down to finding a $y_0(t)$ which is always nonnegative. Well let's search for such a $y_0(t)$ using the technique of reductin of order, i.e let's set $y_0(t)=\lambda y_h(t)$ where $y_h(t)$ is a particular solution of $(H)$. All solutions of $(H)$ are sinusoidal so if this method is going to work we might as well set $y_0(t)=\lambda \sin(t)$. Substituting, we find $\sin(t)\lambda''+2\cos(t)\lambda'=g$. So if $\sin(t)=0$, $\lambda'=g(t)/2$. Let $I_{2k}=(2k\pi,(2k+1)\pi)$, $I_{2k+1}=((2k+1)\pi,(2k+2)\pi)$. Define $L_I=2k\pi$ if $I=I_{2k}$, $L_I=(2k+1)*\pi$ if $I=I_{2k+1}$. If $I \in \{I_{2k},I_{2k+1}\}$ we have $(\frac{d}{dt} [\lambda'\sin(t)])/\sin^2(t)=g$ for all $t \in I$ $\lambda'\sin^2(t)=\int^t_{L_I} g(u)\sin(u)\,du+C_I$ for all $t \in I$ where $C_I$ is a constant of integration.Note that the integral is well-defined since $gsin(u)$ is continuous, and the integral is therefore itself continuous. Note that if $I=I_{2k}$, then the integrand is positive, and $\sin^2(t)$  is always positive, so if we choose $C_I$ correctly then $\lambda'$ is positive. The opposite holds if $I=I_{2k+1}$. This is good because we want $\lambda$ positive on $I_{2k}$ and negative on $I_{2k+1}$. Now note that $\lambda'$ is continuous on all the $I$'s. However if we impose that $\lambda'$ be continuous on $\mathbb{R}$ then we run into a problem because 
$\lim_{t \to L_I, t>L_I}RHS=C_I$, which must be equal to $\lambda'(L_I)\sin^2(L_I)=0$, for all $I$. But then 
$\lim_{t \to L_I, t<L_I}RHS=\int^{L_I}_{L_I'} g(u)\sin(u)\,du=0$, where $I'$ is the interval that precedes $I$. Of course in general $g$ doesn't have to satisfy this. So this method kind of breaks down when we consider continuity but I believe it gives a function $f$ which is continuous and differentiable everywhere (and nonnegative, if we choose $C_I=0$) except for the points $L_I$. Edit: Can you please tell me whether my method could work, or is the only possible solution the magical invaraint one given by achille?",['ordinary-differential-equations']
2001921,Can You Add a Multiple of a Matrix Row to itself?,"I apologize in advance, as it seems this might be one of those questions that is so mind-boggling obvious to those who know the answer that most don't even think to treat upon it. I've searched up and down for information on matrice elementary row operations, but none of them have thought to explicitly treat upon the addition of a multiple of a matrix row to itself. All of them say that it is possible to add a multiple of a matrix row to another row, but does adding a multiple of a matrix row to itself present a special case? Is it legal? It's a relatively straightforward question, I suppose, but I haven't been able to find confirmation one way or another on the topic. :( Thank you very much for your time.","['matrices', 'linear-algebra']"
2001922,Every Real number is expressible in terms of differences of two transcendentals,Is it true that for every real number $x$ there exist transcendental numbers $\alpha$ and $\beta$ such that $x=\alpha-\beta$? (it is true if $x$ is an algebraic number).,"['number-theory', 'transcendental-numbers']"
2001937,Variance and Expectation,"Two standard normal variables $Z_1$ and $Z_2$ have covariance 0.3. Find the mean and variance of the random variable $X = 3Z_1- Z_2$. I found out the variance by using the formulae $$\textrm{Var}[X+Y]=\textrm{Var}[X]+ 2\textrm{cov}(X,Y) +\textrm{Var}[Y]$$ however I am not able to find out the mean (which i think is the expected value). In the second question i also have to find the PDF when $Z_1$ and $Z_2$ are independent. Seems like i am missing a trigger point for the mean. Can someone route me to the right direction?","['statistics', 'normal-distribution']"
2001946,"AÂ $a\times b \times c$Â cube formed byÂ small $1^3$ cubes. If a laser beam travelsÂ alongÂ theÂ diagonal of the cube, how manyÂ small cubesÂ doesÂ it cross?","The way I am thinking about solving this problem is to use inclusion exclusion principle. Let $C_1$ be the number of boxes it crosses from the front side, $C_2$ be number of small boxes it crosses from the side, and $C_3$ number of boxes crossed from the top. Now we can get the desired number as $|C_1|+|C_2|+|C_3|-|C_1\cap C_2|-|C_1 \cap C_3|-|C_2\cap C_3|+|C_1\cap C_2 \cap C_3|=a+b+c-|C_1\cap C_2|-|C_1 \cap C_3|-|C_2\cap C_3|+|C_1\cap C_2 \cap C_3|$. However, I am having trouble calculating the intersection sets. I thought about using the $\mathbb{gcd}$ but not sure how to translate my idea into a solution. Any help would be appreciated.","['inclusion-exclusion', 'combinatorics']"
2001949,Binary expansion of $\frac{1}{\pi}\tan^{-1}\left(\frac{5}{12}\right)$,Prove that the binary expansion of $\dfrac{1}{\pi}\tan^{-1}\left(\dfrac{5}{12}\right)$ has strings of $0$s or $1$s of arbitrary length. I didn't see how we can calculate the binary expansion of $\tan^{-1}(x)$ or $\pi$. Is there some other way of solving this question?,"['number-theory', 'decimal-expansion', 'trigonometry']"
