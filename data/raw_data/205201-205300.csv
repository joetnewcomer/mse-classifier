question_id,title,body,tags
4076259,Result related to 3 standard intersectiing circles,"I am in the middle of solving a problem and stuck at the following link:- Below is a standard diagram showing three circles [namely, $\omega=(ABCD) \; (CDPG) \;and\; (BCGQ)$ ] intersecting each other so that DCQ and BCP are straight lines. Let M be the midpoint of PQ. MC produced cuts $\omega$ at R. If PR cuts $\omega$ at L, prove that L, D, G are collinear. Or equivalently, if we assume that PR and GD produced meet at L, prove that L is a con-cyclic point of $\omega$ .","['euclidean-geometry', 'geometry']"
4076263,Creating a Skyrim Alchemy metric: combining the binomial theorem and birthday paradox,"I am trying to create a metric to sort alchemy ingredients by in Skyrim,  I wanted to sort the ingredients by ""probability of yielding a potion with the number of ingredients I have"". The way I was looking at the problem is that it is a combination of the Birthday paradox and a binomial trial, but I'm having trouble mixing the two concepts, because 1) it's hard 2) I never took a proper stat class. I'm a physicist by training, and my stat education was all based on error propagation and curve fitting. If you haven't played Skyrim, these are the rules (R) for the alchemy mini-game: Skyrim has ingredients which are combined into potions. Each ingredient has 4 effects, which must be ""discovered."" You can attempt to make a potion with 2 or 3 ingredients. If any of the effects in the combination are the same, a potion will be created with those matched effects. A potion can be created whether the effects were known or unknown before the combination. When you combine 2 ingredients, whose effects were unknown before the combination, and those ingredients both have the same effect, the effect will be discovered for both ingredients. The game keeps track of failed combinations of ingredients. If you select ingredient I, ingredients I' that produced a failed experiment will be greyed out. It is currently unknown if this is ""pullable data"", but it is immediately obvious when playing the alchemy mini-game. A Skyrim alchemy tutorial, with relevant part starting at 8:28 https://www.youtube.com/watch?v=3E3PkxusHDc&t=508s The end product would hopefully be a mod that would sort the alchemy ingredients by the metric. It would either be a modification to, or require the mod called SkyUI. See: https://www.nexusmods.com/skyrimspecialedition/mods/12604?tab=posts Assumptions (A) for modelling the problem Total number of effects is known and fixed (56, e.g. Ravage Health, Fortify Barter, Restore Magika) Potions are created using 3 ingredients When searching for the effects ingredient I, ingredients I' and I'' will be chosen so that no common effects are known. i.e. Skyrim reports ""A potion of unknown effect."" For a given ingredient, all 4 effects are different. See: https://elderscrolls.fandom.com/wiki/Ingredients_(Skyrim) No two ingredients have all 4 of the same effects. The number of ingredients can change based on DLC's and mods to the game, but is approximately 80-200. The game's ""sell value"" of an ingredient is a good proxy for its rarity. Not that important at this stage, but if I am to implement this into a mod for Skyrim, I have these functions to work with (I can create more, or make lookup tables): https://www.creationkit.com/index.php?title=Math_Script For any other imposed assumptions or approximations, an error or uncertainty of <=1% is negligible, and up to 10% is acceptable. E.g. an approximation of a binomial coefficient, swapping a binomial distribution for a gaussian, hack to decrease calculation time. Conjectures (C) (statements that are probably true enough to model the problem with) If you are trying to determine unknown effect(s) of ingredient I, it doesn't matter if the effects of I' or I'' are known or not. Effects are randomly and uniformly distributed amongst ingredients, given conditions A4, A5. Not true, but close enough, see https://elderscrolls.fandom.com/wiki/Ingredients_(Skyrim) . A3 implies that each experiment is testing the unknown effects of ingredient I, amongst 8 unique effects in ingredients I' and I''. This isn't completely true, since if you make a combination of 3 ingredients where I' and I'' have at least 1 unknown effect, it is possible that you could discover a matched effect between I' and I'', but not discover an effect in I. Hypotheses (H) A given experiment/combination/trial, can be modelled as a birthday paradox. With a given unknown effect checking for matches amongst 8 effects in ingredients I' and I''. With the number of people in the room being subbed for number of effects (8? 9? 10? 11? 12?), and days of the year being subbed for the total number of effects (56) Repeated trials can be modelled with the binomial distribution. The probability of a match from the birthday trial gets plugged into the probability of success in the binomial distribution. Questions (Q) for the Internet Are hypotheses 1-3 correct? How do I model the birthday problem? It's like 4 people go into a room of 8, none of the 4 entering people have the same birthdate, and the people in the room could have 5-8 unique birthdays selected from 56 days. For the binomial model, I need ""at least 1 success"", how do I model that? Is it using the (binomial?) distribution equation, or the formula for the definite integral of the distribution? Is the chance of a creating a potion based more on the first ingredient, or the 2nd and 3rd ingredients? What are some resources that would help me solve this problem? Assume that I have access to the internet, LibreOffice, scihub, torrent sites, but not a library or a college math department. I theoretically have access to Mathematica, but it would take 1-8 hours to set up on my Raspberry Pi 3B (I tried yesterday, and I think I FUBARed the OS, hopefully not the Pi or SanDisk). Equations (E) I would actually like to obtain/solve using my mod/code/equations. For the following, Let: I be the ingredient under test, k = number of known effects of ingredient I, T=total number of effects in Skyrim (56), q=quantity of ingredient I (practical range 0-10000), n=number of ingredients in inventory (minimum 3, no theoretical max, practical range 40-100). Given 3 Ingredients, where none of the effects are known, what is the probability that a potion will be created in 1 trial. Given an ingredient I, with k of 4 known effects, what is the probability of of creating a potion with ingredients I' and I'', assuming Skyrim says ""A potion of unknown effect"" What is the probability of discovering effect I_e (e is a subscript from 1-4), amongst all ingredients, within n trials, where n is the quantity of ingredient I in inventory. What is the expected gold wasted (game value of ingredients) before effect I_e is discovered, performing the search required for Equation 3.","['statistics', 'probability-distributions', 'combinatorics', 'binomial-distribution']"
4076267,Need help understanding function notation,"I just finished my test and I have no idea what one of the questions was. No academic dishonesty here, I already got graded and I got 75%, but I am just stuck on that exercise and have no idea how to understand the function in the question. It was supposed to be an integration of a function, and it looked like this: $$f: \mathbb{R}^2 \rightarrow \mathbb{R}; (x,y) \rightarrow x$$ I dont even remember the teacher using this notation, any help is appreciated, thanks!","['integration', 'functions']"
4076302,Find all basis for a set V,"The set $V= \{ v_1, v_2,..., v_6 \}$ is represented by $V = \{ (1, 0, 2, 3), (4, 0, 8, 12), (4, 3, 2, 1), (4, 6, -4, -10), (1, 1, 1, 1), (2, 1, 4, 3)$ . Of course the set is in $\mathbb{R^4}$ . Find all the bases that are subsets of V. First, to satisfy the condition for a basis, it first needs to be linearly independent. So should I make this into a matrix, and convert it to reduced row echelon? From there, I can found out which vector is linearly independent by looking at the pivot columns, which are guaranteed to be linearly independent by definition.","['elementary-set-theory', 'change-of-basis', 'linear-algebra']"
4076352,"Incompleteness of $(C[a,b], \| \cdot \|_1)$ [duplicate]","This question already has an answer here : Completeness of $\langle \mathscr{C} [0, 1], \| \cdot \|_1 \rangle$ (1 answer) Closed 3 years ago . Consider $C([a,b])$ , the vector space of continuous functions in $[a,b] \subset \mathbb{R}$ to $\mathbb{R}$ . Let be $f \in C([a,b])$ , we define $\|\cdot\|_1$ as follows: \begin{equation*} 
\|f\|_1 =  \int_{a}^{b}|f(x)|dx 
\end{equation*} I must show that $(C([a,b]), \|\cdot\|_1)$ is not a Banach space. This is my answer the which one is wrong according to my teacher. Consider the following counterexample. Let $c =2^{-1}(a+b)$ y $f_n:[a,b] \rightarrow \mathbb{R} \hspace{.1cm} \forall n \in \mathbb{N}$ with the following association rule and graph: It's clear that $\{f_n\}_{n \in \mathbb{N}} \subset C([a,b])$ . We will show that $\{f_n\}_{n \in \mathbb{N}}$ it's a $||\cdot||_1-Cauchy$ sequence. Let be $\epsilon > 0$ and $N = \displaystyle \left\lceil{ \frac{1}{2 \epsilon}}\right\rceil$ , then: \begin{equation*}
    ||f_n-f_m||_1 = \int_{a}^{b} |f_n(x)-f_m(x)|dx = \left| \frac{1}{2n}-\frac{1}{2m} \right| \leq \max \left\{ \frac{1}{2n}, \frac{1}{2m}\right\} < \epsilon, \hspace{.1cm} \forall n,m \geq N
\end{equation*} So the sequence is $||\cdot||_1-Cauchy$ ,
however though this sequence converges to the following limit function, the limit function is clearly discontinuous. Let be $f:[a,b] \rightarrow \mathbb{R}$ : \begin{equation*} f(x) = 
    \begin{cases}
        1 & \text{si $x \leq c$}\\
        0 & \text{si $x > c$}
    \end{cases}
\end{equation*} Fixed $\epsilon_1 > 0$ and $N_1 = \left\lceil{\displaystyle\frac{1}{2 \epsilon_1}}\right\rceil$ , then: \begin{equation*}
||f_n-f||_1 = \displaystyle\int_{0}^{1} |f_n(x)-f(x)| = \frac{1}{2n} < \epsilon, \hspace{.1cm} \forall n \geq N_1
\end{equation*} $\textbf{Now, the problem is that}$ (according to my teacher) I only have propose a discontinuous function wich is a limit of the sequence, this does not means that do not exits another continuous function wich is also the limit of the sequence. I understand the problem, but nothing comes up to me to solve this.","['banach-spaces', 'normed-spaces', 'analysis', 'calculus', 'functional-analysis']"
4076393,Value of $\sin(œÄ/x)$ for $x$ positive integer,"Is there a known Generalized algebraic formula for the following: $\sin(\pi/n)$ here, $n$ is an positive integer",['trigonometry']
4076478,Expectations of mean squared error,"I am currently studying the expected mean squared error and the derivations of this are as follows: \begin{align}
&E[(y_i - \hat{y}_0 | x_0]\\
&=E [ (f(x) + \epsilon_i - \hat{f}(x_i))^2 | x_0]\\
&=E [(f(x) - \hat{f}(x_i))^2|x_0] + 2 E [(f(x_i)-\hat{f}(x_i))\epsilon_i | x_0] + E[\epsilon_0^2 |x_0]\\
\end{align} From which we can further get the bias and variance. But I do not understand the following, what rule is used in order to rewrite it using the following part: $2 E [(f(x_i)-\hat{f}(x_i))\epsilon_i | x_0]$ ?","['expected-value', 'mean-square-error', 'statistics']"
4076488,Are there any interesting non standard models of $Q$?,"I've encountered a few of the models of Robinson arithmetic here a quick list: $\mathbb{N}\cup {\infty}$ (used to show Robinson arithmetic has a non standard model) $\mathbb{N}\cup \{a,b\}$ where the operations are defined so $a$ and $b$ do not commute. $\mathbb{Z}[x]^+$ which is the set of all integer polynomials with positive leading coefficient. (In this model you can show Robinson Arithmetic cannot prove all elements are odd or even.) All these models seem to be used only to show how weak Robinson Arithmetic is or rather what things it's unable to prove. Besides the natural numbers are there some models that have some importance beyond being used as a counter example? I am aware that any model of $Q$ must contain an isomorphic copy of the natural numbers. But for weak theories like the group axioms there are plenty of important ""models"" for example the symmetric group is studied for other reasons besides proving that the group axioms can't prove commutativity.","['elementary-set-theory', 'model-theory', 'first-order-logic']"
4076505,Isoperimetric inequality,"I'm trying to understand a lemma from this paper . In my understanding, it says that Since $M$ is closed, there exists $\alpha>0$ and a map $\phi:M\to \mathbb{R}^n$ (with some property that I am not familiar with) such that $\phi (B_M(x,\alpha))=B_{\mathbb{R}^n}(\mathbf{0},1) $ . What is this map? For any closed curve $\gamma$ on the boundary of $B_{\mathbb{R}^n}(\mathbf{0},1)$ , the area of $D'=\mathbf{0}\circledast \gamma$ , the cone over $\gamma$ with respect to the origin, is less or equal than the square of the length of $\gamma$ . Is there any way to prove this using some integral?","['differential-topology', 'minimal-surfaces', 'differential-geometry']"
4076533,Proof $f(n)=f(n-1)+2f(n-2)$ by induction.,"I am having a hard time trying to prove the following recurrence relation $$f(n)=f(n-1)+2f(n-2),\quad n\geq 2, \quad f(1)=1 \quad \& \quad f(2)=3.$$ $f(n)$ is the number of ways to tile a $(2\times n)$ -rectangle with tiles of the form $2 \times 1$ , $1 \times 2$ , or $2\times 2$ . I attempt this by induction but I am confused in the base case since I get $f(2)=f(1)+2f(0)$ but we do not know $f(0)$ . Can someone help?","['induction', 'recurrence-relations', 'discrete-mathematics']"
4076567,Proof for a maximal inequality without using optional stopping theorems,"Is it possible to prove the following maximal inequality without using optional stopping theorems or any result related to stopping times: For a nonnegative sub martingale $X_n$ $(\infty$ is a possible value $)$$,\lambda>0$ $$\lambda P(\max_{1 \leq k \leq n}X_k>\lambda)\leq E[X_n1_{\{\max_{1 \leq k \leq n} X_k>\lambda\}}] \ \ \ \ \ \ \ \ \ (1)$$ Considering $F(x)=(x-\lambda)^+$ and $f(x)=1_{[\lambda,+\infty[}(x),$ so $F$ is convex, increasing and $F'=f$ (we can consider the derivative at right). Let $Y_n=\max_{1 \leq k \leq n}X_k,W_n=F(Y_n)-(Y_n-X_n)f(Y_n).$ So $Y_n$ is a sub martingale, since $F(Y_{n+1})-F(Y_n)-(Y_{n+1}-X_{n+1})f(Y_{n+1})+(Y_n-X_n)f(Y_n)=F(Y_{n+1})-F(Y_n)-(Y_{n+1}-X_{n+1})f(Y_{n})+(Y_n-X_n)f(Y_n) \geq f(Y_{n})(Y_{n+1}-Y_n)-f(Y_n)(Y_{n+1}-X_{n+1})+(Y_n-X_n)f(Y_n) \geq f(Y_n)(X_{n+1}-X_n).$ This means that $0 \leq E[F(X_1)] \leq E[F(Y_n)]-E[f(Y_n)(Y_n-X_n)],$ so $$\lambda P(Y_n>\lambda) \leq E[X_n1_{\{Y_n>\lambda\}}].$$ In order to have finite values (integrals are well defined...), tried to truncate: $V_n=\min(k,X_n),$ but $V_n$ is not necessarily a sub martingale. Do you any other ways to prove $(1)$ ? Also in the above way, how to deal with the integrability (especially infinite values)?","['measure-theory', 'real-analysis', 'stochastic-processes', 'martingales', 'probability-theory']"
4076582,Can the gradient operator $\nabla_\mathbf{x}$ be treated as a standalone vector?,"Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a scalar function. Then, the gradient of $f(\mathbf{x})$ is defined as: $$
\nabla_\mathbf{x} f(\mathbf{x}) =
\begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1} \\
\frac{\partial f(\mathbf{x})}{\partial x_2} \\
\vdots \\
\frac{\partial f(\mathbf{x})}{\partial x_n}
\end{bmatrix}
$$ However, if $f(\mathbf{x})$ is a scalar, then wouldn't the following also be valid? $$
(\nabla_\mathbf{x}) (f(\mathbf{x})) =
\begin{bmatrix} \frac{\partial}{\partial x_1} \\
\frac{\partial}{\partial x_2} \\
\vdots \\
\frac{\partial}{\partial x_n}
\end{bmatrix} f(\mathbf{x}) = 
\begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1} \\
\frac{\partial f(\mathbf{x})}{\partial x_2} \\
\vdots \\
\frac{\partial f(\mathbf{x})}{\partial x_n}
\end{bmatrix}
$$ In other words, the gradient $\nabla_\mathbf{x}$ is treated as a standalone vector, and then it is multiplied by the scalar $f(\mathbf{x})$ . Interestingly, if this is indeed true, then I can build the Hessian matrix of $f(\mathbf{x})$ using the following outer product: $$
(\nabla_x \nabla_x^T) (f(\mathbf{x}))
$$ In other words, I left-multiply the row vector $\nabla_x^T$ by the column vector $\nabla_x$ to get an $n \times n$ matrix of second-order partial derivative operators. I then multiply this matrix by the scalar $f(\mathbf{x})$ to get the Hessian matrix. However, I am wondering if this is just a coincidence.",['multivariable-calculus']
4076593,When $G/H$ is cyclic?,"Clearly $G/H$ is cyclic if $G$ is cyclic. But sometimes $G/H$ is cyclic without $G$ necessarily cyclic, for example: $(\mathbb{Z}_6\times \mathbb{Z}_6)/\left \langle (2,3) \right \rangle \cong \mathbb{Z}_6$ In cases like the latter, how can one determine if the group is cyclic? I mean, is there something in general that lets me know? For example Write $(\mathbb{Z}_{20}\times \mathbb{Z}_6)/\left \langle (10,2) \right \rangle $ as an external direct product of cyclic groups of prime power order. Is $(\mathbb{Z}_{20}\times \mathbb{Z}_6)/\left \langle (10,2) \right \rangle $ isomorphic to $\mathbb{Z}_{4}\times\mathbb{Z}_{5}$ or isomorphic to $\mathbb{Z}_{2}\times \mathbb{Z}_2\times \mathbb{Z}_5$ ?","['cyclic-groups', 'finite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
4076598,Fastest way to find derivative of the function at the given point,"What is derivative of $f(x)=\left(\dfrac{\sqrt[3]{x^2+2x}}{x^2-x}\right)^3$ at the point $x=2$ ? $$1)-\frac34\qquad\qquad2)-\frac54\qquad\qquad3)-\frac52\qquad\qquad4)-\frac{15}4$$ This is a problem from an timed Exam, so I am looking for the fastest way to solve this. Here is my solution: We have $f(x)=\dfrac{x^2+2x}{(x^2-x)^3}$ . by using Quotient rule, derivative at $x=2$ is: $$\dfrac{6\times8-(3\times3\times4)\times8}{8^2}=\frac34-\frac92=-\frac{15}4$$ Although the way I putted the values of functions instead of writing the whole derivative of function seems to be fast , it is hard to avoid algebraic mistakes and find the correct answer in the exam condition (having time pressure, stress  and so on). After all, is there a better approach (faster) to solve this problem ?","['calculus', 'derivatives', 'rational-functions']"
4076627,"For $u,v,x \in \mathbb{R}^m$, let $T(u,v)(x)=x - \frac{\langle(u+v),x\rangle}{1+\langle u,v\rangle}(u+v)+2\langle u,x\rangle v$","The function in question appears in lemma $6.3$ of Milnors characteristic classes. Let $\langle \cdot, \cdot\rangle$ denote the dot product on $\mathbb{R}^m$ Let $u,v \in \mathbb{R}^m$ be unit vectors with $u \neq -v$ . Let $T(u,v)$ denote the unique rotation of $\mathbb{R}^m$ which carries vector $u$ to vector $v$ and leaves everything orthogonal to $u$ and $v$ fixed. Alternatively, $T(u,v)$ can be defined by: $$T(u,v)(x)=x - \frac{\langle(u+v),x\rangle}{1+\langle u,v\rangle}(u+v)+2\langle u,x\rangle v$$ Can somebody help me understand why this formula above works? I'm having troubles understanding why this formula gives the desired rotation. Thank you.","['proof-explanation', 'linear-algebra', 'geometry']"
4076658,$\mathbb{CP}^1$ is diffeomorphic to $S^2$.,"I just want to double check that I established that diffeomorphism correctly. First, let's recover the definition of a diffeomorphism $F$ between two smooth manifolds $M$ and $N$ . We say that $F:M\to N$ is a diffeomorphism if $F$ is bijective, $F$ is smooth and $F^{-1}$ is smooth. Consider the map $F:S^2\to\mathbb{CP}^1$ where $$F(x,y,z)=\begin{cases}
      [1;\frac{x}{1-z}+\frac{y}{1-z}i], & \text{if}\ (x,y,z)\neq (0,0,1) \\
      [0;1], & \text{otherwise}
    \end{cases}$$ and the map $F^{-1}:\mathbb{CP}^1\to S^2$ where $$F^{-1}([1,u+vi])=(\frac{2u}{u^2+v^2+1},\frac{2v}{u^2+v^2+1},\frac{u^2+v^2-1}{u^2+v^2+1})$$ and $F^{-1}([0;1])=(0,0,1)$ . We can easily see that $F$ is bijective as $F\circ F^{-1}=F^{-1}\circ F=id$ . Next, consider the stereographic projection charts $\{U_0,U_1\}$ for $S^2$ and the standard charts $\{V_0,V_1\}$ for $\mathbb{CP}^1$ where, for example, $V_0=\{[z_0,z_1]|z_0\neq 0\}$ with the corresponding homeomorphisms $\phi_i$ and $\psi_i$ , $i=0,1$ . To show that $F$ is smooth we want to show that $$\psi_i\circ F\circ \phi^{-1}_j:\phi_j(U_j\cap F^{-1}(V_i))\to V_i\text{ is a smooth map, for all }i,j$$ which follows from the basic computations. For example, $$\psi_1\circ F\circ \phi_0^{-1}(u,v)=(\frac{u}{u^2+v^2},\frac{-v}{u^2+v^2}).$$ Is there another faster way to establish a diffeomorphism?","['manifolds', 'diffeomorphism', 'differential-geometry']"
4076659,Transform the squared error loss function to matrix form,"I have n data, $x_i$ are the inputs, and $y_i$ are the labels. $x_i$ is $d$ -dimension, and $y_i$ is $p$ -dimension. We have a prediction function that is $f(x) = W^Tx$ . $W$ is a matrix of dimension $d \times p$ . The squared error loss function of the prediction function is: $$
  J(W) = \sum_{i=1}^n||y_i - W^Tx_i||_2^2
$$ I would like to write $J(W)$ in matrix form. How should I convert it to an equation involving $Y$ ( $n \times p$ dimension), $W$ ( $d \times p$ dimension) and $X$ ( $n \times d$ dimension). Hence the loss function will be in matrix form instead of a sum?","['matrices', 'linear-algebra']"
4076689,Fatou's lemma - Royden's proof,"Royden states that to prove Fatou's lemma it is ncessary and sufficent to show that if $h$ is any bounded measurable function of finite support for which $0\leq h\leq f$ on $E$ , then $$
\int
_E h\leq \lim \inf \int_E f_n$$ Why did Royden choose to construct such a function instead of working directly with $f$ ?","['measure-theory', 'real-analysis']"
4076701,Is there a function whose derivative is discontinuous everywhere?,"Is there a function whose derivative is discontinuous everywhere? As is well-known by Darboux, a derivative $f'$ could have discontinuity of second type. My question is: whether there exists a function $f$ , such that $f'$ is discontinuous everywhere.","['calculus', 'derivatives']"
4076758,Conditions for weak convergence and generalized distribution functions,"I am having some trouble proving Corollary 6.3.2 in Borovkov's Probability Theory (for reference, this material is on pages 147 to 149 in the book). For convenience, I provide some definitions and related theorems. Skip to the end for a TLDR. Definition: We say a function $G$ is a generalized distribution function if it satisfies monotonicity and right-continuity. We denote the class of generalized distribution functions to be $\mathcal{G}$ and the class of distribution functions to be $\mathcal{F}$ . Of course, $\mathcal{F}\subseteq \mathcal{G}$ and the only difference is that distribution functions have $\lim_{x\to\infty}F(x) = 1$ and $\lim_{x\to-\infty}F(x) = 0$ . Definition: We say that a sequence of generalized distribution functions $\{G_n\}\subseteq\mathcal{G}$ converges weakly to some $G\in \mathcal{G}$ if, for all points of continuity $x\in\mathbb{R}$ of $G$ , we have $G_n(x)\to G(x)$ . Note that the above definition is not as ""nice"" as weak convergence in distribution functions. Recall that, for $\{F_n\}\subseteq \mathcal{F}$ and $F\in\mathcal{F}$ such that $F_n(x)\to F(x)$ for each point of continuity of $F$ , an equivalent condition is to say that, for all bounded continuous functions $f$ , we have $$\int f\ \mathrm{d}F_n \to \int f\ \mathrm{d}F.$$ However, this equivalence is not true for the case where $G_n,G\in\mathcal{G}$ . Some extra definitions: Definition: A sequence of probability measures $\{\mathbb{F}_n\}_{n=1}^\infty$ is said to be tight if, for all $\varepsilon>0$ , there exists some $N\in\mathbb{N}$ such that $$\inf_n \mathbb{F}_n([-N,N]) > 1- \varepsilon.$$ Definition: A class of bounded continuous functions $\mathcal{L}$ is said to be distribution determining if, for $F\in \mathcal{F}$ and $G\in \mathcal{G}$ , $$\int f\ \mathrm{d}F = \int f\ \mathrm{d}G \qquad (\forall f\in \mathcal{L})$$ implies that $F=G$ . The book then presents a variation of Helly's selection theorem and a corollary: Theorem (Helly's Selection Theorem): Let $\{G_n\}_{n=1}^\infty\subseteq \mathcal{G}$ be a sequence of generalized distribution  functions, then there exists some subsequence $\{G_{n_k}\}_{k=1}^\infty$ and $G\in \mathcal{G}$ such that $G_{n_k}$ converges weakly to $G$ for some $G\in \mathcal{G}$ . That is, the space $\mathcal{G}$ is sequentially compact with respect to weak convergence. Corollary: If every convergent subsequence of a sequence $G_n$ converges weakly to the same $G\in\mathcal{G}$ then the entire sequence $G_n$ converges weakly to $G$ . The following theorem is true: Theorem: Let $\mathcal{L}$ be a distribution determining class and $\{\mathbb{F}_n\}_{n=1}^\infty$ be a sequence of probability measures. Then $\mathbb{F}_n$ converges weakly to some probability measure $\mathbb{F}$ if and only if the sequence $\{\mathbb{F}_n\}_{n=1}^\infty$ is tight and $\lim_n \int f\ \mathrm{d}\mathbb{F}_n$ exists for all $f\in \mathcal{L}$ . I will provide a proof of the converse that basically outlines Borovkov's proof: Proof . By Helly's selection theorem, there exists a subsequence of distribution functions (that correspond to the probability measures) $F_{n_k}$ that converges weakly to some $F\in \mathcal{G}$ . Now, let $\varepsilon>0$ . By the tightness of $\mathbb{F}_{n_k}$ , can find some $M$ such that, for all $x\geq M$ , we have $$\inf_k F_{n_k}(x) \geq \inf_k \mathbb{F}_{n_k}([-M,M]) > 1- \varepsilon.$$ Let $x$ be a point of continuity of $F$ with $x\geq M$ (this must exist as $F$ only has countably many points of discontinuity). We then have $$F(x) = \lim_k F_{n_k}(x) \geq \inf_k F_{n_k}(x) >1- \varepsilon.$$ Further, for all $y\geq x$ , we would have $F(y) > 1- \varepsilon$ . Therefore, we have $\lim_{x\to\infty}F(x) = 1$ . A similar argument shows that $\lim_{x\to-\infty}F(x) = 0$ . Hence, we have $F\in \mathcal{F}$ is actually a distribution function. It remains to show that the entire sequence $\mathbb{F}_n$ converges weakly to the $\mathbb{F}$ (given by the distribution function $F$ ) above. By Corollary to Helly's selection theorem, it suffices to show, for arbitrary $\{F_{n_j}\}_{j=1}^\infty$ subsequence that converges weakly to some $G\in \mathcal{G}$ , we have $F=G$ . First, notice that the above argument ensures the limit function is actually a distribution function, so we actually have $G\in\mathcal{F}$ . Let $\mathbb{F}$ and $\mathbb{G}$ denote the probability measures induced by distribution functions $F$ and $G$ , respectively. Since $F_{n_j}$ converges weakly to $G \in\mathcal{F}$ , we have, for all $f\in \mathcal{L}$ , $$\lim_j \int f\ \mathrm{d}\mathbb{F}_{n_j} = \int f\ \mathrm{d}\mathbb{G}.$$ Further, the weak convergence of our original sequence $\{\mathbb{F}_{n_k}\}_{k=1}^\infty$ to $\mathbb{F}$ gives us that, for all $f\in \mathcal{L}$ , $$\lim_k \int f\ \mathrm{d}\mathbb{F}_{n_k} = \int f\ \mathrm{d}\mathbb{F}.$$ Since $\lim_n \int f\ \mathrm{d}\mathbb{F}_n$ exists for all $f\in \mathcal{L}$ , all its subsequences must converge to the same limit, which gives us $$\int f\ \mathrm{d}\mathbb{F} = \lim_k \int f\ \mathrm{d}\mathbb{F}_{n_k} = \lim_j \int f\ \mathrm{d}\mathbb{F}_{n_j} = \int f\ \mathrm{d}\mathbb{G}$$ As the above equality holds for all $f\in \mathcal{L}$ , a distribution determining class, we have $\mathbb{F} = \mathbb{F}'$ and thus $F=F'$ , as desired. However, I am having trouble proving the following corollary (I copied and paste straight from the text): I am having trouble showing that (2) is sufficient. The argument they gave appears to be down the lines of the following (just basing off the proof of the previous theorem) Consider the sequence $F_n$ , by Helly's selection theorem, we have $F_{n_k}$ converges weakly to some $G\in \mathcal{G}$ . Further, we have, for each of those subsequences that converge weakly, $$\lim_{k\to\infty}\int f\ \mathrm{d}F_{n_k} = \int f\ \mathrm{d}F.$$ Now, it would be nice if we also write (I think the author just assumes this, but this does not appear to be true for me) $$\lim_{k\to\infty}\int f\ \mathrm{d}F_{n_k} = \int f\ \mathrm{d}G$$ If that's true, then we're done because that would imply $G=F$ and every subsequence that converges in $F_n$ converges to the same distribution function. But we cannot do that , because $G\in \mathcal{G}$ and we know that weak convergence (as defined by pointwise convergence to all points of continuity in the limit function) is no longer equivalent to convergence in integrals. TLDR: I have having trouble proving (2) is sufficient in the above corollary and the hint the author gave is confusing.","['measure-theory', 'weak-convergence', 'cumulative-distribution-functions', 'real-analysis', 'probability-theory']"
4076778,Nullspace of a block matrix,"Suppose I have $M \in \mathbb{R}^{2n\times 2n}$ such that $$M = \begin{bmatrix} A &B\\0  & C\end{bmatrix}$$ for some $A,B,C \in \mathbb{R}^{n \times n}$ . I wish to: (i) Find the nullspace of $M$ (ii) Prove that rank $(M) \geq \text{rank}(A) + \text{rank}(C)$ For (i) it is clear that for some vector $[x,  y]^T$ will be in the null space if and only if $y \in \text{null}(C)$ and that $Ax + By = 0$ , but I am unsure how to proceed from here. For (ii) I can see why this is true, but am not sure how to approach proving it. Thanks.","['matrices', 'matrix-rank', 'linear-algebra', 'block-matrices']"
4076794,A rigorous proof that $\nabla \cdot E = \frac{\rho}{\epsilon_0}$,"Suppose that $\rho: \mathbb R^3 \to \mathbb R$ is a function that tells us the electric charge density at each point in space. According to Coulomb's law, the electric field at a point $x \in \mathbb R^3$ is $$
E(x) = \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \frac{x - y}{\|x - y \|^3} \, dy.
$$ Question: How do you prove rigorously that, for the function $E$ defined above, we have $$
\tag{1} \nabla \cdot E = \frac{\rho}{\epsilon_0} .
$$ Comments: It's not easy to find a rigorous proof of this because physics textbooks tend to give non-rigorous arguments, whereas math books tend not to discuss this at all because it's about physics. Here's a typical physics derivation of this fact: \begin{align}
\nabla \cdot E(x) &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \, \nabla \cdot \left(\frac{x - y}{\|x - y \|^3} \right) \, dy \\
&= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \, 4 \pi \,\delta(x - y) \, dy \\
&= \frac{\rho(x)}{\epsilon_0}.
\end{align} This non-rigorous argument is based on the assertion that the divergence of the function $x \mapsto \frac{x}{\|x\|^3}$ is $4 \pi \delta$ , where $\delta$ is the delta function on $\mathbb R^3$ . That claim is typically justified by non-rigorous arguments. Bonus question: Where would I look to find a rigorous proof of this or similar physics equations? Is there a book on electromagnetism written for mathematicians? A similar question was asked on the physics stackexchange, but it hasn't received a good answer. Since the question is about providing a rigorous proof I think it's a better fit for math.stackexchange. Edit: I think a key insight I was missing is that proving (1) is equivalent to showing that $$
\tag{2} \text{if} \quad \varphi(x) = -\frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \| x - y \|^{-1} \, dy \quad \text{then}\quad  
\nabla^2 \varphi = \frac{\rho}{\epsilon_0}.
$$ The function $-\varphi$ is called the electric potential in electrostatics. This fact (2) is fundamental to the study of the Laplace equation and Poisson's equation, and it is of course proved rigorously in PDEs textbooks such as Evans. So, it is not that there is a gap in the mathematical literature -- rather, if you want to find a rigorous proof of (1), you just need to recognize that it's equivalent to (2). Why does (2) imply (1)? First notice that if $y \in \mathbb R^3$ and $h(x) = -\|x - y \|^{-1}$ then $\nabla h(x) = \frac{x - y}{\|x - y \|^3}$ for all $x \in \mathbb R^3, x \neq y$ .
It follows that \begin{align}
\tag{3} \nabla \varphi(x) &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \nabla \left( -\| x - y \|^{-1} \right) \, dy \\
&= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \frac{x - y}{\|x - y \|^3} \, dy \\
&= E(x).
\end{align} So, $$
\nabla^2 \varphi(x) = \nabla \cdot \nabla \varphi(x) = \nabla \cdot E(x).
$$ Thus, if we can show that $\nabla^2 \varphi = \frac{\rho}{\epsilon_0}$ , then we will have shown that $\nabla \cdot E = \frac{\rho}{\epsilon_0}$ . By the way, it is still not clear to me how to prove (3) rigorously. I think we need to use the dominated convergence theorem or one of the theorems from measure theory which imply that, under certain conditions, the limit of the integral is the integral of the limit.","['electromagnetism', 'real-analysis']"
4076836,Packing density of 2143 and crossing number of complete graph,"I've observed a numerical coincidence, and I'd like to know if there's something deeper here, or if I'm just observing a numerical coincidence due to Richard Guy's Strong Law of Small Numbers . I've been guilty of that before . I've been looking at permutations $\sigma \in S_n$ and computing the greatest number of occurrences of the pattern $\sigma$ over all permutations in $S_m$ . (This is only interesting when $m > n$ .) I asked about one aspect of this on MSE a few days ago with my question Greatest number of occurrences of the pattern 4213 in a permutation . In an answer, Misha Lavrov pointed me at a 2002 paper called On Packing Densities of Permutations , which addresses this idea. The Observation Using code from Code Golf Stack Exchange Users Noodle9 and Arnauld, it appears that the greatest number of occurrences of the pattern 2143 over all permutations in $S_n$ is equal to $A000241(n+1)$ , the crossing number of complete graph with $n+1$ nodes‚Äîthat is, the fewest number of edge-intersections of the complete graph $K_{n+1}$ when drawn in the plane. This is true for all known crossing numbers of $K_n$ : $A000241(1)$ through $A000241(12)$ . The largest crossing number, $A000241(12)$ , was computed back in 2007, and if bigger values have been computed since, I don't know of it. This means that without computing more crossing numbers (which is hard, presumably), you can't prove that these are unequal by a numerical comparison. Question I suspect that these numbers are eventually different‚Äîalthough permutation patterns show up in surprising places. Is there some correspondence which shows that these values are equal in general? Or can you use some asymptotic argument (or other argument) to show that they're different eventually?","['permutations', 'graph-theory', 'oeis', 'combinatorics', 'discrete-optimization']"
4076855,"$I_n=\int_0^1 \frac{x^n}{\sqrt{1+x^2}}$, find $nI_n$ [duplicate]","This question already has answers here : Define $I_n=\int_0^1\frac{x^n}{\sqrt{x^2+1}}dx$ for every $n\in\mathbb{N}$. Prove that $\lim_{n\to\infty}nI_n=\frac{1}{\sqrt 2}$. (5 answers) limit of integral $n\int_{0}^{1} x^n f(x) \text{d}x$ as $n\rightarrow \infty$ (7 answers) Closed 3 years ago . Let $$I_n=\int_0^1 \frac{x^n}{\sqrt{1+x^2}} dx$$ for $n\in \mathbb{N}$ Find $$\lim_{n\to \infty} nI_n$$ My approach: Let me explain my observations briefly.
Note that $I_{n-1}-I_n=\int_0^1 \frac{x^{n-1}(1-x)}{\sqrt{1+x^2}} dx \ge 0$ Hence $I_n$ is decreasing.Also $I_n\ge 0$ .So, $I_n$ is convergent. Let $\lim_{n\to \infty} I_n =l$ Now $I_n+I_{n-2} =\frac{\sqrt{2}}{n-1} -\frac{2}{n-1} I_n$ (By parts +little bit calculation) Applying limit to the above equation we get, $l=0$ . Also from the above equation we have, $(n-1)I_n+(n-1)I_{n-2}=\sqrt{2}-2I_n$ .
If $J_n := nI_n$ , then $\frac{n-1}{n} J_n +\frac{n-1}{n-2} J_{n-2}=\sqrt{2}-2I_n$ Yes I am only now one step away from getting $\lim_{n\to \infty} J_n=\frac{1}{\sqrt{2}}$ But I am unable to prove that $J_n$ is convergent
üôÅ
Any idea??","['integration', 'calculus', 'sequences-and-series', 'real-analysis']"
4076879,Chromatic polynomial of the cross-polytope and denominators of convergents to e.,"Let $C_n$ denote the $1$ -skeleton of the $n$ -dimensional cross-polytope, and $\chi_{C_n}(x)$ be the chromatic polynomial of $C_n$ . This is equivalent to the way of coloring the $(n-1)$ -dimensional faces of the $n$ -dimensional hypercube with $x$ colors so that if two of them share a $(n-2)$ -dimensional face, they have different colors. Then $\chi_{C_1}(x) = x^2,$ $\chi_{C_2}(x) = x(x-1)(x^2-3x+3),$ $\chi_{C_3}(x) = x(x-1)(x-2)(x^3 - 9x^2 + 29x - 32),$ $\chi_{C_4}(x) = x(x-1)(x-2)(x-3)(x^4 - 18 x^3 + 125 x^2 - 392 x + 465),$ $\chi_{C_5}(x) = x(x - 1) (x - 2)(x - 3) (x - 4)(x^5 - 30 x^4 + 365 x^3 - 
   2240 x^2 + 6909 x - 8544),$ and more generally, $\displaystyle \chi_{C_n}(x) = \left(\prod_{i=0}^{n-1}(x-i)\right)f_n(x)$ . It appears that the absolute value of the constant term of $f_n(x)$ is equal to OEIS sequence $A007677(3n - 4)$ for $n \geq 2$ , where A007677 is a list of ""denominators of convergents to $e$ "". Is this a coincidence, or is there a connection between hypercubes and convergents of $e$ ? I'm surprised by this. Should I be?","['polytopes', 'coloring', 'diophantine-approximation', 'oeis', 'combinatorics']"
4076886,Sum of Kloosterman sums bound by $O(n^{\epsilon})$,"I've been studying the book ""Topics in Classical Automorphic forms"" by Henryk Iwaniec and I am stuck at a bound.
For even $k$ and any $3/4 < \sigma < 1$ the book claims the following for the Kloosterman sum $S(n,n,c)$ where $\epsilon>0 $ is fixed, as $n$ gets large. (page 73) $$\sum_{c>0} c^{-2\sigma}|S(n,n,c)| \ll n^{\epsilon} $$ I've been trying to prove this using the Weyl bound $|S(m,n,c)| \le \gcd(m,n,c)^{1/2} c^{1/2}\tau(c)$ but I haven't managed to do so. Maybe I am missing something basic but in order to create a convergent series the bounds for the $\tau$ function seems to exceed any hopes I have for that.","['analytic-number-theory', 'number-theory', 'modular-forms']"
4076923,When is the standard deviation of the squares of a random number greater than the square of the standard deviation of those numbers?,"I was playing around with the uncertainty of the kinetic energy operator in quantum mechanics, and really desired to have the following inequality be true: $$\Delta K=\frac{1}{2m}\Delta (p^2)\geq\frac{1}{2m}(\Delta p)^2\geq\frac{\hbar^2}{8m(\Delta x)^2},$$ where $\Delta Q$ is the uncertainty (standard deviation) in variable $Q$ and the last step follows from the Heisenberg uncertainty principle. The problem is, $\Delta(Q^2)\geq(\Delta Q)^2$ turns out not to be true in general. I ran some sample distributions in Mathematica, and while this inequality is true most of the time, there are exceptions, though in those cases the difference is generally slight. Is there a condition we can apply (distribution or allowed range of $Q$ ) that will enforce this inequality? If not, ""how often"" does the inequality fail? Expanding in terms of expectation values (after squaring to get a variance on the left and not have square roots) doesn't seem to help: $$\langle Q^4\rangle\geq 2\langle Q^2\rangle(\langle Q^2\rangle-\langle Q\rangle^2)+\langle Q\rangle^4,$$ even though we know $\langle Q^4\rangle\geq \langle Q^2\rangle^2\geq \langle Q\rangle^4$ .","['statistics', 'variance', 'standard-deviation', 'probability-distributions', 'expected-value']"
4076928,"True or False? If the limit of $[f(x)-g(x)]$ as x approaches $a = 0$, then the limit of $f(x), g(x)$, as $x$ approaches $a$, are equal.","TRUE OF FALSE? $$\lim_{x\to\alpha}[f(x)-g(x)]=0\implies\lim_{x\to\alpha}f(x)=\lim_{x\to\alpha} g(x)$$ I'm in senior high school and just recently took a test with this as one of the questions. I initially answered true ‚Äî I mean, it obviously looks too true. Simple distribution and manipulation of terms would easily show $\lim_{x\to\alpha}f(x)=\lim_{x\to\alpha}g(x)$ ‚Äî but just before submitting, I changed it to false. I got a perfect score, but I do feel uneasy not knowing why my teacher remarked this as false. Just for context, maybe the counterexample to make this statement false might involve infinities.  When the limit is $+\infty$ or $-\infty$ , we simply write is as ""does not exist.""
Also we only tackle real number. And our limit definition specifies that $\lim_{x\to\alpha}f(x)$ only exists when $\lim_{x\to\alpha^{+}}f(x)=\lim_{x\to\alpha^{-}}f(x)$ . Qn: The actual question looked like this Here. Link","['limits', 'calculus']"
4076936,"Pullback, 2-form invariant","Let $\mathbb H^2 = \{(x, y) \in \mathbb{R}^2 : y> 0\}$ and consider the $2$ -form in $\mathbb H^2$ defined by $$\varphi = \dfrac{dx \wedge dy}{y^2}.$$ Show that $\varphi$ is invariant $(T^*(\varphi)= \varphi)$ under the transformation $T$ from $\mathbb H^2$ to $\mathbb H^2$ given by $$T(z) = \dfrac{az + b}{cz + d}$$ where $z \in \mathbb H^2 \subset \mathbb{C}$ and $a, b, c, d \in \mathbb{R}$ with $ad - bc \neq 0$ . My attempt was to try to express $T(x, y)$ and calculate $dx $ and $ dy $ to make $ dx \wedge dy $ , and calculate the pullback. I couldn't, the expression for $T$ was strange.",['differential-geometry']
4077003,Mathematical significance of upside deviation bigger than downside deviation,"I have been searching around for an explanation, but could not find any. What is the mathematical significance of a sample set's upside deviation (standard deviation of values larger than the mean) being bigger than the downside deviation? Intuitively I feel that skewness might have to do with it, but I am not so sure.","['statistical-inference', 'statistics', 'probability-distributions', 'probability']"
4077034,Is the Hellinger distance invariant under the choice of the dominating measure?,"Let $\mu_1,\mu_2$ be two probability measures on $(\Omega,\Sigma)$ , and $\nu$ be a $\sigma$ -finite measure on $(\Omega,\Sigma)$ , such that $\mu_1,\mu_2$ are absolutely continuous with respect to $\nu$ . By the Radon-Nikodym Theorem, there exist measurable functions $\frac{d\mu_1}{d\nu},\frac{d\mu_2}{d\nu}$ mapping $\Omega\rightarrow[0,\infty)$ , such that for all $A\in \Sigma$ , $$
\int_A \frac{d\mu_1}{d\nu} d\nu = \int_A d\mu_1,\quad \int_A \frac{d\mu_2}{d\nu} d\nu = \int_A d\mu_2
$$ In a textbook I'm studying there is an exercise which asks to prove that for any two $\sigma$ -finite measures $\nu_1,\nu_2$ , with respect to which $\mu_1,\mu_2$ are absolutely continuous, the following holds: $$
\int\left(\sqrt{\frac{d\mu_1}{d\nu_1}}-\sqrt{\frac{d\mu_2}{d\nu_1}}\right)^2d\nu_1=\int\left(\sqrt{\frac{d\mu_1}{d\nu_2}}-\sqrt{\frac{d\mu_2}{d\nu_2}}\right)^2d\nu_2
$$ In other words, the Hellinger distance is invariant under changes of the dominating measure.
A straightforward step towards showing this would be to expand the two expressions.
After some cancelling, the statment reads: $$
\int \sqrt{\frac{d\mu_1}{d\nu_1}\frac{d\mu_2}{d\nu_1}}d\nu_1=\int \sqrt{\frac{d\mu_1}{d\nu_2}\frac{d\mu_2}{d\nu_2}}d\nu_2
$$ This is where I get stuck.
Symbolically, it seems to make sense, as one might expect something like: $$
\sqrt{\frac{d\mu_1}{d\nu_1}\frac{d\mu_2}{d\nu_1}}d\nu_1=\sqrt{d\mu_1d\mu_2}=\sqrt{\frac{d\mu_1}{d\nu_2}\frac{d\mu_2}{d\nu_2}}d\nu_2
$$ But why is this true? Is it even generally true? How can it be shown?","['measure-theory', 'radon-nikodym', 'probability-theory', 'real-analysis']"
4077038,Is there something wrong with this explanation of Chebyshev's Inequality?,"So I was revising (more like re-learning coz I suck) Chebyshev's Inequality using this document . Specifically, I am referring to Example $7$ on Page $3$ , which I shall reproduce below. Question A coin is weighted so that the probability of landing on heads is $0.2$ . Suppose the coin is flipped $20$ times. Using Chebyshev's Inequality, find a bound for the probability it lands on heads at least $16$ times. Answer \begin{align}
P(X \geq 16) & =
P(0 \leq X \leq 16)
\\[5 mm] & =
P(-8 \leq X \leq 16)
\\[5 mm] & =
P(|X - 4| \geq 12)
\\[5 mm] & \leq
\frac {Var(X)} {12^2}
\\[5 mm] & =
\frac {(20)(0.2)(0.8)} {144}
\\[5 mm] & =
\frac 1 {45}
\end{align} I understand if we skip the first jump from $P(X \geq 16)$ to $P(|X - 4| \geq 12)$ because, as suggested in the document, $X \geq 0$ . However, what I cannot get is how the first equality makes sense. In particular, how does $P(X \geq 16)$ equal $P(0 \leq X \leq 16)$ ? Also, how does $P(-8 \leq X \leq 16)$ equal $P(|X - 4| \geq 12)$ ? I know the answer is correct but I cannot help but feel that the proof is not entirely mathematically right. Anyone who can explain why the first $3$ equalities make sense please do tell.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4077089,Module of differentials of an extension of Dedekind domains is cyclic,"Let ùê¥ be a Dedekind domain with fraction field ùêæ, ùêø|ùêæ a finite separable field extension and ùêµ the integral closure of ùê¥ in ùêø. Assume that all the residue field extensions are separable. In the proof of Serre's ""Local Fields"", chapter III, Proposition 14 (page 59), it is stated that in order to show that the module of differentials $Œ©_{ùêµ|ùê¥}$ is cyclic we can assume that ùê¥ is local and complete. (See this question. ) I understand the reduction to the local case, but I don't understand the reduction to the complete case. I tried to use that the canonical map from $A_p$ to its $p$ -adic completion is faithfully flat but I didn't arrive to the desired conclusion. In particular, I don't understand why, if $Œ©_{ùêµ'/ùê¥'}$ is cyclic with with annihilator the different ideal of B' over A' [where A' and B' denote the $p$ -adic completion of the DVRs A and B respectively], then $Œ©_{ùêµ/ùê¥}$ is cyclic with annihilator the different ideal of B over A. Can you give me some details? Thank you very much.","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'dedekind-domain']"
4077095,Intuition behind Definition of Nilpotent Groups,"Definition: A group $G$ is called nilpotent if there exists a chain of subgroups $N_0, N_1,\ldots, N_k$ such that $$\{e\} = N_0 \le N_1 \le N_2 \le ... \le N_k = G$$ and for $0\le i\le k-1$ , $N_i \vartriangleleft G$ , i.e. $N_i$ is normal in $G$ , $N_{i+1}/N_i \subset Z(G/N_i)$ where $Z(G)$ denotes the center of some group $G$ , and $e$ is the identity of $G$ . What motivated this definition? What is so special about $N_{i+1}/N_i \subset Z(G/N_i)$ ? It feels so random and out of the blue!","['nilpotent-groups', 'group-theory', 'abstract-algebra', 'intuition']"
4077097,Evaluating $ \int_{-\infty}^{\infty} \cosh(x+s)^{-2}\cosh(x)^{-2}dx$,"So I have this function: $$ \int_{-\infty}^{\infty} \cosh(x+s)^{-2}\cosh(x)^{-2}dx$$ And when I try to integrate it, I can obtain $0$ . And, when I evaluate the limits, it also cancels to $0$ . The solution I was given stated that the answer should be some form of: $$\frac{\cosh(s)\cdot s}{\sinh(s)^3}- \frac{1}{\sinh(s)^2} $$ None of what I'm doing seems to get me the answer and as you can see, it's not like Mathematica even makes the output easy to parse. This is my Mathematica expression: In[42]:= Integrate[Cosh[x + s]^-2*Cosh[x]^-2, x]

Out[42]= -2 Coth[s] Csch[s]^2 Log[Cosh[x]]+2Coth[s] Csch[s]^2 Log[Cosh[s + x]]-Csch[s]^2 Sech[s] Sech[s+x] Sinh[x]-Csch[s]^2Tanh[x]","['integration', 'trigonometry', 'improper-integrals', 'trigonometric-integrals']"
4077113,Derivative of $\sin(x)$: how to avoid circular reasoning?,"Dad here, helping his kid out with math. I get it that to determine the derivative of $\sin(x)$ you do: $$\frac{\sin(x+h) - \sin(x)}h = \frac{\cos(x)\sin(h)}h \to \cos(x)$$ However I assume that $\sin(h) = h$ for $h\to 0$ . This correct because the ""angle"" (not sure what the correct word is in English) of $\sin(x) $ is $1$ for $x=0$ . But this is what I'm trying to prove in the first place. So a circular reference. How to solve this?",['trigonometry']
4077127,How many numbers are with $5$ different digits in decreasing order?,"I mean that $54321$ is a valid number, but $16755$ is not. I was thinking that there are $6^5$ possible numbers because there are 6 possibilities for each position in the number, but I don't know exaclty.","['combinatorics', 'discrete-mathematics']"
4077156,The integral of $\mathrm{e}^{-\frac{1}{\sin^2 x}}$,"I have tried playing around an integral $$
\int \mathrm{e}^{-\frac{1}{\sin^2 x}} \,\mathrm{d}x
.
$$ I know this integral does not have a closed form (Wolfram Alpha and many more resources!), but I appreciate an approximation that holds as close as possible to this integral. The Taylor series is not accurate. It can be assumed that the bounds of the integral are $0$ and $x$ , where $0<x<\frac{\pi}{2}$ . Appendix Thanks to the @robjohn's useful answer, I have simulated his three approaches (in the original order presented) for $0<x<\frac{\pi}{2}$ in the following figure: The blue solid line denotes the integral which should be approximated and the other curves are three closed forms for that. Unfortunately, there is a bad divergence between the approximations and the integral after about $x=0.9$ .","['integration', 'improper-integrals', 'analysis', 'real-analysis']"
4077158,Change of variables between different dimensions,"I was reading my statistical mechanics notes, and upon reading $$
\int\cdots\int_{\mathbb{R}^3} H(q,p)\space\mathrm{e}^{-\beta H(q,p)}\space\mathrm{d}^{3n}p\space\mathrm{d}^{3n}q = \int_0^{\infty} E \space \mathrm{e}^{-\beta E} \space \Omega(E) \space\mathrm{d}E
$$ with $(q,p) = (q^1,\dots,q^{3n},p_1,\dots,p_{3n})$ , of course, and $$
\Omega(E) = \frac{\mathrm{d}}{\mathrm{d} E}\int\cdots\int_{H(q,p)\leq E} \mathrm{d}^{3n}p\space\mathrm{d}^{3n}q
$$ and I realised I don't know how to make this formal. This looks like a change of variables, but the first integral is ${6n}$ -dimensional and the second one is just $1$ -dimensional. So, I tried with the simpler example of a function $f(u)$ and another function $g(x,y)$ , and tried to find which $\Omega(z)$ do I need for $$
\iint_{(x_0,y_0)}^{(x_1,y_1)} f(g(x,y))\space\mathrm{d}x\mathrm{d}y = \int_{g(x_0,y_0)}^{g(x_1,y_1)} f(u)\space\Omega(u)\space\mathrm{d}u.
$$ Now, as $$
\mathrm{d}u = \frac{\partial g}{\partial x}\mathrm{d}x+\frac{\partial g}{\partial y}\mathrm{d}y,
$$ I thought maybe I could do $$
\mathrm{d}x = \left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}u - \left(\frac{\partial g}{\partial x}\right)^{-1}\frac{\partial g}{\partial y}\mathrm{d}y
$$ and, assuming $\mathrm{d}x\mathrm{d}y$ is actually $\mathrm{d}x\wedge\mathrm{d}y$ , I could substitute this $\mathrm{dx}$ and find $$
\mathrm{d}x\mathrm{d}y = \left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}u\mathrm{d}y.
$$ After substituting this new measure inside the integral of $f(g(x,y))$ and manipulating a little, it is found that $$
\Omega(u) \equiv \int_{y_1}^{y_2}\left(\frac{\partial g}{\partial x}\right)^{-1}\mathrm{d}y \equiv \int_{x_1}^{x_2}\left(\frac{\partial g}{\partial y}\right)^{-1}\mathrm{d}x,
$$ but I don't see how these two integrals are equal to $$
\frac{\mathrm{d}}{\mathrm{d} u}\iint_{g(x,y)\leq u}\mathrm{d}x\mathrm{d}y,
$$ I'm a physicist and this requires too much rigour. How should I prove this?","['multivariable-calculus', 'change-of-variable']"
4077213,Intuition behind augmented filtration,"Consider an probability space $(\Omega, A,P)$ and the augmented filtration $F(t)= \bigcap_{s>t} \sigma(F^W(t) \cup N),$ where $F^W$ is the natural filtration of an Browian motion $W$ and $N$ the nullsets of $A$ . Why do I need to add in these nullsets in order to make $W$ adapted? What is the intuition behind this?","['brownian-motion', 'probability-theory', 'filtrations']"
4077314,"How many five digit numbers can be formed using digits $1,1,2,3,3,4,4$","With digits $1,1,2,3,3,4,4$ , how many five digit numbers we can form? $1)\frac34\times5!\qquad\qquad2)\frac94\times5!\qquad\qquad3)4\times5!\qquad\qquad4)\frac52\times6!$ Ok so the digits $1,3,4$ appears twice and $2$ appears once. I tried to count different cases: first: having all $1,2,3,4$ digits and choose another digit from $1,3,4$ : $${3\choose1}\times\frac{5!}{2!}$$ second: having two pair of same digit and choose another digit: $${3\choose2}\times2\times\frac{5!}{2!2!}$$ Summing them we have $3\times\dfrac{5!}{2}+3\times\dfrac{5!}{2}=5!\times3$ but I don't have this in the options.","['combinatorics', 'discrete-mathematics']"
4077325,Is there a difference between rejecting a null hypothesis and not accepting a null hypothesis?,"I understand that we never say that the null hypothesis is accepted, instead we say that the null hypothesis is not rejected since we can never prove an effect does not exist through empirical evidence. But does the opposite hold? Is there a difference between rejecting the null hypothesis and not accepting the null hypothesis? In the same sense, how can we prove that an effect does exist? Surely we can only show that there is a low probability, say at 1% significance level, that an effect does not exist and thus we can only not accept it?","['statistical-inference', 'statistics', 'hypothesis-testing']"
4077412,Mean Value Theorem related problem,"Let $f(x)\leq g(x)$ for all $x \in I$ , where $I$ is an interval $\subseteq$ R. Also, let $f(c) = g(c)$ for some $c \in I$ but not an endpoint.
Prove that $f'(c) = g'(c)$ (assume differentiablity) I have tried the Mean Value Theorem, let I = [a,b]. So for $f'(c) = g'(c)$ I will need to prove that $f(a) - f(b) = g(a) - g(b)$ , but I am stuck at this. I have also tried the definition of derivative but I am still unable to go ahead. Please give me some hints on this. Thank you!",['analysis']
4077423,combinatorial argument that $ [\binom{n}{0}+\binom{n}{1}+\dots+\binom{n}{n}]^{2} = \sum_{k=0}^{2n}\binom{2n}{k}$,"Give a combinatorial argument with double counting showing that $$ \Bigg[\binom{n}{0}+\binom{n}{1}+\dots+\binom{n}{n}\Bigg]^{2} = \sum_{k=0}^{2n}\binom{2n}{k}$$ I am unsure on how to approach this problem from a combinatorial argument perspective. I have found an analytical proof using the binomial theorem, but can't formulate an explanation in the former way. From my understanding, $\displaystyle \sum_{k=0}^{n} {n \choose k}$ represents the number of ways we can chose a set of $k$ objects from a group of $n$ objects, for $k \le n$ . So this quantity squared should be equivalent the number of ways to pick a set of $k$ objects from a group of $2n$ objects. Could anyone provide me any hints on how to approach this problem combinatorically ?","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4077430,Rewrite statement using symbols,"I have a statement: Every real number has a unique multiplicative inverse. My answer is ( $\forall x \in \mathbb{R}$ )( $\exists! y \in \mathbb{R}$ )( $xy = 1$ ). Is this correct? Thank you! Please do not care about the truth value of the statement, I know it is wrong because x must be nonzero.
I was not sure about the unique part.","['elementary-set-theory', 'solution-verification']"
4077445,Approximation of a function with this condition,I'm wondering if I have the expression $$f = y(ln(x) +1)$$ and I'm given $x \gg 1$ . Is it valid to approximate $$f \approx yln(x)$$,"['calculus', 'functions', 'approximation', 'real-analysis']"
4077455,Infinite sum - alternatives to residue theorem?,I'm looking at proving the following sum: $$\sum_{k=-\infty}^{\infty}\frac{1}{64k^4+1}=\frac{\pi}{4}\frac{1+\mathrm{sinh}(\pi/2)}{\mathrm{cosh}(\pi/2)}$$ I know this can be done by finding the residues and taking the sum of their negatives. Curious if anyone knows how to prove it in another way - maybe with Fourier transform? I thought about Parseval's theorem except the denominator isn't a perfect square. Thanks,"['summation', 'fourier-analysis', 'complex-analysis', 'sequences-and-series', 'residue-calculus']"
4077513,"Given $E[Y]=1$ , $E[Y^2]=2$ and $E[Y^3]=5$, Y non negative integer random variable, find min of $P[Y=0]$","If $Y$ is a non negative integer-valued random variable with $E[Y]=1$ , $E[Y^2]=2$ and $E[Y^3]=5$ ,where $E$ is the average value of $Y$ .
Find the minimum value of the possibility $P[Y=0]$ . I know that $E[Y]=\sum^{n}_{i=1}y_{i}p(y_{i})$ and $E[Y]=\sum_{y:p(y)>0}yp(y)$ . I know that Markov's inequality is : $P[Y\geq a]
\leq\frac{E[Y]}{a}$ I know that  Chebyshev's inequality is : $P(|Y-E[Y]|\geq k)\leq \frac{Var(Y)}{k^2}$ The book I'm studying is the : A first course in probability 8th edition , Sheldon Ross",['probability']
4077559,"When running a t-test with a Control and Experimental group, are both groups Samples? Or is the Control group a Population?","When running a t-test with a Control and Experimental group, are both groups Samples? Or is the Control group a Population? I am pretty confused on this, since I've pretty much only ever worked with Samples, but have no idea when to switch to a population",['statistics']
4077602,{Proof Check} Let $x_n$ be a convergent sequence. if $x_n \geq 0$ $\forall n \in \mathbb{N}$ then $\lim x_n \geq 0$,"Please check and point out if there are logical flaws in my argument. I'm learning analysis but I'm not that confident yet. Proof: Suppose $x_n \geq 0 $ $\forall n \in \mathbb{N}$ and let $\lim x_n = x < 0$ . Then $|x| > 0$ . Take $\varepsilon = |x|$ then $\exists N \in \mathbb{N}$ s.t. $\forall n \geq N$ , $$|x_n - x| \leq |x_n| + |x| < \varepsilon = |x|$$ $$0 \leq |x_n| = x_n < |x| - |x| = 0$$ This is a contradiction, thus $x \geq 0$ .","['limits', 'real-analysis']"
4077623,"Prob. 15, Chap. 2, in Royden's REAL ANALYSIS: For any set of positive measure and $\epsilon > 0$, there are finitely many disjoint measurable sets ...","Here is Prob. 15, Chap. 2, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: Show that if $E$ has finite measure and $\epsilon > 0$ , then $E$ is the disjoint union of a finite number of measurable sets, each of which has measure at most $\epsilon$ . Here $E \subset \mathbb{R}$ of course, and $m^*(E) < \infty$ , where $m^*(E)$ is the infimum of the the set of all the sums of the form $\sum_{k = 1}^\infty l \left( I_k \right)$ , where $\left\{ I_k \right\}_{k=1}^\infty$ is a countable collection of non-empty, bounded open intervals which cover $E$ and, for each $k$ , $l \left( I_k \right)$ denotes the length of the interval $I_k$ . How to proceed from here?","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
4077660,Fudamental group of the complement of a 2 Link,"I am trying to do the following exercise and I wanted to make sure if my thought process is correct : Let $L$ be the $2$ -component link consisting of two unknotted circles $C\cup D$ in which $C$ wraps around $D$ $k$ times. Where we can think of $C$ being in the torus and $D$ be the generator of the fundamental group of the solid torus.We wanna compute $\pi_1(\mathbb{R}^3-L)$ . So first we note that using Van-Kampen we have that $\pi_1(\mathbb{R}^3-L)=\pi_1(S^3-L)$ . Then we have that $S^3=\partial (D^4)=S^1\times D_2 \cup D_2\times S_1$ . Let's call $A=S^1\times D^2$ and $B=D^2\times S_1$ where $B$ is the torus that contains the point at infinity. Now we will have that $C\subset A\cap B$ , $L\subset A$ and $C\subset B$ . So that $A-L$ and $B-L$ are closed sets such that $A-L \cup B-L=S^3-L$ . Now we note that $(A\cap B-L ,A-L)$ and $(A\cap B-L,B-L)$ are cofibrations. And so we can apply the van-kampen theorem for closed subspaces. Now we have that $\pi_1(A\cap B-L)\cong \mathbb{Z}$ since $A\cap B-L$ is essentially an annulus, $\pi_1(A-L)\cong \mathbb{Z}\bigoplus \mathbb{Z}$ and $\pi_1(B-L)\cong \mathbb{Z}$ . Now we need to find out what are the induced maps. We have that $\pi_1(A\cap B-L)\rightarrow \pi_1(A-L)$ is the map that sends $1$ to $(1,k)$ and that the map $\pi_1(A\cap B-L)\rightarrow \pi_1(B-L)$ is the map that sends $1$ to $k$ . And so by the Seifert‚ÄìVan Kampen theorem we get that $\pi_1(S^3-L)$ will be the group $\langle x,y,z:x^k=zy^k, zy=yz\rangle$ . What do you guys think ?
Any insight is appreciated. Thanks in advance.","['general-topology', 'fundamental-groups', 'algebraic-topology']"
4077720,Generalized Regular Polygon Area Formula to Any Number of Sides,"I was looking for a way to check if the regular polygon area formula of $\frac{ns^2}{4} \cot \frac{œÄ}{n}$ where s is the side length and n is the number of sides could be extended to fractional  and irrational n. Setting s to be 1, we get the constant in exact form of the area of the polygon. This also gives the area of the unit polygon as well. I am assuming the area of the entire polygon not using the even-odd rule just like the Wolfram Alpha example. Let this area be called $A_n=\frac{n}{4}\cot\frac{œÄ}{n}$ Testing n= $\frac{5}{2}$ , we see that the interior angle is Œ∏= $\frac{4œÄ}{5}$ . This creates the pentagram polygon with Schl√§fli Symbol of $\lbrace\frac{5}{2}
\rbrace$ . When putting the formula into use, $A_\frac{5}{2}=\frac{5/2}{4}\cot\frac{œÄ}{5/2}$ = $\frac{\sqrt{5}}{8}\sqrt{5-2\sqrt{5}}$ , but when searching in Wolfram Mathworld, the constant came out to be A(unit pentagram or area constant of pentagram)= $\frac{\sqrt{10}}{4}\sqrt{65-29\sqrt{5}}$ . Similarly, the area of a pentagon can be used in the formula to give $A_5=\frac{5}{4}\cot\frac{œÄ}{5}$ = $\frac{\sqrt{5}}{4}\sqrt{5+2\sqrt{5}}$ , In addition, one interpretation of the number of sides can also be the perimeter as we are assuming the side length is $\frac{5}{2}$ meaning that each side has a length of $\frac{1}{2}$ or less which contradicts the assumption that we can only have one side. However, if we allow the side length to be one without any scaling of the sides as a result of the n-value, the formula still has the constant incorrect. Only when solving for the area of the $36^\circ-126^\circ-18^\circ$ triangle, do we get the value. After multiplying by $10$ , the number of triangles in the shape, we get the same value as in Wolfram Mathworld. This triangle is the triangle when the shape is radially divided into ten equal triangles. My question is what the restrictions are for the formula $A=A(n,s)=\frac{ns^2}{4}\cot \frac{œÄ}{n}$ where n is the number of sides and s is the side length. It definitely works for $n\in\Bbb Z$ an integer assuming negative values of n would correspond to negative areas. If $n=0$ , the formula is undefined as well as for any values $n= \frac{1}{m}$ where $m\in\Bbb Z$ and the formula equals $0$ when n= $\frac{2}{2m+1}$ where $m\in\Bbb Z$ . My second question is if there are restrictions for this equation, what would be a general formula only in terms of $n$ and $s$ defined in the above paragraph? This means $n,s\in\Bbb R$ . How about for $n,s\in\Bbb C$ ? How about $n$ and $s$ in the set of all matrices with complex entries $\Bbb M$ ? It seems like for a regular complex polygon, the area would be in the form of A=2n* $\frac{1}{2}$ b h=n b h=n c*h where 2n is the amount of dividing triangles there are in the polygon, $\frac{1}{2}$ b h is the area of these triangles, and c is the circumradius being equal to the base, denoted by b, of each triangle. This premise assumes the regular complex polygon is ‚Äúfilled‚Äù so that the ‚Äústar‚Äù outline was filled in completely without any self-intersections affecting the area definition. Going onto seedcode and setting the stator rotor (sr) to the number of sides and leaving only one rotor to be inside the stator rotor (sr) which is named rotor 1 (r1) to be the turning number will give a shape reminiscent of the shape with Schl√§fli of $$\left\{ \frac{p}{q}\right\}=\left\{ \frac{sr}{r1}\right\}=\left\{ \frac{\text{number  of sides}}{\text{turning number}}\right\}$$ Be sure to set the pen radius (r) in the app to be r1. In the app, r=r1. Thanks to @Blue for figuring out the formula for a ‚Äúfilled‚Äù star polygon in terms of the circumradius and Schl√§fli number $$\{N\}=\text{ratio}=\left\{\frac{p}{q}\right\}$$ . I also appreciate them for giving insight into the other convention. Going on the star polygon link below, gives the circumradius in terms of the ratio which then can be simplified to $\frac{1}{2} \csc\frac{œÄ}{N}$ . This can be intuitively and geometrically understood by imagining the cosecant operator in the diagram in their post. However, this assumes a unit length and therefore ‚Äúarea constant‚Äù. Interestingly enough, substituting this value into their formula for r and multiplying by $s^2$ for s being the side length simplified down to: $$\text{Area}\left(\text{‚Äúfilled‚Äù polygon with side length }s\text{ and Schl√§fli symbol }\{N\}=\left\{\frac a b\right\}\right)=A(a,b,s)=A_N (s)= \frac{a s^2\csc^2 \frac{œÄ}{N}}{4(\cot\frac{œÄ}{a}+\tan\frac{œÄ}{N})}$$ . Testing the pentagram we get $$A_{5/2} = \frac{\sqrt{10}}{4}\sqrt{65-29\sqrt{5}}=.31027‚Ä¶$$ after a few steps, the same value as in Wolfram Mathworld. Amazingly, when putting $$N=a=\frac a1 =\frac ab$$ we get $$A_N (s)=\frac {a s^2 }4 \cot \frac œÄa$$ which implies the cotangent formula assuming a ‚Äúfilled polygon‚Äù and b=1 can only have n as an integer and n not equal to 0. Even trying a pure imaginary number such as $\{N\}=\left\{\frac i2\right \} $ with side length $i$ experimentally gets: $$A_{\frac i2} (i)=\frac 14\tanh(\pi)\coth(2\pi)\text{csch}(2\pi)=\frac{e^{2\pi}(e^{4 \pi}+1)}{2(e^{2 \pi}+1)^3(e^{2 \pi}-1)}=.00093025‚Ä¶$$ after a few steps. This experimental example has the hyperbolic trig functions. New answers for complex $n,s$ are always welcomed . Please correct me and give me feedback! Here are some links that were used in the making of these questions: Pentegram Generalized Overturned Polygons Generalized Fractional Polygons Shl√§fli Symbol Star Polygon","['euclidean-geometry', 'analytic-geometry', 'geometry', 'trigonometry', 'algebra-precalculus']"
4077728,$f(ct)=cf(t)$ for all rational numbers $c$ and $f$ is continuous,"Given that $f(ct)=cf(t)$ for all rational numbers $c$ and that $f$ is continuous, it must be that $f$ is a straight line. Can the same conclusion can be drawn if $f(2t)=2f(t)$ and $f$ is continuous, i.e. the statement holds only for $c=2$ ? Clearly it cannot if $f$ is not continuous, as you could set $f(0)=0, f(1)=1, f(2)=2, f(4)=4, f(8)=8$ , and then for another ""sequence"" of numbers, set $f(0)=0, f(3)=1, f(6)=2$ , and you would end up with many dots that lie on lines. But I was wondering if the assertion that $f$ is continuous forces $f$ to be a straight line even when the condition is reduced from all rational numbers $c$ to just one number.","['limits', 'sequences-and-series', 'continuity', 'real-analysis']"
4077801,"Likelihood of circle vs. ellipse vs. parabola vs. hyperbola from $ax^2+bxy+cy^2+dx+ey+f=0$ with coefficients uniformly random in $[-1,1]$","Q . What is the probability that a random quadratic equation describes a
circle, an ellipse, a parabola, or a hyperbola?
Let's use this definition of a random quadratic: $$a\, x^2 + b\, x y + c\, y^2 + d\, x + e\, y + f = 0 \;$$ where $a,b,c,d,e,f$ are each uniformly random within $[-1,1]$ . For a circle the probability should be $0$ , but I am unclear on the likelihood of ellipse vs. parabola vs. hyperbola. Each conic can be represented as a point in a $5$ -dimensional projective space.
So I'm asking for the corresponding portions/volumes within this space. My trigger for this question is a quote from Colin Adams:
""So if we want to understand the geometry of surfaces, it's all about
the hyperbolic case."" So I was wondering if hyperbolas dominate even
in the plane. (Colin Adams, ""What is ... a Hyperbolic 3-Manifold?"" AMS 65 , no. 5, pp.544-546. PDF download ). Added : Simulations suggest that hyperbolas occur roughly 73% of the time,
ellipses 14%, and the remainder have only imaginary solutions.","['conic-sections', 'polynomials', 'probability']"
4077848,A linear combination of diffeomorphims still a diffeomorphism?,"Let $G_1, ..., G_s : \mathbb{R}^s \to \mathbb{R}^s$ be diffeomorphisms on an open unit ball $B \subset \mathbb{R}^s$ , centered at $0$ , onto their images. Let $m_1, .., m_s$ be integers not all $0$ , and let $F = m_1 G_1 + ... + m_s G_s$ .
Suppose we know that the determinant of the Jacobian matrix of $F$ at $x$ is non-zero for all $x \in B$ . Does it then follow that $F$ is still a diffeomorphism on $B$ ? If $F$ is not a diffeomorphism on $B$ , is it possible to show that $$
\# F^{-1}(y) < C
$$ for all $y$ in $F(B)$ for some positive $C$ ? Any comments are appreciated! Ps edits had been made based on comments","['diffeomorphism', 'jacobian', 'multivariable-calculus', 'linear-transformations', 'differential-geometry']"
4078955,Global sections and Fiber products $X \times_S \operatorname{Spec} k$,"Let $X $ a $S$ -scheme ( $S$ another scheme). Suppose $k$ is a field
and $S$ has a $k$ -valued point, that is a map $p: \operatorname{Spec} k \to S$ . If $X \times_S \operatorname{Spec} k$ denotes the associated fiber product, can we say something
interesting about global sections $H^0(X \times_S \operatorname{Spec} k, \mathcal{O}_{X \times_S \operatorname{Spec} k})$ if we know global sections of $X $ and $S$ ? Say for sake of simplicity that $S= \operatorname{Spec}(A)$ is also affine
and $k$ corresponds to some prime ideal of $A$ . Then the structure sheaf of $X \times_S \operatorname{Spec} k$ is $O_X \otimes_A k$ . Now if we come
back to global sections of $X \times_S \operatorname{Spec} k$ can we
pull out $k$ from $H^0(X \times_S \operatorname{Spec} k, \mathcal{O}_X \otimes_A k)$ and
reduce the calculation to global sections of $X$ ? Altoght the question seems to arise quite naturally it seems to require (at least if we deal with general fiber products $X \times_S Y$ ) advanced tools to find answers like in this discussion .
That's why I take here for $Y$ a field as the most simple choice with hope to find out if my question can be answered by methods from on 'non research' level. If the answer is negative, are there any sufficient conditions when $H^0(X \times_S \operatorname{Spec} k, \mathcal{O}_{X \times_S \operatorname{Spec} k})= H^0(X,\mathcal{O}_{X}) \otimes_A k $ ?","['algebraic-geometry', 'sheaf-cohomology', 'schemes']"
4078978,"Just using $+$ ,$-$, $\times$ using any given n-1 integers, can we make a number divisible by n? (NO bracket allowed!!)","Probably title is slightly ambiguous but I could not see any way of shortening the problem. I am sure many of you have seen the problem like $2$ $\square$ $2$ $\square$ $2$ $\square$ $2$ $\square$ $2$ $=$ $20$ something like this where between the numbers we can use some sort of operation depending on the question. Well I think the question is motivated by this. Let $n>2$ . Given any set of $n-1$ numbers (we can swap the order) and we can only use addition, subtraction and multiplication, can we make a multiple of $n$ always? Another way of putting it is: Given $n-1$ elements of non-integral domain $\mathbb{Z}/n\mathbb{Z}$ , can we make zero only using $+,-,\times$ ? Oh and by the way bracket is not allowed!!!! I was asked this in an interview randomly and I could not figure it out and the interviewer did not know the answer either. As people suggested I will post the 'proof' if bracket is allowed. If bracket is allowed then this is easy. I will use the second form of the problem. Firstly let $n>2$ and $a_1,...,a_{n-1} \in \mathbb{Z}/n\mathbb{Z}$ If any of the $a_i$ is equal to zero then we could just multiply all the $a_i$ s so assume $a_i$ are all non-zero. Also if bracket is allowed, if $a_i=a_j$ , we could simply put $(a_i-a_j)$ and multiply all other elements together. So we could assume $a_1,...,a_{n-1}$ =1,...,n-1 In which case if bracket is allowed, $(1+(n-1))$ times all other elements would give the wanted answer. But hey. NO Brackets allowed! I have checked for small $n$ s that this is possible and my intuition is that for big $n$ this should easily be possible. But I can't think of a way to show that.","['recreational-mathematics', 'puzzle', 'combinatorics']"
4078983,Proof Verification of $\lim _{x\to 3}\left(\frac{2}{x+3}\right)=\frac{1}{3}$,"Using the epsilon delta definition to prove: $f$ is continuous at $a$ if for every $  Œµ > 0$ there exists $Œ¥ > 0$ such that if $0 < |x - a| < Œ¥$ then $| f (x) - f(a)| < Œµ.$ $$\lim _{x\to 3}\left(\frac{2}{x+3}\right)=\frac{1}{3}$$ $$0 < |x - 3| < Œ¥\,\text{ then }\,\left| \frac{2}{x+3} - \frac{1}{3}\right| < Œµ.$$ After some simplification I get: $$\frac{|x-3|}{|3||x+3|} < Œµ$$ $|x-3|$ is bounded by $Œ¥ $ , $|3|$ is a constant, so $|x+3|$ is the only thing that I have worry about. I have to control it so it doesn't become $0$ or else I got some problems. Since for every $  Œµ > 0$ there exists $Œ¥ > 0$ , I will pick a $Œ¥ > 0$ . So let $Œ¥ = 1$ $$0 < |x - 3| < Œ¥$$ $$-1 < x - 3 < 1$$ plus 6 to both side to ""induce"" $x+3$ , I get $5 < x - 3 < 7$ , now this is nicely bounded, so using this I know that $\frac{|x-3|}{|3||x+3|} < \frac{Œ¥}{3 \times 5}$ Now this is where my problem is Can I just say $\frac{|x-3|}{|3||x+3|} < \frac{Œ¥}{3 \times 5} < \epsilon$ ? If I am allowed to, why? I thought I have prove until I get $\frac{|x-3|}{|3||x+3|} < Œµ$ ? If I were to do this, didn't I just assumed that $\frac{Œ¥}{3 \times 5} < \epsilon$ ? Can someone please continue from here? If I take the min{} of the deltas can I conclude straight away that $| \frac{2}{x+3} - \frac{1}{3}| < Œµ.$ If this is the case, why?","['limits', 'functions', 'solution-verification', 'continuity']"
4078986,Prove by Induction -Inductive Step problem [duplicate],"This question already has answers here : Prove the inequality $n! \geq 2^n$ by induction (3 answers) Closed 3 years ago . The question is prove by induction that $2^n\le n!$ for all $n\ge 4$ So far I have completed the base case so, Base case $n=4$ $ 2^4\le4!$ $16\le24$ Therefore the base case holds Inductive Step
Assume F(n) is true $2^{n+1}\le (n+1)!$ $2^{n+1}\le n!(n+1)$ $2\cdot 2^n \le n!(n+1)$ <- I don‚Äôt know what do after this to get New Approach $2\cdot n!\le n!(n+1)$ $ by F(n) $ $2^{n+1}\le (n+1)!$","['induction', 'discrete-mathematics']"
4078997,Whats the odds of getting my letters?,"I am trying to find the odds of getting the letters I need. We start with 2 groups A B C D      A B C D
E F G H      E F G H The group is random so its not always A..H in that order When you use a letter its taken out of the group. I can only take letters out that are in the first 4 (or top row) in the group. So if I took ""C"" then one letter from the 2nd row moves up into the ""C"" Positon. May look like this A B G D    A B E D
E F H      F G H where a random letter is moved up , not always the same but can be in each group. My question is Whats the odds of both players being able to take A, B, C, D, E in that order from the top row with out skipping a turn. Not sure what Tag to add if you know please let me know and ill add it. Thanks","['statistics', 'probability']"
4079019,Deducing Cayley's theorem from the Yoneda lemma using Wikipedia's recipe [duplicate],"This question already has answers here : Yoneda-Lemma as generalization of Cayley`s theorem? (2 answers) Closed 3 years ago . I'm following Wikipedia in trying to prove that Cayley's theorem emerges as a particular case of the Yoneda lemma. In case that article gets edited, here's the screenshot: A couple of aspects in the proof are unclear: In proving that the set of $G$ -equivariant maps $\alpha_\ast:\mathcal C(\ast,\ast)\to\mathcal C(\ast,\ast)$ is a group, I'm not sure what the inverse of $\alpha_\ast$ is. A (probably) related question: $X$ is the image of the unique object $\ast$ under $H^\ast$ , i.e., $\mathcal C(\ast,\ast)$ . $\text{Perm}(X)$ is the set/group of bijections from $\mathcal C(\ast,\ast)$ into itself. To show that all equivariant maps form a subgroup of this group, we need to show that each equivariant map $\alpha_\ast$ is a bijection from $\mathcal C(\ast,\ast)$ into itself. But I don't see why $\alpha_\ast$ has to be a bijection. $\alpha$ is just a natural transformation, not a natural isomorphism. I'm a little confused about the this point from Wikipedia: ""(2) the function which gives the bijection is a group homomorphism"". What function is meant here? The function from the (dual version of the) Yoneda lemma $[\mathcal C,\textbf{Set}](H^\ast,H^\ast)\to\mathcal C(\ast,\ast)$ doesn't seem to be exactly the right function because its domain has elements that are natural transformations $\alpha$ , but we want to construct a function from the set of $G$ -equivariant maps, which have the form $\alpha_\ast$ for some natural transformation $\alpha$ . So I suspect that the map that is meant is the composite $$\{\alpha_\ast: \alpha:H^\ast\to H^\ast\text{ is a nat. transf.}\}\to \{\text{nat. transf. } \alpha:H^\ast\to H^\ast\}\to \mathcal C(\ast,\ast)$$ where the first map is a ""natural"" bijection (natural in the non-technical sense). But if so, I don't really see why this is a group homomorphism. If we call this composition $f$ , then we need to show that $f(\beta_\ast\circ\alpha_\ast)=f(\beta_\ast)\circ f(\alpha_\ast)$ or equivalently $\beta_\ast(\alpha_\ast(1_\ast))=\beta_\ast(1_\ast)\circ\alpha_\ast(1_\ast)$ . I don't really see why this is true, although this must be something easy.","['representable-functor', 'category-theory', 'abstract-algebra', 'yoneda-lemma', 'group-theory']"
4079067,Open map that is not continuous,"I was trying to find an open map that is not continuous, here was my effort, is it ok? Consider the map $f:\mathbb{R} \rightarrow \mathbb{Z}$ given by $f(x)=\lfloor x \rfloor$ where $\mathbb{R}$ has the standard topology and $\mathbb{Z}$ has the discrete topology. Then $\{1\}$ is open in $\mathbb{Z}$ . However $f^{-1}(\{1\})=[1,2)$ , which is not open in $\mathbb{R}$ , so $f$ is not continuous. Since $\mathbb{Z}$ has the discrete topology, the image of any open set is necessarily open. Thus, $f$ is an open map.","['general-topology', 'solution-verification']"
4079072,Unable to evaluate the integral $\int \csc\left(x-\frac{\pi}{3}\right)\csc\left(x-\frac{\pi}{6}\right) dx $,"The Question $$\int \csc\left(x-\frac{\pi}{3}\right)\csc\left(x-\frac{\pi}{6}\right) dx $$ What I Tried- I tried dividing both numerator and denominator by $\sin \pi/6$ but couldn't get too far, what I got after dividing and doing some simplification- $$2\int \frac{1}{\frac{\sqrt3}{2} - \sin 2x}dx \ = \int \csc\left(\frac{\pi}{6}+x\right)\sec\left(\frac{\pi}{6}-x\right)dx$$ Now I am stuck after this step and don't think there is scope for simplification. Kindly tell where I am going wrong, it may be a very stupid mistake.","['integration', 'trigonometry', 'trigonometric-integrals']"
4079089,"Prove that $AM$ of of $a_1, a_2, a_3....,a_n$ cannot be lesser than $\frac{-1}{2}$, given the recursive relation: $|a_{n+1}| = |a_n+1|$.","A number sequence $a_1, a_2, a_3....,a_n$ is such that $a_1 = 0; |a_2| = |a_1+1|; |a_3| = |a_2+1|; |a_4| = |a_3+1|; .... ; |a_{n+1}| = |a_n+1|$ . Prove that $AM$ of of $a_1, a_2, a_3....,a_n$ cannot be lesser than $\frac{-1}{2}$ . Approach: $$a_1 = 0$$ $$\Longrightarrow |a_2| = 1; a_2 = \pm1$$ $$a_2 = \pm1$$ $$\Longrightarrow |a_3| = 2,0; a_3 = \pm2,0$$ $$a_3 = \pm2,0$$ $$\Longrightarrow |a_4|=3,1; a_4 = \pm3, \pm1$$ I got the terms as above. We need to prove that: $$\cfrac{a_1+ a_2+ a_3+....+a_n}{n}\geqslant\cfrac{-1}{2}$$ Rather than beginning with a proof, I decided to go backwards, so: $${a_1+ a_2+ a_3+....+a_n}\geqslant\cfrac{-n}{2}$$ And to get to it in a fairly easy way I assumed the series to be as: $0,-1,0,-1,0,-1....$ from the terms I found. When $n$ is odd: $$\cfrac{0-1+0-1+0-1....0}{n} = \cfrac{\frac{-(n-1)}{2}}{n} = \cfrac{-n-1}{2n} = \cfrac{-1}{2} + \cfrac{1}{2n} > \cfrac{-1}{2}$$ When $n$ is even: $$\cfrac{0-1+0-1+0-1....-1}{n} = \cfrac{\frac{-(n)}{2}}{n} = \cfrac{-1}{2}$$ This is kind of cheap, I don't know how to generalize this or prove it through induction either. I'm looking for a proof involving the summation operator or induction. Any help would be appreciated!","['summation', 'induction', 'solution-verification', 'sequences-and-series', 'algebra-precalculus']"
4079112,Comparison of definitions for Functions of Bounded Variation,"I have been trying to understand the functions of bounded variation and I came across the following definitions Defintion 1: A function $f:\mathbb{R^d} \rightarrow \mathbb{R}$ is of bounded variation iff $$
\begin{split}
\operatorname{TV}(f)&:=\int\limits_{\mathbb{R}^{d-1}}\mathcal{TV}(f(\cdot,x_2,\cdots,x_d))dx_2 \cdots dx_m +\cdots +\\
& \quad+\cdots+\int\limits_{\mathbb{R}^{d-1}}\mathcal{TV}(f(x_1, \cdots, x_{d-1},\cdot)) dx_1\cdots dx_{d-1} < \infty.
\end{split}
$$ where, for $g:\mathbb{R} \rightarrow \mathbb{R} $ $$
\mathcal{TV}(g):=\sup \left\{\sum\limits_{k=1}^N{\left|g(\xi_k)-g(\xi_{k-1})\right|}\right\}
$$ and supremum is taken over all $M \geq 1$ and all partitions $\{\xi_1,\xi_2,....,\xi_N\}$ of $\mathbb{R}.$ Defintion 2: A function $f:\mathbb{R^d} \rightarrow \mathbb{R}$ is of bounded variation iff $$
\operatorname{TV}(f)= \sup \left\{\,\int\limits_{\mathbb{R}^m}f \operatorname{div}(\phi): \phi \in C_c^1(\mathbb{R^d})^d, \|\phi\|_{L^{\infty}} \leq 1\, \right\} < \infty.
$$ Clearly if $f$ is of bounded variation in the sense of definition 2, it may not be of bounded variation in the sense of definition 1. In this regard, I have the following doubts. If $f$ satisfies definition 1, then do we have $f$ satisfies definition 2? (I felt so but could not prove it rigorously). If  [1] is true then $\operatorname{TV}(f)$ calculated by
definition 1 and definition 2 are they equal? If $f$ satisfies definition 2, does there exist a function $g:\mathbb{R}^d \rightarrow \mathbb{R}$ a.e equal to $f$ such that $g$ satisfies definition 1? If so how to prove it? P.S. : I have read somewhere that 3 is true in one dimension and in-fact we can find $g$ which is right continuous. But I could not find the rigorous proof and also I could not find any such result in multi-d.","['measure-theory', 'analysis', 'bounded-variation', 'functional-analysis', 'total-variation']"
4079136,Suppose $\sum_{n = 1}^{\infty} a_n$ is absolutely convergent. Prove $\sum_{n = 1}^{\infty} b_n$ is absolutely convergent if $|b_n| \leq |a_n|$.,"I am having a bit of trouble with this proof and I am not exactly sure how to prove it. Suppose $\displaystyle\sum_{n = 1}^{\infty} a_n$ is absolutely convergent. Prove $\displaystyle\sum_{n = 1}^{\infty} b_n$ is absolutely convergent if $|b_n| \leq |a_n|$ for all $n \geq 1$ . I started the question by using the Ratio Test, and by definition, the series is absolutely convergent if $\displaystyle \left|\dfrac{a_{n + 1}}{a_n}\right| < 1$ . So if $\displaystyle \left|\dfrac{a_{n + 1}}{a_n}\right| < 1$ , then this means that \begin{equation*}
|a_{n + 1}| < |a_n|
\end{equation*} I am not exactly sure how to proceed from here. I would appreciate some help and advice.","['limits', 'calculus', 'proof-writing', 'sequences-and-series']"
4079147,"In checking whether $\tan^{-1}(2\sin^2x-1)+C$ agrees with $\tan^{-1}(\tan^2x)+C$, why am I getting $\sin^{2} 2x = 2$?","So this began with the integral- $$\int \frac{\sin 2x}{\sin^4x + \cos^4x}dx$$ The integral is simple, I solved it as following- $$\int \frac{\sin 2x}{\sin^4x + \cos^4x}dx=\int \frac{2\sin x\cos x}{\sin^4x + (1-\sin^2x)^2}dx$$ Put $\sin^2 x=t$ $2\sin x\cos x\ dx = dt$ $$\int \frac{dt}{t^2+(1-t)^2}=\frac{1}{2}\int \frac{dt}{(t-\frac{1}{2})^2 + (\frac{1}{2})^2}$$ Now put $t-1/2 = m$ $dt=dm$ $$\frac{1}{2}\int \frac{dm}{m^2 + (\frac{1}{2})^2}$$ Which by standard formula is equal to $$\frac{1}{2}\left( \frac{1}{\frac{1}{2}}tan^{-1}\frac{m}{\frac{1}{2}}\right)+C=tan^{-1}(2t-1)+C=tan^{-1}(2\sin^2 x-1)+C$$ Now the answer according to my worksheet and this page the answer is $tan^{-1}(\tan ^2 x)+C$ Now my answer clearly doesn't match with the given answer so I check whether these two are equal or not- $2\sin^2 x-1=tan^2 x$ $2\sin^2 x=1+tan^2 x$ $2\sin^ 2 x=\sec^2 x$ $2\sin^ 2 x=\frac{1}{\cos^2 x}$ $2\sin^ 2 x \cos^2 x=1$ $4\sin^ 2 x \cos^2 x=2$ $\sin^2 2x=2$ This is impossible and I know I have done something wrong but I cross checked it every time but couldn't find the error. Kindly help me out. EDIT-The standard formula I used above- $$\int \frac{dx}{x^2+a^2}=\frac{1}{a}\tan^{-1}\left(\frac{x}{a}\right)+C$$","['integration', 'calculus', 'trigonometry', 'algebra-precalculus']"
4079173,Evaluating $\int\frac{e^{\tan^{-1} x}}{(1+x^2)^{\frac32}}dx$,I have $$\int\dfrac{e^{\tan^{-1} x}}{(1+x^2)^{\tfrac32}}dx$$ I tried using Integral By Parts: $$\int \dfrac{1}{\sqrt{1+x^2}}\times\dfrac{e^{\tan^{-1} x}}{(1+x^2)}dx=e^{\tan^{-1}x}\times\dfrac1{\sqrt{x^2+1}}+\int x(1+x^2)^{\tfrac{-3}2}e^{\tan^{-1}x}dx$$ $$=\dfrac{e^{\tan^{-1}x}}{\sqrt{x^2+1}}+\int\dfrac{{xe^{\tan^{-1}x}}}{(1+x^2)^{\frac32}}dx$$ But I don't know how to continue,"['integration', 'calculus']"
4079183,Counting The Number of Variants of a Rock Paper Scissors Game with more than 3 Weapons,"The fact that the traditional variant of the game of rock paper scissors has 3 weapons is well known. However with 3 weapons there can be 2 variants: Variant 1:
    Rock beats Scissors
    Scissors beat Paper
    Paper beats Rock
Variant 2: 
    Rock beats Paper
    Paper beats Scissors
    Scissors beat Rock Now, if one wants to increase the number of weapons, the number must be increased to an odd number. So variants that satisfy the following criteria are balanced: There are $n$ weapons; $\{n|n = 2k+1 \land k \in \mathbb{N} \gt 0\}$ Each weapon is weak against $k$ weapons, and strong against the other $k$ . My question is: For $n$ weapons, how many variants can there be? Using an algorithm I've formulated , I found that there are 24 variants for 5 weapons, or in general: $(n-1)!$ variants for $n$ weapons. Summary of the algorithm: Generate the Strengths and Weaknesses Labelled Adjacency Matrix (SWLAM) Switch the rows and then the columns of the SWLAM Relabel the rows and columns. For illustrations and examples, please refer to the given link above. What I'm not sure of is whether or not the algorithm can generate all the variants.","['game-theory', 'combinatorics']"
4079228,A problem on symmetric matrix,"Let $A\in \operatorname{Mat}_n(\mathbb R)$ be a symmetric real matrix. Prove that there exists a diagonal matrix $D \in  \operatorname{Mat}_n (\mathbb R)$ whose entries are chosen from $\{1,-1\}$ satisfying $$\det(A+D)\neq 0$$ It appears in a problem set of linear algebra... I have thought about diagonalizing $A$ and calculating the determinant, but a congruent of $D$ is hard to handle. So I want to reduce the problem to the special case: $D$ is of the form $\operatorname{diag}(-I_s,I_{n-s})$ ( $0\leq s \leq n )$ . But I could not achieve it. It seems hard to commute diagonal entries of $D$ ... Could you share some ideas? Thank you!","['matrices', 'determinant', 'linear-algebra', 'symmetric-matrices']"
4079263,Unusual Constant appearing for definite integral,"I‚Äôm learning integration and have come across a question in a textbook. I arrived to the same answer as the textbook itself, but I know it is not correct. It starts as part a) show that $$\frac{d}{dx} tan^{-1} \left(\frac{3tan(x)}{2} \right)= \frac{6}{5sin^2(x)+4}$$ Hence, find area bounded by curve from $ \dfrac{1}{5sin^2(x) + 4}$ from $x=0$ to $x=7$ . Clearly using part A to integrate and substituting 7 and 0, I arrive to 0.153 square units, in line with the textbook. Yet, by using desmos to graph the curve, using lower rectangles, minimum area has to be $0.111111 \times 7 = 0.77777$ units squared. I put the integral on integral calculator and it yielded $0.153 + \dfrac{\pi}{3}$ , without explaining where this constant came from. I am unfamiliar with constants in definite integrals, and struggled to find anything online to answer this. Thanks. (Note: I am not asking for homework help, Im just asking about how to find constants in these definite integrals, particularly in a question like this, which the textbook itself overlooked. Thank you. Any help is appreciated.)","['integration', 'calculus', 'definite-integrals', 'derivatives']"
4079373,Are these planes always perpendicular?,"The picture shows a pyramid (not necessarily a right pyramid). $V$ is the apex of the pyramid , and $ABCD$ is it's base. Let $\alpha$ be the plane $AVC$ and $\beta$ the plane $BVD$ . True or false: If $ABCD$ is a square, then $\alpha$ is always perpendicular to $\beta$ . Prove it (using only euclidian geometry; vectors/coordinates are not allowed). My thought process so far: I first thought since $ABCD$ is a square, then $AC$ is perpendicular to $BD$ . But this doesn't guarantee $\alpha$ is perpendicular to $\beta$ . There's no theorem saying that if a line in a plane is perpendicular to a line in another plane, then the planes are perpendicular. If $V$ were guaranteed to be straight over the center of $ABCD$ , then it would be easy to prove the proposition is true. The intersection line between $\alpha$ and $\beta$ would be perpendicular to the base and, since $AC$ is perpendicular to $BD$ , the planes would be perpendicular to each other by definition of angle between planes. I think I'm stuck because the pyramid could be oblique.",['geometry']
4079383,Every non-generic point in a curve is closed?,"Let $X$ be a scheme integral scheme of dimension 1. If $X$ is affine, then it is clear that every non-generic point is closed. I wonder if this is true in general. If not, is it true if we suppose that $X$ is a curve? (A variety of dimension 1.)",['algebraic-geometry']
4079432,Avoid the planes - the geometry of grassmannians,"Suppose we have $n$ planes $H_1, \ldots, H_n$ in $\mathbb{R}^m$ of codimension $q$ , or equivalently of dimension $d=m-q$ . I want to choose a vector which does not belong to the planes in a continuous way. There are two versions of this problem, depending on how we parameterize the planes, and the answer can be actually different. Unframed version . Let $Gr_q(m) $ be the grassmannian, that is the space of codimension q planes in $\mathbb{R}^m$ . Does there exist a function $$c : Gr_q(m) ^n \to \mathbb{R}^m$$ Such that $c(H_1, \ldots, H_n) \not \in H_i$ for all $i$ ? Framed version . Let $V_{d,m}$ be the Stiefel manifold, that is the space of orthonormal systems in $\mathbb{R}^m$ of cardinality $d$ . Does there exist a function $$c : V_{d,m} ^n \to \mathbb{R}^m$$ Such that $c(H_1, \ldots, H_n) \not \in H_i$ for all $i$ ? Note . I slightly changed the notation to agree with Chris one; now $d$ denote the dimension of the planes and $q$ the codimension.","['geometry', 'continuity', 'grassmannian', 'linear-algebra', 'algebraic-topology']"
4079437,Can someone answer my question about geometry triangles,"I saw on a children's maths program in early 1980; I think it was Johnny Ball presenting. He drew an obtuse triangle (or I think it was obtuse). Then he put a dot anywhere in the triangle and drew three lines through the dot from each point of the triangle to the other end. He then drew a smaller triangle within the original by joining points created on the sides of the original triangle. Before I state the problem I will try to make my explanation clearer. The dot is placed anywhere inside the obtuse triangle and a line is drawn from each of the three vertices, through the dot to the other end of the triangle. Another smaller triangle is created by drawing three lines from the points (points of intersection of the lines and the sides) on the sides of the larger triangle. Then what Mr Ball did was to find where he could project outwards from the larger triangle and the inner triangle so that the lines met, when you find the projections for all three sides of both triangles that meet, these points create a perfectly straight line. Is there a name for this triangle projection and is there a practical use for this or is it just a fun thing to learn? I have always thought this fascinating. My maths knowledge is limited but I am always inspired by the extent of human ingenuity.","['triangles', 'popular-math', 'geometry']"
4079475,"Is there a function which has limit only given $a_1,a_2...a_n....$ infinite points.","Find a function which has limit only in advanced marked points $(a_1,a_2,....a_n)$ . Here is my example. $g(x) =
\begin{cases}
1,  & \text{if}~x\in\mathbb{Q} \\
0, & \text{if}~x\in\Bbb{R}\backslash \Bbb{Q}
\end{cases}$ $f(x) = (x-a_1)(x-a_2)...(x-a_n)g(x)$ But the problem I can't solve is that,is there a function which has limit only in advanced  given $a_1,a_2...a_n....$ infinite points.","['calculus', 'analysis', 'real-analysis']"
4079482,6 dots & 5 slopes,"I read this question somewhere: Is it possible to mark 6 dots on a surface, and connect them two by two to make lines, in which the total number of slopes is 5 ? There is also the same question but with different numbers. They were easier to solve than this one: $4$ dots & $4$ slopes $5$ dots & $4$ slopes I don't know if I said that correctly, but my point is that if you count all parallel lines as one line, you will have 5 lines in total. I want to know if possible, how and if not, how can we prove that it's impossible.","['graph-theory', 'slope', 'linear-algebra', 'geometry']"
4079533,Are groups with all the same Hom sets already isomorphic?,"I was thinking about the following: Say we got two groups $A$ and $B$, and we know that for any group C, there is a bijection $$Hom(A,C) \to Hom(B,C).$$ Are $A$ and $B$ already isomorphic? If the family of bijections was natural in $C$, this follows from Yoneda's lemma. But does it hold anyway without naturality, just from the cardinalities? I would suspect no but in simple cases (finitely generated). I'd like to hear any thoughts on how to come up with a proof or counterexample. Also, does the answer change when we consider the contravariant case with an identification $Hom(C,A) \to Hom(C,B)$?","['group-theory', 'abstract-algebra', 'category-theory']"
4079557,Using a 'Similarity Variable' to transform a PDE into an ODE?,"I have a PDE: $$\frac{\partial y}{\partial t}=\alpha\frac{\partial^2y}{\partial x^2}\tag{1}$$ So we're looking for a function: $$y=f(x,t)$$ The following substitution with a Similarity Variable then transforms the PDE into a simple ODE: $$z=\frac{x}{2\sqrt{\alpha t}}\tag{2}$$ $$\frac{\mathrm{d}^2y(z)}{\mathrm{d}z^2}=-2z \frac{\mathrm{d}y(z)}{\mathrm{d}z}\tag{3}$$ The latter solves easily to: $$y=c_1\int e^{-z^2}\mathrm{d}z+c_2$$ The trouble is I can't seem to carry out this substitution to get from $(1)$ to $(3)$ , using $(2)$ . I thought of extracting $x$ and $t$ from $(2)$ and differentiating them as $\mathrm{d}t$ and $\mathrm{d}x^2$ but couldn't make that work. So how to make this substitution work?","['partial-derivative', 'multivariable-calculus', 'partial-differential-equations']"
4079593,Is the function differentiable on the entire real line?,"Ok so I know this question has been asked multiple times but in a slightly different format;
Consider the function defined by: $$f(x)=x^2 \sin\left(\frac{1}{x^2}\right)~\text{for}~x \neq0$$ $$f(x)=0~\text{for}~x=0$$ Using the limit derivative definition I found: $$f_R'(0^+)=0~\text{and}~f_L'(0^-)=0$$ By means of the squeeze theorem, but that: $$f'(x)=2x\sin\left(\frac{1}{x^2}\right)-\frac{2\cos\left(\frac{1}{x^2}\right)}{x}~\text{for}~x\neq0$$ So that using this formula we can see that $f'(0^+)$ and $f'(0^-)$ DNE.
However I nor anybody else in my class were deducted full points for saying that the derivative exists for all $x \in \mathbb{R}$ . Can anybody please clarify why?","['derivatives', 'real-analysis']"
4079612,What is the difference between the equations $x^2=4y^2$ and $x^2=2yx$,"Suppose I am given the equation $x=2y$ . My interpretation of this equation is $A=\{\langle x,y \rangle \in \mathbb R \times \mathbb R |\  x=2y\ \}$ Given this interpretation, I want to know the difference between the equations $x^2=4y^2$ and $x^2=2yx$ . For the equation $x^2=2yx$ , we have a single line that completely overlaps with $x=2y$ . For the equation $x^2=4y^2$ , we have two intersecting lines, only one of which overlaps with $x=2y$ . In either case, this is satisfying because it explains the logical concepts of: $$x=2y \implies x^2=4y^2$$ $$\text{and}$$ $$x=2y \implies x^2=2yx$$ which I feel as though I am implicitly using during different algebraic proofs. That is to say, the set of solutions for $x=2y$ also satisfies ( intersects with ) the equations (sets) $x^2=4y^2$ and $x^2=2yx$ . In line with our earlier set interpretation, letting $x^2=2yx$ represent the set $B=\{\langle x,y \rangle \in \mathbb R \times \mathbb R | x^2=2yx\}$ and letting $x^2=4y^2$ represent the set $C=\{\langle x,y \rangle \in \mathbb R \times \mathbb R | x^2=4y^2\}$ , the aforementioned implication suggests $A \subseteq B$ and $A\subseteq C$ . However, given my descriptions of the graphs, it is quite clear that we can be more specific...namely: $A=B$ and $A \subsetneqq C$ I see that for the equation $x^2=4y^2$ , square rooting both sides yields $\pm x=2y$ , which explains its graph of the two intersecting lines. But I am wondering if there is a more fundamental reason that explains this observation . In particular, the quality of inverse exponents versus the quality of inverse multiplication. To simplify $x^2=2yx$ , I am effectively multiplying each side by $x^{-1}$ . Comparatively, to simplify $x^2=4y^2$ (perhaps better written as $x^2=(2y)^2$ ), I am taking the inverse operation of squaring. It seems like the inverse operation of squaring yields ""more solutions"" than the inverse operation of multiplication. Any added insight would be greatly appreciated!","['elementary-set-theory', 'algebra-precalculus', 'logic']"
4079619,"If $f\colon X\to Y$ is an onto, continuous mapping and $X$ is Lindel√∂f then $Y$ is Lindel√∂f","Let $f\colon X\to Y$ be a onto, continuous mapping. Let $\left\{V_t\right\}_{t\in T}$ be a cover of $Y$ ; that is, $$Y\subseteq\bigcup_{t\in T}V_t.$$ Therefore, $$X=f^{-1}[Y]\subseteq\bigcup_{t\in T}f^{-1}[V_t].$$ Since $f$ is continuous, $\left\{f^{-1}[V_t]\right\}_{t\in T}$ is a (open) cover of $X$ . As $X$ is Lindel√∂f, there is a countable set $S\subseteq T$ for which, $$X\subseteq\bigcup_{s\in S}f^{-1}[V_s].$$ Therefore since $f$ is onto, $$Y=f[X]\subseteq\bigcup_{s\in S}f\left(f^{-1}[V_s]\right)\subseteq\bigcup_{s\in S}V_s.$$ Hence, $\left\{V_s\right\}_{s\in S}$ is a countable subcover of $Y$ . By definition, this implies $Y$ is Lindel√∂f. Is the above correct?","['general-topology', 'solution-verification']"
4079631,Most direct way to prove the domain of $A^2$ is dense.,Let $A\colon \operatorname D(A)\to \mathcal H$ be a (generally unbounded but densely defined) self-adjoint operator in a Hilbert space. $$\operatorname D(A^2):=\{\psi \in \operatorname D(A) \text{ s.t. }A\psi \in \operatorname D(A)\}.$$ What is the most direct way to prove that $\operatorname D(A^2)$ is dense in $\mathcal H$ ? Or it is necessary to use the full machinery of the spectral theorem?,"['self-adjoint-operators', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'unbounded-operators']"
4079655,How contents of equivalent classes explain these implications?,"Let's consider equation $s^2 \equiv-1(mod \;p)$ . Theorem says: For $p = 4m+1$ equation $s^2 \equiv-1(mod \;p)$ has two solutions $s
\in \{1,2,...,p-1\}$ For $p = 2$ equation $s^2 \equiv-1(mod \;p)$ has one solution $p =2$ For $p = 4m+3$ equation $s^2 \equiv-1(mod \;p)$ has no solutions Proof For odd $p$ let's put equivalance relation which identifies element with it's inverse and opposite elment on $\mathbb{Z}_p$ . It means that we have equivalence class in form of: $$\{x, -x, \bar{x}, -\bar{x}\}$$ But it can be shorten. Let's consider these three cases: (1) $x \equiv -x$ - it cannot be satisfied, because $p$ is odd (2) $x \equiv \bar{x} \Leftrightarrow x^2 \equiv 1 \Leftrightarrow x =1 \lor x = p-1$ . It means that equivalence class is a set $\{1, p-1\}$ (3) $x \equiv -\bar{x} \Leftrightarrow x^2 \equiv -1 $ This equation can have no solutions or two different ones: $x_0$ and $p-x_0$ . When it does, the equivalence class is in form of $\{x_0, p - x_0\}$ . Set $\{1,2,...,p-1\}$ has $p-1$ elements and we divided it into four-element equivalence classes (or two-element - according to above). Now if $p-1=4m+2$ then we have only one couple $\{1, p-1\}$ and the rest of the classes is four-element. Due to this fact equation $s^2 \equiv-1(\mod \;p)$ has no solutions. $(*)$ But if we take $p - 1 \equiv 4m$ then we have also additional two-element equivalence class containing solutions of $s^2 \equiv-1(mod \;p)$ which we were looking for. $(**)$ Question section I have a question related to this proof -  I marked by $(*), (**)$ parts in proof to which I have queries. How exactly using this facts from contents of equivalence classes we can justify something on solutions of equation $s^2 \equiv -1 (mod \; p)$ ? I just don't quite get it. Could you please give me a hand explaining this part of the proof ? EDIT The remaining thing for me two understand is only that: (1) For $p-1 = 4m+2$ there is only one two-element equivalence class $\{1, p-1\}$ and other are four-element. (2) For $p - 1 = 4m$ there is one equivalence class $\{x_0, p-x_0\}$ , one $\{1, p-1\}$ and remaining ones are four-element. Maybe it's obvious but I don't understand how it can be truth. For example equivalent formulation of (1) is that: $\exists! x_0 : x_0 \equiv \bar{x_0} (mod \; 4m+2)$ and for remaining $x \in \{1,2,...,4m+2\} \setminus \{x_0\}$ we have that equivalence class cannot be shorten. Could you please explain to me how to prove that ?","['divisibility', 'number-theory', 'proof-explanation', 'equivalence-relations', 'elementary-set-theory']"
4079681,Seeking an example of a group with finite presentation for which the Word Problem is not solvable,"In the book Geometric Group Theory of Clara Loh, it is proven that the Word Problem is solvable for hyperbolic groups. It is also stated that the Word Problem is not solvable in all finitely presented groups. Now I was searching for an example of such a group with finite presentation for which the Word Problem is not solvable, but I can't find one. Can anyone help me please? Thanks in advance!","['word-problem', 'geometry', 'combinatorial-group-theory', 'geometric-group-theory', 'group-theory']"
4079685,Upper bound for $\sum_{i = 0}^{k-1} {n \choose i} (1 - \varepsilon)^i\varepsilon^{n-i}$,What is tight upper and lower bound for following expression where $0 < \varepsilon < 1$ and $1\leq k \leq n$ . $\sum_{i = 0}^{k-1} {n \choose i} (1 - \varepsilon)^i\varepsilon^{n-i}$,"['binomial-distribution', 'binomial-coefficients', 'discrete-mathematics', 'binomial-theorem', 'probability']"
4079709,Proving a group of order $35^3$ is solvable,"Is my thinking correct when asked to show that a group $G$ of order $35^3$ is solvable, I first show that by the sylow theorems there exists a sylow $p$ -subgroup of order $5^3$ and another unique sylow $p$ -subgroup of order $7^3$ . Then since these two unique sylow $p$ -subgroups compose the group and are solvable then the entire group is solvable. What are the techniques of showing a group is solvable with the sylow theorems? I'm having trouble proving this one and another where the group is of order $80$ . Thank you for the help.","['group-theory', 'abstract-algebra', 'sylow-theory', 'solvable-groups']"
4079714,We choose $5$ numbers from $1$ to $100$. We order them by value. What is the expected difference between the second and the third?,"I came across this peculiar problem. We choose $5$ numbers from $1$ to $100$ (with repetition). We order them in decreasing order by value. What is the expected difference between the second and the third? For example, we draw $6,67,89,45,33$ .
Difference is $67-45=22$ .","['expected-value', 'order-statistics', 'probability-theory', 'probability']"
4079723,complexification of the vector space $\Bbb R^+$,"Consider $\Bbb R$ and $\Bbb R^+$ as isomorphic vector spaces, with isomorphism $f(x)=\exp x.$ Is the complexification of $\Bbb R$ isomorphic to the complexification of $\Bbb R^+?$ What is the complexification of $\Bbb R^+?$ I understand that when going from $\Bbb R$ to $\Bbb R^+$ under $f$ we have $a+b:=ab$ and $kb:=b^k.$ Here is a relevant link: vector space .","['complex-analysis', 'linear-algebra', 'vector-spaces', 'complex-numbers']"
4079751,Simple question on a quotient space of real projective space,"A silly little detail I am unable to work out on my own that is needed for a bigger thing... I am quite convinced that the quotient space $\mathbb{R} \text{P}^{n} / \mathbb{R} \text{P}^{n-1}$ (where $\mathbb{R} \text{P}^{n-1}$ is included in $\mathbb{R} \text{P}^{n}$ in the standard way) is homeomorphic to $S^n$ on the grounds that it ""makes sense"" thinking about it geometrically in the case of $n=2$ . Nevertheless, I am unable to actually construct a proof of the proposition. If it is true, then this should be a standard result. Alas I cannot find it looking around. Anyone able to help me?","['general-topology', 'geometry']"
4079834,"Proving that $(k + 1) \mid \binom{n}{k}\binom{n+1}{k}$ for positive integers $n, k$ [duplicate]","This question already has answers here : Proof that a certain fraction is always an integer (3 answers) Closed 3 years ago . I've been playing around with some binomial coefficients and their divisibility, and I stumbled upon a relation that seems to hold, at least for $0 < k < n < 200$ (checked with Python): $$ (k + 1) \mid \binom{n}{k}\binom{n+1}{k}. $$ I initially thought that the following reasoning would work. Since $\binom{n}{k}$ has $k$ terms in the numerator, the only way that there is no multiple of $(k+1)$ in the numerator of $\binom{n}{k}$ is when $n \equiv -1 \pmod{k+1}$ . But then $(k+1)\mid(n+1)$ , so there is a multiple of $(k+1)$ in the numerator of $\binom{n+1}{k}$ . Unfortunately, I'm not sure if showing that there is a multiple of $(k+1)$ in the numerator of the expression is sufficient for all values of $k$ . It definitely works when $(k+1)$ is prime, but if not, there is the possibility that the designated multiple of $(k+1)$ is canceled out by other terms in the denominator. So: is there a way to patch up this argument, or could this statement be proven by entirely different means? (Bonus brownie points if there's a combinatorial argument.) Thanks!","['elementary-number-theory', 'binomial-coefficients', 'combinatorics']"
4079859,Extending $\sum_{n=0}^\infty s^{n^2}$ beyond its natural boundary,"Let $\mathbb{D} = \{s \in \mathbb{C} : |s| < 1\}$ . Let $f : \mathbb{D} \rightarrow \mathbb{C}$ where $$ f(s) = \sum_{n=0}^\infty s^{n^2} $$ $f$ is analytic on $\mathbb{D}$ . This is what it looks like: $\partial \mathbb{D}$ is a natural boundary , so ordinary analytic continuation cannot extend $f$ beyond $\mathbb{D}$ . However, there might be another way to extend $f$ beyond $\mathbb{D}$ that is both aesthetically natural and yields a unique result. For example, see generalized analytic continuation and Continuation of functions beyond natural boundaries . Let $$ \Delta = \left\{\exp \left( 2\pi i \frac{2n + 1}{4m + 2} \right) : m, n \in \mathbb{Z}\right\} \subseteq \partial \mathbb{D} $$ Note that $\Delta$ is dense . It seems that \begin{align}
    \lim_{\substack{s \rightarrow \omega \\ s \in \mathbb{D}}} f(s) &= \frac{1}{2} \\
    \lim_{r \uparrow 1} \partial_r f(r \omega) &= 0
\end{align} for all $\omega \in \Delta$ . I have 2 questions: Are there other $\omega \in \partial \mathbb{D} \setminus \Delta$ for which the above limits exist? Is there an analytic (or meromorphic) $g : \mathbb{C} \setminus \overline{\mathbb{D}} \rightarrow \mathbb{C}$ such that \begin{align}
    \lim_{\substack{s \rightarrow \omega \\ s \in \mathbb{C} \setminus \overline{\mathbb{D}}}} g(s) &= \lim_{\substack{s \rightarrow \omega \\ s \in \mathbb{D}}} f(s) \\
    \lim_{r \downarrow 1} \partial_r g(r \omega) &= \lim_{r \uparrow 1} \partial_r f(r \omega)
\end{align} for all $\omega \in \partial \mathbb{D}$ such that both sides are defined, and such that the LHS is undefined iff the RHS is undefined? That is, such that $g$ ‚Äúmatches‚Äù $f$ along $\partial \mathbb{D}$ , in some sense?
If so, is it unique? If not, are there additional conditions, perhaps some analogue of the identity theorem , that can give us a unique extension?","['analytic-continuation', 'complex-analysis', 'lacunary-series', 'sequences-and-series', 'power-series']"
4079863,Invariant problem: Prove that you can't color every line of the graph,"Given a graph with $6$ vertices and $15$ edges, every starting line is colored yellow. On each step we can choose any $3$ lines that create a triangle (triangle's vertices have to match graph's vertices) and change the color of each line. If the color's yellow, change it to red and if it's red change it to yellow. Prove that no matter what you do, you can't get $15$ red lines. Hint: In order to find invariant choose a vertex and look at the adjacent lines. Here's the picture: Well after thinking for a while, I came up with this reasoning: Each vertex has $5$ lines, if the vertex's part of the triangle, then $2$ of it's sides get colored every time. So we want to get $5$ red lines, but we're swapping even lines each time, so we can't reach odd red lines. Therefore we can't color this graph into only red lines. But I think this requires stronger reasoning. How do I prove this more formally?","['graph-theory', 'coloring', 'discrete-mathematics']"
4079891,Is there an intuitive explanation for the maximal ergodic theorem?,The maximal ergodic theorem states that for an $L^1$ integrable function the set $$E(f)=\{x : \max_n\left(\sum_{i=0}^{n-1}f(T^i(x))\right)>0\}$$ has the property that $\int_{E(f)} f d \mu \geq 0$ .  The only time I've seen it used is to prove Birkhoff's ergodic theorem by showing that the set of points where the $\limsup$ and $\liminf$ of the average of the interations of $T$ is not equal has measure zero.  Does the maximal ergodic theorem say anything interesting or is it just a clever tool?,"['measure-theory', 'ergodic-theory', 'analysis', 'real-analysis', 'dynamical-systems']"
4079914,permutation on a circle within some distance,"Assuming $n$ children sitting around a round table. For example, 7 children. At the second time they changed seats, so this is a permutation $\omega$ . Each child's new seat and previous seat define a distance, if the permutation is $6 1 2	5	3	4	0$ , so now the child last round took seat # $0$ is now seating at # $6$ , so he moved $1$ distance, or $d(0)=1$ . Call the biggest distance of all the children the distance of the circle permutation. For example the distance of all the children are $1,0,0,1,1,2,1$ , so the $d(\omega) = \max_i d(i) = 2$ . Now the questions is, for $n$ children, if the distance shall be no more than $k$ , how many permutations $N(n,k)$ are there? Of course $\forall n$ , $N(n,0)=1$ , i.e. no one moves. If $k=1$ it's also simple, $N(n,1) = F(n+1)+F(n-1)+2$ , here $F(n)$ is the Fibonacci numbers starting with $F(0)=0$ , and $F(1)=1$ .
for example if $n=10$ , then $N(10,1)=135$ . But when $k\ge 2$ it's turns quite complex I couldn't find a nice solution. For example by programming it shows $N(10,2) = 6208$ but I couldn't find a close form or recursive form for it. Is there a way to compute it? Or, the simple case, is there a close or recursive form for $N(n,2)$ ?",['combinatorics']
4079938,Algebraic variety and real manifold,"When an algebraic variety, affine or projective, is a manifold ( on $\mathbb{R}$ )? And when the inverse is true?
I think that each algebraic variety is a manifold but for the projective? I would think the same but I would like to have an explanation about the relathionship of these structures","['algebraic-geometry', 'abstract-algebra', 'differential-geometry']"
4080068,"Mean of $Y=\cos(X)$ where $X\sim\mathcal N(0,1)$","Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $X,Y:\Omega\rightarrow\mathbb{R}$ be two random variables. If $X\sim\mathcal N(0,1)$ and $Y=\cos(X)$ , what is $\mathbb{E}[Y]$ (i.e. the mean of $Y$ )? Using the definition of the mean, \begin{align}
\mathbb{E}[Y]&=\mathbb{E}[\cos(X)] \\
&=\int_\mathbb{R} \cos(x)f_X(x) \ dx \\
&=\frac{1}{\sqrt{2\pi}}\int_\mathbb{R} \cos(x)\exp\left(-\frac{x^2}{2}\right) \ dx \\
&=\frac{1}{\sqrt{e}}.
\end{align} However, according to my textbook $\mathbb{E}[Y]=0$ . Is there an error in my logic?","['solution-verification', 'probability']"
4080260,Can the points of a convergent sequence be connected by a smooth curve?,"Suppose $U\subset \mathbb{R}^n$ is the unit ball and $\left\{x_i\right\}_{i\in \mathbb{N}}$ is a sequence in $U$ that converges to $0$ . Does there exist a smooth curve $\gamma \colon \left[0,1 \right] \to U$ such that the image of $\gamma$ contains a subsequence of $\left\{x_i\right\}_{i\in \mathbb{N}}$ and such that $\gamma(0)=x_1$ and $\gamma(1)=0$ ? I do not know how to approach this. The case for finitely many points already troubles me. I know about smoothing piece-wise linear curves but that is not really helpful here. Edit: So, I did have an idea to get a $C^0$ -curve that is analytic on $(0,1]$ , so everywhere except one boundary point. Sadly my approach will probably never explicitly produce a $C^{\infty}$ -curve. Consider $\mathbb{R}^n\subseteq \mathbb{C}^n$ and the given sequence as a sequence in $\mathbb{C}^n$ . That is one gets $n$ sequences in $\mathbb{C}$ by considering the components $\left\{ x_i^j \right\}_{i\in \mathbb{N}}$ . Also one may consider the discrete sequence $\left\{1,2,3,... \right\}$ in $\mathbb{C}$ . As $\mathbb{C}$ is a Stein manifold it follows that there exists holomorphic functions $f^j\colon \mathbb{C} \to \mathbb{C}$ such that $f^j(i)=x^j_i.$ Then $g'^j:=\mathrm{Re}(f^j)\mid_{\mathbb{R}}\colon \mathbb{R} \to \mathbb{R}$ also satisfies the above equation and note that $\lim_{x\to \infty }g'^j(x)=0$ . One may moreover consider the analytic  map $g^j\colon (0,1] \to \mathbb{R},\; x \mapsto g'^j(1/x)$ it satisfies $\lim_{x\to 0} g^j(x)=0$ and thus extends continuously to $[0,1]$ . Moreover, its image contains the entire sequence one started with by construction.","['curves', 'differential-geometry', 'analysis', 'real-analysis']"
4080265,"Why does $\lim\limits_{x \to \infty}{\frac{a^x}{x!}}\;,\;a \in \mathbb{R}^+,$ approach $0?$ [duplicate]","This question already has answers here : Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$. (15 answers) Closed 3 years ago . I'm currently writing a paper about numerical analysis and at one point I needed to calculate $$\lim\limits_{x \to \infty}{\frac{a^x}{x!}}\quad \text{with} \quad a \in \mathbb{R}^+$$ Based on the fact that $x!$ goes faster to infinity than $a^x,$ I thought that it would approach zero, but this isn't really a proof, only a thought. So I'd appreciate it if you'd be able to help me with proving this.","['limits', 'calculus', 'exponential-function']"
