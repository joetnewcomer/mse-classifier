question_id,title,body,tags
719988,computing the limit of $\frac{1}{n} \sum_{k=1}^{n}{\frac{1}{k}}$ [duplicate],This question already has answers here : How to show that $\lim \frac{1}{n} \sum_{i=1}^n \frac{1}{i}=0 $? [duplicate] (8 answers) Closed 6 years ago . Prove that the following limit is 0 $\lim\limits_{n\to\infty} \frac{1}{n} \sum_{k=1}^{n}{\frac{1}{k}}$ Please I don't know how to do it :S. The harmonic series diverges thus it requires some special trick,"['harmonic-numbers', 'calculus', 'real-analysis', 'limits']"
719994,Evaluating trigonometric integral and Cauchy's Theorem,"I am trying to evaluate the following integral: $\int_0 ^\pi {d\theta\over{1+\sin^2\theta}}$ I tried using the substitution of $\sin\theta={1\over 2i}(z-1/z)$, where $z=e^{i\theta}$, and $d\theta={1\over iz} dz$, and got to $1+\sin^2\theta= 1/4 (4iz-iz^3+2iz-i/z)$. I would put this back in the original integral, but I do not know how I should manipulate this. I plan to use Cauchy's Formula in the end.   Please help me out.","['trigonometry', 'integration', 'complex-analysis']"
719995,"Does there exist a nowhere differentiable, everywhere continous, monotone somewhere function?",Is there a nowhere differentiable but continuous everywhere function which is monotone in some small interval however small it is? Until now I have seen only the Weierstrass function and it seems to be oscillating everywhere.,"['functions', 'measure-theory', 'examples-counterexamples', 'real-analysis']"
720001,Why is Dantzig's solution to the knapsack problem only approximate,"For a bunch of items with values $v_i$ and weights $w_i$, and with a total weight $W$ that our bag can carry, how do we achieve maximum total value without breaking the bag? Dantzig proposed that we consider the ratio $v_i/w_j$, and add items for which this ratio is largest until the bag fills up. But this is regarded as an approximate solution. I have also been thinking about this problem. I came up with the same solution as Dantzig and even proved it for the $0-1$ problem. Of course, I was suspicious of my proof even before I knew the method had been proposed before (since it would imply that $P= NP$). I am even more suspicious now. So put me out of my misery. Why doesn't it work?","['optimization', 'np-complete', 'computational-complexity', 'combinatorics']"
720051,The antiderivative of $\sin(1/x)$,"How to prove that the function $f(x)=\sin\frac{1}{x}$ for $x\neq 0,f(0)=0$ has an antiderivative? This means $F(x)=\int^{x}_{0}\sin(1/t)dt$ has derivative $0$ at $x=0$, but I have no idea how to prove it.","['definite-integrals', 'improper-integrals', 'calculus', 'integration']"
720072,Example when f(x) does not vanish at infinity,"I have a homework question that asks to give an example of a random variable X with a probability density f(x) such that the limit as f(x) goes to positive infinity does not exist. So, we want to find an example when f(x) does not vanish at infinity. I am completely stuck on this one, I have no idea where to begin. Any help would be appreciated!","['probability-theory', 'probability']"
720109,Putnam inspired problem,The following is a beautiful problem from Putnam 2003 minimize $|\sin x + \cos x + \tan x + \csc x + \sec x + \cot x|$ I was thinking about a small variation of the above problem minimize $|\sin x + \cos x + \tan x - \csc x - \sec x - \cot x|$ Thanks.,"['trigonometry', 'inequality', 'contest-math']"
720128,Decrease of entropy when iterating a random discrete function,"Let $m$ be a positive integer. Let $S$ be the set of non-negative integers $x$ less than $m$, with $|S|=m$. Let $X_0$ be the discrete uniform distribution over $S$, with $P(x)=\begin{cases}
1/m & \text{if } x\in S \\
0   & \text{if } x\not\in S
\end{cases}$ Let $F$ be any of the $m^m$ discrete functions from $S$ to $S$. For that particular $F$, let $X_{i+1}=F(X_i)$ be the discrete distribution of the output of $F$ for input distribution $X_i$ (or equivalently the discrete distribution of $F^i$ for input distribution $X_0$); and let $H_i=H(X_i)$ be the entropy of $X_i$. Question: How is the distribution of $H_i$ when $F$ is a uniformly random discrete function from $S$ to $S$? In particular, can we find a little-$\mathcal o$ expression of its mean (perhaps variance or standard deviation, skewness..) when $m\to\infty$? An heuristic approximation is proposed here , with $n=\log_2(m)$, in a cryptographic context. It holds that $H_0=\log_2(m)$; and $\forall i\in\mathbb N, H_{m-1}\le H_{i+1}\le H_i\le\log_2(m)$. $H_i$ for an individual $F$ depends only on $i$ and the structure of the Functional Graph of $F$, irrespective of its labeling. Here a graph showing a random $F$ (a truncated hash with $m=2^{10}$). Another example yielding a maximally high $i$ before $H_i$ no longer decreases is with
$$F(x)=\begin{cases}
x-1 & \text{if } 1\le x<m \\
0   & \text{if } 0\le x\le1\text{ and }x<m
\end{cases}$$
we have
$$F^i(x)=\begin{cases}
x-i & \text{if } i\le x<m \\
0   & \text{if } 0\le x\le i\text{ and }x<m
\end{cases}$$
and it comes
$$H_i=\begin{cases}
\log_2(m)                         & \text{if } i=0 \\
\log_2(m)-{i+1\over m}\log_2(1+i) & \text{if } 0\le i\le m-1 \\
0                                 & \text{if } i\ge m-1
\end{cases}$$","['statistics', 'random-functions', 'discrete-mathematics', 'entropy']"
720149,Bound for probability of the intersection of a set of events,"There are $N$ random variables $X_1,\dots X_N$ and $Pr(X_i=1)=p$ $\forall i\in N$. Can we upper bound the probability  that all random variables are $1$, i.e., $Pr(X_i=1,\forall i\in N)$. Note that the random variables are not independent. Edit:
How about a lower bound?
Looking for answers other than $0,1$. My attempt: I am thinking the product (as if independent) is a upperbound, but not sure.",['probability']
720185,Evaluate the limit $\lim\limits_{x\to0+}\left(\frac{3^x+5^x}{2}\right)^{\frac1x}$,"Evaluate $$
\displaystyle\lim_{x\to0+}\left(\frac{3^x+5^x}{2}\right)^{\displaystyle\frac{1}{x}}
$$ And actually I have my answer and just need someone to verify this for me since I haven't done something like this for a long time. First, to deal with the pesky $1/x$ , I take the natural log inside the limit: \begin{align}
\lim_{x\to0+}\ln\left(\frac{3^x+5^x}{2}\right)^{\displaystyle\frac{1}{x}}
&= \lim_{x\to0+}\frac{1}{x}\ln\left(\frac{3^x+5^x}{2}\right)\\
&= \lim_{x\to0+}\frac{\ln(3^x+5^x)-\ln2}{x}\\
&= \lim_{x\to0+}\frac{3^x\ln3+5^x\ln5}{3^x+5^x}......L'Hopital's \;Rule\\
&=\frac{\ln3+\ln5}{2}\\
&=\frac{1}{2}\ln3+\frac{1}{2}\ln5
\end{align} And since what we calculated was the limit the of the natural log, the final answer would be $\displaystyle e^{\frac{1}{2}ln3+\frac{1}{2}ln5}=e^{\sqrt{3}+\sqrt{5}}$ . Please tell me if I did this correctly, thanks.",['limits']
720216,$K$-schemes as varieties: the importance of the structural morphism,"Consider a variety $p:X\longrightarrow\operatorname{Spec K}$ where $X$ is an integral scheme and $p$ is a separated morphism of finite type. Now chose an element $\sigma\in\operatorname{Aut}(K)\setminus\{\operatorname{id}\}$, then we can construct another variety over $K$ namely $\operatorname{Spec}(\sigma)\circ p: X^\sigma\longrightarrow\operatorname{Spec}(K)$, where, despite the two names,  we have that $X^\sigma=X$ as schemes. These two varieties are different (in general not even isomorphic) because the structural morphisms are distinct, but they are defined by the same underlying scheme. I have proved that, in the framework of algebraic subsets (so working with the classical/old concept of variety), this switch from $X$ to $X^\sigma$  is equivalent to changing through $\sigma$ the coefficients of the polynomials that define our varieties.
Formally this is clear, but it is hard to understand how the concept of variety depends so heavily on the structural morphism to  $\operatorname{Spec}{K}$. Simply modifying the morphism, but mantaining the same scheme $X$, we obtain two different objects, and this is so strange!!
Can you point out some other examples of the importance of the structural morphism in the modern definition of algebraic varieties? I'd like also some practical enlightenments
that help me to figure out the above situation.","['algebraic-geometry', 'schemes']"
720236,Proving the reflection principle of Brownian motion,"The reflection principle of Brownian motion states that Brownian motion reflected at some stopping time $\tau$ is still a Brownian motion. The proof found in Mörters & Peres (as well as in several other books), rests on the fact that the concatenation mapping, which takes a continuous path $\{g(t) : t \geq 0\}$ and glues it to the end point of a finite continuous path$\{f(t) : 0 \leq t \leq \tau\}$ to form a new continuous path, is measurable. I don't see how to prove that the concatenation mapping is measurable. Formally, the reflection principle states (I follow Schilling & Partzsch): Let $(B(t), \mathcal{F}_t)_{t \geq 0}$ be a standard, $d$-dimensional Brownian motion and let $\tau$ be an a.s. finite stopping time. Consider the process
  $$
W(t,\omega) := \begin{cases} B(t,\omega), & 0 \leq t < \tau(\omega) \leq +\infty \\ 2B(\tau(\omega), \omega) - B(t,\omega), & \tau(\omega) \leq t < \infty
\end{cases}
$$
  Then $(W(t))_{t \geq 0}$ is again a standard Brownian motion. To prove this theorem we may assume, without loss of generality, that $\tau(\omega) < \infty$ for all $\omega \in \Omega$, and we define the mapping $\Phi : \mathbf{C}_{(0)}\times[0,\infty)\times\mathbf{C}_{(0)} \rightarrow \mathbf{C}_{(0)}$, where $\mathbf{C}_{(0)} := \{f \in \mathbf{C}[0,\infty) : f(0) = 0\}$, thus
$$
\Phi(f,t,g) := \begin{cases}
f(s), & 0 \leq s < t \\
f(t) + g(s - t), & t \leq s < \infty
\end{cases}
$$ It is claimed that $\Phi$ is $\mathcal{B}(\mathbf{C}_{(0)})\otimes\mathcal{B}[0,\infty)\otimes\mathcal{B}(\mathbf{C}_{(0)})/\mathcal{B}(\mathbf{C}_{(0)})$ measurable. Schilling and Partzsch offer the following hint for proving this fact: equip $\mathbf{C}_{(0)}\times[0,\infty)\times\mathbf{C}_{(0)}$ with the topology of locally uniform convergence (in $\mathbf{C}_{(0)}$) and pointwise convergence (in $[0,\infty)$). I'd appreciate help in proving why $\Phi$ is measurable, either by following Schilling & Partzsch's cue, or by another method. Additionally, if it makes life easier, it can be assumed that $d = 1$. Works cited Mörters, Peter and Peres, Yuval. Brownian Motion. The version freely available from Peres's home page . Accessed 2014-02-18. ( Theorem 2.19, pp. 44-45 ) Schilling, René L. and Partzsch, Lothar. Brownian Motion - An introduction to stochastic processes. De Gruyter. 2012. ( Theorem 6.12, pp. 71-72 )","['probability-theory', 'stochastic-processes', 'measure-theory', 'brownian-motion']"
720254,Is there a nonempty open bounded subset of plane whose boundary contains no 1 dimensional interval?,"Someone asked a question here which hasn't received a correct answer because everyone seems to be misinterpreting the question.  I would like to ask the question again. Does there exist a nonempty bounded open subset $\Omega$ of $\mathbb{R}^2$ such that there is no continuous injective map $[0,1]\to \partial\Omega$? Note that I am not asking about a continuous bijection because then the problem is trivial (for example the is no continuous bijection of $[0,1]$ to the boundary of a ball).","['general-topology', 'examples-counterexamples']"
720259,$f$ is convex function iff Hessian matrix is nonnegative-definite.,"Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, $f \in C^2$. Show that $f$ is convex function iff Hessian matrix is nonnegative-definite. $f(x,y)$ is convex if $f( \lambda x + (1-\lambda )y) \le \lambda f(x) + (1- \lambda)f(y)$ for any $x,y \in \mathbb{R}^2$. Hessian matrix is nonnegative-definite if $f_{xx}'' x^2 + f_{x,y}(x+y) + f_{yy}''y^2 \ge 0$ I know the definition but I have no idea how prove the If and only if condition or first and second implication?","['optimization', 'convex-analysis', 'hessian-matrix', 'real-analysis', 'positive-semidefinite']"
720271,Riesz Fischer theorem?,"I was wondering about the following: I read that Fischer-Riesz says that $L^2([0,1])$ is isomorphic to $l^2(\mathbb{N})$. Now it is obvious, that this should not depent on the fact which compact subset $K$ you choose in $L^2(K)$, but my question is: Is $L^2(\mathbb{R}^n)$ also isomorphic to $l^2(\mathbb{N})$?","['calculus', 'functional-analysis', 'real-analysis']"
720280,Proof that a given projection map restricted to a subset is closed.,"$\pi_{1}:\mathbb{R}^2\rightarrow\mathbb{R}, (x,y)\mapsto x$ is a projection map from $\mathbb{R}^2$ with the standard eulcidean topology, $\mathscr{T}_E$ to $\mathbb{R}$ with it's usual euclidean topology $\mathscr{T}_\mathbb{R}$. Let $X:=\{(x,y)\in \mathbb{R}^2\;|\; xy=0\}$ (the axes effectively), endowed with the subspace topology $\mathscr{T}_X$ and let $p:X\rightarrow\mathbb{R}$ be the restriction of $\pi_1$ to $X$. Show that $p$ is closed, i.e. for each V closed in $(X,\mathscr{T}_X)$, $p(V)$ is closed in $(\mathbb{R},\mathscr{T}_\mathbb{R})$. Attempt: So the way I would attempt it for open maps at least is to choose a basis for the topology.
Here, $\mathcal{B}=\mathcal{B}_1 \cup \mathcal{B}_2$ is a basis for $\mathscr{T}_X$, where: $\mathcal{B}_1:=\{(\alpha,\beta)\times\ \{0\}\;|\; \alpha,\beta \in \mathbb{R}, \alpha<\beta\}$ $\mathcal{B}_2:=\{\{0\}\times(\alpha,\beta) \;|\; \alpha,\beta \in \mathbb{R}, \alpha<\beta\}$ So I considered taking the complement of the elements in the basis with respect to X, to form a basis $\mathcal{A}$ say, for the closed sets of the topology, $\mathcal{A}:=\{X\backslash U\;|\;U \in \mathcal{B}\}$. Then consider $p(V)$ for $V$ in $\mathcal{A}$. For $V=X\backslash U$ where $U \in \mathcal{B}_2$, $p(V)=\mathbb{R}$, and for  $V=X\backslash U$ where $U \in \mathcal{B}_1$, $p(V)=(\mathbb{R}\backslash (\alpha,\beta)) \cup \{0\}$. In both cases the sets are closed. But, for sets $U_\lambda, \lambda \in \Lambda$ an indexing set, and $f$ a map, $f(\bigcup\limits_{\lambda \in \Lambda} U_\lambda)=\bigcup\limits_{\lambda \in \Lambda}f(U_\lambda)$. So when proving certain things about open sets reduces to proving it for a basis, and I am assuming because of the corresponding image rule: 
$f(\bigcap\limits_{\lambda \in \Lambda} U_\lambda)\subseteq\bigcap\limits_{\lambda \in \Lambda}f(U_\lambda)$, that I can't conclude the map is closed using the basis for closed sets? Cheers for any help. I'd rather hints on approaches as opposed to full solutions, but a comment on the use of basis would be appreciated.","['general-topology', 'elementary-set-theory']"
720282,Why do we care for uniform convergence on compact sets?,"I was trying to come up with reasons, why we naturally consider the topology of uniform convergence on compact sets as the appropriate framework for spaces of holomorphic functions such as e.g. $H(\mathbb{C}^n)$ (which is the space of entire functions on $\mathbb{C}^n$). I understand that e.g. by Weierstrass' theorem the above space is closed, when $f_n$ converges compact to f. Moreover, that we can make such spaces into Fréchet spaces in case the domain behaves nicely enough (e.g. is the union of countable many compact sets, which clearly is the case for $\mathbb{C}^n$), which provides some nice topological properties. But I would like to hear some more motivation or reasons, why to consider this topology naturally, which stem (purely) from complex analysis. Thanks in advance. PS: this is my first question I ask here, so please don't be to hard on me, if it doesn't belong here.","['soft-question', 'complex-analysis']"
720296,Rank of a jet bundle of a vector bundle.,"I am trying to understand the jet bundles but currently I am stuck on the following questions: Let $\pi: E\rightarrow X$ be a smooth (holomorphic) vector bundle of rank $k$ over a smooth (complex) manifold $X$. I know that the bundle $J_k(E)$ of k-jets of $E$ has the structure of a vector bundle over $X$. I would like to know however what the rank of this vector bundle is. Is $J_k(E)$ holomorphic in the case when $(E, \pi, X)$ is holomorphic? Moreover, when $\pi: E\rightarrow X$ is a fiber bundle with structure group $G$, can we view $J_1(E)$ as the associated principal bundle $P$ associated to $E$ or am I wrong? I have seen an interpretation of $J_1(E)$ as some sort of an ""extended frame bundle"" of E in the sense that its fiber consists of the set of all pairs comprising a basis of $T_pX$   $(T^{1, 0}_pX)$ and a basis of $E_p$, $p\in X$ P.S.: I am new here and I really hope that I don't annoy the experienced audience in this forum with trivialities. I would appreciate any help or suggestions or simply good references. Thank you in advance for your competent help.","['differential-geometry', 'ordinary-differential-equations', 'complex-analysis']"
720371,Question About Notation In Field Theory $F(x)$ vs. $F[x]$,"I have a question about notation specifically square brackets $[$ and round brackets $($. My textbook doesn't explain any of this and I cannot find a reliable source online to confirm the difference. So my question is: What is the difference between round brackets and square brackets in terms of notation in Field Theory? For example, I see $F(x)$ and $F[x]$ in my textbook and I've always assumed they were the same thing. But apparently they're not. Is there ever a time they're the same? I wanted to know the difference, it may be a silly question but it's something I want to make sure I understand.","['notation', 'abstract-algebra', 'field-theory']"
720418,Winding number of a point outside the curve is 0,"I've been looking for the answer to the following question for a little while now: Let $γ$ be a closed (C1-)curve whose image is contained in ${z: |z| < R}$ for some $R > 0$. Show that for any $z$ with $|z| > R$ we have $\operatorname{Ind}(γ,z) = 0$. I think I am supposed to use the definition of the index of a winding number, but I have absolutely no idea of how to do it. To me if $z$ is outside the curve then the index is 0 by definition... Any pointers would be greatly appreciated thanks!","['winding-number', 'complex-analysis']"
720421,"To prove FLT, it suffices to prove it for any prime $n \ge 5$.","I once read somewhere, (can't find link) that to prove Fermat's Last Theorem, assuming it has been proven for $n = 3, 4$, it suffices to prove it for every prime $n \ge 5$.  I have no idea why this is true.  Can somebody explain?",['number-theory']
720433,Proving a statement about prime numbers,"Let $p_1,p_2,p_3,\cdots$ be all the primes sorted in an increasing order. Is $p_1p_2p_3\cdots p_i + 1$ is always prime? Why? How can I prove that?","['prime-numbers', 'discrete-mathematics']"
720442,generalization of Banach fixed-point theorem on short maps?,"If  $ \ T:X \longrightarrow X \ $  is contraction, then using Banach fixed-point theorem we know that the fixed point exists and all other points converge to that point. But what happens if $T$ is not contraction? Let $\quad X = (0, \infty) \quad $ and $ \quad Tx = \sqrt{x} \quad $ for all $x \in X$. Distance between any arbitrary points $ x,y \in X$ is always (strictly! (if $x \neq y$)) decreasing when we transform they using T, but because Lipschitz constant is not $ < 1 $, but equal $1$, we don't have contraction. But still, the fixed-point exists (it is equal $1$) and all other points converge to $1$, even if we don't have Banach fixed-point theorem. Are there some theories, or generalizations of the Banach's theorem, that allows us to say something about transformations like mentioned one, while it does not meet conditions of normal Banach's theorem? P.S. Sorry for mz half-baked english, feel free to edit if you spot some irregularities. Edit: Like Eric Towers said it is not strictly decreasing, my mistake, but still, all $(T^{n}x)$ sequences are Cauchy sequences.","['fixed-point-theorems', 'metric-spaces', 'functional-analysis']"
720454,is there any history at all for this notation of partial anti-derivatives?,"i have searched but can not find examples of any published book or online articles that use this notation: $$\int f(x,y) \partial x$$ seems it would be useful for example here: 
$$\int_I\int_J f(x,y)dxdy = \int_I\color{blue}{\left(\color{black}{\int_J f(x,y)}\partial x\right)}dy$$ is there a history of such notation? 
are there problems with such notation?
any thoughts/help would be much appreciated
thx edited->
some background for the question.. a student writes
$$A=xy$$
then writes
$$dA=xdy+ydx$$
then the student tries to recover the A by integrating
$$\int dA=\int ydx+\int xdy$$
which 'yields'
$$A=yx+c(y)+yx+c2(x)=2xy+c(y)+c2(x)$$
which is NOT the correct value of A, the teacher says $\int ydx\ne yx$ the students replies ""sometimes it is"" we routinely compute  $\int ydx=yx$  when doing the inside of a double integral...so in $\int ydx$ sometimes $y$ is held constant and sometimes not.. yet the notation is indistinguishable...","['notation', 'calculus', 'integration', 'partial-derivative']"
720460,why the matrix is diagonalizable?,"show that matrix $$A = \begin{bmatrix} a & b \\
        0 & a
        \end{bmatrix}$$ is diagonalizable iff $b = 0.$ I do not understand why. Cuz if a, b are reals, I can always find a constant: $a = cb$
and row-reduce. Can anyone explain, please.","['matrices', 'linear-algebra', 'diagonalization']"
720469,Lie algebra: why does it have to be the tangent space at the IDENTITY of a Lie group?,Why is the identity element so important in this construction. I looked up some books and notes but still do not see why. How could the construction started from tangent space of a element other than identity possibly fail to get a Lie algebra so that people can only get it from the tangent space of identity?,"['group-theory', 'abstract-algebra']"
720504,Why is $ \lim_{x \to \infty} \ x^{2/x} = 1$,Why is $\displaystyle \lim_{x \to \infty} \ x^{2/x} = 1$ since this is an indeterminate form $\infty^{0}$ and I can't see any manipulation that would suggest this result?,"['calculus', 'limits']"
720507,Tensor product of quotient rings : A proof using Yoneda lemma,"Martin Brandenburg pointed out elsewhere in the comments that he could give a one line proof, using the Yoneda lemma, of $$\frac{\mathbf{C}[x_1,\ldots,x_{n+m}]}{I(X)^e+I(Y)^e} \cong \frac{\mathbf{C}[x_1,\ldots,x_n]}{I(X)} \otimes_\mathbf{C} \frac{\mathbf{C}[x_{n+1},\ldots,x_{n+m}]}{I(Y)}$$ (for $X,Y$ affine algebraic varieties), but apparently the proof was too long for his margin. How can this be done? Fundamental question aside : why is there no abstract nonsense tag?","['category-theory', 'commutative-algebra', 'algebraic-geometry']"
720516,Product of $1-\operatorname{cis}(2k\pi/n)$,"I'm in a question about polygonals and got stuck at a part. I have to prove that $$\prod_{k=1}^{n-1} \left(1 - \operatorname{cis}(\frac{2k\pi}{n})\right) = n$$ I've tried to multiply it to make $\operatorname{cis}(\frac{2k\pi}{n})$ transform to $\operatorname{cis}(\frac{2k\pi}{n})^n=1$, but it doesn't help.","['trigonometry', 'complex-numbers', 'products']"
720517,Verify the identity: $\tan^{-1} x +\tan^{-1} (1/x) = \pi /2$,"Verify the identity: $\tan^{-1} x + \tan^{-1} (1/x) = \frac\pi 2, x > 0$ $$\alpha= \tan^{-1} x$$ $$\beta = \tan^{-1} (1/x)$$ $$\tan \alpha = x$$ $$\tan \beta = 1/x$$ $$\tan^{-1}[\tan(\alpha + \beta)]$$ $$\tan^{-1}\left
[{\tan\alpha + \tan\beta\over 1 - \tan\alpha \tan\beta}
\right]$$ $$\tan^{-1}\left[
{x + 1/x\over 1- x/x }\right]$$ $$\tan^{-1}\left[{x + (1/x)\over 0} \right]$$ I can't find out what I'm doing wrong..",['trigonometry']
720522,Cauchy's Theorem and Cauchy's formula,"I came across the following problem in our last midterm exam. I am completely stuck as to how to begin the solution: If $|f(z)|\leq$ max $|f(z+re^{it})|$ ($0\leq t\leq 2\pi$), then $|f|$ has no strict local maximum within its domain of analyticity. Currently I am familiar with Cauchy's Theorem and formula. I am not sure why even the first inequality is true? Can anyone lead to some hints/solutions?","['multivariable-calculus', 'complex-numbers', 'cauchy-sequences', 'complex-analysis']"
720551,Alternatives to Rudin,I'm taking an advanced calculus class this semester and we've been using Rudin's Principles of Mathematical Analysis. I was wondering if anyone could suggest some good analysis textbooks aimed toward undergrad students that I can use as a supplement.,['analysis']
720554,Tricky Surface Parametrization,"I am to parametrize the surface given by the ellipse $$9(z-1)^2 + x^2 = 1$$ in the $xz$-plane and rotated about the $x$-axis. I then have to find the volume of the region enclosed. The concept of ""rotated about the $x$-axis"" is causing me some difficulty. I have come up with $$x = cos\theta$$$$z = (\frac1 3 sin\theta + 1)sin\phi$$ Which I am not even sure is right, and then the best I can get for $y$ is $$y = zcos\phi$$ There is a 3d render of it https://i.sstatic.net/rsWxJ.png Any help would be appreciated.",['multivariable-calculus']
720556,Discrete math functions help? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I'm doing a review for my discrete math test on functions and I'm having troubles with a few questions. Can I get some guidance in how to do these questions so I can be more prepared for the test? Thanks (b) Show that the 'rule' $g:Z_6\to Z_9$ defined by $f([a]_6) = [4a]_9$ is not a well-defined function. Define a function $f: N\times N \to N$ by $f((a,b)) = \gcd(a,b)$ (a) show that $f$ is not one-to-one (b) show that $f$ is onto Let $A$, $B$, $C$ be non-empty sets and let $f: A \to B$ and $g: B \to C$ be functions. (a) Show that it $g\circ f$ is onto, then $g$ is onto (b) Find an example of functions $f$ and $g$ such that $g\circ f$ is onto but where $f$ is not onto","['relations', 'discrete-mathematics', 'functions']"
720565,Prime Ideals as Ring theoretic Ultrafilters,"I am confused by the following statement in Awodey's Category Theory p. 35: Ring homomorphisms $A\to \mathbb Z$ into the initial ring $\mathbb Z$ play an analogous and equally important role [to that of boolean algebra morphisms $B\to \mathbb 2$] in algebraic geometry. They correspond to so-called prime ideals , which are the ring-theoretic generalizations of ultrafilters. This is particularly confusing to me since there are many rings which seem to have no morphisms to $\mathbb Z$. For example, any field has no unital ring morphisms into $\mathbb Z$ since units are preserved by ring morphisms.","['category-theory', 'ring-theory', 'algebraic-geometry']"
720605,Conclusion about cardinalty.,Assume that: $$\left| T \right| > {\aleph _0}$$ Why can't one assume immediately that: $$\left| T \right| \cdot \left| T \right| > \left| T \right| \cdot {\aleph _0}$$,"['cardinals', 'elementary-set-theory']"
720608,proving that a map $f : A \rightarrow B$ is a quotient map,"Suppose $A$ and $B$ are topological spaces such that $f : A \rightarrow B$ is a continuous surjective map. Assume that $\forall$ open set $U$ of $A$ its image is open. Then $f$ is a quotient map. The proof of this does not seem to bad, but I am still a little unsure. Usually when I think a proof is ""not to bad"" I start second guessing myself because I feel like it needs more when it doesn't. Is the proof of this as straight forward as I think it is or does it take a little more work? Maybe someone could show me their version of how to prove this.",['general-topology']
720621,finding bases for row space and null space of matrix.,"My problem is: For the matrix $$A = \begin{bmatrix}
      1&  4&  5&  6&  9\\
      3& −2&  1&  4& −1\\
     −1&  0& −1& −2& −1\\
      2&  3&  5&  7&  8\end{bmatrix}$$ (a) Find a basis for the row space of A. (b) Find a basis for the null space of A. (c) Find the rank and nullity of A. I tried searching online and I became more confused, take the example here. http://www2.kenyon.edu/Depts/Math/Paquin/PracticeExam1Solns.pdf As you can see for the column space he takes the columns of the original matrix instead of the rref of A, which I don't understand.","['matrices', 'linear-algebra']"
720628,Solving the functional equation $f(x)=f\left(\frac{x}{2}\right)+\frac{x}{2}\cdot f'(x)$,"find all functiions $f:\mathbb{R}\to\mathbb{R}$ such that $f'$ exists and $$f(x)=f\left(\frac{x}{2}\right)+\frac{x}{2}\cdot f'(x),\forall x\in\mathbb{R}$$","['ordinary-differential-equations', 'functional-equations']"
720649,Prove that a planar graph is connected if it has $p$ vertices and $3p-7$ edges,"Let $p$ be an integer so that $ p\ge3$ and let $G$ be a planar graph having $p$ vertices and $3p-7$ edges. Prove that $G$ is connected. I'm a little unsure of where to begin with this problem. It was suggested that I try to use Euler's Formula which states: Let $G$ be a connected graph with $p$ verticies and $q$ edges. If $G$ has a planar embedding with $f$ faces then $p-q+f=2$ I'm not sure how helpful this is since it doesn't prove connectedness, but is just a property of connected graphs with planar embeddings.","['graph-theory', 'combinatorics']"
720681,Open Sets Boundary question,"I'm really having a hard time with this problem: Find three disjoint, open sets in $\mathbb{R}$ (std. topology) that have the same nonempty boundary. I played around with a few ideas like $\mathbb{Q}$, {$\sqrt{p}+\mathbb{Q}$}, {$\sqrt{q}+\mathbb{Q}$} where $p$ and $q$ are distinct primes, but these sets aren't open. I can find examples that fit two of the conditions, but not all three.",['general-topology']
720690,global sections of structure sheaf of a projective scheme X over a field k?,"Some may have already asked this question.
What are the global sections of structure sheaf of a projective scheme $X$ over a field $k$? By Hartshorne page 18, Chapter 1, Theorem 3.4 , global sections will be $k$ when $k$ is algebraically closed and $X$ in some $\mathbb P^n$ is a projective variety. Could any one give a counter example for the case $k$ is not algebraically closed? 
(What will happen if $k=\mathbb R$, the real numbers, and $X=\mathrm{Proj}(\mathbb R[x,y] \mathop{/} {(x^2+y^2)})$ ?) If $X$ is an integral projective scheme of finite dimension over $k$ (algebraically closed), then $X$ is a projective variety by Hartshorne page 104, Chapter 2, Proposition 4.10 . And its global sections should be $k$. Do I need to fix some very ample sheaf to give an embedding to define its structure sheaf? This is my first time to ask a question. Welcome any advice. Thanks!","['algebraic-geometry', 'schemes', 'projective-schemes']"
720722,$Z(I:J)$ is the Zariski closure of $Z(I)-Z(J)$,Let $(I:J)$ denote the colon ideal (or ideal quotient ). It is pretty clear that the Zariski closure of $Z(I)-Z(J)$ is contained in $Z(I:J)$. How can we prove that the the Zariski closure of $Z(I)-Z(J)$ is precisely $Z(I:J)$?,"['commutative-algebra', 'algebraic-geometry']"
720733,Is $\frac{0}{0}$ different from $\frac{1}{0}$?,"In my mind, zero divided by zero answers the question of what $a$, when multiplied with zero, equals zero: $a * 0 = 0$ Obviously, any real number will satisfy this equation. However, one divided by zero is different. It answers this question: $a * 0 = 1$ This is different, because there are no solutions. Both results are referred to as ""undefined"". To me, these two ""types"" of undefined are completely different. I realize that many applications don't care whether there's no results or infinite results. However, am I correct in my assumption that there are two types of undefined here? And if so, is there any terminology differentiating them that I can search for, possibly related to sets? Question summary: Are there different types of undefined? If so, what are they? I'm sorry if this is a duplicate. I've searched, but it's kind of hard when you don't know what you're searching for. I have read this question , but it doesn't mention any specific terms.","['arithmetic', 'algebra-precalculus', 'infinity']"
720765,How to show a representation is irreducible?,"I have a professor who says that I should be able to show a representation is irreducible simply by looking at its trace (with other possible conditions), but after researching this for a while, I have still not been able to see this. I found a book in the library that considered the following representation of degree 2 given by $\hspace{90pt} U(x) = \left( \begin{array}{ccc}
0 & -1 \\
1 & 0 \end{array} \right) \hspace{10pt}$ and $\hspace{10pt}U(y) = \left( \begin{array}{ccc}
0 & -1 \\
1 & 0 \end{array} \right)$ then showed that there were complex numbers $\lambda, \mu, \alpha, \beta$ such that $\hspace{71pt} AU(x)A^{-1} = \left( \begin{array}{ccc}
\lambda & 0 \\
0 & \mu \end{array} \right) \hspace{10pt}$ and $\hspace{10pt} AU(y)A^{-1} = \left( \begin{array}{ccc}
\alpha & 0 \\
0 & \beta \end{array} \right) \hspace{10pt}$ where $A=\left( \begin{array}{ccc}
a & b \\
c & d \end{array} \right)$ with determinant not $0$. However, by taking determinants of both sides for each, and then taking the trace of each as well, $\mp i a=\pm a$. This only occurs when $a=0$ which implies $c=0$ making the $\det(A)=0$. This was a contradiction, proving that $U$ was irreducible. Does this kind of thing work in general? The book also talked about character tables, but I'm not really sure how they relate to the reducibility of a representation. I'm very new at this and my only background has been from Wikipedia, various webpages, and some books from the library. Any help would be appreciated. The book is ""Groups, Representations, and Characters"" by Victor E. Hill.","['representation-theory', 'group-theory', 'abstract-algebra', 'characters']"
720813,Do four dimensional vectors have a cross product property? [duplicate],"This question already has answers here : Cross product in $\mathbb R^n$ (5 answers) Closed 10 years ago . We know how to make cross product of three dimensional vectors.
$$ \vec A \times \vec B = \vec C$$ Where : $ \vec A = (A_i; A_j; A_k)$ $ \vec B = (B_i; B_j; B_k)$ $ \vec C = (C_i; C_j; C_k)$ $C_i = \left|\begin{matrix}A_j&A_k\\B_j&B_k\end{matrix}\right|$
$C_j = \left|\begin{matrix}A_k&A_i\\B_k&B_i\end{matrix}\right|$
$C_k = \left|\begin{matrix}A_i&A_j\\B_i&B_j\end{matrix}\right|$ But what about if we have four dimensional vectors? Is it possible to make cross product of four dimensional vectors? If it  is possible, then tell me when it can be possible? Let say we have two vectors: $ \vec A = (A_i; A_j; A_k; A_l)$ $ \vec B = (B_i; B_j; B_k; B_l)$ Then how to compute a cross product of this two vectors? Will it again vector?
$$ \vec A \times \vec B = \vec C$$ $ \vec C = (C_i; C_j; C_k; C_l)$ Then how to compute those coordinates? We know that only square matrices have a determinant property! In this case it might not be correct if we will wright... $\color{red}
{\text
{
$C_i = \left|\begin{matrix}A_j&A_k&A_l\\B_j&B_k&B_l\end{matrix}\right|$}   
C_j = \left|\begin{matrix}A_k&A_i&A_l\\B_k&B_i&B_l\end{matrix}\right|
C_k = \left|\begin{matrix}A_i&A_j&A_l\\B_i&B_j&B_l\end{matrix}\right|
C_l = \left|\begin{matrix}A_i&A_j&A_k\\B_i&B_j&B_k\end{matrix}\right|}$ So tell me how to solve this problem?","['vector-spaces', 'geometry', 'vectors', 'cross-product', 'linear-algebra']"
720856,How many elements of order 7 in a simple group of order 168?,"I'm not sure my proof is correct because I don't use the fact that the group is simple anywhere... Since $168=2^3 \cdot 3 \cdot 7$, there exists a subgroup of order 7. Let $n_7$ be the number of Sylow 7-subgroups. Since $n_7 | 24$ and $n_7 \equiv 1$ mod $7$, $n_7 = 8$. Since every group of prime order is cyclic, there exist 8 elements of order 7. I think that I still need to prove that there are no other elements of order 7 and that's where the fact that the group is simple would come in.","['sylow-theory', 'group-theory']"
720867,Rationalization of $\frac{2\sqrt{6}}{\sqrt{2}+\sqrt{3}+\sqrt{5}}$,"Question: $$\frac{2\sqrt{6}}{\sqrt{2}+\sqrt{3}+\sqrt{5}}$$ equals: My approach: I tried to rationalize the denominator by multiplying it by $\frac{\sqrt{2}-\sqrt{3}-\sqrt{5}}{\sqrt{2}-\sqrt{3}-\sqrt{5}}$ . And got the result to be (after a long calculation): $$\frac{\sqrt{24}+\sqrt{40}-\sqrt{16}}{\sqrt{12}+\sqrt{5}}$$ which is totally not in accordance with the answer, $\sqrt{2}+\sqrt{3}-\sqrt{5}$ . Can someone please explain this/give hints to me.","['rationalising-denominator', 'radicals', 'fractions', 'algebra-precalculus']"
720874,asymptotic normality and central limit theorem,"Here's the question Can somebody explain the difference between asymptotic normality and central limit theorem? 
They seem very similar to me.","['statistics', 'probability', 'statistical-inference']"
720891,Can compact sets completey determine a topology?,"Suppose that $\tau_1$ and $\tau_2$ are two topologies on a set $X$ with the property that $K\subset X$ is compact with respect to $\tau_1$ if and only if $K$ is compact with respect to $\tau_2$. Then is this enough information to determine whether $\tau_1 = \tau_2$? If not (which is suspect to be the case) is there a nice counterexample, and what is the minimum amount of extra information required for $\tau_1 = \tau_2$? I feel like there would be an issue regarding 'points at infinity.'","['general-topology', 'examples-counterexamples', 'compactness']"
720898,"Show $(S^1\times [0,1])/$~ is homeomorphic to $D^2$ [duplicate]","This question already has an answer here : $(S^1 \times [0,1])/\sim$ homeomorphic to unit disk $ D^2$ (1 answer) Closed 5 years ago . Define an equivalence relation on $S^1\times[0,1]$ by: $(x,t)$~$(y,s) \iff xt=st$. Show that $(S^1\times [0,1])/$~ is homeomorphic to the unit disc $D^2$. My attempt: Let $g: S^1\times[0,1]\to D^2$ by $g(x,t)=xt$. Now $g(x,t)=g(y,s)\iff xt=ys\iff (x,t)$~$(y,s)$. This shows two things: that $g$ is injective (forward direction) and $g$ is constant on each equivalence class of $S^1\times [0,1]$ (reverse direction). The latter means that $g$ induces a map $f:(S^1\times [0,1])/$~$\to D^2$. Now $S^1$ and $[0,1]$ are compact, so $S^1\times [0,1]$ is compact, and so $(S^1\times[0,1])$/~ is compact by the projection map $\pi$. We know $g$ is a surjection as well because for $y\in D^2$, we have $g(y/\|y\|,y)=y$. Since $g=f\circ\pi$, we have that $f$ is bijective, and we have that $f$ is continuous by the universal mapping property of quotients. Hence, $f$ is a continuous bijection from a compact space to a Hausdorff space, so it is also a homeomorphism. Does anyone see any problems with my argument?",['general-topology']
720900,"Classifying the factor group $(\mathbb{Z} \times \mathbb{Z})/\langle (2, 2) \rangle$","We wish to classify the factor group $(\mathbb{Z} \times \mathbb{Z})/\langle (2, 2) \rangle$, that is, find a group to which it is isomorphic. (According to the fundamental theorem of finitely generated abelian groups. Initially, I thought the group had but two cosets, forcing an isomorphism to $\mathbb{Z}_2$. Obviously, this is wrong, due to the existence of cosets such as $(1, 0) + \langle (2, 2) \rangle$ However, I am unable to see how I am to find an isomorphism here.","['group-theory', 'abstract-algebra']"
720921,Can anyone help to get rid of this infinity-infinity?,How does one get rid of the infinities arising here? $$\lim_{x\to\infty}\left(\frac{\ln|x-1|}3-\frac{\ln|x+2|}3\right)$$ I really have no idea how to handle such natural logarithms.,['limits']
720924,Why do we require radians in calculus?,"I think this is just something I've grown used to but can't remember any proof. When differentiating and integrating with trigonometric functions, we require angles to be taken in radians. Why does it work then and only then?","['calculus', 'integration', 'trigonometry', 'derivatives', 'unit-of-measure']"
720935,Historic proof of the area of a circle,"The area of a circle radius $R$ is $\pi R^2$ which is quite easy to prove with integral calculus. Consider a ring of radius $\mathrm{d}r$ at a distance $r$ from the centre. This ring has area $2\pi r \mathrm{d}r$. Integrating, $$\int_0^R2\pi r \mathrm\,{d}r=\pi R^2$$ But calculus is a relatively new tool, while the area of a circle has been known for what I presume since Archimedes at least. So what is the historic derivation for this? I think it is something related to the method of exhaustion which is in essence a primitive form of integration, the Wikipedia article states Archimedes used the method of exhaustion as a way to compute the area inside a circle by filling the circle with a polygon of a greater area and greater number of sides. The quotient formed by the area of this polygon divided by the square of the circle radius can be made arbitrarily close to $π$ as the number of polygon sides becomes large, proving that the area inside the circle of radius $r$ is $πr^2$, $π$ being defined as the ratio of the circumference to the diameter $\frac{C}{d}$ or of the area of the circle to the square of its radius $A/r^2$. I am unclear as to how Archimedes would have proved that these two definitions of $\pi$ are in fact equivalent and refer to the same constant. Maybe in his proof of $\pi r^2$ the equality emerges but I have been unable to find any reference to or a description of how Archimedes proved this. I don't know what to tag this, apart from reference request, so feel free to re-tag.","['geometry', 'math-history', 'reference-request']"
720938,Finitely generated k-algebra is Noetherian,"If any finitely generated algebra is Noetherian, does that mean that $k[x]$ itself is Noetherian? But in this case we can take ideal $k[x^2] \subset k[x]$ which is not finitely generated.","['finitely-generated', 'algebraic-geometry', 'noetherian']"
720968,limit $\lim_{n \to \infty} \frac{(2n+1)(2n+2) n^n}{(n+1)^{n+2}}$?,Wolfram alpha says that this limit (arising from a ratio test to determine the radius of convergence of a series) should be $4/e$. How does it get this result?,"['sequences-and-series', 'calculus']"
720969,Proving well definedness of addition in real numbers constrructed from cauchy sequences.,"While studying real analysis, I got confused on the following issue. Suppose we construct real numbers as equivalence classes of cauchy sequences. Let $x = (a_n)$ and $y= (b_n)$ be two cauchy sequences,  representing real numbers $x$ and $y$. Addition operation $x+y$ is defined as $x+y = (a_n + b_n)$. To check if this operation is well defined, we substitute $x = (a_n)$ with some real number $x' = (c_n)$ and verify that $x+y  = x'+y$. We also repeat it for $y$. i.e. we verify that  $x+y = x+y'$. Question: Instead of checking that $x+y = x+y'$ and $x'+y = x+y$ seperately, would it suffice to check that $x+y = x' + y'$ in a single operation in order to show that addition is well defined for real numbers. Would it hurt to checking well  definedness? Can any one explain me the logic behind ?","['real-numbers', 'proof-verification', 'real-analysis']"
720971,what do free variable and leading variables mean?,"What do the leading variables and free variables in a matrix mean? I have the system below and am trying to understand which are which. I searched a lot for this, please help me ! $$w + x + y + z = 6 \qquad w + y + z = 4 \qquad w + y = 2$$","['matrix-equations', 'matrices', 'linear-algebra']"
720981,How to find eigenvalues and eigenvectors of this matrix,"Can you help to find eigenvalues and eigenvectors of the following matrix? Here is the matrix: $$C = \small
\begin{pmatrix}
-\sin(\theta_{2} - \theta_{M}) & \sin(\theta_{1} - \theta_{M}) & 0 & \ldots & 0 & \sin(\theta_{2} - \theta_{1}) \\
 \sin(\theta_{3} - \theta_{2}) & -\sin(\theta_{3} - \theta_{1}) & \sin(\theta_{2} - \theta_{1}) & \ldots & 0 & 0 \\
 0 & \sin(\theta_{4} - \theta_{3}) & -\sin(\theta_{4} - \theta_{2}) & \ldots & 0 & 0 \\
0 & \ddots & \ddots & \ddots & \ddots & \vdots\\
\sin(\theta_{M} - \theta_{M - 1}) & 0 & 0 & \ldots & \sin(\theta_{1} - \theta_{M}) & -\sin(\theta_{1} - \theta_{M - 1}) \\
\end{pmatrix}
$$ where $0 = \theta_{1} < \ldots < \theta_{M} < 2 \pi$. I have found 2 vectors for eigenvalue 0: $$
v_{1} = (1, \cos(\theta_{1}), \ldots, \cos(\theta_{M - 1}))^{T}
$$ $$
v_{2} = (0, \sin(\theta_{1}), \ldots, \sin(\theta_{M - 1}))^{T}
$$ In case $\theta_{j} = \frac{2 \pi (j - 1)}{M}, \; j = 1, \ldots, M$ the matrix becomes circulant matrix , and it's obvious to find all its eigenvectors and eigenvalues. If it's essential for somebody, I can explain the application where this matrix appears. In article by Lele, Kulkarni, Willsky - ""Convex-polygon estimation from support-line measurements"" this matrix represents the conditions of consistent support function measurements. Each $i$-th row of matrix represent conditions that $i$-th descrete radius of curvature is positive. These are the criteria of consistency. Here is the detailed description of geometrical interpretation of this matrix. Defintion 1. The support function of convex body $K$ in $\mathbb{R}^{n}$ is defined as follows: $$
h_{K}(u) = \sup \limits_{x \in K} (x, u)
$$ Each value of support function corresponds to 1 support hyperplane of convex body. Suppose that $n = 2$ (we work in plane). Maybe this picture will help you to understand what we are speaking about: We consider $M$ measurements $h_{1}, \ldots, h_{M}$ of support function at directions $u_{1}, u_{2}, \ldots, u_{M}$ (all $||u_{j}|| = 1$, and $u_{j} = (\cos \theta_{j}, \sin \theta_{j}$)), and want to reconstruct the body $K$ from these measurements. In ideal, we can just intersect half-spaces corresponding to support hyperspaces: $$
K = \bigcap \limits_{i = 1}^{M} \{x \in \mathbb{R}^{n} : \; (x, u_{i}) < (h_{i}, u_{i})\}
$$ But measurements are noisy, so there could be no convex body that has such support function values at these directions. So we need to correct them. To do this we need some conditions representing the consistency of support function measurements. Here are these conditions: Theorem 1. A vector $h \in \mathbb{R}^{M}$ is the vector of support function measurements of some convex body if an only if $$
C h \geq 0
$$ The value $h_{i - 1} \sin(\theta_{i + 1} - \theta_{i}) - h_{i} \sin(\theta_{i + 1} - \theta_{i - 1}) + h_{i + 1} \sin(\theta_{i} - \theta_{i - 1})$ is the value of $i$-th descrete radius of curvature. If all these values are positive, then the measurements are consistent. The vectors $$
v_{1} = (1, \cos(\theta_{1}), \ldots, \cos(\theta_{M - 1}))^{T}
$$ $$
v_{2} = (0, \sin(\theta_{1}), \ldots, \sin(\theta_{M - 1}))^{T}
$$ are just vectors of support function values of convex bodies consisting only from one point: $(1, 0)$ for $v_{1}$ and $(0, 1)$ for $v_{2}$. Adding these vectors and their combinations to other support function measurements does not change descrete radio of curvature and correspond to translation of the convex body in $\mathbb{R}^{n}$.","['geometry', 'eigenvalues-eigenvectors', 'differential-geometry', 'matrices', 'linear-algebra']"
720984,Derivation of slope of line formula,The formula for slope of a line as we know: $y_2 - y_1/x_2 - x_1$ or just rise / run What is the derivation for this formula? E.g. Why is it not rise times run for example?,['algebra-precalculus']
720991,projective non-singular curve,"I am working on algebraic curves at the moment and I can not find a proper definition of the projective non-singular curves. My goal is understand that the category of non-singular projective curves is equivalent to the finite generated field extension of k of transendence degree 1.
Now an abstract non singular curves is just a open subset over $C_k$ where this is just the set of all dvr. But what is a projective non-singular curve.
I am glad for any hints.","['algebraic-geometry', 'algebraic-curves']"
720992,Solve $2xy'' + 5y' + xy = 0$ using Frobenius method?,"Using the Frobenius method, solve the differential equation $2xy'' + 5y' + xy = 0$. 
I've done most of the work, but when it comes to getting the indicial equation I am getting stuck. When working with the sums, all summations start at $n=0$, and one starts at $n=2$. So, combining the sum gives a sum starting at $n=2$, leaving behind the $n=0$ and $n=1$ terms of the other sums. This leads to two indicial equations, and I don't know if that should be possible or not. Thanks in advance.","['ordinary-differential-equations', 'summation']"
721007,Bijection with empty intersection [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Let $A$ be an arbitrary set. How can we construct a set $B$, in bijection with $A$, such that 
$A \cap B=\emptyset$?",['elementary-set-theory']
721056,Trigonometric limit $\lim_{x\to0} \frac{\tan^2{(3x)}+\sin{(11x^2)}}{x\sin{(5x)}}$,How to solve this limit: $$\lim_{x \rightarrow 0}\frac{\tan^2{(3x)}+\sin{(11x^2)}}{x\sin{(5x)}}$$,"['trigonometry', 'calculus', 'limits']"
721070,How to solve this elliptic integral ??,Can anyone explain to me how to find the integral ? $$ \int_0^1\sqrt{9x^4+4x^2+1}dx =? $$,"['definite-integrals', 'integration']"
721076,Help with using the Runge-Kutta 4th order method on a system of 2 first order ODE's.,"The original ODE I had was $$ \frac{d^2y}{dx^2}+\frac{dy}{dx}-6y=0$$ with $y(0)=3$ and $y'(0)=1$. Now I can solve this by hand and obtain that $y(1) = 14.82789927$. However I wish to use the 4th order Runge-Kutta method, so I have the system: $$
\left\{\begin{array}{l}
  \frac{dy}{dx} = z \\
  \frac{dz}{dx} = 6y - z
\end{array}\right.
$$
With $y(0)=3$ and $z(0)=1$. Now I know that for two general 1st order ODE's $$ \frac{dy}{dx} = f(x,y,z) \\ \frac{dz}{dx}=g(x,y,z)$$ The 4th order Runge-Kutta formula's for a system of 2 ODE's are: $$ y_{i+1}=y_i + \frac{1}{6}(k_0+2k_1+2k_2+k_3) \\ z_{i+1}=z_i + \frac{1}{6}(l_0+2l_1+2l_2+l_3) $$ Where $$k_0 = hf(x_i,y_i,z_i) \\ k_1 = hf(x_i+\frac{1}{2}h,y_i+\frac{1}{2}k_0,z_i+\frac{1}{2}l_0) \\ k_2 = hf(x_i+\frac{1}{2}h,y_i+\frac{1}{2}k_1,z_i+\frac{1}{2}l_1) \\ k_3 = hf(x_i+h,y_i+k_2,z_i+l_2) $$ and $$l_0 = hg(x_i,y_i,z_i) \\ l_1 = hg(x_i+\frac{1}{2}h,y_i+\frac{1}{2}k_0,z_i+\frac{1}{2}l_0) \\ l_2 = hg(x_i+\frac{1}{2}h,y_i+\frac{1}{2}k_1,z_i+\frac{1}{2}l_1) \\ l_3 = hg(x_i+h,y_i+k_2,z_i+l_2)$$ My problem is I am struggling to apply this method to my system of ODE's so that I can program a method that can solve any system of 2 first order ODE's using the formulas above, I would like for someone to please run through one step of the method, so I can understand it better.","['runge-kutta-methods', 'ordinary-differential-equations', 'systems-of-equations', 'numerical-methods']"
721096,A formula in 'The Matrix Cookbook',"In section 9.4 (Idempotent Matrices), the book says that : if $A$ is idempotent, which means that $AA = A$, then $f(sI + tA) = (I-A)f(s) + Af(s+t)$ but I don't understand the meaning of this formula, can anyone tell me where does it come from or show me how to proof this formula? Thanks.",['matrices']
721108,How to prove that superadditive function has this property? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $f: [0, \infty) \to \mathbb R$ be a function satisfying the following conditions: For any $x,y \ge 0$ , $f(x+y) \ge f(x) + f(y)$ . For any $x \in [0,2]$ , $f(x) \ge x^2 - x$ . Prove that, for any positive integer $M$ and positive reals $n_1,\dots,n_M$ with $n_1+\dots+n_M = M$ , we have $$f(n_1)+\dots+f(n_M) \ge 0$$ How can I prove this statement?","['real-analysis', 'functional-equations']"
721150,"Why does the ""T=0"" method to calculate tangent work?","Given a random equation of a curve: $ax^2 + 2hxy + by^2 + 2gx + 2fy + c = 0$. Suppose we need to find the tangent to this curve at any point $A(x_1, y_1)$. A method given to me by my professor was the 'T' method: The 'T' form of an equation can be obtained by replacing: $$x^2 \rightarrow xx_1$$
  $$y^2 \rightarrow yy_1$$
  $$x \rightarrow \frac{x + x_1}{2}$$
  $$y \rightarrow \frac{y + y_1}{2}$$
  $$xy \rightarrow \frac{xy_1 + x_1y}{2}$$ The tangent to the curve is then the equation $T=0$. For instance, if we need to find the tangent at $(2, 2)$ to the parabola $y^2 - 2x=0$: $T =0$: $\implies yy_1 - 2\frac{x+x_1}{2} = 0$ Substituting $x_1 = 2$ and $y_1 = 2$: $$2y - (x+2) = 0$$ $$\implies 2y - x = 2$$ which is the required tangent. I don't understand how this works ! Could someone help me understand why it does? This also works for other cases like: Deriving the equation of the two tangents to a curve from a certain external point: $SS_1 = T^2$ (S is the equation of the curve and S1 is the value given by the equation when the point is substituted into it (the power of the point wrt the curve))","['geometry', 'conic-sections', 'circles']"
721154,Showing that $\phi:\Bbb R^2 \to \Bbb R^2$ is injective,"I need to show that $$\phi:(x,y)\to(\sin\frac{y}{2}-x, \sin\frac{x}{2}-y)$$ Is a $C^1$-diffeomorphism. So, I need to show it's injective. How can I do this? Just explicitly setting $\phi(x, y)=\phi(x', y')$ just leads to a non-linear system that I can't even begin to solve. Is there some trick for showing that a multivariable function is injective?",['multivariable-calculus']
721155,Is this matrix decomposition possible?,"Given a $2\times2$ matrix $S$ with entries in $\mathbb{Z}$ or $\mathbb{Q}$ , when is it possible to write $S=\frac{1}{3}(ABC+CAB+BCA)$ such that $A+B+C=0$, where $A, B, C$ are matrices over the same ground ring as S. Always? How would I find $A, B, C$?","['matrices', 'ring-theory', 'linear-algebra']"
721176,A question on Fourier Transform,Is there a function which is not absolutely integrable but which has a continuous fourier transform? I know that if a function is absolutely integrable then the fourier transform is continuous but I want to know whether the converse is false. Any help is appreciated . Thanks,"['functions', 'fourier-analysis', 'examples-counterexamples', 'real-analysis']"
721178,How do we make this integration rigorous?,"This is from Jaynes, Probability Theory: The Logic of Science , pp 27-28. We have a function $F$ which is $\mathbb{R}^2 \rightarrow \mathbb{R}$, and we set $v = F(y,z)$. We discover that $$ F_1(y,z) = { \partial F \over \partial y }  = { H(v) \over H(y) } \\
   F_2(y,z) = { \partial F \over \partial z }  = r{ H(v) \over H(z) } $$ where $H$ is arbitrary, but can't change sign in the region of interest. (Specifically, $H$ is such that $F_2(y,z) / F_1(y,z)$ takes the form $r H(y) / H(z) $.) We later discover $r = 1$, so I'm going to ignore that for clarity. Then, since $\mathrm d v = F_1 \mathrm d y + F_2 \mathrm d z $, we get $$ { \mathrm d v \over H(v) } = { \mathrm d y \over H(y) } + { \mathrm d z \over H(z) } $$ So far, so good. Now we define $$ w(x) = \exp\left( \int^x { \mathrm d x \over H(x) } \right) $$ and it follows that $w(v) = w(y) w(z)$. I can kinda sorta see how this happens, but not really. Apparently, $$ \int { \mathrm d v \over H(v) } = \int \left( { \mathrm d y \over H(y) } + { \mathrm d z \over H(z) } \right)  = \int { \mathrm d y \over H(y) } + \int{ \mathrm d z \over H(z) } $$ And then we just apply $e^{a+b} = e^a e^b$. This makes some sense notationally, but I'm not familiar with the rigor behind it. (I've removed the ${}^x$ from the integrals because I don't really know what I'd do with it. Jaynes credits this proof to Cox (1961), which I looked up - it didn't include any intermediate steps to help me, but it also omitted the ${}^x$, so I feel somewhat comfortable doing so myself.) I think there are a few specific things confusing me, where the notation just doesn't mean what I expect it to: I expect expressions like $\int^x f(s) \mathrm d s$ to be a function of $x$, where $s$ is a bound variable, and we can rewrite $s$ as $p$ or $\alpha$ or anything, and rewriting $s$ as $x$ is just about the most confusing choice we can possibly make. In this case, it seems that the symbol used inside the integration is relevant? $w$ looks like a function $\mathbb R \rightarrow \mathbb R $, but $w(v)$ depends upon $\mathrm d v$ as well as $v$? Can anyone clear this up for me?",['multivariable-calculus']
721184,$\operatorname{div}$ and $\operatorname{grad}$ in spherical coordinates. Formula from general relativity goes crazy,"I try to calculate the gradient of a function and the divergence of a vector field in spherical coordinates. Nothing special so far, but a formula that I learned in a general relativity lecture creates confusion. First of all consider a vector field $f$ and a function $h$ on $\mathbb R^3$. Of course in spherical coordinates we have \begin{align*}
\operatorname{div} f &= \frac{1}{r^2}\partial_r (r^2 f^r) + \frac{1}{r \sin \theta} \partial_\theta (\sin \theta f^\theta) + \frac{1}{r \sin \theta} \partial_\varphi f^\varphi\\
\operatorname{grad} h &= e_r \partial_r h + e_\theta \frac{1}{r}\partial_\theta h + e_\varphi \frac{1}{r \sin \theta} \partial_\varphi h 
\end{align*} In the general relativity lecture I learned the two formulas $$\nabla_\mu f^\mu = \frac{1}{\sqrt{|\operatorname{det} g|}}\partial_\mu (\sqrt{|\operatorname{det}g|}f^\mu)$$ and $$\nabla h = (\nabla h)^\nu \partial_\nu = g^{\mu\nu}\partial_\mu h \partial_\nu $$ where the Riemannian Metric in spherical coordinates reads $$g = \text d r^2 + r^2 \text d \theta^2 + r^2 \sin^2 \theta \text d \varphi^2$$ so that $$\sqrt{|\operatorname{det}g|} = r^2 \sin \theta$$ If I now use the two formulas I get different results for the gradient and the divergence: \begin{align*}
\nabla_\mu f^\mu &= \frac{1}{r^2} \partial_r (r^2 f^r) + \frac{1}{\sin\theta}\partial_\theta(\sin\theta f^\theta) + \partial_\varphi f^\varphi\\
\nabla h &= \partial_r h \partial_r + \frac{1}{r^2}\partial_\theta h \partial_\theta + \frac{1}{r^2 \sin^2 \theta}\partial_\varphi h \partial_\varphi
\end{align*} Can someone tell me what went wrong? I guess that something is wrong when I compare the unit vector notation $e_i$ with $\partial_i$","['riemannian-geometry', 'vector-analysis', 'differential-geometry', 'spherical-coordinates']"
721195,"""Poissonization"" and intuition","In a french book, ""Calcul des probabilités"" from Foata and Fuchs, I found this theorem, which they call ""Poissonization"". ""Let $(I_k)_{k \in \mathbb{N}}$ be a sequence of independent variables with Bernoulli distribution, each of the same parameter, $p$. Let $N$ be a variable integer-valued independent from the $I_k$, and define $N_1:= \sum_{k=1\ldots N} I_k$, and $N_2:=\sum_{k=1\ldots N} (1−I_k)$. Then if $N$ has a Poisson distribution with parameter $\lambda$, then $N_1$ and $N_2$ are independent, and have Poisson laws, of parameters $\lambda p$ and $\lambda (1-p)$. Conversely, if $N_1$ and $N_2$ are independent, then $N$ has a Poisson distribution."" What does this situation modelize ? Is there a natural problem where this situation arises ? I really don't understand what I am dealing with.","['intuition', 'probability']"
721209,Shortest distance between ellipse and a line,"I was trying to find the shortest distance between the ellipse $$\frac{x^2}{4} + y^2 = 1$$ and the line $x+y=4$. We have to find the point on the ellipse where 
its tangent line is parallel to $x+y=4$ and find the distance between those two points. 
However, when I used the implicit differentiation, I get $$\frac{x}{2} + 2y\frac{dy}{dx} = 0$$
$$\frac{dy}{dx} = \frac{-x}{4y}$$ If it's parallel to $x+y=4$, then we need $x=4y$. Do I just plug it into ellipse equation and solve for it and calculate the distance between the point and a line or am I doing it wrong? I just wanted to clarify. Any help would be appreciated. Thanks!","['multivariable-calculus', 'calculus']"
721244,Similarity between two curves,"I am trying to find out how well a deterministic version of a MATLAB program predicts the stochastic version. I don't know what statistical test/quantitative analysis to use. 
This is what the output of the program looks like:","['statistics', 'stochastic-processes', 'matlab']"
721283,"Show that $\,\,n!<\mathrm{e}\left(\frac{n}{2}\right)^n$","I'd like to prove that  $\,\,n!<\mathrm{e}\left(\frac{n}{2}\right)^n$. What I have so far: $$\sqrt[n]{n!} = \sqrt[n]{1\cdot 2 \cdot \ldots \cdot n} \leq \frac{1+\ldots +n}{n}=\frac{(n+1)n}{2n}=\frac{(n+1)}{2}.$$ Thus $$\,\,n!<\mathrm{e}\left(\frac{n}{2}\right)^n.$$ But how do I go from $n+1$ to $n$?","['inequality', 'sequences-and-series', 'calculus', 'exponential-function', 'factorial']"
721297,Writing a system of ODEs in polar coordinates,"I have this system of equations: $$\dot{x}=x-y-x(x^2+y^2)+\frac{xy}{\sqrt{x^2+y^2}} \\
\dot{y}=x+y-y(x^2+y^2)-\frac{x^2}{\sqrt{x^2+y^2}}$$ How can I get this in polar coordinates ? I know that $r^2=x^2+y^2$, but how can I find $\dot{r}$ or $\dot{\theta}$ ?","['polar-coordinates', 'ordinary-differential-equations', 'systems-of-equations']"
721298,Applications of Belyi's theorem,"Belyi's theorem (1979)  is stated as follows: A smooth projective curve over $\mathbb C$ is defined over a number field if and only if there
  exists a finite morphism (of varieties over $\mathbb C$)
  $t:X\longrightarrow\mathbb P^1_\mathbb C$ with at most $3$ critical
  values. I'd like to know if there are some important applications of this theorem. Thanks in advance.","['riemann-surfaces', 'algebraic-geometry', 'algebraic-curves']"
721301,Integrating $ \int \limits_{-\infty}^{\infty} \frac{\sin^2(x)}{x^2} \operatorname d\!x $ [duplicate],"This question already has answers here : Proof of $\int_0^\infty \left(\frac{\sin x}{x}\right)^2 \mathrm dx=\frac{\pi}{2}.$ (15 answers) Closed 7 years ago . I'm trying to evaluate $\displaystyle \int \limits_{-\infty}^{\infty} \dfrac{\sin^2(x)}{x^2} \operatorname d\!x $. My first though was to use residue calculus, since we've got the pole of order 2 there at the origin.  Given that the pole actually lies on the real axis though, I'd guess I'd have to use some sort of keyhole contour to include the pole and then limit.  Maybe it's just one of those days, but I'm not seeing an obvious contour to use here.  Any help would be appreciated.","['integration', 'complex-analysis', 'contour-integration']"
721304,Arround Cauchy Schwarz Inequality in semi riemannian geometry,"I have a question.
I seen that Cauchy Schwarz inequality is not valide in the case of pseudo riemannian metric because it is not positive ( or negative) define, I would like to know if there is special cases where this inequality holds, for exemple for spacelike, timelike, .. cases.
Thank you",['differential-geometry']
721341,Poisson distrubution proof question.,"I was reading over the proof for the Poisson distribution and came across this sentence:
 ""But since $$\left[1-\frac{\lambda}{n}\right]^n\rightarrow e^{-\lambda}$$ as $$n\rightarrow\infty$$, ...""
Can someone explain how they have arrived at that result and why the above term doesn't simply become $$1^n=1$$? By the way $$\lambda=np$$ where p is the probability and n is the number of data points.","['statistics', 'calculus', 'probability']"
721348,"isomorphism between divisible, totally ordered, abelian groups","Let $G$, $H$ be divisible, abelian, linearly ordered groups, whose cardinalities are equal and satisfy $\mu := |G|=|H|>\aleph_{0}$. These are supposed to be (order!) isomorphic. And just about every text, that I have looked at, points this out, only without proof. How does one demonstrate the isomorphism? (Obvious as groups they are isomorphic, as they form $\mathbb{Q}$-vector spaces with the same dimension, but here the order structure plays a pivotal role.) I can at most show, there are $\mathcal{G}, \mathcal{H}\subseteq\mathcal{P}(\mu)$ ultrafilters and $\phi:G\to\prod_{\mu}H\ /\ \mathcal{H}$ and $\psi:H\to\prod_{\mu}G\ /\ \mathcal{G}$ monomorphisms (but not first-order embeddings). More at this stage, not. Can one somehow out of these construct an iso? Or is this a wrong way? Thanks in advance! EDIT: I had misread. The existence of an isomorphism refers just to divisible, totally ordered groups. The real claim is simply, that the groups are elementarily equivalent — and a proof eludes me in these books. Would someone kindly point in a right direction, how to prove this?","['abelian-groups', 'model-theory', 'order-theory', 'logic', 'group-theory']"
721360,1-manifold is orientable,"I am trying to classify all compact 1-manifolds. I believe I can do it once I can show every 1-manifold is orientable. I have tried to show prove this a bunch of ways, but I can't get anywhere. Please help, Note, I am NOT assuming that I already know the only such manifolds are [0,1] or $S^1$. This is my end goal.","['compact-manifolds', 'manifolds', 'differential-geometry']"
721364,Why don't taylor series represent the entire function?,"Say, I have a continuos function that is infinitely differentiate on the interval $I$. It can then be written as a taylor series. However, taylor series aren't always completely equal to the function - in other words, they don't necessarily converge for all $x$ in $I$. Why? The way I think of taylor series is that if you know the position , velocity, acceleration, jolt etc. of a particle at one moment in time, you can calculate its position at any time. Taylor series not converging for all $x$ suggests there's a limitation on this analogy. So why do taylor series ""not"" work for some $x$? Using the particle analogy, described above shouldn't taylor series allow you to find the ""location"" of the function at any ""time""? Please note, I am not looking for a proof - I'm looking for an intuitive explanation of why taylor series don't always converge for all $x$.","['sequences-and-series', 'calculus', 'intuition', 'taylor-expansion']"
721378,When does distribution convergence imply expectation convergence?,"If $X_n \xrightarrow{d} X$ what are the minimal hypothesis to have $E[X_n]\rightarrow E[X]$ ? For example I think that if all the second moments are bounded it's true, but I'm not sure if is true if the first moments are bounded.","['probability-theory', 'probability', 'random-variables']"
721392,Having trouble integrating for use of energy method to prove uniqueness,"We are given $u_{tt} - c^2u_{xx} + ru_t$.  To prove only one solution exists, I am taking w = $u_1 - u_2$, assuming they are both solutions to the given wave equation. So: $u_{tt} - c^2u_{xx} + ru_t$ which becomes: $w_{tt} - c^2w_{xx} + rw_t$ I know we are suppose to then multiply both sides by $w_t$ and integrate to show that E(t) is decreasing, but I am confused when it comes to the integration.  Please do NOT solve the entire question, I'm simply seeking help for the integration.  Once I understand that part, I will post the rest of the problem and look for opinions on how I reached the final answer. Thanks!","['ordinary-differential-equations', 'partial-differential-equations']"
721399,Quotient Topology = Coproduct,"Quotient topology seems to satisfy the universal property for coproducts at first glance. However, at second glance they seem to fail to fit into that frame in general since not every map passes to the quotient. So, do I miss some argument or is it simply that the quotient topology belongs to another categorical notion? To be more precise: The quotient topology satisfies a universal property in the sense: A surjective map from some initial topological space induces a unique topology on its codomain s.t. whenever theres a map from the quotient space to some arbitrary topological space then it is continuous iff its composition with the quotient map is continuous. ... But this is somehow stated not in the right direction as it is given for coproducts, i.e. given a map rather from the initial topological space there exists a unique continuous map from the quotient space. This fails to exist iff the map is not constant on fibres.","['general-topology', 'category-theory']"
721418,Elementary proof of prime number theorem?,"From Wikipedia : ""The prime number theorem is also equivalent to: $$\lim_{x \rightarrow \infty} \frac{\psi(x)}{x}=1$$ where $$\psi(x) = \sum\limits_{n \leq x} \Lambda(n)$$
is the Chebyshev function .
and where:
$$\Lambda(n) = \begin{cases} \log p & \text{if }n=p^k \text{ for some prime } p \text{ and integer } k \ge 1, \\ 0 & \text{otherwise.} \end{cases}$$
is the von Mangoldt function . The von Mangoldt function can be calculated as . Edit 27.1.2018 , I rewrote the whole question from here on, trying to use more conventional notation. $n$ stands for row index, and $k$ stands for column index. Mathematica knows that: $$\log(n)=\lim\limits_{s \rightarrow 1}\zeta(s)\left(1-\frac{1}{n^{s-1}}\right) \; \; \; \; \; \; (1)$$
and it has been proven that for $n>1$ the von Mangoldt function is:
$$\Lambda(n)=\lim\limits_{s \rightarrow 1}\zeta(s)\sum\limits_{d|n} \frac{\mu(d)}{d^{s-1}} \; \; \; \; \; \; (2)$$
The Dirichlet series associated with the von Mangoldt function defined in such away  as above, is the infinite symmetric square matrix $T_1$: $$T_1=a(GCD(n,k)) \; \; \; \; \; \; (3)$$ which starts: $$T_1 = \left(   \begin{array}{ccccccc}   +1&+1&+1&+1&+1&+1&+1&\cdots \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&-2&+1&+1&-2&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&+1&+1&-4&+1&+1 \\ +1&-1&-2&-1&+1&+2&+1 \\ +1&+1&+1&+1&+1&+1&-6 \\ \vdots&&&&&&&\ddots \end{array}   \right)$$ where: $$a(n) = \sum\limits_{d|n} d \cdot \mu(d) \; \; \; \; \; \; (4)$$ (better known as the Dirichlet inverse of the Euler totient.) Now by periodicity of the entries in the columns for $m=0,1,2,3,4,5,...$ or $m \in \mathcal ℕ_0$, the column sums satisfy: $$\sum\limits_{n=1+m}^{n=k+m} T_1(n,k)=\begin{cases}1 & \mbox{ if } k=1\\ 0&\mbox{ if } k>1.\end{cases}  \; \; \; \; \; \; (5)$$ and by periodicity of the entries in the rows, the row sums satisfy: $$\sum\limits_{k=1+m}^{k=n+m} T_1(n,k)=\begin{cases}1 & \mbox{ if } n=1\\ 0&\mbox{ if } n>1.\end{cases} \; \; \; \; \; \; (6)$$
by symmetry. Since from $(2)$ for $n>1$: $$\Lambda(n)=\sum\limits_{k=1}^{k=\infty}\frac{T_1(n,k)}{k} \; \; \; \; \; \; (7)$$ and: $$\sum\limits_{n=1+m}^{n=k+m} \frac{T_1(n,k)}{k} =\begin{cases}1 & \mbox{ if } k=1\\ 0&\mbox{ if } k>1.\end{cases}  \; \; \; \; \; \; (8)$$ and since the denominator is $k$ in $(8)$ and the length of the period in the $k$-the column is also $k$, we can say that the average contribution to $\Lambda(n)$ from an entry $T_1(n,k)$ for $k>1$ in matrix $T_1$ must be: $$\sum\limits_{n=1+m}^{n=k+m} \frac{T_1(n,k)}{k} =\begin{cases}1 & \mbox{ if } k=1\\ 0&\mbox{ if } k>1.\end{cases}  \; \; \; \; \; \; (9)$$ Writing:
$$\sum_{n \leq x} \Lambda(n)=\sum_{n \leq x} \sum\limits_{k=1}^{k=\infty}\frac{T_1(n,k)}{k} \; \; \; \; \; \; (10)$$ Combining $(9)$ with the right hand side of $(10)$ and multiplying the denominators $k \cdot k = k^2$ we get: $$\sum_{n \leq x} \Lambda(n)=\sum_{n \leq x} \sum\limits_{k=1}^{k=\infty}\sum\limits_{n=1+m}^{n=k+m} \frac{T_1(n,k)}{k^2} =\begin{cases} (1+o(1))x & \mbox{ if } k=1\\ (0+o(1))x &\mbox{ if } k>1.\end{cases}  \; \; \; \; \; \; (11)$$ and thereby at least heuristically: $$\sum_{n \leq x} \Lambda(n) = (1+o(1)) x  \; \; \; \; \; \; (12)$$ $\square$ Associated Mathematica 8 code: nn = 7
a[n_] := If[n < 1, 0, Sum[d MoebiusMu@d, {d, Divisors[n]}]] 
MatrixForm[
 Table[Table[If[k == 1, 0, a[GCD[n, k]]], {k, 1, nn}], {n, 1, nn}]]
MatrixForm[
 Table[Table[
   Sum[If[k == 1, 0, a[GCD[n, k]]], {k, 1, x}], {x, 1, nn}], {n, 1, 
   nn}]]
MatrixForm[
 Table[Table[
   Sum[If[k == 1, 0, a[GCD[n, k]]], {k, 1, x}]/x, {x, 1, nn}], {n, 1, 
   nn}]] Edit 18.2.2018: I will try to add some details. Starting with matrix $T_1$ defined above, we form the matrix $T_2$, but before that we state again the property of matrix $T_1$, namely: $$\Lambda(k) = \sum\limits_{n=1}^{n=\infty}\frac{T_1(n,k)}{n}$$ and then matrix $T_2$: $$T_2(n,k)=\sum\limits_{k=1}^{k=g}T_1(n,k)$$
where:
$$g=1,2,3,4,5,...$$
and:
$$n=1,2,3,4,5,...$$ Matrix $T_2$ in turn has the property: $$\psi(x)=\sum\limits_{k \leq x} \Lambda(k) = \sum\limits_{n=1}^{n=\infty}\frac{T_2(n,k)}{n}$$ We then form matrix $T_3$: $$T_3(n,k)=\frac{T_2(n,k)}{n \cdot k}$$ Since for $n>1$: $$\lim\limits_{k \rightarrow \infty} T_3(n,k) = 0$$ and since: $$\lim\limits_{k \rightarrow \infty} T_3(1,k) = 1$$ the prime number theorem is true/follows. $\square$ In case I did not get it entirely right I attach this second program in Mathematica as a verification: (*start*)
nn = 12;
TableForm[
  A = Table[Table[If[Mod[n, k] == 0, 1, 0], {k, 1, nn}], {n, 1, nn}]];
TableForm[
  B = Table[
    Table[If[Mod[k, n] == 0, MoebiusMu[n]*n, 0], {k, 1, nn}], {n, 1, 
     nn}]];
TableForm[T1 = (A.B)];
TableForm[
 T2 = Table[Table[Sum[T1[[n, k]], {k, 1, g}], {g, 1, nn}], {n, 1, nn}]]
TableForm[T3 = Table[Table[T2[[n, k]]/n/k, {k, 1, nn}], {n, 1, nn}]]
(*end*)","['elementary-number-theory', 'sequences-and-series', 'proof-verification']"
721432,What to do when the second derivative test fails?,"What do we do when the second derivative test fails? For example, I'm asked to find all the critical points of the function
$$f(x,y)=x^{2013}−y^{2013}$$ And determine the nature of the critical points. The critical point that I have found is at $(0,0)$, but I'm unable to determine its nature as the second derivative test fails here.",['multivariable-calculus']
721448,Unique representation of a degenerate simplex,"I recently learned about simplicial sets, and now I'm studying some basic properties. I've learned that every degenerate simplex is a degeneracy of a unique non-degenerate ( nice ) simplex. However, I suspect that the entire representation must also be unique. (This question started as a request for help when I was stuck, but while I was writing the post I figured it out myself, so I end up asking if you could check my proof.) This means, given a nice simplex $x$  and $$\large
z = s_{j_n}\cdots s_{j_1}x = s_{i_n}\cdots s_{i_1}x,
\qquad 0\le j_1 <\cdots <j_n, \;\; 0\le i_1 <\cdots< i_n
$$
does this imply that $j_k=i_k$ for all $k$? If $s_j x = s_i x,\ i<j$, for a simplex $x$, then 
$$\large
x = d_i s_i x = d_i s_j x = s_{j-1} d_i x
$$
So if $x$ is nice, then $s_j x \ne s_k x$. This can be seen as the induction start. Assume that the claim is true for $n$. 
Let's first consider the case $j_{n+1} > i_{n+1}+1$. If we apply $d_{j_{n+1}}$ to both sides, we get
$$\large
d_{j_{n+1}} z = s_{j_n} \cdots s_{j_1} x = 
s_{i_{n+1}} \cdots s_{i_n}d_{j_{n+1}-n-1} x 
$$
which means that $d_{j_{n+1}} z$ is degeneracy of $x$ and of $d_{j_{n+1}} x$, but then $d_{j_{n+1}}x$ has to be a degeneracy of $x$. We can conclude that  $j_{n+1}\in\{i_{n+1},i_{n+1}+1\}$. Clearly, $j_{n+1} = i_{n+1}$ implies by induction that $j_k = i_k$ for all $k$. But what if $j_{n+1}=i_{n+1}+1$? We can write $$\large\begin{align}
s_{i_n}\cdots s_{i_1} x
& = d_{i_{n+1}} s_{i_{n+1}} \cdots s_{i_1} x \\
& = d_{i_{n+1}} s_{i_{n+1}+1} \cdots s_{j_1} x \\
& = s_{i_{n+1}} d_{i_{n+1}} s_{j_n} \cdots s_{j_1} x
\end{align}$$
If $i_{n+1} > j_n +1$, the last line would transform into an $(n+1)$-fold degeneracy of $d_{i_{n+1}-n} x$. So $d_{i_{n+1}}s_{j_n}$ must vanish, and we can use the induction hypothesis to conclude  that $i_n = i_{n+1}$, which is a contradiction. Is this proof correct? And can it be simplified? Thanks.","['simplicial-stuff', 'category-theory', 'proof-verification', 'combinatorics']"
721449,What are all positive divisors of 7 factorial?,"I need to determine all the positive divisors of 7!. I got 360 as the total number of positive divisors for 7!. Can someone confirm, or give the real answer?","['discrete-mathematics', 'divisibility']"
721459,Poisson distribution proof question,I'm reading over the Poisson distribution proof and trying to understand how $$\frac{n(n-1)\cdots(n-k+1)}{(n-\lambda)(n-\lambda)\cdots(n-\lambda)}$$ tends to 1 as $$n\rightarrow\infty\text{ ?}$$ Thanks.,"['statistics', 'calculus', 'probability']"
721474,Proof about lognormal distribution,"I'm trying to prove a result about the lognormal distribution that seems to me to be fairly intuitive, but I can't get the proof to work. Basically, I'd like to prove that as the mean increases, the expected value below a certain threshold (say $x^*$) increases more by raising the variance than by lowering the mean. Formally, let $V = \int_0^{x^*} x f(x) dx$, where $f(x)$ is the lognormal pdf with mean $y-a$ and variance $\sigma^2$. I'd like to show $\frac{\partial^2V}{\partial y \partial a} < \frac{\partial^2V}{\partial y \partial \sigma}$ I'd really appreciate any help.","['statistics', 'calculus']"
721504,Row swap changing sign of determinant,"I was wondering if someone could help me clarify something regarding the effect of swapping two rows on the sign of the determinant. I know that if $A$ is an $n\times n$ matrix and $B$ is an $n\times n$ matrix obtained from $A$ by swapping two rows, then $$\det(B)=-\det(A)$$ but I don't know how to prove this. I have been looking for proofs at internet, and read in both in textbooks and lectures notes that are available that this result is very hard to prove and most approaches rely on induction and so was wondering if there is something wrong with using that $\det(AB)=\det(A)\det(B)$ and then writing $B=EA$ where $E$ is an elementary matrix swapping two rows and using this result to get $\det(B)=\det(E)\det(A)=-\det(A)$ (since showing that $\det(E)=-1$ in this case is not that hard).","['permutation-matrices', 'permutations', 'matrices', 'linear-algebra', 'determinant']"
721511,"Isomorphism between Möbius transformations and $SL(2,\mathbb{C})/\mathbb{Z}_2$","This is a problem from A Course in Modern Mathematical Physics by Peter Szekeres. Here's the quote to the problem I'm solving: Show that the map $\mu$ from $SL(2,\mathbb{C})$ to the Möbius group is a homomorphism, and that the kernel of this homomorphism is $\{I;-I\}$; i.e the Möbius group is isomorphic to  $SL(2,\mathbb{C})/\mathbb{Z}_2$. What I did was to define $$\mu:\left( \begin{array}{ccc}
a & b \\
c & d \end{array} \right)\rightarrow m(z)=\frac{az+b}{cz+d}$$ where $ad-bc=1$. By taking the image of two unimodular matrix product gives us the composition of two Möbius transformations which is exactly the result we expect, it is easy to see why $\{I;-I\}$ (identity matrix and the ""negative"" identity matrix) is the kernel of this homomorphism, by the theorem that goes: If $\varphi: G\rightarrow G'$ is a homomorphism then the factor group $G/\ker(\varphi)$ is isomorphic with the image subgroup $im(\varphi)\subseteq G' $ I can prove that the group of the Möbius transformations is isomorphic to $SL(2,\mathbb{C})/\{I;-I\}$, can't find a way to prove that it is isomorphic to $SL(2,\mathbb{C})/\mathbb{Z}_2$ because i thought that you could only factor a group by a normal subgroup of that group, I don't know if it is possible to factor it by a group wich is isomorphic the normal subgroup of that group. If that were the case, I could prove that $\{I;-I\}$ is isomorphic to $\mathbb{Z}_2$ and then prove that $SL(2,\mathbb{C})/\mathbb{Z}_2 \cong SL(2,\mathbb{C})/\{I;-I\}$ which by transitivity should be isomorphic to the group os Möbius transformations. If this is not posible, i don't see how $\mathbb{Z}_2$ is a normal subgroup of $SL(2,\mathbb{C})$.","['group-theory', 'abstract-algebra', 'linear-groups']"
721513,Showing the sequence converges to the square root [duplicate],"This question already has answers here : Proof of Convergence: Babylonian Method $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$ (9 answers) Closed last year . For any $a > 0$ , I have to show the sequence $x_{n+1}=$ $ \frac 12\left(x_n+  \frac {a} {x_n}\right)$ converges to the square root of $a$ for any $x_1>0$ . If I assume the limit exists ( denoted by $x$ ) then, $x= \frac 12\left(x+  \frac {a} {x}\right)$ can be solved to $x^2 = a$ How could I show that it does exist?","['sequences-and-series', 'limits']"
721523,Smith Normal Form and classification of factor groups according to the theorem of finitely generated abelian groups,"With reference to this question, it was mentioned in the comments that these problems could be solved using the Smith Normal Form . However, I am unable to extract an exact method from the Wikipedia article alone, and further searching yields few general examples. I am asking for an example where the algorithm is used to classify a factor group.","['smith-normal-form', 'group-theory', 'abstract-algebra']"
