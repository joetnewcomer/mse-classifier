question_id,title,body,tags
171120,Proof that operator is compact,"Prove that the operator $T:\ell^1\rightarrow\ell^1$ which maps $x=(x_1,x_2,\dots)$ to $\left(x_1,\frac{x_2}{2},\frac{x_3}{3},\dots\right)$ is compact. For an arbitrary sequence $x^{(N)}\in\ell^1$ one would have extract a convergent subsequence of $T x^{(N)}$. Maybe via the diagonal argument?","['operator-theory', 'compact-operators', 'functional-analysis', 'real-analysis']"
171126,A permutation in $S_n$ is 'regular' if and only if it is a power of an n-cycle?,"Define a permutation $\alpha\in S_n $ to be regular if either $\alpha$ has no fixed points and it is the product of disjoint cycles of the same length, or $\alpha=(1)$ . Prove that $\alpha$ is regular if and only if $\alpha$ is a power of an n-cycle. It is a homework question in J.J.Rotman's book: A first course in abstract algebra with applications . I have just begun reading the book, and I find this question confusing----I've tried using induction, but couldn't figure out a good way. I would really appreciate your help.",['abstract-algebra']
171127,Functional equations of (S-shape?) curves,"I am looking for the way to ""quite easily"" express particular curves using functional equations. What's important (supposing the chart's size is 1x1 - actually it doesn't matter in the final result): obviously the shape - as shown in the picture; there should be exactly three solutions to f(x) = x: x=0, x close to or equal 0.5 and x=1; (0.5,0.5) should be the point of intersection with y = x; it would be really nice, if both of the arcs are scalable - as shown in the left example (the lower arc is more significant than the upper one). I've done some research, but nothing seemed to match my needs; I tried trigonometric and sigmoid functions too, they turned out to be quite close to what I want. I'd be grateful for any hints or even solutions. P.S. The question was originally asked at stackoverflow and I was suggested to look for help here. Some answers involved splines, cumulative distribution functions or the logistic equation. Is that the way to go?","['plane-curves', 'functions']"
171140,"minimum requirement to be $f=g$ , $f$, $g$ are holomorphic","Given that $f,g:\mathbb{C}\rightarrow \mathbb{C}$  are holomorphic, $A=\{x\in\mathbb{R}:f(x)=g(x)\}$. The minimum requirement for $f=g$ is $A$ is uncountable $A$ has positive lebesgue measure $A$ contains a nontrivial interval $A=\mathbb{R}$ By identity theorem to be $f=g$ we just need a limit point inside $A$, so If $A$ has positive lebesgue measure then it will contain a interval so which will have one limit point. so $2$ is correct?",['complex-analysis']
171141,"lim sup of sequence of continuous function from $[0,1]\rightarrow [0,1]$","$f_n:[0,1]\to [0,1]$ be a continuous function and let $f:[0,1]\to [0,1]$ be defined by $$f(x)=\operatorname{lim\;sup}\limits_{n\rightarrow\infty}\; f_n(x)$$ Then $f$ is continuous and measurable continuous, but need not be measurable measurable, but need not be continuous need not be measurable or continuous. I guess $3$ is correct, but I'm not able to prove it.","['measure-theory', 'sequences-and-series', 'real-analysis', 'limsup-and-liminf']"
171166,$\tbinom{2p}{p}-2$ is divisible by $p^3$,"The problem is as follows:
Let $p>3$ be a prime. Show that $\tbinom{2p}{p}-2$ is divisible by $p^3$. The only thing I can think of is that $(2p)!-2(p!)^2$ is divisible by $p^2$ which doesn't help me much. Can someone point me in the right direction? Is there a combinatorial approach to this problem?
Thanks","['elementary-number-theory', 'combinatorics']"
171173,find all points for intersection between 2 polar equations,"I stumped at one of the exercise in my multivariable calculus textbook. I try to search online but I can't seem to search on how answer no 3 and 4 below is derived. I also plot both of polar coordinates in mathematica but I can't seem to get around on how to solve them algebraically. The question and answer as appear in textbook is as below: The polar equation for the curve $C_{1}$ is $r=\cos 2\theta$,while the curve $C_{2}$ is described by the polar equation $r=1+\cos\theta$. Find all points at which $C_{1}$ and $C_{2}$ intersect. Solution below 1) r=0 (the origin) 2) ($\frac{5-\sqrt{17}}{4}$ , $\pm\arccos \frac{1-\sqrt{17}}{4}$) (I manage this solve this, just put both equation equal to each other) I'm at loss what the formula to derive answer for 3 and 4 below. 3) ($-1$ , $\pm 90 ^\circ$) 4) ($\frac{-1}{2}$ , $\pm 60 ^\circ$) Can anyone point out to me what's the formula used ? Thanks","['multivariable-calculus', 'polar-coordinates']"
171175,Definition of stalk as a colimit of sheaves,"I'm trying to understand the category-theoretic proof that sheafification preserves stalks, using adjoints, as outlined e.g. in this mathoverflow answer: https://mathoverflow.net/questions/77581/sheaf-valued-functors-how-much-can-you-prove-about-them-just-using-category-theo What is bugging me is that I'm having a hard time seeing why the colimit described is actually isomorphic to the stalk (as a skyscraper sheaf based at x using the stalk as the fixed set). Specifically, in his answer, Ryan Reich claims that for a sheaf $F$ on $X$, the stalk $F_x$ is the colimit over opens $U$ containing $x$ of the sheaves $j_* j^* F$, where $j: U \rightarrow X$ is the inclusion map. Every time I try to show that the skyscraper sheaf satisfies the universal property I end up in a seeming dead end. I feel like it shouldn't be THAT hard, and that I must be missing something or confusing myself somehow. Does anyone know a reference for this, or be willing to explain why it works? I'd be very grateful.",['algebraic-geometry']
171176,Is the function $y=\ln x^2$ the same as $y=2\ln |x|$?,"Suppose I have a function
$$y=\ln x^2$$
Then is this function the same as
$$y=2\ln |x|?$$","['logarithms', 'functions']"
171179,Throwing balls into $b$ buckets: when does some bucket overflow size $s$?,"Suppose you throw balls one-by-one into $b$ buckets, uniformly at random. At what time does the size of some (any) bucket exceed size $s$? That is, consider the following random process. At each of times $t=1, 2, 3, \dots$, Pick up a ball (from some infinite supply of balls that you have). Assign it to one of $b$ buckets, uniformly at random, and independent of choices made for previous balls. For this random process, let $T = T(s,b)$ be the time such that At time $T-1$ (after the $T-1$th ball was assigned), for each bucket, the number of balls assigned to it was $\le s$. At time $T$ (after the $T$th ball was assigned), there is some bucket for which the number of balls assigned to it is $s + 1$. What can we say about $T$? If we can get the distribution of $T(s,b)$ that would be great, else even knowing its expected value and variance, or even just expected value, would be good. Beyond the obvious fact that $T \le bs+1$ (and therefore $E[T]$ exists), I don't see anything very helpful. The motivation comes from a real-life computer application involving hashing (the numbers of interest are something like $b = 10000$ and $s = 64$).","['stochastic-processes', 'discrete-mathematics', 'computer-science', 'probability', 'combinatorics']"
171182,Countably compact paracompact space is compact,"The proof that I have seen for the result ""countably compact paracompact implies compact"" involves metacompactness, which follows from paracompactness. I wonder if it can be proved without going through metacompactness.","['general-topology', 'compactness']"
171191,Why does synthetic division work?,"Synthethic division is commonly taught, but I have never actually had a proof/explanation shown to me. Why does it work? Work So Far I related the ""$x$"" to powers to 10, and then proceeded to relate synthetic division to non-polynomial division, but couldn't seem to find the correlation. Research So Far My teacher doesn't seem to have a valid explanation for why it works. A google search doesn't provide any good results either. All I seem to get is a Yahoo answers link with a badly formatted proof that makes it hard to understand and a physics forum link that links synthetic division to ""normal division"" by relating the ""x"" to 10, a conclusion I have already arrived at.","['algebra-precalculus', 'polynomials']"
171204,Approximated solution to differential equation in the form $f(u)u'^2+(u-u_0)^2=k$,"I'm trying to solve the following differential equation, that arises from conservation of energy in a physical problem. $R,k$ are constants.
$$(1+R^4u^4)u'^2+(u-u_0)^2=k$$
Now, according to my book I should find ""approximate solutions"" around the point of equilibrium $u_0$. I started my reasoning with simpler equations and I have some questions: Is there a general good method to solve equations of the form $1/2k_1u'^2+1/2k_2u^2=k_3$? What I usually do is to resort to the physical problem of an harmonic oscillator with mass $k_1$, elastic constant $k_2$ and energy $k_3$ to find a cosinusoidal solution. I can find the amplitude but not the phase difference. In alternative I take the derivative of both sides of the equation and solve the second order linear differential equation that arises. I can find the amplitude of the oscillations by substitution of $u=A\cos t$ in the original equation. Are there more direct approaches? In the case of an equation of the form $1/2k_1u'^2+1/2k_2(u-u_0)^2=k_3$ I solve them either by inspection or by sobstituting $\xi = u-u_0,\ \xi'=u'$. Is that procedure correct? Now the original problem $(1+R^4u^4)u'^2+(u-u_0)^2=k$. My book gives what I think is a wrong answer : $u_0+(\sqrt{k}/u_0)\cos(\sqrt{1+R^4u_0^2})$. How to obtain the correct solution? I thought to substitute $u^4 = u_0^4$ in the leftmost term. Am I allowed to do this or it would be too rough an approximation? If I approximate even to first order $u^4$ the differential equation becomes too difficult for me","['ordinary-differential-equations', 'physics']"
171207,Why these conditions make this map open?,"If $A \subset \mathbb{R}^n$ is an open set and $g: A \to \mathbb{R}^n$ is an injective continuously differentiable function such that $\forall x \in A, \, \det g'(x) \neq 0$, does $g(U)$ is open for each $U \subset A$ open? Why? This is about p. 67 of Spivak's Calculus on Manifolds (down of the page), where he says: ""the collection of all $g(U)$ is an open cover of $g(A)$"".","['general-topology', 'multivariable-calculus']"
171208,Probabilistic Sieve of Eratosthenes,"Consider the following algorithm: function Rand():
    return a uniformly random real between 0.0 and 1.0

function Sieve(n):

    assert(n >= 2)

    for i = 2 to n
        X[i] = true

    for i = 2 to n
        if (X[i])
            for j = i+1 to n
                if (Rand() < 1/i)
                    X[j] = false

    return X[n] What is the probability that Sieve(k) returns true as a function of k ?  What is the limit of this probability as k goes to infinity (if it has one) ?","['probability', 'algorithms', 'number-theory']"
171214,"$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x$ by substitution","I'm trying to solve the following integral:
$$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x$$
Using the substitution method with the substitution $u = \sin\left(x\right)$. The exercise has two parts: the first one is using the substitution $u = \cos\left(x\right)$. No problem.
I'm having difficulties with the second part, which is using the substitution $u = \sin\left(x\right)$. I spent a couple of hours with the exercise before asking here, and after some trials I got this:
$$\int f\left(g\left(x\right)\right)g'\left(x\right)\,\mathrm{d}x = \int f\left(u\right)\,\mathrm{d}u$$
$$g\left(x\right) = \sin\left(x\right)$$
$$g'\left(x\right) = \cos\left(x\right)$$
$$f\left(x\right) = \frac{x}{\cos^2\left(\arcsin(x)\right)} = \frac{x}{1 - x^2}$$
$$\int f\left(u\right)\,\mathrm{d}u = -\frac{1}{2}\log|1 - u^2| + C = -\frac{1}{2}\log|1 - \sin^2\left(x\right)| + C$$
$$1 - \sin^2\left(x\right) = \cos^2\left(x\right)$$
$$\int\frac{\sin\left(x\right)}{\cos\left(x\right)}\,\mathrm{d}x = -\frac{1}{2}\log|\cos^2\left(x\right)| + C = -\log|\cos\left(x\right)| + C$$
But it feels too complicated, $f\left(x\right)$ was really hard for me to discover. What am I missing?","['calculus', 'integration']"
171222,Number of irreducible polynomials with degree $6$ in $\mathbb{F}_2[X]$,"I'm looking for the number of irreducible polynomials with degree $6$ in $\mathbb{F}_2[X]$ with leading coefficient $1$ . First question: The leading coefficient $1$ is redudant because every polynomial of degree $6$ has leading coefficient $1$ ? My solution: The polynomials must be of the form $$ x^6 + \dots + 1$$ to ensure the degree of $6$ and to ensure that $0$ is not a root. Now we have $$ \dots + \alpha_5x^5+ \dots +\alpha_1x+ \dots$$ with $\alpha_i \in \mathbb{F}_2$ . To avoid that $1$ is root, an odd number of $\alpha_i$ must be $1$ . The number of combinations for that would be $\frac{2^{5-1}}{2}= 8.$ Is that correct?","['finite-fields', 'irreducible-polynomials', 'abstract-algebra']"
171226,"Stuck with handling of conditional probability in Bishop's ""Pattern Recognition and Machine Learning"" (1.66)","I've just started working through the book, and I'm stuck with how the author handles conditional probability in (1.66). The context is as follows. In this chapter we are working with a curve fitting task: we try to fit a polynomial $\sum w_ix^i$ to a training set $\{\mathbf {x}, \mathbf {t}\}$ with  an assumption of Gaussian noise, i.e. for a single observed value $t$ for $x$, $p(t|x,\mathbf{w},\beta)=N(t|\sum w_ix^i,\beta^{-1})$, and assuming independence of data points, the likelihood is  $p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)=\prod N(t_n|\sum w_ix^i,\beta^{-1}).$ Prior distribution of $\mathbf{w}$ is a multivariate Gaussian: $p(\mathbf{w}|\alpha)=N(\mathbf{0},\alpha^{-1}\mathbf{I})$. Now, the author states: ""Using Bayes’ theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function: $$p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta)\propto p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha) \tag{1.66}$$ Later, this proportionality is used to maximize the probability on the left to obtain MAP value of $\mathbf{w}$, so significant factors cannot be simply omitted on the right. This is where I'm stuck. The problem is I don't understand how he applies Bayes here. I tried to derive it, and that's what I've got: $$p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta)\propto p(\mathbf{x},\mathbf{t},\mathbf{w},\alpha,\beta)=p(\mathbf{t}|\mathbf{x,w},\alpha,\beta)p(\mathbf{x,w},\alpha,\beta)\tag{A}$$ where the latter equals to $p(\mathbf{w}|\alpha,\beta,\mathbf{x})p(\alpha,\beta,\mathbf{x})$. I see that we can get rid of the second factor here because it is not really interesting if we want to maximize the expression — these are just model parameters or the data which is given. Also, I see that we can rewrite the first factor as $p(\mathbf{w}|\alpha)$. That's why: say we have $p(A|BC)\text{, then } p(A|BC)=\frac {p(ABC)}{p(BC)}=\frac {p(AB)p(C)}{p(B)p(C)}=p(A|B)$, which holds if both ($AB$ and $C$) and ($B$ and $C$) are independent, and indeed both $\mathbf{w}$ and $\alpha$ are independent with both $\beta\text{ and }\mathbf{x}$. But I don't see why we can ""remove"" $\alpha$ from the first factor of (A). For one, $\mathbf{w}$ and $\alpha$ are not independent. Maybe I don't understand the problem well enough? Maybe this probability can be factorized so that we can say ""this factor is irrelevant,let's hide it under $\propto$""? So, the questions: 1) Please help with understanding of the proportionality (1.66). 2) It's hard for me to see the benefit of using conditional distributions on things like $\alpha$ and $\beta$. Is, for example, $p(\mathbf{w})$ of any interest in this task? I don't see how any meaning can be attributed to it. Could someone explain that? 3) Is there some common knowledge about conditioning on several random variables, like the one I used ($p(A|BC)=p(A|B)$ if both pairs ($AB$ and $C$) and ($B$ and $C$) are independent) , that make it obvious?","['bayesian', 'pattern-recognition', 'probability', 'machine-learning']"
171228,Remembering exact sine cosine and tangent values?,"There exists a common trick to remember exact sine cosine and tangent values. The trick is relatively long, so instead of reposting it, please refer to my answer on this page . Although I have used this trick for a while, I've never understood why it works. I understand the tangent values (sine/cosine according to right angle ratios) and the why the cosine values are in ""the opposite order"" of sine values (due to the cosine function being a $90^\circ$ phase shift on the  sine function) but why does doing the above trick provide the correct values for sine (and cosine in opposite order)?",['trigonometry']
171233,If $f:D\to \mathbb{R}$ is continuous and exists $(x_n)\in D$ such as that $x_n\to a\notin D$ and $f(x_n)\to \ell$ then $\lim_{x\to a}f(x)=\ell$?,"Assertion: If $f:X\setminus\left\{a\right\}\to \mathbb{R}$ is continuous and there exists  a sequence $(x_n):\mathbb{N}\to X\setminus\left\{a\right\}$ such as that $x_n\to a$ and $f(x_n)\to \ell$ prove that $\lim_{x\to a}f(x)=\ell$ I have three questions: 1) Is the assertion correct? If not, please provide counter-examples. In that case can the assertion become correct if we require that $f$ is monotonic, differentiable etc.? 2)Is my proof correct? If not, please pinpoint the problem and give a hint to the right direcition. Personally, what makes me doubt it are the choices of $N$ and $\delta$ since they depend on another 3)If the proof is correct, then is there a way to shorten it? My Proof: Let $\epsilon>0$. Since $f(x_n)\to \ell$ 
 \begin{equation}
\exists N_1\in \mathbb{N}:n\ge N_1\Rightarrow \left|f(x_n)-\ell\right|<\frac{\epsilon}{2}\end{equation}
Thus, $\left|f(x_{N_1})-\ell\right|<\frac{\epsilon}{2}$ and by the continuity of $f$ at $x_{N_1}$,
 \begin{equation}
\exists \delta_1>0:\left|x-x_{N_1}\right|<\delta_1\Rightarrow \left|f(x)-f(x_{N_1})\right|<\frac{\epsilon}{2}
\end{equation}
Since $x_n\to a$,
 \begin{equation}
\exists N_2\in \mathbb{N}:n\ge N_2\Rightarrow \left|x_n-a\right|<\delta_1\end{equation}
Thus, $\left|x_{N_2}-a\right|<\delta_1$ and by letting $N=\max\left\{N_1,N_2\right\}$,
 \begin{gather}
0<\left|x-a\right|<\delta_1\Rightarrow \left|x-x_N+x_N-a\right|<\delta_1\Rightarrow \left|x-x_N\right|-\left|x_N-a\right|<\delta_1\\
0<\left|x-a\right|<\delta_1\Rightarrow \left|x-x_N\right|<\delta_1+\left|x_N-a\right|
\end{gather}
 By the continuity of $f$ at $x_N$,
 \begin{equation}
\exists \delta_3>0:0<\left|x-x_N\right|<\delta_3\Rightarrow \left|f(x)-f(x_N)\right|<\frac{\epsilon}{2}
\end{equation}
Thus, letting $\delta=\max\left\{\delta_1+\left|x_N-a\right|,\delta_3\right\}>0$ we have that,
 \begin{gather}
0<\left|x-a\right|<\delta\Rightarrow \left|x-x_N\right|<\delta\Rightarrow \left|f(x)-\ell+\ell-f(x_N)\right|<\frac{\epsilon}{2}\Rightarrow \left|f(x)-\ell\right|-\left|f(x_N)-\ell\right|<\frac{\epsilon}{2}\\
0<\left|x-a\right|<\delta\Rightarrow\left|f(x)-\ell\right|<\left|f(x_N)-\ell\right|+\frac{\epsilon}{2}<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon
\end{gather}
We conclude that $\lim_{x\to a}f(x)=\ell$ Thank you in advance EDIT: The proof is false. One of the mistakes is in this part: ""Thus, letting $\delta=\max\left\{\delta_1+\left|x_N-a\right|,\delta_3\right\}>0$ we have that,
 \begin{gather}
0<\left|x-a\right|<\delta{\color{Red} \Rightarrow} \left|x-x_N\right|<\delta{\color{Red} \Rightarrow} \left|f(x)-\ell+\ell-f(x_N)\right|<\frac{\epsilon}{2}\end{gather}""","['calculus', 'continuity', 'real-analysis', 'limits']"
171241,Approximations for the partial sums of exponential series,"Though the question here ( Partial sums of exponential series - Stack Exchange ) is similar, it is more specialized and I rather need a general approximation for an arbitrary partial sum. Essentially, I am trying to approximate the probability mass function of a particular random variable and I ended up with a Poisson random variable's CDF in the mix. Hence, for my purpose, I need to figure out a reasonable approximation of the sum: $\displaystyle\sum_{k = 0}^{r} \frac{\lambda^k}{k!}$ OR the tail, i.e. $\displaystyle\sum_{k = r}^{\infty} \frac{\lambda^k}{k!}$ Does someone know some approximations for this? Also, if there exist conditions for those approximations to be valid, I'd like to know them as well. Thanks in advance! Addendum: There appears to be a closed form expression for such a partial sum: $\displaystyle\sum_{k = 0}^{r} \frac{\lambda^k}{k!} = e^\lambda \frac{\Gamma(r + 1, \lambda)}{\Gamma(r + 1)}$, where $\Gamma(a, x)$ is defined as: $\displaystyle \Gamma(a, x) = \int_x^\infty t^{a - 1} e^{-t} \,dt$ and $\displaystyle \Gamma(a) = \Gamma(a, 0)$. Is there a simple closed form approximation for the Gamma function? At the end of the day, somehow or the other, I either end up with a summation sign or an integral. I just want to be able to pin down this partial sum as a numeric quantity, that is reasonably approximate.","['approximation', 'real-analysis']"
171249,Question about proof that multiplication in Banach algebra is continuous,"Here's the proof in my notes: Where does the last inequality come from? If I want to show that it's continuous at $((x,y)$ I can use the inverse triangle inequality to get 
$$ (\|x^\prime\| + \|y\|)\varepsilon \leq (\|x\| + \|y \| + \varepsilon)\varepsilon$$ Thanks.","['normed-spaces', 'functional-analysis', 'banach-spaces']"
171258,Recovering the topology of an affine scheme from the specialization preorder,"Let $A$ be a commutative ring. The specialization preorder on $\mathrm{Spec}(R)$ is given by $\mathfrak{p} \prec \mathfrak{q} \Leftrightarrow \mathfrak{p} \in \overline{\{\mathfrak{q}\}} \Leftrightarrow \mathfrak{q} \subseteq \mathfrak{p}$. Is it possible to recover the topology on $\mathrm{Spec}(A)$ from this preorder? If $A$ is noetherian (or just $A_{red}$ noetherian, which has the same spectral space), then the closed subsets are the finite unions of irreducible closed subsets, and the irreducible closed subsets are precicely those of the form $\{\mathfrak{q} : \mathfrak{q} \prec \mathfrak{p}\}$ for some $\mathfrak{p}$. Thus, in this case, we may recover the topology. Is it possible to do so in general? More formally, assume that $X$ is a set, endowed with two spectral topologies. Assume that their specialization preorders are the same. Does this imply that the topologies coincide? Probably not. But what are interesting additional assumptions on these topologies (not on the rings) which make it true?","['general-topology', 'algebraic-geometry']"
171259,Equivalent Definitions of the Operator Norm,"How do you prove that these four definitions of the operator norm are equivalent? $$\begin{align*}
\lVert A\rVert_{\mathrm{op}} &= \inf\{ c\;\colon\; \lVert Av\rVert\leq c\lVert v\rVert \text{ for all }v\in V\}\\
&=\sup\{ \lVert Av\rVert\;\colon\; v\in V\text{ with }\lVert v\rVert\leq 1\}\\
&=\sup\{\lVert Av\rVert\;\colon\; v\in V\text{ with }\lVert v\rVert = 1 \}\\
&=\sup\left\{ \frac{\lVert Av\rVert}{\lVert v\rVert}\;\colon\; v\in V\text{ with }v\neq 0\right\}.
\end{align*}$$","['operator-theory', 'linear-transformations', 'normed-spaces', 'functional-analysis']"
171274,"$-\iint_{A}(y+x)\,dA$ Evaluating","I am bit unsure about the following problem: Evaluate the double integral: $$-\iint_{A}(y+x)\,dA$$ over the triangle with vertices $(0,0), (1,1), (2,0)$ OK, so I figured here that I would do this by first evaluating the integral over the region bounded by the vertices $(0,0), (1,1), (1,0)$ and then evaluate the integral over the region bounded by the vertices $(1,0), (1,1), (2,0)$ before adding the two answers together, and then reversing the sign of this answer (since there is a minus sign in front of the original double integral).  Thus, I begin by finding: $$\int_{0}^{1}dx \int_{0}^{x}(y+x)\,dy$$ When solved this gives me the answer $\frac{1}{2}$. Next I solve: $$\int_{1}^{2}dx \int_{1}^{2-x}(y+x)\,dy$$ When solved this gives me the answer $-\frac{7}{6}$. I have verified both the integrals in Wolframalpha, and they give me the same answer.  I would therefore believe that the final answer should be: $$-(\frac{1}{2} - \frac{7}{6}) = \frac{2}{3}$$ However, the final answer should, according to the book, be $-\frac{4}{3}$. Thus, obviously I do something wrong here.  If anyone can help me out, I would greatly appreciate it.  Is it perhaps that it is not allowed to ""split up"" this into two separate integrals?  I couldn't find a way to solve this without doing this.","['definite-integrals', 'multivariable-calculus']"
171309,Learning differential/Riemannian geometry for PDEs,"I know there have been threads on which books to learn DG/RG from but hopefully this is sufficiently different to avoid closure. Can anyone recommend a book to learn DG/RG (whichever is appropriate) so that I can do PDEs on manifolds? At the moment I am reading through John M Lee's Introduction to Smooth Manifolds and I am wondering whether I really need to learn all the topics in it since it doesn't touch RG which I believe is more used in the theory of PDEs. Maybe there is a better text. Also, any topics to particularly study or avoid would be useful. Thanks","['riemannian-geometry', 'partial-differential-equations', 'reference-request', 'differential-geometry']"
171318,Verify trigonometry equation $\tan A - \csc A \sec A (1-2\cos^2 A)= \cot A$,How would I verify the following trigonometry identity? $$\tan A - \csc A \sec A (1-2\cos^2 A)= \cot A$$ My work so far is $$\frac{\sin A}{\cos A}-\frac{1}{\sin A}\frac{1}{\cos A}(1- \cos^2 A- \cos^2 A)$$,"['trigonometry', 'algebra-precalculus']"
171337,Countable sets?,"Determine whether each of these sets is countable or uncountable. For those that are countably infinite, exhibit a one-to-one correspondence between the set of positive integers and that set. 1) Integers not divisible by $3$. 2) Integers divisible by $5$ but not by $7$. I figured out the first one so it was $3k+1$ or $3k+2$ but the second one has thrown me for a loop I was thinking it could be something like $5k$ but that will give me everything divisible by $5$ and $7$, can't figure out what to do about the not divisible by $7$. Both are countable since I could go though and count the numbers that meet the requirements or am I wrong?",['elementary-set-theory']
171351,Why can you multiply out?,"$$(a+b)(c+d)=ac+ad+bc+bd$$
Why is this? Is there a proof to this? And there is something similar in logic. $$(A \land B)\lor (C \land D)=(A \lor C)\land(A \lor D)\land(B \lor C)\land(B\lor D)$$
Why is this? I used it all my life but I don't know why it works.","['logic', 'algebra-precalculus']"
171366,Application of closed graph theorem.,"I'm having a problem applying the closed graph theorem, which I think stems from distributions still being very new to me. I am reading a proof in Stein and Weiss, Introduction to Fourier Analysis on Euclidean Spaces , 4.13 in Chapter 1 , in the Further Results section, which begins: Suppose for the sake of contradiction that the Fourier transform of every function $f\in L^p (\mathbb{R})$, as a tempered distribution, is a function. The closed graph theorem easily shows, for all $f\in L^p, p>2$, there is a constant $A$ so that the following holds: $$\int_{|x|\le 1} | \ \hat f(x)| \ dx \le A ||f||_p. $$ I don't see how this follows from the closed graph theorem. Any help would be greatly appreciated.","['distribution-theory', 'functional-analysis']"
171369,Limit preserving metrics.,"I need to prove that given a sequence of points $\{a_n\}$ in $(\Bbb R^k,d_m)$ that converges to $a$, then it converges to the same limit in both $(\Bbb R^k,d_e)$ and $(\Bbb R^k,d_t)$ (and conversely), where $$ d_m(x,a)=\max_{1\leq i \leq k} \{|x_i-a_i|\} \;;\;\text{ the max metric}$$ $$d_e(x,a)=\sqrt{\sum_{1\leq i \leq k}(x_i-a_i)^2} \;;\;\text{ the Euclidean metric}$$ $$d_t(x,a)={\sum_{1\leq i \leq k}|x_i-a_i|} \;;\;\text{ the so-called taxicab metric}$$ There is a nice characterization of a point $a$ being a limit of a sequence $\{ a_n\}$ which is Given a sequence of points $\{a_n\}$ in a metric space $(X,d)$ then $\lim a_n = a$ if and only if for every neighborhood $V$ of $a$, only but finitely many of the $a_n$ are not contained in $V$, or equivalently $a_n \in V$ for almost all of the $a_n$. More formally, one can say that for every nbhd $V$ of $a$ there is an $N$ such that whenever $n>N$, $a_n \in V$. This said, one can prove: THEOREM Let $\{a_n\}$ be a sequence of points in $\Bbb R^k$. Then if $\lim a_n=a$ under either one of the metrics $d_t,d_e,d_m$, it follows $\lim a_n=a$ for the other two. (NEW) PROOF We have that $$\tag a d_m(a,x) \leq d_e(a,x) \leq  \sqrt{k}\cdot d_m(a,x) $$
$$\tag b d_m(a,x) \leq d_t(a,x) \leq  {k}\cdot d_m(a,x) $$ Let $\lim a_n=a$ in $(\Bbb R^k,d_m)$. Let $T$,$E$ be neighborhoods of $a$ in $(\Bbb R^k,d_t)$ and $(\Bbb R^k,d_e)$ respectively. Then they contain an open $t$-ball and an open $e$-ball about $a$ for a $\delta>0$. $(a)$ and $(b)$ guarantee that each of these balls contain an open $m$-ball about $a$. But this ball $B_m$ is a neighborhood of $a$ in $(\Bbb R^k,d_m)$ so $a_n\in B_m$ whenever $n>N$ for a some $N$. This in turn means that $a_n \in B_t$ and $a_n \in B_e$ whenever $n>N$ so $\lim a_n=a$ in both the two other metrics. The other two cases follow analogously.","['metric-spaces', 'proof-writing', 'limits']"
171376,"Finding the inverse function, is there a technique?","I came across a way to find whether some number is inside a sequence of numbers.
For example the sequence (simple function for positive odd numbers):
$$a(n) = 2n + 1.$$
So the numbers inside it go: $1, 3, 5, 7, \ldots$ If I want to test if the sequence contains a number $N,$ I reverse the formula to this: $$n = (N - 1) / 2.$$
If $n$ is an integer, then we know that $N$ is inside the sequence. But, here is my question, how can I find the inverse function of this formula:
  $$a(n) = n(2n - 1)$$ Note: Please do comment every step and explain the logic behind it, so I'll understand","['inverse', 'functions']"
171378,"calculus, self-study..recommendations?","I've been trying to educate myself in various areas of mathematics. I have been out of any formal math education for quite some time and so I brushed up on some basic (really, really basic) stuff including algebra and so forth. I'm working my way through some calculus and I'm using Michael Spivak's Calculus (4th ed.), which is fantastic, but I'm finding it to be a bit difficult at times. Do you have any recommendations for a good book or any other path that might make sense for someone without a very strong, formal background in math for learning calculus? I should add that a major motivation in my renewed interest in math is related to my interest in applying it to computer science. I know that many will respond saying that calculus is, therefore, not the best place to spend my energies (discrete math might be better) and I'm sure that's a reasonable point, but I'm also interested in calculus per se. I guess there are a lot of areas in computer science where I feel like I'm lacking because I'm not competent with functions, etc, for example in algorithm analysis and big-O notation. I realize this was a bit rambly and some of this might be totally off -- if it is, it's from pure ignorance -- and for that I apologize. I guess I'm just sort of confused as to where my energies would best be spent given my goals.","['reference-request', 'calculus', 'algebra-precalculus']"
171388,How to transform series of series into series,"I need to prove this equation.
$$ \sum_{k=0}^{i-2} \left( e \space α(k+1)\space\frac{(-1)^{i+k+2}}{(i-k-2)!} \right) = \sum_{k=0}^{i-2} \frac{(i-k)^k}{k!} \space e^{i-k} (-1)^k\space where,\space α(i) = \sum_{k=0}^{i-1}\left(\frac{(i-k)^k}{k!}e^{i-k}(-1)^k\right) \space\space\space\space (1)$$ This equation holds. I have confirmed it by using a math tool, but I don't know how the left side can be transformed into the right side of the equation. I need some formulas to prove this equation. The left side of the equation includes series of series $\sum_{k=0}^{i-2} e \space α(k+1) $, so it is very difficult for me to transform the series of series into the series in the right side. The equation can be rewritten as follows, $$ \sum_{k=0}^{i-2} \left( e \space α(k+1)\space\frac{(-1)^{i+k+2}}{(i-k-2)!} \right) = α(i) - \frac{(-1)^{i+1} \space e}{(i-1)!} \space\space\space(2)$$
$$ α(i) = \sum_{k=0}^{i-2} \left( e \space α(k+1)\space\frac{(-1)^{i+k+2}}{(i-k-2)!} \right) + \frac{(-1)^{i+1} \space e}{(i-1)!} \space\space\space(3)$$ Any hint that will lead me to the correct proof will be highly appreciated. For more information, this $α(i)$ function came from my previous question .","['sequences-and-series', 'algebra-precalculus', 'binomial-coefficients']"
171404,Two questions about ultraweak and ultrastrong topology from Dixmier,"You could reference Dixmier's book on Von Neumann Algebras p.42 Theorem 1 and its proof to know the entirety of the context.  Otherwise, the most relevant things are below: Let $M$ be an ultraweakly closed subspace of $B(H)$ and $K$ a convex subset of $M$.  Then he claims $K$ is ultraweakly closed if and only if $K\cap M_r$ is ultraweakly closed for all balls $M_r$ of radius $r$ in $M$ centered at $0$.  Former implies latter is clear.  What about the other way?  He doesn't really make an attempt to prove this and instead references some outside fact and a seemingly unrelated subclaim.  I would not mind if an entirely new proof were suggested in the answers, although a filling in of the missing details would be equally appreciated. He then uses this fact to prove that if $\phi$ is a linear form on $M$, continuous in norm, for which its restriction to the unit ball is continuous in ultrastrong topology, then $\phi$ is actually ultraweakly continuous.  Along the way, he concludes (and I agree with this given 1.) that $\phi^{-1}({0})$ is ultraweakly closed.  I just don't see why this implies the conclusion, even though he asserts it without further comment in the next step.  Thanks!","['operator-theory', 'operator-algebras', 'functional-analysis', 'analysis']"
171407,Finding two numbers given their sum and their product,"Which two numbers when added together yield $16$, and when multiplied together yield $55$. I know the $x$ and $y$ are $5$ and $11$ but I wanted to see if I could algebraically solve it, and found I couldn't. In $x+y=16$, I know $x=16/y$ but when I plug it back in I get something like $16/y + y = 16$, then I multiply the left side by $16$ to get $2y=256$ and then ultimately $y=128$. Am I doing something wrong?","['algebra-precalculus', 'systems-of-equations']"
171429,Compute $\lim\limits_{n\to{+}\infty}{{(2+n^3)}^{55-7n}}$,"Find: $$\lim_{n\to{+}\infty}{{(2+n^3)}^{55-7n}}$$ 
According to Maple, that is equal to zero.
What theorem could I use?","['calculus', 'real-analysis', 'limits']"
171432,"If two polynomials are equal as functions, are they necessarily equal as polynomials?","Say you have a finite field $F$ of order $p^k$. Suppose that $f,g\in F[X_1,\dots,X_m]$, such that the degree of each $X_i$ is strictly less than $p^k$ in both $f$ and $g$. I'm putting this condition to avoid things like $f=X_1X_2X_3^{p^k}$ and $g=X_1X_2X_3$ which technically define the same polynomial function over $F$ since $X_i^{p^k}-X_i$ is in the kernel of the evaluation homomorphism, but are not equal in the polynomial ring. Under this condition, if $f$ and $g$ define the same polynomial function over $F$, are they equal as polynomials? By equality of polynomial functions, I mean they are equal as sets of ordered pairs. I feel like restricting the degree of each indeterminate should force this to be so, but how can it actually be proven?","['finite-fields', 'abstract-algebra', 'polynomials']"
171438,"If $m\geq2$ is an integer, then $\sum\limits_{n=1}^{\infty}m^{-n^2}$ is irrational","Let $m  \geq2$ be an integer. I want to ask how to prove that the sum of the following series is irrational:
$$\sum _{n=1}^{\infty} \frac{1}{m^{n^2}}$$","['sequences-and-series', 'irrational-numbers']"
171446,Integers that are a sum of two $k$th powers in $n$ different ways,Do there exist infinitely many $k$ such that for all $n$ we can find a sequence $x_i$ of distinct natural numbers such that $x_1^k+x_2^k=x_3^k+x_4^k=\cdots=x_{2n-1}^k+x_{2n}^k$ ?,"['diophantine-equations', 'number-theory']"
171453,Maximal normal subgroup not containing an element,"Do you know results about maximal normal subgroup among normal subgroups not containing a given element $x$ ? The problem can be reduce to the case of free groups. First, such a sugroup exists thanks to Zorn lemma. Secondly, I think that if $x$ is a primitive element, then there is only one maximal normal subgroup among normal subgroups not containing $x$; otherwise, there is not unicity: if $x=[a,b]$, there is a one-to-one correspondance between our normal subgroup and non abelian two-generator groups whose proper quotients are abelian (eg. the dihedral group $D_3$ or the quaternion group $Q_8$). But I have no idea about how construct such subgroups.",['group-theory']
171474,Is there any way to find a angle of a complex number without a calculator?,"Transforming the complex number $z=-\sqrt{3}+3i$ into polar form will bring me to the problem to solve this two equations to find the angle $\phi$: $\cos{\phi}=\frac{\Re z}{|z|}$ and $\sin{\phi}=\frac{\Im z}{|z|}$. For $z$ the solutions are $\cos{\phi}=-0,5$ and $\sin{\phi}=-0,5*\sqrt{3}$.
Using Wolfram Alpha or my calculator I can get $\phi=\frac{2\pi}{3}$ as solution. But using a calculator is forbidden in my examination. Do you know any (cool) ways to get the angle without any other help?","['trigonometry', 'complex-numbers']"
171487,Inner product is jointly continuous,"I'm attempting another exercise from my notes: Show that an inner product on an inner product space is jointly continuous with respect to the induced norm:if $v_n \to v$ and $w_n \to w$ as $n \to \infty$, then $\langle v_n, w_n\rangle \to \langle v,w \rangle$ as $n \to \infty$. I'd not heard jointly continuous before so I googled and found the following definition in Kelley : Let $P: F \times X \to Y$ defined by $(f,x) \mapsto f(x)$. Each topology on $F$ gives rise to a product topology on $F \times X$. A topology for $F$ is said to be jointly continuous iff $P$ is continuous. I think in the case of the inner product, $F$ is the one point space $\{ \langle \cdot, \cdot \rangle \}$ and $X = V \times V$. Then there is only one topology on $F$ and jointly continuous just means that the inner product is continuous. Question 1: Is that correct so far? So to show that $\langle \cdot, \cdot \rangle$ is continuous we need to show that if $(x_n, y_n) \to (x,y) $ then $\langle x_n, y_n \rangle \to \langle x, y \rangle$ in $\mathbb R$. On $V \times V$ we can define the norm $\|(a,b)\| = \max(\|a\|, \|b\|)$. Question 2: How do I show that this norm induces the same topology as the product topology? Now I want to show that for $\varepsilon > 0$ there is an $N$ such that for $n > N$, $| \langle x_n, y_n \rangle - \langle x, y \rangle | < \varepsilon$. In the max norm, I have $\|x_n - x\|^2 = \langle x_n, x_n \rangle - \langle x_n, x \rangle - \langle x, x_n \rangle + \langle x, x \rangle < \varepsilon^2$, for both $x_n,x$ and $y_n,y$. Question 3: How can I use this to show $| \langle x_n, y_n \rangle - \langle x, y \rangle | < \varepsilon$?","['inner-products', 'functional-analysis']"
171488,A proof of Wolstenholme's theorem,"This was inspired by this question . I tried to use the identity $${2n \choose n}=\sum_{k=0}^n {n \choose k}^2$$ (see this question ) to prove that $$\binom{2p}p\equiv2\pmod{p^3}$$ if $p\gt3$ is prime. ( Wikipedia gives a combinatorial proof that this special case of Wolstenholme's theorem implies the general case.) Since $\binom p0=\binom pp=1$, we want $$\sum_{k=1}^{p-1}\binom pk^2\equiv0\pmod{p^3}\;.$$ The binomial coefficients are all divisible by $p$, so we can write this as $$\sum_{k=1}^{p-1}\left(\frac1p\binom pk\right)^2\equiv0\pmod{p}\;.$$ But $$\frac1p\binom pk=\frac{(p-1)\cdots(p-(k-1))}{k(k-1)\cdots1}\equiv\frac1k\frac{(-1)\cdots(-(k-1))}{(k-1)\cdots1}\equiv\pm\frac1k\pmod p\;,$$ and thus $$\sum_{k=1}^{p-1}\left(\frac1p\binom pk\right)^2\equiv\sum_{k=1}^{p-1}\left(\pm\frac1k\right)^2\equiv\sum_{k=1}^{p-1}\left(\frac1k\right)^2\pmod{p}\;.$$ As $k$ traverses the non-zero residues $\bmod p$, so does $1/k$, so this is just twice the sum of the quadratic residues $\bmod p$, which is zero for $p\gt3$ as required. I couldn't find this proof of Wolstenholme's theorem anywhere, so I'm wondering: Is there a mistake in it? If not, is it known? If not, is it relevant? [ Update: ] In the meantime I came across this paper , which does make extensive use of the sum-of-squares identity in the context of Wolstenholme's theorem, though not for proving the theorem itself. Regarding the use of the sum over reciprocals in proving the harmonic-number version of the theorem, as in the paper linked to in Don's answer: I was aware of that use, but the main reason I thought that this alternative proof might nevertheless be of interest is that to prove the congruence for the binomial coefficient from the two congruences for the (generalized) harmonic numbers requires two sums, whereas I'm only evaluating a single sum.","['alternative-proof', 'binomial-coefficients', 'number-theory']"
171498,Is the centroidal mean a generalized mean?,"The centroidal mean of two numbers $a,b$ is the number $\dfrac{2(a^2+ab+b^2)}{3(a+b)}.$ In a trapezoid whose bases have lengths $a$ and $b$, it is the length of the line segment parallel to the bases which passes through the centroid . For $p\in \Bbb R\setminus 0,$ a generalized $p$-mean of two numbers $a,b$ is the number $\left(\dfrac{a^p+b^p}2\right)^{1/p}.$ Is the centroidal mean a generalized $p$-mean for some $p$? I suspect it isn't, but I have no idea how to prove it. Just trying to equate the two expressions has led me nowhere.","['geometry', 'algebra-precalculus']"
171504,understanding this differential operator on a tensor product,"I am currently trying to read the T. Kotake's paper ""An Analytical Proof of the Classical Riemann Roch Theorem"", in which he defines a differential operator which acts on smooth sections of a tensored vector bundle. I have trouble understanding the notation, because I am not very familiar with tensor notation. I'll try to describe my problem and post questions as I go along: We have a compact Riemann surface $\Sigma$ with metric locally given by
$$h(z,\overline{z}) \,dz\otimes d\overline{z}.$$ Question 1 : This metric confuses me a little - should'nt we have a metric depending on one complex variable only, if we are working with Riemann surfaces ? Furthermore we are given a holomorphic vector bundle $\mathcal{E} \to \Sigma$ of rank $N$. It carries a hermitian metric which is determined by a system of locally defined functions $A \colon V \to \text{GL}(N,\mathbb{C})$ whose value at each point is a hermitian matrix (here $V$ denotes a coordinate chart for $\Sigma$.) These are then patched together via a partition of unity. If we let $K = \Lambda^{1,0}\,T^*\Sigma$ denote the canonical bundle, and $\mathcal{E}^*$ the dual bundle of $\mathcal{E}$, we also have the vector bundle $K \otimes \mathcal{E}^*$. It has an induced metric, namely 
$$
h^{-1} \otimes(A^T)^{-1} \colon V \to \mathbb{C}^* \otimes \text{GL}(N,\mathbb{C}).
$$ Question 2 : I am very new to tensor bundles - is this the correct way of writing the metric structure? Now the differential operator is defined on smooth sections of the bundle $K \otimes \mathcal{E}^*$, as it stands in the paper it acts locally as 
$$
\triangle = - A \frac{\partial}{\partial z} (hA)^{-1} \frac{\partial}{\partial \overline{z}}
$$ Question 3 : How do I underrstand the action of $\triangle$ on a section $\omega \otimes s$ ? The problem I have is I don't know what is supposed to act on $\omega$, and what acts on $s$. If I expand the expression, above, I get
$$
\triangle = h^{-1}\frac{\partial^2}{\partial z\partial\overline{z}} - h^{-1}A\frac{\partial A^{-1}}{\partial z} \frac{\partial}{\partial \overline{z}} - \frac{\partial h^{-1}}{\partial z}\frac{\partial}{\partial \overline{z}}
$$
and so my guess is in tensor notation I would write this as
$$
\triangle = h^{-1} \otimes \frac{\partial^2}{\partial z\partial\overline{z}} - h^{-1} \otimes A\frac{\partial A^{-1}}{\partial z} \frac{\partial}{\partial \overline{z}} - \frac{\partial h^{-1}} {\partial z} \otimes \frac{\partial}{\partial \overline{z}}
$$ I am really not sure here, somehow it looks wrong. Any comments on my attempts would be hugely helpful! I hope these questions make sense, I shall try my best to clarify if not!","['vector-bundles', 'complex-geometry', 'differential-geometry', 'differential-forms', 'tensor-products']"
171508,Limit inferior/superior of sequence of sets,"Let $(\Omega, \mathcal{A}, \mu)$ be a measure space, where $\mu(\Omega)< \infty$. Further $(A_n)_{n \in \mathbb{N}}$ is a a sequence of $\mathcal{A}$-measurable sets. I want to prove, that $$ \mu ( \liminf_{n \rightarrow \infty} A_n) \leq \liminf_{n \rightarrow \infty} \mu (A_n) \leq \limsup_{n \rightarrow \infty} \mu (A_n) \leq \mu (\limsup_{n \rightarrow \infty} A_n)$$
holds for any sequence $(A_n)_{n \in \mathbb{N}}$. 
I have no experience working with the limit superior/inferior. Clearly 
$$\mu ( \liminf_{n \rightarrow \infty} A_n) \leq \mu (\limsup_{n \rightarrow \infty} A_n)$$ 
holds, since it is easy to prove that the one is a superset of the other. Also 
$$\liminf_{n \rightarrow \infty} \mu (A_n) \leq \limsup_{n \rightarrow \infty} \mu (A_n)$$
holds, for any sqeuence. But I am stuck how to show the connection. I could use the Definitions, then I get
$$ \mu (\bigcup_n^\infty \bigcap_{k=n}^\infty A_n) \leq \lim_{n \rightarrow \infty} \inf_{k \geq n} \mu(A_n) \leq \inf_{n \geq 0} \sup_{k \geq n} \mu(A_n) \leq \mu (\bigcap_n^\infty \bigcup_{k=n}^\infty A_n) $$
But I don't know if this helps. Anyone got a hint how to go on?","['measure-theory', 'limsup-and-liminf']"
171542,Concrete example of calculation of $\ell$-adic cohomology,"Let $p$ and $\ell$ be distinct prime numbers. Consider in the affine plane $\mathbb{A}^2_{\mathbb{F}_p}$ with coordinates $(x,y)$ the union $L$ of the axes $x = 0$ and $y = 0$. How does one compute the $\ell$-adic cohomology groups with compact support $H^i_c(L,\mathbb{Q}_\ell)$? I thought I had some idea of what $\ell$-adic cohomology is, but I don't even manage to do this example...",['algebraic-geometry']
171543,Where are good resources to study combinatorics?,"I am an undergraduate wiht basic knowledge of combinatorics, but I want to obtain sound knowledge of this topic. Where can I find good resources/questions to practice on this topic? I need more than basic things like the direct question 'choosing r balls among n' etc.; I need questions that make you think and challenge you a bit.","['reference-request', 'combinatorics']"
171556,Gradient operator the adjoint of (minus) divergence operator?,"Recently I found this statement -- the gradient operator is the adjoint of the minus divergence operator -- in one of my lecture notes. Knowing only a little about functional analysis, I'm looking for an intuitive interpretation. I already found this topic which has a few good answers, but I'd like to view it from a somewhat different angle. So say there are two vector spaces $X$ and $Y$, and that $L$ is a linear operator that maps elements from $X$ to $Y$, i.e. $L:X\to Y$. Furthermore, the set of all bounded/continuous linear functionals on $X$ (i.e. $f\;| f:X\to\mathbb{R}$) is called the dual (space) of $X$, marked $X'$. Mutatis mutandis for $Y'$. Then, apparently if we take a look at our $L$, there is an operator (called the adjoint operator) $L'$ that maps elements from $Y'$ to $X'$, right? So $L':Y'\to X'$. To make this a little less abstract, let's take the gradient operator $\nabla$. Now, I don't know how to write down appropriate spaces $X$ and $Y$ such that $\nabla:X\to Y$. I thought about $C^1[a,b]$, the space of continuous differentiable functions on the interval $[a,b]$, which is then mapped to $C^0[a,b]$. But these are only functions of a single variable, right? How to denote the space of functions that depend on two or three variables, say $(x,y)$ resp. $(x,y,z)$? If I know the above mentioned spaces, then it should be possible to think of some operators in their duals, $X'$ and $Y'$. If I understand it correctly, the divergence operator should then be the operator to map operators from $Y'$ to $X'$. So my actual question: how to denote the mentioned spaces, come up with a few operators in their duals $X'$ and $Y'$ (so some examples), and then show that the divergence indeed maps the operators from $Y'$ to $X'$?","['adjoint-operators', 'functional-analysis']"
171562,"Prove $(\mathbb Z \times \mathbb Z, \Sigma)$ to be a partial order and tell if its subset $T'$ is a lattice","Let $T = (\mathbb Z\times\mathbb Z, \Sigma) $ be defined as follows: $$\begin{aligned} (a,b) \text{ } \Sigma  \text { } (c,d) \Leftrightarrow (a,b) = (c,d) \text{ or } a^2b^2<c^2d^2\end{aligned}$$ check $\Sigma $ is a partial order and it's not total; search for $\min(T)$, $\max(T)$, minimal and maximal elements, if there're any. Take $T' = \{(1,6), (-1,1), (0,1), (-1,-1), (2,-3)\} \subset \mathbb Z\times\mathbb Z$ then: draw an Hasse diagram for $(T', \Sigma)$, search for $\min(T')$, $\max(T')$, minimal and maximal elements, supremum and infimum, upper and lower bounds; tell if $(T', \Sigma)$ a totally ordered set; tell if it is a lattice. In order to prove $(\mathbb Z \times \mathbb Z,\Sigma)$ a partial order set, $\Sigma$ reflexivity, anti-symmetry and transitivity has to be shown. Reflexivity $\forall (a,b) \in \mathbb Z\times \mathbb Z$ $(a,b) \text { } \Sigma \text { } (a,b) $ as $(a,b) = (a,b)$. Anti-symmetry Let $(a,b),(c,d) \in \mathbb Z \times \mathbb Z$, then $\Sigma$ is anti-symmetric if $(a,b) \text { } \Sigma \text{ }(c,d)$ and $(c,d) \text { } \Sigma \text{ }(a,b) \Rightarrow (a,b) = (c,d)$ hence $\forall (a,b),(c,d)$ should be valid the following $a^2b^2<c^2d^2$ and $c^2d^2<a^2b^2 \Rightarrow (a,b) = (c,d)$ which clearly can't be, so anti-symmetry is only valid if $(a,b) = (c,d)$. Transitivity $\forall (a,b),(c,d),(e,f) \in \mathbb Z \times \mathbb Z$ $(a,b) \text { }  \Sigma \text { }  (c,d)$ and $(c,d) \text { }  \Sigma \text { }  (e,f) \Rightarrow (a,b) \text { }  \Sigma \text { }  (e,f)$ which is true, because (assuming all pairs being distinct): $a^2b^2 < c^2d^2$ and $c^2d^2<e^2f^2$ then $a^2b^2<e^2f^2$ Conclusion $(\mathbb Z \times \mathbb Z,\Sigma)$ is actually a partial order set and not total. In $(\mathbb Z \times \mathbb Z)$ we can find all $(a,b) : a = 0 \text { or } b = 0$ are minimal elements. However no maximum, minimum or massimal elements have been found.
The Hasse diagram for the above mentioned set is: $(T', \Sigma)$ is not an ordered set, because $\forall (a,b),(c,d) \in T'$ it has to happen $(a,b) \text { } \Sigma \text{ } (c,d)$ or $(c,d) \text { } \Sigma \text{ } (a,b)$, but for the elements $(-1,1),(-1,-1)$ the $\Sigma$ relation doesn't apply. $\sup(T') = (1,6)$, whereas $(1,6)$ is also a maximal. $\inf(T') = (0,1)$, whereas $(0,1)$ is also a minimal. upper bounds are $\{(a,b) \in \mathbb Z \times \mathbb Z : a^2b^2 > 36\}$. no maximum or minumum or lower bounds found. My question would be: does everything hold? How about the anti-symmetry that is valid only if two pairs are equal? Does it mean in $T'$ the relation $\Sigma$ desn't really apply?  How do I prove if this is a lattice?","['lattice-orders', 'abstract-algebra']"
171565,$k$ in trigonometric equality $\sin(a) =\sin(b)$,"On a test there is the question: ""Solve for $x$ on the interval $[-\pi,\pi]$ where $\sin(2x) = \cos(3x)$ I know that: $\cos(x) = \sin(\frac12\pi - x)$ So you can rewrite the equation to: $\sin(2x) = \sin(\frac12\pi - 3x)$ But then in the solution, the next step is this: $2x = \frac12 \pi - 3x + 2\pi k$ or $2x = \pi - (\frac12\pi - 3x) + 2\pi k$ What is the $2\pi k$ for? Later they simplify it to: $x = \frac{1}{10}\pi + \frac25\pi k$ or $x = -\frac12\pi + 2\pi k $ and then it goes like this: $x = \frac{1}{10}\pi - 2 * \frac25\pi = -\frac7{10}\pi$ $x = -\frac12\pi $ $x = \frac1{10}\pi - 1 * \frac25\pi = -\frac3{10}\pi$ $x = \frac1{10}\pi $ $x = \frac{1}{10}\pi + 1 * \frac25\pi = \frac12\pi$ $x = \frac{1}{10}\pi + 2 * \frac25\pi = \frac9{10}\pi$ How does that part work? I can't find any theory on it. Why is $k$ substituted by the range $[-2,2]$?",['trigonometry']
171576,"An irreducible polynomial $f \in \mathbb R[x,y]$, whose zero set in $\mathbb A_{\mathbb R}^2$ is not irreducible","This is an exercise on Page 8 of Hartshone's Algebraic Geometry : Give an example of an irreducible polynomial $f \in \mathbb R[x,y]$, whose zero set $Z(f)$ in $\mathbb A_{\mathbb R}^2$ is not irreducible. I think such an example must come from the fact that $\mathbb R$ is not algebraically closed. But I have no idea as to finding a concrete one. Thanks very much.",['algebraic-geometry']
171578,Coin tosses until I'm out of money,"The question I think is a simple one, but I've been unable to answer or find an answer for it yet: There's a simple game: if you flip heads you win a dollar (from the house), but if you flip tails you lose a dollar (to the house). If I start with n dollars (and the house has infinite money), how many flips can I expect to do before I've lost all my money?  This is different than the common question of how many flips can I do before I have a run of length 'n'.  In this case you can lose your money by never having a run of length more than 2, for example, simply by repeating win 1, lose 2, win 1, lose 2, etc... I can write out a decision tree on this, but I haven't been able to generalize it into a formula yet.","['statistics', 'probability', 'combinatorics']"
171592,Connectedness of centralizer exercise,"I'm having trouble understanding connectedness from a group theoretic perspective. Let $G$ be the symplectic group of dimension 4 over a field $K$, $$G = \operatorname{Sp}_4(K) = \left\{ A \in \operatorname{GL}_4(K) : A^T J A = J \right\} \text{ where } J = \left(\begin{smallmatrix}.&.&.&1\\.&.&1&.\\.&-1&.&.\\-1&.&.&.\end{smallmatrix}\right)$$ and let $C$ be the centralizer of a specific unipotent element $t$, $$C=C_G(t) \text{ where } t = \left(\begin{smallmatrix}1&1&.&.\\.&1&.&.\\.&.&1&1\\.&.&.&1\end{smallmatrix}\right)$$ The exercise asks one to, Show that $t$ does not lie in the connected component of the identity when the characteristic of $K$ is 2. I think K is algebraically closed, though this is perhaps not specified here (and is specified in a nearby exercise). I calculate the centralizers to be: $$C_{\operatorname{GL}_4(K)}(t) = \left\{ \left(\begin{smallmatrix}a&b&c&d\\.&a&.&c\\e&f&g&h\\.&e&.&g\end{smallmatrix}\right) : a,b,c,d,e,f,g,h \in K, ag-ec \neq 0 \right\} \cong \operatorname{GL}_2\left(K[dx]/{(dx)}^2\right)$$
$$C_{\operatorname{Sp}_4(K)}(t) = \left\{ \left(\begin{smallmatrix}a&b&c&d\\.&a&.&c\\e&f&g&h\\.&e&.&g\end{smallmatrix}\right) : a,b,c,d,e,f,g,h \in K, ag-ec = 1, ah+bg=cf+de \right\}$$ I am clueless how to find their connected components. What are the connected components of $C_{\operatorname{GL}_4(K)}(t)$ and $C_{\operatorname{Sp}_4(K)}(t)$? Especially describe the exceptional behavior in characteristic 2. Does the connectedness have anything to do with them being matrices? I would prefer some group theoretic way to find the components, but I worry that the components have nothing to do with the matrices, and depend only on the equations $ag-ec=1$ and $ah+bg=cf+de$, regardless of where these variables are in the matrix. If they don't have anything to do with the group structure, then why would I care if it is connected?","['algebraic-groups', 'group-theory']"
171595,Hyper Birthday Paradox?,"There are $N$ buckets. Each second we add one new ball to a random bucket - so at $t=k$, there are a total of $k$ balls collectively in the buckets. At $t=1$, we expect that at least one bucket contains one ball. At $t=\sqrt{2N\ln{2}}$, due to birthday paradox, we expect that at least one bucket contains two balls. . . At $t=f(m)$, we expect at least one bucket to contain $m$ balls. What is the function $f(m)$?","['balls-in-bins', 'statistics', 'birthday', 'probability', 'combinatorics']"
171599,Jensen's inequality for integrals,What nice ways do you know in order to prove Jensen's inequality for integrals? I'm looking for some various approaching ways. Supposing that $\varphi$ is a convex function on the real line and $g$ is an integrable real-valued function we have that: $$\varphi\left(\int_a^b f\right) \leqslant \int_a^b \varphi(f).$$,"['calculus', 'integration', 'definite-integrals', 'integral-inequality', 'real-analysis']"
171602,What is the importance of functions of class $C^k$?,"In all calculus textbooks, after the part about successive derivatives, the $C^k$ class of functions is defined.
The definition says : A function is of class $C^k$ if it is differentiable $k$ times and the $k$-th derivative is continuous. Wouldn't be more natural to define them to be the class of functions that are differentiable $k$ times?
Why is the continuity of the $k$th derivative is so important so as to justify a specific definition?","['calculus', 'terminology']"
171610,An entire function is identically zero?,"I'm preparing for a PhD prelim in Complex Analysis, and I encountered this question from an old PhD prelim: Suppose $f(z)$ is an entire function such that $|f(z)| \leq \log(1+|z|) \forall z$. Show that $f \equiv 0$. Well, for $z=0$, $|f(0)| \leq 0$. On the other hand, for $z \neq 0$, $\log(1+|z|) > 0$, a positive constant. I'm guessing this would mean that $f$ turns out to be a bounded entire function, so then by Liouville's theorem, $f$ is constant, but this doesn't necessarily mean that $f \equiv 0$, does it? Am I wrong somewhere? Some guidance would be much appreciated!",['complex-analysis']
171612,Generalized Eigenvalue Problem with one matrix having low rank,"I have a specific Generalized Eigenvalue Problem (GEVP) where i am primary not interested in solving this problem but concluding from a standard EVP the spectrum of the GEVP. The Problem Let $A$ be a $n\times x$ possibly complex matrix and $B$ a diagonal, real $n\times n$ matrix with maximal rank of $n-1$ (e.g. the matrix $B$ has at minimum 1 zero column and row). Solving $(B\lambda-A)\cdot v=0$ with $|v|=1$, so that we have $n+1$ equations for $n+1$ unknown is the GEVP. The GEVP can not be reformulated as EVP because $det(B)=0$ and therefore $B$ is not invertible. As I said the goal is not just solving this problem (this could be done by solving $det(\lambda B I-A)=0$ to obtain the eigenvalues) but to conclude eigenvalues for the stated GEVP from the following, already solved, EVP (the $n$ eigenvalues $\mu_1\leq\mu_2\leq\dots\leq \mu_n$ of $A$ are known): $(I\mu-A)\cdot w=0$. What I have already learned *As $A$ and $B$ in general do not commutate it is not possible to diagonalize $A$ and $B$ simultanously. Therefore the spectra will be different. *If the EVP results in eigenvalues $\mu=0$, so there will be the same number of eigenvalues $\lambda=0$ in the GEVP. (Because in both cases $det(A)=0$ must be fullfilled and the geometric multiplicity comes from the dimension of $kern(A)$.) *For every zero-row in $B$ the number of eigenvalues $\mu$ is one less then in the EVP. This is because the order of the characteristic polynom (CP) goes one down for every zero-row in $B$ compared to the order of the CP in the EVP. Questions *Can be said which eigenvalues (in addition to the zeros) of the EVP are also eigenvalues of the GEVP (the eigenvectors may not be the same in both cases, but the eigenvalues). *Is there a pertubation theory? Can I somehow make a taylor series of the CP in the GEVP where the zeroth-term is the CP of the EVP? *The number of eigenvalues in the GEVP is less than in the EVP, can be concluded which eigenvalues vanish? In case anybody wants to know, where my question emerges from (this is not essential for my questions but possibly from general interest): If one wants to conclude the stability of a fix point $x^*$ of ODEs one needs to solve the variational ODE $\dot{\delta x}(t)=D_xf(x^*)\delta x(t)$. Where $\delta x$ is a small pertubation away from the fix point: $\delta x(t)=x(t)-x^*$. Solving this with $\delta x(t)=\delta x_0 e^{\mu t}$ results in the EVP $\mu\delta x_0 = D_x f(x^*) \delta x_0$. Using $D_xf(x^*)=A$ and $w=\delta x_0$ results in the stated EVP. If one has additional constraints in a implicit way $g(x(t))=0$ the stability of a fix point in the ODE may change (e.g. the constraint acts in a unstable direction. The eigenvalue of $A$ in this direction is still greater zero (obviously the matrix $A$ does not change if constraints are imposed) but it is a ""forbidden"" direction as the corrisponding eigenvector is in a direction which is not allowed due to the constraint). Taking the time derivative of $g$ results in $D_x g(x)\cdot \dot{x}(t)=0$. Inserting the pertubation away from the fix point results in $(D_xg(x^*)+D_x(D_xg(x^*))\cdot \delta x)\cdot \dot{\delta x}(t)+\dots=0$ $D_xg(x^*)\cdot \dot{\delta x}(t)+O(\delta x^2)=0$ Inserting the exp-ansatz results in $D_xg(x^*)\cdot \delta x_0\mu\approx0$ This means that the small pertubations need to be orthogonal to the gradient on the invariant manifold near the fix point (e.g. they are inside the invariant manifold). One possible way to go to study the change of stability of the fix point when  constraints are imposed, is to solve the EVP and then to check consistency with the last equation. I want to include the constraint directly in the EVP, which leads to the GEVP by simply adding the last equation to the EVP (with $D_x g(x^*)=\hat{B}$ and $w=\delta x_0$): $(\hat{B}\mu+I\mu-A)\cdot w=0$ and with $\hat{B}+I=B$ $(B\mu-A)\cdot w=0$ The criterion of ""low rank $B$"" comes from the generic constraints like $g(x_1,\dots,x_n)=x_0^1-x_1\rightarrow D_xg=(-1,0,\dots,0)\rightarrow B=\mbox{diag}(0,1,\dots,1)$.","['linear-algebra', 'eigenvalues-eigenvectors']"
171614,"What is $\bigcup\limits_{n=1}^\infty [0,1-\frac{1}{n}]$?","This is probably a pretty dumb question, but I am confused by set theory again. The question is whether 
$$\bigcup_{n=1}^\infty \left[0,1-\frac{1}{n}\right]$$
equals $[0,1]$ or $[0,1)$. However, I am looking for some explanation and not only the result, since I'd like to understand why it's the one or the other.",['elementary-set-theory']
171619,Dolbeault cohomology of the complex projective space.,"Let $X=\mathbb{CP}^n$.  We proved using the hodge decomposition that $H^0(X,\Omega^p)=0$ if $p\neq 0$. But I do not understand why I cannot have global holomorphic differential p-forms not even constants. I want to understand why $H^0(X,\Omega^p)=0$ without using the Hodge decomposition. And without using GAGA I would like to see a proof of $H^0(Proj(\mathbb{C}[x_0,..,x_n]),\Omega_{X/\mathbb{C}}^p)=0$ for $p\neq 0$","['algebraic-geometry', 'complex-analysis']"
171638,Mathematics of change money,"Do you know any results or articles about change money?
Something like the statistics of different value notes in a cash box. Or answers to questions which distribution of notes values is best for starting a day in a shop. I mean obviously you need more small value notes than large ones. After one day of selling you probably have more large notes as they don't go away easily.",['statistics']
171651,Difficulty of functional analysis exam,"I've just written the final exam in my introductory course to functional analysis (2nd year bachelor degree). I felt quite well prepared but nevertheless found the exam pretty challenging in the timeframe of two hours. I'd appreciate any comments about what you think about it! The exam can be found here: http://www.math.lmu.de/~michel/SS12_FA_Final_Test_01.pdf In particular I'd appreciate any hints how to solve 6 ii), which states: Use the fourier-series of $$f(x)=\begin{cases}x^2 &\mbox{ for } x\in[0,\pi] \\ (2\pi-x)^2 &\mbox{ for } x\in (\pi,2\pi ]\end{cases} $$ to calculate $\sum_{n\in\mathbb{N}}\frac{(-1)^{n-1}}{n^2}$","['functional-analysis', 'soft-question']"
171665,"Explanation of statements in paper required (pde, calculus, analysis)","(I am reading a paper called Shortening Complete Plane Curves by Kai-Seng Chou & Xi-Ping Zhu. It is available at Link . Page 476 is relevant) Consider the partial differential equation on the domain $\mathbb{R}\times [0,T]$ : $$u_t - A(x,t)u_{xx} + \text{l.o.t} = f$$ where $$A(x,t) = \frac{(1-k_0(x)^2t)^2}{[(1-k_0(x)^2t)^2 + (k_{0_x}(x)t)^2]^2}$$ where $k_0(x)$ is the curvature of the curve $\gamma_0:\mathbb{R} \to \mathbb{R}^2$ . Recall that a PDE is uniformly parabolic if there exist positive constants $a$ and $b$ such that the term in the front of the Laplacian sits between $a$ and $b$ , i.e., $a \leq A(x,t) \leq b$ . Questions: Why is it true that $A$ is bounded in $C^{k, \alpha}(\mathbb{R} \times [0,T])$ if $\gamma_0 \in C^{k+4, \alpha}(\mathbb{R})$ ? Something to do with the fact that $k_0$ depends on $(\gamma_0)_{xx}$ ? Why is it true that If we restrict $T$ so that, for example, $$T < \frac{1}{2}\inf_x \frac{1}{1+k_0^2(x)},$$ then the PDE is uniformly parabolic. I don't see where that comes from at all. Thanks for any help.","['partial-differential-equations', 'analysis']"
171667,Solve $\ddot\theta +k\sin(2\theta)=0$ given initial value and constraints,"How is it possible to deduce from the equation $$\ddot\theta +k\sin(2\theta)=0$$ where $\theta=\theta(t)$ and $\tan(\theta)={b(t)\over a(t)}$, $k$ is constant, and $a(0)=a_0$, $a(t)^2+ b(t)^2=a_0^2$.
 that $a(t)=a_0\operatorname{sech}(c t)$ where $c$ is a constant? Thanks","['trigonometry', 'ordinary-differential-equations']"
171672,Closed forms for various expectations involving the standard normal CDF,"Suppose that $X\sim\mathcal{N}\left(0,1\right)$ (i.e., $X$ is a standard normal random variable) and $a,b,$ and $c$ are some real constants. Does any of the following expectations have a closed-form? $\mathbb{E}\left[\log\Phi\left(aX\right)\right]$ $\mathbb{E}\left[\Phi\left(aX\right)\log\Phi\left(aX\right)\right]$ $\mathbb{E}\left[\Phi\left(aX\right)\Phi\left(bX+c\right)\right]$ I've tried the usual derivative trick followed by Stein's lemma, but the expressions didn't get much simpler. For the third one if $a=b$ and $c=0$ the closed-form solution is $$\mathbb{E}\left[\left(\Phi\left(aX\right)\right)^2\right]=\frac{\tan^{-1}\left(\sqrt{1+2a^2}\right)}{\pi}$$ but I couldn't reach at any generalization.","['probability-theory', 'normal-distribution']"
171675,Prove $\frac{\sec{A}+\csc{A}}{\tan{A} + \cot{A}} = \sin{A} + \cos{A}$ and $\cot{A} + \frac{\sin{A}}{1 + \cos{A}} = \csc{A}$,Can anyone help me solve the following trig equations. $$\frac{\sec{A}+\csc{A}}{\tan{A} + \cot{A}} = \sin{A} + \cos{A}$$ My work thus far $$\frac{\frac{1}{\cos{A}}+\frac{1}{\sin{A}}}{\frac{\sin{A}}{\cos{A}}+\frac{\cos{A}}{\sin{A}}}$$ $$\frac{\frac{\sin{A} + \cos{A}}{\sin{A} * \cos{A}}}{\frac{\sin{A}}{\cos{A}}+\frac{\cos{A}}{\sin{A}}}$$ But how would I continue? My second question is $$\cot{A} + \frac{\sin{A}}{1 + \cos{A}} = \csc{A}$$ My work is $$\frac{\cos{A}}{\sin{A}} + \frac{\sin{A}}{1 + \cos{A}} = \csc{A}$$ I think I know how to solve this one by using a common denominator but I am not sure.,['trigonometry']
171682,Find all real solutions to $8x^3+27=0$,"Find all real solutions to $8x^3+27=0$ $(a-b)^3=a^3-b^3=(a-b)(a^2+ab+b^2)$ $$(2x)^3-(-3)^3$$ $$(2x-(-3))\cdot ((2x)^2+(2x(-3))+(-3)^2)$$ $$(2x+3)(4x^2-6x+9)$$ Now, to find solutions you must set each part $=0$. The first set of parenthesis is easy $$(2x+3)=0 ; x=-\left(\frac{3}{2}\right)$$ But, what I do not know is how to factor a trinominal (reverse of the FOIL method) I know that $(a+b)(c+d)=(ac+ad+bc+bd)$. But coming up with the reverse does not make sense to me. If someone can only tell me how to factor a trinomial that would be great.","['factoring', 'algebra-precalculus', 'polynomials']"
171688,$3\sin^2x=\cos^2x;$ $ 0\leq x\leq 2\pi$ Solve for $x$,"$3\sin^2x=\cos^2x;$ $0\leq x\leq 2\pi$ Solve for $x$: I honestly have no idea how to start this. Considering I'm going to get a number, I am clueless. I have learned about $\sin$ and $\cos$ but I do not know how to approach this problem. If anyone can go step-by-step with hints. That would be greatly appreciated. EDIT : $$3\sin^2x=1-\sin^2x$$ $$4\sin^2x=1$$ $$\sin^2x=\frac{1}{4}$$ $$\sqrt{\sin^2x}=\sqrt{\frac{1}{4}}$$ $$\sin x=\pm \left(\frac{1}{2}\right)$$",['trigonometry']
171692,Prove $\frac{\sin A \cos A}{\cos^2 A - \sin^2 A} = \frac{\tan A}{1-\tan^2 A}$,How would I simplify this difficult trigonometric identity: $$\frac{\sin A \cos A}{\cos^2 A - \sin^2 A} = \frac{\tan A}{1-\tan^2 A}.$$ I am not exactly sure what to do. I simplified the right side to $$\frac{\frac{\sin A}{\cos A}}{1-\frac{\cos^2 A}{\sin^2 A}}$$ But how would I proceed.,"['trigonometry', 'algebra-precalculus']"
171695,Closedness of sets under linear transformation,"Let $Y$ be a closed subset of $\mathbb{R}^m$ (in fact $Y$ is convex and compact, but I think the extra assumptions are irrelevant). Let $A \in \mathbb{R}^{n \times n}$ be a non-singular matrix (so $A^{-1}$ exists). Let $C \in \mathbb{R}^{m \times n}$ be any matrix. Is the set
$$ Y' = \{ C A x \in \mathbb{R}^m \, : \, x \in \mathbb{R}^n, C x \in Y\} $$
also closed? Note: just to be clear, the definition of $Y'$ means $Y' = C A X = \{ C A x \in \mathbb{R}^m \, : \, x \in X \}$ where $X = \{ x \in \mathbb{R}^n \, : \, C x \in Y \}$.","['general-topology', 'linear-algebra']"
171697,Boundedness of solutions of $x'+x+f(x)=0$,"Let $f \in C^1(\mathbb R, \mathbb R)$ and suppose 
  $$ \tag{H}
\vert f(x) \vert\le \frac{1}{2}\vert x \vert + 3, \quad \forall x \in \mathbb R. 
$$ Then, every solution of
  $$ 
x'(t)+x(t)+f(x(t))=0, \quad t \in \mathbb R 
$$
  is bounded on $[0,+\infty]$. First of all, I would like to say that the text has been copied correctly: I mean, it's really $[0,+\infty]$ so I suppose the author wants me to prove $\displaystyle \lim_{t \to +\infty} x(t) <\infty$. Indeed, boundedness on $[0,+\infty)$ is quite obvious, because we have global existence (its enough to write the equation as $x'=-x-f(x)$ and to observe that the RHS is sublinear thanks to $(H)$). So, we have to prove $\displaystyle \lim_{t \to +\infty} x(t) <\infty$. How can we do? I've got some ideas but I can't conclude. First, I observe that the problem is autonomous: this implies that solutions are either constant either monotonic. First idea: I've fixed $x_0 \in \mathbb R$ and I've written the equivalent integral equation: 
$$
x(t) = x_0 - \int_0^t [x(s)+f(x(s))]ds
$$ Taking the absolute value and making some rough estimates, we get 
$$
\vert x(t) \vert \le \vert x_0 \vert + \left\vert \int_0^t [x(s)+f(x(s))]ds \right\vert \le \vert x_0 \vert + \int_0^t \frac{3}{2}\left\vert x(s) \right\vert +3 ds 
$$
but now I don't know how to conclude. Gronwall's lemma? But how can I use it? Second idea: if $x_0 \in \mathbb R$ is s.t. $x_0 +f(x_0) \neq 0$, the solution of the Cauchy problem is not constant. I can divide both members of equation and I obtain (integrating on $[0,t]$)
$$
-t=\int_0^t \frac{dx}{x+f(x)}
$$
Now I let $t \to +\infty$ but... what can I conclude?","['ordinary-differential-equations', 'real-analysis']"
171715,Solve for $x$; $\cos^2x-\sin^2x=\sin x; -\pi\lt x\leq\pi$,"Solve for $x$; $\cos^2x-\sin^2x=\sin x; -\pi\lt x\leq\pi$
   $$\cos^2x-\sin^2x=\sin$$ Edit $$1-\sin^2x-\sin^2x=\sin x$$ 
$$2\sin^2 x+\sin x-1=0$$
$\sin x=a$
$$2a^2+a-1=0$$
$$(a+1)(2a-1)=0$$
$$x=-1,\dfrac{1}{2}$$
$$x=\sin^{-1}(.5)=30^{\circ}=\dfrac{\pi}{6}$$
$$x=sin^{-1}(-1)=-90^{\circ}=-\dfrac{\pi}{2}$$",['trigonometry']
171719,"If $n = m^3 - m$ for some integer $m$, then $n$ is a multiple of $6$","I am trying to teach myself mathematics (I have no access to a teacher), but I am not getting very far. I am just working through the exercises at the end of the book's chapter, but unfortunately there are no solutions. Anyway, I am trying to prove If $n = m^3 - m$ for some integer $m$, then $n$ is a multiple of $6$. But I do not know how to approach it. I thought of starting with something like $n = 6k$ for the multiple and that $m^3$ is crucial, but I do not know how that would help or where to go next. Does anyone have any hints or suggestions? Please do not post the whole proof because I want to solve it myself, thank you.","['elementary-number-theory', 'algebra-precalculus', 'divisibility']"
171720,Converse to Inverse Function Theorem?,"A fairly general form of the Inverse Function Theorem is: Suppose $X, Y$ are Banach spaces, $U \subset X$ is open and $f:U \to Y$ is continuously differentiable. If for some $x \in U$ the derivative $Df(x)$ is invertible, then there exists a neighborhood $V \subset f(U)$ such that $f(x) \in V$ and a continuously differentiable function $g: V \to U$ such that $f(g(x)) = x$ for all $x \in V$. A question I have had is whether there are any sufficient conditions such that the converse holds, i.e., if $Df(x)$ is not invertible then $f$ is locally not invertible.? Thanks in advance.","['differential-geometry', 'functional-analysis', 'real-analysis']"
171729,Why is the numerical range of an operator convex?,"Let $T$ be a Hilbert space operator. Its numerical range is \begin{equation} W(T)=\{\langle Tx,x\rangle:\|x\|=1\}.\end{equation} It is a well-known fact that $W(T)$ is a convex subset of the complex plane. However, every proof I know is by brute force computation. First for $2\times 2$ matrices, then the general case. Even though the computation can be carried out in clever ways, it still fails to provide some explanation why this is true. What is the link between this result and other concepts of the theory? I wonder whether there is any conceptual explanation for this result. I do not ask the explanation to be rigorous, just some ideas.","['operator-theory', 'spectral-theory', 'hilbert-spaces', 'functional-analysis']"
171736,Expressing the wave equation solution by separation of variables as a superposition of forward and backward waves.,"(From an exercise in Pinchover's Introduction to Partial Differential Equations ). $$u(x,t)=\frac{A_0 + B_0 t}{2}+\sum_{n=1}^{\infty} \left(A_n\cos{\frac{c\pi nt}{L}}+ B_n\sin{\frac{c\pi nt}{L}}\right)\cos{\frac{n\pi x}{L}}$$ is a general (and formal, at least) solution to the vibrating string with fixed ends. How to write this as a superposition of a forward and a backward wave? That is, as $f(x+ct)+f(x-ct)$ for some $f$. (No need to worry about rigour here, an heuristic will do.) I know, by elementary trigonometry, that $$\left(A_n\cos{\frac{c\pi nt}{L}}+ B_n\sin{\frac{c\pi nt}{L}}\right)\cos{\frac{n\pi x}{L}} =\\= (1/2)(A_n\cos +B_n\sin)\left(\frac{c\pi nt}{L} + \frac{n\pi x}{L}\right)+(1/2)(A_n\cos +B_n\sin)\left(\frac{c\pi nt}{L} - \frac{n\pi x}{L}\right), $$ but this doesn't seem to work because the variable $x$ is the one that changes sign, so apparently this cannot be interpreted as a sum of forward and backward waves. Is there a workaround to this? EDIT. The second wave is from another function $g$. The answer is then straightforward after oen's comment.","['fourier-series', 'physics', 'partial-differential-equations', 'wave-equation', 'analysis']"
171737,Continuous spectral value of right shift operator $\ell^2(\mathbb{N})$,"Let $T:\ell^2(\mathbb{N} \to \ell^2(\mathbb{N})$ be the operator that sends$(x_1,x_2,x_3,...) \to (0,x_1,x_2,x_3,....)$. I want to show that $\lambda = 1$ is in the continuous spectrum. To approach this, I first showed that $\lambda$ cannot be an eigenvalue. If it were, 
$Tx = x$ for some $x \in \ell^2$, which would imply $(0,x_1,x_2,x_3,....) = (x_1,x_2,x_3,....)$ and hence we have $x_1 = 0$, $x_2 = x_1$, $x_2 = x_3$, .... which implies $x = 0$. So, $\lambda = 1$ is not an eigenvalue. It is not in the residual spectrum, since the range of $T-I$ is dense. If it weren't, then its adjoint should have nonzero kernel, so Ker$(T^*-I)$ should be nonempty. However, $(T^* -I)x = 0$ implies $(x_1-x_2,x_2-x_3,x_3-x_4,... ) = 0$, so $x_1 = x_2 = x_3 = ....$. This will only be in $\ell^2$ if $x_1 = 0$, so $x = 0$, so the kernel is just $x = 0$, so we have the range of $T-I$ must be dense in $\ell^2$. Now, I get a bit stuck. I know so far that $\lambda = 1$ is neither an eigenvalue nor in the residual spectrum. To show that $\lambda = 1$ is in the continuous spectrum, I need to show that $(T-I)^{-1}$ is unbounded. I considered actually computing explicitly computing the inverse and showing that I can find $y_n \in \ell^2$ such that $||(T-I)^{-1} y_n||$ grows arbitrarily large (with $||y_n|| = 1$).  Here was my attempt, but it doesn't seem correct. Since $(T-I)x = (-x_1, x_1 - x_2, x_2, - x_3, ...)$, if we have $(T-I)^{-1} y = x$, then we know that $y_1 = -x_1$. We next see that $y_2 = x_1 - x_2$, so $x_2 = x_1 - y_2 = -y_ 1- y_2$. Next, we have $y_3 = x_2 - x_3$, so $x_3 = x_2 - y_3 = -y_1 - y_2 - y_3$. This pattern seems to repeat, so it appears that I find that the $j$-th entry of  $(T-I)^{-1} y = \sum_{i=1}^j -y_i$. If I try taking the norm of this, I find $||(T-I)^{-1}y||^2 = \sum_{j=1}^{\infty} ( \sum_{i=1}^j -y_i)^2$. My guess from here was to consider $y_k$ to be the sequence $(\frac{1}{n}$ where $n$ goes from $1$ to $k$ and zeros otherwise. Then, I have $y_k$ is bounded above by $\sqrt{\frac{\pi^2}{6}}$ in the norm (summing the series $\sum \frac{1}{n^2}$), but when I compute $||(T-I)^{-1}y_k||^2 = \sum_{j=1}^k (\sum_{i=1}^j -\frac{1}{i} )^2$, each component has a harmonic series term, so the norm of this thing should be growing very large and get larger as $k \to \infty$. So, even though $||y_k||^2 \leq \frac{\pi^2}{6}$, the  norms of $(T-I)^{-1} y_k$ grow arbitrarily large, so $(T-I)^{-1}$ is unbounded. Am I on the right track at least? If anyone has any suggestions on a better way to approach the problem, that would be very helpful. Is there a ""general"" approach to showing that a value is in the continuous spectrum? If anyone has a suggestion on a textbook that discusses these problems, that would also be quite helpful. Thanks!","['eigenvalues-eigenvectors', 'spectral-theory', 'functional-analysis']"
171738,Oblique asymptotes?,"A rational function, $\frac{p(x)}{q(x)}$ has an oblique asymptote only when the degree of $p(x)=$ degree of $q(x) -1$. What ""causes"" the ""slant"" of the asymptote? Most asymptotes are caused by a function approaching an undefined value - I assume this is the same, but why (unlike others) would these asymptotes be slanted? Why does this only work with a difference of 1 between degrees?",['algebra-precalculus']
171751,A strangely connected subset of $\Bbb R^2$,"Let $S\subset{\Bbb R}^2$ (or any metric space, but we'll stick with $\Bbb R^2$) and let $x\in S$.  Suppose that all sufficiently small circles centered at $x$ intersect $S$ at exactly $n$ points; if this is the case then say that the valence of $x$ is $n$.  For example, if $S=[0,1]\times\{0\}$, every point of $S$ has valence 2, except $\langle0,0\rangle$ and $\langle1,0\rangle$, which have valence 1. This is a typical pattern, where there is an uncountable number of 2-valent points and a finite, possibly empty set of points with other valences. In another typical pattern, for example ${\Bbb Z}^2$, every point is 0-valent; in another, for example a disc, none of the points has a well-defined valence. Is there a nonempty subset of $\Bbb R^2$ in which every point is 3-valent? I think yes, one could be constructed using a typical transfinite induction argument, although I have not worked out the details. But what I really want is  an example of such a set that can be exhibited concretely. What is it about $\Bbb R^2$ that everywhere 2-valent sets are well-behaved, but 
everywhere 3-valent sets are crazy? Is there some space we could use instead of $\Bbb R^2$ in which the opposite would be true?","['geometry', 'metric-spaces']"
171754,Sum of the series : $1 + 2+ 4 + 7 + 11 +\cdots$,"I got a question which says $$ 1 + \frac {2}{7} + \frac{4}{7^2} + \frac{7}{7^3} + \frac{11}{7^4} + \cdots$$ I got the solution by dividing by $7$ and subtracting it from original sum. Repeated for two times.(Suggest me if any other better way of doing this) . However now i am interested in understanding the series 1,2,4,7,11,..... In which the difference of the numbers are consecutive natural numbers. How to find the sum of $1+2+4+7+11+\cdots nterms$ This is my first question in MSE. If there are some guidelines i need to follow, which i am not, please let me know.",['sequences-and-series']
171757,Finding the remainder from equations.,I am having problems solving this question : When n is divide by 4 the remainder is 2 what will the remainder be when 6n is divided by 4 ? Ans=$0$ Here is what I have got so far $\frac{n}{4} => Remainder ~ 2$ so we get $n=4q+2$ $\frac{6n}{4} => Remainder ~ ?$ so we get $6n=4p+r_{emainder}$ How do we solve for remainder here ?,"['elementary-number-theory', 'algebra-precalculus']"
171773,Help with a partial fraction decomposition,One of my homework problems last week was to find the inverse Laplace transform of the following: $$F(s)=\frac{2s+1}{s^2-2s+2}.$$ The answer is $f(t)= 2e^t \cos t + 3e^t \sin t$. Obviously once you have the decomposed fraction the remainder of the problem is simple but I can't seem to get to that point.  I already turned in the assignment (they are ungraded) but I have a test coming up and want to make sure I'm ready.  Could someone please lay out the steps to decompose $F(s)$?,"['ordinary-differential-equations', 'laplace-transform', 'partial-fractions']"
171774,Evaluate $\int^1_0 \frac{dx}{x^2+x+1}$,"Here is an integral which I don't know the answer: $$\int^1_0 \frac{dx}{x^2+x+1}$$ I tried to use complex number to solve i.e. the root of $x^2+x+1$ is $(-1/2 +  \sqrt {3}i/2)$.
Let w=$(-1/2 +  \sqrt {3}i/2)$ , then it becomes $ \int_{0}^{1} 1/(x-w)^2\,\mathrm{d} (x-w)$ , the answer is in terms of complex number. I wanna ask whether my method is correct and seek for another method. Thank you.","['definite-integrals', 'integration']"
171776,infinite sums of trigonometric functions,"Find the sum of the series:
$$\sum_{n = 1}^\infty \left( \sin \left(\frac{1}{n}\right) - \sin\left(\frac{1}{n+1} \right) \right).$$ By the convergence test the limit of this function is $0$ but I'm not sure how to prove whether or not this function converges or diverges.",['sequences-and-series']
171782,How to get better at algebraic manipulations?,"I've always been able to manipulate equations found in school homework easily. But when tackling more challenging questions from puzzle books - where I might need three quarters of a page to manipulate the equation into the ideal form - I find myself easily making mistakes. The obvious solution is more practice. But I can't find a suitable landfill of questions where the focus is on manipulating tediously long/complicated algebraic equations instead of practicing some kind of technique or problem. Has anyone had the same problem as me and found a felicitous way to overcome it? (Other than the obvious advice to be more careful, which you have to sacrifice speed for unless you've already had lots of practice) P.S. I'm not sure if this question belongs here; if it does not, I'll be glad to remove it.","['education', 'algebra-precalculus', 'soft-question']"
171791,Computing a Laurent series,"Let $$f(z) = \frac{1}{(2z-1)(z-3)} $$. Compute the Laurent series about the point z = 1 in the annular domain $$ \frac{1}{2} < |z-1| < 2$$ My attempt:
I broke f(z) up into the partial fraction decomposition: $$ -\frac{2}{5(2z-1)} + \frac{1}{5(z-3)} = -\frac{2}{5}*\frac{1}{(1-\frac{(z+\frac{1}{2})}{2})} +\frac{1}{5}*\frac{1}{1-(z-2)} = $$ $$-\frac{2}{5}\sum_{n=0}^\infty(-1)^{n}\frac{(z+1)^{n}}{2^n}-\frac{1}{5}\sum_{n=0}^\infty(z-2)^n $$ And that was my answer. But I was told I was wrong, and I'm not sure where I went wrong in there. So if someone could point out where I went wrong, it would be greatly appreciated!",['complex-analysis']
171792,Solve for $x$; $\tan x+\sec x=2\cos x;-\infty\lt x\lt\infty$,"Solve for $x$; $\tan x+\sec x=2\cos x;-\infty\lt x\lt\infty$
  $$\tan x+\sec x=2\cos x$$ $$\left(\dfrac{\sin x}{\cos x}\right)+\left(\dfrac{1}{\cos x}\right)=2\cos x$$
$$\left(\dfrac{\sin x+1}{\cos x}\right)=2\cos x$$
$$\sin x+1=2\cos^2x$$
$$2\cos^2x-\sin x+1=0$$ Edit :
$$2\cos^2x=\sin x+1$$
$$2(1-\sin^2x)=\sin x+1$$
$$2\sin^2x+\sin x-1=0$$
$\sin x=a$
$$2a^2+a-1=0$$
$$(a+1)(2a-1)=0$$
$$a=-1,\dfrac{1}{2}$$
$$\arcsin(-1)=-90^\circ=-\dfrac{\pi}{2}$$
$$\arcsin\left(\dfrac{1}{2}\right)=30^\circ=\dfrac{\pi}{6}$$
$$180^\circ-30^\circ=150^\circ=\dfrac{5 \pi}{6}$$
$$x=\dfrac{\pi}{6},-\dfrac{\pi}{2},\dfrac{5 \pi}{6}$$
I actually do not know if those are the only answers considering my range is infinite:$-\infty\lt x\lt\infty$",['trigonometry']
171803,How to solve this Pell's equation $x^{2} - 991y^{2} = 1 $,"How to solve the following Pell's equation? $$x^{2} - 991y^{2} = 1 $$
where $(x, y)$ are naturals. The answer is $$x = 379,516,400,906,811,930,638,014,896,080$$ 
$$y = 12,055,735,790,331,359,447,442,538,767$$ I can't think of any way apart from brute force. Please help. Also, what is the general way of solving any Pell's equation? I read the wiki article on it but couldn't get any general method to solve it.","['diophantine-equations', 'number-theory']"
171827,"How to find convergence region of $\sum_{n\geqslant 0, m \geqslant 0} x^n y^m \binom{n+m}{n}^2$","The following two series are special cases of Appell $F_3$ and $F_4$, namely:
$$
  \mathcal{S}_1 = \sum_{n \geqslant 0, m \geqslant 0} \frac{x^n y^m}{\binom{n+m}{n}}
$$
and
$$
  \mathcal{S}_2 = \sum_{n \geqslant 0, m \geqslant 0} \binom{n+m}{n}^2 x^n y^m
$$
How would one establish that $\mathcal{S}_1$ converges for $\{ (x,y)\colon -1<x<1, -1<y<1 \}$, and $\mathcal{S}_2$ converges for $\{ (x,y) \colon \sqrt{|x|} + \sqrt{|y|} < 1\}$.","['sequences-and-series', 'reference-request']"
171839,Evaluating the contour integral: $\oint_C \frac{\sin 2z}{(6z-\pi)^3}dz$,"I am trying to evaluate the following integral, but don't know how to take the coefficient of $z$ out of the parenthesis to get it into the Cauchy integral form. Any help is appreciated. $$ \oint_C \frac{\sin 2z}{(6z-\pi)^3}dz$$","['complex-analysis', 'contour-integration']"
171851,Finding $\sum\limits_{i=1}^m \frac{i(n-i)! }{ n!}$,What will be the value of the following summation? $$\sum_{i=1}^{m}{ \frac{i(n-i)!}{ n!} }$$ Is it $\frac{m}{2}$ ? Can anybody show the derivation?,"['sequences-and-series', 'real-analysis', 'combinatorics']"
171856,How would I calculate the area of the shaded region of a circle with radius $6$ and length of chord $AB=6. $,How would I calculate the area of the shaded region of a circle with radius 6 and length of chord AB is 6.,"['geometry', 'circles']"
171859,Length of chord in circle - Which property,"In the figure AB=4 , BC=6 , AC=5 and AD=6 what is length of DE ? Ans=9 I know there must be some property here that would solve this problem instantly but I cant figure it out any suggestions ?
Edit:
Since the two triangles are similar there corresponding sides will be equal in ratio , however I am still getting the wrong answer BA   CA   BC
4    5    6
AE   6    DE $$AE = \frac{24}{5}$$
and $$DE = \frac{36}{5}$$ Where am I going wrong ?","['geometry', 'circles']"
171868,connected component of $\rm{GL_n}(\mathbb{R})$ [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: How many connected components does $\mathrm{GL}_n(\mathbb R)$ have? I know $\rm{GL_n}(\mathbb{R})$ is not connected and connected components are $C_1 =\{A:\rm{det}\ A>0\}$ and $C_2=\{A:\rm{det} \ A<0\}$. Given that $C_1= \rm{det}^{-1}(0,\infty)$ and $C_2=\rm{det}^{-1}(-\infty,0)$, $\rm{det}:M_n(\mathbb{R})\to \mathbb{R}$. But how can one prove  that $C_1$ and $C_2$ are connected in $\rm{M_n}(\mathbb{R})$? Thank you.",['general-topology']
171869,Does $\sum\limits_{n=1}^\infty \frac{n^n}{3^n n!}$ converge?,"Test the convergence of the series $$\sum_{n=1}^\infty \frac{n^n}{3^n n!}$$ I know that if the nth term tends to $\infty$ then the series is divergent and if it is tends to 0 it is convergent . Also I'm familiar with some test e.g. Ratio test, d'Alembert test, comparison test etc. But I could not solve it in proper way.
I know as $n$ increases $n^n$ increases more rapidly than $n!$ or $3^n$ but no idea when they both are multiplied","['sequences-and-series', 'calculus', 'real-analysis']"
171877,"Fundamental domain of $\operatorname{GL}(n,\mathbb R)$ acted on by $\operatorname{GL}(n, \mathbb Z)$","What is a simple description of a fundamental domain of $\operatorname{GL}(n,\mathbb R)$ acted on by $\operatorname{GL}(n,\mathbb Z)$? $\operatorname{GL}(n,\mathbb R)$ is the group of all real invertible matrices with matrix multiplication, $\operatorname{GL}(n,\mathbb Z)$ the group of all matrices with integer entries, whose inverses also have integer entries, with matrix multiplication. $h \in \operatorname{GL}(n, \mathbb Z) \subset \operatorname{GL}(n,\mathbb R)$ acts on $g \in \operatorname{GL}(n,\mathbb R)$ by letting $h\cdot g := hg$ Remarks: A fundamental domain $F$ is a subset of $\operatorname{GL}(n,\mathbb R)$ such that for any $x$ in $\operatorname{GL}(n,\mathbb R)$ there is exactly one $h$ in $\operatorname{GL}(n, \mathbb Z)$ such that $hx \in F$. I'm looking for an as clean as possible description of some $F$ in terms of the matrix entries. Clearly $\operatorname{GL}(n,\mathbb R)$ can be replaced with any set of (possibly not invertible) matrices with $n$ rows (possibly with few or more than n columns). The case with one column and $n=2$ is not too different from finding a fundamental domain of the upper half plane with respect to Möbius transformations. Also, $\operatorname{GL}$ could have been replaced with $\operatorname{SL}$. This appears as a very basic question to me, and if it turns out I'm ignorant of some useful tools or theorems I will accept pointers to such. In fact this would be even better than a direct answer to the specific question (since I have many related seemingly basic questions), as long as it helps significantly in answering the specific question.","['lattices-in-lie-groups', 'matrices', 'reference-request', 'lie-groups']"
171878,Formula obtained by using Trignometric approximation for a triangle with a very small side,"I am reading a paper on the force between hooft polyakov monopoles, but I am completely baffled by one of the 'elementary trignometric' equation they have got using an approximation. Consider a triangle say triangle ABC. The author says that when A is very close to B, i.e. when $\cos{C}\approx 1$, we get $\cos{C}-1=-\frac{1}{2a^2}c^{2}\sin^2{B}$. Please can anyone tell me, how has this been done. I am extremely sorry if this is a silly question.","['geometry', 'trigonometry']"
171904,Limit of a sequence involving root of a factorial: $\lim_{n \to \infty} \frac{n}{ \sqrt [n]{n!}}$ [duplicate],"This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) Closed 9 years ago . I need to check if
$$\lim_{n \to \infty} \frac{n}{ \sqrt [n]{n!}}$$ converges or not. Additionally, I wanted to show that the sequence is monotonically increasing in n and so limit exists. Any help is appreciated. I had tried taking log and manipulating the sequence but I could not prove monotonicity this way.","['convergence-divergence', 'sequences-and-series', 'radicals', 'factorial', 'limits']"
