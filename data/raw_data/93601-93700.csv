question_id,title,body,tags
1278508,Is finiteness necessary in this exercise?,"This is from Dummit&Foote Abstract Algebra Chap 3.1 Problem 28. Here is problem & solution of this problem. Let $N$ be finite subgroup of $G$ and suppose $G=\langle T\rangle$ and $N=\langle S\rangle$ for some subsets $S$ and $T$ of $G$. Prove $N$ is normal in $G$ iff $tSt^{-1} \subseteq N$ for all $t \in T$. https://crazyproject.wordpress.com/2010/03/01/a-criterion-for-finite-subgroup-normalcy-on-generating-sets-of-the-subgroup-and-group/ Finiteness of $N$ is used as: $tNt^{-1}=N$ from $tNt^{-1} \subseteq N$. If $N$ is infinite, then $G$ is infinite, so $t^{-1}$ is not always expressed as finite product of elements of $T$. So we can't get $t^{-1}Nt\subseteq N$ from $tNt^{-1} \subseteq N$. So we can't avoid line 
$tNt^{-1}=N$ from $tNt^{-1} \subseteq N$. without finiteness of $N$. I think finiteness of $N$ is crucial in this proof, so I think there is counterexample of this excercise without finiteness. Also hardness of taking counterexample is making $T$ s.t. $\langle T\rangle=G$. But, it should be $T \neq G$ because it implies normalness. I'm trouble to taking non abelian group $G$ and $T\neq G$ s.t. $\langle T\rangle=G$. Can you find counterexample of this excercise or is that still true for without finiteness of $N$?",['group-theory']
1278548,Ackermann's function is $\mu$-recursive,"In my book there is the following proof that Ackermann's function is $\mu$-recursive: We propose to show that Ackermann's funcition is $\mu$-recursive. The first part of the job is to devise a scheme whereby each step of an evaluation of Acerkmann's function can be represented by a single natural number. We will then show that the numbers associated with successive steps in an evaluation can be obtained from a primitive recursive function and that the result of the evaluation can be extracted by a suitably defined $\mu$-recursive function. We begin with the problem of representing evaluations numerically. Acekrmann's function is defined by three rules: Rule 1 : $A(0, y)=y+1$ Rule 2 : $A(x+1, 0)=A(x, 1)$ Rule 3 : $A(x+1, y+1)=A(x, A(x+1, y))$ Three facts are readily apparent from a study of Rules $1$ to $3$: First, each step in the evaluation of $A(x, y)$ results either in a single natural number or else in a nested expression of the form $$A(w_1, A(w_2, A(w_3, \dots A(w_{k-1}, w_k) \dots )))$$ Second, exactly one of the three rules is applicable at any given step. And third, no matter which rule applies, it must always be applied to the rightmost $A$ in the given expression-i.e., it must always be applied at the innermost level of the nest. Perhaps less obvious than these observations is the fact that the evaluation procedure just outlined must always terminate after a finite number of steps. Lemma 1 . For each choice of natural numbers $x$ and $y$, the expression $A(x, y)$ can be reduced to a single natural number by means of a finite number of applications of Rules $1$, $2$, and $3$. Let us now devise a method of representing the steps of an evaluation numerically. Remember that each step consists of an expression of the form $$A(w_1, A(w_2, \dots , A(w_{k-1}, w_k) \dots ))$$ Since the terms in such an expression are always associated to the right, most of the punctuation can be oitted without loss of information. In particular, the xression $A(w_1, A(w_2, \dots , A(w_{k-1}, w_k) \dots ))$ can b unambiguously represented by the $k$-tuple $$(w_1, \dots , w_{k-1}, w_k)$$ Thus each step in the evaluation of Ackermann's function can be described by a tuple of natural numbers. We next use a Gödel-numbering scheme to reduce the description of each step in an evaluation to a single natural number. In particular, we choose to represent the tuple $(w_1, \dots , w_k)$ by the natural number $$2^k 3^{w_1} \cdots p_k^{w_k}$$ Note the length of the tuple is encoded as the exponent of two, while the components of the tuple are encoded as the exponents of the succeding primes. This provides a single way of telling how many components to ""look for"" in a given Gödel number. This scheme allows us to describe the evaluation of Ackermann's function with a sequence of natural numbers. Such a numbering scheme is referred to as an arithmetization of the evaluation of Ackermann's function. Note that the steps of the evaluation are numbered, starting with zero, and the zeroth step correpsonds to the expression $A(x, y)$ whose value is to be determined. We now define the three-variable function $\psi$ so that $\psi(x, y, n)$ is theGödel number of the nth step in the evaluation of $A(x, y)$. In orrder to make $\psi$ a total function, we agree to ""extend"" each evaluation of Ackermann's function as follows. If the actual evaluation of $A(x, y)$ terminates at step $n_0$ and this step  is assigned the Gödel number $z$, we simply set $\psi(x,y,n)=z$ for all $n \geq n_0$. The resulting function $\psi$ will be referred to as the trace function for the evaluation of $A$. One next job is to sho wthta $\psi$ is a primitive recursive function. For this purpose it is conveninet to be able to view every natural number $z$ as being a Gödel number of some tuple. The simplest approach is to ignore any ""extra"" primes in the decomposition of $z$. Thus the natural number $$2^k3^{w_1}5^{w_2} \cdots p_k^{w_k}p_{k+1}^{w_{k+1}} \cdots p_m^{w_m}$$ will be treated as if it were the Gödel number of the $k$-tuple $(w_1, \dots , w_k)$. With this convention in mind, we now assign an auxiliary predicate and an auxiliary function to each of the rules defining Ackermann's function. For each of the values $i=1$, $i=2$, and $i=3$, lewt $Q_i$ denote the predicate such that: $$Q_i(z) \Leftrightarrow z \text{ is a Gödel number of a tuple to which Ruile } i \text{ applies }$$ And let $h_i$ denote a function such that, if $Q_i(z)$ holds, then: $$h_i(z) \text{ is the Gödel number of the tuple that results when Rule }$$ $$i \text{ is applied to the tuple represented by the number } z$$ If $Q_i(z)$ does not hold-i.e., if $z$ does not represnet a tuple to which Rule $i$ applies, the value assumed by $h_i(z)$ is unimportant. (We will in fact choose these ""default values"" so as to ensure the primitive recursiveness of $h_i$.) The predicates $Q_1, Q_2, Q_3$ and the functions $h1, h_2,h_3$ provide an easy way of specifying the trace function $\psi$. Since $\psi(x, y, 0)$ is just the Gödel number of the tuple $(x, y)$, we have: $$\psi(x, y, 0)=2^23^x5^y$$ And since $
\psi(x, y, n+1)$ can be obtained from $\psi(x, y, n)$ by determining whiich rule applies to the tuple represented by $\psi(x, y, n)$ and then computing the effect of applying that rule, we can write: $$\psi(x, y, n+1)=\begin{cases} h_1(\psi(x, y, n)) & \text{ if } Q_1(\psi(x, y, n)) \\ h_2(\psi(x, y, n)) & \text{ if } Q_2(\psi(x, y, n)) \\ h_3(\psi(x, y, n)) & \text{ if } Q_3(\psi(x, y, n)) \\ \psi(x, y, n) & \text{ otherwise} \end{cases}$$ Thus to show that $\psi$ is primitive recursive, it is only necessary to show that the predicates $Q_1, Q_2, Q_3$ and the functions $h_1, h_2, h_3$ are primtive recursive. To this end, we introduce one more auxiliary function $L$, whose values for argument $z$ is the length of the tuple represented by $z$. Since $$L(z)=E(0, z)$$ the function $L$ is certainly primitve recursive. Now, the predicate $Q_1(z)$ is to hold iff $z$ represnets a tuple containing at least two components, of which the next-to-last is zero. Thus we can write $$Q_1(z) \Leftrightarrow (L(z)>1) \wedge (E(L(z)\overset{\cdot }{-}1, z)=0)$$ Similar reasoning yields the following definitions of $Q_2$ and $Q_3$, as the reader may verify. $$Q_2(z) \Leftrightarrow (L(z)>1) \wedge (E(L(z)\overset{\cdot}{-}1, z)\neq 0) \wedge (E(L(z), z)=0) \\ Q_3(z) \Leftrightarrow (L(z)>1) \wedge (E(L(z)\overset{\cdot}{-}1, z)\neq 0)\^ (E(L(z), z) \neq 0)$$ Thus the predicates $Q_1$, $Q_2$, and $Q_3$ are all primitive recursive. The functions $h_1$, $h_2$. and $h_3$ are only slightly more difficult to deal with. First consider the case in which $z$ represents a tuple to which Rule $1$ applies. That is, suppose that the prime decomposition of $z$ has the form: $$z=2^k3^{w_1} \cdots p_{k-2}^{w_{k-2}}p_{k-1}^0p_k^{w_k} \cdots p_m^{w_m}$$ Inspection of Rule $1$ reveals that in ythis case the appropriate value of $h_1(z)$ is: $$2^{k-1}3^{w_1} \cdots p_{k-2}^{w_{k-2}}p_{k-1}^{w_{k+1}}$$ We are therefore led to define $h_1$ as follows: $$h_1(z)=2^{L(z)\overset{\cdot}{-}1} \left [ \prod_{i=1}^{L(z)\overset{\cdot}{-}2}pn(i)^{E(i, z)}\right ] pn(L(z)\overset{\cdot}{-}1)^{E(L(z), z)+1}$$ The choice of this definition ensures that $h_1$ is primitive recursive and that $h_1(z)$ assumes the required values whenever $Q_1(z)$ holds. (Rememver that it does not matter what value $h_1(z)$ assumes when $Q_1(z)$ does not hold.) Similar observations concerning Rules $2$ and $3$ lead to the following definitions of the functions $h_2$ and $h_3$. $$h_2(z)= \left [ \prod_{i=1}^{L(z)\overset{\cdot}{-}2}pn(i)^{E(i, z)}\right ] pn(L(z)\overset{\cdot}{-}1)^{E(L(z)\overset{\cdot}{-}1, z)\overset{\cdot }{-}1}pn(L(z)) \\ h_3(z)=\dots $$ Thus the desired functions $h_1$, $h_2$, and $h_3$, like the predicates $Q_1$, $Q_2$, and $Q_3$, are primitive recursive. And this in urn implies that the trace function $\psi$ is primitive rcursive. We are now ready to show that Ackermann's function is $\mu$-recursive. Fr this purpose we define the two-variable function $\eta$ so that $\eta(x, y)$ is te number of steps needed to evaluate $A(x, y)$. Evidently $\eta(x, y)$ is the smallest value of $n$ for which $\psi(x, y, n)$ represents a single natural number. Thus: $$\eta (x, y) =\mu n(L(\psi(x, y, n))=1]$$ Since $L$ and $\psi$ aare primitive recursive, it follows that $\eta$ is a $\mu$-recursive function. Next note that the outcome of the evaluation of $A(x, y)$ is just the value of the natural number represented by $\psi(x, y, \eta(x, y))$. We may therefore write $$A(x, y)=E(1, \psi(x, y, \eta (x, y)))$$ Since $E$ and $\psi$ are primitive recursive and $\eta$is $\mu$-recursive, we have the finally established: Theorem: Ackermann's function is $\mu$-recursive. $$$$ Could you explain to the idea of the proof?? I haven't understood
  it... Why do we need the functions $h_i$, $\psi$, and so on to show
  that Ackermann's function is $\mu$-recursive??","['computability', 'ackermann-function', 'discrete-mathematics', 'recursion']"
1278550,Peculiar locations of the root and the maximum of $(x+1)^{x+1}-x^{x+2}$,"Related to some other problems, I got interested in this function: $$(x+1)^{x+1}-x^{x+2}$$ Its root is very close to $\pi$. This is Mathematica code that finds the root: NSolve[Power[x + 1, x + 1] - Power[x, x + 2] == 0, x, Reals] {{x -> 3.14104}} ($\pi \simeq 3.14159$) Moreover, the location of its maximum is very close to $e$: FindMaximum[Power[x + 1, x + 1] - Power[x, x + 2], {x, 2.65, 2.78}] {20.0645, {x -> 2.70965}} ($e\simeq 2.71828$) I find these facts almost disturbing. Is there a deeper mathematical explanation for such ""close but not quite"" behavior? I tried various ways of approximation of this function (similar to this ), but don't have enough math knowledge and background to get any result that makes sense. Here is the graph of the function:","['exponential-function', 'pi', 'functions']"
1278558,Can you solve a quadratic equation using matrices?,"I was wondering whether there are any alternatives or more efficient methods to finding a solution to a quadratic equation other than simply trial and error or by using the quadratic formula. I was once told that it could be very easily done using matrices. How would this work? Additionally, are there any other ""better"" alternatives? I would really appreciate if you were to give me examples and explain how to solve them with the alternate method. Thank you :)","['quadratics', 'matrices']"
1278600,Let ${f_n}$ be a sequence of integrable functions on $\mathbb{R}$ such that $f_n\rightarrow f$ almost everywhere.,"Let ${f_n}$ be a sequence of integrable functions on $\mathbb{R}$ such that $f_n\rightarrow f$ almost everywhere. We also have $f\in L^1(\mathbb{R})$ and $\int_{\mathbb{R}}f_n\rightarrow \int_{\mathbb{R}}f$. I want to prove: For all $\epsilon>0$, there exist a measurable set $A\subset\mathbb{R}$ with finite measure, an integrable function $g\geq 0$, and an integer$N\in \mathbb{N}$ such that for all $n\geq N$, $|\int_{\mathbb{R}\backslash A}f_n|<\epsilon$ and $|f_n|\leq g$ on $A$. This question appears in one of the past papers of the real analysis course I am taking this semester. But I have no idea how to do it.","['real-analysis', 'measure-theory']"
1278610,$f(x)=\sum_{i=0}^{\infty} (x^{2^n})/(1-x^{2^{n+1}})$. Find $f(99)$.,$f(x)=\sum_{i=0}^{\infty} (x^{2^n})/(1-x^{2^{n+1}})$. Find $f(99)$. ATTEMPT: The following series can be re-written as $f(x)=\sum_{i=0}^\infty \left(\frac{1}{1-x^{2^n}}\right) \cdot \left( \frac{1}{1+x^{2^n}}\right)$ and then expanded along.,"['power-series', 'sequences-and-series', 'algebra-precalculus']"
1278665,How to check if two rectangles intersect? Rectangles can be rotated,How to check if two rectangles intersect? Each rectangle is defined by three points in 2d space. The rectangles can be rotated around any point as on the image below.,"['rectangles', 'geometry']"
1278681,"Prob 9, Sec 26, in Munkres' TOPOLOGY, 2nd ed: How to prove the generalised tube lemma?","The tube lemma is as follows: Let $X$ and $Y$ be topological spaces. Let $Y$ be compact. Let $x \in X$ . If $N$ is an open set in $X \times Y$ such that $x \times Y \subset N$ , then there is an open set $W$ in $X$ such that $x \in W$ and $W \times Y \subset N$ . This is Lemma 26.8 in Topology by James R. Munkres, 2nd edition. I think I'm clear about it and its proof. Now how (to use the above result ) to derive the following? Let $X$ and $Y$ be topological spaces. Let $A$ and $B$ be subspaces of $X$ and $Y$ , respectively; let $N$ be an open set in $X \times Y$ such that $A \times B \subset N$ . Suppose that $A$ and $B$ are compact. Then there exist open sets $U$ and $V$ in $X$ and $Y$ , respectively such that $$A \times B \subset U \times V \subset N.$$ My effort: (Based upon the valuable feedback from @GregoryGrant. ) Fix an $a \in A$ . Then, for each $b \in B$ , there exists an open set $U_{a, b}$ in $X$ and there exists an open set $V_{a,b}$ in $Y$ such that $$a \times b \in U_{a, b} \times V_{a, b} \subset N. \tag{0} $$ In this way, we obtain a covering $\left\{ \ V_{a, b} \ \colon\  b \in B\  \right\}$ of $B$ by sets open in $Y$ . Since $B$ is compact, there are finitely many of these sets, say, $V_{a, b_1}, \ldots, V_{a, b_n}$ such that $$B \subset \bigcup_{j=1}^n V_{a, b_j}. \tag{1} $$ Let us put $$V_a \colon= \bigcup_{j=1}^n V_{a, b_j} \tag{2} $$ and $$U_a \colon= \bigcap_{j=1}^n U_{a, b_j}. \tag{3} $$ Then $U_a$ is open in $X$ , $V_a$ is open in $Y$ , $a \in U_a$ , and $B \subset V_a$ . Hence $$ a \times B \subset U_a \times V_a. \tag{4} $$ Moreover, if $x \times y \in U_a \times V_a$ , then $x \in U_a$ and $y \in V_a$ ; so $x \in U_{a, b_j}$ for each $j = 1, \ldots, n$ [Refer to (3) above.] and $y \in V_{a, b_k}$ for some $k = 1, \ldots, n$ [Refer to (2) above.]; thus $x \times y \in U_{a, b_k} \times V_{a, b_k}$ . But by virtue of (0) above $$U_{a, b_k} \times V_{a, b_k} \subset N.$$ So $x \times y \in N$ . Hence we have shown that $$ U_a \times V_a \subset N. \tag{5} $$ Therefore, by (4) and (5) we can conclude that, for each element $a \in A$ , we have a basis element $U_a \times V_a$ for the product topology on $X \times Y$ such that $$a \times B \subset U_a \times V_a \subset N. \tag{6} $$ Now $\left\{\ U_a \ \colon \ a \in A \  \right\}$ is a covering of $A$ by sets open in $X$ ; so there are finitely many of them to cover $A$ ; that is, there are finitely many sets, say, $U_{a_1}, \ldots, U_{a_m}$ open in $X$ such that $$A \subset \bigcup_{i=1}^m U_{a_i}. \tag{7} $$ Let's take $$U \colon= \bigcup_{i-1}^m U_{a_i} \tag{8} $$ and $$V \colon= \bigcap_{i=1}^m V_{a_i}. \tag{9} $$ Then the set $U$ is open in $X$ and the set $V$ is open in $Y$ . Now we show that $U \times V \subset N$ . Let $x \times y \in U \times V$ . Then $x \in U$ and $y \in V$ . So $x \in U_{a_r}$ for some $r = 1, \ldots, m$ [Refer to (8) above.] and $y \in V_{a_i}$ for each $i = 1, \ldots, m$ [Refer to (9) above.]. Thus, $x \times y \in U_{a_r} \times V_{a_r}$ . But by virtue of (6) above $$ U_{a_r} \times V_{a_r} \subset N. $$ So $x \times y \in N$ , showing that $U \times V \subset N$ . Now we show that $A \times B \subset U \times V$ . If $a \times b \in A \times B$ , then $a \in A$ and $b \in B$ . So $a \in U_{a_s}$ for some $s = 1, \ldots, m$ [Refer to (7) above.] and $b \in V_{a_i}$ for each $i = 1, \ldots, m$ [Refer to (6) and (9) above.]. Thus [Refer to (8) and (9) above.] $a \in U$ and $b \in V$ . So $a \times b \in U \times V$ and thus $A \times B \subset U\times V$ , as required. From the preceding two paragraphs we have obtained $$ A \times B \subset U \times V \subset N. $$ Moreover, $U$ is open in $X$ and $V$ is open in $Y$ . Is this proof correct now? My initial attempt: For any $a \in A$ and for any $b \in B$ , we have $a \times B \subset N$ and $A \times b \subset N$ . Since $a \times b \in N$ , there is an open set $U_a$ in $X$ and an open set $V_b$ in $Y$ such that $a \times b \in U_a \times V_b \subset N$ . Now $\{U_a \colon a \in A \}$ is a covering of $A$ by sets open in $X$ . So there are finitely many points $a_1, \ldots, a_m$ such that $$A \subset \cup_{i=1}^m U_{a_i}.$$ Similarly, $\{V_b \colon b \in B \}$ is a covering of $B$ by sets open in $Y$ , and since $B$ is compact, therefore there are finitely many points $b_1, \ldots, b_n$ such that $$B \subset \cup_{j=1}^n V_{b_j}.$$ Can we take $$U \colon= \cup_{i=1}^m U_{a_i} \ \ \ \mbox{ and } \ \ \ V \colon= \cup_{j=1}^n V_{b_j}?$$","['general-topology', 'compactness']"
1278693,Finding $\prod_{k=1}^{n-1}\cos\frac{2k\pi}n$,"Finding $$\mu=\prod_{k=1}^{n-1}\cos\frac{2k\pi}n$$
I thought $$z^n=1=e^{i2\pi}\implies z=\cos\frac{2k\pi}n+i\sin\frac{2k\pi}n\quad k\in\{1,2,...,n-1\}$$
Now we have:
$$\begin{align}\mu&=\prod_{k=1}^{n-1}\frac{z+\bar z}{2}\\&=\prod_{k=1}^{n-1}\frac{e^{i\theta}+ e^{-i\theta}}{2}\\&=\frac1{2^{n-1}}\prod_{k=1}^{n-1}e^{-i\theta}\prod_{k=1}^{n-1}(1+ e^{2i\theta})\\
\\&=\frac1{2^{n-1}}e^{-i\frac{2\pi}n(1+2+...(n-1))}\prod_{k=1}^{n-1}(1+ e^{2i\theta})\\
\\&=\frac1{2^{n-1}}e^{-i(n-1)\pi}\prod_{k=1}^{n-1}(1+ e^{2i\theta})\\
\\&=\frac1{2^{n-1}}(-1)^{n-1}\prod_{k=1}^{n-1}(1+ e^{2i\theta})\end{align}$$
Now since $(e^{2i\theta})^{n/2}=1$:
$$\Theta(z)=z^{n/2}-1=\prod_{k=1}^{n-1}(z-e^{2i\theta})$$
At $(-1)$:
$$\Theta(-1)=(-1)^{n-1}\prod_{k=1}^{n-1}(1+e^{2i\theta})=(-1)^{n/2}-1=e^{in\pi/2}-1=i\sin n\pi/2$$
It turns out something imaginary (but it's not clearly) also I would like to work on:
$$\nu=\prod_{k=1}^{n-1}\sin\frac{2k\pi}n=\prod_{k=1}^{n-1}\frac{e^{i\theta}- e^{-i\theta}}{2i}$$
Similarly:
$$\nu=\frac1{(2i)^{n-1}}(-1)^{n-1}\prod_{k=1}^n(e^{2i\theta}-1)=\frac1{(2i)^{n-1}}(-1)^{n-1}(-1)^{n-1}\Theta(1)=\frac1{2^{n-1}i\sin (n-1)\pi/2}0=0$$
But this is also not zero?
The earlier $\eta$ is obviously:
$$\prod_{k=1}^{n-1}\tan\frac{2k\pi}n=\eta=\frac{\mu}{\nu}$$","['complex-numbers', 'trigonometry']"
1278706,"Find $\int \frac{1}{x^4+x^2+1} \,\, dx$","Find $$\int \frac{1}{x^4+x^2+1} \,\, dx$$ I tried to find like that: $\int \frac{1}{x^4+x^2+1} = \int \frac{\frac{1}{2}x + \frac{1}{2}}{x^2+x+1} \,\, dx + \int \frac{-\frac{1}{2}x + \frac{1}{2}}{x^2-x+1} \,\, dx = \frac{1}{2} \Big(\int \frac{2x + 1}{x^2+x+1} - \int \frac{x}{x^2+x+1} \Big) - \frac{1}{2} \Big( \int \frac{2x - 1}{x^2-x+1} \,\, dx - \int \frac{x}{x^2-x+1} \,\, dx \Big)$ but then I don't know how to find integrals:
$\, \int \frac{x}{x^2-x+1} \,\, dx \,$ and  $\, \int \frac{x}{x^2+x+1} \,\, dx \,$ Is there another way to integrate this function or way to end my calculations?","['calculus', 'real-analysis', 'integration']"
1278708,What vector field property means “is the curl of another vector field?”,"I know that a vector field $\mathbf{F}$ is called irrotational if $\nabla \times \mathbf{F} = \mathbf{0}$  and conservative if there exists a function $g$ such that $\nabla g = \mathbf{F}$.  Under suitable smoothness conditions on the component functions (so that Clairaut's theorem holds), conservative vector fields are irrotational, and under suitable topological conditions on the domain of $\mathbf{F}$, irrotational vector fields are conservative. Moving up one degree, $\mathbf{F}$ is called incompressible if $\nabla \cdot \mathbf{F} = 0$.  If there exists a vector field $\mathbf{G}$ such that $\mathbf{F} = \nabla \times \mathbf{G}$, then (again, under suitable smoothness conditions), $\mathbf{F}$ is incompressible.  And again, under suitable topological conditions (the second cohomology group of the domain must be trivial), if $\mathbf{F}$ is incompressible, there exists a vector field $\mathbf{G}$ such that $\nabla \times\mathbf{G} = \mathbf{F}$. It seems to me there ought to be a word to describe vector fields as shorthand for “is the curl of something” or “has a vector potential.”  But a google search didn't turn anything up, and my colleagues couldn't think of a word either.  Maybe I'm revealing the gap in my physics background.  Does anybody know of such a word?","['multivariable-calculus', 'definition']"
1278728,Some clever trick is required for this Integral with irrational power of cosine as integrand.,"See this:
$$\newcommand{\b}[1]{\left(#1\right)}\left\lfloor\frac{\displaystyle\int_0^{\pi/2}\cos^{\sqrt{13}-1}x{\rm d}x}{\displaystyle\int_0^{\pi/2}\cos^{\sqrt{13}+1}x{\rm d}x}\right\rfloor$$
Well I could only think of Cauchy-Schwarz, but it is also not fitting. All calculations by hand, no Beta And that implicitly implies not using others like Gamma too.",['integration']
1278742,How to Formulate this Linear Algebra Fact in a Coordinate Free way?,"There is a result result given in the last paragraph of pg 15 in Hoffman And Kunze's Linear algebra (2nd Edition) which essentially says that THEOREM. Let $F_1$ be a subfield of a field $F$.
  If the entries of an $m\times n$ matrix $M$ lie in $F_1$, and $\mathbf b\in F_1^m$, then the system of equations $M\mathbf x=\mathbf b$ has a solution $\mathbf x\in F_1^m$ if and only if it has a solution in $F^m$. In other words, the theorem says: THEOREM. Let $F_1$ be a subfield of a field $F$ and $V=F^m$ be a vector space over $F$.
  Let $\mathbf A, \mathbf A_1,\ldots, \mathbf A_n$ be vectors in $V$ each having all of it's entries in $F_1$.
  Let $x_1,\ldots,x_n\in F$ be such that $\sum_{i=1}^{n}x_i\mathbf A_i=\mathbf A$.
  Then there exist $y_1,\ldots,y_n\in F_1$ such that $\sum_{i=1}^{n}y_i\mathbf A_i=\mathbf A$. I am looking for a ""coordinate free"" formulation of the above theorem, meaning, I don't want to take $F_1^m$ as my vector space $V$, whose elements are $m$-tuples. I'd like have a finite dimensional vector space $V$ over $F_1$. Can somebody see how to do that? Thanks.","['abstract-algebra', 'linear-algebra']"
1278785,How to find expected angle between two randomly generated vectors?,Let us say two random points have been generated in a d-dimensional space by uniformly sampling from a unit cube centered at origin. How to calculate the expected angle between them?,"['probability-theory', 'geometry', 'probability']"
1278792,"Calculating the rank of a matrix , reduced row echelon or row echelon?","I am trying to calculate the rank of a matrix and everytime I search for the steps required to calculate the rank of a matrix, the answer always uses the terms row echelon form and reduced row echelon form interchangeably when calculating the rank which is really confusing. I have read this question row echelon vs reduced row echelon form in which the given answer says that we use row echelon form instead of reduced row echelon form (as it's a tedious process) to calculate the rank however one the answers from a paper my university gave me had this written on it: Alternatively the rank is obtained by counting number of non-all-zero
rows in reduced matrix form above.
As:
rank of A + nullity of A = dimension of A It seems to me now that it doesn't really matter which form I use? Is there a difference between using row echelon form to calculate the rank and reduced row echelon form to calculate the rank of a matrix? I'm really confused.",['matrices']
1278815,"Number of different integers between $1,000$ and $10,000$","How many integers are there between $1,000$ and $10,000$ divisible by $60$ and all with distinct digits? I know that there are $8,999$ integers in total, and $\lfloor\frac{8999}{60}\rfloor=149$. So there are $149$ integers between $1,000$ and $10,000$ divisible by $60$. For the first digit we have $8$ choices, for the second $9$ choices, for the third $8$ choices and for the last $7$ choices. So there are $4,032$ different integers between $1,000$ and $10,000$. How to combine these results to an answer on the above question?",['combinatorics']
1278824,Limit of the expression,"I want to find following limit 
$$\lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=1}^{\infty}|\ln n-\ln k|\left(1-\frac{1}{n}\right)^k=?$$
To use computer programs is also allowed. Thanks for your helps...","['sequences-and-series', 'probability', 'calculus', 'limits']"
1278836,Understanding $\sum^n_{k=0}\binom nk x^k y^{n - k}$ combinatorially,"In a class of $n$ students, each student is given the choice of solving either one of $x$ different algebra problems or one of $y$ different geometry problems. How many different outcomes are possible? I'll consider a special case where $n = 5.$ Consider $\{A, B, C, D, E\}$. One possible group of algebra problem solvers is $\{A, D, E\}$. Since each student solves one problem we count $3-$lists whose elements stand for an algebra problem. We are counting $3-$lists instead of $3-$sets because each problem is fixed to a student. If there are $x$ algebra problems, then there are $x^3$ algebra problems for every $3-$set of students. So, $x^3$ is the coefficient of $\binom53$. Then, the number of students who solve geometry problems is $2$. For example, $\{B, C\}$. For every $2-$set, there are $y^2$ geometry problems. Also, for every situation like $x^3 \cdot \binom 53$, there are  $y^2$ possibilities meaning  $y^2$ is the coefficient of  $x^3 \cdot \binom 53$. Please, see if that makes sense. Also, is the sum there to capture all the different values of $k$?",['discrete-mathematics']
1278845,"Show that $\omega\mapsto\int_a^bX_t(\omega)\;dt$ is measurable, for a real-valued and continuous stochastic process $X$","Let $(\Omega,\mathcal{A},\operatorname{P})$ be a probability space $X=(X_t)_{t\ge 0}$ be a real-valued and continuous stochastic process on $(\Omega,\mathcal{A},\operatorname{P})$ $0\le a<b$ I want to show, that $$\Omega\to\mathbb{R}\;,\;\;\;\omega\mapsto\int_a^bX_t(\omega)\;dt$$ is $\mathcal{A}$-measurable. Since $X$ is continuous, for all $\omega\in\Omega$ $$X(\;\cdot\;,\omega):[0,\infty)\to\mathbb{R}\;,\;\;\;t\mapsto X_t(\omega)$$ is $\mathcal{B}\left([0,\infty)\right)$-measurable (where $\mathcal{B}(E)$ is the Borel $\sigma$-algebra on a topological space $E$). Thus, $$\mu(A):=\int_AX(\;\cdot\;,\omega)\;d\lambda\;\;\;\text{for }A\in\mathcal{B}\left([0,\infty)\right)\tag{1}$$ is a measure (measure with density $X(\;\cdot\;,\omega)$ with respect to $\left.\lambda\right|_{[0,\infty)}$, where $\lambda$ is the Lebesgue measure on $\mathcal{B}(\mathbb{R})$). However, I don't know how I need to proceed. Maybe, it's wrong to consider $(1)$. What do we need to do?","['probability-theory', 'stochastic-calculus', 'stochastic-processes', 'measure-theory']"
1278871,"Determinant of matrices with entries $a_{i, j} = \operatorname{gcd}(i, j)$","Suppose we have $n \times n$ matrix $A$ with $a_{i,j}={\rm gcd}(i,j)$. What is the determinant of $A$?","['determinant', 'linear-algebra', 'elementary-number-theory', 'matrices']"
1278885,Combinatorial Proof - choose k out of n+k,"I want to prove the following identity combinatorial:
\begin{align}
{n + k \choose k} = \sum \limits_{i=0}^n {n -i +k -1 \choose k -1}.
\end{align}
We want to choose k out of $n+k$. I want to use the following identity: 
\begin{align}
{n \choose k} = {n \choose n-k}.
\end{align}
So we have to prove that:
\begin{align}
{n + k \choose n} = \sum \limits_{i=0}^n {n -i +k -1 \choose n -i}.
\end{align}
In words, choose n out of n+k. I still can not figure out why this is true. Can some body help me out?",['combinatorics']
1278897,2nd order h ODE with non-constant coefficient,"I have $$0 = F''(x) + p(x) F'(x) + cF(x)\\
p(x) = ab(1-x)$$ where $a$, $b$, $c$ are non-zero constants. I'm not very strong in the theories of 2nd order ODE, so I google'ed some solution methods. Turns out that there's a lot for constant coefficients, but many standard/introductory textbooks completely skip solution methods for the part with non-constant coefficients. I am aware of the principle of superposition, but in order to use that, I first need to find two independent solutions. How can I proceed here (or in general?)",['ordinary-differential-equations']
1278900,Gradient of squared distance to a convex set,"I have the following problem: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$, $f(x)=(\operatorname{dist}(x,D))^2$ where $D$ is a convex, close set in $\mathbb{R}^n$. Prove that $f$ is convex and $f'(x)=2(x-P_D(x))$. Here $P_D(x)$ is the projection of $x$ in $D$. I prove convexity, but I can't demonstrate that differential (gradient) is this. I think that the way to go is to use the definition, proving that $$\lim_{x\rightarrow x_0} \frac{|f(x)-f(x_0)-f'(x_0)^{\top}(x-x_0)|}{\|x-x_0\|}=0$$","['convex-optimization', 'multivariable-calculus']"
1278902,Approximate solution of differential equation,"My task: find approximate solution as $$y = y_0(x) + y_1(x)\lambda + y_2(x)\lambda^2 + y_3(x)\lambda^3$$
of differential equation 
$$y' = \sin x + \lambda e^y, y(0)=1-\lambda.   \ \ \ \   (*)$$
My attempt : Let $$y(x,\lambda) = y_0(x) + y_1(x)\lambda + y_2(x)\lambda^2 + y_3(x)\lambda^3.$$
 Then $$ y_0(x) = y(x,0)$$ $$y_1(x) =\frac{ \partial y(x,\lambda)}{\partial \lambda}_{\lambda = 0}$$ $$y_2(x) =\frac{1}{2}\frac{ \partial^2y(x,\lambda)}{\partial \lambda^2}_{\lambda = 0}$$
$$y_3(x) =\frac{1}{6}\frac{ \partial^3y(x,\lambda)}{\partial \lambda^3}_{\lambda = 0}$$ And $$ y_0'(x) = y'_x(x,0)$$ $$y_1'(x) =\frac{ \partial}{\partial \lambda}y'_x(x,\lambda)_{\lambda = 0}$$ $$y_2'(x) =\frac{1}{2}\frac{ \partial^2}{\partial \lambda^2}y'_x(x,\lambda)_{\lambda = 0}$$
$$y_3'(x) =\frac{1}{6}\frac{ \partial^3}{\partial \lambda^3}y'_x(x,\lambda)_{\lambda = 0}$$ After that, using (*), i got
$$y_0'(x)= sin x$$
$$y_1'(x)=e^{(y_0)}$$
$$y_2'(x)=e^{(y_0)}y_1;$$
$$y_3'(x)=e^{(y_0)}(y_1^2+y_2)$$
And i stopped here. There isn't a solution in these integrals. Maybe, there is another solution of my task? Thanks","['approximate-integration', 'calculus', 'ordinary-differential-equations', 'integration']"
1278921,"Let $(X, \mathfrak T)$ be a topological space and suppose that $A$ is a subset of $X$. Then 1. $Cl(A) = Cl(Int(A))$ 2. $Int(A) = Int(Cl(A))$","Let $(X, \mathfrak T)$ be a topological space and suppose that $A$ is a subset of $X$.  Then $Cl(A) = Cl(Int(A))$ $Int(A) = Int(Cl(A))$ I believe both of these statements are false and I think I have two counterexamples and I just want to double check that I am correct. Both these counterexamples are in the usual topology. Let $A= [0,1] \cup \{2\}$ then $Cl(A) = [0,1] \cup \{2\}$ and the $Int(A) = (0,1)$ therefore the $Cl(Int(A))= [0,1]$ which does not equal the Cl(A). Let $A= (0,1) \cup (1,2)$ then the $Cl(A) = [0,2]$  and the $Int(A)= (0,1) \cup (1,2)$ therefore the $Int(Cl(A))= (0,2$) which does not equal the interior of A. My definition of closure is: Let $(X,\mathfrak T)$ be a topological space and let $ A \subseteq X$ . The closure of $A$ is $Cl(A) = \bigcap \{U \subseteq X: U$ is a closed set and $A \subseteq U\}$ 
Based on this I know $A \subseteq Cl(A)$ Am I correct? After typing this I am starting to doubt my calculations for the closure of both sets.","['elementary-set-theory', 'solution-verification', 'general-topology']"
1278951,"Prove that the solution of $y'+y=\arctan(e^x), y(0)=2$ admits horizontal asymptote.","Let us consider the Cauchy problem: 
$$y'+y=\arctan(e^x),\ \ \ \ y(0)=2$$ 
Prove that the function $y(x)$ admits horizontal asymptote without solving the problem.","['calculus', 'ordinary-differential-equations', 'functions']"
1278979,Integer reciprocals in arithmetic progression,"Let $m_1,m_2,\ldots,m_k$ be $k$ positive integers such that their reciprocals are in AP. Show that $k<m_1+2$. Also find such a sequence. Whatever way I tried, whichever formula I used, I could not eliminate the $m_k$ term, which I reckon would be the ideal scenario to find the solution. I am exhausted of any further ideas, please help. Thank you.","['sequences-and-series', 'algebra-precalculus']"
1278986,"Let $y=x^2+ax+b$ cuts the coordinate axes at three distinct points. Show that the circle passing through these 3 points also passes through $(0,1)$.","Let $y=x^2+ax+b$ be a parabola that cuts the coordinate axes at three distinct points. Show that the circle passing through these three points also passes through $(0,1)$. Since, the graph of the function cuts the coordinate axes in 3 different points, the function has real and distinct solutions. The third point of intersection is at $(0,b)$. But, anything I do from here, looks like a dead end. Please help. Thank you.","['quadratics', 'circles', 'analytic-geometry', 'functions']"
1278989,How to calculate the probability of a point being inside a polygon [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Given that a point is in a polygon, I am assuming that this point is more likely to be on (or near) the Centroid of the polygon than it is likely to be on (or near) the edges of the polygon. Is that a correct assumption? If so, I need to prove that. And for doing so I think I need to know: How can I calculate the probability of this point being on the centroid of the polygon? How can I calculate the probability of this point being on one the edges of the polygon?","['probability-theory', 'geometry', 'probability']"
1278997,arccos and arcsin integral contradiction:,"I am shown: $$f(x) = \arcsin x \implies f'(x) = \frac{1}{\sqrt{1-x^2}}$$
$$f(x) = \arccos x \implies f'(x) = -\frac{1}{\sqrt{1-x^2}}$$ These two derivatives can be very readily derived by a bit of implicit differentiation. However... I want to undo my differentiation for both expressions. I set up the integrals and now note that the only difference between the two derivatives is a negative sign. Suppose that for the arccos(x) derivative, I factor out the negative sign before integrating the expression, as $-1$ is a constant and the constnat factor rule states that: $$\int k \frac{dy}{dx} dx = k \int \frac{dy}{dx} dx$$ However, due to this, I end up with the false equality: $$-\arccos(x)=\arcsin(x)$$ Which doesn't hold for any $x$ at all! What have I done wrong, why cant one simply factor out the negative sign for the arccos derivative expression?","['trigonometry', 'integration']"
1279016,General solution of continuous function-dependent ODE,"Given a continuous function $f:I\subseteq\mathbb R\to \mathbb R$ and an ordinary differential equation given by:
$$ y''-xf(x) y' + f(x) y = 0. $$
I'd like to solve this ODE. However, I don't know how to attack the problem and I'd be very pleased if you give me a hint. Edit I It has been relatively easy to me to figure out that $y_1(x) := x$ satisfies the ode for every function $f$. If $y_2 := u \cdot x$ for some function $u:I\to\mathbb R$, then
$$ 0 = u''x+2u'- x f(x) (u'x+u)+f(x) u x = x u'' + [2-x^2 f(x)]u'$$
and setting $v := u'$
$$ 0 = x v' + [2-x^2 f(x)] v \iff \frac{dv}{v} = [x^2 f(x)-2]\frac{dx}{x} $$
in its ""differential form"", so
$$ \ln|v(x)| = \int \left(x f(x) - \frac{2}{x}\right) dx = -2 \ln|x|+xF(x)-\int F(s) ds \implies ...$$
where $F:I\to\mathbb R$ is a primitive of $f$. Is this right? Am I missing something?",['ordinary-differential-equations']
1279026,Complement Probability- Choose A Ball,"An urn contains $n$ balls, one of which is special. If $k$ of these balls are withdrawn one at a time, with each selection being equally likely to be any of the balls that remain at the time, what is the probability that the special ball is chosen? Can we say the following? $\# \Omega={n\choose k}$ $A\equiv special\ ball\ was\ chosen $ $\# A^C={n-1 \choose k}$ $P(A)=1-\frac{{n-1 \choose k}}{{n\choose k}}$","['probability', 'combinatorics']"
1279090,Integrate $\int \frac{x^5 dx}{\sqrt{1+x^3}}$,"I took $1+x^3$ as $t^2$ . I also split $x^5$ as $x^2 .x^3$ . Then I subsituted the differentiated value in in $x^2$ . I put $x^3$ as $1- t^2$ . I am getting the last step as $2/9[\sqrt{1+x^3}x^3 ]$ but this is the wrong answer , i should get $2/9[\sqrt{1+x^3}(x^3 +2)]$. Please help me. Thanks",['integration']
1279092,Proving a corollary to the Jordan Curve Theorem,"A Jordan curve is a continuous closed curve in $\Bbb R^2$ which is simple, i.e. has no self-intersections. The Jordan curve theorem states that the complement of any Jordan curve has two connected components, an interior and an exterior. I would like to prove the following statement, which I think is a corollary to the Jordan Curve Theorem: Let X, Y, Z, and W be four consecutive points (in order) of a Jordan
curve C.  Then if C1 is a continuous curve from X to Z and C2 is a
continuous curve from Y to W, and C1 and C2 are either both entirely in the interior of C, or both entirely in the exterior of C, then C1 and C2 must intersect. This statement is intuitively obvious.  Suppose C1 and C2 are both on the exterior of C, for instance.  Then we can form a Jordan curve D consisting of C1 and the segment of C between X and Z.  And intuitively, it seems like any curve starting from Y which is entirely on the exterior of C must pass through the interior of D.  And it also seems intuitively clear that W cannot be on the interior D.  Therefore, C2 must intersect D, and since it can't intersect C, it must intersect C1. But how would I prove those two statements I said were intuitively obvious? Any help would be greatly appreciated. Thank You in Advance. P.S.  The reason I'm asking this is that this statement is used in this webpage's solution of the famous three utilities problem.","['plane-curves', 'connectedness', 'real-analysis', 'general-topology']"
1279116,Prove this polynomial falls within $\mathbb R[x]$,"[ The problem below is from Yao Musheng (姚慕生), Wu Quanshui (吴泉水), Advanced Algebra (高等代数学) Ed $2$, Fudan University Press, page $207$. ] Suppose $f(x)\in \mathbb C[x]$. If $\forall c\in \mathbb R$, $f(c)\in\mathbb R$, prove:
  $$f(x)\in \mathbb R[x].$$ My attempt is as follows: For convenience I first specify that and $\deg f(x)\ge 2$ (otherwise it is easy to prove). Suppose $f(x)=a_0+a_1x+\cdots+a_{n-1}x^{n-1}+a_nx^n\notin \mathbb R[x]$ , then at least one of its coefficients, say $a_i$ is not real number. Therefore let $a_i=A+Bi$ where $B\ne 0$, and let $\tilde{f}(x):=f(x)-a_ix^i$. If $\tilde{x}\ne 0$ and it satisfies $\tilde{f}(\tilde{x})=0$, then 
$$f(\tilde{x})=a_i\tilde{x}^i$$
If $\tilde{x}$ is real, then so is $\tilde{x}^i$. But $a_i$ is not real, thus $f(\tilde{x})=a_i\tilde{x}^i$ is not real. However, since $\tilde x\in\mathbb R$, $f(\tilde{x})$ must be real. Contradiction. Therefore $\tilde{x}$ cannot be real. Then I don't know how to proceed. I have tried disproving the possibility of $\tilde{x}=0$ case because that doesn't seem to lead to anything. But I also failed. Seems that I have got on the wrong track into a dead end. I am really struggling with this problem now. Could anybody drop a hint or help me out? Best regards.","['abstract-algebra', 'polynomials', 'linear-algebra', 'field-theory']"
1279132,Bound variance proxy of a subGaussian random variable by its variance,"If I know $X$ is a sub-Gaussian random variable, and I know it has finite variance $\sigma^2$. Can I assert that $\sigma^2$ is a valid variance proxy for $X$? Definition (sub-Gaussian Random Variable)
A random variable $X$ is called sub-Gaussian with variance proxy $\sigma^2$, if $E[X] = 0$ $E[\exp(sX)] \leq \exp(s^2\sigma^2/2), \quad \forall s\in\mathbb{R}$ Note the variance proxy is not unique. Any larger number than a valid variance proxy is still a valid variance proxy. I can easily show using the moment generating function that the variance proxy of a sub-Gaussian random variable is greater than or equal to its variance . But I'm not sure about the inverse direction.","['probability-theory', 'probability']"
1279153,Derivatives and Linear transformations,"Let G be a non-empty open connected set in $R^n$, $f$ be a differentiable function from $G$ into $R$, and $A$ be a linear transformation from $R^n$ to $R$. If $f$ '($a$)=$A$ for all $a$ in $G$, find $f$ and prove your answer. I thought of $f$ as being the same as the linear transformation, i.e. $f(x)$=$A(x)$. Is this true?","['linear-transformations', 'derivatives']"
1279165,"Integrals of the form ${\large\int}_0^\infty\operatorname{arccot}(x)\cdot\operatorname{arccot}(a\,x)\cdot\operatorname{arccot}(b\,x)\ dx$","I'm interested in integrals of the form
$$I(a,b)=\int_0^\infty\operatorname{arccot}(x)\cdot\operatorname{arccot}(a\,x)\cdot\operatorname{arccot}(b\,x)\ dx,\color{#808080}{\text{ for }a>0,\,b>0}\tag1$$
It's known$\require{action}\require{enclose}\texttip{{}^\dagger}{Gradshteyn & Ryzhik, Table of Integrals, Series, and Products, 7th edition, page 599, (4.511)}$ that
$$I(a,0)=\frac{\pi^2}4\left[\ln\left(1+\frac1a\right)+\frac{\ln(1+a)}a\right].\tag2$$ Maple and Mathematica are also able to evaluate
$$I(1,1)=\frac{3\pi^2}4\ln2-\frac{21}8\zeta(3).\tag3$$ Is it possible to find a general closed form for $I(a,1)$? Or, at least, for $I(2,1)$ or $I(3,1)$?","['calculus', 'definite-integrals', 'logarithms', 'trigonometry', 'integration']"
1279195,"If $ \tan(20^{\circ}) = p $, find $ \frac{\tan(160^{\circ}) - \tan(110^{\circ})}{1 + \tan(160^{\circ}) \tan(110^{\circ})} $.","I applied the $ \tan(A - B) $-formula to make it $\tan(50^{\circ}) $, then I split it to $ \tan(30^{\circ} + 20^{\circ}) $. My answer came out to be $ \dfrac{\sqrt[3]{p + 1}}{\sqrt[3]{- p}} $, but the actual answer is $ \dfrac{1 - p^{2}}{2 p} $.",['trigonometry']
1279202,Simple limit in multi variable,"For $x=(x_1,x_2,x_3)$, determine the limit $$\lim_{x\to 0} \frac{\sin|x|^2}{|x|^2+x_1x_2x_3}. $$
I want to use that $\lim_{x\to 0} \frac{sin|x|^2}{|x|^2} = 1$ but I can't see how to do that. Any hints?",['multivariable-calculus']
1279232,Determine Isomorphism type,"Determine isomorphism type of quotient group $$\mathbb{Z} \times \mathbb{Z} / \langle(1,1)\rangle $$ using Fundamental Theorem Finite Generated Abelian Groups after looking at the factor group, it seems everything in $$\mathbb{Z} \times \mathbb{Z}$$ is generated except $(0,0)$. All in all, I'm just having a very difficult time even starting this problem.","['abstract-algebra', 'group-theory', 'group-isomorphism', 'finitely-generated']"
1279244,Divisors corresponding to hypersurfaces in projective space,"I'm looking at hypersurfaces on $\mathbb{C}\mathbb{P}^2$. That is, the zero set of an irreducible homogeneous polynomial $f(x_0, x_1, x_2) = 0$. This corresponds to a divisor, $D_f$, let's say which is just the trivial sum of 1 times the hypersurface. Every divisor has an associated line bundle $\mathcal{O}_{\mathbb{P}^2}(D_f)$. Now, since we're working in projective space, we know this must be $\mathcal{O}_{\mathbb{P}^2}(k)$ for some $k \in \mathbb{Z}$. I am trying to find out what $k$ should be. I have gone over a few different arguments and think that no matter what hypersurface, your corresponding line bundle will be the trivial bundle, $\mathcal{O}_{\mathbb{P}^2}(0)$. And then this should work over any $\mathbb{P}^n$ then as well. So the question is, is this correct? Are there straightforward ways to see this? Is there any intuition as to why this would be true? It seems odd to me that I can take any hypersurface and get the same bundle (and the trivial one at that!). Thanks for the help.","['projective-space', 'algebraic-geometry', 'vector-bundles']"
1279246,How can I show that this function is smooth?,"I got an assignment which I just can't find the right way to solve and I hope that someone could help me out here. It goes like this: Let $\Omega\in R^n$ be a domain and $b_1,...,b_n:\Omega\to R^n$ smooth mappings (or function, don't know the correct translation into english), so that for every $x\in \Omega$ the vectors $b_1(x),...,b_n(x)$ are linearly independent. Let $c_1,...,c_n:\Omega\to R$ be mappings (or functions). Show that the function $F(x):=c_1b_1(x)+...+c_n(x)b_n(x)$ is smooth when $c_1,...,c_n$ are smooth.","['multivariable-calculus', 'derivatives']"
1279248,What is the difference between structural induction and ordinary induction?,"I know two basic differences: 1.In structural induction you can use both numeric and string datatype,while in ordinary only numeric is allowed. 2.In structural there is base case and constructor case,while in ordinary there is base case ,induction hypothesis and induction step.And in structural there can be many base cases. --> Kindly confirm if these are right and tell me more differences between structural & ordinary induction.","['induction', 'discrete-mathematics']"
1279267,Roots of unity are distincts,"For every $n\in\Bbb N$ and $$z_{k}:= \cos(2\pi k /n)+i\sin(2\pi k /n), \qquad k = 0,\ldots,n-1$$
we have $z_k^n=1$. How to show, in a simple way, that $z_k\neq z_l$ for every $k\neq l$? By simple I mean that I know roughly the following: I know a bit of trigonometry (high school level) and I just proved De Moivre's formula by induction. I just discovered complex numbers. I know what injectivity/surjectivity/bijectivity is but I never proved that $\sin:[-\pi/2,\pi/2]\to[-1,1]$ and $\cos:[0,\pi]\to[-1,1]$ are bijective. (maybe it's time for it). I don't know what is $e^{z}$ for $z\in\Bbb C$. I know what is a field and that $(\Bbb C,+,\cdot)$ is a field. I don't know what is $\Bbb R^n$ for $n>2$ (and know more or less what is $\Bbb R^2$). Moreover I don't know what is a vector space.","['roots-of-unity', 'complex-numbers', 'algebra-precalculus']"
1279288,Is there anything to prove in this corollary?,"Show that if  $B$ is not finite and $B\subset A$, then A is not finite. I mean the statement is very trivial, but I'm having an issue actually writing what I would deem a good proof of this. The only idea I have is of letting $x\in B$ and  then show it is in $A$, but even that is trivial because I'm given that in my assumption.",['elementary-set-theory']
1279298,"$\inf_{x\in[a,b]}f(x)=\inf_{x\in[a,b]\cap\mathbb{Q}}f(x)$ for a continuous function $f:[a,b]\to\mathbb{R}$","Let $f:[a,b]\to\mathbb{R}$ be continuous. I'm sure it's not hard, but I'm unsure what exactly we need to do to prove $$\inf_{x\in[a,b]}f(x)=\inf_{x\in[a,b]\cap\mathbb{Q}}f(x)$$","['analysis', 'continuity', 'real-analysis']"
1279312,Solve the following summation,"$S = \dfrac{n \choose 0}{1} + \dfrac{n \choose 1}{2} +  
 \dfrac{n \choose 2}{3}+\dotsb+\dfrac{n \choose n}{n+1}$","['summation', 'combinatorics']"
1279325,Decreasing Sequence of Measures,"For $(X,\mathcal{F})$ a measure space, I know that if we have $\mu_{n}(A) \searrow$, i.e. is a decreasing sequence of measures for each $A \in \mathcal{F}$ and $\mu_{1}(X) < \infty$ then $\mu = \lim_{n \rightarrow \infty} \mu_{n}$ is not a measure. The problem asks for a counterexample. I am struggling with coming up with a counterexample for this and would appreciate some help. This problem comes from an early chapter in the book before any discussion of Lebesgue measure, so it should be possible to come up with a counterexample using only the measures discussed at that point which are the counting measure or Dirac measure and linear combinations of measures. A hint would be appreciated. Edit: I am starting to think that this may actually be a measure if $\mu_{1}(X) < \infty$. Is this the case ?",['measure-theory']
1279330,Tricky Rectangle Problem [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How many rectangles are there which do not include any yellow squares?","['geometry', 'combinatorics']"
1279339,the elements of Cantor's discontinuum,"Let $(A_n)_{n \in \mathbb{N}}$ the sequence of subsets of $\mathbb{R}$, given by $A_0 := \bigcup_{k \in \mathbb{Z}}[2k, 2k + 1]$ und $A_n := \frac{1}{3}A_{n-1}$ for $n ≥ 1$. Also, we define $$ A := \bigcap_{n=0}^\infty A_n, \,C:= A \cap [0, 1]$$ We call $C$ Cantor's discontinuum. Given this definition, I now want to prove that $C$ consists of all the real numbers of the form $x = \sum_{k=1}^\infty \frac{a_k}{3^k}$, where $a_k = 0$ or $a_k = 2$. Thanks in advance! I'm beginning to understand what $C$ actually looks like, but I don't really know how to show this. My idea was to maybe use the three-adic representation of numbers?","['analysis', 'sequences-and-series', 'cantor-set', 'real-analysis']"
1279372,The number of elements in a set,"I have a small task, part of my homework, which tends to confuse me because of its simplicity. It makes me think that I am missing something. I have to find the number of elements in the set {w | w ∈ L(R) and |w| = 10}.
I have the regular expression R = (0 ∪ 1)∗0101∗. My intuitive answer would be that the cardinality of this set is 10, because |w| = 10. Is this correct and what am I missing if it is not? thanks","['elementary-set-theory', 'regular-expressions']"
1279376,What is the area leftover from an inscribed circle called,"What are the little triangle things called (displayed as red in the picture)?
If the ones on the corners and the ones on the sides are different, then I would like to know those names too.","['geometry', 'terminology']"
1279378,$\int \frac1{\sqrt{u}}du$ gives two different answers,"I have the following integral
    $$\int \frac{x}{\sqrt{4-x^2}}dx$$ So I do $u$ substitution 
    $$u  = -x^2 + 4$$
    $$du = -2x \, dx$$ and get the following
    $$-\frac12\int \frac1{\sqrt{u}}du$$ I then can get TWO answers 1) Using $\int\frac1x = \ln(x)$ integral[1/sqrt(u)]  ; the integral as it is to this point
   integral[1/w]        ; w = sqrt(u)
   ln(w)                ; evaluate integral
   ln(sqrt(u))          ; replace w
   ln(sqrt(4-x^2))      ; replace u 2) Use general power rule integral[u^(-1/2)]  ; Rewriting 1/sqrt(u) as u^(-1/2)
   u^(1/2) / (1/2)     ; evaluate using power rule
   (2/-2) * u^(1/2)    ; rewrite the (1/2) divisor as multiplying by 2
   -sqrt(u)            ; write u^(1/2) as sqrt(u)
   -sqrt(4-x^2)        ; replace u What's going on? Is it that I can't make the replacement of sqrt(u) with a w? Why not? isn't it just a place holder, so to speak.","['calculus', 'integration']"
1279395,Conditions for being harmonic in a region $R$,"Prove that the function $G=\ln|f(z)|$ is harmonic in a region $R$ if $f(z)$ is analytic in $R$ and also $f(z)\cdot f'(z)$ does not equal zero in $R$. My difficulty here is that the expression for the Laplacian of $G$ is very big and ugly, and I know that I have to apply the Cauchy-Riemann Equations somewhere , but it is not clear to me how and where.
Also the condition of the multiplication of the complex function with its derivative not being zero looks rather mysterious . Any help would be appreciated.",['complex-analysis']
1279398,Let $A$ be a finite simply ordered set.,"Show that $A$ has a largest element. [Hint: Proceed by induction on cardinality of $A$] Attempt: According to the assumption my set $A$ is finite and simply ordered so that would mean $A = \{A_1, A_2,.....A_n\}$ where $n$ is the cardinality of my set $A$ Skipping over the test case of induction, let me just go to assuming that the statement is true for $n = k$ , thus $A_k$ is the largest element Consider the set $B = \{A_1, A_2,.......A_k, A_{k+1}\}$ with the same assumptions of finitness and simply ordered. If we take $A\cup B = \{A_1,A_2,...A_k, A_{k+1}\}$ we once again have a finite set in which order matters, but now $A_{k+1}$ is the largest element, therefore the set $A$ had a largest element. I'm still poor at writing these proofs, so I'll ask is there anything correct about my write up?",['elementary-set-theory']
1279439,Does $1^{\frac{-i\ln 2}{2\pi}}$ equal 2?,"Just out of curiosity, I would like to know if this derivation is correct or not. Let's assume complex numbers and write $1 = e^{2\pi i n}$, for any $n\in\mathbb{Z}$. Then, by exponentiation we obtain $$1^{\frac{-i\ln 2}{2\pi}}=e^{2\pi i n \cdot \frac{-i \ln 2}{2\pi}} = 2^n,$$ and thus for $n=1$, $1=2$. For me, this looks like a big contradiction. Any power of $1$ should be equal to $1$, or? What is the catch here that I don't see? In complex numbers, the power of $1$ doesn't have to be equal to $1$? Thanks.","['complex-analysis', 'complex-numbers']"
1279508,What are the chances that 5 people are all born on the same day#?,"Assuming 30-day months, given 10 people in a room.  What are the chances that 5 or more people are all born on the same day#?  (i.e., 5 born on the 28th, or 5 born on the 6th, etc) (EDIT: changed from chances of 5 to chances of 5 or more) I have tried two answers so far. In the first, you pick any person, and see what the chances are of the other 9, then 8, etc to match the first.  This seems to be 10 * 9/30 * 8/30 * 7/30 * 6/30. In the second, I suppose you could calculate the chances of 5 of the 10 having a birthday on day 1 + the chances of 5 having a birthday on day 2, etc. These answers seem quite different.  What do you all think?",['probability']
1279518,Finding conditions to make roots of a quadratic less than one in magnitude,"I'm doing a problem that asks for you to find the conditions that make $y$ defined:
$$y=x^2-bx+c$$
have real roots with magnitude less than one. Now the condition for the roots being real seems to be: $$b^2\ge4c$$ The problem I have is finding the restrictions necessary for the second condition to be true, since intuitively it seems it should be that:
$$ \left|\frac{b\pm\sqrt{b^2-4c}}{2}\right|<1 \;\;\rightarrow\;\;\left|\,b\pm\sqrt{b^2-4c}\right|<2 $$
I'm not sure how to tackle the problem from here effectively (or indeed if this is the best way to tackle this type of question) since when I try to evaluate cases of the absolute value (using its definition) they seem to give contradictory results, for example I can't see where conditions that $|\,b\,|<2$ come from. From playing with Mathematica the answer it gives is:
$$ (-2 < b \leq 0\; \;\land\;\; -b - 1 < c \leq \frac{b^2}{4}) \;\;\lor \;\; (0 < b < 2 \;\;\land 
   \;\;b - 1 < c \leq \frac{b^2}{4}) $$
Which seems to make sense, at least trying values in those regions seem to work. I'm just wondering the best technique to tackle this kind of problem.","['quadratics', 'algebra-precalculus', 'inequality']"
1279528,Proof of Hamilton's equation from integral invariant,"This is from pages 273 - 274 0f Whittaker's book of analytical dynamics. Its in the public domain. Let $q_1,q_2,\ldots,q_N$ be functions of time. And let $p_1,p_2,\ldots,p_N$ also be functions of time. And they are determined by the following differential equation: $$\eqalign{
\dot{q_k} &= Q_k \cr
\dot{p_k} &= P_k
}$$ Where the $Q's$ and $P's$ are functions on the $q's$ and $p's$ but not on time. And suppose they satisfy the following: $$\eqalign{
\frac{\partial Q_i}{\partial q_k} &= -\frac{\partial P_k}{\partial p_i} \cr
\frac{\partial P_i}{\partial q_k} &= \frac{\partial P_k}{\partial q_i} \cr
\frac{\partial Q_i}{\partial p_k} &= \frac{\partial Q_k}{\partial p_i}
}$$ Then Whittaker claims that this implies the existance of a function $H$ for which $Q_k = \partial H /\partial p_k$ and $P_k=-\partial H/\partial q_k$. He states this without proof, but I don't see it as obvious. Can you explain? For more context, Whittaker is trying to prove Hamilton's equation from Poincare's integral invariant.","['multivariable-calculus', 'ordinary-differential-equations', 'classical-mechanics']"
1279551,Sketching a Cyclic Quadrilateral,"In cyclic quadrilateral $ABCD$ consider $DD_1 ⊥ DC$ with $D_1$ on line $AB$, $BB_1 ⊥ AB$ with $B_1$ on line $DC$.
Prove that $AC ∥ B_1D_1$. I'm having trouble drawing this cyclic quadrilateral. At first, I put $D_1$ at the intersection of $AB$ and $DA$ and $B_1$ at the intersection of $DC$ and $BC$. But such placements for $D_1$ and $B_1$ made $AC$ the exact same line as $B_1D_1$. So that didn't work. Now I guessing and testing to no avail.",['geometry']
1279568,Expectation of trigonometric functions involving random variables.,"This is more a formulation question. I need help making a sales pitch (lol). I am working on an practical engineering problem where I encounter functions of the form: $\cos(\phi + d_\phi)$, $  \tan(\phi+d_{\phi}) $ etc. where $\phi$ is a deterministic variable taking values to the open interval  $(-\pi/2, \pi/2)$ and $d_{\phi}$ is a zero mean random variable, possibly Gaussian, but not necessarily. Under what assumptions, or how should I phrase/justify the following: $E[\cos(\phi + d_{\phi})] \approx E[\cos(\phi)]$ $E[\tan(\phi + d_{\phi})] \approx E[\tan(\phi)]$ $E[\sec(\phi + d_{\phi})] \approx E[\sec(\phi)]$ where $E$ represents the expectation operator. I did plot the above using Matlab and qualitatively the above approximations look OK. I am looking for some form analytical justification.
Please feel free to make any assumptions in your response. For example, I started with the most obvious and conservative assumption on the noise magnitude: $|d_{\phi}|<c$ where $c$ is some small positive constant. Any other suggestion?
Much obliged.","['approximation', 'random-variables', 'expectation', 'trigonometry']"
1279571,"Logarithmic Differentiation equation, Help!","So, I have to differentiate this via $\log$. I am still learning, so please be patient, I will try to explain everything I did. Please tell me if it is correct. $$y=\frac{(x+3)^4(2x^2+5x)^3}{\sqrt{4x-3}}$$ $$\ln(y) = \ln\left((x+3)^4(2x^2+5x)^3)\right) - \ln\left(\sqrt{4x-3}\right)$$ $$\ln(y) = 4\ln(x+3) 3\ln(2x^2+5x) - \frac{1}{2} \ln(4x-3)$$ aaaaaaaaand don't know what to do next, any help in the process or next step?","['logarithms', 'implicit-differentiation', 'calculus', 'derivatives']"
1279592,Definite integral of $\cos (x)/ \sqrt{x}$?,"How do I integrate
$$ \int^{+\infty}_0 \frac{\cos(x)}{\sqrt{x}} dx$$
I tried setting $u = x^{-1/2}$ and $dv = \cos(x)dx$. Then I integrate by part twice to get:
$$ \int^\infty_0 \frac{\cos x}{\sqrt{x}} dx = \lim_{x \rightarrow \infty} \left[\sqrt{x} \sin(x) - \frac{1}{2} \sqrt{x} \cos(x)\right] - \frac{1}{4} \int^\infty_0 \frac{\cos x}{\sqrt{x}} dx $$
Hence:
$$ \frac{5}{4} \int^\infty_0 \frac{\cos x}{\sqrt{x}} dx = \lim_{x \rightarrow \infty} \left[\sqrt{x} \sin(x) - \frac{1}{2} \sqrt{x} \cos(x)\right] $$
I'm not sure how to evaluate the terms on the right. Also, if you think I made a mistake above, please help me point it out.","['definite-integrals', 'integration']"
1279595,Continuity vs. Mapping open sets to open sets?,"I have a question and I have no idea how to solve this:
One problem in my Real Analysis text book says: Show that if $\ell$ is a nonzero linear functional on a normed vector space not necessarily continuous, then $\ell$ maps open sets into open sets. Isn't it against the definition? I mean saying that $f$ is continuous is equivalent to say that it maps open sets into open sets, right? Is there any mistake in this problem? Or is just me that I'm misreading something? Thank you for your help!!","['measure-theory', 'continuity', 'real-analysis', 'general-topology', 'uniform-continuity']"
1279610,A seemingly easy combinatorics brain teaser,"So I have a brain teaser that goes like this: There's a school that awards students that, during a given period, are
  never late more than once and who don't ever happen to be absent for
  three or more consecutive days. How many possible permutations with repetitions of
  presence (or lack thereof) can we build for a given timeframe that
  grant the student an award? Assume that each day is just a state
  On-time, Late or Absent for the whole day, don't worry about specific
  classes. Example: for three day timeframes, we can create 19 such
  permutations with repetitions that grant an award. So I thought about it like that: first, we narrow down our possible space of solutions. We know that there are $2^n$ possible permutations with repetitions with no being late at all (cause then each of the $n$ days is just On-time or Absent) and $n\cdot2^{n-1}$ permutations with repetitions with exactly one Late state. And here comes the hard part - we now have to subtract all the permutations with repetitions which hold 3 consecutive Absent days, 4 consecutive Absent days and so on. At first, I thought about something like that: there are $n - (k-1)$ ways to fit the $k$-day consecutive absency in a $n$-day timeframe with no Late states. Then, I fiddled with finding a formula for $k$-day consecutive absency for timeframes with exactly one Late state and I even started to find some kind-of-working-ish formulas like $2\cdot 3^{(n-k-1)}$ for 3-day consecutive absencies in timeframes with exactly one Late state but it shortly followed that (a) my reasoning was flawed, (b) I couldn't come up with anything generalised for $k$-day absencies and (c) I realized that both for this and for my $n - (k-1)$, I have to also keep in mind all the possible combinations the remaining days may take (for example, all can be On-time but also every second can be Absent, there can be more than one 3-day absency and so on) and all of these things we have to subtract. I guess I went way too deep with it and surely there has to be some less twisted solution. How should I go about it?","['combinations', 'recreational-mathematics', 'combinatorics']"
1279613,How can $\frac{\mathrm{d}}{\mathrm{d}x}\left[y(u(x))\right] = \frac{\mathrm{d}y}{\mathrm{d}x}$?,"I just saw a video on the chain rule, and it stated that $$\frac{\mathrm{d}}{\mathrm{d}x}\left[y(u(x))\right] = \frac{\mathrm{d}y}{\mathrm{d}x}$$ I don't understand this; if I let $y(x) = x^2$ and $u(x) = \sqrt x$ then $$\frac{\mathrm{d}y}{\mathrm{d}x} = 2x$$ and $$\frac{\mathrm{d}}{\mathrm{d}x}\left[y(u(x))\right] = \frac{\mathrm{d}}{\mathrm{d}x} \left[x\right] = 1$$ Clearly, I am completely misunderstanding something. What is it? EDIT: It is my understanding right now that $y(u(x)) = (\sqrt x)^2 = x$. Is this wrong?",['derivatives']
1279621,Multinomial Theorem for Negative Exponents,"Using an analog to Newton's binomial theorem with negative exponents, is it true that $$
\begin{align}
\left(\sum_{k=0}^mx^k\right)^{-n} & = \sum_{0\le i_0+...+i_m=n\lt\infty}\binom{-n}{i_0,i_1,...,i_m}1^{i_0}x^{i_1}...x^{mi_m} \\
& =\sum_{0\le i_0+...+i_m=n\lt\infty}\binom{-n}{i_1,...,n-i_0-...-i_{m-1}}x^{i_1}...x^{m(n-i_1-...-i_{m-1})} \\
& = \sum_{0\le i_0+...+i_m=n\lt\infty}\frac{-n(-n-1)...(-n-i_0-...-i_m+1)}{i_0!i_1!...i_{m-1}!}x^\alpha \\
& = \sum_{0\le i_0+...+i_m=n\lt\infty}(-1)^{i_0+...+i_{m-1}}\frac{n(n+1)...(n+i_0+...+i_m-1)}{i_0!i_1!...i_{m-1}!}x^\alpha \\
& = \sum_{0\le i_0+...+i_m=n\lt\infty}(-1)^\beta\frac{(n+i_0+...+i_m-1)!}{i_0!i_1!...i_{m-1}!(n-1)!}x^\alpha \\
& = \sum_{0\le i_0+...+i_m=n\lt\infty}(-1)^\beta\binom{n+i_0+...+i_m-1}{i_0,...,i_{m-1},n-1}x^\alpha \\
\end{align}
$$
where $\beta=i_0+...+i_{m}$ and $\alpha=mn-mi_0-(m-1)i_1-...-i_{m-1}$?  I would like to keep the summation to ""1"" Sigma and a single coefficient (multinomial) instead of the product of multiple binomials, so i thought this might be a way to accomplish that... EDIT:  I know that we can use the Binomial theorem in order to get an expression, but I'm looking to see if i can write something ""simpler"" and with less notational devices.","['binomial-theorem', 'sequences-and-series', 'multinomial-coefficients', 'combinatorics']"
1279634,"Let $(X, \mathfrak T)$ be a topological space and let $A$ but a subset of $X$ then $Int(Bd(A)) = \emptyset$","Let $(X, \mathfrak T)$ be a topological space and let $A$ but a subset of $X$ then $Int(Bd(A)) = \emptyset$ I need to decide if this is true or not.  I have done a little research and some contemplating and I believe this statement is true for open sets but I am not sure if it is true for closed sets or if I am overthinking and do not need to consider both sets. I am wondering if I am overthinking and this is true and a proof by contradiction would work? My definition of interior is: Let $(X, \mathfrak T)$ be a topological space and let $A \subset X$ is the set of all points $x \in X$ for which there exists an open set $U$ such that $x \in U \subseteq A$. My definition of boundary is: Let $(X,\mathfrak T)$ be a topological space and let $A \subseteq X$. A point $x \in X$ is in the boundary of A if every open set containing $x$ intersects both $A$ and $X−A$.","['elementary-set-theory', 'general-topology']"
1279653,Strong convexity and strong smoothness duality,"A function $f$ is said to be strongly convex with respect to a norm $\|\cdot\|$ at a point $y$ if $f(x) \geq f(y) + \nabla f(y)^T(x-y) + \frac{1}{2}\|x-y\|^2.$ It is said to be strongly smooth with respect to a norm $\|\cdot\|$ at a point $x$ if $f(y) \leq f(x) + \nabla f(x)^T (y-x) + \frac{1}{2} \|y-x\|^2$. The Fenchel dual of a convex function $f: \mathbb{R}^n \to \mathbb{R} $ is defined as: $f^*(x) = \max_y x^T y - f(y)$. Now, it's a general fact that if $f$ is strongly convex with respect to some norm $\|\cdot\|$ everywhere, then $f^*$ is strongly smooth with respect to the dual norm $\|\cdot\|^*$ everywhere. However, I was wondering if it is also true pointwise. Specifically, if $f$ is strongly convex at its minimum $x_0$, can one say that $f^*$ is strongly smooth at 0?","['analysis', 'convex-analysis']"
1279677,Standard Indicator function?,"Is there any standard function that outputs $1$ if an inequality is true and $0$ otherwise? e.g $$F =
\begin{cases}
1 & \text{if }x>D\\
0 & \text{otherwise}
\end{cases}$$","['inequality', 'functions']"
1279707,"If $\sum (a_n)^2$ converges and $\sum (b_n)^2$ converges, does $\sum (a_n)(b_n)$ converge?","If $\sum (a_n)^2$ converges and $\sum (b_n)^2$ converges, does $\sum (a_n)(b_n)$ converge? Could someone help me to solve this or at least give me a hint?, I have tried using Cauchy's criterion, the Dirichlet test for convergence, etc, but I can´t prove it.Honestly I don´t know where to start. Any help will be appreciated.","['sequences-and-series', 'hilbert-spaces', 'real-analysis']"
1279737,Exercise 2.3 Lee's Riemmanian Manifolds,"Statement: Suppose $M\subseteq \tilde{M}$ is an embedded submanifold. a)If $f$ is any smooth function on $M$, show that $f$ can be extended to a smooth function on $\tilde{M}$ whose restriction to $M$ is $f$. Question: So I'm wondering if there is some mistake in the statement, since there seems to be a simple counterexample, namely $f:(0,1)\rightarrow\mathbb{R}$, where $f(x)=\frac{1}{x}$. If it is a simple oversight, how should the question be reworded? I have been able to prove it assuming $f$ is a smooth function on some closed sub manifold, but I am unsure if that was the intended question.","['differential-geometry', 'smooth-manifolds']"
1279754,Lebesgue integrable function $g$ equals characteristic function,"I am trying to solve this problem: Let $g:[0,1] \to \mathbb R$ be a non negative integrable function over $[0,1]$. Prove that if there is $\alpha \in \mathbb R$ such that for all $n \in \mathbb N$, $$\int_0^1 g(x)^ndx=\alpha,$$
then $g=\mathcal X_E$ a.e. for some measurable subset $E \subset [0,1]$. I am pretty lost with this problem. First of all note that since $g$ is non negative, then $\alpha \in \mathbb R_{\geq 0}$. I would like to show that $g(x)=1$ for all $x$ in some measurable set $E$ (or at least almost for all $x$ in $E$ and that $g(x)=0$ almost everywhere else. We have the equality $$\lim_{n \to \infty} \int_0^1 g(x)^ndx=\alpha$$ I thought of writing $[0,1]=\{x:g(x) \in [0,1)\} \cup \{g(x) \geq 1\}$. If I call $S=\{x:g(x) \in [0,1)\}$, then $$\int_0^1 g(x)^ndx=\int_S g(x)^ndx+\int_{S^c} g(x)^ndx$$ Since $g(x)^n \leq g(x)$ for all $x \in S$, then here I can apply the dominated convergence theorem, so $\lim_n \int_S g(x)^ndx=0$. I got stuck here, I don't know what else to do, I would appreciate some help. Thanks in advance.","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1279763,Show that lower semicontinuous function is the supremum of an increasing sequence of continuous functions,"Show that lower semicontinuous function $f:X\rightarrow [0,1]$ on metrizable X is the supremum of an increasing sequence of continuous functions. My attempt: I don't know how to approximate $f(x)$ to within $[f(x)-1/n,f(x)]$ by $h_n(x)$ which is a linear combination of characteristic function of open sets, using lower semicontinuity of $f$. Can anyone help?","['analysis', 'semicontinuous-functions', 'general-topology']"
1279788,Finding angles of hyperbolic triangles,"I am trying to learn about how to find the angles of hyperbolic triangles. Now below is a problem: It has all the steps but I am not understanding the concept (the ones that are underlined in green and yellow) If you notice, $\measuredangle BAC=\frac{\pi}{2}-\measuredangle OAP=\measuredangle OPA$ but I do not know why. On the other hand, I understand the computation parts but it is meaningless if I dont get the concept. Anyone can help out?","['geometry', 'hyperbolic-geometry', 'trigonometry']"
1279824,Show that collection of finite dimensional cylinder sets is an algebra but not $\sigma$-algebra,"I am trying to prove that collection of all finite dimensional cylinder sets is an algebra but not $\sigma$-algebra. Cylinder sets are defined as: $\mathcal{B}_n$ is defined as the smallest $\sigma-$ algebra containing the rectangles $\{(x_1,x_2,\ldots,x_n): x_1 \in I_1,\ldots,x_n \in I_n \}$ where $I_1,I_2,\ldots,I_n$ are intervals in $\mathbb{R}$. An $n$- dimensional  cylinder set in $\mathbb{R}^{\infty} := \mathbb{R}\times \mathbb{R}\times...$ is defined as the set $\{\textbf{x}: (x_{1},x_2,\ldots,x_n) \in B\},\ B \in \mathcal{B}_n$. I am unable to visualize how to prove that finite unions belong in the collection, and also why some infinite unions won't belong to the collection. Can anyone help me with this ?","['probability-theory', 'probability', 'measure-theory']"
1279829,Why isn't $\mathbb{Z}_2 \times \mathbb{Z}_{30}$ isomorphic to $\mathbb{Z}_{60}$?,I know that other groups like $\mathbb{Z}_4 \times \mathbb{Z}_3 \times \mathbb{Z}_5$ are isomorphic to $\mathbb{Z}_{60}$.,['group-theory']
1279861,"Why, intuitively, is the order reversed when taking the transpose of the product?","It is well known that for invertible matrices $A,B$ of the same size we have $$(AB)^{-1}=B^{-1}A^{-1} $$
and a nice way for me to remember this is the following sentence: The opposite of putting on socks and shoes is taking the shoes off, followed by taking the socks off. Now, a similar law holds for the transpose, namely: $$(AB)^T=B^TA^T $$ for matrices $A,B$ such that the product $AB$ is defined. My question is: is there any intuitive reason as to why the order of the factors is reversed in this case? [Note that I'm aware of several proofs of this equality, and a proof is not what I'm after] Thank you!","['transpose', 'soft-question', 'matrices', 'linear-algebra', 'intuition']"
1279862,"Prob 10 Sec 26 in Munkres' TOPOLOGY, 2nd ed: How to give examples of this result failing?","Let $X$ be a compact topological space. Let $f_n \colon X \to \mathbb{R}$ be a sequence of continuous functions such that $f_n(x) \leq f_{n+1}(x)$ for all $x \in X$ and for all $n \in \mathbb{N}$ . Let $f \colon X \to \mathbb{R}$ be a continuous function such that, for each $x \in X$ , the sequence $f_n(x) \to f(x)$ as $n \to \infty$ . Then the sequence $f_n$ converges uniformly to $f$ on $X$ . This is problem 10 (a), Sec. 26, in the book Topplogy by James R. Munkres, 2nd edition. I've managed to give a proof of this result. However, what if we require that $f_n(x) \geq f_{n+1}(x)$ for all $x \in X$ and for all $n \in \mathbb{N}$ ? My assertion is the convergence is still uniform. Here's my proposed proof: Let $\epsilon > 0$ be given. Let $x \in X$ . Since $f_n(x) \to f(x)$ as $n \to \infty$ , there exists $N(x) \in \mathbb{N}$ such that $\vert f_n(x) - f(x) \vert < \epsilon$ if $n \geq N(x)$ . Since $f_n(x)$ is a monotically decreasing sequence, we must have $f_n(x) \geq f(x)$ for all $n$ . So $$\vert f_n(x) - f(x) \vert = f_n(x) - f(x) < \epsilon \ \ \ \mbox{ if } \ n \geq N(x). $$ In particular, $$f_{N(x)} (x) - f(x) < \epsilon.$$ Since $f_{N(x)} - f \colon X \to \mathbb{R}$ is continuous, the set $$U_x \colon= \left( f_{N(x)} - f \right)^{-1} \left( \ (-\infty, \epsilon) \ \right)$$ is open in $X$ and the point $x$ belongs to $U_x$ . In this way, we have an open covering $\{ U_x \colon x \in X \}$ of $X$ . Since $X$ is compact, there is a finite subcollection $U_{x_1}, \ldots, U_{x_k}$ that also covers $x$ . Let $N \colon= \max \left( N(x_1), \ldots, N(x_k) \right)$ . Let $y \in X$ . Then $y \in U_{x_i}$ for some $i = 1, \ldots, k$ . So, $$f_{N(x_i)} (y) - f(y) < \epsilon. $$ So, if $n \geq N$ , then we have $$
\begin{align*}
\vert f_n(y) - f(y) \vert &= f_n(y) - f(y) \\
&\leq f_N(y) - f(y) \\
&\leq f_{N(x_i)} (y) - f(y) \\ 
&< \epsilon. 
\end{align*}
$$ Thus, for every given $\epsilon > 0$ , there exists a natural number $N$ such that, if $n \geq N$ , then we have $$\vert f_n(y) - f(y) \vert < \epsilon$$ for all $y \in X$ . Hence $f_n$ converges uniformly to $f$ on $X$ . Is the above proof correct? But the result breaks down in the following example. Let $X \colon= [0,1]$ , and, for each $n \in \mathbb{N}$ ,  let $f_n \colon X \to \mathbb{R}$ be defined by $$f_n(x) \colon= x^n \ \ \ \mbox{ for all } \ x \in X.$$ Now, for all $n \in \mathbb{N}$ , $$f_n(0) = 0 = f_{n+1}(0),$$ and $$f_n(1) = 1 = f_{n+1}(1);$$ for $x \in (0,1)$ , we have $$f_n(x) = x^n > x^{n+1} = f_{n+1}(x).$$ The sequence $f_n$ converges pointwise to the function $f \colon X \to \mathbb{R}$ defined by $$
f(x) \colon= 
\begin{align*}
0 & \ \mbox{ if } \ 0 \leq x < 1; \\
1 & \ \mbox{ if } \ x = 1. 
\end{align*}
$$ but this convergence is not uniform. But $f$ is not continuous. So the hypotheses of our result are not met fully. (i) What if $X$ is not compact? (ii) What if the neither of the monotonicity conditions is satisfied? Please give an elementary enough example in each case where the result breaks down.","['general-topology', 'compactness']"
1279873,Basic probability: Romeo and Juliette meet for a date.,"Romeo and Juliet have a date at a given time, and each will arrive at the meeting place with a delay between 0 and 1 hour, with all pairs of delays being equally likely. The first to arrive will wait for 15 minutes and will leave if the other has not yet arrived. What is the probability that they will meet? My text has the answer as 7/16, and I just don't get it.  I'm just reading the book for self study - no one to ask! My logic: One of them has to arrive first, or they both arrive at the same time.  The probability they arrive at the exact same time is zero.  Suppose Romeo arrives first. If Romeo is the first to arrive, and he arrives after 45min, they are guaranteed to meet. A = P(Romeo arrives within the first 45min) = 45/60 = 3/4. B = P(Juliette arrives within some 15min interval) = 15/60 = 1/4. P(A and B) = 3/4 * 1/4 = 3/16. Help?",['probability']
1279874,Proof of an equation involving Stirling numbers of the second kind,"I found this equation involving Stirling numbers of the second kind on Math World: $$\sum\limits_{m=1}^n (-1)^m(m-1)!\,S(n,m)=0$$ for every integer $n \geq 2$ .
Here, $S(n, m)$ denotes the appropriate Stirling number of the second kind (i.e., the number of set partitions of $\left\{1,2,\ldots,n\right\}$ into $k$ parts). However, I do not know why this is true. I am looking for a proof or an explanation of this equation.","['stirling-numbers', 'combinatorics']"
1279916,"Show $x^n \geq x_1^n+x_2^n+\ldots+x_k^n \Bigg\vert x_1+x_2+\ldots+x_k = x, x \geq 0, n \geq 1, k\in\mathbb{Z}$","Hello StackExchange Community, This is my first post on the forum. Please forgive me for any errors with formatting and my expressions. I am working on the following proof: Show $$x^n \geq x_1^n+x_2^n+\ldots+x_k^n \Bigg\vert x_1+x_2+\ldots+x_k = x, x \geq 0, n \geq 1, k\in\mathbb{Z}$$ This is to say, I'd like to show that a positive number raised to $n\geq1$ is greater than the sum of any set of addends each raised to $n$. What I have scribbled so far: Let's look at $x^n \geq (\frac{x}{2})^n+(\frac{x}{2})^n$ $x^n \geq2 \frac{x^n}{2^n}$ $x^n \geq 0$ $1 \geq \frac{2}{2^n}$ $2^n \geq 2$ $n \geq 1$, thus true. It is obvious that $x^n \geq k(\frac{x^n}{k^n})$ is also true (for $k$
  $x_k$ that are all equal). Can I make $x_1^n+x_2^n+\ldots+x_k^n$ look like $k(\frac{x}{k})^n$?
  Well if $x_1^n+x_2^n+\ldots+x_k^n < k(\frac{x}{k})^n$, then it is also
  less than $x^n$ (proving original problem). $k \geq 0$ $\frac{k^n}{k}(x_1^n+x_2^n+\ldots+x_k^n) < x^n$ I feel I need to use the information $x_1+x_2+\ldots+x_k = x$ $\frac{k^n}{k}(x_1^n+x_2^n+\ldots+x_k^n) < (x_1+x_2+\ldots+x_k)^n$ I am not sure how to revise my thoughts and continue. I greatly appreciate any comments and questions; thank you in advance for posting! :) With regards, Polite Master","['exponentiation', 'algebra-precalculus', 'proof-writing']"
1279942,Prove series convergence,"The problem states: a) If $\sum_1^\infty {a_n}$ converges and ${b_n}=n^\frac{1}{n}{a_n}$, then $ \sum_1^\infty{b_n}$ converges, and b) If $\sum_1^\infty {a_n}$ converges and ${b_n}=\frac{{a_n}}{(1+|{a_n}|)}$, then $\sum_1^\infty {b_n}$ converges. For a) We have that ${c_n}=n^\frac{1}{n}$ is a stictly decreasing sequence (for $n>e$) and that it's bounded from above by $e^\frac{1}{e}$, so $\sum_1^\infty {a_n}{c_n}=\sum_1^\infty {b_n}$ converges( Abel's test). For b) We have that ${c_n}=\frac{1}{(1+|{a_n}|)}$ is a monotone decreasing sequence, and it's bounded from above by 1, so $\sum_1^\infty {a_n}{c_n}=\sum_1^\infty {b_n}$ converges. That's it, I would like to know if my proof is right, any help will be appreciated.","['sequences-and-series', 'real-analysis']"
1280001,Convergence of an alternating harmonic series,"Consider the series $$\sum_{n=1}^\infty c_n \cdot \tfrac 1n$$ where $c_n$ is either $-1$ or $1$. In my case I have $$c_n = \begin{cases} 1&; \lceil np \rceil - \lceil (n-1)p \rceil = 1 \\ -1 &; \lceil np \rceil - \lceil (n-1)p \rceil = 0 \end{cases}$$ with $p\in(0,1)$. My question: For which $p$ does the series converges? I know that the series converges for $p=\tfrac 12$ because of the alternating series test . What about the cases $p \neq \tfrac 12$?","['sequences-and-series', 'calculus', 'real-analysis', 'convergence-divergence']"
1280003,Poincaré lemma and conservative vector fields,"Let $U$ be some contractible neighbourhood of $0\in\mathbb{R}^n$ and let $X=\sum_{i=1}^nX_i\frac{\partial}{\partial x_i}$ be a (smooth) vector field on $U$. This vector field can be thought as a differential 1-form $X=\sum_{i=1}^nX_i\,dx_i$ on $U$. By Poincaré's lemma, ""every closed form is exact"". Does this mean that ""every vector field on $U$ is conservative (due to exactness) and satisfies $\frac{\partial X_i}{\partial x_j}=\frac{\partial X_j}{\partial x_i}$ (due to closeness)""? Then, should I interpret this as ""every v.f. on $U$ is the gradient of a potential function (0-form) $V:U\to\mathbb{R}$"" It seems to me that the word every is too strong. So, is my interpretation incomplete? What about non-conservative vector fields? Are they ""always"" defined in non-contractible domains?","['differential-geometry', 'dynamical-systems', 'ordinary-differential-equations']"
1280050,How do we find eigenvalues from given eigenvectors of a given matrix?,"For instance let 
$$A=\begin{pmatrix}
        3 & -1 & -1 \\
        2 & 1 &-2 \\
        0 & -1 & 2 \\
        \end{pmatrix}$$
be a matrix  and 
$$u_1=\begin{pmatrix}
        1 \\
        1  \\
        1 \\
        \end{pmatrix},$$
$$u_2=\begin{pmatrix}
        1 \\
        0 \\
        1\\
        \end{pmatrix},$$
$$u_3=\begin{pmatrix}
        0 \\
        -1 \\
        1 \\
        \end{pmatrix}.$$
its eigenvectors. What are its eigenvalues? Is there anything more simple than doing $A-λI$?","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1280053,Explicit description of $\Bbb Q_p \cap \bar{\Bbb Q}$,"Note that we can embed $\Bbb Q_p$ into $\Bbb C$, as it is discussed here . But as far as I understand, this embedding sends the power
series to transcendental elements, so we can't certainly embed $\Bbb Q_p$ into $\bar{\Bbb Q}$ (I love typing barBbbQ). And when we complete $\Bbb Q$ with the $p$-adic norm, then we get, for example, $(p-1)$-th roots of unity (to see this, just note that $x^{p-1} - 1$ factors in $\Bbb Z / p$, so by Hensel's lemma it factors in $\Bbb Q_p$). So the question is: do we get anything else? What?","['p-adic-number-theory', 'number-theory', 'algebraic-number-theory']"
1280054,Let A be an $n\times m$ matrix and B be an $m\times n$ matrix such that AB is invertible. Then which of the following is/are always true?,"Let A be an $n\times m$ matrix and B be an $m\times n$ matrix. For a square matrix D, let Tr(D) denote trace of D, |D| denote the determinant of D. Suppose that AB is invertible. Then which of the following is/are always true? (a) $Tr(AB) = Tr(BA)$. (b) $|AB| = |BA|$. (c) $m\geq n$. (d) BA must be invertible. For a start, I took several simple matrices, e.g. [1 0], [1 2]. The answer choices I could remove are (b) and (d). From what I knew, Tr(AB) = Tr(BA) is true for matrices of same order. So, is this true for a general case $n\times m$ and $m\times n$ as well ? Moreover, I did not expect $m\geq n$ to be true, which seems true using the above example matrices. How do I solve this properly ?","['linear-algebra', 'matrices']"
1280108,Show that MLE of $\lambda = \frac{n-T_n}{S_n+cT_n}$,"$X_i$ are i.i.d exponential, mean $\lambda^{-1}$ for $1 \leq i \leq n$ and, the values are measured such that $X_i = c$ if $X_i \geq c$ and $X_i$ otherwise. Show that MLE of $\lambda = \frac{n-T_n}{S_n+cT_n}$ where $S_n= \sum_{j=1}^n X_jI(X_j < c)$ and $T_n = \sum_{j=1}^n I(X_j \geq c)$ Attempt: The likelihood function is given by $L(\lambda | x_i, x_2, \ldots x_n) = \lambda^n e^{-\lambda \sum x_i} = \lambda^n e^{-\lambda(S_n+cT_n)}$ $\implies$ $log L = n log \lambda -\lambda(S_n+cT_n) $ and hence $\hat\lambda=\frac{n}{S_n+cT_n}$ which is different from the expected answer.","['expectation', 'estimation', 'statistics', 'probability', 'parameter-estimation']"
1280125,"A question about the proof that for prime p, p divides k(p), where k() is the Perrin sequence","Define the Perrin sequence by $k(1)=0$, $k(2)=2$, $k(3)=3$, and $k(n)=k(n-2)+k(n-3)$.  We find that mostly $n$ divides $k(n)$ iff $n$ is prime, although there are a few exceptions called ""Perrin Pseudo-primes."" But the obvious conjecture only fails for $n$ composite, for we find that whenever $p$ is prime, $p | k(p)$.  However, the usual proof goes as follows. Let $\alpha$, $\beta$, and $\gamma$ be the roots of the characteristic polynomial $x^3-x-1$.  Then a simple induction argument shows that $k(n)=\alpha^n+\beta^n+\gamma^n.$ Now consider $(\alpha+\beta+\gamma)^p = \alpha^p+\beta^p+\gamma^p+p\sum(things)$.  Since we have $p$ times something, when we consider this module $p$ we get: $$(\alpha+\beta+\gamma)^p = \alpha^p+\beta^p+\gamma^p$$ and we're done, because $\alpha+\beta+\gamma = 0$.  And that would be fine if the things in the summation were integers. But they're not. So my question is: why is that summation always considered to be an integer, when the things in the summation are the roots of the cubic, and not integral? All the papers I've read seem to take this for granted, so I'm sure I must be missing something simple. Thank you.","['prime-numbers', 'number-theory']"
1280156,Under what conditions does $(I-N)^{-1}$ exist?,"Given an nxn matrix N and $I=I_n$, under what conditions does $(I-N)^{-1}$ exist? On one hand $(I-N)(I + N + N^2 + ...) = (I + N + N^2 + ...) - (N + N^2 + ...) = I?$ On the other hand, $(I-N)(I + N + N^2 + ...) = \lim_{m \to \infty} (I-N) \sum_{i=0}^{m} N^i = \lim_{m \to \infty} \sum_{i=0}^{m} (N^i - N^{i+1}) = \lim_{m \to \infty} (I - N^{m+1}) = I - \lim_{m \to \infty} (N^{m+1}) = I?$ I think $I - \lim_{m \to \infty} (N^m) = I$ if (and only if?) the entries of N are between -1 and 1? Iirc, one of my professors stated it without specifying why it is true in that case if it was true in that case but not true in other cases or why it is true in all cases. This arose in the context of Markov chains which iirc deal with matrices whose entries are between -1 and 1. So, does $(I-N)^{-1}$ always exist? If so, why? If not, under what conditions?","['matrices', 'markov-process', 'markov-chains', 'operations-research', 'linear-algebra']"
1280166,Compute the limit of a matrix,"We need to compute the limit of a sequence as $x \rightarrow \infty$ Using matrix-matrix multiplication we can define power as $A^p=A*\cdots*A$, $p$ times. We need to compute the limit of $A^p$ as $p \rightarrow \infty$ Now my question is, is it fine to compute some of them and telling that it will approach $0$ or not? Like for example starting from computing $A^2$, then $A^4$, $\ldots,A^{20}$. Is this good for showing a limit or not?","['linear-algebra', 'limits', 'matrices']"
1280180,Good video lectures in Differential Geometry,"I was not fortunate enough to learn Differential Geometry during my Masters. As now I am having my thesis in PDEs, and I miss a lot of mathematics from the people who do PDEs on Manifold setting. I badly want to learn Differential geometry, especially from the point of view of PDEs.
Do anyone know good video lectures on the subject? PLEASE let me know. Thanks","['differential-geometry', 'reference-request']"
1280185,Is there an equidissection of a unit square involving irrational coordinates?,"An equidissection of a square is a dissection into non-overlapping triangles of equal area. Monsky's theorem from 1970 states that if a square is equidissected into $n$ triangles, then $n$ is even.  In 1968, John Thomas proved the following weaker statement: there is no equidissection of a unit square into an odd number of triangles whose vertices are rational numbers with odd denominators. I wonder to which extent the result of Thomas fails to cover all cases. It is easy to construct an equidissection where the coordinates are rational numbers with even denominators.  My question is: Is there an equidissection of the unit square containing a triangle with at least one irrational coordinate? Dissecting the square into $6$ or less triangles seems to always yield rational coordinates.","['combinatorial-geometry', 'geometry', 'triangles']"
1280198,An interesting point of a triangle. (Help needed to prove a statement.),"Consider a triangle whose sides are segments of $\color{red}{\text{line}}$, $\color{blue}{\text{line}}$, $\color{green}{\text{line}}$ falling in the circum-circle $c$. Let $\color{red}{\text{P}}$,$\color{green}{\text{P}}$, $\color{blue}{\text{P}}$ be the poles (with respect to  $c$) of the corresponding sides of the triangle. Now, take a point $P$ different from the poles. Connect the poles with $P$. The connecting lines will intersect the corresponding edges or the elongations of theses edges mentioned above (perhaps in the $\infty$). (Corresponding means: $\color{red}{\text{ red broken line}}$ with $\color{red}{\text{ red edge line }}$, etc. Then connect the vertices of the triangle with the the opposite intersection points mentioned above as shown in the figure below (white lines). The white lines will meet in one point. (Perhaps in the infinity; then the white lines are parallel.) I call this point the $P$-pole point of the triangle with respect to its circum-circle and point $P$. I cannot prove that the pole point always exists. (It exists even if the white lines are parallel.) Any help, please? Any known results? The same statement can be told easier in the language of hyperbolic geometry:
Take an ideal triangle and a point $P$ not on the sides. Drop perpendiculars from $P$ to the sides of the triangle. Consider the intersection points. Then connect these intersection points with the opposite vertices with suitable parallels. These parallels will meet in one point, the ""pole point of the ideal triangle-with respect to $P$. (See the figure below.) To be honest I don't have a clue as to how to prove the statements given above. I found the ""pole point"" in the clear blue.","['projective-geometry', 'geometry', 'hyperbolic-geometry']"
1280211,Having a difficult time figuring out if linear equations is true or false.,"I'm having a very difficult time figuring out if these linear equations are true or false:
$$
x + 2y > 6\\
x - y < 3 
$$
How do you identify if these equations are true or false? At first I thought I was supposed to write out the solution, then once I get the answer I was supposed to say if it was true or false. Is that right? 
I created this graph identifying both inequalies, but I just don't know how to figure it's true or not. If anyone could help that would be great.",['algebra-precalculus']
1280213,Prove local minimum of a convex function is a global minumum (using only convexity),"Let $C\subseteq \mathbb{R}^d$ a convex set, and let $f:C\rightarrow \mathbb{R}$ be a convex function. Let $x^*$ be a local minimizer of $f$, that is there exists a value $p>0$ such that for every $x\in C$ : $||x-x^*||\leq p \Rightarrow f(x) \geq f(x^*)$. How do I show that that $x^*$ is a global minimum without using limits, but only using the convexity property ? I know how to prove this using limits. I tried to prove it by contradiction (i.e assume by contradiction that there exists another $x$ that is a local minimum), but have gotten nowhere.","['convex-analysis', 'calculus']"
1280216,Linear decomposition of positive semi-definite matrices,"It is true that the vector space of $n\times n$ Hermitian matrices is an $n^2-$dimensional real vector space and that one can find a basis for this space consisting exclusively of positive semi-definite matrices (see basis for hermitian matrices ). My question is, if we have the linear combination 
$$
M=\sum_{k=1}^{n^2}x_kB_k
$$
where $B_k$ is the aforementioned positive semi definite basis matrix, what are the restrictions on $x_k$ so that $M$ remains positive semi-definite? A sufficient condition is (I think) that all $x_k\geq 0$, but I don't think it is necessary. If $B_k$ were real numbers, the solution to that problem would be one of the two halfspaces into which $\mathbb{R}^{n^2}$ is divided into by the hyperplane $\sum_{k=1}^{n^2}x_kB_k=0$. Could there be an analogy with this case, when $B_k$ belong to the vector space of Hermitian matrices?","['linear-algebra', 'matrices']"
1280217,Neighborhoods vs Open Neighborhoods?,"Since when I started studying general topology there is something concearning neighborhoods which baffles me. Given a topological space $(X, \mathscr{T})$ and $p\in X$ we say $U\subseteq X$ is a neighborhood of $x$ if there is $W\in\mathscr{T}$ such that $x\in W\subseteq U$. Naturally, every open set $W\in\mathscr{T}$ containing $x$ is a neighborhood of $x$ which we call open neighborhood of $x$. There are several topological properties (not in the sense of topological invariance) which can be described in terms of neighborhoods like continuity, haudorffness, etc. Of course if a given property holds for a neighborhood it also holds for an open neighborhood. Can anyone give me some examples where some property holds for open neighborhoods but it fails for arbitrary ones? Continuity and Hausdorfness won't work for we might interchange neighborhoods and open neighborhoods and we get equivalent definitions. Thanks.",['general-topology']
