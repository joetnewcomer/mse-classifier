question_id,title,body,tags
158836,Evaluating: $\lim_{n\to\infty} \int_{0}^{\pi} e^x\cos(nx)\space dx$,"Evaluate the limit: $$\lim_{n\to\infty} \int_{0}^{\pi} e^x\cos(nx)\space dx$$ W|A tells that the limit is $0$, but i'm not sure why is that result or if this is the correct result.","['definite-integrals', 'integration', 'real-analysis', 'limits']"
158866,Find Zariski closure of a set,"Let $X=\{(x,\sin(x)): x \in \mathbb{A}^{2}\}$. I want to find the closure (with respect Zariski topology) of $X \subseteq \mathbb{A}^{2}$. OK I've already shown that $X$ is not a closed set. Now consider $cl(X)$ this is a closed subset of $\mathbb{A}^{2}$ so its dimension is $0,1$ or $2$, it is not $0$ because it is not a point. So either $cl(X)$ has dimension $1$ or $2$. I suspect the answer is $2$. If the dimension is $1$ then $X=V(f)$ for some $f \in k[x,y]$ with $f$ irreducible. This implies then that $f(a,\sin(a))=0$ for every $a \in \mathbb{A}^{1}$. Question: does this implies that $f$ is the zero polynomial? From this it would follow that the dimension is $2$ so $cl(X)=\mathbb{A}^{2}$.",['algebraic-geometry']
158872,convergence of $\alpha$-Hölder-continuous functions,"Let $\Omega\subset\mathbb R^n$ be compact and $C^{0,\alpha}(\Omega)$ the space of all $\alpha$-Hölder-continuous functions. Define $||u||_{C^{0,\alpha}(\Omega)}:=||u||_{\sup}+\sup\limits_{{x,y\in \Omega\space\&\space x\ne y}}\frac{|u(x)-u(y)|}{|x-y|^\alpha}$ and consider $(C^{0,\alpha}(\Omega),||u||_{C^{0,\alpha}(\Omega)})$ and $\alpha\in]0,1]$ . How can you prove that  for any sequence in bounded closed set of $(C^{0,\alpha}(\Omega),||u||_{C^{0,\alpha}(\Omega)})$ there exists a convergent subsequence  (concerning the uniform norm ) and it limes is in  $(C^{0,\alpha}(\Omega))$?","['calculus', 'functional-analysis', 'real-analysis', 'analysis']"
158886,How do I show this formula involving several variables?,"This is from Woll's ""Functions of Several Variables,"" but there's no proof. If $g$ is of class $C^k$ ($k \ge 2$) on a convex open set $U$ about $p$ in $\mathbb{R}^d$, then for each $q \in U$, $
g(q) = g(p) + \sum_{i=1}^d \frac{\partial g}{\partial r_i} \bigg|_p (r_i(q) - r_i(p)) + \sum_{i,j} (r_i(q) - r_i(p)) (r_j(q) - r_j(p)) \int_0^1 (1-t) \frac{\partial^2g}{\partial r_i \partial r_j} \bigg|_{p + t(q - p)} dt.
$ It looks like Taylor's or mean value theorem. I especially don't understand the integral part.",['analysis']
158894,Is this an abuse of set-theoretic notation?,"The expression is this: $$\bigcup_{n\in\mathbb{N}}\ \bigcup_{a_0\in\mathbb{Z}}\cdots\bigcup_{a_n\in\mathbb{Z}}\big\{z\in\mathbb{C}:a_0z^n+a_1z^{n-1}+\cdots+a_{n-1}z+a_n=0\big\}.$$ I hope it's clear what this is meant to denote (the set of algebraic numbers), but I'm uneasy about it since the number of unions depends on an element in the first union. I suspect that the set could be rewritten more concretely as a union indexed by $\mathbb{Z}$-valued sequences that eventually terminate, but I'm not sure how that would look.","['elementary-set-theory', 'analysis']"
158896,"Proving that $a + b = b + a$ for all $a,b \in\mathbb{R}$","Being interested in the very foundations of mathematics, I'm trying to build a rigorous proof on my own that $a + b = b + a$ for all $\left[a, b\in\mathbb{R}\right] $. Inspired by interesting properties of the complex plane and some researches, I realized that defining multiplication as repeated addition will lead me nowhere (at least, I could not work with it). So, my ideas: Defining addition $a+b$ as a kind of ""walking"" to right $\left(b>0\right)$ or to the left $\left(b<0\right)$ a space $b$ from $a$. Adding a number $b$ to a number $a$ (denoted by $a+b$) involves doing the following operation: Consider the real line $\lambda$ and its origin at $0$. Mark a point $a$, draw another real line $\omega$ above $\lambda$ such what $\omega \parallel \lambda$ and mark a point $b$ on $\omega$. Now, draw a line $\sigma$ such that $\sigma \perp \omega$ and the only point in commom between $\sigma$ and $\omega$ is $b$. Consider the point that $\lambda$ and $\sigma$ have in commom; this point is nicely denoted as $a + b$. (Note that all my work is based here. Any problems, and my proof goes to trash) This definition can be used to see the properties of adding two numbers $a$ and $b$, for all $a, b \in\mathbb{R}$. Using geometric properties may lead us to a rigorous proof (if not, I would like to know the problems of using it). So, I started: $a, b \in\mathbb{N}$: $a+b = \overbrace{\left(1+1+1+\cdots+1\right)}^a + \overbrace{\left(1+1+1+\cdots+1\right)}^b = \overbrace{1+1+1+1+\cdots+1}^{a+b} = \overbrace{\left(1+1+1+\cdots+1\right)}^b + \overbrace{\left(1+1+1+\cdots+1\right)}^a = b + a$ (Implicity, I'm using the fact that $\left(1+1\right)+1 = 1+\left(1+1\right)$, which I do not know how to prove and interpret it as cutting a segment $c$ in two parts -- $a$ and $b$. However, this result can be extended to $\mathbb{Z}$ in the sense that $-a$ $(a > 0)$ is a change; from right to left). $a, b \in\mathbb{R}$: Here, we have basically two cases: $a$ and $b$ are either positive or negative; $a$ and $b$, where one of them is negative. Since in my definition $-b, b>0$ means drawing a point $b$ to the left of the real line, there's no big deal interpretating it; subtracting can be interpreted now. So, it starts: $a + b = c$. However, $c$  can be cut in two parts: $b$ and $a$. Naturally, if $a>c$, then $b<0$ -- many cases can be listed. So, $c = b + a$. But $c = a + b$; it follows that $a + b = b + a$. My questions: Is there any problem in using my definition of adding two numbers $a$ and $b$, which uses many geometric properties? Is there any way to solve it from informality? Is there anything right here? Thanks in advance.","['real-analysis', 'definition']"
158902,Must a weakly or weak-* convergent net be eventually bounded?,"Let $\mathfrak{X}$ be a Banach space.  As a standard corollary of the Principle of Uniform Boundedness, any weak-* convergent sequence in $\mathfrak{X}^*$ must be (norm) bounded.  A weak-* convergent net need not be bounded in general, but must it be eventually bounded? It seems like the following should prove that the answer is yes: If $\{y_\nu\}$ is a net in $\mathfrak{X}^*$, suppose it's not eventually bounded.  Then we can recursively construct an unbounded subsequence: since the net is not bounded, there exists some $\nu_1$ with
$\|y_{\nu_1}\| > 1$.  By hypothesis the tail subnet $\{y_\nu \mid \nu \geq \nu_1\}$ is not bounded, so there exists some $\nu_2 \geq \nu_1$ with $\|y_{\nu_2}\| > 2$, and so on.  If the original net were weak-* convergent, so would this unbounded subsequence, contradicting PUB. It would then follow that weakly convergent nets in $\mathfrak{X}$ are bounded as well, because the image in $\mathfrak{X}^{**}$ would be weak-* convergent. Question: This is legit, right?  I'm still not quite comfortable enough with nets or with the weak-* topology to entirely trust myself here, and I'd like to know the answer since I seem to be bumping into this question a lot recently.","['general-topology', 'functional-analysis', 'banach-spaces']"
158906,the solution set of $\left | \frac{2x - 3}{2x + 3} \right |< 1$,"what is the solution set of $\left | \frac{2x - 3}{2x + 3} \right |< 1$ ? I solved it by first assuming:  $-1 < \frac{2x - 3}{2x + 3 } < 1$ ended with: $x > 0 > -3/2$ Is that a correct approach? And how to derive the solution set from the last inequality? Is it  $(-\frac{3}{2}, \infty)$ or $(0, \infty )$ ? Thanks in advance.","['inequality', 'absolute-value', 'algebra-precalculus']"
158917,Random variables and sigma field,"Given $Z_1,Z_2,\cdots$ i.i.d with $E|Z_i|<\infty$. $\theta$ is an independent r.v. with finite mean and $Y_i=Z_i+\theta$. If we define $F_n=\sigma(Y_1,\cdots,Y_n), F_\infty=\sigma(\cup_n F_n).$ Do we have $\theta\in F_\infty$?","['measure-theory', 'probability']"
158932,Is $A^{q+2}=A^2$ in $M_2(\mathbb{Z}/p\mathbb{Z})$?,"I'm wondering, why is it that for $q=(p^2-1)(p^2-p)$, that $A^{q+2}=A^2$ for any $A\in M_2(\mathbb{Z}/p\mathbb{Z})$? It's not hard to see that $GL_2(\mathbb{Z}/p\mathbb{Z})$ has order $(p^2-1)(p^2-p)$, and so $A^q=1$ if $A\in GL_2(\mathbb{Z}/p\mathbb{Z})$, and so the equation holds in that case. But if $A$ is not invertible, why does the equality still hold?","['matrices', 'linear-algebra', 'abstract-algebra']"
158936,What is the probability of two people meeting?,"I am trying to figure out a solution to the following problem: Let there be two groups of people, Group A and Group B. Group A represents x percent (e.g. 1%) of the world's population, and Group B represents y percent (e.g. 2%) of the world's population. What is the probability that a person from Group A will meet a person from Group B? Assume the following things: The average human being meets z people (e.g. 100,000) in a lifetime. The world's population is kept at a constant k people. Everyone in the world was born and will die at the same time. Disclaimer: I came up with this question myself, but I'm not a mathematician, so please feel free to clean this up if need be. Also, if there is not enough information in the problem to solve it, add assumptions and please indicate the reasons for adding them. The assumptions I wrote are my attempt at making the problem easier. If they are not necessary, and removing any produces a more accurate answer, then I encourage the removal of them.","['statistics', 'discrete-mathematics', 'probability']"
158938,How to find Singular solutions of differential equation based physical model?,"To get singular solutions, do we always need a guess or experiment? Can we get it from a relation of family of curves of general solution? For example, $(y')^2-xy'+y=0$ has the general solution $y=cx-c^2$. It has a singular solution of $y=x^2/4$, too. If you draw family of curves of general solution (a bunch of straight lines) as well as curve of singular solution (a parabola), you can find parabola is touching general family of curves with a pattern. Can that be a point to get singular solution? In general, is there a way to calculate singular solutions mathematically?","['singular-solution', 'ordinary-differential-equations', 'calculus', 'differential-geometry']"
158955,How to prove that the tangent to a circle is perpendicular to the radius drawn to the point of contact?,I've tried drawing a parallel chord to the tangent but then how would you prove that the chord is perpendicular to the radius?,"['geometry', 'circles']"
158961,Regular covering implies transitive automorphism group action,"We already know the theorem Theorem Let $p: (Y,y) \rightarrow (X,x)$ be a covering, with $Y$ connected and $X$ locally path connected, and let $p(y) = x$ . If $p_*(\pi_1(Y,y))$ is a normal subgroup of $\pi_1(X,x)$ , then $\pi_1(X,x)/p_*(\pi_1(Y,y))$ is isomorphic to Aut(Y/X). Question As the situation above, T.F.A.E (a) the covering is normal (i.e., $p_*(\pi_1(Y,y))$ is a normal subgroup of $\pi_1(X,x)$ ); (b) the action of $Aut(X/Y)$ on $p^{-1}(x)$ is transitive; (c) for every loop $\sigma$ at $x$ , if one lifting of $\sigma$ is closed (i.e., still a loop at some $y \in p^{-1}(x)$ ), then all liftings are closed. My approach I already proved (b) $\implies$ (c) and (c) $\implies$ (a). So I need to prove (a) $\implies$ (b), and I know I need to use the theorem above. So, is that true that for any $y$ and $y' \in p^{-1}(x)$ , the there exists $[\sigma] \in \pi_1(X,x)$ such that $y' = [\sigma]y$ ? (If this is true, then we are done!) Thank you very much!","['general-topology', 'algebraic-topology']"
158970,Which conformal maps should one have memorized?,"I know this is a bit vague and there's no end to the conformal maps, but I'm just curious which ones you've memorized out of sheer usefulness, particularly if you work in an area related to complex analysis. Which ones should one be able to derive as well?",['complex-analysis']
158972,Characteristic polynomials of powers and sums of matrices,"If I know the characteristic polynomial of a matrix $A$, what can I know about the charpoly of $A^2$? And if I have the charpolys of $A$ and $B$, what can I know about the charpoly of $A+B$? I'm trying to solve the following problem: The eigenvalues of $A$ are $1,-3,0$. Show that the eigenvalues of $A^2+A-2I$ are $0,2,-4$. Thank you! Edit: I now know that the eigenvalues of $A^2$ are the squares of the eigenvalues of $A$. I still need help solving the problem. Thanks!","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
158975,Lebesgue Integration Question,"(Just read the bolded statements if you want to get straight to the point) This question comes as an extension to one posed in Stein and Sakarchi's Real Analysis, and it is related to the notion that an integral of a positive function is equal to the volume bounded by its graph. The text proves that $\int_{\mathbb{R}^{d}}|f(x)|dx=m(A)$ where $A:=\{(x,\alpha)\in\mathbb{R}^{d}\times\mathbb{R} : 0\leq\alpha\leq|f(x)|\}$.  Assuming both $A$ and $f$ are measurable in the appropriate contexts, the proof is a simple computation:
\begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int_{\mathbb{R}^{d}}m(A_{x})dx=\int_{\mathbb{R}^{d}}\chi_{A_{x}}(x)dx=\int\limits_{0}^{\infty}\int_{\mathbb{R}^{d}}\chi_{A}(x,\alpha)dxd\alpha=m(A)\end{equation*} (we are of course applying Tonelli's Theorem). Now, here is the question from Stein and Sakarchi: If $f$ is integrable on $\mathbb{R}^{d}$, then define for each $\alpha>0$ the set $E_{\alpha}:=\{x:|f(x)|>\alpha\}$, and prove \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int\limits_{0}^{\infty}m(E_{\alpha})d\alpha.\end{equation*} The proof is basically an immediate consequence of what was already proven above, except that we use the slices $A_{\alpha}$ instead of $A_{x}$.  In other words, we have: \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|dx=\int_{\mathbb{R}^{d}}m(A_{x})dx=\int\limits_{0}^{\infty}m(A_{\alpha})d\alpha=m(A).\end{equation*}  Since $A_{\alpha}$=$E_{\alpha}$, the problem is solved.  A nice geometric meaning of integrating the separate slices is that integrating $A_{x}dx$ is akin to partitioning the domain, and integrating $A_{\alpha}d\alpha$ is akin to partitioning the range.  Again, the rigorous justifications are from the Fubini/Tonelli theorem. Now, here is the part where I am having difficulty. I want to prove the statement \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int_{0}^{\infty}p\alpha^{p-1}m(E_{\alpha})d\alpha\end{equation*}
where everything is as above, and $1<p<\infty$.  The above case is $p=1$.  Since we are not using $\{x: 0<\alpha<|f(x)^{p}\}$, the above proof technique cannot be used exactly, and I'm not sure how to proceed. It is interesting that \begin{equation*}$|f(x)|^{p}=\int\limits_{0}^{|f(x)|}p\alpha^{p-1}d\alpha\end{equation*} so that we get something like \begin{equation*}\int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int_{\mathbb{R}^{d}}\int_{0}^{|f(x)|}p\alpha^{p-1}d\alpha.\end{equation*}  However, I'm still not sure where to take this.","['integration', 'analysis']"
158980,Is there a nice way to classify the ideals of the ring of lower triangular matrices?,"Suppose $T$ is the subset of $M_2(\mathbb{Z})$ of lower triangular matrices, those of form $\begin{pmatrix} a & 0 \\ b & c\end{pmatrix}$. So $T$ is a subring. Now I know that the ideals of $M_2(\mathbb{Z})$ are all of the form $M_2(I)$ for $I$ and ideal of $\mathbb{Z}$. However, is there a nice way to describe all the ideals in $T$ specifically, or does it not behave quite as nicely?","['matrices', 'ideals', 'linear-algebra']"
158988,Implicit Function Theorem example in Baby Rudin,"I am looking at example 2.29 of Baby Rudin (page 227) of my edition to illustrate the implicit function theorem. This is what the example is: Take $n= 2$ and $m=3$ and consider $\mathbf{f} = (f_1,f_2)$ of $\Bbb{R}^5$ to $\Bbb{R}^2$ given by 
    $$\begin{eqnarray*} f_1(x_1,x_2,y_1,y_2,y_3) &=& 2e^{x_1} + x_2y_1 -4y_2 + 3 \\
f_2(x_1,x_2,y_1,y_2,y_3) &=& x_2\cos x_1 - 6x_1 + 2y_1 - y_3 \end{eqnarray*}.$$
    If $\mathbf{a} = (0,1)$ and $\mathbf{b} = (3,2,7)$, then $\mathbf{f(a,b)} = 0$. With respect to the standard bases, the derivative of $f$ at the point $(0,1,3,2,7) $ is the matrix 
    $$[A] = \left[\begin{array}{ccccc} 2 & 3 & 1 & -4 & 0 \\ -6 & 1 & 2 & 0 & -1 \end{array}\right].$$
    Hence if we observe the $2 \times 2$ block
    $$\left[\begin{array}{cc} 2 & 3 \\ -6 & 1 \end{array}\right]$$
    it is invertible, and so by the implicit function theorem there exists a $C^1$ mapping $\mathbf{g}$ defined on a neighbourhood of $(3,2,7)$ such that $\mathbf{g}(3,2,7 ) = (0,1)$ and $\mathbf{f}(\mathbf{g}(\mathbf{y}),\mathbf{y}) = 0$. Now what I don't understand is from such a $\mathbf{g}$, how does this mean that I can solve the variables $x_1$ and $x_2$ for $y_1,y_2,y_3$ locally about $(3,2,7)$? Also if I wanted to carry out this computation explicitly, how can I do it? We do not have a nice and shiny linear system to solve unlike problem 19 of the same chapter. Thanks.","['real-analysis', 'analysis']"
158999,How to solve a differential equation containing an integral term?,"I'm asking this question on behalf of another person who is not as familiar as me with computers. He has the following problem to solve and unfortunately Mathcad can't solve such type of equations. He seeks help to find the function $u = u(x) = ?$ for $$ \frac{d^2u(x)}{dx^2} =\left[U(x) + \alpha^2\right]u(x) $$ where
$$U(x) = \frac{2}{x}\left(\exp\left(-\frac{x}{\lambda}\right)-\int_0^x 4\pi\left(u(z)\right)^2\,dz\right)$$ The parameters $\lambda$ and $\alpha$ are real. Is another tool (i.e. Mathematica) able to solve it ? Do you have another suggestion ?","['ordinary-differential-equations', 'integration']"
159018,Evaluating: $\lim_{n\to\infty} \frac{\sqrt n}{\sqrt {2}^{n}}\int_{0}^{\frac{\pi}{2}} (\sin x+\cos x)^n dx $,"I'm supposed to compute the following limit: $$\lim_{n\to\infty} \frac{\sqrt n}{\sqrt {2}^{n}}\int_{0}^{\frac{\pi}{2}} (\sin x+\cos x)^n dx  $$ I'm looking for a resonable approach in this case, if possible. Thanks.","['integration', 'real-analysis', 'limits']"
159039,Different integrals for $\mathbb{C} \to \mathbb{C}$ functions,"I am a little confused with the different integrals on the complex numbers. So lets start $\mathbb{R} \to \mathbb{R}$
Standard Lebesgue integration $\mathbb{R}^2 \to \mathbb{R}$
We can either integrate along a path in $\mathbb{R}^2$, then on 
this path we have a $\mathbb{R} \to \mathbb{R}$ or we use measure theory
and the two dimensional Lebesgue measure. $\mathbb{R} \to \mathbb{C}$ (i.e.$\mathbb{R} \to \mathbb{R}^2$)
Just integrate the real and imaginary part separately. Now we are at the core of my problem
4. $\mathbb{C} \to \mathbb{C}$  (i.e. $\mathbb{R}^2 \to \mathbb{R}^2$) So again we have two choices. either do a line integral or we use measure theory
and in each case treat real and imaginary parts differently. Obviously these integrals are not the same. If we take a constant function and
some domain. The integral over this domain using the path integral will be zero.
The other one will not. Unlike the case $\mathbb{R} \to \mathbb{R}$ there is not a unique integral
for functions $\mathbb{C} \to \mathbb{C}$. So my questions are: Why is the path integral the ""right"" one, as it is the one used in complex analysis. Is there a more natural or intrinsic construction of this integral. Does the other integral for $\mathbb{C} \to \mathbb{C}$ occur anywhere in mathematics.
Does it have important applications","['complex-analysis', 'integration', 'real-analysis']"
159045,"Matrix inverse property, show that $(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu}$","Let $u, v \in \mathbb{R}^N, u^Tv \neq -1$. Thereby $I +uv^T \in \mathbb{R}^{N \times N}$ is invertible. Show that: $$(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu}$$ I'm lost, why did the denominator get $uv^T$ as $v^T u$? Where did this $1$ come from? Any hints are very appreciated!",['linear-algebra']
159077,How to prove that $A_5$ has no subgroup of order 30?,I need to show that $A_5$ has no subgroup of order 30. Any ideas?,"['finite-groups', 'group-theory']"
159098,A particular (functional) determinant calculation,"One wants to calculate the quantity, $\det'(\frac{\partial}{\partial t} - i [\alpha, ])$ where the prime on the ""det"" means that one wants to do a product over only non-zero eigenvalues of the operator $\frac{\partial}{\partial t} - i [\alpha, ]$. This operator is acting on the adjoint representation of a Lie algebra. ($\alpha$ itself is in the adjoint representation of the Lie algebera) Now one claims that one can find an eigenbasis basis of matrix functions for $\alpha$ such that whose eigenvalues are $\{ \lambda _i \}_{i=1} ^ {i = n}$ and whose $t$ dependence is $\exp(\frac{i2\pi n t}{\beta})$ Can someone write down the exact equation into which the above translates? I would vaguely guess that if $X_i$ is such an eigenvector then because of the adjoint nature of the representation it means that $[\alpha, X_i] = \lambda _ i X_i$
But I don't seem to see exactly where to fix the exponential dependence. Now one claims that in this basis the determinant is equal to the following expression, $$\prod _{n \neq 0} \prod _ {i,j} [ \frac{i2\pi n}{\beta} - i (\lambda _ i - \lambda _j)]$$ and the above can apparently be simplified to give, $$\det'(\frac{\partial}{\partial t} - i [\alpha, ]) = \left ( \prod _{m \neq 0} \frac{i2\pi m}{\beta} \right )\prod _ {i,j} \frac{2}{\beta (\lambda _i - \lambda _j)}\sin (\frac{\beta (\lambda _ i - \lambda _j)}{2})$$ It would be great if someone can help explain the above simplification.","['physics', 'representation-theory', 'linear-algebra', 'functional-analysis', 'determinant']"
159102,Schwartz class estimation.,"I have a function $f\in \mathcal{S}$ (i.e of Schwartz class), and I want to show there exist constants $C,k>0$ s.t 
$$\|f\|_p \leq C(\sup_{x\in \mathbb{R}} |f(x)| + \sup_{\mathbb{x\in \mathbb{R}}} |x^k f(x)|)$$
for every $ p \in [1,\infty]$. For $p=1,\infty$ it's obvious from definition, I mean I can take f with compact support and this will prove for $p=1$, for $p=\infty$ it's trivial. But for $ p \in (1,\infty)$ I find myself at a mess, I need to do integration by parts inductively but I don't seem to find the right approach, I guess I need to use here Leibnitz general product rule, but I don't see how to come to suitable constants. Any help , is appreciated.","['fourier-analysis', 'real-analysis', 'analysis']"
159115,"Evaluating $\int_{0}^{\frac{\pi}{2}} e^{x+2}\sin(x) \,dx$","Could someone show me how to solve this integral?
$$\int_0^{\frac{\pi}{2}} e^{x+2}\sin(x) \,dx$$
I think that it's improper, but I'm not sure. I tried to solve by parts, but first I sobstitute
$$e^{x+2} = u$$
And this is what I obtained:
$$\int{u\sin(\log(u) - 2)\frac{1}{u}}\,du$$
And at this point, integrating by part seems easy, but... Could you help me? Maybe avoiding to follow the way that uses WolframAlpha because I don't know that method.
Thanks in advance","['improper-integrals', 'integration']"
159123,Question about conjugacy class of alternating group,"This is problem 26 from Grove's ""Algebra."" Suppose $K$ is a conjugacy class in $S_n$ of cycle type $(k_1,...,k_n)$ , and that $K \subseteq A_n$ . If $\sigma \in K$ write $L$ for the conjugacy class of $\sigma$ in $A_n$ . If either $k_{2m} > 0$ or $k_{2m+1} > 1$ for some $m$ show that $L = K$ . I can show $L \subseteq K$ but not $K \subseteq L$ . I don't know how to use the "" $k_{2m} > 0$ or $k_{2m+1} > 1$ "" hypotheses. If $k_{2m} > 0$ for some $m$ then $\sigma \in A_n$ must have an even number of odd transpositions. Can I get a hint? Thank you. Edit: $k_m$ is the number of cycles of length m.",['group-theory']
159127,Show a sequence is decreasing,"I'm stuck trying to show that the following sequence is decreasing
$$a_{n} = \left(\frac{n+x}{n+2x}\right)^{n}$$ where $x>0$.   I've tried treating $n$ as a real number and took derivatives but it didn't lead to anything promising.   Any hints would be appreciated.","['calculus', 'analysis']"
159129,Intersection of the sets of generalized doubly stochastic matrices and of orthogonal matrices,"The definition of a doubly stochastic matrix can be found here . We say a square matrix $A$ is a generalized doubly stochastic matrix if the sums of each rows and columns of $A$ all equal $1$ , but $A$ doesn't have to be non-negative. An interesting fact (which is also easy to prove) about doubly stochastic matrices is: if $A$ is doubly stochastic and orthogonal, then $A$ is actually a permutation matrix. What is the intersection set for a generalized doubly stochastic matrix set and orthogonal matrix set? More specifically, can any one give me an example of an $N \times N$ matrix $A$ , which satisfy the following constraints: $AA^T=I$ $A 1=1$ $A^T 1=1$ there exists at least one entry $A_{i,j}$ , satisfying $A_{i,j}<0$","['orthogonal-matrices', 'matrices', 'linear-algebra']"
159135,Defining a linear map via kernel and image.,"Are linear maps defined in a 1-1 manner by setting their kernel and image? In other words, If I have a vector space, and I define a set to be the kernel of my would-be linear map, and another set to be it's image. Would I get a well defined, one linear map? (Given that my Ker and Im are okay, e.g. the dimensions are fine, etc.)
I have the following task: Let T be a linear map in R^4. Given that
 orthogonal_space_of(k e rT) = { (1, 2, 0, 4) , (  1, 0,1, 0) }, and T(1,0,1,1)=(1,2,1,1), give an example of such T (""you don't have to explicitly find T(x1,x2,x3,x4)."")
Any ideas on how to do that (how can I characterize a linear map T without finding an explicit formula for it?) I thought about finding it's Ker and Im, but that doesn't work. Any thoughts?",['linear-algebra']
159152,A proof of the Morse Lemma,"On page 7 of Milnor's Morse Theory is part of a proof of the Morse Lemma: Suppose by induction that there exist coordinates $u_1, \ldots, u_n$ in a neighbourhood $U_1$ of $0$ so that
  $$f = \pm(u_1)^2 \pm \cdots \pm (u_{r-1})^2 + \sum_{i,j \geq r} u_i u_j H_{ij}(u_1,\ldots,u_n)$$
  throughout $U_1$; where the matrices $(H_{ij}(u_1,\ldots,u_n))$ are symmetric. After a linear change in the last $n - r + 1$ coordinates, we may assume that $H_{rr}(0) \neq 0$. (full text of proof: page 6 , page 7 , page 8 ) I do not understand how he can WLOG assume that $H_{rr}\neq 0$ by doing a ""linear change of variables"".  I don't really even understand what he's saying.  Could someone please spell this out for me?  Note that page 6 handles an entirely different part of the lemma.  If you need context, it should be enough to just assume that the equalities at the top of page 7 hold.  You also must know that $f$ has a nondegenerate critical point at $0$, and $f(0)=0$.","['differential-topology', 'differential-geometry']"
159158,Does this explanation of derangements on Wikipedia make sense?,"On the Wikipedia page on derangements , the following description is given about how to count derangements: Suppose that there are $n$ persons numbered $1,2,\ldots,n$ . Let there be $n$ hats also numbered $1,2,\ldots,n$ . We have to find the number of ways in which no one gets the hat having same number as his/her number. Let us assume that first person takes the hat $i$ . There are $n-1$ ways for the first person to choose the number $i$ . Now there are 2 options: A. Person $1$ does not take the hat $i$ . This case is equivalent to solving the problem with $n − 1$ persons $n − 1$ hats: each of the remaining $n − 1$ people has precisely 1 forbidden choice from among the remaining $n − 1$ hats ( $i$ 's forbidden choice is hat $1$ ). B. Person $1$ takes the hat $i$ . Now the problem reduces to $n − 2$ persons and $n − 2$ hats. Aren't the two bolded statements in contradiction?  Isn't the ""first person"" and ""person 1"" the same person? Is this explanation misworded?","['permutations', 'combinatorics']"
159167,Traces of all positive powers of a matrix are zero implies it is nilpotent,"Let $A$ be an $n\times n$ complex nilpotent matrix. Then we know that because all eigenvalues of $A$ must be $0$, it follows that $\text{tr}(A^n)=0$ for all positive integers $n$. What I would like to show is the converse, that is, if $\text{tr}(A^n)=0$ for all positive integers $n$, then $A$ is nilpotent. I tried to show that $0$ must be an eigenvalue of $A$, then try to show that all other eigenvalues must be equal to 0. However, I am stuck at the point where I need to show that $\det(A)=0$. May I know of the approach to show that $A$ is nilpotent?","['matrices', 'linear-algebra']"
159169,Flipping Cards Probability,"You have a deck of cards, 26 red, 26 black. These are turned over,
  and at any point you may stop and exclaim ""The next card is red."". If
  the next card is red you win £10. What's the optimal strategy? Prove this is the optimal strategy. I feel like the optimal strategy is whenever you are in a state that you flip more black than red cards you call out red card, but I do not know how to prove it, nor can I think of why it must be the absolute best strategy. Thanks for any help.","['puzzle', 'recreational-mathematics', 'probability']"
159176,Question on a transfinite construction in algebra,"I am studying out of Matsumura's Commutative Ring Theory, and in the first section on modules he proves (following Kaplansky) that every projective module over a local ring is free. My questions have more to do with an application of transfinite induction than the actual algebra. In the proof of the above result, Matsumura uses a lemma, and it is the proof of the lemma that I have a question on. The proof can be seen at Google Books: link . I am referencing Lemma 1 on page 10. In the lemma, we define a family of submodules $\{F_\alpha\}$ using transfinite induction. I can follow the proof, i.e., I understand the (very) basic mechanics of transfinite induction and how each $F_\alpha$ is constructed and why the desired result follows. Here are my questions: (1) For every ordinal $\alpha$, we define a submodule $F_\alpha$. The ordinals do not form a set. Is it then true that I cannot speak of the set of all submodules $F_\alpha$? Matsumura writes about half way down the page that ""if $F_\beta = F$ then the construction stops at $F_\beta$."" Must this eventually happen? Must this construction terminate? I am having a hard time wrapping my mind around the fact that we have modules (which are sets) and submodules (which are sets), and then all of the sudden we have a family of submodules which isn't (if I have all this right) a set. (2) Why can I not define a sequence of submodules over $\omega$? Why do I need the full strength of transfinite induction? Why do I need all ordinals? As is obvious, this transfinite stuff is quite new to me; I have never used these techniques before. I would appreciate any insight.","['commutative-algebra', 'ordinals', 'elementary-set-theory']"
159180,prove equality with integral and series,"I am stuck on one question with integral. Help me please to show that with $n=1$ the following is true
$$
\int_{0}^{\infty}\left(\frac{2^n}{t^n}\left(\frac{t^n}{2^nn!}-\frac{1}{2^{n+2}}\frac{t^{n+2}}{1!(n+1)!}+\frac{1}{2^{n+4}}\frac{t^{n+4}}{2!(n+2)!}-\ldots\right)\right)^2tdt=2
$$ Thank you for your help.","['approximation-theory', 'special-functions', 'integration']"
159185,It is possible to prove the existence of Gibbs measures using the Kolmogorov extension theorem?,"It is possible to prove the existence of Gibbs measures using the Kolmogorov extension theorem? If yes how? If the proof is too long to write here is there any reference? Thank you. Edit. Let $S$ be a countable set and $\mathscr{S}=\mathscr{S}(S)=\{\Lambda: \Lambda \subset S, \quad 0 <| \Lambda | <\infty \} $. To keep in mind we can take $S$
as $ \mathbb{N}, \mathbb{Z}$ or $\mathbb{Z}^2$. Let $ \mathscr{E}$ a $\sigma$-algebra on the set $\mathbb{E}$. If $(\mathbb{E}, \mathscr{E}) = (\mathbb{E}^i, \mathscr{E}^i), \quad \forall i \in S $ define:$(\mathbb{E}^\Lambda, \mathscr{E}^\Lambda)=\bigotimes_{i\in \Lambda}(\mathbb{E}^i, \mathscr{E}^i).$ If $\Omega \triangleq \mathbb{E}^S = \{\omega=(\omega_i)_{i\in S}: \omega_i \in \mathbb{E}^i, \forall i\in S \} $ then for $ \Lambda, \Gamma \in \mathscr{S} $ with $ \Lambda\subset \Gamma $ define: $ \Pi_i: \Omega \to \mathbb{E}^i $ as the natural projection of $\Omega \triangleq \mathbb{E}^S $ in $\mathbb{E}^i $, $ \Pi_\Lambda: \Omega \to \mathbb{E}^\Lambda $
as the natural projection of $ \Omega \triangleq \mathbb{E}^S $ in $ \mathbb{E}^\Lambda $, $ \Pi_{\Gamma, \Lambda}: \mathbb{E}^\Gamma \to\mathbb{E}^\Lambda$ as the natural projection of $ \Omega \triangleq \mathbb{E}^S$ in $\mathbb{E}^\Lambda $. Now consider the following $ \sigma $-algebras defined on $\Omega $. $ \mathcal{F}_\Lambda = \sigma \big(\Pi_\Lambda\big) $, $ \mathcal{J}_{\Lambda}=\sigma \big(\Pi_{S/\Lambda}\big)$, $\mathcal{F}=\sigma \big (\{\Pi_\Lambda \}_{\Lambda \in \mathscr{S}}\big)$. And also consider the $ \sigma$-algebras $ \mathcal{F}_{\Gamma, \Lambda} $ and $ \mathcal{J}_{\Gamma, \Lambda} $ defined on $ \Omega_\Gamma = \mathbb{E}^\Gamma $ respectively by $ \sigma \big(\Pi_{\Gamma, \Lambda} \big) $ and $ \sigma \big(\Pi_{\Gamma/\Lambda, \Lambda} \big)$. In this notation the Kolmogorov extension theorem can be stated as follows. Definition Given a family of probability measures 
  $\{\mu^{\Gamma}\}_{\Lambda \in \mathscr{S}}$ on $(\mathbb{E}^\Lambda, \mathscr{E}^\Lambda)$ the equations \begin{equation}\mu^\Lambda(\quad)=\mu^\Gamma\big(\Pi^{-1}_{\Gamma 
 \Lambda} (\;\cdot\;) \big) \quad \forall ,\Gamma, \Lambda \in
 \mathscr{S} \text{ with } \Lambda \subset \Gamma \end{equation} are called Kolmogorov consistency condition. and Theorem [Kolmogorov extension] If $(\mu_\Gamma)_{\Gamma \in\mathscr{S}} $ is a family of
  probability measures on $(\mathbb{E}^\Gamma,\mathscr{E}^\Gamma)$,
  meeting the consistency condition Kolmogorov then there exists a unique
  probability measure on $(\mathbb{E}^S, \mathscr{E}^S = \mathcal{F})$
  such that \begin{equation} 
\mu^\Lambda =
\mu \big(\Pi_{\Lambda}^{-1}(\; \cdot
 \;)\big)
\quad \forall \;\Lambda 
\end{equation} In a brief term as a mean Gibbs measure $ \mu $ on the $(\Omega,\mathcal{F})$ satisfies that the condition Dobrushin-Lanford-Ruelle equivalently that is the same as 
$$
\mu \Big(\Pi_\Lambda(A) \times \{\Pi_{S/\Lambda}(\omega)\}\Big)=\mu\Big(A|\mathcal{J}_\Lambda \Big)(\omega)
$$
for $A\in \mathcal{F}$ and $\mu|_{\mathcal{J}_\Lambda}\mbox{-a.e. }\omega\in\Omega$. Here $\mu|_{\mathcal{J}_\Lambda}$ is the restriction of the measure $\mu$ to $\mathcal{J}_\Lambda$. In other words it's like $\mu$ to be specified by the probability of kernels $(\Omega,\mathcal{J}_\Lambda)$ to $(\Omega,\mathcal{F})$ given by
$$
\mathscr{\mathcal{F}}\times\Omega\ni(A,\omega)
\longmapsto
\mu\Big(\Pi_\Lambda(A)\!\times\!\{\Pi_{S/\Lambda}(\omega)\}\Big)
$$","['probability-theory', 'reference-request']"
159186,Estimate total song ('coupon') number by number of repeats,"If shuffle-playing playlist ×100 resulted in [10 13 10  3  2  2] different songs being repeated [1 2 3 4 5 6] times, what is the estimate for the total number of songs? (assuming shuffle play was completely random) Update: (R code) k <- 50 # k number of songs on the disk indexed 1:k
n <- 100 # n number of random song selections
m <- 20 # m number of repeat experiments
colnum <- 10
mat <- matrix(data=NA,nrow=m,ncol=colnum)
df <- as.data.frame(mat)
for(i in 1:m){
  played <- 1+floor(k*runif(n)) # actual song indices (1:k) selected 
  freq <- sapply(1:k,function(x){sum(played==x)})
  # = number of times song with index x is being played
  histo <- sapply(1:colnum,function(x){sum(freq==x)});
  for(j in 1:colnum){
   df[i,j] <- histo[j]
  }
}
df Resulting in: e.g. 20 distributions (V1=number of single plays, V2=number of double plays, etc): V1 V2 V3 V4 V5 V6 V7 V8 V9 V10
1  15 13 11  1  2  2  0  0  0   0
2  15 12 10  1  4  0  1  0  0   0
3  12 14  7  6  3  0  0  0  0   0
4  17 16  6  4  2  0  1  0  0   0
5  17 10 12  5  0  0  1  0  0   0
6  13 15 11  6  0  0  0  0  0   0
7  10 14  9  3  2  1  1  0  0   0
8  12 17  5  6  3  0  0  0  0   0
9   9 19  8  3  1  2  0  0  0   0
10 13  9 11  6  1  0  1  0  0   0
11 16  9 12  5  2  0  0  0  0   0
12 15  9 11  6  2  0  0  0  0   0
13 19  9  7  4  4  1  0  0  0   0
14 17 11  4  7  3  1  0  0  0   0
15 11 20  8  1  3  1  0  0  0   0
16 14 12 10  5  0  2  0  0  0   0
17  9 12  8  7  3  0  0  0  0   0
18 10 15  9  4  2  0  1  0  0   0
19 14 11 12  7  0  0  0  0  0   0
20 16 14 11  3  1  1  0  0  0   0 Now I need to get from here to the Poisson modelling--my R is a bit rusty (? lmer )...--Any help would be appreciated... Attempted Poisson modelling: disappointing fit?! plot(1:colnum,df[1,1:colnum],ylim=c(0,30),
   type=""l"",xlab=""repeats"",ylab=""count"")
for(i in 1:m){
  clr <- rainbow(m)[i]
  lines(1:colnum,df[i,1:colnum],type=""l"",col=clr)
  points(1:colnum,df[i,1:colnum],col=clr)
}

df.lambda=data.frame(lambda=seq(1,5,0.1),ssq=c(NA));df.lambda
for(ii in 1:dim(df.lambda)[1]){
  l <- df.lambda$lambda[ii]
      ssq <- 0
      for(i in 1:20){
        for(j in 1:10){
          ssq <- ssq + (df[i,j] - n*dpois(j,l))^2
        }
      }
      print(ssq)
      df.lambda$lambda[ii] <- l
  df.lambda$ssq[ii] <- ssq
    }
    df.lambda
    lambda.est <- df.lambda$lambda[which.min(df.lambda$ssq)]
lambda.est # 2.4
points(x <- 1:10, n*dpois(1:10,lambda.est),type=""l"",lwd=2)

100*dpois(1:10,3)
n/lambda.est the estimated lambda stays around 2.3, with an n estimate of around 43; the fitted curve seems very discrepant , and seems to worsen with rising n !? 
Doesn't this have to do with the fact, that our repeats are different from the 'classical' Poisson distributions: it's not just ONE event that repeats itself x number of times, but the sum of repeats of different items (songs)?!",['probability']
159192,Sigma-field of a sequence of Random Variables,"Problem : Suppose $\tilde{X}=(X_1,X_2,\dots)$ is a sequence of RVs on $(\Omega,\mathcal{B})$ . Prove that $\sigma(\tilde{X})$ is generated by events of the form: $$\bigcap_{i=1}^m \{X_i\leq x_i\}\mbox{ for }x_1,\dots,x_n \in\mathbb{R}, n\geq 1.$$ Thoughts/work: I do know that if $X:(\Omega,\mathcal{B})\rightarrow(\Omega',\mathcal{B}')$ then $\sigma(X) = \{\{X\in B\}:B\in\mathcal{B}'\}$ . I also worked in the past to prove that if I take a $\pi$ -class $\mathcal{P}=\{(-\infty,a]:a\in\mathbb{R}\}$ and the class of cylinder events in $\mathbb{R}^{\infty}$ determined by $\mathcal{P}$ . That is: $\mathcal{C}=\{\omega=(x_1,x_2,\dots):x_i\leq b_i,i\leq n\}$ for some $b_i\in\mathbb{R},i\leq n, n\geq 1$ is a $\pi$ -class and $\sigma(\mathcal{C})=\mathcal{B}(\mathbb{R}^{\infty})$ . Attempted solution: $\mathcal{C}=\{\bigcap_{i=1}^n\{X_i\le x_i\},x_1,\dots,x_n\in\mathbb{R},n\ge 1\};\quad 
\sigma(\tilde{X})=\{X_n\le a\},n\in\mathbb{N},a\in\mathbb{R}\\
\{X_n\le a\}=\bigcup_{k=1}^\infty \left ( (\{X_n\le a\}) \cap(\bigcap_{i=1}^{n-1}\{X_n\le k\})\right )\\
\\
k\in\mathbb{N}:k>\max_{1\le i\le n-1}X_i(\omega)\Rightarrow\omega\in \{X_n\le k\},\forall i\in\{1,...,n-1\}\\
\Rightarrow\{X_n\le a\}\in\sigma(\mathcal{C})\\
\Rightarrow\{X_n\le a\}\subset\sigma(\tilde{X})\Rightarrow \sigma(\tilde{X})\subset\sigma(\mathcal{C})$ Problems, etc.: A critique of my solution: You have the general idea but it seems you are confusing classes of events.  1st, you have a sigma-field on the left and a single event on the right.  But even if you meant the class of all such events, it would not come anywhere close to being a sigma-field.  For example, it does not include $\{\limsup X_n \le a\}$ .
2nd, the object is to show that inverse images of all sets in $B(R^\infty)$ are in the sigma-field generated by $C$ .  However, it suffices to consider inverse images of cylinder sets. Finally, cylinder sets are not finite dimensional; they are subsets of $R^\infty$ .  They do have obvious finite-dimensional counterparts, but don't confuse the two kinds of sets.","['probability-theory', 'measure-theory', 'real-analysis']"
159210,Non-Decreasing Digits,"A number is said to be made up of non-decreasing digits if all the digits to the left of any digit is less than or equal to that digit. For example, the four-digit number $1234$ is composed of digits that are non-decreasing.  Some other four-digit numbers that are composed of non-decreasing digits are $0011$, $1111$, $1112$, $1122$, $2223$. Notice that leading zeroes are required: $0000$, $0001$, $0002$ are all valid four-digit numbers with non-decreasing digits. The question is How many four-digit numbers are non-decreasing?","['number-theory', 'combinatorics']"
159214,Evaluation of a product of sines [duplicate],"This question already has answers here : Closed 12 years ago . Possible Duplicate: Prove that $\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$ I am looking for a closed form for this product of sines: \begin{equation}
\sin \left(\frac{\pi}{n}\right)\,\sin \left(\frac{2\pi}{n}\right)\dots\sin \left(\frac{(n-1)\pi}{n}\right),
\end{equation} where $n$ is a fixed integer. I would like to see here a strategy that hopefully can be generalized to similar cases, not just the result (which probably can be easily found).","['trigonometry', 'products']"
159215,Inherited Morita similar rings,"Let $R$ and $S$ be Morita similar rings. If a ring $R$ with the following property:
every right ideal is injective. How do I prove that the ring $S$ has this property? If a ring $R$ with the following property:
$R$ is finitely generated. How do I prove that the ring $S$ has this property?","['ring-theory', 'category-theory', 'abstract-algebra']"
159216,Extending the Hardy Littlewood Maximal Function Estimate to the Case $1<p<\infty$,"This question pertains to the extension of the weak-type estimate of the maximal function in $L^{1}$ to a ""sharp"" estimate in $L^{p}$ for $1<p<\infty$ (the case $p=\infty$ is evidently trivial). If $f$ is integrable on $\mathbb{R}^{d}$, then the Hardy-Littlewood Maximal function $f$ is defined to be:
\begin{equation*}
f^{*}(x)=\sup_{x\in B}\frac{1}{m(B)}\int_{B}|f(y)|dy,
\end{equation*}
where the supremum is taken over all balls containing $x$ (it can be interpreted as a sort of maximal average of $f$). The well-known weak-type estimate for $f^{*}$ in $L^{1}$ is given by
\begin{equation*}
m(\{x:f^{*}(x)>\alpha\})\leq\frac{A}{\alpha}||f||_{L^{1}(\mathbb{R}^{d})}
\end{equation*}
where $A$ depends only on $d$ (taking $A=3^{d}$ is sufficient for the proof). It is well-known that this is the best estimate one can hope for, for despite the estimate showing $f^{*}$ is not too much bigger than $f$, it is nevertheless true that in general $f\in L^{1}$ does not imply $f^{*}\in L^{1}$. Anyway, from this exercise, apparently we can modify the situation slightly to obtain a positive result on the integrability of $f^{*}$. Let $f$ be a measurable function on $\mathbb{R}^{d}$ such that: \begin{equation*}
||f||_{L^{p}(\mathbb{R}^{d})}=\int_{\mathbb{R}^{d}}|f(x)|^{p}dx<\infty.
\end{equation*} Then the task is to prove the following inequality: \begin{equation*}
||f^{*}||_{L^{p}(\mathbb{R}^{d})}\leq C||f||_{L^{p}(\mathbb{R}^{d})},
\end{equation*}
where $C$ depends only on $p$ and $d$. There are some preliminary results which I proved which make it possible to prove this without any advanced ""machinery"" in functional analysis: \begin{equation*}
\text{(1) } \int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int\limits_{0}^{\infty}p\alpha^{p-1}m(\alpha)d\alpha,
\end{equation*} \begin{equation*}
\text{(2) } m^{*}(\alpha)=m\left(\{x:f^{*}(x)>\alpha\}\right)\leq\frac{2A}{\alpha}\int_{\{x:|f(x)>\frac{\alpha}{2}\}}|f(x)dx.
\end{equation*} In the above results $A$ is a constant depending on only $d$, $1<p<\infty$, $\alpha>0$ and $m(\alpha)$ is the same as in LHS of $(2)$ except for $f$. These results I've proved, and using them I compute the following to prove the statement: \begin{align*}
||f^{\star}||_{L^{p}(\mathbb{R}^{d})}
&=\int_{\mathbb{R}^{d}}|f^{*}(x)|^{p}dx\\
&=\int_{\mathbb{R}^{d}}\int_{0}^{|f^{*}(x)|^{p}}p\alpha^{p-1}d\alpha dx\\
&=\int_{0}^{\infty}p\alpha^{p-1}m^{\star}(\alpha)d\alpha\\
&\leq\int_{0}^{\infty}p\alpha^{p-1}\left(\frac{2A}{\alpha}\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}|f(x)|dx\right)d\alpha\\
&=2Ap\int_{0}^{\infty}\alpha^{p-2}\left(\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}|f(x)|dx\right)d\alpha\\
&=2Ap\int_{0}^{\infty}\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}\alpha^{p-2}|f(x)|dxd\alpha\\
&=\ldots\\
&=C\int_{\mathbb{R}^{d}}|f(x)|^{p}dx\\
&=C||f||_{L^{p}(\mathbb{R}^{d})}.
\end{align*} Unfortunately, I am having trouble filling in the $\ldots$ to obtain the conclusion.  I've tried messing rewriting expressions in terms of the previous results and working backwards, but I can't get things to work out.","['integration', 'analysis']"
159223,Computing $ \int_0^{\infty} \frac{ \sqrt [3] {x+1}-\sqrt [3] {x}}{\sqrt{x}} \mathrm dx$,"I would like to show that $$ \int_0^{\infty} \frac{ \sqrt [3] {x+1}-\sqrt [3] {x}}{\sqrt{x}} \mathrm dx = \frac{2\sqrt{\pi} \Gamma(\frac{1}{6})}{5 \Gamma(\frac{2}{3})}$$ thanks to the beta function which I am not used to handling... $$\frac{2\sqrt{\pi} \Gamma(\frac{1}{6})}{5 \Gamma(\frac{2}{3})}=\frac{2}{5}B(1/2,1/6)=\frac{2}{5} \int_0^{\infty} \frac{ \mathrm dt}{\sqrt{t}(1+t)^{2/3}}$$ ...?","['definite-integrals', 'gamma-function', 'calculus']"
159232,Relationship: Rank of a matrix $\leftrightarrow$ # of eigenvalues,"Can someone tell, why the number of the nonzero eigenvalues (counted
according to their algebraic multiplicities) of a matrix of type $A^{*}A$,
where $A$ is an arbitrary real or complexvalued matrix, is equal
to the rank of $A$ ? Here , in step 2, it seems to me that exactly this assertion is made, and
I can't quite understand it. I get, that if 
$$
\text{rank}\left(A^{*}A\right)=r
$$ then
$$
\ker\left(A^{*}A\right)=n-r,
$$
if it happens that $A^{*}A$ is an $n\times n$ matrix. But how can
I conclude from this, that the multiplicity of the $0$ eigenvalue
is $n-r$, i.e. that there are $n-r$ that are mapped to $0$ (couldn't
it by that only linear combination of $n-r$ eigenvectors are mapped to $0$, since the kernel doesn't have to necessarily be
spanned by the eigenvectors themselves, as far as I know) ?","['matrix-rank', 'linear-algebra', 'eigenvalues-eigenvectors']"
159244,What is the correct way to solve $\sin(2x)=\sin(x)$,"I've found two different ways to solve this trigonometric equation $\begin{align*}
\sin(2x)=\sin(x) \Leftrightarrow \\\\ 2\sin(x)\cos(x)=\sin(x)\Leftrightarrow \\\\ 2\sin(x)\cos(x)-\sin(x)=0 \Leftrightarrow\\\\ \sin(x) \left[2\cos(x)-1 \right]=0 \Leftrightarrow \\\\ \sin(x)=0 \vee \cos(x)=\frac{1}{2} \Leftrightarrow\\\\ x=k\pi \vee x=\frac{\pi}{3}+2k\pi \vee x=\frac{5\pi}{3}+2k\pi \space, \space k \in \mathbb{Z} 
\end{align*}$ The second way was: $\begin{align*}
\sin(2x)=\sin(x)\Leftrightarrow \\\\ 2x=x+2k\pi \vee 2x=\pi-x+2k\pi\Leftrightarrow \\\\ x=2k\pi \vee3x=\pi +2k\pi\Leftrightarrow \\\\x=2k\pi \vee x=\frac{\pi}{3}+\frac{2k\pi}{3} \space ,\space k\in \mathbb{Z} 
\end{align*}$ What is the correct one?
Thanks",['trigonometry']
159247,properties of a special set in the proof that there is no Lebesgue measure for all subsets of $\mathbb{R}$,"i am working through a proof that there could be no measure on $\mathbb{R}$ such that $\lambda([a,b]) = b - a$ $\lambda(A) = \lambda(A + \{c\})$ First a set $A \subset [0,1]$ is constructed with
$$
 \forall x \in [0,1] ~ \exists ! y \in A ~:~ x - y \in \mathbb{Q}
$$
(the notation $\exists !$ means there exists exactly one such element) Then the following set is considered
$$
 B := \bigcup_{r \in [-1:1] \cap \mathbb{Q}} (A + \{r\})
$$
and it is claimed that it has the property
$$
 [0,1] \subset B \subset [-1:2]
$$
but i don't see why this inclusion-relations hold?",['measure-theory']
159258,Tangent space of coordinate axes,"Let $X$ be the union of the $3$ coordinate axes in $\mathbb{A}^{3}$. I want to compute the tangent space of $X$ at every $p \in \mathbb{A}^{3}$. One can check that the only singular point of $X$ is the origin so if $p$ is the origin then the tangent space of $X$ at the origin is equal to $\mathbb{A}^{3}$. Question: how do we express the answer if $p \in \mathbb{A}^{3} \setminus \{(0,0,0)\}$? Would it be correct to say: Note that $I(X)=(xy,xz,yz)$ so if $p=(a,b,c) \in \mathbb{A}^{3}$ then: $T_{p}(X):=\{(x_{1},x_{2},x_{3}) \in \mathbb{A}^{3}: \sum_{i=1}^{3} \frac{\partial f}{\partial x_{i}}(a,b,c)(x_{i})=0\ \text{for every f in I(X)}\}$ In case $p$ is not the origin, is there a ""nice"" way to express $T_{p}(X)$ other than the above?",['algebraic-geometry']
159261,Does a surjective ring homomorphism have to be surjective on the unit groups?,"I know ring homomorphisms map units to units, which made me curious about the following. Suppose $f:R\to R'$ is a surjective ring homomorphism, mapping $1$ to $1'$. Is it necessarily surjective from $U(R)\to U(R')$? I know if $f(u)$ is a unit in $R'$ with $f(w)$ its inverse, then $f(uw)=f(wu)=1'$, but I see no reason to conclude $uw=wu=1$ in $R$ without assuming $f^{-1}(1')=\{1\}$. But I can't find a counterexample, so I'm not sure whether it's true or not.","['ring-theory', 'abstract-algebra']"
159269,Take set of values and change scale [duplicate],"This question already has answers here : Range scaling problem (2 answers) Closed 11 years ago . I have a large array of variable-integer keypairs. The integer values range from -5 to 5. I'd like to scale that data to a range of 0 to 2. Logically, -5 would become 0, 0 would become 1 and 5 would become 2. How should I go about doing this on a large scale and with different input ranges?","['algebra-precalculus', 'algorithms']"
159277,Prove/disprove complex affine curves are isomorphic,"Define: $X_{1}:=\{(x,y,z) \in \mathbb{C}^{3}: x^{3}-y^{5}=0\}$ $X_{2}:=\{(x,y,z) \in \mathbb{C}^{3}: z=0,x=y^{2}\} \cup \{(x,y,z) \in \mathbb{C}^{3}: x=0,y=z^{2}\}$. $X_{3}:=\{(x,y,z) \in \mathbb{C}^{3}: z=0,x=y^{2}\} \cup \{(x,y,z) \in \mathbb{C}^{3}: z=0,y=x^{2}\}$. Question 1: Prove or disprove: $X_{3} \cong X_{2}$ as varieties. I think yes, can we simply let $Y=\{(x,y,z) \in \mathbb{C}^{3}: z=0,x=y^{2}\}$ and $W=\{(x,y,z) \in \mathbb{C}^{3}: x=0,y=z^{2}\}$ then map $X_{2} \rightarrow X_{3}$ as follows: if the point $(x,y,z) \in Y$ then let the map be the identity. Otherwise
send $(x,y,z)$ to $(z,y,x)$. Is this OK? Any easier way? Question 2: Prove or disprove: $X_{3} \cong X_{1}$. I think no. To argue we count the number of irreducible components. First note that that only irreducible component of $X_{1}$ is $X_{1}$ itself. Now I claim $X_{3}$ has two irreducible components, so it suffices to show that: $\{(x,y,z) \in \mathbb{C}^{3}: z=0,x=y^{2}\}$ and $\{(x,y,z) \in \mathbb{C}^{3}: z=0,y=x^{2}\}$ are both irreducible (clearly
they are closed). Well the first one is equal to $\{(y^{2},y,0): y \in \mathbb{C}\} \cong \mathbb{C}$ so irreducible. The second one is equal to $\{(x,x^{2},0): x \in \mathbb{C}\} \cong \mathbb{C}$ so irreducible. Therefore $X_{3}$ has two irreducible components so no such isomorphism exists. Is this OK?",['algebraic-geometry']
159284,Product rule for the derivative of a dot product.,"I can't find the reason for this simplification, I understand that the dot product of a vector with itself would give the magnitude of that squared, so that explains the v squared. What I don't understand is where did the 2 under the ""m"" come from. (The bold v's are vectors.) $$m\int \frac{d\mathbf{v}}{dt} \cdot \mathbf{v} dt = \frac{m}{2}\int \frac{d}{dt}(\mathbf{v}^2)dt$$ Thanks. Maybe the book's just wrong and that 2 should't be there...","['multivariable-calculus', 'vector-analysis']"
159297,Hypergeometric functions inequality,"Let $_2F_1(a,b;c,z)$ be the (Gauss) hypergeometric function, and $m$ and $n$ positive integers. From a simple plot it looks like $_2F_1(m+n,1,m+1,\frac{m}{m+n})>\frac{m}{n} \,_2F_1(m+n,1,n+1,\frac{n}{m+n})$ when $m<n$, but how do I prove this? Hope this is not too trivial, I am not very familiar with such functions.","['inequality', 'sequences-and-series', 'special-functions', 'functions', 'analysis']"
159300,Antiderivative simply connected region,"Why do analytic functions always have an antiderivative on a simply connected region? Thank you for your time, Chris",['complex-analysis']
159303,Harmonic Function which cannot be described as real part of a holomorphic function,"We define $f:\mathbb{C}\rightarrow\mathbb{C},\ f(z)=\log|z|$. $f$ is harmonic. Why can't we describe $f$ as a real part of a holomorphic (analytic) function? Thank you very much for your time, Chris",['complex-analysis']
159305,Automorphisms in unit disk,"Let $\mathbb{D}=\{|z|<1,\ z\in\mathbb{C}\}$. Are there any other automorphisms in $\mathbb{D}$ except the Blaschke factor $\displaystyle B_{a}(z)=\frac{z-a}{1-\overline{a}z},\ a\in\mathbb{D}$? I denote with $\overline{a}$ the complex conjugate of $a$. Thank you for your time, Chris",['complex-analysis']
159319,Offset Alternating Series,"I have the following alternating series that I would like to determine whether it is absolutely convergent, conditionally convergent, or divergent: $$  \sum\limits^{\infty}_{n=1} \frac{1+2(-1)^n}{n} $$ I have applied some tests and I find it reasonable to conclude that it is divergent. As a sum of two series:
$$  \sum\limits^{\infty}_{n=1} \frac{1}{n} + \sum\limits^{\infty}_{n=1} \frac{2(-1)^n}{n} $$ I believe a convergent series when added to a divergent series, results in a divergent series.  If this isn't a fact then I would still be left to say that it is inconclusive. Using the Alternating Series Test, with:
$$ a_n = \frac{1+2(-1)^n}{n} $$
although this isn't of 'proper form' $  \sum\limits^{\infty}_{n=1} (-1)^n a_n $ the limit of $a_n $ does approach zero as $ n \rightarrow \infty $.  As for monotonically decreasing, the limit of the ratio of absolute terms is divergent for $ n $ even and inconclusive for $ n $ odd, which has me concluding divergent by The Ratio Test as well as not monotonically decreasing, where: $$ \lim\limits_{n \rightarrow \infty} \left\lvert \frac{2(-1)^n + 1}{2(-1)^n - 1} \frac{n}{n+1} \right\rvert $$ Am I on the right track here?  Am I making any really improper assumptions?  Was there a better way to go about with the proof? Thanks!",['sequences-and-series']
159320,Intuition behind two functions being equal up to order $n$ at $a$,"Two functions $f$, $g: \mathbb R \to \mathbb R$ are said to be equal up to order $n$ at $a$ if and only if $$\lim_{x \to a} \frac{f(x) - g(x)}{(x - a)^n} = 0.$$ What is the intuition behind this definition? What is the idea behind considering two functions to be ""very equal"" versus ""not so equal""? The current picture in my head is this: two functions are ""somewhat equal"" if their graphs ""look like they overlap."" Given $a \in \mathbb R$, two functions are equal up to a ""high order"" if you have to zoom in ""a lot"" at $a$ in order to tell the two graphs apart; they are equal up to a ""low order"" if you can see two distinct graphs even without zooming in at $a$? I don't really know how I got this picture; I think it's somewhat correct, but I can't see how the picture relates to the limit definition.","['calculus', 'integration', 'derivatives', 'taylor-expansion']"
159331,The filtration of a mod l modular form,"I'm reading a paper of Swinnerton-Dyer, ""On l-adic representations for coefficients of modular forms."" He defines the notion of ""filtration"" for mod $\ell$ modular forms: If $\tilde{f} \in \tilde{M}$ (the mod $\ell$ modular forms) which is the sum of monomials in $M_k$ such that all $k$ are congruent modulo $\ell-1$, then the filtration of $\tilde{f}$, denoted $\omega(\tilde{f})$ is defined to be the least $k$ such that $\tilde{f} \in \tilde{M}_k$ (since the Eisenstein series $E_{\ell-1}$ reduces to $1$ modulo $l$, we can multiply by powers of it). This notion turns out to be very useful for classifying the exceptional primes of a Galois representation attached to a modular form. The first question I'd like to ask is, what's the motivation for it? Secondly, I'd like to ask about specific properties. If $f$ is a weight $k$ modular form, what can we say about $\omega(\tilde{f})?$ I am under the impression that the following are true -- am I right? $\omega(\tilde{f})$ must be congruent to $k$ modulo $\ell-1$ (an in particular, it is $k$ if $\ell > k$) $\omega(\tilde{f}) \leq k$. I'm confused because on p. 29, Swinnerton-Dyer says $\omega(\tilde{f}) = k$ if $\ell > 2k$...","['modular-forms', 'number-theory']"
159333,Understanding the Analytic Continuation of the Gamma Function,"So my book proves the convergence of $\Gamma(z) = \int_0^{\infty}t^{z-1}e^{-t}dt$ in the right half plane $Re(z) > 0$, and then goes on to prove the initial recurrence relation $\Gamma(z+1)=z\Gamma(z)$ by applying integration by parts to $\Gamma(z+1)$: $$\int_0^{\infty}t^{z}e^{-t}dt = -t^ze^{-t}|_0^{\infty} + z\int_0^{\infty}t^{z-1}e^{-t}dt$$ The book explicitly states this equality to be true only in the right half plane, since otherwise $-t^ze^{-t}|_0^{\infty} = \infty$, instead of equaling zero.  With this initial recurrence relation we are 'supposably' able to analytically continue the Gamma function to $Re(z) > -1$ (not including the origin) by writing the relation in the form: $$\Gamma(z) = \frac{\Gamma(z+1)}{z}$$ What I don't understand is this relation is still only true in the right half plane, since otherwise $-t^ze^{-t}|_0^{\infty}\neq 0$.  I don't see what reason we have to believe that, for instance, $\Gamma(-\frac{1}{2}) = \frac{\Gamma(\frac{1}{2})}{-\frac{1}{2}}$. Furthermore $\int_0^{\infty}t^{z-1}e^{-t}dt$ is clearly not convergent in the left half plane, so I can't even imagine why it would be plausible to think that a recurrence relation directly based on it could possibly lead to a genuine analytic continuation of its domain.","['gamma-function', 'complex-analysis']"
159346,Analog of Compact Operators,"This is kind of vague question, but I'll try to make it more precise. $T$ is a compact operator on a Hilbert Space, $H$, if $\overline{T(D)}$ is compact in $H$, where of course, $D$ is the closed unit ball in $H$. So we use the underlying Hilbert space in order to define these operators. Now $B(H)$ is a $C^*$-algebra. Can we abstractly define/characterize compact operators only in terms of $C^*$-notions, without appealing to the underlying Hilbert space? Stated otherwise, is it possible to define, say ""compact"" elements in a $C^*$-algebra $A$, which agree with the usual notion if we take $A=B(H)$?","['operator-theory', 'compact-operators', 'functional-analysis']"
159355,Measure on Baire space,"Inspired by the first parenthetical sentence of Joel's answer to this question , I have the following question: is there any useful notion of measurability in the Baire space $\omega^\omega$? Some initial thoughts: Any countably additive, translation-invariant measure has to give measure 0 to any set of the form $$\lbrace f: \sigma\prec f\rbrace.$$ The same holds if we ask for a finitely additive translation-invariant measure which gives finite measure to the whole space. There are several natural surjections $\omega^\omega\rightarrow 2^\omega$, and the latter space has a nice measure theory; so we could, for any such surjection $s$, define a ""measure"" given by $m_s(X)=m(s[X])$. However, it's unclear to me that any of these would be particularly interesting. Even short of a decent ""measure"" on $\omega^\omega$ (and indeed, I believe none exists), we might still be able to talk about subsets of $\omega^\omega$ being ""measurable"" or ""non-measurable"" in terms of what sorts of pathologies they admit. For example, call $X\subseteq\omega^\omega$ definably measurable if $L(\mathbb{R}, X)\models $ ""Every set is measurable."" This is, however, a terrible notion of measurability: there are measure zero (and hence measurable) sets $X\subseteq 2^\omega$ such that $L(\mathbb{R}, X)$ contains a non-measurable set, so this is too restrictive. Thoughts?","['set-theory', 'descriptive-set-theory', 'measure-theory', 'the-baire-space', 'analysis']"
159364,"Sequence in $C[0,1]$ with no convergent subsequence","I am trying to show that $C[0,1],$ the space of all real - valued continuous functions  with the sup metric is not sequentially compact with the sup metric by showing that the sequence $f_n = x^n$ has no convergent subsequence. The sup metric $\|\cdot\|$ is defined as $$\|f - g \| = \sup_{x \in [0,1]} |f(x) - g(x)|$$ where $|\cdot|$ is the ordinary Euclidean metric. Now I know that $f_n \rightarrow f$ pointwise, where $$f = \begin{cases} 0, & 0 \leq x < 1 \\ 1, & x = 1.\end{cases}$$ However $f \notin C[0,1]$ so this means by theorem 7.12 of Baby Rudin that $f_n$ cannot converge to $f$ uniformly. However how does this tell me that no subsequence of $f_n$ can converge to something in $C[0,1]$? Thanks.","['metric-spaces', 'real-analysis', 'analysis']"
159377,Generating function of Lah numbers,"Let $L(n,k)\!\in\!\mathbb{N}_0$ be the Lah numbers. We know that they  satisfy 
$$L(n,k)=L(n\!-\!1,k\!-\!1)+(n\!+\!k\!-\!1)L(n\!-\!1,k)$$ 
for all $n,k\!\in\!\mathbb{Z}$. How can I prove 
$$\sum_nL(n,k)\frac{x^n}{n!}=\frac{1}{k!}\Big(\frac{x}{1-x}\Big)^k$$ 
without using the explicit formula $L(n,k)\!=\!\frac{n!}{k!}\binom{n-1}{k-1}$? Attempt 1: $\text{LHS}=\sum_nL(n\!-\!1,k\!-\!1)\frac{x^n}{n!}+\sum_n(n\!+\!k\!-\!1)L(n\!-\!1,k)\frac{x^n}{n!}\overset{i.h.}{=}?$ Attempt 2: $\text{RHS}\overset{i.h.}{=}$ $\frac{1}{k}\frac{x}{1-x}\sum_nL(n,k\!-\!1)\frac{x^n}{n!}=$ $\frac{1}{k}\frac{x}{1-x}\sum_nL(n\!-\!1,k\!-\!1)\frac{x^{n-1}}{(n-1)!}=$ $\frac{1}{k(1-x)}\sum_nn\big(L(n,k)-(n\!+\!k\!-\!1)L(n\!-\!1,k)\big)\frac{x^n}{n!}=?$","['induction', 'generating-functions', 'combinatorics']"
159381,Evaluate $\int_0^\pi xf(\sin x)dx$,Let $f(\sin x)$ be a given function of $\sin x$. How would I show that $\int_0^\pi xf(\sin x)dx=\frac{1}{2}\pi\int_0^\pi f(\sin x)dx$?,"['definite-integrals', 'integration']"
159389,Locally closed irreducible subset of an affine scheme.,"I'm self-studying some Algebraic Geometry and I have the following question.
Let us take $X=\operatorname{Spec}A$ , where $A$ is a commutative ring. I am trying to show that every locally closed irreducible subset of $X$ contains an unique generic point. This is what I've been thinking so far: Let $Y$ be our irreducible, locally closed subset. We know that $q=I(Y) = \bigcap_{p \in Y} p$ is a prime ideal, I believe we would be done if we could show that $q \in Y$ . Let us write $Y = V(\mathfrak{a}) \cap U$ , for $U$ open. If $U$ would happen to be a principal open, of the form $D(f)$ , then $Y = V(\mathfrak{a}) \cap U$ could be written as $\operatorname{Spec}B$ where $B = A_f / \mathfrak{a}A_f$ . In this case, to say that Spec B is irreducible would simply mean that it had an unique minimal prime, which of course would have to lie in $\operatorname{Spec}B$ . I feel uneasy about this argument, and I'm not totally sure if it is correct. However, if it is done in this special case, how could it be used to show the general case?
I guess we could argue from our special case if we wrote $Y = V(\mathfrak{a}) \cap  (\bigcup_i D(f_i)) = \bigcup_i (V(\mathfrak{a}) \cap D(f_i))$ ,  but I'm not sure. So, I am very grateful for any help here, and answers on how to tackle this.","['affine-schemes', 'algebraic-geometry', 'schemes']"
159395,Can $G≅H$ and $G≇H$ in two different views?,"Can $G≅H$ and $G≇H$ in two different views? We have two isomorphic groups $G$ and $H$, so $G≅H$ as groups and suppose that they act on a same finite set, say $\Omega$. Can we see $G≇H$ as permutation groups . Honestly, I am intrested in this point in the following link. It is started by: Notice that different permutation groups may well be isomorphic as .... See here","['permutations', 'group-theory', 'abstract-algebra']"
159398,The series $ \sum\limits_{k=1}^{\infty} \frac1{\sqrt{{k}{(k^2+1)}}}$,Given the series $$\sum_{k=1}^{\infty} \frac1{\sqrt{{k}{(k^2+1)}}}$$ How can I calculate its exact limit (if that is possible)?,"['sequences-and-series', 'real-analysis']"
159404,Identifying a map by looking at the pair of topologies that makes it continuous.,"Let $\omega_X$ be the set of all topologies on $X$. Given $f:X\rightarrow X$, define $R_f \subset \omega_X \times \omega_X $ as those pairs of topologies on $X$ which make $f$ continuous. For example $\left(\text{Discrete Topology},-\right)$ or $\left(-,\text{Indiscrete Topology}\right)$ are always in $R_f$. But when $f$ can be uniquely determined, by its $R_f$? Here is one such case: $$ \forall x \in X: f(x)=x \iff R_f= \left\{ \left(T_\alpha,T_\beta\right)\subset \omega_X \times \omega_X | T_\beta \subset T_\alpha  \right\}$$ Can some one give me more elaborative examples of this please?",['general-topology']
159406,Estimating maximum value of random variable,"Suppose I have some random variable $X$ which only takes on values over some finite region of the real line, and I want to estimate the maximum value of this random variable. Obviously one crude method is to take many measurements, lets say $X_1$, $X_2$, $\ldots, X_n$ (which we'll say are all iid) and to use 
$$X_{max} = \text{max}(X_1, \ldots X_n)$$
as my guess, and as long as $n$ is large enough this should be good enough. However, $X_{max}$ is always less than the actual maximum, and I'm wondering if there's any way to modify $X_{max}$ so it gives a guess (still with some uncertainty) which is centred around the actual maximum value, rather than always a little less than it. Thanks",['probability']
159410,How much ought a first course in Linear Algebra emphasize proofs?,"I sometimes feel that proofs crowd out a coherent vision for linear algebra. 
However I also think a central theme of a Linear Algebra course is to learn reasoning even though it does not always succeed. The audience is first year undergraduate students studying mathematics and physics but maybe extended to engineers. They generally struggle with the idea of proof.","['education', 'linear-algebra', 'soft-question']"
159415,"How to show that $\mathrm{SL}(2,\mathbb Z) = \langle A, B\rangle$?","Show, that if $\mathbf{A}= \left( \begin{array}{cc} 1&1\\ 0&1 \end{array} \right)$, $\mathbf{B}= \left( \begin{array}{cc} 0&1\\ -1&0 \end{array} \right)$ and $\mathrm{SL}(2, \mathbb{Z}) := \{ \mathbf{C}\in\mathrm{M}(2\times 2;\mathbb{Z})\, |\,  \det(\mathbf{C}) = 1\}$ then $\langle\mathbf{A}, \mathbf{B}\rangle = \mathrm{SL}(2, \mathbb{Z})$. I found this exercise in a textbook for linear algebra in a chapter about the determinant, so it should be solved rather elementarily and without any deeper understanding of group theory ($\mathrm{SL}(2, \mathbb{Z})$ is defined only for the exercise). Showing $\langle\mathbf{A}, \mathbf{B}\rangle \subseteq \mathrm{SL}(2, \mathbb{Z})$ was easy but I got stuck with the opposite direction. Any help would be appreciated.","['linear-algebra', 'determinant']"
159420,"Construction of $[a,b]$-fold Cartesian product over space of all real-valued functions","I am currently reading ""Applied Analysis"", which could be found here , and on page 85 I don't understand example 4.16. There it is said: Suppose that $X$ is the space of all real-valued functions on the interval $[a,b]$. We may identify a function $f: [a,b] \to \mathbb{R}$ with a point $\prod_{x\in [a,b]} f(x)$ in $\mathbb{R}^{[a,b]}$, so $X = \mathbb{R}^{[a,b]}$ is the $[a,b]$-fold Cartesian product of $\mathbb{R}$. In this construction the expression $\prod_{x\in [a,b]} f(x)$ should almost always yield $\infty$ because its a product over an uncountable set and so this identification is not one-to-one, or does i read $\prod_{x\in [a,b]} f(x)$ wrong?","['general-topology', 'analysis']"
159433,"Permutations: Given $P^4$, how many $P^1$s are possible?","Let $P^0$ be the identity tuple $(1,2,...,N)$ Let $P^{i+1}$ be the tuple after a permutation $P$ is applied to $P^i$. For example, if $P$ is $(2,1,3,6,4,5)$ than: $$\begin{align}
P^0 &= (1,2,3,4,5,6) \\
P^1 &= (2,1,3,6,4,5) \\
P^2 &= (1,2,3,5,6,4) \\
P^3 &= (2,1,3,4,5,6) \\
\dots
\end{align}$$ Given the value $P^4$, how many possible values of $P$ are there?","['permutations', 'abstract-algebra', 'combinatorics']"
159438,Prove the convergence/divergence of $\sum \limits_{k=1}^{\infty} \frac{\tan(k)}{k}$,Can be easily proved that the following series onverges/diverges? $$\sum_{k=1}^{\infty} \frac{\tan(k)}{k}$$ I'd really appreciate your support on this problem. I'm looking for some easy proof here. Thanks.,"['sequences-and-series', 'real-analysis']"
159444,show if function is even or odd,"Suppose that we have equation:
$$f(x)=\frac{2^x+1}{2^x-1}$$ There is question if this function even or odd? I know definitions of even and odd functions, namely
even is if $f(-x)=f(x)$  and odd is if $f(-x)=-f(x)$ and when I put $-$ sign in function, found that this is neither even nor odd function, because $2^{-x}\ne-1 \times 2^x$, but my  book says that  it is even, so am I wrong? Please help me to clarify book is correct or me? Thanks",['functions']
159446,Why isn't there a continuously differentiable injection into a lower dimensional space?,"How to show that a continuously differentiable function $f:\mathbb{R}^{n}\to \mathbb{R}^m$ can't be a 1-1 when $n>m$? This is an exercise in Spivak's ""Calculus on manifolds"". I can solve the problem in the case $m=1$. To see this, note that the result is obvious if the first partial derivative $D_1 f(x)=0$ for all $x$ as then $f$ will be independent of the first variable. Otherwise there exists $a\in \mathbb{R}^n$ s.t. $D_1 f(a)\not=0$. Put $g:A\to \mathbb{R}^n, g(x)=(f(x),x_2, \ldots, x_n)$ (with $x=(x_1,\ldots, x_n)$). Now the Jacobian 
$$ 
g'(x) = \left[
          \begin{array}{cc}
             D_1 f(x) & 0 \\
             0        & I_{n-1}     
          \end{array}
        \right]
$$ 
so that $\text{det}\, g'(a)=D_1 f(a)\not=0$. By the Inverse Function Theorem we have an open set $B\subseteq A$ s.t. $g:B\to g(B)$ is bijective with a differentiable inverse. In particular, $g(B)$ is open. Pick any $g(b)=(f(b),b_2,\ldots, b_n)\in g(B)$. Since $g(B)$ is open there exists $\varepsilon > 0$ such that  $(f(b), b_2, \ldots, b_n+\varepsilon) \in g(B)$. Thus we can find $b'\in B$ s.t. $g(b')=(f(b), b_2, \ldots, b_n+\varepsilon)$. By the definition of $g$, $f(b')=f(b)$ with $b'$ and $b$ differing in the last coordinate. Thus $f$ isn't injective. However, I cannot generalize this argument to higher dimenssions.","['multivariable-calculus', 'analysis']"
159449,"If $\sum\limits_{k=1}^{\infty}a_k=S $, then $ a_4+a_3+a_2+a_1+a_8+a_7+a_6+a_5+\dots=?$","if we know that $\sum\limits_{k=1}^{\infty}a_k=S$,  what can we say about the convergence of $$a_4+a_3+a_2+a_1+a_8+a_7+a_6+a_5+a_{12}+a_{11}+a_{10}+a_{9}+\dots$$ ? If it does converges, what is the sum (in terms of $S$)? As per the first question -  it clearly converges since the number of terms in each parentheses is bounded (by 4) and the $(a_n)_{n=1}^\infty$ tends to zero as $n\to\infty$. Second question is where I'm struggling. We don't know that $\sum\limits_{k=1}^{\infty}a_k$ absolutely converges so I don't know what can we say about it's sum. Thanks for your help.","['sequences-and-series', 'calculus']"
159473,Is there an epsilon-delta definition of the second derivative?,"Is there an epsilon-delta definition for the second derivative? I know that there is such a definition for the first derivate $f'(x)$ which can be derived from the limit $f'(x) = \lim_{y\rightarrow x} \frac{f(y)-f(x)}{y-x}$ for a function $f:D\rightarrow \mathbb{R}$: $$\forall \epsilon > 0\, \exists \delta > 0\, \forall y \in D\setminus \{x\}:|y-x|<\delta \Rightarrow \left|\frac{f(y)-f(x)}{y-x}-f'(x)\right|<\epsilon$$ So $f'(x)$ can be described as the number which fulfills the above statement. Is there a similar statement for the second derivative? Update: This MSE thread shows that there are different definitions for the derivative (and thus for the second derivative). So I want to make my question more concrete: My definition of derivation: Let be $f:D\rightarrow\mathbb{R}$ with $D\subseteq\mathbb{R}$ arbitrary . Let $D^*$ be the set off all points $x\in D$ for which there is at least one sequence $(x_n)$ in $D\setminus\{x\}$ with $\lim_{n\rightarrow\infty} x_n=x$. I define the limit $\lim_{y\rightarrow x\ ,y\in D\setminus\{x\}} {f(y)-f(x) \over y-x}$ as the first derivation for a given $x\in D^*$ (if the limit exists). My definition of the second derivative: Let be $f:D\rightarrow\mathbb{R}$ with $D\subseteq\mathbb{R}$ arbitrary. We call $f''(x)$ the second derivative if there exists an open interval $x\in O\subseteq \mathbb{R}$ so that $f$ is differentiable on $O\cap D$ and $f''(x)$ is the first derivative of the function $f': (O\cap D)\rightarrow\mathbb{R}:x\mapsto f'(x)$ at the point $x$ (which also means that $x\in(O\cap D)^*$). My question: Is there a statement $\forall \epsilon > 0: \exists \delta > 0: A(\epsilon, \delta, f, x, c)$ for $f:D\rightarrow \mathbb{R}$ ($D\subseteq \mathbb{R}$) and $c,x\in\mathbb{R}$ which is equivalent to the statement that $f$ is differentiable on a set $x\in O\cap D$ where $O$ is an open interval and that $c$ is the second derivative of $f$ at $x$? I also will accept answers where you need more restrictions to the question. For example you might want to use the value of the first derivative $f'(x)$ (at the same point where you want to define the second derivative) in your statement or you want to restrict $f$ on functions with open domains or domains which are intervals. In this case I will accept your answer and open a new thread asking for a more general solution. Please notice that there is a community wiki post where I want to collect all the progress we made so far.",['calculus']
159477,Determine convergence of $\sum_{n=1}^\infty\frac{\sin(na)}{n^2} $,"I want to detemine the convergence of the next series: $$\sum_{n=1}^\infty\frac{\sin(na)}{n^2} $$ I've solved the limit:
$$\lim_{n->\infty}\frac{\sin(na)}{n^2}=\frac{[ -1,1]}{\infty}=0$$
The series has the necesary condiction of convergence (limit=0), but I dont go further from here","['sequences-and-series', 'calculus']"
159487,Proving the Möbius formula for cyclotomic polynomials,"We want to prove that 
$$ \Phi_n(x) = \prod_{d|n} \left( x^{\frac{n}{d}} - 1 \right)^{\mu(d)} $$
where $\Phi_n(x)$ in the n-th cyclotomic polynomial and $\mu(d)$ is the Möbius function defined on the natural numbers. We were instructed to do it by the following stages: Using induction we assume that the formula is true for $n$ and we want to prove it for $m = n p^k$ where $p$ is a prime number such that $p\not{|}n$. a) Prove that $$\prod_{\xi \in C_{p^k}}\xi = (-1)^{\phi(p^k)} $$ where $C_{p^k}$ is the set of all primitive $p^k$-th roots of unity, and $\phi$ is the Euler function. I proved that. b) Using the induction hypothesis show that 
$$ \Phi_m(x) = (-1)^{\phi(p^k)}  \prod_{d|n} \left[ \prod_{\xi \in C_{p^k}} \left( (\xi^{-1}x)^{\frac{n}{d}} - 1 \right) \right]^{\mu(d)} $$ c) Show that
$$ \prod_{\xi \in C_{p^k}} \left( (\xi^{-1}x)^{\frac{n}{d}} - 1 \right) = (-1)^{\phi(p^k)}   \frac{x^{\frac{m}{d}}-1}{x^{\frac{m}{pd}} - 1} $$ d) Use these results to prove the formula by substituting c) into b). I am stuck in b) and c). In b) I tried to use the recursion formula $$ x^m - 1 = \prod_{d|m}\Phi_d(x) $$ and 
$$ \Phi_m(x) = \frac{x^m-1}{ \prod_{\stackrel{d|m}{d<m}} \Phi_d(x)} . $$ In c) I tried expanding the product by Newton's binom using $\phi(p^k) = p^k ( 1 - 1/p)$. I also tried replacing the product by $\xi \mapsto [ \exp(i2\pi / p^k) ]^j$ and let $j$ run on numbers that don't divide $p^k$. In both way I got stuck. I would appreciate help here.","['galois-theory', 'polynomials', 'number-theory']"
159493,Can there be a function that's even and odd at the same time?,I woke up this morning and had this question in mind. Just curious if such function can exist.,"['even-and-odd-functions', 'algebra-precalculus', 'functions']"
159499,The Duality Functor in Linear Algebra,"I'm trying to gain an intuitive understanding of the following construction: For any vector space $M$ over a field $R$, one can define the algebraic dual of $M$ as $M^* := \mathsf{Hom}(M, R)$ and given another vector space $N$ one can define the algebraic dual of a linear map $A \in \mathsf{Hom}(M,N)$ according to $A^*(\omega) = \omega \circ A$. This establishes a linear mapping from $N^*$ to $M^*$ and since for suitable linear maps $A$ and $B$, $(A \circ B)^* = B^* \circ A^*$ and $(1_A)^* = 1_{A^*}$ the ""duality"" operation $*$ sets up an endomorphic (contravariant) functor on the category of vector spaces. I understand the basics of this construction, but what I would like to see are some good concrete examples that would illustrate the abstractions in a meaningful way. It's easy to come up with specific examples of linear maps and dual spaces where one applies this to specific linear forms, but are there some examples that would shed light on the motivation for these definitions and relations? Ironically, I think I have a better understanding of this in purely categorical terms since the duality construct shows up repeatedly in different contexts and when I think of ""duality"" I think of precisely this construct...","['category-theory', 'linear-algebra', 'examples-counterexamples', 'intuition']"
159501,What is the simplest proof that the mutual information $I(X:Y)$ is always non-negative?,"What is the simplest proof that mutual information is always non-negative? i.e., $I(X;Y)\ge0$","['probability-theory', 'information-theory', 'mutual-information', 'entropy']"
159521,Maximizing the number of points covered by a circular disk of fixed radius.,"Given a set of points in two dimensional space, and a radius r, what is the algorithm to find a disk of radius r that covers the maximum number of points?","['optimization', 'geometry', 'computational-geometry', 'algorithms']"
159546,calculate generally the determinant of $A = a_{ij} = \begin{cases}a & i \neq j \\ 1 & i=j \end{cases}$,"calculate generally the determinant of $A = a_{ij} = \begin{cases}a & i \neq j \\ 1 & i=j \end{cases} = \begin{pmatrix}
1 & a & a & · & a \\
· & · & · & · \\
a & a & a & · & 1 \\
\end{pmatrix}$ Any hints?","['matrices', 'determinant']"
159551,ODE theory. Need someone to jog my memory,"I have two questions regarding the same thing. Let's say I have a homogeneous ODE (is that what they are called?) $ay'' + by' + cy = 0$ The trick in this problem is to multiply both sides by $e^{rt}$ and do some trick to get the homogeneous solution which usually includes strictly exponential or sometimes a mixed with trigonometric functions. This is what most book does, but now that I had forgotten most of this stuff, could someone remind me why we ignore the trivial solution $y = 0$? Let's say the ODE $ay'' + by' + cy = 0$ has the homogeneous solution $y = Ae^{-r_1t} + Be^{r_2t}$ (Let $r_1, r_2 > 0$ and are roots of the characteristic equation). If it happens that $\lim_{x\to \infty} y = 1$. Why can't there be a solution? Is there a theorem that says this? I can't remember it out for the life of me. Thank you for reading",['ordinary-differential-equations']
159561,Completion of regular local rings,"Let $K$ be a complete field with respect to a discrete valuation $v$ and let $O_K$ be its valuation ring, $m$ its maximal ideal. Suppose $K$ has characteristic $0$ and that $O_K/m$ is of characteristic $p$. Let $A$ be an $O_K$ algebra and suppose that $A$ is a complete regular local ring of dimension $d+1$. In general, I know that there exists a complicated result (Cohen Structure Theorem, unequal characteristic case) that asserts that $A$ is more or less a homomorphic image of a ring of power series in $d$ variables over a suitable ring of Witt vectors. My question is: assume that $A\otimes_{O_K} K = K[[X_1, \ldots, X_d]]$ and that the square of the maximal ideal $m_A$ of $A$ does not contain the uniformizer $\pi$ of $O_K$ (in Cohen's language: A is unramified). Can we deduce in this case that $A=O_K[[X_1, \ldots, X_d]]$ (possibly without Cohen's theorem)?","['commutative-algebra', 'algebraic-geometry', 'abstract-algebra']"
159571,Commutator subgroup of a subgroup,"Suppose $H \subset G$ is a subgroup of finite index (assume normal if necessary). Must it be the case that $[H, H] \subset [G,G]$ is of finite index? (i.e. the map $H^{ab} \rightarrow G^{ab}$ is of finite kernel)",['group-theory']
159579,"How to divide by $(a_1,a_2,a_3)$","I have been searching for an explanation in Howard's Linear Algebra and couldn't find an identical example to the one below. The example tells me that vectors $\boldsymbol{a}_1$, $\boldsymbol{a}_2$ and $\boldsymbol{a}_3$ are: $$\boldsymbol a_1 = (a,0,0)$$
$$\boldsymbol a_2 = (0,a,0)$$
$$\boldsymbol a_3 = (0,0,a)$$ And I have to calculate $\boldsymbol b_1$ using equation: $$\boldsymbol{b}_1 = \frac{2 \pi \, (\boldsymbol a_2 \times \boldsymbol a_3)}{(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)}$$ So far I've only managed to calculate the cross product $(\boldsymbol a_2 \times \boldsymbol a_3)$ using Sarrus' rule and what I get is: $$\boldsymbol{b}_1 = \frac{2 \pi \, \hat{\boldsymbol{i}} \, a^2}{(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)}$$ But now I am stuck as I don't know how to calculate with a $(\boldsymbol{a}_1, \boldsymbol{a}_2, \boldsymbol{a}_3)$, as this is first time I've come across something like this. Could you just point me to what to do next, or point me to a good html site as I still want to calculate this myself. Best regards.","['cross-product', 'linear-algebra']"
159614,"Difference between $\left< x\right> \cap \left< x,y\right>^2$ and $\left< x,y\right>^3$","Consider the ideals $I = \left< x\right> \cap \left< x,y\right>^2 = \left<x^3,x^2y, xy^2\right>$ and $J=\left< x,y\right>^3=\left< x^3, x^2y, xy^2, y^3
\right>$ in $k[x,y]$. What is the geometric difference between $I$ and $J$? I know that the zero set for $J$ is a triple point at the origin on $k^2$ while the zero set for $I$ is the $y$-axis together with a double point at the origin on $k^2$. But aren't they both colength $3$ ideals in $k[x,y]$?","['commutative-algebra', 'ring-theory', 'algebraic-geometry']"
159615,Sum of Sines Interval [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? How is it possible to show for integer $m$: $$\frac{1}{M}\sum_{k=1}^{M}\sin(m\cdot y_{k})=0$$ Thank you very much Interval $[-\pi,\pi]$ split into $M$ equal intervals, with the mid point of interval is $y_{k}$",['trigonometry']
159618,Sum Cosine Mod?,"interval $-\pi:\pi$ split into M equal intervals. midpoints are $y_K$ but i dont understand how to show $$
\frac{1}{M}\sum_{j=1}^{M}\cos(mx_{j})=\begin{cases} 1, & \ m \equiv 0\pmod{M}\\ 0, & \text{else} \end{cases}$$ thank you very much","['trigonometry', 'sequences-and-series']"
159659,Which sets are removable for holomorphic functions?,"Let $\Omega$ be a domain in $\mathbb C$, and let $\mathscr X$ be some class of functions from $\Omega$ to $\mathbb C$. A set $E\subset \Omega$ is called removable for holomorphic functions of class $\mathscr X$ if the following holds: every function $f\in\mathscr X$ that is holomorphic on $\Omega\setminus E$ is actually holomorphic on $\Omega$, possibly, after being redefined on $E$. (An example of the above: $E$ is a line interval, $\mathscr X$ consists of continuous functions. In this case $E$ is removable, which is shown in the answer.) It is clear that the larger $\mathscr X$ is, the smaller is the class of removable sets. In the extreme case, if $\mathscr X$ contains all functions $\Omega\to\mathbb C$, there are no nonempty removable sets. Indeed, if $a\in E$, then $f(z)=\frac{1}{z-a}$ (arbitrarily defined at $z=a$) is holomorphic on $\Omega\setminus E$ but has no holomorphic extension to $\Omega$. The problem of describing removable sets is nontrivial in many classes $\mathscr X$ such as $L^{\infty}(\Omega)$, bounded functions $C(\Omega)$, continuous functions $C^{\alpha}(\Omega)$, Hölder continuous functions $\mathrm{Lip}(\Omega)$, Lipschitz functions Which sets are removable for holomorphic functions in these classes?","['dimension-theory-analysis', 'geometric-measure-theory', 'complex-analysis']"
159676,Basis functions for a Schauder-Faber-Basis,"I am working through two proofs that there exists a Schauder Basis for $C([0,1])$ . One proof defines a basis $(f_n)_{n=0}^{\infty}$ with $$
 f_0(x) = 1 \qquad f_1(x) = x
$$ for $2^{k-1} < n \le 2^{k}$ , where $k \ge 1$ , we define $$
 f_n(x) = \left\{ \begin{array}{ll}
                    2^k ( x - (2^{-k}(2n - 2) - 1)) & \mathrm{if} ~ x \in I_n \\
                    1 - 2^k ( x - (2^{-k}(2n - 1) - 1)) & \mathrm{if} ~ x \in J_n \\
                    0 & \mathrm{otherwise}
                  \end{array} \right.
$$ where $$
 I_n = [2^{-k}(2n-2), 2^{-k}(2n-1)) \qquad
 J_n = [2^{-k}(2n-1), 2^{-k}2n).
$$ The graphs of these functions form a sequence of ""tents"" of height one and width $2^{-k+1}$ that sweep across the interval $[0,1]$ . This proof is from this notes page 94. Another proof I found on the internet goes like this, define the ""triangle function"" $$
 \Delta(x) = \begin{cases}
                2x & \mathrm{if } \hspace{2mm} x \in \left[0, \frac{1}{2}\right] \\
\\
                2(1-x)  & \mathrm{if } \hspace{2mm} x \in \left(\frac{1}{2},1\right] \\
\\
                0 & \mathrm{otherwise}.
             \end{cases}$$ Then consider for $n > 0$ $$
 \Delta_n(x) = \Delta(2^j x - k) \quad \mathrm{for } \qquad n = 2^j + k, \quad j \ge 0,\quad 0 \le k < 2^j
$$ and $\Delta_{-1}(x) = 1, \Delta_0(x) = x$ . Then the sequence $\Delta_{-1}, \Delta_0, \Delta_1, \Delta_2, \ldots$ forms a Schauder basis for the Banach space of continuous functions on $[0,1]$ . I found this proof here and here .
Now here's my question. I tried to prove that both, the $\Delta_n$ and the $f_n$ 's, define the same functions, but I am not able to convert the definitions to each other. Do you know how can I proof that these two functions are essentially the same ""tent""-functions?","['functional-analysis', 'analysis']"
159679,Indices in differential geometry,"Often times in differential geometry it is convenient to use Einstein summation notation, and there it is presented to beginning graduates and advanced undergraduates alike that if you see two indices that are the same letter with one upper and the other lower, written next to each other, then there is an implied summation symbol. No seriously mathematically rigorous notion can depend on the particular way in which we choose to communicate, and so even though you could rip apart the Einstein summation convention, asking all sorts of fringe case questions of the type ""does Einstein summation notation apply in this case"" it is generally understood enough that it is just accepted as a way to communicate, no more or less treacherous than the fact that we write symbols on paper. (Or that in general we do not write out all the quantifiers or use logical symbols in prose.)  It is in the same light that I ask the following questions, but I hope that any answerers will also alert me if there is rigorous content in any of this: Sometimes people seem to derive more meaning than just a bookkeeping device for Einstein Summation out of whether an index is written in a particular position.  I don't understand what people are trying to communicate, but I can reconstruct enough of it that you can hopefully help me: Sometimes writing upper indices indicates something of a ""dual"" nature.  For instance, Einstein summation is often used when we contract a dual vector applied to a vector. Sometimes writing something upper indicates inversion.  For instance, $g^{ij}$ is the $ij_{th}$ entry of the inverse matrix $(g_{ij})$ which is the matrix of the Riemannian metric from a particular coordinate basis to itself. Sometimes the left to right order of the indices on a complicated tensor matters. I believe I've seen indices written to the left of a mathematical symbol. My questions are: a. Is there any system to what the location of an index means?  Or is it just special case by special case?  For instance, is 2. an instance of a more general rule, or is it that inversion is only meant in the case of $g_{ij}$.  2. and 1. seem to be in conflict?  Or maybe neither 1. nor 2. hold, there is no meaning, and it just happens that for both the case of inversion and transpose, it is convenient to simply write upper indices so that Einstein summation is in effect? b. Which of these imply more significance than simply a bookkeeping device for Einstein summation notation, among points 1 and 2. above? c. In points 3. and 4. I am not even aware of what the ""face-value"" meaning is, as in I would not know how to perform a computation with these things.  For instance, is a left-side index supposed to mean transpose or something? Feel free to simply connect me to a resource, but the most pedestrian searches on my part haven't revealed anything authoritative.  I was unable to rigorously understand introductory differential geometry for most of my undergrad because few people stopped to explain their notation, so I hope that in addition to helping me right now, this might become useful to someone else struggling in the same way in the future.",['differential-geometry']
159700,Complex analysis exercises,"These two questions are driving me mad as I need to help my daughter but I can't remember all this stuff. $\,(1)\,\,$ Let $\,p(z)\,,\,q(z)\,$ be two non-constant complex polynomials of the same degree s.t. $$\text{whenever |z|=1}\,\,,\,|p(z)|=|q(z)|$$ If all the zeros of both $\,p(z)\,,\,q(z)\,$ are within the open unit disk $\,|z|<1\,$ , prove that $$\forall z\in\mathbb{C}\,\,,\,q(z)=\lambda\, p(z)\,,\,\lambda\in\mathbb{C}\,\,\text{a constant}$$ What've I thought : since the polynomials are of the same degree, I know that $$\lim_{|z|\to\infty}\frac{q(z)}{p(z)}$$ exists finitely, so we can bound $\,\displaystyle{\frac{q(z)}{p(z)}}\,$ say in $\,|z|>1\,$ . Unfortunately, I can't use Liouville's Theorem to get an overall bound as the rational function is not entire within the unit disk... $\,(2)\,\,$ Let $\,f(z)\,$ be analytic in the punctured disk $\,\{z\in\mathbb{C}\;|\;0<|z-a|<r\,,\,\text{for some}\,\,0<r\in\mathbb R\}\,$. Prove that if $\,\displaystyle{\lim_{z\to a}f'(z)}\,$ exists and finite, then $\,a\,$ is a removable singularity of $\,f(z)\,$ . My thoughts: We have a Laurent expansion in the above disk$$f(z)=\ldots +\frac{a_{-n}}{(x-a)^n}+\ldots +\frac{a_{-1}}{z-a}+a_0+a_1(z-a)+\ldots$$ so taking the derivative term-term (is there any special condition that must be fulfilled in this particular case to do so?) we get$$f'(z)=\ldots -\frac{na_{-n}}{(z-a)^{n+1}}-\ldots -\frac{a_{-1}}{(z-a)^2}+a_1+2a_2(z-a)+\ldots$$Now, as the limit  of the above when $\,z\to a\,$ exists finitely, it must be that all the terms with negative power of $\,z-a\,$ vanish, thus $$\ldots =\,a_{-n}=a_{-n+1}=\ldots =a_{-1}=0$$ and we get that the above Laurent series for $\,f(z)\,$ is, in fact, a Taylor one and, thus, the function's limit (not the derivative's!) exists and finite when $\,z\to a\,$ and $\,a\,$ is then a removable singularity. Any help in (1) if I got right (2), or in both if there's some problem with the latter will be much appreciated.",['complex-analysis']
159701,Converge or Diverge? Show that $\sum_{n=1}^{\infty} \frac{1}{n^{{n}/{\log(n)}}}$ converges,"The series $$\sum_{n=1}^{\infty} \frac{1}{n^{{n}/{\log(n)}}}$$ converges according to Wolframalpha. Now I am not sure what the best technique is handling this one. I am thinking about a comparison test. Here is what I thought $n \geq 1 \iff \log(n) \geq 1 \iff \dfrac{n}{\log(n)} \geq 1 \iff n^{\dfrac{n}{\log(n)}} \geq 1 \iff 0 \leq \frac{1}{n^{\frac{n}{\log(n)}}} \leq 1 \iff \sum_{n=1}^{\infty} 0 \leq \sum_{n=1}^{\infty} \frac{1}{n^{\frac{n}{\log(n)}}} \leq \sum_{n=1}^{\infty} 1 $ So by the Comparison Test, it converges. Or I guess i ""sqqqqququuuuzed"" the sum =) Now my concern is that my sum is bounded, but I guess that doesn't imply the sum exist because something like $\sin(n)$ diverges even though it is bounded. Any insights? EDIT: $\log(n)$ isn't the natural log",['sequences-and-series']
159707,Is there any geometric way to characterize $e$?,"Let me explain it better: after this question, I've been looking for a way to put famous constants in the real line in a geometrical way -- just for fun. Putting $\sqrt2$ is really easy: constructing a $45^\circ$ - $90^\circ$ - $45^\circ$ triangle with unitary sides will make me have an idea of what $\sqrt2$ is. Extending this to $\sqrt5$ , $\sqrt{13}$ , and other algebraic numbers is easy using Trigonometry; however, it turned difficult working with some transcendental constants. Constructing $\pi$ is easy using circumferences; but I couldn't figure out how I should work with $e$ . Looking at made me realize that $e$ is the point $\omega$ such that $\displaystyle\int_1^{\omega}\frac{1}{x}dx = 1$ . However, I don't have any other ideas. And I keep asking myself: Is there any way to ""see"" $e$ geometrically? And more: is it true that one can build any real number geometrically? Any help will be appreciated. Thanks.","['algebra-precalculus', 'real-analysis']"
