question_id,title,body,tags
2908265,Why does a flat finite type morphism of irreducible noetherian schemes map generic pt to generic pt,"Basically the title, I came across this statement reading some notes on dimension of fibers of flat maps. (I don't know if Noetherian actually matters for this question) I think that for maps of irreducible schemes, sending generic point to generic point is the same as $f: X \to Y$ being dominant $\leftrightarrow$ the associated ring map (reducing to the case where everything is affine) is injective, so why should the finitely generated $A$-alg $B$, $f^{\sharp}: A \to B$ flat, imply $f$ is injective?","['algebraic-geometry', 'schemes', 'flatness', 'commutative-algebra']"
2908295,"If $f(x+y)=f(x)+f(y) ,\forall\;x,y\in\Bbb{R}$, then if $f$ is continuous at $0$, then it is continuous on $\Bbb{R}.$ [duplicate]","This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 5 years ago . I know that this question has been asked here before but I want to use a different approach. Here is the question. A function $f:\Bbb{R}\to\Bbb{R}$ is such that 
\begin{align} f(x+y)=f(x)+f(y) ,\;\;\forall\;x,y\in\Bbb{R}\qquad\qquad\qquad(1)\end{align}
I want to show that if $f$ is continuous at $0$, it is continuous on $\Bbb{R}.$ MY WORK Since $(1)$ holds for all $x\in \Bbb{R},$ we let \begin{align} x=x-y+y\end{align}
Then, 
\begin{align} f(x-y+y)=f(x-y)+f(y)\end{align}
\begin{align} f(x-y)=f(x)-f(y)\end{align}
Let $x_0\in \Bbb{R}, \;\epsilon>$ and $y=x-x_0,\;\;\forall\,x\in\Bbb{R}.$ Then,
 \begin{align} f(x-(x-x_0))=f(x)-f(x-x_0)\end{align}
 \begin{align} f(x_0)=f(x)-f(x-x_0)\end{align}
 \begin{align} f(y)=f(x_0)-f(x)\end{align} HINTS BY MY PDF: Let $x_0\in \Bbb{R}, \;\epsilon>$ and $y=x-x_0,\;\;\forall\,x\in\Bbb{R}.$ Then, show that  \begin{align} \left|f(x_0)-f(x)\right|=\left|f(y)-f(0)\right|\end{align}
Using this equation and the continuity of $f$ at $0$, establish properly that
 \begin{align}\left|f(y)-f(0)\right|<\epsilon,\end{align}
in some neighbourhood of $0$. My problem is how to put this hint together to complete the proof. Please, I need assistance, thanks!","['continuity', 'functions', 'analysis', 'real-analysis']"
2908296,Obtaining the polar form of the planar system?,"Consider the two variables $x_{1},x_{2}$ which are functions of time $t$, we refer $\dot{x_{1}}$ as the time derivative of $x_{1}$. Let $\alpha$ be a constant parameter, and consider the following system of equations: \begin{align*}
\dot{x_{1}} &= \alpha x_{1} - x_{2} - x_{1}(x_{1}^2 + x_{2}^2)\\
\dot{x_{2}} &= x_{1} + \alpha x_{2} - x_{2}(x_{1}^2 + x_{2}^2).
\end{align*} Now we transform the above system to polar $(\rho,\theta)$ coordinates. I think after the transformation the above equations lead to the following: \begin{align*}
\dot{\rho} &= \rho(\alpha - \rho^2)\\
\dot{\theta} &= 1.
\end{align*} Where $x_{1} = \rho \cos(\theta)$ and $x_{2} = \rho \sin(\theta)$. If I substitute then I am thinking about the partial derivatives involved and I think which variable I should differentiate with respect to the time?
I can observe that the $x_{1}^2 +x_{2}^2 = 1$ which could simplify a lot but how should I deal with $\dot{x_{1}}, \dot{x_{2}}$ which can help me in getting the polar form of the above system?","['polar-coordinates', 'transformation', 'ordinary-differential-equations', 'dynamical-systems']"
2908313,"If we say ""classes of non-zero integers modulo $n$"", why does this not include the $0$ class?","I suppose this is a bit of a wording question more than anything else - I'm working through group theory and was learning that ""the (classes of) non-zero integers modulo $p$ form an Abelian group under multiplication."" It's the wording of this that gets me a bit confused. Let's say $p = 3$. From my understanding the group described is meant to contain $\{[1],[2]\}$. I completely get why this would be an Abelian group under multiplication. However, if we're looking at all the non-zero integers modulo $3$, it seems to me like the ""non-zero"" attribute binds to the integer part only. So it's $\mathbb{Z} - \{0\}$ (which would include $3,-3,6,-6,...$) modulo $3$. Hence, since integer multiples of 3 are included in this set, $[0]$ would also be included. But clearly my interpretation can't be the case since including $[0]$ would mean it's not a multiplicative group. In summary, it's like I'm having trouble with the order of operations here: Interpretation 1: (set of non-zero integers) modulo $p$ Interpretation 2: set of non-zero (integers modulo $p$) So am I supposed to interpret it like interpretation 2? Since it seems like interpretation 1 would include $[0]$.","['elementary-number-theory', 'group-theory', 'modular-arithmetic']"
2908314,Solve for $\theta$: $x = \theta - \sin\theta$ [duplicate],"This question already has answers here : How to solve Kepler's equation $M=E-\varepsilon \sin E$ for $E$? (5 answers) Closed 5 years ago . Solve for $\theta$: $$x = \theta - \sin\theta$$ Is this type of isolation a matter of identities? If so, which one(s)?",['trigonometry']
2908317,Limit of sum of sequences at infinity,"Given two sequences $$a_n=\int_0^1 (1-x^2)^n dx$$ and $$b_n=\int_0^1 (1-x^3)^n dx$$ ,($n\in N$) then find the value of $$L=\lim_{n\to \infty} (10 \sqrt [n]{a_n} +5\sqrt [n]{b_n})$$ My try: $$a_n=\int_0^1 (1-x^2)^n dx= \frac 12 B\left( n+1, 1/2\right)=\frac 12\cdot \frac {\Gamma(n+1)\Gamma(1/2)}{\Gamma\left(n +\frac 32\right)}$$ Similarly $$b_n=\frac 13\cdot \frac {\Gamma(n+1)\Gamma(1/3)}{\Gamma\left(n +\frac 43\right)}$$ Now let's consider $$J=\lim_{n\to \infty} (b_n)^{1/n} =\lim_{n\to \infty} \left(\frac 13\right)^{1/n}\cdot \left(\frac {\Gamma(n+1)\Gamma(1/3)}{\Gamma\left(n +\frac 43\right)}\right) ^{1/n}$$ $$=\lim_{n\to\infty} \left(\frac 13\right)^{1/n}\cdot \left(\lim_{n\to \infty} \left(\frac {\Gamma(n+1)\Gamma(1/3)}{\Gamma\left(n +\frac 43\right)}\right) ^{1/n}\right)$$ $$=1\cdot \lim_{n\to  \infty} \left(\frac {\Gamma(n+1)\Gamma(1/3)}{\Gamma\left(n +\frac 43\right)}\right) ^{1/n}$$ $$=\exp \left( \lim_{n\to\infty} \frac {\ln(\Gamma(n+1)) +\ln(\Gamma(1/3))-\ln(\Gamma(n +4/3))}{n}\right)$$ Now by L'Hospital we have $$\exp {\left( \lim_{n\to \infty} \frac {\ln(\Gamma(n+1)) +\ln(\Gamma(1/3))-\ln(\Gamma(n +4/3))}{n}\right)} =\exp{\left( \lim_{n\to \infty} \left[\psi(n+1) -\psi(n+ 4/3)\right] \right)}$$ But for large $x$, $\psi(x)\sim \ln (x)$ Hence $$\exp {\left( \lim_{n\to \infty} \left[\psi(n+1) -\psi(n+ 4/3)\right]\right)}=\exp {\left( \lim_{n\to \infty} \left[\ln(n+1) -\ln(n+ 4/3)\right]\right)}=1$$ Similarly $$\lim_{n\to \infty} (a_n)^{1/n}= 1$$ Hence giving me the answer as $15$. I want to know whether I am correct or not.  And if I am correct I want to know if there is any other simple and elementary method to solve the question. Edit: By some other method I dont mean that just converting the Gamma function to factorial representation and then using Stirling's approximation","['digamma-function', 'gamma-function', 'calculus', 'sequences-and-series', 'limits']"
2908355,A representation of odd numbers.,"The problem asks to prove:
$$2\mathbb{Z}+1=\{x\in\mathbb{Z}\ |\ \exists\ n\in\mathbb{N}\ s.t. |x|=\frac{d(n^2)}{d(n)}\}.$$
where $d(n)$ is the function that counts the number of positive divisors of $n$. I see that if the prime factorization of $n=p_1^{m_1}\dots p_k^{m_k}$, then $d(n)=\prod_{i=1}^k(m_i+1)$, therefore reduce the problem to 
$$2\mathbb{Z}+1=\{x\in\mathbb{Z}|\exists m_1,\dots,m_k\in\mathbb{N} , s.t. |x|=\prod_{i=1}^k\frac{2m_i+1}{m_i+1}\}.$$
But I have no idea how to proceed, please help.",['number-theory']
2908378,Quotient by action of this matrix group is $S^2$?,"Let $M^{2 \times 2}_{\neq 0}$ denote the set of all non-zero $2 \times 2$ real matrices. Note that this set contains non-invertible elements. By an orthogonal matrix, let's mean a matrix $A$ such that $AA^\top$ is a non-zero scalar multiple of the identity. Note this is different to the usual definition. Write $\mathrm{Orth}^{2 \times 2}$ for the group of all real orthogonal $2 \times 2$ matrices. Let $\mathrm{Orth}^{2 \times 2}$ act on $M^{2 \times 2}_{\neq 0}$ by left-multiplication. I have a hunch that the resulting quotient space is homeomorphic to $S^2$. Is this correct? If so, what's the homeomorphism? Remark. It's explained in the comments that this is the same as trying to find $$\mathbb{RP}^3/O(2).$$ I actually think trying to find $\mathbb{RP}^3/SO(2)$ is the better question, and I think an assumption of orientation-preservingness was implicit in my thinking. However I am interested in both questions.","['matrices', 'general-topology', 'lie-groups', 'smooth-manifolds']"
2908390,Volume of a tetrahedron whose 4 faces are congruent.,"Suppose that I have a tetrahedron such that all four faces consist of congruent triangles, says with the lengths $a,b$ and $c$ for each side. Is there a beautiful method to compute its volume? PS. The reason for me tagging calculus and linear algebra is that I figured that the technique used to calculate such a problem may come from this areas.","['trigonometry', 'calculus', 'linear-algebra', 'geometry']"
2908410,Differential equation $(ax+by+c)dx+(ex+fy+g)dy=0$,"On Wikipedia:Homogeneous differential equation , it is said that a first order differential equation of the form ($a$, $b$, $c$, $e$, $f$, $g$ are all constants)
$${\displaystyle (ax+by+c)dx+(ex+fy+g)dy=0\,}$$ where $af \not= be$ can be transformed into a homogeneous type. What if $af = be$? Is there a general method for solving this kind of equations?",['ordinary-differential-equations']
2908491,$\{f_n\}$ absolutely continuous converging pointwise to $f$. Is $f$ of bounded variation and/or absolutely continuous?,"I'm trying to solve the following problem: $\{f_n\}:[a,b] \to \mathbb{R}$ sequence of absolutely continuous functions converging pointwise to $f: [a,b] \to \mathbb{R}$. Say if $f$ is of bounded variation and/or absolutely continuous if: a) $\exists C : \int_a^b |f_n'(x)|dx \leq C \; \forall n$ b) $\exists g \in L^1((a,b)): |f_n'| \leq g \text{ a.e. in } (a,b) \; \forall n$ My attempt: Consider $a = x_0 < x_1 < \cdots < x_N = b$. Since $f_n$ are absolutely continuous, we have: 
$$
f_n(x_j)-f_n(x_{j-1}) = \int_{x_{j-1}}^{x_j} f_n'(t)dt 
$$ 
$$ 
|f(x_j) - f(x_{j-1})| = \lim_{n \to \infty} |f_n(x_j)-f_n(x_{j-1})| \leq \lim_{n \to \infty} \int_{x_{j-1}}^{x_j} |f_n'(t)|dt  \\
\implies \sum_{j=1}^N|f(x_j) - f(x_{j-1})| \leq \lim_{n \to \infty}  \sum_{j=1}^N \int_{x_{j-1}}^{x_j} |f_n'(t)|dt  = \lim_{n \to \infty} \int_a^b |f_n'(t)|dt  
$$
Then, both $(a)$ and $(b)$ imply $f$ of bounded variation, since: 
$$
(a) \implies \int_a^b |f_n'(t)|dt  \leq C \\
(b) \implies \int_a^b |f_n'(t)|dt \leq \int_a^b g(t)dt < \infty
$$
Is this right? However, I do not know how to proceed for the absolute continuity of $f$. 
Can someone help? Thank you","['measure-theory', 'bounded-variation', 'real-analysis', 'absolute-continuity', 'functional-analysis']"
2908514,Milne Thomson method for determining an analytic function from its real part,"What is the logic behind taking  $z = {\bar {z}}$ while finding analytic function in Milne-Thomson Method ? I mean we write 
$ { f(z)=u(x,y)+iv(x,y)} $ as 
$ 
{\displaystyle f(z)=u\left({\frac {z+{\bar {z}}}{2}}\ ,{\frac {z-{\bar {z}}}{2i}}\right)+iv\left({\frac {z+{\bar {z}}}{2}}\ ,{\frac {z-{\bar {z}}}{2i}}\right)} {\displaystyle } $ Its fine uptil now but then we say $f(z)$ can be regarded as an identity in two independent variables $z$ and $ {\bar {z}}$ so we take $z = {\bar {z}}$, this gives us $ { f(z)=u(z,0)+iv(z,0)} $ Source I am not getting the the part $z = {\bar {z}}$ . How do we regard it an identity of 2 independent variable? Please guide me with an intuitive explanation. ( Please use example if you can ) . I went through this Link but could not understand how $w$ is independent of $z$. I mean whatever value of $x$ and $y$ we take $z$ and $w$ are obtained from the same x and y, so how can we say both are independent! Thanks in advance !","['complex-analysis', 'holomorphic-functions', 'analytic-functions']"
2908533,"If $A=1$, $B=2$, etc, then what word, treated as a product of its letters, has value closest to $1000000$?","Suppose that a product $n$ is the product of the numbers corresponding to its letter, eg. $A = 1$, $B = 2$, etc. What is the word that has a product close $1000000$? Here's some examples: $$\begin{align}
8 &= BAD = 2 \times 1 \times 4 = 8 \\
6 &= CAB = 3 \times 1 \times 2 = 6 \\
168000 &= ADJACENT
\end{align}$$ EDIT : Heres some what I did.
First, I chopped $1,000,000$ into $10^{6}$, or $10 \times 10 \times 10 \times 10 \times 10 \times 10$. Then, I factored each $10$ into $2 \times 5$. Then, I tried to to combine the $2$'s and $5$'s in different quantity. In short, I produced the letters : $A$, $B$, $D$, $E$, $H$, $P$, $J$, and other letters. Then, I think I can't produce some word that has a meaning and makes sense , because it exceeds the limitation of $1,000,000$. How do I get it through?","['algebra-precalculus', 'puzzle', 'products']"
2908536,Exercise 3 of Lecture notes for 18.155 On concentration compactness and soliton solutions for the NLS equation,"As picture below, I want to prove the Proposition 2.1. I want to use Poincare inequality, but fail. How should I do it ? Thanks for any answer or hint.","['functional-analysis', 'partial-differential-equations']"
2908549,Monotonicity of the function $(1+x)^{\frac{1}{x}}\left(1+\frac{1}{x}\right)^x$.,"Let $f(x)=(1+x)^{\frac{1}{x}}\left(1+\frac{1}{x}\right)^x, 0<x\leq 1.$
Prove that $f$ is strictly increasing and $e<f(x)\leq 4.$ In order to study the Monotonicity of $f$, let 
$$g(x)=\log f(x)=\frac{1}{x}\log (1+x)+x\log \left(1+\frac{1}{x}\right).$$
And $f$ and $g$ has the same Monotonicity. By computation,
$$g'(x)=\frac{1}{x^2}\left(\frac{x}{1+x}-\log (1+x)\right)+\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}.$$ 
As we know $\frac{x}{1+x}-\log (1+x)\leq 0$ and $\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}\geq 0$. So it does not determine the sign of $g'(x)$.
If we compute the second derivative $g''(x)$,  you will find it is also difficult to determine the sign of $g''(x)$.
 Our main goal is to prove $$\frac{1}{x^2}\left(\frac{x}{1+x}-\log (1+x)\right)+\log \left(1+\frac{1}{x}\right)-\frac{1}{1+x}>0.$$ Is there some tricks to prove this result. Any help and hint will welcome.","['tangent-line-method', 'analysis', 'calculus', 'inequality', 'exponential-function']"
2908585,How to evaluate the following definite integral $\int_0^1\frac{\arctan(ax)}{x\sqrt{1-x^2}}dx$? [duplicate],"This question already has answers here : Integrating $\frac{\arctan x}{x\sqrt{\smash[b]{1-x^2}}}$ (2 answers) Closed 5 years ago . $$\int_0^1\frac{\arctan(ax)}{x\sqrt{1-x^2}}dx$$ My working is as follows; Let $$I(a)=\int_0^1\frac{\arctan(ax)}{x\sqrt{1-x^2}}dx$$ and thus $$I'(a)=0-0+\int_0^1\frac{x}{(1+a^2x^2)(x\sqrt{1-x^2})}dx$$ (I did this using Leibniz-Newton's formula. I took the partial derivative of the integrand with respect to $a$, and since the two limits of the integral are constants, the first two terms are zero.) Now, I'm stuck... I've been trying this one for the last hour. Can someone hint me towards the right answer? (which I looked up to be $\frac{\pi}{2}\sinh^{-1}a$) P.S: I am aware of this site's norm of keeping MathJax out of the titles, but I'm getting a message that a question with this title already exists... Can someone please help?","['integration', 'calculus', 'definite-integrals']"
2908626,Is Backward-Euler method considered the same as Runge Kutta $2^{\text{nd}}$ order method?,"I have a book that quotes: Euler's method, Modified Euler's method and Runge's method are
  Runge-Kutta methods of first, second and third order respectively. The
  fourth-order Runge-Kutta method is method is most commonly used and is
  often referred to as 'Runge-Kutta method' or 'classical Runge-Kutta
  method' Similary Wikipedia categorizes Backward-Euler's method as ' Implicit methods' under the list of Runge-Kutta methods and also mentions: The backward Euler method is first order. Now the problem is that the same book (from which I have taken the above quote) solves the below problem using a method that seems quite different (at least to me) from the Backward-Euler's method. Consider the first order initial value problem
  $y'=y+2x-x^{2}$,$y(0)=1$,$(0\le x\le\infty)$ with exact solution
  $y(x)=x^2+e^x$. For $x=0.1$, what is solution obtained using a single
  iteration of the second-order Runge-Kutta method with step size
  $h=0.1$ The book then shows the solution using: $$k_1=hf(x_0,y_0)$$ $$k_2=hf(x_0+h,y_0+k_1)$$
  $$y_1=y_0+\frac{1}{2}(k_1+k_2)$$ Here $f$ denotes the differential equation i.e. $y'=f(x,y)=y+2x-x^{2}$. Using the above equations and initial value, it gets the result as $y_1=1.1145$. I tried to calculate the vaule using Backward-Euler's method using: $$y_{1}=y_{0}+hf(x_{1},y_{1})$$
and I get the result as $y_1=1.1322$, which is different from the solution given in the book. So I have the following questions: Is Backward-Euler method considered the same as Runge-Kutta
$2^{\text{nd}}$ order (RK2) method? If yes, is my book incorrect with
the solution? Is the method used in the book the actual Runge-Kutta
    $2^{\text{nd}}$ order method which is completely different from Backward-Euler's method? In case my first question's answer is yes , how can a method be a Runge-Kutta $2^{\text{nd}}$ order (RK2) while also
being a  $1^{\text{st}}$ order in itself? (no need to answer if first question's answer is no ) I am really confused with the way the book used the name Backward Euler as RK2 but then used a different method to solve a question that wanted RK2. Please help me understand this. Note : My book states Backward Euler as Modified Euler's method (In case it's not so obvious).","['runge-kutta-methods', 'numerical-methods', 'ordinary-differential-equations', 'eulers-method']"
2908646,what would the value of determinant of a matrix be if a specific entry changed?,"What will the value of determinant of matrix $A=\pmatrix{1&3&4\\5&2&a\\6&-2&3}$ be change if we change $a$ to $a+2$. This is an easy problem because $|A|=-127+20a$ and if we did that changing, we would get $-87+20a$. My question here is: Can we do this request by doing another way? Where does that $+40$ come from regarding the whole entries of the matrix? Thanks","['matrices', 'linear-algebra']"
2908662,Convergence of $f(x)+ f^{2} (x) + f^{3}(x)+ … $,"Imagine that $x \in (0,1]$ and $f$ is a continuous function such that $0 <f(x) < x$. Also, for any $k \in \mathbf N$ define $f^{k}(x)$ recursively by $f^{k} (x) = f(f^{k-1} (x))$. Are there any theorems that put simple, but also fairly general conditions on $f(x)$ that ensures that the series $f(x)+ f^{2} (x) + f^{3}(x)+ …$ is convergent? Thanks.","['functional-analysis', 'sequences-and-series']"
2908681,Prove that two line segments formed by a line intersecting with three circles are equal.,"Given a line segment $AB$. A point $C$ is chosen on it. Three circles with the diameters $AB, BC, AC$ are constructed. A line comes through the point $C$ and intersects the large circle in the points $G$ and $J$, the smaller one in the points $H$ and $K$. How can I prove that the segments $JK$ and $GH$ are equal? I have tried to solve the problem with the help of the similar right triangles $ACK$ and $CHB$, but after some attempts I feel like it is a dead end. What else can I do? (I feel like I saw this problem somewhere, so apologies if this problem is popular or has been asked more than once)","['circles', 'geometry']"
2908741,Prove that for $f(z)=\sum_{n=0}^{\infty} \frac{z^{n}}{2^n+1}$,"Prove that for $f(z)=\sum_{n=0}^{\infty} \frac{z^{n}}{2^n+1}$, $$\lim_{z \to2} f(z) (2-z) = 2$$ My approach : I was thinking of applying Abel's limit theorem to compute the limit but since it is valid only for $z \to 1 $ I am totally clueless I also tried by this approach:
$$ \lim_{\epsilon \to 0 } f(2-\epsilon)(\epsilon)= \sum_{0}^{\infty} \frac{(2-\epsilon)^n}{2^n +1}(\epsilon)$$ but again i am not sure of how to proceed.","['complex-analysis', 'limits', 'complex-numbers']"
2908798,Is there an equivalent to trigonometry for solid angles?,"Intuitively I would say that it would make no sense, but this question crossed my mind : Is there an equivalent to trigonometry for solid angles?? I haven't found anything yet. Thanks for answering! -- Extracts from the comments : For example, in usual trigonometry, if you multiply the hypotenuse by the sine of the angle, you get the opposite segment length of a rectangle triangle, but would it make sense to define a kind of *sinus function which relates the side surface of a cone with the base surface (opposite to the solid angle), and the like??","['trigonometry', 'pi', 'surfaces', 'solid-angle']"
2908823,Is this differential equation solvable in MATLAB?,"\begin{align}
    r'' - r θ'^2 &= g \sinθ - (M/m)(g - y'')
\\    
    r' + Rθ' + y' &= 0
  \\  
    rθ'' + 2r'θ' + g \cosθ &= 0
\end{align} Sorry may I know how I can solve this with ode45? I am attempting to convert all three equations into 1st Order Coupled equations, but I can't seem to get the second equation into the correct substitution. Is it possible? Or do I have to use another ODE solver (which?) Edit: I attempted rob's solution, and have gotten the following code Taking M=3m, g=9.81, R=4 function dydt = odefun(t,y)
dydt = zeros(9,1)
dydt(1) = y(7)
dydt(2) = y(8)
dydt(3) = y(9)
dydt(4) = sqrt((y(8)-9.81*sin(y(4))-3*9.81+3*y(9))/(2*y(1)))
dydt(5) = -(9.81*cos(y(4))+y(5)*y(7))/(2*y(1))
dydt(6) = -y(2)-y(1)*4
end but received the following error message Not enough input arguments.
Error in <filename> (line 3)
dydt(1) = y(7) It seems matlab does not accept the code if I do not define (θ'')' = ___. Sorry if this is a dumb misunderstanding, I am still trying to figure out how MATLAB works (any resources to recommend?) I have also attempted function dydt = odefun(t,y)
dydt = zeros(9,1)
dydt(1) = y(7)
dydt(2) = y(8)
dydt(3) = y(9)
dydt(4) = sqrt((y(8)-9.81*sin(y(4))-3*9.81+3*y(9))/(2*y(1)))
dydt(5) = -(9.81*cos(y(4))+y(5)*y(7))/(2*y(1))
dydt(6) = -y(2)-y(1)*4 % radius here!
dydt(4) = y(1) % are these 3 more lines correct logically, they seem to 
dydt(5) = y(2) % let matlab run, but for now I am just getting constants
dydt(6) = y(3) % for all solutions
end is this the right code?",['ordinary-differential-equations']
2908827,Is $|x|$ a smooth manifold?,"The function $y = |x|$ is not a differentiable function from $\mathbb{R} \rightarrow \mathbb{R}$. But considering the graph of $y = |x|$ as a subspace of $\mathbb{R}^2$, we can endow this space with a smooth structure consisting of the single smooth chart which projects the graph down to the x-axis. What I am confused about is how to reconcile these two seemingly inconsistent viewpoints. Can someone explain why $y = |x|$ is not a smooth function yet it can be made into a smooth manifold?",['differential-geometry']
2908849,Find the number of complex numbers $z$ such that $z^{2018} =\overline{z}.$,"Find the number of complex numbers $z$ such that
$$z^{2018} =\overline{z}.$$ I have the basic idea of plugging in $z$ as $a+bi$ but it seems as if it is a dead end. $(a+bi)^{2018} = a-bi$ doesn't lead me anywhere. I am puzzled at how to start.","['contest-math', 'algebra-precalculus', 'complex-numbers']"
2908908,Express arccos in terms of arctan,"I'm trying to express $\arccos(x)$ in terms of $\arctan$ because the software I'm using only recognizes $\arctan$.
I know that $\arcsin(x)+\arccos(x)=\frac{\pi}{2}$
and $\arcsin(x)=2\arctan\left(\frac{x}{1+\sqrt{1-x^2}}\right)$.
Does that mean that $\arccos(x)=\frac{\pi}{2}-2\arctan\left(\frac{x}{1+\sqrt{1-x^2}}\right)$?",['trigonometry']
2908939,Linear and algebraic independence of exponentials,"Original question Let $P_1,\ldots, P_n$ be distinct polynomials of a complex variable. We suppose that they are without constant term ( $P_i(0)=0$ ). Is it true that the functions $z\mapsto e^{P_i(z)}$ are linearly independant (over $\mathbb{C}$ ) ? Late edit: The proof given in the selected answer proves the following Theorem Let $(P_i)_{i\in I}$ a family of complex univarariate polynomials without constant term. Then if $i\mapsto P_i$ is injective, $(e^{P_i})_{i\in I}$ is a family of (entire) functions linearly independent over $\mathbb{C}[z]$ . One then has the Corollary Under the same conditions (no constant term), if the family $(P_i)_{i\in I}$ is $\mathbb{Z}$ -independant, then $(e^{P_i})_{i\in I}$ is algebraically independant with respect to $\mathbb{C}[z]$ . Proof Call $G$ the family of exponentials ( $G=(e^{P_i})_{i\in I}$ ). For every multiindex $\alpha \in \mathbb{N}^{(I)}$ , one has $$
G^\alpha=\prod_{i\in I} (e^{P_i})^{\alpha(i)}=
\prod_{i\in I} (e^{\alpha(i)\,P_i})=
e^{\sum_{i\in I}\alpha(i)\,P_i}
$$ but the fact that $(P_i)_{i\in I}$ is $\mathbb{Z}$ -independant (linearly) implies (and is indeed equivalent to) $\alpha \mapsto \sum_{i\in I}\alpha(i)\,P_i$ is into. One has, from the theorem, that $(G^\alpha)_{\alpha \in \mathbb{N}^{(I)}}$ is $\mathbb{C}[z]$ -linearly independant which amounts to say that $G$ is algebraically independant over $\mathbb{C}[z]$ .","['complex-analysis', 'holomorphic-functions', 'calculus']"
2908940,Fundamental Theorem of Algebra for Two Variables,"Is there an extension for the Fundamental Theorem of Algebra for Two or more variables, such in case of polynomials systems: $ \begin{cases} f(x, y) = 0 \\ g(x, y) = 0 \end{cases} $ For single-variable polynomials, the Theorem states that nth-degree polynomials implies n complex roots. And about two or more variables, there is such extension? Perhaps a sum of degrees or the max number of degrees of the variables?",['abstract-algebra']
2908967,"Compute $\frac{d}{dx} \left\{ u^H M^{-1}(x) \ A \ M^{-1}(x) \ u\right\} $, where $M(x) = \left(B + xA \right) ; \ M^H(x) = M(x)$","Problem : Let $x \in \mathbb{R}$, $u \in \mathbb{C}^{n \times 1}$, $B \in \mathbb{C}^{n \times n}$, $A \in \mathbb{C}^{n \times n}$, and 
  $M(x) = \left(B + xA \right) ; \ M^H(x) = M(x)$. Obtain
  \begin{align}
\frac{d}{dx} f(x) = \frac{d}{dx} \left\{ u^H M^{-1}(x) \ A \ M^{-1}(x) \ u\right\} \ .
\end{align}","['derivatives', 'multivariable-calculus', 'matrix-calculus', 'linear-algebra']"
2908983,Convex neighborhoods lemma problem in do Carmo's book,"I'm reading DoCarmo's book, Riemannian Geometry and i dont understand a step in the proof of this lemma. Namely the last one in the ss. I don't get the fact that inner product is zero. And also i don't know how to use Gauss's lemma in that regard. Can some one fill in the details for me please?","['geometry', 'riemannian-geometry', 'differential-geometry']"
2909026,Kirby–Siebenmann class and the 4th Stiefel-Whitney class: $ \operatorname {ks} (M)$ v.s. $w_4(M)$,"Kirby–Siebenmann class $ \operatorname {ks} (M)$ is an element of the fourth cohomology group
$$ 
{\displaystyle \operatorname {ks} (M)\in H^{4}(M;\mathbb {Z} /2)}
$$
which must vanish if a topological manifold $M$ is to have a piecewise linear structure. It is named for Robion Kirby and Larry Siebenmann. However, the 4th Stiefel-Whitney class $w_4(M)$ also is an element of the fourth cohomology group
$$ 
w_4(M) \in H^{4}(M;\mathbb {Z} /2).
$$ Questions: (a) How are Kirby–Siebenmann class $ \operatorname {ks} (M)$ and the 4th Stiefel-Whitney class $w_4(M)$ related? (b) We say that the obstruction of the spin structure is the non-vanishing of the 1st or 2nd Stiefel-Whitney class $w_1(M)\neq 0$ or $w_2(M)\neq 0$. Is the obstruction of having piecewise linear (PL) structure has something to do with the non-vanishing of the Kirby–Siebenmann class $ \operatorname {ks} (M)$? the non-vanishing of the 4th Stiefel-Whitney class $w_4(M)$?","['algebraic-topology', 'geometric-topology', 'manifolds', 'characteristic-classes', 'differential-geometry']"
2909045,Dealing with different definitions of the Ornstein–Uhlenbeck process,"I've run up against a wall in reconciling two different definitions of the Ornstein–Uhlenbeck process, and would appreciate some help. On the one hand, as discussed here , we can define an Ornstein–Uhlenbeck process as a Gaussian process with a kernel function of the form $k(x, y) \propto \exp(-\left\lVert x - y\right\rVert / \theta)$, which is clearly stationary. On the other hand, we have the definition of the Ornstein–Uhlenbeck process as the solution to the stochastic differential equation $du(t) = \theta(\mu - u(t))+\sigma \, dW(t)$, which is given by $$u(t)= u(0) \exp⁡(-\theta t)+\mu(1-\exp⁡(-\theta t) )+\sigma \exp⁡(-\theta t) \int_0^t \exp⁡(\theta\tau) \, dW(\tau).$$ Using the Itô isometry, as shown here and here on SE , this process can be shown to have the covariance $$k(x, y) = \frac{\sigma^2}{2\theta} \exp(-\theta (x+y)) (1 - \exp(2 \theta (x - y))).$$ Not only is this not equal to the previous definition, this is not even stationary. What gives? Are these two distinct definitions of what ""Ornstein–Uhlenbeck process"" means, or can these two processes be shown to be equal, possibly modulo certain assumptions? As a bonus question, just because I've never seen anyone prove it - how do you prove that the solution to the above SDE is a Gaussian process (assuming it is)?","['stochastic-processes', 'statistics', 'stochastic-calculus', 'probability']"
2909062,From Ng video: Using feature normalization with polynomial regression,"In this video on machine learning by Andrew Ng, called ""Features and Polynomial Regression"", at time 4:34, he mentions the possibility of feature normalization in polynomial regression. By which he means, I believe, applying an affine transformation to the feature vector $(1, x, x^2, \dotsc, x^{n-1})$ so that the variances of the $x^i$ are closer to one another. Does anyone know of a reference on this practice? I'd like to investigate it and its effects. (That is, the specific application of feature normalization to polynomial regression. Not just feature normalization in general.)","['statistics', 'regression', 'reference-request', 'machine-learning', 'optimization']"
2909070,"Number of integer solutions of the equation $x + y + z = 30$ if $x \geq 2$, $y \geq 0$, $z \geq -3$","I am trying to solve a problem using permutation/combination but cannot figure out how to proceed. Suppose the sum of three variables $x, y, z$ is $30$. If $x\ge2, y\ge0, z\ge-3$, how many integer solutions exist? I understand that $2\le x\le33, 0\le y\le31, -3\le z\le28$. A simple simulation shows that there are $528$ solutions. However, I am unable to calculate this mathematically. I would like a hint so that I can try this on my own.","['combinations', 'combinatorics']"
2909071,Show that if $A \subset B$ then $\sigma(A) \subset \sigma$ (B),"Hello I am trying to show that for subsets $A,B \in \Omega$ with $A \subset B$ then $\sigma(A) \subset \sigma(B).$ The textbook I am studying is Probability and Measure Theory Second Edition by Robert B. Ash and the definition given in the book for a $\sigma$ field of a collection of subsets $A \in \Omega$ is such that $\sigma(A)$ is closed under complementation and countable union i.e.: $\sigma(A)$ satisfies these properties: $\Omega \in \sigma (A).$ If any subset $X \in \sigma(A)$ then $X^c \in \sigma(A).$ If $X_1, X_2,  X_3, \ldots, \in \sigma (A)$ then $\left( \bigcup \limits_{i=1}^\infty X_i \right) \in \sigma(A).$ Based on this definition, to me it would seem rather obvious that A $\subset$ B would imply $\sigma(A) \subset \sigma(B)$ since the $\sigma$ field of a collection of sets that belong in the domain $\Omega$ expands the original collection of sets to include all possible combinations of the complements and unions of the original elements in the collection of sets so the assumption of $A \subset B$ would imply all the elements of $A$ are in $B$ so taking the $\sigma$ field of $B$ is really taking the $\sigma$ field of $A$ and then adding to it the $\sigma$ field of $(A - B)$ or $A \cap B^c$ i.e. $\sigma(B) = \sigma(A) + \sigma(A - B) = \sigma(A) \cup \sigma(A - B).$ To me this proof seems reasonable enough to lead to the result of $\sigma(A) \subset \sigma(B)$ but I just wanted to make sure if my argument is reasonable enough to lead to this conclusion or if there is anything else that can be added to strengthen it.","['measure-theory', 'probability-theory']"
2909077,Proof that loss function for linear regression is an ellipsoid,"I cannot seem to find a proof that $f(\mathbf{\beta}) = \left\lVert \mathbf{y}-\mathbf{X} \mathbf{\beta} \right\rVert^2$ is an ellipsoid, centered at the OLS solution $\hat{\beta}$. Can anyone show how to convert it to the quadratic form of a general ellipsoid, i.e. $(\beta - \hat{\beta})^T \mathbf{A} (\beta - \hat{\beta})$, where $\mathbf{A}$ is positive definite?",['statistics']
2909099,"Is there a Partition of $[0,1]$ into closed, countably infinite sets?","I'm wondering whether it is possible to Partition the closed interval $[0,1]$ into closed, countably infinite sets. The only observations I could make were as follows: When we remove the endpoints, we effectively end up with $\mathbb R$, and can consider the image of
$$\mathscr S :=\{r+\mathbb N\mid r\in [0,1)\}$$ Because every $S\in \mathscr S$ is bounded, it must contain at least one limit point. Because our partition itself must be uncountable, $\bigcup_{S\in \mathscr S} L(S)$ ist uncountable, and by second countability, has uncountably many limit points. A natural example for a partition into countably infinite sets is $\{r+\mathbb Q\}$, but these elements are not closed – on the contrary, every sets closure is $\mathbb R$ already. I'm not really sure where to go from there. My intuition says that this won't work, because it would get “too crowded” near the boundary points, but I'm not sure how to make this precise.","['general-topology', 'cardinals', 'real-analysis']"
2909154,Topology on the space of matrices,"The vector space of $n\times n$ real matrices is isomorphic as a vector space to $\mathbb R^{n\times n}$. Does it follow that it is ""the same"" as the topological space $\mathbb R^{n\times n}$ with the standard topology (e.g. the box topology, which is the same as the product topology, which is the same as the topology generated by balls)? (What exactly does this statement (i.e. ""the same"") mean?) If I know an open set in $\mathbb R^{n\times n}$ (say $(a,b)^{\times n^2})$, how do I find the corresponding open set in the space of matrices?",['general-topology']
2909186,$n^3+n<3^n$ for $n \geq4$ by induction.,To prove that $n^3+n<3^n$ for $n \geq4$ by induction. I have proved the fact but it became very long and I have to use two more induction proof within the proof. Can someone give a better solution by induction? Thank You.,"['number-theory', 'calculus', 'induction', 'real-analysis']"
2909187,$\nabla\int_{S_1(0)} f(|x|w)d\sigma(w) = \int_{S_1(0)} \nabla f(|x|w)d\sigma(w)$,"Let $U=\{x:x\in \mathbb{R}^n, x\neq 0\}$ and $f\in C^2(U)$. Define,
  for $x\in U$, $$f_{\#}(x) = \int_{S_1(0)} f(|x|w)d\sigma(w)$$ Show
  that $\nabla(f_{\#}) = (\nabla f)_{\#}$ Hint: use the divergence theorem and the formula for polar integration I know that I must use the divergence theorem: $$\int_{\Omega}div(\vec{X})(x) dx = \int_{\partial \Omega}\vec{X}(x)\cdot n(x) dx$$ But I don't know what it means by polar integration formula, since the polar integration technique is only for $\mathbb{R}^2$. I don't see what to do in $n$ dimensions. I tried seeing the integral as $$f_{\#}(x) = \int_{S_1(0)}f(|x|w)d\sigma(w) = \frac{1}{|x|^{n-1}}\int_{S_{|x|}(0)}f(w)d\sigma(w) $$ but then I'd have to take the laplacian of that, which is unpratical. I tried to integrate both sides to end with a ball and try to see some divergence in order to apply the divergence theorem: $$\int_0^1\int_{S_1(0)}|x|^ {n-1}f(|x|w)d\sigma(w) = \int_{B_1(0)}f(|x|w)d\sigma(w)$$ But I don't see how it helps. I think the best way is to begin with $(\nabla)_{\#}$: $$(\nabla f)_{\#}(x) = \int_{S_1(0)}\nabla f(|x|w)d\sigma w $$ I think that if I see the function as a vector field times the normal I can use the divergence theorem to transform this integral to an integral of a ball, but I think I cannot do that.","['integration', 'divergence-operator', 'multivariable-calculus', 'partial-differential-equations']"
2909226,"Let $X_1,X_2...X_n$ be a random sample from $N(\mu,\sigma^2)$. Find the umvue of $\mu^3$. [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Note: I know lehman scheffe theorem and that sample mean is umvue of $\mu$. But how can we find the UMVUE of hiher powers of $\mu $?","['statistical-inference', 'statistics', 'parameter-estimation', 'normal-distribution']"
2909228,"How to construct a bijection from $[0,1]$ to $[0,1] \times [0,1]$? [duplicate]","This question already has answers here : Examples of bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$ (2 answers) Closed 5 years ago . I'm trying to find a bijective map $f$ such that $$
f : [0,1] \mapsto [0,1] \times [0,1]
$$ I've succeeded in constructing one in the 1D case : $[0,1] \mapsto [0,1]$, but I don't know how to approach in 2D case as the above. Is there anyone to help me out?","['functions', 'real-analysis']"
2909233,Stuck On A Proof By Induction,"I need to prove true for all integers greater than and equal to 1 using induction. I'll skip the base case, and the inductive assumption, and jump straight to the inductive step: = What I've done now is to say that is less than . But I don't know what to do from beyond there.","['inequality', 'proof-verification', 'induction', 'discrete-mathematics']"
2909263,Gradient descent for functionals?,"If $f:\mathbb{R}^2\longrightarrow\mathbb{R}$ is smooth, then given an initial point $x_0\in\mathbb{R}^2$, we can use gradient descent to find a sequence of points $\{x_i\}_{i=1}^{\infty}$ that converges to a critical point of $f$ (if one exists). I was wondering if there is any equivalent method for functionals? That is, is there any way of generating a sequence of functions $\{g_i\}_{i=1}^{\infty}$ (preferably polynomials?) that converges to the function $g$ that minimises a functional $E[\phi]$? For example, we could take $E[\phi]$ to be the Dirichlet energy of $\phi$:
$$
E[\phi] = \int_\Omega{\|\nabla\phi(x)\|^2\,\text{d}V}\,.
$$
I have briefly looked at Sobolev gradients but I don't quite understand it. This is slightly different to my specific problem, as the functional I am interested is the vector Dirichlet energy:
$$
E[\mathbf{v}] = \int_\Omega\left((\nabla \cdot \mathbf{v})^2 + \|\nabla \times \mathbf{v}\|^2\right)\,\text{d}V\,.
$$
with conditions
$$
\begin{array}{c}\langle \mathbf{v}, \hat{\mathbf{n}}\rangle=0 \mathrm{\ on\ } \partial \Omega,\\ \int_{\Omega} \|\mathbf{v}\|^2\,dV = 1.\end{array}
$$
Edit: The case that I am interested specifically is when $\Omega$ is a torus with a slightly perturbed boundary or an infinite tube twisted helically around the $z$-axis.","['calculus-of-variations', 'partial-differential-equations', 'optimization', 'gradient-descent', 'differential-geometry']"
2909264,In what $precise$ sense is Minkowski space asymptotically flat?,"I've brought this question over from the physics stack exchange, where it didn't generate interest. We say a manifold $(M,g)$ is conformally compact if it is the interior of some $(\overline M, \overline g)$, such that
$$g = r^{-2}\overline g|_M,\quad \mathcal Z(r) = \partial M,\quad \text{and}\quad \mathrm{d}r_p \neq 0\ \text{for any}\ p\in \partial M.$$
Moreover, we say a conformally compact manifold is asymptotically flat if $|\mathrm{d} r|^2_{\overline g}\equiv 0$ on the boundary. (i.e. the first derivative of the defining function is null on the boundary.) It is ""well known"" that Minkowski space is an asymptotically flat manifold - Penrose's famous compactification supposedly shows this. However, in his compactification, there are two points on the boundary for which $\mathrm{d}r_p = 0$: namely spacelike and timelike infinity. I've made my own coordinate free compactifications of Minkowski space, but they also have some singularities at conformal infinity. How is this formalised? When people say ""conformally compact"" in literature, do they just mean ""there is a dense subset of the boundary satisfying the above conditions""?","['riemannian-geometry', 'conformal-geometry', 'general-relativity', 'mathematical-physics', 'differential-geometry']"
2909330,"Let $f:[a,b]\to\Bbb{R}$ be continuous. Does $\max\{|f(x)|:a\leq x\leq b\}$ exist?","Let $f:[a,b]\to\Bbb{R}$ be continuous. Does \begin{align}\max\{|f(x)|:a\leq x\leq b\} \end{align} exist? MY WORK I believe it does and I want to prove it. Since $f:[a,b]\to\Bbb{R}$ is continuous, then $f$ is uniformly continuous. Let $\epsilon> 0$ be given, then $\exists\, \delta>$ such that $\forall x,y\in [a,b]$ with $|x-y|<\delta,$ it implies $|f(x)-f(y)|<\epsilon.$ Then, for $a\leq x\leq b,$ \begin{align} f(x)=f(b)+[f(x)-f(b)]\end{align}
\begin{align} |f(x)|\leq |f(b)|+|f(x)-f(b)|\end{align}
\begin{align} \max\limits_{a\leq x\leq b}|f(x)|\leq |f(b)|+\max\limits_{a\leq x\leq b}|f(x)-f(b)|\end{align} I am stuck at this point. Please, can anyone show me how to continue from here?","['continuity', 'uniform-continuity', 'analysis', 'real-analysis']"
2909359,Find $\frac{\partial z}{\partial x}$ if $xy+yz+zx = 1$,"Find $\frac{\partial z}{\partial x}$ if $xy+yz+zx = 1$ I don't understand this question at first. It looks like $x,y,z $ are dependent. So we proceed differentiation partially wrt x: $$xy_x +y + y_x z + z_x y + z_x x+z=0$$ This gives $$z_x = \frac{-z-y-y_xz-xy_x}{x+y}$$ But given answer is $\frac{-z-y}{x+y}$, meaning that they take $y_x = 0$, saying that $y$ and $x$ independent. But how this makes sense, then why not take $z$ and $x$ also dependent and say $z_x = 0$ ? Please tell me the reasoning! I think that they mean to say treat $y$ as constant when finding $z_x$","['partial-derivative', 'multivariable-calculus']"
2909367,"For what values of $x$ in $(-3,17)$ does the series $\sum\limits^{\infty}_{n=1}\frac{(-1)^n x^n}{n[\log (n+1)]^2}$ converge?","For what values of $x$ in the following series, does the series converge? \begin{align}\sum^{\infty}_{n=1}\dfrac{(-1)^n x^n}{n[\log (n+1)]^2},\;\;-3<x<17 \end{align} MY TRIAL \begin{align}\lim\limits_{n\to \infty}\left|\dfrac{(-1)^{n+1} x^{n+1}}{(n+1)[\log (n+2)]^2}\cdot\dfrac{n[\log (n+1)]^2}{(-1)^n x^n}\right|&=|x|\lim\limits_{n\to \infty}\left|\dfrac{n}{n+1}\cdot\left[\dfrac{\log (n+1)}{\log (n+2)}\right]^2\right|\\&=|x|\lim\limits_{n\to \infty}\left(\dfrac{n}{n+1}\right)\cdot\lim\limits_{n\to \infty}\left[\dfrac{\log (n+1)}{\log (n+2)}\right]^2\\&=|x|\lim\limits_{n\to \infty}\left(\dfrac{n}{n+1}\right)\cdot\left[\lim\limits_{n\to \infty}\dfrac{\log (n+1)}{\log (n+2)}\right]^2\\&=|x|\left[\lim\limits_{n\to \infty}\dfrac{1}{n+1}\cdot n+2\right]^2\\&=|x|\end{align}
Hence, the series converges absolutely for $|x|<1$ and diverges when $|x|>1$. When $x=1,$
\begin{align}\sum^{\infty}_{n=1}\dfrac{(-1)^n }{n[\log (n+1)]^2}<\infty\;\;\text{By Alternating series test}\end{align}
When $x=-1,$
\begin{align}\sum^{\infty}_{n=1}\dfrac{1}{n[\log (n+1)]^2}<\infty\;\;\text{By Direct comparison test}\end{align}
Hence, the values of $x$ for which the series converges, is $-1\leq x\leq 1.$ I'm I right? Constructive criticisms will be highly welcome! Thanks!","['analysis', 'real-analysis', 'sequences-and-series', 'power-series', 'convergence-divergence']"
2909372,"Positive integers satisfying: for all odd prime powers $p^k < n$, $n - p^k$ is prime","Given positive integer $k$, define the subset $S(k)$ of positive integers $n$ where for every odd prime power $p^k < n$, $n - p^k$ is prime. In other words, $$S(k) = \{n \in \mathbb{N} \mid \forall p \in \mathbb{P} : (p > 2 \wedge p^k < n) \Rightarrow n - p^k \in \mathbb{P}\}.$$ Question: Is $S(k)$ is finite for every $k > 0$? I suspect this is the case from initial computer searches. I am hoping there is an elementary proof of this (if it's true), but I have had little success. Some observations: any $n < 3^k$ will be in $S(k)$ since the condition holds vacuously true on $n$. Furthermore, the largest odd element of $S(k)$ must be $\leq 3^k + 2$ as otherwise it could be written as $3^k + \text{composite even number}$ and thus not be in $S(k)$. So, it suffices to focus attention on the even elements of $S(k)$. Intuitively, it makes sense that the larger you go, the less likely a number belongs to $S(k)$ since there are more ""chances"" for at least one $n - p^k$ to be composite. But it is not at all obvious that this is actually ""impossible to belong to $S(k)$ once we exceed a certain number"".","['number-theory', 'integers', 'prime-numbers']"
2909410,Can't figure out how to solve matrix equation Ax=0,"I tried googling how to solve this matrix through RREF and parametric variables but failed to find something that works similarly to try and solve myself. A is a 2x3 matrix with the values going [ 2 -1 -1 : 1 -2  2 ]
(imagine the set after the colon to be under the first set) x is [x_1, x_2, x_3] but obviously a column instead of a row and for lack of subscript key I just used ""_#"" to denote the same thing. 0 is a 2 row x 1 column matrix with two 0's. To start, I wrote out the matrix's into 2 equations. 2x_1 - x_2 - x_3 = 0 x_1 - 2x_2 + 2x_3 = 0 Then I turned it into a simplified coefficient matrix.
[ 2 -1 -1 0: 1 -2  2 0] (with the second set of numbers past the colon under the first set) Afterward, I reduced it into RREF form to get: [ 1 0 (-4/3) 0 : 0 1 (-5/3) 0 ] (with the second set of numbers past the colon under the first set) Once I got this, I turned the matrix back to these equations: X_1 - (4/3)X_3 = 0 X_2 - (5/3)X_3 = 0 So then I set X_1 = (4/3)X_3 and X_2 = (5/3)X_3. I then tried setting X_3 to t to try and see if I could finagle an answer by solving for 't'. So far, my answers have been wrong every time in comparison to the answer my book gives. Can someone tell me if I messed up somewhere? I deduced it would have to be from my matrix reduction but I did it over a few times in different ways to make sure but I still can't seem to get the answer...","['matrices', 'parametric', 'linear-algebra']"
2909454,"Clifford algebra is isomorphic to exterior algebra, proof in Lawson's","This Proposition 1.2, pg 10 of Lawson's Spin Geometry. Context: We have a homomorphism of graded algebra 
  $$ \wedge^*(V) \rightarrow G^*$$
  induced on each component from the map $$\wedge^r(V) \rightarrow G^r, \quad v_{i_1} \wedge \cdots \wedge {v_{i_r}} \mapsto [v_{i_1} \wedge \cdots \wedge v_{i_r}] $$ where $G^r$ are components of associated graded algebra of $Cl(V,q):=T(V)/J_q(V)$.  where $T(V)$ is tensor algebra, $J_q(V)$ is ideal generated by $\{ v \otimes v + q(v) \rangle  \, : \, v \in V \}$. Question: It suffices to show for a $\varphi \in T^r(V) \cap J_q(V)$ (the $r$ homogenous pieces of elements in $J_q(V)$), $\varphi$ is $0$ in $\wedge^r(V)$, under quotient $T^r(V) \rightarrow \wedge^r(V)$. [...] Any such $\varphi$ can be written as a finite sum $\varphi = \sum a_i \otimes (v_i \otimes v_i + q(v_i) ) \otimes b_i $ where wlog, $a_i, b_i$ are homogenous and $\deg a_i+\deg b_i \le r-2$. The last statement ""wlog"" part is not clear to me. Why cannot $\deg a_i+\deg b_i =r$? This simply means $\sum a_i \otimes v_i \otimes v_i \otimes b_i =0$. and this is possible?","['clifford-algebras', 'abstract-algebra', 'spin-geometry', 'exterior-algebra']"
2909480,What happened in this step of the inductive proof that for all integers $n$ is true that $n^3 - n$ is evenly divisible by $3$?,"Please notice the following before reading: the following text is translated from Swedish and it may contain wrong wording. Also note that I am a first year student at an university - in the sense that my knowledge in mathematics is limited. Translated text: Example 4.4 Show that it for all integers $n$ is true that $n^3 - n$ is evenly divisible by $3$. 1 Here we are put in front of a situation with a statement for all integers and not all positive. But it is enough that we treat the cases when $n$ is non-negative, for if $n$ is negative, put $m = -n$. Then $m$ is positive, $n^3 - n = -(m^3 - m)$ and if $3$ divides $a$, then $3$ also divides $-a$. Now here also exists a statement for $n = 0$ so that we have a sequence $p_0, p_1, p_2, \; \ldots$ of statements, but that the first statement has the number $0$ and not $1$ is of course not of any higher meaning. Statement number $0$ says that $0^3 - 0$, which equals $0$, is evenly divisible by $3$, which obviously is true. If the statement number $n$ now is true, id est $n^3 - n = 3b$ for some integer $b$, then the statement number $n+1$ also must be true for $
\begin{split}
(n + 1)^3 - (n + 1) & = n^3 - n + 3n^2 + 3n \\
& = 3b + 3n^2 + 3n \\
& =3(b + n^2 + n)
\end{split}
$ and $b + n^2 + n$ is an intege. What we was supposed to show now follows from the induction principle. $\square$ 1. That an integer $a$ is ""evenly divisible by 3"" are everyday language rather than mathematical. The precise meaning is that it exists another integer $b$ such that $a = 3b$. In the above written text , I understanding everything (or I at least think so) except $
\begin{split}
(n + 1)^3 - (n + 1) & = n^3 - n + 3n^2 + 3n \\
& = 3b + 3n^2 + 3n \\
& =3(b + n^2 + n).
\end{split}
$ Could someone please explain what happened, because I am totally lost?","['proof-explanation', 'algebra-precalculus', 'induction']"
2909513,Book to learn the use of complex number to solve geometric problem,"I want to learn to use complex number to solve geometric problems, Specially to solve olympiad questions. There are a couple of books in the market and i am confused which one should i buy. Here is some of them--- Geometry of complex number, Hans Schwedtfeger Introduction to the geometry of complex number, Ronald Deaux Complex number and geometry, liang shin hahn Complex number in geometry. I am a very beginner at this thing. I do not know a thing about using complex number in geometry. Some hard and too much advanced book won't suite me.","['complex-geometry', 'book-recommendation', 'geometry']"
2909535,Affine Transformations isomorphic to Heisenberg group,"I want to show that the lie group $G$ of affine transformations of the form $$ \begin{bmatrix} 1 & c & -\frac{c^2}{2} \\ 0 & 1 & -c \\ 0 & 0 & 1 \end{bmatrix} + \begin{bmatrix} a \\ b \\ c \end{bmatrix} $$ for $a,b,c\in\mathbb{R}$ is isomorphic to the Heisenberg group given by matrices of the form $$
\begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix}$$ for $x,y,z\in\mathbb{R}$. My idea was to identify both groups with $\mathbb{R}^3$ and then I hoped to see that the induced group multiplications on $\mathbb{R}^3$ are the same for both groups. But this is not the case (at least the way I choosed my maps). If I identify an element of the Heisenberg group with a vector $(x,y,z)$ then the induced group multiplication is $(x,y,z)\cdot (x',y',z')=(x+x',y+y',z+z'+xy')$. But when I identify an element of $G$ with a vector $(a,b,c)$, then the induced multiplication is $(a,b,c)\cdot (a',b',c')=(a+a'+b'c+c'\frac{c^2}{2},b+b'-cc',c+c')$ (assuming I have no mistake in my computation). So this does not work. Is there a better way to see that those groups are isomorphic?","['group-isomorphism', 'matrices', 'manifolds', 'group-theory', 'lie-groups']"
2909566,Matrix eigenvalues,"Consider the matrix $$A_n=\begin{bmatrix}
	a & b & 0 & 0 & 0 & \dots & 0 & 0 & 0 \\
	c & a & b & 0 & 0 & \dots & 0 & 0 & 0 \\
	0 & c & a & b & 0 & \dots & 0 & 0 & 0 \\
	0 & 0 & c & a & b & \dots & 0 & 0 & 0 \\
	0 & 0 & 0 & c & a & \dots & 0 & 0 & 0 \\
	\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
	0 & 0 & 0 & 0 & 0  & \dots & a & b & 0 \\
	0 & 0 & 0 & 0 & 0  & \dots & c & a & b \\
	0 & 0 & 0 & 0 & 0  & \dots & 0 & c & a  
	\end{bmatrix}_{n\times n}$$ The matrix with $a=2$ and $b=c=-1$ is encountered in finite difference discretization of $u_{xx}.$ (a) If $D_n = \det(A_n),$ show that $D_n = aD_{n-1}-bcD_{n-2}.$ (b) Solve the recurrence analytically to obtain $D_n$ as a function of $n.$ (and ofcourse $D_n$ will also depend on $a, b, c.$) (c) Obtain the eigenvalues of $A_n.$ (Hint: Replace $a$ by $a-\lambda$) $$ $$
$$ $$(a)Part can be shown easily by just simple Laplace expansion. (b)We see that $D_0=1, D_1=a$. Let $D_n=r^n$ be a solution of the recurrence relation \begin{equation}
D_n=aD_{n-1}-bcD_{n-2}
\end{equation}
Then characteristic equation corresponding to (1) \begin{alignat*}{3}
&\quad & r^n-ar^{n-1}+bcr^{n-2} &=0
\\&\implies &r^2-ar+bc &=0
\\&\implies &r_1=\tfrac{a-\sqrt{a^2-4bc}}{2}, r_2 &=\tfrac{a+\sqrt{a^2-4bc}}{2}
\end{alignat*}$ $ Case 1: $a^2-4bc=0$ $r_1=r_2=\frac{a}{2}$ General solution of (1) : $D_n=(C_1+nC_2)(\frac{a}{2})^n$, where $C_1$ and $C_2$ are arbitrary constants. For $n=0$, we get $C_1=D_0=1$. For $n=1$, we get $(C_1+C_2)\frac{a}{2}=D_1=a\implies C_2=1$ Hence $D_n=(1+n)(\frac{a}{2})^n$ 
$$ $$ Case 2: $a^2-4bc\neq0$ General solution of (1) : $D_n=C_1r_1^n+C_2r_2^n$, with where $C_1$ and $C_2$ are arbitrary constants. For $n=0$, we get $C_1+C_2=D_0=1$ For $n=1$, we get $C_1r_1+C_2r_2=D_1=a\implies (C_1+C_2)\frac{a}{2}+(C_2-C_1)\frac{\sqrt{a^2-4bc}}{2}=a  
\implies 2C_2-1=\frac{a}{\sqrt{a^2-4bc}}  
\implies C_2=\frac{r_2}{\sqrt{a^2-4bc}}$ $\therefore C_1=\frac{-r_1}{\sqrt{a^2-4bc}}$ Hence $D_n=\frac{r_2^{n+1}-r_1^{n+1}}{\sqrt{a^2-4bc}}=\frac{1}{2^{n+1}\sqrt{a^2-4bc}}[(a+\sqrt{a^2-4bc})^{n+1}-(a-\sqrt{a^2-4bc})^{n+1}]$ $$------------------------------------$$
I have done this far, but I'm stuck now. Is there any simpler expression for $D_n$? How to obtain eigenvalues, if we consider replacing $a$ by $a-\lambda$?","['tridiagonal-matrices', 'linear-algebra', 'recurrence-relations', 'eigenvalues-eigenvectors']"
2909621,Commuting floor functions,"Positive irrational numbers $a,b>0$ are such that $\lfloor a\lfloor bx\rfloor\rfloor = \lfloor b\lfloor ax\rfloor\rfloor$ for all $x>0$. Must it be that $a=b$? If $a$ and $b$ are allowed to be rational, this is not always true. For example, we can take $a=1$ and $b=\frac{1}{2}$. It is true that $\lfloor x/2\rfloor = \lfloor\lfloor x\rfloor/2\rfloor$ for any real number $x>0$, as when $2n\leq x<2n+2$ for some nonnegative integer $n$, both sides evaluate to $n$.","['algebra-precalculus', 'real-analysis']"
2909658,"Second derivative of f(x,y)","I try do finde the Matrix $D^3f(a)$ where $f(x,y)=x^4-3x^3y^2$ What i tried is $Df(x,y)=\left( \begin{array}{c}
4x^3-9x^2y^2 && -6x^3y\\
\end{array} \right)$ This is a $1\times 2 $ matrix.
Then following this post I get: $
D^2f(x,y)=
\left( \begin{array}{c}
12x^2-18xy^2 && -18x^2y && -18x^2y && -6x^3\\
\end{array} \right)
$ and $
D^3f(x,y)=
\left( \begin{array}{c}
24x-18y^2 && -36xy^2 && -36xy && -18x^2 &&
36xy && -36x^2 && -18x^2 && -18x^3\\\\
\end{array} \right)
$ which is a $8\times1$ matrix. 
I have troubles by understanding how to get $D^rf(x,y)$ for arbitrary functions","['matrices', 'partial-derivative', 'ordinary-differential-equations']"
2909660,Prove: $2^{n+2}\ge n^3$ induction proof,"Prove: $2^{n+2}\ge n^3 $ for every natural n I got this exercise from TAU logarithms exercises, I tried to do the all usual induction technique but I got to a weird dead end: I do the induction proof and I get to:
$$2^{n+2}+2^{n+2} \ge n^3 +3n^2+3n+1$$ So $2^{n+2}\ge n^3$ and I try another induction proof and I get to: $$2^{n+2}+2^{n+2} \ge  3n^2+3n+1+6n+6$$ So $2^{n+2}\ge 3n^2+3n+1$ and I try another induction proof but I get a wrong answer for the base case (which I think will be $n=2$ at this step) because $2^4$ is not greater than $18$ What am I doing wrong here? If there are any other ways to solve this, it will also be welcomed.","['algebra-precalculus', 'proof-writing', 'induction']"
2909682,On the invertibility of the adjacency matrix of a graph,"Which are the sufficient and necessary conditions for an undirected graph with no self edges (i.e. no loop of length $1$) to have an invertible adjacency matrix? In this case, the adjacency matrix is symmetric (i.e. $A = A^\top$). Moreover, all the diagonal elements are $0$ and there is a $1$ in both the entries $(i,j)$ and $(j,i)$, with $i\neq j$, if and only if vertices $i$ and $j$ are connected. A first necessary condition is the following: all vertices must have at least one connection, otherwise the relative row of $A$ is null. But what else?","['matrices', 'adjacency-matrix', 'graph-theory', 'spectral-graph-theory']"
2909758,"Calculate $A^5 - 27A^3 + 65A^2$, where $A$ is the matrix defined below.","If $A=\begin{bmatrix} 0 & 0 & 1 \\
3 & 1 & 0 \\ 
-2&1&4\end{bmatrix}$ , find $A^5 - 27A^3 + 65A^2$ $$A=\begin{bmatrix} 0 & 0 & 1 \\
3 & 1 & 0 \\ 
-2&1&4\end{bmatrix}$$ Let $\lambda$ be its eigenvalue, then $$(A-\lambda I) = \begin{bmatrix} 0-\lambda & 0 & 1 \\
3 & 1-\lambda & 0 \\ 
-2&1&4-\lambda\end{bmatrix}$$ $$|A-\lambda I| = -(\lambda)^3 + 5(\lambda)^2 - 6(\lambda) +5$$ Using Cayley-Hamilton theorem $$A^3-5^2+6A-5=0$$ How do I use this find $A^5 - 27A^3 + 65A^2$ ?","['matrices', 'eigenvalues-eigenvectors']"
2909775,Proving that the sequence $\left \{ \frac{x_{n}}{y_{n}} \right \} \rightarrow \frac{x}{y}$,"Suppose $\left \{ x_{n} \right \}$ and $\left \{ y_{n} \right \}$ converge to the limits $x$ and $y$, respectively. Also, suppose that $y_{n}$'s are nonzero. I want to show that the sequence $\left \{ \frac{x_{n}}{y_{n}} \right \} \rightarrow \frac{x}{y}$. Let $\epsilon >0$. We can pick an $N$ such that $n \geq N$ $\Rightarrow \left | \frac{x}{y} - \frac{x_{n}}{y_{n}}\right | < \epsilon$. I start from $$ \left | \frac{x}{y} - \frac{x_{n}}{y_{n}}\right |$$ $$=\left | \frac{x}{y} - \frac{x}{y_{n}} + \frac{x}{y_{n}} - \frac{x_{n}}{y_{n}}\right | $$ $$= \left | \frac{x\left (y_{n}- y \right )}{y y_{n}} + \frac{\left ( x-x_{n} \right )}{y_{n}}\right | $$ $$\leq \left | \frac{x}{y} \right | \frac{1}{\left | y_{n} \right |} \left | y-y_{n} \right | + \frac{1}{|y_{n}|}\left | x-x_{n} \right | $$ by triangle inequality. I know that I want to make the two terms $\frac{\epsilon}{2}$ + $\frac{\epsilon}{2}$. Since $y_{n}$ converges to $y$, can make $\left | y - y_{n} \right |< \frac{\epsilon \left | y \right |}{\left ( \left | x \right | + 1\right )}$ for some $N_{1}$. But the $y_{n}$ term is giving me a problem, which I can't get rid off. Any suggestions on how to proceed from here (or approaching the problem from a different angle) would be greatly be appreciated.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
2909791,"If $\sum\limits_na_n$ converges, $f$ is a bijection and $|f(n)-n|<X$ for every $n$, for some fixed $X$, then $\sum\limits_na_{f(n)}$ converges",Let $f(n)$ be a bijection from $\Bbb N$ to $\Bbb N$ such that $$|f(n)-n|\lt X$$ for some fixed natural $X$ and $\forall  n \in \Bbb N$. If $$\sum_{n=1}^\infty a_n$$ is convergent then prove that $$\sum_{n=1}^\infty a_{f(n)}$$ is also convergent. I don't know how to proceed with this problem. I am not able to see any direct way to use the cauchy criterion or any other method to prove that the given series is convergent using the fact that $$\sum_{n=1}^\infty a_{n}$$ is convergent and the condition on $f(n)$. Any hints will be helpful.,"['convergence-divergence', 'sequences-and-series']"
2909807,Genus of a complex curve of degree $2n$,"One basic alg geom book convinced me that the genus of a complex curve given by $y^2=x^{2n}+a_{2n-1}x^{2n-1}+...+a_0$ is $n-1$. Another textbook computes the genus of a singular variety given as $(y-b_1x)(y-b_2x)\cdot\cdot\cdot(y-b_{2n}x)$ to be $\dbinom{2n-1}{2}$ and then argues that, because genus is stable under coefficient change, that's going to be the genus of all curves of degree $2n$. Naturally, I am confused about the implied equality $n-1=\dbinom{2n-1}{2}$.",['algebraic-geometry']
2909822,Jensen's inequality for integral without l.s.c. assumption,"I start with the following lemma Lemma 1. (Fundamental inclusion for convex sets) Let $C$ be a closed subset of $\mathbb{R}^n$. Then, $C$ is convex if and only if
  \begin{equation*} \int_\Omega f\;d\mu \in C \end{equation*} for all
  measures spaces $(\Omega,\mu)$ with $\mu(\Omega) = 1$ and all
  $\mathbb{R}^n$-valued integrable functions $f$ on $\Omega$ satisfying
  $f(x)\in C$ for $\mu$-a.e. $x\in\Omega$. A proof can be given based on Hahn-Banach theorem (strictly separable): Proof: Assume the latter condition holds, we show that $C$ is convex. Let $x,y\in C$ and $\lambda\in (0,1)$, we consider $\Omega =
 \{0,1\}$ with the discrete $\sigma$-algebra, $\mu$ be the discrete
   measure $\mu(\{0\}) = \lambda$ and $\mu(\{1\}) = 1-\lambda$, and
   $f:\Omega\longrightarrow C$ defined by $f(0) = x$ and $f(1) = y$, then
   clearly $\int_\Omega f\;d\mu = \lambda x+ (1-\lambda)y \in C$ and
   therfore $C$ is convex. Conversely, if $C$ is convex, we prove by
   contradiction. Assume that there exists $(\Omega,\mu)$ and $f$ as
   above but \begin{equation*} x_0 = \int_\Omega f\;d\mu \notin C.
 \end{equation*} Using Hahn-Banach theorem\footnote{In $\mathbb{R}^n$
   then every two nonempty disjoint convex sets are separable by a closed
   hyperplane.}, there exists $\xi\in \mathbb{R}^n$ and $\alpha\in
 \mathbb{R}$ such that \begin{equation*} \xi\cdot x < \alpha < \xi\cdot
 x_0 \qquad\text{for all}\qquad x\in C. \end{equation*} But since
   $f(z)\in C$ for $\mu$-a.e. $z\in \Omega$, we also have
   \begin{equation*} \alpha<\xi\cdot x_0 =  \xi\cdot\left(\int_\Omega
 f\;d\mu\right) = \int_\Omega \xi\cdot f(z)\;d\mu(z) \leq \alpha 
 \end{equation*} which is a contradiction. Using that lemma we can prove the Jensen inequality with the assumption $\Phi$ is lower semicontinuous as following (we need $\mathrm{epi}(\Phi)$ to be closed so that we can use strictly separation of Hahn Banach theorem) Theorem [Jensen inequality for convex l.s.c. functions] Let $(\Omega,\mathcal{A},\mu)$ be a probability space, i.e., $\mu(\Omega)
 = 1$. If $f$ is a function $\Omega\longrightarrow\mathbb{R}^n$ such that it is $\mu$-integrable and if $\Phi$ is a l.s.c. convex function
   $\mathbb{R}^n\longrightarrow\mathbb{R}$ then \begin{equation*}
 \Phi\left(\int_\Omega f\;d\mu\right) \leq \int_\Omega\Phi\circ
 f\;d\mu. \end{equation*} Proof. Let us define $C = \mathrm{epi}\;\Phi = \{(x,\lambda)\in \mathbb{R}^n\times\mathbb{R} :\Phi(x)\leq \lambda\}$. Since $\Phi$ is
   convex l.s.c, $C$ is a closed convex subset of $\mathbb{R}^{n+1}$. Let
   us define  \begin{equation*}
 \mathcal{F}:\Omega\longrightarrow\mathbb{R}^n\times\mathbb{R}
 \qquad\text{by}\qquad \mathcal{F}(z) = \big(f(z),\Phi(f(z))\big).
 \end{equation*} It is clear that $\mathcal{F}(z)\in \mathrm{epi}\;\Phi
 = C$ for $\mu$-a.e. $z\in \Omega$, hence by Lemma 1 we deduce that  \begin{equation*} \int_\Omega \mathcal{F}(z)\;dz =
 \left(\int_\Omega f\;d\mu, \int_\Omega \Phi\circ f\;\mu\right) \in
 \mathrm{epi}\;\Phi \end{equation*} which gives us our desired claim. My question is, We have a version of Hahn Banach theorem saying that in $\mathbb{R}^n$, any two nonempty disjoint convex sets can be separated by a closed hyperplane (not strictly seperated). Can we use that to relax the condition $\Phi$ is lower semicontinuous in Lemma 1 so that we can generalize Jensen inquality without l.s.c. assumption?","['convex-analysis', 'functional-analysis']"
2909866,Solve: $\log_3(5(2^2+3^2)(2^4+3^4)(2^8+3^8)(2^{16}+3^{16})(2^{32}+3^{32})+2^{64})=?$,"$$\log_3(5(2^2+3^2)(2^4+3^4)(2^8+3^8)(2^{16}+3^{16})(2^{32}+3^{32})+2^{64})=?$$ At a first glance, it seems that I need to do this: $$\log_3((2+3)(2^2+3^2)(2^4+3^4)(2^8+3^8)(2^{16}+3^{16})(2^{32}+3^{32})+2^{64})=?$$ But afterwards, I can't find an algebric manipulation that will lead me to the solution, I took the exercise out of one of the entry tests of TAU.","['exponentiation', 'algebra-precalculus', 'logarithms']"
2909911,Finding the mean ($\bar{y}$ vector) given $y$ vector [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $y$ be an $n \times 1$ vector of observations. Consider the $n \times 1$ vector $\bar{y}$, which contains the mean of $y$ vector. How does one show that the $\bar{y}$ vector is given by $$\bar{y}=\vec{1} \left( \vec{1}^T \vec{1} \right)^{-1} \vec{1}^Ty$$ where $\vec{1}$ is the vector of ones (also $n \times 1$).","['statistics', 'linear-algebra', 'probability-theory', 'probability']"
2909917,Coorientation of contact structures,"When reading about contact geometry one quickly encounters the notion of a cooriented contact structure/form. But I do not seem to be able to find a definition of ""coorientation"". In some places they define a cooriented contact structure as one induced by a globally defined 1-form, but in other places (such as wikipedia) they note that coorientation implies the global definition of the contact form. Is there a general notion of coorientation on manifolds or distributions (on manifolds)? Is this related to the general notion of orientation of a manifold?","['manifolds', 'differential-topology', 'contact-topology', 'differential-geometry']"
2909949,An application of derivatives problem- to show that $f(x+2)-f(x)>2$ for $f(x)=x\cos(1/x)$,"I have this function $$f(x) = x\cos(1/x)$$ defined for all $x \geq 1$. How do I prove that 
  $$f(x+2)-f(x)>2$$ 
  for all $x\geq 1$? I found $$f'(x)= \cos(1/x) + \frac{\sin(1/x)}{x}$$
and $$f''(x)=-\frac{\cos(1/x)}{x^3}.$$ How do i use this information?","['inequality', 'functions', 'derivatives', 'real-analysis']"
2910011,"Show, by computing several values, that there are composite numbers in this sequence.","I was checking this number theory exercise: Consider the numbers $2 + 1, 2 · 3 + 1, 2 · 3 · 5 + 1, 2 · 3 · 5 · 7 + 1, · · · $ Show, by computing several values, that there are composite numbers in
this sequence. (This shows that in the proof of Euclid’s theorem, these
numbers are not necessarily prime, so it is necessary to look at prime
factors of these numbers.) Is there a way to show that all numbers are not necessarily prime by a different way than computing several values?","['number-theory', 'prime-numbers']"
2910085,Bounding the degree of very sparse random graph,"I am confused with how to manipulating with big O notation ,here is a problem from section 2.4(Exercise 2.4.3) high dimensional probability by Roman Vershynin Consider a random graph $G \sim G(n,p)$ with expected degree $d=O(1)$ . Show that with high probability , all vertices of $G$ have degrees $$O (\frac{\log n}{\log \log n})$$ I want use Chernoff bound below to bound the degree of vertices: $$Pr\left[| X-\mu|\geq \delta \mu \right] \leq 2e^ {-c\mu\delta ^2}$$ where c is a constant $\DeclareMathOperator*{\E}{\mathbb{E}} \delta \in [0,1), \mu = \E {X}$ and X is binomial random variable. how should I do that ?","['random-graphs', 'concentration-of-measure', 'network', 'asymptotics', 'probability']"
2910099,Converse of Comparison Theorems for Lebesgue Integrals,"Let $(\Omega,\mathcal{F},\mu)$ be a measure space and $f,g:\Omega\to[0,\infty]$ be two non-negative measurable functions. If $\forall A\in\mathcal{F}: \int_A fd\mu\geq \int_A gd\mu$, can we conclude $f\geq g \, \mu-$a.e.? I know that in case $f,g:\Omega\to\mathbb{R}$ and $f,g\in L^1(\mu)$ the result holds, since $\infty-\infty$ does not happen and the theorem becomes equivalent to $\forall A\in\mathcal{F}: \int_A f-gd\mu\geq0\implies f-g\geq0\, \mu-$a.e., which I know the proof. Is this still true without $L^1$ assumption for non-negative measurable functions, where integral can become $+\infty$ and how can I prove that?","['measure-theory', 'lebesgue-integral', 'real-analysis']"
2910107,Understanding a mistake regarding removable and essential singularity.,"It is a well known fact that $0$ is an essential singularity of the function $e^{1/z}$ therefore it is neither removable nor a pole. On the other hand we know the Rieamann's continuation theorem which states that an holomorphic function $f:U\setminus \{z_0\}\to \mathbb{C}$ has a removable singularity at $ z_0$ if and only if $f(z) $ is bounded in some neighborhood of $ z_0$ . I'm confussed because of the following calculation:
If we pick $\varepsilon>0 $ and $ z$ such that $|z|<\varepsilon $ then $$ |z e^{1/z} |< \varepsilon \sum^\infty_{k=0} \left|\frac{1}{k!z^k} \right|=\varepsilon \sum^\infty_{k=0} \frac{1}{k!\varepsilon^k} =\varepsilon e^{1/\varepsilon}$$ The previous means that $ 0$ is a removable singularity of $ ze^{1/z}$ which implies that $ 0$ is a pole of order 1 of $ e^{1/z}$ which cannot be since the well known fact of the begining. Of course I'm wrong in something but I couldn't tell in what.
Thank you all in advance A (possibly) useful definition My definition of a pole is: $z_0 $ is a pole of order $ m$ of the holomorphic function $f:U\setminus \{z_0\}\to \mathbb{C}$ if $m $ is the minimum integer such that $(z-z_0)^mf(z) $ has a removable singularity at $ z_0$ .","['complex-analysis', 'solution-verification', 'fake-proofs', 'singularity']"
2910173,"Given real angles $\phi_1,\ldots,\phi_N$, there exist infinitely many integers $n$ such that $\cos(n\phi_k) > 0$ for all $k$.","I wonder whether the following claim is correct: Given angles $\phi_1,\ldots,\phi_N\in\Bbb R$, there exist infinitely
  many $n\in\Bbb N$ such that $\cos(n\phi_k) > 0$ for all
  $k=1,\ldots,N$. If all $\phi_k$ are rational multiples of $\pi$, the claim is obviously true and also for $N=1$, but even for $N=2$ I cannot think of a proof.","['trigonometry', 'real-analysis']"
2910175,Why is an uncountable union of null sets not necessarily a null set?,"I ran across this statement ""...for instance, an uncountable union of null sets need not be a null set (or even a measurable set)..."" while looking through Terence Tao's blog site (See the first statement of #5). Since I'm taking a course in measure theory right now, I thought it might be relevant to understand why this is true, but I really don't know where to start. (In fact, I'm not entirely sure if this is relevant to measure theory, but Dr. Tao did mention that such a union might not even be a measurable set...). Intuition told me it might be similar to why 0 $\cdot$ $\infty$ is indeterminate, but I understood that problem to be one of definitions. As such, the only thing I have been able to come up with is that I don't understand the definition(s) either ""null set"", ""uncountable"", or ""union"" precisely enough for me to grasp yet. I'm leaning towards not fully understanding the term ""uncountable,"" as my understanding of transfinites and ordinals is sketchy at best. I was wondering if anyone could define these terms; point me towards something to read, learn, or think about; or provide an example of an uncountable union of null sets not being a null set? Edit: I didn't realize null set and the empty set were different things . Thanks to everyone for the examples and definitions!","['measure-theory', 'soft-question', 'terminology', 'real-analysis']"
2910177,Prove that the eigenvalues of a product of positive semidefinite matrices are nonnegative,"Show that if $A,B \in \mathcal M_n(\mathbb{R})$ are positive semidefinite and $\lambda$ is an eigenvalue of $AB$ then $\lambda \geq 0$. I don't really know what to do here. If $AB$ was semidefinite positive we would be done but the product of positive semidefinite matrices doesn't have to be positive semidefinite itself.","['matrices', 'linear-algebra', 'positive-semidefinite', 'eigenvalues-eigenvectors']"
2910198,Construct traceless symmetric tensors,"I understand how to create a traceless symmetric tensor, like $$ \hat{X}_{ij} = X_{ij} - \frac{1}{N}\delta_{ij}X_{hh} $$ with Einstein convention of summing over repeated indices. (By the way, I'm following here the book ""Group Theory in a Nutshell for Physicists"", by A. Zee). But for a 3-tensor I understand that you contract by pairs of indices, but why this normalization factor? $$ \hat{X}_{ijk} = X_{ijk} - \frac{1}{N + 2}(\delta_{ij}X_{hhk} + \delta_{ik}X_{hhj} + \delta_{jk}X_{hhi}) $$ What is the pattern here? How do you generalize to even higher order tensors? Thanks in advance.","['tensors', 'linear-algebra']"
2910229,An Integral Representation of Logarithmic Derivative of Zeta Function,"How to show that: 
  $$ \int_{0}^{\infty}\frac{1}{e^x-1}\left[\frac{12\,e^x}{(e^x-1)^2}-\frac{12}{x^2}+1\right]\,dx=\frac{5}{2}+\frac{\zeta'(2)}{\zeta(2)}-\frac{\zeta'(0)}{\zeta(0)}\tag{1} $$ 
  Is it possible to split the integral into two integrals representing ${\zeta'}/{\zeta}(0)$ and ${\zeta'}/{\zeta}(2)$. The logarithmic derivative of Riemann Zeta function is mainly defined by the infinte series: 
$$ \log'\left[\zeta(s)\right]=\frac{\zeta'}{\zeta}(s)=-\sum_{n=1}^{\infty}\frac{\Lambda(n)}{n^s} $$ 
Where $\Lambda(n)$ is Von Mangoldt function . ${\zeta'}/{\zeta}$ has some intresting idendities, like : 
$$ \frac{\zeta'}{\zeta}(s)+\frac{\zeta'}{\zeta}(1-s)=\log\pi-\frac{1}{2}\frac{\Gamma'}{\Gamma}\left(\frac{s}{2}\right)-\frac{1}{2}\frac{\Gamma'}{\Gamma}\left(\frac{1-s}{2}\right) $$ 
Does ${\zeta'}/{\zeta}$ has an integral representation? Thanks.","['integration', 'riemann-zeta']"
2910242,Holomorphic tangent vector field determines a infinitesimal holomorphic equivalence?,"Let $X$ be a complex compact manifold. I want to understand the holomorphic tangent vector field on $X$. I know a smooth vector field can define an infinitesimal diffeomorphism of $X$ (more precisely, a one-parameter diffeomorphism group). Similarly, if we consider a holomorphic tangent vector field, will it determine to a infinitesimal holomorphic equivalence of $X$ (a one-parameter holomorphic equivalence group)?","['manifolds', 'algebraic-geometry', 'complex-geometry']"
2910243,How to prove that $\sin(\sqrt{x})$ is not periodic? [duplicate],"This question already has answers here : Prove that $\sin(\sqrt x)$ not periodic (3 answers) Closed 5 years ago . How to prove that $\sin(\sqrt{x})$ is not periodic?
THe definition of a periodic function is $f(x+P)=f(x)$. So I assume that $\sin(\sqrt{x+P})=\sin(\sqrt{x})$. This is equivalent to $\sin(\sqrt{x+P})-\sin(\sqrt{x})=0$. This implies $2cos(\frac{\sqrt{x+P}+\sqrt{x}}{2})\sin(\frac{\sqrt{x+P}-\sqrt{x}}{2})$. What should I do next?",['trigonometry']
2910251,Urn without replacement i-th draw,"Place blue, white and red balls in a urn. Prove that the probability that the ith ball taken from the box without replacement is: $$ P(X_i=r) = \frac{r}{r+b+w}$$ There is a way to prove this result mathematically? Its true for $P(X_2 =r)$ (using total probability law), but how can I prove and interpret this result? Thanks!","['probability-theory', 'probability']"
2910263,Aperiodicity of $\sin[2x+\cos(\sqrt{2}x)]$,Is the function $f(x)=\sin[2x+\cos(\sqrt{2} \cdot x)]$ periodic? I don't know how to prove that this function is not periodic. Can you help me out?,"['periodic-functions', 'trigonometry', 'irrational-numbers']"
2910293,gradient descent method with linear exact search we have $\nabla^t f(x^k)\nabla f(x^{k+1})=0$,"Prove that in the gradient descent method with linear exact search we
  have $\nabla^t f(x^k)\nabla f(x^{k+1})=0$ We know that $x^{k+1}$ is obtained by $x^{k+1} = x^k + \alpha_k \nabla f(x^k)$, for some $0<\alpha_k$ that minimizes $f(x^k + \alpha_k \nabla f(x^k))$ I don't see why they should be orthogonal at all. Did I understand something wrong?","['optimization', 'multivariable-calculus', 'derivatives', 'nonlinear-optimization']"
2910304,"How many ways to pair 6 chess players over 3 boards, disregarding seating arrangement.","The problem is how many chess pairs can I make from $6$ players, if it doesn't matter who gets white/black pieces, and it doesn't matter on which of the $3$ boards a pair is seated. I have a possible solution which doesn't seem rigorous. Can someone tell me if 1) it's correct (the answer and logic) , 2) what is a different way to reason about it ? Seems very laborious to think of it the way I got there. I started thinking about all the possible arrangements from $6$ people, and that's $6!=720$. Now, if I think of the arrangement $A-B ; C-D ; E-F$, it's clear that within the $720$ total arrangements, I will have counted that same arrangement of pairs with each 'pair' seated on different boards  $C-D ; A-B ; E-F$ etc.. for a total of $3!=6$ per arrangement. So if I divide by that, I will basically take each arrangement such as $A-B ; C-D ; E-F$ and count it only $1$ time instead of $6$  which is what I want $\to 720/6 = 120$. So far I can think of the $120$ arrangements left as unique, fixed-position pairs. Meaning that for the arrangement of pairs $A-B ; C-D ; E-F$, I know I won't find the same pairs in different order. I finally need to remove those arrangements where we have pairs swapped, since I don't care about who is playing white/black. I am still counting 
$A-B ; C-D ; E-F$ and $B-A ; C-D ; E-F$, $A-B ; D-C ; E-F$ etc.. Because each of those pairs can be in one of two states,  $2 \cdot 2 \cdot 2 = 8$ gives me all possible arrangements where each pair swaps or doesn't. So if I divide by that number $120/8=15$ I should get the correct number.","['combinations', 'combinatorics', 'discrete-mathematics']"
2910315,How one can show the joint probability density fucntion does not exist?,"Let $X \sim \mathcal{U}(0,1)$ be an unifrom random variable and $Y = X$. I am trying to show that the joint pdf of $X$ and $Y$ does not exist. Here is what I have gone so far.
Suppose there exists a function $f(x,y)$ such that 
$$
F(x,y) = P(X \le x, Y \le y) = \int_0^x \int_0^y f(t_1,t_2)dt_1dt_2.
$$
Note that $1 = f_X(t_1) = \int_0^1 f(t_1,t_2)dt_2$ 
and $1 = f_X(t_2) = f_Y(t_2) = \int_0^1 f(t_1,t_2)dt_1$. Since $X = Y$, we have 
$$
F(x,y) = P(X \le \min\{x,y\}) = \int_0^{\min\{x,y\}} f_X(t_1)dt_1
= \int_0^{\min\{x,y\}} dt_1 = \min\{x,y\}.
$$ I was trying to show somehow $f(t_1,t_2) = 1$ based on $f_X(t_1)=1=f_Y(t_2)$ to draw a contradiction. But not sure how to do this. Any comments/answers will be very appreciated.
Thanks.","['probability-distributions', 'probability-theory', 'probability']"
2910321,"Find ratio between volume of optimal cuboid bound in ellipse, and volume of the ellipse.","I am trying to find the 'general' ratio between the cuboid of maximum volume bound in an ellipse, and the volume of that same ellipse.
After doing some partial derivatives and optimisation, it seems possible to find the ratio but looks like it would take a LONG time.
I have:
$$V_{ellipse}=4\pi*abc/3 : \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$
$$V_{cuboid}=8xyz=8xyc\sqrt{1-x^2/a^2-y^2/b^2}$$ (Attained by rearranging the ellipse formula in terms of z, and then substituting.)
After several partial derivatives:
Optimally; 
$$x=\sqrt{\frac{a^2-\frac{a^2y^2}{b^2}}{2}}$$
$$y=\sqrt\frac{b^2-\frac{b^2x^2}{a^2}}{2}$$
Then you'd proceed to sub this into the cuboid equation, and then finally divide by the volume of the ellipse. This clearly would take a long time, the equation would be a huge mess, and it would be a pain to actually calculate. Surely there is an easier way? Maybe using triple integrals or something like that?","['geometry', 'calculus', 'partial-derivative', 'optimization', 'derivatives']"
2910323,"Can ""relations"" between topological spaces be composed?","(In what follows, $\Omega = \{0,1\}.$) In set theory, we can define that a relation $X \rightarrow Y$ is a function $X \rightarrow \mathcal{P}(Y)$. This is the same as a subset of $X \times Y$, by the following argument. $$\mathcal{P}(X \times Y) \cong \Omega^{X \times Y} \cong (\Omega^Y)^X \cong \mathcal{P}(Y)^X$$ It turns out relations can be composed, and they're pretty useful. I was thinking of setting up something similar in the world of topological spaces. Define that a relation $X \rightarrow Y$ is a sheaf on $X$ valued in the topos $\mathrm{Sh}(Y)$. I'm not quite sure why I'm defining this, but I'm hopeful that there might be a connection to multivalued-functions . I'm not sure if this is the same as a sheaf of sets on $X \times Y$. Anyhoo: Question. Can ""relations"" between topological spaces be composed?","['general-topology', 'category-theory', 'sheaf-theory']"
2910332,Is every funky topology a topology?,"Whenever $X$ is a set, define that a funky topology on $X$ is a collection of subsets of $X$ deemed ""open"" such that: $X$ is open. If $A,B \subseteq X$ is open, then $A \cap B$ is open. If $\mathcal{C}$ is an open cover of $X$, then for all $A \subseteq X$, if $$\forall U \in \mathcal{C}(A \cap U \mbox{ is open}),$$ then $A$ is open. It's clear that every topology is a funky topology. Question. Does the converse hold?",['general-topology']
2910419,How to find the period of $ (\tan x )^{0.45}$?,How to find the period of $(\tan x)^{0.45}$? I know the period of $\tan x$ is $\pi$.  Can anyone please help me?,"['periodic-functions', 'functions']"
2910446,Differential Equation with Integration,"Function $f(x)$ is differentiable. Given further that \begin{eqnarray*}
f’(x)+xf’(x-1) &=& 4 \\ \int_0^1 f(xt) dt + \int_0^x f(t-1)dt &=& x^3 +x^2 +2x
\end{eqnarray*} Determine the function $f(x)$. $\textbf{Attempt}$
I know $\frac{d}{dx} \int_0^x f(t-1)dt = f(x-1)$, but I am confused about what’s $\frac{d}{dx} \int_0^1 f(xt) dt $ . Maybe I can do $ - \frac{d}{dx} \int_1^x f(xt) dt - \frac{d}{dx} \int_0^x f(xt) dt $ ?","['calculus', 'ordinary-differential-equations']"
2910454,The rate at which the value of a definite integral increases,"Say I had a function $f(x) = x^2$ ,how could I find the rate at which $$\int_{0}^{a}{x^2dx}$$ increases for $a$, or more generally for any function. Also, is this equivalent to $\frac{d}{da}f(x)$?","['integration', 'calculus']"
2910458,Question based on pigeon hole reasoning,"Suppose $k^2-k+2$ candies are distributed among a group of $k$ people ($k \geq 3$), such that every person gets at least one candy. Is it true that one person in the group got at least $k+1$ candies? So this was a question on the combinatorics test in an earlier exam. My belief is that if you take the average then $\frac{k^2-k+2}{k}=k-1+\frac{2}{k}$. So by a basic pigeon hole argument you can conclude that one of the persons got at least k candies. But is it true that someone got $k+1$? I asked a few seniors and they claim it is possible. I can't figure out how.","['combinations', 'combinatorics', 'discrete-mathematics']"
2910463,Evaluate $\int_\gamma z \ \Im(z^2) \ dz$,"I am trying to find $$\int_\gamma z\ \Im(z^2) \ dz,$$
  where $\gamma$ is the unit circle traversed once, anticlockwise. My attempt: let $\gamma(t)=e^{it}\implies \gamma'(t)=ie^{it} \ \ \ \ t\in[0,2\pi]$.
\begin{align}
\int_\gamma z\ \Im(z^2) \ dx&=\int_{0}^{2\pi} e^{it}\sin(2t) \ ie^{it} \ dt\\
&=i\int_{0}^{2\pi} e^{2it}\sin(2t) \ dt\
\end{align}
Now, I let 
\begin{align}
I&=i\int_{0}^{2\pi} e^{2it}\sin(2t) \ dt\ \\
&=i\left(\left[\frac{e^{2it}}{2i}\sin(2t)\right]_{0}^{2\pi}-\frac{1}{i}\int_{0}^{2\pi} e^{2it}\cos(2t) \ dt\right) \\
&=-\int_{0}^{2\pi} e^{2it}\cos(2t) \ dt \\
&=-\left(\left[\frac{e^{2it}}{2i}\cos(2t)\right]_{0}^{2\pi}+\frac{1}{i}\int_{0}^{2\pi} e^{2it}\sin(2t) \ dt\right) \\
&=i\int_{0}^{2\pi} e^{2it}\sin(2t) \ dt\
\end{align} Where am I going wrong? Wolfram says the answer is $-\pi$. edit I can see an alternative approach. We could express the integrand as, $$(\cos(2t)+i\sin(2t))\sin(2t)=\cos(2t)\sin(2t)+i\sin^2(2t).$$ But I prefer using integration by parts and would like to see the solution achieved via this approach.","['complex-analysis', 'contour-integration', 'proof-verification']"
2910498,Need help with tautology proof without truth tables.,"I am trying to prove $$[(p\to q)~\&~(q\to r)]\to (p\to r) $$ is a tautology using only logical laws. I have gotten part-way there but I got stuck and am not sure how to proceed. Please state any laws that you use in your answers so that I can reference them. ~ = NOT
& = AND
V = OR
-> = IMPLIES The Proof: $$\begin{array}{ll}
[(p\to q)~\&~(q\to r)]\to (p\to r); & \text{Given} \\
\sim[(\sim p\vee q)~\&~({\sim} q\vee r)]~\vee~({\sim} p\vee r); & \text{Material Implication} \\
[{\sim}({\sim} p~\vee~ q)\vee{\sim}({\sim} q\vee r)]\vee({\sim} p\vee r); & \text{DeMorgan's Law} \\
[(p~\&\,{\sim} q)\vee(q~\&\,{\sim} r)]\vee({\sim} p\vee r); & \text{DeMorgan's Law}
\end{array}$$","['propositional-calculus', 'proof-explanation', 'formal-proofs', 'logic', 'discrete-mathematics']"
2910530,Let $A$ be an infinite subset of $\Bbb N$. Then there exists a bijection from $A$ to $\Bbb N$,"Let $A$ be an infinite subset of $\Bbb N$. Then there exists a bijection from $A$ to $\Bbb N$. My attempt: We define $f:A \to \Bbb N$ by $f(a)=|\{a'\in A\mid a'<a\}|$. $f$ is injective For $a_1,a_2\in A$ and $a_1<a_2$, then $\{a'\in A\mid a'<a_1\} \subsetneq \{a'\in A\mid a'<a_2\}$, then $|\{a'\in A\mid a'<a_1\}| < |\{a'\in A\mid a'<a_2\}|$. Thus $f(a_1)<f(a_2)$. It follows that $f$ is injective. $f$ is surjective Assume the contrary that $f$ is not surjective. Let $k=\min \{n \in \Bbb N \mid n \notin \operatorname{ran}f\}$. It's clear that $0 \in \operatorname{ran}f$ since $|f(\min A)|=|\{a'\in A\mid a'<\min A\}|=|\emptyset|=0$. It follows that $0<k$. Let $k=p+1$, then $p = f(b)=|\{a'\in A\mid a'<b\}|$ for some $b \in A$. Let $c=\min \{a' \in A \mid b<a'\}$, then $\{a'\in A\mid b \le a'<c\}=\{b\}$. Next we prove $f(c)=k$. We have $\{a'\in A\mid a'<c\} =\{a'\in A\mid a'<b\} \bigcup \{a'\in A\mid b \le a'<c\}=\{a'\in A\mid a'<b\} \bigcup \{b\}$, thus $|\{a'\in A\mid a'<c\}|=|\{a'\in A\mid a'<b\}| + |\{b\}|=p+1$. Thus $f(c)=p+1=k$, and consequently $k \in \operatorname{ran}f$. This contradicts the fact that $k=\min \{n \in \Bbb N \mid n \notin \operatorname{ran}f\}$. Hence $f$ is surjective. To sum up, $f$ is bijective. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help! Update: As @saz suggested in his answer, I did not explicitly use the fact that $A$ is infinite in my proof. Let $c=\min \{a' \in A \mid b<a'\}$... To show that such $c$ does exists, we must prove that $\{a' \in A \mid b<a'\}\neq \emptyset$. This claim is justified by the fact that $A$ is infinite.","['elementary-set-theory', 'functions', 'proof-verification']"
2910534,find the value of $p(-\pi)$ and $p(0)$?,Let $p(x)$ be a polynomial of degree $7$ with real coefficient such that $p(\pi)=\sqrt3.$ and $$\int_{-\pi}^{\pi}x^{k}p(x)dx=0$$ for $0\leq k \leq 6$. Then find the value of $p(-\pi)$ and $p(0)$? If I take general polynomial then it is difficult to find and also time consuming so please help to solve. Thanks,"['polynomials', 'analysis', 'real-analysis']"
2910542,upper bound for variance when using estimated values,"Let’s say I have a parameterspace $\Omega$ with known probability density function $p(\omega), \omega \in \Omega$. I want to estimate the expectation $E[x]$ of a variable $x$ depending on $\omega \in \Omega$.  I cannot calculate directly $x$ but I can make an estimation $\tilde{x}$ such that $\tilde{x}- x \leq \epsilon, \epsilon >  0$ and $x \leq \tilde{x}$. It’s easy to prove that $|E[\tilde{x}] – E[x]| \leq \epsilon$. Is there a way to calculate a similar upper bound for  $|Var[\tilde{x}] – Var[x] |$? Kind regards,
Koen","['statistics', 'probability']"
2910547,Consecutive zeros in the binary representation of $\sqrt{3}$,"$\sqrt3=1.b_1b_2...$is the binary representation of $\sqrt3$. i.e. $\sqrt3=1+\dfrac{b_1}{2^1}+\dfrac{b_2}{2^2}+...$ Prove that at least one of the digits $b_n,b_{n+1},...,b_{2n}$ is 1. my attempt: Square both sides: $$\left(1+\sum_{i=1}^\infty\frac{b_i}{2^i}\right)^2=3$$
Expand and subtract $1$ from both sides
$$2\sum_{i=1}^\infty\frac{b_i}{2^i}+\sum_{i=1}^\infty\frac{b_i^2}{2^{2i}}=2$$
divide both side by 2
$$\sum_{i=1}^\infty\frac{b_i}{2^i}+\sum_{i=1}^\infty\frac{b_i^2}{2^{2i+1}}=1$$
Tidy up
$$\sum_{i=1}^\infty\frac{1}{2^i}\left(b_i+\frac{b_i^2}{2^{i+1}}\right)=1$$ Lemma: $\displaystyle\sum_{i=m}^\infty\frac{1}{2^i}\left(1+\frac{1}{2^{i+1}}\right)<\frac{1}{2^{m-2}}$ Proof: multiply both side by $2^{m-1}$$$\sum_{i=1}^\infty\frac{1}{2^i}+\frac{1}{2^{m+1+i}}<2$$which is trivial for any positive integer m. Proceed the proof by contradiction: suppose $b_n,b_{n+1},...b_{2n}$ are all equal to $0$. Then the sum $\sum_{i=1}^{n-1}\frac{1}{2^i}\left(b_i+\frac{b_i^2}{2^{i+1}}\right)$ is in the form of $1-\frac{x}{2^{2n-1}}$. But $$\sum_{i=2n+1}^\infty\frac{1}{2^i}\left(b_i+\frac{b_i^2}{2^{i+1}}\right)<\frac{1}{2^{2n-1}}$$by lemma. Q.E.D. Is this proof correct?
I am also happy for an alternative (and hopefully shorter) solution.","['number-theory', 'algebra-precalculus', 'proof-verification']"
2910554,"Evaluating the integral $\int_0^{\infty}\frac{x^3}{x^2+a^2}\,\mathrm{d}x$","If $\displaystyle\int_0^{\infty}\dfrac{x^3}{x^2+a^2}\,\mathrm{d}x=\large\displaystyle\dfrac1{ka^6}$, then find the value of $\displaystyle\dfrac{k}{8}$. I tried a lot but finally stuck at an intermediate form : $$\begin{align}
&\int_0^{\infty}\dfrac{x^3}{x^2+a^2}\,\mathrm{d}x, \text{with}\, {x^2=t},{2x~\mathrm{d}x=\mathrm{d}t}\\
&=\frac12\int_0^{\infty}\dfrac{(x^2)(2x)}{x^2+a^2}\,\mathrm{d}x=\frac12\int_0^{\infty}\dfrac{t}{t+a^2}\,\mathrm{d}t=\frac12\int_0^{\infty}\dfrac{t+a^2-a^2}{t+a^2}\,\mathrm{d}t\\
&=\frac12\left[\int_0^{\infty}\mathrm{d}t-\int_0^{\infty}\dfrac{a^2}{t+a^2}\,\mathrm{d}t\right]=\frac12\left[t|_0^{\infty}-a^2\ln(a^2+t)|_0^{\infty}\right]
\end{align}$$","['integration', 'improper-integrals']"
2910574,positive integer solutions to $x^3+y^3=3^z$,"I am seeking all positive integer solutions to the equation $x^3+y^3=3^z$ . After doing number crunching, I think there are no solutions. But I am unable to prove it. Attempt If $x$ and $y$ have common divisor $d$ , we have $d^3(m^3+n^3)=3^z$ . So $d$ must be a power of $3$ , and we are back to where we started. So we assume $x$ and $y$ are coprime. Testing the parity, we have sum of 2 cubes to be odd. WLOG, we can assume $x$ is even and $y$ is odd. Trying mod $3$ , we have $x+y=0 \pmod 3$ . Since $x$ and $y$ are coprime, $x$ and $y$ must be congruent to $1$ and $-1$ or vice-versa. If I assume $x=3m+1$ and $y=3n-1$ , expand out and simplify, I get $27(m^3+n^3)+27(m^2-n^2)+9(m+n)=3^z$ . If I assume $z \geq 3$ , this gives $(m^3+n^3)+(m^2-n^2)+\frac{m+n}{3}=3^{z-3}$ . But I don't see how to proceed. I also tried mod $9$ but didn't get anywhere, it didn't cut down the possibilities by much. I also tried letting $y=x+r$ . Then \begin{align*}
x^3+y^3 &= x^3+(x+r)^3 \\
&= x^3 + (x^3+3x^2r+3xr^2+r^3) \\
&= 2x^3+3x^2r+3xr^2+r^3 \\
&= 3^z
\end{align*} Then $3\mid 2x^3+3x^2r+3xr^2+r^3$ , and $3\mid 3x^2r+3xr^2$ , so this implies $3 \mid 2x^3+r^3$ . But this doesn't yield any contradiction. Can anyone supply a proof? Or if my hypothesis is wrong, how to derive all the integer solutions? Thank you.","['number-theory', 'elementary-number-theory']"
2910595,Evaluate $ ¥lim_{x ¥to 0} ¥left( {¥frac{1}{x^2}} - {¥frac{1} {¥sin^2 x} }¥right) $,"$$¥lim_{x¥to0}¥left({¥frac{1}{x^2}}-{¥frac{1}{¥sin^2x}}¥right)$$ Using the L'Hospital Rule I obtained the value $-1/4$ , but the answer is given to be $-1/3$ . I can't find the mistake. Here's what I did; please point out the mistake. ¥begin{align}
¥lim_{x¥to0}¥left({¥frac{1}{x^2}}-{¥frac{1}{¥sin^2x}}¥right)&=¥lim_{x¥to0}¥frac{(¥sin x+x)(¥sin x-x)}{(x¥sin x)(x¥sin x)} ¥¥[1ex]
&=¥lim_{x¥to0}¥left(¥frac{¥sin x+x}{x¥sin x}¥right)¥lim_{x¥to0}¥left(¥frac{¥sin x-x}{x¥sin x}¥right) ¥¥[1ex]
&=¥lim_{x¥to0}¥left(¥frac{¥cos x+1}{¥sin x+x¥cos x}¥right)¥lim_{x¥to0}¥left(¥frac{¥cos x-1}{¥sin x+x¥cos x}¥right) ¥¥[1ex]
&=¥lim_{x¥to0}¥:(¥cos x+1)¥,¥lim_{x¥to0}¥left(¥frac{¥cos x-1}{(¥sin x+x¥cos x)^2}¥right) ¥¥[1ex]
&=¥lim_{x¥to0}¥frac{-¥sin x}{(¥sin x+x¥cos x)(2¥cos x-x¥sin x)} ¥¥[1ex]
&=-¥lim_{x¥to0}¥left[¥frac{1}{1+¥cos x¥left(¥frac{x}{¥sin x}¥right)}¥right]¥left(¥frac{1}{2¥cos x-x¥sin x}¥right) ¥¥[1ex]
&=-¥frac{1}{2}¥left[¥lim_{x¥to0}¥,¥frac{1}{1+¥cos x}¥right] ¥¥[1ex]
&=-¥frac{1}{4}
¥end{align}","['limits', 'calculus']"
