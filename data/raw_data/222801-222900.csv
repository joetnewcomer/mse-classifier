question_id,title,body,tags
4573972,An intelligent way to determine the center of the special linear group of order 2 over the field of order 3.,"The Problem : Show that the center of $SL_2(\mathbb{F}_3)$ is the group of order $2$ consisting of $\pm\mathit{I}$ , where $\mathit{I}$ is the identity matrix. My Question : Obviously one can list all the elements in $SL_2(\mathbb{F}_3)$ and compute the center using brute force; but is there an easy way? I tried to make use of The Class Equation : $$|G|=|Z(G)|+\sum_{i=1}^r|G: C_G(g_i)|$$ ( $g_i$ 's are the representatives of the distinct conjugacy classes of $G$ not contained in $Z(G)$ ), to no avail. Any help would be greatly appreciated.","['matrices', 'group-theory', 'abstract-algebra']"
4574049,"In this checkerboard problem, is there a way to tell if any two situations are equivalent?","There is an infinite square grid chessboard with chess pieces placed on certain squares. There is at most one piece in a grid. We can perform the following operations each time: Split: Select a chess piece $(x,y)$ , if $(x+p,y)(x,y+q)(x+p,y+q)$ are empty, then delete it and put one chess piece in each of the three places. $p,q$ are valued between $\{-1,1\}$ ; Merge: Select a empty grid $(x,y)$ , if $(x+p,y)(x,y+q)(x+p,y+q)$ each has a chess piece, then delete the three chess pieces and add a chess piece to $(x,y)$ . $p,q$ are valued between $\{-1,1\}$ (just the inverse process of Split); In each operation we can decide p and q as we want. Given two situations, they are called equivalent if they can be converted to each other through a number of operations. For example, the following operations shows that the board with
pieces $(0,0),(0,1),(1,2)$ is equivalent to the board with pieces $(0,0),(-1,1),(0,2),(-1,-1),(1,-1)$ . A white piece means that it will be splited in the next step, while orange means that they will be merged in the next step. Is there an algorithm that can determine the equivalence of any two situations? Furthermore, is there an algorithm that can determine the equivalence of any two cases in polynomial time complexity?","['combinatorics', 'discrete-mathematics', 'algorithms']"
4574053,$\sin x+\frac{\sin 3x}{3}+\frac{\sin 5x}{5}+...+\frac{\sin(2n-1)x}{2n-1}>0$,"Let $x$ be a real number with $0<x<\pi$ . Prove that for all natural
numbers $n$ , $$\sin x+\frac{\sin 3x}{3}+\frac{\sin
5x}{5}+...+\frac{\sin(2n-1)x}{2n-1}>0$$ By induction, assume that $$F_n(x)=\sum_{k=1}^{n}\frac{\sin(2k-1)x}{(2k-1)}$$ So, $F_1(x)=\sin x>0$ for $x \in (0,\pi).$ Thus the inequality is true for $n=1$ . Let $F_r(x)>0$ for $r=1,2,...,n-1$ . We will deduce that $F_n(x)>0$ for $x\in(0,\pi)$ . Suppose that $F_n(x_0)\leq 0$ for some $x_0\in(0,\pi)$ and that $F_n(x)$ attains its minimum at $x=x_0$ . Hence $$\frac{d}{dx}[F_n(x)]_{x=x_0}=0$$ . $$\begin{align}\therefore F'_n(x_0)&=\sum_{k=1}^{n}\cos((2k-1)x_0)=0
\\2\sin x_0F'_n(x_0)&=\sum_{k=1}^{n}2\cos ((2k-1)x_0)\sin x_0
\\&=\sum_{k=1}^{n}[\sin(2kx_0)-sin((2k-2)x_0)]
\\&=\sin 2nx_0 \end{align}$$ $$F'_n(x_0)=\frac{\sin 2nx_0}{2 \sin x_0}=0$$ $$\sin 2nx_0=0$$ $$x_0 \in\{\frac{\pi}{2n},\frac{2\pi}{2n},\frac{3\pi}{2n},...,\frac{2n-1}{2n}\}$$ Each of these values will make $F_n(x_0)>0$ , a contradiction. Therefore $F_n(x)>0$ for $x \in (0,\pi)$ How could we prove the proposed inequality by not using calculus? Updated:
Realise there is a similar question asked but no solutions are proposed by not using calculus. Positivness of the sum of $\frac{\sin(2k-1)x}{2k-1}$.","['calculus', 'trigonometry', 'inequality']"
4574142,Are there general methods classifying isotopies between two given embeddings?,"We can define a different isotopy from i
to f
by Eâ€²(x,y,t)=((1âˆ’t)x+tan(tarctan(x/1âˆ’y)),(1âˆ’t)y)
. In this case, we are opening out C
left and right of the North pole while simultaneously stretching it and flattening it onto the x
-axis, where we arrive at t=1
. However, this time our isotopy does not have fixed points and produces bounded arcs for all t<1
. In summary, we have two isotopies from i
to f
. On the one hand E
produces all unbounded arcs but one and has fixed points, while Eâ€²
produces all bounded arcs but one and has no fixed points. Do features like these may in any way allow us to group/classify the set of isotopies between two given embeddings? Is there a general theory for this in the different categories (TOP, DIFF, PL)? Following on this, in the related question, the embedding i
remains the inclusion, but then an embedding f
is defined, which I will call f1
here. The corresponding isotopy from i
to f1
is given by E1(x,y,t)=(1âˆ’t)i(x,y)+tf1(x,y)
. Visually here we are opening up C
left and right of the North pole, without much stretching, while simultaneously compressing it until it lies flattened onto the interval (âˆ’Ï€/2,Ï€/2)Ã—{0}
when we arrive at t=1
. Note how the process produces bounded arcs for all t
this time. Moreover, the isotopy has no fixed points.","['multivariable-calculus', 'calculus', 'functions', 'maxima-minima']"
4574146,On the number of generators of power of ideal in a local ring,"let $(R,m)$ be a local ring and let $I,J$ be an ideals of $R$ , let $\mu(I)$ be the minimal number of generators of $I$ . I've proved that $\mu(IJ)\leq\mu(I)$ but somehow this claim does not feel right to me. actually in here the authors go to great length to prove that if $\mu(I^n)\leq n$ , then for every $r>n$ , $\mu(I^r) \leq n$ . while by my proof this is trivial. my proof is: $I/Im$ is vector space over $R/m$ and $(IJ+Im)/Im$ is subspace of $I/Im$ so the dimension of $(IJ+Im)/Im$ is less then or equals $\mu(I)$ so $IJ+Im$ can be generated by $\mu(I)$ elements. let $A$ be a set containing $\mu(I)$ generators of $IJ+Im$ so $(A)=IJ+Im$ and by Nakayama's lemma $(A)=IJ$ . is this right? and if not, where is my mistake?","['local-rings', 'abstract-algebra']"
4574165,Example to show strong law of large numbers is stronger than weak law of large numbers,"I am aware there are already plenty of answers to this question by reasoning from definition of almost sure convergence versus convergence in probability, so I would like to see an example which satisfies WLLN but not SLLN (I am kind of a learn-by-example learner).","['measure-theory', 'probability-limit-theorems', 'probability-theory']"
4574195,Limited operator,"I am trying to prove that the given operator is bounded in the $L_2[0, 1]$ space: $$ (Ax)(t) = t^{r-1} \int\limits^{t}_{0} \frac{x(s)}{s^r} ds
$$ The way I'm trying to do it is via a HÃ¶lder's inequality, Fubini's theorem and also using Hardy's inequality for integrals. I know that $r$ must be less than $\frac{1}{2}$ in order for the operator above to be bounded. However, I am unable to prove that. Can anyone help me? Thank you.",['functional-analysis']
4574235,A weaker condition of the definition of topology,"Suppose that $(X,\tau)$ is a pair in which $X$ is a non-empty set and $\tau \subset 2^X$ with these conditions: (1) $X,\emptyset \in \tau$ , (2) $\tau $ is closed under arbitrary unions, and (3) $U_1 \cap U_2 \neq \emptyset$ implies $\operatorname{int}(U_1 \cap U_2 )\neq \emptyset$ for every $U_1 ,U_2 \in \tau$ . Is there a pair $(X,\tau)$ satisfying conditinons (1), (2) and (3) but it is not a topological space. Here I define "" $\operatorname{int}$ "" with respect to $\tau$ itself. $\operatorname{int}(A)$ , $A\subset X$ , is defined as the union of all sets $B\in \tau$ with $B\subset A$ . A pair $(X,\tau)$ satisfying (1) and (2) is called a generalized topology in the literature. So in other words, my question is that: Is there a generalized topological space (which is not a topological space) satisfying the condition (3)?",['general-topology']
4574262,Counter-example: approximate an integrable function from below by a continuous function with compact support,"I'm reading this thread in which the OP asked if the following statement is true, i.e., Let $f$ be a non-negative measurable function and suppose that $f\in L^p(\Bbb R^n)$ , then for any $\delta \gt 0$ there exists a non-negative continuous function with compact support, note as $h_\delta$ , such that (1) $\int |f(x)|^p dx - \delta^p \le \int|h_\delta(x)|^pdx$ and (2) $h_\delta(x)\le f(x)$ for a.e. $x\in \Bbb R^n$ . Then @geetha290krm commented that Take $f$ to the characteristic function of a fat Cantor set for a counter-example. In this case $h_\delta \equiv 0$ . I'm trying to formalize @geetha290krm's idea as follows. Let $\lambda$ be the Lebesgue measure on $E := [0, 1]$ . We work with $L_1 (E, \lambda, \mathbb R_{\ge 0})$ . Let $C$ be the fat Cantor set. Then $\lambda(C)>0$ . Let $f := 1_C$ . Then $f=0$ on $E \setminus C$ . Because $h_\delta (x) \le f (x)$ for $\lambda$ -a.e. $x \in E$ , there is a $\lambda$ -null subset $N$ of $E$ such that $h_\delta \le f$ on $E \setminus N$ . It follows that $h_\delta = 0$ on $E \setminus  (C\cup N)$ and thus on $E \setminus  (\overline C \cup N)$ . Because $h_\delta$ is continuous, we get $h_\delta=0$ on $\overline{E \setminus (\overline C \cup N)}$ . Because $C$ is nowhere dense, we get $$
\overline{E \setminus \overline C} = E
\quad \text{and thus} \quad
\lambda \left ( \overline{E \setminus \overline C}  \right ) = 1.
$$ To show that $h_\delta=0$ $\lambda$ -a.e., it suffices to show that $$
\lambda \left ( \overline{E \setminus  (\overline C \cup N)} \right ) = 1.
$$ Then I got stuck. Could you elaborate how finish the proof?","['measure-theory', 'lebesgue-measure', 'examples-counterexamples', 'real-analysis', 'continuity']"
4574266,solutions of Initial value problem of system of differential equations,"I have problems when reading this theorem from the book. I am confused that why here $$
u(\tau)=u^{\prime}(\tau)=\ldots=u^{(n-2)}(\tau)=0, \quad u^{(n-1)}(\tau)=1
$$ but $$y\left(t_0\right)=y^{\prime}\left(t_0\right)=\ldots=y^{(n-1)}\left(t_0\right)=0
$$ And why in the example below the theorem, $y(1)=y'(1)=0$ changed to $y(1)=2,y'(1)=5$ later? Could someone please explain to me! Thanks in advance.
\\ THEOREM 6. Let $L(t, \lambda)=\lambda^n+a_{n-1}(t) \lambda^{n-1}+\ldots+a_1(t) \lambda+a_0(t) \quad$ and $\quad D=\frac{d}{d t}$ .
Denote by $E(t, \tau)$ the uniquely determined solution $u(t)$ of the initial value problem
(16) $\quad L(t, D) u=0$ $$
u(\tau)=u^{\prime}(\tau)=\ldots=u^{(n-2)}(\tau)=0, \quad u^{(n-1)}(\tau)=1
$$ Then
(17) $$
y(t)=\int_{t_0}^t E(t, \tau) g(\tau) d \tau
$$ is the solution of the problem $$
\begin{aligned}
&L(t, D) y=g(t) \\
&y\left(t_0\right)=y^{\prime}\left(t_0\right)=\ldots=y^{(n-1)}\left(t_0\right)=0
\end{aligned}
$$ Example 6. Find the solution of the differential equation
(18) $t^2 y^{\prime \prime}-2 t y^{\prime}+2 y=t^2 \sin t^4, \quad t>0$ , which satisfies $y(1)=2, y^{\prime}(1)=5$ .
Solution: The homogeneous equation $t^2 y^{\prime \prime}-2 t y^{\prime}+2 y=0$ is an Euler equation. The indicial equation is $\lambda(\lambda-1)-2 \lambda+2=0$ , with roots $\lambda=1, \lambda=2$ . The general solution of the homogeneous equation is thus $$
y(t)=C t+D t^2 \quad(C, D \text { constants }) .
$$ We now find the fundamental solution. By (16), this must satisfy the conditions $y(\tau)=0, y^{\prime}(\tau)=1$ . We obtain the following system of equations for $C$ and $D$ : $$
\left\{\begin{array} { l } 
{ C \tau + D \tau ^ { 2 } = 0 } \\
{ C + 2 D \tau = 1 }
\end{array} \quad \Longleftrightarrow \quad \left\{\begin{array}{l}
C=-1 \\
D=1 / \tau
\end{array}\right.\right.
$$ It follows that $$
E(t, \tau)=-t+\frac{t^2}{\tau}, \quad t, \tau>0
$$ By theorem 6 $$
\bar{y}(t)=\int_1^t\left(-t+\frac{t^2}{\tau}\right) \sin \tau^4 d \tau, \quad t>0
$$ is that solution of equation (18) which satisfies $y(1)=y^{\prime}(1)=0$ . (We have divided (18) by $t^2$ to make the coefficient of $y^{\prime \prime}$ equal to 1 .) The general solution of $(18)$ is obtained by adding the general solution of the homogeneous equation, i.e., $$
y(t)=\bar{y}(t)+C t+D t^2
$$ The conditions $y(1)=2, y^{\prime}(1)=5$ lead to $C+D=2, C+2 D=5$ , resulting in $C=-1, D=3$ . Hence the required solution is $$
y(t)=\int_1^t\left(-t+\frac{t^2}{\tau}\right) \sin \tau^4 d \tau-t+3 t^2, \quad t>0
$$","['initial-value-problems', 'systems-of-equations', 'ordinary-differential-equations']"
4574267,Bounds on Jacobian in terms of Volume Distortion,"Let $V\subset U\subseteq \mathbb{R}^k$ and let $\varphi:U\to V$ be a diffeomorphism. Let $J_{\varphi}$ be its Jacobian, so $vol(V) = \int_U |\det J_{\varphi}|$ . I am wondering if there are any general purpose lower bounds on $|\det J_{\varphi}(u)|$ over all $u\in U$ . For example, if $\varphi$ is measure/volume-preserving, $|\det J_{\varphi}| = 1$ . My intuition is that something like $$|\det J_{\varphi}|\ge \inf_{S\subset U}\frac{vol(S)}{vol(\varphi(S))}$$ should hold (with the convention that $\tfrac{0}{0} = 1$ ), but I'm unable to find any resources (and have not been able to furnish a proof nor counterexample -- my multivariable calculus is very rusty). Also, for my particular purpose I know that $vol(\varphi(S))\le vol(S)$ for all $S\subset U$ . Any help/hints are appreciated! EDIT: My intuition also says that an inequality of the other direction should hold as well: $$|\det J_{\varphi^{-1}}|\le \sup_{T\subset V}\frac{vol(\varphi^{-1}(T))}{vol(T)}.$$ Also, in my particular setup, $U = [0,1]^k$ . An idea I have for this is to bound the Jacobian by that of a linear map that $\phi([0,s]^k) = [0,1]^k$ that scales points up by a factor $1/s$ (for $0 < s < 1$ ). We'd choose $s$ to be minimal such that $[0,s]^k\supset V$ or something. But I'm not sure how to make meaningful use of this train of thought.","['jacobian', 'multivariable-calculus', 'diffeomorphism', 'differential-geometry']"
4574289,How many ways distinguishable objects can be distributed amongst distinguishable groups with restrictions.,"I am asked to find the total number of ways I can distribute  12 distinguishable objects  amongst 3 distinguishable groups (ð‘¥1,ð‘¥2,ð‘¥3) such that ð‘¥1 receives at least 2 objects and ð‘¥2,ð‘¥3 receive at least one each. I sense this is related to PIE but I am struggling in applying the principle to this case.
This is my attempt so far: Total number of outcomes including unusable ones = $3^{12}$ = 531441 Let |X1| be the outcomes where x1 received no objects and let |X2| be the set of outcomes where x2 received no objects and let |X3| be the set of outcomes where x3 received no objects. There are $2^{12}$ possible outcomes here. Then |X1 $\cap$ X2|, or |X1 $\cap$ X3| = $1^{12}$ and |X1 $\cap$ X2 $\cap$ X3| = $0^{12}$ Let |X11| be the set of outcomes where ð‘¥1 received only 1 object = $2^{11}$ Now to exclude outcomes where any group received 0 objects: ${3 \choose 1}$ $2^{12}$ - ${3 \choose 2}$ $1^{12}$ + ${3 \choose 3}$ * $0^{12}$ = 12286 531441 - 12286 = 519155 Now to exclude outcomes where any x1 received just one object: 519155 - $2^{11}$ = 517107. Am I close? All advice and constructive criticism would be greatly appreciated!","['combinations', 'inclusion-exclusion', 'combinatorics', 'discrete-mathematics', 'probability']"
4574310,Gibbs sampler for a posterior distribution of hierarchical model,"I'm trying to get a posterior distribution of hierarchical model by using a Gibbs sampler. I haven't dealt with hierarchical models so far and I'm not really familiar with the Gibbs sampler so I'm a bit confused. (1) $y_{i}|\theta_{i},\mu \stackrel{ind}\sim N(\theta_{i},\sigma^{2})$ for i =1,2,...,n ( $\sigma^{2}$ is known) (2) $\theta_{i}|\mu \stackrel{iid}\sim N(\mu,\tau^{2})$ for i =1,2,...,n (3) $\pi(\mu,\tau^{2})\propto \Large\frac{\tau^{2}}{\sigma^{2}+\tau^{2}}$ First Question The thing I want to know is $P(\theta_{i}|y_{i})$ so I'm thinking of getting $(\theta_{(1)},\mu_{(1)},\tau^{2}_{(1)})$ , $(\theta_{(2)},\mu_{(2)},\tau^{2}_{(2)})$ , $(\theta_{(3)},\mu_{(3)},\tau^{2}_{(3)})$ ..... by using a gibbs sampler and then just pulling out only $\theta_{1},\theta_{2},\theta_{3},...$ Then does distribution of $\theta_{i}$ is P $(\theta_{i}|y_{i})$ that I'd like to know? If that's the case...
I actually calculated each of full conditional distributions to use a Gibbs sampler and I'm having trouble getting this one because of the leftmost term. $P(\tau^{2}|\theta_{1},..,\theta_{n},y_{1},..,y_{n},\mu) \large\propto(\tau^{2})^{-\frac{n}{2}}exp[-\frac{\sum(\theta_{i}-\mu)^2}{2r^2}]\large\frac{\tau^{2}}{\sigma^{2}+\tau^{2}}$ I guess I could use Inverse Gamma distribution if I could just get rid of the leftmost term $\large\frac{\tau^{2}}{\sigma^{2}+\tau^{2}}$ and change a little bit. $P(\tau^{2}|\theta_{1},..,\theta_{n},y_{1},..,y_{n},\mu) \large\propto(\tau^{2})^{-\frac{n-2}{2}-1}exp[-\frac{\sum(\theta_{i}-\mu)^2}{2r^2}] \sim IG(\frac{n-2}{2},\frac{\sum(\theta_{i}-\mu)^2}{2})$ Second Question Since we are calculating in proportional, can I consider that term as just 1? ( $\Large\frac{\tau^{2}}{\sigma^{2}+\tau^{2}} \propto 1$ ) so I could use Inverse Gamma distribution. Can anyone help me with this? Anything is welcome. I'm kinda eager to know this:)
Thank you.","['statistics', 'probability-distributions', 'bayesian', 'probability']"
4574373,"If $\lim\limits_{m\to\infty}\frac{f(a+h_m)-f(a)}{h_m}$ does not exist, with $\{h_m\}\to 0$, why can we conclude $f$ not differentiable at $a$?","If we have a function $f(x)$ and we prove that at a point $a$ the limit $$\lim\limits_{m\to\infty} \frac{f(a+h_m)-f(a)}{h_m}$$ does not exist, where $\{h_m\}$ is a sequence that converges to $0$ , why can we conclude that $f$ is not differentiable at $a$ ? In other words, why can we conclude something about the differentiability of a function at a point $a$ based on the limit of a sequence? I think it is related to the following theorem Spivak, Calculus , Ch. 22, Theorem 1 Let $f$ be a function defined in an open interval containing $c$ except perhaps at $c$ . Then $$\lim\limits_{x\to c} f(x)=l$$ $$\iff$$ for every sequence $\{a_n\}$ such that each $a_n$ is in the domain of $f$ each $a_n\neq c$ $\lim\limits_{n\to\infty} a_n=c$ the sequence $\{f(a_n)\}$ satisfies $$\lim\limits_{n\to\infty} f(a_n)=l$$ Let $\{a_n\}=\{a+h_m\}$ be any sequence with $\{h_m\}$ converging to $0$ . Then $a_n\in \mathbb{R}$ $a_n\neq a$ $\lim\limits_{n\to\infty} a_n=a$ Let $g(x)=\frac{f(x)-f(a)}{x-a}$ . Then, $g(a_n)=\frac{f(a+h_m)-f(a)}{h_m}$ is a sequence but we showed previously that it doesn't converge, ie $$\lim\limits_{n\to\infty} g(a_n)$$ does not exist. This seems to mean that the consequent of the theorem above is false. Hence the antecedent is false. And that means that the limit $$\lim\limits_{x\to a} g(x)=\lim\limits_{x\to a} \frac{f(x)-f(a)}{x-a}$$ does not exist, and therefore, $f$ is not differentiable at $a$ . Is this the underlying justification for our conclusion about differentiability of the function $f$ at $a$ based on a limit of a sequence?","['calculus', 'derivatives', 'sequences-and-series']"
4574407,Conditional expectation of random variable parameterized by random variable,"Suppose $M(\lambda)$ is a distribution that satisfies for all $Z\sim M(\lambda)$ that $E[Z]=\lambda$ . Does it follow that if $X \sim M(Y)$ then $E[X | Y] = Y$ a.s.? Intuitively this seems to be the case, but how does one prove it, if it is true? Since this has been cause of a lot of confusion; some clarification: Fix a probability space $(\Omega,\mathcal F, P)$ so that it can accommodate the following random variables. There is only this probability space involved. $Y$ is a random variable with expectation that takes values in $\mathbb R$ and for every $\lambda\in\mathbb R$ , $M(\lambda)$ is a distribution on $\mathbb R$ . Suppose $Q_\lambda$ is the measure with distribution $M(\lambda)$ . I am assuming $Q_\lambda$ to be defined on the Borel sets of $\mathbb R$ . I am defining $X\sim M(Y)$ to mean that $E[1_{\{X\in A\}}|Y] = P(X\in A | Y) = Q_Y(A)$ for all measurable $A\subseteq\mathbb R$ $P$ -a.s. The conditional expectation $E[X | Y]$ is then defined as usual, as the a.s. uniquely determined random variable $H$ satisfying $E[ZH] = E[ZX]$ for all $\sigma(Y)$ -measurable bounded $Z$ .","['conditional-expectation', 'probability-distributions', 'probability-theory']"
4574418,What is the definition of a subspace?,"I have seen people give the definition that a subspace is a vector space contained within a vector space. But is this definition actually accurate? Isn't this a special case, in particular the definition of a linear/vector subspace? For example, I have seen hyperplanes described as subspaces, but they do not contain the zero vector (affine subspaces). Shouldn't the correct definition of a subspace be something like this:
A subset of a set from a space such that the structure of that space still holds on the subset. In other words, my question is whether there is a general definition of a subspace.","['general-topology', 'definition', 'vector-spaces', 'metric-spaces']"
4574432,"Is it decidable that $A^nx=B^mx$ for some $n,m\geq1$?","Let $A$ and $B$ be $k\times k$ matrices with integer coefficients and $x \in \mathbb{Z}^k$ be a fixed vector. Is there and algorithm to determine if there exist $n, m \geq 1$ satisfying $A^nx = B^mx$ ? Such an algorithm would be very useful for a problem in which I am working on.
Unfortunately, I don't have much intuition about its existence. In my case, $A$ , $B$ and $x$ have positive entries, but I suspect that this is not relevant. Any comment is appreciated.","['matrices', 'linear-algebra', 'algorithms']"
4574461,Pigeonhole principle with unique sequence,"Find a sequence of 29 positive integers $a_1, ... , a_{29}$ such that $a_1+...+a_{29} = 49$ and
no consecutive string of numbers in this sequence adds up to 10. And prove that this sequence is the only one that satisfies this condition. I know how to prove a statement like this: $a_1, ..., a_{20}$ are positive integers that sum to 30 and some consecutive group of these numbers is equal to 9. Using the pigeonhole principle. Applying the idea I used for that to this problem, I begin by defining $b_n = \sum^n_{i=1} a_i$ .
Therefore, $b_{29} = 49$ I want to find a sequence of numbers such that for all $m > n$ $b_m - b_n= \sum^m_{i=n+1}a_i$ never equals 10. Defining $c_n = b_n + 10$ I think I need to prove that $c_n \neq b_n$ for all n. I am stuck here.","['pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
4574468,Why is the transpose related to the dual space?,"A matrix with $m$ rows and $n$ columns in the real numbers is a map from $M : \mathbb{R}^n \to \mathbb{R}^m$ ; the transpose of this matrix is then a map $M^T: \mathbb{R}^m \to \mathbb{R}^n$ . However, it seems like the transpose is related to the dual space, i.e., here $M^T: \mathbb{R}^{m*} \to \mathbb{R}^{n*}.$ Intuitively, why is it natural to associate the transpose with the dual space? It doesn't seem to follow with our intuition when dealing with matrices.",['linear-algebra']
4574506,Find all the solutions for $f\left(x\right)	=2f\left(\frac{1}{x}\right)-\frac{2x^{2}-1}{x^{2}+1}$,"The function is $f:\left(0,\infty\right)\rightarrow\mathbb{R}$ , I tried to do it like that, first I saw that: $$f\left(x\right)	=2f\left(\frac{1}{x}\right)-\frac{2x^{2}-1}{x^{2}+1}$$ and then decided to try to put $\frac{1}{x}\in\left(0,\infty\right) $ and came out with: $$f\left(\frac{1}{x}\right)	=2f\left(x\right)-\frac{2\frac{1}{x^{2}}-1}{\frac{1}{x^{2}}+1}
	=2f\left(x\right)-\frac{\frac{2-x^{2}}{x^{2}}}{\frac{1+x^{2}}{x^{2}}}
	=2f\left(x\right)-\frac{2-x^{2}}{1+x^{2}}$$ i used it at the last equation and it came out that: $$f\left(x\right)=\frac{5-4x^{2}}{3\left(1+x^{2}\right)}$$ now i get $f\left(1\right)=\frac{1}{6}$ but i know it should equal 1/2... I have now idea how to move forward, and why it doesn't work, any tips?",['functions']
4574507,Using Godement's criterion to prove that leaf space of a foliation carries a smooth structure compatible with the quotient topology.,"I am trying to prove the following from Differential Geometry by
Rui Loja Fernandes: Let $\mathcal{F}$ be a foliation of a smooth manifold $M$ . The following
statements are equivalent: There exists a smooth structure on $M/\mathcal{F}$ , compatible with the quotient
topology, such that $Ï€ : M â†’ M/\mathcal{F}$ is a submersion. The leaf space $M/\mathcal{F}$ is Hausdorff and there is a cover of $M$ by foliated
charts with the property that each leaf of F intersects each chart at
most once. The Book says to use Godement's criterion which says that: Let $M$ be a smooth manifold and let
âˆ¼ be an equivalence relation on $M$ . The following statements are equivalent: There exists a smooth structure on $M/ âˆ¼$ , compatible with the quotient
topology, such that $Ï€ : M â†’ M/ âˆ¼$ is a submersion. The graph $R$ of âˆ¼ is a proper submanifold of $M Ã—M$ and the restriction
of the projection $p_1 : M Ã— M â†’ M$ to $R$ is a submersion. Where $$ R= \{(x,y)\in M\times M; xâˆ¼y\}$$ I have shown that 1 implies 2. I am having trouble trying to show that 2 implies 1, in particular, that $R$ is a submanifold. I believe that having done that all of the rest follows straightforwardly from the criterion. Any tips/suggestions? (Also if you know of any other way to show this please let me know !)","['quotient-spaces', 'foliations', 'differential-geometry']"
4574532,Trying to understand the formula for counting labelled directed acyclic graphs?,"Let $a_n$ represent the number of directed acyclic graphs on $n$ vertices. Then the wikipedia , gives me the following recurrence: $$a_n = \sum_{k=1}^{n}(-1)^{k-1}\binom{n}{k}2^{k(n-k)}a_{n-k}$$ This formula shouts some sort of inclusion-exclusion, but I don't understand how it is derived. For example: the first term in the formula for $k=1$ is $n2^{n-1}a_{n-1}$ , and I don't understand why is this itself not equal to $a_n$ , because intuitively give $a_{n-1}$ ,you add one node and assume it to be highest in the order in comparison to all other nodes and now you have $n$ ways of doing this due to labelling and $2^{n-1}$ of connecting the $n-1$ nodes. Where is the over-counting ? If some one could point me to an inclusion-exclusion based proof then that would be great. I am not looking for a generating functions based proof.","['graph-theory', 'inclusion-exclusion', 'combinatorics']"
4574574,Converse of Slutsky's theorem (or counterexample)?,"Question Suppose $X_n \overset{d}{\longrightarrow} X$ and $Z_n \overset{p}{\nrightarrow} 0$ . Does it follow that $X_n + Z_n \overset{d}{\nrightarrow} X$ . If not, what's a counterexample? Thoughts This would be a sort of converse to Slutsky's theorem. If $Z_n \overset{p}{\longrightarrow} 0$ , then Slutsky's theorem would tell us that $X_n + Z_n \overset{d}{\longrightarrow} X$ . I imagine this isn't true, since it would probably just be included in standard formulations of Slutsky's theorem if it were. But it would be nice to have a counterexample",['probability-theory']
4574575,Intuition behind derivative as the best linear approximation,"I'm trying to understand how derivative for functions $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ works, which is defined as ""the best linear approximation"". Intuitively, given $T,M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ and $a\in\mathbb{R}^n$ , the affine approximation at $a$ , $\ell(x) = f(a) + T(x-a)$ is better than the approximation $F(x) = f(a) + M(x-a)$ if $||f(x) - \ell(x)|| \rightarrow 0$ faster than $||f(x) - F(x)||$ does as $x\rightarrow a$ , i.e, $$\lim_{x\to a} \frac{||f(x) - (f(a) + T(x-a))||}{||f(x) - (f(a) + M(x-a))||} = 0$$ The intuition says that if $T\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ is such that for all $M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ with $M\not=T$ the previous limit is zero (this means that $\ell$ is a better affine approximation than any other affine approximation), then $T$ should be the derivative of $f$ at $a$ , for functions of one variable this is true, i.e. $\textbf{Proposition.}$ Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a function, $a\in\mathbb{R}$ and $L\in\mathcal L(\mathbb{R})$ , the following statements are equivalent: (a) $f$ is differentiable at $a$ with derivative L: $$\lim_{x\to a} \frac{|f(x) - (f(a) + L(x-a))|}{|x-a|} = 0$$ (b) For all $M\in\mathcal L(\mathbb{R})$ with $M\not= L$ , we get: $$\lim_{x\to a} \frac{|f(x) - (f(a) + L(x-a))|}{|f(x) - (f(a) + M(x-a))|} = 0$$ My question is: Is this true for functions in higher dimensions? i.e. $\textbf{Proposition.}$ Let $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ be a function, $\hat{a}\in\mathbb{R}^n$ and $L\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ , the following statements are equivalent: (a) $f$ is differentiable at $\hat{a}$ with derivative L: $$\lim_{x\to \hat{a}} \frac{||f(x) - (f(\hat{a}) + L(x-\hat{a}))||}{||x-\hat{a}||} = 0$$ (b) For all $M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ with $M\not= L$ , we get: $$\lim_{x\to \hat{a}} \frac{||f(x) - (f(\hat{a}) + L(x-\hat{a}))||}{||f(x) - (f(\hat{a}) + M(x-\hat{a}))||} = 0$$ Is the above statement true?","['multivariable-calculus', 'real-analysis']"
4574605,Why does $g_x$ depends continuously on $x$?,"In Vector Bundles and K-Theory (version 2.2, p.8), Hatcher explains: Lemma 1.1. A continuous map $h:E_1\to E_2$ between vector bundles over the same base space $B$ is an isomorphism if it takes each fiber $p_1^{-1}(b)$ to the corresponding fiber $p_2^{-1}(b)$ by a linear isomorphism. Proof: The hypothesis implies that $h$ is one-to-one and onto. What must be checked is that $h^{-1}$ is continuous. This is a local question, so we may restrict to an open set $U\subset B$ over which $E_1$ and $E_2$ are trivial. Composing with local trivializations reduces to the case that $h$ is a continuous map $U\times\mathbb{R}^n\to U\times\mathbb{R}^n$ of the form $h(x,v)=(x,g_x(v))$ . Here $g_x$ is an element of the group $GL(n,\mathbb{R})$ of invertible linear transformations of $\mathbb{R}^n$ , and $g_x$ depends continuously on $x$ . This means that if $g_x$ is regarded as an $n\times n$ matrix, its $n^2$ entries depends continuously on $x$ . The inverse matrix $g_x^{-1}$ also depends continuously on $x$ since its entries can be expressed algebraically in terms of the entries of $g_x$ , namely $g_x^{-1}$ is $1/(\det g_x)$ times the classical adjoint matrix of $g_x$ . Therefore $h^{-1}(x,v)=(x,g_x^{-1}(v))$ is continuous. $\square$ The part that confuses me is in bold. Why does $g_x$ depends continuously on $x$ ?","['vector-bundles', 'differential-geometry']"
4574613,"a group consists of 21 members, 11 of which are women 10 are men . A new committee of 8 members is to be selected, with precisely 4 women.","the problem I'm facing is in regards to when I have a restriction in how to put together the committee. i have the following restriction. ""Two of the male students in the council strongly dislike each other, and should not be in on the committee together"" The solution i have so far: It just seems like a way too large a number. Is my method correct?
Please also note that there needs to be precisely 4 women in the group at all time.","['combinatorics', 'discrete-mathematics']"
4574632,Smallest overlapping circles containing unit circles in each section,"This is a question inspired by the number of arrangements of n circles in the affine plane which was discussed on Numberphile . One way of overlapping 3 circles looks like this: My informal question is: what are the sizes of the 3 smallest circles for this arrangement subject to the limitation that each section is large enough to enclose a unit circle? The answer perhaps looks like this: That is approximate (and possibly wrong!). In my drawing the 2 lower circles have radius 2, which is clearly the smallest they can be and still enclose 2 distinct unit circles. The upper circle has a radius of... a bit more than 2. I think that to be well-defined, we must specify that we're looking for the smallest total area . What are the radii of circles which form the arrangement of circles that cover the minimum total area, subject to the constraint that each of the 5 sections encloses a unit circle? If anyone can answer the question, that would of course be great. But I'd also appreciate any useful partial answers, since I'm really not sure how to attack the problem. Note: In the overlapping circles problem, circles cannot be tangent. I'm not worried about that detail in the question I'm posing here. If the smallest arrangement includes 2 circles that touch in a tangent point (as I suspect it does), then the answer should more accurately be thought of as the limit as the circles get arbitrarily close.","['circles', 'geometry', 'computational-geometry']"
4574655,(In)dependence of two stochastic processes,"Consider the C-shaped region $C\subset \mathbb{R}^2$ shown below, which has arms I, II $\subset C$ given by I $= I_0\times I_1$ and II $= I_0\times I_2$ for some intervals $I_0, I_1, I_2\subset\mathbb{R}$ , with $I_1$ and $I_2$ disjoint. Let further $r : C\rightarrow \mathbb{R}^2$ be a $C^1$ -diffeomorphism which acts trivially on $C$ in the sense that $$\tag{1}
r(x,y) = (\alpha_1(x), y) \ \ \forall\, (x,y)\in \text{I}, \qquad \text{and} \qquad r(x,y) = (\alpha_2(x), y) \ \ \forall\, (x,y)\in \text{II}
$$ for some $\alpha_i : I_0\rightarrow \mathbb{R}$ strictly monotone, and $\left.r\right|_{C\setminus{(\text{I}\cup\text{II})}} = \left.\mathrm{id}\right|_{C\setminus{(\text{I}\cup\text{II})}}$ . Suppose now that $X = (X^1_t, X^2_t)_{t\in[0,1]}$ is a continuous-time stochastic process in $\mathbb{R}^2$ that evolves inside the C-shaped region shown above, and let the process $$\tag{2} Y=(Y^1_t, Y^2_t)_{t\in[0,1]}:=r(X)\equiv(r(X^1_t, X^2_t))_{t\in[0,1]}$$ be the image of $X$ under $r$ . The components $X^1$ and $X^2$ of $X$ are assumed to be statistically independent My question : Assume that $\alpha_1\neq \alpha_2$ with $\alpha_1(U) > \alpha_2(U)$ for some (nonempty) open $U\subset I_0$ and $\mathbb{P}(X_{t_j}\in U\times I_j)>0$ , $j=1,2$ , for some $t_1, t_2\in [0,1]$ . Does this imply that the processes $Y^1$ and $Y^2$ must be statistically dependent? Remark: It seems plausible that $Y^1$ and $Y^2$ must be dependent since (by the fact that $\alpha_1$ and $\alpha_2$ are different) the evolution of $Y^2(=X^2)$ influences the trajectories of $Y^1$ via the arm I, II that $Y=(Y^1,Y^2)$ lands in. Yet it seems to me that the given assumptions might be too weak to actually prove the independence of $Y^1$ and $Y^2$ .","['probability', 'real-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4574684,Is there a way to find the values of 3 variables with just one equation?,"I was doing a personal project until I came upon this equation that i need to solve to continue, the thing is I donâ€™t thing I have been thought this in math ever so i wonder if this even possible to solve at all if why or why not 2850=2x+4y+6z If i can get more specific, it needs to be real numbers, and cannot be negatives, and must be only integers, no fractions. Can values aside from all real numbers be obtain with those limitations, or is it impossible? And Thank you for thank you time to answer.","['integer-partitions', 'diophantine-equations', 'multivariable-calculus', 'numerical-linear-algebra', 'linear-diophantine-equations']"
4574716,Are there any concrete examples of Baire spaces $X$ in which neither player has a winning strategy in BM(X),"Let $X$ be a topological space. A Banach-Mazur game $BM(X)$ is played by
two players $\alpha$ and $\beta$ , who select nonempty open subsets of $X$ . The player $\beta$ starts a game by selecting a nonempty open subset $V_1$ of $X$ . In return, $\alpha$ -player
chooses a nonempty open subset $W_1$ of $V_1$ . In general, at the $n$ -th stage of the
game, $n \geq 1$ , the player $\beta$ chooses a nonempty open subset $V_n âŠ‚ W_{nâˆ’1}$ and $\alpha$ answers by a nonempty open subset $W_n$ of $V_n$ . Proceeding in this fashion, the
players generate a sequence $(V_n,W_n)_{n=1}^{\infty}$ which is called a play. The player $\alpha$ is said to be the winner of the play $(V_n,W_n)_{n=1}^{\infty}$ if $\bigcap_{n\geq 1}V_n =\bigcap_{n\geq 1}W_n\neq \emptyset$ otherwise the player $\beta$ wins this play. A partial play is a finite sequence of sets
consisting of the first few moves of a play. A strategy for player $\alpha$ is a rule
by means of which the player makes his/her choices. Here is a more formal
definition of the notion strategy. A strategy s for the player $\alpha$ is a sequence
of mappings $s = \{ s_n\}$ , which is inductively defined as follows:
The domain of $s_1$ is the set of all open subsets of $X$ and $s_1$ assigns to each
nonempty open set $V_1 \subset X$ , a nonempty open subset $W_1 = s_1(V_1)$ of $V_1$ .
In general, if a partial play $(V_1, \ldots ,W_{nâˆ’1})$ has already been specified, where $W_i = s_i(V_1, \ldots , V_i)$ , $1 \leq i \leq n âˆ’ 1$ . Then the domain of $s_n$ would be the set $\{(V_1,W_1, . . . ,W_{nâˆ’1}, V ):V \subset W_{nâˆ’1}\}$ can be the next move of $\beta$ -player
and it assigns to each choice $V_n \subset W_{nâˆ’1}$ some nonempty open subset $W_n = s_n(V_1,W_1, \ldots ,W_{nâˆ’1}, V_n)$ of $V_n$ . An $s$ -play is a play in which $\alpha$ selects his/her moves according to the strategy $s$ . The strategy $s$ for the player $\alpha$ is said to be a winning strategy if every $s$ -play
is won by $\alpha$ . A space $X$ is called $\alpha$ -favorable if there exists a winning
strategy for $\alpha$ in $BM(X)$ . My question: Is there a topological space $X$ which is  neither $\alpha$ -favorable nor $\beta$ -favorable?",['general-topology']
4574722,Show $\sqrt 2 + \sqrt[n]{n}$ is irrational,"This is for considering $n$ a positive integer. I understand this number is an algebraic number and I understand if it was rational it would have a minimal polynomial of the form $mx-k$ for $m$ a nonzero integer and $k$ an integer. If I can show the minimal polynomial is not of this form then that would work, but I am unsure how to go about this. Perhaps using $\Bbb Z[\sqrt 2 , \sqrt[n]{n}]$ somehow.","['algebra-precalculus', 'irrational-numbers']"
4574733,motivation for regular schemes,"Clearly regular schemes are like smooth varieties (in the sense of dimension of tangent spaces) and should be very important in algebraic geometry. Is there any big theorem focusing on regular schemes? Is there any property of regular scheme that makes it easier to handle than non-regular schemes? I guess it goes down to the meaning of singularity of a general scheme (resp. $k$ -scheme, resp. a $k$ -variety).","['regular-rings', 'algebraic-geometry', 'motivation', 'schemes']"
4574767,Show that the two elliptic integrals are equal,"Prove or disprove: for any $0<a<1$ $$ \int _{-a}^a \frac{du}{\sqrt{(u^2-a^2)(a^2u^2-1)}}=\int_0^\pi \frac{d\theta}{|e^{2i\theta}-a^2|} \tag{1}\label{eq1}$$ Where did I come across these beasts? We know the Schwarzâ€“Christoffel mapping maps the upper half plane biholomorphically to a rectangle via the map $z\mapsto \int_0^z \frac{du}{\sqrt{(u^2-a^2)(a^2u^2-1)}}$ . I was trying to understand the image of the hemisphere $\{z\in \mathbb H: |z|<1\}$ under this map. After experimenting, I believe the hemisphere also gets mapped to a rectangle. In particular, I believe, the top arc of the hemisphere gets mapped to one of the sides of the rectangle. But a rectangle has opposite sides equal from which I conclude equation (\ref{eq1}). The RHS of (\ref{eq1}) can be re-written as $$\int_0^\pi \frac{d\theta}{\sqrt{(a^2-\cos(2\theta)-\sin(2\theta))(a^2-\cos(2\theta)+\sin(2\theta))}}$$ after expanding $e^{i\theta}$ in sin and cos. But I don't see any $u$ -substitution that takes this to the other one because in LHS the integrand blows up near the boundary points but the integrand of RHS is bounded. Some values: Experimentation using numerical calculators gives more evidence that they are equal. \begin{array}{|c|c|c|c|}
\hline
a& \text{numerical value of LHS}& \text{numerical value of RHS} \\ \hline
 0.3& 3.141671197826467& 3.141671197824304\\ \hline
0.4 &  3.161993625960735 & 3.161993625959924\\ \hline
0.7 &  3.360852415660911 &3.360852415664235\\ \hline
 0.9& 4.030573051205955& 4.030573051201128\\ \hline
\end{array} My real motivation to study all this is to understand the modulus of the annulus which forms a branched cover over the disc $\mathbb D$ over two points $\pm a$ . This is just the restriction of the branched cover of the elliptic curve $\{(z,w):w^2=(z^2-a^2)(a^2z^2-1)\}$ over the Riemann sphere by projection down to $z$ variable.","['integration', 'elliptic-curves', 'quasiconformal-maps', 'complex-integration', 'elliptic-integrals']"
4574774,finding partial derivative using Rudin limit definition,"I was just reading the definition of Rudin on partial derivative given $f:\mathbb{R}^n \to \mathbb{R}^m$ , $e_1,\dots,e_n$ and $u_1,\dots ,u_m$ be basis for the spaces, we have $(D_jf_i)(x)=\lim_{t\to 0}\frac{f_i(x+te_j)-f_i(x)}{t}$ . Where $f_i(x)=f(x)\cdot u_i$ Now let $f(x,y)=\frac{xy}{x^2+y^2}$ and $f(0,0)=0$ , if I want to use the definition to evaluate partial of $x$ at $(x,y)\neq 0$ , I have $D_1f_i(x,y)=\lim_{t\to 0}\frac{f_i(x+t,y)-f_i(x)}{t}$ , what exactly is the component function $f_i$ here? Since $m=1$ I'm assuming $f_i=f(x,y)$ and we have $\lim_{t\to 0}\frac{f_i(x+t,y)-f_i(x)}{t}=\lim_{t\to 0}\frac{\frac{(x+t)(y)}{(x+t)^2+y^2}-\frac{xy}{x^2+y^2}}{t}$ , I've played a lot with this fraction over here but I cant get anywhere, and everything online is evaluating this function at origin. I'm not sure what I am missing here. If anyone can help me understand this? Thanks a lot!","['calculus', 'analysis']"
4574780,"Evaluate $\iint (\frac{(x-y)}{x+y})^4$ over the triangular region bounded by $x+y=1$, $x$-axis ,$y$-axis","Evaluate $\iint (\frac{(x-y)}{x+y})^4$ over the triangular region bounded by $x+y=1$ , $x$ -axis, $y$ -axis. My attempt: I tried using the change of variable concept: Let $u=x-y$ and $v=x+y$ , $|J|= \frac{1}{2}$ Then $x=\frac{u+v}{2}$ and $y=\frac{u-v}{2}$ Now we have to find the limit which is where I am stuck. We were told in class that we could plug in the points and get the equations: In this case the points are $(0,0),(0,1),(1,0)$ . So if we put the first point we get $u=0,v=0$ or $u=-v,u=v$ If we put in the second point we get $u=-1,v=1$ , $u=-v,u=v+2$ Similarly $u=1,v=-1$ , $u=v,u=v-2$ How do I proceed after this? This is making me even more confused on what values to take and what values not to take.can anyone help me out here with an easy explanation?","['integration', 'multivariable-calculus', 'calculus', 'multiple-integral']"
4574792,Perfect groups whose character degrees square divide its order,"This post is an extension of that one from the non-abelian finite simple groups to the finite perfect groups. According to the mentioned post and its second comment, for all non-abelian finite simple group $G$ , there is an irreducible complex character $\chi$ such that $\chi(1)^2$ does not divide $|G|$ . The proof uses the classification of finite simple groups (CFSG), but we are interested in a CFSG-free proof, so if you find one, please post an answer to the mentioned post. Anyway, here we are interested in the finite perfect groups: Question : Is there a finite perfect group $G$ such that for all irreducible complex character $Ï‡$ then $\chi(1)^2$ divides |G|? Bonus question 1 : Is there such a group which is a direct product of non-abelian finite simple groups? Bonus question 2 : If so, what is the minimum number of components for such a direct product? Two? Above bonus questions can be seen as a game with the prime factorization of the order of the non-abelian finite simple groups and their character degrees. Let us call a finite group $G$ thin if for all prime $p$ then there is an irreducible complex character $\chi$ such that $\nu_p(\chi(1)) = \nu_p(|G|)$ , where $\nu_p$ is the p-adic valuation . The seven first non-abelian finite simple groups ( $\mathrm{PSL}(2,q)$ with $q=5,7,9,8,11,13,17$ ) are thin. We wonder whether every $\mathrm{PSL}(2,q)$ is thin. Anyway, for our purpose here, we must consider non-thin groups only. A finite group $G$ will be called $p$ - fat if for all irreducible complex character $\chi$ then $\nu_p(\chi(1)) < \nu_p(|G|)$ . Note that a finite group is non-thin if and only if it is $p$ -fat for some prime $p$ . The first non-thin non-abelian finite simple group is $A_7$ , which is $2$ -fat. Now the notion of $p$ -fat group is not enough for our need. A finite group $G$ will be called $p$ - superfat if for all irreducible complex character $\chi$ then $2\nu_p(\chi(1)) < \nu_p(|G|)$ . The group $A_7$ is $2$ -superfat. The next $p$ -superfat non-abelian finite simple group is $M_{22}$ , which is also $2$ -superfat. We wonder whether for all prime $p$ there is a $p$ -superfat non-abelian finite simple group. Anyway, the goal here is to find relevant direct product of such superfat groups.","['gap', 'characters', 'representation-theory', 'finite-groups', 'group-theory']"
4574793,Clebsch-Gordan coefficients and the decomposition of the tensor product of two irreducible representations of $SO(3)$,"We will use $D^l$ for irreducible representations and $V_l$ for the corresponding vector space. Given two irreducible representations of $SO(3)$ , $D^{l_1}$ and $D^{l_2}$ ( $l_1$ and $l_2$ are the same as $j_1$ and $j_2$ in Clebsch-Gordan coefficients). Their tensor product $D^{l_1} \otimes D^{l_2}$ may be a reducible representation of $SO(3)$ . Indeed, we have $$D^{l_1} \otimes D^{l_2} = \bigoplus_{| l_1 - l_2 | \leq l \leq l_1 + l_2} D^l$$ The identity is basically representing the same vector space in two different ways. The left hand side corresponds with the natural choice of basis of $V_{l_1} \otimes V_{l_2}$ induced by that of $V_{l_1}$ and $V_{l_2}$ . The right hand side is a direct sum of subspaces $V_l$ of $V_{l_1} \otimes V_{l_2}$ , each is invariant under $(D^{l_1} \otimes D^{l_2})(g)$ for any $g \in SO(3)$ . Clebsch-Gordan coefficients are basically a change of basis in the sense that they form a transition matrix that changes from the natural basis of $V_{l_1} \otimes V_{l_2}$ to the basis of $V_l$ 's. Now we still need to choose a basis for each $V_l$ . My question is: what basis is chosen for each $V_l$ in the context of Clebsch-Gordan coefficients and why are they chosen this way or are they considered natural?","['quantum-mechanics', 'representation-theory', 'group-theory', 'linear-algebra']"
4574801,Killing form for Lie algebras vs trace map for field extensions,"I'm currently studying Lie algebras and i ran into the Killing form, defined for a Lie algebra $\mathfrak{g}$ over a field $k$ by: $\mathfrak{g}\times\mathfrak{g} \rightarrow k, (x,y) \mapsto tr(ad(x)ad(y))$ . This map is useful because if $k$ is algebraically closed and its characteristic is zero, the Killing form is non degenerate iff $\mathfrak{g}$ is semisimple. On the other hand, given a field extension of finite degree $K/k$ , one defines the trace map $K \rightarrow k$ which sends $x\in K$ to $tr(r_x)$ , where $r_x \in End_k(K)$ is the multiplication by $x$ . Now, $K/k$ is separable iff the bilinear symmetric form $K \times K \rightarrow k, (x,y) \mapsto tr(r_xr_y)$ is non degenerate. My question is: is there a reason why these two definitions look the same and give such similar results? Edit. Maybe one way to proceed could be to search a Lie algebra whose Killing form is related to the trace map of $K/k$ .
The map $r : K \rightarrow End_k(K), x \mapsto r_x$ sends $K$ into $GL(n,k)$ , so one may consider the Zariski closure of the image $r(K)$ ; this will be an algebraic group with an associated Lie algebra. Now, everything is constructed from $K/k$ , so I'd expect some relations. Could this be the case?","['lie-algebras', 'trace', 'field-theory', 'bilinear-form', 'algebraic-geometry']"
4574822,Convergence in distribution and limit inferior of expectation,"I am trying to solve this exercise in Probability Theory by A. Klenke (3rd version) by applying the continuous mapping theorem or the portemanteau theorem but with no results: Let $X,X_1,X_2,...$ be real random variables with $X_n$ converging in distribution to $X$ . Show that $E(|X|)\leq \liminf_{n\to \infty} E(|X_n|)$ . In order to apply the continuous mapping theorem I think I need to know if $P_X({0})=0$ , as it requires, as a premise, that the set of points of discontinuity has measure zero. But I don't know anything about it. Any suggestions? Thank you.","['convergence-divergence', 'probability-theory', 'weak-convergence']"
4574850,Why doesn't limit of a double/multivariable function needn't exist given that it exists along all straight line?,"I am studying multivariable calculus as of now. I have been told by my mentor that if limit exists along all straight lines, it doesn't mean that limit exists. I got the same information from Wikipidea.org as well as Thomas Calculus. However, I doubt this. I think that I have got something (at least for double variable functions), which can be called as a proof. What I know about limits is that there is a function say $f$ and for calculating limiting value at a point, we evaluate the function for points present in the point's close neighborhood. If all these values are approaching some number, we say that the number being approached is the limiting value. For example let's say that $f(x, y)$ is a function and we want its limiting value at $(0, 0)$ . According to what I have mentioned above about limits, we need to plug in several points in origin's close neighborhood to get the limiting value. If my interpretation of limit is correct, this should give us the correct answer. Let's say I evaluate the limit by approaching origin via lines $y = mx$ . I have full control over m and I can manipulate it as I wish. So, by doing this substitution, we can get any $(x, y)$ given both $x$ and $y$ are in close neighborhood of origin by manipulating value of m (Except for x = 0 for which the limiting value can be calculated separately). So, this method is same as the first method mentioned in the paragraph. But since, from three sources I was getting the same information, I feel that I am wrong. I request to please correct me by telling my mistake. All the three sources refer the following example as a proof: Quoting from en.wikipidea.org A study of limits and continuity in multivariable calculus yields many counterintuitive results not demonstrated by single-variable functions. For example, there are scalar functions of two variables with points in their domain which give different limits when approached along different paths. E.g., the function. $$f(x,y) = \frac{x^2y}{x^4+y^2}$$ approaches zero whenever the point $(0,0)$ is approached along lines through the origin $({\displaystyle y=kx})$ . However, when the origin is approached along a parabola ${\displaystyle y=\pm x^{2}}$ , the function value has a limit of ${\displaystyle \pm 1/2}a$ . Since taking different paths toward the same point yields different limit values, a general limit does not exist there. I believe that limit doesn't exists along all lines in this case. Lets say $x_{0}$ is a non-zero x-coordinate of a point in close neighborhood of origin. Let us evaluate limit along a line $y = mx\space where\space m = x_0$ . Doing the substitution in the above function, we get, $$\lim\limits_{x \to 0}\frac{mx^3}{x^4 + m^2x^2}$$ This can be rewritten as $$\lim\limits_{x \to 0}\frac{\frac{m}{x}}{1 + (\frac{m}{x})^2}$$ Out of the several values in close neighborhood of origin, one will $(x_0, y)$ . If we plug in this point in our above function with $m=x_0$ , for this point, $f$ becomes $\frac{1}{3}$ . If I plug in $x = \frac {x_0} {2}$ , I get $\frac{2}{5}$ . Therefore limit doesn't exists along all the lines. However, Wikipedia, Thomas Calculus, my mentor all say that limit exists along all lines. My questions : Why, if limit exists along all lines, limit needn't exist along every other path. What is wrong in my attempt trying to contradict the example given on wikipidea. Sorry for my poor english. Thank you so much.","['multivariable-calculus', 'limits', 'calculus']"
4574877,Let $f_n\to f$ pointwise for measurable functions $f_n$. Is $f$ measurable?,"The following is a standard result of real-valued measurable functions: Theorem: let $\{f_n\}$ be a sequence of measurable functions $(X,\Sigma_X)\to(\mathbb{R},\mathcal{B}_{\mathbb{R}})$ , where $\mathcal{B}_{\mathbb{R}}$ is the Borel $\sigma$ -algebra generated by the Euclidean topology on $\mathbb{R}$ . Then $$f_n\to f \text{ (pointwise)} \implies f \text{ is measurable}.$$ More generally, given a measurable function $(X,\Sigma_X)\to(Y,\Sigma_Y)$ , if the codomain measure space is Borel we can already talk (because there is a topology on $Y$ ) about limits of sequences i.e. questions such as $$\text{Does $f_n(x)$ have any limits as $n\to\infty$?}$$ make sense. If the topology in question is Hausdorff, the limits -if they exist- must be unique, and we can talk about the function $$f:X\to Y:x\mapsto \lim_{n\to \infty}f_n(x).$$ With all that in mind, I'm wondering if the following, more general result holds: Theorem (?): let $\{f_n\}$ be a sequence of measurable functions $(X,\Sigma_X)\to(Y,\mathcal{B}_{Y})$ , where $\mathcal{B}_Y$ is the Borel $\sigma$ -algebra generated by some Hausdorff topology $\tau$ on $Y$ . Then $$f_n\to f \text{ (pointwise)} \implies f \text{ is measurable}.$$ I know two proofs of the theorem involving real-valued functions: one which uses $\liminf f$ and $\limsup f$ , and another which works with $\pi$ -systems and the fact that $f$ is measurable if and only if $$\{f\le c\} := \{r\in\mathbb{R} : f(r) \le c\}\in\Sigma_X \ \text{for any } c\in\mathbb{R}.$$ Neither proof seems generalizable. In this post the OP asks whether the above result is true for first countable Hausdorff spaces, and it seems they recieve no solid answer on the matter. I'm asking whether it holds for any Hausdorff space.","['borel-sets', 'general-topology', 'pointwise-convergence', 'measure-theory']"
4574887,Application of Lindeberg-Feller Theorem,"I want to prove that for independent variables $X_1,X_2,\dots$ $$\frac{\sum\limits_{k=1}^n X_k}{\sqrt{\mathbb V\left(\sum\limits_{k=1}^n X_k\right)}}\xrightarrow{n\to\infty}\mathcal N(0,1),$$ where $\mathbb P(X_n = n) = \mathbb P(X_n = -n) = \frac{1}{2}$ . I cant use the normal CLT because the $X_n$ are not identically distributed. I have to use the Lindeberg-Feller Theorem. I know that $\mathbb E(X_n) = 0$ and that $\mathbb V(X_n) = n^2$ and that therefore $$\mathbb V\left(\sum\limits_{k=1}^n X_k\right) = \sum\limits_{k=1}^n\mathbb V(X_k) = \sum\limits_{k=1}^n k^2 = \frac{n(n+1)(2n+1)}{6}.$$ I know that I need to find $X_{n,k}$ with $\sum\limits_{k=1}^{k_n}\mathbb E(X_{n,k}^2)\xrightarrow{n\to\infty}1$ and $\sum\limits_{k=1}^{k_n}\mathbb E(X_{n,k}^2 \mathbf 1_{\{|X_{n,k}|>\varepsilon\}})\xrightarrow{n\to\infty}0$ for all $\varepsilon >0$ . How can I find such $X_{n,k}$ . Any help is appreciated.","['probability-theory', 'probability']"
4574925,Show $11$ divides $3^{2n+2}+2^{6n+1}$,"I was requested to show $11|3^{2n+2}+2^{6n+1}$ for $n \in \mathbb{N}$ . I tried using induction. Since $n=1$ yields $81+128=209=11\cdot 19$ we can safely assume $3^{2k+2}+2^{6k+1}=11m$ for some $k \in \mathbb{N}$ . Then we can consider the case for $$3^{2(k+1)+2}+2^{6(k+1)+1}=3^{2k+4}+2^{6k+7}$$ Notice that $$
\begin{align} 
3^{2k+4}+2^{6k+7} &= 3^{2k+2+2}+2^{6k+1+6} \\ &=3^{2k+2}\cdot 3^2+2^{6k+1}\cdot2^6
\end{align}
$$ This seems promising, but I can't find anything to do with it. Yes, we assumed $11|3^{2k+2}+2^{6k+1}$ . However, for $11$ to divide a linear combination of the two terms (as the one we find in our inductive step), we should know $11$ divides each term separately, which is not something we know. Another despaired attempt was trying to make use of the fact that $3$ , $2$ and $11$ are all primes. I know $m|n$ if and only if, for their prime factorizations $m=p_1^{e_1}p_2^{e_2}...p_k^{e_k}, n=p_1^{f_1}p_2^{f_2}...p_k^{f_k}$ , we have $e_i \leq f_i$ for all $i$ . However, what can we know about the prime factorization of $3^{2k+2}+2^{6k+1}$ ? I'm pretty sure using induction is not the simplest way to show this (and I'm totally sure prime properties is completely desperate), and yet I have no other ideas on how to go about the problem. How can this property be demonstrated?","['divisibility', 'discrete-mathematics']"
4574934,Probability that a random walk in $2d$ has small local time at each vertex,"Let $P_{n,k}$ be the probability that a  simple random walk of length $n$ in $\mathbb{Z}^2$ is such that each vertex of $\mathbb{Z}^2$ is visited at most $k$ times by the walk. Certainly this probability decays exponentially with $n$ ,  i.e, $P_{n,k} \sim e^{- c(k) n }$ , but how fast does the exponent $c(k)$ go to zero with $k$ in the limit of large $k$ ? It is reasonable to expect that $c(k) \rightarrow 0$ as $k \rightarrow \infty$ , but I am interested in the precise asymptotic behaviour. This question is related to this other question: Recurrent random walks with bounded local time at each vertex","['random-walk', 'stochastic-processes', 'combinatorics', 'statistical-mechanics', 'probability']"
4574945,Proportions of ornamental gables in gothic architecture,"I'm trying to figure out mathematically precise proportions for gothic architecture-style gables. The gable has the shape of a triangle. There is a central incircle, as well as a smaller incircle in the remaining space at the top. The legs of the triangle are tangent to two additional circles of the same radius as the smaller circle at the top, which are also tangent to the central incircle. If the base of the triangle is drawn so that it is tangent to the central incircle, then the two neighboring circles at the bottom are cut exactly in half. I hope the image makes it more clear. Now, my question is: for any given base or height of the triangle, or for any given radius of the central incircle, what is the radius of the smaller circles? Since the legs of the triangle are tangent to 3 circles on either side, which also touch, and since the smaller circles are of the same size, I feel like there should be exactly one answer where the proportions exactly match. I've tried a few things but my maths is a bit rusty and I can't seem to make a lot of progress. :/ I'd be grateful even for a suggestion from which direction to tackle this problem.","['trigonometry', 'geometry']"
4574964,"How to evaluate the definite integral $\int_0^{2\pi}\frac{\sin^2(M\pi\cos\theta)}{\sin^2(\pi\cos\theta)}\,d\theta?$","Given an integer parameter $M$ , how to find the value of the definite integral $$\int_0^{2\pi}\frac{\sin^2(M\pi\cos\theta)}{\sin^2(\pi\cos\theta)}\,d\theta?$$ This problem is encountered in antenna theory. Thanks.","['integration', 'calculus', 'definite-integrals']"
4575135,Justifying term by term differentiation of spherical harmonics expansion,"I saw in many physics texts term by term differentiation of spherical harmonics expansion, but since they're physics texts they're without rigourous proof. Take for example the following from Wikipedia Given the multipole expansion of a scalar field ${\displaystyle \phi =\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell
 }\phi _{\ell m}(r)Y_{\ell m}(\theta ,\phi ),}$ we can express its gradient in terms of the VSH as ${\displaystyle \nabla \phi =\sum _{\ell =0}^{\infty }\sum _{m=-\ell
 }^{\ell }\left({\frac {d\phi _{\ell m}}{dr}}\mathbf {Y} _{\ell
 m}+{\frac {\phi _{\ell m}}{r}}\mathbf {\Psi } _{\ell m}\right).}$ I know the spherical harmonics $Y_{\ell m}$ forms an orthonormal basis for $L^2$ functions, so that any $L^2$ function $\phi$ can be written as an infinite series expansion of spherical harmonics in the above way. If $\phi$ is $H^1$ function, then we expect its derivative to also have an spherical harmonic expansion. However, how do we know rigourosuly that it can be obtained by term by term differentiation of the series for $\phi$ in the above manner? In other words, how do we know the coefficients for the spherical harmonic expansion of $\nabla\phi$ is related to the coefficients for the spherical harmonic expansion of $\phi$ via term by term differentiation? Since the convergence of the series that I know of is only in $L^2$ , not uniform, I can't quite see how to properly justify it.. Could one justify it if we have more regularity like $C^k$ for some large $k$ ?","['analysis', 'spherical-harmonics', 'functional-analysis', 'sequences-and-series', 'derivatives']"
4575165,Prove or disprove the equality of these two integrals,"Let $\alpha$ be an arbitrary positive real number in: $$
F_1 = \int_0^1 x^2 \left[ \int_{-1}^{+1} \frac{e^{-\alpha\sqrt{1+x^2+2xy}}(xy+1)}{(1+x^2+2xy)^{3/2}}dy\right]dx $$ $$
F_2 = \int_1^\infty x^2 \left[ \int_{-1}^{+1} \frac{e^{-\alpha\sqrt{1+x^2-2xy}}(xy-1)}{(1+x^2-2xy)^{3/2}}dy\right]dx
$$ Prove or disprove that $F_1 = F_2$ . Source of the problem are the equations (3) and (4) in A Paradox of Newtonian Gravitation and Laplaceâ€™s Solution by Amitabha Ghosh and Ujjal Dey.
They have done already numerical experiments that seem to confirm equality. Quote: an analytical proof showing that F1 and F2 are exactly equal will be an interesting mathematical exercise. And that's it. I have no idea how to proceed. Progress so far I promised myself not to do numerical experiments.
But the outcome of the inner integral - the one between square brackets - is indeed terrible.
So what else would be an option?
With MAPLE 8 some values in the publication can be reproduced. for k from 0 to 10 do
alpha := k*0.1;
g(x,alpha) := int(exp(-alpha*sqrt(1+x^2+2*x*y))*(x*y+1)/(1+x^2+2*x*y)^(3/2),y=-1..1);
F1 := evalf(int(g(x,alpha)*x^2,x=0..1));
F2 := evalf(int(-g(x,alpha)*x^2,x=1..10^3)); 
end do; The special case $\alpha = 0$ gives $\,F_1=\frac{2}{3}\,$ and $\,F_2=0\,$ exactly.
So it appears that nearby $\alpha = 0$ the integrals must be unequal.
But MAPLE keeps calculating endlessly for those low values and I had to manually stop the program. Feynman trick . $F_1$ and $F_2$ are both a function of $\alpha$ . We can take derivatives under the integral sign and see what happens near $\alpha=0$ . $$
\left.\frac{dF_1}{d\alpha}\right|_{\alpha=0} =
- \int_0^1 x^2 \left[ \int_{-1}^{+1} \frac{xy+1}{1+x^2+2xy}dy\right]dx = -\frac{1}{2} \\
\Longrightarrow \quad dF_1 = -\frac{1}{2}d\alpha
$$ $$
\left.\frac{dF_2}{d\alpha}\right|_{\alpha=0} =
- \int_1^\infty x^2 \left[ \int_{-1}^{+1} \frac{xy-1}{1+x^2-2xy}dy\right]dx = \infty \\
\Longrightarrow \quad dF_2 = \infty\,d\alpha
$$ Leading to the following heuristics. $F_1(\alpha)$ is somewhat decreasing from $F_1(0)=2/3$ to lower values, but the increase in $F_2(0)=0$ is infinitely large at that place.
Based upon this, it's impossible to keep up appearances )-:
we can actually say nothing yet whether the outcome is $\,F_1(\alpha) = F_2(\alpha)\,$ for all $\,\alpha \gt 0\,$ .","['calculus', 'definite-integrals']"
4575173,Is a general state space a manifold?,"I asked a similar question on the physics SE at this link but did not really get an answer so I'll ask here. My question there is a bit more detailed. From what I have recently learned, in classical mechanics, the configuration of some mechanical systems (say, a double pendulum) can be described by a point on an n -dimensional smooth configuration manifold, $Q$ . The dynamics are then often studied using Lagrangian mechanics on $TQ$ , or using Hamiltonian mechanics on $T^*Q$ , both of which have coordinate representations in $\mathbb{R}^{2n}$ . Long before I ever heard the word manifold, I was ""doing"" dynamics using some 2n ""state space"" variables in $\mathbb{R}^{2n}$ . I know now that when using state space variables of the form $(\pmb{x},\dot{\pmb{x}})$ , I'm really working with some coordinate representation of $(\text{x},\mathbf{v})\in TQ$ . I could transform to some other coordinates $(\pmb{q},\dot{\pmb{q}})$ but, contrary to what I used to think,  this is not actually a different state space but rather just different coordinates for the same $(\text{x},\mathbf{v})\in TQ$ (right?). Similarly, when using state space variables of the form $(\pmb{x},\pmb{p}_{x})$ (phase space coordinates), I'm really working with some coordinate representation of $(\text{x},\mathbf{p})\in T^*Q$ . I could transform to some other $(\pmb{q},\pmb{p}_{q})$ but again this is not a different phase space but just different coordinates for the same $(\text{x},\mathbf{p})\in T^*Q$ (right?). My Question: When working directly with coordinates in $\mathbb{R}^{2n}$ , we also commonly use more general ""state space"" coordinates which are not necessarily of the lagrangian form, $(\pmb{x},\dot{\pmb{x}})$ , nor of the canonical/symplectic form $(\pmb{x},\pmb{p})$ , but are rather just some 2n variables that fully define the state (for example, the classic Keplerian orbit element for the two-body problem). Are such coordinates on $\mathbb{R}^{2n}$ actually a coordinate representation for some ``state space manifold '' ?  Or put differently, it seems $TQ$ and $T^*Q$ are two particular types of a ""state space manifold"" for which the coordinates and equations of motion have certain properties, but is there a more general manifold on which we can study the dynamics? note: I'm aware that in the Hamiltonian formulation we can use some canonical transformation such that the coordinates $(\pmb{x},\pmb{p})$ are not necessarily split into ""position level"" coordinates and ""velocity-level"" coordinates. The Delaunay variables for the two-body problem are an example of this. So this would be an example of ""jumbled"" coordinates on $\mathbb{R}^{2n}$ which are actually just still coordinates for the same $T^*Q$ .","['classical-mechanics', 'dynamical-systems', 'ordinary-differential-equations', 'differential-geometry']"
4575218,Variance of Sum of independent random variables. Wald's identities.,"Let $N: \Omega \rightarrow \mathbb{N}$ be a random variable, and $S_N = \sum_{i=1}^N X_i$ for $(X_n)_n$ i.i.d. Wald id1: $E[S_N] = E[X_1]E[N]$ , Wald id2: $E[S_N^2] = E[X_1^2]E[N]$ .
Prove that $Var(S_N) = -Var(N)\mathbb{E}[X_1]^2 + 2Cov(S_N, N)\mathbb{E}[X_1] + Var(X_1)\mathbb{E}[N]$ . Use Walds first and second identity. My attempt:
I tried proving the equality from right to left. $$-Var(N)\mathbb{E}[X_1]^2 + 2Cov(S_N, N)\mathbb{E}[X_1] + Var(X_1)\mathbb{E}[N]= $$ $$ = - (E[N^2] - E[N]^2)E[X_1]^2 - 2(E[S_N \cdot N] - E[S_N]E[N])E[X_1] + (E[X_1^2]-E[X_1]^2)E[N]$$ $$=  -E[N^2]E[X_1]^2 + E[S_N]^2 - 2E[S \cdot N ]E[X_1] + 2E[S_n]E[N]E[X_1] + E[X_1^2]E[N] - E[X_1]^2E[N]$$ $$ = -3E[S_N \cdot N]E[X_1] + 3E[S_N]^2 + E[S_N^2] - E[S_N]E[X_1]$$ Any help?","['expected-value', 'variance', 'probability', 'random-variables']"
4575253,"Is $\sum_{k=1}^\infty \frac{1}{p_{p_k}}$, where $p_k$ is the $k$-th prime, irrational? transcendental?","I was reading about the reason why the reciprocals of the primes have a divergent sum. So I was thinking of changing the index to the $k$ th prime. We get: $$\sum_{k=1}^\infty \frac{1}{p_{p_k}}=S$$ When I first posted this here , I was thinking that it was a divergent series like its predecessor. But @GregMartin pointed out that this sum actually converges. So I was wondering if this number is irrational? Could it also be transcendental? I am afraid this question is as hard as proving that Euler's constant $\gamma$ is transcendental. Edit: I understand that this question is very hard to solve. So maybe is there any explanation why it might not be solvable by current methods?","['irrational-numbers', 'transcendental-numbers', 'sequences-and-series']"
4575271,Mercator Projection.,"I am trying to derive the metrics for the  Mercator Projection using standard spherical coordinates $f(\phi, \theta) = (\cos\phi \cos\theta, \sin\phi \cos\theta, \sin\theta)  $ . For simplicity, I consider the radius of the Earth is equal to 1. Then I am using a quadratic differential of the first fundamental form: $ds^2=Edu^2+2FDuDv+Gdv^2$ ; where $E, F, G$ are elements of the matrix representation of the first fundamental form of $f$ , that is $E = \cos^2\theta; F =0$ , and $G=1$ . So the metrics of the sphere is $ds^2=\cos^2\theta d\phi^2+d\theta^2$ .
I am looking for projections: functions $x=x(\phi,\theta); y=y(\phi,\theta)$ .
Next, I let loxodrome be a function $\phi=\phi(\theta)$ then the velocity vector will be $v=(\phi',1)$ . And meridian has a condition $\phi = const$ , then as a parameter, I choose $\theta$ , which gives me a velocity vector $m =(0,1)$ .
Then the loxodrome should make a constant angle $\psi$ with the meridian. So I can write: $ \cos\psi =\frac{\lt v,m \gt}{\Vert v \Vert \Vert m \Vert} = \frac{1}{ \sqrt{ \cos^2\theta \phi'^2 +1}  } $ .
I understand the last expression. But can someone explain to me the length of the velocity vector $v$ ? Thank you.",['differential-geometry']
4575305,Prove that $P_j(x)$ and $P_k(x)$ are relatively prime for all positive integers $j\neq k$,"Let for $n\ge 1, P_n(x)= 1+2x+3x^2+\cdots + nx^{n-1}$ . Prove that for any distinct positive integers j and k, $P_j(x)$ and $P_k(x)$ are relatively prime. The above problem is 2014 Putnam A5. Solutions can be found here . I have the following questions about the solutions: In the first solution, how did they compute that $w^n = nw - n+1$ ? I tried using the fact that $z$ is not a nonnegative real number and $P_i(z)=P_j(z)=0,$ but I wasn't able to deduce this result. In the second solution, I can't understand the proof of Corollary 2. In particular, how can one apply lemma 1 to the polynomial $f(x/R)$ if $x/R$ isn't necessarily a root of $f$ ? Also, even if $x/R$ were a root of $f,$ it doesn't seem like the resulting coefficients of the polynomial would be increasing, which is a requirement of lemma 1. I'm not sure how they get the bound $|z|\ge r$ , for similar reasons. I tried proving a variant of Corollary 2 where $a_i/a_{i+1}$ is replaced by $a_{i+1}/a_i$ in the definitions of $r$ and $R$ , but even if this corollary holds, it doesn't seem like it's useful for the given problem.","['contest-math', 'gcd-and-lcm', 'calculus', 'polynomials', 'derivatives']"
4575322,Show $2n+1$ and $n(n+1)$ are coprime. [duplicate],"This question already has answers here : $(a,b)\!=\!1\!=\!(a,c)\Rightarrow (a,bc)\!=\!1$ [coprimes to $\,a\,$ are closed under products] (7 answers) Closed 1 year ago . I was requested to show $2n+1$ and $n(n+1)$ are coprimes. I struggled a bit to find the correct way to show this, and I wanted to know if my proof is correct. Here's what I did. Firstly, I will use the following lemma, whose demonstration I will skip for the sake of brevity: $\text{Lemma}:$ Let $p$ be some prime number. Then $p|ab \implies (p|a  \space \lor \space p|b)$ . $\text{My solution}:$ Assume $p$ is a prime number, $p|(2n+1)$ and $p|n(n+1)$ . From $p|n(n+1)$ it follows, according to the lemma, that $p|n$ or $p|(n+1)$ . If $p|n$ then $p|2n$ and $p\nmid(2n+1)$ . If $p|(n+1)$ then $p|(2n+2)$ and $p\nmid(2n+1)$ . Hence, it is impossible for $p$ to divide $n(n+1)$ and $2n+1$ . Then $2n+1$ and $n(n+1)$ share no prime in their prime factorizations, meaning they are coprime. Is this correct? Thanks in advance.","['solution-verification', 'divisibility', 'discrete-mathematics', 'prime-numbers']"
4575329,Smallest overlapping circles containing unit circles in each section - Second arrangement,"This question is closely related to the question that I posted yesterday . The simpler symmetry of this problem makes me think that this variation will be simpler to solve, so I suggest that anyone who is intrigued by the problems should start with this one. One way of overlapping 3 circles looks like this: What is the smallest size that these circles can have and still be large enough to accommodate a unit circle in each region? With unit circles pictured for clarity and zoomed in, it's something like this (where circles that appear to be approximately tangent should indeed be perfectly tangent): EDIT (improved picture now that I know the exact radius): By pure physical manipulation of circles, it seems to me that the radius of the 3 circles may be a bit less than 15. But I'm having difficulty determining an exact answer. Any solutions--partial or complete--would be appreciated. Note: in writing this I realized that I have made the implicit assumption that the smallest possible area covered by 3 circles meeting these requirements will be done with 3 equally sized circles. As proof of this claim, I offer these waving hands and some mumbled lines about symmetry. ðŸ‘‹ðŸ‘‹","['symmetry', 'circles', 'geometry', 'computational-geometry']"
4575352,"In how many ways we can arrange $10$ objects $a_1, a_2, a_3,....,a_{10}$ in a line such that neither $a_1, a_2$ are together nor $a_3,a_4$.","In how many ways we can arrange $10$ objects $a_1, a_2, a_3,....,a_{10}$ in a line such that neither $a_1, a_2$ are together nor $a_3,a_4$ . Approach $1$ : Let $n(A)$ denotes when $a_1a_2$ are together. $n(B)$ denotes when $a_3a_4$ are together. $n(A\cap B)$ denotes when $a_1a_2$ as well as $a_2a_4$ are together. $\implies n(A)=9!\cdot2!, \quad n(B)=9!\cdot 2!\quad n(A\cap B) =8!\cdot 2! \cdot 2! $ So by inclusion exclusion principle and De Morgan's law $n(\bar{A}\cap \bar{B})=Total-n(A\cup B)$ $\implies 10!-(2\cdot 9!+2\cdot 9!-4\cdot8!)$ So I obtained result as $58\cdot8!$ . Approach $2$ Let first arrange $6$ boys $a_5,a_6,a_7,a_8,a_9,a_{10}$ then place $a_1, a_2$ in between $7$ their gap created by $a_5,a_6,a_7,a_8,a_9,a_{10}$ and arrange them in $2!$ ways And now place $a_3, a_4$ in between $9$ gaps created by $a_1,a_2,a_5,a_6,a_7,a_8,a_9,a_{10}$ and arrange them in $2!$ ways. So this can be done in $6!\cdot {7\choose 2}\cdot 2!\cdot {9\choose 2} \cdot 2!$ using this approach I am obtaining $54\cdot 8!$ I don't know what I am missing in second approach and what is going wrong in second approach","['permutations', 'combinations', 'combinatorics']"
4575358,Question about the proof of compactness of events in Bernoulli measure,"The problem is presented in Example 1.63 (page 29) of ""Probability Theorey"" 3rd edition by Prof. Achim Klenke. We construct a measure for an infinitely often repeated random experiment with finitely many possible outcomes. Notations: Let $E$ be the set of possible outcomes. For a fixed realization of the repeated experiment, let $\omega_{1},\omega_{2},\ldots \in E$ be the observed outcomes. Hence the space of all possible outcomes of the repeated experiment is $\Omega = E^{\mathbb{N}}$ . Define the set of all sequences whose first $n$ values are $\omega_{1},\ldots,\omega_{n}$ : \begin{equation*}
[\omega_1, \ldots, \omega_n]:=\{\omega^{\prime} \in \Omega: \omega_i^{\prime}=\omega_i \text { for any } i=1, \ldots, n\}
\end{equation*} Let $\mathcal{A}_0=\{\emptyset\}$ . For $n \in N$ , define the class of cylinder sets that depend only on the first $n$ coordinates \begin{equation*}
\mathcal{A}_n:=\{[\omega_1, \ldots, \omega_n]: \omega_1, \ldots, \omega_n \in E\},
\end{equation*} and let $\mathcal{A} :=\bigcup_{n=0}^{\infty} \mathcal{A}_n$ . Purpose : Let $A, A_1, A_2, \ldots \in \mathcal{A}$ and $A \subset \bigcup_{n=1}^{\infty} A_n$ . We want to show that there exists an $N \in \mathbb{N}$ such that \begin{equation*}
    A \subset \bigcup_{n=1}^N A_n .
\end{equation*} Proof Let $B_n:=A \backslash \bigcup_{i=1}^n A_i$ . We assume $B_n \neq \emptyset$ for all $n \in N$ in order to get a contradiction. By Dirichlet's pigeonhole principle (recall that $E$ is finite), we can choose $\omega_1 \in E$ such that $\left[\omega_1\right] \cap B_n \neq \emptyset$ for infinitely many $n \in N$ . Since $B_1 \supset B_2 \supset \cdots$ , we obtain \begin{equation*}
    [\omega_1] \cap B_n \neq \emptyset \quad \text { for all } n \in N .
\end{equation*} Successively choose $\omega_2, \omega_3, \ldots \in E$ in such a way that \begin{equation*}
    [\omega_1, \ldots, \omega_k] \cap B_n \neq \emptyset \quad \text { for all } k, n \in N
\end{equation*} Question: Can we now directly conclude that \begin{equation*}
    \omega= (\omega_1, \omega_2, \ldots) \in B_{n} \text{ for all } n \in \mathbb{N} 
\end{equation*} Is there any danger of it?","['measure-theory', 'induction', 'transfinite-induction', 'probability-theory', 'probability']"
4575371,Show that $\prod_{k=1}^\infty \frac{2k+1}{2\pi}\sin{\left(\frac{2\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{k+2}\right)}=\frac{\pi}{2}$,"Show that $$\prod\limits_{k=1}^\infty \frac{2k+1}{2\pi}\sin{\left(\frac{2\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{k+2}\right)}=\frac{\pi}{2}$$ Context: Inspired by this question , I considered the product of the areas of every regular odd-gon inscribed in a circle of area $1$ . This is $\prod\limits_{k=1}^\infty \frac{2k+1}{2\pi} \sin{\frac{2\pi}{2k+1}}=0.18055...$ which I guess does not have a closed form. On a whim, I divided this number by the Kepler-Bouwkamp constant , $\prod\limits_{k=1}^\infty \cos{\frac{\pi}{k+2}}=0.11494...$ , and numerical calculation suggests that the result is $\pi/2$ . My attempt: I used the double angle formula for sine to get $\prod\limits_{k=1}^\infty \frac{2k+1}{\pi}\sin{\left(\frac{\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{2k+2}\right)}$ but this seems just as intractable as the original form of the product.","['circles', 'polygons', 'infinite-product', 'limits', 'trigonometry']"
4575372,Proving (directly) the closedness of the closed ball $B_1(0)$ in a finite dimensional normed space.,"Context. Let $X = (X,\|\cdot\|)$ be a finite dimensional normed space. I am trying to show that the unitary closed ball of $X$ ( $B_1(0)$ ) is, indeed, closed with a direct proof. Attempt. By definition, $$ B_1(0) = \{x \in X \colon \|x\| \leqslant 1\}. $$ We say $B_1(0)$ is closed if it contains all of its limits points, i.e., if $\overline{B_1(0)} \subset B_1(0)$ . Thus, let $x \in \overline{B_1(0)}$ be an arbitrary element. Then, there exists a convergent sequence $(x_k)_{k \in \Bbb N}$ such that $x_k$ converges to $x$ . By definition of convergent sequence, $$ \forall \epsilon > 0, \exists N = N(\epsilon) \colon \forall n \in \Bbb N, n > N \Rightarrow \| x_n-x\| < \epsilon. $$ Now, we just see that $$ \|x\| = \|(x-x_n)+x_n \| = \|x_n - (x_n-x) \| \geqslant |\,\|x_n\| - \|x_n-x\|\,| \geqslant | \, \|x_n\| - \epsilon \, |$$ But I don't know how to proceed from here... Is this aproach correct?","['normed-spaces', 'functional-analysis']"
4575396,Which finite simple groups have trivial Schur multiplier?,"In Lie theory there are only 3 compact simple groups that have both trivial center and trivial fundamental group: $ G_2,F_4,E_8 $ . I was expecting that similarly there would be only a few finite groups which are both simple and superperfect (have trivial Schur multiplier). However it seems that $ PSL(2,2^k) $ is simple and Schur trivial for all $ k \geq 3 $ . (weirdly $ PSL(2,2^2) $ is simple but has Schur multiplier $ 2 $ since it is isomorphic to $ PSL(2,5) \cong A_5 $ ). This leads me to wonder, are there lots more out there...? So I'm curious Which finite simple groups have trivial Schur multiplier? For the same reason that the complex points of the linear algebraic groups $ G_2,F_4,E_8 $ (and thus also the compact real form) are simple centerless and have trivial fundamental group we expect the finite field points of $ G_2,F_4,E_8 $ to be simple and have trivial Schur multiplier. They are indeed all Schur trivial: $ E_8(q) $ $ F_4(q)$ for $ q \geq 3 $ $ G_2(q)$ for $ q \geq 5 $ (here $ q $ is any prime power) with the exceptions of $ F_4(2),G_2(2),G_2(3),G_2(4) $ .","['finite-groups', 'reference-request', 'simple-groups', 'group-cohomology', 'group-theory']"
4575426,"how to prove volume is less or equal the outer measure of any closed interval, $v(I)\leq m^*(I)$","Exercise 26: Let $$
I=\left\{(x, y) \in \mathbb{R}^2 \mid a \leq x \leq b, c \leq y \leq d\right\}
$$ be a closed interval in $\mathbb{R}^2$ . Let $$
\begin{aligned}
&a=a_0<a_1<\ldots<a_m=b \quad \text { and } \\
&c=c_0<c_1<\ldots<c_n=d .
\end{aligned}
$$ For $i=1,2, \ldots, m$ and $j=1,2, \ldots, n$ , define the rectangle $I_{i j}$ by $$
I_{i j}=\left\{(x, y) \in \mathbb{R}^2 \mid a_{i-1} \leq x \leq a_i, c_{j-1} \leq y \leq c_j\right\} .
$$ (This can be thought of as subdividing $I$ into subrectangles along the vertical lines $x=a_1, x=a_2, \ldots, x=a_{m-1}$ and the horizontal lines $y=c_1, y=c_2, \ldots, y=c_{n-1}$ .) Using the definition of volume, prove $$
\sum_{i=1}^m \sum_{j=1}^n v\left(I_{i j}\right)=v(I) .
$$ Exercise 27: Let $$I=\left\{(x, y) \in \mathbb{R}^2 \mid a \leq x \leq b, c \leq y \leq d\right\}$$ be a closed interval in $\mathbb R^2$ . Let $J_1, J_2, \ldots, J_n$ be a finite collection of closed intervals that cover $I$ . That is, $$
I \subseteq \bigcup_{k=1}^n J_k .
$$ By carefully subdividing $I$ and the $J_k$ 's into subrectangles, and use the previous exercise to show that $$
v(I) \leq \sum_{k=1}^n v\left(J_k\right) .
$$ I start with Exercise 26 (thanks to @Balajisb pointing out the telescoping sum), $$
\begin{align}
\sum_{i=1}^m \sum_{j=1}^n v\left(I_{i j}\right)&=\sum_{i=1}^m (v(I_{i1})+v(I_{i2})+\cdots+v(I_{in}))\\
&= \sum_{i=1}^m (a_i-a_{i-1}) ((c_1-c_0)+\cdots+(c_n-c_{n-1}))\\
&= ((c_1-c_0)+\cdots+(c_n-c_{n-1})) \sum_{i=1}^m (a_i-a_{i-1})\\
&= (c_n-c_0) ((a_1-a_0)+\cdots+(a_m-a_{m-1})) \\
&= (d-c)(b-a) \\
&= v(I)
\end{align}
$$ Now, from here I couldn't see how exercise 26 can be used in exercise 27. What they mean by ""By carefully subdividing $I$ and the $J_k$ 's into subrectangles..."" ? Any help will be appreciated. TIA.","['volume', 'measure-theory', 'lebesgue-measure', 'analysis']"
4575439,Proving a certain equality only holds for $p=2$,"While solving a problem I arrived at a specific condition which I am pretty sure is true but I am having a hard time proving it. Let $0<t\leq 1$ and $p\geq 1$ be real numbers satisfying the following equality $$
2^{2/p}+(2-2^p+2^p t^p)^{2/p}=4t^2
$$ Additionally, $2-2^p+2^p t^p\geq 0$ . I want to show that this can only happen when $p=2$ . I suspect this has to do with some inequalities between $l_p$ norms, but I can't quite figure it out.","['inequality', 'functional-analysis']"
4575484,Infimum of the inner product between the gradient of a least square loss and the direction,"Let $A\in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^{m}$ . The gradient of a least square loss ( $f(x)=\frac{1}{2}\|Ax-b\|^2$ ) is $A^{\top}(Ax-b)$ . Is it possible to find a $\delta>0$ for every given $\epsilon>0$ such that the following holds for any $x \in \mathbb{R}^n$ when $x_0$ is given: $$
\inf_{\|x-x_0\|\geq \epsilon} \frac{\langle x-x_0, A^{\top}(Ax-b) \rangle}{\|x-x_0\|^2 } \geq \delta
$$ My try $$
\begin{aligned}
\frac{\langle x-x_0, A^{\top}(A(x-x_0+x_0)-b) \rangle}{\|x-x_0\|^2 } 
&=
\frac{\langle x-x_0, A^{\top}A(x-x_0) \rangle}{\|x-x_0\|^2 } 
+
\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 }
\\
&\geq
\frac{\lambda_{\min}(A^{\top}A)\|x-x_0\|^2 }{\|x-x_0\|^2 }
+
\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } \\
&=
\lambda_{\min}(A^{\top}A)
\end{aligned}.
$$ Now there are two situations: $A$ is full columns rank. Hence, $(\lambda_{\min}(A^{\top}A)>0)$ . Then, how do I know $\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } > \lambda_{\min}(A^{\top}A) $ ? $A$ is not full columns rank. Hence, $(\lambda_{\min}(A^{\top}A)=0)$ . Then, how do I know $\frac{\langle x-x_0, A^{\top}(Ax_0-b) \rangle}{\|x-x_0\|^2 } > 0 $ ?","['matrices', 'inequality', 'linear-algebra', 'eigenvalues-eigenvectors']"
4575565,Stein complex analysis 6.2,"Prove that $$\prod_{n=1}^\infty{n(n+a+b)\over(n+a)(n+b)} = {\Gamma(a+1)\Gamma(b+1)\over\Gamma(a+b+1)}$$ whenever $a$ and $b$ are positive. Using the product formula for $\sin\pi s$ , give another proof that $\Gamma(s)\Gamma(1-s) =\pi/\sin\pi s$ . My attempt: First of all, I guess the assumption $a,b$ are positive can be dropped. I think it should be dropped because we need to use that product formula to show $\Gamma(s)\Gamma(1-s) = \pi/\sin\pi s$ . Recall the product formula for entire function $1/\Gamma(s)$ : $${1\over\Gamma(s)}= e^{\gamma s}s\prod_{n=1}^\infty\left(1+{s\over n}\right)e^{-{s\over n}}$$ for all $s\in\Bbb C$ . In particular, term-by-term multiplication of infinite products is allowed. \begin{align*}
{\Gamma(a+1)\Gamma(b+1)\over\Gamma(a+b+1)} & = {e^{\gamma(a+b+1)}(a+b+1)\prod_{n=1}^\infty\left(1+{a+b+1\over n}\right)e^{-{a+b+1\over n}}\over e^{\gamma(a+1)}(a+1)\prod_{n=1}^\infty\left(1+{a+1\over n}e^{-{a+1\over n}}\right)e^{\gamma(b+1)}(b+1)\prod_{n=1}^\infty\left(1+{b+1\over n}\right)e^{-{b+1\over n}}}\\
& = {e^{\gamma(a+b+1)}(a+b+1)\over e^{\gamma(a+b+2)}(a+1)(b+1)}\prod_{n=1}^\infty{\left(1+{a+b+1\over n}\right)e^{-{a+b+1\over n}}\over \left(1+{a+1\over n}\right)\left(1+{b+1\over n}\right)e^{-{a+b+2\over n}}}\\
& = {e^{\gamma(a+b+1)(a+b+1)}\over e^{\gamma(a+b+2)}(a+1)(b+1)}\prod_{n=1}^\infty{(a+b+n+1)n\over (a+n+1)(b+n+1)}e^{1\over n}\\
& = e^{-\gamma}\prod_{n=1}^\infty{n(a+b+n)\over(a+n)(b+n)}e^{1\over n}\\
\end{align*} Now observe that using $\Gamma(1) =1$ , \begin{align*}
1 & = e^{\gamma}\prod_{n=1}^\infty\left(1+{1\over n}\right)e^{-{1\over n}}\\
e^{-\gamma} & = \prod_{n=1}^\infty{n+1\over n}e^{-{1\over n}}.\\
\end{align*} Hence, \begin{align*}
e^{-\gamma}\prod_{n=1}^\infty{n(a+b+n)\over(a+n)(b+n)}e^{1\over n} & = \prod_{n=1}^\infty{(n+1)(a+b+n)\over(a+n)(b+n)}\\
& = \prod_{n=1}^\infty{n(a+b+n)\over (a+n)(b+n)}\\
\end{align*} Does it make sense?","['complex-analysis', 'solution-verification', 'infinite-product', 'gamma-function']"
4575611,"For which $(a,b)$ is $(round(an),round(bn))$ a 1-1 function?","Let $R(x)$ be the standard rounding function $$R(x)=\lfloor x+\frac12\rfloor$$ It gives the nearest integer to $x$ , halves rounding up. For which $(a, b)\in(0,1)^2$ is the following function one-to-one? $$f:\mathbb Z\to \mathbb Z^2,f(n)=(R(an),R(bn))$$ Let $P$ be the region $$P\subset (0,1)^2:\text {$f$ is 1-1} $$ My progress: $(\alpha,1-\alpha)\in P$ if $\alpha$ is irrational.  Then halves will never matter, and $R(\alpha n)+R((1-\alpha)n)=n$ The same if $\alpha$ is rational with an odd denominator. $(\alpha,1-\alpha)\notin P$ if $\alpha$ is rational with denominator $2m$ because $f(m)=f(m+1)$ $(2p/(2m+1),2q/(2m+1))\notin P$ because $f(m)=f(m+1)=(p,q)$ . If $(a,b)\in P$ then so is $(ma,b)$ for any odd integer (or else $ma\ge1$ ) because you can recover $R(x)$ from $R(mx)$ . The question came up because the New York Times' Spelling Bee has target scores equal to $R(0.4M),R(0.5M)$ and $R(0.7M)$ but the daily maximum score $M$ still can't be deduced if it ends in a $5$ or a $6$ . EDIT:  Here is a graph of rational $(a,b)\in P$ with $a\lt b$ and the common denominator at most $200$ .  Some of the lines they lie on have been drawn in.","['elementary-number-theory', 'discrete-mathematics']"
4575619,limit at infinity of a function of $n$,"I'm trying to solve the following limit: $$
\lim _{n \rightarrow \infty} \sum_{k=0}^n \frac{(2 k) !}{4^k(k !)^2(2 k+1)}-\frac{\pi}{2}
$$ I've thought about using the series: $$
\sum_{n=0}^{\infty} \frac{2^n n !^2}{(2 n+1) !}=1+\frac{1}{3}+\frac{1 \cdot 2}{3 \cdot 5}+\frac{1 \cdot 2 \cdot 3}{3 \cdot 5 \cdot 7}+\cdots=\frac{\pi}{2}
$$ But I still can't see which procedure is the best to continue. Could someone give me a clue or help to continue with the value of the limit? Thank you very much","['ordinary-differential-equations', 'real-analysis', 'calculus', 'combinatorics', 'sequences-and-series']"
4575633,A problem to prove two sets have same number of elements using bijection,"I found a nice problem on bijections. Suppose we have six digit numbers from $000000$ to $999999$ . Let $A$ be the set such that sum of first three digits is same as the sum of last three digits, and $B$ be the set such that the sum of all digits is $27$ . Can we prove that number of elements in $A$ and $B$ are same. Can someone give a hint, how can we define a map from $A$ to $B$ such that the map is bijective?","['elementary-set-theory', 'functions', 'combinatorics']"
4575637,About the equivalence of $\cos(x) = \sqrt{1-\sin^2(x)}$ and $\cos(x) = \cos(2x-x)$,"By the title, my questions may sound trivial or silly, but I have problems with those outputs. I need the sine of $15$ degrees. Now I thought of: $15 = 45-30$ hence: $$\sin(15) = \sin(45-30) = \sin(45)\cos(30) - \cos(45)\sin(30) = \dfrac{\sqrt{6}-\sqrt{2}}{4}$$ So far so good. Yet, when calculating the cosine, I have Mode 1 $$\cos(15) = \sqrt{1 - \sin^2(15)} = \sqrt{1 - \dfrac{8 - 4\sqrt{3}}{16}} = \dfrac{\sqrt{2+\sqrt{3}}}{2}$$ Mode 2 $$\cos(15) = \cos(45-30) = \cos(45)\cos(30) + \sin(45)\sin(30) = \dfrac{\sqrt{6}+\sqrt{2}}{4}$$ The second method returns something more clean than the previous one. I did not really see the numerical equivalence though, so I computed the difference and I found that $$\dfrac{\sqrt{2+\sqrt{3}}}{2} - \dfrac{\sqrt{6}+\sqrt{2}}{4} = 1.11\cdot 10^{-16}$$ (On W. Mathematica). Can someone please explain me this infinitesimal difference between them, and how to pass from Mode 1 to Mode 2 in writing?","['calculus', 'trigonometry']"
4575672,Invertibility of a Rank-One Update of a Symmetric Matrix,"I want to find a necessary and sufficient condition for $B = A + x \, y^\top$ to be invertible, where $A$ is assumed to be symmetric and $x, \, y \, \in \mathbb{R}^n-\{0\}$ . I know that the following holds Theorem . Let $E = I - \alpha \, u \, v^\top$ be a rank-one modification of the identity $I$ , where $\alpha \in \mathbb{R} - \{0\}$ and $u, \, v \, \in \mathbb{R}^n-\{0\}$ . Then, $E$ is invertible if and only if $\alpha \, u^\top v -1 \ne 0$ . Furthermore, $E^{-1} = I - \beta \, u \, v^\top$ with $\beta = \alpha/(\alpha \, u^\top v - 1) $ . Now, I want to find a necessary and sufficient condition for invertiblity of $B$ by using this theorem. I suspect that it should be the positive definiteness of $A$ , but I am having a little bit of difficulty to prove this.","['matrices', 'matrix-rank', 'linear-algebra', 'inverse']"
4575712,"If $xyz = x+y+z$ for $x,y,z > 0$, then $\sqrt{1+x^2} + \sqrt{1+y^2} + \sqrt{1+z^2} \ge 6$","If $x,y,z \in \Bbb R_{>0}$ satisfy $xyz = x+y+z$ , prove that $$\sqrt{1+x^2} + \sqrt{1+y^2} + \sqrt{1+z^2} \ge 6$$ We can express $1+x^2$ as $$1+x^2 = (1-xy)(1-xz) = \frac{(y-xyz)(z-xyz)}{yz} = \frac{(x+z)(x+y)}{yz}$$ since $x + y + z = xyz$ implies $1+ x^2 + xy + xz = 1 + x^2yz$ . Thus, our desired inequality boils down to $$\sqrt{\frac{(x+z)(x+y)}{yz}} + \sqrt{\frac{(y+z)(y+x)}{xz}} + \sqrt{\frac{(z+x)(z+y)}{xy}} \ge 6$$ which looks more complicated. Perhaps I am not headed in the right direction? I'd appreciate any hints or solutions. Thank you!","['contest-math', 'inequality', 'a.m.-g.m.-inequality', 'analysis']"
4575749,"Let $X_n \leq Y_n$ and both converge in distribution $X_n, Y_n \overset{d}{\longrightarrow}F$. Does $|Y_n - X_n| \overset{p}{\longrightarrow} 0\,$?","Let $(X_n)$ and $(Y_n)$ be sequences of random variables such that $X_n \leq Y_n$ for all $n \in \mathbb N$ . Let $F$ be an arbitrary distribution function. Suppose both sequences converge in distribution to $F$ , i.e. $$
P(X_n \leq c) \underset{n \to \infty}{\longrightarrow} F(c) \quad \text{and} \quad P(Y_n \leq c) \underset{n \to \infty}{\longrightarrow} F(c)
$$ for all continuity points $c$ of $F$ . Does it then hold that $|Y_n - X_n| \overset{p}{\longrightarrow} 0\,$ ? Thoughts I'm aware that the weak convergence of random sequences to the same distribution doesn't generally imply this convergence in probability. (Just set $X_n := X$ and $Y_n := Y$ for i.i.d. $X$ and $Y$ with a non-degenerate distribution.) But I'm curious whether it does under the inequality assumption $X_n \leq Y_n$ . This seems intuitive, and I'm having trouble thinking up a counterexample. On the other hand, I haven't been able to prove the statement. So maybe it's time to add yet another counterexample to my collection ;)","['probability-theory', 'probability', 'random-variables']"
4575750,"Let $f : M_n â†’ C$ be a linear map. Show that there exists a matrix $B$ âˆˆ $M_n$ such that $f(A) = tr(BA)$, $âˆ€ A âˆˆ M_n$ [duplicate]","This question already has answers here : $f:M(n,\mathbb R) \to \mathbb R$ be , then $\exists ! C \in M(n,\mathbb R)$ such that $f(A)=Trace (AC) , \forall A \in M(n,\mathbb R)$? (2 answers) Closed 1 year ago . Let $M_n$ be the vector space of $nÃ—n$ complex matrices. Let $f : M_n â†’ C$ be a linear
map. Show that there exists a matrix $B$ âˆˆ $M_n$ such that $f(A) = tr(BA)$ , $âˆ€ A âˆˆ M_n$ , where tr is the normalized trace $(tr(1) = 1)$ on $M_n$ . Please give me an intuition how to solve it. I have no idea how to convert a linear map into form of trace of some matrix. Thank you","['linear-algebra', 'functional-analysis']"
4575752,Pocket Cube (2x2x2 Rubik's cube) as Quotient Group,"The 2x2x2 Rubik's cube (also called Pocket cube) consists of $6 \cdot 4 = 24$ small squares. A rotation of one of the faces of the cube can therefore be described as an element of $S_{24}$ (giving every square a number and checking how they move when we do a rotation of one of the faces). In GAP we could define these rotations (that generate all possible manipulations) as gap> U := (1,2,3,4)(5,17,13,9)(6,18,14,10);
gap> L := (5,6,7,8)(1,9,21,19)(4,12,24,18);
gap> R := (13,14,15,16)(2,20,22,10)(3,17,23,11);  
gap> F := (9,10,11,12)(4,13,22,7)(3,16,21,6);
gap> B := (17,18,19,20)(1,8,23,14)(2,5,24,15);
gap> D := (21,22,23,24)(11,15,19,7)(12,16,20,8); If we now define the pocket cube as the subgroup of $S_{24}$ that is generated by these elements we get a subgroup of size $88179840=2^7 \cdot 3^9 \cdot 5 \cdot 7$ : gap> fullCube := Group(U,L,R,F,B,D);
gap> Print(Size(fullCube));
88179840 It is, however, well known that the pocket cube only has $3674160=2^4 \cdot 3^8 \cdot 5 \cdot 7$ different positions. The thing that went wrong is that rotating the full cube yields (in our eyes) the same cube. However, in the group fullCube it would be a different position. Hence, the idea is to consider the subgroup generated by these ""full rotations"". gap> x := (5,8,7,6)(1,19,21,9)(2,20,22,10)(3,17,23,11)(4,18,24,12)(13,14,15,16); # turn front to top
gap> y := (1,2,3,4)(5,17,13,9)(6,18,14,10)(7,19,15,11)(8,20,16,12)(21,24,23,22); # turn right to front
gap> z := (8,1,14,23)(5,2,15,24)(7,4,13,22)(6,3,16,21)(9,10,11,12)(17,20,19,18); # turn top to right 
gap> rotations := Group(x,y,z);
gap> Print(Size(rotations));
24 This looks good as the subgroup of rotations has $24=2^3 \cdot 3$ elements. So, if we factor fullCube by rotations we should get exactly the group we are looking for with the $3674160=88179840/24$ elements. Two elements in the group fullCube would be considered equal if and only if they represent the same position (without regarding the rotation of the entire cube). Let's try this and first check whether rotations is really a normal subgroup of fullCube : gap> Print(IsNormal(fullCube, rotations));
false Surprise (at least for me...), this is not a normal subgroup. So, we cannot consider the quotient group... This construction seems like the most natural approach for constructing the group. Why does this fail? Is there a way to fix it and still define the pocket cube group as a quotient group? Note that a different way to define the group would be to consider one of the corner pieces fixed and therefore only consider certain face rotations which generated the group: gap> cube := Group(U,L,B); # cube where bottom right cubie is fixed 
gap> Print(Size(cube));
3674160 This does work and yields the right group. It is, however, to me, not as intuitive as the other approach.","['gap', 'group-theory', 'quotient-group', 'rubiks-cube']"
4575836,"Theorem on BFS $X$ and space C of continuous functions in $[0,1]$ and cartesian square of $[0,1]$($[0,1]^2$).","This is the definition which we need for the proof of the theorem: There is the theorem
Let $X$ be a BFS on $I$ . The space $C(I)$ of continuous functions on $I$ is a closed linear subspace of $X$ if and only if there exists a positive constant $c$ satisfying $||\chi_{(a,b)}||_X$ $\geq$ $c$ whenever $0$ $\leq$ $a$ $\lt$ $b$ $\leq$ $1$ . (Mark this condition by ( $\star$ )). There is the proof: For sufficiency part it is enough to show that there is a positive constant $C$ such that for every $f$ $\in$ $C(I)$ , $C$$||f||_{C(I)}$ $\leq$ $||f||_X$ $\leq$ $||f||_{C(I)}$ . (Mark this by ( $\oplus$ )). The second of these inequalities is clear. For the first, let $f$ $\in$ $C(I)$ . There exists $x_0$ $\in$ $I$ such that $||f||_{C(I)}$ $=$ $|f(x_0)|$ ; there exists $\epsilon$ $\gt$ $0$ such that $|f(x_0)|$ $\leq$ $2|f(x)| $ if $x$ $\in$ $(x_0 - \epsilon,x_0 + \epsilon)$ $\cap$ $I$ : $=$ $E$ . Thus from $(\star)$ we see that $||f||_{C(I)}$ $=$ $|f(x_0)|$ $\leq$ $\frac {1}{c}$$|f(x_0)|$$||\chi_E||_X$ $\leq$ $\frac {2}{c}$$||f\chi_E||_X$ $\leq$ $\frac {2}{c}$ $||f||_X$ . Necessity. If $C(I)$ is a closed subset of $X$ , then by the closed graph theorem(there it is https://en.m.wikipedia.org/wiki/Closed_graph_theorem_(functional_analysis) , we have the estimate $(\oplus)$ . Let given any interval $(a,b)$ $\subset$ $I$ be given, if we take a continuous function $g$ on $I$ such that $g$ $\leq$ $\chi_{(a,b)}$ and $||g||_L^{\infty}$ $=$ $1$ we get $(\star)$ . There is my question: This theorem is on $X$ BFS on $[0,1]$ and on $C([0,1])$ space of continuous functions.
How could we proof that if this theorem is fair or false for $X$ BFS on $[0,1]^2$ ( Cartesian product) and on $C([0,1]^2)$ space of continuous functions. Any help would be appreciated.","['measure-theory', 'normed-spaces', 'analysis', 'real-analysis', 'functional-analysis']"
4575862,Understanding a basic ergodic theory physical analogy,"The following excerpt is from the Wikipedia article on ergodic theory : Ergodic theory is often concerned with ergodic transformations. The intuition behind such transformations, which act on a given set, is that they do a thorough job ""stirring"" the elements of that set. E.g. if the set is a quantity of hot oatmeal in a bowl, and if a spoonful of syrup is dropped into the bowl, then iterations of the inverse of an ergodic transformation of the oatmeal will not allow the syrup to remain in a local subregion of the oatmeal, but will distribute the syrup evenly throughout. At the same time, these iterations will not compress or dilate any portion of the oatmeal: they preserve the measure that is density.
The formal definition is as follows: Let $ T: X \rightarrow X$ be a measure-preserving transformation on a measure space $(X, \Sigma, \mu)$ , with $\mu(X)=1$ . Then $T$ is ergodic if for every $E $ in $\Sigma$ with $\mu(T^{-1}(E) \Delta E)=0$ , either $\mu(E)=0$ or $\mu(E)=1$ , How could I formalize pouring the syrup into my oatmeal? Would I describe the oatmeal by a density function in $\mathbb{R}^n$ ? Say $\mu$ is Lebesgue measure on $\mathbb{R}^n$ , then I have some density describing the location of the oatmeal, say $\rho d\mu$ . Would the transformation be ergodic w.r.t. $$m(A) =\int_A fd\mu$$ or with resepct to $\mu$ ? When I describe the combination of the syrup and the oatmeal, would this create a new density at time $t$ ? This is confusing as I would assume the oatmeal and syrup have their own densities so I am unsure which measure the transformation is ergodic with respect to.","['measure-theory', 'ergodic-theory', 'analysis', 'real-analysis', 'probability']"
4575877,"Differential equations of the form $F(x,y')=0$","I have issues with understanding with such functions For example, I was given to solve this differential equation: $$
y'=e^{y'}-x
$$ In this equation I have to ""taking into account the form of the function F, one of these functions is specified (chosen), and the other is found taking into account the identity"". I'm not really sure what it means and how to use this in order to solve the equation If someone can explain me and give an approach in solving such equation I'll be really appreciated!",['ordinary-differential-equations']
4575878,The Inverse Equation for a 2x2 matrix,"I am relatively new to mathematics, especially matrices. I saw a similar question on here about my question but I could not follow what was being said. I am trying to work out the inverse of a $2\times 2$ matrix $A$ . I know the inverse matrix $A^{-1}$ reads: $$A^{-1} = \dfrac{1}{\text{det}(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$$ My question is: how is the inverse matrix obtained? I have tried transposing the cofactor matrix $$\begin{pmatrix}a & -b \\ c & -d\end{pmatrix} \longrightarrow \begin{pmatrix} a & c \\ -b & -d\end{pmatrix}$$ I am using the book Maths for Chemistry Paul Monk Lindsey J Munro as a starting point. Thank you. Alternatively, I have tried using the determinant $(ad-bc)$ therefore, \begin{pmatrix}a & -b \\ -c & d\end{pmatrix} . However, I am not sure how the $a$ and $d$ terms swap as I thought this was not allowed during a transpose, as the principal diagonal remains unchanged.","['matrices', 'matrix-calculus', 'linear-algebra', 'inverse']"
4575880,"Why does $\operatorname{Spec}(\mathbb{C}[[X]])-\langle X\rangle$ ""have topological properties identical"" to $C^1$? (Mumford's Red Book)","In Example F of section II.2 of the Red Book of Varieties and Schemes by Mumford (page 82 in my copy) concerns taking Spec of a noetherian local ring $O$ , and then considering the subscheme $X$ when the unique closed point is removed. Specifically, it says that taking $O$ to be $\mathbb{C}[[X_1, ..., X_n]]$ , then the resulting $X$ turns out to ""have topological properties identical to the ordinary $(2n-1)$ -sphere."" So in the simple case of taking the univariate power series $\mathbb{C}[[T]]$ (switching to $T$ to not confuse it with the subscheme), then removing the maximal ideal generated by $T$ , it sounds like the claim is such a subscheme $X$ is isomorphic to the circle. Is that what it's saying? If so how? I'm having a hard time figuring out what the points of $X$ even would be (with $\mathbb{C}$ being a field, I'm not seeing a lot of opportunities for prime ideals in the power series...), let alone why it would be Hausdorff, why the ""ends would touch"", etc. that a circle is. Maybe it's not saying it's ""the same as"" $C^1$ but rather shares some properties? Clarification of whether I am understanding the point correctly, or help finding elements of $X$ would be appreciated!","['power-series', 'algebraic-geometry', 'schemes']"
4575914,Non-mathematician looking for a complete example of representing points in one 3D coordinate system as points in another,"I am going crazy researching trigonometry, vectors, matrices, quaternions, Euler angles, etc. etc. in pursuit of an efficient, ""best practice"" solution to the problem of representing points in one 3D coordinate system as points in another 3D coordinate system. There are plenty of answers to this question already, but none that I can use because they are too high level, assuming the reader will know how to flesh out the described methods. So I'm looking for a specific, step by step, actual calculation that demonstrates how the job is done. I hope this approach will also help others who may be as confused by their research as I am. Let's say we have object A in its own ""right hand rule"" 3D XYZ coordinate system, ""floating in space"" at no particular location or coordinates, with one point P defined in this object, at X=2, Y=4, Z=7. Let's say we have object B with its own 3D XYZ coordinate system, and this object initially was in the same position and the same orientation as A, their coordinate systems matched (and therefore P would have the same XYZ values in B as in A). But now B is separated from A, in A's coordinates, 10 units on X, 5 units on Y, and 4 on Z. B is additionally rotated, in order and in its own coordinates, 45 degrees on the Z axis, 225 degrees on the Y, and 3 on the X. What are the XYZ values of P in B's coordinates? Please provide the complete math that the reader may reproduce either by hand or programming. I will be grateful for an answer based on any method, but it would be super cool if a number of answers are posted using different methods.","['vectors', '3d', 'matrices', 'linear-transformations', 'rotations']"
4575980,At what point on the paraboloid $y = x^2 + z^2$ is the tangent plane parallel to the plane $x + 2y + 3z = 1$?,"I am not sure how to proceed. This is what I think: I know that the gradient is perpendicular to the level curve of a function. does that mean that the gradient at a point will be parallel to the tangent plane at that exact point? If so, that would mean that the normal of the plane $x+2y+3z = 1$ is perpendicular to the gradient. and then from that, I just set the dot product of the gradient and the normal vector to 0 to find where it happens. However, I highly doubt my approach and it made me realize that I don't really understand the geometric meaning of the gradient. Thus, can someone please how should i approach this problem intuitively.","['multivariable-calculus', 'calculus', 'vector-analysis', 'tangent-spaces']"
4575990,Alternative Definition of Markov Property.,"currently I am trying to solve the following exercise (17.1.1) in Klenke Probability Theory, which states the following: Let $(X_t)_{t \in I}$ be a stochastic process and denote by $\mathcal{F}_{\leq t} = \sigma(X_s : s \in I, \; s \leq t)$ the sigma-algebra of the past until time $t$ , while $\mathcal{F}_{\geq t}$ denotes respectively the sigma-algebra of the future. Then one has the following equivalence: The process $(X_t)_{t \in I}$ admits the (elementary) Markov property iff both $\mathcal{F}_{\leq t}$ and $\mathcal{F}_{\geq t}$ are independent given $\sigma(X_t)$ , where the definition follows: Conditional Independence A family of sub-sigma algebras $(\mathcal{A}_i)_{i \in I} \subset \mathcal{F}$ is called independent of the sub-sigma algebra $\mathcal{A} \subset \mathcal{F}$ if for every finite $J \subset I$ one has $$
P(\cap_{j \in J} A_j \mid \mathcal{A}) = \prod_{j \in J} P(A_j \mid \mathcal{A}) \quad \text{almost surely}.
$$ My try so far: For the first direction, suppose that $X$ admits the Markov Property then let $(A_j)_{j \in J} \subseteq \sigma(X_s)_{s \leq t}$ be a finite family of events from the sigma algebra of the past, observe that for $A = \cap_{j \in J} A_j$ one has almost surely $$
P(A \mid \sigma(X_t)) \stackrel{Def.}{=} E[ \mathbf{1}_A \mid \sigma(X_t) ] \stackrel{M.P}{=} E[ \mathbf{1}_A \mid \mathcal{F}_t ] = \mathbf{1}_A = \prod_{j \in J} \mathbf{1}_{A_j} = \prod_{j \in J} P(A_j \mid \mathcal{F}_t) \stackrel{M.P}{=} \prod_{j \in J} P(A_j \mid \sigma(X_t)),
$$ where we used that due to the filtration property $\mathbf{1}_{A_j}$ are $\mathcal{F}_t$ -measurable. Now for the sigma algebra of the future let again $(A_j)_{j \in J}$ be a finite family of events of $\mathcal{F}_{t \geq}$ then observe first that by the Markov Property one has $$
P(A \mid \sigma(X_t) ) \stackrel{M.P}{=} P(A \mid \mathcal{F}_t) \stackrel{Def.}{=} E[\mathbf{1}_{A} \mid \mathcal{F}_t],
$$ then since for all $j \in J$ one has $A_j \in \mathcal{F}_j$ where $j \geq t$ but now I am stuck since I cannot use the tower property in a good way. I am sure there is a connection with independence and conditional expectation but right now I don't see it, any hints? Thanks in advance.","['stochastic-processes', 'measure-theory', 'probability', 'real-analysis']"
4575991,Every $\sigma$-finite measure is absolutely continuous w.r.t. some finite measure,"I'm trying to prove below theorem mentioned in this comment . This theorem allows us to generalize this property from finite measure spaces to $\sigma$ -finite measure spaces. Theorem: Let $(X, \mathcal A, \mu)$ be a $\sigma$ -finite measure space. There is a finite measure $\nu$ on $(X, \mathcal A)$ such that $\mu$ is absolutely continuous w.r.t. $\nu$ . Could you have a check on my bellow attempt? Proof: Let $(X_m)$ be a countable measurable partition of $X$ such that $\mu(X_m) <\infty$ . We define a measure $\mu_n$ by $$
\mu_n (A) :=  \mu(A \cap X_m) \quad \forall A \in \mathcal A.
$$ Then $\mu_n$ is finite and supported on $X_n$ . We define a measure $\nu_n$ by $$
\nu_n(A) := \frac{\mu_n(A)}{2^{n}\mu_n(X_n)}.
$$ Then $\mu_n \ll \nu_n$ and $\nu_n(X) = 2^{-n}$ . By Radonâ€“Nikodym theorem , there is  a measurable function $f_n:X \to [0, \infty)$ such that $$
\mu_n(A) = \int_A f \mathrm d \nu_n(A) \quad \forall A \in \mathcal A.
$$ Because $\nu_n$ is supported on $X_n$ , we can assume $f_n=0$ on $X \setminus X_n$ . Let $f := \sum_n f_n$ . Then $f$ is the Radonâ€“Nikodym derivative of $\mu$ w.r.t. $\nu :=\sum_n \nu_n$ . Notice that $$
\nu(X) = \sum_n \nu_n(X_n) = \sum_n 2^{-n} = 1.
$$ This completes the proof.","['measure-theory', 'radon-nikodym']"
4576029,Finding the CDF of a transformation of a random variable without first finding its PDF,"This is in the context of random variables and their transformations. Given a random variable $X$ , its probability density function (PDF) $f_{X}$ , and another random variable $Y$ , which is a a function of $X$ , how do I calculate the cumulative density function (CDF) of $Y$ (without first finding the PDF of $Y$ )? Below is a question and my solution: $\\$ Question: The PDF of a random variable $X$ is $f_{X}(x) =\begin{cases}\dfrac{1}{3}&,& -2 < x < 1\\ 0&,&\text{ elsewhere }\end{cases}$ . Find the CDF of $Y$ where $Y=X^{4}$ . $\\$ My solution: From the PDF of $X$ , we get the CDF of $X$ by using $F_X(x)=\int_{-\infty}^{x} f_X(t)\space dt$ . This comes out to be: $F_{X}(x)=\begin{cases}0&,& x <-2\\ \dfrac{x+2}{3}&,& -2\leq x <1\\ 1&,& x\geq 1\end{cases}$ . $\\$ Since $X\in (-2, 1)$ here, and $Y=X^{4}$ , hence $Y\in [0, 16)$ . $\\$ Finding the CDF of $Y=X^{4}$ , that is, $F_{Y}(y)$ : $\begin{align}F_{Y}(y) &= P(Y \leq y) \\ &= P(X^{4} \leq y) \end{align}$ $\\$ Firstly, for $\space $ $-2 < x < 0 \space$ ( $\equiv \space 0 < y < 16$ ), we have $X=-Y^{1/4}$ (since $X$ is negative for these values of $Y$ ). So, $\begin{align} F_{Y}(y) &= P(-y^{1/4}\leq X <0) \\ &=\int ^{0}_{-y^{1/4}}f_{x}(x)\space dx \\ &= \dfrac{x}{3}\Bigg|_{x\space =\space -y^{1/4}}^{x\space =\space 0} \\ &= \dfrac{1}{3}y^{1/4}\end{align}$ $\\$ Now, for $\space $ $0 \le x < 1 \space$ ( $\equiv \space 0 \le y < 1$ ), we have $X=Y^{1/4}$ (since $X$ is non-negative for these values of $Y$ ). So, $\begin{align}F_{Y}(y) &= P(0 \leq X <y^{1/4}) \\ &=\int ^{y^{1/4}}_{0}f_{x}(x)\space dx \\ &= \dfrac{x}{3}\Bigg|_{x\space =\space 0}^{x\space =\space y^{1/4}} \\ &= \dfrac{1}{3}y^{1/4}\end{align}$ $\\$ Combining the above two results, I am getting: (1). For $0 \le y < 1$ : $\space F_{Y}(y) = \dfrac{2}{3}y^{1/4}$ , and, (2). For $1 \le y < 16$ : $\space F_{Y}(y) = \dfrac{1}{3}y^{1/4}$ . The second one above is clearly wrong since it is $\textbf{not}$ giving $\space F_{Y}(16^-) = 1$ , while the first one is correct (as confirmed by the answer that I have). What have I missed here while finding the CDF of $Y$ ? I know we can first find the PDF of $Y=X^{4}$ using a transformation formula and then find its CDF from its PDF, but I do not want to solve this using that formula.","['cumulative-distribution-functions', 'probability-theory', 'probability-distributions', 'transformation', 'density-function']"
4576037,How do we justify the integration of $f'(x)=\arcsin'{(x)}=\frac{1}{\sqrt{1-x^2}}=\sum\limits_{k=0}^\infty \binom{-1/2}{k}x^{2k}$?,"Consider the function $f(x)=\arcsin{(x)}$ and the task of finding the Taylor series at $0$ for this function. We have $$f'(x)=\arcsin'{(x)}=\frac{1}{\sqrt{1-x^2}}$$ Let $g(x)=(1+x)^\alpha$ . Then, for $|x|<1$ and any $\alpha$ , $$g(x)=\sum\limits_{k=0}^\infty \binom{\alpha}{k} x^k$$ That is, $g$ can be represented as an infinite series, the binomial series. Note that this infinite series is a convergent power series centered at $0$ . Hence, it must be the Taylor series of $g$ at $0$ . Using this fact we can show that for $|x|<1$ , $$\frac{1}{\sqrt{1-x^2}}=\sum\limits_{k=0}^\infty \binom{-1/2}{k}x^{2k}$$ Thus, for $|x|<1$ , $$f'(x)=\arcsin'{(x)}=\frac{1}{\sqrt{1-x^2}}=\sum\limits_{k=0}^\infty \binom{-1/2}{k}x^{2k}$$ If we integrate we obtain the result $$f(x)=\arcsin{(x)}=\sum\limits_{k=0}^\infty \binom{-1/2}{k}\frac{x^{2k+1}}{2k+1}$$ Which, if true, means we have another convergent power series centered at $0$ so we have found the desired Taylor series of $\arcsin$ at $0$ . My question is : how do we justify this integration step? If we prove that the sequence of functions $$\{f_n'\}=\left\{\sum\limits_{k=0}^n \binom{-1/2}{k}x^{2k}\right\}$$ converges uniformly to $\arcsin'$ on some interval, then I know of a theorem that says that $$\lim\limits_{n\to\infty}\int_a^b f_n' =\int_a^b \arcsin'$$ This seems like it might involve a lot of calculations. Nonetheless, would it be a valid justification? We could also just use the fact that $\arcsin'$ is continuous and use the first fundamental theorem of calculus. Is this a correct justification?","['calculus', 'uniform-convergence', 'sequences-and-series']"
4576075,"Laplace transforms of Appell functions F1, F2, F3, F4 relative to one of the two variables","Appell's two-variable functions $F_1, F_2, F_3$ and $F_4$ are known to have numerous uses in applied mathematics, notably mathematical Physics. I am looking for generalized Laplace transforms (if they exist) of these functions relative to one of the two variables (I found at least two references that give some of them relative to a linear combination of the two variables). In more formal words, the question is about finding closed forms for: $$ I_1 = \int_0^\infty x^\alpha e^{-s \, x} F_{1}(a, b, c, d; x, y) \, dx$$ $$ I_2 = \int_0^\infty x^\alpha e^{-s \, x} F_{2}(a, b, c, d, e; x, y) \, dx$$ $$ I_3 = \int_0^\infty x^\alpha e^{-s \, x} F_{3}(a, b, c, d, e; x, y) \, dx$$ $$ I_4 = \int_0^\infty x^\alpha e^{-s \, x} F_{4}(a, b, c, d; x, y) \, dx,$$ where $\alpha, s, a, b, c, d, e$ are the parameters and $x, y$ the variables. The only reliable hint I could find is in Harold Exton's handbook ([2]), where Exton gives a method for finding the Laplace transform of $F_1$ using Mellin-Barnes integral representations of $F_1$ . Unfortunately, he did not complete the argument: ""page 99: (...) We make use of the double Barnes integral for the Appell function F1 given by Appell and KampÃ© de FÃ©riet (1926p page 40. This is $$F_1(a,b,b',c;x,y) = \frac{\Gamma(c)}{(2 \pi i)^2 \Gamma(a) \Gamma(b) \Gamma(b')} \int_{-i \infty}^{i \infty}\int_{-i \infty}^{i \infty} \frac{\Gamma(a+u+v)\Gamma(b+u)\Gamma(b'+v)\Gamma(-u)\Gamma(-v)(-x)^u (-y )^v}{\Gamma(c+u+v)} \, du \, dv$$ (5.2.4.26) Some indication is now given as to the evaluation of the Laplace integral of the function $F_1$ [Exton proceeds, interchanging the order of integration and evaluating the inner integral as a gamma function, which gives:] $$P = \frac{(2 \pi i)^2 \Gamma(c) \Gamma(d) \Gamma(d')}{\Gamma(f)} \int_0^{\infty} e^{-s\, t} t^{a-1} F_1(c, d, d', f; xt, yt) \, dt \, \\=  \frac{\Gamma(a)}{s^a} \ \int_{-i \infty}^{i \infty}\int_{-i \infty}^{i \infty} \frac{\Gamma(c+u+v) \Gamma(a+u+v) \Gamma(d+u) \Gamma(d'+v) \Gamma(-u) \Gamma(-v) (-x)^u (-y )^v}{\Gamma(f+u+v)} (-\frac{x}{s})^u (-\frac{y}{s})^v\, du \, dv$$ (5.2.4.28)
In order to obtain a representation of this last result in terms of convergent series, the above integral may be written as an integral of Barnes type of a [Meijer] G-function of one variable. Some rather lengthy manipulation eventually leads to the sum of six double hypergeometric series of higher order with argument $s/x$ and $s/y$ . Exton did not give the six series alluded to but only special cases of interest expressed using KampÃ© de Feriet functions or generalized hypergeometric functions of one variable. He also only considered the case in which both function variables $x$ and $y$ are linked to the Laplace transform by the same integration variable $t$ , but his method is obviously valid if only one variable is linked (so with no $t$ as a multiplier of $y$ in $F_1(c, d, d', f; xt, y)$ for example).
The method looks fine and I have looked around for whether there is any paper giving the forms (probably in Meijer-G representations), without success so far. In any case, series representations should not be used, except for heuristic hints: their domain of convergence is too limited for the Laplace transforms to be defined. But Euler-type integral representations and, as in Exton's book, Mellin-Barnes integral representations could be considered (as analytic continuations giving meaning to the Laplace transforms) or possibly expressions as a series of Gauss hypergeometric functions, since these can have Laplace transforms (see here ).
Again, these may well have been published, but I failed to find anything that neatly answers the question. [2]: Exton, Harold , Handbook of hypergeometric integrals. Theory, applications, tables, computer programs, Mathematics & its Applications. Chichester: Ellis Horwood Limited Publishers. 316 p. (1978). ZBL0377.33001 .","['complex-analysis', 'laplace-transform', 'hypergeometric-function']"
4576083,Does $\sum_{j=0}^n\sum_{k=0}^n\binom{j}{a}\binom{k}{b}\binom{n-j-k}{c}=\binom{n+1}{a+b+c+2}?$,"I'm working on a problem, (a) Let $a,b,n\geq1$ with $a+b\leq n$ . By considering choosing $a+b+1$ numbers from the set $\{0,1,...,n\}$ , and the possibilities for the number in position $a+1$ when the chosen numbers are listed in increasing order, show that $$\binom{n+1}{a+b+1}=\sum_{k=0}^n\binom{k}{a}\binom{n-k}{b}.$$ (b) Hence, or otherwise, express $$\sum_{j=0}^n\sum_{k=0}^n\binom{j}{a}\binom{k}{b}\binom{n-j-k}{c},$$ where $a+b+c\leq n$ , as a single binomial coefficient. Does the following argument make sense for (b)? (I leave a certain amount implicit.) Let \begin{align} 
& j && \text{ be the } (a+1)^\text{th} && \text{number chosen}, \\ 
& j+1+k && \text{ be the } (a+1)+(b+1)=(a+b+2)^\text{th} && \text{number chosen;}
\end{align} Then there are \begin{align}
& \binom{j}{a} && \text{ways of choosing the first $a$ numbers from } [0,j-1], \\
& \binom{k}{b} && \text{ways of choosing the next $b$ numbers from } [j+1,j+k], \\
& \binom{n-j-k}{c} && \text{ways of choosing the next $c$ numbers from } [j+1+k,n];
\end{align} therefore the given expression is equal to $$\binom{n+1}{a+b+c+2}.$$","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4576097,Question about definition of 'distribution',"If a problem says to find the distribution of an RV, can I take that to mean the CDF, or take it to mean the PDF, OR is it ambiguous? Here is a concrete example: ""If $X_1$ and $X_2$ are independent exponential random variables with respective parameters $a$ and $b$ , find the distribution of $Z = \frac{X_1}{X_2}.$","['statistics', 'cumulative-distribution-functions', 'probability-distributions', 'probability', 'density-function']"
4576112,Evaluate $\int_0^\infty \frac{x^2\operatorname{Ti}_2(x^2)}{x^4+1} \space dx$,"At first, I was evaluating $$I=\int_0^{\frac{\pi}{2}}x\sqrt{\tan x}\space dx=\int_0^\infty\frac{2x^2\arctan{x^2}}{x^4+1}dx $$ I substituted $u=\sqrt{\tan x}$ and I followed up by parametrizing $$F(t)=\int_0^\infty\frac{x^2\arctan{tx^2}}{x^4+1}dx$$ $$I=2F(1)$$ I differentiated both sides, evaluated the derived integral and I integrated both sides as normal: $$F'(t)=\frac{\pi}{2\sqrt2}\frac{1}{(t+1)(\sqrt{t}+1)\sqrt{t}}$$ $$F(t)=\int_0^\infty\frac{x^2\arctan{tx^2}}{x^4+1}dx=\frac{\pi}{2\sqrt2}\ln{(\sqrt t+1)}-\frac{\pi}{4\sqrt2}\ln{(t+1)}+\frac{\pi}{2\sqrt2}\arctan(t)$$ We can check the constant is equivalent to zero by setting $t$ to zero, then the rest is easy. No questions here. Just giving context. Anyways, I became curious. Using my knowledge of a few select special functions, I divided both sides by $t$ and then integrated both sides from $0$ to a dummy variable $y$ with respect to t. $$\int_0^\infty\frac{x^2}{x^4+1}\int_0^y\ \frac{\arctan{(tx^2)}}{t}\space dt\space dx$$ $$=\frac{\pi}{2\sqrt2}\int_0^y\frac{\ln{(\sqrt t+1)}}{t}dt-\frac{\pi}{4\sqrt2}\int_0^y\frac{\ln{(t+1)}}{t}dt+\frac{\pi}{2\sqrt2}\int_0^y\frac{\arctan(t)}{t}dt$$ I finally ended up with the following: $$\int_0^\infty\frac{x^2\operatorname{Ti}_2(yx^2)}{x^4+1}dx=-\frac{\pi}{\sqrt2}\operatorname{Li}_2(-\sqrt y)+\frac{\pi}{4\sqrt2}\operatorname{Li}_2(-y)+\frac{\pi}{\sqrt2}\operatorname{Ti}_2(\sqrt y)$$ I plugged in $y=1$ since it's probably the simplest value to evaluate for dilogarithms and the inverse tangent integral alike. I get $$\int_0^\infty\frac{x^2\operatorname{Ti}_2(x^2)}{x^4+1}dx=\frac{\pi^3}{16\sqrt2}+\frac{\pi G}{\sqrt2}$$ Where $G$ is Catalan's constant It's a magnificent result. I don't often see $\pi$ and $G$ multiplied together. My question is how else can we get this result? Addendum: WolframAlpha seems to be making an error. For a finitely large upper bound, WolframAlpha gives a result far from 0, yet for an infinite upper bound, WA gives 0. Strange","['integration', 'improper-integrals', 'definite-integrals', 'special-functions']"
4576184,Integral Involving Harmonic Numbers: $\int_{\sqrt{3}}^{\infty}\frac{\ln\left(x^{4}-1\right)}{x^{2}-1}dx$,"(Motivation) In an attempt to answer this question , I got stuck on evaluating a certain integral. I have made up the following conjecture: $$\int_{\sqrt{3}}^{\infty}\frac{\ln\left(x^{4}-1\right)}{x^{2}-1}dx = \frac{\pi^{2}}{4}+\frac{3}{2}\ln\left(2\right)\ln\left(\frac{\sqrt{3}+1}{\sqrt{3}-1}\right).$$ (Attempt) Let $H_n$ denote the n-th harmonic number $\displaystyle H_{n}=\sum_{k=1}^{n}\frac{1}{k}$ . I will warn this process gets ugly, but it is the best I have so far. Expanding the integrand as a series, we get: $$
\eqalign{
\int_{\sqrt{3}}^{\infty}\frac{\ln\left(x^{4}-1\right)}{x^{2}-1}dx &= \sum_{n=0}^{\infty}\int_{\sqrt{3}}^{\infty}\frac{4\ln\left(x\right)-H_{n}}{x^{4n+2}}dx+\sum_{n=0}^{\infty}\int_{\sqrt{3}}^{\infty}\frac{4\ln\left(x\right)-H_{n}}{x^{4n+4}}dx, \cr
}
$$ which simplifies down to $$\sum_{n=0}^{\infty}\left(\frac{H_{n}\sqrt{3}^{-4n-1}}{-4n-1}-\frac{4\ln\left(\sqrt{3}\right)\sqrt{3}^{-4n-1}}{-4n-1}+\frac{4\sqrt{3}^{-4n-1}}{\left(-4n-1\right)^{2}}\right)+\sum_{n=0}^{\infty}\left(\frac{H_{n}\sqrt{3}^{-4n-3}}{-4n-3}-\frac{4\ln\left(\sqrt{3}\right)\sqrt{3}^{-4n-3}}{-4n-3}+\frac{4\sqrt{3}^{-4n-3}}{\left(-4n-3\right)^{2}}\right).$$ Since both series converge, we can split up the first series like $$-\sum_{n=0}^{\infty}\frac{H_{n}\sqrt{3}^{-4n-1}}{4n+1}+4\ln\left(\sqrt{3}\right)\sum_{n=0}^{\infty}\frac{\sqrt{3}^{-4n-1}}{4n+1}+4\sum_{n=0}^{\infty}\frac{\sqrt{3}^{-4n-1}}{\left(4n+1\right)^{2}}$$ and the second series like $$-\sum_{n=0}^{\infty}\frac{H_{n}\sqrt{3}^{-4n-3}}{4n+3}+4\ln\left(\sqrt{3}\right)\sum_{n=0}^{\infty}\frac{\sqrt{3}^{-4n-3}}{4n+3}+4\sum_{n=0}^{\infty}\frac{\sqrt{3}^{-4n-3}}{\left(4n+3\right)^{2}}.$$ Next, I found that $$\sum_{n=0}^{\infty}\frac{\sqrt{3}^{-4n-1}}{4n+1}=\frac{\pi}{12}+\frac{1}{2}\operatorname{arctanh}\left(\frac{1}{\sqrt{3}}\right)$$ and $$\sum_{n=0}^{\infty}\frac{\sqrt{3}^{-4n-3}}{4n+3}=\frac{1}{2}\operatorname{arctanh}\left(\frac{1}{\sqrt{3}}\right)-\frac{\pi}{12}.$$ However, after trying for a while, I am out of ideas for evaluating the other sums. I realize I skipped a lot of steps, but that is because I don't want this question to be too long. So for your convenience, I put all of these into Desmos , so I believe my process is correct so far based on numerical approximations. (Question) Does anyone have an idea of how to evaluate the integral in question, or how to evaluate the sums I am stuck on? Any hints and ideas are appreciated. (Miscellaneous) Here are some other ideas I have: $$
\eqalign{
\int_{\sqrt{3}}^{\infty}\frac{\ln\left(x^{4}-1\right)}{x^{2}-1}dx &= \int_{\operatorname{arcsec}\left(\sqrt{3}\right)}^{\frac{\pi}{2}}\frac{\ln\left(\left(\sec x\right)^{4}-1\right)}{\sec^{2}x-1}\sec\left(x\right)\tan\left(x\right)dx \cr
\int_{\sqrt{3}}^{\infty}\frac{\ln\left(x^{4}-1\right)}{x^{2}-1}dx &= \int_{0}^{\infty}\frac{\ln\left(\left(x+\sqrt{3}\right)^{4}-1\right)}{\left(x+\sqrt{3}\right)^{2}-1}dx.
}
$$ Maybe I could construct a keyhole contour for the last integral?","['integration', 'improper-integrals', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
4576195,How can we guess one solution of the equation $y''+\frac{2}{x}y'+y=0$?,"I want to solve the equation $$\frac{1}{x^2} \, \frac{d}{dx}\left(x^2\frac{dy}{dx}\right)=-y.$$ I converted it to this equation $y''+\frac{2}{x}y'+y=0$ . How can we guess one solution of the equation $y''+\frac{2}{x}y'+y=0$ ? Is there any way to solve it generally?",['ordinary-differential-equations']
4576275,"If $\sin\alpha\cos\beta=-0.5,$ then find the range of values of $\cos\alpha\sin\beta.$","If $\sin\alpha\cos\beta=-0.5,$ then find the range of values of $\cos\alpha\sin\beta.$ My Attempt: $$-1\le\sin(\alpha+\beta)\le1\\-1\le\sin\alpha\cos\beta+\cos\alpha\sin\beta\le1\\-1\le-0.5+\cos\alpha\sin\beta\le1\\-0.5\le\cos\alpha\sin\beta\le1$$ The answer given is $[-0.5,0.5]$ Can you please confirm?","['contest-math', 'trigonometry', 'solution-verification', 'inequality']"
4576338,Solve the differential equation: $\frac{xdx-ydy}{xdy-ydx}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}$,"Solve the differential equation: $\frac{xdx-ydy}{xdy-ydx}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}$ My Attempt: $$\frac{2xdx-2ydy}{2x^2\frac{xdy-ydx}{x^2}}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac{d(x^2-y^2)}{2x^2d(\frac yx)}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac{\frac{d(x^2-y^2)}{x^2-y^2}}{2\frac{x^2}{x^2-y^2}d(\frac yx)}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac1{x^2-y^2}\sqrt{\frac{x^2-y^2}{1+x^2-y^2}}d(x^2-y^2)=2\frac{1}{1-(\frac yx)^2}d(\frac yx)$$ Put $x^2-y^2=p, \frac yx=q$ $$\frac1p\frac{\sqrt p}{\sqrt{1+p}}dp=\frac{2dq}{1-q^2}\\ \frac{dp}{\sqrt{p^2+p}}=\frac{2dq}{1-q^2}\\ \frac{dp}{\sqrt{(p+\frac12)^2-\frac14}}=\frac{2dq}{1-q^2}\\ \ln|p+\frac12+\sqrt{p^2+p}|=\ln\frac{1+q}{1-q}+\ln c\\ \implies x^2-y^2+\frac12+\sqrt{x^2-y^2}\sqrt{1+x^2-y^2}=c\frac{x+y}{x-y}$$ The answer given is $\sqrt{x^2-y^2}+\sqrt{1+x^2-y^2}=c\frac{x+y}{\sqrt{x^2-y^2}}$ Can we reach the answer with the approach that I followed? The other approach is mentioned here.","['integration', 'contest-math', 'ordinary-differential-equations', 'calculus', 'indefinite-integrals']"
4576340,Generalize Theorem 1. in Diestel/Uhl's Vector Measures,"I'm reading Theorem 1 at page 98 of Vector Measures by Joseph Diestel, John Jerry Uhl. Here we use the Bochner integral . Theorem 1 Let $(\Omega, \Sigma, \mu)$ be a finite measure space, $1 \leq p<\infty$ , and $(X, |\cdot|)$ be a Banach space. Then $L_{p}(\mu, X)^*=L_{q} (\mu, X^*)$ where $p^{-1}+q^{-1}=1$ , if and only if $X^{*}$ has the Radon-NikodÃ½m property with respect to $\mu$ . I'm trying to extend above result to $\sigma$ -finite measure spaces. The idea of the proof is from this comment by @JochenWengenroth. Could you have a check on my below attempt? Thank you so much for your help! Proof: Let $\mu$ be $\sigma$ -finite. We need the following lemma as a bridge, i.e., Lemma Let $(\Omega, \Sigma, \mu)$ be a $\sigma$ -finite measure space. There is a finite measure $\nu$ on $(\Omega, \Sigma)$ such that $\mu$ has a density $f:X \to (0, \infty)$ w.r.t. $\nu$ . Let $(f, \nu)$ be given by the Lemma . By Theorem 1 , there is an isometric isomorphism $$
\varphi: L_{p}(\nu, X)^* \to L_{q} (\nu, X^*).
$$ Notice that $$
g \in L_{p}(\nu, X) \iff \int |g|^p \mathrm d \nu = \int  |f^{-1/p}g|^p \mathrm d \mu <\infty \iff f^{-1/p}g \in L_{p}(\mu, X). \quad (\star)
$$ We define $$
\psi: L_{p}(\mu, X)^* \to L_{p}(\nu, X)^*
$$ by $$
\psi(H) (g) = H (f^{-1/p}g) \quad \forall H \in L_{p}(\mu, X)^*, \forall g \in L_{p}(\nu, X).
$$ It follows from $(\star)$ that $\psi$ is an isometric isomorphism. Again, $$
g \in L_{q}(\nu, X^*) \iff \int |g|^q \mathrm d \nu = \int  |f^{-1/q}g|^q \mathrm d \mu <\infty \iff f^{-1/q}g \in L_{q}(\mu, X). \quad (\star\star)
$$ We define $$
\phi: L_{q} (\nu, X^*) \to L_{q} (\mu, X^*), g \mapsto f^{-1/q}g.
$$ It follows from $(\star\star)$ that $\phi$ is an isometric isomorphism. As such, $$
\phi \circ \varphi \circ \psi : L_{p}(\mu, X)^* \to L_{q} (\mu, X^*).
$$ is an isometric isomorphism . This completes the proof.","['banach-spaces', 'measure-theory', 'functional-analysis', 'dual-spaces', 'radon-nikodym']"
4576345,Totally bounded set in the space of continuous functions,"I have a set $$S  = \{ f \in C^2 [0,1] : \| f\|_c + \|f''\| = 1\}$$ in space $(C^2 [0,1], \| \cdot \|)$ , where $\| f(x) \|_c = \max\limits_{x \in [0,1]} |f(x)|$ . So to check that this space in totally bounded, I want to use ArzelÃ â€“Ascoli theorem, for that I need boundness, which is obvious because $\|f\|_c \le \|f\|_c + \|f'' \| = 1$ , but I have problem with uniformly equicontinuous. I have to check follow condition $$ \forall \varepsilon > 0 ~ \exists \delta > 0 : \forall f \in S ~ \forall x_1, x_2 :|x_1 - x_2| < \delta \hookrightarrow | f(x_1) - f(x_2)| < \varepsilon. $$ I tried to use mean value and Weierstrass theorems, but I from these I got $$|f(x_1) - f(x_2)| = |f'(\xi)||x_1 - x_2| < \varepsilon$$ with $\delta = \frac{\varepsilon}{ \max\limits_{x \in [0,1]} |f'(x)|} $ depends on function, which I don't want. So what I'm missing when trying to pick up the value of $\delta$ not depending on function?","['functions', 'functional-analysis', 'real-analysis']"
4576388,The integral $\int_A f$ exists $\iff$ the series $\sum_i \int_A \phi_i|f|$ converges.,"The theorem 16.5 in Munkres' analysis on manifolds reads: Let $A$ be open in $\mathbb R^n$ ; let $f : A â†’ â„$ be continuous. Let $\{Ï•_i\}$ be a partition of unity on $A$ having compact supports. The integral $âˆ«_A f$ exists
if and only if the series $\sum_i \int_A \phi_i|f|$ converges and in this case $âˆ«_A f=\sum_i \int_A \phi_if.$ The theorem 16.3 reads: Let $\mathscr A$ be a collection of open sets in $â„^n$ ; let $A$ be their union. There exists a sequence $Ï•_1, Ï•_2,â€¦ $ of continuous functions $Ï•_i : â„^n â†’ â„$ such that: (1) $Ï•_i(x) â‰¥ 0$ for all $x$ . (2) The set $S_i = \text{Support } Ï•_i$ is contained in $A$ . (3) Each point of $A$ has a neighborhood that intersects only finitely
many of the sets $S_i$ . (4) $\sum_{i=1}^\infty \phi_i(x)=1 $ for each x âˆˆ A. (5) The functions $Ï•_i$ are of class $C^âˆž$ . (6) The sets $S_i$ are compact. (7) For each $i$ , the set $S_i$ is contained in an element of $A$ . A collection of functions $\{Ï•_i\}$ satisfying conditions $(1)-(4)$ is called a
partition of unity on $A$ . If it satisfies (5), it is said to be of class $C^âˆž$ ; if it
satisfies (6), it is said to have compact supports; if it satisfies (7), it said to
be dominated by the collection $\mathscr A$ . I don't understand the following highlighted part in the proof of theorem 16.5. Suppose that $f$ is non negative on $A$ and that the series $\sum_i \int_A \phi_i|f|$ converges. Let $D$ be a compact rectifiable subset of $A$ . There exists an $M$ such that for all $i > M$ , the function $Ï•_i$ vanishes identically on $D$ for $x âˆˆ D$ . My question is: Why is the highlighted part in the last para true? Please help. Thanks","['multivariable-calculus', 'differential-topology', 'real-analysis']"
4576465,"In probability theory, how may we define a law (without need of defining a random variable first)?","Let $(\Omega, \mathcal{F}, P)$ be a probability triple. The following definitions are from the third chapter of William's Probability with Martingales . (Definition of a distribution function and law given a random variable ) Definition: given a random variable $X$ , we define its distribution $F_X:\mathbb{R}\to[0,1]$ and law $\mathcal{L}_X:\mathcal{B}\to[0,1]$ (which is a probability measure) by \begin{equation}
\label{rel rv d l}
F_X(c) := \mathcal{L}_X(-\infty,c] := P(X\le c) = P\{\omega :X(\omega)\le c\}.
\end{equation} (Definition of a distribution function in general ) Definition: a distribution function $F$ is a function $\mathbb{R}\to[0,1]$ such that (a) $F$ is monotonically increasing. (b) $\lim_{x\to -\infty} F(x) = 0$ and $\lim_{x\to \infty} F(x) = 1$ . (c) $F$ is right-continuous. The two definitions above gives us the following: a function $F$ is a distribution function if and only if there is some random variable $X$ so that $F=F_X:=c\mapsto P(X\le c)$ . With all this in mind, I wonder how may we define a Law $\mathcal{L}$ in general so as to make the following statement true: A function $\mathcal{L}$ is a law if and only if there is some random variable $X$ so that $\mathcal{L}=\mathcal{L}_X:=(-\infty,c]\mapsto P(X\le c)$ . Perhaps I should mention that there is a 'trivial' way of answering the question: a law $\mathcal{L}$ is defined as a function $\mathcal{B}\to[0,1]$ such that there exists a distribution $F$ with $F(c) = \mathcal{L}(-\infty,c]$ for any $c\in\mathbb{R}$ . However, I'm looking for a definition of a law that does not mention a distribution either.","['measure-theory', 'probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4576471,Show that a Levy measure $\nu$ (which arises from a convergence of Infinitely Divisible random vectors) is such that $\int x d\nu(x)=0$,"Let $(X_{jn})_{1\leq j \leq n}$ be a triangular array of $p-$ dimensional random vectors (row independent). Suppose $X_{jn} \sim \mu_{jn}$ and 1. $\,\, E X_{jn}= \int_{\mathbb R^p} x d \mu_{jn}=0$ 2. $\,\,\lim_{n \to \infty} \max_{1\leq j \leq n} P(|X_{jn}|> \epsilon)=0$ , for all $\epsilon > 0$ 3. $\,\,var(S_n):=\sum_{j=1}^n \int_{\mathbb R^p} |x|^2 d\mu_{jn} \leq C < \infty$ , for all $n \in \mathbb N$ . Assume that $S_n := \sum_{j=1}^n X_{jn} \Longrightarrow X $ , form some $X$ . Now, consider $Y_{jn} \sim CP(1,\mu_{jn})$ [ compound Poisson distribution, where the paramenter of the Poisson r.v. is $\lambda =1$ for all $(j,n)$ and the coumpounded vectors  are copies of $X_{jn}$ ]. Define $$S_n' := \sum_{j=1}^n Y_{jn}$$ It is easy to show that $E[S_n']=E[S_n]=0$ and $var[S_n']=var[S_n]$ . Moreover, we can show that the characteristic function of $S'_n$ is given by: $$\varphi_{S_n'}(u)=\exp\left\{ \int_{\mathbb R^p} \left[e^{iu'x} - 1 \right] d\nu_n \right\} = \exp\left\{ \int_{\mathbb R^p} \left[e^{iu'x} - 1 - iu'x \right] d\nu_n \right\}, \quad \nu_n(E):= \sum_{j=1}^n \int_E d\mu_{jn}, \quad E\, \,\hbox{ borelian set.}$$ By an argument of Accompanying Law (section 3.7 from the Varadhan' lecture notes ), we have that $S_n = \sum_{j=1}^n X_{jn} \Longrightarrow X $ if and only if $$S_n'= \sum_{j=1}^n Y_{jn} \Longrightarrow X $$ Using the theorem 8.7, page 41, from the Sato's book , we have $X$ is  Infinitely Divisible (I.D.) and its characteristic function is: $$\varphi_{X}(u) = \exp\left\{ \frac{- u'\sigma u}{2} + \int_{\mathbb R^p} \left[e^{iu'x} - 1 - iu'x \right] d\nu \right\}.$$ Moreover, $$\int f d\nu_n \to \int f d\nu \quad (n \to \infty),\quad \forall f \in \mathcal C_\#$$ ( $\mathcal C_\#$ is the class of continuous and bounded functions vanishing on a neighborhood of $0$ ). The mentioned theorem has another implication involving $\sigma$ , but I don't think it will be useful to mention it. According to this question , the last integral convergence is equivalent to \begin{equation}\label{asd}\tag{I}
\nu_n(E) \to \nu(E), \quad \forall E \in \mathcal{C}_\nu, \,\, 0 \notin \bar E
\end{equation} Where $\bar E$ is clousure of the borelian $E$ . Question: Since $\int_{\mathbb R^p} x d\nu_n = \sum_{j=1}^n \int_{\mathbb R^p} x d\mu_{jn} = 0$ for all $n$ , I suspect that $\int_{\mathbb R^p} x d\nu = 0$ .  How to show this? Although each $\nu_n$ is not a probability measure (since $\nu_n$ is a sum of $n$ probability measures), convergence in (\ref{asd}) looks a lot like a weak convergence of measures. Furthermore, given that $\sup_n \int x^2d\nu_n(x) < C $ , I could apply some similar uniform integrability result to conclude that $\int x d\nu_n(x) \to \int x d\nu (x)$ . Given that $\int x d\nu_n(x) =0$ , I would have the desired result. But I don't know how to do this rigorously. Update As said before, the mentioned theorem has another implication involving $\sigma$ and it was my fault for not specifying. Allow me to add it. First, for any $\epsilon>0$ , define the symetric non-neg-definite matrix $\sigma_{n,\epsilon}$ as  (actually, this involves a certain $\sigma_n$ , but in this case it's zero): \begin{equation}\label{new}\tag{II}
\langle u, \sigma_{n,\epsilon}u \rangle :=  \int_{|x|\leq \epsilon}\langle u ,x\rangle^2 d\nu_n(x), \quad u \in \mathbb R^p
\end{equation} Then: \begin{equation}\label{new2}\tag{III}
\lim_{\epsilon \downarrow 0} \limsup_{n \to \infty} \left| \langle u, \sigma_{n,\epsilon}u \rangle -  \langle u, \sigma u \rangle \right|=0
\end{equation} Initially, this question has already been answered. I appreciate any new answer using this new hypotheses given in (\ref{new}) and (\ref{new2}).","['levy-processes', 'measure-theory', 'weak-convergence', 'convergence-divergence', 'probability-theory']"
4576505,Proving an implication of two dimensional matrix.,"If $A = \begin{bmatrix} x & 1\\ y & 0\end{bmatrix}, B = \begin{bmatrix} z & 1\\ w & 0\end{bmatrix}$ , for $x,y,z,w \in \Bbb{R}$ . I have observed by considering many  examples of $x,y,z,w$ that: If all the eigen values of $A^2B$ and $AB^2$ are less than one in absolute value $\implies$ $\det(AB+A+I)<0$ and $\det(BA+B+I)<0$ is not possible. Any way how to prove it actually? I am thinking if $\det(AB+A+I)<0$ and $\det(BA+B+I)<0$ ,  then perhaps it would violate certain assumptions on the eigenvalues of $A^2B, AB^2$ ? Explicit forms of matrices: $A^2B = \begin{bmatrix} z(x^2+y)+xw & x^2+y\\ xyz+wy & xy\end{bmatrix}$ $AB^2 = \begin{bmatrix} x(w+z^2)+wz & xz+w\\ y(z^2+w) & yz\end{bmatrix}$ $AB +A+I = \begin{bmatrix}  xz+w+x+1& x+1\\ yz+y & y+1\end{bmatrix}$ $BA+B+I = \begin{bmatrix} xz+y+z+1 & z+1\\ xw+w & w+1\end{bmatrix}$","['matrices', 'optimization', 'determinant', 'linear-algebra']"
4576525,Dice game: bidding for sum,"Consider the following two-player game. Both players roll a fair die. They can see their own roll, but not their opponent's roll. Then, both players simultaneously choose a (possibly fractional) amount to bid. Whoever bids higher wins the sum of the two die rolls minus their bid, while the lower bidder gains nothing. When they bid the same amount, both players get nothing. What is the best strategy for this game? What are the Nash equilibria? Let n be the number you roll, then the random variable for the sum is uniform between $[n+1, n+6]$ . The expected value for the opponentâ€™s roll is 3.5 and so the expected sum would be $n + 3.5$ , I will bid as close to the expected sum as possible (so the first whole number that is below the expected sum $n + 3.5$ ), which would give me an expected payoff of $0.5$ . Is this solution correct? I don't know if adding the expected value part is the best strategy, but I assume I want to lower the chance of the opponent getting around the expected value. Edit: The question was not clear about whether the bid is announced to the other player, but since if it is announced we have to consider the sequence (whether I am first or second), I assume the bids are secret. Though I want to know what the strategies would be for the two different cases (secret and non-secret), if I can choose to bid first/second? You can bid any amount and the opponent is a rational player. To give an example of the run, you got 3 and yourâ€â€â€Œâ€Œâ€â€Œâ€Œâ€Œâ€â€Œâ€Œâ€Œâ€â€â€â€â€Œâ€â€â€ opponent got 6, and if you bid 7 and the opponent bids 8, whoever wins the bid will get 9 points minus the points they bid, so the opponent wins and get 1 point in this case. If there is a tie in the bids, then there is no prize. If you bid a 10 instead of a 7, then you would win the bid, and get -1 point.","['expected-value', 'dice', 'game-theory', 'probability-theory', 'probability']"
4576544,Solution of the differential equation $x^2(y-x\frac{dy}{dx})=y(\frac{dy}{dx})^2$ which does not contain singular solution,"Solution of the differential equation $x^2\big(y-x\frac{dy}{dx}\big)=y\big(\frac{dy}{dx}\big)^2$ which does not contain singular solution is A) $x^2(y-xc)=yc^2$ B) $y=cx+c^2$ C) $y^2=cx^2+c^2$ D) $xy=cx^2+c$ My Attempt: I put $\frac{dy}{dx}=p$ and tried differentiating w.r.t $x$ , but couldn't conclude. I also tried to rearrange the equation but couldn't form the Clairaut's equation.","['contest-math', 'calculus', 'derivatives', 'ordinary-differential-equations']"
