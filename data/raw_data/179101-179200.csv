question_id,title,body,tags
3249036,"How to solve the recurrence relation $a_{1}=2, a_{n}=\frac{a_{n-1}+2}{2 a_{n-1}+1}(n \geq 2)$ with generating functions?","There's already a way to solve it, called ""fixed point method"", that is, from the relation we define its characteristic equation as $x=\dfrac{x+2}{2x+1}$ ，then we have $x_1=1,x_2=-1$ . So the following relation established: $$
\frac{a_{n}-1}{a_{n}+1}=\frac{\frac{a_{n-1}+2}{2 a_{n-1}+1}-1}{\frac{a_{n-1}+2}{2 a_{n-1}+1}+1}=-\frac{1}{3} \cdot \frac{a_{n-1}-1}{a_{n-1}+1}
$$ It is obvious that $\displaystyle \frac{a_{n}-1}{a_{n}+1}=\frac{1}{3} \cdot\left(-\frac{1}{3}\right)^{n-1}$ , and then we have $a_{n}=\dfrac{3^{n}-(-1)^{n}}{3^{n}+(-1)^{n}}$ . My question is, how to solve this kind of recurrence relations with generating functions ? Also, ""fixed points"" can be applied to solving recurrences like $a_{n+1}=\dfrac{a_{n}^{2}+b}{2 a_{n}+d}$ , which seems impossible to solve using generating functions.","['generating-functions', 'systems-of-equations', 'recurrence-relations', 'sequences-and-series']"
3249046,Normal orthant probability in six variables,"Let $\mathbf{X} \sim N(\mathbf{0}, \mathbf{\Sigma})$ be a $6$ -dimensional Gaussian vector with covariance matrix of the form $$\mathbf{\Sigma} = \begin{pmatrix} 1 & c \\ c & 1 \end{pmatrix} \otimes \begin{pmatrix} 1 & 1/2 & 1/2 \\ 1/2 & 1 & 1/2 \\ 1/2 & 1/2 & 1 \end{pmatrix} = \begin{pmatrix} 
1 & 1/2 & 1/2 & c & c/2 & c/2 \\ 
1/2 & 1 & 1/2 & c/2 & c & c/2 \\
1/2 & 1/2 & 1 & c/2 & c/2 & c \\
c & c/2 & c/2 & 1 & 1/2 & 1/2 \\
c/2 & c & c/2 & 1/2 & 1 & 1/2 \\
c/2 & c/2 & c & 1/2 & 1/2 & 1
\end{pmatrix}$$ Here $c$ is some constant between $0$ and $1$ . I am interested in $\Pr(\mathbf{X} \geq \mathbf{0})$ , i.e. the probability that all of the six coordinates are non-negative/positive. This is also known as the orthant probability for $\mathbf{X}$ , and explicit formulas for orthant probabilities for arbitrary covariance matrices $\mathbf{\Sigma}$ are known in $2, 3, 4$ dimensions. The analysis seems very tedious and nasty though, and I am not sure if the same techniques generalize easily to $6$ dimensions. For my example of $6$ dimensions, my search has not yet returned any literature, attempting to solve this problem even in special cases with additional structure like above. I also tried computing this probability with Mathematica, but it cannot solve it analytically, and even numerically it seems to have a hard time to return exact results. My question is: is there any way to find the orthant probability for such a structured $6$ -dimensional matrix? Is there any literature I am missing? Or can someone solve this analytically? Edit : The paper https://ieeexplore.ieee.org/document/1054159 describes a derivation of the orthant probability for four variables, where \begin{align}
\mathbf{\Sigma} = \begin{pmatrix} 1 & c \\ c & 1 \end{pmatrix} \otimes \begin{pmatrix} 1 & d \\ d & 1 \end{pmatrix}.
\end{align} Perhaps it is possible to use similar techniques for this case as well (using a similar path of integration approach), although the reduction at the bottom of page 389 would not go from a $4$ -dimensional orthant probability to a $2$ -dimensional one, which can readily be evaluated, but from $6$ dimensions to $4$ dimensions, and adding another integral over these orthant probabilities. Still, this reference might be useful -- it might inspire similar techniques/approaches to this problem.","['geometric-probability', 'reference-request', 'multivariable-calculus', 'multiple-integral', 'probability']"
3249064,How Can the Units of an Angle Measurement Affect the Derivative of Sine?,"I've read the question: ""Why does the derivative of sine only work for radians?"" and I can follow the derivation for the derivative of sine when measured in degrees, but the result confuses me. Does this mean the derivative of the sine changes values when measured in different units? For example, would the derivative of sine at $45$ degrees not be the same as the derivative of the sine at $\pi/4$ radians? How could this be the case?","['calculus', 'derivatives', 'trigonometry']"
3249068,Continuity and differentiability for $g(x)$,"Below is my working: CONTINUITY AT $x=a$ * $\lim_{x\to a^-}g(x)=\lim_{h\to0}g(a-h)=0$ * $\lim_{x\to a^+}g(x)=\lim_{h\to0}g(a+h)=\lim_{h\to0}\int_{a}^{a+h}f(t)dt=\int_{a}^{a}f(t)dt=0$ * $g(a)=\int_{a}^{a}f(t)dt=0$ Therefore $g(x)$ is continuous at $x=a$ CONTINUITY AT $x=b$ * $\lim_{x\to b^-}g(x)=\lim_{h\to0}g(b-h)=\lim_{h\to0}\int_{a}^{b-h}f(t)dt=\int_{a}^{b}f(t)dt$ * $\lim_{x\to b^+}g(x)=\lim_{h\to0}g(b+h)=\lim_{h\to0}\int_{a}^{b}f(t)dt=\int_{a}^{b}f(t)dt$ * $g(b)=\int_{a}^{b}f(t)dt$ Therefore $g(x)$ is continuous at $x=b$ CONTINUITY AT $x=k\in (a,b)$ * $\lim_{x\to k^-}g(x)=\lim_{h\to0}g(k-h)=\lim_{h\to0}\int_{a}^{k-h}f(t)dt=\int_{a}^{k}f(t)dt$ * $\lim_{x\to k^+}g(x)=\lim_{h\to0}g(k+h)=\lim_{h\to0}\int_{a}^{k+h}f(t)dt=\int_{a}^{k}f(t)dt$ * $g(k)=\int_{a}^{k}f(t)dt$ CONTINUITY AT $x=k<a$ and $x=k>b$ It can be shown in a similar way that $g(x)$ is continuous at $x=k<a$ and $x=k>b$ DIFFERENTIABILITY AT $x=a$ * $\lim_{x\to a^-}\frac{g(x)-g(a)}{x-a}=\lim_{h\to 0}\frac{g(a-h)-g(a)}{-h}=\lim_{h\to 0}\frac{0}{h}=0$ * $\lim_{x\to a^+}\frac{g(x)-g(a)}{x-a}=\lim_{h\to 0}\frac{g(a+h)-g(a)}{h}=\lim_{h\to 0}\frac{\int_{a}^{a+h}f(t)dt-0}{h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{a+h}f(t)dt}{1}=\lim_{h\to 0}f(a+h)=f(a)=0$ Therefore $g(x)$ is differentiable at $x=a$ DIFFERENTIABILITY AT $x=b$ * $\lim_{x\to b^-}\frac{g(x)-g(b)}{x-b}=\lim_{h\to 0}\frac{g(b-h)-g(b)}{-h}=\lim_{h\to 0}\frac{\int_{a}^{b-h}f(t)dt-\int_{a}^{b}f(t)dt}{-h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{b-h}f(t)dt-0}{-1}=\lim_{h\to 0}f(b-h)=f(b)=\int_{a}^{b}f(t)dt$ * $\lim_{x\to b^+}\frac{g(x)-g(b)}{x-b}=\lim_{h\to 0}\frac{g(b+h)-g(b)}{h}=\lim_{h\to 0}\frac{\int_{a}^{b}f(t)dt-\int_{a}^{b}f(t)dt}{h}=\lim_{h\to 0}\frac{0}{h}=\lim_{h\to 0}0=0$ Therefore $g(x)$ is not differentiable at $x=b$ DIFFERENTIABILITY AT $x=k\in(a,b)$ * $\lim_{x\to k^-}\frac{g(x)-g(k)}{x-k}=\lim_{h\to 0}\frac{g(k-h)-g(k)}{-h}=\lim_{h\to 0}\frac{\int_{a}^{k-h}f(t)dt-\int_{a}^{k}f(t)dt}{-h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{k-h}f(t)dt-0}{-1}=\lim_{h\to 0}f(k-h)=f(k)=\int_{a}^{k}f(t)dt$ * $\lim_{x\to k^+}\frac{g(x)-g(k)}{x-k}=\lim_{h\to 0}\frac{g(k+h)-g(k)}{h}=\lim_{h\to 0}\frac{\int_{a}^{k+h}f(t)dt-\int_{a}^{k}f(t)dt}{h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{k+h}f(t)dt-0}{1}=\lim_{h\to 0}f(k+h)=f(k)=\int_{a}^{k}f(t)dt$ Therefore $g(x)$ is differentiable in $(a,b)$ DIFFERENTIABILITY AT $x=k<a$ and $x=k>b$ Similarly it can be shown that $g(x)$ is differentiable at $x=k<a$ and $x=k>b$ Am I correct? Is there any short way to solve this?","['integration', 'definite-integrals', 'calculus', 'limits', 'derivatives']"
3249235,Solving the ODE: $ y'(t)=y^2(t) - 5sin(t) - 25cos^2(t)$,How to analytically solve $$y'(t)=y^2(t)-5\sin(t)-25\cos^2(t)$$ Is the equation convergent? (for using numerical methods),['ordinary-differential-equations']
3249242,A definition of differentiable functions for arbitrary topological spaces,"Background It is well-known that there is no notion of derivative for arbitrary topological spaces. However while investigating the notion of derivative as we find in one variable real analysis I came to generalize the notion. I am now wondering what properties must this notion of ""differentiable function"" satisfy in order to become a ""good"" enough definition. Our motivation for the formulation of this definition is mainly Caratheodory's definition of derivative . The Definition Before going straight into the definition itself, let me mention at the outset that this question is a revised version of this deleted MO question. Let us first consider a certain special case of the definition in the following, Definition 1. Let $R$ be a field and $\tau_1,\tau_2$ be any two topologies on $R$ . A continuous function $f:R\to R$ will be said to be $(\tau_1,\tau_2)$ -differentiable on $R$ at $a\in R$ iff there exists a function $g:(R,\tau_1)\to (R,\tau_2)$ such that, $$g(x):=\begin{cases}\dfrac{f(x)-f(a)}{x-a}&\text{if}~x\ne a\\L&\text{else} \end{cases}$$ for all $x\in R$ and $g$ is continuous at $a$ . We can generalize the above definition as follows, Definition 2. Let $X,Y$ be arbitrary topological spaces. Let $g_a:X\to Y\times Y\times X\times X$ be the function defined by, $$g_a(x):=(f(x),f(a),x,a)$$ for all $x\in X$ . A function $f:X\to Y$ is said to be differentiable at $a\in X$ with value in a topological space $Z_a$ with respect to a function $$\psi_a:f(X)\times \{f(a)\}\times X\setminus\{a\}\times \{a\} \to Z_a$$ iff there exist a unique continuous function $$\Phi_a:f(X)\times \{f(a)\}\times X\times \{a\} \to Z_a$$ such that $\Phi_a\circ g_a$ is continuous at $a\in X$ and $$\Phi_a\Big|_{f(X)\times \{f(a)\}\times X\setminus\{a\}\times \{a\}}\equiv \psi_a$$ For example, if $X=Y=Z_a=\mathbb{R}$ (equipped with usual topology) and $f$ is differentiable at $a\in \mathbb{R}$ with, $\psi_a: f(X)\times \{f(a)\}\times X\setminus\{a\}\times \{a\} \to Z_a$ defined as follows, $$\psi_a\Bigl(f(x),f(a),x,a\Bigr)=\dfrac{f(x)-f(a)}{x-a}$$ We may then define $\Phi_a: f(X)\times \{f(a)\}\times X\times \{a\} \to Z_a$ as follows, $$\Phi_a\Bigl(f(x),f(a),x,a\Bigr)=\begin{cases}\dfrac{f(x)-f(a)}{x-a} &\text{if}~x\ne a\\ f'(a)&\text{else}\end{cases}$$ Some Remarks It was pointed out to me by Alexander Schmending in a comment below the deleted MO post which I have mentioned earlier that this paper is related to my query although he explicitly mentioned that the framework of the paper is restrictive than what I want in this post. I have briefly read the paper and indeed found the approach of the paper to be based on the same idea of generalizing Caratheodory's definition of derivative to more general setting. However even after reading the paper I found no obvious way of generalizing the approach of the authors to arbitrary topological spaces. Regarding the specific nature of relationship between $f$ and $\Phi_a\circ g_a$ , what I have in mind is something like the following: if $f$ is differentiable at $a$ then $(\Phi_a\circ g_a)(a)$ denotes the value of a derivative of $f$ at $a$ with value in $Z_a$ . (If we want a derivative of $f$ at $a$ we may impose the restriction that $Z_a$ be Hausdorff.) Question What properties should this notion of differentiable function must have so that it is a ""good"" enough definition of differentiable functions? How should I go about formulating a notion of derivative of $f$ at $a$ with value in $Z_a$ ?","['general-topology', 'definition', 'big-picture', 'metric-spaces']"
3249248,Equivalent ODE forward integration,"Consider a Cauchy problem $$\dot x(t)=f(t,x(t)),\quad x(T)=x_T \qquad (*)$$ where $T>0$ and we would like to compute $x(0)$ . I wonder how to construct an ode such that integrating forward the latter would lead to $x(0)$ .
I don't have precise idea on how to obtain such equivalent ode: I would use time reparameterization, change of coordinates ... I propose the ode: $$\dot z(t) = -f(t,z(T-t)),\quad z(0)=x_T, \quad t\in [0,T]
$$ then we can recover $x$ from $x(t)=z(T-t)$ . But in the general case (without symmetry on the solutions), this may be difficult to handle numerically, because in numerical implementation, we must provide $g(t,z(t))=-f(t,z(T-t))$ which is not suitable because we can't express (in general) $z(T-t)$ in terms of $z(t)$ . Are there any other odes that could be integrated forward to obtain $x(0)$ ?","['vector-fields', 'numerical-methods', 'ordinary-differential-equations', 'dynamical-systems']"
3249280,Time derivative of a measure and convolution,"Suppose $\mu_t : [0, + \infty) \to \mathcal{P}(\mathbb{R}^n)$ is a curve of probability measures on $\mathbb{R}^n$ . For each $\nu \in \mathcal{P}(\mathbb{R}^n)$ I define $\nu \ast \rho$ as the absolutely continuous measure w.r.t. the Lebesgue measure on $\mathbb{R}^n$ which density is given by $$ \int_{\mathbb{R^n}} \rho(x-y) d \nu(y)$$ where $\rho$ is a never vanishing smooth function integrating to $1$ w.r.t. the Lebesgue measure. Moreover I define $\frac{d}{dt} \mu_t$ as the functional on $C^{\infty}_c( [0, +\infty) \times \mathbb{R}^n)$ defined as $$ \langle \frac{d}{dt} \mu_t, \varphi \rangle = -\int_0^{+\infty} \int_{\mathbb{R}^n} \dot{\varphi}(x,t) d \mu_t(x) dt $$ I would like to prove that $$ \frac{d}{dt} (\mu_t \ast \rho) = \biggl ( \frac{d}{dt} \mu_t \biggr ) \ast \rho $$ but I am non sure about the meaning/definition of the RHS. Any help would be really appreciated! EDIT: I think I have understood it. If I use the definition of convolution of a distribution times a function I have that $$ \langle \biggl ( \frac{d}{dt} \mu_t \biggr ) \ast \rho , \varphi \rangle = \langle \frac{d}{dt} \mu_t, \tilde{\rho} \ast \varphi \rangle $$ for every $\varphi \in C^{\infty}_c( [0, +\infty) \times \mathbb{R}^n)$ where $$ \tilde{\rho}(x) := \rho(-x) \quad x \in \mathbb{R}^n $$ and $\rho \ast \varphi$ is the function defined as $$ (\tilde{\rho} \ast \varphi)(x,t) = \int_{\mathbb{R}^n} \tilde{\rho}(x-y) \varphi(x,t) dx = \int_{\mathbb{R}^n} \rho(y-x) \varphi(x,t) dx \quad x \in \mathbb{R}^n \, \, , \, \, t \in [0, +\infty)$$ This should work.","['measure-theory', 'convolution']"
3249281,Minimum number of times monkey must travel,"While working on an unrelated subject, I found a problem which could be alternatively be stated as the following: A monkey must travel a path of length $n-1$ , from $a_1$ to $a_n$ . On every turn, he may jump any distance forward, but cannot go backwards. He may jump any number of times. How many times must he travel the path so that he has jumped at least once from $a_i$ to $a_j$ for all $1\leq i\lt j\leq n$ ? (After reaching $a_n$ , one travel is completed and he starts again from $a_1$ .) The solution to the original problem (in chemistry) was done by brute force, but I am hoping to discover an expression in $n$ . My attempts Suppose the number of such paths is $f(n)$ . Then for a path to $a_{n+1}$ , it must be $f(n+1)$ . This can be achieved in the following method: Jump $1$ unit, then jump in the $f(n)$ ways. Then jump $2$ , then in $f(n-1)$ ways. And so on, thus getting $f(n+1)=\sum_1^nf(k)$ . But this is wrong, as a lot of these paths overlap, and I can find no way of figuring out how many. So I tried reformulation, again. Take series $\langle a_n\rangle_1^n$ of whole numbers such that $\sum a_k=n$ . Every nonzero element appears before every zero element. What is the minimum number of such series which must be taken, such that for every $i\geq1$ , $a_{n-i}$ is equal to every number from $1$ to $i$ in at least one of them? There are, I believe, formulas for the breaking of natural numbers into a sum of whole numbers, but here the conditions are different, and the order too matters. I am unsure of where to go from here. Please help.",['combinatorics']
3249300,"Relating $\int_0^1\frac{(\ln x)^{n-1}(\ln(1-z\,x))^p}{x}dx$ and $\int_0^1\frac{(\ln x)^{n}(\ln(1-z\,x))^{p-1}}{1-z\,x}dx$","This post , after a complicated analysis, evaluates the integral $$I=\int_0^1\frac{\ln^2(x)\,\ln^3(1+x)}xdx$$ simply as $$I
=-\frac{\pi^6}{252}-18\zeta(\bar{5},1)+3\zeta^2(3)\tag1$$ where, $$\zeta(\bar{5},1)=\frac{1}{24}\int^1_0\frac{\ln^4{x}\ln(1+x)}{1+x}{\rm d}x$$ More succinctly, $$I = -12\,S_{3,3}(-1)\tag2$$ with Nielsen generalized polylogarithm $S_{n,p}(z)$ . Question: How do we show that $\zeta(\bar{5},1)$ is also a Nielsen generalized polylogarithm in disguise? More generally, for $-1\leq z\leq1$ , how to show $$\begin{aligned}S_{n,p}(z)  
&= C_1\int_0^1\frac{(\ln x)^{n-1}\big(\ln(1-z\,x)\big)^p}{x}dx\\
&\overset{?}= C_2\int_0^1\frac{(\ln x)^{n}\;\big(\ln(1-z\,x)\big)^{p-1}}{1-z\,x}dx\end{aligned}\tag3$$ where, $$C_1 = \frac{(-1)^{n+p-1}}{(n-1)!\,p!},\qquad C_2 = \frac{(-1)^{n+p-1}}{n!\,(p-1)!}\color{red}z$$ If true, this implies, $$\zeta(\bar{5},1) \overset{\color{red}?}= S_{4,2}(-1)\tag4$$ Edit: It turns out the notation $\zeta(\bar{5},1)$ is a multiple zeta function so, $$\zeta(\bar{a},1)=\sum_{n=1}^{\infty}\frac{H_n}{(n+1)^a}\,(-1)^{n+1} = S_{a-1,2}(-1)$$ with harmonic numbers $H_n$ , hence $(4)$ indeed is true and is just the case $a=5$ . However, $(3)$ still needs to be proved in general.","['integration', 'polylogarithm', 'definite-integrals', 'closed-form']"
3249306,Find Maxima and Minima of $f( \theta) = a \sin^2 \theta + b \sin \theta \cos \theta + c \cos^2 \theta$,"Show that, whatever the value of $\theta$ , the expression $$a \sin^2 \theta + b \sin \theta \cos \theta + c \cos^2 \theta\ $$ Lies between $$\dfrac{a+c}{2} \pm \dfrac 12\sqrt{ b^2 + (a-c)^2} $$ My try: The given expression can be reduced as sum of sine functions as: $$(a-c) \sin^2 \theta + \dfrac b2 \sin 2 \theta + c \tag{*} $$ Now, there is one way to take everything as a function of $\theta$ and get the expression in the form of $ a \sin \theta + b \cos \theta = c$ and dividing it by $ \sqrt{ a^2 + c^2} $ both sides, but square in sine function is a big problem, also both have different arguments. Other way, I can think of is taking $ \tan \dfrac \theta 2 = t$ and getting sine and cosine function as $ \sin \theta = \dfrac{ 2t}{1+t^2} $ while cosine function as $ \dfrac{ 1-t^2} {1+t^2}$ solving. So getting $(*)$ as a function of $t$ , and simplifying we get, $$ f(t) = \dfrac{2 Rt + 2 R t^3 + R_0 t - R_0 t^3}{1+t^4 + 2t^2} + c\tag{1}$$ For $R_0 = 2b, R = (a-c)$ , but this is where the problem kicks in!, The Range of given fraction seems $ (-\infty,+ \infty)$ and is not bounded! So what's the problem here? Can it be solved? Thanks :) Edit : I'd like to thank  @kaviramamurthy for pointing out that as $t \rightarrow \pm \infty, f(t) \rightarrow c$ . That's a mistake here.","['lagrange-multiplier', 'maxima-minima', 'cauchy-schwarz-inequality', 'optimization', 'trigonometry']"
3249310,"If $K$ is compact and $(f_k)\subseteq C(K)$ is dense, then $x_n\to x$ in $K$ iff $f_k(x_n)\to f_k(x)$ for each $k$","Let $(K,d)$ be a compact metric space and $(f_k)_{k\in\mathbb N}\subseteq C(K)$ be dense (wrt the supremum norm). Let $(x_n)_{n\in\mathbb N}\subseteq E$ and $x\in E$ . How can we show that $d(x_n,x)\xrightarrow{n\to\infty}0$ iff $f_k(x_n)\xrightarrow{n\to\infty}f_k(x)$ for each $k\in\mathbb N$ ? The ""only if"" part is trivial, but how can we show the converse? Moreover, I would like to conclude that if $(a_n)_{n\in\mathbb N}\subseteq(0,\infty)$ with $\sum_{n\in\mathbb N}a_n<\infty$ , then $$\rho(x,y):=\sum_{k\in\mathbb N}a_n(|f_k(x)-f_k(y)|\wedge1)\;\;\;\text{for }x,y\in E$$ is a metric on $K$ equivalent to $d$ ? (And I read that since the identity mapping is uniformly continuous with respect to $d$ and $\rho$ , we may assume that $d=\rho$ ... I don't get that.)","['continuity', 'general-topology', 'metric-spaces']"
3249330,Brownian Motion and Complex Analysis,"I was recently taught in a lecture on complex analysis based off of this paper that much of complex analysis can be rephrased in the language of Brownian Motion. The paper gives some simple proofs of standard results in complex analysis using this language, but I would be interested in seeing a proof of Cauchy's Integral formula or his residue theorem. In particular, I am curious about how one could compute an integral using the ideas of Brownian Motion.","['complex-analysis', 'brownian-motion']"
3249356,Law of large numbers for a sequence of random variables,"Suppose we have a sequence of random variables $X^M$ which converges almost surely to a random variable $X^0$ and let $(X_1^M, \ldots, X_M^M)$ be iid samples from $X^M$ for $M \in \mathbb{N}$ . Under which conditions does the law of large numbers hold uniformly in the sense that $$ |M^{-1} \sum_{m=1}^M X_m^M - \mathbb{E}(X^0)| \xrightarrow{M \to \infty} 0 \; \text{ almost surely?}$$ I am happy to assume that the $X^m$ are uniformly bounded, i.e., that there is a constant $K$ such that $$ |X^m| \leq K, \; \text{almost surely for all } m \in \mathbb{N}_0.$$ I looked into uniform LLNs but they generally do not seem to fit the setting above.","['statistics', 'law-of-large-numbers', 'probability-theory']"
3249393,Why derivatives of the matrix annihilating polynomial must be equal to zero?,"In order to get some background in functions of matrices I'm studying Chapter
5 of Gantmacher ""The theory of matrices"" book.
We have two arbitrary polynomials g( $\lambda),
    $ h( $\lambda)$ such that g(A) = h(A), formula (2). Don't understand formula (4) p. 95: why $d'(\lambda_k) = 0$ ? It's quite clear why $d(\lambda_k) = 0$ because $d(\lambda_k)$ consists of multipliers, one of them is the minimal
polynomial, and $\lambda_k$ is one of his roots. But why the same must be hold
for derivatives of $d(\lambda_k)$ ? For example  A = $\begin{matrix}
3 & -3 & 2 \\
-1 & 5 & -2 \\
-1 & 3 & 0
\end{matrix}$ $\lambda_1$ = 2, $\lambda_2$ = 4 g( $\lambda$ ) = $\lambda^3$ - 6 $\lambda^2$ + 8 $\lambda$ h( $\lambda$ ) = $\lambda^4$ - 6 $\lambda^3$ + 8 $\lambda^2$ both are annihilating. d( $\lambda$ ) = g( $\lambda$ ) - h( $\lambda$ ) = - $\lambda^4$ + 7 $\lambda^3$ - 14 $\lambda^2$ + 8 $\lambda$ d( $\lambda_1$ ) = d(2) = 0. d'( $\lambda$ ) = -4 $\lambda^3$ + 21 $\lambda^2$ - 28 $\lambda$ + 8 d'( $\lambda_1$ ) = d'(2) = -4*2^3 + 21*2^2 - 28*2 + 8 $\neq$ 0. So why formula (4) doesn't hold in my case? Reference Felix Ruvimovich Gantmacher (2000)[1977] "" The Theory of Matrices. Volume One "", AMS Chelsea Publishing, pp. x+374. EDIT: $\psi(\lambda) = (\lambda - \lambda_1)^m1(\lambda - \lambda_2)^m2...(\lambda - \lambda_s)^ms$ $d(\lambda) = \psi(\lambda)q(\lambda)$ $d'(\lambda) = \psi(\lambda)'q(\lambda) + \psi(\lambda)q'(\lambda)$ $\psi(\lambda)q'(\lambda) = 0$ $\psi(\lambda)' = \frac{d}{d\lambda}(\lambda - \lambda_1)^m1[(\lambda - \lambda_1)^m2...(\lambda - \lambda_s)^ms] + (\lambda - \lambda_1)^m1\frac{d}{d\lambda}(\lambda - \lambda_2)^m2(\lambda - \lambda_3)^m3...(\lambda - \lambda_s)^ms$ $\frac{d}{d\lambda}(\lambda - \lambda_1)^m1[(\lambda - \lambda_2)^m2...(\lambda - \lambda_s)^ms] = m1(\lambda - \lambda_1)^(m1-1)(\lambda - \lambda_2)^m2...(\lambda - \lambda_s)^ms = 0$ $(\lambda - \lambda_1)^m1\frac{d}{d\lambda}(\lambda - \lambda_2)^m2(\lambda - \lambda_3)^m3...(\lambda - \lambda_s)^ms = (\lambda - \lambda_1)^m1[\frac{d}{d\lambda}(\lambda - \lambda_2)^m2[(\lambda - \lambda_3)^m3...(\lambda - \lambda_s)^ms] + (\lambda - \lambda_2)^m2\frac{d}{d\lambda}(\lambda - \lambda_3)^m3...(\lambda - \lambda_s)^ms]]$ $(\lambda - \lambda_1)^m1[\frac{d}{d\lambda}(\lambda - \lambda_2)^m2[(\lambda - \lambda_3)^m3...(\lambda - \lambda_s)^ms] = (\lambda - \lambda_1)^m1 m2(\lambda - \lambda_2)^(m2-1)(\lambda - \lambda_3)^m3...(\lambda - \lambda_s)^ms = 0$ If we will continue in the same order finally we get all the derivatives equal to zero.","['matrices', 'functions']"
3249483,Lebesgue measurability of subset of the preimage of a measurable set under increasing absolutely continuous function,"For an increasing function F absolutely continuous on $[a,b]$ , $E$ be a subset of $[F(a),F(b)]$ . The preimage of $E$ , $F^{-1}(E)$ , is not neccessarily measurable, however, $F^{-1}(E)\cap\{F'(x)>0\}$ is. This is problem 20 from chapter 3 of Stein's Real Analysis and I'm having problem showing the intersection is measurable. There is a hint suggesting to show $m(O)=\int_{F^{-1}(O)}F'$ for open sets, which I can prove but have no idea how to use.","['measure-theory', 'lebesgue-measure', 'absolute-continuity']"
3249484,Divide square into congruent isosceles triangles,"Can I divide a square using other congruent isosceles triangles than $45-90-45$ ones? For example, can I use $30-120-30$ congruent triangles to completely cover a square without overlapping or extending outside the square? My single argument so for is that the corner of the square must be composed only of one type of angle(isosceles triangles have two ""types"" of them: 2 at the base and one at top). That comes from the fact that $2a + b = 180; a, b > 0; a + b < 90 => a > 90$ (impossible). Also the chosen angle that is chosen for completing the corner must divide 90 exactly. Namely $90/a = i$ , where $i$ is an positive integer.","['triangles', 'geometry']"
3249487,"Can an $(a,b)$-knight reach every point on a chessboard?","An $(a,b)$ -knight moves $a$ units horizontally and $b$ units vertically (or $b$ horizontally and $a$ vertically) for each move. For example, the traditional knight is a $(1,2)$ - or $(2,1)$ -knight. Does there exist general algorithms to the following problems? Given $a,b$ and an infinite chessboard, can an $(a,b)$ -knight reach every point on the chessboard no matter where it starts? Given $a,b$ and an $m\times n$ chessboard, can an $(a,b)$ -knight reach every point on the chessboard no matter where it starts? Here you can make the assumption that $m,n\gg a,b$ so the space won't be too limited for the knight to move. In an infinite chessboard, it should be simpler, because the knight can reach every point if and only if it can achieve a single-unit up, down, left and right movement. For $m\times n$ chessboards though, I guess there might still be problems (or requirement for special treatment) with the edge or corner points, even when $m,n\gg a,b$ ?","['recreational-mathematics', 'discrete-mathematics']"
3249545,Bounding spectral norm of matrix of binomial entries with small probabilities,"Consider an $n \times n$ matrix $X$ where entries $$
    X_{ij} = \begin{cases}
        C, & \text{w.p. } p\\
        0, & \text{w.p. } 1-p,\\
        \end{cases}
$$ where $p$ is very small. I am interested in bounding the spectral norm $\|X\|$ . The entries of $X_{ij}$ are sub-Gaussian with $\|X_{ij}\|_{\psi_2} = \frac{C}{\sqrt{\log(2/p)}}$ , and as such, Theorem 4.4.5 of Vershynin gives $$\|X\| \lesssim \|X_{ij}\|_{\psi_2}\sqrt{n}$$ with high probability. The definition of sub-Gaussian norm $\| \cdot \|_{\psi_2}$ I am using here is Definition 2.5.6 in Vershynin. This is fine if $p=0.5$ or so, but in my case, $p$ is very small. And as such, this bound is not tight at all. I would intuitively expect that the spectral norm should scale as $\sqrt{pn}$ or something similar. In my case, $X_{ij}$ is small because it is only large with very small probability. This is not captured by the sub-Gaussian norm, because all it cares about are the tails (which are sub-Gaussian for any bounded random variable). There is an analogous issue in the scalar setting. The sub-gaussian random variables are exactly those variables that obey a Hoeffding's inequality (Theorem 2.2.2 in Vershynin). However, as he points out in Section 2.3, the Hoeffding inequality is useless for Bernoulli random variables with small $p$ . Instead, you want to use the Chernoff inequality (Theorem 2.3.1) which is sensitive to small $p$ . Are there any bounds for $\|X\|$ when the entries are Bernoulli with small $p$ ?","['normed-spaces', 'binomial-distribution', 'matrices', 'matrix-norms', 'random-matrices']"
3249563,For every sufficiently large $m$ there exists $k$ such that $m = k + \tau(k)$,"Let $\tau(k)$ , be the number of positive divisors of natural number $k$ . Is it true, that there exists $n_0$ , such that for every $m\geq n_0$ there exists $k  \in\mathbb{N}$ such that: $$
m = k + \tau(k)
$$ I have tried to use the following formula for $\tau$ : $$
\tau(p_1^{k_1}\ldots p_{s}^{k_s}) = (k_1 +1)\cdot\ldots \cdot(k_s +1),
$$ Where $p_1, \ldots, p_s$ are different prime numbers. Intuitively, I think that the answer will be no. So, we can assume the contrary (that such $n_0$ exists) and try some infinite series of numbers (primes, factorials, primorials, etc.), which can't be written in the form $k + \tau(k)$ for every $k$ . But my attempts weren't successful. So, I will be grateful for hints and ideas.","['number-theory', 'divisor-counting-function', 'divisibility', 'elementary-number-theory']"
3249623,Least-square fitting to data (sine function) : what is the error of the derived fit parameters?,"I have a set of data. I want to fit it to a sine function of the form : \begin{equation}
f(x)=A sin(\omega x+B)+C
\end{equation} I use the least-square method to find the appropriate fit-parameters which are $A$ , $B$ and $C$ . In this method, each term of the cost-function has a weight calculated from the error-bar of each point in my dataset. Now I want to calculate the visibility $V$ for the fitting curve. The visibility is defined by : \begin{equation}
V=\frac{A}{C}
\end{equation} I obtain a good value of $V=0.95$ , but now I want to know how to calculate $\Delta V$ , the error of the visibility. To get it, I need to know $\Delta A$ and $\Delta C$ . Do you know how to do it ? EDIT : Some people suggested to post the data, so here is the figure on the link below. Figure representing the data Basically, each point has a poissonnian error bar $\Delta Y= \sqrt{Y}$ . I did the weighted least-square method to obtain my fit-function which is the solid line you can see on this plot (there is two data-set actually, red and blue). The area in red/blue represent standard deviation of the distance in errorbar unit from the datapoints to the fit, multiplied by the poissonian error $\sqrt{Y}$ [I don't know if this is okay, maybe it's false to do like that].","['mean-square-error', 'analysis', 'data-analysis', 'least-squares', 'mathematical-physics']"
3249646,"Closed forms of Nielsen polylogarithms $\int_0^1\frac{(\ln t)^{n-1}(\ln(1-z\,t))^p}{t}dt$?","( This summarizes my posts on Nielsen polylogs .) I. Question 1: How to complete the table below? Consider the special cases $z=-1$ and $z=\frac12$ . Given the Nielsen generalized polylogarithm , $$S_{n,p}(z) = C_1\int_0^1\frac{(\ln t)^{n-1}\big(\ln(1-z\,t)\big)^p}{t}dt$$ where, $$C_1 = \frac{(-1)^{n+p-1}}{(n-1)!\,p!}$$ then for what $n,p$ are there closed-forms in terms of ordinary polylogarithms $\rm{Li}_m(x)$ ? Note: For $p=1$ , then the Nielsen polylog just reduces to $\rm{Li}_m(x)$ . MSE has a lot of posts asking for close-forms (see this , this , etc), most of which do not name the integral as a Nielsen polylog. The table below summarizes known results (so far), $$\begin{array}{|c|c|c|c|c|}
\hline
n+p&n&p&z=-1&z=\tfrac12\\
\hline
3&1 &2 &Y&Y\\
3&2 &1 &Y&Y\\
\hline
4&1 &3 &Y&Y\\
4&2 &2 &Y&Y\\
4&3 &1 &Y&Y\\
\hline
5&1 &4 &Y&Y\\
5&2 &3 &Y&Y\\
5&3 &2 &Y&Y\\
5&4 &1 &Y&Y\\
\hline
6&1 &5 &Y&Y\\
6&2 &4 &-&-\\
6&3 &3 &\color{red}N&\color{red}Y\\
6&4 &2 &-&-\\
6&5 &1 &Y&Y\\
\hline
7&1 &6 &Y&Y\\
7&2 &5 &-&-\\
7&3 &4 &-&-\\
7&4 &3 &-&-\\
7&5 &2 &\color{red}Y&\color{red}N\\
7&6 &1 &Y&Y\\
\hline
\end{array}$$ Surprisingly, $S_{3,3}\big(\tfrac12\big)$ is expressible, but $S_{3,3}(-1)$ is not. Let $a=\ln 2$ , then, $$2\,S_{3,3}\big(\tfrac12\big) =\tfrac{23}{16}\zeta(6)-2a\zeta(5)+\tfrac18a^2\zeta(4)-\tfrac1{16}a^3\zeta(3)+\tfrac1{72}a^6-\zeta^2(3)+a\big(S_{3,2}\big(\tfrac12\big)-S_{2,3}\big(\tfrac12\big)+\zeta(2)\zeta(3)\big)$$ Since the two Nielsen polylogs in RHS are expressible as ordinary polylogs (see here ), then so is the LHS. Conversely, $S_{5,2}(-1)$ is expressible, $$128S_{5,2}(-1) = 64\zeta(2)\zeta(5)+112\zeta(3)\zeta(4)-251\zeta(7)$$ but $S_{5,2}\big(\tfrac12\big)$ apparently is not . II. Question 2: What simple relations are there between the inexpressible(?) Nielsen polylogs in the table? Two are, $$16S_{3,3}(-1)-24S_{4,2}(-1)=-4\zeta^2(3)+5\zeta(6)$$ $$128S_{3,4}(-1)-192S_{4,3}(-1)=-64\zeta(2)\zeta(5)-160\zeta(3)\zeta(4)+315\zeta(7)$$ which is mentioned in this and this post. (The two seem to belong to a general form.) Are there others?","['integration', 'definite-integrals', 'big-list', 'polylogarithm', 'closed-form']"
3249670,Intrinsic characterization of sets of subsets of A representing some ordering in A?,"I'm currently working my way through Naive Set Theory by Paul Halmos and am confused by what sort of answer might satisfy this exercise on page 23: Find an intrinsic characterization of those sets of subsets of $A$ that correspond to some order in $A$ . This exercise is in Section 6: Ordered Pairs , and comes after a discussion of how to define the ordering of a quadruplet $\{a, b, c, d\}$ by generating a set where every element is a set that contains the element in question along with every element that comes before the element in the supplied ordering. So the set representing the ordering $c, b, d, a$ would be: $C = \{\{c\}, \{c, b\}, \{c, b, d\}, \{c, b, d, a\}\}$ I can make various statements about this sort of set of subsets, like every element of $C$ has an ordering that's a subset of $C$ But I'm sure this doesn't count as an intrinsic characterization. What does an intrinsic characterization of sets of subsets of $A$ look like? How can I think about the answer to this question? Thank you!",['elementary-set-theory']
3249692,"Proving there is a bijection from set of real continuous function on $[0,1]$ denoted by $C[0,1]$ to the set of reals $\mathbb{R}$ [duplicate]","This question already has answers here : Cardinality of set of real continuous functions (6 answers) Closed 5 years ago . I have been trying to establish that there is a bijection from the set of real continuous function on $[0,1]$ denoted by $C[0,1]$ to the set of reals $\mathbb{R}$ . I have been using Cantor-Schröder-Bernstein theorem to prove this. Consider the function: $f:\mathbb{R} \to C[0,1]$ by $f(x)$ equals the constant function $g:[0,1]\to\mathbb{R}$ given by $g(y)=x$ for each $y\in[0,1]$ . Clearly, $f$ is an injection. I have been trying to find an injection from $C[0,1]$ to $\mathbb{R}$ but have been unsuccessful in doing so.  I suspected that $h:C[0,1]\to\mathbb{R}$ defined by $h(f)=\int_{0}^{1}f$ would work but certainly it doesn't. Hints would be appreciated. TL,DR: Find an injection from $C[0,1]$ to $\mathbb{R}$ .",['elementary-set-theory']
3249710,Proof of Blumenthal's 0-1 law for Brownian Motion,"I am currently reading the book ""Brownian Motion, Martingales, and Stochastic calculus"" by Jean-François Le Gall and am stuck at understanding the proof of Blumenthal's 0-1 law for Brownian Motion. The Setup is the following: Assume we have a one-dimensional Brownian $(B_t)_{t \geq 0}$ on some probability space $(\Omega, \mathcal{F}, P)$ . For $t \geq 0$ let $\mathcal{F}_t= \sigma(B_s: 0 \leq s \leq t)$ and define $\mathcal{F}_{0+}= \cap_{\epsilon > 0} \mathcal{F}_\epsilon$ . Then the following theorem holds. Theorem : The sigma-algebra $\mathcal{F}_{0+}$ is trivial in the sense that for all $A \in \mathcal{F}_{0+}$ , the probability of $A$ is either $0$ or $1$ . I will outline the proof that can be found in the book with the part I do not understand: Proof outline: Using $\cap$ -stable generators, it will be enough to proof that for all $n \in \mathbb{N}$ and $0 < t_1 < ... < t_n$ the sigma-algebra $\mathcal{F}_{0+}$ and $\sigma(B_{t_1},...,B_{t_n})$ are independent. Now let $g: \mathbb{R}^n \rightarrow \mathbb{R}$ be bounded and continuous and $A \in \mathcal{F}_{0+}$ . By continuity and dominated convergence we can write for $0 < \epsilon < t_1$ , $$
\begin{align}
E [1_A g(B_{t_1},...,B_{t_n})] = \lim_{\epsilon \rightarrow 0} E [1_A g(B_{t_1} - B_\epsilon,...,B_{t_n}- B_\epsilon)].
\end{align}
$$ Now by the simple Markov property of Brownian Motion, $(B_{t+\epsilon}-B_\epsilon)_{t \geq 0}$ is independent of $\mathcal{F}_\epsilon$ and thus we can continue to write the above to $$
\begin{align}
\lim_{\epsilon \rightarrow 0} E [1_A g(B_{t_1} - B_\epsilon,...,B_{t_n}- B_\epsilon)] &= P(A) \lim_{\epsilon \rightarrow 0} E[g(B_{t_1} - B_\epsilon,...,B_{t_n}- B_\epsilon)] \\
&= P(A) E[g(B_{t_1},...,B_{t_n})].
\end{align}
$$ This then should be enough to conclude independence. I do not know why this should suffice. If $g$ was allowed to be measurable, then it would be clear. But how does independence of $\mathcal{F}_{0+}$ and $\sigma(B_{t_1},...,B_{t_n})$ follow from $g$ only being bounded continuous? Thanks a lot in advance!","['stochastic-calculus', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
3249719,"If $(E,d)$ is compact and $(f_k)⊆C(E)$ is dense, then $\text E[1∧d(X_n,X)]\to0$ iff $\|f_k(X_n)-f_k(X)\|_{L^1}\to0$ for all $k∈ℕ$","Let $(E,d)$ be a compact metric space and $(f_k)_{k\in\mathbb N}\subseteq C(E)$ be dense. We can show that $$d(x_n,x)\xrightarrow{n\to\infty}0\Leftrightarrow\forall k\in\mathbb N:f_k(x_n)\xrightarrow{n\to\infty}f_k(x)\tag1$$ for all $(x_n)_{n\in\mathbb N}\subseteq E$ and $x\in E$ . Now let $(\Omega,\mathcal A,\operatorname P)$ be a probability space. Are we able to show that $$\operatorname E\left[1\wedge d(X_n,X)\right]\xrightarrow{n\to\infty}0\Leftrightarrow\forall k\in\mathbb N:\left\|f_k(X_n)-f_k(X)\right\|_{L^1(\operatorname P)}\xrightarrow{n\to\infty}0\tag2$$ for all $(\mathcal A,\mathcal B(E))$ -measurable $X_n,X:\Omega\to E$ for $n\in\mathbb N$ ? I wonder if this is somehow immediate from $(1)$ . But since I don't see how, I've tried to mimic the proof of $(1)$ . So, let's consider the "" $\Leftarrow$ "" direction in $(2)$ : As in the proof of $(1)$ , we are able to show that $$\left\|f(X_n)-f(X)\right\|_{L^1(\operatorname P)}\xrightarrow{n\to\infty}0\tag3$$ for all $f\in C(E)$ . However, in the proof of the $\Leftarrow$ direction of $(1)$ , we would now note that, given $\varepsilon>0$ , $$f:=\frac{d\left(\;\cdot\;,{B_\varepsilon(x)}^c\right)}{d\left(\;\cdot\;,{B_\varepsilon(x)}^c\right)+d\left(\;\cdot\;,\overline B_{\varepsilon/2}(x)\right)}$$ is a Urysohn function for ${B_\varepsilon(x)}^c$ and $\overline B_{\varepsilon/2}(x)$ . Now by the result corresponding to $(3)$ , $$f(x_n)\xrightarrow{n\to\infty}f(x)=1\tag4$$ and hence $$f(x_n)>0\;\;\;\text{for all }n\ge N\tag5$$ for some $N\in\mathbb N$ which yields $$x_n\in B_\varepsilon(x)\;\;\;\text{for all }n\ge N.\tag6$$ In the probabilistic setting there is no immediate analogue to this approach. So, how do we need to proceed? Do we even need to start from scratch or is $(2)$ somehow obvious from $(1)$ ? EDIT : Maybe we need to replace the condition on the right-hand side of $(2)$ by $$\sum_{k\in\mathbb N}a_k\left(1\wedge\left\|f_k(X_n)-f_k(X)\right\|_{L^1(\operatorname P)}\right)\xrightarrow{n\to\infty}0\tag7$$ for some $(a_k)_{k\in\mathbb N}\subseteq(0,\infty)$ with $\sum_{k\in\mathbb N}a_k<\infty$ . We may at least note that $$\sum_{k\in\mathbb N}a_k\operatorname E\left[1\wedge\left|f_k(X_n)-f_k(X)\right|\right]=\operatorname E\left[\rho(X_n,X)\right]\tag8$$ by the dominated convergence theorem for all $n\in\mathbb N$ , where $$\rho(x,y):=\sum_{k\in\mathbb N}a_k(1\wedge|f_k(x)-f_k(y)|)\;\;\;\text{for }x,y\in E$$ is a metric on $E$ equivalent to $d$ . Now we may note the following: Writing $2(a\wedge b)=a+b-|a-b|$ for all $a,b\in\mathbb R$ , we see that $2\operatorname E[1\wedge Y]=1+\operatorname E[Y]-\operatorname E[|1-Y|]$ and $2(1\wedge\operatorname E[Y])=1+\operatorname E[Y]-|1-\operatorname E[Y]|$ for all real-valued random variables $Y$ on $(\Omega,\mathcal A,\operatorname P)$ . But since $|1-\operatorname E[X]|=|\operatorname E[1-Y]|\le\operatorname E[|1-Y|]$ , we can conclude that $\operatorname E[1\wedge X]-1\wedge\operatorname E[X]\le0$ . This yields $$\operatorname E[\rho(X_n,X)]\le\sum_{k\in\mathbb N}a_k(1\wedge\operatorname E\left[|f_k(X_n)-f_k(X)|\right]\xrightarrow{n\to\infty}0\tag9$$ and hence $$\rho(X_n,X)\xrightarrow{n\to\infty}0\;\;\;\text{in probability}.\tag{10}$$ This would be enough to conclude (by the dominated convergence theorem) if we could replace $\rho$ by $d$ in $(10)$ . While these metrics are equivalent (i.e. they generate the same topology), I need to admit that I'm not sure if convergence in probability depends on the metric chosen (it should be consistent for strongly equivalent metrics though). EDIT 2 : I guess convergence in probability does only depend on the generated topology, but does anyone has a reference?","['measure-theory', 'metric-spaces', 'continuity', 'general-topology', 'probability-theory']"
3249771,What is wrong with this proof that symmetric matrices commute?,"Symmetric matrices represent real self-adjoint maps, i.e. linear maps that have the following property: $$\langle\vec{v},f(\vec{w})\rangle=\langle f(\vec{v}),\vec{w}\rangle$$ where $\langle,\rangle$ donates the scalar (dot) product. Using this logic: $$\langle\vec{v},AB\vec{v}\rangle=\langle A\vec{v},B\vec{v}\rangle=\langle BA\vec{v},\vec{v}\rangle$$ Where $A$ and $B$ are symmetric matrices. Using the fact that the real scalar dot product is commutative: $$\langle BA\vec{v},\vec{v}\rangle=\langle\vec{v},BA\vec{v}\rangle$$ We therefore have the result: $$\langle\vec{v},AB\vec{v}\rangle=\langle\vec{v},BA\vec{v}\rangle$$ This holds true for any real vector $\vec{v}$ so therefore $AB=BA$ . However, symmetric matrices do not always commute so something is wrong with this proof.","['self-adjoint-operators', 'proof-verification', 'fake-proofs', 'linear-algebra', 'symmetric-matrices']"
3249780,Alternating sum of binomial coefficients multiplied by index to an n-2 extent,"Doing research in probability modelling I obtained an identity, which is correct for $n > 2$ . $$\sum\limits^n_{i=1}(-1)^{n+i}{{n}\choose{i}}i^{n-2}=0$$ How can it be proven directly? In what literature can I find it?","['summation', 'binomial-coefficients', 'combinatorics', 'induction']"
3249846,How to Analytically Solve this PDE?,"Thanks for looking at my question. I'm working through/self-studying the second edition of Partial Differential Equations: An Introduction by Walter A. Strauss. On page three, example two, he says ""Solve the PDE $u_{xx} + u = 0$ . Again, its really an ODE with an extra variable y. We know how to solve the ODE, so the solution is $u = f(y)cos(x) + g(y)sin(x)$ , where again $f(y)$ and $g(y)$ are two arbitrary functions of $y$ . You can easily check this formula by differentiating twice to verify that $u_{xx} = -u$ ."" What I don't understand is how he gets $u = f(y)cos(x) + g(y)sin(x)$ . He says it's basically just an ODE with an extra variable y, but I'm not quite seeing that. I was able to understand Example 1 before it and Example 3 after it, and I can sort of see that this PDE is similar to the ODE form $y'' + y = 0$ , but it's just been a hot minute since I've solved an ODE like this. I see it's homogenous, and one could use the method of integrating factors for it, but since this is a PDE I'm not sure how to solve this. My initial guess of $C_{1}e^{r_{1}t} + C_{2}e^{{r_2}t}$ didn't work, so I'm not sure how they got what they got for $u$ . I get that when you integrate with respect to $x$ the constant you get is a function of $y$ , but that's all I understand about this problem. Could someone show me how Walter got his solution for $u$ , please? Thanks.","['ordinary-differential-equations', 'partial-differential-equations']"
3249848,consistency of Neyman Pearson lemma in the case simple vs simple test for exponential families,"Basics, jump to section 2 for the question : I know that in the case of an exponential family with 1 parameter, meaning the distribution function of the sample variables can be written like : $$ f_X(t) = \exp( \eta( \theta) T(t) - d( \theta) + S( x) ) $$ if we compare two hypothesis like those : $$ H_0 : \theta = \theta_0 \text{ vs }  H_1 :\theta = \theta_1$$ Neyman-Pearson lemma tells us that : if $\eta$ is increasing we get this test : $$ \mathbb{1}_{ \sum t_i \geq q_{1 - \alpha}} $$ if $\eta$ is decreasing we get this test : $$ \mathbb{1}_{ \sum t_i \leq q_{\alpha}} $$ where $q_{\lambda }$ is the solution of this equation  ( $\alpha$ -quantiles ): $$ P( \sum t_i \leq q_{\alpha} | H_0 ) = \alpha $$ When I first started doing exercices using this formula, i didn't know if I should put constants in the $\eta$ or in the $T$ function. I've succesfully proven that whether you put a positive multiplicative constant in one of those, you get the same result, meaning the two tests functions are the same if you multiply one function by $a$ and the other by $1/a$ . However, i can't do it for $ a = -1$ . I got to this point : How do you prove the equality of the two following tests : if I'm applying formulas : $$ \mathbb{1}_{ \sum t_i \geq q_{1 - \alpha}} $$ $$ P( \sum t_i \leq q_{1 -\alpha} | H_0 ) = 1 - \alpha $$ if I write $-\eta( \theta) \times - T(t)$ instead : $$ \mathbb{1}_{ \sum t_i \geq - q_{\alpha}} $$ $$ P( \sum t_i \geq - q_{\alpha} | H_0 ) = \alpha $$ I've succesfully done that for a gaussian distribution, but I can't do the general case. disclaimer : I'm still learning basics of statistics. So sorry for the possible trivialness of the questions. Moreover, i don't yet get everything and maybe, I ve set some hypothesis or constants as obviously shared ideas whereas it wasn't. So please free to ask if i forgot anything. Thank you.","['statistical-inference', 'statistics', 'hypothesis-testing']"
3249859,2 questions about the morphism $\mathbb{A}_k^{n+1}\backslash\{0\} \rightarrow \mathbb{P}_k^n$,"An exercise in Ravi Vakil's algebraic geometry notes says ""Make sense of the following sentence: $$\pi: \mathbb{A}_k^{n+1}\backslash\{0\} \rightarrow \mathbb{P}_k^n$$ given by $$(x_0,x_1...,x_n)\mapsto [x_0,x_1...,x_n]$$ is a morphism of schemes.'"" I assume he means the morphism with $$Spec~ k[x_0,x_1,..,x_n]_{x_i}\rightarrow Spec~(k[x_0,x_1,..,x_n]_{x_i})_0 $$ induced by the inclusion $$(k[x_0,x_1,..,x_n]_{x_i})_0 \subset  k[x_0,x_1,..,x_n]_{x_i}$$ (where here $(k[x_0,x_1,..,x_n]_{x_i})_0$ is the degree zero elements of the ring $k[x_0,x_1,..,x_n]_{x_i}$ ). Is this correct? And I have another question:  I would have thought that the image $\pi(\mathfrak{p})$ of a point $\mathfrak{p}\in\mathbb{A}_k^{n+1}\backslash\{0\}$ could be described as follows:  If $x_i\notin \mathfrak{p}$ , and $$\mathfrak{p} = (f_1,...,f_m)$$ then we use $x_i$ to ""homogenize"" each $f_j$ -- that is we multiply each term of $f_j$ by some power of $x_i$ so that the resulting $f_j'$ is homogeneous. Then $$\pi(\mathfrak{p}) = (f_1',...,f_m') $$ (Here we are thinking of $\mathbb{P}_k^n$ as the space of all homogeneous primes of $k[x_0,..,x_n]$ not containing the irrelevant ideal) Is this correct? This is my intuition, but I can't seem to prove it using the map in the first question.",['algebraic-geometry']
3249944,Coloring $n$ chain with $k$ colors,"Chains are made from beads, each in one of $k$ colors. In each chain there is $n$ beads. We claim that two chains are the same if one can be made from second by cyclic rotation (mirror reflection is not allowed there). How many different chains can we get? I want to use Pólya's theorem. So let's deifine $$G = \left\{0,1,2,...,n-1 \right\} = \mathbb Z_n $$ where element $e \in G$ is treated as cyclic rotation with $e$ positions. Now I should write elements and cycles which are produces by them to cyclic index. \begin{array}{|c|c|c|c|}
\hline
elements& cycles\\ \hline
0 & x_1^n  \\ \hline
1 & x_n^1  \\ \hline
2 &   ? \\ \hline
3 & ?  \\ \hline
... &  ... \\ \hline
n-3 &  ? \\ \hline
n-2 &  ? \\ \hline
n-1 & x_n^1  \\ \hline
\end{array} I know what happened for elements $0,1,n-1$ but I completety don't know how to treat other elements due to the fact there is different approach in different combinations of $k$ , $n$ ...","['polya-counting-theory', 'combinatorics', 'necklace-and-bracelets', 'discrete-mathematics']"
3249959,How to calculate the Lie derivative for 2-forms?,"I know how to calculate Lie derivative for one-forms I use this formula: $$\mathcal{L}_X\alpha = \left(X^j\frac{\partial \alpha_i}{\partial \phi^j} + \alpha_j \frac{\partial X^j}{\partial \phi^i}\right)d\phi^i.$$ But how to calculate Lie derivative for 2-forms or 3-forms ? I think we can write for example a 2-form like a wedge product of 1-forms and then use formula: $$\mathcal L_X\omega_{1}\wedge\omega_{2}=(\mathcal L_X\omega_{1})\wedge\omega_{2}+\omega_{1}\wedge(\mathcal L_X\omega_{2})$$ But I tried and failed.
Can someone explain me how to calculate Lie derivative for 2-forms and 3-forms ? I wish to have an explicit formula or an explicit algorithm please.","['lie-derivative', 'differential-forms', 'differential-geometry']"
3249974,How to distribute $n$ red balls between two bins to maximize the chance of sampling only red balls?,"Consider two bins that contain an unknown number of black balls. We wish to split $n$ red balls (i.e., choose a number $x\in\{0,\ldots,n-1\}$ to place in a random bin) so that if we pick one ball at random from each bin we maximize our chances of sampling only red balls. It seems that regardless of the number of black balls in each, we should split the red balls evenly. Mathematically, show that for any positive numbers $b_1,b_2$ (representing the black balls) and $n$ : $$
\frac{x}{x+b_1}\cdot\frac{(n-x)}{(n-x)+b_2} + \frac{x}{x+b_2}\cdot\frac{(n-x)}{(n-x)+b_1} \le \frac{2\cdot (n/2)^2}{(n/2+b_1)\cdot (n/2+b_2)}.
$$ One way to do that is to consider the function $f(x)$ $$
f(x) = \frac{x}{x+b_1}\cdot\frac{(n-x)}{(n-x)+b_2} + \frac{x}{x+b_2}\cdot\frac{(n-x)}{(n-x)+b_1},
$$ derive it, and find its maximum. I'm wondering if there's some averaging argument that we can use to simplify this proof. Clarification:
Our goal is to choose the split $(x,n-x)$ . We cannot determine whether $x$ will be in the bin with $b_1$ balls or the one with $b_2$ balls since we do not know $b_1$ and $b_2$ . Therefore, I assume that $x$ will be placed with $b_1$ with prob. 1/2 and with $b_2$ with prob. 1/2.","['average', 'calculus', 'balls-in-bins', 'probability']"
3249998,Scaling vector with multivariate normal,"If I have a vector $\mathbf{\bar{X}}=\frac{1}{N}\sum_{i=1}^N \mathbf{X}_i$ such that it is known by CLT that $\sqrt{N}(\mathbf{\bar{X}}-\bar{\mu})$ (here $\bar{\mu}$ is the mean vector) converges in distribution to a multivariate normal $N(\mathbf{\bar{0}},\Sigma)$ what are the parameters for the limiting distribution for just $\mathbf{\bar{X}}$ if its possible to use this fact? Im not sure how to adjust for the scaling in the multivariate case",['statistics']
3250004,Calculating limit of a sum,"Helllo everyone,
I have to calculate a particular limit that contais a sum and I have no idea how to solve such problem. The task is to calculate this limit: $$\lim_{n\to \infty}\left(\frac n6\sum_{i=0}^\infty \left(\frac 56\right)^i\left(1-\left(\frac56\right)^i\right)^{n-1}\right) $$ I will be grateful for any hints or solutions.","['limits', 'calculus']"
3250021,"Given a Cauchy sequence $a_n$, show that $\sqrt{a_n}$ is Cauchy when $a_n>0$ for all $n$.","We have a sequence $a_n$ , that is Cauchy and every term is positive. How do I find that $\sqrt{a_n}$ is also Cauchy? I have seen a similar question posted but in that question $a_n>1$ so it is not the same. I understand well how to do it if $a_n>1$ but I can't understand how to alter the solution to account for the cases where $a_n$ and $a_m$ are less than 1. Thank you.","['limits', 'calculus', 'cauchy-sequences']"
3250022,Does there exist an $n \in \mathbb{N}$ such that the improper Riemann integral $\int_0^{\infty}|\sin(x^{n})|$ converges?,I know that $\int_0^{\infty}\sin(x^{\alpha})$ converges for $|\alpha| \geq 1$ which can be attributed to cancellation of areas and $\int_0^{\infty}|\sin(x^{n})|$ diverges for $n = 2$ since there is no cancellation of areas. It seems unlikely but is there a $n$ such that the area of the oscillations can be made so small that it can be approximated by a geometric series and hence show that this integral converges?,"['calculus', 'real-analysis']"
3250043,Sum involving incomplete beta functions,"I am interested in the evaluation (A) , or at least an asymptotic expansion for large $N$ (B) , of the following finite sum \begin{equation}\begin{split}
S^{(p)}(N)&\equiv2N\sum_{k=1}^{\left\lfloor\frac{N-1}{2}\right\rfloor}\left[\left(1-\frac{k}{N}+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-\frac{2k}{N}}{1-\frac{k}{N}+\frac{1}{2N}};N,p+1\right)\right.\\[8pt]
&\left.\quad-\frac{(1+\frac{1}{N})^{N+p}}{2^p}B\left(\frac{1-\frac{2k}{N}}{1+\frac{1}{N}};N,p+1\right)\right],
\end{split}\end{equation} where $N\in\mathbb{N}$ , $p>0$ and $B(x;a,b)$ is the incomplete beta function $$B(x;a,b):=\int_0^xx^{a-1}(1-x)^{b-1}\,\text{d}x.$$ (A) So far I have elaborated the above expression in the following way. Inspired by the fact that both terms in the square brakets are of the form $$x^qB\left(\frac{y}{x};N,p+1\right)$$ I used one of the possible hypergeometric representations of the incomplete beta function, namely $$B(x;a,b)=\frac{x^a(1-x)^{b-1}}{a}{}_2F_1\left(1,1-b;a+1;\frac{x}{x-1}\right),$$ where ${}_2F_1(\alpha,\beta;\gamma;z)$ is the hypergeometric function . Incidentally, this simplified a little bit the starting expression, which became after some simple algebra \begin{equation}\begin{split}
S^{(p)}(N)&=\frac{2^{1-p}}{N^{N+p}}\sum_{k=1}^{\left\lfloor\frac{N-1}{2}\right\rfloor}(N-2k)^N(1+2k)^p\left[{}_2F_1\left(1,-p;N+1;-\frac{2(N-2k)}{1+2k}\right)\right.\\[6pt]
&\left.\quad-{}_2F_1\left(1,-p;N+1;-\frac{N-2k}{1+2k}\right)\right].
\end{split}\end{equation} I wonder whether further simplifications can be performed, especially considering that the two hypergeometric functions differ only by a factor of $2$ in the last argument. I looked for a possible use of this observation, e.g. here , but I did not find anything readily applicable to the problem. (B) Setting $M=\left\lfloor\frac{N-1}{2}\right\rfloor$ for simplicity I approximated the sum with an integral in the large $N$ limit \begin{equation}\begin{split}
S^{(p)}(N)&\sim 2N^2\int_{\frac{1}{N}}^{\frac{M}{N}}\left[\left(1-x+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-2x}{1-x+\frac{1}{2N}};N,p+1\right)\right.\\[6pt]
&\left.\quad-\frac{\left(1+\frac{1}{N}\right)^{N+p}}{2^p}B\left(\frac{1-2x}{1+\frac{1}{N}};N,p+1\right)\right]\text{d}x.
\end{split}\end{equation} At this point I discovered that Mathematica directly evaluates the integral of the second term in the square brakets, which results in (I am still trying to obtain this result analytically) \begin{equation}\begin{split}
&\int_{\frac{1}{N}}^{\frac{M}{N}}B\left(\frac{1-2x}{1+\frac{1}{N}};N,p+1\right)\text{d}x\\[6pt]
&=\frac{1}{2N}\left\{(2M-\frac{Np}{N+p+1})B\left(\frac{N-2M}{N+1};N,p+1\right)\right.\\[6pt]
&\quad\left.-\left(2-\frac{Np}{N+p+1}\right)B\left(\frac{N-2}{N+1};N,p+1\right)\right.\\[6pt]
&\quad\left.+\frac{1}{(N+p+1)(N+1)^{N+p}}\left[3^{p+1}(N-2)^N-(2M+1)^{p+1}(N-2M)^N\right]\right\}.
\end{split}\end{equation} I wonder if something similar can be obtained for the first term, which can be rewritten as \begin{equation}\begin{split}
&\int_{\frac{1}{N}}^{\frac{M}{N}}\left(1-x+\frac{1}{2N}\right)^{N+p}B\left(\frac{1-2x}{1-x+\frac{1}{2N}};N,p+1\right)\text{d}x\\[8pt]
&\quad=-\left(1+\frac{1}{N}\right)^{N+p+1}\int_{\frac{2(N-2)}{2N-1}}^{\frac{2(N-2M)}{2N-2M+1}}(2-y)^{-N-p-2}B(y;N,p+1)\text{d}y,
\end{split}\end{equation} or equivalently as \begin{equation}
=\frac{1}{N}\int_{\frac{1}{N}}^{\frac{M}{N}}\left(1-2x\right)^N\left(x+\frac{1}{2N}\right)^p{}_2F_1\left(1,-p;N+1;-\frac{2x-1}{x+\frac{1}{2N}}\right)\text{d}x.
\end{equation}","['integration', 'special-functions', 'asymptotics', 'beta-function', 'hypergeometric-function']"
3250051,Is Paley-13 a graceful graph?,"The 13-node Paley graph has vertices 1 to 13 that are connected by an edge when their difference is one of the values $(1,3,4,9,10,12)$ . Is Paley-13 a graceful graph ? Can the 13 vertices be labeled with values from 0 to 39 so that the edges have differences 1 to 39? Paley-13 is also a toroidal graph , if that helps. Here's a graceful labeling for the $9_{123}$ circulant graph. Similarly, is the Shrikhande Graph graceful?","['graph-theory', 'recreational-mathematics', 'combinatorics']"
3250067,Proof that there is no way to have certain profit,"Assume there exists the following game/bet. The player gives $x$ amount of money and bets on the result of a football game. If the player correctly bets that the home team wins, he gets back $a*x$ money. If he correctly bets that the game ends as a draw, he gets back $b*x$ money. If he correctly bets that the guest team wins, he gets back $c*x$ money. I have noticed that when $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}<1$ then the player can bet $b*c$ money on the home team, $a*c$ on the draw and $a*b$ on the guest and always have profit no matter the outcome. That is because if one does so he gains $abc$ money no matter the outcome and loses $ab+bc+ac$ money. And it can easily be proved that $abc>ab+bc+ac$ if $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}<1$ . My question is: Why there is $no$ $way$ to bet $x_1$ money on the home team, $x_2$ on draw and $x_3$ on the guest team, so that one always has profit no matter the outcome, when $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}>1$ ? This question came up to me by seeing the betting odds for football games and noticing that $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}$ is always greater than $1$ or else betting companies would go bankrupt.","['calculus', 'functions', 'real-analysis']"
3250069,A fair six-sided die is thrown n times. How to calculate certain probabilities of the sum S of these rolls?,"To name these probabilities: the sum S being less than a threshold N
the sum S being at least the threshold N
the sum S being exactly the threshold N
the sum being in a certain interval $[N_1,N_2] $ e.g. after 100 rolls less than 367, at least 342, exactly 350 and in $ [350,351]$ ? How could I approach that? Addendum: Could I valuably do that with a Monte-Carlo-simulation if the probabilities should only be in a certain range of precision? I wrote a R-script for the Monte-Carlo and checked it with a ""manual"" calculation. We lnow that with independent variables the expected value and the variance are linear, therefor i can calculate that and get the standard deviation $\sigma$ . With that and the PDF of the standard normal distribution, which we know is usable here because of Irwin Hall, i can calculate the percentage. Does that make sense? Is there a more precise approach?","['statistics', 'combinatorics', 'probability']"
3250077,"Show that : $\displaystyle\int_0^{\infty}\frac{\ln(1+x^2)\operatorname{arc\,cot} x}{x}=\frac{π^3}{12}$",I need to prove this : $I=\displaystyle\int_0^{\infty}\frac{\ln(1+x^2)\operatorname{arc\mkern2mucot} x}{x}=\frac{π^3}{12}$ My try : $I\displaystyle\int_0^{1}\frac{\ln(1+x^2)\operatorname{arc\mkern2mucot} x}{x}$ $+\displaystyle\int_1^{\infty}\frac{\ln(1+x^2)\operatorname{arc\mkern2mucot} x}{x}$ $y=\frac{1}{x}$ so $dx=-\frac{dy}{y^2}$ $I=\displaystyle\int_0^{1}\frac{\ln(1+x^2)\operatorname{arc\mkern2mucot} x}{x}$ + $\displaystyle\int_0^{1}\frac{(\ln(1+x^2)-2\ln x)\arctan x}{x}$ Now I need use series of $\arctan x=\sum_{k=0}^{\infty}\frac{(-1)^{k}x^{2k+1}}{2k+1}$ But I don't know how I complete other integration !!,"['integration', 'calculus', 'closed-form']"
3250092,Find standard deviation of x,"I have a sample size of n=175 and mean of μ =6.78 hours.
What is the standard deviation of X̄ do you need that 95% of all samples will have to mean within 5 minutes of μ I used the equation m(margin of error) = (Z*)(σ / √n) so my solution when I plug in the variables and solve is as followed .08=(1.96)(σ /13.229) which gives me 0.58 but the correct answer is 0.041 What am I doing wrong?","['statistics', 'confidence-interval', 'standard-deviation']"
3250107,Why normal vector's formula for implicit function is different than for explicit function?,"Find tangent plane to surface: $z = x^2 + y^2 - 3$ at point $P(2, 1, 2)$ The normal vector to the plane I am looking for is defined as: $$\vec{n} = [\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, -1] \Big|_{P(2,1,2)}$$ I think I understand everything. Problem is when I am dealing with an implicit form of function. Find tangent plane to $4x^2 + 2y^2 + z^2 - 12 = 0$ at point $P(1, -\sqrt{2}, -2)$ So the normal vector is defined as: $$\vec{n} = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}] \Big|_{P(1, -\sqrt{2}, -2)}$$ I am still getting good results. But I simply cannot understand what is going on. The second, implicit function, could be rewritten in explicit form as something like: $$z = \pm \sqrt{-4x^2 - 2y^2 + 12} = f(x, y)$$ So... it is really confusing to me why in the first normal vector's equation there's $-1$ and in the second there's $\frac{\partial f}{\partial z}$ instead. After all I am dealing with function of two variables in both cases, right? And, after I convert the function from implicit to explicit form (and even before conversion), shouldn't I use $\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, \frac{\partial z}{\partial z}$ notation instead of $\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}$ ? To show what I am struggling with to understand: let's say I have function $y = x^2 = f(x)$ . Clearly I can find $y'$ , but I cannot find $x'$ , right? Can someone help me understand this, how is it possible that the first normal vector equation can be that much different than the implicit one? Are those two functions: implicit and explicit, even equivalent or not?","['surfaces', 'multivariable-calculus', 'implicit-differentiation', 'linear-algebra', 'vector-analysis']"
3250121,"If $\frac{2}{x}=2-x,$ Find $[x^9-(x^4+x^2+1)(x^6+x^3+1)]^3$ without entering $\Bbb C$","If $\frac{2}{x}=2-x,$ Find $[x^9-(x^4+x^2+1)(x^6+x^3+1)]^3$ without entering $\Bbb C$ After we solve for $x$ (its a quadratic), and find that $x=1\pm i$ , it's trivial to see the powers of $x$ in the complex plane, but the problem must be solved without using the complex numbers. With complex numbers i found that the result is $1$ .
Is there any way to solve this in $\Bbb R$ without entering $\Bbb C$ ?","['algebra-precalculus', 'quadratics', 'factoring']"
3250127,Are these 2 complex manifolds homeomorphic?,"This is an example give in Kodaira's Complex Manifolds and Deformation of Complex Structures, Chpt 2, Sec 2, Example 2.10 Consider $M=\mathbb P^1\times\mathbb P^1$ with $S=0\times\mathbb P^1$ where $0$ is identified as $0\in\mathbb C\subset\mathbb P^1$ and $\mathbb C$ is complex plane. Denote $W'(\epsilon)=W(\epsilon)=U_{\epsilon}\times \mathbb P^1$ with $U_{\epsilon}=\{z\in\mathbb C:|z|<\epsilon\}$ . Consider the following gluing map to obtain $M_n=M-W(\epsilon)\cup W(\epsilon)$ via the following identification map. $(W(\frac{1}{2}\epsilon)-S)\to (W'(\frac{1}{2}\epsilon)-S)$ is defined by $(z,t)\to (z,t/z^n)$ . (This is essentially replace $S$ by some twisted $S$ .) The book has shown $M$ not homeomorphic to $M_1$ via computing intersection number on the 2nd homology. The result is obviously extendable to $M$ not homeomorphic to $M_n$ . The book only mentions that for $n\neq m$ $M_n$ 's complex structure is different from $M_m$ 's complex structure. Update: I have found in a link that intersection form $H_2(M_n)$ has some basis element with self intersection $n$ . $\textbf{Q:}$ Even from that intersection form, I cannot conclude whether $M_n$ not homeomorphic to $M_m$ for $m\neq n$ . Furthermore, what is the 2 cycle that gives self intersection $n$ for $M_n$ ? How do I tell whether $M_n$ not homeo to $M_m$ for $m\neq n$ ?","['complex-geometry', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
3250189,show that $u(x) = \frac{1}{2\pi} \int_{R^2} log(|y-x|)\Delta u(y)dy$,"Let $u:R^2 \rightarrow R$ be a $C^2$ function with a compact support. I want to show that $u(x) = \frac{1}{2\pi} \int_{R^2} log(|y-x|)\Delta u(y)dy$ , when $x, y \in R^2$ . This look like it should be solved with harmonic functions and with Green's formulas. I declare $v(x,y) = log(|y-x|)$ which is harmonic in $R^2$ \ $\{ x \}$ . Let $G_{R,\epsilon}$ be $B(x,R)$ \ $\bar B(x,\epsilon)$ . From Green's 3rd formula I get: $$ \int_{\partial G_{R,\epsilon}} u<\nabla v, N> - v<\nabla u,N> dS = \int_{\partial G_{R,\epsilon}} u\Delta v - v\Delta u  = -\int_{\partial G_{R,\epsilon}} v\Delta u$$ when $N$ is the outer unit normal. Now: $$ \int_{\partial G_{R,\epsilon}} u<\nabla v, N> - v<\nabla u,N> dS =  \int_{|y-x| = R} (u<\nabla v, N> - v<\nabla u,N>)dS -  \int_{|y-x| = \epsilon} (u<\nabla v, N> - v<\nabla u,N>)dS$$ I need to somehow evaluate those integrals so that when I take $R \rightarrow \infty$ and $\epsilon \rightarrow 0$ I will get $-2\pi u(x) $ . However, the only integral I managed to evaluate is $\int_{|y-x| = \epsilon} v<\nabla u,N>)$ which I showed that goes to $0$ . But with the others I didn't have much luck. Because I got stuck with $log(R)$ who doesn't go to zero and because $<\nabla, N>$ ends being not so nice of an expression. I fit is needed: $\nabla v = (\frac{y_1-x_1}{(y_1-x_1)^2 + (y_2-x_2)^2}, \frac{y_2-x_2}{(y_1-x_1)^2 + (y_2-x_2)^2}$ and the outer Normals are $\frac{y}{R}$ and $\frac{y}{\epsilon}$ Help would be appreciated.","['multivariable-calculus', 'harmonic-functions', 'vector-analysis']"
3250215,On the mapping cylinder inclusion $i : X \hookrightarrow M_f$ being a cofibration.,"I'm trying to prove that given a continuous map $f : X \to Y$ , the inclusion $$
i : x \in X \mapsto [(x,1)] \in M_f
$$ is a cofibration. Given a homotopy $H : X \times I \to W$ and $g : M_f \to W$ so that $$
H(x,0) = gi(x) = g([(x,1)])
$$ we want a homotopy $J : M_f \times I \to W$ such that for all $x \in X$ we have $$
J([(x,1)],t) = H(x,t) \text{ and } J([(x,0)],t) = g(f(x)).
$$ Since $g$ is a function with domain $M_f$ , which is a pushout, we have continuous functions $g_X : X \times I \to W$ and $g_Y : Y \to W$ such that $$
g_Y(y) = g([y]), \quad g_X(x,t) = g([(x,t)]),
$$ and $g_X(x,0) = g_Y(f(x))$ . Now, since $I$ is locally compact Hausdorff, by the exponential law the endofunctor $$- \times I : \mathsf{Top} \to \mathsf{Top}$$ is left adjoint to $\mathsf{Top}(I,-)  :\mathsf{Top} \to \mathsf{Top}$ . Hence the former preserves colimits and so $M_f \times I$ is the pushout of $X \times I \xrightarrow{i_0 \times 1} (X \times I) \times I$ and $X \times I \xrightarrow{f \times 1} Y \times I$ . Thus, the map $J$ corresponds to a certain selection of maps $\tilde{H} : (X \times I) \times I \to W$ and $\tilde{g} : Y \times I \to W$ so that $$\tilde{g}(f(x),t) = \tilde{H}(x,0,t).$$ My intuition here is to leave the 'initial data' $g$ fixed on $Y$ , and at each instant $s$ to have a function from $X \times I$ that behaves like $H_0$ in the bottom face and coinides with $H_s$ at the top face. This would make $J$ an extension of the original homotopy. Hence I wanted to define $$
\tilde{H}(x,s,t) = H(x,ts) \text{ and } \tilde{g}(y,t) =g_Y(y).
$$ However, for these to induce a map from $M_f$ we should have that $g([(x,0)]) = g([(x,1)])$ for all $x \in X$ . But moreover, for the induced map to preserve the initial data, one should have $g = Ji_0$ and $H = J(i \times 1_I)$ which amounts to proving $$
g([(x,t)]) = g([(x,0)]) \quad (\forall t \in I).
$$ It is not evident to me that this is the case. What's missing here? Are my choices for $\tilde{H}$ and $\tilde{g}$ incorrect? Any help is much appreciated.","['cofibrations', 'category-theory', 'homotopy-theory', 'general-topology', 'algebraic-topology']"
3250229,Normalization of Projective Scheme is Projective,"Let $X$ be a scheme (for sake of simplicity say proper $k$ -scheme) which is projective , therefore there exist a closed immersion $i_X: X \to \mathbb{P}^n_k$ . Let $f: N \to X$ be the normalization morphism of $X$ . Why and how to derive that $Y$ is then also projective? We have to construct a closed immersion $i_N \to \mathbb{P}^m$ ? One way might be a technical overkill: Thereis a criterion that a proper $k$ -scheme $X$ is projective iff it containes an ample sheaf $L$ . So since $f$ is finite we can pullback $L$ to $f^*L$ which stays ample, tada! But I'm looking for a more intuitive /constructive argument in order to understand the geometry behind this fact. Is there a way to construct such a closed embedding $i_N \to \mathbb{P}^m$ explicitely using the local properties (so on ring level) of the normalization map. Are the numbers $n$ and $m$ related?","['algebraic-geometry', 'schemes']"
3250233,Strategies for proving that a critical point is neither maximum nor minimum,"Let $f(x,y) = x^3 + y^3 -3x$ . Since it's everywhere differentiable, we know that its only critical points are $(1,0),(-1,0)$ given that the gradient $\nabla f(x,y) = (3x^2-3,3y^2)$ is zero if and only $x=\pm1$ and $y=0$ . However, $\det Hf|_{(\pm1,0)} = 0$ , so we can't apply the Second partial derivative test. Since $f$ is odd, we might suspect these are saddle points. One way to proceed is to find curves: $$f \circ \alpha (t) = f\circ (t^2+1, (3^{\frac{1}{3}}t^{\frac{2}{3}})) = (t^2+1)^3 -3$$ $$f \circ \beta (t) = f\circ (-t^2+1, (-3^{\frac{1}{3}}t^{\frac{2}{3}})) = (-t^2+1)^3 -3$$ And it's clear that $f \circ \alpha (t)$ has a minimum at $t=0$ and that $f \circ \beta (t)$ has a maximum at $t=0$ , which implies $(1,0)$ is a saddle point of $f$ . But this method depends on one's ingeniousness and it's very time-consuming. I've lost half an hour trying to find those curves! Are there any other strategies to tackle these kinds of problems?","['maxima-minima', 'multivariable-calculus', 'hessian-matrix']"
3250237,Finding Probability that Maximum Value of $N + 1$ Random Variables is Larger than a Given Limit,"For positive random variables $X_{0}, X_{1}, X_{2}, \ldots, X_{N}$ , where $N$ is also a random variable, we know that $X_{1}, X_{2}, \ldots, X_{N - 1}$ are I.I.D. with continuous PDF $f(x)$ and $X_{0} + X_{1} + X_{2} + \ldots + X_{N} = 1$ . What is $P(\max_{i = 0} ^ {N} X_{i} \le d)$ , for a given $d$ ? If an exact solution is not possible, can one find bounds for this probability? Can this problem be solved if $f(x)$ is some well-known distribution (say exponential, log-normal, etc.)? EDIT. An equivalent problem is this: a number of points lie in the interval $[0, 1]$ . If we know that the length of segments between each two consecutive points are I.I.D random variables with PDF $f(x)$ (this does NOT include the distance between $0$ and the first point, or $1$ and the last point), what is the probability that there is no opening larger than $d$ in this interval? $d$ is a given and fixed parameter. EDIT2. A related problem may be this: for a 1-D point process spanning $\mathbb{R}$ where interarrival times are I.I.D. random variables with PDF $f(x)$ , what is the probability of observing an opening bigger that a given value $d$ in the interval $[0, 1]$ ? The spaces between $0$ and the first point falling in the interval and $1$ and the last point falling in the interval count as well. For example, if for some realization there is only one point in the interval $[0, 1]$ at $\frac{1}{2}$ , there are two openings in this interval and both have a length of $0.5$ . If for some other realization there are no points in the interval $[0, 1]$ , there is only one opening in this interval and it has a length of $1$ . Hope this edit clears up the confusion and does not add to it.","['probability-distributions', 'probability-theory', 'probability']"
3250241,Why are Sobolev spaces useful?,"Why are Sobolev spaces useful, and what problems were they developed to overcome? I'm particularly interested in their relation to PDEs, as they are often described as the 'natural space in which to look for PDE solutions' - why is this? How do weak solutions and distributions come into this? There are plenty of books on the subject, but these seem to jump straight into the details and I'm struggling to see the big picture. I know that the Sobolev norm makes the function spaces complete, which would guarantee that infinite linear combinations of solutions do not leave the space, as can be a problem when working with $\mathscr{C}^2$ , for example, but are there any other reasons why this norm is important? I'm also interested in the Sobolev embedding theorems, since I believe that they're important in the problems I'm trying to solve. These are (1) proving the compactness of the integral operator whose kernel is the Green's function for the Laplacian on a bounded domain $\Omega \subset \mathbb{R}^{n}$ with smooth boundary, and (2) understanding why minimising functions of the Rayleigh quotient, $${\arg\min}_{f \in T} \int_{\Omega} \frac{\nabla f \cdot \nabla f}{\left< f , f \right>}$$ always exist, and that they are necessarily smooth ( $\mathscr{C}^\infty(\Omega)$ ) among the set of trial functions $T$ of continuous functions with piecewise continuous first derivatives which vanish at the boundary and are not identically zero. To me, this sounds like the Sobolev space $H_0^1 (\Omega)$ at work, where the smoothness is the result of a Sobolev embedding theorem; however, I'm very new to Sobolev spaces and so don't know much about this. Could anyone provide me with some insight into how results (1) and (2) might be proven?","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
3250256,"Complement, open and closed sets","Definition: Let X be a metric space, and E $\subseteq X$ , E is closed if it is equal to its closure. Definition: A metric subset U of X is open if for every point in U there exists an open ball centered around the point that is contained within U. Theorem: Let X be a metric space. A Subset $Y \subseteq X$ is closed iff $Y^c$ is open. Proof: Assume Y is closed so $Y= \bar{Y}$ (so it is equal to is closure). Which means that $y\in Y \iff \forall$ $r>0 B(y,r) \cap Y \neq \emptyset$ . Let $a \in Y^c$ be arbitrary. Since Y is closed $\exists r>0 : B(a,r) \cap Y = \emptyset$ . This means that $B(a,r) \subseteq Y^c$ . Since a was arbitrary, $Y^c$ is open. Suppose $Y^c$ is open. So $y\in Y^c$ $\iff$ $\exists r>0$ : $B(y,r) \subseteq Y^c$ . Let x $\in \bar{Y}$ be arbitrary. Note that it suffices to show that $\bar{Y} \subseteq Y$ . So for any possible $r>0$ we have $B(x,r) \cap Y \neq \emptyset$ . Since $Y^c$ is open, it follows that $x \in Y$ . Is the proof correct? I would very much love feedback, I'd be very very thankful.","['elementary-set-theory', 'general-topology', 'proof-verification', 'metric-spaces']"
3250258,Proof by induction for nth derivative,"Show the following hold by induction: $$\frac {d^n}{dx^n}\frac {e^x - 1}{x} = (-1)^n \frac{n!}{x^{n+1}} \left( e^x \left(\sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!}\right) - 1 \right)$$ Proof. It's not hard to show the base case hold. For inductive step, we can also write this as: $$\frac {d^n}{dx^n}\frac {e^x - 1}{x} = (-1)^n \frac{n!}{x^{n+1}} e^x \sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!} - (-1)^n \frac{n!}{x^{n+1}} $$ Take derivative on both side: $$\frac {d^{n+1}}{dx^{n+1}}\frac {e^x - 1}{x} = \frac {d}{dx}(-1)^n \frac{n!}{x^{n+1}} e^x \sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!} - \frac {d}{dx}(-1)^n \frac{n!}{x^{n+1}} $$ $$\frac {d^{n+1}}{dx^{n+1}}\frac {e^x - 1}{x} = \underbrace{\frac d{dx}(-1)^n\frac{n!}{x^{n+1}}e^x\sum_{k=0}^n(-1)^k\frac{x^k}{k!}}_? - \underbrace{(-1)^{n+1}\frac{(n+1)!}{x^{n+2}}}_{hold} $$ Therefore, my question is for the first part, how do I show the following hold: $$ \frac d{dx}(-1)^n\frac{n!}{x^{n+1}}e^x\sum_{k=0}^n(-1)^k\frac{x^k}{k!} 
= (-1)^{n+1}\frac{(n+1)!}{x^{n+2}} e^x \sum_{k=0}^{n+1} (-1)^{k} \frac{x^k}{k!}$$","['summation', 'derivatives', 'induction', 'factorial']"
3250276,A generalization of coupon collector's problem: take an adjacent pair each time.,"We have $n$ distinct kinds of coupons (of unlimited supply), the categories are denoted $1\dots n$ .Each time we could draw two coupons, but the two must be of adjacent kind (it could be any of $\{(1,2),(2,3)...,(n-1,n),(n,1)\}$ , the probability of drawing any pair is equal). What is the expected number of draws before collecting all $n$ distinct coupons? Some progress: This is different than just drawing two coupons with replacement randomly, where I already know a formula. And the ""pattern"" of selecting does not matter, this means drawing $\{(1,3),(2,4)...,(n-1,1),(n,2)\}$ yields the same result.","['expected-value', 'coupon-collector', 'probability']"
3250291,"For $m,n\in \omega, m \leq n$ imply $\exists ! p\in \omega\ s.t\ m+p=n$","For a set $A$ ,  we define $A^+:=A\cup\{A\}$ When we define, $$0=\emptyset,\ 1=0^+,\ 2=1^+,\ \cdots$$ set of natural number $\omega$ is defined as $$\omega=\{0,1,2,\cdots\}$$ The order $""\leq""$ is defined as $$a\leq b \ iif \ a\in b \ or \ a=b$$ When $\gamma_m:\omega\rightarrow\omega$ defined by following condition: $\gamma_m (0)=m$ 2. $\gamma_m (n^+) =(\gamma_m (n))^+$ Addition is defined as $$m+n=\gamma_m (n)$$ In this setting how can we prove: $m,n\in \omega, m \leq n$ imply $\exists ! p\in \omega\ s.t\ m+p=n$","['elementary-set-theory', 'natural-numbers']"
3250296,Advanced algebra-precalculus,"Find the value of $\frac{w^{2}}{x+y+z}+\frac{x^{2}}{w+y+z}+\frac{y^{2}}{w+x+z}+\frac{z^{2}}{w+x+y}$ when $\frac{w}{x+y+z}+\frac{x}{w+y+z}+\frac{y}{w+x+z}+\frac{z}{w+x+y}$ = 1, where $w,x,y,z \in \mathbb R $ . I have tried setting one variable equal to zero, two variables equal to zero, and many other combinations to no avail of mine. I am training for a math olympiad, and this question has been boggling my head. A solution to this would be appreciated, but not as much as resources I can use to find a definitive answer to this problem.","['contest-math', 'real-numbers', 'algebra-precalculus']"
3250304,How to convert delay differential equations to a discrete-time agent-based model?,"I am building an agent-based model in discrete time. To do this, I need to convert a continuous time delay and ordinary differential equations to discrete-time equations. Here is the equation system: $$
	\frac{\mathrm{d} S_\text{L}^\text{Q}}{\mathrm{d}t}
=
	\underbrace{B_\text{L}(t)}_\text{birth}
-	\underbrace{d_\text{L} S_\text{L}^\text{Q}}_\text{mortality}
-	\underbrace{a_\text{L} N_\text{H} S_\text{L}^\text{Q}}_\text{encounter success}
,\\
	\frac{\mathrm{d} S_\text{L}^\text{A}}{\mathrm{d}t}
=
	\underbrace{a_\text{L} N_\text{H} S_\text{L}^\text{Q}}_\text{encounter success}
-	\underbrace{d_\text{L} S_\text{L}^\text{A}}_\text{mortality}
-	\underbrace{a_\text{L} N_\text{H} e^{-d_\text{L} τ} S_\text{L}^\text{Q} (t-τ)}_\text{feeding success}
, \\
	\frac{\mathrm{d} S_\text{L}^\text{F}}{\mathrm{d}t}
=
	\underbrace{a_\text{L} N_\text{H} e^{-d_\text{L} τ} S_\text{L}^\text{Q}(t-τ)}_\text{feeding success}
-	\underbrace{d_\text{N} S_\text{L}^\text{F}}_\text{mortality}
$$ How can I rewrite these continuous-time equations to discrete-time equations? What discretization method should be used? In the agent-based model, each time step represents one day. I am beginner in ODEs and DDEs.","['biology', 'delay-differential-equations', 'ordinary-differential-equations']"
3250314,Lower bound for $\sum_{v \in V(G)} d(v)\bar{d}(v)$,"Let $G=(V,E)$ be an $n$ -vertex graph with $e$ edges. For all $v\in V$ , $d(v)$ is the degree of $v$ , and $\bar{d}(v)$ is the ``non-degree"" of $v$ , that is, $\bar{d}(v)=(n-1)-d(v)$ (the degree of $v$ in the complement of $G$ ). Define $k:=\left\lceil\sqrt{2e+1/4}+1/2\right\rceil$ . I want to show that \begin{align*}
		\sum_{v\in V}d(v)\bar{d}(v) 	& \geq 	2(n-2)e - \frac{1}{4}\left(k^4-6k^3-(4e-11)k^2+(20e-6)k+4e(e-7)\right) \\
								& = 		2(n-2)e - \left(\binom{k}{2}-e\right)^2 + (4k-7)\left(\binom{k}{2}-e\right) - 6\binom{k}{3} 
	\end{align*} I will explain where this lower bound comes from. I believe that the graph, call it $G^*$ , which achieves this lower bound is the following construction: Take a single vertex from a $k$ -clique (a complete graph with $k$ vertices) and remove exactly $r=\binom{k}{2}-e$ edges incident to it. Now add $n-k$ isolated vertices. In that case, there is one vertex of degree $k-r-1$ , there are $r$ vertices of degree $k-2$ , there are $k-r-1$ vertices of degree $k-1$ , and $n-k$ vertices of degree $0$ . Hence, for this construction, \begin{align*}
		\sum_{v\in V}d(v)\bar{d}(v) 	& = 	(k-r-1)\left(n-1-(k-r-1)\right) + r(k-2)\left(n-1-(k-2)\right) + (k-r-1)(k-1)\left(n-1-(k-1)\right) \\
								& = 	(k-r-1)(n-k+r) + r(k-2)(n-k+1) + (k-r-1)(k-1)(n-k) \\
								& =  	n(k^2-k-2r) - (k-r)(k-r-1) - r(k-1)(k-2) - k(k-1)(k-r-1) \\
								& = 	2en - 4e + 4e - r^2 +r(4k-3) - k^3 + k^2 \\ 
								& = 	2(n-2)e - \left(\binom{k}{2}-e\right)^2 + (4k-7)\left(\binom{k}{2}-e\right) + 4\binom{k}{2} - k^3 + k^2 \\
								& = 	2(n-2)e - \left(\binom{k}{2}-e\right)^2 + (4k-7)\left(\binom{k}{2}-e\right) - 6\binom{k}{3}
	\end{align*} Could someone help me show that $G^*$ is indeed the ""best"" construction, with ""best"" meaning that $G^*$ is the unique graph which minimizes $\sum_{v \in V} d(v) \bar{d}(v)$ among all $n$ -vertex graphs with $e$ edges. Thank you for your help!!","['graph-theory', 'combinatorics', 'extremal-combinatorics', 'discrete-mathematics']"
3250374,Do isometries of regular convex polytopes generate everything?,"Let $D$ be a regular convex $d$ -dimensional polytope that is symmetric $(D=-D$ ) in $\mathbb R^d$ centred at the origin. Let $G$ be the group of all isometries of $D$ , that is, linear maps/matrices $T\in M_d$ such that $T[D]=D$ . Does the linear span of $G$ in $M_d$ generate everything, that is, ${\rm span}\, G = M_d$ , the space of all $d\times d$ -matrices? It is the case for $d$ -dimensional cubes and octahedrons but is it true in general?","['polyhedra', 'polytopes', 'linear-groups', 'matrices', 'linear-algebra']"
3250410,What's the (limit of the) n-th order derivative of this integral?,"I met a question that asks me to calculate the result of an integral first: $$f(s)=\int_0^1e^{-sx}dx,s\geq0\tag1$$ It is easy, if I am not wrong, the answer should be $1$ when $s=0$ , and $\frac{1-e^{-s}}s$ for $s>0$ . Then it asks me to calculate the limit: $$\lim_{s\rightarrow0}\frac{df(s)}{ds}\tag2$$ It is also easy, the derivative is: $$\lim_{s\rightarrow0}\frac{df(s)}{ds}=\lim_{s\rightarrow0}\frac{e^{-s}s-(1-e^{-s})}{s^2}\tag3$$ because it is a $\frac00$ form limit, by using the L'Hôpital law, we can quickly calculate that the limit is $-\frac12$ . However, I got stuck in the final question, it asks me to calculate $$\lim_{s\rightarrow0}\frac{d^nf(s)}{ds^n}\tag4$$ I tried to calculate the n-th derivative directly first, hoping to find some laws. But as all you can see from (3), it will be only more and more complex as I differentiate (3). I also tried to use the Leibniz formula, like let $u=1-e^{-s}$ and $v=s^{-1}$ , so $f(s)=(1-e^{-s})s^{-1}=uv$ , but I still could not get any idea after just writing a long formula. So how can I calculate (4)? Could you give me some hints? Thank you very much!","['calculus', 'derivatives']"
3250430,Does there exist some sort of classification of incompressible groups?,"It is well known, that any finite group of order $n$ is isomorphic to a subgroup of $S_n$ . Let’s call a finite group $G$ incompressible iff it is not isomorphic to any subgroup of $S_{|G|-1}$ . Does there exist some sort of classification of incompressible groups? What I currently know: Any  non-trivial incompressible group has non-trivial center If the center of a group $G$ is trivial, then it acts faithfully by conjugation on $G \setminus \{e\}$ . If an incompressible group is non-trivially decomposed into a direct product of two its subgroups, it is isomorphic to $C_2 \times C_2$ One can construct a faithful action of $H \times K$ on $H \cup K$ . It is defined as $(h, k)h_0 \mapsto hh_0$ and $(h, k)h_0 \mapsto kk_0$ for $h, h_0 \in H$ , $k, k_0 \in K$ . $|H| + |K| \geq |H||K|$ iff either one of the groups is trivial, or both of them are isomorphic to $C_2$ . $C_2 \times C_2$ is the only possible group and indeed is not contained in $S_3$ . I also conjecture, that «direct product» in this statement can be replaced with «semidirect product», but do not know how to prove that. All cyclic $p$ -groups are incompressible If $p$ is prime, then $S_{p^n - 1}$ does not have an element of order $p^n$ $Q_8$ is incompressible $S_7$ does not contain $Q_8$ as a subgroup","['finite-groups', 'abstract-algebra', 'symmetric-groups', 'group-theory', 'group-actions']"
3250438,Derivation of Gumbel Distribution,The standard generalised extreme value (GEV) distribution is given by $H_{\xi}$ which is $exp(-(1+\xi x)^{-1/\xi}$ if $\xi<>0$ and $exp(-e^{-x})$ if $\xi=0$ In the lecture notes it is stated $1-H_\xi (x)$ approximatle equals $e^{-x}$ for $\xi=0$ for $x_{H_\xi}$ going to infinity which is the Gumbel distribution. $1-H_\xi (x)$ approximatle equals $(\xi x)^{-1/\xi}$ for $\xi=0$ for $x_{H_\xi}$ going to infinity which is the Fréchet distribution. I would like to do the math to derive the Gumbel and the Fréchet from the GEV to understand deriving limits better (I seem to have some deficits). I would be grateful for a solution or a textbook hint. Many thanks.,"['limits', 'extreme-value-theorem', 'probability', 'extreme-value-analysis']"
3250447,Finding the dimensions of scaled down triangles,"I am given the following equilateral triangle: The lengths of GH & GI both equal 1 and since we know that its an equilateral triangle, we know that the angles are 60 degrees for all 3 corners.  Because these are the only givens, I assumed that GP = 0.2 and PI = 0.8.  These are feasible values and so I was able to work out the values for the out-most green triangles (excuse the quality).  Its area turned out to be (0.08)sin60 : Right now, I am being asked to find the area of the shaded region: Which means that I need to find the area of the 5 triangles in the shaded region and add them up.  I know the area, angles and dimensions for the first green triangle and I know the areas for the remaining 4 green & white triangles but I don't know how to make sense of the sides","['trigonometry', 'geometry']"
3250480,Integral of $M^TM$ for matrix $M=B\exp(-At)$,"For non-commuting positive definite matrices $A$ , $B$ , is there a simple expression for $$\int_{0}^{\infty} M_t^T M_t \,\mathrm d t$$ where $M_t:=B\exp(-At)$ ? Based on the commutative case, I'd hope it's something like $\frac{1}{2}A^{-1/2}B^2A^{-1/2}$ but I cannot prove it.","['improper-integrals', 'matrix-exponential', 'matrices', 'matrix-calculus', 'matrix-equations']"
3250542,Finding a symmetric adjacency matrix closest to a given (non-symmetric) adjacency matrix,"I am trying to solve a problem on graphs, which I have reduced to the following optimization problem in matrix $X \in \{0,1\}^{n \times n}$ $$\begin{array}{ll} \text{minimize} & \| X - A \|_F^2\\ \text{subject to} & X 1_n = m 1_n\\ & X=X^\top\end{array}$$ where matrix $A \in \{0,1\}^{n \times n}$ is given. Matrix $X$ is the adjacency matrix of a non-directed $m$ -regular graph, while matrix $A$ is the adjacency matrix of a directed graph. I am quite clueless on how to go on solving this problem and would be happy to get a direction.","['graph-theory', 'adjacency-matrix', 'binary-programming', 'matrices', 'discrete-optimization']"
3250544,Prove that at least $15$ students come from the same country.,"$60$ students had been chosen as participants in a conference by means of telecommunications. Parents had noticed that for every $10$ students, there were at least $3$ students who come from the same country. Prove that at least $15$ students come from the same country. I have only studied combinatorics for quite a while so it is still an elementary subject to me. Could anyone help me with this problem please?",['combinatorics']
3250580,Gradient of scalar field $a^T X^{-1} b$,"During the derivation of GDA as generative algorithm, I am stuck at how to take the gradient $$\nabla_X \left( a^TX^{-1}b \right)$$ where $a, b$ are column vectors independent of $X$ . I have tried using trace operator and chain rule, but could not crack it. How should this derivative be approached? The answer is $$-X^{-T}ab^TX^{-T}$$","['scalar-fields', 'matrices', 'matrix-calculus', 'inverse', 'derivatives']"
3250599,Intuitive explanation of CDF of a Binomial distribution in the volume of a Hyperspherical Cap,"Note: This is my first question ever in stackexchange, I apologize for any mistakes in formatting, on the appropriateness of the question and tags. From Wikipedia, I know the regularized incomplete beta function is related to the CDF of a random variable $X$ from a Binomial distribution: $$\mathcal{F} \left(k; n, p\right) = 1 - I_{p} \left( k+1, n-k \right). $$ ( https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function ) Also from Wikipedia, the expression of a hyperspherical cap in $D$ dimensions is given by $$ V_{D\text{-cap}} = \frac{1}{2}V_{D\text{-ball}} \, I_{(2Rh-h^2)/R^2} \left(\frac{D+1}{2}, \frac{1}{2} \right),$$ where $V_{D\text{-ball}}$ is the the volume of the the $D$ -ball with radius $R$ and $h$ is its height.
( https://en.wikipedia.org/wiki/Spherical_cap ) I re-expressed the previous expression to incorporate heights larger than $R$ and considered the cap is cut at the hyperplane $x=0$ and the ball is centered at $x=x_0$ ( See image ).
Then, $R = h-x_0$ , and $$ \frac{V_{D\text{-cap}} (x_0)}{V_{D\text{-ball}}} = -\frac{1}{2} \text{sgn}\left(\frac{x_0}{R} \right) I_{1-\left( \frac{x_0}{R} \right)^2} \left( \frac{D+1}{2}, \frac{1}{2} \right) + \Theta \left(\frac{x_0}{R}\right),$$ where $\text{sgn}(x)$ is the sign operator, $I_x \left(a,b \right)$ the regularized incomplete beta function and $\Theta(x)$ the Heaviside function. Considering the property from point 1 and $\Theta (x) = \frac{1}{2} + \frac{1}{2} \text{sgn} (x)$ , this can be re-expressed as: $$\frac{V_{D\text{-cap}} (x_0)}{V_{D\text{-ball}}} =\frac{1}{2} + \frac{1}{2} \text{sgn}\left(\frac{x_0}{R}\right) \mathcal{F} \left( \frac{D-1}{2}; \frac{D}{2}, 1-\left( \frac{x_0}{R} \right)^2 \right).$$ So here comes my question: What is the meaning of the CDF of the Binomial distribution in this context? That is, which random variable is associated to this problem that has probability $1- \left( \frac{x_0}{R} \right)^2$ of having $\frac{D-1}{2}$ successes out of $\frac{D}{2}$ trials? EDIT: After some plots I see the expression from step 1 is never true for $k=\frac{D-1}{2}$ and $n=\frac{D}{2}$ . For odd $D$ the Binomial CDF appears to not be defined, I guess a fractional number of experiments $n$ is not well-defined; and for even $D$ the two sides give different values, I imagine a fractional number of successes $k$ is neither well-defined. So as pointed by @fedja this interpretation may be ill-posed.","['probability-distributions', 'geometry', 'reference-request']"
3250620,calculating the arc length of a function,"The function is: $$ f(x) =2\ln(4-x^2)$$ and the length of the arc is from $-1$ to $1$ . So i know the formula for calculating the arc length is $$\int_{a}^{b}\sqrt{1+(f'(x))^2} dx$$ the derivative of the function $f(x)$ is, if i am correct $$2\frac{d}{dg}\ln(g)\frac{d}{dx}(4-x^2)$$ with $g = (4-x^2)$ . So $$ f'(x)= -\cfrac{4x}{4-x^2}$$ and $$(f'(x))^2=\cfrac{16x^2}{(4-x^2)^2}$$ and the formula becomes: $$\int_{-1}^{1}\sqrt{1+\cfrac{16x^2}{(4-x^2)^2}}dx$$ the problem is when i use the $u$ substitution of $$u=1+\cfrac{16x^2}{(4-x^2)^2}$$ And i want to make new boundaries for the integration $i$ have $2$ the same numbers (both are 2,77). 
I don't know what i have done wrong but the answer must be $$4\ln(3)-2$$","['integration', 'substitution', 'arc-length', 'definite-integrals']"
3250713,Example of non regular surface,"I was reading definition of surface in differential geometry book 
 which defined as follows A subset $S\subset \mathbb  R^3$ is regular surface if $\forall p\in S $ there is open set in S such that $p\in V $ and $\exists \phi :U\to V\in S$ where U is open set in $\mathbb R^2 $ such that map is surjective , smooth and homeomorphism with image and also $\forall q\in U, dX_q:\mathbb R^2\to \mathbb R^3$ is injective TO better understand cocept I wanted to know counterexample of regular surface. Please Help me Any Help will be appreciated","['differential-geometry', 'surfaces', 'examples-counterexamples', 'real-analysis']"
3250718,Mathematics behind Neural Networks,"I wasn't sure to ask this here, but based on the related questions, I think this is appropriate. For starters, I'm trying to build a Neural Network using the website below as a reference. It seems like it's just trying to implement Perceptron with a single hidden layer. https://causeyourestuck.io/2017/06/12/neural-network-scratch-theory/ Following the guide, it makes sense, but I'm having trouble understanding the mathematics behind the back propagation part. I understand that you have to use the error function to derive the rate of change for the biases and weights, but I'm confused as to how the derivatives (w.r.t. the parameters) ends up being a 'scalar' multiplication. The derivatives have to be the same size as the parameters, but how do you get to that point (using ${\partial J\over\partial B_2}$ as the example here)? Also, when deriving ${\partial J\over\partial W_2}$ , using the chain rule, how does the derivative of ${\partial Y\over\partial W_2}$ result in the transpose of $H,$ and why does it end up being a dot product with the derivative of the error function (w.r.t the result)? Sorry, my background in Matrix Algebra is not incredibly strong, so having the mathematics explained to me would really help a lot.","['transpose', 'matrix-calculus', 'derivatives', 'neural-networks']"
3250753,Integral $\small \int_0^\infty \frac{(\pi x - 2\log{x})^3}{\left(\log^2{x} + \frac{\pi^2}{4}\right)(1+x^2)^2} \text{d}x$,"Prove $$\int_0^\infty \frac{(\pi x - 2\log{x})^3}{\left(\log^2{x} + \frac{\pi^2}{4}\right)(1+x^2)^2} \text{d}x = 8\pi$$ I tried using a modified version of the integrand and an origin-indented semicircular contour on the positive real half of the complex plane. Despite my best efforts, I am unable to retrieve the desired integral, as the negative factor from the negative imaginary axis is preventing me from being able to simplify the integrals. On top of that, the residue doesn't come anywhere close to the exact result. Are there better methods that I am missing?","['integration', 'improper-integrals', 'complex-analysis', 'contour-integration', 'complex-integration']"
3250764,Derivation of Chern-Simons invariant,"I am currently reading the original paper by Chern and Simons where they introduce their form. I am working out the examples, but I do not seem to be able to derive their formula. We should be able to derive: $$TP_1 (\theta) = \frac{1}{4 \pi ^2} (\theta_{12} \wedge \theta_{13}\wedge \theta_{23}+{\theta}_{12}\wedge\Omega_{12}+\theta_{13}\wedge\Omega_{13}+\theta_{23}\wedge\Omega_{23}) $$ The 3 Chern-Simons form for a 3-manifold.
We should only need the following equation applied to the case $l=2$ (since 3=2(2)-1): $$TP(\theta) = \sum_{i=0}^{l-1} A_i P(\theta \wedge [\theta,\theta]^i\wedge\Omega^{l-i-1}) $$ $$ A_i = (-1)^i l! \frac{(l-1)!}{2^i (l+i)!} (l-1-i)!$$ My work:
I managed to reduce the formula to: $$ TP_1(\theta)=P(\theta \wedge \Omega) - \frac{1}{6} P(\theta \wedge [\theta,\theta])$$ How do I express this last part in the local frame to get the desired formula? 
Also, I will be interested in calculating the higher degree forms. How do I work out the power of bracket and so?","['characteristic-classes', 'differential-geometry']"
3250799,"Calculation of a double integral $\int_{-1}^{1}\int_0^2 \sqrt{\left|y-x^2\right|} \,dx \,dy$","I want to calculate the following integral: $$
\int_{-1}^{1}\int_0^2 \sqrt{\left|y-x^2\right|} \,dx \,dy.
$$ I tried to go first with $y$ which seems the easier of the two, but then the integral with respect to $x$ becomes quite complex. On the other hand, starting with $x$ it is cumbersome and further the integral with respect to $y$ is more complex than the first method... Is there a simple way  to do it??",['multivariable-calculus']
3250839,Translate these English statements into Predicate Logic,"Given: P(x) = ""x is a clear explanation"" Q(x) = ""x is satisfactory"" R(x) = ""x is an excuse I need to translate a) Some clear explanations are satisfactory. b) No excuses are clear explanations. (All excuses are not clear explanations) I have a lot of difficult distinguishing when to use a conjunction and when to use an implication. Here are the translations I came up with: a) ∃x(P(x)∧Q(x)) b) ∀x(R(x) --> ¬P(x)) Are these correct? Regardless of my correctness, can you provide an explanation why I was right/wrong in using the implication over the conjunction and vice versa. I would like to get their correct uses straight in my head. Thanks",['discrete-mathematics']
3250841,"normal test to student test, logic behind it","Let's say we have : $$ X_i  \sim \ N( \mu, \sigma^2 ) $$ iid I'm constructing this test function, in order to test two hypothesis on $\mu$ : $$ \mathbb { 1} {\{ \sum^n X_i < q_a \} } $$ where $q_a$ is the $a$ -quantile of $$ P_{ \mu_0 } ( \sum^n X_i < q_a  ) $$ If I know $\sigma$ , this test is just fine. Because then I can rewrite the test function as : $$ \mathbb { 1} \{ \frac{  \sum X_i - n \mu_0 }{ \sigma \sqrt{n} } < \phi^{-1} (a) \}  $$ But if I don't know $\sigma$ , how can I handle things ? I have been told to replace it by $S$ , such that : $$ S^2 = \frac 1 {n-1} \sum^n (X_i - \overline X)^2 $$ But how do you conclude that the test is now : $$ \mathbb { 1} \{ \frac{  \sum X_i - n \mu_0 }{ S \sqrt{n} } < t_{n-1, a} \}  $$ where $t_{n-1, a} $ is the $a$ -quantile of the student law with n-1 degrees of freedom?","['statistical-inference', 'statistics', 'normal-distribution']"
3250845,Stable points in a GIT quotient,"I have a maybe stupid question on GIT: Let $\mathbb P^N=\mathbb P^N_k$ be the Hilbert scheme of hypersurfaces of degree $d$ in $\mathbb P^n$ , where $N=\binom{n+d}{d}-1$ , and let $G:=PGL_{n+1}(k)$ . Then we can consider the action $\rho:G\times \mathbb P^N \to \mathbb P^N$ . If $x\in \mathbb P^N$ is a stable point, then by definition the orbit $G.x$ should be closed. However, this looks very weird to me. Note that $PGL_{n+1}(k)$ is an open set in $\mathbb P^{n^2+2n}$ and $\mathbb P^N$ is proper, by valuative criterion, there is a unique extension "" $\mathbb P^{n^2+2n}. x$ "" of $G.x$ , which is just the closure $\overline{G.x}$ . Let $x\in \mathbb P^N$ be any stable point. Then $\overline{G.x}-G.x$ really contains some very bad point: since $\mathbb P^{n^2+2n}-PGL_{n+1}(k)$ contains non-invertable linear transformations, $\overline{G.x}-G.x$ contains hypersurfaces defind polynomials with fewer variables, hence is certainly unstable. So, this means $x$ is never stable. I know this must be wrong but I did not find where is my mistake. I would be appreciated if someone could help.","['geometric-invariant-theory', 'algebraic-geometry', 'invariant-theory']"
3250847,"Find the smallest positive integer such that $S(n)=10, S(n^2)=100$.","Consider the numbers: $1, 11, 111, 1111, 11111$ and so on. $S(1)^2=S(1^2)$ and, in fact, $(S(n))^2=S(n^2)$ for all the numbers where $S(n)$ is the sum of the digits  of the number $n$ . Edit 1: $S(n^2)$ is the sum of the digits of the number $n^2$ . For instance, let $n=11, S(11)=1+1=2$ and $S(n^2)=S(11^2)=S(121)=1+2+1=4=({S(11))}^2=2^2=(S(n))^2$ $S(n)$ is the sum of digits of $n$ whatever be $n$ . $n$ does not always need to be of the form: $1$ followed by only $1$ 's. Find the smallest positive integer such that $S(n)=10, S(n^2)=100$ . The immediate hint that comes to my mind is that it has to be less than $1111111111$ (ten 1's) from the context given. But I've no clue as to how I'll proceed further. The answer given is $1101111211$ . I am not allowed to use the calculator. I know that $S(n)$ is equivalent to $n (\mod 9)$ and both $S(n)$ and $S(n^2)$ are equivalent to $1(\mod 9)$ but I don't know how to use this here. Am I missing something? Can anyone suggest a shorter, simpler method? Edit 2: The answer along with the problem was published in Mathematical Excalibur in volume $22$ , number $3$ , page $2$ as Remark $2$ . Here's the link of the PDF version.","['number-theory', 'elementary-number-theory', 'decimal-expansion']"
3250848,Solve $x^2+5x+6 \equiv 0 \pmod{\!11\cdot 17}$,"Solve $x^2+5x+6 \equiv 187 \mod 187$ Solution $$x^2+5x+6 \equiv 187 \mod 187$$ $$ (x+\frac{5}{2})^2 \equiv \frac{1}{4}$$ $$ 4(x+\frac{5}{2})^2 \equiv 1$$ $$ y:= x+\frac{5}{2} $$ $$ 4y^2 \equiv 1 \mod 11 \wedge 4y^2 \equiv 1 \mod 17  $$ $$ ( 2y \equiv 1 \mod 11 \vee 2y \equiv 10 \mod 11 ) \wedge ( 2y \equiv 1 \mod 17  \vee 2y \equiv 13 \mod 17)  $$ $$ ( y \equiv 6 \mod 11 \vee y \equiv 5 \mod 11 ) \wedge ( y \equiv 9 \mod 17  \vee y \equiv 15 \mod 17)  $$ Combining that from CRT I got: $$ y \in \left\{49, 60,83,94 \right\} $$ and for example: $$ x+\frac{5}{2}  \equiv 94 \mod 187$$ $$ 2x  \equiv 183 \mod 187$$ some calculus and get... $$x \equiv 185 $$ And the same thing for each other case. Question Is there any faster (or smarter) way to solve equations like that?","['elementary-number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3250854,"If $\dim X=\infty$, can there be a compact $C$ such that $X\setminus C$ has two components?","Let $X$ be an infinite-dimensional normed space. Can there be a compact $C$ such that $X\setminus C$ has two components? My guess is no because compact sets are kind of ""small"" (their interiors are empty). But how can we prove it?","['connectedness', 'general-topology', 'normed-spaces', 'real-analysis']"
3250863,Solution of Fredholm integral equation,"I am trying to determine the $C^2$ solution of the Fredholm integral equation $$u(x)=x+\int_0^1|x-\xi|u(\xi)\, d \xi , \ \ 0\leq x\leq 1$$ I hve done the following: Let's start with $u_0=1$ then we get $$u_1(x)=Tu_0=x+\int_0^1|x-\xi|u_0(\xi)\, d \xi=x+\int_0^1|x-\xi|\, d \xi=x^2+\frac{1}{2}$$ At the next step we get $$u_2(x)+Tu_1(x)=x+\int_0^1|x-\xi|u_1(\xi)\, d \xi=...=\frac{1}{6}x^2+\frac{1}{2}x^2+\frac{x}{6}+\frac{1}{2}$$ Next we get $$u_3(x)+Tu_2(x)=x+\int_0^1|x-\xi|u_2(\xi)\, d \xi=...=\frac{ x^6}{30}+\frac{x^4}{12}+\frac{x^3}{18}+\frac{x^2}{2}+\frac{3x}{20}+\frac{37}{72}$$ I don't see any pattern. Are my calculations wrong? Or my intial function $u_0$ ? Or is the way I am doing that wrong?","['integration', 'calculus', 'integral-equations', 'ordinary-differential-equations']"
3250889,"Let $f$ be an entire function such that $f(1)=2f(0)$. Prove that $\forall\epsilon>0, \exists z\in\mathbb{C}$ such that $|f(z)|<\epsilon$","I am asked to prove this: Let $f$ be an entire function such that $f(1)=2f(0)$ . Prove that $\forall\epsilon>0, \exists z\in\mathbb{C}$ such that $|f(z)|<\epsilon$ I considered a function $g(z)=f(z+1)-2f(z)$ , which is also entire and has a zero at $z=0$ , but I am not sure this is going to help me solve the problem.",['complex-analysis']
3250902,How to calculate limit as $n$ tends to infinity of $\frac{(n+1)^{n^2+n+1}}{n! (n+2)^{n^2+1}}$?,"This question stems from and old revision of this question , in which an upper bound for $n!$ was asked for. The original bound was incorrect. In fact, I want to show that the given expression divided by $n!$ goes to $0$ as $n$ tends to $\infty$ . I thus want to show: $$\lim_{n\to\infty}\frac{(n+1)^{n^2+n+1}}{n!(n+2)^{n^2+1}}=0.$$ Using Stirling's approximation, I found that this is equivalent to showing that $$\lim_{n\to\infty} \frac{\exp(n)}{\sqrt n}\cdot\left(\frac{n+1}{n+2}\right)^{n^2+1}\cdot\left(\frac{n+1}{n}\right)^n=0.$$ However, I don't see how to prove the latter equation. EDIT: It would already be enough to determine the limit of $$\exp(n)\left(\frac{n+1}{n+2}\right)^{(n^2)}\left(\frac{n+1}{n}\right)^n$$ as $n$ goes to $\infty$ .","['limits', 'factorial']"
3250976,Proof that if $G$ doesn't contain cycle with length $1 \mod l$ then $\chi(G) \le l$,"Let $G$ be a graph and $l \ge 2$ . Proof that if $G$ doesn't contain cycle with length $1 \mod l$ then $\chi(G) \le l$ , where $\chi(G)$ is the minimum number of colours needed to properly colour $G$ (no adjacent vertices in $G$ assigned the same colours). I want to tell something about my current approach but after 1 hour of drawing cycles I didn't see anything interesting. Moreover I think that cycles it is really weird that we consider length modulo. How it can works that for cycle with length $l, l+2$ it would work, $l+1$ doesn't...","['graph-theory', 'discrete-mathematics']"
3250979,Arched coloring of graph: $A(D) \ge \log \chi(D)$,"Let $D$ be a directed graph. Arched coloring of $D$ is any coloring of edges such that $$ \forall_{a,b} (a,b) \mbox{ and } (b,c) $$ have different colors for any $(a,b), (b,c) \in E(D) $ . A(D) is the smallest number of colors on which we can arche color $D$ . Proof that $$ A(D) \ge \log \chi(D) $$ Idea I have drawn $K_4$ and had idea that I can color each vertex with color: $$0,1,2,...,\chi(G)-1$$ and then color $(a,b)$ on $color(a)+color(b) \mod \lceil\log\chi(G)\rceil $ . And for $K_n$ it seems to work. But I have just checked that way on very small example like path $P_3$ . $\chi(P_3) = 2$ and colors will be $0,1,0$ so colors of path will be just $1$ and $1$ so it is wrong.","['graph-theory', 'discrete-mathematics']"
3250981,Isomorphism between projective varieties $\mathbf{P}^{1}$ and a conic in $\mathbf{P}^{2}$,"I'm trying to establish an isomorphism between the projective line $\mathbf{P}^{1}$ and the conic in $\mathbf{P}^{2}$ defined by $Y=Z(g)$ , where $g=x^2+y^2-z^2$ . This is part of exercise I.3.1 in Hartshorne's Algebraic Geometry. I defined $\varphi : \mathbf{P}^{1} \rightarrow Y$ by $\varphi (a,b) = (a^2 -b^2,2ab,a^2+b^2)$ and showed that it's a morphism of varieties. I'm having trouble defining the inverse map. First, I tried using $\psi : Y \rightarrow \mathbf{P}^{1}$ defined by $\psi (a,b,c) = (b,c-a)$ to get $$\psi\varphi(a,b)=\psi(a^2 -b^2,2ab,a^2+b^2)=(2ab,2b^2)=(a,b) \text{ if }b\neq0$$ $$\varphi\psi(a,b,c)=\varphi(b,c-a)=(2a(c-a),2b(c-a),2c(c-a))=(a,b,c)\text{ if }a\neq c$$ This is problematic because of the restrictions on $a,b,c$ , and also because $(1,0,1)\in Y$ but $\psi(1,0,1)=(0,0)\notin \mathbf{P}^{1}$ . Then I tried using $\psi : Y \rightarrow \mathbf{P}^{1}$ defined by $$\psi (a,b,c) = \begin{cases}(b,c-a) \text{ if } a\neq c \\ (1,0) \text{ if } a=c \end{cases}$$ This is troublesome because now I can't show continuity: if $Z(T)$ is a closed set in $\mathbf{P}^{1}$ , then $\psi^{-1}(Z(T))$ is a closed set in $Y$ . (I need $\psi^{-1}(Z(T))$ to be the zero set of some homogeneous polynomials in $k[x,y,z]$ of positive degree, where $k$ is a field of characteristic zero.) Does anyone know how to establish the definition and continuity of the inverse map to $\varphi$ ? I will only be able to understand answers within the scope and context of Hartshorne up to chapter I section 3. Thanks.","['algebraic-geometry', 'projective-varieties']"
3251019,When is the profinite completion of a centerless group itself centerless?,"Having a centerless profinite completion leads to some nice properties. For example, given a short exact sequence $$1\to A\to B\to C\to 1$$ where $A$ is finitely generated and $\hat{A}$ has trivial center, we have an exact sequence $$1\to\hat{A}\to\hat{B}\to\hat{C}\to 1.$$ When does a centerless group $G$ have a centerless profinite completion $\hat{G}$ ? Does this change if $G$ is finitely generated and/or residually finite? I know that if $G$ is residually finite, then we have an injection $G\to \hat{G}$ with dense image, and so $Z(\hat{G})$ would have to live solely within $(\hat{G}\setminus G)\cup\{e\}$ . This seems unlikely, but I don't see the proof.","['group-theory', 'profinite-groups']"
3251063,"Is this ""kissing incircles"" property of a cevian through the Gergonne point (well-)known?","$\require{begingroup} \begingroup$ $\def\Ge{G_{\mathrm{e}}}$ A cevian through the Gergonne point $\Ge$ divides the triangle
into two, whose corresponding incircles 
are ""kissing"" (mutually tangent). For example, given a triangle $ABC$ , 
the points $D,E$ and $F$ are the tangential points of the incircle and 
the Gergonne point $\Ge$ is the point of intersection 
of Gergonne cevians $AD,BE$ and $CF$ . Cevian $CF$ splits $\triangle ABC$ into $\triangle CAF$ and $\triangle CFB$ with corresponding centers and radii 
of the inscribed circles $I_{c1},\,r_{c1}$ and $I_{c2},\,r_{c2}$ . These two incircles 
are tangent at the point $T_c=I_{c1}I_{c2}\cap CF$ , $I_{c1}I_{c2}\perp CF$ . Many properties of the Gergonne point are described in the literature,
for example, in Deko Dekov. “Computer-Generated Mathematics: The Gergonne Point”. 
In: Journal of Computer-Generated Euclidean Geometry 1 (2009), p. 14 but it seems that I failed to find this one explicitly stated/used. Question: Is this a (well-) known property of Gergonne cevian/point?
  Any reference(s)? $\endgroup$","['euclidean-geometry', 'circles', 'geometry', 'reference-request', 'triangles']"
3251076,Orthonormal basis of the Schwartz space,"The Hermite functions $(h_n)_{n \ge 0}$ are the eigenfunctions of $T f = (x+\partial_x)(x-\partial_x) f$ , they are defined by their generating function $g(x,t)=\sum_{n=0}^\infty h_n(x) t^n = e^{-x^2/2+2xt-t^2}$ , looking at $\int_{-\infty}^\infty g(x,t)g(x,u)dx= \sqrt{\pi} e^{2tu}$ , $\|h_n\|_{L^2}^2 = \sqrt{\pi} \frac{2^n}{n!}$ , $\partial_x g,\partial_t g$ , $(x-\partial_x) h_n = (n+1)h_{n+1},(x+\partial_x) h_n = 2 h_{n-1}$ shows that $\left(\frac{h_n}{\|h_n\|_{L^2}}\right)_{n \ge 0}$ is an orthonormal basis of the Schwartz space, in the sense that it is an orthonormal basis of $L^2(\Bbb{R})$ and $f \in \mathscr{S}(\Bbb{R})$ if and only if $f = \sum_{n \ge 0} c_n \frac{h_n}{\|h_n\|_{L^2}}$ where $\forall k, \lim_{n \to \infty} c_n n^k=0$ . A similar behavior appears with $(e^{2i \pi nx})_{n \in \Bbb{Z}}$ the eigenfunctions of $f \mapsto {}{}{}{}{}{} i\partial_x f$ , they are an orthonormal basis of $C^\infty(\Bbb{R/Z})$ again in the sense that they are an orthonormal basis of $L^2(\Bbb{R/Z})$ and $f \in C^\infty(\Bbb{R/Z})$ iff $f = \sum_n c_n e^{2i \pi nx}$ where $\forall k, \lim_{n \to \pm\infty} c_n n^k=0$ . Question : What would be some other relevant examples of such basis ? For what kind of $\scriptstyle\text{(differential, normal, compact resolvent)}$ operator can we hope its eigenfunctions will be such an orthonormal basis of the Schwartz space ?","['regularity-theory-of-pdes', 'schwartz-space', 'functional-analysis']"
3251103,Two questions on the Fourier transform for $\mathcal{L}_2(\mathbb R)$-functions.,"I have read that for $f\in\mathcal{L}_2(\mathbb R)$ , its Fourier transform need not coincide with the familiar form for $\mathcal{L}_1(\mathbb R)$ -functions, on account that this integral might not exist . For $g\in\mathcal{L}_1(\mathbb R)$ the 'familiar form' of its Fourier transform I take to be: $$\hat g(t)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \exp(-ist)g(s)\text{d}s,$$ for $t\in\mathbb R$ . What are examples of $f\in \mathcal{L}_2(\mathbb R)$ for which this idea is exemplified? That is to say, for whom the above integral representation need not exist; the more accessible the example, the better. To circumvent this, for $f\in \mathcal{L}_2(\mathbb R)$ we can take as the expression for its Fourier transform $$\hat f(t)=\lim_{N\to\infty}\int_{-N}^N\exp(-ist)f(s)\text{d}s.$$ The limit here is with respect to the $\mathcal{L}_2(\mathbb R)$ -norm. With regards to this, I have read that this representation is on account of the Dominated Convergence Theorem . How is it, exactly, that the Dominated Convergence Theorem has been applied to give us this workable form of the Fourier transform for $f\in\mathcal{L}_2(\mathbb R)$ ? Also, how does it allow us to avoid the convergence issues of the 'familiar form' first considered?","['integration', 'fourier-transform', 'fourier-analysis', 'functional-analysis']"
3251114,Why is the Gauss map useful?,"Currently I'm studying differential geometry, and more specifically the Gauss map. I'm using the (in)famous Do Carmo Differential Geometry and Surfaces book. I'm having a hard time understanding the use of the Gauss map. I understand what it's doing, but not why. I also do not understand why this map has useful properties (as the second fundamental form can be derived from it). To give you an ""insight"" to my mind, this is how I see the map (correct me if I'm wrong) My view of the Gauss map (not a formal definition) The Gauss map maps the normal vector at each point of a curve to the unit sphere. Therefore, it provides a mapping from every point of the curves to the unit sphere. So, why do we need this map? Why is this useful and why do we need this map to compute important properties such as the second fundamental form? For me, every curve would just be put into an unit circle. If I need to clarify my thinking, please tell me!",['differential-geometry']
3251126,characterisation of Kahler metrics in terms of Levi-Civita connection,"Let $M$ be a complex manifold with the complex structure $J$ and let $h$ be an Hermitian metric, associated with the $(1, 1)$ -form $\Omega$ .
(which means that $\Omega(X, Y) = h(JX, Y))$ It's known that $\omega$ is Kahler if and only if $\nabla J$ = 0, i.e. $J$ is parallel with respect to the Levi-Civita connection on $M$ . How one can prove the equivalent proposition, namely, $\Omega$ is Kahler if and only if $\nabla \Omega = 0$ , where $\nabla$ is the Levi-Civita connection which is compatible with $\operatorname{Re}(h)$ (the real part of $h$ ). Equivalently, that means that the parallel transport is unitary, i.e. preserves the Hermitian structure. It seems that one should differentiate the definition $\Omega(X, Y) = h(JX, Y)$ and apply some kind of a known identity (to use the symmetries/e.t.c.) but i cannot see a way to proceed that. Are there any ways to prove the proposed statement: (1) independently (2) having the fact that $\nabla J = 0$ ? In any case, does the question above somehow relate to the fact that the Chern connection and Levi-Civita connection coincide?","['complex-geometry', 'kahler-manifolds', 'differential-geometry']"
3251166,Notation for sum/product over 1-bits in binary representation,"Is there a notation, which indicates that an operation (sum/product/etc) should be taken over the 1-bits in a number $x$ 's binary representation, i.e. those powers of $2$ that when summed up equal $x$ ? Actually the question is not specific to base $2$ , a notation for any base would great, but it was easier formulating it this way. Edit to explain it in more detail: Let's consider the number $x = 5$ , it's binary representation is $101$ . Now if I would want to take a sum involving these powers of two I could write $$\sum_{\substack{i\\i^{th} \text{ bit in binary representation of x is 1}}  }2^i = x$$ The question is essentially if there's a less cumbersome way of defining such a sum.
And if yes , if there is there a way to generalize it to higher bases.","['notation', 'discrete-mathematics']"
3251195,Existence of a pointwise convergent subsequence,"Let $X$ be a compact metric space and $f_m:X\to[0,1]$ a continuous function for each $m\in\mathbb N$ . Does there necessarily exist a $f:X\to[0,1]$ (not necessarily continuous) and a subsequence $(f_{m_k})$ such that $f_{m_k}(x)\to f(x)$ for each $x\in X$ pointwise? Note that the equicontinuity of $(f_m)_{m\in\mathbb N}$ is not assumed, so that the Arzelà–Ascoli theorem is of no use here. However, the desired conclusion is also weaker: the supposed limit function $f$ need not be continuous and only pointwise convergence is required. Any suggestion would be appreciated.","['general-topology', 'real-analysis']"
3251233,"Calculate $\int_3^4 \sqrt {x^2-3x+2} \, dx$ using Euler's substitution","Calculate $\int_3^4 \sqrt {x^2-3x+2}\, dx$ using Euler's substitution My try: $$\sqrt {x^2-3x+2}=x+t$$ $$x=\frac{2-t^2}{2t+3}$$ $$\sqrt {x^2-3x+2}=\frac{2-t^2}{2t+3}+t=\frac{t^2+3t+2}{2t+3}$$ $$dx=\frac{-2(t^2+3t+2)}{(2t+3)^2} dt$$ $$\int_3^4 \sqrt {x^2-3x+2}\, dx=\int_{\sqrt {2} -3}^{\sqrt {2} -4} \frac{t^2+3t+2}{2t+3}\cdot \frac{-2(t^2+3t+2)}{(2t+3)^2}\, dt=2\int_{\sqrt {2} -4}^{\sqrt {2} -3}\frac{(t^2+3t+2)^2}{(2t+3)^3}\, dt$$ However I think that I can have a mistake because Euler's substition it should make my task easier, meanwhile it still seems quite complicated and I do not know what to do next. Can you help me? P.S. I must use Euler's substitution because that's the command.","['integration', 'definite-integrals', 'real-analysis']"
3251237,Coloring directed wheel graph $W_6$ with $k$ colors,"Let consider directed wheel graph $G_6$ example . We want to color each vertex on $1$ of $k$ colors. How many different coloring there are if two graphs we consider as the same if one can be transformed into second with some isomorphism that maintains the orientation of the edges and the colors of the vertices? Solution Let consider group $\mathbb Z_2 \times \mathbb Z_6$ Elements from first part informs us if we do mirror reflection or not. Elements from second group informs us if we rotate our wheel and how many times. For example $(1,4)$ means that we do mirror reflection and rotate elements $4$ times to right. Ok, now let make cycle index $$\begin{array}{|c|c|c|c|}
\hline
elements& cycles\\ \hline
(0,0) & x_1^7  \\ \hline
(0,0) & x_1 x_6  \\ \hline
(0,0) &  x_1 x_3^2 ? \\ \hline
(0,0) &  x_1 x_2^3 \\ \hline
(0,0) &   x_3^2 x_1 \\ \hline
(0,0) &   x_1 x_6\\ \hline
(1,0) & x_1^7  \\ \hline
(1,0) & x_1 x_6  \\ \hline
(1,0) &  x_1 x_3^2 ? \\ \hline
(1,0) &  x_1 x_2^3 \\ \hline
(1,0) &   x_3^2 x_1 \\ \hline
(1,0) &   x_1 x_6\\ \hline
\end{array} $$ $$I(x_1,x_2,x_3,x_4,x_5,x_6,x_7) = \frac{2x_1}{12}(x_1^6 + 2x_6+ 2x_3^2 + x_2^3) $$ so $$I(k,k,k,k,k,k,k) = \frac{k}{6}(k^6 + 2k+ 2k^2 + k^3) $$ is that solution correct?","['graph-theory', 'polya-counting-theory', 'discrete-mathematics']"
3251251,"For what $n$ can $\{1, 2,\ldots, n\}$ be partitioned into equal-sized sets $A$, $B$ such that $\sum_{k\in A}k^p=\sum_{k\in B}k^p$ for $p=1, 2, 3$?","This is a recent problem in American Mathematical Monthly. The deadline for this question just passed: $\textbf{Problem:}$ For which positive integers $n$ can $\{1,2,3,...,n\}$ be partitioned into two sets $A,B$ of the same size so that: \begin{align}\sum_{k\in A}k&=\sum_{k \in B} k ,&\sum_{k\in A}k^{2}&=\sum_{k \in B} k^{2} , &\sum_{k\in A}k^{3}&=\sum_{k \in B} k^{3} \end{align} It is clear that the set $\{1,2,...,16\}$ can be partitioned into $A=\{1,4,6,7,10,11,13,16\}$ and $B=\{2,3,5,8,9,12,14,15\}$ , and the sum of the elements, its squares and cubes, are equal in $A$ and $B$ . So for any $n$ divisible by $16$ , an extension of this argument will work. It is not too difficult to show that $n$ has to be a multiple of $8$ . Can we have any $n$ which is an odd multiple of $8$ , for which the problem statement is true; say $24$ ?","['integer-partitions', 'elementary-number-theory', 'combinatorics']"
3251254,Counting different cube with size $2$,"In set of $8$ cube (each has edge with length $1$ ) we have $3$ cubes with exactly one white side (other sides are black) and $5$ cubes with all white sides. We make from this cubes, one big cube with size $2$ . Count on how many ways can it be done if two ways of making big cube we consider as the same if one big cube can be transformed into to second cube in some rotation of that cube in $\mathbb R^3$ . Assume that walls are opaque. my try: Let consider rotation of normal cube $1\times 1 \times 1$ \begin{array}{|c|c|c|c|}
\hline
rotation& \mbox{how many of that type  }&cycles\\ \hline
id & 1  & x_1^6 \\ \hline
\mbox{center of sides 90 degree} & 1  & x_1^2x_4 \\ \hline
\mbox{center of sides 180 degree} & 6  & x_1^2x_2^2 \\ \hline
\mbox{diagonals} & 3  & x_3^2 \\ \hline
\mbox{center of edges} & 8  & x_2^3 \\ \hline
\end{array} so $$I(x_1,x_2,x_3,x_4,x_5,x_6) = \frac{1}{24}(x_1^6+x_1^2x_4+6x_1^2x_2^2+3x_3^2+8x_2^3) $$ but due to we have cube $2\times 2 \times 2$ we rotate $4$ sides per one old side. So I should multiply each cycle by $4$ . And get: $$I(x_1,x_2,x_3,x_4,x_5,x_6) = \frac{1}{24}(x_1^{24}+x_1^4x_4^4+6x_1^8x_2^8+3x_3^8+8x_2^{12}) $$ but I am not sure what have I to do now?","['polya-counting-theory', 'combinatorics', 'discrete-mathematics']"
