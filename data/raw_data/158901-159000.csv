question_id,title,body,tags
2730630,Is a function continuous at the point where it ends abruptly?,"Is a function continuous at the point where it ends abruptly? A function $f(x)$ to said to be continuous at a point $a$ iff: 1) $f(a)$ is defined, 2)$\lim\limits_{x \to a} f(x)$ exists, and 3)$\lim\limits_{x \to a} f(x)=f(a)$ At point $P_3$: 1)Left-hand limit exists, which is equal to 1. 2)The function is defined at $P_3$, which is also equal to 1. Therefore $$\lim\limits_{x \to P_3^-} f(x)=f(P_3)$$ So, can it be inferred that the function is continuous at $P_3$ or the rhight hand limit should also exist for the function to be continious at $P_3$?",['limits']
2730669,convention for interior product,"What is the convention that we take for the interior product of a function? i.e., if $X\in\mathfrak{X}(M)$ and $f\in C^\infty(M)$, how do we define $\iota(x)f$? Recall that $\iota(X)\alpha := \alpha(X,\cdot)$ for $\alpha\in\Omega^r(M)$ and then $\iota(X)\alpha\in\Omega^{r-1}(M)$ but this doesn't make sense for $f$. I suppose that we take the convention that $\iota(X)f$ is zero.","['differential-forms', 'differential-geometry']"
2730696,Central limit theorem for dependent random variables with covariance condition,"Consider a sequence of identically distributed real-valued random variables $(X_i)_{i\in\mathbb{N}}$, with $\mathbb{E}\left[X_i\right]=0$ and $\mathbb{E}\left[X_i^2\right]=1$. Suppose that there exists some constant $c\in(0,1)$, such that
$$\mathbb{E}\left[X_iX_j\right]<c^{|i-j|}$$
for any $i,j\in\mathbb{N}$ (with $i\not= j$). Is it true that $$n^{-1/2}\sum_{i=1}^nX_i\overset{d}{\rightarrow}\mathcal{N}(0,1)$$
as $n\rightarrow\infty$? There are many central limit theorems for dependent random variables but I have been unable to find any quite like this. I am not attached to the specific choice of covariance bound, but would love to find a result of this form that depends on nothing more than first and second moments. The motivation for the question in part comes from this nice thread on the law of large numbers: Weak Law of Large Numbers for Dependent Random Variables with Bounded Covariance","['probability-limit-theorems', 'probability-theory', 'covariance', 'central-limit-theorem']"
2730723,Integral $\int_0^{\infty} \frac{x\cos^2 x}{e^x-1}dx$,"I have encountered this integral and I am stuck evaluating it:$I=\int_0^{\infty} \frac{x\cos^2 x}{e^x-1}dx$ My try was to expand the numerator into power series, indeed: $$x\cos^2x=\frac{x}{2}(1+\cos(2x)) =\frac{x}{2} +\sum_{n=0}^{\infty} \frac{(-1)^n 2^{2n-1} x^{2n+1}}{(2n)!}$$ And using  $\zeta{(z)} \Gamma{(z)} =\int_0^{\infty} \frac{x^{z-1}}{e^x-1}dx$ gives: $$I=\frac{1}{2}\zeta{(2)}+ \sum_{n=0}^{\infty} \frac{(-1)^n 2^{2n-1}}{(2n)!} \zeta{(2n+2)} \Gamma{(2n+2)}=\frac{1}{2}\zeta{(2)}+ 2 \sum_{n=0}^{\infty} (-1)^n \zeta{(2n+2)}$$ Is there a way to simplify this? Or maybe another approach to this integral? Edit: According to the answer in the comment, would this show that $\sum_{n=1}^{\infty} (-1)^{n-1} \zeta{(2n)}=\frac{\pi^2}{6}(2-3\text{ csch}^2(2\pi))+\frac{1}{16} $  ?","['improper-integrals', 'integration', 'riemann-zeta']"
2730734,Geometric meaning of an integral on a compact Riemann surface,"Let $X$ be a compact Riemann surface and fix a volume form $\Omega$ on $X$ such that $\int_X\Omega=1$. Now let's fix a function $g:U\subset X\to\mathbb R$ on $X$ with the following properties: $U=X\setminus\{x_1,\ldots,x_r\}$ for $r\in\mathbb N$. $g$ is a $C^\infty$ function on $U$. For any point $x\in X$ there exist a real number $a\in\mathbb R$ and
a $C^\infty$ function $u$ on an open neighborhood of $x$ such that
the equality: 
$$g=a\log|z|^2+u\quad (\ast)$$ holds in an open (punctured)
    neighborhood of $x$ contained in a holomorphic chart $(V,z)$ centred
    in $x$ $\Delta_{\bar\partial}(g)$ is constant In other words $g$ is smooth almost everywhere, on singular point it has logarithmic behaviour and its $\bar{\partial}$-Laplacian is constant. Now consider the integral 
$$\int_X g\Omega$$
(note that the form is integrable cause the singularities are integrable). What is its geometric/intuitive meaning? What are we measuring with such integral? Is it related to the numbers $a$ of equation $(\ast)$ (just finitely many of them are nonzero)? Many thanks in advance","['complex-geometry', 'integration', 'riemann-surfaces', 'differential-forms']"
2730747,"Finding probability -picking at least one red, one blue and one green ball from an urn when six balls are selected","Six balls are to be randomly chosen from an urn containing $8$ red, $10$ green, and 12 blue balls. What is the probability at least one red ball, one blue and one green ball is chosen? Sample space = $\binom{30}{6}$ P = 1 - P(All red + All Green + All Blue + Only red and Green + Only Red and Blue + Only Green and Blue ) $$P = \Large 1 - \frac{\binom{8}{6} + \binom{10}{6} + \binom{12}{6} + \binom{18}{6} + \binom{22}{6} + \binom{20}{6}}{\binom{30}{6}}$$ According to this, I got $\large 1 - \frac{133099}{593775}$, 
which is $0.7758$? Is my approach correct?","['combinatorics', 'probability']"
2730773,Solve $e^x + e^y = 20$ for $y$,"I have an equation $e^x + e^y = 20$,where $e^x=\exp(x)$ and would like to express $y$ from this:
$$e^y = 20 - e^x \\\ln(e^y) = \ln(20-e^x) \\ y = \frac{\ln(20)}{\ln(e^x)} \\ y =\frac{ \ln(20)}{x}.$$
Is this okay?","['algebra-precalculus', 'logarithms', 'exponential-function']"
2730774,Find a collection of pairwise independent events each with probability $< p$,"Given $p>0$, I'm trying to find a collection of events that are pairwise independent and the probability of each event is $\le$ $p$ but the probability that none of them occur is zero. For $p=\frac{1}{2}$, we can do the following. Let $X_1, X_2$ be independent random variables that take values in $\{0,1\}$ with equal probability and let $X_3 = X_1 +X_2 \mod 2$. Then the events $E_1:(X_1=X_2)$, $E_2:(X_2=X_3)$ and $E_3:(X_3=X_1)$ each have probability $\frac{1}{2}$ and are pairwise independent since $P(E_i \wedge E_j) = \frac{1}{4}$. Also, $P(\bar{E_1}\wedge \bar{E_2}\wedge \bar{E_3}) = 0$. How do we proceed for $p<\frac{1}{2}$? Any ideas are appreciated. (I tried using independent $n$-binary strings $X, Y, Z$ such that $z_i = x_i+y_i  \mod 2$ for $n$ large enough such that $\frac{1}{2^n} < p$ and then finding events that are pairwise independent and satisfy what I want. But the plan didn't work so far)","['independence', 'combinatorics', 'probability-theory']"
2730810,Is a scheme $X$ projective iff every component of $X$ is projective?,"Today in a lecture it was claimed Proposition Let $X$ be a proper scheme of finite type and dimension $1$ over a field $k$. Then $X$ is projective. So, I already knew the above statement for integral curves - and I can see how to drop the condition of being reduced. However, I don't see how to reduce to the case where $X$ is irreducible. The proof of the Proposition was only sketched and this step was actually missing, so I wonder, how one might fix that (and, in particular, if the statement as given holds true). On further note, what can one say in higher dimensions - is it true that (for finite type $k$-schemes), projectivity can be checked on irreducible components? If not, is there a handy counterexample? My first attempt in proving such a statement would be looking at the Picard groups of $X$ and its normalization, and then try to argue using ample line bundles. Looking for counterexamples, I am at first sight rather hopeless (because if I recall correctly, counterexamples for proper varieties that are not projective are rather complicated in any case).","['projective-schemes', 'algebraic-geometry']"
2730823,Find shape function for finite element,"Using the unisolvent method to find the shape functions for a regular unit hexagon with vertices: $\{x,y\}=\{(1,0),(\frac{1}{2},\frac{\sqrt{3}}{2})(-\frac{1}{2},\frac{\sqrt{3}}{2}),(-1,0),(-\frac{1}{2},-\frac{\sqrt{3}}{2}),(\frac{1}{2}, -\frac{\sqrt{3}}{2}\}$, using a quadratic polynomial basis for the approximation. Can you give me a hint for this case? 
By trying to solve it, the matrix $A$ resultet singular.","['matrices', 'functional-analysis', 'linear-algebra', 'finite-element-method']"
2730828,Conditions that ensure a bounded probability density function,"Let $X$ be a random variable that is absolutely continuous with respect to the Lebesgue measure. That means that $X$ has a PDF $f$ . Are there simple conditions on $X$ that ensure that $$
\sup_{x\in\mathbb{R}}f(x) < \infty.
$$ with simple conditions. I mean for example, certain moment conditions or other tractable assumptions.","['probability-theory', 'probability-distributions']"
2730851,"Prove $\|A\| \leq \|A\|_{HS}$, where $\|A\|$ is the operator norm of A","I Am trying to solve the following problem Let
$H_1
,H_
2$
be  Hilbert  spaces.   Let
$A
\in B(H_1
,H_2
)$  be  a  Hilbert-Schmidt  operator.   For  a  complete
orthonormal sequence $(
u_n
)$ in
$H_1$, define the
Hilbert-Schmidt norm
$\|.\|_{HS}$
by $\|A\|_{HS}=\left(\sum^\infty_{n=1}\|A(u_n)\|^2\right)^\frac{1}{2} $ Prove  $\|A\| \leq \|A\|_{HS}$, where $\|A\|$ is the operator norm of A. My current attempt is as follows: Let $x \in H_1$ such that $ x=\sum^\infty_{n=1} \langle x,u_n\rangle u_N$ and $\|x\|=\left(\sum^\infty_{n=1}|\langle x,u_n\rangle|^2\right)^\frac{1}{2} $. Then we have, $\|A(x)\|=\|A\left(\sum^\infty_{n=1} \langle x,u_n\rangle u_N\right)\| =\|\sum^\infty_{n=1}A\left( \langle x,u_n\rangle u_n\right)\| = \|\sum^\infty_{n=1} \langle x,u_n\rangle A\left(u_n\right)\| \leq \|\sum^\infty_{n=1}A(u_n)\|\ \|\sum^\infty_{n=1}\langle x,u_n\rangle\|$ I am unsure where to go from here and feel I have made a mistake? Thank you in advance.","['normed-spaces', 'functional-analysis', 'lebesgue-integral', 'spectral-theory', 'operator-algebras']"
2730879,Are there any studies on a sorting function on real line?,"Let assume we have an integer array $[3,4,1,0,7]$, we can sort it into $[0,1,3,4,7]$. There is a sorting function $S: \mathbb{N}^k \to \mathbb{N}^k$ that sort the integer in the array defined as above. If we see an integer array as a function that maps an index set $\mathcal{I} \to \mathbb{N}$, I want to generalize it to the case that $\mathbb{N}$ is replaced by $[0,1]$ of a real line. In this continuum case, are there any studies on the analog to the 
 sorting function $S$ but on real line in the sense that it ""sorts"" a real-valued function $f(x)$ defined on $[0,1]$? i.e. a function $S': F[0,1] \to M[0,1]$, where: $\mathcal{F}[0,1]$ are all function from $[0,1] \to [0,1]$, $\mathcal{M}[0,1]$ are all monotonic increasing (non-decreasing) function from $[0,1] \to [0,1]$. that somehow preserves the values of $f(x)$. I am not sure how to define what it preserves precisely now (I guess should be related to derivative), just a weird idea. EDIT followed by David C. Ullrich's answer What if we restrict $\mathcal{F}[0,1]$ to be some kind of ""nice"" function, such as differentiable almost everywhere on $[0,1]$?","['reference-request', 'real-analysis', 'monotone-functions']"
2730988,Explain why no boundary conditions are needed,"The Question: Consider the operator $$My(x) \equiv \frac{d}{dx}\biggl(a(x) \frac{dy}{dx} \biggr) + b(x)y(x) \; \; \; \; \; \; \; \; , \; \; \; \; \; \; \; \; \alpha<x<\beta $$ Suppose that $a$ satisfies $a(\alpha)=a(\beta)=0$ , and we want to solve $$My(x)=\lambda y(x)$$ I have to explain why no boundary conditions need to be given. I get how the operator is already in Sturm-Liouville form, so that it is self-adjoint, but I honestly don't get how you can solve an ODE with no boundary conditions.","['boundary-value-problem', 'sturm-liouville', 'ordinary-differential-equations', 'eigenfunctions']"
2731034,Get approximations of series involving Cauchy numbers of the first kind and the Möbius function,"We denote for integers $n\geq 1$ the $n$th Gregory coefficient as $G_n$, and the Möbius function as $\mu(n)$. You've here the Wikipedia's article dedicated to the Gregory coefficients. Using an argument of absolute convergence, and the information of previous Wikipedia for the first related series to the Gregory coefficients and the result due to Candelperger, Coppo and Young,  it is obvious to prove that $$\sum_{n=1}^\infty\mu(n)|G_n|\tag{1}$$ and $$\sum_{n=1}^\infty\frac{|G_n|\cdot m(n)}{n}\tag{2}$$ are convergent series, where $m(x)$ denotes the function $$m(x)=\sum_{1\leq k\leq x}\frac{\mu(k)}{k}.\tag{3}$$ Question. Have you an idea/hint to get a good approximation (the first four or six digits) of $(1)$ and $(2)$? Many thanks. I know that there are upper bounds for the absolute value of $(3)$ for large values of $x$.","['real-analysis', 'analytic-number-theory', 'asymptotics', 'mobius-function', 'sequences-and-series']"
2731041,"Given a limit value, prove that a given series is absolutely convergent","The question is: If $\lim\limits_{n \to \infty} n^4|a_n|=1$, then show that $\sum_{i=1}^\infty (-1)^{n+1}a_n$ absolutely converges. What I've got so far: Since the given limit is equal to 1, the series $\sum_{i=1}^\infty n^4|a_n|$ diverges. To show that the series in the problem absolutely converges, I need to show that $\sum_{i=1}^\infty |a_n|$ converges. I thought of using the comparison test, but it would be inconclusive if I compared $|a_n|$ with $n^4|a_n|$. Any help would be appreciated!","['sequences-and-series', 'limits']"
2731044,Proving the infinite direct sum of orthogonal closed subspaces of a hilbert space is a closed linear subspace,"Suppose that $\left\{ \mathcal{H}_n \, \big| \, n\in\mathbb{N} \right\}$ is a set of orthogonal closed subspaces of a Hilbert space $\mathcal{H}$. We define the infinite direct sum
  $$\bigoplus_{n=1}^{\infty} \mathcal{H}_n := \left\{ \sum_{n=1}^{\infty} x_n \, \big| \, x_n \in \mathcal{H}_n \, \text{and} \, \sum_{n=1}^{\infty} \|\ x_n \|^2 < \infty \right\} . $$ 
  Prove that $\oplus_{n=1}^{\infty} \mathcal{H}_n$ is a closed linear subspace of $\mathcal{H}$. I feel like I proved this in a round about manner and am looking for critiques or a 'better' way to prove the result. Here is my proof: Without loss of generality let $\left\{ e_k \, \big| \, k \in A \right\}$ be an orthonormal set in the Hilbert space $\mathcal{H}$. Then, for each $\mathcal{H}_n$ there is some $A_n$ such that $\left\{ e_k \, \big| \, k\in A_n \right\}$ is a orthonormal set. Additionally, $\displaystyle\bigcup_{n=1}^{\infty} A_n = A$.  Let 
$$\mathcal{G}:=\bigoplus_{n=1}^{\infty} \mathcal{H}_n := \left\{ \sum_{n=1}^{\infty} x_n \, \big| \, x_n \in \mathcal{H}_n \, \text{and} \, \sum_{n=1}^{\infty} \|\ x_n \|^2 < \infty \right\} . $$ First we note that $\mathcal{G}$ is well-defined since $\sum_{n=1}^{\infty} x_n$, for $x_n \in \mathcal{H}_n$ converges if and only if $\sum_{n=1}^{\infty} \|\ x_n \|^2 $ converges, which we have by construction of $\mathcal{G}$. Note that the convergent unordered sums can be added term by term, suppose that 
$$\sum_{n \in A} x_n := x, \quad \sum_{n \in A} y_n := y$$
Take $\epsilon>0$, there are finite sets $I, J \subset A$, such that 
$$\|\ \sum_{k \in I}  x_k - x \|\ < \frac{\epsilon}{2}, \quad \quad \|\ \sum_{k \in J} y_k - y \|\ < \frac{\epsilon}{2}.$$
It follows that if $I \cup J \subset K$ is a finite subset of $A$, then 
\begin{align*}
\|\ \sum_{k \in K} (x_k + y_k) - (x+y) \|\ &\leq \|\ \sum_{k \in K} x_k - x \|\ + \|\ \sum_{k \in K} y_k - y \|\ \\
&< \frac{\epsilon}{2}+\frac{\epsilon}{2} \\
&=\epsilon.
\end{align*}
If $x,y \in \mathcal{G}$, with
$$x=\sum_{k\in A} x_k e_k , \quad y=\sum_{k \in A} y_k e_k $$
then
$$x+y = \sum_{k \in A} (x_k + y_k)e_k.$$
Now since $$|x_k + y_k|^2 \leq (|x_k| + |y_k|)^2 \leq 2(\max\left\{|x_k|,|y_k|\right\})^2 \leq 4(|x_k|^2 + |y_k|^2),$$
it follows that
$$\sum_{k \in A} |x_k + y_k|^2 \leq 4\left( \sum_{k \in A} |x_k|^2 + \sum_{k\in A} |y_k|^2 \right) < \infty,$$
so $x+y\in \mathcal{G}$. Similarly, we also have that $\lambda x \in \mathcal{G}$ for all $\lambda \in \mathbb{C}$ and $x\in\mathcal{G}$, so $\mathcal{G}$ is a linear subspace. Now to prove that $\mathcal{G}$ is closed, first note that $\overline{\mathcal{G}} \cap \mathcal{G}^{\perp} = \left\{0\right\}$ because if $x\in\overline{\mathcal{G}}\cap\mathcal{G}^{\perp}$, then $\mathcal{G}=\overline{\mathcal{G}}^{\perp}$ we have $x\perp x$ so $x=0$. Now suppose that $x \in \mathcal{G}$. Let
$$y:=\sum_{k\in A} \langle e_k , x \rangle e_k.$$
By Bessel's inequality,
$$\sum_{k \in A} | \langle e_k , x \rangle |^2 \leq \|\ x \|^2,$$
so $y\in\mathcal{G}$. Moreover, $\langle e_k , x \rangle = \langle e_k , y \rangle$ for all $k\in A$, which implies that $x-y\in\mathcal{G}^{\perp}$. Since $x,y\in\overline{\mathcal{G}}$, it follows that $x-y\in\overline{\mathcal{G}}\cap\mathcal{G}^{\perp}$, so $x=y$, and $x\in\mathcal{G}$, which implies that $\mathcal{G}$ is closed.","['functional-analysis', 'real-analysis', 'hilbert-spaces', 'proof-verification']"
2731058,Showing existence of a diffeomorphism preserving volume forms,"I want to show that given $\alpha,\beta$ two volume forms on a closed manifold such that $\int_M\alpha = \int_M\beta$ how can we show that there exists a diffeomorphism $\phi:M\rightarrow M$ such that $\phi^*\beta = \alpha$ . Attempt: I suspect we have to use Moser's lemma which states that If $(\omega_t),t\in[0,1]$ is a smooth family of symplectic forms which are cohomologue (i.e. $[\omega_t] = [\omega_0]$ for $t\in[0,1]$ ). Then there exists a smooth family of diffeomorphisms $(\phi_t),t\in[0,1]$ such that $\phi^*_t\omega_t = \omega_0$ for $t\in[0,1]$ .","['symplectic-geometry', 'differential-geometry']"
2731082,Proof for total variation distance for product measure using coupling,"If $\mu_1$ and $\nu_1$ are probability distributions on the finite state space $\Omega_1$, $\mu_2$ and $\nu_2$ are probability distributions on the finite state space $\Omega_2$, $\mu_1 \times \mu_2 (x,y) = \mu_1(x)\mu_2(y)$ and  $\nu_1 \times \nu_2 (x,y) = \nu_1(x)\nu_2(y)$ are probability measures on $\Omega_1 \times \Omega_2$. It follows from the triangle inequality that $$\|\mu_1\times\mu_2 - \nu_1\times\nu_2 \|_{TV} \leq \|\mu_1-\nu_1\|_{TV}+\|\mu_2-\nu_2\|_{TV}$$ I heard it mentioned in a few places that this can also be proved using coupling, but could not figure that. Could someone walk me through or point me to the proof?","['probability-theory', 'coupling']"
2731140,"16 programmers were asked if they knew Pascal, C++ and Java. How many of them knew C++ and Java simultaneously but did not know Pascal?","$16$ programmers were asked if they knew Pascal, C++ and Java.
It turned out that: $12$ knew Pascal $15$ knew C++ Java language knew as many programmers as Pascal and C++ knew
simultaneously, $8$ knew pascal and Java. How many programmers knew C++ and Java simultaneously but did not know Pascal? My approach: A - Pascal B - C++ C - Java $16 = |A∪B∪C| = |A|+|B|+|C| -|A∩B| - |A∩C|-|B∩C|+ |A∩B∩C|$ We also that $8$ people know both Pascal and Java so $|A∩C|= 8$, and from third fact number of people who know Java is the number who know both Pascal and C++ so $|C| = |A∩B|$. Back to first equation of $|A∪B∪C| = 16$ we get: $16 = |A|+|B|+|C| -|A∩B| - |A∩C|-|B∩C| + |A∩B∩C|$ $16 - 12 - 15 - |C| + |C| + 8 = -|B∩C|+ |A∩B∩C|$ (as $|A∩B|=C$ and $|A∩C|= 8$) $-3$ = -$|B∩C|+ |A∩B∩C|$ What i'm looking for is $|B∩C|$ but to do so I need to have $|A∩B∩C|$ any ideas?","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
2731145,Evaluate $\int_{-1}^1\frac{\sin(x)}{\arcsin(x)}dx$,"Evaluate$$\int_{-1}^1\frac{\sin(x)}{\arcsin(x)} \, dx$$ Since $\frac{\sin(x)}{\arcsin(x)}$ is an even function, my first step was to simplify it to: $$2\int_0^1\frac{\sin(x)}{\arcsin(x)} \, dx$$ And after trying IBP, I quickly realised that the indefinite integral is non-elementary as verified by WolframAlpha. The usual method I use to continue for these types of problems is by differentiating under the integral sign, however I fail to see how it could help here. My only other idea is that using contour integration could help, but i'm not to good at it and can't use it very well.","['definite-integrals', 'calculus', 'closed-form']"
2731217,strange number-theory,"do exist five different number such as $a_1,a_2,a_3,a_4,a_5\in\mathbb{N}$ which sum of every three of them be divisible by the sum of two other? I guess I have to prove $i<j \implies a_i<a_j$ $a_4+a_5\le a_1+a_2+a_3$ is not possible. but I don't know how to prove it","['number-theory', 'real-analysis']"
2731245,eigenvector of compositions implies eigenvector of respective functions in composition?,"Suppose the matrices $A, B \in Mat(n, \mathbb{F}).$ If a vector $v$ is an eigenvector of the matrix $AB,$ that is, of the composition $f_A \circ f_B,$ then is it also an eigenvector of $A$ and of $B,$ that is, of $f_A$ and of $f_B?$ I think not, for I came up with the example
$$v \mapsto \lambda v + w$$ under $f_A$ for some non-zero vector $w$ that is linearly independent with respect to $v,$ and $$\lambda v + w \mapsto \lambda v$$ under $f_B.$ But, I'm not sure if my example makes any sense. Note that $A$ and $B$ are square matrices. Thank you!","['eigenvalues-eigenvectors', 'matrices', 'eigenfunctions', 'linear-transformations', 'linear-algebra']"
2731250,Functional equation: $f(x)+ 2f \left(\dfrac{2002}{x}\right) =3x$,"Let $f$ be a real valued function such that$f(x)$+ $2f \left(\dfrac{2002}{x}\right) =3x$, find $f(x)$ Attempt: Substituting $x=1$ and $x=2002$ and solving the simultaneous equations obtained, I got: $f(2002)= -2000$ and $f(1)= 4003$ Now, $f(1)+f(2002)= 2003$ Also, there are $2003-1$ integers between $1$ and $2002$ (inclusive). How do I proceed? Any hints?",['functions']
2731286,Conformal branched cover from the hyperbolic plane to the euclidean plane,"The euclidean plane can conformally branch cover the sphere. This is witnessed by the tiled Peirce quincuncial projection . Is there likewise a conformal branched covering map from the hyperbolic plane to the euclidean plane? When I say conformal branch covering map, I mean a branch covering map that is conformal everywhere except at the branching points.
(Ideally, each branch point should have degree two.)","['covering-spaces', 'euclidean-geometry', 'hyperbolic-geometry', 'general-topology', 'conformal-geometry']"
2731296,Show that $x^3 - 2 x^2 +\log(1+x)(x(3x+4) -2(1+x)^2 \log(1+x))$ is positive,"I want to show that (the following just gets rid off large brackets)
$$x^3 - 2 x^2 +x(3x+4)\log(1+x)-2(1+x)^2 \log^2(1+x)>0, \ \ \mbox{for}\ \ x\in(0,\infty).$$ My attempt: Transfer all negative terms to the other side of the inequality. Evaluate the both sides at different limits of $x$, and see that both sides converge to $0$ for $x \to 0$ and increase infinitely for $x \to \infty$.  Take the derivative of both sides with respect to $x$ to demonstrate that the left side increases faster than the right side.  Burn out at that point because showing that the last point holds I need to engage into a similar task as proving the original inequality.","['algebra-precalculus', 'inequality', 'nonlinear-analysis']"
2731306,Show that at least one of the equations $x^2+b_1x+c_1 = 0$ and $x^2+b_2x+c_2 = 0$ has two real root. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Given that $b_1b_2 = 2(c_1+c_2)$. $b_1$,$b_2$,$c_1$,$c_2$ are real number. Show that at least one of the equations $$x^2+b_1x+c_1 = 0$$ and $$x^2+b_2x+c_2 = 0$$ has two real root.",['algebra-precalculus']
2731339,Implicit and Explicit ODE,"I wish to solve the following ODE: $$
\frac{dy}{dt}=f(y) - y
$$ $f$ is a nonlinear function of $y$. I found a paper online that solves the ODE using a ""second-order backward difference for $\frac{\partial y}{\partial t}$ and a second-order Adams-Bashforth for the explicit treatment of the nonlinear term."" These authors arrive at:
$$
(3+2\Delta t) * y(t+2\Delta t)=4y(t + \Delta t)-y(t)+2\Delta t*[2f(t+ \Delta t)-f(t)]
$$
The backwards finite difference expansion of the PDE leads to:
$$
(3+2\Delta t) * y(t+2\Delta t)=4y(t + \Delta t)-y(t)+2\Delta t*2f(t+ 2\Delta t)
$$
I am confused as to how we can combine the Adams-Bashforth method with the backwards finite difference method to arrive at the authors' derived equation.
$$
$$ EDIT 1: Now that I understand that the method is called IMEX, I am confused on how we can combine an explicit method and an implicit method. For the above ODE, the implicit backwards finite difference method on the linear term gives:
$$
3* y(t+2\Delta t)-4y(t + \Delta t)+y(t)=-2\Delta t * y(t+2\Delta t)
$$ The Adams-Bashforth explicit method on the nonlinear term gives: 
$$
y(t+2\Delta t) = y(t+\Delta t) + \Delta t*[(3/2)*f(t+\Delta t) - (1/2)*f(t)]
$$ How can we combine these two finite difference methods into one IMEX equation? What algebraic manipulations must we perform? $$
$$ EDIT 2: So the IMEX method of a second order backward finite difference and an Adams-Bashforth would give: $$
\frac{3* y(t+2\Delta t)-4y(t + \Delta t)+y(t)}{2\Delta t}=\underbrace{-y(t+2\Delta t)}_\text{Implict} + \underbrace{[(3/2)*f(t+\Delta t) - (1/2)*f(t)]}_\text{Adams-Bashforth}
$$ Yet the authors in https://www.sciencedirect.com/science/article/pii/S001046559700115X use the same IMEX method and get (equation 9):
$$
\frac{3 * y(t+2\Delta t)-4y(t + \Delta t)+y(t)}{2 \Delta t}=\underbrace{-y(t+2\Delta t)}_\text{Implicit}+\underbrace{[2f(t+ \Delta t)-f(t)]}_\text{Adams-Bashforth}
$$ Am I missing something or is the paper incorrect?","['numerical-methods', 'ordinary-differential-equations', 'finite-differences']"
2731397,"Matrix with permutations of $\{1,\dots,n\}$ is invertible","Clearly the $1 \times 1$ and the $2 \times 2$ matrices $(1)$ and $\begin{pmatrix} 1 & 2\\ 2 & 1 \end{pmatrix}$ are invertible. I am wondering about the following: if I take any matrix that has in the first line $(1 \ 2 \ \dots \ n)$ and in each other line a different permutation of the numbers $\{1, \dots,n\}$, will it always be invertible? I think this is wrong for higher $n$; it might have to do with the fact that we get more permutations than we can fit in the rows and with the Leibniz formula. But still, it works with all (or most) $3 \times 3$ cases, so I am really getting curious. If it turns out that this does not work, I would still be interested in knowing if there is at least one way of doing it that works (for every $n$) and if it is relevant that we have chosen $\{1,\dots,n\}$ and not any other (real, different, positive) entries. Edit: As pointed out in the comments and in user1551's answer, this is not always the case, not even when in each column and in each row each element appears exactly once. I would still like to know why all the $3 \times 3$ matrices (of this kind) are always invertible and, above all, why the ""shift matrix"" mentioned in Hw Chu's comment is indeed always invertible (and whether there are other procedures that always work).","['permutations', 'linear-algebra', 'determinant']"
2731453,This polynomial has integer coefficient,"In another question, someone discussed about the fact that, if $p_1,\ldots,p_n$ is a list of integer, then the polynomial $$f=\prod_{e_1,\ldots,e_n\in\{\pm1\}}(x+e_1\sqrt{p_1}+\cdots+e_n\sqrt{p_n})$$ has integer coefficient. Can someone explain better this to me? edit: the link of the previous question is Minimal Polynomial of $\sqrt{2}+\sqrt{3}+\sqrt{5}$ , and you have to multiple factor for every possible choice of $e_1,\ldots,e_n\in\{\pm1\}$","['abstract-algebra', 'polynomials']"
2731465,Scaling the NS equation: supercriticality and energy estimate,"There is a ‘rescaling transformation’ that is particularly significant for
the Navier–Stokes equations when they are posed on the whole space, but is
also important in the local regularity theory.
Suppose first that $u(x,t)$ is a solution of the linear heat equation $$\partial_t u-\Delta u=0,~~x\in \mathbb{R}^3$$ It is simple to check that for any $\lambda > 0$ and any $\alpha \in \mathbb{R}$ the function $$u_{\lambda, \alpha}=\lambda^\alpha u(\lambda x,\lambda^2 t)$$ is again a solution of the heat equation. When we consider the Navier–Stokes equations we need the nonlinear term $(u\cdot \nabla)u$ to transform in the same way as $\partial_t u$ and $\Delta u$; it is easy to check that this requires the choice  $\alpha =1$. Therefore if $u(x, 0) = u_0(x)$ gives rise to a solution $u(x, t )$ of the Navier–Stokes equations on $\mathbb{R}^3$, with corresponding
pressure $p(x, t )$, then the rescaled functions $$\lambda u(\lambda x,\lambda^2 t)~~and ~~\lambda^2p(\lambda x,\lambda^2 t)$$ 
still solve the equations, but with rescaled initial data $u_{0,\lambda}(x) = \lambda u_0(\lambda x)$. For $\lambda$ this corresponds to shrinking (spatial) distances by a factor of $\lambda^{-1}$ and increasing the speed by a factor of $\lambda$; it is clear that the time =(distance/speed) should therefore shrink by a factor of $\lambda^{−2}$.
Note that if an initial condition $u_0$ gives rise to the solution $u(x, t )$ and the rescaled initial condition $u_{0,\lambda}$ gives rise to the solution $u_\lambda(x, t )$ then $$u_\lambda \mbox{ is regular on} [0, T_\lambda] \Leftrightarrow u \mbox{ is regular on}  [0, \lambda^2T_{\lambda}].$$
Spaces of functions in which the norm is unchanged by the rescaling
$$u(x) \rightarrow \lambda u(\lambda x)$$
are termed ‘critical spaces’. These are the natural spaces in which to try to
prove ‘small data’ results, i.e. the global existence of smooth solutions when
the norm of the initial condition is small, since the norm of the data is unaffected by the aformentioned rescaling transformation. We give two examples of such spaces: the Sobolev space $\dot{H}^{1/2}$ and the Lebesgue space $L^3$. Critical spaces are important since in some cases local existence in a critical space can be used to deduce global existence. ** 
 Now, let us take the same scaling transformation in the context of **supercritical space such as $L^2$ and blow up this fine-scale behaviour by $\lambda$ to create a coarse-scale solution to Navier-Stokes. Given that the fine-scale solution could (in the worst-case scenario) be as bad as an arbitrary smooth vector field with kinetic energy and cumulative energy dissipation at most $E$, the rescaled unit-scale solution can be as bad as an arbitrary smooth vector field with kinetic energy and cumulative energy dissipation at most $E \lambda$, as a simple change-of-variables shows. Note that the control given by our two key quantities has worsened by a factor of $\lambda$; because of this worsening, we say that these quantities are supercritical – they become increasingly useless for controlling the solution as one moves to finer and finer scales. This should be contrasted with critical quantities (such as the energy for two-dimensional Navier-Stokes), which are invariant under scaling and thus control all scales equally well (or equally poorly), and subcritical quantities, control of which becomes increasingly powerful at fine scales (and increasingly useless at very coarse scales). Knowing that the energy estimate for the solution $u$ is given by $$ \underbrace{\sup_{0 \leq t < T} \frac{1}{2} \int_{{\Bbb R}^3} |u(t,x)|^2\ dx}_{\mbox{kinetic energy}}+  \underbrace{\frac{1}{2}\int_0^T \int_{{\Bbb R}^3} |\nabla u(t,x)|^2\ dx dt}_{\mbox{cumulative energy dissipation}}\leq \underbrace{\|u_0\|_{L^2({\Bbb R}^3)}^2}_{E}$$
Would you please write the same energy estimate associated to $\lambda u$ and explain in mathematical terms how exactly in that case the energy estimate will  fail to control the kinetic and cumulative dissapation energy in contrast with critical spaces case.
References: Why global regularity for NS equations is hard, Terry Tao, what's new blog Robinson, J., Rodrigo, J., & Sadowski, W. (2016). The Three-Dimensional Navier–Stokes Equations: Classical Theory (Cambridge Studies in Advanced Mathematics). Cambridge: Cambridge University Press. doi:10.1017/CBO9781139095143 Thanks","['functional-analysis', 'regularity-theory-of-pdes', 'parabolic-pde', 'partial-differential-equations']"
2731475,How to find a Graph's $K_v$?,"By $K_v$ I mean the minimun amount of vertices to be removed from a connected graph such that it loses its connectivity We define the Graph $G_n$ as a graph with $3^n$ vertices, where each vertex has an assigned string of $n$ digits 0, 1 or 2, and two vertices are connected if and only if they differ in exactly one bit I want to prove that the Graph $G_n, n \ge 2$ is $2n$-connected, but I always get stuck I tried to show that between any two vertices there are $2n$ disjoint paths that connect them, but I can't find an easy way of proving it. I also tried by induction, saying that $G_n$ is connected and then building $G_{n+1}$ with 3 copies of $G_n$, but I also can't seem to go anywhere What else can I try?","['graph-connectivity', 'graph-theory', 'discrete-mathematics']"
2731490,Two disjoint random events,"You roll twice with four-sided die in which the numbers one and two occur with probability $\frac{1}{3}$, and the numbers three and four each with probability  $\frac{1}{6}$. Let X be the number of singles and Y the number of fours that occurred after two throws. How do I create a table of probability function $p_{x,y}(x,y)=P\left \{X=x \wedge Y=y\right \}$ ? This symbols at the end of this quations are little bit confusing to me. $P(X=1)=\frac{1}{3}$, $P(X=2)=\frac{1}{3}$, $P(X=3)=\frac{1}{6}$ and $P(X=4)=\frac{1}{6}$. $P(X,Y)=P(x)P(Y)=\frac{1}{3}\frac{1}{6}=\frac{1}{18}$ So do I just write Table: $x_i$  $P(X=x_i)$ 1       $2*1/3$ 4      $2*1/6$ Because there are two throws or? How do I calculate $P\left \{X+Y>0\right \}$? Do I just add them? 
$P\left \{X+Y>0\right \}=\frac{1}{3}+\frac{1}{6}$","['probability-theory', 'probability', 'probability-distributions']"
2731508,Evaluate $\int_{0}^1 \int_{0}^1 \int_{0}^1 \frac{1}{x^2+y^2+z^2}dxdydz$,"I am trying to evaluate the following integral:
$$I =\int_{0}^1 \int_{0}^1 \int_{0}^1 \frac{1}{x^2+y^2+z^2}dxdydz.$$ Here is my trial. By using spherical coordinate, let 
$x = r\sin(\theta)\cos(\phi), y = r\sin(\theta)\sin(\phi), z = r\cos(\phi).$
Then one can write $I$ as
$$ I = \int_{\Omega} \frac{1}{r^2} r^2\sin(\theta)drd\theta d\phi
= \int_{\Omega} \sin(\theta)drd\theta d\phi$$
where $\Omega$ is something I can't figure out...
Here $\Omega$ is
$$ \Omega = \{(r,\theta,\phi) : 0\le x,y,z \le 1\}.$$
Since $0\le z=r\cos(\phi) \le1$, we have
$$ 0 \le \arccos(1/r) \le \phi \le \pi/2. $$
Since $0\le y=r\sin(\theta)\cos(\phi) \le 1$, we have
$$ 0 \le \theta \le \arcsin(1/(r\cos(\phi)).$$
But I don't see how such classification help in evaluating $I$. Any suggestions/comments/answers will be very appreciated.
Thanks in advance.","['multivariable-calculus', 'integration', 'polar-coordinates', 'calculus']"
2731512,Evaluating $\int_{0}^{\pi\over 2}{\mathrm dx\over \cos(x)+\cos^2(x)}\ln\left({1+a\cos(x)\over 1-a\cos(x)}\right)$,"I am trying to evaluate this integral, where $a<1$
$$\int_{0}^{\pi\over 2}{\mathrm dx\over \cos(x)+\cos^2(x)}\ln\left({1+a\cos(x)\over 1-a\cos(x)}\right)$$ It is look obvious to enforce a substitution of $u=cos(x)$ because of it commonly appeared in the integral. $\sin(x)=\sqrt{1-u^2}$ $\mathrm du=-\sin(x)\mathrm dx=-\sqrt{1-u^2}\mathrm dx$ which lead to $$\int_{0}^{1}{\mathrm du\over (u+u^2)\sqrt{1-u^2}}\ln\left({1+au\over 1-au}\right)$$ This part $\ln\left({1+au\over 1-au}\right)$ still not simplify, so we make a substitution of $v=\ln\left({1+au\over 1-au}\right)$ which lead to after a very long process of simplification $$\int_{0}^{k}ve^{v\over 2}\cdot{e^v+1\over e^v-1}\cdot{\mathrm dv\over a(e^v+1)+e^v-1}$$
where $k=\ln\left({1+a\over 1-a}\right)$ This is now involving hyperbolic functions, but I don't how to use it. 
Where $\tanh\left(v\over 2\right)={e^v-1\over e^v+1}$ $2e^v\sinh(v)=e^v-1$ $2e^v\cosh(v)=e^v+1$ This is how far I got to, unfortunately I can't continued. Can anyone please point me in the right direction to complete the calculation or help to evalaute the integral. Thank.",['calculus']
2731550,Kernel of Cotangent Vector Equals Tangent Space to Submanifold,"Given a $1$-form $\omega=dx+ydz$ on $\mathbb{R}^3$, does there exist a $2$-dimensional submanifold $S\subset \mathbb{R}^3$ such that
$$\ker(\omega_p)=T_pS$$
for all $p\in S$? This is a homework question on which I have been stuck for quite a while. How should one determine the kernel of a covector $\omega_p$ given a general $1$-form $\omega$? I know that if $S$ is the preimage of a regular value of some smooth map $F:\mathbb{R}^3\to \mathbb{R}$, then $T_pS=\ker(T_pF)$. So in this case can I conclude that
$$\ker(\omega_p)=T_pS\quad \forall p\in S\iff \omega_p=T_pF\quad \forall p\in S$$
or is the above not so obvious/false? If $\omega=df$ (i.e. when it is exact) though, we do know that $\omega_p$ is the map
$$T_pf:T_p\mathbb{R}^3\to T_{f(p)}\mathbb{R}$$ But now I'm totally lost since $S$ need not be the preimage of a regular value of $f$. I suspect that the answer to the original problem is no when $\omega$ is not exact. Can anyone give a hint on how to solve this problem? Any help would be appreciated.","['manifolds', 'differential-forms', 'differential-geometry']"
2731561,Incidence correspondence of Grassmannian is a projective variety,"I'm working the following question: Let $$\Sigma = \{(L, p) \in G(k,n) \times \mathbb{P}^{n-1} \mid
 L\subset \mathbb{P}^{n-1}, p \in L\}.$$ Here we're viewing $G(k,n)$ as
  $(k-1)$-dimensional linear subspaces of $\mathbb{P}^{n-1}$. Show that
  $\Sigma$ is a projective variety . A (flawed) attempt : The Grassmannian $G(k,n)$ is itself a (projective) variety and so it has an open cover by affine varieties, say $\{U_i\}_{i\in I}$ where each $U_i$ is an affine open set isomorphic to an affine variety. (The $U_i$s are described in Construction 8.15 here ). I initially wanted to describe a cover of $\Sigma$ by $\{U_i \times \mathbb{P}^{n-1}\}_{i \in I}$. But this really gives a cover of $G(k,n) \times \mathbb{P^n}$, which is larger than what we want. Question(s): Can I modify what I have to give an open cover of $\Sigma$ by affines? From there, I just need to show the diagonal is closed to see that $\Sigma$ is a variety. Alternatively, is a straight-forward way to see how $\Sigma$ is the vanishing set of some homogenous polynomials, and see that $\Sigma$ is a projective variety that way?","['grassmannian', 'algebraic-geometry']"
2731575,How can I find the hypotenuse and opposite sides of triangle?,Based on the given information how can I find the pixel values of the hypotenuse and opposite sides of the triangle?,['trigonometry']
2731607,Complete sequence of sequences,"Who can continue (complete) the following sequence: $$1,n-1,\frac{(n-2)(n-1)}{2},\frac{(n-4)(n-3)(n+1)}{6},\frac{(n-7)(n-4)(n-2)(n+3)}{24},\dots$$ This was emerging in the course of this question as crucial coefficients in the transformation of Fibonacci polynomials. I am pretty sure I have seen this before in another context, but I cannot remember it exactly, I have the vague memory that it contains some more complicated structure than factorials, maybe something like rising factorials/Pochhammer-symbols. Edit The next sequence of this row is numerically $$a_6 = \{-4,12,-21,-24,-9,42,154,360,702,1232\dots\}$$ This is no 5$^{th}$ degree polynomial anymore and I am unable to detect the law. . If fact it is one, this can be seen from the fifth difference sequence which gets constant (when calculated correctly). By interpolation and factorisation I finally got the law (see below). Edit-2 Meanwhile I found an alternative way to generate these series, but however still got stuck at the 6$^{th}$ one. They can be expressed as the coefficients of the series expansion of polynomial fractions $f_i(x)$ at $x=0$, the functions are in detail: \begin{array}{c|c|c|c|c|c|c}
i& 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline
a_i&1 & n-1 & \frac{(n-2)(n-1)}{2}& \frac{(n-4)(n-3)(n+1)}{6}&\frac{(n-7)(n-4)(n-2)(n+3)}{24}&v.i.&v.i.\\ \hline
f_i(x)&\frac{1}{x-1}& \frac{1}{(x-1)^2}&\frac{1}{(x-1)^3}&\frac{(x-2)x}{(x-1)^4}&\frac{(x-2)(1+x(x-3))}{(x-1)^5}&?&\\ 
\end{array} Edit-3 So, the expression for the next sequences are: $$ a_6 = \frac{(n-5)(n-4)(n-1)(n^2-5n-54)}{120} $$ $$ a_7 = \frac{(n-6)(n-3)(n^4-12n^3-71n^2+642n+160)}{720} $$ I am still unable to guess a general law for $f_i(x)$ or $a_i$. Edit-4 I will try to get some recursion relation now from the consideration of the difference series.","['number-theory', 'combinatorics', 'generating-functions', 'elementary-number-theory']"
2731610,Prove: $(a+b) \sqrt {ab}+(a+c) \sqrt {ac}+(b+c)\sqrt {bc}≥ \frac {(a+b+c)^2}{2}$,"EDİTED: I tried to solve the last question.I can not get the state of equality and I can not find my mistake. Can you show me my mistake(s)? 4.
  Let, $a,b,c$ be the lengths of sides of a triangle. Prove the inequality: $$(a+b) \sqrt {ab}+(a+c) \sqrt {ac}+(b+c)\sqrt {bc}≥ \frac {(a+b+c)^2}{2}$$ I will use: $(1)$ $\begin{cases} a+b≥2\sqrt{ab} &\\ b+c≥2\sqrt{bc}&\\ a+c≥2\sqrt{ac} \end{cases}\Longrightarrow \begin{cases} (a+b)\sqrt{ab}≥2ab &\\ (b+c)\sqrt{bc}≥2bc&\\ (a+c)\sqrt{ac}≥2ac \end{cases} \Longrightarrow (a+b)\sqrt{ab}+(b+c)\sqrt{bc}+(a+c)\sqrt{ac}≥2(ab+bc+ac)$ $(2)$ $\begin{cases} b(a+c)>b^2 &\\ c(a+b)>c^2&\\ a(b+c)>a^2 \end{cases}\Longrightarrow 2(ab+bc+ac)>a^2+b^2+c^2 \Longrightarrow \frac{a^2+b^2+c^2}{2}<ab+bc+ac$ Applying $(1)$ and $(2)$ we have $(a+b) \sqrt {ab}+(a+c) \sqrt {ac}+(b+c)\sqrt {bc}- \frac {(a+b+c)^2}{2}≥2ab+2bc+2ac- \frac {(a+b+c)^2}{2}≥ab+bc+ac-\frac{a^2+b^2+c^2}{2}>0≠≥0$","['inequality', 'proof-verification', 'algebra-precalculus', 'proof-writing', 'contest-math']"
2731637,Is the numerator of $\sum_{k=0}^{n}{(-1)^k\binom{n}{k}\frac{1}{2k+1}}$ always a power of $2$ in lowest terms?,"Is the numerator of
$$
\sum_{k=0}^{n}{(-1)^k\binom{n}{k}\frac{1}{2k+1}}
$$
always a power of $2$ in lowest terms, and if so, why? Is there a combinatorial or probabilistic proof of this?","['combinatorics', 'probability']"
2731638,Conditions for extending convergence in distribution of a subsequence to the entire sequence,"Suppose I have a sequence for which I can prove convergence in distribution of some infinite subsequence, e.g. let $\{X_n: n \geq 1\}$ be the original sequence and suppose I can show that $$ X_{n_k} \to X \textrm{ in distribution as } k \to \infty$$
for a subsequence $n_k$ satisfying $n_k \to \infty$ as $k \to \infty$. Question: What is the weakest set of conditions on the entire sequence I can impose in order to be able to extend the convergence of the subsequence back up to the whole subsequence level? I would imagine this would be true if we could bound the deviation of the elements between the elements of the subsequence, but I am just spitballing. For example, something like: for any $k$,
$$ \sup_{n_k \leq n \leq n_{k+1}} \left| X_n - X_{n_k} \right| \to 0 \textrm{ in probability as } k \to \infty $$","['weak-convergence', 'probability-theory', 'convergence-divergence']"
2731648,Proving that if $a+b = c+d$ and $a!b! = c!d!$ then either $a = c$ and $b = d$ or $a = d$ and $b = c$,"I recently have been thinking about how many ways it is possible to get from one square to another on a square grid using the fewest possible number of orthogonal steps. (i.e., using the fewest number of the xiangqi general's move, how many paths are there?) For example, to do an up-right diagonal, you can go up then right or right then up, so for this path, the answer is 2. For a knight's move, there are 3 ways: UUR, URU, and RUU. We can obviously consider paths rotated multiples of 45 degrees, so I will assume the path goes up and right, and it goes more up than right. I was thinking that for each move a certain number of steps away from the starting point, each point might have a different number of ways of getting there. For a point (a,b) each path can be described by a permutation of the multiset that contains a U steps and b R steps. Therefore, the number of paths of (a, b) is the multinomial coefficient:
$${a+b \choose a, b} = \frac {(a + b)!} {a!b!}$$
If (a, b) and (c, d) are the same number of steps away from the starting point, then $a+b = c+d$. So, if we are saying that (a, b) and (c,d) have the same number of paths to them, then the above is also equal to:
$${c+d \choose c, d} = \frac {(c + d)!} {c!d!} = \frac {(a + b)!} {c!d!}$$
So, we are only a little algebra away from:
$$a!b! = c!d!$$
The whole aim was to show that the number of paths is unique for this distance away from the origin. So I want to prove (or disprove) that (a,b) must be the same point as (c, d) ignoring rotations, which amounts to the question I asked. It seems pretty intuitive that it could be true, but I'm not sure what direction to go to prove it and can't find anything online.","['combinatorics', 'factorial']"
2731655,Prove that every zero of $z^{4}-5z+1$ has multiplicity 1,"Let $z \in \mathbb{C}$ I need to prove $z^{4}-5z+1$ has only zeros with multiplicity one ""with minimal calculation"". I've tried using the argument principle, but that requires me to find a curve that surrounds every zero. The only other thing I can think of is proving that $z^{4}-5z+1$ can only be a product of degree 1 irreducible polynomials but that doesn't seem like ""minimal calculation"". Are there any quick ways of solving this?",['complex-analysis']
2731677,If $A\subset\mathbb{R}$ and A is countable then A is a union of countably many nowhere dense sets.,"I am working on the following question with the question specific definition that: a set $A\subset\mathbb{R}$ is nowhere dense iff for any two points $c<d,\exists a<b\ s.t.\ [a,b]\subset[c,d]$ and $[a,b]\cap S=\varnothing.$ If A is countable, show that A is a union of countably many nowhere dense sets(Hint: Singleton). I know that countable union of countable sets are countable but I havent a clue how to go about proving a singleton is nowhere dense using the supplied definition alone. I get that interior of a closure must be empty also, but not given that, and not sure how to derive that from given definition. My current argument going with that A is a singleton by being countable,
$$ [a,b]\cap A=\varnothing\Rightarrow([a,b]\cap A)^\circ=(\varnothing)^\circ\Rightarrow[a,b]^\circ\cap A^\circ=\varnothing\Rightarrow(a,b)\cap \varnothing=\varnothing. $$ Is this enough?","['general-topology', 'real-analysis', 'analysis']"
2731692,Combinatorial rule for a stable stack of bricks.,"Suppose that you have a stack of identical, frictionless, uniform-density $1 \times 2$ bricks arranged in the ordinary configuration (each row is offset by $1$ relative to the row below.) Question Is there a combinatorial rule that captures whether or not a given stack of bricks would be stable? In particular, that no small vertical force will cause the stack to move. In other words, how can one determine whether or not a configuration is stable without doing a static force analysis. (Admittedly, this question is a little hand-wavey, so let me know if I can clarify anything.) Examples Clearly, a stack of bricks that does not have any ""overhangs"" should be stable: But we can also allow a cantilever if there's a brick above: Non-examples However, having a brick above is not sufficient, because a stack like this should not be stable. Similarly, having a brick above is necessary because a configuration like this is in unstable equilibrium; in particular the upper-right brick in this example would fall if an arbitrarily small force were applied to the right side.","['discrete-geometry', 'combinatorics', 'physics']"
2731701,Vitali's Theorem for Convergence in Measure,"Show that Fatou's lemma, the Monotone Convergence Theorem, the Lebesgue Dominated Convergence Theorem, and the Vitali Convergence Theorem remain valid if ""pointwise convergence a.e."" is replaced by ""convergence in measure"" I have done every part except the part about Vitali's theorem. I tried a google search, but I couldn't find very much, so I am hoping the MSE community would be so kind as to critique my proof. For reference, here is the statement of Vitali's theorem with which I am working: Let $E$ be of finite measure. Suppose the sequence of functions $\{f_n\}$ is uniformly integrable over $E$. If $f_n \to f$ pointwise a.e. on $E$, then $f$ is integrable over $E$ and $$\lim_{n \to \infty} \int_E f_n = f$$. Here's my proof. First, I will show that $f$ is integrable. Given $\epsilon = 1$, by uniform integrability there exists $\delta > 0$ such that $A \subseteq E$ measurable with $m(A) < \delta$ implies $\int_A |f_n| < 1$ for every $n \in \Bbb{N}$. Given $\delta$, $E$ can be partitioned into measurable sets $E_1,...,E_k$ with $m(E_i) < \delta$. Hence $$\int_E |f_n| = \sum_{i=1}^k \int_{E_i} |f_n| < k,$$ for every $n \in \Bbb{N}$, and therefore $\liminf \int_{E} |f_n| \le \sup \int_E |f_n| \le k$. By Fatou's lemma for convergence in measure, $$\int_E |f| \le \liminf \int_{E} |f_n| \le k < \infty.$$ Now we prove the limit part. Again, by Fatou's lemma for convergence in measure, we get $\int_E f \le \liminf \int_E f_n$. Note that there exists a subsequence $\{f_{n_k}\}$ for which $$\lim_{k \to \infty} f_{n_k} = \limsup f_n.$$ Since $f_{n_k} \to f$ in measure, there exists a subsequence $\{f_{n_{k_m}}\}$ such that $f_{n_{k_m}} \to f$ pointwise a.e. on $E$. Since $\{f_n\}$ is UI, $\{f_{n_{k_m}}\}$ will also be UI. Hence by Vitali's theorem for pointwise covergence $$\lim_{m \to \infty} \int_E f_{n_{k_m}} = \int_E f .$$ Since $\{\int_E f_{n_{k_m}}\}$ is a subsequence of $\{\int_E f_{n_k}\}$, it must be the case that $$\lim_{m \to \infty} \int_E f_{n_{k_m}} = \limsup \int_E f_n,$$ and therefore $\limsup \int_E f_n = \int_E f \le \liminf \int_E f_n$ which implies $$\lim_{n \to \infty} \int_E f_n = \int_E f$$ $\blacksquare$ Does this sound right?","['real-analysis', 'measure-theory', 'proof-verification']"
2731715,Middle School Stats Book mess up?,"I am curious what everyone's thoughts are on the following problem I found in a middle school textbook I am teaching out of. The chapter is on biased and unbiased data samples. Here is the question: To find how much money the average American family spends to cool their home, 100 Alaskan families are surveyed at random. Of these families, 85 said they spend less than 75 dollars per month on cooling. The researcher concluded that the average American family spends less than 75 dollars on cooling per month. Is the conclusion valid? The book states that it is valid because it is a simple, random sample. I would say otherwise considering they surveyed Alaskans and made a generalization about the whole nation. If you were given the options of unbiased simple, random sample, systematic unbiased sample, biased convenience sample, and biased voluntary sample, which would say is a better answer? I'm thinking a convenience sample.",['statistics']
2731791,"What are the implicit assumptions that justify the usage of the ""relative error""?","In applied statistics, for example, analyzing data from a science experiment, we sometimes use ""absolute error"" while at the most of time, we calculate the ""relative error"". Generally, under what circumstances should we use the ""relative error"" instead of ""the absolute error""? If we use relative error, are we basically implicitly assuming that the variance of the sample distribution is positively correlated with the value of the mean? Is there a fundamental axiom in mathematics or statistics called ""scale independence""? It is same in the financial math literature, the variance of the random variable $X_t$ is usually correlated with the exact value of $X_{t-1}$. If $X_{t-1}$ is greater, the variance of $X_t$ is greater. This often leads to something like a Lognormal distribution.","['statistics', 'probability-distributions']"
2731800,Strictly positive Riemann integrable Function,"I read a good number of posts which deal with this specific topic, but none of them seem to answer my question. If this is a repost, I apologize. Alright so here is the problem, followed by a theorem which I use, and then my attempted proof: Exercise 7.4.4. Show that if $f(x)>0$ for all $x\in [a,b]$ and $f$ is integrable, then $\int_{a}^{b} f>0$. The theorem (and exercise) is from Abbott's Understanding Analysis , and my proof uses Theorem 7.4.2(ii): Theorem 7.4.2.(ii) Assume $f$ is integrable on $[a,b]$. If $m\leq f(x)\leq M$ for all $x\in [a,b]$ then 
$$m(b-a)\leq\int_a^b f(x) \leq M(b-a)$$ Now here is my proof: Proof . Let $f(x)>0$ for all $x\in [a,b]$ and assume $f$ is integrable. Since $f$ is integrable, then we can let ( but can we? )
$$m=\inf\{f(x_0):x_0 \in [a,b]\}\text{ and  }M=\sup\{f(x_0):x_0\in [a,b]\}$$ 
Where both $m$ and $M$ are greater than zero since $f(x)>0$. 
Now by Theorem 7.4.2.(ii), we can write $$m(b-a)\leq \int_{a}^{b} f\leq M(b-a)$$
And since $0<m(b-a)\leq M(b-a)$, then the integral must be greater than zero as well. Q.E.D. I would just like some verification/correction. My doubt is whether or not the values $m$ and $M$ necessarily have to exist, since Theorem 7.4.2.(ii) states that the inequality is only true IF there exists such values for $m$ and $M$. But since $f$ is integrable and $[a,b]$ is compact, then the function should attain a maximum and a minimum, correct?","['real-analysis', 'integration', 'calculus', 'proof-verification']"
2731804,Nonlinear differential equation with absolute value,"I am looking to solve the following nonlinear differential equation: $$y'(t) = |y(t)| + \cos(t),$$
for all $t$ in $[0,4]$ with $y(0) = -1.5$. I am really confused on how to approach it because of the absolute value.","['ordinary-differential-equations', 'absolute-value']"
2731806,What is the explicit formula for $\Phi(x)=\sum\limits_{n=1}^x\phi(n)$?,"I ran across the following claimed explicit formula for $\Phi(x)$. (1) $\quad\Phi(x)=\sum\limits_{n=1}^x\phi(n)$ (2) $\quad \frac{\zeta(s-1)}{\zeta(s)}=\sum\limits_{n=1}^\infty\frac{\phi(n)}{n^s}$ (3) $\quad\Phi_o(x)=\frac{3\,x^2}{\pi^2}+\sum\limits_{k=1}^K\left(\frac{x^{\rho_k}\,\zeta\left(\rho_k-1\right)}{\rho_k\,\zeta'\left(\rho_k\right)}+\frac{x^{\rho_{-k}}\,\zeta\left(\rho_{-k}-1\right)}{\rho_{-k}\,\zeta'\left(\rho_{-k}\right)}\right)+\frac{1}{6}+\sum\limits_{n=1}^N\frac{x^{-2\,n}\,\zeta(-2\,n-1)}{(-2\,n)\,\zeta'(-2\,n)}\,,\\$ $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad K\to\infty\land N\to\infty$ The formula for $\Phi_o(x)$ above doesn't seem correct as it only seems to converge for $x>1$ which is illustrated in the following plot of $\Phi_o(x)$ (orange) using the evaluation limits $K=N=200$. The $\Phi(x)$ function is shown in blue as a reference but is mostly hidden by the evaluation of $\Phi_o(x)$ except for $x<1$. The red discreet portion of the plot illustrates the evaluation of $\Phi_o(x)$ at integer values of $x$. Question : What is the correct explicit formula for $\Phi(x)=\sum\limits_{n=1}^x\phi(n)$? I realize the explicit formula for $\psi(x)$ only converges for $x>1$, but $\psi(x)$ doesn't take a step until $x=2$. Since $\Phi(x)$ takes a step at $x=1$, I was expecting the explicit formula for $\Phi(x)$ to converge for $x>0$ similar to the explicit formulas for $M(x)=\sum\limits_{n=1}^x\mu(n)$ and $Q(x)=\sum\limits_{n=1}^x\left|\mu(n)\right|$.","['number-theory', 'dirichlet-series', 'riemann-zeta', 'totient-function']"
2731819,Existence of matrix with certain property over a finite field,"Let $F_p = \{0,1,2,...,p-1\}$ be field for a prime $p$. Does there exist a matrix $A$ with entries in $F_p$ such that $tr(A^k) = 0$ if $k=2,3,\ldots,p-1$ and $tr(A^k) \neq 0$ if $k=1$?","['matrices', 'abstract-algebra', 'trace', 'finite-fields']"
2731867,Proving a statement false by reverse induction,"On a recent graph theory assignment, I proved a statement false by ""reverse"" induction, showing that $P(2)$ was false, and that $P(k) \implies P(k-1)$ for $k>2$, thus, by the contrapositive, $\neg P(k-1) \implies \neg P(k)$, so $P(n)$ should be false for all $n\in \mathbb{N}$. This seems natural and like it should work in general, but after a quick Google search into ""reverse induction"", nothing like this proof technique showed, at least in the first few results. For completeness, here's the statement and my proof. Does there exist a graph with vertex set $\{v_1,\dots,v_n\}$ such that $\deg(v_i)= i-1$ for $i=1,\dots,n$? Answer: No Proof: It does not work for $n=2$, since the first vertex would be a singleton, and the second vertex must have degree one, so it must be connected to the first, but this is impossible since the first vertex would then have degree one, a contradiction. Now for $n \geq 2$, Consider a labeling $\{v_1,v_2,\dots,v_{n+1}\}$, and suppose that there is a set of edges such that $\deg v_i = i-1$. Then by removing $v_{n+1}$ an all edges connected to it, we would have a graph with vertex set $\{v_1,v_2,\dots,v_n\}$, such that $\deg v_i = i-1$. The contrapositive of this however, is that if there is no vertex set on $\{v_1,v_2,\dots,v_n\}$, such that $\deg v_i = i-1$, then there is no graph with vertex set $\{v_1,v_2,\dots,v_{n+1}\}$ such that $\deg v_i = i-1$. As we have shown this for $n=2$, and the inductive step is proved, we must have that there are no graphs with vertex set $\{v_1,v_2,\dots,v_n\}$, such that $\deg v_i = i-1$ for $n \geq 2$.","['graph-theory', 'proof-writing', 'induction', 'discrete-mathematics']"
2731893,How group action works.,"Checking out Wikipedia and Quora (and some papers) was a bit difficult for understanding Group Action. The closest to understanding was this part: I understand the definition of homomorphism (a structure-preserving map between two algebraic structures of the same type). And from what I understand the symmetric group is a group of functions (bijective functions) mapping elements of a set $X$ onto itself. However I get confused at this line: ...an action of $G$ on $X$ may be formally defined as a group homomorphism $\phi$ from $G$ to the symmetric group of $X$. That seems to be the definition of group action, but I am having trouble understanding how to apply that. What that says to me is it is a homomorphism (function) $f \colon X \rightarrow S$ where $S$ is a set of bijective functions on $X$. That itself seems weird, I thought a group action would result in a number or something related to $\{1, 2, 3\}$ (defined next). Continuing, if we have a set $X = \{1, 2, 3\}$ and a group $(*,\{4, 5, 6\})$, then the symmetric group of $X$ would be something like $S = \{1 \rightarrow 2, 2 \rightarrow 3, \dots \}$, and $\phi$ would be something like $f : \{4, 5, 6\} \rightarrow S$. That doesn't make sense to me and I feel like I'm interpreting something wrong. I don't see how mapping to a set of functions $S$ will result in an action on the set.",['group-theory']
2731954,"If sides $a$, $b$, $c$ of $\triangle ABC$ are in arithmetic progression, then $3\tan\frac{A}{2}\tan\frac {C}{2}=1$","If sides $a$, $b$, $c$ of $\triangle ABC$ (with $a$ opposite $A$, etc) are in arithmetic progression, then prove that 
  $$3\tan\frac{A}{2}\tan\frac{C}{2}=1$$ My attempt: $a$, $b$, $c$ are in arithmetic progression, so
$$\begin{align}
2b&=a+c \\[4pt]
2\sin B &= \sin A+ \sin C \\[4pt]
2\sin(A+C) &=2\sin\frac {A+C}{2}\;\cos\frac{A-C}{2} \\[4pt]
2\sin\frac{A+C}{2}\;\cos\frac{A+C}{2}&=\sin\frac{A+C}{2}\;\cos\frac{A-C}{2} \\[4pt]
2\cos\frac{A+C}{2}&=\cos\frac{A-C}{2}
\end{align}$$","['trigonometry', 'sequences-and-series']"
2731993,A cute little group theory problem,"Let $G$ be a finite group and $S$ is a subset such $|S|>|G|/2$. Then prove that for any $g\in G$ there exist elements $a,b\in S$ such that $g=ab^{-1}$. My idea is if $G$ has the order 2 then it is trivial. If $|G|\geqslant3$ then $S$ has elements $a,b$ such that $ab^{-1}=g$ for some $g\in G$, but I cannot proceed like this. Any Hint, please?","['combinatorics', 'group-theory']"
2732056,Proof Verification: $\lim_{x \to 0} \cos(1/x) $ does not converge,"This is one of the questions from Bartle. While attempting this question, I used the sequential criteria as follows: take $\;(x_n)=1/n$ Then, as $ n \to \infty$ $(x_n) \to 0$ Now, $\cos(1/x)  = \cos (1/1/n) = \cos(n)$ diverges as it oscillates between -1 and 1 Can anyone please verify if this approach is correct? The reason why I'm a bit doubtful is because the solution manual uses a different approach that makes use of $\pi$ and what not and was worried if this rather simplistic approach is ok? Thank you in advance.","['real-analysis', 'sequences-and-series', 'proof-verification', 'limits']"
2732074,Does $A \times B \times C$ make sense? ($\times$: the Cartesian product),"I have seen that being used like in n-ary Cartesian product . But we know that Cartesian product is not associative so $(A \times B) \times C \neq A \times (B \times C)$ and therefore $A \times B \times C$ is not well-defined. (Like the cross product, we need to have the parentheses.) EDIT: My main concern is how all of these things can be defined (listed below)? Are all of them definable at all? As long as we are dealing with only two sets, $A \times B$ can only mean one thing. But ambiguities arise when the number of sets involved increases: (For compactness, I will use the $AB$ notation instead of $A\times B$) 3 sets:
$$(AB)C; \quad ABC; \quad A(BC)$$ 4 sets:
$$ABCD$$
$$(AB)CD; \quad A(BC)D; \quad AB(CD)$$
$$(ABC)D; \quad A(BCD)$$
$$((AB)C)D; \quad A((BC)D)$$
$$(A(BC))D; \quad A(B(CD))$$ And so on. Up to now, It has been made clear that: Sets like $X_1 \cdots X_n$ are accepted to have elements of the form $(x_1,\dots,x_n)$ -- Like $\mathbb{R}^n$. Sets that have clearly shown the order of operation, have elements of the form similar to the parentheses placement. For example $(x_1,((x_2,x_3),x_4)) \in A((BC)D)$. ( Am I right? ) But again some of the items of the above list remain ambiguous. Like $(ABC)D$. Can we define this? Can we set some rule for defining even more complicated combinations?","['notation', 'elementary-set-theory']"
2732077,Gaussian-trigonometric definite integral $\int_0^\infty \frac{e^{-x^2}}{1+a \cos x}dx$,"Is it possible to evaluate this integral in closed form? $$I(a)=\int_0^\infty \frac{e^{-x^2}}{1+a \cos x}dx$$ $$0<a<1$$ I encountered this integral when trying to find a closed form for the series from this question . Denominator doesn't have any zeroes on the real line, however it does have complex zeroes. I'm not sure how to use residues here, because the usual methods involve either a polynomial in the denominator with infinite limits, or trigonometric functions but with limits $\pm \pi/2$. Which is not the case here. Another way would be to expand the denominator as the Taylor series, but I don't know the closed form for $\int_0^\infty e^{-x^2} \cos^k x dx$ much less the resulting series. The function under the integral is even, so the limits can be extended to $\pm \infty$.","['gaussian-integral', 'integration', 'definite-integrals', 'contour-integration', 'trigonometric-integrals']"
2732098,"How to derive $L_t$ in $D_{t}=\exp\left(L_{t}-\frac{1}{2}\left\langle L,L\right\rangle_{t}\right)$","Here comes from a proof in Le Gall's Book Brownian Motion and Stochastic Calculus: Let $D$ be a continuous local martingale taking positive values. There exists a unique continuous local martingale $L$ such that 
  \begin{equation}
D_{t}=\exp\left(L_{t}-\frac{1}{2}\left\langle L,L\right\rangle_{t}\right)=\mathcal{E}(L)_{t}.
\end{equation}
  Moreover, $L$ is given by the formula
  \begin{equation}
L_{t}=\log D_{0}+\int_{0}^{t}\frac{1}{D_{s}}\text{d}D_{s}.
\end{equation} In the proof, we can apply Ito's formula to $\log D_{t}$, and we get
\begin{equation}
\log D_{t}=\log D_{0}+\int_{0}^{t}\frac{\text{d}D_{s}}{D_{s}}-\frac{1}{2}\frac{\text{d}\left\langle D,D\right\rangle_{s}}{D_{s}^{2}}=L_{t}-\frac{1}{2}\left\langle L,L\right\rangle_{t}
\end{equation}
My question is, how can we directly set $L_{t}=\log D_{0}+\int_{0}^{t}\frac{\text{d}D_{s}}{D_{s}}$, and why $\frac{1}{2}\frac{\text{d}\left\langle D,D\right\rangle_{s}}{D_{s}^{2}}=\frac{1}{2}\left\langle L,L\right\rangle_{t}$
Please show me the calculation,thanks!","['stochastic-processes', 'probability-theory', 'stochastic-calculus']"
2732103,"For compass and straightedge problems, are you allowed to use the compass as a ruler?","For compass and straightedge problems, you could have a line between two points A and B, and want to make a line the same size between C and line DE. If you placed the two points of the compass between A and B, and made a circle around C with the same radius, that would achieve this result. But is this something you are allowed to do?","['euclidean-geometry', 'geometric-construction', 'geometry']"
2732220,On the series $\sum \limits_{n=1}^{\infty} \frac{\sin (n \pi y) \sin \left ( n \pi x \right )}{n^2 \pi^2}$,"A friend of mine asked me to help him evaluate the series $$\mathcal{S} = \sum_{n=1}^{\infty} \frac{\sin (n \pi y) \sin \left ( n \pi x \right )}{n^2 \pi^2} \quad , \quad x , y \in (0, 1)$$ It does not ring any bells as to what it could be behind. The only thing I see is Fourier series and probably a dilogarithm function. But this is as far as I can see.. I cannot see to collect the pieces together. I would like to help him and I am asking your help. Is there any closed form in terms of special function for this series?","['special-functions', 'real-analysis', 'sequences-and-series']"
2732238,Double limit in the proof of differentiability,"A sufficient condition for differentiability of a function $f$ about a point $(x_0,y_0)$ is that the partial derivatives exist in a neighborhood of the point and they are continuous at the point. To prove this, Thomas calculus uses the linearization definition of differentiable function  namely $f(x_0+h,y_0+k)=f(x_0,y_0)+hf_x(x_0,y_0)+kf_y(x_0,y_0)+h\epsilon_1+k\epsilon_2$ where $\epsilon_1\to0$ and $\epsilon_2\to0$ as $(h,k)\to(0,0)$.
(See appendix 7 in 11th edition for the proof) My question is on one step used in this proof . The proof they use is given below: $f(x_0+h,y_0+k)-f(x_0,y_0)=f(x_0+h,y_0+k)-f(x_0+h,y_0)\ \ +f(x_0+h,y_0)-f(x_0,y_0)$. Now the function $G$ defined on the interval with end points $y_0,y_0+k$ as $G(y)=f(x_0+h,y)$ is differentiable and hence continuous with derivative $G'(y)=f_y(x_0+h,y)$. By applying the mean value theorem on $G$, we obtain a value $d$ between $y_0$ and $y_0+k$ at which $G(y_0+k)-G(y_0)=k G'(d)$ or $f(x_0+h,y_0+k)-f(x_0+h,y_0)=k f_y(x_0+h,d)$. Similarly the function $F(x)=f(x,y_0)$ is differentiable and hence continuous in an interval with end points $x_0,x_0+h$ and $F'(x)=f_x(x,y_0)$. Thus by appling MVT on $F$, we obtain a point $c$ between $x_0$ and $x_0+h$ at which $F(x_0+h)-F(x_0)=h F'(c)$ or $f(x_0+h,y_0)-f(x_0,y_0)=h f_x(c,y_0)$ The next is the part that is not clear to me. Now, as both $h$ and $k\to0$, we know that $c\to x_0$ and $d\to y_0$. Since $f_x$ and $f_y$ are continuous at $(x_0,y_0)$, the quantities $\epsilon_1=f_x(c,y_0)-f_x(x_0,y_0)$ and $\epsilon_2=f_y(x_0+h,d)-f_y(x_0,y_0)$ both approach zero as $h$ and $k\to0$. Once we get $\epsilon_1\to0$ and $\epsilon_2\to0$ as $(h,k)\to(0,0)$, we are through. It is obvious that $\epsilon_1$ doesn't depend $k$ so that $\epsilon_1\to0$ as $(h,k)\to(0,0)$. But $\epsilon_2$ depends on both $h$ and $k$. What we have directly is the iterated limit $\lim_{h\to0}\lim_{k\to0}\epsilon_2$. I am not quite convinced that the double limit $\boldsymbol{\lim_{(h,k)\to(0,0)}\epsilon_2}$ exists . Don't we need to show that it exists and the limit is indeed 0? Isn't the proof in the text a little sloppy there?
Any kind of help is appreciated. Thank you.","['derivatives', 'limits']"
2732246,"Is the zero function a differentiable, continuous and polynomial function?","For $f(x): \mathbb{R}\rightarrow\mathbb{R}$ Differentiable, as the derivative will always be 0 Continuous, as it is just a horizontal line with no breaks Polynomial, as it can be written as $f(x) = 0 = (1x^n - 1x^n)$ Would any of my reasoning be wrong?","['derivatives', 'continuity', 'polynomials', 'calculus']"
2732260,What are the countable groups for which each complex representation is semisimple?,Each complex representation of a finite group is semisimple (i.e. decomposes into a direct sum of irreducible complex representations). Question : What are the countable groups satisfying the same property? Remark : We do not restrict to finite dimensional representations.,"['finite-groups', 'representation-theory', 'group-theory']"
2732288,Find the limit of sequence $\frac{1}{2}+\frac{3}{2^{2}}+\frac{5}{2^{3}}+\cdots+\frac{2n-1}{2^{n}}$ without using of derivatives and etc.,"I need to find limit of sequence
$$
\lim_{n \to \infty }\left(\frac{1}{2}+\frac{3}{2^{2}}+\frac{5}{2^{3}}+\cdots+\frac{2n-1}{2^{n}}\right)
$$
I tried to solve it and stopped here
$$
f(n+1) = \frac{1}{2}+\frac{3}{2^{2}}+\frac{5}{2^{3}}+\cdots+\frac{2n-1}{2^{n}}+\frac{2n+1}{2^{n+1}}
$$
$$
2f(n+1) = 1+\frac{3}{2}+\frac{5}{2^{2}}+\cdots+\frac{2n-1}{2^{n-1}}+\frac{2n+1}{2^{n}}
$$
$$
2f(n+1) -f(n) = 1+ \left(1 + \frac{1}{2}+\frac{1}{2^2}+\frac{1}{2^3}+\cdots+\frac{1}{2^{n-1}}\right) = 1 + g(n)
$$
I can find the limit of $g$, but what to do with the other parts?","['sequences-and-series', 'limits']"
2732293,Twice contracted Bianchi identity from diffeomorphism invariance,"I've been reading Straumann's book ""General Relativity & Relativistic Astrophysics"". In it, he claims that the twice contracted Bianchi identity: $$\nabla_{\mu}G^{\mu\nu}=0$$ (where $G^{\mu\nu}=R^{\mu\nu}-\frac{1}{2}g^{\mu\nu}R$) is a consequence of the diffeomorphism (diff) invariance of the Einstein-Hilbert (EH) action. Now, I can show (I think) that the EH action is diff invariant, by considering a infinitesimal diff, generated by a vector field $X$: $$\delta_{X}S_{EH}=\phi^{\ast}S_{EH}-S_{EH}=\int_{M}\mathcal{L}_{X}\left(d^{4}x\sqrt{-g}R\right)=\int_{M}\left[\mathcal{L}_{X}\left(d^{4}x\sqrt{-g}\right)R +d^{4}x\sqrt{-g}\mathcal{L}_{X}\left(R\right)\right]\\ \qquad\qquad\qquad\quad\;\;=\int_{M}d^{4}x\sqrt{-g}\left[\nabla_{\mu}X^{\mu}R+X^{\mu}\nabla_{\mu}R\right]=\int_{M}d^{4}x\sqrt{-g}\,\nabla_{\mu}\left(X^{\mu}R\right)\\ =\int_{\partial M}d^{3}x\sqrt{h}\,n_{\mu}X^{\mu}R=0\;\;\qquad\qquad\qquad\quad$$ where $h_{ij}$ is the induced metric on the boundary $\partial M$ of the manifold $M$, with $n^{\mu}$ the normal vector to the boundary. The last equality follows upon the assumption that $X^{\mu}$ has compact support in $M$. However, I'm unsure how one uses this fact to derive the (twice-contracted) Bianchi identity? Straumann simply writes: $$\delta S=\int_{M}d^{4}x\sqrt{-g}\left(\frac{1}{\sqrt{-g}}\frac{\delta S_{EH}}{\delta g^{\mu\nu}}\right)\delta g^{\mu\nu}=\int_{M}d^{4}x\sqrt{-g}\,G_{\mu\nu}\delta g^{\mu\nu}=-\int_{M}d^{4}x\sqrt{-g}\,G^{\mu\nu}\delta g_{\mu\nu}$$ and notes that for an infinitesimal diff (generated by some vector field $X$), $\delta g_{\mu\nu}=2\nabla_{(\mu}X_{\nu)}$, such that $$\delta_{X} S=-2\int_{M}d^{4}x\sqrt{-g}\,G^{\mu\nu}\nabla_{\mu}X_{\nu}=2\int_{M}d^{4}x\sqrt{-g}\,X_{\nu}\nabla_{\mu}G^{\mu\nu}=0$$ and so, since $X^{\nu}$ is arbitrary, it must be that $\nabla_{\mu}G^{\mu\nu}=0$. What confuses me about this, is that one neglects the effect of the Lie derivative on $d^{4}x$ in this case (in the proof that the EH action is diff invariant, it was taken into account). Is the point that an infinitesimal diff is carried out in the same coordinate chart, and so $d^{4}x$ doesn't change in this case?","['lie-derivative', 'diffeomorphism', 'differential-geometry', 'general-relativity']"
2732304,Quasi-coherent $\mathcal{O}_X$-modules,"Suppose $X$ is a separated scheme, $F$ an abelian sheaf on $X$ in the Zariski topology. In order to assign an $\mathcal{O}_X$-module structure on $F$, is it enough to do it on a single open affine cover of $X$, compatibly with pairwise intersections? Or is it necessary to assign the $\mathcal{O}_X$-module structure on every affine open of $X$, compatibly with restriction maps?","['algebraic-geometry', 'homological-algebra', 'arithmetic-geometry', 'commutative-algebra', 'geometry']"
2732340,Benefit of studying series,I think one reason for studying series is for power series. Are there any other application for studying series of numbers? Please let me know if you have any idea or comment for it. Thanks in advance!,"['real-analysis', 'calculus', 'number-theory', 'soft-question', 'sequences-and-series']"
2732343,What does $2|x_1=u$ mean here?,"In the Wikipedia article https://en.wikipedia.org/wiki/Wieferich_prime under the section ""Connection with other equations"" I came across the equation $$2|x_1=u$$ The explanation seems to indicate that $2|x_1$ is the valuation of the lowest solution with respect to $2$. Did I understand this correctly ?","['number-theory', 'prime-numbers', 'notation', 'elementary-number-theory']"
2732366,The Galois group is $\mathbb{Z_{4}}$ if and only if $\frac{\alpha}{\beta}-\frac{\beta}{\alpha}\in\mathbb{Q}$,"This question is from Lang's Algebra Chapter VI Exercise Q8 Let $f(x)=x^4+ax^2+b$ be an irreducible polynomial over $\mathbb{Q}$, with roots $\pm\alpha$, $\pm\beta$ and splitting field $K$. I have shown that the Galois Group is either $\mathbb{Z_{4}}$ or $\mathbb{Z_{2}}\times\mathbb{Z_{2}}$ or $D_{8}$ The second part of this question asks me to show the Galois group is  $\mathbb{Z_{4}}$ if and only if $\frac{\alpha}{\beta}-\frac{\beta}{\alpha}\in\mathbb{Q}$ I have also shown that if the Galois group is $\mathbb{Z_{4}}$, then $\frac{\alpha}{\beta}-\frac{\beta}{\alpha}\in\mathbb{Q}$. However, I don't know how to show the opposite. I have tried that since there are only four possibilities of $\sigma(\alpha)$ and four possiblities for $\sigma(\beta)$, by $\sigma(\frac{\alpha}{\beta}-\frac{\beta}{\alpha})=\frac{\alpha}{\beta}-\frac{\beta}{\alpha}$ we can only have following three possibilities: 1) $\sigma(\alpha)=-\alpha$, $\sigma(\beta)=-\beta$ 2) $\sigma(\alpha)=\beta$, $\sigma(\beta)=-\alpha$ 3) $\sigma(\alpha)=-\beta$, $\sigma(\beta)=\alpha$ Clearly, the second case is what we want, but I don't know how to get rid of the case 1) and case 3). There is a similar post here Galois group of a biquadratic quartic , but I don't know how to connect this post to my question. Any hints or explanations are really appreciated!! EDIT 1: By the hints from Jyrki, I have shown that the case 1) and case 3) are actually the power of case 2), case 1) is the second power, and the case 3) is the third power. Moroever, in case 2), the order of $\sigma$ is $4$ so that we have the cyclic group of order $4$ Then, I found I skipped one step here. I have not shown that the extension degree is four. I think, to show this, I have to show that $K(\alpha, \beta)=K(\alpha)$ so that since $f(x)$ is irreducible with $\alpha$ being a root, the degree is $4$. However, I have no idea how to show this right now. Any ideas? Thank you! EDIT 2: The answer from Jyrki is definitely right, the reason of this edition here is to add one more little point in the whole proof. After all the arguments, we could only say that the Galois group has a cyclic subgroup, in our case, say $<\sigma_{2}>$ of order four, but it cannot conclude that the Galois group is $<\sigma_{2}>\cong \mathbb{Z_{4}}$. The reason behind is that the automorphism $\sigma_{2}$ was picked from the Galois group, and without knowing the degree of the splitting field over $\mathbb{Q}$, we cannot conclude the size of the Galois Group. In other words, if the splitting field is of degree $8$ over $\mathbb{Q}$, we could still get $<\sigma_{2}>$, but it is the subgroup of the Galois Group $\cong D_{8}$ Thus, the degree here is important. The splitting field is $\mathbb{Q}(\alpha,\beta)$, so the degree is $4$ if and only if $\mathbb{Q}(\alpha,\beta)=\mathbb{Q}(\alpha)$, since if you think about the intermediate field $\mathbb{Q}(\alpha)$ between $\mathbb{Q}(\alpha,\beta)$ and $\mathbb{Q}$, the degree between $\mathbb{Q}(\alpha)$ and $\mathbb{Q}(\alpha,\beta)$ can only be $1$ or $2$ (cannot be $3$ as the polynomial cannot only have one rational root, cannot be $4$ as otherwise the Galois group is of order $16$, which not divides $24=|S_{4}|$, but the Galois group should be a subgroup of $S_{4}$, as $f(x)$ irreducible). Then, if the degree between $\mathbb{Q}(\alpha)$ and $\mathbb{Q}(\alpha,\beta)$ is $2$, we get the Galois group to be $D_{8}$, if it is $1$, we get it to be $\mathbb{Z_{4}}$ or $\mathbb{Z_{2}\times Z_{2}}$. In current case it is $\mathbb{Z_{4}}$, but the point is that if the degree is $1$, $\mathbb{Q}(\alpha,\beta)=\mathbb{Q}(\alpha)$ (The ""only if"" part is really similar). Therefore, to prove the degree is $4$, we need to show $\mathbb{Q}(\alpha,\beta)=\mathbb{Q}(\alpha)$. To show this, we need to show that $\alpha$ generates $\beta$. First note that, since $\pm\alpha$, $\pm\beta$ are four roots of $f(x)=x^4+ax^2+b$. We could factor $f(x)$ into $f(x)=(x-\alpha)(x+\alpha)(x-\beta)(x+\beta)$, but it will finally give us $f(x)=x^{4}-(\alpha^{2}+\beta^{2})x^{2}+\alpha^{2} \beta^{2}$. Since $f(x)\in\mathbb{Q}[x]$, $-(\alpha^{2}+\beta^{2})=-\alpha^{2}-\beta^{2}\in\mathbb{Q}$. Thus, $-\alpha^{2}-\beta^{2}=d$ for some $d\in\mathbb{Q}$. Thus, $\beta^{2}=-d-\alpha^{2}$. Then, $\frac{\alpha}{\beta}-\frac{\beta}{\alpha}=\frac{\alpha^{2}-\beta^{2}}{\alpha \beta}=\frac{2\alpha^{2}+d}{\alpha \beta}$. Since $\frac{\alpha}{\beta}-\frac{\beta}{\alpha}\in\mathbb{Q}$, then $\frac{2\alpha^{2}+d}{\alpha \beta}\in\mathbb{Q}$, and thus $\frac{2\alpha^{2}+d}{\alpha \beta}=c$ for some $c\in\mathbb{Q}$. Thus, $\frac{2\alpha^{2}+d}{c\alpha}=\beta$. Thus, $\mathbb{Q}(\alpha,\beta)=\mathbb{Q}(\alpha)$, since $f(x)$ is irreducible of order $4$ with $\alpha$ as a root over $\mathbb{Q}$, then $\mathbb{Q}(\alpha,\beta)=\mathbb{Q}(\alpha)$ is of degree $4$ over $\mathbb{Q}$. Now, we finish the whole proof.","['abstract-algebra', 'galois-theory', 'field-theory']"
2732416,Why do we need to define Lebesgue spaces using equivalence classes?,"When we define an $L^{p}$ space for  $1\leq p \leq \infty$, we say elements of this space are equivalence classes of functions which are equal almost everywhere and $$ \int|f|^{p} dx < \infty $$ Why can we not say elements are functions which satisfy $ \int|f|^{p} < \infty $ ? I understand that if $g=f$  a.e. then $ ||f||_{L^{p}} = ||g||_{L^{p}} $ is this the reason for it? EDIT : The reason for asking is because I am studying an optimal control of PDEs course which says we need to be careful when considering the PDE : $ -\Delta y = f $ on $ \Omega  $ $ y=0 $ on $ \partial \Omega  $ ...since we need to define what it means for $ y=0 $ on $\partial\Omega$,  since  $\partial\Omega$ has zero measure.","['functional-analysis', 'almost-everywhere', 'equivalence-relations']"
2732439,Borel/cylindric sigma algebra of Stochastic Process,"A stochastic process is a collection $(X_t)_{t\in T}$ of random variables from a prob. space $(\Omega,\mathcal{F},P)$ to some measurable space $(E,\mathcal{E})$. Now, in order to understand the whole process as a 'single' random variable one look at it as a map $X:\Omega\to E^T$, where $E^T=\prod_{t\in T}E$ is the set of all functions from $T$ to $E$. $E^T$ is usually equipped with the product sigma algebra $\mathcal{E}^T$ which is produced by the one-dimensional cylinder sets, i.e. $\mathcal{E}^T=\sigma(\pi^{-^1}(A): A\in\mathcal{E}, t\in T)$ (is is the same as the sigma algebra produced by all finite-dimensional cylinders?), where $\pi_t$ is the projection map from $E^T$ to $E$. Now, if we are interested just in a subspace $S\subset E^T$, then it is often the case that $S\notin\mathcal{E}^T$. For example take $T=[0,1]$, $E=\mathbb{R}$, $\mathcal{E}=\mathcal{B}_{\mathbb{R}}$ and $C[0,1]$ as a subspace of $\mathbb{R}^{[0,1]}$, then $C[0,1]\notin\mathcal{E}^T$. By looking at $\mathcal{S}=S\cap \mathcal{E}^T=\{S\cap A:A\in\mathcal{E}^T\}$ one can avoid this problem. Now my main question: If our subspace $S$ is a metric space it is natural to look at the Borel $\sigma$-algebra $\mathcal{B}_S$. Is it always the case that $\mathcal{S}=\mathcal{B}_S$? I think that we need some additional property like separability, but I cannot prove it.","['stochastic-processes', 'probability-theory', 'measure-theory', 'stochastic-analysis']"
2732442,Differential equation $x''+a(t)x'+b(t)x=0$ and wronskian,"Let this differential equation : $$(H)\qquad x''+tx'-(1+t)x=0$$ As $\phi_a=e^t$ , is a solution of $(H)$ , we can deduce $\phi_b$ from the wronskian $W(t)=\begin{array}{|ll|}\phi_a(t)&\phi_b(t) \\ \phi_a'(t) & \phi_b'(t)\end{array}$ Thus $\left(\dfrac{\phi_b(t)}{\phi_a(t)}\right)'=\dfrac{W(t)}{(\phi_a(t))^2}$ As $W(t)$ is a solution of this equation $y'(t)=-t\cdot y(t)$ we have got $\displaystyle W(t)=\exp\bigg[-\int_{t_0}^t s\cdot ds\bigg]=e^{-t^2/2+t_0^2/2}=\lambda e^{-t^2/2}\qquad \lambda :=e^{t_0^2/2}$ So $\left(\dfrac{\phi_b(t)}{\phi_a(t)}\right)'=\dfrac{\lambda e^{-t^2/2}}{e^{2t}}=\lambda\cdot e^{-t^2/2-2t}$ And now I'am blocked, because if $\displaystyle \gamma(t)=\int\lambda\cdot e^{-t^2/2-2t}\cdot dt$ , we have got $\dfrac{\phi_b(t)}{\phi_a(t)}=\gamma(t)$ So $\phi_b(t)=\phi_a(t)\cdot \gamma(t)$ , how to calculate $\gamma(t)$ ???",['ordinary-differential-equations']
2732445,Cohomology with support and Poincare duality,"Say we have hypersurface $D \subset \mathbb{P}^2$. I ran into the following sequence of isomorphisms, justified only with ""by Poincare duality formulated algebro-geometrically"":
$$
H^2(\mathbb{P}^2-D,\mathbb{Q}) \cong H^3_D(\mathbb{P}^2,\mathbb{Q}) \cong H_1(D,\mathbb{Q})
$$
I looked up cohomology with supports, and I could only find something like $H^k_c(X) \cong H^{n-k}(X)^*$. Could you help me with either a reference or an explenation of why the above hold? Thank you.",['algebraic-geometry']
2732451,Monty Hall Variant Game,"There are three doors, behind each is either a car, or a goat.  Unlike the original Monty Hall problem, there are 8, equally likely possibilites for the setup of the doors.  The possibility sample space can be represented by all possible 3-element long sequences using a 0 (a goat), or a 1 (a car).  The sequence 011 represents a goat in the left door, and a car in the remaining two.  Also unlike the original problem, Monty will randomly choose one of the two doors you did not pick.  If Monty opens a door with a car, you can not get the car from that door.  This means that there are possibilities (besides 000 ), where you can not win anything.  There is also the possibility of 111 , where you have a 100% chance of winning a car. The original door you pick is always the left one.  I took the liberty of creating a table with all the possible outcomes, given which door is opened by Monty. In the picture, D m , and D r represent Monty opening the middle and right doors, repectively.  Depending on what is behind the door Monty opens, 1/2 of the sequences are elimated from the sample space of door setups you have.  However, all remaining sequences are still equally likely. My question is this: what is the probability of you winning any giving game?  Does it boil down to 50% Also, if you played repeately, what is the expected value for the number of cars you are expected to win in any given game? What is the value for the number of games you have to play to have ~100% of winning one game?  8?","['statistics', 'monty-hall', 'probability', 'sequences-and-series', 'discrete-mathematics']"
2732483,Problem on normed space.,"Let $X$ be a normed space, $Y$ a dense subspace of $X$ and $Z$ a closed finite-codimensional subspace of $X$. Is $Z\cap Y $dense in $Z$ ? I have no idea how to solve this problem. I am using this website for the first time, any help would be appreciated.","['functional-analysis', 'general-topology', 'topological-vector-spaces', 'analysis']"
2732491,Preimage of the Real Line,"Prove that if $f$ is an analytic function on an open subset $U$ of $\mathbb{C}$
then the non empty preimage of $\mathbb{R}$ under $f$ cannot be compact. I have been trying this problem for a long time now but to no avail. I was trying to prove this by assuming the contrapositive and then showing that $f$ is constant. Can anyone please help me with this? Thanks for any help.","['complex-analysis', 'analytic-functions', 'compactness']"
2732495,Proving that $a_{n+1}=a^{2}_{n}+\frac{1}{4}$ diverges to $+\infty$,"This problem looks very interesting to me. It is a requirement to show that if $$a_{n+1}=a_{n}^{2}+\dfrac{1}{4},\;n=1,2,3,\cdots\;\;\text{and}\;\;a_1=a,\;\;\text{then}$$ Case 1. $$\lim\limits_{n\to \infty}a_n=\dfrac{1}{2}\;\;\text{if}\;\;0<a\leq \dfrac{1}{2}.$$ Case 2. $$\lim\limits_{n\to \infty}a_n=\infty\;\;\text{if}\;\;a>\dfrac{1}{2}.$$ Proof of Case 1. To prove 1, we need to show two things (i) monotonicity (ii) boundedness (i) Monotonicity: $$a_{n+1}-a_{n}=a_{n}^{2}+\dfrac{1}{4}-a_{n}=\left(a_{n}-\dfrac{1}{2}\right)^{2}\geq 0,$$ $$\implies a_{n}\leq a_{n+1}\;\;\forall\;\; n\geq 1.$$ Hence, $\{a_{n}\}$ is a monotone increasing sequence. (ii) Boundedness: Claim. $a_{n}\leq \dfrac{1}{2}\;\;\forall\;\; n\geq 1$ Proof of claim. The statement is true when $n=1$ , since $$a_{2}=a_{1}^{2}+\dfrac{1}{4}=a^{2}+\dfrac{1}{4}\leq \dfrac{1}{4}+\dfrac{1}{4}=\dfrac{1}{2}.$$ Assume it is true for $n=k$ , that is $$ a_{k+1}=a_{k}^{2}+\dfrac{1}{4}\;\;\text{for some}\;\; k\in N$$ Now, we prove it is true for $n=k+1$ , i.e., $$ a_{k+2}=a_{k+1}^{2}+\dfrac{1}{4}=\left(a_{k}^{2}+\dfrac{1}{4}\right)^2+\dfrac{1}{4}\leq \dfrac{1}{2}.$$ Thus, by mathematical induction, it is true for all cases of $k\in N.$ Since it is monotone increasing and bounded above, it converges. The limit is easy to find. Let $$\lim\limits_{n\to \infty}a_n=x,$$ then $$x=x^{2}+\dfrac{1}{4}$$ $$\implies x=\dfrac{1}{2} \;\;(\text{twice}).$$ Thus, it is bounded by $1/2.$ Now, I have some issues with Case 2 but I'll give a useful definition. Definition. A sequence $\{a_n\}$ is said to diverge to $+\infty$ if for any $M\in R,\;\exists\; n_M\in N\;$ such that $$a_n> M\;\;\forall \;\;n\geq n_M.$$ My question is; how do I prove Case 2? If there are better ways of solving Case 1, I'll gladly welcome it. Corrections too, are welcome!","['real-analysis', 'proof-verification', 'algebra-precalculus', 'proof-writing', 'convergence-divergence']"
2732503,Generating good data for linear regression,"I would like to generate a good big data set for a linear regression exercise for my students. We will try to explain the salary (continuous variable) of 50 people in terms of their results high school results (/20, we suppose that those are continuous) in 5 subjects, for example maths, physics, chemistry, English, German. What I mean by a good data set is: -It satisfies usual conditions for linear regression -It has some logic to it, that is if you're good in maths, you're likely to be good in physics , and not bad in chemistry. If you're extremely good in a few subjects, then you'll unlikely fail an exam (<10/20). -Not obvious, so students can see the importance of the linear regression. What I mean is that if I generate 1 vector of salary, 5 vectors of results, and for each vector, order it from the smallest value, to the highest and put all of them in a matrix, we don't really need linear regression to see a pattern, it is too obvious. Does a tool exists for such generation ? How can one proceed ? Preferably in R or matlab.","['regression', 'statistics', 'probability', 'statistical-inference']"
2732546,"If $F(x) = \int_0^x e^{\sin(t)} dt$ , so $F'(0)$ is equal to:","If $F(x) = \int_0^x e^{\sin(t)} dt$ , so $F'(0)$ is equal to: (A) $-2$  $\quad$  (B) $-1$  $\quad$  (C)  $0$  $\quad$ (D)  $\mathbf1$ $\quad$  (E)  $2$ My attempt : $$\frac{d}{dt}F(x) = \frac{d}{dt}\int_0^x e^{\sin(t)} dt\Rightarrow F'(x) = \int_0^x \frac{d}{dt}e^{\sin(t)} dt$$ $$F'(x) = e^{\sin(t)}|_{0}^{x} = e^{x} - 1 \implies F'(0) = e^0-1 = 0$$ (wrong answer) Where am I missing?","['derivatives', 'definite-integrals', 'calculus']"
2732562,Ternary strings with 3 or more consecutive 2's,"I'm trying to figure out how to find the number of ternary strings of length $n$ that have 3 or more consecutive 2's.
So far I've been able to establish that there is $n(2^{n-1})$ with a single 2.
And I think (but am not certain) that this can be extrapolated to the number of strings with a single group of 2's of length $x$ by:
$$\bigl(n-(x-1)\bigr)(2^{n-x})$$
What I'm getting caught on is the 'or more part', any help would be greatly appreciated.",['combinatorics']
2732600,Can Pythagoras Theorem be used to average a set of values for Comparison.,"My basic math training tells me that you can apply Pythagoras theorem in N-Dimensional space allowing you to measure the size of a vector from The origin to a point. If you do this for multiple data points you can compare the length of the vector, to work out which one is larger. Is this a form of averaging? Am I missing something from a Mathematical theory standpoint or is it actually commonplace? Are there benefits of using this approach vs. Geometric Means or Arithmetic Means? You can assume this is for Positive Numbers only.","['means', 'statistics', 'geometry']"
2732656,Find the number of ways so that each boy is adjacent to at most one girl.,"There are $7$ boys and $3$ girls who need to be lined up in a row. Find the number of ways so that each boy is adjacent to at most one girl. In simple terms the situation demands that any distribution of the type $$...GBG...$$ must not come into play. First of all the total number of arrangements are $10!$ and we can actually find a complement of those situations which we don't want. In order to calculate the number of ways in which the wrong position can be true, I considered $GBG$ to be kind of a single package.
The number of ways to make this package are:$${7 \choose 1} \cdot {3 \choose 2} \cdot 2!$$, now considering this package and $6$ boys plus the $1$ girl left,
we can permute them all in 8! ways [$6$ boys, $1$ girl and our ""package""], thus making the total to be 
$$ {7 \choose 1} \cdot {3 \choose 2} \cdot 2! \cdot 8! \tag{1}$$ Things seem to be tractable hithero, but as I was writing this questions I saw one problem in my argument: The cases containing the configurations $GBGBG$ have been possible counted several times thus $(1)$ is not giving the correct number of ways to be subtracted. Can we anyhow make some changes in this approach and find the solution?","['permutations', 'combinatorics', 'inclusion-exclusion', 'combinations']"
2732687,"Taking the derivative of $x^4\sin(x)\cos(x)$, which step is wrong?","I'm trying to take the derivative of $x^4\sin(x)\cos(x)$ and I keep getting the wrong answer. My steps:
$$\frac {d}{dx}[x^4\sin(x)\cos(x)]$$
Apply product rule:
$$\frac {d}{dx}[x^4](\sin(x)\cos(x)+x^4\frac {d}{dx}[\sin(x)\cos(x)]$$
Simplify first part:
$$4x^3\sin(x)\cos(x)+x^4\frac {d}{dx}[\sin(x)\cos(x)]$$
Apply product rule to second part:
$$\cos(x)\cos(x)+(-\sin(x))$$
Add them all together: $$4x^3\sin(x)\cos(x)+x^4\cos^2(x)-\sin(x)$$ So something is wrong as the correct answer is $$-x^4\sin^2(x)+x^4\cos^2(x)+4x^3\cos(x)\sin(x)$$ Got the biggest headache from this one, would really appreciate help! Thanks!","['derivatives', 'calculus']"
2732711,$G$ is cyclic iff $f(H) = H$ for every automorphism $f$,"Let $(G,\cdot)$ be a finite group, $|G| = n \in \mathbb{N}, n \geq 2$ such that $n$ is not divisible with the cube of any prime number. Prove that $G$ is cyclic $\iff$ $\forall f \in Aut(G), f(H) = H$ , for every subgroup $H$ of $G$ . For the direct implication, I tried assuming the contrary and then try to use the fact that $G = <x>$ , but I couldn't solve the problem. For the inverse implication, using the inner automorphisms, we get that every subgroup $H$ of $G$ is normal. But I don't know how to use the fact that $n$ is not divisible with the cube of any prime number in order to prove that $G$ is cyclic.","['finite-groups', 'abstract-algebra', 'group-theory']"
2732712,Find the inflection points of $f(x)={1 +\ln^2 x \over x}$,"Given $$f(x)={1+\ln^2x\over x}, x>0$$ • Find the inflection points of $C_f.$ Personal work: In order to find the inflection points of $C_f$ we first need to find the second derivative and then at which "" $x$ "", $f''(x)=0.$ $${d\over dx}({1+\ln^2 x \over x})={2lnx-1-\ln^2x\over x^2}$$ $${d\over dx}({2lnx-1-\ln^2x\over x^2})=\cdots={2\ln^2-6\ln x+4 \over x^3}$$ or $${d\over dx}({d\over dx}({1+\ln^2 x \over x}))={2\ln^2-6\ln x+4 \over x^3}$$ I'm struggling on finding the sign of $f'(x)$ and the points that $f''(x)=0.$","['derivatives', 'calculus']"
2732725,Second fundamental theorem of calculus on the unit sphere,"Without being so sure, the second fundamental theorem of calculus can be written in the following form: Let $f \in {\cal C}^1(\mathbb{R}^n)$. Then, for all $x, y \in \mathbb{R}^n$, we have 
  \begin{align}
f(y) = f(x) + \int_0^1 \langle \nabla f(x + \tau(y-x)), y-x \rangle d \tau,
\end{align}
  where $\nabla f(x)$ denotes the gradient of $f(x)$. A use case of this formulation can be found in Nesterov's book (Introductory Lectures on Convex Programming) (Lemma 1.2.3). I was wondering if there is form of this equation for the case where $x$ and $y$ live on the unit sphere $\mathbb{S}^{n-1} \subset \mathbb{R}^n$. I am guessing that the formula would contain geodesics, but I couldn't find a solution online.",['differential-geometry']
2732749,"How to recover $k$ lost items in binary data $x_1,x_2,x_3 \dots,x_n$?","$$x_1,x_2,x_3,\dots,x_n$$  where they all are same size binary data. If we lose one of them in series and if we want it to recover,  We just need to store $y_1$ that is  same size like the items in series. $$y_1=x_1 \oplus x_2 \oplus x_3 \oplus ... \oplus x_n$$ where $\oplus$ is the XOR binary operator. Because $\oplus$ can be defined $$ x \oplus x =0 $$
$$ x \oplus 0 =x $$
$$ 0 \oplus x =x $$
$$ x \oplus y = y \oplus x  $$ $$ (x \oplus y) \oplus y =x \oplus (y \oplus y) = x \oplus 0 = x $$ Let's assume that we lost $x_1$. We just need to apply $n-1$ xor operations to recover $x_1$
$$y_1 \oplus  x_2 \oplus x_3 \oplus ... \oplus x_n =x_1$$ My question: if we lose k items in ($x_1,x_2,x_3,.....,x_n$ )  , How can the k lost items be recovered? What are the minimum number of spare items $y_1,y_2,..,y_m$ required to recover the k lost items? Which binary operators and algorithm should be used to recover the k lost  items in series? My attempt to solve 2 lost items :($k=2$) I do not know if they are minimum or not, I used $\oplus$ operator for now. I put my approach without proof below. for $n=3$;
We need minimum 2 store places. 
$$y_1=x_1 \oplus x_2 $$
$$y_2=x_2 \oplus x_3 $$ for $n=4$;
We need minimum 3 store places. 
$$y_1=x_1 \oplus x_2   $$
$$y_2=x_2 \oplus x_3  $$
$$y_3=x_4 \oplus x_1  $$ for $n=5$;
We need minimum 3 store places. 
$$y_1=x_1 \oplus x_2  \oplus x_3  $$
$$y_2=x_2 \oplus x_3  \oplus x_4  $$
$$y_3=x_3 \oplus x_4  \oplus x_5  $$ for $n=6$;
We need minimum 3 store places. 
$$y_1=x_1 \oplus x_2  \oplus x_3  $$
$$y_2=x_3 \oplus x_4  \oplus x_5  $$
$$y_3=x_5 \oplus x_6  \oplus x_1  $$ for $n=7$;
I have just found minimum 3 store places. 
$$y_1=x_1 \oplus x_2  \oplus x_3 \oplus x_4 $$
$$y_2=x_3 \oplus x_4  \oplus x_5 \oplus x_6 $$
$$y_3=x_6 \oplus x_7  \oplus x_1 \oplus x_3 $$ I believe if we can solve the problem for 2 lost items , It can be generalized for k lost items. The problem is also very related with combinatorics. I would like to get comments how to approach to the general problem . Thanks a lot for answers and comments EDIT: 04/25/2018 I would like to write my approach to solve the general problem (for any $n$ and $k$). As @Mike Earnest wrote in his answer , the general solution for $k=2$ can be found via erasure codes $k=2$ $n=2$
$$y_1=x_1$$ 
$$y_2=x_2$$ \begin{matrix}
&y_2&y_1\\1=&0&1&x_1\\2= &1&0&x_2 
\end{matrix} $n=3$
$$y_1=x_1\oplus x_3 $$ 
$$y_2=x_2 \oplus x_3$$ \begin{matrix}
&y_2&y_1\\1=&0&1&x_1\\2= &1&0&x_2 \\3= &1&1&x_3 
\end{matrix} $n=4$
$$y_1=x_1\oplus x_3 $$ 
$$y_2=x_2 \oplus x_3$$ 
$$y_3=x_4 $$ \begin{matrix}
&y_3&y_2&y_1\\1=&0&0&1&x_1\\2= &0&1&0&x_2 \\3= &0&1&1&x_3 \\4= &1&0&0&x_4 
\end{matrix} $n=5$
$$y_1=x_1\oplus x_3  \oplus x_5 $$ 
$$y_2=x_2 \oplus x_3$$ 
$$y_3=x_4 \oplus x_5 $$ \begin{matrix}
&y_3&y_2&y_1\\1=&0&0&1&x_1\\2= &0&1&0&x_2 \\3= &0&1&1&x_3 \\4= &1&0&0&x_4 \\5= &1&0&1&x_5 
\end{matrix} For $k=2$, This sequence goes linear and increase 1 for each new $x_i$. At least one bit always changes for 2 random selected inputs. I would like to extend this idea for higher k $k=3$ $n=3$, Minimum solution
$$y_1=x_1 $$ 
$$y_2=x_2 $$
$$y_3=x_3 $$ \begin{matrix}
&y_3&y_2&y_1\\1=&0&0&1&x_1\\2= &0&1&0&x_2 \\4= &1&0&0&x_3  
\end{matrix} $n=4$,  Minimum solution
$$y_1=x_1 \oplus x_4 $$ 
$$y_2=x_2 \oplus x_4 $$
$$y_3=x_3 \oplus x_4 $$ \begin{matrix}
&y_3&y_2&y_1\\1=&0&0&1&x_1\\2= &0&1&0&x_2 \\4= &1&0&0&x_3 \\7= &1&1&1&x_4 
\end{matrix} $k=4$ $n=4$, Minimum solution
$$y_1=x_1 $$ 
$$y_2=x_2 $$
$$y_3=x_3 $$
$$y_4=x_4 $$ \begin{matrix}
&y_4&y_3&y_2&y_1\\1=&0&0&0&1&x_1\\2= &0&0&1&0&x_2 \\4= &0&1&0&0&x_3 \\8= &1&0&0&0&x_4  
\end{matrix} $n=5$,  Minimum solution
$$y_1=x_1 \oplus x_5 $$ 
$$y_2=x_2 \oplus x_5 $$
$$y_3=x_3 \oplus x_5 $$
$$y_4=x_4 \oplus x_5 $$ \begin{matrix}
&y_4&y_3&y_2&y_1\\1=&0&0&0&1&x_1\\2= &0&0&1&0&x_2 \\4= &0&1&0&0&x_3 \\8= &1&0&0&0&x_4  \\15= &1&1&1&1&x_5 
\end{matrix} My Conjecture for general solution: I have noticed that If we continue the table series in the way I wrote below, they satisfy my request. They all may not be not minimum but they have not failed yet for any number when I tested them. \begin{matrix}
n=&1&2&3&4&5&6&7&8&9&10&...n \\
&-&-&-&-&-&-&-&-&-&-& \\
k=1 |&1&1&1&1&1&1&1&1&1&1&...A_1(n)=1 \\ k=2 |&1&2&3&4&5&6&7&8&9&10&...A_2(n)=n\\ k=3 |&1&2&4&7&11&16&22&29&37&46&...A_3(n)=\binom{n-1}{0}+\binom{n-1}{1}+\binom{n-1}{2}=\frac{n^2-n+2}{2} \\k=4 |&1&2&4&8&15&26&42&64&93&130&...A_4(n)=\binom{n-1}{0}+\binom{n-1}{1}+\binom{n-1}{2}+\binom{n-1}{3} \\k=5 |&1&2&4&8&16&31&57&99&163&256&... A_5(n)=\binom{n-1}{0}+\binom{n-1}{1}+\binom{n-1}{2}+\binom{n-1}{3}+\binom{n-1}{4}\\k=6 |&1&2&4&8&16&32&63&120&219&382&...A_6(n)=\binom{n-1}{0}+\binom{n-1}{1}+\binom{n-1}{2}+\binom{n-1}{3}+\binom{n-1}{4}+\binom{n-1}{5}  
\end{matrix} General formula for the table $A_k(n)$ for $n,k>0$ and 
$A_k(1)=1$ and $A_1(n)=1$
$$A_{k+1}(n+1) =A_{k+1}(n)+A_{k}(n)$$ $$A_k(n)=\sum_{i=0}^{k-1}\binom{n-1}{i}$$ Generating function of $A_k(n)$ : $$e^x\sum_{i=0}^{k-1}\frac{x^i}{i!}=\sum_{n=0}^{\infty} A_k(n)\frac{x^n}{n!}$$ Need to prove for all $A_k(n)$ or disprove for any $A_k(n)$ that does not satisfy the solution. Please help me prove  that my conjecture is a solution or not for general problem. An example:
I would like to give an example how to write solution for $n=10$, $k=6$ $$x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10}$$ We need to recover 6 terms in 10 inputs . \begin{matrix}
&y_9&y_8&y_7&y_6&y_5&y_4&y_3&y_2&y_1\\ 1=&0&0&0&0&0&0&0&0&1&x_1\\2= &0&0&0&0&0&0&0&1&0&x_2  \\4= &0&0&0&0&0&0&1&0&0&x_3\\8= &0&0&0&0&0&1&0&0&0&x_4  \\16= &0&0&0&0&1&0&0&0&0&x_5  \\32= &0&0&0&1&0&0&0&0&0&x_6  \\63= &0&0&0&1&1&1&1&1&1&x_7  \\120= &0&0&1&1&1&1&0&0&0&x_8  \\219= &0&1&1&0&1&1&0&1&1&x_9  \\382= &1&0&1&1&1&1&1&1&0&x_{10}
\end{matrix} If we write  xor equations from table. We need 9 spares. $$y_1=x_1 \oplus x_7 \oplus x_9 $$ 
$$y_2=x_2 \oplus x_7 \oplus x_9 \oplus x_{10} $$ 
$$y_3=x_3 \oplus x_7 \oplus x_{10} $$ 
$$y_4=x_4 \oplus x_7 \oplus x_8 \oplus x_9 \oplus x_{10} $$ 
$$y_5=x_5 \oplus x_7 \oplus x_8 \oplus x_9 \oplus x_{10} $$ 
$$y_6=x_6 \oplus x_7 \oplus x_8 \oplus x_{10} $$ 
$$y_7=x_8 \oplus x_9 \oplus x_{10} $$ 
$$y_8=x_9  $$ 
$$y_9= x_{10}  $$ Note: I do not claim that it is minimum solution. I just claim that it is a solution for problem. EDIT:(4/27/2018)
$$A_k(n) =\sum_{i=0}^{k-1}\binom{n-1}{i}$$
Required spare numbers (m) can be found
$$m=\log_2(A_k(n))+1$$ $n>>k$  and for very big $n$ 
$$A_k(n) \approx  \frac{n^{k-1}}{(k-1)!} $$ $$m\approx\log_2( \frac{n^{k-1}}{(k-1)!})+1$$
$$m\approx(k-1)log_2( n)+1-log_2((k-1)!)$$ $$k=2 ---> m\approx log_2( n)+1$$
$$k=3 ---> m\approx 2log_2( n)$$
$$k=4 ---> m\approx 3log_2( n)-log_2( 3!)+1$$
$$k=5 ---> m\approx 4log_2( n)-log_2( 4!)+1$$
$$k=6 ---> m\approx 5log_2( n)-log_2( 5!)+1$$","['boolean-algebra', 'combinatorics', 'coding-theory', 'binary-operations']"
2732765,Relation between homeomorphism and topological equivalence,"Please consider the following definitions: A: Suppose $d$ and $e$ are metrics on a set $X$. Then, $d$ and $e$ are topologically equivalent metrics if and only if the identity functions from $(X,d)$ to $(X,e)$ and from $(X,e)$ to $(X,d)$ are both continuous. B: Suppose $(X,d)$ and $(Y,e)$ are metric spaces. Then $X$ and $Y$ are said to be homeomorphic or topologically equivalent if and only if, there exists a bijective function $f:X \rightarrow Y$ that is continuous and has continuous inverse; such a function is called a homeomorphism. C : Suppose $d$ and $e$ are metrics on a set $X$. If $d$ and $e$ are topologically equivalent, then certainly $(X,d)$ and $(X,e)$ are homeomorphic, because the identity function $I_{d,e}:(X,d) ]\rightarrow (X,e)$ is continuous and has a continuous inverse $I_{e,d}:(X,e) \rightarrow (X,d)$. However, the converse need not be true In definition C , it is specifically mentioned that the converse need not be true. I assume, by the word : converse, the author implies : if $(X,d)$ and $(X,e)$ are homeomorphic, then,  $d$ and $e$ are topologically equivalent . I reason this as follows: there can exist bijective function $f:X \rightarrow Y$ that is continuous and has continuous inverse; but which is not the identity function. Hence, $d$ and $e$ need not be topologically equivalent even though $(X,d)$ and $(X,e)$ are homeomorphic. Is this correct? Thanks","['general-topology', 'real-analysis', 'metric-spaces']"
2732776,The integer part of $ \sum_{k=2}^{9999} \frac{1}{\sqrt{k}} $,"For $k>0 $ $ \frac{1}{2\cdot \sqrt{k+1}}<\sqrt{k+1}-\sqrt{k}<\frac{1}{2\cdot \sqrt{k}} $, the integer part of $ \sum_{k=2}^{9999} \frac{1}{\sqrt{k}} $ My attempt here was not complying with any of the options given as my attempt was to find the min. value for the series which the question does not ask. What to do? Options are: A. 198 B. 197 C. 196 D. 195 I know $ \sum_{k=1}^{n} \frac{1}{k} \approx log n $ But this does not help in this case. Also $ \sum_{k=2}^{9999} \frac{1}{\sqrt{k}} < 2(\sqrt{10000}-\sqrt{2}) $ This also does not help.","['inequality', 'sequences-and-series']"
2732778,Sub sub sequences and a relation between convergence in probability and a.s convergence,"I am trying to understand an answer to this question (specifically Siméon's answer) Convergence in probability of the product of two random variables Im struggling with the statement ""$X_{n}$ tends to X in probability if and only if , every subsequence of $X_{n}$ has a sub sub sequence that tends to $X$ a.s"" I felt like the above statement deserved a question in itself. If this is true I can see how the proof follows.","['probability', 'sequences-and-series', 'convergence-divergence']"
2732781,Continuity of potential function at an interior point,"Suppose $\Omega \subset \mathbb{R}^3$ is closed, bounded, with smooth boundary and the density $\rho: \Omega \to \mathbb{R}$ is continuous.  The potential function is $$\phi(x) = \int_\Omega \frac{\rho(x')}{|x - x'|} dx',$$ I already showed it continuous for $x \notin \Omega$ and am trying to show it continuous for $x \in \Omega$. My effort: I take a ball $B_\delta(x_0)$ around a fixed point $x_0$ with $|x-x_0| < \delta$ and look at $$|\phi(x) - \phi(x_0)| \leq \left|\int_{\Omega- B_\delta(x_0)}\rho(x') \left(\frac{1}{|x-x'|} - \frac{1}{|x_0-x'|}\right) dx'\right|+ \left|\int_{ B_\delta(x_0)}\frac{\rho(x') }{|x_0-x'|} dx' \right| + \left|\int_{ B_\delta(x_0)} \frac{\rho(x')}{|x-x'|} dx' \right|$$ Since the integrand in the first integral is continuous there exists $\delta_0$ such that if $|x - x_0| < \delta_0$ then $$\left|\int_{\Omega- B_\delta(x_0)}\rho(x') \left(\frac{1}{|x-x'|} - \frac{1}{|x_0-x'|}\right) dx'\right| < \frac{\epsilon}{3}$$ Also convergence of the potential integral means if $\delta$ is small enough then $$\left|\int_{ B_\delta(x_0)}\frac{\rho(x') }{|x_0-x'|} dx' \right| < \frac{\epsilon}{3}$$ My question is how to show the third integral can be made $< \frac{\epsilon}{3}$ since $B_\delta(x_0)$ is not centered at $x$.","['multivariable-calculus', 'potential-theory']"
2732816,The set of matrices of form $A^3+B^3$ with multiplication is a monoid,"Consider the set
  \begin{equation*}
M=\{A^3+B^3|A,B\in\mathcal{M}_n(\mathbb{C})\}
\end{equation*}
  for $n\geq 1$. Prove that $(M,\cdot\;)$ is a monoid. This is a problem I found in a section of ""extra problems"" in the romanian magazine Gazeta Matematica, given as a high school problem. As mentioned in the comment, we need to show that for any complex matrices $A, B, C, D$, one needs to write $$(A^3 + B^3)(C^3+D^3)$$ as $E^3 + F^3$ for some complex matrices $E, F$. Even in the case $n=1$, I can't algebraically find $e, f\in \mathbb C$ so that 
$$(a^3+ b^3)(c^3+d^3) = e^3+f^3$$
for any given $a, b, c, d\in \mathbb C$.","['matrices', 'abstract-algebra', 'monoid']"
2732819,"If $A \otimes B$ is nuclear , then is $B$ nuclear?","Suppose that $A$ and $B$ are unital $C^*$-algebras, and  that $A \otimes B$ is a nuclear $C^*$-algebra. I want to see if $B$ is nuclear or not. Since $A \otimes B$ is nuclear, there exist c.c.p maps $\phi_n: A\otimes B \to M_{k_n}(\mathbb{C})$ and $\psi_n: M_{k_n}(\mathbb{C}) \to A \otimes B$ such that $\psi_n \circ \phi_n \to \text{id}$ in the point norm topology. So I have the following maps: 
 $$B \xrightarrow[j]{b \to 1_A\otimes b}A \otimes B \xrightarrow{\phi_n}M_{k_n}(\mathbb{C})\xrightarrow{\psi_n}A\otimes B \xrightarrow[\pi]{a \otimes b \to b}B$$ Letting $\tilde{\phi_n}=\phi_n \circ j$ and $\tilde{\psi_n}=\pi \circ\psi_n$, we see that $\tilde{\psi_n}\circ \tilde{\phi_n}=\pi\circ(\psi_n \circ \phi_n)\circ j$ and this converges to $\text{id}$ in the point norm topology. The same thing can be done for $A$ as well. The only question I have is whether $\pi$ is well defined or not? If not, then is there a way of showing this? Or, is there an example of two non-nuclear $C^*$-algebras whose tensor product is nuclear? Thanks for the help!!","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras', 'analysis']"
2732826,Solving for $k$ in $\cos \frac{\pi}k - \cos \frac{2\pi}k = P$,"How do I solve trigonometric equations of the type $$\cos \frac{\pi}k - \cos \frac{2\pi}k = P$$ where P is a real constant within the range of the left side. (By solving, I mean finding the value of k) I could convert this into product of two $\sin$ functions but I don't think that will help. The value of P could indeed shed light on the value of k. For example: If $P = \large\frac{\sqrt 3 - 1}2$; A little guess work and you get $k=6$. But this guess work almost instantly fails for $\large\frac{\sqrt 3 + 1}2$. All help will be appreciated.",['trigonometry']
2732828,Expectation with time series model,"Consider the adapted ARCH model: $X_t^2 = \alpha_0 + \alpha_1X_{t-1}^2+\eta_t$, where $\eta_t=(\epsilon_t^2-1)(\alpha_0+\alpha_1X_{t-1}^2)$ and $\epsilon_t \sim \mathcal{N}(0,1)$. My question is, how can I calculate $\mathbb{E}(\eta_tX_{t-s}^2)?$ I have tried simplifying but can't get to the desired result which should be zero... e.g. $$ \mathbb{E}(\eta_tX^2_{t-s})=\mathbb{E}(\eta_t(\alpha_0 + \alpha_1X_{t-1-s}^2+\eta_{t-s}))=\mathbb{E}(((\epsilon_t^2-1)(\alpha_0+\alpha_1X_{t-1}^2))(\alpha_0 + \alpha_1X_{t-1-s}^2+\eta_{t-s}))$$ Not sure if i'm tackling this the wrong way, because this becomes quite messy, am i missing something ? Note I have calculated $\mathbb{E}(\eta_t)=0$.","['stochastic-processes', 'statistics', 'expectation', 'time-series']"
2732835,Evaluating $\cos^{-1}(\sin(-17))$,"I had this question on my test I took today, and I'm confused if my answer's right. I had to find the value of $$\cos^{-1}(\sin(-17))$$ Okay, first, I drew a triangle. And, after, I let a and b for each line related to sin -17, so that we can say sin (-17) is b/a. And then, I realized cos ^(-1)(b/a) is the other angle than -17 and the right angle. I said the answer was 73, subtracting 17 from 90, since I thought -17 was an way of expressing that the triangle is in the third or fourth quadrant, and the actual angle is 17. However, I heard these kids saying that the answer's 107, since the angles should add up to 90, and 107+(-17) =90. I'm simultaneously confused and frustrated. What's the answer? any help would be appreciated.",['trigonometry']
2732838,Stopped $L^2$ - martingale is again $L^2$ integrable,"Given a $L^2$ martingale $X_t$, for a finite stopping time $T$, is it true that $X_T$ is again $L^2$ integrable? If $X_t$ is additionally uniform integrable, then we can use Jensen-inequality:$$E[X_T^2]=E[E[X_\infty\mid F_T]^2]\leq E[X_\infty^2]$$ to obtain the $L^2$ integrability of $X_T$ provided $X_\infty$ is square integrable. However, I don't know if the statement is still true without the assumption on uniform integrability and $X_\infty^2$ is integrable.","['stochastic-processes', 'expectation', 'probability-theory', 'stopping-times', 'martingales']"
