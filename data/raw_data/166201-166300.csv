question_id,title,body,tags
2894982,Convergent piecewise-constant approximation of l.s.c function,"I have a lower semi-continuous (l.s.c) extended-valued function $f: \mathbb{R}^3 \to \mathbb{R} \cup \{+\infty\}$. I know the following properties: $\operatorname{dom}(f)$ is compact, and we have its bounding box $B$, $f$ is bounded, $f$ is piecewise-continuous - there exists a finite partition  $\operatorname{dom}(f) = \bigcup_{i=1}^m D_i$  such that $f$ is continuous on each $D_i$, and $\operatorname{cl}(D_i) = \operatorname{cl} (\operatorname{int} (D_i))$. For my purposes, I would like to approximate $f$ by partitioning $B$ into a regular grid of $\nu$ points along each axis, and defining my approximation $f_\nu$ to be constant in each grid box. My objective is to obtain convergence of $f_\nu$ to $f$ in some sense. My best-case scenario is having epi-graphical convergence, since I want to use the approximation in an optimization context. My question is: what value should my approximation have in each grid cell? I tried the function value at the center of each cell, but I was not able to prove epigraphical convergence. Restrictions: Computationally, I have an oracle which can evaluate $f$ at any given point. In particular, it is not possible to compute the infimum of $f$ on each grid cell. Just evaluate it at a finite number of points. I do not have any information about the discontinuity set, except for the knowledge that the finite partition above exists. Update I posted a proof, which is hopefully correct. Answers providing a simpler proof, or a reference to an existing article which already shows such a result are very welcome, and will receive the bounty.","['approximation', 'optimization', 'functional-analysis']"
2894991,Prove that $1 + \frac{2}{3!} + \frac{3}{5!} + \frac{4}{7!} + \dotsb = \frac{e}{2}$,"Prove that $1 + \frac{2}{3!} + \frac{3}{5!} + \frac{4}{7!} + \dotsb = \frac{e}{2}$. This is problem 4 from page 303 of S.L.Loney's 'Plane Trigonometry'. It seems fairly obvious that the series expansion $e^x$ will be used. However, I am unsure where to start. Should I consider other series?","['trigonometry', 'summation', 'sequences-and-series']"
2895033,"Prove that $x^2$ and $x^3$ are topological conjugated on $[0, \infty)$.","As in the topic. I have to prove that the functions $x ^ 2$ and $x ^ 3$ are topologically conjugated. I tried to write it out by definition: $f (x ^ 2) = f (x) ^ 3$ and choose $f (x) = x ^ a$, but unfortunately it doesn't work. It's my beginnings in this field, so I do not have much experience yet. Do you have any hints?","['dynamical-systems', 'real-analysis']"
2895044,How to minimize the cardinality of the intersection of two sets of pairs from a cyclic group?,"I was in a group of 10 persons and we wanted to make a know-each-other-better game, so we did the following thing: 5 of us were standing in the middle (inner circle) and the other five were standing in the front of a person from the inner circle (this was the outer circle). There was another person (not from this group) asking 5 the questions. After each question, the outer circle would have to shift to the right. After the first round, we wanted to mix up and start another one, but we observed that a lot of the pairs from the first round were also again in the second round. This made me think a bit at home. Given a group of $2n$ people, and two subsets of $n$ people from the original group, I want to know the answer to the following questions: With $n$ questions per round ($n$ shifts), how the people must re-arrange so that the cardinality of the intersection between the generated sets of pairs in the two rounds is minimum? Same question, but with $n - 1$ questions (shifts) per round I think this is a really interesting problem and of course, it can be generalized using cyclic groups and combinatorics and the goal would be to find a permutation/function, but I wasn't able to figure it out. Can some of you give me some ideas about this?
Thanks!","['permutations', 'number-theory', 'cyclic-groups', 'combinatorics', 'elementary-set-theory']"
2895070,Sum of roots of equation $x^4 - 2x^2 \sin^2(\displaystyle {\pi x}/2) +1 =0$ is,"My try:
$$x^4-2x^2\sin^2(\frac{\pi x}{2})+1=0\\x^4+1=2x^2\left (1-\cos^2\left(\frac{\pi x}{2}\right)\right )\\(x^2-1)^2=-2x^2\cos^2\left(\frac{\pi x}{2}\right)\\(x^2-1)^2+2x^2\cos^2\left(\frac{\pi x}{2}\right)=0\\x^2-1=0\,\text{and}\, 2x^2\cos^2\left(\frac{\pi x}{2}\right)=0$$ I am stuck , I am confused now what to do now","['algebra-precalculus', 'roots', 'trigonometry']"
2895124,$p$ is a homogeneous polynomial. Show that $p^{-1}(a)$ is a submanifold of $\mathbb{R}^n$ if $a\neq 0$. The submanifolds of $a>0$ are diffeomorphic.,"Let $p$ be a homogeneous polynomial of degree $m$ in $n$ variables $t_1,\cdots, t_n$. Show that for each $a\neq 0$, $p^{-1}(a)$ is a submanifold of codimension 1 in $\mathbb{R}^n$. Show that the submanifolds obtained with $a>0$ are all diffeomorfic, as well as those with $a<0$. Using Euler's identity one can see that, for each $t\in p^{-1}(a)$, 
$$\sum_{i=1}^n t_i\frac{\partial p}{\partial t_i} (t)=ma.$$
So, for some $j\in\{1,\cdots,n\}$, $\frac{\partial p}{\partial t_i} (t)\neq 0$. So $a$ is a regular value and, therefore, $p^{-1}(a)$ is a embedded submanifold of $\mathbb{R}^n$ of dimensions $n-1$. How do I show the second statement?","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
2895159,"Tao Analysis Vol I questions, ""logically equivalence"" problems","Questions from section A.1. (reading this book on my own / self-studying, not a student, this is not homework): Edit: Apparently I had misinterpreted some or/and stuff so I am redoing my questions now: Suppose that you have shown that whenever $X$ is true, then $Y$ is
  true, and whenever $X$ is false, $Y$ is false. Have you now
  demonstrated that $X$ and $Y$ are logically equivalent? Explain. I believe this is saying, $(X \rightarrow Y) \land (\lnot X \rightarrow \lnot Y)$ which reduces to $(X \land Y) \lor (\lnot X \land \lnot Y)$, and this is the definition of ""if and only if"", so yes, they are logically equivalent. Suppose that you have shown that whenever $X$ is true, then $Y$ is
  true, and whenever $Y$ is false, then $X$ is false. Have you now
  demonstrated that $X$ is true if and only if $Y$ is true? Explain. $(X \rightarrow Y) \land (\lnot Y \rightarrow \lnot X)$, which reduces to $\lnot X \lor Y$, or just $X \rightarrow Y$ which is not the same as if-and-only-if. Suppose that you know that $X$ is true if and only if $Y$ is true, and
  you know that $Y$ is true if and only if $Z$ is true. Is this enough
  to show that $X, Y, Z$ are all logically equivalent? Explain. I think so? We know $X$ is logically equivalent to $Y$, and $Y$ is logically equivalent to $Z$, and by transitive property, $X$ is logically equivalent to $Z$, so they're all logically equivalent. I'm not sure if I am ""allowed"" to just invoke transitive property like that or if I have to show that it even applies here. Suppose you know that whenever $X$ is true, then $Y$ is true; that
  whenever $Y$ is true, then $Z$ is true; and whenever $Z$ is true, then
  $X$ is true. Is this enough to show that $X, Y, Z$ are all logically
  equivalent? Explain. $(X \rightarrow Y) \land (Y \rightarrow Z) \land (Z \rightarrow X)$, right? So by transitive property again, we have $(X \rightarrow Z) \land (Z \rightarrow X)$ so $X$ and $Z$ are logically equivalent, and similar for the other two combinations.","['proof-explanation', 'proof-verification', 'logic', 'analysis']"
2895174,Convergent sequence in Hilbert space,"Let $H$ be a Hilbert space with orthonormal basis $(e_n)_n$ and $(a_n)_n \in \ell^2(\mathbb{N})$, so $$\sum_{n=0}^{\infty} |a_n|^2 < \infty$$
Now, define the sequence $(\xi_n)_n$ in $H$ as $$\xi_n= \sum_{k=0}^{n} a_ke_k $$ I now have to proof that this sequence is convergent in $H$ or, if not, give a counterexample. I think that it's true because $a_n \to 0$ when $n \to \infty$ but I don't know how I can proof this.","['hilbert-spaces', 'convergence-divergence', 'sequences-and-series']"
2895245,Intuition behind conditional probabilty: $P(A|B)=P(B\cap A)/P(B)$,"I've struggled with probability for years. Even the most basic concepts. This is especially something I am not able to understand even after reading for the last 1.5 hours. Conditional probability is 
$$
P(A|B)=\frac{P(B\cap A)}{P(B)}.
$$
However, I fail to see why $P(A|B)=P(B \cap A)$ cannot be true in itself? Why do we have to divide by $P(B)$? A video online gave this example. The probability of being a male and an alcoholic is $\sim 2.25\%$. So what is the probability of being an alcoholic given that you are a male? I would say $2.25\%$ but in fact the answer is different. I cannot see how $P(A|B) \neq P(B \cap A)$. The intuition just isn't there. Is this something I am just supposed to accept and move on?","['conditional-probability', 'intuition', 'probability']"
2895284,Differentiate $\frac{x^3}{{(x-1)}^2}$,"Find $\frac{d}{dx}\frac{x^3}{{(x-1)}^2}$ I start by finding the derivative of the denominator, since I have to use the chain rule. Thus, I make $u=x-1$ and $g=u^{-2}$. I find that $u'=1$ and $g'=-2u^{-3}$. I then multiply the two together and substitute $u$ in to get: $$\frac{d}{dx}(x-1)^{2}=2(x-1)$$ After having found the derivative of the denominator I find the derivative of the numerator, which is $3x^2$. With the two derivatives found I apply the quotient rule, which states that $$\frac{d}{dx}(\frac{u(x)}{v(x)})=\frac{v'u-vu'}{v^2}$$ and substitute in the numbers $$\frac{d}{dx}\frac{x^3}{(x-1)^2}=\frac{3x^2(x-1)^2-2x^3(x-1)}{(x-1)^4}$$ Can I simplify this any further?Is the derivation correct?",['derivatives']
2895288,How to prove the series $\sum\limits_{n=1}^\infty\frac1n\sin(\ln n)$ diverges,"How to prove the following series is divergent:
  $$
\sum_{n=1}^{\infty} \frac{1}{n} \sin(\ln n)\ ?
$$ What I was thinking is, since $\sum\limits_{n=1}^{\infty} \frac{1}{n}$ diverges, $\sin$ is periodical and $(\ln (n+p)-\ln n)$ converges to $0$ when $n$ goes to infinity for every $p$, maybe I can look for a sum of partial terms within some $(2k\pi, 2k\pi + \pi)$ to be greater than a fixed $\epsilon$. But when it comes to the detail, it troubles me. Any hint?","['divergent-series', 'convergence-divergence', 'sequences-and-series']"
2895328,Calculate $\lim_{|\lambda| \to +\infty} \int_{\mathbb{R}}f(x)|\sin(\lambda x)|dx $,"For every integrable function $f \colon \mathbb{R} \to \mathbb{C}$ I have to calculate the limit 
$$\lim_{|\lambda| \to +\infty} \int_{\mathbb{R}}f(x)|\sin(\lambda x)|dx 
$$ We know that this limit exists because it is bounded by the integral of $f(x)$. From the lemma of Riemann-Lebesgue, we know that 
$$\lim_{|\lambda| \to +\infty} \int_{\mathbb{R}}f(x)\sin(\lambda x)dx = 0$$
So I assume that the limit that I need to calculate is bigger than zero, and is somewhere close to $$\int_{\mathbb{R}}f(x)dx$$
but I have no idea how I can prove this.","['integration', 'limits', 'real-analysis']"
2895343,Differentiate $\tan^3(x^2)$,"Differentiate $\tan^3(x^2)$ I first applied the chain rule and made $u=x^2$ and $g=\tan^3u$. I then calculated the derivative of $u$, which is $$u'=2x$$ and the derivative of $g$, which is 
$$g'=3\tan^2u$$ I then applied the chain rule and multiplied them together, which gave me $$f'(x)=2x3\tan^2(x^2)$$ Is this correct? If not, any hints as to how to get the correct answer?","['calculus', 'derivatives']"
2895394,using the laws of set algebra to simplify $(A \cap B^c) \cup (A^c \cap B^c)^c$,I am told to use the laws of set algebra to simplify $(A \cap B^c) \cup (A^c \cap B^c)^c$. And I need to show the reason for each step. $(A \cap B^c) \cup (A^c \cap B^c)^c$ $= (A \cap B^c) \cup (A \cup B)$ By De Morgan's law. $= (B^c \cap A) \cup (A \cup B)$ By commutativity. $= B^c \cap (A \cup A) \cup B$ By associativity. $= B^c \cap A \cup B$ By idempotence. $= B^c \cap B \cup A$ by commutativity $= \emptyset \cup A$ Since we have intersection with complement. $= A$ By identity laws. Does this all look correct? If not then what is the correct way? Thanks for any help.,['elementary-set-theory']
2895396,"What are the maximum and minimum values of $\langle u, v\rangle + \langle v, w\rangle + \langle w, u\rangle$?","Let $V$ be an inner product space over $\mathbb{R}$.
  Suppose that $u$, $v$ and $w $ are three unit vectors in the $xy$-plane.
  What are the maximum and minimum values that
  $$\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle$$
  can attain, and under what conditions? My attempt: I can say that the maximum value is  $\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle=  \langle 1, 1\rangle  + \langle 1, 1\rangle  + \langle 1, 1\rangle= 3$. The minimum value  is $\langle u, v\rangle  + \langle v, w\rangle  + \langle w, u\rangle=  \langle 0, 0\rangle  + \langle 0, 0\rangle  + \langle 0, 0\rangle= 0$. Please tell me if my answers is correct or not, and help me. Thanks in advance.","['inner-products', 'inequality', 'linear-algebra']"
2895413,Find the sum : $\displaystyle\sum_{i=0}^n \frac{2^i}{1+x^{2^{i}}}$,$\displaystyle\sum_{i=0}^n \frac{2^i}{1+x^{2^{i}}}$ What technique is applicable here? I can't find a way to manipulate this sum to make it telescope. Just guide me.,['summation']
2895451,Quick questions about summation notation used in my book,"My Questions In the screenshot below, is the summation notation in the orange (top) box and blue (bottom) box the exact same? So in a $3$ by $3$ matrix you'd only want to sum $a_{12} + a_{13} + a_{23}$ . Is it correct to say that the orange box is exactly the same as $$\sum_{i = 1}^{n-1} \sum_{j = i + 1}^n P(A_i A_j)?$$ Is it correct to say that the BLUE box is exactly the same as $$\sum_{(i,j):i<j} E[\min(X_i,X_j)]?$$ Why would you choose one notation or the other? It seems the blue box is faster to write. Is it correct to say that the following will always hold if $i$ and $j$ span the exact same indices and $a_{ij} = a_{ji}$ for all $i$ and $j$ ? $$\sum_{i \neq j} a_{ij} = 2\sum_{i<j} a_{ij}$$ Thank you for your time and patience.","['matrices', 'notation', 'algebra-precalculus', 'linear-algebra']"
2895488,"Finding covariance for the joint density function $f(y_1, y_2) = 3 y_1$ with $0 \leq y_2 \leq y_1 \leq 1$ and $0$ otherwise","The problem asks to find the covariance. The joint density function is defined as,
$$
  f(y_1,y_2)=
  \begin{cases}
    3y_1, & \text{for $0 \leq y_2 \leq y_1 \leq 1$}, \\
       0, & \text{elsewhere}. \\
  \end{cases}
$$
My text defines
$$
    \operatorname{Cov}(Y_1,Y_2)
  = \mathbb{E}(Y_1 Y_2) - \mathbb{E}(Y_1)\mathbb{E}(Y_2).
$$
I'm having problems finding $\mathbb{E}(Y_2)$. My attempt is
\begin{gather*}
  \mathbb{E}(Y_2) = \int_{-\infty}^\infty y_2 f_2(y_2) \,\mathrm{d}y_2,
  \\
  f_2(y_2) = \int_{-\infty}^\infty f(y_1,y_2) \,\mathrm{d}y_1
           = \int_0^1 3y_1 \,\mathrm{d}y_1 = \frac{3}{2},
  \\
  \mathbb{E}(Y_2) = \int_0^1 y_2 \frac{3}{2} \,\mathrm{d}y_2
                  = \frac{3}{4}.
\end{gather*}
But my book says $\mathbb{E}(Y_2)= 3/8$. Where did I go wrong?","['covariance', 'probability-theory', 'probability']"
2895491,Notation for subset of a set $A$ that is disjoint with every other set but $A$,"I have found myself making my own notation when considering several sets, say $A,B,C,D,E$, and wanting to denote for example the set $A^{\triangle} = \{x: x\in A \land x\notin Y, Y\neq X\}$. I know when considering just two sets like $A$ and $B$ I could write the somewhat more inconvenient $A\setminus B$, but I don't know how to conveniently write this when considering several sets. Is there a simple notation for $A^{\triangle}$? edit: Maybe I expressed myself unclearly. Although I appreciate the help, I don't consider $A\setminus(B\cup C \cup D \cup E)$ a simple way of writing $A^{\triangle}$. It becomes very unpractical when having to write it over and over again, not to speak of when considering more sets than five. I'm looking for something similar to $A^\triangle$, preferably just one small extra symbol denoting the set. Similar to how $\bar{A}$ can denote the complement of $A$. Again, sorry for not making this clear enough.",['elementary-set-theory']
2895492,"If $f:\mathbb R^2\to \mathbb R$ is defined by $f(x,y)=x^2+xy$. What is the meaning of the derivative/linear transformation, at the point $(1,2)$?","Suppose $f:\mathbb R^2\to \mathbb R$ is defined by $f(x,y)=x^2+xy$. The gradient is $(2x+y, x)$ and so the Jacobian is $\begin{bmatrix} 2x+y& x\end{bmatrix}$. At the point $(1,2)$, the derivative is the linear transformation $\begin{bmatrix} 4& 1\end{bmatrix}$, which is the matrix of the transformation $T(x,y)=4x+y$. For the sake of an example, the directional derivative in the direction of $(1,1)$ is $\begin{bmatrix} 4& 1\end{bmatrix}\begin{bmatrix} 1/\sqrt{2}\\ 1/\sqrt{2}\end{bmatrix}=5/\sqrt{2}$. Which I understand is the slope of the tangent plane in the direction of $(1,1)$. However, I am not quite understanding what the linear transformation $T(x,y)=4x+y$ represented by the matrix $\begin{bmatrix} 4& 1\end{bmatrix}$ is supposed to represent. The tangent plane lives tangent to the graph of $f$ at the point $f(1,2)=3$; i.e., at the point $(1,2,3) \in \mathbb R^3$. So, what exactly does the linear transformation $\begin{bmatrix} 4& 1\end{bmatrix}$ represent? What is being transformed?","['geometry', 'calculus', 'linear-algebra', 'linear-transformations', 'derivatives']"
2895498,"Prove, for ever $x$ in a maximal subgroup there is a $n$ such that $p^n x \in G$.","The third part of this question is giving me problems. I think I'm missing something trivial (?). Consider $\mathbb{R}$ as a group under addition. Prove, using Zorn's Lemma, that there is a subgroup $G$ of $\mathbb{R}$ which is maximal w.r.t. the property that $1\not \in G$ . Suppose $G$ is as in (1.). Show that there is a unique prime number $p$ such that $p\in G$ . Let $p$ be as in (2.). Prove that for every $x \in \mathbb{R}$ there is an $n\geqslant 0$ such that $p^n x\in G$ . Proof I've been able to prove 1. and 2. but for 3. I seem to be missing a way of putting 1. and 2. together. I was thinking of the following: Let $G$ be a maximal subgroup with $1\not \in G$ . Choose an $x \in \mathbb{R}$ . If $x\in G$ then $n=0$ is enough. Now let $x\not \in G$ . Consider $H:= \langle G \cup \{x\}\rangle$ . Then $1\in H$ because $G$ was maximal. Which implies $1=g+k\cdot x$ for a certain $g\in G$ and $k\in \mathbb{Z}$ . This implies $p = p\cdot g + pkx$ which implies $pkx \in G$ (because $p\in G$ ). However, how should I continue, how can I use (2.)? (the fact that $p$ is the only prime in $G$ )? Any tips? EDIT Inspired by Derek Holt, I completed the proof as follows: First notice that 2. implies $G\cap \mathbb{Z} = \langle p \rangle$ . Now once again let $x\not \in G$ and consider $\langle G \cup \{x\}\rangle$ , then $1$ must be in this group, which implies $1 = g_0+k_0x$ for certain $g_0, k_0$ . Or $p=p\cdot g_0+pk_0x$ , which implies $pk_0x \in G$ . Now consider $px$ , if $px\in G$ we are done, else consider $\langle G \cup \{px\}\rangle$ which must contain 1 due to maximality of $G$ . So $1 = g_1+\tilde k_0px$ for certain $g_1, \tilde k_0$ . Which implies $\tilde k_0 px -1 \in G\Rightarrow k_0 \tilde k_0px - k_0 \in G$ but since $k_0px\in G$ combining these results in $k_0 \in G$ . Howver $k_0 \in \mathbb{Z}$ . Thus $k_0 \in G\cap \mathbb{Z} = \langle p\rangle$ . I.e. $k_0 = k_1\cdot p$ for a certain $k_1$ where $|k_1| < |k_0|$ . This implies $k_0px = k_1p^2x \in G$ . This process can be continued untill $|k_{n-1}| = 1$ which results in $p^nx \in G$ .",['group-theory']
2895500,Simpler or shorter way to simplify expression $(16^{2} \times 64^{3})\div1024^{2}$ for a power,"I am studying about powers for a discipline in college and the teacher asked me to simplify the following expression to transform it into the form of a single power, $$
(16^{2} \times 64^{3})\div1024^{2}
$$ I can simplify to, $$
2^{6}
$$ But, take many steps to get this result, $$
(16^{2} \times 64^{3})\div1024^{2} \\ \implies(16\times16)\times(64\times64\times64)\div(1024\times1024) \\ \implies 256 \times262144\div1048576\\ \implies67108864\div1048576=64\\ 64\implies2^{6} \\ (16^{2} \times 64^{3})\div1024^{2} \implies 2^{6}
$$ However I would like to know if there is a shorter or simpler way to simplify expression $(16^{2} \times 64^{3})\div1024^{2}$ ?",['algebra-precalculus']
2895510,"Given this operation $\phi$ on natural number sequences, show $\phi^3=\phi$.","Given a sequence of natural numbers $f:\mathbb{N}\to\mathbb{N}$ define a new sequence $\phi f$ as follows: $$ \phi f(n):=|\{ k\in\mathbb{N} \mid k \leq n,f(k)=f(n)\}|$$ In other words $\phi f(n)$ counts up how often $f(n)=f(k)$ for $k\leq n$ . Also, we can apply $\phi$ iteratively, e.g.: $$\begin{array}{rcl} \\
f        & = & (0,&1,&0,&2,&1,&3,&0,&2,&0,&1,&...) \\
\phi f   & = & (1,&1,&2,&1,&2,&1,&3,&2,&4,&3,&...) \\
\phi^2 f & = & (1,&2,&1,&3,&2,&4,&1,&3,&1,&2,&...) \\
\phi^3 f & = & (1,&1,&2,&1,&2,&1,&3,&2,&4,&3,&...) \\
& \vdots
\end{array}$$ Notice that $\phi f=\phi^3 f$ . My conjecture is thus for any sequence of natural numbers $f$ , we have $\phi^{n+1} f = \phi^{n+3} f$ for all $n\in\mathbb{N}$ .",['sequences-and-series']
2895516,What is the radius of the black circle tangent to all three of these circles?,"The red, blue, and green circles have diameters 3, 4, and 5, respectively. What is the radius of the black circle tangent to all three of these circles? I just figured out the radius is exactly $\dfrac{72}{23}$ but I don't know how to do the solution.","['trigonometry', 'geometry']"
2895527,Evaluating improper double integral with Lebesgue,"Consider the improper double integral
$$ I_{\text{Riemann}} = \int_0^1 \int_0^{\sqrt x} \frac{2xy}{1-y^4} \; dy \; dx = \lim_{B \to 1^-} \int_0^B \int_0^{\sqrt x} \frac{2xy}{1-y^4} \; dy \; dx
.$$
The standard ""freshman calculus"" solution goes by swapping the order of integration to get $\int_0^1 \int_{y^2}^1 \frac{2xy}{1-y^4} \; dx \; dy$;
then the inner integral is $\frac{y}{1-y^4} \int_{y^2}^1 2x \; dx = \frac{y}{1-y^4} \left[ x^2 \right]^1_{y^2} = y$ and the answer is $\int_0^1 y \; dy = \frac12$. I'm trying to make sense of this rigorously, particularly the bit about swapping the order of integration (which seems to require some sort of Tonelli/Fubini result). My idea is something like the following: define the double Lebesgue integral
$$I_{\text{Lebesgue}} = \int_{x \in [0,1)} \int_{y \in [0,1)} \mathbf 1(x \ge y^2) \cdot \frac{2xy}{(1-y^4)} \; dy \; dx.$$ Then Tonelli's theorem lets us swap the order of summation to get
$$I_{\text{Lebesgue}} = \int_{y \in [0,1)} \frac{y}{1-y^4} \int_{x \in [0,1)} \mathbf 1(x \ge y^2) \cdot 2x \; dx \; dy.$$ Thus the inner integral is the same as the Riemann one $\int_{y^2}^1 2x \; dx = 1 - y^4$, hence $$I_{\text{Lebesgue}} = \int_{y \in [0,1)} y \; dy = \frac12.$$ However, since the original Riemann integral was improper, I don't really know how to justify the first step (cue $x^{-1} \sin x$ example). So I have the following three questions: Is there some result/theorem that lets me quickly see that $I_{\text{Riemann}} = I_{\text{Lebesgue}}$? Bonus points for not using nonnegativity of $\frac{2xy}{1-y^4}$. Is the calculation of $I_{\text{Lebesgue}}$ correct as written? Are there other ways to justify the interchange of improper integrals? I'm fine appealing to Lebesgue since Lebesgue integrals are ""better-behaved"" anyways, but I'm wondering if I've missed something easier.","['lebesgue-integral', 'measure-theory', 'improper-integrals', 'real-analysis']"
2895581,Why does this series have a different sum when its terms are rearranged?,"The problem is: Give an example of a convergent series such that, when the terms are rearranged, the series sums to a different value. A solution is: Although everything makes sense in this solution, I don't get how what it claims is possible.  How can a series have two different sums? Is this caused by the fact that subtraction is not commutative or associative?  Hence, in fact, we really have two different series here?","['convergence-divergence', 'sequences-and-series', 'summation', 'real-analysis']"
2895603,Prove distributive law of sets,"Let $A,B,C$ be sets. Prove the distributive law $$A \cap\left(B\cup C\right) = \left(A\cap B\right)\cup \left(A\cap C\right)$$ proof First we'll show that $A \cap\left(B\cup C\right) \subset \left(A\cap B\right)\cup \left(A\cap C\right)$, and then the converse. If $x$ is in $A \cap\left(B\cup C\right)$, then $x$ must be in $A$ and $x$ must be in $B$ or $C$. An element $x$ can satisfy this membership by being in either $A$ and $B$, or $A$ and $C$. In symbols, $$x \in \left[\left(A\cap B\right)\cup \left(A\cap C\right)\right]$$ I'm not going to write the other direction. Because I know once I have this way down I can go the other way. What I'm looking for is -- in addition to correctness -- whether my argument is clunky or ambiguous. This ""or"" and ""and"" wording throws me for a loop when I'm trying to use it as a math operation and a word. Ideally, I'd like to know if there's a simpler way to write these types of proofs.","['elementary-set-theory', 'logic']"
2895620,"An Euler type sum: $\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n\cdot 4^n}{2n \choose n}$, where $H_n^{(2)}=\sum\limits_{k=1}^{n}\frac{1}{k^2}$","I've been trying to compute the following series for quite a while : $$\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n\cdot 4^n}{2n \choose n}$$
where $H_n^{(2)}=\sum\limits_{k=1}^{n}\frac{1}{k^2}$ are the generalized harmonic numbers of order 2. I've already successfully computed the value of a lot of similar-looking series involving harmonic numbers and central binomial coefficients (by using Abel's transformation, finding the generating function...), and they all had closed forms in terms of the Riemann Zeta function, so I'm confident this one too has a nice closed form ; but it's a tough guy that resists my previous methods. By numerical test, I suspect that the value is $\frac{3}{2}\zeta(3)$. Any idea for derivation ?","['summation', 'real-analysis', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
2895667,The minimal uncountable well-ordered set has no largest element,"Theorem. There exists a well ordered set $A$ having a largest element $\Omega$ such that the section $S_\Omega$  of $A$ by $\Omega$ is uncountable but every other section of $A$ is countable. Proof. Consider an uncountable well ordered set $B$, consider $C=\{1,2\}\times B$ with the lexicographic order and let $\Omega $ be the smallest element of $C$ for which $S_\Omega=\{x\in C| x<\Omega\}$ is uncountable. Let $A=S_\Omega\cup\{\Omega\}$
  . I'm trying to show that $S_\Omega$ has no largest element. Assume the converse: there is $\Gamma\in S_\Omega$ such that $x\le \Gamma$ for all $x\in S_\Omega$. I need to use that $\Gamma $ is a largest element. It's essential to consider $S_\Gamma$ ($\Gamma\in C$, and $S_\Gamma$ is the section of $C$ by $\Gamma$). Then $S_\Gamma$ is countable. At the same time $S_\Omega$ is not. But I don't see any contradiction here... I guess I need to invoke $\Omega$ and prove something like $S_\Omega\cup\{\Omega\}=S_\Gamma$. But I don't see why this holds. $$S_\Gamma=\{x\in C: x\le \Gamma\}
\\ S_\Omega\cup\{\Omega\}=\{x\in C: x\le \Omega\}$$
Do I need to use $x\le \Gamma$ for all $x\in S_\Omega$ somehow to prove the equality? I don't see how.","['elementary-set-theory', 'logic']"
2895698,Why does $C_1\cap C_2\cap C_3\cap \ldots = \{x:x=2\}$?,"Before I say anything:  this question is somewhat of a follow up to this question here. Suppose $C_1,C_2, C_3\ldots$ are sets where $C_k\supset C_{k+1}$ for $k=1,2,3,\ldots$ and $\lim_{k\rightarrow\infty} C_k= C_1\cap C_2\cap C_3\cap \ldots$  My question is different than the one linked in that $C_k$ is defined by $$C_k=\left\{x:2-\frac{1}{k}<x\leq 2\right\}.$$  I am being asked to find $\lim_{k\rightarrow\infty}C_k$.  According to my book, the solution is $\{x:x=2\}.$  This is absolutely mind boggling to me because I am failing to see the difference between this $C_k$ and the one featured in the linked question because when I compute $$\lim_{k\rightarrow\infty}(2-\frac{1}{k})=2$$ I get the same answer of $$\lim_{k\rightarrow\infty} C_k=\{x: 2<x\leq 2\}=\{\}.$$ What am I missing here?  Is the book wrong?","['elementary-set-theory', 'limits']"
2895699,Showing that the Lie bracket of two Killing fields on a Riemannian manifold is again a Killing field using the Killing equation,"I've read that the Lie bracket of two Killing fields $X$ and $Y$ on a Riemannian  manifold $M$ are again a Killing field so I thought it might be a good exercise to try to prove this using the Killing equation. We want to show that $$\langle \nabla_{Z}[X,Y],W \rangle =-\langle \nabla_{W}[X,Y],Z \rangle$$ for any two (smooth) vector fields $Z$ and $W$ on $M$ . Here is my attempt: $\langle \nabla_{Z}[X,Y],W \rangle= \langle \nabla_{Z}(\nabla_{X}Y-\nabla_YX),W\rangle$ (using symmetry) $=\langle \nabla_{Z} \nabla_X Y,W \rangle -\langle \nabla_{Z} \nabla_Y X,W \rangle$ (using linearity of the connection) $=Z\langle \nabla_XY,W \rangle-\langle \nabla_XY,\nabla_ZW\rangle-Z\langle \nabla_YX,W \rangle +\langle \nabla_YX,\nabla_ZW \rangle$ (compatibility of the metric) At this point I thought about using the fact that $X$ and $Y$ satisfy the Killing equation and applying it to the terms being acted on by $Z$ but this doesn't appear to work. Maybe I'm going down the wrong road here but does anyone have a proof of this using the Killing equation?","['riemannian-geometry', 'differential-geometry']"
2895712,Roots of the Indicial Polynomial of the Legendre equation $(1-z^2)u''-2zu'+v(v+1)u=0$,"Consider the Legendre equation $$(1-z^2)u''-2zu'+v(v+1)u=0.$$ Find the roots of the indicial polynomial if we apply the Frobenius method about $z=1$. My attempt: Let \begin{align}u=\sum_{k=0}^{\infty} A_k(z-1)^{k+r}&\implies u'=\sum_{k=0}^{\infty} (k+r) A_k(z-1)^{k+r-1}\\ 
&\implies u''=(k+r)(k+r-1)\sum_{k=0}^{\infty} A_k(z-1)^{k+r-2}
\end{align}
Substituting this into the Legendre equation, I get:
$$\sum_{k=-2}^{\infty} (k+r+1)(k+r+2)A_{k+2}(z-1)^{k+r}+\sum_{k=0}^{\infty} \left((v^2+v)-(k+r)(k+r-1)-2(k+r)\right)A_k(z-1)^{k+r}=0$$ Now what do I do? edit I have found the roots of the indicial polynomial are $r=0, 1$, which contradicts the answer provided, which states that $0$ is a repeated root. Any advice would be greatly appreciated.","['frobenius-method', 'ordinary-differential-equations', 'legendre-polynomials']"
2895734,How does $f(x)= x \sin(\frac{\pi}{x})$ behave?,"I think this function is increasing for $x>1$ but wanted to find the reason. So I thought about taking the derivative: $f(x)= x \sin(\frac{\pi}{x})$ Aplying the chain an the product rule, we get: $f'(x)= \sin(\frac{\pi}{x})-\frac{\pi}{x} \cos (\frac{\pi}{x})$ The function is increasing if the derative is more than or equal to $0$, so: $\sin(\frac{\pi}{x})-\frac{\pi}{x} \cos (\frac{\pi}{x}) \ge 0$ $\sin(\frac{\pi}{x}) \ge \frac{\pi}{x} \cos (\frac{\pi}{x}) $ Since $ \cos ( x) > 0$, if $ 0< x < \pi$,  $ \cos (\frac{\pi}{x}) > 0 $, because $ 0<\frac { \pi}{x}< \pi$. $ \tan (\frac{\pi}{x}) \ge \frac{\pi}{x}$ I get to this point and don't know how to continue. I'd like you to help me or give me a hint, or maybe see a different way of showing it. Anyway, thanks.",['calculus']
2895737,"Leibniz' Rule: Prove $\int_0^y u_{tt}(x,t) dt = u_y (x,y) - u_y (x,0)$","A First Course in Complex Analysis by Matthias Beck, Gerald Marchesi, Dennis Pixton, and Lucas Sabalka Thm 6.8 Statement of Thm 6.8: Suppose $u$ is harmonic on $\mathbb C$. Then $$v(x,y) := \int_0^y u_x(x,t) dt - \int_0^x u_y(t,0) dt$$  is a harmonic conjugate for $u$. Pf of Thm 6.8: Question: How do we show $$\int_0^y u_{tt}(x,t) dt = u_y (x,y) - u_y (x,0)?$$ It's been awhile since I've done PDEs or multivariable calc, but here goes: First approach: $$\int_0^y u_{tt}(x,t) dt = u_t(x,t)|_{\color{green}{0}}^{\color{blue}{y}} = u_t(x,t)|_{t=\color{blue}{y}} - u_t(x,t)|_{t=\color{green}{0}} = u_\color{blue}{y}(x,\color{blue}{y}) - u_\color{green}{0?}(x,\color{green}{0})$$ I mainly don't get why we might have the $y$ in the last term $$u_\color{green}{y}(x,\color{green}{0}).$$ Second approach: $$\int_0^y u_{tt}(x,t) dt = u_t(x,t)|_{\color{green}{0}}^{\color{blue}{y}} = u_t(x,\color{blue}{y}) - u_t(x,\color{green}{0}) \stackrel{(*)}{=} u_\color{red}{y}(x,\color{blue}{y}) - u_\color{red}{y}(x,\color{green}{0})$$ (*) Here, I guess I get how for the second term we have $u_t(x,\color{green}{0}) = u_\color{red}{y}(x,\color{green}{0}),$ but not how for the first term we have $$u_t(x,\color{blue}{y})=u_\color{red}{y}(x,\color{blue}{y}).$$","['integration', 'analysis', 'multivariable-calculus', 'calculus', 'partial-differential-equations']"
2895747,"Prove that $\int_0^\pi \ln(1+\alpha \cos x)\, dx= \pi \ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right)$","Prove that $$\int_0^\pi \ln(1+\alpha \cos x)\, dx= \pi \ln\left(\frac{1+\sqrt{1-\alpha^2}}{2}\right).$$ This question is under the Leibnitz's Rule section of my book, but I'm not sure if you're supposed to use it here. Using it simply gives the derivative of $\ln(1+\alpha \cos x)$ with respect to $\alpha$, which isn't really useful. Can anyone help? Edit: So I've taken some suggestions from the comments and worked out the question. I was able to get pretty close to the answer, but I'm unable to get the answer exactly. I've skipped a couple of steps here, but do let me know if I did something wrong. $$I(a)=\int_0^\pi \ln(1+\alpha \cos x)\, dx$$ $$I'(a)=\int^\pi_0\frac{cosx}{1+acosx}dx$$ $$I'(a)=\frac{1}{a}\int^\pi_0\frac{acosx+1-1}{1+acosx}dx$$ $$I'(a)=\frac{1}{a}[\pi-\int^\pi_0\frac{1}{1+acosx}dx]$$ By the Tangent Half-Angle Substitution, $$I'(a)=\frac{1}{a}[\pi-\int^\pi_0\frac{1}{1+acosx}dx]$$ $$I'(a)=\pi[\frac{1}{a}-\frac{1}{\sqrt {1-a^2}}]$$ $$I(a)=\int\pi[\frac{1}{a}-\frac{1}{\sqrt {1-a^2}}]da$$ Using normal integrating techniques, $$I(a)=\pi[ln\sqrt{1-a^2}+1]$$ Note that I can't get the last term, $\pi ln2$. If I can get this last term, I can get the answer. Now, I'm wondering if it is because it is a constant term, and thus when I have differentiated by $a$, in the beginning, the information in that term is lost. Can someone help me continue or point out where I went wrong?","['multivariable-calculus', 'calculus']"
2895749,Any idea on how to find a upper bound for a limit in R^2?,"I'm working on this problem of continuity in $\mathbb{R}^2$ the statement is the following: 
Prove by definition that 
$$ \lim_{ (x,y) \to (0,0) } \frac{y^2}{3+\sin^2(x^2+y^2)+y^4}=0 $$
Given $\epsilon >0$, we have that 
$$| \frac{y^2}{3+\sin^2(x^2+y^2)+y^4} | =  y^2 \frac{1}{3+\sin^2(x^2+y^2)+y^4}$$ If we know that 
$ \frac{1}{3+\sin^2(x^2+y^2)+y^4} \leq  \frac{1}{\sin^2(x^2+y^2)} $ then we end with 
$$ y^2 \frac{1}{3+\sin^2(x^2+y^2)+y^4} \leq y^2 
\frac{1}{\sin^2(x^2+y^2)}$$
and even more, 
$$y^2 
\frac{1}{\sin^2(x^2+y^2)} \leq  \frac{x^2+y^2}{\sin^2(x^2+y^2)} $$
I don't see any way to get rid out the $\frac{1}{\sin^2(x^2+y^2)}$ to conclude the $\delta$ since I can't say that $\frac{1}{\sin^2(x^2+y^2)} \leq 1$ because is the contrary, in fact  $\frac{1}{\sin^2(x^2+y^2)} \geq 1$. Any ideas? Thanks in advance.","['multivariable-calculus', 'limits', 'calculus', 'real-analysis']"
2895760,Basic exponential regression,"Background: I'm attempting to learn about basic statistics for the infrastructure asset management industry: Basic math explanation (related to estimating linear regression with no intercept) Problem: I would like to learn how to generate an exponential regression equation for road condition data, just like it was done for me here: Development of a Flexible Framework for Deterioration Modelling in Infrastructure Asset Management Hint: Skip to page 53 (the printed page number, not the PDF page number) In other words, I want to learn how to generate an exponential regression equation, so I can eventually update the coefficient in the existing model (on the full/real dataset). My soloution (mock data): I've mocked up a sample dataset here: +--------------+---------------+
|    X (AGE)   | Y (CONDITION) |
+--------------+---------------+
|       0      |       20      |
|       1      |       20      |
|       2      |       20      |
|       3      |       20      |
|       4      |       20      |
|       5      |       20      |
|       6      |       18      |
|       7      |       18      |
|       8      |       18      |
|       9      |       18      |
|       10     |       16      |
|       11     |       16      |
|       12     |       14      |
|       13     |       14      |
|       14     |       12      |
|       15     |       12      |
|       16     |       10      |
|       17     |        8      |
|       18     |        6      |
|       19     |        4      |
|       20     |        2      |
+--------------+---------------+ Steps in Excel: Column C: Convert Y to be more linear using the natural logarithm function Column D: Calculate a straight line that best fits the data, and then return an array that describes the line (using the LINEST function). Column E: Generate a trend-line on D, and use the coefficient from that trend-line to create an exponential regression equation: =21-exp(0.14723*x) Question: How successful was I? Was my approach mathematically correct? Related: Normal equation in Excel (statistics) Tune an exponential regression estimate using calculus Options for tuning exponential regression? Mimic Excel Solver nonlinear regression? (to reduce ESS of exponential regression) Excel: data table (hit and trial)","['regression', 'statistics', 'exponential-function', 'linear-regression']"
2895772,"$f(x) = (x_1-x_2^2)(x_1-\frac{1}{2}x_2^2)$, verify $\overline{x} = (0,0)^t$ is local min of $\phi(\lambda) = f(\overline{x}+\lambda d)$ but not of $f$","Let $f(x) = (x_1-x_2^2)(x_1-\frac{1}{2}x_2^2)$. Verify that
  $\overline{x} = (0,0)^t$ is a local minimizer of $\phi(\lambda) =
f(\overline{x}+\lambda d)$ for all $d\in\mathbb{R}^2$ but
   $\overline{x}$ is not a local minimizer of $f$ $$\frac{\partial f}{\partial x_1} = 1\left(x_1-\frac{1}{2}x_2^2\right)+(x_1-x_2^2)$$ $$\frac{\partial f}{\partial x_2} = -2x_2\left(x_1-\frac{1}{2}x_2^2\right)-x_2^2(x_1-x_2^2)$$ $$\frac{\partial^2 f}{\partial x_1^2} = 2$$ $$\frac{\partial^2 f}{\partial x_2^2} = 6x^2$$ $$\frac{\partial^2 f}{\partial x_2\partial x_1} = -3x_2$$ $$\nabla^2 f =   \begin{bmatrix}
    2 & -3x_2  \\
    -3x_2 & 6x_2^2 
  \end{bmatrix}$$ $$\nabla^2 f(0,0) =   \begin{bmatrix}
    2 & 0  \\
    0 & 0 
  \end{bmatrix}$$ $$\begin{bmatrix}
    a & b  \\
  \end{bmatrix}\begin{bmatrix}
    2 & 0  \\
    0 & 0 
  \end{bmatrix}\begin{bmatrix}
    a   \\
    b  
  \end{bmatrix} = a^2$$ for $(0,0)$ to be a local minimizer we should have $\nabla f = 0$ (we have) and $\nabla^2\ge 0$. But $\nabla^2$ is not always semipositive definite so $(0,0)$ is not a local minimizer of $f$. Now let's analyze $f((0,0)+\lambda d)$. $$\nabla(\lambda d_1,\lambda d_2) = \begin{bmatrix}
    \lambda d_1-\frac{1}{2}\lambda^2d_2^2 + \lambda d_1 -\lambda^2d_2^2   \\
    -2\lambda d_2(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2)-\lambda d_ 2^2(\lambda d_1-\lambda^2d_2^2)  
  \end{bmatrix}\neq 0$$ and the hessian gives an even worse thing. So I think instead I should analyze $$\phi(\lambda) = f(\lambda d_1,\lambda d_2) = (\lambda d_1-\lambda^2 d_2^2)(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2)$$ but what does it mean for $\overline{x}$ to be a local minimizer of something that depends on $\lambda$? UPDATE: I think I have to minimize $$\phi(\lambda) = f(\lambda d_1,\lambda d_2) = (\lambda d_1-\lambda^2 d_2^2)(\lambda d_1-\frac{1}{2}\lambda^2 d_2^2) = \\\lambda^2 d_1^2 -\frac{1}{2}\lambda^3d_1d_2^2-\lambda^3d_2^2 d_1 + \frac{1}{2}\lambda^4d_2^4\implies \\ \phi'(\lambda) = 
2d_1^2\lambda  -\frac{3}{2}\lambda^2d_1d_2^2 -3\lambda^2d_2^2d_1 + 2\lambda^3d_2^4 = 0\implies \\2d_2^4\lambda^2+\lambda\left(-\frac{9}{2}d_2^2d_1\right) + 2d_1^2 = 0\implies\\ \lambda = \frac{\frac{9}{2}d_2^2d_1\pm\sqrt{\ (\frac{9}{2}d_2^2d_1)^2 -4\cdot 2d_2^4\cdot2d_1^2}}{4d_2^4}, \lambda = 0$$ It's unpratical to test these values on the function to see which of them is smaller. And even with this I don't know how to proceed. Maybe I should test the second derivative too, I don't know what it means to prove $\overline{x} = (0,0)$ is a minimizer of $\phi(\lambda) = f(\overline{x}+\lambda d)$. The only possible thing I can image is $\lambda$ being $0$ giving us the minimum of $\phi(\lambda)$. So since we know $\lambda=0$ gives us derivative $0$, let's see how the second derivative is at this point (if it's positive, then $\lambda=0$ is a minimizer): $$\phi''(\lambda) = 6\lambda^2 d_2^4 -6\lambda d_2^2 d_1 -\frac{6}{2}\lambda d_1d_2^2 + 2d_1^2 \implies\\\phi''(0) = 2d_1^2$$ So for any direction with $d_1\neq 0$, we'll have $\lambda=0$ as minimizer. What about when the direction is $(0,d_2)$ for any $d_2$? In this case we have $$\phi(\lambda) = (0-(\lambda d_2)^2)(0-\frac{1}{2}(\lambda d_2)^2 ) = \frac{1}{2}\lambda^4d_2^4$$ for which $\lambda= 0$ is also a minimizer. So in all cases $\lambda=0$ is a minimizer of $f((0,0) + \lambda (d_1,d_2))$ which means $(0,0)$ is a minimizer of $\phi(\lambda)$ I guess?","['optimization', 'multivariable-calculus', 'maxima-minima', 'derivatives']"
2895779,Is there a sharper upper bound of the spectral norm of Hilbert matrix than $3+2\sqrt{2}$?,"$A_n$ is a real symmetric $n \times n$ matrix defined by $$ A_n =  \begin{bmatrix} 1 & \frac{1}{2} & \frac{1}{3} & \cdots &
 \frac{1}{n} \\ \frac{1}{2} & \frac{1}{2} & \frac{1}{3} & \cdots &
 \frac{1}{n} \\ \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & \cdots &
 \frac{1}{n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\
 \frac{1}{n} & \frac{1}{n} & \frac{1}{n} & \cdots & \frac{1}{n} \\
 \end{bmatrix} $$ Find an upper bound of the eigenvalues of $A_n$ as tight as possible. Let $A_n = U+U^T-D,$ where $U$ is the upper triangular part of $A_n$ and $D$ is the diagonal of $A_n$. Suppose $\lambda$ is the largest eigenvalue of $A_n = U^T U$. For any $X \in \mathbb{R}^n$, 
$$X^T A_n X = X^T U^T U X = \left\Vert UX \right\Vert^2 \leq \lambda \left\Vert X \right\Vert^2 \implies \left\Vert UX \right\Vert \leq \sqrt{\lambda} \left\Vert X \right\Vert.$$
Since $U^T U$ is similar to $U U^T$, $\lambda$ is also the largest eigenvalue of $ U U^T $. Hence we also have
$$ \left\Vert U^T X \right\Vert \leq \sqrt{\lambda} \left\Vert X \right\Vert.$$
Similarly, $$ \left\Vert D X \right\Vert \leq \left\Vert X \right\Vert.$$ Thus $$ \left\Vert A_n X \right\Vert = \left\Vert U X + U^T X - D X \right\Vert \leq \left\Vert U X \right\Vert + \left\Vert  U^T X \right\Vert + \left\Vert  D X \right\Vert \leq \left( 2 \sqrt{\lambda} + 1 \right) \left\Vert X \right\Vert.$$ Take $X$ to be the eigenvector of $A_n$ corresponding to $\lambda$, then $$ \left\Vert A_n X \right\Vert = \lambda \left\Vert X \right\Vert \leq \left( 2 \sqrt{\lambda} + 1 \right) \left\Vert X \right\Vert,$$ we have $\sqrt{\lambda} \leq 1+\sqrt{2} \implies \lambda \leq 3+2\sqrt{2}.$ Is there any tighter upper bound?","['matrices', 'linear-algebra', 'upper-lower-bounds', 'eigenvalues-eigenvectors']"
2895800,Where are the zeroes of complex exponential function?,"There is none because we have $e^z \neq 0$ for every $z \in \mathbb C$. But we have Taylor series that everywhere converges to $e^z$, it is $e^z = \displaystyle \sum_{k=0}^{+ \infty} \frac {z^k}{k!}$. If we truncate that series , say, at natural $m$, then we have Taylor polynomial $\displaystyle \sum_{k=0}^{m} \frac {z^k}{k!}$, which has, counted with maybe possible multiplicity, $m$ complex zeroes. So as the degree of Taylor polynomial grows the number of zeroes increases, but in the limit they all dissapear, why?","['complex-analysis', 'exponential-function', 'roots']"
2895854,Can a flat space be closed?,"A user on a physics forum wanted to construct a metric of a hypothetical closed universe expanding with acceleration. When I proposed a simple example of a 3-sphere $S^3$, he asked if this space could be flat instead like ""infinity mirrors"". His requirements were, ""if you fly straight, you come back to the same point from behind"", and, ""a flat space with no curvature"". I am not a mathematician, so I've decided to ask the professionals here. Intuitively, I can imagine this along one dimention, something like $x=x+D$ where $D$ is the size of the universe in the direction of $x$. As you cross $x=D$, you simply reappear at $x=0$. The same in 3 dimensions defines a cube whose opposite sides are magically joined: getting out on one side puts you back inside on the other. If there is any rational seed in this idea, does this space have a known name or description? Would it have any unique properties? For example, would it support General Relativity (e.g.  locally metric space). Would this space have detectable edges (like the sides of the cube above) or not? Are there any other ways to fulfill these requirements? Thanks for your help!","['general-topology', 'metric-spaces', 'differential-geometry']"
2895880,Inversion of rotation matrix,"For example, I have a two-dimensional rotation matrix
$$
  \begin{bmatrix}
    0.5091 &           -0.8607 \\
    0.8607 & \phantom{-}0.5091
  \end{bmatrix}
$$
and I have a vector I'd like to rotate, e.g. $(1, -0.5)$. My problem is to find an inverse of the rotation matrix so that I can later “undo” the rotation performed on the vector so that I get back the original vector. The rotation matrix is not parametric, created via eigendecomposition, I can't use angles to easily create an inverse matrix.","['matrices', 'rotations']"
2895896,What is a Coxeter Group?,"I've recently started investigating abstract algebra and have now stumbled upon ""Coxeter Groups"", which are a mystery to me. I've read that Coxeter Groups have something to do with reflections (in which way is entirely unclear) are related to ""Coxeter Matrices"" and -""Diagrams"" (which I don't know) are groups with a certain ""presentation"" (which I've looked into but not understood their connection to this) My guess is that Coxeter groups are groups of reflections ""generated"" by some reflections - but I neither know if the terms here are used in any way correctly nor if it's right, partially since there are no useful examples to be found anywhere.","['reflection', 'group-theory', 'coxeter-groups']"
2895915,"In how many difference ways can six players be arranged in a line such that two of them, Abhinav and Manjesh are never together?","here is my attempt since we have six persons so the total number of way of arranging six persons in a line is $6!$ now since 'Abhinav' and 'Manjesh' is saying never together so we can subtract the total number of way of arranging 'Abhinav' and 'Manjesh' together.
 so the total number of ways we can do this is $6! - 5!2! = 480.$ but the actual answer is $\frac{ 6!}{2!}= 360.$ can anybody advise on this question where I am wrong?
any effort is appreciatable.","['permutations', 'combinations', 'discrete-mathematics']"
2895997,Sum to n terms the series $\cos \theta+ 2\cos 2\theta+ \cdots + n\cos n\theta$,Sum to $n$ terms and also to infinity of the following series:$$\cos \theta+ 2\cos 2\theta+ \cdots + n\cos n\theta$$the solution provided by the book is $$S_n=\frac{(n+1)\cos n\theta-n\cos(n+1)\theta-1}{2(1-\cos\theta)}$$Can anyone help me to explain how to get $S_n$. Thanks in advance.,"['summation-method', 'summation', 'trigonometric-series', 'calculus', 'sequences-and-series']"
2896065,"Show that $\int_0^2 \int_0^2 \frac{x}{1+\ln(x^2y^2)} \,\mathrm{d}x \,\mathrm{d}y \leq 4$","Show that
  $$
  \int_0^2 \int_0^2 \frac{x}{1+\ln(x^2y^2)} \,\mathrm{d}x \,\mathrm{d}y \leq 4.
$$ When I think somewhat outside the box, I can see that when I sketch the boundaries in a one-dimensional way, it is clearly a square area with a side length of $2$, and obviously $2^2 = 4$. So in theory, if the definite integral is finding the area in certain contraints, the area has to be $\leq 4$ Is this the correct approach to this kind of question? And if so where should I go next? This is only worth $2$ of $25$ marks which would indicate to me that I am not expected to actually complete the integration. Any help is much appreciated.","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'upper-lower-bounds']"
2896077,Why is there both `:=` and `=` used in set notation? [duplicate],"This question already has answers here : Difference between $:=$ and $=$ (3 answers) Closed 5 years ago . I see two kinds of equal signs in different resources in regards to defining sets. One is := and the other is = An example : S = {1, 2, 3} or S := {1, 2, 3} I realized that resources concerned with mathematical analysis uses the latter whereas others use the former. Is there any difference in the meaning of both notations?","['elementary-set-theory', 'notation']"
2896086,Factorization $\cos\left(\tfrac{\pi z}{4}\right)-\sin\left(\tfrac{\pi z}{4}\right)$,"Prove that 
  $$\cos\left(\frac{\pi z}{4}\right)-\sin\left(\frac{\pi z}{4}\right)=\prod_{n=1}^\infty\left(1+\frac{(-1)^nz}{2n-1}\right)$$ My try:
$$\cos\left(\frac{\pi z}{4}\right)-\sin\left(\frac{\pi z}{4}\right)=\cos\left(\frac{\pi z}{4}\right)-\cos\left(\frac{\pi}{2}-\frac{\pi z}{4}\right)=-\sqrt{2}\sin\left(\frac{\pi z-\pi}{4}\right) $$
Now, substitute the expression of the $\sin(\pi z)$ and get
$$\cos\left(\frac{\pi z}{4}\right)-\sin\left(\frac{\pi z}{4}\right)=-\sqrt{2}\pi\left(\frac{z-1}{4}\right)\prod_{n=1}^\infty\left(1-\frac{(z-1)^2}{16n^2}\right).$$
How should I proceed now?","['complex-analysis', 'trigonometry', 'infinite-product']"
2896136,modelling a differential equation system,"I have a system of ODEs with a pathogen population $(P)$ being modelled as a logistic growth as, ${dP\over dt}=rP(1-{P\over k})$, where $r$ is the replication rate and $k$ is the carrying capacity term and both these parameters are constant values. Now I want to introduce antibiotic effect so that the antibiotic will inhibit the growth of pathogen.For this I am planning on introducing a constant $\alpha \in (0,1)$ so that the replication will be reduced as $\alpha r$. But I want this replication to depend on the antibiotic concentration. So if I model the system as, ${dP\over dt}=\alpha A rP(1-{P\over k})\\
{dA\over dt}=-dA$ where $A$ is the antibiotic concentration, will this be right? ($d$ is the antibiotic decay rate) Or, should this be, ${dP\over dt}= \alpha {1\over A} rP(1-{P\over k})\\
{dA\over dt}=-dA$ as, when $A$ is high the replication should be reduced more (because the antibiotic will be more effective at high concentrations), and as $A$ decreases the reduction in the replication will be less? Out of these two systems which one maps this relationship correctly or is there any other way to model this?","['biology', 'mathematical-modeling', 'ordinary-differential-equations']"
2896147,Dissecting the complexity of prime numbers,"Each prime number greater than $9$, written in base $10$, ends with one of the four digits $1,3,7,9$. Therefore, each ten can be classified according to which of these four digits, summed to the ten, yields to a prime number. For example, for the first ten we have $1 \rightarrow \{1,3,7,9\}$. In fact, $10+1$, $10+3$, $10+7$ and $10+9$ are all primes. Conversely, for the twentieth ten the association reads $20 \rightarrow \{\}$, since there are no primes between $200$ and $209$. It is easy to see that each ten is associated to one (and only one) group of symbols, chosen among the following $16$ distinct alternatives: $\{\}$, $\{1\}$, $\{3\}$, $\{7\}$, $\{9\}$, $\{1,3\}$, $\{1,7\}$, $\{1,9\}$,  $\{3,7\}$, $\{3,9\}$, $\{7,9\}$, $\{1,3,7\}$, $\{1,3,9\}$, $\{1,7,9\}$, $\{3,7,9\}$, $\{1,3,7,9\}$. For the sake of simplicity, we can identify each of these $16$ distinct groups of symbols  with a single symbol, or with a single color , as illustrated below: Each of these colors represents how many prime numbers there are in one ten (and which ones). In practice, we have just split the complexity of primes into tens and colors. This allows us to rearrange the colors within the Pascal's triangle, by means of the associated ten, obtaining the following scheme (the numbers in the squares represent the tens): The complexity of the primes sequence has been now split into rows, diagonals  and colors. An advantage of such representation is that it mixes groups of primes related to far tens, allowing maybe to identify patterns and/or to dig out connections among already known integer sequences. The clear disadvantage is that the patterns on this triangle depend on which base we use. I partially introduced this representation of prime numbers here , but I am not sure whether these further developments overlap with some very well known technique (e.g. Sieve of Eratosthenes?). In conclusion, just not to re-invent the wheel, my question is: Do you know if such representation has been already devised? In that case, could you please give me some reference? Sorry for naivety and incorrectness, and thank you very much for your suggestions and comments! NOTE: The conjecture contained in the following EDIT is FALSE , and there is a mistake in the code! Please, if you have a good software to produce the picture, please tell me! Thanks! (Thanks also to Paul!) EDIT: To reply to some comments, and adopting this somehow enhanced color code, I produced the following plot (omitting the first ten) I hope that there is not some mistake in my code! However, It is conjectural that, for very big numbers, there cannot be colored squares other than on the outer edge of the triangle , which means that, beyond a certain integer $N$, the primes will all fall in tens (powers of ten) that can be written in the form $t=\binom{n}{k}$, where $k$ is $2,3,4$ maximum. This may lead to interesting consequences, considering that this property should not change much according to the base, and primes are infinite. As mentioned in the NOTE, this picture is actually incorrect. It should  look like this: Still working on it!","['number-theory', 'reference-request', 'binomial-coefficients', 'combinatorics', 'prime-numbers']"
2896150,Evaluate integral (Chern article),"My question is evaluate some integral of the article ""A Simple Intrinsic Proof of the Gauss-Bonnet Formula for Closed Riemannian Manifolds"" write by Chern. Let's go: If $(M^n,g)$ is a closed even dimension Riemann manifold with $\nabla$ Levi-Civita connection, we can write locally $\nabla_X V = \theta^i(X)e_i$, where $V = v^ie_i$, $\theta^i = dv^i(X) + v^j\omega_j^i$ and $\omega_i^j$ connection forms. In the same way for Riemann curvature , we have $\Omega_i^j$ curvature forms, satisfying $d\omega_i^j = \omega_i^k \wedge \omega_k^j + \Omega_i^j$. So, pulling-back $\theta_i$ and $\Omega_i^j$ by $\rho: SM \rightarrow M$, where $SM$ is the unit-sphere bundle, we define two kind of intrinsic forms in $SM$, namely $$\Phi_k = \sum_{\sigma \in S_n} sgn(\sigma)v_{\sigma(1)}\theta_{\sigma(2)} \wedge \cdots \wedge \theta_{\sigma(n-2k)}\wedge\Omega_{\sigma(n-2k+1)}^{\sigma(n-2k+1)}\wedge \cdots \wedge \Omega_{\sigma(n-1)}^{\sigma(n)}$$ 
and $$ \Psi_k = \sum_{\sigma \in S_n} sgn(\sigma) \Omega_{\sigma(1)}^{\sigma(2)}\wedge\theta_{\sigma(3)}\wedge \cdots \wedge \theta_{\sigma(n-2k)}\wedge\Omega_{\sigma(n-2k+1)}^{\sigma(n-2k+1)}\wedge \cdots \wedge \Omega_{\sigma(n-1)}^{\sigma(n)}$$ $k = 0, \cdots \frac{n}{2} -1 $. It's not too hard to show the following recurrent relation: $$ d\Phi_k = \Psi_{k-1} + \frac{n - 2k - 1}{2(k+1)}\Psi_k$$ Where $\Psi_{-1} \equiv 0$. Define the form, in $M$, $\Omega = \displaystyle (-1)^{\frac{n}{2}-1}\frac{1}{(2\pi)^{\frac{n}{2}}}Pf(\Omega_i^j)$ (called Euler form), definition of Pfaffian polynomial here , obviously $\rho^{*}\Omega = \displaystyle (-1)^{\frac{n}{2}-1}\frac{1}{2^n\pi^{\frac{n}{2}}\left(\frac{n}{2}\right)!}\Psi_{\frac{n}{2}-1}$, write $\Psi_{\frac{n}{2}-1}$ in terms of $d\Phi_k's$ we obtain $d\Pi = \Omega$ in $SM$, with $\Pi =  \displaystyle \frac{1}{\pi^{\frac{n}{2}}}\sum_{t=0}^{\frac{n}{2}-1}(-1)^t \frac{1}{1 \cdot 3 \cdots (n - 2t - 1)t!2^{\frac{n}{2}+t}}\Phi_t$. With some tricks and Stokes' theorem we show $$\displaystyle \int_M \Omega = \sum_{i=1}^{s}ind_{x_s}(\nabla_{g}f)\int_{SM_{x_s}}\Pi|_{SM_{x_s}} $$ for $x_1, \cdots, x_s$ singularities of $\nabla_{g} f$, $f$ a Morse's function. I'd like $\int_{SM_{x_s}} \Pi|_{SM_{x_s}} = 1$ to use the Hopf index theorem. In the paper, Chern claims $$ \int_{SM_{x_s}} \Pi|_{SM_{x_s}} = \frac{1}{1\cdot3 \cdots (n-1)(2\pi)^{\frac{n}{2}}} \int_{SM_{x_s}}\Phi_0|_{SM_{x_s}}$$ and of course it's equal 1, essentially, I don't know why $ \displaystyle \int_{SM_{x_s}} \Phi_k = 0$, for $k \geq 1$. Thanks!","['differential-forms', 'riemannian-geometry', 'differential-geometry']"
2896155,Show that $P(0<X<6)>\dfrac{2}{3}$,"Show that $P(0<X<6)>\dfrac{2}{3}$, if $X$ has probability density function $f(x)=\dfrac{e^{-x}x^2}{2}$, if $x>0$ and $0$, otherwise. Now, $E(X)=3$ and $Var(X)=3$, so I can not use Chebyshev's inequality . Moreover I used the inequality, $e^{-x}\geq \dfrac{x^2}{2}-x+1$, but also this not working here. What inequality I should use here? Thanks for any help.","['statistics', 'probability-theory', 'probability', 'inequality']"
2896205,How to integrate for a countable sum of measures?,"Generalizing this question , when can we integrate in a similar way for a countable sum of measures? $$\int f\,dm=\sum_{n \in \mathbb{N}} a_n \int f\,dm_n$$","['measure-theory', 'probability-theory']"
2896299,$d_{TV}$ VS correlation coefficient.,"Consider two RV $X,Y$. If $d_{TV}(X,Y)=0$ you may couple them in such a way that $$\rho_{XY}=\frac {\operatorname{COV}(X,Y) }{\sigma_X\sigma_Y}=1.$$ So, is there any formula to bound $d_{TV}$ in terms of $\rho$?","['statistics', 'correlation', 'inequality', 'total-variation', 'probability']"
2896310,Linear ordering $\leq$ on $\mathbb{Z}$ in ZFC,"Given a set of natural numbers $\mathbb{N}$ in ZFC, we define $\mathbb{Z}$ by first defining an equivalence relation $\simeq$ on $\mathbb{N}\times\mathbb{N}$: $(n,m) \simeq (n',m') \Longleftrightarrow n + m' = n' + m$. Then we consider the set $(\mathbb{N}\times\mathbb{N})/{\simeq}$ of equivalence classes of $\mathbb{N}\times\mathbb{N}$ with respect to $\simeq$ as the set of integers $\mathbb{Z}$. However, we then need to define a linear ordering $\leq$ on our newfound $\mathbb{Z}$. The idea is to set $[(n,m)] \leq [(n',m')]$ whenever $n + m' \leq n' + m$. I'm having a trouble in showing that this definition doesn't depend on the choie of $(n,m) \leq (n',m')$. That is, that for any natural numbers $n_1,m_1,n_2,m_2,n_3,m_3,n_4,m_4$ if we have
$$n_1 + m_3 \leq n_3 + m_1,$$
$$n_1 + m_2 = n_2 + m_1,$$
$$n_3 + m_4 = n_4 + m_3,$$
then we have
$$n_2 + m_4 \leq n_4 + m_2.$$
Any help would be appreciated.","['elementary-set-theory', 'inequality', 'integers', 'natural-numbers']"
2896321,$C_0^\infty(\overline \Omega)$ is dense in $H(\operatorname{div};\Omega)$,"I've been looking for a while in different Functional Analysis books such as Luc Tartar, Jean Pierre Aubin and Brezis but couldn't find the proof of the density of $C_0^\infty(\overline \Omega)$ in $H(\operatorname{div};\Omega)$ when $\Omega$ is an open bounded subset of $\mathbb R^n$ with Lipschitz boundary $\Gamma:=\partial \Omega$. Can you recommend me a good source to check it?","['book-recommendation', 'sobolev-spaces', 'functional-analysis']"
2896350,"Meaning of ""up to associates"" in unique prime factorization","We know that the Euclidean Domain has the property of Unique Factorization. More precisely, every nonzero element in a Euclidean ring $R$ can be uniquely written (up to associates) as a product of prime elements or is a unit in $R$. The word ""up to associates"" confusing me a bit. P.S. Let's consider the example in the euclidean domain $\mathbb{Z}[i]$ and consider the following prime factorizations such as: $$(2+i)(1+i) \quad\text{and} \quad (-1+2i)(1-i)$$
Note that $2+i\sim -1+2i$ and $1+i\sim 1-i$. Can anyone explain me the meaning of the phrase ""up to associates"" in the above example, please?","['abstract-algebra', 'gaussian-integers']"
2896396,How to find lines of invariant points?,"Every time I try a question on this topic I get it wrong. My textbook says: Invariant points satisfy $B\begin{pmatrix}u\\ v\end{pmatrix}=\begin{pmatrix}u\\ v\end{pmatrix}$ Re-write this as a system of equations. Check whether both equations are in fact the same. If so, they give a line of invariant points. So I tried this question using that method (and the method needed for finding invariant lines): For [this] matrix, find any lines of invariant points and any other invariant lines through the origin. $\begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}$ What I did was: $\begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}\begin{pmatrix}u\\ mu\end{pmatrix}=\begin{pmatrix}u\\ v\end{pmatrix}=\begin{pmatrix}3u-2mu\\ 4u-3mu\end{pmatrix}$ These equations are not the same, so no lines of invariant points. Then, to find invariant lines: $\begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}\begin{pmatrix}u\\ mu\end{pmatrix}=\begin{pmatrix}u'\\ v'\end{pmatrix}=\begin{pmatrix}3u-2mu\\ 4u-3mu\end{pmatrix}$ $4u-3mu=m\left(3u-2mu\right)$ $2m^2-6m+4\:=0\:\Rightarrow \:m\:=1\: $ or $ \:m=2$ So the invariant lines should be $y=2x$ or $y=x$ . As far as I know this is sort of right, except $y=x$ is apparently also a line of invariant points. What did I do wrong?","['matrices', 'geometry', 'linear-transformations']"
2896412,Probability of events with retries?,"Let's say I want to roll $n$ 20-sided dice, and I want none of those dice to be a 1. I figure that the probability at least one die will be a 1 is $\frac{19}{20}^n$. But now let's say that we will re-roll each individual die that is a 1 up to $r$ times. I want to know 2 things: Given the above, what is the probability one or more of the dice will be a 1? Suppose I play this game a million times. How many dice rolls will a given game make on average? In other words, for each game, I will make $n+t$ dice rolls, where $t$ is the number of retries I've made. What would $t$ be on average?","['statistics', 'probability']"
2896438,Some statements about $f''(x)+xf'(x) = \cos(x^3f'(x))$,"Consider $f:\mathbb{R}\to\mathbb{R} \in C^2$ such that $f''(x)+xf'(x)
= \cos(x^3f'(x)) \;\;\forall x \in \mathbb{R}$ Which of the following are correct? (i) If $f$ has a critic point $x_0$, then it is a local minimum. (ii) $\exists r>0$ such that $f$ is concave upwards in $(-r,r)$. (iii) $f$ can have at most one critic point (iv) $f$ is odd (v) $f(x)=0$ has at most two solutions. Here is my attempt: (i) If $f'(x_0)=0$, then $f''(x_0) = \cos(0) = 1>0$, so it is a local minimum. True. (ii) By continuity, as $f''(0)>0$, then there is a neighborhood $B(0,r)$ where $f''(x)>0 \forall x \in (-r,r)$. True. (iii) Suppose $f'(x_0)=f'(x_1)=0$ for $x_0>x_1$. Then there must be a point $c \in (x_0,x_1)$ such that $f''(c)<0$, since both $x_0$ and $x_1$ are local minima and $f$ is continuous. True. (How can I write this precisely?) (iv) Since $0$ is the only critic point of $f$, then $\lim_{|x|\to\infty} f(x) = \infty$, so it can't be odd. False. (How can I write this precisely?) (v) Since $0$ is the only critic point and is not odd, then it can only cross $x$-axis one time left at $0$ and one time right at $0$, at most. True. (How can I write this precisely?) I think I solved this exercise based on geometric intuition, but I'd like to be rigorous. Please help me formalize these ideas.","['ordinary-differential-equations', 'real-analysis']"
2896447,Principal curvatures of surface of revolution,"Consider the following surface of revolution: $$S(t,\theta)=(t,f(t)\cos\theta,f(t)\sin\theta)$$ To calculate its principal curvatures, first we calculate the coordinate tangent vectors, then normalize their cross product to get the unit normal vector $N$ . Finally, we calculate the shape operator using $S\partial_i=-D_{\partial_i}N$ . This is how I find principal curvatures for two dimensional, parameterized surfaces. But if I revolve the graph $(t,f(t))$ along $x$ -axis in $R^{n+1}$ , giving the surface, $$S(t,w)=(t,f(t)w)=(t,f(t)\cos\theta_1,f(t)\sin\theta_1\cos\theta_2,\cdots)$$ where $w$ is a direction in $S^n$ , how do I calculate its shape operator ? In particular, I am interested in its sectional and mean curvatures. My intuition says that the principal curvatures should be the same in all angular directions.","['multivariable-calculus', 'riemannian-geometry', 'differential-geometry']"
2896500,Prove that every open set is lebesgue-measurable,"Let $\mathbb{R}^n\supset{I}=(a,b)=(a_1,b_1)\times...\times(a_n,b_n)$ with $a,b\in{\mathbb{R}^n}$ such that $a_i\lt b_i 
 $$\forall i$.
   So that the outer measure is defined as: $$
\mu^*(A)=inf\{\sum_{i\in{J}}V(I_i)\mid A\subset\bigcup_{i\in{J}}(I_i)\}
$$ Where:
 $\{I_i\}_{i\in{J}}$ is a cover of A. I have already proven that every $I$ is a Lebesgue-measurable set. Now I want to  prove that every open set is Lebesgue-measurable. I have done this, is it correct? Let A be an open set. For every $x\in A$ with $x=(x_1,..,x_n)$ there exist an $r>0 $  such that $B(x;r)\subset A$. If we take $r$ small enough, there exists  $\alpha > 0$ such that: $$
B(x;r)\subset I_x=(x_1-\alpha,x_1+\alpha)\times...\times(x_n-\alpha,x_n+\alpha)\subset A
$$ Then define $A$ with:
$$
  A=\bigcup_{x\in A} I_x
$$
A is union of Lebesgue-measurable sets so A is Lebesgue-measurable.","['measure-theory', 'lebesgue-measure', 'outer-measure']"
2896540,Comparison of no. of abelian groups of order $p^r$ with that of order $q^r$,"[J.B. Fraleigh, Exercises 11, problem 37] Let $p$ and $q$ be distinct prime numbers. How does the number of abelian groups of order $p^r$ compare with the number of abelian groups of order $q^r$ ? For instance, I take $p = 2$, $q = 3$ and $r=3$. Then, 
for $p = 2, r = 3$, we have $\mathbb Z_8$, $\mathbb Z_4 × \mathbb Z_2$ and $\mathbb Z_2 × \mathbb Z_2 × \mathbb Z_2$. Similarly for $q = 3, r = 3$, we have $\mathbb Z_{27}$, $\mathbb Z_9 × \mathbb Z_3$ and $\mathbb Z_3 × \mathbb Z_3 × \mathbb Z_3$. It seems that they have same no. of abelian groups, since it depends on $r$. But I'm not sure if I'm correct. Any help or hint would be appreciated.","['group-theory', 'abstract-algebra', 'abelian-groups']"
2896546,Constructing a function with constant line integral in $\mathbb{R}^n$?,"Suppose, $\Omega \subset \mathbb{R}^n$ is a bounded convex set. If, there is an integrable function $f:\Omega \to \mathbb{R}$ s.t. $$\int_{\Omega \cap \ell} f = 1$$ for every line $\ell \subset \mathbb{R}^n$ with $\mathcal{H}^1(\ell \cap \Omega) > 0$ . Then, can we claim $\Omega$ is a ball and $f$ is a radial function? My main question is: How'd we go about constructing such a function if there exists one? Any help/hint is appreciated! Thanks.","['integration', 'measure-theory', 'geometric-measure-theory', 'analysis']"
2896565,Deciding when to drop the absolute values in differential equation?,"I am currently solving the following differential equation (link is to another post): $\dfrac{dr}{d \theta}+r\tan \theta =\frac{1}{\cos \theta}$ The following is in standard form (i.e. $\dfrac{dr}{d\theta}+P(\theta)r=Q(\theta)$). Therefore, I can go and head and solve for the integrating factor : $\mu(\theta)=e^{\int_{} P(\theta) d\theta}=e^{\int_{} \tan(\theta) d\theta} =e^{-\ln(|\cos(\theta)|)}=|\cos(\theta)|^{-1}$ Multiplying the entire equation by the integration factor allows us to use the "" Derivative of a Product "" property to yield the following: $\dfrac{d}{dx}(|\cos(\theta)|^{-1}r)=|\cos(\theta)|^{-1}\sec(\theta)$ Integrating both sides yields a "" difficult "" integral: $\int_{} \dfrac{1}{|\cos(\theta)|\cos(\theta)} d\theta$ However, according to solution given here , the absolute value is dropped in the integrating factor (thereby creating an easier problem), meaning $\mu(\theta)=(cos(\theta))^{-1}$. But, why am I allowed to drop the absolute value? Nothing in the problem states the domain of $\theta$ or $r$ and clearly, $|\cos(\theta)|\cos(\theta)\neq \cos^2(\theta)$ for all values of $\theta$.","['integration', 'integrating-factor', 'absolute-value', 'ordinary-differential-equations']"
2896631,Prime divisors of the sequence terms $a_n=a\cdot 2017^n+b\cdot 2016^n$,"I am dealing with the test of the OBM (Brasilian Math Olimpyad), University level, 2017, phase 2. As I've said at another topic (question 1), I hope someone can help me to discuss this test. The question 2 says: Taking fixed positive integers $a$ and $b$, show that the set of the prime divisors of the sequence terms $a_n=a\cdot 2017^n+b\cdot 2016^n$ is infinite. The only thing that is on my mind is Dirichlet's Theorem: Given any $k,k'\in\mathbb{Z}$ coprime, the arithmetic progression of reason k' and inicial term k has infinite primes. However, I don't have ideas about how do it. Thanks very much. Edit September, 01 I was searching about recurrences and I found a little about Lucas sequences, it seems important: Lucas sequence","['contest-math', 'prime-factorization', 'sequences-and-series']"
2896641,Find $a$ that makes $f$ continuous and doesn't have extrema,"Find $a$ that makes $f$ continuous and doesn't have extrema: $f(x)=\begin{cases} 
      ax^2 & x\leq 1 \\
      a^2x-2 & x>1 
   \end{cases}$ Here's what I've been doing: $\lim \limits_{x \to 1^-} ax^2=a$ and $\lim \limits_{x \to 1^+}a^2x-2=a^2-2$ Then I found the roots for $a^2-a-2=0$, which are $-1$ and $2$. Ok, so here's my problem, if you plug in both values for $a$, $f(x)$ is continous for both values, so now I have to find which one makes the function not have extrema... (Clearly looking at the graphic, the answer is $-1$, I just don't know how to prove it by ""algebraic means"") What I tried (I don't know if it's correct), is to find the limit as $x \rightarrow \infty$ for both cases: When $a=-1$ $\lim \limits_{x \to \infty} -x^2=-\infty$ and $\lim \limits_{x \to \infty} x-2=\infty$ By this I just assumed that $f(x)$ doesn't have extrema. When $a=2$ $\lim \limits_{x \to \infty} 2x^2=\infty$ and $\lim \limits_{x \to \infty} 4x-2=\infty$ ??? Btw, I can't use derivatives to find the extrema... Thank you :-)","['limits', 'calculus', 'real-analysis']"
2896653,Show that $\ln\left(1+3x + 2x^2\right) = 3x - \frac{5x^2}{2} + \frac{9x^3}{3} -\cdots + \left(-1\right)^{n-1}\frac{2^n+1}{n}x^n+\cdots$,"Show that $$\ln\left(1+3x + 2x^2\right) = 3x - \frac{5x^2}{2} + \frac{9x^3}{3} -\cdots + \left(-1\right)^{n-1}\frac{2^n+1}{n}x^n+\cdots$$ I know that $$\ln(1+x) = \sum\limits_{n=0}^{\infty}\frac{\left(-1\right)^nx^{n+1}}{n+1}$$ My first thought was to use a series expansion, $$\ln\left(1+3x+2x^2\right) = \sum\limits_{n=0}^{\infty}\frac{\left(-1\right)^n(3x+2x^2)^{n+1}}{n+1}$$ and compare this sum with the sum on the right hand side. 
$$\sum\limits_{n=0}^{\infty}\frac{\left(-1\right)^n\left(3x+2x^2\right)^{n+1}}{n+1} = \sum\limits_{n=0}^{\infty}\frac{\left(-1\right)^{n-1}\left(2^n+1\right)x^{n}}{n}$$ From here, I would try and manipulate the expression within the sum to give the expression on the right hand side. I know I can turn the sum on the left into
$$\sum\limits_{n=1}^{\infty}\frac{\left(-1\right)^{n-1}\left(3x+2x^2\right)^{n}}{n}$$
which is slightly closer to the sum on the right. However, when comparing both sides, the expression I get seems quite incorrect since the highest degree on the left side is $x^{2n}$.
$$\left(3x+2x^2\right)^n = \left(2^n+1\right)x^n$$ Where have I gone wrong? How I can better approach this problem?","['trigonometry', 'summation', 'sequences-and-series']"
2896674,Is the space of maps between Hilbert spaces that have at most polynomial growth of order m a separable Banach space?,"I would like to ask about a problem I met: If $U$ and $H$ are two real Hilbert spaces (they are infinite dimensional), let $C_{m}(U,H)$ denote the space of continuous maps from $U$ to $H$ which have at most polynomial growth of order $m$. It is known that for any $u \in U$, if we have $|u|_{C_{m}}$= $sup_{h \in H} \frac {|u(h)|} {1+|h|^{m}}$, then $C_{m}(U,H)$ is Banach space with this norm. I would like to know 1. Is $C_{m}(U,H)$ also separable? 2. If not, is it possible to give it a norm so that $C_{m}(U,H)$ is a separable Banach space? Thank you very much!","['functional-analysis', 'real-analysis']"
2896677,Showing that $\ln(\sec 3t+\tan 3t)=2\tanh^{-1}(\tan(3t/2))$,"I've been trying to show that
  $$\ln(\sec 3t+\tan 3t)=2\tanh^{-1}(\tan(3t/2))$$ I used the identity
$$\tanh^{-1}x=\frac12\ln\frac{1+x}{1-x}$$
to write:
$$\begin{array}{tcl}
2\tanh^{-1}(\tan(3t/2))&=&2\cdot\frac12\ln\dfrac{1+\tan(3t/2)}{1-\tan(3t/2)}\\
2\tanh^{-1}(\tan(3t/2))&=&\ln\dfrac{1+\tan(3t/2)}{1-\tan(3t/2)}\\
\end{array}$$
Then I used
$$\tan\frac{\theta}{2}=\frac{1-\cos \theta}{\sin\theta}$$
to write:
$$\begin{array}{tcl}
2\tanh^{-1}(\tan(3t/2))&=&\ln\dfrac{1+\dfrac{1-\cos3t}{\sin3t}}{1-\dfrac{1-\cos 3t}{\sin 3t}}\\
2\tanh^{-1}(\tan(3t/2))&=&\ln\dfrac{\sin 3t+1-\cos 3t}{\sin 3t-1+\cos 3t}
\end{array}$$ Now, in order to show that
$$\ln(\sec 3t+\tan 3t)=\ln\dfrac{\sin 3t+1-\cos 3t}{\sin 3t-1+\cos 3t}$$
I need to show that
$$\sec 3t+\tan 3t=\dfrac{\sin 3t+1-\cos 3t}{\sin 3t-1+\cos 3t}$$
Now,
$$\begin{array}{tcl}
\sec3t+\tan3t&=&\dfrac{1}{\cos 3t}+\dfrac{\sin 3t}{\cos 3t}\\
\sec3t+\tan3t&=&\dfrac{1+\sin 3t}{\cos 3t}
\end{array}$$ Now, I am stuck. How do I show that:
$$\frac{\sin 3t+1-\cos 3t}{\sin 3t-1+\cos 3t}=\frac{1+\sin 3t}{\cos 3t}$$
Here is some Matlab code that seemingly verifies the identity. t=linspace(-1.5,1/2,1000);
y1=(sin(3*t)+1-cos(3*t))./(sin(3*t)-1+cos(3*t));
y2=(1+sin(3*t))./cos(3*t);
plot(t,y1,'b','LineWidth',4),hold on
plot(t,y2,'r','LineWidth',2)
hold off And the image.",['trigonometry']
2896695,Deciding the parameter value in a ODE system,"I am working with a system that includes pharmacokinetic and pharmacodynamic elements. To model the antibiotic effect on bacteria I am using a commonly used Hill type function as,${(\phi_{max} - \phi _{min}){({A \over MIC})}^k}\over {{({A \over MIC})}^k-{\phi_{min} \over \phi_{max}}}$,
where, $\phi_{max}$ – maximum bacteria growth rate in the absence of antibiotic which is only limited by resources, and so it i limited by the carrying capacity term as,  $\phi_{max}=r(1-{P \over k})$, where $k$ is the carrying capacity term and $r$ is the replication rate, and $P$ is the bacterial density that change over time. $\phi_{min}$ – minimum bacterial growth rate at high concentrations of the antibiotic. $MIC$-minimum inhibitory concentration of the antibiotic $k$ - Hill coefficient. $\phi_{min},k,MIC$ are all constant values. $A$ is the antibiotic concentration which decays exponentially so that ${dA\over dt}=-dA$ So, my question is, in almost all the articles that I found, the value of $\phi_{min}$ was selected as a negative value. But it is defined as a bacterial growth rate. So, how can a growth rate be negative? When this function was used in my model and the $\phi_{min}$ value was changed from, negative values to positive values, for negative values I get the desired behaviour (the pathogen get cleared). But, for some positive values, in Matlab I get a warning as, Unable to meet integration tolerances without reducing the step size below the smallest
value allowed (1.818989e-12) at time t Could this be because, that it is possible to come up with a situation where, $\phi_{min}=\phi_{max}$ and ${({A \over MIC})}^k=1$, so that the above function reaches $0 \over 0$ case. I believe this does not happen when $\phi_{min}$ is negative, does it? In articles, they just chose low negative value for antibiotics that has high killing ability (e.g $\phi_{min}=-3$)and for lesser effective antibiotics $\phi_{min}=-0.01$. I would like to know if there is mathematical reason for choosing $\phi_{min}$ to be negative? Is my reasoning on $0 \over 0$ case reasonable?","['biology', 'mathematical-modeling', 'matlab', 'ordinary-differential-equations']"
2896715,Given a finite automaton determine if it is deterministic and indicate regular expression,"Given the finite automaton: Make the transition table and indicate if it is deterministic or not. Indicate which of the following regular expressions corresponds to the language recognized by the automaton: $0^\ast11\left(1^\ast+01\right)1^\ast$ $0^\ast11{\left(1+01\right)}^\ast$ $0^\ast11{\left(1^\ast01\right)}^\ast$ The state machine $M=(Q,V,\delta,q_0,F)$ where $Q=\{q_0,q_1,q_2\}$, $V=\{0,1\}$, $\delta:Q\times V\to Q$ and $F=\{q_2\}$ has the following table transition:
$$\begin{array}{c|ccc}
\delta&0&1\\\hline q_0&q_0&q_1\\q_1&-&q_2\\q_2&q_1&q_2
\end{array}$$ This finite automaton is deterministic because it has at most one change of state for each letter of the alphabet. Recall that each language has a single regular expression. Since we can go through the $q_2$ loop or go back and forth from $q_1$ to $q_2$ then the correct regular expression is $$0^\ast11{\left(1+01\right)}^\ast\text.$$ Is that correct? Thank you!","['automata', 'regular-expressions', 'discrete-mathematics', 'regular-language']"
2896756,Is here a specific name for the following theorem? (Sides of inscribed squares of a triangle meet at points collinear with a vertex of the triangle),"For any acute and right triangle, two of the intersections(C and B) of the two inscribed squares and the vertex A of the triangle are collinear. I have a proof for the theorem, but I have not found any specific name for it. I searched ""inscribed squares in a triangle,"" and I still could not find the same shape. Can anyone provide a specific name for this, or a way that I can find the theorem online? For the similar question concerning the cubes inscribed in a tetrahedron, please visit Extend squares inscribed in a triangle to the cubes inscribed in a tetrahedron . Proof: $\tan(\angle ACB) = \frac{IK}{KC}$ $\because LG = IK$ $\therefore \frac{LG}{KC} = \frac{IK}{KC} = \tan(\angle ACB)$ Similarly, $\frac{LH}{JC} = \tan(\angle ACB)$ $\because \angle LHO = \angle LGO = \angle OLC = \angle OKC = 90^{\circ}$ $\therefore \angle JCK = \angle GLH = 360^{\circ} - \angle LHO - \angle LGO - \angle GOH = 360^{\circ} - \angle OJC - \angle OKC - \angle JOK $ $\because \frac{LH}{JC} = \frac{LG}{KC} = \tan(\angle ACB), \angle JCK = \angle GLH$ $\therefore \Delta LHG \simeq \Delta CJK $ (SAS) $\therefore \angle LGH = \angle JKC$ $\because \angle LGO = \angle OKC = 90^{\circ}$ $\therefore \angle HGO = \angle OKJ = \angle LGO - \angle LGH = \angle OKC -\angle JKC$ Similarly, $\angle GHO = \angle OJK$ $\therefore \Delta GOH \simeq \Delta KOJ$ (AA) $\therefore \frac{GH}{JK} = \frac{LG}{KC}$ $\therefore \frac{GO}{OK} = \frac{LG}{KC} = \frac{GH}{JK}$ $\because \angle LGO = \angle OKC$ $\therefore \Delta LGO \simeq \Delta CKO$ (SAS) $\therefore \angle GOL = \angle KOC$ Similarly, $\angle HOL = \angle JOC$ $\because \angle IOJ = \angle POK$ $\therefore \angle LOG + \angle IOJ + \angle JOC = \angle COK + \angle POK + \angle HOL = \frac{1}{2}(360^{\circ}) = 180^{\circ}$ $\therefore$ Points C, O, L are collinear $Q.E.D$",['geometry']
2896780,Secret Santa algorithm that does not rely on a trusted 3rd party?,"With a trusted 3rd party, running Secret Santa is easy: The 3rd party labels each person $1,\dotsc,n$, and then randomly chooses a derangement from among all possible derangements of $n$ numbers.  Person $i$ will then give a gift to the number in position $i$ of the derangement.  The trusted 3rd party is responsible for keeping the derangement secure, and for telling each person whom to give a gift to. The question is: Is there an algorithm that would allow Secret Santa to be played without a trusted 3rd party? I thought perhaps a clever use of secret keys and a one way hash function could accomplish it, but I've failed to find an algorithm so far. So I'm looking for a description of a valid algorithm or a (informal) proof that one does not exist. EDIT I believe my problem is different from the possible duplicate.  I want a solution that will work for a distributed group of players.  That is, you cannot assume the players are in the same room and have the ability to shuffle envelopes or notecards or anything like that. To make this concrete, a valid solution must work across a group instant messenger or over group emails. Also, to clarify again, it must be a random derangement and not merely a random n-cycle.","['permutations', 'information-theory', 'cryptography', 'combinatorics', 'algorithms']"
2896788,"How to calculate $\int\frac{x}{x^2-x+1}\, dx$?","$$\int \frac{x}{x^2-x+1}\,  dx = \int \frac{x}{(x-\frac 1 2)^{2} + \frac 3 4}\,  dx = \int \frac{x}{(x-\frac 1 2)^2 + (\frac {\sqrt{3}} {2})^2}$$ Substitute $u= \frac{2x-1}{\sqrt{3}}, du=\frac{2}{\sqrt{3}}dx$: $$\frac {\sqrt{3}} 2 \int \frac{\frac{\sqrt{3}} {2}u + \frac 1 2}{(\frac{\sqrt{3}}{2}u)^2+(\frac {\sqrt{3}}{2})^2} = \int \frac{u}{u^2+1}du  + \frac{1}{\sqrt{3}}\int\frac 1 {u^2+1}du.$$ This gives $$\frac 1 2\log({u^2+1})+\frac{1}{\sqrt{3}}\arctan{u}.$$ 
Substituting back in x yields $$\frac 1 2\log(\frac 4 3x^2-\frac 4 3 x+ \frac 4 3)+\frac 1 {\sqrt{3}}\arctan(\frac{2x-1}{\sqrt{3}})$$ However, according to Wolfram Alpha, the integral should evaluate to $$\frac 1 2 \log(x^2-x+1)+\frac{1}{\sqrt{3}}\arctan(\frac{2x-1}{\sqrt{3}})$$After working some time on the integral, I know how to reach this solution, but I don't understand why my first attempt didn't arrive at the correct answer. Do you see where I went wrong? Thank you for any help!",['integration']
2896793,How to divide a line into 3 equal parts,"Find the points P1(x1,y1) and P2(x2,y2) on the line segment joining A(2,-1) and B(6,5) that divide the line segment into 3 equal parts. I wasn’t able to figure out how to make this line into 3 parts. I got as far as finding the distance between the two being sqrt(52) and the midpoint being (4,2). Is there a certain way to split it into thirds instead of halves?","['euclidean-geometry', 'analytic-geometry', 'vectors', 'geometry']"
2896839,"If $a,b$ are positive integers such that $a^{n}+n$ divides $b^{n}+n$ for every $n$, then $a=b$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I need help with this question. Let $a$ and $b$ be positive integers such that $a^{n}+n$ divides $b^{n}+n$ for every natural number $n$. Show that $a=b$. Any help would be appreciated! Thanks!","['number-theory', 'elementary-number-theory']"
2896854,Coefficient of Determination and Correlation between observed and fitted value in Multiple Linear Regression.,"Consider Multiple Linear Model $$y= X\beta + \epsilon$$ Then using Ordinary Least Square, we get estimate of $\beta$ as $$\hat{\beta} = (X'X)^{-1}X'y$$ And $$\hat{y} = X\hat{\beta}$$ $$SS_{\rm Res}= (y-X\hat{\beta})'(y-X\hat{\beta})$$ $$R^2= 1-\frac{SS_{\rm Res}}{SS_{\rm Total}}  $$ From here how can we show $$R^2 = (\operatorname{Correlation} ( y, \hat{y}))^2$$ This can be easily shown in Simple Linear Model as there is only One explanatory variable but I am not able to show this in Multiple Linear Model.","['linear-regression', 'statistics', 'linear-algebra', 'correlation']"
2896880,Conic with only two real points,I'm trying to think of an example of an irreducible conic in the projective plane over the complex numbers such that it has only two real points and I am having difficulty creating one. Is this even possible?,"['algebraic-geometry', 'quadratics', 'projective-space']"
2896891,Recursive definition of this sequence,"I'm having some trouble finding a recursive definition for the following sequence: $x_0 = \sqrt{1+n}$ $x_1 = \sqrt{1+n\sqrt{1+(n+1)}}$ $x_2 = \sqrt{1+n\sqrt{1+(n+1)\sqrt{1+(n+2)}}}$ $x_3 = \sqrt{1+n\sqrt{1+(n+1)\sqrt{1+(n+2)\sqrt{1+(n+3)}}}}$ and so on (where $n \in \mathbb{N}$). I tried using something like $x_{i+1} = \sqrt{1 + n f(x_i)}$ where $f$ would be a function that sends $n$ to $n+1$ wherever $n$ appears in $x_i$. So for instance $f(x_0) = \sqrt{1 + (n+1)}$ and $f(x_1) = \sqrt{1+(n+1)\sqrt{1+(n+2)}}$. My problem is that I can't figure out an explicit form for $f$. If it's any help, the limit of the sequence is $n+1$.","['sequences-and-series', 'real-analysis']"
2896895,On Infinite Limits,"I am currently learning about infinite limits in Calculus, basically determining the limit of a function as x approaches infinity. However, I am struggling to understand the method being used to find it. Let’s take the function above. The method above seems to be to ignore all the terms with a lower power except the terms with the highest power. Then, because both of the highest terms are x^5, we cross them out and then divide their coefficients, to get a limit of 2/3. I guess you can do this because at infinity, the value of x^5 would be so big that it would dominate all the other values However, I still have a few problems with this method: But on that logic, why should we care that the coefficient of the numerator of is 4 and the denominator’s coefficient is 6? The value of x^5 is so big that it would dominate both of them anyway? At this rate, because it would dominate everything, both the numerator and denominator of every function approaching infinity should be infinity over infinity! So wouldn’t the limit of all infinite functions be 1? This is another example that confuses me. Apparently, when x is infinity, you can ignore the 10, because infinity would dominate the whole function, and therefore the limit would be 0. But this doesn’t make sense to me! Even at infinity, the difference between the 2 would be 10, not 0. No matter how large a number you sub in, the difference between the 2 functions will be 10, and therefore, how can the functions approach 0 as x approaches infinity? I also I get that if you zoom out in the function above, it would truly seem like that function is approaching 0.  But then that wouldn’t be the limit of the function would it? That would be zooming out! Once we zoom back in, we will be able see that the function is sticking at 10, not getting closer to 0! So how can we say that the limit of the function at infinity is 10? Can someone explain the above to me? Can you also not make the explanation too rigorous? I’m just learning Khan Academy Calculus, and still haven’t touched on things like epsilon delta proofs yet. Thank you!","['limits', 'calculus', 'self-learning', 'limits-without-lhopital']"
2896906,Minimizing a functional.,"I am doing some work on Kernel based machine learning where I encountered the following functional - $$\frac{1}{m}\sum_{i=0}^m(y_i-f(x_i))^2+\gamma\lVert f\rVert^2$$ Here $\gamma$ is a positive real number, $x_i,y_i$ are given (these are the training data), and norm is of the function space(Reproducing Kernel Hilbert Space). Now I need to find a function that minimizes the given functional. This is what I came across - ""To minimize the given functional, we take the functional derivative with respect to $f$, apply it to an element $\bar{f}$ of the function space, and set it equal to $0$. We obtain $$\frac{1}{m}\sum_{i=0}^m(y_i-f(x_i))^2\bar{f}(x_i)-\gamma\left<f,\bar{f}\right> = 0$$
How do we arrive at this equation, or how do we take functional derivative? Reference - ""The Mathematics of Learning:Dealing with Data, Tomaso Poggio and Steve Smale""","['functional-analysis', 'functional-calculus']"
2896909,"Proof that the image of a surface, under a diffeomorphism, is also a surface.","The problem: ""If $F:R^3\rightarrow R^3$ is a diffeomorphism and M is a surface in $R^3$, prove that the image F(M) is also a surface. (Hint: If x is a patch in M, then the composite function F(x) is regular, since $(x)^*=F^*x*$.) My attempt at a solution (I saw a related post but it answers in terms of homomorphisms, which I know nothing about): Each point p in F(M) has a unique point q in M such that p=F(q). Around each point q there is a neighborhood that is the image of a proper patch x. Since diffeomorphisms are 1-1 and invertible, the composite map Fx is a proper patch. Since the image of x is a neighborhood of q in M, its image, under F, is in F(M). So there is a neighborhood around p such that F(M) can be expressed as the image of the patch Fx. Thus F(M) is a surface. Have I omitted anything?",['differential-geometry']
2896922,"If $x:[0,a]\to \Bbb{R}^n$ is differentiable, what is $\frac{d}{dt}\Vert x(t)\Vert^2?$","Let $\Vert\;\Vert$ be the Euclidean norm on $\Bbb{R}^n$. Let $x:[0,a]\to \Bbb{R}^n$ be differentiable. How do I define
\begin{align}\frac{d}{dt}\Vert x(t)\Vert^2?\end{align}
Please, I need help on this! A detailed answer would suit me. Thanks a lot!",['derivatives']
2896964,$10$ letters are placed in $10$ addressed envelopes. Find the number of ways such that at most three letters are not in correct envelopes,"$10$ letters are placed in $10$ addressed envelopes. Find how many ways are there such as at most three letters are not in correct envelopes. My try: I divided the Problem into $4$ cases: Case $1.$ If  exactly $0$ letters are not in correct envelopes implies that all are in correct envelopes which can be done in $1$ way. Case $2.$ if Exactly one letter not in its respective envelope , number of ways is $$ \binom{10}{1} \times \binom{9}{1} \times (D_9+D_8)$$ where $D_n$ is a derangement of length $n$ But for the final two cases, the approach is complicated. Any better way?","['permutations', 'derangements', 'combinations', 'combinatorics']"
2896983,Trajectory of submarine chasing ship,"here's my try First I notice that if I can define L for every point then I define the curve and from the figure $$L^2 = x^2 + (y_1-y_2)^2$$
where $y_1$ is the corresponding y coordinate of $S_1$ and $Y_2$ is the y coordinate of the curve So using the fact that $ y'(x)=\tan\theta$ -where $\theta$ is the angel made with the positive direction of the X axis- is the slope of the tangent line to the curve at any point so an equation for L is $y=y'(x)X$ 
Now my whole idea is to find two different representations for L and equate them to each other to get an equation I can solve and I want here to use the arc length formula..so differentiating the equation of L we get
 $$y'= y''(x)X-y'(x)$$ 
now using the arc length formula 
$$\int\sqrt {1+\left(y''(x)X-y'(x)\right)^2}dx$$..I differentiate that with respect to x to get $\sqrt{1+\left(y''(x)X-y'(x)\right)^2}$ Now I  find another formula for the length of L which I get by using the fact that $(y_1-y_2) =-x\tan\theta = -y'(x)X$ and from the pythagorean theorem 
$$L =\sqrt {x^2 + (y'(x))^2x^2} = x\sqrt {1 + (y'(x))^2}$$ then differentiating this we get $$\sqrt {1 + (y'(x))^2} + \frac{xy''(x)}{\sqrt {1 + (y'(x))^2}}$$
then equating these two expressions for the derivative of the length of L we get
$$\sqrt{1+\left(y''(x)X-y'(x)\right)^2} = \sqrt {1 + (y'(x))^2} + \frac{xy''(x)}{\sqrt {1 + (y'(x))^2}}$$ which I can't solve any way I know even after a lot of simplification I know I didn't use the stated fact that they're moving at constant speeds which may simplify it a little bit but I can't seem to relate the two I also can't see how using time as the independent variable here helps one last thing..I want some problem solving advice on how to deal with situations like these not related to this specific problem..situations where the complexity of the model makes it impractical or unsolvable Sorry I know this is quite long..Thanks in advance",['ordinary-differential-equations']
2897003,Blocking directed paths on a DAG with a linear number of vertex defects.,"Let $G=(V,E)$ be a directed acyclic graph.
Define the set of all directed paths in $G$ by $\Gamma$ .
Given a subset $W\subseteq V$ , let $\Gamma_W\subseteq \Gamma$ be the set of all paths $\gamma\in\Gamma$ supported on $V\backslash W$ (i.e all vertices in $\gamma$ belong to $V\backslash W$ ).
Now define $l(W)$ to be: $$l(W)=\max_{\gamma\in \Gamma_W} |\gamma|$$ Where $|\gamma|$ is the number of vertices in $\gamma$ . I want to prove (or disprove) the following claim: ${\bf Claim:}$ For every $\epsilon>0$ and every $k>0$ , there are constants $L$ and $N$ such that  for any directed acyclic graph $G=(V, E)$ satisfying $|V|>N$ with the sum of incoming and outgoing degrees bounded by $k$ , there exists a subset $W\subseteq V$ such that $\frac{|W|}{|V|}<\epsilon$ and $l(W)<L$ . The claim is true for directed trees (see edit 1 for a proof) but the same proof idea fails to work in more general DAGs.
Moreover, the statement fails to be true if we remove the constant degree requirement for $G$ . Indeed, the maximal topological order on vertices indexed from 1 to n can not be ""blocked"" for any $\epsilon>0$ by any set $W$ of size linear in $n$ . Any direction or idea would be welcome. Edit 1: For trees, a standard proof would go like this: For $0\leq i\leq L-1$ , Define $W_L^i$ to be the set of all vertices reachable from the root with a directed path of length $i \pmod L$ . Since the graph is a tree, any such path is uniquely defined for every vertex and therefore, for a given $L$ , the set $\{W_L^i\}_{0\leq i\leq L-1}$ gives a partition of $V$ . Therefore choosing $L=\frac{1}{\epsilon}$ , there is some $i_0$ such that $|W_L^{i_0}|$ is at most $\frac{|V|}{L}=\epsilon |V|$ . It is left to show that every $W_L^i$ is indeed $L$ -blocking, but this is trivial since any step in a directed path down the tree increases the distance from the root by exactly 1, so the longest path containing no vertices from $W_L^i$ has to be of length at most $L-1$ (Connecting 2 adjacent floors in $W_L^i$ ) Edit 2: In general, the claim is true for any DAG for the special case of $\epsilon = \frac{2k}{2k+1}$ and $L=1$ . To see that consider the following algorithm: 1- choose a vertex $v$ in the graph that still has neighbours. Keep $v$ , and remove all of its neighbours (in both directions) from graph 2 - if any non isolated vertex is left, go back to 1. Otherwise exit. The resulting graph is completely disconnected ( $L=1$ ) and we removed at most an $\epsilon=\frac{2k}{2k+1}$ fraction of vertices from the graph.
The claim follows. Edit 3: As Misha Lavrov showed, the previous bound can be made tighter and we can prove the claim for $\epsilon=\frac{k}{k+1}$ .
I discovered that this bound is not tight when the DAG has total degree bounded by 3. In this case, I will prove the claim for any $\epsilon>\frac{1}{2}$ where the previous bound is only $\epsilon=\frac{2}{3}$ .
Define the in-degree and out-degree of a vertex $v$ in $G$ by $in(v)$ and $out(v)$ respectively. From the assumption, for all $v \in V$ , $in(v)+out(v)\leq 3$ . Define 4 sets: $\{V_i\}_{i=0}^3$ by: $$V_i=\{v\in V | in(v)=i\}$$ Obviously, $\{V_i\}_{i=0}^3$ forms a partition of $V$ . Therefore, either of the sets $V_1$ or $V_2$ has cardinality at most $\frac{n}{2}$ . W.l.o.g, assume it is $V_1$ .
Let $G'$ be the subgraph of $G$ induced by $V_2$ . Obviously, for all $v\in V_2$ , $out(v)\leq 1$ and therefore, $G'$ is a disjoint union of directed trees with one vertex sink. Using the proof for trees, for any $\epsilon>0$ ,  we can find a subset $W\subseteq V_2$ such that $\frac{|W|}{|V_2|}<\epsilon$ and $W$ is $\frac{1}{\epsilon}$ -blocking in $G'$ .
Finally, define $W'= V_1 \cup W$ . On one hand, $|W'|$ is upper bounded by $(\frac{1}{2}+\epsilon)n$ and on the other hand $W'$ is $(\frac{1}{\epsilon}+2)$ -blocking since a directed path in $G$ can either stay in $V_2$ and get blocked by $W$ or get outside of $V_2$ and either be blocked by $V_1$ or do one last step from $V_0$ , to $V_3$ or both. This proves the claim. PS. Crossposted at MO.","['graph-theory', 'combinatorics', 'directed-graphs']"
2897035,Expected value of the total waiting time of all passengers catching a train.,"Suppose that we are at time zero. Passengers arrive at a train station
  according to a Poisson process with intensity $\lambda$. Compute the expected
  value of the total waiting time of all passengers who have come to the
  station in order to catch a train that leaves at time $t$. This problem really made me lose confidence in my understand of probability theory. I can't really find where my thinking is wrong. This is how i try to solve it: Let $X(t)$ denote the number of passengers arriving at the train station in the time interval $[0,t)$. Let $S_i$ be the time of arrival of the $ith$ passenger and $T$ the total time waited of all passengers until time $t$. Then:
$$Y=(t-S_1)+(t-S_2).......+(t-S_{X(t)})$$
Since $E(Y)=E(E(Y|X)), S_i \in Gamma(i,1/\lambda)$ and $X(t) \in Po(\lambda t)$ we get: $$E(Y|X)=E((t-S_1)+(t-S_2).......+(t-S_{X}))=tX-\sum\limits_{i=1}^X E(S_i)=$$
$$tX-\sum\limits_{i=1}^X \frac{i}{\lambda}=tX - \frac{X(X+1)}{2\lambda}$$
Then, $$E(Y)=E(tX - \frac{X(X+1)}{2\lambda})=tE(X)-\frac{1}{2\lambda}(E(X^2)+E(X))=$$ 
$$tE(X)-\frac{1}{2\lambda}(Var(X)+(E(X))^2+E(X))$$
Now, setting $E(X)=Var(X)=t\lambda$ i get that $$E(Y)=\frac{\lambda t^2}{2}-t$$ which does not make sense since it allows for negative values for $t<\frac{2}{\lambda}$. The correct answer is $E(Y)=\frac{\lambda t^2}{2}.$ Where am i mistaken?","['probability-theory', 'poisson-process', 'random-variables']"
2897070,Why is this function only differentiable at zero?,"In my analysis class, we had the following function: 
$$f: \mathbb{R} \to \mathbb{R}: x \mapsto \begin{cases}
x^2 & \text{ if } x \in \mathbb{Q}\\ -x^2 & \text{elsewhere}.\end{cases}$$ At zero, I can see that this function is differentiable: indeed, we have that 
$$\lim_{h \to 0}\Big|{\frac{f(h) -f (0)}{h}}\Big| = \lim_{h \to 0}|h| = 0$$
by continuity of the absolute-value function and therefore, the limit of $h \to 0$ of the quotients also equals zero. My course notes then state that this function is nowhere differentiable except at zero. At this point, I only know about continuity and the definition of differentiability. Another example of my course notes used a sequence converging to zero, such that the quotient in the definition of derivative does not converge or has no limit. Therefore, I tried doing the same, but without result... Any hints on how to solve this? Note that I do know that this function is not continuous (except at zero), but I have not seen the result (differentiable implies continuous) yet, so I'm sure the lecture notes do not intend to use this result...","['limits', 'calculus', 'derivatives']"
2897095,$\lim _{x\to \infty \:}|\frac{x+1}{x-2}|^{\sqrt{x^2-4}}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I need help solving the following tough limit , please. $$\lim _{x\to \infty} \left|\frac{x+1}{x-2}\right|^{\sqrt{x^2-4}}$$","['limits', 'calculus']"
2897099,Is the $\mathbb{Z}$-grading of a Clifford algebra basis independent?,"Let $V$ be a finite dimensional vector space over a field $K$ of characteristic $\neq 2$, and let $q \colon V \to K$ be a quadratic form. One of the first things to show when learning the theory of Clifford algebras is that any orthogonal basis $\{ e_1, \dots, e_n \}$ of $V$ gives rise to a basis $\{ e_{i_1} \dots e_{i_k} : 1 \leq i_1 < i_2 < \dots < i_k \leq n \}$ of $\operatorname{C\ell}(V,q)$ as a $K$-vector space. In that way, we obtain a direct sum decomposition
$$
  \operatorname{C\ell}(V,q) = C_0 \oplus \dots \oplus C_n,
$$
where each $C_k$ is generated by the $k$-products $e_{i_1} \dots e_{i_k}$. It is often regarded as a grading, although it is not a grading of a $K$-algebra in the strict sense. Question 1: By looking at simple examples, it appears to me that this grading is independent of the orthogonal basis of $V$ chosen. Is this true? Question 2: The Wikipedia article on Clifford algebras suggests that the map $\operatorname{C\ell}(V,q) \to K$ sending each element to its $C_0$-part is basis independent (of course this would be implied by a positive answer to the first question). Is this true? Is there some kind of basis independent formula? Thank you in advance!","['clifford-algebras', 'abstract-algebra', 'noncommutative-algebra', 'quadratic-forms']"
2897100,Is there a smooth surjective function $f:\mathbb{R}^2 \to \mathbb{R}^3$?,"Is it possible to have three smooth functions of two variables $(x,y) \in \mathbb{R}^2$ $$\begin{array}{rcl}
u &=& f_1(x,y)\\
v &=& f_2(x,y)\\
w &=& f_3(x,y)\\
\end{array}
$$ such that the image $(u,v,w)$ is equal to  $\mathbb{R}^3$ I was thinking about this in relation to parametric families of member curves/functions where the curves transition smoothly when changing the parameters. Then such a family where the member curves are parameterized by three variables $u,v,w$ could be changed into a family where the member curves are parameterized by two variables $x,y$, while still keeping smooth transitions. I tried something like a tangens function e.g. $u = tan(x)$ and $v = \lfloor \frac{1}{2}+\frac{x}{\pi} \rfloor$ but this is not smooth (at every $x = \frac{1+k}{2} \pi$ ) and $v$ is only in $\mathbb{N}$.","['differential', 'multivariable-calculus', 'differential-topology', 'differential-geometry']"
2897192,Evaluating the sum $\sum\limits_{k=0}^{15} (-1)^k \cos^{560} (k\pi/16).$,"To eliminate the pesky $(-1)^k$ term, I have rewritten this as $ S = -\sum\limits_{k=0}^{15} \cos^{560} (k\pi/16) + 2\sum\limits_{k=0}^{7} \cos^{560} (k\pi/8).$ However, neither of these sums are easy to evaluate. I think that the next step would be finding the minimal polynomial of $1, \cos(\pi/8), \dots, \cos(7\pi/8).$ However, this is just a factor of $T_{16}(x)-1$ where $T_n(x)$ is the $n$th degree Chebyshev Polynomial. This route is clearly messy before we even start factoring. The other sum would be even worse to handle. I'm wondering if there are any other approaches for computing $S.$","['trigonometry', 'combinatorics', 'summation']"
2897199,How to take second derivative implicitly,"Let $$y^4 + 5x = 21.$$
What is the value of $d^2y/dx^2$ at the point $(2, 1)$? I’m stuck at trying to work out the second implicit derivative of this function. As far as I can work out, the first derivative implicitly is $$\dfrac {-5}{4y^{3}}$$ How do you take the second derivative implicitly with respect to $x$ when $x$ has vanished? There would be nowhere to plug in $x=2$. What am I missing here?","['calculus', 'implicit-differentiation', 'derivatives']"
2897205,Condition for fractional ideals to be isomorphic in a Dedekind domain,"The question is : Let $R$ be a Dedekind domain and $I,J$ be fractional ideals in $R$. Then $I \cong J$ as $R-$modules if and only if $IJ^{-1}$ is a principal ideal of the quotient field $K(R)$ of $R$ i.e. $[I]=[J]$ in the class group $C(R)$. The hint given was in one direction : if $\phi : I \to J$ is an isomorphism, then I must show  that $\frac{\phi(a)}a$ is constant (doesn't depend on $a$). In the reverse direction , if $IJ^{-1} = (c)$ for some $c \in K(R)$, then since $I$ is fractional and $R$ is Dedekind, $I = \frac{(a,b)}r$ for some $r$, since every ideal is doubly generated in a Dedekind domain, and similarly for $J$. How does this give me an isomorphism? In the forward direction, if we assume first that $I,J$ are integral (i.e. ideals of $R$ itself) then pick $a \in I$ and note that ,$\phi : I \to J$ induces $\phi_m : S_m^{-1}I \to S_m^{-1} J$ (the localizations of $I$ and $J$ at every maximal ideal $m$, particularly those $m_i$ which contain $a$). Now, $S_{m_i}^{-1}(a)$ is mapped to some $S_{m_i}^{-1}(b)$ since $S_m^{-1}R$ is a discrete valuation ring. But I am not able to bring in $\frac{\phi(a)}a$ anywhere : how do I bring the quotient in, in the first place?","['number-theory', 'algebraic-number-theory']"
2897232,"If $\Vert f(x)-f(y) \Vert\leq k\Vert x-y\Vert,\;\;\forall\;x,y\in\Bbb{R}^n,$ then $x(t,x_0)=x_0,\forall \;t\geq 0,$","We consider the following O.D.E \begin{align}(1)\;\;\;\begin{cases}x'(t)=f(x(t)) & t\geq 0,\\x(0)=x_0\in \Bbb{R}^n&\end{cases}\end{align}
where \begin{align}f: \Bbb{R}^{n}\to \Bbb{R}^{n}\end{align}
Assuming that \begin{align}\Vert f(x)-f(y) \Vert\leq  k\Vert x-y\Vert,\;\;\forall\;x,y\in\Bbb{R}^n.\end{align}
Assuming that $f(x_0)=0.$ Please, do I prove that $x(t,x_0)=x_0,\forall \;t\geq 0,$ where $x(\cdot\,,x_0)$ is the solution of $(1)$. If there are references, I also would appreciate!","['calculus', 'ordinary-differential-equations']"
2897238,Justifying casual claims in algebraic topology [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Especially when you study algebraic topology, you'll encounter a lot of casually stated claims, involving homeomorphisms, quotient space constructions, or deformation retracts. One very famous example would be the identification of $S^n$ as the quotient of $D^n$ by its boundary $S^{n-1}$. Some people may well accept this fact simply by imagining the case $n=2$ where a disc 'wraps up' to become a sphere, and then by hand-waving. More cautious people might look for an explicit formula for a homeomorphism, which is not so difficult in this example. However, as I am not even close to a genius topologist, I occasionally face topological claims which are neither easily visualized in mind (at a first glance) nor easily proved by a detailed formula. For instance, the following examples in Hatcher's Algebraic Topology do not resonate with my (inborn) intuition. $\cdot~ \mathbb{R}^3 - S^1$ is homeomorphic to a $\epsilon$-neighborhood of $S^1 \vee S^2$ embedded in $\mathbb{R}^3$. $\cdot~$ An orientable surface of genus $g$ is obtained by identifying pairs of edges from a polygon with $4g$ sides. $\cdot~$ Given a 2-simplex $[v_0, v_1, v_2]$, identifying the edges $[v_0,v_1]\sim [v_1,v_2]$ produces a Mobius band. Please note that I am not asking for visualization or formal proofs for these specific examples. To me they are somewhat justified but not perfectly. That is, I do have some ways to explain each of these to myself, but I do not consider them as satisfying as a well-written proof in analysis. However, I also don't want to spend much time finding a rigorous argument each time I come across something like these. Believing that I am not the only person in the world feeling this sort of awkward dissatisfaction, I would be very happy to learn his/her attitude or opinion on my worries from someone who had gone through a similar problem. Edit2:
As Theoretical Economist provided a better paraphrase of the question, I would like to include it.
Would there be a strategy to justify non-intuitive topological claims in a mathematically satisfactory (or acceptable) way, but not writing down all the tedious details?
I think there should be some common way of reasoning, considering how many topological statements there are which everyone agrees with but no one actually writes the proof down. Edit:
So my question is, to those who are experienced in topology:
Haven't you ever felt uncomfortable with some casual arguments because they weren't intuitively so clear to you? In cases when you felt so, did you always bother to 'rigorously' prove the argument?
I am asking this because I am not good at quickly understanding or coming up with intuitive topogical imagination, and also believe that it does not significantly hinder studying and understanding core ideas of AT, but it still disturbs me sometimes. Thanks in advance!","['geometric-topology', 'general-topology', 'soft-question', 'algebraic-topology']"
2897272,Dual space of $R^n$ with $\max$ norm,"So I think it's supposed to be the $(R^n,||.||_1)$, but I can't get one of the inequalities: 
So if $\lambda_k = f(e_k)$, for $f$ in the dual, $x = (x_1,...,x_n) \in R^n$ and $\{e_k\}$ basis, then:
$$|f(x)|  \leq \sum|x_k||\lambda_k| \leq \max|x_k|\sum|\lambda_k|$$
Thus $\||f||\leq \sum |\lambda_k|$
But I don't see how to get the reverse inequlity...","['normed-spaces', 'linear-algebra', 'functional-analysis', 'dual-spaces']"
2897386,Does value of differentiation as infinity implies continuity?,"I come across Mean value theorem proof which is attached below Which has an assumption that f has derivative (finite or infinite ) at each interior point and continuity is assumed at endpoint. In proof assumption used that function f is continuous over whole interval. But I am not convinced with fact that f has infinite derivative and continuous at that point . I tried to use definition I get $|f(x)-f(y)|=|x-y|f'(c)$ one side is infinite how to show for continuity. Also Is it possible to have function which every where derivation infinity? I think question is wrong As I could not imagine function every where like a verticle line. But Asking for in case exist?
Any help will be appreciated Edit: Sorry Everyone As In book already specified definition which already assume continuity of function to define the derivative.","['continuity', 'derivatives', 'real-analysis']"
2897396,"In which cases, there is no continuous map from A onto B?","(a) $A=[0,1]\cup[2,3], B=\{1,2\}$ (b) $A=(0,1), B=[0,1]$ (c) $A=\mathbb{Q}, B=\mathbb{Q}$ (d) $A=(0,1)\cup(2,3), B=\{1,3\}$ It was clear for (b) as it was already asked numerous times on this site. For (c), I took identity map. For (d), We can send $(0,1)$ to $1$ and $(2,3)$ to $3$. Map is clearly onto and into a discrete space. It is continuous as inverse image of each singelton is open. What about (a)?","['general-topology', 'real-analysis']"
