question_id,title,body,tags
3673899,"Sup norm of Fourier transform of $ \frac{\sin |x|}{|x|^\lambda} \mathbb 1_{\{2^k\le |x| <2^{k+1}\}}, \ 0<\lambda<n $","It seems to me that in a paper of Charles Fefferman ( open access ), it is claimed in the introduction that (3rd page of the PDF file, 'page 11', $\lambda\in(0,n)$ ) $$\sup_{\xi\in\mathbb R^n}\left| \int_{2^k\le  |x| <2^{k+1}} \frac{\sin |x|}{|x|^\lambda}  e^{2\pi i x \cdot\xi} \, dx \right| \le 2^{(\frac{n+1}2-\lambda)k}.$$ (He claims that Plancharel's formula should be used, I expect this to mean that we should compute the $L^\infty$ norm of the symbol.) By using asymptotics of Bessel functions, I can get the upper bound for $\xi\gg 1$ , but I am worried about $\xi\approx 0$ . At $\xi=0$ , the Fourier transform is just $\int\frac{\sin\dots}{\dots} dx$ leading to the estimate ( $C_n$ is the volume of the unit $n$ -sphere) $$ \left| \int_{2^k\le  |x| <2^{k+1}} \frac{\sin |x|}{|x|^\lambda}   \, dx \right|  \le C_n  2^{(n-\lambda)k} $$ which is bigger for $n>1$ ? Am I missing something simple? Calculation details can be provided on request.","['fourier-analysis', 'harmonic-analysis', 'fourier-transform', 'functional-analysis', 'bessel-functions']"
3673913,"Is there a way to give an injective function from the set of all finite natural sequences to $\mathbb{N}$, without relying on prime numbers?","I'm trying to prove that $|\bigcup_{k\in\mathbb{N}}\mathbb{N}^k|=|\mathbb{N}|$ . I have an idea, but in order for it to work, I must define, rather conviniently, a function $$G\colon\bigcup_{k\in\mathbb{N}}\mathbb{N}^k\to\mathbb{N}$$ that, according to what I have found is needed, must be injective among (probably) other things. The problem here is that for as long as I've been trying, nothing comes to my mind as to how to do it without relying on prime numbers. That said, I'd like to know if it is even possible to define an injective (let alone all the other properties that I surely need) function without the use of prime numbers. Please note that I'm not looking for a full answer, maybe just a hint if it is actually the case. Thanks in advance.",['elementary-set-theory']
3673915,If $f$ is a bijection of $A$ onto $B$ show $f^{-1}$ is a bijection of $B$ onto $A$,"If this is a duplicate you can close this post. Introduction to Real Analysis (Robert G. Bartle) 18. (b) If $f$ is a bijection of $A$ onto $B$ , show that $f^{-1}$ is a bijection of $B$ onto $A$ . Attempt: If $f$ is a bijection $A$ onto $B$ then if $x\in A$ then $f(x)\in B$ and if $x_1\neq x_2$ then $f(x_1)\neq f(x_2)$ . By definition of inverse, $f^{-1}:=\left\{(f(a),a): B\times A, (a,f(a))\in f \right\}$ , so $f^{-1}$ is a surjection from $B$ to $A$ . Moreover if $f(x_1)\neq f(x_2)$ then $x_1\neq x_2$ implying injection since $f$ is already injective hence $f^{-1}$ is injective. Since $f^{-1}$ is injective and surjective $f^{-1}$ is also bijective. I doubt Iâ€™m correct. For one, I want the final step to be if $x_1\neq x_2$ then $f^{-1}(x_1)\neq f^{-1}(x_2)$ . How do we do this?","['elementary-set-theory', 'alternative-proof', 'real-analysis']"
3673960,Converse of Borel-Cantelli,"My Prob training is rusty now.... Borel Cantelli states for part 1:
If $$ \sum_{n=1}^\infty P(E_n)<\infty$$ Then $$P(E_n\text{ occurs infinitely often}) = 0$$ Is the reverse true?
i.e. if: $$P(E_n\text{ occurs infinitely often}) = 0$$ can we say $$ \sum_{n=1}^\infty P(E_n)<\infty$$","['borel-cantelli-lemmas', 'probability-theory', 'probability']"
3673965,"Request for reference to the (inexact?) quote ""The introduction of coordinates to geometry was an act of aggression""","I heard this quote years ago in a class but I can't find any reference to it or who said it and I'd greatly appreciate any information behind it, as I remember it, it goes; ""The introduction of coordinates to geometry was an act of aggression"" I don't think it's exactly that but it's something in that vein. Sorry if this is vague but I can't seem to find any reference to it. Thanks in advance!","['geometry', 'reference-request']"
3674085,How can I compute the correlation coefficient iteratively?,"I'd like to compute the Pearson Correlation Coefficient (PCC) on-line i.e, deriving the PCC for n samples, then updating it with observation n+1.","['statistics', 'correlation']"
3674092,Which vectors have unique representation when it isn't a direct sum?,"I know that $V = U \oplus W$ means that every $v \in V$ can be written uniquely as $v = u + w$ for some $u \in U, w \in W$ . However, what happens if $V = U + W$ is not direct? Then this means that some vector $v$ does not have a unique representation. But can we say exactly which vectors have a unique representation, and which don't? Is this even a useful question? Intuitively, I am thinking ""sometimes we don't have a direct sum, but perhaps we can still work with what we've got. In particular, let's see which vectors we can write uniquely, and maybe we can work with those.""",['linear-algebra']
3674239,"Prove that there exists $c\in[0,1]$ such that $\int_0^cf(t)dt=f(c)^3.$","Question: Let $f:[0,1]\to\mathbb{R}$ be a continuous function with $\int_0^1f(t)dt=0$ . Prove that there exists $c\in[0,1]$ such that $$\int_0^cf(t)dt=f(c)^3.$$ Solution: Let $g:[0,1]\to\mathbb{R}$ be such that $$g(x)=\int_0^xf(t)dt-f(x)^3, \forall x\in[0,1].$$ Now since $f$ is continuous $\forall x\in[0,1]$ , thus, by the first fundamental theorem of calculus, we can conclude that $g$ is continuous $\forall x\in[0,1]$ . Thereafter, observe that $g(x)=0$ for some $x\in[0,1]\iff \int_0^xf(t)dt=f(x)^3$ for some $x\in[0,1]$ . Hence, to prove the statement of the problem it is sufficient to show that $g(c)=0$ for some $c\in[0,1]$ . Now $g(0)=-f(0)^3$ and $g(1)=-f(1)^3$ . Observe that if $f(0)$ and $f(1)$ are of different signs, then $g(0)$ and $g(1)$ are also of different signs, in which case, by IVT we can conclude that $\exists c\in(0,1)\subset[0,1],$ such that $g(c)=0$ . Hence, we are done in this case. Again, if $f(0)=0$ or $f(1)=0$ , then at least one of $g(0)$ and $g(1)=0$ , in which case we are done. Now, we are left with the case that both $f(0)$ and $f(1)$ are of the same sign. Thus, let us assume WLOG that $f(0)>0$ and $f(1)>0$ . Hence, $g(0)<0$ and $g(1)<0$ . Now since $\int_0^1f(t)dt=0$ and $f(0),f(1)>0$ , implies that $\exists$ at least two points $a,b\in(0,1)$ , such that $b>a$ satisfying $f(a)=f(b)=0$ . Thus, we can conclude that $\exists c_1\in(0,1),$ such that $f(x)>0, \forall x\in[0,c_1)$ and $f(c_1)=0$ . Hence, we have $$g(c_1)=\int_0^{c_1}f(t)dt-f(c_1)^3=\int_0^{c_1}f(t)dt>0.$$ Thus, we have $g(c_1)>0$ and $g(1)<0$ , which implies that, by IVT we can conclude that $\exists c\in(c_1,1)\subset[0,1]$ , such that $g(c)=0$ . Hence, we are done in this case too. Hence, we are done with all the cases and in each case we have shown that $\exists c\in[0,1]$ such that $g(c)=0$ . Thus, we are done. Is this solution correct and rigorous enough? If yes, is there any alternative solution?","['solution-verification', 'real-analysis']"
3674256,Solve definite integral $\int_0^{2\pi} \frac{\cos^2x}{(1+b\cos x)^4} dx$,"I am struggling with analytically solving the definite integral $$\int_0^{2\pi} \frac{\cos^2x}{(1+b\cos x)^4} dx$$ I am more generally having issues with solving integrals of the form $\int_0^{2\pi}\frac{1}{(1+b\cos(x))^a} dx$ , how can I solve these (both numerically and analytically. ( $b<1$ ))","['integration', 'trigonometry', 'definite-integrals', 'trigonometric-integrals']"
3674276,How to prove this algebraic version of the sine law?,"How to solve the following problem from Hall and Knight's Higher Algebra ? Suppose that \begin{align}
a&=zb+yc,\tag{1}\\
b&=xc+za,\tag{2}\\
c&=ya+xb.\tag{3}
\end{align} Prove that $$\frac{a^2}{1-x^2}=\frac{b^2}{1-y^2}=\frac{c^2}{1-z^2}.\tag{4}$$ (I suppose that $x,y,z$ are real numbers whose moduli are not equal to $1$ .) I discovered this problem from chapter 3 of Prelude to Mathematics by W. W. Sawyer. Sawyer thought that this problem arose from the sine law: let $a,b,c$ be respectively the lengths of the edges opposite to three vertices $A,B,C$ of a triangle. Define $x=\cos A$ and define $y,z$ analogously. Now equalities $(1)-(3)$ simply relate $a,b$ and $c$ to each other by the cosines of the angles and $(4)$ is just a rewrite of the sine law $$
\frac{a}{\sin A}=\frac{b}{\sin B}=\frac{c}{\sin C}.
$$ However, the algebraic version $(4)$ looks more general. For example, it does not state that $a,b,c$ must be positive or that they must satisfy the triangle inequality. Sawyer wrote that this isn't a hard problem, but he didn't provide any solution. I can prove $(4)$ using linear algebra. Suppose that $(a,b,c)\ne(0,0,0)$ (otherwise $(4)$ is obvious). Rewrite $(1)-(3)$ in the form of $M\mathbf a=0$ : $$\begin{bmatrix}-1&z&y\\ z&-1&x\\ y&x&-1\end{bmatrix}\begin{bmatrix}a\\ b\\ c\end{bmatrix}=0.$$ Since $x^2,y^2,z^2\ne1$ , $M$ has rank $2$ and $D=\operatorname{adj}(M)$ has rank $1$ . Hence all columns of $D$ are parallel to $(a,b,c)^T$ and $\frac{d_{11}}{d_{21}}=\frac{d_{12}}{d_{22}}=\frac{a}{b}$ . Since $M$ is symmetric, $D$ is symmetric too. Therefore $\frac{1-x^2}{1-y^2}=\frac{d_{11}}{d_{22}}=\frac{d_{11}d_{12}}{d_{21}d_{22}}=\frac{a^2}{b^2}$ , i.e. $\frac{a^2}{1-x^2}=\frac{b^2}{1-y^2}$ . As this problem comes from Hall and Knight's book, I think there should be a more elementary solution. Any ideas?","['alternative-proof', 'algebra-precalculus', 'systems-of-equations', 'trigonometry']"
3674286,How are the different definitions of a tensor the same definition?,"I'm mostly self taught in undergraduate physics and maths, so I haven't had much of an education in this stuff. Basically, I know how the theory of general relativity is formulated in terms of tensors and tensor equations. You equate components of the curvature tensor with components of the energy-momentum tensor, as in $R_{\mu \nu} -\frac{1}{2}Rg_{\mu \nu} = \frac{8\pi G}{c^4}T_{\mu \nu}$ . But I wasn't really satisfied with this, because of how much it seemed to depend on the components of the tensor. Hence I started to research other definitions of tensors (after all, vectors can be defined completely independently of components). What I found was that tensors are defined as multilinear maps: \begin{align} T: V^*\times \cdots \times V^*\times V\times \cdots \times V\rightarrow \mathbb{R}.\end{align} Now I understand all of the terms in this definition (e.g. dual spaces, direct products), but I don't know how this relates to the definition I've seen before in physics, as objects that are invariant under coordinate transformations: \begin{align}(T')^{m_1 \cdots} _{n_1 \cdots} = \frac{\partial (x')^{m_1}}{\partial x^{p_1}}\cdots \frac{\partial x^{q_1}}{\partial (x')^{n_1}}\cdots T^{p_1 \cdots}_{q_1 \cdots}.\end{align} So my question is this: how do these definitions relate to each other? They seem completely different, and I'm not even sure how to think of a ""physics tensor"" as a multilinear map to $\mathbb{R}$ . Also, if you're knowledgeable in physics, is there a form of the Einstein Field Equations that is completely independent of coordinates?","['physics', 'tensors', 'differential-geometry']"
3674375,The maths of 3D printing: what makes a set of $\mathbb R^3$ 3D-printable?,"Context (although it's not really important): For a project, I may have to print some complicated 3D sets. The only thing I know about these sets is that they are defined as bounded parts of $\mathbb R^3$ , are formed as a union of tiny balls, and may not be connected (I know that it is vague, but I'm also asking this question to modify the properties of this set to fit the criterion that allows a part of $\mathbb R^3$ to be 3D-printable). The question: So I'm looking for references about maths modelisations of 3D printing (only classics FDM/FFF 3D printers). I want to see whether it is possible to approximate my complicated set with a 3D printable one. So here I am looking for some reference about the maths of 3D printing in general. More specifically, I would like to answer the following questions: What kind of vector space and topology may define a ""3D printable"" set? (or what kind of structure may define this properly?) Given any bounded set, how to find the ""best"" (for some criteria that may depend on the problem) 3D printable approximation? Further details: For example, given a compact set over $\mathbb R^3$ , what makes it 3D printable? To make more sense, what criteria it must meet to have a 3D printable set that is a good approximation? It seems self-evident that a non-connected set will not be 3D-printable. But are all connected bounded set printable for a high enough resolution? It also seems evident that it depends on the resolution and the precision of the printer. But how to characterise it mathematically? $\longrightarrow$ According to the answer from @Ihf, they are generally triangulated polyhedra. I need some more information about it: what structure may we have over the set of triangulated polyhedra? Is there any good reference about it that I may read? If $\mathcal T\mathcal P$ denotes this set, is there any good description of $\overline{\mathcal T\mathcal P}$ ? (the closure of it in $\mathbb R^3$ as the usual metric space). This may give a good description of ""3D printable objects"" which might be $\overline{\mathcal T\mathcal P}$ (objects that are approximable with triangulation) or a subset of it (if we want some physical propriety to be verified). EDIT: I do not have any example, because of the reasons that I just added in the first paragraph of this post. I also edited the title to be more clear: I do not want to print something to illustrate some mathematical ""classic"" forms as suggested in a comment, but to print a set that is generated by iterated function systems for my case (I'm studying them in a general view, so I don't know yet which form I will try to print. That why I'm looking for criteria that it'd have to meet). EDIT2: added further details for question 1. Final edit: Since this subject seems not to be very documented with mathematical approaches, I accept the first answer I got, that answers my question. So to sum up, the set $\mathcal{TP}$ defined in my post answer the problem. And thus, the well approximable sets are the ones in $\overline{\mathcal{TP}}$ since we can approach them with any precision with 3D printable sets. I'm still interested though about good references on the mathematical description of 3D printing, so if you had any, do not hesitate to add another answer to complement the accepted one.","['general-topology', 'vector-spaces', '3d', 'reference-request']"
3674413,Limit $l=\lim_{t\to^{-}} (1-t) \sum_{r=1}^{\infty} \frac{t^r}{1+t^r}$,"Evaluate $$l=\lim_{t\to1^{-}} (1-t) \sum_{r=1}^\infty\frac{t^r}{1+t^r}$$ My solution: $$l=\lim_{t\to1^{-}} \frac{(1-t)}{\ln(t)}\cdot \ln(t)\sum_{r=1}^{\infty} \frac1{1+t^{-r}}$$ $$=\lim_{t\to1^{-}}-\ln(t) \sum_{r=1}^{\infty}\frac1{1+e^{-r\ln(t)}}$$ Let $-\ln(t)=\frac1n,$ as $t\to1^{-},\ n\to+\infty$ So $$l=\lim_{n\to+\infty}\frac1n \sum_{r=1}^{\infty}\frac1{1+e^{r/n}}$$ $$=\int_{0}^{1}\frac{dx}{1+e^x}=\ln \left(\frac{2e}{1+e}\right)$$ Is there any other way to do this question?","['limits', 'calculus', 'definite-integrals']"
3674415,A 6-sided fair die is rolled 24 times. Let $X$ is the sum of all results. Find the probability $P(X \geq 86)$.,How can I proceed with this problem? I started by finding z score in P: $P(X â‰¥ 86) = P(Z > (86-24)/Ïƒ)$ . Is that correct way?,"['statistics', 'probability']"
3674506,"Finding subgroups of $G=\langle x,y,z~|~x^2y^2z^2 \rangle$ using covering space theory","Consider the group $G$ having a presentation $G=\langle x,y,z~|~x^2y^2z^2 \rangle$ . I am trying to find all subgroups of $G$ of index 6 using covering space theory. It is well-known that the connected sum $X=3\Bbb RP^2$ of three projective planes has fundamental group isomorphic to $G$ . Also for each subgroup of $G=\pi_1(X)$ , there is a covering space $p:\tilde{X}\to X$ such that $p_*(\pi_1(\tilde{X}))=H$ , and if the index $[G:H]$ is $n$ , then $p$ is $n$ -sheeted. Thus the question reduces to find all $6$ -sheeted covering spaces of $X$ , but I can't see a way because I've never seen a covering spaces of connected sums. Any hints?","['fundamental-groups', 'homotopy-theory', 'covering-spaces', 'group-theory', 'algebraic-topology']"
3674529,"Are these symbols really used in ""Set Theory""?","I am confused. Please, help me! I am studying ""Set Theory"" and I am really not trusting in the quality of the material provided and since I am not a math expert I decided to ask in order to clear any doubts lest I learn something the wrong way. Are the symbols below REALLY used in set theory: Above is an IMAGE to make sure you are seeing the same thing as I am (I know fonts may be decoded incorrectly). Thank you very much!","['elementary-set-theory', 'notation']"
3674561,"Is there measurable function f: $\mathrm\forall a\int_{0}^{1}{\frac{1}{f(x)-a}}\, d\mu<\infty$?","I would like to find measurable function f: $\mathrm\forall a\int_{0}^{1}{\frac{1}{f(x)-a}}\, d\mu<\infty$ , but I can't find any such function. I know, that f can't be continuous. If f is continuous, we can prove that such integral is not finite by contradiction, using Fatou and Fubini's theorem. However I can't prove there is no such function, if f can be non-continious. Could anybody help me?","['functions', 'lebesgue-integral', 'real-analysis']"
3674622,Does $\sum_{n=1}^{\infty} a_n$ is absolutely convergent $\Rightarrow$ $\sum_{n=1}^{\infty} a_n\sin(nx)$ is absolutely and uniformly convergent?,"I'm proving that if $\sum_{n=1}^{\infty} a_n$ is absolutely convergent $\Rightarrow$ $\sum_{n=1}^{\infty} a_n\sin(nx)$ is absolutely and uniformly convergent. I defined for a set $\mathbb{D} \subset \mathbb{R}$ the following sequence of functions: \begin{equation*}
f_n: \mathbb{D} \to \mathbb{R},
\end{equation*} \begin{equation*}
\phantom{1000}x \mapsto a_n\sin(nx)
\end{equation*} Since $-1 \le \sin(nx) \le 1, $ then $\forall n \in \mathbb{N}, \forall x \in \mathbb{R} : |a_n\sin(nx)| \le |a_n| $ and $\sum_{n=1}^{\infty} a_n$ is absolutely convergent Therefore apliying M-Weierstrass test we got that $\sum_{n=1}^{\infty} a_n\sin(nx)$ is absolutely and uniformly convergent. Is my reasoning correct?","['analysis', 'real-analysis', 'absolute-convergence', 'sequences-and-series', 'uniform-convergence']"
3674651,Barycentric Projection,"Hi I'm reading Ambrosio : Gradient Flows in Metric Spaces 2nd Edition. Let $X$ be a Polish space and let $\mathcal{P}(X)$ be Borel probability measures on $X$ , analogously define $\mathcal{P}(X\times X)$ . Let $\gamma \in \mathcal{P}(X\times X)$ have 1st marginal $\mu\in \mathcal{P}(X)$ , and admit the following disintegration w.r.t to $\mu$ : $$ \gamma=\int_X \gamma_{x_1}d\mu(x_1). $$ Then Ambrosio defines (Page 126 definition 5.4.2) the barycentric projection $\overline{\gamma}:X \to X$ as $$\overline{\gamma}(x_1)=\int_X x_2 d\gamma_{x_1}(x_2).$$ I'm REALLY struggling to see the meaning of this projection/what role it plays. To me this projection just maps an element in $X$ to ""the $\gamma$ conditional distribution (conditioned that the first element is $x_1$ ?) "". Does anyone have some more knowledge about how to view this projection ?","['measure-theory', 'convex-optimization', 'optimal-control', 'optimal-transport', 'barycentric-coordinates']"
3674713,Simplifying Using the Laws of Equivalence,"I am struggling with Simplifying Using the Laws of Equivalence and I'm unsure how to tackel larger 
equations (i.e. what order to simplify) for example show that $((A â†’ B) âˆ¨ (Â¬A â†’ C)) â†’ (B âˆ¨ C) â‰¡ B âˆ¨ C$ I think i must be applying the laws in the wrong order as I get them all to cancel out (like $P$ or not $P$ therefore true) Any help would be appreciated","['logic', 'discrete-mathematics']"
3674717,Some Combinatorics and Some Prime Numbers,"Problem statement: Let $U=\{1,2,...n\}$ and $S$ be the set of all permutations of the elements of $U$ . For any $f \in S$ let $I(f)$ denotes the number of inversions (see remark) of $f$ . Let $A_j$ denotes the number of permutations $f$ in $S$ such that $I(f)\equiv j\pmod{p}$ $[0\leq j \leq p-1]$ where $p$ is an odd prime number. Then prove that $$A_1=A_2=A_3=\ldots=A_{p-1} \Leftrightarrow p\leq n.$$ My solution to this problem uses roots of unity (as I have posted the answer ). I want to find other solutions. Remark For a permutation $\sigma$ of $\{1,2,\ldots,n\}$ we call a pair $(i,j)$ an inversion in $\sigma$ if $i<j$ but $\sigma(i)>\sigma(j)$ .","['number-theory', 'alternative-proof', 'combinatorics', 'polynomials', 'prime-numbers']"
3674724,A special case of double derangement with $ k-1 \ne\sigma(k) \ne k $,"How many permutations on a set $[n]$ does there exist such that : $$\forall k \in [n]:\sigma(k) \ne k \;\;\;\text{and}\;\;\;\forall k \in [n]\setminus\left\{1\right\}: \sigma (k) \ne k-1\;\;\;\text{and}\;\;\;\sigma (1) \ne n$$ Indeed we are looking for the derangement such that there does not exist any elements mapped to the element before itself (with the assumption that $\sigma (1) \ne n$ ). Define: $$\overline A_{}:=\left\{\sigma \in S_{n}:\forall k \in [n]:\sigma(k) \ne k\right\}$$ $$\overline B_{}:=\left\{\sigma \in S_{n}:\forall k \in [n]\setminus\left\{1\right\}:\sigma(k) \ne k-1 \;\;\;\text{and}\;\;\;\sigma (1) \ne n \right\}$$ What we want can be derived using inclusion-exclusion : $$n!-\ \left| A_{} \cup  B\right|$$ $$=n!-\left| A_{} \right|-\left| B_{} \right|+\left| A \cap B_{} \right|$$ $$=n!-\bigcup_{k=1}^{n}\left|A_k\right|-\bigcup_{k=1}^{n}\left|B_k\right|+\bigcup_{k=1}^{n}\left|A_k\right| \cap \bigcup_{k=1}^{n}\left|B_k\right|$$ Where $A_k,B_k$ is the set of permutations with their $k$ th term fixed. From the number of derangement we conclude that : $$\bigcup_{k=1}^{n}\left|A_k\right|=n!\sum_{i=1}^{n}\frac{\left(-1\right)^{i+1}}{i!}=\bigcup_{k=1}^{n}\left|B_k\right|$$ However for $\bigcup_{k=1}^{n}\left|A_k\right| \cap \bigcup_{k=1}^{n}\left|B_k\right|$ I could not find any closed form,even I could not finish the computation,besides I'm not sure if I've done the other part correctly. Note : What the question wants is a special case of a double derangement with two permutations $\text{id}(k),\pi(k) \in S_n$ such that $\text{id}(k)=k$ and $\pi(k)=k-1$ . The other cases has been discussed earlier: The number of derangements on $[n]$ such that $\sigma(n) \ne n-1$ is: $$\frac{\left(n-2\right)D_{n}}{n-1}$$ The number of derangements on $[n]$ such that $\sigma(n) \ne n-1$ and $\sigma(n-1) \ne n-2$ is: $$\frac{\left(n-3\right)D_{n}}{n-1}+\frac{D_{n-1}}{n-2}$$ The number of derangements on $[n]$ such that $\sigma(n) \ne n-1$ and $\sigma(n-1) \ne n-2$ and $\sigma(n-2) \ne n-3$ is: $$\frac{\left(n-4\right)D_{n}}{n-1}+\frac{3D_{n-1}}{n-2}$$ Added: The question is exactly Professor Tait's Problem of Arrangement ,but I Want to know does there exist any way to continue from my way? besides are the three partial cases helpful to derive a formula for the main problem?","['derangements', 'combinatorics']"
3674775,Generalised Fibonacci Sequence & Linear Algebra,"Consider a generalised fibonacci $G$ sequence $1, 1, 1, 3, 5, 9, 17...$ that's created by summing the last 3 entries in the sequence together: $G_0 = 1, G_1 = 1, G_2 = 1$ and $G_{n+1} = G_n + G_{n-1} + G_{n-2}$ for $n \ge 2$ . 1) Find a $3 \times3$ matrix M such that, for any $k \ge 2$ , $$\begin{pmatrix} G_{k+1} \\ G_k \\ G_{k-1} \end{pmatrix} = M \begin{pmatrix} G_{k} \\ G_{k-1} \\ G_{k-2} \end{pmatrix}$$ I figured out that $M = \begin{pmatrix} 1 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}$ from the given equation. 2) Find a numerical value for $G_{25}$ With a bit of thinking, I came up with the equation $$\begin{pmatrix} G_{k+1} \\ G_k \\ G_{k-1} \end{pmatrix} = M^k \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$$ Hence $G_{25}$ would be obtained by the first element from $M^{24}$ times [1, 1, 1]. Using a calculator, I got $3311233$ , which apparently is wrong. Was my equation above wrong? How could I approach this? Find limit of $\frac{\ln G_n}{n}$ to 10 decimal places as $n$ goes to $\infty$ . I have no idea how to approach this. I thought of using eigenvalues and eigenvectors, but since the M I found only gives me one real value of 1.83929... I don't quite see how that's going to be useful. Any help would be really appreciated.","['fibonacci-numbers', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra']"
3674781,Application of the decomposition of prime ideals as $Q_q^{e_1}Q_2^{e_2}\dots Q_R^{e_r}$,"I'm reading Marcus number field book and at page 57 he asks the following We give some applications of Theorem 27. Taking $\alpha=\sqrt{m}$ , we can re-obtain the
  results of Theorem 25 except when p = 2 and m $\equiv $ 1 (mod 4); in this exceptional
  case the result can be obtained by taking $\alpha=\frac{1+\sqrt{m}}{2}$ . Where the theorems are the following Theorem 25 With notation as above, we have: If p | m, then $$ pR=(p,\sqrt
{m})^2.$$ If m is odd, then $$ 2R= \begin{cases} (2,1+\sqrt
{m})^2&\text{if $m\equiv 3\pmod4$}\\
\left(2,\frac{1+\sqrt{m}}{2}\right)\left(2,\frac{1-\sqrt{m}}{2}\right) & 
\text{if $m\equiv 1\pmod8$}\\
\text{prime if $m\equiv 5\pmod8$.}
\end{cases}$$ If p is odd, $p\not| m$ then $$ pR=\begin{cases} (p,n+\sqrt{m})(p,n-\sqrt{m})\; \text{if $m\equiv n^2 \pmod p$}\\
\text{prime if $m$ is not a square mod $p$}
\end{cases}$$ where in all relevant cases the factors are distinct. and Theorem 27 Now let g be the monic irreducible polynomial for $\alpha$ over K. The coefficients
  of g are algebraic integers (since they can be expressed in terms of the conjugates
  of the algebraic integer $\alpha$ ), hence they are in $\mathbb{A}\cap K = R$ . Thus g $\in$ R[x] and we
  can consider $\overline{g}\in$ (R/P)[x]. $\overline{g}$ factors uniquely into monic irreducible factors in
  (R/P)[x], and we can write this factorization in the form $$\overline{g} =\overline{g}_1^{e_1}\dots \overline{g}_n^{e_n}$$ where the $\overline{g}_i$ are monic polynomials over R. It is assumed that the $\overline{g}_i$ are distinct. Let everything be as above, and assume also that p does not divide
  |S/R[ $\alpha$ ]|, where p is the prime of $\mathbb{Z}$ lying under P. Then the prime decomposition
  of PS is given by $$Q_1^{e^1}\dots Q_n^{e_n}$$ where $Q_i$ is the ideal (P, $g_i(\alpha$ )) in S generated by P and $g_i(\alpha)$ ; in other words,
  Qi = PS + ( $g_i(\alpha$ )).
  Also, f ( $Q_i$ |P) is equal to the degree of $g_i$ . I tried doing it but I think I'm doing something wrong. How do I use the relations between p and m? I always get that the minimal polynomial of $\sqrt{m}$ is $x^2-m=(x-m)(x+m)$ and so $Q_1=(P,2\sqrt{m})\wedge Q_2=(P,0)$ whose product is not equal, for example,  to $(p,\sqrt{m})$ . Can you help me?","['number-theory', 'algebraic-number-theory', 'prime-factorization', 'ideals']"
3674864,Expected number of turns for getting six 1's in six dice.,"You have 6 unbiased dice. What is the expected numbers of turns required to get 1 in all the faces such that whenever you get 1 in any of the dice, you do not roll it for next three turns. For example, the die faces could be as follows: Turn 1 $\rightarrow$ 1, 2, 3, 4, 5, 6 (since the first die came up with 1 on top, you do not roll the first die for next 3 turns) Turn 2 $\rightarrow$ $\mathbf{1} $ , 3, 3, 1, 4, 1 Turn 3 $\rightarrow$ $\mathbf{1}$ , 5, 6, $\mathbf{1}$ , 3, $\mathbf{1}$ Turn 4 $\rightarrow$ $\mathbf{1}$ , 4, 4, $\mathbf{1}$ , 2, $\mathbf{1}$ Turn 5 $\rightarrow$ 6, 1, 3, $\mathbf{1}$ , 5, $\mathbf{1}$ Turn 6 $\rightarrow$ 2, $\mathbf{1}$ , 1, 3, 2, 6 and so on. (Numbers in bold indicate that they were not rolled during that turn.)","['conditional-expectation', 'expected-value', 'dice', 'combinatorics', 'probability']"
3674895,Show $\int_{0}^{1}\frac {x^2\ln x }{{(1-x^2)}{(1+x^4)}}dx=\frac{-Ï€^2}{16(2+\sqrt{2})}$,"Question: Prove that $$\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-Ï€^2}{16(2+\sqrt{2})}$$ Using partial fraction,we get \begin{align}
&\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx\\
= &\frac{1}{4}\int_{0}^{1}\frac{\ln x}{1-x}dx+\frac{1}{4}\int_{0}^{1}\frac{\ln x}{1+x}dx+\frac{1}{2}\int_{0}^{1}\frac{(x^2-1)\ln x}{1+x^4}dx
\end{align} I got first integral as $\frac{-\pi^2}{6}$ and second integral as $\frac{-\pi^2}{12}$ $$\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-\pi^2}{16}+\frac{1}{2}\int_{0}^{1}\frac{(x^2-1)\ln x}{1+x^4}dx$$ I got stuck with third integral, which seems difficult to evaluate. ${}{}{}$ A note is also written saying that:- The reader should evaluate the family of integrals ${I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N}$ The computation of the first few special values indicates an interesting arithmetic structure of the answer. How to evaluate integral for $n$ ?","['integration', 'calculus', 'definite-integrals']"
3674921,What is the set of cardinals of the remainders of compactifications of $\mathbb R$?,"If $c:\mathbb R\to c\mathbb R$ is a compactification of $\mathbb R$ then the cardinality $K=|c\mathbb R \setminus \mathbb R|$ of the remainder can be $\mathfrak c=2^{\aleph_0}$ or (at most) $|\beta \mathbb R\setminus \mathbb R|= 2^{\mathfrak c},$ and if $K<\mathfrak c$ then $K=1$ or $K=2$ . What other values of $K$ are consistently possible? Let $S=\{t\in \mathrm{Card}: \mathfrak c\le t\le 2^{\mathfrak c}\}.$ Let $T$ be the set of uncountable cardinals of remainders of compactifications of $\mathbb R.$ Is it consistent that $T\ne S$ ? If so, is there any constraint on members of $S \setminus T$ ? Are there some forcing constructions that can address this?","['general-topology', 'forcing', 'set-theory']"
3674933,"If the matrix rings over two Rings of the same size are isomorphic, then the scalar rings are isomorphic","Let $R$ and $R'$ be rings (with 1 but no further assumptions) and $n \in \mathbb{N}$ . Does the following implication hold? If $M_n(R) \simeq M_n(R')$ then $R \simeq R'$ . If the rings are commutative then it follows from considering the centers of $M_n(R)$ and $M_n(R')$ .
If the rings are division it also holds and even stronger, it holds even if the matrices are not of the same size. I thought about considering the embeddings of $R$ in $M_n(R)$ as diagonal matrices, but since I do not want to asume anything about the isomorphism I was not able to conclude that these subrings of matrices in $M_n(R)$ and $M_n(R')$ are isomorphic. I hope someone can help me or with a proof, an idea or a counterexample. Thank you.","['matrices', 'ring-theory', 'abstract-algebra']"
3674944,Question on Proof Validity: Arbitrary Union of Power Set,"In Exercise 2.24 of Enderton's Elements of Set Theory , we are asked to show that $$\bigcup{\{\mathcal{P}}X \ \mid X \in A\} \subseteq \mathcal{P} \bigcup A.$$ Fascinated by this exhilerating problem, I started off my proof by taking an arbitrary element $x$ of $\bigcup{\{\mathcal{P}}X \ \mid X \in A\}$ and showing that by definition of the arbitrary union, there exists a set $b \in \mathcal{P} X$ such that $x \in b$ . I then proceeded to  show that because $b \in \mathcal{P} X$ implied $b \subseteq X$ , that $x \in X$ , and that because $X \in A$ , that $x \in \bigcup A$ . But as one can probably guess, this is not enough. I must show that $x \subseteq \bigcup A$ . Yet, usually, it is not the case that if $a \in B$ and $B \subseteq C$ that $a \subseteq C$ . And so I am left sad, confused, and a bit upset, because I can't seem to figure out how to come to that conclusion. To add to my confusion, looking up a solutions manual online, I find a proof that assumes just that, claiming that ""it follows that $x \in X$ , but $X \subseteq \bigcup A$ , so $x \subseteq \bigcup A$ "". This solution is probably right, and there's probably some trick involved, but I have no idea what the heck that trick is and I would greatly appreciate any help I can get. As a poor little senior in high school trying to self-study random maths in quarantine, please help a fellow out!",['elementary-set-theory']
3675014,"Proving that for the posets $\mathcal{A} = ((-1,1),\leq)$ and $\mathcal{B}=((-1,0)\cup(0,1),\leq)$, $\mathcal{A} \ncong \mathcal{B}$","I need to prove that for posets $\mathcal{A} = ((-1,1),\leq)$ and $\mathcal{B}=((-1,0)\cup(0,1),\leq)$ , $\mathcal{A} \ncong \mathcal{B}$ , where the binary relation is $\leq$ . I don't exactly understand where to begin. How can they be non-isomorphic, when all we did was to split $(-1,1)$ into a union of the intervals $(-1,0)\cup(0,1)$ ?","['order-theory', 'discrete-mathematics']"
3675106,Killing vector fields are affine,"Let $(M, g)$ be a Riemannian manifold and let $X$ be a smooth vector field on $M$ . We say that $X$ is affine if $L_X \nabla = 0$ , where $\nabla$ is the Riemannian connection on $M$ . How do we prove that Killing vector fields are affine? I know that if $X$ is a Killing vector field, then $L_X g = 0$ , which is equivalent to saying that for all other smooth vector fields $Y$ and $Z$ , $$g(\nabla_Y X, Z) + g(Y, \nabla_Z X) = 0. $$ We can also say that the flow of $X$ preserves $g$ , that is, $(\phi_X^t)^*g = g$ , where $\phi_X^t$ is the flow of $X$ at time $t$ . However, I don't know how to relate this to the Lie derivative of the connection. I also read about a formula relating the coefficients of $L_X \nabla$ with the curvature tensor (in local coordinates). However, I would like to avoid using it.","['riemannian-geometry', 'differential-geometry']"
3675122,Geometric points in fibre of finite Ã©tale morphism $\phi : Y \rightarrow X$ is independent of fibre,"I am reading the following notes http://www.math.toronto.edu/~jacobt/Lecture6.pdf and trying to understand the conclusion of Lemma 2.1. We are trying to show that the number of geometric points above $\bar{x}$ is equal to $n$ , where locally the Ã©tale morphism is induced on affines by $A \rightarrow B$ for $B$ a rank $n$ free $A$ -module. The proof shows that $B \otimes_A \bar{k} = \bar{k}^n$ , where the morphism $A \rightarrow \bar{k}$ is induced by $\bar{x}$ . The proof then suggests that we have $n$ geometric points lying above $\bar{x}$ . Why is this true? On the level of rings, we want to find morphisms $f: B \rightarrow \bar{k}$ such that $A \rightarrow \bar{k}$ is the composition of $f \circ\phi^{\#}$ . Tensoring this commutative triangle with $\bar{k}$ we get maps $A \otimes \bar{k} \rightarrow \bar{k}$ , $A \otimes \bar{k} \rightarrow \bar{k}^n$ and $\bar{k}^n \rightarrow \bar{k}$ such that the analogous commutativity conditions hold. There are $n$ ''natural"" maps $\bar{k}^n \rightarrow \bar{k}$ , namely the projections to each factor. But why would any of these projections cause the triangle to commute, and even then given a such a map how do we get an induced map on rings pre-tensoring i.e. $B \rightarrow \bar{k}$ ? Any help is appreciated.","['etale-cohomology', 'algebraic-geometry', 'schemes']"
3675208,Rate in/Rate Out Integration Question,"So, ran into a bit of a confusing question on some Calculus homework. The question went as follows (I'm quoting the question here):
""Oh no! John's chickens have all escaped their coop. At time $t=0$ there are no more chickens in the coop. The piece-wise linear graph below show the rate $R(t)$ , in number of chickens per minute, at which Mrs. Poland is putting the chickens back in their coop during a $9$ minute period."" There were a couple parts of this question that I was able to figure out, like ""How many chickens are put into the coop from time $t=0$ to $t=9$ , which I figured out to simply just be the integral of this function from $0$ to $9$ (which was $26$ ), but the part where I'm getting lost is the second and third parts of the question. The second part is as follows: I tried to find the anti-derivative, but it doesn't seem to have one? So I found the definite integral of this function from $0$ to $9$ and it gave me $18.12$ but I'm not too sure if that's right/the actual amount of chickens that exited during that amount of time. I'm also confused on how I'd go about finding the amount of chickens in the coop at one moment, taking into account the function $E(t)$ and $R(t)$ .  Any help would be appreciated on being pointed in the right direction here and sorry for the long question, just needed a bit of explaining to make my problem clear. Oh and one more question to add onto there, how would I go about finding whether or not at one moment the amount of chickens in the coop is increasing/decreasing?","['integration', 'calculus', 'functions', 'definite-integrals']"
3675235,Explanation of Proof of Theorem 43 of Chapter 4 of Protter's Stochastic Integration,"Please see below the section from the book Stochastic Integration and Differential Equations by P. Protter containing Theorem 43 of Chapter 4. I'd assume you'd need to be familiar with the book to answer the following questions I have. Please correct me where I am having wrong assumptions. 1) He does not develop his notion of predictable representability for finite time-horizons.
How can he claim (in the red box) the set $\mathcal{A}$ of Brownian motion's stopped at $t_0$ to have the representation property? He avoids an obvious later conflict by assuming $M=M^{t_0}$ in the green box.
In the same way I don't get how he establishes/means $P=Q$ by looking only at $\mathcal{F}_{t_0}$ . How does ""assuming $X$ to be stopped at $t_0$ "" make the claim $\mathcal{M}^2(\mathcal{A})=\{P\}$ rigorous? What does he leave to the reader at that point? 2) Still in the red box: I am not familiar with the argument using the bounded Borel functions to show that $P=Q$ , how does it work? Apart from my issues from question 1), is it standard and/or mentioned somewhere in the rest of the book? 3) In the proof of Corollary 1 in the orange box, how can he claim $P$ to be extremal for $\mathcal{A}=\{X_1,\dots,X_n\}$ where $X_i$ are Brownian motions? They are not $L_2$ -martingales, and the book didn't develop the notion of extremal points for sets stemming from martingales not contained in $M^2$ , the space of $L_2$ martingales. And either way, he should refer to $\mathcal{M}^2(\mathcal{A})$ (not $\mathcal{A}$ ) when speaking of $P$ as an extremal point. I don't doubt that the notion can be extended to local $L_2$ martingales, does he assume the reader to think of that? If yes, I figure the statement is valid because it was shown in the proof of theorem 43 that $P|_{\mathcal{F}_t} = Q|_{\mathcal{F}_t}$ for all $t$ (and $\text{domain}(P)=:\mathcal{F}\supset\mathcal{F}_\infty$ is no problem because?) Thank you very much! I don't doubt that most of my issues can be overlooked, I just want to make things rigorous (at my level).","['stochastic-processes', 'proof-explanation', 'probability-theory', 'martingales']"
3675249,"The improper integral $\int_0^1\sqrt{\frac1x+1}\,dx$","I want to evaluate the integral $$\int_0^1\sqrt{\frac1x+1}\,dx.$$ Letting $u=\sqrt{\frac1x+1}$ , the integral becomes $$\int_{\sqrt 2}^\infty\frac{2u^2}{(u^2-1)^2}\,du,$$ which according to mathematica, equals $\sqrt2+\frac12\log(3+2\sqrt2)$ . But using partial fractions, the antiderivative of $\frac{2u^2}{(u^2-1)^2}$ is $$\int\frac{2u^2}{(u^2-1)^2}\,du=\frac{u}{1-u^2}+\frac12\log\Big(\frac2{1+u}-1\Big),$$ which I can't really make sense of as $u\to\infty$ to apply FTC (or when $u=\sqrt2$ for that matter). Is there an easier way to go about this integral? Or do we necessarily need to involve complex analysis because of log branches?","['integration', 'calculus']"
3675303,There exist infinitely many subsequences of $(f_{m})_{m \geq 1}$ which converge at every point of $E$,"Let $E=\{ \frac{1}{n} | n \in \mathbb{N}\}$ . For each $m \in \mathbb{N}$ define $f_{m} : E \to \mathbb{R} $ by $$
f_{m}(x) = 
\begin{cases}
\cos{(m x)} & \text{if }\,x \geq \frac{1}{m}\\
0 & \text{if }\,\frac{1}{m+10}<x<\frac{1}{m}\\ 
x&\text{if } x \le \frac{1}{m+10}\\
\end{cases}
$$ Then which of the following statements is true? $(1)$ No subsequence of $(f_{m})_{m \geq 1}$ converges at every point of $E.$ $(2)$ Every subsequence of $(f_{m})_{m \geq 1}$ converges at every point of $E.$ $(3)$ There exist infinitely many subsequences of $(f_{m})_{m \geq 1}$ which converge at every point of $E.$ $(4)$ There exist a subsequence of $(f_{m})_{m \geq 1}$ which converges to $0$ at every point of $E.$ Here is what I tried :
Let $\frac{1}{k} \in E$ .Then $\forall n \geq k$ , we have $f_{n}(\frac{1}{k})= \cos{(\frac{n}{k})}$ . I don't understand how to approach further. Any help would be appreciated. Thanks in advance.","['uniform-convergence', 'functional-analysis', 'analysis', 'real-analysis']"
3675356,"Normal approximation, CLT and its approximation error","Let $X_1,X_2\dots X_n$ be $n$ i.i.d $\mathbb{R}^d$ -valued random variables such that $\mathbb{E}[X_i] = 0$ and $\text{Var}(X_i) = \frac{1}{n}I_{d\times d}$ . From Central Limit Theorem (CLT) we know that the distribution of the sum $W = \sum_{i=1}^nX_i$ approaches $\mathcal{N}(0, I_{d\times d})$ . I'm interested in understanding the approximation error of CLT. From Berry Esseen bounds we know that  for any convex measurable set $A$ $$|\mathbb{P}(W\in A) - \mathbb{P}(Z\in A)| \leq c n d^{1/4} \mathbb{E}[\|X_i\|_2^3],$$ for some universal constant $c$ , where $Z\sim\mathcal{N}(0, I_{d\times d})$ .
But I'm interested in a different kind of approximation error. Let $f:\mathbb{R}^d\to\mathbb{R}$ be a Lipschitz continuous function. I'd like to know how small the following quantity is $$\|\mathbb{E}[Wf(W)] - \mathbb{E}[Zf(Z)]\|_2.$$ Assuming $\mathbb{E}[\|X_i\|_2^3] = O(n^{-3/2})$ , does this quantity go down with $n$ ? P.S. I'm not too familiar with CLT and Stein's method. But from my limited understanding, it looks like this could be related to Stein's method. Any references on this would be greatly appreciated.","['central-limit-theorem', 'probability-theory']"
3675393,"Construction of the complex exponential, extending real identities to complex, and proving $e^{ix} = \cos x+i\sin x$ from those identities","Let's say that we have already defined $f(x)=e^x$ on $\mathbb R$ as the solution to the equation $f'(x) = f(x)$ with $f(0)=1$ , and let's say that we've proved the following three properties: $f(x) = \sum\limits_{n=0}^\infty \frac{x^n}{n!}$ $f(x+y)=f(x)f(y)$ $f(x) = \lim\limits_{n\to\infty} (1+\frac xn)^n$ Now we want to extend this function to the entire complex plane analytically, and so (using the identity theorem) the continuation is $f(z) = \sum\limits_{n=0}^\infty \frac{z^n}{n!}$ . First question: I know that on $\mathbb C$ , properties $2$ , $3$ , and $f'(z)=f(z)$ still hold. Is this a surprise, or coincidental? That is, in general is it true that if we have some formulas $F_1, \ldots, F_n$ (like the identities above, or things like continued fractions, etc) involving $g: \mathbb R\to \mathbb R$ , will those formulas $F_1,\ldots, F_n$ hold on $\mathbb C$ as well if we analytically extend $g$ to the complex plane? Now let's say that we have all these properties, and we want to use property $2$ to prove $e^{ix}=\cos x+i\sin x$ . Well, following the lead of this 3b1b video at @18:50: https://www.youtube.com/watch?v=mvmuCPvRoWQ , (maybe start watching at around minute @18:30), Grant says that it ""would be reasonable"" to think that pure vertical shifts would result in pure rotations (i.e. exponentiating a pure imaginary would result in a number on the unit circle). Yes, this is reasonable, but how do we prove it? It seems that property $2$ alone (along with the fact that $f(x+i0)=e^x$ for all $x\in \mathbb R$ ) is not enough to nail down exactly the complex exponential. So: What's the easiest step we need to take fully justify that pure vertical slides correspond to pure rotations? Note that I'm asking for a step starting from the ""group-theoretic"" framework Grant laid out in the video above; that is, I'm NOT asking for just any proof of $e^{ix}=\cos x+i\sin x$ using heavy calculus (like Taylor series, or differential equations). P.S. Are there results like the Bohr-Mollerup theorem for $e^z$ ? Like is it true that any ( continuous /differentiable?) function defined by $f(x+y)=f(x)f(y)$ (+ other conditions?) MUST be $e^z$ ?","['complex-analysis', 'exponential-function']"
3675408,Banach Space Inequality of functionals equivalence,"Let $X$ be a Banach space with $f_1,..,f_n\in X^*$ and $c_1,...,c_n\in\mathbb{R}$ then the following are equivalent: 1. $\exists x_0\in X: f_i(x_0)=c_i,\forall i\in\{1,..,n\}$ 2. $\exists M\geq 0: |\sum_{i=1}^na_ic_i|\leq M\|\sum_{i=1}^na_if_i\|$ for every choice of $a_i\in\mathbb{R}$ The direction $1\implies 2$ is immediate by setting $f=\sum_{i=1}^na_if_i$ and noticing that $\|f\|\|x\|\geq |f(x)|$ by definition of the norm. The desired inequality is then $\|f\|\|x_0\|\geq |f(x_0)|$ where $M=\|x_0\|$ . The direction $2 \implies 1$ is not that trivial however, any ideas to proceed?","['banach-spaces', 'functional-analysis']"
3675476,Proving a Function which is continous everywhere and differential nowhere.,"Let $f(x)=x-\left \lfloor{x}\right \rfloor$ , $s(x)=\begin{cases}
f(x) & \text{when $f(x) \le0.5$,}\\
1-f(x) & \text{when $f(x)>0.5$}
\end{cases}$ , $s_n(x)=\frac{s(2^{n-1}x)}{2^{n-1}}$ , $b(x)=\sum_{n=1}^{n=\infty}s_n(x)=\sum_{n=1}^{n=N} s_n(x)+\frac{b(2^Nx)}{2^N}$ , $L_N(x)=\sum_{n=1}^{n=N}s_n(x)$ which is linear on interval $[\frac{m}{2^N},\frac{m+1}{2^N}]$ for any integer $m$ , and $b(\frac{m}{2^N})=L_N (\frac{m}{2^N})$ , $c$ is the least number in the interval $(a, a+\frac{1}{2^{N-1}}]$ of the form $\frac{m}{2^N}$ . Then $d=\frac{m+1}{2^N}$ and $k=\frac{m+1/3}{2^N}$ which is in the interval so $a<c<k<d\le a+\frac{1}{2^{N-1}}$ , $L$ is linear function which coincides with $L_N$ on the interval $[c,d]$ , By definition $\frac{L(k)-L(a)}{k-a}$ = $\frac{L(c)-L(a)}{c-a}$ so If $b(a)\ge L(a)$ then $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(c)-b(a)}{c-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ . And For case $L(a)\ge b(a)$ , $\frac{L(k)-L(a)}{k-a}$ = $\frac{L(d)-L(a)}{d-a}$ so $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(d)-b(a)}{d-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ . The main objective here is to prove that limit of $\frac{b(x)-b(a)}{x-a}$ doesn't exist as $x\to a$ . So since We got the inequality for both cases we got $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(d)-b(a)}{d-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ and $\frac{b(k)-b(a)}{k-a}$ - $\frac{b(c)-b(a)}{c-a}\ge\frac{b(k)-L(k)}{k-a}\ge 1/2$ from it. I think we can conclude that the limit doesn't exists since $c,k,d$ are in the interval of the neighbourhood of $a$ . Then I checked book's answer which says: Every neighbourhood of $a$ contains an interval of the form $(a,a+1/2^{N-1}]$ and therefore points corresponding to c,k and d. So there can be no limit $g$ such that $|\frac{b(x)-b(a)}{x-a}-g|<1/5$ , for all $x$ inside any neighbourhood of $a$ . And my question again arised from the answer of book. Why is $|\frac{b(x)-b(a)}{x-a}-g|$ less than 1/5? Where and how did 1/5 come out of thin air? I think I don't understand the answer of book properly. Why did they choose that the $\epsilon=1/5$ and why not $1/2$ ? I think $\epsilon$ can't be chosen randomly there must be a way they got it. But which way? So I want to see how to prove that the $b(x)$ is not differentiable more rigorously? I can intutively see that there are infinite amount of sharp turn in self-similar function $b(x)$ which makes it non differentiable but how to prove it is not differentiable using mathematical analysis is somewhat hard for me to do. (If there is some missing information let me know in the comment)","['epsilon-delta', 'real-analysis', 'functions', 'limits', 'derivatives']"
3675526,Derivative of Integral (Fundamental Theorem of Calculus),"Question : Suppose $F(x) = \int^{x^2}_0 \frac{1}{\cos t} dt$ . Find the derivative of $F(x)$ over the region $x\in[0, \frac{\pi}{4}]$ for which it is continuous. Attempt: So I know how to do the following computations: $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \int^{x^2}_0 \sec(t)dt$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = [\ln|\tan(t)+\sec(t)|]^{x^2}_0$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)| - \ln|\tan(0)+\sec(0)|$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)| - \ln1$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)|$$ Note: To find $\int \sec(t) dt$ I used the method of substitution found here . And then using the chain rule to differentiate ( $u=|\tan(x^2)+\sec(x^2)|$ so $F(x)=\ln u$ ), we get $F'(x)=\frac{2x}{\cos (x^2)} = 2x\sec (x^2)$ for $x\in[0, \frac{\pi}{4}]$ . However, this question was given in a real analysis course and I believe I need to use the Fundamental Theorem of Calculus (which is fine considering $F$ is continuous for $x\in[0, \frac{\pi}{4}]$ as stated in the question so not required to prove). I am familiar with two different (but equivalent) definitions of the FTC; this one and this one , but am unsure of which one to use. Any help would be greatly appreciated.","['integration', 'real-analysis', 'continuity', 'calculus', 'derivatives']"
3675585,Functional inequality $f(x+y)f(x-y)\leq f^2(x)-f^2(y)$,"Let $f:\mathbb{R}\to\mathbb{R}$ be $$f(x+y)f(x-y)\leq f^2(x)-f^2(y)\quad \forall x,y\in\mathbb{R}.$$ Then prove that $$f(x)=-f(-x)$$ and $$f(x+y)f(x-y)=f^2(x)-f^2(y),$$ where $f^2(x)=\big(f(x)\big)^2$ . Source: KMO $1987$ The first is easy. Let the given assertion be $P(x,y)$ $$P(0,0)\implies f^2(0)\leq 0\implies f(0)=0$$ $$P(x,-x)\implies f^2(x)-f^2(-x)\geq 0\tag{*}$$ $$P(0,x)\implies -f^2(x)-f(x)f(-x)\geq 0\tag{**}$$ $$(*)+2(**)\implies -(f(x)+f(-x))^2\geq 0\implies f(x)=-f(-x)$$ But second is not easy. I'm trying to prove $f(x+y)f(x-y)\geq f^2(x)-f^2(y)$ . Can anyone help me?","['contest-math', 'functional-equations', 'functional-inequalities', 'functions', 'algebra-precalculus']"
3675602,Prove that $\left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.$,"Question: Let $f,g:[0,1]\to\mathbb{R}$ be two continuous functions such that $f(x)g(x)\ge 4x^2, \forall x\in[0,1].$ Prove that $$\left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.$$ Solution: Since we have $f(x)g(x)\ge 4x^2\ge 0, \forall x\in[0,1]\implies f(x)g(x)\ge 0, \forall x\in[0,1].$ This in turn implies that $f(x)$ and $g(x)$ has the same sign at each $x\in[0,1]$ . Claim: $f$ does not change it's sign in the interval $[0,1]$ . Proof: Let us assume that $f(x)>0$ at some $x=a$ and $f(x)<0$ at some $x=b$ , where $a,b\in[0,1]$ and $b>a$ . Now since $f$ is continuous in $[0,1]$ , thus by IVT we can conclude that $\exists c\in(a,b),$ such that $f(c)=0$ . Hence, we have $f(c)g(c)=0\ge 4c^2\implies 4c^2\le 0,$ but $c>0\implies 4c^2>0.$ Hence we arrive at a clear contradiction. Thus, we can conclude that $f$ does not change it's sign in the interval $[0,1]$ . Using our claim, we can also conclude that, $g$ does not change it's sign in the interval $[0,1]$ . Thus, either $f(x),g(x)>0, \forall x\in[0,1]$ or $f(x),g(x)<0, \forall x\in[0,1]$ . Observe that in any case $$\left|\int_0^1 f(x)dx\right|=\int_0^1|f(x)|dx\text{ and }\left|\int_0^1 g(x)dx\right|=\int_0^1|g(x)|dx.$$ Thus, by Cauchy-Schwarz inequality we have $$\left|\int_0^1 f(x)dx\right|\left|\int_0^1 g(x)dx\right|=\int_0^1|f(x)|dx\int_0^1|g(x)|dx\\=\int_0^1\left(\sqrt{|f(x)|}\right)^2dx\int_0^1\left(\sqrt{|g(x)|}\right)^2dx\\\ge \left(\int_0^1\sqrt{|f(x)|}.\sqrt{|g(x)|}dx\right)^2=\left(\int_0^1\sqrt{|f(x)g(x)|}dx\right)^2=\left(\int_0^1\sqrt{f(x)g(x)}dx\right)^2.$$ Now since $\forall x\in[0,1]$ , we have $$f(x)g(x)\ge 4x^2\implies \sqrt{f(x)g(x)}\ge 2x\\\implies\int_0^1\sqrt{f(x)g(x)}dx\ge \int_0^1 2x dx=1\\\implies \left(\int_0^1\sqrt{f(x)g(x)}dx\right)^2\ge 1.$$ Hence, we can conclude that $$\left|\int_0^1 f(x)dx\right|\left|\int_0^1 g(x)dx\right|\ge 1.$$ Thus, we can conclude that $$\left|\int_0^1 f(x)dx\right|\ge 1\text{ or }\left|\int_0^1 g(x)dx\right|\ge 1.$$ Hence, we are done. Is this solution correct and rigorous enough? Are there any alternative solutions?","['solution-verification', 'real-analysis']"
3675630,$f(x)=f^{\prime}(x)+f^{\prime\prime }(x)$ such that $f(a)=f(b)=0$ then prove that $f=0$,"$f:[a,b] \to \Bbb R$ twice differentiable function is defined in this way that $f(x)=f^{\prime}(x)+f^{\prime\prime }(x)$ such that $f(a)=f(b)=0$ then prove that $f=0$ I have attempted this problem in this way that $f(a)=f(b)=0$ then by Rolle's theorem $\exists c\in (a,b)$ s.t $f'(c)=0$ . Now there are three cases $f""(c)>0$ i.e we have a local minimum then $f(c)=f""(c)>0$ then $\exists \delta>0$ s.t f is decreasing in $(c-\delta,c)$ so there exists a local positive maxima at $d\in (a,c)$ . Now at that point $f(d)>0$ but $f""(d)<0$ a contradiction. $f""(c)<0$ similar like case 1. $f""(c)=0$ . Here we will get $f(c)=0$ so we will repeat those three cases in $[a,c]$ . Now my questions are: Is my proof correct? If it is correct, can I make the proof much more rigorous e.g giving a proper proof of this line ""so there exist a local positive maxima at $d\in (a,c)$ . Now at that point $f(d)>0$ but $f""(d)<0$ a contradiction""? I actually made that statement seeing the picture. Can you give me any other proofs?","['contest-math', 'analysis', 'real-analysis', 'maxima-minima', 'calculus']"
3675664,Is the derivative always nonnegative in a neighbourhood of a minimum?,"Let $f:[0,1] \to \mathbb R$ be a smooth function, and suppose that $f(x) > f(0)$ for every $0< x \le 1$ . Is it true that $f' \ge 0$ in some neighbourhood of $0$ ? $f'(0) \ge 0$ , and by the mean value theorem $$ f'(c(x))=\frac{f(x)-f(0)}{x-0}> 0,$$ where $0<c(x)<x$ . In particular, by taking $x$ to zero, we can construct a sequences $x_n \to 0$ satisfying $f'(x_n) >0$ . I am not sure how to proceed from here. Is there some pathological counter-example?","['monotone-functions', 'examples-counterexamples', 'real-analysis', 'calculus', 'derivatives']"
3675756,Frobenius endomorphism of Abelian varieties,"Let $A$ be an abelian variety over $\mathbb{F}_q$ with $q=p^n$ , such that all of it's endomorphisms are defined over $\mathbb{F}_q$ . Then $End(A)\otimes \mathbb{Q}$ is of Albert type (IV) with center $L$ a CM-field. Let $T_p(A)$ be it's p-divisible group. Due to the Tate-conjecture $End(A)\otimes \mathbb{Z}_p\xrightarrow{\sim} End(T_p(A))$ are isomorphic. But on one hand the Frobenius endomorphism $\pi_A$ of $A$ is in $L$ and on the other hand given the description of $End(T_p(A)) \otimes \mathbb{Q}_p$ from Dieudonne theory, $\pi_{A}$ can not be central in $End(T_p(A)) \otimes \mathbb{Q}_p$ . So what am I missing here?","['algebraic-geometry', 'abelian-varieties']"
3675793,Can elementary row operations be done by both left and right multiplication?,"So I know that interchanging two rows of a matrix $A$ can be done by left-multiplying it by some permutation matrix $P$ to get $PA$ . Similarly, interchanging two columns can be done by right-multiplication, $AP$ . My question is: Can you interchange rows by right -multiplication (of some matrix, not necessarily a permutation matrix)? And interchange columns by left-multiplication? In general, can every elementary row (or column) operation be represented as both a left- and right-multiplication? Edit: if $A$ is invertible, and we want to find a matrix $B$ such that $AB = PA$ , then we can solve for $B$ by multiplying both sides by $A^{-1}$ to get $B = A^{-1}PA$ . So this is how we can interchange two rows by right-multiplication. Similarly for interchanging columns. But what if $A$ isn't invertible? Can we characterize the cases when it is or isn't possible?","['matrices', 'linear-algebra']"
3675817,Is the image of Borel measurable function essentially Borel measurable?,It is known that whenever $f: \mathbb{R} \rightarrow \mathbb{R}$ is Borel measurable it is not necessarily true that $f(X)$ is Borel measurable for Borel $X \subset \mathbb{R}$ . However I have not seen a counterexample yet. It is known that whenever $f$ is injective the claim holds. Question: Does there exists a Borel function $\tilde{f}: \mathbb{R} \rightarrow \mathbb{R}$ such that $f = \tilde{f}$ a.e. and such that $\tilde{f}(X)$ is Borel measurable for each Borel $X \subset \mathbb{R}$ ?,"['measure-theory', 'real-analysis', 'measurable-functions', 'borel-sets', 'descriptive-set-theory']"
3675819,Proving a general reasult in polynomials.,"If $f(x) = (x-a)(x-b)(x-c)(x-d) - 1$ where $a, b, c, d$ are distinct integers. Prove that $f(x)$ can't be factorized into integer polynomial with $deg â‰¥1$ In the above question I proved for three degree polynomial using contradiction. But couldn't use the same for the even degree polynomial.If anyone out there could help me would be of great help.","['functions', 'polynomials', 'analysis']"
3675824,Calculate the probability that the event A occurred based on weighted premises,"I am implementing some code, and want to implement the following: Calculate the probability that event A occurred based on weighted premises. So I want to be able to define some premises (other events that occurred in the system - these either occurred or not - I know whether they occurred for sure or not). These premises have different weights in terms of how much do they commit to the occurrence of event A and are independent of each other. So for instance, a dummy example (weight of the event is in the parenthesis): a) clouds are visible on the sky (4) b) it was raining yesterday (1) c) rain detectors detected water (7) d) relative humidity in the air is high (6) Before calculating I know that these events happened or not. And I want to calculate probability whether it is raining at the moment (event A) based on a), b), c), d) premises (events) that have different weights. Could you point me in the right direction? Thank you","['conditional-probability', 'probability']"
3675897,Evaluate $\int _0^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx$ with real methods,"I started like this: $$\int _0^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx\:
=\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx+\overset {x=\frac{1}{x}}{\int _1^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx}$$ $$=2\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx+5G$$ where $G$ is Catalan's constant. But the integral left is very hard to calculate. As suggested by Zacky one can use the sub $x=\frac{1-t}{1+t}$ and get these integrals. $$\ln 2\int _0^1\frac{1}{1+t^2}\:dt+\ln 5\int _0^1\frac{1}{1+t^2}\:dt+\int _0^1\frac{\ln \left(t^2+1-\frac{2}{\sqrt{5}}\right)}{1+t^2}\:dt$$ $$+\int _0^1\frac{\ln \left(t^2+1+\frac{2}{\sqrt{5}}\right)}{1+t^2}\:dt-5\int _0^1\frac{\ln \left(1+t\right)}{1+t^2}\:dt$$ To evaluate those one can use the identity $$\int _0^1\frac{\ln \left(b+ax^2\right)}{1+x^2}\:dx=\frac{\pi }{2}\ln \left(\sqrt{a}+\sqrt{b}\right)+\text{Ti}_2\left(\frac{\sqrt{a}-\sqrt{b}}{\sqrt{a}+\sqrt{b}}\right)-G$$ In the end, the integral evaluates to \begin{align}
\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx
&= -\frac{3\pi }{8}\ln 2+\frac{\pi }{4}\ln 5-2G\\
&+\frac{\pi }{2}\ln \left(1+\sqrt{1-\frac{2}{\sqrt{5}}}\right)+\text{Ti}_2\left(\frac{\sqrt{1-\frac{2}{\sqrt{5}}}-1}{\sqrt{1-\frac{2}{\sqrt{5}}}+1}\right)\\
&+\frac{\pi }{2}\ln \left(1+\sqrt{1+\frac{2}{\sqrt{5}}}\right)+\text{Ti}_2\left(\frac{\sqrt{1+\frac{2}{\sqrt{5}}}-1}{\sqrt{1+\frac{2}{\sqrt{5}}}+1}\right)
\end{align} But, I have no idea how to simplify the $\text{Ti}_2\left(z\right)$ terms, which seems possible because I found that a similar version can be expressed without these $$\int _0^{\infty }\frac{\ln \left(1+x^5\right)}{\left(1+x^2\right)^2}\:dx=-\frac{5\pi }{8}-\frac{7\pi }{40}\ln \left(2\right)+\frac{\pi }{5}\ln \left(4+\sqrt{10-2\sqrt{5}}\right)
+\frac{\pi }{10}\ln \left(43+7\sqrt{5}+4\sqrt{130+38\sqrt{5}}\right)+\frac{G}{10}$$ NB: $\displaystyle x\in\mathbb{R},\text{Ti}_2(x)=\int_0^x \frac{\arctan t}{t}dt$","['integration', 'calculus', 'improper-integrals', 'real-analysis']"
3675929,Separating Points and Tangent Vectors (real curves),"In [Hartshorne, Proposition 7.3.] as well as in [GÃ¶rtz & Wedhorn, Rem. 13.55] and [Vakil Notes, around 19.2] the following is said: If $X$ is a curve over (let's say) $\mathbb{C}$ (algebraically closed) and we have a complete linear system (or an invertible sheaf and generating sections) which separates points and tangent vectors , then we get a closed embedding into $\mathbb{P}^{n}_{\mathbb{C}}$ . But I would like to start with a curve over $\mathbb{R}$ and get an embedding into $\mathbb{P}^{n}_{\mathbb{R}}$ . Which conditions are needed for getting a similiar statement (involving complete linear systems separating points and tangent vectors) in the real case? For example: My curves are nonsingular, integral, projective, schemes over $\mathbb{R}$ of dimension one. Is there a further references besides the ones from above? Thank you!","['projective-schemes', 'morphism', 'divisors-algebraic-geometry', 'algebraic-geometry', 'real-algebraic-geometry']"
3675936,Open Problems to do with Polynomials and/or Elementary Function Theory,I was wondering what are some open problems (even if deemed impossible) in basic function theory (stuff you'd learn in high school) and/or open problems to do with polynomials... Thank you in advance :),"['elementary-functions', 'functions', 'polynomials', 'open-problem']"
3676063,The Spectrum of the operator in C[0;1],"I have an operator $ Ax(t) = \int_{0}^{t^2} x(s)ds $ in $ C [0;1] $ . I need to find spectrum of this operator. $ A $ is a compact operator, so the spectrum consists of 0 and eigenvalues. 
As I understand there is no eigenvalues, so spectrum = 0. But I don't know how to prove that there is no eigenvalues in spectrum. Would be very glad for any help!","['operator-theory', 'spectral-theory', 'functional-analysis']"
3676067,"Complex numbers $z_1$, $z_2$, $z_3$ and $z_4$ lie on a circle, show $w=\frac{(z_1-z_2)(z_3-z_4)}{(z_4-z_1)(z_2-z_3)}$ is real","Can anyone help with this: I have tried breaking it down like this: and trying to play with the angles like this: I feel there has to be a more elegant way to do this, could anyone see one?","['circles', 'geometry', 'complex-numbers']"
3676069,Understanding stochastic Big-$O$ notation in a stochastic process context,"I am trying to understand an equality in this paper about locally stationary processes on page 24. The equality includes stochastic Big-O (see D5 in this paper for a definition of stochastic O-notation) and is as follows: $$
\sum_{j = 0}^T \left[\prod_{k=0}^{j-1} \alpha\left(\frac{t-k}{T}\right)\right]\varepsilon_{t-j} =
\sum_{j = 0}^T \alpha\left(\frac{1}{T}\right)^j \varepsilon_{t-j} + O_p\left(\frac{1}{T}\right),
$$ for $T \rightarrow \infty$ where $\varepsilon_{t-j}$ is white noise. (I have left some coefficients from the paper out and I have only considered finitely many summands here for simplicity) Here I have already proved that $$
	\prod_{k=0}^{j-1} \alpha\left(\frac{t-k}{T}\right) = \alpha\left(\frac{1}{T}\right)^j + O\left(\frac{1}{T}\right)
$$ with deterministic Big-O notation. However, if I plug this result into the equation above, I get \begin{align*}
	\sum_{j = 0}^T \left[\prod_{k=0}^{j-1} \alpha\left(\frac{t-k}{T}\right)\right]\varepsilon_{t-j} 
	&= \sum_{j = 0}^T \left[\alpha\left(\frac{1}{T}\right)^j + O\left(\frac{1}{T}\right)\right]\varepsilon_{t-j} \\
	&= \sum_{j = 0}^T \alpha\left(\frac{1}{T}\right)^j \varepsilon_{t-j} + \sum_{j = 0}^T O\left(\frac{1}{T}\right) \varepsilon_{t-j}.
\end{align*} Now we would need the following: $$ \sum_{j = 0}^T O\left(\frac{1}{T}\right) \varepsilon_{t-j} = O_p\left(\frac{1}{T}\right)$$ It is easy to see that $O\left(\frac{1}{T}\right) \varepsilon_{t-j} =  O_p\left(\frac{1}{T}\right)$ , but summing up I would get $$ \sum_{j = 0}^T O\left(\frac{1}{T}\right) \varepsilon_{t-j} = \sum_{j = 0}^T O_p\left(\frac{1}{T}\right) = T  O_p\left(\frac{1}{T}\right) = O_p(1)$$ and not the result above.","['stochastic-processes', 'statistics', 'probability-limit-theorems', 'asymptotics']"
3676111,"Are $n!=\sum_{k=0}^{n}kD_{n,k}$ and $n!=\sum_{k=0}^{n}\left(k-1\right)^{2}D_{n,k}$ true?","It's known that : $$n!=\sum_{k=0}^{n}\binom{n}{k}D_{n-k}\tag{I}$$ Where $D_{n-k}$ is the number of derangements on a set $[n-k]$ . On the other hands from the number of partial derangements we know that: $$D_{n,k}=\binom{n}{k}D_{n-k,0}$$ Where $D_{n,k}$ is the number of ways to select $k$ elements from $[n]$ to be fixed and let the others to be deranged (AKA Rencontres numbers ). Clearly $D_{n,0}=D_n$ , from here $(\text{I})$ can be rewritten as: $$n!=\sum_{k=0}^{n}D_{n,k}$$ I know another definitions for $n!$ which are as follows: $$n!=\sum_{k=0}^{n}kD_{n,k}\tag{1}$$ $$n!=\sum_{k=0}^{n}\left(k-1\right)^{2}D_{n,k}\tag{2}$$ However I'm not sure if the one is right,so can someone check the validity of the two definitions and if they're true then prove them combinatorially?(I think the first one is not true)","['derangements', 'combinatorics']"
3676147,Average distance between two points on $U(n)$,"I'm interested in the following problem : Compute the average distance between two points independently chosen at random on $U(n)$ . What i have done so far : Observe that : $$\begin{array}{lll}
\int_{U(n)}\int_{U(n)}d(g_1,g_2)d\mu_1d\mu_2&=&\int\int d(g_1g_2^{-1},1)d\mu_1d\mu_2\\
&=&\int\int d(g,1) d\mu d\mu_2\\
&=&\int d(g,1) d\mu\\
\end{array}$$ where : the second equality comes from the bi-invariance of the Haar-measure; the last equality comes from the fact that $\mu_2({U(n)})=1$ . Now, $g\mapsto d(g,1)$ is invariant by conjugation so Weyl's integration formula applies : $$\int_{U(n)} d(g,1) d\mu = \int_{T} d(t,1) u(t) dt =\frac{1}{n!}\int_{[-\pi;\pi]^n} \sqrt{\theta_1^2+...+\theta_n^2}\prod_{i\neq j}\left|e^{i\theta_i}-e^{i\theta_j}\right|\frac{d\theta_1}{2\pi}...\frac{d\theta_n}{2\pi}$$ This is because, if $t=diag(e^{i\theta_1},...e^{i\theta_n})$ with $\theta_i\in[-\pi,\pi]$ , then $d(t,1)=\sqrt{\theta_1^2+\ldots+\theta_n^2}$ . Indeed, $\gamma:[0,1]\to U(n)$ , $\gamma(s)=diag(e^{is\theta_1},...e^{is\theta_n})$ is a minimal geodesic of length $||(\theta_1,...,\theta_n)||=\sqrt{\theta_1^2+...+\theta_n^2}$ . Questions : Are my reasoning and computation so far correct ? If they are, how can I compute that monstrosity ?! I mean, even in the simplest non trivial case where $n=2$ : $$\int_{U(2)} d(g,1)d\mu = \frac{1}{2\pi^2}\int_{[-\pi;\pi]^2} \sqrt{x^2+y^2}\sin\left(\frac{x-y}{2}\right)^2 dx\,dy$$ Wolfram and back-of-the-envelope simulations with Sage suggest the result is $\approx 2,48$ . Is this the best we can expect ?! If exact computation is not manageable for general $n$ , how can i compute at least an equivalent when $n\to \infty$ ? I'd like to understand how much the unitary case deviates from abelian case of same rank. (Simulations on Sage suggest that the average distance between two points on the standard torus $T_n$ grows like $O(\sqrt{n})$ .) How can I do that ? Surely, these questions must have been investigated but i don't where to look for. What are the key-words/references for these questions ? EDIT2 : i think i can answer question 3. for the torus case ! Indeed, let $(X_i)_{i\in\mathbb{N}}$ be a sequence of i.i.d. variables with uniform distribution $\mathcal{U}([-\pi;\pi])$ . Then $\frac{X_1^2+\ldots+X_n^2}{n}\rightarrow^{a.s.} E[x_1^2]=\frac{\pi^2}{3}$ (law of large numbers). Therefore, $$\int_{[-\pi,\pi]^n}\sqrt{x_1^2+...+x_n^2}\frac{dx_1}{2\pi}...\frac{dx_n}{2\pi} \sim \sqrt{n}\sqrt{\frac{\pi^2}{3}}=\frac{\pi}{\sqrt{3}}\sqrt{n}$$ If this is correct, this settles question 3. for the torus case.","['integration', 'unitary-matrices', 'measure-theory', 'reference-request', 'group-theory']"
3676178,Matrix of paths from graph $G_1$ to graph $G_2$ to graph $G_3$,"If $A$ is the adjacency matrix of graph $G$ , then it is a well know property that $A^n$ is a matrix where the element $(i, j)$ gives the number of walks of length $n$ from vertex $i$ to vertex $j$ . The matrix of paths is different and there are closed forms for paths of length 2 and 3 ( https://mathworld.wolfram.com/GraphPath.html ). For example, the matrix of paths of length 3 is given by: $$
P_3 = A^3 - \text{diag}(A^2) \cdot A - A \cdot \text{diag}(A^2) + A \times A^T - \text{diag}(A^3)
$$ ("" $\cdot$ "" is normal matrix multiplication, "" $\times$ "" is element-wise multiplication and ""diag $(A)$ "" has the same principal diagonal as A with the remaining elements set to zero) I am interested in the matrix of paths of length 3, for 3 (potentially different) graphs. The interpretation of the product of two different adjacency matrices $A \cdot B$ is ( from this post ): ""Cell $(i,j)$ in $A \cdot B$ contains the number of walks from $i$ to $j$ where the
  first step is in $A$ , but the second step is in $B$ "" Given three undirected graphs $G_1$ , $G_2$ and $G_3$ with (symmetric since undirected) adjacency matrices $A$ , $B$ and $C$ , what I need is an expression for the matrix of 3-paths from $G_1$ to $G_2$ to $G_3$ . My attempt at this is: $$
A \cdot B \cdot C + (A \cdot B \cdot C)^T - C \cdot \text{diag}(A \cdot B) - \text{diag}(A \cdot B) \cdot C - \text{diag}(A \cdot B \cdot C)
$$ However, this is just based on observations and tweaking. It works on a non-trivial example, but I do not know if it is correct. What I'd like is: To confirm this is correct, or if it is not, find the correct expression. Have a sketch/intuition of a proof","['graph-theory', 'adjacency-matrix', 'matrices', 'combinatorics', 'discrete-mathematics']"
3676233,stochastic integral is unique,"If you know all about stochastic integration please just skip to my question; the first part is just fixing notation and background. Let $\Omega, \mathcal{A},\mu$ be a probability space. And let $W:\Omega \times [0,T]\rightarrow \mathbb{R}$ be a Brownian motion adapted to a filtration $\mathcal{F}_t\subset \mathcal{A}, t\in [0,T]$ . I just read how one may define a stochastic integral $\int_0^TXdW$ where $$X: \Omega \times [0,T]\rightarrow \mathbb{R}$$ is $ \mathcal{A}\otimes\mathcal{B}([0,T])$ -measurable, X is $\mathcal{F}_t$ -adapted, and where $X\in L^2(\Omega \times [0,T])$ ).  The proof is based on the fact that the $L^2$ condition implies that there is a sequence of adapted $L^2$ ""simple processes"", $$\sigma_n(\omega,t)=\sum_{i=0}^{k}f_i(\omega)\cdot \mathbb{1}_{(t_i,t_{i+1}]}(t), ~~~(f_i\in L^2(\Omega,~\mathcal{F}_{t_i}),~t_0=0,~t_{k_n+1}=T)$$ converging to $X$ in $L^2(\Omega \times [0,T]).$ For $t\in (t_j,t_{j+1}]$ one defines $$(\int_0^T\sigma_ndW)(\omega,t)=f_j\cdot(W_{t}-W_{t_j}) +\sum_{i=0}^{j-1}f_i\cdot(W_{t_{i+1}}-W_{t_i}),$$ and then one shows that $t\mapsto (\int_0^T\sigma_ndW)(\omega,t)$ is continuous for all $\omega,n$ , and that for almost every $\omega$ , the functions $t\mapsto (\int_0^T\sigma_ndW)(\omega,t)$ satisfy the Weierstrass M test, and thus converge to a continuous function $t\mapsto Y(\omega,t)$ .  We define $\int_0^TXdW:=Y$ . That is all clear. Now the book I'm reading ( Intro to Stochastic Calculus Applied to Finance by Lamberton and Lapeyre) says we need to extend this integral by relaxing the $X\in L^2(\Omega \times [0,T])$ condition.  Namely the claim is that if $$\mathcal{H}= \{ X: \Omega \times [0,T]\rightarrow \mathbb{R}~~~|~~X ~is~\mathcal{F}_t- adapted, ~ and~for~almost~all~\omega~we~have~\int_0^T|X(\omega,t)|^2dt<\infty \}$$ and if $\mathcal{C}$ is the space of adapted and almost surely continuous processes on $\Omega\times [0,T]$ , then there exists a unique linear mapping $$\mathcal{H}\rightarrow\mathcal{C},~X\mapsto \int_0^T XdW$$ satisfying the following two properties This integral agrees with the previous definition for simple processes (the third centered line in this post). If $H_n$ is a sequence of processes in $\mathcal{H}$ such that the sequence of function $\bigg(\omega\mapsto\int_0^TH_n^2(\omega,t)dt\bigg)_{n\in \mathbb{N}}$ converges in probability to $0$ , then the sequence $$\bigg(\omega\mapsto sup_{t\leq T}|(\int_0^T H_ndW)(\omega,t)|\bigg)_{n\in \mathbb{N}}$$ also converges to $0$ in probability Finally here's my question: The book says that it is clear that conditions 1 and 2 imply that the new integral agrees with the old (almost surely for all $t\in [0,T]$ ) whenever the integrand $X$ is in $L^2(\Omega \times [0,T])$ . Why?? The only thing I can think to do is to again take a sequence of simple processes, $\sigma_n$ , converging to $X$ in $L^2(\Omega \times [0,T])$ .  Then clearly the sequence $$\bigg(\omega\mapsto\int_0^T|\sigma_n(s,\omega) ds - X(s,\omega)|^2 ds\bigg)_{n \in \mathbb{N}}$$ converges to $0$ in $L^1(\Omega)$ and therefore converges to $0$ in probability. Thus by 2, we have that $$\bigg(\omega \mapsto sup_{0\leq t \leq T}\bigg|(\int_0^T \sigma_ndW)(\omega, t)-(\int_0^TXdW)(\omega, t)\bigg|\bigg)_{n\in \mathbb{N}}$$ converges to zero in probability.  Now what we want is that for almost all $\omega$ , for all $t$ we have $$(\int_0^T \sigma_ndW)(\omega, t)-(\int_0^TXdW)(\omega, t)\rightarrow 0.$$ This would suffice because we know $\int_0^T \sigma_ndW$ agrees with the previously defined integral by property 1, and the sequence $\int_0^T \sigma_ndW$ converges for almost all $\omega$ for all $t$ to the previously defined integral of $X$ .  But as far as I can tell, convergence in probability doesn't guarantee this.","['stochastic-processes', 'stochastic-calculus', 'analysis']"
3676246,Concentration inequality for sum of iid squares of sub-Exponential variables,"If $X_i$ follows sub-Exponetial distribution, can we obtain the following inequality: $$
P(X_1^2+\cdots+X_n^2>nt)\le nP(X_1^2>nt)
$$ I just saw this answer from this page: Concentration inequality for sum of squares of i.i.d. sub-exponential random variables? However I still cannot figure out why it holds. Could someone give me some explanations or references? Thanks a lot!","['statistics', 'probability']"
3676285,Commutation relation between covariant and Lie derivatives,"I am currently working on extrinsic riemannian geometry and I am looking for a sort of commutation relation between the covariant and Lie derivatives. To be more precise : considering an hypersurface $H \subset M$ of a riemannian manifold, $\nu$ a vector field normal to $H$ and $S$ its shape operator (or Wiengarten operator ) defined by $SX = \nabla_X \nu$ , you can consider normal geodesics emanating from $H$ as geodesics veryfing $\gamma(0) \in H$ , $\dot\gamma(0) = \nu$ . Writing the parameters of these geodesics $r$ , you get a vector field $\partial_r = \dot\gamma$ . If $(x^1,\ldots,x^n)$ are local coordinates on $H$ , then you have Fermi coordinates $(r,x^1,\ldots,x^n)$ on $M$ . We have the Ricatti equation, where $R_{\partial_r} = R(\partial_r,\cdot)\partial_r$ : \begin{align*}
\mathcal{L}_{\partial_r}S=\partial_r S = -S^2 - R_{\partial_r}
\end{align*} (in fact, the equation is still true while replacing $\mathcal{L}_{\partial_r}$ by $\nabla_{\partial_r}$ , it's a property of the shape operator). I want to find a differential equation for $\nabla_{\partial_j}S$ where $\partial_j = \frac{\partial}{\partial x^j}$ . My idea is to differentiate the Ricatti equation with respect to $\nabla_{\partial_j}$ and use a sort of commutation relation to get a differential equation involving $S$ , $\nabla_{\partial_j}S$ , $R$ , etc . with variable $r$ . So, my question is : do we have a nice relation between $\nabla_{\partial_j} \mathcal{L}_{\partial_r} S$ and $\mathcal{L}_{\partial_r}\nabla_{\partial_j}S$ ? Thank you for reading me.","['riemannian-geometry', 'ordinary-differential-equations', 'differential-geometry']"
3676313,matrix expression for $e^{iA}Be^{iA}$ in terms of anticommutators?,"I'm familiar with the expression $$e^{-iA}Be^{iA} = \sum_{n=0}^{\infty} \frac{i^n}{n!}[..[B,A],\dots A]_{n \; \rm times}$$ for square matrices $A$ and $B$ and was wondering if equivalently $$e^{iA}Be^{iA} = \sum_{n=0}^{\infty} \frac{i^n}{n!}\{..\{B,A\},\dots A\}_{n \; \rm times},$$ or something similar?","['matrices', 'matrix-exponential', 'linear-algebra', 'functional-analysis']"
3676327,What is the domain of $f^2$ if $f(x)=\sqrt{x+2}$,"Now, $f(x)$ is defined for all values of $x$ for which $x+2 \geq 0$ $x+2 \geq 0 \implies x \geq -2$ So, $\mathrm{Domain}(f)=[-2,\infty)$ which means $f : [-2,\infty) \longrightarrow \Bbb R$ $f^2(x) = \Big (f(x) \Big )^2=(\sqrt{x+2})^2=x+2$ So, $f^2$ is defined for all values of $x$ , right? So, shouldn't $f^2:\Bbb R \longrightarrow \Bbb R$ ? According to my textbook, $f^2:[-2,\infty) \longrightarrow \Bbb R$ But if we take something outside of $[-2,\infty)$ , for example $-5$ and put it in $f^2(x)$ , we get: $f^2(-5) = \Big (f(-5) \Big )^2=(\sqrt{-5+2})^2=(\sqrt {-3})^2=(-3) \in \Bbb R$ Doesn't this mean that $f^2$ is defined for values outside the domain of $f$ as well? So, am I right or is the book right? If the book's right, where am I wrong? Thanks!",['functions']
3676331,Continuous function and non-zero measure set,"Let $g:[0,1] \to\Bbb R$ be a continuous function. Let $m$ denote the Lebesgue measure in this interval. Suppose that it takes a constant value in $A \subset [0,1]$ , and $m(A) \neq 0$ . It is certain that $g$ is constant in a interval in $[0,1]$ ?","['functional-analysis', 'analysis', 'real-analysis']"
3676378,Prove that $\mu^* (F \setminus E) = 0.$,"Let $\mathcal A$ be an algebra of subsets of a set $X$ and $\mathcal S (\mathcal A)$ be the $\sigma$ -algebra of subsets of $X$ generated by $\mathcal A.$ Let $\mu : \mathcal A \longrightarrow [0,+ \infty]$ be a measure on $\mathcal A.$ Let $\mu^*$ be the outer measure induced by $\mu.$ Let $E \subseteq X$ and $F \in \mathcal S (\mathcal A)$ be such that $E \subseteq F$ and $\mu^* (E) = \mu^* (F) < + \infty.$ If $\mu^* (G) = 0,$ $\forall$ $G \in \mathcal S (\mathcal A)$ with $G \subseteq F \setminus E$ then show that $\mu^* (F \setminus E) = 0.$ I have proved that the outer measure $\mu^*$ on $\mathcal P(X)$ induced by $\mu$ can be equivalently defined in terms of the restriction $\bar \mu$ of $\mu^*$ to $\mathcal S (\mathcal A)$ as follows $:$ $$\mu^* (A) = \text {inf}\ \left \{\bar {\mu} (B)\ |\ B \in \mathcal S (\mathcal A), A \subseteq B \right \}.$$ Can it help anyway? I hardly believe that this result will hold. But how to find a counter-example? Thanks in advance.","['measure-theory', 'outer-measure']"
3676384,Understanding sets with an uncountable number of isolated points in $\mathbb{R}$.,"Can someone please verify my proof? Thanks! Prove that a set with an uncountable number of isolated points does not exist in $\mathbb{R}$ . (Added: June 2, 2020) Proof : Let $B = \{x_{1}, x_2, \dots\}$ be the set of isolated points of some set with an uncountable number of isolated points. Since each $x_i$ is an isolated point, $\exists \epsilon_i > 0 \textrm{ s.t. } N(x_i , \epsilon_i) \cap B = \{x_i\}$ which implies $N^*(x_i,  \epsilon_i) \cap B = \emptyset$ We claim that for any two distinct $x_a, x_b \in A$ , $N^{*}(x_a , \frac{\epsilon_a}{2}) \cap N^{*}(x_b , \frac{\epsilon_b}{2}) = \emptyset$ . Assume, to obtain a contradiction, that $\exists z$ satisfying $z \in N^{*}(x_a ; \frac{\epsilon_a}{2}) \cap N^{*}(x_b ; \frac{\epsilon_b}{2})$ . Without loss of generality, suppose $\epsilon_a \geq \epsilon_b$ . Then, \begin{equation*}
	            \left|x_a - x_b\right| = \left|(x_a -z) + (z - x_b)\right|
	             \leq \left|x_a -z \right| + \left|z - x_b\right| < \epsilon_{a/2} + \epsilon_{a/2} = \epsilon_{a}
	        \end{equation*} Thus, $\left|x_a - x_b\right| < \epsilon_{a} \implies x_b \in N^{*}(x_a ; \epsilon_a)$ which contradicts $N^{*}(x_a ; \epsilon_a) \cap B = \emptyset$ . Then, by the density of rationals in reals, we can find a $q_i \in \mathbb{Q}$ s.t. $q_i \in N^*(x_i ; \frac{\epsilon_i}{2})$ . This means that we can draw a $1-1$ correspondence between each $q_{i}$ and $x_{i}$ which is a contradiction since there are only a countable number of $q_{i}$ and an uncountable number of $x_{i}$ .","['general-topology', 'solution-verification', 'real-analysis']"
3676408,Bounded or unbounded for a function $f$ in a metric space.,"Let $X$ be a complete metric space and $f:X\to X$ be a map. Let $\phi: \Bbb R^+ \to \Bbb R$ be any map with $\phi(x)<x$ for all $x\in \Bbb R^+$ . Let $\displaystyle P=\frac{d(fx,fy)-\phi(d(x,y))}{d(x,fx)+d(y,fy)}$ for all $x,y\in X$ , where $d(x,fx)$ and $d(y,fy)$ NOT both zero simultaneously. . I want to find an example of such function $f$ with suitable space $X$ and suitable metric $d$ such that for any function $\phi$ , $P$ becomes unbounded. I have tried by considering various functions and spaces but unable to find such. Please anyone help me to find such. If there is no such $f$ then please give some hint how to prove it?","['metric-spaces', 'analysis', 'real-analysis', 'functions', 'functional-analysis']"
3676422,Edge probability and expected number of edges in the configuration model,"This question is related to question: Probability that exists at least an edge in the configuration model There is something I do not understand about the computation of the expected number of edges between $i$ and $j$ nodes in the configurational model , $p_{ij}$ . The argument given everywhere I've seen is: There are $2m$ stubs in the network, with $k_i$ in node $i$ and $k_j$ in node $j$ . Taking one stub from node $i$ , there are $k_j$ possible stubs to connect it to node $j$ , so the probability to connect it to node $j$ is $\frac{k_j}{2m-1}$ , the $2m-1$ because you can not connect it to the same stub you are coming from. There are $k_i$ stubs in node i, so the expected number of edges is just adding up the different probabilities and $p_{ij} = k_i \times \frac{k_j}{2m-1}$ . I do not understand step 3. I would think once there has been an edge between nodes $i$ and $j$ , the probability to connect the next stub should change accordingly because there is one less available stub at node $j$ : $\frac{k_j-1}{2m-3}$ . But also, each new stub considered in node $i$ has two less possible stubs to be connected (because every other edge already connected has two stub ends), so the total available edges in the denominator should decrease as well: $2m-3$ , $2m-5$ , ..., $2m-2k_i-1$ . Instead, I'd proceed in this way: $$p_{ij} = 1 - \bar{p}_{ij}, $$ where $\bar{p}_{ij}$ is the probability there isn't any edge between nodes $i$ and $j$ . Then, $$\bar{p}_{ij} = \bar{p}_{{i_1}j} \times \bar{p}_{{i_2}j}\times \dots  \times \bar{p}_{{i_{k_i}}j}, $$ where $\bar{p}_{{i_1}j}$ is the probability there isn't an edge between the first stub in node $i$ to node $j$ and $\bar{p}_{{i_1}j} = \frac{2m-1-k_j}{2m-1}$ . Analogously for the other stubs, we get $$\bar{p}_{ij} = \frac{2m-1-k_j}{2m-1}  \frac{2m-3-k_j}{2m-3} \dots \frac{2m-2k_i-1-k_j}{2m-2k_i-1} = \left( 1 - \frac{k_j}{2m-1} \right)   \left( 1 - \frac{k_j}{2m-3} \right)  \dots \left( 1 - \frac{k_j}{2m-2k_i-1} \right).  $$ So $$p_{ij} = 1- \left( 1 - \frac{k_j}{2m-1} \right)   \left( 1 - \frac{k_j}{2m-3} \right)  ... \left( 1 - \frac{k_j}{2m-2k_i-1} \right).$$ I can recover from this expression the other one in the large number of edges limit $m \to \infty$ , then $2m-2k_i-1 \simeq ... \simeq 2m - 3 \simeq 2m - 1$ and $$p_{ij} \simeq 1- \left( 1 - \frac{k_j}{2m-1} \right)^{k_i} \simeq 1 - \left( 1 - \frac{k_i k_j}{2m-1} \right) = \frac{k_i k_j}{2m-1},$$ where in the second step I have used the series expansion $(1 - x)^a = 1 - ax + \mathcal{O}(x^2)$ for $x \to 0$ . Question: Does this mean that only the expected number of edges between $i$ and $j$ nodes in the configurational model is $p_{ij} = \frac{k_i k_j}{2m-1}$ in the large number of edges $m$ limit? If that is the case, I find it strange because they don't specify it in any of the sources I've looked. Instead, they seem to say $p_{ij} = \frac{k_i k_j}{2m-1}$ is the general expression which in the large number of edges limit becomes $p_{ij} = \frac{k_i k_j}{2m}$ .","['random-graphs', 'network', 'graph-theory', 'probability-theory', 'probability']"
3676430,"What is the ""full"" topological requirements for a (classical) spacetime?","The model for (classical) spacetimes are, fundamentally, topological Manifolds. I know that this isn't the complete structure (because in fact we need to realize what is lorentz manifolds and so on...). But, the thing is, topological manifolds are the cement of spacetime definition and there's a lot of equivalent ways to use the ""requirements"" (paracompactness, hausdorff condition, etc...) and define properly what kind of structure we need to impose on they. The fact is that I wish to know all the ""hierarchical path"" to spacetime manifolds. I will explain. The requirements are then: $(\mathcal{M}, \tau)$ , a topological space, of course. Then, the next structures are listed in the following, but I don't know, properly, how is the hierachy between then (which one implies the other and so on...). First Countable Second-Countable Connected Path-Connected Separable Hausdorff Compact Paracompact Metrizable Metric Space I know that we use all of these to define, properly, a spacetime, but I don't know what is the hierarchy between them. Please fell free to be redundant, because the kind of answer that I'm looking for is something like: A spacetime $\mathfrak{M}$ is a topological manifold $(\mathcal{M}, \tau)$ , which is compact, paracompact, metrizable and so on.... So, how can I use the requirements listed above, from $1$ to $11$ , to bake the ""full"" definition of a spacetime? $$ * * * $$ Also there's another concept that I don't know how to fit in between the requirements $1$ to $10$ Normal (or Regular)","['manifolds', 'general-topology', 'differential-topology', 'general-relativity', 'differential-geometry']"
3676494,"A closed form for $ \int_{0}^{\frac{\pi}{2}}{\left(\frac{\sin{\left(nx\right)}}{\sin{x}}\right)^{2p}\,\mathrm{d}x} $?","For $ n,p\in\mathbb{N} $ , define $ I_{p}\left(n\right) $ as follows : $$ I_{p}\left(n\right)=\int_{0}^{\frac{\pi}{2}}{\left(\frac{\sin{\left(nx\right)}}{\sin{x}}\right)^{2p}\,\mathrm{d}x} $$ A closed form can be found for $ I_{0}\left(n\right) $ , $ I_{1}\left(n\right) $ and $ I_{2}\left(n\right) $ , for any $ n\in\mathbb{N} $ , we have the following identities : \begin{aligned} I_{0}\left(n\right)&=\frac{\pi}{2} \\I_{1}\left(n\right)&=\frac{n\pi}{2} \\I_{2}\left(n\right)&=\frac{n\pi\left(2n^{2}+1\right)}{6}\end{aligned} Can we generalise the result for all $ p\in\mathbb{N} $ ?","['integration', 'trigonometry', 'sequences-and-series']"
3676559,Cohomology of a monad.,"Definition . A monad over a projective variety $X$ is a complex $$M : 0 \longrightarrow \mathcal{A} \stackrel{f} {\longrightarrow} \mathcal{B} \stackrel{g} {\longrightarrow} \mathcal{C} \longrightarrow 0$$ of coherent sheaves over $X$ which is exact at $\mathcal{A}$ and at $\mathcal{C}$ , that means $g \circ f = 0$ , $f$ is injective and $g$ is surjective. The coherent sheaf $E : = \dfrac{\text{Ker}(g)}{Im(f)}$ will be called cohomology of $M$ . Consider the monad $M$ and the exact sequences associated with it. $$M : 0 \longrightarrow \mathcal{O}_{\mathbb{P}^{3}}(-1) \stackrel{f} {\longrightarrow} \mathcal{O}_{\mathbb{P}^{3}}^{\oplus 4} \stackrel{g} {\longrightarrow} \mathcal{O}_{\mathbb{P}^{3}}(1) \longrightarrow 0$$ $$0 \longrightarrow \mathcal{O}_{\mathbb{P}^{3}}(-1) \longrightarrow  K \longrightarrow E \longrightarrow 0 \tag{1}$$ $$ 0 \longrightarrow K \longrightarrow \mathcal{O}_{\mathbb{P}^{3}}^{\oplus 4} \longrightarrow \mathcal{O}_{\mathbb{P}^{3}}(1) \longrightarrow  \tag{2}0 $$ $$0 \longrightarrow \mathcal{O}_{\mathbb{P}^{3}}(-1) \longrightarrow \mathcal{O}_{\mathbb{P}^{3}}^{\oplus 4}  \longrightarrow Q \longrightarrow 0 \tag{3}$$ and $$0 \longrightarrow E \longrightarrow Q \longrightarrow \mathcal{O}_{\mathbb{P}^{3}}(1) \longrightarrow 0 \tag{4}$$ where $K = \text{Ker}(g)$ , $Q = \text{coker}(f)$ and $E$ is the cohomology of $M$ . In this case, we have that $E$ is a instanton sheaf of charge $c = 1$ . As your dual $E^\vee$ is a reflexive sheaf and $E \simeq E^{\vee}$ we have that $E$ is a  reflexive sheaf. The goal here is  to calculate $\text{dim}Hom \bigl( E \otimes T_{\mathbb{P}^{3}} \bigr) = h^{0}(E^{\vee} \otimes T_{\mathbb{P}^{3}}) = h^{0}(E \otimes T_{\mathbb{P}^{3}})$ . My attempt . By [I, see proposition 19] we have $E$ is locally free and stable. Twisting the Euler sequence by $E$ , we get $$ 0 \longrightarrow E \longrightarrow E(1)^{\oplus 4} \longrightarrow E \otimes  T_{\mathbb{P}^{3}} \longrightarrow 0 \tag{5}$$ By [II, see Lemma 1.2.5] we have $H^{0}(\mathbb{P}^{3}, E_{norm} = E) = 0$ , because $c_{1}(E) = 0$ . Now, by [III, see Corollary 3.3] we have that $E$ is $1$ -regular. So $H^{i}( E(1-i)) = 0$ for all $i > 0$ . Even with this information and using the exact sequences (1), (2), (3) and (4), I still haven't been able to reach the goal mentioned above. Any help is most welcome. Thank you very much. I) Instanton Sheaves on Complex Projective Space. (Marcos Jardim), II) Vector Bundle on Comlex Projective Spaces. (Okonek), III) Monads and Regularity of Vector Bundles on Projective Varieties. (M.MirÃ³-Roig).","['exact-sequence', 'algebraic-geometry', 'sheaf-cohomology', 'sheaf-theory']"
3676584,FaÃ  di Bruno's formula for $C^k$ Banach-valued functions.,"Let $X,Y,Z$ be Banach spaces, $f:X\to Y,g: Y\to Z$ be two functions of class $C^k$ , which means that $f^{(k)}(x)$ exists as a $k$ -linear form $\mathcal B^k(X;Y)$ and similarly for $g^{(k)}$ . Is the a FaÃ  di Bruno-like formula for computing the $k$ -linear form $(g\circ f)^{(k)}(x)\in \mathcal B^k(X;Z)$ ? The usual FaÃ  di Bruno's formula already looks horrible enough for real-valued $f,g$ . I can't imagine how complicated its Banach-valued would be but I am sure someone must have thought about it. If anyone know where I can look for such a formula I would be very grateful. Alternatively, I would be satisfied with a proof of the following statement: For $f\in C^k(X;Y)$ and $g\in C^k(Y;Z)$ , it is the case that $g\circ f\in C^k(X;Z)$ . It would be a direct consequence of the Banach version of FaÃ  di Bruno's formula (if there is one, which I'm quite sure there is). The statement seems simple enough and I tried proving it using induction. However, applying chain rule twice in the case $k=2$ already looks horrible and I'm not sure what's the correct way to prove it. Maybe I should do an induction on some tree-like structures but I'm not sure of the details.","['differential-geometry', 'real-analysis', 'functional-analysis', 'partial-differential-equations', 'dynamical-systems']"
3676588,If a metric space $M$ is not compact then the space of probability measures $\mathcal M_1(M)$ is not compact.,"Let $M$ be a metric space. Then we define the space $\mathcal M_1(M)$ as the topological space $$\mathcal M_1(M) :=\left\{\mu;\ \mu\ \text{is a } \text{Borel probability measure on }M\right\} $$ endowed with the weak $^*$ topology, $\textit{i.e}.$ the topology generated by the basis of neighborhoods $$V(\mu;f_1,\ldots,f_n;\varepsilon):=\left\{\lambda\in\mathcal M_1(M);\ \left|\int f_i\  \mathrm{d}\mu - \int f_i\  \mathrm{d}\lambda\right|<\varepsilon, \ \forall \ i\in\{1,\ldots,n\}\right\}, $$ where $f_1,\ldots,f_n \in C^0_b(M)=\{g: M\to\mathbb R; \ g \text{ is a continuous bounded function}\}$ . It is well known that if $M$ is a  separable metric space. Then $M$ is compact $\iff$ $\mathcal{M}_1(M)$ is compact. The proof of this fact relies on the result "" $M$ is separable metric space then $\mathcal{M}_1(M)$ in the weak $^*$ topology is metrizable with LÃ©vyâ€“Prokhorov metric "". Question: If $M$ is a non-compact metric space $\Rightarrow$ $\mathcal M_1(M)$ a non-compact topological space in the weak $^*$ topology? My conclusions so far I think I have found a solution. Suppose that $M$ is a non-compact metric space then there exists a sequence $\{x_n\}_{n\in\mathbb{N}}$ , such that, $\{x_n\}_{n\in\mathbb{N}}$ does not admit convergent subsequÃªnce. Therefore, the set $S = \{x_1, \ldots, x_i,\ldots \}\subset M$ is closed, moreover all subsets of $S$ are  closed as well. Now, consider the sequence $\{\delta_{x_i}\}_{i\in\mathbb{N}}\subset\mathcal M_1(M)$ (where $\delta_p$ is the Dirac measure ), we will show that $\{\delta_{x_i}\}_{i\in\mathbb{N}}$ does not admit convergence subnet, and this will imply that $\mathcal M_1(M)$ is not compact in the weak $^*$ topology (because we would be able to find a net that does not admit convergent subnet). Suppose by reductio ad absurdum that there exist a directed set $(\mathcal B, \preccurlyeq)$ , an increasing function $\varphi: \mathcal B\to\mathbb{N}$ , such that $\lim_{\beta}\varphi(\beta)=\infty$ (or cofinal porperty) and $\mu\in\mathcal M_1(M)$ satisfying $$\lim_{\beta} \delta_{x_{\varphi(\beta)}} =\mu \in\mathcal M_1(M). $$ Let us, first of all, prove that $\mu(S)=1$ . Let $f: M\to\mathbb{R}$ a continuous bounded function such that $f(S)=\{1\}.$ Then, by the definition of weak $^*$ topology $$1 = \lim_\beta \int f \ \mathrm d \delta_{x_{\varphi(\beta)}} =   \int f \ \mathrm d \mu.$$ Now consider the continuous (since $S$ is closed) bounded functions \begin{align*}
g_n: M&\to\mathbb{[0,1]}\\
x&\mapsto \max\left\{ 1 - n \cdot d(x,S) ,0\right\},
\end{align*} so $g_n \to \mathbf{1}_S$ pointwiselly ( where $\mathbf{1}_S( S ) =\{1\}$ and $\mathbf{1}_S\left(M\setminus S\right) =\{0\}$ ) and each $g_n$ is a continuous bounded function such that $g_n(S)=\{1\}$ . Note that $|g_i(x)|\leq g_1(x)$ , $\forall$ $x\in M$ . Thus, by dominated convergence theorem $$1 = \lim_{n\to\infty} \int g_n\ \mathrm{d}\mu  =  \int \lim_{n\to\infty}g_n\ \mathrm{d}\mu  =  \int \mathbf{1}_s\ \mathrm{d}\mu = \mu(S). $$ Now, let $i\in\mathbb{\mathbb N}$ and consider $S_i = \{1,\ldots,i\}$ , by hypothesis $S_i$ and $S\setminus S_i$ are both closed, so there exists a continuous function $g: M\to [0,1]  $ such that $g(S_i) = \{1\}$ and $g(S \setminus S_i) = \{0\}$ , since $\varphi(\beta)\to\infty$ , there exists $\beta_0\in\mathcal B$ , $\beta_0\preccurlyeq \beta$ , implies $i < \varphi(\beta)$ . Hence, $$0 = \lim_\beta \int g \ \mathrm d \delta_{x_{\varphi(\beta)}} =   \int g \ \mathrm d \mu \geq \mu(S_i).$$ Since $\mu(S) \leq \sum_{i=1}^{\infty}\mu(S_i) = 0$ . Then $\mu\not\in \mathcal{M}_1(M)$ , implying that $M$ is not compact. Is the proof above correct? If so, why the vast majority of the books (at least all that I have checked) people just state the result for the case where $M$ is separable metric space? Can anyone help me?","['measure-theory', 'ergodic-theory', 'solution-verification', 'functional-analysis', 'general-topology']"
3676627,Proof related to the compostion of limits: am I proving it correctly?,"Let $(X,d_{X})$ , $(Y,d_{Y})$ , $(Z,d_{Z})$ be metric spaces, and let $x_{0}\in X$ , $y_{0}\in Y$ and $z_{0}\in Z$ . Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be functions, and let $E$ be a set. If we have $\displaystyle\lim_{x\rightarrow x_{0};x\in E}f(x) = y_{0}$ and $\displaystyle\lim_{y\rightarrow y_{0};y\in f(E)}g(y) = z_{0}$ , conclude that $\displaystyle\lim_{x\rightarrow x_{0};x\in E}(g\circ f)(x) = z_{0}$ . MY ATTEMPT According to the definition of limit, for every $\varepsilon > 0$ , there is a $\delta_{1} > 0$ such that for every $y\in f(E)$ we have that \begin{align*}
d_{Y}(y,y_{0}) < \delta_{1} \Rightarrow d_{Z}(g(y)),z_{0}) < \varepsilon
\end{align*} Simlarly, for every $\delta_{1} > 0$ there is a $\delta > 0$ such that for every $x\in E$ one has that \begin{align*}
d_{X}(x,x_{0}) < \delta \Rightarrow d_{Y}(f(x),y_{0}) < \delta_{1}
\end{align*} Since $y\in f(E)$ , we can assume that $y = f(x_{0})$ where $x_{0}\in E$ . Gathering both results, we conclude that for every $\varepsilon > 0$ , there is a $\delta > 0$ such that whenever $x\in E$ it results that \begin{align*}
d_{X}(x,x_{0}) < \delta \Rightarrow d_{Y}(f(x_{0}),y_{0}) < \delta_{1} \Rightarrow d_{Z}(g(f(x_{0})),z_{0}) < \varepsilon
\end{align*} that is to say, $(g\circ f)(x)$ approaches $z_{0}$ as $x$ approaches $x_{0}$ . I am mainly interested at knowing if I am writing rigorously and properly the proposed statement. Can someone tell me so?","['limits', 'solution-verification', 'metric-spaces']"
3676630,"If there are more than $\frac{3(n-1)}{2}$ edges then there exist veritces $u,v$ with $3$ vertex-disjoint $(u,v)$-paths","If a graph $G$ with $n \geq 4$ vertices has more than $\frac{3(n-1)}{2}$ edges then there exist $u,v \in V(G)$ with $3$ vertex-disjoint $(u,v)$ -paths. I tried induction but didn't work. Can someone provide me with a hint on how to start?","['graph-theory', 'discrete-mathematics']"
3676680,Compute $\int_0^{+\infty}\frac{\sin x + \cos x}{x^4+1}dx$,"Problem: Compute $$\int_0^{+\infty}\frac{\sin x + \cos x}{x^4+1}dx$$ using the Residue Theorem. My attempt: We know that $$\sin x+ \cos x=\sqrt{2} \sin(x + \frac{\pi}{4})$$ Thus we can reduce to compute: $$\int_0^{+\infty}\frac{e^{ix}}{x^4+1}dx$$ Defining $\alpha_R(t)=t$ for $t \in [0,R]$ , $\beta_R(t)=Re^{it}$ for $t \in [0,\theta]$ and finally $\gamma_R(t)=e^{i\theta}tR$ where $\theta$ is an opportune angle to be chosen I would like to use then Residue Theorem but I cannot say anything about: $$\int_{\gamma_R}f(z)dz$$ where $f(z)=\frac{e^{iz}}{z^4+1}$ .","['complex-analysis', 'calculus', 'definite-integrals', 'residue-calculus']"
3676753,"Unexpected formulas for ""exactly $k$ sets"" and ""at least $k$ sets"" variations of the principle of inclusion-exclusion","There are two formulas that I have derived, and the difference between them is puzzling me. Let $n$ be a positive integer and $A_1,A_2,\ldots, A_n$ be finite sets, and let $k$ be an integer such that $1\le k\le n.$ The number of elements of $\displaystyle \bigcup_{i=1}^{n}{A_i}$ that lie in exactly $k$ of the $A_i$ is $$\sum_{m=k}^{n}{(-1)^{m+k}\binom{m}{k}\sum_{\substack{J\subseteq[n]\\ |J|=m}}{\left|\bigcap_{j\in J}A_{j}\right|}}.$$ The number of elements of $\displaystyle \bigcup_{i=1}^{n}{A_i}$ that lie in at least $k$ of the $A_i$ is $$\sum_{m=k}^{n}{(-1)^{m+k}\binom{m-1}{k-1}\sum_{\substack{J\subseteq[n]\\ |J|=m}}{\left|\bigcap_{j\in J}A_{j}\right|}}.$$ Here, $[n]$ denotes the section $\{1,2,3,\ldots, n\}$ of the positive integers. I am fairly certain that these formulas are correct, as I have tested them in various specific and general cases. For example, $k=1$ in the second formula yields the ordinary PIE formula. In fact, I used the first formula in conjunction with combinatorial identities to prove the second one; and I know that the first formula is true using a combinatorial proof. Anyway, here is my dilemma: For fixed $n$ and $k,$ the second expression is greater than or equal to the first expression. This just seems weird to me because the second expression involves the binomial coefficients of the row above the row in which the binomial coefficients of the first expression lie in Pascal's triangle. I would have expected it to be the other way around. It seems that the alternating signs somehow produce this counterintuitive result, which I have proven but have been unable to fathom. Is there an explanation for this? Is the inequality resulting from saying the second expression is greater than or equal to the first expression evident in some larger theorem or context? As a side note, I was also wondering if this is in some way connected to the Bonferroni inequalities. This is just something I was musing about, inspired by the similarity in form and context, but I'm not too hopeful about it as the stated formulas have binomial coefficients, which Bonferroni does not.","['combinatorial-proofs', 'matrices', 'inclusion-exclusion', 'binomial-coefficients', 'combinatorics']"
3676766,Explain step in Galindo and Pascual (Quantum Mechanics I) proof of self-adjointness of the momentum operator in QM,"In the book Quantum Mechanics (Volume I) by Galindo & Pascual, they define the domain of the QM momentum operator on the Hilbert space $\mathcal{H}=L^2(\mathbb{R})$ as \begin{equation*}
  D(P)=\Biggl\{\psi\in\mathcal{H}: \psi\text{ absolutely continuous,}
  \int_{-\infty}^\infty\!dx\,\Biggl\lvert\frac{d\psi(x)}{dx}\Biggr\rvert^2<\infty\Biggr\}
\end{equation*} and the momentum operator $P$ by $$(P\psi)(x)=-i\frac{d\psi(x)}{dx}.$$ They go on to prove that $P$ is densely defined and symmetric. To prove that $P$ is self-adjoint, they attempt to show that $D(P^\dagger)\subseteq D(P)$ . Here are the next couple of lines of the proof: ... consider a function $\psi\in D(P^\dagger)$ and define $\psi_1=P^\dagger\psi$ ; then $$\langle\psi|P|\varphi\rangle=\langle\psi_1|\varphi\rangle,\quad\forall\varphi\in D(P)$$ can be rewritten as \begin{equation*}
  \begin{split}
    \langle\psi|P|\varphi\rangle
    &=\int_{-\infty}^\infty\!dx\,\psi_1^*(x)\varphi(x)\\
    &=i\int_{-\infty}^\infty\!dx\Biggl[\frac{d}{dx}
    \Biggl(i\int_0^x\!dt\,\psi_1(t)+c\Biggr)^*\Biggr]\varphi(x),
  \end{split}
\end{equation*} where $c$ is an arbitrary constant. Choosing $\varphi\in C^\infty_0$ , integrating by parts,
and taking into account that $\varphi$ is zero outside a finite interval, we obtain \begin{equation}\tag{2.16}
  \int_{-\infty}^\infty\!dx\Biggl(\psi(x)-i\int_0^x\!dt\,\psi_1(t)-c\Biggr)^*
  \Biggl(-i\frac{d\varphi(x)}{dx}\Biggr)=0,\quad\forall\varphi\in C^\infty_0.
\end{equation} [So far, this seems OK to me. It is the next statement that I don't follow:] Since $C^\infty_0$ is dense in $L^2(\mathbb{R})$ , the first factor of the integrand
in (2.16) must be a constant and hence, with a convenient choice $c_0$ for $c$ , we can write
almost everywhere \begin{equation}\tag{2.17}
  \psi(x)=c_0+i\int_0^x\!dt\,\psi_1(t),
\end{equation} [and it goes on from there] I want to concentrate on the validity of going from (2.16) to (2.17). I understand that,
with total lack of rigor, if we have $$\int h\varphi'=0\quad\forall\varphi\in C^\infty_0$$ we'd like to do an integration by parts and write $$\int h'\varphi=\int h\varphi'=0\quad\forall\varphi\in C^\infty_0$$ from which we would get that $h'=0$ almost everywhere hence $h=c$ almost everywhere.
But I don't see how to apply that here since I don't know that $\psi$ is differentiable a.e.
or even a.e. on a compact interval. It even looks like a version of the DuBois-Raymond theorem from variational calculus, but I only know that for continuous functions on a compact interval, so it would seem to not apply here. So, my questions are: how do you get from (2.16) to (2.17)? what element of $L^2(\mathbb{R})$ would they be talking about when they say that $C^\infty_0$ is dense in $L^2(\mathbb{R})$ ?","['hilbert-spaces', 'measure-theory', 'lebesgue-integral', 'functional-analysis']"
3676774,Combinatorics task.,"Four men and four women shall get in line in a supermarket. In how many ways can they line up, if the line has to alternate between men and women (two men and women can not stand next to each other)? My solution: $4 \cdot 4 \cdot 3 \cdot 3 \cdot 2 \cdot 2 \cdot 1 \cdot 1 = 576$ ; Then we multiplicate it by two because a man or a woman can be first in line; $576 \cdot 2 = 1152$ . The answer given in my maths book: $2304$ . What did I do wrong?",['combinatorics']
3676789,Is Leibniz rule applied correctly?,"I wish to know if the Leibniz rule is correctly applied in the following equation or I am missing something: $$\int \int \frac{\partial f(x,y,z)}{\partial x} dy dz =\frac{\partial  \left(\int \int f(x,y,z)dy dz \right)}{\partial x}  $$ $x, y$ and $z$ are independent of each other. The integration limits are constant. Thanks in advance.","['multivariable-calculus', 'calculus', 'leibniz-integral-rule']"
3676810,How do inverse functions exist for exponential functions?,"I know that they exist for exponential functions (we currently have them in class), but to me it doesn't seem ""reasonable"" when I look at the definition of what an inverse function is.
The inverse is defined as a function where you can swap $x$ and $y$ , then solve for $y$ and the notation being $\operatorname{f^{-1}}(x)$ . Since functions are a 1 to 1 mapping this can only be true for some functions. In the textbook we use we have following definition for the domain of functions/inverse functions: $$\mathbb{D}_{f} = \mathbb{W}_{f^{-1}} \rightleftharpoons \mathbb{W}_{f} = \mathbb{D}_{f^{-1}}$$ I also get that some functions don't have inverses or where they only exist for a restricted domain (like $x^2$ where you have to restrict the domain, or some functions where you can't solve for $x$ ). The thing about as example $2^x$ that puts me off is that the input domain $\mathbb{D}$ consists out of all real numbers, whereas the output is made out of positive real numbers only. How can there be a 1 to 1 mapping if the output consists only out of positive real numbers, aren't there less positive real numbers than real numbers? With as example $x^3$ you use up all $x$ and $y$ values, so it having a valid inverse makes intuitive sense to me. We get taught about how important the uniqueness of the mapping between $x$ and $y$ is, but it just feels wrong for exponential functions. Can anyone provide me a pointer to as where I start thinking about this wrongly? I have solved all the problems in our book and on the additional sheet the teacher gave us and have only had a few mistakes (which probably came from lack of sleep). Understanding the composition of functions was pretty easy for me as well, thanks to knowing higher order functions. I'm really sure I'm misunderstanding something elementary the wrong way.","['algebra-precalculus', 'functions']"
3676815,Is a homogeneous Banach space on $\mathbb T$ always well defined?,"A homogeneous Banach space $B$ on group $\mathbb T=\mathbb R/2\pi\mathbb Z$ is a linear subspace $B$ of $L^1(\mathbb T)$ having a norm $\|\ \|_B\ge\|\ \|_{L^1}$ under which it is a Banach space, and having the following properties: If $f\in B$ and $\tau\in\mathbb T$ , then $f_\tau\in B$ and $\|f_\tau\|_B=\|f\|_B$ (where $f_\tau=f(t-\tau)$ ). For all $f\in B\quad $ , $\tau, \tau_0\in\mathbb T,\quad $$\lim_{\tau\to\tau_0}\|f_\tau-f_{\tau_0}\|=0.$ I am wondering whether this is well-defined. For instance, if we have $f_1,f_2\in B\subseteq L^1(\mathbb T)$ and $f_1=f_2$ in $L^1(\mathbb T)$ , then the values of $f_1$ and $f_2$ differ at most at a zero measure set. But it is also possible that $f_1\ne f_2$ in $B$ and in which case, it does not make sense to say the $B$ -norm of $f_1$ or $f_2$ since they are considered to be one element in $L^1(\mathbb T)$ and also in the subspace $B$ of $L^1(\mathbb T)$ . Whereas, $\|f_1\|_B\ne\|f_2\|_B$ is possible.","['harmonic-analysis', 'fourier-analysis', 'functional-analysis']"
3676830,Why do we use this complicated algorithm for finding values of trigonometric functions?,"My Mathematics Textbook covers the topic of Values of Trigonometric Functions at Allied Angles using some general formulae first and then goes on to the topic of finding the values of trigonometric functions at allied angles using an algorithm . The cases discussed in finding the values using some general formulae are : At $(-x)$ At $\Big (\dfrac{\pi}{2} \pm x \Big )$ At $(\pi \pm x)$ At $\Big ( \dfrac {3\pi}{2} \pm x \Big )$ At $(2\pi \pm x)$ which can also be written as $(\pm $ $x)$ The algorithm is as follows : Let the angle be $x$ If $x<0$ and $x = (-a)$ , continue with the further steps as $a$ in place of $x$ and when the final result arrives : if $f$ is an even function, then $f(x) = f(-x)$ , so $f(x) = f(a)$ and if $f$ is an odd function, $f(-x) = -f(x)$ , so $f(x) = -f(-x) = -f(a)$ Express $x$ (or $a$ ) in the form of $\dfrac {n\pi}{2} \pm \alpha$ , where $0<\alpha<\dfrac{\pi}{2}$ or $\alpha \in \Big (0, \dfrac {\pi}{2} \Big )$ If $n$ is odd, then $\sin x = \pm \cos \alpha$ , $\cos x = \pm \sin \alpha$ , $\tan x = \pm \cot \alpha$ , $\cot x = \pm \tan \alpha$ , $\sec x = \pm \csc \alpha$ and $\csc x = \pm \sec \alpha$ If $n$ is even, then $\sin x = \pm \sin \alpha$ , $\cos x = \pm \cos \alpha$ , $\tan x = \pm \tan \alpha$ , $\cot x = \pm \cot \alpha$ , $\sec x = \pm \sec \alpha$ and $\csc x = \pm \csc \alpha$ Determine the quadrant that $x$ lies in and then decide the sign of the value Let's take an example : Find the value of $\sin \dfrac{7\pi}{4}$ . One method to do this will be using the first method. $\sin \dfrac{7\pi}{4} = \sin \Big (2\pi - \dfrac{\pi}{4} \Big )$ We know that $\sin (2\pi-x)=(-\sin x)$ . So, $\sin \Big (2\pi - \dfrac{\pi}{4} \Big ) = \Big ( -\sin \dfrac {\pi}{4} \Big ) = -\dfrac {1}{\sqrt{2}}$ Another method would be to use the algorithm $\dfrac{7\pi}{4}=\dfrac{3\pi}{2}+\dfrac{\pi}{4}$ , so $\dfrac {3\pi}{2} < \dfrac{7\pi}{4} < 2\pi$ and $\dfrac{7\pi}{4}$ lies in the $IV$ quadrant, which means that $\sin \dfrac{7\pi}{4} < 0$ Now, $\dfrac{7\pi}{4} = \dfrac {3.\pi}{2} + \dfrac{\pi}{4}$ . $3$ is odd, so $\sin \dfrac{7\pi}{4} = -\sin \dfrac {\pi}{4} = -\dfrac{1}{\sqrt{2}}$ Now, this algorithm seems like something extremely complex for solving simple questions like these. So, why do we use this algorithm when we can just use the simple formulae that help us find the values of trigonometric functions at allied angles? Are there some advantageous applications of this algorithm? I feel like it's just a generalization for all the cases that appear in the case of allied angles, just like the lens formula is a generalization of all the cases of image formation through lenses. Thanks! EDIT : Also, when should I use which method?",['trigonometry']
3676881,Why is it so computationally hard to determine group isomorphism?,"Finding an isomorphism requires to show that for 2 groups $G$ and $H$ , there exists a bijective map $\phi : G\to H$ such that $$\phi(ab)=\phi(a)\phi(b)$$ For all $a,b \in G$ . This is (probably
naively) pretty straight forward, and there are plenty of theorems that allow us to show that groups of specific orders must be isomorphic to some specific set of groups. So, my question (which I hope isnâ€™t too loaded) is What intrinsically makes finding if 2 groups are isomorphic so hard? Is it showing that they are bijective, is it showing that they are operation preserving, or is it something entirely different?","['group-isomorphism', 'computational-algebra', 'abstract-algebra', 'group-theory', 'computational-complexity']"
3676905,Finding an integration factor $x^ay^b$ to solve an ODE,"I have to solve an ODE: $$2(y-3x)dx+x\left(3-\frac{4x}{y} \right) dy=0$$ I am given that I have to use the integrating factor x^a(y^b) where a and b are real numbers in order to turn the problem into a solvable exact ODE. The problem is that I am unsure of how to find these real numbers. From my understanding, an ODE is exact with the integrating factor (I) if the partial derivative of the dx w.r.t y is equal to the partial derivative of the dy w.r.t x (both multiplied with I). I tried doing this but I am unsure of how to proceed with the algebra. So far I have: If anyone can help with the algebra to find the values for a and b that would be appreciated","['integration', 'integrating-factor', 'ordinary-differential-equations']"
3676911,"What is the Hessian matrix of $f\left(x\right)=\left\langle Ax,x\right\rangle \cdot\left\langle Bx,x\right\rangle $?","I'm trying to understand what is the Hessian matrix of $f\colon\mathbb{R}^{n}\to\mathbb{R}$ defined by $f\left(x\right)=\left\langle Ax,x\right\rangle \cdot\left\langle Bx,x\right\rangle $ where $A,B$ are symetric $n\times n$ matrices. What I know is that
if we let $g\left(x\right)=\left\langle Ax,x\right\rangle $ and $h\left(x\right)=\left\langle Bx,x\right\rangle $ then $\nabla g\left(x\right)=2Ax,\nabla h\left(x\right)=2Bx$ and $\nabla^{2}g\left(x\right)=2A,\nabla^{2}h\left(x\right)=2B$ . Also
by the product rule we have $\left(fg\right)'=f'g+fg'$ which then
gives us \begin{align*}
\left(fg\right)'' & =f''g+f'g'+f'g'+fg''=\\
 & =f''g+2f'g'+fg''
\end{align*} Regarding $\nabla f\left(x\right)$ as a column vector, I tried to
implement this on the given $f\left(x\right)$ and what I got is $$
\nabla f\left(x\right)=\nabla\left(gh\right)\left(x\right)=2Ax\cdot\left\langle Bx,x\right\rangle +\left\langle Ax,x\right\rangle \cdot2Bx
$$ which seems to have worked fine with a concrete example. But then
I got to the Hessian: \begin{align*}
\nabla^{2}f\left(x\right) & =\nabla^{2}\left(gh\right)\left(x\right)=2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{2Ax\cdot2Bx}}+\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{2Ax\cdot2Bx}}+\left\langle Ax,x\right\rangle \cdot2B=\\
 & =2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\right)}}{\underbrace{8Ax\cdot Bx}}+\left\langle Ax,x\right\rangle \cdot2B
\end{align*} Now as $Ax,Bx$ in $\left(\ast\right)$ are both column vectors I
thought I should try this instead $$
\nabla^{2}f\left(x\right)=2A\cdot\left\langle Bx,x\right\rangle +\underset{{\scriptscriptstyle \left(\ast\ast\right)}}{\underbrace{8Ax\cdot\left(Bx\right)^{T}}}+\left\langle Ax,x\right\rangle \cdot2B
$$ But that didn't work with my example. In general I feel the whole process of differentiating functions that
are represented by matrices is quite a mystery to me when it comes
to where I should transpose and so. Any help is appreciated. Thanks
in advance.","['matrices', 'hessian-matrix', 'derivatives']"
3676965,"Definition of a bounded, nonempty subset of real numbers.","When one refers to a ""bounded, nonempty subset of real numbers"" does it have to be a continuous subset or can it be a set of discrete numbers (e.g. $S = \{0, 1, 2, 3\}$ )? Thank you.","['elementary-set-theory', 'real-numbers', 'real-analysis']"
3676977,"If $a^2 + b^2 + c^2$ is divisible by $16$, then show that$ a^3 + b^3 + c^3$ is divisible by $64$. Where $a, b, c \in \mathbb{Z}$.","If $a^2 + b^2 + c^2$ is divisible by $16$ , then show that $a^3 + b^3 + c^3$ is divisible by $64$ ; where $a, b, c \in \mathbb{Z}$ . I began by proving that if $(a^3+b^3+c^3) -(a^2+b^2+c^2)$ is divisible $16$ , then $a^3+b^3+c^3$ will be divisible by $64$ . That is equal to $a^3-a^2+b^3-b^2+c^3-c^2$ $= a(a^2-a)+b(b^2-b)+c(c^2-c)$ . But this doesn't give a clear path for the question. Any help?","['number-theory', 'elementary-number-theory']"
3677015,"Prove that if $b_n$ is a subsequence of $a_n$ and $c_n$ is a subsequence of $b_n$, then $c_n$ is a subsequence of $a_n$.","Let $(a_{n})_{n=0}^{\infty}$ , $(b_{n})_{n=0}^{\infty}$ and $(c_{n})_{n=m}^{\infty}$ be sequences of real numbers. Then $(a_{n})_{n=0}^{\infty}$ is a subsequence of $(a_{n})_{n=0}^{\infty}$ . Furthermore, if $(b_{n})_{n=0}^{\infty}$ is a subsequence of $(a_{n})_{n=0}^{\infty}$ , and $(c_{n})_{n=0}^{\infty}$ is a subsequence of $(b_{n})_{n=0}^{\infty}$ , then $(c_{n})_{n=0}^{\infty}$ is a subsequence of $(a_{n})_{n=0}^{\infty}$ . My solution We say that a sequence of real numbers $(x_{n})_{n=0}^{\infty}$ is a subsequence of $(y_{n})_{n=0}^{\infty}$ if there exists a strictly increasing function $f:\textbf{N}\to\textbf{N}$ such that $x_{n} = y_{f(n)}$ . Based on this definition, we conclude that $a_{n}$ is a subsequence of itself: it suffices to choose $f(n) = n$ . On the other hand, if $b_{n}$ is a subsequence of $a_{n}$ and $c_{n}$ is a subsequence of $b_{n}$ , then exist functions $f:\textbf{N}\to\textbf{N}$ and $g:\textbf{N}\to\textbf{N}$ such that $b_{n} = a_{f(n)}$ and $c_{n} = b_{g(n)}$ . Consequently, $c_{n} = a_{f(g(n))}$ , where $f\circ g$ is strictly increasing because it is a composition of two strictly increasing functions. Could someone please tell me if I am missing any formal step? Any comment is appreciated!","['solution-verification', 'sequences-and-series', 'real-analysis']"
3677036,Monte Carlo and Sampling,"Suppose that we have a finite state space $E$ and a distribution $\pi:E \rightarrow (0,1)$ with $\pi(x) >0$ . The idea behind Monte Carlo is that we generate a Markov chain $X=(X_n,n\in \mathbb{N})$ with transition matrix $p$ such that $p$ is ergodic (irreducible and aperiodic) and that $\pi$ is the unique invariant distribution of $p$ , so that the total variation $||p^n(x,y) -\pi(y)||_{TV} \rightarrow 0$ as $n\rightarrow \infty$ . Question . However, I'm a little confused about the rigorous justification behind sampling. Suppose that we want to compute the expectation $\mathbb{E}f(Y)$ where $Y$ is a random variables with distribution $\pi$ . Then I understand that $\mathbb{E}^x f(X_n) \rightarrow \mathbb{E} f(Y)$ for bounded $f$ . But how do we approximate $\mathbb{E}^x f(X_n)$ ? I would assume that we would like to apply the strong law in ""some way"" with respect to the Markov chain $X$ so that almost surely, we have $$
\frac{1}{n} \sum_{k=1}^n f(X_k) \rightarrow \mathbb{E}f(Y), \qquad n\rightarrow \infty
$$ Of course, this is not properly justified, since $X_1,...$ are not pairwise independent and identically distributed so we can't use the strong law.","['monte-carlo', 'probability-theory', 'markov-chains']"
3677067,Can $4+(2k)!$ ever be a perfect square over the integers?,"Is there any pair of natural numbers $\{ k, m \}$ satisfying: $4+(2k)! = m^2$ ? I tried simplifying this into $$ 2^k k!(2k-1)!! = (m-2)(m+2) \text , $$ where !! denotes the double factorial, i.e., $1 \cdot 3 \cdot 5 \cdot \dots \cdot (2k-1) $ . I suppose that this equation has no solutions but could anyone please help me solve it or suggest how to approach it?","['analytic-number-theory', 'number-theory', 'factorial', 'diophantine-equations']"
3677082,"How to compute $\lim\limits_{n\rightarrow \infty} \{(2+\sqrt{3})^{n}\}$, where $\{x\}$ is the fractional part of $x$?","I've stumbled upon the following problem: $$\lim_{n\rightarrow \infty} \{(2+\sqrt{3})^{n}\}$$ where ""{}"" notates the fractional part. I've never studied this kind of problem, there exists any reference that I could read about
this type of problem?","['limits', 'fractional-part']"
3677207,"Functions of the form $\int_{a} ^{x} f(t) \, dt$ with regard to Riemann and Lebesgue integral","Consider the class $\mathcal{R}[a, b] $ of functions $F:[a, b] \to\mathbb {R} $ which can be expressed as $$F(x) =\int_{a} ^{x} f(t) \, dt$$ for some Riemann integrable function $f$ and consider similar class $\mathcal{L}[a, b] $ of functions where the integral is Lebesgue instead of Riemann. Since Riemann integrable functions are also Lebesgue integrable $\mathcal{R} [a, b] \subseteq \mathcal{L} [a, b] $ . I suspect these sets are not equal. Further it is known that the functions in $\mathcal{L}[a, b] $ are characterized by the following properties: They are continuous on $[a, b] $ They are of bounded variation on $[a, b] $ They satisfy Luzin N property on $[a, b] $ ie if $A\subseteq[a, b] $ is a set of measure zero then $F(A) $ is also a set of measure zero. Clearly these properties are also possessed by the functions $F\in\mathcal {R} [a, b] $ but since this is supposed to be a smaller set compared to $\mathcal{L} [a, b] $ its members must have some other unique properties not possessed by members of $\mathcal{L} [a, b] $ . How can we characterize the members of $\mathcal{R} [a, b] $ ? Any specific examples will help to illustrate the properties involved. The motivation for this question comes from the fact the Lebesgue integrable functions can be much weirder (eg discontinuous everywhere) than Riemann integrable functions (necessarily continuous almost everywhere) and yet their integrals are far more well behaved (continuous and of bounded variation). A weird function like Dirichlet characteristic function of rationals upon integrating gives the constant function $0$ . I think that finding functions which lie in $\mathcal{L} $ and not in $\mathcal{R} $ is not trivial. I also tried to apply the Fundamental Theorem of Calculus for Riemann integrals and figured that the functions in $\mathcal{R} $ are differentiable almost everywhere, but due to bounded variation the same holds for functions in $\mathcal{L} $ also. The difference between these two classes is a bit deep and not easy to figure out at least for me.","['riemann-integration', 'lebesgue-integral', 'real-analysis']"
3677220,Convert Complex to Trigonometric.,"How do we convert $me^{ik}+ne^{-ik}$ to a purely trigonometric expression? I am guessing it has something to do with $e^{it}=\cos(t)+i\sin(t)$ but I can't figure out exactly how. Edit:- I should have been clearer. In particular, I was looking for how to convert the above to $A\cos(k+x)$ where $x$ and $A$ are constants. Edit 2:- Here is what I have achieved so far. $me^{ik}+ne^{-ik}\\$ $=m(\cos(k)+i\sin(k))+n(\cos(-k)+i\sin(-k))\\$ $=m(\cos(k)+i\sin(k))+n(\cos(k)-i\sin(k))\\$ $=\cos(k)(m+n)+i\sin(k)(m-n)\\$ Now what?","['algebra-precalculus', 'trigonometry', 'complex-numbers']"
3677237,Is matrix notation with dots acceptable for papers? What are some alternatives?,"I am very used to this kind notation when dealing with matrices with arbitrary dimensions: $$
X_P^T\Lambda X_P=
\begin{bmatrix}
	x_1 & x_2, &\dots, &x_n\\
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & \dots & 0\\
0 & \lambda_2 & \dots & 0\\
\vdots & \vdots &\ddots \\
0 & 0 & \dots & \lambda_n\\
\end{bmatrix}
\begin{bmatrix}
	x_1 \\ 
	x_2 \\ 
	\vdots\\
	x_n\\
\end{bmatrix}
$$ I like it because it's nice and directly conveys the matrix representation. However, someone mentioned to me that these representations are good for notes and teaching materials but not for publications. In this specific example the matrix is a diagonal matrix, but I am asking more in general, if you have to convey an arbitrary matrix with any dimensions and any entries, how do you convey the pattern in the matrix without doing dot notation?","['matrices', 'notation', 'linear-algebra']"
3677279,Finding $\lim_{n\to\infty} \left(\frac{\sqrt{n^2+n}-1}{n}\right)^{2\sqrt{n^2+n}-1}$,"Find the following limit without using the L'Hopital rule: $$\lim_{n\to\infty} \left(\dfrac{\sqrt{n^2+n}-1}{n}\right)^{2\sqrt{n^2+n}-1}$$ Answer: $e^{-1}$ My attempt: Since the limit is of the form $1^{\infty}$ , I decided to use the standard formula: $$\lim_{x\to a} f^g = e^{\lim_\limits{x\to a}(f-1)g}$$ (See link ) Let $l=(f-1)g$ . We have, $$l=\left(\dfrac{\sqrt{n^2+n}-(1+n)}{n}\right)(2\sqrt{n^2+n}-1)$$ This on solving boils down to $$l=2n+3-\sqrt{1+\frac 1n}(2n+3)+\frac 1n$$ Now if I tend $n$ towards infinity, then $l\to 0$ and the limit i.e. $e^l$ , is equal to $1$ , which contradicts the given answer. Please help. Thanks! Edit: 
A proof for the ""standard formula"" I have used. Edit 2: just noticed a typo in the power which I have now fixed.","['limits', 'calculus', 'limits-without-lhopital']"
3677292,"Evaluate $\sum_{n,k} \binom{n}{k}^{-1} $","Evaluate $$\sum_{n,k}  \frac{1}{\binom{n}{k}}, $$ where the summation ranges over all positive integers $n,k$ with $1<k<n-1$ . Thouhgts: We are trying to evaluate $$\sum_{n=4}^{\infty} \sum_{k=2}^{n-2} \binom{n}{k}^{-1}$$ We may try to find a closed form of the inner summation which is of the form : $$ \frac{1}{ {n \choose 2} } +  \frac{1}{{n \choose 3} }+ \dotsb + \frac{1}{{n \choose n-2} }.   $$ Notice that we may write $\frac{1}{ {n \choose 2} } = \frac{2! }{n(n-1)}$ and if keep doing the same for the other terms we obtain the following: $$ \frac{ (n-2)! + (n-3)! (n-(n-2)) + (n-4)!(n-(n-2))(n-(n-3)) + \dotsb + 2! (n-3)! }{n!}, $$ which equals $$ \frac{ (n-2)! + 2!(n-3)! + 3! (n-4)! + \dotsb + (n-3)! 2! }{n!} $$ and so this equals: $$ \frac{1}{n(n-1)} + \frac{2}{n(n-1)(n-2)} + \dfrac{6}{n(n-1)(n-2)} + \dotsb + \dfrac{2}{n(n-1)(n-2) }. $$ But half of this terms are identical. Therefore, we are trying to sum up series of the form $$\sum_{n \geq k} \frac{1}{(n-1)(n-2)(n-3)\dotso(n-k)} ,$$ which can be done by a telescoping trick, but it seems very formidable. Am I approaching this problem the right way? Any hints/suggestions?","['calculus', 'sequences-and-series', 'summation', 'real-analysis']"
3677305,Are Christoffel symbols associated with a tensor object?,"First of all, my question lies on: Differentiable, real, n-dimensional Manifolds and in the context of differential geometry for General Relativity. Also, my level of academic mathematical language do not cover fibre bundles or more complex structures than the intuitive notion of tangent and cotangent bundles. So, the author in $[1]$ said explicitly the following stroked in red: Then, the whole well-know fact that Christoffel symbols aren't tensors has sinked into a whirlpool of confusion. This whirlpool of confusion is due to the classical tensor analysis realization; which we check that the christoffel symbols in fact do not transform like a tensor object. Furthermore other authors have pointed out this fact of a true tensor nature, of the Christoffel symbols, before: $[2]$ , $[3]$ . The thing is, if they form a components of a $(1,1)-tensor$ $[1]$ , then there must to be such abstract object which is the ""pure"" tensor: $$\Gamma = \Gamma^{a}_{b} dx^{b} \otimes \frac{\partial}{\partial x^{a}} \tag{1}$$ Well, the fact is, concerning the realization of $[1]$ when he said: one for each basis vector $\vec{e}_{\nu}$ this motivated me to write a notation for this phrase: $$ \Gamma^{a}_{(\cdot)b} \tag{2}$$ Where the dot means that, when we put a basis vector $\frac{\partial}{\partial x^{j}}$ we get on return $[2]$ : $$ \Gamma^{a}_{(\cdot)b}\Big(\frac{\partial}{\partial x^{j}}\Big)\equiv \Gamma^{a}_{jb} =: dx^{a}\Big( \nabla_{\frac{\partial}{\partial x^{b}}}\frac{\partial}{\partial x^{j}} \Big) \tag{3}$$ On the other hand, a mathematical fact is that, a Christoffel Symbol can be calculated precisely as $(3)$ .
So, if we omit the basis vector $j$ , then we have, in fact, an operator: $$\Gamma^{a}_{(\cdot)b}: \mathfrak{X}(M) \to C^{\infty}(M)  $$ $$ \Gamma^{a}_{(\cdot)b}=: dx^{a}\Big( \nabla_{\frac{\partial}{\partial x^{b}}} (\cdot)\Big) \tag{4}$$ Or, more gently: $$\Gamma^{a}_{(\cdot)b}: T_{p}M \to \mathbb{R}  $$ . Now, for the tensor transformation law I tried to prove and I get, nicely, a result which is the true tensor law: Consider then the symbols in a coordinate chart $C'$ . $$ \Gamma'^{a'}_{(\cdot)b'} = \Gamma'^{a'}_{b'} \tag{5}$$ Where in $(5)$ is just a change of notation. Then we perfom a change of coordinates to another chart $C \to C'$ Then the Symbols transforms as: $$\Gamma'^{a'}_{b'} = dx'^{a'}\Big( \nabla_{\frac{\partial}{\partial x'^{b'}}} (\cdot)\Big)  = \frac{\partial x'^{a'}}{\partial x^{c}}dx^{c}\Bigg( \nabla_{\frac{\partial x^{d}}{\partial x^{b'}}\frac{\partial}{\partial x^{d}}} (\cdot)\Bigg) = $$ $$ = \frac{\partial x'^{a'}}{\partial x^{c}}\frac{\partial x^{d}}{\partial x^{b'}}dx^{c}\Bigg( \nabla_{\frac{\partial}{\partial x^{d}}} (\cdot)\Bigg) =\frac{\partial x'^{a'}}{\partial x^{c}}\frac{\partial x^{d}}{\partial x^{b'}} \Gamma^{c}_{d}  \tag{6} $$ So indeed the symbols transforms like a tensor, then with an abuse of notation, we can say that ""Christoffel symbols"" transforms like a tensor. The subtle fact is: for every basis vector we have an Christoffel Symbols; therefore the whole symbol $(3)$ do not transform indeed. But since we have the $\Gamma^{a}_{b}$ , and it's tensor nature, we can therefore say that they form components of the Christoffel Tensor . Then, we can indeed conclude it's abstract form $(1)$ . My question is: Is the tensor law expressed in $(6)$ totally correct? Or, in other words, the operator realization given by $(4)$ makes sense? $$ * * * $$ $[1]$ BERTSCHINGER.B. Introduction to Tensor Calculus for General
Relativity . link: http://web.mit.edu/edbert/GR/gr1.pdf pages 20-21. $[2]$ CHRUSCIEL.P.T. Elements of General Relativity . Birkhauser. pages 16-19. $[3]$ WALD.R. General Relativity . pages 29-33","['connections', 'tensors', 'smooth-manifolds', 'general-relativity', 'differential-geometry']"
3677353,Terminology: Product of numerator and denominator of rational number,"I cannot find the correct terminology for the following: Let $r=\frac{a}{b}$ with $a$ , $b \in \mathbb{N}$ and coprime. I define $q=a\cdot b$ . How is $q$ called? I know that one can order rational numbers using the product of their nominator and denominator, but how is the product called? Order of the rational number? I was not able to locate a creditable source. Thanks Edit: I would like to say a sentence such as ""q is the $x$ of the rational number r."" - what is $x$ ?","['fractions', 'algebra-precalculus', 'rational-numbers', 'terminology']"
