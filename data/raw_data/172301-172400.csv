question_id,title,body,tags
3060153,The relationship between vector space dualization and matrix transposition.,"Let $\mathbb{F}$ be a field. The category of matrices $\mathbf{Mat}$ has $\mathbb{N}_0$ as class of objects and $\mathrm{hom}(n,m)=\mathbb{F}^{n\times m}$ for $n,m\in\mathbb{N}_0$ , with $\mathrm{id}_n=\mathbf{1}_n$ and $B\circ A=A\cdot B$ with the usual matrix multiplication whenever defined. The category of finite-dimensional vector spaces $\mathbf{FinVect}$ has $\mathbb{F}$ -vector spaces as objects and linear maps as morphisms, where composition is the usual composition of functions and the identity morphism is the identity map. The categories $\mathbf{Mat}$ and $\mathbf{FinVect}$ are equivalent. There is a contravariant duality functor $(-)^\ast\colon\mathbf{FinVect}\rightarrow\mathbf{FinVect}$ with $V^\ast=\mathrm{Hom}(V,\mathbb{F})$ for any vector space $V$ and $f^\ast\colon W^\ast\rightarrow V^\ast,\,g\mapsto g\circ f$ for any linear map $f\colon V\rightarrow W$ . There also is a contravariant transpose functor $(-)^t\colon\mathbf{Mat}\rightarrow\mathbf{Mat}$ where $n^t=n$ for $n\in\mathbb{N}_0$ and $A^t$ is the usual transpose of a matrix. These two contravariant functors on equivalent categories are related by the fact that for finite-dimensional vector spaces $V,W$ with bases $\mathscr{B},\mathscr{C}$ respectively, we have that $$(T^{\mathscr{B}}_{\mathscr{C}}(f))^t=T^{\mathscr{C}^\ast}_{\mathscr{B}^\ast}(f^\ast),$$ where $T$ denotes the transformation matrix for the respective bases. Is there a way to describe this relationship of functors in category-theoretical terms?","['matrices', 'category-theory', 'dual-spaces']"
3060165,What is the probability of having 10 or more crashes in the first 1000 rides?,"You have recently invested in an electric skateboard. You always wear a helmet and padding to protect yourself (of course), but are worried that at some point you'll take a tumble and get scratched up. After assessing your skating ability, you estimate your probability of a crash on any single ride to be $0.005$ . I got this question while studying geometric and binomial distribution. My current idea is $1 - (P(x=0) + P(x=1) + P(x=2) + \ldots + P(x=9))$ where $x$ is the number of crashes, and get each probability by binomial distribution's formula. I came up with this equation as I thought having $10$ or more crashes means same as not having $9$ or less crashes.
Is my approach correct? Also, if it's correct, is there any way to simplify the calculation?","['statistics', 'binomial-distribution', 'probability']"
3060168,"Injectivity of $f(x) = x + [x/2]$, and finding an explicit inverse","Context: This question comes up as a tangent to an earlier MSE question from today . The OP of this question was, in effect, seeking an explicit inverse to the function $$f(x) = x + \left[ \frac{x}{2} \right]$$ where $[n]$ denotes the rounding function, i.e. outputs $n$ rounded to the nearest integer. It turned out the OP was mistaken and meant to ask about a different function, but this brought about an interesting thought as I played around with it. The Problem / My Questions: Let $f$ be as defined previously. On an intuitive glance at the graph of this function, we can see it is injective: However, a rigorous proof of this (i.e. $f(x_1) = f(x_2) \implies x_1 = x_2$ ) eludes me, namely because of that rounding function. The function $g(x) = x$ is injective, yet $h(x) = [x/2]$ isn't (consider $x_1 = 2$ and $x_2 = 2.2$ ). So obviously it is possible to have an injective function which is the sum of two functions which are not necessarily injective. I've tried rigorously proving the injectivity of $f$ , making use of various identities from Wikipedia - you can express $[n]$ as $\lfloor n + 0.5 \rfloor$ , so since this page had lots of identities (as opposed to their rounding function page) I figured it'd be useful. No real headway, though. Which brings up my questions: How would one prove the injectivity of $f$ ? So far in my coursework I've typically only done so from the definition. Perhaps there's a different trick to use in this case - a property of injective functions perhaps, or perhaps a property of the rounding, floor, or ceiling functions? How would one find the inverse of $f$ ? Clearly, to each $x$ , there is a unique $f(x)$ , so, in theory, it seems like you'd be able to invert the function, to be able to express $x$ in terms of $f(x)$ . This obviously isn't true for the related three functions $$a(x) = \lfloor x \rfloor = \text{floor}(x) \;\;\;\;\;\; b(x) = \lceil x \rceil = \text{ceiling}(x) \;\;\;\;\;\; c(x) = [x] = \text{round}(x)$$ but it seems like introducing the lone $x$ term (as in $f$ ) that would normally change the dynamic. Is it possible that there are indeed just injective (or, what would be even more strange from my experience, bijective) functions for which we cannot find an explicit inverse?","['ceiling-and-floor-functions', 'functions', 'inverse-function']"
3060176,"Are spaces shaped like the digits 0, 8 and 9 homeomorphic topological spaces?","Consider the topological spaces shaped like the numerals ""0"", ""8"" and ""9"" in $\mathbb{R}^{2}$ . Are they homeomorphic? I have an approach that doesnt look very rigorous to me. I wanted to know how to formalize this if its correct. 0 and 8 are not homeomorphic since excluding one point of 0 the space is still connected, but excluding the ""tangent point"" of 8, we have a disconnected space. Same idea for 8 and 9. The space 9 is union of one circle and one arc. The arc is homeomorphic to the circle, so we can view 9 as a union of two circles, then 8 and 9 are homeomorphic PS: the topology of the spaces is induced by topology of $\mathbb{R}^{2}$ .","['general-topology', 'metric-spaces']"
3060179,Galois group of $x^3-x^2-4$,"In determining the Galois group of the polynomial $p(x) = x^3-x^2-4,$ I concluded that is must be the Klein- $4$ group as follows. First, $p(x) = (x-2)(x^2+x+2)$ and the roots of the irreducible quadratic $x^2+x+2$ are: $$x_{1,2} = \dfrac{-1+\sqrt{-7}}{2}.$$ Therefore, the splitting field of $p(x)$ is $\mathbb{Q}(\sqrt{7}, i).$ Since this is a biquadratic extension and none of $i, \sqrt{7}$ and $\sqrt{7}i$ are squares, the Galois group is then Klein- $4$ group. However, I found two different answers that disagree with mine. First is from the Dummit and Foote. Specifically, on page 612 it states that: If the cubic polynomial is reducible and it splits to a linear factor and an irreducible quadratic, it's Galois group is group of order $2.$ The second source is here , where it proceeds to conclude that the polynomial is irreducible and also its Galois group is $S_3,$ on page $5.$ What is the correct answer here?","['field-theory', 'galois-theory', 'abstract-algebra']"
3060190,Solving $e^{\frac{x^2}{4vt}} = 1+\frac{x^2}{2vt}$ for $x$,"I need help with solving this difficult fluid dynamic expression. I have tried using rules of logs, symbolab algebra calculator and Wolfram Alpha calculator, and I have got no solution. How would you solve the following expression for $x$ ? $$e^{\frac{x^2}{4vt}} = 1+\frac{x^2}{2vt}$$ When solving this numerically, the solution is: $x=2.2418\sqrt{vt}$ I want to know how you could solve the first expression to get the solution. So could someone please provide a step-by-step solution please?","['algebra-precalculus', 'exponential-function']"
3060193,Solving $\int_0^{\frac{\pi}{2}}\frac{1}{\sin^{2n}(x) + \cos^{2n}(x)}\:dx$,"Spurred on this question I decided to investigate the following integral: \begin{equation}
 I_n = \int_0^{\frac{\pi}{2}}\frac{1}{\sin^{2n}(x) + \cos^{2n}(x)}\:dx
\end{equation} Where $n \in \mathbb{N}$ . The approach I've taken is rather simple and whilst I've arrived at a closed form solution, I'm wondering whether the resultant sum can be expressed in terms of (non)-elementary functions. Would love to see other methods that can be used to solve this! (Using any methods). First: \begin{equation}
 I_n = \int_0^{\frac{\pi}{2}}\frac{1}{\sin^{2n}(x) + \cos^{2n}(x)}\:dx = \int_0^{\frac{\pi}{2}}\frac{1}{\left[\sin^{2}(x)\right]^n + \left[\cos^{2}(x)\right]^n}\:dx
\end{equation} Using the Double-Angle Formulas : \begin{equation}
  \sin^2(x) = \frac{1 - \cos\left(2x\right)}{2} \qquad \cos^2(x) = \frac{1 + \cos\left(2x\right)}{2}
\end{equation} Thus: \begin{align}
 I_n &= \int_0^{\frac{\pi}{2}}\frac{1}{\left[\sin^{2}(x)\right]^n + \left[\cos^{2}(x)\right]^n}\:dx = \int_0^{\frac{\pi}{2}}\frac{1}{\left[\frac{1 - \cos\left(2x\right)}{2}\right]^n + \left[\frac{1 + \cos\left(2x\right)}{2}\right]^n}\:dx \\
&= 2^n \int_0^{\frac{\pi}{2}} \frac{1}{\left[1 - \cos\left(2x\right)\right]^n + \left[1 + \cos\left(2x\right)\right]^n}\:dx
\end{align} Making the substitution $u = 2x$ \begin{equation}
I_n = 2^n \int_0^{\frac{\pi}{2}} \frac{1}{\left[1 - \cos\left(2x\right)\right]^n + \left[1 + \cos\left(2x\right)\right]^n}\:dx =  2^{n - 1} \int_0^{\pi} \frac{1}{\left[1 - \cos\left(u\right)\right]^n + \left[1 + \cos\left(u\right)\right]^n}\:du
\end{equation} Now apply the Weierstrass (/Tangent half angle) substitution $t = \tan\left(\frac{u}{2}\right)$ : \begin{align}
I_n &=  2^{n - 1} \int_0^{\pi} \frac{1}{\left[1 - \cos\left(u\right)\right]^n + \left[1 + \cos\left(u\right)\right]^n}\:du =  2^{n - 1} \int_0^{\infty} \frac{1}{\left[1 - \frac{1 - t^2}{1 + t^2}\right]^n + \left[1 + \frac{1 - t^2}{1 + t^2}\right]^n} \frac{2\:dt}{t^2 + 1} \\
&=   \int_0^{\infty} \frac{\left[1 + t^2 \right]^{n - 1}}{t^{2n} + 1}\:dt
 \end{align} By the Binomial Theorem : \begin{equation}
 \left[1 + t^2 \right]^{2n - 1} = \sum_{j = 0}^{n - 1} {n - 1 \choose j}t^{2j}
\end{equation} Thus: \begin{align}
I_n &=  \int_0^{\infty} \frac{\left[1 + t^2 \right]^{2n - 1}}{t^{2n} + 1}\:dt = \sum_{j = 0}^{n - 1} {n - 1 \choose j} \int_0^{\infty} \frac{t^{2j}}{t^{2n} + 1}
\end{align} Using the solution I found here we find: \begin{align}
I_n &=   \sum_{j = 0}^{n - 1} {n - 1 \choose j} \int_0^{\infty} \frac{t^{2j}}{t^{2n} + 1} = \sum_{j = 0}^{n - 1} {n - 1 \choose j} \cdot \frac{1}{2n} \cdot 1^{\frac{2j + 1}{2n} - 1}B\left(1 - \frac{2j + 1}{2n} , \frac{2j + 1}{2n}\right) \\
&= \frac{1}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j} B\left(1 - \frac{2j + 1}{2n} , \frac{2j + 1}{2n}\right)
\end{align} Using the relationship between the Beta and Gamma function : \begin{align}
I_n &= \frac{1}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j} B\left(1 - \frac{2j + 1}{2n} , \frac{2j + 1}{2n}\right) = \frac{1}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j} \frac{\Gamma\left(1 - \frac{2j + 1}{2n}\right)\Gamma\left(\frac{2j + 1}{2n}\right)}{\Gamma\left(1 - \frac{2j + 1}{2n} + \frac{2j + 1}{2n}\right)} \\
&= \frac{1}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j}\Gamma\left(1 - \frac{2j + 1}{2n}\right)\Gamma\left(\frac{2j + 1}{2n}\right)
\end{align} As $\frac{2j + 1}{2n} \not \in \mathbb{Z}$ we can employ Euler's Reflection Formula to yield: \begin{align}
I_n &= \frac{1}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j}\Gamma\left(1 - \frac{2j + 1}{2n}\right)\Gamma\left(\frac{2j + 1}{2n}\right) = \frac{1}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j} \frac{\pi}{\sin\left(\pi \cdot \frac{2j + 1}{2n}\right)} \\&= \frac{\pi}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j} \operatorname{cosec}\left(\frac{\pi}{2n}\left(2j + 1\right) \right)
\end{align} Thus, \begin{equation}
\int_0^{\frac{\pi}{2}}\frac{1}{\sin^{2n}(x) + \cos^{2n}(x)}\:dx = \frac{\pi}{2n}\sum_{j = 0}^{n - 1} {n - 1 \choose j} \operatorname{cosec}\left(\frac{\pi}{2n}\left(2j + 1\right) \right)
\end{equation} Edit - Realised that $2n - 1$ should be $n - 1$ Extra Addition: Using the same method as above an by letting \begin{equation}
 S_n(x) = \sin^{2n}(x) + \cos^{2n}(x)
\end{equation} It becomes rather easy to solve: \begin{equation}
 I_{n,m} = \int_0^{\frac{\pi}{2}} \frac{S_n(x)}{S_m(x)}\:dx
\end{equation} Where $n \lt m$ . After applying the double angle formulas, $u$ -substitution, and half tangent formula we arrive at: \begin{equation}
 I_{n,m} = \int_0^{\infty} \frac{S_n(x)}{S_m(x)}\:dx = \int_0^{\infty} \frac{t^{2n} + 1}{t^{2m} + 1} \left[1 + t^2 \right]^{m - n - 1}\:dt
\end{equation} Applying the Binomial Expansion: \begin{align}
 I_{n,m} &=  \sum_{j = 0}^{n - 1} {m - n - 1 \choose j} \int_0^{\infty} \frac{t^{2n} + 1}{t^{2m} + 1} \cdot t^{2j}\:dt \\
&= \sum_{j = 0}^{n - 1} {m - n - 1 \choose j} \int_0^{\infty} \frac{t^{2\left(n + j\right)} }{t^{2m} + 1} \:dt + \sum_{j = 0}^{n - 1} {m - n - 1 \choose j} \int_0^{\infty} \frac{t^{2j} }{t^{2m} + 1} \:dt\\
\end{align} Again applying another of my solutions (referenced above) we arrive at: \begin{align}
 I_{n,m} &= \sum_{j = 0}^{n - 1} {m - n - 1 \choose j} \frac{\pi}{2m}\operatorname{cosec}\left(\frac{\pi}{2m}\left(2\left(n + j\right) + 1\right) \right) + \sum_{j = 0}^{n - 1} {m - n - 1 \choose j}\frac{\pi}{2m}\operatorname{cosec}\left(\frac{\pi}{2m}\left(2j + 1\right) \right) \\
&= \frac{\pi}{2m}\sum_{j = 0}^{n - 1} {m - n - 1 \choose j}\left[\operatorname{cosec}\left(\frac{\pi}{2m}\left(2\left(n + j\right) + 1\right) \right) + \operatorname{cosec}\left(\frac{\pi}{2m}\left(2j + 1\right) \right) \right]
\end{align} Note: Using the same method we can easily solve \begin{equation}
 I_{n,p} = \int_0^{\frac{\pi}{2}}\frac{1}{\left(\sin^{2n}(x) + \cos^{2n}(x) \right)^p}\:dx = \frac{1}{2n\Gamma(p)}\sum_{m = 0}^{np - 1} {np - 1 \choose m}\Gamma\left(p - \frac{2m + 1}{2n}\right)\Gamma\left(\frac{2m + 1}{2n}\right)
\end{equation} Where $p \in \mathbb{N}$","['integration', 'definite-integrals', 'gamma-function', 'trigonometric-integrals', 'beta-function']"
3060194,Covering of Intervals,"For a real interval $I=[x-r,x+r]$ and a number $\alpha>0$ we write $\alpha I:=[x-\alpha r,x+\alpha r]$ , i.e. $\alpha I$ is the interval $I$ blown up by a factor $\alpha$ around its center. For example, the Vitali Covering Lemma says that for any finite collection of intervals $([a_j,b_j])_{1\leq j\leq n}$ there is an index set $J\subseteq\{1,\ldots,n\}$ such that $([a_j,b_j])_{j\in J}$ is a subcollection of pairwise disjoint intervals with $$
\bigcup_{1\leq j\leq n}[a_j,b_j]\subseteq\bigcup_{j\in J}3[a_j,b_j]
$$ Setting: I have two finite collections of intervals $([a_j,b_j])_{1\leq j\leq n}$ and $([c_j,d_j])_{1\leq j\leq n}$ with the following three properties. $|a_j-c_j|\leq\delta$ and $|b_j-d_j|\leq \delta$ for $1\leq j\leq n$ , $\sum_{j=1}^n|(a_j-c_j)-(b_j-d_j)|\leq\delta$ , $\left|\bigcup_{j=1}^n[a_j,b_j]\right|\leq\varepsilon$ , where $|A|$ means the Lebesgue measure of a set $A$ . Here, $\delta$ and $\varepsilon$ are two positive ""small"" numbers. Problem: I would like to show (or disprove) that the Lebesgue measure of $\bigcup_{j=1}^n[c_j,d_j]$ is, up to a constant that does not depend on the intervals or $n$ , like that of $\bigcup_{j=1}^n[a_j,b_j]$ also small. My attempt: I could prove that this is true if the $[a_j,b_j]$ are mutually disjoint: Indeed, by Vitali's Covering Lemma there is an index set $J\subseteq\{1,\ldots,n\}$ such that $([c_j,d_j])_{j\in J}$ is a subcollection of pairwise disjoint intervals with $$
\bigcup_{1\leq j\leq n}[c_j,d_j]\subseteq\bigcup_{j\in J}3[c_j,d_j].
$$ This implies with the help of 2. and 3. that \begin{align*}
\left|\bigcup_{j=1}^n[c_j,d_j]\right|
&\leq\left|\bigcup_{j\in J}3[c_j,d_j]\right|
 \leq\sum_{j\in J}\Big|3[c_j,d_j]\Big|
  =3\sum_{j\in J}|c_j-d_j| \\
&\leq 3\sum_{j\in J}|(a_j-b_j)-(c_j-d_j)|+3\sum_{j\in J}|a_j-b_j| \\
&\leq 3\delta+3\left|\bigcup_{j=1}^n[a_j,b_j]\right| \\
&\leq 3(\delta+\varepsilon).
\end{align*} However, I have not used 1. In case that the intervals $[a_j,b_j]$ are not pairwise disjoint, I don't know how to proceed. Any ideas are highly appreciated. Thanks in advance! EDIT: (Thanks to mathworker21): Vitali's Covering Lemma is not necessary here, we can use the estimate \begin{align*}
\left|\bigcup_{j=1}^n[c_j,d_j]\right|
&\leq\sum_{j=1}^n|c_j-d_j|
\end{align*} instead.","['elementary-set-theory', 'measure-theory', 'lebesgue-measure']"
3060195,On automorphisms of groups which extend as automorphisms to every larger group,"For a group $G$ , let $\operatorname{Aut}(G)$ denote the group of all automorphisms of $G$ and $\operatorname{Inn}(G)$ denote the subgroup of all autmorphisms which is of the form $f_h(g)=hgh^{-1}, \forall g\in G$ , where $h\in G$ . Now if $G_1$ is a group containing $G$ as a subgroup then every $f_h \in \operatorname{Inn}(G)$ extends to an inner automorphism of $G'$ as $f_h(x)=hxh^{-1},\forall x\in G_1$ , so in other words, for every $f\in \operatorname{Inn} (G)$ and every group $G_1$ containing $G$ as a subgroup, $\exists \bar f\in \operatorname{Inn}(G_1) \subseteq \operatorname{Aut} (G_1)$ such that $\bar f|_G =f$ . Now my question is : Let $f \in \operatorname{Aut} (G)$ be such that for every group $G_1$ containing $G$ as a subgroup, $\exists \bar f\in \operatorname{Aut}(G_1)$ such that $\bar f|_G =f$ . Then is it necessarily true that $f \in \operatorname{Inn}(G)$ ? If this is not true in general, then does some extra condition on $G$ makes it true (like $G$ being finite, or simple)?","['automorphism-group', 'finite-groups', 'simple-groups', 'group-theory', 'solvable-groups']"
3060205,"First mean value theorem of integration, $\xi \in (a,b)$ instead of $[a,b]$?","The first mean value theorem of integration states that If $f\in C[a,b]$ and $g\in \mathcal{R}[a,b]$ , $g$ is nonnegative, then $\exists \xi\in [a,b]$ such that $$\int_a^b (f\cdot g)(x) dx=f(\xi)\int_a^b g(x) dx.$$ Is it possible to replace $\exists \xi\in [a,b]$ by $\exists \xi\in (a,b)$ ? It is difficult to imagine how this could be wrong. Edit: added condition $g$ is nonnegative.","['calculus', 'analysis', 'real-analysis']"
3060265,Solve the differential equation $\frac{dy}{dx}=5+xy+2x+2y$,Solve the differential equation $$\frac{dy}{dx}=5+xy+2x+2y$$ Given $y(0)=0$ My try: The given equation can be written as: $$\frac{dy}{dx}=1+(x+2)(y+2)$$ Letting $X=x+2$ and $Y=y+2$ we get $dy=dY$ and $dx=dX$ So the equation becomes $$\frac{dY}{dX}=1+XY$$ or $$\frac{dY}{dX}-XY=1$$ which is a linear differential equation of first order: The integrating factor is $$I(X)=e^{\frac{-X^2}{2}}$$ The solution is: $$Ye^{\frac{-X^2}{2}}=\int e^{\frac{-X^2}{2}}dx+C$$ How to continue here?,"['algebra-precalculus', 'ordinary-differential-equations', 'gaussian-integral']"
3060282,Tangent Plane to Level Surfaces Equation Derivation,"I was going through an resource I found online http://mathonline.wikidot.com/tangent-planes-to-level-surfaces In this part: Are they implying that $r'(t_0)$ equals the vector $(x-x_0,y-y_0,z-z_0)$ ? If so, how did they get that? They have $r(t)$ equal the vector $(x(t),y(t),z(t))$ , are they saying that the derivative of $r(t)$ is $(x-x(t),y-y(t),z-z(t))$ so that $r'(t_0)$ is $(x-x_0,y-y_0,z-z_0)$ ? I'm just rambling but I don't understand how they were able to transition from $r'(t)$ to $(x-x_0,y-y_0,z-z_0)$ in the picture above, or, just how they got $r'(t)$ in the first place...","['partial-derivative', 'multivariable-calculus', 'calculus', 'vectors']"
3060332,Inequality of ODE,"Suppose $u(t)>0$ satisfies following inequality, how do we solve $u$ ? $$u^{'}(t)\leq a \cdot u^{\frac{n+2}{n}}(t)$$ for $t\geq 0$ . This seems a little different from Gronwall's inequality, how do we find inequality of $u(t)$ , you can add some conditions to $u$ if you want. Thanks for any help.",['ordinary-differential-equations']
3060369,Which vector spaces are algebraic dual spaces?,"Let us say that a vector space $V$ is an algebraic dual space if there exist a vector space $U$ such that $V$ is isomorphic to $U^*$ , the vector space of all linear maps from $U$ to the corresponding field of scalars. It is known that if $V$ is finite-dimensional then $V$ and $V^*$ are isomorphic, hence $V$ is an algebraic dual space. On the other side, it is also known that the dimension of an algebraic dual space cannot be countably infinite, hence not all vector spaces are algebraic dual spaces. Question: Is there any characterization of vector spaces that are algebraic dual spaces? I am mostly interested in the case of vector spaces over reals.","['linear-algebra', 'vector-spaces', 'dual-spaces']"
3060379,Why is the exterior power $\bigwedge^kV$ an irreducible representation of $GL(V)$?,"$\newcommand{\GL}{\operatorname{GL}}$ Let $V$ be a real $n$ -dimensional vector space. For $1<k<n$ we have a natural representation of $\GL(V)$ via the $k$ exterior power: $\rho:\GL(V) \to \GL(\bigwedge^kV)$ , given by $\rho(A)=\bigwedge^k A$ . I am trying to show $\rho$ is an irreducible representation. Let $0\neq W \le \bigwedge^kV$ be a subrepresentation. If we can show $W$ contains a non-zero decomposable element, we are done. Indeed, suppose $W \subsetneq  \bigwedge^kV$ . Then, there exist a decomposable element $\sigma=v_1 \wedge \dots \wedge v_k \neq 0$ , such that $\sigma \notin W$ . We assumed $W$ contains a non-zero decomposable element $\sigma'=u_1 \wedge \dots \wedge u_k \neq 0$ . Define a map $A \in \GL(V)$ by extending $u_i \to v_i$ . Then $$\rho(A) (\sigma')=\bigwedge^k A(u_1 \wedge \dots \wedge u_k)=\sigma \notin W,$$ while $\sigma' \in W$ , con So, the question reduces to the following: Why does every non-zero subrepresentation contain a non-zero decomposable element? I asked an even more naive question here -whether or not every subspace of dimension greater than $1$ contains a non-zero decomposable element?","['representation-theory', 'tensor-decomposition', 'group-theory', 'lie-groups', 'exterior-algebra']"
3060404,Can I conclude that $\lim_{x\to0^+}\frac{x^2}{e^{-\frac{1}{x^2}}\cos(\frac{1}{x^2})^2}$ is infinite or it doesn't exists?,"$$\lim_{x\to0^+}\frac{x^2}{e^{-\frac{1}{x^2}}\cos(\frac{1}{x^2})^2}$$ My intuition is that the denominator goes to 0 faster and everything is non-negative, so the limit is positive infinity. I cant think of elementary proof, l'Hopital doesn't help here, and I'm not sure if and how to use taylor. WolframAlpha says it's complex infinity, but I can't understand why it doesn't just real positive infinity. I've tried to use wolfram language to use the assumption that x is real, but couldn't get any result.","['limits', 'wolfram-alpha', 'infinity', 'real-analysis']"
3060406,Finite Engel group is nilpotent.,"A group $G$ is said to be $n$ engel if $$[x,[x, \dots ,[x,y]]\dots ]=1,$$ where $x$ appears $n$ times, and this holds for all $x,y\in G$ . We know there is infinite order engel group which is not nilpotent. But what can we say about finite order engel groups, are they always nilpotent?","['nilpotent-groups', 'group-theory', 'abstract-algebra', 'finite-groups']"
3060424,How do I find out that the following two matrices are similar?,"How do I find out that the following two matrices are similar? $N =
\begin{pmatrix} 
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 
\end{pmatrix}$ and $M=
\begin{pmatrix} 
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 
\end{pmatrix}$ I initially tried to think of the left multiplication of a matrix $P$ as a row operation and tried $P=
\begin{pmatrix} 
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 
\end{pmatrix}$ such that $PN = \begin{pmatrix} 
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 
\end{pmatrix}$ but then $PNP^{-1} \neq M$ . My linear algebra is a bit rusty. Is there a more elaborate way to do this?","['matrices', 'similar-matrices', 'linear-algebra']"
3060482,Positive operator is symmetric?,"If I understand correctly then for an operator $\mathcal{A}$ defined on a Hilbert space $\mathcal{H}$ , $\langle \mathcal{A}x,x\rangle\geq 0$ does not necessarily imply that $\mathcal{A}$ is Hermitian: $\mathcal{A} = \mathcal{A}^\ast$ . See for instance Is a positive operator symmetric? However I was shown the following proof of the supposedly erroneous statement and was wondering if there is something wrong in it for I can't find it myself? Lemma 1: $\langle Tx,x\rangle = 0$ for every $x\in \mathcal{H}$ implies that $T \equiv 0$ . Proof: We show that $\langle Tx,y\rangle = 0$ for every pair $x,y\in \mathcal{H}.$ Indeed \begin{align*}
0 = \langle T(x+y),x+y\rangle - \langle T(x-y),x-y\rangle & = 2\langle Tx,y\rangle+2\langle Ty,x\rangle.
\end{align*} This implies that $$\langle Tx,y\rangle = -\langle Ty,x\rangle.$$ Exchanging $x$ with $ix$ yields $$0 = i\langle Tx,y\rangle -i\langle Ty,x\rangle$$ why $$\langle Tx,y\rangle = \langle Ty,x\rangle$$ all in all we find that $\langle Tx,y\rangle = \pm \langle Ty,x\rangle$ which implies that both are $0$ . Proof that $\langle \mathcal{A}x,x\rangle\geq 0$ implies that $\mathcal{A}^\ast = \mathcal{A}$ : We have that $$\mathbb{R}\ni\langle \mathcal{A}x,x\rangle = \langle x,\mathcal{A}^\ast x\rangle = \overline{\langle \mathcal{A}^\ast x,x\rangle } = \langle \mathcal{A}^\ast x, x\rangle\Rightarrow \langle (\mathcal{A}-\mathcal{A}^\ast)x,x\rangle = 0$$ for every $x$ thus by the lemma $\mathcal{A}-\mathcal{A}^\ast\equiv 0$ .","['matrices', 'operator-theory', 'functional-analysis']"
3060514,Real Analysis Limits Question,"Currently preparing for a first course in analysis exam and my answer disagrees with the model solutions but I can't see where my logic fails The question: Find the limit of: $$a_n :=\frac{2^{3n}-n3^n}{n^{1729}+8^n}$$ My attempt: First rewrite $8^n$ as $2^{3n}$ and divide through by $2^{3n}$ This gives us $$= \frac{1-n\left(\frac{3}{8}\right)^n}{\large \frac{n^{1729}}{2^{3n}}+1}$$ and then divide through by $n$ to give $$= \frac{\frac{1}{n}-\left(\frac{3}{8}\right)^n}{\large \frac{n^{1728}}{2^{3n}}+\frac{1}{n}}$$ My answer is zero using the following limits: $\lim_{n\to\infty}\frac{1}{n} = 0$ and since $\frac{3}{8} <1$ $\implies\lim_{n\to\infty}\frac{3}{8}^n = 0$ The model solutions however suggest the limit is $1$ , any help would be amazing.","['analysis', 'sequences-and-series']"
3060529,How can we see that the proof of this following real analysis problems is reasonable?,"Problem: Let $f(x)$ be a real valued functions defined on $\mathbb{R}$ , prove that the point set $$E = \{x\in \mathbb{R}: \lim_{y\rightarrow x} f(y) = +\infty\}$$ is a finite or a countable set. Proof: Let $g(x) = \arctan f(x), x \in \mathbb{R}$ . Then the point set $E$ can be written as $$E = \{x\in \mathbb{R}: \lim_{y\rightarrow x} g(y) = \frac{\pi}{2}\}$$ Therefore $E$ is a finite or a countable set. The above proof is from a textbook of real analysis. How can we see that the set $E$ in this proof is finite or countable?","['lebesgue-measure', 'analysis', 'real-analysis']"
3060542,Egg drop problem - minimize AVERAGE case,"In the egg drop problem ,  you have two eggs and you want to determine from which floors in a $n$ - floor building you can drop an egg such that is doesn't break. You are to determine the minimum number of attempts you need in order to find the critical floor in the worst case while using the best strategy. Some assumptions : If the egg doesn't break at a certain floor, it will not break at any
floor below. If the eggs breaks at a certain floor, it will break at any floor
above. The egg may break at the first floor. The egg may not break at the last floor. This problem is popular and there are many ways to solve it. I am wondering how to solve it with the following twist : instead of minimizing the worst case, how would one minimize the average case ? We consider that the egg has equal probability $1/n$ to break on a given floor (and above). An attempt : Let $f(2,n)$ denote the minimum number of drops needed to cover $n$ floors with the $2$ eggs.
For the worst case, the following recursive equation holds $$
f(2,n) = 1 + \min_{1 \le x \le n} \bigl\{ \max\{f(1,x-1),f(2,n-x \} \bigr \}
$$ Roughly speaking, when throwing the first egg from a given floor $x$ , there are two scenarios : If the egg breaks, we are left with $1$ egg and have to check the $x-1$ floors below If the egg does not break, the $n-x$ floors above $x$ have to be checked with the $2$ eggs Since  the number of trials in worst case is minimized, we take the maximum of these two cases for every floor and choose the floor which yields minimum number of drops. The extra $1$ accounts for the drop before knowing if the egg broke or not. The base cases are trivially $f(1,0)=f(2,0)=0$ (no drops needed if there are no floors), $f(1,1)= f(2,1) = 1$ (one drop is sufficient if there is only one floor), and $f(1,x) = x$ ( $x$ drops needed if only one egg is available - each floor must be tested one by one). For the average case, I believe the recursive equation becomes $$
f(2,n) = 1 + \min_{1 \le x \le n} \bigl\{ p(x)f(1,x-1)+(1-p(x))f(2,n-x) \bigr \},
$$ where $p(x)$ is the probability that the egg breaks on floor $x$ , i.e., the probability that $x$ is above the critical floor. If this critical floor is represented by a discrete random variable $Y$ with uniform distribution on $[1,n]$ : $$
p(x) = P(x\ge Y) 
$$ I am not sure if this correct. And if so, how can this expression be simplified? Also, the base cases remain $f(1,0)=f(2,0)=0$ , $f(1,1)=f(2,1)=1$ , but $f(1,x)$ no longer equals $x$ , since we are interested in the average case. Any help is welcome, any other approach is welcome. Thanks.","['optimization', 'recreational-mathematics', 'puzzle', 'probability']"
3060561,Infinitesimal generator : what is it exactly?,"Let $(X_t)$ an diffusion Itô process, i.e. a solution of $$dX_t=b(X_t)dt+\sigma (X_t)dB_t.$$ The infinitesimal generator of $(X_t)$ is $$Af(x)=\lim_{t\to 0^+}\frac{\mathbb E^x[f(X_t)]-f(x)}{t},$$ where $\mathbb E^x$ is the expectation wrt $\mathbb P^x$ . Q1) What represent exactly $Af(x)$ for $X_t$ ? For example, for a Brownian motion, if $f$ is $C^2$ then $$A f(x)=\frac{1}{2}\Delta f(x).$$ But I don't really understand which information does $A$ give is. Is it a sort of derivative of $X_t$ ? Q2) What is exactely the measure $\mathbb P^x$ ? I know it is $\mathbb P^x\{X_t\in A\}=\mathbb P(\{X_t\in A\}\mid \{X_0=x\}),$ But does it mean that on $(\Omega ,\mathcal F,\mathbb P^x)$ we have that $\mathbb P\{X_0=x\}=1$ ? (i.e. is deterministic).","['measure-theory', 'probability']"
3060562,"Ideal of regular functions in $Z(xy-z^2)$ vanishing on $(x,y)$ is not principal","Let $H = Z(xy-z^2) \subset \Bbb A ^ {3}$ . And let $L = Z(x,z)$ . I need to show that $L \subset H$ , that $dim(L) = dim (H) -1$ and that the ideal of regular functions on $H$ which vanishing on $L$ is not a principal ideal. So showing $L\subset H$ is easy, and as $xy-z^2$ is irreducible, we have $dim(L) = dim(H) -1$ by principal ideal theorem. Im not sure how to show the last part. The regular functions on $H$ are $k[x,y,z]/(xy-z^2)$ and the ideal of regular functions vanishing on $L$ is $(x,z)$ so I need to show $k[x,y,z]/(xy-z^2) \cap (x,z)$ is not principal. Any ideas how?","['algebraic-geometry', 'affine-geometry']"
3060572,A curious observation regarding eigenvectors of $3 \times 3$ matrices - Hoffman and Kunze's *Linear Algebra*,"I am reading Hoffman and Kunze's Linear Algebra , 2nd ed., and I made a curious observation in a couple of the examples relating to computing eigenvalues and eigenvectors in Chapter 6. In Example 2 on pages 184-185, we have the (real) $3 \times 3$ matrix $$
A = \begin{bmatrix}
3 & 1 & -1\\
2 & 2 & -1\\
2 & 2 & \phantom{-}0
\end{bmatrix}.
$$ The characteristic polynomial for $A$ is $(x-1)(x-2)^2$ . Thus, the characteristic values of $A$ are $1$ and $2$ . We have $$
\begin{align}
A - I &= 
\begin{bmatrix}
2 & 1 & -1\\
2 & 1 & -1\\
2 & 2 & -1
\end{bmatrix}\\\\
A - 2I &= 
\begin{bmatrix}
1 & 1 & -1\\
2 & 0 & -1\\
2 & 2 & -2
\end{bmatrix}.
\end{align}
$$ The characteristic spaces associated to each characteristic value is one-dimensional in this case. The vector $\alpha_1 = (1,0,2)$ spans the null space of $T - I$ and the vector $\alpha_2 = (1,1,2)$ spans the null space of $T - 2I$ . Here, my observation is that $\alpha_1$ is the middle column vector of $A - 2I$ , and $\alpha_2$ is the middle column vector of $A - I$ . A similar thing happens in Example 3 (pages 187-188): $T$ is the linear operator on $\Bbb{R}^3$ which is represented in the standard ordered basis by the matrix $$
A =
\begin{bmatrix}
\phantom{-}5 & -6 & -6 \\
-1 & \phantom{-}4 & \phantom{-}2 \\
\phantom{-}3 & -6 & -4
\end{bmatrix}.
$$ The characteristic polynomial is computed to be $(x-2)^2(x-1)$ . Then, we have $$
\begin{align}
A - I &=
\begin{bmatrix}
\phantom{-}4 & -6 & -6 \\
-1 & \phantom{-}3 & \phantom{-}2 \\
\phantom{-}3 & -6 & -5
\end{bmatrix}\\\\
A - 2I &=
\begin{bmatrix}
\phantom{-}3 & -6 & -6 \\
-1 & \phantom{-}2 & \phantom{-}2 \\
\phantom{-}3 & -6 & -6
\end{bmatrix}.
\end{align}
$$ The null space of $T-I$ is one-dimensional and the null space of $T-2I$ is two-dimensional. The vector $\alpha_1 = (3,-1,3)$ spans the null space of $T-I$ . The null space of $T-2I$ consists of the vectors $(x_1,x_2,x_3)$ with $x_1 = 2x_2 + 2x_3$ , so the authors give an example of a basis of the null space of $T-2I$ as $$\begin{align}\alpha_2 &= (2,1,0)\\ \alpha_3 &= (2,0,1).\end{align}$$ However, we can also take $$\begin{align}\alpha_2 &= (-6,3,-6)\\ \alpha_3 &= (-6,2,-5)\end{align}$$ and we see again that $\alpha_1$ is the first column of $A - 2I$ and $\alpha_2,\alpha_3$ are the second and third columns of $A - I$ . I find this quite curious, more so since the authors don't mention this observation at all. Is there a simple explanation for why this is happening, and can this observation be used to quickly find eigenvectors of linear transformations?","['matrices', 'linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
3060573,Prove triangle area formula for barycentric coordinates,"Let $P_1, P_2, P_3$ be points with barycentric
coordinates (with reference triangle $ABC$ ) $P_i = (u_i, v_i, w_i )$ for $i = 1, 2, 3$ . Then the signed area of $\Delta P_1P_2P_3$ is given by the determinant $$\frac{[P_1P_2P_3]}{[ABC]}=\begin{vmatrix} u_1& v_1& w_1 \\ u_2& v_2& w_2\\u_3& v_3& w_3 \end{vmatrix}$$ I came across this theorem in Evan Chen's ""Euclidean Geometry in Mathematical Olympiads"" where the proof is skipped. I failed to prove this myself and cannot find the proof online. Any help will be appreciated.","['triangles', 'geometry', 'barycentric-coordinates']"
3060593,Discretization of second order nonlinear ODE using finite difference approximation not correct,"I have the differential equation $$y'' + x(y^2)' - 2y^2 = g(x) \Longleftrightarrow y'' + x2yy'-2y^2 = g(x).$$ Using finite-difference approximations $$y''(x_m) \approx \frac{Y_{m-1} - 2Y_m + Y_{m+1}}{\Delta x^2},$$ $$y'(x_m) \approx \frac{Y_{m+1} - Y_{m-1}}{2\Delta x},$$ $$y(x_m) \approx Y_m,$$ I get $$\frac{Y_{m-1} - 2Y_m + Y_{m+1}}{\Delta x^2} + \frac{x_m}{\Delta x}Y_m(Y_{m+1}-Y_{m-1}) - 2Y_m^2 = g(x_m).$$ However, the answer is supposed to be Why is the second term in my answer wrong?","['ordinary-differential-equations', 'finite-difference-methods', 'nonlinear-system', 'numerical-methods', 'finite-differences']"
3060597,Change of Variable formula for a non-differentiable mapping.,"Let $\Omega \subset \Bbb R^n$ . For a diffeomorphism (or merely a differentiable bijection) $\varphi:\Omega \to \varphi(\Omega)$ , we have the formula $$
\int_{\Omega} f\circ\varphi^{-1}(x)\, dx = \int_{\varphi^{-1}(\Omega)} f(y)|D\varphi(y)| \,dy.
$$ How much can we generalize the class in which $\varphi$ is allowed to lie in? Is it enough that we have, says, a bijection $\varphi\in W_{\text{loc}}^{1,\infty}(\Omega ;\Bbb R^n)$ or even $\varphi\in W_{\text{loc}}^{1,1}(\Omega ;\Bbb R^n)$ ? How much does the result depends on the domain $\Omega$ ? Is there a big difference between a compact and an open domain? Does the regularity of the boundary $\partial \Omega$ play any role? I'd also appreciate if you have a good reference to this kind of result so that I can read further into this interesting issue. Happy New Year to all of you.","['measure-theory', 'reference-request', 'real-analysis', 'partial-differential-equations', 'differential-geometry']"
3060606,Powers of bidiagonal Toeplitz matrix,"Consider the following bidiagonal $n \times n$ Toeplitz matrix $A$ $$A = \begin{bmatrix}
  1-p & 0 & 0 & \cdots & 0\\
  p & 1-p & 0 && \vdots \\
  0 & \ddots & \ddots & \ddots & 0 \\
  \vdots && p & 1-p & 0\\
  0 & \cdots & 0 & p & 1-p
 \end{bmatrix}$$ where $0 < p < 1$ . What is $A^m$ for any $m \ge 2$ ? It's easy to show what the matrix is when $n = 2$ for all $m$ , but not for general $n$ . I have seen several papers on powers of tridiagonal Toeplitz matrices but they assume that the off-by- $1$ diagonals are all nonzero, but the ""upper"" diagonal here is all $0$ .","['matrices', 'toeplitz-matrices']"
3060627,Trigonometric inequality $ 3\cos ^2x \sin x -\sin^2x <{1\over 2}$,"I' m trying to solve this one. Find all $x$ for which following is valid: $$ 3\cos ^2x \sin x -\sin^2x <{1\over 2}$$ And with no succes. Of course if we write $s=\sin x$ then $\cos^2 x = 1-s^2$ and we get $$6s^3+2s^2-6s+1>0$$ But this one has no rational roots so here stops. I suspect that Cardano wasn't in a mind of a problem proposer. There must be some trigonometric trick I don't see. I also tried with $$\sin 3x = -4s^3+3s$$ but don't now what to do with this. Any idea? Offical solution is a union of $({(12k-7)\pi \over 18},{(12k+1)\pi\over 18})$ where $k\in \mathbb{Z}$","['algebra-precalculus', 'trigonometry']"
3060657,function continuous ae but not borel measurable,"It is easy to prove that if $f$ is a function continuous almost everywhere, then $f$ is Lebesgue-measurable by using the property that $\mathcal L$ (the Lebesgue-measure) is complete. Though I've been wondering if the statement ""every function continuous ae is Borel-measurable"" is true. I feel like it's not but I have a hard time finding a counterexample. So does such a function (continuous ae and not Borel-measurable) exist?","['measure-theory', 'lebesgue-measure', 'borel-measures']"
3060671,Several ways to prove that $\sum\limits^\infty_{n=1}\left(1-\frac1{\sqrt{n}}\right)^n$ converges,"I believe there are several ways to prove that $\sum\limits^{\infty}_{n=1}\left(1-\frac{1}{\sqrt{n}}\right)^n$ converges. Can you, please, post yours so that we can learn from you? HERE IS ONE Let $n\in\Bbb{N}$ be fixed such that $a_n=\left(1-\frac{1}{\sqrt{n}}\right)^n.$ Then, \begin{align} a_n&=\left(1-\frac{1}{\sqrt{n}}\right)^n \\&=\exp\ln\left(1-\frac{1}{\sqrt{n}}\right)^n\\&=\exp \left[n\ln\left(1-\frac{1}{\sqrt{n}}\right)\right] \\&=\exp\left[ -n\sum^{\infty}_{k=1}\frac{1}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right]\\&=\exp\left[ -n\left(\frac{1}{\sqrt{n}}+\frac{1}{2n}+\sum^{\infty}_{k=3}\frac{1}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right)\right]\\&=\exp \left[-\sqrt{n}-\frac{1}{2}-\sum^{\infty}_{k=3}\frac{n}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right]\\&\equiv\exp \left(-\sqrt{n}\right)\exp \left(-\frac{1}{2}\right)\end{align} Choose $b_n=\exp \left(-\sqrt{n}\right)$ , so that \begin{align} \dfrac{a_n}{b_n}\to\exp \left(-\frac{1}{2}\right).\end{align} Since $b_n \to 0$ , there exists $N$ such that for all $n\geq N,$ \begin{align} \exp \left(-\sqrt{n}\right)<\dfrac{1}{n^2}.\end{align} Hence, \begin{align}\sum^{\infty}_{n=N}b_n= \sum^{\infty}_{n=N}\exp \left(-\sqrt{n}\right)\leq \sum^{\infty}_{n=N}\dfrac{1}{n^2}<\infty,\end{align} and so, $\sum^{\infty}_{n=1}b_n<\infty\implies \sum^{\infty}_{n=1}a_n<\infty$ by Limit comparison test.","['sequences-and-series', 'real-analysis']"
3060673,Limit of $\frac{1}{7}e^{-2x^2}(1-4x^2)$ as $x\to\infty$,I calculated the derivative of $\frac{x}{7}*e^{-2x^2}$ and got $\frac{1}{7}e^{-2x^2}(1-4x^2)$ (I included it cause if I got that wrong calculating the rest is pointless) I don't know how to find the limit of this function: $$\frac{1}{7}e^{-2x^2}(1-4x^2)$$ I tried splitting it into two but I still don't know how to handle this $$\lim_{x \to \infty} \frac{1}{7}e^{-2x^2}+\lim_{x \to \infty} \frac{1}{7}xe^{-2x^2}*(-4x) $$,"['limits', 'exponential-function']"
3060690,On calculating the limit of the infinite product $\prod_{k=3}^n (1-\tan^4\frac{\pi}{2^k})$,"Let $S_n=\prod_{k=3}^n (1-\tan^4\frac{\pi}{2^k})$ . What is the value of $\lim_{n \to \infty} S_n$ ? What I attempted:- $\log S_n=\sum_{k=3}^n \log (1-\tan^4\frac{\pi}{2^k})$ . Since $\lim_{x \to 0} \frac{\tan x}{x}=1$ , $\tan^4\frac{\pi}{2^k}\approx \left(\frac{\pi}{2^k}\right)^4$ Thus, $\log S_n=\sum_{k=3}^n \log (1-\frac{\pi^4}{2^{4k}})\approx \sum_{k=3}^n\left( -\frac{\pi^4}{2^{4k}}\right) $ Taking limit as $n \to \infty$ , $\lim_{n \to \infty} \log S_n=\frac{-\pi^4}{3840}$ . Finally, $\lim_{n \to \infty}S_n=e^{\frac{-\pi^4}{3840}}\approx 1-\frac{\pi^4}{3840}$ Am I correct? Is there any other better method which yield the accurate limit? I was told that the exact limit should be one of the $4$ options:- $\frac{\pi^3}{4},\frac{\pi^3}{16}, \frac{\pi^3}{32},\frac{\pi^3}{256}$ . I guess the third option is correct as it is giving almost the same value that I have obtained.","['infinite-product', 'trigonometry', 'sequences-and-series']"
3060701,Regularity of measure in Lemma 7.2.6 of Bogachev,"In the book ""Measure Theory"" of Bogachev, vol. 2, Lemma 7.2.6 states the following. Let $\mu$ be a $\tau$ -additive, regular Borel measure on a topological space $X$ , and let $\{f_\alpha\}$ be an increasing net of lower-semicontinuous non-negative functions such that $f:= \lim_\alpha f_\alpha$ is bounded. Then $$
\lim_\alpha \int_X f_\alpha \, d\mu \quad = \quad \int_X f \, d\mu .
$$ In the proof it is quite clear where the $\tau$ -additivity comes into play. However, I cannot see why we need $\mu$ to be regular . Bogachev defines this property in the following way (paraphrasing its Definition 7.1.5): A measure $\mu$ on a topological space $X$ is called regular if for every measurable set $A\subseteq X$ and for every $\varepsilon > 0$ there exists a closed set $C_\varepsilon \subseteq A$ such that $\mu(A) - \mu(C_\varepsilon) < \varepsilon$ . Is regularity of $\mu$ in Lemma 7.2.6 then really necessary?","['measure-theory', 'lebesgue-integral', 'borel-measures']"
3060702,Terrence Tao's definition of Lebesgue measurability as an extension of Jordan measurability,"I'm reading Terrence Tao's An Introduction to Measure Theory. On pp. 20, he writes, In analogy with the Jordan theory, we would also like to define
  a concept of “Lebesgue inner measure” to complement that of outer
  measure. Here, there is an asymmetry (which ultimately arises from
  the fact that elementary measure is subadditive rather than superadditive): one does not gain any increase in power in the Jordan inner
  measure by replacing finite unions of boxes with countable ones. I don't quite get this argument. The definition of the Jordan Inner Measure is: For a bounded set, $A$ , in $\mathbb{R}^d$ , \begin{align}
m_{* (J)}(A) = \sup_{E \subset A} m(A), 
\end{align} where $E$ is an elementary set in $\mathbb{R}^d$ definite to be the finite union of boxes, which are just $d-$ dimensional Cartesian products of intervals in $\mathbb{R}$ . If we replace finite unions by countable unions, we can define, \begin{align}
m_{* (L)}(A) = \sup_{E \subset A} m(A),
\end{align} where $m_{*(L)}(A)$ is the Lebesgue inner measure. Clearly, we have, \begin{align}
m_{*(J)}(A) \leq m_{*(L)}(A).
\end{align} This is analogous to the result, \begin{align}
m^{*(L)}(A) \leq m^{*(J)}(A).
\end{align} Since it's not too hard to show that, \begin{align}
m_{*(L)}(A) \leq m^{*(L)}(A),
\end{align} we have, \begin{align}
m_{*(J)}(A) \leq m_{*(L)}(A) \leq m^{*(L)}(A) \leq m^{*(L)}(A).
\end{align} Hence the Lebesgue measure can be though of as a refinement of the Jordan measure. In light of this discussion, why does Tao dismiss the idea of defining the Lebesgue measurable sets as those for which the inner and outer measures coincide? Here are the link to the notes: https://terrytao.files.wordpress.com/2011/01/measure-book1.pdf",['measure-theory']
3060710,Why are two definitions of ellipses equivalent?,"In classical geometry an ellipse is usually defined as the locus of points in the plane such that the distances from each point to the two foci have a given sum. When we speak of an ellipse analytically, we usually describe it as a circle that has been squashed in one direction, i.e. something similar to the curve $x^2+(y/b)^2 = 1$. ""Everyone knows"" that these two definitions yield the same family of shapes. But how can that be proved?","['analytic-geometry', 'conic-sections', 'geometry']"
3060718,How can I guarantee the existence of a solution to this quadratic system of equations?,"I have $n$ real quadratic equations and $n$ real variables, $x_i$ , of the following form: $$\sum_{i\neq j} a_{ijk}x_ix_j+\sum_ib_{ik}x_i+c_k=0 \ \forall  k$$ for $i,j,k\in\{1,\dots n\}$ ; all coefficients are real and I am interested in the existence of a real solution. Is there a condition on the coefficients to guaranty this?","['systems-of-equations', 'algebraic-geometry', 'quadratic-forms']"
3060719,Uniform continuity and compactness,"We know that, if a function $f$ is continuous mapping from a compact metric space to another metric space say $Y$ then $f$ is uniformly continuous. Do we have a generalization of this theorem for general topological space.","['continuity', 'general-topology', 'uniform-continuity', 'compactness']"
3060725,"Probability Question - A large number, $N$ , people go to a convention at a hotel. Each person is assigned one of $N $ hotel rooms....","A large number of people, $N$ , go to a convention at a hotel.  Each person is assigned one of the $N$ hotel rooms.  Before going to the convention, everyone gives their room key to the doorman.  On the way out, the doorman hands the keys back at random. What is the probability that at least one person is given his/her original key? The general equation I have worked out should be, $$P(X \geq 1) = 1 - P(X=0)$$ $$P(X=0) = (1-(1/N))^N$$ Adding up all the probabilities that one person gets the right key.  I have checked with the answer key and this is a solution.  However, it does not work for the case where $N = 2$ . The probability that at least one person gets the right key should be $50% $ ,but this equation returns $75%$ .  Can anyone explain what logical error I am making?","['contest-math', 'derangements', 'discrete-mathematics', 'probability']"
3060774,How to solve this ode $xy''-(1+x)y'+y=x^2$?,"Find the general solution of $xy''-(1+x)y'+y=x^2$ knowing that the homogeneous equation has the following solution: $e^{ax}$ , where $a$ is a parameter you have to find. I have found that $a=1$ or $a=1/x$ .",['ordinary-differential-equations']
3060775,Any good way to calculate $\frac {\alpha ^ n - 1 } {\alpha - 1} \pmod{c}$,"I tried by multiplying modular inverse of denominator to the numerator and then taking modulo $c$ , but there are problems when the inverse does not exist. So is there a good way to solve this problem. Constraints $$ 1 \le \alpha \le 1e9 $$ $c$ is a prime $$ 1 \le n \le 1e9 $$",['number-theory']
3060794,"Tangent line, normal line - confusing.","I am practicing finding tangent and normal line. The tangent/normal line is usually to some graph, and parallel/perpendicular to some other line at the same time. Not that complicated. Can someone, please , verify my solutions? The more I study the less I know, and I don't even know how much I don't know. Thank you! But I'm confused, mostly about the $a$ ""directional coefficient"" of $y=ax+b$ part. Let's say the task is: Find tangent and normal lines to the function $f(x) = \ln(x+1)$ , parallel&perpendicular to $y=\frac{1}{2}x$ . (so 4 variations in total.) What I know: I do know that if a line has to be parallel to other line, then the $a$ coefficients have to be the same. And if perpendicular, then I have to inverse it and change the sign. What I am not sure of: how to properly write the equations. How to properly substitute into the equations. I am doing mistakes here related to $a$ coefficient. I am looking for an algorithmic way so I can easily understand and remember how to solve it. $f(x) = \ln(x+1)$ $f'(x) = \frac{1}{x+1}$ 1. Finding tangent parallel to $y=\frac{1}{2}x$ . $\frac{1}{x+1} = \frac{1}{2}$ $x = 1 = x_{0}$ $f(x_{0}) = f(1) = \ln2$ $\Rightarrow y = \frac{1}{2}x - \frac{1}{2} + \ln2$ 2. Finding tangent perpendicular to $y=\frac{1}{2}x$ $\frac{1}{x+1} = -2$ $x = 1 = x_{0}$ $f(x_{0}) = f(1) = \ln2$ $\Rightarrow y - \ln2 = -2(x-x_{0})$ $\Rightarrow y = -2x + 2 + \ln2$ 3. Finding normal line parallel to $y = \frac{1}{2}x$ $\frac{1}{x+1} = -2$ $x = \frac{-3}{2} = x_{0}$ $f(x_{0}) = f(-\frac{3}{2})= \ln(\frac{-1}{2})$ - does not exist $y = \frac{1}{2}(x-x_{0})$ $y = \frac{1}{2}x + \frac{3}{4}$ 4. Finding normal line perpendicular to $y = \frac{1}{2}x$ $\frac{1}{x+1} = -2$ $x = \frac{-3}{2} = x_{0}$ $f(x_{0}) = f(\frac{-3}{2}) = \ln(\frac{-1}{2})$ - does not exist $y - 0 = -2(x-x_{0})$ $y = -2x - 3$ Thanks for your time!","['tangent-line', 'real-analysis', 'calculus', 'linear-algebra', 'derivatives']"
3060822,Methods to attack integrals that include $(1+x)^{a}\ln^{b}(1+x)$ in the integrand,"I am looking for systematic methods to attack the following class of integrals involving logarithmic functions $$\begin{aligned} I_{0} &= \int_{0}^{1}(1+x)^{a}\ln^{m}(1+x)\,\mathrm{d}x \\
I_{1} &= \int_{0}^{1} x^{a}(1+x)^{b}\ln^{m}x\ln^{n}(1+x)\,\mathrm{d}x \\
I_{2} &= \int_{0}^{1} (1-x)^{a}(1+x)^{b}\ln^{m}(1-x)\ln^{n}(1+x)\,\mathrm{d}x \\
I_{3} &= \int_{0}^{1}x^{a}(1-x)^{b}(1+x)^{c}\ln^{\ell}x\ln^{m}(1-x)\ln^{n}(1+x)\,\mathrm{d}x\end{aligned}$$ where $a, b, c$ are real numbers (usually integers and half-integers) and $\ell, m, n$ are whole numbers such that the integrals converge. Of course, the first three are special cases of $I_{3}$ , but I thought it would be best to tackle the easier cases first before moving to the fully general case in case different techniques are needed. Motivation. The beta function can be written in the form $$ \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} = \int_{0}^{1}t^{a-1}(1-t)^{b-1}\,\mathrm{d}t. $$ From here, it is straightforward to compute integrals of the type $$ \int_{0}^{1}x^{a}(1-x)^{b}\ln^{m}x\ln^{n}(1-x)\,\mathrm{d}x $$ through various series expansions or derivatives of the beta function. Thus one is not afraid of any powers on the logarithms, as one merely has to keep enough terms in the expansion, or compute enough derivatives. Attempt at $I_{0}$ . The presence of $1+x$ as a power and in the logarithm means that one cannot use the beta function directly. I am able to get away with it with the integral $I_{0}$ for now by shifting the boundaries and proceeding as $$\begin{aligned} \int_{0}^{1}(1+x)^{a+\epsilon}\,\mathrm{d}x &= \sum_{m=0}^{\infty}\frac{\epsilon^{m}}{m!}\int_{0}^{1}(1+x)^{a}\ln^{m}(1+x)\,\mathrm{d}x \\ &= \int_{1}^{2}x^{a+\epsilon}\,\mathrm{d}x = \frac{2^{a+1+\epsilon} - 1}{a+1+\epsilon} = \frac{2^{a+1}e^{\epsilon\ln 2} - 1}{a+1+\epsilon} \end{aligned}$$ and keeping the appropriate order coefficient by expanding the exponential in its Taylor series and the denominator in a power series. For example, $$\begin{aligned} \int_{0}^{1}(1+x)^{1+\epsilon}\,\mathrm{d}x &= \sum_{m=0}^{\infty}\frac{\epsilon^{m}}{m!}\int_{0}^{1}(1+x)\ln^{m}(1+x)\,\mathrm{d}x = \frac{4e^{\epsilon\ln 2} - 1}{2+\epsilon} \\
&\approx \left[4\left(1 + \epsilon\ln 2 + \frac{1}{2}\ln^{2}2\,\epsilon^{2}\right) - 1\right]\frac{1}{2}\left(1 - \frac{\epsilon}{2} + \frac{\epsilon^{2}}{4}\right) \\
&\approx \frac{1}{2}\left(3 + 4\ln 2\,\epsilon + 2\ln^{2}2\,\epsilon^{2}\right)\left(1 - \frac{\epsilon}{2} + \frac{\epsilon^{2}}{4}\right)
\end{aligned}$$ so if one requires the $\epsilon^{2}$ coefficient, he merely keep all the terms, multiply by $2! = 2$ to account for the factorial in the original expansion, and arrive at $$ \int_{0}^{1}(1+x)\ln^{2}(1+x)\,\mathrm{d}x = \frac{3}{4} - 2\ln 2 + 2\ln^{2}2 $$ with the integrals $$ \int_{0}^{1}(1+x)\ln(1+x)\,\mathrm{d}x = -\frac{3}{4} + 2\ln 2 \quad\text{and}\quad \int_{0}^{1}(1+x)\,\mathrm{d}x = \frac{3}{2}$$ found for ""free"" if we were not lazy with the multiplication. So we have a method for doing $I_{0}$ for any powers desired. Attempts at $I_{1}$ . The above technique does not seem to generalize to the other integrals. For example, I am currently trying to derive the result $$ \int_{0}^{1}x(1+x)\ln^{2}x\ln(1+x)\,\mathrm{d}x = -\frac{1}{4}\zeta(3) + \frac{4}{27}\ln 2 - \frac{5}{36}\zeta(2) + \frac{37}{72} \tag{1}$$ and many other integrals of a similar form. (Edit: a method for $(1)$ has been found, see $(2)$ below) $$ \int_{0}^{1}(1+x)\ln x\ln^{2}(1+x)\,\mathrm{d}x = -\frac{1}{8}\zeta(3) + 5\ln 2 - 2\ln^{2}2 + \frac{1}{4}\zeta(2) - \frac{23}{8} \tag{2}$$ The class of integrals represented by $I_{1}$ seem difficult, as the zeta functions are familiar from the beta function, the $\ln 2$ is familiar from the treatment of $I_{0}$ , but how an answer like this combines these constants is elusive. Hypergeometric functions seem out of reach, as I seem to lose information about the answer from the Pochhammer symbols. To demonstrate this, one may derive from the Euler integral representation of ${}_{2}F_{1}$ $$\begin{aligned} \int_{0}^{1}x^{\epsilon}(1+x)^{\delta}\,\mathrm{d}x &= \frac{1}{1+\epsilon}\,{}_{2}F_{1}(-\delta, 1+\epsilon; 2+\epsilon; -1) \\
\int_{0}^{1}(1-x)^{\epsilon}(1+x)^{\delta}\,\mathrm{d}x &= \frac{1}{1+\epsilon}\,{}_{2}F_{1}(-\delta, 1; 2+\epsilon; -1) \\
 \int_{0}^{1}x^{\epsilon}(1-x)^{\delta}(1+x)^{\lambda}\,\mathrm{d}x &= \frac{\Gamma(1+\epsilon)\Gamma(1+\delta)}{\Gamma(2+\delta+\epsilon)}\,{}_{2}F_{1}(-\lambda, 1+\epsilon; 2+\delta+\epsilon; -1). \end{aligned}$$ To take the first one as an example, we can write this expression as $$ \frac{1}{1+\epsilon}\,{}_{2}F_{1}(-\delta, 1+\epsilon; 2+\epsilon; -1) = \sum_{k=0}^{\infty}\frac{(-1)^{k}}{k!}\frac{(-\delta)_{k}}{1+k+\epsilon} = \sum_{k=0}^{\infty}{\delta\choose k}\frac{1}{1+k+\epsilon}.$$ Thus one can use power series again to find the desired coefficient of $\epsilon$ , but the $(-\delta)_{k}$ makes things difficult. If we wanted to find the coefficient of $\epsilon\delta^{2}$ , for example as needed in $(2)$ , we would need to sum up an infinite series, since every term $k \geq 2$ will contribute to this coefficient. This particular series does numerically converge to the correct answer, but I cannot find a way to relate it to well-known constants such as $\zeta(3)$ and $\ln 2$ . I have also searched up some other integrals of similar forms on this site, and have found that many of the answers proceed with pages of horrendous algebra involving double series, harmonic numbers, etc. which I would like to avoid if possible as I am not very familiar with manipulating harmonic numbers as they appear in series. Of course, I will accept manipulations of harmonic numbers, etc. if it is the only straightforward way to proceed. Lastly, the references below may be of some use. The second reference may well be the key to solving this problem but I would like some clarification as dealing with the $\ln(1+x)$ still seems problematic... Devoto, A. and Duke, D. Table of integrals and formulae for Feynman diagram calculations. La Rivista del Nuovo Cimento, 1984. Vermaseren, J. A. M. Harmonic sums, Mellin transforms and integrals. International Journal of Modern Physics A, 2012. Sofo, A. Integrals of logarithmic and hypergeometric functions. Communications in Mathematics, 2016.","['integration', 'beta-function', 'definite-integrals', 'gamma-function']"
3060832,"Proof in EGA I Chapter 0, point 3.2.2","I am reading the proof of a necessary and sufficient condition for a presheaf over a base of a topology to be a sheaf, from Élements de Géometrie Algébrique I , chapter 0, point 3.2.2. There is a line where I'm lost and I can't understand where the things the author says come from. Context Let $X$ be a topological space and $B$ a base of its topology, considered as a category with inclusion maps as morphisms. A presheaf on $B$ is just a contravariant functor $\mathcal{F}:B\to \mathcal{C}$ , where $\mathcal{C}$ is any category that admits projective limits. Denote by $\rho^U_V:\mathcal{F}(U)\to\mathcal{F}(V)$ the restriction morphisms. From this we can define a presheaf $\mathcal{F}'$ on $X$ by setting $\mathcal{F}'(U)=\varprojlim\mathcal{F}(V)$ , where $V\subseteq U$ and $V\in B$ . In the proof of 0-3.2.2, the author bassicaly shows that this definition doesn't depend on $B$ and therefore it fulfills the condition needed to be a sheaf with the following hypothesis: For every covering $(U_\alpha)$ of $U\in B$ by sets $U_\alpha\in B$ contained in $U$ , and for every object $T\in\mathcal{C}$ , the map that sends every $f\in\mathrm{Hom}(T,\mathcal{F}(U))$ to the family $(\rho^U_{U_\alpha}\circ f)\in\prod\mathrm{Hom}(T,\mathcal{F}(U_\alpha))$ is a bijection onto the the family $(f_\alpha)$ such that $\rho^{U_\alpha}_V\circ f_\alpha=\rho^{U_\beta}_V\circ f_\beta$ for every pair of indices $(\alpha,\beta)$ and every $V\in B$ with $V\subseteq U_\alpha\cap U_\beta$ . In order to do that, he chooses a base $B'\subseteq B$ , and defines $\mathcal{F}''$ in the same way as $\mathcal{F}'$ but taking projective limit over the elements of $B'$ . Question For every open set $U$ there is a morphism $\mathcal{F}'(U)\to\mathcal{F}''(U)$ which is the projective limit of $\mathcal{F}'(U)\to\mathcal{F}(V)$ for $V\in B'$ . Now, if $U\in B$ , we want to show that $\mathcal{F}'(U)\to\mathcal{F}''(U)$ is an isomorphism, and the following is stated il est immédiat de voir que les composés des morphismes $\mathcal{F}(U)\to\mathcal{F}''(U)$ et $\mathcal{F}''(U)\to\mathcal{F}(U)$ ainsi définis sont les identités. In english: it is immediate to see that the compositions of the morphisms $\mathcal{F}(U)\to\mathcal{F}''(U)$ and $\mathcal{F}''(U)\to\mathcal{F}(U)$ defined like this are identities. What I don't see is how is $\mathcal{F}(U)\to\mathcal{F}''(U)$ defined. The other direction is straight forward from the universal property of the projective limit, but there is no clue in the previous text of how $\mathcal{F}(U)\to\mathcal{F}''(U)$ is defined (or at least I can't find it). Alternative Alternatively, I tried to prove the theorem using the morphisms $\mathcal{F}'(U)\to\mathcal{F}''(U)$ and back, given by the filtering property of any base. But I don't know how to show that these compositions are identities. I would consider my question answered if this alternative is answered.","['proof-explanation', 'algebraic-geometry', 'sheaf-theory']"
3060899,Napoleon-like theorem concerning squares erected on sides of midpoint polygon of octagon,"Given an arbitrary octagon, construct it's midpoint polygon(the midpoint formed by the midpoints of the sides). Erect squares on the sides of the midpoint polygon, all inwards or all outwards. Consider the four segments, each connecting the centroids of two squares corresponding to opposite sides of the midpoint polygon. The midpoint of these segments form a square. I distinctly remember seeing this theorem in some geometry article, but I have been unable to find that article. I believe it was attributed to Van Aubel, however I am not too sure. I want to find the source of this theorem because I have found a rather powerful generalisation and I want to revisit that article. Does anyone find this theorem familiar?","['euclidean-geometry', 'geometry', 'reference-request', 'geometric-transformation', 'complex-numbers']"
3060902,Why do we need both Divergence and Curl to define a vector field?,"I was reading Classical Electrodynamics by J.D.Jacskon (section 1.5) where he said: Perhaps some readers know that a vector field can be specified almost completely if its divergence and curl are given everywhere in space with a comment (almost)-Up to the gradient of a scalar function that satisfies the Laplace equation. why is this so if we want to know $\vec{E}$ in 3-D space we really just want to find out is $E_x,E_y$ and $E_z$ (in cartesian coordinates) which are just 3 scalar functions.
If we know the curl is (let's say) zero, that is $$\nabla \times \vec{E}=0$$ in cartesian coordinates that gives us three equations $$\frac{\partial E_z}{\partial y}-\frac{\partial E_y}{\partial z}=0$$ $$\frac{\partial E_x}{\partial z}-\frac{\partial E_z}{\partial x}=0$$ $$\frac{\partial E_y}{\partial x}-\frac{\partial E_x}{\partial y}=0$$ these are three equations for 3 unknown quantities, that should give us enough* information about these three functions, it obviously isn't so hence we need to know the divergence as well, my question is why?
is there some theorem that proves this (I suspect there is $:)$ ) this is obviously a mathematical question although it is physics motivated
so I hope it is ok that I post it here. (*up to maybe a constant because all the equations involve first derivatives, I can live with that)","['vector-fields', 'electromagnetism', 'ordinary-differential-equations']"
3060911,Solve the homogeneous differential equation with functional coefficients,"I have to solve this equation: $$ y''-\frac{1}{x} y' + \frac{1}{x^3} y =0 $$ This is a nonconstant coefficient homogeneous differential equation. How should I do it? I tried with the supstitution: $y(x)=u(x)\sqrt{x}$ , and then I got this equation: $$u''+(\frac{1}{x^3}-\frac{3}{4x^2})u=0$$ , which seems to me easier to solve, but I don't know how to solve this either! If someone could help me, I would be very grateful.",['ordinary-differential-equations']
3060913,Computing a derivative through Lie series,"Consider the $N$ -dimensional autonomous system of ODEs $$\dot{x}= f(x),$$ where a locally unique solution $x(t)$ , starting from the initial condition $x$ , is denoted as $x(t)=\phi(t,x)$ . Assume that $$\Big(\frac{\partial}{\partial{x}}\phi(t,x)\Big)f(x)=f(\phi(t,x))$$ For the system above, assume that $f(x)$ is analytic (that is, its Taylor series converges to $f$ itself). Let the differential operator $L[\xi]$ be defined as $$L[\xi]=f(x)\boldsymbol{\cdot}\nabla{\xi}=\sum_{n=1}^{N}f_i(x)\frac{\partial{\xi}}{\partial{x_i}}$$ Show that $\phi(t,x)$ can be expressed as $$\phi(t,x)=\sum_{n=0}^{\infty}\frac{t^n}{n!}L^n[x]$$ where $L^n[\xi]$ is the shorthand notation for $$L^n[\xi]=\underbrace{L[L[\cdots{L}[\xi]}_{n\text{-times}}\cdots]]$$ Potentially related questions: How to properly apply the Lie Series Exponential of a function times derivative How to derive these Lie Series formulas I'm stuck on how to approach this problem. Here is all the information that I have gathered so far - Through this question, the one dimensional situation states that $e^{a\partial}f(x)=f(a+x)$ (we can think of this as a shift operator). Inside Ordinary Differential Equations and Dynamical Systems by Teschl, we have the following Lemma (Lemma $6.2$ on page $190$ of the text). Lemma (Straightening out of vector fields): Suppose $f(x_0)\neq0$ . Then, there is a local coordinate transform $y=\varphi(x)$ such that $\dot{x}=f(x)$ is transformed to $$\dot{y}=(1,0,...,0)$$ Teschl list a similar problem on page $191$ (problem $6.5$ for one-parameter lie groups) in which he states that Hint: The Taylor coefficients are the derivatives which can be obtained by
differentiating the differential equation. So, I think that I need to apply what was done in this question alongside Lemma 6.2. I will have to consider what a vector field means in this context. I might be able to make the assumption that a vector field is just a linear operator. We are given that $\dot{x}= f(x)$ is an autonomous system of ODEs $x(t)=\phi(t,x)$ $\Big(\frac{\partial}{\partial{x}}\phi(t,x)\Big)f(x)=f(\phi(t,x))$ $L[\xi]=f(x)\boldsymbol{\cdot}\nabla{\xi}=\sum_{n=1}^{N}f_i(x)\frac{\partial{\xi}}{\partial{x_i}}$ and we need to show that $$\phi(t,x)=\sum_{n=0}^{\infty}\frac{t^n}{n!}L^n[x]$$ I also see that Roger Howe wrote a good introduction to lie theory in these notes (he goes through one-parameter lie groups on pages $604-606$ ). This appears to be an extremely difficult problem for someone unfamiliar with lie theory. I am going to see if I can figure out a more direct approach.","['differential-operators', 'lie-groups', 'ordinary-differential-equations']"
3060919,Solution to the parabolic cylinder equation,"In the Gradshteyn & Ryzhik (7th ed.) the differential equation (9.255) leading to parabolic cylinder functions is $$\frac{d^2u}{dz^2}+(p+\frac{1}{2}-\frac{z^2}{4})u=0.$$ The solutions are $u=D_p(z),D_p(-z),D_{-p-1}(iz),D_{-p-1}(-iz)$ , where $D_p(z)$ is the parabolic cylinder function. These four solutions are linearly dependent. My question is why is there four solutions to the second order ODE? In my case $p$ is complex, and Mathematica gives solution in the form $C_1 D_p(z)+C_2 D_{-p-1}(iz)$ .","['special-functions', 'ordinary-differential-equations']"
3060967,simple proof $f(z) = \arcsin(\frac{2z}{(1+z^2)}) + 2\arctan(z)$,"$f(z) = \arcsin(\frac{2z}{(1+z^2)}) + 2\arctan(z)$ So the thing that I need to do is to show that $f(2019) = \pi$ . So the thing that I have tried is to calculate $f(0), f(1)$ , and to see some kind of connection (or recurrence relation), so I can easily calculate $f(2019)$ , but I couldn't find any. Any tips?","['functions', 'analysis']"
3060980,Integrating Taylor's Approximation,I am reviewing Casella's All of Statistics. A particular integral comes up when reviewing non-parametric statistics in particular Histograms. Assume that some $x\in B_j$ and any other $u \in B_j$ where $B_j$ is some particular bin in the histogram. I don't think the frequencies are going to  be too important here... $$\int_{B_j}f(u)du \approx \int_{B_j}f(x) + (u-x)f^{\prime}(u)$$ $$= f(x)h + hf^{\prime}(x) ( h (j-\frac{1}{2})-x)$$ Now I can see that the first term is just base times height.... this is a histogram after all... But that second term is bothering me. I can't think of a simple geometrical interpretation for what it means. I figure it is some sort of compensation for the value that exists between the histogram and the true $f$ . Is this accurate? And if so where does a derivation of this exist? Or did I just forget my elementary calculus?,"['calculus', 'statistics']"
3060997,Least square problem constrained to projection matrices,"Some times in engineering, it is important to find an optimum subspace in which projecting on it satisfies some properties. Let known matrices $A$ and $B$ belong to $\mathbb{R}^{p\times n}$ and $\|\cdot\|_F$ be Frobenius norm.
How can I find the best subspace in which projecting $A$ on it is as close as possible to $B$ ?
In other words how can I find a solution to the following constrained optimization problem? \begin{eqnarray}
&&\min_P \|PA-B\|_F^2 \\
 &&\mathrm{s.t. \ }P^T=P, \, P^2=P
\end{eqnarray} Update: I have incorporated the symmetry property ( $P^T=T$ ) in objective function as follows. 
Since $P$ is symmetric, $P$ can decompose as $P=Y+Y^T$ where $Y\in \mathbb{R}^{n\times n}$ . Now, the optimization problem reduces to \begin{eqnarray}
&&\min_Y \|C(Y+Y^T)-D\|_F^2 \\
 &&\mathrm{s.t. \ }\, (Y+Y^T)^2=Y+Y^T,
\end{eqnarray} Where $C=A^T$ and $D=B^T$ . Further, using ""vec"" operator, we get
vec $(Y^T)=$$K$ vec $(Y)$ , where $K\in \mathbb{R}^{n\times n}$ is a unique and known matrix.
Using ""Kronecker product"", our optimization problem will be reduced to \begin{eqnarray}
&&\min_y \|(I\otimes C)(I+K)y-d\|_2^2 \\
 &&\mathrm{s.t. \ }\, (Y+Y^T)^2=Y+Y^T,
\end{eqnarray} where $y=$ vec $(Y)$ and $d=$ vec $(D)$ .","['projection', 'matrices', 'linear-algebra', 'linear-transformations', 'numerical-linear-algebra']"
3061002,Using polar coordinates to find the critical points,"I have written the following DE system $$\dot{x} = x+y-x(x^2+y^2)\\
\dot{y} = -x+3y-y(x^2+y^2) $$ as follows in Polar form: $$\dot{r} = -2r\cos^2\theta+r(3-r^2) $$ $$\dot{\theta} = 2\sin\theta\cos\theta-1$$ What I wonder is could I also find all the critical points of this system by setting $\dot{\theta}$ and $\dot{r}$ equal to zero? Since these two systems are actually equivalent to each other this seemed like a logical thing to me and at the same time finding the critical points through setting $\dot{x}$ and $\dot{y}$ equal to zero seems like much more tedious in this state. Solving for $\dot{\theta}=0$ leads to $2\sin\theta\cos\theta = 1$ which is the same as $\sin2\theta=1$ leading to $\theta = \frac{\pi}{4}$ or $\theta = -\frac{3\pi}{4}$ . Substituting these within $\dot{r}$ leads to: $$-2r\cos^2\theta+r(3-r^2) = -2r(\pm\frac{\sqrt{2}}{2})^2 +r(3-r^2)=r(2-r^2)$$ So $\dot{r}=0$ if $r=0$ or $r^2=2$ . This leads us to the following critical points: $(0,0)$ , $(\sqrt{2},\sqrt{2})$ , $(-\sqrt{2},-\sqrt{2})$ . 
However what bugs me is that entering these points into the non-polar system does not lead to $\dot{x}=\dot{y}=0$ for $(\sqrt{2},\sqrt{2})$ , $(-\sqrt{2},-\sqrt{2})$ . What am I missing here? Is the whole thought of trying to find critical points through putting $\dot{\theta}$ and $\dot{r}$ equal to zero unwise? Any help or hint with this will be much appreciated.","['polar-coordinates', 'systems-of-equations', 'ordinary-differential-equations']"
3061055,$\int_A f = \int_A g \implies f = g$ a.e.,"Consider a measure space $(\Omega, \mathcal{F}, \mu)$ and let $f,g: \Omega \to \mathbb{R}$ be $\mathcal{F}$ -measurable integrable functions. If $$\int_A f d \mu = \int_A g d \mu$$ for all $A \in \mathcal{F}$ , then $f = g$ $\mu$ -a.e. Here is the proof my course notes provide: It is sufficient to prove that $\mu\{f < g\} = 0 = \mu\{g < f\}$ . By symmetry, we only prove the first equality. Put $$h:= (g-f)I_{\{g > f\}}$$ Then $$\int_\Omega h d\mu = \int_{\{f<g\}}(g-f) = 0$$ since $f,g$ are integrable functions. Since $h \geq 0$ , it follows that $h = 0$ a.e. and thus $\mu\{g > f\} 
 \leq \mu\{h \neq 0\} = 0$ . This ends the proof. My question: Can we generalise this to integrable functions $f,g:
 \Omega \to [-\infty, \infty]$ (note the change of codomain). I think the proof generalises: the function $h$ is well defined because we can only have $\infty - (-\infty) = \infty$ . Is this correct?","['measure-theory', 'proof-verification', 'lebesgue-integral', 'almost-everywhere']"
3061071,Random projection of a fixed point,"In the book ""High-dimensional probability by Vershynin"", page 111, in the proof of Johnson-Lindenstrauss Lemma, let $E$ be a random $m$ -dimensional subspace in $\mathbb{R}^n$ uniformly distributed in the Grassmannian $G_{n,m}$ , i.e, $$E \sim Unif(G_{n,m}))$$ . Denote the orthogonal projection onto $E$ by $P$ . Let $z\in \mathbb{R}^n$ be a fixed point such that $||z||_2=1$ . The book intuitively claim that instead of random projection $P$ acting on a fixed vector $z$ , we consider a fixed projection $P$ acting on a random vector $z\sim Unif(S^{n-1})$ . Then, the distribution of $||Pz||_2$ is unchanged. I was wondering if there is a rigorous way to show the invariance of distribution. I thin we may use the rotation invariance property of $z$ .","['projection', 'linear-algebra', 'probability-theory', 'probability', 'rotations']"
3061074,Number of closed loops in a square grid,"QUESTION: Given an $m\times n$ grid of squares, is a formula known for the number of closed loops that can be drawn along the perimeters of these squares? For a better description of what I mean, see the link below. MOTIVATION: I have been considering the puzzle game ""Slitherlink"" in which the solution to the puzzle consists of a closed loop drawn in a square lattice grid satisfying certain constraints. So, given an $m\times n$ grid of squares, I would like to calculate the number of closed loops that one can draw in that grid. Does anyone know of a formula that computes this, or a quick algorithm that one can use to determine this number in terms of $m,n$ ? Are there any special cases that are easier to determine than others? Where can I read more about this problem? An equivalent problem: given a graph $G$ , how can one calculate the number of cyclic subgraphs it contains?","['graph-theory', 'puzzle', 'combinatorics']"
3061094,What other kinds of cubic integer rings are there?,"Given an integer $n \in \Bbb{Z}$ , we understand $\root 3 \of n$ to mean the number $x \in \Bbb{R}$ such that $x^3 - n = 0$ . Then $\Bbb{Q}(\root 3 \of n) \subset \Bbb{R}$ , right? The same then goes for the ring of algebraic integers $\mathcal{O}_{\Bbb{Q}(\root 3 \of n)}$ . Since $\root 3 \of {-n} = -\root 3 \of n$ , it follows that $\Bbb{Q}(\root 3 \of n) = \Bbb{Q}(\root 3 \of {-n})$ and likewise $\mathcal{O}_{\Bbb{Q}(\root 3 \of n)} = \mathcal{O}_{\Bbb{Q}(\root 3 \of {-n})}$ . So in order for a ring adjoining a cubic root to $\Bbb{Q}$ to have complex numbers we need a cubic root that is imaginary or complex. Therefore, given $$\omega = \frac{-1 + \sqrt{-3}}{2}$$ and $n > 1$ , the ring of integers of $\Bbb{Q}(\omega \root 3 \of n)$ should contain complex numbers. What about the ring of $\Bbb{Q}(i \root 3 \of n)$ ? That is a distinct kind of rings than the other two I've mentioned, right? Is this all correct? What kinds of cubic rings have I overlooked?","['algebraic-number-theory', 'abstract-algebra']"
3061102,On $\sup|\varphi^{-1}(n)|=+\infty$,"I am trying to find an elementary proof of the following fact: Given some $N\geq 2$ , there are $N$ distinct integers $a_1,\ldots,a_N$ such that $\varphi(a_1)=\ldots=\varphi(a_N)$ with $\varphi$ being Euler's totient function. My original analytic proof goes as follows:
If the number of solutions of $\varphi(x)=N$ were bounded, the series $$ \sum_{n\geq 2019}\frac{1}{\varphi(n)\log^2\varphi(n)}$$ would be convergent by comparison with $\sum_{n\geq n_0}\frac{1}{n \log^2 n}$ and condensation. It is enough to show that the last series is divergent. It is bounded below by a multiple of $$ \sum_{n\geq 2019}\frac{\sigma(n)}{n^2 \log^2(n)}$$ and since $$ \sum_{n\geq 1}\frac{\sigma(n)}{n^{2+s}}=\zeta(s+1)\zeta(s+2)$$ for any $s>0$ , it is enough to prove that the integral $$ \int_{0}^{+\infty}\int_{0}^{+\infty}\zeta(1+t+s)\zeta(2+t+s)\,ds\,dt $$ is divergent, or that the integral $$ \int_{0}^{+\infty} u\,\zeta(1+u)\zeta(2+u)\,du $$ is divergent. On the other hand this is trivial since $u\zeta(1+u)\zeta(2+u)\geq u$ for any $u>0$ . Alternative combinatorial proof: we may consider a very large $N$ and the numbers in $[N,2N]$ with at least $\log\log\log N$ prime factors. They have a positive density in $[N,2N]$ , and they are mapped by the totient function into an interval with length $O\left(\frac{N}{\log \log N}\right)$ . By the pigeonhole principle, at least $\Omega(\log\log N)$ elements of $[N,2N]$ share the same $\varphi$ . I would be happier in having a combinatorial proof possibly not relying on subtle statements about the average order of $\omega(n)$ or Mertens' theorem about $\sum_{p\leq x}\frac{1}{p}$ .","['number-theory', 'pigeonhole-principle', 'arithmetic-functions', 'alternative-proof', 'dirichlet-series']"
3061126,Cannot prove a geometry area ratio between a triangle and a parallelogram,"ABCD is a parallelogram. Prove the following: $\frac{BF}{FA} = \frac{AD}{AE}$ $\frac{S_{ADF}}{S_{AEF}} = \frac{AD}{AE}$ $S_{EBF} = S_{ADF}$ $S_{BCE} = \frac{1}{2}S_{ABCD}$ I solved the first 3, but could not solve the 4th: 1. $$\text{Thale's theorm:}$$ $$\frac{AE}{CB} = \frac{AF}{FB} = \frac{EF}{FC}$$ $$\frac{AE}{AD} = \frac{EF}{FC}$$ $$\downarrow$$ $$\frac{AE}{CB} = \frac{AF}{FB} = \frac{EF}{FC} = \frac{AE}{AD}$$ $$\frac{AF}{FB} = \frac{AE}{AD}$$ $$\downarrow$$ $$\boxed{\frac{FB}{AF} = \frac{AD}{AE}}$$ 2. $$\text{Let P be a point on ED such that FP will be perpendicular to ED.}$$ $$\div\begin{cases} S_{AEF} = \frac{AE\cdot FP}{2} \\ S_{ADF} = \frac{AD\cdot FP}{2}\end{cases}$$ $$\frac{S_{AEF}}{S_{ADF}} =\frac{AE}{AD}$$ $$\downarrow$$ $$\boxed{\frac{S_{ADF}}{S_{AEF}} =\frac{AD}{AE}}$$ 3. $$\text{Let G be a point on AB such that EG will be perpendicular to AB. Then:}$$ $$\frac{S_{AEF}}{S_{EFB}} = \frac{\frac{AF\cdot EG}{2}}{\frac{FB\cdot EG}{2}} = \frac{AF}{FB} = \frac{AE}{AD} = \frac{S_{AEF}}{S_{ADF}}$$ $$\frac{S_{AEF}}{S_{EFB}} = \frac{S_{AEF}}{S_{ADF}}$$ $$\downarrow$$ $$\frac{S_{EFB}}{S_{AEF}} = \frac{S_{ADF}}{S_{AEF}}$$ $$\boxed{S_{EFB} = S_{ADF}}$$ I have absolutely no clue. I see no way of creating a relation between the areas of the triangle and the parallelogram. I thought of trying to somehow prove that the area of BCE is identical to that of BCD or BAD, but couldn't find a way to connect those either, as they don't have a shared perpendicular.",['geometry']
3061154,"For a function defined by parts study continuity, and differentiability at two points","For the function defined by $$F(x)=\begin{cases}\displaystyle\int_x^{2x}\sin t^2\,\mathrm dt,&x\neq0\\0,&x=0\end{cases}$$ analyze continuity and derivability at the origin. Is $F$ derivable at point $x_0=\sqrt{\pi/2}$ ? Justify the answer, and if possible, calculate $F'(x_0)$ . I have been told that I must use the Fundamental Theorem of Integral Calculus but I do not know how to apply it to this case. For the function to be continuous at the origin, it must happen that $F(0)=\lim_{x\to0}F(x)$ . We know that $F(0)=0$ , and $$\lim_{x\to0}F(x)=\lim_{x\to0}\int_x^{2x}\sin t^2\,\mathrm dt\;{\bf\color{red}=}\int_0^{2\cdot0}\sin t^2\,\mathrm dt=0,$$ so the statement holds, but here I do now how to justify the $\bf\color{red}=$ . To find the derivative at $x_0=0$ I tried the differentiate directly $F(x)$ but it is wrong, so I have been told that I must use the definition. So we have to find $$F'(0)=\lim_{x\to0}\frac{F(x)-F(0)}{x-0}=\lim_{x\to0}\frac{\int_x^{2x}\sin t^2\,\mathrm dt}x.$$ Why we have to bound $\left|\sin t^2\right|\leq t^2$ ? How can we do that? Finally, I do not know how to use the aforementioned theorem to justify that the function is derivable in $\sqrt{\pi/2}$ . Using the definition again: \begin{align*}
F'\left(\sqrt{\frac\pi2}\right)&=\lim_{x\to\sqrt{\frac\pi2}}\frac{F(x)-F\left(\sqrt{\frac\pi2}\right)}{x-\sqrt{\frac\pi2}}\\
&=\lim_{x\to\sqrt{\frac\pi2}}\frac{\int_x^{2x}\sin t^2\,\mathrm dt-\int_{\sqrt{\pi/2}}^{2\sqrt{\pi/2}}\sin t^2\,\mathrm dt}{x-\sqrt{\frac\pi2}}\\
&\leq\lim_{x\to\sqrt{\frac\pi2}}\frac{\int_x^{2x}t^2\,\mathrm dt-\int_{\sqrt{\pi/2}}^{2\sqrt{\pi/2}}t^2\,\mathrm dt}{x-\sqrt{\frac\pi2}}\\
&\underbrace=_{A=\sqrt{\pi/2}}\lim_{x\to A}\frac{1/3((2x)^3-x^3)-1/3((2A)^3-(A^3))}{x-A}\\
&=\frac73\lim_{x\to A}\frac{x^3-A^3}{x-A}\\
&=\frac73\lim_{x\to A}\frac{(x-A)(x^2+Ax+A^2)}{x-A}\\
&=\frac73(A^2+A^2+A^2)\\
&=7A^2\\
&=\frac{7\pi}2,
\end{align*} but it is wrong. How can we solve the statement? Thanks!","['continuity', 'calculus', 'derivatives']"
3061157,Questions About the Proof of Cauchy–Pompeiu Integral Formula.,"I am studying function theory in several complex variables and the book I am using is ""Tasty Bits of Several Complex Variables"" by Jiří Lebl: https://www.jirka.org/scv/scv.pdf . At the moment I am reading chapter 4, where he introduces the $\bar{\partial}$ -problem. However, he begins by proving a generalized form of the Cauchy Integral formula and I have some questions about the proof. Theorem: Let $U\subset\mathbb{C}$ be a bounded domain with piecewise $C^1$ -smooth boundary $\partial U$ oriented positively, and let $f:\bar{U}\to\mathbb{C}$ be a $ C^1$ -smooth function. Then for $ z\in U$ : $ f(z)=\frac{1}{2\pi i}\int_{\partial U}\frac{f(\zeta)}{\zeta-z}d\zeta+\frac{1}{2\pi i}\int_U\dfrac{\frac{\partial f}{\partial\bar{z}}(\zeta)}{\zeta-z}d\zeta\wedge d\bar{\zeta}$ . ""Proof"" : This is how he does it. He begins by fixing $z\in U$ and a small disc $\Delta_r(z)$ such that $\Delta_r(z) \subset\subset U$ . He then applies Stokes Theorem, which gives us $$\int_{\partial U}\frac{f(\zeta)}{\zeta-z}d\zeta-\int_{\partial \Delta_r(z)}\frac{f(\zeta)}{\zeta-z}d\zeta=\int_{U\backslash\Delta_r(z)}d\Big ( \frac{f(\zeta)}{\zeta-z}d\zeta \Big )=\int_{U\backslash\Delta_r(z)}\dfrac{\frac{\partial f}{\partial\bar{\zeta}}(\zeta)}{\zeta-z}d\bar{\zeta}\wedge d\zeta.\quad\quad\quad (*)$$ The above equation begs us to let $r$ tend to $0$ . If we manage to get anything out of this, we are done. Notice that $\dfrac{\frac{\partial f}{\partial\bar{\zeta}}(\zeta)}{\zeta-z}$ is integrable over all of $U$ (this follows from an exercise right before the proof). Therefore $$\lim_{r\to 0}\int_{U\backslash\Delta_r(z)} \dfrac{\frac{\partial f}{\partial\bar{\zeta}}(\zeta)}{\zeta-z}d\bar{\zeta}\wedge d\zeta=\int_{U} \dfrac{\frac{\partial f}{\partial\bar{\zeta}}(\zeta)}{\zeta-z}d\bar{\zeta}\wedge d\zeta=-\int_{U} \dfrac{\frac{\partial f}{\partial\bar{\zeta}}(\zeta)}{\zeta-z}d\zeta \wedge d\bar{\zeta},$$ where we used the asymmetry of the operator $\wedge$ in the last equality. As a last step. Jiří argues that, by continuity of $f$ , we find $$\lim_{r\to 0}\frac{1}{2\pi i}\int_{\partial\Delta_r(z)} \dfrac{f(\zeta)}{\zeta-z} d\zeta = \lim_{r\to 0}\frac{1}{2\pi}\int_{0}^{2\pi} f(z+re^{i\theta})d\theta=f(z).\quad\quad\quad (**)$$ Questions: (1) My first question is about the Stokes Theorem. It's quite a long time since I last used Stokes theorem and back then it was in a basic calculus class. I have looked around and I cannot manage to find something which seems to give me $$ \int_{\partial U}\frac{f(\zeta)}{\zeta-z}d\zeta-\int_{\partial \Delta_r(z)}\frac{f(\zeta)}{\zeta-z}d\zeta=\int_{U\backslash\Delta_r(z)}d\Big ( \frac{f(\zeta)}{\zeta-z}d\zeta \Big ).$$ I suspect there is a differential form version of Stokes Theorem I should use? I found this on wikipedia: Stokes' theorem says that the integral of a differential form ω over
the boundary of some orientable manifold Ω is equal to the integral of
its exterior derivative dω over the whole of Ω, i.e., $$\int_{\partial\Omega}\omega=\int_{\Omega}d\omega.$$ Is it the above I am using in some way, if so, could you please tell me how this is being used? Also, as a bonus question, I do not see why we choose a small disc $\Delta_r(z)\subset\subset U$ . But perhaps this will be answered at the same time as the explanation of Stokes Theorem. (2) He argues that the second equality in $(*)$ follows since holomorphic derivatives in $\zeta$ will have a $d\zeta$ and when we wedge with $d\zeta$ we just get zero. I, honestly, do not understand what I just wrote. Can someone explain to me what this means or give another argument why the second equality is true? (3) The computations in $(**)$ seems to be fundamental ones. For instance, it seems to me that we are using the Cauchy integral formula(?) And doing a variable substitution - polar coordinates(?) However, I am not sure about this. I would be really happy if you could tell me how the first equality in $(**)$ holds and where the continuity of $f$ is being used. Just as a last thought. Perhaps the continuity is being used when we compute the limit to obtain the last equality? As you see, I find it quite difficult to understand the proof, so I would be really happy if you could help me by answering (1), (2) and/or (3). Thanks for taking your time!","['complex-analysis', 'several-complex-variables', 'cauchy-integral-formula', 'exterior-algebra']"
3061176,How many of these strings have the property that they are the same as their output string?,"An input is a string of zeros and ones. Write down a row below it using the Pascal triangle rule, where each number is the sum of the two numbers diagonally above it. We are working “modulo $2$ ” so $1 + 1 = 0$ . Repeat until you form a triangle of numbers. Here is an example, where the input is $10111$ . 1----------0----------1---------1---------1
-----1----------1----------0---------0-----
----------0----------1----------0----------
---------------1----------1----------------
---------------------0--------------------- The output of this procedure is the string read along the side of the triangle from the bottom to the top right, in that order. In this case the output is $0 1 0 0 1$ .
We decide to use a string of length $n$ as input, so there are $2^n$ possible input strings. How many of these strings have the property that they are the same as their output string? My attempt Fisrt I was trying to write the strings and wanted to check which pattern gives me the same output and which pattern will not. First observation: if we start writting the string from the right if we get one number where it gives you wrong output(unequal) then there is no point to stretch it to left any further as the numbers are inductive. e.g: $11$ is wrong so $011$ or $....11$ will be wrong 2nd observation: We can't start with $1$ from the right if $n>1$ 3rd observation:From the right we can go on putting (even)zeros and then if you put a single $1$ that is ok but you can't put more than then wrong
e.g: $1100,0100$ wrong. This doesn't apply if we put odd zeros as $11000$ is right. 4th observation: If we put from right $0$ then $1$ and go on writing (even) 1 then we can alternate $0$ and $1$ after that and we can write same number of (even) zeros after (even) ones but we can't extend it any further e.g $1010110$ , $00110$ are right but $000110$ , $100110$ are wrong. 5th observation: If we put from right $0$ then $1$ and go on writing (odd) 1 then we are allowed to put only one zero after that and anything we put after that will give us wrong answer e.g $01110$ is right but both $101110$ and $001110$ are wrong. But I can't see any general pattern please help.","['contest-math', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'computer-science']"
3061204,Subgroup of Möbius transformations which are isometries with respect to the standard metric on the Riemann sphere,"I'm trying to find which subgroup of Mobius transformations are isometries with respect to the standard metric on the Riemann sphere (the one induced from the Euclidean metric on $\mathbb{R}^3$ ). The question hints that the distance on the Riemann sphere corresponds to the distance function on $\mathbb{P}^1$ given by $$d(L_1,L_2) = 2\sqrt{1 - \frac{|\langle v,w \rangle|^2}{||v||^2||w||^2}}$$ where $ v \in L_1\backslash\{0\}$ and $w \in L_2\backslash\{0\}$ , and that a Mobius map corresponds to the action of a matrix $A \in GL_2(\mathbb{C})$ on lines in $\mathbb{C}^2$ , and asks me to consider which $2x2$ matrices automatically preserve this expression for $d$ . I honestly have no idea where to start with this question, so I'd really appreciate whatever help you might be able to give.","['metric-spaces', 'mobius-transformation', 'complex-analysis', 'isometry', 'stereographic-projections']"
3061224,Evaluating an improper integral $\int_{0}^{\infty}\frac{x^2}{(x^4+1)^2}dx$,"I tried to solve the integral: $$\int_{0}^{\infty}\frac{x^2}{(x^4+1)^2}dx$$ using $ x = \sqrt{\tan(u)}$ and $dx = \frac{ \sec^2(u)}{2\sqrt{\tan(u)}} du,$ but I ended up with an even worse looking integral $$ \int_{0}^{\frac{\pi}{2}}\frac{\sqrt{\tan(u)}}{\sec^2(u)}du.$$ Wolfram gave an answer of $ \dfrac{\pi}{8\sqrt{2}},$ but how would one get to that answer?","['improper-integrals', 'calculus', 'definite-integrals']"
3061247,If $f$ is analytic and $f(z)^2$ = $\bar f(z)$ then $f$ is constant,"I'm currently stuck on the following problem. Let f be an analytic function on a non-empty connected open set V. If $f(z)^2$ = $\bar f(z)$ $\forall z\in V$ then f is constant on V. I think I should be working with the Maximum Modulus theorem, but I am not sure how to use it.","['complex-analysis', 'real-analysis']"
3061258,Example of 2 matrices similar but not row equivalent,"If two matrices are row equivalent, they may not be similar because all invertible matrices are row equivalent to $I$ , yet not all invertible matrices have the same trace, eigenvalues etc. Is it also true that if two matrices are similar, they may not be row equivalent? My instinct is that there is no reason that 2 similar matrices need to be row equivalent since having the same rank, eigenvalues, determinant etc does not necessarily make them row equivalent. Any suggestions as to how to find a counter-example? Thanks for help.","['matrices', 'linear-algebra']"
3061265,$f: G \to \mathbb{C}^*$ is a homomorphism. Show that the sum $\sum f (g) = 0$ or $n$,"Let $ \mathbb{C}^*$ be the multiplicative group of non-zero complex numbers. Let $G$ be an abelian group and suppose $f: G \to \mathbb{C}^*$ is a homomorphism. Prove that $\sum_{g \in G} f(g)=n$ or, $\sum_{g \in G} f(g)=0$ , where $n  =o(G)$ Proof attempt: The case is evident for the trivial homomorphism; the sum adds up to $n$ . For the second part We know, the only elements with finite order in the group $ \mathbb{C}^*$ are $1$ and $-1$ , with $o(-1)=2$ . Now, the only case when $f(g)$ can take $-1$ as a value is when $n$ is even. Consider the subgroup $(\{1, -1\}, .) = G'$ of the group $ \mathbb{C}^*$ . We have, from the Isomorphism Theorem, $ G/ \ker( f ) \simeq G' $ [since $f$ takes each value from $G'$ ]. As $o(G')=2$ , $o(G/ \ker( f ))=2$ , i.e $o(\ker (f))= n/2$ . Hence, when summed, the resultant is $0$ . Edit: A foolish assumption has been taken. The finite ordered complex numbers in the said group is of the form $z^n=1$ , so I have 'proved' a very restricted case, which is not at all desired.","['group-isomorphism', 'proof-verification', 'finite-groups', 'abstract-algebra', 'group-theory']"
3061280,Subspace topology is the unique topology that satisfies the Characteristic Property,"Lee has as an exercise that the subspace topology is the unique topology that satisfies the characteristic property which is for $S\subset X$ a function $F:Y \to S$ is continuous iff the composition $i_S\circ F$ is continuous. I think that he means that the only topology that this is true is the subspace topology, but I’m not sure how to prove that no other topology satisfies the property. My first question is what needs to be shown in a proof to say that it is the unique topology, and once I know this I should be able to apply it to the exercise.",['general-topology']
3061315,"Can you make the triviality of $\langle a,b,c \mid aba^{-1} = b^2, bcb^{-1} = c^2, cac^{-1} = a^2 \rangle$ more trivial?","I recently learned the following pleasant fact. (It was in the proof of Proposition 3.1 of this paper - but don't worry, there's no model theory in this question.) Let $G$ be a group, and let $a,b,c\in G$ . If $aba^{-1} = b^2$ , $bcb^{-1} = c^2$ , and $cac^{-1} = a^2$ , then $a = b = c = e$ . Put another way, the group defined by generators and relations $\langle a,b,c \mid aba^{-1} = b^2, bcb^{-1} = c^2, cac^{-1} = a^2 \rangle$ is the trivial group. I came up with the following elementary, but ugly, proof: The relations can be rewritten as (1) $ab = b^2a$ , (2) $bc = c^2b$ , (3) $ca = a^2c$ . Using (1), (2), and (3), we can rewrite $a^4bc = a^4c^2b = c^2ab = c^2b^2a$ . But we can also rewrite $a^4bc = b^{16}a^4c = b^{16}ca^2 = c^{2^{16}}b^{16}a^2$ . So $c^{2^{16}}b^{16}a^2 = c^2b^2a$ . This implies $a = b^{-16}c^{2-2^{16}}b^2$ . Substituting for $a$ in (1) above, $b^{-16}c^{2(1-2^{15})}b^3 = b^{-14}c^{2(1-2^{15})}b^2$ , and cancelling from both sides, $c^{2(1-2^{15})}b = b^{2}c^{2(1-2^{15})}$ . But now by (2), we have $bc^{1-2^{15}} = b^{2}c^{2(1-2^{15})}$ , and $b = c^{2^{15}-1}$ . But then $b$ and $c$ commute, so $bcb^{-1} = c^2$ implies $c = c^2$ , and $c = e$ . It then follows easily that $a = b = c = e$ . Question: Is there a better way to see this? i.e. a more abstract proof, or at least one that doesn't involve manipulating words of length $2^{16}$ ?","['alternative-proof', 'group-presentation', 'group-theory', 'abstract-algebra']"
3061320,Probability of more than n machines down any hour?,"Suppose we have $N$ identical machines, at any given hour, there's a chance $P$ that any given machine went down. A down machine takes $T$ hours to recover. How do I calculate the chances that in a given longer interval $Y$ (assume $Y >> T$ ), what are the probability that there exists hour $t$ , $0 < t < Y$ , such that at $t$ there are more than $R$ machines that are down?","['probability-distributions', 'probability']"
3061329,How many maximum triangles can be made?,"There are $8$ points on a plane no three are colinear how many maximum triangles can be made s.t no two triangles have more than one point in common. Now I can choose $3$ points from $8$ points in $^8C_3$ ways and two triangles can have two points in common if I choose $5$ points make $2$ triangles out of it and that can be made in $^8C_6 \times ^6C_3 \times \frac 12$ ways. So the answer should be $^8C_3-(^8C_5 \times ^5C_3\times \frac 12)$ . Am I double counting anything? After seeing one comment and thinking a bit I feel that method of complementation will be harder here and I am thinking about how many ways to draw a triangle instead of maximum how many triangles, So another approach: I can choose three points from $8$ points and draw the 1st triangle then the second triangle can be drawn taking one point from the first(because we are maximizing) and $2$ others from the remaining $5$ points. So we have used $5$ points and drew $2$ triangles. Then we can draw atmost one more triangle. So $3$ is the answer.","['contest-math', 'graph-theory', 'proof-verification', 'combinatorics', 'discrete-mathematics']"
3061345,"Find all solutions $T$ of $x^{2006} T = 0$ in the space of tempered distributions, $\mathcal{S}'(\mathbb{R})$","I'm modeling my solution after this answer to a similar question . This is as far as I've gotten: Every $\phi \in \mathcal{S}(\mathbb{R})$ that vanishes at $0$ can be expressed as $\phi(x) = x \psi (x)$ . Then, $T\phi = xT(\psi) = 0$ by assumption. Fix $\chi \in \mathcal{S}(\mathbb{R})$ such that $\chi(0) = 1$ . Let $T\chi = a$ . Then, for any $\phi \in \mathcal{S}(\mathbb{R})$ , $$T\phi = T(\phi - \phi(0) \chi + \phi(0) \chi) = T(\phi - \phi(0) \chi) + T(\phi(0) \chi).$$ This is where I've gotten stuf. I'm not sure how, in the linked solution, the answer reduces from $T(\phi - \phi(0) \chi) + T(\phi(0) \chi)$ to $0 + a \phi(0)$ (my primary confusion is $T(\phi - \phi(0) \chi)  = 0$ ) nor how to adapt that for my own question. Any suggestions?","['schwartz-space', 'functional-analysis', 'distribution-theory']"
3061389,Dimension of kernel of a operator,"This question is simply applying a theorem. But I do not understand how . One can treat most of the content as black box. I will provide the definitions. The context: I want to show If $M$ is a compact $n$ -dimensional manifold, $P$ an ellipitic differential operator of order $k$ , then $P:W^{k+l} \rightarrow W^l$ has kernel whose dimension is independent of $l$ . Black box terminology explanation: Definition: Let $E_i \rightarrow M$ be two vector bundles, $P:\Gamma(M,E_0) \rightarrow \Gamma(M,E_1)$ is an elliptic differential operator of order $k$ , if locally $P$ can be written as $$ Pf = \sum_{|\alpha| \le k } A^\alpha(y) \frac{\partial^\alpha}{\partial x_\alpha} f(y).$$ Definition 2: Let $E \rightarrow M$ be a complex vector bundle over a compact $n$ -dimensional manifold. Then we can give the space of sections $\Gamma(M,E)$ a Sobolev norm . We denote $W^k$ be the completion of $\Gamma(M,E)$ with respect to this norm. Let us suppose $P:W^{k+l} \rightarrow W^k$ is well defined and: Theorem: Let $Pu=f$ , $f \in W^l$ , $u \in W^r$ for some integer $r$ , then $u \in W^{l+k}$ . It is claimed that then we have the kernel is independent of $l$ . How does this follow? Reference: Pg 48-49.","['vector-bundles', 'sobolev-spaces', 'functional-analysis', 'differential-geometry']"
3061402,Packing regular tetrahedra of edge length 1 with a vertex at the origin in a unit sphere,"I'm currently venturing through Paul Sally's Fundamentals of Mathematical Analysis. This is an unusual textbook in terms of the difficulty of exercises. I've already been stunned by the very first one: How many regular tetrahedra of edge length 1 can be packed inside a unit sphere with each one has a vertex located at the origin? My instinct told me this is highly nontrivial, as I've struggled with it for several weeks. Could somebody give a hint on how to tackle it? Thanks in advance. Edit: As jmerry and Erik Parkinson shows, the solution has been squeezed between $20$ and $22$ . Can further progress be made?","['geometry', 'spherical-geometry', 'packing-problem']"
3061422,Is there any work on partition a partial order set into minimum number total order subsets?,"The problem is what's the minimum number of total order subsets can a partial order set partition into? For example, (1,2) and (3,4) are comparable i.e. (1,2) < (3,4), and (1,2) and (2,1) are incomparable. For the partial order set { (1,2), (3,4), (2,1)} we can partition into two subsets {(1,2), (3,4)} and {(2,1)}, which both are total order sets. I searched on the web and found little related documents. The question is that whether any works or solutions on this problem exist?","['order-theory', 'set-partition', 'discrete-mathematics']"
3061481,Generalized AM-GM Inequality,"I was discussing means with my friend, and I tried to illustrate the concept of geometric mean using the following idea: Suppose we have two positive quantities $x,y>0$ . The simplest geometric object we can make out of those is an $x \times y$ rectangle. What if we want a regular rectangle (i.e., a square) that ""best approximates this rectangle""? One possibility is a square of side length $$\ell_1 =\frac{x+y}{2} ,$$ keeping the perimeter the same at $2x+2y$ . Another candidate is $$\ell_2 =\sqrt{xy} ,$$ this time keeping the area the same at $xy$ . I then realized I can generalize this idea to higher dimensions: If we have three positive numbers $x,y,z>0$ , consider a $x \times y \times z$ rectangle, and a cube whose side $\ell$ is to be decided: Keeping the 1-dimensional ""length-of-the-skeleton"" the same we get $$4x+4y+4z=12 \ell_1 \implies \ell_1=\frac{x+y+z}{3}. $$ Keeping the 2-dimensional area of the faces the same we get $$2xy+2xz+2yz=6\ell_2^2 \implies \ell_2=\sqrt{\frac{xy+xz+yz}{3}}.$$ Keeping the 3-dimensional volume the same we get $$x y z =\ell_3^3  \implies \ell_3=\sqrt[3]{x y z}.$$ Notice that among the usual arithmetic and geometric means, a different kind of mean has popped up. This idea can go further, using "" $n$ -orthotopes"" or hyperrectangles , producing $n$ distinct means from any sequence $x_1,\dots,x_n$ of positive quantities: For $1 \leq d \leq n$ let $e_d(x_1,\dots, x_n)$ denote the elementary symmetric polynomial on $n$ symbols of degree $d$ . We define $$\ell_d(x_1,\dots,x_n) := \sqrt[d]{\frac{e_d(x_1,\dots,x_n)}{\binom{n}{d}}}.$$ I have two questions about this: Is this concept already known? I believe that the AM-GM inequality generalizes to $\ell_1 \geq \ell_2 \geq \cdots \geq \ell_n$ . Is this correct? Thank you!","['average', 'inequality', 'geometry', 'means']"
3061483,"If two spheres are isometric, does there exist a bijective isometry $T:S\to S$ with $\|Tu-\alpha Tv\|_Y \leq \|u-\alpha v\|_X$ for all $\alpha>0?$","Let $$(S,\|\cdot\|) = \{(x,y)\in \mathbb{R}^2: \|(x,y)\| =1\},$$ that is, $S$ is the collection of all norm one vectors in $\mathbb{R}^2$ with respect to the norm $\|\cdot\|.$ Question : Let $\|\cdot\|_X$ and $\|\cdot\|_Y$ be two norms on $\mathbb{R}^2$ be such that $(S,\|\cdot\|_X)$ and $(S,\|\cdot\|_Y)$ are isometric. 
  Does there exist a bijective isometry $T:(S,\|\cdot\|_X)\to (S,\|\cdot\|_Y)$ such that $$\|Tu-\alpha Tv\|_Y \leq \|u-\alpha v\|_X$$ for all $u,v\in (S,\|\cdot\|_X)$ and all $\alpha>0?$ Note that the norms $\|\cdot\|_X$ and $\|\cdot\|_Y$ may be distinct. I tried $\|(x,y)\|_X = |x|+|y|,$ $\|(x,y)\|_Y = \max\{|x|,|y|\}$ and $$T(x,y) = \begin{pmatrix}
1 & 1 \\
-1 & 1
\end{pmatrix}.$$ Note that $T$ is a rotation matrix. 
Clearly $T$ is a bijective isometry and satisfies the inequality. However, I do not know whether the same holds for general $\|\cdot\|_X$ and $\|\cdot\|_Y.$","['banach-spaces', 'normed-spaces', 'real-analysis', 'functional-analysis', 'isometry']"
3061484,Every nonabelian group of order 6 has a non-normal subgroup of order 2 (revisited),"I am fully aware that this question has already been addressed here and here . The question, however, derives from a Dummit and Foote exercise (section 4.2 exercise 10 page 122) and the answers provided make use of material that appears later in the book, or, at least, I do not clearly understand them in terms of the material that I have studied already. So, I would like to submit the following tentative proof
'from first principles' (i.e. Dummit and Foote before page 122). As I feel insecure about it, I would be grateful if you could check it. Consider a nonabelian group G of order 6. By Cauchy's theorem, the group contains at least one element of order 2, and therefore at least one subgroup of order 2. Suppose this/these subgroups are all normal. This would imply that all elements of order 2 commute with all elements of G. The remaining elements of group G are of order 1 (which trivially is in the center of G), or order 3 (order 6 would imply the group is cyclic and therefore abelian). Since all elements of order 2 are in the center, the order 3 elements will commute with them. The following reasoning shows that they also commute with each other. Let $x≠y, |x|=|y|=3$ . $\langle x\rangle$ and $\langle y\rangle$ are normal in $G$ because their index is $2$ . Now $x\{1, y, y^2\}=\{1, y, y^2\}x$ implies, in the nonabelian case, that $x.y=y^2.x$ and $x.y^2=y.x$ .
Likewise, $y\langle x\rangle=\langle x\rangle y$ implies $y.x=x^2.y$ and $y.x^2=x.y$ . Manipulating these equations shows $y.x = x.y$ , so the nonabelian case is impossible. Hence, order 3 elements are in the center of G as well. Therefore, G would be abelian, contrary to the assumption. So, at least one of the subgroups of order 2 should be non-normal.","['group-theory', 'abstract-algebra']"
3061489,Uncountable sum of vectors in a Hilbert Space,"I am currently reading Hilbert Spaces and confused about a thing. Say, $C=\{e_\alpha : \alpha\in\mathcal{A}\}$ be a complete orthonormal set of a Hilbert Space $H$ , possibly uncountable. Is $\sum_{\alpha\in\mathcal{A}}e_\alpha$ well defined ? I think it should be, is there some kind of convergence needed for these sums?","['hilbert-spaces', 'functional-analysis']"
3061540,"If $\sin 5°+\sin 10°+\sin15°+\cdots+\sin 40°=a$, then $\sin 5°+\sin 10°+\sin15°+\cdots+\sin 175°=?$","I'm stuck in this question If $\sin 5°+\sin 10°+\sin15°+\cdots+\sin 40°=a$ $\sin 5°+\sin 10°+\sin15°+\cdots+\sin 175°=?$ I know that, (I asked before) $\sin 5°+\sin 10°+\sin15°+\cdots+\sin 175°=\tan\frac{175}{2}$ But, I didn't catch a hint here.","['contest-math', 'algebra-precalculus', 'trigonometry', 'summation']"
3061560,"Have these ""calculus-based"" consecutive summations been discovered yet?","Consider the sum-of-powers series $$S_i(n) = 1^i + 2^i + \cdots + n^i = \sum_{k=1}^n y_i(k) \quad\text{with}\; y_i(x)=x^i \tag{1}$$ For $i=1, 2, 3$ , it turns out that we can write $$\begin{align}
S_1(n) &= \frac{n+1}{n}\cdot\int_0^{n}y_1(x)\,dx \tag{2a}\\[4pt]
S_2(n) &= \frac{n+1}{n}\cdot\int_0^{n}y_2(x)\,dx \cdot \frac{y_2^\prime(n)+1}{y_2^\prime(n)} \tag{2b}\\[4pt]
S_3(n) &= \left(\frac{n+1}{n}\right)^2\cdot\int_0^{n}y_3(x)\,dx \tag{2c}\\[4pt]
\end{align}$$ which simplify to the well-known formulas $$\begin{align}
S_1(n) &= \frac12 n(n+1) \tag{3a}\\[4pt]
S_2(n) &= \frac16 n(n+1)(2n+1) \tag{3b}\\[4pt]
S_3(n) &= \frac14 n^2 (n + 1)^2 \tag{3c}\\[4pt]
\end{align}$$ The only thing is that I'm not quite sure why these relations can include derivatives and integrals and somehow even work at all - they were simply a result of an extremely tiresome trial and error process. And can this ""template"" be extended to any $S_i(n)$ series or even more unorthodox variants? Credits for the condensation and formatting of my answer goes to Blue (in the comments)","['calculus', 'sequences-and-series']"
3061577,Froebenius norm is unitarily invariant.,"I'm considering the norm defined on matrices by $$\|A\|_F = \sqrt{\sum_{i,j}|a_{ij}|^2}$$ I want to show that it is unitarily invariant, so that for unitary $U$ we have that $$\|UA\|_F = \|A\|_F = \|AU\|_F$$ however I have trouble doing it directly. Writing $\|UA\|_F$ directly I find by Cauchy-Schwarz that $$\|UA\|_F = \sqrt{\sum_{i,j}\left|\sum_{k=1}^{n}u_{ik}a_{kj}\right|^2}= \sqrt{\sum_{i,j}|\langle U_i,\overline{A_j}\rangle|^2}\leq \sqrt{\sum_{i,j}\|A_j\|^2}$$ where $U_i$ denotes the $i$ th row of $U$ and $A_j$ the $j$ th column of $A$ . However this estimate is to crude and will not equal $\|A\|_F$ . I would like to prove this without refering to trace or singular values and would appreciate a hint, rather than a full solution, on how to tackle this problem. EDIT: Completion of the proof based on the answer from $A.\Gamma$ : Since the rows of $U$ constitute an orthonormal basis for $\mathbb{C}^n$ we find by Parsevals theorem that $$\|UA_j\|_2^2 = \sum_{i=1}^{n}\left|\sum_{k=1}^{n}u_{ik}a_{kj}\right|^2 = \sum_{i=1}^{n}|\langle U_i,\overline{A_j}\rangle|^2 = \|\overline{A_j}\|_2^2 = \|A_j\|_2^2$$","['matrices', 'normed-spaces', 'linear-algebra', 'matrix-norms']"
3061614,Why is the Hausdorff property mentioned in the characterization of profinite groups?,"Profinite groups are usually characterized as compact, totally disconnected, Hausdorff groups. However, as shown here , every totally disconnected topological group already has the Hausdorff property. Still, every textbook I've come across explicitly mentions (and even proves) the Hausdorff property in the characterization. Is there any reason whatsoever for this emphasis?","['group-theory', 'profinite-groups', 'topological-groups']"
3061617,Proving the Fibonacci identity $(−1)^{m−k}(F_{m+k+1}F_{m−k−1}−F_{m+k}F_{m−k}) =F_{k}^2+F_{k+1}^2$,"Prove that for two natural numbers $m$ and $k$ , where $m>k$ the following identity holds: $$(−1)^{m−k}(F_{m+k+1}F_{m−k−1}−F_{m+k}F_{m−k}) =F_{k}^2+F_{k+1}^2$$ Here the exercise comes with a hint: The constant is $F^2 _m$ , try to substitute $k=0$ in. Okay fair enough, let us try this, this will give us: $$ (−1)^{m}(F_{m+1}F_{m−1}−F_{m}F_{m}) =F_{0}^2+F_{1}^2$$ $$ (−1)^{m}(F_{m+1}F_{m−1}−F_{m}^2) =F_{0}^2+F_{1}^2=0^2+1^2=1$$ We can now use a identity by Cassini, namely that for $m>0=k$ we have: $$ (−1)^{m}(F_{m+1}F_{m−1}−F_{m}^2) =(-1)^m(-1)^m=(-1)^{2m}=1 \checkmark$$ Okay great, so our identity works whenever $k=0$ , but I do not see how this helps with a generalisation. Edit/idea: Did I just write down the base case for an approach via induction on $k$ perhaps?","['fibonacci-numbers', 'recurrence-relations', 'discrete-mathematics', 'sequences-and-series', 'induction']"
3061638,The difference between applying a rotation matrix to a vector (points) and to a matrix (transformation),"Suppose that the rotation matrix is defined as $\mathbf{R}$ . Then in order to rotate a vector and a matrix, the following expressions are, respectively, used $\mathbf{u'}=\mathbf{R} \mathbf{u}$ and $\mathbf{U'}=\mathbf{R} \mathbf{U} \mathbf{R}^T$ , where $\mathbf{u}$ and $\mathbf{U}$ are, respectively, an arbitrary vector and an arbitrary matrix. For me, the first one is obvious since you simply multiply the rotation matrix by the vector (for example a point coordinate in 3D) and obtain the rotated vector (rotated point coordinate in 3D). However, the second one is not clear for me and why the rotation should be multiplied from both sides and how this expression is derived. P.S. The matrix $\mathbf{U}$ can be interpreted as a stretch matrix in 3D.","['matrices', 'linear-algebra', 'vectors', 'rotations']"
3061670,Proof that a sequence is Cauchy.,"Show that $\left(  x_{n}\right)  $ is a Cauchy sequence, where $$
x_{n}=\frac{\sin1}{2}+\frac{\sin2}{2^{2}}+\ldots+\frac{\sin n}{2^{n}}.
$$ We try to evaluate $\left\vert x_{n+p}-x_{n}\right\vert $ and to show that
this one is arbitrary small. So \begin{align*}
\left\vert a_{n+p}-a_{n}\right\vert  & =\left\vert \frac{\sin\left(
n+1\right)  }{2^{n+1}}+\frac{\sin\left(  n+2\right)  }{2^{n+2}}+\ldots
+\frac{\sin\left(  n+p\right)  }{2^{n+p}}\right\vert \\
& \leq\frac{1}{2^{n+1}}+\frac{1}{2^{n+2}}+\ldots+\frac{1}{2^{n+p}}.
\end{align*} Now, $\left(  \frac{1}{2^{n+j}}\right)  $ converge all to zero. Hence $\left\vert a_{n+p}-a_{n}\right\vert \rightarrow0$ . 
Does it work this argument according to the definition of Cauchy sequence $$
\forall\varepsilon>0,
\quad
\exists n_{\varepsilon}\in
\mathbb{N}
,
\quad
\forall n\in
\mathbb{N}
,
\quad
\forall p\in
\mathbb{N}
:n,p\geq n_{\varepsilon}\Rightarrow\left\vert a_{n+p}-a_{n}\right\vert
<\varepsilon?
$$ My question was born from the fact that for the sequence $$
a_{n}=1+\frac{1}{2}+\ldots+\frac{1}{n},
$$ we have that \begin{align*}
\left\vert a_{n+p}-a_{n}\right\vert  & =\left\vert \frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{n+p}\right\vert \\
& \leq\frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{n+p}
\end{align*} and also all sequences $\left(  \frac{1}{n+j}\right)  _{n}$ converge to zero,
but this time $\left(  a_{n}\right)  $ is not a Cauchy sequence.",['analysis']
3061743,Kolmogorov extension theorem for measures on the space of continuous functions,"In a text I am reading occurs the following: One has a family of probability measures $(P_T)_{T\geq 0}$ such that $P_T$ is a measure on the measurable space $(C([0,T],\Bbb R^d), \mathcal B (C([0,T],\Bbb R^d)) )$ . It is stated that consistency of the family $(P_T)_{T\geq 0}$ is yielding a measure $P$ on $C([0,\infty),\Bbb R^d)$ . I think consistency means if we take $\pi^S_T: C([0,S],\Bbb R^d) \to C([0,T],\Bbb R^d)$ , $f\mapsto f\vert_{[0,T]}$ , then for every $S\geq T$ we have $P_T = P_S \circ (\pi_T^S)^{-1}$ . P seems to satisfy $P_T = P\circ(\pi_T)^{-1}$ , where $\pi_T: C([0,\infty),\Bbb R^d) \to C([0,T],\Bbb R^d)$ , $f\mapsto f\vert_{[0,T]}$ How is this done? My first thought was to apply the Kolmogorov extension theorem but I did not come far enough with it.","['stochastic-processes', 'measure-theory', 'probability-theory']"
3061753,Evaluate $\sum_{n=2}^\infty \frac{\ln(n)(-1)^n}{n^2} $,"I'm trying to find a closed form for the sum: $$\sum_{n=2}^\infty \frac{\ln(n)(-1)^n}{n^2} $$ It's been proven, $$\sum_{n=2}^\infty \frac{\ln(n)(-1)^n}{n} = \gamma\ln(2)-\frac{\ln(2)^2}{2}$$ Since they're similar, I feel there potentially could be a closed form solution of the first one. I want to know if there is also a closed form solution for the first summation. I managed to rewrite the first sum as a double integral: $$-\int_0^1\int_0^1 \frac{\ln(x)}{\ln(y)}\left(\frac{1}{1+x}-\frac{1}{1+xy} \right)dxdy $$ But I don't see any overt way of evaluating it. I've tried differentiating under the integral, but to no avail. I can't think of any change of variables that would help simplify as well.","['integration', 'calculus', 'summation', 'sequences-and-series']"
3061759,First and second differentiability of the piecewise function $x^4\sin(\frac{1}{x})$ if $x \neq 0$ and $0$ if $x=0$,"I have the following function $f(x)$ defined as $x^4\sin(\frac{1}{x})$ if $x \neq 0$ and $0$ if $x=0$ . And I'm asked if the function is: a) differentiable b) two times differentiable c) two times continuously differentiable This function is of course quite similar to the function $x^2\sin(\frac{1}{x})$ , which can be found on the internet for the same type of exercises. But I still want to be sure that I proceeded in the correct way, because I have no solution to this exercise. Thanks for your feedback. a) At $x=0$ , we have $$\lim_{h\to 0}\frac{f(0+h)-f(0)}{h}=\lim_{h \to 0}\frac{h^4\sin(\frac{1}{h})}{h}=\lim_{h \to 0}h^3\sin(\frac{1}{h})=0$$ Everywhere else, we have simply $4x^3 \sin(\frac{1}{x})-x^2\cos(\frac{1}{x})$ . Thus the function is differentiable everywhere, even at $0$ where its value is $0$ . b) Again, if $x\neq 0$ , we have simply $f^{\prime \prime}(x)=12x^2\sin(\frac{1}{x})-\sin(\frac{1}{x})-6x\cos(\frac{1}{x})$ . At $x=0$ , we have $$\lim_{h \to 0}\frac{f'(0+h)-f'(0)}{h}=\lim_{h \to 0}\frac{4h^3\sin(\frac{1}{h})-h^2\cos(\frac{1}{h})}{h}=\lim_{h \to 0}4h^2\sin(\frac{1}{h})-h\cos(\frac{1}{h})=0$$ So the  function is also two times differentiable at $0$ , where the derivative is $0$ . c) The function is however not two times continuously differentiable, because as said, the second derivative at $0$ is $0$ , but if we take $\lim_{x \to 0_{\pm}} 12x^2\sin(\frac{1}{x})-\sin(\frac{1}{x})-6x\cos(\frac{1}{x})$ , we don't always reach $0$ because $\sin(\frac{1}{x})$ oscillates between $-1$ and $1$ . So the function is not two times continuously differentiable, even if the second derivative is defined everywhere, even at $0$ Are my results correct ? Thanks for your help !","['real-analysis', 'calculus', 'limits', 'derivatives', 'piecewise-continuity']"
3061827,Compute $\int_0^{\pi} \frac{\cos(nx)\cos(x) - \cos(nt)\cos(t)}{\cos(x) -\cos(t)}dt$,"Let $n\in\mathbb{N}$ and $x\in]0,\pi[$ , I am asked to calculate the following : $$ I_n = \int_0^{\pi} \dfrac{\cos(nx)\cos(x) - \cos(nt)\cos(t)}{\cos(x) -\cos(t)}dt$$ From testing on small values of $n$ , it seems that this integral is equal to $n\pi\cdot \cos^n(x)$ but I can't seem to prove it. I tried finding a recurrence formula but didn't succeed. Here is my working for $n=0$ , $n=1$ and $n=2$ :
For $n=0$ , $$ I_0=\int_0^{\pi}\dfrac{\cos(x) -\cos(t)}{\cos(x) - \cos(t)}dt = \pi$$ For $n=1$ , $$ I_1 = \int_0^{\pi} \dfrac{\cos^2(x) -\cos^2(t)}{\cos(x)-\cos(t)}dt=\int_0^{\pi}\cos(x) + \sin(t)dt = \pi\cdot \cos(x)$$ For $n=2$ : $$ I_2 = \int_0^{\pi} \dfrac{2\cos^3(x) - 2\cos^3(t) -\cos(x) + \cos(t)}{\cos(x) - \cos(t)}dt$$ $$ I_2 = 2\int_0^{\pi}\cos^2(x) +\cos(x)\cos(t) + cos^2(t) dt - \pi$$ $$ I_2 = 2\pi\cos^2(x) + \int_0^{\pi}\cos(2t)+1dt - \pi$$ $$ I_2 = 2\pi\cos^2(x) $$ This is my first post here, please tell me if I did anything wrong. I tried searching this integral on this website without any success.","['integration', 'definite-integrals']"
3061834,Combinations of red and black balls,"Given $N$ Identical Red balls and $M$ Identical Black balls, in how many ways we can arrange them such that not more than $K$ adjacent balls are of same color. Example : For $1$ Red ball and $1$ black ball, with $K=1$ , there are $2$ ways $[RB,BR]$ Can there be a general formula for given $N$ , $M$ and $K$ ? I have read about Dutch flag problem to find number of ways to find such that no adjacent balls are of same color. I am bit stuck on how to find for at max K balls.","['number-theory', 'combinatorics', 'dynamic-programming', 'algorithms']"
3061871,What are the conditions on $\text{tr}(AB) \leq \text{tr(A)} \text{tr(B)}$ to be true?,Let $A$ and $B$ be two arbitrary matrix with proper dimension for multiplication. Consider this trace inequlaty which is trace of multiplication of two matrices versus their individual traces $$\text{tr}(AB) \leq \text{tr(A)}  \text{tr(B)}$$ 1- Do we have result for rectangular matrix that satisfy this inequality? 2- If they were square matrices what are the conditions? 3- Is there any specific name for this inequality?,"['matrices', 'trace', 'linear-algebra']"
3061936,"Solving linear ordinary, 2nd order differential equations via global integral bases.","Consider a linear, homogenous  2nd order ODE: \begin{equation}
L\left[y(x)\right] = \left[\frac{d^2}{d x^2} + a_1(x) \frac{d}{d x} + a_0(x)\right] y(x)=0
\end{equation} In https://arxiv.org/pdf/1606.01576.pdf the authors present an interesting heuristic algorithm for finding closed form solutions of those ODEs if such solutions exist. The algorithm can be summarized as follows: For all regular singular points of the ODE in question find exponents and power series expansions of solutions to a given order. If the indicial equation is degenerate (which means the second solution cannot be found) find that second solution via the Wronskian method http://mathworld.wolfram.com/Second-OrderOrdinaryDifferentialEquationSecondSolution.html .
Note that the solutions in question will be of the form: \begin{equation}
y_p(x):= x^{\nu_p} \sum\limits_{i=0}^\infty \left( a_i^{(0)} + a_i^{(1)} \log(x) \right)\cdot  x^i
\end{equation} for $p=1,2$ . Having found the solutions in question (meaning of course series expansions being truncated to some given order) find a pair of differential operators $({\mathcal B}_0, {\mathcal B}_1)$ that are globally integral. A precise definition of that notion is given in the paper above and in references therein however for our purposes it amounts to point out the following intuitive meaning. An operator is globally integral if its action on any of the solutions yields a series expansion that comprises nonnegative powers of both $x$ and $\log(x)$ only. It turns out (see the paper for details)  that such operators have the following form: \begin{equation}
{\mathcal B}_p= r_0(x) + r_1(x) \cdot \frac{d}{d x}
\end{equation} for $p=1,2$ . Choose one of such operators, say ${\mathcal B}_1$ and then construct another differential operator ${\tilde L}$ such that \begin{equation}
{\tilde L}\left[{\mathcal B}_1 y_p(x)\right] = 0
\end{equation} Note that this new operator ${\tilde L}$ is also w linear 2nd order operator.
Now one would naively expect that this new operator will be more complicated than the original operator $L$ . This is indeed the case in general. However if it just happens that $L$ has closed form solutions in terms of special functions then the new operator will be much simpler and will allow us to find those closed form solutions. Now, I have tested this algorithm on the family of ODEs specified in the first example in my answer to Gauge transformation of differential equations I . That family is a five parameter family . I have therefore generated a random five tuple of such parameters by sampling from rational numbers then for such five tuple I have generated the operator $L$ and then found its global integral basis which I then used to find the new operator $\tilde{L}$ .
To my surprise it turns out that the new operator ${\tilde L}$ was always much simpler than the original one and I could guess that it corresponds to an abscissa transformed Gaussian hypergeometric operator. Below I enclose a code snippet that I used: In[929]:= x =.; Dx =.; A =.; B =.; CC =.;
order = 4;

AA = RandomInteger[{1, 10}];
{A2, A1, B2} = Rationalize[RandomReal[{-5, 5}, 3], 1/10];
{A, B, CC} = {1/7, 1/3, 1/2};
{L, mysubst} = GenerateODE[A2, A1, B2, A, B, CC, AA]
g = GlobalIntegralBasis[L];
g1 = NormalizeAtInfinity[L, g]
myStandardForm[L, g1[[1]]]

L00 = ChangeOfVars[{1, (CC - (A + B + 1) x)/(
    x (1 - x)), -((A B)/(x (1 - x)))}, (AA x)^2];
Factor[L00/L00[[1]]]


Out[934]= {{1, -((
   4 (37632 - 52920 x - 3488051 x^2 - 1014720 x^3 + 28451380 x^4 + 
      12779520 x^5))/(
   21 x (-1 + 4 x) (1 + 4 x) (-3584 + 3360 x + 165415 x^2 + 
      49920 x^3))), (
  2 (10752 - 15120 x - 1066847 x^2 - 577920 x^3 + 6640000 x^4 + 
     4792320 x^5))/(
  3 x^2 (-1 + 4 x) (1 + 4 x) (-3584 + 3360 x + 165415 x^2 + 
     49920 x^3))}, 
 b^n $34032_ :> (14 b^(-3 + n$ 34032))/195 - (7 b^(-2 + n $34032))/
104 - (33083 b^(-1 + n$ 34032))/9984 /; n$34032 >= 3}

Out[936]= {{-((
   112 ((80 - 15 x - 1792 x^2) f[x] + 
      48 x (-1 + 16 x^2) Derivative[1][f][x]))/(
   x (-3584 + 3360 x + 165415 x^2 + 49920 x^3))), ((1792 + 560 x - 
      11763 x^2 + 16640 x^3) f[x] + 
   1792 x (-1 + 16 x^2) Derivative[1][f][x])/(
  1792 x^2 (-3584 + 3360 x + 165415 x^2 + 49920 x^3))}, {0, 1}}

Out[937]= {1, (656 x)/(21 (-1 + 4 x) (1 + 4 x)), 64/(
 21 (-1 + 4 x) (1 + 4 x))}

Out[939]= {1, (656 x)/(21 (-1 + 4 x) (1 + 4 x)), 64/(
 21 (-1 + 4 x) (1 + 4 x))} As I said before for the randomly sampled parameter five tuple the code produces firstly the operator $L$ (Out[934]) then it produces the global integral basis (Out[936]) and finally it produces the new operator ${\tilde L}$ (Out[937]) along with an abscissa transformed Gaussian hypergeometric operator (Out[939]). I have played with this code and it was always that the last two lines of the output were the same, meaning Out[937] was the same as Out[939] . This means that the algorithm is always able to solve the ODE in question provided of course it has a closed form solution which in this cases it always has. I have also run the algorithm on other ODEs which do not have a closed form solution and in there the algo has failed as expected. Now having said all this my question would be as follows. Why does this algorithm work? Could anyone enlighten me why an integral basis should be useful as a means for transforming a complicated ODE into a simple one?
Can we prove that this algorithm is going to work if a closed form solution exists?","['closed-form', 'special-functions', 'ordinary-differential-equations']"
3061978,Proof of a.s. uniqueness of probability convergence limit [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Prove that the limit of a random variable sequence (converges in probability) is almost surely uniuque. Let $X, Y$ random variables and let a random variable sequence $(X_n)_n$ such that $X_n$ converges in probability to $X$ and $X_n$ converges in probability to $Y$ (i.e. $X_n \overset{P}{\rightarrow} X, X_n\overset{P}{\rightarrow} Y$ ). Prove that $\mathbb{P}(X=Y)=1$ .","['limits', 'proof-verification', 'probability']"
3061990,What is the Gelfand-Naimark representation of functions that don't vanish at infinity?,"The Gelfand-Naimark theorem says that every commutative C*-algebra is isometrically isomorphic to $C_0(X)$ , the set of continuous functions $f:X\rightarrow\mathbb{C}$ that vanish at infinity, for some locally compact Hausdorff space $X$ . What happens when we apply this construction to a commutative C*-algebra that looks almost like $C_0(X)$ , but we relax one of the requirements? For example, we can consider the C*-algebra $C_b(Y)$ , the set of bounded continuous functions $f:Y\rightarrow\mathbb{C}$ , with no requirement they vanish at infinity. Is there any relation between the the space $Y$ of the original C*-algebra and the space $X$ that we generate using Gelfand-Naimark? Bonus followup: What if Y is not Hausdorff? Or not locally compact?","['abstract-algebra', 'functional-analysis', 'algebras', 'gelfand-representation']"
3062001,Identity Involving Lie Derivative and Local Flows,"I'm trying to show, $$ \frac{d}{dt} \varphi_t^* \omega = \varphi_t^* \left( \mathcal{L}_{X_t} \omega \right)$$ but I have another question as well. Every case in which the lie derivative is mentioned, that notation $\mathcal{L}_{X_t}$ has never been given interpretation and so I would like to understand this notation first. Given $\frac{d}{dt}\bigr|_{t=0} \varphi_t = X_p = X_{\varphi_0(p)}$ and so $\frac{d}{dt} \varphi_t = X_{\varphi_t(p)}$ which we can call $X_t$ . And so, $$\mathcal{L}_{X_t} = \mathcal{L}_{\frac{d}{dt} \varphi_t}$$ Is this correct? Update: attempt to prove identity","['differential-topology', 'symplectic-geometry', 'differential-geometry']"
3062013,Example of sheaf on $\mathrm{Ring}$ that does not come from $\mathrm{Sch}$.,"At the end of Remarque 2.3.6 (p. 221-222) of EGA I, the author says that there are functors in $\mathbf{Fais}|_{\mathbf{Ann}}$ (sheaf on the category of Rings) that are not isomorphic to sheaves that come from schemes. I would like to know one such example or if such example is constructed later on the book. I'm adding the definition and context of each concept below: A functor $G:\mathbf{Aff}^{op}\to\mathbf{Set}$ from the opposite category of affine schemes to the category of sets is called a presheaf . Given an affine scheme $X$ , for any open subscheme $U$ , one can consider the map $U\mapsto G(U)$ . We say that $G$ is a sheaf when this map is always a sheaf in the usual sense. Since there exist an equivalence of categories $F:\mathbf{Aff}^{op}\to\mathbf{Ring}$ between the category of affine schmes and the category of rings, that also defines an equivalence $\mathbf{Hom(Aff^{op},Set)}\cong\mathbf{Hom(Ring,Set)}$ . Hece we can define a sheaf on the category of rings as a (covariant) functor $\mathbf{Ring}\to\mathbf{Set}$ whose image under the previous equivalence is a sheaf in the sense defined earlier. Similarily we can define a sheaf on the category of schemes $\mathbf{Sch}$ , but it turns out that the category of such sheaves is equivalent to that of sheaves on affine schemes. One can prove that, given an scheme $X$ , the functor $h_X:Y\mapsto\mathrm{Hom}(Y,X)$ is a sheaf on $\mathbf{Sch}$ , and since $h:X\mapsto h_X$ is fully faithful, we can identify the category of schemes with a subcategory of the sheaves on $\mathbf{Ring}$ by the previous equivalences.","['algebraic-geometry', 'examples-counterexamples', 'sheaf-theory']"
3062029,Minimum distance between sequence a and all permutations of another sequence b,"Let $a$ and $b$ be finite sequences of length $n$ , i.e. $a=(a_1,a_2,...,a_n), b=(b_1,b_2,...,b_n)$ . I want to calculate the minimum of the distances (in an Lp norm) between $a$ and all permutations of $b$ . Let $\Pi_n$ be the space of all permutations of $n$ numbers, then I am looking for $d = \text{min}_{\pi\in\Pi_n} ||a - (b_{\pi(1)},b_{\pi(2)},...,b_{\pi(n)})||_p$ Is there an efficient method or optimization algorithm (something better than trying out all $n!$ permutations of b) to calculate $d$ and find the corresponding permutation $\pi$ ?","['combinations', 'linear-algebra', 'combinatorics', 'discrete-optimization', 'optimization']"
3062054,How to compute $\mathbb{E}[X_{s}^{2}e^{\lambda X_{s}}]$ where $(X_s)$ is a Brownian motion with drift $\mu$?,"I'm working on a problem and at a certain point I ran into the problem as described in the title. We have that $\{W_t,t\geq 0\}$ is a Brownian motion and $\mathscr{F}_t$ is the corresponding filtration. We have that $\mu>0$ is given in the process $\{X_t,t\geq 0\}$ defined via $X_t:=\mu t+W_t$ . I don't want to post the full problem I was solving yet, rather I'd like to know if what I ended up with is even solvable, because if not, I'll know I'm definitely wrong. As posted in the title, I came at a point where I was left to compute the expectation: $\mathbb{E}[X_{s}^{2}e^{\lambda X_{s}}]$ Earlier in the exercise (it consisted of multiple parts) I used the moment generating function for the normal distribution. However, as far as I know, I cannot take the $X_{s}^{2}$ out of the expectation, stopping me from applying the moment generating function. Is this expectation solvable in a relatively easy way? If not, I'll know I'm wrong and start over.","['expected-value', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
