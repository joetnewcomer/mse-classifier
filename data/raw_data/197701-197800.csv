question_id,title,body,tags
3817911,Why the following function is closed?,"Let $X=[0,1] \cup [2,3]$ and let $Y=[0,2]$ . I have $p: X \to Y$ defined by $$p(x)= 
\begin{cases}
x  \quad &\text{for} \ x \in [0,1] \\
x-1 \quad &\text{for} \ x \in [2,3] 
\end{cases}$$ I have to prove that $p$ is a closed map. I tried a lot of times, but I didn't get it. And I have to prove it without compactness","['closed-map', 'general-topology']"
3817965,Mid-range via minimax,"Warning: crossposted at Statistics SE. Given vector ${\rm a} \in \Bbb R^n$ , $$\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_2^2\end{array} = \frac1n {\Bbb 1}_n^\top {\rm a} \tag{mean}$$ is the (arithmetic) mean of the entries of vector ${\rm a}  \in \Bbb R^n$ , whereas $$\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_1\end{array} \tag{median}$$ is a median of the entries of vector ${\rm a}  \in \Bbb R^n$ . Using the $\infty$ -norm instead, what is the following? $$\color{blue}{\boxed{\,\\\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array}}}$$ It appears to be the mid-range . I append a proof based on linear programming. Assuming that I have made no mistakes and my proof is indeed correct, I am interested in other proofs and in references . My proof $$\begin{array}{ll} \underset{x \in {\Bbb R}}{\text{minimize}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array} $$ Introducing optimization variable $y \in {\Bbb R}$ , $$\begin{array}{ll} \underset{x, y \in {\Bbb R}}{\text{minimize}} & \qquad\qquad y\\ \text{subject to} & -y {\Bbb 1}_n \leq x {\Bbb 1}_n - {\rm a} \leq y {\Bbb 1}_n\end{array} $$ or, alternatively, $$\begin{array}{lrl} \underset{x, y \in {\Bbb R}}{\text{minimize}} & y & \\ \text{subject to} & {\rm a} & \leq (x + y) {\Bbb 1}_n \\ & (x - y) {\Bbb 1}_n & \leq {\rm a}\end{array}$$ Let the entries of vector ${\rm a} \in \Bbb R^n$ be denoted by $a_1, a_2, \dots, a_n$ . Note that there are many redundant inequalities: the set of $n$ inequalities ${\rm a} \leq (x + y) {\Bbb 1}_n$ can be replaced by $$x + y \geq \max \{ a_1, a_2, \dots, a_n \}$$ the set of $n$ inequalities $(x - y) {\Bbb 1}_n \leq {\rm a}$ can be replaced by $$x - y \leq \min \{ a_1, a_2, \dots, a_n \}$$ Subtracting the latter inequality from the former inequality, $$y \geq \frac{ \max \{ a_1, a_2, \dots, a_n \} - \min \{ a_1, a_2, \dots, a_n \} }{2} =: y_{\min}$$ and, thus, $$ \begin{aligned} x &\geq \max \{ a_1, a_2, \dots, a_n \} - y_{\min} = \frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2} \\\\ x &\leq \min \{ a_1, a_2, \dots, a_n \} + y_{\min} = \frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2} \end{aligned} $$ Hence, $$\begin{array}{ll} \displaystyle\arg\min_{x \in {\Bbb R}} & \left\| x {\Bbb 1}_n - {\rm a} \right\|_{\infty}\end{array} = \color{blue}{\frac{ \min \{ a_1, a_2, \dots, a_n \} + \max \{ a_1, a_2, \dots, a_n \} }{2}}$$ Some call this value the mid-range of $\{ a_1, a_2, \dots, a_n \}$ . Related Centroid under the Chebyshev distance The median minimizes the sum of absolute deviations (the $ {\ell}_{1} $ norm) Average of minimum and maximum in set What minimizes the Chebyshev Distance? Term for center of dataset","['statistics', 'central-tendency', 'linear-programming', 'reference-request', 'optimization']"
3817970,Optimal stopping strategy to capture Feebas in Pokemon Emerald,"Some exposition for those who may not be too familiar with the game: A Feebas is a fish-like pokemon which only appears in one location in the entire game - Route 119. To be specific, it can be encountered only on the waters of Route 119, by fishing. There are 400 water ""tiles"" in Route 119. Among these 400, Feebas can appear in exactly 6 specific tiles, with a probability of 0.5. These tiles are randomly chosen when a new game is initialized, and their locations are unknown to the player. If a player can determine that a particular tile contains Feebas however, he can always find Feebas in that specific tile with a 50% probability. According to Bulbapedia , if a player fishes exactly once in each of the 400 tiles, he approximately has a 1.6% chance of not finding any Feebas (or alternatively, a 98.4% chance of finding at least one). This calculation is fairly simple - the probability of finding at least one Feebas by searching just once (S = 1) in each tile (T = 400) is: $P(F>=1|T=400,S=1) = 1 - 0.5^6 = 0.984375$ While this is great, this approach requires a player to make 400 search operations. In fact, if the player fishes twice in each tile, at the cost of 800 operations, his probability of finding at least one fish goes up to: $P(F>=1|T=400,S=2) = 1 - (0.5^2)^6 = 0.999755$ So I was wondering if this problem can be approached a bit differently, maybe from the perspective of optimal stopping problems? What would be a good strategy to minimize the number of search operations/tiles explored, while retaining a reasonable probability $P >= 0.9$ of finding at least one Feebas? Last tidbit: it is actually possible for the player to manually reset the locations of the Feebas into 6 new random unknown positions. (This can perhaps facilitate a different class of strategies, like searching only in 40 tiles, and then resetting the random 6 positions, again trying in those 40 tiles and so on. Not sure how good or bad this would be - just putting it out there.)","['optimization', 'probability', 'stochastic-processes']"
3817994,Combinatorial Proof of Stirling Number Inequality,"Here's a cute inequality for unsigned Stirling numbers of the first kind: $$\genfrac[]{0pt}{}{n}{n-k}\leq\frac{n^{2k}}{2^kk!}.$$ I can prove it using induction (with a beautiful application of AM-GM, see below), but is there a combinatorial proof? Here's the core of the induction proof: $$\begin{align*}
\genfrac[]{0pt}{}{n}{n-k}&=(n-1)\genfrac[]{0pt}{}{n-1}{n-k}+\genfrac[]{0pt}{}{n-1}{n-k-1}\\
    &=(n-1)\genfrac[]{0pt}{}{n-1}{(n-1)-(k-1)}+\genfrac[]{0pt}{}{n-1}{(n-1)-k}\\
    &\leq(n-1)\frac{(n-1)^{2(k-1)}}{2^{k-1}(k-1)!}+\frac{(n-1)^{2k}}{2^kk!}\\
    &=\frac{1}{2^kk!}(2k+n-1)(n-1)^{2k-1}\\
    &\leq\frac{1}{2^kk!}\left(\frac{(2k+n-1)+(2k-1)(n-1)}{2k}\right)^{2k}\\
    &=\frac{n^{2k}}{2^kk!}
\end{align*}$$ where the last inequality (the penultimate step) uses the AM-GM inequality.
I find it really beautiful how the AM-GM inequality works perfectly here with no further estimates needed.","['inequality', 'combinatorics', 'stirling-numbers', 'combinatorial-proofs']"
3818000,Orthogonal and Orthonormal Matrix,"I know that the columns of an orthogonal matrix are perpendicular to each other and additionally if the columns have unit length then they are orthonormal.
But my professor states that the columns of an orthogonal matrix form an orthonormal basis? Is this right? Then what is the difference between orthogonal and orthonormal matrix?",['linear-algebra']
3818066,Quotient rule for multivariable functions,"$\newcommand{\mbf}{\mathbf}$ Let $f,g:\mathbb R^n\to \mathbb R$ be differentiable at $a$ . $Df(a)$ is the unique linear transformation $\mathbb R^n\to\mathbb  R$ such that $$\lim_{\mbf h\to\mbf 0}\frac{|f(\mbf a+\mbf h)-\mbf f(\mbf a)-Df(\mbf a)(\mbf h)|}{|\mbf h|}=0.$$ I want to show that if $g(a)\neq 0$ , then $D(f/g)(a)=\dfrac{g(a)Df(a)-f(a)Dg(a)}{[g(a)]^2}$ . I've shown that $D(f\cdot g)(a)=g(a)Df(a)+f(a)Dg(a)$ . Let $h:\mathbb R^n\to\mathbb R$ be defined by $h(x)=\frac{1}{g(x)}$ . Then $$D(f/g)(a)=D(f\cdot h)(a)=h(a)Df(a)+f(a)Dh(a).$$ Since $h=q\circ g$ where $q:\mathbb R-\{0\}\to\mathbb R$ is defined by $q(x)=1/x$ , $$Dh(a)=D(q\circ g)(a)=Dq(g(a))\circ Dg(a)=D\frac1{g(a)}\circ Dg(a)$$ How can I simplify $D\frac 1{g(x)}$ to prove this?","['multivariable-calculus', 'calculus', 'derivatives', 'real-analysis']"
3818087,Gram-Schmidt method to get a basis for $P_3$,"If $P_3$ is a vector space of third-degree polynomials.
It is known the basis for $P_3$ is ${( 1,x,x^2 , x^3})$ and $\langle p, q\rangle = \int_{0}^{1} p(x)q(x)\, dx.$ is a valid product on $P_3$ I am trying to use the Gram-Schmidt method to get a basis for $P_3$ which is orthonormal with respect to the
above inner product. Even though I found partial solutions or similar problems the explanations are limited. PS. I read the rules before posting my first question. Even though I found similar problems  I didn't understand entirely the method and calculations. Additional Sources the below exercise which has a partial solution, but I am not sure how to calculate the remaining values. this question which is similar but in $P_2$ Finding an orthonormal basis for the space $P_2$ with respect to a given inner product I hope I did not violate any rule. It was my last hope to ask here since due to current conditions I can't ask my Teacher face to face.","['self-learning', 'gram-schmidt', 'linear-algebra']"
3818096,Is there any intuition or meaning regarding Cauchy-Riemann equations?,"Holomorphic functions are one of the most beautiful objects in mathematics. However, Cauchy-Riemann equations are a little bit of a mystery for me. What does that mean intuitively when a function satisfyes those equations?",['complex-analysis']
3818138,A real analysis qualifying exam problem,I was doing a real analysis problem set when I get stuck on this problem. $1<p<\infty$ $f\in L^p(\mathbb{R})$ $\alpha>1-\frac1p$ show that $$\sum_{n=1}^\infty \int_{n}^{n+n^{-\alpha}}|f(x+y)|dy<\infty$$ for a.e. $x\in\mathbb{R}$ . I have tried integrating the series w.r.t. $x$ on a bounded interval and try to show that it is finite using Holder inequality. But it doesn't give the things I want. Any hint or suggestion on it?,"['measure-theory', 'real-analysis']"
3818222,Finding diameter of a circle,"How can I find the diameter of a circle that's been rolled up to a wall when the circle is touching a rectangle in the corner of the wall with height $8$ and width $5$ ? Here's a picture of what I mean: I tried drawing a right triangles in several different areas, but none of them helped me out. I'm pretty sure that's the right approach, but I still can't figure out this problem.","['analytic-geometry', 'circles', 'geometry']"
3818246,"Nontrivial lower-bound for $\lim\sup_n P(|X_n| / (Y_n^2+Z_n^2) \le t) $ if $X_n,Y_n,Z_n \to N(0,1)$ in distribution","Let $(X_n)_n$ , $(Y_n)_n$ , $(Z_n)_n$ be sequence of random variables (no assumptions of independence whatsoever). Suppose that $Y_n \to N(0, 1)$ , $Y_n \to N(0, 1)$ , and $Z_n \to N(0,1)$ in distribution. Let $\epsilon \ge 0$ . Question. What is a nontrivial lower-bound for $p := \lim\sup_n P(|X_n| / (Y_n^2+Z_n^2) \le \epsilon)$ ? Note. If I had $Y_n \to y$ and $Z_n \to z$ in probability (for constants $y,z \in \mathbb R$ ), then I could use Slutsky's theorem to get $p = P(|N(0,1)| \le \epsilon)$ , and then concentration arguments would give $p \ge 1 - 2e^{-\epsilon^2/2}$ .","['statistics', 'concentration-of-measure', 'law-of-large-numbers', 'probability-theory', 'probability']"
3818277,Calculate the gradient of a linear scalar field [duplicate],"This question already has answers here : Gradient of $a^T X b$ with respect to $X$ (3 answers) Derivation of $\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T$ [duplicate] (2 answers) Closed 3 years ago . I am trying to calculate the following gradient $$\nabla_{\mathbf{X}} \left( \mathbf{a}^{T} \mathbf{X} \mathbf{a} \right)$$ where I am using the convention that $\mathbf{a}$ is a column vector. I am wondering what the steps are to extract the solution from the matrix cookbook, which is: $$\nabla_{\mathbf{X}} \left( \mathbf{a}^{T} \mathbf{X} \mathbf{a} \right) = \mathbf{a}\cdot\mathbf{a}^{T}$$","['scalar-fields', 'matrices', 'multivariable-calculus', 'matrix-calculus', 'derivatives']"
3818285,Players and tickets,"You are among N players that will play a competition. A lottery is used to determine the placement of each player. You have an advantage. Two tickets with your name are put in a hat, while for each of the other players only one ticket with her/his name is put in the hat. The hat is well shaken and tickets are drawn one by one from the hat. The order of names appearing determines the placement of each player. What is the probability that you will get assigned the $n$ th placement for $n = 1, 2, . . . , N$ ? The probability that my name is drawn at $k$ th attempts is $\frac{2}{N}(\frac{N-2}{N})^{k-1}$ (ie to say $k-1$ failures before the first success at $k$ ). I know that the solution is $\prod_{k=1}^{n-1}\frac{2}{2+N-n}\frac{N-k}{2+N-k}$ . Let $A_i$ be the event that my name compairs shows up at $i$ th attempts. So: $\mathbb{P}(A_1)=\frac{2}{N+1}$ ; $\mathbb{P}(A_2)=\mathbb{P}(\bar{A_1})\mathbb{P}(A_2|\bar{A_1})=(\frac{N-1}{N+1})(\frac{2}{N})$ $\mathbb{P}(A_3)=\mathbb{P}(\bar{A_1}\cap \bar{A_2})\mathbb{P}(A_3|\bar{A_1}\cap \bar{A_2})=(\frac{N-1}{N+1})(\frac{N-2}{N})(\frac{2}{N-1})$ . Thus I thought that $\mathbb{P}(A_n)=(\frac{N-1}{N+1})\cdot (\frac{N-2}{N})\cdot ... \cdot (\frac{N-n-1}{N+1-n})\cdot (\frac{2}{N-n})$ but I can't lead me to the product above. Where am I wrong?",['probability']
3818316,Let $ABCD$ be a convex quadrilateral prove that an inequality holds true,"I've been trying to do the following problem: Let $ABCD$ be a convex quadrilateral and points $E$ and $F$ on sides $AB,CD$ such that: $\frac{AB}{AE}=\frac{CD}{DF}=n$ If $S$ is the area of $AEFD$ show that $\frac{AB*CD+n(n-1)AD^2+n^2*DA*BC}{2n^2}\ge S$ I attempted to solve it in the following way: $\frac{AB*CD}{2n^2}=\frac{AE*DF}{2}$ $\frac{n^2*DA*BC}{2}=\frac{DA*BC}{2}$ $\frac{n(n-1)AD^2}{2n^2}=\frac{(n-1)AD^2}{2n}$ so we have that $\frac{AB*CD+n(n-1)AD^2+n^2*DA*BC}{2n^2}=\frac{AE*DF}{2}+\frac{DA*BC}{2}+\frac{(n-1)*AD^2}{2n}$ This is as far as I got, I couldn't think of any theorems or how to correlate these results with the area of $S$ . Could you please continue my thought pattern and finish it off? Or explain why it can't be solved using my thought pattern and show an alternative approach?","['contest-math', 'euclidean-geometry', 'proof-writing', 'geometry', 'problem-solving']"
3818317,Do covariant derivatives commute?,"Let $M$ be a differentiable manifold, let $f:M\rightarrow\mathbb{R}$ be smooth and let $v,w$ be vector fields in $M$ . Must it be true that $\nabla_w(\nabla_vf) = \nabla_v(\nabla_wf)$ where $\nabla$ denotes the covariant derivative? If so I would like a proof and if not a counter example. Also, to motivate this question, it seems to me that one can ask this independently of picking a riemannian metric. Because of this, if one had a counter example, then one should be able to put any metric on the space (in particular a flat metric) which would give rise to non commuting derivatives for a smooth function in $\mathbb{R}^n$ which never happens. Because of this I suspect this equality must hold, however I am still unsure. To clarify, the reason I think one can ask this equality without picking a metric is that to my understanding $\nabla_vf$ is defined independently of the metric and results in a function from $M\rightarrow\mathbb{R}$ . That is to say, the covariant derivative of a function from the manifold to the reals with respect to a vector field is itself a function from the manifold to the reals and this is defined independently of the metric. Here is why I think the covariant derivative is defined independently of the metric. Say you have a vector field $v$ and real valued function $f$ on the manifold. To get the covariant derivative at a point $p$ we take the vector of the vector field $v$ at $p$ (call this vector $v_p$ ) and we consider some function from $\phi:\mathbb{R}\rightarrow M$ such that $\frac{d}{dt}\phi(t) = v_p$ . Then the covariant derivative is defined as $\frac{d}{dt} f(\phi(t))$ which is a real because $f\circ\phi$ is a map from $\mathbb{R}$ to $\mathbb{R}$ . At no point is the metric required in this definition. Also, I don't think use of the metric is hiding in the definition of the derivative of a map from the reals to the manifold because a tangent vector can be defined as an equivalence class of functions from the reals to the manifold that all have the same derivative when passed through some chart of the smooth manifold. It is quite likely my confusion stems from some miss understanding which is present in my above explanation in which case please point this out to me. Thanks for taking the time to read this post.",['differential-geometry']
3818329,Probability of seeing one event before another when rolling two dice,"You roll two six-sided dice simultaneously and keep track of the sums.
What's the probability that we see sums $6$ and $8$ prior to seeing
the sum $7$ twice? I know that the probability that the sum of $6, 7, 8$ are $5/36, 1/6, 5/36$ , respectively. However, I don't know how to compute the probability of seeing both $6$ and $8$ prior to seeing $7$ . I thought about keeping a Markov chain with eight states: $000, 001, \ldots, 111$ , where the first bit is on if we see $6$ , second bit is on if we see $7$ , etc. Under my interpretation, we start in $000$ and want the probability of visiting $101$ before visiting $010$ twice. I'm not able to figure this out. Can someone please help me? Alternatively, is there an easier approach (without Markov chains?) Thanks","['dice', 'markov-chains', 'probability-theory', 'probability']"
3818429,"Multivariate generalization of 'If $f'(x) > c >0$, then $\lim_{x\to\pm\infty}f(x) = \pm \infty$'?","In the one dimensional setting we have the following: Assume $$f:\Bbb R\to\Bbb R$$ is differentiable and fulfills $$f'(x) > c >0 ,$$ at least outside of some compact set $K$ . Then we have $$\lim_{x\to\pm\infty}f(x) = \pm \infty,$$ which may be seen for example by mean value theorem. I am wondering how to generalize this to $$ f:\Bbb R^n\to\Bbb R^n.$$ I suspect we have $$\lim_{\|x\|\to \infty}\|f(x)\| = \infty,$$ under some conditions on the Jacobian $J_f$ . By the multivariate mean value theorem, one could get that $$f_i(a) - f_i(b) = \langle J_f(z_i)_i,a-b\rangle ,$$ fore the component functions $f_i$ and some $z_i$ on the line segment between $a$ and $b$ .
At this point I am not sure how to put that into a condition on the Jacobian, for example if I use the matrix norm the inequality goes in the 'wrong' direction. Edit : Maybe a good starting point to think about this problem is strong convexity .
Assume that $f$ is the gradient of some function $g$ : $$f(x) = \nabla_g(x).$$ $g$ is strongly convex, iff there exists some $m>0$ such that $$\langle \nabla_g(x) - \nabla_g(y),x-y\rangle \geq m \Vert x-y\Vert_2^2 .$$ In particular, $g$ is strongly convex iff the Hessian of $g$ , that is $J_f$ , fullfills $$J_f \geq mI_n,$$ that is the smallest eigenvalue $\lambda_*$ of $J_f$ fulfills $\lambda_*\geq m$ . Then one can conclude by Cauchy-Schwarz: $$ \Vert f(x) - f(y) \Vert \Vert x-y \Vert \geq \langle f(x) -f(y), x-y\rangle^2 \geq m^2 \Vert x-y\Vert^4 .$$ Which shows $\lim_{\|x\|\to \infty}\|f(x)\| = \infty.$ Of course, this line of argument requires that $f$ has a potential. Maybe one can find another condition without this assumption?","['limits', 'multivariable-calculus', 'jacobian', 'real-analysis']"
3818454,Sum of 3 unit vectors being shorter than 1,"What is the probability for the sum of three unit vectors to be shorter than 1?
The vectors' direction angles has uniform distribution on $[0, 2\pi]$ . I've made simulations and I also used Wolfram Alpha to solve the final equation below, so I'm pretty sure, that the result is $\frac 1 4$ . How can I prove that? If I rotate the vectors with the same angle so that the first one's direction angle is 0, than the length doesn't change. So the result is the same as this value: $$Prob(\vert e^0 + e^{i\alpha} + e^ {i\beta} \vert < 1)$$ ( $\alpha$ and $\beta$ is uniformly distributed on $[0, 2\pi]$ ) I'll denote the sum by z, so $$z = e^0 + e^{i\alpha} + e^ {i\beta} = 1 + e^{i\alpha} + e^ {i\beta}$$ $$\vert z\vert^2 = z \overline z = (1 + e^{i\alpha} + e^ {i\beta})(1 + e^{-i\alpha} + e^ {-i\beta})$$ $$\vert z\vert^2 = 3 + \left( e^{i\alpha} + e^{-i\alpha} \right)+ \left(e^ {i\beta}+ e^ {-i\beta}\right) + \left(e^ {-i(\alpha - \beta)} + e^ {-i(\alpha - \beta)}\right)= 3 + 2 \cos \alpha + 2 \cos \beta+ 2 \cos (\alpha - \beta)$$ This is non-negative so $\vert z \vert < 1 \Leftrightarrow \vert z \vert^2 < 1 $ , that gives us $$3 + 2 \cos \alpha + 2 \cos \beta+ 2 \cos (\alpha - \beta) < 1$$ $$2 + 2 \cos \alpha + 2 \cos \beta+ 2 \cos (\alpha - \beta) < 0$$ $$1 + \cos \alpha + \cos \beta+ \cos (\alpha - \beta) < 0$$ I need to solve this equation. I have the solution from Wolfram Alpha: and this gives me the $\frac 14$ probability. But how can I get this solution?","['trigonometry', 'geometry', 'complex-numbers']"
3818539,What is the meaning of $\omega$ in the lambda calculus?,"Although I am sure the answer to my question must be trivial, I am still struggling with the exact definition of $\omega$ , and consequently the definitions of $\omega_2 \dotsb \omega_n$ , in the Lambda Calculus. As far as I understood, the general definitions are: $\omega = \lambda x.xx$ and $I = \lambda x.x$ . But then, what is $\omega(\omega)$ ( does it result in an infinitely expanding stack loop of function calls? ), and how exactly does $\omega_3 I$ reduce to $I$ (as in: what ""lambda calls"" in what order?). Although many other concepts in lambda calculus make a lot of sense to me -such as the construction of boolean behaviour, how you can essentially build the whole 'world' from that, and its philosophical implications- I feel like I am missing out on some essential understanding.","['lambda-calculus', 'computability', 'discrete-mathematics', 'computer-science']"
3818597,"If $\nu \ll \mu_1 \otimes \mu_2$, for $d\nu(x,y) = \nu_1(x) \nu^x_2(y)$, is it true that $v_1 \ll \mu_1$ and $\nu_2^x \ll \mu_2$?","I'm trying to prove the chain rule for relative entropy using measure theory, and the following
problem showed up. Assume that $\mathcal X_1, \mathcal X_2$ are bosh polish. Let $\mu_1:\mathcal X_1 \rightarrow [0,1]$ , $\mu_2:\mathcal X_2 \rightarrow [0,1]$ and $\nu:\mathcal X_1 \times \mathcal X_2\rightarrow [0,1]$ (all probability measures). Now, one can use the disintegration theorem to state that $d\nu(x,y) = d\nu_1(x) d\nu_2^x(y)$ for $x \in \mathcal X_1$ and $y \in \mathcal X_2$ . Note that, given a joint distribution $(X,Y) \sim \nu$ , the marginal distribution of the first coordinate is $\nu_1$ , and $\nu_2^x$ is the conditional probability $\nu_2^x(A) = P(Y \in A \mid X = x)$ . With all that being said, if $\nu \ll \mu_1 \otimes \mu_2$ , is it true that $\nu_1 \ll \mu_1$ and $\nu_2^x \ll \mu_2$ ? In other words, is the conditional probability measure absolutely continuous with respect to $\mu_2$ for every $y \in \mathcal X_2$ ?","['conditional-probability', 'measure-theory', 'absolute-continuity', 'probability-theory']"
3818638,Expected value of MSB in one-way ANOVA,"Suppose we have $k$ many machines each producing iron balls and we are interested in weights of the balls. Let $y_{ij}$ be the weight of the $j$ th ball produced by $i$ th machine,where $i=1,2,...,k$ and $j=1,2,...,n_i$ . Let $\mu_i$ be the average weight of a ball produced by machine $i$ . Now let us focus on the following hypothesis. We know that if $H_0: \mu_1=\dots=\mu_k$ is assumed to be true, then we have $E(MSB)=\sigma^2$ . Where $$
MSB
 = \sum_i n_i \frac{(\overline y_{i0}-\overline y_{00})^2}
                   {\sigma^2(k-1)}$$ is the Mean Square Between . Now, when $H_0$ is true, its expected value is $\sigma^2$ . But when $H_0$ is not true, then $$
E(MSB)=\sigma^2+\frac{1}{k-1} \sum n_i \alpha^2_i.
$$ Where $\alpha_i=\mu_i-\mu$ .I am a newcomer in statistics and I know the result only, not its proof. So how to prove this equation using basic knowledge of statistics? Please note that I am considering one-way ANOVA.","['statistics', 'anova', 'normal-distribution', 'expected-value', 'probability']"
3818641,How many times will $\{an+b\}_{n=0}^{\infty}$ intersect in a range of $0 - R$,"Given $2$ arithmetic progressions, $\{an+b\}_{n=0}^{\infty}$ and $\{cm+d\}_{m=0}^{\infty}$ , where $a,c$ are distinct primes. These intersect any time there are integers $(n,m)$ such that $an+b=cm+d$ , and after a single intersection $an_0+b=cm_0+d$ they will continue to intersect at $a(n_0+ct)+b=c(m_0+at)+d$ for every natural number $t$ . The first intersection can be efficiently found using the extended Euclidean algorithm. Now let's say we have $k$ such arithmetic progressions of the form $\{an+b\}_{n=0}^{\infty}$ with distinct values of $a$ , where $a$ is a prime and $0<=b<a$ . Also it is given that $a\approx L$ . What is the number of intersections in a single point, we should expect to see in a range of $0-R$ where $R$ is a natural number? Answering this question will help my research on a new factoring method.","['number-theory', 'probability-distributions', 'intersection-theory', 'probability-theory', 'probability']"
3818678,Limit of a sequence (definition),"Let $$ (a_n)_{n \in \mathbb{N}}$$ be a sequence. According to the definition of limit, it is said that $$ \lim_{n \rightarrow \infty} a_n=L \Leftrightarrow  \left(\forall \varepsilon>0, \ \exists n_0 \in \mathbb{N}, \ n \ge n_0 \Rightarrow \left|a_n-L \right|<\varepsilon \right).
 $$ About the details, does it really matter, whether you say $$ n \ge n_0 \ \mathrm{or} \ n > n_0 ?$$ And why would $$ n_0 $$ have to be an integer?","['epsilon-delta', 'definition', 'sequences-and-series']"
3818788,Lehman Scheffe and Conditioned Expectation,"In the proof of the Lehmann-Scheffe Theorem I came across an equality, which I dont understand. It is the last equality at the bottom. But still, I give the setting, since I dont know, if any of that is relevant. Let $(\chi,\sigma,\mathbb{P}_\theta : \theta \in \Omega)$ be a statistical model and $S:\chi\rightarrow \mathbb{R}$ be a sufficient and complete statistic. And let $T:\chi\rightarrow \mathbb{R}^d$ be an unbiased estimator for the statistical quantity $\tau:\theta \rightarrow \mathbb{R}^d$ .
By the Rao-Blackwell theorem, we get an improved version $T^*$ of $T$ w.r.t. its variance by setting $$T^* = \mathbb{E}[T\mid\sigma(S)].$$ To show uniqueness of $T^*$ , we assume another unbiased estimator $H$ and its Blackwellized version $H^* = \mathbb{E}[H\mid\sigma(S)].$ Then there exist a measurable functions $t,h$ with $$H^*(X)=h(S(X)) \\
T^*(X)=t(S(X))
$$ Then, by their unbiasedness, we can write $$0=\mathbb{E}_\theta[H^*] - \mathbb{E}_\theta[T^*] = \mathbb{E}_\theta[H^* - T^*] = \mathbb{E}_\theta[t(S)-h(S)] = \mathbb{E}_\theta[t-h \mid \sigma(S)]\\
$$ Why does the last equality hold? Is there an intuitive way of explaining that?","['conditional-expectation', 'statistics', 'probability-theory']"
3818820,"Selecting $p-1$ integers from a set of $p+1$ integers, so that their weighted sum is divisible by $p$.","An odd prime number $p$ and a $(p + 1)$ -element set $S$ of integers are given.
Prove that it is possible to choose
distinct numbers $a_1, a_2, a_3, \dots, a_{p-1}\in S$ such that \begin{equation}1\cdot a_1+2\cdot a_2+\cdots + (p-1)a_{p-1} \end{equation} is a multiple of p. My observations For $p=3:$ We have $S=\{a_1,\,  a_2,\, a_3,\, a_4\}$ and we need to show that for some distinct $a_1,a_2\in S$ we have $$1 \cdot a_1 + 2 \cdot a_2 = 3 \cdot k,$$ for some integer $k$ .  This is equivalent to $a_1 - a_2 = 3(k-a_2), \, k-a_2 = \alpha, $ so $a_1 - a_2 = 3\alpha$ . And I don't know how to continue, could you help me. Thank you in advance.","['divisibility', 'combinatorics', 'prime-numbers']"
3818844,Doubly periodic meromorphic function with prescribed poles and zeros,"The field of the meromorphic functions on a complex torus $\mathbb{C} \mathbin{/} \Lambda$ is $\mathbb{C}(\wp, \wp')$ ,
where $\wp$ is the weierstrass p-function to the lattice $\Lambda$ .
Furthermore, for such a function $f$ and its finite set $U$ of poles and zeros holds: $\sum_{ u \in U } \operatorname{ord}_u(f) = 0$ and $\sum_{ u \in U } u \cdot \operatorname{ord}_u(f) \in \Lambda$ , where $\operatorname{ord}_u(f)$ is the order of the pole (if negative) resp. the zero (if positive) of $f$ at $u$ . If now some points $U$ and their orders are given, and fulfill constraints above, I believe (because of the Riemann–Roch theorem) that a corresponding meromorphic function exists and is unique (up to a multiplicative constant), but I cannot figure out how to construct it from $\wp$ and $\wp'$ . Are my claims correct? And if yes, how to construct the meromorphic function in question (with closed-form formula, or recursively)?","['elliptic-curves', 'complex-analysis', 'elliptic-functions', 'divisors-algebraic-geometry', 'meromorphic-functions']"
3818872,Largest Singular Value of Triangular Matrix,"Let $R\in\mathbb{R}^{d\times d}$ be an upper triangular matrix. The eigenvalues of $R$ is then its diagonal entries, that is, $(R_{ii}, e_i)$ is an eigenpair of $R$ since $Re_i=R_{ii}e_i$ . Is it possible to bound the largest singular value $\sigma_\max (R)=\sqrt{\lambda_\max (RR^T)}$ by the diagonal entries of $R$ ? Weyl's inequality tells us $\lambda_\max \le \sigma_\max$ which is the wrong direction. But maybe we can get something like $\sigma_\max < \lambda_\max^2$ assuming $\lambda_\max>1$ ? Or can the largest singular value be arbitrarily larger?","['matrices', 'singular-values', 'linear-algebra', 'eigenvalues-eigenvectors']"
3818878,"Find a function $g$ that makes $f$ continuous, but not differentiable","Consider the function $f:[-1,1] \to \mathbb{R}$ defined piecewise by \begin{align*}f(x) = \begin{cases}0: -1 \leq x < 0 \\ g(x): 0\leq x \leq 1 \end{cases}\end{align*} where $g:[0,1] \to \mathbb{R}$ is some function. Suppose $g$ is a non-zero function given by $$g(x) = a_0 + a_1x + a_2x^2 + a_3x^3.$$ Find non-negative real numbers $a_0, a_1, a_2, a_3$ (not all of which are 0) such that $f$ is continuous but not differentiable. I've been able to find some examples of general continuous, but not differentiable functions, but I have not been able to find one that fits this specific example AND to supply non-negative real numbers that makes it true. My problem is I can't preserve continuity at $f(0)$ but make $f(x)$ non-differentiable given the restriction of the $a_i$ 's.","['continuity', 'derivatives', 'piecewise-continuity', 'real-analysis']"
3818919,Probability that a quadratic equation has real roots,"Problem The premise is almost the same as in this question . I'll restate for convenience. Let $A$ , $B$ , $C$ be independent random variables uniformly distributed between $(-1,+1)$ . What is the probability that the polynomial $Ax^2+Bx+C$ has real roots? Note: The distribution is now $-1$ to $+1$ instead of $0$ to $1$ . My Attempt Preparation When the coefficients are sampled from $\mathcal{U}(0,1)$ , the probability for the discriminant to be non-negative that is, $P(B^2-4AC\geq0) \approx 25.4\% $ . This value can be obtained theoretically as well as experimentally. The link I shared above to the older question has several good answers discussing both approaches. Changing the sampling interval to $(-1, +1)$ makes things a bit difficult from the theoretical perspective. Experimentally, it is rather simple. This is the code I wrote to simulate the experiment for $\mathcal{U}(0,1)$ . Changing it from (0, theta) to (-1, +1) gives me an average probability of $62.7\%$ with a standard deviation of $0.3\%$ I plotted the simulated PDF and CDF. In that order, they are: So I'm aiming to find a CDF that looks like the second image. Theoretical Approach The approach that I find easy to understand is outlined in this answer . Proceeding in a similar manner, we have $$
f_A(a) = \begin{cases}
\frac{1}{2}, &-1\leq a\leq+1\\
0,           &\text{ otherwise}
\end{cases}
$$ The PDFs are similar for $B$ and $C$ . The CDF for $A$ is $$
F_A(a) = \begin{cases}
\frac{a + 1}{2}, &-1\leq a\geq +1\\
0,&a<-1\\
1,&a>+1
\end{cases}
$$ Let us assume $X=AC$ . I proceed to calculate the CDF for $X$ (for $x>0$ ) as: $$
\begin{align}
F_X(x) &= P(X\leq x)\\
&= P(AC\leq x)\\
&= \int_{c=-1}^{+1}P(Ac\leq x)f_C(c)dc\\
&= \frac{1}{2}\left(\int_{c=-1}^{+1}P(Ac\leq x)dc\right)\\
&= \frac{1}{2}\left(\int_{c=-1}^{+1}P\left(A\leq \frac{x}{c}\right)dc\right)\\
\end{align}
$$ We take a quick detour to make some observations. First, when $0<c<x$ , we have $\frac{x}{c}>1$ . Similarly, $-x<c<0$ implies $\frac{x}{c}<-1$ . Also, $A$ is constrained to the interval $[-1, +1]$ . Also, we're only interested when $x\geq 0$ because $B^2\geq 0$ . Continuing, the calculation $$
\begin{align}
F_X(x) &= \frac{1}{2}\left(\int_{c=-1}^{+1}P\left(A\leq \frac{x}{c}\right)dc\right)\\
&= \frac{1}{2}\left(\int_{c=-1}^{-x}P\left(A\leq \frac{x}{c}\right)dc + \int_{c=-x}^{0}P\left(A\leq \frac{x}{c}\right)dc + \int_{c=0}^{x}P\left(A\leq \frac{x}{c}\right)dc + \int_{c=x}^{+1}P\left(A\leq \frac{x}{c}\right)dc\right)\\
&= \frac{1}{2}\left(\int_{c=-1}^{-x}P\left(A\leq \frac{x}{c}\right)dc + 0 + 1 + \int_{c=x}^{+1}P\left(A\leq \frac{x}{c}\right)dc\right)\\
&= \frac{1}{2}\left(\int_{c=-1}^{-x}\frac{x+c}{2c}dc + 0 + 1 + \int_{c=x}^{+1}\frac{x+c}{2c}dc\right)\\
&= \frac{1}{2}\left(\frac{1}{2}(-x+x(\log(-x)-\log(-1)+1) + 0 + 1 + \frac{1}{2}(-x+x(-\log(x)-\log(1)+1)\right)\\
&= \frac{1}{2}\left(2 + \frac{1}{2}(-x+x(\log(x)) -x + x(-\log(x))\right)\\
&= 1 - x
\end{align}
$$ I don't think this is correct. My Specific Questions What mistake am I making? Can I even obtain the CDF through integration? Is there an easier way? I used this approach because I was able to understand it well. There are shorter approaches possible (as is evident with the $\mathcal{U}(0,1)$ case) but perhaps I need to read more before I can comprehend them. Any pointers in the right direction would be helpful.","['integration', 'probability-distributions', 'uniform-distribution', 'probability']"
3818942,Convergence of the solution and their derivative of the equation $y''+y'+y^{3}=0$. [duplicate],"This question already has answers here : Limits for the solution of the non-linear ODE (2 answers) Closed 3 years ago . I have the ODE $y''+y'+y^{3}=0$ and I must prove that the solution $y(t)$ and $y'(t)$ converges to zero when $t\to \infty$ . I try to write the associated system of two equations, this is one form \begin{equation*}
y'=z;\qquad z'=-z-y^3
\end{equation*} and this is the other \begin{equation*}
y'=z-y;\qquad z'=-y^3.
\end{equation*} I try to use Lyapounov method with the function $V(y,z)=z^2+\frac{1}{2} y^4$ and obtain that $\nabla V\cdot (y',z') <0$ . But I don't know how to conclude this proof. On the other hand, using the associated matrix for the linear system, the real part of the eigenvalues is not negative, in fact is zero and I don't know how to continue. I accept any suggestion, hint or book to read.","['stability-in-odes', 'lyapunov-functions', 'ordinary-differential-equations']"
3818956,Why isn't $x^2+x+1$ a factor of $x^{12}+x^6+1$?,"When we solve the equation $$x^{12}+x^6+1=0$$ we obtain $2$ solutions that also satisfy $$x^2+x+1=0$$ namely $-\frac{1}{2}\pm i\frac{\sqrt3}{2}$ .
Shouldn't this imply that $x^2+x+1$ is a factor of $x^{12}+x^6+1$ ? However, the fully factorsied form of $x^{12}+x^6+1$ is $$(x^6-x^3+1)(x^6+x^3+1)$$ The reason I think that it should be a factor is that when we have, for a function $f(x)$ , (here a polynomial,) $f(a)=0$ then we know that $(x-a)$ is a factor of $f(x)$ . If we multiply $(x-(-\frac{1}{2}+ i\frac{\sqrt3}{2}))$ by $(x-(-\frac{1}{2}- i\frac{\sqrt3}{2}))$ which are both factors of $x^{12}+x^6+1$ we get $x^2+x+1$ . It seems to me that this should apply to the polynomials above. I have a feeling that my logic is specious, but I'm not entirely certain why; I think the answer to my problem may be that $x^2+x+1$ is factor of $x^{12}+x^6+1$ , but only if the other factor has complex coefficients, but I'm not sure. Thank you for your help. EDIT Oh wow, I'm so sorry for making such a stupid mistake everyone, thanks for correcting me (see numerous comments and answers). I was looking at a few polynomials simultaneously and got mixed up. Thanks for your help.","['algebra-precalculus', 'roots', 'complex-numbers', 'factoring']"
3818961,Show each infinite set $S \subset \mathbb R$ contains a countably infinite subset,"Show each infinite set $S \subset \mathbb R$ contains a countably infinite subset. I understand that if you remove an object from the set, it will still be infinite, and if we remove another object, it will be countably infinite (I think so at least). I'm very lost on this kind of proof. However if someone can help that would be appreciated.","['elementary-set-theory', 'real-numbers']"
3818977,Limit of $f(x)= -x \tanh(x) +\log(2 \cosh (x))$ at $+\infty$,I am trying to calculate the limit for $x$ going to $+\infty$ of the following function . The problem originated from a physical model actually $$f(x)= -x\tanh(x) +\log(2\cosh(x))$$ I get an indeterminate form of the form $-\infty +\infty $ that I don't know how to solve due to the complexity of the functions. Any idea?,"['limits', 'calculus']"
3818980,"Are the remaining samples still independent, identically distributed (IID) after removing the maximum value of the IID samples?","$X_1, X_2, \cdots, X_N$ are independent identically distributed (IID) random variables and $Y_1$ , $Y_2, \cdots, Y_{N-1}$ are the remaining after removing the maximum value of $X_k, k=1, \cdots, N .$ Is this assumption true that $Y_1$ , $Y_2, \cdots, Y_{N-1}$ are IID?","['statistics', 'probability-distributions', 'order-statistics', 'statistical-mechanics']"
3818982,"Steiner(5,6,12) system: symmetrical split into four or six","I am making a pack of Steiner(5,6,12) cards, and intend to make it available to others.
The plan is that there will be 143 cards, 2¼″×3½″ ≈ 57mm×89mm, comprising: the 132 Steiner cards; one or two jokers of each of the four suits; two unsuited jokers; and, à la ‘bridge card’, a card giving some explanation and asking that games for these cards be tagged ‘#SteinerKirkmanCards’. The 132 Steiner cards are to be symmetrically assigned to four suits [edit: or six — and six might be better], 33 [or 22] cards in each suit.
¿How should this assignment to suits should be done? Each quadruple of letters (e.g., ABCD) appears on exactly four cards.
Is it possible that, for every quadruple, its four cards be three of one suit and one of another?
There are 495 such quadruples, so it cannot be that each suit has the same number of the ones.
Can it be that one special suit has exactly one instance of each quad, and that the other three suits each have three instances of one third of the quads?
Or can it be that one special suit has exactly one instance of each quad, and that the other three suits each have one instance of one third of the quads and two instances of another third (such that every quad is 1:0:1:2, with permutations of last three)?
Please, what of this is possible, and how? Or, if none of that works, can there be a different strong symmetry?
Because the cards are defined by the uniqueness of the quintuples, it would be natural to suit based on quads or triples or pairs. Each letter appears on 66 cards, so letters cannot be even across the four suits.
Can the chosen symmetry be done with the letters similarly distributed across the suits (e.g., each letter’s suit frequencies being 12:15:18:21)?
This is also a desirable type of symmetry. The particular list of hexads (but improved in an answer below) currently being used is not special; if permuting letters would help gain any elegant qualities, please permute. The ‘non-bridge card’ could include concise credit for the assignment to suits. Edit: I’ve been asked by a different channel whether the number of suits must must be four. No. For reasons of game play, I think the number of suits must be ≥3, and ≤6, and a factor of 132: so three or four or six. Indeed, that permits an extra request: it might be that some games could work with a half pack, 66 cards, being half of the suits. If the number of suits is even, and there is a ‘natural’ split of the suits, please do say what that is. (Also a conventional deck has four suits of two colours; this deck might have four suits of two colours, or six suits of three colours.) Though this post is not about the visual layout of the cards, it is possible that the layout might interact with the possible games which might interact with the best choice of symmetry.
Hence there are low-resolution drafts , some ⟳180°.","['combinatorial-designs', 'finite-groups', 'group-theory', 'combinatorics']"
3819058,Computing directional derivative of scalar field $F$ along a straight line,"In Pavel Grinfeld's Introduction to Tensor Analysis and the Calculus of Moving Surfaces , the directional derivative is defined as below: A directional derivative measures the rate of change of a scalar field $F$ along a straing line. Let a straight ray $\ell$ emanate from the point $P$ . Suppose that $P^*$ is a nearby point on $\ell$ and let $P^*$ approach $P$ in the sense that the distance $PP^*$ approaches zero. Then the directional derivative $\frac{\mathrm dF}{\mathrm d\ell}$ at the point $P$ is defined as the limit $$\frac{\mathrm dF(P)}{\mathrm d\ell}=\lim_{P^*\to P}\frac{F(P^*)-F(P)}{PP^*}$$ Several exercises follow, which I've started working through but I'm unsure whether what I'm doing is correct. Evaluate $\frac{\mathrm dF}{\mathrm d\ell}$ for $F(P)=$ ""Distance from point $P$ to point $A$ "" in a direction perpendicular to $AP$ . I made a sketch of the situation: By definition of $F(P)$ , $F(P^*)$ is the length of the line segment $AP^*$ and $F(P)$ is the length of $AP$ . So I think we have $$\begin{align}
\frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{AP^*-AP}{PP^*}\\[1ex]
&=\lim_{P^*\to P}\frac{AP^*-AP}{\sqrt{(AP^*)^2-(AP)^2}}\\[1ex]
&=\lim_{P^*\to P}\sqrt{\frac{AP^*-AP}{AP^*+AP}}=0
\end{align}$$ because $AP^*\to AP$ as $P^*\to P$ . The next exercise is slightly more complicated: Evaluate $\frac{\mathrm dF}{\mathrm d\ell}$ for $F(P)=$ "" $1$ /(Distance from point $P$ to point $A$ )"" in the direction from $P$ to $A$ . The sketch is the same as the previous one, but this time it sounds like $A$ lies on $\ell$ somewhere between the point $P$ and the tip of the ray. Let $a$ be the distance $AP^*$ . Then $AP$ has length $ba$ for some $0<b<1$ , and $PP^*$ has length $(1-b)a$ . So I think we have $$\begin{align}
\frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{\frac1{AP^*}-\frac1{AP}}{PP^*}\\[1ex]
&=\lim_{b\to1}\frac{\frac1a-\frac1{ba}}{(1-b)a}\\[1ex]
&=\lim_{b\to1}-\frac1{ba^2}=-\frac1{(AP)^2}
\end{align}$$ Edit: Evaluate $\frac{\mathrm dF}{\mathrm d\ell}$ for $F(P)=$ ""Angle between $OA$ and $OP$ ,"" where $O$ and $A$ are two given points, in the direction from $P$ to $A$ . $$\begin{align}
\frac{\mathrm dF(P)}{\mathrm d\ell}&=\lim_{P^*\to P}\frac{\theta^*-\theta}{PP^*}\\[1ex]
&=\lim_{\theta^*\to\theta}\frac{\theta^*-\theta}{\sqrt{(OP^*)^2+(OP)^2-2(OP^*)(OP)\cos(\theta^*-\theta)}}\\[1ex]
&=\lim_{\theta^*\to\theta}\frac{\theta^*-\theta}{\sqrt{a^2b^2+a^2-2a^2b\cos(\theta^*-\theta)}}=0&(\text{where }a=OP,b>1)
\end{align}$$ Are the reasoning and solution correct? The first two solutions I've arrived at seem consistent with what I know about basic calculus, but I'm not sure about the last one.","['calculus', 'solution-verification', 'derivatives', 'tensors']"
3819067,Fixed Points of Self-Referential Function of Various Lengths,"Note: The following question is based on a discussion I had in the comments of this question . The function I'll define is a generalization of the function given in that problem. Fix some $n$ . Then define the function $f_n : \mathbb{N}^n \to \mathbb{N}^n$ on $n$ -tuples of natural numbers as follows: The first element of $f_n(\bf{x})$ is the number of distinct values appearing in the tuple $\bf{x}$ . For $j > 1$ , the $j$ th element of the tuple $f_n(\bf{x})$ is the number of times $j-2$ appears in $\bf{x}$ . So, the second element of the tuple $f_n(\bf{x})$ is the number of zeroes in $\bf{x}$ , the third element is the number of ones, and so on. As an example of computing this function, consider $$f_5(2, 3, 2, 0, 0) = (3, 2, 0, 2, 1)$$ The $3$ in the answer comes from there being three distinct digits in the input $\{0, 2, 3\}$ . The next elements come from: ""there are two 0's in the input"", ""there are zero 1's in the input"", ""there are two 2's in the input"", and ""there is one 3 in the input"". Now, the question is: what are the fixed points of this function $f_n$ , i.e. what values of $\bf{x}$ make $f_n(\bf{x}) = \bf{x}$ ? We can immediately reduce this problem to a finite number of cases, because everything in the image of $f_n$ consists only of tuples whose maximum value is $n$ . That is to say, we can compute the fixed points directly. Now, I've written a handy script to compute all of the fixed points for $n \leq 30$ , and it's from this that I want to present the theorem. $n \leq 11$ seems to be mostly chaos, but when $n \geq 12$ , a pattern starts to emerge. Theorem: For $n \geq 12$ , there are exactly two fixed points of $f_n$ . The two fixed points are exactly $\bf{x}$ and $\bf{y}$ as follows: ${\bf x} = (5, n - 6, 3, 0, 1, 0, 1, \dots, 1, 0, 0, 0, 0)$ ${\bf y} = (5, n - 6, 2, 2, 0, 0, 1, \dots, 1, 0, 0, 0, 0)$ where the omitted $(\dots)$ consists only of zeroes. As I said, I've verified this on the computer for $12 \leq n \leq 30$ . It would be interesting to have a general proof of this statement, or a refutation.","['combinatorics', 'fixed-points']"
3819116,"Rigorously, what's happening when I treat $\frac{dy}{dx}$ as a fraction? [duplicate]","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 1 year ago . Consider a very simple differential equation (assuming $y$ is an invertible, continuous function and considering only real numbers, for simplicity): $$\frac{dy}{dx}=y$$ If I were to solve this, I would do something like: $$dy=dx\cdot y \implies \frac{1}{y}\cdot dy = dx$$ Then I would integrate to arrive to a solution. Of course, this works, but why? $\frac{dy}{dx}$ is not a fraction, even if it sometimes acts like one. So what are the missing steps that allow you to treat it as such? I seem to remember an explanation but I can't find it anywhere, so I'm asking here. Thanks!","['calculus', 'ordinary-differential-equations']"
3819148,Lipschitz function is differentiable at a point iff its tangent set is a k-dimensional plane,"I'm reading a proof of Hadamard-Perron theorem from Katok's Introduction to the Modern Theory of Dynamical Systems . I'm having problems with the following part. Let $\varphi:\mathbb{R}^k\to\mathbb{R}^{n-k}$ be a Lipschitz function with Lipschitz constant = $\gamma$ . For $x\in\mathbb{R}^k$ define $$
    \Delta_y\varphi := \frac{(y,\varphi(y))-(x,\varphi(x))}{||(y,\varphi(y))-(x,\varphi(x))||} \text{ for } x \neq y,
    $$ $$
t_x\varphi := \{v \in T_x\mathbb{R}^n : \exists \{x_n\}_{n \in \mathbb{N}} \text{ such that } \lim_{n\to\infty}x_n=x \land \lim_{n\to \infty}\Delta_{x_n}\varphi=v \}.
$$ Then by a tangent set to $\varphi$ at $x$ we mean the set $$\tau_x\varphi := \bigcup_{v\in t_x\varphi}\mathbb{R} v$$ Now we have: Lemma. $\varphi$ is differentiable at $x$ $\iff$ $\tau_x\varphi$ is a $k$ - dimensional plane , where by a $k$ - dimensional plane we mean a $k$ - dimensional subspace of $\mathbb{R}^n$ . There's no proof of this in the book. I know how to prove it in one dimensional case, where $\varphi:\mathbb{R}\to\mathbb{R}$ , because it's all about the limit-definition of a derivative. I tried to adopt one dimensional case to multidimensional by proving that if the tangent set was ""something more"" than a $k$ - dimensional plane, then one of the partial derivatives wouldn't exist but with no success. Also, I couldn't find anything helpful here. If I missed something, please redirect me. P.S. I'm not looking for a solution, hints would work better for me :) Many thanks for any help!","['derivatives', 'lipschitz-functions', 'tangent-spaces']"
3819202,Solving infinite nested square roots of 2 converging to finite nested radical,"Can anyone explain to solve the identity posted by my friend $$2\cos12°= \sqrt{2+{\sqrt{2+\sqrt{2-\sqrt{2-...}}} }}$$ which is an infinite nested square roots of 2. (Pattern $++--$ repeating infinitely) Converging to finite nested radical of $2\cos12° = \frac{1}{2}\times\sqrt{9+\sqrt5+\sqrt{(30-6\sqrt5)}}$ The finite nested radical, I was able to derive $\cos12° = \cos(30-18)°$ as follows $$\cos30°\cdot\cos18° + \sin30°\cdot\sin18°$$ $$= \frac{√3}{2}\cdot\frac{\sqrt{2+2\cos36°}}{2}+\frac{1}{2}\cdot\frac{\sqrt{2-2\cos36°}}{2}$$ Where $\cos18° = \frac{\sqrt{2+2\cos36°}}{2}$ (by Half angle cosine formula) and $\sin18° = \frac{\sqrt{2-2\cos36°}}{2}$ (solving again by half angle cosine formula) $2\cos36° =\frac{ \sqrt5 +1}{2}$ which is golden ratio $\frac{\sqrt3}{2}\cdot\frac{\sqrt{10+2\sqrt5}}{4}+ \frac{1}{2}\cdot\frac{\sqrt{5}-1}{4} = \frac{\sqrt{30+6\sqrt5}}{8}+ \frac{\sqrt5-1}{8}$ Further steps finally lead to the finite nested radical Method actually I tried to solve infinite nested square roots of 2 is as follows. $2\cos\theta = \sqrt{2+2\cos2\theta}$ and $2\sin\theta = \sqrt{2-2\cos2\theta}$ Now simplifying infinite nested square roots of 2, we will get the following as simplified nested radical $$2\cos12° = \sqrt{2+\sqrt{2+\sqrt{2-\sqrt{2-2\cos12°}}}}$$ Simplifying step by step as follows $2\cos12° = \sqrt{2+\sqrt{2+\sqrt{2-2\sin6°}}}$ then $2\cos12° = \sqrt{2+\sqrt{2+\sqrt{2-2\cos84°}}}$ (by $\sin\theta = \cos(90-\theta)$ $2\cos12° = \sqrt{2+\sqrt{2+2\sin42°}}$ $2\cos12° = \sqrt{2+\sqrt{2+2\cos48°}}$ $2\cos12° = \sqrt{2+2\cos24°}$ $2\cos12° = 2\cos12°$ We are back to $\sqrt1$ Actually this is how I got stuck! But for infinite nested square roots of 2(as depicted), if I run program in python I am able to get good approximation ( Perhaps if we run large number of nested square roots in python we get more number of digits matching the finite nested radical), because I'm not able get anywhere solving such a kind of infinite cyclic nested square roots of 2. Dear friends, is there anyway to find the solution by any other means like solving infinite nested square roots Thanks in advance.","['nested-radicals', 'trigonometry']"
3819245,"Another proof that $Hom(V,W)\cong V^*\otimes W$.","This question was already discussed here: Why is Hom(V,W) the same thing as V∗⊗W? But, I want to take a different approach in proving that $$\phi:V^*\otimes W\to \text{Hom}(V,W)$$ is an isomorphism where $V$ and $W$ are finite dimensional vector spaces. Let's fix basis $\{w_1,\dots,w_m\}$ , $\{v_1,\dots,v_m\}$ , and $\{\alpha_1,\dots,\alpha_m\}$ for $W$ , $V$ , and $V^*$ correspondingly where $\{\alpha_i\}$ is a dual basis for $\{v_i\}$ . Next, let's define $\phi$ on basis tensors $\alpha_i\otimes w_j\in V^*\otimes W$ via $$\phi(\alpha_i\otimes w_j)=f_{\alpha_i}(w_j)$$ where $f_{\alpha_i}(w_j)(v)=\alpha_i(v)w_j$ and extend it linearly since every element $x$ of $V^*\otimes W$ can be written as a finite sum of the basis elements i.e. if $x\in V^*\otimes W$ , then $x=\sum_{i,j}a_{ij}\alpha_i\otimes w_j$ and $$\phi(x)=\sum_{i,j}a_{ij}\phi(\alpha_i\otimes w_j)=\sum_{i,j}a_{ij}f_{\alpha_i}(w_j).$$ Next, I claim that it is enough to show that $\ker(\phi)=0$ since $V^*\otimes W$ and $Hom(V,W)$ have the same dimension and $\phi$ is a linear map (rank-nullity Theorem). So, let's show that $\ker(\phi)=0$ . We can do that by fixing the basis for $V$ , $W$ , and $V^*$ and showing that if $\phi(x)=0$ then $x=0$ (Details are skipped due to computations). Does it make sense? A follow-up question: Do we have an isomorphism if we take either of these spaces to be infinite dimensional? What would we do in that case?","['tensor-products', 'solution-verification', 'linear-algebra', 'vector-spaces']"
3819255,"Is $\sigma(X_1,Y_1)=\sigma(X_2,Y_2)$ iff $\operatorname{Lin}(X_1,Y_1)=\operatorname{Lin}(X_2,Y_2)$?","Let $X_1,X_2,Y_1,Y_2$ be random variables on a probability space such that $$\{aX_1+bY_1 : a,b\in \mathbb R \}=\{cX_2+dY_2 : c,d\in \mathbb R \}$$ Are the sigma-algebras $\sigma(X_1,Y_1)$ and $\sigma(X_2,Y_2)$ the same ? Intuitively, I feel it should be true, but I have no clue how to prove it. What about the other way around ? If $\sigma(X_1,Y_1)=\sigma(X_2,Y_2)$ , does it mean $$\{aX_1+bY_1 : a,b\in \mathbb R \}=\{cX_2+dY_2 : c,d\in \mathbb R \}?$$","['measure-theory', 'probability-theory']"
3819265,Find Sylow $p$-subgroup within subgroup,"Let $p$ be a prime number, $G$ a group with subgroup $H$ and $S$ a Sylow $p$ -subgroup of $G$ . Show that there exists $g\in G$ such that $H\cap gSg^{-1}$ is a Sylow $p$ -subgroup of $H$ . Moreover, come up with an example that shows that $g\neq e_G$ holds in general. My attempt: By the first Sylow theorem applied to $H$ , we find a Sylow $p$ -subgroup of $H$ . This should be a $p$ -group of $G$ and thus, by the second Sylow theorem, contained in a Sylow $p$ -subgroup, say $T$ of $G$ . As all Sylow $p$ -subgroups are conjugate, we find $g\in G$ such that $gSg^{-1}=T$ . Is it possible to conclude that $H\cap T$ is a Sylow $p$ -subgroup of $H$ ?","['group-theory', 'sylow-theory', 'prime-numbers']"
3819286,"What is the true, formal meaning and reason for the ""dx"" symbol in integrals","When I encountered integrals for the first time, and learned to write the ""dx"" at the end of every integral, I had no problem interpreting it as something that told me what the variable of integration is, or where the integral ends, and nothing more. But when I encountered u-substitution, we started doing things like du = u'(x)dx, and replacing u'(x) and dx with du in the integral. Well, that seems like ""dx"" was never just a delimiter, but something being multiplied with the function itself. I asked around about this, and people told me that dx is in fact, only a delimiter to tell us our variable of integration, and that the ""multiplication"" I did was just some sort of mnemonic for the reverse chain rule. I thought it was weird to use a mnemonic like that, but I understood it. But then others told me that ""dx"" is part of what's being integrated, and they started saying that we're led to believe that its just a delimiter in early courses because it'd be impossible for teachers to introduce ""differentials,"" which is what things like dx and du are, so u-substitution isn't just a mnemonic, and the multiplication is completely formal. They also said that I haven't been integrating functions, but rather differential forms, and have only been told I'm integrating functions to make things easier until I learn the truth. This is all extremely confusing to me. I have no idea how I've heard so many differing opinions that can't be true at the same time. This all, once again, leaves me wondering, what is the real, formal meaning of the notation we use for integrals, what does that ""dx"" truly represent - is it a part of the computation, or is it something easily replaceable by a string like ""with respect to x""? Do we integrate functions, or do we integrate something called a differential form? How much of what I've been told is true, and what haven't I been told? This has been bothering me for some time, so I'd greatly appreciate it if anyone could try to clear this up for me!","['integration', 'calculus', 'notation', 'differential-forms']"
3819381,Harmonic function and harmonic conjugate,"Let $u:G\subset\mathbb{R} \rightarrow \mathbb{R}$ a harmonic function $v:G\rightarrow \mathbb{R}$ the harmonic conjugate function, with $G$ a domain. Prove that $u^2-v^2$ and $uv$ are harmonic without derivatives. Before this, I proved that $u^2$ is harmonic, if $u$ is an harmonic function. Then, I thought that $u^2$ and $v^2$ are harmonic functions, and I wanted to conclude that $u^2-v^2$ is a harmonic function. Nonetheless, this interpretation is wrong. Thanks in advance","['complex-analysis', 'harmonic-functions']"
3819393,The degree of vector bundle on integral projective curve is the degree of its determinant bundle.,"This is exercise 18.4.J in Vakil's ""The Rising Sea  Foundations of Algebraic Geometry"".
Here the degree of a coherent sheaf $\mathscr{F} $ on integral projective curve $ C $ is defined to be $$ \mathrm{deg} \mathscr{F}=\chi(C,\mathscr{F})-(\mathrm{rank} \mathscr{F})\cdot \chi(C,\mathscr{O}_C).$$ Here the curve may be singular, and thus the local ring at a closed point is not a discrete valuation ring. I have no idea whether we can define the order of zeros or poles of a rational section in this case. A hint is given: Exercise 13.5.H, i.e., given an exact sequence $$0\to \mathscr{F}_1\to \cdots \to \mathscr{F}_n \to 0 $$ of finite rank locally free sheaves on $ X $ , the alternating product of determinant bundle is trivial. My idea is that since $\mathscr{F}(m) $ is gloally generated for $m>>0 $ , we get an exact sequence of locally free sheaf $$ 0\to \mathscr{F_0} \to \oplus\mathscr{O}(-m)\to \mathscr{F}\to 0,$$ but I don't know whether $\mathscr{F_0}$ is direct sum of line bundles, and I'm not sure if we still have $$\mathrm{deg}(\mathscr{F}\otimes\mathscr{G})=\mathrm{deg} \mathscr{F}+\mathrm{deg} \mathscr{G}$$ in this case. When C is regular, this is done by computing the divisor of zeros and poles of a rational section, but here $ C $ may be singular now.","['divisors-algebraic-geometry', 'algebraic-geometry']"
3819487,Sum of squares of numbers equals product of numbers,"Find number of tuples such that the sum of the squares of the numbers equals the product of the numbers. I tried it and found some tuples like $(3,3,3)$ satisfying $3^2+3^2+3^2=3\times 3\times 3$ but I don't know the real approach to find all such numbers. Can anyone try it?","['algebra-precalculus', 'discrete-mathematics']"
3819503,A bartender stole champagne from a bottle that contained 50% of,A bartender stole champagne from a bottle that contained 50% of spirit and he replaced what he had stolen with champagne having 20% spirit. The bottle then contained only 25% spirit. How much of the bottle did he steal? My approach: Let the total quantity champagne containing  (50% of the spirit) be x. $\therefore$ it will contain x/2  spirit and x/2 non-spirit. Suppose he steals y% from it. Let the total quantity of champagne containing(20 % of spirit) be z. $\therefore$ It will contain z/5 spirit. Now I'm not able to form an equation please help.,['algebra-precalculus']
3819505,What is an extension field? Covered differently in math & in cryptography.,"In his book on Cryptography, Paar has this theorem Theorem 4.3.1 A field with order m only exists if m is a prime
power, i.e., m = p^n, for some positive integer n and prime integer
p. p is called the characteristic of the finite field. So here he says that the order has to be a prime power -
He also has this as examples This theorem implies that there are, for instance, finite fields with 11 elements, or with 81 elements (since 81 = 3^4) or with 256 elements (since 256 = 2^8, and 2 is a prime). So he explicitly says that you can have a field with 256 elements - the order of a finite field needs to be a prime power & not necessarily a prime itself. He then goes on to talk about extension fields - he says that if the order of the field is not a prime then it's called as an extension field. In AES the finite field contains 256 elements and is denoted as GF(2^8). This field was chosen because each of the field elements can be represented by one byte. For the S-Box and MixColumn transforms, AES treats every byte of the internal data path as an element of the field GF(2^8) and manipulates the data by performing arithmetic in this finite field.
However, if the order of a finite field is not prime, and 2^8 is clearly not a prime, the addition and multiplication operation cannot be represented by addition and multiplication of integers modulo 2^8. Such fields with m > 1 are called extension fields. So as per this, I get the definition of an extension field as this - an extension field is any finite field where the order of the field is a prime power but not a prime itself. However, when I look at books on abstract algebra, I see a totally different definition of extension fields which seem to be unconnected with what Paar says. For e.g. from ""Topics in Algebra"" by Hernstein: Let F be a field; a field K is said to be an extension of F if K
contains F. Equivalently, K is an extension of F if F is a subfield of K. So are Extension fields described in Cryptography different from those described in Algebra? But is Paar's description wrong? Or are the 2 definitions equivalent in some way?","['extension-field', 'finite-fields', 'abstract-algebra', 'cryptography']"
3819516,Expected number of moves desperate help,"Question:You're trying to get a cat, a fish, a dog, and your lunch across a river, but there's a troll in the way. The troll says, ""I'll allow you to cross the river, but only if you play this game with me. I have a die here showing a cat, a fish, a dog, and your lunch. I'll roll that die, and then you must bring that item across the river, no matter which side it's on. Once you do that, I'll roll the die again. If you can get everything to the other side, I'll let you go."" You quickly realize this is a bad idea: If you leave the cat and fish alone on one side, the cat will eat the fish, and if you leave the dog and lunch alone on one side, the dog will eat your lunch. (If the cat, the fish, and something else are alone on one side, nothing will be eaten. Likewise, if the dog, your lunch, and something else are alone on one side, nothing will be eaten.) You tell this to the troll, who says, ""Fine. When I absolutely need to, I'll re-roll the die to make sure none of your precious cargo is harmed."" Suppose that you make a move when you bring something from one side of the river to the other. (If the troll re-rolls their die, the original roll is disposed of, and this does not count as a move.) Find the expected number of moves you'll need to make before everything is on the other side of the river. So, this is what I have so far: I let $e_i$ represent the expected value of the number of moves in order for all $i$ items to be on the other side of the bridge. Therefore our goal is to find $e_4.$ However, I am having trouble forming the linear recurrences and it's really frustrating me. Can anybody help? Thanks! I also know the problem involves states therefore letting me make the states where 4,3,2 or 1 of the things are on the starting side. However, I am also having trouble connecting the relations.","['contest-math', 'combinatorics', 'probability']"
3819545,Is it possible to tile a $13 \times 13$ board with $4 \times 1$ dominoes such that the center square is left untiled?,"Problem Is it possible to tile a $13 \times 13$ board with $4 \times 1$ dominoes such that the center square is left untiled? I was not able to find a tiling so I am trying to prove that it is no possible. I tried the usual way of coloring the board with $4$ colors using a chessboard style alternating coloring. Lets say the colors are $1, 2, 3, 4$ then I find that we have $43$ $1$ ’s, $42$ $2$ ‘s, $42$ $3$ ‘s, $42$ $4$ ’s and the center ( $7^{\text{th}}$ row and $7^{\text{th}}$ column) cell has color $1$ . But then this meets the demands of the $4 \times 1$ dominoes, so there is no contradiction. Any hint will be helpful. Do I need to do a different kind fo coloring?","['combinatorics', 'tiling', 'discrete-mathematics']"
3819559,Strange Sum of numbers $1$ to $100$,"I came across this problem the other day. It is as follows: The strange sum is as follows. Starting at the any term in a set, when the next term is added to the 'tally', the second term is subtracted from the first if the result is nonnegative, otherwise it is added to the tally. Then the third term is subtracted from the strange sum if the result is nonnegative, otherwise it is added to the 'strange sum'. Repeat the process.
For example, the 'strange sum' of the sequence $1, 3, 4, 2, 5$ is $1+3-4+2+5=7$ Suppose there is a list of numbers from $1$ to $100$ in some order. What is the largest possible 'strange sum'? I couldn't find any clear pattern. However, I did notice that if the sum of the first $99$ terms is $99$ , then $100$ can be added, hence making the total sum $199$ . Any thoughts on this problem? (edit) I just realised that $199$ is impossible as the sum of the first $99$ natural numbers is even and so it would be impossible to achieve a 'strange sum' of $99$ . That would mean that $198$ is the next highest possibility. $98$ is achievable via the 'strange sum' by the first $99$ natural numbers in some order, though I still have not been able to constitute a proof for $198$ .",['sequences-and-series']
3819585,Prove $\int_0^{\infty }\frac1{\sqrt{x}}\left(\frac{\cos(\pi x^2)}{\sinh (\pi x)}-\frac1{\pi x}\right)dx=\frac{1}{\sqrt{2}}\zeta(\frac{1}{2})$,"I encountered an astonishing integral (numerically verified) $$\int_0^{\infty } \frac{1}{\sqrt{x}}\left[\frac{\cos \left(\pi  x^2\right)}{\sinh (\pi  x)}-\frac{1}{\pi x}\right] \, dx=\frac{1}{\sqrt{2}}\zeta\left(\frac{1}{2}\right)$$ What technique should we use to establish it? Any help will be appreciated.","['integration', 'definite-integrals', 'complex-analysis', 'closed-form', 'riemann-zeta']"
3819630,Prove that $f$ is the identically zero function.,"Let $f : [0,1] \times [0,1] \longrightarrow [0,\infty)$ be a continuous function. Suppose that $$\int_{0}^{1} \left ( \int_{0}^{1} f(x,y)\ dy \right ) dx = 0.$$ Prove that $f$ is the identically zero function. My attempt $:$ $f$ is a non-negative measurable function on $[0,1] \times [0,1].$ So by Tonelli's theorem we have $$\iint_{[0,1] \times [0,1]} f(x,y)\ dx\ dy = \int_{0}^{1} \left ( \int_{0}^{1} f(x,y)\ dy \right ) dx = 0.$$ So if there exists $\textbf {x}_0= (x_0,y_0) \in [0,1] \times [0,1]$ such that $f(\textbf {x}_0) > 0,$ by continuity of $f$ at $\textbf {x}_0$ there exists an open ball $B( \textbf {x}_0, \delta)$ of some radius $\delta > 0$ surrounding $\textbf {x}_0$ with $B(\textbf {x}_0, \delta) \subseteq [0,1] \times [0,1]$ such that for any $\textbf {x} = (x,y) \in B(\textbf {x}_0, \delta)$ we have $$f(\textbf {x}) \gt \frac {f(\textbf{x}_0)} {2 \pi {\delta}^2}.$$ Since $f$ is a non-negative function so we must have $$\frac {f(\textbf {x}_0 )} {2} \lt \iint_{B(\textbf {x}_0, \delta)} f(x,y)\ dx\ dy \leq \iint_{[0,1] \times [0,1]} f(x,y)\ dx\ dy = 0$$ a contradiction. This shows that $f \equiv 0$ on $[0,1] \times [0,1].$ Can anybody plaese check my proof to ascertain whether it holds good or not? Thanks in advance.","['continuity', 'multivariable-calculus', 'solution-verification', 'multiple-integral', 'fubini-tonelli-theorems']"
3819653,Why do binomial expansions involving surds get closer to integers as they get larger? [duplicate],"This question already has answers here : Why is $(\sqrt{2}+\sqrt{3})^{2008}$ so close to an integer? (2 answers) Closed 3 years ago . Suppose I have a binomial expansion of the form: $$
(2+\sqrt{3})^n
$$ Why is it that as $n$ approaches $\infty$ that the value of the expansion becomes closer and closer to being an integer?","['elementary-number-theory', 'algebra-precalculus', 'binomial-theorem']"
3819658,"Calculate $\lim\limits_{(x,y)\to (0,0)} \dfrac{x^4}{(x^2+y^4)\sqrt{x^2+y^2}}$","Calculate, $$\lim\limits_{(x,y)\to (0,0)} \dfrac{x^4}{(x^2+y^4)\sqrt{x^2+y^2}},$$ if there exist. My attempt: I have tried several paths, for instance: $x=0$ , $y=0$ , $y=x^m$ . In all the cases I got that the limit is $0$ . But I couldn't figure out how to prove it. Any suggestion?","['multivariable-calculus', 'limits', 'calculus']"
3819738,"Clarification requested: Surface integrals, functions defined on surfaces and dimension","Surface integrals were always confusing for me, mainly because I never studied them carefully and in depth. After some research on several sources (not sufficient enough) I came across with the following questions/statements which I would highly appreciate if someone could explain/verify. Let's consider the unit sphere $\mathbb S^2=\{(x,y,z):\;x^2+y^2+z^2=1\}$ . A function $f$ defined on $\mathbb S^2$ is, in general, a function of $3-$ variables. The surface integral $\int_{\mathbb S^2} f\;dS$ though, can be calculated with an appropriate parametrization as a double integral (with respect to $2-$ coordinates). Is this correct? If yes, then how is this justified? Assume now that $f$ is a function defined on $\mathbb S^2$ as before but depends only on variable $x$ . If I want to integrate $f$ over the set $\{x>x_0\}$ , how would this integral look like? I've seen this formula $\int_{\{x>x_0\}} f\;dS=2\pi \int_{x_0}^1 f(x)\;dx$ which implies (if not mistaken) that there was some extra integration from $0$ to $2\pi$ that resulted in the $2\pi$ factor but I can't understand how this came up. I tried to use cylindrical coordinates but I don't obtain the desired formula. I'm having a really hard time getting my head around these notions and arguments. Any help, hint or comments are very much welcome. Thanks a lot in advance!","['multivariable-calculus', 'calculus', 'surface-integrals']"
3819799,Proving every element of a sequence defined by a recurrence relation is an integer,"Define the sequence $(a_n)_n$ as follows: $$
\begin{cases}
a_0 &= 1 \\
a_1 &= 2 \\
(n+3)a_{n+2}&=(6n+9)a_{n+1}-na_n
\end{cases}
$$ for $n \ge 0$ I am trying to prove that all terms of the sequence are integers. I have proved this using generating functions but it is long and messy. I want to know if there is a simpler proof that just uses elementary number theory. Perhaps an induction argument is possible?","['number-theory', 'recurrence-relations']"
3819803,"Does shortening the notation $n=1,2,3,...$ as $n\in\mathbb{N}$ lead to disambiguity?","I'm confused by this seemingly trivial question. The notation $$n=1,2,3,...$$ usually means 'where $n$ is any natural number', or 'For any $n\in\mathbb{N}$ '. What if I replace $n=1,2,3,...$ by $n\in\mathbb{N}$ , will it lead to disambiguation? For example, are notations (a) and (b) below are equivalent? $$
\text{Let } F  \text{ be a functional such that } F[x^n]=0, \qquad n=1,2,3,...,\tag{a}
$$ $$
\text{Let } F  \text{ be a functional such that }  F[x^n]=0, \qquad n\in\mathbb{N}.\tag{b}
$$ I think (b) might be understood also 'for just one fixed natural number $n$ '. So the notations are not equivalent in general?","['elementary-set-theory', 'notation', 'terminology']"
3819827,Householder QR derivation,"Im looking at the Householder operation, does anyone know how this guy got from the first equation to the second? $$\forall x, Px = x - \frac{2v(x^Tv)}{v^Yv} \implies  P = I-\frac{2vv^T}{v^Tv}$$ I know this is probably very simple but
I am very new to matrices and all their defined properties and operations",['matrices']
3819863,Find $\lim_{n\to\infty} n\int_1^e x^a(\log_ex)^ndx$,Find $\lim_{n\to \infty} n\int_1^e x^a(\log_ex)^ndx$ My idea: The reduction formula by integrating by parts doesn't seem to help so I substituted $\log_ex=t$ which gives $\int e^{t(a-1)}t^ndt$ . I'm not sure how to proceed further.,"['integration', 'limits', 'calculus', 'definite-integrals']"
3819891,Writing a sum in terms of an appropriate function,"I have a solution that is expressed as a series: $$
\sum_{k=0}^{\infty}\left[\frac{(-1)^k t^{2k+1}}{(2k+1)!}\right]\left[4^k\right]
$$ .. and would like to show it in terms of an appropriate function, for instance either in terms of $\sin(t), \cos(t), e^t$ , etc. What is frustrating me is that if only the multiplier (first bracket) were in the sum, as opposed to both
the multiplier and the multiplicand (second bracket), I could express the sum as $\sin(t)$ . However, since they are both in the sum, I cannot find an appropriate function. Does an appropriate function exist, and if so, what is it, and how did you find it? EDIT: It seems $\tanh(x)$ is pretty close match but the coefficients of the even terms are slightly off","['power-series', 'trigonometry', 'taylor-expansion', 'summation']"
3819936,"Consider the ODE $y'(x)=a(x)y(x),\; x>0, \;y(0)=y_{0}\neq 0$. Which of the following statements are true?","Assume that $a:[0,\infty) \to \mathbb{R}$ is a continuous function. Consider the ODE $$y'(x)=a(x)y(x),\quad x>0, \;y(0)=y_0 \neq 0$$ Which of the following statements are true? If $\int_{0}^{\infty}\vert a(x)\vert dx<\infty$ , then $y$ is bounded. If $\int_{0}^{\infty}\vert a(x)\vert dx<\infty$ , then $\lim_{x \to \infty}y(x)$ exists. If $\lim_{ x \to \infty}a(x)=1$ , then $\lim_{x \to \infty}\vert y(x) \vert=\infty$ If $\lim_{ x \to \infty}a(x)=1$ , then $y$ is monotone. My attempt: $f(x,y)=a(x)y(x)$ then $f(x,y)$ is continuous and $\frac{\partial f}{\partial y}=a(x)$ which is continuous at $(0,y_{0})$ . So, it follows that there exists unique solution using Picard uniqueness theorem. I didn't get how to explicitly solve for option 1 and 2. Also for option 3, i solved through an example. So, if $\lim_{ x \to \infty}a(x)=1$ , then choose $a(x)=1$ , clearly satisfies the hypothesis. Then the solution is $y=y_{0}e^x$ and it satisfies options $3$ and $4$ . But can someone please show me how to work on these questions explicitly, without using some examples etc? Thanks in advance. (Also can someone please edit the tags, there does not exists tag for uniqueness-existence of solution and the famous picard's theorem for existsence and uniqueness).",['ordinary-differential-equations']
3819940,Question in Proof of Riemann's theorem on removable singularities,"I have this from Shakarchi, in the proof of the 'Riemann's theorem on removable singularities. He states the following: .
He uses the following contour: and comes to the following conclusions about the smaller circle containing $z_{o}$ and so on. About the second integral he says: How is he arriving that the bounds is $\leq C\epsilon$ ?",['complex-analysis']
3820007,Extending the Burkholder-Davis-Gundy for continuous local martingales using localization,"Proposition 3.26 below is from Karatzas and Shreve's Brownian Motion and Stochastic Calculus, which gives a preliminary result of the Burkholder-Davis-Gundy Inequality. The proposition assumes that $M$ is a continuous martingale with $M$ and $\langle M \rangle$ bounded. The B-D-G inequality gives (3.26) for $m>0$ with only the requirement that $M$ be a continuous local martingale. Question: Remark 3.27 states that a straightforward localization argument shows that (3.27) and (3.29) are valid for any continuous local martingale $M$ . Indeed, we could consider the  stopping time $T_n= \inf \{t\ge 0: |M_t| + \langle M \rangle_t \ge n\}$ , which tends to $\infty$ and gives that $(M^{T_n}_t)_{t\ge 0}$ is a bounded martingale. I can see that then by taking $n \to \infty$ and using monotone convergence, we would get (3.27) and (3.29) for $M \in \mathscr{M}^{c,loc}$ , however, I don't understand why we wouldn't get (3.28) without the additional condition $E(\langle M \rangle_T^m)<\infty$ . Why do we require this condition? In fact, as we can see from the last bit of the proof of 3.26, we get (3.29) from (3.27) and (3.28), so I can't figure out why we wouldn't just get (3.28) for continuous local martinagles without this additional condition.","['stochastic-integrals', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
3820012,"Find all the integer pairs $(x, y)$ which satisfy the equation $x^5-y^5=16xy$","I just came across the following question: Find all the integer pairs $(x, y)$ which satisfy the equation $x^5-y^5=16xy$ I solved it as follows: $x=y=0$ obvious solution. If $xy\neq0$ , let $d=gcd(x, y)$ and we write $x=da$ , $y=db$ , $a, b\in \Bbb{Z}$ with $(a, b)=1$ . Then, the given equation is: $$d^3a^5-d^3b^5=16ab$$ So, by the above equation, $a$ divides $d^3b^5$ and hence $a$ divides $d^3$ . Similarly $b$ divides $d^3$ . Since $(a, b)=1$ we have that $ab$ divides $d^3$ , so $d^3=abr$ with $r\in \Bbb{Z}$ . Then the above equation becomes $abra^5-abrb^5=16ab$ , so $r(a^5-b^5)=16$ . Hence, the difference $a^5-b^5$ must divide $16$ . If $|(a^5-b^5)|\le2$ we have that $(x, y)=(-2, 2)$ is a solution. Otherwise $$|a^5-b^5|=|(x+1)^5-b^5|\ge |(x+1)^5-x^5|=|5x^4+10x^3+10x^2+5x+1|\ge31$$ which is impossible. So only solutions are $(x, y)=(0, 0)$ or $(-2, 2)$ . I believe that this solution is not at all intuitive nor simple. Could you please post a more intuitive and simple solution where you are explaining your intuition on every step?","['contest-math', 'number-theory', 'intuition', 'inequality', 'problem-solving']"
3820112,Prove that Chebyshev's inequality is not sharp,"Problem: Let $(\Omega,\mathcal F,\mu)$ be a probability space and $X$ a random variable on this space. Prove that if $0<E[X^2]<\infty$ , then $$\lim_{a\to\infty}\frac{a^2P(\vert X\vert\geq a)}{E[X^2]}=0.$$ My Attempt: Observe that $$a^2P(\vert X\vert\geq a)=\int_\Omega a^2\cdot\mathbf1_{\{\vert X\vert\geq a\}}\,d\mu\leq\int_{\left\{\vert X\vert^2\geq a^2\right\}}\vert X\vert^2\,d\mu.$$ Therefore, it suffices to prove that $$\lim_{a\to\infty}\int_{\left\{\vert X\vert^2\geq a^2\right\}}\vert X\vert^2\,d\mu=0.$$ To see this, take a sequence of positive real numbers $\{a_n\}_{n=1}^\infty$ with $a_n^2\nearrow\infty$ . Since the sets $\{\vert X\vert^2\geq a_n^2\}$ and $\{\vert X\vert^2<a_n^2\}$ are disjoint and their union is $\Omega$ , it follows from the properties of the Lebesgue integral that for all $n\in\mathbb N$ we have $$\int_{\left\{\vert X\vert^2\geq a_n^2\right\}}\vert X\vert^2\,d\mu=\int_\Omega\vert X\vert^2\,d\mu-\int_{\left\{\vert X\vert^2<a_n^2\right\}}\vert X\vert^2\,d\mu.$$ Now consider the sequence of functions defined by $X_n=\vert X\vert^2\cdot\mathbf1_{\{\vert X\vert^2<a_n^2\}}$ . Then $X_n\to \vert X\vert^2$ almost everywhere and $\vert X_n\vert\leq\vert X\vert^2$ also almost everywhere. Therefore, the dominated convergence theorem implies that $$\int_\Omega\vert X\vert^2\,d\mu-\lim_{n\to\infty}\int_{\left\{\vert X\vert^2<a_n^2\right\}}\vert X\vert^2\,d\mu=0.$$ Thus, $$\lim_{n\to\infty}\int_{\left\{\vert X\vert^2\geq a_n^2\right\}}\vert X\vert^2\,d\mu=0.$$ It follows that $$\lim_{a\to\infty}\int_{\left\{\vert X\vert^2\geq a^2\right\}}\vert X\vert^2\,d\mu=0,$$ and hence $$\lim_{a\to\infty}\frac{a^2P(\vert X\vert\geq a)}{E[X^2]}=0.$$ Could anyone give me some feedback on my proof above? Any comments are much appreciated. Thank you for your time.","['measure-theory', 'solution-verification', 'probability-theory', 'real-analysis']"
3820123,"What is ""it is not the case"" in logic","I am having a doubt in the following question. Suppose p is the statement 'You need a credit card' and q is the statement 'I have a
nickel.' Select the correct statement corresponding to the symbols ~(p∨q). A. You don't need a credit card and I have a nickel. B. It is not the case that either you need a credit card or I have a nickel. C. You don't need a credit card or I have a nickel. D. None of these Applying De Morgan's laws would lead to this statement: You don't need a credit card and I don't have a nickel, which would make one select option D. However, in the answer key, it is given option B. Can someone please explain how to get that answer. Any help is appreciated","['logic', 'discrete-mathematics']"
3820135,Dimension of $r$-jets of maps from manifolds $M$ to $N$,"Differential Topology Hirsch Chapter 2 Section 4 Problem 11: Compute the Dimension of $J^r(M, N)$ $J^r(M, N)$ is the set of all $r$ -jets from $M$ to $N$ . This is an equivalence class $[x, f, U]_r$ of triples $(x, f, U)$ , where $U \subset M$ is an open set, $x \in U$ , and $f: U \rightarrow N$ is a $C^r$ map; the equivlence relation is: $[x, f, U]_r = [x' f', U']_r$ if $x = x'$ and in some (hence any) pair of charts adapted to $f$ at $x$ , $f$ and $f'$ have the same derivatives up to order $r$ . I wanted to check to see if this was right: $J^r(M, N)$ seems to me to only distinguish among different points and functions whose derivatives differ at order $r+1$ and up. Since $x \in M$ we know that the dimension of $J^r(M, N)\geq dim M$ . Now we just have to figure out the dimension of all functions that differ at order $r+1$ and up and add it to dim $M$ ? I can only think that this is infinite...","['function-spaces', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3820145,True random number set,"Consider the double-slit experiment. If we measure position of electron on the screen multiple times, we get different results. But if we repeat the experiment many times we obtain a nice histogram which converges (smoothens) when more experiments are done. So the numbers obtained by this random process are not truly random. By the above statement I mean that the set of all numbers is giving a smooth histogram. If the laws of the universe are not assumed to be that nice, we can imagine a scenario where the histogram doesn't actually converge but keeps changing. How can I make this idea mathematically clear? The probability courses I've studied always assume a distribution for numbers before doing anything. To elucidate what I'm thinking, consider this example. I could be flipping coins and find after 10 flips I had gotten 85% heads, then after 100 flips I had gotten 65% heads, after 1000 flips I had gotten 95% heads, after 10,000 flips I had gotten 15% heads etc. I want to talk about the situation where the fraction doesn't converge. I apologize if the question is vague. Even I'm not clear with what exactly I'm looking for, hence I'm asking if there's some mathematical treatment of probability where the histogram doesn't converge. If the question is not suitable for this site, can someone please suggest the correct stack site.","['logic', 'probability-theory']"
3820156,"bijection $\{0, 1, \ldots, \binom{|m|}{n}-1\} \longleftrightarrow \binom{m}{n}$","I am attempting to generate random numbers which correspond to states in a game. Say there are $|m|$ squares, and $n$ pieces which can occupy the squares. Then I can have a total of $\binom{|m|}{n}$ states, given the pieces are indistinct. However, I would like to transform the randomly generated number $\left[0, \binom{|m|}{n}\right)$ into a specific state, and for this I need a bijection $\{0, 1, \ldots, \binom{|m|}{n}-1\}$ $\longleftrightarrow$ $\binom{m}{n}$ . An algorithm that I can think of (that may work) would in essence order the pieces, and place the $i^\text{th}$ piece on the $i^\text{th}$ square to start. Then iterate over the pieces, shifting the rightmost piece right if the square is not occupied. Whenever a piece $p_0$ to the left of any other given pieces $p_1, \ldots, p_k$ is shifted right, then shift $p_1, \ldots p_k$ left as far as possible such that $p_i$ is still on a lesser square than $p_j$ . I would appreciate guidance in any other directions or any simplifications etc! Thanks!","['elementary-set-theory', 'algorithms']"
3820186,The orthogonal of a set in the Hilbert space $\ell_2$,"Consider the sequences Hilbert space of complex numbers $\quad \ell_2=\{x=(x_k)_{k\in \mathbb{N}^*} \quad|\quad  \sum_{k=1}^{+\infty} |x_k|^2<\infty \}$ with the inner product $<x,y>=\sum_{k=1}^{+\infty} x_k\overline{y_k}$ Let $F$ be the set $F=\{x=(x_k)_{k\in \mathbb{N}^*} \in \ell_2  \quad|\quad  \sum_{k=1}^{+\infty} x_k=0 \}$ What is $F^\perp$ ? I tried to pose $f:\ell_2 \to \mathbb{C}$ st $f(x)=\sum_{k=1}^{+\infty} x_k$ but the problem is that $f$ is not well definied for instance for the harmonic sequence $(1/k)_k$ Second attempt is write $\sum_{k=1}^{+\infty} x_k=<x,y>$ st $y=(1,1,...)$ but again $y$ is not in $\ell_2$","['inner-products', 'complex-analysis', 'hilbert-spaces', 'functional-analysis', 'sequences-and-series']"
3820207,Eigenvalues and matrix kronecker product,"I'm not able to understand why this equivalences are true for the kronecker product of a matrix and why a the eigenvalues of a kronecker product of two matrixes are the product of their eigenvalues. The book goes like this: For $A_{nxn}$ matrix with (possibly nondistinct) eigenvalues ( $\lambda_{1},\lambda_{2},...\lambda_{n}$ ) and $B_{pxp}$ with eigenvalues ( $\mu_{1},\mu_{2},...\mu_{p}$ ) then (np) eigenvalues of $A\otimes B$ are given by $\mu_{j}\lambda_{i}$ for i= 1,2,....n and j=1,2,3....p. To see this write $A$ and $B$ in their jordan form: $A=M_{A}J_{A}M^{-1}_{A}$ $B=M_{B}J_{B}M^{-1}_{B}$ Then $M_{A}\otimes M_{B}$ has inverse given by $M^{-1}_{A}\otimes M^{-1}_{A}$ Moreover, we know that the eigenvalues of $A\otimes B$ are the same as the eigenvalues of: $(M_{A}\otimes M_{B})(A\otimes B)(M^{-1}_{A}\otimes M^{-1}_{A})= (M_{A}AM^{-1}_{A})\otimes (M_{B}BM^{-1}_{B})=J_{A}\otimes J_{B}$ Can someone explain to me why: $(M_{A}\otimes M_{B})(A\otimes B)(M^{-1}_{A}\otimes M^{-1}_{A})= (M_{A}AM^{-1}_{A})\otimes (M_{B}BM^{-1}_{B})$ is true? Thanks in advance.","['matrices', 'kronecker-product', 'matrix-decomposition']"
3820235,Prove that $\lim_{n\to\infty}n^2\int_0^{\frac{1}{n}}x^{x+1}dx=\frac{1}{2}.$,"Question: Prove that $$\lim_{n\to\infty}n^2\int_0^{\frac{1}{n}}x^{x+1}dx=\frac{1}{2}.$$ Solution: Let $$I_n:=n^2\int_0^{\frac{1}{n}}x^{x+1}dx, \forall \in\mathbb{N}.$$ Substituting $nx=t$ in $I_n$ , we have $$I_n=n\int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt.$$ Now for all $0\le t\le 1$ and for all $n\in\mathbb{N}, n+t\le n+1\implies 1+\frac{t}{n}\le1+\frac{1}{n}.$ This implies that for all $0\le t\le 1$ and for all $n\in\mathbb{N}$ , we have $$\left(\frac{t}{n}\right)^{1+\frac{t}{n}}\ge \left(\frac{t}{n}\right)^{1+\frac{1}{n}}.$$ Therefore, for all $n\in\mathbb{N},$ $$\int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt\ge \int_0^1\left(\frac{t}{n}\right)^{1+\frac{1}{n}}dt=n^{-\left(1+\frac{1}{n}\right)}\frac{n}{2n+1}.$$ This implies that $$I_n\ge n^{-\frac{1}{n}}\frac{n}{2n+1},\forall n\in\mathbb{N}.$$ Next note that for all $0\le t\le 1$ and for all $n\in\mathbb{N}$ , $1+\frac{t}{n}>1$ , which implies that $\left(\frac{t}{n}\right)^{1+\frac{t}{n}}<\frac{t}{n}.$ Therefore, $$\int_0^1\left(\frac{t}{n}\right)^{1+\frac{t}{n}}dt<\int_0^1\left(\frac{t}{n}\right)dt=\frac{1}{2n}.$$ This implies that $$I_n<\frac{1}{2},\forall n\in\mathbb{N}.$$ Thus, for all $n\in\mathbb{N}$ , we have $$n^{-\frac{1}{n}}\frac{n}{2n+1}\le I_n<\frac{1}{2}.$$ Now since $$\lim_{n\to\infty}n^{-\frac{1}{n}}\frac{n}{2n+1}=\frac{1}{2},$$ therefore by Sandwich theorem we can conclude that $$\lim_{n\to\infty}I_n=\frac{1}{2}.$$ Is this solution correct and rigorous enough and is there any other way to solve the problem?","['integration', 'solution-verification', 'definite-integrals', 'real-analysis']"
3820251,Are there variations of Ramaswami's formula for the analytic continuation of the Riemann zeta function?,"On p. 286 of Borwein's paper entitled ""Computational Strategies for the Riemann zeta function"", the author mentions a formula due to Ramaswami: $$(1-2^{1-s})\zeta(s) = \sum_{n=1}^{\infty} \binom{s+n-1}{n}\zeta(s+n). $$ I wonder whether variations of this identity also exist. For instance, are there similar binomial sums for $$(1-a^{1-s})\zeta(s) $$ for $a \in \mathbb{Z}\setminus\{2\}$ , or is there something special about $a=2$ that makes it work? And what about products like $$\zeta(s) \prod_{k=1}^{p} (1-a_{k}^{1-s})$$ for some sequence $a_{1}, \dots, a_{p} \in \mathbb{Z}$ , does that expression equal any binomial sum(s) in terms of values of the Riemann zeta function? I've corrected some typos. The $2^{-s}$ , $a^{-s}$ , and $a_{k}^{-s}$ factors should have been $2^{1-s}$ , $a^{1-s}$ , and $a_{k}^{1-s}$ , respectively. Also, the answer to this question can be found here on MO.","['analytic-number-theory', 'binomial-coefficients', 'combinatorics', 'sequences-and-series', 'riemann-zeta']"
3820373,How does this optimal classifier make sense in case of continuous random variable?,"I'm reading about The Bayes Problem in textbook A Probabilistic Theory of Pattern Recognition by Devroye et al. They make use of $\eta(x)=\mathbb{P}\{Y=1 \mid X=x\}$ throughout the proof. In my understanding, the conditional probability $\eta(x)=\mathbb{P}\{Y=1 \mid X=x\}$ is defined only when $\mathbb P \{X=x\} > 0$ . If $X$ is continuous, for example, $X$ follows normal distribution, then $\mathbb P[X=x]=0$ for all $x \in \mathbb R$ . Then $\eta(x)$ is undefined for all $x \in \mathbb R$ , confusing me. Could you please elaborate on this point?","['conditional-probability', 'proof-explanation', 'machine-learning', 'pattern-recognition', 'probability-theory']"
3820481,"If $(a_n)$ is a sequence such that $a_n=a_{f(n)}+a_{g(n)}$, where $\lim \frac{f(n)}{n}+\lim\frac{g(n)}{n}<1$, can we claim that $\lim\frac{a_n}{n}=0$?","The inspiration for this question came with an attempt to solve this . Let $(a_n)_{n\in\Bbb{N}}$ be a sequence of real numbers satisfying $a_n = a_{f(n)} + a_{g(n)}~\forall n\in\Bbb{N}$ , where $f, g: \Bbb{N}\rightarrow \Bbb{N}$ are functions such that $\begin{aligned}\lim \frac{f(n)}{n}+\lim\frac{g(n)}{n}<1\end{aligned}$ . Can we claim that $\begin{aligned}\lim\frac{a_n}{n} = 0\end{aligned}$ (i.e., that this sequence grows slower than any linear function)? I know the statement is true if we assume that the limit exists. In fact, if $\begin{aligned}\lim\frac{a_n}{n} = \alpha\end{aligned}$ , then $$\alpha = \lim\frac{a_n}{n} = \lim \frac{a_{f(n)}+a_{g(n)}}{n} = \lim \Big(\frac{f(n)}{n}\frac{a_{f(n)}}{f(n)}+\frac{g(n)}{n}\frac{a_{g(n)}}{g(n)}\Big) = \alpha\Big( \lim \frac{f(n)}{n}+\lim\frac{g(n)}{n}\Big)$$ Because $\begin{aligned}\lim \frac{f(n)}{n}+\lim\frac{g(n)}{n}\neq 1\end{aligned}$ we have $\alpha = 0$ . Therefore, it remains to be seen whether the limit really exists. My ideia was to try to show that the sequence $\begin{aligned}\Big(\frac{a_n}{n}\Big)_{n\in\Bbb{N}}\end{aligned}$ is decreasing from a certain point, but I don't know how to proceed.","['limits', 'inequality', 'sequences-and-series']"
3820511,"If $a^2+b^2-ab=c^2$ for positive $a$, $b$, $c$, then show that $(a-c)(b-c)\leq0$","Let $a$ , $b$ , $c$ be positive numbers. If $a^2+b^2-ab=c^2$ . Show that $$(a-c)(b-c)\leq0$$ I have managed to get the equation to $(a-b)^2=c^2-ab$ , but I haven't been able to make any progress. Can someone help me?","['number-theory', 'algebra-precalculus', 'provability', 'inequality']"
3820517,How to find the placement of an ellipse inside a scalene quadrilateral where 4 points are tangent?,"I am an artist and want a more formulaic way of finding where to place perspective circles (ellipses) inside perspective squares in 3-point perspective (quadrilaterals where none of the sides are parallel). I asked another question regarding quadrilaterals where two sides are parallel (trapezoids) here . Here is an example of an ellipse inside a quadrilateral. Points E2 and D2 are the Foci. Point H2 is the 'perspective' center of the quadrilateral. The four tangent points must be the perspective centers of each side (Line I2, U1 & the line intersecting H2 that isn't I2, U1). How do I find where to place an ellipse inside a scalene quadrilateral such that the ellipse is tangent to the 4 perspective centers of each side of the quadrilateral?","['projective-geometry', 'conic-sections', 'geometry']"
3820545,How do I find solutions to $2^n+11 \equiv 0 \pmod n$?,"In recent days, I have been studying the equation $m^n+h \equiv 0 \pmod n$ where $m,n \in \mathbb N$ and $h\in\mathbb Z $ , and I have noticed that $2^n+11 \equiv 0 \pmod n$ have no solutions in $1 \leq n \leq 2000000000$ except for $1$ and $13$ . How can I find other solutions that satisfies the equation, or prove otherwise that no solutions $>13$ exists?","['number-theory', 'modular-arithmetic']"
3820596,Proof that $f^{-1}(\bigcap\limits_{\mu \in M} B_{\mu}) = \bigcap\limits_{\mu\in M}f^{-1}(B_{\mu})$,"Consider $f\colon A\to B$ a function and $(B_\mu)_{\mu \in M}$ a family of subsets of $B$ . I have to prove that $f^{-1}(\bigcap\limits_{\mu \in M} B_{\mu}) = \bigcap\limits_{\mu\in M}f^{-1}(B_{\mu})$ and $f^{-1}(\bigcup\limits_{\mu \in M} B_{\mu}) = \bigcup\limits_{\mu \in M}f^{-1}(B_{\mu})$ . Instead of proving $A\subseteq B$ and then proving $B\subseteq A$ (for any sets $A,B$ ), I've decided to use $\iff$ all along. $f^{-1}(\bigcap\limits_{\mu \in M} B_{\mu}) = \bigcap\limits_{\mu\in M}f^{-1}(B_{\mu})$ \begin{align*}
        x\in f^{-1}(\bigcap\limits_{\mu \in M} B_{\mu}) &\iff f(x)\in B_\mu \ \text{for all} \ \mu \in M\\
        & \iff \exists x\in A \ \text{such that} \ f(x)\in B_\mu \ \text{for all} \ \mu \in M\\
        &\iff x\in f^{-1}(B_\mu) \ \text{for all} \ \mu \in M\\
        &\iff x\in \bigcap_{\mu\in M} f^{-1}(B_\mu)
\end{align*} $f^{-1}(\bigcup\limits_{\mu \in M} B_{\mu}) = \bigcup\limits_{\mu \in M}f^{-1}(B_{\mu})$ \begin{align*}
        x\in f^{-1}(\bigcup_{\mu \in M} B_{\mu}) &\iff f(x)\in B_\mu \ \text{for some} \ \mu \in M\\
        & \iff \exists x\in A \ \text{such that} \ f(x)\in B_\mu \ \text{for some} \ \mu \in M\\
        &\iff x\in f^{-1}(B_\mu) \ \text{for some} \ \mu \in M\\
        &\iff x\in \bigcup_{\mu\in M} f^{-1}(B_\mu)   
\end{align*} They seemed too alike to me, which felt strange. Any correction or proof-writing tip is obviously appreciated.","['elementary-set-theory', 'solution-verification']"
3820623,"A differentiable function on $(a,b)$ with nonzero derivative over $(a,b)$ where $f'(c)>0$ for some $c\in(a,b)$ means $f'(x)>0$ for all $x\in(a,b)$.","It certainly seems correct that if you have at least one point $c\in(a,b)$ such that $f'(c)>0$ , where $f'(x)\ne0$ for all $x\in(a,b)$ , then $f'(x)>0$ for all $x\in(a,b)$ . At least from a calculus standpoint, if $f$ is well-defined (not necessarily continuous), it should be the case that this is true. But how does one show this formally? The exact statement I am attempting to prove is this: Suppose $f:(a,b)\to\mathbb{R}$ is a differentiable function such that $f'(x)\ne0$ for all $x\in(a,b)$ . Suppose there exists a point $c\in(a,b)$ with $f'(c)>0$ , then $f'(x)>0$ for all $x\in(a,b)$ . I thought it might be easiest to prove by contradiction, and then use the intermediate value property to derive a contradiction. Any pointers would be helpful on this one! EDIT: My original direct proof went something like this: Let $f:(a,b)\to\mathbb{R}$ be a differentiable function such that $f'(x)\ne0$ for all $x\in(a,b)$ . Take a point $c\in(a,b)$ such that $f'(c)>0$ . Since $f'(x)\ne0$ for any $x\in(a,b)$ there can be no extreme values on $(a,b)$ . Then take some $y\in\mathbb{R}$ such that $f'(c)=y$ and by the intermediate value property, we are guaranteed that $f'(a)<y<f'(b)$ , that is $f$ is strictly increasing on $(a,b)$ . Hence $f'(x)>0$ for all $x\in(a,b)$ . I'm a little worried about some of the logic in this. It feels incomplete to me.","['derivatives', 'real-analysis']"
3820645,Using algebraic isomorphisms to define a topology if one of the algebraic objects has a topology?,"Given an $n$ dimensional vector space $V$ , we know it is isomorphic to $\mathbb{R}^n$ . Can we not use this isomorphism to define a topology on $V$ ?... If you want to know if a set of elements in $V$ is closed or open, just look at the image of these elements under the isomorphism; if that set is open/closed, so is its preimage. I guess you'd have to pick a basis for $V$ first, but still. Is this right? In general, say we have a topological algebra $T$ and an algebra $A$ , and an isomorphism between them. Can't we use the same argument above to use the topology on $T$ to define a topology on $A$ ? Thanks!!","['general-topology', 'abstract-algebra', 'soft-question', 'vector-spaces']"
3820653,Functional equation $f(x+y)=f(x)f(y)$ for complex-valued $f$,"It is well known that the only continuous functions $f: \mathbb{R} \to \mathbb{R}$ which satisfy the equation $$f(x+y)=f(x)f(y)$$ are the exponential functions $f(x)=a^x$ . I am trying to prove a similar result when we allow $f$ to take complex values, i.e. if $f$ is a continuous function from $\mathbb{R}$ to $\mathbb{C}$ which satisfies $f(x+y)=f(x)f(y)$ for all $x,y \in \mathbb{R}$ , then there exists some $z\in \mathbb{C}$ such that $f(x)=e^{zx}$ . However, I am totally stuck after attempting a few different methods. In real analysis, the steps towards proving this result is: (1) Use induction to show that $f(x)=a^x$ is true for all $x\in \mathbb{Z}$ ; (2) Use $f(1)=(f(\frac{1}{n}))^n$ to show that $f(\frac{1}{n})=(f(1))^{\frac{1}{n}}$ , hence $f(x)=a^x$ is true for all $x\in \mathbb{Q}$ ; and (3) Use continuity to extend this to all $x\in \mathbb{R}$ . However, when we allow $f$ to take value in $\mathbb{C}$ , it seems that we're stuck at step (2), since every complex number has n distinct complex n-th roots and we do not know which one to choose for $f(\frac{1}{n})$ . We do have the additional assumption of continuity, but I don't quite see how it can be employed in this step. Any help will be appreciated.","['complex-analysis', 'functional-equations', 'exponential-function']"
3820656,Most groups are noncommutative,"From page 41 of Evan Chen's napkin it states that most groups are noncommutative. This led me to think about an unconventional question: let $$C_n:=\text{number of nonisomorphic abelian groups of order }n$$ $$G_n:=\text{number of nonisomorphic groups of order }n.$$ Are there any known results on $\limsup_{n\rightarrow\infty}\frac{C_n}{G_n}$ or $\liminf_{n\rightarrow\infty}\frac{C_n}{G_n}$ ? Notice from here , for $p$ prime we have $C_{p^3}/G_{p^3}=3/5$ , so we can deduce $$\liminf_{n\rightarrow \infty}\frac{C_n}{G_n}\leq\frac{3}{5}$$ and also since $C_{p^2}/G_{p^2}=1$ , we have $$1\leq \limsup_{n\rightarrow \infty}\frac{C_n}{G_n}.$$ My question is, can these bounds be improved, and is determining the exact value possible? Edit: I realize that since $C_n/G_n\leq 1$ for all $n$ , we obviously have $\limsup_{n\rightarrow\infty}C_n/G_n=1$ . So forget about that.","['group-theory', 'abstract-algebra']"
3820662,Probability that amoeba population dies out completely - why can't the probability be 1?,"Consider an amoeba. At each iteration, it can split into 2, 3 amoebas, stay the same, or die. These 4 events occur with equal probability. Let $E$ denote the event that all current amoeba dies, and $F_1, F_2, F_3, F_4$ denote the above 4 events, then we have \begin{align}
    P(E) = P(E|F_1)P(F_1) + P(E|F_2)P(F_2) + P(E|F_3)P(F_3) + P(E|F_4)P(F_4) \\
    P(F_i) = \frac{1}{4} \\
    P(E|F_1) = 1 \\
    P(E|F_2) = P(E)^2 \\
    P(E|F_3) = P(E)^3 \\
    P(E|F_4) = P(E) \\
    P(E) = \frac{1}{4}[1 + P(E) + P(E)^2 + P(E)^3] \\
\end{align} Solving this cubic, we find $P(E) = 1, 1 \pm \sqrt{2}$ . We throw out the negative root, and we are left with $P(E) = 1, \sqrt{2} - 1$ . In the book I am reading, it restricts the probabilities to $P(E) < 1$ . But intuitively, I do not understand why $P(E) = 1$ isn't possible. Can someone intuitively explain this? In addition, using $P(E) = \sqrt{2} - 1$ , does this mean that the expectation of the number of iterations that the amoeba population dies out is infinity? One thought that occurred to me is that the expected number of amoeba after one iteration is $0.25(1 + 2 + 3 + 0) = 1.5$ , and by induction we see that this continues to grow.","['expected-value', 'probability']"
3820670,The sequence $A_n=\prod_{k=1}^n\left(1+\frac{k}{n^2}\right)$ is decreasing,"Let $A$ be the sequence of real numbers defined by : $$\forall n\in\mathbb{N}^\star,\,A_n=\prod_{k=1}^n\left(1+\frac{k}{n^2}\right)$$ I know how to prove that this sequence converges to $\sqrt e$ , using the following inequalities : $$\forall t>0,\,t-\frac{t^2}2\leqslant\ln(1+t)\leqslant t$$ I found numerical evidence that $(A_n)$ is decreasing, but wasn't able to prove it. Any help will be appreciated.","['products', 'inequality', 'sequences-and-series', 'real-analysis']"
3820684,Find the probability that $[x+y+z]=[x]+[y]+[z]+2$,"Find the probability that the equation $[x+y+z]=[x]+[y]+[z]+2$ is true, where $x,y,z \in R$ . [.] Represents the greatest integer function. I got two different answers by two different methods. 1st method: $x=[x]+\{x \}$ etc in LHS, where {} is the fractional part function. So $[\{x \}+\{y \}+ \{z \}]=2$ So $\{x \}+\{y \}+ \{z \}$ is between $[2,3)$ . All $\{x \},\{y \},\{z \}$ are between $[0,1)$ and are uniformly distributed in this interval. So if we consider the ""expectation"" instead of actual probability.
The expectation that $\{x \},\{y \},\{z \}$ is between $[2,3)$ . = Expectation that $3\{x \} $ is between $[2,3)$ . = Expectation that ${x}$ is between $[2/3,1)$ $= (1-\frac{2}{3})/1 = \frac{1}{3}$ (Due to uniform distribution of {x}). Can we call this the final required probability? Method 2: consider a unit cube with vertices $(0,0,0),(1,0,0),(0,1,0),(0,0,1),(1,1,0),(1,0,1),(0,1,1),(1,1,1)$ and the plane $x+y+z=2$ . The required probability (of $\{x \}+\{y \}+ \{z \}$ is between $[2,3)$ ). is the volume of the cube cut out of the plane(not including the origin) /volume of cube = volume of tetrahedron with vertices $(1,1,1),(1,1,0),(1,0,1),(0,1,1)$ /1 = $\frac{1}{6}$ . Which method is correct (if at all any) and is there any other way to solve this question?","['ceiling-and-floor-functions', 'solution-verification', 'probability']"
3820737,When are eight integers entirely determined by their pairwise sums?,"Alice picks 8 numbers, which are not necessarily different. Once she has picked them, she writes out the addition of all the pairs on a piece of paper, which she gives to Basil. Basil wins if he can guess correctly the original n numbers, which Alice chose. Can Basil be certain that he will win? After a lot of trial and error I found the case where Alice picks the numbers $1,5,7,9,12,14,16,20$ which have the same pairwise sums as the numbers $2,4,6, 10,11,15,17,19$ . However the trial and error method is extremely laborious and tedious. Is there a more mathematical approach, which can immediately give you the solution?","['contest-math', 'elementary-number-theory', 'combinatorics', 'problem-solving']"
3820768,"Prove that $f(z)=\frac{1}{2\pi i}\int_{\Gamma}\frac{f(\zeta)}{z-\zeta}\ d\zeta,$ for all $z\in\mathbb{C}$ with $|z| \gt 2.$ [duplicate]","This question already has an answer here : $f(z)=\frac1{2\pi i}\int_{\Gamma}\frac{f(\zeta)}{z-\zeta}d\zeta$ if $\lim_{z\to\infty}f(z)=0$ and $f$ is analytic in $|z|\ge1$ (1 answer) Closed 3 years ago . Let $\Gamma$ denote the positively oriented circle of radius $2$ with center at the origin. Let $f$ be an analytic function on $\{z\in\mathbb{C}\ |\ |z| \gt 1\},$ and let $$\lim_{z\to\infty} f(z)=0.$$ Prove that $$f(z)=\dfrac{1}{2\pi i}\int_{\Gamma}\dfrac{f(\zeta)}{z-\zeta}\ d\zeta$$ for all $z\in\mathbb{C}$ with $|z|>2.$ By the given condition it is clear that $\lim\limits_{z \to 0} f \left ( \dfrac 1 z \right ) = 0.$ Now define a function $g : B(0,1) \longrightarrow \Bbb C$ by $$
g(z) = \left\{
        \begin{array}{ll}
            f \left (\dfrac 1 z \right ) & \quad 0 \lt |z| \lt 1  \\
            0 & \quad z = 0
        \end{array}
    \right.
$$ Then $g$ is analytic on $B(0,1).$ Let $\Gamma' (t) = \frac {1} {\Gamma (t)},$ where $t$ varies over the parameter interval of $\Gamma.$ Then $\Gamma'$ is a circle of radius $\frac {1} {2}$ traversing in the clockwise direction. Since $g$ is analytic on and inside of $\Gamma'.$ So by Cauchy's integral theorem it follows that for all $z \in \Bbb C$ with $|z| \lt \frac {1} {2}$ we have $$g(z) = - \dfrac {1} {2 \pi i} \int_{\Gamma'} \dfrac {g(\zeta)} {\zeta - z}\ d\zeta.$$ Hence for all $z \in \Bbb C$ with $|z| \gt 2$ we have $$f(z) = g \left ( \frac 1 z \right ) = - \dfrac {1} {2 \pi i} \int_{\Gamma'} \dfrac { g (\zeta) } {\zeta - \frac 1 z}\ d\zeta.$$ Can it be shown that $$-\displaystyle { \int_{\Gamma'} \dfrac {g(\zeta)} {\zeta - \frac 1 z}\ d\zeta = \int_{\Gamma} \dfrac {f(\zeta)} {z - \zeta}\ d\zeta}$$ for all $z \in \Bbb C$ with $|z| \gt 2\ $ ? Any help in this regard will be highly appreciated. Thanks in advance.","['complex-analysis', 'proof-writing', 'cauchy-integral-formula']"
3820770,Proving the cross product matrix tranformation identity with an alternative solution,"I'm solving a problem from the book, Mathematics for 3D Game Programming and Computer Graphics, Third Edition , by Eric Lengley. The problem goes: Let $N$ be the normal vector to a surface at a point $P$ , and let $S$ and $T$ be tangent vectors at the point $P$ such that $S \times T = N$ . Given an invertible 3 $\times$ 3 matrix $M$ , show that $(MS) \times (MT) = (\text{det}M(M^{-1})^{T}(S \times T)$ , supporting the fact that normals are correctly transformed by the inverse transpose of the matrix $M$ . The author provided a hint stating we can represent $(MS) \times (MT)$ as $$
(MS) \times (MT) =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
MT
$$ We then find a matrix $G$ such that $$
GU
=
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
$$ where $$
U =
\begin{bmatrix}
0 & -S_{z} & S_{y} \\ 
S_{z} & 0 & -S_{x} \\ 
-S_{y} & S_{x} & 0 
\end{bmatrix}
$$ and show that $G = (\text{det}M)(M^{-1})^{T}$ to solve the problem. I am aware that there is an alternative solution to this problem, but I would like to solve it through the hints provided. Unfortunately, I am only able to go as far doing: $$
G =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
U^{-1}
$$ At this point, I do not know how to proceed with showing that $G = (\text{det}M)(M^{-1})^{T}$ . How would you proceed? I'd like to ask for hints on solving the problem.","['matrices', 'transformation']"
3820824,An exercise from High Dimensional Probability,"Given a stochastic process $\{X_t\}_{t \in T}$ which has zero-mean, $\textit{i.e.}$ $\mathbb{E}(X_t) = 0$ for all $t\in T$ . Suppose $\{Y_t\}_{t \in T}$ is an independent copy of $\{X_t\}_{t \in T}$ . Prove the following statement is true: $$
\mathbb{E}\sup_{t\in T}[X_t - Y_t] \le 2\mathbb{E}\sup_{t \in T}X_t.
$$ I was doing exercise $7.1.9$ in the book High Dimensional Probability . While the upper bound can be done by simply mimicking the proof of Lemma $6.4.2$ , the lower bound is somewhat tricky since now we cannot apply the triangle equality due to the missing of ""norm"". N.B. The above inequality trivially holds if the increment of the stochastic process is always symmetric.","['stochastic-processes', 'probability-theory']"
3820829,Discrete subgroups of $\mathbb R^1\times S^1$ such that the quotient is compact,"Let $\Gamma\leq\mathbb R\times S^1$ be a discrete subgroup such that $\frac{\mathbb R^1\times S^1}\Gamma$ is compact. Then what can $\Gamma$ be? Also, what are the possible results for the quotient space. Intuitively, I think that for any $\Gamma$ with above properties, $\frac{\mathbb R\times S^1}\Gamma$ will always be diffeomorphic to the torus $S^1\times S^1$ . Is that true?","['quotient-spaces', 'differential-topology', 'lie-groups', 'differential-geometry']"
3820850,How and when to assume WLOG correctly?,"Show that $$x^{2013}y+xy^{2013} \leqslant x^{2014}+y^{2014}$$ I know that this seems to be just an application of the rearrangement inequality, what I wanted to ask is that what does it actually mean when in this case one could say that ""by symmetry"" we can assume $x \leqslant y$ or ""WLOG"" $x \leqslant y$ ? This always trips me off a bit.","['contest-math', 'rearrangement-inequality', 'algebra-precalculus']"
3820861,Cramer's V - example and intuition,"Hy, I self study statistics and I come across Cramer's V and I search on Google. The result I find didn't explain me so I please to answer this questions.
What is measured by Cramer's V? What is an example when Cramer's V is equal to 0 and an example when is 1?",['statistics']
3820871,Find the supremum of the set $A=\{\cos(10^n)\mid n\in\mathbb{N} \}$,"I have just finished learning in class that every non-empty bounded above subset of $\mathbb{R}$ has a least upper bound, but my professor then showed us the following set: $$A=\{\cos(10^n)\mid n\in\mathbb{N} \} $$ and asked us to compute the first $5$ decimal places of the supremum of $A$ . At first glance, I want the supremum of this set to be $1$ , but that is not possible for integer values of $n$ , as $10^n$ is never an integer multiple of $2\pi$ . There seem to be no clear patterns concerning the periodicity of the function. My hypothesis is that there is no method to find the supremum of this set, but all we know is that it exists. I started out by noticing that $10^n$ must equal some integer multiple of $2\pi$ , so I got the equation $10^n=2\pi m$ where $m\in\mathbb{Z}$ . This leads to $n=\log(2\pi m)$ which has no integer solutions, but the question is what value of $m$ gets $\log(2\pi m)$ the closest to an integer, which I have no idea how to begin showing. So my question is, is there a way to find the supremum of this set, and if so, how can I compute the first $5$ decimal places?","['trigonometry', 'supremum-and-infimum', 'real-analysis']"
3820882,Curvature of Lie group endowed with bi-invariant Riemmanian metric.,"Let $G$ be a Lie group endowed by a biinvariant Riemmanian metric $\langle\cdot,\cdot\rangle$ and $\mathfrak{g}$ its Lie algebra. It is known that the metric is fully determined by its behaviour at the identity element, i.e. the scalar product $\langle\cdot,\cdot\rangle_e$ on $T_eG\cong\mathfrak{g}$ . Now, the following properties hold for the curvature $R$ , Ricci tensor $Ric$ the Killing form $B$ , at the identity element, i. $\forall x,y,z\in\mathfrak{g}$ : $R(x,y)z=\frac{1}{4}[[x,y],z]$ $R(x,y,x,y)=\frac{1}{4}||[x,y]||^2$ , $\operatorname{Ric}(x,y)=-\frac{1}{4}B(x,y)$ Now, I agree that the same relations will hold when evaluating the corresponding tensors on the unique left invariant vector fields $X,Y,Z$ determined, respectively by $x,y,z$ . But the sources presenting these results then use them to say that, for instance the sectional curvature is nonnegative or, if $G$ is compact and semisimple $(G,-B)$ is Einstein. I mean this would certainly be true if we could extend this relations to hold for any vector field, and not only for left invariants one. And I don't see why this should be true. What am I missing?","['lie-algebras', 'riemannian-geometry', 'curvature', 'lie-groups', 'differential-geometry']"
3820892,Does a spectral theorem exist for linear operator pencils?,"I was wondering if a version of the spectral theorem (the projection valued measure case) holds for linear pencils of the form $$
A-\lambda B
$$ where $A,B$ are self-adjoint on some Hilbert space $\mathcal{H}$ (and possibly unbounded) but $B$ is strictly positive? If so, is there a good reference on this? Note that under ""nice"" conditions, $B^{-1}A$ is self-adjoint on a weighted inner product (put factors $B^{1/2}$ in the inner product of $\mathcal{H}$ - this is straightforward to prove rigorously), so it may be the case that $B^{-1}A$ is usually studied instead.","['measure-theory', 'complex-analysis', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
3820898,Does the functional square root of the cosine admit a vector-based interpretation?,"In linear algebra, the cosine of the angle between two vectors $a$ and $b$ is defined as $$\cos(a,b) = \frac{\langle a, b \rangle}{||a||\cdot||b||} .$$ The functional square root of the cosine has at various times been studied by mathematicians. It is the function $f(\cdot)$ such that $$f(f(x)) = \cos(x). $$ See for instance this MO question. I wonder whether it is possible to have a vector-based interpretation of this functional square root of the cosine, similar to the cosine itself as defined above.","['functional-equations', 'trigonometry', 'linear-algebra', 'vectors']"
3820928,How to determine if a function is differentiable,"Question Determine the values of $a$ and $b$ such that the following function is differentiable at 0. $$f(x) =
\begin{cases}
ax^3cos(\frac 1 x) + bx + b, & \text{if }x \lt 0 \\
\sqrt{a + bx}, & \text{if }x \geq 0
\end{cases}$$ My solution For $f$ to be differentiable at $0$ , $f$ must first be continuous at $0$ . $$\implies \lim\limits_{x\to0^-}f(x) = \lim\limits_{x\to0^+}f(x)$$ Consider \begin{align}
\lim\limits_{x\to0^-}f(x) & =
\lim\limits_{x\to0^-}[ax^3cos(\frac 1 x) + bx + b].
\\[5 mm]
\because \lim\limits_{x\to0^-}ax^3cos(\frac 1 x) & =
\lim\limits_{x\to0^-}bx
\\[5 mm] & =
0
\end{align} $$\therefore \lim\limits_{x\to0^-}f(x) = b$$ Then, consider \begin{align}
\lim\limits_{x\to0^+}f(x) & =
\lim\limits_{x\to0^+}\sqrt{a + bx}
\\[5 mm] & =
\sqrt{a}.
\end{align} $$\implies a = b^2$$ Furthermore, for $f$ to be differentiable at $0$ , $$\lim\limits_{x\to0^-}\frac {f(x) - \sqrt{a}} x = \lim\limits_{x\to0^+}\frac {f(x) - \sqrt{a}} x$$ When $a = b^2$ , \begin{align}
\lim\limits_{x\to0^+}\frac {f(x) - \sqrt{a}} x & =
\lim\limits_{x\to0^+}\frac {\sqrt{a + bx} - \sqrt{a}} x
\\[5 mm] & =
\lim\limits_{x\to0^+}\frac b {\sqrt{a + bx} + \sqrt{a}}
\\[5 mm] & =
\frac b {2\sqrt{a}}
\\[5 mm] & =
\frac 1 2
\\[5 mm]
\implies \lim\limits_{x\to0^-}\frac {f(x) - \sqrt{a}} x & =
\lim\limits_{x\to0^-}\frac {ax^3cos(\frac 1 x) + bx + b - \sqrt{a}} x
\\[5 mm] & =
\lim\limits_{x\to0^-}\frac {b^2x^3cos(\frac 1 x) + bx} x
\\[5 mm] & =
\lim\limits_{x\to0^-}[b^2x^2cos(\frac 1 x) + b]
\\[5 mm] & =
\frac 1 2
\end{align} $$\because \lim\limits_{x\to0^-}b^2x^2cos(\frac 1 x) = 0$$ $$\therefore b = \frac 1 2$$ $$\implies a = \frac 1 4$$ I would like to know if my proposed solution is logical and correct. Moreover, any alternative solutions that are more elegant or succinct are welcomed as well :) Thank you all in advance! Edit Following a discussion with MPW, looks like all is well, except perhaps the fact that $$\implies a = b^2$$ should have been left as $$\implies \sqrt{a} = b$$","['limits-without-lhopital', 'real-analysis', 'calculus', 'limits', 'derivatives']"
