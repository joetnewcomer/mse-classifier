question_id,title,body,tags
374767,Is a parameterization defined to be surjective and/or injective?,"A parameterization is a mapping used in differential geometry for describing a manifold, and in statistics for describing a family of distributions, and may be used for other applications I don't know or list here. I was wondering if a parameterization is defined to be surjective? I guess yes, because, in statistics, it seems that if a parameterization is injective, we can then take its inverse. I am not sure if my understanding is correct, and if it is the same case in other areas than statistics. Is it not defined to be injective? I think yes, because for example, in statistics, there is another concept identifiability for an injective parameterization and unidentifiability for an noninjective parameterization. I am not sure if my understanding is correct, and if it is the same case in other areas than statistics. Thanks and regards!","['statistics', 'parametric', 'differential-geometry']"
374772,When are two commuting linear operators functions of each other,"I've computed that the following is valid for certain functions but I've hit a slight bump in my proof. I was wondering if someone could clear it up. If we formally consider the integral operators $$E f(s) = \frac{1}{\Gamma(-s)} \int_{0}^{\infty} f(-y) y^{-s-1} dy$$ and $$Y f(s) = \int_{-\infty}^{\infty} f(y) \frac{s^y}{\Gamma(y+1)} dy$$ I've shown that if $Q f(s) = s f(s)$ then $$Q Y E f = Y E Q f$$ and $$\frac{d}{ds} Y E f = Y E \frac{df}{ds}$$ Q : What step should I use to show that, because they commute, $YE$ is a
  function of $Q$ and $ \frac{d}{ds}$ and therefore the constant linear
  operator; $\alpha = \alpha Q^0 = \alpha \frac{d^0}{ds^0}$? I mean that $Y = E^{-1}$, which I have verified for a few functions. Considering only such functions that converge I won't go into that here.","['operator-theory', 'linear-algebra', 'analysis', 'functional-analysis', 'complex-analysis']"
374776,Calculating the Solid Angle,"$\textbf{Problem:}$ Consider we have a class $C^1$ parameterization $\psi:[a_1,b_1]\times[a_2,b_2]\rightarrow\mathbb{R}^3-\{0\}$ for the surface $S$. Also, consider that $S$ is such that the map $\varphi:S\rightarrow S^2$, with $\varphi(x,y,z)=\lVert(x,y,z)\lVert^{-1}(x,y,z)$, is injective. PS: $\lVert(x,y,z)\lVert$ is the 2-norm. The solid angle of $S$ is the area of $\varphi(S)$. I'm asked to show that the solid angle is $$\Bigg|\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert(\psi(s,t))\lVert^3}\psi(s,t)\cdot\big(\psi_s(s,t)\times\psi_t(s,t)\big)dtds\Bigg|. $$ $\textbf{My trial:}$ I started noting that $$\frac{\partial(\varphi\circ\psi)_i}{\partial s}=\frac{1}{\lVert\psi\lVert^3}\Bigg(\lVert\psi\lVert^2\frac{\partial\psi_i}{\partial s}-\psi_i(\psi_s\cdot\psi)\Bigg)$$
for $i=1,2,3$, and analogous for variable $t$. I simplified the notation doing $\psi=\psi(s,t)$, $\psi_s=\bigg(\frac{\partial\psi_1}{\partial s}(s,t),\frac{\partial\psi_2}{\partial s}(s,t),\frac{\partial\psi_3}{\partial s}(s,t)\bigg)$ and $\psi_i$ is the ith coordinate of $\psi(s,t)$. I can parameterize $\varphi(S)$ using the composite $\varphi\circ\psi$, and the following expression gives the area of the parameterized surface $\varphi(S)$: 
$$\int_{a_1}^{b_1}\int_{a_2}^{b_2}\Bigg\lVert\Bigg(\frac{\partial(\varphi\circ\psi)_1,}{\partial s}\frac{\partial(\varphi\circ\psi)_2}{\partial s},\frac{\partial(\varphi\circ\psi)_3}{\partial s}\Bigg)\times\Bigg(\frac{\partial(\varphi\circ\psi)_1,}{\partial t}\frac{\partial(\varphi\circ\psi)_2}{\partial t},\frac{\partial(\varphi\circ\psi)_3}{\partial t}\Bigg)\Bigg\lVert dtds $$
I think the absolute value we have is to take care of some orientation issue. Anyway, using the initial relation and doing the calculations in this integral, we get
$$\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert\psi\lVert^3}\Big\lVert \lVert\psi\lVert(\psi_s\times\psi_t)-\lVert\psi\lVert^{-1}\Big((\psi_t\cdot\psi)(\psi_s\times\psi)-(\psi_s\cdot\psi)(\psi_t\times\psi)\Big)\Big\lVert dtds= $$
$$=\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert\psi\lVert^3}\Big\lVert \lVert\psi\lVert(\psi_s\times\psi_t)-\lVert\psi\lVert^{-1}\Big(\big((\psi_t\cdot\psi)\psi_s-(\psi_s\cdot\psi)\psi_t\big)\times\psi\Big)\Big\lVert dtds $$
I don't know what to do from here, thanks.","['multivariable-calculus', 'solid-angle', 'surfaces']"
374780,Showing $\nabla^2 \phi = \rho$ with $r \phi$ bounded has at most one solution,"Given a continuous function $\rho (x,y,z)$, which is zero for $x^2+y^2+z^2 > a^2 > 0$ find $\phi$ such that: $\nabla^2 \phi = \rho$ with $r \phi$ bounded and $r \displaystyle\frac{\partial \phi}{\partial r} \rightarrow 0$ as $r \rightarrow \infty$ I literally have no idea where to even begin with this question any help would be greatly appreciated. Thanks.",['multivariable-calculus']
374786,Does $\frac{x}{x}=1$ when $x=\infty$?,"This may be a dumb question: Does $\frac{x}{x}=1$ when $x=\infty$? I understand why $\frac{x}{x}$ is undefined when $x=0$:  This can cause errors if an equation is divided by $x$ without restrictions. Also, $\frac{\infty}{\infty}$ is undefined.  So when I use $\frac{x}{x}=1$ to simplify an equation, can it also lead to errors because $x$ can equal infinity?
Or is $x=\infty$ meaningless?","['calculus', 'real-analysis']"
374815,"$S=\{(X,Y) \in \mathcal{P}(A) \times \mathcal{P}(A) \mid \forall x \in X \exists y \in Y(xRy)\}.$ If R is symmetric, must S be symmetric?","I'm working on an exercise from How To Prove It by Velleman, and I'm having a hard time. Suppose $R$ is a relation on $A$ and define a relation S on $\mathcal{P}(A)$ as follows: $$S=\{(X,Y) \in \mathcal{P}(A) \times \mathcal{P}(A) \mid \forall x \in X \exists y \in Y(xRy)\}.$$ (b) If $R$ is symmetric, must S be symmetric? (Parts (a) and (c) ask the same for reflexivity and transitivity, but I'm only having trouble with part (b) at the minute.) I've tried a few examples and everything I've tried appears to be symmetric, but perhaps I'm not considering the right examples. I've started by trying to show that if $(X,Y) \in S$ then $(Y,X) \in S$ . So suppose $(X,Y) \in S$ . We need to show that $(Y,X) \in S$ which means $\forall y \in Y \exists x\in X(yRx)$ . So assume $y \in Y$ . I'm having trouble finding some $x \in X$ such that $yRx$ . I know if I can find any $x \in X$ , I can immediately infer $xRy'$ for some $y' \in Y$ due to $(X,Y) \in S$ , and from there can infer $y'Rx$ since $R$ is symmetric. The only problem I see with this is that without showing that $y'=y$ , I can't infer $(Y,X) \in S$ because $y'$ is a only a specific element of $Y$ . I can't think of any obvious ways of doing this. I also tried considering cases when $X= \varnothing$ , and $X \neq \varnothing$ . It seems to me that $X=\varnothing$ makes no sense at all for $S$ , because we'll never have any elements of $X$ to consider finding a corresponding value of $y$ such that $xRy$ . At least with $X \neq \varnothing$ I would have some $x \in X$ to work with. I think this method suffers from the last method in the same way as before, as we can only consider a specific element $y' \in Y$ where $xRy'$ from the statement $(X,Y) \in S$ . All of this leads me to believe there is some counterexample I am missing. I feel completely lost, any help with this would be greatly appreciated!","['relations', 'proof-writing', 'elementary-set-theory']"
374826,Parametrizing a given line and equations,"1) Parametrizethe given line contraining the points (3,2) and (-5,6). 2) Find the parametric equations for the segment joining the given points (2,3) and (5,5) where $0\leq t \leq 1$. 3) Find the parametric equation for the segment joining the given points (-3,0) and (1,6) where $0\leq t \leq 2$. Im just learning parametric curves and these questions have me on holdup. My attempt is: 1) $c(t) = ( 3-8t, 2+2t )$ 2) $c(t) = ( 2+5t, 1+2t )$ Im not sure what im doing wrong but both of the answers are not correct. For the third one, I dont know what to do.","['parametric', 'calculus', 'algebra-precalculus']"
374842,Does separability imply the Lindelöf property?,"Does separability imply a sort of Lindelöf property? Since I can't prove this fact I'm beginning to think that my conjecture is false. Intuitively, $\mathbb{R}$ has a countable subset $\mathbb{Q}$ which is used to form a countable basis for $\mathbb{R}$ with the usual topology and prove the Lindeöf property.","['general-topology', 'examples-counterexamples']"
374859,Difference Between Limit Point and Accumulation Point?,"I want to clarify the definition of limit point and accumulation point. According to many of my text books they are synonymous that is $x$ is a limit/accumulation point of set $A$ if open ball $B(x, r)$ contains an an element of $A$ distinct from $x$ . But from one of the problems in Aksoy: A Problem Book in Real Analysis says: Show that if $x \in (M,d)$ is an accumulation point of $A$ , then $x$ is a limit point
of $A$ . Is the converse true? So what is the definition?",['real-analysis']
374868,Differentiation with sigma notation,I want to solve one function that contains sigma notation. $f(x)=1- e^{(-x/a)}\sum_{i=0}^{m-1}\frac{(\frac{x}{a})^i}{i!}$,"['calculus', 'derivatives']"
374878,Discrete Pareto Distribution,"What would be the equation to describe a set of 10,000 bugs if you already know they have a pareto distribution. In other words 2,000 of the bugs would equal 80% of all your problems. I'm struggling to make the pareto and/or Zipf equation fit this model... It should be relatively simple. I would like to make a simple ""progress bar"" type graph that will use this equation.","['statistics', 'probability-distributions']"
374881,Recursive formula for variance,"I'd like to know how I can recursively (iteratively) compute variance, so that I may calculate the standard deviation of a very large dataset in javascript. The input is a sorted array of positive integers.","['statistics', 'standard-deviation']"
374899,Application of Strong Law of Large Numbers and Fubini's Theorem,"This problem comes from here . I am not looking for help on solving the problem, actually to understand something said in the setup: Let $F$ be a distribution with $F(0-) =0 $ and$F(1)=1$. Let $\mu$ be the associated law. Define $$\Omega = [0,1] \times [0,1]^{\mathbb{N}},~~~ \mathcal{F} = \mathcal{B} \times \mathcal{B}^{\mathbb{N}},~~~ \mathbb{P} = \mu \times \mathrm{Leb}^{\mathbb{N}}$$
  $$\Theta(\omega) = \omega_0, H_k(\omega) = I_{[0,\omega_0]}(\omega_k).$$ 
  This models the situation in which $\Theta$ is chosen with law $\mu$, a coin with probability $\Theta$ of heads is then minted, and tossed at times $1,2,\ldots$ . The RV $H_k$ is $1$ if the $k$th toss produces heads, $0$ otherwise. Define
  $$
S_n := H_1 + H_2 + \cdots + H_n.
$$
  By the Strong Law and Fubini's Theorem,
  $$
S_n/n \to \Theta,~~ \mathrm{a.s.}
$$
  ... [Statement of the problem.] It is this last comment that confuses me. Of course, it is intuitively obvious that $S_n/n \to \Theta$ by the Strong Law, but I don't see how to actually prove it. I assume the a.s. means $\mu \times \mathrm{Leb}^{\mathbb{N}}$ a.s. (the Leb stands for Lebesgue measure). I think I can say $E[H_i] = E[E[H_i|\Theta]] = E[\Theta]$ but this doesn't help much. Or I could say
$$
\int_\Omega H_i d\mathbb{P} = \int_{[0,1]^\mathbb{N}}\int_0^1 H_id\mu ~d\mathrm{Leb}^\mathbb{N}
$$
but again I don't see how this helps. To use the Strong Law I need IID RVs (Okay since they are uniformly bounded by $1$ independent is sufficient), but they are NOT independent. They are somehow independent once $\Theta$ is known. I don't know how to formulate this rigorously in order to obtain the result though. The author makes it sounds like it should be obvious. How does one arrive at the conclusion $S_n/n \to \Theta$?","['probability-theory', 'measure-theory']"
374902,Let $f(n)$ be the number of prime factors of the positive integer $n$. Find $\lim_{n\to \infty}\frac{f(n)} n$,"Let $f(n)$ be the number of prime factors of the positive integer $n$. Find $\displaystyle \lim_{n\to \infty}\frac{f(n)} n$. I suspect it's equal to $0$, but how can I show this? Thank you.","['limits', 'calculus', 'prime-factorization', 'number-theory']"
374905,Reverse Hölder Continuity and Hausdorff dimension,"Let $f$ be a function on $[0,1]$. Say that $f$ is reverse Hölder continuous of exponent $\beta > 0$ if there is a $C >0$ such that for any $s<t\in [0,1]$, there exists $s',t'\in [s,t]$ such that $|f(s') - f(t')| \geq (t-s)^\beta$. I am trying to show that if this is the case, then the Hausdorff dimension of the graph of $f$ is at least $2-\beta$. I then want to show that standard Brownian motion satisfies this condition with probability 1 for $\beta  > 1/2$, so that the Hausdorf dimension of its graph is at least $3/2$. This is based on an exercise from ""Brownian Motion"" by Morters and Peres. For the first part, I considered a cover of the graph $G$ of $f$ by open balls and tried to show that the projections of various rescalings of these balls still cover $[0,1]$. But, this did not get me anywhere and I still fail to see where the $2-\beta$ comes in. For the second part, involving Brownian motion, I tried to break up $[s,t]$ into small independent increments and show that at least one of these increments is large if there are sufficiently many, but this did not seem to work either (the probability that an increment is large decays too quickly). Does anyone have any suggestions?","['stochastic-processes', 'fractals', 'probability-theory', 'dimension-theory-analysis', 'holder-spaces']"
374940,Intersection of $\sigma$-algebras and set theory,"Theorem : Given $\{E_{\alpha}\}_{\alpha \in \mathcal{A}}$, where each $E_\alpha$ is a $\sigma$-algebra on $X$. Then $E:=\bigcap_{\alpha \in \mathcal{A}}E_\alpha$ is a $\sigma$-algebra. Proof : Take $\{S_i\}_{i=1}^\infty$ with each $S_i \in E$. Then for each $\alpha \in \mathcal{A}$ we have $S_i \in E_\alpha$ and therefore $\bigcup_{i=1}^\infty S_i \in E_\alpha$. So $\bigcup_{i=1}^\infty S_i \in E$. And complement is same reasoning. Now suppose we are given an arbitrary set $F$ and we say that $\{E_{\alpha}\}_{\alpha \in \mathcal{A}}$ is the set of all $\sigma$-algebras containing $F$. Then the above theorem would imply that there exists a unique smallest $\sigma$-algebra that contains $F$. I don't know any set theory but I know that not all sets you can write down are legit (Russell's paradox), so I am a little uncomfortable with this. So I'd like to ask, Suppose we have $\{E_{\alpha}\}_{\alpha \in \mathcal{A}}$. That is, we have a function $f:\mathcal{A} \rightarrow \mathcal{P}\left(\mathcal{P}\left(X\right)\right)$, where $E_\alpha := f(\alpha)$. Can we always say that $\bigcap_{\alpha \in \mathcal{A}}E_\alpha$ is a legit set? Is it easy to show that you can construct $\{E_{\alpha}\}_{\alpha \in \mathcal{A}}$? (I.e. the existence of an $f$ such that $f(\alpha)=E_\alpha$) Do we need to worry about this constructability in practice? Why?","['set-theory', 'measure-theory']"
374944,Questions on Reduced Induced Closed Subscheme,"I've just read the definition of a closed subscheme in Hartshorne's recently and I collected here and there (notes that people put online) the following statement. Claim. Suppose that $(X,\mathcal{O}_X)$ is a scheme, $Y \subset X$ is a closed subset. Then $Y$ can be equipped with a sheaf $\mathcal{O}_Y$ such that $(Y,\mathcal{O}_Y)$ is a reduced closed subscheme of $X$ with the following universal property: for any reduced scheme $Z$ together with a morphism $f: Z \rightarrow X$ such that $f(Z) \subset X$, $f$ factors uniquely through $i: Y \hookrightarrow X$ ($i^{\#}: \mathcal{O}_X \rightarrow i_{\ast}\mathcal{O}_Y$). Here are my questions (I expect an answer to either (1) or (2) and an answer to (3). Thanks! 1) Can you provide me some good references for the statement of the result, for a proof or a construction of this result? 2) Since I haven't found a satisfactory description of the above fact in popular texts yet, I came up with my own try. I define an ideal sheaf $\mathcal{I}_Y(U) = \{s \in \mathcal{O}_X(U) \mid s_p \in \mathfrak{m}_{X,p} \text{ for all } p \in U \cap Y\}$ on $X$, and I define $\mathcal{O}_Y = i^{-1}(\mathcal{O}_X/\mathcal{I}_Y)$. I expect $\mathcal{O}_Y$ to be the desired sheaf on $Y$. So far, I have managed to show that $(Y,\mathcal{O}_Y)$ is a reduced subscheme of $X$, but I haven't had any idea how to prove the universal property yet. Do you know whether or not the above construction gives the right sheaf in the claim? If yes, could you provide me some idea how to prove the universal property? 3) Suppose that $\mathcal{F}$ is another sheaf on $Y$ that makes $(Y,\mathcal{F})$ a reduced subscheme of $X$, does this imply that $(Y,\mathcal{F})$ also has the universal property stated in the claim?",['algebraic-geometry']
374967,Alternate proofs (other than diagonalization and topological nested sets) for uncountability of the reals?,"I recently started studying set theory and am having quite a bit of difficulty accepting Cantor's diagonal proof for the uncountability of the reals. I also saw a topological proof via nested sets for uncountability which still does not satisfy me completely, given that just like the diagonal it relies on a never ending process. In fact, the nested sets proof sounds very much like the diagonalization proof to me. Do all proofs of the uncountability of the reals involve diagonalization? Are there any other proofs I can look at to understand? I couldn't find any on searching stack exchange. Thanks.","['logic', 'set-theory', 'real-analysis']"
374973,"$\int_0^{2\pi}e^{a \cos{\theta}}\cos({\sin{\theta}})\,d\theta$ using residues","How do I find the following integral by converting it into a complex integral and then using residue theorem? $$\int_0^{2\pi}e^{a \cos{\theta}}\cos({\sin{\theta}})\,d\theta$$ My approach is as follows. Substitute $z=e^{i\theta}$ so that $\cos{\theta}=\frac{z+z^{-1}}{2}$, and similarly for sine. For the cos outside the exponent, use this substitution again. But it is just giving me a string of exponents, which i don't know how to integrate. Should I use integration by parts? Any ideas?","['definite-integrals', 'residue-calculus', 'integration', 'complex-analysis']"
374983,Prove that $\frac{a_1^2}{a_1+a_2}+\frac{a_2^2}{a_2+a_3}+ \cdots \frac{a_n^2}{a_n+a_1} \geq \frac12$,"Let $a_1, a_2, a_3, \dots , a_n$ be positive real numbers whose sum is $1$ . Prove that $$\frac{a_1^2}{a_1+a_2}+\frac{a_2^2}{a_2+a_3}+ \ldots +\frac{a_n^2}{a_n+a_1} \geq \frac12\,.$$ I thought maybe the Cauchy and QM inequalities would be helpful. But I can't see how to apply it. Another thought (might be unhelpful) is that the sum of the denominators on the left hand side is $2$ (the denominator on the right hand side). I would really appreciate any hints.","['cauchy-schwarz-inequality', 'inequality', 'algebra-precalculus', 'a.m.-g.m.-inequality', 'contest-math']"
374984,Proof of injections,"Let $f: X\to Y$ be a surjection and $g: Y\to Z$ be such that $g \circ f$ is an injection. Prove that both $f$ and $g$ are injections. My attempt: Since $f$ is surjective then there exists $ x, x'\in X$ such that $(x,y)\in f$ and $(x',y')\in f$, but since $g\circ f$ is an injection then $(x,z)\in g\circ f$ and $(x',z)\in g\circ f$ which implies that $x=x'$ ... but now I am stuck completing the proof.",['elementary-set-theory']
374998,Subspace topology and order topology (2),"Yesterday I asked a very similar question but now I am confused again... Let $I=[0,1]$. The dictionary order on $I\times I$ is just the restriction to $I\times I$ of the dictionary order on the plane $\mathbb{R}\times\mathbb{R}$. now, how can I show that the set $\{1/2\}\times (1/2,1]$ is not open in the order topology? The maximum element of $I$ is 1. So the order topology on $I\times I$ has intervals of the form $(a,1]\times (b,1]$ as basis elements. Isn't $\{1/2\}\times (1/2,1]$ itself a basis element of the order topology?","['general-topology', 'order-topology']"
375006,When do equations represent the same curve?,"Suppose we have two sets of parametric equations $\mathbf c_1(u) = (x_1(u), y_1(u))$ and $\mathbf c_2(v) = (x_2(v), y_2(v))$ representing two 2D planar curves. When I say ""2D planar curves"" I mean that $\mathbf c_1(u)$ and $\mathbf c_2(u)$ are mappings from compact intervals in $\mathbb R$ to $\mathbb R^2$. We can assume (without loss of generality, I think) that $\mathbf c_1:I \to \mathbb R^2$ and $\mathbf c_2:I \to \mathbb R^2$, where $I=[0,1]$. You can assume some continuity or differentiability of 
$\mathbf c_1(u)$ and $\mathbf c_2(u)$, if that helps. I'm interested to know how we can determine that these two sets of equations represent the same curve. In other words, how can I determine that $\mathbf c_1(I)$ and $\mathbf c_2(I)$ are the same point set. An interesting special case: what if the parametric equations are all rational functions? In this case, it's often possible to implicitize -- i.e. convert to equations of the form $f_1(x,y)=0$ and $f_2(x,y)=0$. Then, if the two curves are the same point set, I would guess that something can be said about $f_1$ and $f_2$? Maybe one is a multiple of the other, or something like that?? Even simpler (but still interesting): what if all the functions involved are polynomials. The implicitization doesn't necessarily solve the original problem, though. It's clear that $\mathbf c_1(I)$ is a subset of the zero set $Z_1 = \{(x,y) \in \mathbb R^2 : f_1(x,y) = 0\}$, but it might be a proper subset. So, even if we know how to relate $Z_1$ and $Z_2$, this might not tell us much about how $\mathbf c_1(I)$ is related to $\mathbf c_2(I)$. Can we say anything about when the implicitization approach will work and when it won't? My question was inspired by this one . There might be some connection with this question , but both the question and the answer are written in jargon that's not familiar to me. This has practical applications -- curves in engineering and manufacturing are often described by using rational or polynomial parameterizations, and it would be nice if we had some way to identify when two curves are the same. In engineering & manufacturing, we only care about the shapes of curves (i.e. sets like $\mathbf c_1(I)$ and $\mathbf c_2(I)$), not their parameterization. For example, a circular wheel is still circular, regardless of how the circle curve is parameterized. The parameterization is artificial, in some sense, and I want to be able to ignore its effects when comparing two curves. In case it matters to anyone, this isn't homework  :-). Example (for the rational case) $$\mathbf c_1(t) = \left( \frac{1 - (2 - \sqrt2)t - (\sqrt2 - 1)t^2}
                               {1 - (2 - \sqrt2)t + (2 - \sqrt2)t^2},
                          \frac{\sqrt2 t - (\sqrt2 - 1)t^2}
                               {1 - (2 - \sqrt2)t + (2 - \sqrt2)t^2} \right)$$ $$\mathbf c_2(t) = \left( \frac{1 -t^2}{1 + t^2},
                          \frac{2t}    {1 + t^2} \right)$$ Here $\mathbf c_1(I) = \mathbf c_2(I)$. They are both the first quadrant of the unit circle, actually. Progress (December 2017) Apparently, if two two implicit equations $f_1(x,y)=0$ and $f_2(x,y)=0$ represent the same curve, and $f_1$ and $f_2$ are both irreducible polynomials, then one must be a constant multiple of the other. This result is mentioned (without proof) in this paper by Sendra , so I suppose it must be well-known.","['calculus', 'analytic-geometry', 'algebraic-geometry', 'algebraic-curves', 'abstract-algebra']"
375011,integration as limit of a sum,"If $f$ is continuous on $[0, 1]$ then
$$\lim_ {n\to\infty}\sum_{j=0}^{\lfloor n/2\rfloor} \frac1{n}f\left(\frac {j}{n}\right)  = ? $$
will the answer be that the limit exists and is equal to $ \int_0^1 f(x) dx$ ?","['definite-integrals', 'sequences-and-series', 'real-analysis', 'limits']"
375020,An unusual type of linear algebra problem,"I've come across the following linear algebra problem while trying to derive something in information theory. I'm looking both for numerical ways to solve this type of problem and for anything analytical that can be said about it. If there's a closed-form solution (e.g. in terms of matrix algebra) that would be great. I have a matrix $C$ (which might be singular) and vectors $a$ and $b$ such that $\sum_i a_i = \sum_j b_j$. I'm looking for vectors $\alpha$ and $\beta$ such the following conditions hold simultaneously:
$$
\sum_i \alpha_i C_{ij} \beta_j = b_j\qquad\text{(for every $j$)}
$$
and
$$
\sum_j \alpha_i C_{ij} \beta_j = a_i\qquad\text{(for every $i$).}
$$ Clearly there are cases where no solution exists, such as when $C$ is diagonal and $a\ne b$, but I suspect that in my case there always will be a solution. It looks like this should be easy to solve, but I can't quite see how to go about it. The following may or may not be relevant for answering the question, but in my case the numbers all relate to a joint probability distribution over variables $A$, $B$ and $X$: The elements of $C$ are the marginal distribution for $A$ and $B$. That is, $C_{ij} = p(A=i,B=i)$; $a$ and $b$ are the following marginal conditional probability distributions: $a_i = p(A=i \mathop{|} X=k)$ and $b_j = p(B=j \mathop{|} X=k)$, for some particular $k$; The numbers $\alpha_i C_{ij} \beta_j$ are the elements of an (unknown) conditional distribution $p(A=i,B=j\mathop{|}X=k)$, which is what I'm attempting to find. (Actually it's not the true conditional distribution but a minimum-information estimate of it, which is why it has this particular form.) In terms of the linear algebra problem, this means that $a$, $b$ and $C$ satisfy the following constraints: all the elements of $C$, $a$ and $b$ are real and between 0 and 1. $\sum_{ij}C_{ij} = 1$ $\sum_{i}a_{i} = \sum_j b_j = 1$. $a$ and $b$ can be expressed as the row and column sums of a matrix $D$ with real entries in $[0,1]$ such that $D_{ij}=0$ whenever $C_{ij}=0$, and $\sum_{ij}D_{ij}=1$. (The elements of $D$ are the ""true"" conditional distribution $p(A=i,B=j\mathop{|}X=k)$.)","['probability-theory', 'linear-algebra']"
375026,Since when is 9/10 = 92%?,"This is probably a more basic question than this site is used to, probably because I'm only 13, and as such I'd appreciate if you gave a more basic and simple explanation than the norm for this site. I was reading a BBC News article this morning and I found myself questioning the three diagrams, in the 'Why are two tests better than one?' section. You do a first test and obtain nine heads and one tail... The probability that the coin is fair given this outcome is about 8%, [and the probability] that it is biased, about 92%. You do a second test, and this time you throw eight heads and two tails. Now the probability for a fair coin is about 16%, for a biased coin about 84%. So the naive thought might be that you haven't gained any certainty from this second test.
But if you think about it differently, what you've really done is throw the coin 20 times and get 17 heads and three tails."" This means there's a probability of 98.5% that the coin is biased. I have three questions, all roughly along the same lines: Please see below paragraph, these questions are no longer 'active' - Since when is 9/10 heads a 92% probability, and why?
 - Why is 8/10 an 84% probability, I always thought that 8/10 = 80%
 - And finally, 17/20 is 85%, except when it's 98.5%. Why? After some help from the comments, I now realise that it's not talking about the probability of getting a head (or a tail) but the probability of the coin being biased. Can someone explain (preferably in layman's terms) how the article gets to 92%, 84% and 98.5% respectively? Thanks.",['probability']
375067,Hopf Algebras in Combinatorics,I know that many examples of Hopf algebras that come from combinatorics. But I'm interested in knowing how Hopf algebras are applied in solving combinatorial problem. Are there examples of open problems in combinatorics which were finally solved using techniques from Hopf algebras? Or where Hopf Algebra has been used to get alternative proofs of already known combinatorial results?,"['hopf-algebras', 'soft-question', 'combinatorics']"
375068,Simplifying $\prod_{k=0}^n \cos(2^{-k})$,"A student of mine has trouble with the following, and so do I. The solution should be easy since it has been ask to première S students (equivalent to American 11th grade I guess). The question is following :
Simplify $P = \prod_{k=0}^n \cos(2^{-k})$. Using the identity $\cos x = 2\cos^2\left(\frac{x}{2}\right) - 1$, we can express $P$ as $\prod_{k=0}^n f^{(n)}(\cos 1)$, where $f^{(n)}$ is the $n$th iteration of $t\mapsto \sqrt{\frac{t+1}{2}}$, but that's not really a simplification. If we define the polynomials $g_0 = x$ and $g_{n+1} = xg_n(2 x^2-1)$, then we have $P = g_n(\cos 2^{-n})$, but again it's not really a simplification for a 16 year old student. Do you have an idea ?",['trigonometry']
375071,Uniqueness of $q$ and $r$ of Division Algorithm doesn't hold in a general Euclidean Domain?,"I heard that in a general Euclidean Domain, the quotient $q$ and remainder $r$ when an element $a$ is divided by an element $b$ by Division Algorithm, may not be unique. Is it true? If yes, can anyone give me examples where such a thing happens.","['ring-theory', 'abstract-algebra']"
375080,Is it true that $\cup_{n=N}^{\infty}A_{n}\setminus\cap_{n=N}^{\infty}A_{n}=\cup_{n=N}^{\infty}A_{n}\triangle A_{n+1}$?,"During an exam I have claimed that if $\{A_{n}\}_{n=1}^{\infty}$
  then for any $N\in\mathbb{N}$
  $$\limsup A_{n}\setminus\liminf A_{n}\subseteq\cup_{n=N}^{\infty}A_{n}\setminus\cap_{n=N}^{\infty}A_{n}=\cup_{n=N}^{\infty}A_{n}\triangle A_{n+1}$$ The equality part of my statement was marked as wrong, but I am having difficulty to understand if it is indeed wrong, and if I can replace $=$
  with $\subseteq$
  to make it correct. My argument for the first the first containment is the description of $\limsup A_{n},\liminf A_{n}$
  as the set of all elements of in those sets that are in infinity many sets and as those that are not in a finite number of those $A_{n}$
 . For the second part, I tried to understand what is $$(A\triangle B)\cup(B\triangle C)$$
  and using a diagram I got that it is $$(A\cup B\cup C)\setminus(A\cap B\cap C)$$
  so I figured I can deduce the equality for the infinite case as well, although I can't prove it formally. Is it true that $$\limsup A_{n}\setminus\liminf A_{n}\subseteq\cup_{n=N}^{\infty}A_{n}\setminus\cap_{n=N}^{\infty}A_{n}=\cup_{n=N}^{\infty}A_{n}\triangle A_{n+1} ?$$
  or at least that $$\limsup A_{n}\setminus\liminf A_{n}\subseteq\cup_{n=N}^{\infty}A_{n}\setminus\cap_{n=N}^{\infty}A_{n}\subseteq\cup_{n=N}^{\infty}A_{n}\triangle A_{n+1} ?$$ Any help is greatly appreciated!","['elementary-set-theory', 'limsup-and-liminf']"
375083,given coordinates of beginning and end of two intersecting line segments how do I find coordinates of their intersection?,"There are two line segments. I know for sure they intersect (so I don't have to check it). For both line segment I know coordinates of its both ends. With what formula can I find coordinates of their intersection? I know the method I can use (find their lines' equations and solve them), but I'm lazy and I want just to have ready to use formula.","['analytic-geometry', 'geometry']"
375087,Are these partially ordered set or equivalence relation?,"I) {${((a,b),(c,d)) ∈ (\Bbb Q × \Bbb Q)² : (a<c) ∨ ( a = c ∧ b ≤ d)}$} II) {${(f,g) ∈ \Bbb Q→\Bbb Q × \Bbb Q→\Bbb Q: ∀x ∈ \Bbb Q : f(x) ≤ g(x)}$} III) {(a,b) ∈ $\Bbb Z$ × $\Bbb Z$ : n devides a - b}, n ∈ $\Bbb N$ I have to prove: a) reflexivity ($a\sim a$) b) symmetry (if a~b then b~a) c) antisymmetry (if a ≤ b and b ≤ a then a = b) d) transitivity (if a~b and b~c then c~a) If a,b and d are true it's an equivalence relation. If a,c and d are true it's a partially ordered set. My problem is on how to get started. I tried a few things, but looking at those three tasks (I found a lot more, but I feel like these could be the best examples) makes me feel like there's always something I'm missing. I'm preparing for finals, and this is what always bothered me. I'd be really thankful for any kind of help, even if it's by providing the solution (which would be nice (but really not necessary, anything would help me here) as I'm going on a long trip by train in a few hours and therefore I can't do too much work on paper.)","['relations', 'elementary-set-theory']"
375090,Can it be shown that a set X is infinite if and only if there exists some $F:X\to X$ that is an injection but not a surjection?,"If the function is not surjective, then at least one element of the codomain has no pre-image.  However, because F is a function, every element in the domain is mapped to something in the codomain.  But since we know that F is injective, then we can't use an element in the codomain more than once in our mapping.  That must mean that there is an infinite set of elements in the codomain if we can ""skip"" one and still be injective.  I'm having trouble, however, translating my thoughts into a proof format. The Axiom of Choice is not available to me for the purposes of this proof.
Infinite for this question would be that it as has a countably infinite subset. Thank you to all who addressed my question, both for your help and patience.","['elementary-set-theory', 'functions']"
375164,Definition of correspondence,"A one-to-one correspondence is an alternative name for a bijection between two sets, but to what does the term 'correspondence' alone refer? As far as I can see, it seems to be another term for 'relation', but I think there must be a difference.","['relations', 'terminology', 'functions', 'definition']"
375215,"Closed form for $(a_n)$ such that $a_{n+2} = \frac{a_{n+1}a_n}{6a_n - 9a_{n+1}}$ with $a_1=1$, $a_2=9$",$$a_1 = 1; a_2 = 9; a_{n+2} = \frac{a_{n+1}a_n}{6a_n - 9a_{n+1}}$$ I need to find non-recurring formula for $a_n$. Is there any good way to do this? The only one comes to mind is to guess the formula and then prove it using mathematical induction. Thanks in advance! I've got the result and it looks like this: $a_n = \frac{-3*2^{n-1} + 2^{2n - 1} + 1}{3}$ but I really don't like this way and would love to know how to solve this properly.,['sequences-and-series']
375216,Independence of conditional random variables,"Assume I have two random variables $A$ and $B$, which are not independent. In my particular case they will be values of a stochastic process at two given points in time, where $A$ is observed at an earlier time. Define $(B|A)$ to be a conditional random variable, i.e. a random variable defined by the conditional distribution of $B$ given $A$. Question: is $(B|A)$ independent of $A$? Why? Why not? Under what conditions it is? EXAMPLE:
Let $A$ and $C$ be two independent Gaussian (0, 1) random variables, and let $B=A+C$. Then $(B|A=a)$ is Gaussian (a, 1) and seems to be independent of $A$, but I am not sure how to work formally with these kind of things. EDIT:
As Sebastian Andersson pointed out in the comment, $A$ and $(B|A)$ seem not to be independent. However, what if we condition on $A=a$, where $a$ is a constant? The intuition would be that first we are interested in the uncertain event $A$, and then after it happens (and we know the outcome), we are interested in an event $(B|A=a)$. Does it make sense?","['probability-theory', 'probability', 'random-variables', 'conditional-probability']"
375247,Representing linear operators on infinite sequences as infinite matrices,"So this question arose while I was working on a homework assignment to find the adjoint operator of a continuous operator on $l^2$. Anyway, it seemed like I could find it if I thought about the respective operators as infinite matrices. However, this is something that hasn't been discussed in class and I haven't found anything about on the internet. Is this valid? I've never really thought about infinite matrices before so I am just a little hesitant about what may be different from the finite cases I am use to. Can a linear operator on a sequence always be viewed as an infinite sequence? Can anybody point me in the direction of a website or suggest a book that would give a good introduction to this? Thank you!","['matrices', 'linear-algebra', 'infinite-matrices', 'analysis']"
375250,Why does the standard deviation change from confidence intervals to hypothesis tests?,"When considering two-sample data that involves a difference of proportions, both a confidence interval and a hypothesis test can be done. The standard deviation used for a difference of proportions in creating a confidence interval is $\sqrt{\frac{p_1(1 - p_1)}{n_1} + \frac{p_2(1 - p_2)}{n_2}}$. However, the standard deviation used for confidence intervals is $\sqrt{\frac{p(1 - p)}{n_1} + \frac{p(1 - p)}{n_2}}$, where $p = \frac{x_1 + x_2}{n_1 + n_2}$, $x_1 = p_1n_1$, and $x_2 = p_2n_2$. What I don't understand is why these are different. They're both the standard deviation of the same proportion, so why should they differ?","['statistics', 'statistical-inference']"
375254,Finding the Number of Zeros of a Function in a Given Annulus,"Consider $z^6 - 6z^2 + 10z + 2$ on the annulus $1<|z|<2$. By Rouche's Theorem $|f(z) + g(z)| < |f(z)|$ implies that both sides of the inequality have the same number of zeros.  I understand that when asked to find that there is, say, $1$ zero the idea is to choose the $10z$ term as $f(z)$ so that we can form the proper relationship and conclude that since $10z$ has one zero in the region the function has one zero in the region.  How can it be set up to find a general number of zeros?  What is the trick for picking $f(z)$?","['roots', 'complex-analysis']"
375260,Prove this inequality $a \sqrt{1-b^2}+b\sqrt{1-a^2}\le1 $,"Prove that for $a,b\in [-1,1]$: $$a\sqrt{1-b^2}+b\sqrt{1-a^2}\leq 1$$",['algebra-precalculus']
375263,Bases and linearly independent sets in free R-modules,"Let $R$ be a ring with unit, and let $M$ be any left $R$-module. We say that a subset $\{e_1, \ldots, e_n\}$ of $M$ is linearly indepedent if $\sum r_i e_i = 0$ implies $r_i = 0$. If the subset also generates $M$, then we say that it is a base. In this case $M \cong R^n$, the free $R$-module of rank $n$. In Jacobson's Basic Algebra I it is mentioned that $R^m \cong R^n$ is possible for $m \neq n$ when $R$ is not commutative. I am more interested in the commutative case, and basically I would like to know which results of linear algebra are true in $R^n$ when $R$ is commutative. If $R$ is commutative, then $R^m \cong R^n$ if and only if $m = n$. Let $A = \{e_1, \ldots, e_k\}$ be a subset of $R^n$, where $R$ is commutative. If $A$ is linearly independent, does it follow that $k \leq n$? If $A$ is linearly independent, can we extend $\{e_1, \ldots, e_k\}$ to a base of $R^n$? If $A$ generates $R^n$, does it follow that $k \geq n$?","['modules', 'ring-theory', 'abstract-algebra']"
375267,How find this function $f(x)$,"Let $f:\mathbb{R}\to\mathbb{R}$ be continuous such that $\dfrac{f(x)}{x}=f'\left(\dfrac{x}{2}\right)$. Find  $f(x)$. (2):if $f(x)$ on $x\in[a,b] $ be continuous,find all $f(x)$? I think this is an ODE.","['ordinary-differential-equations', 'functional-equations']"
375270,Size of largest prime factor,"It is well known and easy to prove that the smallest prime factor of an integer $n$ is at most equal to $\sqrt n$. What can be said about the largest prime factor of $n$, denoted by $P_1(n)$? In particular: What is the probability that $P_1(n)>\sqrt n$ ? More generally, what is the expected value of the size of $P_1(n)$, measured by $\frac{\log P_1(n)}{\log n}$ ?","['prime-numbers', 'analytic-number-theory', 'number-theory']"
375283,A principal open set of an affine algebraic set is an affine variety,"Notations $k$ is an algebraic closed field and $\mathbb A^n(k)$ is the topological space $k^n$ with the Zariski topology If $X\subseteq\mathbb A^n(k)$ is an affine algebraic set and $f\in\Gamma(X)$, then $D(f)=\{x\in X\,:\, f(x)\neq0\}$ An affine variety for me is a ringed space $(X,\mathcal O_X)$, where $\mathcal O_X$ is a sheaf of $k$-valued function, that is isomorphic (as ringed space) to $(V,\mathcal O_V)$, where $V$ is an IRREDUCIBLE affine algebraic set and $\mathcal O_V$ is the sheaf of regular functions on $V$. I want to find an affine variety that is not an irreducible affine algebraic set. If $X\subseteq\mathbb A^n(k)$ is an irreducible affine algebraic set, and $f\in\Gamma(X)$, I would prove that $(D(f),\mathcal O_{X|D(f)})$ is an affine variety: Let's consider $\mathfrak a=I(X)\subseteq k[T_1,\ldots, T_n]$ as a subring of $k[T_1,\ldots, T_n, T_{n+1}]$ and let $F\in k[T_1,\ldots, T_n]$ be a representative of $f$. If $Y=V(\big<\mathfrak a, FT_{n+1}-1\big>)$, the function
$$j: Y\longrightarrow D(f)$$
that is the restriction of the projection of $\mathbb A^{n+1}(k)$ on $\mathbb A^n(k)$ is clearly bijective and continuous with inverse $j^{-1}:(x_1,\ldots, x_n)\longmapsto (x_1,\ldots, x_n,\frac{1}{f(x_1,\ldots,x_n)})$. Now How can I prove that $Y$ is irreducible? Why is $j^{-1}$ continuous? (so $j$ is a homeomorphism) Why is $j$  an isomorphism of ringed spaces? If $U\subset D(f)$ is open, should be proved that for all $g\in \mathcal O_X(U)$, then $g\circ f_{|f^{-1}(U)}\in\mathcal O_X(f^{-1}(U))$.",['algebraic-geometry']
375296,Diameter and Hausdorff Distance,"Let $A,B \subset \mathbb R^n$ be non empty compact sets and $d_H$ be Hausdorff distance. I'm thinking that if we know the distance between two sets, the difference between their diameters is bounded. How to (dis)prove that if $d_H(A,B)=r$ then $diam(A)+2r \ge diam(B) \ge diam(A)-2r$?","['analytic-geometry', 'analysis']"
375305,How can I prove $\det(\overline M)=\overline{\det(M)}$?,"Of course $\overline M$ is the complex conjugate of an $n\times n$ matrix $M$. Someone gave me advice to use the definition of determinant, then it means I have to use cofactor expasion here?","['matrices', 'linear-algebra', 'determinant']"
375307,"Duality of $Z(G)$ and $[G,G]$ in representation?","This question and its many wonderful answers illustrate many faces of the duality of $Z(G)$ and $[G,G]$, the centre/ commutator duality of a group. I was thinking about its manifestation in group representations. We might as well look at a finite group $G$ and its classes of irreducible representations $\hat{G}$, and some possible candidates might be \begin{equation}
d_{\alpha}=\operatorname{degree}(\alpha)
\end{equation}  and \begin{equation}
n_{d}=\#\{\alpha\in\hat{G}:d_\alpha=d\}.
\end{equation}Some evidence might be contained in $d_{\alpha}$ divides $\#G/Z(G)$ for all $\alpha\in\hat{G}$. So the larger $Z(G)$ is, the more commutative $G$ is, the smaller $d_\alpha$ is.
On the other hand, $n_1=\#G/[G,G]$. So the larger $[G,G]$ is, the less commutative $G$ is, the smaller $n_1$ is. So, I am wondering whether these can be made precise. And are there other manifestations of the $Z(G)/[G,G]$-duality in group representations? Thanks very much!","['representation-theory', 'duality-theorems', 'abstract-algebra', 'linear-algebra', 'group-theory']"
375310,How to solve a polynomial with power fractions like $a-ax+x^{0.8}-x^{0.2}=0$,"I have something like $a-ax+x^{0.8}-x^{0.2}=0$ with parameter a>0 and variable x>0. I know by trial and error that the equation has three real roots for parameter a greater than certain value, otherwise it has only one root. x=1 is one obvious root. I want to find the value of parameter a that yields multiple roots. Is there a pen and paper solution to that, not using any computational algorithms like Newton's method etc.?","['algebra-precalculus', 'roots']"
375317,"In what sense is the derivative the ""best"" linear approximation?","I am familiar with the definition of the Frechet derivative and it's uniqueness if it exists. I would however like to know, how the derivative is the ""best"" linear approximation. What does this mean formally? The ""best"" on the entire domain is surely wrong, so it must mean the ""best"" on a small neighborhood of the point we are differentiating at, where this neighborhood becomes arbitrarily small? Why does the definition of the derivative formalize precisely this? Thank you in advance.","['derivatives', 'real-analysis', 'numerical-methods']"
375333,What are the factors of $\aleph_0$?,"Extend the system of positive natural numbers with $\aleph_0$. Then we have: $$\aleph_0 = \aleph_0\cdot n,\quad \forall n \in \mathbb{N}^+$$ Does it make sense to talk about factors of $\aleph_0$? What are the factors of  $\aleph_0$? Aside: Are there systems of numbers where it makes sense to talk about factors of infinite numbers?","['cardinals', 'elementary-set-theory', 'infinity']"
375348,"Prove that $\mathbb Z_{m}\times\mathbb Z_{n} \cong \mathbb Z_{mn}$ implies $\gcd(m,n)=1$.","Prove that $\mathbb Z_{m}\times\mathbb Z_{n} \cong \mathbb Z_{mn}$ implies $\gcd(m,n)=1$. This is the converse of the Chinese remainder theorem in abstract algebra. Any help would be appreciated. Thanks!","['group-theory', 'abstract-algebra']"
375353,Every ideal of the localization is an extended ideal,"Let $R$ be a ring, commutative with $1$. Let $S$ be a multiplicatively closed subset of $R$, with $0\notin S,1\in R$. Let $R_S$ be the localization of $R$ at $S$. For every ideal $\mathfrak{a}\subseteq R$ of $R$, define
$$\mathfrak{a}^e:=f(\mathfrak{a})R_S$$
where $f:R\longrightarrow R_S$ is the canonical morphism sending $r$ to $\frac{r}{1}$ and $f(\mathfrak{a})R_S$ means the ideal generated by $f(\mathfrak{a})$ in $R_S$. I want to show (or disprove) that every ideal $\mathfrak{b}$ of $R_S$ is extended, meaning that it is of the form
$$\mathfrak{b}=\mathfrak{a}^e$$
for some ideal $\mathfrak{a}$ in $R$.","['ring-theory', 'abstract-algebra']"
375366,Prove that a topological space equipped with a delta-complex structure is Hausdorff,"Suppose that $X$ is a topological space equipped with a delta-complex structure. Prove that $X$ is Hausdorff. I can actually ""see"" why it should be Hausdorff after the hint from Hatcher asks to ""think of a delta-complex X as a quotient space of a collection of disjoint simplices"". I would appreciate if somebody can write out a full formal argument showing this proof. I'm trying to start from the definition of Hausdorff but I don't know how to carry on.","['general-topology', 'algebraic-topology']"
375370,Second derivative of a metric in terms of the Riemann curvature tensor.,"I can't see how to get the following result. Help would be appreciated! This question has to do with the Riemann curvature tensor in inertial coordinates. Such that, if I'm not wrong, (in inertial coordinates) $$R_{abcd}=\frac{1}{2} (g_{ad,bc}+g_{bc,ad}-g_{bd,ac}-g_{ac,bd})$$
where $"",_i""$ denotes $\partial \over \partial x^i$. How does $$g_{ab,cd}=-\frac{1}{3}(R_{acbd}+R_{adbc})$$? So
$$-\frac{1}{3}(R_{acbd}+R_{adbc})=-\frac{1}{6}(g_{bc,ad}+g_{ad,bc}-g_{ba,cd}-g_{cd,ab}+g_{bd,ac}+g_{ac,bd}-g_{ab,cd}-g_{cd,ab})$$
$$=-\frac{1}{6}(g_{bc,ad}+g_{ad,bc}+g_{bd,ac}+g_{ac,bd})+\frac{1}{3}g_{cd,ab}+\frac{1}{3}g_{ab,cd}$$
But how does $$\frac{1}{6}(g_{bc,ad}+g_{ad,bc}+g_{bd,ac}+g_{ac,bd})+\frac{1}{3}g_{cd,ab}=\frac{2}{3}g_{ab,cd}$$ ...Have I made a mistake? Somebody please? :(","['geometry', 'differential-geometry']"
375375,Verifying Fatou's Lemma,"Royden's Real Analysis Question: Let {$f_n$} be a sequence of nonnegative measurable functions on $R$ such that $f_n\implies f$ pointwise on $E$. Let $M\geq0$ be such that $\int_Ef_n\leq M$ for all $n$. I want to show that $\int_Ef\leq M$. Also I want to verify that this property is equivalent to the statement of Fatou's Lemma. Approach: Here is how I started..
Since $f_n\implies f$  on $E$ then by Fatou's Lemma $\int_Ef\leq lim\inf\int_Ef_n$ where $inf\int_Ef_n\leq M$ this implies $$\int_Ef\leq M$$ Hence verifying Fatou""s Lemma as well. I know I am wrong somewhere but I will appreciate any help given. Thankyou!","['lebesgue-integral', 'measure-theory']"
375400,"Maximal ideals in the ring of real functions on $[0,1]$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $S$ be the ring of all continuous functions from $[0,1]$ to $\mathbb R$. How to prove that all maximal ideals of $S$ have the form $M_{x_0}=\{f\in S \mid f(x_0)=0\}$? Thanks in advance.","['ideals', 'maximal-and-prime-ideals', 'abstract-algebra', 'analysis']"
375421,Condition number of a rectangular matrix,"From what I understand, the condition number of a rectangular matrix $A$ is its largest singular value divided by its smallest nonzero singular value $$\kappa(A) := \frac{\sigma_1 (A)}{\sigma_n (A)}$$ Where $\sigma_1 (A)$ is the operator norm of $A$ and $\sigma_n (A)$ is the operator norm of $A^\dagger$, the pseudoinverse of A. Is this correct and is it generally accepted? I have not been able to find much on the subject online. Is this used in any notable applications?","['matrices', 'singular-values', 'linear-algebra', 'condition-number']"
375426,Radical of $\mathfrak{gl}_n$,"I find it intuitive enough that the radical of $\mathfrak{gl}_n\mathbb F$ is the scalar matrices, but I have trouble finding an easy, but complete proof: Proof . Let $\mathfrak s$ denote the scalar matrices. Clearly $\mathfrak s\subset\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$. Suppose that $\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$ is generated by more than one element, so that $X\in\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$, but $X\notin\mathfrak s$. We can change basis such that $\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$ is upper-triangular. (The change of basis leaves the scalar matrices invariant.) Then $X$ is upper-triangular. I want to conclude that there exists a $Y\in\mathfrak{gl}_n\mathbb F$ s.t. $[X,Y]$ is not upper-triangular. How can I see that $Y$ always exists, except by waving hands?","['matrices', 'lie-algebras']"
375457,show that $P$ is partially ordered set by $\subseteq$,"Let $Y$ be a set and suppose$P \subseteq \mathcal{P}(Y)$. Show that $P$ is partially ordered set by $\subseteq$. So I start off with $\mathcal{P}(Y)=\{\emptyset\subseteq,......\subseteq P....\subseteq..\subseteq Y\}$ How should I use now the reflexivity, antisymmetry and transitivity? I don't think I understand the question. The inclusion is a partial ordering relation on the power set but how to show that one of it's members is partially ordered?",['elementary-set-theory']
375484,Pythagorean theorem and its cause,"I'm in high school, and one of my problems with geometry is the Pythagorean theorem. I'm very curious, and everything I learn, I ask ""but why?"". I've reached a point where I understand what the Pythagorean theorem is, and I understand the equation, but I can't understand why it is that way. Like many things in math, I came to the conclusion that it is that way because it is; math is the laws of the universe, and it may reach a point where the ""why"" answers itself. So what I want to know is, is there an explication to why the addition of the squared lengths of the smaller sides is equal to the squared hypotenuse, or is it just a characteristic of the right triangle itself? And is math the answer to itself? Thank you.","['geometry', 'axioms']"
375496,Groups of order 42 and classification.,"I want to classify the groups of order 42, where the 7-Sylow subgroup $P_7$ is normal, the 2-Sylow subgroup is not normal, and we have 7 3-Sylow subgroups $P_3$. This is what I did : Since $P_7$ is normal, $M = P_7P_3$ is a subgroup. Since $|P_7P_3|=21$, it must be normal in G. We also know that $P_2$ is a subgroup of roder G and $P_2 \cap P_7P_3 = \{e\}$, because otherwise $P_2$ would be a subgroup of $P_7P_3$ but 2 doesn't divide 21. So $G \cong M \rtimes_{\alpha} P_2$. Since M has order 21, we need to classify groups of order 21 to look at the cases for different possibilites of M. |M|= 21 = (3)(7) $n_7 \equiv 1 \pmod 7$ and $n_7|2 \implies n_7=1$. $n_3 \equiv 1 \pmod 3$ and $n_3|7 \implies n_3=1,7$ Case a: $n_3 = 1 \implies M \cong Z_3 \times Z_7 \cong Z_{21}$ Case b: Since $P_7 \vartriangleleft M$, and $P_7 \cap P_3 = \{e\}$ (since 3 doesn't divide 7), $M \cong Z_7 \rtimes_{\bar{alpha}} Z_3$. If $Ker \bar{\alpha} = Z_3$, then $M \cong Z_{21}$. If $Ker \bar{\alpha} = \{e\}$, then $\bar{\alpha}: Z_3 \rightarrow Z^{\times}_7$ is one-to-one. Case 1: $M \cong Z_{21}$, so $G \cong Z_{21} \rtimes_{\alpha} Z_2$. If $ker \alpha = Z_2$, then  $G \cong Z_{42}$.
If $ker \alpha = \{e\}$, then $\alpha: Z_2 \rightarrow Z^{\times}_{21}$ is one-to-one. Case 2: $M \cong Z_7 \rtimes_{\alpha} Z_3$. $G \cong (Z_7 \rtimes_{\bar{\alpha}} Z_3) \rtimes_{\alpha} Z_2$. But I'm not sure how to simplify this...In other words, I want to simplify this to know if it is isomorphic $Z_7 \rtimes_{\alpha} Z_6$... Thanks in advance","['group-theory', 'abstract-algebra']"
375498,"What does $\{(x_1, x_2, x_3) \in\mathbb R^3: x_3 \leq x_2 \leq x_1 \}$ look like?","What does $\{(x_1, x_2, x_3) \in\mathbb R^3: x_3 \leq x_2 \leq x_1 \}$ look like? 
It seems to be a linear convex cone with vertex at the origin.
I am trying to visualize it but cannot. Thanks!",['geometry']
375529,Counting binary operations on a set with $n$ elements,I am trying to solve following problem but not able to find any way to proceed. Let $S$ be a set having $n$ elements. Can we count about number of binary operations that can be defined on a set? Can we also count number of commutative binary operations defined on  $S$? Thanks for the help and suggestions,"['elementary-set-theory', 'abstract-algebra', 'combinatorics']"
375534,"Let $F \supset E \supset K$, $E/K$ and $F/E$ Galois and every $\sigma \in \mathrm{Aut}_K(E)$ extendible to $F$. Then $F/K$ is Galois.","Let $F$ be an extension field of a field $K$ . Let $E$ be an intermediate field such that $E$ is Galois over $K$ , $F$ is Galois over $E$ , and every $\sigma\in\mathrm{Aut}_{K}E$ is extendible to $F$ . Show that $F$ is Galois over $K$ . The definition of a Galois extension $F/K$ in the book is that the fixed field of $\mathrm{Aut}_{K}F$ is $K$ . So I just need to show that $\mathrm{Aut}_{K}F$ does not fix elements of $F\setminus K$ . If $x\in E\setminus K$ , then since $E/K$ is Galois, there exists $\sigma\in\mathrm{Aut}_{K}E$ such that $\sigma(x)\neq x$ . By the hypothesis, $\sigma$ extends to $F$ . If $x\in F\setminus E$ , then since $F/E$ is Galois, there exists $\tau\in\mathrm{Aut}_{E}F$ such that $\tau(x)\neq x$ and also $\tau\in\mathrm{Aut}_{K}F$ . It seems that this completes the argument, am I right?","['galois-extensions', 'extension-field', 'abstract-algebra', 'galois-theory', 'field-theory']"
375540,Gradient in Riemannian manifold,"I have a calculation involving a gradient and a parametrization, but I haven't been able to find out the relation between them. Let me explain. Let $f:X↦R$ be a smooth function and $\mathrm{grad}f\in \mathcal T X$ its gradient. $X$ is a Riemannian manifold, and therefore $\mathrm{grad}f$ has a norm. While my calculation requires this norm, the only expression that I have is through a parametrizion $\alpha:R^n \mapsto X$. This parametrization, yields the vector $\mathrm{grad}f \circ \alpha \in \mathcal T R^n$, whose norm I can compute. However, I haven't been able to find how these two norms are related. I hope someone can point me in the right direction.
Thank you and have a nice day.","['riemannian-geometry', 'differential-geometry']"
375544,Slope of a straight line,Why is this so that a higher value of slope indicates a steeper incline? I can't take it into my head. What could be the reason behind that? I know that it is a fact because I've also noticed it but don't know the reason which could assist my understanding of this concept.,"['analytic-geometry', 'algebra-precalculus']"
375555,Why is $\frac{1}{\frac{1}{0}}$ undefined?,"Is the fraction $$\frac{1}{\frac{1}{0}}$$ undefined? I know that division by zero is usually prohibited, but since dividing a number by a fraction yields the same result as multiplying the number by the fraction's reciprocal, you could argue that $$\frac{1}{\frac{1}{0}} = (1)\left(\frac{0}{1}\right) = 0$$ Is that manipulation permissible in this case? Why or why not?",['algebra-precalculus']
375589,"If $G/H$ and $H$ are finitely generated, then so is $G$","I'm trying to prove that if $H$ is a normal subgroup of a group $G$ such that $H$ and $G/H$ are finitely generated, then G is finitely generated also. I'm trying to find a finite set $X$ such that $G$ is generated by $X$, but I have no ideal how to find this set using the finite generator sets of $H$ and $G/H$. I need help Thanks in advance","['combinatorial-group-theory', 'group-theory', 'abstract-algebra']"
375611,Understanding homotopy types,"I am currently self studying algebraic topology from the book ""topology and groupoids"". I don't understand classifying spaces up to homotopy type. By ""I don't understand"" I don't mean that I don't understand the proof of facts about homotopy types that are in my book. These facts follow easily from the definitions and I did not face any trouble in proving them. By not understanding, I mean that I don't realize why this is being done. I know that homeomorphic  spaces are of the same homotopy type. So it seems to me that homotopy type is some sort of a topological invariant. However, it is not clear to if determining whether 2 spaces are of the same homotopy type is really easier than determining if the 2 spaces are homeomorphic. Determining whether 2 spaces are both connected seems an easier problem to me than determining if they are homeomorphic. I think I will be much more satisfied, if someone can show me an example of showing that 2 spaces are not homeomorphic by showing that they are not of the same homotopy type. Thanks for your time.","['general-topology', 'algebraic-topology']"
375642,How Can the Birthday Problem be solved directly,"How would one go about solving a variant of the birthday problem given below ""What is the probability of at least two people having the same birthday in a group of 23"" I know the answer is just above 0.5 but I am interested in how it is solved when one computes it by taking probabilities of two people having the same birthday, then of three people having the same birthday and so on rather solving it by finding the probability of people not having the same birthday.",['probability']
375655,Extending a smooth map,"When can I extend a smooth map $f:\mathbb{R^2}-\lbrace 0 \rbrace \to S^1$ to a smooth map $\tilde{f}:\mathbb{R^2} \to S^1$.  For instance, consider $g(x,y)=(x,y)/\sqrt{x^2+y^2}$?  Am I able to extend that to a smooth map $g$?  My end goal is having the smooth map from $\mathbb{R^2}$ to $S^1$.  It can be any map, this is just how I am trying to do it. Thanks!","['differential-topology', 'manifolds', 'differential-geometry']"
375666,"$\mathbb{Z}/m\mathbb{Z}$ is free when considered as a module over itself, but not free over $\mathbb{Z}$.","I am struggling to explain the following statement to myself in a convincing way. $\mathbb{Z}/m\mathbb{Z}$ is free when considered as a module over itself, but is not free when considered as a $\mathbb{Z}$-module. But first of all, there are a few related questions that I am in doubt and could not find any useful clues. When we defined a module R$\times$M$\to$M with R a ring and M an Abelian group, does R has to be a subset of M? The definition does not say so. But if we think about it, for example, take R be $\mathbb{Q}$ and M be $\mathbb{Z}$, then the map $\mathbb{Q}\times\mathbb{Z}$ need not be in $\mathbb{Z}$. What is wrong with my reasoning? There is a statements: ""If module is free, not every generating set necessarily contain a basis. Consider, for example, the generating set {2,3} for the $\mathbb{Z}$-module $\mathbb{Z}$. The subset {2} $\subseteq$ $\mathbb{Z}$ is a linearly independent set that can not be extended to a basis."" My questions are: the generating set {2,3} is linearly independent so it is a basis, correct? Then what does it mean by ""can not be extended to a basis"", can we just add the element 3 into {2} then it becomes a basis? Consider again the original question ""$\mathbb{Z}/m\mathbb{Z}$ is free when considered as a module over itself, but is not free when considered as a $\mathbb{Z}$-module"". Suppose I was wrong in point 1. that R has to be a subset of M, clearly $\mathbb{Z}$ is not a subset of $\mathbb{Z}/m\mathbb{Z}$. But in this case, does the $\times$ operation in
$\mathbb{Z}\times\mathbb{Z}/m\mathbb{Z}\to\mathbb{Z}/m\mathbb{Z}$ always mean $\times$ (modulo m), then what is the point of changing the ring R since the operation is the same and the image is the same? Thank you very much for all helps!","['modules', 'ring-theory', 'abstract-algebra']"
375668,Is this definite integral impossible?,"From my understanding when you integrate $f(x)$ you get $F(x)+C$, and when finding a definite integral the $C's$ cancels out due to subtraction. However, I came across an example where the $C$ doesn't cancel out: so I started with the following differential equation: $$(1+x^2) \frac{dy}{dx}=2xy$$ and suppose I wanted to find the area under $dy/dx$ between $a$ and $b$. All you simply have to do is find $y$, evaluate at $b$ and $a$, and subtract. The solution to this equation is $y=(x^2+1)e^C$. Now if you evaluate and subtract, you get $(b^2+1)e^C-(a^2+1)e^C$. Is this integral impossible unless I have more information which allows me to determine $C$? Thanks!","['definite-integrals', 'ordinary-differential-equations', 'calculus', 'integration']"
375685,How do you prove the converse Cantor's intersection theorem?,"I was given a proof in class on how to prove it in one direction, but I was wondering: is there a way you could prove that if the intersection of a decreasing sequence of closed sets with diameters tending to zero is nonempty, then the metric space is complete?","['general-topology', 'metric-spaces']"
375700,Probability integral transform: Is it integral transform? Can it be for discrete distribution?,"From Wikipedia the probability integral transform or transformation relates to the result that data values that are modelled as being random variables from any given continuous distribution can be converted to random variables having a uniform distribution. This holds exactly provided that the distribution being used is the true distribution of the random variables; if the distribution is one fitted to the data the result will hold approximately in large samples. I was wondering if the probability integral transform can be
generalized to a random variable with a discrete distribution? Or any distribution? Is the probability integral transform an integral transform or
some generalization of it? an integral transform is any transform T of the following form: $$
        (Tf)(u) = \int \limits_{t_1}^{t_2} K(t, u)\, f(t)\, dt  $$ Thanks and regards!","['probability-theory', 'integral-transforms']"
375710,What is the half-derivative of zeta at $s=0$ (and how to compute it)?,"[Update 3:] I gave a new partial answer following the ansatz in question Q3 . I leave the other parts of the question untouched, they are also partially answered in specialized other questions in MSE. I'm trying to understand the concept of fractional derivatives and am fiddling with the examples at wikipedia. The a 'th derivative of a monomial in x , where a can be fractional is accordingly $$ {d^a \over dx^a} x^m = { \Gamma(1+m) \over \Gamma (1+m-a) } x^{m-a} $$. Q1: But what happens for some function $f(x)$ if I want to evaluate the half-derivative at zero? Originally I'm interested in the fractional derivatives of the zeta at zero. Because I thought that the monomial-halfiterative is the most easy one to understand I tried first the power-series expression of the zeta-function 
$$ \zeta(x) = - {1 \over 1-x} + \sum_{k=0}^\infty w_k x^k $$ where $w_k$ are some coefficients beginning with $w_0=0.5, w_1=0.081... , w_2=-0.0031... , \cdots $ But if I want to find the (1/2)'th derivative at $x=0$ I need definitions how I should handle the fractional powers of zero. Q2: How can I evaluate the fractional derivative of the leading fraction ${1 \over 1-x}$? Can I do better than to express the fraction by its power series and do the derivations termwise at the monomials? Q3: Or can I do something like with the integer derivatives of the zeta at $s=0$ where I express it as the Dirichlet-series having the logarithms in the numerators? [update]: concerning Q3 , I've now used the alternating-zeta version and assumed, that
$$ {d^{1/2} \over dx^{1/2}} {1 \over k^x}={d^{1/2} \over dx^{1/2}} \exp(x(-\log(k))) =(-\log(k))^{1/2} {1 \over k^x} = i \cdot (\log(k))^{1/2} {1 \over k^x} $$
and then I set $x=0$ and evaluate the alternating series $$ \eta(0)^{(1/2)} \underset{\mathfrak E}{=} \sum_{k=0}^\infty \left((-1)^k log(1+k)^{1/2}\cdot i \right)\sim - 0.347006596200 \cdot i $$ where $\mathfrak E $ means Euler-summation of the divergent series. However, even if that result is meaningful this does not yet help much because I've now no further idea how I could use the Euler's zeta/eta-conversion-formula here. (I have just developed the conversion scheme for the integer derivatives, but that transforms to an infinite series if this is at all generalizable to fractional indexes) [update2] : I tried the Riemann-Liouville-formula for the half-derivative as given by @J.M. in MSE , but the result is inconclusive. First, I need to handle zeros in the denominators, and second, if I replace them by limiting expressions with $\epsilon \to 0$ the numerical integration seems to diverge either to $-\infty$ or $- i \infty$ depending on whether I approach zero from positive or from negative values. So I need some help even for this...","['fractional-calculus', 'sequences-and-series', 'riemann-zeta']"
375711,Eigenvector of matrix of equal numbers,"For matrix the matrix $$A = \begin{bmatrix}
3&1&1\\
1&3&1\\
1&1&3\\
\end{bmatrix}$$ with eigenvalues $\lambda_1=5$, $\lambda_2=2$, $\lambda_3=2$, I am trying to find the corresponding eigenvector corresponding to the eigenvalue 2. I got $$(A - 2I_3) = \begin{bmatrix}
1&1&1\\
1&1&1\\
1&1&1\\
\end{bmatrix}$$ Reducing it (row reduced echelon form), I get: $$\left[
\begin{array}
{ccc|c}
1&1&1&0\\
0&0&0&0\\
0&0&0&0\\
\end{array}\right]$$ Ending up with $x_1 + x_2 + x_3 = 0$. How would I find the eigenvector from there? Usually, I end up getting two equations and it's easy from there. How would you do it with one?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
375742,Find the probability of having 3 cards of the same suit and 2 cards of the same suit in a 5 card hand from a standard 52 card deck? (Method),"I'm trying to understand why my method doesn't work to answer the question: What is the probability of having 3 cards of the same suit and $2$ cards of the same suit (but a different suit than the first three) in a $5$ card hand from a standard $52$ card deck? The method that seems to work uses combinations. You do:
$$
{4 \choose 1}\cdot{13 \choose 3}\cdot{3 \choose 1}\cdot{13 \choose 2} = 267,696
$$
To find the number of successful possibilities and then
$$
{52 \choose 5} = 2,598,960
$$
To find the number of total possibilities. So the answer is:
$$
\frac{267,696}{2,598,960}
$$ I'm wondering why figuring out the individual card chances and then multiplying them doesn't work. My method goes like this: 
$$
\frac{12}{51}\cdot\frac{11}{50}\cdot\frac{39}{49}\cdot\frac{12}{48}
$$ After you've picked your first card you have a $\frac{12}{51}$ chance of getting a matching second card, then you've a $\frac{11}{50}$ chance of getting your third card to match. For the other two cards you've a $\frac{39}{49}$ chance, because you still have three suits left. You need the next one to match, so now you have a $\frac{12}{48}$ chance. Shouldn't multiplying all of those individual probabilities yield the same answer as the first method? I realize that there are different ways to order the individual probabilities, but when they're all multiplying, shouldn't it all work out to be the same? I've noticed that multiplying the answer to my method by $10$ makes it equal to the first method's answer. Is there a reason that works?","['statistics', 'card-games', 'probability', 'combinatorics']"
375763,Characterization of short exact sequences,"The following is the first part of Proposition 2.9 in ""Introduction to Commutative Algebra"" by Atiyah & Macdonald. Let $A$ be a commutative ring with $1$. Let $$M'
\overset{u}{\longrightarrow}M\overset{v}{\longrightarrow}M''\longrightarrow
 0\tag{1} $$ be sequence of $A$-modules and homomorphisms. Then the sequence (1) is exact if and only if for all $A$-modules $N$, the
  sequence  $$0\longrightarrow \operatorname{Hom}(M'', N)
 \overset{\overline{v}}{\longrightarrow}\operatorname{Hom}(M,
 N)\overset{\overline{u}}{\longrightarrow}\operatorname{Hom}(M', N)
 \tag{2} $$ is exact. Here, $\overline{v}$ is the map defined by $\overline{v}(f)=f\circ v$ for every $f\in\operatorname{Hom}(M'', N)$ and $\overline{u}$ is defined likewise. The proof one of direction, namely $(2)\Rightarrow (1)$ is given in the book, which I am having some trouble understanding. So assuming (2) is exact sequence, the authors remark that ""since $\overline{v}$ is injective for all $N$, it follows that $v$ is surjective"". Could someone explain why this follows? Given that $\overline{v}$ is injective, we know that whenever $f(v(x))=g(v(x))$ for all $x\in M$, we have $f=g$. I am not sure how we conclude from this surjectivity of $v$. Thanks!","['modules', 'homological-algebra', 'abstract-algebra', 'exact-sequence']"
375771,How do you calculate the probability of simultaneous events?,"How do you calculate the probability of simultaneous events?  As in, given four simultaneous events each with a 10% probability, what are the odds that ONE of them occurs?  Obviously it isn't 40%, because...well, if you have ten events that probability clearly isn't 100%! (Nope, not homework!  Video games, probability of elemental effects from a given spell)",['probability']
375786,Is there a good intuitive way to understand why matrix B is inverse of A when matrix A|I is turned into I|B,"I'm looking for some help with my intuition of basic matrix operations, specifically finding a matrix's inverse (as per my subject line). I have no problems with the steps. The basic row operations are relatively simple. I'd like to understand why/ how this solves the system of linear equations. I know my question is asking more (or arguably less) than a concrete sequence of steps, a theorem, etc. But I think someone who understands linear algebra much better than I can get through to me better than my texts' treatment, which is little more than a worked example. Thanks in advance..","['matrices', 'linear-algebra', 'inverse']"
375829,Entire + periodic in imaginary direction + bounded on the real line implies constant?,"I was reading some slides from a lecture. In a proof, there arose the need to show a certain function $f : \mathbb{C} \to \mathbb{C}$ was constant. The argument proceeded by checking that $f$ was entire $f(z+i) = f(z)$ for all $z$ $f$ was bounded on $\mathbb{R}$ and then concluding that $f$ must be constant. I followed the proofs of the three claims no problem, but my complex analysis is weak enough that I'm unsure how one is supposed to conclude that $f$ is constant. Clearly Liouville's theorem does not apply directly. My guess is that some kind of boundary principle is being applied to the ""rectangle"" $R = \{ z \in \mathbb{C} : 0 \leq \Im(z) \leq 1 \}$. For instance, if the maximum of $|f|$ on $R$ must occur on the boundary, then the result follows. My complex analysis is patchy enough, though, that I'm unaware if such a result. Added: Relevant: Lindelof's theorem The above article contains an enlightening example. If we take $f(z) = \exp(\exp(2 \pi z))$,  then (1) and (2) are satisfied. (3) is ""half satisfied"" in the sense that $\lim_{t \to -\infty} f(z) = 1$ here.","['maximum-principle', 'complex-analysis']"
375833,Solving quadratic equations by completing the square.,"Graphing $y=ax^2+ bx + c$ by completing the square Add and subtract the square of half the coefficent of $x$ . Group the perfect square trinomial. Write the trinomial as a square of a binomial. Rewrite $y = x^2 + 6x + 8$ into $y = a(x-h)^2 + k$ . I've tried solving this but I get a bit confused at the step where I have to ""write the trinomial as a square of a binomial"". Not exactly sure how to do that.","['factoring', 'algebra-precalculus', 'quadratics']"
375867,Find the value of $\lim_{x \rightarrow \infty}\left(\frac{\pi}{2}-\tan^{-1}x\right)^{\Large\frac{1}{x}}$,Can this limit be solved without using L'Hopital's rule : $$\lim_{x \rightarrow \infty}\left(\frac{\pi}{2}-\tan^{-1}x\right)^{\Large\frac{1}{x}}$$ Answer of this limit is : $1$,"['calculus', 'limits']"
375870,How do you determine the local extrema points for $y=\sqrt{3}\cos(3x)+\sin(3x)$,"$$y=\sqrt{3}\cos(3x)+\sin(3x); 0\le{x}\le{\frac{2\pi}{3}}$$ I know that the local extrema can be determined by using the first derivative test. I took the derivative of $y$ and got $$y'= -3\sqrt{3}\sin(3x)+3\cos(3x)$$ I then solved the derivative for when it's value is $0$ and got $x=\dfrac{\pi}{2}$ . I then used this critical point and subdivided the interval. I found that there were 3 points of local extrema after doing all my work, local maximum $x=0, \dfrac{2\pi}{3}$ and local minimum $x=\dfrac{\pi}{2}$. However according to the online homework, there were 4 different points of extrema. Local maximum $x=\dfrac{\pi}{18},\dfrac{2\pi}{3}$ and local minimum $x=0,\dfrac{7\pi}{18}$. I am really confused as to how there are 4 points of local extrema, did I leave out an answer somewhere? I am also confused as to how they got $x=\dfrac{\pi}{18},\dfrac{7\pi}{18}$ as points of local extrema. Could someone explain this to me?","['optimization', 'trigonometry', 'calculus', 'derivatives']"
375874,show that $f(z)$ is a polynomial in $z.$ [duplicate],"This question already has answers here : to show $f$ is a polynomial in $z$ (3 answers) Closed 11 years ago . Let $f(z); z = x + iy,$ be an analytic function and $u$ be its real part. If
  $u$ is a polynomial in the variables $x$ and $y,$ then show that $f(z)$ is a
  polynomial in $z.$ I need to confirm my attempt: $u,v$ must satifies the C-R equation. So, $u_x=v_y\implies v=\int u_x dy,$ a polynomial in $x,y.$ Consequently $f=u+iv$ is a polynomial in $x+iy.$",['complex-analysis']
375880,Are there only 2 clopen sets on real plane?,How can I prove that the only open and closed sets on the real plane are empty set and real plane itself? Preferably by using order theory. Thanks.,"['general-topology', 'elementary-set-theory', 'order-theory']"
375892,using fixed point theorem,"Hi I want to use the fixed point theorem to show that for 
$G: \mathbb{R}^n \rightarrow \mathbb{R}^n$
$G(x)= \epsilon M x + \max(x,y)$, here $y$ is given and $\max(x,y)$ is the vector with component $\max(x_i,y_i)$, $M$ is negative definite $n \times n$ matrix, $G(x)=x$ has a solution. (Note this is for any $\epsilon >0 $). First, I showed that $G$ is a non expansive map for $\epsilon < \dfrac{1}{\|M\|}$. So the fixed point for contraction map wouldn't work so I changed direction. I know that $G$ is a continuous map, so if I can find a closed set $B$ (ball) in $\mathbb R^n$ such that $G$ maps from $B$ to $B$ then the Brower's fixed point theorem says that $G$ has a fixed point. I can not figure out $B$. I had difficulty to deal with $|\max(x,y)|$. Any thought on this would be very much appreciated!","['complex-analysis', 'linear-algebra', 'real-analysis']"
375898,Evaluate a limit of an integral,"Here is another Prelim Question. Evaluate:
$$
I = \lim_{\epsilon\to 0} \frac{1}{2\pi\epsilon}\int_{-\infty}^\infty\int_{-\infty}^\infty f(x,y)\frac{y}{\epsilon}\exp{\frac{-(x^2+y^2)}{2\epsilon}}dxdy
$$
where $f$ and $\nabla f$ are continuous and bounded. So I have tried to change the integration order and used integration by parts and then changing to polar coordinates to obtain
$$
I = \frac{\partial f}{\partial y}(0,0) + \lim_{\epsilon \to 0}\frac{1}{2 \pi}\int_0^{2\pi}\int_0^\infty\frac{\partial}{\partial r}\left(\frac{\partial f}{\partial y}(r\cos(\theta),r\sin(\theta) )\right) \exp\left(\frac{-r^2}{2\epsilon}\right) dr d\theta
$$
I hope the final answer is the first term and we can show the second term goes to zero.  Can we just apply DCT?  I am getting second order partials in the second term and we only know information for the first order derivatives.  Note that 
$$
\frac{\partial }{\partial r} = \cos(\theta)\frac{\partial}{\partial x}+\sin(\theta)\frac{\partial}{\partial y}
$$","['integration', 'limits']"
375910,"Log problem, $u$ substitution the only way?","Okay so basically I want to know if you can solve this log equation without the use of u substitution: $${\log_4{\log_3{x}}} = 1$$ I believe that u substitution is the only way to solve this problem, but please prove me wrong if theres another way to do so.","['logarithms', 'algebra-precalculus']"
375924,Non-central subgroup which is maximal among abelian normal subgroups is self-centralizing?,"If $G$ is a supersolvabe group, and $A$ is a maximal among abelian normal subgroups of $G$, then the centralizer of $A$ in $G$ is $A$ itself (see link ). My question is about the importance of the hypothesis that ""$G$ is supersolvable"". Question: Let $G$ be any group, $Z(G)$ be its center, and $A$ be a maximal among abelian normal subgroups of $G$, such that $Z(G)\neq A$ (i.e. $Z(G)<A$). Prove or disprove: the centralizer of $A$ in $G$ is $A$ itself ? (The examples I found were direct product of an abelian group with a non-abelian simple group; but here we can not have a maximal abelian normal subgroup $A$ such that $Z(G)\neq A$.)","['finite-groups', 'group-theory', 'abstract-algebra']"
375928,$2^n$ subsets of set with $n$ elements $\overset{?}{\implies}$ $\mathbb{R}$ has $2^\infty=\infty$ subsets,"I'm told that there are $2^n$ subsets of a set with $n$ elements, but does this imply that $\mathbb{R}$ has $2^\infty=\infty$ subsets?",['elementary-set-theory']
375934,Matrix Multiplication - Why Rows $\cdot$ Columns = Columns?,"I'm nearing the end of my first year of Calculus and am pretty confident in the parts of it I've learned, yet I still don't have a good understanding of matrices, which seem like they should be easier to understand and work with. They were never formally taught in any of the math courses my school has given me and were only ever briefly mentioned. I don't fully understand what they represent or the logic behind how they are multiplied. Why are the left matrix's rows multiplied by the second matrix's columns to form columns in the resulting matrix? Why not columns by rows to form rows? Was it an arbitrary decision that was made or does it fit what concept of the matrix? I could memorize how to perform operations on matrices and how to use them to solve certain problems, but what I'm trying to do is understand them conceptually. Specifically, I was reading a page about the matrix representation of 2d transformations as used by CSS3 and found it all very confusing.",['matrices']
375950,Asymptotic expansion of the integral $\int_0^1 e^{x^n} dx$ for $n \to \infty$,"The integrand seems extremely easy: $$I_n=\int_0^1\exp(x^n)dx$$ I want to determine the asymptotic behavior of $I_n$ as $n\to\infty$. It's not hard to show that $\lim_{n\to\infty}I_n=1$ follows from Lebesgue's monotone convergence theorem . However, when I try to obtain more precise results, I confront difficulties. Since it's not of the canonical form of Laplace's method , I have no powerful tools to estimate $I_n$. Is there any good approach for that? I will be pleased if we can obtain the asymptotic expansion of $I_n$. Thanks!","['asymptotics', 'integration']"
375953,"$M$ is compact, non-empty, perfect, and $M \cong M \times M$. Must $M$ be homeomorphic to the Cantor set, the Hilbert cube, or some combination?","Assume that $M$ is compact, non-empty, perfect, and homeomorphic to its Cartesian square, $M \cong M \times M$.  Must $M$ be homeomorphic to the Cantor set, the Hilbert cube, or some combination of them? An interesting triple-starred problem from Pugh's ""Real Mathematical Analysis"".  This is not from an assignment or anything graded, I'm just curious as to what the right answer is and the route that one may take to get there.","['general-topology', 'real-analysis', 'analysis']"
375954,Wisdom of the crowd in estimative calculation,"This question came to mind when I was discussing a contest with a friend in which the challenge was to estimate the number of particles in a glass tube. The amount of particles (close to $5\cdot 10^4$) was large enough to have people give wrong estimates by more than 1 order of magnitude (i.e. $5\cdot 10^3$ and $5\cdot 10^5$). My question is: does the wisdom of the crowd still apply to this situation with orders of magnitude estimation errors? What I could imagine is that if there are equally many people off by an order of magnitude in the 'up' and 'down' directions you would rather need to take something like a logarithmic average. A second question is related to the way many people actually made the estimate. Instead of a random guess they made an estimative calculation saying: well, the length of the particle bed in the tube is $h_t\approx0.5$ m, the tube diameter is $d_t\approx 3 $ cm, the particles are approximately spherical with a diameter of $d_p\approx3$ mm and the void fraction will be about $\epsilon=40\%$ . Then they came up with a number based on the calculation:
$$N_p=\frac{V}{V_p}=\frac{6(1-\epsilon)h_t d_t^2}{d_p^3} $$ My second question is: does the wisdom of the crowd still apply for this estimative calculation? Or should we rather start working with averages per variable (particle diameter, void fraction etc) and use those averages for the final calculation? Or will the analysis be flawed anyway if the guesses are calculated.","['statistics', 'probability']"
375967,Probability density function of a product of uniform random variables,"Let $z = xy$ be a product of two uniform random variables, with $x$ having the range $[a, b)$ and $y$ the range $[c, d)$. What is the probability density function of $z$, and how is it calculated?","['statistics', 'uniform-distribution', 'probability-distributions']"
375972,Eigenvalues of a Hermition Matrix do not cross,"Wikipedia's article on avoided crossing asserts that ""The eigenvalues of a Hermitian matrix depending on N continuous real parameters cannot cross except at a manifold of N-2 dimensions."" If it's true, does anyone have an elegant proof of this statement?  Does anyone have a good interpretation or a good intuition for why this would be true?",['linear-algebra']
375979,A bound on a sum of five sines,"If $0<x_{1},\ldots,x_{5}<\pi$ and $x_{1}+\ldots+x_{5}=\pi$,
is it true that $\sin x_{1}+\ldots+\sin x_{5}\le5\sin\frac{\pi}{5}$?",['calculus']
375982,Rank of the difference of matrices [duplicate],This question already has an answer here : Prove that $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A + B)$ (1 answer) Closed 11 years ago . Let $A$ and $B$ be to $n \times n$ matrices. My question is: Is $\operatorname{rank}(A-B) \geq \operatorname{rank}(A) - \operatorname{rank}(B)$ true in general? Or maybe under certain assumptions?,"['matrices', 'linear-algebra']"
