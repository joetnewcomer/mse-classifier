question_id,title,body,tags
4400200,The category $\mathbf{Ban_1}$ equipped with a non-obvious functor $U_1: \mathbf{Ban_1} \to \mathbf{Set}$,"Wikipedia says that For technical reasons , the category $\mathbf{Ban_1}$ of Banach spaces and linear contractions is often equipped not with the ""obvious"" forgetful functor but the functor $U_1: \mathbf{Ban_1} \to \mathbf{Set}$ which maps a Banach space to its (closed) unit ball. The category $\mathbf{Ban_1}$ , in view of the above functor, is given as an example of a concretizable category. What are these technical reasons ? The choice of the functor seems quite odd and non-obvious. It would be great if someone could throw some light, please!","['banach-spaces', 'functors', 'functional-analysis', 'category-theory']"
4400204,"Prove that $\lim_{nâ†’\infty}\frac{\sqrt{n^2+a^2}}{n}=1$ using ""$\varepsilon \to N$"" definition","how to prove this equation with the "" $\varepsilon \to N$ ""  definition?  I feel have some trouble.Thanks! $\lim_{n \to\infty}\frac{\sqrt{n^2+a^2}}{n}=1\\$ I complete my solution:
when $a=0$ it's obviously true. When $a \neq 0$ : $$\begin{align}
\frac{\sqrt{n^2+a^2}}{n}-1&=\frac{\sqrt{n^2+a^2}}{n}-\frac{n}{n}
\\&=\frac{\sqrt{n^2+a^2}-\sqrt{n^2}}{n}\\&=\frac{1}{n}\cdot \frac{a^2}{\sqrt{n^2+a^2}+n}\lt\frac{a^2}{\sqrt{n^2}+n}\cdot\frac{1}{n}\lt\frac{a^2}{n(\sqrt{n^2}+\sqrt{n^2})}=\frac{a^2}{2n}
\\ for\quad \forall \varepsilon \gt0, \quad when\quad \frac{a^2}{2N^2}\lt \varepsilon, \mathit N\gt \frac{a}{\sqrt{2\varepsilon}}+1,|\frac{\sqrt{n^2+a^2}}{n}-1|\lt\varepsilon.
\end{align}$$","['limits', 'real-analysis']"
4400234,Homotopy equivalence onto special fiber,"The following proposition appears in Peters and Steenbrink's book on Mixed Hodge Structures . Proposition([Peters--Steenbrink, Proposition C.11]) If $f\colon X\to\Delta$ is proper and smooth over $\Delta^\ast:=\Delta-\{0\}$ , then the inclusion $X_0\hookrightarrow X$ is a homotopy equivalence. Does anyone know a reference for the proof of this result?","['complex-geometry', 'morse-theory', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
4400257,Formalization of a simple statement regarding intervals,"I have the following statement: ""The open interval (0,1) contains no 'largest' number"" Which I am asked to ""formalize"" (I hope I am using the correct mathematical jargon here). I think the most correct way to formalize it is: $$\forall a (((a>0) \wedge (a<1)) \rightarrow \exists b ((a<b) \wedge (b<1)))$$ I would like to hear your opinion about it. But is the following also correct? If not, why? $$\forall a \exists b((a>0) \wedge (a<1) \wedge (a<b) \wedge (b<1))$$","['logic', 'discrete-mathematics']"
4400303,Measure of a subset of $\mathbb{R_{>0}}$ that intersects every $\mathbb{N}_{>0}x$,"Imagine you're looking at a light indicator that flashes periodically, but with an unknown period. The goal is to keep your eyes open for a finite amount of time to see it flash at least once. Formally, let $A$ be a subset of $\Bbb{R}_{>0}$ , such that: $$ \forall x \in \Bbb{R}_{>0}, \exists n \in \mathbb{N}_{>0},nx \in A. $$ Another way to put it is: $$ \Bbb{R}_{>0}= \bigcup_{n \in \mathbb{N}_{>0}} \frac{1}{n} A. $$ Assume that $A$ is Lebesgue-measurable. Can it be of finite measure?","['real-numbers', 'measure-theory', 'lebesgue-measure']"
4400308,Why does the variance of the number of occurrences of a subsequence in a random sequence depend on the subsequence?,"Question Why is the variance of the number of times we see the subsequences $[0, 0, 0, 1]$ and $[1, 0, 1, 0]$ in a random sequence of 1024 bits differ from one another and the binomial distribution $B(1021, 2^{-4})$ ? How can I calculate the probability and variance of seeing a particular subsequence? Background When looking at different ways of evaluating the output of a random number generator, I was looking at checking if the number of occurrences of a particular subsequence in a string of bits was within a confidence interval. As I played around with this method, I found some surprising results. I generated sequences of 1024 bits and counted the number of times a specific subsequence of length four occurred. I expected that out of the 1021 sliding window positions, the distribution of the number of matches would follow a binomial distribution of $B(1021,2^{-4})$ , which would have a mean of 63.8 and a variance of 59.8. However, the results I observed were quite different: Other Thoughts The differences in variance seem to be related to the fact that the subsequences are not independent, i.e., if you see the subsequence $[0, 0, 0, 1]$ then if you shift the sliding window over three places, you are guaranteed not to see that subsequence again no matter the next value. However, for the subsequence $[1, 0, 1, 0]$ you can shift over the window two times and have a chance of seeing the same subsequence again. But this would lead me to believe that the subsequence $[1, 0, 1, 0]$ is more likely as you would have more chances of seeing it in a sequence. But this is not the case. Code import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate 100,000 random 1024 bit sequences
random_array = np.random.choice(a=[False, True], size=(100000, 1024))

# Generate a 4 element sliding window view
sliding_window_view = np.lib.stride_tricks.sliding_window_view(
    random_array, 4, axis=1
)  # requires np >= 1.20

# Count the number of sliding windows that equal [1, 0, 1, 0]
number_of_1010 = np.all(sliding_window_view == [1, 0, 1, 0], axis=2).sum(axis=1)
# Count the number of sliding windows that equal [0, 0, 0, 1]
number_of_0001 = np.all(sliding_window_view == [0, 0, 0, 1], axis=2).sum(axis=1)
# Generate my expected distribution
expected = np.random.binomial(1021, 2**-4, size=100000)

# Show density plot
sns.kdeplot(
    number_of_1010,
    bw=0.5,
    label=(
        f""[1, 0, 1, 0]; mean {np.mean(number_of_1010):.2f}; var""
        f"" {np.var(number_of_1010):.2f}""
    ),
)
sns.kdeplot(
    number_of_0001,
    bw=0.5,
    label=(
        f""[0, 0, 0, 1]; mean {np.mean(number_of_0001):.2f}; var""
        f"" {np.var(number_of_0001):.2f}""
    ),
)
sns.kdeplot(
    expected,
    bw=0.5,
    label=(
        f""B(1021, 2^-4); mean {np.mean(expected):.2f}; var""
        f"" {np.var(expected):.2f}""
    ),
)
plt.legend()
plt.show()","['discrete-mathematics', 'combinatorics', 'probability']"
4400312,Confusion when using the product rule (differentiation),"I'm quite confused by something related to the product rule that should be easy. Let $\phi:\mathbb{R}^p \to \mathbb{R}$ a diferentiable function. Define $$\varphi:\mathbb{R}^p \to \mathbb{R}, \quad \varphi(t) = e^{\phi(t)}, \quad t \in \mathbb{R}^p$$ The derivative of $\varphi$ , using the chain rule, is given by: $$\varphi'(t)=e^{\phi(t)} \phi'(t) = \varphi(t)\phi'(t)  \in \mathbb{R}^p$$ I am trying to use the product rule to find $\varphi''(t)$ : $$\varphi''(t)= \varphi'(t)\phi'(t) +\varphi(t)\phi''(t)$$ But note that the respective dimensions are not compatible: $$\underbrace{\varphi''(t)}_{p\times p}= \underbrace{\varphi'(t)}_{p\times 1} \, \,\underbrace{\phi'(t)}_{p\times 1} +\underbrace{\varphi(t)}_{1\times 1} \, \,\underbrace{\phi''(t)}_{p\times p}$$ I'm committing some bullshit because it doesn't make sense. help!","['calculus', 'derivatives']"
4400367,Proof that any element of a free abelian group which is not divisible by any $k>1$ can be extended to a basis,"Let $A$ be a free abelian group of rank $n$ , and let $\alpha_1 \in A\setminus \{0\}$ such that $\alpha_1 \not \in kA$ for all $k > 1$ . Do there always exist $\alpha_2,\ldots, \alpha_n \in A$ such that $\alpha_1,\ldots, \alpha_n$ is a basis for $A$ ? This question implies that the answer is yes, but it doesn't give any justification, and there's no proof in the accepted answer. Can anyone point me to a proof of the claim? Attempting to generalise the example in the linked question, we might try letting $e_1,\ldots, e_n$ be a basis and writing $\alpha_1 = c_1e_1 + \ldots + c_ne_n$ for integers $c_i$ . Then we would need a series of elementary column operations taking the vector $(c_1,\ldots, c_n)$ to a vector containing one $1$ and all other entries $0$ . Since $k\nmid \alpha_1$ for all $k>1$ , we have that the $c_i$ are mutually coprime, so there exist integers $x_1,\ldots, x_n$ with $c_1x_1 + \ldots c_nx_n = 1$ . Maybe we can somehow use this combination to construct the desired $1$ . Beyond that, I'm not too sure how to proceed.","['free-abelian-group', 'free-modules', 'integer-lattices', 'matrices', 'abelian-groups']"
4400393,Equivalent conditions for set-valued function,"Let $f:\mathcal P(X) \rightarrow\mathcal P(X)$ . Suppose that (B1) $f(\emptyset) = \emptyset$ (B2) $f(A) = f(A^\complement)$ (B3) $f(A) \subseteq B\cup f(B)$ if $A\subseteq B$ (B4) $f(f(A))\subseteq f(A)$ (B5) $f(A\cup B)\subseteq f(A)\cup f(B)$ for $A,B\in\mathcal P(X)$ . Proposition 1. Suppose that (B4) and (B5) hold. Then (X4) $f(A\cup f(A)) \subseteq A\cup f(A)$ follows. Proposition 2. Suppose that (B2), (B3), and (X4) hold. Then (B4) follows Proposition 1 and Proposition 2 tell me that I can replace (B4) with (X4) and vice versa. My goal is now to prove Proposition 3 and Proposition 4: Proposition 3. Suppose that (B1), (B2), (B3), (B4), and (B5) hold. Then (X5) $A\cap B\cap f(A\cap B) = A\cap B\cap (f(A)\cup f(B))$ follows. Proposition 4. Suppose that (B1), (B2), (B4) and (X5) hold. Then (B3) and (B5) follow. By (B2) and (B5) it follows that \begin{align*}
f(A \cap B) &= f(A^\complement \cup B^\complement) \\ &\subseteq f(A^\complement) \cup f(B^\complement) \\ &= f(A) \cup f(B). 
\end{align*} So the $\subseteq$ -inclusion for Proposition 3 is easy to show. However, I failed to prove the $\supseteq$ -inclusion. For Proposition 4 I was unable to prove either inclusion. I hope that someone here has an idea. This is a follow-up question of my earlier question Axiomatizations of the boundary operator . Oliver Diaz answer shows that $f$ , given axioms (B1), (B2), (B3), (B4), and (B5), yields the same topology as $f$ given the axioms (B1), (B2), (B4), and (X5). Note that $f$ here is the boundary operator $\partial$ ; see my other question for more details. Now I want to show that both axiomatizations not only yield the same topology but that I can replace (B3) and (B5) with (X5) (without refering to the topology in the proof). Is this possibe? Or do I have to take the ""detour"" via the topology in the proof? The lemma stated in Wang's master's thesis gives me the feeling that the additional condition $A\cap f(A) = \emptyset$ is necessary, which suggests using the ""detour"". However, these are only guesses. I hope someone here has a definite answer. Proof of Proposition 1. Let $A\in\mathcal P(X)$ . By (B5) and (B4) it follows that \begin{align*} f(A\cup f(A)) &\subseteq f(A) \cup f(f(A)) \\ &\subseteq f(A) \cup f(A) \\ &= f(A).\end{align*} Since $f(A) \subseteq A\cup f(A)$ , it follows that $f(A\cup f(A)) \subseteq A\cup f(A)$ . $\square$ Proof of Proposition 2. Trivially, it holds that $f(A)\subseteq A\cup f(A)$ . By (B3) and (X4) it follows that \begin{align*}
f(f(A)) &\subseteq f(A)\cup f(A)\cup f(A\cup f(A)) \\
&\subseteq f(A) \cup f(A)\cup A\cup f(A) \\
&= A\cup f(A).
\end{align*} Likewise, \begin{align*}
f(f(A^\complement)) &\subseteq f(A^\complement)\cup f(A^\complement)\cup f(A^\complement\cup  f(A^\complement)) \\
&\subseteq f(A^\complement)\cup f(A^\complement)\cup f(A^\complement) \cup A^\complement \\
&=A^\complement\cup f(A^\complement). 
\end{align*} By (B2) it follows that \begin{align*}
f(f(A)) &\subseteq (A\cup f(A)) \cap (A^\complement \cup f(A^\complement)) \\
&= (A\cap f(A))\cup A^\complement\cap f(A)) \\
&= (A\cap A^\complement)\cup (A\cap f(A)) \cup (A^\complement\cap f(A)) \cup (f(A)\cap f(A)) \\
&\subseteq \emptyset \cup f(A) \cup f(A) \cup f(A) \\
&= f(A).  
\end{align*} $\square$","['elementary-set-theory', 'general-topology']"
4400415,Construction of dense $C_4$-free graph,"Yesterday I created the topic where we discussed the following theorem: If $G=(V,E)$ is a simple and $C_4$ -free graph, then $|E|\leq \dfrac{|V|}{4}(1+\sqrt{4|V|-3})$ . It is important to notice that this result is false if graph has loops and multiple edges. In the following page of that book you can find the following: I would like to ask only two questions: I would like to check that $G$ is undirected graph. As far as I know in the case of directed graphs edges are ordered pairs of vertices. How to check that $G$ is undirected graph? In the theorem graph should be a simple. However, in this example some edges are loops. So it does not that the bound in Theorem 2.4 is optimal up to a constant. Am I right?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4400423,Matrix inverse step in SVD & ridge regression,"When we do OLS of $y$ on $X$ , with $X$ being a n x p input matrix, the OLS $\beta$ is $(X^TX)^{-1}X^TY$ , and the Ridge regression beta is $(X^TX+\lambda I)^{-1}X^TY$ . Also, the singular value decomposition of X is $UDV^T$ where $U$ and $V$ are orthogonal matrices and $D$ being a diagonal matrix. In equation 3.47 of Elements of Statistical Learning the author states $$
\begin{aligned}
X\beta^{ridge} &= X(X^TX+\lambda I)^{-1}X^Ty\\
&= UD(D^2+\lambda I)^{-1}DU^Ty
\end{aligned}
$$ which seems to suggest that $X^TX = D^2$ . But to arrive at that we first have $$
\begin{aligned}
X^TX &=VDU^TUDV^T\\
&= VD^2V^T
\end{aligned}
$$ Now, I know $VV^T = I$ by property of orthogonal/orthonormal matrix. But there's a $D^2$ between them and I know matrix multiplications are not commutative. So how do we get to $VD^2V^T = D^2$ ?","['matrices', 'least-squares', 'linear-algebra', 'svd']"
4400496,"Is the image of a closed subset in a Banach space under a lineal and continuous projection always closed? If the answer is no, is there any example?","I was wondering if the following conjecture is true:
let $X$ be a Banach space, let $P: X\to X$ be a continuous linear projection, and let $C$ be a closed subspace (unrelated to $P$ ) of $X$ .
Then the image $P[C]$ is closed.
By saying that $C$ is not related to $P$ , I mean that we do not suppose that $C$ is contained in the image of $P$ , that is, we do not know any relationship between $C$ and $P$ .
I have thought that the continuity of $P$ implies the continuity of $I-P$ . But it turns out that $Ker (I-P)$ doesn't depend on $C$ . So I don't know how to proceed. Thanks for your help friends.",['functional-analysis']
4400499,Find $r$ when $r>15$,"A light-bulb flickers after every so often in minutes, and the time taken between each flicker is recorded. In total 5 observations are counted, and the time between each has been recorded as the following: $$x_1=2.5, x_2=5.4, x_3 = 6.4, x_4 = 2.1$$ However, the 5th observation is only recorded when the time between flickers is greater than 15 minutes, so we have $x_5 > 15$ . Calculate the sample mean. Here's what I have tried: $$\frac{1}{n}\sum_{i=1}^5x_i = \frac{2.5+5.3+6.4+2.1+r}{5}=3.26+\frac{r}{5}$$ Where $r$ represents $x_5 > 15$ . However, how do I find a value for $r$ if possible? The original question asks for this distribution to find the MLE of $X \sim \exp(\lambda;x)$ . Give the MLE of $\exp(\lambda;x)$ is $\bar{x}$ , I thought the interpretation I gave would be the answer. Please let me know if an alternative approach was required!","['statistics', 'maximum-likelihood']"
4400570,"If $\lambda(\mathbf{A}+\mathbf{B})=\lambda(\mathbf{A})+\lambda(\mathbf{B})$, are HPD matrices $\mathbf{A}, \mathbf{B}$ simultaneously diagonalizable?","$\mathbf{A}, \mathbf{B} \in \mathbb{C}^{n \times n}$ are Hermitian and positive-definite (HPD) matrices. The following conditions are equivalent: $\mathbf{A}$ and $\mathbf{B}$ commute . $\mathbf{A}$ and $\mathbf{B}$ are simultaneously unitary diagonalizable . $\mathbf{A}$ and $\mathbf{B}$ have the same eigenspace . If either of the above equivalent conditions holds, then $$\lambda(\mathbf{A}+\mathbf{B})=\lambda(\mathbf{A})+\lambda(\mathbf{B}), \tag{1} $$ where $\lambda(\cdot) = (\lambda_1(\cdot), \dots, \lambda_n(\cdot))$ is the $n$ -tuple of the eigenvalues of the corresponding matrix in decreasing order. My question is the opposite of the above statement: If (1) holds, can we say $\mathbf{A}$ and $\mathbf{B}$ satisfy the above conditions, namely, they are simultaneously unitary diagonalizable? If this is true, I guess a place to start to prove might be Theorem 7.6.4 of [1] (see also A property of positive definite matrices ), but I am not sure how to proceed. [1] Horn, R. A., Johnson, C. R. (1990). Matrix Analysis .","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'diagonalization', 'positive-definite']"
4400597,About quotient singularities on a complex projective surface,"Let $S$ be a normal (complex) projective surface with quotient singularities (locally analytically isomorphic to $\Bbb C^2/G$ where $G\subset \text{GL}(2,\Bbb C)$ is a finite group whose action is
fixed point free outside the origin). Let $f:S'\to S$ be the minimal resolution. I am looking for a proof or reference of the following facts: Quotient singularities are log-terminal singularities, so we can write $K_{S'}=f^*K_S -\sum D_p$ , where $D_p=\sum (a_jE_j)$ is an effective $\Bbb Q$ -divisor supported on $f^{-1}(p)=\cup E_j$ and $0\leq a_j <1$ . Quotient singularities are rational, so $p_g(S')=q(S')=0$ . Actually I am reading this paper: https://arxiv.org/pdf/0801.3021.pdf . Fact 1 is given in the last paragraph of p.6, and it is written that it is well-known. Fact 2 is given in the proof of Corollary 3.4, p.8. Is there a reference to find proofs of these? Thanks in advance.","['complex-geometry', 'singularity', 'reference-request', 'divisors-algebraic-geometry', 'algebraic-geometry']"
4400614,Fourier transform of uniform distribution on irreducible representation,"I'm reading through Diaconis' notes Group Representations in Probability and Statistics , and working on the following exercise, where $\hat{P}$ denotes the Fourier transform of $P$ : Recall that the uniform distribution is defined by $U(s) = \frac{1}{|G|}$ where $|G|$ is the order of a group $G$ . Then at the trivial representation $\hat{U}(\rho) = 1$ and at any other non-trivial irreducible representation $\hat{U}(\rho) = 0$ . I think I have proof by contradiction, but I'm a little unsure about it and would love some feedback. Also, if there is a direct proof I would love to hear about it. Proof: First, let $\rho$ be the trivial representation. Then $$\hat{U}(\rho) = \sum_s U(s) \rho(s) = \sum_s \frac{1}{|G|} = 1.$$ Next, let $\rho$ be a nontrivial irreducible representation carried by $V$ .
First, note that $\operatorname{im}\hat{U}(\rho)$ is an invariant subspace of $V$ : $$\rho(t)\hat{U}(\rho)(v) = \rho(t)\left( \frac{1}{|G|} \sum_s \rho(s) \right) v = \left( \frac{1}{|G|} \sum_s \rho(t) \rho(s) \right) v = \hat{U}(\rho)(v)$$ for all $v \in V$ .
Thus, $\operatorname{im}\hat{U}(\rho) = 0$ or $V$ . Suppose the latter for the sake of contradiction.
Then $\hat{U}(\rho)$ is an isomorphism. Let $v \in V$ and $s \in G$ such that $\rho(s)v \neq v$ . Such a pair of $v, s$ exists because otherwise, the span of $v$ would be an invariant subspace corresponding to the trivial representation. Then $$ \hat{U}(\rho)(\rho(s) v) = \hat{U}(\rho) v$$ by translation, contradicting the injectivity of $\hat{U}(\rho)$ . Thus, $\operatorname{im}\hat{U}(\rho) = 0$ , so $\hat{U}(\rho) = 0$ . $\square$ Thanks!","['solution-verification', 'representation-theory', 'fourier-transform', 'probability']"
4400620,The expected squared increment of a continuous local martingale,"Suppose $M=\{ M_t\}_{t\geq 0}$ is a continuous local martingale, and $M_0=0$ . Then I often see the following equation $$\mathbb{E}M_t^2=\mathbb{E}\sum_i(M_{t_{i+1}}^2-M_{t_i}^2)=\mathbb{E}\sum_i(M_{t_{i+1}}-M_{t_i})^2.$$ I am new to stochastic analysis, so the second equation really puzzles me. If we expend the last expression in traditional way, certainly $M_{t_{i+1}}^2-M_{t_i}^2\neq(M_{t_{i+1}}-M_{t_i})^2$ . So how does it work?","['stochastic-analysis', 'stochastic-processes', 'local-martingales', 'martingales', 'probability-theory']"
4400624,What is a top form?,"I'm sorry for such a basic question, but I can't seem to find this term defined anywhere. I'm trying to learn analysis on manifolds, and google has not been any help in figuring out what this thing is. I know what a differential form is, what a volume form is, but I can't figure this one out for some reason.","['differential-geometry', 'multilinear-algebra', 'manifolds', 'differential-forms', 'terminology']"
4400697,How do you find the center of a cake with just a knife?,"Consider an undecorated cylindrical cake and a perfect knife.  We want to find the center of the cross-sectional circle. If we can only score the surface of the cake, this reduces to finding the center of a circle with just a straightedge, which is impossible . But using a knife allows additional constructions.  For example, Sarvesh Iyer mentions: The big difference between straight lines on a circle and knives on a cake is that you can shift any cake pieces you cut, around the shape and match them up with other pieces. The piece that's in a darker shade of orange can be removed and used to replicate that particular angle around the center subtended by it. I don't think one can create duplicate angles with just a straightedge, hence the difference. Similarly, YNK claims that determines the center in seven cuts, although he has not explained the construction procedure. Here are some rules modeling our use of the knife. You can't guarantee any nice properties of the lines like being perpendicular or parallel to another, just chords. The ""canvas"" for connections is only the circle (you can't be cutting the table, just the cake). What is the minimal number of cuts necessary to find the circle?  Is it YNK's 7?","['geometry', 'geometric-construction']"
4400704,Proof by induction that $(b^n-1)\cdots (b^n-b^{n-2})\ge b^{n(n-1)}-b^{n(n-1)-1}$ [duplicate],"This question already has an answer here : An apparently harmless exercise concerning induction (1 answer) Closed 2 years ago . Let $b\ge 2$ and $n$ a natural number at least 1.  Prove that $$(b^n-1)\cdots (b^n-b^{n-2})\ge b^{n(n-1)}-b^{n(n-1)-1}$$ The base-case is obvious. For the inductive case, assume this holds for $n$ and we show it for $n+1$ .  I started by writing $(b^{n+1}-1)\cdots (b^{n+1}-b^{n-1})$ and tried to write a chain of inequalities arriving at $b^{(n+1)n}-b^{(n+1)n-1}$ .  This goal is the same as $b^{n(n+1)-1}(b-1)$ . We can factor a $b$ out of every factor other than the first: $$(b^{n+1}-1)\cdots (b^{n+1}-b^{n-1}) = b^{n-1}(b^{n+1}-1)(b^n-1)(b^n-b)\cdots (b^n-b^{n-2})$$ and apply the inductive hypothesis, so that the above is $$\ge b^{n-1}(b^{n+1}-1)(b^{n(n-1)}-b^{n(n-1)-1}) $$ $$ = b^{n-1+n(n-1)-1}(b^{n+1}-1)(b-1) $$ $$= b^{n^2-2}(b^{n+1}-1)(b-1) $$ Therefore it suffices to show that $$ b^{n^2-2} (b^{n+1}-1) \ge b^{n^2+n-1}$$ or $$b^{n+1}-1\ge b^{n+1}$$ ... which ... eh. Since the only rounding I did was using the inductive hypothesis, it must be that I'm getting a bad result because I'm using the inductive hypothesis in a bad way--like because I'm using it in a product with a large number.  But I don't see an alternate approach.","['induction', 'discrete-mathematics']"
4400826,How can we show this relationship between the sum of divisors function and the sum $p^{m}+2p^{m-1}+3p^{m-2}+\ldots+(m+1)$?,"The sum of divisors function is commonly denoted by $\sigma(n)$ . Now let us introduce a recursive definition of divisor functions: $r_{n,1}=\sigma(n)$ $r_{n,2}=\sum_{d|n}r_{d,1}$ $r_{n,3}=\sum_{d|n}r_{d,2}$ $\ldots$ Note that these are simplified Dirichlet products so that the functions are multiplicative. My question: How can we show that if $p^{m}$ divides $n$ and $p^{m+1}$ does not divide $n$ , then $p^{m}+2p^{m-1}+3p^{m-2}+\ldots+(m+1)$ divides $r_{n,2}$ ? A first idea: Formulated differently, $\frac{\frac{p^{m+2}-1}{p-1}-(m+2)}{p-1}$ divides $r_{n,2}$ and if $n$ is squarefree, we have $\prod_{p|n}(p+2)=r_{n,2}$ .","['number-theory', 'divisor-counting-function', 'divisor-sum', 'elementary-number-theory']"
4400835,Probability of failure of first unit provided that at least one of the two units has failed,"The following question was asked in JEE Main 2021: An electric instrument consists of two units. Each unit must function independently for the instrument to operate. The probability that the first unit functions is $0.9$ and that of the second unit is $0.8$ . The instrument is switched on and it fails to operate. If the probability that only the first unit failed and second unit is functioning is $p$ , then $98p$ is equal to: My attempt: Let us call the event of the first unit not functioning $A$ and the second unit not functioning $B$ . Let the event of failure of the instrument be $F$ . Then $P(F) = 1 - 0.8 \times 0.9 = 0.28$ . From Bayes' Theorem: $$P(A|F) = \frac{P(F|A) P(A)}{P(F)} = \frac{1 \times 0.1}{0.28}$$ $$P(B|F) = \frac{P(F|B) P(B)}{P(F)} = \frac{1 \times 0.2}{0.28}$$ Hence, $$p = P(A|F) \times (1 - P(B|F)) = \frac{10}{98}$$ And the answer should be $98p = 10$ . However, the given answer is: $28$ Where did I go wrong? The solution given by many websites online is: $$p = \frac{P(A) \times (1 - P(B))}{P(F)} = \frac{28}{98}$$ Although this arrives at the correct answer, this method is not really convincing, and my method seems to be the right method to me. However, I can't arrive at what exactly is wrong in this solution (or mine).","['conditional-probability', 'bayes-theorem', 'probability']"
4400866,Proving that : $(\forall n: \sum_{i=1}^{m} a_i x_i^{n} =\sum_{i=1}^{m} a_i y_i^{n} )\rightarrow (\sum_{i=1}^{m}x_i =\sum_{i=1}^{m}y_i )$,"Let $(x_i)^{m}_{i=1}$ , $(y_i)^{m}_{i=1}$ and $(a_i)^{m}_{i=1}$ be  tuples of positive non-zero reals. If for all positive integers $n$ : \begin{equation}
   \sum_{i=1}^{m}a_ix_i^{n} = \sum_{i=1}^{m}a_iy_i^{n} \tag{1}
  \end{equation} Then \begin{equation*}
  \sum_{i=1}^{m}x_i = \sum_{i=1}^{m}y_i
\end{equation*} NOTE: This is not a textbook problem statement. It could potentially be false, so I would accept a counter example as an answer too. MY WORK I have been trying to prove this statement. Through help of MSE, I have been able to prove that if $\{u_{i}\}^{m'}_{i=1}$ and $\{v_{i}\}^{m''}_{i=1}$ are the set of unique entries in $(x_i)^{m}_{i=1}$ and $(y_i)^{m}_{i=1}$ respectively. Then $\{u_{i}\}^{m'}_{i=1}$ and $\{v_{i}\}^{m''}_{i=1}$ are the same set, the proof goes as follows: Let $\{u_{i}\}^{m'}_{i=1}$ and $\{v_{i}\}^{m''}_{i=1}$ be the set of unique entries in $(x_i)^{m}_{i=1}$ and $(y_i)^{m}_{i=1}$ respectively. Also, without loss of generality, we may assume an ordering such that $u_1 > u_2> ... >u_{m'} $ and $v_1 > v_2> ...>v_{m''} $ and also that $m''\geq m'$ . We can rewrite $(1)$ as: \begin{equation}
  \label{eq: lemma_1_equivalence}
 \forall n \in \mathbb{Z^{+}}:  \sum_{i=1}^{m'}c_iu_i^{n} = \sum_{i=1}^{m''}d_iv_i^{n} \tag{2} 
\end{equation} As $n$ grows the leading term on LHS is $c_1u_{1}^{n}$ and on the RHS is $d_1v_{1}^{n}$ . Hence, it must be : \begin{equation*}
  \forall n \in \mathbb{Z^{+}}: c_1 u_{1}^{n}  = d_1 v_{1}^{n} 
\end{equation*} Since, $u_1,v_1,c_1$ and $d_1$ are non-zero positive reals, we can conclude that $u_1=v_1$ and $c_1 = d_1$ . Hence, we may subtract $c_1 u_{1}^{n}$ from both sides in $(2)$ to get : \begin{equation}
  \label{eq: lemma_1_equivalence_1}
 \forall n \in \mathbb{Z^{+}}:  \sum_{i=2}^{m'}c_iu_i^{n} = \sum_{i=2}^{m''}d_iv_i^{n} 
\end{equation} We may now repeat the aforementioned argument and infer that $u_2=v_2$ and $c_2 = d_2$ . Furthermore, repeating this argument $m'$ times, we can infer that $\{u_i\}^{m'}_{i=1} = \{v_i\}^{m'}_{i=1}$ , leaving us with $0 =\sum_{i=m''-m'+1}^{m''}d_iv_i^{n}$ , which is a contradiction, hence, $m'=m''$ . Hence, we have that $\{u_{i}\}^{m'}_{i=1}$ = $\{v_{i}\}^{m''}_{i=1}$ .","['algebra-precalculus', 'polynomials']"
4400873,Manipulation of equation from ratio of two variables to difference between them,"I have an equation of the form $x_1/x_2=\exp(a)$ . I'd like to manipulate the equation so that the difference between $x_1$ and $x_2$ is a function of their average, i.e. $x_1 - x_2 = f(\bar{x})$ , where $\bar{x}=(x_1 + x_2)/2$ . I saw the solution in a paper (provided without steps) to be $2\bar{x}(\exp{(a)} - 1)/(\exp{(a)} + 1)$ but I'm unable to do it myself. I'd be grateful if somebody could show the steps to get to the solution (this is not homework, I'd just like to understand the steps). Here are the steps I tried: $$
x_1-x_2 = \exp{(a)}x_2-x_2
$$ Then I tried to use the definition of the average ( $x_2 = 2\bar{x}-x_1$ ): $$
x_1 - x_2 = \exp{(a)}(2\bar{x}-x_1) - (2\bar{x}-x_1) = x_1(1 - \exp{(a)}) + 2\bar{x}(\exp{(a)} - 1)
$$ At this point, I was stuck because of the $x_1$ on the right hand side.",['algebra-precalculus']
4400879,Vector space translation continuous implies addition continuous?,"In here , it is proved if a topology on a vector space makes the addition function continuous, then the translation is also continuous everywhere. My question is whether the inverse is still true: Let $V$ be a vector space with a topology $\tau$ . If, for every $x\in V$ , the translation $T_x: V\to V$ , $T_x(y):=x+y$ , is continuous, is it guaranteed the addition $+: V\times V \to V$ is also continuous? I know if the translations are continuous everywhere, then they are homeomorphisms, regardless of whether the addition is also continuous. At this point, I'm not assuming anything regarding the scalar multiplications, as in, it's not known whether it is continuous.","['product-space', 'topological-vector-spaces', 'functional-analysis']"
4400914,Primality testing with binomial coefficients,Here is what I observed : Let $n$ be a natural number greater than 2. Let $A = 2\cdot\binom{3n+1}{n}-\binom{3n}{n-1}+\binom{3n-1}{n-2}$ Let $p=2n+1$ $p$ is prime iff $A \equiv 2 \pmod{p}$ You can run this test here . I tried with some prime and composite numbers below 350000 and I didn't find any counterexample. Is there a way to prove it ? I think this is related to Wilson's theorem but I'm not sure.,"['number-theory', 'primality-test', 'binomial-coefficients', 'prime-numbers']"
4400936,Finding max{f(x)} without derivative,Consider the function $f(x)=\frac{\sqrt{x^3+x}}{x^2+x+1}$ the question is about: to find max{f} without using derivative. I can find max with derivation and it is not hard to find. it is $f'(x)=0 \to x=1 $ so $max\{f\}=\frac{\sqrt 2}{3}$ but I am looking for an idea to do as the question said. I am thankful for any help.(because I got stuck on this problem),"['calculus', 'functions', 'algebra-precalculus']"
4400939,Bound the norm for Hessian by Laplacian Schauder estimate,"I was trying to prove the following result which bound the Hessian by Laplacian for smooth function with compact supported. For $u\in C^\infty_c(\Bbb{R}^n)$ , we have $$\|\partial_i\partial_j u\|_p \le C\|\Delta u\|_p$$ For $p>1$ . I can prove the case when $p = 2$ by directly integration by part.For the other case I guess we need to use the standard interpolation + duality arguement, but I don't know how.","['partial-differential-equations', 'functional-analysis', 'real-analysis']"
4400975,Interpolation of $L^p$ spaces (from book by Caffarelli-Cabre),"In the book ""Fully nonlinear elliptic equations"" by L. Caffarelli and X. Cabre, Theorem 4.8 (b) we want to prove the following maximum principle, for any $p>0$ , \begin{align*}
    \sup_{Q_{1/2}}\,u\leq C(p)\left(\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right),
\end{align*} where $C(p)$ is a constant depending only on $d$ , $\lambda$ , $\Lambda$ and $p$ . The proof consists of proving this estimate por a particular $p=\varepsilon$ , which I have no problem with. Then, the authors claim that the general case $p>0$ follows easily by interpolation, but I don't know how. So, for the purpose of this question, we can assume this inequality holds for a fixed $p=\varepsilon$ , and the question is how to generalize this for any $p>0$ . Note that $Q_l\subset \mathbb{R}^d$ is a cube of side length $l$ . This is my work so far: If $p>\varepsilon$ we can apply  Holder inequality and get \begin{align*}
\|u^+\|_{L^\varepsilon(Q_{3/4})}\leq |Q_{3/4}|^\frac{1}{r}\|u^+\|_{L^p(Q_{3/4})}, \quad \frac{1}{r}+\frac{1}{p}=\frac{1}{\varepsilon}. 
\end{align*} Hence \begin{align*}
\sup_{Q_{1/2}}u\leq& C\left(|Q_{3/4}|^\frac{1}{r}\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right)\\
\leq &C\left(\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right),
\end{align*} since $ |Q_{3/4}|^\frac{1}{r}<1$ . The proof for any $p<\varepsilon$ follows by interpolation (I don't know how). Interpolation of $L^p$ spaces: I only know the generalized Holder inequality: If $0<p_0<p_1\leq \infty$ and $g\in L^{p_0}(X)\cap L^{p_1}(X)$ , then for every $\theta\in(0,1)$ and \begin{align*}
\frac{1}{p_\theta}=\frac{\theta}{p_0}+\frac{1-\theta}{p_1}, 
\end{align*} it holds \begin{align*}
\|g\|_{L^{p_\theta}(X)}\leq \|g\|_{L^{p_0}(X)}^{\theta}\|g\|_{L^{p_1}(X)}^{1-\theta}.
\end{align*} If we choose $p_\theta=\varepsilon$ and $p_1=+\infty$ then \begin{align*}
p=\theta \varepsilon\in (0,\varepsilon)
\end{align*} and \begin{align*}
\|u\|_{L^{\varepsilon}(Q_{3/4})}\leq \|u\|_{L^{p}(Q_{3/4})}^{\theta}\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}.
\end{align*} By Young's inequality, we have for every $\delta>0$ \begin{align*}
\|u\|_{L^{p}(Q_{3/4})}^{\theta}\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\leq &\frac{\left(\delta^{-1} \|u\|_{L^{p}(Q_{3/4})}^{\theta}\right)^r   }{r}+\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\\
=&\frac{\delta^{-r} \|u\|_{L^{p}(Q_{3/4})} }{r}+\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s},
\end{align*} where \begin{align*}
\frac{1}{r}+\frac{1}{s}=1 \quad \mbox{ and } \quad r=\frac{1}{\theta}.
\end{align*} Combining with our estimate for $p=\varepsilon$ we get \begin{align*}
\sup_{Q_{1/2}}u-C\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\leq C\left(\frac{\delta^{-r} \|u\|_{L^{p}(Q_{3/4})} }{r}+\|f\|_{L^d(Q_1)}\right).
\end{align*} Problem: I cant take $\delta$ universally small such that \begin{align*}
C\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\leq \frac{1}{2}\sup_{Q_{1/2}}u
\end{align*} because the domains are different.","['lp-spaces', 'maximum-principle', 'analysis']"
4400982,Differentiating $y = x - \frac2x + \frac3{x^2}$,"Another easy question for you guys. I'm differentiating the below to find the equation of the tangent at $(-3,-2)$ $$y = x - \dfrac{2}{x} + \dfrac{3}{x^2}$$ I simplified to: $$ y = x - 2x^{-1} + 3x^{-2}$$ Then differentiated to get: $$ \frac{dy}{dx} = 1 + 2x^{-2} - 6x^{-3}$$ or $$\frac{dy}{dx} = 1 + \frac{2}{x^2} - \frac{6}{x^3}$$ Placing $x = -3$ into this gives me $1$ , and placing $m=1$ into $y=mx+c$ gives me $c = 1$ . Making the simple equation: $0 = x - y + 1$ However, I'm given the answer as: $0 = 13x - 9y + 21$ Where did I go wrong, I've studied it for longer than I'm willing to admit, have I made a stupid mistake somewhere? Thanks,",['derivatives']
4400984,Infinite summation : $\displaystyle S= \sum_{r=0}^{\infty} \frac{\cos (2xr)}{4r^2 - 1}$,"So I ran into this problem on Quora: $$\displaystyle S = \sum_{r=0}^{\infty} \frac{\cos (2xr)}{4r^2 - 1}$$ Initially I thought it cannot be resolved as telescoping method failed. But I tried something else. MY ATTEMPT: Let $\displaystyle S = \mathbb{R} \left [ \sum_{r=0}^{\infty} \frac {e^{-\iota 2rx}}{4r^2 -1} \right ] \equiv \mathbb{R} [J], \ $ where $ \ \mathbb{R} \ $ shows real part.
So let us find $J.$ Now, $\displaystyle J(x) = \sum_{r=0}^{\infty} \frac {e^{-\iota 2rx}}{4r^2 -1} \implies J'(x) = \sum_{r=0}^{\infty} \frac {-\iota  (2r)e^{-\iota 2rx}}{4r^2 -1} \ $ and hence $\displaystyle J''(x) = -\sum_{r=0}^{\infty} \frac {(4r^2)e^{-\iota 2rx}}{4r^2 -1}.$ Therefore some maipulations and we get: $$\displaystyle J'' + J =  - \sum_{r=0}^{\infty} e^{-\iota 2rx} \implies J'' + J =  \iota \sin(x) e^{-\iota x}.$$ We can solve this by variation of parameters: Initial soltuions of the equation $\ \displaystyle J'' + J = 0 \ $ are $\displaystyle e^{\iota x} , e^{-\iota x} $ . And let $\ \displaystyle J= C_0 e^{\iota x} \phi_1 + C_1 e^{-\iota x} \phi_2 \ $ satisfy $ \ \displaystyle J'' + J =  \iota \sin(x) e^{-\iota x}. \ $ Therefrore, $$\displaystyle 
e^{\iota x} \phi_1' +  e^{-\iota x} \phi_2' = 0 \\
e^{\iota x} \phi_1' -  e^{-\iota x} \phi_2' = \sin(x) e^{-\iota x} 
$$ Solving for $ \ \displaystyle \phi_1 , \phi_2\ $ we get: $$\displaystyle J = C_0 e^{\iota x}\left ( \frac{1}{6} \cos (x) + \frac{1}{3} \sin ^2(x)  e^{\iota x} \right ) + \frac{C_1}{2} e^{-\iota x} \cos (x)$$ But the issue is $ \ \displaystyle  C_0 , C_1 .\ $ I was only able to find that $ \ \displaystyle  C_0 + 3C_1 = 3 ,\ $ using the condition : $\displaystyle J(x=0) = J(x=\pi) = J(x=-\pi) = \frac{1}{2} = \frac{C_0}{6} + \frac{C_1}{2}$ I was only able to conclude: $$\displaystyle  S = \mathbb{R} \left[ C_0 e^{\iota x}\left ( \frac{1}{6} \cos (x) + \frac{1}{3} \sin ^2(x)  e^{\iota x} \right ) + \frac{C_1}{2} e^{-\iota x} \cos (x) \right] \\ \text{where} \ C_0 + 3C_1 = 3 $$ I want anyone guide me further. Or suggest some another method to solve this.","['summation', 'complex-numbers', 'ordinary-differential-equations']"
4400986,How do the binomial distribution works?,"I have the following question, we have just been looking at random variables and their distributions. So I know that a random variable (RV) is a function $X:\Omega\rightarrow M$ where $M$ is an arbitrary quantity. Now if we define a measure of probability $P$ on $\Omega$ then we can use RV to project this measure onto the set $M$ and get the image measure $P_X(A)=P(X^{-1}(A)) $ where $A$ is a subset of $\Omega$ , then $P_X$ is called the distribution of our RV.
Is this correct up to here? Now we have looked at the binomial distribution, for example. There we take $p\in [0,1]$ and $X:\Omega \rightarrow \{0,...,n\}$ then we said that $P(X=k)= \binom{n}{k}p^k (1-p)^{n-k}$ . But now it says in a lecture that the binomial distribution itself is $$\sum _{k=0}^n \binom{n}{k}p^k (1-p)^{n-k} \delta_k$$ Somehow I'm a bit confused now, firstly I don't see why we defined it differently and secondly I don't see why we have to take such a sum here. Could someone explain this to me? I would be very grateful.","['binomial-distribution', 'probability-theory', 'probability']"
4401081,"When we say that number 5 can be counted as a polynomial, do we make some additional assumptions about values of $0^0$ or x?","https://www.mathsisfun.com/algebra/polynomials.html At the link above it's said that $5$ is a polynomial. Basically because we can imagine it to be $5x^0$ . But it seems for me, that in order for this to be true, we need to believe/assume one of two things: Either $0^0 = 1$ or $x$ can't be equal to $0$ . Thus, if I have $2x^3 + 5$ , then to count it as a polynomial (if $5$ isn't a polynomial, then neither is $2x^3 + 5$ . By same reason $2x^3  + \#$ isn't a polynomial. In order to get a polynomial via addition, both parts must be polynomials) I must either define $0^0$ as $1$ or forbid $x$ from being equal $0$ . Am I correct?","['algebra-precalculus', 'polynomials']"
4401082,"A cube is dropped on the floor, and the triangular hole created has sides of lengths $68,$ $75,$ and $77.$ Find the depth of the hole.","I dropped a heavy cube the other day, creating a dent in my floor. The triangular hole in the surface of the floor has sides of lengths $68,$ $75,$ and $77.$ Find the depth of the hole. I drew a diagram, but I'm really not sure what to do next. I also tried taking some cross sections of the diagram, but that also doesn't do much. I'm really not sure how to continue from here. :( Thanks in advance!!!","['geometry', '3d']"
4401213,A high school problem on derivatives.,"I came across this problem in an old high school textbook of 1978: Suppose that $f, g$ are polynomials with domain and image all of $\mathbb{R}$ . Prove that if \begin{equation}
    \begin{cases} 
                  f(x)\neq g(x) \\
                  f''(x)\neq g''(x) 
    \end{cases} \forall \hspace{.1cm} x\in \mathbb{R}
\end{equation} there is exactly one solution of $f'(x)=g'(x)$ Clarification: By $f'$ and $f''$ , the first and second derivatives are inferred. I'm not sure what I must do, but I have come up with  a few vague ideas: The polynomials can only be of odd order $\geq 3$ and they must have the same coefficient in their largest power. (Otherwise, a new polynomial of odd order would arise, which would oppose the assumption that $f(x)\neq g(x)$ ) I have started by assuming that there is no $x_0$ such that $f'(x_0)=g'(x_0),$ which in turn implies that the function $h(x)=f(x)-g(x)$ is strictly monotone. Can I consider cases? Would that be helpful? Also because I need $f''(x)\neq g''(x)$ , the function $q(x)=f'(x)-g'(x)$ is also strictly monotone. There are also other ideas that are just floating around, but I just can't poke the problem well enough. Do you have any ideas?","['calculus', 'derivatives']"
4401217,Finding number of edges in giant component of a Uniform Random Graph,"I had previously asked for help in clarification of use of Chebyschevs inequality in relation to a proof of the number of edges in the unique giant component $C_0$ in an uniform random graph. Thanks to @misha-lavrov for his insightful answer here: Incorrect proof: Number of edges in the unique giant component in an uniform random graph I have another problem in relation to the proving the following theorem: Let $c>1$ . If $n$ is the number of vertices and $m = cn/2$ is the number edges of a uniform random graph $G_{n,m}$ , then with high probability (w.h.p.) $G_{n,m}$ consists of a unique giant
component $C_0$ with asymptotically $(1-x/c)n$ vertices and $(1-(x/c)^2)m$ edges. Here $0<x<1$ is the unique solution to $x\exp(-x)=c\exp(-c)$ . This is from Frieze's and Karonski's book (page 34-38 here https://www.math.cmu.edu/~af1p/BOOK.pdf ). I have managed to understand and fill out the details for existence and uniqueness of the the giant component $C_0$ as well as the number of vertices. The only part that is left is computing the number of edges. The idea in Frieze's and Karonski's proof is first showing that $(1-(x/c)^2)m$ is asymptotically the expected number of edges in $C_0$ and then showing that the number of edges $X$ concentrates in the following sense: For every $\epsilon>0$ $$
P((1-\epsilon)(1-(x/c)^2)m \leq X \leq (1+\epsilon)(1-(x/c)^2)m) = P((1-\epsilon)E[X]\leq X \leq (1+\epsilon)E[X]) \rightarrow 1
$$ as $n \to \infty$ . This is equivalent to showing $$
P(\vert X - E[X] \vert > \epsilon E[X]) \rightarrow 0
$$ and this we can use Chebyschevs inequality on so we need to compute the variance of $X$ . As $$
X=\sum_{j=1}^m 1_{\{e_j \in C_0\}},
$$ then \begin{align*}
E[X^2] &= \sum_{j=1}^m P(e_j \in C_0) + \sum_{j \neq i} P(e_j \in C_0, e_i \in C_0) \\
&= m (1-(x/c)^2)+\sum_{j \neq i} P(e_j \in C_0, e_i \in C_0).
\end{align*} Frieze and Karonski now claim that $$
P(e_j \in C_0, e_i \in C_0)=(1+o(1))P(e_i \in C_0)P(e_j \in C_0) 
$$ where $o(1)$ denotes a sequence converging to 0. This is the part I need help with! (Note if would be fine to show $\leq$ instead of $=$ ). My first instinct was that writing $P(e_j \in C_0, e_i \in C_0)=P(e_j \in C_0 \mid e_i \in C_0)P(e_i \in C_0)$ brings us ""halfway there"" but I think it hard to work with this conditional probability so maybe this not a viable approach. If one follows the proof in Frieze's and Karonski's book, the idea is to introduce a subgraph $G_2$ which is exactly the original graph but without the two edges $e_i,e_j$ . Then we can consider the unique giant component here $C_2 \subseteq C_0$ . My attempt at the calculation is then the following (here $e_i \in C_0$ means $e_i$ is an edge of the the component while $e_j \cap C_0 \neq \emptyset$ means that they share at least one vertex): \begin{align*}
\ & P(e_i \in C_0, e_j \in C_0) \\[0.6em]
 &= P(\{e_j \in C_0, e_i \in C_0\} \cap (\{e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset\} \cup \{e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset\}^c ) ) \\[0.6em]
& = P(e_j \in C_0, e_i \in C_0,e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset) \\[0.6em]
& \quad+P(\{e_j \in C_0, e_i \in C_0\}\cap\{e_i \cap C_2 \neq \emptyset\} \cup \{ e_j \cap C_2 \neq \emptyset\}) \\[0.6em]
&\leq P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset) \\[0.6em]
&\quad+ P(e_j \in C_0, e_i \in C_0,e_i \cap C_2 =\emptyset) + P(e_j \in C_0, e_i \in C_0,e_j \cap C_2 = \emptyset) \\[0.6em]
&\leq P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset) \\[0.6em]
&\quad+ P(e_i \cap (C_0\setminus C_2) \neq \emptyset) + P(e_j \cap (C_0\setminus C_2) \neq \emptyset) \\[0.6em]
&\leq P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset) + 2 \underbrace{\frac{Klog(n)\choose 2}{n \choose 2}}_{\to  0 \text{ as } n \to \infty} \\[0.6em]
& \approx P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset)\\[0.6em]
& = P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset, e_j \cap e_i \neq \emptyset)+P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset, e_j \cap e_i = \emptyset)\\[0.6em]
& = P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset, e_j \cap e_i \neq \emptyset) \\[0.6em]
& \quad +P(e_i \cap C_2 \neq \emptyset, e_j \cap e_i = \emptyset \mid e_j \cap C_2 \neq \emptyset)P(e_j \cap C_2 \neq \emptyset)\\[0.6em]
\end{align*} In the fifth step we used that earlier in the proof it was shown that all other small componenets are of order at most $K log(n)$ where $K>0$ is some constant. But now I cannot get any further. Can anyone help me finish the computation? Update: I have made some progress (?). I have shown that $$
P(e_i \cap C_2 \neq \emptyset , e_j \cap C_2 \neq \emptyset, e_j \cap e_i \neq \emptyset) \leq P(e_j \cap e_i \neq \emptyset) = \frac{2(n-2)}{{n \choose 2} - 1} \to 0
$$ as $n \to \infty$ . Furthermore from earlier in the proof it was shown that $P(e_j \in C_0)=P(e_j \cap C_1 = \emptyset) \approx (x/c)^2$ (where $C_1$ denotes the giant without just one of the edges and $x$ and $c$ are constants). By the same reasoning (although I am not completely sure..?) as there one would get that $P(e_j \cap C_2 = \emptyset) \approx (x/c)^2$ . Hence we have $$
P(e_i \in C_0, e_j \in C_0) = (o(1) + P(e_i \cap C_2 \neq \emptyset, e_j \cap e_i = \emptyset \mid e_j \cap C_2 \neq \emptyset) )(1-(\frac{x}{c})^2).
$$ Although this is not exactly what the book want, it would suffice to show that $$
P(e_i \cap C_2 \neq \emptyset, e_j \cap e_i = \emptyset \mid e_j \cap C_2 \neq \emptyset) \approx P(e_i \cap C_2 \neq \emptyset).
$$ It kinda makes sense that as we intersection with the vertices of $e_i$ and $e_j$ being disjoint, the conditioning doesn't really matter and asymptotically we have ""independence"". This is very hand-wavy however and I would like a better argument. Can someone help me finish the computation and/or improve it?","['random-graphs', 'combinatorics', 'probability-theory']"
4401240,Stokes' theorem for a manifold without boundary,"Stokes' theorem states that when $M$ is a compact oriented $m$ -manifold with boundary, and $\omega$ is a $(m-1)$ -form on $M$ , we have $$\int_{\partial M}\omega = \int_{M} d\omega.$$ This is confusing me for the following reason: It seems as if $\partial M$ were empty, i.e., if $M$ was a manifold without boundary, then we would have that the integral of $d \omega$ on the right is necessarily zero. But what if we took a manifold with boundary, and simply got rid of the boundary? This manifold would only differ from the original one on a set of measure zero, which should not affect the integration. This seems to imply that $\int_{M}d \omega$ is necessarily always zero, which of course can not be true. Where is the issue in my reasoning?","['manifolds-with-boundary', 'stokes-theorem', 'manifolds', 'differential-forms', 'differential-geometry']"
4401250,binomial identity: $\sum_{k=x+y}^{\infty}\binom{k-1}{y-1}\binom{k-y}{x}u^k = \binom{x+y-1}{y-1}\left(\frac{u}{1-u}\right)^{x+y}$?,"I met a problem which gave me the left part, and I can compute left part and get right part by Mathematica. However, I don't know how to prove: $$\sum_{k=x+y}^{\infty}\binom{k-1}{y-1}\binom{k-y}{x}u^k = \binom{x+y-1}{y-1}\left(\frac{u}{1-u}\right)^{x+y}$$ with $x, y \in \mathbb{Z}, x \ge 0, y \ge 1, 0 \le u < 1$ . My Questions: How to prove above binomial identity? Is there simple argument behind it? Since it's quite simple, maybe we can construct two equivalent counting processes.","['combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'probability']"
4401306,How does $f(A^c)$ compare to $f(A)^c$?,"Let $f: X \to Y$ be a map of sets and $A \subset X$ . I'm trying to compare $f(A^c)$ to $f(A)^c$ . From drawing a picture, I believe $f(A^c) \subset f(A)^c$ provided that $f$ is injective. Proof of conjecture 1. Suppose $y \in f(A^c)$ . Then there exists $x \in A^c$ such that $y = f(x)$ . As $f$ is injective, for any $x' \in X$ satisfying $f(x') = y = f(x)$ , we have $x = x'$ , so for every $a \in A$ , $f(a) \neq y$ , so $y \not \in f(A)$ , so $y \in f(A)^c$ . I think we have $f(A)^c \subset f(A^c)$ provided that $f$ is surjective. Proof of conjecture 2. Let $y \in f(A)^c$ . Then $y \not \in f(A)$ . But, as $f$ is surjective, there exists $x \in X$ such that $f(x) = y$ , so $x \not \in A$ , so $x \in A^c$ , so $y \in f(A^c)$ . I believe these statements can be strengthened to biconditionals, but I haven't been able to figure out how to prove the reverse implications. Do I exhibit functions where $f(A^c) \not \subset f(A)^c$ when $f$ is not injective and $f(A)^c \not \subset f(A^c)$ when $f$ is not surjective?","['elementary-set-theory', 'solution-verification']"
4401321,"Karatzas and Shreve Exercise 1.8: If paths of process $X$ are RCLL almost surely, show that $A$ can fail to be in $\mathscr{F}_{t_0}^X$","Karatzas+Shreve Textbook Exercise 1.8: Let $X$ be a [stochastic] process whose sample paths are RCLL almost surely, and let $A$ be the event that $X$ is continuous on $[0,t_0)$ . Show that $A$ can fail to be in $\mathscr{F}_{t_0}^X$ , but if $\{\mathscr{F}_t; t \ge 0\}$ is a filtration satisfying $\mathscr{F}_t^X \subset \mathscr{F}_t$ , $t \ge 0$ , and $\mathscr{F}_{t_0}$ contains all $P$ -null sets of $\mathscr{F}$ , then $A \in \mathscr{F}_{t_0}$ Quick background: RCLL means that the function is right continuous at every point and that a left-limit exists at every point. $\mathscr{F}_{t_0}^X$ is defined as the smallest $\sigma$ algebra such that all $X_s$ for $0 \le s \le t_0$ are measurable: \begin{align*}
  \mathscr{F}_{t_0}^X \triangleq \sigma (X_s; 0 \le s \le t_0) \\
\end{align*} The textbook offers a solution, I wanted some help in understanding the parts I've put in bold below: We first construct an example with $A \notin \mathscr{F}_{t_0}^X$ . The collection of sets of the form $\{(X_{t_1},X_{t_2},\dots)\in B\}$ , where $ B \in \mathscr{B}(\mathbb{R}^d) \otimes \mathscr{B}(\mathbb{R}^d) \otimes \cdots $ and $0\le t_1 < t_2 < \cdots \le t_0,$ forms a $\sigma$ -field and each such set is in $\mathscr{F}_{t_0}^X$ . Indeed, every set in $\mathscr{F}_{t_0}^X$ has such a representation. Choose $\Omega = [0,2), \mathscr{F}= \mathscr{B}([0,2))$ , and $P(F) = \operatorname{meas}(F \cap [0,1]);$ $F \in \mathscr{F}$ , where meas stands for ""Lebesgue measure."" For $\omega \in [0,1]$ , define $X_t(\omega)=0,$ $\forall t \ge 0;$ for $\omega \in (1,2),$ define $X_t(\omega)=0$ if $t \neq \omega$ , $X_\omega(\omega)=1$ . Choose $t_0=2.$ If $A \in \mathscr{F}_{t_0}^X$ , then for some $B \in \mathscr{B}(\mathbb{R}^d) \otimes \mathscr{B}(\mathbb{R}^d) \otimes \cdots $ and some sequence $t_k \in [0,2]$ , we have $A = \{(X_{t_1},X_{t_2},\dots)\in B\}$ . Choose $\bar{t} \in (1,2)$ , $\bar{t} \notin \{t_1, t_2, \dots\}.$ Since $\omega = \bar{t} $ is not in $A$ and $X_{t_k}(\bar{t})=0, k=1,2, \dots, $ we see that $(0,0,\dots) \notin B$ . But $X_{t_k}(\omega)=0,$ $k=1,2, \dots,$ for all $\omega \in [0,1]$ ; we conclude that $[0,1] \cap A = \phi$ , which contradicts the definition of $A$ and the construction of $X$ . Explain how $\omega = \bar{t} $ is not in $A$ . I see that $X_{\bar{t}}(\omega) = 1_{\bar{t} = \omega}$ . Since $\bar{t} \not\in \{t_1, t_2, \ldots\}$ , then $X_{\bar{t}}$ is not included in the expression $A = \{(X_{t_1},X_{t_2},\dots)\in B\}$ . And why is $(0,0,\dots) \notin B$ ?","['stochastic-processes', 'filtrations', 'probability-theory', 'stochastic-calculus']"
4401337,Making the Mandelbrot Fractal in Desmos Online Graphing Calculator,"I would like to make the following ""animation"" of the Mandelbrot Set using the Online Desmos Graphing Calculator app - as seen over here: https://www.youtube.com/watch?v=naqgsOOEHJs I couldn't clearly see the equations being used in this video, so I followed the instructions from link provided in the references ( https://www.youtube.com/watch?v=P23UI9cPCQk&t=0s ). I manually entered the 8 equations from the video into the Desmos Graphing Calculator: However, not only does the graph (set of equations) I have made not look like the first graph - when I zoom into the graph I made, I don't see any repeating fractal patterns. Can someone show me what I am doing wrong? What can I do to fix this, so that I also get a fractal pattern I can see through zooming? If possible, could someone try to make a Mandelbrot Set on Desmos that shows ""zoom fractals"" and post the link to the graph on Desmos? Thank you!","['graphing-functions', 'complex-analysis', 'functions', 'complex-numbers', 'fractals']"
4401341,Finding the center of the $1992^\text{nd}$ circle,"Consider the triangle whose vertices are $(0,0),(0,3),$ and $(4,0)$ . Let $\Gamma_{1}$ be the greatest circle that can be drawn inside the triangle. For $n>1$ , let $\Gamma_{n}$ be the greatest circle that can be drawn inside the triangle and does not overlap the $(n-1)$ greater circles, and is not in the region bounded by $\Gamma_{1}$ and the coordinate axes. Which of the following points is closest to the center of $\Gamma_{1992}?$ $\text{(A)}\space(0,1)\space \space \space \space \space\text{(B)}\space(0,3)\space \space \space \space \space\text{(C)}\space(1,0)\space \space \space \space \space\text{(D)}\space(\frac{8}{5},\frac{9}{5})\space \space \space \space \space\text{(E)}\space(4,0)$ This problem appeared in one of the math competitions in my city. The average time to solve this problem is $15$ minutes. I tried to find a pattern between $n$ and the given points. (of course $n \ne 1$ because it is equidistance from $\text{(A),(C)}$ , and $\text{(D)}$ ). For $n=2$ , the correct option is $\text{(E)}$ . For $n = 3$ , the correct option is $\text{(B)}$ . Then for $n \ge 4$ is is tedious! Any help would be appreciated. THANKS!","['contest-math', 'analytic-geometry', 'circles', 'geometry', 'triangles']"
4401347,How to prove product and quotient of smooth functions is smooth,"I'm trying to prove the following problem: Let $A\subset\mathbb{R}^n$ be open. If $f, g: A \rightarrow \mathbb{R}$ are smooth, show that $fg$ and $f/g$ is smooth. (For the quotient case, $g$ is nonzero on $A$ .) I'm not sure how to show the smoothness of $fg$ and $f/g$ . (In this context, ""smoothness"" refers to $C^\infty$ . That is, it means the function has continuous partial derivative of all orders.) For cases like $f+g$ , I used induction to prove that partial derivative of order $n\geq 1$ of $(f+g)$ is sum of partial derivative of order $n \geq 1$ of $f$ and $g$ respectively. But for case of product and quotient, I am not sure how to show they are smooth. I think induction is again a possible solution, but it is quite mind-boggling to write down formula for the partial derivative of some order $k$ for $fg$ and $f/g$ as product rule and quotient rule for derivative have a very complicated formula as order goes up. Could someone help me?","['partial-derivative', 'continuity', 'derivatives', 'smooth-functions']"
4401363,Law of cosines in proving invariance of Mobius energy,"I'm currently reading Freedman's paper on the Mobius invariance of knot energy, and I'm stuck on a particular equality (2.8). Let $\gamma$ be a curve in $\mathbb{R}^3$ parametrized with respect to arc length, and let $T$ be a spherical inversion which does not send the curve to infinity. Then we have that $\frac{|(T\circ \gamma)'(u)| |(T\circ\gamma)'(v)|}{|(T\circ\gamma)(u) - (T\circ\gamma)(v)|^2} = \frac{1}{|\gamma(u) - \gamma(v)|^2}$ for any $u,v$ . The paper, and Wikipedia, says this is a short calculation involving the law of cosines, but I don't see how to apply it, as the tangent vectors, along with the segment joining the two points on the curves, don't form a triangle.","['curves', 'trigonometry', 'geometry', 'mobius-transformation']"
4401407,Problem 9 M.L Krasnov variational calculus,"Warning: Finding extreme value of a multivariable function My question differs from this since I try to use the Hessian criterion so it is not a repeated question. My question: Problem 9 M.L Krasnov. Find extrema of $f:U \to \mathbb{R}$ where $U=\{(x_1,\ldots,x_n)\in \mathbb{R}^n:x_{i}>0 \text{ for all }i\in \mathbb{N}\}$ and $$f(x_1,x_2,\ldots,x_n)=x_1x_2^2\ldots x_n^n(1-x_1-2x_2-\ldots-nx_n). $$ Well this is my attempt:
Critical points are: $$\frac{\partial f}{\partial x_n}=nx_1x_2^2x_3^3(1-x_1-2x_2-\cdots-nx_n)-nx_1x_2^2\cdots x_n^n=0$$ so $$x_1=x_2=\cdots =x_n=\frac{2}{n^2+n+2}.$$ These are critical points of $f$ . Computing second partial derivatives for use Hessian in critical point $x_0=\left(\frac{2}{n^2+n+2},\ldots, \frac{2}{n^2+n+2}\right)$ and suppose no loss of generality $i<j<n$ I get: $$\frac{\partial^2 f}{\partial x_i^2}=i(i-1)\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}}-2i^2\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}-1}.$$ So computing the cross derivatives and substituting into $x_1=x_2=\cdots=x_n=(\frac{2}{n^2+n+2})$ $$\frac{\partial^2 f}{\partial x_j \partial x_i}=-ij\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}-1}.$$ I have tried this exercise for many days without any success, I plan to use Sylvester's criterion to show that this critical point is a local maximum. Sylvester's criterion: The real-symmetric matrix $A$ is positive definite if and only if all the leading principal minors of $A$ are positive. Help me please.","['optimization', 'multivariable-calculus', 'hessian-matrix', 'calculus-of-variations']"
4401422,"What is this derivative, with a diagonal matrix?","Given matrices $\textbf{A}, \textbf{B}, \textbf{C}$ and column vector $\textbf{v}$ , what is the derivative of $\langle \textbf{A} \text{diag}(\textbf{B}\textbf{v}), \textbf{C} \rangle$ with respect to $\textbf{B}$ ? $\text{diag}(\cdot)$ is a diagonal matrix with the argument as the diagonal and the brackets signify the inner product. I am trying to find the solution, but am hung up on differentiating the diagonal matrix. Any assistance would be greatly appreciated. Thank you very much.","['matrices', 'calculus', 'derivatives', 'vectors']"
4401458,Show that $\mu \geq 0$ on Borel subsets of $X.$,Let $X$ be a compact Hausdorff space and $\mu$ be a complex  Borel measure on $X$ such that $\int_{X} f\ d\mu \geq 0$ for every $f \in C(X)$ with $f \geq 0.$ Then show that $\mu$ is non-negative. Could anyone give some suggestion in this regard?,"['integration', 'measure-theory', 'borel-measures']"
4401512,Simplifying $\frac{\sec x + \csc x}{1 + \tan x}$ to an expression in terms of $\sin x$,"i'm having trouble getting this one started please. Simplify the first trigonometric expression by writing the simplified form in terms of the second expression. $$\frac{\sec x + \csc x}{1 + \tan x} \qquad \sin x$$ I have tried converting to $$\frac{\dfrac{1}{\cos x} + \dfrac{1}{\sin x}}{1 + \dfrac{\sin x}{\cos x}}$$ Then $$\frac{ 1 + \dfrac{\cos x}{\sin x}}{\cos x + \sin x}$$ But if I progress this further I cannot seem to yield a result and I'm not sure if I'm heading in the right direction witht this. The answer is apparently, $\dfrac{1}{\sin x}$",['trigonometry']
4401553,Does Weierstrass approximation theorem have any application to differential equations?,"Weierstrass approximation theorem states that continuous periodic functions can be uniformly approximated by trigonometric polynomials. I wonder if it has any applications in differential equations, where you can approximate a solution by approximating a function, and then prove rigorously that it does converge to a real solution. I would be very grateful if you could give me some references.","['partial-differential-equations', 'reference-request', 'ordinary-differential-equations', 'real-analysis']"
4401563,Some thoughts about an equivalent definition of center of a group,"As fancy as it may seem, the following form of the center fits to some variations by analogy: $$Z(G)=\{g\in G\mid \varphi(g)g^{-1}\in\{e\}, \forall\varphi\in\operatorname{Inn}(G)\} \tag 1$$ For example, we can adapt $(1)$ by replacing "" $\operatorname{Inn}$ "" with "" $\operatorname{Aut}$ "", thus getting the absolute center : $$L(G)=\{g\in G\mid \varphi(g)g^{-1}\in\{e\}, \forall\varphi\in\operatorname{Aut}(G)\} \tag 2$$ Likewise, if we replace in $(2)$ the trivial subgroup with $Z(G)$ , we come up to: $$H(G)=\{g\in G\mid \varphi(g)g^{-1}\in Z(G), \forall\varphi\in\operatorname{Aut}(G)\} \tag 3$$ which yields the quotient $H(G)/Z(G)\cong\operatorname{Inn}(G)\cap Z(\operatorname{Aut}(G))$ : the group of the autocentral automorphisms (see here and here ). The next step along this way could be replacing back in $(3)$ "" $\operatorname{Aut}$ "" with "" $\operatorname{Inn}$ "", ending up with: \begin{alignat}{1}
K(G) &= \{g\in G\mid \varphi(g)g^{-1}\in Z(G), \forall\varphi\in\operatorname{Inn}(G)\} \\
\tag 4
\end{alignat} which, if I'm not mistaken, turns out to be isomorphic to $Z(\operatorname{Inn}(G))$ . By similarity with $(1)$ to $(4)$ : are there examples of pairs $(G,N)$ , with nontrivial $N\operatorname{char}G$ , $N\ne Z(G)$ , such that
the subgroups: $$M_A(N)=\{g\in G\mid \varphi(g) g^{-1}\in N, \forall\varphi\in\operatorname{Aut}(G)\}$$ $$\space\space\space\space\space M_I(N)=\{g\in G\mid \varphi(g)g^{-1}\in N, \forall\varphi\in\operatorname{Inn}(G)\}$$ play any role in the knowledge of $G$ ?",['group-theory']
4401564,Integrating a function of a Markov chain on a random length trajectory,"Suppose that $(X_k)_{k\geq 0}$ is a Markov chain in a finite state space $\mathcal{S}$ , with a known transition matrix $\mathbf{P}$ , and let $f:\mathcal{S}\to\mathbb{R}$ be a known function that assigns a real value $f(s)$ to each state in $s\in\mathcal{S}$ . If $n$ is a fixed integer and $X_0 = s_0\in\mathcal{S}$ with probability one, then we can easily compute the sum of $f$ along a trajectory of length $n$ , given by $\mathbf{E}[\sum_{i=0}^{n}f(X_i)]$ . This can be done by computing each term separately and integrating $f$ with respect to the law of $X_i$ , which can be expressed using the transition matrix $\mathbf{P}$ and its powers. I am trying to solve an extension of this problem, where we replace the fixed $n$ by a random integer $N$ that depends on the chain in the following way. Instead of having a trajectory of fixed length $n$ in the sum, I want certain states to be able to increase the length of the trajectory when visited. To formalize this idea, I define a map $m:\mathcal{S}\to\mathbb{N}\cup\{0\}$ with $m(s_0)\geq 1$ , where $m(s)$ represents the increase in trajectory length brought by visiting the state $s \in \mathcal{S}$ , and I define the process $(M_k)_{k\geq 0}$ that will keep track of the remaining trajectory length. We set $M_0 = m(s_0) \geq 1$ , which is the initial trajectory length, and let $M_k = M_{k-1} + m(X_k) - 1$ . With each time step and visited state $s$ , the remaining trajectory length decreases by $1$ , and increases by $m(s)$ . The summation of $f$ along the trajectory stops when $M_k$ reaches zero, i.e. we define $N = \inf\{k\geq 0 : M_k = 0\}$ and are interested in computing $\mathbf{E}[\sum_{i=0}^{N}f(X_i)]$ . To exclude the possibility that $\mathbf{P}(N=\infty)>0$ , I assume that there exists one and only one absorbing state $s_A\in\mathcal{S}$ , such that $m(s_A)=0$ . This makes $N$ almost surely finite. However, if there exists a state $s\neq s_A$ with $m(s)=0$ , it can happen that $X_N \neq s_A$ . Is there a hope to analytically calculate $\mathbb{E}[\sum_{i=0}^N f(X_i)]$ for a general choice of $\mathcal{S}, \mathbf{P}, f$ and $m$ that satisfy the above properties ? The only idea I have is to partition the space according to the value of $N$ , but then the difficulty shifts to computing the quantities $\mathbb{P}(N=n)$ .","['markov-chains', 'probability-theory', 'probability', 'random-variables']"
4401566,How can I prove that both of the following definitions of the uniqueness quantifier in first-order logic are equivalent?,"First definition: $$\exists!x:P(x)\iff\exists x(P(x)\wedge\forall y(P(y)\implies y=x))$$ Second definition: $$\exists!x:P(x)\iff\exists x(P(x)\wedge\forall y\forall z(P(y)\wedge P(z)\implies y=z))$$ I already know that the second definition of uniqueness implies the first one, because if the predicate holds true for all values of $z$ , then it also holds true for only the values of $z$ for which $z=x$ , which would leave us with a predicate which is equivalent to the first definition. But how do I prove that the first definition of uniqueness also implies the second one?","['first-order-logic', 'quantifiers', 'logic', 'definition', 'discrete-mathematics']"
4401567,"Find all numbers $x$ such that $x+3^x <4$ .""spivak calculus""","The question is that find all numbers $x$ such that $x+3^x <4$ . This is a question from problem $4$ from Spivak Calculus book, My attempt: It easy to see that $1$ is a solution of this equation $f(x)=x+3^x -4=0$ ,
and $ f'(x)=1+\ln(3)e^{x\ln(3)}>0$ $ \forall x \in \mathbb{R}$ so $f(x)$ is strictly increasing. So $ \forall (x,y) \in \mathbb{R}$ $x<y $$\Rightarrow $$ f(x)<f(y)$ and we have $x+3^x <4$$\Rightarrow $$f(x)<0$$\Rightarrow$$f(x)<f(1)$ Now if we can show that $f(x)$ is bijective we can say that $f(x)<f(1)$$\Rightarrow $$x<1$ . i have two question : $(q1)$ does my attempt correct? $(q2)$ I am a student in the second year of a university majoring in mathematics and i do self-learning of calculus through ""Spivak calculus"", after finishing the first chapter of the book ,I am confused how I should solve the exercises 'Should I rely only on the previous paragraph or what !!' the above  problem exist at the first chapter(basic properties of numbers) of the book ,so we must to solve it without using some thing like bijective and continuous function...,??!!","['calculus', 'algebra-precalculus', 'analysis', 'real-analysis']"
4401572,Integral inequality and the HÃ¶lder inequality,"Let $\mu:S\rightarrow[0, +\infty]$ be a positive measure on $S$ $\sigma$ -algebra on $X$ such that $\mu(X)=1$ , and let be $f,g:X\rightarrow \mathbb{R}$ be positive $S$ -measurable functions such that: $$f(x)g(x)\geq1$$ $\mu$ -almost everywhere in $X$ .
Prove that: $$\int_X f d\mu\cdot\int_X gd\mu\geq1$$ So, I've proved that $\int_X f(x)g(x)d\mu\geq1$ and then I used the HÃ¶lder inequality and obtained: $$\left(\int_X f^2 d\mu\right)^{\frac12}\cdot\left(\int_X g^2d\mu\right)^{\frac12}\geq1.$$ How do I move forward from this point?","['integration', 'inequality', 'functions', 'holder-inequality']"
4401587,Why progressive processes?,"This question can be seen as an inverse of Why predictable processes? I'm currently trying to get a basic understanding of stochastic integration (in particular ItÃ´ integrals), and I'm a bit confused about the introduction of different kinds of measurability. I'm reading Kallenberg, and when they defined the integral wrt to the Brownian motion we in particular assumed the integrands to be progressive and when generalizing to semi-martingales with possible jump discontinuities we assume the integrands to be predictable. I'm aware that predictability implies progressiveness is needed to ensure that the integrals are semi-martingales when the integrators might not be Brownian motions, so I understand why predictable processes are necessary to define. My question is why do we define progressive processes then? Is it just to have a larger class of integrable processes or is there some other deeper reason? Any help is greatly appreciated.","['stochastic-integrals', 'stochastic-processes', 'measure-theory', 'probability-theory']"
4401595,Solving $\frac{\ 3x}{x^2+x+1}+\frac{2x}{x^2-x+1}=3$ with other approaches,"$$\frac{\ 3x}{x^2+x+1}+\frac{2x}{x^2-x+1}=3$$ $$x=?$$ I solved this problem as follow: $x=0$ is not a root, we  can divide numerator and denominator of each fraction by $x$ : $$\frac{3}{t+1}+\frac{2}{t-1}=3\quad\quad\text{where $t=x+\frac1x$}$$ $$5t-1=3t^2-3\Rightarrow t=2 , \frac{-1}6$$ Only $x+\frac1x=2$ is acceptable and $x=1$ . I'm looking for other elegant methods to solve this equation.",['algebra-precalculus']
4401599,I can't solve this series,"I was given the following series $\
S(n) = 1+\frac{1}{2}\cdot \frac{1}{2}+
\frac{3*1}{4*2}\cdot\left ( \frac{1}{2}\right )^2 + \frac{5*3*1}{6*4*2}\left ( \frac{1}{2} \right )^3 + ... $ i noticed that each individual term looks very similar to the solution for a wallis integral Thus $$
S(n) = \frac{2}{\pi}\int_{0}^{\pi/2}\frac{sin^0x}{2^0} + \frac{sin^2x}{2^1}+\frac{sin^4x}{2^2}+\frac{sin^6x}{2^3}+...dx
$$ Which appeared to be a geometric series,
Hence $$ 
S(n) = \frac{2}{\pi}\int_{0}^{\pi/2} 
\frac{(\frac{sin^2x}{2})^n-1}{\frac{sin^2x}{2}-1}
dx
$$ Which after some algebraic simplifications gives me this horrible integral at which i am stuck $$
= \frac{2^{2 - n}}{\pi}\int_{0}^{\pi/2}\frac{sin^{2n}x - 2^n }{sin^2x - 2} dx
$$ So how can this integral be solved ? is it non-elementary integral ? Or is there another approach to solve this problem ? Edit:
Thanks for your response, i thought at first that the question wanted me to find the sum of the series to the n-th term, In continuing to my previous approach i would be using infinite geometric series formula instead $$
S =\frac{2}{Ï€} \int_{0}^{\pi/2} \frac{dx}{1 - \frac{sin^2x}{2}} = âˆš2
$$ Which can be easily solved by different techniques !","['integration', 'calculus', 'sequences-and-series']"
4401675,"What led Grothendieck to emphasize the ""relative perspective""?","It is often said that one of the most influential concepts Grothendieck introduced through the scheme theory is the emphasis on the ""relative perspective,"" that is, properties should be interpreted as a property of morphisms instead of one of the objects. However, I don't know exactly what was the main idea which made Grothendieck think about this point of view. My guess is that it was the consciousness of the fact that all rings can be seen as a space (i.e. affine scheme): in the pre-Grothendieck era, one thought that a variety is an absolute object that existed by itself (here, the base field $k$ was not seen as a ""space""). After scheme theory, however, variety was defined as a scheme over an affine scheme (which comes from some field $k$ ) $\mathrm{Spec}\ k$ with some nice properties, that is, a variety is a morphism between spaces $V \to \mathrm{Spec}\ k$ . I don't know whether this is the case. I don't have any evidence to support my conjecture. My question is: Is my guess correct? If it is wrong, what is the answer? Sorry for asking a somewhat vague question, but I will appreciate your answers.",['algebraic-geometry']
4401684,How do I show this fact about product of expectation value?,"I have the following problem: We have $X,Y$ two random variables taking values in $\Bbb{N}$ . We assume that they take only two values. I want to show that $X,Y$ are independent iff. $\Bbb{E}(XY)=\Bbb{E}(X)\Bbb{E}(Y)$ . I somehow get confused with the ""...only two values."" Because as I understood it in general if we only consider $\Bbb{E}(X)=\sum_{n\in \Bbb{N}} n P(X=n)$ . But now how do I need to interprete the fact that they only take two values. Do I only have $\Bbb{E}(X)=n_1P(X=n_1)+n_2P(X=n_2)$ and similarly for $\Bbb{E}(Y)=m_1P(Y=m_1)+m_2P(Y=m_2)$ ? Thanks for your help.","['expected-value', 'probability-theory', 'probability']"
4401734,"Show that $\alpha(\mathcal A,\mathcal B)\leq 1/4$.","Let $(\Omega,\mathcal F,P)$ be a probability space, and let $\mathcal A$ , $\mathcal B$ be sub- $\sigma$ -algebras of $\mathcal F$ . I'm asked to show that the $\alpha$ -mixing coefficient $$\alpha(\mathcal A,\mathcal B):=\sup\bigg\{\big|P(A\cap B)-P(A)P(B)\big|:A\in\mathcal A, B \in \mathcal B\bigg\}$$ between $\mathcal A$ and $\mathcal B$ is bounded above by $1/4$ . My attempt: Let $A\in\mathcal A$ and $B\in\mathcal B$ . Suppose first that $P(A)\geq P(B)$ . Then $$P(A\cap B)-P(A)P(B)\leq P(B)-P(B)^2=P(B)P(B^c)$$ and $$P(A)P(B)-P(A\cap B)=P(A\setminus B)P(B)+P(A\cap B)P(B)-P(A\cap B)$$ $$\leq P(B^c)P(B)+P(A\cap B)(P(B)-1)\leq P(B)P(B^c)$$ Therefore $$|P(A\cap B)-P(A)P(B)|\leq  P(B)P(B^c)\leq 1/4$$ where the last inequality is because the function $x\mapsto x-x^2$ has a unique maximum at $x=1/2$ . The case $P(A)\leq P(B)$ can be proven similarly. As $A\in\mathcal A$ and $B\in\mathcal B$ were arbitrary we conclude that $\alpha(\mathcal A,\mathcal B)\leq 1/4$ . Am I missing something? Thanks for your help.","['probability-theory', 'probability']"
4401735,"Is the ""Higher Order Derivative Test"" more informative than the ""Second Order Derivative Test""?","I was reading about the ""Second Order Derivative Test"" which can show whether a given (stationary) point is a minimum, maximum or a saddle point. However, I have heard that the ""Second Order Derivative Test"" is not always conclusive - there are certain conditions in which the ""Second Order Derivative Test"". This lead me to the ""Higher Order Derivative Test"" which if I have understood correctly, can do everything that the ""Second Order Derivative Test"" can do, and aditionally clarify certain inconclusive instances that the ""Second Order Derivative Test"" struggles with. Although there are still some instances in which the ""Higher Order Derivative Test"" can be inconclusive. My Question: Can the ""Higher Order Derivative Test"" be considered to be better than the ""Second Order Derivative Test"", i.e. provide informative results in all cases where the ""Second Order Derivative Test"" works and in additional cases? For high dimensional multivariate functions, is it true that the ""Higher Order Derivative Test"" will have very complex matrices of partial derivatives - thus making it ineffecient to use in these cases, and thus favors the ""Second Order Derivative Test""? Finally, is it true that the ""Second Order Derivative Test"" and the ""Higher Order Derivative Test"" can not tell if a stationary point is a local minimum or a global minimum? The only way to tell if a stationary point is a global minimum would be to compare the value of the function at all stationary points and see which one of these stationary points produces the lowest value of the function? Thanks!","['calculus', 'functions', 'derivatives']"
4401832,"If $\alpha<45^\circ$, then $\tan\alpha<\cot\alpha$ using unit circe","I am trying to show (prove) using the diagram below of the unit circle that if $\alpha<45^\circ$ , then $\tan\alpha<\cot\alpha.$ It all comes down to showing $AP=\tan\alpha<OQ_1=\cot\alpha.$ We can note that since $\alpha<45^\circ,$ then all of its trig functions are positive and we can say they're equal to the respective segments (as I have denoted on the diagram). Also I would like to ask you what happens if $\alpha$ is a negative angle as the text of the problem doesn't expressly states that it cannot be such? As a matter of fact on the diagram in my book the direction of the angle alpha is from OA to OM (a directed arrow). I am not sure this means it's positive.",['trigonometry']
4401841,"Why do we need to assume regularity of a probability measure $\mu$ to say that $\mathrm{supp}(\mu)=\cap\{A:\mu(A)=1,\,A\text{ is closed}\}$?","$\newcommand{\supp}{\operatorname{supp}}$ I have this (I thought easy) problem: Let $K$ be a compact Hausdorff space and $\mu$ a regular Borel probability measure on $K$ . Show that: $$\supp\mu=\bigcap\{A\subseteq K:A\text{ is closed and }\mu(A)=1\}$$ Here, $\supp\mu=\{x\in K:\forall\text{ open }U\ni x,\,\mu(U)\gt0\}$ , the so-called topological support of $\mu$ . Clearly, calling the RHS of the above $L$ , if $x\in K\setminus L$ then there is an open neighbourhood of $x$ disjoint from $L$ , which by measure monotonicity implies the measure of that neighbourhood is $0$ as $\mu(L)=1,\,\mu(K\setminus L)=0$ , so $x\notin\supp\mu$ . Likewise, if $x\notin\supp\mu$ , then one neighbourhood of $x$ is null, so there is at least one closed set disjoint from $x$ with measure $1$ . Thus $x$ is not in the intersection of such closed sets, $x\notin L$ . Easily we have $\supp\mu=L$ . Why did the textbook assume regularity?","['borel-sets', 'general-topology', 'measure-theory']"
4401856,What is the probability of getting TTHH before HHH in repeated fair coin toss?,"We have a fair coin and it is being tossed until either HHH or TTHH appear. What is the probability of getting TTHH before HHH? I know that the answer is $7/12 \approx 0.583$ but I am not getting why this is the answer. Also, this is counter-intuitive. Shouldn't the probability of getting the pattern HHH be larger than the probability of getting the pattern TTHH since, after four tosses, we can only have one TTHH pattern while we have 3 HHH patterns? My attempt: I have drawn the probability tree for 4 consecutive coin tosses. Here, we have three patterns (HHHH, HHHT, THHH) that have HHH and one pattern that has TTHH. So, intuitively it should be that the probability of getting TTHH before HHH is less than 1/4 while the probability of getting HHH before TTHH should be 3/4. But this answer is wrong. Correct answer is Pr.(getting TTHH before HHH) = 7/12. I made a python code to estimate this result attached below. It shows the correct answer. But how to make sense of this result? How to get the close form answer of 7/12 without estimating? Python code for estimating: import random as rd
cHHH = 0
cTTHH = 0
m = 10000
for i in range(0,m):
  num = str(rd.randint(0,1))
  while ( (num[-4:] != ""0011"") and (num[-3:] != ""111"") ):
    num = num + str(rd.randint(0,1))
    if num[-3:] == ""111"":
      cHHH += 1
      break
    if num[-4:] == ""0011"":
      cTTHH += 1
      break

p = cTTHH / m
q = cHHH / m
print(""Probability of getting TTHH before HHH: "", p)
print(""Probability of getting HHH before TTHH: "", q)

#Output: 
Probability of getting TTHH before HHH:  0.5793
Probability of getting HHH before TTHH:  0.4207","['combinatorics', 'probability']"
4401862,"Show that if $\sup_i \text E(X_i^2) < \infty$, then $\text E(X^2) < \infty$","Problem: Let $\{X_i\}_{i = 1}^{\infty}$ be a sequence of random variables on a probability space $(\Omega, \mathcal F, P)$ such that $\lim_{i \to \infty} X_i = X \text{ a.e.}$ Show that if $\sup_i \text E(X_i^2) < \infty$ , then $\text E(X^2) < \infty$ . My Attempt: First I want to note that there is an extremely similar question here but I don't understand Davide Giraudo's answer. For one thing, his answer doesn't have any $X^2$ s in it, which is confusing me. I will try to explain as best I can. First, I have a version of Fatou's Lemma stating that if $\{X_i\}_{i = 1}^{\infty}$ is a sequence of non-negative random variables, then $\text E (\liminf_i X_i) \leq \liminf_i \text E(X_i)$ . If we let $Y_i = X_i^2$ then I have a sequence of non-negative random variables to work with. One concern of mine is this: can I assume that $\lim_{i \to \infty} X_i^2 = X^2$ ? I feel like that's necessary for for what I've written below to work. If I can make that assumption then we have \begin{align*}
\text E (X^2) &= \text E(\liminf_i X_i^2) \textbf{ (Is this justified?)}\\
&\leq \liminf_i \text E(X_i^2) \text{ (application of Fatou's Lemma)}\\
&\leq \sup_i \text E(X_i^2) \text{ (property of real numbers)}\\
&< \infty \text{ (by assumption)}.
\end{align*}","['measure-theory', 'probability-theory', 'analysis', 'real-analysis']"
4401886,Generalizing Scheffe's Lemma using only Convergence in Probability,"I thought about this question recently, because we accidentally stated this lemma with convergence in probability instead of the usual almost sure convergence as an exercise. The usual proof with Fatou's lemma does not work in this case. It turns out that you can in fact generalize Scheffe's lemma to the following Generalized Scheffe's Lemma Assume that $(X_n)_{n\in\mathbb{N}}\subset L^1 $ converges in probability to $X_\infty\in L^1$ . Then the following statements are equivalent: $\mathbb{E}[|X_{n}|]\to \mathbb{E}[|X_{\infty}|]<\infty$ , as $n\to \infty$ . For all $\epsilon>0$ we have $\limsup_{n\to\infty} \mathbb{E}[|X_n|\mathbb{1}_{|X_\infty-X_n|> \epsilon}] \le \epsilon$ $\{X_\infty, X_1, X_2,\dots\}$ are uniformly integrable $X_n\to X_{\infty}$ in $L^1$ , as $n\to \infty$ This result is not new, see references in the comments, but I wanted it to become easier to find. So here is a proof.","['integration', 'measure-theory', 'probability-theory', 'real-analysis']"
4401935,Partial Derivative of Sigmoid,"As it has been stated elsewhere, the derivative of sigmoid is $\sigma$ (x)(1- $\sigma$ (x)). So with that being said I just would like for verification that when taking the partial derivative to a sigmoid function that I am correct in my thinking.  So lets say we had a function f(x) = $\sigma$ ( $w_1$ x + $b_1$ ) and the goal is to take the partial derivative of f(x) with respect to $w_1$ .  We have: $\frac{\partial f}{\partial w_1}$ = $\sigma$ ( $w_1$ x + $b_1$ ) Based off the knowledge of what the derivative of sigmoid is, can we rewrite the problem as? $$\\$$ $\sigma$ ( $w_1$ x + $b_1$ )(1- $\sigma$ ( $w_1$ x + $b_1$ )) $\frac{\partial f}{\partial w_1}($$w_1$ x + $b_1$ ) If so, then proceeding on: $\sigma$ ( $w_1$ x + $b_1$ )(1- $\sigma$ ( $w_1$ x + $b_1$ ))(1*x + 0) Thus the final answer is: $\sigma$ ( $w_1$ x + $b_1$ )(1- $\sigma$ ( $w_1$ x + $b_1$ ))(x) Is this correct thinking?  Thanks!","['partial-derivative', 'machine-learning', 'derivatives']"
4401972,How many lattices does it take to cover a regular $n$-gon?,"Given some positive integer $n\ge 3$ , we can ask how many 2-dimensional lattices $L_1,\ldots,L_k$ are required such that their disjoint union contains all vertices of a regular $n$ -gon. (We don't require that the lattices be centered at the origin.) When $n=3,4,6$ , the polygon is a lattice polygon, and we only need $k=1$ (since these are the only regular lattice polygons, $k\ge2$ for all other $n$ ). Since any noncollinear three points can be covered by some lattice, we have an upper bound of $\lceil\frac n3\rceil$ , which determines $a(5) = 2$ . We can also work out that $a(8) = a(12) = 2$ by taking the union of two squares or hexagons. Via some direct casework on different linear combinations of vectors, I believe no lattice can cover four points on a regular heptagon and so $a(7)=3$ . The sequence thus starts $1, 1, 2, 1, 3, 2, \ldots$ By imposing some bounds on further terms, there are no matches in the OEIS, even accounting for possible initial terms at $n=0,1,2$ with $a(2)=1$ . A simple upper bound on the sequence is given by the following: If $6|n$ , $a(n) = n/6$ . Else if $2|n$ , $a(n) = \lceil \frac n4\rceil$ . Else, $a(n) = \lceil \frac n3\rceil$ . In other words, we place hexagons if we can, otherwise we place rectangles, otherwise we just cover $3$ points at a time arbitrarily. I don't know of counterexamples to the above formula, but I wouldn't be at all surprised if there are some. How can further terms of this sequence be efficiently computed?","['integer-lattices', 'geometry', 'polygons', 'oeis', 'roots-of-unity']"
4401989,Prove that strong induction and standard/weak induction are logically equivalent.,"I would like some guidance in the right direction for my attempt. Proof. ( $\Rightarrow$ ): First assume strong induction to be true and we prove standard induction. Consider the set $A \subseteq \mathbb{N}$ defined by the conditions, $1 \in A$ and $n \in A \Rightarrow n+1 \in A$ . We prove that $A = \mathbb{N}$ . Define another set $B \subseteq \mathbb{N}$ such that $B$ contains all $m \in \mathbb{N}$ such that $m \notin A$ . Clearly $1 \notin B$ since $1 \in A$ . Now assume that $1,...,n \notin B \Rightarrow 1,...,n \in A$ . In particular, notice that $n \in A \Rightarrow n+1 \in A$ (by definition of how $A$ is defined.). So by strong induction, we must have $A = \mathbb{N}$ as desired. ( $\Leftarrow$ ): Now assume standard induction and we prove strong induction. Consider the set $C \subseteq \mathbb{N}$ defined by the conditions, $1 \in C$ and $1,...,k \in C \Rightarrow k+1 \in C$ . Since $1 \in C$ and $k \in C \Rightarrow k+1 \in C$ , we get that $C = \mathbb{N}$ (by use of standard induction). QED. Please point out any circular reasoning, confusing use of notation or simply if some of my arguments don't make any sense. Thanks in advance!","['elementary-set-theory', 'induction']"
4402063,find density function from moment generating function,"I'm suppose to obtain the density function of this mgf for a discrete random variable $Y$ taking values in range $[0, \infty)$ , where $E(e^{ty}) = e^{-l}[1-e^t(1-p)]^{-ld},c\in \mathbb{R}^+,p \in [0,1]$ . My intuition was to find the taylor polynomial of this function but it quickly blew into a incomprehensible mess.","['moment-generating-functions', 'statistics', 'probability']"
4402075,inverse proportional probabilities,"Imagine that we have a series of $n$ positive real numbers $x_1 ,\ldots, x_n$ . We want to assign a probability $p_i \in [0,1]$ to each number $x_i$ proportional to its magnitude so that all the probabilities add up to 1 ( $\sum_{i=1}^{n}p_i=1$ ). One way to do this is to use the expression: $p_i= x_i/\sum_{j=1}^{n}x_j$ The question is: if we want to assign to each number $x_i$ a probability $p_i$ inversely proportional to its magnitude maintaining $\sum_{i=1}^{n}p_i=1$ , how can we do it?","['statistics', 'probability-distributions', 'functions', 'probability', 'random-variables']"
4402076,How can the Loss Functions of Neural Networks be Non-Convex?,"I have heard the following argument being made regarding Neural Networks: A Neural Network is a composition of several Activation Functions Sigmoid Activation Functions are Non-Convex Functions The composition of Non-Convex Functions can produce a Non-Convex Function Thus, Loss Functions for Neural Networks that contain several Sigmoid Activation Functions can be Non-Convex Using the R programming language, I plotted the second derivative of the Sigmoid Function and we can see that it fails the Convexity Test (i.e. the second derivative can take both positive and negative values): e = 2.718

eq = function(x){ (-e^-x)* (1+e^-x)^-2  + (e^-x)*(-2*(1+e^-x)^-3 *(-e^-x))}

plot(eq(-100:100), type='l', main = ""Plot of Second Derivative of the Sigmoid Function"") My Question: (If the above argument is in fact true) Can the same argument be extended to lack of Convexity of Loss Functions of Neural Networks containing several ""RELU Activation Functions"" ? On it's own, the ReLU function is said to be Convex.
Mathematically, we can show that compositions of Convex Functions can only produce a Convex Function ( The composition of two convex functions is convex ). I understand that the composition of two Convex functions can produce a Concave Function - but still, the composition of two Convex Functions can not produce a ""classic type of Non-Convex Function"" (e.g. a function with several local minima and saddle points). However, Neural Networks that contain compositions of (only) ReLU Activation functions make it unclear to me how a Loss Functions that contains (only) ""RELU Activation Functions"" would a Non-Convex. (The only thing I can think of is that the Loss Function in Neural Networks is made of linear combinations of function compositions - and even though compositions of convex functions are always convex, pe rhaps the linear combination of compositions for convex functions might not necessarily be convex ... but I am not sure about this ) Can someone please comment on this? If compositions of Convex Functions can only produce Convex Functions - does this mean that the Loss Function of a Neural Network containing only containing ReLU Activation Functions can never be Non-Convex? Thanks! References: https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html Note: Using some informal logic, I do not think that the Loss Functions of Neural Networks containing RELU Activation Functions are generally Convex. This is because RELU (style) Activation Functions are generally some of the most common types of activation functions being used - yet the same difficulties concerning mon-convex optimization still remain. Thus, I would like to think that Neural Networks with RELU Activation Functions are still generally non-convex. For example, below we can see the Loss Surfaces of (Modern) Neural Networks that clearly look Non-Convex: ( https://www.cs.umd.edu/~tomg/projects/landscapes/ )","['machine-learning', 'functions', 'convex-analysis', 'neural-networks']"
4402077,Probability of getting an obtuse triangle when choosing three points on a circle.,"The problem is: $1153$ points are evenly distributed on a circle. Find the probability of randomly picking $3$ points constituted vertices of an obtuse triangle. The given solution says $$
\frac{\binom{1153}{1}\binom{576}{2}}{\binom{1153}{3}} = \frac{1725}{2302}
$$ What I don't understand is how we get $\binom{576}{2}$ . Where does this come from? I am assuming that $576$ comes from $1153-1 \over 2$ but I cant figure out why we do that, and why we have to choose 2 points from them. Any help?","['contest-math', 'circles', 'geometry', 'triangles', 'probability']"
4402112,Nature of solutions above the maximal equilibrium solutions,"Q. Consider the ordinary differential equation $y^{\prime}=y(y-1)(y-2)$ . Which of the following statements is true? (1) If $y(0)=0.5$ then $y$ is decreasing (2) If $y(0)=1.2$ then $y$ is increasing (3) If $y(0)=2.5$ then $y$ is unbounded (4) If $y(0)<0$ then $y$ is bounded below I can deal with options 1 and 2 since the function $f(t,y)=y(y-1)(y-2)$ is locally Lipschitz in any compact neighbourhood of $(0, y(0))$ . The solutions distinct from equilibrium solutions cannot intersect with equilibrium solutions to preserve the uniqueness. But how to deal with the possible solutions above $y=2$ and below $y=0$ ? Of course the derivative $y'>0$ for the solution above $y=2$ but I feel it is not sufficient to say the solutions blows up to infinity. If there is no more equilibrium solution, how  can we say the solutions above $y=2$ blows up? Or if any solution is asymptotic to $y=y_0$ , is it necessary to say $y=y_0$ is an equilibrium solution?","['ordinary-differential-equations', 'real-analysis']"
4402130,Discontinuous vector field with curl 0,"Let $S$ be a part of the paraboloid $z=1-x^2-y^2$ such that $z\geq 2|y|$ . They ask to calculate $$
\int_C\frac{-y}{x^2+y^2}dx+\frac{x}{x^2+y^2}dy+\frac{1+e^z}{1+z^2}dz
$$ where the curve $C$ is traversed once in an anticlockwise direction if it is observed from the point $(0,0,1)$ . It is easy to see that the curl of $F$ is $(0,0,0)$ . So my initial idea was to use Stokes' theorem with which the answer would be zero. But realizing that $F$ is not a continuous field this is not possible, now in reality it would be necessary to look for a surface that has two borders: one of them $C$ and another $C_0$ (which would be easier to calculate). The following occurs to me, taking the same surface but bounded above with $z=15/16$ . That would make the new surface no longer go through the z axis. My attempt Let $\lambda(t)=(\frac{1}{4}\cos t,\frac{1}{4}\sin t,\frac{15}{16})$ then $\lambda'(t)=(-\frac{1}{4}\sin t,\frac{1}{4}\cos t,0)$ . And we have $F(\lambda(t))=(-4\sin t,4\cos t,\frac{1+e^{15/16}}{1+(15/16)^2})$ . \begin{align}
\int_{C_0} F\cdot dr&=\int_{C} F\cdot dr+\iint_S (\nabla\times F)\cdot dS=\int_C F\cdot dr\\
\int_{C_0} F\cdot dr&=\int_0^{2\pi}\cos^2 t+\sin^2 tdt=2\pi
\end{align}","['vector-fields', 'multivariable-calculus', 'stokes-theorem', 'vector-analysis']"
4402149,Polynomials such that $P(\mathbb{U})\subset \mathbb{U}$,"Question Let $\mathbb{U}$ be the unit circle.
Find all $P\in \mathbb{R}[X]$ such that $P(\mathbb{U})\subset \mathbb{U}$ . My attempt My conjecture is that $P=\mu X^d$ for some $d\geqslant 0$ and $\mu \in \mathbb{U}$ .
I tried to prove it by contradiction, making a recurrence on the number of monomials in the polynom. Unfortunately it doesn't lead to anything interesting. Could someone help me ? NB : It isn't the same the question as Which polynomials fix the unit circle? , since $P(x+iy)\neq P(x)+iP(y)$ in general.","['algebra-precalculus', 'polynomials', 'complex-numbers']"
4402240,Is my proof that $\forall x: x\notin x$ correct?,"Is my proof that $\forall x: x\notin x$ correct? Proof. Let us notice that for all $x$ it's true that $$\neg \big[x\in x\iff \neg (x\in x)\big]\tag{1}$$ but $(1)$ is logically equivalente to $$x\in x\oplus \neg (x\in x)\tag{2}$$ where $\oplus$ is the exclusive or . So, at most, one of the cases in $(2)$ is true. If $x\in x$ for all $x$ then in particular $\emptyset\in \emptyset$ so it must be that for all $x, x\notin x$ .","['elementary-set-theory', 'logic']"
4402262,Class of 24 pupils and 4 chosen for committee members,"A class of 24 pupils consists of 11 girls and 13 boys. To form the class committee, four of the pupils are chosen at random as ""Chairperson"", ""Vice-Chairperson"", ""Treasurer"", and ""Secretary"". Find the number of ways the committee can be formed if (i) the committee consists of at least one girl and at least one boy, (ii) the ""Treasurer"" and ""Secretary"" are both girls, (iii) The teacher requires a group of four students to represent the class in a student survey. Find the number of ways this group of students can be selected if there must be at least 1 girl and at most 2 boys. My answers: (i) GBBG, GBBB, GBGG (11C2 x 13C2) + (11C1 x 13C3) + (11C3 x 13C1) = 9581 (ii) GGGG, BBGG, BGGG 11C4 + (11C2 x 13C2) + (11C3 x 13C1) = 6765 (iii) GGGG, BBGG, BGGG 11C4 + (11C2 x 13C2) + (11C3 x 13C1) = 6765 I have checked the correct answers, it shows that (i) 229944   (ii)  50820   (iii) 6765 I don't see my (i) is wrong, is it the correct answer for (i) of 229944 as incorrect? For (ii) why it is using 11P2 x 22P2 = 50820 for the answer? Why this is a permutation question?","['permutations', 'combinations', 'combinatorics']"
4402284,Composition of measurable functions in Cohn's book on measure theory,"In Measure Theory (2nd ed.) - Cohn, Donald L. it says the following: Proposition 2.6.1. Let (X,A), (Y,B), and (Z,C) be measurable spaces, and let f : (Y,B) â†’ (Z,C) and g: (X,A) â†’ (Y,B) be measurable. Then fâ—¦g: (X,A) â†’ (Z,C) is measurable. I'm wondering why this doesn't contradict the counterexample provided by Mirjam here: Is composition of measurable functions measurable?","['measure-theory', 'lebesgue-measure']"
4402342,Example of utility functions,"I am working on an Economics problem and an example function of a graph that looks like this would be helpful. Suppose the straight line is $ax + by = c$ , and the curvy lines (called Indifference Curves ) are given by the (utility) function $U(x,y)$ when constant. I need example functions $U(x,y)$ which, when constant, look like the curvy lines. Note that $t > s > r$ .","['economics', 'functions']"
4402358,How to define an asymmetric oscillatory function with increasing amplitude and period?,"I am struggling to model the correlation shown in the figure so that I can predict the positive values of y beyond the observed range: The correlation between y and x appears oscillatory and characterised by: increasing amplitude; increasing period; asymmetric oscillations. I was able to define functions that satisfy some of these features, but I could not find a function that satisfies all these features at once. Can you help me do it? Here is what I was able to accomplish so far, using R: A function characterised by increasing amplitude x <- seq(0, 10, by=0.01)
a = 9 # controls amplitude change rate
b = 1.2 # controls frequency change rate
y <- x*b*sin(x*a) # increasing amplitude The grey line shows what I am attempting to model, the red line is the function in the graph title. A function characterised by increasing amplitude and period b = 1.2 # b controls rate of amplitude increase
g = 10
h = 200
# g and h control rate of period increase
y4 <- x*b*sin(exp(g-x)/h) This function goes flat by x~7. The grey line shows what I am attempting to model, the red line is the function in the graph title. A function characterised by asymmetric oscillations I was able to model this with a modified Clausen function of order 2: d=4.5 # changes frequency
f=2 # changes amplitude
clausen2 <- f*sin(x*d) + f*sin(2*x*d)/4 + f*sin(3*x*d)/(3^2) + f*sin(4*x*d)/(4^2) +
f*sin(5*x*d)/(5^2) + f*sin(6*x*d)/(6^2) + f*sin(7*x*d)/(7^2) + f*sin(8*x*d)/(8^2) +
f*sin(9*x*d)/(9^2) The grey line shows what I am attempting to model, the red line is the function in the graph title. I am not sure if I am overcomplicating things and/or missing something obvious.",['functions']
4402383,What would happen if all the sets weren't open?,"Earlier this year I started my course in multivariable calculus, and since the very beginning the importance of the open sets was very emphasized. Now that we have moved forward I can sense it is true, since they appear in every other definition or theorem pretty often. However, I don't really know the reason why that is true. For this reason, I've been wondering what would happen if we switched the open sets in the definitions, theorems... With other types of sets. Would it imply some kind of mathematical catastrophe, making everything fall apart and become nonsense? It wouldn't destroy everything but it would bring a loss of generality? Does the question even make sense in the first place? I'm sorry if I haven't worded it very clearly, English isn't my first language (and math either...).","['multivariable-calculus', 'calculus', 'general-topology']"
4402401,Longest chain of n-digit square numbers where last digit equals first digit of next,"Consider all the square numbers with exactly n digits, I want to arrange them such that the last digit of a square is equal to first digit of the next square and find the longest arrangement, how many elements contain and possibly how many of those longest arrangement are possible. Of course some squares will be sorted out. For n=2 the squares are {16, 25, 36, 49, 64, 81}. So the longest arrangement is {81, 16, 64, 49} with s=4 elements For n=3 there are s=12 elements in the longest arrangement, and one of the them (there are 26 in total) is: {841, 121, 144, 484, 441, 169, 961, 196, 676, 625, 529, 900} Of course there are some criteria to construct the arrangement without just trying: at most 1 square can start with {2,3,7,8} and if so, must be in the 1st position; at most 1 square with 0 as last digit. I asked for help in this thread , hoping to use Mathematica to brute force the solutions, and I received some great and highly detailed answers, but when I try with n=4 the algorithm slows down.
I guess, but I'm not sure, that the answer for n=4 is s=30 and one of the arrangement is: {2025, 5776, 6561, 1225, 5625, 5041, 1369, 9801, 1024, 4624, 4489, 9216, 6084, 4761, 1521, 1764, 4356, 6889, 9604, 4225, 5329, 9409, 9025, 5184, 4096, 6241, 1681, 1156, 6724, 4900} For n = 5 the longest arrangement I found (thanks to @Peter) is: {39204, 47089 ,97344, 46225, 53824, 41616, 66564, 42025, 51984, 43681, 12544, 48841,13225, 52441, 18225, 58081, 10609, 96721, 10404, 45369, 93025, 56644, 47524, 41209, 91809, 93636, 62001, 10816, 65536, 67081, 16641, 10201, 12321,14641, 19321, 13689, 98596, 66049, 95481, 17161, 15129, 94864, 44944, 43264, 47961, 18496, 63504, 49729, 94249, 90601, 13456, 64009, 99225, 53361, 15625, 55696, 69696, 68121, 14161, 11881, 19881, 18769, 97969, 99856, 61009, 92416, 60516, 60025, 57121, 11025, 55225, 50625, 58564, 42436, 68644, 40401, 14884, 45796, 63001, 11664, 46656, 65025, 51076, 69169, 91204, 44521, 17956, 64516, 61504, 40804, 49284, 42849, 96100} with s=93 So, how can I calculate how many elements are in the longest chain of squares with n digits? Thanks","['graph-theory', 'number-theory']"
4402443,Finite Expectation for a non-negative random variable,"I was told the following fact For a non-negative random variable $X$ , we have $$
\mathbb{E}(X) < \infty  \ \ \ \text{if and only if} \ \ \ \sum_{n=0}^\infty \mathbb{P}(X \geq n) < \infty
$$ The only if part is clear to me, as $$
\mathbb{E}(X) = \int_0^\infty \mathbb{P}(X \geq t) dt \geq \sum_{n=0}^\infty \mathbb{P}(X \geq n)
$$ Yet, I have no clue how to approach the other side. Any hints are welcome, thanks!","['expected-value', 'analysis', 'probability-theory', 'probability']"
4402449,Is this function completely monotone?,"Background Long ago I bumped into an exercise in ordinary differential equations, which asks to find
a solution to the differential equation: $$h'(x)=\frac{1}{2(1+xh(x))}$$ It turns out that $h(x)$ is the inverse function of $$G(x)=2e^{x^2}\int_0^xe^{-u^2}\,du=\sqrt{\pi}e^{x^2}\hbox{erf}(x)$$ where $\hbox{erf}(x)$ is the standard error function. This function $G(x)$ (multiplied by $1/2$ ), appears in the problem set of the second chapter of the book ""Special functions and their applications"", by Lebedev, where the reader is asked to prove that it satisfies a certain differential equation, namely, $$G'(x)=2(1+xG(x))$$ whence it easily follows that the inverse $G^{-1}(x)=h(x)$ satisfies the ODE above for $h$ . In Lebedev's book the reader is asked to use the differential equation for $G$ to derive a series expansion for the error function, valid in the entire complex plain. In terms of $G$ , the series is as follows: $$G(z)=\sum_{k=0}^{\infty}\frac{2^kz^{2k+1}}{(2k+1)!!}\qquad (z\in\mathbb{C})$$ Lebedev leaves it at that, but some further comments may be of interest. A simple calculation involving Stirling's formula shows that this series has an infinite radius of convergence, hence it defines an entire function in the complex plain. Moreover, restricting to positive real values only, the series expansion shows that all derivatives of $G$ are positive in $(0,\infty)$ , i.e., $G(x)$ is an absolutely monotone function in $(0,\infty)$ . Moreover, since $G'(0)\neq 0$ , there is a disc around zero where the inverse function $G^{-1}(z)=h(z)$ is analytic, and its series expansion can be computed from that of $G(z)$ . However, it turns out that the radius of convergence of the series for $G^{-1}(z)$ around zero is finite. This perhaps is connected to the fact that $G(z)$ is not one-to-one in the whole complex plain, i.e., does not have a global inverse. The problem Looking into various properties of the inverse function $h(x)=G^{-1}(x)$ , I came across the function $g(x)=\exp(G^{-1}(x)^2)=\exp(h^2(x))$ , where $x\geq 0$ . Recall that a function $f(x)$ defined on $(0,\infty)$ is called completely monotone if it has derivatives of all order and $(-1)^nf^{(n)}(x)\geq 0$ for all $n=0,1,2,\dots$ . The problem is this: Is the function $g''(\sqrt{x})$ completely monotone in $[0,\infty)$ ? Using the differential equation for $h$ above, the second derivative is $$g''(x)=\frac{\exp(h^2(x))}{2(1+xh(x))^3}$$ so $$g''(\sqrt{x})=\frac{\exp(h^2(\sqrt{x}))}{2(1+\sqrt{x}h(\sqrt{x}))^3}$$ Since this is a composition of $g''(x)$ and the square root function $\sqrt{x}$ , and since the derivative of $\sqrt{x}$ is completely monotone, then if we knew that $g''(x)$ is itself completely monotone, we would be able to deduce that $g''(\sqrt{x})$ is completely monotone as well. But that is not the case: $g''(x)$ is not completely monotone, because, for example, it is not convex. Its second derivative, the fourth derivative of $g(x)$ , is not a positive function. A partial result With $f(x)=g''(\sqrt{x})$ , the following holds.
For every $n$ , there is some $x_n>0$ , such that for
all $x>x_n$ we have $(-1)^nf^{(n)}(x)>0$ . My proof is neither elegant nor short. Moreover, I still don't know whether $(-1)^nf^{(n)}(x)>0$ holds for all $x$ . There is lots of information and literature involving completely monotone functions. For example, the property of being completely monotone is equivalent to the property of being the Laplace transform of a positive measure, and in our case, since $g''(0)=\frac{1}{2}<\infty$ , we would have a bounded positive measure. There are lots of tricks and special cases that help determine whether a function is completely monotone, but as far as my research has reached, I could not settle the case for this specific function. It might be interesting to discover new methods of proving -- or disproving -- that a certain function is completely monotone. Any insights, comments, remarks and proofs are welcome.",['real-analysis']
4402462,"Meaning of ""Commas and Periods"" in Mathematical Functions?","Can someone please help me understand the meaning of ""commas and periods"" that appear on the right side of mathematical functions? For example, in another question ( Making the Mandelbrot Fractal in Desmos Online Graphing Calculator ), I learned how to make the ""Mandelbrot Fractal"" using an online graphing calculator: https://www.desmos.com/calculator/hvrhuvaue5 I noticed that in some of these mathematical equations, there are ""periods and commas"" that appear on the right hand side of these equations: Normally, I would have thought that: ""Commas"" generally appear on the left hand side of mathematical equations to indicate which variables are included in the function. For example, a function F(x,y,z) = z*y - x : commas are used to indicate that the function ""F"" is made up of variables ""x"", ""y"" and ""z"". For instance, what does ""x,2z"" mean in the function f(z)? On the other hand, I have usually seen ""periods"" as another notation for indicating ""multiplication"" (apart from indicating ""decimals"", which is clearly not the case here) - but for some reason, I do not think that the ""period"" is referring to multiplication here. I was thinking that maybe the ""period"" might indicate a ""nested function"" , but I also do not think that this is the case here. For instance, what does ""z.y^2"" mean in the function D(z)? Can someone please help me understand the meaning of ""periods and commas"" in mathematical functions? Thank you!","['graphing-functions', 'notation', 'functions', 'computer-algebra-systems', 'algebra-precalculus']"
4402536,"Slutsky's Theorem, Jun Shao's Mathematical Statistics","The statement of the theorem is that if $X, X_1, X_2, \cdots, Y_1, Y_2, \cdots$ are random variables on a probability space, $X_n$ converges in distribution to $X$ and $Y_n$ converges to $c$ a constant in probability, then $X_n + Y_n$ converges to $X + c$ in distribution. My only question about the proof is the claim that $\epsilon$ can be arbitrary. I understand that if $t - c$ is not a continuity points of $F_X$ , then the statement holds trivially for $t$ , so I only consider the case when $t - c$ is a continuity point. But how do I reason that $t - c + \epsilon$ and $t - c - \epsilon$ are continuity points of $F_X$ for arbitrary $\epsilon$ , given that $t - c$ is a continuity point? (The inequalities don't need to hold if $t - c + \epsilon$ and $t - c - \epsilon$ are not continuity points, right?) Thank you very much in advance! Here's the statement and the proof:","['measure-theory', 'probability-theory', 'statistics']"
4402538,"Are Machine Learning Optimization Problem ever Categorized as ""P"" or ""NP""?","In the context of Computer Science and Optimization, I have heard that different problems can be classified using the ""P vs NP"" framework . Essentially, there is a hierarchy of problems based on the inherent complexity of the problem itself. For example, a problem like ""multiplying numbers"" is considered as ""P"" and is considered fundamentally easier to solve than a problem like ""solving a sudoku"" which is ""NP"". In most Statistical and Machine Learning Models, there is usually an optimization problem ""nested"" within the model that is required to solve. For example: Regression Models : In a standard regression model, we try to find the value of the ""beta coefficients"" that either minimize the error between the (candidate) model's prediction of the response variable and true values of the response variable (Ordinary Least Squares - OLS), or we try to find the ""beta coefficients"" such that probability of reproducing the observed response values is maximized (Maximum Likelihood Estimation - MLE) . For simple regression models, there exists ""exact solutions"" to the OLS and MLE optimization problems and we can calculate these ""beta coefficients"" analytically - but in more sophisticated regression models such as Logistic Regression or Regularized Regression (e.g. LASSO, RIDGE), the corresponding optimization problem is usually solved using some approximate and iterative algorithm such as the ""Newton-Raphson"" Method. Decision Trees: A Decision Tree (e.g. CART) is formed by ""splitting"" variables into smaller subsets (i.e. ""nodes"") such that ""purity"" increases in each subsequent subset; ""purity"" is often measured through some sort of ""Information Gain"" that is based on measures such as ""Gini Index"" or ""Entropy"". Thus, Decision Trees can be interpreted as an optimization problem where ""Information Gain"" has to be optimized. I have heard that since Decision Trees often have different variable types (e.g. continuous and categorial), searching for the optimal variable splits that optimize ""Information Gain"" is a Mixed Integer Optimization Problem having an enormous Combinatorial Search Space. For the interest of creating a decent Decision Tree in a reasonable amount of time, ""Information Gain"" is optimized using a ""Greedy Search Algorithm,"" and as a result, the final Decision Tree (i.e. the answer to this Mixed Integer Optimization Problem) is almost certainly unlikely to be the optimal Decision Tree (as there is very high probability that a better Decision Tree likely exists in this large Combinatorial Search Space, but finding this Decision Tree would take too much time): Neural Networks: Successful Neural Networks are largely attributed to the effectiveness of Optimization Algorithms (e.g. Stochastic Gradient Descent) to optimize (i.e. determine the ""neuron weights"") notoriously complicated, non-deterministic (i.e. the loss function is dependent on the observed data), high dimensional and non-convex Loss Functions: My Question: Is it possible to categorize the optimization problems corresponding to Statistical and Machine Learning Models such as Regression Models, Decision Trees, and Neural Networks as ""P"" vs ""NP""? I am aware that categorizing these problems wont really have any effect on solving them, but I have the following guess: When provided with a candidate solution to any of these optimization problems (e.g. Regression beta coefficients, a particular Decision Tree, Neural Network Weights), we have no real way of checking whether this candidate solution is indeed the optimal solution (unlike a sudoku, in which even for an enormous ""n x n"" sudoku, we can instantly check if a candidate solution violates the rules or not). Thus, my guess is that many of these above optimization problems are likely either NP-Complete or NP-Hard. Is this correct? Thanks! Note: I have often heard of Pattern Recognition and Machine Learning Optimization Problems being described as ""Ill-Posed Problems"" , implying that they are inherently more difficult than ""Well-Posed Problems""  This is due to factors such as: solutions to these optimization problems ""may not exist"" solutions ""may not be unique"". slight perturbations to the function/solution can cause large changes within the system it is impossible to determine whether the final solution to the optimization problem is in fact the ""true optimal"" solution - in Machine Learning, we often settle for a ""good enough"" solution I would imagine that many ""ill-posed"" machine learning optimization problems would generally fall under the designation of ""NP"" , ""NP-Complete"" or ""NP-Hard"" compared to simply ""P"". However, I not sure about this.","['statistics', 'regression', 'neural-networks', 'machine-learning', 'optimization']"
4402544,7 Year old School question,"My 7 year old son was given this question as a sort of bonus question and although I managed to solve it using some really awful simultaneous equations I can't help but think there is a simpler more intuitive way to solve it. Afterall, it was given to a 7 year old. The question is this: Amir, Brett and Carly share some money. Amir gets a third of the money. Carly gets 5 times the as much as Brett. Carly gets Â£84 more than Amir How much money does Brett get? Is there a really simple way to solve this? Any help is much appreciated","['algebra-precalculus', 'puzzle']"
4402606,Why is this the result of this integral?,"This problem has to do with the non-approximated solution of the motion of a simple pendulum. I'm asking here instead of at the physics forum because I have a mathematical question. Anyways, the differential equation for a simple pendulum is: $$\frac{\mathrm{d}^2\theta}{\mathrm{d}t^2}+\omega_0^2\sin\theta=0$$ Multiplying both sides by $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ and integrating over $t$ : $$\int\left( \frac{\mathrm{d}^2\theta}{\mathrm{d}t^2}\right)\mathrm{d}\theta=\omega_0^2\cos\theta+C$$ The next step simply states that, therefore: $$\frac{1}{2}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)^2-\omega_0^2\cos\theta=K$$ My problem is with the first term in the left hand side. Why is that the result of the integral? It implies that it's integrating something like: $$\int\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)\mathrm{d}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)=\frac{1}{2}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)^2 + C$$ But I fail to see how that makes any sense with the given steps. Am I missing something obvious here?","['solution-verification', 'ordinary-differential-equations']"
4402636,Why can we only integrate a compactly supported differential form on an open set?,"In the text I'm using, the support of $\phi$ (an n-form on an open $U \subset \mathbb{R}^n$ ) is defined as the closure of the values of $\mathbb{R}^n$ such that $\phi$ is nonzero, and $\phi$ is said to have compact support if its support is compact. Letting $\phi = gdx_1 \wedge\cdots\wedge dx_n$ , we define $\int_{U}\phi$ as $\int_{U}g$ , if it exists. I can't really understand why we need compact support to do this. What is the issue if the n-form does not have compact support, but $U$ is bounded? And, if this is an issue, does this mean that we need to have supp $(\phi)\subset U$ in all cases for the integral to be well-defined?","['smooth-manifolds', 'real-analysis', 'manifolds', 'differential-forms', 'differential-geometry']"
4402669,"Did Kolmogorov's probability ""experiments"" survive in the modern theory?","In Kolmogorov's probability book, he defines the independence of multiple ""experiments"" before using that to define the independence of events. Here is an excerpt from Section 5, Chapter 1 (note that Kolmogorov uses $E$ to refer to the sample space, while I will use $\Omega$ in what follows): Let us turn to the definition of independence. Given $n$ experiments $\mathfrak{A}^{(1)}, \mathfrak{A}^{(2)}, \ldots, \mathfrak{A}^{(n)}$ , that is, $n$ decompositions $$E = A_1^{(i)} + A_2^{(i)} + \cdots + A_{r_i}^{(i)} \quad \quad i = 1,2, \ldots, n$$ of the basic set $E$ . It is then possible to assign $r = r_1 r_2 \ldots r_n$ probabilities
(in the general case) $$p_{q_1 q_2 \ldots q_n} = \mathsf{P} \left ( A_{q_1}^{(1)} A_{q_2}^{(2)} \ldots A_{q_n}^{(n)} \right ) \geq 0$$ which are entirely arbitrary except for the single condition that $$\sum_{q_1, q_2, \ldots, q_n} p_{q_1 q_2 \ldots q_n} = 1$$ DEFINITION I. $n$ experiments $\mathfrak{A}^{(1)}, \mathfrak{A}^{(2)}, \ldots, \mathfrak{A}^{(n)}$ are
called mutually independent , if for any $q_1, q_2, \ldots, q_n$ the following equation holds true: $$\mathsf{P} \left ( A_{q_1}^{(1)} A_{q_2}^{(2)} \ldots A_{q_n}^{(n)} \right ) = \mathsf{P} \left ( A_{q_1}^{(1)} \right ) \mathsf{P} \left ( A_{q_2}^{(2)} \right ) \ldots \mathsf{P} \left ( A_{q_n}^{(n)} \right )$$ In modern language, an ""experiment"" is represented by the sample space, $\Omega$ , of a probability space $\left(\Omega, \mathfrak{F}, P\right)$ , with the elements of $\Omega$ being the elementary outcomes of that ""experiment,"" and elements of $\mathfrak{F}$ being all possible events associated to the ""experiment."" But here Kolmogorov is describing something else: an experiment is a partition of the sample space into a set of disjoint events.  In my understanding, this could be thought of as a specific question about the system, with the disjoint events being the set of all possible answers to that question.  For example, when flipping two coins, you could ask (i) ""Was the first coin heads?"" with experiment $\left\{A, A^c \right\}$ , where $A$ is the event that the first coin was heads, or (ii) ""How many heads appeared?"", where the experiment would be $\left\{A_0, A_1, A_2 \right\}$ with $A_i$ being the event of seeing $i$ heads. To define the independence of events, Kolmogorov states that two events, $A$ and $B$ , are independent if their associated experiments, $\left\{A, A^c \right\}$ and $\left\{B, B^c \right\}$ , are independent. The (four) resulting equations from the final equation in the above excerpt reduce to a single independent equation, which can be taken to be $P\left(A\cap B\right) = P(A)P(B)$ . He then defines the conditional probability of an event, B, with respect to an experiment, $\mathfrak{A}=\left\{A_1, A_2, \ldots \right\}$ , as a random variable that takes the (already defined, standard) value $P(B | A_i)$ when acting on the element of the partition of $\Omega$ defined by $\mathfrak{A}$ .  This definition generalizes nicely in later sections (Chapter 5) to define conditional probability with respect to a random variable, which can be thought of as itself defining a partition of $\Omega$ , and thus an ""experiment."" My question is: what is the purpose of these definitions (Kolmogorov's ""experiments"", and the associated definitions of independence and conditional probabilities) in probability theory?  I ask because I cannot find very much discussion of them in probability textbooks or on the internet.  It seems that either they have been discarded as not vital to the theory, or have been replaced and renamed, so that I cannot find them.","['conditional-probability', 'independence', 'probability-theory', 'probability']"
4402683,Law of the iterated logarithm of a Brownian motion in $\mathbb{R}^q$,"If $W=(W^1,...,W^q)$ is a Brownian motion in $\mathbb{R}^q$ and $\|\cdot\|$ is the Euclidean norm on $\mathbb{R}^q,$ we want to prove that $$\limsup_{u \to +\infty}\frac{\|W_u\|}{\sqrt{2u\ln(\ln(u))}}=1 \text{ a.s.}$$ An answer was given here: Law of the iterated logarithm in higher dimensions , with the property that there exists a finite $I_Ïµ \subset S^{qâˆ’1}$ for $Ïµ>0$ such that for all $x \in \mathbb{R}^q$ and for all $u \in S^{qâˆ’1},$ there exists $y \in I_Ïµ$ such that $$\langle x,u\rangle \leq (1+\epsilon)\langle x,y\rangle$$ The above property, why is it true? Also, is there a law for an arbitrary norm $\|\cdot\|$ ?","['stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4402684,Alternative approach to derivative,"Recall that the derivative of a function $f$ at a point $a$ is defined as the limit: $\lim_{x\to a}\frac{f(x)-f(a)}{x-a}$ , if the limit exists. Can we alternatively formulate it as $\lim_{x\to b}\frac{f(g(x))-f(h(x))}{g(x)-h(x)}$ , where $\lim_{x\to b}g(x)=\lim_{x\to b}h(x)=a$ ? Are these two statements equivalent?","['limits', 'calculus', 'derivatives', 'real-analysis']"
4402687,"A dimension-free upper bound for $\| X \|_{\infty}$ when $X \sim N(0,\Sigma)$","For a $p$ -dimensional random vector $X \sim N(0,\Sigma)$ , it is known that with high probability $$
\| X \|_{\max} \le C \sqrt{\log p}
$$ where $C$ is some constant possibly depending on $\Sigma$ . I would like to know if there is a simple dimension free bound that generalizes this result. For context on a related problem, consider the $\| \cdot \|_2$ setting, then example Exercise 6.3.5. in Vershynin's High Dimensional Probability states that for a $m \times n$ constant matrix $B$ , $Z \sim N(0,I)$ , that $$
P( \| BZ\|_{2} \ge C \| B \|_F + t) \le \exp \left ( -\frac{ct^2}{\| B \|^2_\text{op}}\right)
$$ where the subscript $F$ is the frobenius norm. Choosing $B = \sqrt{\Sigma}$ yields $$
P( \| X \|_{2} \ge C \sqrt{\text{Tr}(\Sigma)} + t) \le \exp \left ( -\frac{ct^2}{\|\Sigma\|^2_{\text{op}}}\right),
$$ so that with high probability $\| X \|_2 \le C\sqrt{\text{Tr}(\Sigma)}$ . In the worst case, $\Sigma = I$ and we get back the standard bound $\| X \|_2 \le C\sqrt{p}$ , but for many problems of interest $\Sigma$ has a nice structure that makes the first bound more useful. Is there a similar bound that generalizes the $\sqrt{\log p}$ bound? Ideally it would involve the trace, $\|X\|_{\max} \stackrel{?}{\le} C \sqrt{\log \text{Tr}(\Sigma)}$ . Note also that all results mentioned hold more generally for subGaussian random variables (with different universal constants), but for simplicity I am just looking at the Gaussian case. An update : I have found a related dimension free result, Lemma 2.3 of this paper by Van Handel states that for $X_1,\dots, X_n$ be not necessarily independent subGaussian random variables with subGaussian norm $\sigma_i$ , then $$
E [\max_{i \le n} X_i] \le C \max_{i \le n} \sigma_{(i)} \sqrt{\log(i+1)}
$$ where $\sigma_{(1)} \ge \sigma_{(2)} \ge \dots \sigma_{(n)}$ are the ordered subGaussian norms. If the $X$ 's are independent Gaussian there is a matching lower bound.","['statistics', 'concentration-of-measure', 'probability-theory', 'normal-distribution']"
4402705,Unique decomposition of square matrices theorem,"My professor mentioned a theorem for decomposing square matrices into a symmetric and anti-symmetric matrix. Paraphrasing, If $A$ is square, then $A$ has the unique decomposition $A=U+V$ , for $U$ symmetric and $V$ anti-symmetric. But I'm not quite sure how to prove this, because it seems like there should be a formula for $U$ and $V$ respectively if they are unique, but I haven't been able to find one in terms of $A$ . Where would I begin to find this?","['matrices', 'linear-algebra']"
4402730,Will differentiable functions always have a point that is tangent to a straight line?,"If $f(x,y)$ is a differentiable function defined on $\mathbb{R}^2$ to $\mathbb{R}$ , and we are given a line $ax + by = c$ , can we necessarily find a point $\left(x^{*} , \frac{c - ax^{*}}{b} \right)$ on some curve $f(x,y) = r$ (where $r$ is a constant) such that $\frac{\partial U(x^{*} , \frac{c - ax^{*}}{b})}{\partial x} = \frac{a}{b}$ ? Note that the straight line has (anti-clockwise) angle between $90^{\circ}$ and $180^{\circ}$ (both inclusive) and each point on the curve $f = \text{constant}$ has slope with (anti-clockwise) angle between $90^{\circ}$ and $180^{\circ}$ (both inclusive). I am solving a problem in which I require this. I am trying to find out if we can always find such a point. If not, under which conditions we can. By drawing graphs, I feel this is true when $f$ is strictly convex, but I can not prove that either.","['multivariable-calculus', 'calculus', 'functions']"
4402762,Probability of a Rubik's Cube being solvable in two moves.,"So, I have recently gotten into speed-cubing, and I ran into a very interesting problem. According to the World Cube Association, a cube is legal if it takes at least two moves to solve. So, I want to find the probability that any given Rubik's Cube is solvable in two moves. So, I wanted to find out the probability that you get a cube that is solvable in exactly two moves, as a figure like that could put in perspective how much luck is involved in speed-cubing. I have no idea where to start because I have no idea how probability works except for the stuff you learn in high-school and early college. Any ideas where to start?","['probability', 'rubiks-cube']"
4402814,Invariant basis number for matrix algebras,"Let $\mathbb{k}$ be a field (we can suppose $k=\mathbb{R}$ or $\mathbb{C}$ if necessary). Let $M_n(\mathbb{k})$ denote the ring of matrices with entries in $\mathbb k$ . Recall that a ring $R$ has the invariant basis number (IBN) property if any two bases of a finitely generated free $R$ -module must have the same cardinality. Question: Does $M_n(\mathbb{k})$ have the invariant basis number property? I've been able to find a result saying that any commutative $R$ has IBN. On the other hand, the simplest $R$ that I came across without IBN, namely the column finite matrices, involves infinite indexing sets. This would suggest that $M_n(\mathbb{k})$ has IBN, but I don't have a proof/reference for this fact.","['matrices', 'ring-theory', 'free-modules', 'abstract-algebra']"
4402837,$g(t)=\frac{\sqrt{3}}{2}t+\frac{1}{2}\sqrt{1-t^2}$ Find an expression for $g^{n}(t)$,"In this question, $g^2(x)$ denotes $g(g(x))$ , $g^3(x)$ denotes $g(g(g(x)))$ and so on. Q: The function $g(t)$ is defined, for $|t| \leq 1$ by $g(t)=\frac{\sqrt{3}}{2}t+\frac{1}{2}\sqrt{1-t^2}$ . Find an expression
for $g^{n}(t)$ for any positive integer $n$ . Hint: Let $t=\sin(\theta)$ I am creating this thread because I don't know how to handle the absolute value which arises from this problem. I choose to consider the positive value of the absolute value and it happened to be the correct solution my textbook was after. But I don't fully understand the problem. $g(\sin{\theta}) = \frac{\sqrt{3}}{2}\sin{\theta} + \frac{1}{2}\sqrt{1-\sin^2{\theta}} = \frac{\sqrt{3}}{2}\sin{\theta}+\frac{1}{2}|\cos{\theta}|$ Now in the following line, I choose to take the positive value of cosine and for subsequent compositions, I keep taking the positive value. But I realize this composition is not the only possible result. We could take $+-+-+-$ and this would result in a composition that is periodic? $g(\sin{\theta}) = \sin({\theta+\frac{\pi}{6}})$ $g^2(\sin{\theta}) = \sin({\theta+\frac{\pi}{3}})$ $g^3(\sin{\theta}) = \sin(\theta+\frac{\pi}{2})$ $$\therefore g^n(\sin{\theta}) = \sin(\theta+\frac{n\pi}{6}) = \sin(\sin^{-1}(t)+\frac{n\pi}{6}) $$ This can then be proven with induction. But I won't bore you with that. I am more interested in how to handle the different cases that arise according to what value you choose for $|\cos({\theta+\frac{\pi n}{6}})|$ in further compositions.","['algebra-precalculus', 'trigonometry']"
