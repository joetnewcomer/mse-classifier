question_id,title,body,tags
2475518,Need help on the Proof of Lemma 7.3.11 in Statistical Inference Casella&Berger (v2),"The Lemma states that if $f(x|\theta)$ satisfies $\frac{d}{d\theta}\mathbb{E}_\theta\left(\frac{\partial}{\partial\theta} \log(X|\theta)\right) = \int \frac{\partial}{\partial\theta} \left[\left(\frac{\partial}{\partial\theta}\log f(x|\theta)\right) f(x|\theta)\right] dx$ then $\mathbb{E}_\theta \left( \left(\frac{\partial}{\partial\theta} \log f(X|\theta)\right)^2\right)  = -\mathbb{E}_\theta \left( \frac{\partial^2}{\partial \theta^2}\log f(X|\theta)\right). $ The proof of the Lemma is left as an Exercise (7.39). what I get is $\mathbb{E}_\theta \left( \frac{\partial^2}{\partial \theta^2}\log f(X|\theta)\right) = \int \left( \frac{\partial^2}{\partial \theta^2}\log f(X|\theta)\right) \log(x|\theta) dx
$ $=\int \frac{\partial}{\partial\theta} \left[\left(\frac{\partial}{\partial\theta}\log f(x|\theta)\right) f(x|\theta)\right] dx - \int\left(\frac{\partial}{\partial\theta}\log f(x|\theta)\right)\frac{\partial}{\partial\theta} f(x|\theta) dx   $ Note that $\mathbb{E}_\theta \left( \left(\frac{\partial}{\partial\theta} \log f(X|\theta)\right)^2\right) =\int\left(\frac{\partial}{\partial\theta}\log f(x|\theta)\right)\frac{\partial}{\partial\theta} f(x|\theta) dx,$ So basically I need that $\int \frac{\partial}{\partial\theta} \left[\left(\frac{\partial}{\partial\theta}\log f(x|\theta)\right) f(x|\theta)\right] dx=0$,
for which I am unable to derive from the assumption of the Lemma. Am I missing anything? Thanks.","['statistics', 'probability', 'statistical-inference']"
2475533,Homeomorphism Between Quotient Space and $\Bbb{R}$,"Define an equivalence relation on the plane $X =\mathbb{R^2}$ as follows: $$(x_0, y_0) \sim (x_1,y_1)  \mbox{ if }  x_0 + y_0^2 = x_1 +y_1^2$$ Let $X^*$ be the corresponding quotient space. It is homeomorphic to a familiar space: what is it? Let $g : \mathbb{R}^2 \to \mathbb{R}$ be defined by $g(x,y) := x + y^2$ . Then certainly it is continuous; moreoever, $\mathbb{R} \ni x = g(x,0)$ which shows that $g$ is surjective. Letting $[(x,y)]$ denotes the equivalence classes associated to the equivalence relation already defined, given $a \in \mathbb{R}$ , $a = w + z^2$ for some $w,z \in \mathbb{R}$ we see that $g^{-1}(\{w+z^2\}) = \{(x,y) \in \mathbb{R}^2 ~|~ g(x,y) = w + z^2 \} = \{(x,y) ~|~ x + y^2 = w + z^2 \} = [(w,z)]$ , which proves that $X^* = \{g^{-1}(\{a\}) ~|~ a \in \mathbb{R} \}$ . Therefore, there exists a continuous bijection $f : X^* \to \mathbb{R}$ such that $f$ is a homeomorphism if and only if $g$ is a quotient map, which we now prove. It suffices to show that $g$ is an open map. Given $(a,b) \times (c,d)$ , I will prove $g((a,b) \times (c,d)) = \bigcup_{y \in (c,d)} [(a,b) + y^2]$ . If $z \in g((a,b) \times (c,d))$ , the $z = g(x,y) = x + y^2$ for some $x \in (a,b)$ and $y \in (c,d)$ , which means by definition $z \in (a,b) + y^2$ . Now, if $z \in \bigcup_{y \in (c,d)} [(a,b) + y^2]$ , then $z \in (a,b) + y^2$ for some $y \in (c,d)$ ; furthermore, there is some $x \in (a,b)$ such that $z = x + y^2 = g(x,y)$ and hence $x \in g((a,b) \times (c,d))$ . Since $(a,b)$ is open and addition is a homeomorphism, each $(a,b) + y^2$ is open and hence $g((a,b) \times (c,d))$ is a union of open sets. How does this sound? I couldn't really find any full solutions online, so I am looking for a critique of my proof. EDIT: Here is an alternative proof, based on Hurkyl's suggestion. For convenience, let $[x,y]$ denote the equivalence classes instead of $[(x,y)]$ . First note that $x+y^2 + 0 = x + y^2$ which implies $[x+y^2,0] = [x,y]$ . One thing I left out above is the fact that $f \circ p = g$ , where $p : X \to X^*$ defined by $p(x,y) = [x,y]$ is the continuous surjection that projects points to the equivalence class to which they belong; this means that $f$ is given explicitly by $f[x,y] = x + y^2$ . Now define $k : \Bbb{R} \to X^*$ by $k(x)=[x,0]$ . Then $f(k(x)) = f([x,0])=x + 0^2 = x$ and $k(f[x,y]) = k(x+y^2)=[x+y^2,0]=[x,y]$ , so that $k =f^{-1}$ . But clearly $k$ is equal to the composition $p \circ h$ of continuous functions. Hence $f$ is a homeomorphism.","['general-topology', 'quotient-spaces']"
2475534,Arithmetic-geometric series which includes Fibonacci,"In connection with a problem I'm solving, I seem to be getting the series $$S = 4 \cdot \frac{F_1}{4}+5 \cdot \frac{F_2}{8}+6 \cdot \frac{F_3}{16}+7 \cdot \frac{F_4}{32}+ \cdots$$
where $F_i$ are the Fibonacci numbers. How does one solve this? I can see here that the Fibonacci sequence is related to a geometric sequence, but I'm not sure how to incorporate that.","['arithmetic-geometry', 'fibonacci-numbers', 'sequences-and-series']"
2475544,Rotation number of composition of orientation-preserving homeomorphisms.,"Let $f, g: \mathbb{S}^1 \rightarrow \mathbb{S}^1$ be orientation-preserving homeomorphisms. Show that if $f \circ g = g \circ f$, then $\rho(f \circ g) = \rho(f) + \rho(g)$ mod $1$. My attempt: Let $F \circ G$ be the lift of $f \circ g$. Then $\rho(F \circ G) = \lim_{n \to \infty} \frac{(F\circ G)^n(x) - x}{n} = \lim_{n \to \infty} \frac{(F^n\circ G^n)(x) - x}{n} = \lim_{n \to \infty} \frac{(F^n(G^n(x)) - x}{n} = \lim_{n \to \infty} \frac{(F^n(G^n(x)) - G^n(x) + G^n(x) - x}{n} = \lim_{n \to \infty} \frac{(F^n(G^n(x)) - G^n(x)}{n} + \lim_{n \to \infty} \frac{G^n(x) - x}{n} = \rho(F) + \rho(G)$ and by the definition of rotation number of lifts, the desired equality holds.","['general-topology', 'dynamical-systems']"
2475552,Proof problem explanation,"Let $f$ be a function defined at $x$. Suppose that every sequence $p_1, p_2, p_3,\dots$ in the domain of $f$ converging to $x$ has the property that $f(p_1), f(p_2), f(p_3),\dots$ converges to $f(x)$. Prove that $f$ is continuous at $x$ by contradiction. Proof: Suppose $f$ is not continuous at $a$. If $f$ is not continuous at $a$ then $a$ cannot be an isolated point, since every function is continuous at an isolated point of its domain. If $f$ is not continuous there is some epsilon, for which no matter how what $\delta$ we choose there is a point $x_n \in S$ with $ \lVert f(x_n) − f(a)\rVert \geq \epsilon$. So let’s take
$\delta = 1/n$ and $x_n \in  S, \lVert x_n − a\rVert < 1/n$, $\lVert f(x_n) − f(a)\rVert \geq \epsilon$. Then $x_n \to a$ but $f(x_n) \to f(a)$. Hence some sequence of points $x_n$ converges to $a$ but $f(x_n)$ does not converge to $f(a)$ So a few questions for this. Can I use any arbitrary letter instead of epsilon? like $M$? Also can anyone explain in better detail the part after If $f$ is not continuous there is some $\epsilon$. I am confused from that point on.",['analysis']
2475553,Uniformly continuous with unbounded derivative,"A bounded derivative is a sufficient condition for uniform continuity, but not necessary. I know the counterexample $f(x) = \sqrt{x}$ on the interval $[0,\infty)$ where the derivative is unbounded at $0$, but the function is uniformly continuous. Is there an example where $f$ is uniformly continuous and $f'(x)$ is unbounded as $x \to \infty$ but bounded on any compact interval?","['uniform-continuity', 'real-analysis']"
2475591,What is the 3rd Derivative of Cos(x) using this Derivative Formula?,"There is a general formula for the derivative of a function: $$\frac{d^n}{dx^n}f(x)=\lim_{\epsilon\to0}\frac{1}{\epsilon^n}\sum_{j=0}^n{((-1)^j\frac{\Gamma(n+1)}{j!\Gamma{(n+1-j)}}f(x-j\epsilon))}$$ Where $\Gamma(x) $ is the Gamma function I tried using the formula to evaluate the 3rd derivative of $\cos(x)$, but I get confused quickly. It would be very appreciated if someone could show a step by step solution to this problem. I'm totally aware the answer is $\sin(x)$, but what's the process to get to that solution?","['derivatives', 'summation', 'calculus', 'limits']"
2475643,Uniform distribution on $\ell_1$ ball,"I am trying to show that a random vector that is uniformly distributed on the $\ell_1$ ball (or a scaled version of the $\ell_1$ ball) in $\mathbb{R}^n$ is isotropic (i.e. $\mathbb{E}X X^T = Id$ ).  I can show that the off-diagonal terms are zero since $(x_1, \cdots, x_i, \dots, x_n)$ and $(x_1, \cdots, -x_i, \dots, x_n)$ are identically distributed so $\mathbb{E} x_i x_j  = - \mathbb{E} x_i x_j$ implies that $\mathbb{E} x_i x_j = 0$ for $i \neq j$ . Additionally, I would like to show that $X$ is not subgaussian where we define subgaussian to mean there exists a constant $C$ such that $\mathbb{E} e^{X^2/C^2} \leq 2$ . Source : Vershynin, High Dimensional Probability , Exercise 3.4.9.","['probability', 'probability-distributions']"
2475661,"Freshman calculus question in Hartshorne, chapter V.","In exercise 4.10, we are asked to prove an identity using ""freshman calculus"" I have tried combining the given inequalities in order to arrive at the desired identity, and all things have failed. My question is: what function should be studied here. And also it would be more beneficial if you could explain ""why"" this specific choice.","['algebraic-geometry', 'calculus']"
2475668,Show that $\int_{0}^{1}\ln\left({1+x\over 1-x}\right){\mathrm dx\over 1+x^2}=C$,"How do we show $(1)$ is correct?
$$\int_{0}^{1}\ln\left({1+x\over 1-x}\right){\mathrm dx\over 1+x^2}=C\tag1$$ Where C is the Catalan constant $u={1+x\over 1-x}$ then $du={2dx\over (1-x)^2}$ $${1\over 2}\int_{1}^{\infty}(1-x)^2\ln(u){\mathrm du\over 1+x^2}=C\tag2$$ $x={u-1\over u+1}$ $${1\over 2}\int_{1}^{\infty}{4\over (u+1)^2}\ln(u){\mathrm (u+1)^2du\over 2(u^2+1)}=C\tag3$$ $$\int_{1}^{\infty}\ln(u){\mathrm du\over 1+u^2}=C\tag4$$","['catalans-constant', 'real-analysis', 'closed-form', 'integration', 'analysis']"
2475715,ODE with modulus,"I want to solve the following ODE: $$y''=|y|$$ However I don't really know how to deal with this modulus! I tried to mess with the particular cases for $y>0$ and $y<0$ which have simple solutions. Well, of course if $y\geq 0$ then $y(x)=Ae^x+Be^{−x}$ is an solution and if $y<0$ $y(x)=C\cos x+D\sin x$ is an solution. But I couldn't find constraints on these constants and neither could I find a general solution I would appreciate any help.",['ordinary-differential-equations']
2475765,Showing a function is constant given $f_{xx} + f_{yy} \geq 0$ on the domain.,"Saw this question when looking through an old vector calculus text and wasn't able to find a reasonably formal way of proving the following assertion: Let $f \in C^2$,  $f:U \to \mathbb{R}$ where $U$ is an open subset of $\mathbb{R}^2$ and suppose that:
$$ \frac{\partial^2  f }{\partial x^2} + \frac{\partial ^2 f}{\partial y^2} \geq 0 \qquad \forall (x,y) \in U $$
Prove that if $f$ has a local maximum at a point $(a,b) \in U$, then $f(x,y)=constant$.
While I figure one can argue this geometrically, I am not sure what tools to use to make prove the result. Thanks.","['multivariable-calculus', 'real-analysis']"
2475772,Solve the differential equations:,"$$y' = {y \over {\sin x}}+\tan{x \over 2}$$ I was trying to do this by substitution $u=y/x $ and it did not work and also with
$$y' - {y \over {\sin x}} = 0$$
$${dy \over dx} = {y \over {\sin x}} $$
$${\ln(y) = \ln\left|\tan\frac{x}{2}\right|+c } $$
$${y = c\cdot\tan\frac{x}{2} } $$
but then when im trying to calculate $y'$ I have a problem and I have too many equations. Is there some easier way or am I making some mistakes.",['ordinary-differential-equations']
2475773,Solve the differential equation: $ y' + y\cos x = \frac{1}{2} \sin2x$,"I tried doing: $$ y' + y \cos x = 0 $$ $$ \frac{dy}{dx} = -y \cos x$$ $$ -\int \frac{1}{y}\,dy = \int \cos x \,dx $$ $$ -\ln y = \sin x +C $$ $$ y = \frac {1}{c(x) e^{\sin x}} $$ Now I should calculate $y'$ and insert $y$ and $y'$ to the $ y' + y\cos x = \frac{1}{2} \sin2x$
When I try to do this:
$$ \sin x \cos x = \frac{-c'(x)}{c^2(x)^{\sin x}} $$ What should I do next?",['ordinary-differential-equations']
2475795,"Using the Normal Approximation to the Poisson Distribution, where appropiate, If X ~ Po(30) find P(X=<31)",P(X =< 31) Z = X - μ/σ = 31-30/30^1/2 = 0.183 -> Φ(0.183) = 0.5726 The Answer is 0.608,"['statistics', 'poisson-distribution', 'normal-distribution']"
2475837,When does a limit of a sequence equal both its lim sup and lim inf?,"Suppose $(a_n)_{n\ge1}$ is a positive (so non-zero), real sequence with $\lim_{n\rightarrow\infty}a_n=L$, where $L\in[0,\infty]$. Is this equivalent with $\limsup_{n\rightarrow\infty}a_n=\liminf_{n\rightarrow\infty}a_n=L$? In particular, does this hold for the infinity case? If so, why? Thank you!","['real-analysis', 'limsup-and-liminf', 'sequences-and-series', 'limits']"
2475842,"Moral justification for ""sheaf=continuously variable set"" and local injectivity","From the topos theory perspective it is a general motto that sheaves are continuously variable sets. The internal logic of sheaf toposes justifies this motto, but I would like an additional ""local"" justification from the bundle picture. It's known the category of sheaves on a topological space is equivalent to the category of local homeomorphisms to $X$ , so it seems a local homeomorphism to $X$ captures the correct notion of a ""set continuously varying over $X$ "". I don't understand ""why"" the variation of the fibers of a local homeomorphism deserves to be identified with ""continuous variation"". Intuitively, I feel a continuously variable set might just be a continuous open map (see below) with discrete fibers (to ensure they are sets and not cohesive spaces). As this answer points out, these properties do not imply local homeomorphy since e.g $z\mapsto z^2$ as a function $\mathbb C\to \mathbb C$ satisfies them all but is not homeomorphic about zero. In addition to continuity and openness, local injectivity is needed (it implies discreteness of fibers). As a simpler example, the function $\mathbb R\to [0,\infty)$ defined by $x\mapsto x^2$ is continuous and open with discrete fibers, and the variation of fibers seems continuous to me. I would like to understand why the ""collapse of fibers"" at zero should be viewed as pathological/discontinuous. Before comparing the variation of fibers at zero with that of a local homeo to $[0,\infty)$ , here's my intuition for the properties involved. So a function $f:X\to Y$ is a local homeomorphism iff it's a continuous open local injection. Continuity means close points have close images. Openness is equivalent to $C\mapsto \forall_f(C)= \left\{ y\in Y\mid f^{-1}(y)\subset C \right\}$ preserves closed sets. I interpret this as saying that a net of fibers always converges ""onto"" a limit fiber. Local injectivity means "" unramified "" (in $\mathsf{Top}$ it is equivalent to having an open diagonal). For a concrete comparison of local homeomorphism vs continuous open map with discrete fibers consider: $x\mapsto x^2$ as a function $\begin{smallmatrix}\mathbb R\\
\downarrow\\
[0,\infty)
\end{smallmatrix}$ . Let $U\subset X$ be an open subset and consider the local homeomorphism $\begin{smallmatrix}X\amalg U\\
\downarrow\\
X
\end{smallmatrix}$ defined by component-wise inclusion. Particularly consider $\begin{smallmatrix}[0,\infty)\amalg (0,1)\\
\downarrow\\
[0,\infty)
\end{smallmatrix}$ . I have tried to draw the variation of fibers below. Question. Why should I think of the upper drawing, where the double fibers ""collapse"" into a point"" as ""discontinuous variation""?","['intuition', 'general-topology', 'sheaf-theory', 'soft-question']"
2475872,Let $E$ be a Banach Space. The set of all continuous linear transformations with inverse also continuous is open,"Let $E$ be a Banach Space. Prove that the set of all continuous linear transformations with inverse also continuous is open in $\mathcal{L}(E,E)$ (the set of continuous transformations from $E$ to $E$). Consider the norm $\|T\| = \mbox{sup}\{\|T(x)\|:x\in E, \|x\|=1\}$. The only fact that i previous know is that if $T$ is linear and $\|T\|<1$, then $I-T$ does have continuous inverse, but i can't really apply this fact. Any help or hint?",['functional-analysis']
2475893,Applying vacuous truths to real-world implications [duplicate],"This question already has answers here : If both $P$ and $Q$ are true , how can I tell that $P$ implies $Q$? (5 answers) Closed 7 months ago . The community reviewed whether to reopen this question 13 days ago and left it closed: Original close reason(s) were not resolved Here is an implication that confuses me when I think about it: $\qquad$ I am holding a pen $\implies$ It is raining outside. This implication seems to say that it will rain outside whenever I hold a pen. If I am not holding a pen, the implication is true. But how can this be so if I can just hold a pen and see that it does not rain? My guess is that implications can be true sometimes and false sometimes, so me holding a pen and seeing it does not rain does not prove that the implication is always false. But if this is the case, what does it even mean for the implication to be true, for the times when I do not hold a pen? $$$$ I can see that the implication $\qquad$ I am holding a pen $\implies$ False , would be true when I do not hold a pen, even though its consequence is never true. And anytime I hold a pen, the consequence in the implication will be false.","['predicate-logic', 'propositional-calculus', 'quantifiers', 'logic', 'discrete-mathematics']"
2475930,Characterization for the convergence in distribution,"Let $(E,d)$ be a metric space such that the convergence is characterized by the following equivalence \begin{equation}
x_n\to x\qquad\text{ if and only if }\qquad f(x_n)\to f(x) \text{ for all }f\in T, \tag{1}
\end{equation} where $T$ is a given family of function from $E$ to $\mathbb{R}$, called ""testing functions."" Now, let $X_n$ a sequence of random variable with values in the metric space $E$. Is there a relation between the convergence in distribution of the sequence $X_n$ and the previous equivalence (1)? In particolare we know that $X_n\to X$ in distribution if and only if $$\mathbb{E}[g(X_n)]\to \mathbb{E}[g(X)]\text{ for all }g\in\mathcal{C}_b(E).\tag{2}$$ How can we ""simplify"" (2) using (1)?","['functional-analysis', 'real-analysis', 'probability', 'measure-theory']"
2475936,Defining $N$ in the $\epsilon$-$N$ definiton of convergence,"In my Real Analysis class we've been spending some time talking about the $\epsilon$-$N$ definition of convergence. The book we are using, Elementary Analysis by Ross, defines convergence as: A sequence of real numbers $(s_n)$ is said to converge to $s$ if For all $\epsilon>0$, there exists $N\in\mathbb{N}$ such that for all $n\geq N$, $|s_n-s|<\epsilon$. So he defines $N$ to be a natural number, yet in all the examples of convergence, for example proving that $\lim_{n\to\infty}1/n^2=0$, he says let $N=1/\sqrt{\epsilon}$. My professor told us that it does not technically matter, so why do we limit ourselves in the definition to the naturals? Wouldn't it just be easier to say there exists an $N$ in the reals? And if we define $N$ to be in the naturals, yet say let $N=1/\sqrt{\epsilon}$, which is clearly not a natural number, why even mention $N$ being a natural number at all?","['real-analysis', 'real-numbers', 'convergence-divergence', 'analysis']"
2475941,"How long can the prime chain $a_1=p$ , $a_{n+1}=a_n^2+4$ be at most?","We start with a prime number $a_1=p$ and iterate $a_{n+1}=a_n^2+4$ until the next number is composite , for example starting with $3$ gives $3$ , $13$ , $173$ . Here the sequence terminates because the next number, is $29933=37\cdot 809$. Can such a prime chain be arbitary long ? If no, how many terms can it contain at most ? The number $\color \red {306167}$ gives a sequence with $5$ entries. If we start with a prime below $10^8$, this is the maximum possible length.","['number-theory', 'recurrence-relations', 'prime-numbers']"
2476029,How many ways to arrange $2n$ people in two lines,"There are $2n$ people, each with a unique height. I am to place them in two rows of $n$ people in each row such that the person in the front row is shorter than the one directly behind. How many ways can I do this? I am assuming that order of how the people are placed matters (for example, row one: 1,2,3 is different from 2,3,1). I'm thinking about this problem as the number of ways to place the $2n$ people in pairs, such that the order of picking pairs matters. Thus, my answer should be 
$${2n \choose 2} \cdot {2n-2 \choose 2}\cdot \cdots\cdot {2 \choose 2}.$$ Is this method of thinking correct? I've tried to see if this works for $n=1,2$ and it seems to hold up.",['combinatorics']
2476109,Finding a sequence $a_n$ such that $\sum a_n$ converges but $\sum a_n^3$ diverges. [duplicate],"This question already has answers here : General infinite series convergence or divergence. [duplicate] (3 answers) Closed 6 years ago . This is an exercise that was given to me, and I would like to emphasize that I am not looking for an answer, but rather a pointer or a hint in the right direction, since it appears I've reached a bit of a dead end. We are looking at sequences over the real numbers (at least that's my assumption, and that's where I've been looking). I've come with various results to help me try such a sequence: The ratio test has to be inconclusive, as does the root test. Since $a_n$ converges to $0$, so does $a_n^3$ The sequences $|a_n|$ cannot be monotone decreasing, otherwise $\sum a_n^3$ will converge. Other various things I've noticed: It's very easy to find $a_n$ such that $a_n$ converges but $a_n^2$ diverges. Is it possible to use that fact? Let $u_n$ be such a sequence. I tried to construct a sum of two sequences so that when we take the third power, $u_n^2$ appears in the binomial expansion. That ""naive"" approach didn't get me too far however. We can make the following observation: 
\begin{align}
\sum a_n^3 &= a_1^3 + a_2^3+a_3^3+a_4^3\cdots \\
& = (a_1+a_2)((a_1-a_2)^2 + a_1a_2) + (a_3+a_4)((a_3-a_4)^2 + a_3a_4) +\cdots \end{align}
This hasn't taken me anywhere. Other things I've thought about: I think a natural form for the sequence would be the product of two functions, $f$ and $g$, such that one converges to $0$ much faster than the other. By using the comparison test, we could try to require that $(fg)^3 > k_n$ for all n for some sequence $k_n$ whose associated series is divergent. I think it is unlikely we could find such a product though, since if $fg$ plummets fast enough, then $(fg)^3$ will plummet even faster. I've tried a few things for lack of better options, using exp, trig functions, polynomials, none of my tries have worked so far, almost always as a product of two functions. If someone could suggest a possible step forward, I would be very grateful!","['sequences-and-series', 'analysis']"
2476126,Is $\sum\limits_{i=1}^{+\infty}\frac {1}{q_i+1}$ irrational?,I cannot at the moment find a way for this one: Does there exist strictly increasing sequence of primes $q_i$ so that we have $$\sum_{i=1}^{+\infty}\dfrac {1}{q_i+1} \in \mathbb Q$$ I am struggling with something a slightly less general but decided to ask it in this form.,"['prime-numbers', 'sequences-and-series', 'rational-numbers']"
2476231,Mathematical Entity That Takes a Function and Returns a Function,"I have been pondering something lately and asked myself whether there is a mathematical construct that takes a function as its parameter and returns another function as its result? That is, it is a function of a function that produces a function. Is there a term for this? What are some basic facts and where can I learn more about such structures?","['functional-analysis', 'abstract-algebra', 'analysis', 'functions']"
2476233,What's the Derivative of Order $\frac{1}{3}$ of $f(x)=x$?,"So, I'm very new to Fractional Calculus, and I don't know too much about series, but I'm very interested in Fractional Calculus, so I wanted to know if you could find the solution to this problem. Using the general formula for derivatives: $$\frac{d^n}{dx^n}f(x)=\lim_{\epsilon \to 0} \frac{\Gamma(n+1)}{\epsilon^n}\sum_{j=0}^{\lfloor\frac{x}{\epsilon}\rfloor}\frac{f(x-j\epsilon)(-1)^j}{j!\Gamma(n+1-j)}$$ Where $\Gamma(x)$ is the gamma function. The upper limit to the summation (because it's a little hard to see) is $\lfloor{\frac{x}{\epsilon}\rfloor}$ , which is the floor function.
I want to find: $$\frac{d^{\frac{1}{3}}}{dx^{\frac{1}{3}}}(x)$$ The place where I get stuck is interpreting the upper limit of the summation. Am I supposed to treat it as an infinite series since it has a variable? The place I got this formula is: Link w/ Formula","['derivatives', 'fractional-calculus', 'summation', 'calculus']"
2476264,A General Process for Finding Correct Bounds on Double Integral?,"So I'm working on some multivariable calculus homework, and I can't seem to figure out why my professor takes this particular approach to the solution... The Question:
$$
S = \{(x,y) \in R^2: 0 \leq x \leq 1, 0 \leq y \leq sin^{-1}x\}
$$ 
And we have to evaluate $\int \int_{S} dA $ My professor's approach to this problem involves changing the integral bounds, so instead of the double integral setup looking like: $\int_{0}^{1} \int_{0}^{sin^{-1}}dydx$, it looks like $\int_{0}^{\pi/2} \int_{sin(y)}^{1}dxdy$ Can someone please explain how he got to this rearranged integral bounds setup, and additionally is there a general process for rewriting the integral bounds for a double integral?","['multivariable-calculus', 'multiple-integral', 'integration', 'calculus']"
2476338,Conjectured continued fraction for the Generalized Rogers-Ramanujan continued fraction,"Given the following Generalized Rogers-Ramanujan continued fraction, with $|q|\lt1$ , which is equation (38) in mathworld $F(a,q)=1-\cfrac{aq}{1-\cfrac{aq^2}{1-\cfrac{aq^3}{1-\cfrac{aq^4}{1-\cfrac{aq^{5}}{1-\cfrac{aq^{6}}{1-\dots}}}}}}\tag1$ where $F(a,q)= \frac{\displaystyle \sum_{n=0}^{\infty}\frac{(-a)^n q^{n^2}}{(q)_{n}}}{\displaystyle\sum_{n=0}^{\infty}\frac{(-a)^n q^{n(n+1)}}{(q)_{n}}}\tag2$ it is conjectured that it is equivalent to the following continued fraction $H(a,q)= \cfrac{1}{1+\cfrac{aq}{1-\cfrac{aq}{1-\cfrac{q}{1+\cfrac{q}{1+\cfrac{aq^2}{1-\cfrac{aq^{2}}{1-\cfrac{q^{2}}{1+\cfrac{q^{2}}{1+\cfrac{aq^3}{1-\cfrac{aq^3}{1-\dots}}}}}}}}}}}\tag3$ How do we prove $F(a,q)\overset{\color{red}?}=H(a,q)$ Remark If we let $a=-1$ and take the reciprocal of $(3)$ , we have the rogers-ramanujan continued fraction formula first conjectured by R. Blecksmith and J. Brillhart and proved in On the generalized rogers-ramanujan continued fraction by Bruce C. Berndt and AE JA YEE","['number-theory', 'q-series', 'conjectures', 'continued-fractions']"
2476342,Some alternating series converging to values relating to $\pi$.,"The following series converge to a value relating to $\pi$:
\begin{align}
\frac{1}{1}-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots&=\frac{\pi}{4},\\
\frac{1}{1^2}+\frac{1}{3^2}+\frac{1}{5^2}+\frac{1}{7^2}+\cdots&=\frac{\pi^2}{8},\\
\frac{1}{1^3}-\frac{1}{3^3}+\frac{1}{5^3}-\frac{1}{7^3}+\cdots&=\frac{\pi^3}{32},\\
\frac{1}{1^4}+\frac{1}{3^4}+\frac{1}{5^4}+\frac{1}{7^4}+\cdots&=\frac{\pi^4}{96},\\
\frac{1}{1^5}-\frac{1}{3^5}+\frac{1}{5^5}-\frac{1}{7^5}+\cdots&=\frac{5\pi^5}{1536}.
\end{align} It seems that if we define $$f(n):=\sum_{i=0}^{\infty}\Big(\frac{(-1)^i}{2i+1}\Big)^n,\quad n\in\mathbb{N}_+,$$ then the values of $f$ are related to $\pi$, and in fact I guess we have $$f(n)=A\pi^n,\quad A\in\mathbb{Q}.$$ This is strongly reminiscent of Basel problem, where we have a famous solution based on the Weierstrass factorization theorem. Trying to imitate that proof, I want to find a real function $g$ with $$Z:=g^{-1}(0)=\Big\{\frac{(-1)^i}{2i+1}:i\in\mathbb{N}\Big\},$$ and $g$ can be factorized as something like $$g(x)=\prod_{a\in Z}\Big(1-\frac{x}{a}\Big),$$ and by comparing the Taylor series of $g$ and applying Vieta's formulas and Newton's identities , we might find the value of $f(1)$ or even more. But these are just wild guesses. I haven't even studied complex analysis, and I'm only imitating the proof for Basel problem. I wonder if this leads to any reasonable solution. My question is: How do we obtain the value of $f(n)$ and how do we prove these? Don't hesitate to post solutions based on complex analysis or more advanced analysis! Thanks in advance.","['real-analysis', 'complex-analysis', 'pi', 'sequences-and-series', 'analysis']"
2476351,mystery wave: what to call it?,"I recently stumbled on a useful periodic waveform, but I don't know what to call it. It's drawn in blue in the enclosed image, with a sine wave drawn in red for reference. Do you know what to properly call this wave? The most efficient way I've found to generate the wave is by offsetting each quadrant of a sine wave, as in the following C code. The even quadrants are displaced, while the odd quadrants are inverted and displaced. double MysteryWave(double fPhase)
{
  double r = fmod(fPhase + 0.25, 1);
  double s = sin(r * PI * 2);
  if (r < 0.25)
    return s - 1;
  else if (r < 0.5)
    return 1 - s;
  else if (r < 0.75)
    return s + 1;
  else
    return -1 - s;
} A wave that is very similar but NOT identical to my mystery wave can be generated via the arcsine of the cubed sine, rescaled to $[-1..1]$ by dividing by $\pi / 2$. In other words the following function is close but not quite the same: 2 * asin(pow(sin(fPhase * PI * 2), 3)) / PI","['periodic-functions', 'trigonometry']"
2476359,"Is there an example of $F_{\sigma}$ set in $[0,1]$ with empty interior and with measure $1$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Is there an example of  $F_{\sigma}$ set in $[0,1]$ with empty interior and with measure $1$? I wonder if it exists.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2476389,Sum of sixth power of roots of $x^3-x-1=0$,"Question: Find the sum of sixth power of roots of the equation $x^3-x-1=0$ My First approach: Let $S_i$ denote the sum of $i^{th}$ power of roots of the given equation. Now, multiplying given equation by $x^3$ , putting each of the three roots and adding the three formed equations we get, $S_6=S_4+S_3$ repeating same above procedure to obtain, $S_4=S_2+S_1$ , $S_3=S_1+1$ hence, $S_6=S_2+2S_1+1=2+0+1=3$ My Second Approach: Let $a,b,c$ be the roots of $f(x)=x^3-x-1$ Clearly, $f^/(x)/f(x)=1/(x-a)+1/(x-b)
+1/(x-c)$ =$\sum(1/x+a/x^2+a^2/x^3+a^3/x^4+\cdots)=3/x+S_1/x^2+S_2/x^3+\cdots$ hence we get, $S_6=5$ $\rule{200px}{0.5px}$ I am getting wrong answer through First Approach, Please point out my mistake or post a better solution. Thank You","['algebra-precalculus', 'polynomials', 'proof-verification']"
2476411,Calculate the Reverse of a recursion function,"I have a recursion function as follows: $$R(0) = 1$$ $$R(1) = 1$$ $$R(2) = 2$$ $$R(2n) = R(n) + R(n + 1) + n ,\qquad \forall n \gt 1$$ $$R(2n + 1) = R(n - 1) + R(n) + 1 ,\quad \forall n\ge 1$$ Now I want to find the reverse of this function. I know that first of all, I should solve the relation then after finding the function calculate the reverse. But the problem is that since the relations are different for even and odd numbers I cant figure out how to find a unique solution or even when I try to solve them separately, since the variables have coefficient it seems complicated. For more clarification, here is an example, of how you can find the inverse of the Tower of Hanoi: $$H_n = 2H_{n−1} + 1$$
$$= 2(2H_{n−2} + 1) + 1 = 2^2 H_{n−2} + 2 + 1$$
$$= 2^2 (2H_{n−3} + 1) + 2 + 1 = 2^3 H_{n−3} + 2^2 + 2 + 1$$
$$.$$
$$.$$
$$= 2^{n−1} H_1 + 2^{n−2} + 2^{n−3} + · · · + 2 + 1$$
$$= 2^{n−1} + 2^{n−2} + · · · + 2 + 1$$
$$= 2^{n} − 1.$$ Now the inverse is $$\log_2(F - 1)$$","['recursion', 'discrete-mathematics']"
2476435,Prove that $(A \subseteq B) \land (A\subseteq C) \Rightarrow A \subseteq (B \cap C)$,"Prove that ${{(A\subseteq B)}\cap{(A\subseteq C)}}$$ \Rightarrow A \subseteq (B \cap C)$ I want to write a formal proof of this but I have a few doubts: (1) Do we have to take into consideration the scenario that the left-hand side of the implication is false? Then the sentence as a whole would be true. (2) Is it necessary to write this sentence in terms of propositions and then resort to a bunch of tautologies to prove the right-hand side of the sentence? Can we simply assume that the left-hand side is true and so write $x \in B$, $x \in A$, $x \in C$ and conclude that $x \in B \land x\in C \rightarrow x\in B\cap C$ and $x \in A$ and if the conjunction of two propositions holds, then their implication holds as well?",['elementary-set-theory']
2476469,"Taking into account symmetry, how many possible games of tic-tac-toe are there?","Same as this other question ( Game combinations of tic-tac-toe ), but taking into account symmetry.",['combinatorics']
2476484,Prove that $\frac{p^p-1}{p-1}$ is not prime if $p \equiv 1 \pmod 4$,"Let $p$ be a prime number such that $p \equiv 1 \pmod 4$. Prove that  $\frac{p^p-1}{p-1}$ is not prime. We can rewrite $\frac{p^p-1}{p-1}$ as $$\dfrac{p^p-1}{p-1} = 1+p+p^2+\cdots+p^{p-1},$$but how do we show this is not prime?",['number-theory']
2476487,Understanding Independence as measure specific,"I'm trying to understand the definition of independent events or sets in a sigma algebra on a probability space to be something determined by the probability measure the space is endowed with. However, I have been unable to come up with a good enough example to illustrate this to myself. For example, would it be possible to have three events $A, B, C$ such that when considered with respect to probability measure $\mu, C$ is independent of $A$ and $C$ independent of $B$ but $C$ not independent of $A \cup B$? However, when considered with respect to some alternative probability measure $\nu$, could we have a situation where each of the events $A$, $B$ and $C$ has strictly positive probability with $\nu$ but are mutually independent?","['independence', 'probability-theory', 'probability', 'measure-theory']"
2476501,"What can be said about the set of points that makes the MVT ""works""?","Consider a (real-valued) function $f$ that is differentiable on an open interval $I$ of $\mathbb R$. For $a\neq b$ in $\mathbb R$, we consider $S_{a,b}=\left\{ x\in I\; \middle|\; f'(x)=\frac{f(b)-f(a)}{b-a}\;  \right\}$. $S_{a,b}$ is non empty by the Mean Value Theorem. Let now $\displaystyle S=\bigcup_{a\neq b \in I}S_{a,b}$ What can be said about $S$ ? Has this set been studied before? Is $S$ an interval? Is $S$ open? Clearly, $S$ is a subset of $I$ but the inclusion can be strict. For example, if $f(x)=x^3$, we have $0\notin S$.  This example also shows that $S$ is not always an interval. Note: feel free to add conditions on $f$ if it allows $S$ to have nice properties.","['reference-request', 'general-topology', 'calculus']"
2476527,Variance of Sample Variance,"Given $X_1,...,X_n$ iid to a certain distribution (not necessarily normal), with $\mathbb{E}(X_i)=\mu$ and $\mathbb{V}(X_i)=\sigma^2$ , I'm trying to deduce the standard and mean squared error of the estimator $\widehat{\sigma}^2=S_n^2$ , where $S_n^2$ is the sample variance, given by $$
S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X}_n)^2.
$$ To do so I need its variance, $\mathbb{V}(S_n^2)$ . Since I know the expectation $\mathbb{E}(S_n^2)=\sigma^2$ , I started by expanding $$
\mathbb{V}(S_n^2)=\mathbb{E}(S_n^4)-(\mathbb{E}(S_n^2))^2=\mathbb{E}(S_n^4)-\sigma^4
$$ but I'm stuck with the expansion of the term $\mathbb{E}(S_n^4)$ . Any ideas? P.S. - I saw user940 's answer on this question , but I was looking for a different approach, also not assuming normal distributed random variables.","['statistics', 'probability', 'statistical-inference']"
2476544,Extending numerical Euler method to higher order differential equations,"If it was a second order DE I can follow the logic however I get very confused when trying to numerically solve a third order DE. I have to estimate  $y(0.1)$ using step size 0.1 $$xy''' + xy'' + x^2y' + y = x,\quad with \quad y(0) = 1, y'(0) = 0, y''(0) = 1$$ where $$y_0 = y(x_0)$$  $$y_{n +1} = y_n + hf(x_n, y_n)$$
$$x_{n +1} = x_n + h$$ Now I can set $u_1=y, u_2=y', u_3=y''  $ which gives $u_1'=y'=u_2$ and $u_2'=y''=u_3$ and $u_3'=y'''$ So I can sub these in to get $$xu_3'+xu_3+x^2u_2+y=x\tag{*}$$ but I can also get 
$$xu_3'+xu_2'+x^2u_1'+y=x\tag{**}$$ Would they both lead to the same answers? The first one does seem easier to work with. And assuming I go with the first one, I still do not know how to set up my iteration because I have no idea how to relate them. Approximate the value of $y (0.2)$ , where $y(x)$ is the solution of the
initial-value problem
$$y''+ xy' + y = 0,   y(0) = 1, y'(0) = 2$$
Substitute $u = y'$ to obtain the system
$y' = u, u ' = −xu − y$
Euler’s scheme then gives
$$y_{n +1}= y_n + hu_n$$ 
$$u_{n +1}= u_n + h( −x_nu_n − y_n)$$ If we use the step-size $h = 0.1$ and $y_0 = 1$, $u_0= 2$, we obtain
$$y_1 = y_0 + (0.1)u_0 = 1 + (0.1)2 = 1.2$$
$$u_1 = u_0 + (0.1)( −x 0u 0 − y_0) = 2 + (0.1)( −1) = 1.9$$
$$y_2 = y_1 + (0.1)u_1 = 1.2 + (0.1)(1.9) = 1.39$$
$$u_2 = u_1 + (0.1)( −x_1 u_1 − y_1) = 1.9 − (0.1)1.39 = 1.761.1.9 + (0.1)$$
Thus we obtain $$y(0.2) ≈ 1.39$$ and $$y'(0.2) ≈ 1.761$$","['numerical-methods', 'ordinary-differential-equations']"
2476573,Prove that orthogonal matrix $Q$ preserves operator norm i.e. $||AQ||_2 = ||A||_2$,"This question proves that $||QA||_2 = ||A||_2$: $||QA||_2 = \sup_{||x||=1}{||QAx||}=\sup_{||x||=1}{\sqrt{(QAx)^T(QAx)}} = 
\sup_{||x||=1}{\sqrt{x^TA^TQ^TQAx|}} = sup_{||x||=1}{\sqrt{x^TA^TAx}} = ||A||_2$ How to prove it for $||AQ||_2$ too?","['matrices', 'normed-spaces', 'orthogonal-matrices', 'orthogonality']"
2476637,"Prove if $p + \sqrt{2} \cdot q = 0$, then $p = q = 0$","Let K be the set $ K := \{p + \sqrt{2} . q : p, q \in \mathbb{Q}\} $ and let addition and multiplication be defined on $\mathbb{R}$ Prove if $p + \sqrt{2} \cdot q = 0$, then $p = q = 0$ Hint: use that $ \sqrt{2}$ is irrational Logically, if $p + \sqrt{2} . q = 0$ this implies $ \sqrt{2} = \frac{-p}{q}$ which is not possible as $ \sqrt{2}$ is irrational. How do I write this as a proof though?","['real-analysis', 'irrational-numbers', 'analysis']"
2476654,An urn containing $r$ red balls and $b$ blue balls.,"Suppose an urn contains $r$ red balls and $b$ blue balls. Suppose $n$ balls are drawn sequentially without replacement. Find the probability that $k$ of the $n$ balls are blue and that the first one is blue. My try: Let $A$ be event first ball is blue and $B$ be event $k$ of $n$ balls are blue. I want to find $P(A \cap B)$ . In total we have ${r+b \choose n }$ outcomes in sample space. for the number of ways $A \cap B$ occurs we have first one is blue, and so we have $b-1$ blue balls from there we pick $k$ of them so ${b - 1 \choose k}$ and for red balls we have ${r \choose n - k}$ . Therefore $$ P(A \cap B) = \frac{ b {b - 1 \choose k} {r \choose n - k} }{{r+b \choose n } }. $$ Is this correct?","['probability', 'proof-verification']"
2476669,Finding extremal of the functional,"Find the stationary point of the functional $$
J[y]=\int \left( x^2y'^2+2y^2 \right) dx
$$
where $y(0)=0, y(1)=2.$ My Solution: E-L equation: $x^2y''+2xy'-2y=0.$ This is also Cauchy-Euler equation. Let $y(x)=x^m$. Substituting to eqn. , we get $m_1=-2, m_2=1$ and
I found the general solution $y(x)=c_1x^{-2}+c_2x$. Now, we will find $c_1,c_2.$ Since $y(1)=2$, we have $c_1+c_2=2$. But when I write $x=0$ to general solution, $0=y(x)=c_1.0^{-2}+c_20$, there is a uncertainty. Please help me.","['euler-lagrange-equation', 'optimal-control', 'variational-analysis', 'calculus-of-variations', 'ordinary-differential-equations']"
2476680,Problem on Wave Function,"A particle of mass $m$ is subjected to a force $F(r) = -\nabla V(r)$ such
that the wave function $\varphi(p, t)$ satisfies the momentum-space Schrödinger
equation $$\left(\frac{p^2}{2m}-a\Delta^2_p\right) \varphi (p,t) = i\frac{\partial \varphi (p,t)}{ \partial t}$$ where $\hbar = 1$, $a$ is some real constant and $$\Delta^2_p \equiv \frac{\partial^2 }{ \partial p^2_x} + \frac{\partial^2}{ \partial p^2_y } + \frac{\partial^2 }{\partial^2_z} \, .$$ How do we find force $F(r) \equiv -\nabla V(r)$? We know that the coordinate and momentum representations of a wave function are related by $$\\psi (r,t) = \left(\frac {1}{2\pi}\right)^{\frac {3}{2}} \int \varphi (k,t) e^{ik\cdot r} \mathrm dk \tag {1}$$ $$\varphi (k,t) = \left(\frac {1}{2\pi}\right)^{\frac {3}{2}} \int \psi (r,t) e^{-ik\cdot r} \mathrm dr \tag {2}$$ where $k \equiv p / \hbar$ with  $Ii  = 1$.","['quantum-mechanics', 'physics', 'mathematical-physics', 'integration', 'ordinary-differential-equations']"
2476687,Second-Order Nonlinear Ordinary Differential Equation - with a scalar multiple,"I studied differential equations back in the day but we never covered second-order nonlinear equations (that I can recall). I have the following equation: $$y''=2\biggl(\frac{y'^{2}}{y}-\frac{y'}{x}\biggr)$$ I tried to follow this Solving a second-order nonlinear ordinary differential equation but the ""2"" out front is throwing me off. I am not a student and this is not homework. I am fairly certain that the first-order solution to this problem will be a Bernoulli type nonlinear equation and that much I can solve. It's just this 2 out front - and getting from a second-order form to a first-order form - that are throwing me. Any help in solving for $y'$ would be greatly appreciated! P. S. This is my first post. My apologies if there are any rules that I have failed to follow. -- Edit: The form of the first-order solution should have the form $$y'(x)=P(x)y(x)+Q(x)y(x)^{2}$$ which would then be solved by the Bernoulli method. But how do we get from the second-order form to this one? I've been trying it on my own and nothing is working. Thanks. Sorry for not being more specific.",['ordinary-differential-equations']
2476755,Suppose $E(2^X)=4$. Prove that $P(X \ge 3) \le {1 \over 2}$.,"Suppose $E(2^X)=4$. Prove that $P(X \ge 3) \le {1 \over 2}$. I thought that this question could be solved by using Jensen's inequality and Markov's inequality like below... $E(2^X)=4 \ge 2^{E(X)}$ which implying
$2 \ge E(X)$ $P(X \ge 3) \le {E(X) \over 3} \le {2 \over 3}$ How I can get  $P(X \ge 3) \le {1 \over 2}$ from this??
Thank you.","['expectation', 'probability', 'jensen-inequality']"
2476848,Finding the ratio between two $8$-dimensional volumes,"EDIT: At this point, geometric interpretations of conditions 2-4 would qualify as an answer. This can include symmetries of the region. I have a real $3 \times 3$ matrix $A$ with entries $a_{ij},$ and I want to find out how much of the unit $9$-ball the $9$-dimensional volume of the region in which $A$ is stable (meaning all its eigenvalues have negative real part) takes up. However, since this region is a cone, it suffices to look at how much of the surface of the $9$-ball the region takes up, so we can look at $\sum_{i,j=1}^3 a_{ij}^2=1.$ We can prove (see below) that $A$ is stable iff $\mathrm{tr}(A)<0$ $\det(A)<0$ $\mathrm{tr}^3(A)<\mathrm{tr}(A^3)$ are all met. To be more precise, if we let $\mathcal{C}$ denote the set of all $3 \times 3$ matrices satisfying all four of the above conditions and let $\mathcal{S}^8$ denote the $8$-sphere (i.e. the surface of the unit $9$-ball), I'd like to find 
$$R=\frac{\mathrm{vol}(\mathcal{C})}{\mathrm{vol}(\mathcal{S}^8)}.$$ But how would I set up the integrals for actually finding this?
Or, better than messing with integrals, can we perhaps infer this ratio from looking at the symmetries of conditions 2-4? From simulations, $R\sim 0.1045.$ Any help is much appreciated. For instance, what would the geometric interpretations of conditions 2-4 be? Proof: (Not necessary reading) The (monic) characteristic polynomial for $A$ is 
$$p_3(z)=z^3+c_2z^2+c_1z+c_0,$$
where
\begin{align}
c_0 &= -\det A\\
c_1 &= \frac{1}{2}\left[\mathrm{tr}^2(A)-\mathrm{tr}(A^2)\right]\\
c_2 &= -\mathrm{tr}A.\\
\end{align} $A$ is stable iff the characteristic polynomial satisfies The Hurwitz Stability Criterion . Written out for the present case, it reduces to \begin{align}
\Delta_1&=c_2         && \mkern-18mu \mkern-18mu >0 \\
\Delta_2&=c_1c_2-c_0  && \mkern-18mu \mkern-18mu >0 \\
\Delta_3&=c_0\Delta_2 && \mkern-18mu \mkern-18mu >0,
\end{align} of which the two first can be written as 
$\mathrm{tr}(A)<0$ and $\mathrm{tr}^3(A)<\mathrm{tr}(A^3).$ Now, we'd like to reduce the third inequality. The idea is that we know that $\det(A)<0$ is necessary for $A$ being stable (since the determinant of a matrix is the product of its eigenvalues). This means we can divide with $c_0$ without changing the inequality, at which point the third inequality reduces to the second. Note that we're ignoring singular matrices, since their contribution to the volume is zero anyway. 
$\Box$","['eigenvalues-eigenvectors', 'matrices', 'volume', 'control-theory', 'linear-algebra']"
2476860,Prime numbers in Fermat's little theorem,"Consider prime numbers $a$ and $p$. It is known  according to Fermat's little theorem   (FLT) that the congruence $a^{p-1}  \equiv 1 \pmod {p}$  holds for any prime $a$ and $p$. From this we have that there are infinitely many numbers of  form $a^m$ which are congruent to $1$ as any power   $(a^{p-1})^k$ is congruent to $1$ if only $a^{p-1}$ is congruent. The question arises however whether a number $m=p-1$ appearing in FLT is the smallest  number (obviously excluding $a^0=1$) which satifies $a^m  \equiv 1 \pmod {p}$ ? I've made some experimental analysis taking into account powers of $a=2$ and sequence of prime numbers $p=5,7,11,13,17, 19 , \dots$. I will name prime numbers $p$ for which equality above is satisfied when there is no less number $m$ than $p-1$ that $2^m  \equiv 1 \pmod {p}$ proper FLT prime numbers - if it happens that however there is a   number $m<p-1$   when $2^m  \equiv 1 \pmod {p}$ is satisfied I will name such number $p$ improper FLT number  (if there is more official terminology let me know) and number $m$ initial number for the given $p$. Having made calculations I received following numbers $p$ for powers  of two congruent to $ 1 \pmod {p}$: Proper LFT numbers:
$5(4),11(10),13(12),19(18),29(28),37(36),53(52),59(58) \dots$ Improper LFT numbers:
$7 (3), 17 (8), 23(11), 31(5),41(20),43(14), 47(23)\dots$ Here in brackets smallest exponents $m$ (named as initial numbers) for satysfying formula $2^m \equiv 1 \pmod {p}$ are given. 
For example $2^8\equiv 1 \pmod {17}$. Evidently initial numbers    for improper FLT numbers are divisors of $p-1$ what is intuitively expected but  why exactly such value not other ?
(see $5$ for $31$, not for example $10$ or see $14$ for $43$ not $7$) it's hard to say. My question: is it any method to discern proper and improper FLT numbers? if we are able to recognize that a given prime number is FLT improper
can we also say what is an initial number associated with it?","['number-theory', 'prime-numbers', 'terminology', 'elementary-number-theory']"
2476863,Closed form of the $n^{th}$-derivative of $f(x) = \arctan x$,"My problem is about characterization of the $n^{th}$-derivative of 
$$f(x) = \arctan x$$ We were asked to solve the following questions Show that $$ f^{(n)} (x) = \frac{P_n(x)}{(1+x^2)^{n}}~~~n\ge1$$
Where $P_n$ is polynomial of degree to be specify. Find a recurrent relationships Between the $P_n's.$ Then, Give the expression of $P_n(x) . $ My attempt I was able to prove just comparison with derivative that $$ P_{n+1}(x) = (1+x^2)P'_n(x) -2nx P_n(x)$$
This therefore gives a solution to questions 1. and 2. 
Note that from this, since $P_1 = 1$ we have $$\deg P_n = n-1$$ I do not know how to solve the last Question.","['derivatives', 'real-analysis', 'polynomials', 'closed-form', 'analysis']"
2476888,Example of a 3-dimensional rationally connected variety that is not rational.,"We will call a proper variety $X$ rationally connected if any two general points of $X$ are in the image of some map $\boldsymbol{P} \to X.$ A variety is rational if it is birationally equivalent to $\boldsymbol{P}^n$ for some $n$. Any rational variety must be rationally connected. These two notions coincide exactly for varieties of dimension two or fewer, but this is not the case in higher dimensions. What is an illustrative example of a three-dimensional variety that is rationally connected, but is not rational?","['projective-space', 'algebraic-geometry', 'birational-geometry']"
2476911,Non Gaussian Additive Channel Capacity,"If I have a additive, stationary, memoryless non-Gaussian noise channel
$$
Y_i = X_i + Z_i
$$
With the fixed mean and covariance on the noise $Z_i$
$$
\mathbb{E}(Z_i) = 0,\ Var(Z_i) = 1
$$
and a covariance constraint on $X_i$
$$
\frac{1}{n}\sum x_i^2 \leq P.
$$
How can I prove the following bound on the channel capacity
$$
\frac{1}{2}\log(1+P) \leq C(P)\leq \frac{1}{2}\log(1+P) + D(P_z||\mathcal{N}(0,1))? 
$$ I think I can get the lower bound as follows. Letting $X_g \sim \mathcal{N}(0,P)$
\begin{align}
C(P) =& \max_{P_x : Var(X) \leq P} I(X;Y)\\
=& \max_{P_x : Var(X) \leq P} I(X;X+Z)\\
\geq& I(X_g; X_g + Z)\\
\geq& \min_{P_z} I(X_g;X_g+Z)\\
=& I(X_g;X_g+Z^*)\\
=& \frac{1}{2}\log(1-P)
\end{align} Because the noise distribution that minimizes mutual information is Gaussian $P_{Z^*} = \mathcal{N}(0,1)$. But I'm having trouble with the upper bound.","['information-theory', 'probability-theory']"
2476942,How can we integrate $\int 1/(e^x + e^{2x}) dx$?,"This is my first post off my phone, so please try to forgive me for the poor format. I’m attempting to take the antiderivative: $$ \int \frac{1}{e^x + e^{2x}} dx.$$ And I have no idea how to do so. I’ve only taken up to calc 3 and I can’t think of anything I’ve learned that can solve this, but there is an answer. I’m not so much interested in the answer as I am what method is used to solve this.","['integration', 'calculus']"
2476954,Does every group have a finite index subgroup?,"Does every group have a finite index subgroup (besides the whole group)? I suspect the answer is no, but I haven't found an example. What about finitely presented or generated groups? Or groups with torsion? I tried something like taking a presentation and only selecting some of the generators, but I don't this works even if $G$ has torsion, since the other generators could interact in a complicated way. Perhaps if we could find a minimal presentation of a group with torsion containing (as a generator) a torsion element, then this would work, but I'm not sure why such a minimal presentation should exist.",['group-theory']
2476963,Does Artin's conjecture imply that the reciprocal sum of primes with a given primitive root would diverge?,"Artin conjectured that every non-square integer $a\ne -1$ is a primitive root for infinitely many primes. Here it is on Wikipedia: Artin's conjecture on primitive roots . The conjecture also includes a possible asymptotic density for the primes having $a$ as a primitive root, under certain conditions. It's not clear to me whether the existence of that density, and the fact that it is positive, would imply that the reciprocals of all such primes form a divergent series. I know that the sum of reciprocals of all primes diverges, and that the sum of reciprocals of primes in an arithmetic sequence diverges (at least I think I remember reading that somewhere), and that leads me to think that a positive asymptotic density is sufficient, but I don't know how to make that entirely clear. Any insights are appreciated.","['number-theory', 'divergent-series', 'prime-numbers', 'primitive-roots']"
2476983,Bolzano theorem and interval solution,"I have a function $f$ witch is continuous at $[1.4]$. I also have that $f(x)\neq 0,\forall x\in [1,4]$ and $f(1)\cdot f(2)\cdot f(4)=8$. I have already proved that $f(x)>0,\forall x\in [1.4]$. Now I want: To prove that the equation $f(x)=2$ has at least one solution at $[1,4]$. So I thought to use Bolzano's theorem. Let function $g$ with type $g(x)=f(x)-2$ which is continuous at $[1,4]$, $g(1)=f(1)-2$ and $g(4)=f(4)-2$. Now I need to prove that $g(1)\cdot g(4)\le 0$, isn't it right? But if it is, how can I prove that? To prove that the equation $f(x)=x$ has at least one solution at $[1,4]$. In here I use the same method as before?","['continuity', 'inequality', 'functions']"
2477025,Lerch's theorem,"In the context of a stats exercice, I need to show that, if for some $h$ measurable and for every real $\mu$, $\displaystyle {\int_{-\infty}^{+\infty} h\left(x \right)\cdot \exp\left(-x^2\right)\cdot \exp\left(-\mu x\right)\,\mathrm{d}x = 0}$, then $h=0\:a.e.$ The correction simply mentions that the Laplace transform is a bijection (between what spaces?), so h is $0\:a.e.$ But the only injectivity statement for the LT I could find was for well-behaved functions. Is it even true in my case ? Can anyone point me to such a proof ? Thanks EDIT: I changed the title since the result holds for continuous, of exponential order functions (Lerch).","['laplace-transform', 'statistics', 'analysis']"
2477091,"Spivak Calculus on manifolds, Theorem 4-8.","In Michael Spivak's book, Calculus on Manifolds, in 4th Chapter theorem 8 (4-8), I am having a difficulty in making sense of the expression, $f^{*}\omega \wedge f^{*}\eta$. Here $f:\mathbb{R}^{n}\to \mathbb{R}^{m}$ is a differentiable function. $Df(p): \mathbb{R}^{n}\to \mathbb{R}^{m}$ is the derivative at $p$ and a linear transformation induced by this is $f_{*}: \mathbb{R}^{m}_{f(p)} \to \mathbb{R}^{n}_{p}$ such that $f_{*}(v_{p}) = (Df(p)(v))_{f(p)}$. $f^{*}\omega(p)((v_{1})_{p},\ldots,(v_{k})_{p}) = w(f(p))(f_{*}(v_{1})_{p}),\ldots,f_{*}((v_{k})_{p}))$ $\omega$ and $\eta $ are $k$-forms on $\mathbb{R}^{m}$. $\omega\wedge\eta(p) = \sum_{1\leq i_{1}<\ldots i_{k}\leq n} 
\sum_{1\leq j_{1}<j_{2}<\ldots < j_{l}\leq n} \omega_{i_{1}i_{2},\ldots,i_{k}}(p)\eta_{j_{1}j_{2},\ldots,j_{l}}\varphi_{i_{1}}(p)\wedge\ldots\wedge\varphi_{i_{k}}(p)\wedge\varphi_{j_{1}} (p)\wedge\ldots\wedge\varphi_{j_{l}}(p)$. From the above two notions, I am unable to make sense of $f^{*}\omega\wedge f^{*}\eta$. Please tell me if I need to give any more clarifications about the question.","['multivariable-calculus', 'definition']"
2477119,Finding the spectral family of $A^2$,"$A$ is a bounded self-adjoint operator on a Hilbert space. The spectrum of $A$ is continuous (meaning, no point or residual spectrum). $\{E_\lambda\}$ is the spectral family of $A$. How can I show that the family $F_\lambda$ defined by \begin{cases}
0,  & \lambda<0 \\
E_\sqrt {\lambda} - E_{-{\sqrt {\lambda}}}, & \lambda\ge0
\end{cases}
is the spectral family of $A^2$? I tried to show that the sum $\sum\lambda_k (F_{\lambda_{k+1}}-F_{\lambda _{k}}) $ converges to $A^2$ in operator norm, but it got me nowhere. And why do I need the assumption about the spectrum of $A$? Any help would be appreciated.","['functional-analysis', 'operator-theory']"
2477196,Three non-overlapping regular polygons of unit edge-length surround a point. What is the maximum perimeter of their union?,"Three non-overlaping regular plane polygons, at least two of which are congruent, all have sides of length $1$. The polygons meet at a point $A$ in such a way that the sum of the three interior angles at $A$ is $360^{\circ}$. Thus the three polygons form a new polygon with $A$ as an interior point. What is the largest possible perimeter that this polygon can have? How can i solve the problem without knowing the number of sides of polygons or at least 2 of them? Please elaborate the solution for me",['geometry']
2477209,Direct proof that there is no Modular form of weight $2$ for $SL_2(\mathbb{Z})$.,"I want to show that there are no Modular forms of weight $2$ for $SL_2(\mathbb{Z})$ without using the dimension formular or the $\frac{k}{12}$- formular. I was given some hints, too. However, there are two things missing. The idea is to look at a antiderivative of $f \in M_2(SL_2(\mathbb{Z}))$ which is given by $$F(\tau)=a_0\tau+\frac{1}{2\pi i} \sum_{n=1}^\infty \frac{a_n}{n}q^n$$ where $f=\sum_{n=0}^\infty a_nq^n$. First I want to show that $F(\gamma\tau)=F(\tau)+C(\gamma)$ where $\gamma\tau$ denotes a mobius transformation and $C(\gamma)$ does not depend on $\tau$. My approach: $$F'(\gamma \tau)=f(\gamma \tau)*(c\tau+d)^{-2} \ \ (chain \ rule)$$ Since $f(\gamma\tau)=f(\tau)*(c\tau+d)^2$ it follows that $$F'(\gamma \tau)=f(\tau)$$ And after integrating both sides I got $$F(\gamma \tau)=F(\tau)+ C(\gamma).$$ The next step is to show that $C(\gamma_1\gamma_2)=C(\gamma_1)+C(\gamma_2)$ (for $\gamma_1,\gamma_2 \in SL_2(\mathbb{Z}$)) but I have no idea how to show that. Additionally, I am not sure how to show that $F$ is holomorphic. I see that $F(\tau+1)=F(\tau)$, so $F$ is bounded along the real axis. However, what happens if $\lim y \rightarrow \infty$, especially since $a_0 \tau$ might be unbounded? Thanks for your help","['complex-analysis', 'modular-forms']"
2477453,"Is a function in the form of $f(x) = (x^2, x + 1)$ injective or surjective?","I understand the method used to prove injectivity or surjectivity, however, I am confused as to how to handle a function that presents itself as a set $(m,n)$. In the example, $f:Z \rightarrow Z \times Z$, $f(x) = (x^2,  x + 1)$ We know that $x^2$ is not injective as we can find for example $f(-1) = f (1) = 1$. However, we also know that the second part of the equation is injective as we can deduct that: $x + 1 = y + 1$ $x = y$ The form $(m, n)$ of the function is what is really confusing me. Would it be correct to conclude that the function is injective since it will produce a set of unique $(m,n)$ values for each $x$? If so, how would we define such a function to be surjective since there will exists coordinates for which there is no corresponding $x$? Would I be correct to conclude that the function is not surjective? Also, if a function was in the form $f(x) = z + 5$, would it be correct to conclude that the function is not well defined, and thus we can't determine if it's surjective\injective due to the fact that it's based on a ambiguous variable $z$?","['elementary-set-theory', 'functions']"
2477478,"Evaluate $\lim\limits_{(x,y)\to(0,0)}\frac{(x+y)^2}{x^2+y^2}$","Evaluate $\displaystyle\lim_{(x,y)\to(0,0)}\dfrac{(x+y)^2}{x^2+y^2}$ Using polar, we have $x=r\cos(\theta),y=r\sin(\theta)$ Our limit becomes: $$\lim_{r\to 0}\dfrac{(r\cos(\theta)+r\sin(\theta))^2}{r^2\sin^2(\theta)+r^2\cos(\theta)}=\lim_{r\to 0}\dfrac{r^2\cos^2(\theta)+2r^2\cos(\theta)\sin(\theta)+r^2\sin^2(\theta)}{r^2}$$ Factoring and dividing removes the $r^2$ in the denominator, and we get $1$ as the limit. However this is not right. If we consider along the $x-axis$, our limit becomes $1$. If we consider along the line $y=x$, our limit becomes $1/2$, and are clearly not equal. This means that the limit does not exist but my polar said it does and it equals $1$. Where did I mess up in my polar coordinates?","['multivariable-calculus', 'polar-coordinates', 'limits']"
2477483,Prove that we cannot inscribe sphere in polyhedron.,Each face of a polyhedron is colored with red or blue such that each two faces with a common edge are colored differently. Suppose that the blue area is larger than the red. Prove that we cannot inscribe a sphere in the polyhedron. Firstly I thought that this one has to do something with Euler formula for the planar graphs. But I have no idea how to start. Any help?,"['polyhedra', 'euclidean-geometry', 'geometry']"
2477500,Find pdf given transformation,"I'm given that the random variable $X$ is distributed as $UNIF(0,1)$. Moreover, I'm given that: $U = X(1-X)$ Using the CDF method (i.e. finding the CDF, then taking the derivative), how would I find the PDF of $U$? I can't isolate X in the equation, so how would I go about getting the PDF of $U$ as a result? Thank you","['derivatives', 'statistics', 'transformation', 'calculus']"
2477512,Apply Leibniz integral rule to derive the formula for power $P=\mathbf{F}\cdot\mathbf{v}$,"I have posed this problem for myself. I am asking it here because I need to know about the calculus specifically, not the physics. Suppose that $\mathbf{r}$ varies with $t$ and that $\mathbf{F}$ varies with $\mathbf{r}$ and $t$. Using the definitions of power and work, derive $P=\mathbf{F}\cdot\mathbf{v}$. When integrating across constant limits $a$ and $b$, Leibniz integral rule reduces to $$\frac{d}{dx}\int_a^b f(x,t)\,dt=\int_a^b\frac{\partial}{\partial x}f(x,t)\,dt$$ Using this rule and the definitions of power and work—
$$\begin{align}
P &= \frac{dW}{dt} \\[2ex]
W &= \int\mathbf{F}\cdot d\mathbf{r} \\
\end{align}$$
—I am trying to proceed with the derivation thusly: $$\begin{align}
P &= \frac{dW}{dt} \tag1\\[2ex]
&= \frac{d}{dt}\int\mathbf{F}\cdot d\mathbf{r} \tag2\\
\end{align}$$ Here is where I get stuck: is the next step $$P=\int\frac{\partial\mathbf{F}}{\partial t}\cdot d\mathbf{r} \tag{3a}$$ or is it $$P=\int\frac{\partial}{\partial t}\left(\mathbf{F}\cdot d\mathbf{r}\right) \tag{3b}$$ These are the vibes I am getting for how to execute the derivation: $\partial/\partial t$ and $d\mathbf{r}$ somehow come together to make $\mathbf{v}$ $\partial/\partial t$ needs to end up with $\mathbf{F}$ as well somehow the integral will somehow “undo” the time-derivative of $\mathbf{F}$ I am also not sure what to do with the extra differential of $\mathbf{r}$: will it become $d(\partial\mathbf{r})$ or $\partial(d\mathbf{r})$, and how does this interact with the integral?","['derivatives', 'physics', 'integration', 'partial-derivative']"
2477537,"Almost a partial order, is a partial order induced anyway?","If I have a set $S$ and a relation $\rho$ on $S$, such that $\rho$ is reflexive and antisymmetric, and I draw the arrow diagram of this, and this diagram is connected, with no cycles, is transitivity induced?","['relations', 'elementary-set-theory']"
2477549,Frobenius method on $x''+\frac{1}{t}x'+x=0$,I am trying to use the Frobenius method on $x''+\frac{1}{t}x'+x=0$ where $x(0)=1$ and $x'(0)=0$. So far I have: Let $x(t)=\sum_{n=0}^\infty a_nt^n$ then $x'(t)=\sum_{n=0}^\infty na_nt^{n-1}\rightarrow \frac{1}{t}x'(t)=\sum_{n=0}^\infty \frac{1}{t}na_nt^{n-1}=\sum_{n=0}^\infty na_nt^{n-2}$ $x''(t)=\sum_{n=0}^\infty n^2a_nt^{n-2}$ So if I put this into the ODE I get $\sum_{n=0}^\infty \bigg[n^2a_nt^{n-2}+na_nt^{n-2}+a_nt^n \bigg]=\sum_{n=0}^\infty \bigg[a_n(n^2+n)t^{n-2}+a_nt^n \bigg] = 2a_1t^{-1}+\sum_{n=2}^\infty a_n(n^2+n)t^{n-2}+\sum_{n=0}^\infty a_nt^n = 2a_1t^{-1}+\sum_{n=0}^\infty a_{n+2}((n+2)^2+n+2)t^{n}+\sum_{n=0}^\infty a_nt^n$ I can't figure out how to finish this. Can someone help me out?,['ordinary-differential-equations']
2477565,Show the limit of Lebesgue sum is Lebesgue integral,"Suppose $\mu(E)<\infty$ and $f$ is Lebesgue integrable on $E$. Prove the Lebesgue integral 
$$\int f~d\mu= \lim_{m(T)\to 0}\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}.$$
$T=\{...t_{-1}<t_0<t_1<...\}$ is a countable partition of $\mathbb{R}$, $m(T)=\displaystyle\sup_k|t_{k+1}-t_k|$ and $\tau_k\in[t_k,t_{k+1})$ are aribitrary points. So, we need to show $$\sup\{\int g~d\mu, g\le f\ \text{and}~g~\text{is simple}\}=\lim_{m(T)\to 0}\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}$$ When the partition is finite nad $f$ is bounded, $\displaystyle\sum_k \tau_k~\mu\{t_k\le f \le t_{k+1}\}$ gives us a simple funtion and I think we can use Lebesgue monotone convergence theorem. However, $f$ are not necessarily bounded and the partition is infinite. How to deal with this case?","['functional-analysis', 'real-analysis', 'integration', 'lebesgue-integral']"
2477575,Image of morphism of projective varieties is projective variety,"Let $k$ be an algebraically closed field, $X,Y$ projective varieties (irreducible algebraic sets) and $f:X\to Y$ a morphism. Is $f(X)$ a projective variety? I think it is because the image of a morphism is closed and continuity preserves irreducibility. Is this correct? I wonder because if $X$ and $Y$ are affine varieties, the statement is not true by this example: Image of a morphism of varieties .","['projective-geometry', 'algebraic-geometry']"
2477637,Spivak Calculus Chapter 9 Question 9(ii),"I am working out of the 3rd edition. The question is to find $f'(x)$ and $f'(x+3)$ for $f(x+3)=x^5$ My working is the following: $$\begin{eqnarray}
f(x)&=&f((x+3)-3)\\
&=&(x-3)^5\\
\implies f'(x)&=&5(x-3)^4\\
\end{eqnarray}$$ According to the answers section in the book, this is correct. Now:
$$\begin{eqnarray}
f'(x+3)&=&f'((x)+3)\\
&=&5((x+3)-3)^4\\
&=&5x^4\\
\end{eqnarray}$$ However, the answer in the back of the book for $f'(x+3)$ is $0$. I can't understand how the result $0$ was obtained at all. Is this a problem with the book, or a problem with my working? I can't find an up to date errata list.","['derivatives', 'calculus']"
2477676,expected value greater than probability,"I'm supposed to prove that for any Random Variable X, $E[X^4] \ge \frac 14 P(X^2\ge \frac 12)$ I tried substituting the definitions of expected value and of the probability into the inequality, but that gets me no where. Any tips on where to go with this proof? Would a moment generating function lead me in the right direction? Thank you","['statistics', 'probability', 'random-variables']"
2477721,Conditions on angles between three points on a sphere (which are uniformly distributed),"Question: Let $A,B,C$ be three random, uniformly distributed, independent points on a sphere. What is the probability that none of these three points is at an angle superior than $\pi/2$ from the two others. As of right now, I know that wlog, we can assume that $A$ is on the north pole and that $B$ is on a great circle passing through the north and south pole. Then, I am trying to calculate the area which isn't covered by the northern hemisphere or by the hemisphere centered at $B$ . I have trouble figuring out how to deal clearly with all of these cases. Thank you in advance for you help!","['independence', 'uniform-distribution', 'probability', 'calculus']"
2477772,Classifying Groups of Order 28,"I am trying to classify groups of order 28. In the course of the problem, I am stuck in showing that three semidirect products are isomorphic to each other. In this problem, $G$ is a group of order 28, $H\in\mathrm{Syl}_{7}(G)$ is the unique Sylow 7-subgroup, and $K\in\mathrm{Syl}_{2}(G)$. I am working on the case where $K\cong \mathbb{Z}_{2}\times \mathbb{Z}_{2}$. We have the following groups to consider: $$K\cong\mathbb{Z}_{2}\times \mathbb{Z}_{2}=\left\langle a,b\:|\:a^{2}=b^{2}=(ab)^{2}=1\right\rangle$$ $$\mathrm{Aut}(H)\cong\mathbb{Z}_{6}=\left\langle x\:|\: x^{6}=1\right\rangle$$ Let $\psi_{j}: K\to \mathrm{Aut}(H)$, with $j\in\{1,2,3,4\}$, be defined as follows: $$\psi_{1}:\left\lbrace \begin{array}{c}
a\mapsto 1\\
b\mapsto 1
\end{array}\right\rbrace \:\:\:\:\:\psi_{2}:\left\lbrace \begin{array}{c}
a\mapsto x^{3}\\
b\mapsto 1
\end{array}\right\rbrace\:\:\:\:\:\psi_{3}:\left\lbrace \begin{array}{c}
a\mapsto 1\\
b\mapsto x^{3}
\end{array}\right\rbrace\:\:\:\:\:\psi_{4}:\left\lbrace \begin{array}{c}
a\mapsto x^{3}\\
b\mapsto x^{3}
\end{array}\right\rbrace$$ I know that because $\psi_{1}$ is trivial, we get $H\rtimes_{\psi_{1}}K\cong H\times K$. With all the previous work that I have done for this problem, this direct product determines the third isomorphism class for my isomorphism types. The problem statement tells me that there are four isomorphism types, so I only need one more. This means that we need $$H\rtimes_{\psi_{2}}K\cong H\rtimes_{\psi_{3}}K\cong H\rtimes_{\psi_{4}}K.$$ However, I do not know how to show that all these semidirect products are actually isomorphic. Thanks in advance for any help!","['finite-groups', 'abstract-algebra', 'group-theory', 'sylow-theory', 'semidirect-product']"
2477817,Calculating the Second Moment of a Binomial Random Variable,"The first step in calculating the variance of a Binomial Random Variable is calculating the second moment. I have no idea as to how the last two steps have happened. Why is a n(n-1)p^2 outside the first summation and a similar expression outside the second? Also, how did the expression turn out to be the last equation?","['binomial-distribution', 'probability']"
2477824,Singular correlation matrix implies linear dependence,"Given a random vector $X = \begin{bmatrix} X_1 \\ \vdots \\ X_n\end{bmatrix}$ Suppose that $R = \mathbb{E}[XX^T]$ is the correlation matrix of the random vector $X$ We claim that if $R$ is singular, then the components of $X$ must
  not be linearly independent (i.e. they are linearly dependent). Does anyone see how to prove this claim? I've tried some thought experiments (e.g. when $R$ is diagonal) but I couldn't see how you would go about proving this.","['expectation', 'probability-theory', 'covariance', 'probability', 'random-variables']"
2477829,Is the subset of $\ell^1$ where the $k$th coordinate is $0$ or $2^{-k}$ perfect?,"I've encountered the following problem: Determine whether or not the following set is perfect in a metric space $(X,d)$: $\prod_{k=1}^\infty \left\{0, \frac{1}{2^k} \right\}$ in $(\ell_1, \|\cdot\|_1)$. The problem is that I don't really understand this notation. First of all, how does the sequence above exactly work? I'm reading it as follows: $$\prod_{k=1}^\infty \left\{0, \frac{1}{2^k} \right\}=\left\{0, \frac12\right\}\times \left\{0, \frac14\right\}\times\dots \times \left\{0, \frac1{2^k} \right\}\times\dots$$ But what kind of a sequence is this? How does the norm on this sequence work? Also, isn't it true by definition that sequences in $\ell_1$ have the metric $\|\cdot\|_1$?","['functional-analysis', 'lp-spaces', 'banach-spaces']"
2477920,How to solve the trigonometric equation $\sin x + \cos x=\sin 2x + \cos 2x$?,Question : Solve the trigonometric equation: $\sin x + \cos x=\sin 2x + \cos 2x$. My attempt : $\sin x + \cos x=\sin 2x + \cos 2x$ $\implies \sin x + \cos x=2\sin x \cos x + \cos^2 x - \sin^2 x$ $\implies \sin x + \cos x=2\sin x \cos x + \cos^2 x - (1-\cos^2 x)$ $\implies \sin x + \cos x=2\sin x \cos x + 2\cos^2 x - 1$ $\implies \sin x - 2\sin x \cos x + \cos x - 2\cos^2 x= - 1$ $\implies \sin x(1-2\cos x)+\cos x(1-2\cos x)=-1$ $\implies (1-2\cos x)(\sin x+\cos x)=-1$ $\implies (1-2\cos x)=-1$ or $(\sin x +\cos x)=-1$ $\implies \cos x=1$ or $\sin^2 x +\cos^2 x + 2\sin x\cos x=1$ $\implies x=2n\pi$ or $\sin 2x=0$ $\implies x=2n\pi$ or $2x=n\pi$ $\therefore x=2n\pi$ or $x=\frac{n\pi}{2}$ But the answers given in my book are $x=2n\pi$ and $x=\frac{(4n+1)\pi}{6}$. Where have I gone wrong? Please help.,"['trigonometry', 'proof-verification']"
2477928,Partition of $\mathbb{C}$ into Borel sets for which a polynomial is a bijection with $\mathbb{C}$,"Background Let $P$ be a polynomial of degree $n>0$ with coefficients in $\mathbb{C}$ . By the Fundamental Theorem of Algebra, for any complex $y$ , there exist $n$ complex numbers $z$ such that $P(z)=y$ (possibly with multiplicity). So, we can consider $P$ to be a sort of "" $n$ -injective"" function, in that, for each output, there exist $n$ inputs that give that output. Question We can assign each complex $c$ with a ""degree"" $\mu(z)\leq n$ in accordance with the multiplicity of the root $z=c$ in the polynomial $P(z)-P(c)$ . Does there exist a (almost) partition of $\mathbb{C}$ into $n$ Borel sets such that:the polynomial $P$ is bijective from each set to $\mathbb{C}$ and each complex number $z$ appears in exactly $\mu(z)$ of the sets? This would almost be a partition since, for almost all $z$ , $\mu(z)=1$ (in fact, $\mu(z)\neq 1$ only for a finite number of $z$ as that would require $P'(z)=0$ ). I've chosen Borel as the qualifier as, as far as I can tell, it's the best way to define a ""nice set"" - obviously if the quantifier is taken away the problem is true - but I'm not sure if I'm using it properly. Progress The statement is trivially true if $n=1$ (all non-constant linear polynomials are bijective, so one can simply take the set to be $\mathbb{C}$ ). If $n=2$ , the polynomial can be completed into $P(z)=a\left((z-b)^2+c\right)$ for some complex $a,b,c$ , so we can choose our sets to be, if $b=x+iy$ where $x,y\in\mathbb{R}$ , The half-plane above $\Im(z)=y$ and the ray $\Re(z)\geq x, \Im(z)=y$ The half-plane below $\Im(z)=-y$ and the ray $\Re(z)\leq x, \Im(z)=y$ . However, this is all based on the fact that, as functions, all quadratic polynomials act like $z^2$ . So this sort of thing is, as far as I can see, not readily generalizable to $n>2$ . Is this a known problem (and if so, where can I read more about it)? Am I missing something simple?","['complex-analysis', 'polynomials', 'complex-numbers']"
2477962,Strange attractors: what is the difference between a map and differential equation system?,"As far as I have been able to understand, there are two main ways of generating (or finding) a strange attractor : Using a map. E.g. the Hénon map (for a given $a,b$): $$x_{n+1} = 1 - a x_n^2 + y_n \ \ \ , \ \ \ \ y_{n+1} = b x_n$$ Using differential equations. E.g. the Lorentz system (for a given $\delta,\rho,\beta$): $$\dot x = \delta (y-x) \ \ \ , \ \ \ \ \dot y = x( \rho -z)-y \ \ \ , \ \ \ \ \dot z = x y - \beta z$$ What I do not understand very well is what differences are between a map and differential equations in this respect. This is what I guess: A map is always a discrete-time dynamical system , so no differential equations are required to generate the strange attractor. In the other hand, a differential equation system is per se a continuous-time dynamical system (due to the fact that it is based indeed on differential equations). Are the above assumptions correct, or are the differences between a map and a differential-equations-based dynamical system more than that? Can a differential equations system be converted into a map (probably adding some restrictions), or likewise, a map into a differential equations system and be able to reproduce the same strange attractor (or a restricted version of the same)?","['recurrence-relations', 'ordinary-differential-equations', 'dynamical-systems']"
2478014,Probability of Bride entering the Church?,"A Bride is standing at the entrance of a church with her father (one step forward will take her into the church). Her father has a basket containing $10$ White roses and $10$ Red roses. He takes $1$ rose at a time from the basket and gives it to the Bride. If the rose is Red, the Bride takes $1$ step towards the church and if it is White, she takes $1$ step away from the church. What is the probability that the Bride enters the church? Assume that Bride's father can not see the rose until he takes the rose out of the basket. I am stuck at this point: If the first rose is Red then the Bride enters the Church and in that case the probability is $\frac{10}{20}$. But then come the cases when the first rose is White: WRR, WWRRR, WRWRR and so on. No matter what, if the first rose is White, the last two roses must be Red. And the total number of roses required (⩽20) to enter the church is Odd, where the number of Red roses will never exceed that of the White roses but once when the Bride finally enters the church. Leaves me in doldrums, though","['combinatorics', 'probability', 'discrete-mathematics']"
2478037,One thousand raffle tickets are sold,"The scenario is: One thousand raffle tickets are sold. There is a grand prize of \$300, 3 second prizes of \$100, and 6 third prizes of \$50.  A person purchased one raffle ticket. Find the probability that the person wins at least \$100. I am thinking that let X denote the amount of money the person wins from the raffle. I am interested in finding $P(X \geq 100)$ (if I am correct). Since the person only buys one raffle, then this turns out to be the probability the person wins either the grand prize or the second prize category, not the third prize because even if the person won that prize, he or she could only win at most 50$. Therefore, the probability I am looking at is (1/1000) + (2/1000); this is incorrect. I am not sure why. Thank you!","['statistics', 'probability', 'central-limit-theorem']"
2478098,"Definite integral over a semicircular area $\int_0^{2a}\int_0^{\sqrt{2ax-x^2}}\frac{\phi'(y)(x^2+y^2)x}{\sqrt{4a^2x^2-(x^2+y^2)^2}}dy\,dx$. [duplicate]","This question already has an answer here : Evaluate the integral $\int_0^{2a}\int_0^\sqrt{2ax-x^2}\frac{\phi'(y)(x^2+y^2)x dxdy}{\sqrt{4a^2x^2-(x^2+y^2)^2}}$ (1 answer) Closed 3 years ago . change the order of integration in
$$\int_0^{2a}\int_0^{\sqrt{2ax-x^2}}\frac{\phi'(y)(x^2+y^2)x}{\sqrt{4a^2x^2-(x^2+y^2)^2}}dy\,dx$$ I was able to change the order of integration here to 
$$\int_0^a\int_{a-\sqrt{a^2-y^2}}^{a+\sqrt{a^2-y^2}}\frac{\phi'(y)(x^2+y^2)x}{\sqrt{4a^2x^2-(x^2+y^2)^2}}dx\,dy$$
Now i am stuck with the integration here w.r.t $x$. I tried substituting $(x^2+y^2)^2$ with $t$ but then replacing value of $4a^2x^2$ becomes a problem. I was thinking of substituting both $x$ and $y$ as $k\cos\theta$ and $k\sin\theta$ but then $y$ won't be constant w.r.t $x$, so that too I believe is out of the question!","['integration', 'definite-integrals', 'area']"
2478113,"Given $x\odot y=x+y+xy$, prove $x\odot x\odot\ldots\odot x= (1+x)^n -1$","Given $x\odot y=x+y+xy$, prove that 
  $$\underbrace{x\odot x\odot\ldots\odot x}_{n\text{ times}}= (1+x)^n -1$$ for all $n\in \Bbb{N}$ and $x \in\Bbb R\setminus\{-1\}$. I have tried to use the binomial theorem on $(1+x)^n$ but was unable to simplify the nested function. Also tried mathematical induction but that didn't work out. The nested function is still stumping me as I haven't figured out how to generalize it for $n$ terms. Also some information on how to deal with nested functions would be greatly appreciated.","['abstract-algebra', 'functions', 'binary-operations', 'abelian-groups', 'group-theory']"
2478121,"Solve $\vert i+2i^2+3i^3+...+ni^n\vert$=$18\sqrt 2$ for $n$, where $i^2=-1$","Suppose $n\in \mathbb N$ such that $\vert
 i+2i^2+3i^3+...+ni^n\vert$=$18\sqrt 2$.Where $i$ is a square root of
  $-1$.Then what is the value of $n$? Solution: Let $S_n=i+2i^2+3i^3+...+ni^n\tag{1}$ Then  $iS_n=i^2+2i^3+3i^4+...ni^{n+1}\tag{2}$ Subtracting $(2)$ from $(1)$, we get $$S_n(1-i)=(i+i^2+i^3+...+i^n)-ni^{n+1}$$ hence $$S_n=\frac{i(1-i^n)}{(1-i)^2}-\frac{ni^{n+1}}{1-i}=\frac{i(1-i^n)}{-2i}-\frac{ni^{n+1}}{1-i}$$ This is where I get stuck.","['complex-analysis', 'summation', 'complex-numbers']"
2478147,Proof that the maximum number of independent vertex sets of a tree is achieved by the star,"I'm trying to prove by induction that for all trees of order $n$ ($n\geq 2$), the star of order $n$ has the largest number of independent vertex sets. I've started this by showing that $n=2$ and $n=3$ are trivial, then assuming $n$ is true. Then let $G$ be a tree of order $n+1$. I'm considering an endpoint $v$, and what would happen when removing v, but getting stuck. I've read a proof by Prodinger and Tichy but don't understand it, and wondering if there is another way. My thoughts are that by removing the leaf (endpoint) from $G$, we are left with a tree of order $n$, so we know the max. no. of sets is achieved by the star. But I can't quite get to the idea of finishing the proof. Thanks!","['graph-theory', 'trees', 'discrete-mathematics']"
2478153,Seven-sided dice from five-sided dice with finite rolls,"I have a 5-sided dice, and I want to use it to simulate a 7-sided dice.
Is there a way to do this with a finite number of rolls?","['uniform-distribution', 'probability', 'dice']"
2478216,$\sigma$-field of an increasing sequence of stopping times,"Let us consider the sequence $(\tau_n)_{n\in \mathbb{N}}$ of stopping times that takes values in $\mathbb{N}$ such that $\tau_n \uparrow \tau$, and $\tau < \infty$. Prove the following equality: $\mathcal{F}_\tau=\sigma(\cup_n \mathcal{F_{\tau_n}})$ I'm having problem with both the inclusions, any suggestions?","['probability-theory', 'measure-theory', 'stopping-times']"
2478232,Expected Payoff for Dice Game Where Six = No Payoff,"In a dice game where a player's payoff is whatever the die is rolled, each player can roll how many times they want. Each payoff gets added cumulatively (e.g. roll 5 and 5, then payoff = 10). The catch is that if the user rolled a six at any point in time, they get 0 (e.g. rolled 5 and 6, then payoff = 0). Intuitively, raising $n$ (the number of die rolls) could raise the expected payoff but also increases the probability of at least rolling one six and getting 0 as a payoff. For example if you picked a really large $n$, the likelihood of rolling at least 1 six and getting 0 payout is extremely likely. But going from $n=1$ to $n=2$ you get a little higher expected payoff (checked the math manually in lieu of a general formula). Going with the weighted average approach to come up with a formula for expected value, one part of the formula must be the weighted average of getting a 0 payout, i.e. the probability of rolling at least one six out of $n$ dice rolls equals $1-\left(\frac{5}{6}\right)^n$. As such, we get: $E(X) =$ weighted average for each payoff $E(X) =$ weighted average of getting 0 + the rest of the weighted average of payoffs $E(X) = \left(1-\left(\frac{5}{6}\right)^n\right) 0$ + the rest of the weighted average of payoffs However, I'm having trouble wrapping my head around coming up with the rest of the equation. How can I solve this problem?",['probability']
2478277,Simple bijection $\mathbb N_0 \times \mathbb N_0 \to \mathbb N_0$,"I'm looking for a 'simple' bijection
$$
\pi \colon \mathbb N_0 \times \mathbb N_0 \to \mathbb N_0,
$$ where 'simple' in this context means that it should be as easy as possible to define and it should be self-evident that said function is in fact a bijection. It's a quest for convenience - not minimal complexity in any rigorous sense. Here are a couple of examples I came up with - none of which satisfy both requirements: Let $\pi(m,n) = c(2^m3^{n+1})$, where $c \colon \{2^m3^{n+1} \mid m,n \in \mathbb N_0 \}\to \mathbb N_0$ is the order isomorphism (or in fact Mostowski collapse) of its domain under the natural ordering, Let $\pi(m,n) = \langle n,m \rangle$ - Gödel's pairing function, Let $\pi(m,n) = m \oplus n$ - the number resulting from ' riffling ' $m,n$ (where we imagine both $m$ and $n$ as in infinite decks of cards whose $i$th element is labeled with its respective $i$th digit). ...","['soft-question', 'elementary-set-theory', 'elementary-number-theory']"
2478280,Derivative of positive function is positive close to zero,"Assume you have a function $f \mapsto \mathbb R \to \mathbb R$. Let $f\in C^1(\mathbb R)$, $f(0) = 0$, $f(x) > 0$ for $x > 0$. Additionally $f'(0) = 0$. I want to prove that exists a $x^* > 0$ such that $f(x)$ is increasing for $0 \le x < x^*$. I have been toying with the theorem of permanence of sign of the derivative (which is continuos) and the fact that the function must be positive, but I can't quite get there. Is it even true? If not, would be cool to have a counterexample :)","['derivatives', 'real-analysis', 'calculus']"
2478293,Finding range of $y=\frac{2x^2+2x-4}{x^2+x}$ analytically (using discriminant),"I am trying to find the range of the function
$$
y=\frac{2x^2+2x-4}{x^2+x}
$$
analytically using the discriminant trick (there's a somewhat helpful post here about that). Here's what I have so far:
\begin{align}
y=\frac{2x^2+2x-4}{x^2+x}
&\iff yx^2+yx=2x^2+2x-4\\[1em]
&\iff x^2(y-2)+x(y-2)+4=0\\[1em]
&\iff (y-2)^2-4(y-2)(4)\geq0\tag{$*$}\\[1em]
&\iff (y-2)(y-18)\geq0.
\end{align}
My temptation is to write that the range of the function is $(\infty,2]\cup[18,\infty)$, but the range is actually $(-\infty,2)\cup[18,\infty)$. How can I determine that from my work above? My thought was I do not have a strict inequality for $(*)$ above because my function can equal $0$. But I am confused as to how I am supposed to know that $2$ is not included in the range. The link posted above mentions how the discriminant will be zero at any max or min $y_0$, but I imagine the discriminant is not $0$ otherwise then? I realize $y=2$ is a horizontal asymptote for my function, but I also know one can easily cross a horizontal asymptote before ""settling on it"" as $x\to\pm\infty$. Can someone explain how to use my work above to determine the proper range as well as explain what is going on concerning max and min as they relate to the discriminant here? In retrospect, my reasoning for not using a strict inequality in $(*)$ seems flawed--it seems like it has more to do with the presence or absence of a max or min perhaps?","['discriminant', 'functions', 'algebra-precalculus', 'analysis', 'quadratics']"
2478319,Strange symmetry regarding sum $\sum_{n=0}^\infty\frac{n^ne^{-bn}}{\Gamma(n+1)}$ and integral $\int_{0}^\infty\frac{x^xe^{-bx}}{\Gamma(x+1)}dx$,"One can show by computation the following for $b>1$
$$\sum_{n=0}^\infty\frac{n^ne^{-b n}}{\Gamma(n+1)}=\frac{1}{1+W_{\color{blue}{0}}(-e^{-b})},\tag{1}$$
(here one assumes that the term with $n=0$ is understood as the limit $\lim_{n\to 0}$ and is equal to $1$) and
$$\int_{0}^\infty\frac{x^xe^{-b x}}{\Gamma(x+1)}dx=\boldsymbol{\color{red}{-}}\frac{1}{1+W_{\color{red}{-1}}(-e^{-b})}.\tag{2}$$ $W_0$ and $W_{-1}$ are different branches of the Lambert W function . One can see that this formulas look similar. I considered them in the hope of obtaining a function for which sum equals integral:
$$
\sum_{n=0}^\infty f(n)=\int_0^\infty f(x) dx.
$$
$(1)$ is the consequence of Lagrange inversion and the integral arises in the probability distribution theory, namely the Kadell-Ressel pdf (see also this MSE post ). Question 1. Can anybody explain the symmetry between $(1)$ and $(2)$ without resorting to direct calculation ? Question 2. Is it possible to alter $(1)$ and $(2)$ to obtain a nice function for which sum equals integral? If $b=1$ then there is the Knuth series $$
\sum_{n=1}^\infty\left(\frac{n^ne^{-n}}{\Gamma(n+1)}-\frac1{\sqrt{2\pi n}}\right)=-\frac23-\frac1{\sqrt{2\pi}}\zeta(1/2),\tag{3}
$$
and the ""Knuth integral""
$$
\int_0^\infty\left(\frac{x^xe^{-x}}{\Gamma(x+1)}-\frac1{\sqrt{2\pi x}}\right)dx=-\frac13.\tag{4}
$$
Again we see there is a discrepancy. Question 3. Is it possible to modify the term $\frac1{\sqrt{2\pi x}}$ in $(3)$ and $(4)$ so that the series and the integral agree? Edit. Of course by mounting some additional terms and parameters one can come up with a formula that technically answers question 2 or 3. What is meant as nice in question 2 might be difficult to formulate explicitly. It is best illustrated by formulas in this MSE post .","['calculus', 'integration', 'definite-integrals', 'special-functions', 'sequences-and-series']"
2478327,Quantiles of comonotone sums,"Let $(\Omega, \mathcal F, P)$ be a probability space. Let $\mathbf{X} = (X_1, X_2, \ldots, X_n)^T$ be a random vector and $U \sim \mathrm{uniform}(0, 1)$ be a random variable, both defined on $\Omega$. We say that $\mathbf{X}$ is comonotone if $\mathbf{X} \stackrel{d}= (F_{X_1}^{-1}(U), \ldots, F_{X_n}^{-1}(U))^T$, where, for a random variable $X$, we define $F_X^{-1}(p) := \inf\{x \in \mathbb R; F_X(x) \ge p\}$, $p \in [0, 1]$. Furthermore, define $F_X^{-1+}(p) := \sup\{x \in \mathbb R; F_X(x) \le p\}$, $p \in [0, 1]$. Set $S := \sum_{i = 1}^n F_{X_i}^{-1}(U)$. Now, suppose that $\mathbf{X}$ is comonotone. Then, for any $p \in [0, 1]$: $F_S^{-1}(p) = \sum_{i = 1}^n F_{X_i}^{-1}(p)$ and $F_S^{-1+}(p) = \sum_{i = 1}^n F_{X_i}^{-1+}(p)$. How can I prove both statements? Thank you for any help!","['probability-theory', 'probability', 'probability-distributions']"
2478359,Convergence of $A_nT$ to $AT$ in operator norm for compact $T$,"$A_n:Y\rightarrow Z$ are operators that strongly converge to $A$. Also, $\|A_n\|_\text {op}\le c$ for $c>0$. Given a compact operator $T:X\rightarrow Y$, I need to show that $A_nT$ converges to $AT$ in operator norm (all spaces in this question are Banach spaces). I was unable to prove this and I also do not understand why we need to assume that $T$ is compact. Any ideas?","['functional-analysis', 'compact-operators', 'convergence-divergence', 'operator-theory']"
2478378,Line integrals and reparametrization,"$C = (1,2,0)$, $B= (1,0,2)$. part B :
I have that my parametric equations are
$x = 1$, $y = 2\cos(t)$, and $z = 2\sin (t)$. part C : I don't know how to approach this. Do I start with the divergence of $G$ and then reparametrize with $\tau$?","['multivariable-calculus', 'integration', 'vector-analysis']"
2478390,decreasing random process depending on previous draw,"I draw a number $x_i \in [n]$ where $n \in \mathbb{N}$. I draw a number again $x_{i+1} \in [x_i]$. I repeat this drawing process till i got an 1. This generates a random sequence $(x_i)_{i=1}^k$.
All draws are uniformly at random. Question 1: What is $P[x_i = 1] $ meaning the probability of stopping at a specific index i. Question 2: $P[x_i = k] $ where k is some number in N. I already solved Question 2 to some recurrence $\forall k > 1$: $s_{i,k}(n) = \sum_{l=k}^n \dfrac{1}{l} s_{i-1,l}(n) = P[x_i = k] $ with base case $s_{1,l} = \dfrac{1}{n}$. But its quite a messy way and not able to be in closed form so im wondering if anyone else has a better way of solving this.","['random-walk', 'statistics', 'random-functions', 'probability-distributions']"
2478506,$\lim_{x \to 1} {P(x^n)\over P(x^m)}$,"if $P$ is a polynomial and $n,m\in\Bbb N^*$ and $P(1)=0$ find: $$\lim_{x \to 1} {P(x^n)\over P(x^m)}$$
Let $P(x)=a_νx^ν+a_{ν-1}x^{ν-1}+...+a_1x+a_0$ and I used L'Hôpital's rule and I found that $$\lim_{x \to 1} {P(x^n)\over P(x^m)}={n\over m}$$ which I'm not sure if it's the correct answer. I'd like a help/hint (maybe without L'Hôpital's rule).","['calculus', 'limits']"
2478507,Cauchy–Riemann equations and complex differentiability at origin,"Consider the function defined by $$f(z)=\begin{cases}0&\text{ if }\operatorname{Re}(z)=0\vee\operatorname{Im}(z)=0\\1&\text{ otherwise.}\end{cases}$$ I would like to show that the real and imaginary parts of $f$ satisfy the Cauchy-Riemann equations at the origin but am not sure if just saying $0=0$ and $0=-0$ is enough or not also is $f$ complex differentiable at the origin, I am not sure on how to show whether it is or not, thank you","['derivatives', 'complex-analysis', 'cauchy-riemann-equations']"
