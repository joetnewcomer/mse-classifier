question_id,title,body,tags
4732018,A question on matrices whose minimal polynomials has no repeated roots,"Question a) Prove that a matrix $A\in M_n(\mathbb{C})$ satisfying $A^3=A$ can be diagonalized. b) Does the statment in (a) remain true if one replaces $\mathbb{C}$ by an arbitrary algebraically closed field $F$ ? Answer First of all, we can see that the matrix $A$ satisfies the polynomial $p(X)=X(X-1)(X+1)$ . Since the minimal polynomial has to divide $p(X)$ , we can conclude that the minimal polynomial of $A$ has no repeated root. This would yield that $A$ can be diagonalized. However, I couldn't answer the part (b). Actually, I do not know what the crucial part $\mathbb{C}$ plays in part (a) either. Thanks a lot in advance...","['matrices', 'diagonalization', 'linear-algebra', 'minimal-polynomials']"
4732020,"Derive the uniformly most powerful test of size $\alpha=0.05$ for $H_0: \mu\le \lambda$, v.s. $H_1: \mu>\lambda$","Let $X$ and $Y$ be independent uniform random variables on $[\lambda, \lambda+1]$ and $[\mu, \mu+1] $ respectively. The problem of testing $H_0: \mu\le \lambda$ , v.s. $H_1: \mu>\lambda$ is invariant under location change $g(X, Y)=(X+c, Y+c)$ , $c \in R$ . Derive the uniformly most powerful test of size $\alpha=0.05$ . My idea is to use the Neyman-Pearson Lemma. The likelihood ratio test is based on the ratio of the likelihoods under the two hypotheses: $$
\Lambda=\frac{L_0(x, y)}{ L_1(x, y)}=\frac{I[\lambda<x<\lambda +1]I[\mu <y <\mu+1]}{?}
$$ I am confused about the ratio. And how to proceed the next step? It seems that we need to find $$
\Lambda>k
$$","['statistics', 'probability-distributions', 'probability', 'hypothesis-testing']"
4732023,How could we define circulation over a surface or a tangential surface integral?,"The definition of surface integral of a vector function is about the projection of a function on the normal vector, so it is impossible for tangential vector fields to be intergrated over a surface, for the integral would be always zero. I would like to extend this thought as follows: I found this definition of $curl$ of a vector field: $$(\nabla \times \mathbf F)(p):=\lim_{V\to 0} \frac {1}{|V|} \oint_S \mathbf n \times \mathbf F dS, $$ where $p$ is a point inside a volume $V$ with boundary the surface $S$ . The vector $\mathbf n \times \mathbf F$ is in fact tangential to the surface, and it sounds nice to integrate over the surface to find the given total rate. But I cannot understand the exact meaning of this ""integral"". I would like to ask: How could we present the circulation over $a$ $surface$ -and not over a curve?","['integration', 'analysis', 'real-analysis']"
4732051,What differential equation has a Laplace transform that contains essential singularities?,"I would like to find a simple differential equation that, on taking the Laplace transform, results in essential singularities. I am looking, when teaching, to find examples of differential equations that results in each of the possible singularities on the complex plane. An example with poles is easy; any linear differential equation with constant coefficients will do. The Laplace transform of the homogeneous solution will give poles. The second order differential equation is a good simple candidate. Along these lines I have started with the equation in the Laplace domain of $L_t=-e^{-\frac{1}{s+\epsilon -i}}-e^{-\frac{1}{s+\epsilon +i}}$ where s is the Laplace variable and $\epsilon$ a parameter. I want real results from the inverse Laplace transform so have included the complex conjugate of the singularity located at $s=-\epsilon +i$ . A plot of the absolute value is Which shows the singularities clearly. Using Mathematica to get the inverse Laplace transform I get $f(t)=-e^{t (-\epsilon -i)} \left(\delta (t)-\frac{J_1\left(2 \sqrt{t}\right)}{\sqrt{t}}\right)-e^{t (-\epsilon +i)} \left(\delta
   (t)-\frac{J_1\left(2 \sqrt{t}\right)}{\sqrt{t}}\right)$ Where $J_1$ is the Bessel function of the first kind.  Plotting this result for $\epsilon =0.001$ gives Which looks complicated. What differential equation would have this as the homogeneous solution? Perhaps this question can be made simpler by alternative forms in the Laplace plane. Any suggestions?","['laplace-transform', 'ordinary-differential-equations']"
4732088,the discrete topology of an infinite set [duplicate],"This question already has an answer here : If a topology contains all infinite subsets, then it is the discrete topology (1 answer) Closed 12 months ago . Hi I am very new to topology and was wondering how to solve the following problem : Let X be an infinite set and τ a topology on X. If every infinite subset of X is in τ, prove that τ is the discrete topology. I have trouble understanding why this would be true. It goes against my intuition of an infinite set and I don't quite understand how this topology would be discrete. As you could still pick any finite subset of X. Can anybody tell me why this true? Thanks in advance.",['general-topology']
4732129,"How to prove that $\phi'(z)<0$ for $\theta\in (0,\pi)$?","Let $a_1=(1,0), a_2$ be two points on the unit circle T of the complex place $C$ . Assume that the angle between $a_1$ and $a_2$ are $\theta$ . See the below graph: Define the function $$
r(z)=\frac{1}{z-a_1}+\frac{1}{z-a_2}
$$ Define the integral: $$
\phi(\theta):=\int_D |r(z)|dz
$$ where $D$ is a unit disk. I guess that $\phi(\theta)$ is decreasing on $\theta\in (0,\pi)$ and increasing on $\theta\in (\pi,2\pi)$ . Moreover, $\phi'(z)=0$ as $z=\pi$ . How to prove that $\phi'(z)<0$ for $\theta\in (0,\pi)$ ? I am not sure how to compute the integral over the disk... I know how to integral it around a closed path.","['complex-analysis', 'analysis']"
4732141,Accumulation in the zeroes of the rate function (Large deviation principle),"Consider a measure space $(\mathcal{X},\mathcal{B})$ , where $\mathcal{X}$ is Polish and $\mathcal{B}$ is the Borel $\sigma$ -field. Let $(\mu_n)$ be a sequence of probability measure that satisfies the Large Deviation Principle with rate function $I$ , i.e. $I \geq 0$ and $I$ is lower semicontinuous. I want to show that every accumulation point of $(\mu_n)$ (every probability $\mu$ such that exist a subsequence $\mu_{n_k}$ which converges in distribution to $\mu$ ) is concentrated in the zeroes of $I$ , that is $\mu(\{I = 0\}) = 1$ . This should be a pivotal fact for the theory but I cannot find the proof anywhere. I only showed that if $C$ is a closed set that does not contain any zeroes of $I$ , $\mu_n(C) \rightarrow 0$ as $n \rightarrow \infty$ . This however should not imply in general that $\mu(C)=0$ , but maybe I'm missing something about the polish hypothesis or i have to add some other one. Thanks for reading!","['measure-theory', 'weak-convergence', 'borel-measures', 'large-deviation-theory', 'probability-theory']"
4732226,Fake proof that the difference of two consecutive cubes is prime [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question We can write $(x+1)^3 - x^3$ as the difference of two consecutive cubes, then simplify it to $3x^2 +3x+1$ , which as the discriminant is less than $0$ we cannot write it as a product of two linear factors without using complex numbers, therefore the quadratic is always prime. Where is the flaw in this argument?","['algebra-precalculus', 'fake-proofs', 'quadratics', 'prime-numbers']"
4732262,"Given a Square $ABCD$, find triangle Area $x$ if the area of the orange triangle is $24$","This is a very nice problem I came across on Instagram, so I’ve decided to post it here. In the diagram below, we have a square $ABCD$ with some triangles in it including a small equilateral triangle, if the area of the orange triangle is $24$ , the goal is to find the area of the blue triangle labeled $x$ : I’ll admit, this was a bit of a tricky problem, I first attempted to the length measures of the orange triangle, but that didn’t leave anywhere. I’m going to post my successful approach as an answer below, please let me know, if my answer is accurate or if my method is correct. Furthermore, please also share your own solutions as there are likely many different ways to solve this.","['euclidean-geometry', 'geometry', 'solution-verification', 'triangles', 'trigonometry']"
4732281,"In a normed space, is the sum of two open sets (open wrt subspace topology of two complements) open?","Let $(V,\|\cdot\|)$ be a (possibly infinite-dimensional) normed space and $V_1,V_2 \subseteq V$ be subspaces such that $V = V_1 \oplus V_2$ . Let $B^i_{r_i} \subseteq V_i$ be open balls in $V_i$ , wrt the induced norms $\|\cdot\|_i$ , of radius $r_i > 0$ . Define the sum of these two sets as $$
B_{r_1,r_2} := B^1_{r_1} + B^2_{r_2} := \{y_1 + y_2 \in V\ |\ y_i \in B^i_{r_i}, i = 1, 2\}
$$ My question is whether this set is open or not. My idea is: If we have $r := \inf \{\|x\|\ |\ x \in \overline{B^i_{r_i}} + \partial B^j_{r_j}\} > 0$ , $i \neq j$ , then $B_{r} \subseteq B_{r_1,r_2}$ . But I don't know how to show exactly, or even if it helps or the set is open at the first place.","['general-topology', 'topological-vector-spaces', 'functional-analysis']"
4732347,Consider the set $A_N$ of all fractions $\left(\frac{3}{2}\right)^n \pmod{1}$ for $n\le N.$ Prove that $\min(A_N)→0$ as $N→∞.$,"This is related to the well-known unsolved problem in number theory that concerns the distribution of $(3/2)^n \pmod{1}$ . This sequence is believed to be uniformly distributed. Has this simpler problem been proven before? I think that it may be done by a simple proof by contradiction, but my main concern is if it has been done before. The set $A_7$ is $\{1/2, 1/4, 3/8, 1/16, 19/32, 25/64, 11/128\}.$ It seems very intuitive that the lower limit is 0. To clarify, this question applies to n, where n is a natural number.","['modular-function', 'uniform-distribution', 'fractional-part', 'discrete-mathematics', 'limits']"
4732360,For which topological spaces does the closure commute with the interior?,"For which topological spaces $X$ is it the case that for all $Y \subseteq X$ , $\mathrm{cl}(\mathrm{int}(Y)) = \mathrm{int}(\mathrm{cl}(Y))$ ? If one replaces equality with inclusion (i.e., $\mathrm{cl}(\mathrm{int}(Y)) \subseteq \mathrm{int}(\mathrm{cl}(Y))$ ), one gets the extremally disconnected spaces . So, such spaces should be special kinds of extremally disconnected spaces. Any discrete space clearly satisfies the condition. Indiscrete spaces with at least two points do not satisfy the condition, however, as for any nonempty proper subset, the closure of the interior is empty, while the interior of the closure is the whole space. Consider the Sierpinski space $(\{0,1\},\{\emptyset, \{1\}, \{0,1\}\})$ . Let's check the condition for each of the four subsets: $\mathrm{cl}(\mathrm{int}(\emptyset)) = \mathrm{cl}(\emptyset) = \emptyset$ and $\mathrm{int}(\mathrm{cl}(\emptyset)) = \mathrm{int}(\emptyset) = \emptyset$ $\mathrm{cl}(\mathrm{int}(\{0\})) = \mathrm{cl}(\emptyset) = \emptyset$ and $\mathrm{int}(\mathrm{cl}(\{0\})) = \mathrm{int}(\{0\}) = \emptyset$ $\mathrm{cl}(\mathrm{int}(\{1\})) = \mathrm{cl}(\{1\}) = \{0,1\}$ and $\mathrm{int}(\mathrm{cl}(\{1\})) = \mathrm{int}(\{0,1\}) = \{0,1\}$ $\mathrm{cl}(\mathrm{int}(\{0,1\})) = \mathrm{cl}(\{0,1\}) = \{0,1\}$ and $\mathrm{int}(\mathrm{cl}(\{0,1\})) = \mathrm{int}(\{0,1\}) = \{0,1\}$ So, the Sierpinski space is a nondiscrete space satisfying the condition. More generally, any extremally disconnected space that is also a door space (i.e., every subset is open, closed, or both) satisfies the condition. Indeed, if $Y$ is open, then $\mathrm{cl}(Y)$ is also open, and so both sides are equal to $\mathrm{cl}(Y)$ , while if $Y$ is closed, then $\mathrm{int}(Y)$ is also closed, and so both sides are equal to $\mathrm{int}(Y)$ . If $Y$ is clopen, then both sides are equal to $Y$ . The Sierpinski space is an example of an extremally disconnected door space, and of course, discrete spaces are also extremally disconnected door spaces. The disjoint union of a family of spaces $(X_i)$ satisfies the condition if and only if each $X_i$ individually satisfies the condition. So, the disjoint union of the Sierpinski space with itself (i.e., $\{0,1,2,3\}$ with the open sets being the subsets that do not contain $0$ without also containing $1$ , nor contain $2$ without also containing $3$ ) satisfies the condition but is not a door space, as $\{0,3\}$ and $\{1,2\}$ are neither open nor closed.",['general-topology']
4732375,What is the property called wherein permuting the input tuple of a function results in the same permutation in the output tuple?,"A function f takes an n -tuple as input and gives an n -tuple as output. If permuting the elements of the input n -tuple of this function always results in the same permutation of the elements of the output n -tuple of this function, what is this property called? For example, if $f(x_1, x_2, x_3) = (y_1, y_2, y_3)$ , then $f(x_1, x_3, x_2) = (y_1, y_3, y_2)$ .","['permutations', 'functions', 'convention', 'group-actions', 'terminology']"
4732398,Triple integral of the great-circle distance function,Numerical integration suggests that $$\mathcal U=\int_0^\pi\int_0^\pi\int_0^\pi\arccos\left(\cos x\cdot\cos y+\sin x\cdot\sin y\cdot\cos z\right) dx dy dz\stackrel{\small\color{gray}?}=\frac{\pi^4}2\tag1$$ (note that the function being integrated represents the great-circle distance $^{[1]}$ $\!^{[2]}$ $\!^{[3]}$ in spherical coordinates). How can we prove it? Is it possible to find a closed form for this one? $$\mathcal W=\int_0^{\tfrac\pi2}\!\int_0^\pi\int_0^\pi\arccos\left(\cos x\cdot\cos y+\sin x\cdot\sin y\cdot\cos z\right) dx dy dz\tag2$$,"['integration', 'spherical-trigonometry', 'conjectures', 'definite-integrals', 'closed-form']"
4732400,"If $f(0)=g(0)=0$ and $\int_0^{\epsilon} f(x)g(x) dx >0$ for any small $\epsilon>0$, do $f(x)$ and $g(x)$ have same sign near $x=0$?","Let $f,g : [0,\infty) \to \mathbb{R}$ be smooth functions such that $f(0)=g(0)=0$ $\int_0^{\epsilon} f(x)g(x)dx >0$ for any small $\epsilon>0$ . Then, I would like to investigate about the sign of $f(x)$ and $g(x)$ near $x=0$ . Is it possible to conclude that there exists some $\epsilon'>0$ such that $f(x)g(x) \geq 0$ for $x \in [0,\epsilon']$ ?","['monotone-functions', 'analysis', 'real-analysis', 'inequality', 'derivatives']"
4732438,Finding $\lim_{x \to 0^{+}} x^{\frac{1}{x}}$ by L'Hôpital's Rule.,"I am a 76 years old retiree who loves to read math textbooks. Yeah, crazy, right? The current textbook I am using is Larson's Calculus, 12e.
Section 5.6, exercise 50 is stumping me: $$\lim_{x \to 0^{+}} x^{\frac{1}{x}}$$ The thrust of this section is to learn how to use L'Hôpital's Rule, particularly to manipulate a limit in indeterminate form, like this one, ( $0^{\infty}$ ), into a suitable form.
Since this exercise has a moving exponent, I've used Logarithmic Differentiation: $$\ln y = \lim_{x \to 0^{+}} \left(\frac{1}{x} \cdot \ln x\right) = \infty \cdot \left(-\infty\right)$$ and again: $$= \lim_{x \to 0^{+}} \frac{\ln x}{x} = \frac{-\infty}{0}$$ But here, I don't know what to do. My graphing calculator says that in the original form of the exercise, the limit = 1. So, I figure I have to come up with a way to get to: $$\ln y = 0$$ $$y = e^{0} = 1$$ Any help is appreciated. Thanks, Jose",['limits']
4732462,"""Proving"" $L^q \subseteq L^p$ for any $p <\infty$.","In Folland's Real Analysis, he gives the following proof (Proposition 6.12) that $L^q \subseteq L^p$ when $0 \leq p < q \leq \infty$ . My question is, nowhere in the proof is the assumption that $p < q$ used. Couldn't this proof be used to ""prove"" that the claim holds even if $q < p$ (which I know is not true, by counterexamples)? Where would that proof breakdown?","['measure-theory', 'lp-spaces', 'analysis']"
4732464,Real part of the solution is not a solution?,"Take for example this very simple differential equation with complex coefficients: \begin{equation}\tag{1}
f'(t) = - i f(t) 
\end{equation} where t is a real variable. The solution is $f(t)=c e^{-i t}$ . Since in Physics only real quantities have sense, a physicist would take only the real part of this solution, i.e. $f(t)= \cos(t)$ . However, it is easy to see that $f(t)=\cos(t)$ is not a solution of (1) anymore. So, the question is: Of which equation is $f(t)=\cos(t)$ the solution? Is there a connection with Eq.(1) ? PS. If one looks for real $f$ solutions, then from Eq.(1) we must have $f'(t)=0$ and $-i f(t)=0$ , i.e. $f(t)=0$ , which is another different solution ! Which is the correct answer ?","['ordinary-differential-equations', 'solution-verification', 'partial-differential-equations', 'physics', 'complex-numbers']"
4732511,What is the distribution of distances between two random points in RGB space?,"Suppose we pick pairs of triples from $\{ 0, 1, 2, \dots, 255\}^3$ with a uniform distribution and would like to find a closed form for the distribution of the Euclidean distances $$ d((x_1, x_2, x_3), (y_1, y_2, y_3)) = \left( \sum_{j=1}^3 (x_j-y_j)^2 \right)^{1/2}.$$ The motivation for this question comes from the cubic lattice that is one of the RGB color spaces . It would be nice to have an exact probability distribution because it would likely have interesting applications in image processing. This question gives the answer for when a cube includes all real numbers in it, and there's Mathai et al. $^\color{magenta}{\star}$ , but what about when it is discrete, like for a cubic lattice ? $\color{magenta}{\star}$ Arak M. Mathai, Peter Moschopoulos, Giorgio Pederzoli, Distance between random points in a cube [ PDF ], Statistica, Volume 59, Number 1, 1999.","['discrete-geometry', 'probability-distributions', 'image-processing', 'discrete-mathematics', 'probability']"
4732520,Why can't I use the power rule on this partial derivative?,"I came across this question on Khan Academy: let $f(x, y, z) = \sqrt{xyz}$ , find $\frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}$ My first intuition was to use the power rule: $$
\frac{\partial f}{\partial x}=\sqrt{yz}\cdot\frac{\partial}{\partial x}\big(\sqrt{x}\big)=\frac{\sqrt{yz}}{2\sqrt{x}}
$$ $$
\frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}=\frac{\sqrt{yz}}{2\sqrt{x}}\bigg\rvert_{(-1, -1, 4)}=\frac{\sqrt{(-1)(4)}}{2\sqrt{-1}}=\frac{i\sqrt{4}}{2i}=\frac{2i}{2i}=1
$$ However, the official solution used the chain rule and arrived at a different result $$
\frac{\partial f}{\partial x}=\frac{\partial f}{\partial (xyz)}\cdot\frac{\partial (xyz)}{\partial x}=\frac{yz}{2\sqrt{xyz}}
$$ $$
\frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}=\frac{yz}{2\sqrt{xyz}}\bigg\rvert_{(-1, -1, 4)}=\frac{(-1)(4)}{2\sqrt{(-1)(-1)(4)}}=\frac{-4}{2\sqrt{4}}=\frac{-4}{4}=-1
$$ Where was my mistake? If it was with the differentiation, what are some situations where I'm not allowed to take the power rule? If it was with complex numbers, what would be a general rule to prevent myself from making this mistake again? (I know $\sqrt{(-1)(-1)} \neq\sqrt{-1}\cdot\sqrt{-1}$ but that's about it)","['partial-derivative', 'multivariable-calculus', 'complex-numbers']"
4732528,Thomas plank is not realcompact,"Let $X = \bigcup_{n\geq 0} L_n$ where $L_n = [0, 1)\times\{1/i\}$ for $i > 0$ and $L_0 = (0, 1)\times \{0\}$ . Define the topology on $X$ as follows: each point $(x, 1/i)$ for $x\in (0, 1)$ and $i > 0$ is isolated, neighbourhoods of $(0, 1/i)$ are subsets $A$ of $L_i$ containing $(0, 1/i)$ with $L_i\setminus A$ finite, and neighbourhoods of $(x, 0)$ are sets $\{(x, 0)\}\cup \{(x, 1/i) : i\geq N\}$ . With this topology, $X$ is called Thomas plank, and it's an example of a Tychonoff space. Is $X$ realcompact?","['tychonoff-spaces', 'examples-counterexamples', 'separation-axioms', 'realcompact-spaces', 'general-topology']"
4732572,Probability of circumcircle of 3 random points in a circle to be contained within that circle,"Imagine a circle $C_1$ that has radius $a$ . We then chose
at random, and independently, three points from the interior of that
circle. These three points, if non-collinear, uniquely determine
another circle $C_2$ ; $C_2$ may or may not be totally contained within $C_1$ .
What is the probability that $C_2$ lies totally inside $C_1$ ? This is from Paul Nahin's book 'Inside Interesting Integrals' (pg. 25 in second edition) Below is the full analytic solution given in the book. However, it goes on to discuss that performing a Monte Carlo simulation gives a value in the range $0.39992$ to $0.400972$ while our answer is $0.418879$ ; alternatively, the answer seems to be $0.4$ which is $\frac{2\cdot3}{15}$ , while our answer is $\frac{2\cdot \pi}{15}$ . So where exactly did we make a mistake? Here are my thoughts- When we are considering all those $C_2$ inside $C_1$ , there will clearly be overlap between the cases. Say case 1 is circle of radius $\frac{a}{3}$ centered at $\frac{a}{2}$ , and case 2 is same radius but centered at $\frac{a}{4}$ , these will have some intersection area along radical axis. Here he seems to have assumed the two cases to be disjoint, but are they really? We call it as 'probability of all 3 being in circumference band of $C_2$ , but in reality ofcourse it is sum over its own continuum of cases- probability of all 3 being in this little arc of the $\Delta(x)$ band of C2 + probability of all 3 being in the next little arc of C2 + and so on. NOW my point is- that little intersection area along radical axis, it will appear in this expansion in both cases, so we are in fact overcounting it. It seems insignificant since we're thinking of only that particular pair, but any particular intersection area square in the plane will be counted infinitely many times (there’s an infinitude of circles of just the right appropriately diff radius and centers that intersect at the exact same place). In other words, number of times an intersection area is overcounted is proportional to number of pairs of circles, but number of times it is supposed to be counted is proportional to just the number of circles.","['integration', 'geometry', 'calculus', 'solution-verification', 'probability']"
4732622,"If $f_n,g_n: [0,1] \to [0,1]$ continuous $\forall n \in \mathbb{N}$ and converge uniformly prove that $f_n \circ g_n \xrightarrow{u.c} f\circ g$","Assume that $f_n,g_n$ converge uniformly to $f,g$ and $[0,1]\subseteq (\mathbb{R},|\cdot|)$ . My work so far:
From the initial assumption we get that $|f_n(x)|\leq 1$ and $|g_n(x)|\leq 1$ $\forall n \in \mathbb{N}$ . Since $f_n\xrightarrow{u.c} f$ and $g_n\xrightarrow{u.c} g$ there exists $n_0 \in \mathbb{N}$ such that if $n\geq n_0 \Rightarrow |f_n(x)-f(x)|<\epsilon/2$ and $|g_n(x)-g(x)|<\epsilon/2 \ \forall x\in [0,1]$ . Now if $n\geq n_0$ we notice that: \begin{align*}
|f_n\circ g_n(x) -f\circ g(x)|&=|f_n\
z g_n(x)-f_n\circ g(x)+ f_n\circ g(x)-f\circ g(x)| \\
&\leq |f_n \circ g_n(x)-f_n\circ g(x)|+|f_n\circ g(x)-f\circ g(x)|
\end{align*} Can someone help me proceed? Is it safe to say that its part of the last sum is $<\frac{\epsilon}{2}$ and why so? Thanks in advance","['functions', 'functional-analysis', 'uniform-convergence', 'real-analysis']"
4732627,"Definition of a function, convention","If $f: [0,1] \rightarrow \mathbb{R}$ , $f(x) = \sin x$ If $g: [1/2, 2] \rightarrow \mathbb{R}$ , $g(x) = \cos x$ Is $h(x)= f(x) + g(x)$ a valid function directly or we need to specify $h:[1/2,1] \rightarrow \mathbb{R}$ and then say $h(x) = f(x) + g(x)$ I wish to know is it automatic or needs clarification?",['functions']
4732679,"$A+B$ and $AB$ are nilpotent matrices, are $A$ and $B$ nilpotent?","Suppose $A+B$ and $AB$ are nilpotent matrices, I'm thinking whether $A$ and $B$ are nilpotent. If $A$ and $B$ transform $A$ and $B$ into Jordan form at the same time, then we can just calculate the Jordan block to see that $A$ and $B$ aren't nilpotent. But the general case is $A$ and $B$ cannot be turned into Jordan form at the same time, what can we do? Any help would be appreciated!","['matrices', 'linear-algebra']"
4732729,Problem with converting predicate expression to Prenex Normal Form,"if I have the following predicate: $$\exists x \neg P(x) \lor (\exists{x} \neg Q(x) \land \forall x \neg R(x))$$ What I did: First, I used the tautology $\forall x A(x) \land \exists x B(x) = \forall y \exists x(A(y) \land B(x))$ to transform the right part of the disjunction , so I got $\exists x \neg P(x) \lor \forall y \exists x( \neg Q(x) \land \neg R(y))$ Now, I put the leftmost existential quantifier before the brackets so I get: $\exists x (\neg P(x) \lor \forall y \exists x( \neg Q(x) \land \neg R(y)))$ Now, we can move the universal quantifier also to the left so we get: $\exists x \forall y( \neg P(x) \lor \exists x( \neg Q(x) \land \neg R(y)))$ Now comes my problem: I was taught that if I want to move the existential quantifier $\exists x$ to the left, I need to rename the variable $x$ because by moving to the left, we encounter a predicate that has $x$ in it (which is $\neg P(x)$ ). So, I rename $x$ and all occurences of it after the existential quantifier  to $z$ : $\exists x \forall y( \neg P(x) \lor \exists \color{red}z( \neg Q(\color{red} z) \land \neg R(y)))$ so finally I  get: $\exists x \forall y \exists z( \neg P(x) \lor ( \neg Q(z) \land \neg R(y)))$ . However, my workbook got a different solution. They didn't employ the usage of the same tautology I used in step 1 so instead of getting $\forall y \exists x (\neg R(y) \land \neg Q(x))$ , they simply got $\exists x(\neg Q(x) \land \forall y \neg R(y))$ , so they ended up with the final result being $\exists x \forall y (\neg P(x) \lor (\neg Q(x) \land \neg R(y)))$ Can anyone tell me what I'm missing?","['predicate-logic', 'first-order-logic', 'quantifiers', 'logic', 'discrete-mathematics']"
4732743,"Find the general term of $a_{2k}=pa_k+qa_{k+1},a_{2k+1}=ra_k+sa_{k+1}$","The question Given that the initial terms $a_1$ , $a_2$ and the recursion: $$
\left\{
\begin{matrix}
a_{2k}=pa_k+qa_{k+1}\\
a_{2k+1}=ra_k+sa_{k+1}\\
\end{matrix}
\right.
$$ Find the general term of $a_n$ in term of $a_1, a_2, p, q, r, s$ and $n$ My work: Let $
A_1=
\begin{pmatrix}
r&s&0\\
0&p&q\\
0&r&s\\
\end{pmatrix}
$ , $
A_0=
\begin{pmatrix}
p&q&0\\
r&s&0\\
0&p&q\\
\end{pmatrix}
$ , $v_1=\begin{pmatrix}
a_1\\
a_2\\
a_3\\
\end{pmatrix}$ and $v_0=\begin{pmatrix}
a_2\\
a_3\\
a_4\\
\end{pmatrix}$ Then we have: $$
\begin{pmatrix}
a_{2k}\\
a_{2k+1}\\
a_{2k+2}\\
\end{pmatrix}
=
A_0\begin{pmatrix}
a_{k}\\
a_{k+1}\\
a_{k+2}\\
\end{pmatrix}
,
\begin{pmatrix}
a_{2k+1}\\
a_{2k+2}\\
a_{2k+3}\\
\end{pmatrix}
=
A_1\begin{pmatrix}
a_{k}\\
a_{k+1}\\
a_{k+2}\\
\end{pmatrix}
$$ For any integer $n$ , $\begin{pmatrix}
a_{n}\\
a_{n+1}\\
a_{n+2}\\
\end{pmatrix}$ can be written as a product of some $A_1$ and $A_0$ actting on $v_1$ or $v_0$ . Their order depends on the binary form of $n$ For example: $6=110_{(2)}$ ，so $$\begin{pmatrix}
a_{6}\\
a_{7}\\
a_{8}\\
\end{pmatrix}=A_0A_1v_1$$ $45=101101_{(2)}$ ，so $$\begin{pmatrix}
a_{45}\\
a_{46}\\
a_{47}\\
\end{pmatrix}=A_1A_0A_1A_1v_0$$ However, it is difficult to find the closed form of the product as $A_1,A_0$ have different eigenvalues. Special Case I: When $p=1+\frac{a}{d},q=1-\frac{a}{d},r=\frac{a}{d},s=2-\frac{a}{d}, a_1=a, a_2=a+d$ , then: $$a_n=a+(n-1)d$$ i.e. ${a_n}$ is an arithmetic sequence.","['recursion', 'recurrence-relations', 'sequences-and-series']"
4732748,"A weaker condition than the operation-preserving one, for a weaker result.","Let $G$ and $\overline G$ be groups, and $f\colon G\to\overline G$ a bijection. Then: $$\forall g,h\in G: f(gh) = f(g)f(h) \tag1$$ if and only if: $$\mathfrak c=\psi_f\bar{\mathfrak c}f \tag2$$ where $\mathfrak c$ and $\bar{\mathfrak c}$ are Cayley's embedding of $G$ and $\overline G$ into $\operatorname{Sym}(G)$ and $\operatorname{Sym}(\overline G)$ , respectively, and $\psi_f\colon\operatorname{Sym}(\overline G)\to\operatorname{Sym}(G)$ is defined by $\sigma\mapsto f^{-1}\sigma f$ . Question . Is there any weaker (than $(1)$ ) condition on the bijection $f$ , which ensures the weaker (than $(2)$ ) result: $$\operatorname{im}(\mathfrak c)=\operatorname{im}(\psi_f\bar{\mathfrak c}f) \tag3$$ (here "" $\operatorname{im}(\_)$ "" stands for ""the image of"")? Motivation . Intuitively, I'd expect $(3)$ being sufficient to get $G$ and $\overline G$ ""somehow isomorphic"".","['group-theory', 'group-isomorphism']"
4732802,How to interpret this sum in Tate's thesis?,"Let $f$ be a Schwartz function on the ring of adeles $\mathbb{A}$ of a number field $K$ , and $d^\times x$ the multiplicative Haar measure on $\mathbb{A}^\times$ . One can embed $K^\times$ diagonally in $\mathbb{A}^\times$ and can take a quotient $\mathbb{A}^\times/K^\times$ . In Tate's thesis, the integral of $f$ over $\mathbb{A}^\times$ is replaced with an integral over $\mathbb{A}^\times/K^\times$ via $$\int_{\mathbb{A}^\times}f(x)d^\times x = \int_{\mathbb{A}^\times/K^\times}\left(\sum_{\kappa \in K^\times}f(\kappa x)\right) d^\times x.$$ I am wondering how to interpret this sum. In particular, why does it converge, and how big is $\mathbb{A}^\times/K^\times$ ? For example, if in the real archimedean component $f_\infty(x) = e^{-\pi x^2}$ , is he integrating over a function which in this component is equal to $$\sum_{\kappa \in K^\times}e^{-\pi \kappa^2 x^2} $$ and does this converge? Also, how does this relate to the Jacobi theta function?","['measure-theory', 'fourier-analysis', 'number-theory', 'analysis', 'analytic-number-theory']"
4732834,Norm of elementary tensors in projective tensor product of Banach spaces,"Let $E$ and $F$ be Banach spaces. We define the projective norm of the elementary tensor product of $E\otimes F$ as follows- For $t\in E\otimes F$ , define $\lVert t\rVert_{\wedge} =\text{inf}\{\sum\limits_{i=1}^n \lVert a_i\rVert\lVert b_i\rVert:\ t=\sum\limits_{i=1}^n a_i\otimes b_i, a_i\in E, b_i\in F\}$ .Now take the completion with respect to this norm and call it $E\hat\otimes F$ . I have to prove that $\lVert a\otimes b\rVert_{\wedge}=\lVert a\rVert\lVert b\rVert$ . It's obvious from the definition that $\lVert a\otimes b\rVert_{\wedge}\le\lVert a\rVert\lVert b\rVert$ Now I have to show the converse inequality i.e. if $a\otimes b=\sum\limits_{i=1}^n a_i\otimes b_i$ , we must have $\lVert a\rVert\lVert b\rVert\le \sum\limits_{i=1}^n  \lVert a_i\rVert\lVert b_i\rVert$ Can anyone suggest me a hint to prove this? Thanks for your assistance in advance.","['tensor-products', 'banach-spaces', 'functional-analysis', 'operator-algebras']"
4732837,Equalities involving Exponential Distributions and Markov process,"I am trying to understand Remark 17.26 of the book Probability Theory by A. Klenke (3rd version), where the author is showing how a condition on the Q-matrix of a discrete Markov process in continuous time is crucial to guarantee that the process does not explode. He defines: $T_1,T_2,...$ as independent exponential random variables with parameter $n^2$ $S_n=T_1+...+T_{n-1}$ $X_t=\text{sup}\{ n \in \mathcal{N_0}: S_n\leq t\}$ I do not understand how he concludes that $$\lim_{s\downarrow 0} s^{-1}P[X_{t+s}=n+1|X_t=n]=n^2$$ and $$\lim_{s\downarrow 0} s^{-1}(P[X_{t+s}=n|X_t=n]-1)=-n^2$$ from this result $$P[X_{t+s}\geq n+1|X_t=n]= \\ P[S_{n+1}\leq t+s|S_n\leq t, S_{n+1}>t]=
\\P[T_n\leq s+t-S_n|S_n\leq t, T_n > t- S_n] = 
\\P[T_n \leq s]=1-\exp(-n^2s)
$$ In particular, I do not understand: Why the event $\{X_{t+s}\geq n+1|X_t=n\}$ is equivalent to $\{S_{n+1}\leq t+s |X_t=n \}$ in the first above equality. Where are the events $X_{t+s}=n+2, n+3, ...$ and so on? How to calculate $P[X_{t+s}=n+1|X_t=n]$ and $P[X_{t+s}=n|X_t=n]$ from the result. Thanks for the help. Let me know if more contest is needed.","['exponential-distribution', 'markov-process', 'probability-theory', 'markov-chains']"
4732852,"Evaluate the integral $\int \frac{1 + x\cos(x)}{x(1-x^2e^{2\sin(x)})}\, dx$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question I'm trying to evaluate the following integral: $$\int \frac{1 + x\cos(x)}{x(1-x^2e^{2\sin(x)})}\, dx$$ I have no idea how to approach this problem at all. Could someone please guide me through the steps to solve this integral? Are there any specific trigonometric identities or techniques that I should apply? Any insights or hints would be greatly appreciated. Thank you in advance for your assistance!","['integration', 'calculus', 'exponential-function', 'trigonometry']"
4732857,"Is there a generalization of the following equality, for arbitrary fractions of $2\pi$?","In the context of group representations of the Icosahedral group symmetry, I noticed that: $$ \cos^2\left(\frac{2\pi}{5}\right) + \cos^2\left(\frac{4\pi}{5}\right) = \frac{3}{4}$$ Is it possible to generalize this for other fractions of $2\pi$ ? In Wikipedia's List of trigonometric identities article, I found Euclid's identity somewhat similar, but I'm not sure it is related, and still I wonder whether a generalization of it exists.","['euclidean-geometry', 'trigonometry']"
4732867,When are complicated trig functions used?,"I am teaching my son calculus. Sometimes there are very tough integrals that involve functions like "" $\sin(2x)\tan(x+1)$ ."" These are sometimes raised to various powers. In any event, the functions have multiple trigonometric functions - could be any of the $6$ trigonometric  functions - multiplied or divided by other trigonometric functions. My question: what are the practical application of these complicated trigonometric  functions? Where are these functions encountered in real life? I'm not a physicist or advanced mathematician, so I'm interested in learning where these complicated trig functions show up in engineering or advanced science. When would I see something that is modeled by these things, because I never run across them myself? EDIT: It's not just integrals. I just saw this on twitter. Where would you see this function in the wild?","['trigonometry', 'trigonometric-integrals']"
4732889,Evaluate $a^7+b^7+c^7+\dfrac {1}{a^7}+\dfrac {1}{b^7}+\dfrac {1}{c^7}$,"Let $a,b,c$ be the distinct complex roots of $P(x)=x^3-x-1$ . Evaluate $$a^7+b^7+c^7+\dfrac {1}{a^7}+\dfrac {1}{b^7}+\dfrac {1}{c^7}$$ My approach According to Vieta formulae we have that $abc=1$ and $a+b+c=0$ and $ab+bc+ac=-1$ . Hence $$-1=ab+bc+ac=\frac {ab+bc+ac}{abc}=\frac {1}{a}+\frac {1}{b}+\frac {1}{c}$$ So, here is my thought. I'm considering expanding $\bigg(\dfrac {1}{a}+\dfrac {1}{b}+\dfrac {1}{c}\bigg)^7=-1$ and $(a+b+c)^7=0$ together to derive $\dfrac {1}{a^7}+\dfrac {1}{b^7}+\dfrac {1}{c^7}$ and $a^7+b^7+c^7$ . The only tricky part is that these expansions are so messed up and complicated.  Should I continue on this path? Is it really reasonable ?","['contest-math', 'algebra-precalculus']"
4732900,"What is the exact function or algorithm Windows 10 is using to calculate taskbar underline color from Registry values, based on data I have collected?","In earlier versions of Windows, it was easy for users to set the exact colors of interface elements. For example, in Windows 7, you could go to [Control Panel -> Personalization -> Window Color], which would bring up a dialog box letting you select specific interface elements and set the color for each of them. In Windows 10, much of that functionality was removed; you can go to [Settings -> Personalization -> Colors], but it will only let you choose an accent color as an overall theme, and the exact colors of interface elements are then all automatically calculated as variations from your accent color. However, as explained in such links as...: https://old.reddit.com/r/windows/comments/3fdwo5/how_to_change_your_accent_color_to_anything_you/ctnujt5 https://www.superuser.com/questions/1558106/color-of-active-window-in-windows-10-taskbar https://www.superuser.com/questions/1717690/make-active-window-more-obvious-on-taskbar-windows-10 ...it is possible to edit the Registry binary value HKEY_CURRENT_USER\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\Accent\AccentPalette to set the colors of elements of the Windows taskbar. To summarize the Reddit link and the results of my own experiments, AccentPalette consists of the bytes GG GG GG 00 HH HH HH 00
JJ JJ JJ 00 KK KK KK 00
LL LL LL 00 MM MM MM 00
NN NN NN 00 PP PP PP 00 These are RGB triplets delimited by a 00 null byte. Each RGB triplet is used by Windows to determine the color of a different element of the interface. These RGB triplets are generally applied to the interface without any modification to their value. For example, LL LL LL is used as the exact background color of the active taskbar button (the button in the taskbar corresponding to the active window), and MM MM MM is used as the exact background color of inactive taskbar buttons (buttons in the taskbar corresponding to inactive windows). The HH HH HH RGB triplet determines the color of the underline beneath taskbar buttons with an open window. The same underline color is used for all open windows, both active and inactive. That underline is a light blue line in the example images here (re-used from the Superuser questions above): The difficulty is that unlike the other RGB triplets, Windows 10 does not use this HH HH HH value without modification. If all three color components are greater than or equal to the hex value B3, then Windows uses the triplet as the color of the underline without modification, but if any of the three color components are less than B3, then Windows applies some sort of lightening blend/transformation to the triplet to get the final color. Let us designate the three color components (red, green, and blue) of this HH HH HH Registry AccentPalette triplet as $S_R$ , $S_G$ , and $S_B$ , and the three color components of the final taskbar underline color as $T_R$ , $T_G$ , and $T_B$ . My question is, based on the data below, what is the exact function or algorithm $f(S_R, S_G, S_B) = (T_R, T_G, T_B)$ of this blend/transformation? Based on the data below, how exactly does Windows 10 calculate the final taskbar underline RGB color from the HH HH HH triplet? Here is a table of data I have manually gathered by editing the Registry, restarting Explorer so that Windows 10 will apply the change, taking screenshots, and then using an image editor's eyedropper tool to determine the final resulting color. The values are all in hex. This is overall an extremely tedious process, hence the limited data (and obviously, I can't manually test all 256 x 256 x 256 = 16,777,216 possible RGB triplets). $S_R$ $S_G$ $S_B$ $T_R$ $T_G$ $T_B$ 00 00 00 B3 B3 B3 00 00 01-B3 75 75 B3 00 00 B4 75 75 B4 00 00 B5 76 76 B5 00 00 B6 76 76 B6 00 00 B7 77 77 B7 00 00 B8 93 93 C6 00 00 B9 78 78 B9 00 00 BA 79 79 BA 00 00 BB 7A 7A BB 00 00 C0 7D 7D C0 00 00 C8 82 82 C8 00 00 CC 85 85 CC 00 00 D0 87 87 D0 00 00 D8 8D 8D D8 00 00 DD 90 90 DD 00 00 E0 92 92 E0 00 00 E8 97 97 E8 00 00 F0 9C 9C F0 00 00 F8 A1 A1 F8 00 00 FF A6 A6 FF 00 01 00 75 B3 75 00 01 01 75 B3 B3 00 01 02 75 94 B3 00 01 03 75 89 B3 00 01 04 75 84 B3 00 01 05 91 9A C2 00 01 06 75 7F B3 00 01 07 75 7D B3 00 01 08 75 7C B3 00 01 12-15 75 78 B3 00 01 16-1F 75 77 B3 00 01 20-40 75 76 B3 00 01 41-B3 75 75 B3 00 01 B4 75 76 B4 00 01 FF A6 A6 FF 74 75 76 B0 B1 B3 B0 B1 B2 B1 B2 B3 B1 B1 B1 B3 B3 B3 B1 B2 B3 B1 B2 B3 B1 B3 B4 B1 B3 B4 B2 B3 B4 B2 B3 B4 B3 00 B3 B3 75 B3 C0 D0 F0 C0 D0 F0 FF 00 FF FF A6 FF FF FF FF FF FF FF Some observations from my experimentation: As mentioned previously, if all three color components are greater than or equal to the hex value B3, then no change is applied. But if any of the three color components are less than B3, then Windows applies some sort of lightening blend/transformation to the triplet to get the final color. The result is deterministic. The same S values always result in the same T values. The result is independent in the sense that it is not affected by any other triplets in the AccentPalette value. Any permutation applied to the S values results in the same permutation being applied to the T values. For example, if $f(S_1, S_2, S_3) = (T_1, T_2, T_3)$ , then $f(S_1, S_3, S_2) = (T_1, T_3, T_2)$ . That is, the red, green, and blue components are interchangeable in the sense that any of them can be swapped, and the same components will be swapped in the result. The mapping is many-to-one. One example from the table above is that if $S_R$ = 00 and $S_G$ = 00, then any value of $S_B$ from 01 to B3 will give the result value (75, 75, B3). That is, $f(00, 00, 01-B3) = (75, 75, B3)$ . The function might look linear at a glance within some ranges, but a closer examination shows it is not truly linear. For example, look in the table above at the results in the range $f(00, 00, B4-B7)$ . Within that range, $T_B$ is simply $S_B$ , but for $T_R$ and $T_G$ , it increments first from 75 to 76, but then stays at 76 for another step before incrementing again to 77. The function generally is fairly smooth, but it has some wild discontinuities. As seen in the table above, it is smooth-ish in the ranges $f(00, 00, B4-B7)$ and $f(00, 00, B9-BB)$ , but $f(00, 00, B8)$ gives a wild jump that is far from the values surrounding it.","['data-analysis', 'functions', 'data-mining', 'algorithms']"
4732997,Expectation of r.v. in the proof of crossing number inequality,"I was reading the proof of crossing number inequality and there was one step in the proof which I cannot prove rigorously.  Firstly, let me remind the definition of the crossing number and then I state the lemma and its proof. I will not write the whole proof since I understood it pretty well except one moment. Definition The crossing number of a graph $G=(V,E)$ , denoted $\text{cr}(G)$ , is the smallest integer $k$ such that we can draw $G$ in the plane with $k$ edge crossings. Lemma (the crossing lemma) Let $G=(V,E)$ be a graph with $|E|\geq 4|V|$ . Then $\text{cr}(G)\geq \frac{|E|^3}{64|V|^2}.$ Proof. Consider a drawing of $G$ with $\text{cr}(G)$ crossings. Set $p=\frac{4|V|}{|E|}$ . The assumption of the lemma implies that $0<p\leq 1$ . We remove every vertex of $V$ from the drawing with probability $1-p$ (together with the edges adjacent to the vertex). More precisely, we consider the probability space $\Omega:=\{V': V'\subset V\}$ and we define probability as follows: $\mathbb{P}(V'):=p^{|V'|}(1-p)^{n-|V'|}$ , where $n=|V|$ . We consider the following three random variables: $$\xi_1:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_1(V'):=|V'|,$$ $$\xi_2:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_2(V'):=|E'|,$$ $$\xi_3:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_3(V'):=\text{cr}(G'),$$ where $|E'|$ is the number of edges in the induced subgraph $G[V']$ and $\text{cr}(G')$ is crossing number of the induced subgraph $G':=G[V']$ . It is not difficult to compute that $\mathbb{E}[\xi_1]=p|V|$ and $\mathbb{E}[\xi_2]=p^2|E|$ . I computed them by the definition of expectation. Can anyone show rigorously why $\mathbb{E}[\xi_3]\leq p^4\text{cr}(G)$ ? The rest of the proof makes sense to me. I'd be very happy if someone can show the detailed proof of this inequality. EDIT: For example, this is how I computed $\mathbb{E}[\xi_1]$ : since $\xi_1(V')=|V'|=\sum_{k=1}^{n}\mathbb{1}_{v_k}(V')$ , where $$\mathbb{1}_{x}(V')=\begin{cases}
1, & \text{if } x\in V' \\
0, & \text{if }x\notin V'
\end{cases}$$ By linearity of expectation we have $\mathbb{E}[\xi_1]=\sum_{k=1}^n \mathbb{E}[1_{v_k}]=\sum_{k=1}^n p=p|V|.$ I wonder if it can be shown that $\mathbb{E}[\xi_3]\leq p^4\text{cr}(G)$ rigorously as I did?","['graph-theory', 'additive-combinatorics', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics']"
4733006,Expected number of coin flips until all cars move to end of array?,"Imagine that we have an array of length $2n$ , where the first $n$ entries are a $C$ (representing a toy car) and the remaining $n$ entries are empty. Additionally, we have $n$ fair coins labeled $1$ through $n$ , where coin $i$ corresponds to car $C_i$ in the array. On each timestep, we flip all $n$ coins. If coin $i$ comes up as heads, then car $C_i$ moves forward in the array by one spot, but only if it is not blocked by another car directly in the slot in front of it. Else, if blocked or the coin comes up tails, car $C_i$ does nothing. The question has two parts: What is the expected number of timesteps until the $n$ - $th$ car reaches the end of the array (reaches slot $2n$ )? What is the expected number of timesteps until all of the $n$ cars have moved from the first $n$ slots of the array to the last $n$ slots? I have worked out part 1 as follows. The expected number of flips for one coin to land as heads is $2$ , and the $n$ - $th$ car has to move $n$ slots to get to the end (and is not blocked by anything ever), so the expected number of timesteps is $2n$ . However, I am lost on the approach to part 2. I reason that it should be on the order $O(n\log n$ ) but do not know how to proceed. I keep running into a long chain of conditional probabilities and wonder if there is a more elegant way I am missing.","['expected-value', 'probability', 'random-variables']"
4733009,A question about $\int_{0}^{\pi/2} \ln(\sec^{2}(x) + \tan^{4}(x)) \mathrm{d}x$,"I'd like to solve the integral $$I := \int_{0}^{\pi/2} \ln(\sec^{2}(x) + \tan^{4}(x)) \mathrm{d}x$$ using the method of differentiating under the integral sign. So, first substituting $\tan x=u$ , we get $\sec^2x \mathrm{d}x = \mathrm{d}u$ , meaning $\mathrm{d}x = \frac{\mathrm{d}u}{\sec^2(\tan^{-1} u)} = \frac{\mathrm{d}u}{1+u^2}$ . So, we have $$I = \int_{0}^{\infty} \ln(1+u^{2} + u^{4}) \frac{\mathrm{d}u}{1+u^2}.$$ Let's define $I(t)$ as $$I(t) := \int_{0}^{\infty} \ln(1+t[u^{2} + u^{4}]) \frac{\mathrm{d}u}{1+u^2},$$ where $I(t=0) = 0$ . Then, differentiating w.r.t. $t$ yields $$I'(t) = \int_{0}^{\infty} \frac{[u^{2} + u^{4}]\mathrm{d}u}{(1+t[u^{2} + u^{4}])(1+u^2)} = \int_{0}^{\infty} \frac{u^{2}\mathrm{d}u}{1+tu^{2}(1 + u^{2})}.$$ Let $x = u^2$ . Then, $\mathrm{d}u = \frac{\mathrm{d}x}{2\sqrt{x}}$ , leading to $$I'(t) = \frac{1}{2t}\int_{0}^{\infty} \frac{\sqrt{x}\mathrm{d}x}{(x+\frac{1}{2})^2 + (\frac{1}{t}-\frac{1}{4})}.$$ Does this integral even converge? If so, how can one evaluate it? If otherwise, what have I done wrong here?","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'trigonometric-integrals']"
4733069,Probability in Pokémon Speedrunning,"Background Hey folks.  So I am, in addition to being a casual math and physics nerd, a Pokémon speedrunner.  I run the game Pokémon Red Version and the category ""Any % Glitchless (Classic).""  For those of you who don't know speedrunning terminology, ""Any %"" means that the only goal is to finish the game (i.e. you are allowed to skip any optional content) and ""glitchless"" means that you may not use any glitches.  In the ""glitchless (classic)"" category, you are also not permitted to ""hard reset"" the game (power it off and back on again), which is necessary to manipulate the game's random number generator function, so for runners of this category, the RNG is functionally truly random. Of course, one of the main purposes that we want to manipulate the RNG function is to control whether we get encounters.  The encounter mechanic in Pokémon (simplified) is that there is a certain probability in any given region of the game that each step in an encounter generating tile* will give an encounter.  If any of y'all have played Generation I Pokémon, you'll know that Mount Moon is the absolute most obnoxious area in the game and tends to generate an absolute torrent of encounters.  This is because, in ""cave"" areas, every tile on the map* is an encounter generating tile, whereas in other areas, only grassy tiles can generate encounters, so grassy tiles can be avoided.  Famously, one Pokémon speedrunner with the handle ""ExtraTricky"" got an absurd 18 encounters in Mount Moon in a previous world record. Naturally, I was wondering what the average number of encounters in Mount Moon should be. Simplified Problem You'll notice that I asterisked a few of the notes about the encounter generation function.  We're going to ignore certain complexities of this function for the purposes of the simplified problem.  In the full problem, we'll reintroduce those complexities. Like I said previously, every region of the map has an ""encounter rate"" that determines the probability that you will generate an encounter when stepping in an encounter-generating tile on the map.  In Mount Moon, that encounter rate is $\frac{10}{256}$ , which maybe doesn't seem that bad, but optimal movement through the area requires 276 steps (for any glitchless classic runners, this assumes no Hiker strats).  This version of the problem is actually pretty simple: it's a binomal distribution.  Thus, we can state the probability function as follows: Let $r$ be the number of encounters. $$ \mathbb P (r) = {276 \choose r} \cdot \bigg ( \frac{10}{256} \bigg )^r \cdot \bigg ( \frac{246}{256} \bigg ) ^{276-r}$$ I even made a graph! The average just ends up being the weighted sum, which I plugged into a spreadsheet, so: $$ \bar r = \sum_{n=0}^{276} n \mathbb P (n) \approx 10.7813$$ Full Problem There are two confounding factors that I want to add in here: Factor 1: Nido DVs and Catch Level So the main Poké that you use in Gen 1 is Nidoking, which means you will catch a Nidoran♂ in Route 22.  You can encounter this Nidoran♂ at levels 2, 3, or 4.  We do not catch level 2 Nidorans, so those may be ignored.  The player is, in random encounters, twice as likely to encounter a level 3 Nidoran♂ as a level 4 Nidoran♂.**  And, of course, the strats differ when using a level 3 Nidoran♂ vs. a level 4 Nidoran♂. Then, each individual Poké in Generation 1 has a ""determinant value"" for each stat that determines how high that stat will be compared so another Poké of the same species and level, which can be any 4 bit number (i.e. 0–15).  This also determines what strats we use.  In particular, if the Nidoran♂ has an attack DV under 5, we reset (we do not continue the run), so those may be ignore.  With a level 3 Nidoran♂  whose attack DV is $5\leq x \leq10$ we take an extra 20 steps in Mount Moon.  With a level 4 Nidoran♂ whose attack DV is $5\leq x \leq 8$ , we also take those extra 20 steps.  These are the above-referenced ""Hiker Strats"".  Assume all DVs for attack are equally likely.  We want to account for how these DV and catch level strats will affect our average. Of course, this isn't actually that hard.  We'll just need to have two separate probability functions based on whether we use Hiker strats or not and multiply those results times the probability of any given Nidoran♂'s catch level and attack DV. Factor Two: The Two-Step Rule This is the asterisk I put above with the encounter rates.  The first two steps upon entering Mount Moon and the first two steps after any battle (trainer or wild encounter) will not generate encounters.  This means that the maximum number of encounters is not, in fact, 276 because you will not generate an encounter until your third step after a battle, meaning the theorhetical maximum number of encounters is $\Big \lfloor \frac{276}{3} \Big \rfloor = 92$ .  In the above step count, I excluded the first two steps after entering Mt. Moon and after every trainer battle, but I did not exclude steps after random encounters. This is the part that has me a little stumped.  I suppose I could some kind of script or program that uses brute force to calculate every possible encounter profile given this two-step rule, but I'm wondering if there is a more elegant way to do it that I'm not seeing. And yes, I know the binomial distribution estimate is probably close enough, but I'm curious if there is a non-ridiculous way to calculate the average number of encounters taking the two-step rule into account. ** I'm not actually sure this is accurate.  In the part of the game where we catch the Nidoran♂, we have a method of only searching for wild encounters when the RNG is likely to give us a Nidoran♂ (called ""DSUMming"") and I don't know if this is biased towards a level 3 or a level 4 and/or by how much if so.",['probability']
4733074,Minimizer over set of $C^1$ functions,"Exercise (1.4) of Renardy and Rogers: ""An introduction to partial differential equations"" asks to Show that there is an infinite family of minimizers of $$
 J(u) = \int_0^1 (1-u'(t)^2)^2\,dt
 $$ over the set of all piecewise $C^1$ functions satisfying $u(0)=u(1)=0$ . Now the corresponding Euler-Lagrange equation is $$
\frac{d}{dt} \left[4 (1-u'(t)^2) u'(t) \right] = 0.
$$ I would argue that the equation above has no solutions satisfying the given boundary conditions. Integrating once leaves us with $$
(1-u'(t)^2) u'(t) = C
$$ and, depending on $C$ , there might be zero to three solutions. However, any solution, if it exists, requires $u'(t)=\mathrm{const}$ and, consequently, $u(t)$ is linear and hence cannot satisfy the boundary conditions (except for the trivial solution). I suspect that the key to the solution lies in the fact that we allow piecewise $C^1$ functions. However, I cannot see how we could employ that. Doesn't any discontinuity in the first derivative show up as a ( $\delta$ ) source term in the Euler-Lagrange equation? I'd be grateful for any pointers. Thanks heaps!","['optimization', 'boundary-value-problem', 'ordinary-differential-equations', 'euler-lagrange-equation']"
4733099,Land-based path from top to bottom if and only if no water based path from left to right,"We are given a $M \times N$ grid where each tile is either a land or water tile. We may move from a land tile to any other land tile that is orthogonally adjacent to the current land tile. We may move from a water tile to any adjacent water tile (both orthogonally or diagonally will work for water tiles). We may not move from land to water or water to land. It seems intuitive that if there exists a land-based path from the top row to the bottom row of the grid, then there cannot exist a water-based path from the left-most column to the right-most column. Conversely, if there is no water-based path from the left-most column to the right-most column, then there is a land-based path from the top row to the bottom row. How might one go about proving the above claim? I am not sure what techniques are appropriate for this type of problem. Originally, I had thought that the water movement needed to be only orthogonally adjacent for the above result to hold, but I found a counterexample. EDIT:
Orthogonally adjacent points of $(m,n)$ are $(m,n+1),(m,n-1),(m+1,n),(m-1,n)$ . Diagonally adjacent points of $(m,n)$ are $(m+1,n+1),(m-1,n-1),(m+1,n-1),(m-1,n+1)$ .","['network-flow', 'discrete-mathematics']"
4733130,How to generalise the limit of the product $\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right) $?,"When I met the result in the post that $$
\boxed{\lim _{n \rightarrow \infty}\prod_{k=1}^n\left(1+\frac{k}{n^2}\right) =\sqrt{e}},
$$ I appreciated very much the application of $G.M.\leq A.M.$ in the answer and modified the solution as below: $$
P_n^2=\prod_{k=0}^n\left[1+\frac{1}{n}+\frac{k(n-k)}{n^4}\right]
$$ Using $G.M.\leq A.M.$ , we have $$
0 \leqslant k(n-k) \leqslant \frac{n^2}{4}
$$ Plugging back yields $$
\left(1+\frac{1}{n}\right)^n \leqslant p_n^2 \leqslant \prod_{k=0}^n\left(1+\frac{1}{n}+\frac{1}{4 n^2}\right)=\left(1+\frac{1}{2 n}\right)^{2 n}
$$ Since $$
\lim _{n \rightarrow \infty}\left(1+\frac{1}{2 n}\right)^{2 n}=e=\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^n,
$$ By Squeezing Theorem, we conclude that $$
\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{k}{n^2}\right)=\sqrt{e}
$$ I then tried to investigate the limit $$
\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right)
$$ by the same technique. Let the product be $Q_n$ . Firstly we are going to find similarly the upper bound of $Q_n^2$ in terms of $n$ . $$\displaystyle \begin{aligned}Q_n^2 & =\prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}+\frac{k(n-k)}{n^4}\right]\end{aligned}\tag*{} $$ For the last terms, using $G.M.\leq A.M.$ again, we have $\displaystyle 0\leq k(n-k) \leqslant \frac{n^2}{4}\tag*{} $ Then plugging back yields $$\displaystyle \begin{aligned}Q_n^2 & \leqslant \prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}+\frac{1}{4 n^2}\right] \\& =\prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{2 n}\right)^2 \\& =\prod_{k=1}^n\left(1+\frac{3}{2 n}\right)^2\\&=\left(1+\frac{3}{2 n}\right)^{2 n} \end{aligned}\tag*{} $$ Next we are going to find the lower bound of $Q_n^2$ in terms of $n$ . $$\displaystyle Q_n^2\geqslant \prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}\right]= \prod_{k=1}^n \left[\left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)\right]\\=\left(1+\frac{1}{n}\right)^n\left(1+\frac{2}{n}\right)^n\tag*{} $$ Now we can find the limits of the bounds of $Q_n^2$ as $n$ tends to $\infty$ . $$\displaystyle \lim _{n \rightarrow \infty}\left(1+\frac{3}{2 n}\right)^{2n}=\left[\lim _{n \rightarrow \infty}\left(1+\frac{1}{\frac{2 n}{3}}\right)^{\frac{2 n}{3}}\right]^{3}=e^3\tag*{} $$ and $$\displaystyle \lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^n\left(1+\frac{2}{n}\right)^n=e \cdot e^2=e^3\tag*{} $$ By the Squeeze Theorem, $\displaystyle \lim _{n \rightarrow \infty} Q_n^2=e^3\tag*{} $ Hence we can conclude that $\displaystyle \boxed{\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right)=e^{\frac{3}{2}}\,} \tag*{} $ I am curious about what happens when both the number of terms and the power of $n$ in the product increase. I started to investigate the limit and surprisingly found that $$\boxed{\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{n^2}+\cdots +\frac{1}{n^{m-1}} +\frac{k}{n^m}\right)=e\,}$$ where $m\ge 3$ .
How to prove it? Let $$R_n=\prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{n^2}+\cdots +\frac{1}{n^{m-1}} +\frac{k}{n^m}\right)$$ and $$
A=1+\frac{1}{n}+\frac{1}{n^2}+\ldots+\frac{1}{n^{m-1}}
$$ Then $$R_n^2 =\prod_{k=1}^n\left[\left(A+\frac{k}{n^m}\right)\left(A+\frac{n-k}{n^m}\right) \right] =\prod_{k=1}^n\left[A^2+\frac{A}{n^{m-1}}+\frac{k( n-k)}{n^{2m}}\right] $$ Using $
0 \leqslant k(n-k) \leqslant \frac{n^2}{4}, 
$ we have $$
A^n\left(A+\frac{1}{n^{m-1}}\right)^n \leqslant R_n^2 \leqslant \prod_{k=1}^n\left(A+\frac{1}{2 n^{m-1}}\right)^2=\left(A+\frac{1}{2 n^{m-1}}\right)^{2 n}
$$ Then we finish the proof if we can prove that $$
\lim _{n \rightarrow \infty} A^n\left(A+\frac{1}{n^{m-1}}\right)^n=e^2= \lim _{n \rightarrow \infty}\left(A+\frac{1}{2 n^{m-1}}\right)^{2 n} $$ My attempt: $$\left(1+\frac{1}{n}\right)^n \leqslant A^n=\left(1+\frac{1}{n}+\frac{1}{n^2}+\ldots+\frac{1}{n^{m-1}}\right)^n \leqslant\left(\frac{1}{1-\frac{1}{n}}\right)^n \Rightarrow 
\lim _{n \rightarrow \infty} A^n=e
$$ My Question: How to prove that $$
\lim _{n \rightarrow \infty}\left(A+\frac{1}{n^{m-1}}\right)^n=\lim _{n \rightarrow \infty}\left(A+\frac{1}{2 n^{m-1}}\right)^n=e?
$$","['real-analysis', 'calculus', 'products', 'limits', 'sequences-and-series']"
4733138,dominated convergence theorem on the heat kernel,"Let $f \in C_c^{\infty}(\mathbb{R}^d)$ and $K_t = (4\pi t)^{-\frac{d}{2}}e^{-\frac{x^2}{4t}} $ for every $t>0$ . I want to prove this : \begin{align*}
\frac{d}{dt}(K_t*f)(x)&=\frac{d}{dt}\int_{\mathbb{R}^d} K_t(x-y) f(y)dy = \lim_{h \to 0} \int_{\mathbb{R}^d}  \frac{K_{t+h}-K_t}{h}(x-y)f(y) dy\\
&=  \int_{\mathbb{R}^d}   \lim_{h \to 0} \frac{K_{t+h}-K_t}{h}(x-y)f(y) dy = \int_{\mathbb{R}^d} \Delta K_t(x-y)f(y)dy=((\Delta K_t)*f)(x)
\end{align*} In order to prove the third equality, I thought I could use the dominated convegrence theorem since $||\Delta K_t * f||_1$$ \leq ||{\Delta K_t}||_1 ||{f}||_1< \infty.$ $$\frac{\partial}{\partial t} K_t(x-y)  f(y) = \lim_{h\to 0} \frac{K_{t+h}-K_t}{h}(x-y)f(y) = \Delta K_t(x-y)f(y) .$$ Now, by definition of the limit, for every $\epsilon > 0 $ , there exists $\delta>0 $ such that for every $h\leq \delta$ , $$\left| \frac{K_{t+h}-K_t}{h}(x-y)f(y) \right| \leq \Delta K_t(x-y)f(y) + \epsilon$$ taking $\epsilon = \Delta K_t(x-y)f(y)$ , there exists $T$ such that for every $h \leq T$ $$\left| \frac{K_{t+h}-K_t}{h}(x-y)f(y) \right| \leq 2\Delta K_t(x-y)f(y) .$$ clearly I have a dependance of T to y which mean I can't use this, I can't find a better bound, does someone have a good idea ?","['measure-theory', 'heat-equation', 'partial-differential-equations']"
4733162,Find the maximum and minimum of a function with three variables,"The problem is finding the maximum and minimum of the following three variables function: $f(x, y, z)=(2x^2+2y^2+z^2+2yz+2zx)e^{-(x^2+y^2+z^2)}$ , $(x, y, z)\in \Bbb R^3$ I noticed since $f(x, y, z)=(2x^2+2y^2+z^2+2yz+2zx)e^{-(x^2+y^2+z^2)}=[(\sqrt 2x +\frac{\sqrt 2}{2}z)^2+(\sqrt 2y +\frac{\sqrt 2}{2}z)^2]e^{-(x^2+y^2+z^2)}\ge0$ and the equality holds iff $(x, y, z)=t(2, 2, -1)$ , $t\in \Bbb R$ , Thus the minimum of $f$ is $0$ . But now I don't know how to get the maximum. There is no way to apply Lagrange multiplier since there's no contraints. I also thought of using Hessian matrix, but there's just too much calculation and I don't think that's the best way to do it. Any help would be appreciated!","['lagrange-multiplier', 'analysis', 'maxima-minima', 'multivariable-calculus', 'partial-derivative']"
4733229,A result on functions in $\mathbb N^{\mathbb N}$,"Assume some function $f\in \mathbb N^{\mathbb N}$ which is strictly increasing (i.e. $f(n+1)>f(n)$ for all $n\in \mathbb N$ ). Then for every natural number $N$ we can find some natural number $N_0$ such that $f(N_0)\ge N$ . I observed this to be true, and so I made an attempt for a proof. So I begun with a proof by contradiction, which is to assume that there exists some natural number $N$ so that we have the following: $f(n)<N$ for all $n\in \mathbb N$ . Which means that the set $f(\mathbb N)=\{f(n)|n\in \mathbb N\}$ is bounded above $N$ . From this point on, I can see that this an immediate contradiction, since $f(\mathbb N)\subseteq \mathbb N$ and that every infinite subset of $\mathbb N$ is not bounded above. But, how would I go about proving this rigorously? Should I make use of the Supremum property?",['functions']
4733237,Asymptotic behaviour of an integral with power and exponential functions,"Consider the following integral $$
\int_1^\infty x^{k-b} e^{-ck x} dx
$$ where $b>0$ and $c\in(0,1)$ are fixed and $k \to \infty$ . What is the behaviour of this integral as $k$ diverges? I have tried to re-express it as $(ck)^{a+1-k}\Gamma(k-a,ck)$ , but this has not been very helpful as I haven't managed to find a suitable characterization of the limiting behaviour of $\Gamma(k-a,ck)$ . If anyone could point me to a useful reference, it would be very helpful.","['gamma-function', 'limits', 'calculus', 'special-functions']"
4733261,Calculating Triple Integral using Cylindrical Coordinates,"I'm given $ E $ is located in $ x^2 + y^2 = (z-1)^2 $ and between $z = 0$ and $z=2$ .
I used level curves to graph this out, and as I see it is a circular cone. First, I set up my region, $$ E = \Big\{(r,\theta,z) | 0 \leq r \leq 1, 0 \leq \theta \leq 2\pi, 0 \leq z \leq 2\Big\}.$$ I then set up my integral as $$ \int_{0}^{2\pi}\int_{0}^{2}\int_{0}^{1} rdrdzd\theta $$ This resulted in $2\pi$ , which was wrong. So then I changed the bounds of my $z$ by solving the equation for $z$ , where $z = 1 \pm \sqrt{x^2+y^2} = 1 \pm r $ so that resulted in this integral: $$  \int_{0}^{2\pi}\int_{0}^{1}\int_{1-r}^{1+r} rdzdrd\theta $$ which resulted in $\frac{4}{3}\pi$ , which was wrong as well. What am I doing wrong? The only thing I can think of is to integrate on the equation $z = 1 + \sqrt{x^2+y^2}$ but I am not sure that is right either.","['cylindrical-coordinates', 'multivariable-calculus', 'multiple-integral']"
4733297,Which of these properties is not true of the directional derivative?,"In Arnold's Ordinary Linear Equations , 1st edition, chapter 2, section 10, subsection 3, the author pulls what is just about the dirtiest, cruelest, most evil trick which could be imagined from an expositor of mathematics. Here is the setup: ""We denote by $F$ the set of all infinitely differentiable functions $f : U \rightarrow \textbf{R}$ . Let $\textbf{v}$ be an infinitely differentiable vector field in $U$ . The derivative of a function of $F$ in the direction of the field $\textbf{v}$ again belongs to $F$ . Thus differentiation in the direction of the field $\textbf{v}$ is a mapping $L_v : F \rightarrow F$ of the algebra of infinitely differentiable functions into itself. Let us consider several properties of this mapping: $L_v(f + g) = L_vf + L_vg$ $L_v(fg) = fL_vg + gL_vf$ $L_{u+v} = L_v + L_u$ $L_{fu} = fL_u$ $L_uL_v = L_vL_u$ ( $f$ and $g$ are smooth functions and $\textbf{u}$ and $\textbf{v}$ are smooth vector fields)."" And then comes the punch to the gut, in tiny text: "" Problem 1. Prove properties 1-5, except for the one which is not true."" I believe the falsehood is number 4, $L_{fu} = fL_u$ . The reason is because $f$ must be a function from a vector field to another vector field to be applied to $u$ , but $L_u$ transforms a real-valued function to another real-valued function, so it doesn't make sense to apply the same $f$ to both $u$ and $L_u$ . Is that correct?","['derivatives', 'vector-analysis']"
4733331,"In what sense does Sylow theory determine ""local structure"" of a group?","I'm reviewing some group theory with Alperin and Bell's Groups and Representations, and they chose to title the chapter containing the Sylow theorems, $p$ -groups and compositions series by Local Structure . To preface the chapter they write ""In many branches of mathematics, it is profitable to study an issue by somehow “localizing” with respect to a given prime number. In this chapter, we adapt this doctrine to group theory by studying finite groups through their subgroups of prime-power order. This notion of looking at the “local structure” of finite groups has proven to be very powerful."" In so far as I understood my first encounter with Sylow theory, we're essentially leveraging the properties of prime numbers to make claims about the properties of groups with prime power orders. Is there an intuitive way to understand why a mathematician might informally refer to this as local structure, or to localizing with respect to a prime? Or is it just because $p$ -Sylow subgroups are ""substructures"" of a larger group $G$ and hence one can view studying the components as being a local property of $G$ ?","['group-theory', 'abstract-algebra', 'soft-question', 'sylow-theory']"
4733364,Proving the formula for distance between a point and a parametric line,"Let $l$ be a line in $\mathbb{R}^3$ with parametric equation $\alpha(t) = a + t\boldsymbol{v}$ and $p$ an arbitrary point. Show that the (shortest) distance between $p$ and $l$ is given by $$d = \frac{||(\boldsymbol{p - a}) \times \boldsymbol{v}||}{||\boldsymbol{v}||}$$ I've attempted to render an image corresponding to the problem, where $Q$ is the point at the foot of a perpendicular dropped from $P$ to $l$ . (Please excuse capital $P$ instead of $p$ in the drawing). By the definition of $sin$ we have $d = \sin\theta||p - a||.$ But from there, I don't see why the cross product of $||p - a||$ with $\boldsymbol{v}$ divided by $||\boldsymbol{v}||$ would be related to $d$ or how it would get rid of the $\sin\theta$ . By the right hand rule, wouldn't the cross product of $p - a$ and $\boldsymbol{v}$ be sticking ""out"" in a direction unrelated to $d$ ? Thank you.","['multivariable-calculus', 'linear-algebra']"
4733366,A ladybug is moving with velocity vector $v$. At what rate is the sum of her distances from two points decreasing at a certain moment?,"Problem : At a certain moment, a ladybug is at position $x_0$ and moving with velocity vector $v$ . At the moment, the angle $<ax_0b = \pi/2$ , her velocity bisects the angle, and her speed is $5$ units/sec. At what rate is the sum of her distances from $a$ and $b$ decreasing at that moment? My question is why my final answer is positive instead of negative. Solution : I will begin by defining two functions. let $p$ be the function that gives the position of the bug at time $t$ such that $p(t) = x_0 + tv$ , and let $s$ be the function that gives the sum of the distances between the current position of the bug and the points $a$ and $b$ such that $s(x) = \rVert x - a\lVert + \rVert x - b\lVert$ . Now I need the function which gives the sum of the distances given the time. This function which I will call $h$ is the composition of $s$ and $t$ , so $h = s \circ p$ . The question is asking for the rate of decrease of $h$ at $t = 0$ so I need the derivative of $h$ at $t = 0$ which is $h'(0) = Ds(p(0))Dp(0)$ . $h'(0)$ is a scalar so it equals its transpose, so $h'(0) = (Ds(p(0))Dp(0))^\intercal = (Dp(0))^\intercal \nabla s(p(0))$ . Now $(Dp(0))^\intercal = v^\intercal$ , and $\nabla s(p(0)) = \dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}$ . So now $h'(0) = v^\intercal(\dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}) = v \cdot(\dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}) = (\dfrac{v \cdot (p(0) - a)}{\rVert p(0) - a\lVert} + \dfrac{v \cdot (p(0) - b)}{\rVert p(0) - b\lVert})$ . From the question we know that $\rVert v\lVert = 5$ and that the angle between $v$ and the two vectors is $\pi/4$ , so $h'(0) = 5 \cos{45} + 5 \cos{45} = 5\sqrt{2}$ but that is a positive number which is wrong as the distance should be decreasing. It seem to me from the picture as if the bug is moving in the direction of the smallest rate of change, so maybe somehow my mistake was in confusing $v$ with $-v$ which should be the direction of the greatest rate of change and the correct answer should be $-5\sqrt{2}$ . Perhaps all my solution is just wrong.","['word-problem', 'multivariable-calculus', 'solution-verification']"
4733368,Find radius of circle in the following circle.,"Let we have two tangent lines from point $A$ to circle. Find radius of circle in the following circle: I draw two radiuses of circle to the
two tangent lines from point $A$ to circle so we have two triangles with angels $90$ but i don't know how we can use other information from question.","['circles', 'geometry']"
4733384,"If $f(x)=\frac{x^5}5+\frac{x^4}4+x^3+\frac{kx^2}2+x$ is a real valued function, find maximum $k^2$ if $f(x)$ is increasing $\forall x \in R$","If $f(x)=\frac{x^5}5+\frac{x^4}4+x^3+\frac{kx^2}2+x$ be a real valued function. Find the maximum value of $k^2$ for which $f(x)$ is increasing $\forall x \in R$ . I differentiated the function, and set up the inequality where the derivative is greater than $0$ . Then I rearranged the terms to get $$k<\frac1x +x^3+x^2+3x$$ To find the maxima and minima of the right hand expression, I took the derivative and setting it equal to $0$ , obtained: $$3x^4+2x^3+3x^2-1=0$$ How do I factorize this? How do I solve this equation(if factorizing is not the optimal way) Now about the graph which I checked using an online calculator, I am getting that $f'(x)=\frac1x +x^3+x^2+3x$ has the range $[- \infty,-3.322]\cup[3.862,\infty  ]$ . If $k$ is
always less than $f'(x)$ , then doesn't that mean that $k$ ranges
from $[-\infty, \infty]$ ? So $k^2$ ranges from $[0, \infty]$ ? But
the answer is given $0.322^2=11.03...$ . How am I interpreting the
inequality wrong?","['calculus', 'functions', 'algebra-precalculus', 'inequality']"
4733412,Why codomain is more than the range in an Inverse function,"While solving inverse function problems, I got confused in a part, like for any Inverse function to be defined, it must be one-one and onto, then in many questions why the codomain is given more than the Range as if we know that the codomain must needs to be equivalent
to the Range for the Inverse function to be valid or defined, then why does they gave us the codomain different from the range? Kindly help me solving this doubt.","['functions', 'function-and-relation-composition', 'inverse-function']"
4733498,Why does Gaussian elimination sometimes work in rings where it should not?,"I think it's best to illustrate this with an example. Take for instance the ring of integers modulo $6$ . If I have the system of equations: $$ \begin{aligned} 2x + 2y &= 4 \\ 3x + 4y &= 3 \end{aligned} $$ I divide the first equation by $2$ : $$ \begin{aligned} x +  y &= 2 \\ 3x + 4y &= 3 \end{aligned} $$ Subtract $3$ times the first equation from the 2nd equation to arrive at a solution for $y$ : $$ \begin{aligned} x + y &= 2 \\ 0 + y &= 3 \end{aligned} $$ Thus, $y = 3$ and $x = 5$ . From my understanding, this should be not possible as in ${\Bbb Z}_6$ there is no equivalence to $\frac12$ as $2$ does not have a multiplicative inverse in this ring, but yet I get a solution that works. The best answer I have is that this is just a particular example in which dividing by $2$ was possible from construction, even if it makes no sense in the ring. I know I can invent situations where this would not be possible, for instance, if equation 1 was: $$ 1x +3y = 1 $$ I'm curious if there is anything else at play here, or if it's just luck of construction that an answer could be found.","['modular-arithmetic', 'finite-rings', 'ring-theory', 'abstract-algebra', 'gaussian-elimination']"
4733590,Determine an operator on a Hilbert space based on a polynomial equation,"Suppose we have an infinite dimensional, separable Hilbert space and an operator $X$ on that Hilbert space  for which we only know that $$ X^2 + bX + c = 0 \qquad (1) $$ with $b,c \in \mathbb{R}$ . With $p,q\in \mathbb{C}$ and $$ (X - p)(X - q) = 0 $$ $$ b = -p-q, \quad pq = c $$ we can introduce projection operators $E_{p,q}$ satisfying $$ E_p + E_q = \text{id} $$ $$ E_p^2 = E_p, \quad E_q^2 = E_q, \quad E_p E_q = E_q E_p = 0 $$ and set $$ X = p \, E_p + q E_q \qquad (2) $$ which obviously solves $(1)$ . Question : Are there other solutions for which X is not simply given by $(2)$ using a pair of projectors $E_{p,q}$ ?","['hilbert-spaces', 'functional-analysis']"
4733595,Fubini theorem on non-$\Sigma$-finite measures,"I have been studying Fubini theorem and its proof on ""Probability and Stochastics"" by Erhan Cinlar.
Premise: a measure $\mu$ on a measurable space $\big( E,\mathcal{E} \big)$ is said to be $\Sigma$ -finite if there is a countable collection of finite measures $\big( \mu_n \big)_{n=1}^{\infty}$ such that $\mu=\sum_{n=1}^{\infty}\mu_n$ . It can be shown that for any $\mathcal{E}$ -measurable function $f$ , it holds $\mu f = \sum_{n=1}^{\infty} \mu_n f$ . Statement Let $\mu$ and $\nu$ be $\Sigma$ -finite measures on $\big( E, \mathcal{E} \big)$ ad $\big( F, \mathcal{F} \big)$ , respectively.
Then, there exists a unique $\Sigma$ -finite measure $\pi$ on $\big( E \times F, \mathcal{E} \otimes \mathcal{F} \big)$ such that, for every positive $f$ in $\mathcal{E} \otimes \mathcal{F}$ , \begin{equation}
\pi f= \int_E \mu(dx) \int_F \nu(dy) f(x,y) = \int_F \nu(dy) \int_E \mu(dx) f(x,y)
\end{equation} Issue with proof The proof starts by considering the first double integral, and observes that, under the assumption that both $\mu$ and $\nu$ are $\Sigma$ -finite, it can be expressed as \begin{equation}
\pi f= \int_E \mu(dx) \int_F \nu(dy) f(x,y) = \\ = \sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\int_E \mu_i(dx) \int_F \nu_j(dy) f(x,y) = \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} (\mu_i \times \nu_j) f \tag{1}
\end{equation} Then, it defines the function \begin{align*}
\hat{f} \colon F \times E & \longrightarrow \mathbb{R_+}\\
(y,x) &\longmapsto f(x,y)
\end{align*} and considers the second iterated integral: \begin{equation}
\hat{\pi} \hat{f}= \int_F \nu(dy) \int_E \mu(dx) \hat{f}(y,x) = \sum_{j=1}^{\infty}  \sum_{i=1}^{\infty} \int_F \nu_j(dy) \int_E \mu_i(dx) \hat{f}(y,x) = \sum_{j=1}^{\infty}  \sum_{i=1}^{\infty} (\nu_j \times \mu_i) \hat{f}
\end{equation} where $\hat{\pi}(B \times A) = \nu(B) \mu(A)$ . The goal is obviously to show that $ \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} (\mu_i \times \nu_j) f = \pi f = \hat{\pi} \hat{f} = \sum_{j=1}^{\infty}  \sum_{i=1}^{\infty} (\nu_j \times \mu_i) \hat{f}$ Since all the integrals inside the summations are non-negative numbers, the order of summation can be exchanged. The goal is then to show that the order of integration can be exchanged. The proof now rightly states that it suffices to show that the integrals can be exchanged for some arbitrarily fixed $i$ and $j$ , i.e. \begin{equation}
\int_E \mu_i(dx) \int_F \nu_j(dy) f(x,y) = \int_F \nu_j(dy) \int_E \mu_i(dx) f(x,y)
\end{equation} The advantage is that now $\mu_i$ and $\nu_j$ are finite, i.e., the proof for $\Sigma$ -finite measures boils down to proving the exchangeability of integrals for finite measures. Hence, in the following we will get rid of the indexes $i$ and $j$ and pretend that we are working under the assumption that $\mu$ and $\nu$ are finite. The textbook now considers a measurable rectangle $A \times B$ , and defines the function \begin{align*}
h \colon E \times F & \longrightarrow F \times E\\
(x,y) &\longmapsto (y,x)
\end{align*} Also, it observes that $f = \hat{f} \circ h$ . Here is the core of the proof, that is conducted under the assumption of finiteness of $\mu$ and $\nu$ , which is not needed in my view. The book observes that \begin{equation}
\pi \circ h^{-1}(B \times A)=\pi(h^{-1}(B \times A)) = \pi(A \times B) = \\ = \mu(A) \nu(B) = \nu(B) \mu(A) = \hat{\pi} (B \times A) \tag{2}
\end{equation} Hence, upon noticing that \begin{equation}
f(x,y)=(\hat{f} \circ h)(x,y)=\hat{f}(h(x,y))=\hat{f}(y,x)=f(x,y)
\end{equation} it holds \begin{equation}
\hat{\pi} \hat{f} = \pi (\hat{f} \circ h) = \pi f
\end{equation} which proves the statement. My problem is: I believe that equation 2 can be applied also to non- $\Sigma$ -finite measures. In this case, I would just do that on the measure $\pi$ in equation 1 and get the exchangeability of the integrals for any product measure, without the need for the $\Sigma$ -finiteness assumption. The way I would proceed I wondered if the actual measure of the rectangle $A \times B$ requires assumption on $\mu$ and $\nu$ in order to allow the exchangeability. In other words, I wondered if the assumption of $\Sigma$ -finiteness of both is required for \begin{equation}
\pi(A \times B) = \int_A \mu(dx) \int_B \nu(dy) = \mu(A) \nu(B) = \nu(B) \mu(A) = \int_B \nu(dy) \int_A \mu(dx) = \hat{\pi}(B \times A)
\end{equation} and I would say that $\mu(A) \nu(B) = \nu(B) \mu(A)$ holds irrespective of any $\Sigma$ -finiteness assumption.","['integration', 'measure-theory', 'fubini-tonelli-theorems', 'real-analysis']"
4733621,2-D inverse Fourier transform of Heaviside function,"Now I have a Heaviside function $H(K-\sqrt{k^2+l^2})$ in a 2D $\hat k$ space, where $k$ and $l$ are two variables in that space. In a paper, it is said that the inverse Fourier transform of this Heaviside function is: $$
\frac{K}{\sqrt{x^2+y^2}} J_1(K\sqrt{x^2+y^2})
$$ where $x$ and $y$ are variables in 2D real space and $J_1$ is the first order Bessel function. How to derive this result? Attempts: I try to perform this transform by: $$
\int_{-\infty}^{+\infty} d k \int_{-\infty}^{+\infty} d l \
H(K-\sqrt{k^2+l^2}) e^{i2\pi(kx+ly)}
$$ Using the properity of Heaviside function, the integral becomes: $$
\iint_{k^2+l^2 \leq K^2} dk \ dl \
e^{i2\pi(kx+ly)}
$$ Switch to polar coordinates(set $k=r\cos\theta$ and $l=r\sin\theta$ ): $$
\int_0^{K} dr \int_0^{2\pi} d\theta \
r e^{i2\pi r(x\cos\theta+y\sin\theta)}
$$ After integral over $r$ , we have: $$
\int_0^{2\pi} d\theta \
\frac{-1+[1-i2\pi K(x\cos\theta+y\sin\theta)]e^{i2\pi K(x\cos\theta+y\sin\theta)}}
{4\pi^2(x\cos\theta+y\sin\theta)^2}
$$ I don't know how to deal with this integral and how to transform it into Bessel function.","['integration', 'step-function', 'fourier-transform', 'bessel-functions']"
4733624,What is the new depth of water in the tilted tank?,"A rectangular tank with width of $8 m$ and length $12 m$ , and height $20 m$ lying flat on its $8 \times 12$ base is filled with water to 30% of its height (i.e. $6 m$ ).  The tank is then tilted such that one of the space diagonals (the diagonal between two spatially opposite vertices) is vertical to the ground.  What is the new depth of water in the tank ?  i.e. how high is the surface of water above the ground ? My Attempt: The center of the water surface in the tilted tank is the same distance away from the base, i.e. $6 m$ ""above"" the base.  In a proper coordinate system, the original space diagonal vector is $(12, 8, 20)$ .  We now have have two vectors: $V_1 = (0, 0, 6)$ is the vector representing the distance between the center of the surface of water and the base, and $V_2 = (6, 4, 0)$ is the vector between the center of the base and the lower-most vertex.  We just have to find the projection of these two vectors onto the space diagonal.  The unit vector along the space diagonal is $ u = \dfrac{(12, 8, 20) }{ \sqrt{ 12^2 + 8^2 + 20^2 } } = \dfrac{1}{\sqrt{38}} (3, 2, 5) $ Therefore, the new depth of water is $ d = \dfrac{1}{\sqrt{38}} (3, 2, 5) \cdot ( (0, 0, 6) + (6, 4, 0) ) = \dfrac{1}{\sqrt{38}} (3, 2, 5) \cdot (6, 4, 6) = \dfrac{56}{\sqrt{38}} = 9.0844 $ m Is my attempt correct ?","['vectors', 'geometry', '3d']"
4733629,Confusion in Hartshorne exercise IV.2.7,"In page 306 of Hartshorne's book Algebraic Geometry , the exercise 2.7 asks me to show that,
for a given curve (in Hartshrone's sense, i.e., an irreducible complete nonsingular curve over an algebraically closed field) $Y$ , there is a 1-1 correspondence between finite étale morphism $f\colon X\to Y$ of degree $2$ , and $2$ -torsion elements of $\mathrm{Pic}(Y)$ . However, in the text Hartshorne does not assume $X$ is connected (hence also a curve). If it is indeed not assumed to be connected, then How to define the degree of a finite étale morphism $f\colon X\to Y$ ? Maybe a reasonable way is to define $\deg f$ by the rank of $f_*\mathscr{O}_X$ as a locally free $\mathscr{O}_Y$ -module via $f_\flat\colon\mathscr{O}_Y\to f_*\mathscr{O}_X$ . In (a) of this exercise, Hartshorne uses the previous exercise IV.2.6(d) to show that $(\det f_*\mathscr{O}_X)^2\simeq\mathscr{O}_Y$ . But exercise IV.2.6 only states for finite morphism $f$ of curves, how to solve this problem? But if we assume $X$ is indeed a curve, then we just take $\mathscr{L}=\mathscr{O}_Y$ in (b) of this exercise, then it seems that $\mathrm{Spec}(\mathscr{O}_Y\oplus\mathscr{L})$ is not integral, because the multiplication \begin{equation}
(a,b)\cdot(a',b')=(aa'+bb',ab'+a'b),
\end{equation} on $\mathscr{O}_Y\oplus\mathscr{L}$ always satisfies that $(1,1)\cdot(1,-1)=(0,0)$ .
Do I miss anything? Edit: It seems that, if $X$ is not connected, then the étale morphism $f$ can be identified with the codiagonal $\nabla\colon Y\sqcup Y\to Y$ , which is a trivial ""étale covering"". So we WLOG just consider the case $X$ is connected.","['algebraic-curves', 'algebraic-geometry']"
4733647,Integrate Gumbel times Normal,"I try to solve an integral of the following form: $$ \int_{-\infty}^\infty e^{-x^2} \, e^{-e^{-x^2}} dx $$ Intuitively, the first term, $e^{-x^2}$ , is related to the pdf of a standard-normal distribution, while the second term, $e^{-e^{-x^2}}$ , is related to the pdf of a Gumbel-distribution (except for the square). From plotting the function, it seems that the integral should be well defined, but I cannot find a solution yet.
Any hint on how to solve this is highly appreciated.","['integration', 'density-function']"
4733681,"How to calculate the probabilities of outcomes, of 3d6 - drop lowest dice","I am currently doing a spot of game-design but I am held back by being only a novice in the arts of mathematics. Here is the problem: I wish to calculate the probabilities of the eleven outcomes of
rolling 3d6 but dropping the lowest dice. So it is like rolling 2d6 but with a significant bias for higher rolls. When rolling 3d6 , there are 216 different dice combinations. In order to get the outcome 2 (with 3d6-drop lowest), all 3 dice must be 1 (1, 1, 1) (Orelse we would be dropping a 1 in favour of a dice that doesn't show 1) the probability of getting the outcome 2, is therefore 1/216. I can also tell that the probability of getting the outcome 12 , is 16/216 since there are 16 different outcomes that result in 12. When rolling 3d6 there are 16 different outcomes that contain at least 2 6'es. That said, counting out all the possible combinations seem like a very manual way to get these probabilities, that will be nearly impossible when it comes to the more likely outcomes, such as 7, 8 and 9... What is a good method to actually figuring out what I want to know? Outcomes: 2: 1/216 3: ?? 4: ?? 5: ?? 6: ?? 7: ?? 8: ?? 9: ?? 10 ?? 11: ?? 12: 16/216 Edit: cleaning some small mistakes in my explanation Edit2: Thank you Henry, there are 11 outcomes of course, not 12.","['statistics', 'dice', 'probability']"
4733697,"Find the Fourier series of a function , determining whether it converges pointwise/uniformly","I've been trying to solve an exercise from a test. I could use your help :) Question Consider the function: $g(x) =  \frac{\pi}{4} $ for $x\in [0,\pi ]$ and $ -\frac{\pi}{4}$ for $x\in (-\pi,0 )  $ . Let $f(n)$ be $g(x)$ 's  periodic continuation. $S_N f(x) = A_0 +\sum_{n=1}^{N}A_n \cos{nx}+i\sum_{n=1}^{N}B_n \sin{nx}$ where $A_0 = \hat{f}(0), A_n = \hat{f}(n)+\hat{f}(-n), B_n = \hat{f}(n)-\hat{f}(-n)$ . Note: The Fourier series of $f$ is $\lim_{N \to \infty } S_Nf$ Find $S_Nf$ . Determine if $S_Nf(x)$ pointwise converges for $x\in \left[-\pi,\pi \right]$ . Determine if $S_Nf(x)$ uniformly converges in $ \left[-\pi,\pi \right]$ . Prove $\lim_{N \to \infty} S_Nf \left( \frac{1}{\pi N}\right) = \int_{0}^{1} \frac{\sin{\left( \frac{t}{\pi}\right)}}{2t} \ dt $ My attempt Sol for 1: $\hat{f}(n)= \frac{1}{2in}$ for $n\in \mathbb{Z}_{odd} $ , else it's $0$ . Hence, $S_N f (x) = \sum_{k=0}^{K}\frac{1}{(2k+1)} \sin{((2k+1)x)}$ where $K=\left\lfloor \frac{N}{2}\right\rfloor$ Sol for 3: $S_N f (x)$ is a continuous function, therefore if $S_Nf$ uniformly converges we know it has to be to $f$ . But $f$ isn't continuous. Therefore we will conclude $S_Nf$ does not uniformly converges. I would appreciate your help in 2 and 4 (if you do please be formal so I can understand better). Thank you!","['limits', 'fourier-analysis', 'fourier-series', 'real-analysis']"
4733777,$(\lambda z. zy)(\lambda z. zy)$ - reducing using $\beta$ reduction and $\alpha$ conversion,"Good day . I need to reduce the following expression of lambda calculus: $(\lambda z. zy)(\lambda z. zy)$ Now, since I am having the variable $y$ in both the left and right pair of parentheses, I think that I should rename one of them to another variable, namely $u$ . $(\lambda z. zu)(\lambda z. zy)$ . Now, by reducing I get: $(\lambda z. zy)u$ , which yields $uy$ However: in the solution of the exercise I found online they did not rename the variable $y$ ! I am not sure why. Since the variable $y$ appears in both pairs of parentheses we'd need to rename one of them. They simply got $(\lambda z. zy)y$ which evaluates to $yy$ . The reason I'm asking is because in one other example, they did rename one of the variables. It's this example: $$(\lambda x. \lambda y. xy)(\lambda x. \lambda y. xy)$$ What they did is they renamed one of the variables $y$ to $z$ so they got $$(\lambda x. \lambda \color{red}z. x \color{red}z)(\lambda x. \lambda y. xy)$$ Can anyone explain why we don't rename the variable in the first example, but we need to do it in the second example?","['computer-science', 'discrete-mathematics', 'lambda-calculus', 'formal-systems', 'computability']"
4733799,Convergence of a sequence with dependence,I want to derive a LLN for conditionally independent random variables. Let $(X_n)_{n \geq 1}$ be a sequence of random variables taking values on a finite subset $B$ . Assume that their conditional probability is bounded above and below by numbers that are independent on the conditioning event. Show convergence of $(X_n)_{n \geq 1}$ .,"['law-of-large-numbers', 'limits', 'convergence-divergence', 'probability-theory', 'random-variables']"
4733802,Complex properties about simple functions $f(x)=2x+1$ and $g(x)=3x+1$.,"This problem came up while trying to prove something for one of my projects. I've checked the first few values with code but I've been looking for a mathematical proof. We define $f(x)=2x+1$ and $g(x)=3x+1,$ do there exist two distinct sets of operations $S$ and $S'$ with the SAME number of elements such that $S(1)=S'(1)?$ (e.g. $S=f(g(...(1)...)=g(f(...(1)...)=S'$ or $S=f(f(g(...(1)...))=f(f(f(...(1)...))=S'$ etc.) If it helps, $S$ can not be comprised of solely $f(x)$ , and that $S'$ can be comprised solely of $g(x)$ and I know an explicit formula based on the application of the equations. thanks!","['functions', 'discrete-mathematics', 'sequences-and-series']"
4733829,"Complex analysis, Ian Stewart Exercise 4.7.5: Proving $\sqrt{z}$ is continuous on $\mathbb{C}\setminus\{x\leq0\}$","This is exercise 4.7.5 in Ian Stewart's ""Complex Analysis
(The Hitch Hiker’s Guide to the Plane)"": Let $C_{\pi} =\{z\in\mathbb{C}:z\neq x\in\mathbb{R},x\leq0\}$ be the 'cut plane' with the negative real axis removed. Define $r : C_{\pi} \to \mathbb{C}$ by $(r(z))^2 = z$ and $\operatorname{Re} r(z) > 0$ . Prove that $r$ is continuous on $C_{\pi}$ . May I ask how can I prove it without using theorems like Inverse function theorem from multivariable calculus?","['analysis', 'complex-analysis', 'continuity', 'multivariable-calculus', 'branch-cuts']"
4733849,Does reparameterization trick make sense?,"Given $x_t=(1-t)x_0+ty+t\epsilon_t\eta_t$ , where $\eta_t\sim N(0,I)$ , how can the analytic form of $p(x_{t-\delta}|x_t,x_0)$ be derived ? Since what we have is $p(x_{t-\delta}|x_0,y)$ and $p(x_{t-\delta}|x_0,y)$ , I tried to rewrite $p(x_{t-\delta}|x_t,x_0)$ as, \begin{align*}
p(x_{t-\delta}|x_t,x_0) = \int_{y} q(x_{t-\delta}|x_t,x_0,y)*q(y|x_0,x_t) \,dy 
\end{align*} I don't know how to continue from here. While I don't know how to solve the problem using the basics of probability, a so called reparameterization trick may help. Since $x_t=(1-t)x_0+ty+\epsilon_t\eta_t$ , we have \begin{align*}
y=\frac{1}{t}x_t-\frac{1-t}{t}x_0-\epsilon_t*\eta_t
\end{align*} . Does this means $y\sim N(\frac{1}{t}x_t-\frac{1-t}{t}x_0,\epsilon_tI)$ ? Since $x_{t-\delta}=(1-t-\delta)x_0+(t-\delta)y+(t-\delta)\epsilon_{t-\delta}\eta_{t-\delta}$ , we may substitute the expression for y and get \begin{align*}
x_{t-\delta}=(1-t-\delta)x_0+\frac{t-\delta}{t}x_t-\frac{t-\delta}{t}(1-t)x_0-(t-\delta)\epsilon_t\eta_t+(t-\delta)\epsilon_{t-\delta}\eta_{t-\delta}
=\frac{\delta}{t}x_0+\frac{t-\delta}{t}x_t+(t-\delta)(\epsilon_t\eta_t+\epsilon_{t-\delta}\eta_{t-\delta})
\end{align*} ,
therefore we may conclude $$
p(x_{t-\delta}|x_t,x_0)=N(x_{t-\delta};\frac{\delta}{t}x_0+\frac{t-\delta}{t}x_t,(t-\delta)\sqrt{\epsilon_t^2+\epsilon_{t-\delta}^2}I)
$$ This is indeed the solution given in the paper (equation 10) https://openreview.net/pdf?id=VmyFF5lL3F .
Is the solution derived by using the reparameterization trick make sense? If so how can we achieve the same solution using the basics of probability ? Thanks for your help in advance !","['conditional-probability', 'statistics', 'probability-distributions', 'probability']"
4733857,A simple contrained optimization problem,"Suppose we have to find out the solution of the following optimization problem $$
{\min \atop x\,y =1}{x^2+y^2 \atop }
$$ Clearly, in this case, we can apply the classical graphical method of finding where the level curves of the function $f(x,y)=x^2+y^2$ are tangent to the constraint $x\,y=1$ . On the other hand if we want to solve $$
{\max \atop x\,y =1}{x^2+y^2 \atop }
$$ it looks like the solutions are $x=+\infty$ and $y=0$ or $x=0$ and $y=+\infty$ , in the sense that moving toward any of these two directions will provide a better solution than that given by the classical graphical method (i.e. $(x,y)=(1,1)$ ). So when the graphical method is not applicable to this kind of problems? In standard textbooks look like it is given for granted (i.e. it is typically said that the curve of the constraint must be tangent and not cross the level curve), so probably I am missing something or my reasoning is flawed and I do not see it.","['optimization', 'multivariable-calculus', 'constraints']"
4733925,Evaluating a Logarithmic Integral,"For everything on this post $n$ and $m$ are positive integers. The other day I found the following integral on the popular post ""Integral Milking"" and decided to give it a go. $$\large\int_{0}^{1} {\frac{\ln{(1+x^2+x^3+x^4+x^5+x^6)}}{x}}dx=\frac{{\pi}^2}{7}$$ After some simple manipulations, I eventually had $$\large\int_{0}^{1} {\frac{\ln{(1+x+...+x^n)}}{x}}dx=\frac{\pi^2}{6}\frac{n}{n+1}.$$ But more interestingly in order to get that result I found $$\large\int_{0}^{1} {\frac{\ln{(1-x^n)}}{x}}dx=-\frac{\pi^2}{6n},$$ which sparked a number of thoughts relating to the further generalization of the following, $$I(n,m)=\large\int_{0}^{1} {\frac{\ln{(1-x^n)}}{x^m}}dx,\hspace{5pt}n\geq{m}.$$ Surprisingly I am already stuck on $m=2$ and have only made a small bit of progress in the following two ways. $$I(n,2)=\large\int_{1}^{\infty} {\frac{1}{x^n-1}}dx+\left(x\ln{\left(1-\frac{1}{x^n}\right)}\right)\bigg|_1^{\infty}$$ $$I(n,2)=\large\sum_{m=1}^{\infty} {\frac{1}{m(nm-1)}}$$ I have a strong suspicion that these have both been asked before, but I have been struggling to find them, and if indeed there is already a post satisfying my intrigue I will delete this one. Regardless it seems there is some Digamma and/or a nice Hypergeometric Series result afoot that I have yet to consider. That or the last integral is simply trivial and I am just dumb. :)","['integration', 'summation', 'definite-integrals', 'special-functions', 'calculus']"
4733944,"Major error in classic ""Banach Spaces of Analytic Functions""","""Banach Spaces of Analytic Functions"" by Kenneth Hoffman is an excellent introduction to $H^{p}$ spaces and is considered a classic mathematical analysis textbook. I was therefore surprised to find major mistakes in the proof of a central theorem (""Fatou's Theorem""). Below you will find the proofs in question (pages 79-81) followed by short explanations of their shortcomings. I will give an overview of the proofs so this can be skipped on first reading. Here $A$ is the space of continuous functions on the closed unit disk that are analytic in the interior and $Q_{r}(t)=\text{Im}(\frac{1+re^{i\theta}}{1-re^{i\theta}})$ . $\textbf{Proofs:}$ Theorem. Let $f$ be an integrable function on the circle and $$v(r,\theta)=\frac{1}{2\pi}\int_{-\pi}^\pi f(\theta-t)Q_r(t)~\mathrm{d}t.$$ If $\theta$ is any number such that the integral $$v(\theta)=-\frac{1}{2\pi}\int_{-\pi}^\pi\frac{f(\theta+t)-f(\theta-t)}{2\tan\frac{1}{2}t}~\mathrm{d}t$$ exists, then $\lim_{r\to1}v(r,\theta)=v(\theta)$ . Proof. Let $$\phi_\theta(t)=\frac{f(\theta+t)-f(\theta-t)}{2\tan\frac{1}{2}t}$$ so that we are assuming $\phi_\theta$ is integrable. Now \begin{align*}
v(r,\theta)-v(\theta)
&=\frac{1}{2\pi}\int_{-\pi}^\pi\phi_\theta(t)\left[1-\frac{2r\sin t\tan\frac{1}{2}t}{1-2r\cos t+r^2}\right]~\mathrm{d}t \\
&=\frac{1}{2\pi}\int_{-\pi}^\pi\phi_\theta(t)\frac{(1-r)^2}{1-2r\cos t+r^2}~\mathrm{d}t.
\end{align*} Now if $$g_r(t)=\frac{(1-r)^2}{1-2r\cos t+r^2}$$ then $0<g_r(t)<1$ and $\lim_{r\to1}g_r(t)=0$ , except at $t=0$ . Since $\phi_\theta$ is integrable, we must have $\int\phi_\theta g_r\to0$ , i.e., $$\lim_{r\to1}v(r,\theta)=v(\theta).$$ Corollary. If $f$ is differentiable at $\theta$ then $$\lim_{r\to1}v(r,\theta)=v(\theta)$$ exists. If, say, $f$ is continuously differentiable on a closed interval $\lvert\theta-\theta_0\rvert\leq\delta$ , then on that interval the functions $v_r$ converge uniformly as $r\to1$ . Proof. The function $\phi_\theta$ is clearly integrable on any interval $\lvert t\rvert\geq\varepsilon>0$ . If $f$ is differentiable at $\theta$ , then $\phi_\theta$ is bounded as $t\to0$ , so $\phi_\theta$ is integrable. If $f$ is continuously differentiable on $\lvert\theta-\theta_0\rvert\leq\delta$ , then we obtain a uniform bound on $\phi_\theta$ for $\theta$ in the interval and $t$ small; it is easy to see that $v(r,\theta)$ is uniformly close to $v(\theta)$ . Theorem (Fatou). Let $K$ be a closed set of Lebesgue measure zero on the unit circle. Then there exists a function in $A$ which vanishes precisely on $K$ . Proof. Let $w$ be an extended real-valued function on the circle such that $w=-\infty$ on $K$ , and tends continuously to $-\infty$ as $e^{i\theta}$ approaches $K$ ; $w\leq-1$ on the circle; $w$ is finite-valued and continuously differentiable on $C-K$ ; $w$ is integrable. Such a $w$ can be found since $K$ has measure zero. One naive way to construct such a function is the following. Since $K$ is closed, the complement $C-K$ is the union of a countable number of disjoint open intervals (arcs) $I_n$ . Let $\varepsilon_n$ be the length of $I_n$ . Choose a strictly positive and continuously differentiable function $y_n$ on $I_n$ such that $y_n\leq e^{-1}$ , $y_n$ tends to zero at the endpoints of $I_n$ , and $$\int_{I_n}\log y_n\geq-2\varepsilon_n.$$ If we define $y$ to be zero on $K$ and $y=y_n$ on $I_n$ , then $0\leq y\leq e^{-1}$ ; the zeros of $y$ are precisely the points of $K$ ; $y$ is continuous on $C$ and continuously differentiable on $C-K$ ; and $\log y$ is integrable. Let $w=\log y$ . Now define $$h(z)=\frac{1}{2\pi}\int_{-\pi}^\pi\frac{e^{i\theta}+z}{e^{i\theta}-z}w(\theta)~\mathrm{d}\theta.$$ Then $h$ is analytic in the open disc and $\operatorname{Re}h\leq-1$ . By property 3 of $w$ and the Corollary above, $h$ is actually continuous on the complement of $K$ in the closed disc. Since $w$ tends continuously to $-\infty$ at each point of $K$ , the function $$\operatorname{Re}h(r,\theta)=\frac{1}{2\pi}\int_{-\pi}^\pi w(t)P_r(\theta-t)~\mathrm{d}t$$ tends radially to $-\infty$ for each $\theta$ in $K$ . Now let $$g=\frac{1}{h}.$$ It is apparent that $g$ is in $A$ , $\operatorname{Re}g\leq0$ , and the zeros of $g$ on the closed disc are exactly the points of $K$ . We remark that $\operatorname{Re}g=0$ exactly on $K$ . $\textbf{Errors:}$ The goal of Fatou's theorem is to construct a function $g$ in $A$ who's restriction to the boundary vanishes exactly on a given subset $K$ of the circle with measure zero. In the proof of Fatou's theorem this is done by first constructing an analytic function in the open disk, $h$ , via a continuous function on the boundary, $w$ . Since the complement of $K$ on the circle consists of open arcs, to show that the imaginary part, $v(r,\theta)$ , of $h$ is continuous on the complement of $K$ in the closed disk, it suffices to show that $v_{r}(\theta)=v(r,\theta)$ converges uniformly as $r\rightarrow 1$ on the corresponding sector. This is the content of the corollary where $w$ will be playing the role of $f$ . $\textbf{1.}$ The biggest problem with the proof of Fatou's theorem is its reliance on the previous Corollary. Hoffman claims that continuous differentiability of $f$ on an interval $[\theta_{0}-\delta, \theta_{0}+\delta]$ implies that $v(r,\theta)$ converges uniformly to $v$ on this interval as $r\rightarrow 1$ , but the proof he gives falls short. He claims in the last sentence of the proof of the corollary that we can find $\epsilon>0$ such that $\phi_{\theta}(t)$ is uniformly bounded on $[\theta_{0}-\delta, \theta_{0}+\delta]\times [-\epsilon,\epsilon]$ . The idea seems clear: since $f'$ is bounded on this interval we can appeal to the mean value theorem to show that $\phi_{\theta}(t)$ is bounded. But a glance at the definition of $\phi_{\theta}(t)$ shows that this argument wont work since when $\theta=\theta_{0}\pm\delta$ , $f(\theta\pm t)$ takes values outside of $[\theta_{0}-\delta, \theta_{0}+\delta]$ whenever $t\neq 0$ . Now, the mean value theorem does imply that $\phi_{\theta}(t)$ is uniformly bounded for $\theta$ in a slightly smaller interval, but then we don't get the uniform convergence of $v_{r}$ on the entire interval $[\theta_{0}-\delta, \theta_{0}+\delta]$ which is needed for the proof of Fatou's theorem (where $[\theta_{0}-\delta, \theta_{0}+\delta]$ plays the role of the arcs $I_{n}$ ). $\textbf{2.}$ The second issue that I see with the proof of Fatou's theorem is with the construction of $w$ , which relies in turn on the construction of a continuous function $y$ on the circle. However the function $y$ he constructs need not be continuous at points of $K$ as he claims, which is necessary for $w$ to converge to $-\infty$ at points of $K$ . Indeed, since the average of $\text{log}(y_{n})$ is greater than $-2$ on $I_{n}$ , $y_{n}$ must take value greater than $e^{-2}$ on each $I_{n}$ . But if $x\in K$ , then it is possible that every neighborhood of $x$ contains an arc $I_{n}$ , and hence $y$ cannot be continuous at $x$ . $\textbf{3.}$ Finally, even if a function $w$ can be constructed with the four desired properties, I do not see why radial convergence of $\text{Re}(h)$ to $-\infty$ at points $\theta\in K$ is sufficient to imply continuity of $1/h$ on the closed disk since we don't know that this convergence happens at the same rate for each point in $K$ . $\textbf{Question.}$ Considering how popular the book was (Walter Rudin calls it a ""classic"" in his Real and Complex Analyis) and how large the errors in this proof are, it is surpising that they were never corrected in a later edition.  I would really like to know if there is any way to save these arguments to provide a correct proof of Fatou's theorem along these same lines. This would require a new argument for showing that $|v(r,\theta)-v(\theta)|$ converges uniformly on $[\theta_{0}-\delta, \theta_{0}+\delta]$ and a new way to construct a function $w$ with the desired properties.","['hardy-spaces', 'measure-theory', 'complex-analysis', 'solution-verification', 'functional-analysis']"
4734038,New depth of water in a cylindrical tank,"A cylindrical tank has a diameter of $60 cm$ and a height of $100 cm$ .  It is filled with water to a depth of $10 cm$ , then tilted by an angle $\theta = 30^\circ$ .  What will be the new depth of water?  i.e. how high above the ground is the water surface in the tilted tank? My Approach: By taking horizontal slices of the water volume in the tilted tank, and integrating them to find the volume as a function of a set value $h$ , and then iterating through Newton's iteration, I found that $h = 23.18cm$ Is there is a simpler or more direct way to solve this problem ?","['newton-raphson', 'analytic-geometry', 'conic-sections', 'geometry']"
4734095,"In how many ways the letters of the word ""LOCKDOWN"" can be arranged such that N is always between O's?","My attempt :- I made all the possible cases by fixing the positions of O in word, for example :- O at the 1st place and 3rd place O at the 1st place and 4th place O at the 1st place and 5th place O at the 1st place and 6th place O at the 1st place and 7rd place O at the 1st place and 8th place O at the 2nd place and 4th place
.
.
.
.
. This process seems too ineffecient for solving this problem. Official answer is given as $\frac{8!}{2!}$ /3 = $6720$ I understand that $\frac{8!}{2!}$ is the total 8 letter rearrangements of the word but why in the solution is the author dividing this by 3 ?",['combinatorics']
4734102,Finding the function that corresponds to a Fourier series,"Im trying to reverse engineer a Fourier series that came about in my work by happenstance. The series in question: $$
\sum^{\infty}_{n=0} \frac{c}{c^{2}+(2n+1)^{2}}\sin((2n+1)x)-\frac{2n+1}{c^{2}+(2n+1)^{2}}\cos((2n+1)x)
$$ The sum in question only needs to be valid on $x\in [0 , t] , t < \pi$ , so it may correspond to not necessarily periodic function. The series is very close to a kind of triangle wave if it weren't for c. You may also notice that the coefficients correspond to the Laplace transform's: $\mathscr{L}\{ \sin(ct)\}(2n+1) , \mathscr{L}\{ \cos(ct)\}(2n+1)$ (for the respective function). This seems like it could be leveraged but I have been unsuccessful in doing so. Another avenue I have persued is expressing it in terms of the Lerch-Zeta fuction as follows. $$
-\frac{1}{2}\operatorname{Re}\bigg(e^{ix}\zeta\bigg(\frac{x}{\pi},1,\frac{1}{2}(1-ic)\bigg) \bigg)
$$ (Edited because it was incorrectly the imaginary part)
But this seems like a very heavy piece of mathematical machinery for a series such as this. I have tried to find lists of known Fourier series but haven't seen it apear in those lists. Any help you can provide on any potential leads would be great, Thank You! Addition : After working the problem for a bit one addition I can make is the following expression is equivalent to the series above. $$
\frac{e^{-cx}}{2}\operatorname{Re}\bigg(\psi\big( \frac{1-ic}{2} \big) \bigg)-e^{-cx}\operatorname{Im}\bigg(\int^{x}_{0} te^{ct}\mathscr{L}^{-1}\{ \ln\big(\Gamma\big( \frac{1+is}{2} \big)\big)  \}(t) dt \bigg)
$$ This was found by taking the Laplace transform of the Lerch-Zeta function, which then produced a series that could be represented with Digamma functions. I then took the inverse Laplace transform of this expression using known properties of the Laplace transform. However, now the problem can be characterized by trying to find the inverse Laplace transform of either the Digamma function or the Log-Gamma function on the $\operatorname{Re}(z)=1/2$ line of the complex plane. I have been unable yet to find such an expression. I have an inkling that it may involve the Jacobi-Theta function and Modular Forms by looking at different expansions of the Digamma/Log-Gamma functions, but haven't been able to over-come some domain conditions and nuances regarding Laplace transforms and complex logarithm's. Again any help you can provide would be greatly appreciated.","['fourier-series', 'zeta-functions', 'sequences-and-series']"
4734106,"Is there an equation similar to square root, but faster for a computer to compute?","I'm making an app that uses a Fermat's spiral to space objects out in an aesthetically pleasing way.  This is a change from my first attempt, which used an Archimedean spiral , but I felt that the outer objects became too far apart. Fermat's spiral uses a square root to calculate the current radius: $r = \sqrt{\theta}$ and $\theta = x$ Square root is a relatively slow algorithm for a computer to calculate for each point, and I don't really need the function to be exactly square root, just to increase slower as it gets larger (preferably non-asymptotically, but could be). When I look for functions ""similar to square root"" I get nowhere.  Remembering high school math, I think $f(x) = 1-\frac{1}{x+1}$ might work alright. What are some more functions $f(x)$ that, similarly to square root, increase fast at first, but slow down as $x$ increases? In case it causes problems, I'm not asking for opinions on which are better, I just want options (or a link to a bunch, if such exist), though help on making my spiral thing better would also be neat, if I'm just barking up the wrong tree for aesthetic fairly-even-distribution around a point.","['functions', 'geometry', 'approximation-theory']"
4734108,"How to evaluate $\int_0^{\frac{\pi}{2}} \frac{\ln(\cos(x))}{1+\sin^2(x)} \, dx$","I saw this problem $$\int_0^{\frac{\pi}{2}}  \frac{\ln(\cos(x))}{1+\sin^2(x)} \, dx$$ on my problem book but I have no idea how to evaluate it.
I reduced the integral like this by king's rule $$I_1= \int_0^{\frac{\pi}{2}}  \frac{\ln(\cos(x))}{1+\sin^2(x)}dx =\int_0^{\frac{\pi}{2}}  \frac{\ln(\sin(x))}{1+\cos^2(x)}dx $$ by using the double angle rule $$I_1=\int_0^{\frac{\pi}{2}}  \frac{\ln(2\sin(x/2) \cos(x/2))}{2\cos(x/2)^2}dx$$ we let $x/2=x$ then $dx=2dx$ $$I_1=\int_0^{\frac{\pi}{4}}  \frac{\ln(2)}{\cos(x)^2}dx+\int_0^{\frac{\pi}{4}}  \frac{\ln(\sin(x))}{\cos(x)^2}dx+\int_0^{\frac{\pi}{4}}  \frac{\ln(\cos(x))}{\cos(x)^2}dx$$ now I will denote $\int_0^{\frac{\pi}{4}}\frac{\ln(2)}{\cos(x)^2}dx$ by $I_2$ , and I will denote $\int_0^{\frac{\pi}{4}}\frac{\ln(\sin(x))}{\cos(x)^2}dx$ by $I_3$ , and I will denote $\int_0^{\frac{\pi}{4}}  \frac{\ln(\cos(x))}{\cos(x)^2}dx$ by $I_4$ $I_2$ is easy to do $$I_3=\frac{\ln(\tan(x))}{\cos(x)^2}dx +I_4$$ I will denote $\int_0^{\frac{\pi}{4}}\frac{\ln(\tan(x))}{\cos(x)^2}dx$ by $I_5$ which is easy to do. the problem was to evaluate $2I_4$ which I couldn't do","['integration', 'calculus', 'definite-integrals']"
4734125,Lipschitz functions (Theorem 1.4 of Condenser Capacities and Symmetrization in Geometric Function Theory),"I am struggling to understand the second part of the following proof of Theorem 1.4 of the book ""Condenser Capacities and Symmetrization in Geometric Function Theory"" by Vladimir N. Dubinin (in what follows $U(z_0,r) \subset \mathbb{C}$ is the open disk of radius $r > 0$ and centre $z_0$ , $\text{Lip}(S)$ refers to the Lipschitz continuous real-valued functions on the set $S \subset \mathbb{C}$ and all the results referred to in the proof of Theorem 1.4 have been provided below the proof of Theorem 1.4): $\textbf{Theorem 1.4 and proof}$ That is, I do not understand what the author means by ""Repeating the above reasoning for each connected component and taking account of Theorem 1.3 we obtain $v(\phi(z)) \in \text{Lip}(\overline{U(z_0,r)})$ "" where this sentence occurs in the image below: The results used in the proof of Theorem 1.4 are Theorem 1.3 and Lemma 1.1, and Theorem 1.1 which are given below for reference. $\textbf{Lemma 1.1 and proof}$ $\textbf{Theorem 1.3 and proof}$ $\textbf{Theorem 1.1 and proof}$ Theorem 1.1 says that a function $v$ defined on a compact planar set $E$ is Lipschitz on $E$ if and only if it is locally Lipschitz on $E$ , that is if and only if there is a collection of open sets $U_{i}$ covering $E$ , such that $v \in \text{Lip}(U_{i} \cap E)$ for every $i$ . $\textbf{What I think is going on:}$ What seems to be happening is that each connected component $C_j$ of $U(z_0,r) \cap B$ should contain $z_0$ on its boundary, and $z_0$ should be contained on an analytic arc of $\partial B$ which forms part of the boundary of $C_j$ . Therefore we should be in an analogous situation to when we assumed $U(z_0,r) \cap B$ were connected, because under these assumptions we should be able to extend $\phi \restriction C_j$ analytically to a small open disk around $z_0$ for each connected component $C_j$ of $U(z_0,r) \cap B$ , and then apply Theorem 1.3 as the author implies. $\textbf{Where I have difficulty understanding:}$ : However, there are several things that I am totally lost on. The first of which being, how do we know for sure that each connected component $C_j$ of $U(z_0,r) \cap B$ has as part of its boundary, an analytic arc containing $z_0$ (more precisely, it seems it is possible to lose analyticity at $z_0$ , e.g. if two curves at $z_0$ form the boundary and have different tangent vectors at $z_0$ )? Moreover, what is a suitable definition of ""boundary consist of finitely many piecewise analytic curves"" (I am assuming the definition coincides with the definition I provide in $\textbf{Edit 3}$ of this post ). It would be great if anyone could shed light on why what the author seems to be doing can be done.","['analytic-continuation', 'plane-curves', 'lipschitz-functions', 'analysis', 'complex-analysis']"
4734135,Any relation between $|\mathbf{a}\times\mathbf{b}|^{2}+|\mathbf{a}\cdot\mathbf{b}|^{2}=|\mathbf{a}|^{2}|\mathbf{b}|^{2}$ and Pythagoras' Theorem?,"With vectors, we have this result : $$\left|\mathbf{a}\times\mathbf{b}\right|^{2}+\left|\mathbf{a}\cdot\mathbf{b}\right|^{2}=\left|\mathbf{a}\right|^{2}\left|\mathbf{b}\right|^{2}$$ (This result also works in the 2D case.) It looks similar to Pythagoras' Theorem so I was wondering if there might indeed be any relation (or if it's just a coincidence). Definitions used: In 3D case, let $\mathbf{a}=(a_1,a_2,a_3)$ and $\mathbf{b}=(b_1,b_2,b_3)$ . Then $\mathbf{a}\times\mathbf{b}=(a_2b_3-a_3b_2,a_3b_1-a_1b_3,a_1b_2-a_2b_1)$ , $\mathbf{a}\cdot\mathbf{b}=a_1b_1+a_2b_2+a_3b_3$ , $|\mathbf{a}|=\sqrt{a_1^2 +a_2^2 +a_3^2}$ . In 2D case, let $\mathbf{a}=(a_1,a_2)$ and $\mathbf{b}=(b_1,b_2)$ . Then $\mathbf{a}\times\mathbf{b}=a_1b_2-a_2b_1$ , $\mathbf{a}\cdot\mathbf{b}=a_1b_1+a_2b_2$ , $|\mathbf{a}|=\sqrt{a_1^2 +a_2^2}$ . Pythagoras' Theorem: If $\mathbf{a}\cdot\mathbf{b}=0$ , then $|\mathbf{a}|^2+|\mathbf{b}|^2=|\mathbf{a}+\mathbf{b}|^2$ .","['linear-algebra', 'vectors']"
4734136,"$M$ is inside the parallelogram $ABCD$. If $\angle MBA=\angle MDA$, then $\angle MAB = \angle MCB$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question Let M be a point inside the parallelogram ABCD, such that $\angle MBA = \angle MDA$ . Prove that $\angle MAB = \angle MCB$ . Source: Romanian Mathematical Gazette no. 5/2023","['contest-math', 'angle', 'geometry']"
4734152,"If $f_n \rightarrow f$ weakly in $L^p$, then $\sqrt{f_n} \rightarrow \sqrt{f}$ weakly in $L^{2p}$?","Suppose $||f_n||_{L^p(\Omega)} \leq C$ , where $\Omega$ is a bounded set in $\mathbb{R}^n$ . Moreover, $f_n \geq 0$ . Using weak compactness, we know that there exists a subsequence $\{f_{n_k} \}$ such that $f_{n_k} \rightarrow f$ weakly in $L^p$ . Since $||\sqrt{f_{n_k}}||_{L^{2p}} \leq C$ , we similarly obtain $\sqrt{f_{n_k}} \rightarrow \sqrt{g}$ weakly in $L^{2p}$ , up to a subsequence. My question is whether $f=g$ . I guess $f=g$ due to the choice of subsequence.
If true, how to prove it?","['lp-spaces', 'weak-topology', 'real-analysis']"
4734160,Classifiying algebraic group extensions of $\mathbb{Z}/p\mathbb{Z}$ by $\mu_p$,"I am trying to solve exercise 2.6 in Milne's book on Algebraic Groups, which states the following: Let $k$ be a a field of characteristic $p$ , show that the isomorphism class of extensions $0 \rightarrow \mu_p \rightarrow G \rightarrow \mathbb{Z}/p\mathbb{Z} \rightarrow 0$ , with G a finite commutative alg. group, are classified by extension $k^\times/(k^\times)^p$ .
Show that $G_{red}$ is not a subgroup of G unless the extension splits. After trying for a bit, I wasn't even able to construct any non trivial extensions that fit into the above sequence, let alone have any idea on how to classify such extensions. I also tried constructing extensions using the functorial approach but couldn't find anything non trivial there either. Help in constructing those extensions would be greatly appreciated! Edit: The following isn't an answer, but is here to complement @AlexYoucis answer based on my time with trying to understand it and working it out, hopefully this will be useful for any future reader. Please read it after reading the below answer. First, for the ring of functions of G. We may work on each coordinate seperately. Our condition in the coordinate $i$ is functions $f(X,Y)$ that satisfy $f(X,T)=f(X(T/S)^i,S)$ where $S,T$ are p'th roots of a, and $X^p=1$ . We may write $f(X,Y) = \Sigma c_{nm}X^nY^m$ where the sum runs over pairs $(n,m) \in \mathbb{Z}/p\mathbb{Z} \times \mathbb{Z}/p\mathbb{Z}$ Easy combinatorics show that for $i\neq0$ this gives a polynomial ring generated by $XY^i$ , which is isomorphic to the field $k(\sqrt[p]{a})$ . For $i=0$ this gives the polynomial ring generated by X, which is isomorphic to $\mathscr{O}(\mu_p)$ . And our ring $\mathscr{O}(G)$ is the product of those rings. Note in part that this ring is non reduced. Secondly, I had an issue with the fact that $G(k)$ doesn't surject onto $\mathbb{Z}/p\mathbb{Z}(k)$ , as would be expected from an exact sequence of groups. This is infact just a misunderstanding by me of what it means for a sequence of algebraic groups to be exact. For an algebraic group sequence to be exact, the functor of points sequence is only required to be left exact. The condition for right exactness here is instead that the map is faithfully flat (alternatively - flat + surjective as a map of schemes), so that the above construction of $G$ does give us a valid extension.","['algebraic-groups', 'positive-characteristic', 'galois-theory', 'algebraic-geometry', 'group-theory']"
4734164,A small proportion of the non-zero eigenvalues lies near zero,"Problem: We consider a sequence $(A_n)_{n\ge 1}$ of matrices such that $A_n$ is a $n\times n$ matrix with integer co-efficients, The sum of the absolute values of entries of any row is at-most $M$ . For every $\delta>0$ , let $N_{\delta}(A_n)$ denote the number of eigenvalues of $A_n$ of modulus strictly between $0$ and $\delta$ . Prove that If $\lambda$ is an eigenvalue of $A_n$ , then $|\lambda|\le M$ . Show that for every $\epsilon>0$ , there exists $\delta>0$ such that for every $n\ge 1$ , $$N_{\delta}(A_n)\le \epsilon n $$ My attempt: I could prove the first part using triangle inequality and stuff like that. But am completely stumped on the second part. I tried to proceed by contradiction and constructing sequence of eigenvalues, but that didn't seem to lead anywhere. I would appreciate some hints towards the correct direction, rather than a complete solution.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4734233,Calculus with little-o notation: Follow-up,"This question is a follow-up of Calculus with little $o$ notation where I asked if it was true that $$
f'(x) = o (x^\alpha) \qquad \implies \qquad f(x) = f(0) + o(x^{\alpha+1}) \quad \forall \quad  \alpha > 0 ~ ?? \tag{1}
$$ where $f : [0,\infty) \to {\mathbb R}$ is a smooth function and we are taking the limit $x \to 0$ . I got two great answers showing that this was indeed true. I have two follow-up questions: I believe those two proofs can be extended trivially to the region $\alpha > -1$ . Is this correct? Can we generalize (1) to $\alpha < -1$ in some way? In particular, is the following true: $$
f'(x) = o (x^\alpha) \qquad \implies \qquad f(x) = o(x^{\alpha+1}) \quad \forall \quad  \alpha \leq -1 ~ ?? \tag{2}
$$ where now $f:(0,\infty) \to {\mathbb R}$ is a smooth function. EDIT: Apparently, if I don't add context for this question, the post will be closed. I'm certain the context won't help at all, but here goes. I'm trying to solve the BPS equations in a $U(1)^3$ truncation of $N=8$ gauged supergravity in five dimensions and trying to understand the near horizon geometry of supersymmetric black holes with charged scalar hair (so-called hairy black holes). After a significant amount of simplification, I managed to boil my problem down to a set of 5 coupled ordinary differential equations for 5 functions. I am able to make progress towards my goal if equation (2) of the post holds (or some version thereof) and that's why I'm asking if such a theorem could possibly be true. For example, one of the 5 differential equations is of the form $$
(2 x U )' = H \sqrt{ 4 + \Phi^2 } . 
$$ I know that $H=r_+^2 + o(1)$ and $\Phi = C x^{-\alpha} + o(x^{-\alpha})$ for some $\alpha,r_+,C > 0$ (as $x \to 0$ which is where the horizon of the black hole is located). Given this information, what can I deduce about the near-horizon behaviour of $U(x)$ (i.e. its behaviour as $x \to 0$ )?","['asymptotics', 'ordinary-differential-equations']"
4734271,A planar cut through an oblique cone,"An oblique cone has a circular base centered at the origin of radius $5$ , and an apex at $(0, 5, 20)$ .  A plane whose equation is $3x-4y+5z = 40$ cuts through the oblique cone, and the resulting cut is an ellipse.  Find the equation of the ellipse in the form: $ P(t) = C + w_1 \cos t + w_2 \sin t $ where $C$ is the center and $w_1$ and $w_2$ are the vectors specifying the semi-minor and semi-major axes of the ellipse. My Attempt: My attempt is incomplete, but the basic idea I am following is to parameterize the plane, as follows, $ r = r_0 + V u $ and then plug this into the equation of the cone, which I don't have. So I still have to find a way to find the algebraic (implicit) equation of the cone.  And from there, it should be straight forward to write a quadratic form in $u$ which will give the two axes of the cut ellipse. Any hints, comments or solutions are highly appreciated.","['analytic-geometry', 'conic-sections', 'geometry', 'linear-algebra', 'quadrics']"
4734327,Are there harmonic 1-forms which induce harmonic circle-valued maps?,"I was reading this question and it got me thinking more about harmonic maps. A smooth circle-valued map $\varphi : M \to S^1$ from a Riemannian manifold is harmonic if $\varphi^*(d \theta)$ is in the kernel of Hodge Laplace operator $\Delta^1 : \Omega^1(M) \to \Omega^1(M)$ . It seems to me that, given a nice enough harmonic $\omega \in \Omega^1(M)$ , one might be able to induce a harmonic map. Suppose that $\omega$ is harmonic, and in addition, that $$\int_\gamma \omega \in \mathbb{Z}$$ for all $[\gamma] \in \pi_1(M)$ . Since $\omega$ is closed, this doesn't depend on the homotopy class of $\gamma$ . One should be able to define a map $\varphi_\omega : M \to S^1 = \mathbb{R}/\mathbb{Z}$ by mapping a base-point $x$ of $M$ to $1$ and defining the rest of the map via $$\varphi_\omega(x') \mapsto exp(2 \pi i \int_\gamma \omega)$$ for any path $\gamma$ from $x$ to $x'$ . My question: is it possible that $$ \varphi_\omega^*(d \theta) = \omega$$ thus recovering the original harmonic and generating a harmonic map? The example in my head is a harmonic on $S^1 \times S^1$ which 'wraps' around the first coordinate.","['laplacian', 'harmonic-functions', 'differential-forms', 'differential-geometry']"
4734331,$L_1$-convergence in the Lebesgue differentiation theorem for general Radon measures?,"Let $\mu$ be a (non-negative) measure on $\mathbb R$ and let $f \in L_1(\mu)$ . For all $h > 0$ and ( $\mu$ -almost-)all $x \in \mathbb R$ , let $$
f_h(x) = \dfrac{\int\limits_{x-h}^{x+h} f(y)\ \mu(dy)}{\mu([x-h,x+h])}.
$$ If $\mu$ is a Radon measure, then Lebesgue's differentiation theorem says that $f_h$ converges to $f$ $\mu$ -almost-everywhere (see e.g. Herbert Federer's ""Geometric Measure Theory"", Chapter 2.9). On the other hand, if $\mu$ is the Lebesgue measure, then it is known that $f_h \in L_1$ for all $h$ , and that $f_h$ converges to $f$ in $L_1$ (i.e. $\Vert f_h - f \Vert_{L_1} \to_{h\to 0} 0$ ). My question is: in general, when $\mu$ is any Radon measure, does $f_h$ converge to $f$ in $L_1(\mu)$ ? In fact, are the $f_h$ even in $L_1(\mu)$ ? And if not, are there additional assumptions on $\mu$ that make it true? (If this is indeed true, or at least true modulo assumptions on $\mu$ , then I'm also curious of any generalisations to spaces other than $\mathbb R$ , similar to how Lebesgue's differentiation theorem generalises to any metric space equipped with a Vitali relation.)","['integration', 'measure-theory', 'real-analysis']"
4734435,Why is $\text{Homeo}_p(X)$ not a topological group for $X = \mathbb{R}^2$?,"Why is $\text{Homeo}_p(X)$ , the space of homeomorphisms of $X$ with topology of pointwise convergence and operation of composition not a topological group for $X = \mathbb{R}^2$ ? This question was asked before and got an answer here but without proof. It's clearly a quasi-topological group, the author claims composition is not jointly continuous. I found somewhere it's a topological group for $X = \mathbb{R}$ for example, so the author might be wrong (not sure). I'm looking for a proof.","['general-topology', 'topological-groups']"
4734454,Is it true that finite-dimensional vector spaces can have infinite sequences as elements?,"I recently encountered the following definition of a finite-dimensional vector space in Axler's Linear Algebra Done Right : A vector space is called finite-dimensional if some list of vectors in it spans the space . This I was then thinking about $V = \{(x, 0, 0, \cdots): x \in \mathbb{R} \}$ i.e., the set of all infinite sequences whose first value is some real number and for which all other values are 0. The book defines ${\mathbb{R}}^{\infty} = \{(x_1, x_2, \cdots): x_j \in \mathbb{R} \text{ for } j = 1, 2, \cdots \}.$ He defines addition and scalar multiplication with ${\mathbb{R}}^{\infty}$ as you'd expect: $(x_1, x_2, \cdots) + (y_1, y_2, \cdots) = (x_1+y_1, x_2+y_2, \cdots)$ and $\lambda(x_1, x_2, \cdots) = (\lambda x_1, \lambda x_2, \cdots)$ . Clearly $V$ is a subspace of $\mathbb{R}^{\infty}$ . Now since any $v = (x, 0, 0 \cdots) \in V$ can be written $v = x(1, 0, 0, \cdots)$ where $x \in \mathbb{R}$ and $(1, 0, 0, \cdots) \in V$ , $(1, 0, 0, \cdots)$ alone spans $V$ . Since this list of just one vector in $V$ spans $V$ , I am inclined to think that $V$ is finite-dimensional. This does not mesh with any pre-existing intuition I had about what finite-dimensional vector spaces were, nor can I find any examples like this of a finite-dimensional vector space online, which leads me to think there is an error in my reasoning. Is this correct? If not, can someone point me to the error in my reasoning?","['vectors', 'linear-algebra', 'vector-spaces']"
4734502,"In $\triangle ABC$, prove: $\frac{\sin^2 \frac A2}{\sin B \sin C}+\frac{\sin^2 \frac B2}{\sin A\sin C}+\frac{\sin^2 \frac C2}{\sin A\sin B} \ge 1$","In a $\triangle ABC $ , prove : $$\frac{\sin^2 \dfrac{A}{2}}{\sin B \sin C} + \frac{\sin^2 \dfrac{B}{2}}{\sin A\sin C} +\frac{\sin^2 \dfrac{C}{2}}{\sin A\sin B}  \geq 1 $$ My approach : $$2\left(\frac{\sin^2 \dfrac{A}{2}}{\sin B \sin C} + \frac{\sin^2 \dfrac{B}{2}}{\sin A\sin C} +\frac{\sin^2 \dfrac{C}{2}}{\sin A\sin B}\right)  \geq 2  $$ $$\frac{1-\cos A}{\sin B \sin C} + \frac{1-\cos B}{\sin A\sin C} +\frac{1-\cos C}{\sin A\sin B} \geq  2  $$ Using $-\cos A = \cos (B+C) = \cos B \cos C -\sin B \sin C  $ $$\frac{1-\cos A }{\sin B \sin C } = \frac{1}{\sin B \sin C} + \frac{ \cos B \cos C -\sin B \sin C}{\sin B \sin C} = \frac{1}{\sin B \sin C}+\cot B \ \cot C - 1 $$ $$ \frac{1}{\sin B \sin C} +\cot B \ \cot C - 1 + \frac{1}{\sin A \sin C} +\cot A \ \cot C - 1 
 + \frac{1}{\sin A \sin B} +\cot A \ \cot B - 1  \geq  2 $$ Using : $ A+B+C = \pi  ,   \cot B \ \cot C   +\cot A \ \cot C 
 +\cot A \ \cot B = 1   $ $$ \frac{1}{\sin B \sin C}  + \frac{1}{\sin A \sin C}  + \frac{1}{\sin A \sin B}   \geq  4 $$ It is equivalent to : $$\frac{\sin A + \sin B + \sin C }{\sin A  \sin B  \sin C } \geq  4    $$ How do I proceed from here?","['contest-math', 'inequality', 'trigonometry', 'triangles', 'algebra-precalculus']"
4734516,Modifying critical point of function on manifold ODE theory,"Assume $M$ is compact Riemannian manifold and $g(t)$ is a time dependent metric. Assume $\Psi:M\times [0,C)\rightarrow \mathbb{R}$ is smooth and $C<\infty$ . Suppose we have $(\partial_t\Psi -\Delta_{g(t)} \Psi)\geq 0$ . If $\Psi\geq 0$ on $M\times 0$ , then, $\Psi \geq 0$ everywhere. Proof is as follows: It suffices to assume that case $\Psi>0$ . Now, assume for contradiction $\Psi<0$ somewhere. Since $M$ is compact, we can find $(x'',t'')$ such that $\Psi(x'',t'')=0$ and $\Psi(x,t)\geq 0$ for all $x\in M$ and all $t\in [0,t'']$ . Then on this point, $\partial_t \Psi$ $\leq 0$ and $\Delta \Psi \geq 0$ . I am confsed as to why compactness implies the existence of $(x'',t'')$ as above and why that implies $\partial_t \Psi\leq 0$ . Note. For fixed $x$ , there exists $t_x$ such that $\Psi(x,t_x)=0$ . For fixed $t$ there exists unique $x_t$ such that $\Psi(x_t,t)\geq  \Psi(x,t)$ for all $x\in M$ . There are proofs but using maximal time, I don't understand why such time exists by compactness.","['partial-differential-equations', 'functional-analysis', 'riemannian-geometry', 'differential-geometry']"
4734538,Doubt hindering my intution about vector bundles,"Lee, Introduction to smooth manifolds, defines a vector bundle $\pi :E \rightarrow M$ via some local trivialization maps $\pi^{-1}(U) \rightarrow U \times R^k$ , where $U \subset M$ . There is something about this that is a bit hindering my intuition. Do I have to consider $R^k$ here as some coordinates of a vector space, or as a fixed vector space itself? Why did not Lee define just a fixed a vector space $V$ and define trivializations as functions $\pi^{-1}(U) \rightarrow U \times V$ ?. A vector space $V$ is still a manifold and in some sense this definition is less confusing for me because $R^k$ can be either considered as a fixed vector space, or some coordinates after fixing a base in $V$ ... $U \times R^k$ looks to me a bit weird since $U$ is just a subset of a manifold, whereas $R^k$ looks like some coordinates in a chart. Please be patient and consider that I am still trying to build a basic understanding/intuition..","['manifolds', 'general-topology', 'self-learning', 'differential-geometry']"
4734580,Equilateral triangle separating two homothetic circles,"This question is a follow-on of a question asked today that has been erased by its author an hour after being asked, for unknown reasons. I found it interesting ; I have a solution, mainly based on analytic geometry. I would appreciate to see other solutions (using synthetic geometry for example), hopefuly less computational. Let me reformulate it in a slightly different way compared to the original question : Given : a fixed line with two points $B$ and $C$ on it. Let us construct $A$ such that $ABC$ is equilateral . Let us take an arbitrary point $F$ on line $BC$ outside line segment $[BC]$ . Let us denote by $D$ , resp. $E$ , the center of the incircle of triangle $FAC$ , resp. the center of the excircle of triangle $FAB$ opposed to $F$ . Prove that $$BE+CD=BC\tag{1}.$$ Solution : Let us define $A'$ as being the symmetrical point of $A$ with respect to line $BC$ . $A',C,D$ are aligned. Indeed, slope(A'C)=slope(C'D) for the following reason : as $\angle ACF = 180°-60°=120°$ , we have $\angle DCF=\angle DCA=\angle ACB = 60°$ . For a similar reason, $A',B,E$ are aligned. Let us call $D',E'$ the projections of $D,E$ resp. on line $AB$ ; (1) amounts to say that : $$BE'+CD'=\tfrac12 BC\tag{2}.$$ Let us take coordinate axes such that : $$B(-1,0),C(1,0),A(0,\sqrt{3}),F(a,0) \ \text{with} \ a>1$$ as one can see on the figure. Let $2 \alpha = \angle AFB$ . We have $$\tan(2 \alpha)=\frac{\sqrt{3}}{a}$$ Using relationship $$\tan(2 \alpha)=\frac{2 t}{1-t^2} \ with \ t:=\tan(\alpha)\tag{3}$$ we get : $$t:=\tan(\alpha)=\frac{\sqrt{3+a^2}-a}{\sqrt{3}}$$ (but in fact, we will not use this relationship). Point $D$ being at the intersection of : $$\begin{cases}\text{Line FD : } & y=-t(x-a)\\ \text{Line A'C : }  & y=\sqrt{3}(x-1)\end{cases}$$ its abscissa, which is the same as the abscissa of $D'$ , is $$x_{D'}=\frac{at+\sqrt{3}}{t+\sqrt{3}}$$ For a similar reason : $$x_{E'}=\frac{at+\sqrt{3}}{t-\sqrt{3}}$$ (2) will be established if we show that : $$\left(-1-\frac{at+\sqrt{3}}{t-\sqrt{3}}\right)+\left(\frac{at+\sqrt{3}}{t+\sqrt{3}}-1\right)=1\tag{4}$$ With some simplifications, the LHS of (4) becomes : $$(at+\sqrt{3})\frac{-2 \sqrt{3}}{t^2-3}-2\tag{5}$$ But, using (3), $$t^2-1=\frac{-2ta}{\sqrt{3}}\tag{6}$$ Plugging (6) into (5) gives this LHS equal to $1$ as awaited. Edit : I hadn't understood at first the track followed by the asker of the initial question, based on the sine law ; it can be completed by checking that for all $\theta$ : $$\frac{\sin(\theta)}{\sin(2\pi/3-\theta)}+\frac{\sin(\pi/3-\theta)}{\sin(\pi/3+\theta)}=1.$$",['geometry']
4734660,Does every operator have a hermitian adjoint?,"If we think of operators as matrices, every matrix can be transposed and its elements can be complex-conjugated. But the identification of the hermitian adjoint with the transpose conjugate comes from inner products. Namely, the hermitian adjoint of $\hat{T}$ is $\hat{T}^{\dagger}$ where $\hat{T}^{\dagger}$ satisfies $$(u,\hat{T}v) = (\hat{T}^{\dagger}u,v).$$ If $T_{ij} = (i,\hat{T}j),$ then $$T_{ij}^{\dagger} = (i,\hat{T}^{\dagger}j) = (j,\hat{T}i)^* = T_{ji}^*.$$ But how do we know that the map $\hat{T}^{\dagger}$ which satisfies the desired relationship exists in the first place? Can we show it without thinking of operators as matrices, just using the inner product? Also, does the relationship $$\big(\hat{T}^{\dagger}\big)^{\dagger} = \hat{T}$$ always hold? Since it seems to be used in the derivation of $T_{ij}^{\dagger}.$","['quantum-mechanics', 'linear-algebra']"
4734695,"“Simple” formula, besides Cartan’s equation, for exterior covariant derivative of vector-valued forms on principal bundle","Let $P$ be a principal bundle with structure group $G$ , equipped with a principal connection, whose connection $1$ -form we denote $\omega$ . Suppose we also have a linear representation $\rho:G\to GL(V)$ (which then gives rise to a right-action $(g,v)\mapsto\rho(g^{-1})(v)$ and by taking the tangent at the identity gives us $\rho_*:\mathfrak{g}\to\text{End}(V)$ , which can be thought of as a bilinear map $\rho’:\mathfrak{g}\times V\to V$ ). Finally, let $\alpha$ be a $V$ -valued $k$ -form on $P$ . My question is if there is any “nice” formula for the exterior covariant derivative $D\alpha$ in general? I know some special cases: if $\alpha=\omega$ is the connection 1-form itself, then we get Cartan’s structure equation for the curvature. if $k\geq 2$ and $\alpha$ is vertical then $D\alpha=0$ (a useful remark when proving Bianchi’s identity $D\Omega=0$ ). if $\alpha$ is horizontal and $G$ -equivariant (for the naturally induced right action of $G$ on the $k^{th}$ exterior power of $TR$ , and the right action of $G$ on $V$ described above), i.e a “tensorial form on $P$ of type $(V,\rho)$ ”, then we have $D\alpha=d\alpha+\omega\wedge_{\rho’}\alpha$ . Is there a more general formula which addresses other cases as well? My guess is not because the definition of $D\alpha$ involves the various horizontal projections, so if we make no further assumptions on $\alpha$ , then it’s going to be hard to simplify (and annihilate many of the terms when unwinding definitions). In fact, even for $k=1$ and assuming $\alpha$ is vertical I run into trouble trying to obtain a “nice” formula, because unlike the connection 1-form, we don’t have the nice property that $\omega(X_{\xi})=\xi$ (where $X_{\xi}$ is the “fundamental” vertical vector field on $P$ induced by $\xi\in\mathfrak{g}$ ) so the proof of Cartan’s structure equation doesn’t really go through here (unless I’m missing something). Note I’m also fine with accepting there isn’t really a general formula (though an accompanying (heuristic even) explanation would be nice), but just curious if there exists one.","['principal-bundles', 'connections', 'differential-geometry']"
4734696,Typo in math brainteaser book?,"A steamboat leaving pier $1$ takes $20$ hours to go against the current upriver to pier $2$ . It can return downriver with the current from pier $2$ to pier $1$ in $15$ hours. If there were no current, how long would it take for the steamboat to travel between the $2$ piers? I am told that I cannot divide the sun of $20$ and $15$ by two to get $17.5$ . I am also told that I can find the distance between the two piers by: $$\left(\dfrac x{20}\right) + \left(\dfrac x{15}\right) = 35$$ where $x$ is the distance between the two piers. But I’m so confused by this. $\dfrac x{20}$ and $\dfrac x{15}$ are rates of speed on the way up and back, respectively, so why would their sum need to equal $35$ , the total number of hours it took to go up and back? SOLUTION On being asked this question, many people will add $20$ and $15$ together and then divide the result by $2$ , obtaining $17.5$ . They will then say that it would take the steamboat $17.5$ hours to complete the trip if there were no tide. It comes as a surprise to many to learn that this result is incorrect. The surprising answer is that if there were no tide, the steamboat would complete the trip between piers in $17.142857$ hours. On the upriver trip, the steamboat would travel three-quarters of the journey in $15$ hours. On the downriver trip, the steamboat traveled the entire journey in 15 hours. Since the steamboat is traveling with the tide and against the tide for the same period of time, the effect of the tide cancels. Therefore, the steamboat would cover 1 3/4 trips in 30 hours. It would therefore do one entire trip in 30/1 3/4), or 17.142857, hours if there were no tide. This puzzle is interesting because, in addition to obtaining the required solution, we can also learn the distance between the 2 piers and the speed of the flow of the river. We know from the question that the steamboat took a total of 35 hours to do the upriver trip and the downriver journey. Let $x$ equal the distance from pier to pier. This allows us to write the following equation: $$\frac{x}{20}+\frac{x}{15}=35$$ This may be rewritten as $$15x+20x=10500$$ $$x=300$$ Therefore, the distance between both piers is 300 miles. The total journey upriver and downriver is 600 miles, and this is accomplished in 35 hours. Thus, the average speed is 600 divided by 35, or 17.142857 miles per hour. Therefore, if there were no current, the steamboat would be traveling at 17.142857 miles per hour, which confirms the answer that we have already arived at. It would therefore complete the 600-mile trip in 600/17.142857, or 35, hours. What is the speed of the river? From the question, we know that the steamboat took 20 hours to go 300 miles upriver and 15 hours to go 300 miles downriver. The steamboat traveling at 17.142857 miles per hour would do the upriver trip in 17.5 hours if there were no tide. But it takes the steamboat 20 hours, or an extra 2.5 hours, to do this journey with the tide flowing against it. Thus, the tide is flowing at the rate of 2.5 miles per hour. On the downriver trip, the steamboat traveling at the rate 17.142857 miles per hour in no tide would do the journey in 17.5 hours, but with the tide in its favor, it does the trip in 2.5 hours less, or 15 hours. SOURCE Henry Ernest Dudeney, More Puzzles and Curious Problems , edited by Martin Gardner (London: Fontana Books, 1970), puzzle 42, pp. 20, 118,",['algebra-precalculus']
4734705,How to evaluate $\int_{0}^{\frac{\pi}{2}} \frac{d\theta}{2+\sin(\theta)}$ using Cauchy's integral theorem even tho it isn't from 0 to 2pi?,"Evaluate using Cauchy's integral theorem $$\int_{0}^{\frac{\pi}{2}} \frac{d\theta}{2+\sin(\theta)}.$$ I understand the idea is to use $\sin(\theta) = \frac{z-z^{-1}}{2i}$ and $z = e^{i\theta}$ to convert into the  complex plane and simplified to: $$2\oint_C{\frac{dz}{z^{2}+4zi-1}}$$ which after plugging into the quadratic equation yields the singularities $i(\sqrt{3}-2), 
-i(\sqrt{3}+2)$ applying Cauchy's Integral Theorem $$2\oint_C{\frac{dz}{z^{2}+4zi-1}} = 2\pi{i}\sum_{k_i}{\operatorname{Res}(f(z);k_i)}$$ where $k_i =$ all singularities. My expectation is that $2\pi{i}\sum_{k_i}{\operatorname{Res}(f(z);k_i)} = \frac{2\pi}{\sqrt{3}}$ If I realize that Cauchy's is applied over $\int_{0}^{2\pi}$ but my original integral was only over $\int_{0}^{\frac{\pi}{2}}$ I also put a $\frac{1}{4}$ conversion factor on the integral to change the bounds, which would yield $\frac{\pi}{2\sqrt{3}}$ . This is what I assumed the answer to be from what I understand about this process. However it seems the true result: $$\int_{0}^{\frac{\pi}{2}} \frac{1}{2+\sin(\theta)} = \frac{\pi}{3\sqrt{3}}$$ obviously $\frac{\pi}{2\sqrt{3}}$ and $\frac{\pi}{3\sqrt{3}}$ are different by a factor of $\frac{2}{3}$ . I then solved the problem also using the form $$\oint_C{\frac{f(z)dz}{(z-z_0)^{n+1}}} = \frac{2\pi{i}}{n!}\cdot f^{(n)}(z_0)$$ to check my result, however this also resulted in the same thing I got the other way, i.e. the wrong answer. After testing everything I have not been able to figure out what it could be unless when converting the integrals from $\int_{0}^{\frac{\pi}{2}}$ to a contour over $\oint_C$ . You can't actually just adjust the range as I did. And I should be getting a factor of $\frac16$ instead of $\frac14$ . But I can't figure out why. Please help. Thank you in advance!","['complex-analysis', 'cauchy-integral-formula']"
4734794,Central limit theorem for uncorrelated and identically distributed random variables,"I'm trying to determine whether a sum of identically distributed but only uncorrelated continous random variables could possibly converge to a normal distribution. First of all we define the i.i.d. random variables $\theta_i\sim Unif(0,2\pi)$ , so a uniform distribution on the interval $(0,2\pi)$ , density function given by: $$
f_{\theta_i}(\theta) = \begin{cases}\frac{1}{2\pi} \mbox{ if $\theta\in(0,2\pi$)}
\\ 0 \mbox{ otherwise} \end{cases}
$$ My question is: what we can say about the sum of random variables $$
\sum_{i<j}^{N}\cos(\theta_i-\theta_j) \mbox{  ?}
$$ First of all we can note that if $i\not = j \not = k \not = s$ then $\cos(\theta_i-\theta_j)$ and $\cos(\theta_k-\theta_s)$ are indipendent (because function of indipendent random variables). The main problem is that the random variables are not mutual indipendent but ""only"" uncorrelated (or at least, I manage to prove that they are uncorrelated but I don't think that they can possibly be indipendent.. although I have to find a counter-example to prove it). On the other hand $\{\cos(\theta_i-\theta_j)\}_{i<j}$ are identically distributed with $\mathbb{E}(cos(\theta_i-\theta_j))=0$ and $Var(cos(\theta_i-\theta_j))=\frac{1}{2}$ . I know that the standard CLT can not be applied here.. but I also know the existence of some more general form of this theorem with weaker hypothesis. Do you know if in this particular case is possible to apply one of them? Thanks in advance for your help! UPDATE 1 Since $$
|\{(i,j)\in\mathbb{N}^2:i<j\}| = \binom{N}{2} = \frac{N(N-1)}{2}
$$ the sum counts $\frac{N(N-1)}{2}$ random variables and so applying CLT (in its standard version (?)) we have: $$
\frac{\sum_{i<j}^{N}\cos(\theta_i-\theta_j)-\binom{N}{2}\mu}{\sqrt{\binom{N}{2}\sigma}} = \frac{\sum_{i<j}^{N}\cos(\theta_i-\theta_j)}{\sqrt{\frac{N(N-1)}{4}}} = \frac{2\sum_{i<j}^{N}\cos(\theta_i-\theta_j)}{\sqrt{N(N-1)}} \longrightarrow Z\sim\mathcal{N}(0,1)
$$ where we use the fact that $\mu = 0$ and $\sigma^2 = \frac{1}{2}$ . MY QUESTION I would like to know how can I justify rigorously this fact. In particular what version of CLT should I use? Observations Note that if we define $X_{ij} = \cos(\theta_i-\theta_j)$ , our sequence of random variables composes a triangular array with entries $\{X_{ij}\}_{1\leq i\leq j-1,j \geq 2}$ where we are interested in the convergence of the sum of all its entries: $$
\sum_{i<j}^{N}X_{ij}
$$ as $N\to +\infty$ . Furthermore I notice that every row of the triangle is composed by indipendent random variables. For example if $j=5$ then: $X_{15},X_{25},X_{35},X_{45}$ are indipendent. Lindeberg-Feller central limit theorem Our array satisfies the Lindeberg-Feller CLT, which implies that if $S_k$ is the sum of the $k-th$ row, then $S_k\longrightarrow\mathcal{N}(0,\sigma^2)$ .. but how to use it to the problem of the convergence of the sum off ALL the entries of the array? Doubts After my answer (you can find below) , where I referred to some papers , suddenly thanks to @jd27 I understand that my generation with R doesn't seem like a normal distribution.. although because of the results I cited it must be. The ingredients I have used are: Pairwise indipendence (i.e. $\forall i_1<j_1$ and $\forall i_2<j_2$ with $(i_1,j_1)\not = (i_2,j_2)$ we have that $\cos(\theta_{i_1}-\theta_{j_1})$ and $\cos(\theta_{i_2}-\theta_{i_2})$ are
indipendent); Symmetry of $X_{ij}$ (i.e. $X_{ij}\sim -X_{ij}$ ); The random variables $X_{ij}$ are identically distributed with
finite expected value and finite variance. I don't understand what I did wrong.. Is one of this properties false in my sequence of random variables? Thanks in advance for your help. Also we can note that: $$
\min {\sum_{i<j}^{N}\cos(\theta_i-\theta_j)} = -\frac{N}{2}
$$ while $$
\max {\sum_{i<j}^{N}\cos(\theta_i-\theta_j)} = \binom{N}{2}
$$ which is strange because it produces a distribution..that does not seem like a normal, although the limit distribution of $$
\frac{\sum_{i<j}^{N}\cos(\theta_i-\theta_j)}{\sqrt{\frac{N(N-1)}{4}}} \longrightarrow W
$$ seems to have $\mathbb{E}(W) = 0$ and $Var(W) = 1$ as you can see from the output of the code: library(EnvStats)
nsample <- 20000
n <- 50
mat <- matrix(rep(0,n^2), n, n)
total <- integer(nsample)
for (k in 1:nsample) {
  sample <- runif(n, min = 0, max = 2*pi)
  for(i in 1:n) {
    for(j in 1:n) {
      if(j > i) {
        mat[i,j] <- cos(sample[i]-sample[j])
      }
    }
  }
  total[k] <- (sum(mat))/(sqrt(n*(n-1)/4))
}
T <- total
CDF <- ecdf(T)
par(mfrow=c(1,2))
plot(CDF)
epdfPlot(T, epdf.col = ""red"")
print(sum(1/nsample*total))
print(var(T)) Limit distribution seems like a log-normal distribution: https://en.wikipedia.org/wiki/Log-normal_distribution But also reminds the Landau distribution: https://en.wikipedia.org/wiki/Landau_distribution","['central-limit-theorem', 'probability-limit-theorems', 'probability-theory', 'probability']"
4734796,Angle of tilt of a rectangular water tank,"A cuboid tank is placed in the $xy$ plane, with its base centered at the origin.  The base rectangle measures $5$ along the $x$ axis, and $7$ along the $y$ axis.  The height is $9$ . It is filled to $\dfrac{2}{3}$ of its height with water.  The is shown on the left of the figure below. Then, it is tilted by a certain angle $\theta$ about an axis of rotation passing through the base vertex $(\dfrac{5}{2} , - \dfrac{7}{2}, 0)$ , and parallel to the vector $(\cos 60^\circ, \sin 60^\circ, 0 ) $ , such that the water surface touches the tank top vertex that is directly above anchor point (i.e. originally at $(\dfrac{5}{2} , - \dfrac{7}{2}, 9)$ ).  This is shown on the right of the figure below.  Find the angle of tilt $\theta$ in degrees. My Attempt: If we freeze the water and un-tilted the tank, then we note that the frozen water surface plane passes through the points $r_1 = (0, 0, 6)$ and $r_2 = (\dfrac{5}{2} , - \dfrac{7}{2}, 9)$ . On the other hand, the unit normal to the tilted water surface is $(0, 0, 1)$ , so when un-tilting by angle $\theta$ , we are rotating this vector about the vector ( $\cos 60^\circ, \sin 60^\circ, 0)$ by an angle $-\theta$ . Applying this rotation gives $ \hat{n} = ( - \sin 60^\circ \sin \theta, \cos 60^\circ \sin \theta, \cos \theta )$ This vector is normal to $r_2 - r_1$ , so that $ \hat{n} \cdot (r_2 - r_1) = 0 $ Plugging in the numerical values: $ ( - \dfrac{\sqrt{3}}{2} \sin \theta, \dfrac{1}{2} \sin \theta , \cos \theta ) \cdot (\dfrac{5}{2} , - \dfrac{7}{2}, 3) = 0 $ This reduces to, $ \sin \theta (-5 \sqrt{3} - 7) + 12 \cos \theta = 0 $ So that, $ \tan \theta = \dfrac{12}{5 \sqrt{3} + 7 } $ Hence, $ \theta \approx 37.4619^\circ $ My question: Is the method I used correct? And, could someone verify the numerical answer ?  Thanks to all.","['vectors', '3d', 'geometry', 'solid-geometry', 'solution-verification']"
4734877,Proving a non-isolated singularity is an essential singularity,"Here stated more rigorously, Let $\{p_n\}_{n\in\mathbb{N}} $ be the set of singularity of an analytic function $f(z)$ on a bounded domain $U\backslash{\{p_n\}}$ . We know that $p_n$ has at least a limit point (By Bolzano Weierstrass). Call one of the limit point $p$ . Then $p$ is an essential singularity of $f(z)$ . Here's my attempt Proof Define $F(z)=(z-p)^N f(z)$ for the sake of contradiction, we assume that $p$ is not an essential singularity of $f$ , ie. there exists $N\in\mathbb{N}$ such that $\lim_{z\to p} (z-p)^N f(z)$ exists which I am going to call it $\alpha _0$ . - $F(z)$ has poles at $z\in\{p_n\}\backslash p$ So for all $M\in\mathbb{N}$ : $$\lim_{z\to p_M}F(z)=\mathrm{Diverges}$$ At this point, I guess I could conclude that $${\underbrace{\lim_{M\to \infty}\lim_{z\to p_M}F(z)}_{=\lim_{z\to p}F(z)}}={\underbrace{\lim_{M\to \infty}\mathrm{Diverges}}_{\mathrm{True \, for \, all }\, M\in \mathbb{N} }}=\mathrm{Divergent}$$ And $\lim_{z\to p}F(z)=$ Divergent but also equal to $\alpha_0$ which is a contradiction.
This finishes the proof But I can seem to see that if this is intuitively true: ${\underbrace{\lim_{M\to \infty}\mathrm{Diverges}}_{\mathrm{True \, for \, all }\, M\in \mathbb{N} }}=\mathrm{Divergent}$ . Ie. does limit preserve divergence?","['complex-analysis', 'limits']"
