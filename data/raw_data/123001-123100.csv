question_id,title,body,tags
1852773,Perturbing a regular submanifold to ensure submersion.,"Suppose $M$ is a regular compact submanifold with boundary of dimension $n$ in $\mathbb R^{n+1}\smallsetminus 0$, and assume that for each $v\in S^n$ the ray emanating from $v$ intersects $M$ in at most one point. Denote by $S$ the subset of $S^{n}$ where this happens. The surjective submersion $r: \mathbb R^{n+1}\smallsetminus 0\longrightarrow S^n$ restricts to a bijection $r : M\longrightarrow S$. Note that $\ker r_{p,\ast}=\langle p\rangle$ intersects $M$ at one point, but it can happen that $\langle p\rangle \subseteq T_pM$ (for example, draw a projected $x\mapsto x^3$ around $S^1$). However, in the one dimensional case, there are only finitely many such points where this fails, and one can perturb $M$ by a diffeomorphism in a neighbourhood of such points to preserve all hypothesis and guarantee that $r$ restricts to a submersion $r: M\longrightarrow S$ which must then be a diffeomorphism. Suppose now that $n>1$. One can mimic the above to obtain a submanifold $M$ where the points where $\langle p\rangle \subseteq T_pM$ form a $n-1$ dimensional submanifold in $S^n$. For example, by extending the previous example in higher dimensions, one can produce a circle inside $S^2$, etc. Can one in general modify $M$ slightly by a diffeomorphism so that $\langle p\rangle + T_p M =T_p\mathbb R^{n+1}$?","['differential-geometry', 'differential-topology']"
1852826,Inverse of the Pascal Matrix,"Let $P_n$ be the $(n+1) \times (n+1)$ matrix that contains the numbers of Pascal's triangle in the upper triangle. For example in the case of $n=3$
$$
P_3 = 
\begin{pmatrix}
1 & 1 & 1 & 1 \\
0 & 1 & 2 & 3 \\
0 & 0 & 1 & 3 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
$$
or in general 
$$
(P_n)_{ij} = \binom{j}{i} \lfloor  i \leq j  \rceil  ~~~\text{for}~~ i,j \in \{0,...,n \} 
$$
using the definition
$$
\lfloor  A  \rceil :=
\begin{cases}
    1 & \text{A is true} \\
    0 & \text{A is not true}
    \end{cases}
$$
This matrix is invertible since $\det P_n = 1$.
For smaller cases like $n=3$, I calulated the inverse of the matrix by hand and found
$$
P_3^{-1} = 
\begin{pmatrix}
1 & -1 & 1 & -1 \\
0 & 1 & -2 & 3 \\
0 & 0 & 1 & -3 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
$$
$n=2,4$ led to similar results, so I'm guessing that the inverse should be
$$
(P_n^{-1})_{ij} = (-1)^{j+i}(P_n)_{ij} = (-1)^{j+i} \binom{j}{i} \lfloor  i \leq j  \rceil
$$
But I have not been able to prove or disprove this yet. So far I tried multiplying the two matirces which gives
$$
\sum^j_{k=i} (-1)^{j+k} \binom{j}{k} \binom{k}{i} = \delta_{ij}
$$
if one asumes that the result is the unit matrix. For $i=0$ and $j>0$ this gives
$$
\delta_{0j} = 0 
= \sum^j_{k=0} (-1)^{j+k} \binom{j}{k} \binom{k}{0}
= \sum^j_{k=0} (-1)^{k} \binom{j}{k}
$$
which is an identity I know to be true, so it reasures me a little bit that the above should also be true.","['matrices', 'binomial-coefficients', 'polynomials']"
1852885,Bridges across a tiled floor,"A few years back, a friend of mine did a seminar on ""Bridges across a tiled floor"".  A ""bridge"" was defined as a row or column of an $n \times n$ binary matrix consisting entirely of $1$'s, for example the third column and fourth row of \begin{bmatrix}
1&0&1&0 \\
0&0&1&0 \\
0&1&1&1 \\
1&1&1&1
\end{bmatrix} The problem is to find the probability of selecting an $n\times n$ binary matrix with at least one bridge, when selecting from all $n\times n$ binary matrices.  My friend made an algorithm using Markov chains for calculating it for a given $n$, but we never found a closed formula.  I was wondering if there was a simple approach, or if anyone knows how to find the solution. I made several attempts.  My first attempt was to try a purely combinatorial solution, but the interconnectivity made it a bit ridiculous.  I tried to solve the complementary problem by placing $0$'s on the main diagonal, permuting them, and considering all other choices for the other entries, but this resulted in multiple ways of attaining the same matrix.  I tried solving the simpler problems of only column bridges or row bridges, which had simple solutions, but combining them proved difficult.  And most recently (which I haven't fully fleshed out), I tried setting up a recursive relationship from the $n-1$ case to the $n$ case. Any insight would be greatly appreciated.","['matrices', 'combinatorics', 'probability']"
1852934,A generalization of holomorphic functions,Let's fix a  matrix $A\in M_{2}(\mathbb{R})$.  Assume that the  following vector space of  smooth  functions is  closed under complex  multiplication: $$\mathcal{S}_{A}=\{f:\mathbb{C}\to \mathbb{C}\mid Df.A=A.Df  \}$$ Here  $Df$  is  the  Jacobian of  $f:\mathbb{R}^{2}\to \mathbb{R}^{2}$ (We  identify  $\mathbb{C}$  with $\mathbb{R}^{2}$). Does  this  imply that  $A$ is  in the  form $A=\begin{pmatrix} a&-b\\b&a \end{pmatrix}$? Note that For $A=\begin{pmatrix} 0&-1\\1&0 \end{pmatrix}$  the  relation $Df.A=A.Df$  is equivalent to the Cauchy Riemann equations for $f=u+iv$ so we obtain the class of  holomorphic  functions.,"['complex-numbers', 'partial-differential-equations', 'matrices', 'matrix-calculus', 'complex-analysis']"
1852993,Solving the trig inequality $|\sin{x} + \cos{x}| > 1$,"$|\sin{x} + \cos{x} |> 1$
How to solve this kind of question? Is there any websites to learn trigonometry inequalities? My teacher only taught us the simple question but not the complicated one. Thank you.","['algebra-precalculus', 'inequality', 'trigonometry']"
1853004,Chance of receiving all elements of a set,"I have a set of $n$ different elements. I will select $i$ times a subset $S_j$ of $n/k$ elements randomly. Each element can only occur once in each $S_j$, but can be part of multiple different subsets $S_j$. I want to know the probability of having all elements in the sum of the subsets. My attempt: This would be a possible outcome of what I want to do: $n= 4$ | number of different items $elements = \{ A,B,C,D \}$ | set of items $k=2$ | nr. of elements per subset $ i=3$ | nr. of subsets generated $S_1=\{A,B\}$ | 1st random subset $S_2=\{C,A\}$ | 2nd random subset $S_3=\{B,A\}$ | 3rd random subset $total = S_1 \cup S_2 \cup S_3 = \{A,B,C\}$ | all elements in at least 1 subset I am looking for the probability $p(n,k,i)$ for the event ""Each element from $elements$ is contained in $total$""","['probability-theory', 'probability']"
1853018,Straightforward software for publication-quality geometrical sketches.,"What is the simplest, most straightforward package for draw simple geometrical 2D sketches that involve circles, straight lines and vectors, for a professional mathematical Tex-made document? I am using Microsoft Windows. The following sketch was made in Microsoft Word, but I need to make it more precise (ie. make right angles actually look like right angles, circles perfectly circular, and so on). Thanks!","['math-software', 'geometry']"
1853022,Derivative of $X_u A X B X_u^T$ w.r.t. $X_u$,"How to solve this $\frac{d X_u A X B X_u^T}{d X_u}$, where $X, A, B \in \mathbb{R}^{n \times n}$ and $X_u$ is the $u$-th row in $X$?","['matrices', 'matrix-calculus', 'linear-algebra', 'derivatives']"
1853026,"the Greatest Number Among $3^{50} ,4^{40} ,5^{30}$ and $6^{20}$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question how to find the Greatest Number Among  $3^{50} ,4^{40} ,5^{30}$ and $6^{20}$   please give a short cut method",['algebra-precalculus']
1853069,"Find $\lim_{n\to\infty}\frac{g(t+n)}n$ for $g(t)=\int_0^tf(x)\,dx$, where $f(x+1)=f(x)$","Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(x+1)=f(x) \quad \forall x\in\mathbb{R}$ . Define $g(t)=\displaystyle\int_0^tf(x)\,dx$ , $t\in\mathbb{R}$ and $h(t)=\displaystyle\lim_{n\to\infty}\frac{g(t+n)}{n}$ (provided it exists). Show that $h(t)$ is defined $\forall t\in \mathbb{R}$ and is independent of $t$ . As the period of $f$ is $1$ , we can say that $\displaystyle\int_0^tf(x)\,dx=t\displaystyle\int_0^1f(x)\,dx$ . But what about $\displaystyle\int_0^{t+n}f(x)\,dx$ ? It is also seen that $g'(t)=f(t)$ . How do I find the limit?","['real-analysis', 'periodic-functions', 'limits', 'calculus', 'integration']"
1853089,Calculating variance for a window of samples which already contains pre-calculated variances,"In a previous answer , the following solution was given for calculating the variance from a stream of sample values (from Knuth via John D. Cook ): $$
\begin{align*}
m_k&=m_{k-1}+\frac{x_k-m_{k-1}}k \\
v_k&=v_{k-1}+(x_k-m_{k-1})(x_k-m_k)
\end{align*}
$$ with variance $$\sigma^2=\frac{v_k}{k-1}$$ If we kept track of $m_k$, $v_k$, and $\sigma^2$ for each sample, then could we easily compute the variance for a given window of samples between $i$ and $i+j$ with a simple manipulation of those saved values?","['statistics', 'algorithms', 'variance']"
1853121,"How to show that $x \sin \frac{\pi}{x} > \pi \cos \frac{\pi}{x}$ for $x \in (1, \infty)$?","How could I prove that $x \sin \frac{\pi}{x} > \pi \cos \frac{\pi}{x}$ for $x \in (1, \infty)$? Dividing both sides through by $x \sin \frac{\pi}{x}$ and letting $y = \frac{\pi}{x}$ gives the inequality $1> y \cot y$, if $y \in (0,\pi)$, but then I don't know how to go ahead and actually prove that?","['calculus', 'analysis']"
1853122,solutions of $\bar z = |z-2\Im(z)|^2$.,I need to find all the solutions of $\bar z = |z-2\Im(z)|^2$. I know that $z=x+iy$ and $\bar z=x-iy$ and then $2\Im(z)=2y$. But can someone show the algebra for what I do next?,"['complex-analysis', 'complex-numbers']"
1853185,Killing/Isometry correspondence: Domains of flows generated by vector fields,"I am wondering about the correspondence between the isometry group $\mathcal{I}$ and the Lie Algebra of Killing vector fields $\mathcal{K}$ on a pseudo-Riemannian manifold $(\mathcal{M}, \mathbf{g})$. Specifically $\mathcal{L}_X \mathbf{g} = 0$ defines $\mathcal{K}$ and gives a necessary condition for $X$ to be in the Lie-Algebra $\mathcal{i}$ of $\mathcal{I}$. But as my understanding goes this is not a sufficient condition i.e not all elements of $\mathcal{K}$ generate flows $\theta_t : \mathcal{M} \rightarrow \mathcal{M}$ with $t > 0$ that are in $\mathcal{I}$.
Now I have found obscure references, that only elements of $\mathcal{K}$ that generate global flows are in $\mathcal{i}$. I guess my question boils down to something like: Given a vector field $X$, is it possible that its maximal flow domain contains a finite interval $0 \in [a,b]$ for all $p \in \mathcal{M}$ without the flow being global? My intuition tells me: no, because if this were the case, initial conditions should be ""preserved"" in some sense, forcing e.g $\theta_{2b}$ to be defined as well.","['differential-geometry', 'vector-analysis']"
1853187,"Asymptotic Moments of the Binomial Distribution, $E(X/(np))^k = 1 + O(k^2/n)$?","Let $X \sim \text{Binomial}(n, p)$ be the sum of $n$ Bernoulli($p$) random variables. What is the value of $E(X/(np))^k$, where $k$ is a large integer, as $n$ grows large? From calculations the first values are E1        = 1
E(X/np)   = 1
E(X/np)^2 = 1 + r/n
E(X/np)^3 = 1 + 3r/n + ((-1+p)(-1+2p))/(np)^2
E(X/np)^4 = 1 + 6r/n + O[1/n]^2
E(X/np)^5 = 1 + 10r/n + O[1/n]^2
E(X/np)^6 = 1 + 15r/n + O[1/n]^2
E(X/np)^7 = 1 + 21r/n + O[1/n]^2
... where $r = (1-p)/p$ So my guess would be that one could obtain a result like $E(X/(np))^k = 1 + O(k^2/n)$, but I'm not sure how I'd proceed. For sums of $\{+1, -1\}$ random variables, I'm aware that we can bound the moments by replacing each one with a normal random variable with the same variance. However converting this result to the non central case doesn't seem obvious?","['normal-distribution', 'probability-distributions', 'binomial-distribution', 'probability', 'moment-generating-functions']"
1853208,"Why is this easy ""proof"" of Brouwer's Fixed Point Theorem not correct/common?","Brouwer's Fixed Point Theorem states, essentially, that any continuous function on a closed disc to itself has a fixed point. I am familiar with the proof based on the impossibility of a retraction from a disc to its boundary and the proof based on Sperner's Lemma. Wikipedia lists a number of other proofs. However, it seems there is a simpler ""proof"" - quoted because it could be wrong - that uses no fancy machinery and I'm wondering if it is right and if so, why it isn't well known. We will prove that any continuous $f : [0, 1]^n \rightarrow [0, 1]^n$ has a fixed point by induction on $n$. $n = 1$ amounts to the Intermediate Value Theorem. For $n > 1$ our space is $[0, 1] \times [0, 1]^{n-1}$. By the 1-d case, for each $\mathbf{u} \in [0, 1]^{n-1}$, $x \mapsto f(x, \mathbf{u})_1$, the first component of $f(x, \mathbf{u})$, has a fixed point $x$. By continuity of $f$ we may choose this fixed point, $x(\mathbf{u})$, to vary continuously in $\mathbf{u}$. By the $(n-1)$-d case, for each $y \in [0, 1]$, $\mathbf{v} \mapsto f(y, \mathbf{v})_{2, \ldots, n}$ has a fixed point $\mathbf{v}$ and we may let $\mathbf{v}(y)$ vary continuously. If $x(\mathbf{v}(0)) = 0$ then $(0, \mathbf{v}(0))$ is a fixed point of $f$; similarly for 1. Otherwise let $X = \{(x(\mathbf{u}), \mathbf{u}) \mid u \in [0, 1]^{n-1}\}$. $X$ is the graph of a continuous function so it is closed and $[0, 1]^n \setminus X$ is open. Furthermore, $\mathbf{v}(0)$ and $\mathbf{v}(1)$ are in different components of $[0, 1]^n \setminus X$ so by an argument like the proof of the Intermediate Value Theorem, $\mathbb{v}$ must cross $X$ at some point, which is a fixed point of $f$.","['algebraic-topology', 'fixed-point-theorems', 'general-topology', 'proof-verification']"
1853221,"If $f$ has more than one root in $K$, then $f$ splits and $K/k$ is Galois?","Let $f \in k[x]$ be an irreducible polynomial of prime degree $p$ such that $K \cong k[x]/f(x)$ is a separable extension. How do I see that if $f$ has more than
one root in $K$, then $f$ splits and $K/k$ is Galois?","['galois-theory', 'polynomials', 'abstract-algebra', 'algebraic-number-theory', 'number-theory']"
1853223,Distribution of Primitive Pythagorean Triples (PPT) and of solutions of $A^4+B^4+C^4=D^4$,"If we define a $PPTCountingFunction(n)$ as a function that returns the number of PPF with $c < n$ and $a>b$, then up to first $n=100,000$ it is near linear and $\dfrac{n}{PPTCountingFunction(n)}=2\pi$ I have several questions (third question is the most interesting to me): (1) Is this also an asymptotic behavior of this function, or does it have some other slowly changing factors that are not showing up when n is small? (2) Is there a clear reasoning for frequencies of PPT? (3) Can we apply similar reasoning to estimate the frequency of primitive counterexamples to Euler's hypothesis for $n=4$ (solution s of $A^{4}+B^{4}+C^{4}=D^{4}$)? Regarding (3). First solution appears at $95800^{4} + 414560^{4} + 217519^{4} = 422481^{4}$. This is the only solution with $D<2000000$. Another known solution (not necessarily second)  is  $2682440^{4}  +  15365639^{4}  +  18796760^{4}  =  20615673^{4}$. I am curious if there is a point to look for a solution between these two.","['diophantine-equations', 'probability', 'pythagorean-triples']"
1853262,"A continuous onto/surjective function from $[0, 1) \to \Bbb R$.","Does there exist a continuous onto/surjective function from $[0, 1) \to \Bbb R$? Finding difficult to site an example...","['real-analysis', 'analysis', 'functions']"
1853281,What branch/field of mathematics is this? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question I do not want solutions, I just want the field/branch of mathematics that these problems deal with, and possibly a good online source or two to learn it. Problems :- 1:- 2:- 3:- 4:- Sorry about that, the problems I was referring to are: https://i.sstatic.net/1ptnh.jpg","['real-analysis', 'reference-request', 'elementary-set-theory', 'analysis', 'discrete-mathematics']"
1853334,"If $x^2+\frac{1}{2x}=\cos \theta$, evaluate $x^6+\frac{1}{2x^3}$.","If $x^2+\frac{1}{2x}=\cos \theta$, then find the value of $x^6+\frac{1}{2x^3}$. If we cube both sides, then we get $x^6+\frac{1}{8x^3}+\frac{3x}{2} \cdot \cos \theta=\cos ^3 \theta$ but how can we use it to deduce required value?","['algebra-precalculus', 'trigonometry']"
1853361,What does it mean to integrate a Brownian motion with respect to time?,"I am reading about stochastic process, but could not make sense if one equation I encountered. Can anyone help me understand it? The equation states that suppose R(s) is an interest rate process, then the discount process is $D(t)=e^{- \int_0^t R(s)ds} $. Suppose R(t)=W(t) is a simple Brownian motion, what does $\int_0^t R(s)ds$ mean?
Is it a Lebesgue integral? Or is it an Ito's integral? How to interpret it intuitively? This is from chapter 5 of Shreve's stochastic calculus for finance, equation 5.2.17 on page 215.","['stochastic-processes', 'probability-theory']"
1853364,"How to find this function, and what method to use? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question The function is $f(x-\frac{1}{x})= x^3-\frac{1}{x^3}$ and they are asking us to find out what $f(-x)$ is?","['algebra-precalculus', 'functions']"
1853388,Pointwise convergence of holomorphic functions,"Let $(g_n)_n$ be a sequence of holomorphic functions on $U$, where $U$ is the open unit disk.
Suppose the first $k$ derivatives of $g_k$ at zero all vanish, $g_k(0) = 0$, and finally that $g_n$ converges pointwise to a holomorphic function $g$ on $U$. Must $g$ be the zero function? (I'm guessing yes. It would for example suffice to show that $g_k'$ converges pointwise to $g'$.)","['functional-analysis', 'complex-analysis', 'convergence-divergence']"
1853418,Trigonometric identities: $ \frac{1+\cos(a)}{1-\cos(a)} + \frac{1-\cos(a)}{1+\cos(a)} = 2+4\cot^2(a)$,"I don't really know how to begin, so if I'm missing some information please let me know what it is and I'll fill you guys in :). This is the question I can't solve:
$$
\frac{1+\cos(a)}{1-\cos(a)} + \frac{1-\cos(a)}{1+\cos(a)} = 2+4\cot^2(a)       
$$ I need to prove their trigonometric identities. I have the $5$ basic set of rules, I could write them all here but I suppose it's not needed, if it is please let me know since it's not gonna be simple to type. I have over $40$ questions like these and I just couldn't seem to understand how to prove them equal, my best was $4 \cot^2(a) = 2 + 4 \cot^2(a)$ Thanks for everything!",['trigonometry']
1853421,How do we prove that $\int_{0}^{1}\int_{0}^{1}{\left(\ln{x}\ln{y}\right)^s\over 1-xy}dxdy=\Gamma^2(1+s)\zeta(2+2s)$?,"How do we prove that $$\int_{0}^{1}\int_{0}^{1}{\left(\ln{x}\ln{y}\right)^s\over 1-xy}dxdy=\Gamma^2(1+s)\zeta(2+2s)$$ Integrate with respect to x first, let $s=1$ $$\int_{0}^{1}{\ln{y}\ln{x}\over 1-yx}dx$$ $u=\ln{x}\rightarrow xdu=dx$ $$\ln{y}\int_{0}^{\infty}{u\over y-e^{-u}}du$$ I don't think I am in the right track here, any hints please.","['integration', 'definite-integrals', 'calculus', 'closed-form']"
1853430,Why is the Maximum in the Min-Max Principle for Self-Adjoint Operators attained?,"Let's consider a self-adjoint operator $A$ (not necessarily bounded) on a Hilbert space which is bounded from below, with domain $D$ and whose resolvent is compact. Then, the spectrum consists solely of isolated eigenvalues which are given (in increasing order) by the min-max principle: \begin{equation}
\lambda_k = \min_{\substack{V \subset D\\ \dim V  = k}} \max_{\substack{x \in V \\ x \neq 0}} \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}, \ k \in \mathbb{N}.
\end{equation} The proof I know shows $\lambda_k \geq \min \max \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}$ and $\lambda_k \leq \min \max \frac{\langle \,x , Ax \rangle}{\langle \, x, x \rangle}$ by using a orthonormal basis of eigenvectors. But how can we really write ""min"" and ""max"" instead of ""inf"" and ""sup"", i.e. why is the minimum and maximum really attained? Does anybody have a proof or a source for this assertion? Edit: I asked the question for the minimum separately since I want the possibility to start a bounty there and accept the answer here at the same time. See here: Why is the Minimum in the Min-Max Principle for Self-Adjoint Operators attained? .","['functional-analysis', 'spectral-theory']"
1853432,Understanding twisted differential forms,"I'm trying to understand twisted differential forms.  I do know that they are like regular differential forms but under coordinate transformations they pick up an extra factor of the sign of the determinant of the transformation.  Somehow this means that they can be used to integrate on non-orientable manifolds. (???) While Googling, I saw this question on Physics Forums , where one of the answers seems like a good start at understanding twisted differential forms.  I'll quote the answer here for reference. Consider a line segment. There are two ways one can orient this: along the segment and across the segment. For example, if you wanted to represent a segment of the world-line of a particle, then the first type of orientation is appropriate. On the other hand, imagine a circle drawn on a plane. A segment of this circle naturally has an orientation of the second type: it is oriented 'across' the segment, depending on which side of the circle is 'inside' and which is 'outside' This is the main difference between differential forms and their twisted counterparts, i.e. the type of orientation. The contour lines of a function have the 'across' orientation, and are represented by 1-forms. But if we wanted to represent coutour lines with an orientation along them instead of across, you would use a twisted 1-form. Imagine 2+1 dimensional spacetime. I assume you're familiar with the usual picture of a 2-form in a three dimensional space. The 'tubes' or 'boxes' in the picture of this 2-form will have an orientation that is 'around' them, i.e. clockwise or anticlockwise. Of course, one can always convert from clockwise/anticlockwise to up/down using things like right-hand rules, but that is not the natural type of orientation of a current. For a twisted 2-form, on the other hand, the tubes or boxes will have the correct 'along' orientation. So, in 2+1 dimensional spacetime, current density is a twisted 2-form. Similarly, in 3+1 dimensions, it is a twisted 3-form. Though this isn't the way I usually think of differential forms, I am somewhat familiar with the geometric interpretation -- at least for $1$ -forms -- as stacks through which vectors penetrate.  However I'm still not entirely able to see what a twisted differential $1$ -form would be.  Geometrically, is it supposed to be like a curve which ""counts"" the projections of the tangent vectors onto the tangents of the curve along it?  If so, how is that picture obtained from the definition?  And how does one visualize higher dimensional twisted forms -- because I don't really understand that part of the post at all.","['mathematical-physics', 'differential-forms', 'differential-geometry']"
1853465,The roots of $ax^2+bx+c$ are 6 and $P$. The roots of $cx^2+bx+a$ are $Q$ and $R$ what is the value of $P\times Q\times R$,"Problem The roots of $ax^2+bx+c$ are 6 and $P$. The roots of $cx^2+bx+a$ are $Q$ and $R$
And we are asked to find $P\times Q\times R$ by using the identities: $P(x)=Q(x)\times D(x)+R(x)$
where $P(x)$ is the equations and $Q(x)$ is the quotient and $D(x)$ is the divisor and $R(x)$ is the remainder. My thought process so I had set $$ax^2+bx+c=Q(x)\times(x-6)\times(x-P)+0$$
and in the next equation:
$$cx^2+bx+a=Q(x)\times(x-P)(x-Q)+0$$ I really don't know where to go from here",['algebra-precalculus']
1853468,What is the average distance of two points chosen uniformly on a unit square? [duplicate],"This question already has an answer here : Average distance between two random points in a square (1 answer) Closed 4 years ago . What is the average distance of two points chosen uniformly on a unit square? What I am asking is how to calculate $E\left(\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\right)$ for $x_1, x_2, y_1, y_2$ spread uniformly on $[0,1]$.","['statistics', 'probability']"
1853497,Is the set of hyperreal numbers a quotient ring?,"It is easy to see that the set of real sequences $\mathbb{R}^{\mathbb{N}}$ is a ring. It suffices to define, for all $r,s\in\mathbb{R}^{\mathbb{N}}$, the operations $r\oplus s =(r_n+s_n)_{n\in\mathbb{N}}$ and $r\odot s=(r_n\cdot s_n)_{n\in\mathbb{N}}$. Let $\mathcal{U}$ be a nonprincipal ultrafilter on $\mathbb{N}$. For all $r\in\mathbb{R}^{\mathbb{N}}$, we define the set $r^{(0)}=\{n\in\mathbb{N} \mid r_n=0\}$. My question: Is the set of the $\textit{almost null sequences}$
$$
\mathbb{I} = \{r\in\mathbb{R}^{\mathbb{N}}\mid r^{(0)}\!\in\mathcal{U}\}
$$
a two-sided ideal of $\mathbb{R}^{\mathbb{N}}$? 
I think yes, because if $s\in\mathbb{I}$ and $r\in\mathbb{R}^{\mathbb{N}}$, then $(s\odot r)^{(0)}\in\mathcal{U}$ (i.e. the product of any sequence and an almost null sequence is almost null). If yes, is the set of the hyperreal numbers the quotient ring $\mathbb{R}^{\mathbb{N}}\diagup \mathbb{I}$? In this case two sequences should belong to the same class if their difference is almost null (namely, they match on an index set which belongs to the ultrafilter $\mathcal{U}$)","['nonstandard-analysis', 'abstract-algebra', 'elementary-set-theory']"
1853501,Proving that R is a partial Order.,"Define the relation $\Bbb R \times \Bbb R$ by $(a,b) \; R$ $ (x,y)$ iff $a \le x$ and $b \le y$ , prove that R is a partial ordering for $\Bbb R\times\Bbb R $ . A partial order is if R is reflexive on A, antisymmetric and transitive. One must prove these properties true. My question for this problem is trying to comprehend why this problem is antisymmetric and why it is transitive. $(i)$  $R $ is reflexive as we say $x=a$ and $y = b$. Thus we can conclude that that $x\le x , y\le y$. $(x,y)R(x,y)$. $(ii)$ if $a\le x$ and $x\le a$ then $x= a $ If $ b \le y $ and $ y \le b$ then $y =b$. Since you interchange these would it not be symmetric? $(iii)$ Suppose $(a,b) R(x,y)$ and $(x,y) R (c,d)$ This is as far as I got for transitive. Any advice on how prove this partial ordering true would be appreciated.","['relations', 'elementary-set-theory', 'order-theory']"
1853528,Loaded Dice Conditional Probability,"If I have two dice, one regular and one loaded. The loaded die has the probability 1/2 of landing a six and rest of the numbers are equally probable. If you select a die randomly and throw it and it shows 6 in one of the throws and not a six in other. What is the probability of having a weighted die? My approach: Is this correct or I am doing something wrong?","['bayes-theorem', 'probability-theory', 'probability']"
1853532,Pigeonhole Principle Question: Jessica the Combinatorics Student,"Jessica is studying combinatorics during a $7$-week period. She will
  study a positive integer number of hours every day during the $7$
  weeks (so, for example, she won't study for $0$ or $1.5$ hours), but
  she won't study more than $11$ hours in any $7$-day period. Prove that
  there must exist some period of consecutive days during which Jessica
  studies exactly $20$ hours. Here are my thoughts so far: Let $f(n)$ represent the total number of hours Jessica has studied after day $n$. Clearly, there are $49$ days in total, and the domain of $f$ is integers in the interval $[0,49]$. Proving that there must exist some period of consecutive days during which Jessica studies exactly $20$ hours is equivalent to proving that there must exist $i$ and $j$ such that $f(i)-f(j)=20$. This is a really interesting question, but I don't see a clear path forward. How do you solve this question? Please try not to use extremely advanced math or I won't understand :P","['combinatorics', 'pigeonhole-principle']"
1853545,Limit of $\lim_{t \to \infty} \frac{ \int_0^\infty \cos(x t) e^{-x^k}dx}{\int_0^\infty \cos(x t) e^{-x^p}dx}$,"Let 
\begin{align}
f(t,k,p)= \frac{ \int_0^\infty \cos(x  t) e^{-x^k}dx}{\int_0^\infty \cos(x  t) e^{-x^p}dx},
\end{align} My question: How to find the following limit of the function $f(t,k,p)$
\begin{align}
\lim_{t \to \infty} f(t,k,p),
\end{align}
 for any $p>0$ and $k>0$. What is known Some facts about the function Note that  $\int_0^\infty \cos(x  t) e^{-x^k}dx$ is a fourierier transform of $e^{-{|x|^k}}$. For $0<k \le 2$ we have that $\int_0^\infty \cos(x  t) e^{-x^k}dx$ is non-negative function and has no zeros. See this question . For $k>2$ we know that  $\int_0^\infty \cos(x  t) e^{-x^k}dx$ has countable many zeros. See this questions . A related question was asked here . Because for the case of $p>2$ the denominator has countable many zeros I am not sure if $\lim_{t \to \infty} f(t,k,p)$ even exists. It would be nice to show if it exists or not. Other trivial case include  $k=1,p=2$ and $k=2,p=1$ since inverse fourier transforms of $e^{-|x|}$ and $e^{-|x|^2}$ are know in closed form. Clearly, the case of $k=p$ is trivial. So, we would like to analyze $k>p$ and $p<k$. Numerical Simulations: Numerical simulations seem to suggest that
\begin{align}
\lim_{t \to \infty} f(t,k,p)&=-\infty, \ k>p, \\
\lim_{t \to \infty} f(t,k,p)&=\infty, \ k<p.
\end{align} Method of the Steepest Descent: (See the answer in progress via this method by @tired) It has been suggested by @tired that the method of steepest descent might be a possible approach for solving the limit. 
That is since
\begin{align}
\int_0^\infty \cos(x  t) e^{-x^k}dx&= \mathsf{Re}  \int_0^\infty e^{it} e^{-x^k}dx, \\
&=  t^{\frac{1}{k-1}} \mathsf{Re}  \int_0^\infty  e^{ t^{\frac{k}{k-1}} (-u^k+iu)}du, \\
\end{align} 
where in the last step we have used substitution $x=u t^{\frac{1}{k-1}}$. Note that this now take the form of $\int_I  e^{ A S(u)}du$ which can be handle by method of steepest desend if $S(u)$ satisfies: $S(u)$ is holomorphic $\mathsf{Re}(S(u))$  has a single maximum: $\max_{ u \in I } \mathsf{Re}(S(u))=\mathsf{Re}(S(u_0))$ for execly one $u_0\in I$. $u_0$ is non-degenare saddle point. That is $S''(u_0)\neq 0$. Adopting this to our case we have that the maximum  of
\begin{align}
\max_{ u \in I } \mathsf{Re}(S(u))=\max_{ u \in [0,\infty) } \mathsf{Re}(-u^k+iu)=\max_{ u \in [0,\infty) } \mathsf{Re}(-u^k)= 0,
\end{align}
where $u_0=0$. Note also that $S''(u)= k (k-1) u^{k-2}$ and therefore $S''(0)=0$ so the maximum is a degenaret saddle point. This violates the third conditon. I really hoped that this method was going to work. Am I making any mistakes in the above? Thank you for any help or suggestions you might have. This the second bounty posted on this question. Things that did not work: Approach with expansion of $e^x$ \begin{align}
f(t,k,p)&= \frac{ \int_0^\infty \cos(x  t) e^{-x^k}dx}{\int_0^\infty \cos(x  t) e^{-x^p}dx}\\
&=\frac{ \int_0^\infty \cos(u) e^{-(u/t)^k}du}{\int_0^\infty \cos(u) e^{-(u/t)^p}du}\\
&=\frac{ \int_0^\infty \cos(u) (1- (u/t)^k+O((u/t)^{2k})) du}{\int_0^\infty \cos(u)  (1- (u/t)^p+O((u/t)^{2p}))du},
\end{align} but the integrals, do not converge. Don't think this approach works. Integration by parts approach: Note that  by tntegration by parts
\begin{align}
\int_0^\infty \cos(x  t) e^{-x^k}dx=  \frac{k}{t^{k+1}} \int_0^\infty \sin(u)  u^{k-1} e^{-(u/t)^k} du,
\end{align} using this we have that
\begin{align}
f(t,k,p)=\frac{k}{p} t^{k-p} \frac{ \int_0^\infty \sin(u) u^{k-1} e^{-(u/t)^k}du}{\int_0^\infty \sin(u) u^{p-1} e^{-(u/t)^p}du}.
\end{align} The question is how to proceed next? How do we know how the ratio of the two integrals behaves?","['real-analysis', 'laplace-method', 'fourier-analysis', 'limits', 'integration']"
1853564,Covering space is path-connected if the action of $\pi_1$ on a (single) fiber is transitive,"Let $p\colon X\to Y$ be a covering map. Suppose that $Y$ is path-connected, locally path-connected and semi-locally simply connected. Let $x,x'\in X$ be two points of $X$. $\textbf{Question:}$Is it true that $\pi_1(Y,p(x))$ acts transitively on $p^{-1}(p(x))$ if and only if $\pi_1(Y,p(x'))$ acts transitively on $p^{-1}(p(x'))$? Equivalently, is $X$ path-connected if there exists some $x\in X$ such that $\pi_1(Y,p(x))$ acts transitively on $p^{-1}(p(x))$? Here is my try: Suppose $\pi_1(Y,p(x))$ acts transitively on $p^{-1}(p(x))$. Let $z'\in p^{-1}(p(x'))$. Since $Y$ is path-connected, we may choose a path $\gamma$ from $p(x')$ to $p(x)$. The monodromy functor then induces a map of sets $\phi\colon p^{-1}(p(x'))\to p^{-1}(p(x))$. Let $z=\phi(z')$. Then there exists a loop $\delta\in \pi_1(Y,p(x))$ and a point $\tilde{x}\in p^{-1}(p(x))$ such that the end point of a lift $\tilde{\delta}$ of $\delta$ beginning at $\tilde{x}$ is equal to $z$. Now what I would like to do is to consider something like the conjugation of $\delta$ with respect to $\gamma$ and lifting $\gamma$ to $\tilde{\gamma}$ and then considering the composition $(\tilde{\gamma})^{-1} \tilde{\delta}\tilde{\gamma}$. However, this does not make sense in general sine we do not know $\tilde{\delta}(0)=\tilde{\gamma}(1)$. Moreover, this idea does not seem to use much of the relation between $z$ and $z'$, $\phi(z')=z$. $\textbf{Edit:}$ The map $\phi$ can be described more explicitly. I will write that out and see how that helps. In the notation from above $\phi(z')=z$ means that there exists a lift $\tilde{\gamma}$ of $\gamma$ starting at $z'$ and ending at $z$. We can do a similar thing with $\tilde{x}$, say $\psi(\tilde{x})=\tilde{z}$ and let $\epsilon$ be the used lifting, where $\psi\colon p^{-1}(p(x)) \to p^{-1}(p(x'))$ is induced by $\gamma^{-1}$ using the monodromy functor. Then $(\tilde{\gamma})^{-1}\tilde{\delta}\epsilon^{-1}=:\tilde{\omega}$ is path from $\tilde{z}$ to $z'$. Letting $\omega=p\tilde{\omega}$ we see that this is a loop at $p(x')$ since $\tilde{z},z' \in p^{-1}(p(x'))$. This proves $[\omega].\tilde{z}=z'$, i.e. $\pi_1(Y,p(x'))$ acts transitively on $p^{-1}(p(x')) $. $\textbf{Edit:}$ Actually, this doesn't prove the transitivity. In fact, I had the wrong definition of transitivity in mind when writing the above. However, making a similar approach with the correct definition gives the result. I added an answer in the answer section.","['algebraic-topology', 'general-topology']"
1853575,$x(a^{1/x}-1)$ is decreasing,Prove that $f(x)=x(a^{1/x}-1)$ is decreasing on the positive $x$ axis for $a\geq 0$. My Try: I wanted to prove the first derivative is negative. $\displaystyle f'(x)=-\frac{1}{x}a^{1/x}\ln a+a^{1/x}-1$. But it was very difficult to show this is negative. Any suggestion please.,"['derivatives', 'calculus']"
1853612,How to adjust the parameters of Lotka-Volterra equations to fit the extremal values of each population,"I found a graph that represents predator and prey populations over time, and I was hoping that someone could explain how I could generate two Lotka-Volterra equations that would give a good approximation of this graph.  That is, given that the populations satisfy the initial conditions in the graph, I would like the equations to have the prey population oscillate between 20000 and 80000 individuals and the predator population oscillate between 10000 and 45000.  Would someone be able to explain how I could quickly generate these equations myself, or give me the equations?  I don't have access to any high-powered software (like Matlab), so I can't use any answer that involves software.  Thanks!","['ordinary-differential-equations', 'mathematical-modeling']"
1853613,"Jessica the Combinatorics Student, part 2","The original question about Jessica , which I encourage review of, is as follows: Jessica is studying combinatorics during a $7$-week period. She will study a positive integer number of hours every day during the $7$ weeks (so, for example, she won't study for $0$ or $1.5$ hours), but she won't study more than $11$ hours in any $7$-day period. Prove that there must exist some period of consecutive days during which Jessica studies exactly $20$ hours. My follow-up question is this: How far into the course (how many days) do you need before you can be sure that Jessica has had some sequence of consecutive days with a twenty-hour study total? My suspicion is that the answer is: twenty days. I think my answer at the original question can be used to demonstrate that four weeks is certainly enough","['combinatorics', 'pigeonhole-principle']"
1853629,Sum to closed form,"I need to evaluate the following summation: $$
\sum_{n\in\mathbb{Z}} \frac{-1}{i(2n+1)\pi -\mu}
$$ where $n$ is summed over all the integers from $-\infty$ to $\infty$ including 0. Putting this into Mathematica gives $\frac{1}{2}\tanh\frac{\mu}{2}$. What is the intermediate steps to get from the summation to the closed form?","['summation', 'sequences-and-series', 'closed-form']"
1853666,"$\int_a^af(x) \, dx$ always $0$?","I was studying integrals and just out of curiosity, Does there exist any ' continuous ' functions such that $\int_a^af(x) \, dx$ ($a$ is any number) equals a value other than $0$? Since continuous functions are Riemann integrable, so I think it should be $0$. Is this correct? Also, with out the condition ' continuous ', does there exist any function such that $\int_a^af(x) \, dx$ isn't $0$? EDIT I'm looking for any function that $\int_a ^a\ f(x) \neq 0$. Can anyone find me one?",['integration']
1853675,Application of Derivatives rigorous proof,"Let $f:R\rightarrow R$ be a function such that all its successive derivatives exist in all $R$ and also $f(x)f''(x)\leq 0$ everywhere. If $\alpha$ and $\beta$ be two successive roots of $f(x)=0$. Then prove that $f'''(x)=0$ for atleast one $\gamma \in(\alpha,\beta)$. My Attempt: I began by taking an example $f(x)=(x-1)(2-x)$ and let $x=1$ and $x=2$ be its two successive roots.The statement is trivially true. Then I took the function $f(x)=e^{-x}(x-1)(2-x)$.The statement is true here also so on and so forth. But what would be the exact proof I wonder. It appears to be a question of Rolle's Theorem.","['derivatives', 'calculus']"
1853695,New Proof of Pythagorean Theorem (using inscribed circle)?,"I was solving an easy problem for fun when I stumbled onto this, and was wondering if this was a correct and possibly a new proof of the Pythagorean Theorem. Given right triangle $\triangle ABC$, and side lengths $a$, $b$, and $c$.  Inscribe in  $\triangle ABC$ a circle, which has radius $r$, and origin point $O$.  Connect $O$ to vertices $A$, $B$ and $C$, such that you form $\overline{AO}$, $\overline{BO}$, and $\overline{CO}$.  This creates three trianlges: $\triangle ABO$, $\triangle BCO$, and $\triangle ACO$.  Obviously the area of these three new triangles equals that of $\triangle ABC$.  Notice that the radius, $r$, of the inscribed circle is the height of the three new triangles.  Adding the areas together, we get: $$\frac{ar}{2}+\frac{br}{2}+\frac{cr}{2}=\frac{ab}{2}$$  Solving for $r$, you get: $$r=\frac{ab}{a+b+c}$$ Now look at this picture: By the property of tangential distances, we know that: $$(a-r)+(b-r)=c$$  So solving for $r$ again, we get: $$r=\frac{a+b-c}{2}$$  Now setting the two equations equal to $r$ equal to each other and some slight algebra: \begin{align}
\ \frac{a+b-c}{2}&=\frac{ab}{a+b+c}
\\ 2ab&=a^2+ab-ac+ab+b^2-bc+ac+bc-c^2
\\ 2ab&=a^2+2ab+b^2-c^2
\\ c^2&=a^2+b^2
\end{align} Q.E.D. Thoughts?","['circles', 'proof-writing', 'proof-verification', 'geometry']"
1853751,Intuition Behind the Hyperbolic Sine and Hyperbolic Cosine Functions,"After enough time studying mathematics, we develop an instinct for the sine and cosine functions and their relationship to our standard Euclidean Geometry. I have come across the functions $\sinh(x)$ and $\cosh(x)$ multiple times while studying math including: $(1)$ Lorentz Transformations $(2)$ Integrals and Identities $(3)$ Complex Analysis. Taken at face value, I understand these functions and their definitions $-$ but I feel like I'm missing the point. What is a natural way for me to understand these functions as intuitively as I understand $\sin(x)$ and $\cos(x).$ Note: I have consulted other answers looking for the answer to this question. I am searching for a more fundamental explanation of how these functions came about analogous to the natural representations of $\sin$ and $\cos$ in terms of angles on the unit circle. Of course If I overlooked such an explanation, please simply point me to it.","['hyperbolic-geometry', 'functions', 'geometry']"
1853792,Proof about a Topological space being arc connect,"While reading a book i found a topological space described as: Let $(X,\tau)$ be the topological space formed by adding to the
  ordinary closed unit interval $[0,1]$ another right end point,say
  $1*$, with the sets $(a,1)\cup${1*} as a local neighborhood basis. Then it says that such topological space is arc connected.
I found almost exactly the same question here which has yet to be solve,however i'll provide some details. The book itself states that since [0,1] and [0,1)$\cup${1 } are homeomorphic as subspaces,and the subspace topology on [0,1] is
  Euclidean,X is the union of two compact subspaces and thus compact,by
  the same reasoning it is arc connected.* How can such argument prove me that there is a injective path from $1$ to $1*$? Is it possible to explicit such path? Further details: As the book states: Path and arc connectednesss relate to the existence of certain continuous functions from the unit interval into a topological
  space.Continuous functions from the unit interval are called paths,if
  they are one-to-one they are arcs.","['general-topology', 'metric-spaces', 'connectedness']"
1853807,"The Jordan Decomposition Theorem, Folland","The Jordan Decomposition Theorem - If $\nu$ is a signed measure, there exists unique positive measures $\nu^+$ and $\nu^-$ such that $\nu = \nu^+ - \nu^-$ and $\nu^+\perp \nu^-$. Attempted proof - Let $X = P\cup N$ be a Hahn decomposition for $\nu$ where $P$ and $N$ are positive and negative sets respectively.. Then let us define the positive and negative measure as such $$\nu^{+}(E) := \nu(E\cap P) \ \ \ \nu^{-}(E) := - \nu(E\cap N)$$ then, 
\begin{align*}
\nu^+(E) - \nu^-(E) &= \nu(E\cap P) + \nu(E\cap N)\\
&= \nu(E)
\end{align*}
So we have $\nu = \nu^+ - \nu^-$ and $\nu^+\perp \nu^-$. Now I believe to complete this proof we need to show uniqueness. As in assuming we have $\nu = \tilde{\nu^{+}} - \tilde{\nu^{-}}$ is another such pair and we have $\tilde{\nu^{+}}\perp\tilde{\nu^{-}}$ we can find $\tilde{N}$ and $\tilde{P}$ such that $X = \tilde{P}\cup\tilde{N}$ and we need to check that $\tilde{P}$ is positive and $\tilde{N}$ is negative (not sure how to do that yet). Then we need to show that $\tilde{\nu^{+}} = \nu^+$ and $\tilde{\nu^{-}} = \nu^-$ again I am not sure how to do that either yet. Any suggestions is greatly appreciated.","['real-analysis', 'measure-theory']"
1853808,Product of a vector and its transpose (Projections),"I am doing a basic course on linear algebra, where the guy says $a^Ta$ is a number and $aa^T$ is a matrix not.m Why? Background: Say we are projecting a vector $b$ onto a vector $a$. By the condition of orthogonality, the dot product is zero
$$a^T(b-xa)=0$$ 
then $$x =\frac{a^Tb} {a^Ta}$$.
The projection vector $p$ since it lies on $a$ is:
$$p=ax$$
$$p=a\frac{a^Tb} {a^Ta}$$ $$p=\frac{aa^T} {a^Ta}b$$
To me both $aa^T$ and $a^Ta$ are dot products and the order shouldn't matter. Then 
$p=b$. But it is not. Why?","['projection-matrices', 'transpose', 'linear-algebra', 'vectors']"
1853824,Existence of a homeomorphism that does not return much,"Let $f:X\rightarrow X$ a homeomorphism where $X$ is a compact metric space. Fix $x\in X$, denote $O(f,x)=\{ f^n(x):n\in \mathbb{Z}\}$ the orbit of $f$ by $x$. For $m\in \mathbb{N}$ denote $O(f,x,m)=\{ f^j(x): \vert j\vert \leq m\}$ and $\#(A)$ is the cardinal of $A$. I am interested in the existence of an example of $f$ such that: there is $\delta>0$ with the following property $$\displaystyle{\lim_{m\to\infty}}\frac{\#(B[z,\delta]\cap O(f,x,m))}{\#(O(f,x,m))}=0$$
where $z\in \overline{O(f,x)}\setminus O(f,x)$ and $B[z,\delta]$ is the ball closed. I appreciate if you could give me some suggestion to know if such homeomorphism exists.","['real-analysis', 'metric-spaces', 'dynamical-systems', 'analysis']"
1853838,radius of convergence of $1/(1+z^2)$ about $z=2$ using geometric series approach,"I would like to calculate the radius of convergence of $f(z)= 1/(1+z^2)$ about $z=2$ using the geometric series approach. Let me first state that according to a theorem, the radius of convergence about a point $z_o$ is the smallest distance from $z_o$ to a singularity. Using this method, the distances from $z=2$ to the singularities $z=i$ and $z=-i$ are $\sqrt{5}$ which is the radius of convergence. However, I get a different answer using the geometric series approach. I can re-write f(z) as $(1/5)/(1+(4/5)(z-2)+(1/5)(z-2)^2)$. Comparing this to the expression $1/(1+z)$, we want $|(4/5)(z-2)+(1/5)(z-2)^2|<1$ for convergence.
This results in $|z|<3$ implying the radius of convergence is $3-2=1$. Why is the answer here different? thanks","['complex-analysis', 'taylor-expansion', 'sequences-and-series', 'convergence-divergence']"
1853845,Find $\sin \theta $ in the equation $8\sin\theta = 4 + \cos\theta$,Find $\sin\theta$ in the following trigonometric equation $8\sin\theta = 4 + \cos\theta$ My try -> $8\sin\theta = 4 + \cos\theta$ [Squaring Both the Sides] => $64\sin^{2}\theta = 16 + 8\cos\theta + \cos^{2}\theta$ => $64\sin^{2}\theta - \cos^{2}\theta= 16 + 8\cos\theta $ [Adding  on both the sides] => $64\sin^{2}\theta + 64\cos^{2}\theta= 16 + 8\cos\theta + 65\cos^{2}\theta$ => $64 = 16 + 8\cos\theta + 65\cos^{2}\theta$ => $48 = 8\cos\theta + 65\cos^{2}\theta$ => $48 = \cos\theta(65\cos\theta + 8)$ I can't figure out what to do next !,['trigonometry']
1853872,Symmetrical and skew-symmetrical part of rotation matrix,"Every matrix can be decomposed to symmetrical and skew-symmetrical part with the formula:
$ A=\dfrac{1}{2}(A+A^T)+\dfrac{1}{2}(A-A^T)$. However if it is known only symmetrical part (we assume here that the whole matrix is unknown) it's impossible without additional information to reconstruct exactly skew-symmetrical part and vice versa. In the case of 3D rotation matrix we have additional constraints and probably such reconstruction is possible.
Let's look at Rodrigues formula and two (symmetrical and skew-symetrical) parts  of rotation matrix: $R(v,\theta)= \{I+(1-cos(\theta))S^2(v)\} + sin(\theta)S(v)$ where $S(v)=\begin{bmatrix}
v\times{i} &  v\times{j}&  v\times{k} 
\end{bmatrix}^T$, skew-symetric matrix itself  ($3$ DOF set by components of an axis $v$) One can notice that having skew-symmetrical part of rotation matrix it is  relatively easy to reconstruct symmetrical part. Indeed $skew(R)=sin(\theta)S(v)$ and the whole expression $skew(R)$ can be decomposed to the product $kK$ in such a way that the sum of squares of matrix $K$ entries i.e. $ \begin{bmatrix}
1 &  1 & 1 
\end{bmatrix}  (K\circ{K}) \begin{bmatrix}
1  &  1  & 1 
\end{bmatrix}^T =2 $
... then $k=sin(\theta)$ and $K=S(v)$ and we can calculate $ cos(\theta)$ and $ S(v)^2$ what makes possible reconstruction of symmetrical part. Exactly we have two solutions because $sin(\theta)=sin(\pi-\theta)$. In the second case when we want to reconstruct skew-symmetrical part the solution seems to be difficult to find (at least for me) so my question is: how to obtain skew-symmetrical part of rotation matrix $skew(R)$ knowing its
symmetrical part $sym(R)$? additionally: why do such asymmetry in difficulty of solutions exist at all ? ( symmetry and skew symmetry in the first formula for the decomposition of any matrix $A$ seem not to differ too much ) what is the situation for higher dimensions? (when we don't have a Rodrigues formula)","['matrices', 'rotations', 'linear-algebra']"
1853894,What is the expected distortion of a linear transformation?,"Let $A: \mathbb{R}^n \to \mathbb{R}^n$. I am interested in the ""average distortion"" caused by the action of $A$ on vectors. (i.e stretching or contraction of the norm). Consider for instance the uniform distribution on $\mathbb{S}^{n-1}$, and the random variable $X:\mathbb{S}^{n-1} \to \mathbb{R}$ defined by $X(x)=(\|A(x)\|_2)^2$. What is the expectation of $X$? Using SVD, it is easy to check that the problem reduces to $A$ being a diagonal matrix with non-negative entries. So, the question amounts to calculating $$\int_{\mathbb{S}^{n-1}} \sum_{i=1}^n (\sigma_ix_i)^2 $$ (and dividing by the volume of $\mathbb{S}^{n-1}$). Is there a closed formula for this integral? Also, one could take the expected value of the norm, and not its square (I thought this should be easier if there are no sqaure roots involved...)","['matrices', 'probability', 'linear-algebra', 'random-variables']"
1853903,Intuitive reasons of ring modulo maximal ideal or prime ideal,"Are there any intuitive reasons that can help us remember that $R/I$ is a field iff $I$ is a maximal ideal; $R/I$ is an integral domain iff $I$ is a prime ideal? (I can understand the proof, but have problems with remembering the result correctly and intuitive understanding.) I can roughly understand that if $I$ is maximal ideal, $R/I$ will have the minimal number of ideals namely zero ideal and itself, which is the criteria of being a field? How about if $I$ is prime ideal? How do we intuitively see that $R/I$ is a domain?","['abstract-algebra', 'ring-theory', 'ideals']"
1853953,1-1 correspondence of class group of an order '$\mathcal{O}$' and elliptic curves having complex multiplication by $\mathcal{O}$,"I came across these two results Let $\mathcal{O}$ be an order in an imaginary quadratic field.There is a 1−1 correspondence between the ideal class group $C(\mathcal{O})$
and the homo-thety classes of lattices with $\mathcal{O}$ as their full ring of complex multiplication. There is a 1-1 correspondence between invertible ideal classes of ring $\mathcal{O}$ and the set of triples $$\left\{(a,b,c) \in \mathbb{Z}^3:{\begin{split}& a>0;\  \gcd(a,b,c)=1 ;\\&|b| \leq a\leq c;\ b^2-4ac=D;\\& b\ >0\text{ whenever }|b|=a\text{ or }a=c\end{split} }\right\}$$ where $D$ is the discriminant of the number ring $\mathcal{O}$ My question is how can I relate both of these results, It looks to me that $2$ can be inferred from $1$. Any refrence for the same would be of great help.","['number-theory', 'complex-multiplication', 'elliptic-curves', 'ideal-class-group']"
1853989,Conditional expectation and independence on $\sigma$-algebras and events,"In many statistics papers, proofs might proceed as follows: Under the event $A$, the random variables $X$ and $Y$ are independent. (Often this means that on $A^C$, they might be dependent). Then some properties of conditional independence might be used, e.g. to calculate $ \mathbb E[ X \mid A, Y] $. I feel quite uncomfortable with the latter type of manipulations. Therefore, I am wondering two things: 1) What is the proper definition of conditional independence when conditioning on both events and $\sigma$-algebras? My guess would be that we have to check the factorization property only for sets $A \cap B_X$ and $A \cap B_y$, where $B_X \in \sigma(X)$, $B_Y \in \sigma(Y)$ the generated $\sigma$-algebras of the above random variables. (Similarly for the definition of conditional expectation.) 2) Is there any reference on properties of such expectations which are taken conditionally on both $\sigma$-algebras and events? Something akin to the standard properties of conditional expectations (conditioned on $\sigma$-algebras) used to simplify expressions or to calculate them. Or is there any simple trick to generalize the standard results to this setting?","['probability-theory', 'conditional-expectation', 'measure-theory']"
1853990,How do we interpret and perform integrals using infinitesimals?,"If $\mathrm{d}x$ is treated as a hyperreal infinitesimal we can easily do derivations. How do we interpret and perform integrals using infinitesimals? What is the $\mathrm{d}x$ in $\int x^3\,\mathrm{d}x$ ?","['nonstandard-analysis', 'infinitesimals', 'integration']"
1853992,How to write the commutator subgroup in terms of the generators of the group?,"Let $G=\langle\ S\ |\ R\ \rangle$ be a finitely presented group. The commutator subgroup of $G$ is the group generated by $\{[a,b]\ |\ a,b\in G\}$ and is denoted by $[G,G]$, where $[a,b]=aba^{-1}b^{-1}$. 1. Is it true that $[G,G]$ is generated by $\{[s,t]\ |\ s,t\in S\}$ ? I need to show that any $[a,b]$ can be written as a word in $[s,t]$ but I am not sure if it can be done. The above is a generalized question which I asked myself when considering the particular case below - Let $W=\langle\ S\ |\ R\ \rangle$ where, $$S=\{s_1,\cdots,s_{2n}\}$$ $$R=\left\{s_i^2 : 1\le i\le 2n\right\}\bigcup\left\{(s_is_j)^2:1\le i,j\le 2n,j\neq i+n\right\}$$ 2. Then is $[W,W]$ is generated by $\{[s_i,s_{i+n}]\ |\ 1\le i\le n \}$ ? Thank you.","['abstract-algebra', 'group-theory', 'group-presentation']"
1854029,Is the projective line minus one point always isomorphic to the affine space?,"I'm thinking about the following problem: If I take a general point $p \in \mathbb{P}^1$ out of the projective line, is $\mathbb{P}^1 - \{ p \}$ isomorphic to the affine space $\mathbb{A}^1$? I ask this because if $p = [1, 0] \in \mathbb{P}^1$, the map $[x, 1] \mapsto x$ gives an isomorphism $\mathbb{P}^1 - \{[1, 0] \} \cong \mathbb{A}^1$. I guess that this should be true somehow for a general point $p$, but I can't quite get my head around the map that I need to define.","['abstract-algebra', 'projective-geometry', 'algebraic-geometry']"
1854038,Stochastics exam Exercise,"The professor uploaded an exam to practice, but unfortunately I have no solutions. Let U be a unifomly distributed random variable on $[0,1]$. 1) Let $X=-ln(U)$. Show that $X$ is distributed exponentially. 2) Compute $\mathbb{E}(\ln(U))$ and $Var(\ln(U))$ 3) Let $U_1, U_2, ...$ be a sequence of independent random variables, all with the same distribution as $U$. We define $V_n=\Pi_{i=1}^n U_i$. Show that the sequence $V_n^{1/n}$ converges almost surely and compute its limit. 4) Let $(X_n)_{n\in\mathbb{N}}$ be a sequence of real valued random variables that converges in distribution to a random variable $X$. Prove: If $f:\mathbb{R}\to\mathbb{R}$ is a continuous function, then the sequence $(f(X_n))_{n\in\mathbb{N}}$ converges in distribution to $f(X)$. 5) Let $W_n = (e^nV_n)^{1/\sqrt{n}}$. Show using 4) that $W_n$ converges in distribution for $n\to\infty$ to a random variable W. Determine the distribution of $W$. 
Hint: Consider $X_n=\ln W_n$ and express $W_n$ as $W_n=\exp(X_n)$ What I did so far: 1) $\mathbb{E}(f(X))=\mathbb{E}(f(-\ln(U)))=\int_0^1f(-\ln(u))du$ using change of variables $-\ln(u)=x$ implies $u = e^{-x}$ thus $\int_0^1f(-\ln(u))du = \int_0^\infty f(x) e^{-x}dx$. Hence $X$ is distributed $Exp(1)$ 2) $\mathbb{E}(\ln(U))=\int_0^1 \ln(u)du=-1$ 
For the Variance we need $\mathbb{E}(\ln(U))^2)=\int_0^1\ln(u)^2=...=2.$ Hence $Var(\ln(U)) = 1.$ 3) I tried using the definition of almost sure convergence $\mathbb{P}(\lim_{n\to\infty} (V_n)^{1/n} = V)=1$ $\mathbb{P}(\lim_{n\to\infty} (U_1,...,U_n)^{1/n} = V)=1$ But the U_n have the same distribution
$\mathbb{P}(\lim_{n\to\infty} (U^n)^{1/n} = V)=1$ But this doesn't make much sense since the $n$'s cancel.","['probability-theory', 'random-variables']"
1854049,A space more fundamental than Euclidean space,"Summary: The mathematical physicist Paolo Budinich attributes to Élie Cartan the statement that the geometry of pure spinors is ""more elementary"" or more ""fundamental"" than Euclidean geometry, which is ""more complicated"".  This raises several questions: In what sense do pure spinors form a geometry?  What sort of geometry is this? Is there a precise sense in which this geometry is simpler than Euclidean geometry? Did Cartan ever make such a direct statement, or is Budinich inferring that Cartan held such views from the overall philosophy of Cartan's work?  What is the current status of this statement, e.g. vague philosophy, more or less precise body of conjecture, well-established theory? Budinich also suggests that there is a connection of this more fundamental geometry to the geometry of minimal surfaces, in particular, to the property that they are generated by null vectors by means of the Weierstrass–Enneper parameterization .  Is there some sort of geometry underlying the Weierstrass–Enneper parameterization that is more elementary or more fundamental than Euclidean geometry? Background: In a filmed conversation ( at $\color{blue}{\text{14:00}}$ mark ) involving Paolo Budinich, Abdus Salam, Dennis Sciama, and Ed Witten following Witten's 1986 Dirac Medal lecture (the caption at the start of the film, dating it to 1990, is certainly in error), Budinich makes the following remark: By which space-time is different than we thought. And by which I mean one could go back perhaps to Cartan who said the same thing in ’37, that Euclidean geometry is a very complicated geometry, while the geometry of zero vector is the more elementary one. Now there is a lot of zero vectors in these minimal surfaces—zero vectors. This was known 150 years ago, that zero vectors generate, by Weierstrass, minimal surfaces, so maybe that is also—that the Euclidean geometry is not the most elementary one. It seems likely that Budinich meant to say ""null vector"" here rather than ""zero vector"". The abstract of a contemporaneous paper by Budinich, P. Budinich, Null vectors, spinors, and strings, Commun. Math. Phys. 107 455–465 (1986), contains the following: It is shown how, in the frame of the Cartan conception of spinors, the old theorems on minimal surfaces, as generated from null curves formulated by Enneper–Weierstrass (1864–1866) for 3-dimensional ordinary space, and by Eisenhart (1911) for 4-dimensional space time, may be reformulated in terms of complex 2- and 4- component projective spinors respectively. A decade later, in the preprint Geometrical aspects of quantum mechanics in compactified momentum space , he writes of Cartan, He stressed the simplicity and elegance of spinor geometry and, because of this, he formulated the hypothesis of its fundamental character, insofar all the properties of euclidean geometry may be naturally derived from it, by considering the euclidean vectors as squares of simple spinors * (their components as bilinear polynomia of spinor components) [1]. * The historical fact that euclidean geometry was discovered long ago, and was thereafter universally considered as the most elementary form of geometry, might be due to the fact that its elements like planes, lines, points, are well accessible to our common, everyday intuition, based on our optical sensorial perceptions, while spinors were discovered much later, in the frame of advanced mathematics, and they are less accessible to our intuition, in fact a simple spinor may be conceived as a totally null plane, that is a plane whose (null) vectors are all orthogonal to each other, however, once the abstract mathematical reasoning is adopted, one is easily convinced of their geometrical simplicity and elegance which induced Cartan to formulate his conjecture on their fundamental role in elementary geometry. These quotations come from page 2 of the preprint.  Reference [1] is E. Cartan, Leçon sur la Théorie des Spinors (Hermann, Paris, 1937), which was translated into English as The Theory of Spinors .  The term ""simple spinor"" is used interchangeably with ""pure spinor"".  Similar statements can be found in many of Budinich's papers, spanning several decades. Remark: Michael Atiyah is quoted by G. Farmelo in his biography of Paul Dirac as having written No one fully understands spinors.  Their algebra is formally understood but their geometrical significance is mysterious.  In some sense they describe the 'square-root' of geometry and, just as understanding the concept of the square root of $-1$ took centuries, the same might be true of spinors. This seems to suggest that the idea that there is a simpler spinor geometry underlying Euclidean geometry is more of an uncompleted hope than a fully-developed theory.","['mathematical-physics', 'spin-geometry', 'clifford-algebras', 'differential-geometry']"
1854066,Ring of algebraic integers as lattice points in the complex plane,"Let, $i=\sqrt{-1}$ and $\omega = e^{\frac{2\pi i}{3}}$. I know that we can represent the ring of integers $\mathbb{Z}[i]$ and $\mathbb{Z}[\omega]$ as square and triangular lattice on complex plane respectively. Motivated by this and this question I wish to know: Is there a way to plot such lattice diagrams for each and every ring
  of algebraic integer? If yes, then how? Is there a way to determine the shape of lattice units (rectangular, triangular,...) without actually plotting them? By ""how"" I am asking for an example of code for a software (preferably open-source) which can be used to plot all such lattices.","['number-theory', 'reference-request', 'algebraic-number-theory']"
1854071,How does restriction of scalars interact with tensor products?,"Say that we have a morphism of commutative rings $f: R \to S$. Does the restriction of scalars functor $f^*: S \text{Mod} \to R \text{Mod}$ commute with tensor products? In other words, I would like to know if we have an isomorphism of $R$-modules:
$$f^*(V \otimes_S V') \cong f^*V \otimes_R f^*V'.$$ 
If it does not hold in general, then under what conditions is it true? The reason I am wondering is that it seems that an $S$-bilinear map $\phi: V \times V' \to V''$ determines an $R$-bilinear map under restriction of scalars. I get the feeling that it is not quite right to say that $f^*$ is a monoidal functor, but I am not sure what the correct relationship to tensor products should be.","['abstract-algebra', 'ring-theory', 'modules']"
1854082,Variable triples vs. single quad card game. Who has the advantage?,"$2$ people decide to count wins on a card game using a well shuffled standard $52$ card deck. Community (shared) cards are drawn one at a time from the deck without replacement until there is a winner for a hand. The rules are player A can initially win if $4$ triples appear (such as $KKK,444,AAA,777$).  Player B wins if a single quad appears (such as $QQQQ$).  As soon as there is a winner, the hand is finished, the win is awarded, all drawn cards are returned to the deck, the cards are reshuffled well, and the next hand will be drawn.  There is one twist however. If B wins a hand, then next hand will have a lower win threshold for A.  For example, initially the win threshold for A is $4$ triples.  However, if B wins the first hand, then the new threshold will be $3$ triples.  Conversely, each time A wins, A's threshold will be increased by $1$.  So for example, if A wins the first hand, A's new win threshold will be $5$ triples to win.  The minimum # of required triples is $1$. No more than $39$ cards will ever need to be drawn to determine a winner since even if $13$ triples are required, the $39$th card will guarantee a winner but it is likely a quad would have appeared way before then. Note that the triples and quads need not be in any order.  For example, $Q,2,Q,4,Q,7,A,10,Q$ is a quad.  The requirement is not $4$ like ranks in a row however that is also a quad but very unlikely. I am thinking since the difficulty of A winning is variable based on who wins the previous hand(s), this should reach some type of equilibrium where it is about equally likely for each to win in the longrun but how can I show this mathematically? I ran a computer simulation and it looks like my initial hypothesis is right.  There seems to be an equally likely chance for A or B to win on average.  Even with as few as $10$ trials ($10$ winning hands), I am seeing mostly $5$ wins for each but sometimes $4$ vs. $6$ but I didn't even see $3$ vs. $7$ yet so it seems to hit equilibrium VERY quickly.  Since the # of required trials (wins) is so low, you can probably just try this with a real deck of cards and confirm it is about $50/50$ with as few as $10$ winning hands.  Just remember to update the winning threshold for A properly (for example, $4, 5, 4, 3$...).  Unlike a fair coin toss where $8$, $9$, or even $10$ heads are possible, it seems almost impossible for that to happen with this type of ""self adjusting"" game. So how can it be shown mathematically that this type of game will reach equilibrium where either player has about the same chance to win overall? If you do a computer simulation of this, it is somewhat amazing how many times it will hit exactly half and half.  For example, if I run $1000$ decisions, I usually get $500$ A wins and $500$ B wins.  It is very consistent (yet I am using different random numbers each run).  This is WAY more predictable than something like fair coin flips which could get off to a ""rocky"" start.  I think you can call this type of game ""self adjusting"" in that it will reach a ""fair equilibrium"" very quickly. This seems like a hard problem to state mathematically so I put a bounty of $100$ points on it as an extra incentive to those of you who want to try to solve it.  If $2$ different users submit good answers then what I usually do is give one person the checkmark and the other the bounty to be more fair to both.  Good luck.","['probability', 'card-games']"
1854083,Not understanding the proof that there is no surjection from a set to its powerset,"Here is the question: If a set, $A$, is finite, then $|A| < 2^{|A|} = |P(A)|$, and so there is no surjection from set $A$ to its powerset. Show that this is still true if $A$ is infinite. Here is the proof: We prove there is no surjection by contradiction: suppose there was a surjection f : A → P(A) for some set A. Let W ::= {x in A | x not in f(x)}. So by definition, (x in W) ←→ (x not in f(x)) for all x in A. But W ⊆ A by definition and hence is a member of P(A). This means W = f(a) for some a in A, since f is a surjection to P(A). So (x in f(a)) ←→ (x not in f(x)) for all x in A. Substituting ""a"" for ""x"" yields a contradiction, proving that there cannot be such an f. My question is what exactly does the definition of W mean? I don't understand the condition of ""x not in f(x)"". Second, I'm trying to learn proofs and I'm struggling with how one would be able to know to create W, which seems to be important to this proof. Third, how does one know how to create auxiliary information such as W when proving things in general?","['proof-writing', 'elementary-set-theory', 'proof-explanation']"
1854124,The problem of congruent areas in a triangle.,A problem was posed in front of me and I couldn't solve it after multiple attempts-- Consider any triangle and 3 concurent cevians are drawn from each of its 3 points . Now the figure formed has 6 sub triangles - if the areas of 3 alternate sub triangles are equal then prove that the point of concurrence is the Centriod.,"['median', 'triangles', 'geometry', 'contest-math', 'centroid']"
1854187,"For any $x\in \mathbb{N}$ does there exist $m\in \mathbb{N}$ such that $2x+1+2m, 2x+1+4m$ are both prime?",Could someone please give me a proof (or counter example) for this (I believe it is true): For any $x$ (Whole Number) there exists some $m$ (Also Whole) such that $2x+1+2m$ and $2x+1+4m$ are both prime. An equivalent is for any odd $n$ there exists some even $y$ such that $n+y$ and $n+2y$ are both prime. I am pretty sure a proof would be based around Dirichlet's Theorem.,"['number-theory', 'prime-gaps', 'prime-numbers']"
1854197,Calculate the number of integers in a given interval that are coprime to a given integer,"We can calculate the number of integers between $1$ and a given integer n that are relatively prime to n , using Euler function: Let $p_1^{\varepsilon1}\cdot p_2^{\varepsilon2} \cdots p_k^{\varepsilon k}$ be the prime factorization of n . Then $\phi(n)=n\cdot\frac{p_1-1}{p_1}\cdot\frac{p_2-1}{p_2}\cdots\frac{p_k-1}{p_k} $ But why stop here? My question is: Is there a fast algorithm to calculate the number of integers between two integers that are relatively prime to a given integer? According to A059956 , The probability that two integers x and x + y picked at random are relatively prime is $1/\zeta{(2)}$ . Can I estimate the answer to be:  $$\frac{y}{\zeta{(2)}}=\frac{6y}{\pi^2}$$
Is there a better method that can give me a more accurate answer to my question?","['zeta-functions', 'number-theory', 'algorithms', 'probability', 'prime-numbers']"
1854213,Prove an equality of complex matrixes,"If $A \in M_2(\mathbb{C})$ a matrix so that $$\det\left(A^2 + A +
 I_2\right)=\det\left(A^2 - A + I_2\right)=3 \tag1$$ then $$A^2\left(A^2 + I_2\right)=2I_2. \tag2$$ I tried to use Cayley-Hamilton theorem, without success. I think one step might be to prove $A$ is invertible. Meanwhile I found (2) is equivalent to:
$\left(A^2 -I_2\right)\left(A^2 +2I_2\right)=O_2. \tag3$","['matrices', 'determinant']"
1854217,Does absorbing Markov chain have steady state distributions?,"If I am not mistaken, the steady state distribution is independent of initial state distribution, and regular Markov chains satisfies this definition. On the other hand, since the row of each limiting matrix for an absorbing Markov chain is the same, the state distribution after a large number of transitions for an absorbing Markov chain is dependent on the initial state distribution. However, I read somewhere that an absorbing Markov chain can have steady state distributions - which contradicts what I have always believed. I mean, I know what they are trying to imply (as in the chain will converge to a fixed probability), but is it the correct term?","['terminology', 'markov-chains', 'probability-theory']"
1854253,Infinite Ordinal Sum,"When working with ordinal numbers, would it be correct to say that:
$$ \sum_{i=0}^{\infty}1 = \omega$$
Or does this simply not make sense? In the ordinals, does the notation $\sum^\infty_{i=0}$ even make sense or would $\sum_{i=0}^\alpha$, with $\alpha$ being a (potential infinite) ordinal, be the only correct notation? Thank you very much.","['sequences-and-series', 'ordinals']"
1854254,Proof/derivation of $\lim\limits_{n\to\infty}{\frac1{2^n}\sum\limits_{k=0}^n\binom{n}{k}\frac{an+bk}{cn+dk}}\stackrel?=\frac{2a+b}{2c+d}$?,"I just came up with the following identity while solving some combinatorial problem but not sure if it's correct. I've done some numerical computations and they coincide.
$$\lim_{n\to \infty}{\frac{1}{2^n}\sum_{k=0}^{n}\binom{n}{k}\frac{an+bk}{cn+dk}}\;\stackrel?=\;\frac{2a+b}{2c+d}$$
Here $a$, $b$, $c$, and $d$ are reals except that $c$ mustn't $0$ and $2c+d\neq0$. I wish I could explain how I came up with it, but I did nothing but comparing numbers with the answer then formulated the identity, and just did numerical computations.","['combinatorics', 'summation', 'binomial-coefficients', 'limits']"
1854268,Prove that $(2^n-1)(3^n-1)$ is not a perfect square,"Prove that $(2^n-1)(3^n-1)$ is not a perfect square. I have tried this problem for a few days already and I feel I am really far from solving it. Most of my approaches have been analyzing how many times 2 divides the number, and how many times 3 divides it, as well as various mods. I am starting to think the proof is going to be factoring on a weird field or something like that instead. We can see that if $n$ is odd then $3^n-1$ is divisible by $2$ exactly one time so the exponent of $2$ in the prime factorization of the number is $1$ and thus it is not a perfect square. Furthermore by lifting the exponent lemma we know that since $n$ is even the exponent of $2$ in the prime factorization of $3^n-1$ is $3-1+v_2(2) = 2+v_2(n)$ so we need $v_2(n)$ to be even. Therefore it is greater tan or equal to $2$ i.e $4$ divides $n$. Similarly by lifting we can see that the exponent of $3$ in $2^n-1$ is $1+v_3(n)$ so we have $v_3(n)$ is odd i.e $3$ divides $n$. Therefore if the expression is a perfect square we must have $12|n$.","['number-theory', 'diophantine-equations']"
1854279,"Solve linear system with $A_{i,j} = \langle e_i, e_j\rangle^2$, edges of a triangle","I have three vectors in $e_i\in\mathbb{R}^3$ that form a triangle. Let us consider now the linear equation system $Ax=b$ with
$$
A_{i,j} = \langle e_i, e_j\rangle^2,\\
b_i = \langle e_i, e_i\rangle.
$$
I can solve these problems numerically, but somehow I feel I'm missing out. Perhaps the solution can even be constructed explicitly from $e_i$? Any ideas?","['matrices', 'simplex', 'triangles', 'geometry', 'linear-algebra']"
1854289,Concept of Trigonometric identities [duplicate],This question already has answers here : Show that $\frac {\sin x} {\cos 3x} + \frac {\sin 3x} {\cos 9x} + \frac {\sin 9x} {\cos 27x} = \frac 12 (\tan 27x - \tan x)$ [duplicate] (3 answers) Closed 7 years ago . The value of the expression $$\dfrac{\sin x}{ \cos 3x} + \dfrac{\sin 3x}{ \cos 9x} + \dfrac{\sin 9x}{ \cos 27x}$$ in terms of $\tan x$ is My Approach If I take L.C.M of this as $\cos 3 \cos 9x \cos 27x$ and respectively multiply the numerator then it is getting very lengthy. Even if I will use the identity of $\sin 3x$ then also I am not getting appropriate answer. Please suggest some nice and short way of doing this question.,"['trigonometry', 'trigonometric-series']"
1854300,Textbook for Multivariable and/or Vector Calculus,"I'm looking for a tetxbook that covers Multivariable Calculus and/or Vector Calculus theoretically. I have done Analysis (single-variable) at the level of Introduction to Real Analysis by Bartle and Sherbet and Principles of Mathematical Analysis by Walter Rudin. Except for the theory of integration. I haven't done a lot of exercises from ""Baby Rudin"" but I have done a course on Topology so some of the exercises will be approachable now, though I may have to go and revise Topology to refresh my concepts/recall the theorems. Anyway, I'm looking for a textbook that extends the material in the aforementioned books to Multivariable and/or Vector Calculus. Any suggestions?","['multivariable-calculus', 'reference-request', 'book-recommendation']"
1854307,How to find the smallest set of generating elements in a group?,Is there a systematic procedure for finding the smallest set of generating elements of a finite group?,"['finite-groups', 'computational-algebra', 'group-theory']"
1854310,"When deciding the Hypothesis, do I, in some way, define the rejection region?","This has always troubled me a bit.
When I choose my hypothesis, do I define in some way the rejection region [RR], or, do I do that by choosing the test statistic I want to use? By fixing the significance level, I'm in a way determining the area/volume of the RR. In two different contexts(different null hypothesis), I've seen the same statistic being used with two different RR. In some books, the authors give the sense that once we decide the hypothesis, we've chosen the RR. Others, state explicitly that at least in some hypothesis, the RR is not completely defined, and we need other criteria... I would like to structure this as best as possible. Any help would be appreciated","['statistics', 'probability', 'statistical-inference']"
1854348,Limit $\lim_{x\to \infty} \frac{\sqrt{x+1}+\sqrt{x+2}-\sqrt{4x+12}}{\sqrt{x+2}+\sqrt{x}-\sqrt{4x+12}}$,"Using the equivalency, I got the answer equal to $1$, however the answer is given as $\frac34$. WolframAlpha also gives $\frac34$. Could someone tell me that the given answer is wrong or I made a mistake!","['radicals', 'limits']"
1854420,"Does a ""backwards"" choice function imply the Axiom of Choice?","One of my favorite formulations of the Axiom of Choice is that for any nonempty family $A$ of nonempty sets, there is a choice function $F\colon A\to\cup A$ such that $F(X)\in X$ for each $X\in A$. Using this, I was able to prove that there is also a function $f\colon\cup A\to A$ such that $x\in f(x)\in A$ for all $x\in\cup A$. I did this by considering any $x\in\cup A$, and letting $B_x=\{X\in A\mid x\in X\}$, and then letting $C=\{t\in\mathscr{P}(A)\mid \exists_{x\in\cup A}t=B_x\}$, i.e. $C$ is the set of all $B_x$. Now each $B_x$ is nonempty, so there is a choice function $F$ such that $F(B_x)\in B_x$ for each $B_x$. Defining $f(x)=F(B_x)$, I have $x\in f(x)\in A$ for every $x\in\cup A$. I suppose this is what I mean by a ""backwards"" choice function, even though it's not really one. Is this equivalent to AC? That is, if for any set $A$, there exists a function $f\colon\cup A\to A$ such that $x\in f(x)\in A$ for all $x\in\cup A$, then AC holds? I couldn't immediately see a way to imply the Axiom of Choice, in any equivalent formulation assuming that the above is true for any set $A$. Is it just a one way implication? Thanks.","['elementary-set-theory', 'axiom-of-choice']"
1854424,Are the elements of a module also called vectors?,"Are the elements of a module also called vectors? Or if someone says 'vector', are they talking only about a vector space? If no context is given, are there some standard assumptions?","['terminology', 'abstract-algebra', 'modules', 'linear-algebra']"
1854426,"My proof that if $P(A) \subseteq P(B)$, then $A \subseteq B$","I'm not sure if my proof is sound. Here it is: Assume that $P(A) \subseteq P(B)$, so any subset C of A is also a subset of B. Therefore, any element in C is also an element of A, and by the same reasoning is an element of B. Thus $A \subseteq B$, as all the elements in A are proven to be elements in B. I believe that I've covered that every single element in A must be an element in B, but for some reason I feel doubtful. Is my proof correct?","['proof-writing', 'elementary-set-theory']"
1854474,What does it mean for a pdf to have this property?,"What does it mean for a probability density function $f(x)$ to have the following property?
  $$1+\int_{x=0}^{\infty}x^2 \left(\frac{f'(x)^2}{f(x)}-f''(x)\right)dx>0$$ I have tried a lot to simplify this condition and see what it means (in terms of moments of $f(x)$, etc), but no luck yet. Do you have any idea?","['stochastic-processes', 'probability-distributions', 'statistics', 'probability', 'stochastic-calculus']"
1854482,$\|D_{f}(x) v\|=\|v\|$ $\implies$ $f$ is an isometry,"Let $f\colon \mathbb{R}^m \to \mathbb{R}^m$ be a $C^2$ map such that $\|D_f(x)v\|=\|v\|$ for all $v\in\mathbb{R}^m$, where $D_f(x)$ is the derivative of $f$ at $x$. Then I am asked to prove that $\|f(x)-f(y)\|=\|x-y\|$ for all $x,y\in\mathbb{R}^m$. There's a hint that says to use the Schwarz theorem. I just can prove that $\|f(x)-f(y)\| \le \|x-y\|$, using the image of $\alpha \colon [0,1] \to \mathbb{R}^m$, $\alpha(t)=x+(y-x)t$. Consider $\gamma\colon [0,1] \to \mathbb{R}^m$, $\gamma(t)=f(\alpha(t))$, we have that $\gamma '(t)=D_{f}(x +(y-x)t).(y-x)$. Since $\gamma$ connect $f(x)$ and $f(y)$ and it is $C^{1}$ we have: $\|f(x)-f(y)\| \leq L(\gamma) = \displaystyle\int_{[0,1]} \|\gamma '(t)\|dt=\displaystyle\int_{[0,1]}\|D_{f}(x +(y-x)t).(y-x)\|dt$.
By assumtion this is equals to $\displaystyle\int_{[0,1]} \|x-y\| \leq \|\displaystyle\int_{[0,1]} x-y\space dt\|= \|x-y\|$. I have no idea how use schwarz theorem in this case. Any help is welcome.",['analysis']
1854487,Combinations Proof [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Show that $2^n = \sum_{i=0}^n \binom ni$. I tried expanding $\sum_{i=0}^n \binom ni$ into sum of: $\frac{n!}{(n-r)!r!}$, and found the common ratio and then used the sum formula, but I did not get to $2^n$.","['permutations', 'statistics', 'probability', 'combinations']"
1854488,"If B(X) is isomorphic to B(Y), does that mean X is isomorphic to Y (for X and Y Banach spaces)?","Let $X$ and $Y$ be Banach spaces such that $\mathcal{B}(X)$ is linearly isomorphic to $\mathcal{B}(Y)$ (where $\mathcal{B}(\cdot)$ denotes the algebra of bounded linear operators).  Must it always be the case that $X$ is therefore isomorphic to $Y$? I suspect this is not true, but I can't immediately see how to produce a counter-example.  Surely, whether it is true or false, this must be well-known.","['functional-analysis', 'banach-spaces', 'banach-algebras']"
1854491,"In set theory, what does it mean for a variable to have a bar symbol above it?",Please see the image below. What does the bar symbol mean in this context?,"['notation', 'elementary-set-theory']"
1854501,"Rational numbers as vectors in infinite dimensional space with the basis $( \log 2,\log 3, \log 5, \log 7, \dots, \log p, \dots) $","Since every natural number can be represented as $a=2^{n_1}3^{n_2}5^{n_3}7^{n_4}\cdots p_k^{n_k}\cdots$ it makes sense to represent natural numbers by vectors, using the properties of logarithms: $$\log a=n_1 \log 2+n_2 \log3+n_3 \log5+\cdots$$ This space appears to be similar to the usual Euclidean space if we extend it to an infinite number of dimensions. If we allow negative coordinates, we can also put all rational numbers in this space. For example, here is part of the plane $(\log 2, \log 3)$: Does this space have any application in number theory? If it is studied, then how is it usually defined? Are the usual Cartesian vector dot product and the usual Euclidean norm used? Or does it make sense to use a different norm (for example, taxicab norm)?","['number-theory', 'rational-numbers', 'linear-algebra', 'vector-spaces']"
1854517,Show that the sum of a sequence of random variables converges almost surely,"Let $(X_n)_{n\in\mathbb N}$ be a sequence of non-negative iid random variables with $\mathbb E[X] < \infty$. How could one go about showing that $\sum^{\infty}_{k=0} e^{X_k} c^k < \infty$ almost surely for some $c \in (0,1)$? I've tried using the Borel-Cantelli lemma but I just can't make it work. Any suggestions?","['probability-theory', 'convergence-divergence']"
1854525,Is there a winning strategy for this tic-tac-toe?,"The figure shows a variation of a tic-tac-toe board, with the usual rules: three small balls each player and three in a row wins. Is it possible to transform the layout isomorphically so the same analysis applies regardless of the board layout?","['puzzle', 'general-topology', 'tic-tac-toe']"
1854528,Question on the coefficients of $(1+x+x^2+x^3+x^4)^{496}$,"Consider the expansion $$(1+x+x^2+x^3+x^4)^{496} = a_0+a_1x+\cdots+a_{1984}x^{1984}.$$
  $\quad$ (a) Determine the greatest common divisor of the coefficients $a_3,a_8,a_{13},\ldots,a_{1983}$. $\quad$ (b) Prove that $10^{340} < a_{992} < 10^{347}$. Is there an easier way to solve this and is there a formula for the multisection for just the sum of the coefficients and not including $x^k$? I thought of using the Multisection formula to prove (a). That is, $$\sum_{k \equiv r \pmod{m}}a_kx^k = \dfrac{1}{m} \sum_{s=0}^{m-1} \epsilon^{-rs} f(\epsilon^s x)$$ where $\epsilon$ is a primitive $m$th root of unity and $\displaystyle f(x) = (1+x+x^2+x^3+x^4)^{496}$. Thus we have $r = 3, m = 5$ and so \begin{align*}\sum_{k \equiv 3 \pmod{5}}a_kx^k &= \frac{1}{5} \sum_{s=0}^{4} w^{-3s} (1+w^s x+(w^{s}x)^2+(w^{s} x)^3+(w^{s}x)^4)^{496}\\&=\dfrac{1}{5}\sum_{s=0}^4 w^{-3s}\left(\dfrac{(w^s x)^{5}-1}{w^s x-1}\right)^{496}\\&= \dfrac{1}{5}\sum_{s=0}^4 w^{-3s} \left(\dfrac{x^{5}-1}{w^s x-1}\right)^{496}.\end{align*}",['number-theory']
1854550,When is $\overline{K}/K$ a Galois extension of $K$?,"When is $\overline{K}/K$ a Galois extension of $K$, where $\overline{K}$ stands for the algebraic closure of $K$? I have the following three extensions: $\overline{\mathbb{Q}}/\mathbb{Q}$,$\overline{\mathbb{F}_p}/\mathbb{F}_p$,$\overline{\mathbb{F_p} (t)}/\mathbb{F}_p (t)$ I know that algebraic closure is always normal. So I only need to check whether these algebraic closures are separable or not. But algebraic extension of characteristic zero field and finite field is always separable, so all three are Galois extension. Is my reasoning correct? Thank you.","['abstract-algebra', 'galois-theory', 'proof-verification']"
1854588,"$\ker \phi = (x_1-a_1, ..., x_n-a_n)$ for a ring homomorphism $\phi: R[x_1, ..., x_n] \to R$","Let $R$ be a commutative ring, $a_1, ..., a_n$ its elements and $\phi: R[x_1, ..., x_n] \to R$ defined by $ \phi(f(x_1, ..., x_n)) = f(a_1, ... ,a_n)$ a ring homomorphism. Prove: $\ker \phi = (x_1-a_1, ..., x_n-a_n)$ It is obvious that $(x_1 - a_1, ..., x_n -a_n) \subseteq \ker \phi$. I'm not sure how to prove the converse. At this point I don't know any division algorithms for multivariable polynomials, only for the ones in $R[x]$(and the book from where I taken the exercise doesn't assume the reader to know something beyond basics of rings and ideals, and the division algorithm for $R[x]$). Though I know this could be solved for $i = 1$ by dividing by $x-a$: Let $f(x) \in \ker \phi$, divide by $x-a: f(x) = q(x)(x-a) + r, f(a) = r = 0$, so $f(x) = q(x)(x-a) \in (x-a)$.","['abstract-algebra', 'ring-theory', 'polynomials']"
1854603,inverse function theorem and matrix square root,"Define $\mathbf{f}(A) = A^2$, for $A \in \mathbb{R}^{n \times n}$. (a) Applying the Inverse Function Theorem, show that  every matrix $B$ in a neighbourhood of $I$ has (atleast) 2 square roots $A$, that is $A^2 = B$, each varying as a $C^1$ function of $B$. (b) Can you decide if there are precisely 2 or more ? (Hint: in  2x2 case, what is $D\mathbf{f}\left ( \begin{bmatrix} 1 & 0 \\
                                           0  & -1 \end{bmatrix} \right )$ ? Now I can apply the inverse function theorem to $\mathbf{f}(A)$ at $A =I$  to conclude the inverse exists. But I cannot see how to apply it and ``show that  every matrix $B$ in a neighbourhood of $I$ has (atleast) 2 square roots $A$"" is to be solved and the bit about number of square roots. I would appreciate some hints. This is a problem from Ted Shifrin's book on Mutivariable Mathematics.","['multivariable-calculus', 'matrix-calculus', 'inverse-function-theorem']"
1854622,Does there exist a general technique for solving systems of multivariable linear congruences,"I'm aware for coprime moduli we have the CRT for solving the problem $$ \begin{matrix} a_0 x \equiv b_0 \mod m_0 \\ a_1 x \equiv b_1 \mod m_1 \\ \vdots \\ a_n x \equiv b_n \mod m_n  \end{matrix} $$ And if we relax the ""coprime"" condition, we still have a general technique of substitution, at our disposal. But now i'm curious, what about the general problem of finding classes of $x_0 ... x_k$ such that: $$ \begin{matrix} a_{00}x_0 + a_{01} x_1 + ... a_{0k} x_k  \equiv b_0 \mod m_0 \\ a_{10} x_0 + a_{11} x_1 + ... a_{1k} x_k  \equiv b_1 \mod m_1 \\ \vdots \\ a_{n0} x_0+ a_{n1} x_1 + ... a_{nk} x_k  \equiv b_n \mod m_n  \end{matrix} $$ Does there exist a general algorithm for this, akin to substitution from earlier?","['modules', 'number-theory', 'modular-arithmetic', 'linear-algebra', 'elementary-number-theory']"
1854641,Writing domains: $∈$ or $⊆$?,"Usually when we write domains for functions (e.g. $f(x)=x^2$) in set notation, we would write something like this:
$$D=\{x∈ℝ\}$$
This means that all values of x are part of the set of real numbers. However, would it not be more appropriate to write
$$D=\{x⊆ℝ\}$$
or
$$D=\{x⊂ℝ\}$$
Because the set of $x$ is a subset of the set of real numbers? Why do we write the domain the first way rather than the second or third ways?","['algebra-precalculus', 'functions']"
1854642,Can different choices of regulator assign different values to the same divergent series?,"Physicists often assign a finite value to a divergent series $\sum_{n=0}^\infty a_n$ via the following regularization scheme: they find a sequence of analytic functions $f_n(z)$ such that $f_n(0) = a_n$ and $g(z) := \sum_{n=0}^\infty f_n(z)$ converges for $z$ in some open set $U$ (which does not contain 0, or else $\sum_{n=0}^\infty a_n$ would converge), then analytically continue $g(z)$ to $z=0$ and assign $\sum_{n=0}^\infty a_n$ the value $g(0)$.  Does this prescription always yield a unique finite answer, or do there exist two different sets of regularization functions $f_n(z)$ and $h_n(z)$ that agree at $z=0$, such that applying the analytic continuation procedure above to $f_n(z)$ and to $h_n(z)$ yields two different, finite values?","['mathematical-physics', 'divergent-series', 'sequences-and-series', 'regularization']"
1854652,Spectacular failure of Lebesgue differentiation for rectangles,"Let $\mathcal{R}$ be the set of rectangles in the plane and, given $f \in L^1$ let $$
f^*(x) = \sup_{x \in R \in \mathcal{R}} \frac{1}{ \lvert R \rvert} \int_R \lvert \, f  \,\rvert
$$
as defined in this question . You can show that the weak-type inequality fails for this operator, that is, there is no constant $A$ such that
$$
m(\{ f^* > \alpha \}) < \frac{A}{\alpha} \lVert \, f \, \rVert_1
$$
for each integrable $f$. From Stein and Shakarchi's book on real analysis I'd like to show the following, much stronger claim: there is an integrable $f$ such that $$
\limsup_{\text{diam}(R) \to 0} \frac{1}{|\,R\,|} \int_R \lvert \, f \, \rvert = \infty \text{ a.e.}
$$
where diam, of course, is the diameter. According to the book this ""should"" follow from the failure of the weak type inequality, but I don't really know how to do it. I was thinking something like the following, but it failed. It suffices to find and $f$ such that the desired conclusion holds only on a set of positive measure, for we can translate around and get the desired effect. So I wanted to show that if the $\limsup$ expression is finite everywhere, the weak-type inequality holds, for that specific function. That would get me what I want. However I got nowhere with finding a function for which the weak-type inequality fails, sadly. Edit: Whoops, this is actually very hard. In its most general form it relies on a result found on page 441 of Stein's book Harmonic Analysis: Real-Variable Methods, Orthogonality, and Oscillatory Integrals. Hopefully this specific case can be had more easily, though?","['functional-analysis', 'real-analysis', 'lebesgue-integral']"
1854710,Understand a part of the proof the Schroder Bernstein theorem,"This is the Lemma in a book:
Let X be a set and p:℘X→℘X a function which is monotonic, in the sense that if A⊆B⊆X, then p(A)⊆p(B). Then there is a set Z⊆X such that p(Z)=Z This is its Proof in the book:
We set Z =⋃{A⊆X:A⊆p(A)}. Take z∈Z. Then there is a set A⊆X such that z∈A and A⊆p(A). So z∈p(A). Moreover, A⊆Z,****question1**** so p(A)⊆p(Z) by hypothesis. Thus z∈p(Z). We have shown that z⊆p(Z). Again by hypothesis ****question2****, p(Z)⊆p(p(Z)). So p(Z) is one of the sets in the family whose union is Z. But this means that p(Z)⊆Z. So we have Z=p(Z), as claimed My question 1) is why is he so sure that ""Moreover, A⊆Z""? And my question 2) is: the hypothesis says A⊆B⊆X so how is he so sure that p(Z)⊆p(p(Z)) if what he only knows is that z⊆p(Z), but he hasn't proved that p(Z)⊆X? at all if the hypothesis clearly says A⊆B⊆X ? (This lemma has costed me a lot of time trying to understand it, it is part of the proof of schroder Bernstein theorem that says that if there is an injective function from X to Y and an injective function from Y to X then there is a bijective function from X to Y but this is the lemma that I have problems with)",['elementary-set-theory']
1854738,$R[X]/(f)$ separable $\iff \Delta(f) \in R^*$?,"Let $R$ be a commutative ring with $1$. Let $f \in R[X]$ be monic. I have to prove the following:
$$
\text{The discriminant of } f \text{ is invertible}
\quad \quad
\iff
\quad \quad
R[X] / (f) \text{ a is separable algebra over R}
$$
Could you please help me to do so? The definitions I work with For me the discriminant of a polynomial $f$ is understood to be
$$
\Delta(f) \ := \ \prod_{1 \leq i < j \leq n} (\alpha_i - \alpha_j)^2.  
$$
We call an algebra $B$ separable iff the following map is bijective:
$$
\Phi \ : \ B \rightarrow Hom(B,R) \ : \ 
x \ \longmapsto \ \left(y \mapsto Tr(xy)\right)
$$
where the trace of an element $b \in B$ is defined as the trace of the map
$ x \mapsto bx$. My own thoughts We tried to use another fact about separability of algebras
from this source : Let $R$ be a commutative ring with $1$, B an $R$-algebra with basis 
  $\{w_1, w_2, \dots, w_n \}$, then 
  $$
\det(Tr(w_iw_j):1 \leq i,j \leq n ) \text{ is invertible}
\quad \quad
\iff
\quad \quad
\text{ B is a separable algebra over $R$}
$$
  Where $Tr(a)$ is a notation for the trace of the map
  $$
B \ \longrightarrow \ B \ : \ x \ \longmapsto \ ax
$$ Thus it would make sense to try to prove that $\Delta(f) = (Tr(w_iw_j):i,j)$, but we only managed to prove this for polynomials of degree two. In the general case both sides are very hard to calculate Some computational effort Lets write $f(X) = a_0 + a_1X + \dots + a_nX^n$. 
In our case $\{1,X,X^2,\dots, X^{n-1}\}$ is a basis for $A[X]/(f)$.
The determinant we have to calculate looks like if $n=3$:
$$
 \left| \begin{array}
        Tr(1) & Tr(X) & Tr(X^2) \\
        Tr(X) & Tr(X^2) & Tr(X^3) \\
        Tr(X^2) & Tr(X^3) & Tr(X^4) \\
        \end{array}
\right|
$$
and is similar for other $n \in \mathbb{N}$. Below I ""sketched"" what the matrix of a map $f(X) \mapsto X^j f(X)$ looks like. $$ \left[
    \begin{array}{cccc|cccc}
      0& 0& \dots &0 \\
      \vdots& \vdots & & \vdots \\
      0& 0& \dots &0 \\
      \hline
      1 &0 &\dots &0 & \\
      0 &1 &\dots &0 && \\
      \vdots & \vdots &  \\
      0 &0 & \dots &1  &&&&  \\
    \end{array}
\right] $$
The empty parts are hard to spell out. At least we can see this way that for taking the trace we only need te consider the elements on the right.
This is still hard, though. The discriminant isn't easy to calculate either. Could you please help me to solve this? Feel free to use any method you like.","['matrices', 'polynomials', 'algebras']"
1854750,Sorting rows then sorting columns preserves the sorting of rows,"From Peter Winkler's book: Given a matrix, prove that after first sorting each row, then sorting each column, each row remains sorted. For example: starting with $$\begin{bmatrix}
1 & -3 & 2 \\
0 & 1 & -5 \\
4 & -1 & 1
\end{bmatrix}$$ Sorting each row individually and in ascending order gives $$\begin{bmatrix}
-3 & 1 & 2 \\
-5 & 0 & 1 \\
-1 & 1 & 4
\end{bmatrix}$$ Then sorting each column individually in ascending order gives $$\begin{bmatrix}
-5 & 0 & 1 \\
-3 & 1 & 2 \\
-1 & 1 & 4
\end{bmatrix}$$ And notice the rows are still individually sorted, in ascending order. I was trying to find a 'nice'  proof that does not involve messy index comparisons... but I cannot find one!","['matrices', 'sorting', 'puzzle', 'popular-math']"
