question_id,title,body,tags
2131530,Why is it important for a manifold to have a countable basis?,"I would like to understand the reason why we ask, in the definition of a manifold, for the existence of a countable basis. Does anybody have an example of what can go wrong with an uncountable basis? When does the problem arise? Does it arise when we want to differentiate something or does it arise before? Thank You","['manifolds', 'general-topology', 'differential-geometry', 'differential-topology']"
2131548,Geometry puzzle with two nested triangles,"This ""toy"" problem actually came out of my research. How does $\frac{AC}{BC}$ change as we vary $\theta$? More specifically: let me use lowercase letters for distance from $C$. Let $\phi$ be the angle $A_1 C A_2$. I want to understand $\frac{a}{b}$ as a function of $\frac{a_1}{b_1}$ and $\frac{a_2}{b_2}$ as we vary $\theta$ from $0$ to $\phi$. Is it possible to construct an example where
  $$ \frac{a}{b} ~ > ~ \alpha \frac{a_1}{b_1} + (1-\alpha) \frac{a_2}{b_2} \qquad \qquad (*) $$
where the point $A = \alpha A_1 + (1-\alpha)A_2$? In other words, $\alpha = \frac{c}{c+d}$ in the below picture. Here's what I know so far: If $a_1 = a_2$ and $b_1 = b_2$, there is no example satisfying $(*)$, though it's not as obvious as it looks. (Proof: the right-angle triangles $CAA_1$ and $CBB_1$ are similar, so the ratio of $AC$ to $A_1C$ equals that of $BC$ to $B_1C$.) Even if we assume $b_1 = b_2$ or $a_1 = a_2$, I don't know the answer yet. (Two updates: some very rough simulations suggest there is no counterexample, and I think I may be able to sketch a very messy proof. Still looking for an elegant approach to this.)","['puzzle', 'trigonometry', 'geometry']"
2131549,Filtration for canonical version of a stochastic process,"Suppose $X$ is a $\mathbb{R}$-valued stochastic process defined on $(\Omega,\mathscr{G},P)$ such that $X$ has cadlag sample paths $X$ is Markov with respect to its natural filtration $\mathscr{G}_t^X$ $M_t^f := f(X_t)-\int_0^t(Af)(X_s)ds$ is a $\mathscr{G}_t^X$ martingale. Since $X$ has cadlag paths, I want to view it in the canonical set-up $(\mathbb{D}[0,\infty),\mathscr{F},Q)$ where $\mathbb{D}$ is the Skorohod space, $\mathscr{F}$ is the Borel $\sigma$-algebra of $\mathbb{D}$ and $Q$ is the probability measure induced by $P$. The $\sigma$-algebra $\mathscr{F}$ is same as the one generated by the coordinate random variables of $\mathbb{D}$ (Prop III.7.1 in Ethier-Kurtz). To be clear, the coordinate random variables are defined as follows: for any $t\geq 0$, the map $\pi_t:\mathbb{D}\to \mathbb{R}$ given by $\pi_t(\omega)=\omega(t)$ is a coordinate random variable. Let the filtration generated by $\{\pi_t\}_{t\geq 0}$ be denoted by $\mathscr{F}_t$. Note that $\mathscr{F}_t$ is intrinsic to $\mathbb{D}$ and does not depend on what process we are studying on $\mathbb{D}$. My question is this: When we want to view $X$ in the canonical set-up, what is the filtration used on the canonical set-up? Is it obtained by some operation on $\mathscr{G}_t^X$? OR, is it the filtration $\mathscr{F}_t$? These filtrations might not be same because the process $X$ might not be capable of exploring all sample paths of $\mathbb{D}$. Would $Q$ be Markov if the filtration $\mathscr{F}_t$ is used? Would $M_t^f$ remain martingales if $\mathscr{F}_t$ is used? In section IV.3 (page 174) of Ethier-Kurtz, the concept of martingale problem on $\mathbb{D}$ is defined. But it is not clear what is the filtration that is being used.","['stochastic-processes', 'probability-theory', 'stochastic-calculus']"
2131588,What is the shortest digit sequence containing all 4 digits sequences?,"Let's consider all 4 digits sequences: 0000
0001
0002
...
9999 Obviously there is a large sequence containing all of them as substrings (the concatenation of all of them). My question is, what is the length of the smallest possible such sequence (since the minimum is attained)? For 1 digit, the answer is clearly : 0123456789 (or any permutation). For 2 digits numbers, it it can be shortened, since ""001"" contains both ""00"" and ""01"". But I have no clue on how to get to a definitive answer. I spotted this similar question , but I'm not interested in the permutations of the alphabet, but 4-sequences.","['permutations', 'combinatorics']"
2131594,"If locally convex topologies exhibit the same dual spaces, do they exhibit the same continuous linear operators?","Consider the following setting: Let $X, Y$ be vector spaces over the field $\mathbb{K} \in \{\mathbb{R}, \mathbb{C}\}$. Furthermore, let $\tau_1, \tau_2$ be locally convex topologies on $X$ and $\sigma_1, \sigma_2$ locally convex topologies on $Y$ such that their topological duals coincide, i.e. 
  $${X}^{*_{\tau_1}} = X^{*_{\tau_2}}\quad\text{and}\quad {Y}^{*_{\sigma_1}} = Y^{*_{\sigma_2}}. \tag{$\ast$}$$ Under which additional assumptions do we have that their linear continuous operators coincide, i.e.
  $$
\mathfrak{L}_{\tau_1, \sigma_1}(X,Y) = \mathfrak{L}_{\tau_2, \sigma_2}(X,Y) ?
$$ I have a proof for this property once I assume there exista a closed graph theorem for the pairs $(\tau_1, \sigma_1)$ and $(\tau_2, \sigma_2)$. See below for a proof. I suppose that the assumptions can be weakened further if we only assume one pair, e.g. $(\tau_1, \sigma_1)$ to have a closed graph theorem available and the second pair as weak topologies relative to $\tau_1$ and $\sigma_1$. In this case, I'm sure that one can adapt the proof as in this post or that other one. What do you think? My Idea: I think that we can build on the fact that $(\ast)$ gives us that the closure of convex sets coincides, i.e. if $C \subseteq X$ and $K \subset Y$ are convex sets, we have
$$
\overline{C}^{\tau_1} = \overline{C}^{\tau_2} \quad\text{and}\quad \overline{K}^{\sigma_1} = \overline{K}^{\sigma_2}.
$$ Furthermore, we need some closed graph theorem for locally convex spaces. This should give us the first restriction to Fréchet spaces ( can we say more? I was hoping to get a result which would solve this problem , but afaik strong and weak operator topologies are not Fréchet ). Then, I would argue as follows: If $T \in \mathfrak{L}_{\tau_1, \sigma_1}(X,Y)$, then its graph is closed w.r.t.  the product topology $\tau_1 \times \sigma_1$ on $X\times Y$. This is once again a locally convex space and
$$
(X \times Y)^{*_{\tau_1 \times \sigma_1}} = X^{*_{\tau_1}} \times Y^{*_{\sigma_1}} =  X^{*_{\tau_2}} \times Y^{*_{\sigma_2}} = (X \times Y)^{*_{\tau_2 \times \sigma_2}}.
$$
Furthermore, since the graph of a linear operator is a convex set, we have 
$$
\Gamma(T) = \overline{\Gamma(T)}^{*_{\tau_1 \times \sigma_1}} 
= \overline{\Gamma(T)}^{*_{\tau_2 \times \sigma_2}}.
$$
Once again, applying the closed graph theorem but in the other direction gives $T \in \mathfrak{L}_{\tau_2, \sigma_2}(X,Y)$.","['operator-theory', 'functional-analysis', 'general-topology', 'locally-convex-spaces', 'topological-vector-spaces']"
2131598,Did I get these trigonometric functions correct?,"Ok so I came across another question. Once again it says let $\cot\theta= 4/3$ , with $\cos\theta<0$ . Find the remaining trigonometric functions. By using the identities I got: \begin{align}
\sin\theta&=-3/5 \\
\cos\theta&=-4/5 \\
\tan\theta&=3/4  \\
\csc\theta&=-5/3 \\
\sec\theta&=-5/4
\end{align}",['trigonometry']
2131633,"Problems understanding proof of if $x + y = x + z$ then $y = z$ (Baby Rudin, Chapter 1, Proposition 1.14)","I'm having trouble with whether Rudin actually proves what he's tried to prove. Proposition 1.14; (page 6)
The axioms of addition imply the following statements: a) if $x + y = x + z$ then $y = z$ The author's proof is as follows:
$ y = (0 + y) = (x + -x) + y = -x + (x + \textbf{y})$
$$ = -x + (x + \textbf{z}) = (-x + x) + z = (0 + z) = z $$ I emphased the section which troubles me. 
How does Rudin prove that $ y = z $ if he substituted $y = z$?","['real-analysis', 'proof-writing', 'proof-explanation']"
2131639,Question about Cesàro summation,"Consider $$S_n = \sum_{i=0}^n a_i$$
and its CesĂ ro sums, defined as
$$ C = \lim_{n \to \infty} \frac1n\sum_{k=0}^n S_k$$
Is it always true that
$$ C = \lim_{n \to \infty} \frac1{L(n)}\sum_{k= n - L(n)}^n S_k$$
where $L(n)$ is any strictly increasing function such that $ 2 < L(n) < \ln(n)$ for every $n$?","['cesaro-summable', 'real-analysis', 'summation', 'limits']"
2131643,Triangular and Fibonacci numbers: $\sum_{k=0}^{2n}T_{2n-k}\color{red}{F_k^2}=F_{2n}F_{2n+1}-n$,"Well known Fibonacci square series $(1)$ $$0^2+1^2+1^2+2^2+3^2+\cdots F_{n}^2=F_{n}F_{n+1}\tag1$$ $T_n=0,1,3,6,10,..$ and $F_n=0,1,1,2,3,...$ For $n=0,1,2,3,...$ Now we included Triangular numbers into $(1)$ as shown below $$T_0F_0^2=F_1F_2-1$$ $$T_1F_0+T_0F_1=F_2F_3-1$$ $$T_2F_0^2+T_1F_1^2+T_0F_2^2=F_3F_4-2$$ $$T_3F_0^2+T_2F_1^2+T_1F_2^2+T_0F_3^2=F_4F_5-2$$ $$T_4F_0^2+T_3F_1^2+T_2F_2^2+T_1F_3^2+T_0F_4^2=F_5F_6-3$$ $$T_5F_0^2+T_4F_1^2+T_3F_2^2+T_2F_3^2+T_1F_4^2+T_0F_5^2=F_6F_7-3$$ Observing the series involving Triangular and Fibonacci numbers together we found the following closed form. For even terms $$\sum_{k=0}^{2n+1}T_{2n+1-k}\color{red}{F_k^2}=F_{2n+1}F_{2n+2}-n-1\tag2$$ For odd terms $$\sum_{k=0}^{2n}T_{2n-k}\color{red}{F_k^2}=F_{2n}F_{2n+1}-n\tag3$$ How can we prove $(2)$ and $(3)$? An attempt: Knowing that $T_n={n(n+1)\over 2}$ then $(3)$ becomes $${1\over 2}\sum_{k=0}^{2n}(2n-k)(2n-k+1)F_k^2=F_{2n}F_{2n+1}-n$$ Simplified down to $$(4n^2+2n)\sum_{k=0}^{2n}F_k^2+\sum_{k=0}^{2n}(k^2-k-4nk)F_k^2=2F_{2n}F_{2n+1}-2n$$ finally down to $$\sum_{k=0}^{2n}(k^2-k-4nk)F_k^2=(2-2n-4n^2)F_{2n}F_{2n+1}-2n$$ we are not sure what to do next...","['fibonacci-numbers', 'sequences-and-series', 'elementary-number-theory']"
2131653,Proof that convergent sequence in $\Bbb R$ is bounded.,"Convergent sequence in $\Bbb R$ is bounded. Proof: In the definition of a convergent sequence: $$(\forall \varepsilon>0), (\exists n_\varepsilon\in\Bbb N), (\forall n\in\Bbb N), ((n>n_\varepsilon)\Rightarrow(|a_n-a|<\varepsilon))$$ 
  let $\varepsilon=1$, then there exists $n_\varepsilon\in\Bbb N$ such
  that $(n>n_\varepsilon)\Rightarrow(|a_n-a|<1$). Now for
  $n>n_\varepsilon$ we have $|a_n|\leq|a_n-a|+|a|\leq 1+|a|$. Let
  $M=\max\{|a_1|,...,|a_{n_\varepsilon}|,1+|a|\}$ Then $\forall n\in\Bbb
> N, \ |a_n|\leq M$, i.e. the sequence is bounded. What's the idea behind this proof? I understand the first part, but then when they define $M$ I'm lost. I don't see how is that related to the first part of the proof.","['proof-explanation', 'sequences-and-series', 'calculus', 'limits']"
2131666,Limit points of irrationals,"We know that the set of all limit points of $\Bbb Q$ is $\Bbb R$. This means that if $a \in \Bbb Q$,  we can find a rational number as close to $a$ as we want. We also know that between every two rational numbers there exists an irrational number. My question is: Is $\Bbb R$ the set of all limit points of $\Bbb R \setminus \Bbb Q$ (of the irrational numbers)? Can this be concluded only from the above?","['real-analysis', 'irrational-numbers', 'limits']"
2131674,$AD:DC = BE:EA = 1:2$. Lines $BD$ and $CE$ meet at point $O$. Prove that $\angle AOC = 90^{\circ}$,"Points $D$ and $E$ divide sides $AC$ ans $AB$ of an equilateral triengle in the ratio $AD:DC = BE:EA = 1:2$. Lines $BD$ and $CE$ meet at point $O$. Prove that $\angle AOC = 90^{\circ}$ My Work Take point $F$ in $BC$ so that $CF:FB = 1:2$. The line $AF$ intersects line $BD$ and $CE$ at point $Q$ If we draw lines $FF_1 \parallel BD$ and $FF_2 \parallel CE$ then it is easy to see that $AP:PF = 3:4$ and $AQ:QF = 6:1$. So $AP:PQ:QF = 3:3:1$. Now how to continue from here. I think I am almost there. But missing something. If I can prove that $PQ = PQ = OQ$ then I am done, as $P$ is the midpoint of $AQ$. But how to do this. I'd also like to know any over-killing strategy that will simply solve this problem.","['ratio', 'triangles', 'geometry']"
2131681,"On the integral $\int_{e}^{\infty}\frac{t^{1/2}}{\log^{1/2}\left(t\right)}\alpha^{-t/\log\left(t\right)}dt,\,\alpha>1.$","Let $\alpha>1$. I would like to find a closed form or an upper bound of $$f\left(\alpha\right)=\int_{e}^{\infty}\frac{t^{1/2}}{\log^{1/2}\left(t\right)}\alpha^{-t/\log\left(t\right)}dt.$$ For the closed form I'm very skeptical but I have trouble also for an upper bound. I tried, manipulating a bit, to integrate w.r.t. $\alpha$ since $$\frac{\partial}{\partial\alpha}\alpha^{-t/\log\left(t\right)}=-\frac{t}{\alpha\log\left(t\right)}\alpha^{-t/\log\left(t\right)}$$ but it seems quite useless and at this moment I didn't see a good way to proceed. 
Maybe it is interesting to see, using some trivial substitutions, that $$f\left(\alpha\right)=\int_{e}^{\infty}\frac{\left(e^{3/2}\right)^{-W_{-1}\left(-1/v\right)}}{v\left(-W_{-1}\left(-\frac{1}{v}\right)\right){}^{1/2}}\frac{W_{-1}\left(-\frac{1}{v}\right)}{W_{-1}\left(-\frac{1}{v}\right)+1}\alpha^{-v}dv$$ $$=\int_{e}^{\infty}g\left(w\right)\alpha^{-v}dv$$ where $W_{-1}\left(x\right)$ is the Lambert $W$ function. So it seems that $f(\alpha)$ is somehow connected to the Mellin transform of $g(w).$ Thank you.","['real-analysis', 'integration', 'closed-form']"
2131705,State Space of Finite-Dimensional Abelian C*-Algebras,"I'm studying some properties of C*-algebras and I found the following statement. In the case of $\mathbb{C}$ the state space $\mathcal{S}(\mathbb{C})$ is a point and in the case of $\mathbb{C} \bigoplus \mathbb{C}$ the state space $\mathcal{S}(\mathbb{C} \bigoplus \mathbb{C})$ is the interval $[0,1]$. Well, I don't understand how I can get these results. The state space $\mathcal{S}(\mathbb{C})$ is the identity map? I'm using the following definition: a state on a unital $C^*-algebra$, $\mathfrak{A}$, is map $\omega: \mathfrak{A} \to \mathbb{C}$ s.t. $\omega(A)\ge 0$, $\forall A \in \mathfrak{A}^+$ and $\omega(\mathbb{I})=1$ (the map is normalized). In the previous definition we have $\mathfrak{A}^+=\{A \in \mathfrak{A}_{\mathbb{R}}:\sigma(z)\subset\mathbb{R}^+ \}$ with $\mathfrak{A}^+_{\mathbb{R}}$ the set of self-adjoints elements of $\mathfrak{A}$ and $\sigma(A)$ the spectrum of $A$.","['c-star-algebras', 'banach-algebras', 'abstract-algebra', 'functional-analysis', 'von-neumann-algebras']"
2131760,Completion and completeness,"Let $\widehat{M}$ be the completion of a module $M$ via a filtration $\mathcal{F}=\{M_i:i\in\mathcal{I}\}$ where $\mathcal{I}$ is a directed set. I understand the completion as:
$$\varprojlim_{i\in\mathcal{I}} M/M_i$$
as the latter one I think as coherent sequences: $(x_i)_{i\in\mathcal{I}}$ with $\theta_{n+1}(x_{n+1})=x_n$. I was trying to explore why is $\widehat{M}$ topologically complete. A lot of people at this point will be satisfied with saying if $M\simeq \widehat{M}$ then it is complete, but this is just a definition. We can show that $\widehat{M}$ is topogically complete by checking at Cauchy sequences in $\widehat{M}$. My idea: If $((a_{ij}))_{i,j\in\mathcal{I}}$ is a Cauchy sequence of coherent sequences, then I showed the $a_{ij}$ form a direct system for a fixed i, hence one can define:
$$a_i=\varinjlim_{i\in\mathcal{I}}a_{ij}$$
and claim that $$((a_{ij}))_{i,j\in\mathcal{I}}\rightarrow (a_i)_{i\in\mathcal{I}}$$
I'm given the coherent sequences the subspace topology of the product topology om $M/M_i$, i.e., basic open sets look like $\prod U_i\cap \widehat{M}$, where almost all $U_i$ are $M/M_i$ and the others look like $M_\ell/M_i$, where $\ell<i$. Here is where I'm struggling. We want to show that there is $N\in\mathcal{I}$ such that if $n>N$ then $(a_{ij})-(a_i)\in \prod U_i \cap \widehat{M}$. Can anyone show me how to use the Cauchyness to prove that this is in fact the limit? Is this something worth thinking of? Haven't seen it in any book so far.","['algebraic-number-theory', 'algebraic-geometry', 'commutative-algebra']"
2131773,Find the Smallest Set with the Following Property,"Setting Let $W = \{w_1, w_2, \dots, w_n \}$ be some set and denote $2^W$ as its power set. 
For any set $S \subseteq 2^W$, define $F(S)$ as $\{ U \cup U'\mid U\in S, U'\in S \}$. Example) When 
$S=\{\{ \}, \{w_1 \}, \{w_2\},\{ w_1, w_3 \} \},\tag{1}$ we have 
$F(S) = \{\{ \}, \{w_1 \}, \{w_2\},\{ w_1, w_2 \},\{w_1, w_3\}, \{w_1, w_2, w_3\} \},\tag{2}$ where we use the equality $\{w_1\} \cup \{w_1, w_3\} = \{w_1, w_3\}$. Problem For any $n$, find the set $S$ with minimum cardinality such that $F(S) = 2^W$. Any ideas, pointers, references would be appreciated. Example) For $n=3$ the set where we add $\{w_3\}$ to Eq.(1) is the desired set, and for $n=4$, $S = \{\{ \}, \{w_1 \}, \{w_2\},\{ w_3 \},\{w_4\}, \{w_1, w_2\}, \{w_3, w_4\} \}$ is the desired set. So far, I had no luck on finding an explicit formula on how to create a set with the above property. Even for small $n$ such as $n=5$, it turned out to be highly non-trivial. Although I am not quite sure if this is the minimal set, $S = \{\{ \}, \{w_1 \}, \{w_2\},\{ w_3 \},\{w_4\}, \{w_5\},\{w_1, w_2\}, \{w_3, w_4\}, \{w_3, w_5\}, \{w_4, w_5\},  \{w_3, w_4, w_5\}\}$ has the property $F(S) = 2^W$, where $|S|=11$. One simple observation I found so far is that, we need $|S| \ge \Omega(2^{n/2}), \tag{1}$ since $|F(S)| = |2^W|=2^n$ and $|F(S)| \le |S|^2$. For intuition on the latter inequality, think of a 2-by-2 matrix where the columns and rows are indexed by the elements of $S$, and fill each entry of the matrix by taking the union of elements that the column and row index represents. Then, the set of entries of the matrix must $2^W$ by the property of $S$. There are a lot of duplicate entries in the matrix, but this gives us a rough upper bound for $|F(S)|$. Note @Test123 gave a more tighter upper bound of $|F(S)| \le (n^2 + n)/2$.",['combinatorics']
2131774,Alternative Proof to irrationality of $\sqrt{2}$ using linear algebra,"I am taking my first Proof course, and have been researching alternative proofs to the irrationality of $\sqrt{2}$. One that particularly interested me could be found on this site as number $10$ , by D. Kalman. To avoid butchering the format I'm not going to attempt to rewrite it here, but I would love to see some other proofs to this popular theorem using linear algebra in some way, and I couldn't find any others online. If you happen to know another please share your knowledge, and thanks in advance!!","['alternative-proof', 'rationality-testing', 'linear-algebra', 'proof-verification']"
2131803,Why can a matrix without a full rank not be invertible?,"I know you could just say because the $\det = 0$. But during the introduction of determinants the professor said, obviously if two columns of the matrix are linearly dependent the matrix can't be inverted, therefore it is zero. He made it sound like it is an intuitive thing, a simple observation, but I always have to resort to the properties of determinants to show it. How does one trivially see that you can not invert a matrix without a full rank?","['matrices', 'linear-algebra', 'determinant']"
2131807,Why we cannot apply Componendo and Dividendo in this question,Please see the step enclosed in yellow ring in the above image. My question is why we cannot apply Componendo-Dividendo so the integration becomes easy. The correct answer to the question is (x - y)^2 = Cxy × e^(-y/x) but I do not get this answer by applying Componend-Dividendo. Why am I not getting correct answer? Please help. Here is what I did :,"['ratio', 'ordinary-differential-equations']"
2131811,How can I find the entries of the continued fraction of $2^{1/3}$ more efficiently?,"I wanted to detect large entries in the continued fraction of $$2^{1/3}$$ I found two ways to find the continued fraction of an irrational number like $$2^{1/3}$$ in PARI/GP. The most obvious is the contfrac-command which requires a very large precision, if I want to calculate, lets say , $10^7$ entries. If the continued fraction expansion of $2^{1/3}$ is $[a_1,a_2,a_3,\cdots]$, then define $x_k:=[a_k,a_{k+1},a_{k+2},\cdots]$ I had the idea that I can calculate the minimal polynomials of the numbers $x_k$ (See the above definition). I wrote the program ? maxi=0;j=0;f=x^3-2;while(j<10^7,j=j+1;s=0;while(subst(f,x,s)<0,s=s+1);s=s-1;if
(s>maxi,maxi=s;print(j,""  "",s));f=subst(f,x,x+s);f=subst(f,x,1/x)*x^3;if(pollead
(f)<0,f=-f))
1  1
2  3
4  5
10  8
12  14
32  15
36  534
572  7451
1991  12737 which works very well. The output shows the first few successive record entries in the continued fraction expansion. The program does not need floating-point calculations. It only calculates the new minimal polynomial and determines the truncated part of the (unique) positive root. But the coefficients of the minimal polynomial get soon extremely large, hence the program needs already much longer for the first $2\cdot 10^4$ entries than the cont-frac-command. Does anyone know a faster method with which I can calculate $10^7$ or $10^8$ entries in a reasonable time ? The contfrac-command requires much memory , so a way to avoid large arrays would be very appreciated.","['number-theory', 'continued-fractions', 'math-software']"
2131819,Solve: $y''+y=cos3x$,"$$y''+y=cos3x$$ $$\lambda^2+\lambda=0\Rightarrow \lambda_{1,2}=\pm i$$ $$y_{h}=c_{1}cosx+c_{2}sinx$$ Because the homogeneous solutions is $ c_{1}cosx   $ and not $c_{1}cos3x$ we can the particular solution to be: $$y_{p}=Acos3x+Bsin3x$$ Substituting the particular solution to the ODE: $$y'_{p}=-3Asin3x+3Bcos3x$$ $$y''_{p}=-9Acos3x-9Bsin3x$$ $$-9Acos3x-9Bsin3x+Acos3x+Bsin3x=cos3x$$ $$-8Acos3x-8Bsin3x=cos3x$$ Is it correct that there is no sin3x on the RHS of the equation? usually it is because the particular solution was incorrect",['ordinary-differential-equations']
2131823,Prove that $f(7) = 56$ given $f(1)$ and $f(9)$ and $f' \le 6$ [duplicate],"This question already has answers here : Let $f(x)$ be differentiable at $\mathbb R$, s.t $|f^\prime (x)| \le 6$ . Its given that $f(1)=20$, and $f(9)=68$ , prove that $f(7)=56$. (2 answers) Closed 7 years ago . Let $f(x)$ be continues function at $[1,9]$ and differentiable at $(1,9)$ and also $f(1) = 20 , f(9) = 68 $ and $ |f'(x)| \le 6$ for every $x \in (1,9)$. I need to prove that $f(7) = 56$. I started by using the Lagrange theorem and found that there exist $ 1<c<9$ such that $f'(c) = 6$ but I'm not sure how is this relevant and how to proceed.","['derivatives', 'calculus', 'limits']"
2131850,Prove that every non-constant complex power series with infinite radius of convergence is unbounded,"I know and can prove that any non-constant polynomials are unbounded, and then I was wondering whether this is true for power series (like infinite degree polynomials). For expamle this is true for the exponential function (when expressed as a power series). After some research, I know that this is true by Liouville's theorem via the Cauchy's integral formula. But is there any simpler way of proving this result?","['complex-analysis', 'analysis']"
2131873,Evaluate a limit involving a definite integral,"Let $(I_n)_{n \geq 1}$ be a sequence such that: 
$$I_n = \int_0^1 \frac{x^n}{4x + 5} dx$$ 
Evaluate the following limit:
$$\lim_{n \to \infty} nI_n$$ All I've been able to find is that $(I_n)$ is decreasing and converges to $0$. Thank you!","['definite-integrals', 'sequences-and-series', 'limits']"
2131911,Are there more than $\beth_1$ non-homeomorphic topological subspaces of $\Bbb R$?,"I've been asked by a younger student about a certain claim he had on a classification of topological subsets of $\Bbb R$. The overall idea was a bit fuzzy, but in hindsight it revolved around taking the $\sigma$-algebra generated by six (Borel) subsets + translations. I successfully (and, I hope, instructively) argumented against it. However, this led me to the question: Could I just cut it short and fancy with a cardinality argument? Specifically, if $\sim$ is the homeomorphism equivalence on $\mathcal P(\Bbb R)$, is $\operatorname{card}\left(\mathcal P(\Bbb R)/\sim\right)>\beth_1$ ? Intuitively, I'd say yes, because, ""come on, there are $\beth_2$ nasty non-Borel sets"". And, ""at chit-chat level, homeomorphisms $(a,b)\to(c,d)$ are monotone functions"". However, this is neither a proof nor a sufficient reason for my question to even be decidable in ZFC. In fact, on the topic I found this weaker fact : ""closed subsets up to homeomorphism are exactly $\beth_1$"". Thank you for links and/or answers.","['general-topology', 'set-theory']"
2131973,Second-order linear differential equation of the form $x^2 y'' + (ax-b)y' - ay =0$,"I need to solve the following differential equation
\begin{equation}
x^2 y'' + (ax-b)y' - ay =0
\end{equation}
with $a,b>0$, $x\geq 0$ and $y(0)=0$. The power series method will fail since there is a singularity at $x=0$, while the form of the equation does not conform with the Frobenius method. What other methods can I try in order to solve this?","['frobenius-method', 'ordinary-differential-equations', 'power-series']"
2132010,"If there is a bounded linear extension $W^{k,\:p}(Λ)\to W^{k,\:p}(ℝ^d)$, are we able to conclude the existence of such an extension for other $k,p$?","Let $d\in\mathbb N$ $\Lambda\subseteq\mathbb R^d$ be open $k\in\mathbb N$ $p\in[1,\infty)$ We say that $\Lambda$ has the $(k,p)$-extension property $:\Leftrightarrow$ There is a bounded linear operator $W^{k,\:p}(\Lambda)\to W^{k,\:p}(\mathbb R^d)$ with $$\left.Eu\right|_\Lambda=u\;\;\;\text{for all }u\in W^{k,\:p}(\Lambda)\;.\tag1$$ My question is: If $\Lambda$ has the $(k,p)$-extension property, are we able to conclude that $\Lambda$ has the $(k',p')$-extension property for some $k'\in\mathbb N$ and $p'\in[1,\infty)$? In particular, if $\Lambda$ has the $(2,2)$-extension property, does it have the $(1,2)$-extension property too?","['functional-analysis', 'sobolev-spaces']"
2132015,Evaluation of given indefinite integral,Evaluate the given integral $$\int e^x \bigg[\frac{2-x^2}{(1-x)\sqrt{1-x^2}} \bigg]dx$$ I was trying to convert it to $\int e^x (f(x)+f'(x))dx=e^x \cdot f(x)+C$ but did not succeed in algebraic manipulations. Could someone hint me to something so that I could proceed?,"['indefinite-integrals', 'integration', 'calculus']"
2132046,$M$ is orientable $\Leftrightarrow$ determinant bundle $\det(TM)$ is trivial,"Let $M$ be a differentiable manifold and $TM$ be its tangent bundle. I need to prove the following: $M$ is orientable if and only if $\det(TM)$ is trivial. The definition of determinant bundle I'm using is the following: Given a vector bundle $E$ over $M$ with transition functions $g_{\alpha\beta}$, then the determinant vector bundle $\det(TM)$ over $M$ is the line vector bundle whose transition functions are $\det(g_{\alpha\beta})$. I get the orientability and $\det(TM)$ are closely related since the transition functions of $\det(TM)$ are just the determinant of the Jacobian matrix of the change of chart for an atlas of $M$.",['differential-geometry']
2132047,How to find the tangent lines to an algebraic curve of a singular point?,"I think the question is self-explanatory, but I'd like to put an example of what I am referring to. Let $C = V(f)$ be the curve defined by the polynomial
$$f= -X^3 +4X^2Y-3XY^2+Y^3-2XY+Y$$
and 
$$F = -X_1^3+4X_1^2X_2-3X_1X_2^2+X_2^3-2X_0X_1X_2+X_0^2X_2$$
its projective completion. 
The point $(1,1) \sim (1:1:1)$ is singular to $C$, and I'd like to find the tangent line to that point. I think I should find the tangent cone of $f$, that is, the homogeneous terms of less degree, which in this case would be $$V(X^2)$$
so $C$ would have a double tangent line on $(1,1)$, so the point $(1,1)$ would be a cuspid. The thing that is bugging me off is that in almost all of the examples I've seen, the singular point is the origin $(0,0)\sim(1:0:0)$, so I'm not sure if I should just ignore the fact that the point is not the origin, or I should find the less degree terms of $f(X-1,Y-1)$ instead of $f(X,Y)$. Furthermore, what happens when there are more than one singular point? For example, let $D=V(G)$ where
$$G=X_0^2X_1^2-X_0^2X_2^2+X_1^2X_2^2$$
The points $(1:0:0), (0:1:0)$ and $(0:0:1)$ are all singular to $D$. What would be the tangent lines to those three points? Thank you in advance. EDIT I've just realized that my second example doesn't make any sense, since that is the case where we only have one singular point $(0,0)$ on the affine space, while the other two lie in the infinity line $X_0 = 0$. I'm not sure about how to interpret that, but looking at the plot of the curve: I guess those two points are singular because that's where the branches of the curve meet (that's roughly a wild guessing, so any help clarifying that would also be helpful).","['algebraic-curves', 'tangent-line', 'algebraic-geometry']"
2132091,$n$-dimensional holes,"I am confused by the terminology concerning $n$ -dimensional holes in algebraic topology. A circle is said to have a one-dimensional hole, and a sphere a two-dimensional hole for example. However I cannot see why the circle should be described to have a one-dimensional hole $-$ surely if drawn in two dimensions the 'gap' left in the middle of the circle is two-dimensional? If we think of the circle as a one-dimensional space only, then there is nowhere to have a 'hole' in the space?","['homology-cohomology', 'terminology', 'algebraic-topology', 'general-topology', 'definition']"
2132099,First Order ODE with Delta through Integrating factors,"I'm trying to solve a differential equation similar to the one shown as a first solution here: Inverse Fourier transform of $ \frac{1}{a+jw} $ The DE is the following: $y' + 2y = \delta(x)$ I'm trying to solve without using Laplace but got stuck here: $y=e^{-2x}\int^{}e^{2x}\delta(x)dx$ There should be some way to prove that $\int^{}e^{2x}\delta(x) = H(x)$ but I just can't see it. As far as I know $\int^{}e^{2x}\delta(x) = 1$, so there should be a way to shift the delta inside the integral as $\delta(x-a)$ when $a \neq 0$ so $H(x)$ makes sense.","['dirac-delta', 'integration', 'ordinary-differential-equations']"
2132133,Understanding the definition of limit superior,"I know there were many questions here on this topic, but I didn't find an answer that cleared things up to me. The thing is, I have the following definition of limit superior and I didn't see it used very often: A number $L\in\Bbb R$ is the limit superior of a sequence $(a_n)_n$ if and only if: $1.$ $\forall \varepsilon>0,  \ a_n<L+\varepsilon$ for almost all terms of the sequence $2.$ $\forall \varepsilon>0, \ L-\varepsilon<a_n$ for infinitely many terms of the sequence I can see this is very similar to the definition of supremum that I have: If $S$ is a non-empty bounded set $S \subseteq \Bbb R$ then $L=\sup S$ if and only if: $1. \forall x\in S, x\leq L$ $2. \forall \varepsilon>0, \exists x\in S, \ L-\varepsilon<x$ In the defintion of limit superior I don't understand why $a_n<L+\varepsilon$ and what does it mean for ""almost all terms of sequence""? Couldn't we put $a_n<L, \ \forall n\in\Bbb N$ for the first condition, and for the second condition: $\exists n\in\Bbb N$ such that $L-\varepsilon<a_n$? Why does that have to be true for  ""infinitely many terms of sequence""? Can someone explain this definition using an example? Thank you!","['limits', 'limsup-and-liminf', 'calculus', 'sequences-and-series', 'definition']"
2132150,Calculating the curvature of a surface,"From https://en.wikipedia.org/wiki/Curvature#Local_expressions : For a plane curve given parametrically in Cartesian coordinates as $f(t)=(x(t), y(t))$, the curvature is $$\kappa = \frac{|x'y''-y'x''|}{\left(x'^2+y'^2\right)^\frac32}$$ What is the general expression for a 3 or higher dimensional surface, e.g. the curvature of $f(t) = (x(t), y(t), z(t))$?","['analytic-geometry', 'differential-geometry', 'curvature', 'geometry']"
2132160,Hitting time of an open set is not a stopping time for Brownian Motion,"Let $(B_t)$ be a standard Brownian motion and $\mathcal F_t$ the associated canonical filtration. It's a standard result that the hitting time for a closed set is a stopping time for $\mathcal F_t$ and the hitting time for an open set is a stopping time for $\mathcal F_{t+}$. Is there an elementary way to see that the hitting time for an open set is not in general a stopping time for $\mathcal F_t$? Say the hitting time for an open interval $(a,b)$? I'm interested in this question because it would show the filtration generated by a right-continuous process need not be right-continuous. There are other counterexamples for this on M.SE but they're all somewhat artificial. My apologies if this is obvious. I just started learning about such things.","['stochastic-processes', 'probability-theory', 'brownian-motion', 'stopping-times']"
2132215,A real function which is additive but not homogenous,"From the theory of linear mappings, we know linear maps over a vector space  satisfy two properties: Additivity : $$f(v+w)=f(v)+f(w)$$ Homogeneity : $$f(\alpha v)=\alpha f(v)$$ which $\alpha\in \mathbb{F}$ is a scalar in the field which the vector space is defined on, and neither of these conditions implies the other one. If $f$ is defined over the complex numbers, $f:\mathbb{C}\longrightarrow \mathbb{C}$, then finding a mapping which is additive but not homogenous is simple; for example, $f(c)=c^*$. But can any one present an example on the reals, $f:\mathbb{R}\longrightarrow \mathbb{R}$, which is additive but not homogenous?","['functional-equations', 'linear-algebra', 'functions', 'linear-transformations']"
2132224,Derivatives of exp(f(x)) and partitions of an set,"I am trying to express the $k$-th derivative of $g(x) = exp(f(x))$ in a meaningful way. My logic got me so far.
For the first derivative, the result would be
$$
g' = f' g
$$
Second derivative would be
$$
g'' = (f'' + f'^2) g
$$
Third derivative is
$$
g''' = f''' + 3 f'' f' + f'^3
$$
and so on. The correspondence that I made was that each derivative can be expressed as
$$
f ''' \equiv (0,0,3)
$$
$$
f'f'' = (0,1,2)
$$
and
$$
f'^3 \equiv (1, 1, 1).
$$
This is identical to representing the partitions of the integer 3. 
This is similar to the partitions of a set; a set of size 3, $\left\{a,b,c\right\}$, can be partitioned into
$$
\left\{a,b,c\right\} \equiv(0,0,3)
$$
$$
\left\{a\right\} \left\{b,c\right\}, \left\{b\right\} \left\{a,c\right\}, \left\{c\right\} \left\{a,b\right\} \equiv(0,1,2)
$$
$$
\left\{a\right\} \left\{b\right\}  \left\{c\right\} \equiv(1,1,1).
$$
The coefficient, for example, of $f'f''$, is the number of partitions corresponding to $(0,1,2)$. Then, I know that these coefficients can be calculated with the help of multinomial coefficients . My question is: is there a proof of (or a reference about) this correspondence between the derivatives of $g$ and counting the number of partitions?","['derivatives', 'combinatorics', 'multinomial-coefficients', 'integer-partitions']"
2132238,Proving an integral is finite,"I have the following integral: $$\displaystyle \int_{\mathbb{R}^2} \left( \int_{\mathbb{R}^2} \frac{J_{1}(|\alpha|)J_{1}(|k- \alpha|)}{|\alpha||k-\alpha|} \ \mathrm{d}\alpha \right)^2 \ \mathrm{d}k,$$ where both $\alpha$ and $k$ are vectors in $\mathbb{R}^2$, with $k \neq 0$, and $J_{\nu}$ denotes the Bessel function of the first kind. I'm having some trouble with the best way to approach this integral. If we focus on the inner integral first, then using the fact that for sufficiently large, positive $z$ we have $|J_{\nu}(z)| \leqslant C|z|^{-1/2},$ then the inner integral can be reduced to $$\displaystyle \int_{\mathbb{R}^2} |\alpha|^{-3/2}|k-\alpha|^{-3/2} \ \mathrm{d}\alpha.$$ However, as can be seen in this answer , this integral is $O(|k|^{-1})$, which, after squaring, is clearly not integrable over all $|k| \geqslant 1$ after switching to polar co-ordinates (obviously not including $0$ in the lower limit of the outer integral). We would need an estimate of at least $O(|k|^{-1 - \epsilon})$ for any $\epsilon > 0$ to guarantee convergence of the outer integral. One idea might be to try to bring the outer integral inside (though one would need to justify interchanging the order of integration). Using the asymptotics for the Bessel functions gives a product of cosines, and then one can use polar co-ordinates (taking $r = |\alpha|$). This would cancel out the $|\alpha|$ in the denominator, but then the $|k-\alpha|$ terms get very messy, which seems to make things worse. The Bessel functions appear to cause the most trouble. Does anyone have any ideas on how to proceed? One idea is to notice (as someone suggested in the comments) that the above is the $L^2$ norm of a convolution, and also that (up to a constant) we have $$\displaystyle f(\xi) = \frac{J_{d/2}(|\xi|)}{|\xi|^{d/2}} = \mathcal{F}(\chi)(\xi),$$ where $\chi$ is the characteristic function of the unit ball in $\mathbb{R}^d$. It can be shown that $f \in L^2(\mathbb{R}^d)$ and even $L^{p}(\mathbb{R}^d)$ for any $p \geqslant 2$. This lets us write the entire expression as $$\|f \ast f\|_2^2 = \|\mathcal{F}(\chi) \ast \mathcal{F}(\chi)\|_2^2,$$ but unfortunately there is no kind of convolution theorem for functions on $L^2(\mathbb{R}^d)$ that I am aware of, without going into the theory of distributions. Moreover, $f$ does not even belong to $S(\mathbb{R}^d)$ for any $d$, so we cannot say much about the convolution. Thus, the problem is equivalent to asserting the finiteness of the above norm, $\|f \ast f\|_2$. If anyone has any ideas on how else the problem could be approached, then I would be very keen to hear about them.","['bessel-functions', 'real-analysis', 'asymptotics', 'integration', 'estimation']"
2132243,Can I absorb the sign into the constant of a solution to the homogeneous equation in a second order ode?,"I want to solve $$y''+\frac{x}{1-x}y'-\frac{1}{1-x}y=(x-1)e^{x}$$ using variation of parameters. Now, I could guess a solution to the inhomogeneous, i.e. $y_1 = x$. Then I used the formula of reduction of order $$y_2 = x\int\frac{e^{-\int \frac{x}{1-x}dx}}{x^2}dx =x\int \frac{e^{x+\ln(1-x)}}{x^2} = x\int\frac{(1-x)e^x}{x^2} = x\left[\int \frac{e^{x}}{x^2}-\int\frac{e^{x}}{x}\right] = x\left[-x^{-1}e^x+\int\frac{e^{x}}{x}-\int\frac{e^{x}}{x}  \right] = -e^{x}$$ to find $y_2  =-e^x$. However Mathematica says e solution is $y_2 = e^x$ so my question is: Can I absorb the sign into the constant and take $y_2  =e^x$ ?","['reduction-of-order-ode', 'integration', 'ordinary-differential-equations']"
2132290,Questions on near-misses in $1/\sum_{i=2}^n\zeta(i)$,"Would anybody have a proof (or at least a sketch) of the following approximation? Considering $1/\sum_{i=2}^n\zeta(i)$ for $n=25,50,100$ we get: $1/\sum_{i=2}^{25}\zeta(i)=0.04000000004768466\dots$ $1/\sum_{i=2}^{50}\zeta(i)=0.020000000000000000355271\dots$ $1/\sum_{i=2}^{100}\zeta(i)=0.0100000000000000000000000000000000788860905221\dots$ so that, if I got this right, we could guess (at least for $n\in 25\mathbb{N^*}$) the approximation $1/\sum_{i=2}^n\zeta(i)=\frac{1}{n}+10^{-2-8\left \lfloor{\frac{n}{25}}\right \rfloor}u_n$ where $u_n\in[0,1)$.","['number-theory', 'riemann-zeta']"
2132308,How can I prove that the set of natural numbers $\mathbb N$ is infinite?,"I had the following idea but it doesn't look like a proper proof: Let's say $\mathbb N$ is finite. Now consider an arbitrary number $k$ in $\mathbb N$ such that it is the biggest number in the set $\mathbb N$. Now let's consider the number $k+1$. $k+1$ is in $\mathbb N$ and is greater than $k$. Hence we can conclude that $\mathbb N$ has no greatest element because I can always add $1$, and thus is infinite. EDIT 1: I just want to prove it using basic set theory , without using cardinality or other complicated stuff. EDIT 2:  I want to know if this is a formal proof that proves that the natural number set is infinite, supposed we do not know anything about the natural number. In other words i want to know if this can work as stand alone proof. EDIT 3: I can use the following : If the size of the natural numbers set $\mathbb{N}$ is less than or equal to the size of a set $F$ then $F$ is infinite. $\mathbb{N}$ is less than or equal to $F$ iff there is an injective function from $\mathbb{N}$ to $F$",['elementary-set-theory']
2132334,Confidence Interval and Variance of Coefficient of Variation,"I'm currently struggling to find the confidence interval of a statistic. I'm calculating the coefficient of variation for a specific sample. The coefficient of variation is $$\frac{\sigma}{\mu}$$ I would like to construct a normal confidence interval around the estimator. I'm stuck trying to get the variance of my estimator. $$ \operatorname{Var}\left[\frac{\sigma}{\mu}\right] = \operatorname{Var}\left[\frac{\sum(x_i-\bar{x})^2}{\sum(x_i)}\right] $$ I start expanding it and a lot of it resolves by itself. However, I'm very confused about the following quantities: $$ \operatorname{Var}\left[\frac{\sum(x_i)^2}{\sum(x_i)}\right] $$ and $$ \operatorname{Cov}\left[\frac{\sum(x_i)^2}{\sum(x_i)},X\right] $$ Can anyone lend me a hand here?","['statistics', 'variance', 'confidence-interval']"
2132348,Sufficient conditions for separately measurable functions being jointly measurable.,"Let $(X, \Sigma_X)$, $(Y, \Sigma_Y)$ and $(Z, \Sigma_Z)$ be measurable spaces and consider a mapping $f : X \times Y \to Z$. The following sufficient condition for the measurability of the sections is well-known: Suppose $f : X \times Y \to Z$ is $(\Sigma_X \otimes \Sigma_Y, \Sigma_Z)$-measurable. Then the sections $$f^x : (Y, \Sigma_Y) \to (Z, \Sigma_Z)$$ and $$f^y : (X, \Sigma_X) \to (Z, \Sigma_Z)$$ are $(\Sigma_Y, \Sigma_Z)$-measurable for each $x \in X$ respectively $(\Sigma_X, \Sigma_Z)$-measurable for each $y \in Y$. The converse of the above result does not hold in general. However, I was curious whether there are conditions on the measurable spaces such that the converse implication does hold. Any comment or reference is highly appreciated.","['measurable-functions', 'measure-theory']"
2132395,Prove that three medians divide a triangle into 6 triangles with equal surface area.,"I have attempted to solve this task: I have drawn the medians and described the lengths of the segments.
I also noticed that triangles ADC and BCD share the same height, thus $S_{total}= xh$ Whereas the surface of triangles ACD and BCD $S= xh/2$ And so I have already made it so far as to get the halves. But I haven't got a faintest idea how to get to the sixths. I would be most grateful if you gave your answer in simple terms. Geometry is my Achilles's heel.","['triangles', 'median', 'geometry']"
2132401,Proving the inequality $(a^2-ab+b^2)(c^2-ac+a^2)(b^2-bc+c^2) \le 12$.,"This is a follow up question to my previous post ""Inequalities of expressions completely symmetric in their variables"" . An answer provided a counterexample to me reasoning: under the constraints $a,b,c\in\Bbb{R}^+$ and $a+b+c=3$, $$
(a^2-ab+b^2)(c^2-ac+a^2)(b^2-bc+c^2) \le 12.
$$ I demanded a proof for this inequality, however since it was an entirely different question, I felt the need for a new post.","['algebra-precalculus', 'inequality', 'a.m.-g.m.-inequality', 'symmetric-polynomials']"
2132412,Is there an explicit formula that gives the value of $\sqrt{2+\sqrt{2+\sqrt{2+\cdots}}}$ for $n$ square roots? [duplicate],"This question already has answers here : Proof of an equality involving cosine $\sqrt{2 + \sqrt{2 + \cdots + \sqrt{2 + \sqrt{2}}}}\ =\ 2\cos (\pi/2^{n+1})$ (2 answers) Closed 6 years ago . $$\sqrt{2+\sqrt{2+\sqrt{2+\cdots}}}$$ I know that with infinite square roots it's $x = \sqrt{2 + x}$, but what about a non-infinite number of roots? I've searched around a lot for this, and can't find anything useful, nor can I make a dent in the problem myself. Maybe I'm searching using the wrong vocabulary?","['substitution', 'trigonometry', 'calculus', 'algebra-precalculus', 'nested-radicals']"
2132432,"Let $\mathrm{Spec}\,A$ be an open subset of $\mathrm{Spec}\, B$. Must $\mathrm{Spec}\, A$ be a distinguished open?","Suppose we have a ring $A$, and an open subset $U\subseteq\mathrm{Spec}\, A$ such that $(U,\mathcal{O}_{\mathrm{Spec}\,A}\vert_U)$ is affine. Must $U=D(f)$ for some $f\in A$? The contrary is true, by a well-known theorem in basic algebraic geometry.","['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
2132449,Calculating $\int \sqrt{1 + x^{-2}}dx$,"I would like to find $$\int \sqrt{1 + x^{-2}}dx$$ I have found that it is equivalent to
$$
\int \frac{\sqrt{1 + x^2}}{x}dx
$$
but I am not sure what to do about it. With trig substitution $x = \tan(x)$ I get
$$
\int \frac{1}{\sin(\theta)\cos^2(\theta)}d\theta
$$
but that seems to be a dead end.","['integration', 'trigonometric-integrals', 'calculus']"
2132466,How can we show that :- $\sum_{n=1}^{\infty}\left({\phi^2+\gamma\over n+1}-{\phi+\gamma\over n}-\ln{n+1\over n}\right)=-\phi^2$,"How can we prove $(1)$? $$\sum_{n=1}^{\infty}\left({\phi^2+\gamma\over n+1}-{\phi+\gamma\over n}-\ln{n+1\over n}\right)=-\phi^2\tag1$$ $\phi$;Golden ratio $\gamma$;Euler's constant an attempt: Using $$\sum_{n=1}^{\infty}\left({1\over n}-\ln{n+1\over n}\right)=\gamma\tag2$$ $(2)-(1)$ $$(\gamma+\phi^2)\sum_{n=1}^{\infty}\left({1\over n}-{1\over n+1}\right)=\gamma+\phi^2\tag3$$ Telescope sum
$$\sum_{n=1}^{\infty}\left({1\over n}-{1\over n+1}\right)=1\tag4$$ This is not a proof. Any help.","['golden-ratio', 'sequences-and-series', 'euler-mascheroni-constant']"
2132468,Triple Integral $\iiint x^{2n}+y^{2n}+z^{2n}dV$,"Evaluate:
$$\iiint_{x^2+y^2+z^2 \leqslant 1} x^{2n}+y^{2n}+z^{2n} dV $$ I have tried to convert to spherical polars and then compute the integral, but it gets really messy because of the 2n power. Any tips?","['multivariable-calculus', 'integration', 'definite-integrals', 'calculus']"
2132471,Can someone confirm my method and answer for this trig problem?,Find the exact value of: $$\cos 1^{\circ}+\cos 2^{\circ}+\cos 3^{\circ}+ \ldots +\cos 358^{\circ}+\cos 359^{\circ}$$ I got $0$ as I did this by assigning either a positive or negative $x$ variable for each quadrant. Is this method valid and my answer right?,['trigonometry']
2132474,Minimum of two measures is no measure,"Let $(X,\mathcal A, \mu ), (X, \mathcal A, \nu)$ be two measure spaces.
  Show $\lambda(A)=\min(\mu(A), \nu(A))$ is in general no measure on $(X, \mathcal A)$ It may be an easy question but I am really going crazy as I can't find a counterexample. I tried combinations of the trivial measure and counting measure but never got the desired results. Please release me from this pain.",['measure-theory']
2132484,Why this algorithm for egyptian fractions doesn't terminate in ~$2$% cases?,"I thought up yet another algorithm for egyptian fraction expansion which turned out to be very effective (in terms of the length and the denominator size) - in most cases. However, for some fractions it doesn't terminate at all - it leads to an infinite loop. Here is the algorithm: Let $\frac{p}{q}<1$ and $p,q$ coprime. Find the minimal $m$ such that $q/(p+m)$ is an integer. We only need to consider $m \in [1,q-p]$. Represent the fraction as: $$\frac{p}{q}=\frac{p+m}{q}-\frac{m}{q}$$ Now to obtain a positive term instead of a negative one, we split the first fraction in two:
$$\frac{p+m}{q}-\frac{m}{q}=\frac{p+m}{2q}+\frac{p+m}{2q}-\frac{m}{q}=\frac{p+m}{2q}+\frac{p-m}{2q}$$ Here is a conditional: if $p<m$, then repeat the previous step (add $\frac{p+m}{4q}$ to the negative fraction). And so on, until both terms are positive. If $p>m$ then $\frac{p}{q} \to \frac{p-m}{2q}$ and we repeat the first step of the algorithm. The working name is complementary method, so I will use CM to denote it for now. Despite its simplicity (it's not at all obvious why we are dividing by $2$ instead of using some other way to expand the first term) the algorithm works very well. In a lot of cases it beats every other algorithm I tried. Since the greedy algorithm and Engel expansion are usually bad in terms of denominator size, I used two other methods to compare: Binary Remainder Method and my own 'Splitting-Joining method' (the details can be found in my Mathematica SE question . I also compared it to a modification of Engel proposed by Daniel Fischer in this answer and CM is mostly better for the examples he provided. Some examples of the best results (a sequence of denominators is provided in each case): 4/49: CM {14,98}; BR {16,98,196,392,784}; SJ {13,325,925,1813} 3/35: CM {14,70}; BR {16,70,140,560}; SJ {20,28} 47/104: CM {4,8,13}; BR {4,8,16,104,208}; SJ {4, 14, 26, 28, 52, 70, 104, 130, 182} Some examples of the worst results (but still valid - algorithm terminates): 94/191: CM {4, 8, 16, 32, 64, 256, 512, 1024, 2048, 4096, 8192, 24448, 48896, 97792, 195584, 391168, 782336, 1564672} 65/157: CM {4, 8, 32, 256, 512, 1024, 2048, 4096, 10048, 20096, 40192, 80384, 160768, 643072} 52/139: CM {4, 16, 32, 64, 128, 278, 556, 1112, 2224, 8896, 17792} However, in these cases both BR and SJ methods also give long expansions with large denominators. Now the real problem which I'm trying to solve - why in some cases the algorithm doesn't terminate, but leads to loops ? From large scale experiments I estimates the proportion of such fractions to be about $1.8$% (for numerators and denominators below $1000$). The examples of such 'bad' fractions are: $$\frac{41}{111},\frac{5}{87},\frac{8}{87},\frac{14}{87},\frac{47}{87},\frac{61}{102},\frac{17}{69},\frac{33}{119},\frac{38}{93},\frac{77}{177},\frac{32}{57},\frac{99}{185},\frac{98}{141},\frac{100}{129},\dots$$ The most common denominator is $87$ for some reason. Note that not all of the denominators and/or numerators are prime. The problem can be solved by using $\frac{p-1}{q}$ instead, but not in every case, for example $7/87$ doesn't work either. However, both $6/87$ and $2/87$ work, and give different denominators, so we can expand $8/87$ after all. I think the problem might be related to the use of the expansion $1=1/2+1/2$ to divide the fist term. However, when I tried some other schemes, I didn't get good results (for example, I've got repeating fractions when using $1=1/3+2/3$). The working Mathematica code for the algorithm is: x=6/87;
p0=Numerator[x];
q0=Denominator[x];
S=0;
Nm=100;
a=Table[1,{k,1,Nm}];
m=Table[1,{k,1,Nm}];
p1=p0;
q1=q0;
j=1;
While[Abs[p0]>1&&j<=Nm&&q0<10^35,M=Catch[Do[If[FractionalPart[q0/(p0+k)]<1/10^55,Throw[k]],{k,0,q0-p0}]];
m[[j]]=M;
a[[j]]=(p0+M)/(2 q0);
p1=Numerator[a[[j]]-M/q0];
q1=Denominator[a[[j]]-M/q0];
While[p1<0,a[[j]]=a[[j]]/2;
p1=Numerator[a[[j]]+p1/q1];
q1=Denominator[a[[j]]+p1/q1]];
If[a[[j]]!=1,S+=a[[j]]];
j++;
p0=p1;q0=q1];
a[[j]]=p1/q1;
S+=a[[j]];
Denominator[Table[a[[k]],{k,1,j}]] And the second question: how to modify the algorithm so it always terminates? Update: Among the first $10000$ fractions with $p \neq 1$ in lexicographic order there are $269$ fractions for which this algorithm doesn't work. (Seems to be more than $2$%). They are: 5/33,5/51,2/55,32/55,4/57,7/57,13/57,23/57,32/57,6/65,43/66,4/69,8/69,11/69,17/69,40/69,50/69,8/85,59/85,4/87,5/87,7/87,8/87,14/87,34/87,47/87,62/87,65/87,76/87,5/93,7/93,10/93,19/93,38/93,50/93,67/93,6/95,8/95,63/95,61/102,9/110,59/110,4/111,7/111,8/111,13/111,16/111,22/111,25/111,31/111,41/111,44/111,59/111,62/111,68/111,82/111,7/114,65/114,71/114,83/114,103/114,6/115,11/115,17/115,63/115,3/119,5/119,10/119,15/119,16/119,33/119,37/119,45/119,61/119,66/119,67/119,71/119,73/119,78/119,96/119,101/119,4/123,5/123,8/123,10/123,11/123,17/123,20/123,26/123,29/123,35/123,46/123,49/123,67/123,70/123,76/123,86/123,92/123,4/129,5/129,10/129,13/129,14/129,19/129,28/129,31/129,37/129,47/129,53/129,71/129,74/129,80/129,91/129,100/129,77/130,53/132,119/132,11/138,31/138,77/138,85/138,91/138,103/138,4/141,5/141,7/141,8/141,14/141,16/141,17/141,23/141,32/141,35/141,41/141,52/141,55/141,74/141,79/141,82/141,88/141,98/141,101/141,110/141,121/141,3/143,7/143,21/143,40/143,42/143,60/143,73/143,80/143,98/143,120/143,138/143,6/145,8/145,13/145,21/145,64/145,79/145,93/145,122/145,6/155,7/155,9/155,12/155,14/155,69/155,99/155,102/155,107/155,131/155,5/159,7/159,10/159,11/159,14/159,19/159,20/159,23/159,32/159,38/159,58/159,64/159,83/159,85/159,91/159,113/159,116/159,125/159,136/159,9/161,11/161,101/161,103/161,16/165,41/165,61/165,116/165,151/165,33/170,101/170,7/174,37/174,43/174,65/174,95/174,97/174,101/174,103/174,115/174,155/174,8/175,11/175,78/175,108/175,111/175,113/175,116/175,148/175,4/177,5/177,8/177,10/177,11/177,13/177,17/177,19/177,20/177,22/177,26/177,29/177,35/177,38/177,44/177,64/177,67/177,70/177,77/177,94/177,95/177,97/177,103/177,122/177,128/177,131/177,137/177,140/177,154/177,4/183,5/183,7/183,10/183,11/183,13/183,14/183,19/183,20/183,22/183,28/183,34/183,37/183,40/183,49/183,65/183,68/183,71/183,74/183 Update 2 (Important) The question was deleted for a time, because for some of the listed fraction algorithm seems to work just fine if it's done by hand. There is some error in my code, which I hadn't been able to find yet. But there are fractions which lead to loops by hand as well (such as $41/111$) so the question still stands.","['divisibility', 'number-theory', 'algorithms', 'egyptian-fractions', 'elementary-number-theory']"
2132500,"Prove that $\int _0^1x^a\left(1-x\right)^bdx$ = $\int _0^1x^b\left(1-x\right)^adx$, where $a,b\in \mathbb{R}$","Prove that $$\int _0^1x^a\left(1-x\right)^bdx = \int _0^1x^b\left(1-x\right)^adx$$ How can I even get started on this? I evaluate the integral with parts, but it just gets more and more tedious since I'm working with these constants here.","['indefinite-integrals', 'integration', 'definite-integrals', 'calculus']"
2132501,Geodesics between singular points in a translation surface,"Consider a translation surface $X$ with $n\ge 2$ points of conical singularity $x_1,\dots,x_n$ of cone angle $\theta_i=2k_i\pi$, $k_i>1$. Suppose that the geodesic $\sigma$ from $x_1$ to $x_2$ for the singular flat metric is a straight segment. By ""geodesic"" I mean a global geodesic, meaning that the length of $\sigma$ with respect to the singular flat metric equals the distance of the two points with respect to the induced metric. Now consider any smooth point $x\in X$ such that the geodesic $\tau$ from $x_1$ to $x$ for the singular flat metric is a segment and such that the angle at $x_1$ between $\sigma$ and $\tau$ is greater than $\pi$ (by ""angle"" I don't mean Alexandrov's definition of angle, but simply the angle measured at the conical point, where the total angle is $2k_1\pi$). Question 1: Is $\sigma\ast \tau^{-1}$ always the geodesic from $x$ to $x_2$? Or could such geodesic be a straight segment or pass through another singular point? Question 2: If $x_2$ were a smooth point then the answer to the previous question is always yes?","['riemannian-geometry', 'differential-geometry']"
2132502,Confusion Concerning Quadratic Forms (Statistics),"There are two ways to represent the same random variable: $$\varepsilon_{1,t} \sim N(\vec{0},\Sigma\Sigma'), \text{ or } \Sigma\varepsilon_{2,t} \text{ where } \varepsilon_{2,t} \sim N(\vec{0},\mathbb{I}_n).$$ Consider the scalar term $\varepsilon_{1,t}'\delta'\delta\varepsilon_{1,t}$, and  its equivalent $\varepsilon_{2,t}'\Sigma'\delta'\delta\Sigma\varepsilon_{1,t}$ where $\delta$ is a $(1 \times n)$ non-stochastic vector, and $\Sigma$ is $(n\times n)$ and also non-stochastic. Also $\Sigma\Sigma'$ is PSD. According to the properties of the quadratic form , $\mathbb{E}[\varepsilon_t'\Lambda\varepsilon_t]= \operatorname{tr}(\Lambda \Sigma^*) + \mu'\Lambda\mu$] where $\varepsilon_t \sim N(\mu,\Sigma^*)$ (doesn't need to be normal). Assuming $\varepsilon_{1,t}=\Sigma\varepsilon_{2,t}$ does hold true, I get $\operatorname{tr}[\delta'\delta\Sigma\Sigma']=\operatorname{tr}[\Sigma'\delta'\delta\Sigma]$, since $\mu=\vec{0} \Rightarrow \mu'\Lambda\mu = 0$ Plugging arbitrary values into Matlab tells me this does not hold true. Where does my logic fail?","['matrices', 'statistics', 'quadratic-forms']"
2132503,The Covariance Matrix after a Functional Transformation,"Consider the random p-vector $X$ passed through some non-linear function $g: \mathbb{R}^p \rightarrow \mathbb{R}^q$ so that we have $g(X)$. I would like to compute the $q \times q$ covariance matrix $\text{Cov}(g(X))$. Here is the caveat though: we only have access to $\text{Cov}(X)$ and $g$. I am not sure if this is possible, but is there a way of recovering $\text{Cov}(g(X))$ from just $\text{Cov}(X)$ and $g$ (e.g., by using derivative info for $g$)? Obviously if $g$ is linear, then we have $\text{Cov}(A X) = A \text{Cov}(X) A^T$ for some real matrix $A$, but what if $g$ is non-linear?","['statistics', 'covariance', 'probability-distributions']"
2132526,Tensor Product of dual linear maps,"Suppose $V$ and $W$ are finite dimensional linear spaces and $V^*$ as well as $W^*$ are their appropriate linear duals. Now let $f: V \to W$ and $g: V \to W$ be linear maps. Is the following identity correct? $f^* \otimes g^* = (f \otimes g)^*$ That is the tensor product of the dual linear maps, is the linear dual of the tensor product of the maps. Can't find this neither on the Wikipedia page of the tensor product, nor on the Wikipedia page of the dual linear maps. Therefore its properly wrong? Don't think so.","['tensor-products', 'linear-algebra']"
2132531,Number of integral quotient of $n^2$,"Is there a theorem which states the number of integral quotient of $n^2$ divided by $\{1,2,3, ... n^2\}$ is $2n-1$? Example: If $n=4$,then $16 \div \{1,2,3, ... 16\} = 16,8,5,4,3,2,1$. There are $7$ integral quotients.",['number-theory']
2132541,Numbers removed from a set,"Let $A=\left \{ 1,2,3,...,1000 \right \}$ be a set. Determine the greatest number $m$, so that for any $m$ numbers removed from $A$, there will exist $a,b\in A$, with $a|b$. The only useful thought I had was that if we remove the numbers from $1$ to $500$, we won't find two numbers from $501$ to $1000$, with one of them dividing another. Hence, $m\leq 499$.","['number-theory', 'elementary-set-theory']"
2132551,Fixed point of an invariant mapping,"Let $X$ be non empty and $T: X \to X$ a mapping such that $T^n$ has a unique fixed point $x$, then $x$ is also a unique fixed point of T. If I assume $T$ has two fixed points $x$ and $v$ $\implies$ $T^nv=v$ so $v$ is also a fixed point of $T^n$ which is a contradiction. Is this the proper way to do the proof? Edit: All of the details of the question are below. Let $X$ be a nonempty set and $T : X \to X$ be a mapping. If, for an $n \in N$,
$n \geq 2$, there exists a unique fixed point $x \in X$ for $T^n$
, then $x$ is a also a unique fixed point
for $T$.","['functional-analysis', 'linear-algebra']"
2132560,Behavior of the real part of complex polynomial $P(z)$ as $\text{Re } z \to \infty$,"I have the following question. Let $P(z)$ be a complex polynomial. I'm looking to either prove or disprove that $$\lim\limits_{\text{Re } z\to\infty} \text{Re } P(z) = \pm \infty.$$ It feels intuitively like it should be true, since $|P(z)|$ must go to infinity - except I can't find any way to prove or disprove it. I can't use $|P(z)|$ because the modulus relies on the imaginary part as well - which is where I got stuck. If it's not true, though, then I wonder if it's possible to find a sequence such that the above limit holds?","['complex-analysis', 'complex-numbers']"
2132561,weak*-convergence and weak operator topology - multiplication operator,"The setting : let $(\Omega,\mu)$ $\sigma$-finite measure space and let $M_\phi : L^2(\Omega,\mu) \to L^2(\Omega,\mu)$ the multiplication operator with $\phi \in L^{\infty}(\Omega,\mu)$ I want to show  : If $M_{\phi_{i}} \to M_\phi $ in weak operator topology, then $\phi_i \to \phi$ in weak*-topology I already managed to show the reverse statement. I don't know if this helps or even is true : Maybe I can write every $f \in L^1$ as product of two functions in $L^2$ ?","['functional-analysis', 'weak-convergence', 'operator-theory', 'hilbert-spaces']"
2132580,Local/Global isometries of the torus,"I am looking at isometries of the two-torus with the flat metric. From a local perspective, $T^2 = S^1 \times S^1$ has coordinates $(x,y)$, with metric:
$$ds^2 = dx^2 + dy^2$$
The difference between $T^2$ and $\mathbb{R}^2$ is in the topology, and comes from imposing the periodicity of the coordinates $x \sim x+1$ and  $y \sim y+1$. I know that $\mathbb{R}^2$ has three Killing vectors: $\{ \partial_x,\partial_y, x\partial_y - y\partial_x \}$, which are simply the translations, and rotation of the plane. These are also local Killing vectors for the torus, but on the other hand I know that $T^2$ has only two global Killing vectors, the translations. How, explicitly, do I see that the rotation Killing vector is not a global Killing
  vector for the two-torus?","['symmetry', 'riemannian-geometry', 'differential-geometry', 'differential-topology']"
2132583,Good PDE Book That Explains Why,"I am interested in studying the basics of PDEs. I cannot find a good book on PDEs. I want a book that is more on the abstract side and explains why (so not Evan's). For example, if the topic is second order linear PDEs, I want the book to explain why they are classified into elliptic, hyperbolic and parabolic, and then explain how the time coordinate can be separated in the hyperbolic and parabolic cases (preferably before trying to solve them). I have a fairly strong background in introductory mathematics and prefer that the book takes advantage of functional analysis (but with motivation) and (sometimes) considers general Banach spaces. I want specific PDEs (such as Laplace's equation) to only be examples.","['reference-request', 'analysis', 'partial-differential-equations']"
2132609,When is a matrix function the Jacobian matrix of another mapping,"Suppose $J(x)$ is a continuous matrix function $\mathbb{R}^D \to \mathbb{R}^D \times \mathbb{R}^D$. Do there always exist a mapping $f: \mathbb{R}^D \to \mathbb{R}^D$ so that $J = \nabla f$. If not, are there well-known conditions such that this mapping exists?","['multivariable-calculus', 'functional-calculus', 'real-analysis', 'analysis']"
2132619,What is the decomposition of the ring $\mathbb{F}_p[x]/(x^n-1)$?,"Let $p$ be a prime, and $n\ge 1$ an integer. I'd like to decompose the ring $\mathbb{F}_p[x]/(x^n-1)$ into a direct product of artinian local rings. I know we can write $x^n-1 = \prod_{d\mid n}\Phi_d(x)$, but how do the cyclotomic polynomials $\Phi_d(x)$ decompose mod $p$? I know their irreducible factors should have degree equal to the order of $p$ modulo $d$. Can $\Phi_d(x)$ have distinct irreducible factors (or do they always decompose as a power of an irreducible?)? Can $\Phi_d(x),\Phi_{d'}(x)$ share irreducible factors for $d\ne d'$? Is there a nice way to write this decomposition?","['finite-fields', 'abstract-algebra', 'number-theory', 'commutative-algebra', 'group-theory']"
2132620,"Understanding a ""Trivial"" Result In Optimal Control Theory","In Lawrence C. Evans' online notes : Optimal control theory, page 33, Evans makes a very trivial looking statement,which doesn't seem trivial to me. I shall elaborate, giving necessary details, so that this reference is only for further reading. Let $\alpha : [0, \infty) \to [-1,1]^m$ be a measurable function. Consider the differential equation:
$$
\dot x(t) = M x(t) + N\alpha(t) \\ x(0) = x_0
$$ Where $M,N$ are constant real matrices of appropriate dimension, so that $x(t)$ has codomain $\mathbb{R}^n$. Now, we can solve this equation, with the solution:
$$
x(t) = X(t)x_0 + X(t) \displaystyle\int_0^t X^{-1}(s) N\alpha(s) \operatorname{ds} 
$$ Now, fix a  time $T$, and define the following set: $$K(T,x_0) = \left\{ X(T)x_0 + X(T) \displaystyle\int_0^T X^{-1}(s) N\alpha(s) \operatorname{ds}\right\}$$ where the $\alpha$ can vary over all measurable functions possible from $[0,\infty) \to [-1,1]^m$. In words, we are trying to find all reachable points at time $T$. If $x_1 \in K(T,x_0)$, there exists $\alpha_{x_1}$ such that $x_1 =  X(T)x_0 + X(T) \displaystyle\int_0^T X^{-1}(s) N\alpha_{x_1}(s) \operatorname{ds}$. One can prove that $K(T,x_0)$ is convex and closed, the former using an obvious candidate, and the latter via an application of the Banach-Alaoglu theorem. Now, suppose  that $0 \in K(\tau, x_0)$ for some $\tau$, but it is not true that $0 \in K(t, x_0)$ for all $t < \tau$. I want to prove that $0$ is a boundary point of $K(\tau,x_0)$. In words: ""If $0$ is reachable in time $\tau$, but is not reachable in any time smaller than $\tau$, then $0$ is a boundary point of the set of all points reachable in time $\tau$"". It did not seem trivial, so I tried letting $0$ be an interior point. It is clear that in a neighbourhood of $0$, there are points which can be reached in time $< \tau$ (when we take the trajectory from $x_0$ to $0$, it enters the neighbourhood before time $\tau$, so all points falling on that trajectory are reachable in time $< \tau$, naturally).However, I am unable to show why there must be points which are not reachable in time $\tau$ in a neighbourhood of $0$. This is because I am unable to use the nature of the given set to my benefit. Surely if the question is trivial, then I am missing something. Do guide me across this one.","['optimal-control', 'matrices', 'functional-analysis', 'general-topology', 'ordinary-differential-equations']"
2132639,What is the geometric interpretation of the rowspace?,"I am missing something essential in my understanding of matrices as linear mappings (or matrices in general). To explain, I think I have a decent intuition for columnspace of a matrix. Basically, the columnspace can be thought of as the set of points other than the origin which can be mapped by the matrix of a linear mapping (I like to think of matrices in general as linear mappings). What I have trouble with is the significance and geometric interpretation of the rowspace of a matrix. By the definition, I can see that the rowspace and columnspace are analagous; the rowspace is merely the columnspace of the transpose of the matrix of the linear mapping. However, I cannot explain this anymore intuitively than above. To quote Albert Einstein, ""if you don't understand it simply, you don't understand it well enough"". I am looking for a simple explanation (or geometric interpretation that will contribute to my imagination of linear mappings). How would you explain this concept? To give you some context, I ran into problems when my course started to discuss the following: For an $n \times n$ matrix $P$, the following are equivalent: (1) The columns of $P$ form an orthonormal basis for $\mathbb{R}^n$ (2) $P^T = P^{-1}$ (3) The rows of $P$ form an orthonormal basis for $\mathbb{R}^n$. I understand the algebra perfectly, but I seek a level of intuition so that I would not need to fiddle around with algebra to discover these facts. How would you explain the above concepts intuitively or geometrically, without the need for algebra to make a case? Many thanks in advance.","['orthonormal', 'linear-algebra', 'linear-transformations', 'vector-spaces']"
2132652,How do I find the value $(1+\tan1^\circ)(1+\tan2^\circ)(1+\tan3^\circ)\cdots(1+\tan45^\circ)$ [duplicate],"This question already has answers here : A ""fast"" way for  computing $ \prod \limits_{i=1}^{45}(1+\tan i^\circ) $? (3 answers) Closed 7 years ago . What is the strategy to solve for: $$(1+\tan1^\circ)(1+\tan2^\circ)(1+\tan3^\circ)\cdots(1+\tan45^\circ)$$ I don't know where to start. Thanks.","['trigonometry', 'sequences-and-series']"
2132654,"How to evaluate $\sum\limits_{n \geq 0} \left(S_{n + 2} + S_{n + 1}\right)^2(-1)^n$, given the multivariable recurrence relation?","The given multivariable recurrence relation is that for every $n \geq 1$
$$S_{n + 1} = T_n - S_n$$
where $S_1 = \dfrac{3}{5}$ and $T_1 = 1$. Both $T_n$ and $S_n$ depend on the following condition
$$
\dfrac{T_n}{S_n} = \dfrac{T_{n + 1}}{S_{n + 1}} = \dfrac{T_{n + 2}}{S_{n + 2}} = \dots
$$
The goal is to evaluate
$$\sum\limits_{n \geq 0} \left(S_{n + 2} + S_{n + 1}\right)^2 (-1)^n$$ Since the change between $T_n$ and $T_{n + 1}$ is not constant, I believe that the way to approach this problem is to have all terms with consistent coefficient. However, I am not skillful enough to simplify the summation into a single variable.","['recurrence-relations', 'summation', 'calculus', 'discrete-mathematics']"
2132694,How are these problems called and how are they solved?,"I'm self learning calculus and I stumbled upon the following problem: Express $I_n =\int \frac{dx}{(x^2+a^2)^n}$ Using $I_{n-1}$ ($a$ is a positive parameter and $n=2,3,4,...$) Is this about double integrals? Could anyone please elaborate a bit more so I can learn how to solve this type of problems? ================= EDIT Continuing @SimplyBeautifulArt's answer: $I_n=a^{1-2n}\int{cos^{2n-2}(u)du} = a^{1-2n}\int{cos^{2n-3}(u)cos(u)du}$ $I_{n-1}=a^{3-2n}\int{cos^{2n-4}(u)du}$ Integrating by parts($f:cos^{2n-3}(u); dg:cos(u)du$): $I_n=a^{1-2n}(cos^{2n-3}(u)sin(u) + (2n-3)\int{cos^{2n-4}(u)sin^2(u)du})$ $I_n=a^{1-2n}(cos^{2n-3}(u)sin(u) + (2n-3)(\int{cos^{2n-4}(u)du} -\int{cos^{2n-2}(u)du}))$ $I_n=a^{1-2n}cos^{2n-3}(u)sin(u) + (2n-3)(\frac{a^{3-2n}}{a^2}\int{cos^{2n-4}(u)du} -a^{1-2n}\int{cos^{2n-2}(u)du})$ $I_n=a^{1-2n}cos^{2n-3}(u)sin(u) + (2n-3)(\frac{I_{n-1}}{a^2} -I_n)$ $I_n=(a^{1-2n}cos^{2n-3}(u)sin(u) + (2n-3)\frac{I_{n-1}}{a^2})/2n-2$ Recall $u=arctan(\frac xa)$ $I_n=(a^{1-2n}\frac{1}{\sqrt{1+(x/a)^2}}^{2n-3}\frac{x/a}{\sqrt{1+(x/a)^2}} + (2n-3)\frac{I_{n-1}}{a^2})/2n-2$ Is that all?","['self-learning', 'integration', 'calculus']"
2132739,Do integrable functions vanish at infinity? [duplicate],"This question already has answers here : Prove that $f$ continuous and $\int_a^\infty |f(x)|\;dx$ finite imply $\lim\limits_{ x \to \infty } f(x)=0$ (5 answers) Closed 7 years ago . If $f$ is a real-valued function that is integrable over $\mathbb{R}$, does it imply that $$f(x) \to 0 \text{ as } |x| \to \infty?  $$ When I consider, for simplicity, positive function $f$ which is integrable, it seems to me that the finiteness of the ""the area under the curve"" over the whole line implies that $f$ must decay eventually. But is it true for general integrable functions?",['measure-theory']
2132770,Is $ (A \cup B \cup C) \cap (A^c \cup D \cup E) \cap (B^c \cup D^c \cup F) \cap (C^c \cup E^c \cup F^c) $ empty?,"For sets $A, B, C, D, E,$ and $F$ where any two of these sets have a nonempty intersection. Is \begin{align*}
& (A \cup B \cup C)\\
{}\cap{} & (A^c \cup D \cup E)\\
{}\cap{} & (B^c \cup D^c \cup F)\\
{}\cap{} & (C^c \cup E^c \cup F^c)
\end{align*} empty?",['elementary-set-theory']
2132820,Is this procedure a Simple Random Sample?,"Page 62 Question 3 of Sampling: Design and Analysis Each of the 10,000 shelves in a certain library is 300 cm long. To estimate how many
  books in the library need rebinding, a librarian takes a sample of 50 books using the
  following procedure: He first generates a random integer between 1 and 10,000 to
  select a shelf, and then generates a random number between 0 and 300 to select a
  location on that shelf. Thus, the pair of random numbers (2531, 25.4) would tell the
  librarian to include the book that is above the location 25.4 cm from the left end of
  shelf number 2531 in the sample. Does this procedure generate an SRS of the books
  in the library? My thoughts: In order to be an SRS, every unit in population has the same probability of being in the sample of size n. As a result, I believe that this method would in fact generate an SRS, as each book has the same probability of being selected. Is this correct to assume?","['statistics', 'sampling']"
2132838,Large-value approximation of (real) error function series expansion,"I want to compute an approximation of the error function in order to compute probabilities for very extreme events (> 20 standard deviations), and I fail to reconcile results from different sources. The standard power series expansion of the error function for small x is given by (wikipedia): $$ erf(x) = \frac{2}{\sqrt{\pi}} \sum_{n=1}^{\infty} \frac{(-1)^n x^{2n-1}}{n!(2n+1)}$$ Now according to some sources (eq x12) there is an approximation for large x: $$ erf(x) \approx 1 - \frac{e^{-x^2}}{\sqrt{\pi}x} \left( 1 - \sum_{n=1}^{\infty} \frac{\prod_{k=1}^{n}2k-1}{(2x^2)^n} \right) $$ Whereas according to other sources (eq 1.15) it's: $$ erf(x) \approx 1 - \frac{e^{-x^2}}{\sqrt{\pi}x} \left( 1 - \sum_{n=1}^{\infty} (-1)^{n+1} \frac{\prod_{k=1}^{n}2k-1}{(2x^2)^n} \right) $$ Basically, the difference (if I haven't made any mistakes this late at night) is just the flipping signs. Can someone explain to me what I'm missing here, or what the right approximation is?","['statistics', 'probability']"
2132863,How would a triangle for sin 90 degree look,"I am studying trigonometry in my school and learned that in a triangle the side opposite to angle theta should be taken as perpendicular side - hypotenuse remains the same and the third remaining side is the base. The same is required for calculating the sine / cosine etc of the angle theta for below formula for distance / object height in applications of trigonometry - sin theta = Perpendicular / hypotenuse 
 cos theta = Base / hypotenuse The remaining other can be created using the above two. So in a Triangle ABC if Angle B is 90 degree it is easy to find sin A or sin C - I mean which side is the Perpendicular, hypotenuse or the base. But then how to find the sides for sin 90 - should we take hypotenuse as the perpendicular side for above triangle as sin B is sin 90 since side opposite to angle will be the perpendicular side - then which side should be taken as hypotenuse and base in such a case",['trigonometry']
2132906,Is the locus where the fiber is finite open?,"Let $X$ and $Y$ be integral, finite-type $\mathbb{C}$-schemes with $Y$ nonsingular and $\dim X=\dim Y$. Let $f:X\rightarrow Y$ be a dominant morphism. Let $U\subseteq Y$ be the set of $y\in Y$ for which $f^{-1}\{y\}$ is a nonempty finite set of points. By generic flatness, $U$ contains a dense open subset of $Y$. What I would like to know is if $U$ itself is open. Note 1: This will be the case when $f$ is a closed map by upper semicontinuity of fiber dimension on the target. Note 2: David Speyer gives an example of $U$ failing to be open when $Y$ is not normal. See his answer here. So some kind of niceness re: singularities of $Y$ must be used if $U$ is to be proved open. Perhaps some form of Zariski's Main Theorem would be useful, but it's not clear to me how. Note 3: One can reduce to the case $X$ and $Y$ affine, so I'm adding the commutative algebra tag. Let $A\subseteq B$ be an extension of finitely generated $\mathbb{C}$-domains of the same Krull dimension with $A$ regular. If $p$ is a prime ideal of $A$ with finitely many primes of $B$ lying over it, does there exist $f\in A-p$ such that every prime of $A_{f}$ has finitely many primes of $B$ lying over it?","['algebraic-geometry', 'commutative-algebra']"
2132993,Prove that a Cauchy sequence is convergent,"I need help understanding this proof that a Cauchy sequence is convergent. Let $(a_n)_n$ be a Cauchy sequence. Let's prove that $(a_n)_n$ is bounded. In the definition of Cauchy sequence: 
  $$(\forall \varepsilon>0) (\exists n_\varepsilon\in\Bbb N)(\forall n,m\in\Bbb N)((n,m>n_\varepsilon)\Rightarrow(|a_n-a_m|<\varepsilon))$$
  let $\varepsilon=1$. Then we have $n_1\in\Bbb N$ such that $\forall n,m\in\Bbb N (n,m>n_1)\Rightarrow(|a_n-a_m|<1)$. From there for $n>n_1$ we have $|a_n|\leq |a_n-a_{n1+1}|+|a_{n1+1}|(*).$ Now $M=\max\{|a_1|,...|a_{n1}|,1+|a_{n1+1}|\}$ such that $|a_n|\leq M,\ \forall n\in\Bbb N.$ Bounded sequence $(a_n)_n$ has a convergent subsequence $(a_{p_n})_n$, i.e. there exists $a=\lim_n a_{p_n}$. Let's prove $a=\lim_n a_n$. Let $\varepsilon>0$ be arbitrary. From the convergence of subsequence $(a_{p_n})_n$ we have $n'_\varepsilon\in\Bbb N$ such that 
  $$(n>n'_\varepsilon)\Rightarrow(|a_{p_n}-a|<\frac{\varepsilon}{2}).$$
  Because $(a_n)_n$ is a Cauchy sequence, we have $n''_\varepsilon\in\Bbb N$ such that 
  $$(n,m>n''_\varepsilon)\Rightarrow(|a_n-a_m|<\frac{\varepsilon}{2}).$$ 
  Let $n_\varepsilon=\max\{n'_\varepsilon, n''_\varepsilon\}$ so for $n>n_\varepsilon$ because $p_n\geq n$ we have $$|a_n-a|\leq|a_n-a_{p_n}|+|a_{p_n}-a|<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon \ (**)$$ i.e.  $a=\lim_n a_n$. $(*)$Where did $|a_n|\leq |a_n-a_{n1+1}|+|a_{n1+1}|$ come from? I understand why that inequality is true, but I don't see the point in writing in like that. $(**)$ Why is $|a_n-a_{p_n}|<\frac{\varepsilon}{2}?$","['real-analysis', 'cauchy-sequences', 'sequences-and-series', 'proof-explanation']"
2133071,Mathematical Induction for an alternating series,"It's been a while since I have done a problem like this. I have this problem $$\sum_{i=0}^n (-1)^{i+1} i^2 = \frac {(-1)^{n+1}n(n+1)}{2}$$ So I have gotten this far: Base Case : 
$$n=1$$ $$(-1)^2+1^2 = \frac{ (-1)^2 1(2) }{2}$$
$$ 1 = 1$$ Assume : $$\sum_{i=0}^n (-1)^{i+1} i^2 = \frac {(-1)^{n+1}n(n+1)}{2}$$ Prove :
$n = n+1$ $$\sum_{i=0}^n+1 (-1)^ {i+1} i^2 = \frac {(-1)^{n+2}(n+1)(n+2)}{2} $$ I think that I can just replace the i's with (n+1) too Which would be this $$\sum_{i=0}^{n+1} (-1)^ {n+2} (n+1)^2 = \frac {(-1)^{n+2}(n+1)(n+2)}{2} $$ I don't remember where to go from here.","['induction', 'summation', 'discrete-mathematics']"
2133076,Number of ways to get $xy^2$ from $(x+y+5)^5$,I am trying to understand a proof of counting number of trees using the conclusion of Cayley's formula. At the end they got a $(x+y+5)^5$ and they say that the number of ways to get $xy^2$ from this is $5 \cdot \binom{4}{2} \cdot 5^2 = 750 $ and I can't figure how. Can someone please explain? thanks.,"['combinatorics', 'graph-theory', 'discrete-mathematics']"
2133077,"True or False: If $A^2=0$ for a 10x10 matrix $A$, then rank($A$) $\le 5$","True or False:
  If $A^2=0$ for a $10$ by $10$ matrix $A$, then the inequality rank($A$) $\le 5$ must hold. I am guessing that this is false. Apparently proving or disproving this uses knowledge of basis, change of coordinates, or dimension (the chapter of the problem is about these). I tried proof by contradiction, which didn't work so far.","['matrices', 'matrix-rank']"
2133107,Bijections from $\mathbb{N}$ to $\mathbb{N}\times\dots\times\mathbb{N}$.,"It is well known that $\mathbb{N}$ is bijective with $\mathbb{N}\times\mathbb{N}$, there are many possible such bijections as described in this site. What I am curious, what are some bijections that generalizes well to higher dimensions, i.e. generalizes to bijection from $\mathbb{N}$ to $\mathbb{N}\times\dots\times\mathbb{N}$? For instance the ""Cantor tuple function"" https://en.wikipedia.org/wiki/Pairing_function generalizes to higher dimensions. Are there any others? Thanks for any references.","['reference-request', 'elementary-set-theory']"
2133131,"Number of tagged trees $T=(V,E)$ on $10$ vertices such that $deg(v_i)=i$ for $1 \le i \le 3$","How can I find the Number of tagged trees $T=(V,E)$ with $V=\{ v_1, v_2, v_3, v_4, v_5, v_6, v_7, v_8, v_9, v_{10} \} $ such that $deg(v_i)=i$ for $1 \le i \le 3$ ? I need a way using Cayley's formula. Thanks!","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2133163,How to truly understand integration and differentiation?,"I took Calc I course and currently studying Calc II. I'm pretty good at integration and differentiation in terms of mathematically solving equations, but I don't truly understand the concepts. When I apply integration rules to a solve a problem it just feels like magic to me, so I don't really get what's going on. For instance, if try to a solve an unfamiliar application problem I won't really be able to solve it. I feel like I'm just a useless calculator. At the beginning, When I started to learn about differentiation I had a pretty good understanding like finding the instantaneous rate of change, and understood the relation distance => velocity => acceleration (and why it works that way). Now I lost track. I want to be able to use these concepts creatively in real-world applications. Can you suggest a book or a give me an advice on how to truly understand these concepts?","['derivatives', 'integration', 'calculus']"
2133189,"If $\tan^3A+\tan^3B+\tan^3C=3\tan(A)\tan(B)\tan(C)$, prove triangle ABC is equilateral triangle","If $\tan^3A+\tan^3B+\tan^3C=3\tan(A)\tan(B)\tan(C)$, prove triangle ABC is equilateral triangle Now i remember a identity which was like if $a+b+c=0$,then $a^3+b^3+c^3=3abc$. So i have $\sum_{}^{} \tan(A)=0$. How do i proceed? Thanks.","['self-learning', 'trigonometry']"
2133195,To evaluate $\frac{\cot25+\cot55}{\tan25+\tan55}+ \frac{\cot55+\cot100}{\tan55+\tan100}+\frac{\cot100+\cot25}{\tan100+\tan25}$,To evaluate $$\frac{\cot25^{\circ}+\cot55^{\circ}}{\tan25^{\circ}+\tan55^{\circ}}+ \frac{\cot55^{\circ}+\cot100^{\circ}}{\tan55^{\circ}+\tan100^{\circ}}+\frac{\cot100^{\circ}+\cot25^{\circ}}{\tan100^{\circ}+\tan25^{\circ}}$$ i took lcm and after usual trigonometric identities i have reduced above expression to $$\cot25^{\circ}\cot55^{\circ}+\cot55^{\circ}\cot100^{\circ}+\cot100^{\circ}\cot25^{\circ}$$ How do i proceed from here? Thanks,['trigonometry']
2133217,Minimal distance to a cube in 2D and 3D from a point lying outside,This is kind of a geometrical question. For my program I want to compute the minimal distance $r$ from a given point to the cube. Here is a drawing which shows what I mean: I have two vectors $\vec{p}$ and $\vec{q}$ which indicate the position of my two points. Point $p$ can be anywhere outside the cube. Point $q$ is exactly in the middle of the cube. The distance from point $q$ to the cubes surface is always $d$. I can easily compute $R$ which is the distance from point $q$ to point $p$. But what I need is the minimal distance $r$ from point $p$ to the cube. I am sure that I have to distinguish several cases depending on where point $p$ is located. I think there are three cases: 1) The minimal distance $r$ from point $p$ is to the edge of the cube (as drawn in the picture) 2) The minimal distance $r$ from point $p$ is to the corner of the cube 3) The minimal distance $r$ from point $p$ is to the surface of the cube After hours of trying to find a nice solution I hope someone can give me a hint.,"['optimization', 'geometry']"
2133223,Why is negative binomial distribution a sum of independent geometric distributions?,"I was trying to prove the expected value of $X$ when $X\sim NB(n,p)$, then I came across this proof: for geometric distribution $X_1, X_2,\ldots, X_n$ with the same parameter $p$, $$E(X)=\sum_{i=1}^n E(X_i)=\frac{n}{p}$$ (At least I think) I get the idea; since NBD is focusing on the number of trials before we get $n$ successes, it would be reasonable to think of it as a combination of $n$ independent GDs. But we need at least $n$ trials for $n$ successes, whereas only 1 trial is enough for each $X_i$. That is, $X$ prevents $n$th success from occuring before $n$th trial, but it is not the case for $X_i$s (since they are all independent). Why is it okay to express $X$ as $\sum X_i$?","['probability', 'probability-distributions']"
2133246,Expressing Zeta function using Gamma series,"Motivated by Gautschi double inequality , 
$$ \frac{n^{s}}{n^{\small1}}\ge\frac{\Gamma(n+s)}{\Gamma(n+1)}\ge\frac{(n+1)^{s}}{(n+1)^{\small1}}\ge\frac{\Gamma(n+1+s)}{\Gamma(n+1+1)}\ge\,\cdots \quad\colon\,0\lt{s}\lt1\tag{1} $$ 
From the main definition of zeta function , 
$$ \zeta(s)=\sum_{n=1}^{\infty}\frac{1}{n^s}\,\,\,\colon\,Re\{s\}\gt1 \space\Rightarrow\space \zeta(1-s)=\sum_{n=1}^{\infty}\frac{n^{s}}{n^{\small1}} \qquad\colon\,Re\{s\}\lt0\tag{2} $$ 
And the sum identity of gamma function , 
$$ \sum_{n=0}^{\infty}\frac{\Gamma(n+s)}{n!}=0 \quad\Rightarrow\quad \Gamma(s)=-\sum_{n=1}^{\infty}\frac{\Gamma(n+s)}{\Gamma(n+1)} \qquad\colon\,Re\{s\}\lt0\tag{3} $$ How to Prove, Disprove, or Justify: 
  $$ \zeta(1-s)+\Gamma(s)=\sum_{n=1}^{\infty}\left[\,\frac{n^{s}}{n^{\small1}}-\frac{\Gamma(n+s)}{\Gamma(n+1)}\,\right] \qquad\qquad\colon\,0\lt{s}\lt1\tag{4} $$ 
  Does it hold if extended to complex plan $\,s\in\mathbb C\,$ inside the critical strip $\small 0\lt Re\{s\}\lt1\,$ ? “ By the monotonic decreasing behavior of the inequality, the subtracting result of the two divergent series converge one step forward, covering the critical strip! ”","['analytic-continuation', 'riemann-zeta', 'divergent-series', 'sequences-and-series', 'gamma-function']"
2133260,Prove that $\sin x+\sin y=1$ does not have integer solutions,"Suppose $x$ and $y$ are angles measured in radians. Then how to show that the equation 
$$\sin x+\sin y=1$$
does not have a solution $(x,y)\in\mathbb{N}\times\mathbb{N}$? This question is prompted by curiosity. I don't have any ideas how it can be approached.","['number-theory', 'trigonometry']"
2133274,"Swap sums, change of indices","there are two equations I don't understand. (Probability Theory): $$\sum_{k=1}^{\infty} P(X \ge k) = \sum_{k=1}^{\infty} \sum_{n=k}^{\infty} P(X=n) = \sum_{n=1}^{\infty} \sum_{k=1}^{n} P(X=n) = \dots$$
I really don't get the second equation. I know that you can swap sums if all the addends are non-negative. But why do you change the indices like that? Markov Chains: $$\dots \sum_{n=1}^{\infty} \sum_{k=1}^{n} f_{ij}^{k} \Pi^{n-k}(j,j) = \sum_{k=1}^{\infty} \sum_{m=0}^{\infty} f_{ij}^{k} \Pi^m (j,j) = \dots $$ where $\Pi$ is the transition matrix. This time I know that you replace $n-k$ by $m$, but why is the first sum from 1 to infinity? Thank you for all your help.","['summation', 'probability']"
2133277,How do they got the solution for this $2\times 2$ matrix?,"For $y'=\frac{1}{x}\begin{pmatrix}0 &1 \\ 2 & -1\end{pmatrix}y$ the solutions are given by $y_1=\begin{pmatrix}x \\ x\end{pmatrix},y_2=\begin{pmatrix}x^{-2} \\ -2x^{-2}\end{pmatrix}$ with $x\in \Bbb R,y\in \Bbb R^2$ I would like to know how they got this solutions. I tried the following:$$det\frac{1}{x}\begin{pmatrix}0-\lambda &1 \\ 2 & -(1+\lambda)\end{pmatrix}=\frac{1}{x}(\lambda^2+\lambda-2)$$
$\Rightarrow \lambda_{1,2}=-\frac{1}{2}+/-\frac{3}{2}\Rightarrow\lambda_1=-2 , \lambda_2=1$ Than $$ker\begin{pmatrix}-1&1 \\ 2 & -2\end{pmatrix}\Rightarrow v_1=\begin{pmatrix}1 \\ 1\end{pmatrix}\Rightarrow y_1=e^x\begin{pmatrix}1 \\ 1\end{pmatrix}$$ also $$ker\begin{pmatrix}2&1 \\ 2 & 1\end{pmatrix}\Rightarrow v_1=\begin{pmatrix}1 \\ -2\end{pmatrix}\Rightarrow y_2=e^{-2x}\begin{pmatrix}1 \\ -2\end{pmatrix}$$ I guess this way is wrong Edit: $y_1'=\frac{1}{x}y_2 $ and $y_2'=\frac{1}{x}(2y_1-y_2)$",['ordinary-differential-equations']
2133316,Maclaurin series converges to function,Show that the maclaurin series for $ (1+x)^{-3/2} $ converges to the function for $|x|<1$ I'm supposed to use the remainder term $ \frac{f^{n+1}(c)x^{n+1}}{(n+1)!} $ and show that the limit of that remainder goes to zero. This has been simple for functions like $ sinx $ But it is not that easy to find the nth derivative expression for this function. Is there a simple way to this without using binomial series?,['sequences-and-series']
2133392,"$(1,1)$ tensor vs a linear transformation (matrix)","Take $d$-dimensional Vector space $V$ with Field $R$. A typical linear algebra linear transformation $V \to V$ can be represented by a $d \times d$ matrix $A$ such that for some $v,w \in V$, $Av=w$. I'm learning about tensors, and I understand that a $(1,1)$ tensor $T$ is a linear transformation $V^* \times V \to R$. I've read that such a $(1,1)$ tensor is equivalent to such a matrix. However, I find it very difficult to imagine what $V^*$ (the dual space, i.e. set of all maps $V\to R$) has to do with a simple linear transformation from $R^d$ to $R^d$. Moreover, the tensor components apparently are defined as $T^i_{\space \space j}=T(\epsilon_i, e^j)$, where $e^j, \epsilon _i$ are the $d$ bases of $V$ and $V^*$ respectively. This means that if we would write $T$ as a 2-dimensional array, it would have nothing to do with a matrix as in linear algebra. So how are these two concepts connected? This post is related to my question, but it doesn't really go into the difference between the matrix and tensor form.","['matrices', 'tensors', 'linear-algebra']"
2133402,Are there further gaps in the Eisenstein primes?,"I recently played around with Eisenstein primes a bit (in an admittedly very amateurish way) and noticed among other things that there are no primes on the hexagonal ring that goes through (8,0) on the Eisenstein grid of the complex plane: I thought this was a neat feature of the distribution of the primes and started looking for further such gaps. To my astonishment I haven't been able to find a single such gap up to at least a ""radius"" of 40,000,000. So now I'm wondering whether 8 is indeed the only such gap (ignoring the trivial cases of 0 and 1), or whether there might be further gaps at larger radii. My Google efforts haven't turned up anything on this and I'm not sure how one would go about answering the question short of keeping the search running in hopes of finding another gap (which of course will never yield the answer ""no further gaps exist""). I assume one could make a statistical argument based on the density of the Eisenstein primes, but I'm not sure how the prime number theorem applies to them.","['number-theory', 'prime-numbers', 'eisenstein-integers']"
2133449,Not sure how to calculate $\int_0^1 \frac 1 {1+y\cos(x)}dx$,"$$\int_0^1 \frac 1 {1+y\cos(x)}dx$$ If there was no $y$, I would multiply by $1-\cos(x)$ and finish it quickly. But the $y$ stops me from doing that. I tried the trigonometric substitution but failed. I assume there is a simple way to solve this. I would appreciate if anyone could help me with this. Thanks. EDIT: When trying the Weirstrass Sub. I got here and wasn't able to find a way to go forward: $$\int_0^{\pi/4}\frac {2dt}{1+y+t^2(1-y)}$$","['integration', 'definite-integrals', 'calculus']"
2133527,"Where to Purchase Physical Copies of Grothendieck's EGA, SGA, and FGA","I am really interested in purchasing the physical copies of Grothendieck's legendary books: EGA, SGA, and FGA. I could not find prices through Amazon or eBay, which makes me to suspect that foreign bookstores will sell them.  By any chance, do you know any website that I could order physical copies?","['reference-request', 'book-recommendation', 'algebraic-geometry']"
2133552,If $f:R \to R$ such that $(f(x))^7=x-f(x)$ find required area,"If $f:R \to R$ be a differentiable function such that $(f(x))^7=x-f(x)$, then find find the area bounded by curve $y=f(x)$ between the ordinates $x=0$ and $x=\sqrt{3}$ and $x-$axis. (A) $\frac{f(\sqrt3)}{8}[8 \sqrt{3}-(f(\sqrt3))^7-4 f(\sqrt3)]$ (B)  $\frac{f(\sqrt3)}{8}[8 \sqrt{3}-(f(\sqrt3))^7]$ (C) $\sqrt3 f(\sqrt3)-\frac{93}{8}$ (D) None of these. Here $(f(x))^7=x-f(x)$ Hence $x=(f(x))^7+f(x)$ which gives $f^{-1} (x)=x^7+x$ i.e. but there is no way to reach $f(x)$ itself. Could someone help me with this?","['definite-integrals', 'calculus', 'functions']"
2133568,"If $T$ is a bounded linear operator between Hilbert Spaces and $\lVert{T}\rVert = \lVert{T^{-1}}\rVert =1$, is $T$ unitary?","If $T:K \rightarrow L$ is a bounded linear operator between two Hilbert Spaces $K$ and $L$, then we have automatically that if $T$ is unitary, then $\lVert{T}\rVert = \lVert{T^{-1}}\rVert = 1$ by the following: $\lVert{Tx}\rVert^{2}_{L} = \langle Tx, Tx\rangle_{L} = \langle x, x\rangle_{K} = \lVert{x}\rVert^{2}_{K} \Rightarrow \lVert{Tx}\rVert_{L} = \lVert{x}\rVert_{K}$ Then immediately from the definition of the operator norm we get $\lVert{T}\rVert = 1$, similarly we can obtain $\lVert{T^{-1}}\rVert = 1$. However, I get a little confused when going the other way, proving or disproving the converse... (Any insight or hints are much appreciated!).",['functional-analysis']
