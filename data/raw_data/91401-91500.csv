question_id,title,body,tags
1237370,How to calculate MISE - Mean Integrated Squared Error?,"I might have misunderstood something, but it seems like taking a definite integral from expectation or expectation from definite integral. But the first stage will return a number in both cases. I thought that integral under expectation is indefinite, but it looks like just a ""notation"" in many places, because further digging brings negative answer - for example: see ยง 2.2.4 here But I have only one variable - $x$, and estimated $\hat{f}(x,h)$ and theoretical $f(x)$ functions. Can anyone explain me how to calculate $MISE(h) = E \int_{\mathbb{R}}(\hat{f}(x,h)-f(x))^2dx$ ? I see that there are some further manipulations involving the structure of kernel, but I just want to perform a simple computation in Wolfram having two functions, like MISE[ h_ ]:=NIntegrate[ NExpectation [ ...]... ]","['probability', 'statistics']"
1237376,Directional derivative vs. function restriction and then derivative,"Say I have a function of two variables, and a line in the plane, and I'd like to ""take the derivative along the line"". Is this an indication to use the directional derivative, OR is it expected that I first restrict the function to the line in the plane, so that it's a function of one variable, and then take a derivative in, say, the x variable? And whichever one is the right answer, what is the other option doing then? (this specifically is in relation to a hermite finite element example; the line in question is one side of a triangle, so two points on the line are given (two of the triangle vertices); if the directional derivative is what is in fact meant, then I guess they mean to evaluate it at one of the two points.)","['finite-element-method', 'derivatives']"
1237390,Proving $ \frac{1}{c} = \frac{1}{a} + \frac{1}{b}$ in a geometric context,"Prove or disprove $$
\frac{1}{c} = \frac{1}{a} + \frac{1}{b}.
$$ I have no idea where to start, but it must be a simple proof. Trivia. This fact was used for determination of resistance of two parallel resistors in some circumstances long time ago.","['geometry', 'triangles']"
1237455,Difference between generators and basis,"What is the difference between the terms ""generator set"" and ""basis""?  Don't they both just mean a set of objects that you can use to obtain all of the objects in a larger set under some operations?  Is a basis just a particular kind of generator set or are they completely distinct ideas?","['abstract-algebra', 'definition']"
1237459,"If a function fg is surjective under composition and f is surjective, is g surjective?","If a function $fg$ is surjective under composition and $f$ is surjective, is
$g$ surjective? I think not, since $f$ could be a many to one function and $g$ could send 
elements only once to elements in g's domain which are many to one elements. However if $fg$ is injective and $f$ is injective it seems to me that $g$ is too?
Is that right? Thanks",['elementary-set-theory']
1237464,Probability Question: What percentage of the bag of balls is marked?,"In a bag of reds and black balls, $30\%$ were red, and $90\%$ of the black balls and $80\%$ of the red balls are marked balls. What percentage of the bag of balls is marked? I thought I would have to use: Let $A = \text{marked red balls},$ $B = \text{marked black balls}.$ $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ But the answer is simply $0.8\times0.3 + 0.7\times0.9$. How come we don't have to take $P(A \cap B)$ into consideration?","['probability', 'statistics']"
1237491,Integrating a jacobian to find the volume.,"I want to solve the following: Prove that $$\displaystyle \int_R \sin^{n-2}\phi_1 \sin^{n-3}\phi_2\cdots\sin \phi_{n-2} d\theta d\phi_1\cdots d\phi_{n-2} = \frac{2\pi^{n/2}}{\Gamma(n/2)}$$ where $ R=[0,2\pi] \times [0,\pi]^{n-2}$. Hint: Compute $ \int_{\mathbb R^n}e^{-|x|^2}dx$ in spherical coordinates. So I am having problems to calculate the latter integral in spherical coordinates because I dont know how to integrate (in finite steps) $sin^{n}(x)$ and I dont know how this results to be a division of integrals.Can you help me to solve this please?, Thanks a lot in advance :)","['spherical-coordinates', 'integration']"
1237502,Lower semicontinuous energy functional on compact space of Lipschitz functions,"Let $\Omega \subset \mathbb{R}^{n}$ be a bounded open subset containing $0$ and let $L>0$ be some positive constant. Consider the space 
$A_{0}=\{f \in C^{\infty}(\overline{\Omega}) \mid f \text{ is L-Lipschitz}, f(0)=0\}$ 
 and let $A$ be the closure of $A_{0}$ with respect to the $C^{0}-$norm. By Arzela-Ascoli $A$ is compact space consisting of $L$-Lipschitz functions which vanish at $0$. Define the functional
$\mathcal{E} : A \rightarrow \mathbb{R}$ by $\mathcal{E}(f)=\int_{\Omega}|\nabla f|^{2}dx$. I have the following questions: How can I make sense of $\nabla f$ since for general $f\in A$, $f$ is not differentiable anymore? I thought using Rademacher theorem but I do not know exactly how. Is $\mathcal{E}$ continuous or semicontinuous? If its semicontinuous then is it lower or upper semi-continous. My feeling tells me that it is lower semi-continous. But I dont know how to prove it.
Do you have any idea?","['analysis', 'calculus-of-variations', 'real-analysis', 'functional-analysis']"
1237523,Calculate Intersection with a Non Finite Set?,"What is the best way to answer Intersection or Union based questions with a set that is not finite? such as this: Calculate: $A \cap B$ $$\begin{align}
A&=\{x\mid x=n+9, n\in\mathbb N\}\\
B&=\{x\mid x=4n+1,n\in\mathbb N\}
\end{align}$$","['elementary-set-theory', 'discrete-mathematics']"
1237549,Calculate the ring of integers of quadratic number field $\mathbb{Q}(\sqrt{d})$ [duplicate],"This question already has answers here : Why is quadratic integer ring defined in that way? (6 answers) Closed 9 years ago . Calculate the ring of integers of quadratic number field $\mathbb{Q}(\sqrt{d})$ Solution: Let $F$ be an algebraic number field. Then an element $b\in F$ is integral iff its monic irreducible polynomial has integer coefficients. For example, $\sqrt{d}$ for integer $d$ is integral. If $d\equiv 1 \bmod 4$ , then the monic irreducible polynomial of $(1+\sqrt{d})/2$ over $\mathbb{Q}$ is $x^2 -x + (1-d)/4 \in \mathbb{Z}[x]$ , so $(1+\sqrt{d})/2$ is integral. Thus the integral closure of $\mathbb{Z}$ in $\mathbb{Q}(\sqrt{d})$ contains the subring $\mathbb{Z}[\sqrt{d}]$ , and the subring $\mathbb{Z}[(1+\sqrt{d})/2]$ if $d\equiv 1 \bmod 4$ . We show that there are no other integral elements. An element $a+b\sqrt{d}$ with rational $a$ and $b\neq0$ is integral iff its monic irreducible polynomial $x^2 -2ax +(a^2 -db^2)$ belongs to $\mathbb{Z}[x]$ . Therefore, $2a$ , $2b$ are integers. If $a=(2k+1)/2$ , for $k\in\mathbb{Z}$ , then it is easy to see that $a^2 - db^2 \in \mathbb{Z}$ iff $b=(2l+1)/2$ for some $l\in\mathbb{Z}$ , and $(2k+1)^2 - d(2l+1)^2$ is divisible by $4$ . The latter implies that $d$ is a quadratic residue modulo $4$ , i.e. $d\equiv 1 \bmod 4$ . In turn, if $d\equiv 1 \bmod 4$ then every element $(2k+1)/2 +(2l+1)\sqrt{d}/2$ is integral. Thus, integral elements of $\mathbb{Q}(\sqrt{d})$ are equal to $\mathbb{Z}[\sqrt{d}]$ if $d\not \equiv 1 \bmod 4$ , and $\mathbb{Z}[(1+\sqrt{d})/2]$ if $d\equiv 1 \bmod 4$ . Can someone explain why $\sqrt{d}$ for integer $d$ is integral, and why the monic irreducible polynomial of an integral element $a+b\sqrt{d}$ is of the form $x^2 -2ax +(a^2 -db^2)$ ?","['abstract-algebra', 'algebraic-number-theory']"
1237569,Show that $f$ is one-to-one if and only if it is onto.,"Suppose that $f$ is a function from A to B, where A and B are finite sets with $| A |= |B|$. Show that $f$ is one-to-one if and only if it is onto. How should I begin?",['functions']
1237585,Book Suggestion for Statistics,"I recently finished my Masters in Mathematics, more incline to analysis and algebra. I do not know if I should blame my school for laying a poor foundation in statistics or I should blame myself for avoiding it completely all the while till date (I have not done any course work on it). I am absolutely zero in Statistics, and presently for a competitive exam I am suppose to write in two months (CSIR-UGC NET Examination), $1/4^{th}$ of the questions are from Stats (Other 3/4th portion includes analysis, algebra and differential equations). Can someone suggest a good book which like ""Schaum's Outline for Linear Algebra"" (I found it a good self contained book, with bare minimum facts required for survival for newbies, and I use it to revise what I studied in past) for statistics? I came to know about ""Schaum's Outline of Statistics"" and it is priced very high here in India, and I don't know if it is worth the money. There are other Schaum's Outline books as well for statistics but I do not know which one to buy. I did go through similar posts and checked out the suggestions there, but they are proper complete text books. Can someone suggest some good book(s) for statistics, starting from zero, concise with material up to Masters (or at least undergraduate) syllabus? Thanks in advance.","['book-recommendation', 'statistics']"
1237593,Lebesgue non-measurable function,"Can we give an example of Lebesgue non-measurable function, for which set $\{x: f(x)=C\}~\forall C\in\mathbb{R}$ is measurable? Thanks.","['analysis', 'real-analysis', 'measure-theory']"
1237604,"How to evaluate $\log x$ to high precision ""by hand""","I want to prove $$\log 2<\frac{253}{365}.$$ This evaluates to $0.693147\ldots<0.693151\ldots$, so it checks out. (The source of this otherwise obscure numerical problem is in the verification of the Birthday problem .) If you had to do the calculation by hand, what method or series would you use to minimize the number of operations and/or size of the arguments involved in order to get this result? I'm doing a formal computer proof, so it's not exactly by hand, but I still want to minimize the number of evaluations needed to get to this result. One method is by rewriting it into $2<e^{253/365}$ and using the power series; since it is a positive series you know you can stop once you have exceeded $2$. Working this out, it seems you need the terms $n=0,\dots,7$ of the sum, and then it works out to $$2<\sum_{n=0}^7\frac{(253/365)^n}{n!}=\frac{724987549742673918011}{362492763907870312500},$$ which involves rather larger numbers than I'd like. There is also the limit $(1+\frac{253}{365n})^n\to$ $e^{253/365}$, but the convergence on this is not so good; it doesn't get past $2$ until $n=68551$, at which point we are talking about numbers with $507162$ digits. For $\log 2$ there is of course the terribly converging alternate series $\log 2=\sum_{n=1}^\infty\frac{-(-1)^n}n$, which requires $71339$ terms to get the desired bound. This can be improved by pushing the series into its geometrically convergent region as $\log 2=2\log{\sqrt 2}=-2\sum_{n=1}^\infty \frac{(1-\sqrt2)^n}n$, but now there is the added complication of estimating $\sqrt 2$ to sufficient precision. Assuming that $\sqrt 2$ is known exactly, you need to take this series out to $12$ terms, at which point we are verifying $$\frac{1959675656 \sqrt2-2771399891}{1011780}<0\Leftarrow2771399891^2>1959675656^2\cdot 2.$$ What other methods are there to do a calculation like this? Is there a way to use a root-finding method like Newton's to get a strict bound out with fast convergence?","['approximation', 'real-analysis', 'inequality']"
1237613,Gradient of the Fourier transform of a function,"I am wondering if there is a simple way to express the first variation of the Fourier transform of a function as a function of said function. In other words, if $g:x\mapsto F(f)(x)$, where $F(f)$ is the Fourier transform of f, is there a simple expression for $\dfrac{\partial g}{\partial f} $?
Thanks in advance for any insight or reference!","['calculus-of-variations', 'fourier-analysis', 'functional-analysis', 'derivatives']"
1237647,How many subsets of $S$ are there that contain $x$ but do not contain $y$?,"Let $S$ be a set of size $37$, and let $x$ and $y$ be two distinct elements of $S$. How many subsets of $S$ are there that contain $x$ but do not contain $y$? This question is on a practice exam that I am reviewing for tomorrow. The answer key says that the answer to this question is $2^{37} - 2^{35}$. I fail to see how this is correct as $2^{37}$ would be the amount of all subsets and $2^{35}$ is the amount of subsets containing x and y (I believe), therefore the answer would be the # of subsets that do not contain x or y, which doesn't answer the question. I thought the answer should be $2^{36} - 2^{35}$, where $2^{36}$ is the number of subsets containing $x$, taking away the number of subsets that contain both $x$ and $y$, therefore being left with the number of subsets that contain $x$ and not $y$. Can someone please confirm that I am right or explain to me why I am wrong?","['elementary-set-theory', 'discrete-mathematics']"
1237655,Pseudo-Cauchy sequence,"I have never seen this terminology before, so I will provide the given definition. A Pseudo-Cauchy sequence is : A sequence $(a_n)$ if for any $\epsilon > 0$ there exists $N \in \mathbb{N}$ such that $|a_{n+1} - a_n | \leq \epsilon \space \forall \space n \geq  N$ So then my question is that is a pseudo-cauchy sequence always converging?","['cauchy-sequences', 'real-analysis']"
1237693,Let $x \in \mathbb Q \setminus \{0\}$ and $y \in \mathbb R\setminus \mathbb Q$. Prove that $\frac{x}{y} \in \mathbb R \setminus \mathbb Q$,Let $x \in \mathbb Q\setminus  \{0\}$ and $y \in \mathbb R\setminus \mathbb Q.$ Prove that $\frac{x}{y} \in \mathbb R \setminus \mathbb Q$ I saw this question in a basic analysis test but it confuses me because intuitively it makes sense but how do you show mathematically that the set of rationals is not in the solution space?,"['analysis', 'real-analysis']"
1237694,Let a be a positive number. Then $\lim_{n \to \infty}[\frac{1}{a+n}+\frac{1}{2a+n}+\cdots +\frac{1}{na+n}]$,"Problem : Let $a$ be a positive number. Then $$\lim_{n \to \infty}\left[\frac{1}{a+n}+\frac{1}{2a+n}+\cdots +\frac{1}{na+n}\right]$$ Please suggest how to proceed in such limit problems, will be of great help thanks.","['calculus', 'limits']"
1237713,Proving $(p\to q)\land(p\to r) \equiv p\to(q\land r)$ using logic laws -- short cut or incorrect?,"Working through this problem: Using logic laws, show that the following are logically equivalent: $$(p\to q)\land(p\to r)\qquad\text{and}\qquad p\to(q\land r).$$ The way I did the problem is short and straight forward. I don't see any problems with it but want to be sure its kosher the way I did it. My solution followed by the book's solution:","['logic', 'proof-verification', 'discrete-mathematics', 'propositional-calculus']"
1237736,Proving $A$ is a subset of $B$,"I'm trying to understand the proof behind showing a set is a subset of another set, but I'm struggle to do so. Can some one help using this example to show: $A \subseteq B$? Here $A = \{x | x = 4n + 3,  n \in \mathbb{N}\}$, $B = \{x | x = 2n+1,  n \in \mathbb{N}\}$.","['elementary-set-theory', 'discrete-mathematics']"
1237743,Classify the singularities of the function .,"Classify the singularities of the function $\frac{1-\cos(z)}{z^2(z-1)}$. I think my answer may be that I have a simple pole at $z=0$ and a removable singularitie at $z=-1$ however i am not too sure. Some help, appreciated!",['complex-analysis']
1237746,A question about unitary block matrix,"For $n,m \in \mathbb N$, let $M_{n,m}(\mathbb C)$ denote the set of complex $n \times m$ matrices and put $M_{n}(\mathbb C):=M_{n,n}(\mathbb C)$. For matrices $A \in M_{n}(\mathbb C), B \in M_{n,m}(\mathbb C), C \in M_{m,n}(\mathbb C)$ and $D \in M_{m}(\mathbb C)$, we define the matrix $P \in M_{m+n}(\mathbb C)$ as
$$P : = \begin{pmatrix}A &B \\
C & D
\end{pmatrix}.$$
Give a necessary and sufficient condition that $P$ is unitary. My attempt:
We can find that $$P^* =  \begin{pmatrix}\overline{A^T} &\overline{C^T} \\
\overline{B^T} & \overline{D^T}
\end{pmatrix}.$$
Therefore, $P$ is unitary iff $PP*=I_{m+n}$ ($I$ is the identity matrix) iff
$$\begin{pmatrix}A\overline{A^T} +B\overline{B^T}&A\overline{C^T} +B\overline{D^T}\\
C\overline{A^T}+D\overline{B^T} &C\overline{C^T}+ D\overline{D^T}
\end{pmatrix} = I_{m+n}.$$
Then we end up with 
$$A\overline{A^T} +B\overline{B^T}= I_n, A\overline{C^T} +B\overline{D^T}=0_{n,m}, C\overline{A^T}+D\overline{B^T} =0_{m,n}, C\overline{C^T}+ D\overline{D^T}=I_m$$
is the necessary and sufficient condition that $P$ is unitary. Is that a final answer? Can we find another better or more explicit answer?","['linear-algebra', 'matrices']"
1237755,How to understand the product of two conditional probabilities?,"I am struggling a  little bit with making sense of the distribution of bigrams in an artificial language I randomly generated from english. Every word occurs with an equal probability but the syllables and phones that make up the word have a specific distribution such that the transitional probabilities within a word are much higher than between words. I am trying to show that the information entropy of a word (log2(1/#words)) is the summation of the IE of it's individual components.  I tried working it out on my own and got stuck, maybe someone could help get my gears going again? So I am aware of the product rule: p(A,B) = p(A|B) * p(B) Where A and B are considered independent events if p(A|B) = p(A,B)/p(B) = p(A) If A and B are considered independent events then we can define the probability of p(A,B) as p(A) * p(B). However what does it mean when you multiply two separate conditional probabilities in this fashion: p(B|A) * p(C|B)? Does this simplify to p(A, B, C)? Also does this generalize when we write the information entropy of A, B and C? I.e] H(A,B,C) = H(B|A) + H(C|B) Where H(x) = log2(1/p(x)) Note that I am interested in the case where A, B and C occur in that exact order.","['probability', 'information-theory']"
1237764,Find a series convergent $\sum a_n$ such that $\sum\sqrt{a_n/n}$ diverges.,"This is part of an exercise 8.22 from Apostol's Mathematical Analysis.  I've looked at things like, $a_n=1/\sqrt n-1/\sqrt{n+1}$, and $a_n=1/\log n^{\log n}$, but I can't seem to find anything that works.","['sequences-and-series', 'real-analysis']"
1237772,A linear non homogeneus recurrence relation,"Im using the minimax algorithm for a very simple game and when counting the tree nodes found the recurrence $T(n)=T(n-1)+T(n-2)+1$, with $0$ and $1$ as initial values. I tried generating functions: $G(x)=\sum_{n=0}^\infty T(n)x^n$ and then \begin{align*}
G(x) &= 0+1+\sum_{n=2}^\infty (T(n-1)+T(n-2)+1)x^n \\
     &= 1+xG(x)+x^2G(x)+x^n \\
     &= \frac{1+\sum x^n}{1-x-x^2}
\end{align*}
and now I dont know whath to do. Any help please?",['discrete-mathematics']
1237798,sum approximation of a Lipschitz-continuous function,"Let $f: [0, 1] \to \mathbb{R}$ be a Lipschitz continuous function with a Lipschitz constant $L > 0$, meaning: $$|f(x) - f(y)| โค L|x - y|  \space\space\space \forall x, y \in [0, 1]$$ For the approximation of $f$ using Riemann-sums with equidistant supporting points, proof the following statement: $$\left|\int_0^1 f(x)dx - \frac{1}{n} \sum_{k=1}^n f\left(\frac{k}{n}\right) \right|\space โค \frac{L}{n} $$ Now, as I haven't worked much with Riemann sums yet, I thought that solving this using the mean value theorem for integration might be a proper way, but I haven't come so far yet. Thanks in advance!","['analysis', 'real-analysis', 'definite-integrals', 'integration']"
1237826,How we can find the sign for trigonometric functions without graph,"For $\sin(x)$ or $\cos(x)$ etc. how we can show that it is negative on $\left[\pi ,2\pi \right]$ ? without graph? So if we have $\sin(2x)$ or $\cos(2x)$ how we can find the sign on $\left[0,2\pi \right]$, but don't tell me $\sin(2x)$ gets its negative value like in the interval $\sin(x)$ on $\left[\pi ,2\pi \right]$ it is negative and that's the reason .. Can we prove that with derivative of the function?","['calculus', 'trigonometry']"
1237862,What does it mean for a manifold to be oriented?,"I'm currently working through Spivak's Calculus on Manifolds . I've got to Stokes' Theorem, which is stated thus (the bold is my emphasis): Stokes' Theorem If $M$ is a compact oriented $k$-dimensional manifold-with-boundary and $\omega$ is a $(k-1)$-form on $M$, then 
$$\int_M d \omega = \int_{\partial M} \omega $$ I've read and understood the proof Spivak gives, but I'm unsure about what oriented means. In applications (such as in fluid mechanics), I've sometimes seen that this condition is ignored, or hand-waved over, but it's obviously key. I understand what is meant by an orientation (the fact that any non-zero $\omega \in \Lambda (\mathbb{R}^n)$ splits the bases of a linear space into two groups - those with $\omega(v_1,...,v_n) >0$ and those with $\omega(v_1,...,v_n) <0$, and these two groups are orientations for $V$), but I don't see how I would verify that a given manifold-with-boundary is oriented (other than it having something to do with the orientation of the tangent space). To make things more concrete, I know (i.e. have it on good authority) that the Mรถbius Band is not oriented. However, I know that the 2-sphere is oriented. So, in short: Given a manifold-with-boundary, how do we know if it's oriented or not?","['manifolds', 'manifolds-with-boundary', 'integration']"
1237885,How to extended a unitary operator to a larger space?,"Problem (the following is the exercise problem from Neilson and Chuang) Suppose $V$ is a Hilbert space with a subspace $W$. Suppose $U: W\rightarrow V$ is a linear operator which preserves inner products, that is, for any $\left|w_1\right>$ and $\left|w_2\right>$ in $W$, $$\left <w_1|U^ {\dagger} U|w_2\right> = \left<w_1|w_2\right> $$ Prove that there exists a unitary operator $U':V\rightarrow V$ which extends $U$. That is, $U'\left|w\right>=U\left|w\right>$ for all $\left|w\right>$ in $W$, but $U'$ is defined on the entire space $V$. Usually we omit the prime symbol $'$ and just write $U$ to denote the extension. Solution Following is the solution I found online from here Solution . It says that the desired operator can be defined as   $U'=U\otimes I$, where the identity is defined only on $V \perp W$. My approach and doubt In the solution I found above the operator $U'=U\otimes I$, will act on a space with dimension $m*n$ where $m$ is dimension of space $W$ and $n$ is dimension of space $V \perp W$, but we wanted an operator which acted on space $V$ which has dimension $m+n$. I know that the operator $U$ will have range as some space $W^{'}$ ( some subspace of space $V$ ) of the same dimension as space $W$. What I came up as a solution was let $\{i\}$ and $\{j\}$ for $1<=i<=m$, be the orthonormal basis for spaces $W$ and $W^{'}$ respectively such that $U=\sum | j \rangle \langle i| $, then we can extend the orthonormal basis of space $W$ to an orthonormal basis of space $V$ ie. $\{i\}$ for $1<=i<=m+n$ and the define $U^{'}=\sum | j \rangle \langle i| $. But I was still doubtful. Am I correct that the solution I found has a flaw and is my approach correct ?","['vector-spaces', 'linear-algebra']"
1237910,Invertiblity of the Derivative Matrix ?! (to use Inverse function Theorem),"In the Analysis2 midterm exam, we had the following problem: Let the equation $a_nx^n+\cdots+a_1x+a_0=0$ has $n$ simple real roots (distinct) $\{\alpha_1,\cdots,\alpha_n\}$. Prove that the above equation has still $n$ distinct real roots when the change in coefficients is small enough ! I'm pretty sure that $(a_1,\cdots,a_n,\alpha)\mapsto a_n\alpha^n+\cdots+a_1\alpha+a_0$ plus implicit function theorem will work. But it didn't came to my mind. Instead, I thought that the coefficients are $C^\infty$ function of the roots by Vietta Theorem . So I hoped the map $\psi:(\alpha_1,\cdots,\alpha_n)\mapsto(a_0,\cdots,a_{n-1})$ has a full rank derivative at the current roots and start to apply Inverse function Theorem to conclude that, locally, $(\alpha_1,\cdots,\alpha_n)$ is $C^\infty$ diffeomorphism map of $(a_1,\cdots,a_n)$. Hence, I conjectured the following proposition : Conjecture. We know by Vietta's Theorem that :
  $$\left\{\begin{array}{ll}
\psi_1=a_0=(-1)^n \alpha_1\cdots\alpha_n\\
\psi_2=a_1=(-1)^{n-1} \displaystyle\sum_{r=1}^n \alpha_1\cdots\hat{\alpha_r}\cdots\alpha_n\\
\vdots\\
\psi_{n-1}=a_{n-2}=\displaystyle\sum_{i,j=1}^n \alpha_i\alpha_j\\
\psi_{n}=a_{n-1}=-(\alpha_1+\cdots+\alpha_n)
\end{array}\right.$$ Then the matrix 
  $$D_{(\alpha_1,\cdots,\alpha_n)}\psi=\left[\begin{matrix}
\frac{\partial\psi_1}{\partial\alpha_1}&\cdots&\frac{\partial\psi_1}{\partial\alpha_n}\\
\vdots&\ddots\\
\frac{\partial\psi_n}{\partial\alpha_1}&&\frac{\partial\psi_n}{\partial\alpha_n}
\end{matrix}\right]=
\left(\begin{matrix}
(-1)^n\alpha_2\cdots\alpha_{n}&(-1)^n\alpha_1\alpha_3\cdots\alpha_{n}&\color{red}{\cdots}&(-1)^n\alpha_1\cdots\alpha_{n-1}\\
\color{red}{\vdots}&\color{red}{\ddots}&\color{red}{\vdots}\\
\alpha_2+\cdots+\alpha_{n}&\alpha_1+\alpha_3+\cdots+\alpha_{n}&\color{red}{\cdots}&\alpha_1+\cdots+\alpha_{n-1}\\
-1&-1&\color{red}{\cdots}&-1
\end{matrix}\right)$$
  is Invertible , whenever $\alpha_j$s are pairwise distinct. For case $n=2$ and $n=3$, I prove that $\det\big( D_{(\alpha_1,\cdots,\alpha_n)}\psi\big)=0$ if and only if $\alpha_i=\alpha_j$ for some $i\neq j$. But I don't know what to do for general $n$. Is it a famous matrix ? Is this conjecture correct for general $n$? Proof for $n=2$ and $n=3$ : n=2 :$\quad D_{(\alpha,\beta)}\psi=\left[\begin{matrix}
\beta&\alpha\\
-1&-1
\end{matrix}\right]$. So, $\boxed{\det(D_{\alpha,\beta}\psi)=0 \leftrightarrow \alpha=\beta\rightarrow\bot}$ n=3 $ :\quad D_{(\alpha,\beta,\gamma)}\psi=\left[\begin{matrix}
-\beta\gamma&-\alpha\gamma&-\alpha\beta\\
\beta+\gamma&\alpha+\gamma&\alpha+\beta\\
-1&-1&-1
\end{matrix}\right]
$. Now by computing determinant respect to the last row: $\begin{align}
\det(D_{(\alpha,\beta,\gamma)}\psi)= +\big[-\alpha^2\gamma-\alpha\beta\gamma+\alpha^2\beta+\alpha\beta\gamma\big]&-\big[-\alpha\beta\gamma-\beta^2\gamma+\alpha\beta^2+\alpha\beta\gamma\big]\\
&-\big[-\alpha\beta\gamma-\beta\gamma^2+\alpha\beta\gamma+\alpha\gamma^2\big]
\end{align}$.
$$\Rightarrow\det(D_{(\alpha,\beta,\gamma)}\psi)=
[\alpha-\beta]\color{red}{\gamma^2}+[\beta^2-\alpha^2]\color{red}{\gamma}+[\alpha\beta(\alpha-\beta)] $$
Now the equation $\det(D_{(\alpha,\beta,\gamma)}\psi)=0$, while $\alpha\neq\beta$, becomes the following quadratic equation respect to $\gamma$ :
$$\boxed{(\gamma-\alpha)(\gamma-\beta)=\color{red}{\gamma^2}-(\alpha+\beta)\color{red}{\gamma}+\alpha\beta=0
\leftrightarrow \gamma=\alpha\vee\gamma=\beta\longrightarrow\bot}$$","['implicit-function-theorem', 'determinant', 'real-analysis', 'linear-algebra', 'derivatives']"
1237936,First axiom of sheaves: in noetherian topological spaces the direct limit presheaf is a sheaf.,"Consider a topological space $X$ and a direct limit of sheaves and morphisms $\{ \cal{F}_i, f_{ij}\}$. Define the direct limit presheaf by $U \to \varinjlim \cal{F}_i $. In general this is just a presheaf. If $X$ is noetherian it is a sheaf, and this is the result I want to prove. My attemp: I tried to prove the first axiom of sheaves (I suspect that the second will need similar arguments). Let be an open set $U$ and an open cover $\{ V_i \}_i$ of $U$. Since $X$ is noetherian it can be can supposed that the cover is finite. Let $s \in \varinjlim \cal{F_i(U)}$ such that $s|_{V_i}=0 \quad \forall i$. The goal is to show that then $s=0$. Being $s \in \varinjlim \cal{F_i(U)}$, we know that there exists some $j / \quad s=f_j(U)(t_j)$ for some $t_j \in \cal{F}_j(U)$ ($f_i(U)$ are the morphisms of the direct limit of the abealian groups $\cal{F}_i(U)$ ). I can consider the germs ${t_j}_{V_i}$. If I could show that $s|_{V_i}=0 \Rightarrow {t_j}_{V_i}=0$ then the problem would be done, because the $\cal{F}_i$ are sheaves, in particular they verify the first axiom of sheaves, so $t_j=0$ and then $s=f_j(t_j)=0$.","['algebraic-geometry', 'sheaf-theory']"
1237964,Sierpinski (Triangle) for Other Polygons,"The Sierpinski triangle can be ""generated"" by the algorihm where you start in the triangle, pick a vertex at random, then move half the distant towards it, draw a dot and then repeat this. I wasn't able to find anything on whether doing this for other polygons produces similar sort of structures, so I tried it out and this is what I got: After these there just seems to be a small hole in the middle. Well the square isn't interesting. But what about the holes in the pentagon and the looming star structure in the hexagon? Are these of any interest?","['geometry', 'fractals']"
1237976,"For matrices, if $AB=BA$, then does it follow that $B^{2}A=AB^{2}$?","Suppose $AB=BA$ ($A, B$ are $n\times n$ matrices). Does that mean $B^{2}A=AB^{2}$ ?
I looked for counter cases and couldn't find any. I tried to prove this by multiplying both sides and comparing, but I got stuck since I don't know how to effectively use the fact that $AB = BA$.
Any advice or general direction would be greatly appreciated.","['linear-algebra', 'matrices']"
1238017,Derivative of a linear basis function over a moving mesh,"Given a moving mesh $0=x_0(t)<x_1(t)<\cdots<x_N(t)<x_{N+1}(t)=1,$ where $t$ denotes the current time so that the mesh is moving with time. The linear basis function is then defined as
$\phi_j(x,t)=\dfrac{x-x_{j-1}(t)}{x_j(t)-x_{j-1}(t)},$ for $x\in[x_{j-1}(t),x_j(t)]$ and $\phi_j(x,t)=\dfrac{x_{j+1}(t)-x}{x_{j+1}(t)-x_j(t)},$ for $x\in[x_j(t),x_{j+1}(t)]$ and $0$ otherwise, for all $j=1,\cdots,N.$ Then it comes with the conclusion that a direct calculation shows that 
$$
\dfrac{\partial\phi_j}{\partial t}(x,t)=-\dfrac{\partial\phi_j}{\partial x}(x,t)X_t(x,t),
$$
where $X_t(x,t)$ is the linear interpolant of the nodal mesh speeds, i.e,
$$X_t(x,t)=\sum\limits_{j=1}^N\dfrac{dx_j}{dt}(t)\phi_j(x,t).$$ However, I am not able to deduce this equation. Is anybody able to do that?","['finite-element-method', 'derivatives']"
1238027,In a Hausdorff space the intersection of a chain of compact connected subspaces is compact and connected,"Prove that if $X$ is Hausdorff and $\mathfrak{C}$ is a nonempty chain of compact and connected subsets of $X$, then $\bigcap \mathfrak{C}$ is compact and connected. Here are the definitions which were provided: A family $\mathfrak{F}$ of sets is a chain if for all $A,B \in \mathfrak{F}$ either $A \subset B$, or $B \subset A$.  (If $\mathfrak{C}$ is a nonempty chain of nonempty compact sets, then $\bigcap \mathfrak{C} \neq \emptyset$.) A space is Hausdorff (or $T_2$) if for all distinct $a, b \in X$ there are $U, V \in \mathcal{T}$ so that $a \in U$, $b \in V$ and $U \cap V = \emptyset$. Here, $\mathcal{T}$ is the topology on $X$. My confusion starts in what seems to me like a contradiction between definitions. We are assuming that $X$ is Hausdorff, so there is a separation of $X$ so there are disjoint $U, V \in \mathcal{T}$ with $a \in U$ and $b \in V$. But we are also given that there is a chain of compact and connected sets $\mathfrak{C}$, which by definition, $U \subset V$ or $V \subset U$. But they are supposed to be disjoint according to the Hausdorff property. Can somebody please explain this to me and assist me in solving this problem with a proper proof? Many thanks in advance for your time and patience, it is greatly appreciated.","['connectedness', 'general-topology', 'compactness']"
1238039,Solving $y=\prod_{n=1}^{\infty}\frac{d^ny}{dx^n}$,"There is the trivial $y=0$, but beyond that, could there be further solutions for $y$ in terms of $x$ such that $$y=\prod_{n=1}^{\infty}\frac{d^ny}{dx^n}\mbox{ pointwise}$$ ? I posed this problem to myself, so I have no idea. I began with:
$$\ln(y)=\sum_{n=1}^{\infty}\ln\left(\frac{d^ny}{dx^n}\right)\mbox{ pointwise}$$
$$\frac{y'(x)}{y(x)}=\sum_{n=1}^{\infty}\frac{y^{(n+1)}(x)}{y^{(n)}(x)}\mbox{ pointwise}$$
And I've fiddled with various manipulations, but without success. Any ideas?","['sequences-and-series', 'infinite-product', 'ordinary-differential-equations']"
1238051,"If two stochastic processes are modifications of each other and almost surely continuous from the right, then they are undistinguishable","Let $(\Omega,\mathcal{A},\operatorname{P})$ be a probability space $I\subseteq\mathbb{R}$ $E$ be a metric space and $\mathcal{E}:=\mathcal{B}(E)$ be the Borel-$\sigma$-algebra on $E$ $X:=(X_t)_{t\in I}$ and $Y:=(Y_t)_{t\in I}$ be stochastic processes on $(\Omega,\mathcal{E}$ with values in$(E,\mathcal{E})$ $N_t:=\left\{X_t\ne Y_t\right\}\color{blue}{:=\left\{\omega\in\Omega:X_t(\omega)\ne Y_t(\omega)\right\}}$ for $t\in I$ Now, suppose $I$ is an intervall $\operatorname{P}[N_t]=0$ for all $t\in I$ $\color{blue}{\text{(We say that }X\text{ and }Y\text{ are modifications of each other})}$ For $\operatorname{P}$-almost every $\omega\in\Omega$ $$I\to E\;,\;\;\;t\mapsto X_t(\omega)$$ is continuous from the right $\color{blue}{\text{(We say that the paths of }X\text{ are almost surely continuous from the right)}}$ and the same holds for $Y$ Let $$R':=\left\{X,Y\text{ are continuous from the right}\right\}$$ By our assumption, there exists a $R\in\mathcal{A}$ such that $R\subseteq R'$ and $\operatorname{P}[R]=1$. Now, let $$\tilde{I}:=\begin{cases}I\cap\mathbb{Q}&\text{, if }I\text{ is right-open}\\I\cap\mathbb{Q}\cup\max I&\text{, otherwise}\end{cases}$$ and $\tilde{N}:=\bigcup_{q\in\tilde{I}}N_q$. Since $X$ and $Y$ are modifications of each other, it's easy to prove $\operatorname{P}[\tilde{N}]=0$. My question: Clearly, the inclusion $$N_t\cap R\subseteq\bigcup_{q\in\tilde{I}:q\ge t}\left(N_q\cap R\right)\;\;\;\text{for all }t\in I\tag{1}$$ is somehow implied by the continuity from the right. But how exactly can we derive $(1)$ ?","['probability-theory', 'probability', 'stochastic-processes', 'measure-theory']"
1238074,How to Calculate csc(2.85) in Calculator?,"In my calculator (TI-84), there are only $sin, cos,$ and $tan$ commands (and inverse sin, inverse cos, inverse tan). I had a question that was as follows: Calculate $csc(2.85)$ in which I was permitted use of the TI-84 calculator. As $csc$ is the inverse of $sin$, I wrote $csc(2.85) = 1/sin(2.85)$, which came out to be $3.4785,$ which I believe to be the right answer. I wanted to take $csc^{-1}$ of $3.4785$ to check my answer, however when I did so, the answer came out to be $0.29159$ rather than $2.85.$ I checked with other random numbers that taking the inverse of the answer should give the correct number of radians back. Where did I go wrong?",['trigonometry']
1238121,Is there anything known about the value where the Euler and Hadamard products for $\zeta(s)$ are equal?,"Take the Hadamard product for the Riemann $\xi$-function ($\rho$ is a non-trivial zero of $\zeta(s)$): $$\xi(s) =\frac12\, s\,(s-1) \,\pi^{-\frac{s}{2}}\, \Gamma\left(\frac{s}{2}\right)\, \zeta(s) =\frac12\,\prod_\rho \left(1- \frac{s}{\rho} \right) \left(1- \frac{s}{1-\rho} \right)$$ Firstly strip out the factor $\frac12$ on both sides and then find the solution for: $$s\,(s-1) \,\pi^{-\frac{s}{2}}\, \Gamma\left(\frac{s}{2}\right)=1$$
This yields the unique value of $\mu=3.171242735975879847\dots$ and therefore: $$\zeta(\mu) = \prod_\rho \left(1- \frac{\mu}{\rho} \right) \left(1- \frac{\mu}{1-\rho} \right)$$
Hence, since $\mu \gt 1$ this gives a direct relation between the products of primes and $\rho$'s: $$\prod_{p \in \mathbb{P}} \left( \dfrac{1}{1-p^{-\mu}} \right)=\prod_\rho \left(1- \frac{\mu}{\rho} \right) \left(1- \frac{\mu}{1-\rho} \right)$$ Is there anything known about $\mu$? (I tried link to known constants on Wolfram, but no success)","['number-theory', 'infinite-product', 'riemann-zeta']"
1238129,"Joint Random Variable: Given f(x,y), find P(X>Y)","There are 2 continuous random variables, X and Y. Say the joint pdf of (X,Y) is f(x,y). How do you find the P(X>Y) generally? Like I am not sure where to start with.","['probability', 'statistics', 'random-variables']"
1238152,Showing that $\sum\limits_{n=1}^{\infty} (a_1+2a_2+\dots+na_n)/n(n+1) = \sum\limits_{n=1}^\infty a_n $,Let $\sum\limits_{n=1}^{\infty} a_n$ a convergent series of positive terms. Show that $\sum\limits_{n=1}^{\infty} \frac{a_1+2a_2+\dots+na_n}{n(n+1)}$ converges to the same value of $\sum\limits_{n=1}^\infty a_n$ . I think I can use the Cauchy condensation test but I would like to know if there is an easier way.,"['sequences-and-series', 'calculus']"
1238160,"You are making cookies and add N chips to dough randomly, and split it into 100 equal cookies, again at random. How many chips should go into dough?","Question: You are making chocolate chip cookies. You add N chips randomly to the dough and you randomly split the dough into 100 equal cookies. How many chips should go into the dough to give a probability of at least 90% that every cookie has at least one chip? I tried to attempt to solve this using IID random variables. I am not sure how to set the problem up. I know that there should at least be 100 chocolate chips or else the cookies will not meet the ""at least 1 chip per cookie"" requirement and that there is 10% chance that the cookies do not have a chip.","['probability-theory', 'probability', 'statistics', 'discrete-mathematics']"
1238165,Prove $\lim\limits_{n \to \infty} \frac{2n^2+2}{3n^3+1}=0$ directly from the definition of limit.,"One of my homework questions is: Prove $\lim\limits_{n \to \infty} \frac{2n^2+2}{3n^3+1}=0$ directly
  from the definition of limit. In trying to follow: Prove that $\lim \limits_{n\to\infty}\frac{n}{n^2+1}  = 0$ from the definition , i have so far: Proof: It must be shown that for any $\epsilon>0$, there exists an integer $N$ such that $\left|\frac{2n^2+2}{3n^3+1}-0\right| <\epsilon$ whenever $n>N$. $\left|\frac{2n^2+2}{3n^3+1}-0\right|=\frac{2n^2+2}{3n^3+1}>\frac{2n^2}{3n^3}=\frac{2}{3n}$. Because in the problem im given, the left side of the inequality that im trying to simplify i guess, is instead greater than the right side, im stuck. And i dont understand the how or why behind the algebraic manipulations that the original author and one of the answers performs, in the linked question: $$\left|\frac{n}{n^2 + 1}\right| < \epsilon \text{ whenever }x \gt M.$$ $$ n \lt \epsilon(n^2 + 1) $$ $$n \lt  \epsilon n^2 + \epsilon$$ Truly do feel clueless at this point!","['limits', 'real-analysis', 'epsilon-delta']"
1238171,Question about the definition of independent events,"If you have two events A and B that are independent, then it is said that $p(A)p(B)=p(A\cap B)$, and illustrated in a venn diagram as two areas that do not overlap. The opposite goes for dependent events A and B, where if A is dependent on B, then $p(A|B)p(B)=p(A\cap B)$ and in the venn diagram, the areas representing these events do overlap. My questions are: Are there any instances where event A does overlap B but is not dependent on B? Are there instances where A does not overlap with B, but is dependent on B? My guess here is that there are no such cases, where if one instance is satisfied then it by definition calls for the other. For instance, regarding question 1, if A overlaps B, then a dependence between the two exists. For question 2, if A does not overlap B, then it cannot be dependent on it. Are these ideas correct, or are there any situations where this might not work?","['probability', 'statistics']"
1238181,Questions about the definition of convergence,"I am having difficulty understanding the definition of convergence. I've been rereading and looking at examples during this past week and I haven't made any progress. Definition: We say that {${a_{n}}$} converges to a point $a \in \mathbb{R}$ if for any $\epsilon$, there exists a positive integer $N$ such that for any $n \in \mathbb{N}$ with $n\geq N$, one has $|a_{n}-a|< \epsilon$. My questions: (1) What role does $\epsilon$ play? Is that the actual limit? I thought that $a$ is what we are ""assuming"" is the limit? (2) Why does $n \geq N$? I'm asking this because we were just given the definition of what it means to be convergent (in Real Analysis, not the Calculus sequence), no formal proof. (3) In various examples they are trying to set $N$ to be less than or equal to $\epsilon$. Why? My main issue is that I don't understand how the components of this definition work. I can follow the examples, but I'd rather understand why it works then just take it on blind faith. Thank you for any input/suggestions.","['real-analysis', 'definition']"
1238196,Energy inequalities with negative sobolev number.,"Let $\phi\in H^{s}$ such that the following energy inequality is true: $$\|\phi(t,\cdot)\|_s \le\int^t_0 C \| P\phi(t,\cdot)\|_s \, dt  $$ where $P$ is a strictly hyperbolic  linear operator. For concreteness, let $P$ be the wave operator $\square_{g}$. Now is the energy inequality true for $-s$? I have attempted the following: Let $\phi\in H^{-s}$. Then we can define $\psi=(1-\Delta)^{-s} \phi\in H^s$ So we have $$\|\phi\|_{-s}=\| \psi\|_s \le C \int \| P\psi\|_s $$ Now if we estimate $\| P\psi\|_s $ in terms of $\|\phi\|_{-s}$ and $\| P\phi\|_{-s}$ The proof will be over. Notice that $$P\phi=P(1-\Delta)^s \psi=(1-\Delta)^s P \psi+ [P,(1-\Delta)^s]\psi $$ Hence, $$\| P \psi\|_s \le  \Arrowvert P\phi\Arrowvert_{-s} +\|[P,(1-\Delta)^s]\psi\|_{-s} $$ Can someone point me out if there are some estimates for the  commutator? In the book ""The Cauchy problem in General Relativity "" by Ringstrom it is stated that the following proposition: Let $m$ and $l$ be non-negative integers, $\alpha\le l+m$, $u\in S$ and $f\in C^{\infty}$. Then
$$||f\partial^{\alpha}u||_{-m}\le C ||u||_{l}$$ gives the following bound for the commutator \begin{equation}
  C(||\psi||_{s}+||\psi_{t}||_{s-1})
\end{equation}
Although I am not clear how he gets it. Also he expresses the problem as a first order PDE. Is this necessary? I also think that the result can be shown using the theory of pseudo-differential operators. The idea will be two show that $$[P,(1-\Delta)^s]$$ is a bounded linear operator from $H^{s}$ to $H^{-s}$. We know that  $(1-\Delta)^s\in OPS^{2s}$ and that $P\in OPS^{2}$. Is there any theorem that might show the desire result?","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
1238240,Solving System of Boundary Value problem,"The boundary value problem: $$y'' + Q(t)y = f(t)$$
satisfying $$Ay(a) +By(b) = g$$
where A, B and Q are the matrices of order n.
After calculation, we can get the form of solution will be 
$$y(x) = -(A+B)^{-1}\left[\int_x^bB \cdot f(s)ds + \int_a^xA \cdot f(s)ds\right]$$
$$= \int_a^xD^{-1}A \cdot f(s)ds - \int_x^b D^{-1}B \cdot f(s)ds$$
where $D=A+B$
$$= \int_a^bG(x,s) \cdot f(s)ds$$
and $$G(x,s) = \begin{cases}D^{-1}A \text{ if } x >s \\ -D^{-1}B \text{ if } x < s\end{cases}$$
So, we used the Green function to get the solution of boundary value system. 
My question is what happened if the system is solvable, but $D$ is not invertible ?","['numerical-methods', 'ordinary-differential-equations']"
1238288,Combinatorics Question VS CS solution!,"I was wondering for some conceptual understanding to a question of this form: In how many ways may we choose three distinct integers from [1, 2, ..., 80] so that one of them is the average of the other two? I can solve this problem using basic computer science, but what is the correct way using combinatorics for something like this?","['discrete-mathematics', 'combinatorics']"
1238298,Galois action on the fibre of a morphism determined by a linear system,"If $X$ is an elliptic curve, let $P,Q\in X$, then $|P+Q|$ determines a morphism $g:X\to \mathbb{P}^1$. It is easy to see $K(X)/K(\mathbb{P}^1)$ is a Galois extension of degree 2. Let $\sigma$ be the nontrivial element in $Gal(K(X)/K(\mathbb{P}^1))$, then $\sigma$ induces an automorphism of $X$ which maps each fibre of $g$ to itself. My question is: how do we know $\sigma$ indeed interchanges the two elements in each fibre? My thanks in advance.","['elliptic-curves', 'algebraic-geometry']"
1238365,Is this differential equation separable?,"$$x\frac{dy}{dx}-y^2 = \frac{dy}{dx}+5$$ I have found that this equation is differentiable as shown in the following. $$x\frac{dy}{dx}-\frac{dy}{dx} = y^2+5$$ $$dy(\frac{x}{dx}-\frac{1}{dx}) = y^2+5$$ $$\frac{x-1}{dx} = \frac{y^2+5}{dy}$$ $$\frac{dx}{x-1} = \frac{dy}{y^2+5}$$ $$dx(\frac{1}{x-1}) = dy(\frac{1}{y^2+5})$$ Is this method valid? Or to be more specific, is the following equation valid? $$x\frac{dy}{dx}-\frac{dy}{dx} =dy(\frac{x}{dx}-\frac{1}{dx})  $$","['calculus', 'ordinary-differential-equations']"
1238403,Killing vector field along a geodesic,"I was trying to show that a Killing vector field satisfies the Jacobi Equation for a geodesic, just by assuming that 
\begin{equation}
\nabla_\mu X_\nu + \nabla_\nu X_\mu=0
\end{equation}
Indeed, if I take into account that $[X,\gamma']=0$ being $\gamma$ the geodesic, I can show it easily without using the Killing equation. Yet I can't understand how to prove that the commutator vanishes by using this equation.","['general-relativity', 'differential-geometry', 'mathematical-physics', 'geodesic']"
1238451,Prove $\lim\limits_{n \to \infty} \frac{n^2+1}{5n^2+n+1}=\frac{1}{5}$ directly from the definition of limit.,"Prove $\lim\limits_{n \to \infty} \frac{n^2+1}{5n^2+n+1}=\frac{1}{5}$ directly
  from the definition of limit. So far ive done this: Proof: It must be shown that for any $\epsilon>0$, there exists a positive integer $N$ such that for $n\in \mathbb{N}$ with $n\ge N$, 
one has $$\left|\frac{n^2+1}{5n^2+n+1}-\frac{1}{5}\right| <\epsilon \text{ (or equivalently,}\frac{1}{5}-\epsilon \lt \frac{n^2+1}{5n^2+n+1} \lt \frac{1}{5} + \epsilon \text{).}$$ $$\begin{align}
\left|\frac{n^2+1}{5n^2+n+1}-\frac{1}{5}\right| & = \left|\frac{(5(n^2+1))-(1(5n^2+n+1))}{5(5n^2+n+1)}\right| \\
 & = \left|\frac{5n^2+5-5n^2-n-1}{25n^2+5n+5}\right| \\ 
 & = \left|\frac{4-n}{25n^2+5n+5}\right| \\
\end{align}$$ How do i go about choosing the specific terms or factors of $n$ to compare against the numerator and denominator in this particular problem and i guess these type of problems in general? Like in the answer: https://math.stackexchange.com/a/553466/227134 How do you know to setup the inequality $4n+7\le5n$ for the numerator and so forth? I cant quite figure out what to put on the right side of $4-n\le $  ?","['limits', 'real-analysis']"
1238463,Multiple part problem concerning the proof that $\sum_{k=1}^n k^3=\left(\frac{n(n+1)}{2}\right)^2$ by induction,"So I'm having trouble with $c,d$ and $e$. For $c$ so far I have: Inductive Hypothesis: $(\frac{n(n+1)}{2})^2 = (\frac{(k+1)(k+2)}{2})^2$ is that correct?","['sequences-and-series', 'induction', 'discrete-mathematics']"
1238468,Let $\alpha(s)$ be a unit speed curve in $R^2$. Show $\kappa=|\frac{d\theta}{ds}|$,"I'm lost on solving the following problem. Let $\alpha(s)$ be a unit speed curve in $R^2$. Show $\kappa=|\frac{d\theta}{ds}|$, where $\theta$ is the angle between the positive $x$-axis and the tangent line to the curve $\alpha$ measured in the counterclockwise sense. I'm used to curves in $R^3$ and I'm not sure how to tackle this problem, which is restricted to $R^2$. I don't know what property of curvature I should be using. I would greatly appreciate any solutions, hints or suggestions.","['differential-geometry', 'vector-analysis', 'multivariable-calculus']"
1238475,Show two random variables have same distribution,"Let X, Y be two non-negative random variables satisfying the condition $\mathbb{E}[X^\alpha] = \mathbb{E}[Y^\alpha]$ for all $\alpha \in (0, 1/2)$. How can one show that X and Y are equal in distribution? Edit: (only if you find this helpful) $\mathbb{E}[X], \mathbb{E}[Y]$ also exist, but a priori one does not know whether they are equal or not. If you believe that the claim is wrong, I would also be happy to see counterexamples, or at least some intuitive explanations.","['probability-theory', 'probability']"
1238494,"If $\alpha$ is a unit speed curve of constant curvature lying in a sphere, then $\alpha$ is a circle.","I'm trying to solve the following problem but got stuck along the way. I would like some help on getting this through. Prove that if $\alpha$ is a unit speed curve of constant curvature lying in a sphere, then $\alpha$ is a circle. Solution: My goal is to show that the torsion is zero. We have $\alpha \cdot \alpha =r^2$, so taking the derivatives, $T \cdot \alpha =0$. So we can let $\alpha=xN+yB$ for some functions $x$ and $y$. Now differentiating the previous equation again, we get $T' \cdot \alpha + T\cdot T=0$, so using Frenet Formula, we get $\kappa N \cdot \alpha =-1$, and another differentiation and Frenet Formula yield $\tau B\cdot \alpha =0$. So by the assumption on $\alpha$, we get $\tau y=0$. However, here is where I have a problem. If I know that $y\neq0$, then I'm done. But I cannot guarantee that $y$ is nonzero, so I can't show that $\tau$ must be zero. How can I solve this? I would greatly appreciate any solutions or suggestions.","['differential-geometry', 'vector-analysis']"
1238514,Finite measure space & sigma-finite measure space,"A measure space $(X, \Sigma, \mu)$ is finite if $\mu(X)<\infty$ . It is equivalent to saying that $(X, \Sigma, \mu)$ is finite if $\mu(E)<\infty$ for all $E \in \Sigma$ A measure space $(X, \Sigma, \mu)$ is $\sigma$ -finite if X is a countable union of sets with finite measure. Does $\sigma$ -finiteness imply that $\mu(E)<\infty$ for all $E \in \Sigma$ ? If $\mu(E)<\infty$ for all $E \in \Sigma$ , dose it imply $\sigma$ -finiteness or finiteness of a measure space?","['lebesgue-measure', 'lebesgue-integral', 'measure-theory']"
1238533,Find the coefficient of $x^4$ in the expansion of $(1 + 3x + 2x^3)^{12}$?,"I have not learnt the multinomial theorem yet, and was trying to approach this using the binomial theorem. I divided the terms as $a$ being $(1+3x)$ and $b$ being $2x^3$. I then used $${12\choose 3}(1+3x)^8(2x^3)^3$$. However now do I have to expand the second term again? Please let me know where I am getting wrong. It seems there should be an easier way to approach this problem.","['binomial-theorem', 'multinomial-coefficients', 'binomial-coefficients', 'discrete-mathematics']"
1238534,"Is ""imposing"" one function onto another ever used in mathematics?","First of all, let me define what I mean by ""imposing,"" and let me clarify that I've only studied this operation in 2D Euclidean space. Now then, to impose one function onto another, you need two things: A function upon which to impose, called the receiver . A function to impose, called the imposer . Now, let me first explain the concept generally. The idea is that, rather than graphing some function with respect to the x-axis, we treat the receiver as the x-axis, graphing the imposer with respect to receiver . So, what do I mean by ""graphing some function with respect to the x-axis?"" Well, first we'll let $p_0$ be the point on the x-axis at some $x$, and we'll let $l$ be the line which is normal to the x-axis at $p_0$. It should be clear that $p_0=(x,0)$ and that $l$ is a vertical line which passes through $p_0$. Then, for some function $g$, let $p_1$ be the point on $l$ whose distance is equal to $g(x)$. It should be clear that $p_1=(x,g(x))$ since the distance from $(x,0)$ to $(x,g(x))$ is equal to $g(x)$. If you do the previous procedure for all $x$ and plot every $p_1$ on a graph, you will have successfully graphed $g$ with respect to the x-axis. So, to reiterate my second paragraph, the idea is that we can graph any function with respect to some other function. The way we do this is by following the same procedure we used in the last paragraph. However, there are two main differences: Instead of letting $p_0$ be the point on the x-axis at some $x$, we let $p_0$ be the point on some parametric function $f(t)$, the receiver , for some $t$. Instead of letting $l$ be the line which is normal to the x-axis at $p_0$, we let $l$ be the line which is normal to $f$, the receiver , at $p_0$. For example, this is $g(t)=cos(t)$, the imposer , imposed upon $f(t)=(t,a \cdot sin(t))$, the receiver , where $a$ is simply a real value which oscillates between $-1$ and $1$ with time. Basically, $a$ is the reason functions below are moving. The black function which resembles a standing wave , as I said before, is an oscillating sine function, and it is also the receiver . The blue function which, if you look closely, occasionally looks like a cosine function is the function resulting from imposing $g(t)$ onto $f(t)$. The green line segments are to illustrate the act of finding the point on the normal of $f(t)$ at $t$ with a distance of $g(t)$, like we covered in the above paragraphs. If you're interested, here's the raw math to impose one function onto another: Given some parametric equation $f:f(t) = (x(t),y(t))$ upon which we wish to impose some function $g(t)$: $$
Let\;h(t)=\frac{\frac{d}{dt}(y(t))}{\frac{d}{dt}(x(t))}=f'(t)
$$
$$
Let\;j(t)=tan^{-1}(h(t))\pm\frac{\pi}{2}
$$
Then $g$ imposed upon $f$ becomes the following parametric equation, in terms of $t$.
$$
(g(t)cos(j(t))+x(t),g(t)sin(j(t))+y(t))
$$ I feel as though I should clarify now that, for most $f$ and $g$, this operation will produce two resultant functions. This fact is a result of the way I have defined this operation. That is, we are looking for any point $p_1$ on the normal line of $f$ at $p_0$ such that the distance between $p_0$ and $p_1$ is equal to $g(t)$. Put more simply, we're looking on a line for a point which is a specific distance away from another point on the line. We already know that, for any point $p$ on a line $l$, there will always be exactly two points on $l$ with a distance $\delta$ away from $p$, for all $\delta > 0$. This fact is the reason for which we are adding or subtracting $\frac{\pi}{2}$ in $j$. The only case I can think of wherein this operation does not produce two resultant functions is when $g(t) = 0$, as each resulting function will be exactly equal to the parametric $f(t)$; however, there very well may be more. So, ignoring any incorrect notation or terminology I might have used, is this type of transformation used anywhere in mathematics? If so, could you show me where I could get some more information on it?","['graphing-functions', 'parametric', 'calculus']"
1238572,Countable dense subsets of $\mathbb R$ are homeomorphic,"Suppose countable subsets $A,B$ of the real line $\mathbb R$ satisfy $\overline{A}=\overline{B}=\Bbb R$. How can one show that $A$ is homeomorphic to $B$? I even have no idea how to get a bijection between $A$ and $B$.",['general-topology']
1238581,The sequence $\frac{2}{2-u_n}$ diverges,"Let $(u_n)$ be a sequence defined with $u_{0}$ a real number such that $u_0 \notin \{0,1,2\}$ and $$u_{n+1} = \frac{2}{2-u_n}$$ Prove that $(u_n)$ diverges. I try to use the fact that this sequence fluctuates, having negatives values followed by values smaller than 1, then getting values bigger than 1 to get a contradiction using the definition of convergence. The problem is that I can't get any additional information after I find a value bigger than 1, because I can't eliminate the possibility that from that point, the sequence will be bound by 2. Am I missing something here? Is there another route I'm not considering?",['sequences-and-series']
1238595,Show $h:A \rightarrow B$ is continuous,"I am working through some practice questions, and I am not sure if I am on the right track with this one: Let $X = \cup_{nโฅ1}C_n$, be a space and assume that a map h : A โ B
  is such that each  $h: C_{n} โ B$, โn โฅ 1 is continuous.
  If $C_{n}$ โ Int$(C_{n+1})$, f is continuous I'm not sure if what I have done is too simple - and I am actually missing some important information. But I can't see why this argument would not be complete at the moment. Let $x$ be an arbitrary element of $X$. Since $C_{n} \in$ Int$(C_{n+1})$ for $\forall n \ge 1 $ and $X = \cup_{nโฅ1}C_n$ Then $x \in C_{i}$ for some $i \ge 1 \implies x \in C_i \subseteq$ Int$(C_{i+1}) \subseteq C_{i+1}$. Since $f:C_{i+1} \rightarrow Y$ is continuous $\implies f $ is continuous at $x$. Since the choice of $x$ was arbitrary $\implies $ f is continuous at every $x$. Hence $f:X \rightarrow Y$ is continuous. Thanks for any feedback you can give me!","['metric-spaces', 'real-analysis', 'general-topology']"
1238606,"Is any compact, path-connected subset of $\mathbb{R}^n$ the continuous image of $[0,1]$?","If $f:[0,1] \to \mathbb{R}^n$ is any continuous map, then the image $f([0,1])$ is a compact, path-connected set, which is easy to show using some elementary topology. My question is the converse: Namely, if $K \subset \mathbb{R}^n$ is compact and path-connected, then does there exist a continuous map $f:[0,1] \to \mathbb{R}^n$ such that $K = f([0,1])$? My attempt at the problem: I have a hunch that it might be true. For any $k \in \mathbb{N}$, there exists a continuous surjection $f:[0,1] \to [0,1]^k$, which can be realized with a space filling curve. Therefore, any finite-dimensional cube can be realized as the continuous image of $[0,1]$. Let $K \subset \mathbb{R}^n$ be compact and path-connected. My friend suggested that it would suffice to show that $K$ has the structure of a CW-complex with a finite number of cells, and then use the fact that any finite-dimensional cube is realized as the continuous image of $[0,1]$. However, I don't know if this is true. Edit: It turns out that the answer is even more interesting than I anticipated, and is provided by the HM theorem: http://en.wikipedia.org/wiki/Space-filling_curve#The_Hahn.E2.80.93Mazurkiewicz_theorem","['compactness', 'continuity', 'connectedness', 'general-topology', 'analysis']"
1238669,Separable ODE and singular solutions,"In most introductory ODE textbooks we can find the following definition: A separable first-order ODE is the one of the form $$y'=g(x)h(y)$$ and if $h(y)\neq0$, then the general solution is found by integration (using chain rule). Next, we must find every $y_0$ such that $h(y_0)=0$ and the constant function $y=y_0$ satisfies the above ODE (called singular solutions). I'm trying to learn this method in a rigorous way by using the existence and uniqueness of solution theorems and adding restrictions over $g$ and $h$, but can't deal with the following: Does there exist a solution $f$ defined in some $A\in \mathbb{R}$ such that there exists $x_0\in A$ such that $h(f(x_0))=0 $ but $f$ is not constant? I mean, if $f(x_0)=y_0$, then $h(y_0)=0$, so we can't divide by $h(y)$ and integrate using separable equation method. I know that $f(x)=y_0$ is a solution, but what can we say about a non-constant $f/f(x_0)=y_0$? Also, can you recommend books explaining this kind of solving-ODE's methods but in an absolutely rigorous way? EDIT : This edit is made after the bounty (I thought it was implicit in the spirit of the question, but maybe I couldn't state it properly due to my lack of expertise in english), but I'd really want to know what further hypothesis need to be given in order to assure that every singular solution must be constant. Any help is highly appreciated. Thanks and regards","['singular-solution', 'ordinary-differential-equations']"
1238680,"How to descent to smaller groups ""by chopping off a node of the Dynkin diagram""?","I read in section 2 of this paper : ""There is a well-defined chain to descent from $E_8$ to smaller
  groups by chopping off a node of the Dynkin diagram ."" What exactly is here referring to here? What is this process called to descent from a group to smaller groups and how does it work?","['lie-groups', 'group-theory', 'lie-algebras', 'dynkin-diagrams']"
1238694,How can I prove that $f$ is inner product function,"We know the polarization identity in inner product space :
$$\langle x,y\rangle= \frac{1}{4} (\|x+y\|^2-\|x-y\|^2) + \frac{i}{4} (\|x+iy\|^2-\|x-iy\|^2) $$
But the question is if we have $(X,\|\cdot\|)$ is normed space and the function $f$ defined by : 
$$f(x,y)= \frac{1}{4} (\|x+y\|^2-\|x-y\|^2) + \frac{i}{4} (\|x+iy\|^2-\|x-iy\|^2)$$ how can I prove that $f$ is inner product function without use the polarization identity ( I mean I can only use the properties of norm to prove $f$ is inner product function )","['inner-products', 'functional-analysis', 'normed-spaces']"
1238699,Find generating functions for the Perrin and Padovan sequences,"The Perrin sequence is defined by $a_0 = 3, a_1 = 0, a_2 = 2$ and $a_k = a_{k-2}+a_{k-3}$ for $k \ge 3$. The Padovan sequence is defined by $b_0 = 0, b_1=1, b_2=1$ and $b_k=b_{k-2}+b_{k-3}$ for $k\ge 3$. Find generating functions in the form of rational functions for the Perrin sequence and the Padovan sequence. I am a little bit confused about this question , any hint?","['recurrence-relations', 'generating-functions', 'discrete-mathematics']"
1238712,$a^2b + abc + a^2c + ac^2 + b^2a + b^2c + abc +bc^2$ factorisation,"I came across this from a university mathematics resource page but they do not provide answer to this. What I did was this: $(a^2+b^2+c^2)(a+b+c) - (a^3 + b^3 + c^3) + 2abc$ But I don't think this is the correct solution.
How should I spot how to factorise expression here? I wish to learn more of this seemingly complex and uncommon algebra factorisation. Can you recommend me a book or a website for this? There seem to be only common factorisations when I google. Many thanks in advance, Chris",['algebra-precalculus']
1238737,Reference request for stochastic process,"I studied the book, ""Probability, Random Variables and Random Signal Principles"" by Peyton Peebles. And I am a little bit familiar with statistical analysis like signal estimation and detection. In this case, of the books below, which book is the most adequate material to study stochastic processes (mainly including Marcov chains, Random walk, and Queuing theory)? 1) A First Course in Stochastic Processes or A Second Course in Stochastic Processes (by Karlin and Taylor) 2) Stochastic Processes (by Ross) 3) Probability and Random Processes (by Geoffrey R. Grimmett, David R. Stirzaker) Thank you!","['probability-theory', 'book-recommendation', 'reference-request', 'stochastic-processes']"
1238817,Categorical Banach space theory,"Consider the category $\mathsf{NormVect}_1$ of normed vector spaces with short linear maps $^{\dagger}$ and the full subcategory $\mathsf{Ban}_1$ of Banach spaces with short linear maps. Both categories are complete, cocomplete, and have a closed symmetric monoidal structure, given by the projective tensor product (see here ). The forgetful functor $\mathsf{Ban}_1 \to \mathsf{NormVect}_1$ is continuous (but not cocontinuous), in fact has a left adjoint (which is symmetric monoidal), the Cauchy completion. Question. Can you name a categorical property of $\mathsf{Ban}_1$ which is useful in practice, but which is not satisfied by $\mathsf{NormVect}_1$? Background: There is a branch called categorical Banach space theory, and I really wonder why there one does not consider the larger category of all normed vector spaces somehow as a first approximation. In functional analysis it is well-known that (and why) Banach spaces are more useful than normed vector spaces. I would like to know if or why this is also true for the corresponding categories. $^{\dagger}$ Notice that the subscript $1$ indicates that we restrict ourselves to short linear maps, which is quite important for having the mentioned categorical properties. For me, the moral of this choice is that if you use continuous linear maps, you don't take the whole structure of the objects into account, which tends to be bad.","['monoidal-categories', 'functional-analysis', 'banach-spaces', 'normed-spaces', 'category-theory']"
1238831,Is there an infinite field such that every non-zero element has finite multiplicative order?,"Is there an infinite field such that every non-zero element has finite multiplicative order? I did not find any example of such a field, but also did not see anything that forbids the existence of one. Any help?","['abstract-algebra', 'field-theory']"
1238862,Flow property of a differential equation,In order to solve this question I must fix $s$ while allowing $t$ to vary. I am confused as to why this is allowed.,['ordinary-differential-equations']
1238873,Evaluate $\lim_{n \to \infty} \int_{0}^1 \frac{n+1}{2^{n+1}} \left(\frac{(t+1)^{n+1}-(1-t)^{n+1}}{t}\right) \mathrm{d}t$,"Evaluate 
  $$\lim_{n \to \infty} \int_{0}^1 \frac{n+1}{2^{n+1}} \left(\frac{(t+1)^{n+1}-(1-t)^{n+1}}{t}\right) \mathrm{d}t$$ For this integral, I have tried using integration by parts and then evaluating the limit, but I don't think the integral inside converges. However, the limit does exist and the answer given in my book is $2$. Any help will be appreciated. Thanks in advance!","['calculus', 'limits', 'definite-integrals', 'algebra-precalculus', 'integration']"
1238898,Conditional distribution of exponential random variable conditioned on sum of i.i.d. exponential random variables,"Let X,Y be i.i.d. exponentially distributed with parameter $\lambda$. Show that for $Z:=X+Y$ and a measurable, non-negative function $h$ we have: $\mathbb{E}(h(X)|Z)=\frac{1}{Z}\int_0^Zh(u)\mathrm{du}$. I know that $Z$ is gamma-distributed with shape $2$ and rate $\lambda$. Then using the joint distribution of $X$ and $Y$ I calculated the joint density of $X$ and $Z$ to be $f_{X,Z}(x,z)=\chi_{z\ge x}\lambda^2\exp(-\lambda z)$. Using this, and the formula for conditional expectation, I got $\mathbb{E}(h(X)|Z=z)=\int_0^\infty h(u)\cdot f_{X,Z}(x,z)/f_Z(z) \mathrm{du}=\int_0^Zh(u)\mathrm{du}$. I can't see why I didn't get the correct result. Also, do we need the assumption that $h$ is non-negative? Edit: Of course, right after posting this, I find that a accidentally lost the $1/z$ along the way in the last equality. The second part of the question is still not clear to me though.","['probability-theory', 'conditional-expectation']"
1238995,Does fourth-order Runge-Kutta have an higher accuracy than the second-order one?,"I'm writing a presentation on modelling fluid flow. We used Runge-Kutta second order to describe the flow as a numerical method. I just want verify that Runge-Kutta fourth order would be of a higher degree of accuracy - I can't find it anywhere online. I'm 99% sure but seeing as it counts towards my final mark, I figured I'd ask the intellects of MSE to verify this.","['runge-kutta-methods', 'numerical-methods', 'ordinary-differential-equations']"
1238998,I need help understanding the proof of Lemma 2.4-1 from Kreyszig's Functional Analysis.,"Lemma: Let $\{x_1, \ldots, x_n \}$ be a linearly independent set of vectors in a normed space $X$ (of any dimension). Then there is a number $c > 0$ such that for every choice of scalars $\alpha_1, \ldots, \alpha_n$, we have 
$$\Vert \alpha_1 x_1 + \ldots + \alpha_n x_n \Vert \geq c (\lvert\alpha_1\rvert + \ldots + \lvert\alpha_n\rvert).$$ I have understood the proof until the second page where the author writes each sequence $(\beta_j^{(m)})$ is bounded. I don't understand next exactly how $(y_{n, m})$ is a subsequence and how we obtain the subsequence $(y_{n,m})=(y_{n, 1}, y_{n, 2}, \cdots)$ of $(y_m)$",['functional-analysis']
1239004,Inequalities and Differentiation,"Having become so accustomed to differentiation and integration being applied just like normal algebraic operators, I was somewhat suprised yesterday when I realized that $f(x) \geq g(x)$ does not imply $f'(x) \geq g'(x)$. Intuitively this makes sense, but it's somewhat surprising considering that $f(x) \geq g(x) \Rightarrow \int_a^b f(x) \geq \int_a^b g(x)$. Can anyone think of a way to demonstrate this 'symmetry breaking' with some rigor? Thanks in advance!","['derivatives', 'real-analysis', 'inequality']"
1239008,Bounds-negative binomial distribution,"Suppose $Y=\sum_{i=1}^{n} X_{i}$  where each $X_{i}$ is an independently and identically distributed geometric random variable with success parameter $p$, so that $Y$ has a negative binomial distribution. Are there any good upper bounds on $\mathbb{P}(Y>l)$ other than the standard Markov inequality for general $l$? I'm not necessarily looking for bounds on the concentration around the mean.","['probability-theory', 'probability', 'probability-distributions']"
1239036,How sample size affects confidence interval.,"Suppose the weight of n primary one students has sample mean of 20KG. If n = 40, a certain percentage of confidence interval for the population mean is (15.5,24.5). Find the confidence interval if we decrease the sample size to 30. Am I doing it right this way? Method 1 Given margin error = $\frac{\sigma}{\sqrt n}$ It means if sample size is quadrupled, margin error will be halved. Since $n = 40, 0.75n = 30$ Margin error now is $4.5$ New margin error = $\frac{4.5}{\sqrt {0.75}}$ = $3\sqrt 3$ Therefore, new interval = $20-3\sqrt 3$ to $20+3\sqrt3$
= $14.804 to 25.196$ Method 2 I solve for $\sigma$ first. Given margin error = $\frac{\sigma}{\sqrt n}$ = $4.5$ and $n = 40$ $\sigma = 4.5\sqrt{40}$ Sub new $n$ to find new margin error New margin error = $\frac{4.5\sqrt{40}}{\sqrt{30}}$ = $3\sqrt 3$ Which turns out the be the same. But I have one question for this method. Wouldn't the sample deviation, $\sigma$ changes whenever our sample size changes(which means sampled data will be different)? Why can I still use method 2 which assumed $\sigma$ remained unchanged?","['statistics', 'statistical-inference', 'sampling']"
1239090,$x^3-9=y^2$ find integral solutions,Find all integral solutions $x^3-9=y^2$ I tried many times but still no idea how to solve it. I will be grateful for any help.,"['number-theory', 'diophantine-equations', 'mordell-curves']"
1239104,Identifying the two-hole torus with an octagon,"I am aware that the 2-hole torus can be identified with the octagon with the equivalence relation as given in this picture: However, today in a topology revision lecture, the lecturer said the 2-hole torus can be represented by the octagon with opposite sides identified. This is different to the picture, so was our lecturer wrong? If so what surface is given by the octagon with opposite sides identified?",['general-topology']
1239106,Proving $6^n - 1$ is always divisible by $5$ by induction,"I'm trying to prove the following, but can't seem to understand it. Can somebody help? Prove $6^n - 1$ is always divisible by $5$ for $n \geq 1$. What I've done: Base Case:
$n = 1$: $6^1 - 1 = 5$, which is divisible by $5$ so TRUE. Assume true for $n = k$, where $k \geq 1$:
$6^k - 1 = 5P$. Should be true for $n = k + 1$ $6^{k + 1} - 1 = 5Q$ $= 6 \cdot 6^k - 1$ However, I am unsure on where to go from here.","['discrete-mathematics', 'induction', 'elementary-number-theory']"
1239120,Prove that the solution of an ODE can be prolonged to $\infty$,"I need an help understanding some general techniques in ordinary differential equations. I've never attended a course on ODE, so I'm quite confused on the argument, but I'm trying to improve my knowledgle. We have the following Cauchy problem: $$\begin{cases}\frac{dv}{dt}=b(v(t)+f(t))\\
v(s)=0\end{cases}$$ 
where $b\in C^1(\mathbb R^n, \mathbb R^n)$ and it is monotone, i.e. $\left<b(x)-b(y),x-y\right>\leq k(1+|x-y|^2)\quad \forall x,y\in \mathbb R^n.$ $f:[s,+\infty]\to \mathbb R^n$ is continuous. We know that a  unique solution $v$ exists defined on the interval $[s,T)$ (since $F(t,x):=b(x+f(t))$ is continuously differentiable with respect to $x$).
We want to prove that we can prolong the solution to $+\infty$, i.e. there exists a unique solution defined on $[s,+\infty).$ Now: Using monotonicity assumption on $b$ we have that $$\frac{1}{2} \frac{d}{dt}|v(t)|^2=\left< \frac{d}{dt}v(t),v(t)\right> \leq C(1+|v(t)|^2);$$ Integrating the above expression on the interval $[s,t]$ and using Gronwall's Lemma we have $$|v(t)|^2\leq 2C(T-s)e^{2C(T-s)}, \quad \forall t\in[s,T).$$ I've written on my notes that from 1. and 2. follows $$\sup_{t\in[s,T)} \left|\frac{dv}{dt}\right|=R_T<\infty,\tag{$*$}$$ and so we can prolong the solution to $\infty$. Now, I agree with the fact that if the solution does not explode we can prolong it, but I don't get why from 1. and 2. follows $(*).$ Can someone help me? Any suggestion is really appreciated.","['real-analysis', 'ordinary-differential-equations']"
1239124,An interval with width greater than one contains an integer.,"If I have an interval $(a, b)$ such that $b - a > 1$, how can I prove that this contains an integer? It seems 'obvious', but a formal proof eludes me.",['elementary-set-theory']
1239132,Find the coefficient of $x^{30}$.,"Find the coefficient of $x^{30}$ in the given polynomial
$$
\left(1+x+x^2+x^3+x^4+x^5+x^6+x^7+x^8+x^9+x^{10}+x^{11}+x^{12}\right)^5
$$ I don't know how to solve problems with such high degree.","['polynomials', 'combinatorics']"
1239157,How is the boundary in product spaces defined?,"The general question: how is the boundary defined in product spaces? Given two topological spaces $X,Y$, I'd say that $\partial(X\times Y)=\partial X\times\partial Y$. But looking at what follows it doesn't seem to be right: Clearly here $\partial_0 P=\partial\Delta_1\times\cdots\times\partial\Delta_n$, but it doesn't coincide with $\partial P$. So, what's the difference between $\partial P$ and $\partial_0 P$?","['several-complex-variables', 'complex-analysis', 'general-topology']"
1239171,Proving $2^n -1 = \sum_{i=0} ^{n-1} 2^i$ for all $n\geq 1$ by induction,"I'm practicing proofs by induction, and equalities seem to be the toughest for me. Can somebody please help to prove that for all integers $n \geq 1$: 
$$
2^n -1 = \sum \limits _{i=0} ^{n-1} 2^i\;?
$$","['induction', 'discrete-mathematics']"
1239196,An inequality for symmetric random walk,"I need to show that if $(X_j)$ are symmetric i.i.d. random variables with partial sums $S_n:= \sum_{j=1}^n X_j$, then for all $x \geq 0$ $$P(|S_n| > x) \geq \frac{1}{2} P(\max_{1 \leq j \leq n} |X_j| > x). $$ My attempt is to define an event $$\Delta:=\{\max_{1 \leq j \leq n} |X_j| > x\}$$ and to define on $\Delta$ the random variable $\tau$ to be the first $j$ such that $|X_j| > x$ (and put $\tau = \infty$ on $\Delta^c$). Fixing $n$ and writing $$S_j':= \sum_{1 \leq i \leq n, i \neq j } X_i  \quad  ,$$ I can write $$P(|S_n| > x) \geq \sum_{j=1}^n P( \mathrm{sign}(S_j')=\mathrm{sign}(X_j); \tau = j ) .$$ The event $A:= \{ \mathrm{sign}(S_j')=\mathrm{sign}(X_j) \}$ has probability $1/2$ by symmetry and independence, so we could conclude if only $A$ and $\{\tau= j\}$ were independent. Although this seems plausible, I'm not sure it's true, or how to prove it. Resolution : By conditioning on the variables involved in the event $A \cap \{\tau=j\}$, it is possible to switch the signs of the $X_k \hspace{3pt} (k \neq j)$ to prove the unsurprising equation: $$ P( \mathrm{sign}(S_j')=\mathrm{sign}(X_j); \tau = j ) = P( -\mathrm{sign}(S_j')=\mathrm{sign}(X_j); \tau = j ) ,$$ which is enough to conclude. The key is to use condition to swap the signs one by one.","['probability-theory', 'probability', 'random-walk']"
1239215,$\bigcup \alpha$ where $\alpha$ is a finite ordinal.,"Given a finite ordinal, is it correct in saying $\bigcup \alpha = \alpha - 1$? As an illustrative example consider $3 = \{\emptyset , \{\emptyset\}, \{\emptyset, \{\emptyset\}\}\}$. I believe $\bigcup 3 = \{\emptyset , \{\emptyset\}\} = 2$. Is this accurate?",['elementary-set-theory']
1239256,When is $HK \cong H \times K$?,"Suppose $G$ is a group and $H$ and $K$ are subgroups such that $G = HK$ and $H \cap K = \left\{e\right\}$, the identity element of $G$. When can we say that $HK \cong H\times K$? I tried to set up the canonical map $(h,k) \to hk$ and worked out that this is an isomorphism if and only if $H \subset C(K)$ or $K \subset C(H)$, where $C$ denotes the centralizer, since
$ h_{1}k_{1}h_{2}k_{2} = h_{1}h_{2}k_{1}k_{2}$ and therefore $hk = kh$ for each $h \in H$, $k \in K$. Is there a better way of saying this? How do I obtain a characterization of when $HK \cong H\times K$? Here I just picked a map (albeit canonical) and worked out when it would be an isomorphism. What if some other map works?","['abstract-algebra', 'group-theory']"
1239264,Quotient topology by identifying the boundary of a circle as one point,"The following is an example taken from Munkres topology book: I don't understand why does $X^{*}$is homeomorphic to $S^{2}$, is
this a basic fact that I don't understand or is it an example of something
more advanced ? Also, I don't understand figure 22.4, I think that a saturated set
is either contained in $\{(x,y)\mid x^{2}+y^{2}<1\}$ or contains
$S^{1}$and I don't see how this is described in the image, can someone please explain that ?","['quotient-spaces', 'general-topology']"
1239267,f continuously differentiable implies f is Lipschitz on compact subsets,"It is a more general form of the question here , only here $U$ is not a convex set but an open and connected subset of $\mathbb{R}^n$. I need to show that $f$ is $M$ Lipschitz on any compact $K \subset U$. My attempt goes like this: $U$ is an open connected set, and $K \subset U$ so for any two points $x,y \in K$ there is a finite set of points $\{x_i\}_{i=1}^{r}\in K$ and we'll denote $x_1:=x, x_r:=y$ so for every $i=2,3,...,r$ the straight segment $[x_{i-1},x_{i}]$ is contained by $K$. By the mean value theorem for several variables , $\forall i=2,3,...,r:\ \lvert f(x_{i-1})-f(x_i) \rvert= \lVert f'(s_i) ( x_{i-1} - x_i ) \rVert$ when $s_i\in U$. Notice that since $s_i ,\ i=2,3,...,r$ is finite thus bounded, $f'$ exists and continuous - thus $\exists T := \max_{i=1,2,..,r} \{ \lVert f'(s_i) \rVert \}$, and because $K$ is compact, $\exists D:= \ diam \{ K \} \geq \max_{i=2,3,...,r} \{ \lVert x_{i-1} -x_i \rVert \}$ hence:
$$\lvert f(x)-f(y) \rvert=\lvert f(x_{1})-f(x_2) +f(x_2)-f(x_3)+.... +f(x_{r-1})-f(y)\rvert \leq  TB\cdot r$$ and since the diameter of $K$ is finite, every $x \neq y \ \in K$ have a real positive number $t$ so that $\lVert x-y \rVert \cdot t = B$ and that gives $||f(x)-f(y)|| \leq T \cdot t \cdot||x-y||$. Is it o.k?","['calculus', 'real-analysis', 'proof-verification', 'multivariable-calculus']"
1239268,Multiples of some set has density,"Nathanson gives a proof in Elementary Methods in Number Theory (Theorem 7.14) that, if a set $S$ of positive integers has
$$
\sum_{s\in S}\frac1s<+\infty
$$
then the set of positive multiples of $S$ has a natural density, i.e.,
$$
\lim_{x\to+\infty}\frac{|\{m\in M(S):\ m\le x\}|}{x}
$$
exists. I'd like to cite this result; does anyone know when it was first stated or proved? (""As far as I know Nathanson was first"" would be a valid answer.) Edit: Hall ( Sets of Multiples ) states that this is the best possible result in the sense that for any $\xi(x)\to+\infty$ there are sets $S$ with
$$
\sum_{s\in S,s\le x}\frac1s<\xi(x)
$$
for which the set of multiples of $S$ does not have a density. But I'm still looking for a reference on the original.","['number-theory', 'limsup-and-liminf', 'reference-request']"
1239279,basic question about holonomy,"I'm struggling to understand how conditions on the metric put conditions on the holonomy group and vice-versa. My understanding is that the holonomy principle says that there's a one-to-one correspondence between parallel tensors, constant tensors (i.e. with $\nabla S=0$), and elements of the fibre $E_p$ preserved under the holonomy group. I'm trying to understand the 2 following examples: If $(M,g)$ is a manifold and we have a complex structure $J_p:T_pM\to T_pM$ such that $J_p$ is invariant under the holonomy group, I've read that in order to extend $J$ to the whole manifold we need Hol$(g)\subset U(n)$. Why? Isn't all we need that $P_\gamma(J_p)=J_p$. I know that $U(n)=GL(n,\mathbb{C})\cap O(2n)$. So why is the condition $P_\gamma(J_p)=J_p$ equivalent to $P_\gamma$ commuting with $J$ and preserving $g$? The other example is showing that Hol$(g)\subset SO(n)$ if and only if $M$ is orientable. So to use the holonomy principle, start with non-zero $\alpha_p\in \Lambda^n(T^\ast_pM)$. We want $\alpha_p$ to be preserved under Hol$(g)$ so that it can be extended, by the holonomy principle, to a nowhere vanishing form. Again, we just need $\alpha_p$ to be fixed by the holonomy group. How does this happen if and only if the group is contained in SO$(n)$. I apologize for asking 2 questions but I feel the concept I am missing is the same in both. Thanks very much.","['holonomy', 'differential-geometry', 'kahler-manifolds']"
1239290,Smooth function conditions,"A curve defined by $x=f(t)$, $y=g(t)$ is smooth if $fโฒ(x)$ and $gโฒ(x)$ are continuous and not simultaneously zero. Why do we have the second condition(simultaneously zero)?","['partial-derivative', 'continuity', 'functions']"
1239313,fiber bundle in topological category and smooth category.,"Let $M$ be a smooth manifold and $G$ be a Lie group. Denote by $Bun(M,G)$ the set of all equivalent smooth Principal bundle on $M$ with structural group $G$ in smooth category. And denote by $Bun(M,G)_{top}$ the set of all equivalent Principal bundle on $M$ with structural group $G$ in the topological category when $G$ is regard as a topological group. Then we have a natural map  $\varphi:Bun(M,G) \rightarrow Bun(M,G)_{top}$. If $\varphi$ is a bijection and how to prove it ?","['differential-geometry', 'algebraic-topology']"
1239346,"At how many points will $\lfloor(sin x + cos x )\rfloor$ be discontinuous in the interval [0,2$\pi$]","At how  many points will $\lfloor(sin x + cos x )\rfloor$ be discontinuous in the interval [0,2$\pi$] ? How should the graph be ?","['trigonometry', 'functions']"
1239368,Computing an integral using residues,"I am trying to find an integral: $$\int_{-\infty}^{+\infty}\frac{e^{-\sqrt{(x^2 + 1)}}}{(x^2 + 1)^2}\,\mathrm dx$$ I went about applying contour integral over a semicircle with diameter along $ x = +\infty$ to  $- \infty $ enclosing the pole at  $x = +i $.  The residue is $(-i/2)$ as shown here. So the integral should be $(2\pi i)\times (-i/2)=\pi$ However since it is a well behaved function if I do a quick numerical integration in Mathematica it is giving me a value of $0.475$ A plot confirms that the function converges very fast. Mathematica code: NIntegrate[  E^{- Sqrt[1 + y^2]}/(1 + y^2)^2, {y, -1000, 1000}] What am I doing wrong? Many thanks!","['contour-integration', 'complex-analysis', 'definite-integrals', 'residue-calculus']"
1239374,If $n\le |f(1/n)|\le n^{3/2}$ then prove that $f$ has a simple pole at $z=0$,"Let, $f:\{z\in \mathbb C:0<|z|<1\}\to \mathbb C$ be analytic such that $n\le |f(1/n)|\le n^{3/2}$ for $n=2,3,...$. Assume that $z^2f(z)$ is bounded in $|z|<1$. Show that $f$ has a pole of order $1$ at $z=0$. My Attempt: First I consider $g(z)=z^2f(z)$.  So $g$ has zero of order $2$ at $z=0$. Also $g(z)$ is analytic in $|z|<1$. Also $|g(z)|\le M$ for all $z$ in $|z|<1$. So applying Schwartz lemma , $|g'(0)|\le 2M$ and $|g(z)|\le M|z|^2$. Now if we can find $z_0 (\not =0)$ in $|z|<1$ such that one of these inequalities hold then we can find $g$ explicitly..... But I could not show it... Please help on this OR suggest any other way to solve it....","['complex-analysis', 'complex-numbers']"
