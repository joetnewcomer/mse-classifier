question_id,title,body,tags
1906544,Example when the tail $\sigma$-algebra is not generated by the even and odd tail $\sigma$-algebras,"The even tail $\sigma$-algebra $T^0$ and odd tail $\sigma$-algebra $T^1$ are always contained in the tail $\sigma$-algebra $T$.
  Now I want to prove that it may happen that $T\neq\sigma(T^0,T^1)$. I have tried defining different families of $\sigma$-algebras (converging vs non converging sequences, starting with constant number of zeros or ones, ending with ones or zeros, etc.) but I can't find an example to demonstrate my claim that $T\neq\sigma(T^0,T^1)$. Here are the relevant definitions. Let $\Omega = \{0,1\}^{\mathbb N}$ the set of all binary sequences. If $\mathcal F_n$ is a family of $\sigma$-algebras, we define $$T_n=\sigma\left(\bigcup_{k\geq n}\mathcal F_k\right)\qquad T^0_n=\sigma\left(\bigcup_{k\geq n}\mathcal F_{2k}\right)\qquad
T^1_n=\sigma\left(\bigcup_{k\geq n}\mathcal F_{2k-1}\right)$$
and
$T=\bigcap\limits_nT_n$ the tail $\sigma- $algebra,
$T^0=\bigcap\limits_nT^0_n$ the even tail $\sigma-$ algebra,
$T^1=\bigcap\limits_nT^1_n$ the odd tail $\sigma-$ algebra.","['probability', 'measure-theory']"
1906551,"Is there a clean way to write down this ""product"" of a vector and a matrix? Does it have any interpretation?","Let $$v = \begin{pmatrix}
  v_1 \\
  \vdots \\
  v_m
 \end{pmatrix}$$ and $$A = 
 \begin{pmatrix}
  a_{11} & \cdots & a_{1n} \\
  \vdots & \ddots &  \vdots\\
  a_{m1} & \cdots & a_{mn}
 \end{pmatrix}$$ If we define the operation $\Box$ as follows $$v \Box A = \begin{pmatrix}
  v_1 a_{11} & \cdots & v_1 a_{1n} \\
  \vdots & \ddots &  \vdots\\
  v_m a_{m1} & \cdots & v_m a_{mn}
 \end{pmatrix},$$ is there a clean way to write it down? Does the operation have any interpretation? I was hoping it has something to do with tensor products but, after looking up about tensors, it doesn't look like it.","['tensor-products', 'notation', 'matrices', 'linear-algebra', 'vectors']"
1906559,Finding all graphs with a given automorphism group on $n$ vertices,"Is there a way to algorithmically find all graphs that have a certain automorphism group on $n$ vertices? For example, is there a way to find all graphs with automorphism group $A_5$ on $1000$ vertices? Obviously you could look at all graphs, and then compute the automorphism group of each graph, but this is very inefficient. I would also be interested in a way to count the number of graphs with a given automorphism group, or to generate (or count) all graphs that have an automorphism group of a certain order. Special cases (e.g. trivial automorphism group) would be interesting as well. Clearly the only graph on $n$ vertices with automorphism group $S_n$ is $K_n$.","['graph-theory', 'algebraic-graph-theory', 'group-theory']"
1906582,Find average and sample deviation of the set of samples,"Let we have a sample of $N$ idependent identical normally distributed random variables with expectation $a$ and variance $\sigma$. Now we take $k$ samples of initial $N$ to $m$ elements in each. I need to find the average and sample deviation of averages in 2 cases: (a) sampling with replacement (b) sampling without replacement. I try to write an example to clarify the statement. Let we have 1000 people and measure their height. We know that their height is disributed normally with average 175 sm and dispersion 4 sm. Now we 25 times choose 20 people out of 1000. And in each such group of 20 people compute the average height. So we will have 25 numbers that indicate the average height in each group. And I need to calculate the average and variance of the average heights of 25 groups in two following cases: a) Every time we choose 20 random distinct people of 1000 (it can be two same samples for example) b) We do not choose people who we chose before, ie, after the first sampling, we will only select from the remaining 980, then 960, etc. I have an idea that in the case (a) every subsample will have the same average as the whole sample. So in the case (a) all averages will be the same and we have the answer $a$ and dipersion $0$. Am I right? And I absolutely do not know what to do in case (b). Great thanks for the help!","['probability-theory', 'probability', 'statistics']"
1906596,About pro-$p$ groups,"Let $p$ be a prime number.  A pro-$p$ group is the inverse limit of an inverse system of discrete finite $p$-groups.
(I've just read the definition). I have two questions: Is it true that a the $p$-part of an abelian profinite group is a pro-$p$ group? (I think so) How is an abelian pro-$p$ group a $\mathbb{Z}_p$-module?","['modules', 'abstract-algebra', 'profinite-groups', 'algebraic-number-theory', 'group-theory']"
1906641,A limit involving the Regularized Incomplete Beta Function,"I'm trying to evaluate $$\lim_{n\to\infty} nx^{n-1}I\left(1-\frac{x^2}{4}; \frac{n+1}{2}, \frac{1}{2}\right)$$ where $I(x; a, b)$ is the Regularized Incomplete Beta Function and $-2\leq x \leq 2$, and I'm stuck on where to begin. I've tried expanding the beta function as the quotient of two integrals and then differentiating using L'Hopital's rule, and I've tried using a series representation for the Beta function, but nothing seems to work. I've tried plotting this function of $x$ with $n=199999$ using WolframAlpha , and because of the result I strongly suspect that it tends to $0$ everywhere but $x=\sqrt{2}$ where it tends to $\infty$, but I can't seem to prove this. How would I start?","['special-functions', 'beta-function', 'limits']"
1906659,"Difference of two regularly open sets, if non-empty, has non-empty interior","Let $T$ be a metric space. A subset of $T$ is regularly open it is equal to the interior of its closure. Given a proper inclusion $A\subset B$ of two regularly open sets  in $X$, must the difference $B\setminus A$ have non-empty interior?","['general-topology', 'metric-spaces']"
1906664,How are random variables random? [duplicate],"This question already has an answer here : Why is the function $\Omega\rightarrow\mathbb{R}$ called a random variable? (1 answer) Closed 7 years ago . Given a probability space $(\Omega, \mathcal F, \mu)$, a random variable is a function $X:\Omega\to\Bbb R$. That's the definition I was given of what a random variable is. I don't see what's random about them, or how they capture the idea of the 'uncertainty' of some events. Could anyone provide some intuition about random variables?","['intuition', 'probability-theory', 'random-variables']"
1906683,Every Suslin set is the continuous image of some closed set in $\mathbb N^{\omega}$.,"Let $A$ be some countable set, and let $A^{\omega}$ be the set of all sequences $\mathbb N \to A$ equipped with the product topology. A subset $X \subseteq A^{\omega}$ is called Suslin set if there is a countable set $B$, a Borel subset $Y \subseteq B^{\omega}$ and a continuous map $f : B^{\omega} \to A^{\omega}$ such that $X = f(Y)$, i.e. it is the continuous image of some Borel set. Does anybody has a proof of the following result: If $X \subseteq A^{\omega}$ is a Suslin set, then there exists a closed set $Y \subseteq \mathbb N^{\omega}$ and a continuous map $f : \mathbb N^{\omega} \to A^{\omega}$ such that $X = f(Y)$. It is mentioned in a book, but its proof is faulty and I cannot find it anywhere else.","['descriptive-set-theory', 'set-theory', 'analysis']"
1906696,Local homeomorphism passes to the quotient under proper group action?,"Let $X$ and $Y$ be locally compact topological spaces, both equipped with a proper $G$-action where $G$ is a locally compact group. Let $f: X \to Y$ be a $G$-equivariant local homeomorphism. Is the quotient map $\bar{f} : X/G \to Y/G$ still a local homeomorphism? I think that it is not hard to see that $\bar{f}$ is continuous and open, but I guess it does not have to be locally injective, though I can't think of a counter-example.","['group-actions', 'general-topology', 'topological-groups', 'quotient-spaces']"
1906700,"Example of a $C^{1,1}$ surface with constant Gaussian curvate equal to 1, which is not a sphere?","In a paper I'm reading the author talk about the construction of a $C^{1,1}$ surface with constant Gaussian curvature equal to $1$. I do not know what to imagine other then a sphere though, which makes the article difficult to read. So does someone have an example of such surface that is not a sphere?","['curvature', 'riemannian-geometry', 'differential-geometry', 'surfaces']"
1906718,"How to prove $\,{\rm order}(a^k) = n/\gcd(n,k)\,$ for $\,n={\rm order}(a)$?","This is an exercise from ""Contemporary Abstract Algebra"" I'm not sure how to solve. Exercise: Let $\langle a\rangle $ be a (cyclic) group of order $n$. Prove that the order of $a^k=\frac{n}{\gcd(n,k)}$. Direction: (1) Let $d=\gcd(n,k)$, thus by the Euclidian algorithm we can find $X,Y\in\mathbb{Z}$ s.t. $d=Xn+Yk$, thus, $a^d=a^{Xn+Yk}=a^{Xn}a^{Yk}=(a^n)^X(a^k)^Y=(a^k)^Y$. What to do from here? (2) We know that $d|n$, thus $\langle a^{n/d} \rangle$ is of order $d$ and $\langle a^d \rangle$ is of order $\frac{n}{d}=\frac{n}{\gcd(n,k)}$. Is it mean that $d=k$? Where is my mistake?","['finite-groups', 'abstract-algebra', 'group-theory']"
1906721,Taylor expansion of composite function,"My confusion is slightly related to this question . Suppose we have two nice functions $f(x)$ and $g(x)$, how do we find Taylor series of $f(g(x))$? To be more concrete, consider $f(x^2)$. In this case, we can regard it as $f(g(x))$ where $g(x) = x^2$. One way to find the Taylor series around $1$ is just $$f(x^2)=f(1)+f'(1)(x^2-1)+\frac{1}{2}f''(1)(x^2 - 1)^2+\cdots$$ However, what if I do this? $$f(x^2)=f(g(x))=f(1)+\color{blue}{\frac{d}{dg}f(g(x))\cdot\frac{d}{dx}g(x)\biggr|_{x=1}}(x-1)+\frac{1}{2}\frac{d^2}{d^2 x}f(g(x))\biggr|_{x=1}(x-1)^2+\cdots$$ This ends up like
$$f(x^2) = f(1)+2f'(1)(x-1)+\cdots$$ Which looks different from what you should get. I am confused because I thought both methods are valid and they should agree (although I've always been using the first one, regarding $x^2$ as a ""number"" instead of a function).","['real-analysis', 'sequences-and-series']"
1906741,Are all matrices that fulfill $x^n-x=0$ are diagonalizable?,"Let $f(x)=x^n-x$ prove/disprove all matrices such that $f(A)=0$ are diagonalizable? I have tried to find conditions on the minimal polynomial but it did not work, any suggestions?",['linear-algebra']
1906795,Non-trivial questions about confocal ellipses,"This question arises from an attempt to give an elementary solution to an interesting problem . Main question. Assume that $\Gamma_{AB}$ and $\Gamma_{AC}$ are two ellipses with foci at $A,B$ and $A,C$ respectively, meeting at $P$.
  Let $Q$ be the other point of intersection of $\Gamma_{AB}$ and
  $\Gamma_{AC}$: how to construct $Q$ with straightedge and compass,
  given $A,B,C,P$? My attempt was to write the equation of both ellipses in polar coordinates and perform some trigonometry, but that does not translate easily into a simple straightedge and compass construction. So I wondered about some hidden projective property of two confocal ellipses, and indeed something striked me. Secondary question. In the previous configuration, let $U$ be the pole of $PQ$ with respect to $\Gamma_{AB}$, i.e. the intersection of
  the tangents at $P$ and $Q$, and let $V$ be the pole of $PQ$ with
  respect to $\Gamma_{AC}$. Then $A$ lies on $UV$. I was not able to prove this fact, but I discovered many interesting theorems in this Dolgirev paper , and I believe his techniques solves both the secondary question and the main question. May I ask some help from you, guys?","['conic-sections', 'euclidean-geometry', 'projective-geometry', 'geometry']"
1906878,Prove $\left|\int_a^b \phi(t) \ dt \right|\le \int_a^b |\phi(t)| \ dt$ [duplicate],"This question already has an answer here : Prove $\left| \int_a^b f(t) dt \right| \leq \int_a^b \left| f(t) \right| dt$ (1 answer) Closed 7 years ago . Let $\phi(t) = \phi_1(t)+i\phi_2(t)$ be a complex function in a real interval $t$ In order to prove:
$$\left|\int_a^b \phi(t) \ dt \right|\le \int_a^b |\phi(t)| \ dt$$ I tried:
$$\int_a^b \phi(t) \ dt  = \int_a^b \phi_1(t) + i\phi_2(t) \ dt  = \int_a^b \phi_1(t) \ dt + \int_a^b \phi_2(t) \ dt \rightarrow$$ $$\left|\int_a^b \phi(t) \ dt \right| = \sqrt{\left(\int_a^b \phi_1(t) \ dt\right)^2+\left(\int_a^b \phi_2(t) \ dt\right)^2}\le \sqrt{\left(\int_a^b \phi_1(t) \ dt\right)^2}+\sqrt{\left(\int_a^b \phi_2(t) \ dt\right)^2} = $$ $$\left|\int_a^b \phi_1(t) \ dt\right|+\left|\int_a^b \phi_2(t) \ dt\right|\le \int_ a^b |\phi_1(t)| \ dt + \int_a^b |\phi_2(t)| \ dt = \int_a^b |\phi_1(t)|+|\phi_2(t)| \ dt$$ But how to arrive at $\int_a^b |\phi(t)| \ dt$?","['complex-analysis', 'integration', 'calculus', 'proof-verification']"
1906899,Commuting analogue of the cross product,"Let $V$ be a finite-dimensional real inner product space. I am fairly sure that the following operation cannot exist, and would like a nice conceptual explanaition for its impossibility. Question: Can there exist a product $(x,y) \mapsto xy : V \times V \to V$ satisfying Bilinearity Commutativity Orthogonality meaning $xy$ is perpindicular to $x$ (and $y$ too, by commutativity) for all $x,y \in V$? Added: As was pointed out in the comments, I want to exclude the zero product. My thoughts: I didn't think of anything too useful, just that the orthogonality gives us that for fixed $x$, the map $y \mapsto xy$ is an antisymmetric transformation. So we have the identity $(xy,z) = -(y,xz)$. This lets us play around a bit, e.g.
$$(xy,z) = (yx,z) = -(x,yz) = -(x,zy) = \ldots$$ but I didn't get anywhere this way.","['linear-algebra', 'multilinear-algebra']"
1906924,Why must the universal cover of a variety with a rational point also have a rational point?,"After reading: Why is the fundamental group a sheaf in the etale topology? I followed read the linked survey paper by Minhyong Kim: http://people.maths.ox.ac.uk/kimm/papers/leeds.pdf where at the top of page 8, he says that if $X$ is a variety over $\mathbb{Q}$ with a $\mathbb{Q}$-rational point $b\in X(\mathbb{Q})$, then the universal cover $\tilde{X}$ (defined as the inverse limit of all finite etale covers of $X$ by geometrically connected $\mathbb{Q}$-varieties) must also have a rational point $\tilde{b}\in\tilde{X}$. This seems patently false. For example, let $X = \mathbb{P}^1_{\mathbb{Q}} - \{0,1,\infty\}$, then by Belyi's theorem every curve over $\mathbb{Q}$ admits an unramified map to $X$, including curves without any rational points (for example, conic sections like $x^2 + y^2 = -1$), which would imply that the universal cover of $X$ doesn't have any rational points. Perhaps I'm misinterpreting his statement? Could he be assuming that $X$ is proper? Even so, I don't see why every cover of $X$ must have a rational point.","['arithmetic-geometry', 'algebraic-geometry']"
1906935,Determine the number of simple graphs with 10 vertices and 7 edges such that all vertices degree are 0 or 2.,"Determine the number of simple graphs with 10 vertices and 7 edges such that all
vertices degree are 0 or 2. Confused with that degrees 0 and 2 on simple graphs, any help will do, thank you.","['graph-theory', 'discrete-mathematics']"
1906937,What happens when the product of Sigma Algebras is taken over an uncountable set??,"Let $\{ X_{\alpha} \}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X = \prod_{\alpha \in A}X_{\alpha}$, and $\pi_{\alpha}: X \rightarrow X_{\alpha}$ the coordinate maps. If $\mathcal{M}_{\alpha}$ is a $\sigma$-algebra on $X_{\alpha}$ for each $\alpha$, the $\textbf{product $\sigma$-algebra}$ on $X$ is the $\sigma$-algebra generated by :
$$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mathcal{M}_{\alpha}, \alpha \in A \}.$$ Then we have these next two propositions: 1.If $A$ is countable, then $\textbf{product $\sigma$-algebra}$ is generated by $\{\prod_{\alpha \in A} E_{\alpha}:E_{\alpha} \in \mathcal{M}_{\alpha}  \}.$ Suppose $\mathcal{M}_{\alpha}$ is generated by $\mu_{\alpha},\alpha \in A$. Then $\textbf{product $\sigma$-algebra}$ is generated by $$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mu_{\alpha}, \alpha \in A \}.$$ Furthermore, if $A$ is countable and $X_{\alpha} \in \mu_{\alpha}, \alpha \in A$, then $\textbf{product $\sigma$-algebra}$ is generated by $\{\prod_{\alpha \in A} E_{\alpha}:E_{\alpha} \in \mu_{\alpha}  \}.$ The proof is easy. The first bit hinges on the observation that 
$$\prod_{\alpha \in A} E_{\alpha}=\cap_{\alpha \in A}\pi^{-1}_{\alpha}(E_{\alpha})$$ and if $E \subset \sigma(F)$(The sigma algebra generated by F), then $\sigma(E) \subset \sigma(F)$. The first part of second bit follows by constructing a set with the required property and the lemma
(if $E \subset \sigma(F)$(The sigma algebra generated by F), then $\sigma(E) \subset \sigma(F))$, The second bit follows from the first bit along with the use of lemma. Well I was wondering what happens when $A$ is uncountable. Clearly the proof of $1$ won't go through. I was not able to get any counterexample though. I was thinking of taking $A=\mathbb{R}$ and $X_{\alpha}=[0,1]$ for each $\alpha$. But couldn't succeed. Similarly the proof of $2$ won't go through as well. Can I have a counterexample for this as well?? Also what happens when $X_{\alpha} \not\in \mu_{\alpha}$?? This is certainly used in the proof. Thanks for the help!!","['products', 'real-analysis', 'measure-theory', 'analysis']"
1906941,"Perturbation theory for least squares for very different A, b","Consider the least squares problem $f(x;A,b) = \|Ax-b\|_2^2$ and define $x^*$ the minimizer of $f(x;\hat A,\hat b)$, and $\hat x$ the minimizer of $f(x; A_2, b_2)$. I want to put some bound on $\|x^* - \hat x\|$. Looking through Golub/Van Loan, I see a lot of stuff that is basically some function of $\epsilon = \max\{\|A-\hat A\|, \|b-\hat b\|\}$, but in some sense that's not the best you can do. For example, if $A =\left[\begin{matrix} 0 \\ C\end{matrix}\right], \hat A =\left[\begin{matrix}  C\\ 0\end{matrix}\right], b =\left[\begin{matrix} 0 \\ d\end{matrix}\right], \hat b =\left[\begin{matrix}  d\\ 0\end{matrix}\right]$ then $\epsilon = \max\{2\|C\|, 2\|d\|\}$ which may be very large, but $\|x^*-\hat x\| = 0$. Are there existing bounds that take this into account? It has to be some bound on some function that involves $a_i$ AND $b_i$ (for $A= [a_1^T, ...]$ and $b = [b_1;...]$) and not just some stuff on range space of $A$. I suspect there is some result in machine learning, since this is basically about regression solutions if you get enough ""important"" samples. Can anyone point me to any known results? Thank you!","['regression', 'machine-learning', 'linear-regression', 'linear-algebra']"
1906958,Weak Whitney embedding theorem,"I am currently reading Shigeyuki Morita's ""Geometry of Differential Forms"" and I have found one of his proofs to be a little bit on the difficult side. On page 36 he states the theorem "" An arbitrary compact $\textit{C} \ ^\infty$ manifold M can be embedded into $R^N$ for sufficiently large N"". The way he goes about proving the theorem goes as follows. He uses a finite collection of coordinate neighborhoods $\big\{(U_i,\phi_i)\big\}_{i=1}^r$ such that each $\phi_i:U\to R^n$ is an open disk $D(2)$ of radius 2 with center at the origin, and if we put $V_i=\phi_{i}^{-1}(D(1))$, $\big\{V_i\big\}_{i=1}^r$ is already a covering of M. Do not worry about the existence of such an atlas, it follows form the paracompactness of all manifolds and the fact that M itself is compact. Now he next defines for each $i$ a $C^\infty$ map $f_i:M\to S^n$ with the following two conditions: (i) the restriction of $f_i$ to $V_i$ is a diffeomorphism from $V_i$ onto the southern hemisphere $\big\{x\in S^n: x_{n+1} < 0\big\}$ of $S^n$ and (ii) $f_i$ maps the complement of $V_i$ to the northern hemisphere. He asks the reader to provide the map but I can't figure out how to construct such a map. A diffeomorphism from $V_i$ to the southern hemisphere is simple but I can't find a function which satisfies property (ii) as well as property (i). Any help is greatly appreciated!","['differential-geometry', 'differential-topology']"
1906973,Is there a homeomorphism between the $n$-dimensional cube to the $n$-dimensional simplex that doesn't create hard corners?,"Is there a homeomorphism between the $n$-dimensional cube to the $n$-dimensional simplex that doesn't create hard corners?  This is related to a similar question found here .  However, the mappings in response to that question create corners.  Specifically, I've been playing with schemes where Set a direction to squeeze the point $x\in[0,1]^n$, $v$ Project $x$ into the nullspace of $v$, $y = x - \frac{v^Tx}{v^Tv}v$ Draw a line between $x$ and $y$, $l(\alpha) =  \alpha(y-x)+x$ $l(0)=x$, so we know it's in the hypercube.  Find
$$
  \alpha_l = \arg\max\{\alpha\geq 0:l(\alpha)\in[0,1]^n\}
  $$
$$
  \alpha_r = \arg\min\{\alpha\leq 0:l(\alpha)\in[0,1]^n\}
  $$
Basically, how far left and right we can travel and stay inside the cube Define the squeeze factor
$$
  \alpha = \frac{\alpha_l}{\alpha_l-\alpha_r}
  $$
Basically, how far between the leftmost and rightmost points we are along the direction $v$. Determine a new leftmost and rightmost point on the simplex,
$$
  z_l = l(\alpha_l)
  $$
$$
  z_r = \frac{1-\sum x_i}{\sum (y_i-x_i)}
  $$
It turns out the left most point is the same for the cube and the simplex. Use the squeeze factor to determine the new mapping for $x$
$$
  w=\alpha(z_r-z_l)+z_l
  $$ Anyway, that was a little circuitous, but it works.  It turns out if $v=(1,\dots,1)$, we get the first answer to the linked question above.  If we set $v = x$, we get the second scheme. Alright, so what's the problem?  This mapping makes something that kind of looks like a reentrant corner on smooth areas mapped to the simplex.  For example, if we set $v=(1,\dots,1)$ and map a circle centered at $(0.5,0.5)$ of radius $0.25$ to the simplex, we go from to Basically, we get a heart shaped object with these hard corners.  I've tried a bunch of different schemes for choosing $v$ and I keep getting corners, which is undesirable.  I really want something that's mapped smoothly to the simplex. Anyway, is there a smooth map from the hypercube to the simplex where where something like a circle mapped from the hypercube to the simplex won't have these corners? Edit 1 I think @Del has a good answer and here's what it looks like.  Given a rectangular grid The scheme transforms this into which shows how the grid is squished to fit into the simplex.  It turns out this is smooth for the use case I described above.  We can see this with the deformed circle Anyway, thanks for the help!","['algebraic-topology', 'general-topology', 'conformal-geometry', 'geometry']"
1906988,Is there a function $f(x)$ which is defined near $x = c$ and infinitely differentiable near $x = c$ and satisfy the following properties,"Is there a function $f(x)$ which is defined near $x = c$ and infinitely differentiable near $x = c$ and satisfy the following properties: For any positive real number $\delta$, there exist real numbers $x, x^{'}$ such that $c - \delta < x, x^{'} < c$ and $f(x) > f(c)$ and $f(x^{'}) < f(c)$ . For any positive real number $\delta$, there exist real numbers $x, x^{'}$ such that $c < x, x^{'} < c + \delta$ and $f(x) > f(c)$ and $f(x^{'}) < f(c)$ .",['derivatives']
1907013,Crazy pattern in the simple continued fraction for $\sum_{k=1}^\infty \frac{1}{(2^k)!}$,"The continued fraction of this series exhibits a truly crazy pattern and I found no reference for it so far. We have: $$\sum_{k=1}^\infty \frac{1}{(2^k)!}=0.5416914682540160487415778421$$ But the continued fraction is just beautiful: [1, 1, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 1, 1, 601080389, 2, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 1, 1, 1832624140942590533, 2, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 2, 601080389, 1, 1, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 1, 1, 23951146041928082866135587776380551749, 2, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 1, 1, 601080389, 2, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 2, 1832624140942590533, 1, 1, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 2, 601080389, 1, 1, 5, 2, 69, 1, 1, 5, 2,...] All of these large numbers are not just random - they have a simple closed form: $$A_n= \left( \begin{array}( 2^n \\ 2^{n-1}  \end{array} \right) -1$$ $$A_1=1$$ $$A_2=5$$ $$A_3=69$$ $$A_4=12869$$ $$A_5=601080389$$ And so on. This sequence is not in OEIS, only the larger sequence is, which contains this one as a subsequence https://oeis.org/A014495 What is the explanation for this? Is there a regular pattern in this continued fraction (in the positions of numbers)? Is there generalizations for other sums of the form $\sum_{k=1}^\infty \frac{1}{(a^k)!}$ ? Edit I think a good move will be to rename the strings of small numbers: $$a=1, 1, 5, 2,\qquad b=1, 1, 5, 1, 1,\qquad c=2,5,1,1,\qquad d=2, 5, 2$$ As a side note if we could set $1,1=2$ then all these strings will be the same. Now we rewrite the sequence. I will denote $A_n$ by just their indices $n$ : $$[a, 3, b, 4, c, 3, c, 5, d, 3, a, 4, b, 3, c, 6, d, 3, b, 4, c, 3, d, 5, a, 3, a, 4, b, 3, c, 7, \\ d, 3, b, 4, c, 3, c, 5, d, 3, a, 4, b, 3, d, 6, a, 3, b, 4, c, 3, d, 5, a, 3, a,...]$$ $$[a3b4c3c5d3a4b3c6d3b4c3d5a3a4b3c7d3b4c3c5d3a4b3d6a3b4c3d5a3a,...]$$ Now we have new large numbers $A_n$ appear at positions $2^n$ . And postitions of the same numbers are in a simple arithmetic progression with a difference $2^n$ as well. Now we only have to figure out the pattern (if any exists) for $a,b,c,d$ . The $10~000$ terms of the continued fraction are uploaded at github here . I also link my related question , from the iformation there we can conclude that the series above provide a greedy algorithm Egyptian fraction expansion of the number, and the number is irrational by the theorem stated in this paper .","['number-theory', 'combinatorics', 'continued-fractions']"
1907044,Show $\log \prod_p\frac{1}{1-p^{-s}}=\sum_p\sum_{n=1}^\infty \frac{1}{np^{ns}}$,"I'm trying to show: $$\log \prod_p\frac{1}{1-p^{-s}}=\sum_p\sum_{n=1}^\infty
 \frac{1}{np^{ns}}$$ Using the properties of the logarithm I get: $$\log \prod_p\frac{1}{1-p^{-s}}=\sum_p\log\frac{1}{1-p^{-s}}=\sum_p(\log 1-\log (1-p^{-s}))=$$
$$=-\sum_p \log(1-p^{-s})=-\sum_p \log\bigg(\frac{p^s-1}{p^s}\bigg)$$ The log can be expanded more, but it seems to get me farther from the result. I realize that the sum over $n$ comes from a geometric sum, but I don't see where to get it from, or how to get rid of the logarithm. Thanks in advance for any help or tips!","['number-theory', 'sequences-and-series', 'calculus']"
1907048,Difficulty understanding manipulation of $\epsilon$ and $N$ in the definition of a limit in proofs,"The definition I am working with is from Foundations of Mathematical Analysis by Johnsonbaugh and Pfaffenberger. Def : Let $\{  a_n \}_{n=1}^\infty$ be a sequence of real numbers. We say that $\{  a_n \}_{n=1}^\infty$ has limit $L\in \Bbb R$ if for every $\epsilon > 0$, there exists a positive integer $N$, such that if $n \geq N$, then $$|a_n - L|<\epsilon$$ In the proof of the theorem stating that the sum of the the two convergent sequences is convergent to the sum of their limits a simple strategy is to take the epsilon for each sequence individually and use the definition and divide the epsilon by two so that using triangle inequality you can say: $$|(a_n + b_n)-(L+M)| \leq |a_n - L| + |b_n - M| < \epsilon/2 + \epsilon/2 = \epsilon$$ I don't understand why this is okay to do, I kind of understand the manipulation of $N$ so that $n$  holds the same inequality of the definition but something seems logically flawed in the assertion above and it is used often for a variety of limit proofs, particularly in proving basic algebra of limit proofs.","['real-analysis', 'proof-writing', 'limits']"
1907064,Use MVT to prove that $1-\cos x<x^2$ for $x\neq 0$,"Use MVT to prove that $1-\cos x<x^2$, $x\neq 0$ $$1-\cos x<x^2\implies 1<x^2+\cos x$$
Let $f(x)=x^2+\cos x$, Notice that $f(-x)=(-x)^2+\cos(-x)=x^2+\cos(x)=f(x) \implies f$ is symmetric about the y-axis.
It suffices to show that $x^2+\cos x>1$ for $x>0$ due to this symmetry. $x>0 \implies (0,x)$ Since $x^2$ and $\cos x$ are differentiable (and thus continuous) $\forall x\in \mathbb R$, MVT yields that in $(0,x)$, $\exists c \in (0,x)$ such that
$$\frac{(x^2+\cos x)-(0^2-\cos 0)}{x-0}=2c-\sin c$$
$$\frac{x^2+\cos x-1}{x}=2c-\sin c$$
$-1\le\sin c\le1 \implies -1\le-\sin c\le1 \implies 2c-1\le 2c-\sin c\le 2c+1$
$$\frac{x^2+\cos x-1}{x}\ge 2c-1$$
And now i'm stuck because i can't get $x(2c-1)>0$ Is my proof wrong somewhere? UPDATE: Thanks for everyone that gave their input. I managed to solve it using $f(x) = 1-\cos x$ directly. However, i'm still curious as to why my initial method fails? is there something that I overlooked?","['inequality', 'trigonometry', 'calculus', 'analysis']"
1907088,How to prove $\frac{\arccos\frac{\sqrt{3}}{6}}{\pi}$ is not a rational number? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question The number:
$$\frac{\arccos\frac{\sqrt{3}}{6}}{\pi}$$
How to prove it's not a rational number?","['trigonometry', 'rational-numbers']"
1907123,"How many ways can a natural number n be expressed as a sum of one or more positive integers, taking order into account?","Q: The number 4 can be expressed as a sum of one or more positive integers, taking order into account, in 8 ways: 
\begin{array}{l} 4&=1+3&=3+1&=2+2&=1+1+2\\
&=1+2+1&=2+1+1&=1+1+1+1. \end{array}
In general, given $\mathbb n$ $\in$ $\mathbb N$, how many ways can $\mathbf n$ be so expressed? Query: I managed to solve it via a combinatoric proof, but the solution provided is as such:
\begin{align} &~~Idea: n= x_1 + x_2 +...+x_k, k \in \mathbb N, x_k \gt 0.\\
 &\implies x_1 - 1 + x_2 - 1 +...+ x_k -1 = n - k\\ 
 &\implies x_1* + x_2* + ... + x_k* = n-k \qquad-(*) \end{align}
Since $H^n_r$ = \begin{pmatrix} r+n-1 \\ r \end{pmatrix}, we have $H^k_{n-k}$ = 
\begin{pmatrix} n-k+k-1 \\ n-k \end{pmatrix}
And the answer is as such: $$\sum_{k=1}^n H^k_{n-k} =$$ $$\sum_{k=1}^n \begin{pmatrix} n-1 \\ n-k \end{pmatrix} = 2^{n-1}$$.
I have no idea why $H^n_r$ is applied and also why $$\sum_{k=1}^n H^k_{n-k}$$ is used to derive the desired result $$2^{n-1}$$. Some explanation on this will be deeply appreciated.","['permutations', 'combinatorics', 'combinations']"
1907142,"What are ""derivatives of $f$ at point $a \in \text{domain}$""?","The 1994 paper ""Higher Order Derivatives and Differential Cryptanalysis"" uses the following unfamiliar (to me) syntax in a definition: Let $(S,+)$ and $(T,+)$ be Abelian groups. For a function $f : S \to T$, the derivatives of $f$ at point $a \in S$ is defined as
  $$\Delta_af(x) = f(x+a) - f(x)$$ I don't understand this syntax, or how this could be true.  Consider the function $f(x) = x + 9$.  The derivative of this (expressed either as $\frac{d}{dx}$ or $f'(x)$) is 1.  Yet according to the definition given, it should be $$\Delta_2f(x) = f(x+2) - f(x) = 2$$ My questions are as follows. Does the syntax used indicate some different meaning of the term ""derivative""?  If so, what does ""derivative"" mean here? What does it mean to take a ""derivative of $f$ at point $a \in S$""?  Ordinarily I would assume that this meant to take the derivative, then substitute $x=a$ and solve, but that doesn't appear to be what is meant here.","['derivatives', 'calculus']"
1907156,Basic Complex Analysis: Is $0^{1/2}$ undefined?,"So I'm having a tiny crisis here. With real numbers, when faced with an expression like $0^{1/2}$, we simply have that $0^{1/2} = 0$. In fact, $0^x = 0$ $\forall x\ne0$ for real values of $x$.My textbook, however, defines complex exponentiation as: 
$$z^c = e^{c \log{z}}, z \in \mathbb{C}$$
Thus if we're in the complex system, we have that $0^{1/2} = e^{\frac{1}{2} \log(0)}$, which is undefined for all branches of the log function... This incosistency is bothering me. This came about as part of a question that was asking why the principal branch of $z^{1/2}$ doesn't have a Maclaurin series, which I thought was as straightforward as the fact that the derivative doesn't exist at $z = 0$. My textbook claims, however, that $z^{1/2}$ has a singularity at $z = 0$. I don't like leaving tiny things like this unresolved in my mind, so my question I guess is.. Is it indeed the case that $0^{1/2}$ is undefined?","['complex-analysis', 'complex-numbers']"
1907180,Finding the roots of a 3rd degree polynomial,"I found this question from an old math olympiad questionnaire. Let A, B, and C be the roots of $$x^3-4x-8=0$$ Find the numerical value of the expression $$\frac{A+2}{A-2}+\frac{B+2}{B-2}+\frac{C+2}{C-2}$$ I tried using Rational Roots Theorem. And then, I realized this equation involves complex numbers. Afterwhich, I tried various manipulations. But everytime I do manipulate I either go back from where I started or I went off too far and probably did something wrong in my calculations. I also tried tried doing something like ""let u be x squared"" and then tried manipulations there. I did found a value for x which are 0,0,+/-2 sqrt of 3, and I know these are wrong, so I gave up. I do not know how to continue or what else to try. Can anyone help?","['algebra-precalculus', 'polynomials', 'complex-numbers', 'symmetric-polynomials']"
1907184,How to measure how much an discrete geometric model deviates from a sphere,"I have a set of vertices $V$ that resemble a sphere (acquired by 3d-scanning a sphere). Now I need a measure on how much its shape deviates from a sphere. The most simple measure I have is from fitting a sphere by the least-squares method and taking the differences between the fitted radius $r$ and the actual radii from the fitted center $c$. But now I also want a measure on the more systematic kinds of error, such as stretching or skewing along axes and if there are bumps or dents in the surface. I already considered the curvature for discrete geometry, but it is hard to tell which would be the ideal values, as there is no ""perfect"" discrete sphere (i. e. a sphere with $N$ evenly distributed points) I could compare it to. Also, according to Gauss-Bonnet, the total curvature would be the same for a sphere as for an egg. What other options do I have to detect these kinds of deviations?","['discrete-geometry', 'spheres', 'geometry']"
1907196,Inverse Laplace transform of asymptotic expansion,"I recently encountered the following technique in a book on differential equations. Suppose we know the asymptotic expansion for the Laplace transform $F(q)$ of some function $f(z)$ for large $q$. To get an asymptotic expansion of $f(z)$ for small $z$ we simply apply the inverse Laplace transform to the asymptotic series of $F(q)$ term by term. This was done in an informal way in the text, and I'm not that familiar with the theory of asymptotic expansions. Can someone tell me how legitimate this procedure is? Specifically, provided the term by term inversion makes sense, how certain can I be, in general, that I end up with an asymptotic expansion of $f(z)$? Appropriate references also welcome in lieu of answers.","['fourier-analysis', 'laplace-transform', 'asymptotics', 'complex-analysis', 'sequences-and-series']"
1907200,Remainder Theorem when Divisor not linear $x-a$,"Upon receiving a question, it seemed to have needed me to use the remainder theorem however for a divisor that was not linear. Now while I could long divide it (or synthetically), I was wondering how $x-a$ would apply to something like $x^2-a$ or in general $x^k-a$. This would be a great help, thank you :)",['functions']
1907235,When talking about gradient $\nabla f(x)$ are we assume $f(x)$ is scalar valued?,Previously I always thought $\nabla f(x)$ is just the transpose of $Df(x)$. But recently I noticed that I wasn't able to find $\nabla f(x)$ where $f(x)$ is vector value function in my reference. Is it widely accepted that $\nabla f(x)$ should be scalar valued?,"['derivatives', 'calculus']"
1907243,Why is the distance between two circles/spheres that don't intersect minimised at points that are in the line formed by their centers?,"From GRE 0568 From MathematicsGRE.Com : I'm guessing the idea applies to circles also? Is there a way to prove this besides the following non-elegant way? Form a line between centers $C_1$ and $C_2$ Given a point on circle/sphere 1 $(x_1,y_1)$, minimise $$f(x_2,y_2) = (x_1-x_2)^2 + (y_1-y_2)^2$$ to get $(x_2^*,y_2^*)$ Minimise $$g(x_1,y_1) = (x_1-x_2^*)^2 + (y_1-y_2^*)^2$$ to get $(x_1^*,y_1^*)$ Show that the $(x_2^*,y_2^*)$ and $(x_1^*,y_1^*)$ are on the line.","['circles', 'optimization', 'gre-exam', 'calculus', 'spheres']"
1907269,Prove that every prime except for $2$ and $5$ is a factor of some number of the form $111\dots1$ [duplicate],"This question already has answers here : Primes dividing $11, 111, 1111, ...$ [duplicate] (5 answers) Closed 4 years ago . How do we prove that for every prime $p\neq2,5$ there exists a positive integer $n$ such that $(10^n-1)/9$ is divisible by $p$? BTW, I suspect that it holds not only in base $10$, but also in every other base $b$, for every prime which is not a factor of $b$. My work so far - not much, except for manually testing the first few numbers of the form $111\dots1$: $11=11$ $111=3\cdot37$ $1111=11\cdot101$ $11111=41\cdot271$ $111111=3\cdot7\cdot11\cdot13\cdot37$ $1111111=239\cdot4649$ $11111111=11\cdot73\cdot101\cdot137$ Thanks","['number-theory', 'prime-factorization', 'prime-numbers', 'repunit-numbers']"
1907303,Finding polynomial $p(x)$ when it leaves remainders $1$ and $2$ when divided by $x^{100}$ and $(x-2)^3$.,"Find a polynomial $p(x)$ that simultaneously satisfies the following properties: (i) When $p(x)$ is divided by $x^{100}$ the remainder is the constant polynomial $1$ . (ii) When $p(x)$ is divided by $(x − 2)^3$ the remainder is the constant polynomial $2$ . We can write $p(x)=x^{100}g(x)+1=(x-2)^3h(x)+2$ for some polynomial $g$ and $h$ . Then, $p'(x)=x^{99}\left(xg'(x)+100g(x)\right)=(x-2)^2\left((x-2)h'(x)+3h(x)\right)$ , whence $p'$ is divisible by $x^{99}$ and $(x-2)^2$ . So $p'$ is also divisible by $x^{99}(x-2)^2$ . Let $p'(x)=x^{99}(x-2)^2f(x)$ for some polynomial $f$ . Then, $p(x)=f(x)\displaystyle\int x^{99}(x-2)^2\,dx=f(x)\left(\frac{x^{102}}{102}-\frac{4x^{101}}{101}+\frac{4x^{100}}{100}\right)+c$ Now, $p(0)=1\Rightarrow c=1$ and $p(2)=2\Rightarrow f(2)\left(\dfrac{2^{102}}{102}-\dfrac{4\times2^{101}}{101}+\dfrac{4\times2^{100}}{100}\right)+1=2,$ from which I get $f(2)$ and hence $f(x)$ (since I assumed $f$ is constant). Therefore I find $p(x)$ . But is there a justification of treating $f$ as a constant or was it a mistake on my part? I would particularly like to know what are all different ways to solve problems like this? I know it can be solved using the Chinese remainder theorem. Would that be the simplest way to proceed? EDIT. This is a similar question where the remainders are non-constant polynomials.","['algebra-precalculus', 'polynomials', 'elementary-number-theory']"
1907312,Number of ways to select n letters from a word?,"I came across the following question: ""In how many ways can we select 4 letters from the word MISSISSIPPI?"" This question can be solved by considering the different possibilities as follows and adding the numbers: Ways of selecting {(4 alike)+(3 alike and 1 different)+(2 alike and 2 different)+(4 different)} But I directly went and did 11C4 and got a marginally BIG number. What have I done wrong? Why should I approach all word-letter question like this? Edit: I'm truly very sorry that I didn't look at other questions regarding the same topic first but I am glad that I posted again because I have received answers other than the ones related to the generating function, which is a bit too advanced for students at my level; the stars and bars method, as suggested by Shagnik can be understood much better. Please help! Thanks so much in advance :) Regards.",['combinatorics']
1907352,Can somebody show me a proof for this?,"Equation of the Volume of a Cone: $$\mathbf{V}=\frac{1}{3}\pi r^2 h$$ Taking the Derivative in perspective to time because the radii, and height are either decreasing or increasing: $$\frac{dV}{dt}=\frac{\pi}{3}[2rh\frac{dr}{dt}+r^2\frac{dh}{dt}]$$ I understand how to take the derivative of it and all, moreover, I am lost on how to prove this a different way because I have seen it done one way substituting $r$ and $h$ for $r(t)$ and $h(t)$ but do not know any other to prove it. Is there another way to do it, and can somebody please show me?","['derivatives', 'calculus']"
1907368,Quadratic variation of the sum,Let $X_t$ and $Y_t$ be two independent martingales whose quadratic variations are given by $\langle X\rangle_t$ and $\langle Y\rangle_t$ respectively... If we define $Z_t=a X_t+ bY_t$ can we say anything about the quadratic variation of $Z_t$ ? $a$ and $b$ are two constants.,"['stochastic-processes', 'probability', 'quadratic-variation']"
1907378,"If $A^{4}=I$, does this imply $A$ is invertible?","If I have an $n\times n$ matrix, and $A^{4}=I_n$, does this imply that $A^{-1}$ exists? My reasoning is $A^{4}=I$, so $(A^{4})^{-1}=I=(A^{-1})^{4}$. Is this valid? Thanks for your time in answering what is probably a super simple question.",['matrices']
1907406,Discrete Mathematics Resolution Principle.,"I am having a problem in understanding this following excerpt from Discrete Mathematics Book. The Resolution Principle : Given a set S of clauses, a (resolution) deduction of C from S is a finite sequence C 1 , C 2 , ... , C k of clauses such that each C i either is a clause in S or a resolvent of clauses preceding C and C k = C. My questions What does this theorem states ? What is meant by the line ""each C i either is a clause in S or a resolvent of clauses preceding C and C k = C"" ?","['propositional-calculus', 'logic', 'discrete-mathematics']"
1907417,Doubt in the formulation of Cauchy's integral formula,"I'm reading Conway's complex analysis book and on page 84 he says: Quite often I see this version of the theorem: Let $f$ be analytic in the simply connected domain $D$ and let $C$ be
  a simple closed positively oriented contour that lies in $D$. If $z_0$
  is a point that lies interior to $C$, then $$f(z_0)=\frac{1}{2\pi
 i}\int_C\frac{f(z)}{z-z_0}$$ (From this book page 235) The Conway's formulation seems more general. The problem is I can't understand why Conway's formulation implies in the second formulation. Is the winding number in the case of the second formulation one?",['complex-analysis']
1907433,Centre of mass on a sphere,"Suppose that $p_1,\ldots,p_n$ are points on the round sphere $S^2$ and that $p_i$ has a mass $m_i$. How would one go about computing the position $\overline p$ of their centre of mass? I guess one question is, how should we even define the centre of mass? I think it's reasonable to say the notion should correspond the to idea that, if we place the sphere on a surface with the centre of mass being the point of contact with the surface, then the sphere should not roll (i.e. their torques with respect to any great circle through $\overline p$ should cancel). This leads us to note that the antipodal point of the centre of mass will be another centre of mass, though probably only one of them will be the ""stable"" centre off mass, corresponding to the idea that, if the points are confined to a sufficiently small part of the sphere, there is really only one centre of mass ""in the middle of them"".","['classical-mechanics', 'centroid', 'calculus', 'geometry']"
1907439,Providing counterexamples for a false statement,"Let $A, B, C$ be three sets so that the family $\{A, B, C\}$ is disjoint. Then at least one of $A \cap B, B \cap C, A \cap C$ has no element. Since they are disjoint sets, shouldnt the sets have no common intersection, which would make this sentence true?",['elementary-set-theory']
1907442,"Generalization of Hahn-Banach theorem: given $f : U \to W$, can we always extend it to get $g : V \to W$?","We know Hahn-Banach theorem: ""If $V$ is a normed $K$-vector space with linear subspace $U$ (not necessarily closed) and if $f : U \to K$ is continuous and linear, then there exists an extension $g : V \to K$ of $f$ which is also continuous and linear and which has the same norm as $f$."" Does it stay true if we replace $K$ by any $K$-vector space $W$? Or is there a counter-example? To be clear: is there any example of a normed space $V$, a subspace $U \subset V$, a normed space $W$ and a continuous linear map $f : U \to W$ which has no extension $V \to W$? (I'm trying to see if, given any normed vector spaces $V$ and $W$, it is possible to build a non-trivial continuous linear map $g : V \to W$. Is it always possible? Apparently there are sufficiently many such $g$, see here (and using Hahn-Banach theorem to get $X' \neq \{0\}$)). Thank you very much!","['functional-analysis', 'normed-spaces']"
1907472,Proving Deligne's formula,"If $A$ is a noetherian ring, $X = \text{Spec } A$, and $U = X - V(\mathfrak{a})$ for $\mathfrak{a}$ an ideal of $A$, then the following formula holds for any $A$-module $M$ (with associated $\mathcal{O}_X$-module $\tilde{M}$):
$$\Gamma(U,\tilde{M}) \cong \text{colim}_n \hom_A(\mathfrak{a}^n,M).$$
I've tried to prove this result by showing $\Gamma(U,\tilde{M})$ satisfies the required universal property. To that end, I defined a cocone to $\Gamma(U,\tilde{M})$ from the $\hom_A(\mathfrak{a}^n,M)$, taking
$$h_n: \hom_A(\mathfrak{a}^n,M) \to \Gamma(U,\tilde{M}), \quad h_n(f)(\mathfrak{p}) = \frac{f(x)}{x} \in M_\mathfrak{p},$$
where $\mathfrak{p} \in D(x) = V(x)^c$ for $x \in \mathfrak{a}^n$. It's easy to check that $h_n(f)$ is a well-defined section over $U$. The next step is to show that for any other cocone $(C, \gamma_n)$, there is a unique mediating morphism $\theta: \Gamma(U,\tilde{M}) \to C$. Finding $\theta$ should depend on realising an arbitrary $g \in \Gamma(U,\tilde{M})$ as a value of some $h_n$, but this is where I've become stuck. Are my maps $h_n$ wrong? If not, how to produce $g = h_n(f)$? Reductions so far: since $A$ is noetherian, $g$ can be represented as a tuple $\left ( \frac{m_1}{x_1^r}, \dots, \frac{m_k}{x_k^r} \right )$, where $m_i \in M$, the $x_i$ generate $\mathfrak{a}$, and $r$ is large.","['schemes', 'algebraic-geometry', 'commutative-algebra']"
1907508,Bijective mappings and transpositions,"While self-teaching bijective mappings I came across the following question: 
Given that the bijective mapping $f : X \mapsto Y$ exists and $x$ and $y$ are elements of $X$ and $Y$ respectively, prove that there exists a bijection $g : X \mapsto Y$ such that $g(x)=y$. 
I am still new to the concepts of bijective mappings etc. but the proof I came up with was as follows: If $y = f(x)$ then it is obvious that such a function $g$ exists and that $g=f$, and since $f$ is bijective then $g$ must also be bijective. Now suppose that $f(x) = y_0 \neq y$. Then there will exist a composite function $g(x) = (\tau_{y_0, y}\circ f)(x) = y$ where the $\tau$ notation is a transposition of elements and is bijective, so then there must exist some $g(x) = y$ Am I going about the proof in the wrong way, or is this OK?","['functions', 'proof-verification']"
1907512,Proving that two projections are at most distance 1 apart,"I am trying to solve the very first problem in ""The Blue Book"" by M. Rørdam. The problem is the following: Let $p$ and $q$ be projections in a $C^*$-algebra $A$. Prove that $\Vert p - q \Vert \leq 1$. To me, it seems like this problem should have a simple and elegant solution, but I am not able to produce it. I have managed to reduce the problem slightly as follows. Put $s = p-pq$ and $ t = q-pq$. By adding and subtracting $pq$ we obtain $\Vert p - q \Vert = \Vert (p-pq) - (q-pq) \Vert = \Vert s-t \Vert.$ It is straightforward to check that $s^*t = 0 = t^*s$ and that $st^* = 0 = ts^*$. This means that if we realize $A$ on a Hilbert space, then the operators $s$ and $t$ have orthogonal range and domain. I would now like to conclude that $\Vert p - q \Vert = \Vert s-t \Vert \overset{?}{\leq} \max\{ \Vert s \Vert, \Vert t \Vert \} = 1$. The last equality holds since $\Vert s \Vert = \Vert p(1_{\tilde A} - q) \Vert \leq 1$ since $p$ and $1_{\tilde A} - q$ are both projections, and similarly for $t$. So, what I really would like to know is whether the bound above actually holds, and if so, why. Namely: Whether two operators $S,T \in \mathcal{B}(H)$ satisfying $S^*T = 0$ and $ST^* = 0$ also satisfy $\Vert S-T \Vert \leq \max\{ \Vert S \Vert, \Vert T \Vert \}$? Alternative proofs (or mere suggestions) of the originial problem is also appreciated!","['functional-analysis', 'c-star-algebras', 'operator-algebras', 'operator-theory']"
1907517,Prove $\alpha+\beta+\gamma \ge \alpha\frac{\sin \beta}{\sin \alpha}+\beta\frac{\sin \gamma}{\sin \beta}+\gamma\frac{\sin \alpha}{\sin \gamma}$,"Prove the inequality $$\alpha+\beta+\gamma \ge \alpha\cdot\frac{\sin \beta}{\sin \alpha}+\beta\cdot\frac{\sin \gamma}{\sin \beta}+\gamma\cdot\frac{\sin \alpha}{\sin \gamma}$$ for any three numbers $\alpha, \beta, \gamma \in \left(0;\frac{\pi}2\right)$ My attempt: I plan to use Rearrangement inequality . But I do not know how to start.","['inequality', 'trigonometry']"
1907566,Maximum value of trigonometric equation $\cos^2(\cos\theta) + \sin^2(\sin\theta)$,"For any real $\theta$ the maximum value of $$\cos^2(\cos\theta) + \sin^2(\sin\theta)$$ A. $1$ B. $1 + \sin^21$ C. $1 + \cos^21$ D. does not exist I tried it by converting the whole expression into $\sin$ but getting nowhere with that. $$1-\sin^2(\cos\theta) + \sin^2(\sin\theta)$$ Now since 1 is constant therefore, $$[\sin^2(\cos\theta) + \sin^2(\sin\theta)]$$ should be minimum but I don't know how to minimize it. Also is there a way to think about it's solution graph. I have to solve this without using calculus.
Kindly help.",['trigonometry']
1907569,On pullback of global sections of invertible sheaves,Let $f:X \to Y$ be a dominant/surjective morphism of projective schemes and $\mathcal{L}$ an invertible sheaf. Is it true that $H^0(\mathcal{L})=H^0(f^*\mathcal{L})$? The fact I am not totally sure of is whether $H^0(f^*\mathcal{L})=H^0(f_*f^*\mathcal{L})$? If this is true then the statement follows from the projection formula.,"['global-dimension', 'sheaf-cohomology', 'algebraic-geometry', 'commutative-algebra']"
1907630,Show that $f(z)=\bar{z}$ is nowhere analytic,"Let us consider the function: $f(z)=\bar{z}$. How can I show that this function is nowhere analytic? It is evident that $$\lim\limits _{z\to 0} \frac{f(z)-f(0)}{z-0}=\lim\limits_{z\to 0}\frac{\bar{z}}{z}$$ does not exist, but I am stuck to show it is not differentiable on the entire plane.","['complex-analysis', 'analytic-functions']"
1907634,Is it possible that a Runge-Kutta method has a global error of 0?,"In a quiz of a course I am taking, there was the following multiple choice question: Decide which of the following statements is false, and justify your election showing a counterexample: (a) There are numerical methods for solving an Initial Value Problem of fifth order (b) There are integrals that cannot be computed exactly using the midpoint rule. (c) Every numerical solution of an Initial Value Problem generated by a Runge-Kutta method will have a global error greater than zero (d) Newton's method allows finding solutions for a non linear system of equations. (b) and (d) are obviously true (I can easily  find examples of these). (a) Every numerical method we covered in the class are for first order differential equations, but for higher order you can do variable substitution to get a system of first order equations that can be solved using the methods. So I can say (a) is true. Discarding every other choice, I came to the conclusion that (c) is false, but I cannot find a counter example to this. It is supposed that the global error for a $RK_{pq}$ is proportional to the step $h$, $global\_error = Ch^p$, this will always be greater than zero unless C is 0, but I cannot come up with an example to this. Is this statement true or false and could someone give me an example?","['numerical-methods', 'ordinary-differential-equations', 'runge-kutta-methods']"
1907650,Optimization on a Riemannian manifold,"Consider a Riemannian manifold $(M,g)$, which we assume geodesically complete. Given a smooth function $f : M \to \mathbb{R}^+$. Assume that $f$ has a global minimum on $M$. I am interested in the following optimization problem :
$$ \text{Find } p^{\ast} \in \mathop{\mathrm{argmin}} \limits_{p \in M} f(p). $$
Let $\nabla^{g}f$ denote the gradient of $f$ with respect to the Riemannian metric $g$. Let $p \in M$. By definition, the gradient of $f$ at $p$, denoted by $\nabla^{g}f(p)$ is the vector in $\mathrm{T}_{p}M$ which satisfies to :
$$ \forall v \in \mathrm{T}_{p}M, \, df_{p}(v) = \left\langle \nabla^{g}f(p),v \right\rangle_{p}$$
The critical points of $f$ are the points $q \in M$ such that $\nabla^{g}f(q)=0$. If $M$ was the Euclidean space $\mathbb{R}^n$, one could start by finding the critical points of $f$ and then check whether a given critical point is a minima or a maxima of $f$. My question is : why can we also proceed like this for Riemannian manifolds ? In other words : what ensures that the extrema of $f$ are among the critical points of $f$ ? Is it based on a Talyor expansion ? This point is not clear to me.","['optimization', 'riemannian-geometry', 'differential-geometry', 'maxima-minima']"
1907652,A total well ordered set cannot be dense,"Let $E$ be totally ordered under $<$ , and dense. I would like to prove that a well ordered set is never dense. To prove this, I tried to fid a non-empty subset $X$ of $E$ such that there is no least element. So, I take $X=\{z\in E: \forall x,y\in E\quad x<z<y\}$, it's non-empty because E is dense. If I assume that there is a least element, noted $l$. We have $$\forall w\in X\quad l<w.$$ As $l\in X$ and $<$ is dense, we can find an element between $x$ and $l$, therefore $l$ is not a least element. Is it correct?","['order-theory', 'elementary-set-theory', 'proof-verification']"
1907660,Prove periodic solution of an ODE,Consider the differential equation $x'=x+cos(t) $ a) find the general solution of this equation b) prove there is a unique periodic solution for this equation c) compute the Poincaré map $p:{t=0} \to {t=2\pi}$ for this equation and use this to verify again that there is a unique periodic solution. I solved part a) and found $x(t)=\frac{1}{2}sin(t)-\frac{1}{2}cos(t)+ce^t$ but I'm unsure how to prove the solution is periodic or how to do the map.,"['periodic-functions', 'ordinary-differential-equations']"
1907671,Find the value of $g \left(\frac{1}{4} \right)+g \left(\frac{1}{2} \right)+g \left(\frac{5}{4} \right)$,"If $$f(x)=4x^3-x^2-2x+1$$ and $$g(x)=\begin{cases} 
      \min\{f(t):0 \leq t\leq x\} & 0\leq x\leq 1\\
      3-x & 1\leq x\leq 2 
   \end{cases}$$
then find the value of $$g \left(\frac{1}{4} \right)+g \left(\frac{1}{2} \right)+g \left(\frac{5}{4} \right)$$ Could someone explain to me what does the condition $\min\{f(t):0 \leq t\leq x\}$, $  0\leq x\leq 1$ mean and how to use it here?","['calculus', 'functions']"
1907749,How to find the center of a circle and its radius in a 3D space given 3 points,"I'm working on writing a code in Octave (C++) for a helical spring. I need to figure out the center line of this spring in an effort to find any trends between different spring platforms as the geometry changes throughout the manufacturing processes. I have a file with the center line of the wire, which is made up of 2000 or so points. Since I learned that you need 3 points to make a circle, I'm figuring i will be able to approach it that way (but haven't figured out how) so.... Given 3 points in 3 Dimensional space, how could I find the center and the radius of a circle in a code friendly manor. Thanks","['circles', 'coordinate-systems', 'analytic-geometry', 'geometry', '3d']"
1907759,Inequivalent norms,"On an infinite dimensional, do there exist two norms which are not equivalent? I actually know that on a infinite dimensional space, the number of inequivalent norms are $ 2^{dimX} $
However, I want to know if we can explicitly construct two inequivalent norms on any given space.","['functional-analysis', 'normed-spaces']"
1907773,The Determinant of an Operator Equals the Product of Determinants of its Restrictions,"Consider the following definitions and proved theorems Definitions $1$. The field $\Bbb{F}$ is $\Bbb{R}$ or $\Bbb{C}$. $2$. $V$ is a vector space over $\Bbb{F}$. $3$. $\mathcal{L}(V)$ is the vector space of all operators on $V$ (that is, linear maps $V \to V$) and $T \in \mathcal{L}(V)$. $4$. $T|_{U}$ is the restriction of $T$ to the invariant subspace $U$. $5$. $G(\lambda_i,T)$ is the generalized eigen-space of $T$ corresponding to eigen-value $\lambda_i$. The multiplicity of $\lambda_i$ is defined to be $d_i=\dim G(\lambda_i,T)$. $6$. If $\Bbb{F}=\Bbb{C}$ then $\lambda_1,\cdots,\lambda_m$ are distinct eigen-values of $T$.  If $\Bbb{F}=\Bbb{R}$ then $\lambda_1,\cdots,\lambda_m$ are distinct eigen-values of  the complexification of $T$, denoted by $T_{\Bbb{C}}$. Then $\det T = \lambda_1^{d_1}\cdots\lambda_m^{d_m}$ where each $d_i$ is the multiplicity of $\lambda_i$. Proved Theorems $1$. Suppose $V$ is a vector space over $\Bbb{C}$ and $T \in \mathcal{L}(V)$. Let $\lambda_1,\cdots,\lambda_m$ be distinct eigen values of $T$. Then
  $V=G(\lambda_1,T) \oplus \cdots \oplus G(\lambda_m,T)$ where each $G(\lambda_i,T)$ is invariant under $T$ . Furthermore, $\dim V = d_1+\cdots+d_m$ where $d_i$ is the multiplicity of $\lambda_i$. Now, I want to prove the following theorem just using the above tools. Any hint or help is appreciated. Question If $T \in \mathcal{L}(V)$ and $V=V_1 \oplus \cdots \oplus V_M$ with each $V_j,\,j=1,\cdots,M$ invariant under $T$ then $\det T = \det T|_{V_1} \cdots \det T|_{V_M}$. This problem happened in example $10.28$ of the book Linear Algebra Done Right . There was just one sentence for the proof of this in the text. Because the dimensions of generalized eigen-spaces in $V_j$ add up to $\dim V$. but I don't understand it!",['linear-algebra']
1907783,Largest $n\times m$ matrix with no $n\times (n+1)$ submatrix having $n$-wise independent columns,"What is the largest $m$ for which an $n \times m$ matrix with the given property that any subset of $n+1$ columns have rank $n$, may contain no subset of $n+1$ columns in which the columns are $n$-wise independent? In general one can see that the assumptions imply that $m \leq 2n$, but I feel it should be possible to determine $m$ exactly. If $n = 3$ I can verify by hand that the maximum $m$ is $5$. What should it be in general? Edit: As some people have pointed to, the question can be generalized to matroids. However in the interest of keeping things concrete, I am willing at this point to accept any answer that exhibits a matrix with entries over any field that improves the general bound $n+2 \leq m \leq 2n$. If the example of Heptagon can be extended to arbitrary $n$, this would for instance show that $2n-1 \leq m \leq 2n$ over the set of realizable matroids.","['combinatorics', 'linear-algebra']"
1907922,"Continuum Hypothesis - Why does my ""proof"" fail?","While thinking about the Continuum Hypothesis, I stumbled across a way of thinking about it that seems to intuitively make sense to me. But, being that I'm not a mathematician and Gödel/Cohen together have shown that the $\sf{CH}$ is independent of the $\sf{ZFC}$ axioms, I understand that this is $99.999999999999\%$ likely to be wrong. Anyway, here is my thinking, and if possible, I'd like to know where it is that I went awry: Idea: The assumption underlying my ""proof"" is that the number of natural numbers you need to ""index"" a set corresponds to its cardinality. Therefore, if you have a set $A$ with a cardinality $a$ , and a set $B$ with a cardinality $b$ , and you need $i$ indices to index $A$ , and $j$ indices to index $B$ , and you can show that there exists no number of indices between $i$ and $j$ that would generate a different cardinality than either $a$ or $b$ , then there must be no cardinalities between $a$ or $b$ . So, let me show you what I mean by indices. The members of the set of natural numbers need only $1$ natural number to index them. Duh. $1$ gets labeled by $1$ , $2$ by $2$ , etc. So, a set with cardinality $\aleph_0$ only needs $1$ index. We also know that ordered $n$ -tuples (with finite $n$ ) also are countable, requiring only $1$ index to label them. What is the next possible number of indices you could require? $\aleph_0$ . The very next possible number of indices that would generate a different cardinality is $\aleph_0$ . If you have $\aleph_0$ indices, each index being a natural number, the set of all such objects would basically be an ordered $\aleph_0$ -Tuple. The cardinality of such a set is that of the continuum. Therefore, since there is no possible number of indices to label members of sets between finite natural numbers and $\aleph_0$ , there are no cardinalities between $\aleph_0$ and the cardinality of the continuum. Why wouldn't something like this work? It seems like either something outside my assumption is wrong, or my assumption is independent of $\sf{ZFC}$ , and it's just one of many possible axioms that $\sf{ZFC}$ could be extended with...",['elementary-set-theory']
1907925,Can one partition the plane $\mathbb{R}^2$ by closed intervals of equal length?,"I found in this post the following question: Can one partition the plane $\mathbb{R}^2$ by closed intervals of equal length? Then it is written: ""The answer to the first question is ""yes"""". My question is: why is the answer 'yes'? I think that the length should be $>0$ , otherwise it is obviously true and uninteresting. According to Partitioning $[0,1]$ into pairwise disjoint nondegenerate closed intervals , Can $\mathbb R$ be written as the disjoint union of (uncountably many) closed intervals? , Is $[0,1]$ a countable disjoint union of closed sets? , this can't be done in dimension $1$ . Thank you!","['general-topology', 'real-numbers']"
1907927,Quickest self-contained way of finding $\pi_1(\text{SU}(2))$ and $\pi_1(\text{SO}(3))$?,"As the question title suggests, what is the quickest self-contained way of computing $\pi_1(\text{SU}(2))$ and $\pi_1(\text{SO}(3))$?","['algebraic-topology', 'abstract-algebra', 'general-topology', 'lie-groups']"
1907974,"Divisor on an arithmetic surface and ""base change""","Fix a number field $K$ with ring of integers $O_K$; moreover $\sigma:K\to \mathbb C$ is an embedding of fields. Let $X\to\operatorname{Spec} O_K$ be an arithmetic surface ($X$ is regular and projective amd geometrically irreducible) and let $D=\sum_Yv_Y(D)Y$ be a divisor in $X$. I was wondering what is the object usually indicated in the literature with the symbol  with $D_\sigma$
  (see for example Moriwaki's book about Arakelov geometry
  at page 101, line -6). Here my guess : Usually $X_\sigma$ is the Riemann surface defined as the base change $X\times_{\operatorname{Spec} O_K}^\sigma \operatorname{Spec} \mathbb C$ through the morpshism $\operatorname{Spec}(\sigma):\operatorname{Spec} \mathbb C\to\operatorname{Spec} O_K$.The pullback of $D$ through the projection $p: X_\sigma\to X$ is  a well defined divisor, since $p$ is flat. Therefore we put $D_\sigma:=p^\ast D$. Is this argument correct?","['algebraic-geometry', 'number-theory', 'arithmetic-geometry', 'schemes', 'divisors-algebraic-geometry']"
1907979,Rolling an $n$-sided die until repeat is found,"Problem: We are rolling an $n$-sided die. You roll until you reach a number which you have rolled previously. I need to calculate the probability $p_m$ that we have rolled $m$ times for such a repeat. My first thought was to try some inputs. I took $n=6$. I noticed that when $m=1$, we will always get a probability of $0$, since you are only rolling one time. Also, for $m>7$, we will also have $0$, since we will never reach that case. Now, I don't get how to find a general formula for when $1<m<8$",['probability-theory']
1908005,Relationship between l'Hospital's rule and the least upper bound property.,"Statement of L'Hospital's Rule Let $F$ be an ordered field. L'Hospital's Rule. Let $f$ and $g$ be $F$-valued functions defined on an open interval $I$ in $F$. Let $c$ be an endpoint of $I$. Note $c$ may be a finite number or one of the symbols $-\infty$,$+\infty$. Suppose $f'(x)$ and $g'(x)$ are defined everywhere on $I$. Suppose $g(x)$ and $g'(x)$ are never zero and never change sign on $I.$ Suppose one of the following two hypothesis is satisfied: $\displaystyle \lim_{x \rightarrow c} f(x) = \lim_{x \rightarrow c} g(x) = 0$ $\displaystyle \lim_{x \rightarrow c} g(x) = \pm \infty$ Suppose $$\displaystyle \lim_{x\rightarrow c} \frac{f'(x)}{g'(x)} = L$$ 
where $L$ is a finite number or one of the symbols $-\infty$, $+\infty$. Then  $$\displaystyle \lim_{x\rightarrow c} \frac{f(x)}{g(x)} = L.$$ Motivation The standard proof of l'Hospital's rule (when $F=\mathbb{R}$) uses Cauchy's mean value theorem. See: Taylor, A. E. (1952), ""L'Hospital's rule"", Amer. Math. Monthly, 59: 20–24 https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule#General_proof Apostol, ""Mathematical Analysis"" For an ordered field, the mean value theorem is equivalent to the least upper bound property. See: Equivalence of Rolle's theorem, the mean value theorem, and the least upper bound property? ) https://arxiv.org/abs/1204.4483 Another proof of l'Hospital's rule can be based on the convergence of bounded monotone sequences and the statement that $f' > 0$ on an interval $I$ implies $f$ strictly increasing on $I$. See Taylor, A. E. (1952), ""L'Hospital's rule"", Amer. Math. Monthly, 59: 20–24. The convergence of bounded monotone sequences is equivalent to the least upper bound property. The statement that $f' > 0$ on $I$ implies $f$ strictly increasing on $I$ is also equivalent to the least upper bound property. L'Hospital's rule is false for $F=\mathbb{Q}$. The following proof outline is adapted from Exercise 7.8 of Korner's book ""A Companion to Analysis."" Choose a sequence $a_n \in \mathbb{R} \setminus \mathbb{Q}$ with $4^{-n-1} < a_n < 4^{-n}$ for $n=1,2,\ldots$. Define $I_0 = \{x \in \mathbb{Q} : a_0 < x \}$ and $I_n = \{x \in \mathbb{Q} : a_n < x < a_{n-1}\}$. Notice $4^{-n-1} < x< 4^{-n}$ whenever $x \in I_n$. Define $f:\mathbb{Q} \rightarrow \mathbb{Q}$ by $f(0)=0$ and $f(x) = 8^{-n}$ if $|x| \in I_n$. Remember we work in the ordered field $F=\mathbb{Q}$. We have $f'(x)=0$ for all $x \in \mathbb{Q}$, and 
$$
\lim_{x \to 0}\frac{f(x)}{x^2} = \infty \quad \text{and} \quad \lim_{x \to 0}\frac{f'(x)}{2x} = 0.
$$ Question What is the logical relationship between l'Hospital's rule and the least upper bound property? Does l'Hospital's rule imply the least upper bound property? Or is there an ordered field with l'Hospital's rule but not the least upper bound property? Related Here is a related question. Limit of the derivative and LUB It asks (in my notation) whether the following differentiability criteria implies the least upper bound property. Differentiability Criteria. Let $f$ be an $F$-valued function defined on open interval $I$. Suppose $f$ is continuous on $I$. Suppose $f$ is differentiable on $I$ except at one point $c$ in $I$. If $\lim_{x \rightarrow c} f'(x)$ exists, then $f'(c)$ exists and equals this limit. L'Hospital's rule implies this differentiability criteria. See for example https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule#Corollary","['real-analysis', 'reverse-math', 'limits-without-lhopital']"
1908008,Why isn't there a contravariant derivative? (Or why are all derivatives covariant?),"Question: If there exists a covariant derivative, then why doesn't there also exist a ""contravariant derivative""? Why are all or most forms of differentiation ""covariant"", or rather why do all or most forms of differentiation transform covariantly? What aspect of differentiation makes it intrinsically ""covariant"" and intrinsically "" not contravariant""? Why isn't the notion of differentiation agnostic to ""co/contra-variance""? Motivation: To me it is unclear (on an intuitive, i.e. stupid/lazy, level) how notions of differentiation could be restrained to being either ""covariant"" or ""contravariant"", since any notion of differentiation should be linear*, and the dual of any vector space is exactly as linear as the original vector space, i.e. vector space operations in the dual vector space still commute with linear functions and operators, they same way they commute with such linear objects in the original vector space. So to the extent that the notion of linearity is ""agnostic"" to whether we are working with objects from a vector space or from its dual vector space, so I would have expected any notion of differentiation to be similarly ""agnostic"". Perhaps a better word would be ""symmetric"" -- naively, I would have expected that if a notion of ""covariant differentiation"" exists, then a notion of ""contravariant differentiation"" should also exist, because naively I would have expected one to exist if and only if the other exists. However, it appears that no such thing as ""contravariant derivative"" exists ( see here on Math.SE , also these two posts [a] [b] on PhysicsForums), whereas obviously a notion of ""covariant derivative"" is used very frequently and profitably in differential geometry. Even differential operators besides the so-called ""covariant derivative"" seemingly transform covariantly, see this post for a discussion revolving around this property for the gradient. I don't understand why this is the case. (* I think)","['derivatives', 'tensors', 'riemannian-geometry', 'differential-geometry']"
1908035,"Is $\{x\mid\frac{x}{2} \in \mathbb{N}, x \in \mathbb{N}\}$ not a valid representation of multiples of two?","In my pre-calculus class, I suggested $\{x\mid\frac{x}{2} \in \mathbb{Z}, x \in \mathbb{Z}\}$ as set-builder notation for multiples of two. I was told that is not correct, with the correct answer being $\{x\mid 2n=x, x \in \mathbb{Z}\}$. To my understanding, the conditionals simply need to be true in order for a number to be contained in the set, and all multiples of two satisfy the conditionals in my notation. Why is is that my answer is incorrect? I apologize for my lack of vocabulary, I have only been introduced this concept less than a week ago.",['elementary-set-theory']
1908043,Statistics - Expectation of OLS residual squared,"Suppose that $y = \beta_0+\beta_1x + \epsilon$ where $\operatorname{E}[\epsilon\mid X]=0$ and $\operatorname{Var}[\epsilon\mid X] = \sigma^2$. In what special case is $\hat{\epsilon_i}^2$ an unbiased estimator for $\sigma^2$? Is $\hat{\epsilon_i}^2$ a consistent estimator for $\sigma^2$? I have proved that $$E[\hat{\epsilon_i}^2] = \sigma^2\left(1-\frac{1}{n}-  \frac{(x_i -\bar{x})^2}{\sum_{j=1}^n (x_j -\bar{x})^2} \right)$$ But I don't know how it can be unbiased or maybe I have proved wrong? My proof: 
\begin{equation}
\begin{split} \nonumber 
\hat{\epsilon_i} & = y_i - \hat{y_i} \\ 
& = \beta_0 + \beta_1 x_i + \epsilon_i - \hat{\beta_0} - \hat{\beta_1}x_i \\ 
& = \epsilon_i + (\beta_0 - \hat{\beta_0}) + (\beta_1 - \hat{\beta_1 })x_i \\ 
\hat{\beta_0} &= \bar{y} - \hat{\beta_1} \bar{x} \\ 
& = \frac{1}{n} \sum_{i=1}^{n}(\beta_0 + \beta_1 x_i + \epsilon_i) - \hat{\beta_1} \bar{x} \\ 
& = \beta_0 + (\beta_1 - \hat{\beta_1})\bar{x} + \frac{1}{n} \sum_{i=1}^{n} \epsilon_i \\
\hat{\beta_1} &= \frac{\sum (x_i - \bar{x}) y_i}{\sum (x_i - \bar{x})^2} \\ 
& = \frac{\sum (x_i - \bar{x})(\beta_0 + \beta_1 x_i + \epsilon_i)}{\sum (x_i - \bar{x})^2} \\ 
& = \beta_1 + \frac{\sum (x_i - \bar{x})\epsilon_i}{\sum (x_i - \bar{x})^2} \\ 
\therefore \hat{\epsilon_i} &= \epsilon_i - \frac{1}{n}\sum_{j=1}^{n} \epsilon_j + (\beta_1 - \hat{\beta_1})(x_i - \bar{x}) \\ 
& =  \epsilon_i - \bar{\epsilon} - \frac{\sum_{j=1}^{n} (x_j - \bar{x})\epsilon_j}{\sum_{j=1}^{n} (x_j - \bar{x})^2}  (x_i - \bar{x}) \\ 
\therefore \hat{\epsilon_i}^2 & = \epsilon_i^2 + \bar{\epsilon}^2 +\frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} [\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j]^2 \\
& -2 \epsilon_i \bar{\epsilon} - 2\epsilon_i \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j + 2 \bar{\epsilon} \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j \\                                                     
\end{split}
\end{equation}
Now take expectation of the six parts of $\hat{\epsilon_i}^2$ given X (conditional symbol is omitted): 
\begin{equation}
\begin{split} \nonumber 
(1) \ E[\epsilon_i^2] &= \sigma^2 \\ 
(2) \ E[\bar{\epsilon}^2] &= \frac{\sigma^2}{n} \\ 
(3) \ E[\cdot] & =  \frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} E\left[[\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j]^2 \right] \\ 
 &= \frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} E\left[ \sum_{j=1}^{n}(x_j-\bar{x})^2 \epsilon_j^2 + 2 \sum_{j=1}^{n-1} \sum_{k=j+1}^{n} (x_j-\bar{x})\epsilon_j (x_k-\bar{x})\epsilon_k \right] \\ 
 & =  \frac{(x_i -\bar{x})^2 \sigma^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \\ 
(4) \ E[-2\epsilon_i \bar{\epsilon}] & = -\frac{2}{n} E[\epsilon_i \sum_{j=1}^{n} \epsilon_j] \\ 
& =  -\frac{2\sigma^2}{n} \\ 
(5) \ E[\cdot] &= -2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2}  E\left[ \epsilon_i \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j\right] \\ 
& =  -2 \frac{(x_i - \bar{x})^2 \sigma^2}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \\ 
(6) \ E[\cdot] &= 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} E\left[ \bar{\epsilon} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j\right] \\ 
& = 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \frac{1}{n} E[\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j^2] \\ 
& = 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \frac{1}{n} \sigma^2 \sum_{j=1}^{n} (x_j -\bar{x}) \\ 
& = 0 \\ 
\therefore E[\hat{\epsilon_i}^2] & = \sigma^2 - \frac{\sigma^2}{n} -  \frac{(x_i -\bar{x})^2 \sigma^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \\  
& = \sigma^2\left(1-\frac{1}{n}-  \frac{(x_i -\bar{x})^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \right)
\end{split}
\end{equation}",['statistics']
1908080,"Can floor functions be used to ""cycle"" between values?","As I was graphing functions in Desmos graphing calculator, I typed in the function $$\lceil{x-\lfloor{x}\rfloor}\rceil$$ which, after some reasoning, unsurprisingly generates the values $0$ or $1$. My question is, can you - with any given amount of floor and ceiling functions - get a the values $0,1$ and $2$? In general, can you prove that with any amount of floor and ceiling functions you can obtain only whole numbers from $0$ to $n$. Note??? Modular/remainder functions aren't allowed.
I'm still in high school so I would appreciate an informal way of either proving the existence of such a function or actually showing a function that can cycle from $0$ to $n$.","['functions', 'ceiling-and-floor-functions']"
1908101,Evaluating this complicated integral using complex analysis,"I am trying to evaluate this integral: $$\boxed{\int_{-\pi}^\pi\sin(2\cos\theta)\cos((2m+1)\theta)\,d\theta}$$
where $m\in\mathbb{N}$, using complex analysis methods. What I have done is to find the residue of $\sin(z+\frac 1z)$: Since $$\sin(z+\frac 1z)=\sum_{k=0}^\infty\frac{(-1)^k(z+z^{-1})^{2k+1}}{(2k+1)!}$$, and using the Binomial theorem, we have $$(z+z^{-1})^{2k+1}=\sum_{r=0}^{2k+1}{{2k+1}\choose r}z^{2k+1-2r}$$. Combining these two facts we have that the residue (coefficient of $z^{-1}$) is:
$$\sum_{k=0}^\infty\frac{(-1)^k}{(2k+1)!}{{2k+1}\choose{k+1}}$$. After this I am kind of stuck. I am aware of the standard technique of substituting $\cos\theta=\frac{z+z^{-1}}{2}$, $d\theta=\frac{dz}{iz}$, etc, but I am not sure what to do with the $\cos((2m+1)\theta)$. Thanks for any help. By the way, if it helps, the answer I am supposed to get is $$\boxed{2\pi\sum_{k=m}^\infty\frac{(-1)^k}{(2k+1)!}{{2k+1}\choose {k-m}}}$$. Update: Using David Holden's excellent hint, I proceded to find the residue of the integrand. So the integral becomes $\frac{1}{2i}\int_C\sin(z+z^{-1})(z^{2m}+z^{-2m-2})\,dz$. I find the residue of the integrand to be $$\sum_{k=m}^\infty \frac{(-1)^k}{(2k+1)!}{{2k+1}\choose{k+m+1}}+\sum_{k=0}^\infty\frac{(-1)^k}{(2k+1)!}{{2k+1}\choose{k-m}}$$ I may have made a mistake along the way, I can't get to the final answer.",['complex-analysis']
1908112,Counting Subsets of a Set—how does this work?,"I know a couple of ways to do this. However, I couldn't quite follow the logic of the accepted answer in this question... What is the proof that the total number of subsets of a set is $2^n$? ...which says that there are two possibilities of an element being present in any subset of a given set, so all of them are multiplied. I fail to understand exactly how many times I should do that. And that exactly is the answer. It doesn't seem that the answer explains this issue, but how does it work then?","['combinatorics', 'elementary-set-theory', 'discrete-mathematics']"
1908127,"If $f(x/n)\to0$ when $n\to\infty$, for every $x$ and $f$ is continuous, then $f(x)\to0$ when $x\to0$, or not?","Let $f:(0,\infty)\to \Bbb R$ be continuous and such that for each $x>0$ the sequence $\{f(\frac{x}{n})\}\to 0$. Does it imply that $\lim_{x\to 0^+} f(x)=0$? My try :Fix $x>0$.Since $f$ is continuous then $\lim _{n\to \infty}f(\frac{x}{n})=f(\lim_{n\to \infty} \frac{x}{n})=f(0)\implies f(0)=0$ Since $f$ is continuous and $f(0)=0\implies \lim _{x\to 0^+} f(x)=0$ Is the solution correct?In the book it is  given to use the Baire Category Theorem.","['continuity', 'real-analysis', 'limits']"
1908131,Group associated with this curve,"Consider an Elliptic curve with a singularity (double point) over $\mathbb{F}_p$ $$y^2=x^2(x-a) $$ where $a$ is a quadratic residue in ${\mathbb{F}_p}^*$. We can easily count the number of (non-singular)points on this curve over ${\mathbb{F}_p}$ (They are $p-1$). Also it is well known that $$E_{ns}({\mathbb{F}_p}) \cong {\mathbb{F}_p}^* $$ Now consider an exact analogous condition, Now my curve is  $$ y^3=x^3(x-a) $$ over ${\mathbb{F}_p}$ such that  $3|p-1$ . The nonsingular points on this curve is again $p-1$. My question is does the group associated with this curve (Picard group or Jacobian variety) known ? My guess would be $\oplus_{i=1}^{k}{\mathbb{F}_p}^*$","['algebraic-curves', 'finite-fields', 'elliptic-curves', 'algebraic-geometry']"
1908145,How do we prove that the derivative of $x^r$ is $r x^{r-1}$ for all $r$?,"Let the function $f$ be defined as:
$f(x)=x^r, r\in \mathbb R$. We have all heard that $f'(x)=r x^{r-1}$. And apparently, there exists a proof for that . But in that proof, there is something that doesn't look convincing to me. As you see, we first take this into account:
$$f(x)=x^r=e^{r\ln{x}}$$
and then, use the existing rules for the derivative of exponential functions. But let's take a little step back. As far as I know, the exponential function is defined as the inverse of the logarithm function. And the logarithm is defined as:
$$\ln(x)=\int_1^x \frac{1}{t} dt,\quad x>0$$ At the first steps, it is proved that this function is well-defined, continuous and anywhere differentiable in its domain. Then we prove that it is injective, and denote its inverse function by $\exp(x)$ and we don't mention anything about the term exponential . Then it is shown that the derivative of this particular function is equal to itself, by using those abstract definitions and some smart-looking workarounds. But something caught my attention that, in the process of those proofs, it is taken as an assumption that the derivative of the power function is equal to $rx^{r-1}$, and this seems like a loop to me. Also if the other definition of the $\exp$ is used, which is based on the Taylor series , we would still be stuck to the derivative of power function. So, is there a way to prove the derivative of the power function other than using the exponential function? Or in other words, is there any way to differentiate the so-called $\exp$ without using the power function?","['derivatives', 'calculus']"
1908158,How do I determine the time difference in seconds between two clocks that show only minutes?,"So, after a power outage, I found myself having to reset a bunch of clocks in my apartment. The most handy clock I had was my cell phone, which only shows time down to the minute, on the home screen. That got me thinking, how do I tell how far off my oven, microwave, etc. are off from my cell phone without using another clock? To put it more concretely, I know HH:MM from Clock A. I know HH:MM from Clock B. Is there a way to determine how far off Clock A is from Clock B down to the second, without using a third clock? I've been thinking in terms of random sampling. I know that if Clock A is 5 sec. ahead of Clock B, then if I take a random sample, I have a 55/60 chance of finding that Clock A and Clock B read the same time, and a 5/60 chance of finding that Clock A reads a minute ahead of Clock B. But how do I exploit that to find an unknown offset? Is what I'm trying to do even possible? Am I going about it in completely the wrong way?","['algebra-precalculus', 'statistics']"
1908167,Poincaré–Bendixson Theorem on the Mobius Strip,"In the book of  J. Jr. Palis and W. de Melo Geometric theory of dynamical systems: an introduction on the page $18$, there is a type of Poincaré-Bendixson theorem on $S^2$ or in plane or on cylinder  as follows: Theorem: Let $\mathcal M$ denotes the phase space, which may be the plane, cylinder, or two-sphere. $X \in \chi ^r(\mathcal M) $ ($r\ge 1$) be a vector field with a finite number of singularities. Take $p \in \mathcal M$ and let $\omega(p)$ be the $\omega$-limit set of $p$. Then one of the following possibilities holds $(1)$ $\omega(p)$ is a singularity; $(2)$ $\omega(p)$ is a closed orbit; $(3)$ $\omega(p)$ consists of singularities $p_1, ... , p_n$ and regular orbits such that if $\gamma \in \omega(p)$ then $\alpha(\gamma) = p_i$, and $\omega(\gamma) = p_j$ for some $1\le i,j \le n$. The proof uses from the Jordan Curve Theorem : every continuous closed curve without selfintersections separates $\mathcal M$ into two regions (two connected surface). In the exercises of the end of the chapter, exercise $5$ the authors want to prove the following: [Let $M^2$ be a 2-dimensional smooth manifold and] Let $F \subset M^2$ be a region homeomorphic to a Mobius band and let $X \in \chi^r(M^2)$ ($r\ge 1$) be a vector field such that $X_t(F) \subset F$ for all $t \ge 0$. If $X$ has a finite number of singularities in $F$ then the $\omega$-limit of the orbit of a point $p \in F$ either is a closed orbit or consists of singularities and regular orbits whose $\omega$ and $\alpha$-limits are singularities. But on the Moebius strip, we can draw closed curves which have no “inside” or “outside”. Consider the curve which divides the strip in half, running halfway between the free edges. If we take a pair of scissors and cut along this curve, we will be left with a single connected surface. So, how we can prove the statement for the Moebius band?","['dynamical-systems', 'smooth-manifolds', 'differential-geometry']"
1908181,"How to integrate $\int_1^\infty e^{-\alpha x}J_0(\beta\sqrt{x^2-1})\mathrm{d}x \,$?","This integral is from (6.616.2) in Gradshteyn and Ryzhik.
$$
\int_1^\infty  e^{-\alpha x}J_0(\beta\sqrt{x^2-1})\mathrm{d}x \,=\frac{1}{\sqrt{\alpha^2+\beta^2}}e^{-\sqrt{\alpha^2+\beta^2}}
$$ I want to know how to do this integral and the restriction of $\alpha$ and $\beta$ . The integral table doesn't mention it. I doubt the results must have some restriction on $\alpha$ and $\beta$, because: it seems that when $\mathrm{Re}\,\alpha <0$, the integral diverges. also when $\alpha$ is purely imaginary, and for $\beta$ real, the result should be complex conjugate when $\alpha$ takes conjugate purely imaginary pairs, $\pm i$ for example, however the results given depends only on $\alpha^2$. I also found by Mathematica numerical integration that when $\alpha$ is purely imaginary, the integration also seems troublesome. Edit: The answer by @Fabian give a general condition that when $\mathrm{Re}\alpha>\mathrm{Im}\beta$, the integral converges. However, what about $\mathrm{Re}\alpha=\mathrm{Im}\beta$. For the simplest case when $\alpha$ is purely imaginary and $\beta$ is real, Mathematica can give the sensible result when $|\alpha|>|\beta|$, while seems diverges when $|\alpha|<|\beta|$: \[Alpha]=-3I ;\[Beta]=2;
NIntegrate[Exp[-\[Alpha] x]BesselJ[0,\[Beta] Sqrt[x^2-1]],{x,1,Infinity}]
f[a_,b_]:=Exp[-Sqrt[a^2+b^2]]/Sqrt[a^2+b^2]
f[\[Alpha],\[Beta]]//N
-0.351845-0.276053 I
-0.351845+0.276053 I","['bessel-functions', 'integration', 'definite-integrals']"
1908184,"Explicit solutions for differential system $x'=y^2,y'=x^2$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Could the system
$$ \begin{cases}
x'=y^2 \\
y'=x^2 
\end{cases}
$$
be explicitly solved?
I should determine if for every initial condition the system has a solution defined in all $\mathbb{R}$.","['real-analysis', 'ordinary-differential-equations']"
1908210,Largest divisor of $P(n) = (n + 1)(n + 3)(n + 5)(n + 7)(n + 9)$,Let $P(n) = (n + 1)(n + 3)(n + 5)(n + 7)(n + 9)$. What is the largest integer that is a divisor of $P(n)$ for all positive even integers $n$? I can see that obviously it is divisible by $3$ and $5$ and hence answer is $15$ but I can't prove it.,"['number-theory', 'divisibility']"
1908252,How many boys and girls should be included in the sample.,"This is a statistics related question which follows an introduction to stratified sampling. A survey to estimate the number of vegetarians in a mixed college with 660 boys and 540 girls is carried out. A sample of 40 students is required. How many boys and girls should be included? If you assign weights to boys and girl then you should chose 22 boys and 18 girls, which is the answer in the book. But is this the correct approach to use here? Why should we assign weights based on gender here? I feel a random selection of 40 would be representative in this case. Something else to consider: Why stratify based on gender? Why not stratify based on other factors that might affect whether someone is vegetarian or not? For example whether the student was raised on a farm or in a town. Or whether the student is an athlete or not? I just feel that unless your experiment requires data on boys and girls then this stratification is unnecessary here.",['statistics']
1908334,How to evaluate the limit $\lim_{x \to \infty}\left(\left(x+\frac{1}{x}\right)\arctan(x)-\frac{\pi}{2}x\right)$?,"$$\lim\limits_{x \to \infty}\left(\left(x+\frac{1}{x}\right)\arctan(x)-\frac{\pi}{2}x\right)$$ From graphs, I know that the limit is exactly $-1$ (this function limit is ""$b$"" from $f(x)=ax+b$, where $f(x)$ is asymptote of another function). I managed to get to a point, by using l'Hospital rule, where my limit equals 0. I've checked calculations few times, and can't find, where the mistake is. Could you please show me steps, how to evaluate this limit?","['calculus', 'limits']"
1908336,Find $ ? = \sqrt[3] {1 + \sqrt[3] {1 + 2 \sqrt[3] {1 + 3 \sqrt[3] \cdots}}} $,"I wonder about a closed form for $ ? = \sqrt[3] {1 + \sqrt[3] {1 + 2 \sqrt[3] {1 + 3 \sqrt[3] {1 + 4 \sqrt[3] {1 + 5 \sqrt[3] \cdots}}}}} $ To be clear $$? = \sqrt[3]{ 1 + \color{Red}{1}\sqrt[3]{ 1 + \color{Red}{2} \sqrt[3]{ 1 + \color{Red}{3} \sqrt[3]{\cdots}}}} $$ Where the red coefficients are just the natural numbers. I tried solving the related equation $ f(x) ^3 = 1 + (x+y) f(x+1) $ for various fixed integer values $y,$ but I failed. It appears $$ ? = \sqrt[3] {1 + \sqrt[3] 5} $$ But I am not able to prove it. See also https://en.m.wikipedia.org/wiki/Nested_radical#Ramanujan.27s_infinite_radicals","['recursion', 'nested-radicals', 'calculus', 'limits']"
1908341,"Points of discontinuity of a bijective function $f:\mathbb{R} \to [0,\infty)$","We know that the points of discontinuity of a monotone function on an interval $[a,b]$ are countable. Using this can we prov that: Any bijection $f: \mathbb{R} \to [0,\infty)$ has infinitely many points of discontinuity. If yes, how or otherwise how to prove the above result?","['real-analysis', 'analysis']"
1908355,Simplify the Order statistics,"Let $Z_1,...,Z_S$ be the i.i.d. random variables with the following common cdf:
$${F_Z}(x) = \sum\limits_{n = 0}^{N} {{b_n}{{\left( {1 - {e^{ - {c_n}x}}} \right)}^Q}}  = \sum\limits_{n = 0}^N {\sum\limits_{q = 0}^Q {\left( \begin{array}{c}
Q\\
q
\end{array} \right)} {{( - 1)}^q}{b_n}{e^{ - q{c_n}x}}},$$
where $b_n>0$ and $c_n>0$ for $n =0,...,N$. The cdf of the $k$th order statistic ($k$th largest among $Z_1,...,Z_S$) is given by 
\begin{align}
{F_{{Z_{k:S}}}}(x) &= {\zeta _{k|S}}\sum\limits_{i = 0}^{S - k} {{S-k \choose i}\frac{{{{( - 1)}^i}}}{{k + i}}} {[{F_Z}(u)]^{k + i}}\\
 &={\zeta _{k|S}} \sum\limits_{i = 0}^{S - k} {S-k \choose i}\frac{{{{( - 1)}^i}}}{{k + i}}\sum\limits_{\sum\limits_{v = 0}^{O-1} {{r_v}}  = k + i} {k + i\choose {r_0}, ...,{r_{O-1}}
} \\ &~~~\times \left(\prod\limits_{q = 0}^Q {{{\left( { {Q \choose q}{{\left( { - 1} \right)}^q}} \right)}^{\sum\limits_{n = 0}^N {{r_{n(Q + 1) + q}}} }}} \right)\left(\prod\limits_{n = 0}^N {b_n^{\sum\limits_{i = 0}^Q {{r_{n\left( {Q + 1} \right) + i}}} }} \right)\\&~~~\times {e^{ - x\sum\limits_{n = 0}^N {{c_n}\sum\limits_{i = 0}^Q {i{r_{n(Q + 1) + i}}} } }},
\end{align}
where $\zeta_{k|S}= \frac{S!}{{(k - 1)!(S -k)!}}$ and $O = (N+1)(Q+1)$. I tried to express this cdf as the following form: 
$$
{F_{{Z_{k:S}}}}(x) = \sum\limits_{n = 1}^{T} {{A}(n){e^{ - x{B}(n)}}},
$$
where $T$, $A(n)$, and $B(n)$ are needed to be specified. 
How do I specify these values and what is the general expression? My approach: Without loss of generality, suppose that $B(1)< B(2)<....< B(T)$. Since $F_{Z}(\infty)=1$, $B(1)=0$ and $A(1)=1$. In order to specify $A(2)$, I use 
$$
\lim_{x\to\infty}{e^{xB(2)}}\left( {F(x) - A(1)} \right) = A(2) + \lim_{x\to\infty}\sum\limits_{n = 2}^T {A(n){e^{ - x\left( {B(n) - B(2)} \right)}}} =A(2),
$$
where the last equality comes from the fact $B(2)<B(n)$ for $n>2$. Recursively, I can specify $A(3)$, $A(4)$,..., and $A(T)$. This approach requires the sorting operation on $\{B(n)\}$ so that it is hard to obtain the general expression of $A(n)$.","['order-statistics', 'functions']"
1908370,Principle branch of $\sqrt{1-z}$,"I am asked to find the principle branch of the complex function $$f(z) = \sqrt{1-z}$$ I know that the principle branch of $z^{1/2}$ is given by $\exp(\frac{1}{2} \log(z))$ where $$\log(z) = \log(|z|)+i\arg(z)$$ is the principle branch of the logarithm. So then to find the principle branch of $\sqrt{1-z}$, is it just $$\exp\bigg(\frac{1}{2} \log(1-z)\bigg)$$
with $\log(1-z)$ being the principle branch of the logarithm i.e. $$\log(1-z) = \log(|1-z|) + i\arg(1-z).$$ I think I am confusing myself. Is the principle branch correct?",['complex-analysis']
1908371,What is the remainder when $16^{15}-8^{15}-4^{15}-2^{15}-1$ is divided by $96$?,"I found this question from an old math questionnaire. What is the remainder when $$16^{15}-8^{15}-4^{15}-2^{15}-1$$ is divided by 96? I already know the answer. It is 31, says the answer at the back of the questionnaire. I just do not know why 31 is the answer. I do not know how the process of getting the remainder is done. I tried a lot of crazy things and got answers like -135 and -63/32 which are obviously wrong, so I would not tell anymore what I did. Can anyone help me?","['algebra-precalculus', 'modular-arithmetic', 'elementary-number-theory']"
1908385,Reference request: Strong Law of Large Numbers for V-statistics,"I'm requesting a reference for a Strong Law of Large Numbers theorem for V-statistics (similar to Hoeffding's 1961 paper for U-statistics). That is, I am searching for an almost sure convergence theorem, saying $$
V_n = \frac{1}{n^k}\sum_{i_1=1}^n \cdots \sum_{i_k=1}^n h\left(X_{i_1},...,X_{i_k}\right) \stackrel{\mbox{a.s.}}{\longrightarrow}_n \quad?
$$ where $(X_n)$ is an i.i.d. sequence of random elements with values in some space $\mathcal{X}$ and $h:\mathcal{X}^k \to \mathbb{R}$ is a symmetric kernel with some moment requirements. If Hoeffdings SSLN for U-statistics can be used to derive this result, an explanation of how this is done, would also more than suffice.","['law-of-large-numbers', 'reference-request', 'probability-theory', 'asymptotics', 'statistics']"
1908389,show that $\frac{c^2}{a}+\frac{a^2}{c}+16\sqrt{ac}\ge 9(a+c)$,"Let $a,c>0$ show that
$$\dfrac{c^2}{a}+\dfrac{a^2}{c}+16\sqrt{ac}\ge 9(a+c)$$ It seem AM-GM inequality.How?","['radicals', 'inequality', 'a.m.-g.m.-inequality', 'algebra-precalculus', 'fractions']"
1908405,"Showing that $\lim_{n\to\infty}\int_{[0,1]}|f-f_n|=0$ where $f_n(x)=f(x-1/n)$","I am trying to tackle the following question, but I actually don't know how to start... Let $f:\Bbb{R}\to\Bbb{R}$ be measurable function with period $1$ and $\displaystyle \int_{0}^{1}|f|<\infty$ . Define $\displaystyle f_n(x)=f\left(x-\frac{1}{n}\right)$ . Show that $$\lim_{n\to\infty}\int\limits_{[0,1]}|f-f_n|=0$$ I have tried to use periodicity of $f$ and manipulate $f_n(x)$ , but to no avail. Please help, thanks!","['real-analysis', 'convergence-divergence']"
1908411,Is there a name for an equation in which different variables are multiplied together? e.g. $xz+2x-5yx=21$,"This is quite a simple question I suppose, but i'm trying to categorise equation types in my mind. From what I know, linear equations can have any number of variables in them, but in each term there is only one variable
e.g. $w+2x-3y+4z=5$ And quadratics have the highest power of any variable in a term having a power of two:
e.g. $x^2+2y-3=0$ But when it comes to equations such as $xy+x-2=0$ and $3xy^2-9x+4=0$, are these referred to as quadratics and cubics respectively because of the highest number of variables multiplied together in any one term? Or perhaps I am thinking about this in the wrong way and there is simply the set of all equations (within which all of the above fall), and there exist subsets of equatios with specific names/categories (e.g. one subset could be 'equations in only one variable' which is formed of the sets 'linear equations', 'quadratics', 'cubics' etc; and another subset could be 'equations in more than one variable but in which each term contains only one variable' in which the subsets are just named y the highest power of any term in the equation i.e. 'linear', 'quadratic' etc...) And so perhaps these other equations just belong to the set of all equations, but not to any other sub-set which has a particular name?","['elementary-set-theory', 'exponentiation']"
1908442,Compressed Sensing,"I really would like to know more about compressed sensing. I am currently a PhD student working on array signal processing. The field of compressed sensing is super attractive to my opinion, but i really think it might waste my time in case i do not know where to start. I am saying that because there is a very large literature on the topic. If anyone out there is an expert or, at the least, knows about the domain, please help me with some advices. Thank you so very much.","['optimization', 'linear-algebra', 'discrete-mathematics']"
1908444,Prove that it is always possible to subdivide a given trapezium into two similar trapeziums.,"Given that ABCD is a trapezium with AB // DC. Let $a = AB \lt CD = b$, $DA = c$ and $BC$ be also known. Prove that there is a line PQ (with P on DA and Q on BC) drawn parallel to AB such that (Trap ABQP) ~ (Trap PQCD). If the title is true, then (1) How far is P from A (in terms of a, b, c)? and (2) How can PQ be constructed in the Euclidean way? Obviously, the trapeziums are equi-angular.","['euclidean-geometry', 'geometry']"
1908459,Surjections of functions and proof by counterexample,"Here is the question: $f : X \mapsto Y$ and $g : Y \mapsto Z$ are functions. (a) Prove that if $g \circ f: X \mapsto Z$ is a surjection, then $g$ is a surjection. (b) Give a counterexample to show that the converse is not true. My proof was as follows: (a) $g \circ f$ is a surjection $\implies$ $\forall z \in Z$, $\exists x \in X$ : $z = g\circ f (x)$ But $\exists y \in Y$ : $y = f(x)$ Thus $\forall z \in Z$, $\exists y \in Y$ : $z = g(y)$ $\therefore g$ is surjective. (b) Suppose that $f : ${1} $\mapsto$ {2, 3} and $g : ${2, 3} $\mapsto$ {4, 5} Now let $g(2) = 4$ and $g(3) = 5$. This makes $g$ a bijection and therefore a surjection. Also let $f(1) = 3$. Then $g \circ f = 5$ and 4 is left out of the mapping, so $g \circ f$ is not a surjection. Is this correct? I'm self taught so I don't have a way of checking.","['functions', 'proof-verification']"
1908502,Second order sufficiency test for multivariable function,"Question : Suppose $f$ is a $\mathcal{C}^2$ function and $x^*$ is a point of its domain at which $\nabla f\left( {{x^*}} \right)d \geq 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ for every non-zero feasible direction. Is $x^*$ necessarily a local minimum? Basically the question is asking whether the above two conditions are sufficient. My work so far: If $\exists $ a feasible direction $d\in \mathbb{R}^n$ at $x^*$ such that $ \nabla f(x^*)d<0 $ then $\exists \epsilon >0 $ such that for $|\alpha| < \epsilon$ we have $ x(\alpha) = x^* + \alpha d \in D $. Then, $$ f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( \alpha  \right)$$ $$ f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \alpha \nabla f\left( {{x^*}} \right)d + o\left( \alpha  \right) $$
where for small $\alpha$ $$ \alpha \nabla f\left( {{x^*}} \right)d + o\left( \alpha  \right) < 0$$
since $\alpha \geq 0 $ and $\nabla f\left( {{x^*}} \right)d < 0$. This implies $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) < 0$ contradicting that $x^*$ is a minimum. 
Now I claim if $\nabla f\left( {{x^*}} \right)d = 0$ then, 
$${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$$
To see why note, 
$$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + \frac{1}{2}{\left( {x\left( \alpha  \right) - {x^*}} \right)^T}{\nabla ^2}f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( {{\alpha ^2}} \right) $$
which simplifies to
$$f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) = \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right)$$ Note that $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) < 0$ for small $\alpha$ if ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d < 0 $ which would contradict that $x^*$ is a minimum. Hence we must have $${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$$ Claim :If $\nabla f\left( {{x^*}} \right)d > 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ then $x^*$ is a necessarily local minimum. To see why, note that:
$$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \nabla f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + \frac{1}{2}{\left( {x\left( \alpha  \right) - {x^*}} \right)^T}{\nabla ^2}f\left( {{x^*}} \right)\left( {x\left( \alpha  \right) - {x^*}} \right) + o\left( {{\alpha ^2}} \right)$$
simplifies to, 
$$f\left( {x\left( \alpha  \right)} \right) = f\left( {{x^*}} \right) + \alpha \nabla f\left( {{x^*}} \right)d + \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right)$$
$$ \Rightarrow f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) = \alpha \nabla f\left( {{x^*}} \right)d + \frac{1}{2}{\alpha ^2}{d^T}{\nabla ^2}f\left( {{x^*}} \right)d + o\left( {{\alpha ^2}} \right) $$
We have $\alpha>0$ by definition of feasible direction. Moreovoer  $\nabla f\left( {{x^*}} \right)d > 0$ and  ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ and since these two terms dominate, we always have $f\left( {x\left( \alpha  \right)} \right) - f\left( {{x^*}} \right) > 0$. Since $d$ is an arbitrary feasible direction, we see that moving away from $x^*$ only increases the value of $f$ which implies that $x^*$ is indeed the minimum. We can relax the condition $\nabla f\left( {{x^*}} \right)d > 0$ to $\nabla f\left( {{x^*}} \right)d \geq 0$ and the above will still hold. To see why ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ guarantees sufficiency note that if we only require ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d \geq 0$ the $o\left(\alpha^2\right) $ term could make the LHS $<0$ when both $\nabla f\left( {{x^*}} \right)d = 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d = 0$. However for the case when only $\nabla f\left( {{x^*}} \right)d \geq 0$ and ${d^T}{\nabla ^2}f\left( {{x^*}} \right)d > 0$ the whole term still remains positive. Hence the conditions are sufficient. Apparently this proof is wrong and the two conditions in the question are not sufficient conditions. It has something to do with non-convex domains where a ""nonlinear motion"" may take you to a minimum. But I am having trouble wrapping my head around this. Does anyone know of a counter-example or a proof that the above are not sufficient conditions?","['multivariable-calculus', 'optimization']"
