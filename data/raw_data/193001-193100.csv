question_id,title,body,tags
3688687,Probability that the mean of a sequence of random variables never exceeds a critical value,"Let $p,\mu_c \in [0,1]$ . Let $(X_i)_{i \in \mathbb{N}}$ be a sequence of i.i.d random variables, each following a Bernoulli distribution $\mathcal{B}(p)$ . For $n \in \mathbb{N}$ , let $ \mu_n = \frac{1}{n+1} \sum_{i=0}^n X_i $ and $A_n$ the event: "" $\mu_n \leq \mu_c$ "". I am looking for a way to compute $$ \mathbb{P} \left(\bigcap_{n \in \mathbb{N}} A_n \right), $$ that is, the probability that the mean of the sequence never exceeds the critical value $\mu_c$ . You can assume $\mu_c > p$ , since this probability is $0$ when $\mu_c \leq p$ . I first wanted to compute it using a martingale, but the optional stopping theorem does not seem to help. I don't know if computing it ""by hand"" (combinatorially) is possible, but it would be quite unsatisfying anyway. Eventually, it may be somehow linked to this post , but I don't understand Brownian motion well enough to use it correctly. Thank you for any advice! Edit: When $p = \frac{1}{2}$ and $\mu_c = \frac{3}{4}$ , simulations give $$\mathbb{P} \left(\bigcap_{n \leq 1000} A_n \right) \approx 0.456.$$ I think that this is a very good approximation of $\mathbb{P} \left(\bigcap_{n \in \mathbb{N}} A_n \right)$ (going up to 2000 does not affect the result). This fact may be used to check any suggested answer. I find it surprisingly high, since $\mathbb{P}(A_0)$ alone is equal to $\frac{1}{2}$ .","['means', 'probability']"
3688691,Simplifying $\prod\limits_{k=0}^{n-1}\left(\sin\frac\pi{2^{k+3}}+\frac1{\sqrt{2}}\right)$,"I have recently stumbled upon the sequence $\left( u_n \right)_{n \in \mathbb{N}}$ defined as follows : $$\forall n \in \mathbb{N}, ~ u_n = \prod\limits_{k=0}^{n-1} \left[ \: \sin \left( \dfrac{\pi}{2^{k+3}} \right) + \dfrac{1}{\sqrt{2}} \: \right] $$ and I am trying to find a more concise expression of its general term (ie without the $\boldsymbol{\prod}$ sign). Firstly, one can easily see that : $$\begin{align*} 
\forall n \in \mathbb{N}, ~ u_n &= \: \prod\limits_{k=0}^{n-1} \: \left[ \: \sin \bigg( \dfrac{\pi}{4} \bigg) + \sin \left( \dfrac{\pi}{2^{k+3}} \right) \: \right] \\
&= \: \prod\limits_{k=0}^{n-1} \: \left[ \: 2 \times \sin \left( \dfrac{\pi}{8} + \dfrac{\pi}{2^{k+4}} \right) \times \cos \left( \dfrac{\pi}{8} - \dfrac{\pi}{2^{k+4}} \right) \: \right] \\
&= \: 2^n \times \left[ \: \: \prod\limits_{k=0}^{n-1} \: \sin \left( \dfrac{\pi}{8} + \dfrac{\pi}{2^{k+4}} \right) \: \right] \times \left[ \: \: \prod\limits_{k=0}^{n-1} \: \cos \left( \dfrac{\pi}{8} - \dfrac{\pi}{2^{k+4}} \right) \: \right] \quad ( * )
\end{align*}$$ But after that, I had a lot of trouble trying to simplify $\prod\limits_{k=0}^{n-1} \: \sin \left( \dfrac{\pi}{8} + \dfrac{\pi}{2^{k+4}} \right)$ and $\prod\limits_{k=0}^{n-1} \: \cos \left( \dfrac{\pi}{8} - \dfrac{\pi}{2^{k+4}} \right)$ , and eventually got stuck. Am I heading in the right direction ? How can I simplify $\boldsymbol{u_n}$ even more ? I also found that : $$\newcommand{\isEquivTo}[1]{\underset{#1}{\sim}}
u_n \isEquivTo{+ \infty} \dfrac{C}{\left( \sqrt{2} \right)^n}$$ where $C \approx 2.564448944368$ , if it can help. I am also trying to figure out the literal expression of $\boldsymbol{C}$ (ideally, in terms of fundamental constants only). #################################################################### UPDATE/EDIT : While I did not manage to find a simplified expression of $u_n$ , I think I did make some progress. $1)$ Firstly, since the sequence $( ( \sqrt{2} )^n \, u_n )_{n \in \mathbb{N}}$ converges (towards $C$ ), I find it convenient to let : $$\forall n \in \mathbb{N}, ~ v_n = ( \sqrt{2} )^n \, u_n = \prod\limits_{k=0}^{n-1} \left[ \: 1 + \sqrt{2} \, \sin \left( \dfrac{\pi}{2^{k+3}} \right) \right]$$ Finding the expression of $u_n$ is now equivalent to finding the expression of $v_n$ . $2)$ In order to make the following points easier to read, let : $$\forall n \in \mathbb{N}, \,
\begin{cases}
\, c_n = \cos \left( \dfrac{\pi}{2^{n+4}} \right) \\[10pt]
\, s_n = \sin \left( \dfrac{\pi}{2^{n+4}} \right) \\[10pt]
\, f(n) = \prod\limits_{k=0}^{n-1} \, \sin \left( \dfrac{\pi}{8} + \dfrac{\pi}{2^{k+4}} \right) \\[10pt]
\, g(n) = \prod\limits_{k=0}^{n-1} \, \cos \left( \dfrac{\pi}{8} - \dfrac{\pi}{2^{k+4}} \right)
\end{cases}
$$ Then, by $( * )$ , we have : $\forall n \in \mathbb{N}, \, v_n = ( 2 \sqrt{2} )^n \times f(n) \times g(n)$ $3)$ My idea was then to rewrite $f(n)$ , $g(n)$ and $f(n) \times g(n)$ as ""polynomial expressions"" of $\cos \left( \dfrac{\pi}{8} \right)$ and $\sin \left( \dfrac{\pi}{8} \right)$ . In order to achieve this, notice the following : $$\forall n \in \mathbb{N}, \,
\begin{cases}
\, f(n) = \prod\limits_{k=0}^{n-1} \, \left[ s_k \cos \left( \dfrac{\pi}{8} \right) + c_k \sin \left( \dfrac{\pi}{8} \right) \right] \\[10pt]
\, g(n) = \prod\limits_{k=0}^{n-1} \, \left[ c_k \cos \left( \dfrac{\pi}{8} \right) + s_k \sin \left( \dfrac{\pi}{8} \right) \right]
\end{cases}
$$ We can then use a generalization of Newton's binomial expansion to obtain : $$\forall n \in \mathbb{N}, \,
\begin{cases}
\, f(n) = \sum\limits_{k=0}^{n} \, F_n(k) \times \cos^k \left( \dfrac{\pi}{8} \right) \times \sin^{n-k} \left( \dfrac{\pi}{8} \right) \\[10pt]
\, g(n) = \sum\limits_{k=0}^{n} \, G_n(k) \times \cos^k \left( \dfrac{\pi}{8} \right) \times \sin^{n-k} \left( \dfrac{\pi}{8} \right)
\end{cases}
$$ where : $$\forall n \in \mathbb{N}, \, \forall k \in \left[ 0,  n \right], \, F_n (k) = 
\begin{cases}
\, \sum\limits_{0 \, \leq \, i_1 \, < \, \dots \, < \, i_k \, \leq \, n-1} \, \left( \, \prod\limits_{l=1}^{k} s_{i_l} \times \prod\limits_{ \substack{m=0 \\ m \notin \{ i_1, \, \dots \, , \, i_k \} } }^{n-1} c_m \, \right) \quad \mathrm{if} \, \, k \neq 0 \\[10pt]
\, \prod\limits_{m=0}^{n-1} c_m \quad \mathrm{if} \, \, k = 0
\end{cases}
$$ and : $$\forall n \in \mathbb{N}, \, \forall k \in \left[ 0,  n \right], \, G_n (k) = 
\begin{cases}
\, \sum\limits_{0 \, \leq \, i_1 \, < \, \dots \, < \, i_k \, \leq \, n-1} \, \left( \, \prod\limits_{l=1}^{k} c_{i_l} \times \prod\limits_{ \substack{m=0 \\ m \notin \{ i_1, \, \dots \, , \, i_k \} } }^{n-1} s_m \, \right) \quad \mathrm{if} \, \, k \neq 0 \\[10pt]
\, \prod\limits_{m=0}^{n-1} s_m \quad \mathrm{if} \, \, k = 0
\end{cases}
$$ One can note that : $\forall n \in \mathbb{N}, \, \forall k \in \left[ 0, n \right], \, G_n(n-k) = F_n(k)$ $4)$ We can then prove that : $$\forall n \in \mathbb{N}, \, f(n) \times g(n) = \sum\limits_{i=0}^{2n} \, \beta_n(i) \times \cos^i \left( \dfrac{\pi}{8} \right) \times \sin^{2n-i} \left( \dfrac{\pi}{8} \right)$$ where : $$\begin{align*}
\forall n \in \mathbb{N}, \, \forall i \in \left[ 0, 2n \right], \, \beta_n(i) &= \sum\limits_{k \, = \, \max(i-n, \, 0)}^{\min(i, \, n)} \, F_n(k) \times G_n(i-k) \\[10pt]
&= \sum\limits_{k \, = \, \max(i-n, \, 0)}^{\min(i, \, n)} \, F_n(k) \times G_n(n-(n+k-i)) \\[10pt]
&= \sum\limits_{k \, = \, \max(i-n, \, 0)}^{\min(i, \, n)} \, F_n(k) \times F_n(n+k-i)
\end{align*}$$ We can also note this very nice property : $\forall n \in \mathbb{N}, \, \forall i \in \left[ 0, 2n \right], \, \beta_n(i) = \beta_n(2n-i) \quad \quad \quad ( ** )$ $5)$ Therefore, we have : $$\begin{align*}
\forall n \in \mathbb{N}, \, v_n &= (2 \sqrt{2})^n \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times \cos^i \left( \dfrac{\pi}{8} \right) \times \sin^{2n-i} \left( \dfrac{\pi}{8} \right) \\[10pt]
&= (2 \sqrt{2})^n \times \sin^{2n} \left( \dfrac{\pi}{8} \right) \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times \cot^i \left( \dfrac{\pi}{8} \right)
\end{align*}
$$ But, since we have $\cos \left( \dfrac{\pi}{8} \right) = \dfrac{1}{2} \sqrt{2 + \sqrt{2}} \,$ and $\, \sin \left( \dfrac{\pi}{8} \right) = \dfrac{1}{2} \sqrt{2 - \sqrt{2}} \,$ , we then obtain : $$\forall n  \in \mathbb{N}, \, v_n = ( \sqrt{2} - 1 )^n \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times ( 1 + \sqrt{2} )^i \quad \quad \quad \quad ( *** )$$ But, by $( ** )$ , we also have ( $i \leftrightarrow 2n-i$ ) : $$\begin{align*}
\forall n \in \mathbb{N}, \, v_n &= (2 \sqrt{2})^n \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times \cos^{2n-i} \left( \dfrac{\pi}{8} \right) \times \sin^{i} \left( \dfrac{\pi}{8} \right) \\[10pt]
&= (2 \sqrt{2})^n \times \cos^{2n} \left( \dfrac{\pi}{8} \right) \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times \tan^i \left( \dfrac{\pi}{8} \right) \\[10pt]
&= ( 1 + \sqrt{2} )^n \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times ( \sqrt{2} - 1 )^i \quad \quad \quad \quad ( *** \, * )
\end{align*}
$$ $6)$ Thus, since $u_n = \dfrac{v_n}{\left( \sqrt{2} \right)^n}$ , we finally obtain, by $(***)$ and $(*** \, *)$ : $$\boxed{
\begin{align*}
\forall n \in \mathbb{N}, \, u_n &= \left( 1 - \dfrac{1}{\sqrt{2}} \right) ^n \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times ( 1 + \sqrt{2} )^i \\[10pt]
&= \left( 1 + \dfrac{1}{\sqrt{2}} \right) ^n \times \sum\limits_{i=0}^{2n} \, \beta_n(i) \times ( \sqrt{2} - 1 )^i
\end{align*}
}
$$ I find these expressions of $u_n$ really intriguing, since the sums remind me a lot of the binomial expansion of $(1+x)^{2n}$ . Indeed, $(1+x)^{2n}= \sum\limits_{i=0}^{2n} \, \binom{2n}{i} \times x^i$ . Additionally, the fact that $\binom{2n}{i}$ equals $\binom{2n}{2n-i}$ is analogous to $(**)$ ... $7)$ My hope was that I could finally finish this off by simplifying $\beta_n(i)$ . Yet, I still haven't managed to do so. However, here are some particular values of $\beta_n(i)$ : $$\begin{cases}
\, \beta_n(0) = \beta_n(2n) = \dfrac{1}{2^n} \times \prod\limits_{k=0}^{n-1} \, \sin \left( \dfrac{\pi}{2^{k+3}} \right) \quad \quad (\forall n \geq 0) \\[10pt]
\, \beta_n(1) = \beta_n(2n-1) = 2 \times \beta_n(0) \times \sum\limits_{k=0}^{n-1} \, \dfrac{1}{\sin \left( \dfrac{\pi}{2^{k+3}} \right)} \quad \quad (\forall n \geq 1) \\[10pt]
\, \beta_n(n) = \sum\limits_{k=0}^{n} \, F_n(k)^2 \quad \quad (\forall n \geq 0) \\[10pt]
\end{cases} 
$$ What more can be done with $\boldsymbol{\beta_n}$ ? Side note : everything claimed in this post was proved by hand, but was also double-checked numerically using Python.","['limits', 'trigonometry', 'sequences-and-series']"
3688710,Addressing the probability of a category as a whole,"I think this is rather an English language question, but I've asked this in ell.sx , and a person there insists that this is a concern of mathematics. Let's say I have $k_g$ green, and $k_r$ red balls.  I want to select one, randomly, but with a bias towards the red balls, as follows; Each of the $k_g$ green balls have $p_g$ probability of getting chosen. Each of the $k_r$ red balls have $p_r$ probability of getting chosen. Obviously (I hope), $k_g p_g + k_r p_r = 1$ . Now, without giving any of the lengthy explanation above, and without saying $P_g = k_g p_g$ , I want to summarize all this information with the following paragraph: The bag I have consists of $k_g$ green, and $k_r$ red balls.  The setup is so arranged that the probability of choosing a green ball is $P_g$ .  Same color balls have the same probability of getting chosen. Is the part written in bold in the above paragraph any ambiguous?  If it is, how else should I say it? To clarify:  What I'm trying to say is (1) ""the chances of a selected ball being green is $P_g$ "", and not (2) ""each green ball has a $P_g$ chance of getting chosen"".  I find the sentence (1) much more clear, but it is rather a weird way to put it.","['discrete-mathematics', 'combinatorics', 'probability']"
3688717,The area of a triangle determined by two diagonals at a vertex of a regular heptagon,"In a circle of diameter 7, a regular heptagon is drawn inside of it. Then, we shade a triangular region as shown: What’s the exact value of the shaded region, without using trigonometric constants? My attempt I tried to solve it with the circumradius theorem : $A=(abc)/(4R)$ , where $a$ , $b$ , and $c$ are the three sides, and $R$ is the circumradius of the triangle. However, I needed to find the exact value of $\cos(5\pi/14)$ , $\cos(4\pi/7)$ , and $\sin(5\pi/14)$ . Finally, I found an explicit formula for this particular triangle, but the proof was missing. You can find the formula in Wikipedia's ""Heptagonal triangle"" entry .",['geometry']
3688745,"For arbitrarily small number $\epsilon$ > 0, is there a natural number $n$ such that $\frac{\phi(n)}{n}< \epsilon$? [duplicate]","This question already has answers here : Proving $\phi (n_k) \ll \frac{n_k}{\log \log n_k}.$ (2 answers) Closed 2 years ago . For arbitrarily small number $\epsilon$ > 0, is there a natural number n such that $\frac{\phi(n)}{n}< \epsilon$ ? $\phi(x)$ is the Euler's totient function, representing number of positive integers up to a given integer $x$ that are relatively prime to $x$ . I do think the answer would be yes but I'm stuck with the proof. Any insights?",['number-theory']
3688840,On a tamely ramified extension of $\mathbb{Q}_{p}$,"I'm stuck with the following problem given in a book which I'm reading, it's about creating a tamely ramified extension of $\mathbb{Q}_{p}$ . Let $p\in\mathbb{Z}$ be a prime number, and let $\mathbb{Q}_{p}$ be the field of $p$ -adic numbers. Given a finite field extension $K/\mathbb{Q}_{p}$ , let $k$ be the residue field of $K$ . We define the ramification index $e(K/\mathbb{Q}_{p})$ as the index group of the value group of $K$ in the value group of $\mathbb{Q}_{p}$ . Also, we define the inertia/residue degree $f(K/\mathbb{Q}_{p})$ as the degree extension of $k/\mathbb{F}_{p}$ . By a theorem, we know that $[K:\mathbb{Q}_{p}]=e(K:\mathbb{Q}_{p})f(K:\mathbb{Q}_{p})$ . Finally, we say that $K/\mathbb{Q}_{p}$ is tamely ramified if $p\nmid e(K/\mathbb{Q}_{p})$ . Here is the problem. Consider the polynomial $f=\frac{X^{p}-1}{X-1}=X^{p-1}+X^{p-2}+\cdots+1\in\mathbb{Q}_{p}[X]$ . Let $\zeta_{p}$ be a root of $f$ , the claim is that $\mathbb{Q}_{p}(\zeta_{p})/\mathbb{Q}_{p}$ is tamely ramified. Here is my attempt: I have proved that $f$ is irreducible over $\mathbb{Q}_{p}$ , so let $\zeta_{p}$ be a root of $f$ , then the extension $\mathbb{Q}_{p}(\zeta_{p})/\mathbb{Q}_{p}$ has degree $p-1$ . Since $(p,p-1)=1$ , in order to prove that the extension is tamely ramified, we need to show thet the residue fiel of $\mathbb{Q}_{p}(\zeta_{p})$ is $\mathbb{F}_{p}$ or that the ramification index of $\mathbb{Q}(\zeta_{p})/\mathbb{Q}_{p}$ agrees with the degree of the extension, i.e., $p-1$ . My attempt is to show that $\mathbb{F}_{p}$ is the residue field of $\mathbb{Q}_{p}(\zeta_{p})$ . To do this, let $k$ be the residue field of $\mathbb{Q}_{p}(\zeta_{p})$ . It is clear that $\mathbb{F}_{p}\subset k$ . So, to see $k\subset\mathbb{F_{p}}$ , let $x$ in the valuation ring of $\mathbb{Q}_{p}(\zeta_{p})$ , i.e., $|x|\leq 1$ . I can write $x=a+b\zeta_{p}$ , where $a,b\in\mathbb{Z}_{p}$ . If we denote with $\overline{x}$ the class of $x$ in $k$ , we have $\overline{x}=\overline{a}+\overline{b}\overline{\zeta_{p}}$ . Note that $\overline{a},\overline{b}\in\mathbb{F}_{p}$ , so we only need to show $\overline{\zeta_{p}}\in\mathbb{F}_{p}$ . Previously, I proved that $\zeta_{p}=1+\lambda_{1}$ , where $|\lambda_{1}|=p^{-1/(p-1)}<1$ , hence the class of $\lambda_{1}$ is zero, thus $\overline{\zeta_{p}}=\overline{1}$ , which proves that $\overline{x}\in\mathbb{F}_{p}$ , so we can conclude that $\mathbb{Q}(\zeta_{p})/\mathbb{Q}_{p}$ is tamely ramified. Is my argument corect? I would appreciate any correction or any hint, thanks.","['number-theory', 'p-adic-number-theory', 'ramification']"
3688858,Vieta's formulas for $x^2+px+1$ and $x^2+qx+1$.,"Let $\alpha$ and $\beta$ be the roots for $x^2+px+1$ and let $\gamma$ and $\delta$ be the roots
  for $x^2+qx+1$ . Show that $$(\alpha-\gamma)(\beta-\gamma)(\alpha+\delta)(\beta+\delta)=q^2-p^2.$$ This seemed to be rather peculiar. It should be a simple application of Vieta's formulas, but I couldn't get it to the form they wanted... We have $\alpha+\beta=-p$ , $\alpha\beta=1$ and $\gamma+\delta=-q$ , $\gamma\delta=1$ . Also $\alpha^2+p\alpha+1=0 \Rightarrow p = -\alpha-\frac{1}{\alpha} \Rightarrow p^2 = \alpha^2+2+ \frac{1}{\alpha^2}$ and $\gamma^2+q\gamma+1=0 \Rightarrow p = -\gamma-\frac{1}{\gamma} \Rightarrow q^2 = \gamma^2+2+ \frac{1}{\gamma^2}$ . This should imply that $q^2-p^2 = (\gamma^2+2+\frac{1}{\gamma^2})-(\alpha^2+2+ \frac{1}{\alpha^2}) = \gamma^2+\frac{1}{\gamma^2} - \alpha^2-\frac{1}{\alpha^2}$ . What should I do here?","['algebra-precalculus', 'polynomials']"
3688891,Integral $\int^{\pi/2}_{0}\ln(\alpha\sin^2x+\beta\cos^2x)dx$,"I have tried this integral in multiple ways but I can not reach a solution, it is done using integration under de integral sign. $$\int^{\pi/2}_{0}{\ln{\left(\alpha\sin^2x+\beta\cos^2x\right)}dx}\quad \alpha,\beta\gt0$$ After doing the corresponding differentiation I ended up with this expression $$\int^{\pi/2}_{0}{-\frac{\cos^2x\sin^2x}{\left(\alpha\sin^2x+\beta\sin^2x\right)^2}dx}$$ Maybe I am doing something wrong, but I can't figure out what to do next.","['integration', 'calculus', 'definite-integrals']"
3688896,Word problem and quotient group,"Let $G$ be given by the group presentation $G = \langle a,b \mid S \rangle$ , where $S = \{aa,bbb\}$ . The formal definition of $G$ is: Let $F$ be the free group on $\{a,b\}$ . Let $H$ be the conjugate
  closure of $S$ , i.e. $H$ is the subgroup of $F$ generated by $\{fsf^{-1} : f \in F, s \in S\}$ . Then we can define the quotient
  group $G = F/H$ . Let $x,y \in F$ be words consisting of only letters $a, b$ . Using the formal definition of $G$ above, how can we show that if $x=y$ in $G$ (i.e. $xH = yH$ ), then there is a finite sequence of operations changing $x$ to $y$ where in each operation we either add/remove an $aa$ or add/remove a $bbb$ to $x$ ? Thoughts: We can write $y^{-1}x = \text{reduce}(f_1 s_1 f_1^{-1} f_2 s_2 f_2^{-1} \dots f_k s_k f_k^{-1})$ , where $f_i \in F$ , and $s_j \in S' = \{aa,bbb,a^{-1}a^{-1},b^{-1}b^{-1}b^{-1}\}$ . Here reduce( $w$ ) is the reduction of $w$ in the free group by cancellation of consecutive $a,a^{-1}$ or $b,b^{-1}$ . But I'm not sure how to proceed from here. Edit: Can we also show this holds for arbitrary $S$ ? For example, $S = \{aa,bbb,ababab\}$ ?","['group-presentation', 'combinatorics-on-words', 'quotient-group', 'abstract-algebra', 'group-theory']"
3688980,Random walk- minimizing expected distance to the origin,"Given $\delta\in [0,1]$ and $n\in \mathbb{N}$ , consider a (biased) random walk $S_n(\delta) = \sum_{i = 1}^n X_i$ where $\{X_i:1\le i\le n\}$ are i.i.d. and $X_i = 1$ with probabiltiy $(1+\delta)/2$ and $-1$ otherwise.  I am wondering whether the expected distance to the origin increases as the bias $\delta$ increases.  Formally, if $0\le\delta\le\delta'\le 1$ , for all $n\in \mathbb{N}$ $$\mathbb{E}[|S_n(\delta)|]\le \mathbb{E}[|S_n(\delta')|].$$ where $|\cdot|$ is the one norm. Note that the second moment is increasing as $\delta$ increases, because $\mathbb{E}[S_n(\delta)^2] = n^2\delta^2+n(1-\delta^2)$ .  Additionally, by Chebyshev's inequality, we can prove the above inequality for large enough $n$ .  I am wondering if the inequality holds for all $n\ge 1$ .  Maybe it can be proved by a coupling argument.","['inequality', 'random-walk', 'coupling', 'probability']"
3688995,Understanding Fraleigh's proof of: Every finite integral domain is a field,"Here's how Fraleigh proves: Every finite integral domain is a field in his book: Let \begin{equation*}
        0, 1, a_1, \dots, a_n
    \end{equation*} be all the elements of the finite domain $D$ . Now, consider \begin{equation*}
        a1, aa_1, \dots, aa_n
    \end{equation*} Since the multiplicative cancellation laws hold in $D$ , it means that each of $a1, aa_1, \dots, aa_n$ are distinct from each other since $aa_i = aa_j \implies a_i = a_j$ . Also, since $D$ has no divisors of $0$ , neither of $a1, aa_1, \dots, aa_n$ can be zero. Hence, $a1, aa_1, \dots, aa_n$ are elements $1, a_1, \dots, a_n$ in some order. So, either $a1 = 1 \implies a = 1$ or $aa_i = 1$ for some $i$ . My addition: If $a = 1$ , then the conditional in question is trivially satisfied and there is nothing to prove. So, without loss of generality, assume $aa_i = 1$ . This shows that $a$ has a multiplicative inverse, $a_i$ . $\square$ I have two questions: firstly, is my addition to the proof valid? Secondly, how does $D$ has no divisors of $0$ imply ""neither of $a1, aa_1, \dots, aa_n$ can be zero"" (in bold above). The definition of 0 divisors that Fraleigh has given is: If $a$ and $b$ are two nonzero elements of a ring $R$ s.t. $ab = 0$ , then $a$ and $b$ are divisors of 0. To conclude that ""neither of $a1, aa_1, \dots, aa_n$ can be zero"" from this definition, I think  we would need to know that the product of any two terms from $a1, aa_1, \dots, aa_n$ is zero but we don't know this. What am I missing? Thanks!","['field-theory', 'proof-explanation', 'abstract-algebra', 'domain-theory']"
3689064,"Technique for simplifying, e.g. $\sqrt{ 8 - 4\sqrt{3}}$ to $\sqrt{6} - \sqrt{2}$","How to find the square root of an irrational expression, to simplify that root. e.g.: $$
\sqrt{ 8 - 4\sqrt{3} } = \sqrt{6} - \sqrt{2}
$$ Easy to verify: \begin{align}
(\sqrt{6} - \sqrt{2})^2 = 6 - 2\sqrt{12} +2 
 = 8 - 4 \sqrt{3}
\end{align} But how to work it out in the first place? I feel there's a standard technique (Completing-the-square? Quadratic formula?), but don't recall it or what it's called... BTW: this came up in verifying equivalence of different calculations of $\cos{75°}$ (the above divided by $4$ ), as $\cos{\frac{90°+60°}{2}}$ vs $\cos{(45°+30°)}$ , from 3Blue1Brown's lockdown video on complex numbers and trigonometry .","['algebra-precalculus', 'radicals']"
3689069,"if $B$ is countable, then the following are equivalent","Suppose $B \neq \varnothing$ . Prove the following are equivalent ${\bf A.}$ B countable ${\bf B.}$ there is a surjection $f: \mathbb{Z}_+ \to B$ ${\bf C.}$ there is an injection $g: B \to \mathbb{Z}_+ $ Attempt: (I already proved $A \implies B$ ) First we prove $B \implies C$ . Let $f$ be surjection. Since $B$ is not empty, it has a smallest element, say $b_1$ and $f$ surjection $\implies$ there is some $i_1 \in \mathbb{Z}_+$ such that $f(i_1) = b_1$ Now, consider $B \setminus \{ b_1 \}$ . If this set is empty, then $g(b_1) = i_1$ is desired injection. If not, then there is smallest element in $B \setminus \{b_1\}$ , call it $b_2$ and so $\exists i_2 \in \mathbb{Z}_+$ so that $f(i_2) = b_2$ Now, if $B \setminus \{ b_1, b_2\}$ is empty, then $g(b_k) = i_k $ for $k=1,2$ If we continue in this fashion, we obtain a list $\{ b_1,b_2,...... \}$ so that $g(b_k) = i_k $ where $i_1,i_2,.....$ are positive integers. ${\bf C \implies A}$ Take $g: B \to \mathbb{Z}_+$ an injection. We need to prove $B$ is countable. By contradiction if $B$ is uncountable, then there is ${\bf NO}$ bijection from $B \to \mathbb{Z}_+$ but this really doesnt help, we can still have injections. My other idea is to procceed as : since $g$ is injection then $g$ maps some $b_i$ from $B$ in one-to-one correspondence to positive integers: $g(b_i) = i$ say $i \leq n$ But I am having trouble seeing how to extend this to a surjection. Any help? Is my first implication correct?","['elementary-set-theory', 'calculus', 'solution-verification', 'real-analysis']"
3689096,Find all positive integers $n$ such that $\varphi(n)$ divides $n^2 + 3$,"This was the Question:- Find all positive integers $n$ such that $\varphi(n)$ divides $n^2 + 3$ What I tried:- I knew the solution and explanation of all positive integers $n$ such that $\varphi(n)\mid n$ . That answer was when $n = 1$ , or $n$ is of the form of $2^a$ or $2^a3^b$ . I tried to relate this fact with this problem in many ways, but couldn't get to a possible solution. Any hints or suggestions will be greatly appreciated","['number-theory', 'totient-function', 'elementary-number-theory']"
3689142,An integral and an infinite sum of incomplete Gamma functions,"While trying to evaluate the integral $\int \frac{dx}{x+e^x}$ , we see that: $$ \int \frac{dx}{x+e^x} = \int \frac{1+e^x}{x+e^x}dx  - \int \frac{e^{x}}{x+e^x}dx.$$ Now, the first integral on the RHS is of the form $\int \frac{dv}{v}$ and can be evaluated easily. Coming to the second one, $$ \int \frac{e^{x}}{x+e^x} dx= \int \frac{dx}{\frac{x}{e^x}+1}. $$ We can expand the integrand and write it as an infinite sum: $$ \frac{1}{\frac{x}{e^x}+1} = 1-\frac{x}{e^x}+\frac{x^2}{e^{2x}}-\frac{x^3}{e^{3x}}+\dots=\sum_{n=0}^{\infty}(-1)^nx^ne^{-nx}.$$ The integral of each of the term in the series can be represented as an incomplete Gamma function as $$\int x^ne^{-nx}dx=\frac{\Gamma(n+1,nx)}{(-n)^{n+1}}.$$ However, I don't see possible any further simplification. Suppose it was a definite integral with limits $|x|<1$ , then can we say anything more about the integral? Any help is appreciated.","['integration', 'calculus']"
3689229,Why am I getting the wrong answer when I factor an $i$ out of the integrand?,"Consider the following definite integral: $$I=\int^{0}_{-1}x\sqrt{-x}dx \tag{1}$$ With the substitution $x=-u$ , I got $I=-\frac{2}{5}$ (which seems correct). But I then tried a different method by first taking out $\sqrt{-1}=i$ from the integrand: $$I=i\int^{0}_{-1}x\sqrt{x}dx=\frac{2i}{5}[x^{\frac{5}{2}}]^{0}_{-1}=\frac{2i}{5}{(0-(\sqrt{-1})^5})=-\frac{2i^6}{5}=+\frac{2}{5} \tag{2}$$ which is clearly wrong. I understand that $x\sqrt{x}$ is not even defined within $(-1,0)$ , but why can't we use the same 'imaginary approach' ( $\sqrt{-1}=i$ ) to treat this undefined part of the function (i.e. the third equality in $(2)$ ). I can't find a better way of phrasing my question so it may seem gibberish, but why is $(2)$ just invalid?","['integration', 'calculus', 'definite-integrals', 'complex-numbers']"
3689253,"$n$ insects on $|z|=1$ ""occupy"" a point if the product of their distances to it is at most $1$. How much of $C$ can they occupy?","The Devourers of the Unit Circle There are $n$ tiny insects living on the unit circle $C = \{z\in \mathbb{C}, |z|=1\}$ . They act as a swarm and ""occupy"" a point $w$ if the product of distances from $w$ to each of them is at most $1$ . I.e if the insects are at points $z_j$ they occupy the set $$\left\{w\in C : \prod_{j=1}^n |w-z_j| \leq 1 \right\}$$ How much of $C$ can they occupy at most? Here's an interactive version of the picture below (Put the points you want to be active to the list $P_{ts}$ .) Here's what I've done so far. Let the insects be at points $e^{ia_j}$ for $0=a_1 \leq a_2 \leq \dots \leq a_n \leq 2\pi$ . Denote $f(z)=\prod_{j=1}^n |z-z_j|$ . Let's study $$g(t) = \prod_{j=1}^n |e^{it} - e^{ia_j}|^2 \\
= \prod_{j=1}^n \left|e^{-i\frac{t+a_j}{2}} \right|^2|e^{it} - e^{ia_j}|^2 \\
= \prod_{j=1}^n \left|e^{i\frac{t-a_j}{2}} - e^{-i\frac{t-a_j}{2}}\right|^2 \\
= \prod_{j=1}^n 4\sin^2 \left( \frac{t-a_j}{2} \right) \\
= 4^n \prod_{j=1}^n \sin^2 \left( \frac{t-a_j}{2} \right)
$$ Now, for the occupied area to be one connected blob, in the edge case we need to find $t_1, t_2 \dots, t_{n-1}$ where $g'(t_j) = 0$ and $g(t_j) = 1$ (these are the points where the level set $f=1$ is tangent to the unit circle. EDIT I think it's better to have the middle one (if $n$ odd) at $1$ and then others in conjugate pairs so we can collapse the pair $|(e^{it}-e^{ia})(e^{it}-e^{-ia})|^2$ into $(\cos t - \cos a)^2$ and the formulas come a bit simpler. With the formula of WimC's answer I made this picture of how the devouring evolves as more insects arrive: Here's the Sage-code (better version): def getQ(n):
    a = 4^(1/n)
    return 2*a^(-n) * (1 + (1 if n%2 else -1) * chebyshev_T(n,x)(a*(x-1)+1) )

#can find the roots directly
def getRootsOfQ(n):
    a = 4^(1/n)
    #cos(theta) = a*(x-1)+1
    return [N((cos(pi*((2*m+(1-n%2))/n))-1)/a+1) for m in range(n)]

def makePic(n):
    
    x_coords = getRootsOfQ(n)
    x_coords = sorted(list(set(x_coords)))
    pts = [(x1, sqrt(1-x1^2)) for x1 in x_coords] + [(x1, -sqrt(1-x1^2)) for x1 in x_coords[:(-1 if n%2 else len(x_coords))]]
    p = lambda x,y: prod((x-b[0])^2+(y-b[1])^2 for b in pts)
    lim = 1-2*4^(-1/n)
    
    g = Graphics()
    g += circle((0,0), 1, color='black')

    var('y')
    g += contour_plot(p, (x, -1.2, 1.2), (y, -1.2, 1.2), contours=[1], fill=False)
    g += points(pts, color='blue', pointsize=40)

    return g

makePic(100).show(figsize=6)","['complex-analysis', 'optimization']"
3689267,Sum of distances at most ${n \choose 2}$,"If $T$ is a tree and $x \in V(T)$ then $$ s(x) = \sum_{u \in V(T)}
 d(x,u) \leq {n \choose 2} $$ My question is: Is this statement also true for simple connected graphs (not necessarily trees)? Intuitive approach: Since the only difference is that we allow cycles, then in general the distances either decrease or stay the same and thus the inequality still holds. Any thoughts?","['graph-theory', 'discrete-mathematics']"
3689297,Conjecture about crossing paths in $n\times n$ grid: counterexample or ideas,"This asks for ideas or suggestions regarding a conjecture about crossing paths in a grid, or a counterexample. For background, I am starting with a known result, and will articulate the conjecture based on it (the picture gives an example for the conjecture). Definition: Given an $n\times n$ grid, where exactly 1 diagonal is randomly placed in each unit square. Existence Lemma: Going along the diagonals, there is always a path crossing the grid from one side to the opposite side (top-down or left-right). There are several proofs of this lemma. One goes by exploration; one uses a separation theorem from topological dimension theory; one is based on a dual graph approach. They are presented in the original post ( https://mathoverflow.net/q/112067/156936 ). There is another proof making iterated use of the Lemma of Sperner ( https://math.stackexchange.com/a/3677664/782412 ). Definition to cover a special case just to keep the formulation of the conjecture simple: (1) The top left corner of the grid is defined to belong to the top side and to the left side, and similarly for the other three corners of the grid. (2) In light of this corner definition, a path going from the top left corner to the bottom right corner is seen as two paths , i.e. one from top to bottom and one from left to right. Similarly for a path crossing from top right corner to bottom left corner. Conjecture: For $n>1$ , there are at least two paths along the diagonals crossing the grid. Remark: Using the picture above to illustrate the definition of what counts as different paths: Clearly, the paths A-B, A-C, B-C and x-y are no crossing paths, as they don't cross the grid from one side to the opposite side. For the same reason, A-y is not a crossing path. The paths A-x, B-y, and C-y count as three different paths that are crossing the grid. Finally, B-x also counts as a crossing path because it contains A-x, and A-x is a crossing path. (This is consistent with the definition of the special case connecting opposite corners.) In summary, the picture is an example with 4 crossing paths A-x, B-x, B-y, C-y. More generally, two paths can have common diagonals. If we have a crossing path that branches in two paths right before the grid border, it counts as two paths. Two paths can both cross in the same direction, for example left to right crossing for both paths. Question This conjecture is based on samples for $n<10$ . I have tried to extend the proofs of the Existence Lemma, but with no success. Do have any ideas or suggestions for alternative proof approaches, or maybe a counterexample? Maybe as a starting point, does someone have the computing power to check a complete set of examples for small $n$ ?","['graph-theory', 'computational-mathematics', 'combinatorics', 'probability-theory']"
3689325,Perturbation property of Orthonormal basis for Hilbert space,"Suppose $\{e_i\}_{i\in I}$ is an orthonormal basis for some Hilbert space. $\{f_i\}_{i\in I}$ is an orthonormal family with the property $\sum^\infty_{i\in I} \|e_i-f_i\|^2<\infty$, show that $\{f_i\}_{i\in I}$ is also orthonormal basis. I can think of for a $x$ with $(x,f_i)=0$ for all $i$, prove $x=0$. However, I don't know how to continue to the next step.","['hilbert-spaces', 'functional-analysis']"
3689503,Smallest Number $k \in \mathbb{N}$ Such That $(2k-9)! \equiv 0\pmod{k!^2}$,"Find the Smallest Number $k \in \mathbb{N}$ Such That $(2k-9)! \equiv 0\pmod{k!^2}$ My Attempt We want a natrual number k such that $\frac{(2k-9)!}{k!^2}$ is a whole number. so: $\frac{(2k-9)!}{k!^2}$ = $\frac{(k+1)(k+2)...(2k-9)}{k!}$ We can see that some terms of $k!$ might cancel out, like $k+2 $ and $\frac{k}{2} +1$ if $k$ is even. Although, I haven't been able to generalize such behaviour. What can we do now? P.S. I ran a relatively efficient python script, and the number (I don't know what it is) is bigger than $10^5$ . Also, I have defined the sequence $a_n = $ the smallest number $k \in \mathbb{N}$ such that $(2k-n)! \equiv 0\pmod{k!^2}$ . This is how I thought of this problem. From the python script: $a_1 = 1,a_2 = 1,a_3 = 210,a_4 = 210,a_5 = 3478,a_6 = 3478,a_7 = 8178,a_8 = 8178, a_9 = ?$ Is there any info on this?","['number-theory', 'recreational-mathematics', 'factorial', 'modular-arithmetic']"
3689513,Ways for the differential equation $y' + {{y\ln(y)}\over x}= xy$,"I've been stuck on one of my homework numbers. 
The number precise that the following equation is a non-linear equation of order 1 with x>0. $$y' + {{y\ln(y)}\over x}= xy$$ So far, I tried 2 different methods to solve them. As suggested by internet (link below): Bernoulli, and as an inexact one. However, I get stuck every time. Bernoulli sample **As a small side note, can someone explain to me the following from the link: how, by passing ln(y) to the right side, we get y^1+1 ? For the Bernoulli equation, when I try to solve it manually, I get in a kind of infinite integral, no matter how many time I integrate. Am I missing something?","['integration', 'derivatives', 'ordinary-differential-equations']"
3689528,Computing $\int_{0}^{\infty} \frac{x}{x^{4}+1} dx$ using complex analysis.,"I want to compute $$\int_{0}^{\infty} \frac{x}{x^{4}+1} dx$$ using complex analysis. Now the first thing that strikes me is that $f(x)$ is not an even function. So this troubles me a bit since I would normally use $$\int_{0}^{\infty} f(x)dx = \frac{1}{2} \left[\lim_{R \rightarrow \infty} \int_{-R}^{R} f(z)dz + \int_{C_{R}}f(z)dz \right],
$$ where $C_{R}$ is the semi cirle connecting $R$ to $-R$ in the positive imaginary part. Now we see that we have to compute the singularities of $f(z)$ , which we can do by computing the fourth root of $z$ . We then find $$
\begin{align*}
z^{4} &= e^{i (\pi + 2n\pi)} \\
z &= e^{i ( \frac{\pi}{4} + \frac{n\pi}{2} )}
\end{align*}.
$$ Since we are only interested in singularities above the real line, we find $z_{0} = e^{i \frac{\pi}{4}}$ and $z_{1} = e^{i \frac{3\pi}{4}}$ . Then we let $p(z) = z$ and $q(z) = z^{4}+1$ , which makes $q'(z) = 4z^{3}$ . 
We then compute $p(z_{0}), q(z_{0})$ and $q'(z_{0})$ and finally $\frac{p(z)}{q'(z)}$ which equals the residue at $z_{0}$ . However, when I do the above I find $\text{Res}(z_{0}) = - \frac{i}{4}$ and $\text{Res}(z_{1}) = \frac{i}{4}$ but this would make the integral equal zero since $2\pi i (\frac{i}{4} - \frac{i}{4})=0$ . Can anybody point me to my mistake? Also, when would find the value for this integral, I would argue we can not simply take half of it, since the initial function is not even. How would we fix that?","['complex-analysis', 'calculus', 'singular-integrals', 'improper-integrals']"
3689544,Which analogy between polynomials and differential equations did Rota have in mind in his TEN LESSONS?,"Rota writes in his TEN LESSONS I WISH I HAD LEARNED BEFORE I STARTED TEACHING
DIFFERENTIAL EQUATIONS : ""It states that every differential polynomial in the two solutions of
  a second order linear differential equation which is independent of
  the choice of a basis of solutions equals a polynomial in the
  Wronskian and in the coefficients of the differential equation (this
  is the differential equations analogue of the fundamental theorem on
  symmetric functions, but keep it quiet)."" And: ""Worse, no one realizes that changes of variables are not just a
  trick; they are a coherent theory (it is the differential analogue of
  classical invariant theory, but let it pass)."" And: ""For second order linear differential equations, formulas for changes
  of dependent and independent variables are known, but such formulas
  are not to be found in any book written in this century, even though
  they are of the utmost usefulness. Liouville discovered a differential
  polynomial in the coefficients of a second order linear differential
  equation which he called the invariant. He proved that two linear
  second order differential equations can be transformed into each other
  by changes of variables if and only if they have the same invariant.
  This theorem is not to be found in any text. It was stated as an
  exercise in the first edition of my book, but my coauthor insisted
  that it be omitted from later editions."" Where can I learn more about this?",['ordinary-differential-equations']
3689618,Smooth function interpolating between two points.,"How would I find a parametric function that smoothly interpolates between two points $(0, 1)$ and $(N, 0)$ , where the parameter $t \in [-1, 1]$ can be used to change the gradient. Somewhat like this. I know that the functions below the straight line between the points follow some exponential decay, whereas those above follow the form $f(x) = -\tanh(k(x-N))$ . I thought just adding these two functions together and providing an additional parameter $\lambda$ for the weighting of these two terms would work, but it leads to additional bumps.","['interpolation', 'functions']"
3689624,Finding $a_{1996}$ if $\prod_{n=1}^{1996}(1+nx^{3^n})= 1+a_1x^{k_1} + a_2x^{k_2} + \cdots + a_mx^{k_m}$,"I need to find the coefficient $a_{1996}$ $$\prod_{n=1}^{1996}(1+nx^{3^n})= 1+a_1x^{k_1} + a_2x^{k_2} + \cdots + a_mx^{k_m}$$ $a_1, a_2, ... , a_m$ are non zero. $k_1 < k_2 <... < k_m$ So if $x=1$ you can find the sum of all coefficients, but I am not sure how to use this fact Do I need to simplify the product somehow plug different values of x and check if something can be approximated that way, or would that be a waste of time? Thanks,","['algebra-precalculus', 'polynomials']"
3689627,Taylor expansion of a function of a symmetric matrix,"First of all let me tell you that the answer to this question is likely to confirm a not-so-minor error in a very popular (and excellent) textbook on optimization, as you'll see below. Background Suppose that we have a real-valued function $f(X)$ whose domain is the set of $n\times n$ nonsingular symmetric matrices. Clearly, $X$ does not have $n^2$ independent variables; it has $n(n+1)/2$ independent variables as it's symmetric. As is well known, an important use of Taylor expansion is to find the derivative of a function by finding the optimal first-order approximation. That is, if one can find a matrix $D \in \mathbb{R}^{n\times n}$ that is a function of $X$ and satisfies $$f(X+V) = f(X) + \langle D, V \rangle + \text{h.o.t.},  $$ where $\text{h.o.t.}$ stands for higher-order terms and $\langle \cdot, \cdot \rangle$ is inner product, then the matrix $D$ is the derivative of $f$ w.r.t. $X$ . Question Now my question is: What is the right inner product $\langle \cdot, \cdot \rangle$ to use here if the matrix is symmetric? I know that if the entries of $X$ were independent (i.e., not symmetric), then the $\text{trace}$ operator would be the correct inner product. But I suspect that this is not true in general for a symmetric matrix. More specifically, my guess is that even if the $\text{trace}$ operator would lead to the correct expansion in the equation above, the $D$ matrix that comes as a result won't give the correct derivative. Here is why I think this is the case. A while ago, I asked a question about the derivative of the $\log\det X$ function, because I suspected that the formula in the book Convex Optimization of Boyd & Vandenberghe is wrong. The formula indeed seems to be wrong as the accepted answer made it clear. I tried to understand what went wrong in the proof in the Convex Optimization book. The approach that is used in the book is precisely the approach that I outlined above in Background. The authors show that the first-order Taylor approximation of $f(X)=\log\det X$ for symmetric $X$ is $$ f(X+V) \approx f(X)+\text{trace}(X^{-1}V). $$ The authors prove this approximation by using decomposition specific to symmetric matrices (proof in Appenix A.4.1; book is publicly available ). Now this approximation is correct but $X^{-1}$ is not the correct derivative of $\log\det X$ for symmetric $X$ ; the correct derivative is $2X^{-1}-\text{diag}(\text{diag}(X^{-1}))$ . Interestingly, the same approximation  in the formula above holds for nonsymmetric invertible matrices too (can be shown with SVD decomposition), and in this case it does give the right derivative because the derivative of $\log\det X$ is indeed $X^{-T}$ for a matrix with $n^2$ independent entries. Therefore I suspect that $\text{trace}$ is not the right inner product $\langle \cdot, \cdot \rangle$ for symmetric matrices, as it ignores the fact that the entries of $X$ are not independent. Can anyone shed light on this question? Added: A simpler question Based on a comment, I understand that the general answer to my question may be difficult, so let me ask a simpler question. The answer to this question may be sufficient to show what went wrong in the proof in the Convex Optimization book. Suppose $g(X)$ is a function $g: \mathbb{R}^{n\times n} \to \mathbb R$ . Is it true that the first-order Taylor approximaton with trace as inner product, i.e., $$g(X+V) \approx g(X) + \text{trace}\left( \nabla g (X)^T V \right), $$ implicitly assumes that the entries of $X$ are independent? In other words, is it true that this approximation may not hold if entries of $X$ are not independent (e.g., if $X$ is symmetric)?","['matrices', 'matrix-calculus', 'linear-algebra', 'taylor-expansion']"
3689667,Derivatives and definition,"I’m currently doing a course in Mathematical Analysis at University level. I ask myself a simple question; when you’re finding the derivative of a function, you’re essentially finding the rate at which the output is changing (dy) over the rate at which the input is changing (dx). However, whenever my lecturer says the graph is not differentiable at a certain point (eg |x| not differentiable at the origin), they say that there’s isn’t a unique tangent at the origin for that graph. Could someone help me understand what exactly is meant by unique tangent and derivative at a point? From what I gather: If a function is differentiable at a point, there exists a unique tangent at that point of the graph.","['tangent-line', 'derivatives', 'real-analysis']"
3689692,Coordinate rings of projective varieties as UFDs,"I can't find the mistake in my logic and so I hope you can help me. Let $k$ be an algebraically closed field. It is well known that the projective algebraic sets $\mathbb{P}_{k}^1$ and $V=V(x^2+y^2-z^2)$ are isomorphic. In an exercise problem, we were supposed to prove that coordinate rings of rational curves (i.e. curves which are birationally equivalent to $\mathbb{P}^1$ ) are UFDs. However, $k[V] = k[x,y,z]/(x^2+y^2-z^2)$ is not a UFD (see e.g. MSE/413506 ). Where is my mistake?","['algebraic-curves', 'algebraic-geometry', 'unique-factorization-domains']"
3689743,What does expanding $e^{2x}$ in powers of $\left(x-1\right)$ means?,"It was given that I have to expand $e^{2x}$ in powers of $(x-1)$ up to four terms. The Maclaurin series I've calculated is $$f(x)=1+2x+2x^2+\frac{4}{3}x^3.$$ Now, replacing $x$ with $x-1$ gives, $$f(x)=1+2(x-1)+2(x-1)^2+\frac{4}{3}(x-1)^3.$$ I don't understand what are we doing in this whole process. Why can't we raise the function before hand, that is, $f(x)=e^{2x-2}$ ?","['power-series', 'calculus', 'derivatives']"
3689757,"Proving $\pi=(27S-36)/(8\sqrt{3})$, where $S=\sum_{n=0}^\infty\frac{\left(\left\lfloor\frac{n}{2}\right\rfloor!\right)^2}{n!}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I have to prove that: $$\pi=\frac{27S-36}{8\sqrt{3}}$$ where I know that $$S=\sum_{n=0}^\infty\frac{\left(\left\lfloor\frac{n}{2}\right\rfloor!\right)^2}{n!}$$ Where do I get started?","['summation', 'calculus', 'pi', 'sequences-and-series', 'power-series']"
3689790,"Why does this ""gradient field test"" not work on the spin field $S / r^2$? (from Strang's Calculus)","In section 15.2, Strang's Calculus explains that for any gradient field $\bf{F} = Mi + Nj$ , ${\partial M \over \partial y} = {\partial N \over \partial x}$ . (Strang calls this ""test D"" for identifying a vector field as being a gradient of some function.) This makes sense, since the components of a ""gradient field"" are the partial derivatives of some function $f$ , and we know that for any $f$ , ${\partial f \over \partial x \partial y} = {\partial f \over \partial y \partial x}$ . The gradient of $f = \tan^{-1}\left({y \over x}\right)$ is ${-y \over x^2 + y^2}i + {x \over x^2 + y^2}j$ . However, this vector field does not seem to pass ""test D"", since $$
{\partial \over \partial x}\left({-y \over x^2 + y^2}\right) = {2xy \over (x^2 + y^2)^2}
$$ But $$
{\partial \over \partial y}\left({x \over x^2 + y^2}\right) = -{2xy \over (x^2 + y^2)^2}
$$ I'm sure something is wrong with my reasoning, but I am struggling to find the mistake. Can anyone point it out?","['multivariable-calculus', 'calculus', 'vector-fields', 'vector-analysis']"
3689837,"Derive $P[D = 1|X]$ from $X = f(X,-1)P[D =-1|X] + f(X,1)P[D =1|X]$","Given the random variables $X: \Omega \to \mathbb R$ and $D: \Omega \to \{-1,1 \}$ , and the (measurable) function $f: \mathbb R \times \{-1, 1 \} \to \mathbb R$ . Show that if $$ 
X = f(X,-1)P[D =-1|X] + f(X,1)P[D =1|X], 
$$ then $$ 
\frac{X - f(X, -1)}{f(X, 1) - f(X, -1)} = P[D = 1 |X].
$$ All I can see is the immediate algebraic manipulation $$
\frac{X -  f(X, -1)P[D=-1|X]}{f(X, 1)} = P[D = 1|X]
,$$ but then I am perplexed how to proceed. Most grateful for any help provided!","['conditional-expectation', 'probability-theory']"
3689873,Approximation of smooth diffeomorphisms by polynomial diffeomorphisms?,"Is it possible to (locally) approximate an arbitrary smooth diffeomorphism by a polynomial diffeomorphism ? More precisely: Let $f:\mathbb{R}^d\rightarrow\mathbb{R}^d$ be a smooth diffeomorphism. For $U\subset\mathbb{R}^d$ bounded and open and $\varepsilon>0$ , is there a diffeomorphism $p=(p_1, \cdots, p_d) : U\rightarrow\mathbb{R}^d$ (with inverse $q:=p^{-1} : p(U)\rightarrow U$ ) such that both $\|f - p\|_{\infty;\,U}:=\sup_{x\in U}|f(x) - p(x)| < \varepsilon$ , $\ \textbf{and}$ each component of $p$ and of $q=(q_1,\cdots,q_d)$ is a polynomial, i.e. $p_i, q_i\in\mathbb{R}[x_1, \ldots, x_d]$ for each $i=1, \ldots, d$ ? Clearly, by Stone-Weierstrass there is a polynomial map $p : \mathbb{R}^d\rightarrow\mathbb{R}^d$ with $\|f - p\|_{\infty;\,U} < \varepsilon$ and such that $q:=(\left.p\right|_U)^{-1}$ exists; in general, however, this $q$ will not be a polynomial map. Do you have any ideas/references under which conditions on $f$ an approximation of the above kind can be guaranteed nonetheless?","['diffeomorphism', 'approximation-theory', 'real-analysis', 'inverse', 'polynomials']"
3689892,Measurable functions : $f(A) \in \mathcal{B}$,"I am new to measure theory and here is the definition I have : (1) A function $f:(X, \mathcal{A}) \to (Y, \mathcal{B})$ is mesurable iff
  : $\forall B \in \mathcal{B}, f^{-1}(B) \in \mathcal{A}$ Why this definition and not this one ? (2) A function $f:(X, \mathcal{A}) \to (Y, \mathcal{B})$ is mesurable
  iff : $\forall A \in \mathcal{A}, f(A) \in \mathcal{B}$ Thus with (2) a function is measurable iff it maps measurable sets to measurable sets. It's seems more natural to me. 
I know that the first definition extend the notion of continuity, but this explanation still doesn't convince me that (1) should be the most natural definition. So are functions that respect (2) have a name ? And why (2) is not the definition of measurable functions ?","['measure-theory', 'analysis', 'real-analysis', 'measurable-sets', 'measurable-functions']"
3690009,Tight bounds on the partial Möbius sum $\sum_{\substack{d|n\\d<Q}}\mu(d)$,"An important area of study in Analytic Number Theory is the behavior of the Möbius function $\mu(n)$ . I was trying to prove a different theorem when I came about a very interesting behavior. If you look at the partial sums of the sum of the Möbius function over the divisors of $n$ , namely $$A_Q(n)=\sum_{\substack{d|n\\d<Q}}\mu(d)$$ We see extraordinary cancelling. If we take $Q$ to larger than the maximum divisor than $n$ then of course this sum will be $0$ for $n>1$ , but I would still have expected large partial sums. If we define $$\omega_Q(n)=\sum_{\substack{p|n\\p<Q}}1$$ to be the number of distinct prime factors of $n$ less than $Q$ , then my conjecture is that there exists an integer $k$ such that $$\left| A_Q(n)\right|=O\left(\omega_Q(n)^k\right)$$ There is strong numerical evidence to back this up. I have experimentally seen that, for a given count $j$ of prime factors less than $Q$ , $\left| A_Q(n)\right|$ finds its largest values when $n=p_1p_2\cdots p_j$ is the product of the first $j$ primes. I then used Desmos to create the best-fit cubic which has an $r^2$ value of $.999$ : It is essentially a perfect fit. This means that is highly probable that $$A_Q(n)=O\left(\omega_Q(n)^3\right)$$ but even proving that it is polynomial in $\omega_Q(n)$ is difficult. Does anyone have any ideas or know of any papers that discuss this topic?","['analytic-number-theory', 'number-theory', 'mobius-function']"
3690012,Name of topology analogous to Zariski topology for an arbitrary model of a first-order theory,"In the Zariski topology , a set of points in $A^n$ , $w$ , is closed by definition when there exists a set of polynomials in $n$ variables, $S$ , such that the following holds. $$ \forall u \in A^n \mathop. \bigg( u \in w \iff (\forall f \in S \mathop. f(u) = 0) \bigg) $$ By squinting, we can come up with a topology with a similar definition that works for an arbitrary model of a first-order theory and convince ourselves that it's really a topology. I'm wondering what this topology's name is (if it has one) and whether it is at all useful for classifying models in some way. Let $L$ be a language with constant, function, and relation symbols. Let $\theta$ be a set of $L$ -sentences. Let $M$ be our model, $M \models \theta$ . Let $V$ be a set of well-formed formulas where each well-formed formula has exactly one free variable $x$ . We can assume that the free variable is named $x$ without losing any generality. We define $w$ to be a closed set when the following holds: $$ \forall u \in M_D \mathop. \bigg( u \in w \iff (\forall v \in V \mathop. v[x:=u] \;\;\text{is true in $M$}) \bigg) $$ We can show that this thing is really a topology. Let $F$ be a family of sets of wff's. The subset of $M_D$ associated with $\cup F$ is closed, therefore the closed sets of our topology are closed under arbitrary intersection. To show closure under finite union, it suffices to show closure under binary unions. Suppose we have two sets of wffs with one free variable $A$ and $B$ . Further suppose that in each wff in $A$ and each wff in $B$ the free variable is named $x$ . Define $C$ as follows: $$ C \stackrel{\text{def}}{=\!=} \{ a \lor b \mathop. a \in A \land b \in B \} $$ In other words, we take every pair of wffs where the left formula comes form $A$ and the right formula comes from $B$ and join them together with a $\lor$ . The empty set is given by the formula $\exists x \mathop. x \neq x$ . The whole domain of the model is given by the formula $\exists x \mathop. x = x$ .","['zariski-topology', 'general-topology', 'model-theory']"
3690033,"A sequence includes $a_p=\sqrt2$, $a_q=\sqrt3$, $a_r=\sqrt5$ for some $1\leq p<q<r$. Can these be terms of an arithmetic progression? harmonic?","In a sequence $a_1, a_2,\dots$ of real numbers it is observed that $a_p=\sqrt{2}$ , $a_q=\sqrt{3}$ , and $a_r=\sqrt{5}$ , where $1\leq p<q<r$ are positive integers. Then $a_p$ , $a_q$ , $a_r$ can be terms of (A) an arithmetic progression (B) a harmonic progression (C) an arithmetic progression if and only if $p$ , $q$ , and $r$ are perfect squares (D) neither an arithmetic progression nor a harmonic progression I have tried using the definition of AP and argued that if the first option is true then $\frac{\sqrt{3}-\sqrt{2}}{\sqrt{5}-\sqrt{3}}=\frac{q-p}{r-q}$ is true which implies that the left hand side is an rational number. I don't know whether the left hand side is a rational number or not. I am stuck here. Also, I don't know how correct this approach is. How do I approach and solve this problem?","['algebra-precalculus', 'arithmetic-progressions', 'sequences-and-series']"
3690065,Necessary and sufficient condition for weak convergence and convergence of density,"Let $(\mu_n)_n$ and $\mu$ be two probability measure, having respectively density $(f_n)_n$ and $f$ for the measure $\lambda$ on $(\mathbb{R},B(\mathbb{R})).$ Prove that the following statement are equivalent: a) $(\mu_n)_n$ converges weakly to $\mu$ and $$\forall \epsilon>0,\exists \delta>0;\forall n \in \mathbb{N}, \forall E \in B(\mathbb{R}),\lambda(E)\leq \delta\implies\int_Ef_n(x)dx \leq \epsilon$$ b) $(\mu_n)_n$ converges weakly to $\mu$ and $$\lim_{k\to+\infty}\sup_{n \in \mathbb{N}}\int_{\left\{f_n>k \right\}}f_n(x)dx=0.$$ c) $\forall E \in B(\mathbb{R}),\lim_{n\to+\infty}\mu_n(E)=\mu(E).$ If $(\mu_n)_n$ converges weakly to a probability measure $\sigma$ and for all $\epsilon>0,$ there exist $\delta>0$ such that for all $n \in \mathbb{N},$ for all $E \in B(\mathbb{R})$ such that $\lambda(E)\leq \delta,\int_Ef_n(x)dx \leq \epsilon.$ Is it true that $\sigma$ have a probability density ? (There exist $\phi:\mathbb{R}\to\mathbb{R}^+,$ such that $\int_{\mathbb{R}}\phi(x)dx=1, \sigma(U)=\int_U\phi(x)dx,$ for all $U \in B(\mathbb{R})$ ) This is the attempt so far. a) $\implies$ b). Take $\epsilon>0.$ there exist $\delta>0$ such that $$\forall n \in \mathbb{N},\forall E \in B(\mathbb{R}),\lambda(E) \leq \delta \implies \int_Ef_ndx \leq \epsilon.$$ Let $k \geq \frac{1}{\delta}.$ So $$\forall n \in \mathbb{N},\lambda(\left\{f_n>k \right\}) \leq \frac{1}{k} \leq \delta$$ which means that $$\forall n \in \mathbb{N},\int_{\left\{f_n>k \right\}}f_n \leq\epsilon,$$ Then $\sup_n\int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon.$ b) $\implies$ a). Let $\epsilon>0.$ there exist $k>0$ such that $$\sup_n \int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon/2.$$ Let $n \in \mathbb{N},E \in B(\mathbb{R})$ such that $\lambda(E) \leq \frac{\epsilon}{2(k+1)}.$ $$\int_E f_n(x)dx \leq k\lambda(E)+\int_{\left\{f_n>k \right\}}f_n(x)dx \leq \epsilon.$$ How can we proceed with c) $\implies$ a)? 2) is the statement correct?","['integration', 'measure-theory', 'weak-convergence', 'real-analysis', 'probability-theory']"
3690116,Higher order derivatives and the chain rule,"So here I have an assignment about higher order derivatives and the chain rule, and a relation to be proved: 
Show that for a rotation in the plane $$\begin{bmatrix}u\\v \end{bmatrix} =\begin{bmatrix}\cos\theta & -\sin\theta\\\sin\theta & \cos\theta\end{bmatrix} \begin{bmatrix}x\\y \end{bmatrix}$$ and any twice differentiable function $f,$ there holds $$\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = \frac{\partial^2f}{\partial u^2}+\frac{\partial^2f}{\partial v^2}.$$ What I got so far is that $ f(u,v)=f(x\cos \theta-y\sin\theta, x\sin\theta + y\cos\theta) $ from the matrix multiplication. 
But I don't really understand how to get $\frac{\partial f}{\partial u} $ and $\frac{\partial f}{\partial v}$ .","['partial-derivative', 'multivariable-calculus', 'calculus', 'chain-rule']"
3690146,Proving the orthogonality of $\sin\frac{2\pi x}{\pi-e}$ and $\cos\frac{2\pi x}{\pi-e}$,"I want to prove the orthogonality of the functions: $\sin\left(\dfrac{2\pi x}{b-a}\right)$ and $\cos\left(\dfrac{2\pi x}{b-a}\right)$ , where $b=\pi$ and $a = e$ My work: $$\begin{align}
\int^{\pi}_{e} \frac{1}{2} \sin\left(\frac{4\pi x}{\pi - e}\right)dx 
&= -\frac{\pi - e}{8 \pi} \left[\cos\left(\frac{4\pi x}{\pi - e} \right)\right]^{\pi}_e \tag{1}\\[6pt]
&= \frac{e-\pi}{8\pi}\left[\cos\left(\frac{4\pi^2}{\pi - e} \right) - \cos\left(\frac{4\pi e}{\pi - e}\right) \right] \tag{2}\\[6pt]
&= \frac{\pi - e}{4\pi} \left[\sin\left(\frac{2\pi (\pi - e)}{\pi - e} \right)\sin\left(\frac{2\pi (\pi + e)}{\pi - e} \right) \right] \tag{3}\\[6pt] = 0 
\end{align}$$ Haven't I made any mistake?","['integration', 'trigonometry', 'functions', 'orthogonality']"
3690151,Definition of an Induced Representation. Technicalities.,"Consider a group $\,G\,$ , a Hilbert space $\,{\mathbb{V}}\,$ with a dot product $\,\langle~,~\rangle\,$ , and a space $\,{\cal{L}}^G\,$ of functions $\varphi$ on this group: $$
 {\cal{L}}^G\;=\;\left\{~\varphi~\Big{|}~~~\varphi:\,~G\longrightarrow{\mathbb{V}}\,\right\}~~.
 $$ Let $\,D\,$ be a representation of a subgroup $\,K\leq G\,$ in the said Hilbert space: $$
 D~:\quad K~\longrightarrow~GL({\mathbb{V}})\;\;.\qquad\qquad\qquad (1)
 $$ On a group element $\,g\in G\,$ , a function $\,\varphi\in{\cal{L}}^G\,$ assumes the value $\,\varphi(g)\in{\mathbb{V}}\,$ . Since this value is a vector in the Hilbert space, we can act on it with some $\,D(k)\,$ , $\,k\in K\;$ : $$
 k\in K~:\quad \varphi(g)\;\mapsto\;D(k)\,\varphi(g)\;\;,\qquad\varphi(g)\in{\mathbb{V}}\;\;.
 $$ For a fixed $\,g\,$ , this is a mapping of one Hilbert-space vector to another. However, the set of all these mappings, for all $\,g\in G\,$ , generates a mapping of a function to a function: $$
 k\in K~:\quad \varphi\;\mapsto\;D(k)\,\varphi\;\;,\quad\varphi\in{\cal{L}}^G\;\;.
 $$ $$
$$ QUESTION 1: May I write the latter as $$
 D~:\quad K~\longrightarrow~GL({\cal{L}}^G)\;\;,\qquad\qquad\qquad (2)
 $$ using the same notation $\,D\,$ as was used in equation (1)? REMARK: $~$ While interconnected in an obvious way, the two $\,D$ 's are two different representations, because they are acting in different spaces: one in $\,{\mathbb{V}}\,$ , another in $\,{\cal{L}}^G\,$ . Hence the above question. $$
$$ QUESTION 2: Would it be possible to say that these two $\,D$ 's are, in some sense, equivalent? $$
$$ QUESTION 3: The induced representation $\,\operatorname{Ind}_K^GD\,$ is implemented with the left translations $$
  U_g\varphi(x)=\varphi({g^{-1}}x)~~,\qquad g,\,x\in G\;, 
  $$ acting in the subspace $\,\Gamma\in{\cal{L}}^G$ of the Mackey functions: $$
  \Gamma\;=\;\left\{~\varphi~\Big{|}~~~\varphi:\,~G\longrightarrow{\mathbb{V}}\;,\quad \varphi(xk)=D^{-1}(k)\varphi(x)\,\right\}~~.
  $$ Which of the two $\,D$ 's is actually being induced here? -- the $\,D\,$ given by (1) or the $\,D\,$ given by (2)?","['group-theory', 'definition', 'representation-theory', 'lie-groups']"
3690185,Why is $\int_{0}^{1}{(1+x^2)^n dx} \sim \frac{2^n}{n} $?,"By $a_n \sim b_n$ I mean that $\lim_{n \rightarrow \infty} \frac{a_n}{b_n} = 1$ . I don't know how to do this problem. I have tried to apply binomial theorem and I got $$\int_{0}^{1}{(1+x^2)^n dx} = \int_0^1 \sum_{k=0}^n{\binom{n}{k}x^{2k}dx} = \sum_{k=0}^n \int_0^1{ \binom{n}{k}x^{2k}dx} = \sum_{k=0}^n  \frac {\binom{n}{k}}{2k+1}$$ But I don't know what I could do with this, nor if it is a correct approach.","['integration', 'limits', 'calculus']"
3690218,"Find $\lim\limits_{x \to \infty}{\mathrm{e}^{-x}\int_{0}^{x}{f\left(y\right)\mathrm{e}^{y}\,\mathrm{d}y}}$","Given $f(x)$ is a continuous function defined in $(0,\: \infty)$ such that $$\lim_{x \to \infty}f(x)=1$$ Then Find $$L=\lim_{x \to \infty}{\mathrm{e}^{-x}\int_{0}^{x}{f\left(y\right)\mathrm{e}^{y}\,\mathrm{d}y}}$$ My try: we have $$L=\lim_{x \to \infty}{\frac{\int_{0}^{x}{f\left(y\right)\mathrm{e}^{y}\,\mathrm{d}y}}{e^x}}$$ If the numerator is finite then $L=0$ else by L'Hopital's Rule we have $\infty/\infty$ form we get $$L=\lim_{x \to \infty}\frac{f\left(x\right)\mathrm{e}^{x}}{\mathrm{e}^x}=\lim_{x \to \infty}{f\left(x\right)}=1$$ But how to tell whether $\lim\limits_{x \to \infty}\int_{0}^{x}{f\left(y\right)\mathrm{e}^{y}\,\mathrm{d}y}$ is Finite or Infinite?","['integration', 'limits']"
3690234,Showing that if a point is an accumulation point then there exists a sequence of distinct points that converges to it.,"The problem says: Show that if $x_0\in A^´$ , then there exists some sequence $\{x_n\}$ in $A$ of distinct points such that $$\lim_{n\rightarrow\infty}x_n=x_0$$ . I've seen proofs of a similar result that omits the requirement of the points being distinct.
The proof that I came up with is, in shorter words. First, construct a sequence $\{x_n\}$ of points such that $x_n\in\ (V_{\frac{1}{n}}(x_0)\cap A)\setminus\{x_0,x_1,...,x_{n-1}\}$ (the sequence starting at $x_1$ ). This can be done because $V_{\frac{1}{n}}(x_0)\cap A$ is an infinite set and $\{x_0,x_1,...,x_{n-1}\}$ is finite. Then, it´s easy to show that this sequence converges to $x_0$ . My question is if there is an error by defining a sequence this way. My doubt comes from the fact that all of the proofs that I've seen of the other result make use of the Axiom of choice, and I don't know if I'm using it when constructing the sequence or if the sequence is not valid.","['metric-spaces', 'analysis', 'real-analysis', 'calculus', 'general-topology']"
3690273,"Cocomplete concrete category, morphism of sheaves on a space is an isomorphism iff every induced map on stalks is an isomorphism","Reading Hartshorne, I had this question: Suppose $F$ and $G$ are $C$ -valued sheaves on a space $X$ , where $C$ is a cocomplete concrete category. Is it the case that $\phi:F→G$ is an isomorphism iff each induced map from $\phi$ on stalks is an isomorphism? Hartshorne’s proof assumes $C(A,B)^x=C(A,B)\cap Set(A,B)^x$ , where $C(A,B)^x$ denotes the isomorphisms from $A$ to $B$ . What if we don’t even assume $C$ is concrete? Thank you!","['algebraic-geometry', 'sheaf-theory']"
3690297,Is there a closed form for $\int_0^1 \binom{1}{x}\frac{\log^2(1-x)}{x}\ \mathrm{d}x$?,"Do we know if there is a closed form for $$
I :=\int_0^1 \binom{1}{x}\frac{\log^2(1-x)}{x}\ \mathrm{d}x\mathrm{?}
$$ Wolfram alpha gives an approximation of $2.66989$ which may be equivalent to: $$10\sqrt{\frac{2\pi}{77\log(\pi)}}.$$ As stated by @Mariusz Iwaniuk, in the comments, we have the equivalent representation of $$I\equiv \int_0^1 \frac{\sin(\pi x)\log^2(1-x)}{\pi x^2(1-x)}.$$ Another question, presumably simpler, could be $$\int_0^1 \binom{1}{x}\frac{\log(1-x)}{x} \equiv -\int_0^1 \binom{1}{x}\frac{\mathrm{Li}_1(x)}{x}.$$ I believe I can find a closed form for the latter; if I do, I will edit the post. In general, I am curious as to if we may be able to somehow employ Ramanujan's Beta integral or any of the other Beta integrals. Another approach may be the series representation for $\binom{1}{x}$ . Thanks!","['definite-integrals', 'real-analysis', 'polylogarithm', 'calculus', 'binomial-coefficients']"
3690303,Explain a counter-example showing that Kolmogorov-Chentsov Theorem cannot be relaxed.,"The $1-$ dimensional Kolmogorov-Chentsov Theorem is as follows: Suppose $(X_{t})_{t\in [0,1]}$ that satisfies $$\mathbb{E}|X_{t}-X_{s}|^{\alpha}\leq C|t-s|^{1+\beta},\ \text{for all}\ s,t\in[0,1]$$ and for some $\alpha,\beta, C>0$ . Then there exists a continuous modification of $X$ . In the note by Amir Dembo, he gives a proof showing that the coefficients cannot be relaxed. That is, $\beta=0$ cannot work. The example is as follows: Consider the stochastic process $X_{t}(\omega)=\mathbb{1}_{\{\omega>t\}}$ for $t\in [0,1]$ and the uniform probability measure on $\Omega=(0,1]$ . Then $$\mathbb{E}|X_{t}-X_{s}|=U\Big((s,t]\Big)=|t-s|\ \text{for all}\ 0<t<s\leq 1.$$ Thus, $\{X_{t}, t\in[0,1]\}$ satisfies the ``insufficient inequality'' with $C=1$ , $\beta=0$ and $\alpha=1$ . However, if $\{\tilde{X}_{t}\}$ is a modification of $\{X_{t}\}$ then a.s. $\tilde{X}_{t}(\omega)=X_{t}(\omega)$ at all $t\in(0,1]\cap\mathbb{Q}$ , from which it follows that $s\mapsto \tilde{X}_{s}(\omega)$ is discontinuous at $s=\omega$ . I don't understand the last paragraph of his argument. Why does the existence of the continuous modification $\tilde{X}_{t}$ implies $\tilde{X}_{t}=X_{t}$ for $t\in [0,1]\cap\mathbb{Q}$ and Why does it follow that $\tilde{X}_{s}(\omega)$ is discontinuous at $s=\omega$ ? I am really confused... Thank you!","['proof-explanation', 'analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
3690348,"I wrote a proof of L'Hospital's rule, am I right?","Suppose $f$ and $g$ are differentiable and $g'(x)\neq 0$ on an open interval $I$ that contains $a$ . Suppose that $$\lim_{x\to a}f(x)=L=\lim_{x\to a}g(x)\quad L=0\,or\,\infty\,or-\infty$$ ,
then $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}$$ I proved the case when $L=0$ ,and I wrote a proof about the case when $L=\infty$ ,I found it is different from the proof on the internet, so I don't know whether it is right. My proof: $$\lim_{x\to a}f(x)=\infty=\lim_{x\to a}g(x)$$ $$\lim_{x\to a}\frac{1}{f(x)}=0=\lim_{x\to a}\frac{1}{g(x)}$$ $$\lim_{x\to a}\frac{\frac{1}{g(x)}}{\frac{1}{f(x)}}=\lim_{x\to a}\frac{[\frac{1}{g(x)}]'}{[\frac{1}{f(x)}]'}$$ . $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{g'(x)[f(x)]^2}{f'(x)[(g(x)]^2}$$ $$\lim_{x\to a}\frac{f(x)}{g(x)}[1-\frac{f(x)g'(x)}{g(x)f'(x)}]=0$$ . I think L'Hospital assumes that $\lim_{x\to a}\frac{f(x)}{g(x)}$ exist, or is $\infty$ , or $-\infty$ , and so $$\lim_{x\to a}\frac{f(x)g'(x)}{g(x)f'(x)}=1$$ $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}$$ . If it is wrong, please tell me why. Thank you very much!",['calculus']
3690355,Subalgebra of $M_n(\mathbb{C})$ generated by two elements (along with unity),"Let $M_n(\mathbb{C})$ denote the algebra of $n\times n$ matrices over the field of complex numbers $\mathbb{C}$ . Let $h_1,h_2\in M_n(\mathbb{C})$ be two Hermitian matrices. Suppose that $h_1,h_2$ are ""relatively irreducible"", i.e. they don't have common invariant proper subspaces. What is the dimension of the subalgebra $\mathcal{A}$ generated by $e,h_1,h_2$ , where $e$ is the $n\times n$ identity matrix? Especially, what is the condition on $h_1,h_2$ such that $\mathcal{A}=M_n(\mathbb{C})$ ? [Note: it is easy to see that if $h_1,h_2$ can be simultaneously block diagonalized, then $e,h_1,h_2$ cannot generate $M_n(C)$ .] For example when $n=2$ , the Pauli matrices $\sigma^z,\sigma^x$ are enough to generate $M_2(\mathbb{C})$ . When $n=3$ , it is easy to check that the Gellmann matrices $$\lambda_1 = \begin{pmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix},~~\lambda_4 = \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix},$$ along with identity matrix $e$ could generate $M_3(\mathbb{C})$ . I'm curious about what can be said in general.","['matrices', 'abstract-algebra', 'linear-algebra', 'noncommutative-algebra']"
3690358,Assume the bipartition of $G$ is $V=A\uplus B$. Prove that $|A|=|B|$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Let $G$ be a bipartite graph whose every vertex has the same degree $d$ . Assume that the bipartition of $G$ is $V=A\uplus B$ .  Prove that $|A|=|B|$ . I'm not great at figuring out how to start proofs, would appreciate some guidance. EDIT: This is the first part of a 4-part question.  The question is set up as follows: An edge coloring of a graph $G$ with $n$ colors is a function $f:E \to \{1,...,n\}$ .  We say that an edge coloring is proper if any two edges which share a common endpoint are different colors.  Just as with vertex colorings, we can define the edge chromatic number $\chi'(G)$ to be the smallest number of colors needed to properly color the edges of $G$ . Now let $G$ be a bipartite graph whose every vertex has the same degree $d$ .  In this problem we will prove that $\chi'(G) = d$ . That is all the information given to solve the title question. I'm thinking maybe its something to do with pigeonhole principle but I have no clue really where to start.","['graph-theory', 'proof-writing', 'bipartite-graphs', 'discrete-mathematics']"
3690365,Infinite Cartesian Product: Understanding [duplicate],"This question already has answers here : How to read infinite Cartesian product definition (2 answers) Closed 4 years ago . I'm having a bit of trouble understanding the definition of the infinite cartesian product, particularly with the intuition behind it. According to my textbook, Enderton's Elements of Set Theory , the infinite cartesian product takes the cartesian product of each set $X_i$ for $i \in I$ . This idea makes sense to me, but the definition of $$\prod_{i \in I} X_i = \left\{\left. f: I \to \bigcup_{i \in I} X_i\ \right|\ (\forall i)(f(i) \in X_i)\right\}$$ does not. For example, if I make a function $X = \{(1,\{2\}), (2,\{3\}), (3, \{4\})\}$ where $X_1 = \{2\}$ , $X_2  = \{3\}$ , and $X_3  = \{4\}$ if I take the cartesian product of them, don't I get $(2,3,4)$ ? How is this a function and how does it relate to the definition? I am very aware that my misunderstanding most likely comes from an inadequate knowledge of cartesian products, and that my example may be incorrect. If so, please let me know what misconceptions I may have so that I can grow and learn!","['elementary-set-theory', 'definition']"
3690519,Homological class of a singular variety,"Suppose $X$ is a compact complex manifold and $V\subseteq X$ is an irreducible analytic variety. Since $V$ may not be smooth, how does it make sense of saying $[V]$ as a homological class? In the case when $V$ is a smooth $k$ -dimensional submanifold, its Poincare dual $\eta_V\in H^{n-k}(X)$ can be thought of: The unique class in $H^{n-k}(X)$ such that $\int_V\omega=\int_X \omega\wedge\eta_V$ for all $\omega\in H^k(X)$ The unique class in $H^{n-k}(X)$ such that $\int_W\eta_V=\text{Int}(V,W)$ for all $(n-k)$ -dimensional submanifold, where Int is the intersection number. How to interpret those in the case when $V$ is singular? Is it true that the singular point of $V$ is measure zero so the integration makes sense?","['algebraic-geometry', 'algebraic-topology']"
3690525,"Does $X \sim Y$ and $P[D|X]=P[D|Y]$ imply $(D,X) \sim (D,Y)$?","Given random variables $D: \Omega \to \{-1,1\}$ , $X: \Omega \to \{1,\dots, n\}$ and $Y: \Omega \to \{1,\dots, n\}$ , such that $$X \sim Y$$ and $$P[D| X] = P[D|Y]$$ may one conclude that $$(D,X) \sim (D,Y)?$$ One may be tempted to use the definition of conditional probability and argue that $$P[D=i, X=k ] = \int_{\{X=k\}}1_{\{D=i\}} dP = \int_{\{Y=k\}}1_{\{D=i\}} dP = P[D=i, Y=k ] ,$$ but since $P[X= k ] = P[Y= k ] $ does not necessarily imply that $\{X= k \} = \{Y=k \}$ , I don't necessarily think this might be true... Is it still possible to show that $(D,X) \sim (D,Y)$ ?","['conditional-expectation', 'conditional-probability', 'probability-theory']"
3690529,Spivak Calculus Chapter 1 Problem 5 (ii),Prove : If $a < b$ then $-b < -a$ My proof : $a + (-b) < b + (-b) $ $a - b < 0$ $a - b + (-a) < 0 + (-a)$ $a + (-a) -b < -a$ $-b < -a$ Is my proof correct?,"['algebra-precalculus', 'solution-verification', 'inequality']"
3690530,Classification of Critical Points of Second Order Differential Equation,"I am given a single second order differential equation: $\ddot{x}-x^3 - 2x^2\dot{x} + 1 = 0$ .. and am asked to classify the critical points as stable, unstable or saddle points. Finding the critical points is an easy task for first order differential equation(s), both single equation and system of equations. However, I have never done so for second order, and higher, equations. I have an idea on how to solve it but not sure the approach is correct. Am I correct in saying I need to split the single differential equation into two differential equations and rename the relevant terms? Doing so on the above equation gives: $\dot{x_1} = x_2$ $\dot{x_2}-x_1^3-2x_1^2x_2+1 = 0$ Thereafter, I follow the same process as starting off with a set of two first order differential equations. That is, set $\dot{x}_1$ and $\dot{x}_2$ to $0$ , and solve for the intersection of $x_1$ and $x_2$ to find the fixed points. The nature of the fixed points would then be determined by calculating the Trace and Determinant of the Jacobian at the specific fixed points. Is my thinking on the right track?","['systems-of-equations', 'ordinary-differential-equations']"
3690554,The Axiom of Choice: Proof Validity,"Synopsis In Enderton's Element's of Set Theory , he introduces several forms of the Axiom of Choice. Currently, I've gotten through the first and second forms. Mainly: (1) For any relation $R$ , there is a function $H \subseteq R$ with dom $H$ = dom $R$ (2) For any set $I$ and any function $H$ with domain $I$ , if $H(i) \neq \varnothing$ for all $i \in I$ , then $\prod_{i \in I} H_i \neq \varnothing$ . After introducing the second form, he asks us to show that the two forms are equivalent. I would greatly appreciate it if you would check the validity of my attempt, and also perhaps give me an explanation for how you personally understand and think about the axiom of choice. I have a vague notion right now in my head, and I think an alternative explanation of the same concept my give me a deeper understanding. Now, onto the proof. Proof Suppose the first form is true. Define a relation $R$ as follows: $$R = I \times \bigcup_{i \in I} H(i).$$ By the first form of the axiom of choice, we can construct a function $f \subseteq R$ with dom $f$ = dom $R$ $= I$ . This means that $f(i) = R(i)$ for all $i \in I$ and by definition of $R$ , $f(i) \in H(i)$ . Hence, $f \in \prod_{i \in I} H_i$ . Now for the converse, suppose the second form is true. Then  for a relation $R$ , let $I =$ dom $R$ . Define a function $H: I \rightarrow \mathscr{P}(\text{ran } R)$ where $H(i) := \{x \in \text{ran } R \mid iRx \}$ . By the axiom of choice, $\prod_{i \in I} H_i \neq \varnothing$ , so there exists a function $f$ with $\text{dom }f = I$ such that $(\forall i \in I) f(i) \in H(i)$ . That means $(\forall i \in I) iRf(i)$ . So $f \in R$ and $\text{dom } f = \text{dom } R$ . Thus, the two forms are equivalent. Q.E.D. Thank you so much for your time, and I'll diligently pay attention to any comments or takes on how you understand the Axiom of Choice and/or how I can better my proof-writing abilities.","['elementary-set-theory', 'proof-writing', 'solution-verification']"
3690609,How to translate this statement into a mathematical one(using appropriate quantifiers)?,"The statement I'd like to translate into a mathematical one is "" Every American has a dream "". Let $A$ and $D$ denote the set of all Americans and the set of all dreams, respectively, and $P(a,d)$ denote the proposition ""American $a$ has a dream $d$ "". The mathematically equivalent statement I've deduced is $$\forall a\in A.\exists d\in D.P(a, d)$$ However, I suspect the above statement implies that for every American there exists a common dream $d$ such that $P(a,d)$ holds true. I would like to know how to rectify this error(if there is one).","['solution-verification', 'predicate-logic', 'discrete-mathematics']"
3690751,Maximum number of possible intersections between tangent line and function $x^3$,I'm struggling with a problem but can't find a way how to solve it: Calculate maximum number of possible intersections between tangent line of function $f(x) = x^3$ and function $f(x) = x^3$ where $x \in \mathbb{R}$ . I know I should use derivatives but I don't have a clue how.,"['functions', 'derivatives', 'real-analysis']"
3690754,A set has measure zero iff for every $\epsilon>0$ there is a countable covering of open rectangles such that $ \sum_{i=1}^\infty v(Q_i)<\epsilon $,"What shown below is a reference from ""Analysis on manifolds"" by James R. Munkres. Definition Let $A$ a subset of $\Bbb{R}^n$ . We say $A$ has measure zero in $\Bbb{R}^n$ if for every $\epsilon>0$ , there is a covering $Q_1,Q_2,...$ of $A$ by countably many rectangles such that $$
\sum_{i=1}^\infty v(Q_i)<\epsilon
$$ Theorem A set $A$ has measure zero in $\Bbb{R}^n$ if and only if for every $\epsilon>0$ there is a countable covering of $A$ by open rectangles $\overset{°}Q_1,\overset{°}Q_2,...$ such that $$
\sum_{i=1}^\infty v(Q_i)<\epsilon 
$$ Proof . If the open rectangles $\overset{°}Q_1,\overset{°}Q_2,...$ cover $A$ , then so the rectangles $Q_1,Q_2,...$ . Thus the given condition implies that $A$ has measure zero. Conversely, suppose $A$ has measure zero. Cover $A$ by rectangles $Q'_1,Q'_2,...,$ of total volume $\frac{\epsilon}2$ . For each $i$ , chose a rectangle $Q_i$ such that $$
1.\quad Q'_i\subset\overset{°}Q_i\text{ and }v(Q_i)\le 2v(Q'_i)
$$ (This we can do because $v(Q)$ is a continuous function of the end points of the component intervals of $Q$ ). Then the open rectangles $\overset{°}Q_1,\overset{°}Q_2,...$ cover $A$ and $\sum v(Q_i)<\epsilon$ . So I don't understand why it is possible to make the rectangles $Q_i$ such that they respect the condition $1$ and so I ask to well explain this: naturally I don't understand Munkres explanation and so you can or to explain better what Munkres said or to show another explanation. So could someone help me, please?","['integration', 'calculus', 'measure-theory']"
3690780,Can we say that: $|g|<M\text{ a.e} $,"Let $(E,\mathcal{A},\mu)$ be a finite measure space and $$
\mathcal{L}^1=\left\{f:E\to \mathbb{R}: \int_{E}{|f(t)|d\mu(t)}<\infty\right\}
$$ Let $\{f_n\}\subset \mathcal{L}^1$ , such that: $$
\forall t\in E :|f_n(t)|\leq M\qquad (1)
$$ $$
\exists g\in\mathcal{L}^1, \text{such that } f_n\underset{n}{\to}g\text{ weakly in }\mathcal{L}^1\qquad (2)
$$ Can we say that: $$
|g|<M\text{ a.e}
$$","['integration', 'measure-theory']"
3690799,Pseudoinverse of block diagonal matrix,"Suppose I have some block diagonal matrix $A$ , defined as: $A = \begin{bmatrix} A_1 & 0 & ... & 0 \\ 0 & A_2 & ... & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & A_n \end{bmatrix}$ Where $\{A_i\}_{i \in \{1,...n\}}$ are the blocks of $A$ . Is it true that the Pseudo-inverse of $A$ , $A^+$ , is given by: $A^+ = \begin{bmatrix} A_1^+ & 0 & ... & 0 \\ 0 & A_2^+ & ... & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & A_n^+ \end{bmatrix}$ If so, why?/why not?","['matrices', 'matrix-rank', 'matrix-equations', 'pseudoinverse']"
3690901,About non-normal nilpotent subgroups,"Suppose that all non-normal abelian subgroups of a finite group $G$ are cyclic. What can I say about non-normal nilpotent subgroups of $G$ ?
Is it true that such supgroups are cyclic? I appreciate your help.","['nilpotent-groups', 'group-theory', 'finite-groups']"
3690902,"Question about ""G.A.G.A"" theorem.","In the book Principles of Algebraic Geometry, there is a theorem: Every meromorphic function on an algebraic variety $V\subset \mathbb{P}^n$ is rational. And the proof of this assertion is in two stages: first, we express $V$ as a branched cover of a linear subspace $\mathbb{P}^k\subset \mathbb{P}^n$ by projection $\pi$ , and deduce from this representation that the pullback $\pi^{*}K(\mathbb{P}^k)$ to $V$ of the field of rational functions on $\mathbb{P}^k$ has index at most $d= \operatorname{deg}(V)$ in the field of meromorphic functions $\mathfrak{M}(V)$ ; we then show that the field of rational functions $K(V)$ on $V$ is an extension of degree at least $d$ over $\pi^{*}K(\mathbb{P}^k)$ . I don't know why we can conclude the assertion? Since every meromorphic function $\varphi$ satisfies a polynomial relation of degree $d$ over fields of rational functions $\pi^{*}K(\mathbb{P}^k)$ $\varphi^d-\pi^{*}a_1 \varphi^{d-1}+\pi^{*}a_2 \varphi^{d-2}+...+(-1)^da_d \equiv0$ where $a_i$ are rational functions, we could say $\varphi$ is rational? Hope someone could help. Thanks!","['complex-geometry', 'algebraic-geometry']"
3690966,Help to inductively define finite trees,"In my assignment, I have an in-depth question regarding finite trees.
We are presented with the trees in list form, and an empty list is symbolized as $\emptyset$ . Example: A symmetrical tree with two branches (read: 1 ROOT node with 2 children) is presented this way: $(\emptyset \  \emptyset)$ (In this example, these two children are also LEAF nodes). The task is to inductively define a set T of finite trees with roots:
The ROOT node is the node you can envision at the bottom of the tree graphic, as a root in real life.
The LEAF node is the one at the top, and there can be multiple LEAF nodes. If the finite tree only consists of the empty list, the LEAF node and the ROOT node is the same node. If a node doesn't have a child, it is a LEAF node. In my inductive definition of the set T, I have written the base case as such (loosely translated): The base case states that the assumption holds for the empty list, represented as $\emptyset$ . In the base case, $\emptyset$ is thus both the ROOT- and the LEAF node. This node has no children. Another important note is this: the assignment specifies that trees are non-commutative, meaning $((\emptyset) \ \emptyset)$ is different from $(\emptyset \ (\emptyset))$ . Now in the induction step I struggle. How can I make this ""not"" infinite? I have tried several times to define this step (the induction step) but I can't wrap my head around this task. Worth to mention that I'm not particularily talented within this type of operation. Because I believe the nature of the question can be confusing (it already is for me), here's some additional details for context: It's a Norwegian course and the main chapter of focus here is called ""Closure and inductively defined sets"". We are later tasked to give recursive definitions of functions that are connected to the above presented assignment, but that's not the question I present in this post. EDIT: Made some changes to hopefully clarify some points more clearly.","['logic', 'recursion', 'definition', 'discrete-mathematics', 'induction']"
3690994,Left hand derivative and Right hand derivative of the inverse of a non-differentiable function,"Let $f(x)$ be an injective function with domain $[a,b]$ and range $[c,d]$ . If $\alpha$ is a point in $(a,b)$ such that $f$ has left hand derivative $l$ and right hand derivative $r$ at $x=\alpha$ with both $l$ and $r$ being non-zero different and negative , then prove that the left hand derivative and the right hand derivative of $f^{-1}(x)$ at $x=f(\alpha)$ are $\frac{1}{r}$ and $\frac{1}{l}$ respectively. My Attempt: If $g(x)$ be the inverse of $f((x)$ then $f(g(x))=x$ which gives $g'(x)=\frac{1}{f'(g(x))}$ . So if $x=f(\alpha)$ then $g'(f(\alpha))=\frac{1}{f'(g(f(\alpha)))}=\frac{1}{f'(\alpha)}$ . But now how do I get the left hand derivative and the right hand derivative. I could frame an example $f(x)=\left\{
\begin{array}{ll}
      \frac{1}{x} & 0<x<2 \\      
      \frac{2}{x^2} & x\geq 2 \\
      \end{array} 
\right.$ Left hand derivative at $x=2$ equals $\frac{-1}{4}=l$ Right hand derivative at $x=2$ equals $\frac{-1}{2}=r$ $f^{-1}(x)=\left\{
\begin{array}{ll}
      \\\sqrt {\frac{2}{x}} & 0<x\leq \frac{1}{2} \\      
      \frac{1}{x} & x> \frac{1}{2} \\
      \end{array} 
\right.$ Left hand derivative of $f^{-1}(x)$ at $x=\frac{1}{2}$ equals $-2=\frac{1}{r}$ Right hand derivative of $f^{-1}(x)$ at $x=\frac{1}{2}$ equals $-4=\frac{1}{l}$ But I am not able to get a proper method.","['calculus', 'inverse-function', 'derivatives', 'real-analysis']"
3690995,System of differential equations in RC,"I am struggling with c) part these differential equations, I am not sure i obtained it correctly or not, but i solved in matlab also, and i dont think it is correct answer. My work: $$-50q_1'-100000q_1+100(I-q_1')=0$$ $$-50q_2'-100000q_2+150(I-q_2')=0$$ $$-50q_1'-100000q_1-50q_2'-100000q_2+75=0$$ Can you check my equations also? Here $I$ is the function of time $t$ If R1 and R3 are in parallel   and R2 and R3 in parallel always for any time t, then this question is a piece of cake for me. Can u confirm are they in parallel ? ? I solved it using laplace and both MATLAB now, Thanks if anyone else has tried","['physics', 'calculus', 'derivatives', 'ordinary-differential-equations']"
3691021,The action of vectors on one forms,"I have searched the web for an answer to this question but couldn't quite find what I was looking for. Given a Manifold $M$ and a choice of local coordinates $\lbrace{x}^{i}\rbrace$ one can give the expression for a tangent vector $V \in {T}_{p}M$ in these local coordinates as $$V={V}^{i} {\partial}_{i}$$ Now a vector can be defined as a linear operator from smooth functions on $M$ to the reals, i.e. ${C}^{\infty} \left( M \right) \rightarrow \mathbb{R}$ and this is a concept I am completely fine with, however my trouble comes when considering the action of Vectors on one forms. Take a one form in the same coordinate system $$\omega = {\omega}_{i} d{x}^{i}$$ where ${\omega}_{i} \in {C}^{\infty} \left( M \right)$ then what is the action of the vector on this one form? Is it defined? I know that from the usual construction of one forms as elements of the dual space of ${T}^{*}_{p}M$ allows us to think of one forms as $\mathbb{R}$ -linear maps ${T}_{p}M \rightarrow \mathbb{R}$ with the following relation between the dual basis elements $$ d{x}^{i} \left( {\partial}_{j} \right) = \frac{\partial {x}^{i}}{\partial {x}^{j}}= {{\delta}^{i}}_{j}$$ Where the action of the basis element on an arbitrary vector $$d{x}^{i} \left( V \right) = {V}^{j} \frac{\partial {x}^{i}}{\partial {x}^{j}}={V}^{i}$$ Which is just the directional derivative of the $i$ th coordinate function along $V$ and so this gives us our action $$ \omega \left( V \right) = {\omega}_{i} d{x}^{i} \left( V \right) = {\omega}_{i}{V}^{i}$$ Is there a well defined action for $V \left( \omega \right)$ ? If not is there a reason why this action is not well defined and thus can't be constructed? My apprehension is derived from the fact the basis elements of $T_{p}M$ are differential operators and naturally obey a Leibniz law. Thanks for reading and any responses will be greatly appreciated!","['vectors', 'differential-forms', 'differential-geometry']"
3691036,Improper integral with two variables,"I've got the following problem: Proof or disproof: there is a g $\in C(\mathbb{R^2})$ so that a $h(y) := \int_{-\infty}^{\infty}g(x,y)dx$ for every $y \in \mathbb{R}$ exists, but is not continuous. A hint suggests I should consider that for every $f \in C(\mathbb{R})$ that is improperly integrable and $y>0$ it applies that: $\int_{-\infty}^{\infty}yf(yx)dx = \int_{-\infty}^{\infty}f(x)dx$ I honestly don't quiet know how to even start solving this problem. I think I don't have the right properties for improper integrals with two variables in mind to work with them. Does anyone have an Idea?","['continuity', 'multivariable-calculus', 'improper-integrals']"
3691045,$M<K\rtimes H$ is a semidirect product?,"Let $H,K$ be two finite groups, $K$ abelian, and let $M$ be a subgroup of $K\rtimes H$ . Consider the projection $\pi:K\rtimes H \rightarrow H$ on the Second factor. Let us suppose that $\pi(M)=H$ . Is true in this case that $M$ is isomorphic to $K'\rtimes H$ for a suitable $K' <K$ ? My claim is that it is true and that it is possible to construct an injective group omomorphism $j:Ker \pi_{|M} \rtimes H \rightarrow M$ and then conclude using some classical arguments about group extensions. Any help (or remark about the incompleteness of the question) is well accepted.
Thank you.","['group-extensions', 'semidirect-product', 'group-theory', 'abstract-algebra']"
3691062,Difference between a number and a set with one number,"I am studying Analysis in Tao's textbook and he mentions the following: The set $\{3, \{3, 4\}, 4\}$ is a set of three distinct elements, one of which happens to itself be a set of two elements. However, not all objects are sets; for instance, we typically do not consider a natural number such as $3$ to be a set. (The more accurate statement is that natural numbers can be the cardinalities of sets, rather than necessarily being sets themselves) but my question is wouldn't $\{3\}$ be a set of cardinality one? Is this just a matter of notation (if we include the curly braces then we consider it a set and if not then it's just a natural number)?","['elementary-set-theory', 'real-analysis']"
3691106,Difficulty interpreting high order derivatives in $\mathbb{R}^n$,"If $f:U\subseteq\mathbb{R}^m\to \mathbb{R}^n$ is differentiable function then its derivative $$
f':U\to M_{n\times m}(\mathbb{R})\simeq\mathcal{L}(\mathbb{R}^m;\mathbb{R}^n)
$$ can be seen, for each $x\in U$ , as a linear transformation $f'(x):\mathbb{R}^m\to\mathbb{R}^n$ . If $f$ is of class $\mathcal{C}^2$ then its second order derivative is a function $$
f'':U\to\mathcal{L}(\mathbb{R}^m\times\mathbb{R}^m;\mathbb{R}^n)
$$ that carries each $x\in U$ into a bilinear transformation $f''(x):\mathbb{R}^m\times\mathbb{R}^m\to\mathbb{R}^n$ and inductively if $f$ is of class $\mathcal{C}^k$ its $k$ th derivative is a function $$
f^{(k)}:U\to\mathcal{L}(\mathbb{R}^m\times\cdots\times\mathbb{R}^m;\mathbb{R}^n).
$$ Maybe I'm lacking some linear algebra background, but using the isomorphism $\mathcal{L}_2(\mathbb{R}^m\times\mathbb{R}^m;\mathbb{R}^n)\simeq\mathcal{L}(\mathbb{R}^m;\mathcal{L}(\mathbb{R}^m;\mathbb{R}^n))$ and its correspondent in the $k$ -linear case I can see that the $k$ th derivative is a $k$ -linear map. But I don't see how to relate (in the case of the second derivative to simplify) the partial second derivatives $\frac{\partial f_i}{\partial x_j\partial x_k}(x)$ to the matrix of $f''(x)$ as a bilinear map. In the case of a bilinear form it would be a $m\times m$ matrix but it's not the case. Can someone please explain me better? P.S.: I have seen this question but I didn't understand his notation in the last part of the answer (and the first part which answers that question I'm ok with).","['multivariable-calculus', 'multilinear-algebra', 'linear-algebra', 'real-analysis']"
3691137,Using integral comparison to estimate how fast the partial sums of $\sum_{k=1}^N\frac1{k^3}$ converge,"Consider the following infinite sum and its partial sums. $$ S = \sum_{k=1}^{\infty } \frac{1}{k^3} $$ $$ S_N = \sum _{k=1}^{N} \frac{1}{k^3} $$ Use integral comparison to estimate how fast $S_N$ tends to $S$ . That is, show that $0<S−S_N<CN^{−q}$ giving the best values you can for the constants $C$ and $q$ . My work is as follows: The lower Riemann sum is given by $\sum_{k=2}^{\infty} \frac{1}{k^3}$ . The upper Riemann sum is $\sum_{k=1}^{\infty} \frac{1}{k^3}$ . Since $ S = \sum_{k=1}^{\infty } \frac{1}{k^3} $ , the the lower Riemman Sum is $S-1 = \sum_{k=2}^{\infty} \frac{1}{k^3}$ . Thus, $S-1 < \int_{1}^{\infty} \frac{dx}{x^3} < S$ . We could evaluate the improper integral if it converges, however it shows to be inconclusive. It happens to be equal to $\zeta(3)$ , however that is out of scope of the class I am taking and it is not accepted. I don't know where to go from there. Also, what can I do with $ S_N = \sum _{k=1}^{N} \frac{1}{k^3} $ , I am stuck on how to proceed.","['integration', 'limits', 'calculus', 'summation']"
3691195,"$x_{n+13}=x_{n+4}+2x_{n}$, $x_{143}=...$","Given $x_1=x_2=\dotsc=x_{12}=0$ , $x_{13}=2$ , and $x_{n+13}=x_{n+4}+2x_{n}$ for every natural number $n$ . Find $x_{143}$ . I tried to find some pattern for some of the first term but did not notice any pattern that did not require some brute force to calculate $x_{143}$ . So I decided to read and understand the solution : Consider the equation $9p+13q=143$ where $p,q$ are nonnegative integers. Using euclid's algorithm, we have $(p_1,q_1)=(13,2)$ and $(p_2,q_2)=(0,11)$ . And after that, the solution claims that for $n=9p+13q$ where $p,q$ are nonnegative integers, we have $$x_n=\sum\limits_{i=1}^{k}2^{q_i}\cdot\binom{p_i+q_i-1}{p_i}$$ where $k$ is the number of solution for $(p,q)$ and $(p_i,q_i)$ is the $i$ -th solution for $n=9p+13q$ for positive integer $i\leq k$ . I do not understand where the general form comes from. I tried using induction method to prove the general form, but did not done well. One of the things that prevent my induction proofing is that if $n+13$ has $k$ solution for $(p,q)$ , it does not assure that $n+4$ and $n$ has exactly $k$ solution for $(p,q)$ . Comments and answer is appreciated.","['generating-functions', 'combinatorics', 'recursion', 'sequences-and-series']"
3691253,Set Theoretic naming for a union property,"I am introducing a new operation for countable sets, say $\mathcal{X}(\cdot)$ is the operation. I have proved that for two sets $S_1, S_2$ we have: $$\mathcal
X(S_1 \cup S_1) = \mathcal{X}(S_1) \cup \mathcal{X}(S_2).$$ How can I name this property in set-theory language?","['elementary-set-theory', 'terminology']"
3691310,Find $x(t)$ given $\frac{dx}{dt}$ and $\frac{dy}{dt}$,"For $0 \leq t \leq 1$ , a particle is moving along a curve so that its position at time $t$ is $(x(t),y(t))$ . At time $t=0$ , the particle is at position $(0,0)$ . We are given that $$\frac{dx}{dt} = \frac{t}{\sqrt {1+t^2}}$$ $$\frac{dy}{dt} = \sqrt {\frac{1-t^2}{1+t^2}}$$ Find $x(t)$ Here is my work: $$x(t)=\int \frac{dx}{dt}= \int \frac{t}{\sqrt{1+t^2}}dt$$ Substituting: $$u=t^2+1 \implies du= 2t dt$$ We get; $$x(t) = \int \frac{t}{\sqrt{1+t^2}}dt = \sqrt{t^2+1}+C$$ Since it is asking for $x(t)$ and not $y(t)$ I believe I'm ok","['integration', 'indefinite-integrals', 'calculus', 'derivatives']"
3691332,What is the size of each side of the square?,"The diagram shows 12 small circles of radius 1 and a large circle, inside a square. Each side of the square is a tangent to the large circle and four of the small circles. Each small circle touches two other circles. What is the length of each side of the square? The answer is 18 CONTEXT: This question came up in a Team Maths Challenge I did back in November. No one on our team knew how to do it and we ended up guessing the answer (please understand that time was scarce and we did several other questions without guessing!) I just remembered this question and thought I'd have a go but I am still struggling with it. There are no worked solutions online (only the answer) so I reaching out to this website as a final resort. Any help would be greatly appreciated. Thank you!","['circles', 'geometry', 'sangaku', 'recreational-mathematics', 'problem-solving']"
3691334,Let $T:\textbf{R}^{n}\to\textbf{R}^{m}$ be a linear transformation. Show that there exists a number $M > 0$ such that $\|Tx\|\leq M\|x\|$.,"Let $T:\textbf{R}^{n}\to\textbf{R}^{m}$ be a linear transformation. Show that there exists a number $M > 0$ such that $\|Tx\|\leq M\|x\|$ . Conclude in particular that every linear transformation from $\textbf{R}^{n}$ to $\textbf{R}^{m}$ is continuous. My solution Let $[T]_{\mathcal{B}}^{\mathcal{B}'} = [T(e_{1})^{T},T(e_{2})^{T},\ldots,T(e_{n})^{T}]$ , where $\mathcal{B} = \{e_{1},e_{2},\ldots,e_{n}\}$ and $\mathcal{B}' = \{f_{1},f_{2},\ldots,f_{m}\}$ . Thus, according to the triangle inequality as well as the Cauchy-Schwarz inequality, we have that \begin{align*}
\|Tx\| & = \|x_{1}T(e_{1})^{T} + x_{2}T(e_{2})^{T} + \ldots + x_{n}T(e_{n})^{T}\|\\\\
& \leq|x_{1}|\|T(e_{1})^{T}\| + |x_{2}|\|T(e_{2})^{T}\| + \ldots + |x_{n}|\|T(e_{n})^{T}\|\\\\
& = \langle(|x_{1}|,|x_{2}|,\ldots,|x_{n}|),(\|T(e_{1})^{T}\|,\|T(e_{2})^{t}\|,\ldots,\|T(e_{n})^{T}\|)\rangle\\\\
& \leq \sqrt{|x_{1}|^{2} + |x_{2}|^{2} + \ldots + |x_{n}|^{2}}\sqrt{\|T(e_{1})^{T}\|^{2} + \|T(e_{2})^{T}\|^{2} + \ldots + \|T(e_{n})^{T}\|^{2}}
= M\|x\|
\end{align*} where $M = \sqrt{\|T(e_{1})^{T}\|^{2} + \|T(e_{2})^{T}\|^{2} + \ldots + \|T(e_{n})^{T}\|^{2}}$ and we assume that $T\neq 0$ . If $T = 0$ , then any $M + 1$ does the job. Consequently, $T$ is continuous because it is Lipschitz. I would like to know if I am reasoning correctly. Any other solution is equally welcome.","['cauchy-schwarz-inequality', 'solution-verification', 'linear-algebra']"
3691362,Monotonicity of the Lebesgue Integral,"I am working through Baby Rudin, and encountered the following remark: If $f$ and $g\in\mathcal{L}(\mu)$ on $E$ , and if $f(x)\leq g(x)$ for $x\in E$ , then $$\int_{E}fd\mu\leq \int_E gd\mu.$$ Given a measurable function $f$ , $\int_{E}fd\mu$ is always defined, but $f$ only belongs to $\mathcal{L}(\mu)$ when $$\Bigg|\int_{E}fd\mu\Bigg|<\infty.$$ (Edit: as harfe pointed out, it should be $\int_{E}|f|d\mu$ instead). My question is, does the remark remain true if we only assume $f$ and $g$ to be measurable instead of being in $\mathcal{L}(\mu)$ ? It seems to be true, because of two cases: If $\int_{E}fd\mu=-\infty$ , then there is nothing to show. If $\int_{E}fd\mu=\infty$ , then it must follow (?) that $\int_{E}gd\mu=\infty$ as well, so we get $\infty\leq \infty$ . Any help appreciated. Edit: I just realized that in my situation, I only need $f$ measurable and $g\in \mathcal{L}(\mu)$ , but comments at any level of generality are welcome.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
3691372,Cramer-Rao lower bound for exponential distribution,"Given a sample $X_1,\dots , X_n$ from a population $X\sim \operatorname {Exp} (\lambda )$ , I have to calculate Cramer-Rao bounds for the estimation of $\lambda$ and $\frac 1 \lambda$ ; I also must determine if there are estimators that limit. Now, we have that $\frac {\partial}{\partial \lambda} \operatorname {ln(\lambda e ^{-\lambda x}) }= \frac 1 \lambda -x$ , and so $\mathbb E[ (\frac 1 \lambda -x)^2]=\frac 1 {\lambda^2}$ , since it is the variance by definition. So in the case that we are estimating $\lambda$ , the Cramer-Rao bound is $\frac {\lambda^2} n$ , while in the other case the bound is $\frac {\lambda^2} n \cdot \frac 1 {\lambda^4}= \frac 1 {n\lambda^2}$ . It is clear that the sample mean is an estimator for $\frac 1 \lambda$ with variance exactly $\frac 1 {n\lambda^2}$ ; however I don't know how to proved in the case of estimating $\lambda $ . If a estimator $T_n $ for $\lambda $ equals Cramer-Rao bound, the we would have $\sum_i\frac {\partial}{\partial \lambda} \operatorname {ln(\lambda e ^{-\lambda x_i}) }=K (n,\lambda) ( T_n -\lambda )$ ; so that $\sum (\frac 1 \lambda -x_i)= K (n,\lambda) ( T_n -\lambda )$ . Since we want $\lambda $ and not $\frac 1 \lambda$ , the only way is to multiply everything for $\lambda^2$ ; however with this operation we can't obtain a estimator from the $x_i $ , because we will still have a dependence from $\lambda$ . I'm not sure about th last statement: did I actually prove that there are no estimators that equal Cramer-Rao bound for $\lambda$ ? Thanks in advance for your help",['statistics']
3691394,"Find $g(x)$ if $f(x)= \begin{cases} -1, & -2 \leq x \leq 0 \\ x-1, & 0 < x \leq 2 \end{cases}$ and $g(x) = |f(x)| + f(|x|)$","$$f:[-2,2] \rightarrow \Bbb R$$ $$\text {and }f(x)= \begin{cases} -1, & -2 \leq x \leq 0 \\ x-1, & 0 < x \leq 2 \end{cases}$$ And, let $g(x)$ be equal to $|f(x)|+f(|x|)$ We need to find the value of $g(x)$ (define it). I begin by finding the value of $|f(x)|$ first : $$|f(x)|=\begin{cases} |-1|, & -2 \leq x \leq 0 \\ |x-1|, & 0 < x \leq 2 \end{cases}=\begin{cases} 1, & -2 \leq x \leq 0 \\ |x-1|, & 0 < x \leq 2\end{cases}$$ Now, to determine what $|x-1|$ would be, we need to determine whether $(x-1)$ is positive or negative or zero. If $x-1 \geq 0$ , $|x-1| = x-1$ and if $x-1 < 0$ , then $|x-1| = -(x-1) = 1-x$ If $x-1 \geq 0$ , then $x \geq 1$ and if $x - 1 < 0$ , then $x < 1$ . We can now split the condition $0 < x \leq 2$ from the earlier definition of $|f(x)|$ as $0<x<1$ and $1 \leq x \leq 2$ , where $0<x<1 \implies |x-1| = 1-x$ and $1 \leq x \leq 2 \implies |x-1| = x-1$ So, $$ |f(x)| = \begin{cases} 1, & -2 \leq x \leq 0 \\ 1-x, & 0<x<1 \\ x-1, & 1 \leq x \leq 2 \end{cases}$$ Now, we need to define $f(|x|)$ . Here's what I do : $$f(|x|) = \begin{cases} -1, & -2 \leq x \leq 0 \\ |x|-1, & 0<|x| \leq 2 \end{cases}$$ Now, is the next step that I do here correct or even necessary? Now, $|x|$ can never be negative but can be zero when $x=0$ . So, the condition $-2 \leq |x| \leq 0$ can be replaced by $x = 0$ from which we obtain the following definition for $f(|x|)$ : $$f(|x|) = \begin{cases} -1, & x=0 \\ |x|-1, & 0 < x \leq 2 \end{cases} = \begin{cases} |x|-1, & 0 \leq |x| \leq 2 \end{cases}$$ I did the last one because we observe that $0$ is mapped to $-1$ and $|0|-1 = -1$ , so it will still be mapped to $-1$ if we put it in the second condition. So, basically, for any value of $x$ that is a part of domain of $f$ , $f(|x|) = |x|-1$ . Now, we add $|f(x)|$ and $f(|x|)$ to obtain $g(x)$ . $$g(x) = |f(x)| + f(|x|) = \begin{cases} |x|-1+1, & -2 \leq x \leq 0 \\ |x|-1+1-x, & 0 < x < 1 \\ |x|-1+x-1, & 1 \leq x \leq 2\end{cases}$$ . In the first condition, the value of $x$ is either negative or $0$ , so if it is negative, $|x| = -x$ and else, it is 0 which is also equal to $-0$ , which means that in the first condition, $|x|$ can be substituted by $-x$ . In the second and, the value of $x$ is always positive, so $|x| = x$ . So, we can substitute $|x|$ by $x$ in the second and third conditions and arrive at the definition of $g(x)$ that is : $$g(x) = \begin{cases} -x-1+1, & -2 \leq x \leq 0 \\ x-1+1-x, & 0 < x < 1 \\ x-1+x-1, & 1 \leq x \leq 2 \end{cases} = \begin{cases} -x, & -2 \leq x \leq 0 \\ 0, & 0 < x < 1 \\ 2x-2, & 1 \leq x \leq 2 \end{cases}$$ Now, I want to know if this process is correct and if there is some alternative, better approach to this problem. Sorry for the long post, I thought that showing my line of reasoning would make the question better. Thanks","['functions', 'solution-verification']"
3691414,Doesn't $0.\overline9=1$ lead to consequences like $a-0.\overline01=a$ and $2=1.\overline931415926$?,"I'm just starting to learn calculus, but this was the first idea presented: $$0.\overline9=1$$ This would mean that this is true: $$a-0.\overline01=a$$ When I thought about it, then I realised that if the above was true, then this other statement must be considered true: $$((((a-0.\overline01)-0.\overline01)-0.\overline01)...)=(a-0.\overline01)-0.\overline01=a-0.\overline01=a$$ This is confusing because it presents a lot of weird consequences. One such consequence would be a different behavior of open and closed intervals, since numbers that I previously considered to be different, are now considered to be the same. For instance, $2$ is now equal to $1.\overline931415926$ . I can't help but think that I am missing a crucial piece of information.","['calculus', 'algebra-precalculus', 'decimal-expansion']"
3691418,Can this complete metric space be a Banach space?,"Let $(S,d)$ be the space of all sequences in $\mathbb{R}$ with the metric $$d(\mathbf{x},\mathbf{y})=\sum_{i=1}^{\infty}\dfrac{1}{2^i}\dfrac{|\xi_i-\eta_i|}{1+|\xi_i-\eta_i|}$$ where $\mathbf{x}=(\xi_i)$ and $\mathbf{y}=(\eta_i)$ . This is a complete metric space, but the metric does not come from a norm. Therefore the topology of $S$ cannot be defined by a norm. My question is: does there exist any complete norm on the underlying vector space $S$ of all sequences in $\mathbb{R}$ ?","['banach-spaces', 'complete-spaces', 'functional-analysis', 'metric-spaces']"
3691540,Proving intersection of POsets is a POset - Reflexive,"I need to prove that the intersection of 2 POsets R and S is a POset. So we basically want to prove that if $R$ and $S$ are POsets then $R \cap S$ is reflexive, transitive and anti-symmetric. The problem is in the reflexive, I proved it like this: We need to prove that $\forall a \in A ~~~ <a,a> \in R \cap S$ Because $R$ and $S$ are POsets, if $R \cap S = \emptyset$ then the intersection is reflexive (empty-wise) Else, because $R$ and $S$ are POsets, if $<a,b> \in R \cap S$ then $<a,a> \in R$ and $<b,b> \in S$ because they are reflexive. And thus $<a,a> , <b,b> \in R \cap S$ and thus it is reflexive. In the comments the professor said:  Reflexive is not correct, you
  wrote that one thing is in R and other thing is in S so it is in the
  intersection (?)  (-4 pts) I don't know why I am wrong here, I would appreciate if you could point me to the mistake Note: this does not have to be 'well written', we are first year students..I just need to find the logical mistake, not the semantic. Thank you!","['order-theory', 'solution-verification', 'discrete-mathematics']"
3691546,How to find path for method of steepest descent,"We have integral: $$\int_0^1\exp\left(n\left(\frac{itz}{\sqrt{a(1-a)n}}+a\ln(z)+(1-a)\ln(1-z)\right)\right)dz=\int_0^1\exp(nf(z))dz,$$ where $0<a<1$ . We want to approximate this integral when $n\rightarrow\infty$ by method of steepest descent. Why deforming $[0,1]$ to the contour through saddle point has the following expansion: $$z=a+\frac{it\sqrt{a(1-a)}}{\sqrt{n}}+O\left(\frac{1}{n}\right)?$$ I found saddle point $z_0=a$ from $\text{Re}'(f(z))=0$ . I don't know what to do next? I can’t understand how they got it? I hope for your help! Thank you!","['integration', 'approximation', 'laplace-method', 'asymptotics', 'calculus']"
3691567,How to construct the matrix of a 2-D system of differential equations,"The task is to find and classify the fixed (equilibrium) points of the system : $$ \begin{cases} \dot x = (2x - y) (x - 2) \\ \dot y = xy - 2\end{cases}$$ Solving $$ \begin{cases} \dot x = (2x - y) (x - 2) = 0 \\ \dot y = xy - 2 = 0\end{cases}$$ gives three points $(1,2), (2,1)$ and $(-1,-2)$ . I have already classified the first two so no problems with those. However, for $(-1, -2)$ , if we shift the origin by changing the variables $$ \begin{cases} x = p - 1 \\ y = q - 2\end{cases}$$ we have $$ \begin{cases} \dot p = (2x - y) (x - 2) = (2(p - 1) - q + 2 )(p - 3) = -6p + 3q -pq + 2p^2\\ \dot q = xy - 2 =(p-1)(p-2) + 2 = -2p - q + pq\end{cases}$$ Now, to find the linear approximation around the origin, I know that the higher-order terms can be ignored. What about the $pq$ terms? If I ignore them too, then I'll have $$\begin{cases} \dot p \approx -6p + 3q\\ \dot q \approx -2p - q \end{cases} \implies \lambda_{1,2} = \{-3, -4\}$$ Which means the equilibrium point is a sink or stable non-degenerate node . Is that right?","['linear-approximation', 'systems-of-equations', 'approximation-theory', 'ordinary-differential-equations']"
3691609,Solve $(1-x^2)y^{\prime\prime} + xy^\prime-y=0$ using Frobenius method,"I'm stuck with this problem because the equation has 2 singular regular points $x =-1$ and $x=1$ , so how can I get the solution?",['ordinary-differential-equations']
3691615,"Let $F=\langle xy^2, 3z-xy^2, 4y-x^2y\rangle$ Find the maximum value of the line integral of $F$ over a simply closed curve C in the plane $x+y+z=1$","Let $F=(xy^2, 3z-xy^2, 4y-x^2y)$ . Find the maximum value of the line integral of F over a simply closed curve $C$ in the plane $x+y+z=1$ . What is the curve that maximises it? I am a bit confused on how to approach this question. I tried parameterising $C$ which clearly didn't help as I needed two parameters to do so. I then tried using Stokes theorem. I took out the dot product of the curl and the normal vector which came out to be $1-x^2+2x-y^2-2xy $ but I don't know how to proceed further. What substitution would we make?","['multivariable-calculus', 'line-integrals', 'stokes-theorem']"
3691688,Lifting of meromorphic function along a finite morphism,"I am currently reading the book ""Geometry of algebraic curves II"", by Arbarello, Cornalba and Griffiths, and I am having some difficulties understanding a passage p.105. The setting is the following: we are working with separated schemes of finite type over $\mathbb{C}$ , and we are given a finite surjective morphism $Z \rightarrow \bar{M_g}$ , where $Z$ is normal, and a meromophic map $\dot{\Delta} \rightarrow \bar{M_g}$ (where $\dot{\Delta}$ denotes a punctured disc). The authors then claim the following: ""after a harmless base change, we can lift any such map to a map $f$ from $\dot{\Delta}$ to Z"". My questions are the following : First, I am not certain what they mean by a meromorphic function to some general scheme, but I expect it to be a morphism $ \operatorname{Spec}( \mathbb{C}\{X\}[X^{-1}]) \rightarrow  \bar{M_g}$ ? If this is correct, How do we get the lifting after base change they are talking about ? I was thinking about the following: let $K = \mathbb{C}\{X\}[X^{-1}] $ , $x\in \bar{M_g}$ be the image of the given morphism, and $z\in Z$ be any point mapping to $x$ . We have 2 extensions of $\kappa(x)$ , namely $\kappa(z)$ and $K$ , and we would like to embed $\kappa(z)$ in $K$ modulo finite extension. Suppose that $\kappa(x)\rightarrow \kappa(z)$ is a finite extension of degree $d$ , could we find $d'\geq d$ such that an extension of this form would do the job ? $$\begin{align} K& \rightarrow K \\ 
X & \mapsto  X^{d'}
\end{align}$$ Or should I look for some ramification index of $Z \rightarrow \bar{M_g}$ at $z$ ? Any help would be greatly appreciated, thanks in advance.","['complex-geometry', 'morphism', 'algebraic-geometry', 'meromorphic-functions']"
3691692,Find all real values of a such that $x^2+(a+i)x-5i=0$ has at least one real solution,"Find all real values of a such that $x^2+(a+i)x-5i=0$ has at least one real solution. $$x^2+(a+i)x-5i=0$$ I have tried two ways of solving this and cannot seem to find a real solution. First if I just solve for $a$ , I get $$a=-x+i\frac{5-x}{x}$$ Which is a complex solution, not a real solution... Then I tried using the fact that $x^2+(a+i)x-5i=0$ is in quadratic form of $x^2+px+q=0$ with $p=(a+i)$ and $q=5i$ So I transform $$x^2+(a+i)x-5i=0$$ to $$(x+\frac{a+i}{2})^2=(\frac{a+i}{2})^2+5i$$ Now it is in the form that one side is the square of the other but I don't know how to find the roots since I'm not sure if I'm supposed to convert $(\frac{a+i}{2})^2+5i$ to polar form since I can't take the modulus of $(\frac{a+i}{2})^2+5i$ (or at least I don't know how). At thins point I feel like I'm just using the wrong method if anyone could guide me in the right direction I would very much appreciate it. Thank you.","['algebra-precalculus', 'quadratics', 'complex-numbers']"
3691750,$\operatorname{tr}(AB)$ in terms of $\operatorname{tr}(A)$,I have two symmetric and positive semi-definite matrices $A$ and $B$ . I know $\operatorname{tr}(AB) \neq \operatorname{tr}(A)\cdot \operatorname{tr}(B)$ . Are there any ways to think of $\operatorname{tr}(AB)$ in terms of $\operatorname{tr}(A)$ ?,"['matrices', 'linear-algebra']"
3691757,How do I prove the floor identity $⌊x + n⌋ = ⌊x⌋ + n$ in a more precise way?,"I am having trouble understanding the proof provided by the author for the property stated after ""Goal:"". Except from the text here is a list of useful properties: (PROPERTY 1a) $⌊x⌋ = n$ if and only if $n ≤ x < n + 1$ (1b) $⌈x⌉ = n$ if and only if $n − 1 < x ≤ n$ (1c) $⌊x⌋ = n$ if and only if $x − 1 < n ≤ x$ (1d) $⌈x⌉ = n$ if and only if $x ≤ n < x + 1$ (2) $x − 1 < ⌊x⌋ ≤ x ≤ ⌈x⌉ < x + 1$ (3a) $⌊−x⌋ = −⌈x⌉$ (3b) $⌈−x⌉ = −⌊x⌋$ (4a) $⌊x + n⌋ = ⌊x⌋ + n$ (4b) $⌈x + n⌉ = ⌈x⌉ + n$ Goal: As an exercise, prove the property $⌊x + n⌋ = ⌊x⌋ + n$ Proof: We will prove the property using a direct proof. Suppose that $⌊x⌋ = m$ , where $m$ is a positive integer. By property (1a), it follows
  that $m ≤ x < m + 1$ . Adding $n$ to all three quantities in this chain of
  two inequalities shows that $m + n ≤ x + n < m + n + 1$ . Using property
  (1a) again, we see that $⌊x + n⌋ = m + n = ⌊x⌋ + n$ .  This completes the
  proof. Proofs of the other properties are left as exercises. From: Rosen, Kenneth. Discrete Mathematics and Its Applications (p. 159) My understanding of the property we are tasked with proving is that it is an identity that is stating the proposition $∀x∀n(⌊x + n⌋ = ⌊x⌋ + n)$ where the domains of discourse for $x$ and $n$ are the set of all real numbers and the set of all integers, respectively. With that in mind, since this is a direct proof, shouldn't we start with arbitrary values of the domains (represented by real number $x$ and integer $n$ , and NOT JUST POSITIVE INTEGER $n$ ) and try to show that their properties imply the equation $⌊x + n⌋ = ⌊x⌋ + n$ ? (Ignoring the positive-n issue, how could we even do this if the only properties we are allowed to assume 
about $x$ and $n$ are that $x$ is real and $n$ is an integer?) Also, why was he able to start with and assume $⌊x⌋ = m$ (""suppose $⌊x⌋ = m$ "") out of nowhere?
If anyone can provide a version of his proof that is more precise and doesn't skip steps of reasoning or explanations, that would answer my question.","['proof-explanation', 'ceiling-and-floor-functions', 'functions', 'discrete-mathematics']"
3691836,Equivalence Relations: Understanding Compatibility,"Synopsis My textbook, near the end of the section on equivalence relations, mentions the problem of ""defining functions on a quotient set"". Specifically, assume that $R$ is an equivalence relation on $A$ and that $F: A \rightarrow A$ . We ask if there exists a corresponding function $\hat{F}: A/R \rightarrow A/R$ such that for all $x \in A$ , $$\hat{F} ([x]_{R}) = \ [F(x)]_R.$$ After introducing this notion, he goes on to state that $\hat{F}$ is ill-defined unless $(xRy) \Rightarrow (F(x) R F(y))$ . If this requirement is satisfied, then the function $F$ is ""compatible"" with $R$ Questions I have a few questions on this topic. Most of them relate to the conceptual understanding of it. (1) Why is such a question important? What applications will come from such a function $\hat{F}$ ? Why does he place so much importance on it (he stars it and spends much more time on it than on other sections)? How could I visualize or understand such a function more intuitively? (2) What does it mean for something to be ""well-defined""? I'm guessing it means that the something, or function in this case, is applicable to the definition, but I'm not entirely sure. (3) What's so special about these compatible functions besides being well-defined?","['elementary-set-theory', 'self-learning', 'equivalence-relations']"
3691872,Is it true that $\frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(c)}$,"Let $f$ and $g$ be continuous functions on the closed interval $[a,b]$ , and differentiable on the open interval $(a,b)$ , where $a<b$ . If $g'(x)\neq 0$ on $(a,b)$ , then does there exists $c$ on $(a,b)$ such that $\frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(c)}$ ? Note that $g(b)-g(a)\neq 0$ by Rolle's theorem. By Lagrange's mean value theorem, there exist $c$ and $d$ on $(a,b)$ , such that $\frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(d)}$ . I guess that $c$ and $d$ can have the same value.","['calculus', 'analysis']"
3691883,Homotopy of Jordan Curves?,"A Jordan curve is an injective continuous map from $S^1$ to $\mathbb{R}^2$ . If $\gamma_1,\gamma_2,\gamma_3,\gamma_4$ are four counter clockwise Jordan curves, such that $\gamma_{i+1}$ is contained in the exterior region of $\gamma_i$ for each $i=1,2,3$ . Let $A$ be the intersection of the of exterior region of $\gamma_1$ and the interior region of $\gamma_4$ , is it true that there is a homotopy from $\gamma_2$ to $\gamma_3$ in $A$ ? Here homotopy means homotopy as maps from $S^1$ to $\mathbb{R}^2$ . I thought this seems reasonable, but I couldn't come up with a proof myself, although I do think we may assume that $\gamma_i$ 's are all polygonal curves, but it is still hard after that. Maybe it is possible to use the Jordan Schoenflies theorem, generalized to four curves?","['homotopy-theory', 'algebraic-topology', 'analysis']"
3691932,Are null sets the only ones that are disjoint but not distinct?,"While studying analysis (which includes a chapter about set theory) I read this: Two sets $A,B$ are said to be disjoint if $A ∩ B = ∅$ . Note that this is not the same concept as being distinct, $A  \neq B$ . For instance, the sets $\{1, 2, 3\}$ and $\{2, 3, 4\}$ are distinct (there are elements of one set which are not elements of the other) but not disjoint (because their intersection is non-empty). Meanwhile, the sets $∅$ and $∅$ are disjoint but not distinct. Note: Not including curly braces for the empty sets isn't a typo on my part, I copied everything as it is. While I understand why these $2$ sets are disjoint but not distinct, it seems very illogical to me that any sets with non-null elements would have this property. I think the same would apply to sets such as $\{∅,∅\}$ and $\{∅,∅\}$ (someone correct me if I am wrong) but I can't think of any examples such that the set has at least one element that it is not null, so is it the case that null sets the only ones that are disjoint but not distinct (as this wasn't mentioned in the textbook) or am I missing something?",['elementary-set-theory']
3691938,About probability of arranging books on a shelf,"Lets suppose that we have 4 math books, 6 statistics books, and 10 books of other subjects, and we want to order a shelf with those books. The restriction we have is that the math and the statistic ones must be together in the shelf (I mean, the math ones should be together, and next to the statistics books). So the question is: What is the probability about this event happens? Remark: Every book is different","['discrete-mathematics', 'combinatorics', 'probability', 'permutations']"
3691951,Could we ever hope to integrate all functions?,"The Riemann integral has a weakness, in that it cannot integrate many functions of interest, such as Dirichlet's function $\boldsymbol{1}_\mathbb{Q}$ . The Lebesgue and Henstock-Kurzweil integrals extend the range of functions which can be integrated, but still, there are functions which these integrals cannot deal with. My question is thus. Let $\mathcal{A} \subseteq \mathcal{P}(\mathbb{R})$ be a ""rich enough"" family of ""nice enough"" subsets of $\mathbb{R}$ (e.g. $\mathcal{A}$ could be the collection of all (bounded?) Borel subsets of $\mathbb{R}$ ). Let $\overline{\mathbb{R}} = \mathbb{R} \cup \{ -\infty, +\infty \}$ be the extended real line. Is it possible to define an ""integration functional"" $\mathbf{I}: \mathbb{R}^\mathbb{R} \times \mathcal{A} \to \overline{\mathbb{R}}$ satisfying the following: $\mathbf{I}$ is total (i.e. it gives a value for every function $f: \mathbb{R} \to \mathbb{R}$ and set $B \in \mathcal{A}$ ); $\mathbf{I}$ has the familiar ""nice"" properties we expect of integrals, e.g.: Linearity: $\mathbf{I}(cf+dg,B) = c\mathbf{I}(f,B) + d\mathbf{I}(g,B)$ ; Monotonicity: if $f(x) \leq g(x)$ for all $x \in B$ , then $\mathbf{I}(f,B) \leq \mathbf{I}(g,B)$ ; Translation invariance: if $f_c$ is defined by $f_c(x) = f(x+c)$ , then $\mathbf{I}(f,B) = \mathbf{I}(f_c,B+c)$ ; If $B,C \in \mathcal{A}$ are disjoint, then $\mathbf{I}(f,B \cup C) = \mathbf{I}(f,B) + \mathbf{I}(f,C)$ ; Perhaps there are other properties we might want that I've overlooked. Whenever $f: \mathbb{R} \to \mathbb{R}$ is (Riemann/Lebesgue/HK/etc)-integrable on $B$ , we have $\mathbf{I}(f,B) = \int_B f$ . In other words, is it possible to define a notion of integration which extends standard integration, and can integrate any function? Ideally, we would want $\mathcal{A} = \mathcal{P}(\mathbb{R})$ , but I recognise that we might have to restrict it to get a meaningful/nontrivial answer. My bet is that the answer will be much the same as Vitali's theorem. I'm guessing that over $\mathsf{ZF}$ , $\mathsf{AC}$ would prove there is no such $\mathbf{I}$ , while other ""nice"" axioms such as $\mathsf{AD}$ might allow one to construct such an $\mathbf{I}$ .","['integration', 'axioms', 'real-analysis', 'descriptive-set-theory', 'set-theory']"
3691976,Motivation For Weight Choice In Pooled Variance,"In the formula for pooled variance , the estimated variance of each population of size $n_i$ is weighted by $n_i-1$ .  Is there a good motivation for this?  I would assume the formula is always unbiased, even when different weights are chosen.  But my guess is that the variance of the variance estimation is minimized by this choice, assuming a nice distribution of the 'real' error.  If that's true, where can I read a proof of it?  If not, what other motivation is there for this choice?","['statistics', 'variance', 'reference-request']"
3692031,Non-zero element in exterior power,"Let $R=\mathbb{Z}[\sqrt{5}]$ and consider $I=(2,\, 1+\sqrt{5})$ as an $R$ -module. I'm struggling to prove that the element $2\wedge (1+\sqrt{5})$ of the exterior power $\Lambda^2(I)$ is non-zero. I should construct some alternating bilinear map from $I\times I$ in $\mathbb{Z}$ (regarded in some way as an $R$ -module?) not sending $(2,\, 1+\sqrt{5})$ in $0$ : I'm thinking of something similar to the determinant since each element $u\in I$ can be expressed as $2a+b(1+\sqrt{5})$ , but it seems it doesn't work so well... Any idea about how to do this? See also Remark 4.7 here: https://kconrad.math.uconn.edu/blurbs/linmultialg/extmod.pdf","['ring-theory', 'abstract-algebra', 'tensor-products']"
