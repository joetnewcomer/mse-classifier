question_id,title,body,tags
4487189,Nice proof that expected number of $k$-cycles in a permutation of $n$ is $\frac1k$ whenever $0<k\leq n$,"Taking the combintorialist point of view that a cycle of a permutation $~\sigma$ of a finite set $S$ is an orbit of the action of the subgroup generated by $~\sigma$ in its (natural) action on $~S$ , it is known that whenever $S$ admits $k$ -cycles at all (so $0<k\leq n$ where $n$ is the size of $S$ ), the expected number of $k$ -cycles of $~\sigma$ where $\sigma$ is chosen uniformly at random among all permutations of $~S$ , is precisely $~\frac1k$ . Stated differently, the statistic ""number of $k$ -cycles"" takes an average value of $~\frac1k$ when $\sigma$ runs over all permutations of $~S$ . I am looking for nice, intuitive, transparent, proofs of this fact, notably where the fraction $~\frac1k$ comes about naturally, preferably as the probability of obtaining a specific value when choosing an element uniformly from a $k$ -element set. While I know a few easy computations that prove the result, none I've seen so far have achieved this highest standard, though some arrive at the fraction after some very basic cancellations. The cases $k=1$ (the expected number of fixed points of a permutation is precisely $1$ ) and $k=n$ (there are $(n-1)!$ distinct cyclic permutations of $S$ , which is $\frac1n$ of all permutations) are very well known and with easy proofs, but I would like a proof that covers all allowed values of $k$ in a uniform manner. I've seen quite a few questions on this site that come close to this one, but few state the result in isolation and none specifically ask for elegant arguments, so I don't feel this question is truly a duplicate of any of them. This question was inspired by watching this video (with a rather click-bait title) where the deduction of the case for $k=n$ (from 8:17 on) is followed by the irrefutable argument ""this is a general result"" (implying it's validity for all $k$ ) with no mention of expectation (for $k=n$ the only possible numbers of cycles are $0$ and $1$ , so the expected value is just a probability) and no explanation whatsoever.","['permutations', 'combinatorics', 'probability']"
4487193,Prove that a half-open line is uncountable - corrected,"I have to provide a proof that a halfopen line $L_a:={ð‘¥ âˆˆ â„:x > a}$ is uncountable. I have used SchrÃ¶der-Bernstein-Cantor and tried to show that an injection exists between $L_a$ and $â„$ an injection between $â„$ and $L_a$ to conclude that a bijective function exists between $â„$ and $L_a$ to prove that $L_a$ is uncountable. I chose the following two functions: $f:L_a \rightarrow â„:x \rightarrow x$ is injective $g:â„ \rightarrow L_a:x \rightarrow a+e^x$ is injective It follows that a bijection exists between both sets, so $L_a$ is uncountable. EDIT: corrected use of SBC, it should now work. Thanks for your feedback.","['elementary-set-theory', 'functions', 'analysis']"
4487231,Bound on the probability of infinity norm of a Gaussian vector,"I consider a random vector $X\in \mathbb{R}^N$ following a multivariate Gaussian distribution of mean zero and covariance matrix $\Sigma_X$ such that $\forall 1\le i \le N, (\Sigma_X)_{ii}=1$ (there are only 1's on the diagonal). I am interrested in the maximum of the absolute value of the elements of $X$ : $\lVert X \rVert_\infty = \max_{1\le n \le N} |X_n|$ . I have the intuition that the following inequality is true: \begin{equation}\label{goal}
\forall K>0, \; \mathbb{P}(\lVert X \rVert_\infty \le K) \ge \mathbb{P}(\lVert Y \rVert_\infty \le K) \qquad \mathrm{where} \qquad Y\sim \mathcal{N}(0,I_N)
\end{equation} In other words, the probability of $\lVert X \rVert_\infty$ being smaller than $K$ is minimum in the case where all the elements of $X$ are iid centered normal variables. I managed to prove this in the case $N=2$ by direct calculation of the above probability but the integrals get too complicated for bigger $N$ . Does this result holds for $N>2$ and is there a proof ? In the case that the result is not true, are there smaller classes of covariance matrices $\Sigma_X$ (for example Toeplitz matrices, or Toeplitz matrices with $1, \rho, \rho^2,\rho^3,\dots$ on the first line) for which the inequality holds ?","['probability-distributions', 'probability', 'real-analysis']"
4487244,Finding the value of $\sqrt[4]{(4+\sqrt7)^{-1}}\sqrt{1+\sqrt7}$ with other approaches,"It is a problem from a timed exam, What is the value of $\sqrt[4]{(4+\sqrt7)^{-1}}\sqrt{1+\sqrt7}$ ? $1)1\qquad\qquad2)\sqrt[4]2\qquad\qquad3)2\qquad\qquad4)2\sqrt[4]2$ I solved it with two approaches. First approach, $$\sqrt[4]{\frac{1}{4+\sqrt7}}\times\sqrt{1+\sqrt7}=\sqrt[4]{\frac{2}{(1+\sqrt7)^2}}\times\sqrt{1+\sqrt7}=\sqrt[4]2$$ Second approach, $$\sqrt[4]{\frac{1}{4+\sqrt7}}\times\sqrt{1+\sqrt7}=\sqrt[4]{\frac{1}{4+\sqrt7}}\times\sqrt[4]{8+2\sqrt7}=\sqrt[4]{\frac{2(4+\sqrt7)}{4+\sqrt7}}=\sqrt[4]2$$ I'm wondering is it possible to solve this problem with other efficient approaches?","['nested-radicals', 'algebra-precalculus']"
4487261,"Finding a dominant root $\alpha$ for a semisimple, irreducible Lie-algebra $\mathfrak{g}$.","IÂ´m working myself right now through this article and have trouble understanding a part of the proof of Proposition 6.4, case II) on page 345-346. I try to give a short outline of the situation: Suppose $V$ is an irreducible $\mathfrak{g}$ module, with highest weight $\lambda$ . And $\mathfrak{g}$ being a semisimple Lie-algebra, with a subalgebra $\mathfrak{s} \simeq \mathfrak{sl}(2,\mathbb{C})$ . Also let $V$ be a complex vector space [Edit] that as an $\mathfrak{s}$ -module is the direct sum $V=V(1)\oplus V_0$ , where $V(1)$ is the usual 2-dimensional (irreducible) representation of $\mathfrak{s}$ , and $V_0$ is a direct sum of trivial 1-dimensional representations of $\mathfrak{s}$ [/Edit, JL]. Then the statement i want to verify goes as follows: Then there exists a dominant root $\alpha$ for $\mathfrak{g}$ , s.th. $(\lambda, \alpha^{\lor})=1, (w_0 \lambda, \alpha^{\lor})=-1, (\mu, \alpha^{\lor})=0$ for all weights $\mu$ with $w_0 \lambda < \mu < \lambda$ . Here $w_0$ is the longest element in the Weyl group $\mathfrak{W}$ , and $<$ the usual ordering. My thoughts so far: By the theorem of the highest weight there exists a highest weight $\lambda$ satisfying $$2\frac{(\lambda, \alpha)}{(\alpha,\alpha)} \in \mathbb{Z},$$ which coincides with $(\lambda, \alpha^{\lor})$ , using the definition of the coroot of a semisimple Lie-algebra. Using the Properties of the longest element $w_0$ in the Weyl-group of a semisimple Lie-algebra $\mathfrak{g}$ and concidering that $\mathfrak{sl}(n,\mathbb{C})$ has root system $A_{n-1}$ , $w_0$ satisfies $w_0\lambda = - \lambda$ . So in particular we have $$(w_0 \lambda, \alpha^{\lor}) = - (\lambda, \alpha^{\lor}).$$ The trivial representation of $\mathfrak{s}$ has a single weight, $0$ . Let $V(1)=\mathbb{C}^2$ be the standard representation. Write $e_1,e_2$ for the standard basis. Note that $$H=\begin{pmatrix}
1 & 0\\
0 & -1\\
\end{pmatrix}.$$ Then $He_1 = e_1$ and $He_2 = -e_2$ . Thus the set of weights of $V(1)$ is $\{\pm1\}$ .","['lie-algebras', 'representation-theory', 'root-systems', 'abstract-algebra', 'group-theory']"
4487264,Is the converse of this also true$?$,"If the first derivative of $f$ changes sign at $x$ , does $f$ necessarily have an extremum at $x$$?$ I was just wondering about maxima and minima and I thought of this. I know that if $f'$ changes from positive to negative at $x=a$ , then $f$ has a local maximum at $a$ and if $f'$ changes from negative to positive at $x=a$ , then the point is a local minimum at $a$ and if $f'$ does not change sign at $x=a$ , then $f$ does not have a local extrema at $a$ . Is the converse of the last line also true $?$","['functions', 'derivatives']"
4487279,Getting Prime by Changing 2 Digits,"I just got a result that one cannot get a prime from arbitrary natural number $n$ by changing only one digit of its decimal expansion. For example, you cannot get prime by changing only one digit of $200$ since you need to change the last digit. Another proof is one can use prime number theorem by assuming we can change only one digit of $10k, k\in \mathbb{N}$ to get prime, but this means \begin{align*}
\pi(x)\geq \sum_{k\leq x/10}1\geq \lfloor x/10 \rfloor
\end{align*} which contradicts prime number theorem. My question is, can we get prime by changing exactly two digits of $n$ for arbitrary $n \geq 100$ ?","['analytic-number-theory', 'number-theory', 'elementary-number-theory']"
4487288,"If every closed curve on a surface has zero total torsion, then the surface is totally umbilic","Let $\Sigma$ be an oriented, simply connected surface in $\mathbb{R}^{3}$ , let $\xi$ be its unit normal, and let $\alpha$ be a a smooth curve in $\Sigma$ . In this paper , the authors prove that, if every closed curve $\alpha$ has vanishing total torsion, i.e., if $$
\int_{\alpha} \tau = 0 \quad \text{for all closed curves $\alpha$},
$$ then $\Sigma$ is totally umbilic. (In fact, they prove a stronger statement by considering a Riemannian ambient space.) I am having a hard time understanding a step in their proof of Lemma 2, which establishes the said result. After proving that the integral of the geodesic torsion $\tau_{g}$ over any closed curve $\alpha$ vanishes, they claim that continuity forces the geodesic torsion to vanish identically ; see text after equation (26) in the picture below. I am not sure why continuity is invoked here. I agree that, if $\nabla_{X}\xi$ and $X$ are not parallel at $p$ , then they remain not parallel in a neighborhood of $p$ , but how can one then conclude that every closed curve around $p$ has $\tau_{g} =0$ ?","['submanifold', 'riemannian-geometry', 'proof-explanation', 'curves', 'differential-geometry']"
4487308,Tensor product of irreducible representations of finite groups in GAP,"Is there a way with GAP to get the tensor product of representations in a certain basis? To be more specific, I would like to cross-check with GAP the tensor products of, for example A5, as given in equations (82)-(86) in https://arxiv.org/abs/1003.3552 I give here eq.(82), so you don't need to open the paper itself to see an example of what I'm looking for. \begin{eqnarray}
\left(
\begin{array}{c}
x_1  \\
x_2  \\   
x_3  \\
\end{array}
\right)_{{\bf 3}} 
\otimes 
\left(
\begin{array}{c}
y_1  \\
y_2  \\   
y_3  \\
\end{array}
\right)_{{\bf 3}}
&=&
(x_1y_1+x_2y_2+x_3y_3)_{{\bf 1}}
 \oplus
\left(
\begin{array}{c}
x_3y_2 - x_2y_3  \\
x_1y_3-x_3y_1   \\   
x_2y_1-x_1y_2 \\
\end{array}
\right)_{{\bf 3}}
\nonumber\\
& \oplus &
\left(
\begin{array}{c}
x_2y_2 - x_1y_1   \\
x_2y_1+x_1y_2   \\   
x_3y_2+x_2y_3  \\
x_1y_3+x_3y_1 \\
-\frac{1}{\sqrt3}(x_1y_1+x_2y_2-2x_3y_3)
\end{array}
\right)_{{\bf 5}},
\end{eqnarray} For A5 I can do it fairly easy by hand, but for larger groups it would be nice to cross-check with GAP or something else.","['gap', 'representation-theory', 'finite-groups', 'tensor-products', 'group-theory']"
4487327,"Compute $\int_0^1 \frac{\sqrt{t(1-t)}}{a+(t-b)^2} \ dt$ for $a,b>0$","I would like to compute the integral $$\int_0^1 \frac{\sqrt{t(1-t)}}{a+(t-b)^2} \ dt$$ where $a$ and $b>0$ are positive parameters. Wolfram Alpha is able to provide an answer for the indefinite integral, but is struggeling with computing the definite one https://www.wolframalpha.com/input?i=integrate+sqrt%28t*%281-t%29%29%2F%28a%2B%28t-b%29%5E2%29+dt+","['integration', 'complex-analysis', 'calculus', 'definite-integrals']"
4487356,Determining if $\displaystyle (a_n)_{n \in \mathbb{N}}$ is bounded if $\displaystyle\sum_{n=1}^\infty \frac{a_n}{n} <\infty$.,"Consider the sequence of real numbers $(a_n)_{n \in \mathbb{N}}$ such that $a_n>0$ and such that the following series, $$
\sum_{n=1}^\infty \frac{a_n}{n} < \infty.
$$ Is the sequence $(a_n)_{n \in \mathbb{N}}$ bounded? The intuition says to me that it must be bounded because if we let it grow, then in someway it would be comparable to the harmonic series which we know it's divergent. But I can't find a nice proof to it or if there's some article or book to read about it.","['sequences-and-series', 'upper-lower-bounds', 'real-analysis']"
4487386,"Given matrix $B$, find all possible matrices $A$ satisfying $ A(A-2B) = -(A-2B)A $","Let $A, B \in \Bbb R^{3 \times 3}$ such that $A(A-2B) = -(A-2B)A$ . Given $$ B = \begin{pmatrix} 2 & -2 & 1 \\ -1 & 3 & -1 \\ 2 & -4 & 3 \end{pmatrix} $$ find all possible matrices $A$ satisfying the equation above. I hope there is some way faster than compute directly...","['matrices', 'matrix-equations', 'linear-algebra']"
4487396,Simplifying binomial sum of factorials,"When evaluating a multi-indexed derivative (what exactly is not important now) I ran across the sum $$
\sum_{\nu \leq \mu} \frac{\mu!}{\nu!(\mu - \nu)!} |\nu|!|\mu - \nu|!=(|\mu| + 1)!
$$ for $\mu$ a Multi index with finite support (i.e. $|\mu| = \sum_{j \geq 1}\mu_j < \infty$ ). When evaluating the sum numerically, I get the suspicion that this equality holds. However, I have not been able to provide a proof. Can somebody help me with this? I tried to proof this by induction on the 'dimension' of the multi index $\mu$ . The base case is trivial, but I cannot get the inductive step to work: (with $\tilde\mu = (\mu_1 ,\cdots, \mu_{n-1})$ etc.) $$
\sum_{\nu \leq \mu} \frac{\mu!}{\nu!(\mu - \nu)!} |\nu|!|\mu - \nu|!\\
=\sum_{\nu_n = 0}^{\mu_n}  {\mu_n \choose \nu_n}  \sum_{\tilde\nu \leq \tilde\mu} \frac{\tilde\mu!}{\tilde\nu!(\tilde\mu - \tilde\nu)!} |\nu|!|\mu - \nu|!
$$ From which I cannot apply the induction hypothesis because I cannot seem to isolate $\sum_{\tilde\nu \leq \tilde\mu} \frac{\tilde\mu!}{\tilde\nu!(\tilde\mu - \tilde\nu)!} |\tilde\nu|!|\tilde\mu - \tilde\nu|!$ to apply it. Even a simple case for $\mu=(\mu_1,\mu_2)$ has me confused. I cannot get $$
\sum_{\nu_1 = 0}^{\mu_1}\sum_{\nu_2 = 0}^{\mu_2}{\mu_1 \choose \nu_1}{\mu_2 \choose \nu_2} (\nu_1+\nu_2)!(\mu_1+\mu_2-\nu_1-\nu_2)!
$$ To simplify to $(\mu_1 + \mu_2 + 1)!$ without running into nasty factorials everywhere. For reference, I calculated it numerically by hand for some small examples and for larger examples using python: import math
import numpy as np

np.random.seed(0)
def fun(mu, processed_mu=[], processed_nu=[]):
    if len(mu) == 0:
        return math.factorial(sum([a - b for a, b in zip(processed_mu, processed_nu)])) * math.factorial(
            sum(processed_nu))
    processed_nu = processed_nu + [0]
    processed_mu = processed_mu + [0]
    summand = 0
    for nu_i in range(mu[0] + 1):
        binomial_coefficient = math.comb(mu[0], nu_i)
        processed_nu[-1] = nu_i
        processed_mu[-1] = mu[0]
        summand = summand + binomial_coefficient * fun(mu[1:], processed_mu, processed_nu)

    return summand

for i in range(10):
    mu_loc = np.random.randint(3, size=10)
    print(f""{mu_loc}: Sum={fun(mu_loc)}, (|\\mu| + 1)!={math.factorial(sum(mu_loc) + 1)}"") [0 1 0 1 1 2 0 2 0 0]: Sum=40320, (|\mu| + 1)!=40320
[0 2 1 2 2 0 1 1 1 1]: Sum=479001600, (|\mu| + 1)!=479001600
[0 1 0 0 1 2 0 2 0 1]: Sum=40320, (|\mu| + 1)!=40320
[1 2 0 1 1 1 0 2 0 2]: Sum=39916800, (|\mu| + 1)!=39916800
(...) Can anybody help me with this proof? (Or, of course, show me that this is simply not true?) Extra Furthermore, I am looking at a similar question for the triple equivalent, hoping that a proof for the first question will help me with this. $$
\sum_{\nu^{(1)}+\nu^{(2)}+\nu^{(3)} = \mu} \frac{\mu!}{\nu^{(1)}!\nu^{(2)}!\nu^{(3)}!} |\nu^{(1)}|!|\nu^{(2)}|!|\nu^{(3)}|! = (1 + \frac{|\mu|}{2})(|\mu|+1)!
$$","['summation', 'combinatorics', 'factorial']"
4487403,"Finding a closed form, or reasonably simple series representation, for $ \lim_{x\to 1^-} (1-x)\sum_{n\geq 1}\log(1-x^n)$","When dealing with the computation/approximation of $\int_{0}^{1}\sqrt{\prod_{n\geq 1}(1-x^n)}\,dx$ I faced the following problem: Find a closed form, or a reasonably simple series representation, for $$ \lim_{x\to 1^-} (1-x)\sum_{n\geq 1}\log(1-x^n) $$ The tricky parts comes from the fact that we are not allowed to exchange $\lim$ and $\sum$ . Here we have a sketch of the convergence of $(1-x)\sum_{n=1}^{N}\log(1-x^n)$ to $f(x)=(1-x)\sum_{n\geq 1}\log(1-x^n)$ : I tried to compute the above limit as $$ \lim_{m\to +\infty} \int_{0}^{1} (m+1) x^m f(x)\,dx $$ and I got stuck in the evaluation of series involving harmonic numbers (sadly). I guess the Jacobi triple product can be helpful. Numerically the limit is $\approx -1.64493$ . Post-mortem addendum : the answers below are both great, and I checked that also my idea leads to $-\psi'(1)=-\zeta(2)$ as a value of the limit. As a by-product we have the approximation $$ \prod_{n\geq 1}(1-x^n) \approx \exp\left(\frac{-\pi^2 x}{(1-x)(\pi^2-x(\pi^2-6))}\right) $$ over $[0,1]$ , whose absolute error is less than $0.003$ .","['limits', 'logarithms', 'sequences-and-series']"
4487404,Is there a 12x12 symmetric Hadamard matrix? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question More generally, is there a simple condition for which $n$ there are symmetric Hadamard matrices of order $n$ ? This set of $n$ is closed under multiplication via the Kronecker product.","['matrices', 'hadamard-matrices']"
4487408,Plotting the streaklines knowing the pathlines,"I have a two-dimensional velocity field $$\mathbf{V}=\begin{pmatrix}
u \\
v
\end{pmatrix}=\begin{pmatrix}
0.5+0.8x \\
1.5+2.5\sin(\omega t)-0.8y
\end{pmatrix}$$ from Example 4-5 in ""Fluid Mechanics: Fundamentals and Applications"" 2nd Edition. I have correctly derived the streamlines from $$\frac{dy}{dx}=\frac{v}{u}$$ at $t=2$ giving $$y=\frac{C}{0.8(0.5+0.8x)}+1.875$$ in agreement with the text. Using a Runge-Kutta numerical integration method I approximated the pathlines from $0<t<2$ s using these equations. $$u=\frac{dx}{dt}=0.5+0.8x$$ $$v=\frac{dy}{dt}=1.5+2.5\sin(\omega t)-0.8y$$ Plotting the equations agrees with the figure in the text. My results are on the left red: starting from $(0.5,0.5)$ blue: starting from $(0.5,2.5)$ green: starting from $(0.5,4.5)$ and the light pink curves in the right figure are from the text. These questions ( here , and here ) deal with methods to find the streaklines . This link , however, has been the most helpful in understanding how to derive the necessary equations. But my results do not agree with the text. The text defines the method for plotting the streaklines this way. Finally, the streaklines are simulated by following the paths of many fluid tracer particles released at the given three locations at times between $t=0$ s and $t=2$ s, and connecting the locus of their positions at $t=2$ s. I want to try and avoid solving the ODEs by hand since $y(t)$ will involve an integrating factor, but more importantly, to arrive at equations for $x=x(x',t,t')$ and $y=y(y',t,t')$ without going through lots of error-prone algebra. From the third link above $(x',y')$ are the future positions of the particle at time $t'$ and $t$ is the current time. To close out the question, I am looking for a way to use my approximate values for the pathlines to plot my streaklines from $0<t<2$ s. EDIT: It has just occurred to me that in the first of the previous links, an ODE that defines a streakline is mentioned. The ODE is from Wikipedia and is given here. $$\left\{\begin{matrix}
\frac{d\vec{x}_{str}}{dt}=\vec{u}_P(\vec{x}_{str},t) \\
\vec{x}_{str}(t=\tau_P)=\vec{x}_{P0}
\end{matrix}\right.$$ From Wikipedia... $\vec{u}_P(\vec{x},t)$ is the velocity of a particle $P$ at location $\vec{x}$ and time $t$ . The parameter $\tau_P$ , parameterizes the streakline $\vec{x}_{str}(t,\tau_P)$ and $t_0\le\tau_P\le t$ , where $t$ is a time of interest. The second link from above describes this parameterization, but I am struggling to understand how to work it out in general, not just for this velocity field. But if I can make that work then the ODE should be able to be solved numerically with a similar Runge-Kutta method. However, this link suggests that there are errors in the Wikipedia article. The parameterized ODEs from $0<\tau<t$ should be the following. $$\frac{dx_\tau}{d\theta}=0.5+0.8x_\tau(\theta)$$ $$\frac{dy_\tau}{d\theta}=1.5+2.5\sin\left(\omega(\tau+\theta)\right)-0.8y_\tau(\theta)$$ Getting some help setting up the numerical integration is where I'm stuck. I'm unsure how to deal with the $\tau$ and $\theta$ terms; i.e, which term is marching forward and what values to put in for $\tau$ .","['ordinary-differential-equations', 'vector-fields', 'runge-kutta-methods', 'multivariable-calculus', 'fluid-dynamics']"
4487470,Finding the sum of unknown elements of a set given certain conditions,"Let A={ $1,a_1,a_2,...,a_{18},77$ } be a set of integers with $$1\lt a_1\lt a_2....\lt a_{18}\lt77.$$ Let the set A+A={ $x+y:x,y\in A$ } contain exactly 39 elements. Then the value of $a_1+a_2+a_3+..+a_{18}$ ? In this, i can clearly see that 36 elements would be of the type $a_1+1,a_2+1....,a_{18}+1$ and $a_1+77, a_2+77, a_3+77...,a_{18}+77$ But I don't understand how to proceed from here and which pairs I can consider to be common? Or am I interpreting the problem in a completely wrong manner?
Thank You.","['elementary-set-theory', 'combinations', 'permutations']"
4487501,"Mathematical notation in a machine learning problem, majority rule","(I apologise that the title may be a bit confusing and I don't know if this is the right community to ask my question.) This is a mathematical notation problem in the field of machine learning. A little context: In my machine learning problem, I want to classify an individual based on the samples I have of each individual (i.e. each person has multiple samples).
Once I have made the prediction, I get the predicted class for each of the samples, but what I want is a global classification of the individual to one of the two classes.
What I do is to count the number of samples that have been assigned to each class, and depending on this establishing a threshold, k, where an individual will be classified as Class 1 if the number of samples classified as Class 1 exceeds that threshold. My problem is that I don't have much knowledge of formal mathematical writing, and I would like to translate the above into mathematical notation. I have written the following but I don't know if it is correct. $$
Y=1, \sum_{i=0}^{n}\hat{y_{i}}\Leftrightarrow \hat{y_{i}}=1 > k\sum_{i=0}^{n}\hat{y_{i}}
$$ where $\hat{y_{i}}$ , denotes each individual element of the vector with the predictions for each sample. In summary, what I want to write is: The individual will be assigned to class 1 if, the number of samples classified as '1' is greater than k times the total number of samples of the individual.","['notation', 'statistics', 'machine-learning']"
4487514,Is my proof right and enough?: A non-convergent series of non-negative terms diverges to infinity,"""Proof that a non-convergent series of non-negative terms diverges to infinity"" I just want to be sure that my proof is right and complete. Could someone help me to check it? My attempt: Let $\sum_{n=1}^\infty x_n$ be a non-convergent series of non-negative terms and let $M>0$ , since $\sum_{n=1}^\infty x_n$ is non convergent, it does not converges to zero, then there exists $N_1 \in \mathbb{N}$ such that: $$\left| \sum_{m=1}^{N_1} x_n \right| = \sum_{m=1}^{N_1} x_n \geq M$$ Since $x_n \geq 0$ for all $n$ and since the series is non convergent, there must be an infinite amount of terms that are strictly greater than zero (otherwise the series would converge to the finite sum of positive terms). Hence, there exists $N_2 \in \mathbb{N}$ such that: $$ \sum_{m=1}^n x_n > \sum_{m=1}^{N_1} x_n \geq M$$ for all $n \geq N_2$ Which completes the proof.","['real-analysis', 'calculus', 'solution-verification', 'sequences-and-series', 'algebra-precalculus']"
4487523,"Is there a ""No Free Lunch Theorem"" for Polynomial Approximation?","In search and optimization, a popular (set of) theorem(s) exist called the ""No Free Lunch Theorem(s)"". The authors of this theorem describe it as ""for any algorithm, any elevated
performance over one class of problems is offset by performance over another class"" ( https://ieeexplore.ieee.org/document/585893 ). These theorems have important interpretations in many applied fields such as Machine Learning. From a layman's perspective, the implication of the ""No Free Lunch Theorem"" asserts that there is no ""universally best"" class of algorithm for all ""classes of optimization problems"". I was wondering if a similar type of theorem exists for Polynomial Approximation. For example, we know that there are many methods in mathematics that can be used for function approximation - for instance, Taylor Polynomials, Lagrange Polynomials, PadÃ© Polynomials, etc. As in the case of Machine Learning algorithms, can any claims similar to those asserted by the ""No Free Lunch Theorems"" be made regarding different methods of Polynomial Approximations? For example - is it possible that Taylor Polynomials might ""better"" approximate some functions compared to PadÃ© Polynomials; but by virtue of this fact, there also might exist some other functions that PadÃ© Polynomials approximate better than Taylor Polynomials? Or are have we been able to mathematically determine that some approximation methods are ""universally better"" than other approximation methods for broad classes of functions? Thank you! Note: I suppose that in this context, ""better"" can be quantified through properties such as ""strength of convergence"", ""size of the radius of convergence"" or properties of the ""bounded error"" (e.g. magnitude) - but I am not sure about this.","['approximation', 'machine-learning', 'functions', 'taylor-expansion', 'polynomials']"
4487531,Why isn't the dimension of the space of curvature-like tensors equal to the dimension of the Grassmannian?,"It's a well known fact that if $V$ is a vector space of dimension $n$ , then the vector space consisting of all curvature-like tensors ( see this text for a precise definition and a proof ) has dimension $\frac{n^2(n^2-1)}{12}$ . It's also well known that any curvature-like tensor $R$ is determined by its sectional curvatures (i.e, the values $R(x, y, x, y)$ with $x, y$ going through all of $V$ ). The sectional curvature can be thought of as a function $K: \mathrm{Gr}_2(V) \to \mathbb{R}$ . In my mind these two separate facts appear to be in contradiction: if the sectional curvatures determine $R$ , shouldn't the vector space consisting of all curvature-like tensors have the same dimension as $\mathrm{Gr}_2(V)$ (which has dimension $2(n-2)$ )? Can someone explain what I'm missing here? Thanks in advance!","['riemannian-geometry', 'geometry', 'smooth-manifolds', 'grassmannian', 'differential-geometry']"
4487581,Prove that $\operatorname{adj} (kA) = k ^ {n - 1} \operatorname{adj} A$,"Prove that $\operatorname{adj} (kA) = k ^ {n - 1} \operatorname{adj} A$ where $A$ is $n \times n$ matrix. Hello, I know the proof for this when $\det A \neq 0$ : For $k = 0$ , this holds. Now, for $k \neq 0$ $$(kA) \operatorname{adj} (kA) = k ^ n \det (A) I_n$$ $$A \operatorname{adj} (kA) = k ^ {n - 1}$$ Pre-multiply both sides by $\operatorname{adj} A$ (this is possible) $$\underbrace{\operatorname{adj} (A) A}\operatorname{adj}(kA) = \det (A)k^{n - 1}\operatorname{adj} A$$ $$\det A \cdot I_n \operatorname{adj}(kA) = \det (A)k^{n - 1} \operatorname{adj} A$$ Cancel $\det A$ : $$\operatorname{adj} (kA) = k^{n - 1} \operatorname{adj} A$$ But, what if $\det A = 0$ ? Thanks","['matrices', 'linear-algebra']"
4487591,"Find the minimum value of u, where $xyz = k^3$ and $u = (x + a)(y + b)(z + c)$","Find the minimum value of u, where
a) $$x^2 + y^2 = 1$$ and $$u = \frac{(ax^2 + by^2)}{\sqrt{(a^2x^2 + b^2y^2)}}$$ b) $$xyz = k^3$$ and $$u = (x + a)(y + b)(z + c)$$ and $$a > 0; b > 0; c > 0$$ I could do the first one with Lagrange  multiplier but it's really tedious. Can anyone give me any better way to solve the second one?? I don't want to use another tedious calculation with Lagrange multipliers. I've asked this same question few months ago but I couldn't work with the Holder inequality hint and also I failed to solve the equations arising using Lagrange multipliers.","['maxima-minima', 'multivariable-calculus', 'lagrange-multiplier', 'real-analysis']"
4487600,Evaluate $\displaystyle\lim_{n\to\infty} \frac{a_n}{n^2}$,"Problem : Let $f\colon[0,\infty)\to\mathbb{R}\quad f(x)=\sin (\pi x)$ . For each $n\in\mathbb{N}$ , let $x_1,x_2,\cdots,x_k$ denotes roots of $$ f(x)=\frac{x}{2n}$$ Evaluate $$\lim_{n\to\infty} \frac{a_n}{n^2}$$ where $a_n$ is sum of all $x_1, x_2, \cdots, x_k$ . My Attempt I think $\frac{a_n}{n^2}\to 2$ . From observation, the equation $$f(x)=\frac{x}{2n}$$ has $2n$ distinct real roots for each $n\in\mathbb{N}$ . Rewrite this roots as $x_1, x_2, \cdots, x_{2n}$ . If $n\to\infty$ , $\frac{x}{2n}\to 0$ . This means the equation will be changed to : $f(x)=0$ under $n\to\infty$ . And, real roots of $f(x)=0$ is well known : $0, 1, 2, \cdots$ So I think $$\left\{x_1,x_2,\cdots,x_{2n}\right\}\to\left\{0,1,\cdots,2n-1\right\}.$$ This means $$a_n \approx \sum_{k=0}^{2n-1}k = n(2n-1)$$ And finally $$\lim_{n\to\infty} \frac{a_n}{n^2} \approx \lim_{n\to\infty}\frac{n(2n-1)}{n^2}=2$$ I have two questions : Is this valid? Is there any nice approach like squeeze theorem? (I think it's hard to represent $x_n$ with $\arcsin$ , so I think squeeze theorem will be the best approach.)","['limits', 'calculus', 'solution-verification']"
4487618,"""Pushforward"" of de Rham cohomology along ""normal"" surjective local diffeomorphisms?","Notation: for $X$ a smooth manifold, let $T^n(X)$ denote the space of (global, smooth) covariant rank- $n$ tensors on $X$ , i.e. sections of $(T^*X)^{\otimes n}$ . Let $X\xrightarrow{Q}Y$ be a smooth, surjective, local diffeomorphism between smooth manifolds. Let us also assume $Q$ is ""normal"" in that its group of deck transformations acts transitively on each fiber. We say $s \in T^n(X)$ is "" $Q$ -invariant"" if $s$ is invariant under all deck transformations of $Q$ . Denote by $\text{inv}_{Q}T^n(X)$ the space of $Q$ -invariant covariant rank- $n$ tensors on $X$ . Then we claim for each integer $n\geq 0$ there is a unique homomorphism $Q^n_! : \text{inv}_{Q}T^n(X) \rightarrow T^n(Y)$ , such that for every open $U\subset X$ , if $Q|_U$ is a diffeomorphism onto $Q(U)$ with inverse $P : Q(U)\xrightarrow{\approx} U$ , then for any $\omega \in \text{inv}_{Q}T^n(X)$ we have $$
(Q_! \omega)|_{Q(U)} = P^*(\omega|_U)
$$ Furthermore: For $\omega \in T^n(Y)$ we have $Q^*\omega$ is $Q$ -invariant, and we should have $Q_!Q^*\omega = \omega$ . $Q^n_!$ preserves symmetric and antisymmetric tensors, thus also restricting to maps $\text{inv}_{Q}\mathrm{Sym}^n(X)\rightarrow \mathrm{Sym}^n(Y)$ and $\text{inv}_{Q}\Omega^n(X)\rightarrow \Omega^n(Y)$ . $Q^2_!$ should preserve signatures; therefore, $Q^2_!$ of a $Q$ -inv Riemannian metric (or pseudo-Riemannian of some signature) is another Riemannian (or pseudo-Riemannian of same signature) metric. For $\omega \in \text{inv}_{Q}\Omega^n(X)$ we have $d\omega \in \text{inv}_{Q}\Omega^{n+1}(X)$ and $Q^{n+1}_!(d\omega) = d(Q_!^n \omega)$ . So also, $Q_!$ gives a chain complex morphism $\text{inv}_{Q}\Omega^\bullet(X)\rightarrow \Omega^\bullet(Y)$ . Also, by the 1st bullet point, if $Q_!\omega$ is exact (on $Y$ ) then $\omega$ is exact (on $X$ ). It should follow that $Q^n_!$ induces injective maps $H^n_{\mathrm{dR}}(X)\rightarrow H^n_{\mathrm{dR}}(Y)$ , provided every de Rham class on $X$ has a $Q$ -invariant representative. My 1st question is whether the above is valid and makes sense? My 2nd question is whether the last condition above is satisfied, and how to show this?
I.e., for every closed $n$ -form $\omega$ on $X$ , is there an $Q$ -invariant (i.e. invariant under deck transformations of $Q$ ) closed $n$ -form $\omega'$ on $X$ , which is cohomologous with $\omega$ (i.e. their difference is exact)? Or (more weakly?) can we show the inclusion of the $Q$ -invariant de Rham complex into $X$ 's full de Rham complex is a quasi-isomorphism (induces isomorphisms on all cohomology groups)? Update : Here is my thinking thus far on the 2nd question: Let us further assume: that the group $G$ of deck transformations is a topological group, acting continuously on $X$ , and it admits a Haar measure $\mu$ with finite total volume; and that each $g \in G$ induces the identity (not just an isomorphism) on the de Rham cohomology of $X$ . Then, given a closed $\omega \in \Omega^n_{\text{closed}}(X)$ , we would like to define a form via an integral of a $G$ -dependent form over $G$ , $$
\omega' = \frac{1}{|G|}\int_{g\in G} (g^*\omega)\,d\mu(g) \,,
$$ which we expect to be $G$ -invariant by the usual arguments.
Here $|G| = \mu(G)$ is the finite volume of $G$ .
Furthermore, since for each $g\in G$ we know $g^*$ induces the identity on $H^n_{\mathrm{dR}}(X)$ it follows that $\omega$ is cohomologous with $g^*\omega$ , so also $\omega = \frac{1}{|G|}\int_{g\in G}\omega\,d\mu(g)$ is cohomologous with $\omega' = \frac{1}{|G|}\int_{g\in G} (g^*\omega)\,d\mu(g)$ . Would this be valid? Are the above assumptions necessary? The 2nd assumption bothers me, since e.g. this is not satisfied by $S^2 \xrightarrow{\text{mod $\mathbb{Z}/2\mathbb{Z}$}} \mathbb{R}\mathbb{P}^2$ .","['solution-verification', 'de-rham-cohomology', 'smooth-manifolds', 'differential-geometry']"
4487652,An odd series: $\sum_{n \geq 0} \cos\left(\frac{\pi}{4}\alpha^n\right)$,"Let $\alpha = 7+4\sqrt{3}$ . Consider the series $$
\sum_{n\geq 0}\cos\left(\frac{\pi}{4}\alpha^n\right)
$$ I'm having a doubt. On one hand, I've plotted (I think) the evolution of the partial sum $\left(S_n\right)$ and I've obtained: So I've assumed that it was divergent. However, it can be easily shown that $$
\frac{1}{2}\left(\alpha^n+\frac{1}{\alpha^n}\right) = 2p_n+1
$$ where $p_n$ is a sequence positive integers. As a result $$
\cos\left(\frac{\pi}{4}\alpha^n\right) = \cos\left(\frac{\pi}{4}\left[4p_n+2-\frac{1}{\alpha^n}\right]\right) = \cos\left(\frac{\pi}{2}+\pi p_n -\frac{\pi}{4\alpha^n}\right)=\left(-1\right)^{p_n}\sin\left(\frac{\pi}{4\alpha^n}\right)
$$ Noting $\alpha \approx 14 > 1$ , I've concluded $$
\left|\cos\left(\frac{\pi}{4}\alpha^n\right)\right| \underset{(+\infty)}{\sim}\frac{\pi}{4}\frac{1}{\alpha^n}
$$ which is then is the general term for a convergent series. So does it converge or diverge? Could someone point me in the right direction?","['convergence-divergence', 'sequences-and-series']"
4487676,"Is there a surjective local homeomorphism which is ""normal"" but not a covering map?","Let $X\xrightarrow{Q}Y$ be a surjective local homeomorphism, between topological spaces. Suppose $Q$ is ""normal"", in that for every $x,x' \in X$ , if $Q(x) = Q(x')$ , then there exists a self-homeomorphism $f \in\mathrm{Homeo}(X)$ such that $f(x) = x'$ and $Q \circ f = Q$ . Then, can $Q$ still fail to be a covering map?","['general-topology', 'covering-spaces']"
4487682,Why is Matt Parker approximation of the average of the highest value of M N-sided dice more and more wrong for higher M?,"In his video about the unexpected logic behind rolling multiple dice and picking the highest , Matt Parker, says that the formula in the end is $$\frac{m}{m+1}\times n+0.5$$ where $m$ is the number of dice, and $n$ the number of sides on the dice. I wrote a program that computes the actual values of the averages by simulating all the possible rolls for 20-sided dice (up to 8 dice because it takes too much more time after that). The values I get from my program are also very similar to Matt's values when he simulates the rolls, but those values differ by a greater value at every increase of $m$ . $m$ Matt's formula result Computed result Difference 1 10.500 10.500 0 2 13.833 13.825 0.008 3 15.500 15.487 0.013 4 16.500 16.483 0.017 5 17.166 17.145 0.021 6 17.642 17.617 0.025 7 18.000 17.970 0.030 8 18.277 18.244 0.033 Furthermore, if we take much higher values like $m=99$ , we get an average of $20.3$ , which is absurd because the average is then higher than the maximum rollable value. So to me, the $+0.5$ part is totally hand-waived and isn't correct from a mathematical point of view. So where is Matt wrong with the $+0.5$ part, and how to fix his formula so that it's accurate?","['average', 'statistics']"
4487699,What is the mathematical definition of the integrals seen in quantum mechanics?,"Given a separable Hilbert space, we can find a Hilbert basis,which is countable and each element can be expressed as an infinite linear combination with $l^2$ coefficients. Ok this is clear and well-known in maths. But in QM, they also use uncoutnable family of states and state that each element of the Hilbert can be written as an integral of those with ""coefficients"" a function in $L^2$ My question : what is the name of this, mathematically ? It is not an instance of Lebesgue integral, since the function we are integrating is not complex valued.","['quantum-mechanics', 'functional-analysis']"
4487711,Write the Radon-Nikodym derivative on a subset $Y$ as a sum of the derivative over the entire space,"Let $f:X \to X$ a locally invertible non-singular* map of the Lebesgue space [1] $(X,\mathcal{B},m)$ and a partition $\{X_i\}$ of $X$ with $m(X_i)<\infty$ and $\mathcal{B}$ is the smallest $\sigma$ -algebra containing $\bigcup f^{-n}(X_i)$ which is complete with respect to $m$ . Define $$(m\circ f)(A):=\sum_{i=0}^{\infty}m(f(A \cap X_i)).$$ We have that $m\ll m\circ f$ so let $g=\frac{dm}{dm\circ f}$ be the Radon-Nikodym derivative. Let $Y\subset X$ be a measurable subset, we can restrict the map and the measures in this subspace, so that $g_{Y}=\frac{dm_Y}{dm_Y \circ f|_Y}$ . Now I need to show the identity, $$\log{g_Y}=\sum_{i=0}^{\tau_Y-1}{\log{g\circ f^i}}\tag{1}\label{eq}$$ where $\tau_Y=\inf{\{n\ge1:f^n \in Y\}}$ is the first return time. A measurable map $f:X\rightarrow X$ on a measure space $(X,\mathcal{B},m)$ is nonsingular if $m(f^{-1}(A))=0$ for all $A\in \mathcal{B}$ such that $m(A) = 0$ My first attempt was to work "" backwards "" so that I could see how to get to $\eqref{eq}$ , but I only managed to reach the expression $$g_Y = \prod_{i=0}^{\tau_Y-1}g\circ f^i.$$ This expression is very similar to an exercise in Viana's book, Foundations of Ergodic Theory . Exercise 9.7.5 , but I couldn't solve it. I believe that by the uniqueness of the Radonâ€“Nikodym derivative, in $Y$ we have to have the equality $g = g_Y$ , but I don't know how this can be useful to me. Does anyone have any idea how to find this equality? And is she known? It seems to me that yes, it is very similar to the definition of entropy....","['measure-theory', 'lebesgue-measure', 'ergodic-theory', 'real-analysis', 'radon-nikodym']"
4487724,"Spivak, Ch. 14, Prob 12d : Prove if $f'$ is periodic with period $a$ and $f$ is periodic (with some period not necessarily $=a$), then $f(a)=f(0)$.","In Spivak's Calculus , there is the following problem in Ch. 14 ""Fundamental Theorem of Calculus"" A function $f$ is periodic, with period $a$ , if $f(x+a)=f(x)$ for all $x$ . *(d) Prove that if $f'$ is periodic with period $a$ and $f$ is periodic (with some period not necessarily $=a$ ), then $f(a)=f(0)$ . The solution manual solution (with my filling in some intermediate steps) is as follows Let $g(x)=f(x+a)-f(x)$ . Then $g'(x)=f'(x+a)-f'(x)=0$ , because $f'$ is periodic. Therefore, $g$ is constant and $g(x)=g(0)$ , so $$f(x+a)-f(x)=f(a)-f(0)$$ $$f(x+a)=f(x)+f(a)-f(0)$$ $$f(na)=f[(n-1)a]+f(a)-f(0)$$ $$=f[(n-2)a+a]+f(a)-f(0)$$ $$=f[(n-2)a]+2f(a)-2f(0)$$ $$=f(0)+nf(a)-nf(0)$$ $$=n(f(a)-f(0))+f(0)$$ Since $f$ must be bounded (since it is periodic), then $f(a)=f(0)$ ,
otherwise $f$ would be unbounded. Is there an interesting interpretation for this result? f apparently can have some period $b\neq a$ which we didn't need to make use of in the proof above. Doesn't the fact that $g$ is constant mean that $f$ has a period of $a$ ? $$g(x)=f(x+a)-f(x)=0, \text{ for all } x $$ $$\implies f(x+a)=f(x), \text{ for all }x$$ When I tried to solve this problem I started on the assumption that $f$ had a period $b\neq a$ $$f'(x+a)=f'(x)$$ $$f(x)=\int_0^x f' + f(0)$$ $$f(x+b)=\int_0^{x+b}fâ€™ +f(0)=f(x)$$ But didn't get much further than this.","['integration', 'periodic-functions', 'derivatives', 'real-analysis']"
4487727,Determined vs Uniquely Determined [duplicate],"This question already has an answer here : Meaning of expression : ""uniquely determined"" (1 answer) Closed 1 year ago . What is the difference between using the term â€˜determinedâ€™ vs â€˜uniquely determinedâ€™? The definition supplied by Google is Mathematics: Specify the value, position, or form of (a mathematical or geometric object) uniquely . It seems as though the uniqueness is built into the definition of the word determined, however I only ever see the phrase 'uniquely determined'. For example, should the following statement be interpreted the same, regardless of if we include 'uniquely' or not? The value of a multilinear function at some point in the domain is (uniquely) determined by the values of the function at all combinations of basis vectors. If the two statements are equivalent, is there a reason adding 'uniquely' is seemingly more common?","['definition', 'functions', 'soft-question']"
4487819,"Game with $n$ players, respective chances of winning however many games in succession","A number of persons $A$ , $B$ , $C$ , $D$ , $\ldots$ play at a game, their chances of winning any particular game being $\alpha$ , $\beta$ , $\gamma$ , $\delta$ , $\ldots$ respectively. The match is won by $A$ if he gains $a$ games in succession: by $B$ if he gains $b$ games in succession; and so on. The play continues till one of these events happens. How do I see that their chances of winning the match are proportional to $${{(1 - \alpha)\alpha^a}\over{1 - \alpha^a}},{{(1 - \beta)\beta^b}\over{1 - \beta^b}}, {{(1 - \gamma)\gamma^c}\over{1 - \gamma^c}}, \text{etc.}$$ and that the average number of games in the match will be $$1/\left({{(1 - \alpha)\alpha^a}\over{1 - \alpha^a}} + {{(1 - \beta)\beta^b}\over{1 - \beta^b}} + {{(1 - \gamma)\gamma^c}\over{1 - \gamma^c}} + \ldots\right)?$$","['markov-chains', 'expected-value', 'combinatorics', 'algebra-precalculus', 'probability']"
4487825,Finding supersingular elliptic curves over $\Bbb F_p$ with rational endomorphisms of degree 3 using Deuring correspondence,"I found this question on this website, which gives a code that almost does what I want. Basically, I am looking for supersingular elliptic curves $E/\mathbb F_p$ that have endomorphisms of degree 3, 6 or (worst case) 9, such that those endomorphisms' kernel consists of points defined over $\mathbb F_p$ . I thought about using Deuring correspondence to do so, by designing a suitable maximal quaternion order and then finding a corresponding elliptic curve. This is very close to what the code here does : def j_with_embedding(p, q):
 â€ƒF = GF(p) 
 â€ƒR.<x> = PolynomialRing(F) 
 â€ƒK = QuadraticField(-q) 
â€ƒ o = K.maximal_order() 
â€ƒ d = o.discriminant() 
â€ƒ H = hilbert_class_polynomial(d)
â€ƒ return R(H).roots(multiplicities=False) so I tried to run it while keeping $q = 3$ , but for the endomorphisms that I find, the condition that the kernel has only points in $\mathbb F_p$ is never satisfied. I guess this is because the criteria ""E must have an endomorphism $\phi$ such that $\phi^2 = [-3]$ "" is too restrictive. I thought about changing q = 3 for q = -3, but I get an error at the line H = hilbert_class_polynomial(d) which is D (=12) must be negative How can I adapt this code to get the j-invariants at the end ? Thanks in advance :)","['sagemath', 'elliptic-curves', 'algebraic-geometry', 'isogeny', 'quaternions']"
4487856,"Let $R$ be a ring with unity and $u\in R$.If there is a unique $v\in R$ such that $uv=1_{R}$,show that $vu=1_{R}$,and therefore u is a unit.","My try and where I am stuck : (note:A zero-divisor is a nonzero element $a$ of a commutative ring R such that there is a nonzero element $b\in R$ with $ab=0_{R}$ ) Proof: Assume $uv=1_{R}$ then clearly ( $u\ne 0_{R}$ ) , ( $v\ne 0_{R}$ ) and  ( $uv$ is non zero divisor because $uv=1_{R}$ ). now ( $v \cdot(uv)=v \cdot 1_{R}=v$ ) (Multiplying $uv=1_{R}$ by $v$ from left.) $\rightarrow$ ( $vuv=v)$ $\rightarrow$ ( $vuv-v=0_{R}$ ) (Subtracting $v$ from both sides) $\rightarrow$ $(vu-1_{R})v=0_{R}$ (By right distributive law) $\rightarrow$ $(vu-1_{R}=0_{R}$ which is $vu=1_{R} )$ because $v\neq0_{R}$ .Here is my problem : How to remove the probability that $v\neq 0_{R}$ may be zero divisor and thus $vu \neq 1_{R}$ ?","['ring-theory', 'abstract-algebra']"
4487862,$ \int \frac{\sin^{8}(x)}{\sin^{8}(x) + \cos^{8}(x)} dx $,"$$ \int \frac{\sin^{8}(x)}{\sin^{8}(x) + \cos^{8}(x)} dx $$ is there more efficient and elegant way? Other than the solution below. a solution: Let $ I = \int \frac{\sin^{8}(x)}{\sin^{8}(x) + \cos^{8}(x)} dx $ , $$ \int \frac{\sin^{8}(x)}{\sin^{8}(x) + \cos^{8}(x)} dx  = \int \frac{\sin^{8}(x) + \cos^{8}(x) - \cos^{8}(x)}{\sin^{8}(x) + \cos^{8}(x)} dx = x - \int \frac{\cos^{8}(x)}{ \sin^{8}(x) + \cos^{8}(x) }dx $$ so we have $2I$ is: $$ 2I = x + \int \frac{\sin^{8}(x)-\cos^{8}(x)}{ \sin^{8}(x) + \cos^{8}(x) }dx $$ Now, $$ \sin^{8}(x) - \cos^{8}(x) = \left[ \sin^{4}(x) - \cos^{4}(x) \right] \left[ \sin^{4}(x) + \cos^{4}(x) \right]  $$ $$ = \left[ \sin^{2}(x) - \cos^{2}(x) \right]\left[ \sin^{2}(x) + \cos^{2}(x) \right] \left[ (\sin^{2}(x) + \cos^{2}(x))^{2} - 2 \sin^{2}(x)\cos^{2}(x) \right]  $$ $$ = \left[ \sin^{2}(x) - \cos^{2}(x) \right] \left[1 - 2 \sin^{2}(x)\cos^{2}(x) \right] $$ For the denominator: $$ \sin^{8}(x) + \cos^{8}(x) = \left[ \sin^{4}(x) + \cos^{4}(x) \right]^{2} - 2 \sin^{4}(x)\cos^{4}(x) $$ $$ =  \left[ (\sin^{2}(x) + \cos^{2}(x))^{2} - 2 (\sin(x)\cos(x))^{2} \right]^{2} - 2 \sin^{4}(x)\cos^{4}(x) $$ $$ = \left[1 - 2 (\sin(x)\cos(x))^{2} \right]^{2} - 2 (\sin(x)\cos(x))^{4} $$ Now $$ 2I =x+ \int \frac{ \left[ \sin^{2}(x) - \cos^{2}(x) \right] \left[1 - 2 \sin^{2}(x)\cos^{2}(x) \right] }{\left[1 - 2 (\sin(x)\cos(x))^{2} \right]^{2} - 2 (\sin(x)\cos(x))^{4}} dx $$ Let $U = \sin(x) \cos(x)$ , then $U'(x) = \cos^{2}(x) - \sin^{2}(x)$ , so $$ 2I = x- \int \frac{\left[1 - 2 U^{2} \right] }{\left[1 - 2 U^{2} \right]^{2} - 2 U^{4}} dU $$ $$ =  x- \int \frac{\left[ 2 U^{2}  - 1 \right] }{ (1 - (2 + \sqrt{2})U^{2})(1 + (\sqrt{2} - 2)U^{2})} dU $$ And from here I can continue using Partial Fraction: $$- \int \frac{\left[ 2 U^{2}  - 1 \right] }{ (1 - (2 + \sqrt{2})U^{2})(1 + (\sqrt{2} - 2)U^{2})} dU = -\frac{1}{2}\int \frac{1}{ (1 - (2 + \sqrt{2})U^{2})} + \frac{1}{(1 - (2 - \sqrt{2})U^{2})} dU $$ and then for each of the 2 terms we use Partial Fraction again. First, let $\alpha = \sqrt{2+\sqrt{2}}U$ , $\beta = \sqrt{2-\sqrt{2}}U$ , so we have $$ = -\frac{1}{2\sqrt{2+\sqrt{2}}}\int \frac{1}{ 1 - \alpha^{2}} d \alpha - \frac{1}{2\sqrt{2-\sqrt{2}}}   \int \frac{1}{(1 - \beta^{2})} d \beta $$ then we have $$ = -\frac{1}{2\sqrt{2+\sqrt{2}}} \left( \int \frac{1/2}{1-\alpha} d \alpha + \int \frac{1/2}{1+\alpha} d\alpha \right) - \frac{1}{2\sqrt{2-\sqrt{2}}} \left(   \int \frac{1/2}{1-\beta } d \beta +  \int \frac{1/2}{1 + \beta } d \beta \right) $$ $$ = -\frac{1}{4\sqrt{2+\sqrt{2}}} \left( -\ln(1-\alpha) + \ln(1+\alpha) \right) - \frac{1}{4\sqrt{2-\sqrt{2}}} \left(   -\ln(1-\beta) + \ln(1+\beta)  \right)  $$","['integration', 'indefinite-integrals', 'trigonometry', 'trigonometric-integrals']"
4488066,Conditions for chain rule for Gateaux derivatives,"Let $X,Y,Z$ be locally convex topological vector spaces over $\mathbb R$ (not necessarily Banach), $D_X \subseteq X$ , $D_Y \subseteq Y$ and let $f \colon D_X \to D_Y$ , $g \colon D_Y \to Z$ . Let us assume that $f$ is Gateaux-differentiable in some $x \in D_X$ and $g$ is Gateaux-differentiable in $f(x)$ . Which are the least well-known additional conditions for the chain rule to hold? By the chain rule I mean the assertion that $g \circ f$ is Gateaux-differentiable and $\mathrm d(g \circ f)(x;\cdot) = \mathrm d g(f(x);\mathrm d f(x;\cdot))$ . Wikipedia says that continuity of the derivatives of $f$ and $g$ is required, but it does not specify whether this means that only $\mathrm df \colon D_X \times X \to Y$ should be continuous or, more than that, $x \mapsto \mathrm df(x,\cdot) \in L(X,Y)$ should be continuous (and, of course, the same for $g$ ). And is it generally assumed that $D_X$ and $D_Y$ are open or is it enough to know that they are neighborhoods of $x$ and $f(x)$ ? Please also help me to find a citable reference. I looked into some books, but have not found this chain rule. Since this question got no answers after a few days, I also posted it on mathoverflow .","['gateaux-derivative', 'reference-request', 'functional-analysis', 'derivatives', 'chain-rule']"
4488071,A question about Lebesgue measurable function,"Can you guys help me with this question? I've been thinking over it all day long. Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a Lebesgue measurable function, and there exist two constants $a,b$ $\in \mathbb{R} $ $s.t.$ for $\forall$ $l,n$ $\in \mathbb{Z}$ that are all not equal to zero, $la+nb \neq 0$ and $ f(x) \doteq f(x+a), f(x) \doteq f(x+b)$ . Prove that there exists a constant c $\in \mathbb{R} s.t. f \doteq c$ . Dotequal means ""equality holds a.e. by Lebesgue measure"" here.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4488097,Preimage of discontinuous function,"Let $f:[0,1]\to\mathbb{R}$ which is defined by \begin{align}
f(x)=\begin{cases}
1,\quad x=1,\\
0,\quad x\in[0,1).
\end{cases}
\end{align} I want to find $f^{-1}(U)$ for any open set $U\subseteq \mathbb{R}$ . But, I find difficulties when I try to do that.
This is what I have done so far. Let $(a,b)$ be any open set in $\mathbb{R}$ with $a<b$ . Case I: if $a<b<0$ , then $f^{-1}((a,b))=\emptyset$ . Case II: if $a<0<b$ , then \begin{align}
f^{-1}((a,b))=f^{-1}((a,0))\cup f^{-1}([0,b))=f^{-1}([0,b))
\end{align} because $f^{-1}((a,0))=\emptyset$ . Then, for $f^{-1}([0,b))$ , I divided it into two cases again, if $b\leq 1$ , then $f^{-1}([0,b))=[0,1)$ , if $b>1$ , then $f^{-1}([0,b))=f^{-1}([0,1))\cup f^{-1}([1,b))=[0,1)\cup\{1\}=[0,1]$ . Therefore, \begin{align}
f^{-1}((a,b))=\begin{cases}
[0,1),\quad b\leq 1,\\
[0,1],\quad b>1.
\end{cases}
\end{align} Case III: if $0<a<b$ , I divided it into two cases again, if $b\leq 1$ , then $f^{-1}((a,b))=\emptyset$ , if $b>1$ , then $f^{-1}((a,b))=\{1\}$ . Is it correct? Or there are another various possible preimages that I missed?
Thanks for any advice.","['elementary-set-theory', 'functions']"
4488102,"Prove that $[0, \infty)$ is not order isomorphic to $(\frac{1}{2}, \infty)$.","Prove that $[0, \infty)$ is not order isomorphic to $(\frac{1}{2}, \infty)$ . My attempt: Let $f: [0, \infty)\rightarrow (\frac{1}{2}, \infty)$ is an order isomorphism. Let $a= f(0) \in (\frac{1}{2}, \infty)$ . Now choose $b$ such that $(\frac{1}{2}<b<a$ . Now  I can  not proceed further. How to construct a contradiction?","['functions', 'real-analysis']"
4488118,Polyhedron with only Ï€/4 dihedral angles,"I have a puzzle to construct a polyhedron where the measure of every dihedral angle is $\frac\pi4$ radians or prove that none can exist.  The polyhedron may self-intersect however it must be finite, bounded, and dyadic as well as have planar faces. Here's a quick glossary of terms as I use them: Polyhedron: A polytope in Euclidean 3-space. Finite:  There are only a finite number of elements in the polyhedron. Bounded: There exists a sphere of finite radius which encloses every vertex of the polyhedron. Dyadic : Each edge is incident on exactly two faces. Planar faces: For every face all vertices of that face lie on some plane. These are all pretty standard when talking about polyhedra. As an example, if we were to imagine a 2-dimensional version of this problem, a regular octagram ( SchlÃ¤fli symbol $\{8/3\}$ ) would be a solution.  A square bowtie would be another solution ( illustration ). Both are polygons with all interior angles measuring $\frac\pi4$ radians. Of course a polyhedron is tougher since there is more freedom to arrange things in 3 space.","['polyhedra', 'geometry']"
4488150,Do these functional equations imply linearity?,"$f:[0,1] \rightarrow [0,1]$ is a strictly decreasing function (i.e. $f(x+\delta)<f(x)\;\forall\;x \in[0,1), \delta>0$ ), such that $f^{-1}=f$ . Further, $f(x)+f(1-x)=1\;\forall\;x\in[0,1]$ . Edit: I earlier said ""There is no requirement of continuity"". However, while there is no explicit continuity requirement, I think these properties imply continuity as pointed out in the comments. Does this imply $f$ must be linear? My approach: I drew the pictures. Due to $f^{-1}=f$ , $f$ must look the same when we rotate the page by $90^{o}$ so that the y-axis becomes the x-axis. By strict decreasingness, it must be strictly above $1/2$ for $x<1/2$ , equal to $1/2$ for $x=1/2$ (by $f(x)+f(1-x)=1$ ) and strictly below $1/2$ for $x>1/2$ . In addition, due to $f(x)+f(1-x)=1$ , when you reflect it around the $y=1/2$ line and then the $x=1/2$ line, you get back the same function. To me it seems this means $f$ in $[1/2,1]$ is a replica of $f$ in $[0,1/2]$ except ""dragged down"", I think by $1/2$ but I'm not sure. i.e. $f(x)=f(x-1/2)-1/2$ for $x>1/2$ , but I'm not sure. To me it seems  the replica property combined with the rotational symmetry property described above, indicates that this can't hold unless $f$ is linear. Obviously, the above is vague, and I'm not even sure it goes loosely in the right direction. Any help is most appreciated.","['symmetry', 'functions', 'graphing-functions']"
4488164,"How can I find the asymptotics of $h\int_0^{\pi}\frac{\sin^2(i\pi-he^{it})}{\sinh^2(he^{it})}dt$, $h\to0$, to evaluate a certain contour integral?","$\newcommand{\d}{\,\mathrm{d}}\newcommand{\sech}{\operatorname{sech}}\newcommand{\csch}{\operatorname{csch}}$ It was recently asked, and then deleted, how to evaluate the following using contour integration: $$I=\int_0^\infty\frac{\sin^2(x)}{\sinh^2(x)}\d x\overset{?}{=}\frac{\pi\coth\pi-1}{2}$$ There is a simple real method, and I credit @KStar for finding the series expansions: If $f(x)=-\frac{2}{1+e^{2x}}$ , then $f'(x)=\sech^2(x)$ and by expanding $f$ as a geometric series we find, for $x\gt0$ : $$\sech^2(x)=4\sum_{n=1}^\infty(-1)^{n-1}n\cdot e^{-2nx}$$ And letting $x\mapsto x+i\pi/2$ yields: $$\csch^2(x)=4\sum_{n=1}^\infty n\cdot e^{-2nx}$$ For $x\gt0$ . Then: $$\begin{align}\int_0^\infty\frac{\sin^2(x)}{\sinh^2(x)}\d x&=2\sum_{n=1}^\infty n\cdot\int_0^\infty(1-\cos(2x))e^{-2nx}\d x\\&=2\sum_{n=1}^\infty n\cdot\left(\frac{1}{2n}-\frac{2n}{4n^2+4}\right)\\&=\sum_{n=1}^\infty\frac{1}{n^2+1}\\&=\frac{\pi\coth\pi-1}{2}\end{align}$$ By the Mittag-Leffler expansion of $\coth$ , or equivalently via an argument using the digamma function. The new user who posted and subsequently deleted their question suggested taking a rectangular contour, limiting in $R\to\infty$ on the rectangle with base $-R\to R$ , of height $i\pi$ , and with a semicircular inward indent around the point $i\pi$ , say of radius $\varepsilon$ . The integrand $f(z)=\frac{\sin^2z}{\sinh^2z}$ is holomorphic on the boundary and interior of this contour, so the integrals over all paths sum to zero. Moreover the small strips $R\to R+i\pi,-R\to-R+i\pi$ obviously vanish. We have: $$\begin{align}\sin(x+i\pi)&=\sin(x)\cosh(\pi)+i\sinh(\pi)\cos(x)\\\sin^2(x+i\pi)&=\sin^2(x)\cosh^2(\pi)-\cos^2(x)\sinh^2(\pi)+2i\sin(x)\cos(x)\sinh(\pi)\cosh(\pi)\\&=\sin^2(x)(1+2\sinh^2(\pi))-\sinh^2(\pi)+2i\sin(x)\cos(x)\sinh(\pi)\cosh(\pi)\end{align}$$ For asymptotically large $R$ , asymptotically small $\varepsilon\gt0$ we then need to use: $$0=o(1)+I-\int_{[-R,-\varepsilon]\cup[\varepsilon,R]}\frac{\sin^2(x)(1+2\sinh^2(\pi))-\sinh^2(\pi)}{\sinh^2(x)}\d x+i\varepsilon\int_{-\pi}^0\frac{\sin^2(i\pi+\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}e^{it}\d t\\\overset{R\to\infty}{\longrightarrow}-2\sinh^2(\pi)\cdot I+\\\lim_{\varepsilon\to0^+}\left[2\sinh^2(\pi)(1-\coth(\varepsilon))+i\varepsilon\int_{0}^\pi\frac{\sin^2(i\pi-\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}e^{it}\d t\right]$$ The original asker claimed that it is possible to use this method to calculate the final answer (they didn't go as far, but it was implied). Taking this on good faith, I will assume this works - but I am very uncertain how to do this. The limit of the semicircular integral is, well, nasty - I'd appreciate help with the asymptotics here. I am fairly certain my calculations thus far are correct, but I unfortunately do not own a copy of Mathematica or equivalent to help me here. It is my (purely intuitive) suspicion that the limit does not exist, but this is weird since the limit must exist, as all the other limits do and the total limit of zero obviously exists. To present a concrete target - to get the correct evaluation, we need to show that: $$\begin{align}2\sinh^2(\pi)(I-1)&=\pi\sinh(\pi)\cosh(\pi)-3\sinh^2(\pi)\\&\overset{?}{=}\lim_{\varepsilon\to0^+}\left[i\varepsilon\int_0^\pi\frac{\sin^2(i\pi-\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}\d t-2\sinh^2(\pi)\coth(\varepsilon)\right]\end{align}$$","['complex-analysis', 'contour-integration', 'asymptotics']"
4488232,Does there exist a sequence of step functions which converge pointwise to the Dirichlet function?,"There is already a similar question here on MSE except the op asked for an increasing sequence. This is quickly seen to be false, so I'm asking about a sequence which is not necessarily increasing. My intuition seems to be that there is no such sequence and I've tried to prove it but failed. The argument I was think of is that given a particular $x$ in $Q$ if a step function converges to it then for an irrational point ""near enough"" $x$ it shouldn't; however, since pointwise convergence depends on the point x (i.e. finding N depends on x) and the sequence of step functions isn't necessarily increasing, I got stuck in attempting to prove it. Perhaps my intuition is wrong and I would appreciate help answering this question.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4488247,Is 'Jensen type inquality' true in the following form $h\circ(g*f)\leq g* (h\circ f)$,"Given a convex function $h$ and some other  integrable functions $g$ and $f$ , is it true that $$h\circ(g*f)\leq g* (h\circ f)\;?$$ If not, which additional assumption would lead to the statement holding? Note that $\circ$ denotes composition, while $*$ denotes convolution. My idea: $$ h\bigg(\int_{0}^{t}g(t-s)f(s)ds\bigg)=h\bigg(\int_{0}^{t}f(s)\mu_{g}(ds)\bigg),$$ where $\mu_{g}$ is the measure with density $g(s)$ with respect to the Lebesgue measure. I assume now we can apply Jensen, and then we are done?","['integration', 'real-analysis', 'functional-analysis', 'inequality', 'convex-analysis']"
4488249,A step in a proof that a set is Lebesgue measurable if and only if its translation is,"I'm trying to fill in the gaps in the following proof: In the above, $\mu^*$ refers to the Lebesgue outer measure , the CarathÃ©odory definition of measurability states that a set $A$ is CarathÃ©odory measurable if for any subset $E$ of $\mathbb{R}^n$ one has $$\mu^*(E)=\mu^*(E\cap A) + \mu^*(E\cap A^c),$$ and a set is defined as measurable if it belongs to the $\sigma$ -algebra formed by all the CarathÃ©dory measurable subsets of $\mathbb{R}^n$ (equivalently, if it is CarathÃ©odory measurable). I understand why $\mu^*(A+h)=\mu^*(A)$ , but I fail to understand why a set $A$ is measurable if and only if so is $A+h$ .
What we wish to prove is that, if $A$ is measurable, then for any subset $E$ of $\mathbb{R}^n$ we have that $$\mu^*(E)=\mu^*(E\cap (A+h)) + \mu^*(E\cap (A+h)^c),$$ but I do not see how the fact that $$(E+h)\cap (A+h) = (E\cap A) + h$$ is helpful here.","['invariance', 'measure-theory', 'lebesgue-measure', 'outer-measure']"
4488289,"A continuous function on $[0,1]$ orthogonal to each monomial of the form $x^{n^2}$","Let us consider the continuous functions over $[0,1]$ fulfilling $$ \int_{0}^{1} f(x) x^n\,dx = 0 $$ for $n=0$ and for every $n\in E\subseteq\mathbb{N}^+$ . The MÃ¼ntzâ€“SzÃ¡sz theorem gives that $$ \sum_{n\in E}\frac{1}{n} = +\infty \Longleftrightarrow f(x)\equiv 0 $$ so there is a non-zero continuous function $f(x)$ such that $$ \int_{0}^{1} f(x) x^{n^2}\,dx = 0 \tag{1}$$ holds for every $n\in\mathbb{N}$ . Question : can we construct a nice, explicit function $f\neq 0$ fulfilling $(1)$ for every $n\in\mathbb{N}$ ? We may consider functions of the form $$ f(x) = \sum_{n\geq 0} c_n P_n(2x-1) $$ with $P_n(2x-1)$ being the $n$ -th shifted Legendre polynomial. The orthogonality to $1$ and $x$ translates into $c_0=c_1=0$ , the orthogonality to $x^4$ translates into $\frac{2}{35}c_2+\frac{1}{70}c_3+\frac{1}{630}c_4=0$ , the orthogonality to $x^9$ translates into $\frac{3}{55}c_2+\frac{21}{715}c_3+ \frac{9}{715}c_4 +\frac{3}{715}c_5+\frac{3}{2860}c_6+\frac{9}{48620}c_7+\frac{1}{48620}c_8+\frac{1}{923780}c_9=0$ and so on. The minimal (with respect to the $\ell^2$ norm) solution of this infinite system with $c_2=1$ (or $c_4=1$ ) should give a sequence $\{c_n\}_{n\geq 0}$ ensuring the continuity of $f(x)$ , but this is non-trivial and I would appreciate a more explicit construction / example of such $f$ . Addendum : another possible construction is to apply the Gram-Schmidt process to $\{1,x,x^4,x^9,\ldots\}$ in order to get a sequence of polynomials $\{p_n(x)\}_{n\geq 0}$ such that $p_n(x)=\sum_{k=0}^{n} c_k x^{k^2}$ $n\neq m \Longrightarrow \langle p_n(x),p_m(x)\rangle = 0$ $\max_{x\in [0,1]} |p_n(x)| = 1$ or $\langle p_n(x),p_n(x)\rangle = 1$ then take $f(x)$ as the pointwise limit of a convergent subsequence of $\{p_n(x)\}_{n\geq 0}$ . Still, not really explicit. A more promising approach is to consider some lacunary Fourier series, like $$ g(\theta) = \sum_{n\geq 1}\frac{\cos(n\theta)}{n^2} - \sum_{n\geq 1}\frac{\cos(n^2\theta)}{n^4}, $$ which clearly fulfills $\int_{-\pi}^{\pi}g(\theta)\cos(n^2\theta)\,d\theta = 0$ , then turn such $g(\theta)$ into an $f(x)$ fulfilling $(1)$ via some slick substitution. Yet another way is to consider the inverse Laplace transform of $$ \frac{1}{s}\prod_{k=0}^{n}\frac{k^2+1-s}{k^2+1+s} $$ evaluated at $-\log x$ . This gives a polynomial, bounded between $-1$ and $1$ , which is orthogonal to $1,x,x^4,\ldots,x^{n^2}$ . Is this sequence of polynomials (or a subsequence of this sequence) convergent to a continuous function? I do not know. If so, $$ f(x)=\mathcal{L}^{-1}\left(\frac{\sin(\pi\sqrt{s-1})}{\sqrt{s-1}}\cdot\frac{\sqrt{s+1}}{\sin(\pi\sqrt{s+1})}\cdot \frac{1}{s}\right)(-\log x)$$ is an explicit solution. Here it is a plot of the first polynomials produced by the last approach:","['weierstrass-approximation', 'orthogonal-polynomials', 'sequences-and-series']"
4488295,Question regarding order of evaluation of elements in Group Action example,"I'm learning about group actions (just the basics right now) and an example has come up in the book for which I do not see how the algebra works the way it does. I'm sure it's something simple I'm overlooking but would like some clarification, nevertheless. The example is set up as follows: Suppose $G$ acts on a set $X$ . For any two sets $X$ and $Y$ recall that $Y^X$ is the set of functions from $X$ to $Y$ . If $G$ acts on $X$ , we claim that $$g \cdot f(x) = f(g^{-1} \cdot x)$$ defines a group action of $G$ on $Y^X$ Now in proving the second property (the non-identity one) from the function definition of a group action we see that for $g,h \in G$ and $f \in Y^X$ $$\begin{align}
g \cdot (h \cdot f)(x) &= h \cdot f(g^{-1} \cdot x) \\
&= f(h^{-1} \cdot (g^{-1} \cdot x)) \\
&= f(h^{-1}g^{-1} \cdot x) \\
&= f((gh)^{-1} \cdot x)\\
& = gh \cdot f(x).
\end{align}$$ Now, why is it that we don't get, for the second expression (after the first equality) that $g \cdot f(h^{-1} \cdot x)$ ? I ask because up to this point every expression in the text has been evaluated right to left, however this seems to be evaluated left to right in terms of the elements of $G$ in acting on the element of $Y^X$ .","['functions', 'group-actions', 'group-theory', 'abstract-algebra']"
4488327,Number of ways to place $n$ rooks on $n$ by $n$ board such that they don't hit each other. But someone has cut out squares of one of diagonals.,"I was thinking about asking this is chat because I thought that my concern is not serious enough, but it became too long so here I am. I hope that the question is clear: you just can't place rooks on squares with coordinates $(1, 1), 2, 2)$ and so on. I will call these squares forbidden. For small board sizes the answer is:
2x2 - 1,
3x3 - 2,
4x4 - 9 (if I haven't made a mistake which I'm not so sure about). This problem is harder than it felt at the first glance. What makes it harder is the fact that after placing a rook on the 1st line and figuring out ways for all the lower lines, these lower lines are not equal: one of them already has forbidden square on a column on which you placed the 1st line rook, so nothing is changed for this line, but for other lines you have lessen the number of vacant places by one. To make it more clear, think of coordinates as index of a matrix element, and say: forbidden squares are $(1, 1), (2, 2), \ldots$ , you place a rook on square $(1, 2)$ , and thus squares $(2, 2), (3, 2), (4, 2), \ldots$ are now closed, but $(2, 2)$ was already closed before that, so nothing changed. My question is: can you evaluate ""offhand"" if it would be hard to solve on a computer? By solving I mean continue the sequence $1, 2, 9, \ldots$ for increasing chessboard size. And secondly: is such a programm really would be easier to write on python than c/c++? To me it seems like an easy thing for a computer to calculate, but what if I'm missing something. I'm not even asking to solve this problem, but if you want then I don't mind. If you want to do it on a computer than I also would be very glad. But maybe this question is already known or explained in some book? I don't know because I came up with this problem myself, but I don't consider myself an expert in combinatorics, so maybe I just don't know something. It seems obvious that for any $n$ and arbitrary set of cut squares there can't be any algorythm. In any case, thank you a lot. Add:
After posting I realised why it seems easy to caltulate on a machine. In a ""worst case scenario"" you could just place $n$ rooks on a special board in any way possible and than add $+1$ if no one of them are hitting each other. Of course it would take a huge amount of time, but it should work. For regular board it would be $8\cdot7 = 56$ squares, $C_{56}^8$ seems like impossible number, but you could hugely decrease it by placing only one rook on a line. So it's just $7^8$ positions to check which is still a lot.","['python', 'derangements', 'chessboard', 'combinatorics', 'sequences-and-series']"
4488341,"If whenever $|x-a|<\delta$, $|y-a|<\delta$ then $|f(x)-f(y)|<\epsilon$, is this equiv. to $f$ cont. at $c$ then $f$ cont. on some interval around $c$?","Is the following theorem If $f$ is continuous at $a$ , then for any $\epsilon>0$ there is a $\delta>0$ so that whenever $|x-a|<\delta$ and $|y-a|<\delta$ , we have $|f(x)-f(y)|<\epsilon$ . equivalent to saying that if $f$ is continuous at a point $c$ then it is continuous on some interval around that point $c$ ?","['limits', 'calculus', 'continuity', 'epsilon-delta']"
4488347,Is $ SU_2(\Bbb Z[1/\sqrt{2}]) $ finite?,"Let $ G=SU_2(\mathbb{Z}[1/\sqrt{2}]) $ be the group of $ 2 \times 2 $ unitary determinant one matrices with entries whose real and imaginary parts are from $\mathbb{Z}[i,1/\sqrt{2}]$ . Is $ G $ finite? For $ SU_2(\mathbb{Z}[1/\sqrt{p}]) $ for $ p=3,5 $ etc I have already found some matrices of infinite order. But for $ p=2 $ I am having difficulty finding any matrices of infinite order or any other indication that $ G $ is infinite.","['matrices', 'group-theory', 'finite-groups', 'algebraic-groups']"
4488364,Let $\alpha$ be a regular curve of curvature and torsion not zero. Show that $\alpha$ is helix if and only if $\frac{k}{\tau}$ is constant.,"Definition. A regular curve $\alpha: I \to \mathbb R^3$ is a helix if there is a unit vector $v$ that forms a constant angle with $\alpha'(t), \forall t \in I$ . We can assume $\alpha$ parameterized by arc length. If $\alpha$ is a helix, then there is a unit vector $v$ such that $\langle \alpha'(s), v \rangle$ is constant. So $\langle \alpha''(s),v \rangle = 0$ , that is $k(s)\langle n(s), v
 \rangle = 0$ . As $k(s) \ne 0$ , it follows that $v$ belongs to the plan determined by $t(s)$ and $b(s)$ , for each $s \in I $ . So be $$ v= \cos\theta(s)\,t(s) + \sin\theta(s)\,b(s)$$ Differentiating and using Frenet's formulas, we get $$0 = -\sin\theta(s)\theta'(s)t(s) + (k(s)\cos\theta(s)+\tau(s)\sin\theta(s))n(s) + \cos\theta(s)\theta'(s)b(s)$$ Therefore, $\forall s \in I$ , $$\sin\theta(s)\theta'(s) = 0,$$ $$\cos\theta(s)\theta'(s) = 0,$$ $$k(s)\cos\theta(s) + \tau(s) \sin\theta(s) = 0$$ The first two equations determine $\theta'(s) = 0, \forall \in I$ . Therefore, $\theta(s)$ is constant. Also, the constant $\cos\theta$ is non-zero, otherwise we would have $\tau(s)=0$ , which contradicts the hypothesis. It follows from the third equality that $\frac{k}{\tau}$ is constant. Conversely, if $\frac{k}{\tau}$ is constant, we set $\theta$ such that $tg\theta = -\frac{k}{\tau}$ . So $$v=cos\theta t(s) + sin \theta b(s)$$ is a constant unit vector and $\forall s \in, \langle t(s), v \rangle = cos\theta $ is constant. So $\alpha$ is helix. Apparently the answer is correct but incomplete, I'm not able to complete it. Thanks for any help.","['curves', 'solution-verification', 'differential-geometry']"
4488439,A proof on the generating subset of a group,"I am a beginner and currently reading Fraleigh's textbook ""A first course in abstract algebra"". Is this sentence true of false? (I didn't find such a ""theorem"" in this textbook or in the practice problems) If subgroup $H$ is generated by subset $\{a_1, a_2, ..., a_k\}$ of $\mathbb{Z}_n$ (or $\mathbb{Z}$ for infinite case), where $1\le a_1<a_2<...<a_k \le n-1$ ,
then $H= \langle d \rangle$ , where $d=\gcd(a_1, a_2, ...,a_k)$ And I try to prove it in the following way (both finite and infinite case): Because $H$ is Abelian, we can write $H=\{m_1 a_1+...+m_k a_k~|~m_1\in \mathbb{Z},...,m_k\in \mathbb{Z} \}$ (1) For any $x\in H$ , $x=m_1 a_1+...+m_k a_k$ for some $m_1\in \mathbb{Z},...,m_k\in \mathbb{Z}$ , Since $d=\gcd(a_1, a_2, ...,a_k)$ , which means $d|a_1,...,d|a_k$ , so $a_1=r_1 d, ...,a_k=r_kd$ , where $r_1\in \mathbb{Z},...,r_k\in \mathbb{Z}$ $x=m_1r_1d+...+m_kr_kd=(m_1r_1+...+m_kr_k)d$ , so, $x\in \langle d \rangle$ , Therefore, $H\subseteq \langle d \rangle$ (2) For any $x\in \langle d \rangle$ , $x=rd$ , where $r\in \mathbb{Z}$ , Because $d=\gcd(a_1, a_2, ...,a_k)$ , there exists $r_1\in \mathbb{Z},...,r_k\in \mathbb{Z}$ , such that $r_1a_1+...+r_ka_k=d$ ( BÃ©zout's identity ) so we have: $x=r(r_1a_1+...+r_ka_k)=(rr_1)a_1+...+(rr_k)a_k$ , so, $x\in H$ Therefore, $\langle d \rangle \subseteq H$ By (1) and (2), we have $H=\langle d \rangle$ . Question: If this proof is correct, according to the isomorphism, $\mathbb{Z}_n\longleftrightarrow \text{""finite cyclic group""}$ , $\mathbb{Z}\longleftrightarrow \text{""infinite cyclic group""}$ , it is also true for all cyclic groups, right?","['group-theory', 'abstract-algebra']"
4488484,What's wrong with this 'proof' that probability of being $2$ m away is $1/9$?,"Suppose there is a chicken (which we will assume to simply be a point), which is encolsed in a circular barn of radius $6$ m. At the centre of the barn, there is a well, (which we will also assume to simply be a point). If the chicken was equally likely to be at any point within the barn, what is the probability that the chicken is exactly $2$ m away from the well? I know it should be zero , since this is a continuous distribution. However, I was wondering, what goes wrong in the following reasoning to obtain an answer which isn't zero. The only way for the chicken to be exactly $2$ m away from the well, is if it were confined to a concentric circle of radius $2$ m at the well. The length of the circumference can then be thought of as all the possible points the chicken could be at in order to be $2$ m away from the well, so that's $2\pi r' = 2\pi(2) = 4\pi$ . All the possible points in the barn is simply the area, so that's $\pi r^2 = \pi(6^2) = 36\pi$ . Hence, if we let $X$ be the random variable of how far the chicken is away from the well, then $$ \mathbb{P}(X = 2) = \dfrac{4\pi}{36\pi} = \dfrac{1}{9}. $$ So to be extra clear: Question: What goes wrong in the reasoning above? Thank you for any help.","['fake-proofs', 'probability']"
4488487,"Let R = $\{1, a\}$, S = $2^R$ and $T = 2^S$. List all the elements of S and T","I am having a little difficulty understanding how to approach this question.
When S = $2^R$ and T = $2^S$ , would that mean that the elements of S = $\{2^1,\space 2^a\}$ and T = $\{2^2,\space 2^{2a}\}$ ? By that understanding, would $|S| = 2$ , $|T| = 2$ , $|S \cup T| = 4$ , and $|S \cap T| = \emptyset$ ? I am completely new to these topics and I am slowly learning. Your patience is much appreciated. Thank you.",['discrete-mathematics']
4488488,ZF Axiom of Extensionality implies unique determination?,"The Principle/Axiom of Extensionality says sets that have the same elements are equal, i.e., $$\forall x (x \in A \iff x \in B) \implies A = B$$ But I've seen in texts the statement that the AoE implies that a set $S$ is uniquely determined by its elements The axiom expresses the basic idea of a set: A set is determined by
its elements. I suppose some sort of proof would enlighten me, but to declare a set as a collection of elements seems fairly basic, nothing further needed. So why is an axiom about equality of sets bolstering the basic idea of a set being determined by its elements?",['elementary-set-theory']
4488666,Find $\lim_{n \to \infty}\frac{x_n}{\sqrt{n}}$ where $x_{n+1}=x_n+\frac{n}{x_1+x_2+\cdots+x_n}$,"Assume a positive sequence $\{x_n\} $ satisfies $$x_{n+1}=x_n+\frac{n}{x_1+x_2+\cdots+x_n}.$$ Find $\lim\limits_{n \to \infty}\dfrac{x_n}{\sqrt{n}}$ . Assume the limit we want is $L$ . Then by Stolz theorem, one can obtain \begin{align*}
L&=\lim_{n \to \infty}\frac{x_n}{\sqrt{n}}=\lim_{n \to \infty}\frac{x_{n+1}-x_n}{\sqrt{n+1}-\sqrt{n}}=\lim_{n \to \infty}\frac{2n\sqrt{n}}{x_1+x_2+\cdots+x_n}\\
&=\lim_{n \to \infty}\frac{2n\sqrt{n}-2(n-1)\sqrt{n-1}}{x_n}=\lim_{n \to \infty}\frac{3\sqrt{n}}{x_n}=\frac{3}{L},
\end{align*} which implies $L=\sqrt{3}$ . But how to prove the limit exists?","['limits', 'sequences-and-series', 'recurrence-relations', 'real-analysis']"
4488730,Composition of formal power series of exp and log,"I'm considering the formal power series $$\exp(X)=\sum_{n\geq 0} \frac 1 {n!} X^ n\in \mathbb Q [[X]]$$ and $$\log(1+X) =\sum_{n\geq 1} \frac {(-1)^{n-1}}{n} X^n\in \mathbb Q [[X]].$$ The composition of formal power series $f =\sum_{n\geq 0} a_n X^n$ and $g=\sum_{n\geq 1} b_n X^ n$ is defined as follows: For $k\geq 0$ the power series $g^k = \sum_l c_{k,l} X^ l$ is given by the Cauchy product. Note that $c_{k,l}=0$ if $k>l$ since $b_0=0$ . Now $$f\circ g(X)=f(g(X)) := \sum_k a_k \sum_l c_{k,l} X^ l =  \sum _l \left(\sum_{k=0}^l a_k c_{k,l}\right) X^l.$$ How can I see that $\exp(\log(1+X))=1+X$ as formal power series only using reordering of terms? I'm also interested in the other composition $\log(\exp(X))=X$ where $\log(\exp(X)=\log (1 + (\exp(X)-1))$ which is well-defined since the power series $\exp(X)-1$ has vanishing absolute term.","['power-series', 'combinatorics', 'exponential-function', 'logarithms']"
4488745,Professor gets wet,"Problem : A professor has $N$ umbrellas . He walks to the office in the morning and walks home in the evening . If it's raining he likes to carry an umbrella and if it's fine he doesn't . Suppose that it rains on each journey with probability $p$ , independently of past weather . What's the long-run proportion of journeys on which the professor gets wet ? Question : I want to see if my plan below is viable . Or you can share your approach . (The problem's cited directly from exercise 1.10.2 of Markov Chains by James R. Norris .) My interpretation : Suppose now professor leaves his home . If he sees it's raining ,  he will pick up an umbrella , if any , and he'll not get wet , otherwise he will . The umbrella will follow him to office , which he may not bring back .  If he sees it's not raining , he'll just walk out , and he'll not get wet because I assumed it'll not rain in the middle of his journey .  By journey I mean each commute between home & office , so 2 journeys every day . ""The long-run proportion of journeys ..."" means $\frac{\text{ no.journeys he gets wet}}{ \text{total no. of journeys } }$ when total no. of journeys tends to infinity . My plan : Assume $(1-p)p \neq 0 , N<\infty$ . Let $(X_n)_{n\ge 0}$ be the number of umbrellas at his home at night , suppose $X_0 = N$ . The state space is $ \{ 0,...,N \}$ and transition probabilities $$
\left\{
\begin{array}{cc}
p_{i,i+1} = p_{i,i-1} = (1-p)p , & p_{ii} = 1 - 2(1-p)p , \; i= 1,..,N-1 \\
p_{01} =  p , &p_{00} = 1 - p   \\
p_{N,N-1} =  (1-p)p , & p_{NN} = 1 - (1-p)p  
\end{array}\right.
$$ .
This time homogeneous markov chain is irreducible and recurrent . Let $$
Y_n =
\left\{
\begin{array}{cc}
1 & \text{ if } \{\text{Professor gets wet tomorrow}\}\\
0 & \text{otherwise} 
\end{array}\right.
$$ where $$
\{\text{Professor gets wet tomorrow}\} = 
(\{ X_n = 0 \} \cap \{ \text{ rains tomorrow morning } \} )\\ \cup 
 (\{ X_n = N \} \cap \{ \text{ doesn't rain tomorrow morning } \} \cap   \{ \text{ rains tomorrow evening } \} )
$$ Note that he will only get wet on at most 1 journey each day .  Let $Z_n=(X_n,Y_n)$ , the state space is $$
 \{(0,1)  , (N,1)\} \cup  \{(0,0) , ... , (N,0) \} 
$$ The transition probabilities (please scroll) $$
\left\{
\begin{array}{cccc}
p_{(i,0)(i+1,0)} = p_{(i,0)(i-1,0)} = (1-p)p , & p_{(i,0)(i,0)} = 1 - 2(1-p)p , \; i= 2,..,N-2 \\
p_{(1,0)(0,0)} =  (1-p)^2p , & p_{(1,0)(0,1)} =  (1-p)p^2  , 
& p_{(1,0)(2,0)} = (1-p)p ,  &  p_{(1,0)(1,0)} =  1 - 2(1-p)p    \\
p_{(N-1,0)(N,0)} =  (1-p)p(p+(1-p)^2) , & p_{(N-1,0)(N,1)} =  ((1-p)p)^2 , & 
p_{(N-1,0)(N-2,0)} = (1-p)p , & p_{(N-1,0)(N-1,0)} =  1 - 2(1-p)p \\
p_{(0,0)(0,1)} = (1-p)p , & p_{(0,0)(1,0)} = p  , &  p_{(0,0)(0,0)} = 1 - (1-p)p - p \\
p_{(N,0)(N,1)} = \left(1 - \frac{(1-p)p}{p+(1-p)^2}\right)(1-p)p , & p_{(N,0)(N-1,0)} = \frac{(1-p)p}{p+(1-p)^2} , &  p_{(N,0)(N,0)} = 1 - \left(1 - \frac{(1-p)p}{p+(1-p)^2}\right)(1-p)p  -  \frac{(1-p)p}{p+(1-p)^2} \\
p_{(0,1)(0,0)} = (1-p)^2 , & p_{(0,1)(1,0)} = p , & p_{(0,1)(0,1)}  = 1 - (1-p)^2 - p \\
p_{(N,1)(N,0)} = p+(1-p)^2 , & p_{(N,1)(N-1,0)} = 0 , & p_{(N,1)(N,1)} = 1 - p-(1-p)^2
\end{array}\right.
$$ $(Z_n)_{n\ge 0}$ is also a time homogeneous irreducible recurrent Markov chain . Informally , this Markov chain looks like 2 triangles connected to each end of a bar . My plan is to find the the expected return times $m_{(0,1)} , m_{(N,1)} $ to states $(0,1) , (N,1) $ resp. by solving linear equations using recurrence relation and minimization  , and the desired quantity will be $\frac{1}{m_{(0,1)}} + \frac{1}{m_{(N,1)}}  $ by Theorem 1.10.2  . Theorem 1.10.2  Let $P$ be irreducible and let $\lambda$ be any distribution . If $(X_n)_{n\ge 0}$ is Markov $(\lambda,P)$ then $$
\mathbb{P}\left( \frac{V_i(n)}{n} \to \frac{1}{m_i} \text{ as } n \to \infty  \right) = 1 
$$ where $V_i(n) = \sum_{k=0}^{n-1} 1_{\{X_k = i\}}$ and $m_i$ is expected return time to state $i$ . (From Markov Chains by James R. Norris)","['ergodic-theory', 'markov-chains', 'solution-verification', 'discrete-mathematics', 'probability']"
4488767,"$f$ integrable on $[a,b]$, differentiable at $c\in (a,b)$. $F(x)=\int_a^x f$. Out of the three ways $F'$ can be discont. at $c$, which are possible?","Assume a function $f$ is integrable on $[a,b]$ , differentiable at a point $c\in (a,b),$ and define $$F(x)=\int_a^x f$$ Let's investigate the theoretical conditions under which $F'$ could be discontinuous at $c$ . Since $f$ is differentiable at $c$ , this implies $f$ is continuous at $c$ . By the first fundamental theorem of calculus (FTC1) applied to $f$ on $[a,b]$ , we have that $F$ is differentiable at $c \in (a,b)$ and $F'(c)=f(c)$ . Let's recall the definition of continuity Spivak, Ch. 6, Definition: A function $f$ is continuous at $c$ if $$\lim\limits_{x\to c} f(x)=f(c)\tag{1}$$ What are the ways a function can be discontinuous at a point $c$ ? $f$ isn't defined at $c$ , hence $f(c)$ isn't defined and so $(1)$ doesn't make sense and is false $\lim\limits_{x\to c} f(x)$ does not exist, so again $(1)$ doesn't make sense and is false $f$ is defined at $c$ and the limit above does exist but it differs from $f(c)$ . Which of the above scenarios is possible in our setup of $f$ and $F$ , for $F'$ to be discontinuous at $c$ ? Here is what I came up with In the case of our function $F$ , it is differentiable wherever $f$ is continuous. If $f$ were to be continuous in an interval around $c$ , then $F$ would be differentiable and hence continuous on that interval, including at $c$ . Therefore, $f$ has to be discontinuous on every interval containing $c$ . We've established that $F'(c)=f(c)$ so $F'$ is defined at $c$ and hence option 1. above is not possible. If 3. were true, then we'd have $$\lim\limits_{x\to c} F'(x)\neq F'(c)=f(c)\tag{2}$$ Here is a line of reasoning I am unsure about Since the limit above exists, then $F'$ is defined everywhere around $c$ . Is this true? Ie, when we defined a limit $$\lim\limits_{x\to c} f(x)$$ as $$\forall \epsilon>0\ \exists \delta>0\ \forall x, |x-c|<\delta\implies |f(x)-f(c)|<\epsilon$$ Did the $\forall x$ mean literally for every single real number within $\delta$ of $c$ , or just the $x$ where $f$ is defined? if the above is indeed true, then we have the following happening (i) $F'$ defined in an interval containing $c$ (ii) since $f$ is integrable on $[a,b]$ , $F$ continuous on $[a,b]$ , and in particular at $c$ (iii) $\lim\limits_{x\to c} F'$ exists I believe the following theorem is thus applicable Spivak, Ch. 11, Theorem 7: Suppose $F$ is continuous at $c$ , $F'(x)$ exists for all $x$ in some interval containing $c$ , except possibly at $x=c$ . Suppose, moreover, that $\lim\limits_{x\to c} F'(x)$ exists. Then, $F'(c)$ exists and $F'(c)=\lim\limits_{x\to c} F'(x)$ This theorem basically says that if a function is continuous at a point, then its derivative cannot have a jump discontinuity there. Applying to $F$ , we conclude that $F'(c)=\lim\limits_{x\to c} F'(x)$ , which contradicts $(2)$ , and hence scenario 3. isn't possible. Which leaves only scenario 2., which is possible by virtue of the fact that I can come up with an example for it: $$f(x)=\begin{cases} 0, \text{ if } x \in \left (\frac{1}{n+1},\frac{1}{n} \right ] \\ x^2, \text{ if } x \in \left (\frac{1}{n},\frac{1}{n-1} \right ] \end{cases}$$ for $n\in\mathbb{N}$ . Edit For the record I thought I'd mention this is a problem from Spivak's Calculus and there is a (terse solution) that isn't very didactical. Here it is If we assume that $f$ is continuous in an interval around $c$ , then $F'$ will be continuous at $c$ , since we will have $F'(x)=f(x)$ in
this interval, and differentiability of $f$ at $c$ implies continuity
of $f$ at $c$ . But without this assumption $F'$ may not even be exist
at all points near $c$ . For example, $f$ could be the function shown
below","['integration', 'limits', 'calculus', 'derivatives']"
4488775,Change all $2^m$ bits to the same value with some restrictions.,"There are $n$ bits $A_1, A_2,\cdots,A_n$ on a row $(n \in \mathbb{N}, n \ge 2)$ , each of them is either $0$ or $1$ . For every second, the bit $A_i ~ (i=1..n)$ changes its value according to the following rule: If $A_{i-1}$ and $A_{i+1}$ has the same value with $A_i$ (i.e $A_{i-1}=A_i=A_{i+1})$ then $A_i$ is changes to $0$ , otherwise it changes to $1$ (we always assume that $A_0=A_1$ and $A_n = A_{n+1}$ at any time). Proof that if $n=2^m$ for some positive integer $m$ , there will always be a time that all the bits have value $0$ . For example with $n=4$ and the initial values of the bits are $(1,1,0,1)$ , then we have: $$(1,1,0,1) \rightarrow (0,1,1,1) \rightarrow (1,1,0,0) \rightarrow (0,1,1,0) \rightarrow (1,1,1,1) \rightarrow (0,0,0,0)$$ My attempt: We call a sequence ""good"" if it changes to all $0$ s at some point. From what I've noticed, if a sequence of $2n$ bits is good then there should be a point where it becomes a palindrome (the first palindrome that appears in the above example is $(0,1,1,0)$ ) and the problem becomes proving an $n$ bits sequence is good. Unfortunately I had no idea of what to do next.","['combinatorics', 'discrete-mathematics']"
4488778,"Upper and lower bounds for solution to $ y' = t - y^2, y(0)=0.$","I am working on the initial value problem $$ y' = t - y^2,\qquad y(0)=0.$$ Assuming that the solution exists on $[0,\infty)$ , I am supposed to show that $$ 0\leq y(t) \leq \sqrt{t}\qquad \forall t\geq 0.$$ I've managed to prove an upper bound of $t^2/2$ by using the fundamental theorem of calculus, but I don't seem to be getting anywhere near the $\sqrt{t}$ that the question is asking for. Any help would be appreciated :)","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4488783,Evaluating $\int_{-\infty}^\infty \left (\frac{1 - x^2}{1 + x^2 + x^4}\right )^n \ \mathrm{d}x$,"I am trying to find a general form of $$I_n = \int_{-\infty}^\infty \left (\frac{1 - x^2}{1 + x^2 + x^4}\right )^n \ \mathrm{d}x$$ as well as methods of solving it (preferably elementary). This is easy enough to solve for individual $n$ , which leads me to believe that there is likely a $\frac{\pi}{\sqrt{3}}$ term in the general expression, but I am having trouble finding a general solution. I have tried constructing a recurrence via IBP, but I always have an extra $\frac{(1 - x^2)^m}{(1 + x^2 + x^4)^n}$ term with $m \neq n$ . I also tried creating a $2D$ recurrence, but that doesn't seem solvable, nor can I figure out how to incorporate the requirement of $m \leq 2n - 1$ (for the integral to be finite). Additionally, I tried using a semicircular contour in the upper half-plane, but computing the residues at $z = e^{\frac{i\pi}{3}}, e^{\frac{2i\pi}{3}}$ seems very bashy. I have tried using Mathematica, and that just outputs the input. Any help would be great.","['integration', 'complex-integration']"
4488834,Is taking (co)limits exact in an Abelian category?,"Let $\mathcal{A}$ be a complete and cocomplete Abelian category. Let $J$ a small category, and let $F_1 \xrightarrow{r} F_2 \xrightarrow{s} F_3$ be a sequence of diagrams/functors $F_i : J\rightarrow \mathcal{A}$ with $r,s$ natural transformations. Suppose for each object $j\in \mathrm{Obj}(J)$ the sequence $0\rightarrow F_1(j) \xrightarrow {r_j} F_2(j) \xrightarrow {s_j} F_3(j)\rightarrow 0$ is a short exact sequence in $\mathcal{A}$ . If we take the limits (or colimits) $M_i = \mathrm{(co)lim}F_i$ we get an induced sequence $0 \rightarrow M_1 \xrightarrow {\rho} M_2 \xrightarrow{\sigma} M_3 \rightarrow 0$ in $\mathcal{A}$ . In general is the sequence of $M_i$ 's necessarily exact?","['limits-colimits', 'category-theory', 'exact-sequence', 'abstract-algebra', 'abelian-categories']"
4488853,A characterization of the Euclidean norm,"Suppose that a norm $||\cdot||$ on $\mathbb{R}^2$ has the following property: Whenever norm one vectors $x$ and $y$ satisfy $||x+y||=||x-y||$ then $||x+y||=||x-y||=\sqrt{2}$ . Does it follow that $||\cdot||=||\cdot||_2$ , the Euclidean norm on $\mathbb{R}^2$ ? I tried to show that the parallelogram identity holds for this norm, thus implying it is Euclidean, but without success. I don't know how to deal with vectors of norm one that are not $\sqrt{2}$ apart. I expect this to be true, I would be surprised by a counterexample. There is no source that I am aware of for this question. I am trying to understand isometric properties of Hilbert spaces, and this two dimensional version seems like a natural question to me.","['normed-spaces', 'geometry', 'linear-algebra', 'functional-analysis', 'linear-transformations']"
4488856,Is the boundary of a $k$-simplex missing some $(k-1)$-faces contractible?,"An $k$ -simplex $T$ is the convex hull of $k+1$ affinely independent vectors  in $\mathbb{R}^k$ (for example, $0$ and the $k$ standard basis). And any convex hull formed by $k$ of them, which itself is a $(k-1)$ -simplex, is called a $(k-1)$ -face. Let $S$ be the complex formed by removing the interior of $T$ (so that $S$ is homeomorphic to $S^{k-1}$ .) After removing one $(k-1)$ -face from $S$ , the resulting space is contractible. I am wondering after removing some $(k-1)$ -faces from $S$ , is it still contractible?
If not, for the resulting space, can the minimum $j$ such that its $j$ th reduced Betti number (over rational number field) be strictly smaller than that of $S^{k-1}$ (which is $k-1$ )?","['simplex', 'combinatorics', 'algebraic-topology']"
4488863,Probability of 3 red balls out of 200 balls placed in 5 bins,"Given 3 red balls, 197 black balls, and 5Â bins.Â 
TheÂ balls are randomly distributed with 40Â balls in each bin.Â 
What is the probability that the three red balls are in three separate bins
(each of the three bins has one red ball;
i.e.,Â no two red balls are in the same bin)? Different variants of this question has been asked â€“
but somehow I still have trouble computing the correct probability. One answer I found was: $$\frac{\left(\begin{array}{l}
5 \\
1
\end{array}\right)\left(\begin{array}{l}
4 \\
1
\end{array}\right)\left(\begin{array}{l}
3 \\
1
\end{array}\right)}{125} = 12/25$$ It only focuses on red balls, and there are 125 ways that 3 red balls can be allocated in bins. However, another alternative way of computing this seems to be: $$
\frac{5 \times {3 \choose 1} {199 \choose 39} {3 \choose 1} {199 \choose 39} {3 \choose 1} {199 \choose 39} {200 \choose 40} {200 \choose 40}}{{200 \choose 40}{200 \choose 40}{200 \choose 40}{200 \choose 40}{200 \choose 40}} = 27/25
$$ Obviously it seems that the alternative way is incorrect (it's above 1 as a probability). So my question is: is this alternative way of computing incorrect? How so? How can I fix it?","['combinatorics', 'balls-in-bins', 'probability']"
4488865,Conditional probability with 4 coins,"The question: A box contains four coins, two of which are fair, one double-headed (i.e., heads on both sides), and the third is biased in such a way that it comes up heads with probability 1/4. A coin is drawn at random from the box and flipped twice. If both flips result in heads, what is the probability that the coin drawn was double-headed? From what I understand, there are 2 fair coins with a 1/2 chance to get heads, 1 coin that has 100% chance of getting heads, and one coin with a 1/4 chance to get heads. Does this mean, for example, that the chance of getting heads in both flips from a fair coin is 1/8? Since there's a 2/4 (1/2) chance to pick one? I just cannot understand the formulation of the question but I assume we have to use Baye's theorem here? I would appreciate any help here! Thanks","['statistics', 'bayes-theorem', 'probability']"
4488885,Proof for positivity of solutions of an ODE system,"I have an ODE system  that takes a mathematical model describing the dynamics between HCV and the immune system. My question is about the proof that the solution of the ODE system is positive if the initial conditions are all positive. I tried this proof: Consider the DE $\dot{x}=f(x).$ The vector function $f$ is said to be essentially nonnegative if for all $i=1,\ldots,n, f_i(X)\geq 0,$ where $X\in\mathbb{R}^n\geq 0$ such that $X_i=0,$ where $X_i$ denotes the $i$ -th element of $X.$ So for my system: \begin{align}
\dot{x}&= \lambda âˆ’ dx âˆ’ \beta vx \\
\dot{y}&= \beta vx âˆ’ ay âˆ’ pyz \\
\dot{v}&= ky âˆ’ uv âˆ’ qvw \\
\dot{w}&= gvw âˆ’ hw \\
\dot{z}&= cyz âˆ’ bz
\end{align} I have an issue with proving that, the r.h.s. of $\dot{y}$ for $y=0$ is $\beta vx,$ is non-negative for all $v.$","['epidemiology', 'mathematical-modeling', 'ordinary-differential-equations', 'population-dynamics']"
4488890,Proof involving composing sums with itself.,"I'm not sure how to go about proving that $$
\sum_{i_1=1}^{\phi+1}\left(\sum_{i_2=1}^{i_1}\left(\sum_{i_3=1}^{i_2}...\left(\sum_{i_{n-1}}^{i_{n-2}}i_{n-1}\right)...\right)\right) = {\phi +n \choose n}
$$ I know that they equal by trying to solve the problem of how many decreasing sequences (i.e $a_{n+1} \leq a_n$ ) can be made with $n$ terms which are all integers (i.e $a_1,a_2,a_3,...a_{n}$ ) with every term between $0$ and $\phi$ inclusive, in 2 different ways and they both come out to equal but I'm not sure about a direct algebraic proof.","['alternative-proof', 'functions', 'combinatorics']"
4488939,Does Spec send all ring epimorphisms to injective maps?,"We know that the Spec construction, without the structure sheaf and Zariski topology, gives a functor $\mathrm{CommRings}^{\mathrm{op}}\rightarrow \mathrm{Set}$ , and furthermore
this functor sends (the opposite arrows of) the following kinds of ring homomorphisms to injective maps in Set: surjective ring homomorphisms (quotients by ideals); and localizations (at multiplicative subsets). Both of these are examples of ring epimorphisms.
I was wondering, in general does $\mathrm{Spec}$ send all ring epimorphisms to injective maps?","['ring-theory', 'abstract-algebra']"
4488948,Order 3 projective plane with playing cards,"The projective space based on $\mathbb{F}_3$ , the finite field with 3 elements (PP3), has 13 lines and 13 points, with every two lines meeting at one point and every two points determining a line. Each of the 13 lines consists of four points. A possible listing would be A23T, 456T, 789T,  A47J, 258J, 369J, A59Q, 267Q, 348Q, A68K, 249K, 357K, and TJQK. I note that 13 lines with 4 points to a line suggests using playing cards, explaining my notation, and indeed I got out a set of playing cards and dealt them out into a 4x13 rectangle that represents PP3. But playing cards have suits too, and I rearranged the cards so that each line contains one card from each suit. The resulting arrangement seemed haphazard, so I tried it with the Fano plane, using 3 suits and 7 lines. I found that the 124, 235, 346, 457, 561, 612, 723 arrangement (1 = Ace) can be obtained by adding 1 mod 7 to each card in a line to get the next line. In that case, then let each first element of the line be a heart, each second a diamond, and each third a spade.  I tried doing this with PP3 and it didnâ€™t seem to work. Is there such an arrangement for PP3, where the points represent elements of $\mathbb{Z}_{13}$ ? Is there an orderly way of assigning suits to the cards? And how many ways are there of assigning suits to the points in each line in PP3?","['combinatorial-designs', 'geometry']"
4488949,"An undirected graph is partitioned. For every partition $V_1 \cup V_2 = V$, $v_1 \in V_1$ is connected to some $v_2 \in V_2$. Show it is connected.","We have an undirected graph $G = (V, E)$ partitioned into two non-empty subsets $V_1 \cup V_2 = V$ . For this graph it holds that $(V_1 \cup V_2 = V) \implies (\exists v_1, v_2 : v_1 \in V_1 \land v_2 \in V_2 \land (v_1,v_2) \in E)$ . So for every partition some $v_1 \in V_1$ is connected to some $v_2 \in V_2$ . We need to show $G$ is connected, i.e.: to show that there is a path between any two elements in the graph. This problem came from a problem set in a course on discrete mathematics. I solved the problem in two ways. One being by contradiction. Namely by assuming that the graph is not connected, then there exists a partition $V_{1}$ and $V_{2}$ such that no edge $(v_{1}, v_{2})$ exists with $v_{1} \in V_{1}$ and $v_{2} \in V_{2}$ which means the assumption that the graph is not connected is false. And so the graph must be connected. The other approach I came up with is based on induction and I would like to put this approach up to scrutiny. In this approach I incorporate the definition that for all partitions there is a $v_1 \in V_1$ and $v_2 \in V_2$ with $(v_1, v_2) \in E$ in an induction hypothesis. The induction hypothesis (IH) $P(n)$ being that ""for any partition $V_1$ with $n$ elements these elements are part of an induced (sub)graph $G[V_1]$ which is connected and there exists a $v_1 \in V_1$ and $v_2 \in V_2$ with $(v_1, v_2) \in E$ "". $G[V_x]$ is the induced subgraph of $G$ with elements in $V_x$ . Since the partitions are defined to be non-empty the base case contains one element. Applying the induction hypothesis to the base case $P(1)$ one gets ""for any partition $V_1$ with 1 element this element is part of an induced (sub)graph $G[V_1]$ which is connected and there exists a $v_1 \in V_1$ and $v_2 \in V_2$ with $(v_1, v_2) \in E$ "", which is true since any graph of one element is trivially connected and furthermore by definition this element is connected to some element $v_2 \in V_2$ . So there exists no element which is not connected to some other element, i.e.: every element in $V$ is connected to some other element in $V$ . Since we have added $(v_1, v_2)$ to a connected $G[V_1]$ we get that $G[V_1 \cup v_2]$ is connected. By definition the induced subgraphs consisting of two elements in $V_1$ are connected to some element $v_2 \in V_2$ . And so $P(1) \implies P(2) $ . Generalising we get that $P(n) = $ ""for any partition $V_1$ with $n$ elements these elements are part of an induced (sub)graph $G[V_1]$ which is connected and there exists a $v_1 \in V_1$ and $v_2 \in V_2$ with $(v_1, v_2) \in E$ "", which implies that every induced subgraph $G[V_1 \cup v_2] = (V_1 \cup {v_2}, E_1 \cup (v_1, v_2))$ of $G$ with $n + 1$ elements is connected and furthermore by IH these subgraphs contain some element connected to some element in $G[V_2 \setminus v_2]$ . And so $P(n) \implies P(n + 1)$ . And now we have that $P(1)$ and $P(n) \implies P(n + 1)$ and so $P(n)$ holds for all $n \leq \#V - 1$ (since partitions are non-empty). To finish things off we consider $P(m - 1)$ with $m = \#V$ . We have shown that all induced subgraphs $(V_1, E_1)$ of $G$ of size $m - 1$ are connected and by IH there is some element in this subgraph connected to an element in $V \setminus V_1$ . There is only one element left and since this element is connected to a connected subgraph the graph $G$ is connected. I can't find anything wrong with this reasoning but I'm not certain it's correct either. If anyone can point out something wrong with this, or provide a better induction hypothesis or something of that nature it would be much appreciated :).","['graph-connectivity', 'discrete-mathematics', 'computer-science']"
4488968,"For $F\in C^1(\mathbb{R}^n)$, if its Jacobian is singular at least one point, could its inverse be Lipschitz?","Consider map $F \in C^1(\mathbb{R}^n)$ , if there exists $x_0$ such that Jacobian $JF(x_0)$ is singular, we know that the inverse $F^{-1}$ may exist (e.g. $x\mapsto x^3$ ) and if it exists, it cannot be also in $C^1(\mathbb{R}^n)$ (because it is not differentiable at $x_0$ ). But the question is, in this case, could the inverse be Lipschitz? I can neither prove that the inverse is definitely not Lipschitz nor provide an example. Any hints? Thanks!","['lipschitz-functions', 'analysis']"
4488978,"Is the $(3,4,5)$ triangle the only rectangular triangle with this property?","While solving a loosely related exercise, by luck I found out that the $(3,4,5)$ triangle has the following property : The product of the lengths ( $\sqrt{2}$ and $\sqrt{5}$ ) of the two shorter line segments from a corner to the center of the inscribed circle equals the length ( $\sqrt{10}$ ) of the longest one. Somewhat satisfied with this, for me own new found, result I now wonder if any other rectangular $(a,b,c)$ triangles have this particular property. $(a,b,c)$ does not need to be a Pythagorean triple (but it would be extra nice). It tried some straightforward algebraic equations but failed to find answer ... Maybe finding non rectangular such triangles is easier, but ideally I ask for rectangular ones. update Can this property of certain pythagorean triples in relation to their inner circle be generalized for other values of $n$? is already linked to this one but I take the freedom to explicitly mention it here in post for following reasons the question asked there is about generalizing answer given here the answers to both questions always left some exercises for reader myself I am not able (I continue to try) to do these exercises Maybe someone can fully write out the missing gaps.","['triangles', 'pythagorean-triples', 'geometry']"
4488980,Is there anything wrong with this proof that $\lim_{x\to0} \frac{\sin(x)}{x} = 1$?,"Definitions Since I'm asking for verification of a proof, I'll start with some basic definitions. I believe these definitions are  more or less equivalent to the ""standard"" definitions of curve length, radian, and sine/cosine: Let the length of a continuous curve $f(t): [a,b] \rightarrow \mathbb{R}^2$ be defined as $\lim_{N\to\infty}\sum_{i=1}^N \sqrt{(x(t_{i})-x(t_{i-1}))^2 + (y(t_{i})-y(t_{i-1}))^2}$ where $(x(t_{i}),y(t_{i}))=f(t_{i})$ and $t_{i} = a+i(b-a)/N$ . Let the angle measured by the counter-clockwise rotation of a point $P$ on a unit circle $C$ , such that the locus of $P$ generates a curve of length $\theta$ , be defined as $\theta$ radians. This angle measured by a clockwise rotation can be defined as $-\theta$ radians. Let the point on the Cartesian plane reached by rotating the point $(1,0)$ on the unit circle centered at $(0,0)$ by an angle of $\theta$ radians be defined as $(\cos(\theta), \sin(\theta))$ . Detailed Proof Consider the curve $f(t) = (\cos(t), \sin(t))$ for $t \in [0,1]$ . On the one hand, this is just a circular arc that subtends 1 radian from the center of a unit circle, and must therefore have length 1, by the definition of a radian. On the other hand, we can compute the length of the curve by the definition of curve length, which means that: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{(\cos(i/N)-\cos((i-1)/N))^2 + (\sin(i/N)-\sin((i-1)/N))^2} = 1$$ Using basic trigonometric properties and algebra (see appendix for details), we can get: $$\lim_{N\to\infty}2N\sin(\frac{1}{2N}) = \lim_{N\to\infty}\frac{\sin(\frac{1}{2N})}{\frac{1}{2N}} = 1$$ Since we can always choose an $x\in\mathbb{R}$ such that $0<x<\frac{1}{2N}$ , we have that: $$\lim_{x\to0^+}\frac{\sin(x)}{x} = 1$$ Now we just observe that $\sin(x)$ is an odd function since we rotate starting from the x-axis, and therefore a counter-rotation reflects the y-coordinate along the x-axis, so we have: $$\lim_{x\to0^-}\frac{\sin(x)}{x} = \lim_{x\to0^+}\frac{\sin(-x)}{-x} = \lim_{x\to0^+}\frac{-\sin(x)}{-x} = \lim_{x\to0^+}\frac{\sin(x)}{x} = 1$$ Therefore: $$\lim_{x\to0}\frac{\sin(x)}{x} = 1$$ QED Note, however, that to show that the length of the curve is $\lim_{N\to\infty}2N\sin(\frac{1}{2N})$ , we don't need to use trigonometric properties or algebra at all. Consider the following diagram where we break a circular arc of length 1 into N=3 parts: The summation in the definition of the curve length is just the sum of the lengths of the orange segments in the above diagram. Notice that all the orange segments are of equal length since all the triangles are congruent isosceles triangles with two side lengths of 1 joined at an angle of 1/N = 1/3 radians. Therefore, the sum of the lengths of all segments is 2N multiplied by half the length of a single segment. To find half the length of a single orange segment, we split the bottom triangle into two right triangles and rotate the bottom half like so: This shows that half the length of a segment is $\sin(1/6)$ for N=3 and $\sin(\frac{1}{2N})$ in general. Therefore, the general the sum of the lengths is $2N\sin(\frac{1}{2N})$ . TL;DR Proof If I was showing this as a basic proof, with the same level of rigor as I usually see the proof of $\lim_{x\to0} \frac{\sin(x)}{x} = 1$ thrown around everywhere, it would be much more straightforward: Here's a diagram splitting an arc of length 1 into N equal parts: Observe that there are N parts each of length $2\sin(\frac{1}{2N})$ , so the total length is $2N\sin(\frac{1}{2N})$ . This implies that: $$\lim_{N\to\infty}2N\sin(\frac{1}{2N}) = \lim_{N\to\infty}\frac{\sin(\frac{1}{2N})}{\frac{1}{2N}} = 1$$ Now let $x = \frac{1}{2N}$ and we get: $$\lim_{x\to0}\frac{\sin(x)}{x} = 1$$ QED Motivation Why do I bring up this proof? If it's valid, I honestly think this is a much better proof than the common proof that relies on the squeeze theorem and geometric areas demonstrating that $\frac{\sin(\theta)}{2}\leq\frac{\theta}{2}\leq\frac{\tan(\theta)}{2}$ . The common proof hides the fact that we ultimately need to find the limit of an infinite process. To do this, it relies on the area of a circular sector. Ironically, many common proofs that we are taught for the area of a circle, if expressed more rigorously, are actually relying on this very same limit. You can even see this in the above diagrams. If we split a sector of arc length $\theta$ into N triangles, each one has area $\frac{1}{2}\sin(\frac{\theta}{N})$ and therefore the total area is: $$\lim_{N\to\infty}\frac{1}{2}N\sin(\frac{\theta}{N}) = \lim_{N\to\infty}\frac{1}{2}\frac{\sin(\frac{\theta}{N})}{\frac{1}{N}} = \frac{1}{2}\lim_{x\to0}\frac{\sin(\theta x)}{x}$$ And it is precisely the proof shown here that can be trivially extended to show that: $$\lim_{x\to0}\frac{\sin(\theta x)}{x} = \theta$$ So in short, I believe the advantages of using the proof here, if valid, as the typical proof for demonstrating that $\lim_{x\to0} \frac{\sin(x)}{x} = 1$ are: It avoids circular reasoning if we're relying on a modern notion of Archimedes' proof for the area of a circle. It avoids dealing with areas at all and instead directly deals with the ratio of two measures of length. It easily extends to a more general result, namely that $\lim_{x\to0} \frac{\sin(\theta x)}{x} = \theta$ . Appendix $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{(\cos(i/N)-\cos((i-1)/N))^2 + (\sin(i/N)-\sin((i-1)/N))^2}$$ The above expression will contain $(\cos(i/N)^2 + \sin(i/N)^2)$ and $(\cos((i-1)/N)^2 + \sin((i-1)/N)^2)$ so once we simplify we will get the expression: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N)\cos((i-1)/N) + \sin(i/N)\sin((i-1)/N))}$$ Since cosine is an even function and sine is an odd function, this is equivalent to: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N)\cos((1-i)/N) - \sin(i/N)\sin((1-i)/N))}$$ Since $\cos(a+b) = \cos(a)\cos(b) - \sin(a)\sin(b)$ , we get the expression: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N + (1-i)/N))} = \lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2\cos(1/N)}$$ Since $1 - \cos(\theta) = 2\sin^2(\theta / 2)$ , we get: $$\lim_{N\to\infty} N\sqrt{2*2\sin^2(\frac{1}{2N})}$$ And the final result: $$\lim_{N\to\infty} 2N\sin(\frac{1}{2N})$$","['limits-without-lhopital', 'geometry', 'alternative-proof', 'calculus', 'solution-verification']"
4489084,"If differential are covectors, what is the geometric meaning of the exterior derivative of covectors?","I was thinking of the fact that differentials form a contravariant basis, and I understand the interpretation of the line integral: $$\int_P{f_i\ dx^i} $$ as the aplication of a covector field ${f_idx^i}$ over a curve $P$ . But I also had the intuition that integrating over differential forms corresponds to computing the acting of the $k-$ vector field of the exterior algebra to a domain $D$ of integration: $$\int_D{f_i\ dx^i\wedge dx^I} $$ and so for example, the integral... $$ \int_S{fi\ dx^i\wedge dx^j}$$ I've thought of as the integral of a $2-$ vector field over a surface $S$ . I don't know if that intuiton is correct, but thinking otherwise, what would be the geometric (tensor) meaning of $d(\omega)$ , if $\omega$ is a covector? How can be united the ""tensor calculus"" and ""differential form"" interpretation? Edit: My question goes in the dirrection that, knowing that covectors are linear forms , how you arise from the exterior derivative of ""a function of the Dual vector space "" to the parrallelogram interpretation of $2-$ forms. Also, what I searched now is that, may they be like $2-D$ covectors or something like that.","['integration', 'exterior-derivative', 'tensors', 'differential-geometry']"
4489102,Differential forms on $\mathbb{R}^n$,"I just read section 4 of the manifold book by Loring Tu. I can understand the $1$ -form without questions as covector fields. A $k$ -form on $U$ is a function that assigns every $p\in U$ an alternating $k$ -linear function on $T_p(\mathbb{R}^n)$ . I understand that we need to assign a $k$ -linear function for integration to make sense. However, why do we need the alternating instead of just $k$ -linear function?","['differential-forms', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
4489142,"If $\mu_n \overset{\ast}{\rightharpoonup}\mu$, then $\mu^+_n \overset{\ast}{\rightharpoonup} \mu^+$ and $\mu^-_n \overset{\ast}{\rightharpoonup}\mu^-$","Let $X$ be a Polish space and $\mu, \mu_n$ finite signed Borel measures on $X$ . Assume that $\mu_n \overset{\ast}{\rightharpoonup} \mu$ , i.e., $$
\int_X f \mathrm d \mu_n \to \int_X f \mathrm d \mu
$$ for all bounded continuous functions $f:X \to \mathbb R$ . Let $\mu = \mu^+ - \mu^-$ and $\mu_n = \mu_n^+ - \mu_n^-$ be their Jordan decompositions. Is it true that $\mu^+_n \overset{\ast}{\rightharpoonup} \mu^+$ and $\mu^-_n \overset{\ast}{\rightharpoonup} \mu^-$ ?","['measure-theory', 'signed-measures', 'weak-convergence']"
4489162,$\left\lfloor\frac{8n+13}{25}\right\rfloor-\left\lfloor\frac{n-12-\left\lfloor\frac{n-17}{25}\right\rfloor}{3}\right\rfloor$ is independent of $n$,"If $n$ is a positive integer, prove that $$\left\lfloor\frac{8n+13}{25}\right\rfloor-\left\lfloor\frac{n-12-\left\lfloor\frac{n-17}{25}\right\rfloor}{3}\right\rfloor$$ is independent of $n$ . Taking $$f(n)=\left\lfloor\frac{8n+13}{25}\right\rfloor-\left\lfloor\frac{n-12-\left\lfloor\frac{n-17}{25}\right\rfloor}{3}\right\rfloor$$ I could prove that $$f(n+25)=f(n)$$ But, I have no idea how to show $$f(n+k)=f(n)\\\forall k\in \{1,2,\dots 24\}$$ Of course, we can check these finite number of cases by force. But is there any other elegant method to handle this?","['number-theory', 'ceiling-and-floor-functions']"
4489176,"Calculate $\lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx$.","Let $k\geq 1$ be an integer, $f:[0,1]\to\mathbb R$ be a Riemannian integrable function and $g:[0,1]\to \mathbb R$ be a continuous function. Calculate $$\lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx,$$ where $\{x\}$ denotes the fractional part of the real number $x$ . This is W33 of JÃ³zsef Wildt International Mathematical Competition, 2021 . Yesterday, I planned to ask this question here because I could only handle the case where $k=1$ at that time. However, when I was ready to click the button ""Review your question"", an idea suddenly came to my mind: Why not consider $g\equiv 1$ first? After 1 hour or 2, that idea became a full answer to this problem. Now I write it here. It is a little bit long, because I proved two lemmas that is useful for us. My question. Is my proof presented below correct or not? Does there exist a shorter and more elegant proof? Lemma 1. If $f:[0,1]\to\mathbb R$ is measurable and bounded, then $$\lim_{n\to\infty}\int_0^1 f(\{nx^k\})\,dx=\int_0^1f(x)\,dx.$$ Proof of Lemma 1. Firstly, $$\int_0^1 f(\{nx^k\})\,dx=\sum_{m=0}^{n-1}\int_{\left(\frac mn\right)^{1/k}}^{\left(\frac {m+1}n\right)^{1/k}}f(nx^k-m)\,dx=\int_0^1\sum_{m=0}^{n-1}\frac1{nk}f(t)\left(\frac{m+t}n\right)^{\frac1k-1}\,dt.$$ Let $$f_n(t)=\sum_{m=0}^{n-1}\frac1{nk}f(t)\left(\frac{m+t}n\right)^{\frac1k-1},\qquad n\geq 1, t\in(0,1].$$ For fixed $t\in(0,1]$ , we have $$f_n(t)\to f(t)\int_0^1 \frac1k u^{\frac1k-1}\,du=f(t),\qquad \text{as }\ \ n\to\infty.$$ On the other hand, \begin{align*}
	|f_n(t)|&\leq |f(t)|\frac1{nk}\left(\frac tn\right)^{\frac1k-1}+|f(t)|\sum_{m=1}^{n-1}\frac1{nk}\left(\frac{m}n\right)^{\frac1k-1}\\
	&\leq |f(t)|\frac1{k}t^{\frac1k-1}+|f(t)|\frac1k\int_{1/n}^1\left(u-\frac1n\right)^{\frac1k-1}\,du\\
	&\leq \frac{\|f\|_{L^\infty}}{k}t^{\frac1k-1}+|f(t)|.
	\end{align*} Therefore, DCT implies that $\int_0^1 f_n(t)\,dt\to\int_0^1 f(t)\,dt$ , concluding the proof of Lemma 1. Lemma 2. If $f\in L^\infty(0,1)$ , and $0<\eta\ll1$ , then there exists a positive cosntant relying only on $k$ , $C(k)>0$ such that $$\int_{1-\eta}^1|f(\{nx^k\})|\,dx\leq \left(\eta+\frac{C(k)}{n}\right)\|f\|_{L^1}.$$ Proof of Lemma 2. Assume that $\left(\frac mn\right)^{1/k}\leq 1-\eta<\left(\frac {m+1}n\right)^{1/k}$ for some $m\geq \frac n2+1$ , then \begin{align*}
		\int_{1-\eta}^1|f(\{nx^k\})|\,dx&\leq \int_{\left(\frac mn\right)^{1/k}}^1|f(\{nx^k\})|\,dx\\
		&=\int_0^1|f(t)|\sum_{p=m}^{n-1}\frac1{nk}\left(\frac{p+t}n\right)^{\frac1k-1}\,dt\\
		&\leq \|f\|_{L^1}\sum_{p=m}^{n-1}\frac1{nk}\left(\frac{p}n\right)^{\frac1k-1}\\
		&\leq \|f\|_{L^1}\frac1k\int_{\frac mn}^1\left(u-\frac1n\right)^{\frac1k-1}\,du\\
		&\leq \|f\|_{L^1}\left[\left(1-\frac1n\right)^{\frac1k}-\left(\frac{m-1}n\right)^{\frac1k}\right].
	\end{align*} Let $\phi(s)=s^{1/k}$ for $s\in(0,1]$ , then \begin{align*}
		\phi\left(\frac mn+\frac1n\right)-\phi\left(\frac mn-\frac1n\right)&\leq \frac2n\max_{1/2\leq s\leq 1}|\phi'(s)|=\frac{2^{2-\frac1k}}{nk}\\
		&=\frac1{nk}+\frac{C(k)}{n}=\frac1n\min_{1/2\leq s\leq 1}|\phi'(s)|+\frac{C(k)}{n}\\
		&\leq \phi(1)-\phi\left(1-\frac1n\right)+\frac{C(k)}{n}.
	\end{align*} Hence \begin{align*}
	\int_{1-\eta}^1|f(\{nx^k\})|\,dx&\leq \left(\phi\left(1-\frac1n\right)-\phi\left(\frac mn-\frac1n\right)\right)\|f\|_{L^1}\\
	&\leq \left(\phi(1)-\phi\left(\frac mn+\frac1n\right)+\frac{C(k)}{n}\right)\|f\|_{L^1}\\
	&\leq \left(\eta+\frac{C(k)}{n}\right)\|f\|_{L^1}.
	\end{align*} This completes the proof of Lemma 2. Proof of the main problem. We prove $$\lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx=g(0)\int_0^1f(x)\,dx.\tag{1}$$ For each $\epsilon>0$ and $0<\eta\ll1$ , since $g$ is continuous at $x=0$ , we can find $\delta>0$ such that $|g(x)-g(0)|\leq \epsilon$ for all $x\in[0,\delta]$ . For $n$ large enough, we have $(1-\eta)^n<\epsilon$ and thus $$|g(x^n)-g(0)|\leq \epsilon,\qquad \forall x\in[0,1-\eta].$$ Therefore, for all large $n$ , we have \begin{align*}
		&\ \ \ \left|\int_0^1 f(\{nx^k\})g(x^n)\,dx-g(0)\int_0^1f(x)\,dx\right|\\
		&\leq\left|g(0)\int_0^1f(\{nx^k\})\,dx-g(0)\int_0^1f(x)\,dx\right|+\int_0^1|f(\{nx^k\})||g(x^n)-g(0)|\,dx\\
		&\leq|g(0)|\left|\int_0^1f(\{nx^k\})\,dx-\int_0^1f(x)\,dx\right|+\epsilon \int_0^{1-\eta}|f(\{nx^k\})|\,dx+2\|g\|_{L^\infty}\int_{1-\eta}^1|f(\{nx^k\})|\,dx\\
		&\leq|g(0)|\left|\int_0^1f(\{nx^k\})\,dx-\int_0^1f(x)\,dx\right|+\epsilon \int_0^{1}|f(\{nx^k\})|\,dx+2\|g\|_{L^\infty}\int_{1-\eta}^1|f(\{nx^k\})|\,dx.
	\end{align*} By Lemma 1 and Lemma 2, taking $\limsup_{n\to\infty}$ on each side gives that $$\limsup_{n\to\infty}\left|\int_0^1 f(\{nx^k\})g(x^n)\,dx-g(0)\int_0^1f(x)\,dx\right|\leq \epsilon\|f\|_{L^1}+2\eta\|g\|_{L^\infty}\|f\|_{L^1}.$$ Letting $\epsilon,\eta\to0$ , we get $(1)$ . I would like to express my gratitude to anyone who are reading this long post. Comments or remarks on a easier proof, or improved results ( e.g. convergence rates, the same conclusion under weaker conditions), or anything else related to this problem, are very welcome.","['contest-math', 'real-analysis', 'alternative-proof', 'solution-verification', 'limits']"
4489188,Solve the differential equation $L^2 y=0$ where $L=x^2\frac{d^2}{dx^2}+x\frac{d}{dx}-(x^2+1)\text{id}$,"Consider the linear differential equation defined by $$
L^2y=0,\quad \text{where}\ L:=x^2\frac{d}{dx^2}+x\frac{d}{dx}-(x^2+1)\text{id}. \tag{1}$$ Here $\text{id}$ denotes the identity operator, and $L^2$ denotes the operator $L$ applied twice. Written out explicitly, (1) is of the form $$x^4\frac{d^4 y}{dx^4}+6 x^3\frac{d^3y}{dx^3}+(5x^2-2x^4)\frac{d^2y}{dx^2}-(x+6x^3)\frac{dy}{dx}+(x^2-1)^2y=0. \tag{2}$$ I am interested in the general solution to (1) which is bounded at $x=0$ . Since the solution that we are looking for is bounded at $x=0$ , we may write it as $$y(x)=\sum_{n=0}^\infty a_n x^n,$$ near the origin. Substituting the above solution into (2) and equating the coefficient of $x^n$ to be zero, we obtain the following recurrence relation $$ a_2=\frac{2}{9}a_0,\quad a_3=\frac{1}{8}a_1,\quad a_4=\frac{1}{75}a_0,\quad a_5=\frac{1}{192}{a_1},\quad a_6=\frac{4}{11025}a_0,\quad\dots.$$ Setting $(a_0,a_1)=(1,0)$ and $(a_0,a_1)=(0,1)$ , we obtain two linearly independent solutions whose linear combination gives the general solution to (1) bounded at $x=0$ . The solution to with $(a_0,a_1)=(0,1)$ can be written as $$
y_1(x)=x+\frac{1}{8}x^3+\frac{1}{192}x^5+\cdots=2I_1(x),$$ where $I_1(x)$ is the modified Bessel function of the first kind, while the solution with $(a_0,a_1)=(1,0)$ can be represented as $$
y_2(x)=1+\frac{2}{9}x^2+\frac{1}{75}x^4+\frac{4}{11025}x^6+\cdots$$ My question is that: can $y_2(x)$ be represented in terms of any special function (like $y_1(x))$ ? Update: As pointed out in the comments, the $x^0$ coefficient relation gives $a_0=0$ , so the solution $y_2(x)$ doest not exist.","['ordinary-differential-equations', 'bessel-functions']"
4489253,Sine wave on ellipse.,"I want to evenly draw a sine wave around the circle and ellipse. Here's for circle: $r$ is radius, $t$ from $0$ to $2\pi$ , 20 is no. of waves. $x = (r + \sin(20t))\cos(t)$ $y = (r + \sin(20t))\sin(t)$ Now I want to draw the same thing for ellipse evenly Here's for ellipse: $a$ and $b$ are major and minor axis, $t$ from $0$ to $2\pi$ , 20 is no. of waves. $x = (a + \sin(20t))\cos(t)$ $y = (b + \sin(20t))\sin(t)$ As you can see, the waves are concentrated at the ends but I want it to be evenly distributed. I prefer elementary function as I want to keep computation cheap. Accuracy is not important in my case, even 20 to 40 percent error will do the work.","['parametric', 'conic-sections', 'geometry']"
4489286,"In how many ways can you arrange the word ""director"" so that the vowels' orders stay the same?","In how many ways can you arrange the word ""director"" so that the vowels' orders stay the same? I found an answer where it said we find all possible permutations of this word and then divide it by the possible permutations of the vowels. The possible permutation of the vowels i, e, o are $3! = 6$ .
The $6$ possible arrangements of the vowels are: ieo (the same as in the word), ioe, eio, oie, eoi, oei.
Now, we're supposed to keep the arrangements with the order ieo, then why do we have to divide with $6$ instead of $5$ ? What am I missing here?","['permutations', 'combinatorics', 'discrete-mathematics']"
4489346,Why do we construct Lagrangian submanifolds after symplectic reductions,"I am learning about Hamilton-Jacobi actions, symplectic reductions and Lagrangian submanifolds and I am trying to understand the relation between these concepts. I have read that Lagrangian submanifolds are physically interesting as they can be thought of the space of all momenta at fixed coordinate, locally. Moreover, they allow us to recover the variational form of the Hamiltonian mechanics of the system we are dealing with. As I understand it, symplectic reductions arose from the interest of taking quotients of symplectic manifolds under group actions and the Hamilton-Jacobi group action is the one that makes it possible (via the momentum map $\mu$ ) as it satisfies several requirements as the dimension and the symplectic structure of the manifold. In particular, the Mardsen-Weinstein-Meyer theorem states that the quotient $\mu^{-1}(0) / G$ is a symplectic manifold. Finally, I have read that this latter quotient captures the original Hamiltonian mechanics. From this, most of the documents I have read naturally turn to the wish to recover Lagrangian submanifolds from symplectic reductions. For this, they use the level sets of the momentum map. It is a rough summary and obviously any correction/precision will be appreciated. My confusion comes from the wish to construct Lagrangian manifolds from symplectic reductions. First, $\mu^{-1}(0) / G$ seems to provide some physical insights on our system and so do Lagrangian submanifolds. So, either they do it in a more interesting/accurate way, or I am missing the point in doing it ""twice"". Then, the problem is probably that I am a bit short on basic knowledge here but why are we interested in many level sets of $\mu$ and not only the $0$ one? Is it because the variational form of the Hamiltonian mechanics is local?","['submanifold', 'symplectic-geometry', 'classical-mechanics', 'moment-map', 'differential-geometry']"
4489363,commonness of non measurable sets,"Many times in math the pathological examples are the standard and the nice examples are the exceptions to the rule, like differentiable functions within continuous ones, continuous ones within all functions, algebraic numbers within reals, principal ultrafilters within all ultrafilters. Is there any sense of measure on the subsets of reals so that the statement ""almost every subset is non measurable"" makes sense? Lebesgue ones have the same cardinality as all subsets unlike borel, so it's not immediately obvious from cardinality. If not, is there any sense of how ""common"" non measurable sets are?","['measurable-sets', 'measure-theory']"
4489368,Mathematical formulation of a gravitational law for galaxies beyond newtonian,"The author is not familiared with the current theories and the specific model for dark matter, nor the PDE aspect of mathematical theories related to the dark matter problem, so I apologize in advance if the opinions are out of date or the question is mathematically ill-posed currently, and welcome any advise and improvement! Some backgrouds: Originally dark matter was posed to explain the flattening of spiral rotational curve observed in galaxies, which does not match the density distribution measured from luminosity, modelled by Boltzmann equation (and its deviations). See for example [Chandrasekhar S.;Principles of Stellar Dynamics]. There is also an interesting stability theory which explained the formation of spiral arms [C.LIN,F.SHU;On the spiral structure of disk galaxies]. To quote from Poincare's celebrated ""Les MÃ©thodes Nouvelles de la MÃ©canique CÃ©leste"": 'Le but final de la MÃ©canique cÃ©leste est de rÃ©soudre cette grande question de savoir si la loi de Newton explique Ã  elle seule tous les phÃ©nomÃ¨nes astronomiques.' Motivated by this, if one considers a simplified  2-dimensional system, with axially-symmetric velocity field $u_i(x,t)$ given by rotation curves observed, and $\rho_i(x,t)$ the correponding density distribution measured. My question may be formulated: Question: How to find the functional $F(u_i,\rho_i,\frac{\partial u_i}{\partial x},\frac{\partial \rho_i}{\partial x},\dots,x,t)=0$ which hold for all $i$ , such that  the functional $F$ coincide with newtonian gravitational law locally in $x$ , and is generic in the functional space? A first impression is to do as Kepler did: trial and error. But from a mathematical point of view, there are still many unexplored area in differential\difference equations which may lead to the well-posedness (or ill-posedness) of the above question. Inverse problems in PDEs seem to deal with the case where the differential equations underlining the problem are given, and to find the coefficients. But is there any progress on finding the underlying differential equation itself?","['mathematical-astronomy', 'ordinary-differential-equations', 'functional-analysis', 'partial-differential-equations', 'dynamical-systems']"
4489377,Maxima/minima and monotonicity of $y(x)$ if $y(x)$ is solution of differential equation $\frac{dy}{dx}=y^2-1+\cos x$,"On the open interval $(-c,c)$ , where $c$ is a positive real number, $y(x)$ is an infinitely differentiable solution of the differential equation $$\frac{dy}{dx}=y^2-1+\cos x$$ with the initial condition $y(0)=0$ . Then which one of the following is correct? (A) $y(x)$ has a local maximum at the origin (B) $y(x)$ has a local minimum at the origin (C) $y(x)$ is strictly increasing on the open interval $(-\delta,\delta)$ for some positive real number $\delta$ (D) $y(x)$ is strictly decreasing on the open interval $(-\delta,\delta)$ for some positive real number $\delta$ My Attempt Since $y(0)=0$ and function is differentiable and hence continuous so it should be very close to $0$ in the neighborhood of $x=0$ . So $y^2+\cos x$ could be greater than $1$ in the neighborhood of $x=0$ .On the other hand the opposite may as well be true.So, $(C)$ or $(D)$ may be correct. But what will be actual/logical explanation.","['ordinary-differential-equations', 'monotone-functions', 'real-analysis', 'maxima-minima', 'calculus']"
4489391,The Series $\sum_{n=1}^{\infty}\frac{1}{n^4\sin n}$ converges or diverges?,"The question is closely linked with Convergence of the sequence $\frac{1}{n\sin(n)}$ . I am afraid if I am not interpreting it wrong (read it for the first time when I was asked to do the exercise that I mentioned above). Also the comment box of the above mentioned question gave a helpful info. It was mentioned that if we refer to the sequence $\frac{1}{n^u\sin^vn}$ , then it converges to $0$ as soon as $1+\frac{u}{v}>7.6063$ . The exercise that I mentioned above does not hold it fine. So if it doesn't hold then the sequence will not get close to $0$ . Sine the initial criteria for the convergence of a series doesn't hold, therefore the series will diverge as well. To write its theoretical proof, the same situation as in above link mentioned will happen. To which non-zero irrational number the sequence in my question will converge(if it do)? Correct if I misinterpreted.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4489396,"Do irreducible finite-type dimension-n schemes admit a ""dimension-n atlas""?","We know (eg, see Vakil's Foundations of Algebraic Geometry, 11.1.B)
that a scheme of finite dimension $n$ admits an open covering by affines of dimension $\leq n$ with equality holding at least once. I was wondering, if $X$ is furthermore irreducible and of finite type, then can we further assume that the covering by open affines above can be chosen to consist of (scheme morphisms whose underlying base-space maps are) homeomorphisms onto their images in $X$ , with domains (Specs) all of dimension $n$ ?","['dimension-theory-algebra', 'algebraic-geometry', 'schemes']"
4489427,Strong Convergence of Resolvents,"Let $T$ be an operator on a Banach space $X$ . Let $\lambda \in \mathbb C$ and let $(\lambda_n)_{n \in \mathbb N}$ be a sequence in $\rho(T)$ . The following result is well known: If $(\lambda - \lambda_n)R(\lambda_n, T) \to 0$ with respect to the operator norm, then $\lambda \in \rho(T)$ . However, I asked myself whether $(\lambda - \lambda_n)R(\lambda_n, T) x \to 0$ for all $x \in X$ is enough to deduce that $\lambda \in \rho(T)$ . In this case, one knows that $\sup_{n \in \mathbb N} \lVert (\lambda - \lambda_n)R(\lambda_n, T) \rVert < \infty$ by the uniform boundedness principle. Hence, there is $c > 0$ such that $$\frac{\lvert \lambda - \lambda_n \rvert}{\operatorname{dist}(\lambda_n, \sigma(T))}  \leq \lVert (\lambda - \lambda_n)R(\lambda_n, T) \rVert \leq c < \infty $$ but that does not seems to be enough to deduce that $\lambda \in \rho(T)$ . Maybe it is wrong but then I would like to now a counter example.","['complex-analysis', 'operator-theory', 'functional-analysis', 'real-analysis']"
4489440,Condition for Poncelet's porism for circles,"Prove for $n=1,2$ that if $$\frac{1}{(R+d)^n}+\frac{1}{(R-d)^n}=\frac{1}{r^n}$$ where $R,r$ are the radii of two circles with the smaller one being inside the larger one and $d$ being the distance between their centers, then there is a $n+2$ - polygon inscribed in the smaller one and inscribing the larger one. Is this true for $n\geq 3$ ? We know using Poncelet's porism it is enough to prove this happens for a polygon symmetric around the axis between the two centers and using that I've been able to calculate $n=1$ but $n=2$ was too complicated. Is there a simpler way to see if it's correct?","['projective-geometry', 'circles', 'geometry']"
4489455,Is turtle graphics a group?,"I'm trying to construct examples of combinatorial group theory flavor to introduce the ideas to young students. More specifically, I'm trying to introduce the idea that you can have strings of letters that have some replacement rules (such as ""you can delete $\texttt{XX}$ "" or ""you can replace $\texttt{XY}$ with $\texttt{YX}$ ""), and that you can just play with these things as formal symbols. But it would be nice to also have real examples that the students can act/draw out to verify their computations with the abstract pencil-and-paper manipulations. It struck me that turtle graphics could be one such example. Here's the idea: Let's say that the turtle graphics group $\mathcal{T}$ is generated by elements $L,R,F,B$ . In the typical setting of a turtle with a pen moving in the standard integer lattice in the plane, $L$ means to make a quarter turn left, $R$ a quarter turn right, $F$ move forward one step drawing e.g. a black line, $B$ move backward one step drawing e.g. a blue line. We'll keep track of the multiplicity of lines between the lattice paints, and let's say that black lines and blue lines along the same edge cancel. Then $L=R^{-1}$ and $B=F^{-1}$ , and I think it follows that this is a group. Can anyone give a presentation for this group? I.e. what are the relators? It's clear that $R$ has order $4$ and $F$ has infinite order, so a presentation might start at $\mathcal{T}=\langle R,F \mid R^4,\dotsc \rangle$ . But it's unclear to me whether $R$ and $F$ have any relation. Is the group in fact just the free product $\mathbb{Z}/4 * \mathbb Z$ ? Edit: to make the question more rigorous along the lines of the comment from @Kyle Miller, there are a few things at play here. There is a monoid generated by the symbols $R$ and $F$ . There is a set $D$ (for drawings?) that indexes possible states of the turtle graphics system. This set can be described as follows: Let $V=\mathbb{Z^2}$ denote the set of integer points in the plane ( $V$ for vertices). This is the set of possible positions of the turtle. Let $E$ denote the set of edges in the standard square grid. If you like, the elements of this set are all intervals of the forms $\{x\} \times [y,y+1]$ and $[x,x+1] \times \{y\}$ , where $x,y \in \mathbb Z$ . Then $D$ is the set $V \times \{0,1,2,3\} \times \mathbb{Z}^E$ where the first coordinate is the position of the turtle, the second is the turtle's orientation, and the third tells you how many times each edge has been drawn with multiplicity (positive for a forward step, negative for a backwards one). Then there is an action $\langle R,F \rangle \to \operatorname{Sym}(D)$ , and I suppose $\mathcal{T}$ is the group that is the image of this representation.","['combinatorial-group-theory', 'group-presentation', 'group-theory', 'abstract-algebra']"
4489459,$f(x)=\left\{ \begin{aligned} \cos(x)\quad x \ne 0\\ 0 \quad x=0\\ \end{aligned} \right.$ is discontinuous but differentiable at $0$?,"According to wikipedia : A function is said to be continuously differentiable if its derivative is also a continuous function; I thought of the discontinuous function  : $f(x)=\left\{
\begin{aligned}
\cos(x)\quad  x \ne 0\\
0 \quad x=0\\
\end{aligned}
\right.$ which doesn't exist for $x=0$ and is discontinuous . it's derivative is : $f'(x)=\left\{
\begin{aligned}
-\sin(x)\quad  x \ne 0\\
0 \quad x=0\\
\end{aligned}
\right.$ which exists for $x=0$ and is continuous . Am I doing something wrong ? if not were my assumptions true that discontinuous function can have continuous derivatives ?","['continuity', 'definition', 'derivatives']"
4489465,Calculation of Residue.,"Let $c=\cos \dfrac{\pi}{5}, f(z)=\dfrac{z^2-2cz+1}{z^4-z^3+z^2-z+1}$ . $e^{\frac{3\pi}{5}i}$ is one of the pole of $f$ . (This is because $f$ can be written as $f(z)=\frac{(z+1)(z^2-2cz+1)}{z^5+1}$ .) Then, calculate the Residue of $f$ at $e^{\frac{3\pi}{5}i}=:a$ . I calculated using the formula of Residue, but the calculation is complicated and I don't know how I should proceed. \begin{align}
\mathrm{Res}(f,a)
&=\displaystyle\lim_{z\to a} (z-a)f(z)\\
&=\lim_{z\to a} \dfrac{(z-a)(z^2-2cz+1)}{z^4-z^3+z^2-z+1}\\
&=\lim_{z\to a}\dfrac{z^2-2cz+1+(z-a)(2z-2c)}{4z^3-3z^2+2z-1}\\
&=\dfrac{a^2-2ca+1}{4a^3-3a^2+2a-1}.
\end{align} I have to simplify this, but I don't know how I can do. I think I have to use some technical method. Thanks for any idea.","['complex-analysis', 'residue-calculus']"
4489550,A definition of outer measure in $\mathbb{R}^d$ by using closed boxes,"In my text, the outer measure of a set $E \subseteq \mathbb{R}^d$ is defined using closed boxes or rectangles ${Q_k} = \prod [a_i, b_i]$ via: $$
m^*(E) = \inf \left\{ \sum \text{vol}(Q_k); E \subseteq \cup_k Q_k \right\}
$$ The problem is to show that the usual definition is equivalent to the above: instead of requiring that $E \subseteq \cup Q_k$ , we could have $E \subseteq\cup Q_k^\circ$ just as well. I know simply that that since $E \subseteq \cup_k  Q_k^\circ\subseteq \cup_k{Q_k} $ we have $$
m^*(E) \leq \inf \left\{ \sum \text{vol}(Q_k); E \subseteq \cup_k Q_k^\circ \right\}  
$$ how to go about in the reverse direction? Edit: My (incomplete) Solution Consider any cover $\{Q_k\}$ such that $E \subseteq \cup Q_k^\circ$ . Take closed boxes $\{Q_k'\}$ in the interior of $\{Q_k\}$ such that $E \subseteq \cup Q_k' \subseteq \cup Q_k^\circ$ and $\text{vol}(Q_k') + \epsilon/2^k \geq \text{vol}(Q_k)$ , then: $$
\sum_k \text{vol}(Q_k')\geq \sum_k\left(\text{vol}(Q_k)-\frac{\epsilon}{2^k}\right)=\sum_k \text{vol}(Q_k)-\epsilon \geq \inf \sum_k \text{vol}(Q_k)-\epsilon 
$$ at which point I am stuck.","['self-learning', 'measure-theory', 'lebesgue-measure', 'real-analysis']"
4489595,Conditions for $I-x A$ to be a convergent matrix for some $x\in \mathbb{R}$,"I'm looking for interpretable necessary/sufficient conditions on $A$ which guarantee that $(I-x A)$ is a convergent matrix for some $x\in \mathbb{R}$ For instance, $A$ normal with non-zero eigenvalues having real parts of same sign seems sufficient . What about non-normal $A$ ? For instance, let $A=\left(\begin{matrix}1&2\\0&1\end{matrix}\right)$ , we can visualize trajectories of $\left(I-x  \left(\begin{matrix}1&2\\0&1\end{matrix}\right)\right)^k$ below. $A$ satisfies this condition because of spectral radius condition, $\rho(I-xA)<1$ for any $0<x<2$ Following this it seems the example above is provably convergent because this map is contractive under coordinate change $y=Px$ with $$P=\left(
\begin{array}{cc}
 \frac{2 \sqrt{\frac{29}{3}}}{3} & -\frac{4}{\sqrt{87}} \\
 0 & \frac{10}{\sqrt{87}} \\
\end{array}
\right)$$ Coordinate transformation $P$ under which $x=Ax$ becomes contractive for any stable $A$ can be obtained by solving the following (comes from discrete Lyapunov equation) $$P^T P = T\left(\frac{T^{-1}A^{-T}U}{\lambda(A)-\lambda(A^{-1})^T}\right)U^{-1}$$ with $$\begin{align}
T&=\text{columns of right eigenvectors of } A\\
U&=\text{columns of left eigenvectors of } A^{-1}\\
\lambda&=\text{column vector of eigenvalues}\\
a-b&=\text{subtraction with numpy broadcasting rules}\\
a/b&=\text{componentwise (Hadamard) division}
\end{align}
$$ notebook","['contraction-operator', 'linear-algebra', 'fixed-point-theorems', 'dynamical-systems']"
4489597,Proposition 2.31 of Mumford's algebraic geometry I,"I'm reading Algebraic Geometry I: Complex Projective Varieties by Mumford and have trouble understanding the proof of proposition 2.31. Let $p_2$ be a projection, and the following proof is only partial. (2.31) Proposition. Let $S \subset \mathbb{C}^n \times \mathbb{C}^m$ be a constructible set. Then $p_2(S)\subset \mathbb{C}^m$ is a constructible set. In particular, if $S$ is a subvariety of $\mathbb{C}^{n+m}$ and $\overline{p_2(S)}$ is the Zariski-closure of the image, then $p_2(S)$ contains a Zariski-open set in $\overline{p_2(S)}$ . Proof. By taking compositions, we are reduced to the case of the projection $$p_2:\mathbb{C} \times \mathbb{C}^m \longrightarrow \mathbb{C}^m.$$ By induction on the dimension of $\overline{p_2(S)}$ , we see easily that it is enough to prove the special case : if $S \subset \mathbb{C}^{m+1}$ is a subvariety and $S_0 \subset S$ is Zariski-open, then $p_2(S_0)$ contains a Zariski-open subset of $\overline{p_2(S)}$ . Let $T = \overline{p_2(S)}$ , then $T$ is variety. Let the affine coordinates rings of $S$ and $T$ be $R_S$ and $R_T$ . Then, there is the canonical monomorphism $$\mathbb{C}[X_2, ... X_{m+1}]/I(T)=R_T \longrightarrow R_S = \mathbb{C}[X_1, ..., X_{m+1}]/I(S).$$ Thus $R_S \cong R_T[X_1]/U$ for some ideal $U$ . We distinguish two cases: $U = (0)$ hence $S = \mathbb{C}\times T$ , and $U \neq (0)$ hence $\dim S = \dim T$ . In the first case, ... How do we prove the proposition from the special case? Thank you.",['algebraic-geometry']
4489607,A construction of an uncountable product of independent Bernoulli variables,"I have an intuitive stochastic process as follows, but not sure how to construct it rigorously on some probability space. Consider the unit interval $[0,1]$ , each point $x\in [0,1]$ is associated with an independent Bernoulli( $0.5$ ) random variable. Then each realization can be viewed as a function $f$ from $[0,1]$ to $\{0,1\}$ . The level set $\{x: f(x) = 1\}$ is then a random set on $[0,1]$ . I want to argue the set is almost surely (Lebesgue)-measurable, and almost-surely has measure $0.5$ . However, the above construction does not seem to be rigorous. Therefore, I want to know if there exists a probability space $\Omega$ that admits the a stochastic process $f(t,\omega) \in \{0,1\}$ , such that $1.$ all the finite dimensional distribution is a product of independent Bernoulli. $2.$ For almost every $\omega$ , the set $\{x: f(x,\omega) = 1\}$ is measurable, and has probability $0.5$ . It seems the first condition can be guaranteed using the Kolmogorov's extension theorem. But I have not idea how to guarantee the second condition.","['measure-theory', 'bernoulli-distribution', 'stochastic-processes', 'probability-theory', 'probability']"
4489614,Show $S_7$ is isomorphic to the subgroup of all those elements of $S_8$ which leave the number $5$ fixed,"First of all I don't know how to write with correct notation in this site, sorry about that. Second, I am a beginner with group theory so I am trying to solve this exercise (Fraleigh). Until now I have a function that maps a permutation in $S_7$ to $S_8$ like this: Suppose $\sigma$ , any permutation in $S_7$ . $5$ if $x=5$ $\sigma(x)$ if $\sigma(x)\neq 5$ Here I have the first problem, because I have two cells free: $8$ and the element that is sent to $5$ in $S_7$ , so I maps like this: $\sigma(5)$ if $\sigma(x)=5$ $8$ if $x=8$ So I would appreciate your help in this exercise, I don't know if this function is correct and how to prove if in fact is an isomorphism.","['symmetric-groups', 'group-theory', 'group-isomorphism']"
