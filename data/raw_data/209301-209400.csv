question_id,title,body,tags
4201089,"Can we make a set out of $\epsilon$-$\delta$ pairs, given limit?","If I was given that $\lim_{x\to x_0}f(x)=L$ , for some $f:\mathbb R\to\mathbb R$ . This means that for every $\epsilon>0$ , there is some $\delta>0$ , such that $|x-x_0|<\delta\implies|f(x)-L|<\epsilon$ . These $\delta$ values for corresponding $\epsilon$ exist, but are unknown. Given this can I make a set like: $$S=\{(\epsilon,\delta)\in\mathbb R\times\mathbb R\mid (\epsilon>0)\wedge(\delta>0:(\forall x:|x-x_0|<\delta\implies|f(x)-L|<\epsilon))\}$$ (I didn't know how to write it in a way for everyone to understand). Essentially I want to make a set (or show it exists) of all possible $\epsilon$ - $\delta$ pairs satisfying the limit definition. Is the set possible? Why / why not? I can think of the Zorn's lemma, as this seems like a 'maximal set', but I don't know how to apply it. What kind of ordering would it be? What are the chains? Maybe I can make a family: $F=\{U_i\mid i\in\mathbb R\}:U_i=\{\delta\mid\forall x: |x-x_0|<\delta\implies |f(x)-L|<i\}$ ? I've only ever seen Zorn's lemma in a proof showing every vector space has a basis.","['elementary-set-theory', 'limits', 'epsilon-delta']"
4201093,Sinai’s Walk | Proof of basic result | how does the basic result follow from the two theorems in Sinai’s paper?,"In “The Limiting Behavior of a One-dimensional Random Walk in a Random Medium” by Sinai, two theorems are proved in the last sections. I fail to understand why the basic result follows from these theorems. Basic result : It exists random variable $m(0)$ independent of the environment, such that $\forall\eta>0$ : \begin{align}
\mathbb{P}\left(\left|\frac{\mathsf{X}_{n}}{\log^{2}n}-m(0)\right|>\eta\right)\longrightarrow_{n\rightarrow\infty}0.
\end{align} Theorem 1 : Let $M_{i1},M_{i2}\in\mathfrak{M}$ and $[M_{i1},M_{i2}]$ is a valley with the depth $d([M_{i1},M_{i2}])>1$ and $0\in[M_{i1},M_{i2}]$ . Then: \begin{align*}
\mathbb{P}\left(\mathsf{X}_{m}\in[M_{i1}\log^{2}n,M_{i2}\log^{2}n],\forall 0\leq m\leq n\right)\to_{n\to\infty}1.
\end{align*} Theorem 2 : We consider an environment of $C_{n}$ . Then for $\frac{1}{2}n\leq r\leq n$ \begin{align*}
\mathbb{P}\left(\omega_{r}=m^{(0)}\right)\to_{n\to\infty}1
\end{align*} uniformly for the environment of $C_{n}$ . Does anyone know? An heuristic approach is absolutely fine! For more details see the paper by Sinai: https://epubs.siam.org/doi/abs/10.1137/1127028","['random-walk', 'probability-distributions', 'limits', 'probability-theory', 'probability']"
4201146,Prove Directional Derivative Exists For All Unit Vectors,"I'm pretty stuck on the following problem. Define $f: R^2 \rightarrow R$ by $$f(x,y) = \frac{xy^2}{x^2+y^2} \quad\text{ if }\quad (x,y) \neq (0,0),$$ $$f(x,y) = 0 \quad\quad\quad\text{ if }\quad\quad (x,y) =(0,0).$$ Prove $D_uf$ exists for all $u$ . So I know that this is the directional derivative, and that $u$ can be any unit vector. But there are infinite possibilities for $u$ , so how can I show the derivative exists for all of them?","['partial-derivative', 'derivatives', 'real-analysis']"
4201166,Not every finite metric space embeds in an $\mathbb{R}^{k}$,"This problem is present in ""Supplements to the Exercises in Chapter 1-7 of Walter Rudin's Principles of Mathematical analysis"" by Prof. George M. Bergman, which states as follows, Let $X$ be a $4$ -element set $\{w, x, y, z\}$ , and let $d$ be the metric on $X$ under which the distance from $w$ to each of the other points is $1$ , and the distance between any two of those points is $2$ . Show that no function $f$ of $X$ into a space $\mathbb{R}^{k}$ is distance-preserving, i.e., satisfies $|f(p) - f(q)| =  d(p, q)$ for all $p, q \in X$ . The above example has the property that every $3$ -point subset of $X$ can be embedded (mapped by a distance-preserving map) into space $\mathbb{R}^{k}$ for some $k$ , but the whole $4$ -point space cannot be so embedded for any $k$ . Can you find a $5$ -point metric space, every $4$ -point subset of which can be so embedded but such that the whole $5$ -point space cannot? I searched on the internet about this topic and found the Cayley-Menger determinant may be helpful to part 2. However, here is a concrete $5$ -point case and I was wondering if there are some intuitive counterexamples? And I also have no clue about part 1, can anyone give me some hints on it. Thanks in advance.","['euclidean-geometry', 'metric-spaces', 'geometry', 'real-analysis', 'isometry']"
4201191,Analytical solution to non-linear 2nd order ODE,"Is there an analytical solution to the following non-linear ODE? $$y''(x) = A \frac{y}{B+\alpha y} ... (1)$$ where $A,B,\alpha$ are constants. The boundary conditions are: $$y'(x = 0) = y'(x = L) = 0$$","['nonlinear-analysis', 'ordinary-differential-equations']"
4201238,"Are there any primitive examples besides 56,65 where the sum of a number's cube and its digital reverse's cubes is square?","I saw a question a while back which asked (paraphrased): how many examples exist of the form $a^n + b^n = c^2$ where $b$ is $a$ 's digits in reverse, $n \geq 2$ , and $a,b,c$ are coprime. For $n = 2$ , the proof is not too hard that there are zero solutions (via mod 3). For $n \geq 4$ , the reverse part isn't even needed, there are no solutions ( http://matwbn.icm.edu.pl/ksiazki/aa/aa86/aa8631.pdf contains a proof). For $n = 3$ , $56^3 + 65^3$ is a square. My question is whether there exists more cases. Trying mod 9 gives some restrictions, but I couldn't narrow it that significantly. Coding a quick test shows there are no other ""small solutions"" (Up to a = 10 million). Code viewable here: https://codeinterview.io/YRGDWJVCAH","['number-theory', 'perfect-powers', 'elementary-number-theory', 'decimal-expansion']"
4201258,functional equation on integers with divisibility: $f(a) + f(b)\mid 2(a + b - 1)$,"Find all $f : \mathbb{N} \to \mathbb{N} $ such that $f(a) + f(b)$ divides $2(a + b - 1)$ for all $a, b \in \mathbb{N}$ . I think the answer is $\boxed{f(x)=2x-1, f(x)=1}.$ $$P(1,1) \implies f(1)+f(1)| 2(1+1-1)=2\implies f(1)=1$$ $$P(1,a)\implies  f(1)+f(a)|2(1+a-1)=2a \implies f(a)+1|2a \implies f(a) \le 2a-1.$$ We have $$P(1,2)\implies f(1)+f(2)|2(1+2-1)=4\implies f(2)=3~~\text{or}~~f(2)=1.$$ For $p$ an odd prime. We get, $$P(1,p)\implies f(p)+1|2p \implies f(p)=1,p-1, 2p-1$$ Now if $f(p)=p-1 $ for some $p$ then $$P(p,p)\implies f(p)+f(p)=2(p-1) | 2(2p-1)$$ Not possible. This was my progress...  Any hints or solutions? I would prefer if someone can send a sketch of hints( Rather than a full solution). Thanks in advance.","['number-theory', 'functional-equations', 'functions', 'elementary-number-theory']"
4201262,Any nice way to solve this recursive relation?,"The problem defines a function as $f(x,0)=f(x-1,1)$ , $f(0,y) = (y+1) \mod 5$ and $f(x,y) = f(x-1, f(x,y-1))$ , want to compute $f(333,3)$ ? Recursively with tedious algebra, the problem can be computed, but I really hope to learn some nice trick to reveal an easy path. Tried but couldn't find one. Thank you~~",['functions']
4201287,Prove that $ \det(A^4 + A^2 B^2 + 2A^2 + I) \geq 0 $,"Problem: Let $ A $ and $ B $ be an $ n \times n $ matrices with real entries. If $ AB = -BA $ , prove that $$ {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} \geq 0. $$ My Approach: If $ A $ invertible, then $$ AB = -BA \implies AA^{-1}B = -BA^{-1}A \implies B = -B \implies B = O. $$ So, $$ {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} = {\det}{\left(A^4 + 2A^2 + I\right)} = {\det}{\left(\left(A^2 + I\right)^{2}\right)} = \left({\det}{\left(A^{2} + I\right)}\right)^2 \geq 0. $$ If $ B $ invertible, then $$ AB = -BA \implies AB^{-1}B = -BB^{-1}A \implies A = -A \implies A = O. $$ So, $$ {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} = {\det}{(I)} = 1 \geq 0. $$ My Questions: Can we 'center' multiply both sides of the equation $ AB = -BA $ with a matrix? For example, in my solutions, $ AB = -BA \implies AA^{-1}B = -BA^{-1}A $ . How do we prove it when $ A $ and $ B $ not invertible? Thanks","['matrices', 'inequality', 'determinant']"
4201348,How to find the integral of $\frac{(1+x\ln(x)) (1+\ln(x))}{x^2 \ln(x) (1+\ln(x))+1}$,"$$\int \frac{\ln{\left( \mathrm{e}\,x^{x+1} \right)} + \left[ \ln{ \left( x^{\sqrt{x}} \right) } \right]^2 }{1 + x\ln{ \left( x \right) } \ln{ \left( \mathrm{e}^x\,x^x \right) }}dx=f(x)+C $$ where $f(1)=0$ , then $e^{e^{f(2)}-1}$ is equal to? $\displaystyle \begin{align*} \frac{\ln{\left( \mathrm{e}\,x^{x+1} \right)} + \left[ \ln{ \left( x^{\sqrt{x}} \right) } \right]^2 }{1 + x\ln{ \left( x \right) } \ln{ \left( \mathrm{e}^x\,x^x \right) }} &= \frac{ \ln{\left( \mathrm{e} \right) } + \ln{ \left( x^{x+1} \right) } + \left[ \sqrt{x} \, \ln{ \left( x \right) } \right] ^2 }{ 1 + x\ln{ \left( x \right) } \left[ \ln{\left( \mathrm{e}^x \right) } + \ln{ \left( x^x \right) } \right] } \\ &= \frac{ 1 + \left( x + 1 \right) \ln{ \left( x \right) } + x \, \left[ \ln{ \left( x \right) } \right] ^2 }{ 1 + x \ln{ \left( x \right) } \left[ x + x\ln{ \left( x \right) } \right] } \end{align*}$ , but could not solve it further.","['integration', 'indefinite-integrals', 'calculus']"
4201360,How do i approach ahead in this question,"Let $a,b \in\Bbb N$ , $a$ is not equal to $b$ and the quadratic equations $(a-1)x^2 -(a^2 + 2)x +a^2 +2a=0$ and $(b-1)x^2 -(b^2 + 2)x +b^2 +2b=0$ have a common root, then the value of $ab/5$ is So what I did was, I subtracted the two equations and got $x=(a+b+2)/(a+b)$ I tried putting it in equation,it didn’t work,then  i tried adding the equations and then put the value,still didn’t work. I cant seem to figure out how to approach this problem.Can anybody help out?","['algebra-precalculus', 'quadratics', 'roots']"
4201375,"Find the minimum of $P(X=0)$ when $E[X]=1,E[X^2]=2,E[X^3]=5$ using the probability generating function","I was given the following exercise. Let $X$ be a random variable that takes non-negative natural number
values such that $E[X]=1,E[X^2]=2,E[X^3]=5$ . Find the minimum value of $P(X=0)$ using the taylor expansion of the probability generating function
at $z=1$ . I know the method not using the generating function stated in this question . My attempt: Let $G(z)=E[z^X]$ be the probability generating function of $X$ . Then by definition, $G(z) = P(X=0)+P(X=1)z+P(X=2)z^2+\cdots $ Also, since $E[\frac{X(X-1)\cdots (X-n+1)}{n!}]=\frac{G^{(n)}(1)}{n!}$ according to Wikipedia , we have $\displaystyle G(z) = \sum_{n=0}^{\infty}E\left[\frac{X(X-1)\cdots (X-n+1)}{n!}\right](z-1)^n$ Substituting $z=0$ yields $\displaystyle P(X=0) = \sum_{n=0}^{\infty}E\left[\frac{X(X-1)\cdots (X-n+1)}{n!}\right](-1)^n$ Note that for $n=0,1,2,3$ , the coefficient can be calculated as follows: $\begin{align} E[1] &= 1  \\ E[X]&= 1 \\ E[X(X-1)/2] &= E[X^2]/2 -E[X]/2 = 1-1/2 =1/2 \\ E[X(X-1)(X-2)/3!] &= E[X^3]/6 -E[X^2]/2 +E[X]/3 = 5/6-2/2+1/3 \\ &= 1/6 \end{align}$ Therefore, $\displaystyle \begin{align} P(X=0) &= \sum_{n=0}^{\infty}E\left[\frac{X(X-1)\cdots (X-n+1)}{n!}\right](-1)^n \\ &=1-1+\frac{1}{2} -\frac{1}{6} +\sum_{n=4}^{\infty}E\left[\frac{X(X-1)\cdots (X-n+1)}{n!}\right](-1)^n \\ &= \frac{1}{3} + \sum_{n=4}^{\infty}E\left[\frac{X(X-1)\cdots (X-n+1)}{n!}\right](-1)^n\end{align}$ According to the question linked above, $1/3$ is the minimum, so I think we need to prove that the sum is non-negative to complete the proof. However, I was unable to do so. Am I on the right path? If not, what is the correct one? If yes, how can I finish it?","['probability', 'generating-functions']"
4201430,Area Enclosed by $x^{2n} +y^{2n}=2n$,"Recently I was just playing with the idea of shapes with higher coefficients of x and y. Using Desmos, I plotted graphs of the form $x^{2n}+ y^{2n} = 1$ All these of,course pass through the four points $\{(1,0),(0,1),(0,-1),(-1,0)\}$ . As one increases the value of $n$ , The graph sort of pushes towards the lines $x,y=1,-1$ . This is reasonable as none of the variables can actually exceed unity. But when instead of 1 we define the shape as $x^{2n}+ y^{2n} = 2n$ , the graph bulges outwards, but with increasing values of $n$ , shrinks back, visually and intuitively I think the area enclosed by the graph would become 4 as ${n\to \infty} $ . But I could not mathematically prove it using integration or otherwise. Also, is there any name for such shapes and do we use them anywhere for some practical benefits?
And can we manipulate the graphs to change the vertices of the limiting ""square"", if there is one? PS: Just to clarify my knowledge base: I am a Class 12th student in India and preparing for JEE ADVANCED, so I am familiar with basic concepts of finding areas using calculus.","['integration', 'calculus', 'graphing-functions']"
4201480,Proof of Theorem $7.13$ in Rudin's RCA,"Theorem $7.13$ , Walter Rudin's Real and Complex Analysis . Why does it suffice to prove for the case $\mu\ge 0$ ? $\mu = \mu^+ - \mu^-$ is the Jordan decomposition of $\mu$ , where $\mu^+,\mu^-$ are positive measures. How does the general case follow from $\mu\ge 0$ and the Jordan decomposition? Why is $\overline D\mu$ a Borel function? I see that $\sup_{0<r<1/n} Q_r\mu(x)$ decreases as $n$ increases, and is a lower-semicontinuous function for each $n$ (following the reasoning in Section $7.2$ .). However, I don't see how this implies that $\overline D\mu$ is a Borel function. I also know that any lower-semicontinuous function is Borel, so it'd suffice to prove lower-semicontinuity, if we can. Why is it true that for every $x\in K^c$ , $$(\overline D\mu)(x) = (\overline D\mu_2)(x)$$ Proof attached for reference: Thank you!","['proof-explanation', 'measure-theory', 'real-analysis']"
4201492,Bounded martingale is in $L^2$?,"Suppose $Y$ is a local martingale started from $0$ and define for each $k \gt 0$ the stopping time $T_k=\inf\{t \gt 0 : |Y_t|=k\}$ . Then, for a given $k$ , $(Y^{T_k}_t)_{t\geq 0}$ is a bounded continuous martingale. Is it true that it is also an $L^2$ martingale? I think it is, and in fact that it is in any $L^p$ but I would like to have confirmation. I mean, why can't I just write $$ E[(Y^{T_k}_t)^2] \leq E[k^2]=k^2$$","['martingales', 'probability-theory']"
4201501,Exist Contravariant derivative?,"I'm confused about the index representation, If the index up represents a contravariant tensor I can do this with the derivative and get it contravariant derivative?Or when it is said that, it is covariant or contravariant by ""essence""(before the contravariant and covariant formulation like the gradient Which is  transformed covariant). I can't get an image because I don't have a score or whatever then I'll try to write, (Contravariant) $T^{i'}=\dfrac{\partial x^{i'}}{\partial x_j}T^j$ (covariant). $V_{i'}=\dfrac{\partial x^j}{\partial x^{i'}}V_j$ Then $g^{ij}\partial_j=\partial^i$ , I can transformed it contravariant, For it has index above, But the derivative is always covariant, im confused, what is $\partial ^j$ .","['definition', 'tensors', 'derivatives', 'differential-geometry']"
4201521,Finding the maximum number of members in the math club.,"I asked a question few days ago which was from a local math contest in my city. The question and the solution seems interesting to me and I am interested in solving the generalization of the problem. So, the generalized problem is as follows: Professor Liyung wants to make a math club consisting of his $n$ students. But there is a problem. Each student is enemies with exactly $k$ ( $1\leq k \leq n-1$ ) students. And no one wills to be a member of the club if any of his enemies is already a member of the club. Let $M$ be the maximum number of members the club can have. Find all possible values of $M$ . (If student $A$ is an enemy with student $B$ , then student $B$ is an enemy with student $A$ . Student $A$ is not enemy with himself.) Here, $n,k$ are given numbers such that they satisfy the conditions of the problem (i.e. it is possible to draw a graph with $n$ vertices each of the vertices having degree $k$ ). Here are my workings regarding the problem: My workings I first tried for a fixed value of $k$ . And I got the following: For $k=1$ , the problem statement is true for only even $n$ . Then, $M$ has only one possible value that is $\frac n 2$ . For $k=2$ , all values of $M$ are possible in the range $[\lceil\frac n 3\rceil,\lfloor\frac n 2\rfloor]$ . (Explanation is in the link.) For $k=3$ , the upper bound of $M$ is $\lfloor \frac n 3\rfloor$ and the lower bound of $M$ is $\lceil\frac n 4\rceil$ . However, I don't have a nice argument to prove the upper bound. The proof of the lower bound is as follows: Proof: If we choose $\lceil\frac n 4\rceil -1$ students as members, then there are at most $3\lceil\frac n 4\rceil -3$ students who can't be members of the club. Then, we can choose at least $1$ students from the remaining students as a member of the club. This proves that $M$ is at least $\lceil\frac n 4\rceil$ for $k=3$ . So, I think all values of $M$ are in $[\lceil\frac{n}{k+1}\rceil,\lfloor\frac{n}{k}\rfloor]$ . The lower bound can be proved by following the similar steps as in for $k=3$ . However, I am unable to show that all values in $[\lceil\frac{n}{k+1}\rceil,\lfloor\frac{n}{k}\rfloor]$ can be a value of $M$ . I hope my thoughts are correct. I need to complete the solution i.e. proving the upper bound and showing that all values in the stated interval are possible. The original problem involves graph theory in its solution. So, can the generalized problem be solved with graph theory?","['contest-math', 'graph-theory', 'extremal-combinatorics', 'combinatorics', 'recreational-mathematics']"
4201550,John Conway's proof of Riesz representation theorem,"I'm studying Functional Analysis from John Conway's ""A Course in Functional Analysis"" and I needed to go over some things from the first chapter and decided to reread the proof of Riesz representation theorem, it goes as follows: Theorem: Let $L: \mathcal{H} \rightarrow \mathbb{F}$ be a bounded linear functional, then there is a unique vector $h_0$ such that $L(x) = \langle x, h_0 \rangle \forall x \in \mathcal{H}$ . Proof: Let $M = Ker(L)$ . Because L is continuous M is a closed subspace of $\mathcal{H}$ . We may assume that $M \neq \mathcal{H}$ and hence $\mathcal{H}= M \oplus M^\perp$ . So we may choose $f_0 \in M^\perp$ such that $L(f_0) = 1$ From here and until here I understand the proof but I don't get what guarantees the existence of such a $f_0$ , does every linear functional map something to 1? If so why is that? I am sorry if I'm missing something obvious.","['proof-explanation', 'functional-analysis', 'riesz-representation-theorem']"
4201582,Proving a prior distribution defined by a trignometric basis on $L^2(\mathbb{T})$ is Hölder continous,"Consider $L_2(\mathbb{T})$ with the basis $$\phi_{2k}(x)=\sqrt{2}\cos(2\pi k x)\\ \phi_{2k-1}(x)=\sqrt{2}\sin(2\pi k x)$$ for $k\in\mathbb{N}$ . The functions $\phi_k$ belong to the domain $H^{2p}$ of the operator $C_0^{-1}=\eta(-\triangle)^p+KI$ where $\triangle$ is the Lapalcian and I the identity,K and $\eta$ are constants.
A simple application of the operator on the basis above yields: $$C_0^{-1}\phi_{2k-1}=\eta((4\pi^2k^2)^p+k)\phi_{2k}\\C_0^{-1}\phi_{2k}=\eta((4\pi^2k^2)^p+K)\phi_{2k-1} $$ It follows that $C_0$ is the operator on $L_2(\mathbb{T})$ which is diagonalized by the basis of $\phi_k$ , with eigenvalues: $$\lambda_k=\eta \left(\left( 4\pi^2\left[\frac{k}{2}\right]^2\right)^p +K\right)^{-1} $$ A prior distribution can be of the centered Gaussian process can be defined $V=\{V_x\}_{x\in\mathbb{T}}$ : $$V_x=\sum_{k\in\mathbb{N}}\sqrt{\lambda_k}\phi_k(x)Z_k$$ where $Z_1,Z_2....$ are independent, standard Gaussian random varaibles. Using this series representation a number of basic properties of the prior can be easily derived: Lemma :
(i) There exists a version of $V$ which a.s has sample paths that are Hölder continous of order $\alpha$ for every $\alpha<p-\frac{1}{2}$ Proof : Note that $\sqrt{\lambda_k}\sim k^{-p}$ asymptotically. Using also the differential relations between the basis functions $\phi_k$ it is straightforward to see that the process $V$ has $p-1\geqslant 1$ weak derivatives in the $L^2$ sense. Moreover, using Kolgomorov's classical continuity theorem it can be shown that this $(p-1)$ th derivative has a version with sample paths that are Hölder continuous of order $\gamma$ for every $\gamma<\frac{1}{2}$ . Combining this we see that $V$ has a version of $\alpha$ -Hölder sample paths, for every $\alpha<p-\frac{1}{2}$ . In particular, it holds that all the mass of the prior $\Pi$ is concentrated on $C^1(\mathbb{T})$ . $\blacksquare$ I am trying to understand by completing the proof of the above Lemma for that I started by applying the Kolgomorov theorem to (p-1) the derivative of prior. For the theorem to be applicable the following condition must be verified: \begin{equation}\mathbb {E} [d(X_{t},X_{s})^{\alpha }]\leq K|t-s|^{1+\beta }\end{equation} given $X:[0,\infty)\times \Omega\to S$ For this case I picked up two distincted point $x,y$ and then $E(||\nabla^{p-1}V_x-\nabla^{p-1}V_y||^2)\leqslant \langle \sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(x)Z_k-\sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(y)Z_k ,\sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(x)Z_k-\sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(y)Z_k \rangle\langle \sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(x)Z_k-\sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(y)Z_k ,\sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(x)Z_k-\sum_{k\in\mathbb{N}}\sqrt{\lambda_k}(2\pi k)^{p-1}\phi_k(y)Z_k \rangle$ I do not know how to compute the above and get an expression similar to $K|t-s|^{1+\beta }$ . Question : Can someone help me complete the proof? How can the author deduce that $V$ is Hölder continuous if its $p-1$ th derivative is? Thanks in advance! If there is a need for more details I will be very happy to update my question.","['probability-theory', 'functional-analysis']"
4201594,"Is $\left\{ a \sqrt{3} - b \sqrt{2} | a, b \in \mathbb{N} \right\}$ dense in $\mathbb{R}$?","I come along this problem while messing around with some other problems. So the question is: Is $\left\{ a \sqrt{3} - b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\}$ dense in $\mathbb{R}$ ? I know for sure that: $A = \left\{ a \sqrt{3} + b \sqrt{2} \ | \ a, b \in \color{red}{\mathbb{Z}} \right\}$ is dense in $\mathbb{R}$ . $B = \left\{ a \sqrt{3} + b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\}$ is not dense in $\mathbb{R}$ . But what about $\left\{ a \sqrt{3} - b \sqrt{2} \ | \ a, b \in \mathbb{N} \right\}$ ? I suspect that it may be dense; however, I cannot prove it. Can someone please give me a push? Thanks very much in advance, :*","['general-topology', 'metric-spaces', 'real-analysis']"
4201618,Why is it necessary to take the 2nd derivative to determine concavity?,"I'm having trouble understanding why you need the second derivative to determine concavity. For example, if I have the equation: $y = -4x^2 + 24x + 42$ $y' = -8x +24$ I know from the first derivative alone that the slope is -8 from what you learn from $y=mx+b$ . So what's the point in taking the second derivative? Isn't taking the slope of the slope (second derivative) redundant at this point? For context: I have read / listened to explanations online. And I understand the explanations about getting the first derivative for the slope. But then the explanation says something along the lines of, ""So it follows that the second derivative will give us what we need for concavity. If the slope is greater than 0,...If the slope is less than 0,..."" But if the first derivative is a tangent line (straight line) then what are we taking the slope for again? I thought taking the derivative could be used for straight lines, but was specifically useful for non-linear graphs. Otherwise, if it's linear, we could just use $y=mx+b$ to determine the slope. Or am I oversimplified this? And the real point is that if you have higher order equations, you can differentiate until you have no variables and that gives you the slope? Note:
I did see this question, but I'm still confused. Concavity & Second Derivative","['calculus', 'derivatives']"
4201643,What is the philosophy behind solving ODE's? (a question from someone who hasn't taken a formal ODE's class),"I've started learning ODE's on my own and here is something that I don't understand. I've noticed that the book I am following (and all the other books that I have) is hand wavy when it comes to specifying the interval of the solution and doesn't realy worry too much about dividing by $0$ . I will provide an example: let's solve the ODE $$t^2x'=x^2+tx+t^2,$$ where $x=x(t)$ . We divide by $t$ and the equation becomes $x'=\left(\frac{x}{t}\right)^2+\frac{x}{t}+1$ . We make the variable change $y=\frac{x}{t}$ and after some computations we get that $\arctan y=\ln t+C$ for some constant $C\in \mathbb{R}$ . Now, the book says that this implies that $y=\tan(\ln t+C)$ , so $x=t\tan(\ln t+C), C\in \mathbb{R}$ . I have two questions here: Why can we divide by $t$ at the beginning? I mean, yes, I agree that this solves our equation, but aren't we kind of missing some solutions? Here is the first philosophical problem that I have with ODE's: is the focus on somehow obtaining a solution, even though we make some assumptions along the way, that is defined on some interval $I\subset \mathbb{R}$ that we don't even care if it is really really small rather than on trying to find all the differentiable functions that satisfy our identity (as the focus was in, say, functional equations that appear at high school math contests)? Why after $\arctan y=\ln t+C$ we may write that $y=\tan(\ln t+C)$ for any real constant $C$ ? I mean, the $\tan$ function is not defined everywhere and we most certainly can choose some $C$ such that for some $t$ we have $\ln t+C=\frac{\pi}{2}$ for instance. Is the philosophy here the same that I presented in 1 i.e. assuming that the interval on which our solution is defined is chosen appropriately so that everything makes sense?","['calculus', 'ordinary-differential-equations']"
4201713,On the angle of a polar decomposition of a matrix,"Let $A\in\mathbb{M}_{n\times n}(\mathbb{C})$ . $A$ is non-singular. Then $A=UR$ where $U$ is a unitary matrix, and $R$ is a positive-semidefinite Hermitian matrix. I understand the intuition that this is analogous to $U$ encoding some $\theta$ and $R$ encoding the modulus/radius $r$ , taking the complex plane in polar coordinates. I have seen it confidently written that $|A|=|U|\cdot|R|=\exp(i\theta)\cdot r$ . Intuitively, by the definition above, this should make sense, since any unit square/cube/etc. would be rotated and scaled under $A$ , and this determinant agrees with that. However, being more formal, I wonder how a person might prove this, or find $\theta.$ $|\exp(A)|=\exp(tr(A))$ , and also any unitary $U=\exp(iH)$ , where $H$ is Hermitian. Therefore $|U|=|\exp(iH)|=\exp(tr(iH))=\exp(i\cdot tr(H))$ . If $|U|$ is to be $\exp(i\theta)$ , then the mystery $H$ must have a trace equal to $\theta$ . Is this correct? As a complete side-question from the main focus of this post, is the finding of $H$ a messy problem almost always left to computers - can the trace= $\theta$ idea be leveraged for this? And when Wikipedia mentioned $U=\exp(iH)$ , did they mention it out of trivia, or because of some utility, some application of this fact?","['unitary-matrices', 'hermitian-matrices', 'linear-algebra']"
4201772,Exponential formula for Poisson random measure,"We want to prove the following result. Let $M$ be a Poisson random measure with intensity measure $\mu$ . Then for every measurable set $B$ such that $\mu(B) < \infty$ and for all functions $f$ such that $\int_B e^{f(x)} \mu(dx) < \infty$ $$ \mathbb{E} \exp \left\{ \int_B f(x) M(dx) \right\} = \exp \left\{ \int_B  (e^{f(x)}-1 ) \mu(dx) \right\}$$ We should condition by the expectation on $\mu(B)$ . From Course Online : Definition 1.6 Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $E\subset \mathbb{R}^k$ , and $\rho$ a measure on $(E,\mathcal{E})$ . A Poisson random measure on $E$ with intensity $\rho$ is a function with values in $\mathbb{N}$ : $$ \begin{array}{ccccc}
M : & \Omega \times \mathcal{E} & \to & \mathbb{N} & \text{s.t.} \\
& (\omega, A) & \mapsto & M(\omega, A), & 
\end{array}$$ $\forall \omega \in \Omega$ , $M(\omega, \cdot)$ is a Radon measure on $E$ , that is, $\forall A \in \mathcal{E}$ measurable and bounded, $M(A) < \infty$ is a r.v. with values in $\mathbb{N}$ ; $\forall A \in \mathcal{E}$ , $M(\cdot,A) = M(A)$ is a Poisson r.v. with parameter $\rho(A)$ ; for any $A_1, \dots, A_n$ disjointed sets, the r.v. $M(A_1), \dots, M(A_n)$ are independents. Now let $M$ be a Poisson random measure with intensity measure $\mu$ and let $A$ be a measurable subset s.t. $0 < \mu(A) < +\infty$ . Then the following two random measures on the subsets of $A$ have the same distribution conditionally on $M(A)$ : $M|_A$ , the restriction of $M$ to $A$ . $\widehat{M}\!{}_A$ defined by $\widehat{M}\!{}_A(B) = \sharp\{X_i \in B\}$ for all measurable subsets $B$ of $A$ , where $X_i$ , $i = 1, \dots, M(A)$ are independent and distributed on $A$ with the law $\frac{\mu(\mathrm{d}x)}{\mu(A)}$ . This implies in particular that $$ \boxed{\mathbb{E}\exp\left(\int_{A}f(X) \, M(dx)\right) = \exp\left( \int_{A} (e^{f(x)} - 1) \mu(dx) \right)} \tag{1.7}$$ for any function $f$ such that $\int_A e^{f(x)} \mu(dx) < +\infty$ . This can be obtained by conditioning the expectation on $\mu(A)$ and by the previous result on $M|_A$ and $\widehat{M}\!{}_A$ . Q. We have a proof here, could you explain the first line $E(e^{iuX} \mid N=n)$ ?","['levy-processes', 'probability-theory', 'poisson-process']"
4201793,Pullback and etale cohomology,"I am pretty sure I don't understand well the action of a pullback of some etale map on the first etale cohomology group. In fact, let $f : T \rightarrow X$ (etale map) be a $X$ -torsor for some algebraic group $G$ . It gives a cohomology class on $H^{1}_{et}(X, G)$ . Now, I consider $g : X \rightarrow X$ be another etale map, and I look to $g^*(T) = T \times_X X$ , which is a $X$ -torsor (given by the second projection). First example : when we take $X=T=\mathbb{C}-\{0\}$ , $f : T \rightarrow X$ given by $z \mapsto z^n$ , and $g : X \rightarrow X$ given by $z \mapsto z^m$ , with $\gcd(n,m)=1$ , some straighforward computations give that the map $g^* : H^{1}_{et}(X, G) \mapsto H^{1}_{et}(X, G)$ is the multiplication by $m$ . My problem : In general, we have that $g^*(T)= T \times_X X$ is a $X$ -torsor for the second projection, with group action of $G$ given by the action of the first coordinate. Now, let $(U_i \rightarrow X)_i$ be a family which trivialize the $X$ -torsor $T$ . Then, denoting $U_{ij} := U_i \times_X U_j$ , for $s_i \in U_i$ , $s_j \in U_j$ , there exists a unique $g_{ij} \in G(U_{ij})$ such that $s_j = g_{ij} s_i$ , and this is how we identify the $X$ -torsor $T$ to $[(g_{ij})_{ij}] \in H^{1}_{et}(X, G)$ . Now, let $U_i' := U_i \times_{X, g} X$ . Let $s_i : U_i \rightarrow T$ a section ( $s_i \in T(U_i)$ ). Then, if $(x_1, y_1) \in U_i \times_{X, g} X$ , we have that $f(s_i(x_1))= (U_i \rightarrow X)(x_1) = g(y_1)$ , and then we can construct a section $s'_i : U_i' \rightarrow g^*(T)$ by $(x_1, y_1) \mapsto (s_i(x_1), y_1)$ . It gives us an application $\pi_i : T(U_i) \rightarrow g^*(T)(U'_i)$ . As the action of $G$ on $g^*(T)$ is given by the action of $G$ on the first coordinate, the previous application is compatible with the action of $G$ , where the map $\text{res}_i : G(U_i) \rightarrow G(U'_i)$ is given by the ""restriction"" map (the composition with the projection $U_i' = U_i \times_{X, g} X \rightarrow U_i$ ). But then, if $s_j = s_i g_{ij}$ , we have $\pi_j(s_j) = \pi_i(s_i) \text{res}_{ij}(g_{ij})$ . Then, the class of $g^*(T)$ in $H^{1}_{et}(X, G)$ is $[\text{res}_{ij}(g_{ij})_{ij}]$ which is the same class as $[(g_{ij})_{ij}]$ . The ""proof"" is clearly false, otherwise $g^*$ would be trivial for every $g$ ... In fact, I never use $g$ in the proof. Where is the mistake ? It's clearly wrong, but I don't see what is the point. Thank you !","['etale-cohomology', 'algebraic-geometry']"
4201842,Is taking the partial derivative on both sides of an equation a valid way of solving it?,"Given $h(t),$ and $p(r,t)$ are both functions independent of a variable $z.$ Is it valid to solve the following equation by differentiating both sides with respect to $z$ twice: $\left[z^2-zh(t)\right]\left[\frac{\partial^2 p(r,t)}{\partial r^2}+\frac{\partial p(r,t)}{\partial r} \frac{1}{r}\right] + \frac{dh(t)}{dt}  \frac{1}{h(t)} = 0$ (1) My thought process was that by doing this I could end up with the following PDE: $\frac{\partial^2 p(r,t)}{\partial r^2}+\frac{\partial p(r,t)}{\partial r} \frac{1}{r} = 0$ (2) Update/Edit: I tried the technique in order to obtain the above differential equation (2) for the p(r,t) function. Next, I was given the following boundary conditions: $p(r_1,t) = 0$ $p(r_2,t) = 0$ Solving the PDE I got: $p(r,t) = e^{c_1(t)}*ln(r) + c_2(t)$ Solving using the boundary conditions I ended up with $c_2(t) = 0$ $c_1(0) = ln(0)$ which is very wrong... Am I solving this PDE wrong or is my initial method of obtaining the PDE flawed? Edit #2: Regarding the origins of this PDE (note everything is in cylindrical coordinates) $p(r,t)$ is a function representing pressure, where pressure doesn't depend on $\theta$ or $z$ $h(t)$ represents the height of a certain surface The geometries are irrelevant and thus a diagram is not necessary. It is also given that the components of the velocity field are $v_r(r,z,t) = \frac{1}{2 \mu} * \frac{\partial{p(r,t)}}{\partial r} * [ z^2 - zh(t)]$ $v_\theta = 0$ $v_z(z,t) = \frac{\partial{h(t)}}{\partial t} * \frac{z}{h(t)}$ I got the PDE by applying the continuity equation (a consequence of conservation of mass): $\nabla \cdot \vec{v} $ For an incompressible fluid and cylindrical coordinate that becomes: $ \frac{1}{r} \frac{\partial{(r v_r)}}{\partial r} + \frac{1}{r} \frac{\partial{(v_\theta)}}{\partial \theta} + \frac{\partial{(v_z)}}{\partial z} = 0$","['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'homogeneous-equation']"
4201854,What is the meaning of the exterior derivative of a nonholonomic constraint?,"Suppose we have a nonholonomic mechanical system, say Lagrangian, for example the Chaplygin sleigh is a model of a knife in the plane. Its configuation space is $Q = S^1\times \mathbb{R}^2$ with local coordinates $q = (\theta,x,y)$ . The nonholonomic constraint is ""no admissible velocities are perpendicular to the blade"" which is specified by a one-form $$\omega_q = \sin\theta\,\mathrm dx + \cos\theta\,\mathrm dy.$$ When evaulating on a velocity $\dot{q}$ we specify the velocity constraint $$v = \omega_q\cdot \dot{q} = -\dot{x}\sin\theta + \dot{y}\cos\theta = 0.$$ What is the mathematical/physical meaning of the exterior derivative of the constraint one-form? Since $\omega_q$ is a one-form, in physics, we typically interpret it as a force. Mathematically, we integrate this form over a 'line' to get the work due to the force over the 'line'. The exterior derivative is a 2-form, (does this have any physical significance??) $$\mathrm d\omega = -\cos\theta\,\mathrm d\theta \wedge\,\mathrm dx - \sin\theta\,\mathrm d\theta \wedge\,\mathrm dy$$ If we evaluate this 2-form along a tangent (velocity) vector $\dot{q}$ , we find the 'force' $$\alpha_q = \mathrm d\omega(\dot{q},.) = \left(\dot{x}\cos\theta + \dot{y}\sin\theta \right)\,\mathrm d\theta - \cos\theta\dot{\theta}\,\mathrm dx - \sin\theta \dot{\theta}\,\mathrm dy.$$ Does this 'force', $\alpha_q \in T^*Q$ , have any direct/obvious physical/mathematical physics/differential geometric  interpretation?? Thanks.","['classical-mechanics', 'differential-forms', 'differential-geometry']"
4201916,The Hexagonal Property of Pascal's Triangle,"Any hexagon in Pascal's triangle, whose vertices are 6 binomial coefficients surrounding any entry, has the property that: the product of non-adjacent vertices is constant. the greatest common divisor of non-adjacent vertices is constant. Below is one such hexagon. As an example, here we have that $4 \cdot 10  \cdot 15 = 6 \cdot 20  \cdot 5$ , as well as $\gcd(4, 10, 15) = \gcd(6,20,5)$ . $$ 1 \\
1 \qquad  1\\
1\qquad 2\qquad 1\\
1\qquad3\qquad3\qquad1\\
1\qquad\mathbf{4}\qquad\mathbf{6}\qquad4\qquad1\\
1\qquad\mathbf{5}\qquad10\qquad\mathbf{10}\qquad5\qquad1
\\
1\qquad6\qquad\mathbf{15}\qquad\mathbf{20}\qquad15\qquad6\qquad1$$ There is a quick proof here (pdf). The original proof should be in V. E. Hoggatt, Jr., & W. Hansell. ""The Hidden Hexagon Squares."" The Fibonacci Quarterly 9(1971):120, 133. but I cannot access it. I am, however, intereseted in a purely combinatorial proof. I do not know how to approach this at all: I cannot see what the non-adjacent vertices represent and/or I do not know how to remodel their meaning. Can anyone help? EDIT: To specify my question more closely, what I am looking for is some natural bijection between the two sets of triads that create the hexagon. Thanks.","['alternative-proof', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4201973,Where does the $2$ in Ricci flow come from?,"I started learning about Ricci flow recently, which is always given as $$
\frac{\partial g}{\partial t}=-2\textrm{Ric}.
$$ It would seem more natural to me to define Ricci flow instead by the equation $$
\frac{\partial g}{\partial t}=-\textrm{Ric},
$$ which omits the $2$ . The only real difference in behavior between this flow and Ricci flow is that this one flows at half the rate. Wikipedia claims that the choice of $2$ in the equation is an arbitrary convention, but this is hard for me to stomach, since conventions in math are almost always motivated by something. Is there a good reason that Ricci flow is defined the way it is, or is the convention really arbitrary?","['convention', 'ricci-flow', 'riemannian-geometry', 'differential-geometry']"
4201994,Clarification on the definition of General Solution,"Given the differential equation $\frac{dy}{dx}=3{y^\frac{2}{3}}$ , the general solution is $\sqrt[3]{y}=x+C$ . But, there are two solutions curves that pass through (2,0) namely, $\sqrt[3]{y}=x-2$ and $y=0$ . Why do we call $\sqrt[3]{y}=x+C$ a general solution?  I thought ""general solution"" meant that it describes all solution curves, one for each value of $C$ .  But it clearly doesn't, as $y=0$ is a solution curve and it's not included. I believe I need help defining a ""general solution"". This seems to be glossed over in the textbook I am reading.","['initial-value-problems', 'ordinary-differential-equations', 'terminology']"
4202049,coordinates for vertices in a non-regular polygon,"I want to perform measurements around a tree. As the trees are not perfectly spherical I have the following problem. I use the circumference of the tree and divide it in 40 evenly long segments (sides (mm), in this case my measurement points). I have also information of the lengths (mm) of the diagonals of the opposite vertex or any other vertex (I measure with a tree calliper). What I now need are coordinates of individual vertices (in my case 40 vertices = measurement points). Can anyone provide a formula how I can calculate coordinates for my 40 vertices (measurement points) as I need them in another software to outline the shape of the tree.  Thanks in advance Matt",['geometry']
4202188,"Groups with prescribed $\text{C}(x)$, $\text{C}(y)$, $\text{C}(xy)$.","My question is inspired by this question , but I want to ask something much more specific. Let $X,Y,Z$ be finite groups with $1\ne x\in \text{Z}(X)$ , $1\ne y\in \text{Z}(Y)$ , and $1\ne z\in \text{Z}(Z)$ . Under what conditions does there exist a group (not necessarily finite) $G\geqslant X,Y,Z$ such that $xyz=1$ and $\text{C}_G(x)=X$ , $\text{C}_G(y)=Y$ , $\text{C}_G(z)=Z$ ? It seems to me that there may be tight restrictions if $(o(x),o(y),o(z))$ is one of $(2,2,k)$ , $(2,3,3)$ , $(2,3,4)$ , $(2,3,5)$ but  beyond that I have no intuition.",['group-theory']
4202201,Product of primes below some number $n$,"I was asked this question in an exam For an integer $n>3$ denote by $f(n)$ the product of all prime numbers
less than $n$ . So $f(6) = 30$ , $f(5) = 6$ . Which of the following are true? A. There are only finitely many $n$ such that $n|f(n)$ . B. There are only finitely many $n$ such that $n>f(n)$ . C. Given any $k,l \in \mathbb N$ , $f(n)=kn+l$ has infinitely many solutions. D. Let $S_k=\{l\in \mathbb N : f(l)=k\}$ . Let $a_k=\frac 1 {|S_k|}$ if $S_k\neq\phi$ , else $a_k=0$ . Then $\sum_{i=1}^\infty a_i$ does not converge to a rational number. Now, for A , I figured out an idea. Consider the set of the first $k$ primes $P_k=\{p_i : i\in \mathbb N_k \}$ where $\mathbb N_k=\{1,2,\dots ,k\}$ and let the $(k+1)$ -th prime be $p_{k+1}$ . Now consider the set of all possible products in $P_k$ given by $\prod P_k=\left\{\prod_{i=1}^m p_{\pi (i)} : p_j\in P_k, m\in \mathbb N_k, \pi \text{ is any permutation of }\mathbb N_k \right\}$ . Now, if there is at least one element $x\in\prod P_k$ which is in the interval $(p_k,p_{k+1})$ , then it is trivial that $x|f(x)$ . And, I guess, there will always be such an $x$ . However, a proof of this, or a better way will be appreciated. For B , it is clear that $f(n)$ increases much much faster than $n$ . So, it's no doubt that it's true. Again, I would like to have a rigorous proof of this. For C , if we put $k=l=1$ , we can intuitively see that $f(n)=n+1$ will not have infinitely many solutions (since $f(n)$ increases much much faster than $n+1$ ). Again, a proof of this will be appreciated. My main problem was with D . I can figure out that $|S_k|\neq \phi$ iff $k$ is of the form $\prod_{i=1}^m p_i$ where $p_1,p_2,\dots ,p_m$ are the first $m$ primes. So, $a_k$ will either be $0$ (when $|S_k|=\phi$ ), or $a_k$ will be equal to the number of integers between $(p_{m}+1)$ and $p_{m+1}$ . But, I can't see how to relate it with whether the sum will converge to a rational number or not. Please help me in these. Edit: The answers of this question (according to the answer key) are B and D , all of which are done either on the comments or in the answers!!! Thanks to you all :)","['number-theory', 'prime-gaps', 'elementary-number-theory', 'prime-numbers']"
4202315,Partitioning the interior of Bean curve,"The bean curve is defined as $\left(x^{2}+y^{2}\right)^{2}=x^{3}+y^{3}$ in cartesian coordinates, $r=\left(\cos\left(\theta\right)\right)^{3}+\left(\sin\left(\theta\right)\right)^{3}$ in polar coordinates and $$\gamma(t) = \left(\left(\left(\cos\left(t\right)\right)^{3}+\left(\sin\left(t\right)\right)^{3}\right)\cos\left(t\right),\left(\left(\cos\left(t\right)\right)^{3}+\left(\sin\left(t\right)\right)^{3}\right)\sin\left(t\right)\right)$$ as parametric equation for $0\le t \le \pi$ . My aim is to partition the interior in smaller bean curves(just like in the figure). For instance, in the case of unit circle centred at origin, every point in the interior can be written uniquely as $(s\cos(\theta),s\sin(\theta))$ where $s$ is the distance of the point from origin and $\theta$ denotes the angle from the $x$ -axis. Thus the interior in this case is the union of smaller circles centred at origin. Is it possible to do the same for the bean curve? A few possibilities that don't work are $s\gamma(t)$ and $s\gamma(t)+\frac{s(1-s)}{2}(1,1)$ , because for every $0\le s \le1$ , $(0,0) = s\times \gamma(3\pi/4)$ and the same problem with the second one. For the same point there are multiple bean curves.","['euclidean-geometry', 'geometry']"
4202363,Is there a name for functions with only one extremum?,"(Non-mathematician here) Is there a general term for a function that has only one extremum?  That is, for example, a function for which there are no local minimums that are not also the global minimum?  The function in question is not necessarily differentiable at the extremum (for example, $ y=\mathopen|x\mathclose| $ ).","['calculus', 'algebra-precalculus', 'terminology']"
4202372,Best constant in Weak-$L^p$-triangle inequality,"What is the best constant $C_p$ in the ""triangle inequality"" $$
\| f + g \|_{p,\infty} \le C_p ( \|f\|_{p,\infty} + \|g\|_{p,\infty})$$ for the weak $L^p$ spaces ? Here, I am mostly interested in the case $p \in [1,\infty)$ . Typical proofs show $C_p \le 2$ and I have an example proving $C_p \ge 2^{1/p}$ . Moreover, in the limit case $p = \infty$ we have $C_\infty = 1$ , which gives the clue that maybe $C_p = 2^{1/p}$ is correct.","['measure-theory', 'weak-lp-spaces', 'functional-analysis']"
4202374,"Does it necessarily mean $A'\cap B'=$infinite, if $A\cap B=$infinite?","This seems to be a silly question. Suppose $A,~B,~A',~B'$ are all infinite sets such that $A' \subset A$ and $B' \subset B$ . $(1)$ Does it necessarily mean $A\cap B=$ infinite, if $A'\cap B'=$ infinite? $(2)$ Does it necessarily mean $A'\cap B'=$ infinite, if $A\cap B=$ infinite? The answer of question $(1)$ is trivial because $A' \cap B' \subset A \cap B$ . What about the question $(2)$ ? It may happens that $A \setminus A'=$ infinite and $B\setminus B'=$ infinite such that $(A \setminus A') \cap (B \setminus B')=$ infinite while $A' \cap B'$ might have only few elements in common, but still we have $A \cap B=$ infinite. So the answer of question $(2)$ is NO. Am I correct ? Thanks for discussion.",['elementary-set-theory']
4202375,Roots with Same Size in a Family of Polynomials,"Let $k\geq 3$ be an integer and define the polynomial $f_k(x):=x^{k}-2x^{k-1}-x-1$ . By using elementary tools it is easy to prove that all roots of $f_k(x)$ are simple. However, I would need a little more, namely, that if $\alpha$ and $\beta$ are roots of $f_k(x)$ such that $|\alpha|=|\beta|$ , then $\alpha$ is either $\beta$ or $\overline{\beta}$ (its complex conjugate). I tried to use the standard approach by writing $\alpha=re^{i\theta}$ and $\beta=re^{i\gamma}$ , with $r>0$ and $\theta,\gamma\in (-\pi,\pi]$ . Now, it suffices to prove that $\theta=\pm \gamma$ . After many calculations I arrived in a trigonometric equality, however I had no idea how to deduce what I need from it and I was not able to think in another better approach for the original problem. The trigonometric equality is the following: $$
2r^{k-1}\cos(x/2)\cos(y/2)=r\cos((k-1)x/2)\cos((k-1)y/2)-2\cos((k-2)x/2)\cos((k-2)y/2),
$$ where $x:=\theta+\gamma$ and $y:=\theta-\gamma$ . Any suggestion? Thanks in advance.","['calculus', 'polynomials', 'trigonometry', 'real-analysis']"
4202392,Does this set have area in $\mathbb{R}^2$?,"Let $\mathcal{A} \subset \mathbb{R}^2$ the following set: $\mathcal{A} = A_1 \cup A_2 \cup A_3 \cup A_4$ , where: $$A_1 = \{(q_1, 0) \in \mathbb{R}^2 : q_1 \in [0,1] \cap \mathbb{Q}\}$$ $$A_2 = \{(1, q_2) \in \mathbb{R}^2 : q_2 \in [0,1] \cap \mathbb{Q}\}$$ $$A_3 = \{(q_1, 1) \in \mathbb{R}^2 : q_1 \in [0,1] \cap \mathbb{Q}\}$$ $$A_4 = \{(0, q_2) \in \mathbb{R}^2 : q_2 \in [0,1] \cap \mathbb{Q}\}$$ That is, $\mathcal{A}$ are the points over the edges of the unitary square $[0,1] \times [0,1]$ having only rational coordinates. Now I know this set has measure zero because $\mathbb{Q} \cap [0,1]$ is a countable set and therefore $A_1, A_2, A_3$ and $A_4$ are countable, and $\mathcal{A}$ is therefore a finite union of countable sets, which makes it countable too. However, I'd like to see whether or not the set has area in the sense that $ \int_\mathcal{A} 1_\mathcal{A} $ exists, where $1_\mathcal{A} (x,y) = 1$ if $(x,y) \in
\mathcal{A}$ and $1_\mathcal{A} (x,y) = 0$ otherwise. I built this set based on the fact that $\int_0^1 1_\mathbb{Q} (x)\ dx$ doesn't exist. I suspect $\mathcal{A}$ has no area, but I'm not quite sure. Being the devil's advocate, we can cover this set by a finite collection of degenerate rectangles such as $\{0\} \times [0,1]$ that have volume (area) zero, but as far as I know the rectangle covering definition of volume (area) doesn't take into account degenerate rectangles. I'm trying to see if this set is an example of measure zero sets which have no volume (in this case, area). Thank you for reading me! EDIT: All integrals here are Riemann integrals.","['measure-theory', 'area', 'volume', 'real-analysis', 'riemann-integration']"
4202406,Is a normed vector space with an isometry group acting transitively on its unit ball an inner product space?,"In section 2.3 of The Octonions , Baez discusses a proof for the Hurwitz theorem on normed division algebras via Clifford algebras that goes through the following argument. Supposing that $\mathbb K$ is a normed division algebra, and $a \in \mathbb K$ with $\Vert a\Vert = 1$ , the operators $$ L_a : \mathbb K \rightarrow \mathbb K\\\qquad\, x\mapsto ax$$ are norm-preserving and map any point of the unit sphere to any other point (because $\mathbb K$ is assumed to be a division algebra). Baez then writes that “the only way the unit sphere in $\mathbb K$ can have this much symmetry is if the norm on $\mathbb K$ comes from an inner product”. Is it true that a normed vector space must in fact be an inner product space if there is a group of isometries acting transitively on its unit sphere? If so, how so? If not, did Baez mean something else?","['geometry', 'normed-spaces', 'functional-analysis', 'clifford-algebras']"
4202407,"Rudin Real and Complex analysis - Step II theorem 2.14, Riesz","Trying to understand this very long theorem of which I think good understanding is very educational. I am going through all the steps specifically now there's a subtlety in the conclusion of step II which I cannot get. I am sure is a silly thing. I'll write the proof for reference Step II : If $K$ is compact, then $K \in \mathcal{M}_F$ and $$
\mu(K) = \inf\left\{\Lambda f : K  \prec f \right\} \;\;\;\; (7)
$$ This implies assertion (b) of the theorem. Proof: If $K \prec f$ and $0 < \alpha < 1$ , let $V_\alpha = \left\{x : f(x) > \alpha \right\}$ . Then $K \subset V_\alpha$ and $\alpha g \leq f$ whenever $g \prec V_\alpha$ . Hence $$
\mu(K) \leq \mu(V_\alpha) = \sup \left\{ \Lambda g : g \prec V_\alpha \right\} \leq \alpha^{-1} \Lambda f
$$ Let $\alpha \to 1$ to conclude that $$
\mu(K) \leq \Lambda f \;\;\;\; (8)
$$ Thus $\mu(K) < \infty$ . Since $K$ evidently satisfies (3), $K \in \mathcal{M}_F$ .
If $\epsilon > 0$ , there's $V, K \subset V$ with $\mu(V) < \mu(K) + \epsilon$ . By Urysohn's lemma $K \prec f \prec V$ for some $f$ . Thus $$
\Lambda f \leq \mu(V) < \mu(K) + \epsilon
$$ which, combined with (8), gives (7) Here's the question or maybe clarification... combining (7) with (8) provides to me that $$
\mu(K) \leq \Lambda f \leq \mu(K) + \epsilon \;\;\;\; (9)
$$ and bedause $\epsilon > 0$ is arbitrary we end up with $$
\mu(K) = \Lambda f
$$ However in (8) the relationship $\mu(K) \leq \Lambda f$ holds for any $f$ with $K \prec f$ and specifically we have $$
\mu(K) \leq \inf \left\{ \Lambda f : K \prec f \right\}
$$ On the other hand for $9$ the $f$ used depends on the open $V$ chosen therefore to me the combination of (8) and (9) to get (7) should be done as $$
\mu(K) \leq \inf \left\{\Lambda f : K \prec f \right\} \leq \Lambda f \leq \mu(K) + \epsilon \Rightarrow
\mu(K) \leq \inf \left\{\Lambda f : K \prec f \right\} \leq \mu(K) + \epsilon 
$$ and as $\epsilon \to 0$ we have $$
\mu(K) = \inf \left\{\Lambda f : K \prec f \right\}
$$ Is this the right conclusion?","['proof-explanation', 'measure-theory', 'riesz-representation-theorem', 'real-analysis']"
4202440,The Multiplication Operator $M_f: L^2(\mu) \to L^2(\mu)$ such that $M_f g = fg$ (Rudin),"This post is related but does not answer my question. Question 2 is partly unaddressed. Perhaps we can split into two cases, (i) where $\mu(\{f = 0\}) = 0$ and (ii) $\mu(\{f = 0\}) > 0$ ? If $\mu(\{f = 0\}) = 0$ , then $g = h/f$ a.e. Problem $5.17$ (paraphrased), Rudin's Real and Complex Analysis. If $\mu$ is a positive measure, each $f\in L^\infty(\mu)$ defines a multiplication operator $M_f$ on $L^2(\mu)$ into $L^2(\mu)$ , such that $M_f(g) = fg$ . Prove that $\|M_f\| \le \|f\|_\infty$ . I have shown that $\|M_f\| \le \|f\|_\infty$ . Question 1: For which measures $\mu$ is it true that $\|M_f\| = \|f\|_\infty$ for all $f\in L^\infty(\mu)$ ? Question 2: For which $f\in L^\infty(\mu)$ does $M_f$ map $L^2(\mu)$ onto $L^2(\mu)$ ? My work: We need to find measures $\mu$ for which the reverse inequality, i.e. $\|M_f\| \ge \|f\|_\infty$ also holds, for all $f\in L^\infty(\mu)$ . More explicitly, we need $$\sup\{\|M_f g\|_2: g\in L^2(\mu), \|g\|_2 \le 1\} = \|f\|_\infty \quad (f\in L^\infty(\mu))$$ where $$\|f\|_\infty = \inf\{a\in\mathbb R: \mu(\{|f| > a\}) = 0\}$$ Consider $h\in L^2(\mu)$ . We want $g\in L^2(\mu)$ such that $M_f g = fg = h$ . If $1/f \in L^\infty(\mu)$ , the map is surjective. Is the converse true? If not, could one help me get an iff condition without assuming anything on $\mu$ besides that it is a positive measure? I'd appreciate any help, thank you! Update: The proof below by Jose Avilez, and Jean L., Theorem $1.5$ tells us that $\|M_f\| = \|f\|_\infty$ for all $f\in L^\infty(\mu)$ iff $\mu$ is semi-finite.","['measure-theory', 'functional-analysis', 'real-analysis']"
4202466,Forward characterization of measurable functions?,"In topology, the standard definition of continuity works in the ""backward"" direction, since it puts a condition on the pre-images of a function rather than images: $f$ is continuous if the pre-image under $f$ of every open set is open. However, there is an alternative definition of continuity that works in the ""forward"" direction: $f$ is continuous if $p\in\overline A\implies f(p)\in\overline{f(A)}$ . The standard definition of measurable functions is analogous to the first definition of continuity (just replace ""open"" with ""measurable""). Can measurable functions be defined without reference to pre-images, by analogy with continuity? EDIT: The original version of this question did not specify that I was looking for a definition without any use of pre-images. The proposed duplicate has an answer that considers the ""forward"" direction, but it still uses pre-images in part of its formulation. First I made a new post with a refined question, but was then kindly instructed to re-open this one.","['measure-theory', 'continuity', 'definition', 'measurable-functions', 'general-topology']"
4202480,"$\int_0^1\int_x^1f(t) \,dt \,dx=\int_0^1t f(t) \,dt$","Lets suppose that f is continuous in $[0,1]$ .
I want to prove that $$\int_0^1\int_x^1f(t) \,dt \,dx=\int_0^1t f(t) \,dt$$ for the left part I set the following function: $$F(y)=\int_0^y f(t)\,dt$$ so from the left part I have that $$\int_0^1\int_x^1f(t)\,dt\,dx$$ $$=\int_0^1(F(1)-F(x))\,dx$$ $$=F(1)-\int_0^1F(x)\,dx$$ $$=F(1)-F(0)-\int_0^1F(x)\,dx$$ Then, I tried to solve the right part by integrating by parts but I got confussed because of the limits of integration. $u=t$ , $du=dt$ , $v=\int f(t)\,dt$ , $dv=f(t)$ . Here is were I´am stuck, can you help me?","['integration', 'contest-math', 'definite-integrals', 'calculus', 'indefinite-integrals']"
4202490,Algebraic function that acts like $\sin(1/x)$ near zero. (or non-trig function) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Trying to construct an example for a Business Calculus class (meaning trig functions are not necessary for the curriculum). However, I want to touch on the limit problem involved with the $\sin(1/x)$ function. I am sure there is a simple function, or there isn't... But would love some insight. I also understand that the functions that satisfy this condition are maybe way outside the scope of the course. I'm just looking for different ""flavors"" of showing limits that don't exist besides just showing the limit from the left and the limit from the right does not exists.","['limits', 'calculus', 'education']"
4202493,How can I show the trefoil group is torsion-free,"I would like to show the trefoil group is torsion-free. The trefoil group has the presentation \begin{equation}
G = \langle a, b \mid a^3 = b^2\rangle.
\end{equation} I tried to map this to a simpler torsion-free group, for instance, if $h: G\to \mathbb{Z}$ by \begin{equation}
a\to 2, b\to 3,
\end{equation} then the torsion of $G$ must be in the kernel of $h$ . However, the kernel is still pretty complicated. Any ideas will be greatly appreciated!","['group-presentation', 'group-theory', 'low-dimensional-topology', 'algebraic-topology']"
4202497,What went wrong in my evaluation of this limit? $\lim_{x\to1}\frac{1-\sqrt x}{(\cos^{-1} x)^2}$,"My process: \begin{align}
\lim_{x\to 1}\frac{1-\sqrt x}{(\cos^{-1} x)^2} &=\lim_{x\to 1}\frac{1- x^2}{(\cos^{-1} x)^2(1+\sqrt x)}\\
&=\lim_{x\to1}\frac{(\sin(\cos^{-1}x))^2}{(\cos^{-1} x)^2(1+\sqrt x)}\\
&=\lim_{x\to1}\frac{1}{(1+\sqrt x)}=\frac{1}{2}.
\end{align} The answer is infact $\frac{1}{4}$ what went wrong ?","['limits', 'calculus']"
4202509,Recursive formula for a tree problem,"Question: A binary is defined as a tree in which 1 vertex is the root, and any other vertex has 2 or 0 children. A vertex with 0 children is called a node, and a vertex with 2 children is called an inner vertex. The order between the children is important. A binary tree can be defined with recursion:
a binary tree is one out of 2 options : A single vertex. A vertex in which two sub-trees, that were build before, are connected to him. Now, let $D_n$ be the number of valid binary trees, >with $n$ inner vertices. For this question, a binary tree is called $valid$ if for each inner vertex $v$ , the left sub-tree connected to $v$ in the left side, contains an even amount (0,2,4,...) of inner vertices. Find the recursive formula with starting conditions for $D_n$ such that the formula can use all values before. In addition, calculate $D_6$ . $Solution.$ in order to build a valid binary tree of size $n$ we can take all options for a valid binary tree of size $n-1$ and take all options for a valid binary tree of size $n-2$ . Therefore, we get: $$D_n=D_{n-1}+D_{n-2}$$ For the starting conditions: we consider $D_1$ which is 1 because we have only the root, which has an even amount of inner vertices (0). For $D_2$ we have the next tree: Therefore, $D_2=1$ . Calculation for $D_6$ : $$D_6=D_5+D_4=D_4+D_3+D_3+D_2=D_3+D_2+(D_2+D_1)\cdot 2 + D_2=D_2+D_1+D_2+2D_2+2D_1+D_2=5D_2 +3D_1=5+3=8 $$ Now, I am not sure that this is correct, thus, I will be glad for some help. I think that might be better to convert this problem to another problem, but I think that this is good too. Thanks!","['trees', 'recurrence-relations', 'discrete-mathematics', 'recursion']"
4202515,"Draw 7 lines on the plane in an arbitrary manner. Prove that for any such configuration, 2 of the those 7 lines form an angle less than 26◦","I have been working on this question for a while now and I think that this is one of the many applications of The Pigeonhole Principle. However, I don't seem to draw a conclusion.
So, I figured that the lines must intersect somehow to form a Heptagon and the sum of the exterior angles must be 360 degrees which would be distributed among the pairs of lines that will be formed. I also noticed that $\frac{180}{7}=25.714$ approximately which incentivized me to carry out this procedure, however, I don't see a continuation. Thanks! Edit: It seems that the case of 2 lines being parallel is breaking the statement of the title, so I think it is safe to assume that we are talking about taking 7 lines arbitrarily where no 2 lines are parallel to each other.","['pigeonhole-principle', 'combinatorial-geometry', 'combinatorics', 'extremal-combinatorics']"
4202525,"If $T_n\to T$ in the weak operator topology, does $p(T_n)\to p(T)$ in the weak operator topology for any polynomial $p$?","Let $(T_n)_{n=1}^\infty$ be a sequence of bounded operators mapping a Banach space $X$ to itself, and suppose that $T_n\to T$ in the weak operator topology; that is, for every $y^*\in X$ * and any $x\in X$ , we have $y^*(T_nx)\to y^*(Tx)$ . Does it follow that for any polynomial $p:\mathbb{C}\to\mathbb{C}$ , we have $p(T_n)\to p(T)$ in the weak operator topology as well? I'm pretty sure that this is true if we replace weak convergence by strong convergence. In this case we know that the sequence $(\Vert T_n\Vert)_{n=1}^\infty$ is bounded by the uniform boundedness principle, and that multiplication is jointly continuous on bounded sets in the strong operator topology, so that $T_n^k\to T^k$ for any $k\in\mathbb{N}$ . However we don't generally have joint continuity in the weak operator topology (as this example on Wikipedia shows). Can we still show that $p(T_n)\to p(T)$ if we only assume weak convergence $T_n\to T$ ? If so, could you please provide a proof or a reference? Edit: Even if this is false in general, I'd be interested to know if the convergence holds under special conditions - for example, all $T_n$ and $T$ commuting, the Banach space $X$ being reflexive, etc., as these conditions hold in the case I am currently interested in.","['operator-theory', 'functional-analysis', 'weak-convergence', 'weak-topology']"
4202528,Embeddings of several Lie groups and their geometry embedding,"The question concerns some problems about the Lie groups and representations. And the geometry embedding of the several Lie groups. We start from a fixed common special unitary group SU(2), with the given SU(2) representations, then try to generate two Lie groups SU(4) and SU(4)' out of the fixed SU(2). The question concerns: what is the minimal Lie group $$G =?$$ that contains both the two generated SU(4) and SU(4)'? (Here SU(4) and SU(4)' are isomorphic as the Lie group, but the ways that it acts on the SU(2) representations are different.) See the figure for an imaginative illustration below: Consider the SU(2) representations: the 1-dimensional ${\bf 1}$ and the 2-dimensional ${\bf 2}$ . We take four of ${\bf 1}$ and three of ${\bf 2}$ of SU(2) in total. So they combine to be: $$
{\bf 1} \oplus {\bf 1} \oplus {\bf 1} \oplus {\bf 1} \oplus {\bf 2} \oplus {\bf 2}\oplus {\bf 2} \text{ of SU(2)}.
$$ More precisely, let us distinguish them by some sublabel (subindices): $$
{\bf 1}_a \oplus {\bf 1}_b \oplus {\bf 1}_{c'} \oplus {\bf 1}_{d'} \oplus {\bf 2}_{A'} \oplus {\bf 2}_{C} \oplus {\bf 2}_{D} \text{ of SU(2)}.
$$ 1.First SU(4) Now we generate the first SU(4) by defining a 4-dimensional ${\bf 4}$ and a 6-dimensional ${\bf 6}$ , as $$
{\bf 4} \text{ of SU(4)}\mapsto {\bf 1}_a \oplus {\bf 1}_b \oplus {\bf 2}_{A'}
\text{ of SU(2)}
$$ and $$
{\bf 6} \text{ of SU(4)}\mapsto {\bf 1}_{c'} \oplus {\bf 1}_{d'} \oplus {\bf 2}_{C} \oplus {\bf 2}_{D} \text{ of SU(2)}
$$ So using the red and orange colors to break down the above information, we have $$
\color{red}{{\bf 4}} \oplus
\color{orange}{\bf 6} \text{ of SU(4)}\mapsto \color{red}{({\bf 1}_a \oplus {\bf 1}_b \oplus {\bf 2}_{A'})} \oplus
\color{orange}{({\bf 1}_{c'} \oplus {\bf 1}_{d'} \oplus {\bf 2}_{C} \oplus {\bf 2}_{D})}
\text{ of SU(2)}
$$ Or the original ${\bf 1}_a \oplus {\bf 1}_b \oplus {\bf 1}_{c'} \oplus {\bf 1}_{d'} \oplus {\bf 2}_{A'} \oplus {\bf 2}_{C} \oplus {\bf 2}_{D}$ becomes $$
\color{red}{{\bf 4}} \oplus
\color{orange}{\bf 6} \text{ of SU(4)}\mapsto
\color{red}{{\bf 1}_a} \oplus \color{red}{{\bf 1}_b} \oplus \color{orange}{{\bf 1}_{c'}} \oplus\color{orange}{ {\bf 1}_{d'}} \oplus \color{red}{{\bf 2}_{A'}} \oplus \color{orange}{{\bf 2}_{C}} \oplus \color{orange}{{\bf 2}_{D}}\text{ of SU(2)}. \tag{1}
$$ We can determine how the SU(4) is generated out of the chosen representations of SU(2). 2.Second SU(4)' Now we generate the second SU(4)' by defining another 4-dimensional ${\bf 4}'$ and another 6-dimensional ${\bf 6}'$ , as $$
{\bf 4}' \text{ of SU(4)'}\mapsto {\bf 1}_{c'} \oplus {\bf 1}_{d'}  \oplus {\bf 2}_{A'}
\text{ of SU(2)}
$$ and $$
{\bf 6}' \text{ of SU(4)'}\mapsto {\bf 1}_{a} \oplus {\bf 1}_{b} \oplus {\bf 2}_{C} \oplus {\bf 2}_{D} \text{ of SU(2)}
$$ So using the blue and purple color to break down the above information, we have $$
\color{blue}{{\bf 4}} \oplus
\color{purple}{\bf 6} \text{ of SU(4)'}\mapsto \color{blue}{( {\bf 1}_{c'} \oplus {\bf 1}_{d'}  \oplus {\bf 2}_{A'})} \oplus
\color{purple}{({\bf 1}_{a} \oplus {\bf 1}_{b} \oplus {\bf 2}_{C} \oplus {\bf 2}_{D})}
\text{ of SU(2)}
$$ Or the original ${\bf 1}_a \oplus {\bf 1}_b \oplus {\bf 1}_{c'} \oplus {\bf 1}_{d'} \oplus {\bf 2}_{A'} \oplus {\bf 2}_{C} \oplus {\bf 2}_{D}$ becomes $$
\color{blue}{{\bf 4}} \oplus
\color{purple}{\bf 6} \text{ of SU(4)'}\mapsto
\color{purple}{{\bf 1}_a} \oplus \color{purple}{{\bf 1}_b} \oplus \color{blue}{{\bf 1}_{c'}} \oplus\color{blue}{ {\bf 1}_{d'}} \oplus \color{blue}{{\bf 2}_{A'}} \oplus \color{purple}{{\bf 2}_{C}} \oplus \color{purple}{{\bf 2}_{D}} \text{ of SU(2)}. \tag{2}
$$ We can determine how the SU(4)' is generated out of the chosen representations of SU(2). Question Again, we should contrast: $$
\color{red}{{\bf 4}} \oplus
\color{orange}{\bf 6} \text{ of SU(4)}\mapsto
\color{red}{{\bf 1}_a} \oplus \color{red}{{\bf 1}_b} \oplus \color{orange}{{\bf 1}_{c'}} \oplus\color{orange}{ {\bf 1}_{d'}} \oplus \color{red}{{\bf 2}_{A'}} \oplus \color{orange}{{\bf 2}_{C}} \oplus \color{orange}{{\bf 2}_{D}} \text{ of SU(2)}. \tag{1}
$$ $$
\color{blue}{{\bf 4}} \oplus
\color{purple}{\bf 6} \text{ of SU(4)'}\mapsto
\color{purple}{{\bf 1}_a} \oplus \color{purple}{{\bf 1}_b} \oplus \color{blue}{{\bf 1}_{c'}} \oplus\color{blue}{ {\bf 1}_{d'}} \oplus \color{blue}{{\bf 2}_{A'}} \oplus \color{purple}{{\bf 2}_{C}} \oplus \color{purple}{{\bf 2}_{D}} \text{ of SU(2)}. \tag{2}
$$ We start from a fixed common special unitary group SU(2), given the SU(2) representations, then try to generate two Lie groups SU(4) and SU(4)' out of the fixed SU(2). The two SU(4) and SU(4)' overlap with the same Lie group SU(2), in the sense of geometry embedding. The question concerns: what is the minimal Lie group, called $G$ , that contains both the two generated SU(4) and SU(4)'? (Is that, for example, SU(6) or SU(8)?)
Again, the question: what is the minimal Lie group $$G =?$$ See the figure for an imaginative illustration above. For your answer, give a precise $G$ , and what is the $G$ as a minimal manifold (minimal surface) satisfying all conditions?","['lie-algebras', 'representation-theory', 'manifolds', 'lie-groups', 'differential-geometry']"
4202619,Is every differential 1-form a linear combination of closed forms?,"Let $M$ be a smooth manifold. We know that the $C^\infty (M)$ -module $\Omega^1 (M)$ is finitely generated, i.e. there exists $1$ -forms $\{\alpha_1, \ldots, \alpha_k \}$ such that for any $1$ -form $\omega$ , we can write $\omega = \sum_{i=1}^k f_i \alpha_i$ for some $f_i \in C^\infty (M)$ . I'm wondering if the $\alpha_i$ can be chosen to be closed, or, furthermore, exact. I'm guessing there must exist a counterexample, as I haven't seen this result in any of the standard textbooks or online sets of notes, and it might make computations a little too easy. I've been toying around with this idea for a while but haven't gotten any leads in either direction, except that this is trivially true in $\mathbb{R}^n$ . Have any of you seen this result or know a counterexample?","['differential-forms', 'differential-geometry']"
4202649,Proving a derivative exists given the limit of f',"I have almost finished a problem from Baby Rudin, but I can't quite figure out the last step. The problem is as follows (5.9): Let $f$ be: a continuous real function on $\mathbb{R}^{1}$ , of which it is known that $f'(x)$ exists for all $x\neq0$ , and $f'(x) \rightarrow 3$ as $x \rightarrow 0$ . Does it follow that $f'(0)$ exists? Here's what I have so far: By (1), there exists a $\delta_{1}$ with $|x|<\delta_{1} \rightarrow |f(x)-f(0)|<\frac{\epsilon}{3}$ . By (2), there exists a $\delta_{2}$ with $|x-t|<\delta_{2}, x\neq0 \rightarrow |\frac{f(t)-f(x)}{t-x}-f'(x)|<\frac{\epsilon}{3}$ . By (3), there exists a $\delta_{3}$ with $x\neq0, |x|<\delta_{3} \rightarrow |f'(x)-3|<\frac{\epsilon}{3}$ . So when $|x|<\min(\delta_{1},\delta_{3})$ , $|x-t|<\min(\delta_{2},1)$ : $|\frac{f(t)-f(x)}{t-x}-f'(x)|+|f'(x)-3|+ |f(x)-f(0)| < \epsilon$ $|\frac{f(t)-f(x)}{t-x}-f'(x)+f'(x)-3+f(x)-f(0)| < \epsilon$ $|\frac{f(t)-f(x)}{t-x}-f'(x)+f'(x)-3+\frac{f(x)-f(0)}{t-x}| < \epsilon$ $|\frac{f(t)-f(0)}{t-x}-3|<\epsilon$ Which is very close to $|\frac{f(t)-f(0)}{t}-3|<\epsilon$ which would yield a solution to the problem. But I can't figure out what condition to put on x or t-x to get from $|\frac{f(t)-f(0)}{t-x}-3|<\epsilon$ to $|\frac{f(t)-f(0)}{t}-3|<\epsilon$ . Any help would be very, very appreciated.","['limits', 'calculus', 'analysis', 'real-analysis']"
4202668,How to prove the uniqueness of the smallest ball containing finite points?,"Problem. Let $A$ be a set of finite points in Euclidean space $\mathbb R^n$ . Let $B$ be the ball containing $A$ with smallest radius. Prove the uniqueness of $B$ . The natural idea is two use contradiction argument, i.e. assume that there exist two smallest balls with same radius and distinct centers. But how do I deduce a contradiction from here? I spent the whole day to think about this but no progress. Thank you for any idea! Comment. I found a paper of Mordukhovich et. al. , but they used variational
analysis in their arguments, which is too strong for such a ""simple"" problem, I think.","['euclidean-geometry', 'spheres', 'geometry', 'linear-algebra', 'optimization']"
4202676,Prove the following integral is constant: $\int_{-1}^{+1} \frac{ \ln |a-x|^2 }{\sqrt{1 - x^2}} dx$,"There is a famous improper integral with exact solution given by $$\int_{0}^{+1} \frac{ \ln x }{\sqrt{1 - x^2}} dx = -\frac{\pi}{2} \ln 2$$ From this, it is pretty easy to generalize to the following variation: $$\int_{-1}^{+1} \frac{ \ln |x|^2 }{\sqrt{1 - x^2}} dx = -2 \pi \ln 2$$ From what I can tell, we can even extend this further to show that: $$\int_{-1}^{+1} \frac{ \ln |a-x|^2 }{\sqrt{1 - x^2}} dx = -2 \pi \ln 2$$ for any $|a| \leq 1$ . Unfortunately, I have only been able to verify this numerically (i.e., testing it for various values in Wolfram Alpha). However, I cannot think of a straightforward way to prove it generally. The identity has an interesting physics application I am working on. It shows that a surface charge density with the form $$\sigma(x) = \frac{1}{\sqrt{1 - x^2}}$$ will also result in a constant potential along a thin strip from [-1,1]; an interesting result for transmission line models and a nice validation for simulation methods. I would love to see a straightforward proof of this. I suspect it has been done, as I have seen it mentioned in the literature. I just can't find a reference to it, and I do not see any easy way to replicate it myself. Thanks in advance!","['integration', 'calculus']"
4202684,"Value of $I=\inf_{f\in S} \int_0^1 f(x)^2 \, \text{d}x$","I was asked the following question Consider the set $$S=\left\{f:[0,1]\to \mathbb R : f \text{ is continuous}, \int_0^1 f(x)\,\text{d}x=5, \int_0^1 xf(x) \, \text{d}x=3\right\}$$ Let $$I=\inf_{f\in S} \int_0^1 f(x)^2 \,\text{d}x$$ Find the value of $I$ . I can understand that we need an inequality (or an equality) involving the terms $\int_0^1 f(x) \, \text{d}x=5$ , $\int_0^1 xf(x)\,\text{d}x=3$ and $\int_0^1 f(x)^2 \,\text{d}x$ . But I don't have any idea of where to start or how to proceed. I guess, first we need to find a relation between $\left(\int_0^1 f(x) \, \text{d}x\right)^2$ and $\int_0^1 f(x)^2 \,\text{d}x$ , but still, it's not clear to me how to proceed. Any help would be appreciated.","['integration', 'calculus', 'supremum-and-infimum']"
4202686,Sequence of polynomials $g_n$ converging pontwise with $g(\mathbb{C})=\mathbb{Z}$.,"Prove that there exists a sequence of polynomials $g_n(z)$ that converges for all $z\in \mathbb{C}$ to a limit function $g(z)$ with $g(\mathbb{C})=\mathbb{Z}$ . This question was on my complex analysis prelim last August. I have given it a go at various points throughout the time since, but I seriously have no idea where I could concretely start. Clearly the limit function can not be analytic, because this violates the open mapping theorem. Any hints would be excellent!",['complex-analysis']
4202728,"Show that if $G$ is an equivalence relation in $A$, then $G \circ G=G$","This is my attempt of proving this proposition, but it is probably incorrect. $G$ is an equivalence relation if it is: Reflexive $$(x,x)\in G$$ Symmetric $$(x,y)\in G \Rightarrow (y,x)\in G$$ Transitive $$(x,y)\in G \wedge (y,z)\in G \Rightarrow(x,z)\in G$$ In order to prove the proposition we should prove that $G \subseteq G\circ G$ and $G\circ G \subseteq G$ . The second on seems easy enough to prove: $$(x,y)\in G\circ G \Rightarrow \exists z\in A \ni (x,z)\in G \wedge(z,y)\in G$$ Since $G$ is transitive, $$(x,y)\in G\circ G \Rightarrow (x,y)\in G$$ Therefore, $$G\circ G\subseteq G$$ For the converse, $$(x,y)\in G \Rightarrow (x,x)\in G \wedge (x,y)\in G$$ Because $G$ is reflexive. So, $$(x,y)\in G \Rightarrow \exists z=x\in G \ni (x,z)\in G \wedge (z,y)\in G \Rightarrow (x,y)\in G\circ G$$ Therefore, $$G\subseteq G\circ G$$ QED. However, I haven't utilised the Symmetry of $G$ , so I suspect that the proof is not correct. Could anyone fix it?","['elementary-set-theory', 'equivalence-relations', 'relations']"
4202743,Does it make sense to try to extend the concept of a raised set to a real number?,"The definition of $a^b$ when $a$ and $b$ are natural numbers involves repeated multiplication. This definition can be extended to arbitrary exponents, motivated by the desire to preserve the identity $a^x\times a^y=a^{x+y}$ . Is it possible to extend the definition of the cartesian product in an analogous manner? It is clear what $\mathbb{R}^2$ or $\mathbb{R}^3$ means, but it is not obvious how $\mathbb R^{t}$ should be defined for arbitrary real values of $t$ .","['elementary-set-theory', 'exponentiation', 'definition']"
4202833,Product rule for independent functions,Suppose we have two independent functions $f(u)$ and $g(v)$ then (i) :- $$\frac {d \;(f(u)\cdot g(v))}{dx} = \frac {d (f(u)\cdot g(v))}{du}\cdot\frac{du}{dx} $$ Since $g(v)$ doesn't change with changing $u$ $$\frac {d (g(v))}{du} = 0$$ So the first equation becomes (ii) :- $$\frac {d \;(f(u)\cdot g(v))}{dx} = g(v) \cdot f'(u) \cdot\frac{du}{dx} $$ Similarly we can get this equation (iii) :- $$\frac {d \;(f(u)\cdot g(v))}{dx} = f(u)\cdot g'(v)\cdot \frac{dv}{dx} $$ Now if I add both the equations I will get $$ \frac {d \;(f(u)\cdot g(v))}{dx} = \left( \frac{1}{2} \right) \left(g(v)\cdot f'(u)\cdot\frac{du}{dx} + f(u)\cdot g'(v)\cdot\frac{dv}{dx}\right)$$ But this can't be right since it is half of what we get from the product rule. So where am I wrong ?,"['calculus', 'derivatives']"
4202836,How to represent a matrix in index notation when it's a combination of more than two multiplications?,Let's say there is an arbitrary matrix $A$ . $A$ is $a_{ij}$ in index notation. $A^2$ can be written as $\sum_{ij}a_{ij}a_{ji}$ . But I have no idea how to represent $A^3$ or $A^4$ . I tried to find a way by computing every element of $A^3$ and $A^4$ but I didn't catch any rule to condense each element in index notation in index notation. Can you give me any insight?,"['matrices', 'notation']"
4202941,Verify nth derivative satisfies differential equation,"I've been working my way through  an old series of maths books (An Analytical Calculus by Maxwell) and finally got stuck on a question midway through book 2 (of 4). If anyone could help that would be great (as there are quite a few like this following). The question is in two parts and I can get the first part. I figure that the first part is to be used in the solution of the second part (but could be wrong). We haven't covered ODEs yet, so any methods using theorems from ODEs shouldn't be used. a) Prove that $$
(x+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}~\left\{(x+1)^n(x-1)^{n+1}\right\}
=(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n\left\{(x+1)^{n+1}(x-1)^n\right\} 
$$ --> I can do this part. b) Prove also that the function $$
\left(\frac{\mathrm d}{\mathrm d x}\right)^n~{(x+1)^{n+1}(x-1)^n}
$$ Satisfies the equation: $$
(1-x^2)\left(\frac{\mathrm d}{\mathrm d x}\right)^2y-(1+x)\left(\frac{\mathrm d y}{\mathrm d x}\right)+(n+1)^2y=0
$$ --> I can't do this part. Worst case I thought I could do it via a recurrence relation between terms in a power series in the DE, and then check the function satisfies it, but it gets messy fast. I can't find any similar solved problems anywhere! The first part is solved via Leibniz: $$
(x+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}{(x+1)^n(x-1)^{n+1}}=\left(\frac{\mathrm d}{\mathrm d x}\right)^{n+1}{(x+1)^{n+1}(x-1)^{n+1}}-(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n{(x+1)^n(x-1)^{n+1}}
=\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^{n+1}(x-1)^n}+\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^n(x-1)^{n+1}}-(n+1)\left(\frac{\mathrm d}{\mathrm d x}\right)^n{(x+1)^n(x-1)^{n+1}}=\left(\frac{\mathrm d}{\mathrm d x}\right)^n(n+1){(x+1)^{n+1}(x-1)^n}
$$","['calculus', 'derivatives']"
4202956,Can I show that $n \cdot \binom{n-1}{n}=0$?,"A brief question : I know that $\binom{n}{k}$ isn't defined for $n<k$ , however, can I show that $n\cdot \binom{n-1}{n}=0$ ? if so how can I do that? I saw through wolfram that it really is zero, and I was wondering how he got into it. I have encountered this debate in a question when I needed to find a closed form of the next series: $$\sum _{k=0}^{n}\binom{n}{k}( n-k)$$ And instead of getting $n$ out first, I have tried to do the following steps: $$\sum _{k=0}^{n}\binom{n}{k}( n-k) =\sum _{k=0}^{n}\frac{n!}{k!( n-k) !}( n-k) =\sum _{k=0}^{n}\frac{n!}{k!( n-k-1) !}\\
\\
=n\sum _{k=0}^{n}\frac{( n-1) !}{k!( n-k-1) !} =n\sum _{k=0}^{n}\binom{n-1}{k} =n\left[\binom{n-1}{n} +\underbrace{\sum _{k=0}^{n-1}\binom{n-1}{k}}_{ \begin{array}{l}
\text{by Newton's binomial}\ \\
\text{it is }2^{n-1}
\end{array}}\right]\\
=n\left[\binom{n-1}{n} +2^{n-1}\right] =n\binom{n-1}{n} +n2^{n-1}$$ Now is it valid to right this as final result of a closed form for $\sum _{k=0}^{n}\binom{n}{k}( n-k)$ ? I will be happy to see from you if both questions I wrote are correct, and why.","['binomial-coefficients', 'combinatorics', 'binomial-theorem', 'discrete-mathematics']"
4203013,"Is there a square with all corner points on the spiral $r=k\theta$, $0 \leq \theta \leq \infty$?","While reading about the square peg problem , I found this paper of Jerrard , where he described that for the spiral $$r = k\theta \quad 2\pi \leq \theta \leq 4\pi $$ if we join the endpoints, you can only draw one square that all of its corner points lie in this new closed curve. Indeed, he remarked that ""The square has one corner point on the straight line segment, and does not lie entirely in the interior"". The problem is that, in a way, he said that the spiral alone (open curve) doesn't contain any inscribed square. I haven't seen a proof of this in the literature, so the question is: Is there a square that all of its corner points lie in the spiral $$r = k\theta \quad 0 \leq \theta \leq \infty $$ ? Update: Users have shown that it's possible to draw ""quasi""-squares in the spiral. This support the idea that a square should exist and that an approach using a system of equations and a fixed point theorem could be the solution. I'm wondering, what parameters (polar coordinates, @sirous parameters) should be used to write the equations for the side/diagonal lengths in the most clear way? Let's try! Let $A$ , $B$ , $C$ , $D$ the points of our desired square in the spiral and without losing generality, we take $k=1$ . Using the cosine theorem: $$d_{A,B}^2=\theta_{A}^2+\theta_{B}^2-2\theta_{A}\theta_{B}\cos(\theta_{A}-\theta_{B})$$ $$d_{B,C}^2=\theta_{B}^2+\theta_{C}^2-2\theta_{B}\theta_{C}\cos(\theta_{B}-\theta_{C})$$ $$d_{C,D}^2=\theta_{C}^2+\theta_{D}^2-2\theta_{C}\theta_{D}\cos(\theta_{C}-\theta_{D})$$ $$d_{D,A}^2=\theta_{D}^2+\theta_{A}^2-2\theta_{D}\theta_{A}\cos(\theta_{D}-\theta_{A})$$ where $\theta_{X}$ is the angle that define the point $X$ in the spiral and $d_{center,X} = \theta_{X}$ by the polar equation of the spiral. This gives 3 independent equations: $$d_{A,B}^2=d_{B,C}^2$$ $$d_{B,C}^2=d_{C,D}^2$$ $$d_{C,D}^2=d_{D,A}^2$$ and we need to add a fourth one to discriminate between a square and a rhombus. This could be a right angle between the segment AB and CD or as @Intelligenti pauca said, 2 times the length of the side squared should be equal to the length of the diagonal squared. $$2d_{A,B}^2=d_{A,C}^2$$ This should give 4 equations and 4 unknowns ( $\theta_{A},\theta_{B},\theta_{C},\theta_{D}$ ), exciting!, but the system is non-linear, so we can't establish if it has a solution a priori PD: I created this Geogebra graph , so you can slide the points along the spiral and try new ideas!","['curves', 'geometry', 'fixed-point-theorems', 'analysis', 'inequality']"
4203058,Are there numerical methods to solve a differential equation which are actually faster than numerically computing its analytical solution?,"In the topic of numerical solutions of ODE and PDE, usually it's said that many times it's impractical to try to find an analytical solution due to the complexity of the boundary conditions, or even outright impossible due to the nature of the equation, for example: \begin{equation}
    y'=e^{-x^{2}}
\end{equation} doesn't have an analytical solution we can write using other known functions.
It is implied, though, that an analytical solution it's always preferable whenever possible to speed up calculations. Given that known solutions of differential equations can turn out to be a really complicated mess of nested trigonometric and exponential functions
(as an example, this is the solution to the heat equation in 2 spatial coordinates: $$\theta(x,y)=\frac{2}{\pi}\sum_{n=1}^\infty\frac{(-1)^{n+1}+1}{n}\sin\frac{n\pi x}{L}\frac{\sinh(n\pi y/L)}{\sinh(n\pi W/L)}$$ )
and given that, for practical purposes, to output a value, the known functions like sines, cosine and exponentials which make up the solution eventually have to be computed numerically, can there be an equation for which the analytical solution is known, but so complex that solving the DE with a common numerical method like Euler or Runge-Kutta turns out to be actually faster than computing the analytical solution using Taylor series? [Edit: It was pointed out that $ y'=e^{-x^{2}} $ does indeed have an analytical solution, just not a closed form in terms of elementary functions]","['numerical-calculus', 'numerical-methods', 'ordinary-differential-equations', 'partial-differential-equations']"
4203063,Wedge products of V-valued forms.,"Let $(P, \pi, M, G)$ be a principal $G$ -bundle. Let $\omega$ be a Lie-Algebra valued one-form (connection one-form) on $P$ . Then, the two-form $\omega \wedge \omega$ that comes in the connection two-form $\Omega := d \omega + \omega \wedge \omega$ is to be interpreted as $(\omega \wedge \omega)_p (X_p,Y_p) := [\omega(X_p), \omega(Y_p)]$ . This makes sense to me. Now, suppose further that $\theta$ is a $V$ -valued one-form (for example the solder form), where V is a dim(M) dimensional representation space of the lie group $G$ . How am I to interpret $\omega \wedge \theta$ ? I am referring to the ' $\wedge$ ' that appears in the torsion "" $ \Theta := d \theta + \omega \wedge \theta$ "". How is the $\wedge$ to be interpreted in the expression $D \Theta  = \Omega \wedge \theta$ (The second Bianchi identity for principal bundles)? A side question: Suppose you have a representation $(\rho,V)$ of the lie group $G$ , then the lie group acts on $V$ by simply $v \to \rho(g)v$ . Can the Lie-Algebra $\mathfrak{g}$ aslo act on V? If so, how?","['differential-geometry', 'principal-bundles', 'connections', 'lie-groups', 'exterior-algebra']"
4203076,Existence of solution for linear system of ODE,"Consider linear system of ODE given by \begin{eqnarray}
u'_i(t)&=&\sum\limits_{i=1}^{n}a_{ij}(t)u_j+b_i(t) &\quad i=1,2,\ldots,n\\
u_i(0)&=&u_i^0 &\quad i=1,2,\ldots,n
\end{eqnarray} Suppose $a_{ij},b_i \in L^{\infty}(\mathbb{R}^+)$ and $u_i^0\in \mathbb{R}$ for $i=1,2,\ldots,n,$ then do we have the existence of solution? ?If so how to prove it? In other words, can we relax continuity assumption  on $a_{ij},b_i$ in the existence proof?","['analysis', 'ordinary-differential-equations']"
4203077,FInding the number of eigenvalues of the given $\mathbb{C}$ linear transformation.,"$T: \mathbb{C}[x] → \mathbb{C}[x]$ be the $\mathbb{C}$ -linear transformation defined on the complex vector space $\mathbb{C}[x]$ of one variable complex polynomials by $T (f(x)) = f(x + 1)$ . How many eigenvalues does T have? The basis of $\mathbb{C}[x]$ over $\mathbb{C}$ are $\{1,x,x^2,\cdots , x^{n} ,\cdots\}$ . Then, $$T(1) = 1$$ $$T(x)= x+1 $$ $$T(x^2) = (x+1)^2 = x^2 + 2x + 1$$ $$T(x^n) = (x+1)^n = x^n +  (n_{C_1})x^{n-1}+ \cdots 1$$ Now my intuition is that all the diagonal entries of the matrix will be $1$ and it will be an upper triangularized matrix then the eigen-value of this linear transformation is $1$ .We need to find the number of times it occurs Let $f(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots a_0$ be a polynomial such that $T(f(x)) = f(x) \implies f(x+1) = f(x)$ from this equation we can conclude that the only possible polynomial that will satisfy this equation is $c$ where $c \in \mathbb{C}$ . Hence , $T(1) = 1$ so $1$ is the only possible eigen-vector and $1$ is the only eigen-value. Is my way of approaching the problem correct?","['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
4203119,Connected components of fixed point set in a connected manifold,"The following questions occurred to me while reading this paper (p. 573). Let $M$ be a closed, connected, oriented Riemannian manifold of even dimension. Let $T$ be an isometry of $M$ (orientation-preserving if needed). Let $F\subseteq M$ be the fixed point submanifold of $T$ . Question 1: Does $F$ have only finitely many connected components? Question 2: Must each component of $F$ have even dimension? Both of these properties of $F$ are claimed in the paper. For the first claim, I guess one would need to know that the connected components are isolated from each other. So I'm curious, for example, why can't the fixed point set consist of an infinite number of (non-isolated) discrete points? If there is a reference that explains this, that would also be nice.","['fixed-point-theorems', 'riemannian-geometry', 'differential-geometry']"
4203130,"If for all $x>0$, we have $\lim_{n\to\infty} g(xz_n)=+\infty$, does this imply $\lim_{x\to\infty} g(x)=\infty$?","Let $g:\mathbb{R}_{>0}\to\mathbb{R}_{>0}$ be some function (not necessarily continuous), and let $(z_n)_{n\geq 1}$ be an increasing, diverging sequence of positive real numbers with the following property: For all $x>0$ , we have $\displaystyle\lim_{n\to\infty} g(xz_n)=+\infty$ . Then is it true that $\displaystyle\lim_{x\to\infty} g(x)=\infty$ ? Intuitively I would say that this isn’t true because we have only information on a very small subsample of diverging sequences, but still we have a continuum at hand which makes it hard to cook up a counter example.","['limits', 'sequences-and-series', 'real-analysis']"
4203133,"Proof of the polynomial mentioned by Gauss in his Disquisitiones Arithmeticae whose roots are $\cos(\frac{2\pi}{n}k)$ for $k = 0, 1, 2, \ldots, n-1 $.","In his book Disquisitiones Arithmeticae in ""Section VII. Equations defining sections of a circle"" in Art. 337 (p. 408-409) Gauss briefly mentions the existence of a series of polynomials whose roots are the trigonometric functions evaluated at $ \frac{2\pi}{n}k$ for $k=0,1,\ldots, n-1$ . I have only been able to get an image of this page in Spanish, but it is easy to understand: Screenshot of Article 337 . Does anyone know a proof? This is what I have so far Here I get a polynomial whose roots are $\cos(\frac{2\pi}{n}k)$ for $k = 0, 1, 2, \ldots, n-1 $ but I don't see how to get to the form in which Gauss expresses it.","['trigonometry', 'abstract-algebra']"
4203144,Evaluate $\lim\limits_{n\to\infty}\frac{1\cdot3+2\cdot4+\dots+n(n+2)}{n^3}.$,"Evaluate: $$\lim\limits_{n\to\infty}\frac{1\cdot3+2\cdot4+\dots+n(n+2)}{n^3}.$$ I'm learning limits for the first time and this is an exercise problem from my book. Here is my solution to the problem: Let $S=1\cdot3+2\cdot4+\dots+n(n+2)\\ =(1^2+2)+(2^2+4)+\dots+(n^2+2n)\\ =(1^2+2^2+\dots+n^2)+2(1+2+\dots+n)\\ =\frac{n(n+1)(2n+1)}{6}+2\cdot\frac{n(n+1)}2\\ =\frac13n^3+\frac32n^2+\frac76n$ Hence, $\lim\limits_{n\to\infty}\frac{1\cdot3+2\cdot4+\dots+n(n+2)}{n^3}\\ =\lim\limits_{n\to\infty}\frac13+\frac3{2n}+\frac7{6n^2}\\ =\frac13.$ I'm quite sure about the solution. But my book says the answer is $\frac16$ . So, is the answer in the book wrong, or am I missing something? And can the problem be solved with L'HĂ´pital's rule? (I've just started learning the rule and I don't know how to solve this using this). Some other methods to solve the problem are also welcome.","['limits', 'calculus', 'solution-verification', 'limits-without-lhopital']"
4203164,Covering of a polytope by balls with midpoints at the vertices,"I have a polytope $P=\operatorname{conv}(v_1,\ldots,v_m)\subset \mathbb{R}^n$ and a ball $B_r(x)$ (with center $x\in \mathbb{R}^n$ and  radius $r>0$ ), such that $P\subset B_r(x)$ . Is the statement $$ P\subset \bigcup_{i=1}^mB_r(v_i)$$ true? Intuitively, I think it is true, but it is hard for me to prove it. For every point $w\in P$ I can show $d(w,v_i)\leq 2r$ , since $d(w,v_i)\leq d(w,x)+d(x,v_i)\leq 2r$ . But this only implies $$ P\subset B_{2r}(v_i)\quad\forall i\in\{1,\ldots,m\}.$$ Maybe it's better to formulate it without balls. I have $m$ points $v_1,\ldots, v_m$ and i know a point $x$ and a real number $r>0$ with $d(x,v_i)\leq r$ for all $v_i$ . I claim for every $w\in\operatorname{conv}\{v_1,\ldots, v_m\}$ there is an $i$ with $d(w, v_i)\leq r$ .","['linear-algebra', 'polygons', 'convex-hulls']"
4203195,Do Carmo's definition of line of striction: Is proof of independence on directrix really needed?,"I have a question about Do Carmo's treatment of ruled surfaces in his classic Differential geometry of curves and surfaces . At pages 193–194 (I have Dover's edition) he defines the line of striction of a ruled surface. To such end, he considers the ruled parametrization \begin{align}
x \,\colon I \times \mathbb{R} &\to \mathbb{R}^{3}\\
(t,v) &\mapsto \alpha(t) + v w(t),
\end{align} where $\alpha$ is a smooth curve in $\mathbb{R}^{3}$ and $w$ a smooth unit vector field along $\alpha$ such that $w'(t)\neq 0$ for all $t\in I$ . He thus searches for a ""parametrized curve $\beta(t)$ such that $\langle \beta'(t),w'(t)\rangle =0$ , $t\in I$ , and $\beta(t)$ lies on the trace of $x$ "" (such curve is called the line of striction ); in particular, he notes that $\beta$ lying on the trace of $x$ is equivalent to the existence of a function $u = u(t)$ such that \begin{equation}\tag{1}\label{eq1}
\beta(t)= \alpha(t)+ u(t)w(t).
\end{equation} All good so far. However, as soon as he has found such a curve, he goes on to prove that "" the curve $\beta$ does not depend on the choice of the directrix $\alpha$ for the ruled surface. "" [emphasis added] Question : Why do we need to prove that $\beta$ does not depend on $\alpha$ ? EDIT: In reply to Ted Shifrin's comment, and to make the question self-contained, I am adding the proof of existence. By definition of line of striction, we need to find a curve in the image of $x$ that is everywhere orthogonal to $w'$ . As already mentioned, a curve $\beta$ is in the image of $x$ if and only if equation \eqref{eq1} holds for some function $u=u(t)$ . Differentiating \eqref{eq1} and dotting with $w'$ , we obtain \begin{align}
\beta'&= \alpha'+ u'w+uw',\\
\langle \beta',w' \rangle &= \langle \alpha',w'\rangle + u \langle w',w' \rangle.
\end{align} From the last equation, we observe that the curve $\beta$ satisfies $\langle \beta'(t),w'(t)\rangle =0$ for all $t \in I$ exactly when \begin{equation}
u=-\frac{\langle\alpha',w'\rangle}{\langle w',w' \rangle}.
\end{equation} Hence, we conclude that the desired curve is given by \begin{equation}
\beta=\alpha-\frac{\langle\alpha',w'\rangle}{\langle w',w' \rangle} w.
\end{equation} EDIT 2: If I had to prove independence on $\alpha$ , then I would do it as follows, without using the final expression of $\beta$ . Suppose that, for another directrix, the line of striction is different from $\beta$ . Then there are two lines of striction, say $\beta_{1}$ and $\beta_{2}$ . It follows that there exist two functions $u_{1}$ , $u_{2}$ of $t$ such that $\beta_{i}=\alpha+u_{i} w$ , with $i=1,2$ . However, both of them are orthogonal to the same $w'$ ; as explained below, this fact implies $u_{1}=u_{2}$ . Differentiating $\beta_{i}= \alpha + u_{i} w$ , we obtain $\beta_{i}'= \alpha' + u_{i}' w+u_{i}w'$ , which in turn implies $\langle \beta_{i}',w'\rangle= \langle\alpha',w'\rangle + u_{i} \langle w',w'\rangle$ . Since $\langle \beta_{1}',w'\rangle = \langle \beta_{2}',w'\rangle=0$ , it follows that $\langle\alpha',w'\rangle + u_{1} \langle w',w'\rangle = \langle\alpha',w'\rangle + u_{2} \langle w',w'\rangle$ , from which one concludes that $(u_{1}-u_{2}) \langle w',w'\rangle=0$ . But $\langle w'(t),w'(t)\rangle \neq0$ for all $t$ by assumption, and so $u_{1}=u_{2}$ .","['surfaces', 'riemannian-geometry', 'proof-explanation', 'definition', 'differential-geometry']"
4203197,Prove that a triangle with a given base and angle must be isosceles to have maximum perimeter,"I can't solve the following problem: Prove that triangle $ABC$ with known side $AC$ and angle $B$ has maximum perimeter when $AB=BC$ . Purely geometric proof is preferred, calculus is not allowed. I started by assuming the triangle is circumscribed by a circle uniquely determined by $AC$ and $\angle B$ , and showing that triangle height perpendicular to $AC$ is maximized when it divides $AC$ in half. Which means that area is maximized when $AB=BC$ . Which leads to $AB\times BC \gt AM\times MC$ , where M is some other point on the circle. And there I'm stuck, not sure how to continue further.","['trigonometry', 'geometry', 'inequality']"
4203230,"Objective function of complex variables, with real constraints","I have an objective function $L:\mathbb{C}^M \to \mathbb{R}$ that can be written: $$ L(x) = x^H A x - b^H x - x^H b + d $$ where superscript $H$ denotes the conjugate transpose. $A \in \mathbb{C}^M$ is a hermitian matrix of size $M$ (so $A^H = A$ ), $b$ is a complex vector $b \in \mathbb{C}^M$ , and $d$ is a real constant. I believe $L(x)$ is known to be convex, as it can be equivalently written as the squared euclidean norm of an affine function of $x$ (i.e. $L(x) = (Fx - g)^H (Fx - g)$ ). I'm looking to solve the following minimization problem, with or without the added constraint that the vector $x$ is real. Formally: $$ \text{minimize} \; L(x) $$ $$ \text{subject to} \; \text{Im}(x) = 0 $$ In other words, I want to find the real vector $x \in \mathbb{R}^M$ that minimizes the convex loss function, which is formally a function of complex input vector $x$ . I have a feeling this should be straightforward but I'm not sure exactly how to go about doing so. In the case that the ""reality"" constraint is quite non-trivial, it can be discarded. Discarding the constraint, I believe Wirtinger calculus can be used to solve this. Unfortunately, I'm not 100% clear on how to apply Wirtinger derivatives to actually obtain first-order conditions for a minima. For example, to solve the unconstrained problem, is it valid to write: $$ L(x) = x^H A x - 2 \text{Re}(b^H x) + d $$ $$ \nabla{L}(\hat{x}) = A\hat{x} - 2\text{Re}(b) = 0$$ $$ A \hat{x} = 2\text{Re}(b) $$ Thus obtaining the solution $\hat{x} = A^{-1} 2\text{Re}(b)$ ?","['convex-optimization', 'several-complex-variables', 'complex-analysis', 'linear-algebra', 'numerical-linear-algebra']"
4203233,"Is the collection of arithmetic means of a dense subset of $[0,1]$ a dense subset of $[0,1]$?","Let $\{ q_i \}_{i=1}^{\infty}$ be a dense subset of $[0,1]$ . Define the arithmetic sample mean of the sequences $\{ q_i \}$ by $$ m_n := \frac{1}{n} \sum_{i=1}^n q_i .$$ Question : is $\{ m_n \}_{n = 1}^{\infty}$ also a dense subset of $[0,1]$ ? Because $\{q_i \}$ is dense, then there always exists subsequences of $\{q_i \}$ converging to any $x \in [0,1]$ , but does this property carry over via the arithmetic mean?","['general-topology', 'analysis', 'real-analysis']"
4203243,"Relation between $SU(8)$, and $Spin(8)$ and $SO(8)/(\mathbf{Z}/2)$","It is east to use the special unitary group to contain the special orthogonal group
so $$SU(n) \supset SO(n) .$$ For $n=8$ , we have $$SU(8) \supset SO(8).$$ We know that the $SO(8)$ has a double cover $Spin(8)$ so $Spin(8)/(\mathbf{Z}/2)=SO(8)$ and also there is a $PSO(8)=SO(8)/(\mathbf{Z}/2)$ . My question is that does $$SU(8) \supset Spin(8)?$$ $$SU(8) \supset SO(8)/(\mathbf{Z}/2)?$$ What are some relations that we can say between $SU(8)$ , and $Spin(8)$ and $SO(8)/(\mathbf{Z}/2)$ ?","['spin-geometry', 'representation-theory', 'group-theory', 'lie-groups', 'differential-geometry']"
4203251,Relationship between $f(X - A)$ and $Y - f(A)$,"I am trying to solve the following problem. For $A \subset X$ , we define $f(A)$ to be the subset of $Y$ consisting of all elements $y \in Y$ for which there exists $x \in A$ with $f(x) = y$ . How does $f(X - A)$ compare with $Y - f(A)$ ? It seems to me that there may be no relationship or the subset relation could be in either or both directions. If $y \in f(X - A)$ , then $y =   f(x)$ for some $x \in X - A$ . But $f$ may not be injective, so we could also have $y = f(t)$ for some $t \in A$ , so $f(X - A) \not \subset Y - f(A)$ if $y$ has two such preimages. If $f$ is injective, then $y = f(x)$ uniquely for some $x \in X - A$ . So $y \not \in f(A)$ , hence $y \in Y - f(A)$ , so $f(X - A) \subset Y - f(A)$ . If $y \in Y - f(A)$ , then $y \not \in f(A)$ , but $f$ may not be surjective and $y$ may not even be in the image of $f$ , so there's no guarantee that $y$ be in the image of $X - A$ . If $f$ we re surjective, then $y = f(x)$ , and since $x \not \in A$ , $x \in X - A$ , so $y \in f(X - A)$ and $Y - f(A) \subset f(X - A)$ . So my conclusion based on this is: if $f$ is injective, $f(X - A) \subset Y - f(A)$ . If $f$ is surjective, $Y - f(A) \subset f(X - A)$ . So if $f$ is bijective, $f(X - A) = Y - f(A)$ . I'm not able to tell whether these are biconditional statements, but I believe they are.","['elementary-set-theory', 'solution-verification']"
4203264,If $\rho(z)<1$ then exists some $N$ such that $\lVert R(z)^N \rVert <3/4$ for all $z\in U$,"Let $\rho(z)$ denote the spectral radius of $R(z)$ , where $R:U \to \text{Hom}(\mathcal{L},\mathcal{L})$ and $U$ is a closed subset of $\mathbb{C}$ , here $\mathcal{L}$ can be understood as a Banach subspace of functions. Let $\lVert\,\cdot\,\rVert $ denote the operator norm in $\text{Hom}(\mathcal{L},\mathcal{L})$ then $R(z)=\sum_{n\ge 1}z^nR_n$ where $R_n\in \text{Hom}(\mathcal{L},\mathcal{L})$ and, $$\rho(z)=\lim_{n\to\infty} \lVert R(z)^n \rVert^{1/n}$$ And by hypothesis the spectral radius is less than $1$ , and as $U$ is compact the maximum will be reached. $$\max_{z\in U} \rho(z)<1 \tag{1}\label{eq}$$ The step I didn't understand is this, For every $z\in U$ there exists $n=n(z)$ such that $\lVert R(z)^n \rVert<1/2$ . Since $z \mapsto \lVert R(z) \rVert$ is continuous in $U$ , there is some $r=r(z)$ such that $\lVert R(w)^n \rVert<3/4$ for all $w\in B_r(z)$ . Sinze $U$ is compact, there exists some $N$ such that for all $z\in U,\, \lVert R(z)^N \rVert <3/4$ . So, my doubt is in this choice of $N$ . I've detailed a little more what was done. I believe this is the way, but I'm not sure as I couldn't finish the proof. This is my attempt to understand what was done: Since $\max_{z\in U} \rho(z)<1$ , we have that $\lVert R(z)^n \rVert\to 0$ , therefore for every $z\in U$ exists $n_z$ (that depends on $z$ ), $\lVert R(z)^n \rVert<1/2$ for all $n\ge n_z$ . I understood that now fixing the $z$ and $n_z$ , there is $r$ such that $\lVert R(w)^{n_z} \rVert<3/4$ for all $w\in B_r(z)$ . As $U$ is compact, there is a finite subcover of these balls, ie, $$U\subseteq \bigcup_{i\in\Lambda} B_{r_i}(z_i)\quad \text{where for all } w\in B_{r_i}(z_i),\, \lVert R(w)^{n_i} \rVert<3/4$$ and $\Lambda=\{1,2,\cdots,k\}$ . First I had thought of taking $N$ as the maximum of these $n_i$ , but I think taking the sum would be better. Let $N=n_1+\cdots + n_k$ and $w\in U$ , so $$\lVert R(w)^{N} \rVert \le \lVert R(w)^{n_1} \rVert\cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert $$ Without loss of generality we can assume that $w\in B_{r_1}(z_1)$ , so $$\lVert R(w)^{N} \rVert \le \lVert R(w)^{n_1} \rVert\cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert \le \frac{3}{4} \cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert$$ So what I'm having trouble with is how to ensure that the remainder is at least less than 1, to get the 3/4 of the proof. Because note that $n_i$ depends on $z$ and it might happen that $n(w)$ is greater than all $n_i$ so I can't guarantee that $\lVert R(w)^{n_i} \rVert<1/2$ . I don't know if this is the best choice for $N$ , but I couldn't think of another one. Does anyone suggest something? Another thing I thought was to see if it was possible to change the order of maximum and limit in \eqref{eq}, but for that I would need the sequence to be decreasing and I don't think I can guarantee that. Edit: It seems to me that this is a standard result, maybe someone would also have a reference where to find it? Reference: If reference is needed, this is the ""preliminaries"" of Lemma 4 of this article .","['banach-spaces', 'analysis', 'complex-analysis', 'functional-analysis', 'spectral-theory']"
4203293,"What does ""pc group"" mean in GAP?","For example, when I enter G:=CyclicGroup(2); in GAP, the program returns with the information <pc group of size 2 with 1 generators> . Question: What does ""pc group"" mean in the above?","['gap', 'group-theory', 'definition']"
4203294,How to derive this series for $\gamma$ that is only involving odd integer values of $\zeta(s)$?,"With $\gamma$ being the Euler Mascheroni constant , this series is well known: $$1- \sum_{n=2}^{\infty} \frac{\zeta(n)-1}{n} = \gamma \tag{1}$$ The following series involving $\zeta(2n+1)$ also seems to converge to the same value, albeit slower: $$1- \sum_{n=1}^{\infty} \frac{\zeta(2n+1)}{(n+1)\,(2n+1)} = \gamma \tag{2}$$ Is there a way to derive (2) from (1) ?","['riemann-zeta', 'euler-mascheroni-constant', 'sequences-and-series']"
4203312,(Sheldon Ross) Proving the independence of sample mean and sample variance,"Please refer to the proof given in : Sheldon Rose, A first course in probability : The author says that since $\{Y, X_i − \overline X, i = 1, \cdots , n\}$ and $\{\overline X, (X_i − \overline X), i = 1, \cdots, n\}$ have the same joint distribution, thus showing that $\overline X$ is independent of the sequence of
deviations $X_i − \overline X, i = 1, \cdots n.$ Somehow, the statement in bold doesn't look too obvious. Could someone please clarify? Am I missing something?","['statistics', 'normal-distribution']"
4203313,Solving $\exp(-x)=\sin(x)$ analytically,"This is related to finding out the time it takes for the capacitor to discharge in a full-wave rectifier ( link to ee.se ), and it's of the form: $$\mathrm{e}^{-x}=\sin(x) \tag{1}$$ To my knowledge it's impossible to determine $x$ analytically, because whether with $\ln()$ or $\arcsin()$ , one term gets buried. Using the exponential equivalent formula for $\sin(x)$ doesn't seem to work, either. But I'm not fluent in math, so I'm asking it here: is it possible, maybe with some tricks, cheats, anything? I shouldn't have presumed people to know what a full-wave rectifier with parallel RC load is or does, so for the sake of clarity this is what interests me: The (ideal) theory is that the sine wave charges the capacitor. At the peak and for a short interval after it (on the downslope), the voltage across the capacitor is the same as the sine. When the two slopes are equal, the capacitor voltage is no longer a sine but an $\mathrm{e}^{-x}$ , continuing from the last voltage value. The sine has an absolute value, so the second half of the period sees the value of the sine rising again, until it meets the discharging exponential -- this is what is needed here. The cycle continues: For the sake of simplicity, here, on math.se, the question deals with a generic formula, (1), not the absolute value of it, and no complications with finding out the time and value when the capacitor voltage stops being a sine and continues as an exponential. There are also no time constants involved, or frequencies, therefore, the simplified version looks like this: The capacitor discharges with the blue trace until it meets the red trace. Only the first point of intersection is needed (black dot), any other subsequent points are discarded (green circle). If this is solved, then $\mathrm{e}^{-ax}=\sin(bx)$ can also be solved, and even the moment when the waveform switches shapes, though I suspect that will be a tad more complicated (and not part of this question).","['calculus', 'algebra-precalculus']"
4203318,When is the derivative of a function equal to the integral of the same function?,"My question basically is : $$\text{When is}\space  \dfrac{d}{dx} f(x) = \int f(x)$$ Is there any condition for these two to be equal? My attempt was to take the derivative of both the sides, getting: $$\dfrac{d^2}{dx^2} f(x) = f(x)$$ which is equivalent to solving the differential equation: $$\begin{align} f''(x) = f(x) \Rightarrow f''(x) - f(x) = 0 \Rightarrow f(x) = C_1 e^x + C_2 e^{-x}
\end{align}$$ Is my approach correct? Is there any other approach which leads to the same answer? Thanks in advance","['integration', 'solution-verification', 'derivatives', 'ordinary-differential-equations']"
4203324,Why do people say the Fundamental Theorem of Calculus is so amazing? [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 2 years ago . Improve this question The fundamental theorem of calculus in the fifth edition of Stewarts Calculus text is stated as: Suppose $f$ is continuous on $[a,b]$ . Then: 1). If $g(x) = \int_a^x f(t)dt$ , then $g'(x)=f(x)$ 2). $\int_a^bf(x)dx=F(b)-F(a)$ where $F$ is any antiderivative of $f$ . So, when we interpret $g(x)$ as the function that tells us the ""area so far"" under the graph of $f$ , I think $(1)$ is pretty straightforward... honestly, it seems like with all the brilliant minds that came before Newton/Leibniz that this is something that should have already been clearly understood. So, the reason the FTC is so ""amazing"" is usually stated as ""OMG, we can calculate the area under the integral by knowing the value of ANY of its antiderivatives at ONLY the two boundary points!"" However, I feel a bit cheated. We define the function $g(x)$ as a limit of Riemann sums that involve the entire curve. So yeah, it's going to give the entire area under the graph of $f$ from $a$ to $x$ even though we only plug $x$ into the function, but that's not to say the calculation of our function didn't involve the whole curve, you know? After this, it follows quite directly from the mean value theorem that any two antiderivatives differ by a constant, and so we arrive at $(2)$ . Now, I hope I don't come off too sarcastic in this post, because I genuinely suspect that there is something that my feeble mind is not seeing that makes this more amazing, and that one of the more adequate thinkers that linger here can enlighten me. edit: antiderivatives and integrals are defined in very different ways. The fact that $\int_a^x f(t)dt$ is an antiderivative of $f(x)$ is very cool.",['calculus']
4203416,How do I account for the tilting of Earth's axis in right ascension and declination in a model solar system?,"I am building a simplified model solar system in GeoGebra. Celestial objects are placed in a heliocentric coordinate system with the sun at the origin, the x-y-plane as the ecliptic, and the x-axis aligned with Earth's March Equinox. Earth's center is located at the point (EarthOrbitRadius cos(2π t), EarthOrbitRadius sin(2π t)), where t is the time in years. Earth's equator and axis of rotation are rotated by AxialTilt° about the line y = EarthOrbitRadius sin(2π t), which marks the directions of the March and September equinoxes. I want to know the right ascension (RA, the angle eastward along the celestial equator between the line marking the March equinox and the hour circle of the target point, in the range 0° ≤ RA < 360°) and declination (DEC, the angle along the hour circle of the target point, perpendicular to the celestial equator, in the range −90° ≤ DEC ≤ 90° with negative values being south of the celestial equator and positive values being north) for the sun as measured from Earth. The hour circle is the great circle that passes through the target point and the two celestial poles; in the case of the Earth, these are the north and south poles along the axis of rotation. Put another way, the hour circle of a point lies on the plane formed between the line of the axis of rotation and the line through the point and the center of the Earth. If the Earth's axis was not tilted, this would be easy. The RA of the sun would be simply 2π t, and the DEC of the sun would be a constant 0°. With the axis tilted, however, the values change. The RA acquires a wobble, and the DEC becomes approximately (but not exactly, it also has a wobble) AxialTilt sin(2π t). The wobbles in the RA and DEC values appear to be roughly sinusoidal, but I can't figure out an exact form for them. The RA anomaly looks approximately like sin(4π t) with the peaks shifted and scaled down vertically by some relation to the AxialTilt value, while the DEC anomaly looks approximately like −sin(2π t) − sin(6π t), also scaled vertically by some relation to the AxialTilt value. In both cases, the larger the AxialTilt value, the larger the wobble. Is there an exact form for these wobbles?","['spherical-trigonometry', 'trigonometry', 'mathematical-astronomy']"
4203427,Is it possible to prove $ \lim_{ x \to \infty}M(x)/\sqrt{x} - \sum_{n \le x}\mu(n)/\sqrt{n} $ converges?,"With $\mu(n)$ being Möbius function and $M(x) = \sum_{n \le x} \mu(n)$ being Martens function: Is it possible to prove that the difference between two diverging terms given below $$ \lim_{ x \to \infty} \Big ( \frac{M(x)} {\sqrt{x}} - \sum_{n \le x} \frac{\mu(n)}{\sqrt{n}} \Big) $$ converges? Both terms above diverge but,  computationally, the difference between them seems to be converging to $-1/\zeta(1/2)$ .","['riemann-zeta', 'number-theory']"
4203532,Why did mathematicians choose the inner product to be linear in the first argument instead of the second?,"From my limited experience with inner product spaces, it seems like the inner product being linear in the second argument would facilitate smoother notation. For instance, for $x \in H$ , we could define $x^* \in H^*$ by $$x^*y = \langle x, y\rangle $$ Then this would generalize the fact that $x^T y = \langle x, y\rangle$ on $\mathbb{R}^n $ . Does linearity in the first argument make for smoother notation in some other aspect of Hilbert space theory?","['hilbert-spaces', 'inner-products', 'linear-algebra']"
4203558,Is there a meaningful definition of $\lim_{f(x)\to a}g(x)$?,"Fairly often, I have seen the notation $\lim_{f(x)\to a}g(x)$ being used in the context of evaluating limits, e.g. $$
\lim_{x \to 0}\frac{\sin(2x)}{2x}=\lim_{2x\to0}\frac{\sin 2x}{2x}\overset{y=2x}=\lim_{y \to 0}\frac{\sin y}{y}=1 \, .
$$ While the meaning of such equations is clear enough, I consider this to be a mild abuse of notation for the following reason: when we write $\lim_{x \to a}f(x)$ , the variable $x$ is bound by the limit operator; however, only single-letter variables such as $x$ and $y$ can be bound, not arbitrary algebraic expressions such as $2x$ . So is there a formal definition of $\lim_{f(x)\to a}g(x)=l$ that avoids this problem? Perhaps we say that $\lim_{f(x)\to a}g(x)=l$ if $a$ is an accumulation point of $\operatorname{dom}(g)\cap\operatorname{ran}(f)$ , and $$
\forall\varepsilon>0:\exists\delta>0:\forall x\in\operatorname{dom}(g)\cap\operatorname{ran}(f):0<|f(x)-a|<\delta\implies |g(x)-l|<\varepsilon \, .
$$","['real-analysis', 'notation', 'calculus', 'definition', 'limits']"
4203564,Finding the surface area of a sphere within a cylinder and above the $xy$-plane using double integral,"I'm trying to find the surface area of the part of the sphere $x^2+y^2+z^2=a^2$ that lies within the cylinder $x^2+y^2=ax$ and above the xy-plane. I want to do this using $$A(S)=\iint_D \sqrt{[f_{x}(x,y)]^2+[f_{y}(x,y)]^2+1} \; dA.$$ My understanding is that $f=z=\sqrt{a^2-x^2-y^2}$ , $z_x=-x(a^2-x^2-y^2)^{-1/2}$ , $z_y=-y(a^2-x^2-y^2)^{-1/2}$ . What I don't understand is what the bounds for the integrals should be. I think it has to be done using polar coordinates, but I'm lost as to what the radius and angle bounds would be -- I'm not visualizing the cylinder and sphere very well. Any help would be appreciated.","['integration', 'multivariable-calculus', 'multiple-integral', 'geometry']"
4203586,Finding a best fit second order polynomial,"Problem: Assume we have the following points: $(x_0,y_0), (x_1,y_1), (x_2,y_2), (x_3,y_3)$ where $x_0 = -3$ , $x_1 = -2$ , $x_2 = -1$ and $x_3 = 0$ .
Given the function $f(x) = Ax^2 + Bx + C$ find the constants $A$ , $B$ and $C$ such that $f(0) = y_3$ and $$d = \sum_{i = 0}^{2} (f(x_i) - y_{i})^2$$ is minimized. Answer: First we apply the requirement that $f(0) = y_3$ . \begin{align*}
f(0) &= A(0) + B(0) + C = y_3 \\
C &= y_3 \\
d &= \sum_{i = 0}^{2} (A(x_i)^2 + B(x_i) + y_3 - y_{i})^2
\end{align*} We write $d_A$ to represent the partial derivative of $d$ with respect
to $A$ . \begin{align*}
d_A &= \sum_{i = 0}^{2} 2(x_i)^2(A(x_i)^2 + B(x_i) + y_3 - y_{i}) \\
d_B &= \sum_{i = 0}^{2} 2(x_i)(A(x_i)^2 + B(x_i) + y_3 - y_{i}) \\
\end{align*} Now we set the partial derivatives to $0$ to find a minimum. \begin{align*}
\sum_{i = 0}^{2} 2(x_i)^2(A(x_i)^2 \\
 +& B(x_i) + y_3 - y_{i}) &= 0 \\
\sum_{i = 0}^{2} 2(x_i)(A(x_i)^2 + B(x_i) \\
  +& y_3 - y_{i}) &= 0 \\
\sum_{i = 0}^{2} (A(x_i)^2 + B(x_i) + y_3 - y_{i}) &= 0 \\
\sum_{i = 0}^{2} (A(x_i)^2 + B(x_i) + y_3 - y_{i}) &= 0 \\
\end{align*} Am I right so far? Now how do I proceed? Now I solve the equation $d_A = 0$ . Here is what I get: \begin{align*}
 (A(-3)^2 + B(-3) + y_3 - y_{0})
 	+  (A(-2)^2 + B(-2) + y_3 - y_{1})
	+  (A(-1)^2 + B(-1) + y_3 - y_{2})
 &= 0 \\
%
9A - 3B + y_3 - y_{0} +  4A - 2B + y_3 - y_{1} + A - B + y_3 - y_{2}
	&= 0 \\
%
14A - 6B  + 3y_3 - y_{0} - y_{1} - y_{2} &= 0 \\
\end{align*} Now, I need to solve the equation $d_B = 0$ . However, that will produce the same equation as $d_A = 0$ . Therefore, I do not know how to find a unique value for $A$ and $B$ . Based upon the group's comments. I have updated my solution again. I believe my formula for A is correct. However, my formula for B
is wrong. Answer: First, I apply the requirement that $f(0) = y_3$ . \begin{align*}
f(0) = y_3 &= (A)(0) + (B)(0) + C \\
C &= y_3 \\
f(x) &= Ax^2 + Bx + y_3 \\
d &= (f(x_0) - y_{0})^2 + (f(x_1) - y_{1})^2 + (f(x_2) - y_{2})^2  \\
d &= ( Ax_0^2 + Bx_0 + y_3 - y_{0})^2 + ( Ax_1^2 + Bx_1 + y_3 - y_{1})^2
	+ ( Ax_2^2 + Bx_2 + y_3 - y_{2})^2 
\end{align*} I will write $d_A$ for the partial derivative of $d$ with respect to $A$ . \begin{align*}
d_A &=  2x_0^2( Ax_0^2 + Bx_0 + y_3 - y_{0})
	+ 2x_1^2( Ax_1^2 + Bx_1 + y_3 - y_{1})
	+ 2x_2^2( Ax_2^2 + Bx_2 + y_3 - y_{2}) \\
d_B &=  2x_0( Ax_0^2 + Bx_0 + y_3 - y_{0})
	+ 2x_1( Ax_1^2 + Bx_1 + y_3 - y_{1})
	+ 2x_2( Ax_2^2 + Bx_2 + y_3 - y_{2}) \\
\end{align*} Now I set $d_A = 0$ and $d_B = 0$ . I then solve for $A$ and $B$ . \begin{align*}
x_0^2( Ax_0^2 + Bx_0 + y_3 - y_{0})
	+ x_1^2( Ax_1^2 + Bx_1 + y_3 - y_{1})
	+ x_2^2( Ax_2^2 + Bx_2 + y_3 - y_{2})
&= 0 \\
x_0( Ax_0^2 + Bx_0 + y_3 - y_{0})
	+ x_1( Ax_1^2 + Bx_1 + y_3 - y_{1})
	+ x_2( Ax_2^2 + Bx_2 + y_3 - y_{2})
&= 0 \\
\end{align*} Now, I am going to work on the first equation. I will substitute values
for $x_0$ , $x_1$ and $x_2$ \begin{align*}
9( 9A - 3B + y_3 - y_0)
	+ 4( 4A - 2B + y_3 - y_{1})
	+ 1( A - B + y_3 - y_{2})
	&= 0 \\
98A - 36B + 14y_3 - 9y_0 - 4y_1 - y_2 &= 0 \\
\end{align*} Now, I work on the second equation. \begin{align*}
-3( 9A  - 3B + y_3 - y_{0})
	- 2( 4A - 2B + y_3 - y_{1})
	- ( A - B + y_3 - y_{2})
&= 0 \\
-36A + 14B - 6y_3 + 3y_0 + 2y_1 + y_2 &= 0 \\
\end{align*} \begin{align*}
14B &= 36A + 6y_3 - 3y_0 - 2y_1 - y_2 \\
B &= \dfrac{ 36A + 6y_3 - 3y_0 - 2y_1 - y_2  } { 14 }
\end{align*} Now we can solve the first equation for $A$ . \begin{align*}
98A - 36 \left( \dfrac{ 36A + 6y_3 - 3y_0 - 2y_1 - y_2  } { 14 } \right) + 14y_3 - 9y_0 - 4y_1 - y_2 &= 0 \\
%
98A - 18 \left( \dfrac{ 36A + 6y_3 - 3y_0 - 2y_1 - y_2  } { 7 } \right)
	+ 14y_3 - 9y_0 - 4y_1 - y_2 &= 0 \\
%
686A - 18 \left( 36A + 6y_3 - 3y_0 - 2y_1 - y_2 \right)
+ (7)(14)y_3 - 63y_0 - 28y_1 - 7y_2 &= 0 \\
%
686A - 18 \left( 36A + 6y_3 - 3y_0 - 2y_1 - y_2 \right)
	+ 98y_3 - 63y_0 - 28y_1 - 7y_2 &= 0 \\
%
38A - 18( 6y_3 - 3y_0 - 2y_1 - y_2 ) + 98y_3 - 63y_0 - 28y_1 - 7y_2 &= 0 \\
%
38A - 10y_3 - 18( - 3y_0 - 2y_1 - y_2 ) - 63y_0 - 28y_1 - 7y_2 &= 0 \\
38A - 10y_3 + 18( 3y_0 + 2y_1 + y_2 ) - 63y_0 - 28y_1 - 7y_2 &= 0 \\
38A - 10y_3 - 9y_0 + 8y_1 + 11y_2 &= 0 \\
\end{align*} \begin{align*}
38A &= 10y_3 + 9y_0 - 8y_1 - 11y_2 \\
A &= \dfrac{ 10y_3 + 9y_0 - 8y_1 - 11y_2 }{38 }
\end{align*} Now we solve for $B$ . \begin{align*}
B &=
	\dfrac{ 36 \left(  \dfrac{ 10y_3 + 9y_0 - 8y_1 - 11y_2 }{38 }\right)  + 6y_3 - 3y_0 - 2y_1 - y_2  } { 14 } \\
%
B &=
	\dfrac{ 18 \left(  \dfrac{ 10y_3 + 9y_0 - 8y_1 - 11y_2 }{19 }\right)  + 6y_3 - 3y_0 - 2y_1 - y_2  } { 14 } \\
B &=
	\dfrac{ 18(  10y_3 + 9y_0 - 8y_1 - 11y_2 ) + 19( 6y_3 - 3y_0 - 2y_1 - y_2  ) }{ 19(14)} \\
B &= \dfrac{  180y_3 +162y_0 - 144y_1 - 198y_2
		+ 6(19)y_3 - 3(19)y_0 - 28y_1 - 19y_2  } { 19(14)} \\%
B &= \dfrac{  180y_3 +162y_0 - 144y_1 - 198y_2
		+ 114y_3 - 57y_0 - 28y_1 - 19y_2  } {266} \\
B &= \dfrac{  294y_3 + (162-57)y_0 -144y_1 - 28y_1 - 198y_2 - 19y_2  } {266} \\
%
B &= \dfrac{  294y_3 + 105 y_0 - 172y_1 - 217y_2  } {266} \\
\end{align*} Now I will try out the formula on the following set of points: $$( -3, 0), (-2,1), (-1,2), (0,5)$$ We have: \begin{align*}
y_0 &= 0 \\
y_1 &= 1 \\
y_2 &= 2 \\
y_3 &= 5 \\
C &= 5 \\
A &= \dfrac{ 10y_3 + 9y_0 - 8y_1 - 11y_2 }{38 } \\
A &= \dfrac{ 10(5) + 9(0) - 8(1) - 11(2) }{38 }
	= \dfrac{ 50 - 8 - 22 }{38 } \\
A &= \dfrac{ 10 }{ 19 } \\
B &= \dfrac{  294y_3 + 105 y_0 - 172y_1 - 217y_2  } {266} \\
B &= \dfrac{  294(5) + 105(0) - 172(1) - 217(2)  } {266} \\
B &= \dfrac{  1470 - 172 - 217(2)  } {266} \\
B &= \dfrac{  1470 - 172 - 434  } {266} = \dfrac{ 864 }{ 266} \\
B &= \dfrac{ 432 } { 133 } \\
B &\doteq 3.2481203
\end{align*} Now I will try out the formula on the following set of points: $$( -3, -1), (-2,1), (-1,2), (0,10)$$ We have: \begin{align*}
y_0 &= -1 \\
y_1 &= 1 \\
y_2 &= 2 \\
y_3 &= 10 \\
C &= 5 \\
A &= \dfrac{ 10y_3 + 9y_0 - 8y_1 - 11y_2 }{38 } \\
A &= \dfrac{ 10(10) + 9(-1) - 8(1) - 11(2) }{38 } \\
A &= \dfrac{ 100 - 9 - 8 - 22 }{38 } \\
A &= \dfrac{ 61}{38} \\
B &\doteq 1.6052632 \\
B &= \dfrac{  294y_3 + 105 y_0 - 172y_1 - 217y_2  } {266} \\
B &= \dfrac{  294(10) + 105(-1) - 172(1) - 217(2)  } {266} \\
B &= \dfrac{  2940 - 105 - 172 - 434 } {266}   \\
B &= \dfrac{ 2229 } {266} \\
B &\doteq 8.3796992
\end{align*} According to R, the correct value for $A$ is $1.605263$ and the
correct value for $B$ is $8.34210475$ .","['multivariable-calculus', 'statistics']"
4203596,Why is the drift of the stock price not important for options pricing?,"This question is motivated by MSE question 4199364: Bachelier model option pricing . There, one considers the price of a stock depending on time $t$ , given by the family of random variables $(S_t)_{t\in[0,\infty[}$ as $$S_t = 1+\mu t+W_t$$ for some $\mu\in\mathbb R$ , where $(W_t)_{t\in[0,\infty[}$ is a standard Wiener process. (Note that the precise form of $S_t$ is not important for the question asked below, for instance, one might ask the same question if $S_t$ is a, say, geometric Brownian motion, given by $S_t=S_0 \exp((\mu-\sigma^2/2)t+\sigma W_t)$ for constants $\mu\in\mathbb R$ , $\sigma \in \mathbb{R}^+$ (i.e. the solution to $\mathrm dS_t = \mu S_t\,\mathrm dt + \sigma S_t\,\mathrm dW_t$ , where stochastic differential notation was used), or more generally, any stochastic process with drift.) The question was what the price of an option paying $1$ dollar (or any other currency) if $S_1>1$ and paying $0$ dollars if $S_1<1$ would be in an idealized market (as far as I know, the usual ""idealized"" market is characterized fully by some technical assumptions about which I know nothing and the condition that there is no arbitrage). Now, naïvely I would expect the price of the option described above to be just $\mathsf P(S_1>1)$ dollars (where $\mathsf P$ is the probability measure of the probability space on which $(W_t)_{t\in[0,\infty[}$ was defined (sometimes called a ""Wiener space"")). However, I was informed that, in fact, the price of the option is independent of $\mu$ . Why is that so? Why does a higher probability of getting $1$ dollar not result in a higher price for the option?","['stochastic-analysis', 'finance', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
4203635,Powers of AM-GM-HM triples,"Let AM, GM, HM respectively denote the arithmetic, geometric and harmonic mean. An elementary consequence of Maclaurin's inequality shows that $$\text{AM}^{n-1} \text{HM} \geq \text{GM}^n.$$ Relating to the following problem I raised on MO, concerning an alternative to a well-known trace estimate in Riemannian Geometry , I'm an interested in whether an inequality of the form $$\text{AM}^a \text{HM}^b \geq \lambda \text{GM}^c,$$ for $a,b,c, \lambda >0$ forces $a = n-1, b = 1$ and $c=n$ ? Here, $\lambda$ is a negligible numerical constant. Of course, the means all scale homogeneously of degree $1$ , and therefore, an elementary scaling argument forces $a+b = c$ . Moreover, taking various powers of the original inequality should be deemed equivalent.","['algebra-precalculus', 'means', 'riemannian-geometry', 'inequality']"
4203638,Chances to go infinite in Magic: The Gathering using Dungeons & Dragons set dice rolling,"So Wizards of the Coast recently released a Dungeons & Dragons -themed Magic: The Gathering set. In this set they have the following two creatures: Pixie Guide and Delina . With those two cards attacking, a funny thing happens: A 20-sided dice is rolled for each Pixie Guide on the field + 1. If at least one of those dice rolls 15 or greater, a copy of Pixie Guide is made, then the process is repeated starting from 1. Question: What are the chances that this goes infinite? Partial solution: Given $i$ dice, the probability of low-rolling all rolls is $p^i$ , where $p=14/20$ . The probability of high-rolling at least one dice is thus $1-p^i$ , and the probability of going infinite is thus $$P=\prod_{i=2}^{\infty}(1-p^i)$$ Note that the product starts at $i=2$ , because the very first time we roll two dice. Does $P$ have a closed expression for $0 < p < 1$ ? A simulation suggests it converges to 0.14105299 for 100 terms.","['limits', 'sequences-and-series']"
4203652,Showing that the natural numbers are totally ordered with respect to set membership,"Working with the usual set theoretic construction of the natural numbers, denoted $\omega$ for now. I am trying to show that $\omega$ is totally ordered with respect to set membership, that is, $n<m$ if and only if $n\in m$ is a total ordering on $\omega$ . I can show irreflexivity and transitivity, but I am struggling to show totality. Here is an attempt using induction: Fix $m\in\omega$ . Define $W=\{n\in\omega:[n \neq m\Longrightarrow (n\in m)\vee (m\in n)]\vee n=m\}$ . If $m=\emptyset$ , then $\emptyset\in W$ . If $m\neq\emptyset$ , then $\emptyset\in m$ so $\emptyset\in W$ . Suppose that $n\in W$ for some $n\in\omega$ . Now either $m=n\cup\{n\}$ or $m\neq n\cup\{n\}$ . If $m=n\cup\{n\}$ , then $n\cup\{n\}\in W$ . Otherwise, $n\cup\{n\}\neq m$ . If $n=m$ , then $m\in n\cup\{n\}$ so $n\cup\{n\}\in W$ . If $n\neq m$ , then $n\in m$ or $m\in n$ . If $m\in n$ , then $m\in n\cup\{n\}$ . If $n\in m$ , then $n\subset m$ and $\{n\}\subseteq m$ so $n\cup\{n\}\subset m$ . This is as far as I can get. I have been unable to show that $n\subset m$ implies $n\in m$ , so my proof comes to a halt at the last case. If I had that fact, then I would be able to say $n\cup\{n\}\in m$ which would complete the proof. Trying to show that fact inductively has felt more difficult than it should be. What else would I need to show in order to complete the proof, or what other route could I take to show that set membership totally orders $\omega$ ?","['proof-explanation', 'natural-numbers', 'elementary-set-theory', 'induction', 'set-theory']"
4203667,Cohomology ring of grassmannian and Pieri rule,"I am learning Schubert variety and I came across a problem to understand a particular detail (I asked the same question on mathoverflow : https://mathoverflow.net/questions/397999/cohomology-ring-of-grassmannian-and-pieri-rule ): Let $X=OG(n,2n+1)$ , where $OG(n,2n+1)$ denotes the variety of $n$ -dimensional isotropic subspaces of a vector space $\mathbb{C}^{2n+1}$ with a nondegenerate symmetric bilinear form. According to Theorem 2.2 a) (Page 17, Anders Skovsted Buch, Andrew Kresch, Harry Tamvakis, Quantum Pieri rules for isotropic grassmannians, https://arxiv.org/pdf/0809.4966.pdf ), the cohomology ring of X is given by $$ H^{*}(X,\mathbb{Z})=\mathbb{Z}[\tau_{1},\ldots, \tau_{n}]/I,$$ where $I$ is the ideal generated by $$ \tau_{r}^{2}-2\tau_{r+1}\tau_{r-1}+2\tau_{r+2}\tau_{r-2}+\cdots +(-1)^{r}\tau_{2r}$$ for $1\leq r\leq n$ . In particular, if $n=4$ , then the ideal $I$ is generated by the following four elements $$
\tau_{1}^{2}-\tau_{2},\quad \tau_{2}^{2}-2\tau_{3}\tau_{1}+\tau_{4}, \quad \tau_{3}^{2}-2\tau_{4}\tau_{2},\quad \tau_{4}^{2}.\tag{*}\label{*}$$ But if I apply Pieri rule for X (Theorem 2.1, Page 16, Anders Skovsted Buch, Andrew Kresch, Harry Tamvakis, Quantum Pieri rules for isotropic grassmannians, https://arxiv.org/pdf/0809.4966.pdf ) to $\tau_{2}\cdot \tau_{2}$ , I get the following relation $$\tau_{2}^{2}-2\tau_{3}\tau_{1}-\tau_{4} \tag{**}\label{**}$$ Therefore, combining (\ref{*}) and (\ref{**}), I get $2\tau_{4}=0$ in $H^{*}(X,\mathbb{Z})$ . I seems that some computation is wrong, but I don't know where I made a mistake.","['representation-theory', 'grassmannian', 'algebraic-geometry', 'homology-cohomology', 'schubert-calculus']"
4203682,Cumulative distribution function leftside limit,"Define $F(a-)=\lim _{x\uparrow a}F(x)$ . Then, if $F$ is nondecreasing, $F(a-)=\lim _{n\rightarrow\infty} F(a-1/n)$ . Use (1.1) to show that if a random variable $X$ has cumulative distribution function $F_X$ , that $P(X<a)=F_X(a-)$ . Also show that $P(X=a)=F_X(a)-F_X(a-)$ . $B_1\subset B_2 \subset \dots \Rightarrow \mu\left(\bigcup_{n=1}^\infty B_n\right)=\lim_{n\rightarrow\infty} \mu(B_n)$ (1.1) Here is a relevant definition. If $(\mathcal {E, B}, P)$ is a probability space, where $e\in \mathcal E$ are called outcomes, $B\in\mathcal B$ are called events, and $P(B)$ is called the probability of $B$ , $$\begin{split}F_X(a-)&=\lim_{n\rightarrow\infty} F_X(a-1/n)\\
&=\lim_{n\rightarrow\infty} P(\{e\in\mathcal E:X(e)\le a-1/n\})&&\text{ definition of cdf}\\
&=P\left(\bigcup_{n=1}^\infty \{e\in\mathcal E:X(e)\le a-1/n\}\right)&&\text{ from (1.1)}\\
&=P\left(\{e\in\mathcal E:X(e)<a\}\right)\\
&=P(X<a)\end{split}$$ And for the next part, $$\begin{split}F_X(a)-F_X(a-)&=P(X\le a)-P(X<a)\\
&=P(\{e\in \mathcal E: X(e)\le a\})-P(\{e\in \mathcal E: X(e)< a\})\\
&=P(\{e\in\mathcal E: X(e)=a\})&&\text{ def. of measure}\\
&=P(X=a)\end{split}$$ Does it seem correct? In particular, for the second part, I can't directly go from step 1 to step 4 even though it might be obvious from what a probability is, I think, since all I know is that $P$ is a probability measure. And I also was wondering if using $P(\{e\in\mathcal E:X(e)\dots\})$ is necessary.","['cumulative-distribution-functions', 'measure-theory', 'solution-verification', 'probability-theory']"
4203690,Proving $f\left(\bigcup\limits_{i \in I} A_i\right) = \bigcup\limits_{i \in I} f(A_i)$. [duplicate],"This question already has answers here : Show that $\bigcup_i f(A_i) = f(\bigcup_i A_i)$ (3 answers) Closed 2 years ago . I am trying to prove that Proving $f\left(\bigcup\limits_{i \in I} A_i\right) = \bigcup\limits_{i \in I} f(A_i)$ . Here is my attempt. Given $y \in Y$ , we have: \begin{align*}
y \in f\left(\bigcup\limits_{i \in I} A_i\right) & \iff \exists a \in \bigcup\limits_{i \in I} A_i, \; y = f(a) \\
& \iff \exists i \in I, \; a \in A_i, \; y = f(a) \\
& \iff \exists i \in I, \; y \in f(A_i) \\
& \iff y \in \bigcup\limits_{i \in I} f(A_i).
\end{align*} How does this look?","['elementary-set-theory', 'solution-verification']"
4203707,A problem about the Borel-Cantelli lemma in Feller's Introduction to Probability,"The problem is from Chapter 8 of the book, and it states the following. ""In a sequence of Bernoulli trials let $A_n$ be the event that a run of $n$ consecuitive successes ocurrs between the $2^n$ th trial and the $2^{n+1}$ st trial. If $p \ge \frac{1}{2}$ , there is probability one that infinitely many $A_n$ occur; if $p<\frac{1}{2}$ , then with probability one, only finitely many $A_n$ ocurr"" I've had several issues when trying to solve this problem, but the most important one is how the book solves this problem: ""Note that $Pr(A_n) < (2p)^n$ , but $Pr(A_n) > 1 - (1-p^n)^{\frac{2^n}{2n}} > 1-e^{\frac{(-2p)^n}{2n}}$ . If $p=\frac{1}{2}$ the last quantity is approximately $\frac{n}{2}$ ; if $p>1$ then $Pr(A_n)$ does not even tend to zero"". Can someone explain the proof that is given? Because it kinda confused me.","['probability-theory', 'solution-verification', 'borel-cantelli-lemmas', 'problem-solving', 'probability']"
