question_id,title,body,tags
186922,how to show that power series is analytic inside the radius of convergence?,Let $f(z) = \sum a_n z^n$ be a power series with radius of convergence $R$. How do we show that $f$ is analytic in the circular region of radius $R$?,"['reference-request', 'complex-analysis']"
186923,Random variables with the same distributions,"Let $K$ and $L$ be locally compact Hausdorff spaces. Also, let $P$ be a Radon probability measure on $K$ so that $(K,P)$ is a probability space. I want to know, whether two random variables $X,Y\colon K\to L$ (they might be assumed to be continuous) have the same distributions. Is it sufficient if I check that $$\int_K f(X(\omega))P(d\omega) = \int_K f(Y(\omega))P(d\omega)$$ for each $f\in V$, where $V$ is some dense subspace of the Banach space $C_0(L)$ of continuous functions on $L$ which vanish at infinity?","['probability-theory', 'functional-analysis', 'banach-spaces']"
186925,Is morphism between curves projective?,"Suppose $X$ is a smooth, projective curve, $Y$ is an arbitrary curve(may be singular), and both curves are over an algebraically closed field $k$ of characteristic 0. Let $f: X \to Y$ be a morphism between curves. Is $f$ a projective morphism? Here, projective morphism is in the sense of Hartshorne, i.e. $X \to Y$ factors through $X \to \mathbb{P}^{n}_{Y} \to Y$ , with $X \to \mathbb{P}^{n}_{Y}$ a closed embedding, $\mathbb{P}^{n}_{Y} \to Y$ the the natural projection to $Y$ factor. I guess it is projective by the following general heuristic argument: Statement :Suppose $X \subset \mathbb{P}^{n}$ is a closed subvariety, $Y$ is another variety, then any morphism $f: X \to Y$ is projective. One can define $f' :X \to \mathbb{P}^{n}_{Y}$ by $x \mapsto (x,f(x))$ , and this is an injective map. Moreover, because $X$ is proper, its image must be closed. I guess these guarantee $f'$ is a closed embedding, and the projection $\mathbb{P}^{n}_{Y} \to Y$ is easy to define. I am not quite sure about the above argument, especially $f'$ being a closed embedding. Any suggestions?",['algebraic-geometry']
186930,find all the entire functions that satisfies a given condition,I have been struggling to find a solution for this problem: Find all the entire analytic functions $f(z)$ (analytic in the complex plane) that satisfy the condition $|z^2f(z)-3+e^z|\leq3$ for all $z \in \mathbb{C}$. Any ideas? Thank you in advance.,['complex-analysis']
186959,Correlation matrix from Covariance matrix,"This is for a project which I've been trying to find some information for Covariance matrix and correlation matrix. I understand that for a $n \times n$ matrix $A, AA^T$ will give me the covariance matrix. Is there any relationship between the covariance and correlation matrix? Sorry maybe I wasn't clear. I wanted to use Cholesky decomposition to generate correlated variables from random variables. I do know how to do it using matlab. And I understand how it works for 2 variables. But when I scale up the matrix to $n \times n$ instead of $2 \times 2$, I am not sure how it will work out. would appreciate if someone could provide more hint on the mathematics.","['statistics', 'linear-algebra']"
186968,"Given digits $2,2,3,3,4,4,4,4$ how many distinct $4$ digit numbers greater than $3000$ can be formed?","Given digits $2,2,3,3,4,4,4,4$ how many distinct $4$ digit numbers greater than $3000$ can be formed? one of the digits which can be formed is $4444$ $4$ digit numbers greater than $3000$, which consists of only $2's$ and $4's$ are $4224$, $4242$, $4244$, $4422$, $4424$, $4442$ is there a well defined technique to solve this question.",['combinatorics']
186972,How can LU factorization be used in non-square matrix?,"In my textbook, there is some information about LU factorization of square matrix $A$, but not about non-square matrix. How can LU factorization be used to factorize non-square matrix?",['linear-algebra']
186991,Complex Conjugate of Complex function,"I am currently reading Hamming's Numerical Methods for Scientists and Engineers . On pg. 79 he discusses the topic of finding the zeros of a complex analytic function. He then proceeds to discuss different types of complex conjugation for a function $w(z)$. Here are examples of the three types for $w(z)=\sin z = \frac{e^{iz} - e^{-iz}}{2i}$ $\overline{w}(z)$ : replace $i$ with $-i$ in $w$, e.g. $\overline{w}(z)= \frac{e^{-iz} - e^{iz}}{-2i} = \sin z$. $w(\overline{z})$ : conjugate the argument, $w(\overline{z}) = \frac{e^{i\overline{z}} - e^{-i\overline{z}}}{2i}$ $\overline{w(z)}$ : Hamming describes this as conjugating the values which I take to mean the conjugate of the image of $w(z)$ in the codomain. Although, I am not clear on this. These three definitions have left me a bit confused. 2 and 3 seem relatively straight forward. But 1 leaves me a bit baffled in that it doesn't appear to be an actual complex conjugate of anything. It appears that the first version can only be applied to functions of $z$, because if I rewrite $w(z)=\sin z$ as $w(x + iy) = \sin(x+iy) = \sin x \cosh y + i \cos x \sinh y$ then $\overline{w}(x + iy) = \sin x \cosh y - i \cos x \sinh y \neq w(x+iy) = \sin(x + iy)$ gives what I would expect to be the value of case 3 and a different answer then in the $z$-form where $w(z)=\overline{w}(z) = \sin z$. Also, he later appears to state that $\overline{w(z)} = \overline{w}(\overline{z})$ although it is not clear whether that is true for all analytic functions or just those that have the property of $w(z) = \overline{w}(z)$ like $\sin z$. My question is several-fold. Is the definition of conjugation given in 1 standard? What is its meaning? Also, is my interpretation of 3 correct and is $\overline{w(z)} = \overline{w}(\overline{z})$ the proper definition of 3. Addendum: The motivation behind understanding definition 1 is that Hamming uses it in the proof that analytic complex functions have zeros that are conjugate pairs if the function is real over the real domain. He states without proof that if $w(z)$ is real for real $z$ then $w(z)=\overline{w}(z)$. He then provides the following proof for the above. $w(a+bi)=0=\overline{w(a+bi)}=\overline{w}(a-bi)=w(a-bi)$","['complex-numbers', 'complex-analysis']"
186998,Subgroups of $\Bbb{R}^n$ that are closed and discrete,"I am trying to prove that every closed discrete subgroup of $\Bbb{R}^n$ under addition is a free abelian group of finite rank. I have tried to do this by induction on the dimension $n$. Base case: We claim that a subgroup $H \subset \Bbb{R}$ has a least positive element $\alpha$. The completeness axiom tells us that there is a least positive real number $\alpha$ among all positive reals in $H$, and being closed implies that $\alpha \in H$. Now assume the inductive hypothesis and consider a closed, discrete subgroup $H$ of $\Bbb{R}^n$. Choose some element $x \in H$ of least positive distance to the origin. This can be done because otherwise we get a contradiction like the above. Now let $L$ be the subspace spanned by $x$ and write $\Bbb{R}^n = L \oplus W$ for some complement $W$. Consider the projection (as a linear map) $p : \Bbb{R}^n \to W$.  Now this is also a group homomorphism and so we have $$\textrm{Im} (H) \subset W$$ being a subgroup and so by the induction hypothesis is isomorphic to $\Bbb{Z}^l$ for some $1 \leq l \leq n-1$. However from here I am having some trouble concluding that $H$ itself must be isomorphic to $\Bbb{Z}^l \oplus \Bbb{Z}x$. If I know the existence of a map $l : \textrm{Im}(H) \to H$ such that $$p \circ l = \textrm{id}_{\textrm{Im}(H)}$$ then by the splitting lemma I can conclude my problem. However, I don't have such a map so how do I conclude the problem? Please do not post complete solutions. Thanks. Edit: Perhaps I should add some context. I am trying to conclude that the kernel of the exponential map $\exp : \mathfrak{g} \to G$ where $G$ is a connected abelian matrix Lie group is a free abelian group of finite rank. We know that the assumptions on our matrix Lie group mean that $\exp$ is a group homomorphism, so that $\ker \exp$ is a subgroup of the Lie algebra $\mathfrak{g}$. Edit: The problem is reduced to showing that the image of $H$ under the projection is indeed discrete, because otherwise we cannot apply the inductive hypothesis.","['lie-groups', 'lie-algebras', 'group-theory', 'abelian-groups']"
187001,$G/H$ is a finite group so $G\cong\mathbb Z$,"Let $G$ is an abelian infinte group such that for all nontrivial subgroups $H$ $$\forall H\leq G, \left|\frac{G}{H}\right|<\infty$$ Prove that $G\cong\mathbb Z$. What I have done: Clearly, it is enough to show that $G$ is a cyclic group. Moreover, I see as $G/H$ is a finite group for all subgroups $H$, then $G$ cannot have any elements with the finite order. This means to me that if $H \leq G$ and $H$ is cyclic then $H$ cannot be finite. Help your friend for the rest. Thanks. :)","['finite-groups', 'group-theory']"
187003,Primality using $\Gamma(x)$,"Wilson's theorem states $n \in \mathbb N$ is prime iff $(n-1)! \equiv -1\pmod n$. The $\Gamma$-function extends the usual factorial to complex numbers. What are the complex numbers such that $\Gamma(z)+1 = nz$ , $n \in \mathbb Z$? Eisenstein or Gaussian primes don't necessarily satisfy the requirement, take for 
example $2+\omega$ and $5+12i$ respectively. What I've tried: Let $z=a+ib$. From the definition of the $\Gamma$-function, we have $\Gamma(z)=(z-1)\Gamma(z-1)$. $\Gamma(a+ib)=(a-1+ib)\Gamma(a-1+ib)$ $=(a-1+ib)(a-2+ib)\Gamma(a-2+ib)$ $=(a-1+ib)(a-2+ib)\cdots(a-k+ib)\Gamma(a-k+ib)$ $=\Gamma(ib)\prod_{k=0}^{a-1}{(k+ib)}$ Now, turning to the imaginary-$\Gamma$, a brick wall I ran into... $$\Gamma(ib)= \int_0^{\infty}\frac{t^{-1+ib}}{e^t}\mathrm{d}t$$ ... and cannot evaluate. Questions How do we evaluate $\Gamma(a+ib)$? How should we go about solving for $z$ once 1. is done? Computational 'evidence' Wolfram|Alpha thinks these $z$ exist, infact they seem plentiful. I'm not sure if approximation is muddling the results, but I doubt it. I'm using solve Gamma(a+ib) + 1= n(a+ib) and plugging in values of $a,b,n$.","['complex-numbers', 'gamma-function', 'complex-analysis']"
187029,The first column of the $n$th power for a triangular matrix,"I have found a interesting thing but I cannot prove it. Given $k_i$ are positive for any $i\geq1$, and we have $M+1$ by $M+1$ matrix $A$, which is
$$
A=\left[\begin{array}{ccccc}
0\\
k_{1} & 0\\
k_{2} & \frac{1}{2}k_{1} & 0\\
\vdots &  &  & \ddots\\
k_{M} & \frac{M-1}{M}k_{M-1} & \cdots & \frac{1}{M}k_{1} & 0
\end{array}\right] 
$$
I found that the first column of $n!A^{n}$ is always equal with the first column of $B^{n}$ for any $n\in\mathbb{N}$, where
$$
B=\left[\begin{array}{ccccc}
0\\
k_{1} & 0\\
k_{2} & k_{1} & 0\\
\vdots &  &  & \ddots\\
k_{M} & k_{M-1} & \cdots & k_{1} & 0
\end{array}\right] 
$$
Can you help me to prove it? Thanks in advance. The following is a reply to a comment from @GerryMyerson. (Updated on Aug. 27.) To @GerryMyerson. Here is the problem when I use the induction: Use induction method. First, it is obvious that when $n=0$  and $1$, the equality is hold for any $M\in\mathbb{N}$. Then, we assume when $n=N>1$, the equality is hold, and the matrix is
$$
N!A_{M+1}^{N}=B_{M+1}^{N}=\left[\begin{array}{ccccccc}
0\\
\vdots\\
0\\
t_{1} & 0\\
t_{2} & s & 0\\
\vdots & \vdots &  & \ddots\\
t_{M+1-N} & u & \cdots & v & 0 & \cdots & 0
\end{array}\right]. 
$$
Here, we only focus on the first column. When $n=N+1$, the left side is
$$
\left(N+1\right)A_{M+1}\times\left(N!A_{M+1}^{N}\right)=\left(N+1\right)\left[\begin{array}{ccccccc}
0\\
k_{1} & 0\\
k_{2} & \frac{1}{2}k_{1} & 0\\
\vdots &  &  & \ddots\\
k_{i} & \frac{i-1}{i}k_{i-1} & \cdots & \frac{1}{i}k_{1} & 0\\
\vdots &  &  &  &  & \ddots\\
k_{M} &  &  &  &  & \frac{1}{M}k_{1} & 0
\end{array}\right]\left[\begin{array}{ccccccc}
0\\
\vdots\\
0\\
t_{1} & 0\\
t_{2} & s & 0\\
\vdots & \vdots &  & \ddots\\
t_{M+1-N} & u & \cdots & v & 0 & \cdots & 0
\end{array}\right], 
$$
then the $\left(i+1,1\right)$  entry is $\left(N+1\right)\left(\frac{i-N}{i}k_{i-n}t_{1}+\frac{i-N-1}{i}k_{i-N-1}t_{2}+\cdots+\frac{1}{i}k_{1}t_{i-N}\right)$. On the other side, the $\left(i+1,1\right)$ of $B$ entry is $k_{i-N}t_{1}+k_{i-N-1}t_{2}+\cdots k_{1}t_{i-N}$. Now, I cannot prove that these two are equal.","['factorial', 'matrices', 'linear-algebra', 'combinatorics']"
187058,Why should faithfully flat descent preserve so many properties?,"This question is based on the following proposition (EGA IV, 2.7.1) Let $f: X \rightarrow Y$ be a $S$-morphism of $S$-schemes, $g: S'\rightarrow S$ a faithfully flat and quasi-compact morphism. Denote $X \times_S S'$ by $X'$, and denote $Y \times_S S'$ by $Y.$ We have a natural morphism $f': X\rightarrow Y.$ Consider the following properties of morphisms: (i) separated (ii) quasiseparated (iii) locally of finite type (iv) locally of finite presentation (v) finite type (vi) finite presentation (vii) proper (viii) isomorphism (ix) monomorphism (x) open immersion (xi) quasi-compact immersion (xii) closed immersion (xiii) affine (xiv) quasi-affine (xv) finite (xvi) quasi-finite (xvii) entire (I'm not sure exactly what a ""morphisme entier"" is, but some reading of the french wikipedia gave me the impression that it's an integral morphism) If $P$ is one of the preceding properties, then $f$ has property $P$ if and only if $f'$ has property $P.$ My impression of the proof is that there are quite a few ingredients needed to prove this proposition, where different ingredients are needed for different properties, and it surprises me that there seems to be no ""unifying principle"" that covers all the proofs. What I would like is something like this theorem: Let $P$ be a property closed under composition and base change. Then, if $f:X \rightarrow Y$ is a map of $Z$-schemes, such that the structure morphism $X \rightarrow Z$ is in $P$ and the diagonal morphism of the structure morphism $Y \rightarrow Z$ is in $P$, then $f$ is in $P$. I like this theorem because it explicitly states the conditions the property needs to satisfy, and the conditions are fairly loose. Then, you have essentially the same proof of this theorem for every such property $P$. Is there any such unification of the proofs for this result? To me, it seems like this result is some incredibly mysterious miracle. I would appreciate any intuition behind this result.","['algebraic-geometry', 'schemes']"
187066,Understanding Alexandroff compactification,"Is the Alexandroff one-point compactification of a locally compact Hausdorff space ($\mathbf{LCHaus}$) a functor to the category of compact Hausdorff spaces ($\mathbf{CHaus}$)? It seems to me that one has to consider only proper continuous maps as morphisms. If one does so, then the most natural definition for the induced map $\hat f\colon\hat X\to \hat Y$ seems to work (just send $x\mapsto f(x)$ and $\infty_X\mapsto \infty_Y$, continuity has to be checked only on open sets containing $\infty$) but a lot of interesting situations seem to be excluded: one hopes that (e.g.) a map $h\colon (0,1)\to \mathbb R$ such that $$\lim_{x\to 1^-}h(x)=\lim_{x\to 0^+}h(x)$$
admits an extension $\hat h\colon \widehat{(0,1)}\to \mathbb R$; but what if $h$ is not proper? It is ""outside"" the category I'm considering. So: How can one define suitable topological categories between which $\widehat{(-)}$ is a functor? Morally a compactification should be a left adjoint to an inclusion (in this case $\iota\colon \mathbf{CHaus}\hookrightarrow\mathbf{LCHaus}$), but even if it seems evident that (taking only proper maps) $$\hom_{\mathbf{CHaus}}(\hat X,Y)\cong \hom_{\mathbf{LCHaus}}(X,\iota Y)$$ the Alexandroff correspondence seems to be ill-behaved with respect to colimits... is there any hope to make $X\mapsto \hat X$ adjoint to something? Thanks for your attention","['general-topology', 'category-theory', 'compactness']"
187077,How to solve this limit related to series?,"How to solve the following limit?
$$\lim_{N\rightarrow+\infty}\frac{1}{N}\sum_{k=1}^{N-1}\left(\frac{k}{N}\right)^N$$","['sequences-and-series', 'limits']"
187091,Number of elements of order $7$ in a group of order $28$,"Given a group $G$ with order $28 = 2^2 \cdot 7$. Sylow-Theory implies that there is a exactly one $7$-Sylow-Subgroup of order $7$ in $G$, and $1$ or $7$; $2$-Sylow-Subgroups. Where to go from here concerning the number of elements of order $7$?","['finite-groups', 'group-theory', 'abstract-algebra']"
187093,Matrices whose condition number is $1$,"The condition number of the identity matrix $I$ always equals $1$ . Are there any other matrices that have a condition number equal to $1$ , but are neither the identity matrix nor $\lambda I$ (for any scalar $\lambda$ )? (because if $A$ is a matrix, then $\mbox{cond}(\lambda A) = \mbox{cond}(A)$ )","['matrices', 'linear-algebra', 'singular-values', 'condition-number']"
187097,Explicit counter-example to corona problem,"The corona problem is known to fail for the complex polydisk, for dimension greater than 2. Does anyone has an explicit example of such functions?","['several-complex-variables', 'examples-counterexamples', 'complex-analysis']"
187107,Calculate coordinates of 3rd point (vertex) of a scalene triangle if angles and sides are known.,"I am writing a program and I need to calculate the 3rd point of a triangle if the other two points, all sides and angles are known. A (6,14)
            ^
           / \
    14.14 /   \ 10.14
         /     \
        /       \
B (16,4)--------- C (x,y)
          10.98 A (6,14), B (16,4). Side AB is 14.14, AC is 10.14 and BC is 10.98
Angle A, B, C are 50, 45 and 85 degrees respectively... I want to calculate the position of C. I don't know which formula to use for this. Actually i am performing triangluation. I am calculating the position of an incoming object (C).","['trigonometry', 'triangles', 'triangulation']"
187110,Irreducible polynomial over field of order p,Let $p$ be a prime and $F=\mathbb{Z}/p\mathbb{Z}$ and $f(t)\in F[t]$ be an irreducible polynomial of degree $d$. I need to show that $f(t)$ divides $t^{(p^{n})}-t$ if and only if $d$ divides $n$.,"['ring-theory', 'finite-fields', 'abstract-algebra', 'polynomials']"
187147,convolution of a function with itself equals itself,"In a homework question,  I was asked to show: (1) in $L^1(R)$, if $f*f = f$, then $f$ must be a zero function. (2) In $L^2(R)$, find a function $f*f=f$. I don't know how to proceed. for (1), $f*f=f$ gives $\widehat{f*f}=\hat{f}$, which is equal to $\hat{f}\cdot\hat{f}=\hat{f}$, but this does not guarantee the result. I tried to prove by contradiction, no success. 
for (2), I don't know where to proceed. Is there any help that I could get? Thanks.","['convolution', 'examples-counterexamples', 'real-analysis', 'analysis', 'functional-analysis']"
187174,How not to prove the Riemann hypothesis,"I remember reading somewhere that there is a (probably a family of) quick false proof of the Riemann hypothesis that starts by using complex logarithms in a bad way, then does some elementary calculations and out pops the result. A perusal of the General Mathematics section of the arXiv also shows an abundance of presumably false proofs of the same. These tend to be somewhat... unclear and I'd prefer avoiding to look at them in any detail. Question: Can anyone describe a quick false proof of the Riemann hypothesis? Preferably I'd like one that has a very clear false step that misuses complex logarithms or $n$-th roots or some such gadget in an identifiable way. This is both because of my curiosity, and because I figure I could use it as an exercise problem for undergrads (if I ever end up teaching a course on complex analysis) to explain why they should be careful about taking logarithms.","['riemann-hypothesis', 'complex-analysis', 'fake-proofs']"
187197,Two players and two coins,"Two players are playing a game. The first player has unlimited gold coins of 2 types, $C_1=2\$$ and $C_2=5\$$. Each turn he chooses one of these coins and hides it in his hand. If the second player guesses correctly which type of coin the first player is hiding in his hand, he gets this coin; otherwise he loses $x$ cents. Find the largest integer $x$ for which the game is beneficial to the second player. I knew the answer, but I forget how we got it. I would appreciate it if someone would explain it for me. Thank you. Answer is x=316","['game-theory', 'probability']"
187211,Maximum intersecting subsets,There are n elements. What is the maximum number of subsets chosen at any one time so that every pair of subsets from collection intersect?,['combinatorics']
187214,"If $I$ is injective and $P$ is projective, is $\operatorname{Hom}(P,I)$ injective?","In Bourbaki's Algebra, there is the following exercise ($A$ is an arbitrary ring with unity): Suppose that $I$ is an $(A,A)$-bimodule which is injective as a left $A$-module and a right $A$-module and that for every (left or right) cyclic $A$-module $E\neq 0$, there exists a non-zero $A$-homomorphism of $E$ into $I$. Show that under these conditions, if $P$ is a left projective $A$-module, $\operatorname{Hom}_A(P,I)$ is a right injective $A$-module. The following straightforward proof does not use the condition on the cyclic modules, so there is probably something wrong with it, and I hope someone can point out the faulty step. $\operatorname{Hom}(P,I)$ can be seen as a right $A$-module in a canonical way, using the bimodule structure of $I$. To show that $\operatorname{Hom}(P,I)$ is injective, it suffices to show that, for any right $A$-module $N$ and a submodule $N'$ of $N$, the homomorphism $\Phi$ of $\operatorname{Hom}(N,\operatorname{Hom}(P,I))$ into $\operatorname{Hom}(N',\operatorname{Hom}(P,I))$ given by the restriction of mappings is surjective. Since $I$ is injective, the homomorphism of $\operatorname{Hom}(N,I)$ into $\operatorname{Hom}(N',I)$ given by restriction is surjective. Since $P$ is projective, the induced mapping $\Psi$ of $\operatorname{Hom}(P,\operatorname{Hom}(N,I))$ into $\operatorname{Hom}(P,\operatorname{Hom}(N',I))$ is surjective. But $\operatorname{Hom}(P,\operatorname{Hom}(N,I))$ is canonically isomorphic to $\operatorname{Hom}(N,\operatorname{Hom}(P,I))$, and likewise $\operatorname{Hom}(P,\operatorname{Hom}(N',I))$ to $\operatorname{Hom}(N',\operatorname{Hom}(P,I))$. $\Phi$ and $\Psi$ correspond to each other via these isomorphisms, so $\Phi$ must be surjective as well. Where is the mistake? If you think there is none, please tell me so.","['modules', 'category-theory', 'abstract-algebra']"
187218,What is the intution behind the ping-pong lemma?,Is the ping-pong lemma a difficult characterization of free groups? Or is it just me? Does someone have a nice intuition about its idea or should I carry on staring at the statement?,"['geometric-group-theory', 'group-theory']"
187239,About algebraic integer and algebraic number,Let $u$ a algebraic number. Prove that exists a natural number $n\in \mathbb{Z}$ such that $nu$ is a algebraic integer If $u$ is algebraic integer  and $n\in \mathbb{Z}$ then $u+n$ and $nu$ are algebraic integers. I don't see how can I start. Remember that: $u$ is algebraic integer if it is a root by a monic polynomial $f(x)\in \mathbb{Z}[x]$.,['abstract-algebra']
187277,What are applications of Lagrange's identity?,"I recently proved for homework the following identity on $\mathbb{C}$: if $a_1, \ldots , a_n, b_1, \ldots, b_n\in\mathbb{C}$, then
$$
\left|\sum_{i=1}^na_ib_i\right|^2 = \left(\sum_{i=1}^n|a_i|^2\right)\left(\sum_{i=1}^n|b_i|^2\right) - \sum_{1\leq i<j\leq n} |a_i\overline{b_j}-a_j\overline{b_i}|^2.
$$
This identity is called Lagrange's identity. I was wondering what are some applications of this identity. I know that one can infer Cauchy's inequality, but I was wondering if there were any other uses of it. Thanks!!","['applications', 'linear-algebra', 'real-analysis', 'complex-numbers', 'complex-analysis']"
187284,How to evaluate $ \int_c (\sin z)/ z^6$?,"Evaluate:
$$ \int_c {\sin z\over z^6} \, dz$$
Where, $c$ is a circle of radius $2, |z| = 2$. 
Don't understand why ${\pi i \over 5!} $ (Answer from book).
$$ {\sin z \over z^6} = {1 \over z^6}\left [ z-\frac{z^3}{6}+\frac{z^5}{120}-\frac{z^7}{5040}+\frac{z^9}{362880}-\frac{z^{11}}{39916800}+O(z)^{12} \right ]$$ What happens to $ \int_c {1 \over z^5}\,dz$ and $- {1\over 3!} \int_c {1 \over z^3} \, dz $?","['integration', 'complex-analysis']"
187289,Minimal condition for the existence of a limit in 2 dimensions,"Let $f$ be a continuous function on $\mathbb R^2\setminus \{0\}$. In a standard multivariable calculus course, one learns that even if $x\mapsto f(x,0)$ and $y\mapsto f(0,y)$ extend to be continuous at 0,  $f$ need not do so. Indeed, it is not even sufficient to have $t\mapsto f(at, bt)$ continuous at 0 (with the same value there) for all $a,b\in \mathbb R$. This can be seen by taking $$f(x,y) = \frac{x^2 - y^3}{x^2+y^3},$$ which satisfies $f(at, bt) \rightarrow 1$ as $t\rightarrow 0$ for any $a$ and $b$, but $f(t^{3/2}, t) \rightarrow 0$. I am wondering if there is a sufficient condition on the limits of compositions of $f$ with one-variable functions which guarantees that $f$ extends to be continuous at 0. Is it sufficient to have $f(p(t), q(t))$ continuous at 0 (with the same value at 0) for all polynomials $p(t)$ and $q(t)$ which vanish at the origin? What about for all analytic functions?","['multivariable-calculus', 'real-analysis']"
187290,The difference between convergence in $L^{\infty}$ and almost uniformly,"I am reading these notes http://terrytao.wordpress.com/2010/10/02/245a-notes-4-modes-of-convergence/ by Terry Tao. I have a question about the difference between convergence in $L^{\infty}$ and convergence almost uniformly. Is the difference that convergence almost uniformly guarantees that you can get uniform convergence outside a set of arbitrarily small but still positive measure, while convergence $L^{\infty}$ gets uniform convergence outside a set of exactly measure zero? Formal definitions follow to make ideas precise. Let $(X, \mathcal{M}, \mu)$ be a measure space. Let $f, f_1, f_2, \ldots$ be a measurable functions. We say that $f_n \to f$ in $L^{\infty}$ if for all $\varepsilon > 0$ there is an $N_{\varepsilon}$ such that $|f_n(x) - f(x)| \leq \varepsilon$ $\mu$--a.e. when $n \geq N_{\varepsilon}$. We say that $f_n \to f$ almost uniformly if for all $\varepsilon > 0$ there is a set $E \in \mathcal{M}$ with $\mu(E) \leq \varepsilon$ such that $f_n \to f$ uniformly on $E^c$. I.e., for each $\delta > 0$ there is an $N_{\delta}$ such that $|f_n(x) - f(x)| \leq \delta$ for all $x \in E^c$ when $n \geq N_{\delta}$.","['measure-theory', 'real-analysis']"
187295,A Geometric Proof of $\zeta(2)=\frac{\pi^2}6$? (and other integer inputs for the Zeta),"Is there a known geometric proof for this famous problem? $$\zeta(2)=\sum_{n=1}^\infty n^{-2}=\frac16\pi^2$$ Moreover we can consider possibilities of geometric proofs of the following identity for positive even inputs of the Zeta function:
$$ \zeta(2n)=(-1)^{n+1} \frac{B_{2n}(2\pi)^{2n}}{2(2n)!}$$
and negative inputs:
$$ \zeta(-n)=-\frac{B_{n+1}}{n+1}$$","['alternative-proof', 'sequences-and-series', 'zeta-functions']"
187304,Why are two vectors that are parallel equivalent?,"Why are two parallel vectors with the same magnitude equivalent? Why is their start point irrelevant? How can a vector starting at $\,(0, -10)\,$ going to $\,(10, 0)\,$ be the same as 
a vector starting at $\,(10, 10)\,$ and going to $\,(20, 20)\,$?","['geometry', 'calculus', 'multivariable-calculus']"
187315,Looking for a definitive source about Dirichlet finally proving the Unit Theorem in the Sistine Chapel,"There is a remark one can find in various books or survey articles (e.g., page 49 of Helmut Koch's ""Number Theory: Algebraic Numbers and Algebraic Functions"") saying Dirichlet figured out a proof of the unit theorem while listening to an Easter concert in the Sistine Chapel. My question is: what is the evidence for this story? Today I did an internet search and found that Kummer wrote on p. 343 of volume 2 of  Dirichlet's collected works that Dirichlet could work on math in all kinds of situations, and then Kummer says ""Als Beispiel hierfür kann ich anführen, dass er die Lösung eines schwierigen Problems der Zahlentheorie, womit er sich längere Zeit vergeblich bemüht hatte, in der Sixtinischen Kapelle in Rom ergründet hat, während des Anhörens der Ostermusik, die in derselben aufgeführt zu werden pflegt"" (translation: ""As an example I can say that he found the solution to a difficult problem in number theory, which he had worked on for a considerable amount of time without success, in the Sistine Chapel in Rome while he was listening to the Easter music that tends to be played there."") Notice Kummer does not say precisely what the ""difficult problem"" was. Maybe it is just an oral tradition that the problem is the unit theorem, but I would like a more definitive source. I don't read German well, but if you do then Kummer's essay on Dirichlet can be read online. It starts on http://archive.org/stream/glejeunedirichl00dirigoog#page/n323/mode/1up and page 343 is http://archive.org/stream/glejeunedirichl00dirigoog#page/n355/mode/1up .","['math-history', 'number-theory']"
187316,How can I describe the area between two ellipses?,"Given two ellipses that take up regions $E_1$ and $E_2$ in $\mathbb{R^2}$, with the following properties: Centers defined in the Cartesian coordinate system $(c_1, 0)$ for $E_1$ and $(c_2, 0)$ for $E_2$ such that $c_2>c_1$ Semi-diameters $x_1$ & $y_1$ for $E_1$ and $x_2$ & $y_2$ for $E_2$ such that the two ellipses intersect at exactly two points. Let the surface $\Sigma=E_1\cap E_2$ Describe $\Sigma$ with two parameters, $u$ and $v$, in the form $a\le u\le b$ and $f(u)\le v \le g(u)$ for $\left\{a,b:a,b\in\mathbb{R}\land a<b\right\}$, $f:\mathbb{R}\rightarrow\mathbb{R}$, $g:\mathbb{R}\rightarrow\mathbb{R}$, and $g(u)>f(u)$ over $[a,b]$.  In other words, please find the Lebesgue measure of the set of points that satisfy both of the aforementioned inequalities. - My work so far - The two equations for the ellipses are easy to find given the conditions.
$$
\frac{y^2}{y_1^2}+\frac{(x-c_1)^2}{x_1^2}=1
$$
$$
\frac{y^2}{y_2^2}+\frac{(x-c_2)^2}{x_2^2}=1
$$
After multiplying by a constant and subtracting one from another, I arrive at two solutions for $x$:
$$
x=\frac{b\pm \sqrt{b^2-4ac}}{2a}
$$
Where $a=\frac{y_1^2}{x_1^2}-\frac{y_2^2}{x_2^2}$, $b=\frac{y_2^2c_2}{x_2^2}-\frac{y_1^2c_1}{x_1^2}$, and $c=\frac{y_1^2c_1^2}{x_1^2}+\frac{y_2^2c_2^2}{x_2^2}$.  However, from the fact that $c_2>c_1$ and that the ellipses only intersect at two points, I suppose that the only value of $x$ must be the larger one.  I evaluated and checked the determinant with Mathematica , and it is not equal to 0.  How can I be sure to pick the right value of $x$? Even assuming I found the right value of $x$ and therefore have the intersection points $(x_0, y_0)$ and $(x_0, -y_0)$, with $y_0=\sqrt{y_2^2(1-\frac{(x_0-c_2)^2}{x_1^2})}$, I still have the problem of defining the intersecting area. If I have $u=y$ and $v=x(y)$, then I'm assuming I have a type II region, whereas I really have a type II region combined with two type I regions.  Pursuing that piecewise definition of $\Sigma$ is not optimal, since I would have to find what parts of the two curves break the horizontal line test, set up three different integrals, etc.  A potential workaround I see is converting to polar, but I don't know how to approach that. So, to summarize: If you could find a way to solve the original, that would be much appreciated. If you could tell me which $x$ value in the quadratic equation is $x_0$, please let me know. If you could elaborate a polar approach, I'd love to see it. EDIT As pointed out in the comments section below, the value $x_0$ must be only one of the following solutions $x=\frac{b\pm \sqrt{b^2-4ac}}{2a}$, since the $x$ value farther from $c_2$ will yield an imaginary $y_0$ value.  However, this means I cannot find $x$ symbolically.  If someone has a different approach to finding the intersection, please let me know.","['multivariable-calculus', 'algebra-precalculus']"
187324,Estimating the covariance matrix with a set of vectors for the Mahalanobis distance,"I am trying to figure out how to use the Mahalanobis distance still.  I am having trouble figuring out how to produce my own covariance matrix. I guess the relevant link is http://en.wikipedia.org/wiki/Estimation_of_covariance_matrices , but I am still stuck.  What I have is a set of points in space, every single point in my data set.  I want to find the distance between any two of them using this type of distance, but how do I form the covariance matrix? The webpage is not very clear to me.","['statistics', 'matrices']"
187326,How to prove a function is the Fourier transform of another $L^{1}$ function?,"If $m(\xi)$ satisfies $$D^{\alpha}m(\xi)\leq \frac{C}{(1+|\xi|)^{|\alpha|+1}}$$
then is $m$ a Fourier transform of a $L^{1}$ function? (Note that the Bernstein theorem can't be applied here, since $m(\xi)$ may not be in $H^{s}$, where $s>\frac{n}{2}$.) Generally, are there some simple ways to make sure that a given function belongs to $\mathcal{F}L^{1}$?","['fourier-analysis', 'real-analysis']"
187327,Measure-driven differential equations,"Background: I need some help to understand the concept behind measure-driven differential equations. The solution of an ordinary differential equation is continuous. In order to describe discontinuous trajectories we use the concept of distributions (very well described in the book ""Functional Analysis"" by W. Rudin). In brief, every function $x:\Re\to\Omega\subseteq \Re^n$ that is locally integrable over the open set $\Omega$ is mapped to a functional $T_x:\mathcal{D}(\Re^n)\to\Re^n$ as follows: $$
T_x(\phi)= \int_\Omega x\phi d\mu
$$ where $\mathcal{D}(\Re^n)$ is the set of test functions (infinitely many times differentiable and with compact support). $\mu$ is the Lebesgue measure over $\Omega$ - meaning that the integration is carried out in the Lebesgue sense. Every distribution (i.e. a functional $T\in\mathcal{D}^\star(\Omega)$) has a derivative given by: $$
(DT)(\phi)=-T(D\phi)
$$ In that sense we can construct generalized differential equations that look like: $$
DT=g(T)
$$ Using this framework we can describe solutions that encounter jumps such as impulsive differential equations. This is accomplished using the Dirac functional $\delta(\phi)=\phi(0)$. (I don't want to go into much detail to keep the question short). The problem: I recently stumbled on a thing called ""Measure-driven differential equations"". These have the form: $$
dx=f(x(t),u(t))dt+g(x(t))d\mu(t)
$$ where $\mu:\mathcal{B}([t_0,t_1])\to\Re_+$ is a positive measure with the property $\mu(A)\in K$ for all $A\subseteq [t_0,t_1]$ where $K$ is compact. $u$ and $\mu$ here serve as external ""signals"". The solution of such an equation is reportedly: $$
x(t)=x(t_0) + \int_{t_0}^t f(x(s),u(s))ds + \int_{[t_0,t]}g(x(s))\mu(ds)
$$ (see this article for example). The questions: (i) I'm a bit puzzled with the notation $d\mu(t)$ and $\mu(ds)$. Can someone elaborate a bit on that? Since $\mu$ is a measure, what exactly is the meaning of $\mu(ds)$? (ii) Is there any advantage from using measures instead of distributions to describe phenomena with discontinuous trajectories? (iii) I would appreciate some reference (preferably a book) to get started with these things.","['ordinary-differential-equations', 'measure-theory', 'reference-request']"
187336,How to strictly mathematically prove that definition is wrong?,"I started to learn calculus by myself. First chapter of my textbook is about the limit of the sequence. I did all exercises in my textbook except one problem. There is some special problems in the end of this chapter: you need to find a mistake in the given definition. The last one is very weird. I don't understand how to strictly mathematically prove why this definition is wrong: $L(a_n)$ - length of the curve $a_n$. $D(a_n(P),S_{AB})$ - distance between point $P \in a_n$ and segment $S_{AB}$ (perpendicular from the point P to the segment $S_{AB}$). Definition: Sequence of smooth continuous curves  $a_n$ is called an approximation for segment $S_{AB}$ if: All curves $a_n$ begins at point A and ends at B. For any $m<n, \{m,n\} \in \mathbb{N}, \ L(a_m) \geq L(a_n)$. For each $\epsilon >0$ there exists a natural number $N$ such that, for every $n\geq N$, for every points $P \in a_n$  we have $D(a_n(P),S_{AB})<\epsilon$. If (1-3) true then the sequence of smooth continuous curves $a_n$ is an approximation for a segment $S_{AB}$, their length tends to the limit L, which is length of a segment $S_{AB}$. It is definitely wrong. With this definition we can prove that $5=4$. May be we should change in 2) that $L(a_m) > L(a_n)$? Or this is unfixable?","['calculus', 'limits']"
187337,An operator $T:\mathbb{R}^4\to \mathbb{R}^4$ such that $T$ has no (real) eigenvalues.,Give an example of an operator $T:\mathbb{R}^4\to \mathbb{R}^4$ such that $T$ has no (real) eigenvalues. How can I find this operator? Thanks for your help.,['linear-algebra']
187343,Inequality with two absolute values: $ |x-3|-|x-4|<x $,"I'm new here, and I was wondering if any of you could help me out with this little problem that is already getting on my nerves since I've been trying to solve it for hours. Studying for my next test on inequalities with absolute values, I found this one: $$ |x-3|-|x-4|<x $$
(I precisely found the above inequality on this website, here to be precise , but, the problem is that when I try to solve it, my answer won't be $(-1,+\infty)$, but $(1,7)$. I took the same  inequalities that the asker's teacher had given to him and then put them on a number line, and my result was definitely not $(-1,+\infty)$ Here are the inequalities: 
$$ x−3 < x−4 +x $$ 
$$ x−3 < −(x−4) +x $$
$$ −(x−3)<−(x−4)+x $$ And here are my answers respectively:
$$ x>1, \quad x>-1, \quad x<7 $$ I will really appreciate if anyone could help me out, because I'm already stressed dealing with this problem, that by the way, it is not demanding that I solve it, but you know, why not?","['inequality', 'absolute-value', 'algebra-precalculus']"
187353,Why is pi used to represent prime numbers here?,"I've just begun the ""Concrete Mathematics"" book by Knuth et al. In the first section about sums (and I apologise if this is really trivial, but I'm new and struggling a little);
and they show this as one of the examples: $\sum_{k = 1}^{\pi(N)}  \frac{1}{p_{k}}$ ""This is the sum of all reciprocals of prime numbers between 1 and N."" If you didn't have the explanation here, how would you know that N is supposed to be a prime number? Is it because of the pi symbol? Also, does pi often represent a number as prime?",['discrete-mathematics']
187359,"Is the ""Constant Rank Theorem"" the same as the ""Domain Straightening Theorem""? Which theorem is which?","Wikipedia says that the inverse function theorem is a special case of the "" constant rank theorem "". I'm pretty sure this is supposed to be the same theorem as the ""Rank Theorem"" on p. 47 of Boothby (especially because the wikipedia article also footnotes to Boothby...), and then in Boothby, he says in a footnote that it is also known as the ""Straightening Out Theorem."" Wikipedia also has an article on a "" Domain Straightening Theorem "". Which seems vaguely related but does not explicitly discuss anything about rank. Could someone please help me sort out which theorem is which? My main goal is to find a more in-depth discussion of the ""Constant Rank Theorem"" (or whatever the true general case of the IFT is) (reading suggestions welcome!), but I would also like to know which of these names refers to the same theorem, and which doesn't.","['multivariable-calculus', 'coordinate-systems', 'calculus', 'differential-geometry', 'linear-algebra']"
187371,Is there a standard name for a category all of whose contravariant hom functors are sheaves?,"What prompted this question is the definition of a 
pseudogroup in nlab : Given a X a topological space. Then a pseudogroup is a subgroupoid of the groupoid of transitions between open sets in X, contains the groupoid of identity transitions, and satisfies a sheaf condition. (Pseudogroups of continuous/smooth transitions are used to define the atlases for manifolds of the respective kind). It seems to me a pseudogroup is morally a groupoid G that satisfies the sheaf condition for each presheaf G[-,V] for V an object of G.","['category-theory', 'differential-geometry']"
187401,Morse theory and homology of an algebraic surface (example),"Let $T_n$ denote the $n$-th Chebyshev polynomial and define $f_n(x,y,z)\!:=\!T_n(x)\!+\!T_n(y)\!+\!T_n(z)$ and $$Z_n:=\mathcal{Z}(f_n) \subseteq \mathbb{R}^3,$$
the Bachoff-Chmutov surface , where in general, $\mathcal{Z}(f_1,\ldots,f_k)$ denotes the zero set of polynomials $f_1,\ldots,f_k$, i.e. $\{(x,y,z)\!\in\!\mathbb{R}^3;f_1(x,y,z)\!=\!\ldots\!=\!f_k(x,y,z)\!=\!0\}$. Let us prove, that this is a surface. By the implicit function theorem , it suffices to prove that the points, where $[D_x{f_n},D_y{f_n},D_z{f_n}]$ is zero, do not lie in $Z_n$ (here $D_x$ is just the partial derivative). This is quivalent to showing that the set $$\mathcal{Z}(f_n,D_xf_n,D_yf_n,D_zf_n)=\mathcal{Z}(T_n(x)\!+\!T_n(y)\!+\!T_n(z),D_xT_n(x),D_yT_n(y),D_zT_n(z))$$ is empty. This can be done by using (from wiki page) $D_xT_n(x)\!=\!nU_{n-1}(x)$ and Pell's equation $T_n(x)^2\!-\!(x^2\!-\!1)U_{n-1}(x)^2\!=\!1$, to obtain $\mathcal{Z}(1\!+\!1\!+\!1)\!=\!\emptyset$. Let us observe the height function $Z_n\!\rightarrow\!\mathbb{R},\,(x,y,z)\!\mapsto\!ax\!+\!by\!+\!cz\!=\![a,b,c][x,y,z]^t$. It is linear, so its derivative is $[a,b,c]\!:T_pZ_n\!\rightarrow\!T_p\mathbb{R}\!=\!\mathbb{R}$. Its critical points are therefore those, where the tangent plane $T_pZ_n$ has normal $[a,b,c]$. But the tangent plane of $\mathcal{Z}(f)$ always has normal $[D_xf,D_yf,D_zf]$. Thus the critical points of our height function are those $x,y,z$ where $[D_xf_n,D_yf_n,D_zf_n]=[a,b,c]$, i.e. the critical points are $\mathcal{Z}(f_n,T_n(x)\!-\!a,T_n(y)\!-\!b,T_n(z)\!-\!c)$. Now I don't know how to check if these critical points are nondegenerate. I don't even have local parametrizations to work with. Question: How can one calculate the Euler characteristic $\chi(Z_n)$ by using elementary methods from Morse theory (i.e. handle decomposition, Morse inequalities, Morse complex)?","['differential-geometry', 'algebraic-geometry', 'algebraic-topology', 'differential-topology', 'surfaces']"
187402,Existence of an Infinite Length Path,"I came across the following simple definition A path $\gamma$ in $\mathbb{R}^n$ that connects the point $a \in \mathbb{R}^n$ to the point $b \in \mathbb{R}^n$, is a continuous $\gamma : [0, 1] \to \mathbb{R}^n$ such that $\gamma(0) = a$ and $\gamma(1) = b$. We denote by $\ell(\gamma)$ the (Euclidean) length of $\gamma$. $\ell(\gamma)$ is always defined and is either a non-negative realnumber or $\infty$. However, I cannot seem to think of a path, defined in this manner (specifically, where the domain is compact), the length of which is infinite.
Can anyone provide an example ?",['multivariable-calculus']
187419,When are the limit operations commutative?,"I'll come up with a question that has bothered me for a long period of time. The question seems relatively simple, but I personally didn't manage to find an answer to it. In many cases I met problems where I had to deal with multiple limits and I've never known what is the rule that allows the limits to be interchanged. I'm referring at the situation when the limit operations are commutative. For some problems this would be of enormous help since the problem you have to deal with becomes much more easier when you change the limit operations. Could you help here? 
Thanks.","['faq', 'calculus', 'real-analysis', 'limits']"
187421,An inequality involving Stirling numbers of the second kind,"The task is to prove the following inequality: $\begin{Bmatrix} mn\\  n \end{Bmatrix} \geqslant \frac{(mn)!}{(m!)^nn!}$ , where $m, n \in \mathbb{N_+}$ and to determine when the equality holds. A simple substitution shows that the two sides are equal for $(m, n) = (1, 1) \wedge (m, n) = (1, n)$, but this observation doesn't bring me closer to the solution. The identities I know (which could help simplify any side of the inequality) don't seem of much use here (at least I don't see how to apply them), nor can I think of a good combinatorial approach. Any idea how to tackle this problem?","['stirling-numbers', 'discrete-mathematics', 'inequality', 'combinatorics']"
187432,Evaluate the integral $ \int_{-\infty}^{\infty} {\sin ^2 (x) \over x^2 (x^2 + 1)}\:dx$,"Can we evaluate the integral using Jordan lemma ?
$$ \int_{-\infty}^{\infty} {\sin ^2 (x) \over x^2 (x^2 + 1)}\:dx$$ What de we do if removeable singularity occurs at the path of integration?","['integration', 'complex-analysis']"
187442,Cube with the product uniform measure,"Let $([0,1],\mathscr B([0,1]),\lambda)$ be the probability space where $\lambda$ is th Lebesgue measure and $\mathscr B([0,1])$ is the Borel $\sigma$-algebra of the unit interval $[0,1]$. Let us define a new probability space $(\Omega,\mathscr F,\mathsf P)$ where
$$
  \Omega = [0,1]^\mathbb N
$$
and $\mathscr F$ is its product $\sigma$-algebra, and $\mathsf P$ is its product measure. I wonder if there is a standard name either for the measurable space $(\Omega,\mathscr F)$ - something like Hilber cube -  or even for the whole probability space $(\Omega,\mathscr F,\mathsf P)$.","['probability-theory', 'measure-theory', 'terminology']"
187450,"How is it that $\tan(A +B) = \frac{\tan A + \tan B}{1-\tan A\tan B}$ for all angles, even though the derivation holds only for $\cos A\cos B\neq 0$?","How is it that
$$\tan(A +B) = \frac{\tan A + \tan B}{1-\tan A\tan B}$$ for any value of $A$, $B$? I have doubts about this since we arrive at this by dividing the numerator and denominator of $$\frac{\sin(A+B)}{\cos(A+B)}$$ by $\cos A \cdot \cos B$, which can only be done when $\cos A\cdot\cos B$ is not equal to zero.",['trigonometry']
187451,Question about 8.4 in Humphreys,"I am reading section 8.4 in Humphreys' book Introduction to Lie Algebras and Representation Theory. He is showing that the only scalar multiples of a root are 1 and -1, but I have trouble understanding his reasoning: He considers the direct sum $M=\bigoplus_{c\in\mathbb{F}} L_{c\alpha}$ for some fixed root $\alpha$. He shows that the only even weights of $h_\alpha$ on $M$ are 0, 2 and -2. Then, he says that this proves that twice a root is never a root. Why is that true? The function $\frac 12\alpha$ could still be a root.","['linear-algebra', 'representation-theory', 'lie-algebras']"
187453,Variants of isotopy extensions,"I am interested in slight variations of the usual isotopy extension theorems. In short, my question is the following : Can one extend isotopies of $C \subseteq M$, where $C$ is compact and $M$ is a manifold with boundary ? Now to the details. The usual reference for these is the book Differential Topology by Hirsch, where I found the following theorems : 1.3 Let $V \subseteq M$ be a compact submanifold and $F:V \times I \rightarrow M$ an isotopy of $V$. If either $F(V \times I) \subseteq \partial M$ or $F(V \times I) \subseteq M \setminus \partial M$, then $F$ extends to a diffeotopy with compact support. 1.4 Let $U \subseteq M$ be an open set and $A \subseteq U$ a compact set. Let $F: U\times I \rightarrow M$ be an isotopy of $U$ such that $\widehat{F}(U \times I) \subseteq M \times I$ is open. Then there is a diffeotopy of $M$ having compact support, which agrees with $F$ on a neighborhood of $A \times I$. I am quite confused by both the statements and the proofs. Here are my questions : Is the 'submanifold' hypothesis in Theorem 1.3 really needed, or can one relax it to any compact set ? More specifically, I am looking to apply it to a graph embedded on a surface, is there any obstruction against it ? I took a close look at the proof and did not see any trouble with that, but I'm puzzled since every reference I found seems to include this hypothesis as well. A way to obtain the result for any compact set would be to take an open tubular neighborhood and apply Theorem 1.4, but does it hold that any isotopy of a compact set extends to its open tubular neighborhood ? This seems obvious but I cannot find a straightforward proof. Does Theorem 1.4 hold for manifolds with boundary ? Looking at the proof, Hirsch does not seem to take the precautions he took in Theorem 1.3 to avoid getting an 'isotopy' that moves the boundary to the interior of the manifold. More generally, both results emane from Theorem 1.1 which does not seem to hold for manifolds with boundaries (but nothing is explicitly written), because integrating a vector field with vectors on the boundary pointing towards the interior does not give a homeomorphism; so I am a bit worried. And a last question, a bit disconnected from the others : These proofs use vector fields, and as such we need smooth isotopies. If two sets included on a surface (say for instance graphs) are isotopic, are they smoothly isotopic ? The usual way to tackle these problems is to use a Theorem by Munkres stating that homeomorphisms of a surface are homotopic to diffeomorphisms, but it does not seem to do the job here..? Thanks for any hint/answer !","['differential-topology', 'differential-geometry']"
187456,sufficient conditions for linear combinations of indefinite matrices to be indefinite,"In general, sum of indefinite matrices need not be indefinite. For example, $A=\begin{pmatrix}2&0\\0&-2\end{pmatrix}$, and $B=\begin{pmatrix}-1&0\\0&1\end{pmatrix}$ are indefinite matrices but matrix $A+B$ is positive definite. Do we have any sufficient conditions for the sum of general $n$th order indefinite matrices to be indefinite as well? Do we have conditions for any linear combination of those matrices to be indefinite as well? For example, the matrices $\begin{pmatrix}-1&0&0\\0&1&0\\0&0&1\end{pmatrix}$ and $\begin{pmatrix}1&0&0\\0&1&0\\0&0&-5\end{pmatrix}$ give indefinite matrix for any of its linear combinations.","['matrices', 'linear-algebra']"
187460,Showing that the pull-back of the Euclidean metric by an entire function is a metric,"Suppose $f$ is a non-constant entire function. Define
$$ d_f(a,b) = \inf_{\gamma} \ell(f\circ \gamma), $$
where $a,b \in \mathbb{C}$, $\ell$ is the euclidean length, and $\gamma$ is a path connecting $a$ and $b$.
I'm trying to prove $d_f$ is a metric. I'm having trouble showing that $$ a\ne b  \, \Longrightarrow \, d_f(a,b) \ne 0 .$$
Now if $f(a)\ne f(b)$, this is clear. If, however, $f(a)=f(b)$, is it not possible that there are paths, $\gamma_n$, connecting $a$ and $b$, such that $\ell(f\circ \gamma_n)$ approches $0$ ?",['complex-analysis']
187471,The set of all sets of the universe?,"I can't understand Russell's paradox. What I understand is that Russell's paradox arises because the set of all sets that are members of themselves is empty. That it's impossible to find a set that's a member of itself, but one can define the set of all sets of the universe that clearly contain itself. Does it mean that there is no set of all sets of the universe? Please, make answers as simple as possible, I'm nearly ignorant in set theory.",['elementary-set-theory']
187476,Strictly increasing function,"If $f(x)$ is a continuous function on $\mathbb R$, and $|f(-x)|< |f(x)|$ for all $x>0$. Does it imply that $|f(x)|$ is strictly increasing on $(0,\infty)$? I tried to use the definition: let $a,b \in (0,\infty)$ with $a<b$, we need to show that $|f(a)|<|f(b)|$. We have 
$|f(-a)|< |f(a)|$ and $|f(-b)|< |f(b)|$, and I don't know how to proceed!","['calculus', 'real-analysis']"
187486,$p\mid [G:H]$ then $p\mid [N_G(H):H]$,"I encountered the following problem for the first time. I sketched a proof for it. I will be thankful if I know it is correct or not. Thanks. $p$ is a prime and $H$ is a $p$-subgroup of a finite group $G$ such that $p\mid [G:H]$ . Prove that $p\mid [N_G(H):H]$. I assume $|G|=p^\alpha m, (p,m)=1$ and $|H|=p^\beta, \beta\lneqq\alpha$. According to Sylow's theorem, there is a $p$-sylow subgroup of $G$ including $H$ as a subgroup, say $K$. I see that $H<K$ $\mathrm{so^{(1)}}$ one theorem tells me $H<N_G(H)$ or $p\mid [N_G(H):H]$. (1): Once $H<K$, then in agreement with a theorem, $H<N_K(H)$. But obviously, $N_K(H)\leq N_G(H)$ so $H<N_G(H)$ which means that $p\mid [N_G(H):H]$.","['finite-groups', 'group-theory']"
187493,How long does it take to consume the same amount of food,For a group of 32 students food lasts for 45 days. For how many days will the same food last for 72 students?,"['algebra-precalculus', 'word-problem']"
187497,Proof that determinant rank equals row/column rank,"Let $A$ be a $m \times n$ matrix with entries from some field $F$. Define the determinant rank of $A$ to be the largest possible size of a nonzero minor, i.e. the size of the largest invertible square submatrix of $A$. It is true that the determinant rank is equal to the rank of a matrix, which we define to be the dimension of the row/column space. It's not difficult to see that $\text{rank} \geq \text{determinant rank}$. If some submatrix of $A$ is invertible, then its columns/rows are linearly indepedent, which implies that the corresponding rows/columns of $A$ are also linearly indepedent. Is there a nice proof for the converse?","['matrices', 'linear-algebra', 'matrix-rank', 'determinant']"
187503,proof using (fixed point theorem),"I am seeking to solve for a Nash equilibrium in pure strategies $(d_2,d_2)$ involving two players, $1$ and $2$. Given that $h'(.)$ is s strictly decreasing and continuous function, $\Phi(d_1-d_2)$ denoting a convolution function, and $F(.)$ denoting a CDF, I want to prove for existence and uniqueness of equilibrium. My guess is that we we use a fixed point theorem to prove existence. The following is the first order condition for maximization. $$g_1(d_1) \equiv h'(d_1)-\gamma\Phi(d_1-d_2)-\eta(1-F(m-d_1))=0 \\
g_2(d_2)≡ h'(d_2)-\gamma[1-\Phi(d_1-d_2)]-\eta(1-F(m-d_2))=0 $$ Note that the parameters are all positive and $d_1$ & $d_2$ are continuous and $m$ is a constant.  I highly appreciate any suggestion towards the proof.","['statistics', 'game-theory', 'economics']"
187510,Eigenvalue of a polynomial evaluated in a operator,"Suppose $T:V\to V$, $p\in \mathcal{P}(\mathbb{C})$ (polynomials with complex coefficients), and $a\in \mathbb{C}$. Prove that $a$ is an eigenvalue of $p(T)$ if and only if $a=p(\lambda)$ for some eigenvalue $\lambda$ of $T$. I can prove: if $a=p(\lambda)$ then $a$ is a eigenvalue of $p(T)$ because:
$$Tv=\lambda v$$
$$T^kv=\lambda^k v$$
$$p(T)v=p(\lambda)v=av$$ But, how can I justify the other direction? Thanks for your help.",['linear-algebra']
187511,Favorite problems that lead to interesting diophantine equations?,"I am looking for interesting problems (in number theory, or otherwise) that lead to interesting diophantine equations. The solution to the problem may be known, or it may be open... I just care for connections between problems and equations that one can use to motivate the study of diophantine equations, arithmetic geometry, and so on. I am more interested in problems that can be stated in elementary terms (that an undergraduate can understand), but I'll be happy to see any problems that you think fit the bill. I'll start with one of my favorites: the congruent number problem leads to the study of elliptic curves of the form $y^2=x^3-n^2x$. PS: the problem does not need to be a famous problem, any problem that is interesting, cute, entertaining, and leads to a diophantine equation also works!","['diophantine-equations', 'number-theory']"
187525,Sufficiency to prove the convergence of a sequence using even and odd terms,"Given a sequence $a_{n}$, if I know that the sequence of even terms converges to the same limit as the subsequence of odd terms: $$\lim_{n\rightarrow\infty} a_{2n}=\lim_{n\to\infty} a_{2n-1}=L$$ Is this sufficient to prove that the $\lim_{n\to\infty}a_{n}=L$? If so, how can I make this more rigorous?  Is there a theorem I can state that covers this case?","['convergence-divergence', 'sequences-and-series', 'real-analysis', 'limits']"
187533,Convergence of test-functions is not induced by any metric.,"By $\mathcal{D}(\mathbb{R})$ we denote linear space of smooth compactly supported functions. We say that $\{\varphi_n:n\in\mathbb{N}\}\subset\mathcal{D}(\mathbb{R})$ converges to $\varphi\in\mathcal{D}(\mathbb{R})$ if for all $k\in\mathbb{Z}_+$ the sequence $\{\varphi_n^{(k)}:n\in\mathbb{N}\}$ uniformly converges to $\varphi^{(k)}$. there exist a compact $K\subset \mathbb{R}$ such that $\mathrm{supp}(\varphi_n)\subset K$ for all $n\in\mathbb{N}$. Could you give me a hint to prove the following well known fact. There is no metric $d$ on $\mathcal{D}(\mathbb{R})$ such that convergence described above is equivalent to convergence in metric space $(\mathcal{D}(\mathbb{R}), d)$.","['distribution-theory', 'functional-analysis']"
187541,Lecture notes for measure theoretic probability theory,"I'm looking for good lecture notes (or concise books) that develop probability theory from a measure theoretic point of view. In particular, I'm looking for a text where the measure theoretic part is developed only as far as needed for probability theory. (I'm not really interested in measure theory on its own.)","['probability-theory', 'reference-request']"
187563,Majority in Parliament Problem,"In the parliament of a certain country there are 201 seats, and 3 political parties. How many ways can these seats be divided among the parties such that no single party has a majority?
Is there any generalization of solution of this problem?",['combinatorics']
187564,"What is exactly ""Algebraic Dynamics""?","Could somebody please give a big picture description of what exactly is the object of study in the area of Algebraic Dynamics? Is it related to Dynamical Systems? If yes in what sense? Also, what is the main mathematical discipline underpinning Algebraic Dynamics? Is it algebraic geometry, differential geometry e.t.c.?","['dynamical-systems', 'arithmetic-dynamics', 'algebraic-geometry', 'algebraic-number-theory']"
187572,Clarification on proof: Order of left cosets equal,"There is a lemma that says that all left cosets $aH$ of a subgroup $H$ of a group $G$ have the same order. The proof given is as follows... The multiplication by $a \in G$ defines the map $H \rightarrow aH$ that sends $h\mapsto ah$. This map is bijective because its inverse is multiplication by $a^{-1}$. I don't quite understand the proof. Why does having a bijective map mean that all sets of left cosets have the same order?
Thank you",['abstract-algebra']
187577,Find distribution that has pdf $\frac{1}{\sqrt{\pi}}t^{-1/2} e^{-t}$ on the positive reals.,"Is there a well-known prob. distribution (or a combination thereof) that has pdf: $$\frac{1}{\sqrt{\pi}}t^{-1/2} e^{-t}$$
on $t \ge 0$ and $0$ everywhere else.",['probability']
187595,"$\{(x,y)\!\in\!\mathbb{B}^n; -\varepsilon\leq-\|x\|^2\!+\!\|y\|^2\leq\varepsilon\}\approx\mathbb{B}^k\!\times\!\mathbb{B}^{n-k}$","The question is motivated by the notion of handle attachment , Morse theory , critical points of index $k$, Morse lemma, sublevel sets , etc. For $0\!\leq\!k\!\leq\!n$ and $0\!<\!\varepsilon\!<\!1$,  how  can I find a homeomorphism 
$$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\{x\!\in\!\mathbb{B}^n;\, -\varepsilon\leq-\sum_{i=1}^k\!x_i^2+\!\sum_{i=k+1}^n\!x_i^2\leq\varepsilon\}\;\;\approx\;\;\mathbb{B}^k\!\times\!\mathbb{B}^{n-k}\;\;\;\text{ that sends}$$
$$\{x\!\in\!\mathbb{B}^n;\, -\varepsilon=-\sum_{i=1}^k\!x_i^2+\!\sum_{i=k+1}^n\!x_i^2\}\;\;\approx\;\;\mathbb{S}^{k-1}\!\times\!\mathbb{B}^{n-k}\;\;\;?$$ The case $n\!=\!2$, $k\!=\!1$: The case $n\!=\!3$, $k\!=\!1$: The case $n\!=\!3$, $k\!=\!2$:","['geometric-topology', 'differential-geometry', 'general-topology', 'algebraic-topology', 'differential-topology']"
187610,How to convert radicals to decimals without a calculator,"How can one convert radicals to decimals(approximate value) when the number is not perfect such as   $\sqrt2$,  $\sqrt3$, $\sqrt5$, etc. Without the use of calculators.","['approximation', 'algebra-precalculus']"
187617,Continuous Deformation Of Punctured Torus,"This is problem 11 (b) from the first chapter of ""Basic Topology"" by M.A. Armstrong.  The author hasn't had time to develop many theorems or mathematical machinery, so this problem should be able to be solved by just picturing a series of intermediate steps.  It goes Imagine all the spaces shown in Fig. 1.23 to be made of rubber.  For
  each pair of spaces X, Y, convince yourself that X can be continuously
  deformed into Y. I'm having trouble with one of the pairs of spaces (the other examples in the problem are unrelated, so I neglected to draw them).  The two spaces which I can't seem to think of a continuous deformation for are The caption for the first picture reads ""X = punctured torus"", while the caption for the second picture is ""Y = Two cylinders glued together over a square patch"".  I'm trying to think of some intermediate steps in the problem.  Working backwards, I can see how each of the cylinders in the second picture could be deformed to spheres with two punctures each, but I'm having trouble seeing how the ""handle"" on the torus is created.","['general-topology', 'algebraic-topology']"
187619,Lift a variety over a number field to the complex numbers,"Let's say I have a number field $k$ and a curve $C$ over $k$, can we lift $C$ to a variety $V$ over $\mathbb{C}$? It is like ""complexifying"" the curve. I read somewhere that it is possible to lift an elliptic curve $E$ over $k$ to an elliptic curve $\mathbb{C}/\Lambda$ where $\Lambda$ is a suitable lattice in $\mathbb{C}$ and the endomorphism on $E$ lift uniquely as well. I also want to lift not only a curve but a variety over $k$ and an endomorphism on $V$ to some variety over $\mathbb{C}$ and an endomorphism on $V$ respectively. Can this be done?",['algebraic-geometry']
187620,Blow-up of ODE solution,"I am a newcomer to ODEs. The relevant theorem that I can think of is about the maximum open interval of existence of the solution. But I have not learned to find the interval on which the solution exists. Let $f : \mathbb{R}^{n}\to \mathbb{R}^{n}$ be $C^{1}$ and bounded
on $\mathbb{R}^{n}$ . Is it possible to have a solution of $\dot{x}=f(x)$ that blows up in finite time?",['ordinary-differential-equations']
187633,Is independence preserved by conditioning?,"$X_1$ and $X_2$ are independent. $Y_1|X_1\sim\mathrm{Ber}\left(X_1\right)$, $Y_2|X_2\sim\mathrm{Ber}\left(X_2\right)$. Are $Y_1$ and $Y_2$ necessarily independent? (Assume $\mathrm{P}\left(0<X_1<1\right)=1$, $\mathrm{P}\left(0<X_2<1\right)=1$)",['probability-theory']
187635,Study the convergence of the following series,"I have to study the convergence of the following series: $$\sum_{n\ge1} \frac{ n! } { p (p+1) \cdots (p + n - 1) }\text{ where }p > 0.$$ I tried d'Alembert criterion but $\lim_{n\to\infty} \frac {a_{n+1}} {a_n} = 1$ (where $a_n = \frac{ n! } { p  (p+1) \cdots (p + n - 1) }$). Because that limit is $1$ the nature of the series is inconclusive. Intuitively I can say that the series is convergent because when $p\in\{1,2,n\}$ the sum becomes: For $p = 1$, $\sum_{n\ge1} \frac{ n! } {1\cdot2\cdots(1+n-1)} = \sum_{n\ge1} \frac{n!}{n!} = n$ is convergent and For $p = 2$, $\sum_{n\ge1} \frac{ n! } {2\cdot3\cdots(2+n-1)} = \sum_{n\ge1} \frac{n!}{2\cdot3\cdots(n+1)} =  \sum_{n\ge1} \frac{n!}{(n+1)!} =\sum_{n\ge1} \frac{1}{n+1}$ is convergent and For $p = n$, $\sum_{n\ge1} \frac{ n! } {n(n+1)\cdots(2n-1)} = \sum_{n\ge1} \frac{(n-2)!}{(n+2)(n+3)\cdots(2n-1)}$ is convergent (d'Alembert) How do I proof my intuition is a rigorous mathematical way?","['calculus', 'analysis']"
187658,Matrix Chain Multiplication?,"The following are questions about using dynamic programming for matrix chain multiplication. Pseudocode can be found in the Wikipedia article on matrix chain multiplication . 1) Why is the time complexity for trying all bracket permutations $\mathcal{O}(2^n)$, where $n$ is the number of matrices?? 2) Why are there $\frac{n^2}{2}$ unique subsequences? (This question is refering to memoization where all unique subproblems are stored. A rephrasing of this question would be: Why are there $\frac{n^2}{2}$ unique subproblems to be stored?) 3) Why does using memoization reduce the time to $O(n^3)$? I don't have made any progress on the above questions, therefore there is no work for me to show.","['matrices', 'linear-algebra', 'computer-science']"
187674,Name or notation for $\mathbb Z/2\mathbb Z\ast\mathbb Z/2\mathbb Z\ast\cdots\ast \mathbb Z/2\mathbb Z$,"Is there a standard notation for the n-fold free product of a group with itself? In particular, I'd like to know a nice name or notation for the the $n$-fold free product of $\mathbb Z/2\mathbb Z$ with itself.","['notation', 'group-theory']"
187688,How to solve this literal equation,I have a literal equation that needs to be solved for $\theta$: $$mg \sin(\theta) = \mu mg \cos(\theta)\left({ M+m \over m}\right)  $$,['trigonometry']
187691,Subsequential limit of sequence,I'm trying to determine all subsequential limit points of the following sequence X_n = cos(n) Not sure how to decompose this into subsequences. Anyone know how to tackle this problem? Thanks!,"['sequences-and-series', 'limits']"
187698,Proving XY is perpendicular on :CD,"In cyclic quadrilateral $ABCD$ the point $E$ is in the middle of $BC$, the perpendicular on $BC$ pass the point $E$ and intersect $AB$ in $X$, and the perpendicular on $AD$ pass the point $E$ and intersect $CD$ in $Y$, what is the proof that $XY$ is perpendicular on $CD$. I posted this problem  before and i deleted it,because the diagram was not good .",['geometry']
187715,Derivative of a matrix norm,"Consider the function $V:\mathbb{R}\to\mathbb{R}$ given by $$
V(t)=\|I - e^{At}\|^2
$$ where $I$ is the identity matrix and $A$ is a square matrix. The norm is the Euclidean norm on $M_n(\mathbb{R})$: $$\|X\|=\sqrt{\lambda_{max}(X'X)};\ X\in M_n(\mathbb{R})$$ that is induced by the matrix norm $\|x\|^2=x'x$ on $\mathbb{R}^n$. I want to calculate the derivative $\frac{dV(t)}{dt}$. Is this possible in any way? Note: Maybe the use of the Frobenius norm would facilitate things a bit but I wouldn't prefer it as it is not an induced norm (by some matrix norm).","['linear-algebra', 'analysis']"
187717,"Using distance formula to find slope, any reason to use the concluding equation?","So, today I was observing a class that I will be a TA for this semester and the professor started to talk about the distance formula $d=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$. Well, my mind wandered a little and I started to think about slope. That's when I noticed, with a little bit of algebra we can convert the distance formula into a representation of slope. $$d=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$ $$d^2=(x_2-x_1)^2+(y_2-y_1)^2$$ $$\left(\frac{d}{x_2-x_1}\right)^2=1+\left(\frac{y_2-y_1}{x_2-x_1}\right)^2$$
$$\frac{y_2-y_1}{x_2-x_1}=\sqrt{\left(\frac{d}{x_2-x_1}\right)^2-1}$$ I was wondering if anyone knows of any practical reason to use this, or if it's utterly pointless. My first impression is that it's pointless, unless you are given distance and two $x$ values and asked to find slope. But excluding that very unlikely case, I cannot think of a reason.","['algebra-precalculus', 'soft-question']"
187719,How to find the $\arcsin 2$?,"How would I find $\arcsin 2$? I'm helping my little sister with her calculus ""pre-test"" before classes begin, and I don't remember how to do it in order to explain to her. Help?",['trigonometry']
187729,"Evaluating $\int_0^\infty \sin x^2\, dx$ with real methods?","I have seen the Fresnel integral $$\int_0^\infty \sin x^2\, dx = \sqrt{\frac{\pi}{8}}$$ evaluated by contour integration and other complex analysis methods, and I have found these methods to be the standard way to evaluate this integral.  I was wondering, however, does anyone know a real analysis method to evaluate this integral?","['trigonometry', 'integration', 'real-analysis']"
187734,"If $A$ and $B$ are compact, then so is $A+B$.","This is an exercise in Chapter 1 from Rudin's Functional Analysis . Prove the following: Let $X$ be a topological vector space. If $A$ and $B$ are compact subsets of $X$, so is $A+B$. My guess: Let $\cup V_{\alpha}$ be an open covering of $A+B$, if we can somehow split each $V_{\alpha}$ into two parts \begin{equation}
V_{\alpha}=W_{\alpha}+U_{\alpha}
\end{equation} with \begin{equation}
\cup W_{\alpha}\supset A, \cup U_{\alpha}\supset B
\end{equation} then we can easily pass the compactness of $A$ and $B$ to $A+B$. However, I cannot find such a way to split $V_{\alpha}$. I admit this is the only nontrivial part of this problem. Any hint would be helpful. Thanks!","['topological-vector-spaces', 'compactness', 'functional-analysis', 'sumset']"
187743,Proving that the sum of Good-Turing estimators is $1$,"I want to know how to go about proving that the Good-Turing estimator has a total probability of $1$. I have seen this proof (page 2) but I found unclear the first step: $$\sum_j \theta[j] = \sum_r \theta[r]N_r = \frac{1}{N}\sum \left[(r+1) \frac{N_{r+1}}{N_r}\right]N_r$$ $\theta[j]$ is the probability of having an $n$-gram, $\theta[r]$ is the probability of a $n$-gram occurring $r$ times and $N_r$ is the number of $n$-grams that occur $r$ times. Since $\sum_r(r+1)N_{r+1}$ = $\sum_r rN_r$, it's more or less straightforward that it actually sums $1$. However, as I said, I don't understand the first part: $\sum_j \theta[j] = \sum_r \theta[r]N_r$. What is going on there? Thanks in advance.","['statistics', 'parameter-estimation', 'probability-theory']"
187751,Cardinality of the set of all pairs of integers,"The set $S$ of all pairs of integers can be represented as $\{i \ | \ i \in \mathbb{Z} \} \times \{j\ | \ j \in \mathbb{Z}\}$. In other words, all coordinates on the cartesian plane where $x, y$ are integers. I also know that a set is countable when $|S|\leq |\mathbb{N}^+|$. I attempted to map out a bijective function, $f : \mathbb{N}^+ \rightarrow S$. $1 \rightarrow (1,1) \\
2 \rightarrow (1,2)\\
3 \rightarrow (1,3) \\
\quad \vdots
$ I determined from this that the natural numbers can only keep up with $(1,*)$. But there is the ordered pairs where $x=2,3,4,\cdots$ not to mention the negative integers. In other words, $|S|\geq |\mathbb{N}^+|$ and therefore $S$ is not countably infinite. Is this correct? (I don't think it is... Something to do with my understanding of infinite sets)",['elementary-set-theory']
187760,Variation of $f(x)=x^\eta\sin^\varepsilon(\frac{1}{x})$,"I'm asked to characterize the values of the parameters $\eta, \varepsilon$ for which the above function is of bounded variation on $[0,1]$, when we set $f(0)=0$. By ""bounded variation"", I mean that the following sum is bounded by some constant $c$, where $t_i$ are the boundary points of any partition $\mathcal{P}$ of the interval into finitely many segments:
$$\sum_{j=1}^n |f(t_j)-f(t_{j-1})|$$ I've made just a bit of progress: we need $\varepsilon$ rational with odd denominator, or else $\sin^{\varepsilon}(\frac{1}{x})$ won't be defined on the whole interval. Furthermore we need $\eta$ and $\varepsilon$ nonnegative, positive if we ignore the trivial cases when they're 0, or the function will be unbounded at the zeroes of $\sin(\frac{1}{x})$ or near zero, respectively, while bounded-variation functions never have essential discontinuities. I can differentiate $f$: 
$$f'=x^{\eta-1}\sin^{\varepsilon-1}(\frac{1}{x})(\eta\sin(\frac{1}{x})-\varepsilon\cos(\frac{1}{x}))$$ This gives me critical points at the zeroes of $\sin{\frac{1}{x}}$ and at the infinitely many points where $\tan\frac{1}{x}=\frac{\varepsilon}{\eta}$. Ideally I'd estimate the variation by taking a partition at each critical point, since including all local extrema in the partition should guarantee, roughly, that I capture ""all $f$'s variation"". Now I'm at a loss how to proceed. Is there some nice series by which I might bound the sum I'd get in this way? Should I try a completely different approach than this using critical points? Thanks for your suggestions. EDIT : After discussing with some other members of the course, we're pretty sure that
the value of $\varepsilon$ is immaterial and $\eta>1$ gives bounded variation while $\eta \leq 1$ does not. But the closest I have to an argument for this is to point out that the former case is just when $f'$ is absolutely integrable on $[0,1]$, which seems pertinent for satisfying Sasha's condition in the comments.","['integration', 'real-analysis']"
187768,Why $ \lim_{n \rightarrow \infty}\frac{(5n^3-3n^2+7)(n+1)^n}{n^{n+1}(n+1)^2} =5e$?,"I have a small exercise and I don’t know who to get the result. The exercise is: $$ \lim_{n \rightarrow \infty}\frac{(5n^3-3n^2+7)(n+1)^n}{n^{n+1}(n+1)^2} $$ I did following transformations:
$$
\frac{(5n^3-3n^2+7)(n+1)^{n-2}}{n^{n+1}} \\
(5n^{2-n}-3n^{1-n}+7^{-1-n})(n+1)^{n-2} \\
(\frac{5}{n^{-2+n}} - \frac{3}{n^{-1+n}} + \frac{7}{n^{1+n}})(n+1)^{n-2}
$$ But none of them helped me to see the result.
It would be great if someone could explain it to me. Edit @adrian-barquero
Ok. Fist you factories $^n$ and get
$$
\frac{(n+1)^n}{n^n} = (1+\frac{1}{n})^n = e \\
$$ In the other fraction I could extend with $n^3$
$$
\frac{5n^3-3n^2+7}{n(n + 1)^2} = \frac{n^3(5 - \frac{3n^2}{n^3} + \frac{7}{n^3})}{n^3(1 + \frac{2n^2}{n^3} + \frac{n}{n^3})} = 5
$$",['limits']
187781,The Odds of A Dice Game Ending,A fair dice roll : On a 6 you win €1 on everything else you lose €1. The game continues until you profit €500. What are the odds of the game ever ending? Update : While initially I hadn't clarified as to whether the player can enter debt in fairness to @joriki I've marked his answer as correct as it answers the initial problem. Updated - Assumptions : One cannot enter debt. Eg) If you are at €0 and roll a 1 you remain at €0.,"['probability-theory', 'dice']"
187787,Proving that $u_n $ is arithmetic sequence,"Let $u_n$ be a sequence defined on natural numbers (the first term is $u_0$) and the terms are natural numbers ($u_n\in \mathbb{N}$ ) We defined the following sequences: $$\displaystyle \large x_n=u_{u_n}$$
$$\displaystyle \large y_n=u_{u_n}+1$$ If we know that both $y_n$ and $x_n $ are arithmetic sequences ,how we can prove that $u_n $ is also arithmetic sequence",['algebra-precalculus']
187794,Why did my friend lose all his money?,"Not sure if this is a question for math.se or stats.se, but here we go: Our MUD (Multi-User-Dungeon, a sort of textbased world of warcraft) has a casino where players can play a simple roulette. My friend has devised this algorithm, which he himself calls genius: Bet 1 gold If you win, bet 1 gold again If you lose, bet double what you bet before. Continue doubling until you win. He claimed you will always win exactly 1 gold using this system, since even if you lose say 8 times, you lost 1+2+4+8+16+32+64+128 gold, but then won 256 gold, which still makes you win 1 gold. He programmed this algorithm in his favorite MUD client, let it run for the night. When he woke up the morning, he was broke. Why did he lose? What is the fault in his reasoning?","['stochastic-processes', 'probability', 'martingales']"
187820,What's a measure valued solution of a PDE?,"What's a measure valued solution of a PDE? For instance the Fokker-Planck equation \begin{align}
\partial_t\mu_t+\sum_i\partial_i(b_i\mu_t)-\frac{1}{2}\sum_{ij}\partial_{ij}(a_{ij}\mu_t)=0
\end{align} it says that for a measure $\mu=\mu(t,x)=\mu_t(x)$ being a solution of the above equation means \begin{align}
\frac{d}{dt}\int_{\mathbb{R}^N}\phi(x)d\mu_t(x)=\int_{\mathbb{R}^N}\left(\sum_ib_i(t,x)\partial_i\phi(x)+\frac{1}{2}\sum_{ij}a_{ij}(t,x)\partial_{ij}\phi(x)\right)d\mu_t(x)
\end{align} How do we get this? Also if compare this with the weak formulation, What's the connection between a measure valued solution and a distributional solution?","['measure-theory', 'partial-differential-equations', 'analysis']"
187855,Properties of generalized eigenvectors,"Let $A\in\mathbb{R}^{n\times n}$ denote some symmetric, and $B\in\mathbb{R}^{n\times n}$ some
positive-definite matrix. The generalized eigenvalue problem, $[A, B]$,
corresponds to a scalar-vector pair, $(\lambda, u)$, satisfying
$$Au=\lambda Bu.$$ What is the property of generalized eigenvectors $u$, e.g.,  are they mutually orthogonal? Are they somehow related to eigenvectors of $A$ (or $B$ )?","['vector-spaces', 'matrices', 'linear-algebra']"
187860,Solutions to $\mathbf{AX}=\mathbf{B}$,"Let $\mathbf{A},\mathbf{B}$ be $n\times n$ matrices over a field $\mathbb{F}$. How can we find if there exist a $n\times n$ matrix $\mathbf X$ s.t. $\mathbf{AX}=\mathbf{B}$? (and how can we find $\mathbf X$ if it exists?) Note: if $|\mathbf A|\neq 0$ then it's easy since $\mathbf{X}=\mathbf{A}^{-1}\mathbf{B}$, but I stumbled on a problem where my $\mathbf A$ is not invertible.","['matrices', 'linear-algebra']"
187900,Finding Lyapunov functions,"I came across the following question in one of my professor's past exams: Find a Lyapunov function for $(0,0)$ in the system:
$$
\left\{
\begin{array}{ll}
\dot{x} = -x -2y + x^2\\
\dot{y} = x - 4y + xy
\end{array}\right.
$$ I know there is no formula for finding Lyapunov functions for a system, so how do I start solving such problems? Thanks!",['ordinary-differential-equations']
187909,Statistics: Bertrand's Box Paradox [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: Probability problem This is the Bertrand's Box Paradox I read about on Wikipedia: Assume there is three boxes: a box containing two gold coins, a box with two silver coins and a box with one of each. After choosing a box at random and withdrawing one coin at random, if that happens to be a gold coin, the probability is actually 66% instead of 50%. And the problem is equivalent to asking the question ""What is the probability that I will pick a box with two coins of the same color?"". No matter how hard I try, I just couldn't comprehend this.. How is the possibility of picking a gold coin the same as the probability of picking a box with two coins of the same color? Does this imply there is a 66% chance of picking a gold coin and a 66% chance of picking a sliver coin? If so, can we just say there is 50% chance of picking either one of them since both stand a 66% chance....?! and suddenly everything makes no sense.. [UPDATES] It is actually the probability of the remaining coin to be gold is 66% but not the probability of obtaining the gold coin is 66%.. I've misread it.... And everything makes sense now :D !","['statistics', 'probability', 'paradoxes']"
187915,"For which $\mathbf X$ (matrix) does there exist a scalar $c$ such that $\mathbf{AX} = c\mathbf{X}$? ($\mathbf A$ is a matrix, in the message)","For which $\mathbf X$ (matrix) does there exist a scalar $c$ such that $\mathbf{AX} = c\mathbf{X}$?
$$
\mathbf{A} = \begin{bmatrix} 5 & 0 & 0\\ 1 & 5 & 0\\ 0 & 1 & 5 \end{bmatrix}
$$
I have thought about the idea that $\mathbf{X}$ is $0$ so $c$ is $0$, but is it the only solution? Thank you!","['matrices', 'linear-algebra']"
