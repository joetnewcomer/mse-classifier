question_id,title,body,tags
2013683,The union of Edge sets of distinct $u - v $ Path must contain a cycle.,"The solution goes here and i have doubts in many points-: We use induction on the sum $l$ of the lengths of the two paths, for all vertex pairs simultaneously. If $P$ and $Q$ are $u$ $v$ paths, then $l \geq 2$ . If $l =2$ , then we have distinct edges consisting of $u$ and $v$ , and together they form a cycle of length $2$ . Here $AB$ and $CD$ are cycle Am i getting it right? For the induction step, suppose $l \gt 2$ . If P and Q have no common internal vertices, then their union is a cycle. (same as above figure)!! Am i right there ? Not getting from here
**If $P$ and $Q$ have a common internal vertex $w$ , then let $P^{'}$ $P^{''}$ be
the $uw$ subpath of $P$ and the $wv$ subpath of $P$ . Similarly define $Q^{'}$ $Q^{''}$ Then $P^{'}$ $Q^{'}$ are $uw$ paths with total length less than $l$ .Similarly, $P^{''}$ $Q^{''}$ are $wv$ paths with total length less than $l$ . Since $P$ $Q$ are distinct, we
must have $P^{'}$ $Q^{'}$ distinct or $P^{''}$ $Q^{''}$ distinct. We can apply the induction
hypothesis to the pair that is a pair of distinct paths joining the same end
points. This pair contains the edges of a cycle, by the induction hypothesis,
which in turn is contained in the union of $P$ and $Q$ ** Please help me out !!","['graph-theory', 'proof-explanation', 'discrete-mathematics']"
2013862,Why is it impossible to root a whole equation,I have this equation: $x^2+(x-7)^2=13^2$ Why can't squaring the whole equation be used to solve it? $\sqrt{x^2+(x-7)^2=13^2}\equiv x+x-7=13$ My question is merely asking when square cannot be used to solve an equation.,"['self-learning', 'trigonometry', 'linear-algebra']"
2013880,Continuous image of separable space,"Prove that every continuous image of a separable space is separable. Let $(X, \mathcal T)$ be a separable space. Then there exists some countable subset $A \subseteq X$ such that $\overline{A}=X$.
Let $f:X\to Y$ be a continuous mapping. Notice that $f:X \to f(X)\subseteq Y$ is clearly surjective. Since $A\subseteq X$, and functions preserve set inclusion, we have that $$f(A) \subseteq f(X)= f(\overline{A}).$$ Also, it is clear that $f(A)$ is itself also countable. What (I think) I need to show, however, is that $\overline{f(A)}=f(\overline{A})$ (which I am not sure if it's true). This will thus show that $f(A)$ is a countable, dense subset of $f(X).$ Is this the correct approach, or do I have a mistake in my method?  Can anyone please help show me how I can go about finishing this proof?","['general-topology', 'separable-spaces']"
2013886,why is the definition of the determinant so weird? [duplicate],"This question already has answers here : What's an intuitive way to think about the determinant? (18 answers) Closed 7 years ago . I learned linear algebra from books of Friedberg, Gilbert & Strang, Anton etc. by myself. I dare say, that I learned all that stuff eagerly. Studying by myself, I could not intuitively understand the definition of the determinant (its even – odd manner). I could only memorize the definition, and then use it (or try to use it) to solve some related readers' homework problems or other exercises. As you know, the purpose of a determinant is literally to determine whether a given system of equations has a unique solution or not. In other words, the ""determinant"" will determine whether the row vectors (and equivalently, column vectors) of a given square matrix are independent or not. If those are mutually independent, then they can geometrically represent an
$n$-dimensional quantity (for example, area in 2 dimensions or volume in 3D). If not, some of them are dependent, so they cannot form the $n$-dimensional quantity, and correspondingly the determinant is zero. A multiple of any row can be added to another, this kind of row elementary operation does not change the determinant value. The picture below illustrates an intuitive understanding of that, too. Writing the sides of the parallelogram as rows or columns of a square matrix, this transformation transforms it to another with the same value of the determinant. It can be transformed to Gauss–Jordan form, in this case, each of the row / column vectors are orthogonal because their inner products are all zero.
(I tried this with the Gram–Schmidt process; however, intuitively, the result is surely the same.) Those vectors are orthogonal so it is very clear that just multiplication of the diagonal terms should give directly the aforementioned $n$-dimensional quantity, so that's the determinant in such case. I understand the determinant in this manner, and it makes sense intuitively. However the textbook definition mentioned above (defined in the ""even–odd"" manner) looks very weird to me. What is the motivation of that definition? And can it be generally derived from my intuition about the $n$-dimensional quantities? I succeeded in doing so for the $2\times2$ and $3\times3$ cases, but I cannot see any generalized relation. It seems to me that the definition of the determinant comes down magically, without enough logic. I was wondering if you could help me. Thank you in advance.","['linear-algebra', 'determinant']"
2013899,Prove that all numbers in a sequence are equal,"There is a sequence $a_1, a_2, ..., a_n \in \mathbb R$, ($n$ is odd) such that if we delete one (any) number from it, then the rest of the numbers can be divided in two subsets of size $\lfloor \frac{n}{2} \rfloor$ and equal sum (sum of the first subset is equal to the sum of the second subset). How to prove that $a_1 = a_2 = ... = a_n$ ? What I have tried I can solve this task assuming that those numbers are integers, but I have no idea how to even start when those numbers are reals. For integers I would prove that all of those numbers have the same parity, then substract from every number the smallest number, so that at least one of them is 0 after this operation. But because all numbers have the same parity and there is 0, so all numbers are even. I can divide all of them by any power of 2, and the sequence will be still ok, so it means that all numbers are equal. This proof also works for rational numbers, because we can delete denominators (by multiplying all numbers by LCM of their denominators).","['number-theory', 'combinatorics']"
2013902,Evaluate without L'Hopital: $\lim_{x \to 3} (x-3)\csc\pi x $,"I'm supposed to evaluate this limit without using L'Hopital's rule. $$\lim_{x \to 3} (x-3)\csc\pi x $$ I find the indeterminate form of $0$ or $\frac{0}{0}$. The latter tells me that L'Hopital's is an option, but since we haven't seen derivatives yet I'm not allowed to used it. Previously I already tried swapping the $\csc\pi x$ for $\frac{1}{\sin\pi x}$ but when doing this I can't seem to get rid of the sinus. I also believe that since the limit goes to $3$ and not to $0$, the $\lim_{x \to 0}\frac{\sin ax}{ax} = 1$ rule is not an option either. I tried making the $\sin(\frac{\pi}{2})$ so that $=1$, but without any success. Can anyone give me a hint? I don't need a full solution as I want to try and find it myself.","['trigonometry', 'limits-without-lhopital']"
2013999,When is the one parameter group of diffeomorphisms isomorphic to the circle group $T$?,"Let $M$ be a smooth manifold of dimension $m$ and
$$\phi: \mathbb{R} \times M \rightarrow M : (t,x) \to \phi_t(x)$$
such that: $1)\ \phi_0(x) = x$ for all $x \in M$ and $2)\ \phi_t(\phi_s(x)) = \phi_{t+s}(x)$ for all $t, s \in \mathbb{R}$ and for all $x\in M$. Then
$$G := \lbrace \phi_t \mid t\in \mathbb{R} \rbrace$$
is called the one parameter group of diffeomorphisms of $M$. Show that $G$  is isomorphic to $(\mathbb{R}, +)$ or the circle group $T = \lbrace z\in \mathbb{C} \mid |z| = 1 \rbrace$ What I tried was first trying to construct candidates for isomorphism and then checking if everything holds. The candidates I came up with were $$h_1 : \mathbb{R} \rightarrow G : h_1(t) = \phi_t$$
and 
$$h_2 : G \rightarrow T : h_2(\phi_t) = e^{it}$$
Now, $h_1$ and $h_2$ are both group homomorphisms, and both maps are surjective (correct me if I'm wrong). Now the issue here is the injectivity. So I tried to distinguish two cases depending on whether the map $\phi: \mathbb{R} \times M \rightarrow M$ is injective or not. If $\phi$ is injective, then $(t,x) \neq (s,y)$ implies $\phi_t(x) \neq \phi_s(y)$. So if $t \neq s$, then $h_1(t) \neq h_1(s)$ by the infectivity of $\phi$ and hence $G$ would be isomorphic to $(\mathbb{R},+)$. Now I'm stuck with the case where $f$ is not injective. I thought of considering the set 
$$H = \lbrace t\in \mathbb{R} \mid \phi_t(x_0) = x_0 \rbrace$$
for some fixed $x_0 \in M$. I know that if $H = \{0\}$, then the orbit of $G$ determined by $x_0$ is injective, and that if $H \neq \{0\}$, the orbit of $G$ determined by $x_0$ is diffeomorphic to a circumference. But since we are assuming that $\phi$ is not injective, it must be the case where $H \neq \{0\}$, so the orbit must be diffeomorphic to a circumference. I think this already proves the result, since we have a diffeomorphism, let's call it $g : \mathbb{S}^1 \rightarrow O(x_0)$, where $O(x_0)$ stands for the orbit of $x_0$, but I lack of a complete proof of this.","['differential-geometry', 'smooth-manifolds', 'group-theory', 'group-isomorphism']"
2014002,prove $(n!)^{n+1}\mid((n^2)!)!$,Prove: $(n!)^{n+1}\mid((n^2)!)!$ I tried doing this through coefficient formula. I think there has to be a term which is to be added in proof but i cant figure out what exactly has to be done.,"['combinatorics', 'discrete-mathematics', 'elementary-number-theory']"
2014049,Solutions of $y''+y'+by=\cos x$ that satisfy $\lim\limits_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$,"Find for which values of $b$ every two solutions $y_1,y_2$ of the equation $y''+y'+by=\cos x$ satisfy $$\lim_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$$ My attempt : Since the difference of every two solutions of the original equation is a solution of the homogeneous equation $y''+y'+by=0$ we are interested only in the solutions of the latter. The characteristic roots are: $$m_{1,2}=\frac{-1\pm\sqrt{1-4b}}{2}$$ If $b=1/4$ then $m_{1,2}=-1/2$ so the general homogeneous solution is $y_h(x)=(C_1+C_2x)e^{-x/2}$ and $$\lim_{x\to\infty}\frac{(C_1+C_2x)e^{-x/2}}{e^x}=\lim_{x\to\infty}\frac{C_1+C_2x}{e^{3x/2}}=0$$ If $0 \leq b < \frac{1}{4}$ then $0<\sqrt{1-4b}\leq 1$ and so we have two distinct nonpositive real roots (at least one is strictly negative). Therefore the general homogeneous solution is $$y_h(x)=C_1 e^{m_1 x}+C_2e^{m_2 x}=\frac{C_1}{e^{|m_1|x}}+\frac{C_2}{e^{|m_2|x}}$$ So
$$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( \frac{C_1}{e^{(|m_1|+1)x}}+\frac{C_2}{e^{(|m_2|+1)x}} \right )=0$$ If $b<0$ then $\sqrt{1-4b}>1$ we have two distinct real roots (with opposite signs). If $m_1<0$ and $m_2>0$ then the general homogeneous solution is $$y_h(x)=C_1 e^{m_1 x}+C_2e^{m_2 x}=\frac{C_1}{e^{|m_1|x}}+C_2e^{m_2 x}$$ Therefore $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( \frac{C_1}{e^{(|m_1|+1)x}}+C_2e^{(m_2-1) x} \right )=0 \iff m_2<1 \iff b>-2$$ If $b>\frac{1}{4}$ then there are two complex roots of the form $-\frac{1}{2}\pm i\beta$ so the general homogeneous solution is $$y_h(x)=e^{-x/2} (C_1 \cos (\beta x)+C_2 \sin (\beta x))$$ Hence $$\lim_{x\to\infty}\frac{y_h(x)}{e^x}=\lim_{x\to\infty} \left( e^{-3x/2} (C_1 \cos (\beta x)+C_2 \sin (\beta x)) \right )=0$$ In conclusion for all $b>-2$ every two solutions $y_1,y_2$ of the equation $y''+y'+by=\cos x$ satisfy $$\lim_{x\to\infty}\frac{y_1(x)-y_2(x)}{e^x}=0$$ Is it correct? If yes, is there an easier way to solve this problem?","['ordinary-differential-equations', 'proof-verification']"
2014094,how to prove that $\hat \sigma^2$ is a consistent for $\sigma^2$,"Consider a regression model $Y_n=X_n\beta +\varepsilon$, where $X_n$ is a $n \times p_n$ matrix, and $\varepsilon=(\varepsilon_1,...,\varepsilon_n)'$ consists of independent and identically distributed variables with $E(\varepsilon_1)=0$ and $Var(\varepsilon_1)=\sigma^2$. Suppose $\hat \sigma ^2$ is the estimator of $\sigma^2$. Let $$\hat \sigma ^2=\frac{\left\|Y_n-X_n\hat\beta\right\|^2}{n-p_n}.$$ How to prove that $\hat \sigma^2$ consistent for $\sigma^2$?","['regression', 'statistics']"
2014106,Prove universal property of direct sum,"Let $\{X_\alpha\}_{\alpha\in A}$ be a collection of groups and let $X$ be a group such that for all $\alpha \in A$ there exists an injection $i_\alpha \colon X_\alpha\hookrightarrow X$ . Furthermore suppose that $X$ has the following property: For all groups $Y$ with a collection of homomorphisms $\{f_\alpha\}_{\alpha\in A}$ where $f_\alpha \colon X_\alpha\to Y$ for $\alpha\in A$ there exists a unique homomorphism $f \colon X\to Y$ with the property $f\circ i_\alpha=f_\alpha$ for all $\alpha\in A$ . Prove that $X$ is isomorphic to $\bigoplus_{\alpha\in A}X_\alpha$ . Here is what I have done: Let $Y=\bigoplus_{\alpha\in A}X_\alpha$ and for $\alpha\in A$ we define $f_\alpha:X_\alpha\to Y$ by $f_\alpha(x_\alpha)=(f^\beta_\alpha(x_\alpha))_{\beta\in A}$ where $f^\beta_\alpha(x_\alpha)=e_{\beta}$ (where $e_\beta$ is the neutral element of $X_\beta$ ) if $\beta\neq\alpha$ and $f^\beta_\alpha(x_\alpha)=x_\alpha$ if $\beta=\alpha$ for $x_\alpha\in X_\alpha$ . We see that $\{f_\alpha\}_{\alpha\in A}$ is a family of homomorphisms, so by hypothesis there exists a unique homomorphism $f:X\to Y$ with $f\circ i_\alpha=f_\alpha$ for all $\alpha\in A$ . So we try to show that $f$ is an isomorphism. Surjectivity: Let $(x_\alpha)_{\alpha\in A}\in \bigoplus_{\alpha\in A}X_\alpha$ with $x_\alpha\ne e_\alpha$ if and only if $\alpha\in\{\alpha_1,\ldots,\alpha_r\}$ for an $r\in\mathbb{N}_0$ . Now consider $x=i_{\alpha_1}(x_{\alpha_1})\cdot\ldots\cdot i_{\alpha_r}(x_{\alpha_r})$ ; we see that $f(x)=f\left(i_{\alpha_1}(x_{\alpha_1})\cdot\ldots\cdot i_{\alpha_r}(x_{\alpha_r})\right)=f\left(i_{\alpha_1}(x_{\alpha_1})\right)\cdot\ldots\cdot f\left(i_{\alpha_r}(x_{\alpha_r})\right)=f_{\alpha_1}(x_{\alpha_1})\cdot\ldots\cdot f_{\alpha_r}(x_{\alpha_r})=(x_\alpha)_{\alpha\in A}$ . Thus $f$ is surjective. Injectivity: Equivalently we try to show that $\ker f=\{e_X\}$ . We see that if $x\in\langle i_\alpha(X_\alpha)\ \mid\ \alpha\in A\rangle$ then $f(x)=e_X\implies x=e_X$ . However, I fail to see how to argue if $x\notin\langle i_\alpha(X_\alpha)\ \mid\ \alpha\in A\rangle$ . I think it has something to do with the uniqueness of the homomorphism $f \colon X\to Y$ , as so far I haven't used this property. Am I on the right track? Is what I have done so far correct? How to prove the injectivity? Thanks.","['direct-sum', 'group-theory']"
2014146,Is there a constructive proof that a Euclidean domain is a UFD?,"The proof (at least the proof I know) that a principal ideal domain is a unique factorization domain uses the axiom of choice in multiple ways, and the usual way to show that a Euclidean domain is a UFD is to show that it's a PID (which is easy and constructive). Is there a direct proof, not using the axiom of choice, that a Euclidean domain is a UFD?","['abstract-algebra', 'ring-theory', 'unique-factorization-domains']"
2014171,"Prove by elementary methods: the plane cannot be covered by countably many copies of the letter ""Y""","As a consideration from the post "" Prove by ""elementary methods"": The plane cannot be covered by finitely-many copies of the letter ""Y"" "", on the basis of the remark made in previous post by the user Moishe Cohen,  is it still possible to apply elementary methods to prove weaker results, namely: The plane cannot be covered by countably many copies of the letter Y. As in the previous post, by ""letter Y"", it is meant that letter Y consists of 3 closed segments that have common point. In other words, how to extend the idea from the case where there are finitely-many copies of the letter Y, however still using only elementary methods? Advices/hints/solutions very appreciated!!!","['low-dimensional-topology', 'contest-math', 'general-topology', 'geometry']"
2014324,If $f$ is diagonalisable then its minimal polynomial is the product of distinct linear factors,"I'm trying to prove that if a linear operator $f$ is diagonalisable then its minimal polynomial is the product of distinct linear factors. This is what I have so far: Let $f$ be diagonalisable. So there exists a basis relative to which $f$ has a diagonal matrix, say $D$. So the characteristic polynomial of $f$ is given by $p_f(x)=(x-\lambda_1)(x-\lambda_2)\ldots(x-\lambda_s),$ where $\lambda_i$ are the diagonal entries of $D$. I know that the minimal polynomial must divide the characteristic polynomial and have the same linear factors. Without loss of generality let the first $i$ linear factors be distinct. So I claim that the minimal polynomial $m_f(x)=\pm (x-\lambda_1)(x-\lambda_2)\ldots(x-\lambda_i).$ However, how can I now verify that $m_f(D)=0$ ?","['minimal-polynomials', 'linear-algebra']"
2014347,Why is this expression for $1/e$ so similar to the definition of $e$?,"Consider this probability thought experiment: A barrel is filled with $n$ grains of rice. All grains are white, except for a single grain of brown rice. Pick a grain at random from the barrel. Observe it and put it back in the barrel. Do this $n$ times. For very large values of $n$, what is the probability that you never picked the grain of brown rice? On any given iteration of this procedure, the probability of picking the brown grain is $P_{brown}=\frac{1}{n}$. So the probability of picking a white grain on a given iteration is $1-\frac{1}{n}$. Therefore the probability of picking a white grain $n$ times is: $$
P_n=(1-\frac{1}{n})^n
$$ And for very large $n$, we get: $$
P = \lim_{n \to \infty}{(1-\frac{1}{n})^n}
$$ Empirically, it's easy to see that this limit is equal to $\frac{1}{e}$. For example, we can plug in $n=999,999,999$ to get: $$
\frac{1}{P_{999,999,999}} = (1-\frac{1}{999,999,999})^{-999,999,999}
$$ This equals equals 2.7182817629 , which is within $10^{-7}$ of $e$. Now, notice how similar is the limit mentioned above to one of the most popular definitions of $e$: $$
e \equiv \lim_{n \to \infty}{(1+\frac{1}{n})^n}
$$ The only difference is that the ""$-$"" is now a ""$+$"". Here's the question my waste bin and I have been struggling with: how can one prove that $$
\lim_{n \to \infty}{P_n} = e^{-1}
$$ or, equivalently, that $$
\lim_{n \to\ \infty}{(1-\frac{1}{n})^n} = \frac{1}{\lim_{n \to \infty}{(1+\frac{1}{n})^n}}
$$","['probability', 'calculus']"
2014430,Variation of parameters with nonconstant coefficients,"In my class we learned how to solve DEs using the variation of parameters when the coefficients are constant, we use undetermined coefficients to get two homogeneous solutions then apply the method of variation of parameters to get a particular solution. But I was never taught how to use method of variation of parameters when the coefficients are non-constant. For example $$x^2y'' -3xy'+3y=12x^4$$ Has solutions $y_1=x$ and $y_2=x^3$. I found this example online but even it doesn't show how these homogenous solutions were found... But it uses them to find the particular solution.... Which I'm fine with. How do I find these solutions and is there a general strategy? Thanks!",['ordinary-differential-equations']
2014474,Examples in which it is very difficult to find a $\delta$?,"I'm reading Henle/Kleinenberg's Infinitesimal Calculus . They say on page $8$: Can you provide examples in which it is very difficult to find a $\delta$? The example they provided in the previous page is very elementar, I'd like to see how hard it could be.","['epsilon-delta', 'real-analysis', 'limits']"
2014489,What is the best strategy for roulette?,"You start with $\$10$. You have a $\dfrac {18}{38}$ chance of winning, and if you win you get back double the money you spent. The minimum bet is $\$1$. How should you split your bets so that you make $\$20$ the fastest? This question was given to me by a friend, who in turn got the question from another student. So unfortunately I don't know the context or the exact wording. The only way I thought of interpreting the question is to see which strategy has the best expected value. If you bet $\$10$ directly, your expected value is $E_1 = 10 \cdot \dfrac {18}{38} - 10 \cdot \dfrac {20}{38}$. If you bet $\$5$ twice, your expected value is $E_2 = 2\left( 5 \cdot \dfrac {18}{38} - 5 \cdot \dfrac {20}{38} \right) = E_1.$ I don't see how splitting the bets in different ways would ever make a difference.","['statistics', 'probability']"
2014494,Why don't we use partial notation for double integrals?,"Now I realize there are a few questions like this on here, but none of them really get to the heart of what I'm asking... In single variable calculus we learn that the following can describe the relationship between a derivative and an integral... $$
\int \frac{dy}{dx} dx = \int dy \frac{dx}{dx} = \int dy = y 
$$ In multivariable calculus we learn to take partial derivatives using the  $\partial$ symbol but then unintuitively learn to take double integrals with this notation...
$$
\int\int f(x,y)dxdy
$$
But using the relationships from single variable calculus (of a derivative to an integral), this would intuitively seem to be the way to notate a double integral... $$\int\int \frac{\partial z}{\partial x \partial y} \partial x \partial y = \int\int \partial z \frac{\partial x}{\partial x } \frac{\partial y}{\partial y } = \int\int \partial z = z
$$ As opposed to the common notation of...
$$
f(x,y)=\frac{\partial z}{\partial x \partial y} \rightarrow \int\int f(x, y) dx dy = z
$$
Wouldn't this, however, imply...
$$\int\int \frac{\partial z}{\partial x \partial y} dx dy = \int\int \partial z \frac{dx}{\partial x } \frac{dy}{\partial y } 
$$ For what reason do we use this notation when it doesn't seem algebraically consistent like single variable notation? Does $ \frac{dx}{\partial x} = 1 $ in the same way that $ \frac{dx}{dx} = 1 $ ? Am I wildly overthinking things?","['multivariable-calculus', 'integration', 'notation']"
2014536,"If any two sets on a $\pi$-system with certain properties have equal probability, then is the same true on the generated $\sigma$-algebra?","Let $(\Omega,\Sigma)$ be a measurable space and let $X:\Omega\to\mathbb R$ be a random variable that is measurable $\Sigma/\mathscr B$. Here, $\mathscr B$ denotes the Borel $\sigma$-algebra on $\mathbb R$. Suppose furthermore that $\mathscr P$ is a $\pi$-system on $\mathbb R$ (closed under finite intersections) such that $\mathscr P$ generates the Borel $\sigma$-algebra $\mathscr B$; $\nu$ is a Borel probability measure on $\mathbb R$; if $H_1,H_2,\in\mathscr P$ are such that $X^{-1}(H_1)=X^{-1}(H_2)$, then $\nu(H_1)=\nu(H_2)$. NOTE: $\nu$ is not assumed to be the probability distribution of the random variable $X$. Conjecture: If $H_1,H_2,\in\mathscr B$ are such that $X^{-1}(H_1)=X^{-1}(H_2)$, then $\nu(H_1)=\nu(H_2)$. That is, I want to extend the probabilistic indistinguishability of two sets such that the preimages of $X$ coincide on the $\pi$-system to the generated $\sigma$-algebra. I tried using Dynkin’s theorem, but I hit a dead end. Any hints would be greatly appreciated.","['probability-theory', 'measure-theory']"
2014576,"For $a$, $b$, $c$ the sides of a triangle, prove $\left| \frac{a}b-\frac{b}a+\frac{b}c-\frac{c}b+\frac{c}a-\frac{a}c\right|< 1$","The inequality I have to prove that if $a$, $b$, and $c$ are the sides of a triangle, then $$\left| \frac{a}b-\frac{b}a+\frac{b}c-\frac{c}b+\frac{c}a-\frac{a}c\right|< 1$$ I initially thought of changing it in terms of sine using the law of sines, which gives me $$\left| \frac{\sin A}{\sin B}-\frac{\sin B}{\sin A}+\frac{\sin B}{\sin C}-\frac{\sin C}{\sin B}+\frac{\sin C}{\sin A}-\frac{\sin A}{\sin C}\right| < 1$$ However, from there I'm not sure how to proceed. Any tips?","['inequality', 'trigonometry', 'absolute-value', 'contest-math', 'fractions']"
2014601,How to find the Fourier series for limacons?,"In paper Shadows of rotating black holes approximated by
Durer-Pascal limacons , the author tries to find an approximation to the equations $$
x=\frac{r ∆+r Q^2 −M(r^2 −a^2)}{a(r −M)\sin θ}
$$ 
and 
$$
y^2=\frac{4r^2∆}{(r −M)^2}−(x+a\sinθ)^2
$$ 
where $∆ = r
^2 − 2Mr + a^
2 + Q^
2$ Note: $r$ does not represent the radius here, i.e $r\neq\sqrt{x^{2}+y^{2}}$. The author shows that the above equations can be approximated using limacon equation 
$$
(x
^2 +y^
2 −Ax)^
2 =B
^2
(x
^2 +y^
2
)
$$
for $Q=0$ and $\theta=\frac{\pi}{2}$. Then it was stated (without proof) that for different values of $\theta$ the
""paramters A and B are given by the following simple Fourier series"" $$
A_a(θ) = A_a \sin θ +0.2 \,a \sin^ 3
θ \cos ^2
θ
$$
$$
B_a(θ) = B_a +0.23M\left(
1−
\sqrt{
1-\frac{
a^4}{
M^4}}\right)
\cos^4
θ
$$ My question is how did the author use the fourier series to arrive at the above equations for the paramters A and B?","['fourier-series', 'ordinary-differential-equations', 'approximation', 'geometry']"
2014616,Can the Ricci tensor determine a manifold up to homeomorphism?,"I don't know how to ask this question. Maybe there are many improper  statements. Let $(M, g)$ and $(N, h)$ be two Riemannian manifolds and $f:M \to N$ a  bijection such that $\operatorname{Ric}(x)$ is equal to $\operatorname{Ric}(f(x))$ up to a coordinate transformation. Are $M$ and $N$ homeomorphic? In fact, I feel ""up to a coordinate transformation"" is not suitable, because, locally, I feel (not sure) that I can choose suitable coordinates making $\operatorname{Ric}(x) = \operatorname{Ric}(f(x))$. But I also don't know how to ask it. I just want to know whether the Ricci tensor can determine a manifold up to homeomorphism.","['riemannian-geometry', 'curvature', 'manifolds', 'general-topology', 'differential-geometry']"
2014617,Fourier Series involving modulus function,"I've come across this problem as stated: Let $f(x)$ be the function $f(x) = |x| + \pi$, if $-\pi < x < \pi$. Find the exact values of the first two non-zero terms of the Fourier Series for $f(x)$. I understand that it is an even function given that $f(-x) = |-x| + \pi = |x| + \pi = f(x)$, thus $b_n = 0$ for all $n=1,2,3...$ However, as I got stuck and referred to suggested solution, in the steps the $\pi$ was omitted from $f(x)$, leaving only $f(x) = |x|$ in the sense that when finding $a_0$, the solution took $|x|$ as $f(x)$ instead of $|x| + \pi$. Why was the solution presented this way? Thank you.","['fourier-series', 'calculus', 'functions']"
2014634,"Do ""exotic vectorfields"" exist?","By an exotic vectorfield on $\mathbb{R}^n$, I mean a non-zero derivation on the algebra $C^0(\mathbb{R}^n)$. Do such things exist?","['examples-counterexamples', 'differential-geometry']"
2014644,Can we get correct answer when we add fractions in a wrong way,"Are there any two fractions $\frac{a}{b}$ and $\frac{c}{d}$ such that $$\frac{a}{b}+\frac{c}{d}=\frac{a+c}{b+d}$$ with conditions $HCF(a,b)=1$ and $HCF(c,d)=1$ and $b \ne d$ I have just simplified the given equation and got $$ad^2=-b^2c$$ so one of $a$ and $c$ should be negative. so is there any possibility of such fractions?","['algebra-precalculus', 'number-theory', 'elementary-number-theory']"
2014659,Computing the Fubini-Study metric in affine chart,"Someone asked this question recently and then deleted it, but I still would like to figure out the answer. Let us try to compute the Fubini-Study metric in inhomogeneous coordinates on $\mathbb{C} P^n$,  using the Hopf fibration. For simplicity, let us do $n=1$. Start with $\mathbb{C}^2$. In the standard coordinates $(Z_0, Z_1)$, the standard flat metric is given by $g= dZ_0\,d\overline{Z_0} + dZ_1\,d\overline{Z_1}$. This metric restricts to the round metric on the unit sphere $S^3$ seen as $S^3 = \{|Z_0|^2 + |Z_1|^2 = 1\}$. The group of unit complex numbers $U(1)$ acts on $S^3$ by multiplication, and the quotient is the complex projective line $\mathbb{C}P^1$. (Note that this is the Hopf fibration $S^1 \to S^3 \to S^2$). In homogeneous coordinates $[Z_0 \colon Z_1]$ on $\mathbb{C}P^1$, the projection map $\pi \colon S^3 \to \mathbb{C}P^1$ is just $(Z_0, Z_1) \mapsto [Z_0 \colon Z_1]$. In the inhomogeneous coordinate $z = Z_1/Z_0$ (such that $[Z_0 \colon Z_1] = [1 \colon z]$), the projection map is $(Z_0, Z_1) \mapsto z = Z_1/Z_0$. Since the metric on $S^3$ is invariant under the action of $U(1)$, it descends to a metric on $\mathbb{C}P^1$ such that the projection map $\pi \colon S^3 \to \mathbb{C}P^1$ is an isometry. This metric on $\mathbb{C}P^1$ is the Fubini-Study metric. In order to compute it, we can use any local section $s \colon \mathbb{C}P^1 \to S^3$, for instance:
$$s(z) = \left(Z_0 = \frac{1}{\sqrt{1+z\overline{z}}}, Z_1 = \frac{z}{\sqrt{1+z\overline{z}}}\right)~.$$
The Fubini-Study metric in the coordinate $z$ will simply be given by the pull-back metric $g_{FS} := s^* g = dZ_0\,d\overline{Z_0} + dZ_1\,d\overline{Z_1}$ where $Z_0$ and $Z_1$ denote (somewhat abusively) the functions of $z$ given by the definition of $s(z)$ as above. A direct computation gives:
$$dZ_0 = \frac{-\overline{z}\,dz - z\,d\overline{z}}{2(1+z\overline{z})^{3/2}}$$
$$dZ_1 = \frac{(2+z\overline{z})\,dz - z^2 \,d\overline{z}}{2(1+z\overline{z})^{3/2}}$$ 
which yields
$$dZ_0\,d\overline{Z_0} + dZ_1\,d\overline{Z_1} = \frac{-\overline{z}^2\,dz^2 - z^2\,d\overline{z}^2 + 2(2+z\overline{z})\,dz\,d\overline{z}}{4\,(1+z\overline{z})^2}$$ The problem is that this is not what is expected: instead we expect
$$g_{FS} = \frac{dz\,d\overline{z}}{(1+z\overline{z})^{2}}~.$$ Where have I gone wrong?","['riemannian-geometry', 'complex-geometry', 'kahler-manifolds', 'projective-space', 'differential-geometry']"
2014670,"Suppose that $f'(x)g(x)=f(x)g'(x)$ and $g(x)\ne 0$ on (a,b). How are $f$ and $g$ related?","Suppose that $f'(x)g(x)=f(x)g'(x)$ and $g(x)\ne 0$ on (a,b). How are $f$ and $g$ related? I posted this earlier and accidentally deleted it. But thus far, I have: Let $$h(x)=\frac{f(x)}{g(x)}$$ Then
$$h'(x)=\frac{f'(x)g(x)-f(x)g'(x)}{(g(x))^2}$$ Since $f'(x)g(x)=f(x)g'(x)$, I propose we replace $f(x)g'(x)$ in $h'(x)$ and we get $$h'(x)=\frac{f'(x)g(x)-f'(x)g(x)}{(g(x))^2}$$
$$=\frac{0}{(g(x))^2}$$ Is it enough, then, to say that $f$ and $g$ are constants on $(a,b)$?","['derivatives', 'real-analysis']"
2014730,Finding domain of variables in joint density for marginal density,"Let $(X,Y)$ have joint density $f(x,y) = \frac{1}{2}(1+x+y)$ for $0<x<1$ and $0<y<1$. So the joint density of $X$ and $U=X+Y$ is $f_{X,U}(x,u)=\frac{1}{2}(1+u)$. Now it is simple to get the domain of $X$ since it is provided in the problem description, but to get the domain of $U$ seems so be trickier since you need to extract the case of $U<1$ and $U\geq 1$. I need these values in order to get the marginal densities, but I am not quite sure what I need to do after I get $0<U-X<1$.","['statistics', 'density-function']"
2014796,Wanting to know if my set up for these integrals using spherical and cylindrical coords is correct.,"So as usual I've been doing problems out my book for practice, only to find one of the answers pages with this section torn out... I don't expect to get answers for all of them here, or even full answers for a few, but I was hoping maybe some could tell me if the set up on the few I've done is correct, after all, I can check the evaluation on symbolab. At least then when trying other problems I'll have something more to go off when it comes to the set up. If this isn't an ok question or should be broken up into separate ones, just let me know, just hoping to get all the practice I can. Using Cylindrical: $(A)$ $\int\int\int (z) dv$ where E is enclosed by the paraboloid $z=x^2+y^2$ and the plane $z=4$ From the fact that it's a paraboloid I have theta between $0$ and $\pi$, r between $0$ and 2 by plugging the given z value into the paraboloid equation, and z between 4 and $r^2$ which I got from the x,y equation swapping to polar, which is the part I'm not really sure about. $\int^\pi_0\int_0^2\int^4_{r^2}(zr)dzdrd\theta$ $(B)$ $\int\int\int_E (x) dv$ where E is the solid in the first octant that lies under the paraboloid $z=4-x^2-y^2$."" Since it's the first octant I have $0<=\theta<=\pi/2$, r between 0 and 4 from the z equation, and zero through the the z equation for z: $\int^{\pi/2}_0\int^4_0\int^{4-r^2}_0[(4-r^2)r]dzdrd\theta$ Using Spherical: $(C)$ $\int\int\int(xe^{x^2+y^2+z^2})dv$ where E is the portion of the unit ball $x^2+y^2+z^2<=1$ that lies in the first octant. From the first octant part I have theta between 0 and $\pi/2$, and from $\rho^2=x^2+y^2+z^2$ I have $\rho$ between zero and 1. I'm not sure how to determine $\phi$ in this one though going off a similar problem I seen I have it between zero and $\pi/4$. Replacing all the x and y's with the matching cartesian to spherical identities: $\int^{\pi/2}_0\int_0^{\pi/4}\int_0^1(\rho sin(\phi)e^{\rho^2}\rho^2 sin(\phi))d\rho d\phi d\theta$ ...And I think I'll leave it there. Have a couple more I tried in spherical but this seems like enough for one post.","['multivariable-calculus', 'integration', 'spherical-coordinates', 'cylindrical-coordinates']"
2014823,Mean of pure birth process with distinct birth rates when odd and when even,"Let $\{N(t)\}_{t\geq 0}$ a pure birth process for which 
  $$P(N(t+h)-N(t)=1\mid N(t)\ \text{is odd})=ah+o(h)$$
  $$P(N(t+h)-N(t)=1\mid N(t)\ \text{is even})=bh+o(h)$$
  Determine $E[N(t)]$. I think that it's a good idea use the law of total expectation:
  $$E[N(t)]=E[N(t)|N(t) odd]P[N(t)odd]+E[N(t)|N(t) even]P[N(t)even]$$
How can I proceed to find the value of $E[N(t)]$?","['poisson-process', 'probability-theory', 'conditional-expectation']"
2014841,Proving $\sum_{k=1}^{n}{(-1)^{k+1} {{n}\choose{k}}\frac{1}{k}=H_n}$,I've been trying to prove $$\sum_{k=1}^{n}{(-1)^{k+1} {{n}\choose{k}}\frac{1}{k}=H_n}$$ I've tried perturbation and inversion but still nothing. I've even tried expanding the sum to try and find some pattern that could relate this to the harmonic series but this just seems surreal. I can't find any logical reasoning behind this identity. I don't understand Wikipedia's proof . Is there any proof that doesn't require the integral transform?,"['coupon-collector', 'binomial-coefficients', 'inclusion-exclusion', 'summation', 'harmonic-numbers']"
2014852,"Erratum for Billingsley’s $\textit{Probability and Measure}$, Problem 33.9(c)","This is yet another verification request on a(n allegedly) false statement in Patrick Billingsley’s Probability and Measure textbook. I have been self-studying this excellent book for quite a while and my aim here is to document some minor mistakes for my records and for the public (see also this thread and this one ). This time, I am concerned with Problem 33.9(c) on page 443 in the third edition of the book. The problem states the following: Show that conditional probabilities can be defined as genuine probabilities on spaces of the special form $(\Omega,\sigma(X_1,\ldots,X_k),\mathbb P)$. That is, suppose that the probability space $(\Omega,\mathscr F,\mathbb P)$ is such that the $\sigma$-algebra $\mathscr F$ is generated by a finite number of random variables $X_i:\Omega\to\mathbb R$. A conditional probability of any fixed set $A\in\mathscr F$ with respect to a $\sigma$-subalgebra $\mathscr G\subseteq\mathscr F$ is defined as a function $\mu(A,\omega):\Omega\to\mathbb R$ such that the function $\omega\mapsto\mu(A,\omega)$ is measurable with respect to the $\sigma$-subalgebra $\mathscr G$; the function $\omega\mapsto\mu(A,\omega)$ is integrable: $$\int|\mu(A,\omega)|\,\mathrm d\mathbb P(\omega)<\infty;\quad\text{and}$$ for any $G\in\mathscr G$, one has that $$\int_G\mu(A,\omega)\,\mathrm d\mathbb P(\omega)=\mathbb P(A\cap G).$$ It can be proven via the Radon–Nikodym theorem that conditional probabilities—according to the definition above—always exist. The extra statement to be proven in this exercise is that the conditional probabilities can be chosen in such a way that $\underline{\text{$\mu(\cdot,\omega)$ is a genuine probability measure on $\mathscr F$ for} \textit{ every } \omega\in\Omega}$, as long as the $\sigma$-algebra $\mathscr F$ is generated by a finite number of random variables. (A common terminology is that this condition requires that the conditional probability be regular .) While conditional probabilities always exist, it may be possible that the fourth underlined condition above can in no way be satisfied. Indeed, Problem 33.11 on the same page of the textbook outlines a counterexample: conditional probabilities do exist, but they cannot be constructed in such a way that the extra underlined condition above is satisfied. This counterexample in Problem 33.11 relies on the construction of a somewhat pathological yet very interesting $\sigma$-algebra, which had been discussed in detail earlier in the textbook—see Problem 4.10 on page 66. If you take the claim of Problem 33.9(c) at face value, then you must conclude that the pathological $\sigma$-algebra constructed in Problem 4.10 cannot have been generated by a finite number of random variables. Yet, if you define $X_1(\omega)\equiv\omega$ for every $\omega\in\Omega$ and $X_2$ as the indicator function of the set $H$ (in the context and notation of Problem 4.10), then it is not difficult to show that the pathological $\sigma$-algebra $\mathscr F$ in question is, in fact, $\sigma(X_1,X_2)$, the $\sigma$-algebra generated by the random variables $X_1$ and $X_2$. Therefore, the statement of Problem 33.9(c) cannot be true. In fact, I have tried very hard to prove this statement before realizing that it is false. ( This other thread is related to the process of my attempted proof.) I think the root of the problem is the following subtlety. Theorem 33.3 on page 439 yields a construction of the conditional distribution of a random variable (or random vector), which concept is closely related to that of conditional probability. This theorem shows that the conditional distribution can always be defined to be a genuine probability measure on the Borel $\sigma$-algebra on $\mathbb R$ (or $\mathbb R^k$) for every realization $\omega\in\Omega$ of the sample space. However, a probability distribution is a pushforward measure: it is defined on the real line via the corresponding random variable, and it “pushes forward” the probabilistic behavior of the random variable on the underlying sample space $\Omega$ onto the real line $\mathbb R$. What Problem 33.9(c) seems to ask to do is consider a $\sigma$-algebra generated by a finite number of random variables, take the conditional distribution (which is always well-defined pointwise), and “pull back” the random variables’ values’ probabilistic behavior on $\mathbb R^k$ onto the sample space. However, this may not be possible to do in such a way that the resulting conditional probability (defined on the sample space) is a genuine probability measure pointwise (that is, such that the underlined extra condition above holds). The reason is that while pushforward measures are easy to construct, the analogous construction of a hypothetical “pullback” measure is problematic and usually ill-defined (unless special addition conditions, such as injectivity and Borel isomorphy, are imposed). Any feedback and comments are appreciated.","['probability-theory', 'measure-theory', 'proof-verification']"
2014888,Probability of common sequences,"Let $\textbf{X}, \textbf{Y}$ be two fixed length sequences of length $d_x, d_y$, composed of a language of $K$ symbols. $p(\textbf{X}_i, \textbf{Y}_i = k) = \frac{1}{K}$, and there is no seuquential dependence to the symbols. What is the probability that $\textbf{X}$ and $\textbf{Y}$ have a common sequence of length $d'$ ($d' \leq d_x,d_y$), not necessarily in an unbroken sequence. Example: $\textbf{X} = \textbf{1}423\textbf{2}3134\textbf{21}2\textbf{4}211$ $\textbf{Y} = 423\textbf{11}31342211\textbf{4}$ $d_x = 16, d_y = 14, d'=11$. Common sequence: $42331342211$. Here's what I have so far, but I think I'm missing some things. The probability of any given sequence of length $d'$ is $(\frac{1}{K})^{d'}$. It can occur in any of the sequences ${d_x}\choose{d'}$, ${d_y}\choose{d'}$ times. So the probability that it is common to both of them is ${d_x}\choose{d'}$$*$${d_x}\choose{d'}$$*$$(\frac{1}{K})^{d'^{2}}$. Then, since we don't care about which sequence it is, I think we have to sum over all possible strings, and then we get $d'^K {d_x \choose d'} {d_y \choose d'} K^{-2d'}$.","['combinatorics', 'probability']"
2014919,Recurrence formula for the Eulerian and derangement polynomial,"For the Eulerian polynomial $$A_n(x)=\sum_{\pi\in S_n}x^{\mathrm{des}(\pi)}$$ it is well known, that we have the nice recurrence formula $$A_n(x)=\sum_{k=0}^{n-1}\binom{n}{k}A_k(x)(x-1)^{n-1-k}.$$ I would like to prove a similar formula for the derangement polynomial $$d_n(x)=\sum_{\pi\in \mathcal{D}_n}x^{\mathrm{exc}(\pi)},$$ where $\mathcal{D}_n$ is the set of derangements, i.e. permutations without a fixed point and $\mathrm{exc}$ is the number of excedances, i.e. positions with $\pi(i)>i$ . My question: Is there a combinatorial and constructive proof for the recurrence formula for the Eulerian polynomial? Especially, I am looking for a map $S_i\to S_n$ which gives us the right multiplicities (and hopefully also works for the derangements). All the literature I found so far (for example Petersen's new book on Eulerian numbers) only works with the function and some other recurrences itself to get to this recurrence. Maybe something similar would be possible for $d_n(x)$ but a combinatorial way is preferred. To prove the recurrence from the polynomial itself (i.e. by analytic methods) Petersen is using the identity $$A_n(x)=(1-x)^{n+1}\sum_{k\geq0}(k+1)^n x^k.$$ Is there a similar formula known for $d_n(x)$ ? Thanks! Richard","['eulerian-numbers', 'reference-request', 'derangements', 'permutations', 'combinatorics']"
2014940,Adding constant to the variable in a Limsup,"Let $f(t)$ be a real valued function and $C$ a constant. Is it true that $$
\limsup_{t\to\infty}\frac{f(t)}{t}=\limsup_{t\to\infty}\frac{f(t+C)}{t} ?
$$ I have tried to prove it using a change of variable $s=t+C$, but the denominator seems problematic.",['calculus']
2014980,Integer solutions (star and bar),"Consider the equation $x_1+x_2+x_3+x_4+x_5=10$. How many non-negative integer solutions if $x_1 > x_2$? Apparently counting $x_1$ and $x_2$ one by one is too slow, and impractical if the sum is not 10 but 100 instead. Is there a general way to solve this kind of problems?",['combinatorics']
2015001,Is $\mathbb{R^2} \subset \mathbb{C}$?,"Is $\mathbb{R^2} \subset \mathbb{C}$? I've heard that $\mathbb{R^2}$ is isomorphic to $\mathbb{C}$, but can we say that $\mathbb{R^2} \subset \mathbb{C}$? $\mathbb{R^2}$ is defined as $$\mathbb{R^2} = \{ (x,y) | x, y \in \mathbb{R}\}$$ and correct me if I'm wrong, but we can define $\mathbb{C}$ over the reals as follows: $$\mathbb{C} = \left\{(x, iy) | \ x, y \in \mathbb{R} \  \ \text{and} \  \ i = \sqrt{-1}  \right\}$$ But I don't see how we could show $a \in \mathbb{R^2} \implies a \in \mathbb{C}$. A counterexample could be that $(1,1) \in \mathbb{R^2}$, but $(1,1) \not\in \ \mathbb{C}$, however $(1, i) \in \mathbb{C}$. So is my conclusion that $\mathbb{R^2} \not\subset \mathbb{C}$ correct?","['complex-analysis', 'linear-algebra', 'complex-numbers', 'vector-spaces']"
2015072,Each finite cyclic group of order $n$ contains unique subgroup of order $d$ where $d\mid n$,"A cyclic group $G := \langle x \rangle$ of finite order $n$ has a
  unique subgroup of order $d$, namely $$\langle x^{n/d} \rangle = \{g
 \in G : g^d = 1\}$$ for every divisor $d$ of $n$. I wanted to show the equality $\langle x^{n/d} \rangle = \{g \in G : g^d = 1\}$. Now for the inclusion $\subseteq $ we take an element of $\langle x^{n/d} \rangle$ which has the form $x^{kn/d}$ for some natural number $k$. Hence $$(x^{kn/d})^d = x^{kn} = (x^n)^k =1^k = 1$$ and so $x^{kn/d} \in \{g \in G : g^d = 1\}$. However I am a bit lost proving the inclusion $\supseteq$.","['abstract-algebra', 'group-theory', 'cyclic-groups']"
2015077,why scale a normal distribution by the square root of the variance?,"Given a standard normal distribution $N(0,1)$ why, if you wish to scale it, you need to multiply by the square root of the variance? 
Ie given a variance t, the new scaled distribution is $N(0, t)$ which is equivalent to $\sqrt{t}*N(0,1)$? Isn't the pdf of the general normal $\frac{1}{\sigma}$*(standard normal pdf)?
An intuitive explanation and proof would be quite helpful.",['probability']
2015131,"If $(1+\sqrt{2})^n=a_{n}+b_{n}\sqrt{2}\;,(\forall n\in \mathbb{N}).$Then $\lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $","Let $a_{n},b_{n}\in \mathbb{Q}$ such that $(1+\sqrt{2})^n=a_{n}+b_{n}\sqrt{2}\;,(\forall n\in \mathbb{N}).$Then $\displaystyle \lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $ $\bf{My\; Try::}$ Using $$(1+\sqrt{2})^n = 1+\binom{n}{1}(\sqrt{2})+\binom{n}{2}(\sqrt{2})^2+\cdots \cdots +\binom{n}{n}(\sqrt{2})^n$$ So $$a_{n} = \binom{n}{0}+\binom{n}{2}\cdot 2+\binom{n}{4}\cdot 2^2+\cdots \cdots $$ So $$b_{n} = \binom{n}{1}+\binom{n}{3}\cdot 2+\binom{n}{5}\cdot (2)^2+\cdots$$ Now How can i solve $$\lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $$ Help required, Thanks",['limits']
2015225,On the uniqueness of a simple continued fraction,"Expanding an irrational number in form of a simple continued fraction each of the terms $a_i$ in $[a_0;a_1, a_2, a_3, \dots ]$ are 'uniquely' determined since the lowest integer less than or equal to some positive number is unique. But does the uniqueness hold in the reverse direction? That is, is it true that if both $x=[a_0;a_1, a_2, a_3, \dots ]$ and $x=[b_0;b_1, b_2, b_3, \dots ]$ then $a_i=b_i$? If so, how to prove it? PS The same problem can happen for $x$ as a rational but if I can know the method for an irrational case so may I work on the rational case.","['number-theory', 'continued-fractions', 'elementary-number-theory']"
2015233,"Find the value of $\lim\limits_{n \to \infty}n\left(\left(\int_0^1\frac{1}{1+x^n}\,\mathrm{d}x\right)^n-\frac{1}{2}\right)$","Find the value of the limit $$\lim_{n \to \infty}n\left(\left(\int_0^1 \frac{1}{1+x^n}\,\mathrm{d}x\right)^n-\frac{1}{2}\right)$$ I can't solve the integral $\int_0^1 \mathrm{\frac{1}{1+x^n}}\,\mathrm{d}x$. But maybe the questions doesn't require solving the integral. Apparently the $\lim_{n \to \infty}(\int_0^1 \mathrm{\frac{1}{1+x^n}}\,\mathrm{d}x)^n$ should be $\frac{1}{2}$ for the question to make sense. That's all I know.","['integration', 'limits']"
2015312,Parametrization of surface of revolution,"A surface of revolution is obtained by picking a curve $C$ on a plane and rotating it around some axis contained on the plane. Choosing the coordinates in a convenient manner, we can pick $C$ to be contained on the plane $xz$ and chose the axis to be the $z$ axis. In that case, if $R_z(\theta) : \mathbb{R}^3\to \mathbb{R}^3$ is a rotation around the $z$ -axis, the surface of revolution will be $$S = \{R_z(\theta)\cdot p \in \mathbb{R}^3 : p\in C, \theta\in [0,2\pi]\}.$$ Now, I want to show that $S$ is a regular surface, in the sense that it satisfies Do Carmo's definition: A regular surface is $S\subset \mathbb{R}^3$ such that for each $p\in S$ there is $V$ open in $S$ , $U\subset \mathbb{R}^2$ open in $\mathbb{R}^2$ and $\mathbf{x}: U\to V$ such that $\mathbf{x}$ is differentiable, $\mathbf{x}$ is injective, $\mathbf{x}$ has injective derivative, that is, $d\mathbf{x}_q : \mathbb{R}^2\to \mathbb{R}^3$ is injective for each $q\in U$ . Now, without thinking too much about domains for a while, a natural choice for $\mathbf{x}$ would be as follows: we parametrize $C$ by $\alpha : I\subset \mathbb{R}\to \mathbb{R}^3$ and define $\mathbf{x} : [0,2\pi]\times I\to S$ $$\mathbf{x}(\theta,t)=R_z(\theta)\cdot \alpha(t).$$ This has two problems: $[0,2\pi]\times I$ is not open. Furthermore, $R_z(0)=R_z(2\pi)$ , hece we would not have injectivity. This problem would be solved if we replace $[0,2\pi]$ by $(0,2\pi)$ and if $I$ is an open interval. I can't identify the open set $V\subset S$ used with this. I mean, I know that this $\mathbf{x}$ should work, but I don't know how to find open $V$ in $S$ for each $p\in S$ so that there is one $\mathbf{x}$ like that. I mean, defining $\mathbf{x}$ isn't that hard. What I'm finding quite complicated is finding the correct domains on $\mathbb{R}^2$ and $S$ , and then proving inside the domains that $1,2,3$ are satisfied. How can this be done in this general context?","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'surfaces', 'analysis']"
2015319,How to do Statistical hypothesis testing,"Let $\Omega$ be the finite sample space and random variable $X$ is defined on $\Omega$ sending $x$ to $x$. The probability distribution function of $X$ is denoted by $f(x;p_1,..p_r)$ where $p_i$ are parameters. Given a sequences $S=\{a_{i}\}_{i=1}^{n}$ where $a_{i} \in \Omega$, I want to know whether this sequence is sampled according to that probability distribution $(\Omega,f(x))$ or not. For example $\Omega = \{0,1\}$,
\begin{eqnarray}f(x;p)=
\begin{cases}
p, &x=0\cr 1-p, &x=1 \cr 0, &otherwise\end{cases},
\end{eqnarray}
$p=0.1$,$n=10$,$S=0100100011$, Then how to do Statistical hypothesis testing ?","['statistical-inference', 'probability-theory', 'sampling', 'hypothesis-testing', 'statistics']"
2015337,Let $G$ be a group with 4 elements. Show that it is abelian.,"Hey so i tried this but i dont really know if its correct. I need some suggestions. Let $G$ be $\{e,a,b,c\}$ Let's assume $G$ is not abelian. This means there exist $a,b$ such that $ab\neq ba$. We can choose 2 elements $a, b$ who are not inverse to each other. We know that $(ab≠b \land ab≠e \land ab≠a) \implies ab=c$; we also know that $(ba≠b \land ba≠e \land ba≠a \land ab≠ba) \implies ba=d$ which is a contradiction to our assumption that G had 4 elements. Is this proof legit ?","['finite-groups', 'group-theory', 'proof-verification']"
2015382,Special Woodbury identity,"I wish to understand linear transformations of the type $M = \left(\alpha I_n+ D^TD\right)^{-1}$ where $\alpha \neq 0$ and $D$ is a (full-rank) $k$ by $n$ matrix. ($k<n$) My understanding is that though $D^TD$ is singular and thus does not have an inverse, $\left(\alpha I_n+ D^TD\right)^{-1}$ for $\alpha \ll 1$ could be a ""good"" approximation for it. (or am I mistaken?) Ideally I would like to relate the eigenvectors of $M$ to my original $D$ (probably dependent also on $\alpha$) (see footnote [1]) So my question is: Is it possible to compute the eigenvalues and eigenvectors of this matrix $M$? 
Is there any interesting fact that you know about these types of matrices (thinking them of linear transformations) I tried to use Woodbury identity to arrive at an expression $$M = \left(\alpha I_n+ D^TD\right)^{-1}\\
=\alpha^{-1}I_n - \alpha^{-1}D^T\left(I_k+\alpha^{-1}DD^T\right)^{-1}D\alpha^{-1}\\
=\alpha^{-1}I_n - \alpha^{-1}D^T\left(\alpha I_k+DD^T\right)^{-1}D\\
= \alpha^{-1}I_n - \alpha^{-1}D^T\left(\alpha^{-1}I_k - \alpha^{-1}D\left(\alpha I_n+D^TD\right)^{-1}D^T\right)D\\
= \alpha^{-1}I_n - \alpha^{-2}D^TD + \alpha^{-2}D^TD\left(\alpha I_n+D^TD\right)^{-1}D^TD\\
= \alpha^{-1}I_n - \alpha^{-2}D^TD + \alpha^{-2}D^TDMD^TD$$ and got to an implicit equation for $M$ but could not solve it. Any help is appreciated either in finding a closed-form solution or geometrical interpretations of the effect of $M$ [1] Some initial experimentation with simple $D$ lead me to believe that the rows of $D$ are eigenvectors for $M$ is this possible?","['eigenvalues-eigenvectors', 'matrix-equations', 'linear-algebra']"
2015424,Find the maximum value of the $f(x) = \sqrt {8x - x^2} - \sqrt {14x - x^2 - 48}$.,"The largest value of the function $f(x) = \sqrt {8x - x^2} - \sqrt {14x - x^2 - 48}$ is written in the format $""m \sqrt n""$. Find $m + n$. My attempt: A traditionalway would be to differentiate the function w.r.t. $x$ and equate it to $0$. $$f'(x) = \frac {8 - 2x}{\sqrt {8x - x^2}} - \frac {14 - 2x}{\sqrt {14x - x^2 - 48}}$$ From here, I concluded that : $6 < x < 8$ Equation $f'(x)$ to $0$, I get $x = 6.4$. But that is not the answer. Answer is indeed $x = 6$ and thus $m+n = 5$. (m and n are integers) My first doubt is $6$ gives $f'(x)$ as '$- \infty$', which is not a maxima or minima because we know that slope of any curve at maxima or minima is $0$ and thus $f'(x)$ should be $0$. Second is that even if we suppose that $6$ is the answer, why was $6.4$ not taken as the answer.","['derivatives', 'maxima-minima', 'functions']"
2015438,Prove that $\tan 3a = \tan 3b = \tan 3c$ if $\sin a + \sin b + \sin c = 0$ and $\cos a + \cos b + \cos c = 0$,"Let $a, b$ and $c$ be three angles such that:
$$\sin a + \sin b + \sin c = 0$$
$$\cos a + \cos b + \cos c = 0$$ Prove that $\tan 3a = \tan 3b = \tan 3c$. I haven't done anything meaningful yet on this problem because I have no idea how I should start. Thank you in advance!",['trigonometry']
2015468,Limit of $\left(x^{3} - 1\right)\left(x^{2} - 1\right)^{1/3} \over 5\ln\left(x\right)$ when $x\to1^{+}$,"I am searching a way to solve an equation with the asymptotic method; we can't use the hopital nor sostitution, but we must find a way to solve it using just significant limitations and basic properties of limits. $\displaystyle{\left(x^{3} - 1\right)\left(x^{2} - 1\right)^{1/3} \over 5\ln\left(x\right)}$.
The limit of this functon as it approches to  $1^{+}$. Thanks a lot for the help, I would appreciate to understand not just the result ( that I know ) but you could make it. Thanks.","['limits-without-lhopital', 'asymptotics', 'calculus', 'limits']"
2015505,empirical quantile function - uniform convergence,"Let $X_1,...,X_n$ denote independent and identically distributed random variables, with $X_i \sim F$, $1 \leq i \leq n$. Assume $F$ is continuous. Then we know that its generalized inverse (quantile function) $F^{\leftharpoonup}(u):= \inf\{x: \, F(x)>u\}$ exists. If $F_n$ denotes the empirical cumulative distribution function, i.e. $F_n(x)=\frac{1}{n} \sum_{i=1}^n \textbf{1}(X_i \leq x)$, by Glivenko-Cantelli we know $\Vert F_n - F \Vert_\infty \overset{a.s.}{\to}0$. Now, what can we say about $ \Vert {F_n}^{\leftharpoonup}- F^{\leftharpoonup} \Vert_\infty $? Are there conditions under which a.s. uniform convergence can be obtained for the empirical quantile function ${F_n}^{\leftharpoonup}$?","['statistics', 'probability', 'asymptotics']"
2015507,Does $\lim\limits_{x\to0}\operatorname{sgn} (x)$ exist?,"I have a problem with this exercise Does this limit exist? $$\lim\limits_{x\to0} \operatorname{sgn} (x)$$ this limit should exist and its value is $0$ according to our textbook. It is also written, that we can prove it by using one-sided limits.
And there is a problem, because as I see it $$\lim_{x\to0^-} \operatorname{sgn} (x) = -1$$ $$\lim_{x\to0^+} \operatorname{sgn} (x) = 1$$ (Because the limit goes very close to $0$, but it never reaches it. I also think it is very similar to prove of non-existence $\displaystyle \lim_{x\to0} \sin\frac 1 x$) I also tried online limit calculators and they said, that one-sided limits equals $0$. Could you help me find a problem in my approach? Thanks for your time!",['limits']
2015556,"Bijection from $[0, 1]\cup(1,2)\cup${$3$} to ($0, 1$)","I need a bijection from $[0, 1]\cup(1,2)\cup${$3$} to $(0, 1)$. At first I thought it was a trivial problem, but after struggling with it for some time I think it's harder than it looks. For instance, if you try to try to send $(0, 1)$ to $(0, \frac{1}{2})$ and {$3$} to $\frac{1}{2}$ and ($1, 2$) to ($\frac{1}{2}, 1$), then you are still left with a place to send $0, 1$ since they are included in [$0, 1$]. I've tried some variations but can't get it. Help/hints would be appreciated.",['elementary-set-theory']
2015577,Find all positive integers $n>1$ such that $\frac{n(n+1)}{2}-1$ is a prime,"For $n=2$ and $n=3$, $\frac{n(n+1)}{2}-1$ is a prime. How to prove that there are no more possibilities for $n>3$?","['prime-numbers', 'discrete-mathematics']"
2015579,Integral with $sin^3(\theta)$,"Me and one other person got this problem from a pdf online, $\int^{\pi/4}_0sin^3(\theta)d\theta$ The answer in the pdf is $2-\frac{5}{2\sqrt{2}}$ with a decimal value around $0.232233$. All of us are getting and answer that comes out to $0.07741$ or $(\frac{1}{6\sqrt{2}}-\frac{1}{\sqrt{2}})+\frac{2}{3}$. We changed it to $sin^2sin$ and used $sin^2=1-cos$ and the then $u = cos$ and $du = -sin$to cancel out the remaining sin. And after doing the integration got $[\frac{cos^3(\theta)}{3}-cos(\theta)]^{\pi/4}_0$ and then our final answer of  $(\frac{1}{6\sqrt{2}}-\frac{1}{\sqrt{2}})+\frac{2}{3}$. We can't figure out hwo they got the other answer.","['multivariable-calculus', 'integration']"
2015649,normal distribution help,"The tensile strength (ksi) of a certain type of steel at room temperature is normally distributed with a mean of 100 ksi and a standard deviation of 8.2 ksi. If X is the tensile strength of a randomly selected sample of this steel find P(X>115); well, since this is normal distribution, I found this to be P(Z/sqrt(8.2) > 5.115) by using x-mew / sigma > 115 - mew / sigma
but i can't find the value since the z table only goes up to 3.49. any help would be appreciated Agggh! I somehow misread the question. many thanks to pointing it out!","['statistics', 'normal-distribution']"
2015770,Norm preservation properties of a unitary matrix,"Tried to prove the following facts: If $B \in M_{n\times n}(\mathbb{C})$ is unitary  (i.e. $ B^{-1} = B^{*}$ ), then: $\forall x\in \mathbb{C}^n :\| Bx \|_2 = \| x \|_2  $ $\forall A\in M_{n\times n}(\mathbb{C})$ : $ \| BA \|_F = \| A \|_F  $ (where $ \| . \|_F $ denotes Frobenius norm). $\forall A,B\in M_{n\times n}(\mathbb{C}): \| AB \|_{ \infty} \leq \| A \|_{ \infty}\| B \|_{ \infty}   $ In (1),tried to conduct direct algebraic manipulations from the definitions of the norms, but I obtained no results. 
  In (2) the use of definition $ \| B \|_{ \infty} := max\{\frac{\|Bx\|}{\|x\|}: x\in \mathbb{C}^n \setminus \{0\} \}$ and the inequality $\|AB\| \leq \|A\|\|B\|$ gives straightforward result. However, the use of definition $\|B\|\ _{\infty} :=max\{ \sum_{j=1}^{n}|b_{ij}|: i\in \{1,...n\}\}$ does not give direct results. I would be thankful for hints/advices!","['matrices', 'normed-spaces', 'linear-algebra', 'unitary-matrices']"
2015831,"Does there exist a continuous $f:\Bbb R^2\to\Bbb R$ not expressible as a finite sum $f(x,y)=\sum_i g_i(x)h_i(y)$?","This feels like it should be very trivially false, but I'm struggling to find some criterion that holds for all functions $\sum_i g_i(x)h_i(y)$ that does not hold for all continuous $\Bbb R^2\to\Bbb R$, and I haven't managed to construct any obvious counter example.","['general-topology', 'real-analysis', 'examples-counterexamples']"
2015838,"$n$ by $n$ matrices such that $Ax=0$ implies $Bx=0$, then what can we say about $A$ and $B$?","Ok, this is one of the interesting questions I encountered in my GRE Maths exam 2 weeks ago. Fix an $x\in \mathbb{R}^n$ (doesn't have to be $0$), and $A$, $B$ are $n$ by $n$ matrices such that $Ax=0$ implies $Bx=0$, then what can we say about $A$ and $B$? I think the choices were: $\exists C$ an $n$ by $n$ matrices such that: 
a) $A=BC$, b) $A=CB$, c) $B=CA$, d) $B=AC$, e) $B=C^{-1}AC$. Well, the set of matrices $D$ such that $Dx=0$ form an left ideal, but I only know what bi-ideals look like in this case. Also the wording the this question is funny, we dont know if $A$ is in the ideal or not, we just know if $A$ is in the ideal then $B$ is in the ideal. Edit: As we discussed below, I think I misunderstood the question in the exam (although I don't have the actually paper now). $x$ is probably not fixed.","['matrices', 'gre-exam', 'linear-algebra', 'ideals']"
2015845,Hadamard's inequality - alternative proof(the use of Lagrange multiplier method),"Heard that Hadamard's inequality: $$\left|\det(A)\right| \leq {\prod_{i=1}^{n}\sqrt{\sum_{j=1}^{n} |a_{ij}|^2} } $$ can be proved by the use of Lagrange multiplier methods. I saw and understand the proof by the use of methods from linear algebra. However, I do not see how to apply such tool from mathematical analysis to detive the aforementioned proof. If anyone would be able to provide proof, which uses Lagrange multiplier method, I would be very thankful!","['real-analysis', 'inequality', 'determinant', 'contest-math', 'analysis']"
2015853,Conditional expectation of $X_1$ given $S_n$,"Let $(\Omega, \mathcal{F}, P)$ be a probability space, and let $X_1,X_2,\dots$ be a sequence of i.i.d. random variables. Let $$S_n:=X_1+\cdots+ X_n,$$ and define $\mathcal{G}:=\sigma(S_n)$, the $\sigma$-field generated by the sum $S_n$. Compute $E(X_1\vert S_n)$. For reference, by definition, $E(X_1\vert S_n):= E(X_1\vert\sigma(S_n))$, where the latter is the a.s. unique $\sigma(S_n)$-measurable random variable such that $$\int_A E(X_1\vert\sigma(S_n))\,dP=\int_A X_1\,dP\quad\text{for all }A\in\sigma(S_n).$$ My question is if the independence of the $X_i$ is unnecessary? This seems to be because my first attempt only requires the fact that they're identically distributed: $$S_n = E(S_n\vert S_n) = E(X_1\vert S_n) +\cdots + E(X_n\vert S_n) =: Z_1+\cdots Z_n.$$ The first equality is immediate from the definition and the second is from linearity. Then for any $1\le i< j\le n$ and any $A\in\sigma(S_n)$ we have that $Z_i$ and $Z_j$ are $\sigma(S_n)$-measurable and: $$\int_A Z_i\,dP = \int_A X_i\,dP = \int_A X_j\,dP = \int_A Z_j\,dP,$$ since the $X_i$ are identically distributed and $\sigma(S_n)\subset \mathcal{F}$. Thus by definition we see that $Z_i=Z_j$, so that $E(X_1\vert S_n) = E(X_i\vert S_n)$ for all $i=1,\dots, n$. Hence $$S_n = n\cdot E(X_1\vert S_n) \implies E(X_1\vert S_n) =\frac{1}{n} S_n.$$ Of course, in most of the equalities I mean almost sure equality. It doesn't seem like independence is necessary, but I want to verify.","['probability-theory', 'conditional-expectation', 'proof-verification']"
2015865,Find the exponential generating function for the number of ways to distribute $r$ people into 6 different rooms with between 2 and 4 in each room.,"Find the exponential generating function for the number of ways to distribute $r$ people into six different rooms with between two and four in each room. I understand that an exponential generating function is of the form: $$ a(x) = \sum_{i=0}^{\infty} \frac{a_i}{i!} x^i$$ My approach to it is that since each room can have either 2, 3 or 4 people in it, then we need to account for the cases where $i=2$ , $i=3$ , and $i=3$ . This means that the exponential generating function would be $$ \frac{a_2}{2!}x^2 + \frac{a_3}{3!}x^3 + \frac{a_4}{4!}x^4 $$ Since each person being distributed can be treated as the same (only the amount of people being distributed matters) then the integer sequence is just going to be $(1, 1, 1, ...)$ . Plugging in $a$ gives us $$\frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!}$$ However, this equation is just accounting for the number of ways to distribute people in one room. Since there are 6 rooms, and the amount of ways to distribute people in all the room together is just $$ (\frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!})^6 $$ Is this correct?","['generating-functions', 'combinatorics']"
2015872,"What is the ""differential form"" of an ODE?","I'm reading an introductory textbook on Ordinary Differential Equations and the author does something I think needs some justification. Here it goes: Consider an ODE $y' = f(x,y)$, so we can write $f(x,y)=\frac{-M(x,y)}{N(x,y)}$. Therefore $y'=\frac{-M(x,y)}{N(x,y)}$, which we can write in ""differential form"" as $M(x,y)dx+N(x,y)dy=0$. Question: What is exactly the ""differential form"" of an ODE? Is it just a purely formal reasoning or there is some elementary context in which we can interpret the $dx,dy$ in the above equation? Thank you.",['ordinary-differential-equations']
2015915,How to show that trajectories are either ellipses or lines,"Equations of motion for a pendulum came out to be $$x(t) = c_1\cos(\omega t)+c_2 \sin(\omega t)$$
$$y(t) = c_3\cos(\omega t)+c_4 \sin(\omega t)$$ How can it be shown that the trajectories of these solutions are either ellipses or lines? I think one can work directly with the two ODEs, rewrite them as a decoupled system of four linear ODEs, and then analyze their eigenvalues and sketch the phase portrait based on that. But in this case, a simpler approach is needed. Here's what I've done so far: $x(t) = A\cos(\omega t + \delta)$ can be rewritten as $x(\tau) = A\cos(\omega\tau)$, where $\tau := t+\frac{\delta}{\omega}$. The question is now how to rewrite $y$ in such a way is to be able to substitute $x$ into $y$ and get a suitable equation for trajectories. I took $y(\tau) = B\sin(\omega \tau)$, hence we get, after rearrangement and division, the following equation $$ \frac{y^2}{B^2} + \frac{x^2}{A^2}=1 $$, which is clearly an ellipse. But what about lines? If $B=0$ then $y^2 + \frac{B^2}{A^2}x^2=y^2=1$, so $y=\pm 1$, which are horizontal lines. What about lines in general? Would appreciate your feedback.","['physics', 'ordinary-differential-equations', 'geometry']"
2015920,Multivariable limit - perhaps a trickier problem I am stuck on. [duplicate],"This question already has answers here : Check if limit exists and its value (2 answers) Closed 5 years ago . I am trying to solve the following limit: $\lim_{(x,y) \to (0,0)} \frac{x^4y^4}{(x^2 + y^4)^3}$ (This is a more challenging problem from Folland Calculus, it seems). I am pretty sure this limit does not exist (however, this is just a guess, and I am not 100% sure.) I am trying to approach via the paths y = mx and x = my but... they don't seem to work. I have tried simpler cases such as x = 0 and y = 0. Any help appreciate.",['multivariable-calculus']
2015921,What's the maximum number of points at distance r or more inside a d-dimensional sphere of radius r?,"We have a sphere of radius $r$ in a $d$-dimensional space. What is the maximum amount of points that I can fit inside the sphere such as the distance between any pair of points is at least $r$? And strictly bigger than $r$? I believe this is equivalent to packing d-dimensional spheres of radius r/2 inside a sphere of radius r. If you have an idea on the order of the answer I would also appreciate it. Thanks! Related questions: This question says that the number is 12 for d=3, what about for a general d? As opposed to this question , I'm only concerned for points at distance $r$, not any arbitrary distance.","['packing-problem', 'discrete-geometry', 'computational-geometry', 'spheres', 'geometry']"
2015947,Continuity - Why does it require open sets in this definition?,"My notes claim the following theorem: A function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous if and only if whenever $U \subseteq R^m$ is an open set, then $f^{-1}(U) \subseteq \mathbb{R}^n$ is an open set. Why do we require both sets to be open? What if they were closed?",['multivariable-calculus']
2015956,Distribution-free statistics on compact Lie groups,"Here is some background. Let $(X_i)_{i=1}^n$ be iid random variables with joint cdf $F$. Recall that the empirical distribution function is:
$$
F_n(x) = \frac{1}{n} \sum_{i=1}^n \chi_{[-\infty,x]}(X_i) .
$$
Note that $F_n$ is a function-valued random variable. It is known that the Kolmogorov–Smirnov statistic:
$$
\mathrm{KS}_n = \|F-F_n\|_\infty ,
$$
converges almost surely to zero. Even better, the normalized statistic:
$$
K_n = \sqrt{n}\cdot \mathrm{KS}_n 
$$
converges to the Kolmogorov distribution (sup of the Brownian Bridge), which doesn't depend on the distribution of the $X_i$. Let $G$ be a compact Lie group (semisimple, if that helps). Is there a similar statistic measuring how close a sequence in $G$ is to a given probability distribution, such that the limiting statistic is distribution-free? Any ideas or pointers to a reference would be appreciated!","['statistics', 'probability', 'lie-groups']"
2015963,Meaning of square brackets in Euler's summation formula,"I am working my way through Apostol's text on number theory. Euler's summation formula is given as: $$
\sum_{y < n \leq x}f(n) = \int_y^x f(t)dt + \int_y^x(t-[t])f'(t)dt + f(x)([x] - x) - f(y)([y] - y)
$$ My question is, what is the meaning of the square bracket notation?","['number-theory', 'notation', 'elementary-number-theory']"
2015981,Is it possible for a continuous function to have a nowhere-continuous derivative? [duplicate],This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) How discontinuous can a derivative be? (1 answer) Closed 7 years ago . This is motivated by a question I saw elsewhere that asks whether there is a real-valued function on an interval that contains no monotone subintervals. Edit: Note that I am asking for a function whose derivative exists but is not continuous anywhere.,"['derivatives', 'real-analysis', 'examples-counterexamples', 'calculus', 'continuity']"
2015991,Usefulness of the characteristic function (set-theory),"First a definition for those who need it: If $S$ is a set and $S_0$ is a subset of $S$ then the characteristic function of $S_0$ as a subset of $S$ is the function $f$, with domain $S$ defined by $f(s) = 1$ if $s \in S_0$ and $f(s) = 0$ if $s \notin S_0$. The definition is clear and I understand the proof of the theorem that for any set $S$ the set of any characteristic function with domain $S$ has the same cardinality as the power set of $S$. But I'm failing to see what purpose it will play in what sorts of problems. Can anyone give an example of a problem for which the solution includes introducing the characteristic function of a set? Edit I believe it is also known as the Indicator function (as per Wikipedia). Edit 2 I mostly mean applications to problems in elementary set theory.",['elementary-set-theory']
2016019,Why do we define vector spaces over a general $\mathbb{F}$ (field) rather than $\mathbb{C}$ (complex numbers)?,"In my linear algebra class, many times we define vector spaces over the field $\mathbb{F}$ (i.e. $\mathbb{F}^n$) and then prove things about them. The instructor has defined $\mathbb{F}$ as ""either $\mathbb{C}$ or $\mathbb{R}$"". Because $\mathbb{R}$ is a subset of $\mathbb{C}$, why can we not simply define vector spaces over $\mathbb{C}$ instead of introducing new notation? Isn't it true that a property holding over  $\mathbb{C}$ immediately tell us it holds over $\mathbb{R}$ because $\mathbb{R}\subset \mathbb{C}$?","['linear-algebra', 'vector-spaces']"
2016033,Doubt in Pratt's Lemma,"Pratt's Lemma: Let $X_n \leq Y_n \leq Z_n, n=1,2 \cdots$. Suppose with probability $1$, $X_n\rightarrow X, Y_n\rightarrow Y \ \rm{and}  \ Z_n\rightarrow Z $. Show that if $E[Z_n]\rightarrow E[Z]$ and $E[X_n]\rightarrow E[X]$, then $E[Y_n]\rightarrow E[Y]$. Proof: $Z_n-Y_n\geq 0$ and $Y_n-X_n\geq 0$, By Fatou's Lemma, $E(Z-Y)\leq \liminf E(Z_n-Y_n)$, hence $\limsup E(Y_n)\leq E(Y)$. Similary,$E(Y)\leq \liminf E(Y_n)$. Thus, $\lim E(Y_n)=E(Y)$. What I don't understand is that $X$ and $Z$ are not given to be integrable, so how can we cancel $E(Z)$ and $E(X)$?","['probability-theory', 'probability', 'measure-theory']"
2016045,How to solve the differential equation: $yy'^2-2xy'+y=0$,"Solve the differential equation: $yy'^2-2xy'+y=0$ (*) I have $(*)\Leftrightarrow (yy')^2-2xyy'+y^2=0\\
\Leftrightarrow z'^2-4xz+4z^2 =0 \ \ (z=y^2)$ I can't know how to continue. Could you give me some hints. Thanks for helping.",['ordinary-differential-equations']
2016047,How can I find the perimeter of a concave pentagon?,"We know that a regular pentagon has five sides with identical lengths. But, the irregular pentagon has five sides with different angles. and moreover, the perimeter of a regular pentagon is 5a, where a is the side length of the regular pentagon. But how can I find out the formula of perimeter of an irregular concave pentagon? There is given a figure of a concave pentagon. Here in the picture, the length of the AB and CD are Equal and the length of AB and BC are given. I have to find out the perimeter of the concave pentagon. I couldn't able to solve this anyway.","['algebra-precalculus', 'polygons', 'geometry']"
2016062,Nilpotent and cyclic quotient group implies abelian,"I'm stuck on the follow question, and honestly, I cannot think of a way to deal with it. Does anyone have any suggestions?  Any hints would be very appreciated! Thanks Suppose that $G$ is a nilpotent group and that $G/G'$ is a cyclic
  group, where $G'$ denotes the commutator subgroup of $G$. Show that
  $G$ is abelian. Intuition:  My idea is to do something along the line of induction on the length of the upper central series that $G$ has, and showing that, if $1 =G_n \leq \dots \leq G_2=G' \leq G_1=G $, then $1=G_n = \dots = G'$.","['nilpotence', 'finite-groups', 'abstract-algebra', 'solvable-groups', 'group-theory']"
2016076,Does $\sum\frac{4^n(n!)^2}{(2n)!}$ converge?,"Does $\sum\frac{4^n(n!)^2}{(2n)!}$ converge? The ratio test is inconclusive. I know that $\sum\frac{2^n(n!)^2}{(2n)!}$ converges, if that's of any help.","['real-analysis', 'sequences-and-series', 'calculus', 'analysis']"
2016079,"What is knot theory about, exactly?","""In topology, knot theory is the study of mathematical knots."" This is how Wikipedia defines knot theory. I have no idea what this is supposed to mean, but it does seem interesting. The rest of the article is full of examples of knots, their notation and such, which I understand a little bit better, but I still fail to understand why and how they are studied. So, what exactly is knot theory about? What characteristics of knots are studied, and how does it connect to the rest of mathematics? Full disclosure: I have experience with analysis, know at least the basics of abstract algebra, and I think I could understand elementary topology. So feel free to give me only a moderately technical description.","['knot-theory', 'soft-question', 'geometry', 'algebraic-topology', 'general-topology']"
2016094,"For $g_1,g_2$ in finite abelian group with $g_1\neq g_2$, there is $\phi\in\mathrm{Hom}(G,\mathbb C^\times)$ such that $\phi(g_1)\ne\phi(g_2)$","I am having trouble with the following problem, taken from Herstein's Topics in Algebra 2nd Edition: If $G$ is an abelian group, let $\hat G$ be the set of all homomorphisms of $G$ into the group of nonzero complex numbers under multiplication.
If, $\phi_1,\phi_2\in \hat G$ , define $\phi_1 \cdot \phi_2$ by $\phi_1 \cdot \phi_2(g) = \phi_1(g) \phi_2 (g)$ for all $g \in G.$ Show that, given $G$ is also finite and $g_1\neq g_2$ are in $G$ , there is a $\phi \in \hat G$ with $\phi (g_1) \neq \phi (g_2)$ . I have already proved that $\hat G$ is abelian and that for $\phi \in \hat G$ we have $\phi(g)$ a root of unity for every $g \in G$ . I also know that $G$ is isomorphic to $\hat G$ . What I am basically proving is that each $\phi \in \hat G$ is one-to-one. Is that correct? How can I leverage the fact that $G$ and $\hat G$ are isomorphic to show that each element of $\hat G$ is in fact an isomorphism?","['finite-groups', 'abstract-algebra', 'abelian-groups', 'group-theory']"
2016132,Showing lower-semi-continuity of a function.,"Let $f_\lambda: \mathbb{R} \rightarrow \mathbb{R} (\lambda \in \Lambda)$ be a family of continuous functions. Let 
$$ F(x) = \sup_{\lambda \in \Lambda} f_\lambda (x), x \in \mathbb{R}.$$ 
Show that $F$ is lower-semi-continuous. My attempt: We must show that $\liminf_{x \rightarrow x_0} F(x) \geq F(x_0)$ for all $x_0 \in \mathbb{R}$.
Since each $f_\lambda$ is continuous, it is also lower-semi-continuous. Thus, $$\liminf_{x \rightarrow x_0} f_\lambda(x) \geq f_\lambda (x_0).$$ Is it true then that $\liminf_{x \rightarrow x_0} \sup f_\lambda(x) \geq \sup f_\lambda (x_0)$? Why? Is there a way I should justify this? Is it because $\liminf_{x \rightarrow x_0} \sup f_\lambda(x) \geq \sup (\liminf_{x \rightarrow x_0} f_\lambda(x)) \geq \sup f_\lambda (x_0)?$","['real-analysis', 'analysis', 'proof-verification']"
2016144,What type of bifurcation occurs from the transition from a saddle to a center,"If I have a system that takes the form
$\dot x =by$ $\leftrightarrow$ $\dot y = y^2 + x$,
I get a Jacobian matrix at the origina of the form $$
        \begin{matrix}
        0 & b & \\
        1 & 0 & \\
        \end{matrix}
$$ We see that $\tau$ = 0 and $\Delta$=-$b$
This means that that eigenvalues $\lambda$ = $\pm$$\sqrt b$.
As the value $b$ passes through 0 the behavior around the origin goes from a center to a saddle. What kind of bifurcation is this called?","['dynamical-systems', 'bifurcation', 'parametric', 'nonlinear-system', 'ordinary-differential-equations']"
2016158,Complex exponential series convergence $\sum_{n=0}^{\infty} \frac{z^n}{n!}$,"I'm hoping someone can help me with the following question from Stein and Shakarchi Volume 1 on Fourier Analysis (p. 24): For $z \in \mathbb C$, we define the complex exponential by 
$$\sum_{n=0}^{\infty} \frac{z^n}{n!}.$$ 
Prove that the above definition makes sense, by showing the series converges for every complex number $z$. Moreover, show that the convergence is uniform on every bounded subset of $\mathbb C$. I know that convergence is easily shown with the ratio test or an absolute convergence argument, but I'm trying to prove convergence formally, and keep getting stuck. So far, I've tried the following. A series converges if the sequence of partial sums 
$$\sum_{j=0}^{n} \frac{z^j}{j!}$$ 
converges as $n \to \infty$. So showing that 
$$\lim_{n \to \infty} \left| \sum_{j=0}^{n} \frac{z^j}{j!} - e^z \right| = 0$$ 
appears to be the goal. We could also use the $N-\epsilon$ definition, which may help, or show the sequence is Cauchy. But I can't seem to get any of these to work. With the uniform convergence, I seem to run into the same problems. Any help or hints would be appreciated.","['complex-analysis', 'fourier-series', 'fourier-analysis', 'sequences-and-series']"
2016177,A picture frame measures $14$ cm by $20$ cm. $160$ cm$^2$ of the picture shows inside the frame. Find the width of the frame.,"A picture frame measures $14$cm by $20$cm. $160$cm$^2$ of the picture shows inside the frame. Find the width of the frame. This is the question I was given, word-for-word. Is it asking for the width of the picture? Becuase the way I see it, the width is simply $14$... EDIT: Is this the correct interpretation? (Sorry did a really quick drawing in MS paint, red represents what I am supposed to find)","['algebra-precalculus', 'geometric-interpretation', 'geometry']"
2016215,standard deviation probability of a poission distribution,"Just looking for a bit of help on the topic.
The question is basically this: Number of drivers who travel between a particular origin and destination during a designated time period has a Poisson distribution with parameter µ = 20. What is the probability that the number of drivers will be within two standard deviations of the mean value? Because it's a Poisson distribution, the expected value and the variance are the same, so mean value = µ = 20. Because the variance is 20, the standard deviation is the squareroot of 20 = 4.47. Two standard deviations is then 4.47*2 = 8.9 Two standard deviations from the mean is (20-8.9 <= x <= 20+8.9)
So we're looking for P(11 <= x <= 29). I put it in matlab like so poisscdf(29,20) - poisscdf(11,20) and I get 0.9568 .... the answer however is .945 . I'm just wondering where I went wrong!","['statistics', 'probability', 'poisson-distribution']"
2016236,CHMMC 2015 Individual Problem 0.1.,"I am practicing for the CHMMC , and a lot of the problems are very hard for me... Question $0.1.$ states: The following number is the product of the divisors of $n$: $$2^63^3$$What is $n$?
  I did some searching on google and found this , but I don't completely understand it. Can anyone give some needed formulas, hints, or solutions to help me solve this problem?","['algebra-precalculus', 'contest-math', 'elementary-number-theory']"
2016265,what are some examples of Abelian varieties?,"I am looking at Milne's notes on Abelian varieties.   Elliptic curves have an equation: $$ y^2 z = x^3 + a x z + b z^3 \text{ and }4a^3+27b^2 \neq 0$$ but also as complex manifold.  $E(\mathbb{C}) = \mathbb{C}/\Lambda$.  For Abelian varieties, there is no equation for dimension > 1.  That's fine. Product two elliptic curves is an Abelian variety.  OK. What was original motivation for Abelian varieties?  What are some situations that motivate study of Abelian varieties?","['abelian-varieties', 'algebraic-geometry']"
2016281,Prove convergence of hyperbolic recursive series,"How to prove that the series $\{x_n\}$ converges, and find its limit, given: $$ \frac{1}{x_n^2} = \frac{1}{a^2} + \frac{1}{b^2+x^2_{n-1}}$$ I think, to prove convergence for recursive series, we can prove that it has a bound (I think this one has upper bound), and monotonicity. For monotinicity, we should be able to do it by deduction (assuming it holds for $n$). I am having a hard time transforming the series into a form that can be analyzed. If $b = 0$, I can prove it, but in this case, $b$ and $a$ are real positives. EDIT: (adding my own thoughts and approach) Consider a hyperbolic curve $f(x)=\frac{1}{x^2}$, then we could represent this recursive series with hyperbolic trigonometric functions. That is, assume $\alpha$ and $\alpha'$ are twice the angle of the vector $\{x_{n-1}^2, \frac{1}{x_{n-1}^2}\}$ and $\{x_{n}^2, \frac{1}{x_{n}^2}\}$ respectively. See image below. Now, with this reparametrization, we could transform the original series for $x_{n-1} \rightarrow x_{n}$ into a series for $\alpha \rightarrow \alpha'$. With some simple trigonometry, we have: $$ \left(cosh(\alpha) + b^2\right)^2 - \left(cosh(\alpha') + \frac{1}{a^2}\right)^2 = 1. $$ I think, this could again be reparameterized with trigonometry to prove that $\alpha \rightarrow \alpha'$ converges. However, I think I am stuck here. Maybe we should expand the this expression into series, or maybe use imaginary representation. Any idea for the proof?","['real-analysis', 'limits', 'convergence-divergence', 'hyperbolic-geometry', 'sequences-and-series']"
2016285,Dimension of an object,"I know the dimension of a straight line, a circle is 1. But how can we prove it by using vector space? The dimension a vector space is the number of elements in the basis.",['linear-algebra']
2016341,Coproduct of non-unital commutative algebras,"If $A,B$ are non-unital commutative algebras over a field $R$, what should their (categorical) coproduct? I know for unital algebras the tensor product is the coproduct, but I think the construction of tensor product as a coproduct only worked there because we could have a unit element in multiplication, so we could just send, for example, $a\otimes 1_B \to f(a)g(1_B)=f(a)$ for some morphisms $f:A\to C$, $g:B\to C$ where $C$ is another algebra. Apparently without unit element we can't do this anymore. So what would coproduct be in this case?","['category-theory', 'abstract-algebra']"
2016353,How to get the gradient of the trace of a matrix?,"Like the one I formulate below,
$\pi \in \mathbb{R}^n$ 
$$\frac{\partial \text{trace}\left((X^T\text{diag}(\pi)X)^{-1}\right)}{\partial \pi} = ?$$
I tried to derive,
\begin{align*}
\text{trace}\left((X^T\text{diag}(\pi)X)^{-1}\right) \\
= \text{trace}\left(\left(\text{diag}(\pi)X\right)^{-1}X^{-T}\right) \\
= \text{trace}\left(X^{-1}\text{diag}(\frac{1}{\pi})X^{-T}\right) \\
= \sum_i\sum_jX^{-1}_{ij}\frac{1}{\pi_j}X^{-T}_{ji} \\
\frac{\partial \sum_i\sum_jX^{-1}_{ij}\frac{1}{\pi_j}X^{-T}_{ji}}{\partial \pi} = X^{-1}\odot X^{-T}\mathbf{1} \frac{\partial 1/\pi}{\partial \pi}\\
= -\frac{1}{\pi^2}X^{-1}\odot X^{-T} \mathbf{1}\\
= -\frac{1}{\pi^2}\text{trace}(X^{-T}X^{-T}) \mathbf{1}
\end{align*} Am I right with my derivation?","['matrices', 'multivariable-calculus', 'linear-algebra', 'calculus']"
2016535,Integral with trigonometric and dilogarithm,"The following integral can be characterised as an exotic one! Evaluate the integral: $$\int_{0}^{\pi/2} \left ( \frac{\log  \left ( \tan x +1 \right )}{\log \tan x} - \frac{{\rm Li}_2 \left ( -\cot x \right )}{\log^2 \cot x} - \frac{\zeta(2)}{2 \log^2 \tan x} \right ) \, {\rm d}x$$ It was proposed by Cornel Ioan Valean, Romania. I don't have any clue on how to attack this. Also, I don't think I can come up with any strategy that could work. So, I leave it entirely up to the community to come up with a clever approach. Edit: The name of the proposer is Cornel not Corel that I had written.","['special-functions', 'real-analysis', 'integration']"
2016550,Integral of the square of the exponential of a matrix [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 years ago . Improve this question Let $A$ be a $n\times n$ square matrix such that the real part of all eigenvalues are negative. It is well known that: 
$$ \int_0^\infty \exp(At)dt = -A^{-1}.$$
In other word, the element $(i,j)$ of the above matrix is $\int_0^\infty \exp(At)_{ij}dt = -A^{-1}_{ij}$. Is it possible to simplify a similar expression where each element is squared: 
$$ \int_0^\infty (\exp(At)_{ij})^2 dt = ??$$ For one-dimensional matrices (a.k.a. real numbers), this is easy and we obtain $(-2A)^{-1}$ because $\exp(-at)^2=\exp(-2at)$. This does not work for $n\ge2$ and in this case the answer is not $(2A)^{-1}$. I am wondering if it is possible to simplify the above expression.","['multivariable-calculus', 'integration', 'linear-algebra']"
2016573,Adjoint matrix as pseudo-inverse,"I'm very new to signal processing (seismic 1,2 and 3D-signal) and have read many papers recently. One thing I encounter quite often is the use of adjoint matrix. If $d = Am$ where $d$ is the data, $A$ is an operator that models a physical process and $m$ is the model, many people define $\widetilde m=A^Td$  or $\widetilde m=A^*d$ be the pseudo-inverse for $m$ and sometimes use $\widetilde m$ as a starting model. I know $A^{-1}=A^T$ sometimes but in general it's not the case. So how good is this pseudo-inverse? Can someone give me some more insight or give me some reference on this?","['matrices', 'pseudoinverse', 'adjoint-operators', 'transpose']"
2016611,Which class of morphisms preserves smoothness of points?,"Let $f\colon X\to Y$ be a morphism of complex algebraic varieties. I am interested in those $f$ that have the following property: For any smooth point $x\in X$, the image $y=f(x)$ is a smooth point of $Y$. Certainly, open immersions have this property, closed immersions certainly do not. Which well-known, larger classes of morphisms have this property?",['algebraic-geometry']
2016643,Cayley-Hamilton equation for a given matrix $A$ and other matrices,"We know  that ""every square matrix over a commutative ring (such as the real or complex field) satisfies its own characteristic polynomial"" i.e. $p(A)=0$ My question is: what other matrices are satisfying this particular equation $p(A)=0$ Evidently it is satisfied  by any matrix which is generated from $A$ through a change of basis and also by some more simply generated matrices, for example $A^T$ or $\lambda_i{I}$, where $\lambda_i $ is eigenvalue for matrix $A$. But maybe it is possible to find a general form of the matrix which is satisfying characteristic equation generated for a matrix A?","['matrices', 'polynomials', 'linear-algebra']"
2016686,"Gradients ""backward flow"" calculation rulls","I am reading article Hacker's guide to Neural Networks. . Becoming Becoming a Backprop Ninja section The quote: lets just use variables such as a,b,c,x, and refer to their gradients as da,db,dc,dx respectively. Again, we think of the variables as the “forward flow” and their gradients as “backward flow” along every wire. Our first example was the * gate: x = a * b;
// and given gradient on x (dx), we saw that in backprop we would compute:
da = b * dx; I have rewatched Calculus lectures about derivatives and gradient but still don't understand this simple part. My reasonings: The partial derivative of x with respect to a is equal to b . dx/da = b (1) During gradient based optimisation we have dx - that is x gradient(vector that points to the maximum and is the sum of partial derivatives vectors). This gradient tells how mutch should we adjust independent variables (that is b here) to get to the maximum. From (1): da = dx/b So here we need to take gradient of x that came from cost function and divide it into b. Then we can adjust b in that direction. So why does the author say about da = b * dx; and not da = dx/b here?
Also if you see some mistakes in my reasoning please tell me.","['derivatives', 'optimization', 'gradient-flows', 'gradient-descent', 'ordinary-differential-equations']"
2016716,Number of singular $2\times2$ matrices with distinct integer entries,"Given an integer $n$, what is the total number of singular $2 \times 2$ matrices with distinct elements from $\{0,1,\ldots,n\}$? Example: a) For $n=6$, the numbers to be considered are $\{0,1,2,3,4,5,6\}$. So the valid matrices whose determinants equal $0$ are: 1)$$
        \begin{vmatrix}
        1 & 2 \\  
        3 & 6 \\       
        \end{vmatrix}=0
$$
2)$$
        \begin{vmatrix}
        1 & 3 \\  
        2 & 6 \\       
        \end{vmatrix}=0
$$
3)$$
        \begin{vmatrix}
        2 & 1 \\  
        6 & 3 \\       
        \end{vmatrix}=0
$$
4)$$
        \begin{vmatrix}
        2 & 4 \\  
        3 & 6 \\       
        \end{vmatrix}=0
$$,etc like these there are 16 possible ways for n=6 , So the answer is 16 b)For n=50 there are 5824 ways c) For n=24 , there are 920 ways So I need To know the way of evaluating this problem","['matrices', 'combinatorics', 'linear-algebra', 'determinant']"
2016722,Prove that the product topology is the coarsest topology on which the projection mapping is continuous.,"Let $\{ (X_\alpha, Y_\alpha): \alpha \in A\}$ be a family of topological spaces for some index set $A$. Prove that the product topology $\mathcal T$ on $X= \displaystyle \prod_{\alpha \in A} X_\alpha$ is the coarsest topology on which the projection mapping $p_\alpha:\displaystyle \prod_{\alpha \in A} X_\alpha \to X_\alpha$ is continuous for each $\alpha \in A$. Firstly, in my notes, the product topology $\mathcal T$ is defined as the topology on $X$ which has as a basis the collection $$\mathcal B = \{O_\alpha: O_\alpha \in \mathcal T_\alpha \text{ for all but a finite number of } \alpha \in A \}.$$ My attempt: Suppose that, for each $\alpha \in A$, $p_\alpha$ is continuous on some topology $\mathcal T'$ of $X$. We must show that $\mathcal T'\supseteq \mathcal T$. Let $W=W_{\alpha_1}\times W_{\alpha_2}\times W_{\alpha_n}\times X_{\alpha_{n+1}}\times \cdots$ be a basis element of the product topology $\mathcal T$ on $X$, then $W \in \mathcal T$. Now (this is the part I'm not sure is correct) $$W=\bigcap_{k=1}^n\left(W_{\alpha_k}\times\prod_{\alpha_k \in A, \alpha_k \neq k}X_{\alpha_k}\right) = \bigcap_{k=1}^n \left(p_{\alpha_k}^{-1}(W_{\alpha_k})\right)$$
Then, since $p_{\alpha_k}$ is continuous on $X$ w.r.t. $\mathcal T'$ (by hypothesis), we must have that $p_{\alpha_k}^{-1}(W_{\alpha_k}) \in \mathcal T'$ and so we also have that $W \in \mathcal T'$, since $\mathcal T'$ is closed under finite intersection. That is, $\mathcal T' \supseteq \mathcal T$. Is the proof given above correct? If not, can anyone please point out to me where I went wrong?","['continuity', 'general-topology', 'proof-verification']"
2016732,Necklace problem with Burnside's lemma,"How many necklaces can be made with two red beads, two green
  beads, and two violet beads? I'm trying to solve this with Burnside's Lemma however I'm stuck. Here's what I've got so far: Let $S=\{$necklace of length 6 with 2 read, 2 green and 2 violet$\}$, $|S|=\frac{6!}{2!2!2!}=90$, let $G=\mathbb{D}_{6}$, $|G|=2*6=12$. Now let $g\in G$, $g=e\Rightarrow |fix(g)|=|S|=90$ since $e$ sends any $s$ back to itself. For when $g\neq e$, if $g$ is any one of the rotations by $\frac{\pi}{6}$ clockwise, what should I do from here? I intuitively know that not all rotations sends $s$ back to itself and that different configuration of the color beads also plays a role, yet I don't have a clear route to follow. And what about flips? Thanks for any help!","['abstract-algebra', 'group-theory']"
2016733,To show: $\det\left[\begin{smallmatrix} -bc & b^2+bc & c^2+bc\\ a^2+ac & -ac & c^2+ac \\ a^2+ab & b^2+ab & -ab \end{smallmatrix}\right]=(ab+bc+ca)^3$,"I've been having quite some trouble with this question.
I'm required to show that the below equation holds, by using properties of determinants (i.e. not allowed to directly expand the determinant before greatly simplifying it). $$\begin{vmatrix}
-bc & b^2+bc & c^2+bc\\ 
a^2+ac & -ac & c^2+ac \\
a^2+ab & b^2+ab & -ab
\end{vmatrix}= (ab+bc+ca)^3$$ I tried everything: $R_1\to R_1+R_2+R_3$ and similar transformations to extract that $ab+bc+ca$ term, but to no avail. $C_2\to C_1+C_2$ and $C_3\to C_3+C_1$ seemed to be a good lead, but I couldn't follow up. How can I solve this question?","['matrices', 'linear-algebra', 'determinant']"
