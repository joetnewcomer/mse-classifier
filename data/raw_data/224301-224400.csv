question_id,title,body,tags
4615935,What does the result of this integral mean or represent,"If we have a wire shaped like C parabola $y=1-x^2$ , the length of the wire should be $\int _C dl$ where dl is a infinitesimal element of the length what will $\int_C F(x,y) dl$ generally  mean or represent ?","['integration', 'multivariable-calculus', 'calculus']"
4615942,Minimum number of Bernoulli trials until sum reaches threshold with high probability,"Let $X_1, X_2, \dots$ be i.i.d. $Bern(p)$ with $p\in (0, 1)$ . Let $\delta \in (0,1)$ and $m \in \mathbb{N}$ . What is the smallest integer $n \in \mathbb{N}$ such that $$P\left( \sum_{i=1}^n X_i \geq m \right) \geq 1- \delta \text{ ?} $$ Are there any bounds/approximations of $n$ in terms of $p$ , $m$ , and $\delta$ ? (It very much reminds of the Chernoff bound, but this is in some sense the reverse direction I guess.)","['statistics', 'probability-theory', 'probability', 'upper-lower-bounds']"
4616005,High-precision second-order difference quotient of 2 variables functions,"For one variable function, 2th order difference quotient can be: $$ 
\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}, 
$$ which can also be: $$
\frac{-f(x-2h) + 16f(x-h) - 30f(x) + 16f(x+h) - f(x+2h)}{12h^2}
$$ with a higher precision. For two variables function, difference quotient of $\displaystyle \smash[t]{\frac{\partial^2 f}{\partial x \, \partial y}}$ can be: $$
\frac{f(x+h,y+h) - f(x+h,y-h) - f(x-h,y-h) + f(x-h,y+h)}{4h^2}.
$$ Is there some other form of this difference quotient with a higher precision?","['multivariable-calculus', 'calculus', 'numerical-methods', 'derivatives', 'finite-differences']"
4616018,"The letters B, E, I, M, O, Z are arranged in alphabetical order. In a list where all the permutations are put, which place would the word ZOMBIE be?","So I know that $6$ different letters can be arranged in $6!$ different ways. Which means $$
6 \times 5 \times 4 \times 3 \times 2 \times 1 = 720
$$ ways. and since the word $\mathsf{ZOMBIE}$ has the letters $\mathsf{Z}$ , $\mathsf{O}$ , $\mathsf{M}$ in the start means that $6 \times 5 \times 4$ disappears which leaves it $3!$ ways left to arrange it in. How should I go from here to find which place the word $\mathsf{ZOMBIE}$ is placed in out of the $720$ possible combinations? How do I go from here to solve this?","['permutations', 'combinatorics', 'discrete-mathematics']"
4616024,Quotient Space by group action,"Let $G$ a group and $X$ a topological space. We can define a group action of $G\times G$ over $X\times X$ in the obvious way, and give the quotient topology to $(X\times X)/(G\times G)$ . How I can prove that there exists a homeomorphism $(X\times X)/(G \times G)\cong (X/G)\times(X/G)$ ?","['general-topology', 'group-actions', 'quotient-spaces']"
4616058,Where does $\mathsf{DEBABE}$ appear in a Krilandic dictionary? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question The Krilandic language over the alphabet $\Sigma = \{ \mathsf{A,B,C,D,E} \} $ consists of all $6$ -letter words without a letter repeating twice successively. In what place does the word $\mathsf{DEBABE}$ appear in a Krilandic dictionary (listing all Krilandic words, lexicographicly)? I know how to calculate the number of such words using the inclusion–exclusion principle, but how is this helpful here?",['discrete-mathematics']
4616071,How to compute $\lim_{ x\to 0} \frac{\sqrt{x+2}-\sqrt{2-x}}{\sqrt[3]{x+2}-\sqrt[3]{2-x}}$?,"How to compute the following limit? $$\lim_{ x\to 0} \frac{\sqrt{x+2}-\sqrt{2-x}}{\sqrt[3]{x+2}-\sqrt[3]{2-x}}$$ In the context of the book I am reading, I must use some change of variables or multiply the expressions by some ""conjugate"". I tried to multiply it by $\left(\sqrt{2-x}+\sqrt{x+2}\right)$ to obtain: $$\frac{2 x}{\left(\sqrt[3]{x+2}-\sqrt[3]{2-x}\right)
   \left(\sqrt{2-x}+\sqrt{x+2}\right)}$$ And I tried to make the following change of variables $2+x =t^6$ and then I obtained: $$\frac{t^3-\sqrt{4-t^6}}{t^2-\sqrt[3]{4-t^6}}$$ But I couldn't go much farther than this. Can you give me a hint?","['limits', 'limits-without-lhopital']"
4616107,Is Lebesgue integral a continuous functional in this context?,"Statement: I asked a similar question before which was closed. I tried to provide some context in this version. Let $p$ be a probability measure whose support is $\left[a,b\right]$ . Consider the set of continuous functions with a domain of $\left[a,b\right]$ and a codomain of $\left[c,d\right]$ , denoted by $\mathcal{F}$ . The reason I restrict the codomain is to ensure the dominated convergence theorem can be applied. Take $\int \cdot dp$ as a mapping from $\mathcal{F}$ to $\mathbb{R}$ . If I endow $\mathcal{F}$ with the topology of pointwise convergence and $\mathbb{R}$ with the standard topology, is then $\int \cdot dp$ a continuous functional on $\mathcal{F}$ ? Note that the integral and limit can be exchanged in this case. But since the topology of pointwise convergence is not metrizable, this is only a necessary condition for $\int \cdot dp$ to be continuous. I tried to go back to the definition of continuity for the general topological space, that is, the preimage of an open set is an open set. But that means when the value of the integral  is within a union of several open intervals, the value of the functions must be constrained in an open set at several points (I don't know how to describe this precisely but what I mean is the notion of subbasis for the topology of pointwise convergence on page 281 of Topology (2nd edition) by Munkres) and I have no idea how to verify that. I was wondering whether there are some ways to know whether $\int \cdot dp$ is a continuous functional or not.","['lebesgue-integral', 'real-analysis', 'functional-analysis', 'general-topology', 'probability-theory']"
4616113,Does subspace topology affect the compactness?,"Consider the function $f(x)=x+2$ . Let the domain be $\left(0,1\right)$ , then the range is $\left(2,3\right)$ . $\left(0,1\right)$ is not compact in $\mathbb{R}$ since it is not closed in $\mathbb{R}$ . But if I consider the subspace topology with $\left(0,1\right)$ , $\left(0,1\right)$ is closed (in $\left(0,1\right)$ ). But since $\left(2,3\right)$ is not compact in $\mathbb{R}$ and $f$ is continuous, $\left(0,1\right)$ should not be compact even with the subspace topology. I was wondering whether there is a general result like: if a set $S$ is not compact in $T$ , then it is not compact in any set $U$ such that $S\subseteq U \subseteq T$ . In other words, one cannot make a non-compact set compact by applying a subspace topology.",['general-topology']
4616129,Number of binary strings with $k$ ones or $k$ zeros and no three consecutive ones,"I have been trying to find a way to count the number of binary strings of length $n$ such that it has no three consecutive $1$ 's, but also either has exactly $k$ $1$ 's or exactly $k$ $0$ 's.
For example, if $n = 4$ and $k = 3$ , then there are six valid such binary strings: $0001$ , $0010$ , $0100$ , $1000$ , $1011$ , and $1101$ .
Similar to this post , we can model the problem with a recurrence relation.
My first attempt is to define the recurrence relation $F(n,p,q)$ denoting the number of binary strings of length $n$ with no three consecutive $1$ 's with exactly $p$ $1$ 's and $q$ $0$ 's.
Here, I have $F(n,p,q) = F(n-3,p-2,q-1) + F(n-2,p-1,q-1) + F(n-1,p,q-1)$ for $n \geq 4, p \geq 2, q \geq 1$ , with the following initial conditions: $$
F(n,p,q) =
\begin{cases}
    0 & \text{if } n = 0, \text{ or } p + q \neq n\\
    1 & \text{if } p = 0, q = n\\
    n & \text{if } p = 1, q = n - 1 \text{ or } n = p = 2, q = 0\\
    3 & \text{if } (n,p,q) = (3,1,2) \text{ or } (n,p,q)=(3,2,1)
\end{cases}
$$ Essentially, the recurrence relation is derived from how such binary strings of length $n$ can be formed by strings of the form $s011$ , $s01$ , or $s0$ , where $s$ is again a binary string with no three consecutive ones of length $n-3$ , $n-2$ , or $n-1$ , respectively.
The string $s011$ adds two extras $1$ 's and one extra $0$ to $s$ .
The string $s01$ adds one extra $0$ and one extra $1$ to $s$ .
Lastly, the string $s0$ adds an extra $0$ to $s$ . Notice that to get the answer to our original problem, we can define a function $\tilde{F}(n,k)$ representing the number of binary strings of length $n$ with no three consecutive ones consisting of either $k$ ones or $k$ zeros as $$
\tilde{F}(n,k) = 
\begin{cases}
    F(n,n/2,n/2), \text{ if $n$ is even and $k = n/2$} \\
    F(n,k,n-k) + F(n,n-k+k), \text{ otherwise}
\end{cases}
$$ Using the idea from this post , we can use generating functions to count the exact formula for the recurrence relation $F(n,p,q)$ .
After doing some calculations (you can see the detail here if interested), this is the generating function I found we can use for $F(n,p,q)$ : $$
G(x,y,z) = \sum_{n \geq 0, p \geq 0, q \geq 0} F(n,p,q) x^n y^p z^q = \frac{2x^3y^2z - x^2y^2 - x^2yz - xy - xz}{x^3y^2z + x^2yz + xz - 1}
$$ Now, the question is how do we compute the coefficient $[x^ny^pz^q] \left( \frac{2x^3y^2z - x^2y^2 - x^2yz - xy - xz}{x^3y^2z + x^2yz + xz - 1} \right)$ ? I am really new to generating functions and have been struggling because of how the denominator cannot be factorized, hence we cannot do any useful partial fractions on it. Also, I have not been very successful in finding a common series similar to the one we are investigating. Any ideas or sources that might be helpful to solve this?
Is it even possible?
If it is impossible, can we at least get the asymptotic upper bound for such a recurrence relation?
I conjectured that the upper bound for $F(n,p,q)$ is exponential in terms of $n$ , $p$ , and $q$ , but I do not know how to prove it. Thanks in advance. Update : As suggested by @VTand, we can redefine $F$ in terms of only $p$ and $q$ as $n$ is a redundant variable. Then, we have $F(p,q) = F(p-2,q-1) + F(p-1,q-1) + F(p,q-1)$ for $p \geq 2, q \geq 1$ with these initial conditions: $$
F(p,q) = \begin{cases}
  0 & \text{if $q = 0, p > 2$}\\
  1 & \text{if $p = 0$ or $(p,q) = (2,0)$}\\
  q + 1 & \text{if $p = 1$}
\end{cases}
$$ Apparently, the generating function we can work with now is much simpler (you can see the calculation here if interested), that is: $$
G(x,y) = \sum_{p \geq 0,q \geq 0} F(p,q) x^p y^q = \frac{x^2 + x + 1}{1 - y(x^2 + x + 1)}
$$ To calculate for $F(p,q)$ , we can use the value $[x^py^q] \left( \dfrac{x^2 + x + 1}{1 - y(x^2 + x + 1)} \right)$ , i.e. the coefficient of $x^py^q$ of the power series $G(x,y)$ . However, I am still struggling to extract the coefficient or determine the asymptotic upper bound. Any help would be very much appreciated.","['combinatorics', 'recurrence-relations', 'generating-functions']"
4616155,How can i write $f(x)=\cos(x)$ as the difference of two monotonically increasing functions?,"This is a Question from an Analysis 1 exam. The question is as follows: Decide if the functions $f: \mathbb{R} \longrightarrow \mathbb{R}$ can be written as the difference of two monotonically increasing functions a) $f(x) = \cos(x)$ b) $f(x) = x^2$ For the moment I’m working on a) my first thought would be to use the MVT and receive something in the form of $\cos(x)+2x = -\sin(x)-2x$ but as we see - $\sin(x)$ is not monotonically increasing. Obviously one could also answer with $\cos(x) = (\cos(x)+2x) - 2x$ but I fear this answer would not be accepted by my professor.
If you have any tips or answers for either a) or b) id be grateful",['analysis']
4616163,Nilpotent quotients of a residually nilpotent group,"I am working on a problem that requires taking quotients by normal subgroups that do not intersect a finite set of members of a group $G$ . Is there a known collection of groups which always allows taking quotients by progressively smaller normal subgroups such that the normal subgroups do not intersect progressively larger finite set of group elements and the quotient is nilpotent? I have come across the notion of residually nilpotent groups. Do they have this property? In particular here is the definition of residually nilpotent group. My question regarding them follows. A group is residually nilpotent if given any non-identity element, there is a normal subgroup not containing that element, such that the quotient group is nilpotent. I am wondering if it is possible to escape a finite set of non-identity elements instead of just one. That is whether the following statement is true: if $G$ is residually nilpotent, then given any finite set of non-identity elements, there is a normal subgroup not containing those elements, such that the quotient group is nilpotent. If not, then is there a collection of groups for which this statement is true? Is this collection of groups strictly larger than the collection of virtually nilpotent groups.","['nilpotent-groups', 'group-theory']"
4616180,Asymptotic behaviour for ODE,"Imagine I have an ordinary linear homogeneous second-order differential equation of the form: $$
y''=f(x)y'+g(x)y,
$$ where the functions $f$ and $g$ have the regularity you wish, are bounded, and satisfy $$
\lim_{x\rightarrow \infty}f(x)=a, \qquad \lim_{x\rightarrow \infty}g(x)=b,
$$ where $a$ and $b$ are real constants. Heuristically, this tells that the asymptotics of a solution of our ODE will behave as the solution of the autonomous ODE: $$
v''=av'+bv.
$$ For example, if we have a solution for the first equation, $y$ , converging to zero, and $v$ is a solution for the second equation converging to zero (i.e., exponential decay), do we have $$
y(x)=O(v(x)),\qquad v(x)=O(y(x))
$$ as $x \rightarrow \infty$ ? EDIT: To clarify my question, I will point out here an example suggested in one of the answers. Let $f(x)=e^{-x}+1$ ang $g(x)=0$ . Then we have \begin{equation}
y''(x)+(e^{-x}+1)y'(x)=0 \implies y(x) = c_1e^{e^{-x}}+c_2.
\end{equation} And between all these solutions, take for example the ones converging to zero, so take $c_1=-c_2$ . I claim that asymptotically this solution and the solutions converging to zero of the approximation \begin{equation}
y''(x)+y'(x)=0 \implies y(x) = \tilde{c}e^{-x}
\end{equation} are the same. In this case, this is true, since $$
\lim_{x \rightarrow \infty} \frac{c_1e^{e^{-x}}-c_1}{\tilde{c}e^{-x}}=c_1/\tilde{c}.
$$ Question: Is this true in general? If so, is there any kind of result I can use to formalize this? Note that I assumed the solutions to converge to zero for simplicity, but I think the asymptotics would be the same between any two reasonable solutions (as far as we do not compare solutions with a finite limit and solutions going to infinity, for example). I have tried to compute the error between the approximation and the first solution, and prove that this error has a faster decay than the solution itself, but it didn't work.","['asymptotics', 'ordinary-differential-equations', 'dynamical-systems']"
4616198,Structure on moduli space of topological/smooth vector bundles,"If $M$ is paracompact (let us assume smooth manifold of dimension $d$ ), then one has that the set of isomorphism classes of vector bundles of rank $n$ over $M$ is isomorphic to the set of homotopy classes $V_n:=[M,G_n\left(\mathbb{R}^{n+d}\right)]$ from $M$ to a Grassmannian. This holds both in the topological and the smooth category over $M$ . I was wondering if there is any ""nice"" topological/smooth structure on $V_n$ ? Does it carry a well-behaved and non-trivial topology? Are there results on how ""many"" elements $V_n$ usually has? Finitely many, Countably many? This is really not my area, so these questions may have simple, well-known answers.","['smooth-manifolds', 'vector-bundles', 'manifolds', 'general-topology', 'differential-geometry']"
4616274,Homogeneous Space $G_2/P$ is a Quadric Hypersurface inside $\mathbb{P}^6 $,"Let $G_2$ be one of the exceptional simple Lie groups and $\mathfrak{g}_2$ it's Lie algebra. On pges 355/356 in Fulton & Harris' Representation Theory is shown that the stadard representation of Lie algebra $\mathfrak{g}_2$ is seven-dimensional irrep $V = \mathbb{C}^7$ . In the Chapter 23.3 on homogeneous spaces, page 391, is discussed how irreps of $\mathfrak{g}_2$ give rise for parabolic subgroups $P \subset G_2$ . The action of the Lie algebra on the standard rep $V$ induces an action on projectivized space $\mathbb{P}V \cong \mathbb{P}^6 $ . Via exponentiation this induces an action of $G_2$ on $\mathbb{P}V$ . Let $p \in \mathbb{P}V$ be the line corresponding to the eigenspace of the highest weight $\lambda_V$ of $V$ , because $V$ is irrep. The correspondence tells us that the parabolic group $P_{\lambda} \subset G_2$ corresponds to the stabilizer of $p$ and thus the orbit $G \cdot p$ equals $G_2/P_{\lambda}$ . The part I not understand: the book claims then that the orbit $G \cdot p$ can only be a quadric hypersurface, since it is homogeneous. Why that's the case? The way the argument is phrased suggests that since the orbit $G \cdot p$ is a hyperplane of $\mathbb{P}^6 $ which is by construction a homogeneous space, one should conclude that it must be a quadric surface. This conclusion is not clear me. I'm not sure if it helps but on pages 355/356 one obtained as result of the careful analysis of properties of $V$ , that the action of $\mathfrak{g}_2$ on $V$ preserves a quadratic form. Could one maybe use somehow this insight to conclude that $G \cdot p$ is given as quadric hypersurface of $ \mathbb{P}V$ ? A remark: as @hm2020 noticed one should expect that it's possible to use immediately the classification result for homogeneous spaces in terms of their line bundles. But I'm not sure that the authors had this argument in mind, since the book is relatively self contained and this proposed approach is not discussed there, that's why I doubt that the autor's would conclude it from such a deepcresult without loosing any couple of words about it. Moreover the book's style is characterized by attepts to unravel the geometric picture. Therefore I hope that the proposed claim that the orbit of $p$ is a quadric hypersurface can be somehow directly concluded using only the methods the book provide.","['lie-algebras', 'algebraic-groups', 'algebraic-geometry', 'homogeneous-spaces', 'lie-groups']"
4616299,Intuition behind the connection between Entropy and Dirichlet Form,"Consider some Markov process $X$ on a finite state space $E$ admitting a reversible measure $\mu$ . The following two quantities are extremely useful in its study: The relative entropy $$
H(\nu_t|\mu) = H(f_t) = -\mathbb{E}_\mu[f_t\log f_t]\geq 0,
$$ where we assume for simplicity that the distribution $\nu_t$ of $X_t$ has density $f_t$ w.r.t. $\mu$ . The Dirichlet Form $$
D(f) := \langle f, (-L)f\rangle_\mu \geq 0,
$$ where $L$ denotes the generator of the Markov process and $\langle\cdot,\cdot\rangle_\mu$ denotes the $L^2(\mu)$ -inner product. It is relatively easy to show (see e.g. Appendix 1, Theorem 9.2, of Scaling Limits of Interacting Particle Systems by Kipnis and Landim) that $$
\partial_t H(f_t) \leq -2D(\sqrt{f_t})
$$ which constitutes a very important statement as it allows us to estimate the rate of convergence of $X_t$ to its stationary state. The problem that I have is that I cannot find any intuitive reason for this relationship to hold. The proof itself does not help as it uses a simple but uninformative inequality to prove it. I am wondering if there is a heuristic why this should hold true or if I am too optimistic...","['entropy', 'stochastic-processes', 'markov-process', 'soft-question', 'probability']"
4616312,Prove the closed-form of $\int_{0}^{1}\frac{K^\prime\left (\frac{1-x^2}{2}\right)}{\sqrt{1+x^2}}\text{d}x$,"Question : Prove that $$
\int_{0}^{1} \frac{K^{\prime}\left ( \frac{1-x^2}{2}  \right ) }{\sqrt{1+x^2} }
\text{d} x
=\frac{\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{48\pi\sqrt{3}},
$$ where $K^\prime(x)=K\left(\sqrt{1-x^2}\right)$ and $K(x)$ is known as a complete elliptic integral of the first kind with the elliptic modulus $x$ . $\Gamma(z)=\int_{0}^{\infty}t^{z-1}e^{-t}\text{d}t$ is the Euler Gamma function. The equality's right hand side is probably given by an elliptic integral singular value $K(k_6)$ . Here are my thoughts. We can determine a similar one $$
\int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} }\text{d}x=\frac{1}{\pi\sqrt{2}}\int_{0}^{\pi} \int_{0}^{\pi} \int_{0}^{\pi} 
\frac{1}{3-\cos x-\cos y-\cos z}\text{d}x\text{d}y\text{d}z\\
=\frac{\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{32\pi\sqrt{3}}.
$$ The direct calculation shows the first equality. The second one is refered to the third of Watson's triple integrals . So we only need to show $$
\int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} }\text{d}x
=\frac32\int_{0}^{1} \frac{K^\prime\left ( \frac{1-x^2}{2}  \right ) }{\sqrt{1+x^2} }
\text{d}x.
$$ So far, however, I can't see any connections between two integrals. Some subtle transformations and substitutions may be helpful. But I am not able to come up with them by myself. By integrating the function $f(x)=\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} }$ along the real axis, we deduce $$
\int_{0}^{\sqrt{2} } \frac{K^\prime\left (1-x^2\right ) }{
\sqrt{1+x^2} }\text{d}x
=\frac{\sqrt{2}}{2}\int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} }\text{d}x\\
=\frac{\sqrt{6}\,\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{192\pi}.
$$ The desired integral is actually represented as follows $$
\int_{-1}^{1} \int_{1}^{\infty} \int_{1}^{\infty} 
\frac{1}{\sqrt{x^2-1}\sqrt{y^2-1} \sqrt{1-z^2}  }
\frac{1}{3+x+y+z} \text{d}x\text{d}y\text{d}z
$$ which is equivalent to $\int_{0}^{\infty}e^{-3z}K_0(z)^2I_0(z)\text{d}z=\frac{\pi^2}3\int_{0}^{\infty}e^{-3z}I_0(z)^3\text{d}z$ after some complex integral technique. Detailed explanations are writing now.","['integration', 'definite-integrals', 'gamma-function', 'calculus', 'elliptic-integrals']"
4616325,Find $\sum_{i=1}^{200}f(i)$,"$$f(x)=\left\{ \begin{array}{ c l } \left[\frac{1}{\{\sqrt{x}\}}\right] & \quad \textrm{if $x≠k^2$} \\ 0 & \quad \textrm{otherwise} \end{array} \right.$$ Where $x,k\in \mathbb{N}$ and [.] represents greatest integer function and {.} represents frictional part. Find $$\sum_{i=1}^{200}f(i)$$ The only method in my mind is to solve is by calculating each term value and adding. Are there any methods one can solve this in notebook.","['summation', 'ceiling-and-floor-functions', 'fractional-part', 'functions', 'linear-algebra']"
4616342,Prove $tr_B(vec(A)vec(B)^T)=AB^T$,"Let $A,B\in \mathbb{R}^{n\times n}$ . And let $vec$ be a map : $\mathbb{R}^{n\times n}\to \mathbb{R}^{n^2\times1}$ : $vec(e_a e_b^T)=e_a\otimes e_b$ , here $e_a$ denotes the vector with   with an entry $1$ on index $a$ and other entries $0$ . Intuitively the operation $vec$ puts every row of a matrix into a new vector. Partial trace is defined as $tr_B(A\otimes B)=tr(B)A$ . Then how to prove $tr_B(vec(A)vec(B)^T)=AB^T$ ? Here $\otimes$ denotes the Kronecker product. Attempts: I try to decomposite $vec(A)vec(B)^T$ into the form $\sum_{ij} X_i\otimes Y_j$ so $tr_B(vec(A)vec(B)^T)=\sum_{ij}(X_i tr(Y_j))$ , but how to achieve that? I have one new idea: $vec(A)=\sum_{ij}a_{ij} e_i\otimes e_j$ and $vec(B)=\sum_{kl}b_{kl}e_k\otimes e_l$ , I will have a try along this way.","['matrices', 'quantum-information', 'linear-algebra', 'kronecker-product']"
4616365,"Real-Analysis Methods to Evaluate $\int_0^\infty \frac{x^a}{1+x^2}\,dx$, $|a|<1$.","In THIS ANSWER , I used straightforward contour integration to evaluate the integral $$\bbox[5px,border:2px solid #C0A000]{\int_0^\infty \frac{x^a}{1+x^2}\,dx=\frac{\pi}{2}\sec\left(\frac{\pi a}{2}\right)}$$for $|a|<1$. An alternative approach is to enforce the substitution $x\to e^x$ to obtain $$\begin{align}
\int_0^\infty \frac{x^a}{1+x^2}\,dx&=\int_{-\infty}^\infty \frac{e^{(a+1)x}}{1+e^{2x}}\,dx\\\\
&=\int_{-\infty}^0\frac{e^{(a+1)x}}{1+e^{2x}}\,dx+\int_{0}^\infty\frac{e^{(a-1)x}}{1+e^{-2x}}\,dx\\\\
&=\sum_{n=0}^\infty (-1)^n\left(\int_{-\infty}^0 e^{(2n+1+a)x}\,dx+\int_{0}^\infty e^{-(2n+1-a)x}\,dx\right)\\\\
&=\sum_{n=0}^\infty (-1)^n \left(\frac{1}{2n+1+a}+\frac{1}{2n+1-a}\right)\\\\
&=2\sum_{n=0}^\infty (-1)^n\left(\frac{2n+1}{(2n+1)^2-a^2}\right) \tag 1\\\\
&=\frac{\pi}{2}\sec\left(\frac{\pi a}{2}\right)\tag 2
\end{align}$$ Other possible ways forward include writing the integral of interest as $$\begin{align}
\int_0^\infty \frac{x^a}{1+x^2}\,dx&=\int_{0}^1 \frac{x^{a}+x^{-a}}{1+x^2}\,dx
\end{align}$$ and proceeding similarly, using $\frac{1}{1+x^2}=\sum_{n=0}^\infty (-1)^nx^{2n}$. Without appealing to complex analysis, what are other approaches one can use to evaluate this very standard integral? EDIT: Note that we can show that $(1)$ is the partial fraction representation of $(2)$ using Fourier series analysis.  I've included this development for completeness in the appendix of the solution I posted on THIS PAGE .","['definite-integrals', 'real-analysis']"
4616378,What is the largest eigenvalue of the special matrix?,"The question is a simplified question to my last question. Let $I_m$ denote the $m\times m$ identity matrix, and $\alpha\in\mathbb{R}^{n-1}$ . Let $$
\mathbf{B}=\left[\begin{array}{ccccc}
-\alpha & 0 & &  &\\
\alpha & -\alpha & 0 & \\
0 & \alpha & \ddots & \ddots \\
& & & & &-\alpha\\
\end{array}\right]\in \mathbb{R}^{(mn-m)\times (m-1)}
$$ I want to calculate the largest eigenvalue of $$
\left[\begin{array}{ccccc}
I_{mn-m} & B \\ 
B^{\top} & I_{m-1}
\end{array}\right].
$$ The answer should be $\sim3- O(\frac{1}{n m^2})$ , but I want to know the exact value.","['matrices', 'matrix-equations', 'linear-algebra', 'eigenvalues-eigenvectors']"
4616392,"If $w \ge 0$ and $\mu$ is a finite measure on $X$ with $\mu(w = 0) \ge \mu(X)/2$, is $\mu(w > t) \le 2 \mu(|w - c| > t/2)$?","Suppose we have a finite measure space $(X, \mu)$ and a measurable function $w \ge 0$ on $X$ satisfying $\mu(w = 0) \ge \mu(X)/2$ , is it true that for any real $c$ we have $\mu(w > t) \le 2 \mu(|w - c| > t/2)$ whenever $t > 0$ ? If $c > t/2$ then clearly $\{ w = 0 \} \subset \{c - w > t/2\}$ , but $c - w(x) > t/2 > 0$ implies $|w(x) - c| = c -w(x) > t/2$ , so we have $$
\mu(X)/2 \le \mu(w = 0) \le \mu(|w -c| > t/2). 
$$ On the other hand, $\{ w > t\} \subset \{ w \neq 0\}$ so $$\mu(w > t) \le \mu(X) - \mu(w = 0) \le \mu(X)/2 \le \mu(|w - c| > t/2),
$$ which proves the claim when $c > t/2$ . When $c \le t/2$ , we have $w > t \ge c + t/2$ so $\{ w > t\} \subset \{w - c > t/2\} \subset \{ |w - c| > t/2\}$ , from which we again see that $\mu(w > t) \le \mu(|w - c| > t/2)$ . However, in Petersen's Riemannian Geometry (in his proof of Theorem 7.1.13), and in Lemma 2.2 of this paper , the inequality given is $$
\mu(w > t) \le 2 \mu(|w -c| > t/2),
$$ this extra factor of $2$ leads me to believe that I must have done something wrong, but the argument is so simple I cannot find any problems with it. Am I missing something obvious?","['sobolev-spaces', 'riemannian-geometry', 'differential-geometry']"
4616426,Attempt to prove that every orientable connected manifold has exactly two orientations (Munkres),"I am looking at Munkres' book Analysis on Manifolds and come across the following exercise: Show that if M is a connected orientable $k$ -manifold in $\mathbb R^n$ , then M has precisely two orientations Here is my attempt (following the hint from the book): Choose an orientation of $M$ ; it consists of a collection of coordinate patches $\{\alpha_i\}$ . Let $\{\beta_j\}$ be an arbitrary (fixed) orientation of $M$ . Given $\mathbf x \in M$ , choose coordinate patches $\alpha_i$ and $\beta_j$ about $\mathbf x$ and define $\lambda(\mathbf x)=1$ if they overlap positively at $\mathbf x$ , and $\lambda(\mathbf x)=-1$ if they overlap negatively at $\mathbf x$ . First we note that $\lambda$ is well defined. Choose any two pairs of patches, say $\alpha_1,\beta_1$ and $\alpha_2,\beta_2$ , with $$
\alpha_1(t_1)=\beta_1(s_1)=\mathbf x=\alpha_2(t_2)=\beta_2(s_2).
$$ Suppose $\alpha_1,\beta_1$ overlap positively at $\mathbf x$ , then $$
\alpha_2^{-1}\circ\beta_2=\left(\alpha_2^{-1}\circ\alpha_1\right)\circ\left(\alpha_1^{-1}\circ\beta_1\right)\circ\left(\beta_1^{-1}\circ\beta_2\right).
$$ at some neighrborhood around $\mathbf x$ . It follows that $\alpha_2,\beta_2$ overlap positively iff $\alpha_1,\beta_1$ do. Now, suppose $\alpha:U\to V$ and $\beta:O\to P$ overlap positively at $\mathbf x$ , say with $\alpha(t_0)=\beta(s_0)=\mathbf x$ . Since $V\cap P$ is an open subset of $M$ containing $\mathbf x$ , take a connected open neighborhood $W$ of $s_0$ so that $\beta(W)\subset V\cap P$ . Since $\beta^{-1}$ is continuous, $\beta(W)$ is an open neighborhood of $\mathbf x$ . Consider the function $f:W\to\mathbb R$ defined by $f(s)=\det(D(\alpha^{-1}\beta)(s))$ , then $f(s_0)>0$ by assumption. Moreover, $f\ne0$ on $W$ since transition functions have nonsingular derivative. Since $W$ is connected, $f$ does not change sign on $W$ , so $\alpha$ and $\beta$ overlap positively on $W$ . An entirely similar argument holds for the case where $\alpha$ , $\beta$ overlap negatively at $\mathbf x$ . Hence, $\lambda$ is continuous. Thus, since $M$ is connected, $\lambda$ is either identically $1$ or identically $-1$ . If $\lambda$ is identically $1$ , then every patch in $\{\beta_j\}$ overlaps positively with patches in $\{\alpha_i\}$ , whence $\{\beta_j\}\subset\{\alpha_i\}\subset\{\beta_j\}$ by the definition of orientation. The other case is similar. Hence, $\{\beta_j\}$ is either the same as $\{\alpha_i\}$ or the opposite orientation to $\{\alpha_i\}$ . Can anybody tell me if my proof is correct? I have seen some similar questions on stackexchange, but they seem to assume different backgrounds and do not apply to the book I'm reading now. Any help is appreciated. Thanks in advance! Edit: If two patches $\alpha,\beta$ ""overlap positively,"" the book means the transition function $\alpha^{-1}\circ\beta$ is orientation-preserving, i.e. its derivative has positive determinant everywhere on the domain ( $\det(D(\alpha^{-1}\circ\beta)(x))>0$ ).","['general-topology', 'differential-topology', 'differential-geometry', 'real-analysis']"
4616443,Wedge sum of circles and the Hawaiian Earring,"The (countably infinite) wedge sum of circles is the quotient of  a disjoint countable union of circles $\coprod S_i$ , with points $x_i\in S_i$ identified to a single point, while the Hawaiian earring /infinite earring H is the topological space defined by the union of circles in the Euclidean plane $\mathbb{R}^2$ with center $(1/n, 0)$ and radius $1/n$ for $n = 1, 2, 3, ...$ . In the definition of the countably infinite wedge sum of circles, it is not specified the size of circles, the points which to be identified to a single point etc. So we can take disjoint union of circles of radius $1/n$ and identify a point from each to a common single point to get the Hawaiian earring. I couldn't understand the difference between these two topological spaces. Can someone explain more precisely the difference between these two spaces?",['algebraic-topology']
4616445,MacLaurin Expansion of $\log(1+x) \cdot e^x$,"Could anyone enlighten me on how to go about expanding the following function around $x_0 = 0$ : $$
f(x):= \log(1+x)e^{x}
$$ I have tried using Cauchy Product Series and bruteforce computation of the coefficients but I always find myself with a double sum which is difficult to work with when trying to estimate the radius of convergence. As always any comment or answer is welcome and let me know if I can explain myself clearer!","['functions', 'sequences-and-series', 'taylor-expansion', 'real-analysis']"
4616456,Verify my proof: $A \subseteq B$ iff $\mathscr P(A) \subseteq \mathscr P(B)$,"I'm self learning from the book ""How to Prove it"" by Velleman (3rd edition). I don't have access to a math professor, so I need a little help from the community. Please verify my proof. Problem 8 pag. 140 Prove that $A \subseteq B$ iff $\mathscr P(A) \subseteq \mathscr P(B)$ Proof . $(\to)$ Suppose $ A \subseteq B$ . Let's take an arbitrary subset $x$ from $A$ , then $x \subseteq B$ . But also $x \in \mathscr P(A)$ and $x \in \mathscr P(B)$ . But this is exactly what $ \mathscr P(A) \subseteq \mathscr P(B)$ means. $(\gets)$ Suppose $ \mathscr P(A) \subseteq \mathscr P(B)$ . Because $A \in \mathscr P(A)$ , it means that $A \in \mathscr P(B)$ and also $A \subseteq B$ $\blacksquare$","['elementary-set-theory', 'proof-writing', 'solution-verification']"
4616493,Eigenvectors not stable under matrix perturbation,"Let $A,B$ be square matrices of the same size but arbitrary components. It is a fact that the eigenvalues of $A$ and $A+\epsilon B$ are close for $\epsilon$ small, but the eigenvectors may be drastically different. This latter point is what I am interested in. I am mainly looking for references or work that has been done on this. There is a lot on perturbation of eigenvalues, but I can't really find anything about how perturbations affect eigenvectors. A simple example is $A=\left(\begin{matrix}1 & 0\\0&1\end{matrix}\right)$ , $B=\left(\begin{matrix}-1 & 1\\1&-1\end{matrix}\right)$ and let $\epsilon>0$ be small. Then the eigenpairs for $A$ are just $1,(1,0)^\top$ and $1,(0,1)^\top$ . The eigenpairs for $A+\epsilon B$ are $1,(1,1)^\top$ and $1-2\epsilon,(1,-1)^\top$ . So the eigenvalues are just perturbed from the original ones for $A$ , but the eigenvectors are clearly very different. I saw this wikipedia article that showed some long calculations for the Hermitian, positive definite case with distinct eigenvalues, and, as far as I can tell, the eigenvectors are only perturbed slightly in that case. In my example, if I change $A_{1,1}=1+\delta$ for a small $\delta$ and make $\epsilon$ small enough, then I do get that the eigenvectors are only slightly perturbed, but it seems $\epsilon$ must be very small compared to $\delta$ (e.g. $\epsilon=\delta/100$ seems to work well). My question is this: (1) Are there other cases (other than Hermitian, positive definite case with distinct eigenvalues) where the eigenvectors are indeed only perturbed? I'm specifically interested in nonnegative matrices that are not symmetric (but maybe still with distinct eigenvalues), but also if this is ever possible with repeated eigenvalues. (2) If the eigenvectors are not simply perturbed but drastically different, is there some relation between the new eigenvectors and the old, e.g. something like the each new eigenvector being a linear combination of the old eigenvectors or some other tractable relationship? I find for the upper triangular $2\times2$ case with $A=\left(\begin{matrix}a & b\\0&d\end{matrix}\right)$ , $B=\left(\begin{matrix}0 & 0\\1&-1\end{matrix}\right)$ that we get $\lambda_\pm$ for $A$ with eigenvectors $(b, \lambda_\pm-a)^\top$ . For $A+\epsilon B$ we get the eigenvalues perturbed by $O(\epsilon)$ . The eigenvalues for $A$ , leaving unsimplified, are $$\lambda_\pm=\frac{a+d}2\pm\sqrt{\left(\frac{a-d}2\right)^2}$$ with eigenvectors $(b, \lambda_\pm-a)^\top$ . And the eigenvalues for $A+\epsilon B$ are $$\lambda_\pm=\frac{a+d}2-\frac\epsilon2\pm\sqrt{\left(\frac{a-d}2\right)^2}\cdot\sqrt{1+4\epsilon \frac{b+d}{(a-d)^2}+O(\epsilon^2)}=\lambda_\pm+O(\epsilon)$$ with eigenvectors $(b,\lambda_\pm-a+O(\epsilon))^\top$ So in this case, where the matrix $A$ is clearly not symmetric (at least for certain values of $a,b,d$ ), the eigenvectors are simply perturbed. Also, we can clearly see that as $a$ and $d$ get close, then $\epsilon$ needs to be much smaller to make the eigenvectors close. It's also not too hard to work out the same calculation for the $2\times2$ case of general $A=\left(\begin{matrix}a & b\\c&d\end{matrix}\right)$ , $B=\left(\begin{matrix}-1 & 1\\-1&1\end{matrix}\right)$ to get that the eigenvalues and eigenvectors are similarly changed by $O(\epsilon)$ . I have a feeling this $O(\epsilon)$ perturbation holds for the general $2\times2$ case as long as the eigenvalues are distinct (i.e. as long as $A$ isn't triangular with identical diagonal components), but I haven't worked it out carefully. I definitely don't want to try such a calculation for the $3\times3$ case! But maybe there is something already known for other cases?","['matrices', 'perturbation-theory', 'eigenvalues-eigenvectors']"
4616527,"Find all $4<n,m\in\mathbb N$ for which $\cos^2(\frac{2\pi}n)=\cos(\frac{2\pi}m).$","Find all $4<n,m\in\mathbb N$ for which $\cos^2\left(\frac{2\pi}n\right)=\cos\left(\frac{2\pi}m\right).$ I think it has one non-trivial solution, $n=8,m=6$ . Intuitive idea for a proof: $\cos(x)\approx1-\frac{x^2}{2}$ . $\left(1-\left(\frac{2\pi}{n}\right)^2\right)^2\approx1-\left(\frac{2\pi}{m}\right)^2$ . $1-2\left(\frac{2\pi}{n}\right)^2+\left(\frac{2\pi}{n}\right)^4\approx1-\left(\frac{2\pi}{m}\right)^2$ ; $\sqrt2 m\approx n$ . On the other hand, $\mathbb Q\to\mathbb Q\left[\cos\left(\frac{2\pi}{m}\right)\right]\to \mathbb Q\left[\sqrt{\cos\left(\frac{2\pi}{m}\right)}\right]$ is of degree $2\varphi(m)$ . So, $2\varphi(m)=\varphi(n)$ . From here, I think it is possible to formalize these arguments and arrive at a contradiction for large enough $m,n$ .","['galois-theory', 'trigonometry', 'diophantine-equations']"
4616539,Differentials as linear maps on $\Bbb R^n$,"Currently going through some multivariable analysis and I'm trying to understand why the differentials $\left(dx_{1}\right)_{p},\left(dx_{2}\right)_{p},\ldots ,\left(dx_{n}\right)_{p}$ at a point $p$ form a basis for the vector space of linear maps from $\mathbb {R}^{n}$ to $\mathbb{R}$ and I'm reading through the this article on Wikipedia. Is there some simple example which I could use to understand this better? They refer to this trick on defining the coordinates $x_1, \dots, x_n$ in $\Bbb R^n$ such that for $p = (p_1, \dots, p_n)$ one has $x_j : \Bbb R^n \to \Bbb R$ for which $p \mapsto p_j$ . Is there any visual way to understand why the differentials $\left(dx_{1}\right)_{p},\left(dx_{2}\right)_{p},\ldots ,\left(dx_{n}\right)_{p}$ at a point $p$ form a basis for the dual space $(\Bbb R^n)^*$ ? I would be even interested in the cases when $n$ is small say $2$ or $3$ . It would be perhaps easier for me to first understand why the partial derivatives form a basis for the tangent space on $\Bbb R^n$ since we can use those to define the dual?","['multivariable-calculus', 'calculus', 'real-analysis']"
4616546,$Y\sim Poi(\lambda)$ trials are conducted with a probability of success $p$. Find the distribution of the number of successful trials.,"For a given experiment, $Y\sim Poi(\lambda)$ independent trials are conducted, with each trial having a probability of success $p$ and a probability of failure $1-p$ . Find the distribution of the number of successful trials $X$ ."" First of all, $Y$ Bernouilli trials are conducted, thus $$X\sim\mathit{Bin}(Y, p)$$ Therefore $$P(X=k)=\binom{Y}{k}p^k(1-p)^{Y-k}$$ From $Y\sim Poi(\lambda)\Longrightarrow P(Y=t)=\frac{\lambda^t}{t!}e^{-\lambda}$ Am I on the right path? How can I continue now?","['poisson-distribution', 'binomial-distribution', 'probability']"
4616547,Can this intuitive explanation of the Change of Variable Theorem be turned into a proof?,"Reading the following intuitive explanation of the Change of Variable Theorem, I wonder if the explanation provided can be made into a rigorous proof. As of now, I'm merely interested in the case of Riemann integration: Theorem: let $A\subseteq \mathbb{R}^n$ be a $C^1$ injective function such that $\det g'(a) \ne 0$ for any $a\in A$ . If $f:g(A)\to \mathbb{R}$ is (Riemann) integrable, then $$\int_{g(A)}f = \int_A(f\circ g) |\det g'|.$$ Other proofs I've seen take a different, seemingly less intuitive route. As an example, Spivak's Calculus on Manifolds first applies multiple 'reductions' (e.g. showing that it is sufficient to prove the result whenever $g$ is a linear transformation), and continues by applying induction on the dimension $n$ of $\mathbb{R}^n$ .","['integration', 'proof-explanation', 'alternative-proof', 'multivariable-calculus', 'riemann-integration']"
4616553,Smooth family of metrics,"Let $M$ be a smooth manifold and $(g(t))_{t\in[a,b]}$ be a family of complete Riemannian metrics (on $M$ ) that smoothly depend on $t$ . If $x$ and $y$ are two arbitrary points on $M$ , we can define a distance function $f(t):=d_{g(t)}(x,y)$ that varies through time. The distance $d_{g(t)}$ is the usual distance induced by the Riemannian  metric $g(t)$ . What I am interested in is the regularity of the function $f$ , and specifically I want to use the Rademacher's theorem which guarantees me that $f$ is almost everywhere differentiable on $M$ if I can prove that $f$ is Lipschitz continuous or at least locally Lipschitz continuous. A first idea is to use the completeness of $M$ to deduce that there is a minimising unit-speed geodesic $\gamma_t$ between $x$ and $y$ at any time, so we write : $$d_{g(t)}(x,y)=L_{g(t)}(\gamma_t)$$ But I can't prove why the family of minimising geodesic $(\gamma_t)$ would also depend smoothly on time. Moreover, I want to also prove an explicit formula for the derivative (when $f$ is differentiable ) which is : $$\frac{d}{dt}|_{t_0}d_{g(t)}(x,y)=L_{g(t_0)}(\gamma_{t_0})$$ Any help is appreciated.","['riemannian-geometry', 'lipschitz-functions', 'smooth-manifolds', 'derivatives', 'differential-geometry']"
4616573,"If $A\geq S^TS$ and $A\geq \lambda$ for symplectic $S$, is $A\geq R^TR\geq \lambda$ for some symplectic $R$?","Consider a $2n$ -by- $2n$ real matrix $A$ such that there exists symplectic matrix $S \in Sp(2n,\mathbb{R})$ with $A \geq S^TS$ and $A \geq \lambda \mathbb{1}$ for some $\lambda \in [0,1]$ . Does there exist symplectic matrix $R \in Sp(2n,\mathbb{R})$ such that $A \geq R^T R \geq \lambda \mathbb{1}$ ? Note that this is equivalent to asking: For PSD $A$ with symplectic eigenvalues greater than $1$ and eigenvalues greater than $\lambda$ , does there exist PSD $B$ with unit symplectic spectrum and $A \geq B \geq \lambda \mathbb{1}$ ? For more on symplectic eigenvalues, see e.g. On Symplectic eigenvalues... by Bhatia and Jain.","['symplectic-linear-algebra', 'linear-algebra', 'positive-definite']"
4616581,Finding the squared expectation of a random variable that is a composite of a summation of random variables.,"Given the following, what is the squared expectation of random variable g(x)? g(x) = $\pi$ f( $x_1$ ) + (1- $\pi$ )f( $x_2$ ), and we are told simply that $f_1$ and $f_2$ have means $\mu_1$ and $\mu_2$ and variances $\sigma_1^2$ and $\sigma_2^2$ respectively.  Furthermore, pi is ""some number"" existing between 0 and 1, inclusive. I have been working on this problem, and my current solution is: E[ $X^2$ ] = $\pi^2$$\sigma_1^2$ + $(1 - \pi)^2\sigma_2^2$ + $\sigma_1\sigma_2$ + $\pi\mu_1$ + $(1 - \pi)\mu_2$ My reasoning is that E[g(x)] = $\pi\mu_1 + (1-\pi)\mu_2$ , since pi is a constant, and according to the rules of expectations, the constant should be simply multiplied against the expectation of the density it is a coefficient to. $E[X^2] = E[g^2(x)] = Var(X) + E[X]$ , which is a rearrangement of the definition of the variance. Said variance ought to be the first three terms of my solution, if I understand it right, and the constants are squared according to the rule that $Var(aX) = a^2Var(X)$ .  The third term comes from the fact that $Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$ , and that the $Cov(X,Y) <= SD(X)SD(Y)$ Put everything together, and we should arrive at my current solution. That being said, the question is a multiple choice one, and none of the possible solutions match mine.  I'm not confident that this solution I have can be simplified to arrive at one of the choices either.","['statistics', 'covariance', 'variance', 'expected-value', 'probability']"
4616585,Prove that any continuous function $f:S_\Omega \rightarrow \mathbb{R}$ is eventually constant.,"Let $\Omega$ be the first non-numerable ordinal number ( $\aleph_1$ is the first cardinal number greater than $\aleph_0$ when treated as an ordinal number is denoted by $\Omega$ ) and let $[0,\Omega)$ be the subspace of the ordinal space $[0,\Omega]$ . Then every continuous $\varphi : [0,\Omega)\rightarrow E^{1}$ must be constant in a tail of $[\beta,\Omega)$ . My attempt It must be proved that the function $\varphi$ is constant from a certain value of the subspace $[0,\Omega)$ , i.e., for all $n\in \mathbb{Z}^{+}$ there exists a $\alpha_n < \Omega$ for all $\xi> \alpha_n:|\varphi(\xi)-\varphi(\alpha_n)|< \frac{1}{n}$ (*). Suppose this were not true, then there exists a $n_0$ for every $\alpha<\Omega$ there exists a $\xi>\alpha$ such that $|\varphi(\xi)-\varphi(\alpha)|< \frac{1}{n_0}$ .
Since each element of $[0, \Omega)$ has an immediate predecessor then, we can use induction to construct a countable succession, that is $\{\xi_i: i\in \mathbb{Z}^{+}\}$ such that. $$\xi_i<\xi_{i+1}$$ and $$|\varphi(\xi_i)-\varphi(\xi_{i+1})| \geq \frac{1}{n_0}$$ for each $i$ ,
choosing $\xi_1=0$ and assuming that $\xi_1, \xi_2,...,\xi_k$ are defined, since $\xi_k$ has a successor, then $\xi_{k+1}$ is the first element in $[\xi_k, \Omega)$ that satisfies the hypothesis. The succession $\{\xi_i\}$ thus constructed,  would have a minimum upper bound $\gamma<\Omega$ and then $\varphi$ would not be continuous in $\gamma$ , since any neighborhood $(\eta, \gamma]$ contains some $\xi_i$ and therefore any $\xi_k$ , $k>i$ cannot have an image contained in the $$\left( \varphi(\gamma)-\frac{1}{3n_0}, \varphi(\gamma)+\frac{1}{3n_0}\right)$$ neighborhood of $\varphi(\gamma)$ . Since this contradicts the definition of continuity, let's see. Let $(\eta,\gamma]$ be an open containing $\gamma$ . This open contains infinitely many $k_i$ , for $k>i$ we have $$|\varphi(\xi_k)-\varphi(\gamma)|$$ but $$|\varphi(\xi_k)-\varphi(\xi_{k+1})+\varphi(\xi_{k+1})-\varphi(\gamma)|=|\varphi(\xi_k)- \varphi(\gamma)|\leq \frac{1}{3n_0}$$ luego $$|\varphi(\xi_k)-\varphi(\xi_{k+1})|-|\varphi(\xi_{k+1})-\varphi(\gamma)|\leq \frac{1}{3n_0}$$ $$\frac{1}{n_0}-|\varphi(\xi_{k+1})-\varphi(\gamma)|\leq \frac{1}{3n_0}$$ from where $$\frac{1}{n_0}\leq \frac{1}{3n_0}+\frac{1}{3n_0}$$ with which we have that $1\leq \frac{2}{3}$ , this is a contradiction, therefore it is satisfied that for all $n\in \mathbb{Z}^{+}$ there exists a $\alpha_n < \Omega$ for all $\xi> \alpha_n: |\varphi(\xi)-\varphi(\alpha_n)|< \frac{1}{n}$ . Now, let $\beta$ be an upper bound of $\alpha_n$ ;then $\beta <\Omega$ and $\varphi$ is constant in $[\beta,\Omega)$ : We must prove that if $\beta \in [\beta,\Omega)$ , then the images are equal, otherwise we have that $$|\varphi(\zeta)-\varphi(\alpha_{n})|<\frac{1}{n}$$ and $$|\varphi(\beta)-\varphi(\alpha_n)|<\frac{1}{n}$$ for all $n$ , thus. \begin{equation*}
     \begin{split}
|\varphi(\zeta)-\varphi(\beta)|=&|\varphi(\zeta)-\varphi(\alpha_n)+\varphi(\alpha_n)-\varphi(\beta)|\\
\leq & |\varphi(\zeta)-\varphi(\alpha_n)|+|\varphi(\alpha_n)-\varphi(\beta)|\\
\leq & \frac{1}{n}+\frac{1}{n}\\
=&\frac{2}{n}
     \end{split}
 \end{equation*} for all n. And therefore $\varphi(\zeta)=\varphi(\beta)$ . the proof is correct ? and I would also like to justify the step (*)","['ordinals', 'continuity', 'functions', 'constants', 'general-topology']"
4616589,Exact sequence of direct sums of $\mathbb{Q}/\mathbb{Z}$,Suppose $$0 \to (\mathbb{Q}/\mathbb{Z})^m \to (\mathbb{Q}/\mathbb{Z})^{m + n} \to (\mathbb{Q}/\mathbb{Z})^n$$ is a left exact sequence of abelian groups. Is it also right-exact (i.e. is $(\mathbb{Q}/\mathbb{Z})^{m + n} \to (\mathbb{Q}/\mathbb{Z})^n$ surjective?) I want to say that the cokernel is finite and therefore $0$ since $(\mathbb{Q}/\mathbb{Z})^n$ has no finite quotients. But how do you get around the fact that these groups are not finitely generated (in which case it would be easy by just looking at ranks)?,"['modules', 'exact-sequence', 'abstract-algebra', 'abelian-groups', 'commutative-algebra']"
4616638,"Let $F:L^3[0,2]\to \mathbb R$ defined as $F\left(f\right)=\int _0^2t^{-\frac{2}{5}}f\left(t\right)dt.$ Find $||F||$.","Find $||F||$ . Let $F:L^3[0,2]\to \mathbb R$ defined as $$F\left(f\right)=\int _0^2t^{-\frac{2}{5}}f\left(t\right)dt.$$ My attempt:- $|T(f)|=|\int _0^2t^{-\frac{2}{5}}f(t)dt|\leq\int_0^1|t^{\frac{2}{5}}f(t)|dt\leq (\int_0^2 (t^{-\frac{2}{5}})^{\frac{3}{2}}dt)^{\frac{2}{3}}(\int_0^2 |f(t)|^3dt)^{\frac{1}{3}}(\because$ by Holder, inequality) $\leq (\int_0^2 |f(t)|^3dt)^{\frac{1}{3}}=||f||_3\implies ||F||\leq 1.$ Let $f(t)=\frac{1}{2} t^{2/5}\in L^3[0,2].$ I am getting $F(f)=1\implies ||F||=1.$ (Attempt was wrong) Sorry. It was mistaken. $f$ is not in the closed unit ball. I am not able to find the function which gives $T(f)=1.$","['normed-spaces', 'functional-analysis']"
4616651,Lee Introduction to Smooth Manifolds Problem 8-15: Extension Lemma for Vector Fields on Submanifolds,"Three questions have been asked previously on this topic. First, there was Proving The Extension Lemma For Vector Fields On Submanifolds , asked over 10 years ago (!), which did not copy the problem statement exactly correctly. What was left out was that the embedded
submanifold $S\subseteq M$ could actually be a submanifold with boundary. When I tried to
work the first part of this problem, the solution given in the accepted answer to the
referenced question seemed to not work when $S$ had a boundary. The second question asked
previously on this topic was Lee book introduction to smooth manifold problem 8.15 , which correctly stated the problem as printed in the book. The OP of this question wrote that
they ""could manage"" the first part of the problem. Unfortunately, the OP didn't share
the proof they had. (Maybe the margin was too small to contain it.)
The OP went on to say that they were asking for help with the reverse
direction of the second part of the problem. Interestingly, the one answer given for this
question did not address what the OP originally asked for, but rather gave a claimed
proof for the first part. However, there seems to be a technical detail missing from the
answer which has stymied me. The third question asked earlier was Better proof that vector fields on submanifolds extend globally iff submanifold is closed , which didn't go into the proof of the
first part of the problem in either the question or the answer. Here's the technical detail in the answer to the second question that troubles me.
(I'm paraphrasing the answer given): $n=\dim M$ , $k=\dim S$ , $X\in\mathfrak{X}(S)$ , $p\in S$ , $(U_p,\phi_p)$ is a
smooth slice (or half-slice) chart centered at $p$ for $S$ in $M$ , $V_p=S\cap U_p$ , $\pi\colon\mathbb{R}^n\to\mathbb{R}^k$ is the projection on the first $k$ coordinates, $\psi_p=\pi\circ\phi_p|_{V_p}$ , $(V_p,\psi_p=(x^i))$ is a smooth
chart centered at $p$ for $S$ , and $X$ is expanded as $$X|_{V_p}=X^i\frac{\partial}{\partial x^i},$$ where for $i=1,\dots,k$ , $X^i\colon V_p\to\mathbb{R}$ is a smooth function on $V_p$ as an open submanifold with or without boundary of $S$ .
Due to the slice condition, $V_p$ is also a closed subset of $U_p$ .
At this point the answer suggests
that we can use the Extension Lemma for Smooth Functions (Lemma 2.26 in Lee ISM) to
extend $X^i$ smoothly to $U_p$ . But I don't see how the hypotheses of that lemma
are met in this case.
Specifically, one needs to show that $X^i$ is smooth on $V_p$ , and I'm having
a hard time doing that. All my attempts fail because given a point $q$ in $V_p$ (or even
some subset of $V_p$ ), I can't seem to find a neighborhood of that point in $U_p$ on
which a smooth function is defined which agrees with $X^i$ in the appropriate overlap
set. So my question is, how do you fix (or get around) this problem? Note that I am not asking
about a complete solution to Problem 8-15. I'm only requesting help with the first part,
and only of the part before we bring in the smooth partition of unity. That is, I
just want to be able to prove that there is a smooth extension of $X^i$ to $U_p$ .
I also understand that it would not hurt to restrict $V_p$ first if necessary, so long
as it still contains $p$ .","['vector-fields', 'submanifold', 'smooth-manifolds', 'differential-geometry']"
4616667,Equivalence classes of polynomials under function transformations,"Consider the sets of degree $n$ polynomials, $$P_{n} = \big\{ a_n x^n + a_{n-1}x^{n-1} + \cdots a_1 x + a_0\ : a_{n} \neq 0 \big\},$$ and the collection of classical function transformations: horizontal dilation : $p(x) \mapsto p(x/k), \quad \text{where $k\neq 0$} $ vertical dilation : $p(x) \mapsto k p(x), \quad \text{where $k \neq 0$}$ horizontal translation : $p(x) \mapsto p(x-k)$ vertical translation : $p(x) \mapsto p(x) + k$ It's clear that any composition of these transformations will map $P_n$ to itself. Moreover, it seems reasonable that these operations should allow us to define a set of congruence classes on the set $P_n$ defined as follows: For $p(x)$ and $q(x)$ in $P_n$ , we say that $p(x) \sim q(x)$ if and only if there is a set of function transformations which transform $p(x)$ into $q(x)$ . Obviously this relationship is reflexive, since $p(x) \sim p(x)$ by the identity transformation; and since each transformation has an inverse transformation, then the symmetric and transitive properties of $\sim$ should hold as well. This brings me to my actual question: What (if anything) can be said about the set of equivalence classes, $P_{n} \,/ \sim$ ? Of note, since all parabolas are similar this means, interestingly enough, that $\left|P_{2}\, / \sim\right| = 1$ , since the equivalence class $\left[x^2\right]$ contains all quadratics. I'm just not sure how to even approach something like $P_3 \,/ \sim$ or beyond, so any suggestions would be quite welcomed.","['elementary-set-theory', 'equivalence-relations', 'transformation', 'polynomials']"
4616713,Geometric characterization of the $n$-th derivative of $f$ being positive (convexity for $n=2$).,"Does the following claim hold? Let $n\in \mathbb N_{\geq 2}$ and let $f:\mathbb R \to \mathbb R$ be an $n$ -times differentiable function. Then the $n$ -the derivative $f^{(n)}(x)\geq 0$ for all $x\in \mathbb R$ iff for any $a_1<a_2<\ldots<a_n$ $$
  f(x) \leq p(x;a_1,\ldots,a_n) \quad \forall x \in [a_{n-1},a_n], \tag{*}\label{fp}
$$ where $p(\cdot; a_1,\ldots,a_n)$ is the $n-1$ degree polynomial fitting the points $\big(a_1,f(a_1)\big),\ldots,\big(a_n,f(a_n)\big)$ . Note: The condition \eqref{fp} can be geometrically represented as follows: For any $n$ different points of the graph of $f$ consider the curve line segment between the last two points (those most on the right) of the polynomial curve fitting the graph of $f$ at the $n$ points. Then the curve line segment is contained in the epigraph of $f$ . Observations: For $n=2$ the condition \eqref{fp} simplifies into that the line segment between any two points of a graph of $f$ is in the epigraph of $f$ – the standard characterization of $\mathop{epi} f$ being a convex set. In general the condition $f^{(n)}(x)\geq 0$ can be interpreted as that $f^{(n-1)}$ is increasing, and equivalently that $f^{(n-2)}$ is a convex function. The intuitive idea is that if $f^{(n)}$ was constant then $f$ would be identical to the polynomial $p$ that fits any $n$ points in the graph of $f$ , and as $f^{(n)}$ is increasing it can be expected that $f$ would be above $p$ for $x$ large. Extensions: Extensions of this claim (with the condition that $f^{(n-1)}$ is increasing instead of that $f^{(n)}$ is positive) into the cases when $f$ is not necessarily $n-1$ times differentiable is discussed in A higher-order analogy to that if $f'$ is increasing on $\Bbb R$ except for countably many points, then $f$ is convex and A higher-order analogy to that every convex function is differentiable at all but countably many points . A characterization of the condition \eqref{fp} as a property of $\mathop(epi)(f)$ in the case of $n=3$ is discussed in Geometric characterization of functions with positive third derivative .","['convex-geometry', 'geometry', 'examples-counterexamples', 'real-analysis', 'convex-analysis']"
4616745,Derivative of Inverse Linear Transformation,"I am going through Sagle and Walde's Introduction to Lie Groups and Lie Algebras and am in the first chapter reviewing (advanced) calculus. I am mostly OK with the advanced calculus stuff, I have been through most of Widder and some of Spivak. There is an exercise in the differentiation section I don't quite get: Show the map $$
f: GL(V) \rightarrow GL(V) : T \rightarrow T^{-1}
$$ is differentiable at P and $$
Df(P)T = -P^{-1}TP^{-1}.
$$ Thus show $f\in C^{1}(GL(V),End(V))$ I understand the question, however, I don't see how we can end up with a $P^{-1}$ multiplied on both sides. If I say: $$
\begin{align}
f(P)&=P^{-1} \\
Pf(P)&=I \\
f(P) + PDf(P) &= 0 \\
PDf(P) &= -f(P) \\
PDf(P) &= -P^{-1} \\
Df(P) &= -P^{-2}
\end{align}
$$ With this derivative, we would get $$
Df(P)T = -P^{-2}T
$$ Additonally, the derivative should be a linear transformation, and so it should have an inverse. In my case, that inverse is $P^{2}$ . In the case of what the book claims, I don't see how there can be a single inverse independent of $T$ . I am guessing I am missing something because the book is Volume 51 in the Academic Press series Pure and Applied Mathematics, so I am sure a lot of good eyes were on it. Any clarification would be appreciated.","['multivariable-calculus', 'derivatives', 'analysis']"
4616794,"Let $x$ is the angle of the diagonal lines of a unit cube, then what are the possible values of $\cos x$?","Let $x$ is the angle of the diagonal lines of a unit cube(the cube with edges $1$ ), then what are the possible values of $\cos x$ ? My attempt: Since the dot products of the diagonal lines are $1$ or $-1$ ,and the lengths of them are all $3^{1/2}$ , so $\cos x$ are $$1/(3^{1/2}3^{1/2})=\cos x=1/3$$ or $$-1/(3^{1/2}3^{1/2})=\cos x=-1/3.$$ Am I right? If I am wrong, then please correct my answers. Moreover, it should have $16$ possible $\cos x$ values, right?","['inner-products', 'trigonometry', 'geometry', 'geometric-construction']"
4616859,Flatness as an integrability condition in the context of Riemannian geometry,"I am reading Y. Eliashberg and N. Mishachev's book Introduction to the $h$ -principle . At the beginning of section 9.2.F, on page 82, the authors say: Note that a symplectic (complex) submanifold $S\subset V$ of a symplectic (complex) manifold $V$ ingerits an integrable symplectic (complex) structure. This contrasts with the Riemannian case: the symplectic integrability condition is analogous to the condition in the Riemannian case of being locally Euclidean, but submanifolds of locally Euclidean manifolds need not, of course, be locally Euclidean. In the symplectic setting, integrable means that the nondegenerate 2-form $\omega$ on $V$ provided by an almost symplectic structure is indeed a symplectic structure, i.e., it satisfies the condition $d\omega=0$ . On the other hand, integrable in the complex case means that the almost complex structure $J$ on $V$ satisfies that its Nijenhuis tensor vanishes. I am terribly confused about the aforementioned statement; I actually don't know what the authors are trying to express. For instance, what do we mean by locally Euclidean ? All manifolds are locally Euclidean. Maybe the authors are pointing out that, in symplectic geometry, all symplectic manifolds are built up from Darboux coordinates glued toghether by symplectomorphisms, and that Riemannian manifolds, in contrast, are not generally flat , or rather, flatness is not a property that is preserved by taking the restriction of the metric to the corresponding submanifold. But again, why being flat should be considered an integrability condition ? It makes sense from the point of view of its construction, that is, flat manifolds are built from charts to Euclidean space with the usual metric, glued toghether by isometries, but I think I am missing to what this structure integrates to. Thanks in advance for your answers.","['symplectic-geometry', 'riemannian-geometry', 'complex-geometry', 'differential-topology', 'differential-geometry']"
4616905,Geometric Interpretation of existence of a trivial subrepresentation inside $\mathfrak{sl}_2 \mathbb{C} $-rep $ \text{Sym}^4(\text{Sym}^3 V))$,"Let $V \cong \mathbb{C}^2$ be the standard representation of $\mathfrak{sl}_2 \mathbb{C} $ ragarded as subalgebra inside $\text{Mat}_2(\mathbb{C})$ .  We follow the convention from Fulton & Harris' Representation Theory book and choose as generators for $\mathfrak{sl}_2 \mathbb{C}$ the matrices $H, X,Y$ given on page 147 and so we obtain the eigenvalue/ weight decomposition $V= V_{-1} \bigoplus V_1= \mathbb{C} \cdot y \oplus \mathbb{C} \cdot x$ with respect to the both weights (here also eigenvalues) $1,-1$ of the generator $H$ of the Cartan algebra. Thinking about $x$ and $y$ as polynomials allows to give a natural interpretation to symmetric powers $\text{Sym}(V)$ as vector space of homogeneous polynomials of degree $n$ in $x$ and $y$ . The Exercise 11.24. on page 158 asks the reader to analyze the induced representation $ \text{Sym}^4(\text{Sym}^3 V))$ and to show that it contains in it's decomposition in irreducible pieces a trivial one-dimensional subrepresentation $V_0 \cong \mathbb{C}$ . Then it is remarked that the fact that in the decomposition of $ \text{Sym}^4(\text{Sym}^3 V))$ occurs the trivial one-dimensional subrepresentation, gives an interesting geometric interpretation. Namely, to say that $ \text{Sym}^4(\text{Sym}^3 V))$ has such an invariant one-dimensional subspace is to say that there exists a quartic surface in $\mathbb{P}^3 \cong \mathbb{P}(\text{Sym}^3 V)$ preserved under all motions of $\mathbb{P}^3$ carrying the rational normal curve $C$ into itself. This part I not understand. In §11.3, page 153 there is explained how to think about rational normal curve as subset $C= \{[v \cdot v \cdot ... \cdot v] : v \in V\} $ inside $ \mathbb{P}(\text{Sym}^n V) \cong \mathbb{P}^n$ . But how to use the existence of the trivial one-dimensional subrepresentation of $ \text{Sym}^4(\text{Sym}^3 V))$ to conclude the existence of the quartic surface inside $\mathbb{P}(\text{Sym}^3 V)$ ? Why is it a quartic? Moreover the text claims that one can say explicitely what this surface is: The answer is simple: it is the tangent
developable to the twisted cubic, that is, the surface given as the union of the
tangent lines to $C$ . How all these amazing conclusions can be drawn just from the existence of this trivial one-dimensional subrepresentation in the decomposition of $\mathbb{P}(\text{Sym}^3 V)$ ?","['algebraic-geometry', 'representation-theory', 'lie-algebras', 'lie-groups']"
4616907,How many copies of $K_3$ in $K_n$,"I don't have any idea how to do these questions. Can I have some help ? $(i)$ How many copies of $K_3$ are there in $K_n$ ? I know there are $n$ choose $3$ ways of choosing the $3$ vertices. I could then multiply this by $3!$ to order them. Not sure what to do from there. $(ii)$ Fix distinct vertices $x$ and $y$ in $K_n$ . How many copies of $K_3$ in $K_n$ contain the edge $xy$ ? $(iii)$ Fix distinct vertices $x, y$ and $z$ in $K_n$ . How many copies of $K_3$ in $K_n$ contain at least one of the edges $xy, yz$ and $xz$ ? $(iv)$ Prove that every graph with $6$ vertices and at least $10$ edges contains a copy of $K_3$","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4616928,Can I use use simplification (rule of inference) inside a negated expression?,"The question gives these premises and asks for the conclusion. $P \land Q$ $P \implies \neg(Q\land R)$ $S\implies R$ Also there are choices: a) $\neg S$ b) $\neg P$ c) $\neg R$ d) None of the above Here is my approach: $P \land Q$ , premise $P$ , simplification from 1 $P \implies \neg (Q\land R)$ , premise $\neg (Q \land R)$ , modus ponens from 2 and 3 $\neg Q \lor \neg R$ , De Morgan's law applied on 4 $S \implies R$ , premise $\neg S \lor R$ , logically equivalent to 6 $\neg Q \lor \neg S$ , resolution from 5 and 7 At this point, should I choose none of the above? or if I continue as $\neg (Q\land S)$ , from 8 $\neg (S)$ , simplification of $(Q \land S)$ in step 9 My question: Is step 10 true? can I do simplification inside the negation and then apply the negation? and in this case the answer would be (a).","['propositional-calculus', 'logic', 'discrete-mathematics']"
4616939,Characteristic function of a random variable with P({X=k}) = $2^{-k}$,"Find the characteristic function of a random variable X such that P({X=k}) = $2^{-k}$ , $k =1,2,3,4,5, \ldots$ What I was doing is: $$
\phi_x(t) = E(e^{itx}) = \sum^{\infty}_{k=1} 2^{-k} * e^{itk} = \sum^{\infty}_{k=0} \left(\frac{e^{it}}{2}\right)^k -1
$$ From that, I don't know what else to do.","['geometric-series', 'characteristic-functions', 'probability']"
4617017,"Why is $\mathrm{gcd}(n^a+b,(n+1)^a+b)$ almost always prime when $n$ is large?","Given is this function: $$
m=\mathrm{gcd}(n^a+b,(n+1)^a+b)\\
a,b,c\in\mathbb{N} 
$$ On OEIS A118119 , for $2\leq a\leq 84$ and $b=1$ , the smallest values for $n$ in each case where this expression is greater than 1 are given. Comparable lists for $b=2$ to $b=19$ are OEIS A255852 to OEIS A255869 . Among them you can find quite large values for n. For example OEIS A255852 says (not in the actual list, but in the comment) that the smallest $n$ in the case of $a=47,b=2$ is a number with 320 decimal places. I am most interested in values of $m$ that are composite numbers, because that would help me a lot in solving another problem . (See also my comment at the end of my question.) The values for $m$ are not shown in the tables linked above, but you can easily work them out for yourself. For example, I get these values: $$
a=8, b=18, n=5 \rightarrow m=187 = 11\cdot 17\\
\mathrm{gcd}(5^{8}+18,(5+1)^{8}+18)=187\\
\,\\
a=9, b=8, n=5 \rightarrow m=11557 = 7\cdot 13\cdot 127\\
\mathrm{gcd}(5^{9}+8,(5+1)^{9}+8)=11557\\
\,\\
a=50, b=1, n=2 \rightarrow m=12625 = 5^3\cdot 101\\
\mathrm{gcd}(2^{50}+1,(2+1)^{50}+1)=12625\\
\,\\
a=72, b=1, n=2 \rightarrow m=55969=97\cdot 577\\
\mathrm{gcd}(2^{72}+1,(2+1)^{72}+1)=55969
$$ But as soon as $n$ becomes bigger, all values listed in the 20 tables mentioned above, result in values for $m$ that are prime. Here is an examples: $$
a=9, b=16, n=5982 \rightarrow m=31177\,\,\mathrm{is\,prime}\\
\mathrm{gcd}(5982^{9}+16,(5982+1)^{9}+16)=31177
$$ I am interested in those combinations of $a$ and $b$ , where $n$ has more than about 20 decimal digits. But in all of the cases listed in the tables, I always get prime numbers for $m$ : $$
a=23, b=13, n=320594291825643656342 \rightarrow m=735616280024182244143\,\,\mathrm{is\,prime}\\
\,\\
a=17, b=9, n=8424432925592889329288197322308900672459420460792433 \rightarrow m=8936582237915716659950962253358945635793453256935559\,\,\mathrm{is\,prime}\\
\,\\
a=19, b=6, n=1578270389554680057141787800241971645032008710129107338825798 \rightarrow m=5299875888670549565548724808121659894902032916925752559262837\,\,\mathrm{is\,prime}
$$ My Questions: Why is it the case, that composite numbers for $m$ appear only for small values of $n$ ? Is there a combination of $a$ and $b$ not listed in the tables mentioned above, such that $n$ has more than 20 digits and $m$ is a composite number? Comment, not part of the question: Why am I searching for composite numbers for $m$ when $n$ is large? This has to do with another question I asked a few days ago. Let's assume that the smallest $n$ for $a=40, b=12$ where $m=\mathrm{gcd}(n^a+b,(n+1)^a+b)$ is greater than 1 has the value $N>10^{20}$ and the value of $m$ in this case is $M$ where $M=2\cdot M'$ . Then I could write these two Python programs: # tests only odd values for n
import math
n = 1
gcd = 1
while gcd == 1:
    n += 2
    gcd = math.gcd(n ** 40 + 12, (n + 1) ** 40 + 12) and # tests only even values for n
import math
n = 2
gcd = 1
while gcd == 1:
    n += 2
    gcd = math.gcd(n ** 40 + 12, (n + 1) ** 40 + 12) Then only one of these programs would halt when it reaches $n=N$ while the other would run forever.","['number-theory', 'gcd-and-lcm', 'big-numbers', 'coprime', 'prime-numbers']"
4617020,High school combinatorics problem,"Problem: consider a three  code $abc$ such that $a,b, c$ are assigned one of the values from the numbers $1,2,3,4,5.$ i) find the total number of possible codes where each value can be repeated (example: $121$ or $444$ ). ii) assuming no value is repeated. My thoughts, for i) isn't it just $5^3$ as repetition is allowed? Also for ii) since no repetition then wouldn't it be just $5\times 4 \times 3$ ? I am not entirely sure about. my answers so was checking, I will appreciate the help if the answers are wrong","['permutations', 'solution-verification', 'combinatorics']"
4617025,How do I do it using complex numbers?,"If $\;\cos\alpha+\cos\beta+\cos\gamma= \sin\alpha+\sin\beta+\sin\gamma=0$ , then show that $\cos^2\!\alpha+\cos^2\!\beta+\cos^2\!\gamma=\sin^2\!\alpha+\sin^2\!\beta+\sin^2\!\gamma=\!3/2$ I have done this problem with just trigonometric identities, but I want to know how can I get it with complex numbers. I have tried and gotten the following results, $\mathrm{CiS}(\alpha)+\mathrm{CiS}(\beta)+\mathrm{CiS}(\gamma)=0$ $\mathrm{CiS}^2(\alpha) + \mathrm{CiS}^2(\beta) + \mathrm{CiS}^2(\gamma) = 0$ I have seen very similar questions with the same given condition and tried methods from there but I couldn't solve it all the way. Thanks.","['trigonometry', 'complex-numbers']"
4617049,Almost sure convergence of $L_1$-Wasserstein distance between the empirical and true CDFs,"Let $X_1, X_2, \ldots$ be i.i.d. from a distribution with cdf $F$ . Let $F_n$ denote the empirical cdf $F_n(t) = \frac{1}{n}\sum\limits_{i=1}^n I(X_i \leq t)$ . How can we prove that $\int_{\mathbb{R}}|F(t) - F_n(t)|\text{ d}t \overset{a.s.}{\longrightarrow} 0$ if $\operatorname{E}[X_1] < \infty$ ? Barrio, Giné, and Matrán (1999) claims that this should follow from Glivenko-Cantelli, Law of Large Numbers, and Dominated Convergence Theorem. Matsak (2006) claims that this should follow from the Law of Large Numbers on Banach spaces. Despite these references, I was unable to fill in the details to prove the claim.","['measure-theory', 'cumulative-distribution-functions', 'probability-distributions', 'probability-theory', 'random-variables']"
4617058,Simulation of N-parameter Wiener Field,"In order to do some numerical experiment, I would like to simulate an N-parameter (or at least 2-parameter) Wiener Field (see : https://encyclopediaofmath.org/wiki/Wiener_field for definitions). I am sure this is pretty standard, but as I don't know much about stochastic simulations, I don't really see how to do it. Thus, if you know any references where this problem is tackled, that would allow me to fill my lack of knowledge !","['simulation', 'reference-request', 'computational-mathematics', 'brownian-motion', 'probability-theory']"
4617075,Does weakly Riemann integrable implies strongly Riemann integrable in operator theory?,"Let $f:[0,1]\to X$ be a function where $X$ is a Banach algebra. We will say $f$ is strongly Riemann integrable if the limit of the sum $\sum_{i=1}^k f(t_i)(t_{i}-t_{i-1})$ as nets where the poset is the set of all partions $\{0=t_0<t_1<\cdots<t_k=1\}$ of $[0,1]$ with refinement as order. We say $f$ is weakly Riemann integrable means for all linear functionals $x^\ast$ on $X$ we have $x^\ast(f)$ is Riemann integrable. My question is are both definitions equivalent?","['operator-theory', 'functional-analysis', 'real-analysis']"
4617097,"A solution to $\dot{y}(t)=y(t)^2−y(t)+1$ with initial condition $y(0)=1$ is increasing, decreasing or constant?","A solution to $\dot{y}(t)=y(t)^2−y(t)+1$ with initial condition $y(0)=1$ is increasing, decreasing or constant ? I am in a bit of a dead-end with this question. I've tried solving the equation, but I get this complicated expression (from Wolfram Alpha) that I'm not even sure how to interpret: $$ y(t)=\frac{\sqrt{3}}{2}\tan\left[\frac{\sqrt{3}}{2}(t+c_1)\right]+\frac{1}{2}. $$ Finally, I've also considered using the regular fixed-point method for dynamical systems – setting $\dot{y}(t)=0$ and analysing the qualitative behaviour of the equation. However, the resulting expression $y^2-y+1=0$ does not have real roots, which makes me suspect that this is not the way to approach this, either. Any ideas on what the question may be asking, or ideas on how to do this? Thanks.","['ordinary-differential-equations', 'dynamical-systems']"
4617103,What is the universal property of the thickening $Y[\varepsilon]$?,"Given an $S$ -scheme $Y$ , let $Y[\varepsilon]$ denote thickening $Y[\varepsilon]=Y \times_S D_S$ of $Y$ . Here $D_S$ is the $S$ -scheme $D\times_\mathbb Z S\to S$ where $D = \operatorname{Spec} \mathbb{Z}[\varepsilon]/(\varepsilon^2)$ . The $S$ -scheme $Y[\varepsilon]$ should have a mapping out universal property in $\mathrm{Sch}_{/S}$ , which is probably related to derivations. Unfortunately I cannot find a good reference. A morphism $f \in \mathrm{Hom}_S(Y[\varepsilon],X)$ is the same thing as a map $f \in \mathrm{Hom}_S(Y,X)$ plus what data? Help is appreciated. Context: I am trying to understand the proof of Lemma 5.12.1 in Martin Brandenburg’s PhD thesis .","['algebraic-geometry', 'tangent-bundle', 'category-theory', 'quasicoherent-sheaves']"
4617106,A function with positive $n$-th derivative has at most $n$ roots – an inequality version of the Fundamental theorem of Algebra.,"Claim: Let $n\in \mathbb N$ , and let $f:\mathbb R \to \mathbb R$ be such that its $n$ -th derivative $f^{(n)}(x)>0, \ \forall x\in \mathbb R$ , then $f$ has at most $n$ roots. Context: The fundamental theorem of algebra states (when only the real numbers are concerned) that an $n$ -th degree polynomial has at most $n$ real roots. Since an $n$ -th degree polynomial can be characterized as a function $f:\mathbb R \to \mathbb R$ with constant $n$ -th derivative, the fundamental theorem of algebra can be equivalently stated as: Let $n\in \mathbb N$ , and let $f:\mathbb R \to \mathbb R$ has constant $n$ -th derivative, then $f$ has at most $n$ roots. Analogy: Thus the claim I'm questioning can be interpreted as a version of the fundamental theorem of algebra with the condition that the function's $n$ -th derivative is constant replaced by the condition that the function's $n$ -th derivative is strictly positive. Special cases: for $n=1$ the claim follows from the fact that strictly increasing functions have at most one root; likewise for $n=2$ the claim follows from the fact that strictly increasing functions have at most two roots (strictly convex function is strictly negative between any two roots, thus there can not be a third root). Does the claim hold for $n\geq 3$ as well?","['roots', 'real-analysis', 'rolles-theorem', 'real-algebraic-geometry', 'derivatives']"
4617153,All the possible tuples from a starting set satisfying specific conditions,"This should not be a super hard problem, but I have been headbutting against it for two days. Let's suppose a set $A = [a_1,\dots,a_n]$ , an element $a_j \in A$ , and a subset of fixed element $A_l \subset A$ . I am interested in counting all the possible tuples of $A$ with each element appearing at most once (written as $C(A)$ ) such that $a_j$ appears before any element in $A_l$ . For example, if I take $A = [x,y,z]$ , $A_l = [y]$ and $a_j = x$ , then the subsets satisfying the condition are: $[x]$ , $[x,y]$ , $[x,z]$ , $[z,x]$ , $[x,y,z]$ , $[z,x,y]$ , $[x,z,y]$ and $C(A) = 7$ . Bonus point if I can find the number of satisfactory cases as a function of the cardinality of the tuples (i.e. $C(A_1) = 1, C(A_2) = 3, C(A_3) = 3$ and $\sum_{i = 1}^{|A|} C(A_i) = C(A)$ ). Thanks for the help!","['elementary-set-theory', 'combinatorics', 'permutations']"
4617196,Optimal bounds for the product of the divisor function $d(n)$ in short intervals,"Let $d(n)$ denote the number of divisors of a positive integer $n$ .
It is pretty obvious that $d(n) \ge 2$ for any given number $n \ge 2$ , since every number is divisible by $1$ and itself. $2$ is also the best lower bound since there is an infinite amount of prime numbers. One can also quite easily see that $d(2n)d(2n+1) \ge 8$ for any number $n \ge 3$ since $2n$ is divisible by $1$ , $2$ , $n$ and $2n$ , so $d(2n) \ge 4$ , while $d(2n+1) \ge 2$ . To my knowledge, it is not known whether that bound is optimal or not. To generalize that result, let $s(m)$ denote the optimal bound for $\displaystyle\prod_{k=0}^{m-1} d(mn+k)$ for $n$ big enough, meaning $$s(m)=\liminf_{n \to \infty} \prod_{k=0}^{m-1} d(mn+k)$$ I'm now interested in the growth rate of $s(m)$ , especially whether or not $\displaystyle \max_m(s(m)^\frac1m) = \infty$ It is clear that $s(m) \ge 2^m$ , so it grows at least exponentially. Are there any better lower bounds? Edit: Does it make a difference whether one starts at $mn+0$ and ends at $mn+m-1$ , or multiplies over any arithmetic progression of length $m$ for big enough $n$ ? If it does, how does the lower bound $s(m)$ change?","['number-theory', 'limsup-and-liminf', 'analytic-number-theory', 'divisor-counting-function', 'prime-numbers']"
4617212,"If $\partial_2 f\equiv 0$ on domain, then $f$ is independent of that variable?","Let $z=f(x,y)$ be a function of class $C^1(G;\mathbb{R})$ . a) If $\frac{\partial f}{\partial y}(x,y)\equiv 0$ in $G$ , can one
assert that $f$ is independent of $y$ in $G$ ? b) Under what condition on the domain $G$ does the preceding question
have an affirmative answer? Sorry if this question has been asked many times but my question is a bit different and it is about part b). But I did not find my question in existing topics. Part a) The answer is NO because one can consider the following function $f:G\to \mathbb{R}$ , where $G=\{(x,y)\in \mathbb{R}^2: 1<x^2+y^2<4,\ x>0\}$ and $$f(x,y) =
\begin{cases}
0, & \text{if }(x,y)\in G, x\in [1,2), \\
(x-1)^2, & \text{if }(x,y)\in G,\ x\in (0,1),\ y>0, \\
-(x-1)^2, & \text{if }(x,y)\in G,\ x\in (0,1),\ y<0.
\end{cases}$$ It is easy to see that $f$ depends on $y$ since $f(\frac{1}{2},1)\neq f(\frac{1}{2},-1)$ and $\frac{\partial f}{\partial y}\equiv 0$ on $G$ . Part b) I can prove that if $G$ is convex open set in $\mathbb{R}^2$ then the answer to part a) is YES. But I believe that it is true for larger family of sets in $\mathbb{R}^2$ . More precisely, if $G\subset \mathbb{R}^2$ is an open nonempty set with connected first projections then the answer to part a) is still YES. Here by connected first projections I mean that if $x\in \pi_1(G)$ , then for any $y_1, y_2$ such that $z_1:=(x,y_1)\in G$ and $z_2:=(x,y_2)\in G$ the line segment $[z_1,z_2]:=\{(x,\theta y_1+(1-\theta)y_2):\theta\in [0,1]\}\subset G$ . This is a larger class of sets since every convex set has this property. The proof is relatively easy. Indeed, let $x_0\in \pi_1(G)$ , then $\exists y_0: (x_0,y_0)\in G$ . Let $y$ be such that $(x_0,y)\in G$ and WLOG $y_0<y$ . Consider a function $\varphi:[y_0,y]\to \mathbb{R}$ defined as $t\mapsto f(x_0,t)$ . One can check that $\varphi$ is continuous and differentiable on $[y_0,y]$ and by MVT, we have: $\varphi(y)-\varphi(y_0)=\varphi'(c)(y-y_0)=\frac{\partial f}{\partial y}(x_0,c)(y-y_0)=0.$ Therefore, $\varphi(y)=\varphi(y_0)$ which is equivalent to $f(x_0,y)=f(x_0,y_0)$ . We have shown that for any $(x,y)\in G$ , we have $f(x,y)=f(x,F(x))$ , where $F:\pi_1(G)\to \mathbb{R}$ which is defined as: $x_0\mapsto y_0$ , where $(x_0,y_0)\in G$ . For example, this function $F$ can be constructed by Axiom of Choice. We are done since we have shown that $f(x,y)$ is independent of $y$ . So far I do not see any mistake. Thank you!","['smooth-functions', 'real-analysis', 'multivariable-calculus', 'partial-derivative', 'derivatives']"
4617218,Why do the Gaussian rationals make these patterns?,"I was looking for a cool thing to visualize, when I found this picture on Wolfram MathWorld. I obtained the image below by taking all integers $a, b, c, d$ in a given range (between $-15$ and $15$ here). I then calculated $\frac{a+bi}{c+di}$ and plotted it to give this image. If I'm correct, these complex irreducible rationals are called the ""Gaussian rationals"". One thing that immediately pops out here are the empty regions with flower-like shapes. It kind of reminds me of the Farey sequence , and it seems that the gaps here line up well with the gaps in the Farey sequences. There are gaps at $i$ , $-i$ , $1$ , and $-1$ , and smaller gaps at $\frac{1}{2}, \frac{i}{2}$ , etc. My question is - why? Why are there gaps at regular intervals in this picture, and why are there gaps in the Farey series?","['farey-sequences', 'number-theory', 'recreational-mathematics', 'gaussian-integers']"
4617227,A function with weakly positive $n$-th derivative has at most $n$ roots – an inequality version of the Fundamental theorem of Algebra.,"This is an attempt to generalize the result in [1] . Claim: Let $n\in \mathbb N$ , and let $f:\mathbb R \to \mathbb R$ be such that its $n$ -th derivative $f^{(n)}(x)\geq 0, \ \forall x\in \mathbb R$ . Then the set $R$ of roots of $f$ either consists of at most of $n$ isolated points, or it is a (non-degenerate) closed interval. Note: As argued in [1] , under the stronger assumption $f^{(n)}(x)> 0$ the case of $R$ being a closed interval can be excluded. The analogy of this claim to the fundamental theorem of algebra (restricted to real numbers) is shown in [1] . Special cases: for $n=1$ the claim states that if a function has nonnegative derivative, it has a single root or a closed interval of roots, e.g. consider the function $f(x) = \min\{x+1,0\} + \max\{x-1,0\}$ for which $R = [-1,1]$ ; for $n=2$ the condition $f^{(n)}(x)\geq 0$ implies that $f$ is (weakly) convex, and example could be the function $f(x)=x^2-1$ which has two roots, or $$
  f(x) = \begin{cases}
0, &\text{ if } x\in [-1,1] \\
(|x|-1)^2, &\text{otherwise},
\end{cases}
$$ in which case $R= [-1,1]$ . The reason why I think that $[a,b]\subset R$ for some $a<b$ implies that $f$ has no isolated roots is that this implies that all the derivatives of $f$ are zero on $(a,b)$ , and so (assuming that the interval $[a,b]$ is maximal among the closed intervals contained in $R$ ) the derivatives $f^{(n-1)}(x),f^{(n-2)}(x),f'(x),f(x)$ are all positive for $x>b$ as $f^{(n)}(x)\geq 0$ for $x>b$ with inequality strict at $x=b+\varepsilon$ for $\varepsilon>0$ arbitrarily small. Does the claim hold?","['roots', 'real-analysis', 'rolles-theorem', 'real-algebraic-geometry', 'derivatives']"
4617230,Electric Field by integral method is not the same as for Gauss's Law,"I'm trying to calculate the Electric Field over a thick spherical sphere with charge density $\rho = \frac{k}{r^2}$ for $a < r < b$ , where $a$ is the radius of the inner surface and $b$ the radius of the outer surface. From Gauss's Law I've got it to be $\vec{E} = \frac{k}{\epsilon_0}\frac{r-a}{r^2}$ . But also tried to integrate the charge over the surface and I was hoping to get the same result but didn't. My attempt was, in spherical coordinates: $$
\begin{equation}
\begin{aligned}
\vec{E} &= \frac{1}{4 \pi \epsilon_0}\int_a^r \int_0^\pi \int_0^{2\pi} \frac{\rho r^2 \sin \phi}{r^2} d\theta d\phi dr \vec{e_r}\\ &= \frac{1}{4 \pi \epsilon_0}\int_a^r \int_0^\pi \int_0^{2\pi} \frac{k \sin \phi}{r^2} d\theta d\phi dr \vec{e_r}\\ \\ &= -\frac{k}{\epsilon_0}(\frac{1}{r} - \frac{1}{a})\\ &= \frac{k}{\epsilon}\frac{r-a}{ra}\vec{e_r}
\end{aligned}
\end{equation}
$$ Which are not the same. But they should be! I know that my Gauss method gave me the right expression, but I can't figure out why the integral method didn't.","['integration', 'vector-fields', 'electromagnetism', 'multivariable-calculus', 'vector-analysis']"
4617285,Existence of minimizing geodesic,"I have read in a post in Terence Tao's blog that in every connected smooth manifold, any two points can be joined by $C^1$ -piecewise minimising geodesic, but I am not so sure that this result is true in not necessarily complete manifolds, because otherwise we wouldn't need the completeness condition in the theorem of Hopf-Rinow. The classic counter-example is $\mathbb{R}^2\setminus\{0\}$ in which any point and its central inversion can't be joined by a minimising geodesic. So if anybody can confirm or invalidate Tao's statement, it would be appreciated.","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4617297,Which fields are closed with respect to roots of polynomial maps but not algebraically closed?,"Before yesterday, I had heard of basically two examples of algebraically closed fields, the algebraic numbers $\overline{\mathbb{Q}}$ and $\mathbb{C}$ . I tried to come up with more examples and started thinking about $\mathbb{F}_2$ , the field with two elements. All the polynomial maps on $\mathbb{F}_2$ that are non-constant have a root because the domain consists of just $0$ and $1$ and thus any function without a root is constrained to be constantly 1. However, there are indeed polynomials in $\mathbb{F}_2[x]$ that are nonconstant and lack roots, such as $x^2 + x + 1$ . This question shows the construction for the algebraic closure of a finite field of prime order. Let's call the weaker property closure with respect to roots of polynomial maps . I'm wondering which fields are closed with respect to roots of polynomial maps but not algebraically closed besides $\mathbb{F}_2$ , if there are any . $\mathbb{F}_3$ does not have this property because $x^2 + 1 \mapsto \{0+1, 1+1, 1+1\} = \{1, 2\}$ has no roots. For the other finite fields of odd prime order, I think I can use the polynomial $x^{p - 1} + 1$ to witness the failure of the weaker property since the multiplicative group of $\mathbb{F}_p$ has order $p-1$ and this function is nonconstant since $p \ge 3$ . I think my question is distinct from this question about the distinction between polynomials and polynomial maps because I'm asking about consequences of using the ""wrong"" notion specifically when characterizing the algebraically closed fields, not what the difference is per se.","['field-theory', 'abstract-algebra']"
4617328,"When do the group-homomorphism ""separate points""?","Today, I asked myself the following question: Suppose I have some group $G$ and I look at the homomorphism $\mathrm{Hom}(G, \mathbf{Q})$ (or the first cohomology group, if you want to call it that). Let $g, h \in G$ . What are the conditions on $g, h$ so that $\phi(g) = \phi(h)$ for all $\phi \in \mathrm{Hom}(G, \mathbf{Q})$ ? A suffcient condition that comes to my mind would be that $g$ and $h$ lie in the same conjugacy class. However, I'm wondering if there is any characterisation of this property. So: What are the sufficient and necessary conditions on $g, h \in G$ such that $\phi(g) = \phi(h)$ for all $\phi \in \mathrm{Hom}(G, \mathbf{Q})$ ? I really have no clue what the right buzzword is, so I called the property ""separating points"" as in functional analysis. If anybody knows the right terminology, let me know! Also, I guess an equivalent formulation of my question would be: How are the elements in $\bigcap_{\phi \in \mathrm{Hom}(G, \mathbf{Q})} \mathrm{ker}(\phi)$ characterized?","['group-theory', 'abstract-algebra', 'group-cohomology']"
4617371,Integrate $\ddot{u}(x)=(a+bx)u(x)$,"I wish to integrate the following differential equation $$\ddot{u}(x)=(a+bx)u(x)$$ to solve for $u(x)$ . I tried to characterize it as a Sturm-Liouville problem and introduce an integrating factor, but I was unable to make much headway. I would like to try integrating it without appealing to a power series and recurrence relationship. (I was already capable of producing a power series solution, but it did not elucidate me.) If the differential equation was first order, I would have immediately used separability. I also tried the ansatz $u(x) = e^{\pm \sqrt a x} w(x)$ which produced the equation $$
\ddot w(x) + 2\sqrt {a} \dot w(x) - b x w(x) = 0
$$","['sturm-liouville', 'ordinary-differential-equations']"
4617393,AM-GM & Minimization Proof [duplicate],"This question already has answers here : Proofs of AM-GM inequality (27 answers) Closed last year . I want to prove that for all $x, y > 0$ , $$\cfrac{x+y}{2} \geq \sqrt{xy}$$ Particularly, I want to show that the minimum of $(x+y)/2$ is exactly $\sqrt{xy}$ . This is my attempt: $\textbf{Proof}$ (Contradiction). Assume if $x, y > 0$ , then $(x+y)/2 < \sqrt{xy}$ . But \begin{align*}
x+y &< 2\sqrt{xy} \\
x^2+2xy+y^2 &< 4xy \\
x^2-2xy+y^2 &< 0 \\
(x-y)^2 &< 0
\end{align*} gives us a contradiction since for all $x, y > 0$ , we know $(x-y)^2 > 0$ . Therefore there exists no positive reals $x$ and $y$ such that $(x+y)/2 < \sqrt{xy}$ , or $\min\bigg(\cfrac{x+y}{2}\bigg) = \sqrt{xy}$ . $\qquad \square$","['algebra-precalculus', 'solution-verification', 'a.m.-g.m.-inequality']"
4617396,Making dense subset of continuous functions using a single continuous function,"I have a question about set of continuous function on the compact interval. Denote the set of all continuous $n$ -dimensional real functions on $[0,L]$ as $\mathcal{C}_{[0,L]}$ ( $\mathcal{C}_{[0,L]} = \left\{ u: [0,L] \rightarrow \mathbb{R}^n\right\}$ ) Is there any $T>L$ and a continuous function $f: [0,T] \rightarrow \mathbb{R}^n$ such that finite combination of $L$ -length segments of $f$ is dense in $\mathcal{C}_{[0,L]}$ with respect to supremum norm? I mean if we define $f_{[t,t+L]}:[0,L] \rightarrow \mathbb{R}^n$ as $f_{[t,t+L]}(\cdot) = f(\cdot + t)$ for $t \in [0,T-L]$ , is it possible to find continuous function $f: [0,T] \rightarrow \mathbb{R}^n$ such that any finite linear combination of $f_{[t, t+L]}$ , is dense subset of $\mathcal{C}_{[0,L]}$ with respect to sup-norm. If not, is there any space of functions that satisfies a similar property?
(e.g., rather $L^2$ space satisfies such a condition etc.) Thank you for your attention.","['dense-subspaces', 'functional-analysis']"
4617409,Proving a basic form of monotone convergence,"Problem: Definition 4.4 of Probability Theory: A Comprehensive Course by Klenke says that If $f \colon \varOmega \to [0, \infty]$ is measurable, then we define the integral of $f$ with respect to $\mu$ by \begin{equation}
\int f \, d\mu := \sup \bigl\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \bigr\}
\end{equation} where $\mathbb{E}^+$ is the space of non-negative simple functions and $I \colon \mathbb{E}^+ \to [0, \infty]$ is such that $I(f) = \sum_{i = 1}^m \alpha_i \, \mu(A_i)$ . I know that for $f \in \mathbb{E}^+$ , there is a sequence of non-negative simple functions $(f_n)_{n \in \mathbb N}$ such that $f_n \uparrow f$ . I would like to convince myself that \begin{equation}
\lim_{n \to \infty} \int f_n \, d\mu = \int f \, d\mu
\end{equation} so that I can think of $\int f \, d\mu$ as the limit of the integrals of non-negative simple functions that approximate $f$ . My attempted proof that $\lim_{n \to \infty} \int f_n \, d\mu = \int f \, d\mu$ is shown below. Attempted Proof: I know that $f \leq g$ implies that $I(f) \leq I(g)$ . This gives us a monotone increasing sequence $\left(I(f_n)\right)_{n \in \mathbb N}$ of real numbers. I know that \begin{equation}
    \lim_{n \to \infty} I(f_n) = \sup \left\{ I(f_n) \colon n \in \mathbb N \right\}
\end{equation} i.e. the sequence converges to its supremum (which may be infinite). It remains to show that $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} = \int f \, d\mu$ . By definition, $\int f \, d\mu := \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ . If $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} \neq \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ , then because $\left\{ I(f_n) \colon n \in \mathbb N \right\} \subset \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ , we must have $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ . Suppose this is true. Then there exists $g \in \mathbb{E}^+$ , $g \leq f$ such that $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < I(g)$ . However, because $f_n \uparrow f$ , we know that there exists an $n \in \mathbb N$ such that $g \leq f_n$ . This implies that $I(g) \leq I(f_n)$ , which implies that $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < I(f_n)$ , which is clearly a contradiction.","['measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis', 'solution-verification']"
4617513,"If $(X_n)_{n\ge 1}$ is independent, then $\mathcal F_n$ is independent of $\mathcal G_{n+1}$","Let $(X_n)_{n\ge 1}$ be a sequence of random variables defined on a probability space $(\Omega, \mathcal F, \mathbb P)$ . For $n \in \mathbb N$ , let $$
\mathcal F_n := \sigma(X_1, \ldots, X_n)
\quad \text{and} \quad
\mathcal G_{n+1} := \sigma(X_{n+1}, X_{n+2}, \ldots).
$$ In proving Kolmogorov's zero–one law, my lecture note uses the following result Theorem If $(X_n)_{n\ge 1}$ is independent, then $\mathcal F_n$ is independent of $\mathcal G_{n+1}$ . Could you have a check on my below attempt? Proof Let $\mathcal A := \sigma(X_1) \cup \cdots \cup \sigma(X_n)$ and $\mathcal B := \sigma(X_{n+1})  \cup \sigma(X_{n+2}) \cup \cdots$ . By definition, $\mathcal F_n := \sigma(\mathcal A)$ and $\mathcal G_{n+1} := \sigma (\mathcal B)$ . Let $\lambda(\mathcal A)$ be the smallest $\lambda$ -system that contains $\mathcal A$ . We define $\lambda(\mathcal B)$ similarly. Let's prove that $\lambda(\mathcal A)$ is independent of $\lambda(\mathcal B)$ . Fix $A \in \lambda(\mathcal A)$ and $B \in \lambda(\mathcal B)$ . Then there are $A_i \in \sigma(X_i)$ for $i=1, \ldots, n$ such that $A=\bigcap_{i=1}^n A_i$ . There is $N \in \mathbb N$ and $A_i \in \sigma(X_i)$ for $i=n+1, \ldots, N$ such that $B=\bigcap_{i=n+1}^N A_i$ . Because $(X_n)$ is independent, $$
\begin{align}
\mathbb P[A \cap B] &= \mathbb P \left [ \bigcap_{i=1}^N A_i \right ] \\
&= \prod_{i=1}^N \mathbb P[A_i] \\
&= \left (\prod_{i=1}^n \mathbb P[A_i] \right ) \left (\prod_{i=n+1}^N \mathbb P[A_i] \right ) \\
&= \mathbb P \left [ \bigcap_{i=1}^n A_i \right ] \mathbb P \left [ \bigcap_{i=n+1}^N A_i \right ] \\
&= \mathbb P[A] \mathbb P[B].
\end{align}
$$ Finally, independent $\pi$ -systems generate independent $\sigma$ -algebras. This completes the proof.","['independence', 'probability-theory']"
4617556,How is $U$ measurable in this construction involved Taylor's expansion?,"I'm reading below lemma in this note. The result is used in the proof of central limit theorem. Lemma 9.11. Let $g \in C_b^3 (\mathbb R)$ and $X, Y, Z$ be real-valued random variables such that $X$ is independent of both $Y$ and $Z, \mathbb{E}(Y)=\mathbb{E}(Z), \mathbb{E}\left(Y^2\right)=\mathbb{E}\left(Z^2\right)$ and $\mathbb{E}\left(|Y|^3\right)<+\infty, \mathbb{E}\left(|Z|^3\right)<+\infty$ . Then $$
|\mathbb{E}(g(X+Y))-\mathbb{E}(g(X+Z))| \leq \frac{C}{6}\left(\mathbb{E}\left(|Y|^3\right)+\mathbb{E}\left(|Z|^3\right)\right)
$$ where $C=\sup _{x \in \mathbb{R}}\left|g^{\prime \prime \prime}(x)\right|$ . What this lemma is essentially saying is that provided both $Y$ and $Z$ are ""small"" random variables, one may trade $Y$ for $Z$ in the expression $\mathbb{E}(g(X+Y))$ without changing much the value of the expectation. Proof. By Taylor's expansion, we obtain for real numbers $x, y$ : $$
g(x+y)=g(x)+y g^{\prime}(x)+\frac{y^2}{2} g^{\prime \prime}(x)+\frac{y^3}{6} g^{\prime \prime \prime}(u)
$$ for some $u$ such that $|u-x| \le |y|$ . The independence of $X$ and $Y$ then implies that $$
\mathbb{E}(g(X+Y))=\mathbb{E}(g(X))+\mathbb{E}(Y) \mathbb{E}\left(g^{\prime}(X)\right)+\frac{1}{2} \mathbb{E}\left(Y^2\right) \mathbb{E}\left(g^{\prime \prime}(X)\right)+\frac{1}{6} \mathbb{E}\left(Y^3 g^{\prime \prime \prime}(U)\right)
$$ where $U$ is a random variable satisfying $|U-X| \leq|Y|$ . Similarly, one may write $$
\mathbb{E}(g(X+Z))=\mathbb{E}(g(X))+\mathbb{E}(Z) \mathbb{E}\left(g^{\prime}(X)\right)+\frac{1}{2} \mathbb{E}\left(Z^2\right) \mathbb{E}\left(g^{\prime \prime}(X)\right)+\frac{1}{6} \mathbb{E}\left(Z^3 g^{\prime \prime \prime}(V)\right)
$$ where $V$ is a random variable satisfying $|V-X| \leq|Z|$ . By the assumptions made, we obtain $$
\mathbb{E}(g(X+Y))-\mathbb{E}(g(X+Z))=\frac{1}{6}\left(\mathbb{E}\left(Y^3 g^{\prime \prime \prime}(U)\right)-\mathbb{E}\left(Z^3 g^{\prime \prime \prime}(V)\right)\right)
$$ So $$
|\mathbb{E}(g(X+Y))-\mathbb{E}(g(X+Z))| \leq \frac{C}{6}\left(\mathbb{E}\left(|Y|^3\right)+\mathbb{E}\left(|Z|^3\right)\right)
$$ which completes the proof. My understanding For each $x,y \in \mathbb R$ , there is $u \in \mathbb R$ such that $|u-x| \le |y|$ and that $$
g(x+y)=g(x)+y g^{\prime}(x)+\frac{y^2}{2} g^{\prime \prime}(x)+\frac{y^3}{6} g^{\prime \prime \prime}(u).
$$ Assume our probability space is $(\Omega, \mathcal F, \mathbb P)$ . Then for each $\omega \in \Omega$ , we can define $U(\omega) := u$ where $u$ satisfies above conditions for $x := X(\omega)$ and $y := Y(\omega)$ . Could you explain how $U$ is a random variable, i.e., it is measurable?","['measure-theory', 'proof-explanation', 'real-analysis', 'taylor-expansion', 'probability-theory']"
4617594,Center is origin for linearized system,"I have the following system \begin{equation}
  \begin{aligned}
\dot{x} & {}={} 1 + x^2y - 3x = f(x,y) \\
\dot{y} & {}={}2x - yx^2 = g(x,y)
  \end{aligned} \tag{1}
\end{equation} with one equilibrium point $A(1,2)$ . After linearizing around $A$ , I obtain the system \begin{equation}
  \begin{aligned}
\dot{x} & = x + y \\
\dot{y} & = -2x - y
  \end{aligned} \tag{2}
\end{equation} The eigenvalues of the matrix $J$ of system (2) are $λ_{1,2} = \pm \mathrm{i}$ . Because the origin is center for system (2) we can't apply Hartman-Grobamn Theorem. But can I do this instead? Let us consider $F=(f,g)$ . Then $\mathrm{div}F = 2xy - 3 - x^2 < 0$ around $A(1,2)$ if we consider a region $S = \{ (x,y) \in \mathbb{R}^2: \frac{1}{2} \leq x \leq \frac{3}{2} \ \mathrm{and} \ \frac{3}{2} \leq y \leq \frac{5}{2} \}$ . Then, the equilibrium point $A$ will be an attractor. Thus, $A$ is asympotically stable focus. Is this right or wrong? Do we know that if the origin is center for the linearized system then the equilibrium point for the nonlinear system will be either a center or a focus? Thanks for any help / guidance.","['nonlinear-system', 'linearization', 'ordinary-differential-equations', 'dynamical-systems']"
4617609,Number of possible combinations of positive integer polynomial entries with restrictions.,"This question might be stupid to someone being an expert in this field. But I was not able to find a satisfying answer in the textbooks I have studied so far. My question is: how many possible different solutions are there to the following inequality: $\sum_{i=1}^{m} \log_2(a_i)n_i \leq \frac{\ln(w)}{ln(2)}$ where $a_i$ is an integer sequence which is given, $w$ is a choosen integer and there for given too, if $n_i$ are the free-variables with a positive integere value? Or maybe better: How many combinations for $n_i$ are there with given $a_i, w$ such that the inequality holds true? I hope this is understandable but I have myself some issues with properly stating the question. So if someone even just is able to reformulate the question a bit more understandable this might help too for others to partially or fully answer it. Edit: We have some restrictions to $a_i$ which might simplify the task a bit. First $a_1=1$ and all other $a_i>1$ . Also none of the $a_i$ takes a positive integere value. In fact they are all irrational.","['combinations', 'combinatorics', 'polynomials', 'discrete-mathematics', 'probability']"
4617633,Convergent sequence of real numbers inside spectrum of operator,"I'm trying to solve the following. Let $T$ be a compact and self-adjoint operator on a prehilbertian space.
Determine wether the spectrum of T contains a sequence of real numbers that converge to zero. Some context: By a prehilbertian space, we're referring to a vector space over a field $\mathbb{K}$ , generally $\mathbb{R}$ or $\mathbb{C}$ , that has an inner product. Every inner product naturally induces a norm so every prehilbertian space is also a normed vector space. The resolvent of an operator $T$ is defined as $\rho(T) = \{\lambda\in\mathbb{C}:(\lambda Id-T) \ \text{has an inverse}\}$ and therefore the spectrum is $\sigma(T) = \mathbb{C}-\rho(T)$ . Finally, an operator $T:X\longrightarrow Y$ between normed spaces is compact if $T(B_X)$ is a relatively compact set in $Y$ (i.e. its closure is compact in $Y$ ), where $B_X$ is the unit ball in $X$ . My attempt at the proof I'm having issues understanding what the restriction of not being a Hilbert space implies. Is the completeness of the space necessary to prove the claim? I mean, if we were on a Hilbert space, then by the spectral theorem there exists a Hilbert base and sequence of real (since $T$ is self-adjoint) numbers such that $Te_k = \lambda_ke_k$ , with $\lambda_k\to 0$ as $k\to\infty$ , therefore those real numbers are indeed eigenvalues, which are obviusly a subset of the spectrum. But what happens if it's not a Hilbert space? As far as I've been told, the result is true, but I don't know what results to use to come up with the answer.","['hilbert-spaces', 'operator-theory', 'spectral-theory', 'functional-analysis']"
4617636,A system of three `eigenequations' (sort of),"In my research I have stumbled upon the following problem: Find all pairwise orthonormal triples of vectors $\mathbf{x},\mathbf{y},\mathbf{z} \in \mathbb{C}^n$ ( $n \geq 4$ ) satisfying the three `eigenequations' $(a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{x}=(c-b)bc \,\mathbf{x} \\ (a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{y}=(c-a)ca \,\mathbf{y} \\ (a\mathbf{A}+b\mathbf{B}-c\mathbf{C})\mathbf{z}=(a+b)ab \,\mathbf{z}$ where - and this is crucial - the $n\times n$ matrices $\mathbf{A},\mathbf{B},\mathbf{C}$ themselves depend on $\mathbf{x},\mathbf{y},\mathbf{z}$ , respectively, being defined as $A_{ij} = \delta_{ij}\sum_k D_{kj}|x_k|^2 - x_i D_{ij} \bar{x}_j \\ B_{ij} = \delta_{ij}\sum_k D_{kj}|y_k|^2 - y_i D_{ij} \bar{y}_j \\ C_{ij} = \delta_{ij}\sum_k D_{kj}|z_k|^2 - z_i D_{ij} \bar{z_j}$ where $\delta_{ij}$ is the Kronecker symbol and $D_{ij}$ ( $i,j =1,\ldots,n$ ) are certain given coefficients satisfying $D_{ii} = 0$ , $D_{ij} = D_{ji} > 0$ for $i \neq j$ . Notice that the matrices are positive semi-definite and satisfy $\mathbf{A} \mathbf{x} = \mathbf{B} \mathbf{y} = \mathbf{C} \mathbf{z} = 0$ (I think their rank is actually $n-1$ ) as well as $\mathbf{y}^+\mathbf{C}\mathbf{y} = \mathbf{z}^+\mathbf{B}\mathbf{z}, \quad \mathbf{z}^+\mathbf{A}\mathbf{z} = \mathbf{x}^+\mathbf{C}\mathbf{x}, \quad \mathbf{x}^+\mathbf{B}\mathbf{x} = \mathbf{y}^+\mathbf{A}\mathbf{y}$ . Using orthogonality, one can easily show from the equations that all the `mixed terms' vanish, i.e. $\mathbf{y}^+\mathbf{A}\mathbf{z} = 0$ and so on. As for the scalars $a,b,c$ , they are assumed positive and can be shown to be equal to, $a = \sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}} = \sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}, \quad b = \sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}} = \sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}, \quad c = \sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}} = \sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}$ . I've been able to notice that any triple of distinct canonical basis vectors (with arbitrary phase factors) solves the above problem. In other words, taking $x_j = e^{i\phi_1}\delta_{i_1 j}, \quad y_j = e^{i\phi_2}\delta_{i_2 j}, \quad z_j = e^{i\phi_3}\delta_{i_3 j}$ for any distinct $i_1,i_2,i_3 \in \{1,\ldots,n\}$ and any angles $\phi_1,\phi_2,\phi_3$ does the trick. Sometimes there exist other solutions, e.g. if $D_{ij} = 1 - \delta_{ij}$ , then any orthonormal triple $\mathbf{x},\mathbf{y},\mathbf{z}$ will do. And so my question is: Assuming all $D_{ij}$ 's are distinct (other than $D_{ii} = 0$ and $D_{ij} = D_{ji}$ , that is), is it possible to show that the solutions mentioned above are the only ones? Let me add that with all of the above in mind, one can rewrite the equations in a quite nice form $(\frac{\mathbf{B}}{\sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}}}-\frac{\mathbf{C}}{\sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}})\mathbf{x}=(\sqrt{\mathbf{x}^+\mathbf{B}\mathbf{x}}-\sqrt{\mathbf{x}^+\mathbf{C}\mathbf{x}}) \mathbf{x} \\ (\frac{\mathbf{A}}{\sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}}-\frac{\mathbf{C}}{\sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}}})\mathbf{y}=(\sqrt{\mathbf{y}^+\mathbf{A}\mathbf{y}}-\sqrt{\mathbf{y}^+\mathbf{C}\mathbf{y}}) \mathbf{y} \\ (\frac{\mathbf{A}}{\sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}}}+\frac{\mathbf{B}}{\sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}})\mathbf{z}=(\sqrt{\mathbf{z}^+\mathbf{A}\mathbf{z}}+\sqrt{\mathbf{z}^+\mathbf{B}\mathbf{z}}) \mathbf{z}$ But I've not been able to move much further. I tried playing for a while with determinants, but came back empty-handed. Any hint would be much appreciated, even for the real case. EDIT: I've noticed that the original system of three `eigenequations' actually concerns one and the same matrix $a\mathbf{A}+b\mathbf{B}-c\mathbf{C}$ and have now included this fact in the question. Also some other minor errors are now corrected.","['orthonormal', 'vectors', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra']"
4617652,"Show that $x_i\leq \max{x_0,x_n,0}$, discrete maximum principle","Let $x=(x_0,...,x_n)\in \mathbb R^{n+1}$ that satisfies $a_ix_{i-1}+b_ix_i+c_ix_{i+1}<0$ with coefficients $a_i,b_i,c_i\in\mathbb R$ with $a_i,c_i<0, b_i>0, a_i+b_i+c_i\geq 0$ for $1\leq i< n$ . Show that $x_i\leq \max\{x_0,x_n,0\}$ for all $i\in\{0,...,n\}$ . I tried to prove it by contradiction. This means there exists $i\in\{1,...,n-1\}$ so that $x_i\geq \max\{x_{i-1},x_{i+1},0\}$ and $x_i\geq \min\{x_{i-1},x_{i+1}\}$ . Can someone give me a hint?","['ordinary-differential-equations', 'maxima-minima', 'maximum-principle', 'discrete-mathematics', 'partial-differential-equations']"
4617654,Generalization of Commutativity For Unions of Sets,"In Paul Halmos' book on naïve set theory (or axiomatic set theory in a naive point of view) in the chapter about families is provided a generalization of associativity for unions of sets: $\bigcup _{i\in I_j}A_i = \bigcup_{j\in J}(\bigcup_{i\in I_j}A_i)$ such that $\{I_j\}$ is a family indexed by the index set $J$ , and $\{A_i\}$ is a family indexed by the index set $\{I_j\}$ As I believe I understood here, the number of paranthetical groups of unions of members of $\{A_i\}$ depends on the size of $\{I_j\}$ , and the number of elements in union within each paranthetical group depends on how many members are in the sets $I_j$ in the family $\{I_j\}$ . (It would be kind if someone could confirm whether my apprehension is correct) The book then challenges the reader to try and construct a generalization for the commutativity of unions of sets. The way I reasoned was as follows: commutativity refers to the fact that the union of two sets denotes the same result whether it's the first set to the second, or the second to the first; hence I could easily generalize commutativity over the union of two sets as: $(\bigcup_{i\in I_1}A_i)\bigcup$ $(\bigcup_{i\in I_2}A_i) = (\bigcup_{i\in I_2}A_i) \bigcup$ $(\bigcup_{i\in I_1}A_i)$ However, this is only generalized over two sets. And I have trouble expressing this most generally. It is requested that the reader help me complete the task so required. Thank you in advance.",['elementary-set-theory']
4617697,Integrate over geodesic ball on $n$-sphere,"Let $e_1=(1,0,\dots,0)\in S^{n-1}\subset\mathbb{R}^n$ . The geodesic ball of radius $\epsilon$ centered at $e_1$ is \begin{align*}
B_\epsilon(e_1) = \{\cos(t)e_1 + \sin(t)v : \lVert v\rVert=1, v_1=0, t\in[0,\epsilon)\}.
\end{align*} Let $f:S^{n-1}\to\mathbb{R}$ be smooth. Is the following true? \begin{align*}
\int_{B_\epsilon(e_1)}f(p)dp = \int_{S^{n-2}}\int_0^\epsilon f(\varphi(v,t))\sin^n(t)dtdv.
\end{align*} Here $dp$ is the volume form on $S^{n-1}$ and $dv$ is the volume form on $S^{n-2}\simeq \{v\in S^{n-1}\subset\mathbb{R}^n : v_1=0\}$ (in what follows I identify $v=(v_1,\dots,v_{n-1})\in S^{n-2}\subset\mathbb{R}^{n-1}$ with $v=(0,v_1,\dots,v_{n-1})\in T_{e_1}S$ , the space of unit tangent vectors to $e_1$ ). I obtained this by considering the diffeomorphism $\varphi:S^{n-2}\times [0,\epsilon)\to B_\epsilon(e_1)$ given by $(v,t)\mapsto \cos(t)e_1+\sin(t)v$ and applying change of variables (see for example Proposition 9.8 here ). At the risk of being overly naive, the derivative of $\varphi$ is \begin{align*}
D\varphi(v,t) = \begin{pmatrix} 0 & \cdots & 0 & -sin(t) \\ 
& \sin(t)I_{n-1} & & \vdots \end{pmatrix}
\end{align*} which has determinant $-\sin^n(t)$ , hence the factor above.","['integration', 'riemannian-geometry', 'differential-geometry']"
4617717,How to find the taylor series of $f(z)=\frac{1}{z^2+z+1}$,I'm trying to find the taylor expansion of the function $f(z)=\frac{1}{z^2+z+1}$ about $z_0=i$ and its radius of convergence. I'm not sure how to do it without differentiating.,"['complex-analysis', 'taylor-expansion']"
4617729,stability Sturm-Liouville equation,"Consider the solutions $u_1,u_2$ of the Sturm Liouville equations $$
\begin{array}{ll}
(p_1(x)u_1^\prime(x))^\prime+u_1(x)=f(x) && x\in (a,b)\\
u_1(x)=g(x) && x\in \{a,b\} \\
\end{array}$$ and $$\begin{array}{ll}
(p_2(x)u_2^\prime(x))^\prime+u_2(x)=f(x) && x\in (a,b)\\
u_2(x)=g(x) && x\in \{a,b\} \\
\end{array}$$ for $p_1,p_2\in C^1[a,b],f\in C[a,b], p_1(x),p_2(x)\geq p_0>0.$ Show that there exists $C=C(p_0,f,g,(a,b))>0$ so that $$\int_a^b |u_1(x)-u_2(x)|^2+|u^\prime_1(x)-u^\prime_2(x)|^2dx\leq C\|p_1-p_2\|^2_\infty$$ Hint: You can use (without a prove) that the solution $u$ of $$
\begin{array}{ll}
(p(x)u^\prime(x))^\prime+u(x)=0 && x\in (a,b)\\
u(x)=g(x)  && x\in \{a,b\} \\
\end{array}$$ satisfies $$\int_a^b |u(x)|^2+|u^\prime(x)|^2dx\leq C(g,p,(a,b))$$ I looked at $v(x)=u_1(x)-u_2(x)$ and tried to find a Sturm Liouville equation which is satisfied by $v$ to use the hint but this does not lead to the goal.","['ordinary-differential-equations', 'sturm-liouville', 'stability-in-odes', 'calculus', 'partial-differential-equations']"
4617738,"Doubt in the proof of Theorem 6.4 in Rudin, Functional Analysis, 1973","Let $X$ a vector space and $(Y, \tau)$ a topological vector space such that $Y \subset X$ . Consider $W \subset X$ such that $Y \cap W \in \tau$ .  If $f \in Y \cap W$ , I have to prove that there exists an $\delta > 0$ such that $f \in (1- \delta) W$ . Edit 1: W is a convex and balanced set, that is, $\lambda W \subset W$ , for all $|\lambda| \leq 1$ . Edit 2: Y is a vector subspace of $X$ . I don't know if this is true. This doubt appeared while I was studying the proof of Theorem 6.4 in Rudin, Functional Analysis, 1973. There, I think he takes $X = C^{\infty}_0(\Omega)$ , $(Y,\tau) = \mathfrak{D}_K$ and $f = \phi - \phi_i$ :","['topological-vector-spaces', 'topological-groups', 'analysis', 'functional-analysis', 'general-topology']"
4617749,Existence of a matrix satisfying a given constraint.,"Can we say that for any ordered pair $(A,Q)$ of matrices chosen from the set of all invertible square matrices of the same size, there exists a matrix $B$ such that $BAB=Q$ ? I understand that if only real matrices are allowed, then no such $B$ exists when $\det A$ and $\det Q$ have opposite signs. I want to know what can be said about the existence of such a matrix $B$ when $A$ and $Q$ are matrices with complex entries.","['matrices', 'determinant', 'linear-algebra']"
4617785,Bounding the Hilbert-Schmidt norm of a certain linear operator,"This is a follow up to the post A bound for the Hilbert-Schmidt norm of a linear operator without using commutivity or simultaneous diagonalizability Consider an infinite dimensional separable Hilbert space $\mathcal{H}$ , and let $A$ , $B_m$ , and $L$ denote linear, compact operators. Suppose further that $\|B_m\|_{op} \le \rho^m$ for some $0<\rho < 1$ , and $L$ is symmetric and positive definite, so that the spectral theorem gives $$
L(\cdot) = \sum_{\ell=1}^\infty \lambda_\ell \langle \phi_\ell,\cdot\rangle \phi_\ell, \;\; \sum_{\ell=1}^\infty \lambda_\ell < \infty.
$$ We define a pseudo-inverse of $L$ as $$
L^{-1}\pi_n(\cdot) = \sum_{\ell=1}^n \frac{ \langle \phi_\ell,\cdot\rangle}{\lambda_\ell} \phi_\ell.
$$ ( $\pi_n$ is the projection onto $span(\phi_1,...,\phi_n)$ ). We assume that $$
D = \sum_{\ell=1}^\infty \frac{ \|A(\phi_{\ell})\|^2}{\lambda_\ell} < \infty.
$$ What I want to then show is that $$
\xi_m =\|L^{1/2}B_mL^{-1}\pi_n A^* \|_{HS}^2 = \sum_{j=1}^\infty \|L^{1/2}B_mL^{-1}\pi_n A^*(\phi_j)\|^2 \le Const. \rho^m.
$$ It is relatively easy to show this if $B_m$ and $L^{1/2}$ commute, but in general it is not true. https://math.stackexchange.com/users/22016/stephen-montgomery-smith devised a very nice counter example to this. My question now is: is it possible to establish this bound if instead $B_m = R^m$ , where $R$ is a linear operator satisfying $\|R\|_{op}<1$ ? This seems more likely to hold now since, in reference to the counter example devised by @Stephen Montgomery-Smith, the range space for $B_m$ cannot be shifting around as before to meet spaces with larger eigenvalues. Any ideas by you smart folks are much appreciated!","['operator-theory', 'functional-analysis', 'real-analysis']"
4617852,Find the limit of $x^2 +xc + d$ when $x$ goes to $0^{-}$ using the $\varepsilon-\delta$ definition of a limit,"I want to show that $$
\lim\limits_{x\to 0^{-}} (x^2+cx+d) = d
$$ Here is my attempt: We want to show that $$
\forall\varepsilon>0,\exists\delta>0 : -\delta\leq x\leq 0\implies\left\lvert x^2+cx + d - d\right\rvert = \left\lvert x^2 +cx\right\rvert \leq\varepsilon \tag{1}\label{1}
$$ Fix an $\varepsilon>0$ and put $\delta =  \min\left(1,\frac{\varepsilon}{1+\left\lvert c\right\rvert}\right)$ . By the triangle inequality we have for $x\in[-\delta,0)$ : $$
\left\lvert x^2 +cx\right\rvert\leq \left\lvert x\right\rvert^2 +\left\lvert c\right\rvert\left\lvert x\right\rvert\leq \delta^2 +\left\lvert c\right\rvert\delta\leq\delta\left(1 + \left\lvert c\right\rvert\right) = \frac{\varepsilon(1+\lvert c\rvert)}{1+\lvert c\rvert} = \varepsilon \tag{2}\label{2}
$$ The minimum here ensures us that when $\varepsilon$ is ""big"" $($ namely $\varepsilon > 1+\left\lvert c\right\rvert)$ we still have $\eqref{2}$ which is satisfied. EDIT : Many thanks to Surb for the correction et Nerrit for improving the format of my question.","['analysis', 'continuity', 'calculus', 'solution-verification', 'limits']"
4617863,Conditional expectation Folland 3.2.17,"Let $(X,M,\mu)$ be a finite measure space and $N$ , a sub- $\sigma$ -algebra of $M$ . And $\nu=\mu|_N$ . If $f\in L^1(\mu)$ , there exists $ g\in L^1(\nu )$ , such that $\int_{E}fd\mu = \int_{E}gd\nu $ for all $E\in N$ . This problem is from Folland's Real Analysis (3.2.17). In order to show the equality above we define a new measure $\lambda(E)=\int_{E}fd\mu$ then apply Radon-Nikodym to restriction of it to $N$ . However, I wonder if we can compare $ \int_{}|g|d\nu$ and $ \int_{}|f|d\mu$ .Is it true that the former is less than the latter, if so how can we show this?","['measure-theory', 'radon-nikodym']"
4617936,A non-zero function on $\mathbb{R}^n$ whose integral over any ball of radius $1$ is zero?,"Does there exist a measurable function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ , for some $n>1$ , which is not equal to $0$ almost everywhere such that for any ball $B$ of radius exactly $1$ we have $$\int_Bfd\mu=0,$$ where $\mu$ is the Lebesgue measure on $\mathbb{R}^n$ ? Motivation: In my original thought, I substituted the shape of ball with another (fixed) convex shape and I let this integral rule hold for all these shapes and at all scales. I proved that in that case the only function is that which is equal to $0$ a.e. My goal was to restrict this integral rule to shapes of the scale equal to $1$ only, but then I figured that it is not enough to conclude $f=0$ a.e. if we took the hypercube to be our convex shape, for example. I was then wondering, does for any shape there exist a counter-example? That's my question reduced to the case when the shape is the ball. The solution for the case when $B$ 's are the cubes of side length $2$ : $$f(x)=-1+2\bigg(\Big(\sum_{k=1}^n\lfloor x_k\rfloor\Big)\;\text{mod}\;2\bigg),$$ where $\lfloor x_k\rfloor$ is the floor of the $k$ -th coordinate of $x$ .","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
4617966,Is it always possible to cut out a piece of the triangle with half the area?,"This is a sequel to my highly upvoted question (at the time of writing, my third-best post). Let there be an equilateral triangle that has $n+1$ notches on each edge (corners included) to divide each edge into $n$ equal parts. We can make cuts on the triangle from notch to notch. Is it always possible to cut out a connected piece with area $\frac{1}{2}$ the area of the original triangle if $n≥2?$ It is possible for multiples of $2$ or $3,$ but I don't know any other numbers for which this is possible. If it is possible for $n,$ it is possible for any multiple of $n.$ For simplicity, in my answers and comments (and in the solution for $3$ ), I'd use a transformed version of the coordinate system so that the vertices of the triangle are $(0,0), (n,0),$ and $(0,n).$ Since this is a linear transformation, it has constant determinant, so ratios of areas won't be affected. $2$ : Cut along a median. $3$ : Cut from $(0,1)$ to $(3,0)$ and $(0,3)$ to $(1,0),$ then take the piece with the diagonal side as one of its sides.","['dissection', 'area', 'geometry']"
4617974,Is this a thing? Visualizing complex functions with 3D animation,"I have read about various ways to visual complex functions, e.g. colored graphs, vector fields, conformal maps, etc. Here is another way: 3D animation. Picture a 3D plot with three axes for $\text{Re}(z), \text{Im}(z), \text{Re}(f(z))$ . Press ""play"" and the plot starts moving, with time $t=\text{Im}(f(z))$ . Alternatively, the three axes could be $|z|, \operatorname{arg}(z), |f(z)|$ , with time $t=\operatorname{arg}(f(z))$ . Questions: Is this method of visualizing complex functions known? Could it be useful? (Needham's book Visual Complex Analysis does not seem to mention this method.)","['complex-analysis', 'visualization', 'functions']"
4617988,Matrix derivative of $f(X^T Y)$ w.r.t X,"Given $X\in \mathbb{R}^{m\times n}$ , $Y\in \mathbb{R}^{m\times k}$ , $X^\top Y=Z\in \mathbb{R}^{n\times k}$ ,
and $f:\mathbb{R}^{n\times k} \to \mathbb{R}$ , we have the following: \begin{equation}
f(X^\top Y)=f(Z)
\end{equation} What is the derivative of $f$ with respect to $X$ , i.e., what is $\frac{\partial f}{\partial X}$ ? I tried to ""expect"" $\frac{\partial f}{\partial X}$ just by trying to match the dimension $m,n,k$ as follows: \begin{equation}
\begin{aligned}
\frac{\partial f}{\partial X}
=\frac{\partial f}{\partial Z}\frac{\partial Z}{\partial X}
=Y\left(\frac{\partial f}{\partial Z}\right)^\top
\end{aligned}
\end{equation} Since $\frac{\partial Z}{\partial X}$ should be a fourth-order tensor , I tried to calculate $\frac{\partial Z}{\partial X}$ by using $\frac {\partial AXB}{\partial X}=B^T\otimes A$ like this: \begin{equation}
\begin{aligned}
\frac{\partial Z}{\partial X}=\frac{\partial (X^\top Y)}{\partial X}
{=\left(\frac{\partial ((X^\top) Y)}{\partial (X^\top)}\right)^\top}
{=\left(Y^\top \otimes I \right)^\top}
{=(Y^\top)^\top \otimes I^\top}
{=Y \otimes I}
\end{aligned}
\end{equation} Where $\otimes$ is the Kronecker product and $I$ is the $n\times n$ identity matrix. From this, we have: \begin{equation}
\begin{aligned}
\frac{\partial f}{\partial X}
=\frac{\partial f}{\partial Z}\frac{\partial Z}{\partial X}
=\frac{\partial f}{\partial Z}\left(Y \otimes I\right)
\end{aligned}
\end{equation} From above equations, this should be true: \begin{equation}
\begin{aligned}
\frac{\partial f}{\partial X}
=\frac{\partial f}{\partial Z}\left(Y \otimes I\right)
=Y\left(\frac{\partial f}{\partial Z}\right)^\top
\end{aligned}
\end{equation} The last equation seemed counter intuitive. I can not figure it out.
Do I calculate $\frac{\partial f}{\partial X}$ correctly ? If I did it wrong, how should I calculate $\frac{\partial f}{\partial X}$ instead?","['matrices', 'matrix-calculus', 'derivatives']"
4617990,Solving ODE by Laplace transformation,"Consider the differential equation $ty''(t)+2y'(t)+ty=0$ , $t>0$ , $y(0+)=1$ and $y'(0+)=0$ . If $Y(s)$ is the Laplace transform of $y(t)$ then find the value of $Y(1)$ . Taking Laplace transform both sides of the given equation and solving then we get, $$Y(s)=-\tan^{-1}(s)+C$$ where $C$ is integrating constant. My question is how to find the constant $C$ ? Any hint. please.","['integration', 'laplace-transform', 'ordinary-differential-equations']"
4617999,Do limits of (continuous) functions have a meaning in the distribution sense?,"This question came to my mind when working with distributions. We know that locally integrable functions can be realized as distributions. Particularly, if $f$ is locally integrable on $\Omega$ , then for any $\phi \in C^{\infty}_c ( \Omega )$ , we have the action of $f$ on $\phi$ given by $$f ( \phi ) = \int\limits_{\Omega} f ( x ) \phi ( x )\, \mathrm{d}x.$$ Now, we also know that continuous functions are locally integrable, and hence can be realized as distributions. My question is whether we can make sense of the point values $f ( x )$ as distributions? Further, can we make sense of "" $\lim\limits_{x \rightarrow x_0} f ( x ) = f ( x_0 )$ "" in the sense of distributions? Any insights into this are appreciable!","['limits', 'distribution-theory', 'real-analysis']"
4618008,Two hundred people are assigned a number and enter a room. Matching numbers leave. What will be the highest total in the room?,"I thought up this expected value problem: Two hundred people are each assigned an integer from 1-100 so that there is a pair of each number. They are standing outside of a large waiting room. A random person will enter the room, one at a time. If their pair is there, they both continue on their way, otherwise they wait in the room until their pair arrives. This goes on until there are no people left outside of the waiting room. Question: What is the expected value of what will be the greatest sum of the values in the waiting room? A pair that immediately leaves doesn't contribute to the sum. My thoughts: The lower bound is clearly 100 , if everyone got their pair immediately. The upper bound is 1+2+3+...+100 = 5050 , if there were no pairs for the first 100 people. In the simplified case of 4 people (1,1,2,2) there are 8 orders that will lead to a 2 and 1 being in the room at the same time and 16 orders of 2 being alone, so the expected value should be $\frac{8(2+1)+16(2)}{4!} = 2 \frac 13$ . How can this idea be applied to the larger group?","['statistics', 'problem-solving', 'probability', 'recreational-mathematics']"
4618012,Representation of harmonic function as a series [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Let, $u:D\to\mathbb{C}$ is a harmonic function then how to show that $u(z)=\sum_{n\in\mathbb{N}} a_n z^n + \sum_{n\in\mathbb{N}}b_n\bar{z}^n$ .
(D be the unit disk).","['complex-analysis', 'harmonic-functions']"
4618017,Does a function whose derivatives never attains values $\pm 1$ have a fixed point?,"I started wondering if the following is true. Consider a differentiable function $f:\mathbb R \to \mathbb R$ . If $f'(\mathbb R) \cap \{-1,1\} = \emptyset$ , then $f$ has a fixed point. From Darboux theorem it is apparent that one of the following cases holds: $f'(\mathbb R) \subset (-1,1)$ ; $f'(\mathbb R) \subset (1,\infty)$ ; $(-f)'(\mathbb R) \subset (1,\infty)$ . However I don't see how to continue from there.","['fixed-point-theorems', 'derivatives', 'examples-counterexamples', 'real-analysis']"
4618047,Taylor series without factorial term,"We all know and love the fact that in most of cases $ f(z)=\sum_{n=0}^{\infty}\frac { f^{(n)}(0)z^n}{n!} $ In context of my question, what I need is general method of evaluation $\sum_{n=0}^{\infty}f^{(n)}(0)z^n$ as transformation of $f (z) $","['laplace-transform', 'functions', 'taylor-expansion', 'sequences-and-series', 'power-series']"
4618136,Canonical coordinates and tautological one-form: about a paragraph in the Wikipedia article on the Tautological One-form,"As @peek-a-boo wrote in one of his answer, ""the word ""momentum"" gets thrown around more often than candy during Halloween"". I found two definitions of momentum generalized coordinates I want to reconcile one way or another. We go with the usual adapted coordinate charts on a manifold $M$ : the ones on $TM$ are noted $(q, \dot{q})$ and the ones on $T^*M$ are noted $(q, p)$ . The first definition is given in the Wikipedia article about Canonical coordinates as a function on the set of vector fields on $M$ to the set of functions on $T^*M$ by $$X\to \mu_X(p)=p(X(\pi(p))$$ where $p\in T^*M$ and $\pi: T^*M\to M$ the projection. The second definition is a function on the set of functions on $TM$ to the set of one-forms on $TM$ given by $$f\to\Theta_f=(\text{F}f)^*\alpha$$ where $\alpha$ is the tautological one form defined on $T^*M$ , and $\text{F}f$ the Legendre transform of $f$ . We get $\mu_{\frac{\partial}{\partial q}}=p$ and I want to find a way to get $p$ using the second moment map $\Theta_{\dot{q}}$ if possible (here I have edited my question following the correction of @peek-a-boo in his comment). How can I do it?","['moment-map', 'symplectic-geometry', 'differential-geometry']"
4618138,Cauchy-Schwarz inequality with non-standard dot product,"I have the following problem: Let's have the scalar product in space $ℝ^2$ given by the expression: $$ <x, y> = 2x_1 y_1 + x_2 y_2 + x_1 y_2 + x_2 y_1  $$ For a defined dot product, formulate the precise definition of the Cauchy-Schwarz inequality. For a given dot product, define the projection matrix onto the span{a} of the line a defined by the vector $ v = (1, 0)^T$ . For task 1, I was not really sure how to express the norm inducted by the dot product; therefore, I am struggling with the RHS of the inequality: $$ |<x, y>| ≤ ||x|| \ ||y|| $$ More precisely: $$ 2x_1 y_1 + x_2 y_2 + x_1 y_2 + x_2 y_1 ≤ \lVert x \rVert \ \lVert y \rVert $$ Should I express the euclidian norm in the way of $ \sqrt{<x, x>} $ , then get rid of the root and express it in some more suitable way? For task 2, I was able to determine the projection onto the line generated by span(a): $$ x_u = \frac{ax^T}{a^Ta} $$ from that, we get after substituting into the defined dot product the following equation: $$ \frac{<x, v>}{\lVert v \rVert} v = \frac{(2x_1 + x_2)}{\sqrt{2}} . (1, 0)^T $$ I believe this should be it, but now I need to express it using the projection matrix. I believe it by multiplying given vectors; I got the following matrix: $$ \begin{bmatrix}
2 & 0 \\
1 & 0 
\end{bmatrix} $$ But somehow, It does not seem right. Could anyone help me out? Thanks in advance.","['projection', 'inner-products', 'vector-spaces', 'matrices', 'linear-algebra']"
4618206,"In a Lie group, is it true that $i_*v^L=-v^R$?","If $G$ is a Lie group and $v\in T_eG$ , then I want to show that $$i_*v^L=-v^R$$ where $i$ is the inversion map $G\to G: g\mapsto g^{-1}$ and $v^L$ and $v^R$ are the left and right invariant vector fields, respectively, extending $v$ . I already have: $$(i_*)_g(v^L)_g=(i_*)_g((L_g)_*)_ev=((R_{g^{-1}})_*)_e(i_*)_e v=-(v^R)_{g^{-1}}$$ where I have used that $i(L_g(h))=R_{g^{-1}}(i(h))$ . Is it then true that $(v^R)_{g^{-1}}=(v^R)_g$ ? Any help would be welcome!","['lie-algebras', 'lie-groups', 'differential-geometry']"
4618207,What is the minimum area of a rectangle containing all circles of radius $1/n$?,"What is the minimum area of a rectangle containing all (non-overlapping) circles of radius $1/n$ , $n\in\mathbb{N}$ ? The total area of the circles is finite: $\sum\limits_{n=1}^\infty \frac{\pi}{n^2}=\frac{\pi^3}{6}\approx5.168$ . Below I show the rectangle of smallest area that can contain circles with radii $1$ and $\frac{1}{2}$ . This rectangle has height $2$ and width $\frac{3}{2}+\sqrt2$ , so the area is $3+2\sqrt2\approx5.828$ . I placed circles with radii $1, \frac{1}{2}, \frac{1}{3}, ..., \frac{1}{20}$ in no particular pattern. Here is the desmos graph . My intuition tells me that the other circles can also fit. The others would occupy only $\dfrac{\pi^3/6-\sum_{n=1}^{20}\pi/n^2}{3+2\sqrt2-\sum_{n=1}^{20}\pi/n^{2}}\approx 0.188$ of the remaining space. But how could we prove that all the circles can fit in this rectangle? My question was inspired by a question about fitting harmonic circles on a unit disk.","['discrete-geometry', 'circles', 'geometry', 'packing-problem']"
4618215,Does existence and continuity of partial derivatives imply differentiability in Normed Vector Spaces?,"I'm wondering if the 'Normed Vector Space version' of the following theorem holds: Theorem: let $A\subseteq \mathbb{R}^n$ be open and let $f:A\to \mathbb{R}^m$ have continuous partial derivatives $\partial f_i/\partial x_j$ on $A$ . Then $f$ is differentiable on $A$ . In particular, the total derivative can be extended to NVS by considering the Fréchet Derivative . If we extend the definition of partial derivatives as follows: Throughout the post let $V$ and $W$ be NVSs, with $V'\subseteq V$ open and $f$ a function $V'\to W$ . Definition: given $a\in V'$ and a vector $v\in V$ , we define (assuming the limit exists) $$\frac{\partial f}{\partial v}(a) := \lim_{t\rightarrow 0}\frac{f(a+tv)-f(a)}{t},$$ and, in the case $v=e_i$ for a basis vector $e_i$ , $$\frac{\partial f}{\partial x_i}(a) := \frac{\partial f}{\partial e_i}(a).$$ then, is either of the following statements true? Theorem (?): if $f:V'\to W$ has continuous partial derivatives $\partial f/\partial v$ (for any $v\in V$ ) on $A$ , then $f$ is differentiable on $A$ . Theorem (?): let $\dim V = n$ and $\dim W = m$ . If $f:V'\to W$ have continuous partial derivatives $\partial f_i/\partial x_j$ on $A$ , then $f$ is differentiable on $A$ .","['partial-derivative', 'frechet-derivative', 'multivariable-calculus', 'normed-spaces']"
