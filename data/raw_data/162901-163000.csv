question_id,title,body,tags
2830385,Connected components of nonempty level sets form a foliation of $M$.,"Let $M,N$ smooth manifolds and $F:M \to N$ a smooth submersion. Show that the connected components of nonempty level sets form a foliation of $M$. My idea is to use the Global Fröbenius theorem for the distribution $D$, where $D_{p}=ker (dF)_{p}$, I proved that $D$ is involutive distribution and for each $p \in M$, $F^{-1}(F(p))$ is an integral manifold for $M$ passing by $p$. My question is: If I prove that the connected components of $F^{-1}(F(p))$ are integral manifolds for $M$, then is each one a maximal integral manifold passing by $p$ and the global Frobenius theorem assures that connected components are a foliation for $M$?","['smooth-manifolds', 'differential-geometry', 'foliations']"
2830397,How to evaluate this integral using this definition?,"Let $a$ and $b$ be any two real numbers such that $a < b$, and let $f$ be the real-valued function defined on $[a, b]$ by the formula 
$$ f(x) = \mathrm{e}^x \ \mbox{ for all } \ x \in [a, b]. $$ Then how to evaluate 
$$ \int_a^b f(x) \ \mathrm{d} x, $$
using the definition of the integral given here ? My Attempt: Since $f$ is strictly increasing on $[a, b]$ [Can we show this fact using the machinery developed in the first seven chapters of Baby Rudin?], so it is Riemann-integrable on $[a, b]$; that is, the integral $\int_a^b f(x) \ \mathrm{d} x$ exists as a real number. Thus the upper integral $\overline{\int}_a^b f $ and the lower integral $\underline{\int}_a^b f $ are equal, where 
  $$ \overline{\int}_a^b f \colon= \inf \left\{ \ U(P, f) \ \colon P \mbox{ is a partition of the interval } [a, b] \ \right\}, $$
  and 
  $$ \underline{\int}_a^b f \colon= \sup \left\{ \ L(P, f) \ \colon P \mbox{ is a partition of the interval } [a, b] \ \right\}, $$ Thus, for every partition $P$ of $[a, b]$, we have 
  $$ L(P, f) \leq \int_a^b f \leq U(P, f). $$ Again as $f$ is Riemann-integrable on $[a, b]$, so, corresponding to every real number $\varepsilon > 0$, we can find a partition $P_\varepsilon$ of $[a, b]$ such that 
  $$ U \left( P_\varepsilon, f \right) - L \left( P_\varepsilon, f \right) < \varepsilon. $$ Now let $P \colon= \left\{ \ x_0, x_1, \ldots, x_{n-1}, x_n \ \right\}$ be any partition of the interval $[a, b]$, where 
  $$ a = x_0 < x_1 < \cdots < x_{n-1} < x_n = b. $$
  As $f$ is a monotonically increasing function on $[a, b]$, so for each $i = 1, \ldots, n$, we see that 
  $$ m_i \colon= \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = f \left( x_{i-1} \right) = \mathrm{e}^{x_{i-1}}, $$
  and 
  $$ M_i \colon= \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = f \left( x_{i} \right) = \mathrm{e}^{x_{i}}; $$
  therefore we have 
  $$ L(P, f) \colon= \sum_{i=1}^n m_i \left( x_i - x_{i-1} \right) = \sum_{i=1}^n \mathrm{e}^{x_{i-1}} \left( x_i - x_{i-1} \right), $$
  and 
  $$ U(P, f) \colon= \sum_{i=1}^n M_i \left( x_i - x_{i-1} \right) = \sum_{i=1}^n \mathrm{e}^{x_{i}} \left( x_i - x_{i-1} \right). $$ Now for each positive integer $n$, let $P_n$ be the partition of $[a, b]$ given by 
  $$ P_n \colon= \left\{ \ a, a + \frac{b-a}{n}, a +  \frac{2(b-a)}{n}, \ldots, a + \frac{ (n-1) ( b-a ) }{n}, b \ \right\}; $$
  that is, $P_n$ partitions the interval $[a, b]$ into $n$ equal subintervals. Thus 
  $$ P_n = \left\{ x_0, x_1, \ldots, x_n \ \right\}, $$
  where $$ x_i \colon= a + \frac{ i(b-a)}{n} $$
  for each $i = 1, \ldots, n$. 
  Then we have 
  $$ 
\begin{align} 
 L \left( P_n, f \right) &= \sum_{i=1}^n \mathrm{e}^{a + \frac{ (i-1)(b-a)}{n} } \frac{b-a}{n} \\
&= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \mathrm{e}^{ \frac{ (i-1)(b-a)}{n} } \\ 
&= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i-1} \\
&= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=0}^{n-1} \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i} \\
&= \frac{b-a}{n} \mathrm{e}^a  \frac{ \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{n}  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\
&= \frac{b-a}{n} \mathrm{e}^a  \frac{ \mathrm{e}^{ \frac{ n (b-a) }{n} }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\
&=  \frac{b-a}{n} \mathrm{e}^a  \frac{ \mathrm{e}^{b-a }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 }  \\
&=  \frac{b-a}{n}   \frac{ \mathrm{e}^{b }  -  \mathrm{e}^a }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\
&= \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right), 
\end{align} $$
  and 
  $$ 
\begin{align} 
 U \left( P_n, f \right) &= \sum_{i=1}^n \mathrm{e}^{a + \frac{ i(b-a)}{n} } \frac{b-a}{n} \\
&= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \mathrm{e}^{ \frac{ i(b-a)}{n} } \\ 
&= \frac{b-a}{n} \mathrm{e}^a  \sum_{i=1}^n \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{i} \\
&= \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} } \frac{ \left( \mathrm{e}^{ \frac{ b-a}{n} } \right)^{n}  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\
&= \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} }  \frac{ \mathrm{e}^{ \frac{ n (b-a) }{n} }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\
&=  \frac{b-a}{n} \mathrm{e}^a \mathrm{e}^{ \frac{ b-a}{n} } \frac{ \mathrm{e}^{b-a }  - 1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 }  \\
&=  \frac{b-a}{n} \mathrm{e}^{ \frac{ b-a}{n} }  \frac{ \mathrm{e}^{b }  -  \mathrm{e}^a }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \\
&= \frac{b-a}{n}  \frac{ \mathrm{e}^{ \frac{ b-a}{n} } }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \\
&= \frac{b-a}{n} \left\{ 1 + \frac{1 }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \\
&= \left\{ \frac{b-a}{n}  + \frac{ \frac{b-a}{n}  }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) . 
\end{align} $$ Is what I have done so far correct? If so, then what next? How to proceed from here, preferably using only the machinery developed by Walter Rudin as far as Chapter 7 in the book Principles of Mathematical Analysis , third edition? P.S.: We note that, for every partition $P$ of $[a, b]$, we have 
  $$ L(P, f) \leq \underline{\int}_a^b f  \leq \overline{\int}_a^b f \leq U(P, f). \tag{A} $$ Now from (A), we can conclude that, for each $n \in \mathbb{N}$, by using the partitions $P_n$, we have 
  $$ L \left( P_n, f \right) \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq U \left( P_n, f \right); $$
  that is, 
  $$ \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq \left\{ \frac{b-a}{n}  + \frac{ \frac{b-a}{n}  }{ \mathrm{e}^{ \frac{ b-a}{n} } - 1 } \right\} \left( \mathrm{e}^{b }  -  \mathrm{e}^a \right) \tag{B} $$
  for each $n \in \mathbb{N}$. Now we see that 
  $$
\begin{align}
 \lim_{r \to 0} \frac{ r }{ \mathrm{e}^r - 1 } &= \lim_{r \to 0} \frac{1}{\mathrm{e}^r - 0} \qquad \mbox{ [ using the L'Hosptial's rule ] } \\ 
&= \frac{1}{1} \\
&= 1. \tag{1}
\end{align}
$$ Now as $n \to \infty$, $(b-a)/n \to 0$, and so from (1) we can conclude that 
  $$ \lim_{n \to \infty} \frac{ \frac{b-a}{n} }{ \mathrm{e}^{ \frac{b-a}{n} } - 1 } = 1. \tag{2} $$ Finally letting $n \to \infty$ in (B) and using (2), we find that 
  $$ \mathrm{e}^b - \mathrm{e}^a \leq \underline{\int}_a^b f \leq  \overline{\int}_a^b f \leq \mathrm{e}^{b }  -  \mathrm{e}^a.  $$
  Therefore 
  $$ \underline{\int}_a^b f  = \mathrm{e}^b - \mathrm{e}^a = \overline{\int}_a^b f. $$
  That is, 
  $$ \int_a^b f =  \mathrm{e}^b - \mathrm{e}^a. $$ I hope I've completed the proof satisfacorily enough in the P.S., haven't I?","['real-analysis', 'riemann-integration', 'integration', 'definite-integrals', 'analysis']"
2830407,"$X=\sum_{i=1}^{N}X_i$,estimator for $N$(continuation)","Let $X_i$ , $i\geq 1$ , be independent and identically distributed random variables having the uniform distribution over $(0,1)$ . Let $X$ be defined as $X=\sum_{i=1}^{N}X_i$ , where $N$ is an unknown integer. (a) Find an unbiased estimator $T(X)$ of $N$ . (b) Decide with adequate reasons, if $\dfrac{T(X)}{N}$ converges to $1$ almost surely, as $N$ goes to infinity. Now, in my previous question I wanted to know something different ( link ) but not the solution. So, I tried to find the solution after @Ben's hint. Now, we know that $E(2X)=N$ . But since, $2X$ is not always natural number, I need to find the value, $E([2X]+1)$ . Here, $[\cdot]$ is box function. To do that, we need to find the distribution of $[2X]$ . Now, $P([2X]=k)=P(k\leq 2X<k+1)$ , $k=0,1,\dots,2N-1$ Although, I could not find a way to compute $P(k\leq 2X<k+1)$ . Any help appreciated.","['law-of-large-numbers', 'statistics', 'probability', 'statistical-inference']"
2830424,Show that $B$ $\in$ $GL_{n-1}($K$)$ $\Rightarrow$ $A$ $\in$ $GL_n$($K$),"Let $K$ be a field and A = $ 
 \begin{bmatrix}
-1 & A_{12} \\
 0 & B \\
 \end{bmatrix}
$  $\in$ $K^{n,n}$ and $B$ $\in$  $K^{n-1,n-1}$. Show that $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$. My thoughts and questions : $B$ is obviously  a (n - 1) $\times$ (n - 1) matrix in the field $\mathbb{K}$. For example: For n = 4 is $B$ a $\underbrace{3}_{4-1}$ $\times$ $\underbrace{3}_{4-1}$ matrix in the field $\mathbb{K}$. But generally I don't understand how $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$. I thought about using this lemma: Let $A$ $\in$ $K^{n,m}$ with $A$ = $
\begin{bmatrix}
A_{1} \\
A_{2} \\
\end{bmatrix}$ which means that $A_{1}$ $\in$ $K^{l,m}$, $A_{2}$ $\in$ $K^{n-l,m}$. Therefore we have that Rank(A) $\le$ Rank($A_{1}$) + Rank($A_{2}$). But it is made for  $A$ $\in$ $K^{n,m}$ and not for $A$ $\in$ $K^{n,n}$. How do I beginn this proof? and how does  $A$ $\in$ $GL_n$($K$) applies if $B$ $\in$ $GL_{n-1}($K$)$? (I really don't why this correct). Any hints guiding me to the right direction I much appreciate.","['matrices', 'matrix-rank', 'linear-algebra']"
2830429,Expected number of correct guess in a game,"Question is as follow. There is a bag. In the bag, there are $a$ red cubes and $b$ blue cubes. Assume that she knows exactly how many cubes for each of the colors before the draw.  Mary is going to draw all the cubes one by one out of the bag randomly. For every turn, she will make a guess on the color before drawing the cube. Find the expected number of correct guess in the game. Trial:
I can solve for a simpler case, that is when she doesn't know the number of cubes of each color. But if she does, then I can figure out her strategy, that is she is going to guess the color which is greater in number in the bag. Also, I tried the find the probability of getting one correct only but cant gets a success because it seems to depend on the previous results.","['probability-theory', 'probability']"
2830445,How to recover the covariant derivative from the pull back from that on the principal bundle,"I am watching these lecture series by Fredric Schuller. Covariant derivatives - Lec 25 - Frederic Schuller @minute 01:10:11 When we arrive at the covariant derivative from the principal bundle $P$ by pulling back to the base manifold $M$ we have: $$\nabla _{T} S=dS(T)+\omega^{u, \phi} \triangleright S$$ where $S:u \to F$, $F$ any finite dimensional vector space on an open subset $u$ on $M$, $\phi: u \to P$, is the section on the principal G-bundle,  $T \in T_{x}M$, is a tangent vector at point $x$ in the base manifold $M$, and $\omega^{u, \phi}$ is the Lie algebra valued one-form on the principal bundle . Now my question is that how to recover, if possible step by step, the more familiar covariant derivative for e.g. a vector $V$ on the base manifold which is written as: $$\nabla_{\mu}V^{\nu}=\partial _{\mu}V^{\nu}+\Gamma_{\mu \lambda}^{\nu}V^{\lambda}$$ I am a bit confused when I plug a vector $V$ instead of $S$ in the general equation above and how should I put the exterior derivative of this one-form vector-valued object and operate it on another vector $T$ and also how to operate a connection one form pulled back to the base manifold on the vector $V$?","['derivatives', 'pullback', 'principal-bundles', 'differential-geometry', 'lie-algebras']"
2830450,Field having exactly two extensions of each degree,"It is well-known that a finite field has a unique extension of degree $n$, in a given algebraic closure, for every $n \geq 1$. Is there a field $F$ such that, in some given algebraic closure of $F$, there are exactly two extensions of $F$ of degree $n$, for every $n \geq 3$ (or $n$ big enough)? (Here, ""exactly two"" means that $F \subset K, L \subset \overline F$ simply satisfy $K \neq L$. In particular, I'm not identifying isomorphic fields or $F$-algebras.) A more general question would be to replace ""exactly two"" by  ""exactly $N$"" (for some fixed integer $N > 1$). Actually, the most general setting is the study of the maps $d_F$ (resp. $d_{F, \text{iso}}$, resp. $d_{F, F\text{-iso}}$) which assign, to every integer $n \geq 2$, the cardinal of the set of (resp. field isomorphism classes, resp. $F$-algebras isomorphism classes of) extensions of $F$ of degree $n$. For instance: Can $d_F$ be a constant map $n \mapsto c$ for some $1<c<\aleph_0$ ? Can $d_F$ be a bounded function (different from the constant function $1$) ? Can $d_F$ take both finite and infinite values? It can be shown that a local field of characteristic $0$ has finitely many extensions of some given degree, but the number might grow with the degree (instead of being bounded by $N$ as I want). Thank you!","['abstract-algebra', 'extension-field', 'field-theory']"
2830475,"An interesting proof that $\sin^2(x) + \cos^2(x) = 1$ (using only series, no trigonometry).","This question concerns an interesting proof of the fact that $\sin^2(x) + \cos^2(x) = 1$, but only using the series that defines them, not any trigonometry. So define
$$
s(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots
$$
and 
$$
c(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \ldots
$$ Step 1: we prove that $s' = c$ and $c' = -s$. This can be done by differentiating the series componentwise:
$$
s'(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \ldots = c(x), 
$$
and 
$$
c'(x) = - x + \frac{x^3}{3!} - \frac{x^5}{5!} + \ldots = -s(x).
$$ Step 2: we prove that $(s^2+c^2)' = 0$. Using the chain-rule on both terms and then using our result of step 1 we compute:
$$
(s^2+c^2)' = 2s \cdot s' + 2c \cdot c' = 2sc+2c(-s) = 0. 
$$ Step 3: we prove that $s^2 + c^2 = 1$. The idea here is to use step 2, to obtain something like 
$$
s^2 + c^2 = \int (s^2 + c^2)'\,dx = \int 0 \, dx = 1. 
$$
However, I cannot figure out the details of this last step. In particular, as far as I know, $\int (s^2 + c^2)'\,dx = s^2+c^2 + C$, and $\int 0\,dx = C'$. What happens with these constants?","['real-analysis', 'sequences-and-series', 'calculus']"
2830477,Critical values of a map to a Riemann surface,"Let $f:X\to S$ be open proper holomorphic map, $X$ a complex manifold and $S$ a Riemann surface. Is it then true that the critical values $C\subset S$ of  $f$ are a discrete supset? So far I only noted that this would be true, if the set of critical points $K\subset X$ was discrete. But I don't know how to prove it. If $X$ was also a Riemann surface, I could apply the identity principle to $f'$, but in the general case I don't know how to proceed. Edit: I know about Sard's theorem. I don't see why it is strong enough though.","['derivatives', 'complex-geometry', 'complex-analysis', 'differential-geometry', 'analysis']"
2830524,"If $\{a_n\}$ is a sequence such that $0\leq a_{m+n}\leq a_m+a_n$, show that $\lim_{n\to\infty}\frac{a_n}{n}$ exists [duplicate]","This question already has answers here : Prove $\lim_{n\to\infty} \frac{a_n}{n}$ exists for positive sequence where $a_{n+m} \leq a_n + a_m$ (3 answers) Closed 6 years ago . Let $\{a_n\}$ be a sequence satisfying $\forall m,n\in\Bbb{N}:0\leq a_{m+n}\leq a_m+a_n$. Prove that $$\lim_{n\to\infty}\frac{a_n}{n}$$ exists. At first I tried to do it with some $\epsilon-\delta$. We wish to prove that there exists some $L$ such that $\forall\epsilon>0:\exists n_0\in \Bbb{N}:n>n_0\Rightarrow|\frac{a_n}{n}-L|<\epsilon$. Which then got me to something like this:$$-\epsilon\cdot n<a_n-Ln<\epsilon\cdot n$$ which got me nowhere and I didn't find a way to make use of the inequality given. The sequence is obviously bounded below but non-decreasing, which tells me nothing. I don't see any way to use this fact to prove the existence of given limit. Any useful tips, hints appreciated.","['sequences-and-series', 'limits']"
2830537,Floor function of a real number,"The floor of a real number $x$, denoted by $⌊x⌋$, is the largest integer that is less than or equal to $x$. The ceiling of a real number x, denoted by $⌈x⌉$, is the smallest integer that is greater than or equal to $x$. Let $x$ and $y$ be any real numbers. Let $n, m$ and $k$ be any integers. $⌊x⌋ \le x < ⌊x⌋+1$, equivalently $x-1 < ⌊x⌋ \le x$. $⌈x⌉-1 < x \le ⌈x⌉$, equivalently $x \le ⌈x⌉ < x+1$. If $n \le x < n+1$ (or $x-1 < n \le x$), then $n=⌊x⌋$; similarly, if $n-1 < x \le n$ (or $x \le n < x+1$), then $n=⌈x⌉$. If $x \le y$, then $⌊x⌋ \le ⌊y⌋$ and $⌈x⌉ \le ⌈y⌉$. Similarly for $x \ge y$. If $n \le x$, then $n \le ⌊x⌋$. If $n \ge x$, then $n \ge ⌈x⌉$. If $x$ has an integer value, then $⌊x⌋=x=⌈x⌉$. If x has a non-integer value, then $⌈x⌉=⌊x⌋+1$. Furthermore, we have the well-known inequality: $ \frac {x-1}{x} \le \ln x \le \frac {x^2 -1}{2x} \le x-1$
I am trying to find all pairs of positive real numbers $(x,y)$ such that $ \ln⁡(⌊x+y⌋ )=⌊y⌋+⌊x⌋,$ but the above identities are hard to follow. Thank you",['number-theory']
2830563,"Let $A\in M_{n\times n}(\Bbb R)$ so that $I\notin span(A,A^2,...,A^n)$. Prove that $\det(A)=0$.","Let $A\in M_{n\times n}(\Bbb R)$ so that $I\notin span(A,A^2,...,A^n)$. Prove that $\det(A)=0$. I was thinking of showing $A$ is not invertible, meaning it has an eigenvalue of $0$. Since no matter the power you give to $A$, it is still not the identity, you can deduce that at least one eigenvalue is indeed $0$. However this doesn't work if you have different eigenvalues in the Matrix, so I got stuck. Help will be most appreciated.","['matrices', 'linear-algebra', 'determinant']"
2830600,"What about the definite integral $\int_0^1\int_0^1\frac{(\operatorname{arctanh}(xy))(\arctan(xy))}{\log(xy)}\,dx\,dy$?","I would like to know if it is possible to justify the calculation of next definite integral $$\int_0^1\int_0^1\frac{(\operatorname{arctanh}(xy))(\arctan(xy))}{\log(xy)}\,dx\,dy.$$ Question.(Corrected, see the comments) My reasonings and expriments with Wolfram Alpha online calculator, suggest me that the following identity holds $$\int_0^1\int_0^1\frac{(\operatorname{arctanh}(xy))(\arctan(xy))}{\log(xy)}\,dx\,dy=-\frac{1}{32}\left(16C-\pi^2+4\pi\log 2\right),$$
  where $C$ denotes the Catalan's constant. Am I right? Do you know it or is it possible to justify? Many thanks. My motivation was to compute an example of a reduction formula for integrals that I've known from a preprint by M.L. Glasser (Universidad de Valladolid). I've deduced the conjeture in the Question from my subsequent calculations for the limits of integration to deduce the closed-form of our doble integral  (to me seem that were difficult calculations and the justification should be tedious, this is why I am asking here) with the mentioned CAS.","['integration', 'definite-integrals', 'closed-form']"
2830651,Inequalities with absolute values,"My question is: Show that for all $|x-1|+|x-2|+\dots+|x-10| > 23$ I have solved above problem as below, If $x-1 > 0$ and $x-2 > 0$ and ......$x-10 > 0$ then LHS $= x-1+x-2+x-3+\dots+x-10 = 10x-55 > 23$ (because $x>10$) If $x-1<0$ and $x-2 < 0$ and ......$x-10 < 0$ then LHS $= -x+1-x+2-x+3.....-x+10 = -10x+55 > 23 $ (because $x<1$) If the above solution is wrong, please give me the correct method","['algebra-precalculus', 'absolute-value', 'functions']"
2830684,Integrating over integral limits,"What does $$\int_{-\infty}^{\infty} \Big( \int_{x-1/2}^{x+1/2}f(t)dt \Big) dx$$ mean? Is it the same as $$\int_{-\infty}^{\infty} f(x) dx$$ or do the boundaries ""overlap""? Are there any good theorems to use here to access the limits of the integral? Is it possible to somehow change the order of integration? I am trying to figure out this interval for a function $f$ with $$\int_{-\infty}^{\infty} f(x) dx=1$$ and I suspect that $$\int_{-\infty}^{\infty} \Big( \int_{x-1/2}^{x+1/2}f(t)dt \Big) dx=1$$","['integration', 'analysis']"
2830807,"If $K\subset{\mathbb{R}}$ closed and bounded, then K compact Proof (without covers)","I have already proved the reverse (Heine-Borel Theorem) by use of the Bolzano Weierstrass Theorem, but, reading several proofs of this forward direction, am yet to come across a (short) proof which shows this without the need for open covers - note, here I define a compact set to be a subset $K\subset{\mathbb{R}}$ such that every sequence in $K$ has a subsequence that converges to a limit that is also in $K$ Could someone please show thus this forwards direction (closed and bounded, then compact) without resorting to open covers (not for lack of knowledge, but as covers appear on the next page of this theorem in the book I'm studying).","['general-topology', 'sequences-and-series', 'elementary-set-theory']"
2830846,How to remove the second two leading terms in the general quintic with just algebra?,"Motivated by How to transform a general higher degree five or higher equation to normal form? The goal of the linked question is to transform the general quintic $$x^5+ax^4+bx^3+cx^2+dx+e=0$$ into Bring-Jerrard normal form. Tito Piezas III begins his answer with the quadratic Tschirnhausen transformation, $$y=x^2+mx+n$$ and by using resultants which may be calculated by WolframAlpha , one can write the result as $$y^5+c_1y^4+c_2y^3+c_3y^2+c_4y+c_5=0$$ where we proceed to make $c_1=c_2=0$. However, it is not immediately obvious to me how one performs this step, particularly the process of eliminating $x$ and replacing it with $y$. How can I perform this step without referring to resultants and anything outside of simple algebra? Or, if it makes any difference, how can I go from $$x^5+ax^4+bx^3+cx^2+dx+e=0$$ to $$y^5+c_3y^2+c_4y+c_5=0$$ ?","['algebra-precalculus', 'substitution', 'resultant', 'polynomials']"
2830860,Binomial sums from Bieberbach conjecture,"It seems that I need some kind of hint or help with Exercise 33 from this wonderful blog post by T. Tao, namely with the following equality: $$
\sum_{j=0}^{n - k} (-1)^j {2k + 2j \choose j} {n + j + k + 1 \choose n - k - j} = \frac{1 + (-1)^{n - k}}{2},
$$
for all $n \geq 1$ and $0 \leq k \leq n$. It must be some easy exercise since no hints provided, but I still have no ideas though I had already tried to use some induction or straightforward counting. Any help appreciated. Thanks in advance.","['combinatorics', 'summation', 'binomial-coefficients']"
2830863,How to prove that $A - (A \cap B) = A - B$,"I've found a very similar question , but couldn't follow the only answer. I don't know how to get this: $A\cap(A^c\cup B)=(A\cap A^c)\cup (A\cap B)$, even if I see that they're equivalent. I want to know how to prove this using set properties as $A \cap A' = 0$, $(A \cap B)\cap C = A \cap    (B \cap C)$, $A \cap \varnothing = \varnothing$, $A \cup \varnothing = A$,  if that's possible. I can usually think with Venn diagrams and get the answer, especially because of the visual intuition, but they can't be used to prove anything and they won't get me far anyway. I understand that $A - (A \cap B) = A - B$, my problem is in proving it.",['elementary-set-theory']
2830875,Why does the concept of differentiation have the meaning only on open sets?,"I am just stuck on the same problem asked in the following question . The book asks to implicitly differentiate two equations. Then it turns out the answer is ""neither of them is implicitly differentiable"". Alex M. in the previous question answers saying: ... $y = \pm x$ but this equality does not define on open set, only a finite set of points, and the concept of derivability makes no sense on such sets. Can someone explain what is going on?","['derivatives', 'implicit-differentiation', 'general-topology', 'calculus']"
2830925,Matrix with $A_{ij} = a_i a_j$ for some vector $a\in\mathbb{R}^n$,"If $f(x) = g(\langle a,x\rangle)$ for $a,x\in\mathbb{R}^n$ and $g: \mathbb{R}\to\mathbb{R}$, with the usual inner product, then the matrix of second partials is $D^2f(x) = g''(\langle a,x\rangle) A$, where $A$ has components $A_{ij} = a_i a_j$. Is there a name for such a matrix constructed from a vector in this manner, or more generally $A_{ij} = a_i b_j$ for $b\in\mathbb{R}^m$ with $m$ not necessarily equal to $n$?","['matrices', 'multivariable-calculus', 'linear-algebra']"
2830997,Strong maximum principle for subharmonic functions?,"I think I just ""proved"" the Strong MP for sub harmonic functions. But I don't know where things went wrong. Suppose $U$ is a connected bounded region in $\Bbb R^n$, $u\in C^2(\bar U)$ and $\Delta u\ge 0$ in $U$. Then I try to prove in the following that if $u$ isn't constant, then $\max_{\bar U}u$ is only attained on the boundary $\partial U$. Proof: for any $x\in U$ and $r>0$ such that $B_r(x)\subset U$, consider the spherical mean 
$$\phi(r):=\frac1{|\partial B_r(x)|}\int_{\partial B_r(x)} u(y)dS(y)=\frac1{|\partial B_1(0)|}\int_{\partial B_1(0)} u(x+rz)dS(z).$$
Then 
$$
\begin{align}
\phi'(r)&=\frac1{|\partial B_1(0)|}\int_{\partial B_1(0)} Du(x+rz)\cdot zdS(z)\\
&=\frac1{|\partial B_1(0)|}\int_{B_1(0)}\Delta u(x+rz)dz\ge 0
\end{align}
$$
Hence $\phi(r)$ is monotonously increasing and in particular $\ge \phi(0^+)=u(x)$ when $r>0$. Hence, if at $x\in U$ is attained the max $M$, then the above mean value inequality forces $u=M$ on a small ball centred at $x$. So $E:=\{x\in U\mid u(x)=M\}$ is open in $U$, but is also closed by continuity. So either 
$E$ is empty (max is only attained on the boundary) or $E$ is all of $U$ ($u$ is constant).","['real-analysis', 'harmonic-functions', 'partial-differential-equations', 'proof-verification', 'ordinary-differential-equations']"
2831001,When is the Frobenius norm bounded by the nuclear norm?,"I am reading the Recht (2011) paper titled,  ""A Simpler Approach to Matrix Completion"", and I cannot figure out the last inequality of the last line on page 3422 (page 10 of the document). The intermediate step seems to be $$\| M \|_I -\frac{1}{2} \|\mathscr P_{T^\perp}(Z)\|_F + \frac{1}{2} \|\mathscr P_{T^\perp}(Z)\|_*  \geq \|M \|_* $$ where $\| \cdot \|_F$ refers to the Frobenius norm and $\| \cdot \|_*$  refers to the nuclear norm. This would be possible if $\|\mathscr P_{T^\perp}(Z)\|_* \geq \|\mathscr P_{T^\perp}(Z)\|_F $, but I do not think the nuclear norm is an upper bound for the Frobenius norm in general.  I believe it is true if the matrix has spectral norm of 1, but I don't think that is necessarily the case here. What is the relationship between the Frobenius norm and the nuclear norm that I am missing to explain this last step?","['matrices', 'matrix-norms', 'nuclear-norm']"
2831070,How to derive Riemann-Lebesgue lemma from Bessel inequility?,"I encountered Riemann-Lebesgue Lemma in Functional Analysis. It can be viewed as a corrollary of Bessel inequtility in the following picture: However, I can't see why it is ' in particular ' ?  Can anyone give me some hint? My brain just short-circuit ...","['functional-analysis', 'real-analysis', 'analysis']"
2831090,Solving a non-linear ordinary differential equation that includes $\tanh$ and $\cos$,"$$\big(A+B\tanh (Cy+D)\big)\frac{\mathrm dy}{\mathrm dx}+y=P\cos(\omega x),$$
where $A$, $B$, $C$, $D$ and $\omega$ are constant. I am really looking for an analytical solution to this differential equation. But I really don't know where to start. Can someone please help? Thank you.","['ordinary-differential-equations', 'nonlinear-system']"
2831104,Prove $ \frac{1}{2\sqrt{1}}+\frac{1}{3\sqrt{2}}+\dots+\frac{1}{(n+1)\sqrt{n}}<2$,For any positive integer $n$ prove by induction that: $$ \frac{1}{2\sqrt{1}}+\frac{1}{3\sqrt{2}}+\dots+\frac{1}{(n+1)\sqrt{n}}<2.$$ The author  says that   it  is  sufficient to prove  that $$ \frac{1}{2\sqrt{1}}+\frac{1}{3\sqrt{2}}+\dots+\frac{1}{(n+1)\sqrt{n}}<2-\frac{2}{\sqrt{n+1}}.$$ Why? Where this $\frac{2}{\sqrt{n+1}}$ term come from?,"['algebra-precalculus', 'induction']"
2831108,"Find all odd solutions for $n$ for which $3n^2+8$ is equal to a number in base $10$ which is formed by only one digit(e.g.-$222,8888888,4,99$ etc.)","Find all odd solutions for $n$ for which $3n^2+8$ is equal to a number in base $10$ which is formed by only one digit(e.g.-$222,8888888,4,99$ etc.) My approach:- Let $3n^2+8=...mmmmmm...$($k$ digits) Applying 'Sum of GP', $$\frac{m(10^k-1)}{9}=3n^2+8$$
$$m×10^k=27n^2+72+m$$ Now, if $n$ is odd, $27n^2+72$ can have $5,7,9$ as last digits. Now I can look into what can be the last digits of $m$ and analyse further, but only with the help of somebody.","['number-theory', 'elementary-number-theory']"
2831162,How to prove that if the determinant of the matrix is zero then at least one eigenvalue must be zero? [duplicate],"This question already has answers here : Show that a matrix $A$ is singular if and only if $0$ is an eigenvalue. (8 answers) Closed 5 years ago . For a matrix $A$, if $\det(A)=0,$ prove and provide an example that at least one eigenvalue must be zero. At first, I tried using the identity that the product of eigenvalues is the determinant of the matrix, so it follows that at least one must be zero for the determinant to be zero. Is this correct? Could I also prove it by using $(A-\lambda I)X=0$, for some $X\neq 0?$ If $\lambda=0,$ then we have $AX=0$, but I can't say $\det(A)\cdot \det(X)=0$ because $X$ is not a square matrix and doesn't have a determinant. How would I continue?","['matrices', 'eigenvalues-eigenvectors', 'determinant']"
2831182,Calculate volume enclosed by cylinder and paraboloid (integration).,"I need to calculate the volume enclosed by:
$$x^2 + y^2 = z, \space y = x^2, \space z=0, \space y = 1$$ The shape of the volume I get gets me confused. It is a paraboloid ($x^2 + y^2 = z$) intersected with cylinder ($y = x^2$) and limited by specific $z$ and $y$ plains. When I tried drawing this I saw that the volume is not limited by the ""upper"" $z$ plain, therefore it seems to be infinite. Did the lecturer provide us with ""wrong"" conditions, so the volume is infinite? Am I right? If yes, how can I calculate the volume if I change my previous condition ($z = 0, \space y = 1$) to $0\le z \le 1$? I tried approaching this ""updated"" problem, but also didn't have any luck. Any help would be appreciated. EDIT: The answer including the integral solution was posted - see below. The whole problem was caused by me thinking about the volume ""inside"" the paraboloid, while the task was to calculate it ""outside"", enclosed by the cylinder.","['volume', 'integration']"
2831197,Is it true that if continuous $f(x)$ has a limit then $f(nx_0)$ has a limit for any $x_0$?,"If $f(x)$ is continuous on interval $(0, +\infty)$ and $\lim_{x\to +\infty}{f(x)} \in \mathbb {R}$ exists, is it true that $\lim_{n \to \infty}{(x_n)}$ also exists for $(x_n)=f(nx_0)$ and any $x_0>0$? I realized that if $f(x)$ is not continuous then the converse is not true (for example, we can consider $f(x)=\frac{x}{2}$ if $x$ is rational and $f(x)=x$ otherwise). Also I realized that the converse is always true for continuous $f(x)$. But I can't realize is my initial supposition stated in this question is true.","['real-analysis', 'sequences-and-series', 'functions', 'limits']"
2831199,Probability of getting 6 k times in a row,What is the probability of getting $6$ $K$ times in a row when rolling a dice N times? I thought it's $(1/6)^k*(5/6)^{n-k}$ and that times $N-K+1$ since there are $N-K+1$ ways to place an array of consecutive elements to $N$ places.,['probability']
2831214,Extension of a function which is uniformly continuous on compact subsets,"Let $d\in\mathbb N$ $\Lambda\subseteq\mathbb R^d$ be open $-\infty<a<b<c<\infty$ If $f:\Lambda\times\left((a,c)\setminus\left\{b\right\}\right)\to\mathbb R$ is uniformly continuous on any compact subset of $\Lambda\times\left((a,c)\setminus\left\{b\right\}\right)$, are we able to deduce that $f$ has a unique extension $\tilde f$ to $\Lambda\times(a,c)$ and that $\tilde f$ is uniformly continuous? I know that a uniformly continuous function into a Banach space has a unique extension to the closure of its domain.","['uniform-continuity', 'analysis']"
2831228,Is there any scalene non right angled triangle having its sides as simple rational numbers and whose angles have very simple trigonometric ratios?,"I am currently learning conditional trigonometric identities such as $ \sin{2A} + \sin{2B} +\sin{2C} = 4\sin{A}\sin{B}\sin{C}$  for $A + B+C = \pi$,  that is they are angles in a triangle. I am preparing for a multiple choice exam where time per question is only about $2$ minutes. Such identities are often asked in the form where LHS is given as the given and one is asked to chose the correct option for the RHS expression. It is very time consuming to actually prove the identity and then chose the correct option. Hence there is often a short cut to such problems wherein one can check all four options by substituting ratios of some common simple triangle such as the $30-60-90$ triangle. This works most of the times. However sometimes it so happens that two options seem to be correct when we check using the $30-60-90$ triangle but in reality only one answer is correct. This happens because I am using a special type of triangle (right triangle). To avoid this I am looking for a general triangle (scalene and non right angled) with simple rational sides (like $2$ or $\frac{3}{4}$) as well as angles whose trigonometric ratios are simple enough so that my method is feasible.","['contest-math', 'trigonometry', 'triangles']"
2831234,Diffeomorphisms and measurable sets,Suppose we have a diffeomorphism $\varphi: U \rightarrow V$ where U and V are open sets of $\mathbb{R}^n$. Can we show that the image of a measurable set A is again measurable. I know that this is the case for Borel-sets but can we show it for Lebesgue-sets? If so how can we show it?,"['measurable-sets', 'lebesgue-measure', 'measure-theory']"
2831246,First variation of induced metric on a hypersurface,"Reference: Here At page 15 , we could see that the first variation of the induced metric on a hypersurface $\Sigma(t)$ computed as $$\frac{\partial}{\partial t}g_{\Sigma(t)} = v(t) g_{\Sigma(t)},  \forall t \in (-\delta,\delta) \tag{1}$$ since $\Sigma(t)$ is umbilic and $H_t$ is constant, where $v$ is a real function. And then for all $t \in (-\delta, \delta)$ $$g_{\Sigma(t)}  = e^{\int^t_0 v(s) ds} = u(t)^2 g_{\mathbb S^2} \tag{2}$$ where $u(t)=a e^{\int^t_0 v(s) ds}$ with $a^2=|\Sigma|/4\pi \in (0,1)$. Question: Where do I start to get the expression (1) and (2)? I don't have any clue at all. Thank you.","['riemannian-geometry', 'differential-geometry', 'surfaces']"
2831329,How to prove a function is a covering map?,"I am learning differential geometry. The problem is I can't find some solved exercises so it is a bit hard to understand how can I solve them. I found this exercise on the web:
Let $B = \{(u,v)\ |\ u^2+v^2 \leq 1\}$ and $S^2 = \{(x,y,z)\ |\ x^2+y^2+z^2=1\}$. Let $f:S^2\rightarrow B,\ f(x,y,z) = (x,y)$. Check if $f$ is a covering map. Starting with the definition in my mind ( http://mathworld.wolfram.com/CoveringMap.html ), I really don't know how to start or what should I do to check that. Thank you very much!","['covering-spaces', 'differential-geometry', 'differential-topology']"
2831338,Disconnected Zariski open subsets of $\mathbb{C}^n$,"Can we find a Zariski open subset of $\mathbb{C}^n$ for some $n\ge 1$ which is not connected? If $n=1$, this is not possible, since Zariski open subsets of $\mathbb{C}$ are just complements of finite sets. Definition. A subset $U\subseteq\mathbb{C}^n$ is Zariski-open if its complement $\mathbb{C}^n-U$ is a zero set of polynomials $p_1,\ldots,p_k\in\mathbb{C}[x_1,\ldots,x_n]$.","['abstract-algebra', 'algebraic-geometry']"
2831387,Upper bounds on the approximate inverse of a singular matrix,"Let $A$ be a singular matrix and $\zeta>0$ such that $A+\zeta I$ is nonsingular. Soft question: Is it true that if $\zeta$ is sufficiently small, then $$(A+\zeta I)^{-1}A \approx I$$ (or $A(A+\zeta I)^{-1} \approx I$)? More precisely: Let $E:=(A+\zeta I)^{-1}A - I$. If the answer to the above (soft) question is ""yes"", what are some known bounds on $E$? Note. Using the condition number $\kappa(A)$, a well-known bound is 
$$
\frac{\Vert(A+B)^{-1} - A^{-1}\Vert}{\Vert A^{-1}\Vert}
\le \kappa(A)\frac{\Vert B\Vert}{\Vert A\Vert },
$$ but this bound requires that both $A$ and $A+B$ are nonsingular. In my question, $A$ is singular while $A+B$ is nonsingular.","['numerical-linear-algebra', 'matrices', 'inverse', 'numerical-methods', 'linear-algebra']"
2831405,Second derivative of $\det(\mathbb{1}+tA)$?,"I am given a function $F : \mathbb{R} \to \mathbb{R}$ defined by $$F(t)=\det(\mathbb{1}+tA)$$ where $A \in \mathbb{R}^{n \times n}$. As far as I know, the following is true. $$\frac{d}{dt}\bigg|_{t=0} F(t) = \text{tr}~ A$$ However, how to find the second derivative? $$\frac{d^2}{dt^2}\bigg|_{t=0} F(t)$$","['derivatives', 'real-analysis', 'matrix-calculus', 'determinant', 'linear-algebra']"
2831439,Showing that $4b^2+4b = a^2+a$ has no non-zero integer solutions?,"The problem is Show that $4b^2+4b = a^2+a$ has no integer solutions where none of $a, b$ are zero. I have a solution but I think there must be some better ways: My Solution: $$4b^2+4b = a^2+a$$ $$(2b+a)(2b-a)+4b-a= 0$$ Now letting $x = 2b + a$ and $y = 2b-a$, we see that $x+y = 4b$. Substituting, $$xy+\dfrac {x+y}{2}+y=0$$ $$2xy+x+3y=0$$ From this we see that $y|x$, so we can substitute $x = ky$ for some integer $k$ $2ky^2+ky+3y = 0$ $$k = \dfrac {3}{2y+1}$$ From here we get that $y \in \{-2, -1 , 1 \}$, and each of the cases can be checked individually.","['algebra-precalculus', 'elementary-number-theory']"
2831442,"Crandall, Pomerance - Prime Numbers: A Computational Perspective - Exercise 6.8 at page 274","I am trying to solve the exercise in the title. I write it here: prove for $d,n\in \mathbb{N}$
$$\frac{3}{2}\left(\frac{d}{\ln 2}\right)^d<n\quad\Rightarrow\quad n<2\lfloor{n^{1/d}\rfloor}^d$$ I have tried to solve it in this way:
$$\frac{3}{2}\left(\frac{d}{\ln 2}\right)^d<n\quad\Rightarrow\quad
\frac{3^{1/d}}{2^{1/d}}\frac{d}{\ln 2}<n^{1/d}$$
so
$$n<2\lfloor{n^{1/d}\rfloor}^d\quad\Leftarrow\quad
n<2(n^{1/d}-1)^d\quad\Leftarrow\quad
n<2\left(\frac{3^{1/d}}{2^{1/d}}\frac{d}{\ln 2}-1\right)^d
$$
But this way is an impasse.
Then I tried in this way
$$n<2\lfloor{n^{1/d}\rfloor}^d\quad\Leftarrow\quad
n<2(n^{1/d}-1)^d\quad\Leftrightarrow\quad
2\sum_{i=0}^d{\binom{d}{i}n^{(d-i)/d}(-1)^i}-n>0
$$ But I can't proceed. Could anyone give me a hint please?","['algebra-precalculus', 'algebraic-number-theory']"
2831443,How To Find Minimal Polynomial,"$$A=\left(\begin{array}{ccccc} 4 & 1 & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 & 0 \\ 0 & 0 & 4 & 0 & 0 \\ 0 & 0 & 0 & 9 & 0 \\ 0 & 0 & 0 & 0 & 9 \end{array}\right)$$ I know that the characteristic polynomial is $(\lambda-4)^3(\lambda-9)^2$
I know that the minimal polynomial can be a least $(\lambda-4)(\lambda-9)$ and $(\lambda-4)^3(\lambda-9)^2$ at most. The matrix is $\text{diagonal}(J_2(4),J_1(4),J_1(9),J_1(9))$ How can I continue?","['matrices', 'linear-algebra', 'minimal-polynomials']"
2831446,Does $V/(\ker f \cap \ker g) \cong \text{im} f +\text{im} g$?,Let $k$ be a field and $V$ be a finite dimensional $k$-vector space. Let $f$ and $g$ be two $k$-linear endomorphisms of $V$ such that $f\circ g=g\circ f$. Do we have an isomorphism of $k$-vector spaces $V/(\ker f \cap \ker g) \cong \text{im} f +\text{im} g$ ? Many thanks!,"['exact-sequence', 'matrix-rank', 'linear-algebra']"
2831469,A third order nonlinear ordinary differential equation.,"How can we solve $$y'''(t)+a(y''(t))^2+b(y'(t))^3=0$$ Could one make some kind of least common denominator argument to decide possible substitutions? Since the chain rule will come into play, I suppose a substitution both for the variable $t$ and the function $y$ could be useful. Possibly some powers of them?",['ordinary-differential-equations']
2831510,"Evaluate $\int_{-\infty}^\infty e^{x/2}\operatorname{sech}(x)\,dx$ without Residue Calculus","I want to integrate: $$\int_{-\infty}^\infty e^{x/2}\operatorname{sech}(x)\,dx.$$ I've trying to test some of my integration skills by attempting some integrals that are standard with residues, but for this one, I must be making a very silly mistake because after checking things with a calculator it doesn't align with what should be right. I started with integration by parts letting: $u=e^{x/2} => du=\frac{1}{2}e^{x/2}\,dx$ and $dv=\operatorname{sech}(x)\,dx => v=2\arctan(e^x)$ or $-2\arctan(e^{-x})$ I split the integral of $\operatorname{sech}(x)$ into two integrals so it would converge for the limits at infinity (this seemed to work for other integrals I did). After plugging everything:$$-2e^{x/2}\arctan(e^{-x})|_{-\infty}^{\infty}-\int_0^\infty e^{x/2}v\,dx =$$
$$-\int_{-\infty}^0 e^{x/2}\arctan(e^x)\,dx+\int_0^{\infty}e^{x/2}\arctan(e^{-x})\,dx. $$ This, however, is not correct, and I've been staring at it for too long. It would be much appreciated if some help could be given!","['improper-integrals', 'integration', 'trigonometry']"
2831526,Finding the mean of all 9-digit numbers formed from four $4$s and five $5$s,"I need to find the mean of the numbers ($9$-digit) formed of four $4$s and five $5$s . MY WORK: In order to find the sum I do the following and find sum of digits :
$$25\times5!=3000$$
$$16\times4!=384$$ So,  sum of all possible numbers :
$$3384(1+10+10^2+ ... + 10^8)$$
$$=3384\times10^7$$ For finding the amount of numbers formed,  I do :
$$\frac{9!}{4!5!}$$
$$=126$$ This,  the mean is :$$\frac{3384\times10^7}{126}$$ I don't know if I'm correct or not...","['combinations', 'combinatorics']"
2831537,Relative Fano variety of lines isomorphic to the usual one?,"Let $\mathfrak X\to \mathbb P^1$ be a projective family, where $\mathfrak X\subset \mathbb P^n\times\mathbb P^1$. Then we have the relative Fano variety of lines $F(\mathfrak X/\mathbb P^1)$; besides, using Segre embedding, $\mathfrak X\subset \mathbb P^n\times \mathbb P^1\subset\mathbb P^{2n+1}$, so we can talk about $F(\mathfrak X)$, the Fano variety of lines in the usual sense. My question is: Is $F(\mathfrak X/\mathbb P^1)$ isomorphic to $F(\mathfrak X)$? (Because the lines in Segre embedding only lies in one fibre, I think it is reasonable to guess they are the same)",['algebraic-geometry']
2831558,A connection between vector space operations and intuitionistic connectives?,"As I was working my way through Steven Roman's Advanced Linear Algebra , I came across this in Chapter 11, exercise 1. Let $U,W$ be subspaces of a metric vector space $V$. Show that (a) $U\subseteq W \implies W^\bot\subseteq U^\bot$ (b) $U\subseteq U^{\bot\bot}$ (c) $U^\bot = U^{\bot\bot\bot}$ Additionally, $(U+W)^\bot = U^\bot \cap W^\bot$ and $U^\bot + W^\bot \subseteq (U\cap W)^\bot$. I'm having some trouble with the reverse inclusion (at least it's not as obvious as the other ones were). All of these properties look strikingly similar to the behavior of connectives in intuitionistic logic, e.g. $p\rightarrow q \vdash \neg q\rightarrow \neg p$, $\vdash p \rightarrow \neg\neg p$, $\vdash \neg p \leftrightarrow \neg\neg\neg p$, $\vdash \neg(p \lor q) \leftrightarrow \neg p \land \neg q$, $\vdash \neg p \lor \neg q \rightarrow \neg(p\land q)$. Is there a connection here? If so, what is it?","['logic', 'linear-algebra']"
2831559,Given an abelian group with a divisor $D$ s.t. $L(D)$ gives projective embedding. Then $L(2D)$ is generated by degree 2 monomial elements of $L(D)$?,Given an abelian group torus $A=\frac{C^n}{\Lambda}$ with a divisor $D$ s.t. $L(D)$ gives projective embedding. $\textbf{Q:}$ Is $L(2D)$ generated by degree 2 monomial elements of $L(D)$? How do I see this.(I want to see the effect of Veronese embedding showing up somehow.) Riemann-Roch does not tell me this necessarily true? Is this true for large $n$ twist of $D$? Say $L(nD)$ for $n$ larger. Then $L(knD)$ is generated by degree $k$ monomial elements of $L(nD)$? How do I prove this? Hint only or reference only please.,"['abstract-algebra', 'complex-analysis', 'algebraic-geometry']"
2831599,Definition of a smooth 4-manifold,"A smooth (i.e. $C^{\infty}$) $n$-manifold $M$ can be defined as a topological manifold such that each point has a neighborhood which is diffeomorhic to an open subset of $\mathbb{R}^n$. In particular, every point has a neighborhood which is diffeomorphic to $\mathbb{R}^n$ (see e.g. here ). This definition is unambiguous if $n\neq 4$, since then $\mathbb{R}^n$ has a unique smooth structure. However, in dimension 4, Euclidean space $\mathbb{R}^4$ has uncountably many incompatible smooth structures. Suppose $M$ is a smooth 4-manifold and $p$ is a point in $M$. Then $p$ has a neighborhood $U$ which is diffeomorphic to $\mathbb{R}^4$, but which smooth $\mathbb{R}^4$? Is the convention that the smooth structure on $\mathbb{R}^4$ is taken to be the standard one? Are there smooth 4-manifolds such that any point has a neighborhood diffeomorphic to an exotic $\mathbb{R}^4$ (other than the exotic $\mathbb{R}^4$s themselves)? In other words, can one construct exotic 4-manifolds from exotic $\mathbb{R}^4$s? (For example, if you take a quotient $\mathbb{R}^4/\mathbb{Z}^4$ of an exotic $\mathbb{R}^4$, do you get an exotic torus?)","['differential-geometry', 'differential-topology']"
2831604,Limit of composite functions,"Let $f$ and $g$ be some functions, assuming all right conditions that allow function composition, I want to prove that $$\lim_{x \to \infty} f(g(x))=f\left(\lim_{x \to \infty}g(x)\right) $$ As long as $\lim_{x \to \infty}g(x)$ exists and it's equal to, let's say, $L$ , and $f$ is continuous at $L$ . Basically these conditions must be sufficient to guarantee that $\lim_{x \to \infty} f(g(x))$ exists and it's equal to $f(L)$ . So my proof so far goes as follows: Let $\epsilon>0 $ , since $f$ is continuous at $L$ , then there exists $\delta$ such that $|f(y)-f(L)|<\epsilon$ provided that $|y-L|<\delta$ . Let $y=g(x)$ , then provided that $|g(x)-L|<\delta$ , it follows that $|f(g(x))-f(L)|<\varepsilon$ . But since $\lim_{x \to \infty}g(x)=L$ , for any $\delta>0$ there exists $N\in\Bbb{R}^+$ such that if $x>N$ then $|g(x)-L|<\delta$ . That is, for any $\epsilon>0$ , there exists $N\in\Bbb{R}^+$ such that if $x>N$ then it follows that $$|f(g(x))-f(L)|<\epsilon$$ Which proves that $\lim_{x \to \infty} f(g(x))$ exists and it's equal to $f(L)$ . It's very important to me that the proof includes the existence of this limit, since it has great computational value when calculating limits that look a bit ""messy"". I'd love some notes on the proof, anything that I might be stating wrong or unclear (writing this kind of proofs is not my forte). Thanks in advance!","['proof-verification', 'function-and-relation-composition', 'functions', 'limits']"
2831645,Why doesn't the quadratic equation contain $2|a|$ in the denominator?,"When deriving the quadratic equation as shown in the Wikipedia article about the quadratic equation ( current revision ) the main proof contains the step:
$$
\left(x+{\frac {b}{2a}}\right)^{2}={\frac {b^{2}-4ac}{4a^{2}}}
$$
the square root is taken from both sides, so why is 
$$\sqrt{4a^2} = 2a$$ 
in the denominator and not
$$ \sqrt{4a^2} = 2\left |a  \right | $$
Could somebody explain this to me? Thank you very much","['algebra-precalculus', 'quadratics']"
2831652,"Stuck with integral $\int_{-\infty}^\infty \left( \frac{\sin(a t+b)}{at+b} \right)^2 \, dt$","I am stuck with the following integral: $$\int_{-\infty}^\infty \left( \frac{\sin(a t+b)}{at+b} \right)^2 \, dt$$ I would like to show that $\varphi(t)=\frac{\sin(at+b)}{at+b}$ belongs to $L^2(\mathbb{R})$ and/or $L^1(\mathbb{R})$, i.e. $\int_{-\infty}^\infty | \varphi |^2 \,dt < \infty$   and/or $\int_{-\infty}^\infty | \varphi | \,dt < \infty $. So far, I know that it is $|\frac{\sin(at+b)}{at+b}| \leq |\frac{1}{at+b}|$, but as $\int_{-\infty}^\infty |\frac{1}{at+b}|^2 \, dt$ does not converge, I cannot be conclusive. Looking at the plot it can be stated that it converges and then $\varphi \in L^2(\mathbb{R})$.","['functional-analysis', 'banach-spaces', 'integration', 'hilbert-spaces']"
2831676,When does Order of Second Partial Derivatives Matter? [duplicate],"This question already has an answer here : When does order of partial derivatives matter? (1 answer) Closed 6 years ago . My professor was saying that, for a function of multiple variables, usually the order in which you take the order of partial derivatives did not matter. (ex: $f_{xy} = f_{yx}$). Under what circumstances is this not true?",['multivariable-calculus']
2831700,A question about Egoroff theorem,"Consider the Lebesgue measure space $(\mathbb{R},\mathcal{M}(\mathbb{R}),m)$. Let us consider the sequence of functions $f_n:[0,1]\to \mathbb{R}$ defined by $f_n(x)=x^n$ for all $x\in [0,1]$ and for all $n\in \mathbb{N}$. Then $f_n$ converges pointwise $m$-a.e. on $[0,1]$ to $f\equiv 0$. I want to show that for all $E\subset [0,1]$ with $m(E)=0$, $f_n$ can not converge uniformly to $f$ on $[0,1]\setminus E$. Please help!","['lebesgue-measure', 'measure-theory', 'uniform-convergence']"
2831791,Constructing a field in which polynomial has root,"The problem Say we have a field $F$ and an irreducible polynomial $g \in F[x]$ of degree $\geq 1$. Let $(g)$ denote the (maximal) ideal generated by the $g$ in $F[x]$. Then define the field extension $F_1 = F[x]/(g)$ of $F$. Then $g$ has a root $\alpha$ in $F_1$, being $x + (g)$. My question(s) Why is $F_1$ a field extension of $F$? I don't see how the field $F$ can be contained in a quotient ring. I do know that $F_1$ is a field because $(g)$ is a maximal ideal in $F[x]$ so no need to explain this. Why is $\alpha$ a root of $g$? Perhaps because $\pi(g(x)) = g(\pi(x))$ with $\pi$ the surjection from $F[x]$ to $F_1$? This would only work in my eyes if $\pi$ 'fixes' constants but it doesn't to that or I don't understand why it should.","['abstract-algebra', 'roots', 'extension-field', 'field-theory']"
2831816,Can the gravitational force of a many-body system be calculated via matrix operations?,"I'm working on a software project at work that calculates the ""gravitational attraction"" between points in 1, 2, or 3 dimensions. This is an $O(n^2)$ runtime efficiency problem, however, if I can write it as a sequence of matrix operations, then I might be able to leverage the GPU to run my simulation which would be substantially faster. My linear algebra chops aren't that great however. Is it possible to represent the mass and position as a matrix and compute the net gravitational force for each element in the matrix as a vector? Edit: The problem I'm trying to solve is this: Given an array of positioned-masses, compute the net Gravitational force that each mass experiences from the other elements in the array. I can easily do this using for-loops, but if it can be done using matrices then I can leverage the GPU for faster calculation. Non-OP edit: given masses $m_{1..n}$, positions $\underline{r}_{1..n}$. Calculate the matrix $F$, for which $F_{ij}=m_im_j\frac{\underline{r}_i-\underline{r}_j}{|\underline{r}_i-\underline{r}_j|^3}$. The goal is to do this with ""simple"" vector/matrix operations.","['matrices', 'linear-algebra']"
2831840,Calculate bounding size to fit rectangle,"I hope I have phrased my question correctly. Let's assume I have a rectangle with a width and height of X and Y . Then I pick an aspect ratio of 0.56 . How can I calculate the size of the rectangle that bounds the original rectangle, without shrinking it? In other words, I want to keep the original rectangle as is, and ""place"" it inside a canvas that obeys the aspect ratio, I'm trying to calculate the canvas size. Any help would be highly appreciated. Best regards, Roi","['algebra-precalculus', 'discrete-mathematics', 'rectangles', 'geometry']"
2831848,Proving a matrix is invertible given equation (without identity matrix),"I'm given a square matrix ${A}$ (3×3) and the following equation ${A}^3-2017{A}^2 + {A} = {0}$ and I have to find if ${A}$ is invertible in some cases, no cases, or all cases. I can find ${A}=0$ as an answer for the non invertible case, but I can't seem to solve the equation. In most other examples I've found, there was an identity matrix, which made it easy to find the invertible of ${A}$ like this: ${A}*invertible=I$ but this is not the case here. I've tried doing this: $A*(A*(-A+2017*I))=A*I$, but I don't think I can divide both parts by ${A}$ because I haven't proven that ${A}$ is invertible.","['matrix-equations', 'linear-algebra']"
2831952,Complete reducibility for the Poincaré group,"If $(\rho,V)$ is a unitary representation of a group $G$ which is finite dimensional, then complete reducibility is kind of easy to prove. Indeed, if $V$ is not irreducible, then it has one proper invariant subspace $W$. The orthogonal complement of $W$, namely $W^\perp$ is then another proper invariant subspace so that $V$ decomposes as $$V = W\oplus W^\perp,$$ decomposing $\rho$ as $\rho|_W\oplus  \rho|_{W^{\perp}}$. Furthermore, since $V$ is finite dimensional and $W$ is a proper subspace, $\dim W,\dim W^\perp < \dim V$. We can repeat this process with $W,W^\perp$ and so on, until we get irreducible representations, and we are assured this will end because each step lowers the dimension. Now consider the Poincaré group $\mathbb{R}^4\rtimes SL(2,\mathbb{C})$. One looks for unitary representations of this group. It is then one theorem that there are no finite dimensional unitary representations. So they are all infinite dimensional. The procedure above doesn't work. Each step won't lower the dimension. Still, physicists seem to still try to somehow get complete reducibility here. What they do, which is actually quite cryptic really is: let $(U,\mathscr{H})$ be a unitary representation in the Hilbert space $\mathscr{H}$. Consider a pure translation, namely, $U(a,1)$. This is $U(a,1)=e^{-ia^\mu P_\mu}$ for hermitian operators $P_\mu$. Then they ""diagonalize"" these operators with the improper basis $\Psi_{p,\sigma}$ so that $$P_\mu \Psi_{p,\sigma}=p_\mu \Psi_{p,\sigma}.$$ Then one works on this basis, and notices that a pure $SL(2,\mathbb{C})$ transformation, $U(0,\Lambda)$ acting on $\Psi_{p,\sigma}$ is such that $$P_\mu U(0,\Lambda)\Psi_{p,\sigma}=\sum_{\sigma'}C_{\sigma\sigma'}(p,\Lambda)\Psi_{\Lambda p,\sigma'}.$$ From this, Weinberg, e.g., says: In general it may be possible by using suitable linear combinations of the $\Psi_{p,\sigma}$ to choose the $\sigma$ labels in such a way that the matrix $C_{\sigma'\sigma}(\Lambda,p)$ is block-diagonal; in other words, so that the $\Psi_{p,\sigma}$ with $\sigma$ within any one block by themselves furnish a representation of the inhomogeneous Lorentz group. It is natural to identify the states of a specific particle type with the components of a representation of the inhomogeneous Lorentz group which is irreducible, in the sense that it cannot be further decomposed in this way. So it seems that all this procedure I described above is some sort of ""complete reducibility under the hood"". It seems more like ""complete reducibility parametrized by $p$"" actually. But I confess I don't get the point. Actually I've read in this blog post that the matter is actually so subtle so a ""direct integral"" decomposition would be required. So what is going on here? How is complete reducibility actually dealt with for the Poincare group? What is this ""physicists procedure"" Weinberg shows and comments all about?","['mathematical-physics', 'functional-analysis', 'representation-theory', 'group-theory', 'lie-groups']"
2831976,Kiselev's Geometry Problem Problem 107,"Prove that if A and A' and B and B' are 2 pairs of points
  symmetric about a line XY, then the 4 points lie on the same circle. I thought about it for a long time, until I obtained this proof: Since A and A' and B and B' are symmetric about XY, A and A' are equidistant from XY and so are B and B'. Now, because XY is the locus of points equidistant from A, A' and B, B', there is bound to be at least one point, say R, which is equidistant from all points A, A', B, and B'. That is, AR = A'R = BR = B'R. $\therefore$, the points A, A', B, B' are the geometric locus of R, which is a circle. Thus A, A', B, B' are on the same circle. After writing this proof, I felt a bit uncomfortable, because something didn't seem quite right with the way that I wrote it. If anyone could give suggestions or point out if I was wrong, that would be greatly appreciated. Thanks.","['euclidean-geometry', 'geometry']"
2831981,Calculating a flux integral,"Let $$F=(xe^{xy}-2xz+2xy\cos^2 z, y^2\sin^2 z-y e^{xy}+y, x^2+y^2+z^2)$$ and $V$ be the solid in space bounded by $z=9-x^2-y^2$ and $z=0$. I am trying to compute the flux integral $\iint_{\partial V}F\cdot n \ dS$, $n$ being the outward unit normal. Setting $r(x,y)=(x,y,9-x^2-y^2)$, I found that $r_x\times r_v=(2x,2y,1)$ and $$\iint_{\partial V}F\cdot n \ dS=\iint_D F\cdot r_x\times r_y \ dA$$ where $$F\cdot r_x\times r_y=2e^{xy}(x^2-y^2)-36x^2+4x^4+4x^2y^2+4x^2y\cos^2(9-x^2-y^2)+2y^3\sin^2(9-x^2-y^2)+2y^2;$$ after the substitution $x=r\cos t, \ y=r\sin t$ I get $$f(r,t)=r(F\cdot r_x\times r)=2r^3e^{r^2\sin t \cos t}(\cos^2 t-\sin^2 t)-36r^3\cos^2 4r^5\cos^2 t + \\4r^4\cos^2 t\sin t \cos^2{(9-r^2)}+2r^4\sin^3 t \sin^2(9-r^2)+2r^3\sin^2 t $$ and I need to compute $$\int_0^{2\pi}\int_0^3 f(r,t)drdt$$ I wonder whether I can further simplify $f(r,t)$? The current expression looks to cumbersome and it seems like a hassle to compute the integral if no simplifications can be made.","['multivariable-calculus', 'real-analysis', 'integration', 'calculus']"
2831983,A Killing field on a compact Riemannian manifold $M$ of positive sectional curvature has a singularity,"This problem comes from do Carmo's book on page 104. I've almost got this thing worked out but am stuck at one point. The problem is long so I'll try to break it down. Assume $M$ is a compact Riemannian manifold of even dimension whose sectional curvature is positive. Show that every killing field $X$ on $M$ has a singularity, i.e. a point $p$ where $X_p = 0$. We have the following maps: $A_{X}: \mathfrak{X}(M) \rightarrow \mathfrak{X}(M)$ defined by $A_{X}(Z)=\nabla_{Z}X$ $f: M \rightarrow \mathbb{R}$ defined by  $f(q)=\| X \|_{q}^{2}=\langle X, X \rangle_{q}$. Let $p \in M$ be a critical point of $f$, i.e. where $df_{p}=0$. I proved the following lemma (exercise 2 from p. 104): Lemma: For any $Z \in \mathfrak{X}(M)$, we have at the critical point $p$ (i) $\langle A_X(Z),X \rangle_p = 0$ (ii) $ \langle A_X(Z),A_X(Z)\rangle_p = \frac{1}{2}Z_p(Z\langle X,X \rangle)+  R(X,Z,X,Z)(p)$ We will look at a point $p \in M$ where $f$ attains its minimum. Assume for contradiction that $X_p \neq 0$. The map $A_X$ defined above induces a map $A:T_pM \rightarrow T_pM$ by $A(y)=A_XY(p)=\nabla_Y X(p)$ where $Y$ is any extension of $y \in T_pM$. Let $E \subset T_pM$ be orthogonal to $X_p$. I want to show that the restriction $A: E \rightarrow E$ is an antisymmetric isomorphism. Note it follows from Lemma (i) that $A$ actually gives a map $E \rightarrow E$. I've already showed antisymmetric so I'm going to only show my work on isomorphism. It suffices to show that $A: E \rightarrow E$ is injective. If $A(y_1)=A(y_2)$ then $A(y_1-y_2)=0$, so let $Z=Y_1 -Y_2$ where $Y_1$, $Y_2$ are extensions of $y_1$ and $y_2$. Then, using Lemma (ii) we have $0=\langle(A(y_1-y_2),A(y_1-y_2) \rangle_p = \langle A_X(Z),A_X(Z) \rangle_p = \frac{1}{2}Z_p(Z\langle X,X\rangle) + R(X,Z,X,Z)(p)$ By the assumption on curvature, $R(X,Z,X,Z)(p)>0$, so if we can show $Z_p(Z\langle X, X \rangle) \geq 0$ then we will have a contradiction. This is where I'm stuck.","['riemannian-geometry', 'differential-geometry']"
2832005,Convolution of two PDF: $1/(2\sqrt{3x})$,"I'm trying to find the result of the sum of two PDF that is the square of two continuous uniform distribution (rectangular distribution) that have $\mu =0$ and $\sigma =1$. 
The square of this continuous uniform distribution is defined from 0 to 3 and is $$p(x)= \frac{1}{2 \sqrt{3x}}$$
Now i need to find the PDF of the sum of two random variables with p(x) as distribution.
The convolution integral is $$p(u)=\int_0^3 p(u-t)p(t)dt=\int_0^3 \frac{dt}{12 \sqrt{t(u-t)}}= \frac{arcsin \left( \sqrt{ \frac{3}{u}} \right)}{6}$$
This funcion is not defined on R from 0 to 6.
If i extract the real part of $(p(u))$ i obtain the correct PDF from 0 to 3 but the part from 3 to 6 is wrong and p(u) is not normalized from 0 to 6.
I simulate this PDF with python, here is an image Simulation of PDF: https://i.sstatic.net/0GRjF.jpg Plot of the PDF: https://i.sstatic.net/pvcQG.jpg what did I do wrong?","['statistics', 'probability', 'analysis']"
2832009,"Finding critical points of $f(x,y)= \sin x+\sin y + \cos(x+y)$","Find the critical points of function$$
f(x,y)=\sin x + \sin y + \cos(x+y),$$
  where $0<x<\dfrac{\pi}{2}$, $0<y<\dfrac{\pi}{2}$. What I have done:
$$f_{x}=\cos(x)-\sin(x+y),\\
f_{y}=\cos(y)-\sin(x+y).$$ From $f_{x}=0$, $\cos(x)=\sin(x+y)$. From $f_{x}=0$, $\cos(y)=\sin(x+y)$. I do not know where to go from here. My attemps:
$$\sin\left(\frac{\pi}{2}-x\right)=\sin(x+y)=\sin\left(\frac{\pi}{2}-y\right).$$",['multivariable-calculus']
2832033,2-dimensional vector bundles on projective 3-space,"Let $V,V'\subset T\mathbf{CP}^3$ be smooth (real) 2-dimensional subbundles of the tangent bundle of complex projective 3-space. Suppose that $V$ and $V'$ are isomorphic as topological vector bundles, i.e. there exists a continuous vector bundle isomorphism between $V$ and $V'$. Are $V$ and $V'$ necessarily isomorphic as smooth vector bundles (i.e. does there exist a smooth map $V\to V'$ which induces a linear isomorphism on the fibers)? I am aware of a classification of complex vector bundles over projective spaces, but couldn't find anything on real smooth vector bundles. I also think that the fact that both are subbundles of the tangent bundle should make this easy, but I don't know how to use that condition.","['vector-bundles', 'differential-geometry']"
2832100,Conditional mean squared error vs unconditional mean squared error,"Suppose you are trying to predict $Y$ from a set of predictors $X$. When you considered $E[Y-\hat{Y}]^2$ or $E[(Y-\hat{Y})^2|X]$, the minimizer is $E[Y|X]$. As $E[Y|X]$ is hard to compute we can rely on a linear predictor $\tilde{Y} = a + b[X-E[X]]$. To find the values of $a$ and $b$, we usually minimize $E[Y - \tilde{Y}]^2$ not $E[(Y-\tilde{Y})^2|X]$. I get that if we minimize the latter it results a function of $E[Y|X]$. Is it the only reason to minimize the former over latter in the case of linear predictors?","['statistics', 'conditional-expectation', 'linear-algebra']"
2832105,Being a local homeomorphism implies that stalks correspond to fibers. Is the converse true?,"Let $X, Y$ be topological spaces and $f : X \to Y$ be continuous. At any point $p \in Y$, we have the fiber $F = f^{-1}(p)$ over $p$ and the stalk $S$ whose elements are germs at $p$ of sections of $f$. There is an evident mapping $f' : S \to F$ which sends each germ to its value at $p$. If $f$ is a local homeomorphism, then each $f'$ is a bijection. Is the converse true?",['general-topology']
2832119,Limit of y(x) in Second Order Differential Equation,"So, even though I know how to solve ODEs, I don't know how I should proceed about this question: Let $a$, $b$, and $c$ positive constants. If $y = y(x)$ is solution to the differential equation $ay'' + by' + cy = 0$, then $\lim_{x\to\infty}$ $y(x)$: (a) doesn't exist and tends to $+\infty$. (b) exists and is $0$. (c) doesn't exist and tends to $-\infty$. (d) exists and is $\pi$. (e) exists and is $e$. I tried to take the limit of the possible solutions but even if $a$, $b$, and $c$ are positive-only numbers, there are many possibilities so I couldn't achieve anything. Thanks for the reading!","['ordinary-differential-equations', 'limits']"
2832123,How do we know that automorphisms on polynomials have a polynomial like form?,"Let $F$ be a field and $\sigma:F[x]\to F[x]$ be automorphism, $\sigma(a) = a$ for all $a\in F$. I'm supposed to show that $\sigma(f(x)) = f(ax+b)$ for some $a\not = 0$ and $b$ in $F$. Now I've got a solution that my professor gave me that seems to assume that the automorphism must have the form $\sigma(f(x)) = f(p(x))$ for some $p(x)\in F[x]$. So my question is how does $\sigma$ being an automorphism on $F[x]$ and $\sigma(a) = a$ for all $a\in F$ give us that $\sigma(f(x) = f(p(x))$ for some $p(x)\in F[x]$, why can't there be some weirder looking automorphism? I've looked at Automorphisms of $F[x]$ , however the only solution seems to make the same assumption that my professor makes.","['abstract-algebra', 'polynomial-rings']"
2832128,"""Successive extension of invertible $\mathcal{O}_S$-modules""","I am currently reading Takeshi Saito's book ""Fermat's Last Theorem: The Proof"". In the proof of Proposition 8.12, there is a part that I do not understand clearly. For my problem in particular, I am not sure that all the context is actually needed in order to understand what is going on. However, just to be sure, let me write down what we are doing. The statement is the following. Let $S$ be a scheme, let $E$ be a smooth curve over $S$, let $N\geq 1$ be an integer. Suppose $X$ is a closed subscheme of $E$ that is finite flat of finite presentation over $S$ of degree $N$. For sections $P_1,\ldots ,P_N:S\rightarrow X$, the following are equivalent: 1. $P_1,\ldots ,P_N$ form a full set of sections of $X$. 2. The following equality of effective Cartier divisors holds: $$X=\sum_{i=1}^N[P_i]$$ Actually, my problem lies in the easiest part of the proof, that is $2.\Rightarrow 1.$ Here is how the proof of the book goes. We may assume that $S=\operatorname{Spec}(R)$ is affine. For $i=1,\ldots ,N$, we let $\mathcal{I}_i$ be the defining sheaf of ideals of the effective Cartier divisor $[P_i]$ of $E$. By the equality of divisors $X=\sum_{i=1}^N[P_i]$, the finitely generated free $\mathcal{O}_S$-module $\mathcal{O}_X$ is a successive extension of the invertible $\mathcal{O}_S$-modules $\prod_{j=1}^{i-1}\mathcal{I}_j/\prod_{j=1}^{i}\mathcal{I}_j$. For any element $f\in \Gamma (X,\mathcal{O})$, the multiplication-by-$f$ map of $\mathcal{O}_X$ induces the multiplication-by-$f(P_i)$ map of $\prod_{j=1}^{i-1}\mathcal{I}_j/\prod_{j=1}^{i}\mathcal{I}_j$. Hence, we have $\operatorname{Norm}_{X/S}(f)=\prod_{i=1}^Nf(P_i)$. That is the proof. Now, the part of it that bugs me is the one I wrote in italic: what does ""a successive extension of $\mathcal{O}_S$-modules"" actually mean? I understand that $\mathcal{O}_X=\mathcal{O}_E/\prod_{j=1}^{N}\mathcal{I}_j$ (or, to be more exact, this is rather $i_{\star}\mathcal{O}_X$ where $i$ denotes the closed immersion of $X$ in $E$). What does ""successive extension"" then mean? Should we exploit the algebraic fact that if $L\subset M\subset N$ are modules, then $(N/L)/(M/L)=N/M$? If so, how does this writing allows us to make sense of the following argument about the multiplication by $f$ endomorphism? I thank you very much for your explanations.","['modules', 'algebraic-geometry', 'curves', 'schemes', 'proof-explanation']"
2832177,Proving Two Statements on Independence and Mutual Exclusion,"GIVEN $P(A)>0$ and $P(B)>0$. If $A$ and $B$ are independent, then they cannot be mutually exclusive. My proof: Let $A$ and $B$ be two independent sets such that $\mathbb{P}(A)>0$ and $\mathbb{P}(B)>0 $. 
$$\Rightarrow \mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)>0$$
Now suppose that $A$ and $B$ are mutually exclusive, then $$\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)=0$$
But this is a contradiction, as $\mathbb{P}(A)\mathbb{P}(B)>0$ by design.
Hence $A$ and $B$ cannot be mutually exclusive. If $A$ and $B$ are mutually exclusive, then they cannot be independent. My proof: Let $A$ and $B$ be two sets such that $\mathbb{P}(A)>0$ and $\mathbb{P}(B)>0 $. If $A$ and $B$ are mutually exclusive, then
 $$\mathbb{P}(A\cap B)=0$$
Now suppose $A$ and $B$ are independent, then
$$\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)=0$$
$$\Rightarrow \mathbb{P}(A)=0 \ \ \  \text{and/or} \ \ \ \mathbb{P}(B)=0$$
But this is a contradiction, as $\mathbb{P}(A)>0$ and $\mathbb{P}(B)>0$ by design. Hence $A$ and $B$ cannot be independent. Are these proofs correct? I have tried to make them simple, but are they too simple that they fail to prove each relevant statement?","['independence', 'statistics', 'probability', 'proof-verification']"
2832183,Is the set of upper(and separately lower)-triangular matrices a ring?,"I was reading lecture notes which mentioned the set of upper (and separately lower)-triangular matrices of a certain dimensionality is a group under matrix multiplication. That made me wonder if they also form a ring under addition and multiplication. So first, they are an abelian group under matrix addition: The sum of any number of triangular matrices is itself a
triangular matrix. The 0 matrix is the 0 element. There is an additive inverse. (Element-wise negation) Matrix addition is commutative. Then, they are a monoid under multiplication. The product of any number of triangular matrices is itself a triangular matrix. The identity matrix is the multiplicative identity. And finally, multiplication distributes over addition. Is that correct?","['matrices', 'abstract-algebra', 'ring-theory']"
2832211,Proof verification : Almost sure convergence implies convergence in probability,"PROOF VERIFICATION Show that almost sure convergence implies convergence in probability. We define
$$A:=\{ω \in Ω:X_n(ω) \to X(ω) \text{ as } n \to \infty\}.$$
By almost sure convergence, we have $P(A)=1$. Also define
$$A_n(ε):=\{ω \in Ω:|X_n(ω)-X(ω)|<ε\}.$$
Convergence in probability is equivalent to $\lim\limits_{n \to \infty} P(A_n(ε))=1$ for every $ε > 0$, which is what we want to show. Now, $ω \in A \implies \omega \in A_n(ε)$ for sufficiently large $n$ $[\geq N=N(ε)]$. Hence, $P(A) \leq P(A_n)$ for $n \geq N=N(ε)$. Thus,
$$\lim_{n \to \infty} P(A_n(ε)) \geq P(A)=1.$$ This gives the result since $ε>0$ is arbitrarily chosen. What is bothering me is that the proof seems surprisingly simple. Am I missing something? Thanks in advance.","['probability-theory', 'convergence-divergence', 'proof-verification']"
2832233,Taylor coefficients of $\exp⁡(1/(1-z))$ [duplicate],"This question already has an answer here : Coefficient growth in the power series $\sum u_n z^n = e^{1/(1-z)}$? (1 answer) Closed 5 years ago . I'm specifically interested in estimating the growth of the coefficients $(a_n)_{n\in\mathbb{N}}$ of the Taylor series of $$\exp⁡(1/(1-z))$$ centered in $0$: knowing that $\limsup_{n\rightarrow+\infty}|a_n|^{1/n}=1$, what I'm trying to figure out is whether the growth of $(a_n)_{n\in\mathbb{N}}$ is at most polynomial or super-polynomial... I tried to calculate explicitly the coefficients in order to find some regularity in the sequence of the coefficients, but I quickly got lost in the process. Thanks in advance for any answer or suggestion.","['taylor-expansion', 'combinatorics', 'complex-analysis']"
2832251,Solving system of three partial differential equations,"I have three differential equations: $$\dfrac{\partial H}{\partial y}-\dfrac{\partial G}{\partial z}=A$$ $$\dfrac{\partial F}{\partial z}-\dfrac{\partial H}{\partial x}=B$$ $$\dfrac{\partial G}{\partial x}-\dfrac{\partial F}{\partial y}=C$$ where $A$ , $B$ and $C$ are functions of $(x,y,z)$ How shall I write $F,G,H$ in terms of $A, B, C$ ? EDIT: There is the condition : $\nabla.(\nabla \times \vec{A})=0$ Now how shall I proceed to get a solution?","['partial-derivative', 'calculus', 'functions', 'multivariable-calculus', 'ordinary-differential-equations']"
2832253,Two generators of $F_2$ must be free?,"Suppose we are given $a, b \in F_2$, that happen to generate it. Then must they be free generators? That is, there is no non-trivial reduced word on $a^{\pm 1}, b^{\pm 1}$ defining the identity. Equivalently, $a, b$ satisfy the universal property defining $F_2$. I tried attacking this the first way: suppose $u$ and $v$ are free generators, then if I take a word on $a$ and $b$ I can rewrite it in terms of $u$ and $v$. But how do I know that if I took a non-trivial reduced word on $a, b$ defining the identity, it does not become trivial after reduction with $u$ and $v$? I also tried to use the universal property. Then we get that if $a$ and $b$ are not free, $F_2$ is isomorphic to one of its propert quotients. Maybe this is not the case for $F_2$ specifically, but it may be for other groups, so no contradiction in sight once again. I know this is true because it is used in a class I am taking. To prove that $SL_2(\mathbb{Z})$ is not free, the professor just exhibited two generators and a non-trivial relation.","['abstract-algebra', 'group-theory', 'free-groups']"
2832282,How to get rid of the absolute value function when solving an ODE?,"Problem: $y'+yx=0, \quad y(0)=-1$ We separate it: $\frac{dy}{dx}=-yx \Rightarrow \int\frac{-1}{y}dy=\int x dx = \frac{1}{2}x^2 + C_1$ With $\int\frac{-1}{y}dy=\log(\frac{1}{|y|})+C_2$ we get $\log(\frac{1}{|y|})=\frac{1}{2}x^2 + C$ We solve for $y$: $|y|=e^{-\frac{1}{2}x^2}\cdot e^C=e^{-\frac{1}{2}x^2}\cdot \hat{C}$ Now, what is the best argumentation to get ""rid"" of $|\cdot |$? Like I know that e.g. $|a|=b \Leftrightarrow a=\pm b$ but then I still have $\pm$. I ""know"" that some wil ltell me that I can ""put it into $C$"" but that not really an arugmentation. Maybe I just don't get how $C$ can determine if we have the positive or negative solution or why we can't have both at the same time.  It just feels like I lack proper understanding to properly argument here.",['ordinary-differential-equations']
2832287,"Is there a continuous a.e. bijective function from $[0,1]$ to $[0,1]^2$?","We know that there does not exist a continuous bijective function from $[0,1]$ to $[0,1]^2$. (More generally, $[0,1]$ is not homeomorphic to $[0,1]^2$.) However, is there a continuous a.e. (almost everywhere) bijective function from $[0,1]$ to $[0,1]^2$? The motivation for asking this question comes from the following: 1) $([0,1],B_{[0,1]})$ is borel isomorphic to $([0,1]^2,B_{[0,1]^2})$, i.e., there exists a measurable bijective function $f$ from $[0,1]$ to $[0,1]^2$, and its inverse $f^{-1}$ is also measurable. 2) By Lusin's theorem, we have that for every $\varepsilon > 0$, there exists a compact $E ⊂ [0, 1]$ such that $f$ restricted to $E$ is continuous almost everywhere and $\mu (E)>1-\varepsilon $, where $\mu$ is the Lebesgue measure. So the answer for my question seems to be positive, at least for the case with $[0,1]$ and $[0,1]^2$ replaced by some 1-D space $E$ and 2-D space $E'$. Is there anybody can prove it or disprove it?","['real-analysis', 'measure-theory', 'probability-theory']"
2832302,Understanding a definition of convergence,"I have been trying to understand the definition of convergence, but one small detail in the definition is bother me. The detail is '$n \geq N$' So the definition I am using is $(a_{n}) \rightarrow a$ if for every $\epsilon>0 , \exists N \in \mathbb{N}$ such that whenever $n \geq N$ it follows that $|a_{n}-a|< \epsilon$ So in my view it seems like a sequence converges if you can pick a point in the sequence s.t. after this point in the sequence the sequence will KEEP staying inside the epsilon neighborhood. Just to stay concrete suppose $a_{n}=\frac{1}{n}$ then when I look at ''$\exists N \in \mathbb{N}$ s.t. whenever $n \geq N$'' I get confused. If I pick $\epsilon = \frac {1}{10}$ Then the sequence is inside the epsilon neighborhood whenever $n>10$ and here my question is what does $n \geq N$ means? does it mean that this $N$ is the index ? so we have that $a_{3}(n) \geq a_{2}(N)$ or is the meaning that the output you get when you plugin the number into the sequence so we have $a_{2}(n) \geq a_{100}(N)$? because $1/2 \geq 1/100$?","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2832366,Bayes decision theory - step in derivation,"I am self studying Bayes Decision theory from these lecture notes page 30 / 31 and there is a step a struggle to understand mathematically Background context Given Bayes risk defined as: $$ r_B(\pi, \hat \theta) = \int_{\Theta} R(\theta, \hat \theta) \ \pi(\theta) \ d \theta$$ Prior distribution $\theta \sim \pi(\theta)$ Distribution of data given $\theta$ $z | \theta \sim f(z | \theta)$ Marginal distribution of $z$ : $m(z) = \int f(z | \theta) \pi (\theta) \ d\theta$ Posterior distribution of $\theta$ $\pi(\theta | z) = \frac{f(z | \theta) \pi(\theta)}{m(z)}$ Frequentist risk $R$ : $R(\theta, \hat \theta) = E[L(\theta, \hat \theta)| \theta]$ We can express Bayes Risk in terms of the posterior risk, a function of $z$ $$ r_B(\pi, \hat \theta) = \int_{\mathcal{Z}}  r(\hat \theta | z) m(z) dz$$ Which means that the Bayes decision rule can be obtained as \begin{aligned}
	r_b(\pi, \hat \theta_\pi^{Bayes}) &= \inf_{\hat \theta} r_b(\pi, \hat \theta)
	\\
	&= \inf_{\hat \theta} \int_{\mathcal{Z}}  r(\hat \theta | z) m(z) dz
\end{aligned} Step I don't grasp The above theorem implies that the Bayes rule can be obtained by taking the Bayes action for each particular $z$. For each fixed $z$ we choose $\hat \theta(z)$ to minimize the posterior risk $r(\hat \theta | z)$. $$ \arg \min_{\hat \theta} \ r(\hat \theta | z) =  \int L(\theta, \hat \theta(z)) \pi(\theta | z) \ d\theta $$ This guarantees us to minimize the integrand at every $z$ and hence minimize Bayes risk. How do we get from the inf to the argmin and why does one minimize the other ? I feel this is a stupid question but I am missing something... thanks !","['self-learning', 'bayes-theorem', 'decision-theory', 'bayesian', 'statistics']"
2832425,Function value in set theory,"I found the following exercise in a set theory book. Let $f(x)$ denote $\bigcup \left\{y : \langle x,y\rangle \in f\right\}$. Prove that for any function $f$ and $x\in  \mathsf{dom}(f)$  we have:
\begin{equation}
\langle x,y\rangle \in  f\leftrightarrow  y = f(x)
\end{equation} As I undertand it, being $f$ a function, there is a single $y$ such that $\langle x,y\rangle \in f$, hence  the union is $\left\{z : z=y \right\}$, abbreviated by $\left\{y \right\}$. What I get is  therefore   $\left\{y \right\}= f(x)$ rather then $y = f(x)$.",['elementary-set-theory']
2832466,"A twice differentiable function $f$ satisfies $f′′(x)+f(x)=−xg(x)f′(x)$, $\forall x\ge 0$.","Consider $f\in C^2$ so that $$f''(x)+f(x)=-x\,g(x)f'(x), \ \forall x\ge0 $$ where $g(x)\ge 0$. Then ($\forall x\ge 0$) (A) $f(x)^2+f'(x)^2$ is non-increasing, (B) $f(x)^2<3f(0)^2+(2f'(0))^2$, (C) $|f(x)|\le\alpha$, where $\alpha$ is a fixed real constant. (D) $\lim_{x\to\infty} f(x)\sin\left(\dfrac{1}{x}\right)$ exists. original task description Answer Given: (A),(B),(C),(D) What I have tried : I differentiated option (A) and then with the help of the given equation in the question I was able to show that  it is less than zero . Hence I was able to infer that option (A) is correct . Now coming to option (B). 
I assumed the function in option (A) to be $h(x)$. $$ h(x) = f(x)^2 + f'(x)^2 $$ Since $h'(x)<0$ ,therefore $h(x)$ is a decreasing function hence for all $x\ge 0$ $h(x)< h(0)$ which implies $(f(x))^2 + (f'(x))^2<(f(0))^2 + (f'(0))^2$ From the above expression we can get the second option . Is my method correct ?. If not , please show the right method . I am unable to get the the 3rd and 4th option .","['derivatives', 'ordinary-differential-equations', 'calculus']"
2832467,Pullback of $\mathcal{O}(1)$ via the degree $n$ map $\mathbb P^1 \to \mathbb P^1$,"Let $k$ be a field and consider the degree $n$ map $f: \mathbb P^1_k \to \mathbb P^1_k$ given by $(x:y) \mapsto (x^n: y^n)$.  I want to show that $f^*(\mathcal O_{\mathbb P^1_k}(1)) \simeq \mathcal O_{\mathbb P^1_k}(n)$. My attempt is as follows:  by definition, we have to calculate 
$$f^*(\mathcal O_{\mathbb P^1_k}(1)) = f^{-1}(\mathcal O_{\mathbb P^1_k}(1)) \otimes_{f^{-1}(\mathcal O_{\mathbb P^1_k})} \mathcal O_{\mathbb P^1_k},$$ so we have to describe the sections of this sheaf over every affine open set $U = {\rm Spec}(R)$ and show that they agree with $\mathcal O_{\mathbb P^1_k}(n)({\rm Spec}(R))$.  How to describe these sections?  I guess the sections of $f^{-1}(\mathcal O_{\mathbb P^1_k}(1))({\rm Spec}(R)))$ given by rational functions of the form $g(x^n,y^n)/h(x^n,y^n) \in R(x,y)$ where $g(x,y)$ and $h(x,y)$ are homogeneous polynomials with $deg(g) - deg(h) = 1$; is this correct?  How to get this?  If this is true, then I guess we can compute the tensor product in the display above to get that $f^*(\mathcal O_{\mathbb P^1_k}(1))({\rm Spec}(R))$ consists of rational functions of the form $$\frac{g(x^n,y^n)}{h(x^n,y^n)} \cdot \frac{p(x,y)}{q(x,y)},$$ where $p$ and $q$ are homogeneous of the same degree.  But why are these all the rational functions $\frac{r(x,y)}{s(x,y)}$, where $r$ and $s$ are homogeneous with $deg(r)-deg(s)=n$?","['schemes', 'algebraic-geometry', 'pullback']"
2832575,Spherical coordinates when the ball is not centered in the origin,"I need to calculate $$\iiint _V\sqrt{x^2+y^2+z^2} \,dx \,dy\, dz$$ where $V$ is the ball $$x^2+y^2+z^2 \leq 4z \Leftrightarrow x^2 + y^2+(z-2)^2 \leq 4$$ The hint is to use origin centered spherical coordinates. So, after substitution I get:
$$
r \leq 2 \cos \phi.
$$
This obviously implies that $0\leq r \leq 2\cos \phi$. But as far as I can understand, it also implies that $\phi$, which is always bounded in $\left[ 0,\pi \right] $, now satisfies $0\leq \phi \leq \frac{\pi}{2}$. Is it true that indeed $\theta$, which has no constraints on it, will satisfy $0\leq \theta \leq 2\pi$? Is it true that the final integral is
$$
\int_0^{2\pi}d\theta \int_0^{\frac{\pi}{2}} \int_0^{2\cos \phi} r^3\sin\phi \,dr\,d\phi ?
$$ Thanks a lot!","['multivariable-calculus', 'integration']"
2832587,Roots of polynomial over finite field form a group,"I was playing around with some polynomials over finite fields (specifically $\mathbb{F}_p^*$ where $p$ is prime), and I was wondering if there is in general a condition for the roots of a polynomial to form a group. As an example, the polynomial that prompted the question was $x^3 - 1$. Playing around with the roots in different fields, I noticed that the set of solutions is always either $\{1\}$ or $\{1,k,k^{-1}\}$, which are obviously subgroups of the finite field we work in. It's easy to show that this is always true. Next came the generalization to polynomials of the form $x^q-1$ for any $q\in \mathbb{N}$, and it again it's fairly straightforward to show that their roots form a group. However after investigating some more, it looks like I got lucky with those polynomials, as other ones I've looked don't have that nice structure in their roots. The base condition that already limits our choice quite a bit is that $1$ must be root, but I haven't found any more restrictions. What are some more examples of such polynomials and where could I learn some more about them?","['finite-fields', 'polynomials', 'group-theory']"
2832681,Donsker's Theorem in Higher Dimensions,"Donsker's theorem states that a random walk $X_n = \xi_1+...\xi_n$ with step size of mean 0 and variance 1, after rescaling by a factor of $\sqrt{n}$, converges to a Brownian motion weakly:
$$
\left(\frac{X_{nt}}{\sqrt{n}}\right)_{t \geq 0} \overset{d}{\to} (B_t)_{t \geq 0}.
$$ Is there an analog for Brownian motions in $\mathbb{R}^d$, $d \geq 2$?","['stochastic-processes', 'probability-limit-theorems', 'probability-theory', 'soft-question']"
2832699,Tate module of an elliptic curve.,"In the case of an elliptic curve $E$ defined over a field $K$, I know that there is a good definition of the so-called Tate module, for every prime $p$, which is the $\mathbb{Z}_p$-module $T_p(E)=\underset{\underset{n}{\longleftarrow}}{\lim}E[p^n](K^s)$, where $E[p^n]$ is the subscheme of $E$ representing its $p$-torsion, and $K^s$ is the separable closure of $K$. I know that this object satisfies very good properties, e.g. it realizes a representation of the Galois Group of $K$. I also know that, under suitable hypothesis, $T_p(E)$ can be used to describe all the possible isogenies between $E$ and itself (or another elliptic curve). Now, I learned via Katz-Mazur book on Arithmetic moduli of Elliptic Curves, that it is possible to define an elliptic curve over arbitrary base rings, and even schemes. I was wondering, but I didn't find any good reference, wether or not it is possible to define a kind of global Tate module. In fact, consider $R$ a ring, and $E/R$ an elliptic curve, i.e. a proper, smooth scheme over $R$ equipped with a zero section, and whose fibers are elliptic curves in the usual sense. Then it is proved in Katz-Mazur that this object again comes equipped with a Group structure, so, is it is meaningful to consider the $p^n$-torsion for varying $n$. Is it possible to define a kind of relative version of the Tate module? My guess is to define it as a functor $T_p(E)$, which sends any $R$-algebra $A$ to $T_p(E)(A)=\underset{\underset{n}{\longleftarrow}}{\lim}E[p^n](A)$. Clearly this defines a sheaf over the category of $R$-Algebras, being an inverse limit of sheaves. Is it representable? And, is there any beautiful and known result about it? For example situations under which it classifies isogenies of elliptic curves over $R$? Thanks a lot for any suggestion or reference!","['schemes', 'arithmetic-geometry', 'elliptic-curves', 'algebraic-geometry']"
2832727,"In ""between-class"" scatter matrix, what does ""overall mean"" $m$ refer to? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question In ""between-class"" scatter matrix, what does ""overall mean"" $m$ refer to? Because the formulation is $$\sum_{i=1}^c N_i (m_i - m) (m_i - m)^T$$ where $N_i$s is the size of $i$th sample (each column is a sample), $m_i$ is the sample mean of $i$th sample. But what does $m$, the overall mean, refer to?","['means', 'machine-learning', 'statistics']"
2832758,"Prove that $1+x+y$ not equal to $xy$, when $x$ and $y$ are positive odd numbers","I was engaging in this equation and with the help of desmos.com I noticed it is not possible. So how can we prove? In which way it is easier, should I leave $x$ or $y$ alone, or is there any method different?",['algebra-precalculus']
2832794,Does the following object have such property?,"Let $S_n$ denote the set of bijections on the set $M = \{1, 2, ... , n\}$. Suppose that a set $\Omega \subset S_n$ satisfies the following condition: there is $k \leq n$ such that, for each nonempty subset $A \subset M$ with $|A| \leq k$ and each $a \in A$ there exist exactly $\frac{|\Omega|}{|A|}$ permutations $\pi \in \Omega$ for which $\min_{x \in A} \pi (x) = \pi (a)$. Is it true, that for every nonempty subset $A \subset M$ with $|A| \leq k$ and every $m \in \{1, 2, ... |A|\}$ there exist exactly $\frac{|\Omega|}{|A|}$ permutations $\pi \in \Omega$ such that $\pi (a)$ is the $m$-th largest element of $\pi (A)$? For small $n$-s the statement seems to be true. However, I have no idea how to prove it in general. Any help will be appreciated.","['group-actions', 'order-theory', 'permutations', 'combinatorics', 'group-theory']"
2832820,"Geometric Brownian motion, product ansatz rationale","My question is why does the subsequent product ansatz for the geometric Brownian motion work? Suppose we have the gBm
$$dS_t=\mu S_tdt+\sigma S_tdB_t,\ S(0)=S_0$$
We assume the solution is given by the product $V_t\cdot U_t$ where $V_t$ is the solution of the ODE $$dV_t=\mu V_tdt,\ V(0)=S_0$$ given by $$V_t=S_0e^{\mu t}$$ and $U_t$ is the solution of the SDE $$dUt=\sigma U_tdB_t,\ U(0)=1$$ given by (Ito's lemma) $$U_t=e^{-\tfrac{1}{2}\sigma^2t+bB_t}.$$
Inserting them in the product ansatz we indeed get the solution $$S_t=V_tU_t=S_0e^{(a-\tfrac{1}{2}\sigma^2)t+bB_t}$$
of the gBm. My background in differential equations is limited, therefore I would appreciate some information regarding the rationale, which makes this work.","['stochastic-processes', 'brownian-motion', 'ordinary-differential-equations', 'stochastic-calculus', 'stochastic-differential-equations']"
2832826,Does identifying opposite points in Euclidean space result in a smooth manifold?,"Taking Euclidean space $\mathbb{R}^n$ and identifying all pairs of points $\{\mathbf{x}, -\mathbf{x}\}$ results in a topological quotient space $\mathbb{R}^n/\mathbb{Z}_2$. Is this quotient space a smooth (Riemannian, etc.) manifold? If so, does that manifold have a simpler or more standard equivalent description? What is its algebraic topology? Intuitively, it seems to me that the quotient space should be a smooth manifold, except possibly at the origin. (If we'd started off with the sphere $S^n$ instead of Euclidean space $\mathbb{R}^n$, then we would of course get the real projective space smooth manifold $\mathbf{RP}^n$.) A few further thoughts: if we foliate $\mathbb{R}^n$ into concentric spheres, then we see that this topological space consists of a bunch of nested copies of $\mathbf{RP}^n$. So away from the origin it should locally look like $\mathbf{RP}^n \times \mathbb{R}$ and so should be a smooth manifold, but I'm not sure what happens at the origin. Another way to approach the problem is to distinguish one Cartesian coordinate (WLOG let it be the first one) and think of $\mathbf{x} \in \mathbb{R}^n$ as a direct sum $(x^1, \mathbf{y}) \in \mathbb{R} \oplus \mathbb{R}^{n-1}$. We can then forget about the half-space $x^1 < 0$ since it's identified with the other half-space (with the appropriate inversion of the orthogonal subspace). If we denote the quotient space $\mathbb{R}^n / \mathbb{Z}_2$ by $Y_n$, then the identification reduces the boundary hyperplane $(x^1 = 0) \cong \mathbb{R}^{n-1}$ to a lower-dimensional version of the same problem, so we can recursively describe $Y_n$ as an $n$-dimensional Euclidean half-space bounded by $Y_{n-1}$. I suspect there's a simpler way to think about it though. P.S. I'm just a dumb physicist without much background in advanced math. I'd appreciate any visual intuition for the $n = 2$ and $n = 3$ cases.","['general-topology', 'smooth-manifolds', 'projective-geometry', 'quotient-spaces']"
2832827,Have I found the Jordan form correctly?,I am given that the minimal polynomial and characteristic polynomial of a matrix are both $(x-1)^2(x+1)^2$. I have found the Jordan form to be $$\begin{bmatrix}1&1&0&0\\0&1&0&0\\0&0&-1&1\\0&0&0&-1\end{bmatrix}.$$ Is this correct or have I made a mistake somewhere?,"['matrices', 'jordan-normal-form', 'linear-algebra', 'minimal-polynomials']"
2832839,Why don't terms cancel in this application of Bayes' Rule?,"I have been looking at several different sites trying to wrap my head around the content of Sebastian Thrun's Intro AI course on Udacity. At the moment I'm trying to understand the lesson on ""explaining away"" in the case of two confounding causes. I found this SE Mathematics article but the top answer just left me with more questions. Let 
S = It is sunny
R = I got a raise
H = I am happy The first step in the article linked above is to go from P(R|H,S) to P(R,H,S) / P(H,S) I understand how to make that step. What I don't understand is why the denominator doesn't cancel out the P(S) and the P(H) in the numerator. I thought that P(R,H,S) was the same as P(R)P(H)P(S), so dividing that by P(H)P(S) would leave you with P(R). I've always struggled with math, so I'm sure I'm just missing something dumb.","['bayes-theorem', 'statistics', 'probability']"
2832848,Bounds for the Harmonic k-th partial sum.,"I need to bound the k-th partial sum or the Harmonic series. i.e. $$ln(k+1)<\sum_{m=1}^{k}\frac{1}{m}<1+ln(k)$$ I'm triying to integrate in $[m,m+1]$ in the relation $\frac{1}{m+1}<\frac{1}{x}<\frac{1}{m}$ for all $x\in[m,m+1]$ and I get:
$$\int_{m}^{m+1}\frac{1}{m+1}dx<ln(m+1)-ln(m)<\int_{m}^{m+1}\frac{1}{m}dx$$
then $$\sum_{m=1}^{k}\frac{1}{m+1}<ln(k)<\sum_{m=1}^{k}\frac{1}{m}$$ but I don know how conclude or continue... please help.","['divergent-series', 'sequences-and-series', 'bounds-of-integration']"
2832856,Calculate modulo solution of a number with a high exponent [duplicate],"This question already has answers here : Modular exponentiation by hand ($a^b\bmod c$) (12 answers) Closed 6 years ago . I need some help calculating the solution for the following equation: $ 4^{217} = x \text{ (mod 391)} $ Using power rules ($4^{217} = 2^{434}$) and Eulers theorem ($\phi(391) = 352$) I was able to reduce the term to: $ 2^{82} = x \text{ (mod 391)} $ This is where my search for a solution comes to a halt. I have been able to further reduce this term to $ (3^6 * (31^2)^3 * 2^4) = x \text { mod (391)} $ but that is where I got stuck, because I feel like there should be a far more elegant way than the brute force attempt I took from $2^{82}$. What other tricks can I use to find the correct solution for this equation?","['cryptography', 'modular-arithmetic', 'totient-function', 'discrete-mathematics']"
2832865,Solution to a 2nd order ODE with a Gaussian coefficient,"I am trying to find the solutions to this differential equation: \begin{align}
\frac{d^2y}{dx^2}+a e^{-x^2}y=0\ ,
\end{align}
where $a\in\Re$. I know that equations of the form
\begin{align}
\frac{d^2y}{dx^2}-\left(f(x)^2+\frac{df}{dx}\right)y=0
\end{align}
have the solution
\begin{align}
y(x)=\exp\left(\int f(x)dx\right)\ .
\end{align} Thus, to solve my first equation, I need to solve
\begin{align}
f(x)^2+\frac{df}{dx}=-ae^{-x^2}\ ,
\end{align}
which is a type of Riccati equation. I know that the homogenous part of this equation gives me a Bernoulli equation with solution \begin{align}
y(x)=\frac{1}{x+c_1}
\end{align} where $c_1$ is a constant. However, I am now stuck with finding the particular solution. I am not sure of the best method to solve for it. I also tried solving it with both Maple and Mathematica, but they were unable to do so. I found the book, Handbook of Exact Solutions for Ordinary Differential Equations , but their equations contain exponential functions (Sections 1.2 and 2.1.3) include $e^{-x}$, rather than a Gaussian.","['homogeneous-equation', 'ordinary-differential-equations']"
2832866,trigonometry finding 2 possible $\cos$ values of an angle,"In a triangle $ABC$, $AB = 10$ cm and $AC= 5$ cm. The area is $15$ cm${}^2$ and the angle $BAC$ is equal to $\theta$. Give two possible values of $\cos(\theta)$. I was able to find one of the values of $\cos(\theta)$ which is 0.8 but how can I get a second value for $\cos(\theta)$?","['problem-solving', 'trigonometry']"
2832886,"For an equilateral triangle with $n$ dots on a side, how many lines are needed to connect each dot to every other dot?","For an equilateral triangle of side length $n$ dots, as shown in this diagram below, construct a
  function, $f(n)$, which outputs the number of lines needed to connect up every dot to every other dot. A straight line through three or more dots counts
  as only one line! E.g. $f(3) = 9$ and $f(4) = 24$. Can anyone point me in the right direction with this problem? Perhaps tell me what area of mathematics or what concepts would help me solve this? If this is a trivial problem don't give the answer but tell let me know. Thanks.","['functions', 'geometry']"
