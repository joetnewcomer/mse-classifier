question_id,title,body,tags
3394373,Permutation of coordinates: how many linearly independent vectors will this generate?,"Let $V$ be a vector space and $\dim V=n$ . Under a basis, the vector $\mathbf v$ is represented by the coordinate $(a_1,a_2,\ldots, a_n)$ . Let $S_n$ be the group of all permutations on the set $\{1,2\ldots, n\}$ represented by matrices. $S_n\subseteq M_n(\mathbb R)$ . Let's form the set $$
P_\mathbf v=S_n\mathbf v=\{M\mathbf v:M\in S_n\}.
$$ I try to investigate the dimension of $\text{span }P_\mathbf v $ . It appears to me that a maximal subset of independent vectors in $P_\mathbf v$ can either be very large ( $n$ vectors) or very small (one vector), but not something in between. What are the possible dimensions of $\text{span }P_\mathbf v $ ?","['permutations', 'group-theory', 'linear-algebra']"
3394389,An injective $C^*$-algebra is generated by its projections,"Let $A$ be a unital, injective $C^*$ -algebra. Recall that $A$ is called injective if whenever $S\subseteq T$ is an embedding of operator systems, and $\phi: S\to A$ is a unital completely positive map, then $\phi$ extends to a unital completely positive map $T\to A$ . In the paper ""Topological boundaries of unitary representations"" ,in the proof of Proposition 3.16, the authors use the fact that injective $C^*$ -algebras are generated by their projections.
I could not find a reference for this argument (injective $C^*$ -algebras are monotone complete, and therefore also AW* algebras, but I don't know wether it helps). Thank you for any help.","['von-neumann-algebras', 'c-star-algebras', 'functional-analysis', 'operator-algebras']"
3394447,"Function as a ""constant of integration""","I'm reading a book Differential Equations with Applications and Historical Notes , 3rd edition, specifically section 8 about exact equations. The author is trying to prove that iff $\partial M/\partial y = \partial N/\partial x$ then equation \begin{equation}
M(x,y)dx + N(x,y)dy = 0
\end{equation} is exact differential equation. At some point we integrate equation \begin{equation}
\frac{\partial f(x,y)}{\partial x} = M(x,y)
\end{equation} to get \begin{equation}
f(x, y) = \int M(x,y)dx + g(y)
\end{equation} The author states that function $g(y)$ appears as a constant of integration because if we take derivative of both sides with respect to $x$ , $g(y)$ would disappear because it doesn't depend on $x$ . That's the part that I have trouble with, $y$ is a dependent variable and $x$ is independent variable so wouldn't derivative of $g(y)$ with respect to $x$ be \begin{equation}
\frac{d\,g(y)}{dy} \frac{dy}{dx}
\end{equation} and not $0$ ?","['calculus', 'ordinary-differential-equations']"
3394470,Domain/Points of differentiability,"How would I find the domain or points of a function where it is differentiable? Let's say I had the function $f: \mathbb R \rightarrow \mathbb R$ defined by $f(x) = |x|$ . I know that a function $f: \mathbb R \rightarrow \mathbb R$ differentiable at $x_0 \in \mathbb R$ means that for all $\epsilon > 0$ , there exists a $\delta > 0$ such that for some $L \in \mathbb R$ , $|\frac{f(x) - f(x_0)}{x - x_0} - L| < \epsilon$ for all $x \in \mathbb R$ with $|x - x_0| < \delta$ . However, I'm not sure how to find all possible values of $x_0$ from this alone, so am I supposed to be doing this a different way? I'm confused. All responses are much appreciated.","['functions', 'derivatives', 'real-analysis']"
3394515,Computing the curvature of a connection,"I'm trying to compute the curvature of a connection $\nabla$ on a 2-dimensional real vector bundle $E$ over a 2-dimensional manifold $M$ . I'm using the definition $$F_{\nabla}(X,Y,s)= \nabla_X\nabla_Yf-\nabla_Y\nabla_Xf -\nabla_{[X,Y]}f$$ where $X,Y$ are vector field over $M$ and $s$ is a section of $E$ . I know that $F_{\nabla}$ is $C^{\infty}(M)$ -linear in all components, hence my idea is to compute the curvature just for vector field of the form $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}$ , since they locally give a basis for the $C^{\infty}$ -module of vector fields. Now, I'm not really familiar with Lie derivative, so I'm thinking of $[X,Y]$ just as an operator, but I think $[\frac{\partial}{\partial x},\frac{\partial}{\partial x}]$ , $[\frac{\partial}{\partial x},\frac{\partial}{\partial y}]$ , $[\frac{\partial}{\partial y},\frac{\partial}{\partial x}]$ and $[\frac{\partial}{\partial y},\frac{\partial}{\partial y}]$ should all be zero. It follows that in my computation $\nabla_{[X,Y]}f$ always vanishes. Is it possible? Am I allowed to work locally like that?","['connections', 'vector-bundles', 'curvature', 'differential-geometry']"
3394523,Finding the resonance of a second order differential equation,$$y'' + 100y = \frac{1}{3} \cos(bx) $$ with $$ y(0) = 0$$ and $$ y'(0) = 0.1$$ I believe the homogenous (general) solution is $$\frac{1}{100} \sin(10x)$$ however I am having trouble finding the inhomogeneous (particular) solution as well as the values of b at which resonance will occur (I think these two are intertwined).,['multivariable-calculus']
3394571,Suppose $R$ and $S$ are transitive relations. Prove that if $S \circ R \subseteq R \circ S$ then $R \circ S$ is transitive.,"Proposition. Suppose $R$ and $S$ are transitive relations on $A$ . Prove that if $S \circ R \subseteq R \circ S$ then $R \circ S$ is transitive. My attempt: Suppose $S \circ R \subseteq R \circ S$ . Suppose $(x,y) \in R \circ S$ and $(y,z) \in R \circ S$ Then there exists $a$ such that $(x,a) \in S$ and $(a,y) \in R$ And there exists $b$ such that $(y,b) \in S$ and $(b,z) \in R$ Since $(a,y) \in R$ and $(y,b) \in S$ , we conclude that $(a,b) \in S \circ R$ . Since $S \circ R \subseteq R \circ S$ , we have $(a,b) \in R \circ S$ Since $(a,b) \in R \circ S$ , there exists some $c$ such that $(a,c) \in S$ and $(c,b) \in R$ . We know that $S$ is transitive. Since $(x,a) \in S$ and $(a,c) \in S$ , we conclude that $(x,c) \in S.$ We know that $R$ is transitive. Since $(c,b) \in R$ and $(b,z) \in R$ , we conclude that $(c,z) \in R.$ Since $(x,c) \in S$ and $(c,z) \in R$ , we conclude that $(x,z) \in R \circ S$ . Arbitrary elements were considered, hence $R \circ S$ is transitive. $\Box$ Is it correct?","['elementary-set-theory', 'proof-verification', 'relations']"
3394636,Calculate $\lim_{x\to \pi/4}\cot(x)^{\cot(4*x)}$ without L'Hôpital's rule,"How can I calculate limit $$\lim_{x\to \pi/4}\cot(x)^{\cot(4*x)}$$ without using L'Hôpital's rule? What I have tried so far: I tried to use the fact that $\lim_{\alpha\to 0}(1 + \alpha)^{1/\alpha} = e$ and do the following: $$\lim_{x\to \pi/4}\cot(x)^{\cot(4 \cdot x)} = \lim_{x\to \pi/4}(1 + (\cot(x) - 1))^{\cot(4 \cdot x)} = \lim_{x\to \pi/4}(1 + (\cot(x) - 1))^{\frac{1} {\cot(x) - 1} \cdot (\cot(x) - 1) \cdot \cot(4 \cdot x)} = \lim_{x\to \pi/4}e^{(\cot(x) - 1) \cdot \cot(4 \cdot x)} = e^{\lim_{x\to \pi/4}{(\cot(x) - 1) \cdot \cot(4 \cdot x)}} $$ But I have problems calculating limit $$\lim_{x\to \pi/4}{(\cot(x) - 1) \cdot \cot(4 \cdot x)}$$ I tried to turn $\cot(x)$ into $\frac{\cos(x)}{\sin(x)}$ as well as turning it into $\tan(x)$ , but I do not see any workaround afterwards. I would appreciate any pieces of advice. Thank you!","['limits', 'trigonometry', 'limits-without-lhopital']"
3394729,Why does the reduction of the total cards (pool) make no difference to the probability that I will get a certain card in my hand,"Some friends and I had a heated debate about this and I am still quite confused. The problem I will use an example of 8 cards in a bag, Each card has a letter on it, e.g. A,B,C,D,E,F,G,H. You draw 3 cards . What are the chances of getting a specific card (e.g. B) in that final hand (i.e. what are the chances that one of the cards you now have is the one you wanted)? My logic: I thought surely its 1/8 then 1/7, then 1/6 as each time a card is taken there are less cards? This would be 1/8 + 1/7 + 1/6 = 43.5% chance. Someone pointed out to me that on the second, and third times your card may already be chosen, but in my head this should not be a reason your chances get worse... but i'm sure that's the key thing i don't understand. They said it actually remains a 1/8 each time so 1/8 + 1/8 + 1/8 = 3/8 = 37.5% Which is correct (see simulation at bottom), and someone else pointed out that if you extrapolate my logic to picking 8 cards 1/8 + 1/7 + 1/6 + 1/5 + 1/4 + 1/3 + 1/2 + 1/1 You get over 200% which is obviously wrong. The Question/Confusion Why does the reduction not affect (appear in) the maths? I feel the ""reduction"" in the pool should impact the maths or at least be cancelled out and that's what I struggle to grasp. Why does the reduction in the pool seemingly make no difference whatsoever, its seems at odds with what I feel I learnt from the Monty Hall problem.... I even felt the need to simulate it and sure enough *the answer was 37.5%. My gut tells me that the reduction might become statistically relevant in some cases, but my brain says there is missing (i.e. cancelled out maths) above that would explain that this is not true. Could someone help the penny drop and enlighten me? simulation: https://dotnetfiddle.net/ebTl5B --- [Full Maths] --- (Thanks Ethan!) Added here as it will render nicer then in the comments, but you will want to read Ethans answer below for why you do this. $$\frac{1}{8}  + (\frac{7}{8} * \frac{1}{7}) + (\frac{7}{8} * \frac{6}{7} * \frac{1}{6}) = $$ $$\frac{1}{8}  + \frac{1}{8} + \frac{1}{8} = $$ $$\frac{3}{8} =$$ $$ 0.375 $$","['proof-explanation', 'statistics', 'probability']"
3394755,Some lesser known open problems/ conjectures in number theory,"What are some lesser known problems/ conjectures in number theory ( especially on prime numbers ) which have evolved in these recent years and didn't got much of attention, and obviously wasn't featured in any global mathematical mainstream , It includes equivalences of pre existing conjectures/ results... Please let me know, thanks in advance","['number-theory', 'conjectures', 'prime-numbers']"
3394764,Motivation for an inequality,"I've seen the following inequality arise in applying the probabilistic method (in combinatorics); potentially it arises elsewhere. For all $n\in\mathbb{N}$ and $0\le x\le 1$ , $$1-(1-x)^n\le nx.$$ This inequality is routine to establish with calculus, but that doesn't yield much insight. I realize this question is a bit vague, but is there a perspective in which this inequality is ""natural""?","['inequality', 'combinatorics']"
3394810,"A linear algebra ""game"" with binary matrices","Let $n\in\mathbb N$ and an arbitrary binary matrix $M\in\{0,1\}^{n\times n}$ of full rank be given such that the last row of $M$ equals $(1,1,\ldots,1)$ . The following problem arose from a research question I have been working on recently. The core mechanic of the ""game"" is about taking any two rows $m_j,m_k$ of $M$ and partitioning them into $\min\{m_j,m_k\},\max\{m_j,m_k\}$ where $\min,\max$ operate entrywise. Just as an example to make things clear, if $m_j=\begin{pmatrix}1&0&1&0  \end{pmatrix}$ and $m_k=\begin{pmatrix} 1&1&0&0 \end{pmatrix}$ then $$
\min\{m_j,m_k\}=\begin{pmatrix}1&0&0&0\end{pmatrix}\qquad
\max\{m_j,m_k\}=\begin{pmatrix}  1&1&1&0\end{pmatrix}
$$ and, obviously, $m_j+m_k=\min\{m_j,m_k\}+\max\{m_j,m_k\}$ . 
Starting from $M_0:=M$ there are three possible scenarios: Neither $\min\{m_j,m_k\}$ nor $\max\{m_j,m_k\}$ are a row of $M_0$ . Then do nothing. Either $\min\{m_j,m_k\}$ or $\max\{m_j,m_k\}$ are a row of $M_0$ (but not both). Then extend $M$ by the other row vector. (Here we use the convention that the $0$ vector is a row of $M$ so if $\min\{m_j,m_k\}=0$ then extend $M_0$ by $\max\{m_j,m_k\}$ to get a new matrix $M_1$ .) Both $\min\{m_j,m_k\}$ and $\max\{m_j,m_k\}$ are a row of $M_0$ . Then do nothing. Repeat this process (where $M_0$ becomes $M_l$ and $M_1$ becomes $M_{l+1}$ in step $l\in\mathbb N_0$ ) until for every two rows of $M_l$ scenario 2 does not occur anymore, i.e. it is maximal in this sense and no other row is added via this scheme (then denoted by $M_\text{max}$ ). Now the ""game"" ends with a victory if there exist rows $m_{i_1},\ldots,m_{i_n}$ of $M_\text{max}$ such that $$
\begin{pmatrix}m_{i_1}\\\vdots\\m_{i_n}\end{pmatrix}=\begin{pmatrix}
1&0&\cdots&0\\
\vdots&\ddots&\ddots&\vdots\\
\vdots&&\ddots&0\\
1&\cdots&\cdots&1
\end{pmatrix}\underline{\sigma}\tag{1}
$$ for some permutation $\sigma\in S_n$ (where $\underline{\sigma}$ is the corresponding permutation matrix. This is equivalent to saying that $m_{i_{l+1}}$ arises from $m_{i_l}$ by turning a $0$ into a $1$ for all $l=1,\ldots,n-1$ . This concept should become a lot clearer again by considering an example. Example 1. Let $$
M=M_0=\begin{pmatrix}
1&0&0&0\\
0&1&1&0\\
1&0&1&1\\
1&1&1&1
\end{pmatrix}=\begin{pmatrix}m_1\\m_2\\m_3\\m_4\end{pmatrix}\tag{2}
$$ which is of full rank so we may start the game using this matrix. Now take for example $m_2$ and $m_3$ so $$
\min\{m_2,m_3\}=\begin{pmatrix} 0&0&1&0\end{pmatrix}\qquad\max\{m_2,m_3\}=\begin{pmatrix} 1&1&1&1\end{pmatrix}\,.
$$ Because $\max\{m_2,m_3\}$ is a row of $M$ but $\min\{m_2,m_3\}$ is not, apply Step 2 to get $$
M_1=\begin{pmatrix}
1&0&0&0\\
0&0&1&0\\
0&1&1&0\\
1&0&1&1\\
1&1&1&1
\end{pmatrix}
$$ Doing the same for $m_1$ and (now) $m_3$ yields $\max\{m_1,m_3\}=(1\ 1\ 1\ 0)$ and $\min\{m_1,m_3\}=(0\ 0\ 0\ 0)$ . By our convention this again is Scenario 2 so we extend $M_1$ by $(1\ 1\ 1\ 0)$ to get $$M_2=\begin{pmatrix} 1&0&0&0\\
0&0&1&0\\
0&1&1&0\\
1&0&1&1\\
1&1&1&0\\
1&1&1&1 \end{pmatrix}$$ Once more for $m_4,m_5$ and we arrive at $$
M_3=\begin{pmatrix}
1&0&0&0\\
0&0&1&0\\
0&1&1&0\\
1&0&1&0\\
1&0&1&1\\
1&1&1&0\\
1&1&1&1
\end{pmatrix}=M_\text{max}\,.
$$ One readily verifies that for any two rows of $M_3$ the above rule ends in Scenario 3 (so in particular not in Scenario 2, nothing new can be generated anymore) which shows maximality. 
  The submatrices of $M_\text{max}$ which satisfy the winning condition read $$
\begin{pmatrix}
1&0&0&0\\
1&0&1&0\\
1&0&1&1\\
1&1&1&1	
\end{pmatrix},
\begin{pmatrix}
1&0&0&0\\
1&0&1&0\\
1&1&1&0\\
1&1&1&1
\end{pmatrix},
\begin{pmatrix}
0&0&1&0\\
1&0&1&0\\
1&0&1&1\\
1&1&1&1
\end{pmatrix},
\begin{pmatrix}
0&0&1&0\\
1&0&1&0\\
1&1&1&0\\
1&1&1&1
\end{pmatrix},
\begin{pmatrix}
0&0&1&0\\
0&1&1&0\\
1&1&1&0\\
1&1&1&1
\end{pmatrix}\tag{3}
$$ One can also reformulate the problem in terms of directed graphs, which may (or may not) be a good alternative approach here but it certainly visualizes this problem in a nice way. Turn row vectors $\{m_1,\ldots,m_l\}$ into vertices $\{v_1,\ldots,v_l\}$ and draw an arrow from $v_j$ to $v_k$ whenever $v_k-v_j$ is a standard basis vector (i.e. $v_k-v_j$ consists of $n-1$ zeros and one $1$ ). Again by convention, $(0\ \ldots\ 0)$ is turned into a vertex as well. The victory condition then is met if and only if there exists a path from $(0\ \ldots\ 0)$ to $(1\ \ldots\ 1)$ which is equivalent to $A_\text{max}^n\neq 0$ with $A_\text{max}$ the adjacency matrix of the graph corresponding to $M_\text{max}$ . Example 1 (directed graphs-version). The graph of the original $M$ from (2) is with adjacency matrix $$A_0=\begin{pmatrix} 0&1&0&0&0\\0&0&0&0&0\\0&0&0&0&0\\0&0&0&0&1\\0&0&0&0&0 \end{pmatrix}\,.$$ Then $M_1$ converts to with adjacency matrix $$A_1=\begin{pmatrix} 0&1&\color{blue}1&0&0&0\\0&0&\color{blue}0&0&0&0\\\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1&\color{blue}0&\color{blue}0\\0&0&\color{blue}0&0&0&0\\0&0&\color{blue}0&0&0&1 \\0&0&\color{blue}0&0&0&0\end{pmatrix}$$ followed by $$A_2=\begin{pmatrix} 0&1&1&0&0&\color{blue}0&0\\
0&0&0&0&0&\color{blue}0&0\\
0&0&0&1&0&\color{blue}0&0\\
0&0&0&0&0&\color{blue}1&0\\
0&0&0&0&0&\color{blue}0&1\\
\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1\\
0&0&0&0&0&\color{blue}0&0 \end{pmatrix}$$ and $$
A_3=A_\text{max}=\begin{pmatrix}0&1&1&0&\color{blue}0&0&0&0\\
0&0&0&0&\color{blue}1&0&0&0\\
0&0&0&1&\color{blue}1&0&0&0\\
0&0&0&0&\color{blue}0&0&1&0\\
\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}0&\color{blue}1&\color{blue}1&\color{blue}0\\
0&0&0&0&\color{blue}0&0&0&1\\
0&0&0&0&\color{blue}0&0&0&1\\
0&0&0&0&\color{blue}0&0&0&0  \end{pmatrix}\,.$$ The blue numbers are the ones that are added in the corresponding step.
  Also note that the adjacency matrices of the individual steps are necessarily triangular.
  Now the victory condition is met if and only if $A_\text{max}^4\neq 0$ . Indeed $A_\text{max}^4=5 e_1e_n^T$ so there exist 5 different paths (corresponding to eq.(3)) that lead from $(0\ \ldots\ 0)$ to $(1\ \ldots\ 1)$ . Be aware that $A_2^4=e_1e_n^T\neq 0$ becasue there is one path from $\vec 0$ to $\vec 1$ so we already won after the second step. The whole time I put the word ""game"" in quotation marks for the following reason: Conjecture 1. For every $M$ satisfying the above conditions the game ends in a victory, i.e. $M_\text{max}$ contains at least one submatrix of form (1). Equivalently the directed graph corresponding to $M_\text{max}$ contains at least one path leading from from $(0\ \ldots\ 0)$ to $(1\ \ldots\ 1)$ , i.e. $A_\text{max}^n\neq 0$ . A useful intermediate step might be the following: Conjecture 2. Let $M$ satisfying the above conditions be given. For any two rows $m_j,m_k$ of $M_\text{max}$ only scenario 3 can occur. These conjectures are supported by a plethora of examples I have computed (up to $n=6$ ) which makes me rather confident that this is true. Two short comments to make here. The full rank condition on $M$ cannot be waived or replaced by something like ""all rows of $M$ have to be pairwise distinct"". For this consider $$
M=\begin{pmatrix} 1&1&0\\0&1&1\\1&1&1 \end{pmatrix}=M_\text{max}
$$ which does obviously not satisfy the winning condition (although for any two rows, Scenario 3 occurs). Having one row equal to $(1\ \ldots\ 1)$ is necessary. Consider $$
M=\begin{pmatrix} 1&1&0\\1&0&1\\0&1&1\end{pmatrix}=M_\text{max}\,,
$$ cf. also mjacobse's answer. I am currently lacking ideas on how to approach this problem. Working with the adjacency matrices seems like a possibility, although I neither see how the rank of $M$ is reflected in $A_\text{max}$ nor do I see a general pattern there yet. The only thing I see is that if $M_1,M_2\in\{0,1\}^{n\times n}$ of full rank satisfy $M_1=M_2\underline{\tau}$ for some permutation $\tau\in S_n$ then $M_1$ leads to victory if and only if $M_2$ does---so we can reduce this problem to equivalence classes of initial matrices but this should at best be a simplification, not an idea for a proof. Thank you in advance for any comments and or ideas!","['matrices', 'graph-theory', 'linear-algebra']"
3394824,Prove the asymptotic for the sum $\sum_{d\mid n}\frac{\mu(d)\ln(d)}{d}=O(\ln(\ln(n)))$,"I need to prove, that : $$\sum_{d\mid n}\frac{\mu(d)\ln(d)}{d}=O(\ln(\ln(n)))$$ where $n\geq3$ .By using: $\sum_{n\leq x}\mu(n)\ln(n)=o(x\ln(x))$ I was able to give: $$\sum_{d\mid n}\frac{\mu(d)\ln(d)}{d}=o(\ln^2(n))+O(\frac{\ln(n)}{n})$$ Here is my proof: $\sum_{d\mid n}\frac{\mu(d)\ln(d)}{d}\\
=\frac{1}{n}\sum_{d\mid n}\mu(d)\ln(d)\frac{n}{d}\\
=\frac{1}{n}\sum_{d\mid n}\mu(d)\ln(d)(\left[\frac{n}{d}\right]+\left\{\frac{n}{d}\right\})\\
=\frac{1}{n}\sum_{d\mid n}\mu(d)\ln(d)\sum_{k\leq\frac{n}{d}}1+O(\frac{1}{n}\sum_{d\mid n}\mu(d)\ln(d))$ for the first term we have: $\frac{1}{n}\sum_{d\mid n}\mu(d)\ln(d)\sum_{k\leq\frac{n}{d}}1\\
=\frac{1}{n}\sum_{k\leq n}\sum_{d\mid n \:k\leq \frac{n}{d}}\mu(d)\ln(d)\\
=\frac{1}{n}\sum_{k\leq n}\sum_{m\geq k}\mu(\frac{n}{m})\ln(\frac{n}{m})\\
=o(\ln^2(n))
$ for the second term we have: $O(\frac{1}{n}\sum_{d\mid n}\mu(d)\ln(d))\\
=O(\frac{\Lambda(n)}{n})\\
=O(\frac{\ln(n)}{n})$ I find it hard to deal with the term $\sum_{d\mid n}f(n)$ with tools like Euler's formular,so I am looking for ways to express $\sum_{d\mid n}f(n)$ in the form of $\sum_{n\leq x}g(n)$ .","['number-theory', 'asymptotics']"
3394873,Prove that $\sum\limits_{i\leq n} a_n X_n$ converges almost surely iff $\sum\limits_{i\leq n}a_n^2$ converges.,"Assume $(X_n)$ is a sequence of independent random variables with $\mathbb{P}(X_n = 1) = \mathbb{P}(X_n = -1) = \frac{1}{2}$ and $(a_n)$ a sequence of real numbers. Prove that $\sum\limits_{i\leq n} a_i X_i$ converges almost surely if and only if $\sum\limits_{i\leq n}a_i^2$ converges. I would really appreciate a hint for the direction where I assume that $\sum\limits_{i\leq n} a_n X_n$ converges almost surely. The other direction was easier for me; I proved it with Kolmogorov's three series theorem.
I tried to prove it indirectly by looking at the third series of Kolmogorov's three series theorem: $$\sum\limits_{i\leq n} Var(a_i X_i [\vert a_i X_i \vert \leq \varepsilon]) = ... = \sum\limits_{i\leq n} a_i 1_{[\vert a_i \vert \leq \varepsilon]}.$$","['measure-theory', 'probability-theory', 'analysis', 'real-analysis']"
3394939,About a density property of the Nearest Neighbor algorithm,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $(\mathcal{X},d)$ be metric space. Suppose that $X,X_1,X_2,X_3,... : \Omega\to\mathcal{X}$ are $\mathbb{P}$ -i.i.d. random variables. Get a closed set $K$ of $(\mathcal{X},d)$ and $x\in\partial K$ . Suppose that: $$\exists r_x>0, \exists \delta_x >0, \forall r\in(0,r_x), \frac{\mathbb{P}(X\in K\cap B_r (x))}{\mathbb{P}(X\in B_r (x))}\ge \delta_x+\frac{\mathbb{P}(X\in K^c\cap B_r (x))}{{\mathbb{P}(X\in B_r (x))}},$$ where $B_r(x)$ is the open ball centered in $x$ of radius $r$ in $(\mathcal{X},d)$ . Define: $$\forall m\in\mathbb{N}, \pi_m^x: \mathcal{X}^m\to\{1,...,m\}, (x_1,...,x_m)\mapsto \min\left(\operatorname{argmin}_{k\in\{1,...,m\}}\left(d\left(x,x_1\right),...,d\left(x,x_m\right)\right)\right).$$ Define: $$\forall m\in\mathbb{N}, Z_m^x:\Omega\to\mathcal{X}, \omega\mapsto X_{\pi_m^x\left((x,X_1(\omega),...,X_m(\omega)\right)}(\omega).$$ Is it true that $\mathbb{P}(Z_m^x\in K^c)\to 0, m\to\infty?$ Intuitively, since if $m$ is big enough we have that $\mathbb{P}(Z_m^x\in K^c\cap B_{r_x}(x))$ is close to $\mathbb{P}(Z_m^x\in K^c)$ , the dynamic is definitively governed by what happens in $B_{r_x}(x)$ and inside this ball is more likely to get $X\in K$ instead of $X\in K^c$ ... however, I didn't find a way to convert this intuition into a proof.","['measure-theory', 'geometric-measure-theory', 'martingales', 'probability-theory', 'probability']"
3395010,Inequality involving maximum of continuous functions,"Today, my Calculus teacher has used the following inequality without proving it, can someone give me a hint about its proof? Let $K$ be a compact in $\mathbb{R^n}$ and $f,g$ two continuous function over $K$ . 
Then, it is satisfied that $$\frac{\max\{\|f(x)+g(x)\|:x\in K\}}{1+\max\{\|f(x)+g(x)\|:x\in K\}}\leq  \frac{\max\{\|f(x)\|:x\in K\}}{1+\max\{\|f(x)\|:x\in K\}}+\frac{\max\{\|g(x)\|:x\in K\}}{1+\max\{\|g(x)\|:x\in K\}},$$ where $\|\cdot\|$ denotes the usual norm in $\mathbb{R}^n.$","['analysis', 'real-analysis', 'complex-analysis', 'functional-analysis', 'general-topology']"
3395044,Polynomial vs power series vs formal power series?,"Wikipedia states: In mathematics, a formal power series is a generalization of a polynomial , where the number of terms is allowed to be infinite; this implies giving up the possibility of replacing the variable in the polynomial with an arbitrary number. Thus a formal power series differs from a polynomial in that it may have infinitely many terms, and differs from a power series , whose variables can take on numerical values. What I am getting from this is that in both polynomials and formal power series, the variables ""don't represent numbers"". But I'm not exactly sure what this means, or what they do represent. Also it seems to be inconsistent with how I've been using polynomials, which is very much as ""variables representing numbers"". So basically I'm conceptually confused about what this means, and can't really understand how they're being used.","['power-series', 'polynomials', 'analysis', 'formal-power-series']"
3395098,How many real solutions does $\lambda_1 e^{y - \lambda_1 e^y} (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y} (1 - \lambda_2 e^y ) = 0$ have?,"I am trying to work out for what $\lambda_1, \lambda_2 > 0$ is it true that $f(y) = \lambda_1 e^{y-\lambda_1 e^y} + \lambda_2 e^{y-\lambda_2 e^y}$ is unimodal? Experimentally it seems it is unimodal when $\lambda_1 < \lambda_2$ and $\frac{\lambda_2}{\lambda{1}} <  7.5$ . To work this out I started with: $$\frac{d}{dy} \left(\lambda_1 e^{y-\lambda_1 e^y} + \lambda_2 e^{y-\lambda_2 e^y} \right) = \lambda_1 e^{y - \lambda_1 e^y}  (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y}  (1 - \lambda_2 e^y )$$ It seems we then need to check when $$\lambda_1 e^{y - \lambda_1 e^y}  (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y}  (1 - \lambda_2 e^y ) = 0$$ has more than one solution when solved for $y \in \mathbb{R}$ .  How can we determine the conditions under which it has different numbers of solutions? Added: Substituting $z = e^y$ and dividing by $e^{y-1}$ we are trying to determine how many solutions $$
 \lambda_1 e^{1-\lambda_1 z}(1-\lambda_1 z) +\lambda_2e^{1-\lambda_2 z}(1-\lambda_2 z) = 0
$$ has with $z > 0$ . Examples: Example $\lambda_1 = 1, \lambda_2 = 7$ with only one mode (code in python): import matplotlib.pyplot as plt
import numpy as np
def pdf_func(y, params):
    return sum([lambd*np.exp(y - lambd * np.exp(y)) for lambd in params])
params = [1, 7]
xs = np.linspace(-10,10,1000)
plt.plot(xs, [pdf_func(y, params) for y in xs]) Example $\lambda_1 = 1, \lambda_2 = 50$ with two modes: Questions How can one prove (assuming it is true) that that the number of local maxima that $f(y)$ has is either 1 or 2 and there are no other possibilities? Is it true that for $\lambda_2 > \lambda_1 > 0$ , there exists a threshold $c$ so that if $\frac{\lambda_2}{\lambda_1} < c$ then $f(y)$ is unimodal and if not it has two local maxima? (My guess is that the answer is yes and this threshold is around $7.5$ .)","['maxima-minima', 'calculus']"
3395155,Finding this math YouTube channel,"So last year, by chance I found a YouTube channel while I was looking for videos related to 3D geometry and multivariable calculus. The channel had very few subscribers and most video had views only in the hundreds. The channel had amazing animation and it had animated mascots. A guy wearing funky glasses appears in the beginning of every video and speaks in an animated old TV set. Can anybody help me find that channel please.
Thank you. Edit: Apparently no mascots or funky glasses, it's just my memory.","['multivariable-calculus', 'vectors', 'education', '3d']"
3395188,Computing the Second Moment/Expectation,"This seems like a relatively simple equation, but I have not really found an explanation that works for me. In my probability class, we were simply given that the kth moment of a random variable X is $E[X^k]$ . Are we supposed to be squaring the probability as well when we are computing the second moment? I.e. computing E[X] means computing $\sum_i iP(X=i)$ . So then is $E[X^2]$ equal to $\sum_i (iP(X=i))^2$ ? Or is it $\sum_i i^2P(X=i^2)$ or $\sum_i i^2P(X=i)$ ? From reading around the internet, I assume it is the last equation, but I do not really understand the intuition behind this. Here's what I understand: Moments are used to determine how much data points are spread out. So the Expectation of {5, 5, 5, 5, 5} is the same as the expectation of {3, 4, 5, 6, 7}, but they have different second moment-expectations. I understand the concept, but not much more. Clarification, intuition, or links to visual representations would be much appreciated Edit: Additionally, if someone could provide an explanation for this simple fact, it would help: "" $E[X_i]^2 = E[X_i] = \frac{1}{2}$ where $X_i$ is 1 if you toss a coin and get heads, and 0 if you get tails."" What is $E[X^2]$ for n coin tosses?","['expected-value', 'statistics', 'probability']"
3395208,What is the probability that at least one card is in the right box,"Four Christmas cards are randomly placed in their recipients' boxes
  (one in every box). What is the probability that at least one card is
  in the right box? So, if we number the boxes with $1,2,3,4$ and let the first card should be placed in the box with number $1$ , the second in the box with number $2$ , etc. If only one card is in the right box, we have $4$ possibilities; if only two cards are in the right boxes, we have $6$ possibilities. I am having troubles with what happens when three cards are in the right boxes. Do we have only $1$ way or $4$ ways?","['discrete-mathematics', 'probability']"
3395225,Minimal possible order of a group that contains a specific subset,"Suppose $G$ is a group. Suppose $A \subset G$ is a subset of $G$ satisfying the following condition: $\forall a \in A \exists ! b \in A$ such that $[a, b] \neq e$ . Suppose $|A| = 2n$ . What is the minimal possible order of $G$ ? I can build such group of order $2^{2n+1}$ , namely $G = \langle a_1, … , a_n, b_1, … , b_n, c| a_i^2 = b_i^2 = c^2 = [a_i, c]=[b_i, c]=[b_i,b_j] = [a_i, a_j] = e, [a_i, b_j] = c^{\delta_{ij}}\rangle$ , where $\delta$ stands for the Kronecker delta function , and $A = \{a_1, … ,a_n, b_1, … , b_n\}$ . However, I do not know, whether $2^{2n+1}$ is the minimal possible order, or is there some  better construction…","['finite-groups', 'group-theory', 'abstract-algebra', 'combinatorics']"
3395227,How does one prove that $n^2 +5n + 16$ is not divisible by $169$ for any integer $n$?,"How does one prove that $n^2 +5n + 16$ is not divisible by $169$ for any integer $n$ ? THOUGHTS : This is equivalent to say that $$
n^2 +5n + 16=0\pmod{169}
$$ has no solutions. One can also observe that $169=13^2$ . And of course one cannot expect to prove this case by case since $\mathbb{Z}$ is not a finite set. 
But I really don't know how to proceed from here. Can any one help?","['elementary-number-theory', 'divisibility', 'modular-arithmetic', 'discrete-mathematics']"
3395282,About a density property of the Nearest Neighbor algorithm: part 2.,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $(\mathcal{X},d)$ be metric space. Suppose that $X,X_1,X_2,X_3,... : \Omega\to\mathcal{X}$ are $\mathbb{P}$ -i.i.d. random variables. Get a closed set $K$ of $(\mathcal{X},d)$ and $x\in\partial K$ . Suppose that: $$\exists \delta_x \in(0,1], \frac{\mathbb{P}(X\in \partial K\cap B_r (x))}{\mathbb{P}(X\in B_r (x))}\to \delta_x, r\to 0^+$$ and $$\frac{\mathbb{P}(X\in K^c\cap B_r (x))}{{\mathbb{P}(X\in B_r (x))}}\to0, r\to 0^+$$ where $B_r(x)$ is the open ball centered in $x$ of radius $r$ in $(\mathcal{X},d)$ . Define: $$\forall m\in\mathbb{N}, \pi_m^x: \mathcal{X}^m\to\{1,...,m\}, (x_1,...,x_m)\mapsto \min\left(\operatorname{argmin}_{k\in\{1,...,m\}}\left\{d\left(x,x_k\right)\right\}\right).$$ Define: $$\forall m\in\mathbb{N}, Z_m^x:\Omega\to\mathcal{X}, \omega\mapsto X_{\pi_m^x\left((X_1(\omega),...,X_m(\omega)\right)}(\omega).$$ Is it true that $\mathbb{P}(Z_m^x\in K^c)\to 0, m\to\infty?$ This is a version with stronger hypothesis of this other question that has a negative answer: About a density property of the Nearest Neighbor algorithm","['measure-theory', 'geometric-measure-theory', 'martingales', 'probability-theory', 'probability']"
3395302,Probability of wearing a coat & having it snow.,"I haven't been able to solve this, could you please help? Steve has lived in Boston for a long time. From experience, he knows that each day has a 5%
chance to be snowy. Steve looks ahead to the next 10 days. Steve decides that, without checking the weather, he will wear his heavy coat to work 3 of
those days. What is the probability Steve will wear his coat on at least one snowy day? Thank you","['statistics', 'discrete-mathematics', 'probability']"
3395338,Sums of two Gaussian Integer squares,"The well known Sum of Two Squares Theorem states that an integer greater than one can be written as a sum of two squares if and only if its prime decomposition contains no prime congruent to $3\bmod4$ raised to an odd power. I wondered about extending this to Gaussian Integers: which Gaussian Integers are expressible as the sum of two squares of two other Gaussian Integers? After exploration with Mathematica, a clear pattern seemed to emerge, using which I made the following conjecture: Given $z\in\mathbb{Z}[i]$ , there exists ${\space} z_1,z_2 \in\mathbb{Z}[i]$ such that $z=z_1^2+z_2^2$ ${}$ iff ${}$ $\Im{(z)}\equiv{0\bmod4}$ ${}$ OR ${}$ $(\Im{(z)}\equiv{2\bmod4}\space\land\space\Re{(z)}\not\equiv2\bmod4)$ . Has this conjecture been made/proven before? If so, how can one prove it?","['number-theory', 'sums-of-squares', 'gaussian-integers', 'complex-numbers']"
3395391,Proving Quotient Space of Torus Homeomorphic to Klein Bottle,"Problem. Let $T=S^1\times S^1$ , where $S^1=\{z\in\mathbb{C}:|z|=1\}$ . Prove the quotient space of $T$ by the equivalence relation $(z,w)\sim(\bar{z},-w)$ is homeomorphic to the Klein bottle. Theorem I. Let $X$ be compact and $Y$ be Hausdorff. If $f:X\rightarrow Y$ is continuous and surjective with the point inverses in $Y$ under $f$ being the equivalence relation $\sim$ on $X$ , then $X/\sim$ is homeomorphic to $Y$ . Attempt. The Klein bottle is $K=I \times I/[(t,0)\sim(t,1),(0,t)\sim(1,1-t)]$ . I want to use (I). The equivalence relation that we must factor by represets a reflection of each point through the origin (right?). I'm having trouble finding a continuous function that has preimages make up the equivalence class.","['general-topology', 'klein-bottle', 'algebraic-topology']"
3395422,Injective function from reals to irrationals? [duplicate],"This question already has answers here : Showing that $\mathbb{R}$ and $\mathbb{R}\backslash\mathbb{Q}$ are equinumerous using Cantor-Bernstein (4 answers) Closed 4 years ago . Is there a known ""standard"" injective function $f: \mathbb{R} \to \mathbb{R} \setminus \mathbb{Q}$ ? Or actually any injective function, doesn't have to be ""standard/simple.""","['elementary-set-theory', 'general-topology']"
3395449,Question on Borel Measurable Functions and Borel Sigma Algebras.,"On an assignment, we have been given a the following setup:
Let $ f:A\rightarrow \bar{\mathbb{R}}$ be a borel measurable function. prove that if $f$ is Borel measurable and $B$ is a Borel set, then $f^{-1}(B)$ is a Borel set. The definition of Borel Measurability we were given is as follows:
""A function $f : \mathbb{R} \to \mathbb{R}$ is said to be Borel measurable provided
its domain A ⊆ R is a Borel set and for each c, the set { $x ∈ A : f (x) < c$ } is a Borel
set. We were not given any description of where this set lives, I'm assuming $B \subset \mathbb{R}$ but this may be incorrect. I figure we need to show that the set { ${B \subset \mathbb{R}:f^{-1}(B)}$ is a Borel set} is a sigma algebra but I am unsure how to do this. I know its a little silly because all you need to do is check that the definitions of a sigma algebra are met but showing those things is proving more difficult than i anticipated. We may also use the fact that borel measurable functions are Lebesgue measureable. Any help would be greatly appreciated!","['borel-sets', 'measure-theory', 'borel-measures', 'real-analysis']"
3395457,Grouping of combinations with elements in common,"Consider all the $\left(\begin{array}{c} n\\k\end{array}\right)$ possible combinations of $k$ elements from a fixed set of $n$ elements. Let’s now form groups of these combinations with the following property: In each group there exist at least one combination with at least $s<k$ elements in common with all other combinations of the group. Is it possible to find the minimum number of groups that cover all the $\left(\begin{array}{c} n\\k\end{array}\right)$ combinations, as a function of $n$ , $k$ and $s$ ? The exact number would be great, but some (sharp) upper limit would also be very useful. In other words, I would like to find the $s$ -net covering number of the set of all $\left(\begin{array}{c} n\\k\end{array}\right)$ possible combinations, with respect to the distance metric corresponding to the number of non-overlapping elements.","['elementary-set-theory', 'statistics', 'combinatorics', 'clustering']"
3395465,The proof of $(n+1)!(n+2)!$ divides $(2n+2)!$ for any positive integer $n$,"Does $(n+1)!(n+2)!$ divide $(2n+2)!$ for any positive integer $n$ ? I tried to prove this when I was trying to prove the fact that ${P_n}^4$ divides $P_{2n}$ where $n$ is a positive integer, where $P_{n}$ means the multiplication of all $k!$ from $1$ to $n$ , in other words, $P_{n}=1!2!...n!$ So I tried a stronger statement with induction, since if I were to prove the statement "" ${P_n}^4$ divides $P_{2n}$ "" using induction, ${(n+1)!}^4$ does not divide $(2n+1)!(2n+2)!$ when $n=2$ So instead I tried proving ${P_n}^4 (n+1)$ divides $P_{2n}$ using induction (first motivated by the want of $n+1$ term in the dividend), but this too seems to be a bit hard since there will be a $n+2$ term in the bottom, but then I tried by plugging a few values from $1$ to $13$ into calculators, it was thus found out that maybe $(n+1)!(n+2)!$ divides $(2n+2)!$ . I tried this by first considering the number of $p$ (prime) dividing the divisor must be smaller or equal to the number of the $p$ dividing the dividend. Namely: $(\left \lfloor{\frac{n+1}{p}}\right \rfloor + \left \lfloor{\frac{n+1}{p^2}}\right \rfloor +...)$ + $(\left \lfloor{\frac{n+2}{p}}\right \rfloor + \left \lfloor{\frac{n+2}{p^2}}\right \rfloor +...)$ $\leq$ $\left \lfloor{\frac{2n+2}{p}}\right \rfloor + \left \lfloor{\frac{2n+2}{p^2}}\right \rfloor +...$ . Then by letting $j$ be an arbitrary positive integer, it was proven that $\left \lfloor{\frac{n+1}{p^j}}\right \rfloor + \left \lfloor{\frac{n+2}{p^j}}\right \rfloor \leq \left \lfloor{\frac{2n+2}{p^j}}\right \rfloor$ by considering many cases of whether $p^j$ divides $n+1$ . Is there any other proof more intuitive and more ""elegant"" than this one where we have to consider the many cases? Or is there even any better approach of proving the original problem? (preferably an attempt of both method, induction and without induction.) Thanks Than","['prime-factorization', 'ceiling-and-floor-functions', 'factorial', 'modular-arithmetic', 'discrete-mathematics']"
3395485,Distribution of $F(X)$ where $F$ is the distribution function of a continuous random variable $X$,"Let $X$ be a random variable such that it's distribution function $F(x):=P(X \le x)$ is continuous. Then is it true that $F(X)$ follows a uniform distribution ? I can show this if $F$ is differentiable, but otherwise I'm not sure. Please help.","['probability-distributions', 'probability-theory', 'random-variables']"
3395559,"Is there a better/faster way to take anti-derivatives of simple functions than ""reversing"" the derivative rules?","Update : For what it's worth, I will wait another few hours to see if anyone else has a more comprehensive answer to my question. But if not, I will ""accept"" one of the two extant answers, both of which are very good although not quite as comprehensive as I had hoped. I am taking AP Calculus AB right now and we are learning about anti-derivatives (indefinite integrals) for Unit II. Before that, we learned some basic derivative rules for transformed functions, such as: $ [f (x + a)]]' = f'(x + a)$ $[f(ax)]'=a*f'(ax)$ (We also learned the derivatives of some elementary functions, e.g. polynomials, exponential functions, sine, and cosine.) For anti-derivatives we have likewise memorized (and proved) formulas for some basic functions, but unlike with derivatives we have not been taught very much at all about what to do with transformed functions. Consider, say, finding the anti-derivative $F(x)$ if $f(x)=1/(4x)$ . Or finding the anti-derivative $G(x)$ if $g(x)=\cos(4x)$ . (Or even worse, how about if $f(x)$ was actually $1/(4x-3)$ , and $g(x)$ was actually $\cos(4x-3)$ ?) I am not entirely sure how to systematically and carefully go about solving such problems. Should I try to learn integral u -substitution or any tricks like that (which we haven't covered in class yet), or am I better off just trying to intuitively ""reverse"" the differentiation rules as best I can? I want to figure out a relatively efficient method of integrating basic functions but right now am pretty confused. (Often attempting to reverse the differentiation rules kinda gives me a headache haha and I get utterly lost because it's hard to think about things backwards.)","['integration', 'advice', 'calculus', 'indefinite-integrals', 'derivatives']"
3395594,Is there a name for this apparent paradox?,"I was recently reminded of this probability ""paradox"" (in quotes because in fact there's nothing really paradoxical about it, but it's surprising to the intuition), which I first encountered back in the rec.puzzles Usenet group.  I want to know if it has a name.  Here it is: Suppose I write any two distinct (real) numbers on two blank cards that I then put face down. You get to look at the number on one of the cards, of your choice.  Now, I ask you whether that number is the lower or the higher of the two. Can you answer in such a way that your probability of being correct is strictly greater than $\frac12$ ? At first, it seems incredible that this is even being asked. I can use any method I choose to select the two numbers, and you have no way of knowing what the other number is. Nonetheless, you can answer in such a way that your response is more likely than not to be correct. Method: Select any cumulative distribution function $F(x)$ that is strictly increasing; that is, pick a function $F(x)$ such that $F(x) > F(y)$ for all $x > y$ $\lim_{x \to -\infty} F(x) = 0$ $\lim_{x \to +\infty} F(x) = 1$ To make things simple, a suitable function is $F(x) = \frac{2^x}{1 + 2^x}$ .  (This is similar to the standard logistic function, but with base $2$ instead of base $e$ .) Now, if the number you look at is $x$ , you say ""higher"" with probability $F(x)$ , and ""lower"" with probability $1-F(x)$ .  For instance, if you turn over a card and see the number $3$ , you say ""higher"" with probability $F(3) = \frac{2^3}{1+2^3} = \frac89$ , and ""lower"" with probability $1 – F(3) = \frac19$ . Your probability of getting the right answer can now be determined as follows: Suppose the two numbers I selected were $a$ and $b$ , with $b > a$ .  Your answer is correct if either you looked at $a$ and said ""lower"" (with probability $1-F(a)$ ), or you looked at $b$ and said ""higher"" (with probability $F(b)$ ).  Since each of these two scenarios is equally likely—remember, you got to choose the card you looked at, with no prior information—your overall probability of guessing correctly is $$
P(\text{correct}) = \frac{1-F(a)+F(b)}{2} = \frac12 + \frac{F(b)-F(a)}{2}
$$ But since $F(x)$ is strictly increasing, and $b > a$ , we must have $F(b) > F(a)$ , and so your probability of being correct must be strictly greater than $\frac12$ . Has anyone else heard of this seeming paradox, along with a name? Update (2019-10-16-0413Z): Math.SE didn't provide this tip before I posted the question, but now this shows up as a ""related question"": Who discovered this number-guessing paradox? I'll have to take a closer look at that!","['puzzle', 'probability']"
3395609,"if $A \subseteq B$, then $A \cap C \subseteq B\cap C$","So I tried doing this problem myself, and the answer that I got seems right, yet at the same time I feel like the way I did it is kind of.... wonky? It seems weird basically, and I was hoping someone can help me validate my answer proof: Suppose $x \in A \subseteq B$ . Then $x \in A$ and $x \in B$ . Thus, if $x \in A \cap C$ , Then $x\in A$ and $x\in C$ . Since $x \in A$ and $x \in B$ and $x \in C$ , Then $x \in B$ and $x \in C$ , which implies that $x \in B \cap C$ . Thus $A\cap C \subseteq B \cap C$ . Therefore, if $A \subseteq B$ , then $A\cap C\subseteq B\cap C$ I'm just not sure if it was alright to assume that $x \in A \cap C$ , which is what makes me feel like my proof may be wrong and weird.","['elementary-set-theory', 'proof-verification']"
3395659,"Let $H$ and $K$ be subgroups of a finite cyclic group $G.$ Prove $|H \cap K| = \gcd(|H|,|K|)$ [duplicate]","This question already has answers here : Subgroups of a cyclic group and their order. (5 answers) Closed 2 years ago . Let $H$ and $K$ be subgroups of a finite cyclic group $G.$ Prove $|H \cap K| = \gcd(|H|,|K|)$ My attempt: $H$ and $K$ are subgroups of $G.$ Therefore, $H$ and $K$ are cyclic. Further, $|H|$ and $|K|$ divide $|G|$ . Every divisor $m$ of $G$ has a unique cyclic subgroup of order $m$ . So $$H = \langle g^\frac{|G|}{|H|}\rangle, \quad K = \langle g^\frac{|G|}{|K|}\rangle$$ By Lagrange's theorem, $\frac{|G|}{|H|} = [G : H]$ and $\frac{|G|}{|K|} = [G : K]$ So: $$|H\cap K| = |\langle g^{[G:H]}\rangle \cap \langle g^{[G:K]}\rangle|$$ I don't know how this implies that this equals $\gcd(|H|,|K|)$ . Any help would be appreciated.","['group-theory', 'abstract-algebra', 'finite-groups', 'cyclic-groups']"
3395675,"Is there a perfect square (other than 9) all of whose digits are 7, 8, or 9?","Clearly, $3^2=9$ is a perfect square, all of whose digits are $7$ , $8$ , or $9$ . 
 Are there any other perfect squares with this property? This is an interesting question that does not seem to be solved yet, coming from AoPS ( https://artofproblemsolving.com/community/c6h1928519 ).  As duck_master seems to show, it should be impossible to solve this problem by analyzing the quadratic residues modulo $10^n$ for some $n$ . I strongly suspect the answer is no.  I have been running a Python script for quite some time, and it has checked squares up to $(50,000,000,000)^2$ with no results (unless I messed up the code).",['number-theory']
3395688,"175 speak German ,150 French, 180 English, 160 Japanese. How many speak all of them?","Among 200 journalists, there are: 175 speak German 150 speak French 180 speak English 160 speak Japanese Each journalist can speak at least one of the 4 languages.
What is the maximum possible number of journalists who can speak all of them?
What is the minimum possible number of journalists who can speak all of them? Attempt: For the maximum part, it should be $150$ , because if there is more than 150, it means there are more than 150 who speak French which is not the fact. From PIE $$|G \cap F \cap E \cap J| = 465 - \left( |G \cap F| + |G \cap E| + | E \cap F| + |E \cap J| + |G \cap J| + |F \cap J| \right) + \left( |G \cap F \cap E| + |G \cap J \cap F| + |G \cap J \cap E| + |E \cap F \cap J| \right)  $$ so to find the minimum i have to find the maximum of the $$ \left( |G \cap F| + |G \cap E| + | E \cap F| + |E \cap J| + |G \cap J| + |F \cap J| \right) + \left( |G \cap F \cap E| + |G \cap J \cap F| + |G \cap J \cap E| + |E \cap F \cap J| \right)  $$ What is the idea?","['inclusion-exclusion', 'combinatorics']"
3395689,"When $n\geq2$, let $a_n = \lceil \frac{n}{\pi}\rceil$ and let $b_n = \lceil{\csc({\frac{\pi}{n}})}\rceil$.","When $n\geq2$ , let $a_n =\left \lceil \frac{n}{\pi}\right\rceil$ and let $b_n = \left\lceil{\csc({\frac{\pi}{n}})}\right\rceil$ . The terms of the sequences starting with $n = 2$ are: { $a_n$ } = $1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, ...$ and { $b_n$ } = $1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, ...$ Note that the sequences differ when $n=3$ . Is it true that $a_n = b_n$ for all $n>3$ ?","['ceiling-and-floor-functions', 'discrete-mathematics', 'sequences-and-series']"
3395744,How to find the cube of a uniform distribution?,"I came across this question in a quiz and I was not sure on how to do it since my lecturer didn't clearly teach this. Can anyone assist me with this: Let $X$ ~ $U(0,3)$ , find the density $f(u)$ for $U = X^3$ and calculate its value at $u=2$ I first tried attempting it by doing: $F_{X^3}(x) = \mathbb{P}(X^3 \leq x) = \mathbb{P}(X \in [0,\sqrt[3]{x}]) = \sqrt[3]{x}$ . But this is completely wrong apparently, can someone please clarify this for me?","['statistics', 'uniform-distribution', 'probability']"
3395748,Is the joint chain formed of a Metropolis-Hastings chain and the corresponding proposal sequence a Markov chain?,"Let $(X_n)_{n\in\mathbb N_0}$ denote the Markov chain generated by the Metropolis-Hastings algorithm $^1$ with proposal kernel $Q$ and target distribution $\mu$ , $(Y_n)_{n\in\mathbb N}$ denote the corresponding proposal sequence and $$Z_n:=(X_{n-1},Y_n)\;\;\;\text{for }n\in\mathbb N.$$ How can we show that $(Z_n)_{n\in\mathbb N}$ is a time-homogeneous Markov chain and how can we determine its transition kernel? By definition, $$Z_n\sim\mathcal L(X_{n-1})\otimes Q\tag1$$ and $$(X_{n-1},X_n)\sim\mathcal L(X_{n-1})\otimes\kappa\tag2$$ for all $n\in\mathbb N$ . Moreover, there is a real-valued process $(U_n)_{n\in\mathbb N}$ independent of $(Z_n)_{n\in\mathbb N}$ with $U_n\sim\mathcal U_{[0,\:1]}$ and $$X_n=\begin{cases}X_{n-1}&\text{on }\{α(X_{n-1},Y_n)<U\}\\Y_n&\text{on }\{α(X_{n-1},Y_n)\ge U\}.\tag1\end{cases}$$ Now let $n\in\mathbb N$ and $A_n,B_{n+1}\in\mathcal E$ . Since $U_n$ and $(Z_n,Y_{n+1})$ are independent, $$\operatorname P\left[Y_{n+1}\in B_{n+1}\mid U_n,Z_n\right]=\operatorname P\left[Y_{n+1}\in B_{n+1}\mid Z_n\right]\tag2.$$ Moreover, $$\operatorname P\left[U_n\le\alpha(X_{n-1},Y_n),Z_{n+1}\in A_n\times B_{n+1}\in U_n,Z_n\right]=\delta_{Y_n}(A_n)1_{[0,\:\alpha(X_{n-1},\:Y_n)]}(U_n)\operatorname P\left[Y_{n+1}\in B_{n+1}\mid Z_n\right]\tag3$$ and $$\operatorname P\left[U_n>\alpha(X_{n-1},Y_n),Z_{n+1}\in A_n\times B_{n+1}\mid U_n,Z_n\right]=\delta_{X_{n-1}}(A_n)1_{(\alpha(X_{n-1},\:Y_n),\:1]}(U_n)\operatorname P\left[Y_{n+1}\in B_{n+1}\mid Z_n\right]\tag4.$$ Thus, $$\operatorname P\left[Z_{n+1}\in A_n\times B_{n+1}\mid U_n,Z_n\right]=\left(\delta_{Y_n}(A_n)1_{[0,\:\alpha(X_{n-1},\:Y_n)]}(U_n)+\delta_{X_{n-1}}(A_n)1_{(\alpha(X_{n-1},\:Y_n),\:1]}(U_n)\right)\operatorname P\left[Y_{n+1}\in B_{n+1}\right]\tag5$$ and hence $$\operatorname P\left[Z_{n+1}\in A_n\times B_{n+1}\mid Z_n\right]=\left(\delta_{Y_n}(A_n)\operatorname E\left[1_{[0,\:\alpha(X_{n-1},\:Y_n)]}(U_n)\mid Z_n\right]+\delta_{X_{n-1}}\operatorname E\left[1_{(\alpha(X_{n-1},\:Y_n),\:1]}(U_n)\mid Z_n\right]\right)\operatorname P\left[Y_{n+1}\in B_{n+1}\mid Z_n\right]\tag6.$$ So, all what's left is to determine $\operatorname E\left[1_{[0,\:\alpha(X_{n-1},\:Y_n)]}(U_n)\mid Z_n\right]$ , $\operatorname E\left[1_{(\alpha(X_{n-1},\:Y_n),\:1]}(U_n)\mid Z_n\right]$ and $\operatorname P\left[Y_{n+1}\in B_{n+1}\mid Z_n\right]$ . EDIT : It's straightforward to show that $$\operatorname E\left[1_{[0,\:\alpha(X_{n-1},\:Y_n)]}(U_n)\mid Z_n\right]=\alpha(X_{n-1},Y_n)\tag7$$ and $$\operatorname E\left[1_{(\alpha(X_{n-1},\:Y_n),\:1]}(U_n)\mid Z_n\right]=1-\alpha(X_{n-1},Y_n)\tag8.$$ Using the result of this question , we obtain \begin{equation}\begin{split}&\operatorname P\left[Y_{n+1}\in B_{n+1}\mid Z_n\right]\\&\;\;\;\;\;\;\;\;\;\;\;\;=(1-\alpha(X_{n-1},Y_n))Q(X_{n-1},B_{n+1})+\alpha(X_{n-1},Y_n)Q(Y_n,B_{n+1}.\end{split}\tag9\end{equation} Thus, \begin{equation}\begin{split}&\operatorname P\left[Z_{n+1}\in A_n\times B_{n+1}\mid Z_n\right]\\&=\left(\left(1-\alpha(X_{n-1},Y_n)\right)\delta_{X_{n-1}}(A_n)+\alpha(X_{n-1},Y_n)\delta_{Y_n}(A_n)\right)\left(\left(1-\alpha(X_{n-1},Y_n)\right)Q(X_{n-1},B_{n+1})+\alpha(X_{n-1},Y_n)Q(Y_n,B_{n+1})\right).\end{split}\tag{10}\end{equation} However, the correct transition kernel of $(Z_n)_{n\in\mathbb N}$ should be $$((x_{n-1},y_n),A_n\times B_{n+1})\mapsto\delta_{y_n}(A_n)Q(y_n,B_{n+1})\alpha(x_{n+1},y_n)+\delta_{x_{n-1}}(A_n)Q(x_{n-1},B_{n+1})(1-\alpha(x_{n+1},y_n))\tag{11}$$ instead. Which of my equations $(1)$ to $(10)$ is wrong? I can't find my mistake. And how can we prove that $(11)$ is indeed the transition kernel? $^1$ To be precise, let $(E,\mathcal E,\lambda)$ be a measure space; $p:E\to[0,\infty)$ be $\mathcal E$ -measurable with $$\int p\:{\rm d}\lambda=1$$ and $\mu:=p\lambda$ ; $q:E^2\to[0,\infty)$ be ${\mathcal E}^{\otimes2}$ -measurable and $$Q(x,\;\cdot\;):=q(x,\;\cdot\;)\lambda\;\;\;\text{for }x\in E;$$ $$\alpha(x,y):=\left.\begin{cases}\displaystyle1\wedge\frac{p(y)q(y,x)}{p(x)q(x,y)}&\text{, if }p(x)q(x,y)\ne0\\1&\text{, otherwise}\end{cases}\right\}\;\;\;\text{for }x,y\in E;$$ $$r(x):=1-\int Q(x,{\rm d}y)\alpha(x,y)\;\;\;\text{for }x\in E;$$ $$\kappa(x,B):=\int_BQ(x,{\rm d}y)\alpha(x,y)+r(x)1_B(x)\;\;\;\text{for }(x,B)\in E\times\mathcal E.$$","['markov-process', 'measure-theory', 'probability-theory', 'markov-chains']"
3395775,What does a twice differentiable function mean?,"In many contexts, I have seen phrases like ""... function $f$ is twice differentiable ..."". I can understand that the function $f$ can be differentiated twice or in other words its second derivative exists. But does this mean third derivative (or higher) does not exist or is equal to $0$ ? In the same way, what would we call a function like $e^x, \sin x, \dots$ which can be differentiated any number of times?","['calculus', 'soft-question', 'derivatives']"
3395798,Definition of a matrix by matching two vectors,"There are two vectors: $\boldsymbol{\hat{y}} = (\hat{y_1}, \hat{y_2}, \dots, \hat{y_n})$ $\boldsymbol{{y}} = ({y_1}, {y_2}, \dots, {y_n})$ The vectors $\boldsymbol{\hat{y}}$ and $\boldsymbol{{y}}$ have some matches, but also some values that do not match. From these vectors I would like to create a matrix, which contains like the identity matrix in the diagonal ones, but only if the values match, otherwise zero. For example : $\boldsymbol{\hat{y}} = (1.0, 2.0, 3.0, 4.0)$ $\boldsymbol{{y}} = (1.0, 2.2, 3.0, 4.0)$ From these vectors results: $$ M= \left(\begin{matrix}1&0&0&0\\0&0&0&0\\0&0&1&0\\0&0&0&1\end{matrix}\right)$$ Is there a way to describe the definition of $M$ mathematically?","['matrices', 'vectors']"
3395872,"Must a continuous, non-constant, and periodic functions have a smallest period?","Let $D\subset\mathbb R$ and let $T\in(0,\infty)$ . A function $f\colon D\longrightarrow\mathbb R$ is called a periodic function with period $T$ if, for each $x\in D$ , $x+T\in D$ and $f(x+T)=f(x)$ . If $D\subset\mathbb R$ and if $f\colon D\longrightarrow\mathbb R$ is a continuous, not constant and periodic function, must there be, among all periods of $f$ , a minimal one? I posted a similar question a year ago. The difference is that now I am adding an extra hypothesis, namely that $f$ is not constant. All that I was able to prove was that the infimum of the set of periods has to be greater than $0$ .","['continuity', 'periodic-functions', 'real-analysis']"
3395875,Definition of principal $G$-bundle might be missing details or have implicit assumptions on actions on each trivializing open set,"My book is Connections, Curvature, and Characteristic Classes by Loring W. Tu (I'll call this Volume 3), a sequel to both Differential Forms in Algebraic Topology by Loring W. Tu and Raoul Bott (Volume 2) and An Introduction to Manifolds by Loring W. Tu (Volume 1). I refer to Section 27.1 (part 1) , Section 27.1 (part 2) and Section 27.1 (part 3) . Firstly: I believe the book has no explicit definition for an action $\mu$ to be ""transitive"" and neither does Volume 1. I think this is okay for the book since Proposition 27.6 is not (explicitly) used later on in the book. 1.1. If this wouldn't be okay for the book, then I would ask how, if possible, we could deduce from Tu's definition of principal $G$ -bundle that from the action $\mu: P \times G \to P$ , we get that $\mu(P_x \times G) \subseteq P_x$ , where $P_x := \pi^{-1}(x)$ , which is saying something like $\mu$ is fiber-preserving, such that we can define an action $\mu_x: P_x \times G \to P_x$ and then begin to discuss whether or not each $\mu_x$ is transitive. 1.2 Even though I'm not asking (1.1), what I'm about to ask has a similar underlying problem. Anyway, I assume the definition that an action $\mu$ is ""transitive"" is the one here , assume that definition is equivalent to the one on Wikipedia and assume that that both definitions are equivalent to ""for each $x \in M$ , the map $\mu_x : G \to M, \mu_x(g) = \mu(x,g)$ , is surjective, where $\mu: M \times G \to M$ is the right action of $G$ on $M$ "". Now: Tu's definition of principal $G$ -bundle doesn't say anything about transitive or fiber-preserving, but it may be equivalent to a definition with transitivity (see here ). I mean that transitive or fiber-preserving could be somehow deduced from Tu's definition (as stated). Tu's definition is possibly the ""Definition 3"" in the previous link ). I guess the alternative is that Tu made a mistake in the definition of principal $G$ -bundle. I actually notice that for each $U \in \mathfrak 
U$ , while we are given an explicit action $\sigma_U: U \times G \times G \to U \times G$ , which is $\sigma_U((x,h),g)=(x,hg)$ , we are not given an explicit definition of the action $\zeta_U: P_U \times G \to P_U$ , where $P_U := \pi^{-1}(U)$ . 4.1. Edit : Oh wait that was kind of wrong. What I meant was to say that $\zeta_U$ is not even declared to exist in the first place. I really think the text is unclear here. I think the text should've said something like "" $G$ acts on $U \times G$ (in the way of $\sigma_U$ ), and then $G$ acts on $\pi^{-1}(U)$ in such a way that $\phi_U$ is invariant"". Otherwise, it seems kinda weird that you just say a map is equivariant even though you haven't declared the existence of an action on both domain and range. It just seems that somehow the action $\mu$ on $P$ induces $\zeta_U$ 's. 4.1.1. Edit : Probably, there should even be some prior proposition that starts with ""given a map $f: N \to M$ and action $\zeta$ by $G$ on $N$ we can define an action $\sigma$ by $G$ on $M$ "" or that starts with ""given a map $f: N \to M$ and action $\sigma$ by $G$ on $M$ we can define an action $\zeta$ by $G$ on $N$ "" and then the next part would be ""that makes $f$ equivariant"" and then there might be another proposition or some exercise that says that the defined $\zeta$ or $\sigma$ is unique. I'm thinking of something similar to the pullback metric , from earlier in the book . 4.1.2. Edit : A comment of autodavid : In the definition of principal $G$ -bundle, the way in which $G$ is acting on $P$ should make the trivialization maps $G$ -equivariant when restricting to a trivialization patch..... Oh okay, there would be some problem because we don't know whether the restrictions are legal. I'm not an expert but I guess Tu implicitly requires that the restrictions to be legal, by talking about equivariance. I'm expecting something like, for the action $\mu: P \times G \to P$ , we get that 5.1. $\mu(P_x \times G) \subseteq P_x$ and $\mu(P_U \times G) \subseteq P_U$ such that we can define, respectively,  maps $\mu_x: P_x \times G \to P_x$ and $\mu_U: P_U \times G \to P_U$ . These turn out to be actions, probably smooth actions. 5.2. Each $\mu_x$ in (5.1) is transitive. (Well, this is what Proposition 27.6 says.) 5.3. $\zeta_U = \mu_U$ : Each $\mu_U$ in (5.1) is the action $\zeta_U$ used to determine whether or not $\varphi_U$ is $G$ -equivariant Questions : Is this definition of principal $G$ -bundle missing some details, such as any notion (explicit or implicit) of fiber-preservation of the action $\mu: P \times G \to P$ or any explicit description of the actions $\zeta_U: P_U \times G \to P_U$ ? 1.1 Edit : Or any explicit mention of the relationship between $\zeta_U$ 's and $\mu$ 1.2 Edit : Or mention of some kind of proposition that tells us the $\zeta_U$ 's, which may or may not be related to $\mu$ , are unique provided $\phi_U$ equivariant and $\sigma_U$ given as such If the definition is in fact not missing any (explicit or implicit) notion of fiber-preserving ( Edit : fiber-preserving or trivializing-open-subset-preserving ) of the action $\mu: P \times G \to P$ because we can somehow deduce some kind of notion of fiber-preserving ( Edit : fiber-preserving or trivializing-open-subset-preserving ) of the action $\mu$ or that any of (5.1),(5.2) or (5.3) is true, then which are true, and how do we deduce these? Are $\zeta_U$ and $\sigma_U$ necessarily smooth based on Tu's definition (as stated)? If not, then, based on other definitions of (smooth) principal $G$ -bundle that you know, are $\zeta_U$ and $\sigma_U$ likely intended to be smooth? I think I was able to prove $\sigma_U$ 's are smooth by writing each $\sigma_U$ as a combination of maps, by compositions and multiplication of maps, where the maps include various projection maps and the law of composition on the Lie group $G$ . To clarify, the $\sigma_U$ 's are free and transitive right? I think this follows from what I believe is the freedom and transitivity of the left multiplication group action of any group on itself based on its law of composition . Update: Can we just omit $\mu$ in the definition and then just later make a proposition about $\mu$ in the following way? I'm thinking we instead first define that for each $U \in \mathfrak U$ , $G$ acts on $U \times G$ on the right, still by the given $\sigma_U$ and then we say that $G$ acts on $\pi^{-1}(U)$ by some smooth right action $\zeta_U$ (I guess we don't have to include free or transitive since $\sigma_U$ is free and transitive and then freedom and transitivity are preserved under bijective equivariant or whatever), where $\zeta_U$ satisfies some compatibility condition like $\zeta_U|_{U \cap V} = \zeta_V|_{U \cap V}$ for all $V \in \mathfrak V$ makes $\phi_U$ is $G$ -equivariant. Later , we can make a propositions Lemma A . $\phi_U$ is $G$ -equivariant if and only if the $\zeta_U$ is given by $$\zeta_U(e,g) = \phi_U^{-1}(\sigma_U(\phi_U(e),g)) = \phi_U^{-1} \circ \sigma_U \circ ([\phi_U \circ \alpha_U] \times \beta_U) \circ (e,g), \tag{A*}$$ where $\alpha_U: \pi^{-1}(U) \times G \to \pi^{-1}(U)$ and $\beta_U: \pi^{-1}(U) \times G \to G$ are projection maps. (In this case, I guess $\alpha_U$ is the smooth trivial action by $G$ on $\pi^{-1}(U)$ .) Exercise A.i . Check that $\zeta_U$ in $(A*)$ is a smooth, right, free and transitive action by $G$ on $\pi^{-1}(U)$ . Exercise A.ii . Check that $\zeta_U$ in $(A*)$ satisfies the above compatibility condition. Equivalent Definition A.1 . We use Lemma A , Exercise A.i and Exercise A.ii to say instead that $\zeta_U$ is given by ( $A*$ ). Theorem B . $G$ globally acts on $P$ by some (smooth) right, free and transitive global action $\mu$ that turns out to be from collecting all the local actions, the $\zeta_U$ 's, together: $\mu(p,g):=\zeta_U(p,g)$ for $p \in \pi^{-1}(U)$ for any $U \in \mathfrak U$ , which is well-defined either by the compatibility condition assumption on $\zeta_U$ 's in the original definition, where we don't yet know the formula for $\zeta_U$ or by Exercise A.ii , if we use $\zeta_U$ given by ( $A*$ ). Corollary C1 . $\mu$ is trivializing-open-subset-preserving , i.e. $\mu((U \times G) \times G) \subseteq U \times G$ Corollary C2 . $\mu$ is fiber-preserving, i.e. $\mu((x \times G) \times G) \subseteq x \times G$ Bounty message : I really believe there's at least one of the following here: ambiguity or implicit relationship between $\mu$ and $\zeta_U$ 's, implicit rule about uniqueness or existence of an action (in this case $\zeta_U$ 's) on domain of a map that makes a map equivariant given an action (the $\sigma_U$ 's) on the range circular reasoning or circular definitions or something that need to be remedied either by some assumption $\mu$ preserves fibers or trivializing open subsets or by first defining smooth compatible local actions, the $\zeta_U$ 's on the $P_U$ 's, that make $\phi_U$ 's equivariant and then later deducing a global action $\mu$ on $P$","['principal-bundles', 'abstract-algebra', 'group-theory', 'group-actions', 'differential-geometry']"
3395903,"Suppose $f:[a,b]→\mathbb R$ and $g:[a,b]→\mathbb R$ are both continuous. Let $T=\{x|f(x)=g(x)\}$. Prove that T is closed.","In a previous homework problem, I proved that $[a,b]\subset \mathbb R$ is closed. Therefore, in the case that $f(x)=g(x)$ for all $x\in [a,b]$ , then $T=[a,b]$ which is closed by a previous proof. I'm allowed to cite that problem in my work. Since, the greatest case is $f(x)=g(x)\space \forall x\in[a,b]$ and T would be closed-- $T$ can only become a smaller set from this point on. However, I think this problem is trying to get us to prove that any subset of a closed set, is also closed. I don't know if this is true, but this is what my intuition is telling me the problem actually is. I don't know how to write a proof saying that maybe card $T$ is finite and so its a subset of the greater $T$ mentioned earlier which is closed so it must be closed. However, if we look at $f(x)=x^2$ and $g(x)=x$ which have two intersection points. In this case, $T=\{0,1\}$ and so looking at the definition of a closed set, it requires that every accumulation point of the set, belongs to the set. Digging further, looking at the definition of an accumulation point, $0,1$ can only be accumulation points iff every neighborhood of $0$ or $1$ contain infinitely many points in $T$ , but $T$ is not infinite. Leading me to think that maybe every subset of the ""greater"" closed $T$ mentioned earlier is not closed. I'm not sure where to go from here.","['continuity', 'uniform-continuity', 'analysis', 'real-analysis']"
3395941,Proof that divergence is independent of choice of coordinate system,"I am attempting to prove that divergence of a smooth vector field $X$ over a $n$ -dimensional Riemannian manifold $(M,g)$ is invariant under change of coordinates (or invariant of the choice of frame). I am stuck in the proof and here is my attempt. I am starting from the formula given in [p. 33, 1].
So, we have for $$X=X^i \frac{\partial}{\partial x^i}$$ the divergence is $$
\text{div}\left( X^i \frac{\partial}{\partial x^i}\right)
= 
\frac{1}{\sqrt{\det g_{ij}} }\frac{\partial}{\partial x^i}\big( X^i \sqrt{\det g_{ij}}\big).
$$ As a first step I am trying to prove that the RHS of that formula is the same for two different orthonormal frames. So, I assume an orthonormal  frame $\{E_i,E_2,...,E_n\}$ with dual frame $\{e^1,e^2,...,e^n\}$ .
First of all in this frame we have $$X^i = e^i(X)$$ and due to orthonormality we obtain $$ \det g_{ij} = 1.$$ I shall use $\text{div}(X)_{Ee}$ to emphasize the dependence on the frame.
So we obtain $$\text{div}(X)_{Ee} = E_i(e^i(X))$$ Now I consider another orthonormal frame $\{F_i,F_2,...,F_n\}$ with dual frame $\{f^1,f^2,...,f^n\}$ . 
For this frame we have $$\text{div}(X)_{Ff} = F_i(f^i(X))$$ Now, to prove invariance under the two frames, I need to prove $$\text{div}(X)_{Ff} = \text{div}(X)_{Ee}$$ I expand the second frame in terms of the first as $$\begin{eqnarray} F_i &=& F_i^l E_l \\
f^i &=& f^i_m e^m\end{eqnarray}$$ Then we have $$\begin{eqnarray} 
\text{div}(X)_{Ff} &=& F_i(f^i(X))  \nonumber \\
&=& F_i^lE_l(f^i_m e^m(X)) \nonumber \\
&=& F_i^lE_l(f^i_m) e^m(X) + F_i^lf^i_m E_l(e^m(X))   \\
&=& F_i^lE_l(f^i_m) e^m(X) + \delta^l_m E_l(e^m(X)) \\
&=& F_i^lE_l(f^i_m) e^m(X) + \text{div}(X)_{Ee}
\end{eqnarray}$$ Where the third step is from Leibniz's rule and the fourth step is because the matrices $[f_m^i]$ and $[F_i^l]$ are inverses of each other. So essentially I need to prove $$F_i^lE_l(f^i_m) e^m(X) = 0$$ and I am not able to do this, can someone help? [1] Lee, John M. Introduction to Riemannian manifolds. Vol. 176. Springer, 2018. P.S. I did go through Divergence of a smooth vector field and that answer uses the covariant derivative. I am attempting to avoid using it.","['divergence-operator', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
3396031,How to write this as a multiplication: $4 x^2 y^2 - (x^2 + y^2 - z^2)^2$?,"I have the following polynomial: $$ 4 x^2 y^2 - (x^2 + y^2 - z^2)^2$$ (which comes up, for example, in computing the area of a triangle using the cosine law ). I would like to convert this to a product.
Wolfram tells me it's $$ -(x - y - z) (x + y - z) (x - y + z) (x + y + z)$$ How can I find this form if I don't already know it? What operations should I perform?","['triangles', 'algebra-precalculus', 'factoring', 'polynomials']"
3396062,Can every algebraic integer of degree $3$ be approximated by a quotient of linearly recurrent integer sequences of degree $3$?,"Given a zero $\alpha \in \mathbb{R}$ of an irreducible monic third degree polynomial $x^3 - a_2x^2 - a_1x - a_0$ , are there always integer sequences $(p_n)_{n=1}^\infty$ and $(q_n)_{n=1}^\infty$ satisfying recursions $$p_{n+1} = b_2p_n + b_1p_{n-1} + b_0p_{n-2} \\q_{n+1} = c_2q_n + c_1q_{n-1} + c_0q_{n-2},$$ with some integer coefficients, such that $\lim_{n\rightarrow \infty} \frac{p_n}{q_n} = \alpha$ ? For quadratic irrationals, this is possible. Indeed, if $p(x) = x^2 - a_1x - a_0$ is the minimal polynomial of $\alpha$ , and $p_n$ any sequence satisfying $p_{n+1} = a_1p_n + a_0p_{n-1}$ , then the sequence $\frac{p_n}{p_{n-1}}$ converges to the zero with the bigger absolute value, and $\frac{a_0p_{n-1}}{p_n} $ converges to the smaller one, as long as $\alpha \neq \pm \sqrt{d}$ . In that case the solutions of Pell's equation $x^2 - dy^2 = 1$ yield the desired sequences. This trick also yields approximations for the largest and smallest real zero of any monic integer polynomial, but I couldn't find a similar trick to get at a possible ""middle"" zero.","['number-theory', 'diophantine-approximation', 'sequences-and-series']"
3396160,Polynomial invariants of finite groups preserved under epimorphism,"I am trying to understand Proposition 4.1 of Nakajima's paper 'Invariants of finite groups generated by pseudo-reflections in positive characteristic' (link here: https://pdfs.semanticscholar.org/da5f/8e1b4adc0fbb0141491c6aa6330123d0fc47.pdf ). Statement: Let $k$ be a field, $G$ a finite group (here $k$ has arbitrary characteristic $p > 0$ : in particular I'm interested in the case $|G| \equiv 0 $ mod $p$ ). Let $V$ and $W$ be finite-dimensional $G$ -faithful $kG$ -modules, and let $\phi: V \to W$ be a $kG$ -linear epimorphism. If $k[V]^G$ is polynomial, then $k[W]^G$ is polynomial. Proof: Write $k[V] = \sum_{i=1}^{|G|} k[V]^G f_i$ for some $f_i \in k[V]$ . Let $\widetilde{\phi}: k[V] \to k[W]$ be the induced epimorphism. Then $k[W] =  \sum_{i=1}^{|G|} k[W]^G \widetilde{\phi}(f_i)$ . Nakajima then claims that since $G$ acts faithfully on $W$ , $k[W]$ is a free $k[W]^G$ -module. It is this claim that I don't currently understand. This statement then implies that $k[W]^G$ is polynomial. Any help or references would be appreciated.","['group-theory', 'ring-theory', 'finite-groups', 'invariant-theory']"
3396161,Why does $\mathbb{RP}^2$ not continuously embed in $\mathbb{R}^3$?,"Ok, I know the answer: any closed hypersurface of $\mathbb R^3$ is orientable while $\mathbb R \mathbb P^2$ is not. But I know how to prove that only for smooth embeddings. Is there a simple way to prove that there is no continuous embedding of the projective plane in Euclidean tridimensional space?","['general-topology', 'low-dimensional-topology', 'algebraic-topology']"
3396264,Does there exist an endofunctor of the category of countable sets that lacks an initial algebra?,"There exist endofunctors of $\mathbf{Set}$ that are deficient inasmuch as they lack an initial algebra. The canonical example is the covariant powerset functor (the initial algebra of $\mathcal{P}$ , if it existed, would include the whole cumulative hierarchy). Now let $\mathbf{Set}_{< \aleph_1}$ denote the category of countable sets. I'm curious to know if there exist endofunctors of $\mathbf{Set}_{< \aleph_1}$ that also lack an initial algebra. Since we can't take powersets while remaining in the countable realm, it will be interesting to see if anyone can find an example of such a thing.","['foundations', 'set-theory', 'abstract-algebra', 'category-theory']"
3396266,About degree of a smooth map between Manifolds,"So from chapter 5 of Milnor's Topology from the Differentiable Viewpoint, there are two theorems about the degree of a smooth map where the degree is defined as follows: $f: M \to N$ is a smooth map between manifolds of same dimension, M is compact without boundary and N is connected. $deg(f;y) = \sum_{x \in f^{-1}(y)} sign(df_x)$ , where $y$ varies over the set of regular values of $f$ . From the fact that $\#f^{-1}y$ is a locally constant function and determinant is a smooth map, we can say that $deg(f;y)$ is locally constant. Now the theorems are as follows: Theorem A : The integer $deg(f; y)$ does not depend on the choice of
regular value y. Theorem B : If $f$ is smoothly homotopic to $g$ , then $deg$ $f$ = $deg$ $g$ . For the proof of theorem A, I thought about using the fact that since $deg(f;y)$ is a locally constant function and $N$ being a connected manifold, we should have that $deg(f;y)$ is constant but then $y$ varies over all regular values of f which may not be even connected. I think I have to use Sard's theorem but I have got no clue. For the proof of theorem B, Milnor states that ""The proof will be essentially the same as that in §4"" which states that $deg$ $mod (2)$ of smoothly homotopic maps are equal. But then again I don't understand how to use that. I have been thinking about these two for over a day now. A sketch of the proofs or even some hint will be appreciated.","['geometric-topology', 'differential-topology', 'differential-geometry']"
3396267,"Prove inequality $|y \ln{y} - x \ln{x}| < 2 |\ln{\frac{1}{|y-x|}}|$ when $x,y \in (0,1]$, $x \neq y$.","EDIT: Counter-example found. Statement is FALSE. However, I think the argument still has value. It is true if you restrict the domain to $[0.223,0.716]$ . Maybe $[\frac{3}{10},\frac{7}{10}]$ so it’s less “obvious”? Prove inequality $|y \ln{y} - x \ln{x}| < 2 |\ln{\frac{1}{|y-x|}}|$ , when $x,y \in (0,1]$ , $x \neq y$ . Assume WLOG $y > x$ . I have tried Mean Value Theorem. For some $c \in (0,1)$ : \begin{align*}
|y \ln{y} - x \ln{x}| &= (1+\ln{c})(y-x) = (\ln(e)+\ln(c))(y-x)\\
& \leq 2 \ln{\frac{1}{(y-x)}} \text{ if and only if } \frac{(y-x)^2}{e} \leq c \leq \frac{1}{e(y-x)^2}
\end{align*} So the inequality is true when $$
\frac{(y-x)^2}{e} \leq x \leq y \leq \frac{1}{e(y-x)^2}
$$ in which $c$ is between $x$ and $y$ . Now we need to prove for the following cases in a different way: \begin{align*}
x < \frac{(y-x)^2}{e} &\implies x^2 - (2+e)x + 1 > 0 : \text{when 0<x<0.222ish}\\
\frac{1}{e(y-x)^2} < y &\implies y > e^{-1/3} : \text{when y > e^-1/3 = 0.716ish }
\end{align*} as $1-x>y-x>y$ . I think this statement should be true because some of the cases can be taken care by MVT, and cases where it can’t be solved by MVT are when $x$ and $y$ are near the endpoints. I tried using taylor $\ln(1+x)$ but I can’t get anything. Any help would be appreciated.","['inequality', 'logarithms', 'examples-counterexamples', 'real-analysis']"
3396288,Algebraic proof of $\sum_{k}\binom{n}{2k}\binom{2k}{k}2^{n-2k}=\binom{2n}n$ (Combinatoric proof is given),"I had a IMO training about double counting. Then, there is a problem  which I hope there is a combinatoric proof. Here comes the problem: For every positive integer $n$ , let $f\left(n\right)$ be the number of all positive integers with exactly $2n$ digits, each having exactly $n$ of digits equal to $1$ and the other equal to $2$ . Let $g\left(n\right)$ be the number of all positive integers with exactly $n$ digits, each of its digits can only be $1,2,3$ or $4$ and the number of $1$ 's equals the number of $2$ 's. Prove that $f\left(n\right)=g\left(n\right)$ . It is obvious to see that $f\left(n\right)=\binom{2n}{n}$ , and $g\left(n\right)=\sum_{k\le\lfloor\frac{n}{2}\rfloor}\binom{n}{2k}\binom{2k}{k}2^{n-2k}$ . However, it is hard to prove this in an algebraic way. I hope there are someone to prove it by algebraic way . Thank you! Combinatoric proof We can establish a one-to-one correspondence between $f\left(n\right)$ and $g\left(n\right)$ . Let $F\left(n\right)$ be the set of all positive integers with exactly $2n$ digits, each having exactly $n$ of digits equal to $1$ and the other equal to $2$ . Also, let $G\left(n\right)$ be the set of all positive integers with exactly $n$ digits, each of its digits can only be $1,2,3$ or $4$ and the number of $1$ 's equals the number of $2$ 's. Then, we can do this operation for all numbers in $F\left(n\right)$ : For every two digits of the numbers in $F\left(n\right)$ , $$\begin{cases}11\Rightarrow 1\\22\Rightarrow 2\\12\Rightarrow 3\\21\Rightarrow 4\end{cases}$$ Then all the numbers will change into a set which is totally same as $G\left(n\right)$ , as we find that the difference between the number of $1$ 's and $2$ 's doesn't change at all. Therefore, we make a one-to-one correspondence between $F\left(n\right)$ and $G\left(n\right)$ .","['contest-math', 'combinatorial-proofs', 'alternative-proof', 'combinatorics', 'algebraic-combinatorics']"
3396302,"Prove that if $\sum_{i=1}^n x_i=0$ and $\sum_{i=1}^n x_i^2=1$ there exist $i,j$ such that $x_ix_j\leq -{1\over n}$","Problem Let $x_1, x_2, \ldots, x_n$ be reals such that \begin{align}
\sum_{i=1}^n x_i=0
\qquad \text{ and } \qquad
\sum_{i=1}^n x_i^2=1 .
\end{align} Prove that there exist $i,j$ such that $x_ix_j\leq -{1\over n}$ . my attempt I found that if there exist $x_i,x_j$ such that $$x_i\geq {1\over\sqrt{n}}, x_j\leq -{1\over \sqrt{n}} $$ then the problem solved, but I can't prove this.","['contest-math', 'inequality', 'linear-algebra', 'quadratic-forms']"
3396338,Does an integral preserve strict inequality?,"Suppose that $f$ and $g$ are real valued continuous functions on $[0,1]$ such that $f(t)<g(t)$ for all $t\in[0,1]$ . Is it true that $$\int_{0}^{1}f(t) \ dt<\int_{0}^{1}g(t) \ dt \ ?$$ I know that this result is true when we replace "" $<$ "" by "" $\leq$ "".","['integration', 'real-analysis', 'continuity', 'integral-inequality', 'inequality']"
3396341,"$(G,\circ)$ is an abelian group, where $x\circ y=\frac{x+y+a(1+xy)}{1+xy+a(x+y)}$","Let $G=(-1,1)$ and $a\in G$ be fixed. Prove that $(G,\circ)$ is an abelian group, where $$x\circ y=\frac{x+y+a(1+xy)}{1+xy+a(x+y)}, \forall x,y\in G.$$ To me, it seems extremely tedious to prove the axioms of the group in this case. Proving associativity is horrendous and I don't believe that any other of the axioms (apart from commutativity) is provable without extremely long computations. In order to avoid this, I tried to use the so-called structure transport i.e. finding a bijective function from $G$ to some well-known group. I couldn't come up with any function, so I don't know how to actually solve this question. I doubt that it can be solved by proving each of the group axioms, but if anyone finds a way to do this I would be both amazed and grateful.","['group-theory', 'abstract-algebra', 'abelian-groups']"
3396384,Normal copula vs Fréchet copula,"I'm trying to understand how to generate $2$ random variables $U_1,U_2$ which are correlated using the above $2$ methods with a rank correlation $P_s = 0.3$ . First I create $2$ random variables $X_1,X_2$ from $U(0,1)$ $Y_1 = X_1$ and $Y_2 = X_1(P_s) + X_2 \sqrt{(1-Ps^2)}$ Now my $U_1$ and $U_2$ are Inverse $(Y_1,Y_2)$ . (by Inverse it is meant inverse of normal distribution) The above believe is from the normal copula. The Fréchet copula is given by $$C(U_1,U_2) = p \,\min(U_1,U_2) + (1-p)\,\, \max (0,U_1+U_2-1)$$ With limited background in mathematics. I'm not quite sure how to do this, I assume I just use my $P_s$ value in place of $p$ but I'm not sure what the min and max term actually means. 
Thanks for any help.","['statistics', 'random-variables']"
3396414,Prove or Disprove Subset and Powerset,"I'm working on a problem but I need additional feedback to see if its correct. I'm trying to figure out if.... $$P(A \cup B) ⊆  P(A) \cup P(B) \cup P(A \cap B)$$ Where power set is denoted by P.
I used A = {1,2,3}  and B = {2,4}. I did all the power sets for each of the following. And I came up that this in fact is disproved because from $P(A \cup B)$ there exists {2,4} that only exists in $P(B)$ but not in $P(A)$ or $P(A \cap B)$ . Am i correct to assume that the above statement is disproved and incorrect???",['elementary-set-theory']
3396449,"Dimension of a Subspace of $\text{Hom}_\mathbb{K}(\mathcal{V},\mathcal{W})$ Consisting of Only Linear Transformations of Rank $\leq r$","Background. I have a conjecture (stated in two ways below), which I would like to see whether it is true (both the inequality part and the part involving the equality cases).  The motivation comes from this thread .  The conjecture (both the inequality part and the equality cases) is known to be true when the base field $\mathbb{K}$ has the property that, for any positive integer $l$ , the only solution $(x_1,x_2,\ldots,x_l)\in\mathbb{K}^l$ to $$x_1^2+x_2^2+\ldots+x_l^2=0$$ is $x_1=x_2=\ldots=x_l=0$ .  Subfields of $\mathbb{R}$ are examples, but there are other fields with this property such as pure transcendental extensions of $\mathbb{R}$ and ordered fields.  (They are all fields of characteristic $0$ , by the way.) Conjecture (Version I). Let $\mathbb{K}$ be a field.  Let $\mathcal{V}$ and $\mathcal{W}$ be two finite-dimensional vector spaces over $\mathbb{K}$ with dimensions $n$ and $m$ , respectively.  For an integer $r$ such that $$0\leq r\leq \min\{m,n\}\,,$$ suppose that $\mathcal{T}$ is a subspace of the $\mathbb{K}$ -vector space $\text{Hom}_{\mathbb{K}}(\mathcal{V},\mathcal{W})$ such that every linear transformation $\varphi:\mathcal{V}\to \mathcal{W}$ in $\mathcal{T}$ is of rank at most $r$ .  Then, $$\dim_\mathbb{K}(\mathcal{T})\leq r\,\max\{m,n\}\,.$$ Here are the equality cases. (a) For $m< n$ , there exists an $r$ -dimensional subspace $\mathcal{I}$ of $\mathcal{W}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\text{im}(\varphi)\subseteq \mathcal{I}$ . (b) For $m>n$ , there exists an $(n-r)$ -dimensional subspace $\mathcal{K}$ of $\mathcal{V}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\ker(\varphi)\supseteq \mathcal{K}$ . (c) For $m=n$ , there are two possibilities. There exists an $r$ -dimensional subspace $\mathcal{I}$ of $\mathcal{W}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\text{im}(\varphi)\subseteq \mathcal{I}$ . There exists an $(n-r)$ -dimensional subspace $\mathcal{K}$ of $\mathcal{V}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\ker(\varphi)\supseteq \mathcal{K}$ . Conjecture (Version II). Let $\mathbb{K}$ be a field.  For integers $m,n>0$ and $r$ such that $$0\leq r\leq \min\{m,n\}\,,$$ suppose that $\mathcal{M}$ is a $\mathbb{K}$ -vector subspace of the $\mathbb{K}$ -vector space $\text{Mat}_{m\times n}(\mathbb{K})$ such that each matrix in $\mathcal{M}$ is of rank at most $r$ .  Then, $$\dim_\mathbb{K}(\mathcal{M})\leq r\,\max\{m,n\}\,.$$ Here are the equality cases. (a) For $m< n$ , there exists an $r$ -dimensional subspace $\mathcal{C}$ of $\mathbb{K}^m$ such that $\mathcal{M}$ consists of matrices whose column spaces are contained in $\mathcal{C}$ . (b) For $m> n$ , there exists an $r$ -dimensional subspace $\mathcal{R}$ of $\mathbb{K}^n$ such that $\mathcal{M}$ consists of matrices whose row spaces are contained in $\mathcal{R}$ . (c) For $m=n$ , there are two possibilites. There exists an $r$ -dimensional subspace $\mathcal{C}$ of $\mathbb{K}^m$ such that $\mathcal{M}$ consists of matrices whose column spaces are contained in $\mathcal{C}$ . There exists an $r$ -dimensional subspace $\mathcal{R}$ of $\mathbb{K}^n$ such that $\mathcal{M}$ consists of matrices whose row spaces are contained in $\mathcal{R}$ . Warning. While I am quite confident that the inequality is true, I am not very sure that I got the equality cases right.  If there are counterexamples, I would love to see. Remark. I just noticed that joriki posted this reference under the question here .  I have not yet digested it, but it seems says that the inequality holds when $m=n$ for any field $\mathbb{K}$ .  Maybe, the proof can be modified to allow $m\neq n$ . There does not seem to be any mention of the equality cases in this paper. Theorem 3 of the paper gives the equality condition. Edit. The conjecture (both the inequality part and the equality cases) is true when $m=n$ .  Now, we have to find out what to do when $m\neq n$ .  By the way, using duality or transpose, we may assume that $m<n$ .  (Perhaps, we can even allow $\mathbb{K}$ to be a division ring.)","['matrix-rank', 'vector-spaces', 'matrices', 'linear-algebra', 'linear-transformations']"
3396460,Find the number of different $10$ digit numbers having no digit repeated and divisible by $99$.,"Find the number of different $10$ digit numbers having no digit repeated and divisible by $99$ . Obviously the number is divisible by $9$ because $0+1+2+3+4+5+6+7+8+9=45$ and $45$ is divisble by $9$ . Now let the number be $a_1b_1a_2b_2a_3b_3a_4b_4a_5b_5$ . For divisbilty by $11$ , $(a_1+a_2+a_3+a_4+a_5)-(b_1+b_2+b_3+b_4+b_5)=0 or 11 or 22 or 33$ I could not think anything from here. Help. Thanks.","['elementary-number-theory', 'combinatorics']"
3396464,Durrett's proof of Kakutani dichotomy,"In Durrett's ""Probability: Theory and Examples"" (4th ed., page 244) to state that two measures are either singular or absolutely continuous the author reasons as follow: (I ommited the entire proof and kept only what's relevant for this question) First, it proves that $\mu(A) = \int_A X d\nu$ $+$ $\mu(A\cap\{X=\infty\})$ ; here $X \geq 0$ It is known that: $\nu(\{X=\infty\}) = 0 $ $\nu(\{X=0\}) \in \{0, 1\}$ From these facts the author concludes that either $\mu \ll \nu$ or $\mu \bot \nu$ . I can see that if $\nu(\{X=0\}) = 1$ then $\int_A X d\nu = 0$ $\forall A \in \mathcal{F}$ so in this case $\mu(A) = \mu(A\cap\{X=\infty\})$ and $\mu \bot \nu$ since $\nu(A\cap\{X=\infty\}) = 0$ $\forall A \in \mathcal{F}$ But I cannot see the other implication: if $\nu(\{X=0\}) = 0$ then for some $A$ with $\nu(A) = 0$ I can only say that $\mu(A) = \mu(A\cap\{X=\infty\})$ and from this alone I cannot derive that $\mu(A)=0$ I would greatly appreciate any help, and provide any details if needed. I tried to keep the post stick to what's needed for the proof. Thank you very much in advance! EDIT: Thanks to Nate Eldredge's comment now I see the information is insufficient. I state the hypothesis for the Theorem below: Both $\mu$ and $\nu$ are measures on $(\mathbb{R}^\mathbb{N}, \mathcal{R}^\mathbb{N})$ that make the coordinates $\xi_n(\omega) = \omega_n$ independent. From the body of the proof I can see that both are probability measures, but this is not stated by the author. This filtration is defined: $\mathcal{F}_n = \sigma(\xi_m : m \leq n)$ and $\mu_n$ and $\nu_n$ are the restrictions of $\mu$ and $\nu$ , respectively, to $\mathcal{F}_n$ . $X_n = \frac{d\mu_n}{d\nu_n}$ is the sequence of the Radom-Nykodim derivatives and $X = \lim \sup X_n$ . It is proved that $X_n \to X$ , $\nu$ -a.s.","['measure-theory', 'probability-theory']"
3396514,Successive minima,"Definition: The $n$ successive minima $\lambda_1,..,\lambda_n$ of $C$ with respect to lattice $L$ are defined as follow $\lambda_i$ is the minimum of all positive reals $\lambda$ such that $\lambda C \cap L$ contains at least $i$ linear independent points. See also: https://en.wikipedia.org/wiki/Minkowski%27s_second_theorem Determine the two successive minima of $$C=\{ (x_1,x_2) \in
 \mathbb{R}^2| \left| x_1- \sqrt{2}x_2 \right| \leq 1, \left| x_1-
 \sqrt{3}x_2 \right| \leq 1 \}$$ with respect to $\mathbb{Z}^2$ . By changing variables $u:= x_1-\sqrt{2}x_2$ and $v:=x_1 -\sqrt{3}x_2$ we have the square with side $2$ . Hence, we have $$vol(C)= 2^2(\sqrt{2}+\sqrt{3})$$ By first Minkowski's convex body theorem, if we have $vol(\lambda C) \geq 2^2 $ then $\lambda C \cap \mathbb{Z}^2 \neq 0$ . This implies $\lambda_1 \leq \sqrt{\dfrac{1}{\sqrt{3}+\sqrt{2}}}=\sqrt{\sqrt{3}-\sqrt{2}}$ . But I do not have lower bound for first successive minima. Does anyone have any idea?","['number-theory', 'integer-lattices', 'diophantine-approximation']"
3396593,A representation induced from one subgroup and restricted to another,"Denote with $\,{\cal{L}}^G\,$ the space of all continuous functions $\varphi(g)$ mapping the elements of a Lie group $G$ to some vector space ${\cal{L}}$ : $$
 {\cal{L}}^{G}~=~\left\{~\varphi~~~{\Large{|}}~~~\varphi\,:~G\,\longrightarrow\,{\cal{L}}\right\}~~.
 $$ Consider a subgroup $K<G$ and its representation $D(K)$ by linear operators on ${\cal{L}}^{G}\,$ : $$
 D:\quad K~\longrightarrow~{\rm{GL}}({\cal{L}}^{G})~.\qquad\qquad\qquad\qquad\qquad\qquad\qquad (1)
 $$ It can be induced to the representation $\,\mbox{Ind}_K^GD\,$ on the subspace of functions obeying $$
 \varphi(xk^{-1})=D(k)\varphi(x)~.~~~\qquad\qquad\qquad\qquad\qquad\qquad\qquad (2)
 $$ We now restrict the induced representation to a subgroup $\,B<G\,$ . Denoted with $$
 U(B)~\equiv~\left(D(K)~\uparrow~G\right)~\downarrow~B~~,\quad\quad\quad\qquad\qquad\qquad\qquad (3)
 $$ the new representation will be realised with the left translations $$
 U(b)~\varphi(g)~=~\varphi(b^{-1}g)~~.~\qquad\qquad\qquad\qquad\qquad\qquad\qquad (4)
 $$ This representation fixes the double cosets $\,Bu_iK\,$ . So the expansion $$
 G\,=\,\,\bigcup_i{\cal{U}}_{\,i}\,=\,\,\bigcup_iB\, u_i\, K\,\;\qquad\qquad\qquad\qquad\qquad\quad\quad~ (5)
 $$ yields a decomposition of $\,U(B)\,$ into subrepresentations: $$
 U(b^{\,\prime})\,\varphi(b\, u\, k)~=~\varphi({b^{\,\prime}}^{-\, 1}\, b\,u\, k)~~,\qquad\qquad\qquad\qquad\qquad (6)
 $$ where $~~b\, u\, k~$ and $~{b^{\,\prime}}^{-\, 1} b\, u\, k~$ belong to the same $~B\, u\, K\,$ . Define the projection operator $\,P_u\,$ so that a function $\,P_u\varphi\,$ coincides with $\,\varphi\,$ on $\,B\, u\, K\,$ and is zero outside it.
 Then interpret $\,\varphi(g)\,$ as a set of functions $\,\varphi_u\,$ on double cosets: $$
 \varphi~=~\left\{\,\varphi_u\,\right\}~~,\qquad\varphi_u\,=~P_u\,\varphi~~.\qquad\qquad\qquad\qquad\quad (8)
 $$ A function $\,\varphi_u\,$ is fully defined by its values on $\,B\, u\,$ , owing to (2) which now becomes $$
 \varphi_u(b\, u\, k)=D(k^{-1})\varphi_u(b\, u)~\,.\qquad\qquad\qquad\qquad\qquad~~~ (9)
 $$ Defining $$
 q\,\equiv\, u\, k\, u^{-1}\,\in\, K_u\qquad\qquad\qquad\qquad\qquad (10)
 $$ and introducing a representation of $\,K_u\,$ $$
 D_u(q)\,\equiv\, D(u^{-1}\, q\, u)~~,~~\qquad\qquad\qquad\qquad (11)
 $$ we can also write the subsidiary condition on $\,\varphi\,$ as a subsidiary condition $$
 \Phi_u(b\, q)~=~D^{-1}_u(q)~\Phi_u(b)\qquad\qquad\qquad\qquad (12)
 $$ on a new function $$
 \Phi_u(b\, q)~\equiv\,\varphi_u(b\, q\, u)\qquad\qquad\qquad\qquad\qquad (13)
 $$ acting as $$
 \Phi_u\,:~~~B\,K_u~\longrightarrow~{\cal{L}}~~.~~\qquad\qquad\qquad\qquad (14)
 $$ Thus, our subrepresentation becomes $\;
 D_u(K_u)\uparrow B
 \;$ , while the entire representation is $$
 \left(D(K)~\uparrow~G\right)~\downarrow~B~=~\int_{B\backslash G/K}du ~D_u(K_u)\uparrow B
 ~,\qquad\qquad\qquad (15)
 $$ where the integration goes over all distinct double cosets. Strangely, this is not what I see in the books. E.g., the Mathematical Encyclopedia says that https://www.encyclopediaofmath.org/index.php/Induced_representation $$
 \left(D(K)~\uparrow~G\right)~\downarrow~B~=~\int_{B\backslash G/K}du ~(D_u(K_u)\downarrow L_u)\uparrow B
 ~,\qquad~~~ (16)
 $$ where $$
L_u~\equiv~K_u\cap B~=~u~K~u^{-1}\cap B~~.
 $$ My question is: why $\,~\downarrow L_u\,$ ? This restriction is equivalent to an assumption that in condition (12) we must restrict, by hand, $\,q\,$ from $\,K_u\,$ to $\,L_u\,$ . Or to enforce an equivalent restriction of $\,k\,$ in $\,D(K)\,$ . Or to restrict the domain of $\,\Phi\,$ to $\,B\,$ in (12). Why should we do that? The desired restriction of the induced representation to $\,B\,$ implies that the left translations must be performed by the elements $\,b\in B\,$ . I, however, see no reason why we also should restrict to $\,B\,$ the domain of the functions $\,\Phi\,$ on which this restricted representation acts. PS. A possible clue to this problem may lie in the following observation. The subgroup $\,L_u\,$ and the quotient space $\,B/L_u\,$ naturally emerge when we split a double coset $\,BuK\,$ into left cosets $\,\textsf{b}uK\,$ . Indeed, if some $\,b\,$ and $\,b_1\,$ lie in the same left coset, $$
b~u~K~=~b_1~u~K~~,
$$ then $$
b~K_u~=~b_1~K_u~~~
$$ or, equivalently, $$
b_1^{-1}b~K_u~=~K_u~~.
$$ This obviously entails $\,b_1^{-1}b\,\in\,K_u\,$ , wherefrom $$
b^{-1}_1b\,\in\,L_u~~.
$$ So the left cosets $\,b\, u\, K\,$ and $\,b_1 u\, K\,$ coincide iff $\,b\,$ and $\,b_1\,$ belong to the same left coset in the quotient space $\,B/L_u\;$ , $\,$ with some  representative $\,\textsf{b}\,$ : $$
 b\,,\,b_1\,\in\,\textsf{b}\,{{L}}_{u}\;\,,\;\;\,\textsf{b}\,\in\,B
$$ Thus, as many left cosets $\,\textsf{b}\, L_u\,$ in the quotient space $\,B/L_u\,$ so many left cosets $\,X\,=\, \textsf{b}\, u\, K\,$ in the double coset $\,{\cal{U}}=BuK\,$ . We also can say that if $\,b\,$ lies in the left coset $\,b\, u\, K\,$ , then $$
b~=~\textsf{b}~l~~,~~~l\in L_u~~,
$$ where $\,\textsf{b}\,$ is a representative of the left coset $\,\textsf{b}\,L_u\,$ .
Thence, $$
\Phi_u(b)~=~D_u^{-1}(l)~\Phi_u(\textsf{b})~~,~~~~~~~~~~~~~~~~~~~~~~~~~~~~\qquad\qquad(17)
$$ so the function $\,\Phi_u\,$ is fully defined by its values on a section of the quotient space $\,B/L_u\,$ . This could have answered my question if not for one circumstance: in eqn (12) we still have $\,q\,$ which lies in $\,K_u\,$ , not in $\,L_u\,$ . So equation (17) becomes $$
\Phi(bq)~=~D_u^{-1}(lq)~\Phi_u(\textsf{b})~~,
$$ and the subrepresentation is defined not on $\,L_u\,$ but on $\,K_u\,$ -- unless we fix this `` by hand'' (but on what grounds?)","['group-theory', 'representation-theory', 'lie-groups']"
3396612,Strong Law of Zero Mean i.i.d Random Variables with a Bounded Sequence of Non-Random Constants,"This question regards Theorem 1.8.6 on Durrett page 52 which states as (The strong law of large numbers) Let $X_{1}, X_{2},\cdots$ be i.i.d random variables with $E|X_{i}|<\infty$ . Let $E(X_{i})=\mu$ and $S_{n}=X_{1}+\cdots+X_{n}$ . Then $S_{n}/n\longrightarrow \mu$ a.s. The proof it requires at least two lemmas and Kolmogorov's One Series Theorem, I have read through them without problems. However, I am thinking about a little transformation of this law. What if we set $E(X_{i})=0$ and there is a bounded sequence of non-random constants $c_{n}$ and we let $S_{n}:=c_{1}X_{1}+\cdots+c_{n}X_{n}$ , will $S_{n}/n$ converges to $0$ almost surely? That is: Let $X_{1},X_{2},\cdots$ be i.i.d integrable random variables with $E(X_{i})=0$ . If $c_{n}$ is a bounded sequence of non-random constants, and we set $S_{n}:=c_{1}X_{1}+\cdots+c_{n}X_{n}$ , show that $S_{n}/n\longrightarrow 0$ a.s. I've been thinking about using the similar proof of the strong law , since the strong law is just let $c_{n}=1$ for all $n$ , and $\mu\neq 0$ . So firstly I tried to directly make $Y_{n}:=c_{n}X_{n}$ for each $n$ , and argue just for $Y_{n}$ . The good thing here is that $E(Y_{n})=0$ so we don't change the limit in the almost sure convergence, however, since $c_{n}$ are different, even though $Y_{n}$ 's are still independent, they are not identically distributed any longer. How could I make all those different $c_{n}$ to be one thing? (so that in this way they are i.i.d again). Or perhaps I am heading on a wrong direction? Thank you in advance for any discussion, hint, or solution! Edit 1: Since $c_{n}$ is bounded, $c_{n}\leq M$ for all $n$ . Thus, if replace $Y_{n}:=MX_{n}$ , and write $Z_{n}:=Y_{1}+\cdots+Y_{n}$ then they are still i.i.d, and then I can definitely show that $$\dfrac{Z_{n}}{n}\longrightarrow\mu\ \text{a.s.}$$ Now, note that $S_{n}:=c_{1}X_{1}+\cdots+c_{n}X_{n}\leq Z_{n}$ , so this problem can be reduced to if if $Z_{n}/n\longrightarrow 0$ a.s. and $Z_{n}/n\geq S_{n}/n$ , then $S_{n}/n\longrightarrow 0$ a.s. I don't really know if this is true... If it is, how could I prove it? Edit 2: Okay I figured it out. The point here is that even though $c_{k}X_{k}$ is not i.i.d, you can still use i.i.d when you have $P(|c_{k}X_{k}|>n)$ , since you can directly divided by $|c_{k}|$ . Since it is bounded, everything will be fine. For details, please see my answer of my own post. Edit 3: Since I noticed that some users voted me and favorite this post during me writing the proof in my answer, I make an edit here to let the system alert you that there is an edit so that you could see my answer. Thank you for your vote and favorite :) Enjoy my proof!","['alternative-proof', 'law-of-large-numbers', 'probability-theory']"
3396655,Why is the Rademacher complexity/distribution named after Hans Rademacher?,"In machine learning theory, the Rademacher complexity of a function class $\newcommand{\cF}{\mathcal{F}}\newcommand{\E}{\mathbb{E}}\newcommand{\R}{\mathbb{R}} \cF: X \mapsto \R$ over a particular set of inputs $x_{1:n} \in X^n$ is defined as $$
\operatorname{R}(F, x_{1:n}) 
= 
\frac{1}{n}
   \E \left[
   \sup_{f \in F}
   \sum_{i=1}^m \sigma_i f(x_i) 
\right],
$$ where $\sigma_i$ is a random variable distributed uniformly over $\{-1, +1\}$ . In machine learning literature [1], the Rademacher complexity is defined without etymology. The variable $\sigma_i$ is called a Rademacher-distributed random variable . However, in a biography of Rademacher [2], the words ""distribution"" and ""random variable"" do not occur. It seems that Rademacher was mainly a number theorist. Why do these objects bear his name? From what I can tell, Rademacher did not introduce the definition $R$ above, and it seems strange to name $R$ after the distribution of $\sigma_i$ when $R$ itself is such a rich construct. [1] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning . 2nd edition, MIT Press, 2018. https://www.dropbox.com/s/7voitv0vt24c88s/10290.pdf?dl=1 [2] Bruce C. Berndt. ""Hans Rademacher (1892–1969)."" Acta Arithmetica LXI.3 , 1992. http://matwbn.icm.edu.pl/ksiazki/aa/aa61/aa6131.pdf .","['rademacher-distribution', 'math-history', 'probability', 'terminology']"
3396659,"MLE, convergence in probability","The MLE $\tilde{\theta}_n$ of a sample of random variables $X_i$ from parametric model $\{f(x,\theta): x\in\mathbb{R}, \theta \in \Theta\}$ is called consistent if $\tilde{\theta}_n$ converges in probability to $\theta_0$ , i.e. $\tilde{\theta}_n$ $\xrightarrow{P}$ $\theta_0$ , whenever $X_i$ are generated from $ f(x,\theta_0)$ . My question is regarding the definition of convergence in probability of estimators . I know that the MLE $\tilde{\theta}_n$ is itself a random variable (i.e. a measurable function). So, does $\tilde{\theta}_n$ $\xrightarrow{P}$ $\theta_0$ mean that $\tilde{\theta}_n$ converges in probability to $\theta_0$ when viewed as a measurable function? That is, does convergence in probability of an estimator mean the following: $$ \tilde{\theta}_n \xrightarrow{P} \theta_0 \Leftrightarrow \mathbb{P}(\{x \in \mathbb{R}: |\tilde{\theta}_n(x)-\theta_0|\ge \epsilon \})\rightarrow 0 $$ The confusion comes from the fact that everywhere I read about the consistency of the MLE, they seem to treat the sequence ( $\tilde{\theta}_n$ ) like a sequence of real numbers, but then convergence in probability $\tilde{\theta}_n$ $\xrightarrow{P}$ $\theta_0$ , only makes sense when talking about measurable functions. Can anyone please help me clarify the above. Thank you very much.","['statistics', 'measure-theory', 'maximum-likelihood']"
3396664,Is the function $f(x)=\int_0^{\infty} \left[ 1 - a^x \sin \left( \frac{1}{a^x} \right) \right] da$ writable in a nicer way?,"I was idly exploring properties of this strange function exploiting wolframalpha and I can't understand it. Some calculus seems to show that this function is defined only for $x>\frac{1}{2}$ (if $x \le \frac{1}{2}$ the integral diverges) and has a minimum near $1$ (maybe in $1$ ?). Maybe for bigger $x$ the function has limit $1$ . For $x > 1$ , $f(x)$ seems takes the form $\xi(x) \Gamma \left( 1 - \frac{1}{x} \right)$ (but $\Gamma$ function is surely present even in points smaller than 1, for example $f \left( \frac{3}{5} \right) = \frac{9\sqrt{3}}{32} \Gamma \left( \frac{1}{3} \right) \approx 1,3 $ ) where $\xi(x)$ satisfies $\xi \left( \frac{4}{3} \right) = \frac{2}{7} \sqrt{2 - \sqrt{2}} $ $\xi \left( \frac{3}{2} \right) =\frac{3}{10}$ $\xi \left( 2 \right) = \frac{\sqrt{2}}{3} $ $\xi \left( 3 \right) = \frac{3\sqrt{3}}{8} $ $\xi \left( 4 \right) = \frac{2}{5} \sqrt{2 + \sqrt{2}} $ $\xi \left( 5 \right) = \frac{5}{12} \sqrt{\frac{5+\sqrt{5}}{2}} $ $\xi \left( 6 \right) = \frac{3}{7} \sqrt{2+\sqrt{3}} $ ... I can't see a pattern and I have no idea of how wolframalpha was able to solve these integrals (but it can't solve the general integral, it solves only if I assign a value to $x$ ). In addiction $f(1)$ has the simple value $\frac{\pi}{4}$ , this suggests an charming way of seeing $\pi$ : it is the area under this pathological function","['integration', 'pi', 'gamma-function']"
3396686,"Sample all elements from a set at least once, with replacement","I've seen the Coupon collector's problem and I believe this is a variant of it but I can't quite wrap my head around it. This is not a homework assignment. I have a set of k elements. I randomly sample s elements from the set and then replace them. If I do this n times, what is the probability that I will have sampled every single element at least once?","['statistics', 'combinatorics', 'coupon-collector', 'probability']"
3396711,limit of sum of little-oh functions,"I am wondering if what I show below is allowed, and why it is allowed. Assume you have an interval $[0,T]$ . For each $n$ you divide the
  interval in $2^n$ n parts. In each interval you have different functions $o(\Delta t)$ . We have the summation $\sum\limits_{i=1}^{2^n}o(\Delta t)=2^no(\Delta t)=T\frac{o(\Delta
 t)}{\frac{T}{2^n}}=T\frac{o(\Delta t)}{\Delta t}$ . So this goes to zero as n goes to infinity. I have seen this argumentation in a book. Is it allowed or do we need more constraints on the $o(\Delta t)$ functions? The problems I have are: Let n be given, the $o(\Delta t)$ functions may be different, so can we just say they are $2^n$ times an $o(\Delta t)$ function? When $n$ increases, we get new $o(\Delta t)$ -functions. Does this affect the result? For the limit to go to zero as $n$ increases we need it to be the same $o(\Delta t)$ -function? So does the result hold, or are there counterexamples where they don't hold? Do we need more conditions on the functions for it to hold? The argument is on page 50(printed page) or page 104-105(scroll-meny page) on the link here: https://www.nb.no/items/URN:NBN:no-nb_digibok_2011011705004?page=103 UPDATE: I think the statement is false. Assume for each $i,n$ we have functions $o_{i,n}(\Delta t)$ . They are defined by $o_{i,i}(T/2^i)=1$ , and zero for every other combination of $i$ and $n$ and $\Delta t$ . Then these functions are $o(\Delta t)$ because they are identically zero when $\Delta t$ gets small enough. So: $\sum\limits_{i=1}^{2^n}o_{i,n}(T/2^n)\ge o_{n,n}(T/2^n)=1.$ Is this argument correct? If so, is there a reason why it would work in the link I provided?","['approximation', 'real-analysis', 'calculus', 'limits', 'probability-theory']"
3396773,"Functions which converge pointwise to f, but $\int_E f_n$ does not converge to $\int_E f$ for some measurable $E$?","We have that statement that if $\{f_n\}$ are positive real-valued measurable functions $X \rightarrow \mathbb{R}$ such that $f_n$ converges pointwise to $f$ everywhere on $X$ and $\lim \int f_n = \int f < \infty$ , then $\lim \int_E f_n = \int_E f$ for every measurable $E$ . There is a solution given here: Prove that $\int_E fd\mu = \lim \int_E f_n d\mu$ for all measurable set $E$ My question is, why does this fail if $\int f = \infty$ ? Specifically, what is a sequence of measurable functions $f_n$ which converge pointwise to $f$ with $\lim \int f_n = \int f = \infty$ , and a measurable function $E$ , such that $\lim \int_E f_n < \int_E f$ ? I know that the other direction must always hold, since we do not use the finiteness condition to prove that inequality. However, I cannot find any examples where the strict inequality in the other direction holds.","['measure-theory', 'real-analysis']"
3396820,How to evaluate $\int_0^1\frac{\ln x\ln(1+x^2)}{1-x^2}dx$ in an elegant way?,"How to prove that: $$\int_0^1\frac{\ln x\ln(1+x^2)}{1-x^2}dx=\frac74\zeta(3)-\frac34\ln2 \zeta(2)-\frac{\pi}{2}G$$ where $\zeta$ is the Riemann zeta function and $G$ is Catalan constant. I came across this integral while working on evaluating some harmonic series. I am tagging ""harmonic series"" as its pretty related to logarithmic integrals.","['integration', 'definite-integrals', 'real-analysis', 'harmonic-numbers', 'calculus']"
3396882,Proving expectation to be infinite with an inequality,"Let $X$ be a non-negative random variable, and suppose that $P(X \geq
 n) \geq 1/n$ for each $n \in \mathbb{N}$ . Prove that $E(X) = \infty$ . I have been stuck with this problem for a few days now. I guess it can make some sense intuitively because you have some probability mass everywhere, and we're looking at probability of it being greater than some value. I tried to use inequalities like Markov's and Chebyshev's with no luck. I was hoping if someone can please explain to me how to answer this problem. It is coming from an introductory probability with measure theory  book, and I am trying my best to get better at these kind of problems.","['measure-theory', 'probability-distributions', 'expected-value', 'probability-theory', 'probability']"
3396906,Find all solutions in modular arithmetic,"I need to find all solutions to: $$4x\equiv3\pmod7$$ I know the solutions are in ${0, 1, 2, 3, 4, 5, 6}$ and I got $x \equiv 6 \pmod7$ so my answer was 6 but I don't know if that's all the answers. I have the same problem with: $$3x+1\equiv4\pmod5$$ I got $x\equiv1\pmod5$ so my answer was $1$ since it is in ${0, 1, 2, 3, 4}$ .","['abstract-algebra', 'modular-arithmetic', 'discrete-mathematics']"
3396909,Flat extensions of group rings,"Let $R$ be a commutative ring, $f:H\to G$ a surjective group homomorphism and consider $RG$ as a $(RG,RH)$ -module via $g\cdot h := g\cdot f(h)$ as usual. Now suppose that $RG$ is flat over $H$ , meaning that $$RG\otimes -:H\text{-}\mathbf{Mod} \to G\text{-}\mathbf{Mod}$$ is exact. What can we say about $f$ ? If $RG$ was even projective over $H$ , we would get a section $s:RG\to RH$ telling us that $\#\mathrm{ker}(f)<\infty$ is a unit in $R$ , but I suppose, something like this does not work if $RG$ is only assumed to be flat?","['group-rings', 'representation-theory', 'group-theory', 'linear-algebra']"
3396958,"Indefinite integral $\int \frac{1}{2-\cos(x)}\,dx$ has discontinuities. How to fix?","Using the standard tangent half-angle substitution, we get $$\int \frac{1}{2-\cos(x)}\,dx = \frac{2}{\sqrt{3}}\tan^{-1}(\sqrt{3}\tan\frac{x}{2}) + C$$ The resulting antiderivatives are piecewise continuous functions with discontinuities at $x = (2k+1)\pi$ . However, the integrand is continuous everywhere, so any antiderivative must be continuous everywhere (??) according to FTC. So, how do we deal with this problem? What is the correct form of the antiderivatives? Is there a form that avoids the discontinuities?","['integration', 'indefinite-integrals', 'calculus', 'real-analysis']"
3396961,Inverse of x^x? [duplicate],"This question already has answers here : Inverse function of $x^x$ (3 answers) Closed 4 years ago . What is the inverse of $y = x^x$ ? I know it won't be in terms of elementary functions, but I believe some function analogous to Lambert W function for $z = ye^y$ would be defined? I couldn't find it on google.","['functions', 'inverse-function']"
3397015,Deriving continuity equation from time component of $T_{\nu ; \mu}^{\mu}=0$,"I know that $T_{\nu ; \mu}^{\mu}=0$ . I want to derive the continuity equation from examining the time component in the Newtonian limit. I have $$T_{; \nu}^{t \nu}=\rho_{0} U^{\nu} U_{, \nu}^{t}+\rho_{0} U^{\nu} \Gamma_{\gamma \nu}^{t} U^{\gamma}+p_{, \nu}\left(g^{t \nu}+U^{t} U^{\nu}\right)$$ but I am not sure how to get $$\frac{\partial \rho}{\partial t}+\vec{\nabla} \cdot(\rho \vec{v})=0$$ from the above terms.","['fluid-dynamics', 'general-relativity', 'differential-geometry']"
3397053,"What's the correct definition of ""conjugate"", and do we identify them?","What does a ""conjugate"" in math mean? I know the definition says if we have $x+y$ , then $x-y$ is its conjugate. So, $5+ \sqrt{5}$ and $5-\sqrt{5}$ are conjugate of one another. 
Are $8$ and $2$ conjugate of one another as they can be $5+3$ and $5-3$ ? Now, $5$ and $\sqrt{2}$ are not conjugate. How? We can write them like $\frac{5 + \sqrt{2}+(5-\sqrt{2})}{2}$ and $\frac{5 + \sqrt{2}-(5-\sqrt{2})}{2}$ .
Now, they are. So, what's the concept?","['algebra-precalculus', 'terminology']"
3397057,Probability and marbles,"My brother brings a certain number of his marbles to play with in my room. Each marble is distinct. He has 8 total marbles that are either red or blue. One day, I spotted two red marbles in my room. The probability that any two of his marbles (of those that he plays in my room), randomly chosen, both being red is 1/2. How many marbles does he bring into my room? I tried doing this: let x = number of red marbles So $(x/8)$ = probability of picking red marble and then $(x-1)/(8 - 1)$ = probability of picking second red marble. $(x/8)(x-1)/(7) = 1/2$ , but I got x to be a decimal which is not possible. EDIT: 
I kept guessing and checking $\frac{x}{b}\cdot\frac{x-1}{b-1}=\frac{1}{2}$ , where $x =$ number of red balls, and $b=$ number of balls he brings into my room to get that $b=4$ and $x=3$ , but unsure how to get this solution formally.",['probability']
3397086,How large can the set of discontinuous points of a coordinately continuous function be?,"I temporarily say function $f(x,y)$ (from $\mathbb{R}^2$ to $\mathbb{R}$ ) is coordinately continuous (shortly as c.c.) iff it is continuous everywhere regarded as an single-variable function while another coordinate is given. I am curious about how large the set of uncontinuous (in sense of $\mathbb{R}^2$ ) points of a c.c. function can be . It is not hard to see that there are c.c. functions which is not continuous at one point. For example, $f(x,y)= 1−4(xy/(x^2+y^2))^2$ is not continuous at $(0,0)$ , which can be seen from the form of polar coordinate $1 - \sin(2\theta)^2$ except $(0,0)$ . Intuitionly, I think discontinuity while maintaining c.c. property needs a well designed neighbor. Therefore I think the set of discontinuity points is not dense. It has been proved that such function cannot be discontinuous everywhere: separately continuous functions $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ but nowhere continuous","['continuity', 'functions', 'real-analysis']"
3397132,Inverse of $y=\frac{\sin^2(x)}{x^2}$?,"I'm having trouble finding the inverse of $y=\dfrac{\sin^2(x)}{x^2}$ . So far what I have is $$\begin{align}
\frac{\sin^2(y)}{y^2} &= x \\[4pt]
\sin^2(y) &= xy^2 \\[4pt]
\sin(y) &= \sqrt{xy^2} = \sqrt{x}y \\[4pt]
y &= \sin^{-1}(\sqrt{x}y)
\end{align}$$ But I don't know how to take it any further than that. Does an inverse exist?","['trigonometry', 'inverse-function']"
3397146,Proposition: Al knows only Bill,"This is an example from my textbook: Translate the
  proposition ""Al knows only Bill"" into symbolic form. Let’s use K(x, y)
  for the predicate x knows y. The translation would be K(Al, Bill) ∧ ∀x
  (K(Al, x) → (x = Bill)). Why do we need: K(Al, Bill) isn't this enough: ∀x(K(Al, x) → (x = Bill)) ?","['predicate-logic', 'discrete-mathematics', 'logic-translation']"
3397150,A sudoku-like grid with pairs of numbers,"It's easy to generate a 12x12 grid such that: Each row contains the numbers 1 - 12 Each column contains the numbers 1 - 12 No row or column contains the same number twice I am trying to determine if it's possible to generate a 12x12 grid where each cell contains two numbers -- A RED number between 1 and 12 and a BLUE number between 1 and 12 -- for which the following is true: If you consider ONLY the RED numbers, the grid satisfies the three conditions above If you consider ONLY the BLUE numbers, the grid satisfies the three conditions above Every one of the 144 blue-red combinations is present in the grid. I am pretty certain it's possible because I can do it with 3x3 and 4x4. Here is the 4x4, for example: How can I generate the 12x12 grid? UPDATE: Arthur's answer below gave me enough info to create one. I am curious about an algorithm though. I'm guessing there is one based on the fact that this problem appears to fall into a documented category of problems (thanks Lord Shark the Unknown for point it out!!): https://en.wikipedia.org/wiki/Graeco-Latin_square If anybody finds an algorithm I would love to see it.","['permutations', 'combinatorics', 'latin-square', 'orthogonality']"
3397156,Jacobi Symbol: $\sum_{n=1}^{p}\left(\sum_{m=1}^{h}\left(\frac{m+n}{p}\right)\right)^2=h(p-h)$,"Show that if $p$ is and odd prime and $h$ is an integer, $1\le h \le p$ , then $$\displaystyle\sum_{n=1}^{p}\left(\sum_{m=1}^{h}\left(\frac{m+n}{p}\right)\right)^2=h(p-h)$$ where $\left(\frac{m+n}{p}\right)$ denotes the Jacobi symbol. My solution: For $h=1$ , we have $\left(\frac{1+n}{p}\right)^2$ is always 1 or 0. It is 0 only when $n=p-1$ . So the sum comes out to be $p-1$ , which is accordance with $h(p-h)$ .
Similarly, for $h=p$ , and the sum is zero. But I am having trouble when $ h \neq 1 or p$ , how will I proceed in that case? This question has been taken from the book : An introduction to theory of numbers by Niven, Zuckerman, Montgomery. Section 3.3., question 19. Thanks in advance.","['legendre-symbol', 'number-theory', 'elementary-number-theory', 'jacobi-symbol', 'quadratic-reciprocity']"
3397208,Does $\lim_{s\to \infty}F(s)=0$ for all Laplace transforms?,"Let $f(t)$ be a piece-wise continous function of exponential order $\alpha$ .
Then $F(s)$ exists. I must prove then that $\lim_{s\to\infty} F(s)=0$ but i have no idea on how to do it. I tried to prove it by the $\varepsilon,\delta$ definition of limits, using the piece-wise continous and exponential order properties of $f(t)$ , but didn't reach the result. Perhaps I'm doing something wrong? My definition of  the limit would be that for every $\varepsilon>0\ \exists\ \delta>0$ such that $|F(s)|<\varepsilon$ for every $s\geq\delta$ .","['laplace-transform', 'ordinary-differential-equations']"
3397256,What's an example where the inclusion map $\iota: A \to B$ is smooth and a topological embedding but not an immersion?,"Context: 1. Are manifold subsets submanifolds? 2. Can manifold subsets always be made into submanifolds? 3. Why is the inclusion from a submanifold smooth? Let $A,B$ be topological spaces with $A \subseteq B$ and $A$ a topological subspace of $B$ . Suppose $A$ and $B$ become smooth manifolds $(A,\mathscr A)$ and $(B,\mathscr B)$ with respectively with dimensions $a$ and $b$ . What's an example where the inclusion map $\iota: (A,\mathscr A) \to (B,\mathscr B)$ is smooth and a topological embedding but not an immersion? There are a lot of examples of smooth immersions that are not topological embeddings (and thus not smooth embeddings) like this . There are also examples of smooth topological embeddings that are not immersions (and thus, again, not smooth embeddings) like this . In some of the questions linked above, there were examples that where $\iota$ wasn't smooth or even continuous. The purpose of this question is to ask specifically about the inclusion map and the case that the inclusion map is smooth. If such examples exists, then this tells me there's nothing particularly different about the inclusion map. What's an example where the inclusion map $\iota: (A,\mathscr A) \to (B,\mathscr B)$ is smooth but not a topological embedding? Just checking my understanding. If there are no such examples, then (1) could simply ask ""smooth but not immersion"".","['smooth-manifolds', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
3397285,How to know if a curve is plane without calculating its torsion,"Given $$\alpha(t)=\left(t,\frac{1+t}{t},\frac{1-t^2}{t}\right)$$ I want to know if there is way of knowing if this curve is plane or not without calculating its torsion. I considered the option of trying to know if its contained in a plane. But I don't know how to proceed. Any ideas? Thanks in advance.",['differential-geometry']
3397303,f(X) and f(Y) are identically distributed,"Suppose that $X$ is a random variable defined on a probability space ( Ω , $\mathcal F$ , P ) and $P_X$ is the probability measure on  ( $\mathbb{R} $ , $\mathcal R$ ) and $Y$ is a random variable defined on  ( Ω' , $\mathcal F'$ , P' ) IF $ X $ and $Y $ are identically distributed If $f:$ $\mathbb{R} $ $->$ $\mathbb{R} $ is Borel Measurable, And $X$ and $Y$ are identically distributed, then if we define $U = f(X)$ and $V= f(Y)$ . $U$ and $V$ are also identically distributed (Tried Solution): After many hints and many hours all I manage to do is the below : We know $P_x$ $=$ $P_Y$ and $P_X(B)$ , $B \in \mathcal R$ $= $$P_Y(B)$ , $B \in \mathcal R$ $P(X^{-1}(B)) = P(Y^{-1}(B))$ $ P(\omega\in Ω : X(ω) \in B)  =   P(\omega\in Ω' : Y(ω) \in B)$ Somehow we have to show that the above is equal to the below:. $$......$$ $$......$$ $ P(\omega\in Ω : f o X(ω) \in B)  =   P(\omega\in Ω' : foY(ω) \in B)$ Hence, $ P(\omega\in Ω : U(ω) \in B)  =   P(\omega\in Ω' :V(ω) \in B)$ Thus, $U = f(X)$ and $V= f(Y)$ are identically distributed . Could you please explain the solution thoroughly, because I really want to understand the solution.","['measure-theory', 'probability-theory', 'borel-measures']"
3397326,Limit of an inverse function,"Let $f:\mathbb R\to \mathbb  R$ be an invertible function such that $$\lim_{x\to a} f(x)=b$$ for some $a,b\in \mathbb R$ . Does it follow that $$\lim_{x\to b}f^{-1}(x)= a,$$ where $f^{-1}$ denotes the inverse function of $f$ ? Edit: When I consider the $\epsilon,\delta$ -definition of the limit, I feel that there should be an example that $\lim_{x\to b}f^{-1}(x)\neq a$ due to the fact that $\epsilon,\delta$ -definition is not symmetric (for a given $\epsilon>0$ , we find $\delta>0$ such that ....). However, if we further assume that $f$ is cont., $$b=\lim_{x\to b}x=\lim_{x\to b}f\circ f^{-1}(x)=f(\lim_{x\to b}  f^{-1}(x)).$$ It follows that $\lim_{x\to b}  f^{-1}(x)=f^{-1}(b)=a$ . Thus, one needs a discontinuous function to have a counter example. I wonder whether there is any simple function with this property. @Floris Claassens'a answer shows that there are some ""ugly functions"" with this property.","['limits', 'calculus']"
3397331,Probability of euclidean distance between two random points inside a unit circle/sphere greater than 1,"Problem: Say there are two points inside the circle; A and B , and they are both randomly drawn according to a uniform distribution where the boundary is the circumference of the unit circle/the surface of the unit sphere. What's the probability that the euclidean distance between two randomly drawn points inside a unit circle/sphere greater than 1? This question has two versions; the 2D one and the 3D one. I have almost gotten down the expression of the integral in 2D one, but I still get stuck at the late stage of the problem, I haven't tried the 3-D version just yet, but I guess I will get stuck at a similar stage. The following is my attempt on the 2-D version of the problem: Phase 1: for the sake of simplicity, we can ""fix"" the angle θ of A to a particular fixed value θa and only vary its r value in the polar coordinate system, so A could be (0,θa), (0.2,θa), (1,θa) etc. 

For B, we can vary everything including radius r and the angle θ of another point `B`. Phase2: The required probability should be equal to the sum of all of the conditional probability from r=0 to r= 1, where each increment of r is very very small: ΣP{|A - B| > 1 | A= (r, θa)  } Upon taking this limiting process to the sum, this becomes a definite integral over the conditional probability density function from r=0 to r=1. This is where I got stuck, I don't know how to transform the conditional probability to the conditional pdf inside the definite integral and possibly integrate it. And after this 2-D version, the 3-D version is gonna be another beast that I need help in order to deal with that. Note: These are the pictures of my drafts and my guesses, not sure whether they are helpful.","['definite-integrals', 'geometric-probability', 'conditional-probability', 'probability-distributions', 'probability']"
3397348,Express $\partial_x$ by $\partial_r$ and $\partial_{\phi}$,"I want to express $\partial_x$ and $\partial_y$ by $\partial_r$ and $\partial_{\phi}$ . From $$ \frac{\partial }{\partial r} = \frac{\partial x}{\partial r}\frac{\partial }{\partial x} + \frac{\partial y}{\partial r}\frac{\partial }{\partial y}$$ $$ \frac{\partial }{\partial \phi} = \frac{\partial x}{\partial \phi}\frac{\partial }{\partial x} + \frac{\partial y}{\partial \phi}\frac{\partial }{\partial y}$$ , substituting $x=r\cos\phi$ and $y=r\sin\phi$ , we get follows. $$ \frac{\partial }{\partial r} = \cos{\phi}\frac{\partial }{\partial x} +\sin\phi\frac{\partial }{\partial y}$$ $$ \frac{\partial }{\partial \phi} = -r\sin{\phi}\frac{\partial }{\partial x} + r\cos\phi\frac{\partial }{\partial y}$$ Rewrite this by matrix as below. $$
\begin{pmatrix}
\frac{\partial }{\partial r}  \\
\frac{1}{r}\frac{\partial }{\partial \phi} \\
\end{pmatrix}=
\begin{pmatrix}
\cos\phi & \sin\phi \\
-\sin\phi & \cos\phi \\
\end{pmatrix}
\begin{pmatrix}
\frac{\partial }{\partial x}  \\
\frac{\partial }{\partial y} \\
\end{pmatrix}$$ $$\Leftrightarrow 
\begin{pmatrix}
\frac{\partial }{\partial x}  \\
\frac{\partial }{\partial y} \\
\end{pmatrix}=
\begin{pmatrix}
\cos\phi & -\sin\phi \\
\sin\phi & \cos\phi \\
\end{pmatrix}
\begin{pmatrix}
\frac{\partial }{\partial r}  \\
\frac{1}{r}\frac{\partial }{\partial \phi} \\
\end{pmatrix}$$ Then, we can express $\partial_x$ and $\partial_y$ by $\partial_r$ and $\partial_\phi$ , we get follows. $$ \frac{\partial }{\partial x} = \cos\phi\frac{\partial }{\partial r} - \frac{\sin \phi}{r}\frac{\partial }{\partial \phi}$$ $$ \frac{\partial }{\partial y} = \sin\phi\frac{\partial }{\partial r} + \frac{\cos\phi}{r}\frac{\partial }{\partial \phi}$$ However, if we directly calculate $\partial_x$ and $\partial_y$ from $ \frac{\partial }{\partial x} = \frac{\partial r}{\partial x}\frac{\partial }{\partial r} + \frac{\partial \phi}{\partial x}\frac{\partial }{\partial \phi}$ and $ \frac{\partial }{\partial y} = \frac{\partial r}{\partial y}\frac{\partial }{\partial r} + \frac{\partial \phi}{\partial y}\frac{\partial }{\partial \phi}$ ,
result does not meet with each other.
For example, \begin{align}
 &\frac{\partial \phi}{\partial x} \\
=&\frac{1}{r}\frac{\partial \phi}{\partial \cos\phi} \\
=&\frac{1}{r\sin\phi}
\end{align} Here, I use the relationship, $\frac{\partial f(x)}{\partial x} = \left(\frac{\partial x}{\partial f(x)}\right)^{-1}$ . However, from the first calculation, this $\frac{\partial \phi}{\partial x}$ should be equal to $-\frac{\sin\phi}{r}$ . 
What is the origin of this contradiction?
I find this error (many times) when I'm scoring freshman's physics class test as a TA, however I cannot nicely explain why such latter calculation fails.","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
3397395,Extensions of $FP_{n}$ groups,"I am reading a paper where they write the following: Since $S_{0}$ is a group of type $FP_{n}(\mathbb{Q})$ and there is a series $$S_{0}\triangleleft S_{1}\triangleleft \cdots \triangleleft S_{l}=T$$ where each $S_{i+1}/S_{i}$ is finite or infinite cyclic, then by the obvious induction, $T$ is of type $FP_{n}(\mathbb{Q})$ . I understand that if $S_{i+1}/S_{i}$ is finite and $S_{i}$ of type $FP_{n}(\mathbb{Q})$ , then so is $S_{i+1}$ , because that property is inherited in finite extensions. Nevertheless, I really can't prove the same when $S_{i+1}/S_{i}$ is infinite cyclic. Why is this true? Maybe it does not always hold, but in my case yes: I am working in direct products of limit groups, so $S_{0}$ is a full subdirect product of limit groups, $S_{0}\leq \Gamma_{1}\times \cdots \times \Gamma_{n}$ .","['homological-algebra', 'group-theory', 'cyclic-groups']"
3397446,Two values of minima of a function by two methods.,"I had a problem of finding minima of a function $$f(x)=2^{x^2}-1+\frac{2}{2^{x^2}+1}$$ I solved it using AM-GM inequality, $$2^{x^2}-1+\frac{2}{2^{x^2}+1}$$ $$2^{x^2}+1+\frac{2}{2^{x^2}+1}-2$$ $$2^{x^2}+1+\frac{2}{2^{x^2}+1}\ge\ 2\sqrt2$$ $$2^{x^2}-1+\frac{2}{2^{x^2}+1}\ge\ 2\sqrt2-2$$ But in the solution answer was given as 1 and it was solved using differentiation, $$f'(x)=\frac{2x.ln2.2^{x^2}(2^{x^2}+1-\sqrt2)(2^{x^2}+1+\sqrt2)}{(2^{x^2}+1)^2}$$ $$2^{x^2}\ge1$$ $$2^{x^2}+1-\sqrt2\ge2-\sqrt2>0$$ At $x=0$ , $f(x)$ is least. Least value = $f(0)$ $=1$ I cannot understand how can there be two values by two different methods,please help me in the problem.",['derivatives']
3397465,On $\int_0^1\frac{\ln(1-e^{\pi i/3}x)}{e^{-\pi i/3}-x}\ln^3xdx$ and its generalization,"Motivation Consider $$I_n=\int_0^1\frac{\ln(1-\omega x)}{\bar\omega-x}\ln^nxdx=\int_0^1\frac{\omega\ln(1-\omega x)}{1-\omega x}\ln^nxdx$$ where $\omega=e^{i\pi/3}$ . It is known that $I_0=\frac1{18}\pi^2$ which can be deduced by integrating directly, and $$I_1=\frac23\zeta(3)-\frac\pi3\operatorname{Cl}_2\left(\frac\pi3\right)+i\cdot\frac1{324}\pi^3,$$ where $\text{Cl}$ is the Clausen Cl function.
I evaluated $I_1$ by using known antiderivative of $\frac{\ln(x-a)\ln(x-b)}{x}$ . One thing I noticed why it is special for $\omega$ is that if we replace it with other complex numbers different from $\pm1$ and $0$ , the result of $I_2$ won't be very beautiful, such as $$\int_0^1\frac{\ln(1-zx)}{1-zx}\ln^2xdx,\text{ where $z=i$}$$ The result of the latter involves polylogarithm values that can not be simplified. It equals $$\tiny4 i\Re\operatorname{Li}_4\left(\frac{1}{2}+\frac{i}{2}\right)+\frac{35 \pi  \zeta (3)}{64}+\frac{35}{32} i \zeta (3) \log (2)-\frac{47 i \pi ^4}{1536}+\frac{1}{96} i \ln^4(2)-\frac{5}{192} i \pi ^2 \ln^2(2)+2\beta(4)$$ Also, a CAS knows how to handle $I_2$ . It gives the result $$I_2=\frac{23}{9720}\pi^4+i\left(\frac49\pi\zeta(3)-2\operatorname{Cl}_4\left(\frac\pi3\right)\right)$$ (CAS knows the antiderivative) So the following question comes out: For a general $n\in\mathbb N$ , does a closed form of $I_n$ exist? Higher to $n\ge3$ , the polylog-styled antiderivative no longer exists. The method  becomes invalid. But the numerically-verified closed form for the real part of $I_3$ still exist. $$\Re I_3=\frac{43}6\zeta(5)-\frac16\pi^2\zeta(3)-2\pi\operatorname{Cl}_4\left(\frac\pi3\right)$$ A failed attempt is trying to convert $I_n$ to series form: $$\sum_{k=1}^\infty H_k\omega^{k+1}\int_0^1x^k\ln^n\frac1xdx=n!\sum_{k=1}^\infty \frac{H_k\omega^{k+1}}{k^{n+1}}$$ and separating it into 6 series. No luck for continuing this method so far. Edit: I'm not looking for a proof of the result above. I'm looking for a closed form of $I_n$ .","['integration', 'euler-sums', 'definite-integrals', 'sequences-and-series']"
3397484,Maximum and minimum points overlapped by moving circle on square grid.,"We have a square grid, of points spaced evenly at distance $u$ , like on a math notebook. We have a moving circle of radius $r$ , like a coin sliding around on it.
A decent approximation of points overlapped by the circle is $$
c\frac{\mathrm{Area}}{u^2} = \pi\cdot\Big(\frac{u}{r}\Big)^{\!2}.
$$ So far so good. 
This falls apart on the edge cases, especially on sparse grids: Under those conditions, pMin is 3, pMax is 6, and my approximation is 4. Not a great estimate. Feel free to play with the online toy here . So, is there any way to reliably, mathematically find pMin, pMax, and maybe some sort of average? An intuition to me would be that there are key discrete points in between those points, which is what I'll be working on. Thank you!",['geometry']
3397548,Prove or disprove $\lim\limits_{n \to \infty}\Delta x_n=0.$,"For a sequence $\{x_n\}_{n=1}^{\infty}$ , define $$\Delta x_n:=x_{n+1}-x_n,~\Delta^2 x_n:=\Delta x_{n+1}-\Delta x_n,~(n=1,2,\ldots)$$ which are named 1-order and 2-order difference , respectively. The problem is stated as follows: Let $\{x_n\}_{n=1}^{\infty}$ be bounded , and satisfy $\lim\limits_{n \to \infty}\Delta^2 x_n=0$ . Prove or disprove $\lim\limits_{n \to \infty}\Delta x_n=0.$ By intuiton, the conclusion is likely to be true. According to $\lim\limits_{n \to \infty}\Delta^2 x_n=0,$ we can estimate $\Delta x_n$ almost equal with an increasing $n$ . Thus, $\{x_n\}$ looks like an arithmetic sequence . If $\lim\limits_{n \to \infty}\Delta x_n \neq 0$ , then $\{x_n\}$ can not be bounded. But how to prove it rigidly?","['limits', 'calculus', 'sequences-and-series']"
3397653,Symmetric power of a locally free sheaf in $\mathbb{P}^{n}$,"In Algebraic Geometry, by Hartshorne, we have: Definition 1 : Let $A$ be a ring and let $M$ be an $A$ -module. Let $T^{n
}(M)$ be the tensor $M \otimes \cdots \otimes  M$ of $M$ with itself $n$ times for $n\geq 1$ . Then $T(M) = \bigoplus_{n \geq 0} T^{n}(M)$ is a $A$ -algebra, which we call the tensor algebra of $M$ . Definition 2 : We define the symmetric algebra $S(M) = \bigoplus_{n \geq 0} S^{n}(M)$ of $M$ to be the quocient of $T(M)$ by the two-sided ideal generated by all expressions $x \otimes y - y \otimes x$ , for all $x, y \in M$ . Its component $S^{n}(M)$ in degree $n$ is called the $\text{nth}$ symmetric product of $M$ . Suppose that $\mathcal{F}$ is locally free sheaf of $\text{rank}$ $n$ . then $S^{n}(\mathcal{F})$ is also locally free of $\text{rank}$ , ${n+r-1 \choose n}$ . For example, according to definition 2 above, what would be $S^{1}(\mathcal{F})$ and $S^{2}(\mathcal{F})$ where $\mathcal{F} = \mathcal{O}_{X}(a+b)$ with $a, b \in \mathbb{Z}$ and $X= \mathbb{P}^{n}$ . I thank you in advance for your help.","['algebraic-geometry', 'sheaf-theory']"
3397672,What are some counterintuitive results that are really simple to explain?,"I am aware of other questions asking about counterintuitive mathematical results, all of which have many great answers. However, most examples rely on subtle concepts of probability (e.g., Monty Hall, birthday paradox) or infinity (e.g., Gabriel's Horn, Banach-Tarski). Recently I came across a beautiful counterintuitive result that relies only on simple and finite Euclidean geometry. It should surprise anyone with a basic sense of scale, and yet it is easily explainable to someone with a middle-school understanding of geometry. Moreover, there's not much room for argument (""how do you define probability?"", or ""what is infinity, anyway?""). Back in Ancient Greece, Zeus commissions a blacksmith to forge him a ring that will go around the Earth. The blacksmith obliges, but he makes a mistake and forges the ring one meter too large in circumference. Zeus goes ahead and places the ring around the Earth, pressing it up against the South pole. How large is the gap left at the North pole, and what sort of animal can go through? The counterintuitiveness comes from the fact that an error of one meter in circumference is negligible compared with the Earth's circumference. In fact, you might say the blacksmith did a great job in keeping the relative error so small. If you think like me, your intuition will say that the gap must also be negligible. However, the gap is actually around 32 cm: enough for a human to crawl through! Granted, you could argue it is still negligible compared with the Earth's diameter, but it is still much bigger than I would have guessed without calculating. Perhaps even more surprisingly, the gap does not even depend at all on the circumference of the Earth or the ring - just on their difference. You could put a ring one meter too large in circumference around the sun or your favorite ginormous star, and the gap would still be 32 cm. Do you know of any similar, counterintuitive facts that should surprise and be explainable to more or less anyone?","['euclidean-geometry', 'circles', 'geometry', 'examples-counterexamples', 'soft-question']"
