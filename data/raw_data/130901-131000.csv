question_id,title,body,tags
2039672,Number of triangles formed by n lines and m parallel lines.,"This question was posted back in 2012, but a proper answer was never given.
Here's the question I'm stuck on: ""How many triangles are formed by $n$ lines, and $m$ parallel lines, where no 3 lines can intersect at one point."" These $m$ parallel lines are all parallel to each other, so all $m$ lines have the same slope.  I've been trying to draw out cases and cannot find some type of pattern.  Any help would be great. We did this for $n$ non-parallel lines, and found out that the amount of triangles formed was $\binom{n}{3}$. So we'd have $n-m$ non-parallel lines, and we were trying to find a pattern by drawming more lines, and more non/parallel lines with no success.","['combinations', 'combinatorics', 'discrete-mathematics', 'geometry']"
2039719,"Sheaves on basic open sets of an affine scheme, confused about Vakil's notes","I thought I was generally getting my head around the sheaf structure defined by basic open sets for an affine scheme, but a line of a proof in Ravi Vakil's notes found here (page 130) has me confused. So for a ring $A$ with $s \in A$, $s$ defines a global section of the structure sheaf on $\text{spec}A$ given by taking $s \pmod{\mathfrak{p}}$. To show the identifying property for the structure sheaf, Vakil supposes that the restriction of $s$ to a basic open set, $D(f)$, is $0$. That is, he assumes,
$$ \text{res}_{\text{specA}, D(f)}s = 0. $$
He then concludes that this means that there is some integer $m$ such that $f^{m}s = 0$. I am confused about why he draws this conclusion. My reasoning was as follows: If $s$ is zero on the restriction to $D(f)$, then it means that every prime ideal not containing $f$ must contain $s$. This is equivalent to saying that every prime ideal contains $fs$. In other words, $fs$ is the zero function on $\text{spec}A$. In other words again, $fs$ is in the nilradical. But then why does Vakil say that there is an $m$ such that $f^{m}s = 0$? If he meant $fs$ as an element of $A$, then surely he means $(fs)^{m} = 0$, and if he was talking about $fs$ as a global section, then he meant $fs = 0$ as a function. So why $f^{m}s = 0$? What am I missing? Thanks","['sheaf-theory', 'algebraic-geometry', 'commutative-algebra']"
2039720,Prove that $R$ is a ring with division.,"I'm having problems trying to know if my proof is wrong or not. The problem states: Let $R$ a ring with 1, not necessary commutative, such that for every $a\in R\setminus\{0\}$, there exists $b\in R\setminus\{0\}$ (which depends on $a$) such that $a\cdot b=1$. Prove that $R$ is a division ring. I have almost everything to prove that is a ring with division, I think I only miss the part that $b\cdot a=1$, what I have done so far is the following: We have $ab=1$ so multiplying $b$ on the left side we have
$$b\cdot a\cdot b=b\cdot 1$$
Since $1$ is the $1$ of $R$, we get:
$$b\cdot a\cdot b=1\cdot b$$
(this is the part I'm not sure about) since we are in a group, we can cancel $b$ on the left side 
$$a\cdot b=1$$
and we are done?","['abstract-algebra', 'ring-theory']"
2039764,Geometry of $2 \times 2$ matrix mappings,"Suppose $\mathbf{A}$ is a $2 \times 2$ matrix. I'm interested in the geometry of the mapping $T(\mathbf{x}) = \mathbf{A}\mathbf{x}$ from $\mathbb{R}^2$ to $\mathbb{R}^2$. The effect on the vectors $\mathbf{e_1} = (1,0)$ and $\mathbf{e_2} = (0,1)$ is clear -- $T(\mathbf{e_1})$ and $T(\mathbf{e_2})$ are just the columns of $\mathbf{A}$. So, by linearity, a unit square gets mapped to a parallelogram. The green square gets mapped to the pink parallelogram in the pictures below. Now let's consider the effect on  a unit circle. It gets mapped to an ellipse, and I'm interested in how the geometry of this ellipse is related to the matrix $\mathbf{A}$. One case seems clear: if $\mathbf{A}$ is symmetric, then the axes of the ellipse are the eignevectors of $\mathbf{A}$, and its semi-axis lengths are the eigenvalues. The ""stretching"" of the circle to form the ellipse is nicely related to eigenvalues and eigenvectors. Fabulous. This is illustrated in the following picture: Another case is also clear: if the eigenvalues of $\mathbf{A}$ are not real, then presumably there is no relationship whatsoever to the geometry of the ellipse. Now the case that's puzzling me: what if $\mathbf{A}$ is not symmetric, but still has real eigenvalues. What sort of geometric relationship exists in this case, if any? This case is illustrated in the following picture:","['eigenvalues-eigenvectors', 'linear-transformations', 'geometry', 'conic-sections', 'linear-algebra']"
2039765,Coin flipping probability problem.,"You flip a fair coin until you see three tails in a row. What is the average number of heads
that youâ€™ll see until getting T T T? (3 tails in a row). My math got me to about 21 heads before it happens, and I wanted to know if that is correct. Thanks!","['probability', 'discrete-mathematics']"
2039829,"If $A$ is infinite and $A$ is a subset of $B$, then $B$ is infinite.","My question reads: Prove: If $A$ is infinite and $A$ is a subset of $B$, then $B$ is infinite. If finite then $A$ is either the empty set or $A$ is equivalent to $N_{k}$ for some $k\in \mathbb{N}$, where $N_{k}=\{1,2,\ldots ,k\}$.","['proof-writing', 'elementary-set-theory']"
2039846,Show that $3^{2n+1}-4^{n+1}+6^n$ is never prime for natural n except 1.,"Show that $3^{2n+1}-4^{n+1}+6^n$ is never prime for natural n except 1. I tried factoring this expression but couldn't get very far. It is simple to show for even n but odd n was more difficult, at least for me.","['exponential-function', 'prime-factorization', 'number-theory', 'prime-numbers', 'elementary-number-theory']"
2039867,Elliptic curves as branched coverings of $\mathbb{CP}^1$,"Any smooth projective curve $X$ of genus one admits a morphism $f:X\to\mathbb{P}^1$ of degree 2, which by the Riemann-Hurwitz formula is ramified at exactly four points. Given a $4$-tuple $\{\alpha_1,\alpha_2,\alpha_3,\alpha_4\}\subset\mathbb{P}^1$, I would like to give an explicit such morphism ramified exactly at $\alpha_1,...,\alpha_4$. How can I do this? For instance, for $\{\alpha_1,\alpha_2,\alpha_3,\infty=[1:0]\}\subset\mathbb{P}^1$ we can consider \begin{equation} X=\{[x:y:z]\in\mathbb{P}^2\mid zy^2=(x-\alpha_1z)(x-\alpha_2z)(x-\alpha_3z)\}, \end{equation} which is the closure in $\mathbb{P}^2$ of the affine curve $\{(x,y)\in\mathbb{C}^2\mid y^2=(x-\alpha_1)(x-\alpha_2)(x-\alpha_3)\}$, and the morphism $F:X\to\mathbb{P}^1$ given by
 \begin{equation}
    F([x:y:z]) = \left\{\begin{array}{lr}
        \left[1:0\right] & \text{if } [x:y:z]=[0:1:0]\\
        \left[x:z\right] & \text{otherwise}
        \end{array}\right\}, 
  \end{equation} which looks like $(x,y)\mapsto x$ on the open chart $\mathbb{C}^2$. This construction satisfies all my requirements (in particular, $X$ is smooth). However, if I take $\{\alpha_1,..,\alpha_4\}\subset\mathbb{P}^1\setminus\{\infty\}$ and I consider the curve \begin{equation} Y=\{[x:y:z]\in\mathbb{P}^2\mid z^2y^2=(x-\alpha_1z)(x-\alpha_2z)(x-\alpha_3z)(x-\alpha_4z)\},\end{equation} then $Y$ has a singular point at $[0:1:0]$, and the corresponding morphism $Y\to\mathbb{P}^1$ is ramified at $\alpha_1,...,\alpha_4$ $\textbf{and}$ $\infty=[1:0]$, so this construction doesn't satisfy my requirements. So what is the correct construction in this case?","['elliptic-curves', 'algebraic-geometry']"
2039880,On solutions of a certain $n\times n$ linear system whose coefficients are all $\pm1$,"Let $n>2$ and $A \in M_n(\mathbb{R})$ be a $\{-1,1\}$-matrix (whose elements are $-1$ or $1$). Let $b\in\mathbb R^n$ contains the row-wise counts of minus ones in $A$ (i.e. $b_i$ is the number of minus ones in the $i$-th row of $A$). Suppose $Av = b$. Prove or disprove that if $Ax=0$ has only the trivial solution, then $v$ has at least two identical elements. For example,
$$
A = \pmatrix{1 & 1 & 1 \\ -1 & 1 & 1\\ -1 & -1 & 1},
\ b=\pmatrix{0\\ 1\\ 2},
\ v=\pmatrix{-\frac12\\ -\frac12\\ 1}.
$$
I couldn't find a counterexample, so for the moment I'm assuming it is indeed true. Perhaps the more pertinent issue I'm having is not knowing where to start, given the peculiar structure of the matrix and of $b$. I considered proving the contrapositive (if $v_1 = \ ...\  = v_n$, then there exists some nonzero/nontrivial solution to $Ax = 0$), but I didn't get far with that. I appreciate all help, even if it's just a nudge in the right direction (or if anyone finds a counterexample). Thank you kindly! Edit: In addition, I have not found a counterexample for the case where each $a_{ij}$ is either $-\delta$ or $\delta$ for $\delta \in \mathbb{R}$, so if the initial statement is indeed true, then something that would point to this fact would be optimal (or at the very least interesting). Thanks again!","['matrices', 'linear-algebra', 'systems-of-equations']"
2039884,Related rates question using similar triangles,"So I have this related rates problem here: A man $6$ feet tall is standing still in a gymnasium which has a ceiling that is $30$ feet high. Ten feet in front of him a bright light starts to fall from the ceiling and the distance it falls in $t$ seconds is $16t^2$ feet. After one second, what is the rate of change in the length of his shadow created by the falling light? Can someone help me set this up? It's similar to the shadow questions I used to do but the guy is still this time, he isn't walking. I'm stuck on how to represent the falling light. I drew a diagram trying to represent the problem. However, I feel like I have too many variables to relate and I can't figure how to write them in terms of another.","['derivatives', 'calculus']"
2039907,Closure of identity element in a topological group is the intersection of its neighborhood system,"Consider a topological group $G$. I want to prove that if $\cal{U}$ is the set of all neighborhoods of $e$ (the identity element), then $\overline{\{e\}}=\cap_{U\in \cal{U}}U$. What I thought was: i) If $x\in \overline{\{e\}}$, then every neighborhood $V$ of $x$ intersects $\{e\}$, hence every $V$ is a neighborhood of $e$ as well, i.e. $V\in \cal{U}$. ii) On the other hand, if $x\in \cap_{U\in \cal{U}}U$, then $x\in U$ for all $U \in \cal{U}$, meaning every $U$ is a neighborhood of $x$ intersecting $\{e\}$. Now, I think that the missing part is an argument justifying that there is no neighborhood of $e$ that is not a neighborhood of $x$ (part i) ), and that there is no neighborhood of $x$ that is not a neighborhood of $e$ (part ii) ). My idea for this is to argue that there is a homeomorphism taking $e$ to $x$, therefore the neighborhood system of one cannot have more elements that the neighborhood system of the other. Is this correct? If not, is there a way I can fix it?","['general-topology', 'topological-groups', 'proof-verification']"
2039930,Eigenvalue decomposition of $A = I - xx^T$ [duplicate],"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 7 years ago . Let $A = I - xx^T$, where $x \in \mathbb{R}^n$ and $I$ is the identity matrix of $\mathbb{R}^n$ We know that $A$ is a real symmetric matrix, therefore there exists an eigenvalue decomposition of $A$ such that $$A = Q^T\Lambda Q$$ Is it possible to find $Q$, $\Lambda$? $I - xx^T = Q^TQ - xQ^TQx^T = Q^TQ - (Q^Tx)^T(x^TQ)^T...$","['matrices', 'matrix-decomposition', 'eigenvalues-eigenvectors', 'linear-algebra']"
2039969,Find the residue of $f(z)$,Find the residue of $f(z)$ $$f(z) = \frac{z^{(1/4)}}{z+1}$$ So this is a pole of order 1 with a singularity at $z=-1$ $$z^{1/4}\Big|_{-1}\ = (-1)^{1/4}$$ And I'm not sure what to do with that. The book says the answer is $\frac{1+i}{\sqrt2}$,['complex-analysis']
2039971,Is it possible to create a system of two equations with 3 variables and only one solution?,"I know that a system of equations with 3 variables usually has to contain at least 3 equations, but are there any special cases where 2 equations with 3 variables have just one solution? I've researched a bit, and found 2 equations with 3 variables with finite solution sets, but are there any that have exactly one solution? The equations could be linear or quadratic or cubic or sinusoidal or anything... but does such a case exist? Also, is it possible to find such a case if we know any 2 variables? Like, for example, $a + b + c = w$ and $b + c = d$, given b and d. (of course, this example has more than 1 solution, but are there any like this that have just 1 solution) Thanks for any help! Edit: To clarify, I'm looking for the general form of such an equation. To be more specific, are there any systems of 2 equations, with 3 variables, that only have 1 solution, but which can be modified to have any 1 solution I desire.","['algebra-precalculus', 'systems-of-equations']"
2039980,why is E(XX') the covariance matrix,"In regression people often refer to the $X'X$ term as the estimate of $E(xx')$ which makes sense by LLN. However, people also refer to $E(xx')$ as the covariance matrix of $x$ which only seems to make sense if $E(x)=0$. Why does it makes sense if the $x$'s are not demeaned?","['matrices', 'covariance', 'regression']"
2039992,"Function mapping, which is both one to one and onto?","For the function  $f(x)= \sqrt x$ which of the following mappings is both one to one and onto? $f : \mathbb{R}  \to \mathbb{R}$ $f : \mathbb{R} \to \mathbb{R}^+$ $f : \mathbb{R}^+ \to \mathbb{R}^+$ $f : \mathbb{R}^+ \to \mathbb{R}$ Sorry for the incorrect formatting on this stuff, I'm pretty new to this. I don't think that it is 4, because the square root of a positive real number is a positive real, but I am not sure about the rest. Could someone help with the rest of this? Thanks!","['functions', 'discrete-mathematics']"
2040003,Line integral with respect to arc length,"I ran into a problem that, initially, I thought was a typo. $$\int_C\ e^xdx $$ where C is the arc of the curve $x=y^3$ from $(-1,-1)$ to $(1,1)$. I have only encountered line integrals with $ds$ before, not $dx$ (or $dy$, for that matter). At first, I thought the $dx$ was supposed to be a $ds$, but that led to an unsolvable integral. Unfortunately, my book does not cover this topic very well, and the online answers I have found are rather vague. I tried plugging in $x=y^3$ to get $$ \int_{-1}^{1}\ e^{y^3}3y^2dy $$ which evaluates to $e - \frac{1}{e}$. This is also what I get when I do $ \int_{-1}^{1}\ e^{x}dx $, so I am inclined to believe it is correct. However, I'm not sure, and I'd like to know for certain if my intuition is valid.","['definite-integrals', 'calculus', 'line-integrals']"
2040009,Too Restrictive Axiom- Example,Can someone give me an example of an object where the rules (axioms) are so restrictive that the result leads to few mathematical structures?,"['abstract-algebra', 'axioms']"
2040015,Maximum Likelihood Estimation with Indicator Function,"I need to solve this exercise from the book below. $\textbf{Mathematical Statistics, Knight (2000)}$ $\textbf{Problem 6.17}$ Suppose that $X_1,\ldots,X_n$ are i.i.d. random variables with frequency function \begin{equation}
  f(x;\theta)=\begin{cases}
    \theta                 & \text{for $x=-1$}.\\
    (1- \theta)^2 \theta^x & \text{for $x=0,1,2,\ldots$}
  \end{cases}
\end{equation} (a) Find the Cramer-Rao lower bound for unbiased estimators based on $X_1,\ldots,X_n$. (b) Show that the maximum likelihood estimator of $\theta$ based on $X_1,\ldots,X_n$ is $$\hat{\theta}_n = \frac{2 \sum_{i=1}^{n} I_{(X_{i}=-1)} + \sum_{i=1}^n X_i}{2n + \sum_{i=1}^n X_i}$$ and show that $\{\hat{\theta}_n\}$ is consistent for $\theta$. (c) Show that $\sqrt{n}(\hat{\theta}_n-\theta)\to_d N(0,\sigma^2(\theta))$ and find the value of $\sigma^2(\theta)$. Compare $\sigma^2(\theta)$ to the Cramer-Rao lower bound in part (a). No clue on how to solve (a) or (c). I started to solve (b) but I can't seem to arrive at the desired solution. I'm getting this: \begin{align}
\mathcal{L} &= \prod_{i=1}^n (1-\theta)^2 \theta^{x_i I_{(X_i \geq 0)} + I_{(X_{i}=-1)}} \\
\mathcal{L} &= (1-\theta)^{2 \sum_{i=1}^n I_{(X_i \geq 0)}} \theta^{\sum_{i=1}^n x_i I_{(X_i \geq 0)} + \sum_{i=1}^n I_{(X_{i}=-1)}} \\
\log \mathcal{L} &= 2 \sum_{i=1}^n I_{(X_i \geq 0)} \log(1-\theta) + \sum_{i=1}^n x_i I_{(X_i \geq 0)} \log \theta + \sum_{i=1}^n I_{(X_i=-1)} \log \theta
\end{align} $\textbf{FOC}$ \begin{align}
0 &= - \frac{2 \sum_{i=1}^n I_{(X_i \geq 0)}}{1-\theta} + \frac{\sum_{i=1}^n x_i I_{(X_i \geq 0)}} \theta + \frac{\sum_{i=1}^n I_{(X_i=-1)}} \theta \\ \\
\hat{\theta}_n &= \frac{\sum_{i=1}^n I_{(X_i=-1)} + \sum_{i=1}^n x_i I_{(X_i \geq 0)}}{\sum_{i=1}^n I_{(X_i=-1)} + 2 \sum_{i=1}^n I_{(X_i \geq 0)} + \sum_{i=1}^n x_i I_{(X_i \geq 0)}}
\end{align} which differs from the result I'm given... Any help would be greatly appreciated.","['maximum-likelihood', 'probability']"
2040041,"Using real analysis, how can we show that for all $x > 0$, $ \frac{2(e^x-(1+x))}{x^2} < e^x$",I was able to think that the numerator will always be positive and will overpower the denominator as well. But couldn't proceed from there.,"['real-analysis', 'analysis']"
2040063,"When both $\frac{dy}{dt}$ and $\frac{dx}{dt} = 0$, what happens?","As we know that if $\frac{dy}{dt}=0$ and $\frac{dx}{dt}\neq0$ at $x_0$, then the equation has a horizontal line at $x_0$. Moreover, when $\frac{dx}{dt}=0$ and $\frac{dy}{dt}\neq0$, then there is a vertical line. What happens if both $\frac{dy}{dt}=0$ and $\frac{dx}{dt}=0$?","['derivatives', 'parametric']"
2040087,Can kurtosis measure peakedness?,"Wikipedia says kurtosis only measures tailedness but not peakedness. But I remember my teacher said several times that high excess kurtosis usually corresponds to fat tails AND thin peak. High excess kurtosis accompanied by fat tails can be easily seen by the usual definition of kurtosis(fourth central moment). But what about peakedness? If kurtosis doesn't measure it, is there any statistic that can do the job? My Statistics textbook isn't clear about this part.",['statistics']
2040100,Aren't vacuous statements True and False simultaneously?,"Wikipedia states ""a vacuous truth is a statement that asserts that all members of the empty set have a certain property"". Clearly the statement: 'all elements of said (empty) set possess said property' is vacuously true. However, one could argue that the negation of the statement: 'no elements in said set posses said property' is also true. Shouldn't that mean that the statement is both true and false. I understand there may be slightly different definitions of what constitutes a vacuous statement, but I suppose this particular issue will show up nevertheless.","['logic', 'elementary-set-theory', 'quantifiers']"
2040121,Inverse Nasty Integral,"Intrigued by the original question on a nasty integral , one wonders what functions $f(x)$ exist such that $$\int_0^\infty f(x)\; dx=\frac {\pi}2$$ Something to do with the area of a half-circle with unit radius perhaps? Edited To Add It should be specified that $f(x)$ should not contain $\pi$, and is preferably a rational function. The intention of posting the question was to take a different approach to the the original nasty integral by first considering the answer and then working backwards, sort of a heuristic approach. This has yielded an excellent answer posted by @achillehui below, which also addresses the original nasty integral question in a very simple way.","['integration', 'definite-integrals', 'rational-functions']"
2040133,How do I convert this double integral to polar?,"I'm having trouble converting this double integral from Cartesian to polar: $$\int_0^1\int_0^{\sqrt{(1-x^2)}}e^{x^2+y^2}dydx$$ What I know:
$$e^{x^2+y^2}dydx = re^{r^2}drd\theta$$
$$\sqrt{1-x^2} = \sqrt{r^2\sin^2{\theta}} = r\sin{\theta}$$ My issue: I can't figure out how to convert the limits. I graphed $y=\sqrt{1-x^2}$ using wolfram alpha, and found that it forms half of an ellipse on the positive side of the x-axis with the origin being the center. Since it forms an ellipse, I don't know the limits of r because the radius is not uniform around the shape. I assume that the limits of $\theta$ are $0\le\theta\le{\pi/2}$ since the limits of X in Cartesian form are $0\le{x}\le1$ and the shape is an ellipse centered around the origin. Could I get some guidance in the conversion of the limits?","['multivariable-calculus', 'polar-coordinates']"
2040208,Distributivity of subspaces,"I need to either give a proof or find a counterexample to a statement: $$L+(Mâˆ©N) = (L + M)âˆ©(L + N)$$ Where $L$,$M$,$N$ are subspaces of a vector space $V$. 
I could do $LHSâŠ†RHS$ proof, but I'm stuck with backwards proof. I would be really grateful if someone could help me out.",['linear-algebra']
2040230,$ab$ and $ba$ invertible elements imply $a$ and $b$ invertible in a Banach algebra,"I encountered the following question as a question about linear operators. If $AB$ and $BA$ are invertible for some $A, B \in \frak{L}$$(E)$ where $E$ is a Banach space, then show that $A$ and $B$ are also invertible. I worked this out by showing that $A$ is bijective and its inverse is linear and bounded. But that required making explicit  the action of $A$ and $B$ as operators on an underlying space. So my question is whether there is an 'algebraic' way of showing the statement is true, i.e. whether if $ab$ and $ba$ are invertible elements in a Banach algebra imply $a$ and $b$ invertible.","['functional-analysis', 'ring-theory', 'banach-spaces', 'banach-algebras']"
2040241,Summation related to $\tan^2$,"What is $\displaystyle\sum _{n=1}^3 \tan^2 (\frac {n\pi}{7}) $. I substituted $7x=n\pi $, thus the summation changed to 
$$\tan^2 (x)+\tan^2 (n\pi-5x)+\tan^2(n\pi-4x)$$
which even on expanding doesn't prove useful . I also did $\tan (n\pi-x)= -\tan x $ . But that doesn't help either. Any hints will be useful. Thanks!",['trigonometry']
2040272,"Is $\{((a,b),(c,d))|a+b\ge c+d\}$ an anti-symmetric relation?","Question Let $R$ be a relation on the set $\mathbb Z^+\times \mathbb Z^+$ defined as follows:
$R=\{((a,b),(c,d))|a+b\ge c+d\}$. Is this anti-symmetric? Attempted Solution I'd argue that it isn't. For $((a,b),(c,d))\in R$ and $((c,d),(a,b))\in R$ to be true, then $a+b$ must equal $c+d$. This is because -- per the definition above -- $a+b\ge c+d$. If $(a,b)>(c,d)$ then $((c,d),(a,b))$ could not exist in the relation, thus they must be equal in terms of sum. In such a scenario, we find that the above relation is not anti-symmetrical since  $((10,5),(5,10))\in R$ and $((5,10),(10,5))\in R$ is true, but $(5,10)\ne (10,5)$. Did I do this right?","['relations', 'discrete-mathematics']"
2040285,Find all function $ f:\Bbb R\to\Bbb R$ such that $ f(x+y)f(x-y)=(f(x)+f(y))^2-4x^2f(y)$.,"While doing some INMO questions, one entry went this way: Find all function $ f:\Bbb R\to\Bbb R$ such that $f(x+y)f(x-y)=(f(x)+f(y))^2-4x^2f(y)$. I made an approach similar to this : On Putting $x=0, y=0$ we get $$f(0+0)f(0-0)=(f(0)+f(0))^2-4\times0^2f(0)$$  $$f(0)^2=(2\times f(0))^2$$Which gives us $f(0)=0$. Then, on putting $x=1, y=1$,
 $$f(1+1)f(1-1)=(f(1)+f(1))^2-4\times1^2f(1)$$ 
$$f(2)f(0)=(2\times f(1))^2+-4\times f(1)$$
$$4\times f(1)=4\times f(1)^2$$ Which gives us $f(1)=0 $  or $f(1)=1$. From here, I can't go further. I think that method I m working on is quite right and will take me to the right answer. But the problem is that I can't find that right answer. I shall be thankful if you can provide me a hint or a complete solution. Thanks. SIDE NOTE: I am using this method for a while (I got this one from then answer of a post). Now I m thinking to switch, If you know some other method to solve such functional equations, please try to give your answer by that method.","['contest-math', 'functions', 'functional-equations']"
2040318,Sequence of norms,"Suppose that we have a sequence of norms $(\|\cdot\|_n)_{n\geq 1}$ on a finite dimensional space $X$ such that, as functions from $X$ to $\mathbb{R}_+$, they point-wise converge to a limit norm $\|\cdot\|_\infty$. Since we are in a finite-dimensional setting, all of the norms are equivalent among them, and, in particular, all of the norms in the sequence are equivalent to $\|\cdot\|_\infty$. An implication of this fact is that there is a sequence of constants $c_n$ such that
$$ \forall n \geq 1,  \forall x\in X, \quad \|x\|_n \geq c_n\|x\|_\infty. $$ Now, the equivalence constants $c_n$ are not necessarily unique, but, since the norms $\|\cdot\|_n$ approach $\|\cdot\|_\infty$ in the limit, I am wondering how can one prove that they can be chosen to have $1$ as a limit (i.e. $\lim_{n\to\infty} c_n = 1$) ?","['normed-spaces', 'sequences-and-series', 'limits']"
2040321,Chain Rule $f(x) = (ax + b)^n$,"So I'm fairly new to this and I just wanted to check my understanding of the chain rule . Suppose $f(x) = (ax + b)^n$ and we want to find $f'(x)$. We first work out the derivative of the first function, then multiply it to the derivative of the second function giving: $$n(ax +b)^{n-1}\cdot(ax+b)'$$ To calculate the derivative of the second function, we use the sum rule , but this is where I get a little stuck. I know I have to find both $ax'$ and $b'$, so here's how I think I should do it...please tell me if I'm crazy and wrong: As $b$ is a constant it follows that $b' = 0$. My textbook hasn't gone into this yet, so it isn't clear why. Anyway, moving on. This leaves us with: $$ax' = a$$ What I REALLY don't understand is why we don't use the product rule for $ax$. Any clues?","['derivatives', 'chain-rule', 'calculus']"
2040341,When Two Connections Determine the Same Geodesics,"It's my first question!  I hope I'm correctly formatting it. I'm trying to prove that two connections $\nabla, \widetilde{\nabla}$ on a manifold determine the same geodesics iff their difference tensor is alternating.  The difference tensor of two connections is the tensor $D(X,Y) = \widetilde{\nabla}_XY - \nabla_XY$, so this is saying that the geodesics are the same for the two connections iff $D(X,Y) = -D(Y,X)$ for all vector fields $X, Y$ on $M$. Since $D$ is a tensor it can be written as a sum of its symmetric and alternating parts, $D = S + A$.  Since $A(X,X) \equiv 0$ we have $D(X,X) = S(X,X)$.  So if $S(X,X) = 0$ then so does $D(X,X)$, and if $D(X,X)$ is always $0$, then $$ D(X+Y,X+Y) =S(X+Y, X+Y) = S(X,X) + S(Y,Y) + 2S(X,Y) = 2S(X,Y)$$ Then $S(X,Y) \equiv 0$.  So it's enough to check that having the same geodesics is the same as $D(X,X) \equiv 0$.  This observation was made in Spivak Vol. II Ch. 6, Addendum 1, but his proof that this latter condition is equivalent to the connections having the same geodesics doesn't make sense to me. Does anyone know how to show that if the connections have the same geodesics then the difference tensor is $0$ on the diagonal?  That's the direction I'm stuck on.","['riemannian-geometry', 'differential-geometry', 'manifolds', 'connections', 'geodesic']"
2040363,Comparing LU or QR decompositions for solving least squares,"Let $X \in R^{m\times n}$ with $m>n$. We aim to solve $y=X\beta$ where $\hat\beta$ is the least square estimator. The least squares solution for
  $\hat\beta = (X^TX)^{-1}X^Ty$ can be obtained using QR decomposition
  on $X$ and $LU$ decomposition on $X^TX$. The aim to compare these. I noticed that we can use Cholesky decomposition instead of $LU$, since $X^TX$ is symmetric and positive definite. Using $LU$ we have: $\hat\beta = (X^TX)^{-1}X^Ty=(LU)^{-1}X^Ty$, solve $a=X^Ty$ which is order $O(2nm)$, then $L^{-1}b=a$ at cost $\sum_1^{k=n} (2k-1)$ and finally $U^{-1}a$ at the same cost of $\sum_1^{k=n} (2k-1)$. I didn't count the cost of computing $L^{-1}$ and $U^{-1}$. Using $QR$ we have: $\hat\beta = (X^TX)^{-1}X^Ty=((QR)^TQR)^{-1}R^TQ^Ty=R^{-1}Q^Ty$, where we solve $Q^Ty=a$ at cost $O(n^2)$ and $R^{-1}a$ with cost $\sum_1^{k=n} (2k-1)$. Comparing the decompositions: It seems that QR decomposition is much better than LU. I think the cost of computing QR is higher than LU, which is why we could prefer to use LU. On the other hand if we are given the decompositions, we should use QR. $SVD$ decomposition: Is there any advantage to use SVD decomposition?","['least-squares', 'matrices', 'matrix-decomposition', 'statistics', 'linear-algebra']"
2040364,quotient of SL_n,"Does anybody know why the quotient of $SL_n$ by its subgroup $Q_m:=\left\{\left(\begin{matrix}\xi&*&\cdots&*\\0&*&\cdots&*\\ \vdots&\vdots&\ddots&\vdots\\ 0&*&\cdots&*\end{matrix}\right)\mid\xi\in\mu_m\right\}$ is isomorphic to $\mathbb{C}^n/\mu_m$, where $\mu_m$ is the cyclic group of order $m$ and acts diagonally on $\mathbb{C}^n$?","['invariant-theory', 'group-theory', 'algebraic-geometry']"
2040382,Confusion - Am I on the right track on this power computation?,"So the question asks: It has been generally believed that average daily caffeine consumption, $Î¼$, is at most $200$ mg with $Ïƒ^2 = 225$.  To examine whether at present time $Î¼$ is greater than $200$ mg, it is agreed that daily caffeine consumption of a random sample of $100$ adults be examined.  If the sample mean is greater than $203.50$, the decision will be to reject the null hypothesis. The following information may be useful: pnorm(0.67) $=0.75$, pnorm(1) $=0.84,$ pnorm(1.56) $=0.94$, pnorm(2.33) $=0.99$ (a) What are the null and alternative hypotheses? (b) What is the test statistic and what is it distribution? (c) What is the probability of a Type 1 error? (d) What is the power of the test if the true mean is 205mg? (e) Is the assumption that the population distribution of daily caffeine consumption is normal necessary
for the hypothesis test? Explain your reasoning. My answers: (a) $H_0: Î¼ \le 200$ (average daily caffeine consumption is at most $200mg$) $H_1: Î¼ > 200$ (average daily caffeine consumption is more than $200mg$) (b) For large sample, and known population standard deviation, use 1-sample $Z$ test. Distribution: Normal. $$Z=\frac{\bar X-Î¼}{\sigma/\sqrt{n}}
=\frac{203.50-200}{15/10}
=2.3333.$$ (c) probability of Type I error, $\alpha=0.05$ (d) Since it is a right-tailed test, therefore, one commits a Type II error (fail to reject $H_0$), when one gets a test statistic less than 1.645. Compute $X_\text{critical}$ by substituting the value in following Z score formula $Z =1.64=\frac{X_\text{critical}-\mu}{\sigma/\sqrt{n}}=\frac{X_\text{critical}-200}{1.5}$ $X_\text{critical}=202.46$ $$P(\bar X<202.46)=P\left(Z<\frac{202.46-205}{1.5}\right)=P(Z<-1.69333333)=0.0455.$$ Power of test: $1-0.0455 =0.9545.$ (e) Not necessary because the sample size is sufficiently large. So I am not sure about my answers in d and e. Should I calculate the new x, then use z-test to get the power of the test? And in e, does my answer sound reasonable enough?","['normal-distribution', 'hypothesis-testing', 'probability-distributions', 'statistics', 'probability']"
2040421,If series is divergent will a constant also keep it diverging?,"If $\sum_{n=0}^{\infty} b_n$ diverges, and $c \in \mathbb{R}$ then does $\sum_{n=0}^{\infty} cb_n$ diverge? My answer: NO. Let $c=0$, then the sum is $\sum_{n=0}^{\infty} 0 = 0$. True conclusion?","['sequences-and-series', 'calculus']"
2040434,The Projection Matrix is Equal to its Transpose [duplicate],"This question already has answers here : Why is a projection matrix symmetric? (4 answers) Closed 7 years ago . By inspecting its formula one can see easily that the matrix for projection onto a subspace is equal to its transpose. But what is the underlying ""geometric"" reason for this equality?  I have hard time figuring out why it has to be so? EDIT:
After reviewing the suggested links, it became more clear, but the reasoning was still escaping my intuition.    However, I think that I can (based on the formal arguments in the links) put forward a rough argument.  Basically, for any operator $A$ and two vectors $x$, $y$ $$<Ax,y>=<x,A^Ty>$$ For a vector $y$ in the orthogonal complement to the subspace S we're projecting onto using the $P^S$ projection operator, $$<P^Sx,y>=0=<x,(P^S)^Ty>$$ Because we took any vector $x$, it means that $(P^S)^Ty=0$.  As $y$ is in the orthogonal complement, $P^Sy=0$ this means that $$(P^S)^Ty=0=P^Sy$$
(in plain English, for any vector in the orthogonal complement to $S$ the action of $P^S$ makes it 0, which is a symmetric action). For $y$ in $S$ and any $x$, $<P^Sx,y>=<x,y>$ and $P^Sy=y$ (by the properties of the projection operation), but also $<P^Sx,y>=<x,(P^S)^Ty>$ (true for any operator). Putting these together that yields that:$$(P^S)^Ty=y=P^Sy$$. (in plain English, for any vector in $S$ the action of $P^S$ is the identity, which is a symmetric action). To sum it up, this shows that the operator $P^S$ acts symmetrically on both elements of S and on elements in its orthogonal complement.  Being that it is a linear operator, then it acts symmetrically on all vectors $x$. A big thing why this works (I think) is that the concept of projection implies (finite) dimension inner product space and that is much more structured than a simple (finite) dimensional vector space. LAST EDIT:
Even more clear after digesting the answer of Trial and Error. Given a subspace M to project onto, any vector $z_1$ can be written as the sum of two orthogonal components: $$z_1= (z_1-P_{\mathcal{M}}z_1) + P_{\mathcal{M}}z_1,$$ But the key geometric thing is that these two components belong to orthogonal subspaces , M and $\perp M$ (any vector from one subspace is perpendicular on any vector from the other subspace). The projection operator will leave the  component in M unchanged and annihilate the component in $\perp M$. Because of the fact that the subspaces themselves are orthogonal the product of ANY two arbitrary vectors $z_1, z_2$ $$<z_1,z_2>=<(z_1-P_{\mathcal{M}}z_1) + P_{\mathcal{M}}z_1,(z_2-P_{\mathcal{M}}z_2) + P_{\mathcal{M}}z_2>$$
becomes (by the orthogonality of M and $\perp M$:
$$<z_1,z_2>=<(z_1-P_{\mathcal{M}}z_1),(z_2-P_{\mathcal{M}}z_2)>+<P_{\mathcal{M}}z_1,P_{\mathcal{M}}z_2>$$ But applying $P_M$ annihilates/zeroes the first bracket and leaves unchanged the second bracket.  This holds whether we apply it to $z_1$ or to $z_2$ . Ergo the effect of $P_M$ is the same whether applied to the first of second term of the scalar product.  So its matrix should be equal to its transpose. The bolded text indicates key issues that I was not fully appreciating before. Thank you for your patience!","['linear-algebra', 'linear-transformations']"
2040440,The residue field of the algebraic closure of a complete valued field,"The following is taken from Cassels' ""Local Fields"": Let $k$ be a field which is complete w.r.t. a non-arch valuation and whose residue field $\rho$ is perfect. On page 123, in Corollary 3, he makes the following claim: ""the residue field of the algebraic closure of $k\;$ = $\;$the
  algebraic closure of the residue field of $k$"" He proves this as follows: 1) Take any irreducible polynomial $f$ in $\rho[X].$ 2) Lift it to a polynomial $\widetilde{f}$ in $k[X]$. 3) Every root of $\widetilde{f}$ lies in the algebraic closure $\bar{k}$ of $k.$ 4) Hence every root of $f$ lies in the residue class field of $\bar{k}.$ 5) Done. Question. Why we are done after step 4?","['number-theory', 'algebraic-number-theory', 'field-theory', 'commutative-algebra']"
2040450,"Using Mean Value Theorem, Prove that ${\tan x\over x}>{x\over\sin x}$","Prove using Mean Value Theorem, $${\tan x\over x}>{x\over\sin x} \space\forall \space x \space\in(0, \pi/2) $$ Attempt:: $f(x) = x-\sin x$ $f^\prime(x) = 1-\cos x > 0 $ Hence $x-\sin x>0, {x\over\sin x}>1$ Similarly, I got ${\tan x\over x}>1$ But how do I compare them and get the required inequality?","['inequality', 'calculus', 'limits']"
2040602,Representing a differential equation as a gradient of a function,"Suppose I have a linear dynamical system that is described by the following differential equation $$\dot{\bf{x}} = \bf{A}\bf{x}$$ I wish to represent the right-hand side of this system as a gradient of a function $U({\bf x})$ . Now, if $\bf{A}$ is symmetric, I can construct a quadratic form of $\bf{A}$ , so that: $$ U({\bf x}) = \frac12 \bf{x}^\top \bf{A} \bf{x} $$ and $$ \dot{{\bf x}} = {\bf I} \dfrac{dU}{d{\bf x}} $$ where $\bf I$ is identity matrix of a proper order. However, how to construct $U({\bf x})$ when $\bf{A}$ is not symmetric? Note that for the problem I am solving I can't have $\bf I$ something else rather than identity. One idea I was trying to explore was decomposing $\bf{A}$ as a sum of symmetric and skew-symmetric matrices, but it led me nowhere.","['matrices', 'gradient-flows', 'multivariable-calculus', 'ordinary-differential-equations', 'linear-algebra']"
2040710,Proving a function is well defined,"The question I'm working on asks: Let $J_4 =\{0,1,2,3\}$. Then $J_4 âˆ’\{0\}=\{1,2,3\}$. Student C tries to define
  a function S: $J_4 -\{0\}\to J_4 -\{0\}$ as follows: For each $x \in J_4 - \{0\}$,
  $S(x)$ is the number $y$ so that $(xy) \bmod 4 = 1$. Student F claims that $S$
  is not well defined. Who is right: student C or student D? Justify
  your answer. I have several questions: What is the salience of $J_4 - \{0\}$ ? I see that it removes $0$ from the list of elements, but this notation confuses me. I'm not sure exactly what it does, and it appears to be done twice (See function $S$). Does subscript in $J_4$ mean $\bmod 4$ anything in the set? The statement ""$S(x)$ is the number $y$ so that $(xy) \bmod 4 = 1$"" is also confusing me. $x$ remains the input and $y$ is the result of function $S$, right? To show that this is ill-defined would require showing that for an $x$, there are multiple $y$, making this not a function. To that it is well-defined I would have to do the opposite, right? For a problem like this, should I start off by just plugging numbers in and seeing what happens, or is there a systematic approach I should be aware of?","['modular-arithmetic', 'discrete-mathematics']"
2040717,Complex Differentiation - why treat z as a single variable,Why does one treat in Complex Analysis in the case of a function of a complex variable $z=x+i y$ the complex variable $z$ as a single variable and not the imaginary part just as another axis? In this way one could define a directional derivative and have more freedom?,['complex-analysis']
2040720,Combinatorics for a 3-d rotating automaton,"Let's suppose that we have some kind of special 3-dimensional
rotating automaton. The automaton is capable to generate rotation about selected  $X$ or $Y$ or $Z$ axis (in a current frame) in steps by only constant + $\dfrac{\pi}{6}$ angle (i.e. rotation can be generated only in one direction - reverse rotation is prohibited) so  transition from matrix $R_{i-1}$ to  $R_{i}$ (right-lower indices denote here states before and after a single step) is achieved with the use of formula: $R_{i}=Rot_{x,y,z}( \dfrac{\pi}{6})R_{i-1}$ Initial state is coded as the identity matrix $R_0=I$, all other states are described as rotation matrices in reference to the frame representing by this  $I$ matrix. Questions: how many $n$ distinct states (coded in generated matrices) can be
achieved for  not limited number of steps.  This full set of achievable states coded $\{^{1}R ,^{2}R ...{^{n}R} \}$ might be named to be a full space of rotating automaton (here left-upper indices should be somehow reasonably organized, but hard to say how - it's open issue) - all states and transitions between states can be, perhaps, visualized with the use of a graph how many distinct states can be generated by exactly $6$ steps in
the automaton (...maybe there is a general formula for $n$ steps ?) by how many ways can be achieved multi-step rotation from $I$ to $I$
with the condition that on this trajectory of states the same one step  transition ${^{j}R}{\rightarrow}{^{k}R}$ (if possible) is allowed only one time. (for example if it were only rotation about a single axis allowed - the number would be obviously $3$ i.e. three 12-step transitions, but in general case rotations about different axes can be mixed)","['graph-theory', 'rotations', 'matrices', 'spherical-trigonometry', 'combinatorics']"
2040726,How to compute $\int\limits_{\mathbb{R}^3}\det D^2 u(\mathbf{r}) d\mathbf{r}$?,"Let $u\in C^3(\mathbb{R}^3)$ and $u$ is zero outside some bounded domain. I need to find the value of 
$$
\int\limits_{
\large\mathbb{R}^3}\det \mathrm D^2 u(\mathbf{r})
\, \mathrm d\mathbf{r}
$$
I suspect it equals zero. I can show this for some specific cases. Here $\mathrm D u$ is a gradient of $u$, while $\mathrm D^2u$ is a Hessian.","['integration', 'calculus', 'vector-analysis']"
2040767,Details of Spivak's Proof of Stokes' Theorem,"In Spivak's Calculus on Manifolds, the proof of Stokes Theorem on $\mathbb{R}^n$ begins as follows... It seems to me that there's something here which can be very confusing: When you pull back the $k-1$ form $f dx^1 \wedge ... \wedge \widehat{dx^i} \wedge ... \wedge dx^k$ along ${I^k}_{(i,\alpha)}$, the result is again a $k-1$ form, which should be integrated over a $(k-1)$-cube. However, in the line below, the integral is over $[0,1]^k$. It amounts to the same  thing, since ${I^k}_{(i,\alpha)}^*(f dx^1 \wedge ... \wedge \widehat{dx^i} \wedge ... \wedge dx^k) = f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1}$ and then $$ \begin{aligned}& \int_{[0,1]^{k-1}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1} \\ = & \int_{[0,1]}\left(\int_{[0,1]^{k-1}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^{k-1}\right)dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^i, ..., x^{k-1})\,dx^1 \wedge ... \wedge dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^{i}, ..., x^{k-1})\,dx^1 ... dx^k \\ = & \int_{[0,1]^{k}}f(x^1, ..., x^{i-1},\alpha,x^{i+1}, ..., x^{k})\,dx^1 ... dx^k\end{aligned}$$ where the second line follows since the pulled back form is constant with respect to $x^k$ and the last line follows since we're working with the Riemann integral over $[0,1]^k$ so we're really just renaming variables. I think it's a bit of a stretch to ask the reader to 'note' that without any further indication as to why it's true. Spivak pulls a similar trick later on in the proof, which I noticed another StackExchange question on . After having run through the steps of the proof on a small example, I'm guessing that the reason for doing this is to avoid having to talk about renaming variables. So, my two questions are: Is there a simpler way to make sense of the 'note' which I addressed above? Am I correct in thinking that the extra integration is done to make the proof more concise and avoid discussion of renaming variables? Or is there some other reason I'm missing?","['stokes-theorem', 'differential-forms', 'calculus', 'vector-analysis']"
2040827,What can you say about a mapping $f : \Bbb Z\to \Bbb Q$?,"Written with StackEdit .
  Which of the following can be true for a mapping $$ f : \Bbb Z \to \Bbb Q $$ A. It is bijective and increasing B. It is onto and decreasing C. It is bijective and satisfied $f(n) \ge 0$ if $n \le 0$ D. It has uncountable image Here's my attempt at finding a mapping that satisfies C(Following Method to prove countability of $\Bbb Q$ from Stephen Abbot  ) Define $A_n = \{ \frac ab : a,b\ge 0, \gcd(a,b) = 1,a+b=n,b \ne 0 \}$ $B_n = \{ \frac {-a}{b} : a,b > 0, \gcd(a,b) = 1, a+b = n \} $ Mapping the negative integers and 0 to the set $A_n \forall  n \in \Bbb N$ and positive integers to the set $B_n \ \forall  n \ \in \Bbb N$ in the same manner as Stephen Abbot, the mapping will be surjective because the sets $A_n$ and $B_n$ would certainly cover $\Bbb Q$ and would be injective because no two sets have any elements in common. Correct Answer - C Source - Tata Institute of Fundamental Research Graduate Studies 2014 Is my proof correct? In any case, I can't imagine such a method to be required in this problem so is there a more 'easy' mapping? Also, why can't we have a mapping as desired in A and B?","['elementary-set-theory', 'real-analysis', 'functions', 'proof-verification']"
2040841,Show that $f$ is the zero function if $f''(x)=f(x)$ and $f(0)=f'(0)=0$,"Suppose that $f''(x)=f(x)$ for all real numbers $x$, and that $f(0)=f'(0)=0$. Show that $f$ is the zero function. I know that $f''(0)=0$ from the assumptions listed. I want to consider the Taylor series for $f$ centered at $x=0$, which is $\sum$ $[f^{(n)}(0)/n!]x^n$. I am not sure where to go from here.","['real-analysis', 'ordinary-differential-equations']"
2040851,Are transformations dense in operators?,"Assume that $M$ is a paracompact Hausdorff space (second countable, if necessary) $\mu$ is a Borel measure on $M$ (Radon or regular, if necessary) Let $Homeo(M)$ be the group of homeomorphisms $\phi:M\to M$, and let $Homeo(M,\mu)\subset Homeo(M)$ be the monoid of such $\phi$ that the Radon-Nikodym derivative $d\mu/d\mu_\phi\in L^\infty(M,\mu)$, where $\mu_\phi=\mu\circ\phi$. Consider the regular representation of $Homeo(M,\mu)$ on the Hilbert space $L^2(M,\mu)$,
$$
T_\phi f(x)=f(\phi(x)).
$$ 
I guess all $T_\phi\in\mathcal{L}(L^2(M,\mu))$ (bounded) for $\phi\in Homeo(M,\mu)$. Unless $\phi$ is an isometry for $\mu$ in which case $\mu_\phi=\mu$ and $T_\phi$ is unitary, $T_\phi$ is hardly goig to be normal, so no reference to spectral theory can be made. Question: Consider the complex subalgebra $\mathbb{C}[T_{Homeo(M,\mu)}]\subset\mathcal{L}(L^2(M,\mu))$ generated by the image of $Homeo(M,\mu)$ through $T$. Is this subalgebra dense in the weak operator topology? Comments are welcome. Answers for special cases as $M$ being a manifold and $Diff(M)$ instead of $Homeo(M)$ are welcome as well. References would be most appreciated. Thank you.","['functional-analysis', 'measure-theory', 'operator-theory']"
2040861,Is logarithm the only analytic function such that Re[f(z)]=Re[f(|z|)]?,Complex logarithm has an amazing property: its real part is rotationally invariant in the complex plane: $$\Re\ln z=\Re\ln|z|.$$ But even squaring $\ln$ leads to a non-symmetric real part. So I wonder: is logarithm the only nonconstant analytic function with such property?,['complex-analysis']
2040892,Sum of all consecutive natural root differences on a given power,"I accidentally observed that $\sqrt{n} - \sqrt{n-1}$ tends to $0$ for higher values of $n$, so I've decided to try to sum it all, but that sum diverged. So I've tried to make it converge by giving it a power $m$. $$S_m=\sum_{n=1}^\infty (\sqrt{n} - \sqrt{n-1})^m$$ How would one calculate the values of this sum for a choosen $m\in\mathbb R$? Not just estimate but write them in a non-decimal form if possible, preferably using a better converging formula. It converges if $m>2$. The values seem to tend to numbers with non-repeating decimals. Thanks to achille hui and his partial answer here , it looks like $S_m$ for odd values of $m$ is a linear combination
of riemann zeta function at negative half-integer values: \begin{align} S_3 &\stackrel{}{=} -6\zeta(-\frac12) \approx 1.247317349864128...\\ S_5 &\stackrel{}{=} -40\zeta(-\frac32) \approx 1.019408075593322...\\ S_7 &\stackrel{}{=} -224\zeta(-\frac52) - 14\zeta(-\frac12) \approx 1.00261510344449... \end{align} If we decide to replace the constant $m$ with $n\times{k}$ where $k$ is a new constant, then we can talk about $S_k$, Which converges if $k>0$; $$S_k=\sum_{n=1}^\infty (\sqrt{n} - \sqrt{n-1})^{nk}$$ I wonder if these values could also be expressed in a similar way as $S_m$. Values still tend to numbers with seemingly non-repeating decimals according to the Wolfram Alpha : $$ S_1 \approx 1.20967597004937847717395464774494290
$$ Also notice that the functions of these sums are similar to the zeta function; $\color{blue}{S_m} \sim \color{red}{\zeta}$ But I think it's only due the fact that they all approach $1$?","['recreational-mathematics', 'riemann-zeta', 'sequences-and-series']"
2040961,Does $\sum\limits_{i=1}^{\infty}|a_i||x_i| < \infty$ whenever $\sum\limits_{i=1}^{\infty} |x_i| < \infty $ imply $(a_i)$ is bounded?,"Written with StackEdit . Suppose $(a_i)$ is a sequence in $\Bbb R$ such that $\sum\limits_{i=1}^{ \infty} |a_i||x_i| < \infty$ whenever $\sum\limits_{i=1}^{\infty} |x_i| < \infty$. Then  is $(a_i)$ a bounded sequence? Look at the end of the question for the right answer. If the statement '$(a_i)$ is a properly divergent sequence implies that there exists some $k \in \Bbb N$ such that $\sum\limits_{i=1}^{\infty} {1/{a_i}}^k$ is convergent' was true, we could have easily proven $(a_i)$ is bounded by using sub-sequences but since that is dis-proven by $ln(n)$, can we use something around it? Like can all the functions which do not satisfy the 'statement' I mentioned be considered as a special case of functions? Correct Answer - Yes, $(a_i)$ is bounded. Source - Tata Institute of Fundamental Research Graduate Studies 2013","['real-analysis', 'inequality', 'sequences-and-series']"
2041048,Show $\sum_{n=1}^\infty \left(\frac{n}{2n-1}\right)^n$ converges,"I'm trying to show that $\sum_{n=1}^\infty \left(\frac{n}{2n-1}\right)^n$ converges. Using the Limit Ratio Test for Series, we want to show that $\lim_{n\to \infty} \left\lvert \frac{a_{n+1}}{a_n}\right\lvert<1$. However, I'm having trouble finding said limit (I know that it is equal to $\frac{1}{2}$, but I don't know how show it). Thanks in advance","['sequences-and-series', 'convergence-divergence', 'limits']"
2041128,"If $f$ is differentiable at $c$, then $F'$ is continuous at $c$ where $ F(x)=\int_a^x f(t)\ dt$?","Let $f$ be Riemann integrable on $[a, b]$, let $c\in(a, b)$, and let
$\displaystyle F(x)=\int_a^x f(t)\ dt$, $a\le x\le b$. For the following statement, give either a proof or a counterexample: If $f$ is differentiable at $c$, then $F'$ is continuous at $c$. My attempt: My hunch is that the statement is false. Since $f$ is differentiable at $c$, $f$ is continuous at $c$ and so by the first Fundamental Theorem of Calculus, $F'(c)=f(c)$. But $F'$ need not be equal to $f$ at other points. Is this correct? If so, how do I find a counterexample? If not, how do I prove the statement?",['real-analysis']
2041142,Number of intersections of $n-2$ dimensional spheres inside $S^{n-1}$,"Consider the sphere $S^{n-1} = \{ (x_1,\ldots, x_n)\in\mathbb{R}^n:\ x_1^2+\ldots +x_n^2=1 \}$ and denote by $S_i$ (for $i=1\ldots n$) the $n-2$ dimensional subsphere of $S^{n-1}$ orthogonal to $e_i$. More precisely, $S_i = \{ (x_1,\ldots, x_{i-1},0,x_{i+1},\ldots, x_n):\ x_1^2+\ldots+x_{i-1}^2+x_{i+1}^2+\ldots+x_n^2=1 \}$. The image below gives an illustration in the 3 dimensional case. In dimension 3, it's easy to see the number of intersection between the $S_i$'s. We have that $\sharp (S_1\cap S_2) = \sharp(S_1\cap S_3) = \sharp(S_2\cap S_3) = 2,$ $\sharp(S_1\cap S_2\cap S_3) = 0$. I'm interested in the number of intersections in the general case. My guess is the number of intersections will be a power of 2, depending on how many $S_i$'s I am intersecting. I don't know how to make this calculations and couldn't find any article or textbook about this. My last hope is to share my problem here. Thank you!","['combinatorics', 'spheres']"
2041154,Calculating probability of Type II Error for a one-sample t test [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question So I have the following problem: A transportation company is suspicious of the claim that the average useful life of certain tires is at least 28,000 miles. To verify that, 40 tires are placed in trucks and an average useful life of 27463 is obtained with a standard deviation of 1348 miles. a) Test this hypothesis with a level of significance of Î± = 0.01 b) If $\mu_1 = 27230,$ calculate the probability of Type II Error Any hint on how to do b)?","['statistics', 'hypothesis-testing']"
2041186,solving/approximating integral of lognormal cdf,"I am trying to solve the integral, \begin{equation}
\int_{0}^b\Phi_{LN}(x, \mu,\sigma)dx
\end{equation} where $\Phi_{LN}(x, \mu,\sigma)$ is the lognormal cdf, evaluated at $x$.  i.e., $$\int^x_{0}\frac{1}{\sqrt{2\pi\sigma}y}\exp\left({-\frac{1}{2}\left(\frac{\log y-\mu}{\sigma}\right)^2}\right)dy$$. Being able to efficiently compute the outer integral (top equation) would be great. This would ideally involve expressing it in a form that most software could efficiently solve/approximate. I thought I could have solved this problem by solving the integral to a standard normal cdf, and simply exchanging $x$ with $\frac{log(x)-\mu}{\sigma}$, however this does not seemed to have worked. In case someone wants to use the integral of the standard normal cdf to answer this question it is shown following, $$
\int\Phi_{N(0,1)}(x) = x\Phi_{N(0,1)}(x) - \int x\Phi^{'}_{N(0,1)}(x)dx
$$ and here is my failed attempt to convert it to work as the lognormal equivalent, > # define function to integrate N(0,1) CDF
> int_LN_CDF = function(x, mu, sigma){
+   # transform to lognormal RV
+   y = (log(x) - mu)/sigma
+   # integral of standard normal cdf
+   y*pnorm(y) + 1/sqrt(2*pi)*exp(-(y^2)/2)
+ }
> # set arbitary values of mu and sigma 
> mu = log(2); sigma = 0.4
> # test function against numerical approach
> int_LN_CDF(2, mu, sigma) - int_LN_CDF(1, mu, sigma)
[1] 0.3820694
> integrate(function(y) pnorm(y, mu, sigma), lower = 1, upper = 2)
[1] 0.9491263 with absolute error < 1.1e-14 Any help or guidance would be appreciated. Thanks","['statistics', 'definite-integrals', 'probability', 'normal-distribution']"
2041189,Mondrian Art Problem Upper Bound for defect,"Divide a square of side $n$ into any number of non-congruent rectangles. If all the sides are integers, what is the smallest possible difference in area between the largest and smallest rectangles? This is known as the Mondrian Art Problem .  For example, here's a division for a square of size $n=138$ .  The largest area is 1200, the smallest 1178, with a difference of 22. So far, corresponding sequence A276523 of known optimal Mondrian dissections have values 2, 4, 4, 5, 5, 6, 6, 8, 6, 7, 8, 6, 8, 8, 8, 8, 8, 9, 9, 9, 8, 9, 10, 9, 10, 9, 9, 11, 11, 10, 12, 12, 11, 12, 11, 10, 11, 12, 13, 12, 12. $\left \lceil{\frac n{\log(n)}}\right \rceil +3$ seems to be an upper bound for the Mondrian Art Problem. So far, the differences between that upper bound and the known optimal values comprise A278970 . Values to a(65) are verified, with best known values to a(100). x x 4 2 3 2 2 1 2 0 2 1 1 3 1 1 2 2 2 1
1 2 3 2 1 2 2 3 3 1 2 3 1 1 2 2 3 4 3 2
2 3 3 3 2 3 4 2 4 3 2 4 3 2 3 3 3 3 4 3
3 5 4 4 4 3 1 1 2 1 2 0 1 1 1 2 1 0 1 2
1 2 2 1 1 5 1 3 1 0 1 2 2 0 0 1 1 2 -2 1 From the current best-known solutions, next ""4"" values are at 176, 241, 245, 289. a(86)=a(139)=5, a(526)=6, a(280)=a(435)=7, a(700)=8, a(324)=a(811)=9, a(138)=a(832)=10. I have a ""14"" value for square 758, defect 104 (35964-32860) with 16 rectangles. Can anyone beat the upper bound by more than 14? The last ""hole"" in the first 96 squares was for the following, with best known value a(78)=0 $((\left \lceil{\frac {78}{\log(78)}}\right \rceil +3)-(390-369) = 0)$ . Under 105, the only a(99)=-2 has a best known solution going over the upper bound. Is there a better solution for a(99)?  I'm not sure why that simple upper bound seems to work so well for a problem with this much chaos. Is it valid? For eye candy, here are optimal rectangle dissections for squares 3 to 32 optimally packed into a rectangle ( method ): In a related problem, A279596 looks at the same problem where rectangles can be re-used if they have a different orientation. $\left \lceil{\frac n{\log(n)}}\right \rceil$ seems to be an upper bound. The distance from that bound is currently known as follows for the first 100 terms. The first 45 terms of A279848 are verified optimal by R. Gerbicz. x x 1 1 1 1 1 1 2 1 1 1 2 2 1 1 2 1 1 3   
1 2 2 2 2 2 2 2 2 1 3 4 4 2 3 3 3 3 3 3   
4 4 4 4 3 3 1 2 1 1 5 2 2 1 2 2 1 1 0 3   
0 2 1 2 0 0 1 1 1 1 0 1 1 4 1 0 2 0 3 1   
4 3 1 1 4 2 3 1 0 4 4 0 1 1 0 0 -2 1 -1 -1 A plot of best known values to a(96) follows. Here's the best known division of the size 51 square, into rectangles of area 160 to 168, for a defect of 8 (168-160). $\left \lceil{\frac {51}{\log(51)}}\right \rceil=13$ , so this division has a quality of 5 (13-8). From the current best-known solutions, next ""4"" values are at 74, 81, 85, 90, 91, 137, 150, 280, 435. a(151)=a(700)=5. a(324)=a(811)=6. a(138)=a(832)=7. a(103)=9. a(758)=11. Can anyone beat a quality value of 11? The best known values for squares 97, 99, 100 exceed the upper bound. Are those fixable? The last ""hole"" in the first 96 squares was for the following, with best known value a(83)=1 $(\left \lceil{\frac {83}{\log(83)}}\right \rceil -(468-450) = 1)$ . Is upper bound of $\left \lceil{\frac n{\log(n)}}\right \rceil$ valid for this variety of the Mondrian Art problem? Another packing problem with an odd upper bound is Oblongs into minimal squares .","['graph-theory', 'packing-problem', 'recreational-mathematics', 'combinatorics', 'oeis']"
2041234,Connection on Tautological Bundle over $\mathbb{C}P^1$.,"I'm trying to compute the Chern-class of the bundle $$\gamma = \{(c,\ell): c \in \ell \} \subseteq \mathbb{C}^2 \times \mathbb{C}P^1$$ over $\mathbb{C}P^1$. I'm running into a problem defining an affine connection on this bundle. Any tips? So far I've tried to use the bundle $\mathbb{C}^2 \times \mathbb{C}P^1$. I don't see a natural way to take a derivative of a section of this bundle over $\mathbb{C}P^1$. And yes, this computation is easy via topological argument, but I'm interested in the Chern-Weil computation.","['algebraic-topology', 'characteristic-classes', 'differential-geometry']"
2041240,Why is $ 1^{\infty} \not= \lim_{t \to \infty} 1^t = 1$?,"Why is $ 1^{\infty} \not= \lim_{t \to \infty} 1^t = 1$? It seems like they're saying the same thing. After all, $1$ to the power of any positive integer is equal to $1$. I suspect that I'm misunderstanding some fundamental concept and would appreciate it if someone could explain it to me. Thank you.","['infinity', 'limits']"
2041254,"Find all solutions to $x^2\equiv 1\pmod {91},\ 91 = 7\cdot 13$","I split this into $x^2\equiv 1\pmod {7}$ and $x^2\equiv 1\pmod {13}$. For $x^2\equiv 1\pmod {7}$, i did:
$$ (\pm1 )^2\equiv 1\pmod{7}$$ $$(\pm2 )^2\equiv 4\pmod{7}$$ $$(\pm3 )^2\equiv 2\pmod{7}$$ Which shows that the solutions to $x^2\equiv 1\pmod {7}$ are $\pm1$. For $x^2\equiv 1\pmod {13}$, i did:
$$ (\pm1 )^2\equiv 1\pmod{13}$$ $$(\pm2 )^2\equiv 4\pmod{13}$$ $$(\pm3 )^2\equiv 9\pmod{13}$$ $$ (\pm4 )^2\equiv 3\pmod{13}$$ $$(\pm5 )^2\equiv {-1}\pmod{13}$$ $$(\pm6 )^2\equiv 10\pmod{13}$$Which shows that the solutions to $x^2\equiv 1\pmod {13}$ are $\pm1$. Thus, I concluded that the solutions to $x^2\equiv 1\pmod {91}$ must be $\pm1$. I thought that $\pm1$ were the only solutions, but apparently I am incorrect! How do I go about finding the other solutions to this congruence?","['number-theory', 'modular-arithmetic']"
2041311,"If $\sum\limits_{n=1}^âˆž|a_n|^2$ converges, then $\prod\limits_{n=1}^âˆž(1+a_n)$ converges if and only if $\sum\limits_{n=1}^âˆža_n$ converges.","If $\sum\limits_{n=1}^{\infty}|a_n|^2$ converges, then $\prod\limits_{n=1}^{\infty}(1+a_n)$ converges if and only if $\sum\limits_{n=1}^{\infty} a_n$ converges. My attempt is to use the Cauchy citerion, given $m>n$ , $$\left|\prod_{i=1}^{m}(1+a_i)-\prod_{i=1}^{n}(1+a_i)\right|=\left|\prod_{i=1}^{n}(1+a_i)\left[\prod_{i=n+1}^{m}(1+a_i)-1\right]\right|\\
=\left|\prod_{i=1}^{n}(1+a_i)\right|\left|\prod_{i=n+1}^{m}(1+a_i)-1\right|,\\
\left|\prod_{i=1}^{n}(1+a_i)\right| \leqslant \prod_{i=1}^{n}(1+|a_i|),$$ then how to proceed?","['complex-analysis', 'infinite-product']"
2041344,Radius of convergence and convergence sets of $\sum\limits_n\frac{2n+1}{(n-1)^2}x^n$ and $\sum\limits_n(-1)^n(\sqrt{n+1}-\sqrt{n})x^n$,I want to find the radius of convergence of the following series and the set of $x\in \mathbb{R}$ in which the series converge. $$\sum_{n=2}^{\infty}\frac{2n+1}{(n-1)^2}x^n$$ $$\sum_{n=0}^{\infty}(-1)^n(\sqrt{n+1}-\sqrt{n})x^n$$ To find the radius of convergence we have to compute the limit $\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}$. Do we do something elese at the first case where the sum starts from $2$ and not from $0$ ? I have done the following: $$\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}=\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{(n-1)^2}x^n\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{(n-1)^2}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{2n+1}{n^2-2n+1}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{\frac{2}{n}+\frac{1}{n^2}}{1-\frac{2}{n^2}+\frac{1}{n^2}}\right|}$$ $$\lim_{n\rightarrow \infty}\sqrt[n]{|a_n|}=\lim_{n\rightarrow \infty}\sqrt[n]{\left|(-1)^n(\sqrt{n+1}-\sqrt{n})x^n\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\sqrt{n+1}-\sqrt{n}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{(\sqrt{n+1}-\sqrt{n})(\sqrt{n+1}+\sqrt{n})}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{n+1-n}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{1}{\sqrt{n+1}+\sqrt{n}}\right|}=|x|\lim_{n\rightarrow \infty}\sqrt[n]{\left|\frac{1}{\sqrt{n+1}+\sqrt{n}}\right|}$$ $$$$ Is this correct so far? How could we continue?,"['real-analysis', 'convergence-divergence', 'analysis']"
2041396,Is a quotient of group schemes well defined?,"I will first define the notions of exact,surjective for group schemes and then ask my question. Let $B$ and $C$ be fppf group schemes over S. The book I am reading defines a homomorphism $f:B\to C$ over $S$ to be surjective if for every $S$-scheme $T$ and element $c \in C(T)$, there is an fppf morphism $T'\to T$ such that the image of $c$ in $C(T')$ is the image of some $b\in B(T')$. Call a sequence of fppf group schemes $A\to B\to C$ exact at $B$ if the composition is the trivial homomorphism and the induced homomorphism $A\to \ker(g)$ is surjective. If $A$ is the kernel of $B\to C$, then the book defines the quotient $B/A$ to be $C$. Question: Is this well defined? That is, if $f:B\to C$ and $g:B\to D$ are two surjective homomorphisms such that $\ker(f) \cong \ker(g)$ over $B$, does this imply that $C\cong D$ so that the relevant square commutes? My attempt: It suffices to show that for all affine schemes $T$, there is a functorial bijection $C(T) \to D(T)$. If I define $\hat T$ to be the inverse limit of fppf maps $T'\to T$, then since for locally finite presentation schemes, it's functor of points commutes with taking direct limits of rings, I can show that there is a bijection $C(\hat T) \to D(\hat T)$. I don't think this is enough to show that $C\cong D$ however...","['arithmetic-geometry', 'group-schemes', 'algebraic-geometry']"
2041456,What does this $0$ mean in Wolfram|Alpha's application of the chain rule?,"If I enter this command to differentiate $(2+3x)^4$ I get the step-by-step output displayed here . The relevant part of the output is as follows: Possible derivation: $\frac{d}{d x}\left((2+3x)^4\right)$ Using the chain rule, $\frac{d}{d x}\left((3x+2)^4\right) = \frac{d u^4}{d u}0$, where $u=3x+2$ and $\frac{d}{d u}\left(u^4\right)=4u^3$: $4(2+3x)^3\left(\frac{d}{d x}(2+3x)\right)$ I picked here a very simple example that could easily be done by hand. However, Wolfram|Alpha would use the same notation even if I picked a more complex example. I understand in basic terms what needs to be done to reach a solution, but I don't understand what the $0$ is doing in the notation.","['derivatives', 'calculus', 'wolfram-alpha']"
2041462,To solve the non linear IVP $y'(t)=1-y^2(t)$,"Let $y: \mathbb{R} \rightarrow \mathbb{R}$ satisfy the initial value problem $$y'(t)=1-y^2(t), \ t \in \mathbb{R},$$ $$y(0)=0$$
  Then which of the following is true $(A)$ $y(t_1)=1$ for some $t_1 \in \mathbb{R}$ $(B)$ $y(t)>-1$ for all $t \in \mathbb{R}$ $(C)$ $y$ is strictly increasing in $\mathbb{R}$ $(D)$ $y$ is increasing in $(0,1)$ and decreasing in $(1,\infty)$ I am a biology researcher learning differential equations for related work. I did  an undergraduate course in differential equations but have forgotten most of it. Any help would be appreciated.",['ordinary-differential-equations']
2041487,Variances of unbiased estimator $W_1$ and $E(W_2|T)$ for another unbiased estimator $W_2$ and a sufficient statistic $T$,"Let $W_1, W_2$ be unbiased estimators of $\theta$ and let $T$ be a sufficient statistic for $\theta$. Is it true that $Var(E(W_1|T)) \leq Var(W_2)$? I think that it may fails to be true. If the statement is true then $Var(E(W_1|T))$ is the minimum among the variances $Var(W)$ of all unbiased estimator $W$ so $E(W_1|T)$ is a UMVUE of $\theta$. But Lehmann-Scheffe's theorem tells us that if $T$ is a complete sufficient statistic and $U$ is a unbiased estimator then $E(U|T)$ is a UMVUE of $\theta$. So we don't need the completeness of $T$ if the statement is true. However I cannot prove it. Anyone can help me?","['statistics', 'probability', 'statistical-inference']"
2041499,A particular solution to $\frac{d^2y}{dx^2}+y =\csc x$,"The solution to the BVP $\frac{d^2y}{dx^2}+y =\csc x$ , $0 < x < \frac{\pi}{2}$ $y(0)=0$ , $y(\pi/2)=0$ is $(A)$ Concave $(B)$ Convex $(C)$ Negative $(D)$ Positive For the homogeneous problem of course $\sin x$ and $\cos x $ are linearly independent solutions. I have trouble finding a particular solution to the non-homogeneous problem. Any help would be much appreciated. PS: There could be multiple correct options.",['ordinary-differential-equations']
2041578,Independence of non-constant real variables,"Show that if $\Omega$ has $n \in \Bbb N$ elements, then it exist at most $2^n$ independent non-constant real-valued random variables on $(\Omega,\mathcal F, P)$. A real-valued random variable is called constant if $P(X=a)=1$ for some $a \in \Bbb R$. The only idea that I have in mind for now is to somehow use variance since a random variable in not a constant one if its variance is not $0$.
Also I suppose that the given sigma-algebra has exactly $2^n$ elements so it might be applied in the solution ? I just really a starting point for this proof. Any hint will be highly appreciated.","['independence', 'probability-theory']"
2041587,The sequence $a_n=\frac{2n+1}{(n-1)^2}$ decreases monotonically,How could we show that the sequence $a_n=\frac{2n+1}{(n-1)^2}$ decreases monotonically? When we take the quotient $\frac{a_n}{a_{n+1}}$ we get $\frac{n^2(2n+1)}{(n-1)^2(2n+3)}$. Howcan we conclude that this quotient is $\geq 1$ ?,"['real-analysis', 'inequality', 'sequences-and-series', 'analysis']"
2041611,Does a glass of water sing because of the SO(2) symmetry?,"This might very well be a crucially flawed reasoning. But I think has to have something true behind. I was trying to explain basic ideas of representation of Lie Groups to an 11 years old girl who asked what I was studying. What I wanted to explain was the relation between special functions and symmetries. The mathematical thing I wanted to explain was that if we have a group $G$ acting on a space $X$, and we look at the space of infinitely derivable functions on $X$, i.e. $\mathcal C^\infty(X)$, then there is a natural representation of $G$ on $C^\infty(X)$. So I thought about the simplest example I had in mind which was the case of of the circle $G = S^1$ which has ($SO(2)$ symmetry). Then the representation are given by harmonic analysis of Fourier series. To explain this I said to consider a glass of water which has cylindrical symmetry (""if I rotate the glass you cannot say how much I did rotate the glass, so it has a symmetry"") then the vibration and deformations of the edge of the glass are the functions on $S^1$ that can be classified in harmonics... That's why - I concluded - glasses are used to sing with water... I said that but really I'm not sure at all if it's effectively the case. I mean that I'm note sure until what extension my reasoning was correct. Is the role of the water just the one to annihilate every representation but one (or some) which get excited and that's why the glass emits only one definite sound? Do you think the reasoning has a tragic flaw somewhere? To what extension is valid?","['real-analysis', 'functional-analysis', 'representation-theory', 'soft-question', 'lie-groups']"
2041670,Prove that two matrices commute iff the square of the matrices commute,"In my textbook there is a task in which I have to prove the relation
\begin{equation}
AB=BA\Leftrightarrow A^2B^2=B^2A^2.
\end{equation}
For ($\Rightarrow$) it is easy
\begin{equation}
AB=BA\Rightarrow (AB)^2=(BA)^2\Rightarrow ABAB=BABA\Rightarrow BBAA=AABB.
\end{equation}
But how do I prove ($\Leftarrow$)?",['matrices']
2041672,Placing dolls on shelves,"I have this problem for my school project.
I have eleven different dolls, and have to put them on four shelves.
Each shelve can hold all of the dolls. It matters which doll is on which shelf, and also the order of the dolls matters. Ex. (ABCD | EFGH | I | JK) is not the same as (ABCD | EFGH | I | KJ ). I have done some research, but everyone in their problems seems to not care about the order, or has more things of a kind. Question a) How could I compute number of possibilities to arrange the dolls, if no constraints are presented? b) Verify my thought process for ""on each shelf must be at least two dolls"" ad b)
Using function V(k, n) = n!/(n-k!) I'd choose two dolls for each shelf V(2, 11) * V(2, 9) * V(2, 7) * V(2, 5) and then use the solution of the first problem, only this time for three dolls only. Is this a valid solution or am I an idiot? Thanks","['combinatorics', 'discrete-mathematics']"
2041709,Derivative of intertwined matrix expression,"What are the derivatives of $\mathbf{\hat{y}}(\mathbf{w})$ with respect to the elements $w_{i}$ of the vector $\mathbf{w}$, where \begin{equation} 
\mathbf{\hat{y}}(\mathbf{w}) =  \mathbf{\Omega}^{T}\mathbf{a} + \mathbf{1}b
\end{equation} and $\mathbf{a}$ and $b$ can be found as follows
\begin{equation}
\begin{bmatrix} 
b \\ 
\mathbf{a} 
\end{bmatrix}
=
\begin{bmatrix} 
0 & \mathbf{1}^T \\ 
\mathbf{1} & \mathbf{\Omega}+diag(\frac{1}{\gamma\mathbf{w}})
\end{bmatrix}^{-1}
\begin{bmatrix} 
0 \\ 
\mathbf{y} 
\end{bmatrix}
\end{equation} Bold letters indicate vectors, $\mathbf{1}$ is a vector of ones, $\Omega$ a square matrix and $diag(\cdot)$ is a diagonal matrix with entries $(\gamma w_i)^{-1}$. I am a bit at loss due to the intertwined structure. Is this even possible? Both equations are related to the LSSVM classifier as described in ftp://ftp.esat.kuleuven.be/sista/ida/reports./98-72.pdf","['derivatives', 'statistics', 'matrix-calculus']"
2041721,How to write $\cos^3\left(\frac{\pi x}{2a}\right)$ in the form $\sum\limits_{j=1}^n k_j\cos\left(\frac{j\pi x}{2a}\right)$?,"For context; this is from a quantum mechanics problem where I need to write the wavefunction $$\psi(x)=\cos^3\left(\frac{\pi x}{2a}\right)$$ as a sum of single powered cosine terms with only the arguments and the constants $k_j$ for each term varying: $$\sum\limits_{j=1}^n k_j\cos\left(\frac{j\pi x}{2a}\right)\tag{1}$$ where $a$ is a constant (half the width of the infinite square well) This is my attempt so far: Since $$\cos^3\left(\frac{\pi x}{2a}\right)$$ is an even function $$\sum\limits_{j=1}^n k_j\cos\left(\frac{j\pi x}{2a}\right)$$ will only consist of terms with odd $j$  since I am solving a infinite square well problem for 
$$u_n(x)=\begin{cases}
\cos\left(\frac{j\pi x}{2a}\right),  & \text{if $j$ is odd} \\
\sin\left(\frac{j\pi x}{2a}\right), & \text{if $j$ is even}
\end{cases}$$ where each $u_n(x)$ is an eigenfunction or a term in the summation $(1)$ So if I make an ansantz that I can write $(1)$ in the form $$k_1\cos\left(\frac{\pi x}{2a}\right)+k_2\cos\left(\frac{3\pi x}{2a}\right)$$ with $j=1$ and $3$ only. But truthfully this is just a guess, and I still have no idea if it could be written for $j=5,7,9$ etc. Anyhow, proceeding with the assumption has me stuck as I don't understand how to find the values of $k_1$ and $k_2$. The correct answer is $$\fbox{$\cos^3\left(\frac{\pi x}{2a}\right)=\frac34\cos\left(\frac{\pi x}{2a}\right)+\frac14\cos\left(\frac{3\pi x}{2a}\right)$}$$ But how does one go about determining that $k_1=\frac34$ and $k_2=\frac14$? But most importantly; How do you know how many terms ($j=1,3$ in this case) are required? Is it really just a guess or is there some method to it? Best Regards. Edit: This question has had no response thus far. Perhaps I was asking too many questions before, so for now I will simply ask if anyone is able to give me some hints or tips on how to show that $k_1=\frac34$ and $k_2=\frac14$? Edit #2: In the comment below one user has given an incredibly useful link that explains the origin of that formula. But my question is; Unless you knew this from memory how would you find that formula? There must be some method that exists for finding the coefficients $k_1$ and $k_2$. If it is believed to be absolutely impossible to find these coefficients and no such method exists, then please state this. It is a completely valid answer. Many thanks.","['summation', 'trigonometry', 'trigonometric-series']"
2041747,Why is a gradient not a linear functional?,"I'm reading this book and on page 104 they define: Afterwards they said that if $m=1$, the function $f$ is real-valued and $T$ is the gradient which has to be a linear functional according to the definition. The problem is this is not true in general, see for example: $f:\mathbb R^2\to \mathbb R$ defined as $f(x,y)=\sin x$. The gradient is $\nabla f=(\cos x,0)$ which is not a linear functional.","['multivariable-calculus', 'analysis', 'definition']"
2041827,Why is ($dx)^2$ tiny quantity of $x^2$ and not of $x?$,When we say dx we are referring to a tiny quantity of $ x$ . And from literature the $(dx)^2$ is a tiny quantity of $x^2$ This is the part I am not clear about. A minute is $\frac{1}{60}$ of an hour so we could denote $\frac{1}{60}$ as $dx$ and the hour as $x$ Now a second $(\frac{1}{60})^2$ is a tiny quantity of a minute and therefore a tiny quantity of a tiny quantity of an hour. But I already said that hour is $x$. So how can we also say that it is a tiny quantity of $x^2$? (which is I guess $hour^2$?),"['derivatives', 'math-history', 'terminology', 'calculus']"
2041837,Inner product in dual Hilbert space,"Let $H^*$ be a dual space of a Hilbert space $H$. Then inner product is defined as
$$(f,g)=(J^{-1}f,J^{-1}g),$$
where $f,g\in H^*$ and $J\colon H\to H^*$ is the canonical isomorphism. I want to prove that 
$$\|f\|=\|J^{-1}f\|=\sqrt{(J^{-1}f,J^{-1}f)}=\sqrt{(f,f)}.$$
Any ideas on how to approach this proof?","['functional-analysis', 'hilbert-spaces']"
2041849,Abstract Algebra- Cosets,"I'm stuck on the following questions ""In the alternating group $A_4$, let $H$ be the cyclic subgroup generated by $(123)$. Find all the right cosets of $H$ and all the left cosets of $H$. Is every right coset also a left coset."" The answer is as follows ""$H = [idx_4 , (123), (132)]$ so each coset will have order 3, and since
$|A_4| = 12$ there will be 4 distinct cosets. We find the following right
cosets: $H = [idx_4 , (123), (132)]$ $H(234) =[ (234), (12)(34), (134)]$ $H(124) = [(124), (13)(24), (243)]$ $H(143) = [(143), (14)(23), (142)]$ and the following left cosets: $H = [idx_4 , (123), (132)]$ $(234)H = [(234), (13)(24), (142)]$ $(124)H = [(124), (14)(23), (134)]$ $(143)H = [(143), (12)(34), (243)]$ and we see that $H$ is the only coset that is both a left and a right coset."" Now I understand how you find the left and right cosets, the thing I'm stuck on is why did they choose $(234), (124), (143)$ out of the possible options from the alternating group $A_4$ as the permutations that $H$ is used on?","['permutations', 'abstract-algebra', 'group-theory']"
2041868,Characterization: Injective morphism of sheaves,"Let $X$ be an arbitrary topological space and $\varphi : \mathcal{F} \to \mathcal{G}$ be a morphism of sheaves over $X$. We call $\varphi$ injective if $\mathcal{K}:=\ker(\varphi) = 0$. I need to show that this is equivalent to the statement that for all $p \in X$ the morphisms $\varphi_p : \mathcal{F}_p \to \mathcal{G}_p$ induced on the germs are all injective. My main problems are the following: Embarrassingly, I do not get what $\ker(\varphi) = 0$ really means. Let us say we have sheaves over rings. Then the kernel is the zero ring. But does this also mean that for any open set $U \subset X$ the sections $\mathcal{K}(U) = \ker(\varphi_U)$ is also the zero ring? I still have no intuition for stalks and germs. We have defined stalks as some set equipped with some weird equivalence relation and germs as their equivalence classes. Neither do I know what injectivity means for the $\varphi_p$ nor how I get the desired conclusions with these. Could anyone explain this to me? Thanks in advance.","['morphism', 'sheaf-theory', 'algebraic-geometry']"
2041945,Must a group of exponential growth is virtually free?,"Let $G$ be a finitely generated group. As we know that if $G$ contains a non-abelian free group of finite index, then $G$ has exponential growth. Does the converse of it holds? i.e. if $G$ is of exponential growth does it imply that $G$ must contain a non-abelian free group of finite index.","['geometric-topology', 'riemannian-geometry', 'group-theory', 'geometric-group-theory']"
2041988,How to get inverse of formula for sum of integers from 1 to n? [duplicate],"This question already has an answer here : Reversing the $T(n) = \frac{n(n+1)}{2}$ formula (1 answer) Closed 7 years ago . I know very well that the sum of integers from $1$ to $n$ is $\dfrac{n\times(n+1)}2$. What I'm interested in today, and cannot find a solution for, is performing the opposite operation. Let $m = \dfrac{n^2 + n} 2$. Knowing the value of $m$, how do I figure out the value of $n$? I could easily program a solution but I'd much prefer an algebraic one.","['algebra-precalculus', 'summation']"
2041989,Olympiad level assignment problem having a use of geometry and trigonometry.,I have got an assignment having olympiad problems. One entry of these questions went this way: There are $n$ line segments in the plane with the sum of lengths equal to one. Prove that there exist a straight line such that the sum of lengths of projection of the segments on the line equals to $\frac{2}{\pi}$ . I was trying to use a bit of geometric construction and trigonometry but I donâ€™t think the data is sufficient for the method I want to use. I shall be highly thankful if you guys can suggest me how to approach for the problem. Thanks in advance.,"['contest-math', 'trigonometry', 'geometry']"
2042005,Infinite sums in vector spaces,"In Linear Algebra we often talk about vector spaces and the defined operations there ( excuse my english, I hope I can get my point across ). But whenever infinite sums come up or the possibility thereof, the professor strictly prohibits them and always mentions that they must be finite. I currently read additional literature and everywhere you find ""remember, XYZ must be finite!"". Wikipedia ( in an article about the basis vector ) says: ""The sums in the above definition are all finite because without additional structure the axioms of a vector space do not permit us to meaningfully speak about an infinite sum of vectors."" Still, this doesn't make it very clear to me, what the problem is. In calculus you can talk about them just fine and they have lots of nice properties, too. What is it about Linear Algebra, that makes infinite sums meaningless?","['sequences-and-series', 'linear-algebra', 'vector-spaces']"
2042019,Find $\sin \theta$ if $\tan \theta +\sec \theta =1.5 $,"Find $\sin \theta$ if $\tan \theta +\sec \theta =1.5 $ $$\tan \theta +\sec \theta =1.5 $$ $$2\tan \theta +2\sec \theta =3 $$ $$2\sec \theta =3-2\tan \theta$$ $$4\sec^2 \theta =(3-2\tan \theta)^2$$ $$4+4\tan^2 \theta =9-12\tan \theta+4\tan^2 \theta$$ So I get $$\tan \theta = \frac{5}{12}$$
Thus $$\sin\theta=\frac{5}{13}$$ But If I do like this , $$\frac{\sin \theta}{\cos \theta}+\frac{1}{\cos \theta} =\frac{3}{2}$$ $$ 2\sin\theta +2=3\cos \theta$$ $$ (2\sin\theta +2)^2=9\cos^2 \theta$$ $$ 4\sin^2\theta+8\sin\theta +4=9-9\sin^2\theta$$ $$13\sin^2\theta+8\sin\theta-5=0$$ Therefore I get two answers  $$\sin\theta=\frac{5}{13} , \sin\theta =-1$$ What is the reason behind this ? Why am I getting two answers in one method and one in another ?","['trigonometry', 'soft-question']"
2042101,Applying Cauchy's integral formula,"I'm trying to compute $$\int_\gamma \frac{z^4+z^2+1}{z^3-1} \, dz$$ where $\gamma$ is the circle $|z-i|=1$, using Cauchy's integral formula My solution is as follows: The integral can be rewritten as $$\int_\gamma \frac{\frac{z^4+z^2+1}{(z-1)\left(z+\frac{1}{2}-\frac{\sqrt{3}}{2}i\right)}}{z+\frac{1}{2}-\frac{\sqrt{3}}{2}i} \, dz = I$$ Then Cauchy's integral formula can be applied, giving $$I=2\pi if\left(-\frac{1}{2}+\frac{\sqrt{3}}{2}i\right)=0$$ Is this correct?","['complex-integration', 'cauchy-integral-formula', 'complex-analysis', 'contour-integration', 'solution-verification']"
2042107,Understanding a probability paradox,"Three prisoners are informed by their jailer that
one of them has been chosen at random to be
executed and the other two are to be freed. Prisoner
A asks the jailer to tell him privately which of
his fellow prisoners will be set free, claiming that
there would be no harm in divulging this information
because he already knows that at least one of
the two will go free. The jailer refuses to answer
the question, pointing out that if A knew which
of his fellow prisoners were to be set free, then
his own probability of being executed would rise
from $\frac 13$ to $\frac 12$ because he would then be one of two
prisoners. What do you think of the jailerâ€™s
reasoning? If the jailer refuses to say anything, then the probability that prisoner $A$ is excecuted is $\frac{1}{3}$ . If the jailer says to prisoner $A$ that prisoner $B$ will walk free, then $2$ prisoners remain to be considered, $A$ and $C$ . One dies, one does not. Heads or tails essentially. $\frac{1}{2}$ ought to be the conditional probability that $A$ dies given that $B$ walks free no? Apparently not though, allegedly the correct answer is still $\frac{1}{3}$ . Even my attempt to calculate the correct answer yielded the result $\frac{1}{2}$ . Let $A_D$ and $C_D$ respectively denote the event of $A$ and $C$ dying. Let $B_F$ denote the event that $B$ walks free. Assume that the jailer tells prisoner $A$ that prisoner $B$ will walk free. Here's my attempt. $$P(A_D\mid B_F)=\frac{P(A_D\cap B_F)}{P(B_F)}=\frac{P(A_D\cap B_F)}{P((B_F\cap A_D)\cup (B_F\cap C_D))}=\frac{P(A_D)P(B_F\mid A_D)}{P(A_D)P(B_F\mid A_D)+P(C_D)P(B_F\mid C_D)}=\frac{\frac{1}{3}\times 1}{\frac{1}{3}\times 1+\frac{1}{3}\times 1}=\frac{1}{2}$$ What am I doing wrong? Edit 1: Intuitively I am still troubled but I understand now that $B_F$ may occur even though the jailer does not necessarally say $B$ . Edit 2: I suppose that it makes some sense if the faiths of the prisoners had already been decided before prisoner $A$ asked the jailer the question. If the jailer decides to reveal one of the others who will walk free then that must've been the case.","['probability', 'paradoxes']"
2042207,How to understand the projective compactification of a vector bundle?,"The following question confused me a bit: Given a rank $n$ vector bundle or locally free sheaf $\mathcal{E}$ on $X$, each fiber of this vector bundle is a vector space of dimension $n$. Therefore, we can compactify it into a projective space $\mathbb{P}^n$. And if we do this compatibly for every fiber we get a $\mathbb{P}^n$-projective bundle. Now my question is whether this projective bundle is $\mathbb{P}(\mathcal{E}\oplus\mathcal{O}_X)$ or $\mathbb{P}(\mathcal{E}^{\vee}\oplus\mathcal{O}_X)$. Here by $\mathbb{P}(\mathcal{F})$ for some locally free sheaf $\mathcal{F}$ I mean the projective bundle of hyperplanes (instead of the projective bundle of lines). At first I thought the answer should be $\mathbb{P}(\mathcal{E}\oplus\mathcal{O}_X)$. Then something seems to be wrong. If I consider the zero section of the vector bundle $
\mathcal{E}$, it can also be viewed as a section of the projective bundle $\mathbb{P}(\mathcal{E}\oplus\mathcal{O}_X)$. Then it should corresponds to the surjection $\mathcal{E}\oplus\mathcal{O}_X \rightarrow \mathcal{O}_X\rightarrow 0$ by sending $\mathcal{E}$ to $0$ because what else it can be. However if I consider a concrete example, namely the blowing up of $\mathbb{P}^2$ at a point, this is not the case. The blow-up can be viewed as a $\mathbb{P}^1$-projective bundle (i.e. ruled surface) over $\mathbb{P}^1$. Let $X=\mathbb{P}^1$. Then the resulting space of the blow-up is $\mathbb{P}(\mathcal{O}_X(1)\oplus\mathcal{O}_X)$. Therefore, we can also see it as the projective compactification of the line bundle $\mathcal{O}_X(1)$. But then the surjection
$$
\mathcal{O}_X(1)\oplus\mathcal{O}_X\rightarrow \mathcal{O}_X\rightarrow 0
$$
actually determines the section at infinity, which is the exceptional divisor of the blow-up. Instead, for any section $s\in H^0(X,\mathcal{O}_X(1))$, there is a natural map
$$
\mathcal{O}_X\rightarrow \mathcal{O}_X(1)
$$
given by multiplication of $s$. Therefore, when considered as a section of $\mathbb{P}(\mathcal{O}_X(1)\oplus\mathcal{O}_X)$, $s$ is determined by the induced surjection
$$
\mathcal{O}_X(1)\oplus\mathcal{O}_X\rightarrow \mathcal{O}_X(1)\rightarrow 0.
$$ This example makes me believe that the projective compactification of $\mathcal{E}$ is actually $\mathbb{P}(\mathcal{E}^{\vee}\oplus\mathcal{O}_X)$. Now everything seems to be consistent. Let $s\in H^0(X,\mathcal{E})$. Then we have a map
$$
\mathcal{O}_X\rightarrow\mathcal{E} 
$$
given by multiplication of $s$. Take the dual we have a map
$$
\mathcal{E}^{\vee}\rightarrow \mathcal{O}_X.
$$
And the induced surjection
$$
\mathcal{E}^{\vee}\oplus\mathcal{O}_X\rightarrow \mathcal{O}_X \rightarrow 0
$$
defines the section $s$. It seems a little bit weird that the projective compactification of $\mathcal{E}$ might be $\mathbb{P}(\mathcal{E}^{\vee}\oplus\mathcal{O}_X)$ instead of $\mathbb{P}(\mathcal{E}\oplus\mathcal{O}_X)$. Is there any explanation for this (if I am not completely talking about nonsense)? My guess is that it should have something to do with the fact I am considering projective bundle of hyperplanes. According to my previous experience, if some results involving projective bundle differs by a dual, it is caused by the two different conventions whether it is projective bundle of lines or hyperplanes.","['vector-bundles', 'algebraic-geometry']"
2042233,"Is $\sin(nx)$ equicontinuous on $[0,1]$?","Consider $f_n=\sin(nx),\,x\in[0,1]$. In order to show that this is not an equicontinuous family, take $x=0,y=\frac{1}{N}$ where $N \in \mathbb{N}$ can be arbitrarily large, so $\forall \, \delta>0 \rightarrow y<\delta$. Now, if we consider $|f_N(x)-f_N(y)|=|\sin(N\frac{1}{N})|=|\sin(1)|$, hence there exists $\varepsilon_0 > 0$ such that $\forall \, \delta>0,\, \exists\, x=0,\, y=\frac{1}{N},\, N \in \mathbb{N} \,  \text{ such that } |x-y|=y<\delta, \, \text{ but } |f_N(x)-f_N(y)| \geq \varepsilon_0$ Is there anything not right with that logic?","['real-analysis', 'equicontinuity', 'sequences-and-series']"
2042278,Surjective regular morphism from affine space to punctured plane,"Does there exist $d$ and a regular (=polynomial) map from the affine space $\mathbb{A}^d$ to $\mathbb{A}^2$ whose image is exactly the punctured plane $\mathbb{A}^ 2\smallsetminus\{0\}$? Here the base field is algebraically closed, and of characteristic zero if necessary. Note that there exist regular maps from the affine space onto the projective line, and more precisely a regular map $\mathbb{A}^1\to\mathbb{A}^2\smallsetminus\{0\}$ (namely $z\mapsto (z,z^2+1)$) whose composite with the quotient map $\mathbb{A}^2\smallsetminus\{0\}\to\mathbb{P}^1$ is surjective, see the MathSE question ""Does there exist a regular map $\mathbb{A}^1\to\mathbb{P}^1$ which is surjective?"" If there's a terminology for those varieties admitting a surjective regular map from some affine space, it would help (such varieties are connected, unirational, and all their non-constant regular maps (to $\mathbb{A}^1$) are surjective, excluding, for instance, $\mathbb{A}^1\smallsetminus F$ for $F$ finite nonempty). Edit Oops, $(a,b,c)\mapsto (a(1+bc)+c,1+bc)$ works (indeed it does not vanish, $(0,-x^{-1},x)\mapsto (x,0)$ for $x\neq 0$ and $(\frac{x+1}{y}-1,1,y-1)\mapsto (x,y)$ for $y\neq 0$. So the question remains only for $d=2$.",['algebraic-geometry']
2042281,can continuously differentiability deduced from continuous partial differentiable?,"If not, what is the sufficient and necessary condition of continuously differentiability? I have searched a lot, but I can not find a detailed reference discussing continuously differentiability.","['multivariable-calculus', 'reference-request', 'real-analysis']"
2042293,Zero subscheme of a section: Making computations.,"In the Algebraic Geometry course I am following we have defined vector bundles in the following way: Given a locally free sheaf $\mathscr{E}$ on a scheme $S$ we can define a functor, $$\mathbb{V}(\mathscr{E}): \operatorname{Sch}/S \to \operatorname{Set}$$ $$(Y \xrightarrow{f} S) \mapsto f^{*}\mathscr{E}(Y)$$
The vector bundle over $S$ associated to $\mathscr{E}$ will be the representing object of this functor $\vert \mathscr{E} \vert \xrightarrow{\pi} S$. I had to show that for every $s \in \mathscr{E}(S)$ we can find a closed subscheme of $S$, $V(s)$ such that a morphism $Y \xrightarrow{f} S$ factorizes through $V(s)$ if and only if $f^{*}(s)=0$. To do this I tried to define the ""vanishing set"" of the section using the set of points where the ""coordinates"" of $s$ lie in the maximal ideal but I had problems to finish the proof. However we can give a more geometrical interpretation: $\mathbb{V}(\mathscr{E})(X \xrightarrow{id} X)=\mathscr{E}(X)$ therefore we can identify global sections of the sheaf with global sections of the bundle. Now we note that $f^{*}\mathscr{E}$ is also locally free therefore it defines a bundle $\vert f^{*}\mathscr{E} \vert \xrightarrow{\pi'} Y$ and this bundle is the pullback of $\pi$ and $f$. The composition $s \circ f$ yields a section as pictured below Now the condition $f^{*}(s)=0$ translates to the fact that the associated section we have constructed equals to the zero section of $\vert f^{*} \mathscr{E}\vert$. Let us denote the zero sections of both spaces by $0_S,0_Y$. If we define $V(s)$ as the pullback of $s,0_S$ we obtain the desired subspace and the factorization property is encoded in the universal property of the pullback. Furthermore this subscheme is unique. This seems very nice theoretically but not very useful in practice. For the second part of the question I have to show that the Segre embedding of $\mathbb{P}^1_R \times_R \mathbb{P}^1_R \to \mathbb{P}^3_R$ arises as a vanishing as some $V(s)$.I know that it should be the vanishing locus of some homogeneous polynomial of degree 2 so I should be looking at $\mathcal{O}_{\mathbb{P}^3_R}(2)$ as a vector bundle. However with the tools that I have I don't see how I could possible translate the previous disgression into some actual computations because I don't know how to turn a global section of the sheaf into a geometrical section. I know that morphisms into $ X \to \mathbb{P}^3_R$ are given by surjections $\mathcal{O}_X^4 \to \mathscr{L}$ with $\mathscr{L}$ an invertible sheaf. Can we link this description of the morphisms with the sections?",['algebraic-geometry']
2042362,Union of $\sigma$-algebras is a $\pi$-system...?,"I thought that the  the union of $\sigma$-algebras is a $\pi$-system (which I believe is what is said at the beginning of this this wikipedia article ). When I went to prove it myself, I had some trouble. I keep thinking of the following example: let $\Omega$ be the sample space, and let $A \neq B$ be two subsets of $\Omega$ such that $A\cap B\neq \emptyset$. Now
\begin{align*}
\mathcal{F}_{1} &= \{ \emptyset, A, A^{c}, \Omega\}\\
\mathcal{F}_{2} &= \{ \emptyset, B, B^{c}, \Omega\}\\
\\
\implies \mathcal{F}_{1} \cup \mathcal{F}_{2} &=  \{ \emptyset,A,A^{c}, B, B^{c}, \Omega\}\\
\end{align*}
where clearly $\mathcal{F}_{1}$ and $\mathcal{F}_{2}$ are $\sigma$-algebras, but $\mathcal{F}_{1}\cup\mathcal{F}_{2}$ is not a $\pi$-system. Can someone end my misery and either (i) point out the problem with my counterexample and provide a proof that the union of $\sigma$-algebras is a $\pi$-system, or (ii) clarify what the intro of this wikipedia article is saying and elaborate on the conditions under which the union of $\sigma$-algebras is a $\pi$-system?","['real-analysis', 'measure-theory']"
2042363,Compatible germs and the espace Ã©talÃ©,"The following is a confusion I'm having that I cannot find answers to anywhere. If this question has already been asked, I apologise, but I couldn't find any answers after some pretty extensive searching. I know this is four questions, but I think really it's just one (i.e. how do I understand these two concepts in light of each other). After starting to read Vakil's The Rising Sea (which is fantastic, by the way), I have one big confusion. There is the concept of compatible germs and also the concept of the Ã©talÃ© space . They seem very linked, but I can't quite pin down how. Edit: question. In the comments and answers there has been plenty of help with the first and last question, so it's really just the two questions in bold that I'm left with now :) Here's what I've come up with after thinking about this some more, as a more concrete version of the remaining questions (hopefully): we know that taking sections of $p\colon\sqcup_{x\in X}\mathcal{F}_x\to X$ gives us the sheafification of $\mathcal{F}$, as does taking compatible germs. So is there an association between compatible germs and sections $\sigma$ of $p$, e.g. a bijection between the two? Let $\mathcal{F}$ be a sheaf (of sets) on a topological space $X$, and $U$ an open set of $X$.
Here are some facts/definitions (largely from Vakil's The Rising Sea ): The natural map $\varphi\colon\mathcal{F}(U)\to\prod_{x\in U}\mathcal{F}_x$ is injective. An element $(s_x)_{x\in U}\in\prod_{x\in U}\mathcal{F}_x$ is a collection of compatible germs if any of the following equivalent properties hold: for all $x\in U$ there exists a neighbourhood $U_x\subset U$ and a section $f\in\mathcal{F}(U_x)$ such that for all $y\in U$ we have $s_y=f_y$ (where $f_y$ is the germ of $f$ at $y$); $(s_x)_{x\in U}$ is the image of a section $f$ under the map $\varphi$ (i.e. the above condition holds but with $U_x=U$ for all $x$). The espace Ã©talÃ© $\Lambda(\mathcal{F})$ associated to $\mathcal{F}$ (or more generally any pre sheaf) is constructed as follows: as a set, $\Lambda(\mathcal{F})=\coprod_{x\in X}\mathcal{F}_x$; as a topological space, the basis for the open sets of $\Lambda(\mathcal{F})$ is given by the $\{V_{U,\,f}\mid U\in\mathsf{Op}(X), f\in\mathcal{F}(U)\}$ where $V_{U,\,f}=(f_x)_{x\in U}$; as an Ã©talÃ© space, the local homeomorphism is given by projection, i.e. $p\colon\Lambda(\mathcal{F})\to X$ acts as $f_x\mapsto x$. The sheaf $\Gamma(p\colon E\to X)$ associated to a continuous map $p\colon E\to X$ acts on open sets as follows: $\Gamma(p\colon E\to X)(U)=\{\sigma\colon U\to E \mid p\circ\sigma=\mathrm{id}_U\}$. Sheafification, which can be constructed by taking only compatible germs, is just $\Gamma\Lambda$. (The last fact is emphasised because it seems to me like it should be the thing that ties everything together.) Questions: Is all of the above correct? How can we think of compatible germs in terms of the Ã©talÃ© space of a (pre)sheaf? I am almost certain that I have read somewhere it is equivalent to the continuity of the sections $\sigma$ or something similar, but I can't find this anywhere. It seems like a collection of germs is compatible if and only if it is open in $\Lambda(\mathcal{F})$, but this doesn't sound right to me (or at least not the whole picture), especially when you ask... ... why does the germ map $\varphi$ use the product of sets while the Ã©talÃ© space uses the coproduct? Does this mean we can't link the two concepts? Is there a less confusing notation for elements of $\prod_{x\in U}\mathcal{F}(U)$? Writing $(s_x)_{x\in U}$ always looks to me like we take one section $s$ and look at all of its germs (i.e. compatibility!), but writing something like $(s_x^{(x)})_{x\in U}$ (trying to emphasise that the section that we take the germ of varies with the point we're taking the germ at) seems quite cumbersome (and also something I've never seen!).","['sheaf-theory', 'algebraic-geometry']"
2042368,Evaluate the following integral: $\int_\limits{0}^{\pi /4}\dfrac{\tan(x)^2 dx}{1+x^2}$,I tried to find and antiderivative but I think it's imposible so I would like to know if there is another way to find its value.,"['integration', 'definite-integrals']"
2042414,Differences between two formulations of the Lehmann-Scheffe Theorem,"I have seen different formulations of the Lehmann Scheffe Theorem but am not sure if they are the same. For example, on wikipedia it states: If $T$ is a complete sufficient statistic for $\theta$ and $E(g(T)) = \tau(\theta)$, then $g(T)$ is the uniformly minimum-variance unbiased estimator (UMVUE) of $\tau(\theta)$. But here it states that: If $T(X)$ is a complete
sufficient statistic and $W(X)$ is an unbiased
estimator of $Ï„(Î¸)$, then $Ï†(T) = E(W|T)$ is
an UMVUE of $Ï„(Î¸)$. Furthermore, $Ï†(T)$ is the
unique UMVUE in the sense that if $T^*$ is any
other UMVUE, then $P\theta(Ï†(T) = T^*) = 1$ for all
$Î¸$. At a glance, it seems that the first wikipedia statement is needed for the second, and that the second uses the Rao-Blackwell Theorem. Are these two statements one and the same?",['statistics']
2042583,"Exercise 4.9, Chapter I, in Hartshorne","Let $X$ be a projective variety of dimension $r$ in $\mathbf{P}^n$ with $n\geq r+2$ . Show that for suitable choice of $P\notin X$ , and a linear $\mathbf{P}^{n-1}\subseteq \mathbf{P}^n$ , the projection from $P$ to $\mathbf{P}^{n-1}$ induces a birational morphism of $X$ onto its image $X'\subseteq \mathbf{P}^{n-1}$ . My way: W.L.O.G., assume that $X\setminus U_0\neq\emptyset$ . Since $X$ is a projective variety, then $K(X)\cong S(X)_{(0)}$ , which implies that $K(X)=k(x_1/x_0,\dots,x_n/x_0)$ . Since $\dim X=r$ , by Theorem 4.8A and Theorem 4.7A on page 27 in Hartshorne, then W.L.O.G., we can assume that $x_1/x_0,\dots,x_r/x_0$ is a separating transcendence base for $K(X)$ over $k$ , which implies that $x_{r+1}/x_0,\dots,x_{n}/x_0$ are separable over $k(x_1/x_0,\dots,x_r/x_0)$ . By Theorem 4.6A on page 27 in Hartshorne, $K(X)=k(x_1/x_0,\dots,x_r/x_0)[y]$ , where $y$ is a $k(x_1/x_0,\dots,x_r/x_0)$ -linear combination of $x_{r+1}/x_0,\dots,x_n/x_0$ . Now I do not how to continue.",['algebraic-geometry']
2042676,Definition of ' blow up time' in the context of PDE,"$$ \partial_t u - x \partial_x u = -u^2$$ $$ u(x,0) = u_0 (x)$$ (i) Find the blow up time $t_*$ for the cauchy data $u_0(x) = cosx $ and determine the position $x(t_*)$ which the solution develops poles. Found a solution for this PDE using method of characteristics, but never heard the term ' blow up time ' ever for the context of PDE. Seems like it has something to do with solution going infinity but not sure. Any help how to solve this problem??","['ordinary-differential-equations', 'partial-differential-equations']"
2042678,"How would you go about writing $1/3=\sum\limits_{n=-k}^{\infty}a_n 2^{n}$ where $a_n=0,1$","I want to show that every $p$-adic number can be written as $\sum\limits_{n=-k}^{\infty}a_n p^{n}$ where $a_n=0,1,...,p-1$. But I got stuck with the example I came up with. By write I am asking how to figure out the coefficients $a_n$.","['number-theory', 'p-adic-number-theory']"
2042698,The Three-Cornered Duel,"I am analyzing the following problem from the the book ""Fifty Challenging Problems in Probability with Solution"" by Frederick Mosteller. It seems to me that the solution to the The Three-Cornered Duel Problem presented in the book is incomplete. Can anybody confirm if my calculation is ok? The problem A, B and C are to fight a three-cornered pistol duel. All know that
  A's chance of hitting his target is 0.3, C's is 0.5, and B never
  misses. They are to fire at their choice of target in succession in
  the order A, B, C, cyclically (but a hit man loses further turns and
  is no longer shot at) until only one man is left unit. What should A's
  strategy be? Mosteller's solution: A is naturally not feeling cheery about this enterprise. Having the
  first shot he sees that, if he hits C, B will then surely hit him, and
  so he is not going to shoot at C. If he shoots at B and misses him,
  then B clearly shoots the more dangerous C first, and A gets one shot
  at B with probability 0.3 of succeeding. If he misses this time, the
  less said the better. On the other hand, suppose A hits B.  Then C and
  A shoot alternately until one hits. A's chance of winning is
  $$(.5)(.3)+(.5)^2(.7)(.3)+(.5)^3(.7)^2(.3)+â€¦$$
  Each term corresponds to a sequence of misses by both C and A ending with a final hit by A. Summing the geometric series we get
  $$(.5)(.3)+\{1+(.5)(.7)+[(.5)(.7)]^2+â€¦ \}= \frac{(.5)(.3)}{1-(.5)(.7)}= \frac{0.15}{0.65}= \frac{3}{13} < \frac{3}{10}$$ 
  Thus hitting B and finishing off with C has less
  probability of winning for A than just missing the first shot. So A
  fires his first shot into the ground and then tries to hit B with his
  next shot. C is out of luck. My calculation case 1
$$P(A survives \ in \ case \ A \ shots \  B )= \mathbf{0.3} [(.5)(.3)+(.5)^2(.7)(.3)+(.5)^3(.7)^2(.3)+â€¦] = \mathbf{0.3} \frac{(.5)(.3)}{1-(.5)(.7)}= \frac{\mathbf{0.3}(0.15)}{0.65}=0.069$$
case 2
$$P(A \ survives \ in \ case \ A \ misses \ the \ shot \ at \  B )= (0.7) \ 1 \ (0.3) = 0.21$$ Probability of survival is higher in case 2 therefore A should miss the first shot at B. Is my calculation correct?","['game-theory', 'probability']"
