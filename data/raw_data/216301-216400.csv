question_id,title,body,tags
4397262,Geometric/vector explanation of $\det(A)=0\iff$ unique solution doesn't exist to system of linear equation,"Currently, I am self-studying Multivariable Calculus. I have prior knowledge about Vectors, Matrices and System of Linear Equations. However, the linkages between the three are not explicitly covered by my prior education and curriculum. I reckon it is beneficial for me to build a correct understanding about these linkages. I am not most familiar with the language of Mathematics. I apologize for any inaccuracies or intracies throughout my question in advance. TL;DR: Explain the following geometrically / from ""vector view"": $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\impliedby&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align}$$ Suppose there is a system of linear equations in the forms $$x\overrightarrow{a}+y\overrightarrow{b}+z\overrightarrow{c}=\overrightarrow{d}$$ and $$\mathbf{A}\overrightarrow{x}=\overrightarrow{d}$$ From my understanding, solving this system of linear equations is conceptually equivalent to: in a ""vector view"", expressing $\overrightarrow{d}$ in terms of linear combination(s) of $\overrightarrow{a}$ , $\overrightarrow{b}$ and $\overrightarrow{c}$ ; in a ""matrix view"", solving $\overrightarrow{x}=\mathbf{A}\overrightarrow{d}$ . My goal is to understand "" $\det(\mathbf{A}^{-1})=0\iff\text{unique solution doesn't exist}$ "" from the ""vector view"". I believe there is a logical explanation to my question: $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\iff&\det\mathbf{A}=0\\\iff&\text{unique solution doesn't exist}\\\iff&\text{no solution or infinitely many solutions}\\\iff&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\\&\text{ or there exist infinitely many linear combinations of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ expressing }\overrightarrow{d}\end{align}$$ To put it simply, the logical explanation is $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\iff&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\\&\text{ or there exist infinitely many linear combinations of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ expressing }\overrightarrow{d}\end{align}$$ From the definition of basis vectors, I know that $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly independent}\\\iff&\overrightarrow{d}\text{ can be expressed in }\mathbf{unique }\text{ linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align}$$ and I understand that the negation of the above statement would give the previous statement. **My understanding is stuck in the fact that ** $$\begin{align}&\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\text{ are linearly dependent}\\\impliedby&\overrightarrow{d}\text{ cannot be expressed in any linear combination of }\overrightarrow{a}\text{, }\overrightarrow{b}\text{ and }\overrightarrow{c}\end{align}$$ $\det{A}=0\impliedby\text{no solution}$ is the reason why the above statement is true. I lack the intuition that it is true. Could anyone fill me in with the ""vector view"" of such statement? Thank you.","['systems-of-equations', 'vectors', 'matrices', 'multivariable-calculus', 'linear-algebra']"
4397276,Solve $\sin(x)\cos(x)=\sin(x)+\cos(x)$,"My initial idea was $$(\sin(x)\cos(x))^2=1+2\sin(x)\cos(x)$$ Let $t=\sin(x)\cos(x)$ ; $$t^2=1+2t \quad\Leftrightarrow\quad t=1-\sqrt2$$ (since $1+\sqrt2>1$ ).
I.e. $$\sin(x)\cos(x)=1-\sqrt2 \quad\Leftrightarrow\quad \tfrac12\sin(2x)=1-\sqrt2 \quad\Leftrightarrow\quad x=\tfrac{1}{2}\arcsin(2(1-\sqrt2))$$ but I didn't get any ‘elegant’ final solution. Any better ideas?",['trigonometry']
4397313,Is $ SU_2 \otimes SU_2 $ conjugate to $ SO_4(\mathbb{R}) $ in $ SU_4 $?,"Let $ A,B $ be matrix groups (with entries in the same field). Then the tensor/Kronecker product $ A \otimes B $ is a matrix group and $$
\pi: A \times B \to A \otimes B 
$$ is a group homomorphism. Taking $ A=B=SU_2 $ we have a map $ \pi: SU_2 \times SU_2 \to SU_4 $ given by $$
(A,B) \mapsto A \otimes B 
$$ The only nontrivial element of the kernel is $ (-1,-1) $ . So the image $ SU_2 \otimes SU_2 $ of $ \pi $ is a subgroup of $ SU_4 $ isomorphic to $$
 SU_2 \times SU_2/ (-1,-1) \cong SO_4(\mathbb{R}) 
$$ Is $ SU_2 \otimes SU_2 $ conjugate to $ SO_4(\mathbb{R}) $ in $ SU_4 $ ? If so what is a matrix conjugating one to the other? (this part was unanswered for a while but is now answered in the update) Since matrices in $ SU_2 $ have real trace then all matrices in $ SU_2 \otimes SU_2 $ have real trace. So it is at least plausible that $ SU_2 \otimes SU_2 $ and $ SO_4(\mathbb{R}) $ are conjugate. Also what is the normalizer in $ SU_4 $ of $ SU_2 \otimes SU_2 $ / the normalizer in $ SU_4 $ of $ SO_4(\mathbb{R}) $ ? Certainly $ iI $ normalizes $ SO_4(\mathbb{R}) $ and $$
T=
\zeta_8
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0& 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
$$ normalizes $ SU_2 \otimes SU_2 $ . EDIT: Just to reiterate what Jason DeVito both $ SO_4(\mathbb{R}) $ and $ SU_2 \otimes SU_2 $ are normalized by $$
T=
\zeta_8
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0& 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
$$ where $$
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0& 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}=SWAP \in O_4(\mathbb{R})
$$ is the $ SWAP $ operator and $ \zeta_8 $ just normalizes the determinant. Both $ SO_4(\mathbb{R}) $ and $ SU_2 \otimes SU_2 $ contain $ -I $ and both do not contain $ iI $ . Since \begin{align*}
(\zeta_8 SWAP)^2&=iI\\
 (\zeta_8 SWAP)^4&=-I
\end{align*} we can combine this with the results from https://arxiv.org/pdf/math/0605784.pdf to conclude that both $ SO_4(\mathbb{R}) $ and $ SU_2 \otimes SU_2 $ have cyclic $ 4 $ component group generated by $ \zeta_8 SWAP $ . UPDATE: A specific unitary matrix conjugating $ SU_2 \otimes SU_2 $ to $ SO_4(\mathbb{R}) $ is $$
Q=\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 0 & 0 & i \\
0 & i & 1 & 0 \\
0 & i & -1 & 0 \\
1 & 0 & 0 & -i
\end{bmatrix}
$$ in other words there is an isomorphism $ SU_2 \otimes SU_2 \to SO_4(\mathbb{R}) $ given by $$
M \mapsto Q^{-1} M Q
$$ This is Theorem 1 from https://arxiv.org/pdf/quant-ph/0002045.pdf the matrix $ Q $ is the change of basis from standard basis to the Bell basis well known in quantum computing. This theorem is an interesting and short read for anyone interested in quantum information or just Lie groups. The argument proceeds by concocting a definite form that is preserved by local unitaries $ SU_2 \otimes SU_2 $ , instantiating the claims in the answers below that such a form must exist.","['unitary-matrices', 'exceptional-isomorphisms', 'matrices', 'tensor-products', 'lie-groups']"
4397317,Integrating density over measurable sets yields probability?,"I have a bit of trouble proving a statement expressing the expectation of a function of random variables as the conditional expectation, namely: $$\mathbb{E}g(X,Y) = \int_{\mathbb{R}^n} \int_{\mathbb{R}^m} g(x,y) f(y|x) \, dy \cdot f_X(x) \, dx = \mathbb{E}(\mathbb{E}[g(X,Y)|X])$$ with $X,Y$ appropriately dimensioned real-valued random variables, $f(y|x) := \frac{f_{X,Y}(x,y)}{f_X(x)}$ being the conditional density of $f_{X,Y}$ with respect to $(X,Y)$ and $f_{X}$ to $X$ . There is a statement leading up to this claim in my notes: $$\forall A \in \mathcal{B}^n, B \in \mathcal{B}^m: \mathbb{P}(X \in A, Y \in B) = \int_{A}\int_{B} f(y|x) \, dy f_{X}(x) \, dx$$ I have already seen and proven a similar statement for a single random variable, but how does it work out in the multivariate case? Can it also be done using the fact that $\mathcal{B}^i$ is generated by half open box sets? I played around with the idea and using the integral representation of the densities cdf's, but the fact that there are multiple variables to be considered leaves me stumped. As a bonus: How do any of the equalities in the first paragraph make any sense?? They are stated as is in my notes without any further exposition. I know all of the listed quantities and a couple of identities for the conditional expectation. Thank you for reading!","['conditional-expectation', 'statistics', 'probability-theory', 'density-function']"
4397320,Bourbaki... caveman(?) symbol,"I don't know if this is the best place to ask this question, but I'm sure many of you are familiar with the Bourbaki ""dangerous bend"" symbol: The idea behind the symbol is to indicate a particularly conceptually challenging part of the text, which could potentially be skipped when reading through for the first time. Now, my question is: is there an equivalent ""caveman"" symbol? I came across this in a set of notes on analysis: The symbol appears next to all definitions, I suppose the idea is that when a concept is defined (especially in analysis) we are bound to discover something new (akin to a cave man). What is this symbol called? Is it commonly used, similarly to Bourbaki? Is there a LaTeX package for it?","['soft-question', 'book-recommendation', 'real-analysis']"
4397322,Using diagonalisation of a symmetric on quadratic form,"Let $f (x, y) = 7x_2 + 4y_2 + 4xy$ . Use the diagonalisation of a symmetric matrix to write this quadratic form in the form $λ_1u_2^2 +λ_2v_2^2$ , with u and v linear combinations of x and y. Here's what I have tried: step 1. get the symmetric matrix $$f (x, y) = 7x_2 + 4y_2 + 4xy=\left(\begin{matrix}7&2\\2&4\\\end{matrix}\right)$$ step 2. find the eigenvalues $$det(A-\lambda I) \\
\left(\begin{matrix}7-\lambda&2\\2&4-\lambda\\\end{matrix}\right)=(7-\lambda)(4-\lambda)-4=(\lambda-3)(\lambda-8)$$ step 3. find the eigenvectors case for $\lambda =3$ $$\left(\begin{matrix}7-3&2\\2&4-3\\\end{matrix}\right)=\left(\begin{matrix}4&2\\2&1\\\end{matrix}\right) 
$$ Row reduced $$\left(\begin{matrix}4&2\\2&1\\\end{matrix}\right)=\left(\begin{matrix}1&\frac{1}{2}\\0&0\\\end{matrix}\right) \implies \left(\begin{matrix}x\\y\\\end{matrix}\right)=y_1\left(\begin{matrix}-\frac{1}{2}\\1\\\end{matrix}\right)$$ Doing the same for $\lambda =8$ (i'll skip the calculation), I get $\left(\begin{matrix}x\\y\\\end{matrix}\right)=y_2\left(\begin{matrix}2\\1\\\end{matrix}\right)$ Finding the orthogonal eigenvectors $$u_1 = \frac{y_1}{||y_1||} = \frac{2}{\sqrt{5}}\left(\begin{matrix}-\frac{1}{2}\\1\\\end{matrix}\right) \\
u_2 = \frac{y_1}{||y_1||} = \frac{1}{\sqrt{5}}\left(\begin{matrix}2\\1\\\end{matrix}\right) \\
P=(u_1,u_2)=\left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right) \\
D = P^TAP = \left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right)\left(\begin{matrix}7&2\\2&4\\\end{matrix}\right)\left(\begin{matrix}-\frac{1}{\sqrt{5}}&\frac{2}{\sqrt{5}}\\\frac{2}{\sqrt{5}}&\frac{1}{\sqrt{5}}\\\end{matrix}\right) = \left(\begin{matrix}3&0\\0&8\\\end{matrix}\right) \\
x^TAx = \lambda^TD\lambda =  \left(\begin{matrix}\lambda_1&\lambda_2\\\end{matrix}\right)\left(\begin{matrix}3&0\\0&8\\\end{matrix}\right)\left(\begin{matrix}\lambda_1\\\lambda_2\\\end{matrix}\right) = 3\lambda_1^2+8\lambda_2^2$$ However, my result does not look like it's in the same form as the question: $λ_1u_2^2 +λ_2v_2^2 \ne  3\lambda_1^2+8\lambda_2^2$ , how do I proceed from here?","['matrices', 'diagonalization', 'linear-algebra', 'quadratic-forms']"
4397355,Stein Shakarchi Real Analysis Exercise 3.5.2,"I don't follow a couple parts of the proof of this problem. Problem 3.5.2 Suppose $\{K_\delta\}$ is a family of kernels that satisfies for all $\delta > 0$ : $|K_\delta(x)| \le A\delta^{-d}$ $|K_\delta(x)| \le A\delta/|x|^{d+1}$ $\int_{\mathbb{R}^d} K_\delta(x)dx = 0$ Show that if $f$ is lebesgue-integrable on $\Bbb R^d$ , then $$
(f*K_\delta)(x) \rightarrow 0
$$ for a.e. $x$ , as $\delta \rightarrow 0$ . Proof We have $$
(f*K_\delta(x) = \int f(x-y)K_\delta(y)dy
$$ For convenience, let $$g(x,y,\delta) := |f(x-y)||K_\delta(y)|$$ Using the monotonicity of the integral, it suffices to show that $$
\int f(x-y)K_\delta(y)dy \le \int g(x,y,\delta)dy \rightarrow 0
$$ as $\delta \rightarrow 0$ We break up this integral as follows $$
\tag{1}
\int g(x,y,\delta)dy = \int_{|y| \le \delta}g(x,y,\delta)dy + \sum_{k=0}^\infty \int_{2^k \delta < |y| \le 2^{k+1} \delta} g(x,y,\delta)dy
$$ We use the property of good kernels to observe that $ 
\tag{2} \int_{|y| \le \delta}|K_\delta(y)|dy \rightarrow 0
$ as $\delta \rightarrow 0$ . Issue 1: For the second term, choose each term in the sum to be less than $\epsilon/2^k$ for any $\epsilon > 0$ . Issue 2: For the first term, use $\int K_\delta = 0$ and the two estimates of good kernels to see that $$
\int_{|y| \le \delta} g(x,y,\delta) \le CA\delta / \epsilon \int |f(x-y)|
$$ Two issues Issue 1 For issue 1, I see that we can make the expression in (2) arbitrarily small for sufficiently small $\delta$ , but it's not clear to me that this is true when you multiply the integrand by $|f(x-y)|$ . For each term in the sum, since $f$ is integrable, I could say that $$
\int |f| = M < \infty
$$ But it does not follow that $f$ is even bounded a.e., so I can't say $
\int g(x,y,\delta) \le M \int |K_\delta(y)|dy
$ So I'm not sure how to bound the terms in the sum of (2). Issue 2 I don't follow this result, in particular how $\epsilon$ ends up in the denominator. It would help to see some intermediate steps.","['measure-theory', 'lebesgue-integral']"
4397377,Extract Coefficients to Solve Recurrence Relation,"I'm having difficulty moving from the generating function, $h(x)$ , expressed in the form of partial fractions (which I believe I've successfully found) to the closed form of the recurrence relation, $e_n$ . The recurrence relation is: $e_0=e_1=1, e_2=2$ , and $e_n=3e_{n-1}-3e_{n-2}+e_{n-3}$ . I have calculated the generating function, $h(x)$ , as: $h(x)=\frac{1-2x+2x^2}{1-3x+3x^2-x^3}$ . Applying partial fractions, obtain: $h(x)=\frac{2}{1-x}+\frac{2}{(1-x)^2}+\frac{1}{(1-x)^3}$ . From Wolfram, I know that $e_n=\frac{1}{2}(n^2-n+2)$ , but I can't see how the coefficients of $x_n$ are extracted from the terms to arrive at this solution (with the exception of the first term of the partial fraction, which clearly becomes $2$ ).","['recurrence-relations', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
4397391,There is no natural isomorphism between torsion functor and identity,"Let $\mathcal{C}$ be the full subcategory of $\textbf{Ab}$ whose objects are finitely generated abelian groups. Let $F:\mathcal{C}\rightarrow\mathcal{C}$ be the functor sending $A\in\mathcal{C}$ to $A_{\text{tor}}\oplus A/A_{\text{tor}}$ . I want to show there is not natural isomorphism $F\rightarrow\text{id}_{\mathcal{C}}$ . For the sake of contradiction, suppose there exists a natural isomorphism $u:F\rightarrow\text{id}_\mathcal{C}$ . For each $A\in\textbf{Ab}$ , let $v(A):=\iota_{A,2}\circ\pi_A$ where $\pi_A:A\rightarrow A/A_{\text{tor}}$ is the canonical surjection and $\iota_{A,2}:A/A_{\text{tor}}\rightarrow A_{\text{tor}}\oplus A/A_{\text{tor}}$ is the canonical injection. Clearly $v:\text{id}_{\mathcal{C}}\rightarrow F$ is a natural transformation. Thus we have $u\circ v:\text{id}_{\mathcal{C}}\rightarrow\text{id}_{\mathcal{C}}$ and so there exists $n\in\mathbb{Z}$ such that $$u(A)\circ v(A)=n\cdot\text{id}_A$$ for all $A\in\mathcal{C}$ . In fact $n=u(\mathbb{Z})(v(\mathbb{Z})(1))=u(\mathbb{Z})(0,\pi_{\mathbb{Z}}(1))$ . To get a contradiction, it's sufficient to show that necessarily $n=\pm1$ . How to prove this? Edit: Note that $A_{\text{tor}}$ is the torsion subgroup of the abelian group $A$ .","['category-theory', 'finitely-generated', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4397407,"How to write proof for there is no injective function from $[0, 1]$ to $\mathbb{N}$?","I am given the following problem: Let $f$ be a function from $[0, 1]$ to $\mathbb{N}$ . Prove there exists $x, y\in [0, 1]$ such that $x\neq y$ and $f(x) = f(y)$ . Clearly, this is true because $|[0, 1]| = |\mathbb{R}| > |\mathbb{N}|$ , and there cannot be an injective function from an uncountable set to a countable set. But... how should I write my solution? They are asking for a proof and I do not know how to formally prove the statement above. Thank you!","['elementary-set-theory', 'cardinals', 'functions']"
4397417,"Difficult Coin Toss Probability Problem (Probability, Calculus)","I posted a question earlier, but I couldn't speak English, so the explanation was lacking. sorry. (so I'm asking a question again using a translator.) Anyway, I would like to ask a question about the problem of tossing a coin, which is classified as calculus. . . Problem : If a coin with ""probability of coming up heads"" p is tossed n 1 times, it comes up heads m 1 times.
Also, when a coin with ""probability of coming up heads"" q is tossed n 2 times, it comes up heads m 2 times.
In this case, find the probability that p < q. ex ) n 1 = 2, m 1 = 1, n 2 = 4, m 2 = 3 : Ans = 5/7 ex2 ) n 1 = 8, m 1 = 4, n 2 = 16, m 2 = 8 : Ans = 1/2 ex3 ) n 1 = 2, m 1 = 0, n 2 = 6, m 2 = 1 : Ans = 8/15 ex4 ) n 1 = 2, m 1 = 0, n 2 = 2, m 2 = 1 : Ans = 4/5 . . The equation I established The equation I established was as follows, but when I substituted n 1 = 2, m 1 = 0, n 2 = 2, m 2 = 1, I could see that the equation was wrong because 11/70 came out. (Since the answer needs to be 4/5...) (+ Now that I look at it, I wrote the relationship between p and q in reverse.) Please help me with how to set up the equation, or solve this problem. Thank you.","['calculus', 'probability']"
4397434,An absolutely convergent series of rational numbers which does not converge to a rational number [duplicate],"This question already has answers here : Absolute convergence to a rational number (6 answers) Closed 2 years ago . A standard theorem concerning series of real numbers states that every absolutely convergent series of real numbers converges. I would like to know a counterexample to this statement when we are dealing only with rational numbers. More precisely, I would like to know an example of a series $\sum_{n=0}^\infty q_n$ of rational numbers such that $\sum_{n=0}^\infty|q_n|$ converges to a rational number and that $\sum_{n=0}^\infty q_n$ converges to an irrational number. Furthermore, I want that the reason why the example works is understandable by someone who is only aware of basic statements concerning series. If it wasn't for the last requirement, I would know how to do it. One possibility would be to consider the power series $$\sum_{n=0}^\infty\binom{-1/2}nx^n,$$ which converges to $1/\sqrt{1+x}$ in $(-1,1)$ . In particular, $$\sum_{n=0}^\infty\binom{-1/2}n\left(\frac34\right)^n=\frac2{\sqrt7}\notin\Bbb Q.$$ But \begin{align}\sum_{n=0}^\infty\left|\binom{-1/2}n\left(\frac34\right)^n\right|&=\sum_{n=0}^\infty(-1)^n\binom{-1/2}n\left(\frac34\right)^n\\&=\sum_{n=0}^\infty\binom{-1/2}n\left(-\frac34\right)^n\\&=2\in\Bbb Q.\end{align} Another possibility consists in using a counting argument (although this only proves that a counter-example exists, rather than exhibiting one). The numbers of the form $$\sum_{n=0}^\infty\frac{\varepsilon_n}{3^n},$$ where $(\varepsilon_n)_{n\in\Bbb Z_+}$ is a sequence which takes only the values $1$ and $-1$ , form an uncountable set. So, for some sequences $(\varepsilon_n)_{n\in\Bbb Z_+}$ , the sum is irrational. But $$\sum_{n=0}^\infty\left|\frac{\varepsilon_n}{3^n}\right|=\frac32\in\Bbb Q.$$","['absolute-convergence', 'rational-numbers', 'examples-counterexamples', 'real-analysis']"
4397483,Understanding the proof that specific sequence does not have limit,"In mathematical analysis book, there is example of sequence $a_n$ : $1$ , $0$ , $\frac{1}{2}$ , $\frac{1}{2}$ , $\frac{1}{3}$ , $\frac{2}{3}$ , $\frac{1}{4}$ , $\frac{3}{4}$ , $...$ Odd numbers are part of sequence: $a_n = \frac{1}{n}$ , $\lim\limits_{n\to \infty} a_n = 0$ Even numbers are part of sequence: $a_n = 1 - \frac{1}{n}$ , $\lim\limits_{n\to \infty} a_n = 1$ Assuming since two subsequences have two different limits, whole sequence cannot have limit. This one is clear to me, but where I am lost is explanation or an attempt to find that $\epsilon_0\in\mathbb{R}$ exists: $$|a_\tilde{n} - a| \geq \epsilon_0$$ i.e. there exists least one $\epsilon_0\in\mathbb{R}$ disproving definition of limit: $$|a_n - a| < \epsilon$$ for $a,\epsilon\in\mathbb{R}$ , $a$ is limit of sequence $a_n$ , $\forall n > n_0(\epsilon)$ ; $n,n_0\in\mathbb{N}$ . To show that sequence does not have limit, there is first proposition that for $\epsilon_0$ , where for each $n\in\mathbb{N}$ there exist $\tilde{n} > n, \tilde{n}\in\mathbb{N}$ such as $|a_\tilde{n} - a| \geq \epsilon_0$ , we choose number for $\epsilon_0$ : $$\epsilon_0 = \frac{1}{2}\max(|a|, |a - 1|)$$ For 1st assumption $|a - 1| \geq |a|$ , because: $$a_{2k} = 1 - \frac{1}{k} \quad \textrm{and} \quad \frac{1}{k} \to 0$$ there exists $k_0(\epsilon_0)\in\mathbb{N}$ , where for $k > k_0(\epsilon_0)$ is always: $$|a_{2k} - 1| < \epsilon_0, k\in\mathbb{N}$$ and therefore: $$|a_{2k} - a| = |a_{2k} - 1 + (1 - a)| \geq |1 - a| - |a_{2k} - 1| \geq 2\epsilon_0 - \epsilon_0 = \epsilon_0$$ (let's call this inequality $A$ ) Therefore for any $n$ therefore exists $ñ$ as even number, which has to satisfy following conditions: $$ñ > n \quad \textrm{and} \quad ñ > 2k_0(\epsilon_0)$$ For example: $$ñ = 2\max(n, k_0(\epsilon_0))$$ Then: $$a_{2k}\in \langle1 - \epsilon_0, 1\rangle, k > k_0(\epsilon_0)$$ and therefore difference between $a_\tilde{n}$ and $a$ has to be at least $\epsilon_0$ . For 2nd assumption $|a - 1| \leq |a|$ would turn out by analogy, that $a$ has to be different to at least $|a|/2$ members of $a_{2k - 1}$ for $k$ big enough. My question is, or rather I am stuck on how we got to the inequality $A$ in the first place based on previous assumptions and from that rest of assumptions that seem to stem from it. I am sharing photos as well as link to pdf source for this example (in Czech language): https://www2.karlin.mff.cuni.cz/~halas/MA/MA1/kopacek_-_mat._analyza_nejen_pro_fyziky_1.pdf PS: Let me know if something there is not clear, I will try to make it more understandable if needed.","['limits', 'proof-explanation', 'analysis']"
4397485,Restricting Domain in Wolfram Alpha for Nested Quantifiers,"Is there a way to restrict the domain to all positive integers when you're using Resolve[] on nested quantifiers in Wolfram Alpha?  If so, how?  If not but there is another tool which can do this, would one be able to point me to it? For example, say I am determining the truth of the quantified expression $\forall_n \forall_m \exists_pP(n,m,p),\quad p = (m+n)/2 \quad$ where $n,m,p\in \Bbb R$ In Wolfram Alpha I could put: Resolve[ForAll[n, ForAll[m, Exists[p, p = (m+n)/2]]]] and it correctly comes back with True as a result since there is a real number $p$ for any real values $m,n$ . Suppose though I wanted the domain to be positive integers $\Bbb Z^+$ .  Now the answer to the same problem would be False.  Is it possible to add the restriction to the domain in Wolfram Alpha or another software tool?","['quantifiers', 'predicate-logic', 'wolfram-alpha', 'discrete-mathematics']"
4397535,Listing possibilities of a group based on some hints,"I'm trying to list all the possibilities for a group given the following conditions: $G$ is of odd order, non-cyclic, contains no element of order $2$ , and contains an element of order greater than $6$ . Also the order of $G$ divides $120$ . My first reaction was to look at the permutation groups, but it seems they all contain an element of order $2$ . Any hints are appreciated (please no full solutions!)","['group-theory', 'abstract-algebra', 'finite-groups']"
4397664,Topological Minors on Simple Graphs --- Antichains?,"This is a follow-up discussion on @296991 , with further questions. In the following discussion, ""graphs"" are finite but not necessarily simple (i.e. $|V(G)|<\infty,$ but allow loops and repeated edges). $H$ is a topological minor of $G$ ( $H\preceq_{\textrm{topo.minor}}G$ ) if $H$ is a subdivision of a subgraph of $G$ . A well quasi-order is a quasi-order without infinite antichain, i.e. $\nexists \{a_n\}_{n\in\omega}$ s.t. $\forall n<m$ , $a_n\not\preceq a_m$ . It is known that the topological minor order is not a well quasi order on graphs, witnessed by e.g. the antichain $\underline{\{A_n\}_n}$ (from Excluding a long double path minor ). Theorem 3.4 of the same paper proved that if $\mathscr{G}$ is a minor-closed class of graphs, then $(\mathscr{G},\preceq_{\textrm{topo.minor}})$ is a well-quasi-order iff $\mathscr{G}$ contains finitely many $A_n$ 's. However the class of simple graphs is not minor-closed: contracting an edge in a triangle gives a non-simple path. It was claimed in @Harry Altman's comment on @269961 that by subdivision, one may find a $\preceq_{\textrm{topo.minor}}$ -antichain in the class of simple graphs as well. I wonder how this is achieved: in particular, if we subdivide e.g. each $A_n$ into a simple graph $A'_n$ , it is not obvious to me that $A_n\not\preceq_{\textrm{topo.minor}} A_m\implies A'_n\not\preceq_{\textrm{topo.minor}} A'_m$ . Moreover, if subdivision does not work, are there any good sources proving or disproving the well-quasi-orderedness of (simple graphs, $\preceq_{\textrm{topo.minor}}$ ) ? Thank you.","['graph-theory', 'combinatorics', 'algorithms']"
4397757,Regarding verifying Gauss-Divergence theorem,"If $\vec{F}=4x \hat{i}-2y^{2}\hat{j}+z^{2}\hat{k}$ taken over the region bounded by the cylinder $x^{2}+y^{2}=4$ , $z=0$ and $z=3$ . Verify Divergence theorem. Attempted Solution : Here is what I've done as yet. To verify the theorem, we have to compute $\iiint_{V}\nabla\cdot \vec{F}\mathrm dV$ and $\iint_{S}\vec{F}\cdot \hat{n}\mathrm dS$ and show that they're equal. $$\iiint_{V}\nabla\cdot\vec{F}\mathrm dV =\int_{0}^{3}\int_{-2}^{+2}\int_{-\sqrt{4-x^{2}}}^{+\sqrt{4-x^{2}}}(4-4y+2z)\mathrm dy\mathrm dx\mathrm dz$$ As for the surface integral, we need to consider the three surfaces, viz. , two disks and a curved surface. I'm having slight trouble computing the integral over the curved surface. $$\begin{aligned}
x^{2}+y^{2}&=4 \\ 
\phi(x,y)&=x^{2}+y^{2}-4 \\
\hat{n}&=\frac{\nabla \phi}{|\nabla\phi|}=\frac{x\hat{i}+y\hat{j}}{2} \\
I&=\iint_{C}(2x^{2}-y^{3})\mathrm dS
\end{aligned}$$ I don't know how to proceed. I'm having trouble writing $\mathrm dS$ in terms of Cartesian coordinates and that is the source of confusion. If I were to project it onto the plane, would I make separate cases for each, or is there some neat way to solve this compactly. Thanks in advance.","['integration', 'multivariable-calculus', 'divergence-theorem']"
4397763,3D rotation matrix around a point (not origin),"I'm trying to find the rotation matrix for when rotating around a point that is not origin. I read another post and the post had said that we can think about moving the point back to the origin, but I think the question may have been about 2D. (or referring to a situation where the point is (a,b,0) where a,b != 0) I'm not sure how the method will apply to 3D situation or a situation where the point is (a,b,c) where all of a,b,c are nonzero. I would really appreciate help!","['matrices', 'rotations']"
4397789,"Is there any closed form for the integral $\int_{0}^{\infty} \frac{\ln ^{n} x}{1+x^{2}} d x$, where $n\in \mathbb N?$","Latest Edit Thanks to @Claude Leibovici and @Gary for giving us the closed form of the integral. Using MA , we have $$\frac{d^{ n}}{d{x}^{n}}\left(\sec x\right)= \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k-n) !} E_{2 k} x^{2 k-n} $$ $$
\begin{aligned}
\int_{0}^{\infty} \frac{\ln ^{2 n} x}{1+x^{2}} d x=&\left.\frac{\pi}{2} \frac{\partial^{2 n}}{\partial{a}^{2 n}}\left(\sec \left(\frac{a \pi}{2}\right)\right)\right|_{a=0} \\
=&\left.\left(\frac{\pi}{2}\right)^{2 n+1} \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2 k-2 n) !} E_{2 k} \left(\frac{a\pi}{2} \right) ^{2 k-2 n}\right|_{a=0} \\
=&\left(\frac{\pi}{2}\right)^{2 n+1}(-1)^{n} E_{2 n} \\
=&\left(\frac{\pi}{2}\right)^{2 n+1}\left|E_{2 n}\right|
\end{aligned}
$$ where $E_{2 k}$ is an Euler number . For example, $$
\begin{aligned}
\int_{0}^{\infty} \frac{\ln ^{20} x}{1+x^{2}} d x &=\frac{\pi^{21}}{2^{21}}|E _{10}| =\frac{370371188237525 \pi^{21}}{2097152},
\end{aligned}
$$ checked by MA . Noting that $$
\begin{aligned}
\int_{0}^{\infty} \frac{\ln^{2n-1}x}{1+x^2} d x &=\int_{0}^{\infty} \frac{\ln \left(\frac{1}{x}\right)}{1+\frac{1}{x^{2}}} \cdot \frac{d x}{x^{2}} =-\int_{0}^{\infty} \frac{\ln ^{2 n-1} x}{x^{2}+1} d x,
\end{aligned}
$$ we have $$
I_{2n-1}=\int_{0}^{\infty} \frac{\ln ^{2 n-1} x}{1+x^{2}} d x=0
$$ How about $$I_{2n}=\int_{0}^{\infty} \frac{\ln ^{2 n} x}{1+x^{2}} d x?$$ I like to switch the integration problem to a differentiation problem by defining a new integral. $$
J(a):=\int_{0}^{\infty} \frac{x^{a}}{1+x^{2}} d x
$$ Then $$
I_{k}=J^{(k)}(0)
$$ By my post $$\int_{0}^{\infty} \frac{x^{r}}{x^{m}+1} d x=\frac{\pi}{m} \csc \frac{(r+1) \pi}{m},$$ we have $$
J(a)=\frac{\pi}{2} \csc \frac{(a+1) \pi}{2}=\frac{\pi}{2} \sec \left(\frac{a \pi}{2}\right)
$$ Then $$
\begin{aligned}
\int_{0}^{\infty} \frac{\ln ^{2n} x}{1+x^{2}} d x &=\left.\int_{0}^{\infty} \frac{1}{1+x^{2}} \frac{\partial^{2n}}{\partial a^{2n}}\left(x^{a}\right) d x\right|_{a=0} \\
&=\boxed{\frac{\pi}{2} \frac{d^{2n}}{d a^{2n}}\left[\left.\sec \left(\frac{a \pi}{2}\right) \right]\right|_{a=0}}
\end{aligned}
$$ For examples, $$
\begin{aligned}
I_{2}&=\left.\frac{\pi}{2} \cdot \frac{d^{2}}{d a^{2}} \sec \left(\frac{a \pi}{2}\right)\right|_{a=0}=\frac{\pi^{3}}{8} \\
I_{4}&=\left.\frac{\pi}{2} \cdot \frac{d^{4}}{d a^{4}} \sec \left(\frac{a \pi}{2}\right)\right|_{a=0}=\frac{5 \pi^{5}}{32}\\ &\qquad \vdots
\end{aligned}
$$ Alternative method (By reduction formula) Applying Leibniz’s Rule and differentiating the following equation w.r.t $a$ by $n$ times $$
J(a) \cos \frac{a \pi}{2}=\frac{\pi}{2} ,
$$ we have $$
\sum_{k=0}^{2 n}\left(\begin{array}{c}
2 n \\
k
\end{array}\right) \left[\cos \left(\frac{a \pi}{2}\right)\right]^{(2 n-k)}  J^{(k)}(a)=0
$$ Putting $a=0$ yields $$
\sum_{k=0}^{2 n}\left(\begin{array}{c}
2 n \\
k
\end{array}\right) \left(\frac{\pi}{2}\right)^{2 n-k}\cos \left(\frac{(2 n-k) \pi}{2}\right)I_{k}=0
$$ Since $I_{2k-1}=0,$ we have found a reduction formula relating $I_{2k}$ . $$
\boxed{\sum_{k=0}^{n}\left(\begin{array}{l}
2 n \\
2 k
\end{array}\right)\left(\frac{\pi^{2}}{4}\right)^{n-k} \cos (n-k) \pi I_{2 k}=0}
$$ For example, $$
\begin{aligned}
\left(\begin{array}{l}
4 \\
0
\end{array}\right)\left(\frac{\pi^{2}}{4}\right)^{2} I_{0}+\left(\begin{array}{l}
4 \\
2
\end{array}\right)\left(\frac{\pi^{2}}{4}\right)  I_{2}+\left(\begin{array}{l}
4 \\
4
\end{array}\right) I_{4}=0 . \\
\frac{\pi^{4}}{16} \cdot \frac{\pi}{2}-6 \cdot \frac{\pi^{2}}{4}\left(\frac{\pi^{3}}{8}\right)+I_{4}=0 \\
I_{4}=-\frac{\pi^{5}}{32}+\frac{6 \pi^{5}}{3 \cdot 2}=\frac{5 \pi^{5}}{32}
\end{aligned}
$$ checked by WA . My Question Is there any formula for the $n^{th}$ derivative of $\sec x$ ? Your opinions and alternative solutions are highly appreciated.","['integration', 'calculus', 'derivatives']"
4397870,Multivariable Calculus Exam Mistake?,"This question was from an exam taken in January 2022 on a course on introductory multivariable calculus and was worded exactly as follows: ""For a general surface $S$ bounded by a closed curve $C$ show using Stokes theorem that for a vector field, $\mathbf{F}(\mathbf{r})$ $$\int_S\nabla\times(\mathbf{F}\times\mathrm{d}\mathbf{S})=\alpha\int_C\mathbf{F}\times\mathrm{d}\mathbf{r}$$ and identify the constant $\alpha$ ."" The "" $\mathrm{d}\mathbf{S}$ "" is used to denote a surface integral and the "" $\mathrm{d}\mathbf{r}$ "" to denote a line integral. I have asked a couple of mathematicians who work in applied mathematics and they have not been able to prove this either. The LHS is apparently the area of concern - having "" $\nabla\times(\mathbf{F}\times\mathrm{d}\mathbf{S})$ "" seems to be what's throwing people off. I have been told that the answer should result in a vector even though, in the course, vector-valued integrals were never defined and so the notions of "" $\times\mathrm{d}\mathbf{S}$ "" and "" $\times\mathrm{d}\mathbf{r}$ "" were also not defined, so while I personally believe this question was unfair, I'm asking more about whether or not it's possible. If this is a mistake, could you explain why? And if it isn't, can you prove it?","['integration', 'vector-fields', 'multivariable-calculus', 'calculus', 'stokes-theorem']"
4397922,Integrability/measurability of a map to the bounded operators on $L^2$,"For each $t\in [0,1]$ , let $T_t\colon L^2(\mathbb{R})\to L^2(\mathbb{R})$ be the operator that shifts everything to the right by $t$ , i.e. $$T_t f(x)=f(x-t)$$ for all $f\in L^2(\mathbb{R})$ . Then $t\mapsto T_t$ defines a map $$\gamma\colon[0,1]\to\mathcal{B}(L^2(\mathbb{R})).$$ Note that $\gamma$ is not continuous. Question: Is $\gamma$ Bochner-integrable, i.e. does the integral $$\int_{[0,1]}T_t\,dt$$ converge in $\mathcal{B}(L^2(\mathbb{R}))$ ? Thoughts: Since each operator $T_t$ has norm, the question is whether $\gamma$ is strongly measurable . In particular, there is a question of whether there exists a subset of $J\subseteq[0,1]$ of measure $1$ such that $\gamma(J)$ is separable. Certainly $\gamma([0,1])$ is not separable, and I suspect that no such subset $J$ exists, but I'm not sure how to show this.","['integration', 'c-star-algebras', 'measure-theory', 'operator-algebras', 'functional-analysis']"
4398071,Summation of cosines,"Find the sum $\displaystyle\sum_{n=1}^\infty\left(\frac{2\cos n}{\sqrt{n}}-\frac{1}{\sqrt{n}}\right)\left(\frac{2\cos n}{n}\right)^3\left(\frac{2\cos n}{\sqrt{n}}+\frac{1}{\sqrt{n}}\right)$ . I tried in many ways, like applying Fourier series, GP formula, $\cos^3x$ , conversions but nothing worked out.","['fourier-series', 'trigonometry', 'sequences-and-series']"
4398119,Associative Laws,Associative laws: $\begin{align}(p\lor q)\lor r&\equiv p\lor(q\lor r)\\(p\land q)\land r&\equiv p\land(q\land r)\end{align}$ Just curious is $(r\land p)\land q$ also the same as $(p\land q)\land r$ ? Like are there $3$ options here because I'm only seeing two options.,['discrete-mathematics']
4398122,Why do we need a function to be defined in a neighbourhood around $a$ so that it's differentiable at $a$?,"Compare it to the definition of continuity of a function $f$ , if $x \in B(a,\delta)$ , then $f(x) \in B(f(a),\varepsilon)$ . This definition doesn't need $f$ to be defined around a neighbourhood around $a$ , in fact $a$ can be an isolated point in the domain of $f$ . (I just checked), when we talk about the (single-var case) derivative of $f$ at $x_0$ , we do not need $f$ to be defined in a neighbourhood around $x_0$ . $$
\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} = f'(x_0) \iff |x-x_0|< \delta \implies |\frac{f(x)-f(x_0)}{x-x_0} - f'(x_0)| < \varepsilon.
$$ We just need that if $B(x_0,\delta)$ contains points, then the inequality above holds. However, in the $f:\mathbb{R}^p \to \mathbb{R}$ case, $f$ differentiable at $a \iff \exists l(x)$ linear function st. $f(x) = f(a) + l(x-a) + \varepsilon(x)\cdot|x-a|$ , where $\varepsilon(x) = 0$ if $x \to a$ . Why do we need $f$ to be defined in some neighbourhood here? The equivalent condition for differentiability of $f$ at $a$ is: $$
\lim_{x\to a} \frac{f(x)-f(a)-l(x-a)}{|x-a|} = 0 \iff \forall x \in B(a,\delta), |\frac{f(x)-f(a)-l(x-a)}{|x-a|}| < \kappa.
$$ I don't understand why we need $f$ to be defined in some neighbourhood around $a$ .","['multivariable-calculus', 'calculus']"
4398127,Concrete example of Lie derivative of a vector field,"I am struggling a lot with the concept of Lie derivative. I am studying it just in $\mathbb{R}^n$ not in a general manifold context. I have that its definition is: $$[v,w]:= \frac{d}{dt}((g_v^{-t})_*w)|_{t=0}$$ where $v,w$ are vector fields on an open $U \subset R^n$ , $g_v^t$ is the ""phase flow"" of $v$ and where the lower star index denotes the ""push-forward map"". Could you please provide an example of its calculation? Maybe working in $\mathbb{R}^2$ and choosing simple vector fields. For instance, let $v$ be the vector field given by $v_{(x,y)} = x (d/dx) + y (d/dy)$ and $w_{(x,y)} = (d/dx) + (d/dy)$ . What does $[v,w]$ equal to here?","['lie-derivative', 'differential-geometry']"
4398146,Is the limit of a family of sheaves a sheaf?,"So, I can prove that the kernel of a morphism of sheaves or a product of sheaves is a sheaf, but I do not know how to prove in general that $lim F_{i}$ is a sheaf for $F_{i}$ sheaves. I know that if we regard the $F_{i}$ as presheaves, then the limit exists in the category of presheaves but I do not know how to argue about the category of sheaves.","['homological-algebra', 'algebraic-geometry', 'limits-colimits', 'sheaf-theory']"
4398156,how to calculate a variable royalty number based on sold units,"First thing please accept my apologies for the way i'm explaining my question - I'm not a math or econ major. I have a product that another company want to sell for me and pay me royalty in cents per unit sold. now I need a formula that as number of sold units are going up, the royalty per unit decreases at the same time my income keep increase to make it interesting for both me and the sellers to keep working with each other. what that formula be so my income look like a square root graph.","['calculus', 'functions', 'derivatives', 'mathematical-modeling']"
4398169,Find the area of the shaded region. (Apparently this is a Chinese primary school math question!) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Rectangle $ABCD$ has an area of $1$ . $AE = ED$ . $3BF = AB$ . What is the shaded area?","['rectangles', 'triangles', 'area', 'geometry']"
4398214,An exercise on the implicit function theorem,"I am trying to learn the implicit function theorem and this is one exercise about it; I have solved it and would be grateful for any feedback on my solution, thanks. Let $f\begin{pmatrix}x\\ y\\ z\end{pmatrix}=x y^2+\sin(xz)+e^z$ and $\textbf{a}=\begin{bmatrix}1\\ -1\\ 0 \end{bmatrix}$ . (a) Show that the equation $f=2$ defines $z$ as a $\mathcal{C}^1$ function $z=\phi\begin{pmatrix}x\\ y\end{pmatrix}$ near $\textbf{a}.$ (b) Find $\frac{\partial\phi}{\partial x}\begin{pmatrix}1\\-1\end{pmatrix}$ and $\frac{\partial\phi}{\partial y}\begin{pmatrix}1\\-1\end{pmatrix}.$ (c) Find the equation of the tangent plane of the surface $f^{-1}(\{2\})$ at $\mathbf{a}$ in two ways. What I have done: (a) Let $F=f-2$ ; $F$ is a $\mathcal{C}^1$ function, $F(\mathbf{a})=0,\ DF\begin{pmatrix}x\\ y\\ z\end{pmatrix}=\begin{bmatrix}y^2+z\cos(xz) &2xy & x\cos(xz)+e^z\end{bmatrix}$ and $DF(\mathbf{a})=DF\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}=\begin{bmatrix}1 & -2 & 2\end{bmatrix}$ so in particular $\frac{\partial F}{\partial z}(\mathbf{a})=2\neq 0$ thus there exists a neighborhood V of $\begin{pmatrix}1\\-1\end{pmatrix}$ and $W$ of $0$ and a $\mathcal{C}^1$ function $\phi:V\to W$ so that $z\in W\Leftrightarrow z=\phi\begin{pmatrix}x\\ y\end{pmatrix},\ \begin{pmatrix}x\\ y\end{pmatrix}\in V.$ (b) $$\frac{\partial\phi}{\partial x}=-\frac{\frac{\partial F}{\partial x}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}{\frac{\partial F}{\partial z}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}=-\frac{1}{2}$$ and $$\frac{\partial\phi}{\partial y}=-\frac{\frac{\partial F}{\partial y}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}{\frac{\partial F}{\partial z}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}=-\frac{-2}{2}=1.$$ (c) Tangent plane: $$\begin{bmatrix}1 & -2 & 2\end{bmatrix} \begin{bmatrix}x-1\\ y+1\\ -z\end{bmatrix}=x-1-2(y+1)-2z=0\Leftrightarrow x-2y-2z=3$$","['multivariable-calculus', 'implicit-function-theorem', 'real-analysis']"
4398247,Axiomatizations of the boundary operator,"The German Wikipedia page on topological boundary [1] states that the boundary operator $\partial$ can be characterized by the following four axioms: $\partial\emptyset = \emptyset$ , $\partial(\partial U) \subseteq \partial U$ , $\partial(U^\complement)= \partial U$ , and $(U\cap V)\cap \partial(U\cap V) = (U\cap V)\cap (\partial U\cup \partial V)$ for each $U,V\subseteq X$ Is this really correct? There are two references given of which one is available online [2]. But the author apparently distinguishes between boundary and frontier. The author defines the interior $\mathbf{int}(S)$ of a set $S$ as the largest open subset of $S$ ; the boundary is defined as ""[the union of] the points of $S$ which are not interior points constitute [the boundary] $\mathbf{bd}(S)$ ."" The author proceeds and defines the frontier $\mathbf{fr}(S)$ of $S$ as ""the union of $\mathbf{bd}(S)$ and $\mathbf{bd}(S')$ "", where $S'$ denotes the complement of $S$ in $X$ . (Note that I replaced $X$ with $S$ to be constitent with the above quote from Wikipedia). I understand the definitions as \begin{align*} \mathbf{bd}(S) &= S\setminus\mathbf{int}(S) \qquad\text{and}\\ \mathbf{fr}(S) &= \mathbf{bd}(S)\cup \mathbf{bd}(S^\complement).\end{align*} Consider $\mathbb R$ together with the usual topology, and let $S = [0, 1)$ . Then $\mathbf{bd}([0,1)) = \{0\}$ , and $\mathbf{fr}([0,1)) = \{0,1\}$ . I see the boundary $\partial S$ of the set $S$ typically defined as $\partial S = \mathbf{cls}(S) \cap \mathbf{cls}(S^\complement)$ , or, equivalently, defined as $\partial S = \mathbf{cls}(S) \cap \mathbf{int}(S)^\complement$ . Since $\partial([0,1)) = \{0,1\}$ , I suppose that $\partial = \mathbf{fr}$ . Is this correct? I am skeptical because I usually see the boundary operator $\partial$ axiomatized by $\partial \emptyset = \emptyset$ , $\partial(\partial U) \subseteq \partial U$ , $\partial(U^\complement) = \partial U$ , $\partial U\subseteq V\cup \partial V$ if $U\subseteq V$ , and $\partial (U\cup V) \subseteq \partial U\cup \partial V$ for all $U,V\subseteq X$ . But I don't see how to derive 4. from 4.1. and 4.2., as well as the converse, i.e. derive 4.1 and 4.2 from 4. So, do both axiomatizations agree with each other? And if yes, how can I show it? This is far from obvious for me. References [1] https://de.wikipedia.org/wiki/Rand_(Topologie)#Randaxiome [2] Vaidyanathaswamy: Set topology. 1964, p. 57–58.
URL: https://books.google.de/books?id=yDMipybQ64kC&printsec=frontcover&hl=de#v=onepage&q&f=false",['general-topology']
4398390,Understanding the quotient of $S^2$ by a non-free action of $\mathbb{Z}_2$,"I was reading this post and wondered about a similar thing. Let $\mathbb{Z}_2$ act on $S^2$ by letting the non-trivial element take $$(x,y,z)\mapsto(-x,-y,z).$$ This action is not free, so one would expect something to go ""wrong"" with the quotient space $S^2/\mathbb{Z}_2$ . On the surface, it seems like $S^2/\mathbb{Z}_2$ is a nice space; it is homeomorphic to $S^2$ , which is a topological manifold. However, the north and south ends of $S^2/\mathbb{Z}_2$ seem somewhat crushed and ""conical"", instead of smooth. But I'm not sure how to match this intuition with something rigorous. Question 1: Does $S^2/\mathbb{Z}_2$ inherit a smooth structure from $S^2$ ? If not, why not? Question 2: Is there some precise sense in which the north and south ends of $S^2/\mathbb{Z}_2$ are conical? (This would seem to assume that $S^2/\mathbb{Z}_2$ has a natural Riemannian metric; why is this the case?)","['riemannian-geometry', 'smooth-manifolds', 'manifolds', 'group-actions', 'differential-geometry']"
4398421,Reading off connection 1-forms from Cartan's structural equation $de=-\omega\wedge e$,"Suppose we have a Lorentzian metric of the form \begin{align}
g&=-f(r)^2\,dt^2+ h(r)^2(dr^2+r^2\,d\theta^2+r^2\sin^2\theta\,d\phi^2)
\end{align} Where $f,h$ are say strictly positive functions. We use the Levi-Civita connection. I introduced the 1-forms \begin{align}
e^0=f(r)\,dt,\quad e^1=h(r)\,dr,\quad e^2=rh(r)\,d\theta,\quad e^3=rh(r)\sin\theta\,d\phi
\end{align} which diagonalize the metric, and now I'm trying to use these to calculate the connection 1-forms $\omega^a_{\,b}$ using Cartan's structural equation $de=-\omega\wedge e$ (since Levi-Civita connection is torsion free). Question 1. The issue I'm facing is that once I calculate $de$ , I'm not sure how to identify $\omega$ from those equations: initially I tried the most naive thing by just looking at the appropriate coefficient and calling that the appropriate component of $\omega$ , but I think this naive approach is wrong, probably because the wedge-product of non-zero forms can still be zero (so ""cancelling"" terms won't work). To be more explicit, I calculated \begin{align}
\begin{cases}
de^0= f'(r)\,dr\wedge dt\\
de^1= 0\\
de^2=(h(r)+rh'(r))\,dr\wedge d\theta\\
de^3= (h(r)+rh'(r))\sin\theta\,dr\wedge d\phi+ rh(r)\cos\theta\,d\theta\wedge d\phi
\end{cases}
\end{align} When I first did the calculation, I naively concluded that \begin{align}
de^0=f'(r)\,dr\wedge dt=-\left[-\frac{f'(r)}{f(r)}\,dr\right]\wedge e^0,
\end{align} and thus that $\omega^0_0=-\frac{f'(r)}{f(r)}\,dr, \omega^0_1=\omega^0_2=\omega^0_3=0$ . Next, from $de^1=0$ I naively concluded that $\omega^1_{\,b}=0$ for all $b=0,1,2,3$ . I did a similar thing with the other equations. But now I realize this is wrong, because for example, we can also write \begin{align}
de^0=f'(r)\,dr\wedge dt=
-\left[\frac{f'(r)}{h(r)}\,dt\right]\wedge e^1,
\end{align} so if I were to use my above logic, I would have $\omega^0_0=0, \omega^0_1=\frac{f'(r)}{h(r)}\,dt, \omega^0_2=\omega^0_3=0$ . So clearly my mistake stems from the fact that the wedge of non-zero forms can be zero. But now I'm not sure what the correct approach is. I have read this answer by @Ted Shifrin, and it seems like the correct answer is the second approach, but I'm not sure why. Also, I can't really understand that answer because it's not clear to me why certain certain $\omega^a_b$ are equal to certain functions and why others are multiples of some $e^i$ , and why some others are zero. Question 2. The equation $de=-\omega\wedge e$ consists of four equations relating $2$ -forms. However, $\omega$ being a $4\times 4$ matrix (in this case) of $1$ -forms, consists a-priori of 16 unknowns. I believe in this case due to the Lorentzian signature and the diagonalizability of the metric, there is some relationship between $\omega^a_b$ and $\omega^b_a$ , so that it can be written as \begin{align}
[\omega^a_b]&=
\begin{pmatrix}
0&\alpha_1&\alpha_2&\alpha_3\\
\alpha_1&0&\beta_1&\beta_2\\
\alpha_2&-\beta_1&0&\beta_3\\
\alpha_3& -\beta_2&-\beta_3&0
\end{pmatrix}
\end{align} for some 1-forms $\alpha_1,\beta_i$ . So, now there are only 6-unknowns, but this is still too many  unknowns for the number of equations. So my question is whether we can always use this structural equation to determine $\omega$ completely? I believe the answer is yes because for the case of Christoffel symbols $\Gamma^i_{jk}$ we have explicit formulas for it in terms of the metric, and now since $\omega$ are related to $\Gamma$ in some fashion, the same ought to hold true; but now I'm not sure how to reconcile this with the above counting argument (6 unknowns vs 4 equations).","['connections', 'semi-riemannian-geometry', 'differential-forms', 'differential-geometry']"
4398442,"Upper and lower bounds for $\int_0^\infty e^{-u(u^\epsilon -1)}\,du$","I am interested in reasonable to obtain upper and lower bounds, as well as techniques used in doing so, for $\int_0^\infty e^{-u(u^\epsilon -1)}\,du$ for epsilon in some right-neighborhood of zero, i.e. valid for all $0<\epsilon<\delta$ with $\delta>0$ . I believe I was able to prove the integral is $\mathcal{O}(1/\epsilon)$ as $\epsilon\to0^+$ by using the fact that $u^\epsilon -1\ge \epsilon\log u$ and then $\int_e^\infty e^{-\epsilon u\log u}\,du\le\int_e^\infty e^{-\epsilon u}\,du$ but didn't get much further. Edit: So it seems that thanks River Li's comment, we now have the bounds $$\frac{c}{\sqrt{\epsilon}}<\int_0^\infty e^{-u(u^\epsilon -1)}\,du < \frac{C}{\epsilon}$$ for some constant $C, c$ in some right neighborhood of zero. Can we do better?","['inequality', 'definite-integrals', 'upper-lower-bounds', 'real-analysis']"
4398498,Indicator random variables of two events,"Before someone closes this question or marks it as a duplicate, I would like to point out that this question is based on another identical asked question here . It never got a real answer so this is my own attempt at solving the problem. Let A be an event, and let $I_{A}$ be the associated indicator random variable: $I_{A} (\omega)=1$ if $ω∈A$ , and $I_{A}(ω)=0$ if $ω∉A$ . Similarly, let $I_{B}$ be the indicator of another event, $B$ . Suppose that, $P(A)=p$ , $P(B)=q$ , and $P(A∪B)=r$ . To find $E[(I_{A}−I_{B})^2]$ in terms of $p$ , $q$ and $r$ $$E[(I_{A}−I_{B})^2]=E[(I_{A}−I_{B})(I_{A}−I_{B})]$$ $$E[(I_{A}−I_{B})^2]=E(I^2_{A}-2I_{A}I_{B}+I^2_{B}]$$ Given $I^2_𝐴=I_𝐴$ and $I_𝐴I_𝐵=I_{𝐴∩𝐵}$ , then $$E[(I_{A}−I_{B})^2]=E[I_{A}-2I_{A∩B}+I_{B}]$$ $$E[(I_{A}−I_{B})^2]=E[I_{A}]-2E[I_{A∩B}]+E[I_{B}]$$ $$E[(I_{A}−I_{B})^2]=P(A)-2P(A∩B)+P(B)$$ Given $𝑃(𝐴∩𝐵)=𝑃(𝐴)+𝑃(𝐵)−𝑃(𝐴∪𝐵)$ , then $$E[(I_{A}−I_{B})^2]=P(A)-2(P(A)+P(B)-𝑃(𝐴∪𝐵))+P(B)$$ $$E[(I_{A}−I_{B})^2]=2𝑃(𝐴∪𝐵)-P(A)-P(B)=2r-p-q$$ I am not completely certain about going from $-2I_{A∩B}$ to $-2E[I_{A∩B}]$ What I would like to know, is this transition legit? If so, determine $\text{Var}(I_{A}−I_{B})$ in terms of $p$ , $q$ and $r$ by substituting. Given $𝖵𝖺𝗋(𝑋)=E[𝑋]^2−(E[𝑋])^2$ , then $$𝖵𝖺𝗋(I_{A}−I_{B})=E[I_{A}−I_{B}]^2−(E[I_{A}−I_{B}])^2$$ $$𝖵𝖺𝗋(I_{A}−I_{B})=E[(I_{A}−I_{B})^2]−(E[I_{A}]−E[I_{B}])^2$$ $$𝖵𝖺𝗋(I_{A}−I_{B})=2r-p-q−(p−q)^2$$ Anyone feel free to show another method or to correct me if I am wrong.","['expected-value', 'probability', 'random-variables']"
4398552,Probability of A winning the game [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The following question is from problem 30 of the 2021 level 11-12 Mathematical Kangaroo: A certain game is won when one player gets 3 points ahead. Two players A and B are playing the game and at a particular point, A is 1 point ahead. Each player has an equal probability of winning each point. What is the probability that A wins the game? Apparently, the solution is 2/3, but I don’t quite know how the solution is gotten. Any help will be much appreciated!","['puzzle', 'probability-theory', 'probability']"
4398553,"What is the expected length of the ""Slimy Slug"" cellular automata","A ""Slimy Slug"" is a simple probabilistic cellular automata defined as follows: The Slimy Slug lives on an infinite, two-dimensional, white-coloured, square grid. For each iteration, the slug follows these rules: Paint the cell the Slug is on black. Look at the 4 orthogonal adjacent cells (i.e. N,E,S,W) If zero of the cells are white, Halt . If exactly one of the cells is white, move to the white cell. If more than one of the cells are white, randomly move to one of the white cells. (e.g. if the white cells are N,E,W, each cell has a 1/3 probability of being the destination.) Go to rule 1. The slug will follow a random walk, and although it could theoretically continue for an infinite number of steps, it will generally halt when it traps itself. An example of a possible trail of the slug is shown below (Instead of black and white, I have false-coloured the cells with a gradient to illustrate the path of the slug). This trail has a length of 99 steps, and the slug started in the top right, and worked it's way down to the bottom left. Is it possible to arrive at an expression for the expected length of the Slimy Slug's trail? This is outside my field, so I haven't had much success in researching if this is a studied problem or not. Via numerical simulation, I have arrived at a value of 70.799611 for a set of 1,000,000 runs, but I'm interested in seeing if a value can be arrived at without numerical simulation. This is motivated by a related problem in game design/educational exercise design.","['cellular-automata', 'random-walk', 'probability']"
4398600,MLE for uniform distribution with two parameters,"A finite number of random variables $X_1 ... X_n$ have a uniform distribution $[a, b]$ with $b>a$ , such that it has the following density: $$f_X(x) = \frac{1}{b-a}, a\le x \le b$$ find the MLE. Here's what I have tried: $L(b,a;x) = \prod_{i=1}^nf(x_i;b, a) = \prod_{i=1}^n\frac{1}{b-a} = \left(\frac{1}{b-a}\right)^n \\  
\mathbf{L}(b,a;x) = \log L(b,a;x) = -n\log(b-a)$ Taking the first and second derivative in respect to b, and then with respect to a. $\frac{\partial\mathbf{L}(b,a;x)}{\partial b} = -\frac{n}{(b-a)} \\ 
\frac{\partial\mathbf{L}(b,a;x)}{\partial a} = \frac{n}{(b-a)} \\
\frac{\partial^2\mathbf{L}(b,a;x)}{\partial b^2} = \frac{2n}{(b-a)^2} \\
\frac{\partial^2\mathbf{L}(b,a;x)}{\partial a^2} = -\frac{2n}{(b-a)^2} \\
\frac{\partial^2\mathbf{L}(b,a;x)}{\partial ba} = -\frac{2n}{(b-a)^2}$ When I set the first two derivatves to zero to find the turning points, and these are when $b=a$ .
To find the local maximum, I plug these into the hessian matrix: $$H :=\begin{pmatrix} \frac{\partial^2\mathbf{L}(b,a;x)}{\partial b^2}&\frac{\partial^2\mathbf{L}(b,a;x)}{\partial ba} \\ \frac{\partial^2\mathbf{L}(b,a;x)}{\partial ba}&\frac{\partial^2\mathbf{L}(b,a;x)}{\partial a^2} \end{pmatrix} \implies \begin{pmatrix} \frac{2n}{(b-a)^2}&-\frac{2n}{(b-a)^2} \\ -\frac{2n}{(b-a)^2}&-\frac{2n}{(b-a)^2} \end{pmatrix} \\ 
\\
= -\left(\frac{2n}{(b-a)^2}\right)\left(\frac{2n}{(b-a)^2}\right) -\left(\frac{2n}{(b-a)^2}\right)\left(\frac{2n}{(b-a)^2}\right) =  -\frac{8n^2}{(b-a)^4}$$ Given that $\frac{\partial^2\mathbf{L}(b,a;x)}{\partial b^2}>0$ and $Det(H) < 0$ , then our turning point is not a local maximum as the reverse inequality should happen for this to be the case. However, this cannot be true. The actual question assumes three random variables such that $x_1 = 1.5, x_2 = 4.6, x_3 = 7.2$ , and I'm supposed to plug these into the MLE. However, if there is no MLE then I cannot do this. So I have gone wrong somewhere.","['statistics', 'maximum-likelihood']"
4398606,"I made my own question, but I can't solve it, any help appreciated","I'm a math teacher and this is a question for my students. I'm certain I'm missing something very simple but I cant seem to get it. I made the question just through sketching pretty pattern, seeing the length in that case for $n=12$ was almost the golden ratio and it got me interested. $x$ and $y$ are the lengths seen in the picture. EDIT: changed the wording thanks to suggestions",['geometry']
4398669,Bayesian Prediction,"It is Question #35 from this https://math.illinoisstate.edu/actuary/ExamC/ExamCMay2005.pdf . I understand all other parts except the first line of the answer key. For the geometric distribution, $Pr(X_1=2|\beta) = \frac{\beta^2}{(1+\beta)^3}$ . I can see that it is based on the equation $\frac{\beta^{k-1}}{(1+β)^k}$ , but I have no idea where did that come from. Can someone please help me? From what I understand, the geometric distribution is $p(1-p)^k$ .","['actuarial-science', 'statistics', 'bayesian']"
4398714,Ellipse through $5$ points,"I have seen it stated that you need 5 points to define an ellipse, and that an ellipse can be drawn through 5 points as long as any $3$ points aren't on the same line. How can an ellipse be drawn through these $5$ points: or am I missing some other condition on when ellipses can be formed from a set of points $(1,1),(1,-1),(-1,1),(-1,1),(0,1/2)$ ?","['conic-sections', 'geometry']"
4398715,Closed form for $T_n$ if $T_{n+3}=2 T_{n+2}+2 T_{n+1}-T_{n}$,"Consider the recurrence relation $$T_{n+3}=2 T_{n+2}+2 T_{n+1}-T_{n}$$ with first three terms as: $T_{1}=20, T_{2}=12, T_{3}=70$ Find the closed form expression for $T_n$ . My try:
Since it is a constant coefficient difference equation, the auxiliary equation is: $$\lambda^3-2\lambda^2-2\lambda+1=0$$ whose roots are: $-1,\frac{3\pm\sqrt{5}}{2}$ Thus we have: $$T_{n}=A(-1)^{n}+B\left(\frac{3+\sqrt{5}}{2}\right)^{n}+C\left(\frac{3-\sqrt{5}}{2}\right)^{n}$$ where the constants $A,B,C$ to be determined by values of $T_1,T_2,T_3$ but its becoming too tedious to solve. Any alternate approach?","['algebra-precalculus', 'recurrence-relations', 'sequences-and-series']"
4398727,Is the continuous image of a Borel subset Lebesgue measurable?,"Let $f:\mathbb R^n \to \mathbb R^n$ be a continuous map and $B \subset \mathbb R^n$ a Borel subset. It is well known that $f(B)$ may not be a Borel subset. My question is, can we prove that $f(B)$ must be measurable for Lebesgue measure?","['measure-theory', 'lebesgue-measure', 'real-analysis', 'measurable-sets', 'borel-sets']"
4398729,"Example for a continuous function $x \geq 0$ on $[0,\infty)$ so that $x(0)=0$ and $\left(x(t) \right)^2\leq 2+\int_{0}^{t}x(u)du,~~~\forall ~t\geq 0$","Q. Suppose $x:[0,\infty)\to [0,\infty)$ is continuous and $x(0)=0.$ If $$\left(x(t) \right)^2\leq 2+\int_{0}^{t}x(u)du,~~~\forall ~t\geq 0,$$ then which of the following is TRUE? $x(\sqrt{2})\in [0,2]$ $x(\sqrt{2})\in [0,\frac{3}{\sqrt{2}}]$ $x(\sqrt{2})\in [\frac{5}{\sqrt{2}},\frac{7}{\sqrt{2}}]$ $x(\sqrt{2})\in [10,\infty)$ By letting $y(t)=2+\int_0^tx(u)du$ , we have $\sqrt{x(t)^2}=x(t)=y'(t) \leq \sqrt{y(t)},$ and thereby considering the monotonicity of the function $g(t)=2\sqrt{y(t)}-t$ , we get $x(\sqrt{2})\leq 3/\sqrt 2$ . Now, can you point out a function $x$ with $2<x(\sqrt 2) \leq 3/\sqrt 2$ to ignore option 1?","['integral-inequality', 'ordinary-differential-equations', 'real-analysis']"
4398760,Doubt in the definition of manifolds?,"Let $M$ be a Hausdorff space. Assume that there exists an open covering $\{U_{\alpha} : \alpha \in A\}$ of $M$ and homeomorphisms $\varphi_{\alpha}$ , from $U_{\alpha}$ onto an open subset $\varphi_{\alpha}(U_{\alpha})$ of $\mathbb{R}^m(\alpha)$ with $m(\alpha)$ a nonnegative integer. There exists $k \in \mathbb{N} \cup \infty$ such that whenever for $\alpha,\beta \in A$ we have then the map $U_{\alpha} \cap U_{\beta} \ne \phi$ then the map: $$\varphi_{\alpha} \circ \varphi_{\beta}^{-1}:\varphi_{\beta}(U_{\alpha} \cap U_{\beta}) \to \varphi_{\alpha}(U_{\alpha} \cap U_{\beta})$$ is $C^k$ . The intuition of the definition behind the manifold is that it behaves locally as a subset of $\mathbb{R}^n$ ( there exists a homeomorphism between $U_p$ a neihbourhood of all possible points $p \in M $ such that $\varphi(U_p)$ is a subset of $\mathbb{R}^n$ ) such that any function $f:U \to \mathbb{R}$ is $C^k$ [( $f \circ \varphi^{-1}):\varphi(U) \to \mathbb{R} $ is $C^k$ ( $k$ times differentiable)]. I can understand the first line of the definition but I am stuck with $$\varphi_{\alpha} \circ \varphi_{\beta}^{-1}:\varphi_{\beta}(U_{\alpha} \cap U_{\beta}) \to \varphi_{\alpha}(U_{\alpha} \cap U_{\beta}) is C^k $$ - how is it related?","['manifolds', 'differential-geometry']"
4398762,There exist on some discrete probability space independent events $A_n$ satisfying $P\left(A_n\right)=p_n$.,"Suppose that $0\leq p_n\leq 1$ and put $\alpha_n=\min\left\{p_n,\,1-p_n\right\}$ . Show that, if $\sum\alpha_n$ converges, then on some
discrete probability space there exist independent events $A_n$ satisfying $P\left(A_n\right)=p_n$ . This is problem 5.15 in Billingsley's Probability and Measure . In this section, it has already proved that for any sequence of measures $\left\{\mu_n\right\}$ on the class of all subsets of $\mathbb{R}$ with finite support, there exist on some probability space a sequence of independent simple random variables $\left\{X_n\right\}$ whose distribution is $\mu_n$ . But the probability space constructed in that proof is just the unit interval and I am unable to modify it to meet the requirement of this problem.","['measure-theory', 'probability-theory']"
4398793,"""Guessing"" the Shape of a Function based on its Gradient-Vector Field","I was looking at this link here: https://en.wikipedia.org/wiki/Gradient This page contains visualizations of two different functions along with the Gradient-Vector Fields: The second picture (picture on the right hand side, orange color) seems to be straightforward: We can clearly see that this function has a global minimum that is located somewhere in the middle of the plot. If I understand correctly, the ""blue arrows"" are showing the Gradient-Vector field : For instance, take any ""blue arrow"" and project it orthogonally upwards on to the function - if you were to ""reverse the direction"" of a given ""blue arrow"" (i.e. ""negative direction""), this ""reversed direction"" would now point in the direction towards the minimum of the function. I am now looking at the first picture (picture on the left hand side, red/blue/green/yellow colors). Just based on this picture, I think the corresponding function two regions where the derivative is 0 - and the (negative direction of these) arrows are pointing towards these regions (I think these are Saddle Points?). However, I am not sure about this. I tried to make a 3D plot of this function [ x * 2.718^(-x^2 + y^2) ]: Here is a 3D plot of the same function but from a different perspective: I think my assertion is correct? Is what I have described the actual relationship between the surface of a function and its Vector-Gradient field? Thank you!","['maxima-minima', 'functions', 'derivatives', 'vector-fields']"
4398878,Union of sets having $\binom{n - 1}{2}$ elements with each two of them having $n - 2$ common elements has cardinality of at least $\binom{n}{3}$.,"Let $S_1, S_2, \dots, S_n$ sets that have each of them $n - 1 \choose 2$ elements, with $n - 2$ common elements for each two of them. Prove that their union has at least $n \choose 3$ elements. Find an example for equality case. Let us examine the simplest case $n = 3$ . Each of them has $2 \choose 2$ $= 1$ elements and each two of them have $1$ element in common, so, in fact, the three sets are identical, so their union has exactly $3 \choose 3$ $= 1$ . (Equality case confirmed) Examining $n = 4$ , each set should have $3 \choose 2$ $= 3$ elements and $2$ in common every $2$ . We may construct $\{a, b, c\}$ , $\{a, b, d\}$ , and the last set may be $\{a, b, e\}$ or $\{a, c, d\}$ , both of them satsifying the conditions. However, I am unable to generalize the problem","['combinations', 'combinatorics', 'extremal-combinatorics']"
4398968,"Why is there a pattern in exponents where x^y and (x+1)^y increases by y! ""accelerated"" by y in?","I've noticed with exponents a certain pattern that occurs. 1^2=1 | 2^2=4 | 3^2=9 | 4^2=16 | 5^2=25 1+ 3 =4 | 4+ 5 =9 | 9+ 7 =16 | 16+ 9 =25 You find 3, 5, 7, and 9; all 2 in between. It takes 2 times of taking the difference to reach a constant number. 1^3=1 | 2^3=8 | 3^3=27 | 4^3=64 | 5^3=125 1+ 7 =8 | 8+ 19 =27 | 27+ 37 =64 | 64+ 61 =125 7+ 12 =19 | 19+ 18 =37 | 37+ 24 =61 12, 18, and 24 are all 6 apart; and it takes three (since it cubed) times to get there. I've tried this more times, I left it out so it doesn't take too much space. It comes to a constant 2 (or 2!) after 2 times for squared, and 6 (or 3!) with it taking three times to get there. It makes sense that there would be a pattern like that, but I can't understand why or find any proof.","['induction', 'recursion', 'discrete-mathematics', 'algebra-precalculus', 'finite-differences']"
4399003,"Having some trouble with proving for some sets $M,N$ that $M \cap N = M$ if and only if $M \subseteq N$","Proposition: $\textit{Let M and N be sets. Prove that $M \cap N = M$ iff $M \subseteq N$}\\$ My Proof:
Going from the definition of set intersection and subsets, the above statement can be rewritten as follows: $$\{a: (a \in M \wedge a \in N) \equiv a \in M \} \equiv \{a: a \in M \Rightarrow a\in N\}.$$ Now let $p = a \in M$ and $q = a \in N$ . This allows us to further rewrite the above statement in predicate logic: $$((p \wedge q) \equiv p) \equiv (p \Rightarrow q).$$ This is a tautology, as shown by the truth table below. Therefore the intersection of the sets M and N equal N if and only if M $\subseteq$ N. Is my proof even correct? My math professor doesn't find it sound to convert this statement into predicate logic and go from there, he just breaks apart the if and only if into two implications, and then goes by the definition of subsets.  Which approach is better? Am I not employing the correct definition of set equivalence in terms of logic?","['predicate-logic', 'proof-writing', 'alternative-proof', 'solution-verification', 'elementary-set-theory']"
4399026,Proving that a set is dedekind-infinite if and only if it has a countably infinite subset,"I am interested in proving the following statement: A set is Dedekind-infinite if and only if it has a countably infinite subset. Here is my attempt: $(\Leftarrow)$ Suppose that $A$ has a countably infinite subset $E$ . Arrange the terms of $E$ in a sequence $x_0,x_1,x_2,\dots$ of distinct elements. Now let $f:A\to A\setminus\{x_0\}$ be defined as follows: $$
f(x)=
\begin{cases}
x_{n+1} &\text{if $x=x_n$ for some $n\in\mathbb N$} \\
x &\text{otherwise}
\end{cases}
$$ Then, $f$ is a bijection from $A$ to a proper subset of $A$ , namely $A\setminus\{x_0\}$ . However, I am unsure how to prove the forward implication. Does it require a form of the axiom of choice?","['elementary-set-theory', 'axiom-of-choice', 'solution-verification', 'set-theory']"
4399028,Proof that this is a Norm,"Let $k$ be any natural number. I'd like to prove that the expression $$
\|f\|_{C^{k}[a, b]}=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|
$$ is a norm on $C^{k}[a, b]$ . What I have accomplished: For $f \in C^k[a, b]$ we have $0 \leq\|f\|<\infty$ .
Positivity: $\|f\| \geq 0$ is obvious and $\|f\|=0$ is equivalent to $$
\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|=0,
$$ i.e. $f(t)=0$ for all $t \in [a,b]$ , i.e. $f=0.$ Absolute homogeneity: $$
\|\lambda f\|=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} \lambda f(t)}{d t^{l}}\right|=|\lambda| \max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l} f(t)}{d t^{l}}\right|=|\lambda|\|f\| .
$$ Triangle inequality: For each $t \in [a,b]$ , the following applies $$
\left|\frac{d^{l}}{d t^{l}} (f(t)+g(t))\right | \leq\left|\frac{d^{l}}{d t^{l}} f(t)\right |+\left|\frac{d^{l}}{d t^{l}} g(t)\right | \leq\|f\|+\|g\|,
$$ from which follows $$
\|f+g\|=\max _{0 \leq l \leq k} \max _{a \leq t \leq b}\left|\frac{d^{l}}{d t^{l}} (f(t)+g(t))\right | \leq|f\|+\|g\| . 
$$ I'd like to know if this is flawless or if there is room for enhancement. Thanks in advance!","['solution-verification', 'normed-spaces', 'functional-analysis']"
4399047,Proof $(A \times B) \cup (C \times D)$ is subset of $(A \cup C) \times (B \cup D)$,"I am trying to figure out how to do this subset proof. I was given a proposition in class, which is true. The proposition is $(\mathrm{A}\times\mathrm{B})\cup(\mathrm{C}\times\mathrm{D}) \subseteq (\mathrm{A}\cup\mathrm{C}) \times (\mathrm{B}\cup\mathrm{D})$ where $\mathrm{A}\times\mathrm{B} = \{(a,b):a\in\mathrm{A}\wedge b\in\mathrm{B}\}$ . The proof we were given was as follows: $$(a,b)\in(\mathrm{A}\times\mathrm{B})\cup(\mathrm{C}\times\mathrm{D})$$ $$\Rightarrow(a\in\mathrm{A}\wedge b\in\mathrm{B})\vee(a\in\mathrm{C}\wedge b\in\mathrm{D})$$ the transition from the previous step to the next one is where [I thought] the error [was]: $$\Rightarrow(a\in\mathrm{A}\vee a\in\mathrm{C})\wedge(b\in\mathrm{B}\vee b\in\mathrm{D})$$ $$\Rightarrow (a,b)\in(\mathrm{A}\cup\mathrm{C}) \times (\mathrm{B}\cup\mathrm{D})$$ Unless I am mistaken, if the distribution were correct, this would mean that the two original sets were equivalent and $\Leftrightarrow$ would be used instead of $\Rightarrow$ . But that is not true, a counterexample to this would be $a\in\mathrm{A}$ and $b\in\mathrm{D}$ which is an element of $(\mathrm{A}\cup\mathrm{C}) \times (\mathrm{B}\cup\mathrm{D})$ but not an element of $(\mathrm{A}\times\mathrm{B})\cup(\mathrm{C}\times\mathrm{D})$ . With that being said, how can this subset proof be done correctly ? I asked my teacher about it and she said that she was right, and I didn't want to go further with it because I had to get to my next class and hadn't thought about it since last week. Or am I making a dumb mistake? Thanks for the help.","['elementary-set-theory', 'proof-writing', 'logic']"
4399099,Continuous and bounded function on the unit sphere of a Banach space may not attain its maximum.,"Give an example of a Banach space and a continuous function $f$ on it such that $f$ is bounded on the unit sphere $S$ but $f$ cannot attain its maximum on S, i.e., $f(S)$ is not closed. Obviously we need an infinite dimensional Banach space, but I have no idea about the approciate function. I have not found post on MSE which answers my question. Appreciate any help or hint!","['banach-spaces', 'functional-analysis', 'compactness']"
4399139,local expresssion of the Hessian operator,"I was reading Lee's IRM book, in page 328,there is a local expression for Hessian operator that I can't work it out. Which shows in the normal coordinate (geodesics coordinate) the Hessian operator for distance function inside this neiborhood has the form: $$\mathscr{H}_{r}=g^{i j}\left(\partial_{j} \partial_{k} r-\Gamma_{j k}^{m} \partial_{m} r\right) \partial_{i} \otimes d x^{k}$$ I try to prove it as follows but fails, first we can assume $\mathscr{H}_r = \omega^i_k \partial_i\otimes dx^k$ , then $\mathscr{H}_r(\partial_k) = \omega_k^i \partial_i$ , so $$\omega^i_k g_{ij} = g(\omega_k^i\partial_i, \partial_j)=g(\mathscr{H}_r(\partial_k),\partial_j) = Hess\ r(\partial_k,\partial_j) = g(\nabla_{\partial_k}\nabla r,\partial_j) = g(\nabla_{\partial_k}((\partial^mr) \partial_m)),\partial_j) \tag{*}$$ where $\partial^mr$ denote the $m$ -th component of the $\nabla r$ , Now we need to expand the last term using product rule: $$g(\partial_k(\partial^mr)\partial_m,\partial_j) + g(\partial^mr\nabla_{\partial_k}\partial_m, \partial_j) = \partial_k(\partial^mr)g(\partial_m,\partial_j) + \partial^mr\Gamma^{l}_{km}g(\partial_l,\partial_j) = \partial_k(\partial^mr)g_{mj}+\partial^mr\Gamma^{l}_{km} g_{lj}.$$ So $$\omega^i_k  g_{ij}= \partial_k(\partial^mr)g_{mj}+\partial^mr\Gamma^{l}_{km} g_{lj}$$ So $$\omega^i_k = g^{ij}(\partial_k(\partial^mr)g_{mj}+\partial^mr\Gamma^{l}_{km} g_{lj})$$ Not the desired expression $$\omega^i_k=g^{i j}\left(\partial_{j} \partial_{k} r-\Gamma_{j k}^{m} \partial_{m} r\right)$$ I have tried many times to prove this identity, I am not sure where went wrong? To get minus sign in the second expression , it seems more reasonable to expand the last terms in (*) using metric compatibility condition. But Still seems not clear, the main problem is $g^{ij}$ and $g_{nm}$ in the expression will cancel out.","['riemannian-geometry', 'differential-geometry']"
4399164,Poincaré's Theorem using Schwarz Lemma in $\mathbb C^n$,"I have been asked to prove Poincaré's theorem using Schwarz Lemma. The statements are as follows: Schwarz Lemma. Let $B^n_1(\mathbf{0}) := \{(z_1, \ldots, z_n)\in \Bbb C^n: 
|z_1|^2 + \ldots + |z_n|^2 < 1\}$ and $f: B^n_1(\mathbf{0}) \to \Bbb C$ be holomorphic with $f(\mathbf{0}) = 0$ . Let $|f(z)| \le M$ for all $z\in B^n_1(\mathbf{0})$ for some $M > 0$ , i.e. $f$ is bounded. Then, $|f(z)| \le M\|z\|$ for all $z\in B^n_1(\mathbf{0})$ and $\|f'(\mathbf{0})\|\le M$ . With $B^n_1(\mathbf{0})$ (the unit ball in $\Bbb C^n$ ) as above, let $P^n_1(\mathbf{0}):= \{(z_1, \ldots, z_n)\in \Bbb C^n: |z_i| < 1 \text{ for all }1\le i\le n\}$ the unit polydisc in $\Bbb C^n$ . Poincaré's Theorem. If $n \ge 2$ , there does not exist a biholomorphism between $B^n_1(\mathbf{0})$ and $P^n_1(\mathbf{0})$ . My work. Suppose, for a contradiction, that $n\ge 2$ and there exists a biholomorphism $f: B^n_1(\mathbf{0}) \to P^n_1(\mathbf{0})$ , that is, $f,f^{-1}$ are holomorphic and bijective. Let $f = (f_1, f_2, \ldots, f_n)$ be the coordinate-wise representation of $f$ . Also, denote the derivative map of $f$ at $\mathbf{0}$ by $f'(\mathbf{0}): \Bbb C^n\to \Bbb C^n$ . Without loss of generality, we can assume $f(\mathbf{0}) = \mathbf{0}$ . If $f(\mathbf{0}) \ne \mathbf{0}$ , then we can consider $\widetilde{f} = g \circ f: B^n_1(\mathbf{0}) \to P^n_1(\mathbf{0})$ where $g$ is an automorphism of the polydisc, sending $f(\mathbf{0})$ to $\mathbf{0}$ . Using Schwarz Lemma as stated above, I have proved that if $f(\mathbf{0}) = \mathbf{0}$ then $f'(\mathbf{0})$ maps $B^n_1(\mathbf{0})$ into $P^n_1(\mathbf{0})$ . If we can show (how?) that the image of $B^n_1(\mathbf{0})$ under $f'(\mathbf{0})$ is $P^n_1(\mathbf{0})$ , then, $f'(\mathbf{0})$ maps $\partial B^n_1(\mathbf{0})$ into $\partial P^n_1(\mathbf{0})$ . Also, $f'(\mathbf{0})$ is invertible (use chain rule). Knowing that linear maps send spheres to ellipsoids, $f'(\mathbf{0})(\partial B^n_1(\mathbf{0})) \subset \partial P^n_1(\mathbf{0})$ is a contradiction. Question. Could I get some help with showing that $f'(\mathbf{0})$ maps $B^n_1(\mathbf{0})$ onto $P^n_1(\mathbf{0})$ ? If this is not true, how else can I prove Poincaré's Theorem using Schwarz Lemma? Thanks!","['complex-analysis', 'alternative-proof', 'several-complex-variables', 'analysis']"
4399182,Question about counting measure.,"A set function $c$ defined on all of $\mathbb R$ , is defined as follows. Define $c(E)$ to be infinity if $E$ has infinitely many members and $c(E)$ to be equal to the number of elements in $E$ if $E$ is finite; define $c(\varnothing)=0$ . Show that $c$ is a countably additive and translation invariant set function. This set function is called the counting measure. To prove this question, I must prove: Countably Additive for two cases: infinite and finite. Translation Invariant for two cases: infinite and finite. Let $E$ is infinite set. Countably Additive: We must prove: \begin{align}
c\left( \bigcup\limits_{n=1}^\infty E_n\right)=\sum\limits_{n=1}^\infty E_n
\end{align} Now I try to prove \begin{align}
c\left( \bigcup\limits_{n=1}^\infty E_n\right) = c(E_1\cup E_2\cup \ldots)=\infty=\infty+\infty+\ldots=c(E_1)+c(E_2)+\ldots = \sum\limits_{n=1}^\infty E_n
\end{align} Translation Invariant, we must prove:
If $E\in \mathcal{P}(\mathbb R)$ and $E+x=\{x+y\mid y\in E\}$ then $c(E)=c(E+x)$ . Since $E$ is infinity then $c(E)=\infty$ . Since $E$ is infinity, the set $E+x$ for all $x\in \mathbb R$ is also infinite set. Then, $c(E+x)=\infty$ . So, $c(E)=c(E+x)$ . For the same way, finite case is similar. I'm not sure with my effort. Is the prove above is true?",['measure-theory']
4399184,Generalized Ladyzhenskaya inequality for $n=2$.,"Is there a way to prove the following generalized Ladyzhenskaya inequality for $n=2$ ? The statement goes as follows: Let $n = 2$ and $0 < p < \infty$ . Then, for all $f \in C^1(\mathbb{R}^2)$ , we have $$\|f\|^{p+2}_{L^{p+2}} \leq C \|f\|_{L^2}^{2} \|\nabla f\|_{L^2}^p$$ with the constant $C$ uniform in $f$ . We know that for $p = 2$ , this reduces to the usual Ladyzhenskaya inequality: $$\|f\|_{L^4}^2 \leq \|u\|_{L^2} \|\nabla f\|_{L^2} $$ and a proof of this can be readily found online, as such in this blog post . However, I find it hard to generalize this proof for $L^{p+2}$ . Although this might follow from the general class of Gagliardo–Nirenberg interpolation inequalities, I was thinking if there is a somewhat elementary way of doing this. Any help would be appreciated!","['inequality', 'functional-analysis', 'partial-differential-equations']"
4399212,Comparison between $L^1$ Wasserstein distance and total variation distance,"Wasserstein $-k$ distance between two probability measures $\mu,\nu$ on $\mathbb{R}^d$ is defined as: $$W_k(\mu,\nu)=\left(\inf_{(X,Y)\in\mathcal{C}(\mu,\nu)}\mathbb{E}\left[\|X-Y \|^k \right]\right)^{1/k}$$ where $\mathcal{C}(\mu,\nu)$ is the set of all couplings of $\mu,\nu$ . Total variation distance between the same probability measures is defined as: $$\|\mu-\nu\|_{\rm{TV}}=\max_{A\subset \mathbb{R}^d}|\mu(A)-\nu(A)|=\inf \{\mathbb{P}(X\neq Y): (X,Y)\,\text{is a coupling of $\mu$ and $\nu$.}\}$$ where the $\inf$ is again over all couplings of $\mu,\nu$ . I am trying to check if the total variation distance is smaller than Wasserstein- $1$ distance for any two probability measures. $$\|\mu-\nu\|_{\rm{TV}}\leq \mathbb{P}(X\neq Y)$$ where $(X,Y)$ is any coupling and then I was trying to apply Markov's inequality but did not succeed. Any ideas?","['optimal-transport', 'probability-theory', 'real-analysis']"
4399226,Why isn't $\sqrt{64x^4y^8z^6}$ equal to $8x^2y^4z^3$?,"I simplified $$\sqrt{64x^4y^8z^6}$$ by taking the square root of $64$ (getting $8$ ), $x^4$ (getting $x^2$ ), $y^8$ (getting $y^4$ ), and $z^6$ (getting $z^3$ ). My answer, $8x^2y^4z^3$ , not quite right and I am having a hard time understanding why not. I even placed the problem into WolframAlpha.com (here) and got the same answer, which was really confusing. Any pointers of what I might be doing wrong?","['algebra-precalculus', 'radicals']"
4399235,cohomology of a symmetric group,"Let $\mathbb{F}_3$ be the sign representation of $S_3.$ What is $H^i (S_3, \mathbb{F}_3)?$ I think it is $\mathbb{F}_3$ in all degrees but I am not sure in my calculations. Namely, I used Serre-Leray spectral sequence and reduced it to $H^i (\mathbb{Z}/3, \mathbb{F}/3)^{\mathbb{Z}/2}$ where the action is trivial and it seems to me that $\mathbb{Z}/2$ doesn't act on these cohomology groups. Any references are appreciated.","['group-theory', 'homology-cohomology', 'representation-theory']"
4399292,Mathematical Induction validity,i was going through Discrete mathematics by Rosen and stuck on below paragraph. I fail to understand the line that is highlighted. How is P(m-1) -> P(m) is true.,"['induction', 'discrete-mathematics']"
4399311,"Can someone explain ""infinitely often"" by simple example?","I am trying to understand the Borel-Cantelli lemma, however, I cannot understand what exactly is ""infinitely often"". Can someone please explain ""infinitely often"" by a simple example?","['measure-theory', 'probability-theory']"
4399330,"If $f$ is uniformly continuous and $\int_\mathbb{R}|f(x)x^a|dx<\infty$ then $|f(x)x^b|$ is bounded for some $b\in(0,a]$","It is known that if $f$ is a continuous function on $\mathbb{R}$ and $\int_\mathbb{R}|f(x)|dx<\infty$ then $f$ is not necessarily bounded. However, Barbalat’s Lemma tells that if $f$ is uniformly continuous and integrable then $f$ is not only bounded but also vanishes at infinity. Now suppose that $f$ is uniformly continuous and bounded on $\mathbb{R}$ and $\int_\mathbb{R}|f(x)x^a|dx<\infty$ for some $a>0$ . Can we say that $|f(x)x^b|$ is bounded for some $b\in(0,a]$ ?","['integration', 'real-analysis']"
4399344,Baby Rudin theorem 10.33 ( STOKES' THEOREM),"The definitions which we need for the proof of the theorem.
We define the standard simplex $Q^k$ to be the set of all $u$ $\in$ $R^k$ of the form $u$ = $\sum_{i=1}^k$ $\alpha_i$ $e_i$ . Assume now that $p_0$ , $p_1$ ,... $p_k$ are points of $R^n$ . The oriented affine $k$ -simplex $\sigma$ $=$ [ $p_0$ , $p_1$ ,... $p_k$ ] is defined to be the $k$ -surface in $R^n$ with parameter domain $Q^k$ which is given by the affine mapping $\sigma$ ( $\sum_{i=1}^k$ $\alpha_i$ $e_i$ ) $=$ $\sigma(u)$ $=p_0$ + $\sum_{i=1}^k$ $\alpha_i$ ( $p_i$ - $p_0$ ).
Note that $\sigma$ is characterized by $\sigma(0)$ = $p_0$ , $\sigma(e_i)$ = $p_i$ (for $1$ $\leq$ $i$ $\leq$ $k$ ). For $k$ $\geq$ $1$ , the boundary of the oriented affine $k$ -simplex $\sigma$ $=$ [ $p_0$ , $p_1$ ,... $p_k$ ]
is defined to be the affine ( $k-1$ )-chain $\partial$$\sigma$ = $\sum_{j=0}^k$ $(-1)^j$ [ $p_0$ ,..., $p_{j-1}$ , $p_{j+1}$ ,.., $p_k$ ].
For $1$ $\leq$ $j$ $\leq$ $k$ , observe that the simplex $\sigma_j$ = [ $p_0$ ,..., $p_{j-1}$ , $p_{j+1}$ ,.., $p_k$ ] has $Q^{k-1}$ as its parameter domain and that is defined by $\sigma_j(u)$ = $p_0$ + $Bu$ ( $u$ $\in$ $Q^{k-1}$ ) The class $\mathscr C'$ means the class of  continuously differentiable functions and etc. $x_j$ = ${ \begin{cases} {u_j (1 \leq j \lt r),} \\ {1 - (u_1 + ... + u_{k-1})  (j=r),} \\ {u_{j-1}  (r \lt j  \leq k). }  \end{cases} } $ ( this is $(98)$ ). $x_j$ = ${ \begin{cases} {u_j (1 \leq j \lt i),} \\ { 0  (j=i),} \\ {u_{j-1}  (i \lt j  \leq k). }  \end{cases} } $ . ( this is $(99)$ ). I dont'understand how do we get the $(98)$ and $(99)$ . Any help would be appreciated.","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'functional-analysis']"
4399390,Proof that there is a point inside an acute-angled triangle so that perpendiculars dropped to the sides will form vertices of equilateral triangle,"There is a character limit to the title length and the task may not be clear. So here is a little bit longer task description. Task description: You are given an acute-angled triangle $ABC$ . Proof that there is a
point $O$ inside $ABC$ so if you drop perpendiculars $OD,
OE, OF$ to the sides: $$D \in AB,\; E \in BC,\; F \in AC,\; OD \bot AB,\; OE \bot
BC,\; OF \bot AC$$ then $DEF$ is an equilateral triangle. Questions I found useful: Inscribe an equilateral triangle inside a triangle Three lines are given. Find three points on these lines, one point on each line, that are vertices of an equilateral triangle Current idea: Choose $D' \in AB$ Inscribe an equilateral triangle $D'E'F'$ inside $ABC$ (Problem here is that sometimes $E'$ and $F'$ are outside of $BC$ and $AC$ but let's assume they are inside) Draw perpendiculars to $AB$ through $D'$ , to $BC$ through $E'$ , to $AC$ through $F'$ Let's say perpendiculars through $E'$ and $F'$ intersect at $O'$ It seems that if we move $D'$ from $A$ to $B$ then the $O'$ will be to the $B$ side from perpenducilar through $D'$ at the beginning and to the $A$ side at the end (I can't proof this). So there will be the moment when perpendicular through $D'$ will go through $O'$ . At this point $O'$ seems to be $O$ we are searching for but I can't proof that $O'$ will be inside a triangle $ABC$ . Will be pleased for any help (ideas/links to similar questions) UPDATE: Found point $O$ in Encyclopedia of Triangle Centers ( https://faculty.evansville.edu/ck6/encyclopedia/ETC.html ): X(15) = 1st ISODYNAMIC POINT The pedal triangle of X(15) is equilateral. UPDATE 2: Here is how to find $O$ with straightedge and compass: https://mathworld.wolfram.com/IsodynamicPoints.html","['problem-solving', 'geometry', 'geometric-construction']"
4399410,Projective tensor product $\ell^2 \hat{\otimes}_\pi \ell^2$,"In Ryan Introduction to Tensor Products of Banach spaces , Example 2.10, it is shown that the space $\ell^2 \hat{\otimes}_\pi \ell^2$ , where $\hat{\otimes}_\pi$ denotes the projective tensor product and $\ell^2$ the usual space of square-summable sequences, contains $\ell^1$ as a subspace, and that $\ell^1$ is complemented in $\ell^2 \hat{\otimes}_\pi \ell^2$ , that is, a subspace $M$ exists such that $\ell^2 \hat{\otimes}_\pi \ell^2 = \ell^1 \oplus M$ . Is anything known about this space $M$ ? I'd be also intersted if there are other references that discuss the space $\ell^2 \hat{\otimes}_\pi \ell^2$ in more detail.","['tensor-products', 'banach-spaces', 'functional-analysis']"
4399415,Does this define cos and sin in the real line uniquely?,"I was wondering if the following defines the functions $C$ and $S$ uniquely: There is a positive number $p$ such that $$\gamma (x)=(C(x),S(x))$$ is a path with period $2p$ that bijects $[0,2p)$ onto the unit circle in $\bf{R} ^2$ , $\gamma (0)=(1,0)$ , $\gamma (\frac{p}{2})=(0,1)$ , and for all $x\in [0,2p)$ the length of $\gamma$ from $0$ to $x$ is $x$ . Here ‘path’ simply means continuous map and the length from $a$ to $b$ is $$\sup{\sum_{i}{\lvert \gamma(x_{i})-\gamma(x_{i-1})\rvert}}$$ the sup being taken over all $x_0,\dotsc,x_n$ such that $a=x_0\le \dotsb\le x_n=b$ . In other words, I am wondering if there are real valued functions $C$ and $S$ other than cosine and sine having these properties. The reason I'm interested in this is because it would define cosine, sine and $\pi$ somewhat ‘geometrically’ rather than the usual definitions involving power series.","['trigonometry', 'real-analysis']"
4399438,An example of a non-trivial martingale with constant second moment?,"Let $(M_t)$ is a martingale for $t \in [0,1]$ . Then, we immediately have from the martingale property that $E[M_0]=E[M_t]$ for all $t \in [0,1]$ and $E[M_s^2]\leq E[M_t^2]$ for all $s\leq t$ in $[0,1]$ . When do we have $E[M_s^2]= E[M_t^2]$ for all $s\leq t$ in $[0,1]$ ? I feel that $(M_t)$ must be constant a.s.. Are there martingales that are not constants that satisfy $E[M_s^2]= E[M_t^2]$ for all $s\leq t$ in $[0,1]$ ?","['stochastic-processes', 'probability-theory', 'martingales']"
4399440,Maximum sum after some elements are deleted from the set,"Let $n$ be positive integer and set $A=\{1,2,...,2n-1\}$ . Alice deletes at least $n-1$ integers from the set $A$ , such that : For every $a\in A$ and $2a\in A$ if $a$ is deleted, $2a$ is also deleted. For every $a,b\in A$ and $a+b\in A$ if $a,b$ are deleted, $a+b$ is also deleted. Find maximum sum of the elements in a set $A$ after Alice's operations. I think the answer is $n^2$ .The set $\{1,3,...,2n-1\}$ satisfies given conditions. But how can I prove that there isn't a better set?","['contest-math', 'combinatorics']"
4399459,Looking for a permutation with disjoint image in a generated subgroup,"Let $n,m$ be integers with $m\geq 2n, n \geq 1$ , and $A=\lbrace 1,2,\ldots, n \rbrace$ . Let $S_m$ be the symmetric group on $[|1..m|]$ . For every $k\in[|1,n|]$ , take a permutation $\sigma_k\in S_m$ satisfying $\sigma_k(k)=k+n$ , and consider the subgroup $G$ of $S_m$ generated by $\sigma_1,\ldots,\sigma_n$ . Question: Must $G$ always contain a permutation $\sigma$ such that $\sigma A \cap A = \emptyset$ ? My thoughts : I can show that the answer is yes for $n=2$ . Suppose by contradiction that $\sigma A\cap A \neq \emptyset$ for every $\sigma \in G$ . I will denote $\sigma_1$ by $a$ and $\sigma_2$ by $b$ , so that $a(1)=3$ and $b(2)=4$ . From $aA\cap A \neq \emptyset$ , we deduce $a(2)\in\lbrace 1,2 \rbrace$ , and similarly $bA\cap A \neq \emptyset$ forces $b(1)\in\lbrace 1,2 \rbrace$ . Suppose first that $a(2)=1$ . Then $a^2(2)=3$ , so from $a^2A\cap A \neq \emptyset$ we deduce $a^2(1)\in\lbrace 1,2\rbrace$ , or $a(3)\in\lbrace 1,2\rbrace$ , which forces $a(3)=2$ . We then have $ab(2)=a(4)\not\in \lbrace 1,2\rbrace$ , so from $abA\cap A \neq \emptyset$ we deduce $ab(1)\in\lbrace 1,2\rbrace$ , and hence $b(1) \in a^{-1}\lbrace 1,2\rbrace=\lbrace 2,3\rbrace$ . But we already know that $b(1)\in\lbrace 1,2 \rbrace$ , so $b(1)=2$ . Next, we have $ba^{-1}(1)=4$ , so $ba^{-1}(2)\in \lbrace 1,2\rbrace$ , and hence $b(3)=1$ . But then $b^2A=\lbrace 4,b(4)\rbrace$ is disjoint from $A$ which is excluded. We are therefore done with this $a(2)=1$ case. We may therefore assume $a(2)=2$ , and by symmetry we may also assume $b(1)=1$ . Then $ab(1)=3$ , so $a(4)=ab(2)\in \lbrace 1,2\rbrace$ and hence $a(4)=1$ . Similarly, $ba(2)=4$ , so $b(3)=ba(1)\in \lbrace 1,2\rbrace$ and hence $b(3)=2$ . But then $ba^{-1}A=\lbrace 4,b(4)\rbrace$ is disjoint from $A$ which is absurd. This finishes the proof.","['permutations', 'combinatorics', 'combinatorial-group-theory']"
4399467,Bounding spectral radius of special matrix,"Let $A$ be an $n \times n$ matrix with all nonnegative entries and row sums strictly
less than one, let $V$ be an $n \times n$ nonnegative diagonal matrix satisfying $V \leq I$ (entrywise), let $B\equiv\left(I-AV\right)^{-1}$ and finally let $X$ be a vector in the $n$ -dimensional simplex, i.e., $x_j \geq 0,\sum_j^n x_j=1$ . Consider the matrix $$M \equiv \left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[ V\mathrm{diag}\left\{ X\right\} + (I-V) \mathrm{diag}\left\{ B^{T}X\right\} \right]B\mathrm{diag}(\iota-A \iota),$$ where $\mathrm{diag}\left(u\right)$ is the diagonal matrix formed from vector $u$ and $\iota$ is the vector of all ones.
I want to show that the spectral radius of $M$ is (weakly) lower than one, $\rho(M)\leq 1$ . Two simple cases are illustrative. First, if $V = I $ then $M\iota = \iota$ and so $\rho(M)=1$ . Second, if $A$ is diagonal then $M$ would be diagonal and so we would just need to show that each diagonal element is lower than one. But each of diagonal element of $M$ would be of the form $$\left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av},$$ which is readily shown to be lower than one. The problem above, namely showing that $\rho(M)\leq 1$ , comes from a more general problem, which I ultimately need to solve. Let $D_1$ and $D_2$ be two strictly positive diagonal $n \times n$ matrices and let $$\tilde{M}\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right]D_{1}B\mathrm{diag}\left(\iota-A\iota\right)D_{2}.$$ I want to show that $\rho(\tilde{M})\leq 1$ provided that $$ \tag{*} D_{1}\left(I-A\right)^{-1}\mathrm{diag}\left(\iota-A\iota\right)D_{2}\iota\leq\iota.$$ This is now posted as a separate question here: Bounding spectral radius of special matrix (extension) The simpler question stated above obtains from this more general question in the special case in which $D_{k}=d_{k}I,k=1,2$ with $d_1,d_2$ being positive scalars. In that case $$\tilde{M}=\left(\mathcal{\mathrm{diag}}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathcal{\mathrm{diag}}\left\{ X\right\} +\left(I-V\right)\mathcal{\mathrm{diag}}\left\{ B^{T}X\right\} \right]B\mathrm{diag}\left(\iota-A\iota\right)d_{1}d_{2},$$ while condition (*) simply becomes $d_{1}d_{2} \leq 1$ , and so we can simply prove that $\rho(M)\leq 1$ .","['matrices', 'spectral-radius', 'linear-algebra', 'upper-lower-bounds', 'inequality']"
4399504,Conditions for $f\left(\sqrt{x^2+y^2}\right)$ to be $C^p$,"Let $f : \mathbb{R} \to \mathbb{R}$ be a $C^p$ function. I'm looking for a necessary and sufficient condition for the function $$(x,y) \in \mathbb{R}^2 \mapsto f\left(\sqrt{x^2+y^2}\right)$$ to be $C^p$ as well. Now, obviously, it is $C^p$ on $\mathbb{R}^2\setminus \{(0,0)\}$ so the unique problem is at $(0,0)$ . I have used some brute force and compute partial derivatives up to order 3. It seems that a sufficient condition is the cancelation of all the derivatives of $f$ at $0$ . However it is just a conjecture and, moreover, it would only be a sufficient condition not a necessary one. Any help or thought on this subject will be much appreciated.","['partial-derivative', 'multivariable-calculus', 'derivatives', 'real-analysis']"
4399540,Isometries of n-dimensional hyperbolic space,"I'm aware that the isometries of hyperbolic 2 dimensional and 3 dimensional space are given by elements of $SL(2,\mathbb{R})$ and $SL(2,\mathbb{C})$ respectively and are then categorised according to rules on the trace of the element, or equivalently the number of fixed points of the element. My question is whether there exists a form of isometries of n dimensional hyperbolic space and a similar way to make sense of this in terms of a function and number of fixed points (or planes etc.)?","['linear-fractional-transformation', 'abstract-algebra', 'hyperbolic-geometry', 'geometry']"
4399557,"Proving $\int_0^1\int_0^1\chi_Sdxdy=0,$ while $\int_S\chi_S$ doesn't exist.","Let $S=\{(x,y)\in\Bbb R^2\mid x\in\Bbb Q,0<x<1,\text{ and if } x=\frac{p}m,\gcd(p,m)=1,y=\frac{k}m,k=1,\ldots,m-1\}.\tag 1$ Prove that $\displaystyle \int_0^1\int_0^1\chi_S dxdy=0$ while $\displaystyle \int_S\chi_S$ doesn't exist. My thoughts: I tried to write down $(1)$ differently, so I fixed some $y\in\Bbb Q.$ Then $y_0=\frac{p_0}{q_0},\gcd(p_0,q_0)=1.$ Then, for every $n\in\Bbb N$ all the points of the form $$\left(y_0,\frac{p}{q_0n}\right),\gcd(p,q_0n)=1,p<q_0n\tag 2$$ are in $S.$ I tried to show that the set of discontinuities of the function $$x\mapsto\begin{cases}1, &(x,y_0)\in S\\0,&(x,y_0)\notin S\end{cases}$$ is of measure zero and exactly the set $S$ and that the function $$y\mapsto\int_0^1\chi_Sdx$$ is non-zero on the set of the Jordan measure $0$ . I thought I could prove that, for a given $y_0,$ the points in $(2)$ might form a discrete set,which would have a boundary of the Jordan measure zero, but I failed. For the second integral, I thought one might argue that, even though, for some $x_0\in\Bbb Q\cap(0,1)$ there are only finitely many points $(x_0,y)\in S,$ since $\Bbb Q\cap(0,1)$ is dense in $[0,1],$ if we take a good enough subdivision of $[0,1]\times[0,1],$ letting $m\to\infty,$ there are infinitely many points from $S$ in a rectangle of the subdivision so the lower Darboux sum would equal $0,$ while the upper would be $1$ and therefore, $\displaystyle\int_{[0,1]^2}\chi_S$ wouldn't exist, and neither would the given integral. I'm not sure if I'm on the right track at all and what I've written seems messy even to me. Does anybody have any advice/hint on what to do?","['integration', 'multivariable-calculus', 'proof-writing']"
4399559,Incorrect proof: Number of edges in the unique giant component in an uniform random graph,"I am working my way through a Random Graphs using the book ""Introduction to Random Graphs"" by Frieze and Karonski (which is available here: https://www.math.cmu.edu/~af1p/BOOK.pdf ) I have however stumbled into a proof which seems incorrect to me: Theorem 2.14 and proof on pages 34-38 which among other things states: asymptotically the number of edges in the unique giant component in an uniform random
graph with $n$ vertices and $m=m(n)$ edges is $(1-(x/c)^2)m$ where $c$ and $x$ are constants. The relevant part of the proof starts on line 12 on page 37: The idea is to calculate the asymptotic expected number of edges in the giant $C_0$ and then show the the random variable corresponding to this concentrates. Mathematically, we enumarte all the edges $e_1,\dots,e_m$ and then define: $$
X = \sum_{j=1}^m 1_{\{e_j \in C_0 \}}. 
$$ Then I arrive, just as in the book, that $$
E[X] \approx m(1-(\frac{x}{c})^2)
$$ where $\approx$ denotes that these two quantities are asymptocally equivalent (that their ratio converges to 1). Now as the book mentions, we want to show concentration using Chebyschevs Inequality: $$
P(\vert X - E[X] \vert > t) \leq \frac{V[X]}{t^2}
$$ So I presume this boils down to showing that $V[X] \rightarrow 0$ for $n \to \infty$ (number of vertices). They compute that for $i\neq j$ $$
P(e_i \in C_0, e_j \in C_0)=(1+o(1))P(e_i \in C_0)P(e_j \in C_0)
$$ where $o(1)$ denotes a sequence converging to 0. Hence as I see it, we get that \begin{align*}
V[X] &= E[X^2]-E[X]^2 \\
&= \sum_{j=1}^m \sum_{i=1}^m P(e_i \in C_0, e_j \in C_0) - E[X]^2 \\
& =\underbrace{ \sum_{i=1}^m P(e_i \in C_0)}_{= E[X]} - \sum_{i\neq j} P(e_i \in C_0, e_j \in C_0) -  E[X]^2 \\
& \approx m(1-(\frac{x}{c})^2)- (m^2-m)(1-(\frac{x}{c})^2)^2  - m^2(1-(\frac{x}{c})^2)^2 \\
& = m(1-(\frac{x}{c})^2)-m(1-(\frac{x}{c})^2)^2 \rightarrow \infty
\end{align*} as $m(n) \rightarrow \infty$ . This is then a too crude estimate on the concentration and one would need something else. Am I doing something wrong or is the proof in the book wrong? Does anyone know if the result is even true and know of another reference/proof? Any clarification would be much appreciated. Note: $1-(x/c)^2$ is a real number strictly between 0 and 1. Update: Is it possible the joint probabilities $P(e_i \in C_0, e_j \in C_0)$ are not the same for all terms in the sum?","['random-graphs', 'solution-verification', 'combinatorics', 'probability-theory']"
4399560,Does the set $\emptyset^\emptyset$ exist?,"If $A$ and $B$ are two set then $B^A$ is the set of all function from $A$ to $B$ , that is $$
B^A:=\{f\in\mathcal P(A\times B):f\,\,\text{function}\}
$$ Now if $A$ and $B$ are finite it is not hard to show that the cardinal $|B^A|$ of $B^A$ is the cardinal $|B|$ raised to the power of the cardinal of $|A|$ , that is $$
|B^A|=|B|^{|A|}
$$ So if $B$ was empty then the set $A\times B$ would be empty so that the set $\mathcal P(A\times B)$ is a singleton set containing the empty set $\emptyset$ as element and thus in this case the set $B^A$ is a singleton set containing the empty function as element and this is consistent with respect the identity $$
n^0=1
$$ for any $n>0$ . However if $A$ and $B$ was   simultaneously empty then due to the same argumentations $B^A$ is a singleton set containing the empty function but this, apparently, is absurd because in ordinary arithmetic the exponentiation $0^0$ is not defined.
In any case I know that if $g_m:\Bbb N\times\Bbb N\rightarrow\Bbb N$ is for any $m\in\Bbb N$ the function defined as $$
g_m(x,y):=x\cdot m
$$ for any $x,y\in\Bbb N$ then the recursion theorem guarantees the existence of a function $f_m:\Bbb N\rightarrow\Bbb N$ such that $$
f_m(0)=1\\
f_m(n+1)=g_m\big(f_m(n),n\big)\,\,\forall n\in\Bbb N
$$ and is not hard to show that this function is exactly such that $$
f_m(n)=m^n
$$ for any $n\in\Bbb N$ so that apparently it is possible to define $0^0$ because we did not establish any constraint on the values of $m$ . So could someone explain if the set $\emptyset^\emptyset$ exist, please?","['exponentiation', 'definition', 'solution-verification', 'elementary-set-theory', 'arithmetic']"
4399574,Commutativity of $\nabla$ and $\Delta$,"Let $M$ be a (closed if necessary) Riemannian manifold with Levi-Civita connection Let $\nabla^*$ be the formal adjoint of $\nabla$ with respect to the $L^2$ inner product. Let $\Delta=\nabla^*\nabla$ denote the Laplacian. Question 1: In general, do we have $\nabla\circ\Delta$ = $\Delta\circ\nabla$ , as operators $C^\infty(M)\to C^\infty(M)\otimes C^\infty(T^*M)$ ? Question 2: If not, is this relation true if the metric is flat? Comment: I feel that the answer to Q2 at least must be yes, but I am not good with these computations. So I would appreciate it if someone could work through the computation (or perhaps share a reference) for the commutator $[\nabla,\Delta]$ involving the curvature terms.","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4399583,"Name of the functional $p \mapsto P_X[p] = X\int_X^\infty p(x)\,dx$","Let $p(x): \mathbb{R} \rightarrow \mathbb{R}^+_0$ be a probability distribution with $\int_{-\infty}^\infty p(x)\,dx = 1$ . Is there a special name for the parametrized functional $$p \mapsto P_X[p] = X\int_X^\infty p(x)\,dx\ ?$$ The name of $p \mapsto I_X[p] = \int_X^\infty p(x)\,dx$ is just the definite integral of $p$ from $X$ to infinity and $p \mapsto Id_X[p] = X$ possibly doesn't have a name due to its triviality. But what about the product $X\int_X^\infty p(x)\,dx$ ? Whether there is a special name or not: Which use cases are there for this functional, e.g. in physics, economics, or statistics? I came across it when I tried to calculate the expected revenue when selling a whole stock of products for a given price $X$ per item with $p(x)$ the distribution of willingness to pay . Note the difference with the expected value $$p \mapsto \mathbb{E}[p] = \int_{-\infty}^\infty x\ p(x)\,dx,$$ which is just another functional. To show what the functional may be good for, I plotted it (blue) for a number of distributions $p(x)$ of willingness to pay (green). The maximum is at the price $X$ for which a maximal revenue can be expected. Depicted in red is the reservation price of the supplier ( willingness to accept , set to $1$ ).","['applications', 'expected-value', 'economics', 'functional-analysis', 'terminology']"
4399591,Geodesic Polar Coordinates and Area of a Circle in Hyperbolic Space,"I am trying to calculate the area of a circle of radius $R$ in the hyperbolic plane. I know that the answer is supposed to be $4\pi(\sinh(\frac{R}{2}))^2$ . However, I am not getting this. To calculate the area, I am trying to use hyperbolic polar coordinates. I know that $d\mu(z)=(2\sinh r)drd\phi$ , so if $B_R$ is the ball of radius $R$ , then we have that the area is given by: $$
\iint_{B_R}d\mu(z)=\int_0^\pi\int_0^R(2\sinh r)drd\phi=2\pi(\cosh(R)-1)
$$ However, this does not seem correct to agree with what several books and the internet have told me that the area of a circle in hyperbolic space is. Is it the case that $2\pi(\cosh(R)-1)=4\pi(\sinh(\frac{R}{2}))^2$ or where did I go wrong. As a related aside, I want to similarly calculate the perimeter of a hyperbolic circle, and I similarly wish to use geodesic polar coordinates, and I know that $ds^2=dr^2+(2\sinh r)^2d\phi^2$ , but the squaring of the differentials always confuses me, so I think I should do something along the lines of the perimeter is given by $$
\int_{\partial B_R}ds=\int_{\partial B_R}\sqrt{dr^2+(2\sinh r)^2d\phi^2}
$$ but I'm not entirely sure what I'm supposed to be integrating in this case, and I fear that perhaps I might not get $2\pi \sinh(R)$ which is what the world has seemed to agreed that the perimeter of a hyperbolic circle is. Any and all help is greatly appreciated. Note: In regards to the perimeter calculation, I figured it out. We have that $$
\int_{\partial B_R}ds=\int_0^\pi\sqrt{(2\sinh r)^2+(\frac{dr}{d\phi})^2}d\phi
$$ Now in geodesic polar coordinates, we have that our curve is $r=R$ , so we will get that $\frac{dr}{d\phi}=0$ , and this is just $$
\int_0^\pi 2\sinh(R)=2\pi\sinh(R)
$$","['circles', 'geometry', 'polar-coordinates', 'change-of-variable', 'hyperbolic-geometry']"
4399597,Resolvent of Dirichlet Laplacian via Fourier Transform,"For $-\Delta$ considered as a self-adjoint operator on $L^2(\mathbb{R}^d)$ , one may write it's resolvent as the Fourier multiplier $g(\xi)=(|\xi|^2-z)^{-1}$ . Now let $\Omega\subset \mathbb{R}^d$ be a bounded domain and let $-\Delta^\Omega$ be the Dirichlet Laplacian on $\Omega$ , defined, for instance, via the Friedrichs Extension Theorem. Is there an expression for $(-\Delta^\Omega-z)^{-1}$ in terms of the operator $g(\xi)$ and some multiplication operator $f(x)$ ? If not, can we approximate $(-\Delta^\Omega-z)^{-1}$ by operators of the form $f(x)g(\xi)$ ? It seems that something like $f(x)g(\xi)f(x)$ might work for $f$ some bump function of $\Omega$ , but the fact that $-\Delta^\Omega$ has to be defined abstractly leaves me unsure of the details. My motivation is that I would like to prove that $-\Delta^\Omega$ has compact resolvent from the fact that an operator of the form $f(x)g(\xi)$ is compact if $f$ and $g$ vanish at $\infty$ . Since one usually proves that the resolvent is compact from the Rellich embedding theorem and one can prove this theorem from the above compactness criterion, I have reason to believe this should work.","['laplacian', 'sobolev-spaces', 'functional-analysis', 'greens-function', 'spectral-theory']"
4399608,"Do Carmo Differential Geometry book: Proposition 4, Chapter 2.2. Mistake in the proof?","In Proposition 4 of Chapter 2.2 in Do Carmo's differential geometry book, the author claims that if we already know that a set $S\subseteq \mathbb{R}^3$ is a regular surface, and we have a candidate $\textbf{x}: U \subset \mathbb{R}^2 \rightarrow \mathbb{R}^3 $ for a parametrization, we do not have to check that $\textbf{x}^{-1}$ is continuous, provided that the other conditions (for $\textbf{x}$ being a parametrization) hold. I think there is an error in the proof. I will first state the relevant definition: Definition 1: A subset $S\subset \mathbb{R}^3$ is a regular surface if, for each $p \in S$ , there exists an open set $V$ in $\mathbb{R}^3$ , and a map $\textbf{x}: U \rightarrow V \cap S$ of an open set $U \subseteq \mathbb{R}^2$ onto $V \cap S$ such that $\textbf{x}$ is infinitely differentiable. $\textbf{x}$ is homeomorphism. For each $q \in U$ , the differential $d\textbf{x}_q : \mathbb{R}^2 \rightarrow \mathbb{R}^3$ is one-to-one. Proposition 4, Chapter 2.2: First, I think that at the statement, he should also mention that $U$ is open and that $x(U)=V\cap S$ , for some open set $V\subset \mathbb{R}^3$ . My main issue is that in the proof (attached below), he does not use the fact that $S$ is a regular surface . So, if the proof is correct, then this would imply that Condition 2 in Definition 1 can be replaced by a requirement that $\textbf{x}$ is one-to-one. I think this is not possible to do. Here is the proof in the book: I think that the mistake is this: To argue that $\textbf{x}^{-1}$ is continuous at some point $w\in \textbf{x}(U)$ , it suffices to show that for some $A\ \subseteq S$ that is open in $S$ and contains $w$ , $\textbf{x}^{-1}$ restricted to $A$ , is continuous. In the proof, this set $A$ is $\textbf{x}(V_1)$ , but we don't know that this is open in $S$ . From the inverse function theorem, we only know that the projection of this set in the $xy$ plane (the set $V_2$ in the proof) is open. The issue that I am describing had been raised here , but there hasn't been any clarifying answer.","['surfaces', 'differential-geometry']"
4399610,Pearson Correlation as a measure for non-linear dependence.,"It is known that $\rho$ , the pearson correlation, is a measure for the linear dependence of two random variables say $X$ , $Y$ . But can't you say just transform $X$ and $Y$ such that we have, $$ \rho_{X,Y}(f(X),g(Y))$$ where $f$ , $g$ are non-linear functions such that it measures other kinds of dependce (take for example $f(s)=g(s)=s^2$ for quadratic dependence).","['statistical-inference', 'statistics', 'correlation', 'probability']"
4399638,Find the value of $\displaystyle \int \limits _{0}^{\infty} \dfrac{\mathrm dx}{\sqrt{x^n+a} + \sqrt{x^n+b}}$,"I have a question which asks to find the value of: $\displaystyle \tag*{} \int \limits _{0}^{\infty} \dfrac{\mathrm dx}{\sqrt{x^n+a} + \sqrt{x^n+b}}$ Where, $a,b >0$ and $n >2$ I tried to rationalize the denominator and arrived: $\displaystyle \tag*{} \dfrac{1}{a-b}\int \limits _{0}^{\infty}\sqrt{x^n+a} - \sqrt{x^n+b} \ \ \mathrm dx $ Since individual integrals do not converge. But couldn't proceed any further. Any help would be greatly appreciated. Thank you!","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'discrete-mathematics']"
4399669,What is the tensor power series of a vector?,"In this Wikipedia article https://en.m.wikipedia.org/wiki/Jet_(mathematics) there’s Taylor’s theorem for functions $f: \mathbb{R}^m \to \mathbb{R}^n$ where $f(x)=\sum_{n\in \mathbb{N}} D^n f(x_0) (x-x_0)^{\otimes n} /n!$ . What does this tensor product mean here and where can I learn more about it? If it’s just the normal krownecker/tensor  product, then how do they sum to a vector?","['jet-bundles', 'tensor-products', 'taylor-expansion', 'online-resources', 'derivatives']"
4399722,"Disprove $\dim (A \otimes_k B)= \dim (A)+\dim(B)$ for local noetherian rings $A,B$ with residue field $k$.","Let $A,B$ be local noetherian rings with residue field $k$ . $A,B$ contain the residue field $k$ as a subring. I want to find a counterexample to the statement $$\dim (A \otimes_k B)= \dim (A)+\dim(B).$$ The statement is true when $A, B$ are of finite type. In that case, one can use Noether Normalization to make $A, B$ finite $k[x_1, x_2, \dots, x_n]$ -module and finite $k[y_1, y_2, \dots, y_m]$ -module, respectively. Then the tensor product of polynomial algebras injects into $A \otimes_k B$ , which is further finite over the tensor product. Therefore we conclude, $$\dim(A \otimes_k B) = \operatorname{trdeg}(\operatorname{Frac}(A \otimes_k B)) = n+m =  \dim (A)+\dim(B).$$ However, it is not sure if the case is true for $A, B$ that are not finitely generated. I don’t think this is true, but I cannot think of any easy counter examples. For a simple example, let $A = \mathbb{Z}_p[[x]]$ and $B = \mathbb{Z}_p[[y]]$ be two ring of formal series with finite field $\mathbb{Z}_p$ . Then it is obvious that $A, B$ are local noetherian rings with residue field $\mathbb{Z}_p$ . Note that $$\dim(A\otimes_k B) = \dim(\mathbb{Z}_p[[x,y]]) =2= 1+1= \dim (\mathbb{Z}_p[[x]]) + \dim (\mathbb{Z}_p[[y]])= \dim (A)+ \dim (B).$$","['algebraic-geometry', 'commutative-algebra']"
4399723,Oscillation of the function at each point,"I would like to find the oscillation of the function $f:\Bbb R^2\to\Bbb R,$ $$f(x,y)=\begin{cases}\sin\left(\frac1x\right)+\sin\left(\frac1y\right),&x,y\ne 0\\0, x,=0\text{ or } y=0.\end{cases}$$ This is our definition: Let $A\subset\Bbb R^2$ and $f:A\to\Bbb R$ any function. Oscillation $O(f,c)$ of the function $f$ at the point $c\in A$ is defined as $$O(f,c)=\inf_{U\ni c\\ U\text{ open }}\sup_{x_1,x_2\in U\cap A}|f(x_1)-f(x_2)|.$$ And a lemma: Function $f:A\to\Bbb R$ is continuous at $c\in A$ if and only if $O(f,c)=0.$ My attempt: According to the lemma, since the given function is continuous on $S:=\{(x,y)\in\Bbb R^2\mid xy\ne 0\},O(f,c)=0,\forall c\in S_1.$ Now, suppose $x_0\ne 0$ and $y_0=0.$ For any $\varepsilon>0,$ there is $n\in\Bbb N$ s. t. $\left(x_1,\frac{2\pi}{4n+1}\right),\left(x_2,\frac{2\pi}{4n+3}\right)\in B((x_0,0);\varepsilon)$ for $x_1,x_2$ close enough to $x_0$ since $x\mapsto\sin\left(\frac1x\right)$ is uniformly continuous on segments not containing $0$ . Then $$f\left(x_1,\frac{2\pi}{4n+1}\right)-f\left(x_2,\frac{2n}{4n+3}\right)\to 2,$$ and I think this is the oscillation at such point, but I'm not sure if this is right.
If $x=y=0,$ then we can consider expressions $$f\left(\frac{2\pi}{4n+1}, \frac{2\pi}{4n+1}\right)-f\left(\frac{2\pi}{4n+3}, \frac{2\pi}{4n+3}\right),$$ but $|f(x_1,y_1)-f(x_2,y_2)|\le 4,\forall (x_1,y_1),(x_2,y_2)\in\Bbb R^2,$ so I think this works. Can somebody help me with the case of the axes?","['multivariable-calculus', 'supremum-and-infimum']"
4399744,Finding a particular nth permutation of a number [duplicate],"This question already has an answer here : Find the Kth element of a set with arrangements of digits from 1 to n (1 answer) Closed 2 years ago . Let's suppose we have a number 123, and we want the exact 3rd permutation of this number. The permutations in order would be {123, 132, 213, 231, 312, 321}; so the answer to our question would be 213. But suppose we have a number that is much bigger, like 1234567, we cannot simply list out all the permutations and count to which one we want. Is there any way to MATHEMATICALLY determine what the nth permutation of a number is? I have tried to write some code: def permute(s, answer):
    if (len(s) == 0):
        print(answer, end = ""  "")
        return
     
    for i in range(len(s)):
        ch = s[i]
        left_substr = s[0:i]
        right_substr = s[i + 1:]
        rest = left_substr + right_substr
        permute(rest, answer + ch)

answer = """"
 
s = input(""Enter the string : "")
 
print(""All possible strings are : "")
permute(s, answer) However this only shows me all the basic permutations that the number can have, and it doesn't really help me understand the concept.
I have tried to modify the equations for both combination and permutations but they haven't worked either, so any help or guidance would be appreciated.","['permutations', 'calculus', 'combinatorics', 'discrete-mathematics']"
4399756,Spectral Theorem for Normal Operators - Help understanding proof,"Theorem: Let $(V, \langle\,,\rangle)$ be a complex inner product space and $T: V \to V$ be a normal operator. Then for any eigenvalue $\lambda \in \textrm{spec}(T)$ , there exists an orthogonal projection $P_{\lambda}$ with $\sum_{\lambda \in \textrm{spec}(T)}P_{\lambda}=I$ and $T = \sum_{\lambda \in \textrm{spec}(T)} \lambda P_{\lambda} $ . Proof: Let $\textrm{spec}(T)=\{\lambda_1, ..., \lambda_k\}.$ It has been proved that $V = \bigoplus_{i=1}^kV_{\lambda_i}$ , where $V_{\lambda_i}$ is the eigenspace corresponding to eigenvalue $\lambda_i$ , $i=1, ..., k$ . This implies $\sum_{\lambda \in \textrm{spec}(T)}P_{\lambda}=I$ . Applying $T$ to this relation gives the second relation. Question: How does the decomposition of $V$ into eigenspaces imply the first relation? I can understand how the second is obtained. Could someone help me understand how the decomposition of $V$ into eigenspaces implies the first relation? Thank you for your help.","['inner-products', 'linear-algebra', 'eigenvalues-eigenvectors']"
4399774,Proving $\frac{\pi}{2}=\sum^\infty_{l=0} \frac{(-1)^l}{2l+1}\big(P_{2l}(x)+\text{sgn}(x)P_{2l+1}(x)\big)$,"Can someone help me in proving the following: $$
\frac{\pi}{2}=\sum^\infty_{l=0} \frac{(-1)^l}{2l+1}(P_{2l}(x)+\text{sgn}(x)\cdot P_{2l+1}(x)),
$$ for any value of $x$ , $-1\le x\le 1$ ? (Here $P_l(x)$ is the Legendre polynomial of degree $l$ , and $\text{sgn}(x)$ is the sign function .)  It's the craziest result I've seen (that the left hand side is actually independent of $x$ ), and it's seemingly difficult to prove.  I'm thinking maybe you could prove it using elliptic integrals somehow? Or maybe an easier way? Putting it into the matlab verifies the result (going to high enough $l$ ).","['elliptic-integrals', 'legendre-polynomials', 'bessel-functions', 'sequences-and-series']"
4399825,Possible bayes theorem question regarding a shared car,"The question: Suppose 2 friends share the use of a car evenly, we will call the two friends Roger and James, respectively. We know that Roger will only use the car to drive to the grocery store, on the other hand when James uses the car, 1/3rd of the time he will drive to the grocery store. If we were to see Roger and James car parked outside the grocery store with nobody inside, what are the odds that Roger is inside the grocery store ? Solution:
My logic is to draw a tree diagram and focus only on the possibilities that result in Roger at the grocery store, out of the total that includes both Roger and James, which so happens to be 3/4, can anyone confirm if this is correct ?","['trees', 'statistics', 'discrete-mathematics', 'bayes-theorem', 'probability']"
4399867,why does contour lines intersect at saddle points?,"Consider functions of two variables. A saddle point of $f(x,y)$ is a point in the domain of $f$ (or on the graph of $f$ by an abuse of language) where the gradient is $\bf 0$ , but which is not a local extremum of the function. A classic example is $(0,0)$ for $f(x,y)=y^2-x^2$ . However, there are two types of saddle points. Type I: the graph looks like a saddle. Type II: the graph does not look like a saddle. e.g. $f(x,y)=x^3$ , and $f(x,y)=x^2+y^3$ . I have two questions: Question 1: How to rigorously define saddle points of the above two types? Question 2: A folklore theorem says $P$ is a saddle point of type I if and only if the contour lines (aka level curves) intersect at $P$ . I don't find a proof of such a result, and I'm not sure if the theorem holds as ""if and only if"" or just holds in one direction.","['multivariable-calculus', 'calculus', 'vector-analysis', 'differential-geometry']"
4399880,Bell Polynomials,"The complete Bell polynomials $B_n(x_1, x_2, \ldots, x_n)$ are defined through the relation $$\sum_{n=0}^{\infty} B_n(x_1, x_2, \ldots, x_n) \frac{t^n}{n!} =\exp\Big( \sum_{n=1}^{\infty} x_n \frac{t^n}{n!}\Big).$$ Is there any known formula for the $B_n(x_1, x_2, \ldots, x_n)$ ? I am looking for an expression $$B_n(x_1, x_2, \ldots , x_n)=\sum_I \alpha_I x_{1}^{i_1}x_2^{i_2}\cdots x_n^{i_n},$$ where the sum is taken over partitions of $n$ .","['combinatorics', 'polynomials', 'multinomial-coefficients', 'generating-functions', 'bell-numbers']"
4399886,A generalization of Tietze's extension theorem,"I'm very interested in proving the following theorem because it is a generalization of Tietze's extension theorem. Let $(X,\tau)$ be a normal topological space, $A$ a closed subset of $X$ , $f \in C(A)$ and $P$ and $Q$ equicontinuous non-empty subsets of $C(X)$ . Suppose $p(x) \leq f(x) \leq q(x)$ for each $p\in P$ , $q \in Q$ and $x \in A$ . If $r(x) = \sup\{p(x) \colon p \in P\} \leq \inf\{q(x) \colon q \in Q\} = s(x)$ por all $x \in X$ , then there exists an extension $F \in C(X)$ of $f$ such that $p(x) \leq F(x) \leq q(x)$ for each $x \in X$ and each $p \in P $ and $q \in Q$ . Note: Something that worries me about this theorem is the existence of $r$ and $s$ . My attempt The equicontinuity hypothesis implies that $r,s \in C(X)$ . Let us define $g \colon A \rightarrow \mathbb{R}$ by $g(x) = f(x)-r(x)$ , since $g$ is continuous there is, (by Tietze's extension theorem), a map $G \in C(X)$ such that $G|_A = g$ . Let us now define $h \colon X \rightarrow \mathbb{R}$ given by \begin{equation}
h(x) = \begin{cases} 0 & \text{si} \hspace{0.3cm} G(x) < 0\\ s(x)-r(x) & \text{si} \hspace{0.3cm} s(x) -r(x) < G(x)\\ G(x) &  \text{si} \hspace{0.3cm} 0\leq G(x) \leq s(x)-r(x)\end{cases}
\end{equation} I want to show that the function $F \colon X \rightarrow \mathbb{R}$ given by $F(x) = h(x)+r(x)$ is the desired extension of $f$ . But there are three things that I still haven't been able to do to finish: (1) I must guarantee that $h(x) \in C(X)$ , that is, I must prove that $h(x)$ is continuous on $X$ . How can I do that? I know that the functions $h_1(x) = 0$ , $h_2(x) = s(x)-r(x)$ and $h_3(x) = G(x)$ are continuous in $X$ , but I don't know if it is enough to say that $h $ is continuous on $X$ . (2) I must also show that $F(x) = f(x)$ for all $x \in A$ . If I fix $a \in A$ , then $F(a) = h(a)+r(a)$ and I think $F(a)=f(a)$ if $h(a) = G(a)$ , but this happens if $0 \leq G(a) \leq s(a)-r(a)$ . Why would this happen if $a \in A$ ? (3) Finally, how can I deduce that $p(x) \leq F(x) \leq q(x)$ for all $x \in X$ ? Any help is appreciated Here are some definitions: Definition: $C(X) = \{f \colon X\rightarrow \mathbb{R} \;\vert\; f \hspace{0.2cm}\text{is continuous}\}$ Definition: A subset $S \subset C(X)$ is said to be equicontinuous at $x_0$ if given any $\varepsilon > 0$ there exists a neighborhood $U_{x_0}$ of $x_0$ such that $|f(x) -f(x_0)| < \varepsilon$ for each $x \in U_{x_0}$ and each $f \in S$ . The subset $S$ is equicontinuous if it is equicontinuous at every point of $X$","['continuity', 'general-topology', 'functions']"
4399892,Can the ratio of chord to radius be constant if the chord divides the semi-circle equally?,"Suppose there is a semi-circle. It has a chord parallel to the diameter which divides the area into two parts of equal area. Would the angle made by the chord at the center be constant? Would the ratio of radius and chord be constant? If yes, what would be the value? One of the approach to solve the second problem would be like following: Let the radius be $R$ and the length of chord be $C$ . Area of circular segment(⌓) would be $\pi R^2/4$ . Another way to calculate the area of circular segment would be to calculate the area of circular sector(⌔)  first and then subtract the area of the triangle formed by two radius and chord. Hence, $\pi R^2/4 = R^2\theta/2 - R^2\sin\theta/2$ $\theta = 2\sin^{-1}\frac{C}{2R}$ where $\theta$ is the angle made by the chord at the center. The hunch says $\theta$ must be constant. But someone is stuck as he doesn't know how to proceed from here.","['trigonometry', 'area', 'geometry']"
4400029,Can you list me all the geometrical conditions of two ellipses with a common focus?,"I'm dealing with a mathematics/astrodynamics problem that consist into writing a Matlab code that computes the possible intersections of two ellipses with a common focus.
I'm writing a series of if-statements but I'm having trouble to set all conditions.
I have to cover all possible cases, i.e., ellipses contained one inside the other (no intersections), the case of same line of apsides, all the cases of tangency condition, all cases of two intersection points and so on.
Can you help me? Here is my code, but it requires some adjustments: clear all; close all; clc;

global tol twopi
a1=200000;
e1=0.6;
a2 =250000;
e2=0.4;
D_omega=0;  % difference between pericenters

tol   = 1.e-30; % tolerance for numeric zero
twopi = pi + pi;



% Solutions initialization
xiA = 0.0;
yiA = 0.0;
xiB = 0.0;
yiB = 0.0;

cosDomeg = cos(D_omega);
sinDomeg = sin(D_omega);
p1 = a1*(1.0-e1*e1);  % semilatus rectum
p2 = a2*(1.0-e2*e2);

r_p1 = a1*(1.0-e1);   % pericenter radius
r_a1 = a1*(1.0+e1);   % apocenter radius
r_p2 = a2*(1.0-e2);
r_a2 = a2*(1.0+e2);

if(r_a1 < r_p2)     % ellipse 1 contained inside ellipse 2
    nSol = 0;
    str = 'ellipse 1 contained inside ellipse 2';
elseif(r_a2 < r_p1) % ellipse 2 contained inside ellipse 1
    nSol = 0;
    str = 'ellipse 2 contained inside ellipse 1';
elseif(r_p1 < r_p2 && r_a1 < r_a2 && abs(D_omega) < tol) % two ellipses with same apse line (Delta theta = 0), ellipse 1 contained inside ellipse 2
    nSol = 0;
elseif(r_p2 < r_p1 && r_a2 < r_a1 &&  abs(D_omega) < tol) % two ellipses with same apse line (Delta theta = 0), ellipse 2 contained inside ellipse 1
    nSol = 0;
elseif(abs(p2-p1) < tol && abs(e2-e1) < tol && abs(D_omega) < tol)  % two identical ellipses
    % identical ellipses
    nSol = -1;
else
    a = p1-p2;
    b  = p1*e2*cosDomeg-p2*e1;
    c = -p1*e2*sinDomeg;
    
    % general solution
    k1 = b*b + c*c;
    k2 = a*b;
    k3 = a*a - c*c;

    discr = k2*k2 - k1*k3;
    
    
    if(discr < tol) % they do not intersect  !!!(qui non dovrebbe essere discr<tol ?)!!!!
        nSol = 0;
        str = 'delta negative: no intersections';
    elseif(abs(discr) < tol && abs(sinDomeg) > tol)
        % the discriminant is null and the two axes are neither parallel nor
        % antiparallel
        % TWO IDENTICAL SOLUTIONS (tangency point)
        nSol = 1;
        cth1 = -k2/k1;
        sth1 = (a + b*cth1)/c;
        cth2 = cth1;
        sth2 = sth1;
        str = 'delta = 0: two identical solutions!';
    else
        % TWO DISTINCT SOLUTIONS
        nSol = 2;
        if(abs(sinDomeg) > tol) 
            cth1 = (-k2 + sqrt(discr))/k1;
            cth2 = (-k2 - sqrt(discr))/k1;
            sth1 = (a + b*cth1)/c;
            sth2 = (a + b*cth2)/c;
            str = '2 solutions: general case';
        else  
            if(abs(a/b - 1) < tol)  % ellipses are parallel or anti-parallel and are tangent at theta = 180 (apocenter)
                nSol = 1;
                cth1 = -1.0;
                cth2 = cth1;
                sth1 = 0.0;
                sth2 = 0.0;
                str = '1 solution: tangency at apocenter';
            elseif(abs(a/b + 1) < tol)  % ellipses are parallel or anti-parallel and  are tangent at theta = 0 (pericenter)
                nSol = 1;
                cth1 = 1.0;
                cth2 = cth1;
                sth1 = 0.0;
                sth2 = 0.0;
                str = '1 solution: tangency at pericenter';
            else  % special case: same apse line with 2 distinct solutions 
                cth1 = -a/b;
                cth2 = cth1;
                sth1 = (1 - cth1^2)^0.5;
                sth2 = -sth1;
                str = '2 solution: same apse line';
            end
        end
        th1 = atan2(sth1,cth1);
        th1 = mod(th1,twopi);
        th2 = atan2(sth2,cth2);
        th2 = mod(th2,twopi);
        
        %Compute coordinates of intersection points in the perifocal r.f.
        %of ellipse 1
        [xiA,yiA] = Get_perifocal_coordinates(a1,e1,th1);
        [xiB,yiB] = Get_perifocal_coordinates(a1,e1,th2);    
        
    end
end P.S. My work activity: Study possbile intersections between asteroids and Keplerian orbits derived by propagation of Invariant Manifolds computed starting from Lyapunov orbits (by assuming to be outisde of the circle of influence of the Earth and not sphere since I'm considering the simpe 2D case). So, we are in heliocentric perspective, within the ecliptic plane (with a good approximation).","['matlab', 'conic-sections', 'geometry']"
4400033,The number of feasible subsets of the knapsack problem - the combinatorial explosion,"I am reading the book ""Integer Programming"" by Wolsey (1998). In 1.4, the author is counting the number of the feasible subsets of a knapsack problem. The formulation is $\max \sum_{j=1}^n c_j x_j$ s.t. $\sum_{j=1}^n a_j x_j \leq b.$ In p. 9, the author claims that ""The number of subsets is $2^n$ . When $b=\sum_{j=1}^n a_j / 2$ , at least half of the subsets are feasible, and thus there are at least $2^{n-1}$ feasible subsets."" I do not understand where $2^{n-1}$ comes from. Thanks.","['operations-research', 'combinatorics', 'integer-programming', 'discrete-mathematics', 'optimization']"
4400123,"Show that $(Y,||\cdot||_Y)$ is a Banach space","Let $(X,||\cdot||_X)$ be a Banach space and $(e_n)_{n\in\mathbb{N}}$ a Schauder basis of X. How can I prove that $Y:=\{\alpha:\mathbb{N}\to\mathbb{R} \space| \lim_{N\to\infty}\sum_{n=0}^N\alpha_n e_n \;\text{exists}\}$ is a Banach space with the norm $||\alpha||_Y:=\sup_{N}||\sum_{n=0}^N\alpha_n e_n||_X$ . Especially how can I show that $||\cdot||_Y$ is complete. So far I could only show that if we have a Cauchy sequence $(\alpha^{(l)})_{l\in\mathbb{N}}\subset Y$ then we have pointwise $\alpha^{(l)}_j\to\alpha_j$ as $l\to\infty$ for some $a_j\in\mathbb{R}$ . This way one can define $\alpha:\mathbb{N}\to\mathbb{R}$ . But I don't know how to show $\alpha^{(l)}\to\alpha$ as $l\to\infty$ in $||\cdot||_Y$ .","['banach-spaces', 'complete-spaces', 'cauchy-sequences', 'analysis', 'functional-analysis']"
4400128,Solving $\lim_{n\to\infty}(\frac{(2n)!}{n^n\cdot n!})^{1/n}$,"Find the following limit: $$\lim_{n\to\infty}\left(\dfrac{(2n)!}{n^n\cdot n!}\right)^{1/n}$$ My work: Lets assume the given limit be $y$ $$\begin{align}y& = \lim_{n\to\infty}\left(\dfrac{(2n)!}{n^n\cdot n!}\right)^{1/n}\\\\&= \lim_{n\to\infty}\left(\dfrac{(2n)(2n-1)(2n-2)...2.1}{n^n\cdot n!}\right)^{1/n}\\\\&= \lim_{n\to\infty}\left(\dfrac{\Big[(2n)(2n-2)(2n-4)...4.2\Big]\Big[(2n-1)(2n-3)(2n-5)...3.1\Big]}{n^n\cdot n!}\right)^{1/n} \\\\&= \lim_{n\to\infty}\left(\dfrac{\Big[2^n(n!)\Big]\Big[(2n-1)(2n-3)(2n-5)...3.1\Big]}{n^n\cdot n!}\right)^{1/n}\end{align}$$ Taking $\log$ both sides, $$\begin{align}\log(y)&= \lim_{n\to\infty}\left(\dfrac{(4n-2)(4n-6)(4n-8)...(10)(6)(2)}{n^n}\right)^{1/n}\\\\&= \lim_{n\to\infty}\dfrac1n\log\left(\dfrac{(4n-2)(4n-6)(4n-8)...(10)(6)(2)}{n^n}\right)\\\\&\overset{({\Large*})}= \lim_{n\to\infty}\dfrac1n\sum_{r=1}^n\log\left(\dfrac{4n - 2(2r-1)}{n}\right)\\\\&\overset{({\Large*})}= \lim_{n\to\infty}\dfrac1n\sum_{r=1}^n\log\left(4 - \dfrac{4r}{n} + \dfrac2n\right)\\\\&\overset{({\Large*})}= \int_0^1\log(4 - 4x ) dx\end{align}$$ I'm  not sure if the steps (*) are correct or not. Can anyone guide me please.","['definite-integrals', 'factorial', 'logarithms', 'calculus', 'limits']"
4400144,All roots of a specific polynomial lie inside the unit disc,"I am currently working on the following problem: Let $m,n \in \mathbb{N}$ and $p: \mathbb{C} \to \mathbb{C},~ z \mapsto z^{m+1}-z^m+(\frac{1}{4n})^m$ .
Show that if $p(z)=0$ then $|z|<1$ , i.e. that all roots of the polynomial $p$ lie inside the open unit disc. I first approached the problem by trying to use results from complex analysis: The theorem of Gershgorin yields an upper bound on the absolute value of the roots, which is strictly larger than 1 Rouché's theorem does not seem to be applicable here in a suitable way, since the sum of any two coefficients is larger than the remaining coefficient. Next, I tried to argue by contradiciton:
Assume that there exists a $z\in \mathbb{C}$ with $|z|\geq 1$ such that $p(z)=0$ , so $z^{m+1}-z^m+(\frac{1}{4n})^m = 0$ or equivalently $z^m(z-1) = -(\frac{1}{4n})^m$ . Then for the absolute value we have \begin{aligned}
 |z^m(z-1)| & =  (\frac{1}{4n})^m \\
|z^m||(z-1)| & =  (\frac{1}{4n})^m \\
|z-1| & = (\frac{1}{4n|z|})^m \leq (\frac{1}{4n})^m 
\end{aligned} Thus, it holds $z \in B_{\leq (\frac{1}{4n})^m}(1)$ and $z \notin B_{\leq 1}(0)$ . The polynomial $\zeta^m(\zeta-1)$ has a single real root at 1, so I think it should be possible to show that the root $z$ of $p$ close to 1 also is real. This would yield a contradiciton becuase then there exists an $\varepsilon > 0$ such that $z = 1+ \varepsilon$ , but then we would have $(1+\varepsilon)^m(1+\varepsilon-1) = (1+\varepsilon)^m \varepsilon \overset{!}{=} -(\frac{1}{4n})^m$ . I was not capable to show that the root is real, my only idea is to argue about the complex argument of $z^m(z-1)$ , which would be 0 because $-(\frac{1}{4n})^m$ is real. Any ideas are highly appreciated!","['complex-analysis', 'roots', 'polynomials', 'roots-of-unity']"
4400180,Does the mean of the ratio of the number of distinct prime factors to the number of divisors of natural numbers converge?,"Let $d(n)$ and $\omega(n)$ be the number of divisors and the number of distinct prime factors of $n$ respectively. What is the limiting value of $$
\lim_{n \to \infty} \frac{1}{n}\sum_{r=1}^n \frac{\omega(r)}{d(r)}
$$ For $n \le 23275000000 $ , the value is approximately $0.275967$ .","['elementary-number-theory', 'number-theory', 'analytic-number-theory', 'limits', 'prime-numbers']"
