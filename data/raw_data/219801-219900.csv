question_id,title,body,tags
4496685,The function $y=f(x)$ is represented parametrically by $x=t^5-5t^3-20t+7$ and $y=4t^3-3t^2-18t+3$ $(-2\lt t\lt2)$. The minimum of $y=f(x)$ occurs at,"Question: The function $y=f(x)$ is represented parametrically by $x=t^5-5t^3-20t+7$ and $y=4t^3-3t^2-18t+3$ $(-2\lt t\lt2)$ . The minimum of $y=f(x)$ occurs at $t=...$ My Attempt: For extrema, I am calculating $\frac{dy}{dx}$ $\frac{dy}{dx}=\frac{dy}{dt}\frac{dt}{dx}=\frac{12t^2-6t-18}{5t^4-15t^2-20}=\frac{6(2t^2-t-3)}{5(t^4-3t^2-4)}=\frac{6(2t-3)(t+1)}{5(t^2-4)(t^2+1)}$ Around $t=\frac32, f'(x)$ sign changes from positive to negative. So $t=\frac32$ is local maxima. Around $t=-1, f'(x)$ sign changes from negative to positive. So $t=-1$ is local minima. But the answer given is $\frac32$ What's wrong in my approach?","['maxima-minima', 'calculus', 'parametric', 'polynomials', 'derivatives']"
4496711,Nonstandard algebraic geometry: Fundamental Theorem of Algebra,"I have been trying to study the basics of algebraic geometry using nonstandard analysis and I can't wrap my head around this issue. Let $^*\mathbb{C}$ be the extension of the complex numbers. Now fundamental theorem of algebra says that all polynomials in $\mathbb{C}[x]$ have solutions in $\mathbb{C}$ . It also implies that a degree $n$ polynomial will have $n$ complex solutions. By transfer, this should also be true for polynomials in $^*\mathbb{C}[x]$ . Now if you have a degree $n$ polynomial $f \in \mathbb{C}[x]$ , then it has $n$ zeroes in $^*\mathbb{C}$ and these zeros must all be standard. This also means that if $x \in {}^*\mathbb{C}$ is nonstandard, then $f(x) \neq 0$ . However, I'm reading Nonstandard Generic Points by Guy Wallet, and he uses Nullstellensatz to show that for a prime ideal in $\mathbb{C}[x]$ , $P$ , there is a point $x \in {}^*\mathbb{C}$ such that $f(x)=0$ iff $f \in P$ . This means there are standard polynomials that are zero on nonstandard numbers. Does this contradict the fundamental theorem of algebra? I appreciate any explanation. Thanks.","['nonstandard-analysis', 'algebraic-geometry', 'nonstandard-models', 'polynomials']"
4496719,Comparing the growth of two functions,This is my almost final result for comparing two functions. $$\lim\limits_{x\to \infty} =\frac{f(x)}{g(x)}=\frac{4x}{2x}=2$$ From here can we say which one of these functions grows faster?,"['limits', 'functions']"
4496730,A function on $\mathbb R^4$ descends to a function on $RP^3$,"Consider the smooth function $f:\mathbb R^4\to \mathbb R; (x,y,z,t)\mapsto 4x^2+3y^2+3z^2+t^2.$ Show that $f$ descends to a smooth function $g\colon RP^3\to \mathbb R$ such that $g\circ \pi = f$ . Here $\pi:\mathbb R^4-\{0\}\to RP^3$ is the canonical projection. Find the critical values of $g$ . I am having difficulty understanding this question. If $g$ is well-defined on $RP^3$ then for $p:=(x,y,z,t)\in \mathbb R^4-\{0\}$ and $\alpha\in \mathbb R-\{0\}$ , we should have $$f(p)=g\circ \pi(p)=g\circ\pi(\alpha p)=f(\alpha p)=\alpha^2 f(p).$$ But this cannot hold for any non-zero $\alpha$ .","['smooth-manifolds', 'analysis', 'differential-geometry']"
4496736,$y''+2\tan x y'-y=0$,"Question: Use the variation of parameter method to find the general solution of the following differential equation $$(\cos x) y''+(2\sin x) y'-(\cos x) y =0\;\;\;\;,\;\;\;\;0<x<1$$ My Try: I think the question is wrong, since the right hand side term is 0, so the particular integral will also be zero. Thus, the general solution will be equal to homogenous solution. So, I think no use of using the variation of parameter formulas since $y_p(x)=0$ always.
I reduced the equation as in the subject or title and then used integrating factor $$y=(\cos x )z$$ to eliminate the term $y'$ but I got another difficult DE as $z''-2\sec^2xz=0$ Please help with any suggestions or do you think question is correct. Is there a way to solve it ?","['integration', 'calculus', 'ordinary-differential-equations']"
4496755,Express $|\pi - \frac{23}{7}|$ without the absolute value symbol,"Express $\left|\pi - \dfrac{23}{7}\right|$ without the absolute value symbol. I know I have to check if $\pi - \dfrac{23}{7}$ is greater than (or equal to) zero, but how can I do it analytically (without a calculator)? I know that $\pi \gt 3=\dfrac{21}{7}$ but how to compare $\pi$ with $\dfrac{23}{7}$ ?","['calculus', 'algebra-precalculus', 'arithmetic']"
4496772,find the number of subspaces $W$ such that $U\subseteq W$,"Let $V$ be a vector space of dimension $n$ over a finite field $F$ with $q$ elements, and let $U\subseteq V$ be a subspace of dimension $k$ . How many subspaces $W\subseteq V$ of dimension $m$ such that $U\subseteq W $ do we have $(k\leq m\leq n)$ ? Hint : look at the set: $$
\{ (U,W) \mid U \subseteq W \subseteq V; 
\, \dim(U)=k, \, \dim(W)=m \}
$$ I have no idea how to use the hint. I know that the number of subspaces of dimension $k$ is: $$
\frac{\prod_{i=0}^{k-1} (q^{n-1}-1)}{\prod_{i=0}^{k-1} (q^{k-1}-1)}
$$ and I don't know how to proceed from here.","['finite-fields', 'linear-algebra', 'vector-spaces', 'combinatorics']"
4496783,How to solve a second order ODE with no constant coefficients?,"Consider $x''(t)=a(t)x(t)$ on some time interval $[0,T]$ . For a first order equation one can formally divide by $x(t)$ and hence get the solution by the exponential function. For constants coeffcients one could take the ansatz $x(t)=e^{\lambda t}$ . Now both approaches do not seem to work...I would be grateful for any hints how to solve this kind of equations!","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4496815,"If $P(n, m)$ is the number of permutations such that $m$ is the first fixed point, prove that $P(n, m + 1) = P(n, m) - P(n - 1, m)$","For $n, m \in \mathbb{N}, m \leq n$ , let $P(n, m)$ denote the number of permutations of length $n$ for which $m$ is the first number whose position is left unchanged. Thus, $P(n, 1) = (n - 1)!$ and $P(n, 2) = (n - 1)! - (n - 2)!$ . Show that $$P(n, m + 1) = P(n, m) - P(n - 1, m)$$ for each $m = 1, 2, \cdots, n - 1$ . Hello, can someone help me with the combinatorial proof for this? I can prove it in other way, by proving that $$P(n, m) = \sum_{i = 0}^{m - 1}(-1)^i\binom{m-1}{i}(n - i-1)!$$ using PIE. Now, turning $P(n, m) - P(n - 1,m)$ into $P(n, m+1)$ is just algebraic manipulation. I'd be thankful if someone could help in proving this combinatorially. Thanks","['combinatorial-proofs', 'recurrence-relations', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4496835,What is the intuition behind using the harmonic mean?,Why do we sometimes prefer the harmonic mean to the arithmetic mean? When is it more relevant to use the former than the latter?,"['average', 'statistics', 'means']"
4496877,How do I solve a recurrence relation of the type $a_{n+1} = K(n)a_{n} + P(n)a_{n-1}$ where $K(n)$ and $P(n)$ are rational functions?,"I've been stuck on a problem for days now and was able to boil it down to this recurrence relation. I researched a bit about ways to solve it -- generating functions, characteristic polynomials, homogenous and non-homogenous recurrence relations -- but couldn't find a method of tackling this specific one. Any tips, solutions, or suggestions would be welcomed. Thanks. Edit: As made clear in the comments, treating the functions as any rational function is too general to solve. The specific problem I am solving requires $K(n) = (2n+5)/(n+2)$ and $P(n) = (n+1)(n+3)^2/(n+2)$ Edit #2: As requested, $a_1 = 5 ; a_2 = 33 ; a_3 = 168$","['recurrence-relations', 'discrete-mathematics']"
4496913,"If $x=\frac12(\sqrt[3]{2009}-\frac{1}{\sqrt[3]{2009}})$, what is the value of $(x+\sqrt{1+x^2})^3$?","If $x=\frac12(\sqrt[3]{2009}-\frac{1}{\sqrt[3]{2009}})$ , what is the value of $(x+\sqrt{1+x^2})^3$ ? I solved this problem as follow, Assuming $\sqrt[3]{2009}=\alpha$ , we have $x=\frac12(\alpha-\frac1{\alpha})$ and $$(x+\sqrt{1+x^2})^3=\left[\frac12(\alpha-\frac1{\alpha}) +\sqrt{1+\frac14(\alpha^2+\frac1{\alpha^2}-2)}\right]^3=\left[(\frac{\alpha}2-\frac1{2\alpha}) +\sqrt{\frac{\alpha^2}4+\frac1{4\alpha^2}+\frac12)}\right]^3=\left(\frac{\alpha}2-\frac1{2\alpha}+\left|\frac{\alpha}2+\frac1{2\alpha}\right|\right)^3=\alpha^3=2009$$ I'm wondering, is it possible to solve this problem with other approaches?","['algebra-precalculus', 'radicals']"
4496923,"Spivak, Ch. 15, Problem 25. Interpretation of proof that $|\sin{x}-\sin{y}|<|x-y|$ for all numbers $x\neq y$.","Prove that $|\sin{x}-\sin{y}|<|x-y|$ for all numbers $x\neq y$ . Let $x\neq y$ . The Mean Value Theorem tells us that there is some $c\in (x,y)$ such that $$\frac{\sin{x}-\sin{y}}{x-y}=\sin'(c)=\cos(c)$$ $$\left | \frac{\sin{x}-\sin{y}}{x-y} \right | = | \cos{c}| \leq 1$$ $$|\sin{x}-\sin{y}|\leq |x-y|$$ Equality holds if $|\cos{c}|=1$ , which means $c=k\pi, k\in\mathbb{Z}$ . The claim in the problem statement is essentially that it never occurs that we have $x\neq y$ and a $c\in (x,y)$ from the MVT such that $|\cos{c}|=1$ , correct? To prove this we take two numbers $x<y$ , and a third number $z\in (x,y)$ chosen such that there is no number of form $\pi k\in (x,z)$ . Then $$|\sin{y}-\sin{x}|=|\sin{y}-\sin{z}+\sin{z}-\sin{x}|$$ $$=|(y-z)\cos{c_1}+(z-x)\cos{c_2}|\tag{1}$$ where $c_1\in(z,y)$ with $|\cos{c_1}|\leq 1$ , and $c_2\in (x,z)$ , so $c_2\neq k\pi \implies |\cos{c_2}|<1$ . Therefore $$|\sin{y}-\sin{x}|<|(y-z)\cdot 1 +(z-x)\cdot 1| \tag{2}$$ $$= |y-x|$$","['integration', 'calculus', 'solution-verification', 'derivatives']"
4496925,Estimating the value of this Integral,"If I= $\int_0^1x^{sinx+cosx}dx$ then find the value of $[10I]$ where $[.]$ represent greatest integer function. A)3 $\boxed{B)4}$ C)5 D)6 Mathongo, JEE sample questions Method 1 : I took a few points on the given curve and estimated out the area to be slightly less than 0.5 in the given interval, thus yielding the correct answer: 4. Legend: Black line $y=x$ Curve bordering red area $x^{\sin x + \cos x}$ Bounds as given. Here is the attached Desmos link to depict the same Method 2 (prompted): $\hspace{42px} 1\leq sinx+cosx \leq \sqrt{2}$ $\implies x \geq x^{sinx+cosx} \geq x^{\sqrt{2}}$ $\implies \int_0^1 xdx \geq \int_0^1x^{sinx+cosx}dx \geq \int_0^1x^{\sqrt{2}}dx$ $\implies 0.5 \geq I \geq 0.414$ Thus, $\boxed{[10I]=4}$ . Would be glad if the community could come up with more alternative approaches.","['integration', 'area', 'estimation']"
4496946,Estimate the number of conjugacy classes in a group,"Let $\rho: G \to GL(V)$ be a finite dimensional complex irreducible representation of $G$ . Let $H$ be an abelian subgroup of $G$ . I want to show that the number of irreducible characters of $G$ is greater than or equal to $\frac{\lvert H \rvert^2}{\lvert G \rvert}$ . Here's what I did: Suppose $W \neq 0$ be a subspace of $V$ that is invariant under the $H$ -action, i.e., $\rho(h)W \subseteq W$ for all $h \in H$ . Since $H$ is abelian, we can assume $\text{dim}_{\mathbb{C}}(W) = 1$ . Consider the subspace $W_0 = \sum_{g \in G}\rho(g) W$ . Since $W_0$ is $G$ -invariant and $\rho$ is irreducible, it follows that $W_0 = V$ . In addition, we note $\rho(g_1)W = \rho(g_2)W$ if $g_1H = g_2H$ . Hence, $W_0 = \sum_{i = 1}^{k}\rho(g_i)W$ , where $g_1,\dots,g_k$ are coset representatives of $G/H$ . Therefore, $\text{dim}_{\mathbb{C}}(V) \leqslant \text{dim}_{\mathbb{C}}(W_0) \leqslant k = \frac{\lvert G \rvert}{\lvert H \rvert}$ . To prove the statement, it suffices to show that the number of conjugacy classes in $G$ is greater than or equal to $\frac{\lvert H \rvert}{\text{dim}_{\mathbb{C}}(V)}$ . But I don't know how to do the last part.","['group-theory', 'representation-theory']"
4497003,What is $\mathbb{1}$ with a subscript? [duplicate],"This question already has an answer here : meaning of subscript notation $\ 1_{a=a'}$ (1 answer) Closed 1 year ago . In my textbook it's estimating $\mu = P(X > 2)$ with monte carlo estimation, and I'm confused about the line $\mu = P(X > 2) = E(\mathbb{1}_{\{X>2\}})$ . What would the $\mathbb{1}_{\{X>2\}}$ mean ? It's used earlier as a remark ""Let $A$ be a proper subset of $\mathbb{R}^d$ . If $g(x) = \mathbb{1}_A(x)$ , then $E(g(X)) = E(\mathbb{1}_A(X)) = 0 \times P(\mathbb{1}_A(x) = 0) + 1 \times P(\mathbb{1}_A(x) = 1) $","['notation', 'statistics', 'monte-carlo']"
4497017,Prove $\left|1+z_1\right| +\left|1+z_2Â \right| + \left|1+z_1z_2\right|\geq 2$,"I am struggling with this problem Let $z_1,z_2\in\mathbb{C}$ prove that $$\left|1+z_1\right| +\left|1+z_2Â \right| + \left|1+z_1z_2\right|\geq 2$$ I know a similar question has been solved: if $|z_i|=1$ prove $|z_1+1|+|z_2+1|+|z_1z_2+1|\ge 2$ .
However, I don't have the $|z_1|=|z_2|=1$ condition, so I am not sure if the question is wrong or the last condition is implicit. This is the work I have done: \begin{align*}
            |1+z_1|+|1+z_2|+|1+z_1 z_2| &= |1+z_1| + |1+z_2|+|-(1+z_1z_2)|\\
                                &\geq |1+z_1| + \left|(1+z_2)-(1+z_1z_2)\right|\\
                                &= |1+z_1|+ |z_2-z_1z_2|\\
                                &\geq |(1+z_1)+(z_2-z_1z_2)|
  \end{align*} I'll appreciate your help. I am not sure what I am missing here.","['complex-analysis', 'inequality', 'cauchy-schwarz-inequality', 'complex-numbers']"
4497019,"Show that $\left(\nabla_c\nabla_d-\nabla_d\nabla_c\right)v^a=R_{bcd}^av^b$ for vector field, $v$, and the Riemann curvature tensor, $R_{bcd}^a$","I'm going to be asking about parts of the author's solution to the following question. I feel this is a very important question to understand as it touches on some crucial aspects of the covariant derivative. Show that (without torsion) $$\left(\nabla_c\nabla_d-\nabla_d\nabla_c\right)v^a=R_{bcd}^av^b\tag{1}$$ where $$R_{bcd}^a=\partial_c\Gamma_{bd}^a-\partial_d \Gamma_{bc}^a+\Gamma_{ec}^a\Gamma_{bd}^e-\Gamma_{ed}^a\Gamma_{bc}^e$$ is the Riemann curvature tensor. Hint: determine $\nabla_c\nabla_dv^a$ . I'm trying to understand the following solution to the question above: $$\nabla_c\nabla_dv^a=\partial_c\nabla_dv^a-\Gamma_{dc}^b\nabla_b v^a+\Gamma_{bc}^a\nabla_d v^b\tag{a}$$ $$=\partial_c\left(\partial_dv^a+\Gamma_{bd}^av^b\right)-\Gamma_{dc}^b\left(\partial_bv^a+\Gamma_{eb}^av^e\right)+\Gamma_{bc}^a\left(\partial_dv^b+\Gamma_{ed}^bv^e\right)\tag{b}$$ $$=\left(\partial_c\Gamma_{bd}^a\right)v^b+\Gamma_{bc}^a\Gamma_{ed}^bv^e + \text{contribution symmetric in $c$ and $d$ indices}\tag{c}$$ $$=\left(\partial_c\Gamma_{bd}^a\right)v^b+\Gamma_{ec}^a\Gamma_{bd}^ev^b + \text{contribution symmetric in $c$ and $d$ indices}\tag{d}$$ on exchanging dummy $b$ and $e$ indices in the second term. Subtracting $\nabla_d\nabla_cv^a$ gives the stated result. In order for me to make sense of the author's solution, I need to understand each of the four equalities, $(\mathrm{a})-(\mathrm{d})$ , in turn. So starting with $(\mathrm{a})$ , for a contravariant vector field $v^a(x)$ , the covariant derivative, $\nabla_{\text{cov}}$ is defined through $$\nabla_{\text{cov}}v^a=\partial_{\text{cov}}v^a+\Gamma_{b,\,{\text{cov}}}^av^b\tag{2}$$ Similarly, for a covariant vector field $v_b(x)$ , the covariant derivative, $\nabla_{\text{cov}}$ is defined through $$\nabla_{\text{cov}}v_b=\partial_{\text{cov}}v_b-\Gamma_{b,\,{\text{cov}}}^av_a\tag{3}$$ [Please note that I have purposely used ' $\mathrm{cov}$ ' to represent the index for the covariant derivative, this was done to avoid writing say, ' $\nabla_c$ ', which would cause confusion with the notation that follows]. For a tensor field of type $(1,1)$ , $T_b^a$ , the covariant derivative of this tensor field, $\nabla_c$ is $$\nabla_c T_b^a=\partial_c T_{b}^a+\Gamma_{cd}^a T_{b}^d-\Gamma_{bc}^e T_{e}^a \tag{4}$$ Since the gradient of a vector field is a tensor (field), let the tensor (field) of type $(1,1)$ be $T_d^a=\nabla_d v^a$ . Now, using the hint at the end of the question and eqn $(4)$ , $$\begin{align}\nabla_c\left(\nabla_dv^a\right)&=\nabla_cT_d^a\\&=\partial_cT_d^a+\Gamma_{bc}^aT_d^b-\Gamma_{cd}^e T_e^a\\&=\partial_c\nabla_dv^a+\Gamma_{bc}^a\nabla_dv^b-\Gamma_{cd}^e \nabla_ev^a\tag{5}\end{align}$$ and this is the equivalent of $(\mathrm{a})$ in the author's solution. The next equality, $(\mathrm{b})$ , can be found by direct substitution of eqn $(2)$ into $(5)$ . To try to understand the last two equalities, $(\mathrm{c})-(\mathrm{d})$ , I multiply out eqn $(\mathrm{b})$ : $$\begin{align}\nabla_c\nabla_dv^a &=\partial_c\left(\partial_dv^a+\Gamma_{bd}^av^b\right)-\Gamma_{dc}^b\left(\partial_bv^a+\Gamma_{eb}^av^e\right)+\Gamma_{bc}^a\left(\partial_dv^b+\Gamma_{ed}^bv^e\right)\\&=\partial_c\partial_dv^a+\partial_c\Gamma_{bd}^av^b-\Gamma_{dc}^b\partial_bv^a-\Gamma_{dc}^b\Gamma_{eb}^av^e+\Gamma_{bc}^a\partial_dv^b+\Gamma_{bc}^a\Gamma_{ed}^bv^e\tag{6}\\& \stackrel{\color{red}{?}}{=}\left(\partial_c\Gamma_{bd}^a\right)v^b+\Gamma_{bc}^a\Gamma_{ed}^bv^e+ \text{contribution symmetric in $c$ and $d$ indices}\tag{7}\end{align}$$ But the final equality (marked with a red question mark above the equals sign) can only be true if out of the six terms in eqn $(6)$ we have $$\partial_c\partial_dv^a=-\Gamma_{dc}^b\partial_bv^a=\Gamma_{bc}^a\partial_dv^b=0\tag{8}$$ and therefore I identify $$-\Gamma_{dc}^b\Gamma_{eb}^av^e\tag{9}$$ as the contribution symmetric in $c$ and $d$ indices in $(7)$ . I'll reserve asking about part $(\mathrm{d})$ for now until I have understood part $(\mathrm{c})$ . So my questions about eqn $(\mathrm{c})$ are: What justifies those particular three terms from eqn $(6)$ being equal to zero in $(8)$ , and why are the others non-zero? Is $(9)$ the unidentified symmetric contribution in the $c$ and $d$ indices eluded to $(7)$ ? Here I embed images of the question and its' solution in case I made any typos while typesetting this post: and here is the solution: Update: Even though a good answer has already been provided the part I still can't understand is the resulting expression $(\mathrm{c})$ : $$\nabla_c\nabla_dv^a=\left(\partial_c\Gamma_{bd}^a\right)v^b+\Gamma_{bc}^a\Gamma_{ed}^bv^e + \text{contribution symmetric in $c$ and $d$ indices}$$ I understand how $(\mathrm{d})$ was obtained from $(\mathrm{c})$ but since my question revolves almost entirely about how to get from eqn $(\mathrm{b})$ to $(\mathrm{c})$ , for the purposes of this question and to stay consistent with the answer given by @ContraKinta I will use eqn $(\mathrm{c})$ . I think the line ""contribution symmetric in $c$ and $d$ indices"" is the part causing me the most confusion. For instance, if I were to explicitly write down these 'symmetric contributions' when indices $c$ is switched with $d$ and vice versa then I think the resulting expression should be: $$\nabla_c\nabla_dv^a=\left(\partial_c\Gamma_{bd}^a\right)v^b+\Gamma_{bc}^a\Gamma_{ed}^bv^e+\left(\partial_{\color{blue}{d}}\Gamma_{b\color{red}{c}}^a\right)v^b+\Gamma_{b \color{blue}{d}}^a\Gamma_{e\color{red}{c}}^bv^e\tag{e}$$ Where the third and fourth terms are just the first and second terms repeated, respectively, but with the positions of the $d$ and $c$ indices interchanged, as marked by the blue and red colors, respectively. But this is not what I see when I look at the full expression for $\nabla_c\nabla_dv^a$ : $$\nabla_c\nabla_dv^a=\underbrace{\partial_c\nabla_dv^a}_{(\mathrm{f})}-\underbrace{\Gamma_{dc}^b\nabla_b v^a}_{(\mathrm{g})}+\underbrace{\Gamma_{bc}^a\nabla_d v^b}_{(\mathrm{h})}$$ $$=\partial_c\left(\partial_dv^a+\Gamma_{bd}^av^b\right)-\Gamma_{dc}^b\left(\partial_bv^a+\Gamma_{eb}^av^e\right)+\Gamma_{bc}^a\left(\partial_dv^b+\Gamma_{ed}^bv^e\right)$$ $$=\partial_c\partial_dv^a+\left(\partial_c\Gamma_{bd}^a\right)v^b+\Gamma_{bd}^a\partial_cv^b\tag{f}$$ $$-\Gamma_{dc}^b\partial_bv^a-\Gamma_{dc}^b\Gamma_{eb}^av^e\tag{g}$$ $$+\Gamma_{bc}^a\partial_dv^b+\Gamma_{bc}^a\Gamma_{ed}^bv^e\tag{h}$$ $$=\fbox{$\partial_c\partial_dv^a+\left(\partial_c\Gamma_{bd}^a\right)v^b-\Gamma_{dc}^b\partial_bv^a-\Gamma_{dc}^b\Gamma_{eb}^av^e+\Gamma_{bc}^a\Gamma_{ed}^bv^e+\Gamma_{bd}^a\partial_cv^b+\Gamma_{bc}^a\partial_dv^b$}$$ The labelled eqns ( $\mathrm{f}$ - $\mathrm{h}$ ) show the expansion of each of the terms above in the two equations above. There are in fact $7$ terms, the last two terms are symmetric in the $c$ and $d$ indices, that is, swapping the $c$ and $d$ indices in the 6th term of the boxed expression gives the 7th and final term, namely, $\Gamma_{bc}^a\partial_d 
v^b$ . By comparing the boxed equation for $\nabla_c\nabla_dv^a$ and eqn $(\mathrm{c})$ , in fact, the only two terms common to both the boxed eqn and $(\mathrm{c})$ are the 2nd and 5th terms in the boxed eqn. So what happened to the other terms? More importantly, why did the 1st term, $\partial_c\partial_dv^a$ disappear? I can't invoke $\partial_c\partial_dv^a-\partial_d\partial_cv^a=0$ as explained in the answer as I have not subtracted off $\nabla_d\nabla_cv^a$ yet. For the sake of completeness I will now compute $\nabla_d\nabla_cv^a$ (though this is only for reference and is not needed to answer the questions I have here): $$\nabla_d\nabla_cv^a=\underbrace{\partial_d\nabla_cv^a}_{(\mathrm{i})}-\underbrace{\Gamma_{cd}^b\nabla_b v^a}_{(\mathrm{j})}+\underbrace{\Gamma_{bd}^a\nabla_c v^b}_{(\mathrm{k})}$$ $$=\partial_d\left(\partial_cv^a+\Gamma_{bc}^av^b\right)-\Gamma_{cd}^b\left(\partial_bv^a+\Gamma_{eb}^av^e\right)+\Gamma_{bd}^a\left(\partial_cv^b+\Gamma_{ec}^bv^e\right)$$ $$=\partial_d\partial_cv^a+\left(\partial_d\Gamma_{bc}^a\right)v^b+\Gamma_{bc}^a\partial_dv^b\tag{i}$$ $$-\Gamma_{cd}^b\partial_bv^a-\Gamma_{cd}^b\Gamma_{eb}^av^e\tag{j}$$ $$+\Gamma_{bd}^a\partial_cv^b+\Gamma_{bd}^a\Gamma_{ec}^bv^e\tag{k}$$ $$=\fbox{$\partial_d\partial_cv^a+\left(\partial_d\Gamma_{bc}^a\right)v^b-\Gamma_{cd}^b\partial_bv^a-\Gamma_{cd}^b\Gamma_{eb}^av^e+\Gamma_{bd}^a\Gamma_{ec}^bv^e+\Gamma_{bc}^a\partial_dv^b+\Gamma_{bd}^a\partial_cv^b$}$$ The eqns ( $\mathrm{i}$ - $\mathrm{k}$ ) simply represent expanded forms of the each of the terms in the equations above. I didn't want to resort to doing this and it was tedious to type this all out in full, but I did it for a reason: I am beginning to doubt myself on the validity of the boxed expressions I have worked out. For instance, what is to stop me writing the last term of the 2nd boxed expression as $\Gamma_{cd}^b\partial_bv^a$ ? This term has the same contravariant index as the LHS, $\nabla_d\nabla_cv^a$ , namely, $a$ and the same covariant indices, $d$ and $c$ on the LHS. So I know this may seem like a question in its' own right, but I consider it to be pertinent to this post, so does this mean that $\Gamma_{bd}^a\partial_cv^b=\Gamma_{cd}^b\partial_bv^a$ ? I never thought they could be equal but now I'm not so sure as the live indices are all preseved. Final remarks: Due to work commitments, I was unable to respond to this post or even self-study until now. My apologies for this and for an excessively long post, I laboured the point somewhat to convey my confusion in the most effective way I could think of. For these reasons I think that placing a bounty on this question is the least I could do. 2nd update addressing recent answers/comments: I see some very nice answers emerging, which, for the most part answers my main question; how to obtain eqn $(\mathrm{c})$ from $(\mathrm{b})$ . I would like to thank all participants for your kind help so far. One of the questions I asked that did go overlooked, was What is to stop me from writing the last term of the 2nd boxed expression as $\Gamma_{cd}^b\partial_bv^a$ ? This term has the same contravariant index as the LHS, $\nabla_d\nabla_cv^a$ , namely, $a$ and the same covariant indices, $d$ and $c$ on the LHS? In short, does the 7th (final) term of the second boxed expression satisfy $$\Gamma_{bd}^a\partial_cv^b=\Gamma_{cd}^b\partial_bv^a\tag{?}$$ If that was too vague, then put another way, am I at liberty to switch around whichever tensor indices I like (even if they belong to differential operators, etc), with the caveat that I preserve the overall labelling of the contravariant and covariant indices? If any term in the expression on the LHS or RHS has a contravariant index $a$ and covariant indices $d$ and $c$ as long as this is maintained throughout all other terms may I switch the indices to different factors of the terms ( $\Gamma_{bd}^a\partial_cv^b=\Gamma_{cd}^b\partial_bv^a$ )?","['riemannian-geometry', 'proof-explanation', 'connections', 'tensors', 'calculus']"
4497028,"Given an algebraic number $a$, find the closed form of $\arctan (a)$","More precisely: Given an algebraic number $a\ge0$ , can we determine if there exists a rational number $b$ such that $$\arctan (a)=\int_0^a \frac{dx}{x^2+1}=\pi b?$$ If so, can we find the rational number $b$ ? Examples: $$\arctan (2-\sqrt{3})=\frac{\pi}{12}$$ $$\arctan \left(\sqrt{5-2\sqrt{5}}\right)=\frac{\pi}{5}$$ Note: I know how to do the ""reverse procedure"" (i.e. given a rational number $b$ , find the algebraic number $\tan (\pi b)$ ).","['trigonometry', 'definite-integrals', 'closed-form']"
4497031,Analytical solution for length of star trails?,"Assuming there is an (infinitely) large sphere (the sky) that rotates counterclockwise as seen from above around its axis of rotation (which goes through its center point) at a fixed rate Ï‰_sky, with a point (star) on its inner surface at 0Â° â‰¤ Ï†_star < 360Â°, -90Â° â‰¤ Î¸_star â‰¤ 90Â°. In the center, a vector (a startracker) is aligned with the axis of rotation toward the north pole, but misaligned by a degree (say, Î¸_tracker = 90Â°- ð›¿, ð›¿=1Â°, Ï†_tracker=0Â°) and rotates at the same rate and in the same direction as the sky. Finally, a vector (camera) is fixed to the rotation of that vector, but is initally pointed to the star. Over a duration (full rotation of both sky + (tracker+camera)), from the perspective of the camera, the point/star describes a closed path. Viewed from the outside of the sphere, fixed on the changing point on the sphere the camera points to (red dot): From the inside, showing how the star/fixed point on the sphere changes position from the perspective of the camera: Which when stacked in a long exposure can look like this: Typical exposures are much shorter, so their resulting trail is just a very short segment of this closed path. Is it possible to calculate the length of this path (star trail) in degrees over a certain range of (exposure) time analytically, for a given star position and misalignment ð›¿?","['integration', 'calculus', 'angle', 'spheres']"
4497050,A functor $(Sch)^{op}\to(Sets)$ can not represented by a scheme?,"This is M. Olsson's book ""Algebraic Spaces and Stacks"". Exercise 1.D.(b): 1.D.(a) Let $$\mathbb{A}^n-\{0\}:(Sch)^{op}\to(Sets)$$ be a functor sending a scheme $Y$ to the set of $n$ -tuples $(y_1,...,y_n)$ of sections of $\Gamma(Y,\mathscr{O}_Y)$ such that for all $y\in Y$ , the image of $y_i$ in field $\kappa(y)$ are not all zero. Show that $\mathbb{A}^n-\{0\}$ is representable; (b) Let $$\mathbb{A}^n-\{0\}/\mathbb{G}_m:(Sch)^{op}\to(Sets)$$ be a functor sending a scheme $Y$ to the quotient of the set $(\mathbb{A}^n-\{0\})(Y)$ by equivalence relation $$(y_1,...,y_n)\sim(y_1',...,y_n')$$ if there exists $u\in\Gamma(Y,\mathscr{O}_Y^*)$ s.t. $y_j=uy_j'$ for all $j$ . Show that $\mathbb{A}^n-\{0\}/\mathbb{G}_m$ is NOT representable. I have solved (a) but I don't know how to prove a functor is not representable. Like (b). Thank you for your help!!",['algebraic-geometry']
4497060,"$f=f(u)$ and $u=u(x,t)$, does partial derivatives be exchanged, $\frac{d}{dx}\left(\frac{df}{du}\right) = \frac{d}{du}\left(\frac{df}{dx}\right)$?","I am working with compressible Navierâ€“Stokes equations. When calculating the derivatives to linearize the equation, I got confused about one term. If $f=f(u)$ , and $u=u(x,t)$ , are the two derivatives equal, i.e., $\frac{d}{dx}\left(\frac{df}{du}\right) = \frac{d}{du}\left(\frac{df}{dx}\right)$ ? I've tested a simple scalar case, $f=u^2$ and $u=\sqrt x$ . In this case, the partial derivatives are not exahcngeable. But I was wondering if there is any requirement, by satisfying which makes them exchangable?","['calculus', 'partial-differential-equations', 'derivatives', 'mathematical-physics', 'fluid-dynamics']"
4497063,"Prove/Disprove $|1 + z_1| + |1 + z_2| + |1 + r z_1 z_2| \ge 1 + \min(r, 1/r)$","Let $r$ be a positive real number.
Let $z_1, z_2 \in \mathbb{C}$ . Prove or disprove that $$|1 + z_1| + |1 + z_2| + |1 + r z_1 z_2| \ge 1 + \min(r, 1/r).$$ I came up with this problem after I saw this question ( $r = 1$ ): Prove $\left|1+z_1\right| +\left|1+z_2Â \right| + \left|1+z_1z_2\right|\geq 2$ I used Mathematica to do some numerical experiment which supports the claim. Also, if $0 < r < 1$ , $\mathrm{LHS} = \mathrm{RHS}$ occurs when $z_1 = -1, z_2 = -1$ ; if $r \ge 1$ , $\mathrm{LHS} = \mathrm{RHS}$ occurs when $z_1 = -1, z_2 = 1/r$ .","['complex-analysis', 'inequality', 'complex-numbers']"
4497065,Exponential Hindman's theorem,"I'm not familiar with the combinatorial proof of Hindman's theorem, but the proof using ultrafilters is quite slick and essentially only relies on the associativity of addition. As such, it also automatically gives the multiplicative version of Hindman's theorem: that for every $F:\mathbb{N}\rightarrow\{1,...,k\}$ there is an infinite $A\subseteq\mathbb{N}$ such that $F$ is homogeneous on the set of products of distinct elements of $A$ . My question is whether the exponential analogue of Hindman's theorem is true: Is it the case that, for every $F:\mathbb{N}\rightarrow\{1,...,k\}$ , there is an infinite $A\subseteq\mathbb{N}$ such that $$\mathsf{FinExp}(A):=\{x_1^{x_2^{.^{.^{.^{x_k}}}}}: x_1,...,x_k\in A\mbox{ distinct}\}$$ is $F$ -homogeneous? Certainly the ultrafilter-based proof breaks down: we can define a version of exponentiation in $\beta\mathbb{N}$ , but it isn't a semigroup so we don't get idempotents. (EDIT: And I don't even see how to prove that, for every $F:\mathbb{N}\rightarrow\{0,1\}$ , there are $a<b$ such that $\{a,b,a^b,b^a\}$ is $F$ -homogeneous.) On the other hand, I don't see how to construct a counterexample (and the numbers get sufficiently big sufficiently quickly that experimentation hasn't been much help either).","['exponentiation', 'logic', 'combinatorics']"
4497078,How to Calculate the Product of Summations [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let $X_1, \dots X_n \overset{i.i.d.}{\sim} \mathbb{E}[X_i] = \mu, \mathbb{V}[X_i] = \sigma^2, \mu\in\mathbb{R}, \sigma^2 > 0, \mathbb{E}[(X_i)^4] < \infty.$ $$
\mathbb{E}\left[\sum_{i=1}^n X_i\sum_{j=1}^n X_j\sum_{k=1}^n X_k \sum_{l=1}^n X_l \right]\\
= \sum_{i=1}^n \mathbb{E}\left[(X_i)^4 \right] + 4\sum_{i=1}^n \sum_{l\neq i}^n\mathbb{E}\left[(X_i)^3X_l \right] + 3\sum_{i=1}^n \sum_{j\neq i}^n\mathbb{E}\left[(X_i)^2(X_j)^2 \right]\\ + 6\sum_{i=1}^n \sum_{j\neq i}^n \sum_{k\neq i,j}^n\mathbb{E}\left[(X_i)^2 X_j X_k \right] + \sum_{i=1}^n \sum_{j\neq i}^n \sum_{k\neq i,j}^n \sum_{l\neq i,j,k}^n \mathbb{E}\left[X_i X_j X_k X_l \right]
$$ In the expansion of this equation, $4$ , $3$ and $6$ appear in the coefficients, but I do not know how to find these coefficients.","['statistics', 'summation', 'probability']"
4497084,"Finding the vertices of a regular pentagon centred around 0,0","I would like to preface this question by saying that I'm not the best at maths. Thanks to another post here , I was able to find the corners of a rotated square centred on 0,0. Now however, I need to be able to find the vertices of a regular pentagon where the length of each side and the rotation of the pentagon are variable. I was able to get basic equations for each coordinate, as follows (where L = side length): $c_1 = L\cos\left(\frac{2\pi}{5}\right)$ $c_2 = L\cos\left(\frac \pi5\right)$ $s_1 = L\sin\left(\frac{2\pi}{5}\right)$ $s_2 = L\sin\left(\frac{4\pi}{5}\right)$ $P_1 = (0, L)$ $P_2 = (s_1, c_1)$ $P_3 = (s_2, -c_2)$ $P_4 = (-s_2, -c_2)$ $P_5 = (-s_1, c_1)$ How would I factor in rotation of the pentagon into this?","['trigonometry', 'geometry']"
4497094,Prove that [UVT]/[URS]=1/10,"Given $\triangle QUS$ , let R be a point on $\overline{QS}$ such that $QR = RS$ and T be a point on $\overline{US}$ such that $ST=3UT$ . Also, note that $\overline{UR}$ and $\overline{QT}$ intersect at V. Prove that $\frac{[UVT]}{[URS]}$ = $\frac{1}{10}$ . Note: When I presented this to a friend, he suggested that I use mass points. However, I am not yet familiar with the topic so I am hoping to solve this using more elementary concepts such
as similar triangles or area ratios. Edit: Thanks for your comment, @Ivan Kaznacheyeu. There were indeed typos to my question. I've edited it for clarity.",['geometry']
4497098,"Spivak, Ch. 15, Problem *26c: show $\lim\limits_{\lambda\to \infty} \int_a^b f(x)\sin{(\lambda x)}dx=0$ for any $f$ integrable on $[a,b]$.","The following problem is from Ch. 15 of Spivak's Calculus . My question is about understanding the solution manual solution to item $(c)$ . I've asked a separate question about my own attempt at a solution which differs from the solution manual . *26. It is an excellent test of intuition to predict the value of $$\lim\limits_{\lambda\to \infty} \int_a^b f(x)\sin{(\lambda x)}dx$$ Continuous functions should be the most accessible to intuition, but
once you get the right idea for a proof the limit can easily be
established for any integrable $f$ (a) Show that $\lim\limits_{\lambda\to \infty} \int_a^b \sin{(\lambda
 x)}dx=0$ , by computing the integral explicitly. (b) Show that if $s$ is a step function on $[a,b]$ , then $\lim\limits_{\lambda\to \infty} \int_a^b s(x)\sin{(\lambda x)}dx=0$ . (c) Finally, use Problem 13-26 to show that $\lim\limits_{\lambda\to
 \infty} \int_a^b f(x)\sin{(\lambda x)}dx=0$ for any function $f$ which
is integrable on $[a,b]$ . This result, like Problem 12, plays an
important role in the theory of Fourier series; it is known as the
Riemann-Lebesgue Lemma. Item $(a)$ is just the limit of a definite integral $$\lim\limits_{\lambda\to \infty} \int_a^b \sin{(\lambda x)}dx$$ $$=\lim\limits_{\lambda\to \infty} -\frac{1}{\lambda}\left ( \cos{(\lambda d)}-\cos{(\lambda c)} \right )=0$$ For item $(b)$ , note that a step function is defined based on a partition $P=\{t_0,...,t_n\}$ of $[a,b]$ . Then $$\lim\limits_{\lambda\to \infty}\int_a^b s(x)\sin{(\lambda x)}dx=\lim\limits_{\lambda\to \infty} \sum_{i=1}^n s_i\int_{t_{i-1}}^{t_i} \sin{(\lambda x)}dx=0$$ For the items above, my solution coincided with the solution manual. For item $(c)$ my solution was different . For now I'd like to understand the solution manual solution (specifically the last part of it). Here it is Solution Manual Solution to $(c)$ For any $\epsilon>0$ there is, by Problem 13-16, a step function $s\leq f$ with $$\int_a^b [f(x)-s(x)]dx<\epsilon$$ Now $$\left | \int_a^b f(x)\sin{(\lambda x)}dx-\int_a^b s(x)\sin{(\lambda
 x)}dx \right |\tag{1}$$ $$=\left | \int_a^b [f(x)-s(x)]\sin{(\lambda x)} dx \right
 |$$ $$\leq \int_a^b [f(x)-s(x)]\cdot |\sin{(\lambda x)} |dx$$ $$\leq \int_a^b [f(x)-s(x)]dx<\epsilon\tag{2}$$ So far so good Part $(b)$ then shows that $$\lim\limits_{\lambda\to \infty} \left | \int_a^b f(x)\sin{(\lambda
 x)}dx \right |<\epsilon$$ Since this is true for every $\epsilon>0$ , the limit must be $0$ . I'd like to understand this last part. I think what happened is that from $(1)$ we had $$\left | \int_a^b f(x)\sin{(\lambda x)}dx\right | -\left | \int_a^b s(x)\sin{(\lambda
 x)}dx \right |\leq\left | \int_a^b f(x)\sin{(\lambda x)}dx-\int_a^b s(x)\sin{(\lambda
 x)}dx \right |<\epsilon$$ and then we took the limit $$\lim\limits_{\lambda\to \infty}\left | \int_a^b f(x)\sin{(\lambda x)}dx\right | -\lim\limits_{\lambda\to \infty}\left | \int_a^b s(x)\sin{(\lambda
 x)}dx \right |<\epsilon$$ But since from $(b)$ we know that $\lim\limits_{\lambda\to \infty}\left | \int_a^b s(x)\sin{(\lambda
 x)}dx\right |=0$ we have $$\lim\limits_{\lambda\to \infty}\left | \int_a^b f(x)\sin{(\lambda x)}dx\right |<\epsilon$$ Is this filling in of the steps correct?","['integration', 'proof-explanation', 'calculus', 'limits', 'derivatives']"
4497118,"When $f(z) = 1/z$ and $g(z) = 1-z$, why is $f \circ g \circ f \circ g \circ f \circ g (z) = g \circ f \circ g \circ f \circ g \circ f (z) = z$?","A recent contest question I was attempting involved a long string of repeated and random applications of the two complex functions $f(z) = 1/z \,$ and $\, g(z) = 1-z \,$ to an unknown complex number other than $0$ or $1$ . Since $f$ and $g$ are obvious involutions, such a string could be reduced to a string of alternating applications of $f$ and $g$ . When I started to analyze such a string, I quickly came across the following: $$f \circ g \circ f \circ g \circ f \circ g \, (z) = g \circ f \circ g \circ f \circ g \circ f \, (z) = z$$ This may be a well-known property of these two functions, but I'd never come across it before and I was quite surprised by it, in particular that it took three iterations of each to become an identity - somehow I think I would have been less surprised if it had taken two or four iterations. The algebra is quite straightforward; it's easy to see that applying the first of the two above compositions gives $$z \to \frac{1}{z} \to \frac{z-1}{z} \to \frac{z}{z-1} \to \frac{1}{1-z} \to 1-z \to z$$ and that the second gives the same chain in reverse. However, this gave me no insight into why three is the magic number. I tried looking at this geometrically, but my complex geometry is admittedly pretty weak. As best as I could see, $f(z)$ reflects a point in the real axis and then inverts the image in the circle $r=1$ ; and $g(z)$ reflects a point in the line $x=\frac{1}{2}$ and then reflects the image in the real axis. Since for either function the order of the two transformations can be interchanged, when applying one function followed by the other the two reflections in the real axis would cancel each other out and result in an inversion in the circle $r=1$ followed by a reflection in the line $x=\frac{1}{2}$ , or vice-versa. However, this did not help as I could not see why doing this three times would return a point to its original position. Even restricting to just real values didn't lead to any enlightenment as the transformations would still be essentially the same. So, is there some not-too-complicated way to see why it is that three alternating iterations of each of these two functions is an identity?","['complex-analysis', 'functions']"
4497192,Find a closed form for :$\int_{0}^{1}\frac{x\log{x}\log^2{(1-x)}}{1+x^2}dx$,"First of all, hello everyone, i am a new in  MSE community. I hope you guys all well.
I found this integral on a web: $$\int_{0}^{1}\frac{x\log{x}\log^2{(1-x)}}{1+x^2}dx$$ I tried to IBP with $u=\log{x}\log^2{(1-x)}$ and $dv=\frac{x}{1+x^2}$ it to get: $$\int_{0}^{1}\frac{\log{x}\log{(1-x)}\log{(1+x^2)}}{1-x}dx-\frac{1}{2}\int_{0}^{1}\frac{\log{(x^2+1)}\log^2{(1-x)}}{x}dx$$ From here I got stuck, i know may be I misdirected. I tried to search our forum and found this link seems similar. The technique is unique, but I don't know how to apply it for my case.
Can you guys give some advices or ideas? Thank you so much.","['integration', 'improper-integrals', 'special-functions', 'calculus', 'sequences-and-series']"
4497216,Existence of a negative eigenvalues for a certain symmetric matrix,"Given $x>0$ and \begin{bmatrix}
1 & 1+x & 1\\
1+x & 1 & 1+x\\
1 & 1+x & 1
\end{bmatrix} Show that there is exactly one positive and one negative eigenvalue. I solved this problem. This is easy to do by simply computing the characteristic polynomial, $0$ is an eigenvalue and two others of different signs. Now, I want to generalize this to $n \times n$ case ( $x>0$ ). Note that the $n \times n$ analogue will have rank $2$ and hence exactly two non-zero eigenvalues both of which are real because the matrix is symmetric. Now also the trace is $n$ so there is a positive eigenvalue. Now I want to show that there is a negative eigenvalue. Computing the characteristic polynomial will be clumsy now, even though it would finally look like $t^{n-2}(t^2 +at +b)$ . If both the other roots are positive then we have that the spectral radius is strictly less than $4$ . Can we use this?","['matrices', 'linear-algebra', 'symmetric-matrices', 'eigenvalues-eigenvectors']"
4497218,Doubt in a differential equation (solution given),"Let $y=y(x)$ be the solution of the differential equation $\frac{\mathrm{d} y}{\mathrm{~d} x}+\frac{\sqrt{2} y}{2 \cos ^{4} x-\cos 2 x}=x \mathrm{e}^{\tan ^{-1}(\sqrt{2} \cot 2 x)}, 0<x<\pi / 2$ with $y\left(\frac{\pi}{4}\right)=\frac{\pi^{2}}{32} .$ If $y\left(\frac{\pi}{3}\right)=\frac{\pi^{2}}{18} \mathrm{e}^{-\tan ^{-1}(\alpha)}$ , then the value of $3 \alpha^{2}$ is equal to Source : Jee Main 2022 29th June shift 1 Now I  started off by simplifying the trigonometric expression(s) and then searching for the DE. Simplfying the DE I simplified the trignometric term in the denominator by the following sequence of equalities: $$ \begin{align}2(\cos x)^4 -\cos2x &=2(\cos x)^4 -2(\cos x)^2+1 \\  &= 1-2(\sin x)^2(\cos x)^2\\ &= 1-\frac{(\sin2x)^2}{2} \end{align}$$ My differential equation simplifies as: $$\frac{dy}{dx} + \frac{\sqrt{2}y}{1-\frac{(\sin2x)^2}{2}} =xe^{\tan^{-1}{\sqrt{2}\cot2x}}              $$ on further simplification I reached this $$ \frac{dy}{dx} + \frac{4\sqrt{2}y}{3+\cos4x} =xe\large^{\tan^{-1}{\sqrt{2}\cot2x}} $$ Finding IF We have by the standard formulas, $$IF=  e^{\int\frac{4\sqrt{2}}{3+cos4x}dx } $$ here I use substitution ( I have put $\tan2x=t$ )
so that $ \frac{dt}{2(1+t^2)}$ =dx and I wrote $\cos4x$ as $ \large \frac{1-t^2}{(1+t^2)}$ Doing everything gives me IF as $$IF= \huge e^{\tan^{-1}(\frac{t}{\sqrt{2}})} $$ now t was tan2x so  my equation becomes (when we multiply IF on both sides of the equation) $$\large d(y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})})=xe^{\tan^{-1}{\sqrt{2}cot2x}+\tan^{-1}(\frac{tan2x}{\sqrt{2}})} dx $$ now the solution given to me was that I should convert $\tan^{-1}{\sqrt{2}cot2x}+\tan^{-1}(\frac{tan2x}{\sqrt{2}})$ into $\frac{\pi}{2}$ I think that this step is wrong as this is only valid if x was in $0<x<\frac{\pi}{4}$ as then $2x$ would be less than $\frac{\pi}{2}$ We know that $\cot^{-1}{x}-\pi =\tan^{-1}{\frac{1}{x}}$ when $x<0$ so we can't say that its pi/2 and integrate it. BTW if you assume it to be pi/2 the answer matches to what was given to me. Edit: I think it should be like this: $for0<x<\frac{\pi}{4}$ $$\large y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\frac{x^2}{2} e^{\frac{\pi}{2}} +c$$ and $for\frac{\pi}{4}<x<\frac{\pi}{2}$ $$\large y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\frac{x^2}{2} e^{\frac{-\pi}{2}} +c'$$ Final conclusion : Thanks to Mr IvanKaznacheyeu for pointing out that I only needed the second one of my 2 solutions for solving the problem. Here for completely solving the problem: We consider 2nd part of my answer. Here taking limits on both sides to solve for c'. $$\large \lim\limits_{0\to \frac{\pi}{4}+} y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\lim\limits_{0\to \frac{\pi}{4}+}\frac{x^2}{2} e^{\frac{-\pi}{2}} +c'$$ solving the above will give c=0 and finally when we put x=pi/3 after some rearrangement this yields $\alpha$ =( $\sqrt\frac{2}{3}$ ) hence answer is 2","['solution-verification', 'ordinary-differential-equations']"
4497258,$L^{p}$ convergence equivalent condition,"I have to show that for $p\in[0,\infty)$ , $f_{n},f\in L^{p}(\mathbb{R})$ , (i) $f_n\rightarrow f$ in $L^{p}([-N,N])$ for all $N\in \mathbb{N}$ and (ii) $\lvert\lvert f_{n}\rvert\rvert \rightarrow \lvert\lvert f\rvert\rvert$ implies $f_{n}\rightarrow f$ in $L^{p}(\mathbb{R})$ . My approach was to first deconstruct $\lvert\lvert f_{n}-f\rvert\rvert_{p}^{p}$ as $$
\lvert\lvert f_{n}-f\rvert\rvert_{p}^{p}=\int_{-\infty}^{-N}\lvert f_{n}-f\rvert^{p}+\int_{-N}^{N}\lvert f_{n}-f\rvert^{p}+\int_{N}^{\infty}\lvert f_{n}-f\rvert^{p}.
$$ (The integral is taken w.r.t. the Lebesgue measure). By (i) we know that the above decomposition is valid for all $N\in \mathbb{N}$ and as we take the limit $n\rightarrow \infty$ , the middle term vanishes. The problem is how to get an upper bound $\epsilon_{n}$ for the left and right integral s.t. $\epsilon_{n}\rightarrow 0$ as $n\rightarrow \infty$ . I don't see how we could use (ii) for that. I tried to show that  for all $n$ sufficiently large we get an $N^{*}$ such that the left and right integral are bounded and that this bound approaches $0$ as $n \rightarrow \infty$ , but didn't succeed. Could anyone help?","['lp-spaces', 'lebesgue-integral', 'functional-analysis', 'real-analysis']"
4497261,$\cos \left(\frac{2\pi }{7}\right)^{\frac{1}{3}}+\cos \left(\frac{4\pi }{7}\right)^{\frac{1}{3}}+\cos \left(\frac{6\pi }{7}\right)^{\frac{1}{3}} =?$,"I have been trying to solve the following question for a long time: Find $a,b,c,d$ such that: ( $a,b,c,d$ are primes) $\cos \left(\frac{2\pi }{7}\right)^{\frac{1}{3}}+\cos \left(\frac{4\pi }{7}\right)^{\frac{1}{3}}+\cos \left(\frac{6\pi }{7}\right)^{\frac{1}{3}} = \left(\frac{a-b\sqrt[3]{c}}{d}\right)^{\frac{1}{3}} $ What I have tried so far is that: Let $x=\cos \left(\frac{2\pi }{7}\right), y=\cos \left(\frac{4\pi }{7}\right),z=\cos \left(\frac{6\pi }{7}\right)$ , then $x+y+z=-1/2$ , $xyz =1/8 $ $xy+yz+zx=-1/2$ So, $x,y,z$ are the roots of $8t^3+4t^2-4t-1=0$ and can be found by Cardan's method and hence $x^{1/3}+y^{1/3}+z^{1/3}$ can be found. However that method was way too long and it seems that the question may have a more elegant solution (given the form). Also, I tried to cube both sides of the equation in the question and substitute using the above three relations, but that required the value of $x^{2/3}+y^{2/3}+z^{2/3}$ , which I coundn't find. Would someone pls help me out (without using Cardan)? Thanks in advance!","['algebra-precalculus', 'trigonometry']"
4497305,Existence and uniqueness for IVP with a square root,"This question is from an old exam that a classmate shared with me. Question: Determine if the following IVP have a solution and if it is unique or not. i) $$\left \{\begin{matrix}y'(s) & = & \sqrt{y(s)+s}, \\ y(0) & = & x,\end{matrix}\right .$$ where $x$ is a given real number. ii) $$\left \{\begin{matrix}y_1'(s) & = & y_1(s)y_2(s), \\ y_2'(s) & = & y_1(s)+\sqrt{s}, \\ y_1(0) & = & x_1, \\ y_2(0) & = & x_2,\end{matrix}\right .$$ where $x_1,x_2$ are given real numbers. Ideas/Attempt: For i) Wolfram gives a solution using Lambert's $W$ -function, and this isn't something I've been taught (or know) about yet. For ii) I know how to solve for $y_1$ as a function of $s$ and $y_2$ (integrating factor, linear diff eq, etc.), but I don't know how to prove/disprove the existence of a solution. I'll like to solve this using Peano or Picard-LindelÃ¶f to prove existence and uniquness if possible, but I'm confused because both theorems have as hypothesis that the function $f(s,y)$ is defined in an open neighbourhodd of the point determined by the initial conditions, in this case, $(0,\text{something})$ . What should I do in this case? Is there some kind of (relatively common) trick that works for differential equations involving square roots (or derivatives squared) that I should be aware of? Thank you in advance.",['ordinary-differential-equations']
4497321,Suggested books on Leibniz notation (differentials)?,"I'm a freshman in Computer Engineering (but will probably switch to Math very soon) and I've followed some course in Analysis 1 and 2. I found the concept of differentials really interesting, but I've never really grasped the real meaning of the Leibniz notation, such as in: $$\int {x\ dx} \ \ \ \ \ \ \ \ \ \frac{dy}{dx}$$ In integrals I think of it as a multiplication between a quantity $x$ and another one $dx$ . This machine is building small rectangles that will add up to an area. I've studied the integral definitions and I know that there is definitely more behind it (particularly sums), but this is a useful representation that I have in my mind that allow me to conceptualize problems, especially in phisics. In derivatives I see $\frac{dy}{dx}$ as a division, between something that I want to calculate ( $dy$ ), that is getting smaller and smaller, and an inifnitesimal distance $dx$ . But, especially for derivates, I don't really get how is it possible for the ratio of two infinitesimals to output a real number. I know that this is involved with limits, and limits can do these magical things, but it's just not intuitive for me. Not considering the last doubt, I think that there is much more about the concept of differential that I do not really know. Thus, considering our beautiful math sections we have in our university library, I'm searching for book suggestions about this awesome topic to learn something more about it!
Thanks.","['integration', 'differential', 'book-recommendation', 'calculus', 'derivatives']"
4497323,Convergence of this finite difference sum on a small lattice,"Consider the unit box $Q_1=[0,1]^3$ and the associated network of size $\epsilon$ : $\epsilon \mathbb{Z}^3 \cap Q_1$ . Let $W_\epsilon$ the set of functions $v : \mathbb{R}^3 \rightarrow \mathbb{R}$ such that for every $i \in \mathbb{Z}^3$ , there exists $v_i \in \mathbb{R}$ with $$v(i \epsilon +y \epsilon)= v_i \text{ for all } y \in \mathring{Q_1}$$ (understand that $v$ is piece-wise constant on all cubic tiles of the network $\epsilon \mathbb{Z}^3$ ).
We then define the following space by restricting functions of $W_\epsilon$ to $Q_1$ : $$V_\epsilon=\{ v|_{Q_1}, \ v \in W_\epsilon\}$$ I'm considering the following sequence $(u_\epsilon)_\epsilon$ such that : $u_\epsilon \in V_\epsilon$ for all $\epsilon > 0$ $u_\epsilon$ converges toward a linear profile $x \mapsto \xi \cdot x$ in $L^2(Q_1)$ as $\epsilon$ goes to zero, where $\xi$ is a given vector in $\mathbb{R}^3$ . I would like to understand the following limit if it exists : $$\underset{\epsilon \rightarrow 0}{\lim} \ \epsilon \sum_{(x,y)\ \in (\epsilon \mathbb{Z}^3 \cap Q_1)^2 \\ \quad |x-y|=\epsilon} |u_\epsilon(x)-u_\epsilon(y)|^2$$ which is just a sum of finite differences over all neighboring couple $(x,y)$ on the lattice. With a quick formal analysis, we have $|u_\epsilon(x)-u_\epsilon(y)|^2 \approx |\xi \cdot(x-y)|^2 \approx |\xi|^2 \epsilon^2$ and the sum becomes $$\epsilon^3  \sum_{(x,y)\ \in (\epsilon \mathbb{Z}^3 \cap Q_1)^2 \\ \quad |x-y|=\epsilon} |\xi|^2 \approx \epsilon^3 \frac{|Q_1|}{\epsilon^3} |\xi|^2 \approx |\xi|^2 |Q_1|,$$ so much that I expect the limit to be $|\xi|^2 |Q_1| = |\xi|^2$ . However, I'm struggling to prove rigorously the result and it might be false. Do you have any clue on how to compute this limit ? For what matters, I'm also fine having just a lower estimate for the liminf of this quantity, something in this flavor: $$\underset{\epsilon \rightarrow 0}{\liminf} \ \epsilon \sum_{(x,y)\ \in (\epsilon \mathbb{Z}^3 \cap Q_1)^2 \\ \quad |x-y|=\epsilon} |u_\epsilon(x)-u_\epsilon(y)|^2 \geq C |\xi^2| |Q_1|.$$ Thanks for your help.","['network', 'approximation', 'finite-differences', 'sequences-and-series']"
4497352,Finding the number of intersections of a function and $y = x$,"Let $f(x)$ be a real valued function defined for all real numbers $x$ such that $|f(x) -f(y)| \leq \frac{1}{2} |x-y|$ for all x,y. Then the number of points of intersection of the graph of $y = f(x)$ and the line $y = x$ is A) $0$ $\boxed{B)1}$ C) $2$ D)none of the foregoing numbers. My Method: Consider $x = y + h$ ,
We have: $|f(y+h) - f(y)| \leq \frac{1}{2} |h|$ $\implies \frac{|f(y+h) - f(y)|}{|h|} \leq \frac{1}{2}$ $\implies \lim_{h \rightarrow 0} \frac{|f(y+h) - f(y)|}{|h|} \leq \frac{1}{2}  \hspace{2cm}     \ldots (1)$ $\implies |f'(y)| \leq \frac{1}{2}$ $\implies \frac{-1}{2} \leq f'(y) \leq \frac{1}{2}$ $\implies \frac{-y}{2} \leq f(y) \leq \frac{y}{2}  \hspace{2cm} \ldots (2)$ Thus we have, $ f(x) \in [\frac{-x}{2} , \frac{x}{2}]$ for $x \in \mathbb{R}$ , The graph of $f(x)$ must lie within the shaded portion and thus we have no solutions for $x>0$ but from the bounds it is clear that $f(0) = 0$ and we have one intersection at $(0,0)$ . The shaded portion is $ \frac{-x}{2} \leq y \leq \frac{x}{2}$ and the green line is $y = x$ , Which gives us a total of $1$ intersection. But would limit in $(1)$ necessarily exist and can $(2)$ be directly written from the previous step. Would be glad if the community could come up with any alternative approaches (using elementary methods).","['calculus', 'functions', 'solution-verification']"
4497358,"Proof that When all the sides of two triangles are congruent, the angles of those triangles must also be congruent (Side-Side-Side Congruence)","I am studying triangle congruence. It is a very basic subject and maybe because of that I am having a very hard time finding more rigorous explanations. I am specifically interested in the Side-Side-Side Congruence . I know that ""When all the sides of two triangles are congruent, the angles of those triangles must also be congruent"" . But I can not find a proof for that. Can anyone provide me a proof?","['euclidean-geometry', 'triangles', 'geometry']"
4497376,Absolutely continuous push forward measure,"Consider a measure space $(E,\mathcal{B}(E),\mu)$ and let $f:E\to E$ be a measurable mapping. The push forward of the measure $\mu$ by the mapping $f$ is defined to be $$ f_\#\mu(A) = \mu(f^{-1}(A)) \quad \text{for all } A\in\mathcal{B}(E). $$ My question is the following: what regularity do we need to assume on $f$ to make sure that $f_\#\mu\ll\mu$ ? I was writing this question when I found this other post that helped a lot. But now I am wondering, for the Lebesgue measure on $\mathbb{R}^n$ , instead of locally Lipschitz mappings, could we relax this condition to locally HÃ¶lder with some $\alpha \in (0,1)$ ? Thanks in advance!","['measure-theory', 'functional-analysis', 'real-analysis']"
4497434,Find the value $\alpha$ so the random walk is recurrent,"Consider a random walk on $\mathbb{Z}$ with step distribution $$\mathbb{P}(X_1 = n) = \frac{1}{2}\left ( \frac{1}{|n|^{\alpha}} - \frac{1}{(|n| + 1)^{\alpha}} \right ), \ \ \ \ n \neq 0.$$ I am trying to find the values $\alpha >0$ that makes the random walk recurrent.  I know that, if the random walk starts at $1$ , I need to show that $$\sum_{n=1}^{\infty}\mathbb{P}(S_n = 1) = \infty.$$ However, I cannot find a closed formula for the return probability. For example, if the random walk starts at $1$ , then $\mathbb{P}(S_2 = 1)$ is easy to find since that means the random walk went up $n$ steps and then went down $n$ steps. But for $n>2$ it becomes much more complicated. Can anyone help? Thanks!","['probability-theory', 'random-walk']"
4497470,Expected value in coin flipping process,"You flip a coin, and if the result is tails, you lose. If the result is heads, you get to play again. What is the expected value of throws before you lose? My Approach The expected value is the sum of all the outcomes multiplied by their respective probabilities: $$\sum_{i=1}^{n}V_iP_i$$ So for this problem: $$\sum_{i=1}^{\infty}i(\frac{1}{2^i})=0.5+0.5+0.375+â€¦$$ I canâ€™t figure out how to find the sum, even though I know it converges.","['convergence-divergence', 'probability', 'sequences-and-series']"
4497478,How does $G\frac{\frac{1}{2}m_{1}\cdot m_{2}}{\frac{1}{4}r^2}$ become $8\left(G\frac{m_{1}\cdot m_{2}}{r^2}\right)$?,"I'm busy doing a highschool physics exercise which involves some algebra, and I'm having trouble seeing how they got from one step to the next. Here's what it looks like: $$
...\\
F_{g}=G\frac{\frac{1}{2}m_{1}\cdot m_{2}}{\frac{1}{4}r^2}\\
F_{g}=8\left(G\frac{m_{1}\cdot m_{2}}{r^2}\right)\\
...
$$ I just can't figure out how they got from the first line to the next? Like, I can factor out the $\frac{1}{2}$ from the numerator, ending up with $F_{g}=\frac{1}{2}(G\frac{m_{1}\cdot m_{2}}{\frac{1}{4}r^2})$ , but I'm stuck on how to get the $\frac{1}{4}$ out of the denominator. Here's a screenshot of it from my book. I noticed that the numerator doesn't actually have the second mass, like how I copied it over to my question, but I'm handling that as a typo lol. P.S. I'm not sure how to tag this question. Feel free to retag it, if there's better ones.",['algebra-precalculus']
4497492,On Tanaka's SDE,"Throughout this question, for an arbitrary real valued continuous time stochastic process $Y$ , I define $\{\mathcal{F}_t^Y\}_{t \ge 0}$ to be the filtration generated by $Y$ .  I also define $\text{sgn}(x) = \mathbf{1}(x > 0) - \mathbf{1}(x \leq 0)$ The standard argument that Tanaka's SDE has no strong solutions for a particular probability space is as follows, but I do not understand the last part of it: it suffices to consider a probability space $(\Omega, \mathcal{F}, P)$ on which there exists a Brownian motion $B$ , and we define our filtration $\{\mathcal{F}_t\}_{t \ge 0}$ to be that which is generated by $B$ .  If $$dX_t = \text{sgn}(X_t) dB_t $$ has a strong solution, then $$B_t = \int_0^t \text{sgn}(X_s)dX_s = |X_t| - L_t^0$$ where $L_t^0$ is the local time of $X$ about $0$ at time $t$ .  It is well known that $$L_t^0 = |X_t| - \lim_{h \rightarrow 0} \frac{1}{h}\text{Leb}\left( \{s \in [0,t] : \left|X_s \right| \leq h  \right)$$ and thus $$\mathcal{F}_t^{X} \subseteq \mathcal{F}_t^B \subseteq \mathcal{F}_t^{|X|}$$ which is a contradiction, since $$\text{sgn}(X_t) \text{ cannot possibly be } \mathcal{F}_t^{|X|} \text{ measurable.} \qquad \qquad \textbf{(1)}$$ Intuitively, $\textbf{(1)}$ is true, but how does one rigorously go about showing this? I try to do so by contradiction, but it leads me nowhere after a bit.  If $\text{sgn}(X_t)$ were $\mathcal{F}_t^{|X|}$ measurable, then there would exist some functional $f$ defined on the space of continuous functions on $[0,t]$ such that $$\text{sgn}(X_t) = f(|X_s|, s \leq t) $$ I know this is intuitively obvious to be a contradiction, but I cannot formally show it.  Any help would be appreciated!","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4497536,"If $G$ is a triangle-free and $\delta(G)$ is large, then $G$ is bipartite graph","Show that every $n$ -vertex triangle-free graph with minimum degree
greater than $2n/5$ is bipartite. First of all this result is vacuously true for $n=1,3,5$ . For $n=2$ and $n=4$ , it is trivially true since the desired graphs are $K_2$ and $K_{2,2}$ . We assume that $n\geq 6$ and $V(G)=\{v_1,\dots,v_n\}$ . Since $\deg(v_1)>2n/5$ , then take $v_2\in N(v_1)$ and consider $N(v_2)$ also. It is easy to see that $N(v_1)$ and $N(v_2)$ are disjoint and independent sets. Moreover, $|V(G)\setminus (N(v_1)\sqcup N(v_2))|<n/5$ . Consider the most trivial case, when $V(G)\setminus (N(v_1)\sqcup N(v_2))=\varnothing$ . Hence $V(G)=N(v_1)\sqcup N(v_2)$ and $G$ is a bipartite graph with vertex sets $N(v_1)$ and $N(v_2)$ , i.e. $G=G[N(v_1),N(v_2)]$ . If $|V(G)\setminus (N(v_1)\sqcup N(v_2))|=1$ , then $V(G)= N(v_1)\sqcup N(v_2)\sqcup \{w\}$ .
If $N(w)\cap N(v_1)=\varnothing$ or $N(w)\cap N(v_2)=\varnothing$ , then $G$ is bipartite graph. Indeed, assume WLOG that $N(w)\cap N(v_1)=\varnothing$ , then $G$ is a bipartite with vertex sets $N(v_1)\sqcup \{w\}$ and $N(v_2)$ . Question 1. What if $|N(w)\cap N(v_1)|=k>0$ and $|N(w)\cap N(v_2)|=\ell>0$ ? I sketched a picture and I see that $G$ is not bipartite in this case. I was wondering how to prove it rigorously? Question 2. I was wondering is it possible to solve the problem with that approach? It seems a bit difficult to me but I did not checked the details since I stucked in question 1.","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'bipartite-graphs', 'extremal-graph-theory']"
4497545,"Finding a closed form for $((2\circledast3)\circledast4)\circledast\cdots\circledast n$, where $x \circledast y = \frac{x+y}{1+xy}$","TL;DR: I've made something work (again) but in the process I used something which makes no sense - I wonder why it all works in the end. Please explain it to me or disprove my findings. Let's define an operator $\circledast$ in this way: $x \circledast y = \frac{x+y}{1+xy}$ I want to find $$((2\circledast3)\circledast4)\circledast\cdots\circledast n, \;n \ge 3$$ My Approach First thing to notice is that: $(x \circledast y) \circledast z = x \circledast (y \circledast z)$ so it doesn't make any difference which order are we applying it and so we can remove all the parenthesis and read it as say ""left to right"". Now remembering that $\tanh(x+y)=\frac{\tanh x+\tanh y}{1+\tanh x\cdot \tanh y}$ we can see that $\tanh(\tanh^{-1}x+\tanh^{-1}y)=\frac{x+y}{1+xy} = x \circledast y$ . Therefore we can rewrite the expression we want as $S = \tanh(\tanh^{-1}2+\tanh^{-1}3+...+\tanh^{-1}n)$ , so $$\tanh^{-1}S = \sum_{k=2}^n\tanh^{-1}k$$ ""So far so good"" - I would've said, but the issue is that inverse hyperbolic tangent has only domain $(-1, 1)$ over reals. But - let's roll with it, the ""why it works"" is part of the question. From here we use the logarithmic form : $\tanh^{-1}x=\frac{1}{2}\ln\frac{1+x}{1-x}$ - again, ignoring the glaring issues with the domain. We get: $$\frac{1}{2}\ln\left(\frac{1+S}{1-S}\right) = \frac{1}{2}\sum_{k=2}^n\ln\left(\frac{1+k}{1-k}\right)=\frac{1}{2}\ln\left(\prod_{k=2}^n\frac{1+k}{1-k}\right)$$ Here's another ""red flag"" - using the logarithm property of sum to product on something that potentially doesn't exist - or even if it does, it is complex, and if I understand correctly, complex version of $\ln$ is $\mathrm{Log}$ which is multivalued so it's not right to claim $\ln(ab)=\ln(a)+\ln(b)$ . Finally, we notice that if we flip the sign of the denominator we will end up with a telescoping product on the fraction shifted by 2 on the numerator to the right. Hence we end up with this: $$\frac{1}{2}\ln\left(\frac{1+S}{1-S}\right) = \frac{1}{2}\ln\left(\prod_{k=2}^n\frac{1+k}{1-k}\right) = \frac{1}{2}\ln\left((-1)^{n-1}\frac{n(n+1)}{2}\right)$$ We have $(-1)^{n-1}$ because there will be in total $n-1$ members (we start at $2$ and end at $n$ ). And now.. you guessed it. We got $\frac{1}{2}$ on both sides, we got $\ln$ on both sides - let's ""cancel"" them (another ""red flag"" here). If we do so we end up with $\frac{1+S}{1-S}=(-1)^{n-1}\frac{n(n+1)}{2}$ thus $$S=\frac{(-1)^{n-1}(n^2+n)-2}{(-1)^{n-1}(n^2+n)+2}$$ which is not only well-defined, but also is a correct expression for $S$ (it works for any $n$ ) Question is - why does this work despite operating on things which are not well-defined? Or am I wrong and the $\tanh^{-1}x$ is well-defined outside of $(-1,1)$ ? (as well as $\ln$ and its properties over complex numbers)? EDIT :
There's a proposed fix to the solution, courtesy of @Tortar . He has a great observation that $x \circledast y = \frac{1}{x} \circledast \frac{1}{y}$ and therefore we can avoid problematic domain issues if we rewrite $S = \frac{1}{2} \circledast \frac{1}{3} \circledast ... \circledast \frac{1}{n}$ . This, however, still has issues First, it doesn't explain why the original ""solution"" works. It all makes sense if I would apply this transformation before using $\tanh^{-1}$ but I do not, hence the question still stands More importantly - and this is why I decided to add this to the body of the question itself - this will lead to an incorrect result. That is because on the logarithms step we will get all valid $\ln\left(\frac{1+\frac{1}{k}}{1-\frac{1}{k}}\right) = \ln\left(\frac{k+1}{k-1}\right)$ which means that when we will combine the fractions together and remove repeating terms from it we end up losing $(-1)^{n-1}$ as there's nothing to negate. This will lead to the final expression for $S$ being invalid for all even $n$ . Therefore while this relation establishes a link between the ""wrong"" way I do it and the ""safe"" way to do it, it looks like it's not as simple as just replacing $n$ with $\frac{1}{n}$ and something is still amiss.","['hyperbolic-functions', 'logarithms', 'proof-explanation', 'fake-proofs', 'algebra-precalculus']"
4497577,For which topological spaces $X$ is $X^n$ homeomorphic to the space of unordered n tuples of points in $X$,"Consider a topological space $X$ . Consider the spaces $A_n=X^n$ and $B_n=X^n/q_n$ , the space of where $q_n$ is the equivalence relation where two points in $X^n$ are equivalent if one can be constructed from another via a permutation of the points. For which topological spaces $X$ are $A_n$ and $B_n$ homeomorphic for all natural numbers $n$ ? For example, when $X=\mathbb{R}$ , $A_n$ and $B_n$ are not homeomorphic in general, consider $n=2$ in particular. Another example, when $X=\mathbb{R}^2$ , $A_n,B_n$ are homeomorphic for each natural $n$ . This can be proven by thinking of $X$ as the complex plane, $A_n$ as the space of monic polynomials of degree $n+1$ over $\mathbb{C}$ , and thinking of $B_n$ as the space of unordered $n$ tuples of roots. It is a theorem that the map which takes the coefficients of a polynomial to its roots is bijective and bicontinuous, thus these spaces are homeomorphic. Some more details on this proof: 1: Each list of coefficients gives a unique set of roots and vice versa 2: the coefficient list as a function of the roots is continuous because this function is a polynomial 3: the roots as a function of the coefficients is continuous in the quotient space $B_n$ (for a proof, see Bhatia matrix analysis, beginning of chapter 6). I'd like to know for which other spaces this works.",['general-topology']
4497582,calculate the limit of a function,"I want to calculate the limite of this function when $x\to\infty$ . $\lim_{x\to\infty}\left(\frac{c+\sqrt{x}}{-c+\sqrt{x}}\right)^x\exp(-2c\sqrt{x})$ , where $c$ is a constant. Numerically, I plot a graphic of this function, and I think the answer is 1. But theoretically, I have no idea how to proceed.","['limits', 'calculus', 'limits-without-lhopital', 'analysis']"
4497605,Balancing balls in bins,"The following is a classic problem: There are $k$ bins with $n_1, \ldots, n_k$ balls such that $n_1+\dots+n_k = n$ . If $n_i < n_j - 1$ , we may move one ball from bin $j$ into bin $i$ . By defining the potential function $n_1^2 + \dots + n_k^2$ , it is easy to show that this process ends after at most $n^2$ moves. Consider this variant: There are now positive constants $c_1,\ldots, c_k$ (which may not be integers). If $c_in_i < c_j(n_j-1)$ , we may move one ball from bin $j$ into bin $i$ . I can show that this process again terminates. But does it always terminate after at most $n^2$ (or even some polynomial in $n$ ) moves?","['combinatorics', 'balls-in-bins']"
4497631,Notion of $G$-stable subsets in Moerdijk and MrÄun's Introduction to foliations and Lie groupoids,"This question is related to this this older question. Let $M$ be a smooth manifold and $G$ be a group of diffeomorphisms of $M$ . In page 35 of Introduction to Lie foliations and Lie groupoids , Moerdijk and MrÄun call an $S\subset M$ a $G$ -stable subset of $M$ when it is connected and given $g\in G$ , either $S\cap gS=S$ or $S\cap gS=\emptyset$ . Just after the definition, they point out that the $G$ -stable subsets of $M$ are precisely the components of $G$ -invariant subsets of $M$ . I am having a hard time to understand the implication, i.e., how a $G$ -stable subset would need to be a connected component of a $G$ -invariant subset of $M$ . For instance, consider $M=\mathbb{C}$ , $G=\mathbb{S}^1$ and $S=\{1\}$ , where $G$ acts on $M$ by rotations around $0\in M$ . In this case, $S$ is $G$ -stable yet it is not a connected component of its group-action closure, $gS=G$ . In this context, don't we have to modify a little the definition of $G$ -stable subsets only to include open subsets of $M$ or restrict it to finite subgroups of diffeomorphisms of $M$ ?","['differential-geometry', 'diffeomorphism', 'differential-topology', 'group-actions', 'orbifolds']"
4497662,"If a vector identifies a hyperplane, what does a matrix identify?","Given a vector $x\in\mathbb{R}^n$ this identifies a hyperplane (through the origin) of equation $$
w^\top x = 0.
$$ How to generalize this to a matrix $X\in\mathbb{R}^{m\times n}$ ? What geometrical object can it identify? Presumably something ""linear"" like a hyperplane but with larger codimension? Possible Idea My guess is it would identify the following space $$
\{w\in\mathbb{R}^n\,:\, Xw = 0\} = \text{nullspace}(X).
$$ Now this is a subspace but what is it geometrically? Does is still look and behave like a hyperplane? Presumably, each row vector in $X$ essentially identifies a hyperplane. So perhaps it's something like this? Looks like an ""envelope"" of hyperplanes..","['matrices', 'linear-algebra', 'vector-spaces']"
4497674,Prove a particular subset of a group is closed under inverse,"Let G is a group, $\phi$ is an automorphism, and $ A=\{ g\in G\mid \phi(g)=g^{-1} \}$ . Consider $a \in A$ and $B_a =\{ g\in A\mid ga \in A \}$ . Prove that $g \in B_a$ implies $g^{-1} \in B_a$ . It seems this problem can be solved directly. However, I could not solve it without using centralizer subgroups. This is my solution: Lemma: $A \cap Z(a) =B_a$ , where $Z(a)$ is the centralizer of $a$ . Proof: $g \in B_a \Rightarrow ga \in A \Rightarrow \phi(ga)=(ag)^{-1}$ . Also $\phi(ga)=\phi(g) \phi(a)=g^{-1} a^{-1}=(ga)^{-1}$ . So, $ag=ga$ and $B_a \subset Z(a)$ . Since $B_a \subset Z(a)$ and $B_a \subset A$ , then $A \cap Z(a) =B_a$ . $\blacksquare$ $g \in A$ implies $g^{-1} \in A$ , and $g \in Z(a)$ implies $g^{-1} \in Z(a)$ . Therefore, with using the lemma, $g \in A \cap Z(a)=B_a$ implies $g^{-1} \in B_a$ . Now, I am wondering why I cannot solve it using the following way: $g \in B_a$ implies $ga=ag \in A$ . Also, $ga^{-1}=a^{-1}g$ . Now, in order to prove $g^{-1} \in B_a$ , I only need to prove $g^{-1}a \in A$ or $ga^{-1} \in A$ , but I cannot. Does anyone have another solution or know how to continue this approach?","['group-theory', 'abstract-algebra', 'group-isomorphism']"
4497711,Prove that $Y$ is the closure of some open set if and only if $Y$ is the closure of its interior.,"Prove that $Y$ is the closure of some open set if and only if $Y$ is the closure of its interior. $\implies: $ Let $Y=\overline Z$ for some open set $Z$ . Then $Y$ is the smallest closed set containing $Z$ . Since $\mathring Y$ is the largest open set contained in $Y$ , therefore $Z\subseteq \mathring Y$ . However, since $\overline{\mathring Y}$ is the smallest closed set containing $\mathring Y$ , we have that $\overline{\mathring Y} \subseteq Y$ , but being that $Y$ is the smallest closed set containing $Z$ it is also the case that $Y\subseteq \overline{\mathring Y}$ and hence $Y= \overline{\mathring Y}$ . $\Longleftarrow:$ Assume that $Y=\overline{\mathring Y}$ . Since $\mathring Y$ is the largest open set contained in $Y$ , for any other open set $Z$ in $Y$ we must have that $Z\subseteq \mathring Y$ . And since $\overline Z$ is the smallest closed set containing $Z$ , we have $\overline Z \subseteq \overline{\mathring Y}$ . However, since $\overline{\mathring Y}$ is the smallest closed set containing $\mathring Y$ , it is also true that $\overline{\mathring Y} \subseteq \overline Z$ , so $\overline Z = \overline{\mathring Y}=Y$ . $\Box$ Is this correct, and can this be simplified at all? The two directions seem largely similar in reasoning. This problem confused me for a while since it was basically equivalent to saying, ""Y is the smallest closed set containing an open set Z if and only if Y is the smallest closed set containing the largest open set contained in Y"" which was hard for me to parse at first.","['elementary-set-theory', 'general-topology', 'solution-verification', 'analysis']"
4497730,Is curl of a particle's velocity zero?,"The question Consider the motion of a particle specified by $\mathbf{x} (t): \mathbb{R} \mapsto \mathbb{R}^3$ , where $\mathbf{x} = (x_1,x_2,x_3)$ in cartesian coordinates. The curl of its velocity $\mathbf{v} = (v_1, v_2, v_3)$ can be calculated as $$
\nabla \times \mathbf{v} = (\frac{\partial v_3}{\partial x_2} -\frac{\partial v_2}{\partial x_3},\frac{\partial v_1}{\partial x_3} -\frac{\partial v_3}{\partial x_1}, \frac{\partial v_2}{\partial x_1} -\frac{\partial v_1}{\partial x_2} ).
$$ From the interchangeability of ordinary and partial derivatives , $$
\frac{\partial v_i}{\partial x_j} 
= \frac{\partial}{\partial x_j} \frac{\mathrm{d}x_i}{\mathrm{d}t} 
= \frac{\mathrm{d}}{\mathrm{d}t} \frac{\partial x_i}{\partial x_j} 
= \frac{\mathrm{d}}{\mathrm{d}t} \delta_{ij}
=0,
$$ which makes every component of the curl zero for any $\mathbf{x}(t)$ . However, this does not make much sense to me, since I can easily imagine a rotating velocity field. Also, such fields seem to be quite common in rotational dynamics and fluid dynamics, such as in this post . Some contexts I am trying to derive the equation of motion for a charged particle with rest charge $m$ and charge $q$ in a given electric potential $\phi(t,\mathbf{x})$ and magnetic vector potential $\mathbf{A}(t,\mathbf{x})$ , whose Lagrangian is given by $$
\mathcal{L} = -mc^2 \sqrt{1-|\mathbf{\dot{x}}|^2/c^2} - q \phi + q \mathbf{\dot{x}} \cdot \mathbf{A}.
$$ The above question arises when evaluating the term $\nabla (\mathbf{\dot{x}} \cdot \mathbf{A})$ (the spatial gradient), which appears in ${\partial \mathcal{L}}/{\partial x_i}$ . Using the vector calculus identities, we can evaluate the spatial gradient as $$
\nabla(\mathbf{A} \cdot \mathbf{\mathbf{\dot{x}}})  =\  (\mathbf{A} \cdot \nabla)\mathbf{\mathbf{\dot{x}}} \,+\,  (\mathbf{\mathbf{\dot{x}}} \cdot \nabla)\mathbf{A} \,+\,  \mathbf{A} {\times} (\nabla {\times} \mathbf{\mathbf{\dot{x}}}) \,+\,  \mathbf{\mathbf{\dot{x}}} {\times} (\nabla {\times} \mathbf{A}) \\
= \,  (\mathbf{\mathbf{\dot{x}}} \cdot \nabla)\mathbf{A} +  \mathbf{\mathbf{\dot{x}}} {\times} (\nabla {\times} \mathbf{A}),
$$ where the second step uses the identity in the question. The answer to a similar question by Shuhao Cao (non-relativistic, but the terms of interest are identical) explains the second step as ""the usual postulate that $\mathbf{v}$ is not explicitly a function of $\mathbf{x}$ .""","['electromagnetism', 'euler-lagrange-equation', 'multivariable-calculus', 'vector-analysis', 'physics']"
4497745,A kind of pertubation of laplacian is an Fredholm operator,"Let $U, V$ be Banach spaces and $L(U,V)$ be the space of bounded linear operators from $U$ to $V$ . An operator $T \in L(U,V)$ is said to be a Fredholm operator if dim $N(T) < +\infty$ and $\mathrm{codim} R(T) < +\infty$ . In this case, we define the index of a Fredholm operator $T$ as the integer $$
\mathrm{ind}(T) = \mathrm{dim} N(T) - \mathrm{codim} R(T).
$$ Denote by $\mathrm{Fred}_0 (U,V)$ the set of the Fredholm operators with index zero from $U$ to $V$ . Given $\lambda \in \mathbb{R}$ , consider the operator $$T := -\Delta - \lambda I : C^{2}(\Omega) \rightarrow C(\overline{\Omega})$$ I'm trying to prove that $T$ is a Fredholm operator with $\mathrm{ind}(T) = 0$ , for all $\lambda$ . I already know that the operator $-\Delta : C^{2}(\Omega) \rightarrow C(\overline{\Omega})$ is in $\mathrm{Fred}_0(C^{2}, C)$ , since its kernel is trivial by maximum principle and is surjective by Lax Milgram theorem and regularity theory. In addition, I know that $\mathrm{Fred}_0(U,V)$ is an open subset of $L(U,V)$ . So, as $-\Delta \in \mathrm{Fred}_0(C^{2}, C)$ , for small values of $\lambda$ , $-\Delta - \lambda I$ is in a ball centered in $-\Delta$ which is contained in $\mathrm{Fred}_0(C^{2}, C)$ . I don't know if there's another justification to conclude this with arbitrary values of $\lambda$ . I would appreciate any help.","['variational-analysis', 'bifurcation', 'analysis', 'functional-analysis', 'partial-differential-equations']"
4497748,Highly symmetric colorings of the sphere,"If you're dropped onto a planet with an exact map of whatever's on the surface, then by comparing the map to your surroundings, you learn something about where you are (and which direction you're facing). Can this surface structure be made arbitrarily un informative using symmetry? That is, for every $\epsilon>0$ , is there a nonconstant function $f:S^2\to Y$ such that for all $t\in SO(3)$ , there exists $u\in SO(3)$ such that $d(u,I)<\epsilon$ and $f\circ u = f\circ t$ ? Here $S^2$ is the unit sphere in $\Bbb R^3$ , $Y$ is any codomain, $I$ is the identity transformation, and $d$ is the standard metric on $SO(3)\subset M_{3\times3}(\Bbb R)$ . The idea is that $f$ is $\epsilon$ -uninformative if every possible orientation $t$ of the $f$ -colored sphere is indistinguishable from some orientation $u$ that's within $\epsilon$ of the reference orientation. I think a similar question is whether there exist isohedral spherical polyhedra with arbitrarily small faces (in terms of the ratio of the diameter of a face to the diameter of the polyhedron). Some web searching has led me to things like the classification of finite subgroups of $SO(3)$ and various lists of symmetric polyhedra, which often end with dodecahedral/icosahedral shapes. This leads me to suspect that the answer to my main question is no, but I'm not sure how the various types of polyhedral symmetry relate to the type of symmetry I've described.","['geometric-topology', 'symmetry', 'geometry', 'polyhedra']"
4497751,Folland Advanced Calculus Ex. 2.5.7,"Suppose that the variables $E$ , $T$ , $V$ , and $P$ are related by a pair of equations, $f(E,T,V,P)=0$ and $g(E,T,V,P)=0$ , that can be solved for any two of the variables in terms of the other two, and suppose that the differential equation $\partial_VE-T\partial_TP+P=0$ is satisfied when $V$ and $T$ are taken as the independent variables. Show that $\partial_PE+T\partial_TV+P\partial_PV=0$ when $P$ and $T$ are taken as the independent variables. If we put $\phi(V,T)=(E(V,T),T,V,P(V,T))$ and $F=(f,g)$ , then $(F\circ\phi)(V,T)=0$ and hence $F'(\phi(V,T))\phi'(V,T)=0$ . How to proceed any further?",['multivariable-calculus']
4497762,Reference request: a fundamental theorem of calculus for monotone functions,"It is well known that the fundamental theorem of calculus is essentially characterised by absolutely continuous functions.
In fact, it fails drastically when we look at the class of bounded variation functions, e.g. the Cantor stair function increases from 0 to 1 however has derivative zero almost everywhere. When learning measure theory as an undergraduate, I was shown a fundamental theorem of calculus, attributed to de la VallÃ©e Poussin, which accounts for these defects in the usual fundamental theorem of calculus for monotone functions: Theorem. (de la VallÃ©e Poussin) Let $f: \mathbb R \to \mathbb R$ be non-decreasing and continuous, and denote $\mu$ its associated Lebesgue-Stieltjes measure. Then $f$ is differentiable Lebesgue-a.e. and $\mu$ -a.e., with $|\{f' = \infty\}| = 0$ and \begin{equation}
f(b) - f(a)= \int_{(a, b] \cap \{f' < \infty\}} f' \, dx + \mu((a, b] \cap \{f' = \infty \}).
\end{equation} Showing that the integral of the derivative is controlled by the left-hand side is a routine textbook result, c.f. for example Stein-Shakarchi. However, it would be greatly appreciated if someone could point me to a reference to the result above, such as the original paper or a textbook, as I have not been able to locate one myself (apart from old lecture notes!)","['calculus', 'geometric-measure-theory', 'measure-theory', 'reference-request']"
4497780,Prove that $f(A_1 \cap A_2) \subseteq f(A_1) \cap f(A_2)$,"1) Earlier, I was asked to provide a counterexample for: $f(A_1 \cap A_2) = f(A_1) \cap f(A_2)$ My attempt: Consider the function $f: R â†’ R$ defined by $f(x)=x^2$ . Let $A_1=[0, \infty)$ and $A_2=(-\infty,0]$ . Then $A_1 \cap A_2 = \{0\}$ so $f(A_1 \cap A_2) = \{0\}$ . However, $f(A_1) = f(A_2) = [0,\infty)$ , so $f(A_1) \cap f(A_2) = [0,\infty)$ . This happened because two different elements in the domain are mapped to the same element in the range. 2) Then, I was asked to prove: $f(A_1 \cap A_2) \subseteq f(A_1) \cap f(A_2)$ My attempt: let $x \in f(A_1 \cap A_2)$ That implies that $x \in f(A_1)$ and $x \in f(A_2)$ Since $x \in f(A_1)$ and $x \in f(A_2)$ , we can conclude that $x \in A_1 \cap A_2$ , which also means that $x \in A_1$ and $x \in A_2$ Thus $f(A_1) \cap f(A_2)$ I would really appreciate the feedback on the latter proof mainly . Thank you!","['set-theory', 'solution-verification', 'discrete-mathematics']"
4497786,Proof verification: $\int_{-\infty}^{\infty}\operatorname{sech}\left(x\right)\ln\left(\cosh\left(x\right)\right)dx\ =\ \pi\ln\left(2\right)$.,"This is more of a post I wanted to share. I have decided to tackle the following integral via contour integration: $$\int_{-\infty}^{\infty}\operatorname{sech}\left(x\right)\ln\left(\cosh\left(x\right)\right)dx.$$ Proof. Let that integral equal $I$ . Let $$f(z) = \frac{\text{Log}{(z+i)}}{z^2+1},$$ where Log represents the principal logarithm. We shall traverse, in a counterclockwise direction, a path $C$ defined by $$C = \left[-R,R\right] \cup \Gamma,$$ which is a large semicircle of radius R above the real axis. We can express the contour integral over $C$ as $$\oint_Cf(z)dz = \int_{-R}^{R}f(z)dz + \int_{\Gamma}f(z)dz$$ Notice the only singularity that resides in $C$ is $i$ . Taking the residue at that pole yields $$\oint_Cf(z)dz = 2\pi i\text{Res}{(f(z),i)} = 2\pi i \lim_{z \to i}f(z) = \pi\ln{(2)} + i\frac{\pi^2}{2},$$ after evaluating the limit and doing some algebra. By transitivity and taking the limit as R goes to infinity, we see that $$\lim_{R \to \infty} \int_{-R}^{R}f(z)dz + \lim_{R \to \infty} \int_{\Gamma}f(z)dz = \pi\ln{(2)} + i\frac{\pi^2}{2}.$$ Let's call the first and second integrals $I_1$ and $I_2$ , respectively. Rewriting $I_1$ gives us $$\eqalign{\int_{-R}^{R} \frac{\text{Log}(z+i)}{z^2+1}dz 
  &= \int_{-R}^0 \frac{\text{Log}(z+i)}{z^2+1}dz + \int_{0}^R \frac{\text{Log}(z+i)}{z^2+1}dz \cr
  &= \int_{R}^0 \frac{\text{Log}(-y+i)}{(-y)^2+1}d(-y) + \int_{0}^R \frac{\text{Log}(z+i)}{z^2+1}dz \cr
  &= \int_{0}^R \frac{\text{Log}((i-y)(i+y))}{y^2+1}dy \cr
&= i\frac{\pi^2}{2} + \int_0^R \frac{\ln{(1+y^2)}}{1+y^2}dy\cr}$$ after doing some basic manipulations and integration. Note if $|z| = R$ , then $z$ is on $R$ . Dealing with $I_2$ as $R$ approaches infinity, consider the following inequalities: $$|\text{Log}(z+i)| = |\ln|z+i| + i\text{Arg}(z+i)| \leq ||z+i| + i\pi| \leq R+1+\pi;$$ $$\left|z^2+1\right| \geq \left|\left|z^2\right|-1\right| = R^2 - 1.$$ By applying the ML-Inequality and Squeeze Theorem, we get $$0 \leq \left|\int_{\Gamma}f(z)dz\right| \leq |f(z)|\left(\frac{2\pi R}{2}\right) \leq \frac{R+1+\pi}{R^2-1}$$ $$\lim_{R \to \infty} 0 \leq \lim_{R \to \infty} \left|\int_{\Gamma}f(z)dz\right| \leq \lim_{R \to \infty} \frac{R+1+\pi}{R^2-1}$$ $$0 \leq \lim_{R \to \infty} \left|\int_{\Gamma}f(z)dz\right| \leq 0.$$ This implies that $$\lim_{R \to \infty} \int_{\Gamma}f(z)dz = 0.$$ Going back and taking R to infinity, we see that $$\eqalign{
\pi \ln{(2)} + i\frac{\pi^2}{2} 
  &= i\frac{\pi^2}{2} + \int_0^{\infty} \frac{\ln{(1+y^2)}}{1+y^2}dy + 0 \cr
  \pi \ln{(2)} &= \int_0^{\infty} \frac{\ln{(1+y^2)}}{1+y^2}dy. \cr
}$$ Letting $y = \sinh{(x)}$ , we get $$
\int_0^{\infty} \frac{\ln{(1+y^2)}}{1+y^2}dy = \int_0^{\infty}\frac{\ln{(\sinh^2{(x)}+1)}\cosh{(x)}}{\sinh^2{(x)}+1}dx = 2\int_0^{\infty}\operatorname{sech}\left(x\right)\ln\left(\cosh\left(x\right)\right)dx.
$$ Since $\operatorname{sech}\left(x\right)\ln\left(\cosh\left(x\right)\right)$ is an even function, we see that $$2\int_0^{\infty}\operatorname{sech}\left(x\right)\ln\left(\cosh\left(x\right)\right)dx = \int_{-\infty}^{\infty}\operatorname{sech}\left(x\right)\ln\left(\cosh\left(x\right)\right)dx.$$ Therefore, $$\int_{-\infty}^{\infty}\operatorname{sech}\left(x\right)\ln\left(\cosh\left(x\right)\right)dx\ =\ \pi\ln\left(2\right).$$ Q.E.D. Let me know if you know any other strategies you have. If you have any suggestions on optimizing the solution or find any errors, please do not hesitate to share them with me! P.S. Does anyone know how to draw a semicircle contour using LaTeX? Like using a Tikz package or something?","['improper-integrals', 'complex-analysis', 'contour-integration', 'solution-verification', 'residue-calculus']"
4497807,"Find pair of product of four groups that has the same order, but not isomorphic.","Find a set of $16$ -element product of four groups that have same number of elements of each order, but are not isomorphic. Choices to form two equal order products of four groups: $C_1, C_2, C_4, C_8, C_{16}, Q_8.$ Need find products of four groups, and to form such two products having equal order, but not isomorphic. Le, the first product of four groups be denoted by $G= a\times b\times c\times d$ , and the second by $H= e\times f\times g\times h.$ The choice need to show equal order, while failing at Isomorphism. So, $G_1= C_{16}\times C_1\times C_1\times C_1$ , and $H= C_8\times C_8\times C_1\times C_1$ will not help; as then will have equal orders as well as Isomorphism too. A possible choice is : $G_1= C_{8}\times C_1\times C_1\times C_1$ , and $H= Q_8\times C_1\times C_1\times C_1$ Another choice: $G_1= C_{16}\times C_1\times C_1\times C_1$ , and $H= Q_8\times C_8\times C_1\times C_1$ But, the answer key states: On writing group product, in decreasing group order, have only one choice for determining $G,H$ of order $16$ . There should be many choices possible, as not specified to use a group only one time in product. Seems it is implicit to use a group only one time in a product. [cancel]{Edit:. One possible answer is $$G: Q_8\times C_2\times C_1\times C_1,$$ $$ H: C_{16}\times C_4\times C_2\times C_1,$$ both of order $|G|=|H|=16.$ But, there should be many choices possible, unless each group can be used once in a product.}[/cancel] Final Edit: Seems with restriction on both product groups having the same number of elements order-wise, it is impossible to state even the factors of each product.
Getting two products (or, even one ), of length $16$ from given set of values is impossible.","['automorphism-group', 'group-theory', 'group-isomorphism']"
4497819,"Exercise 13, Section 2.C - Linear Algebra Done Right.","Exercise: Suppose $U$ and $W$ are both $4$ -dimensional subspaces of $C^6$ . Prove that
there exist two vectors in $U \cap W$ such that neither of these vectors is a
scalar multiple of the other. My attempt at a proof is as follows. Proof: Let $u_1,. . .,u_4$ be a basis of $U$ and let $w_1,. . .,w_4$ be a basis of $W$ . Then, $u_1,. . .,u_4,w_1,. . .,w_4$ spans $U+W$ . Because $U+W$ is a subspace of $C^6$ , the $\dim(U+W)\le 6$ . Thus, $u_1,. . .,u_4,w_1,. . .,w_4$ can be reduced to a basis of $U+W$ . In the process, none of the $u's$ get removed as $u_1,. . .,u_4$ is linearly independent. Thus, some of the $w's$ get removed in the process. Because $\dim(U+W)\le 6$ , at least two of the $w's$ get removed. These are the $w's\in U\cap W$ . Because $w_1, . .,w_4$ is linearly independent, none of these two vectors are a scalar multiple of each other. Is the proof correct? Edit: I implicitly use that theorem that every spanning list in a vector space can be reduced to a basis of that vector space. In the process, we remove those vectors that are in the span of the previous ones. Thus, if we have the list $v_1,. . .,v_k$ . We remove $v_j$ only if $v_j$ is in the span of $v_1,. . .,v_{j-1}$ . Edit 2: I have come to know that the proof is wrong. For future readers, I am writing another proof that is also suggested as a hint in the answers. Proof 2: Using the formula $\dim (U+W)=\dim(U)+\dim(W)-\dim(U\cap W)$ , we see that $\dim(U\cap W) \ge 2$ . This is because $\dim(C^6)=6$ and $U+W$ is a subspace of $C^6$ . Thus, $\dim(U+W)\le 6$ . Let $j\in Z^+$ with $2\le j\le 6$ . Let $\dim(U\cap W)=j$ . Let $v_1,. . .,v_j$ be a basis of $U\cap W$ . Then we have that $v_1,v_2\in U\cap W$ are not scalar multiples of each other as they are linearly independent. Completing the proof.","['solution-verification', 'linear-algebra', 'vector-spaces']"
4497856,Example of quasi-monotone map that is not pseudo-monotone?,"A quasi-monotone map is a function $F$ such that for all $x,y$ , \begin{equation}
F(x) \cdot (y-x) > 0 \implies F(y) \cdot (y-x) \geq 0
\end{equation} A pseudo-monotone map is a function $F$ such that for all $x,y$ , \begin{equation}
 F(x) \cdot (y-x) \geq 0 \implies F(y) \cdot (y-x) \geq 0
\end{equation} It is not clear to me why the quasi-monotone maps contain the set of pseudo-monotone maps. Can someone provide an example showing that you can have a quasi-monotone map that is not pseudo-monotone?","['convex-optimization', 'monotone-operator-theory', 'functions', 'optimization', 'convex-analysis']"
4497926,Prove $\liminf_{n\to\infty} \left(n^2(4a_n(1-a_{n-1})-1)\right)\leq\frac14$ for any non-negative seqence $\{a_n\}$.,"Let $\{a_n\}_{n=1}^\infty$ be a sequence of non-negative numbers. Prove that $$\liminf_{n\to\infty} \left(n^2\left(4a_n(1-a_{n-1})-1\right)\right)\leq\frac14.$$ I found this problem on one of my old notebooks, and clearly I forgot where it came from. I searched using Approach0 and it led me to a post on AOPS without solutions. I have to admit that I don't know how to start. First of all, it is easy to notice that if we have a subsequence $\{a_{n_k}\}$ with $a_{n_k}\geq 1$ for all $k$ , then we must have $\liminf_{n\to\infty} \left(n^2(4a_n(1-a_{n-1})-1)\right)\leq0\leq \frac14$ ; also, the case where there are infinitely many $0$ 's is easy to handle. So, WLOG we can assume that $a_n\in(0,1)$ for all $n$ . Secondly, notice that $4x(1-x)-1=-(2x-1)^2\leq 0$ , with equality holds iff $x=\frac12$ . It may suggest that $\frac12$ is an important number here. Now it seems natural to write $a_n=\frac12+b_n$ , so we have $b_n\in(-1/2,1/2)$ and we want to prove that $$\liminf_{n\to\infty}\left(n^2(b_n-b_{n-1}-2b_nb_{n-1})\right)\leq\frac18.$$ But this is not obvious, either. Remark. The bound $1/4$ (or $1/8$ in terms of $b_n$ ) is sharp. If we choose $b_n=-\frac1{4n}$ , or equivalently $a_n=\frac12-\frac1{4n}$ for $n\geq1$ , then $$n^2(b_n-b_{n-1}-2b_nb_{n-1})=\frac{n^2}{8n(n-1)}\to\frac18,\qquad n\to\infty.$$ The above are simplest (kind of silly) observations.  Any help would be appreciated!","['limsup-and-liminf', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4497975,Some Questions on the Berry-Esseen Theorem,"Berry-Esseen Theorem (Jacod - Protter): Let $(X_j)_{j \geq 1}$ be i.i.d. and suppose that $E\{|X_j|^3\}<\infty$ . Let $G_n(x) = P\left(\frac{S_n-n\mu}{\sigma \sqrt{n}}\leq x\right)$ where $\mu = E\{X_j\}$ and $\sigma^2 = \sigma^2_{X_j} < \infty$ . Let $\Phi(x) = P(Z \leq x)$ , where $L(Z) = N(0,1)$ . Then $$\sup_{x\in\mathbb R}\left|G_n(x)-\Phi(x)\right| \leq c \frac{E\{|X_j|^3\}}{\sigma^3 \sqrt{n}},$$ where $c$ is a constant. Help me to understand this theorem intuitively. Some questions: How is this related to the Classic Central Limit Theorem? What does this $c$ represents? I've searched and some says that ""It is quantitative version of the CLT"", what does it mean? The book of Jacod-Protter hinted about the ""rate of convergence"", how does it relate to the Berry-Esseen Theorem, is there a relation of the ""rate of convergence"" and the constant $c$ ? If so, how are they related? Thank you for your help.","['central-limit-theorem', 'rate-of-convergence', 'probability-theory']"
4498046,Binary operation on sets,"Assume there exists at least a set (namely the empty set: $\emptyset$ ) and assume also that you have defined just one binary operation $P$ on any collection of sets (including those formed through the operation itself): $$ P(A, B) = \begin{cases} \{A, B\} & \text{if } A \neq B \\ \{A\} & \text{if } A=B  \end{cases}
$$ where $\{A, B\}$ denotes an unordered pair and $\{A\}$ denotes a singleton. Now, my questions are: How many sets can I construct using only the operation $P$ ? It is clear that the answer is infinitely many, however then it remains to clarify if it's a countable or uncountable infinity we are talking about; Is there the possibility that applying P in different ways could yield the same sets? I do not even know how to start answering this one. As always, any hint or comment is highly appreciated and let me know if I can explain my self clearer!","['elementary-set-theory', 'general-topology', 'logic', 'binary-operations']"
4498058,Does anything change if we allow complex roots in system of linear equations?,"Say we have two variable, two equation system ( $a_i, b_i, c_i \in \mathbb{R}$ ). $$a_1x + b_1y + c_1 = 0$$ $$a_2x + b_2y + c_2 = 0$$ If equation $2$ is just equation $(1)$ multiplied by a constant, then the system has infinite solution. Otherwise, if $a_1b_2=a_2b_1$ , then the system has no solution. Else, the system has a unique solution. My question is: Does anything change if we allow $x, y$ to take complex values? For example, is it possible for the system to have no solution for real $x, y$ , but have a solution for complex $x, y$ ? What if the system has a unique solution, is it possible that system gets infinite solution when we extend $x, y$ to complex? Sorry for stupid question. Thanks","['algebra-precalculus', 'systems-of-equations', 'linear-algebra', 'complex-numbers']"
4498071,Calculating the center of mass of a lemniscate rotated around the x-axis?,"This is a problem I have been stuck on a while, it goes as follows: A lemniscate has the equation $(x^2+y^2)^2 = 4(x^2-y^2)$ . Let the part of the curve that lies in the first quadrant rotate around the $x$ -axis to create an object. This object has an homogeneous massdistribution with the density $1$ . Determine the coordinates of the center of mass $(x_T,y_T,z_T)$ , if $x_T = \frac{1}{M}\int _K x dm$ , where $M$ is the mass of the object. I have so far managed to switch to polar coordinates and found the relation $r^2 = 4\cos{2\theta}$ and tried to calculate the volume (which equals the mass in this case due to the density being equal to 1) with the formula $\int _K \pi y^2 dx$ . After the switch to polar coordinates, a point on the curve is $(x,y) = (r\cos{\theta},r\sin{\theta})$ , which gives $\pi y^2 = \pi r^2\sin^2{\theta} = 4\pi\cos{2\theta}\sin^2{\theta}$ . I then reason that a small change $dx$ in polar coordinates, corresponds to a small change in $\theta$ , hence I substitute $dx$ with $\frac{dx}{d\theta} 2\sqrt{\cos{2\theta}}\cos{\theta} \Leftrightarrow dx = -2(\frac{\sin{2\theta}\cos{\theta}}{\sqrt{\cos{2\theta}}}+ \sin{\theta}\sqrt{\cos{2\theta}}) d\theta$ . But when I try and calculate the integral $\int^2_0 4\pi\cos{2\theta}\sin^2{\theta} \cdot (-2)(\frac{\sin{2\theta}\cos{\theta}}{\sqrt{\cos{2\theta}}}+ \sin{\theta}\sqrt{\cos{2\theta}}) d\theta $ , I get an incorrect volume/mass. The correct solution is $(x_T,y_T,z_T) = (\frac{2}{3\sqrt{2}\ln{(\sqrt{2}+1)}-2},0,0)$ I would greatly appreciate any tips on how to proceed, thanks in advance!","['volume', 'definite-integrals', 'analysis']"
4498092,Hoeffding's type inequality for isotonic regression,"Let $n \in \mathbb{N}$ and let $0 \le y_1 \le \dots \le y_n \le 1$ . Suppose that $(Y_{k,t})_{k\in\{1,\dots,n\}, t\in\mathbb{N}}$ is a family of independent random variables such that, for each $k\in\{1,\dots,n\}$ , $(Y_{k,t})_{t\in\mathbb{N}}$ is a sequence of Bernoulli random variables of parameter $y_k$ . Let $T_1,\dots,T_n\in\mathbb{N}$ . Let $(\hat{Y}_1,\dots,\hat{Y}_n)$ be the solution of the quadratic constrained minimization problem \begin{equation*}
   \min_{0\le\hat{y}_1\le\dots\le\hat{y}_n\le1} \sum_{k=1}^n \sum_{t=1}^{T_k}(\hat{y_k}-Y_{k,t})^2
\end{equation*} I'm looking for Hoeffding's inequality type guarantees for these estimators. Precisely, can we found (sharp, or nearly sharp, and hopefully better than the ones that hold for Hoeffding's inequality) constants $c_1,c_2>0$ such that, for each $n \in \mathbb{N}$ , each $0\le y_1 \le \dots\le y_n \le 1$ , each $T_1,\dots, T_n\in\mathbb{N}$ , each $a_1,\dots,a_n \in \mathbb{R}$ and each $\delta\in (0,1)$ it holds \begin{equation*}
   \mathbb{P}\bigg[ \bigg| \sum_{k=1}^n a_k\hat{Y}_k- \sum_{k=1}^n a_ky_k\bigg|\ge \sqrt{c_1 \sum_{k=1}^n\frac{a_k^2}{T_k} \log \Big(\frac{c_2}{\delta}\Big)}    \bigg] \le \delta \;?
\end{equation*} (Notice that this inequality would hold with $c_1 = \frac{1}{2}$ and $c_2 = 2$ if we replace each estimator $\hat{Y}_k$ with $\frac{1}{T_k} \sum_{t=1}^{T_k} Y_{k,t}$ , due to Hoeffding's inequality ). Even the cases where all the $a_k$ are $0$ except one whose value is $1$ , or all the $a_k$ are $0$ except two whose value is $-1$ and $1$ respectively, or all the $a_k$ are equal to $1/n$ , would be interesting for me. I've seen that the solution to the previous minimization problem goes under the name isotonic regression , but I realized that the literature is extremely vast and found myself quickly lost.
I would very much appreciate even a pointer to some reference where this problem is properly addressed.","['statistics', 'concentration-of-measure', 'convex-optimization', 'regression', 'large-deviation-theory']"
4498101,Standard limit proof,Everyone knows the standard result $$\boxed{\lim_{x\to\infty}\left(1+\dfrac{1}{x}\right)^x=e}$$ My friend gave a proof for this result using binomial expansion of $$\left(1+\dfrac{1}{x}\right)^n=1+nx+\dfrac{n(n-1)}{2!}x^2+\dfrac{n(n-1)(n-2)}{3!}x^3+\dots$$ which is valid only when $|1/x|<1$ and $n\in\mathbb{R}$ . Here is how he proceeded- $$\begin{align*}\lim_{x\to\infty}\left(1+\dfrac{1}{x}\right)^x=\lim_{x\to\infty}\left(1+x.\dfrac1x+\dfrac{x(x-1)}{2!}\dfrac1{x^2}+\dfrac{x(x-1)(x-2)}{3!}\dfrac1{x^3}+\dots\right)=\lim_{x\to\infty}\left(1+1+\dfrac{x(x-1)}{2!}\dfrac1{x^2}+\dfrac{x(x-1)(x-2)}{3!}\dfrac1{x^3}+\dots\right)=1+1+\lim_{x\to\infty}\dfrac{x(x-1)}{2!}\dfrac1{x^2}+\lim_{x\to\infty}\dfrac{x(x-1)(x-2)}{3!}\dfrac1{x^3}+\dots=1+1+\frac1{2!}+\frac1{3!}+\frac1{4!}\dots=\boxed{e}\end{align*}$$ The method of binomial expansion is clearly invalid as the exponent $x$ is variable not a constant. Yet the method ends up getting correct answer as $e$ . Am I missing what is incorrect here? I am interested to know why this wrong approach leads to correct answer.,"['calculus', 'taylor-expansion', 'sequences-and-series', 'limits', 'exponential-function']"
4498104,What's the measure of angle $x$ in the triangle below?,"Any point $D$ , which is interior to the triangle $\triangle ABC$ , determines vertex angles
having the following measures: $m(BAD) = x,$ $m(ABD) = 2x,$ $m(BCD) = 3x,$ $m(ACD)= 4x,$ $m(DBC) = 5x.\\$ Find the measure of $x$ .
(Answer: $10^\circ$ ) My progress: $\dfrac{AB}{AD}=\dfrac{\sin(180âˆ’3x)}{\sin(2x)}\\
\dfrac{AC}{AD}=\dfrac{\sin(11x)}{\sin(4x)}
AB=AC \implies 
\dfrac{\sin(3x)}{\sin(2x)}=\dfrac{\sin(11x)}{\sin(4x)}$ but it is a complex equation to solve. Is there another way?","['euclidean-geometry', 'geometry', 'plane-geometry']"
4498159,"Does there exist a bijective, continuous map from the irrationals onto the reals?","Let $\mathbb{P}$ be the irrational numbers as a subspace of the real numbers. $\mathbb{P}$ is homeomorphic to $\mathbb{N}^\mathbb{N}$ , which is also called the Baire space .
It is well known , and fairly easy to see, that there is a continuous map from $\mathbb{P}$ onto the reals, or that there is a closed subset $A$ of $\mathbb{P}$ and a bijective, continuous map from $A$ onto the reals. But does there exist a bijective, continuous map from $\mathbb{P}$ onto the reals? I'm sure this question has been asked already and the answer is well-known. But I couldn't find a reference. Nor could I prove or disprove it.","['general-topology', 'the-baire-space', 'descriptive-set-theory']"
4498168,On the eighth powers $A^8+B^8=C^8+D^8$. Are there non-trivial solutions to it?,"It's conjectured that there are no non-trivial solutions to the Diophantine equation: $$
A^8+B^8=C^8+D^8
$$ I was trying to play around with it using some substitutions.. In particular I first write it as: $A^8-D^8=C^8-B^8$ . Then I replace $(A,D,C,B)$ with $(p+q,p-q,r+s,r-s)$ to obtein the following: $$
p q (p^2 + q^2) (p^4 + 6 p^2 q^2 + q^4) = 
 r s (r^2 + s^2) (r^4 + 6 r^2 s^2 + s^4)
$$ Then I replaced $(p,q,r,s)$ with $(ax+by,ax-by,cx+dy,cx-dy)$ to obtein: $$
a^8 x^8 - b^8 y^8 = c^8 x^8 - d^8 y^8
$$ or: $$
\frac{x^8}{y^8} = \frac{b^8-d^8}{a^8-c^8}
$$ I was trying to substitute some values but I only manage to get trivial solutions.. I would like to find some values such that $(A,B)\not=(C,D)$ . Any kind of suggestions would be much appreciated. Thanks in advance for your help! Some references I have already looked at: https://arxiv.org/pdf/math/0505629v2.pdf , https://www.jstor.org/stable/pdf/2007700.pdf?refreqid=excelsior%3A139ca229e2c0550603c138b5ada591c3&ab_segments=&origin=&acceptTC=1 , https://sites.google.com/site/tpiezas/013 . EDIT 1:
My last substituion is useless. I have just written $(A,D,C,B)$ as $(ax,by,cx,dy)$ in the end.. And there are no non-trivial solution in this case.","['number-theory', 'conjectures', 'exponentiation', 'diophantine-equations']"
4498199,Proof of $(x\subseteq y)\leftrightarrow(x\cap y =x)\leftrightarrow(x\cup y=y)$,"Exercise 1.2.1(vii) from Page 5 of Keith Devlin's ""The Joy of Sets"": Prove the following assertion directly from the definitions. The
drawing of ""Venn diagrams"" is forbidden; this is an exercise in the
manipulation of logical formalisms. $$(x\subseteq y)\leftrightarrow(x\cap y =x)\leftrightarrow(x\cup y=y)$$ Attempt at solution : This is not a difficult claim to understand or prove in terms of sets, but I can't figure out the pure logical formalism. I guess I should break this into seven individual implications? $\forall w(w\in x\rightarrow w\in y)\rightarrow \forall z(z\in x\rightarrow (z\in x\enspace\wedge\enspace z\in y)) $ $\forall w(w\in x\rightarrow w\in y)\rightarrow \forall z((z\in x\enspace\wedge\enspace z\in y)\rightarrow z\in x))$ $\forall z(z\in x\rightarrow(z\in x \enspace\wedge\enspace z\in y))\rightarrow \forall w(w\in x\rightarrow w\in y)$ $\forall z(z\in x\rightarrow(z\in x \enspace\wedge\enspace z\in y))\rightarrow \forall w((w
\in x \enspace\lor\enspace w\in y)\rightarrow w\in y)$ $\forall z(z\in x\rightarrow(z\in x \enspace\wedge\enspace z\in y))\rightarrow \forall w(w\in y\rightarrow(w
\in x \enspace\lor\enspace w\in y))$ $\forall w((w\in x\enspace\lor\enspace w\in y)\rightarrow w\in y)\rightarrow\forall z((z\in x \enspace\wedge\enspace z\in y)\rightarrow z\in x)$ $\forall w((w\in x\enspace\lor\enspace w\in y)\rightarrow w\in y)\rightarrow\forall z(z\in x \rightarrow (z\in x \enspace\wedge\enspace z\in y))$ In addition to wondering if I'm doing this correctly, I also feel like I gained nothing from writing out these implications.","['elementary-set-theory', 'logic', 'first-order-logic']"
4498220,How to calculate $\sum _{n=1}^{\infty }\:\left(\frac{2}{3}\right)^n\sin\left(\frac{\pi }{3}n\right)$,"I believe that Complex numbers should be used in order to calculate this. Let $z = \frac{2}{3}e^{\frac{i\pi }{3}}$ , So, $$\sum _{n=1}^{\infty }\left(\frac{2}{3}\right)^n\sin\left(\frac{\pi }{3}n\right)=\sum _{n=1}^{\infty }\text{Im}\left(\frac{2}{3}e^{\frac{i\pi}{3}}\right)^n=\text{Im}\left[\sum _{n=1}^{\infty }\left(\frac{2}{3}e^{\frac{i\pi}{3}}\right)^n\right]$$ Is this correct?
How do you go about solving it further? I believe this might be a infinite geometric series where, $a_1 = z, q = z$ ?","['complex-numbers', 'sequences-and-series']"
4498234,"Condition where $L_1$ convergence implies pointwise a.e. convergence, using Stein's Maximal Principle.","Question: Consider a measure space $(X, \mathcal{X}, \mu)$ , with $\mu(X)< \infty$ and measurable functions $(f_n)_{n \geq 1}, f$ from $\mathcal{X}\to\mathbb{R}$ . If the following are satisfied, (1) $\Vert f_n -f \Vert_{L_1(\mu)}:=\int_X|f_n(x)-f(x)|\mu(dx)\to_{n \to \infty}0$ ; AND (2) $\sup_{t>0} t \cdot \mu\left( \big\{x\ :\ \sup_{n\in\mathbb N} |f_n(x)-f(x)| >t\big\}\right) <\infty,$ does that imply $f_n$ converges to $f$ pointwise almost everywhere? I know that (1) implies there exists a subsequence $(f_{n_j})$ converging to $f$ pointwise a.e. but I am interested in the convergence for the entire sequence $(f_n)_{n \geq 1}$ . Background: My question was inspired by this post where someone suggested this may be the case because of Steins Maximal Principle , but I am really struggling to see how that can imply pointwise a.e. convergence in a case like this. Here is an example where the commentor uses Stein to show something does not converge pointwise a.e. but I'm failing to see how to do the converse. If someone has an alternative to (2) to make this work (besides monotonicity) that would also be highly appreciated.","['measure-theory', 'real-analysis', 'lp-spaces', 'functional-analysis', 'convergence-divergence']"
4498255,Computing the value of $(x+y)^4$ if $x^4+y^4=5$ and $x^2+xy+y^2=10$,"Let $x^4+y^4=5$ and $x^2+xy+y^2=10.$ Find $(x+y)^4.$ First, I tried expanding $(x+y)^4$ using the binomial theorem to get $5+4x^3y+6x^2y^2+4xy^3,$ so simplifying I got $5+4xy(x^2+y^2)+6(xy)^2.$ Then I rearranged the given equation to get $x^2+y^2=10-xy,$ so the expansion becomes $5+4xy(10-xy)+6(xy)^2.$ I further simplified to get $2x^2y^2+40xy+5,$ but I'm not sure how to continue off here. Factoring this expression doesn't seem to help. May I have some help? Thanks in advance.","['algebra-precalculus', 'systems-of-equations']"
4498271,Does this system of coupled second order differential equations have a closed form solution?,"I'm trying to evaluate the evolution of two scalar fields but their equations of motion are coupled via a potential term $$ V(\phi, \psi) \supset \frac{1}{2}\lambda \phi^{2}\psi^{2}.$$ From the lagrangian, the equations of motion are: $$ \ddot{\phi} - 3H\dot{\phi} + m^{2}\phi - \lambda \phi \psi^{2} = 0,  $$ $$ \ddot{\psi} - 3H\dot{\psi} + m^{2}\psi - \lambda \psi \phi^{2} = 0, $$ where $H$ is the Hubble constant (although it depends on time, for my purpose it can be set to a constant) and the dots denote a differentiation in respect to time.
I'm looking for a closed solution or a way to decouple those equations avoiding the obvious $\lambda = 0$ situation.",['ordinary-differential-equations']
4498272,solve $f'(t) = f(t) (a-bf(t))$ [duplicate],"This question already has an answer here : Transform a logistic equation into a linear ODE using substitution (1 answer) Closed 1 year ago . Solve $f'(t) = f(t) (a-bf(t)), f(t_0) = y_0$ , where $a,b, t_0\in\mathbb{R}$ and $f$ is a real-valued function taking real-values. Apparently the solution is $$f(t) = \dfrac{a y_0 e^{a(t-t_0)}}{by_0 e^{a(t-t_0)} + (a-y_0 b)},$$ but I don't know how to derive this. I know the standard approach for solving first order linear differential equations, but that's clearly not applicable in this case. I also tried doing something like multiplying both sides by some sort of integrating factor like $e^{-at}$ . Doing so yields $$(e^{-at}f(t))' = -e^{-at}b f(t)^2.$$ Therefore $$\dfrac{(e^{-at} f(t))'}{f(t)^2} = -e^{-at}b.$$ It doesn't seem like integrating both sides is useful in this case. Perhaps some sort of substitution might be useful?","['calculus', 'ordinary-differential-equations']"
4498285,In a box are n red socks and n+1 blue socks. I choose 2 socks. Use a proof to show that I am twice as likely to pick one of each rather than 2 blue.,In a box are $n$ red socks and $n+1$ blue socks. I choose $2$ socks. Use a proof to show that I am twice as likely to pick one of each rather than $2$ blue. I am unsure which combinatorial proof to use. This looks like pigeonhole but I don't know how I would use it.,"['proof-explanation', 'combinatorics', 'discrete-mathematics']"
4498309,"$\alpha(x)=\int_0^x (1+t^2)^{-1}$, $\tan(x)=\alpha^{-1}(x)$, $\sin{x}=\frac{\tan{x}}{\sqrt{1+\tan{x}^2}}$. Prove $\lim\limits_{x\to\pi/2^-}\sin{x}=1$.","Imagine we know nothing about trigonometric functions and we define the following function $$\alpha(x)=\int_0^x (1+t^2)^{-1}$$ It can be shown that $\alpha'(x)=(1+x^2)^{-1}>0$ , so $\alpha$ is increasing. therefore, $\alpha^{-1}$ is defined on the image of $\alpha$ which is $(-\pi/2,\pi/2)$ $\alpha$ is odd $\lim\limits_{x\to\infty} \alpha(x)$ and $\lim\limits_{x\to-\infty} \alpha(x)$ exist If we define $$\pi=2\lim\limits_{x\to\infty} \alpha(x)$$ then $\lim\limits_{x\to\infty} \alpha(x)=-\lim\limits_{x\to-\infty} \alpha(x)=\frac{\pi}{2}$ Now let's define the following functions $$\tan(x)=\alpha^{-1}(x)$$ $$\sin{x}=\frac{\tan{x}}{\sqrt{1+(\tan{x})^2}}$$ Let's say we want to show that $\lim\limits_{x\to \frac{\pi}{2}^-}\sin{x}=1$ . We will need the following result $$(\alpha^{-1})'(x)=\frac{1}{\alpha'(\alpha^{-1}(x))}=1+[\alpha^{-1}(x)]^2$$ Then $$\lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{\tan{x}}{\sqrt{1+(\tan{x})^2}}$$ Now, the first thing I am unsure of is how to show, in the context of this problem, that $$\lim\limits_{x\to \frac{\pi}{2}^-} \tan{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \alpha^{-1}(x)=\infty$$ Assuming we can show it, then $$\lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\frac{\infty}{\infty}$$ and by L'Hopital $$=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1+[\alpha^{-1}(x)]^2}{\frac{\alpha^{-1}(x)(\alpha^{-1})'(x)}{\sqrt{1+[\alpha'(x)]^2}}}$$ $$=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1}{\sin{x}}$$ That is, we have $$\lim\limits_{x\to \frac{\pi}{2}^-} \sin{x}=\lim\limits_{x\to \frac{\pi}{2}^-} \frac{1}{\sin{x}}$$ At this point, the solution manual says ""so the limit is $\pm 1$ . What is it that allows us to conclude this latter step?","['integration', 'proof-explanation', 'calculus', 'limits', 'derivatives']"
4498326,Probability of first increase in ordering of iid random variables,"What is the probability that the first $n-1$ terms of iid Unif(0,1) random draws are in decreasing order, but the first $n$ terms are not?
$$ I know that due to exchangability, $\mathbb{P}(X_{1}<X_{2}<\cdots<X_{n-1})=\frac{1}{n-1!}$ . Why can't a similar exchangability argument hold to show that $\mathbb{P}(X_{1}>X_{2}>\cdots>X_{n-1}<X_{n})=\frac{1}{n!}$ ? My reasoning is that the distribution of $(X_1,X_2,\dots,X_n)$ is symmetric, so no matter which permutation of the random variables exist in the random vector, they will all have the same probability.","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4498328,Does the $32$-inator exist?,"Background It is common popular-math knowledge that as we extend the real numbers to complex numbers, quaternions, octonions, sedenions, $32$ -nions, etc. using the Cayley-Dickson construction , we lose algebraic properties at each step such as commutativity, associativity, alternativity, etc. We can express this phenomenon in a more illustrative way. Consider the following multilinear maps $F_k$ of degree $k$ : Number two: $F_0: [\:] = 1 - (-1) = 2$ . Imaginary part: $F_1: [x] = x - x^*$ . Commutator: $F_2: [x, y] = xy - yx$ . Associator: $F_3: [x, y, z] = (xy)z - x(yz)$ . $16$ -inator: $F_4: [x, y, z, w] = (x(yz))w+(w(yz))x-(xy)(zw)-(wy)(zx)$ . They measure the failure of the algebra to be of characteristic two, Hermitian, commutative, associative and Moufang , respectively. All of them are well-known except for the last one, which I adapted from the quadrilinear map described in section 5 of this paper and essentially comes from a linearized Moufang identity. (Note that depending on the convention, imaginary part, commutator and associator are often defined with an extra factor of $1/2$ , and in the context of complex numbers, imaginary part almost always refers to $F_1$ divided by $2i$ ). Since the maps are multilinear, they are determined by their values at the basis elements $1=e_0, e_1, e_2, e_3, \ldots, e_{2^n-1}$ (under the standard labeling) in the $n$ th Cayley-Dickson algebra over the reals. All the maps above with degree $k \le n$ can then be checked to satisfy the following properties: (A1) $[e_a, e_b, e_c, \ldots] = m e_i$ for some $i$ , where $m\in\{-2,0,2\}$ . (A2) $[e_a, e_b, e_c, \ldots] = 0$ whenever $a, b, c, \ldots <2^{k-1}$ . (A3) $[e_1, e_2, e_4, \ldots, e_{2^{k-1}}] = 2 e_{2^{k}-1}$ . (A4) If $[x, y, \ldots] = z$ , then $[\sigma(x), \sigma(y), \ldots] = \sigma(z)$ for any algebra automorphism $\sigma$ . Properties (A2) and (A3) together imply (rather trivially) that given a Cayley-Dickson algebra $\mathbb{A}$ , $F_k$ vanishes in the next algebra $CD(\mathbb{A})$ if and only if both $F_k$ and $F_{k-1}$ vanish in $\mathbb{A}$ ; for $k < 4$ this fact holds in more generality for any $*$ -algebra (see e.g. here ) and provides a more uniform way to explain why commutativity, associativity, etc. break in sequence as we iterate the Cayley-Dickson process. But rather than talk about algebraic properties, for the purposes of this question I would like to treat the maps themselves as the objects of interest. So let these properties (and multilinearity) be the axioms that define a $2^n$ -inator in a real Cayley-Dickson algebra. The first four maps $F_0, \ldots, F_3$ are in fact uniquely determined by these axioms when $k=n$ . For example, (A3) implies $[\:]=2e_0=2$ ; (A2) and (A3) together with linearity imply $[a e_0+b e_1] = a[e_0]+b[e_1] = a\cdot 0 + b\cdot 2e_1 = 2b e_1$ ; the map $F_2$ in the quaternions is completely fixed by (A2), (A3) and (A4) applied twice, with an automorphism that cyclically permutes the imaginary basis elements and with the involution $e_1 \mapsto e_2, e_2 \mapsto e_1, e_3 \mapsto -e_3$ ; and similarly for $F_3$ in the octonions. I haven't checked whether the $16$ -inator in the sedenions is uniquely recovered from the axioms, but I suspect it's probably nonunique, since the automorphism groups of the sedenions and above are ""small"" relative to the size of the algebras (the groups are all isomorphic to the $14$ -dimensional octonionic automorphism group ( $G_2$ ) times a finite group, while the algebras themselves double their dimension at each step). Nevertheless, it is natural to ask if one can continue the sequence by finding nontrivial examples of $F_k$ for $k>4$ ; my main question deals with the smallest case $k=n=5$ . This paper implies that if a $F_5$ exists, it is necessarily not expressible in terms of multiplication and real constants alone, unlike $F_0, F_2, F_3$ and $F_4$ . There could still be a closed-form expression if we allow the use of conjugation as with $F_1$ , but it looks unlikely to me. The possibility of using non-real constants seems more promising, because the automorphism groups of sedenions and above do not act transitively on imaginary elements of norm $1$ , so some basis elements are ""more special"" than others. In fact, one can define new ""conjugations"" $x^{(8)} = e_{8} x e_{8}$ , $x^{(16)} = e_{16} x e_{16}$ , etc. which can be shown to be invariant under all automorphisms of the algebra; this follows from Lemma 2.1 here . An obvious followup to my main question would be whether the $2^n$ -inators can be expressed in terms of multiplication, real constants, and the conjugations $x^*, x^{(8)}, x^{(16)}, x^{(32)}, \ldots, x^{(2^n)}$ . But in principle it's not guaranteed that $F_n$ will be algebraically expressible at all, if it exists. Question My question is: Is there a $32$ -inator in the $32$ -nions, i.e. a multilinear map $F_5: [x,y,z,w,v]$ satisfying axioms (A1)-(A4)? Followup: can it be expressed in terms of multiplication, real constants and conjugations? Such a map would necessarily be nontrivial by (A3), and would be identically zero in any sedenion subalgebra by (A2) and (A4). Some remarks: In page 12 of the paper I linked above, it is suggested that the possible existence of higher-order maps might be related to projective geometry over the field $\mathbb{F}_2$ . The first problem looks simple enough that it could be solved with a computer search, but the search space seems to be quite large at first glance. The obvious upper bound on possible cases to check, taking into account only multilinearity and (A1), would be $(2\cdot 32 + 1)^{32^5} = 65^{2^{25}} \lesssim 2^{2^{28}}$ . Perhaps some clever argument could reduce this bound to a more manageable number.","['octonions', 'abstract-algebra', 'multilinear-algebra', 'sedenions', 'recreational-mathematics']"
4498379,"In calculus, how is taking $\ln$ on both sides justified?","For example, if we want to differentiate $y=x^x,$ we can turn it into $e^{\ln(y)}=y=e^{x\ln(x)},$ seemingly ignore when $x<0,$ differentiate, then convert back. This method can make differentiation (and solving limits) easier, so is obviously important, but I can't find a general proof that justifies it. EDIT : $x^x$ has domain $\mathbb R^+,$ so the above was a bad example! Instead, consider $y:\mathbb{R}\to\mathbb{R}.$ If we want to differentiate or take the limit of $y,$ can we take $\ln$ of $y$ without considering when $y\le0$ for $x\in\mathbb{R}\:?$ Then, differentiate or take the limit of $\ln y$ with respect to $x,$ then solve the equation for $\frac{\mathrm dy}{\mathrm dx}$ or $y$ and get the answer? I'm guessing that since the domain was never explicitly stated, I missed that $y>0$ for $\ln y.$ So, if $x\in\mathbb{R}$ and we change $y=x$ to $\ln y=\ln x,$ we let $x=-x$ for $x<0,$ right?","['calculus', 'derivatives', 'logarithms']"
4498386,"Why do we choose only the positive sign when substituting $x=\sin(t)$ into $\int \sqrt{1-x^2}\,\mathrm dx\;?$","When we substitute $x=\sin(t)$ into $$\int \sqrt{1-x^2}dx,$$ why we only take the "" $+$ "" sign even though $\sqrt{1-\sin^2(t)}=\pm \cos(t)\;?$ EDIT: Thank you to @Cameron Williams for the suggested problem . I know for the definite integral, we need to discuss carefully according to the quadrant for + or -, and that's no ambiguity. But here I want to ask for the indefinite case, where there is no specified quadrant, then why we only take the ""+"" sign? EDIT: Thank you to @EeveeTrainer for the suggested problem . But here it is not exactly to take square root on a number, but a function. $\sqrt{4}=2$ is not the same as $\sqrt{\cos^2(x)}=\cos(x)$ . Because $\cos(x)$ might be negative, depends on the quadrant of $x$ previously specified. Is the underneath assumption to choose a quadrant such that both $\sin$ and $\cos$ are positive, even though there is no quadrant specified for an indefinite integral? EDIT: Thank you to @finch's comment on the conventional interval chosen for $\sin(x)$ (or $\cos(x)$ ) so that they are invertible. For $\sin(x), ~~[-\frac{\pi}2, \frac{\pi}2];$ for $\cos(x), ~~[0, \pi].$ So, if I want to use substitute $$x=\sin(\theta)$$ into $$I_2=\int \sqrt{x^2}\sqrt{1-x^2} dx,$$ which interval should I choose? Do I choose $$[-\frac{\pi}2, \frac{\pi}2]\cap [0, \pi] ~?$$ ADDENDUM Say, I choose the domain for $\theta $ as $$[-\frac{\pi}2, \frac{\pi}2].$$ Then the integral becomes $$I_2=\int \sqrt{\sin^2(\theta)}\sqrt{\cos^2(\theta)} \cos(\theta)~d\theta=\int |\sin(\theta)|\cos(\theta) \cos(\theta)d\theta$$ $$\begin{align}
\text{If}~~~ \theta\in[-\frac{\pi}{2},0],~~I_2&=\frac{1}{3}\cos^3(\theta)+C=\frac{1}{3}(1-x^2)^{3/2}+C\\
\\
\text{If} ~~~\theta\in(0,\frac{\pi}{2}],~~I_2&=-\frac{1}{3}\cos^3(\theta)+C=-\frac{1}{3}(1-x^2)^{3/2}+C
\end{align}$$ Why do we usually choose only the bottom solution?","['integration', 'indefinite-integrals']"
4498411,What is the total number of ways a student appearing in the examination to get $5$ marks?,"In an examination there are $5$ multiple choice questions with $3$ choices, out of which exactly one is correct. There are $3$ marks for each correct answer, $-2$ marks for each wrong answer, and $0$ marks if the question is not attempted. Then what is the total number of ways a student appearing in the examination to get $5$ marks? One case possible is possible in which he scored $5$ marks when $3$ questions are correct and $2$ are incorrect. So number of ways become $\binom{5}{3}=10$ . But this answer is incorrect.","['combinations', 'combinatorics']"
4498434,Is this proof of a binary representation rigorous?,"From what I have seen so far, intro discrete math books usually provide a proof that every natural number can be uniquely expressed as a sum of distinct powers of $2$ in two steps: proving the (1) existence, and then (2) uniqueness. I believe that I have found a way that combines both statements, and I wonder if it may be used as a rigorous proof since it does not look ""typical."" Claim: Any natural number $N\geq1$ may be uniquely expressed as a sum $N=\sum_{k=1}^{n}2^{a_k}$ , where $a_{1}<a_{2}<...<a_{n}.$ Proof: Suppose $\exists X\subset\mathbb{N} \ \ | \ \  X=\{N\in\mathbb{N}:N\neq\sum_{k=1}^{n}2^{a_k}$ }, then, by the Well-Ordering Principle, there exists the smallest element $x\in X,$ which is the smallest counterexample. Therefore, suppose the claim is true for $x-1$ . We need to rule out the possibility of $x=1,$ since $1-1=0\ngeq1.$ Thus, Basis case: $1=2^{0}.$ Since the claim is true for $x-1$ , $x-1=2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n},\implies x=1+2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n}=2^0+2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n}.$ If $\nexists a_i=0$ for $i= \{ 1,2,...,n\},$ then the binary representation for $x$ exists and is unique. Hence, we reach a contradiction, and the claim is true. If $\exists a_i=0$ for $i= \{ 1,2,...,n\},$ then $2^{0}+2^{a_i}=2^0+2^0=2^1.$ Similarly, if $\nexists a_i=1$ for $i= \{ 1,2,...,n\},$ then the binary representation for $x$ exists and is unique. However, once again, if $\exists a_i=1$ for $i= \{ 1,2,...,n\},$ then $2^{1}+2^{a_i}=2^1+2^1=2^2.$ Continuing like that, we either stop at a particular $a_i<n$ which was not used in the expression for $x-1$ (and get a unique representation), or finally run into the case $a_i>n$ , which is obviously also unique as $a_i$ has not been used in an expression for $x-1$ . Thus, since the binary representation for $x$ exists ( $\Rightarrow \! \Leftarrow$ ), the claim is proven.","['induction', 'examples-counterexamples', 'discrete-mathematics']"
4498457,Which ultrafilters are (sort-of) commutators?,"There is a natural way to extend a binary operation $\star$ on $\mathbb{Z}$ to a semicontinuous binary operation $\widehat{\star}$ on $\beta\mathbb{Z}$ , the latter being the set of ultrafilters on $\mathbb{Z}$ ; see here (replacing $\mathbb{N}$ with $\mathbb{Z}$ as appropriate) . In general, algebraic properties of $\star$ are not preserved by this extension - e.g. $\widehat{+}$ has nontrivial idempotents and is non-commutative - but this extension can nonetheless be useful. I'm interested in understanding how noncommutative $\widehat{+}$ is (right now, the best result I know is that it has trivial center ). Normally one might attack this sort of question by looking at the commutator subgroup of $(\beta\mathbb{Z};\widehat{+})$ , but since $\widehat{+}$ is non-cancellative this doesn't make sense. We can however whip up a ""brute force"" analogue by considering $\widehat{+}$ and $\widehat{-}$ explicitly. This leads to the following: Question 1 : Is every nonprincipal ultrafilter $\mathcal{U}$ on $\mathbb{Z}$ equal to $$[(\mathcal{W}_1\widehat{+}\mathcal{W}_2)\widehat{-}\mathcal{W}_1]\widehat{-}\mathcal{W}_2$$ for some ultrafilters $\mathcal{W}_1,\mathcal{W}_2$ on $\mathbb{Z}$ ? Of course, this is a rather naive attempt to whip up a commutator-like notion: Question 2 : Is there a more useful analogue of the notion of commutator when looking at non-cancellative semigroups (namely $(\beta\mathbb{Z};\widehat{+})$ )? I know that there is a lot of universal algebra around generalized commutator theory , but to my limited understanding it works best when the algebra in question is (say) congruence modular, and it's not clear to me whether $(\beta\mathbb{Z};\widehat{+})$ is congruence modular.","['universal-algebra', 'logic', 'semigroups', 'general-topology', 'set-theory']"
4498466,Inverse image of a p-adic disk under a polynomial,"For a research project, I'd like to have the following result: Let $f \in \mathbb{Z}_p[x]$ be a polynomial of degree $d$ , and $r$ be a positive integer. Then the set of $a \in \mathbb{Z}_p$ such that $f(a) \equiv 0 \mod p^r$ is the union of at most $d$ disks in $\mathbb{Z}_p$ . I was able to cobble together a proof by induction on $r$ , but the result is so elegant that I can't be the first person to consider it. Does it crop up in standard texts, such as in the development of $p$ -adic analysis or Berkovich spaces?","['local-field', 'number-theory', 'p-adic-number-theory']"
4498498,Count number of pair of sequences,"Lets say you have two sequences of non negative integers each of length $n$ . ie $(a_1,a_2,...,a_n)$ and $(b_1,b_2,...,b_n)$ such that $\max(a_i) < k$ and $\max(b_i) < k$ Game rule: You can edit both sequence with $\mathrm{swap}(a_i, b_i)$ for $1 â‰¤ iâ‰¤ n$ , Goal: $a_i â‰¤ a_j$ for all $i â‰¤ j$ and $b_i â‰¤b_j$ for all $i â‰¤ j$ But not all initial sequence $a$ and $b$ can be solved. For example $(2,0)$ and $(0,1)$ is a pair of sequence that can't be solved. Now given $n$ and $k$ , count number of different pair of initial sequence $(a,b)$ that can be solved with game described above. Example: for $n=1$ , $k=2$ :
These are the cases: ${[(0),(0)],[(0),(1)],[(1),(0)],[(1),(1)]}$ .
Hence answer would be $4$ .","['game-theory', 'combinatorics', 'sequences-and-series']"
4498517,Evaluating curl of $\hat{\textbf{r}}$ in cartesian coordinates,"I am reading book about Classical Mechanics, which states that a spherically symmetric central force is always conservative, and I want to prove it. Spherical symmetry of a function $f$ means that in spherical coordinates the partial derivatives wrt the polar and azimuthal angles are zero: $$\frac{\partial f}{\partial \phi}=\frac{\partial f}{\partial \theta}=0$$ (I am using the mathematician's convention that $\phi$ is the polar and $\theta$ is the azimuth.) A central force is just a force of the form $\textbf{F}=f(\textbf{r})\hat{\textbf{r}}$ , where $\textbf{r}$ is the position. Thus, a spherically symmetric central force is of the form $\textbf{F}=f(r)\hat{\textbf{r}}$ . The task to show this to be conservative, which is the same as showing that the curl vanishes (a result of Stokes' theorem). So the task is: $\nabla\times f(r)\hat{\textbf{r}}=0$ Now, I can easily evaluate this curl in spherical coordinates without a problem, but I do want to evaluate this in cartesian coordinates just to see that it works, despite the fact that it's probably a bad idea. Anyway, $\hat{\textbf{r}}$ in cartesian coordinates is $$\hat{\textbf{r}}=\sin(\phi)\cos(\theta)\hat{\textbf{x}}+\sin(\phi)\sin(\theta)\hat{\textbf{y}}+\cos(\phi)\hat{\textbf{z}}$$ For simplicity, I ignore the $f(r)$ factor. The curl is $$\nabla\times \hat{\textbf{r}}=
\begin{bmatrix}
\frac{\partial \cos(\phi)}{\partial y}-\frac{\partial \sin(\phi)\sin(\theta)}{\partial z}\\
\frac{\partial \sin(\phi)\cos(\theta)}{\partial z}-\frac{\partial \cos(\phi)}{\partial x}\\
\frac{\partial \sin(\phi)\sin(\theta)}{\partial x}-\frac{\partial \sin(\phi)\cos(\theta)}{\partial y}
\end{bmatrix}$$ I thought that I could use the chain rule on $\frac{\partial\cos(\phi)}{\partial y}$ in a perhaps hand-wavy manner: $$\frac{\partial\cos(\phi)}{\partial y}=\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial \phi}{\partial y}\right)=\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial y}{\partial \phi}\right)^{-1}$$ But I tried it on the $x$ coordinate: $$\frac{\partial \cos(\phi)}{\partial y}-\frac{\partial \sin(\phi)\sin(\theta)}{\partial z}=$$ $$\frac{\partial\cos(\phi)}{\partial \phi}\left(\frac{\partial y}{\partial \phi}\right)^{-1}-\frac{\partial \sin(\phi)}{\partial \phi}\left(\frac{\partial z}{\partial \phi}\right)^{-1}\sin(\theta) -\frac{\partial \sin(\theta)}{\partial \theta}\left(\frac{\partial z}{\partial \theta}\right)^{-1}\sin(\phi)=$$ $$-\sin(\phi)\left(\frac{\partial y}{\partial \phi}\right)^{-1}-\cos(\phi)\left(\frac{\partial z}{\partial \phi}\right)^{-1}\sin(\theta) -\cos(\theta)\left(\frac{\partial z}{\partial \theta}\right)^{-1}\sin(\phi)= (*)$$ We know: $y=r\sin(\phi)\sin(\theta)$ , $z=r\cos(\phi)$ , so we obtain $$\frac{\partial z}{\partial \phi}=-r\sin(\phi)\qquad \frac{\partial y}{\partial \phi}=r\cos(\phi)\sin(\theta)$$ $$(*)=-\sin(\phi)\left(r\cos(\phi)\sin(\theta)\right)^{-1}-\cos(\phi)\left(-r\sin(\phi)\right)^{-1}\sin(\theta) -\cos(\theta)\left(-r\sin(\phi)\right)^{-1}\sin(\phi)$$ $$=-\tan(\phi)/r\sin(\theta)+\sin(\theta)/r\tan(\phi) +\cos(\theta)/r$$ $$=\frac{\sin^{2}(\theta)+\cos(\theta)\sin(\theta)\tan(\phi)-\tan(\phi)\sin(\theta)}{r\sin(\theta)\tan(\phi)}$$ $$\neq 0$$ I suspect something went wrong during the application of the chain rule. So how would this curl be evaluated correctly? How does this generalize to the function $f(r)\hat{\textbf{r}}$ ?","['multivariable-calculus', 'vector-analysis']"
4498522,"How can we study $\lim_{t\to\infty}\int_{\mathbb{R}}\psi_t(x)dx$, when $\lim$ and $\int$ do not commute?","I am dealing with the problem of studying $$\mathcal{I}= \lim_{t\to\infty}\int_{\mathbb{R}}\psi_t(x)dx$$ In my problem $\psi_t(x)$ is of the form $f_t(x)\mathbf{1}_{x>t}$ . Moreover, $f_t(x)=\frac{g(x)}{g(t)}\varphi(t-x+k)$ where $g>
0$ is the probability density function of a continuous unbounded support random variable and $\varphi$ is the probability density function of a standard normal ( $k$ being a constant). It is clear that $\lim_{t\to\infty}\psi_t(x)$ is pointwise $0$ . As a consequence, whenever I can bound $|\psi_t(x)|$ by an integrable function it's easy to conclude $\mathcal{I}=0$ by dominated convergence theorem. The ratio appearing in the integrand is always smaller than one (at least when $g$ is eventually decreasing) but, since the $f$ factor translates, it is difficult to find the desired bound. Actually, depending on shape of $g$ , this may not be possible.Indeed, numerical simulation suggest that, for many interesting choices of $g$ , we have $\mathcal{I}\neq 0$ . In these cases, it seems that it must not be possible to exchange integral and limit. How to proceed ? EDIT It is useful to notice that by de l'hospital $$-\lim_{t\to\infty}\frac{g'(t)}{g(t)}=\lim_{t\to\infty} \frac{g(t)}{1-G(t)}$$ which appears in the calculations below. Here Computation of $ \lim_{t\to\infty}\int_t^\infty\frac{g(x)}{\sigma g(t)}f(\frac{t-x}{\sigma}+k)dx$ with $g$ and $\varphi$ density functions. I attempt a probably flawed calculation.","['integration', 'measure-theory', 'real-analysis', 'calculus', 'probability']"
4498529,Asymptotics applied to approximate solutions of trascendental equations,"I'm self studying ""Olver - Asymptotics and Special Functions"": I'm new to asymptotics and so there are some things I am not getting. First, the author states the following theorem: Theorem 3.1. Let $\sum_{s=0}^\infty a_s z^s$ converge when $|z|<r$ . Then for fixed $n$ it is $\sum_{s=n}^\infty a_s z^s=O(z^n)$ in any disk $|z|\le\rho$ such that $\rho <r$ . And proceeds to study the asymptotic behavior of the roots of trascendental equations: for instance, he considers the equation $x \tan x=1$ for large $x>0$ . Inverting, it is $x=n\pi+\arctan(1/x)$ for $n\in\mathbb{Z}$ for the principal value of arctan. Since arctan is bounded, it is $x \sim n\pi$ as $n \to \infty$ . Since for $x>1$ the Taylor's series of $\arctan(1/x)$ converges, he then writes: $$x=n\pi+\frac{1}{x}-\frac{1}{3x^3}+\frac{1}{5x^5}-\frac{1}{7x^7}+\dots$$ Finally, he claims that hence $x=n\pi+O(x^{-1})=n\pi+O(n^{-1})$ and that next two substitutions produce $$x=n\pi+\frac{1}{n\pi}+O\left(\frac{1}{n^3}\right) \tag{1}$$ $$x=n\pi+\frac{1}{n\pi}-\frac{4}{3(n\pi)^3}+O\left(\frac{1}{n^5}\right) \tag{2}$$ First question: why is it $n\pi+O(x^{-1})=n\pi+O(n^{-1})$ ? I know that $f(x)=O(g(x))$ as $x \to x_0$ means that $|f(x)|\le K|g(x)|$ for some constant $K$ in a neighborhood of $x_0$ , maybe this comes from the Archimedean property and I can always find a natural $n$ such that $n<x$ and so $\frac{1}{x}<\frac{1}{n}$ , so I can say (for $x>0$ ) that $f(x)=O(x^{-1})\implies f(x)\le \frac{K}{x}<\frac{K}{n}\implies f(x) \le \frac{K}{n} \implies f(x)=O(n^{-1})$ ? Or this is wrong and he is using another kind of reasoning? Second question: I am not getting the calculations to obtain equations $(1)$ and $(2)$ . What I tried so far is the following. From theorem 3.1 with $n=1$ , I know that $$x=n\pi+\frac{1}{x}+O\left(\frac{1}{x^3}\right)$$ Substituting $x=n\pi+O(n^{-1})$ and using the properties $O(cf)=O(f)$ for $c>0$ , $fO(g)=O(fg)$ and $O(f)O(g)=O(fg)$ , it is $$x=n\pi+\frac{1}{n\pi+O\left(\frac{1}{n\pi}\right)}+O\left(\frac{1}{\left(n\pi+O\left(\frac{1}{n\pi}\right)\right)^3}\right)=n\pi+\frac{1}{n\pi+O\left(\frac{1}{n\pi}\right)}+O\left(\frac{1}{n^3\pi^3+O(n)+O\left(\frac{1}{n}\right)+O\left(\frac{1}{n^3}\right)}\right)$$ But I don't know how to manipulate the big-Os in the denominators. I have similar problems for $(2)$ , putting $n=2$ in theorem 3.1 and have no clue how to manipulate the big-Os.","['asymptotics', 'analysis']"
4498537,How can we prove $\sqrt{2+\sqrt{2+....\sqrt{2+\sqrt{2}}}} = 2\cos\left(\frac {\pi }{2^{n+1}}\right)$ without induction,"I wanted to know the proof without induction / substitution method  for the equation $$\underbrace {\sqrt{2+\sqrt{2+...\sqrt{2+\sqrt{2}}}} }_{\text{n-times}}= 2\cos\left(\frac {\pi }{2^{n+1}}\right)$$ Proof with induction: \begin{align}
& n=1: \\
& \sqrt{2}=2\cos\left( \dfrac{\pi}{4} \right). \\
\ \\
& \text{Assume that the equation is valid when }n=k. \\
\ \\
&n=k+1; \\
& \underbrace {\sqrt{2+\sqrt{2+...\sqrt{2+\sqrt{2}}}} }_{\text{k+1-times}}=\sqrt{2+\underbrace {\sqrt{2+\sqrt{2+...\sqrt{2+\sqrt{2}}}} }_{\text{k-times}}} \\
& = \sqrt{2+2\cos\left( \dfrac{\pi}{2^{k+1}} \right)} = \sqrt{2}\cdot\sqrt{2\cos^2\left( \dfrac{\pi}{2^{k+2}} \right)} = 2\cos\left(\dfrac{\pi}{2^{k+2}}\right). \blacksquare
\end{align}","['trigonometry', 'solution-verification']"
4498546,Prove that the set of all ternary sequences is uncountable,"I have the following question in Discrete Math 2 in one of my assignments. A ternary sequence is defined as a function $t\colon\mathbb{N}_0\longrightarrow\mathbb{N}_0$ such that $t(3n+1) = t(3n)+1$ and $t(3n+2) = t(3n+1)+1 = t(3n)+2$ , where $\mathbb{N}_0$ is the set of all natural numbers including zero (in my course the natural numbers don't include zero). The task is to prove that the set of all ternary sequences (denoted as $T$ ) is uncountable. For that I need to show that $|T|\ge\aleph_0 \land |T|\ne\aleph_0$ . I have managed to show that $|T|\ge\aleph_0$ but I am struggling to show that $|T|\ne\aleph_0$ .
I need to use (as the question demands that) Cantor's diagonal argument (with proof by contradiction) but I am having troubles finding a proper function. I assumed there exists a function $F\colon\mathbb{N}_0\longrightarrow T$ which is bijective. As $F(n)$ is a function I'll use the following notation $F_n = F(n)$ From this I can assume that the function looks something along the lines of: $F_0 = \{(0,F_0(0)),(1,F_0(1)),(2,F_0(2)),\dots\} \\ F_1 = \{(0,F_1(0)),(1,F_1(1)),(2,F_1(2)),\dots\} \\ F_2 = \{(0,F_2(0)),(1,F_2(1)),(2,F_2(2)),\dots\} \\ \vdots$ I am not sure how to build a sequence $t\colon\mathbb{N}_0\longrightarrow\mathbb{N}_0$ such that $t\in T$ and that I won't be able to find an input for it in $F$ . I'd love it if someone can give me a hint so that I will be able to solve this question. Thanks in advance.","['set-theory', 'cardinals', 'discrete-mathematics']"
4498583,"Minimum value of $\sum_{k=1}^n|1+z^{(2^k)}|, z\in\mathbb{C}$ in terms of $n$","What is the minimum value of $S=\sum_{k=1}^n|1+z^{(2^k)}|, z\in\mathbb{C}$ in terms of $n$ ? Experimenting on desmos and wolfram suggests the following claims: $S$ is minimized when $|z|=1$ and $\arg{z}=\dfrac{\lfloor{\frac{2^n}{3}}+\frac{1}{2}\rfloor}{2^n}\pi$ (The numerator is the Jacobsthal sequence .) $S_\text{min}=n-f(n)$ where $\lim_{n\to\infty}f(n)\approx0.747771497$ I do not know how to prove any of these claims. It is easy to show that if $|z|=1$ then $S=2\sum_{k=0}^{n-1} {|\cos{(2^kx)}|}$ where $x=\arg{z}$ . This is related, but I'm not sure if it helps: $|\cos x|+|\cos 2x|+\cdots+|\cos 2^nx|\geq \dfrac{n}{2\sqrt{2}}$ . That's all I've been able to do. (This question was inspired by similar questions such as this , this , this and this .)","['complex-analysis', 'maxima-minima', 'sequences-and-series', 'trigonometry', 'complex-numbers']"
4498608,Region of $ \frac{1-x}{1-y} $,"In the book ""Ordinary Differential Equations: An Elementary Textbook for Students of Mathematics, Engineering, and the Sciences"" by Tenenbaum. He defines a region as follows: A set in the plane is called a region if it satisfies the following two conditions: Each point of the set is the centre of a circle whose entire interior consists of points of the set Every two points of the set can be joined by a curve which consists entirely of points of the set Now, in Exercise 2, section 8 he ask to determine the domain of $ z = \frac{1-x}{1-y} $ . The domain is all pairs of points $ (x,y) $ in the set of real numbers such that $ y \neq 1 $ . Later, he says this is a region. However, I think it must be wrong as the vertical plane intersecting the line $ y = 1 $ cuts the plane XY and makes the second point of the definition of region unfeasible as we can have two points of the set on opposite sides that can't be joined by a curve which consists entirely of points of the set. Let's say (0, -1) and (0, 3) Is an errata or am I missing something?","['functions', 'ordinary-differential-equations']"
4498619,Good Source for practicing discrete mathematics puzzles?,"I am looking to practice questions like these: Problem 1 (see text below) Problem 2 Problem 3 As you can see, these problems involve discrete mathematics, and are sort-of discrete mathematics puzzles. I want to practice more questions like these. I know that there are multiple books on discrete mathematics, but I have observed that these problems don't test your theoretical knowledge much, but rather your problem-solving skills. Discrete Mathematics books, however, would focus more on the theoretical aspect. I know that there is a book: Algorithmic Puzzles by Anany and Maria Levitin which seems really good. Can anyone recommend any other recommendations / books for such problems? Kind Regards Canopy PS: I can not embed images so this is the gist of the first problem in case link doesn't work: A flexadecimal number consists of a sequence of digits, with the rule that the rightmost
digit must be $0$ or $1$ , the digit to the left of it is $0$ , $1$ , or $2$ , the third digit (counting
from the right) must be at most $3$ , and so on. We write flexadecimal numbers in angle brackets to distinguish them
from ordinary, decimal numbers. Thus $\langle 34101\rangle$ is a flexadecimal number, but $\langle231\rangle$ is
not, because the digit $3$ is too big for its place. (If flexadecimal numbers get very long,
we will need â€˜digitsâ€™ with a value more than $9$ .) Thus, the numbers $1$ to $13$ are
represented as $1, 10, 11, 20, 21, 100, 101, 110, 111, 120, 121, 200, 201$ flexadecimal numbers. (i) Write the numbers from $5$ to $13$ in flexadecimal. (ii) Describe a workable procedure for converting flexadecimal numbers to decimal,
and explain why it works. (iii) Describe a workable procedure for converting decimal numbers to flexadecimal. (iv) Describe a procedure for addition that works directly on the digits of two flexadecimal numbers, (v) Given a flexadecimal number, how could you test whether it is a multiple of $3$ without converting it to decimal? (vi) If the $\langle100000\rangle$ arrangements of the letters abcdef are listed in alphabetical order and numbered $\langle0\rangle: abcdef , \langle1\rangle: abcdfe, \langle10\rangle: abcedf$ , etc., what arrangement
appears in position $\langle34101\rangle$ in the list?","['contest-math', 'book-recommendation', 'reference-request', 'discrete-mathematics', 'soft-question']"
4498622,Convergence of integral on every measurable subset,"Let $\{f_n\}$ be sequence in $L^2[0,1]$ and $f$ be a Lebesgue measurable function such that for every Lebesgue measurable set $E$ in $[0,1]$ , $\lim_{n \to \infty} \int_{E}f_n dx = \int_{E}fdx$ . Assume also that $\sup_{n}\int_{0}^{1} \lvert f_n \rvert^2 dx < \infty$ . I want to show that $f \in L^2[0,1]$ . My thought so far: If we manage to show there is a subsequence of $f_n$ converging pointwise a.e. on $[0,1]$ to $f$ , we're done. Indeed, if this is true, then by Fatou's lemma, we have $$\int_{0}^{1} \lvert f \rvert^2 dx \leqslant \liminf \int_{0}^{1} \lvert f_{n_k}\rvert^2 dx \leqslant \sup_{n}\int_{0}^{1} \lvert f_n \rvert^2 dx < \infty$$ But I don't know how to achieve that. I tried to show $f_n \to f$ in $L^1$ or in measure but I failed. Any insight would be appreciated.","['measure-theory', 'real-analysis']"
4498627,Proving an identity regarding character of irreducible representation,"Let $\frak{X}$ be an irreducible representation of a finite group $G$ affording the character $\chi$ . Prove that for every $x,y\in G$ : $$\chi(x)\chi(y)=\frac{\chi(1)}{|G|}\sum_{z\in G}\chi(yzxz^{-1}).$$ My attempt: I know that I can replace $zxz^{-1}$ with $w$ , and then the sum will be only on the conjugacy class of $x$ , we'll denote it by $[x]$ , but I don't know how many times every summand will appear after the change of variables, and to be honest I don't know how to take it from here.
Any help would be appreciated.","['characters', 'representation-theory', 'finite-groups', 'group-theory', 'group-actions']"
