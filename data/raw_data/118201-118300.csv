question_id,title,body,tags
1753365,Average volume of set of cubes using the mean and variance of its side lengths.,"First, i tried this question:
The side lengths of a set of squares have an average of 5 and variance of 4. What is their average area? Let X = The Side Length From this question , I knew we had to calculate $E(X^2)$. i.e. $$Var(X) = E(X^2) - E(X)^2$$ and as, $$Var(X) = 4, E(X) = 5$$
The average area of the set of squares is therefore $E(X^2) = 29$. However, if the question were talking about a set of cubes, how would you work out their average volume? Is it possible from just the information given? (Does it involve $E(X^3)$ by any chance too?)",['statistics']
1753368,"Why does $\left(\frac b2\right)^2$ ""geometrically complete the square?""","I was just reading this MathisFun article on completing the square. It states that geometry can help complete the square. It starts off with a square and a rectangle (pictures come from link): Then, it cuts $b$ in half, and moves it under the $x^2$ square: Now, the square is ""nearly completed"", but it has this part that completes the square that equals $\left(\frac b2 \right)^2$ (circled in blue): My question is, where did that part come from, and why does it equal $\left(\frac b2 \right)^2$. The article doesn't give me a reason and there are no other sources as to why.","['completing-the-square', 'quadratics', 'geometry']"
1753370,Understanding simplifications of complex terms $\exp(-ik\pi/4)$,I read that $1\over{2}$$\pi$$i$($\exp[-3i\pi/4]+\exp[-9i\pi/4])$ = $1\over{2}$$\pi$$i$($-\exp[i\pi/4]+\exp[-i\pi/4])$ = $\pi$$\sin(\pi/4)$ = $\pi\sqrt{2}$ Can you help me to understand how we move from the first line to the second? Thanks.,"['trigonometry', 'complex-numbers']"
1753376,Left-invariant vector fields on the circle $S^1$,"I'm trying to find the left-invariant vector fields on the circle $S^1$. If I understand correctly, $S^1$ is given the group structure of the multiplicative group of complex numbers on the unit circle in the complex plane,
and $S^1$ is made into a $C^\infty$ manifold by forming the maximal atlas from the four projection maps from open semi-circles of $S^1$ onto the $x$- and $y$-axes. Given a point $p\in S^1$, if $p$ is on the upper or lower open semicircles, then a basis for $T_pS^1$ is given by $\{\partial/\partial \bar x \rvert_p\}$, where $\bar x$ is the projection onto the $x$-axis,
while if $p$ is on the left or right open semicircles, then a basis for $T_pS^1$ is given by $\{\partial/\partial \bar y \rvert_p\}$. Given a vector field $X$ on $S^1$, $X$ is left-invariant provided that for all points $g,h\in S^1$,
$$
(\ell_g)_{*,h}(X_h) = X_{gh} \text,
$$
where $\ell_g\colon S^1 \to S^1$ is left-multiplication by $g$. To obtain an arbitrary left-invariant vector field $X$ on $S^1$, I should be able to just pick an arbitrary tangent vector $A \in T_1S^1$ and see what it generates.
Let $A = a(\partial/\partial \bar y\rvert_1) \in T_1S^1$.
Then given a point $p\in S^1$, I can define
$$
X_p = (\ell_p)_{*,1}(A).
$$
However, I don't see how this is getting me any closer to concretely writing down what $X$ looks like. Any suggestions?","['manifolds', 'smooth-manifolds', 'differential-geometry', 'lie-groups']"
1753412,If $\varphi(x) = m$ has exactly two solutions is it possible that both solutions are even?,"If $\varphi(x) = m$ has exactly two solutions is it possible that both solutions are even? Here, $\varphi(x)$ is Euler's phi function, the number of positive integers less than or equal to $x$ that are relatively prime to $x$. It appears that when $\varphi(x) = m$ has exactly two solutions then one of the solutions, $x$, is odd and the other solution is even, specifically $2\times x$.  The first few integers $x$ such that $\varphi(x) = m$ has exactly two solutions are:
$1,11,23,29,31,47,53,81,59,\dots$  where the other solution is necessarily $2\times x$.  For example, $\varphi(81) = \varphi(162) = 54$  and there are no other integers $k$ such that $\varphi(k) = 54$.  See the sequence $A007366$ in Sloane's OEIS.  I determined that the initial terms of this sequence were all odd with a brute force Mathematica code that is accurate (and timely) for about $1000$ terms. The emperical evidence suggests that if $\varphi(x) = m$ has exactly two solutions then exactly one of them is odd.  I want to prove this statement. It is straight forward to show that both solutions cannot be odd since if $x$ is odd then $\varphi(x) = \varphi(2\times x)$. I am considering the case where both solutions are even, hoping for a contradiction.  I have determined that if both solutions, say $x$ and $y$, are even then $x$ and $y$ are both divisible by $4$.  Also, in this case, at least one of $x$ or $y$ is divisible by $3$.  Now I am stuck. Is there some way to reach a contradiction here. Is there another way to prove (or attempt to prove) the conjecture?","['number-theory', 'totient-function', 'elementary-number-theory']"
1753454,Hilbert function and homogenous polynomials.,"Let $\{[1:0:0],[0:1:0],[0:0:1],[1:1:1] \} = \{p_1,p_2,p_3,p_4\}$ be four points in the projective space $\mathbb{P}^2$. For every $p_i$, show there is a homogenous polynomial $f_i$ such that $$\left\{\begin{matrix}
f_i(p_j) \neq 0 &  i= j\\ 
 f_i(p_j) = 0 & i \neq j
\end{matrix}\right..$$ Now if $I$ is the ideal associated with these four points, then the Hilbert function $H(R/I,t) = 4$ for all $t > 1$. I was thinking $f_i = x_1x_2x_3 + x_i^3$ might work, but the last point $[1:1:1]$ is killing me here.","['abstract-algebra', 'projective-space', 'algebraic-geometry', 'commutative-algebra']"
1753461,Bizarre failure of integrating factor in elementary differential equation,"I have an issue with an extremely elementary problem. Consider the differential equation $y' + \cot(x) y = 1$. Obviously, one can use an integrating factor of $e^{\int \cot(x) dx} = e^{\ln(\sin(x)) } $ (the arbitrary constant would cancel out) $= \sin(x)$ to solve the differential equation, obtaining the correct answer $y = - \cot(x) + C \csc(x)$. However, the assertion $ \int \cot(x) dx = \ln(\sin(x)) +C $ is true only modulo subtle things involving branches of $\ln$ in the complex plane. Restricted to the real line, we use $\int \cot (x) dx = \ln |\sin(x)| +C$. If we do the above method, we get an integrating factor not of $\sin(x)$ but of $|\sin(x)|$: $$ |\sin(x)| y' + \cot(x) | \sin(x)| y = |\sin(x)|$$ Indeed, $\frac{d}{dx} |\sin(x)| = \cot(x) | \sin(x)|$, so this is a valid alternate choice of integrating factor. Proceeding, we have $$ \frac{d}{dx} ( |\sin(x)| y ) = |\sin(x)| $$
$$ y = \frac{\int |\sin(x)| dx}{|\sin(x)|}$$ But this is not equal to the correct answer of $- \cot(x) + C \csc(x)$! What is going on? EDIT: It seems every calculus solution manual ever is wrong. EDIT 2: Alternatively, it seems every introductory calculus textbook, including Stewart, gives the wrong definition of ""general solution"".","['integration', 'ordinary-differential-equations']"
1753485,Polynomial equation: $P(\sin t) = P(\cos t)$,"Let $P(X)$ be a polynomial with real coefficients such that $P(\sin t) = P(\cos t), \, \forall t \in \mathbb R$. Prove that there exists a unique polynomial $Q(Y)$ with real coefficients, such that $P(X) = Q(X^4-X^2)$. (The converse is trivially true.) NOTE :  This problem has almost nothing to do with trigonometry, despite appearances to the contrary. It's really about polynomials. Edit :  It occurred to me I should add this - for full disclosure: I created this problem many years ago, for the math Olympiad in the East European country I grew up in; I am not looking for a solution for myself. I am offering it as a fun challenge to the math fans on this forum.","['algebra-precalculus', 'polynomials']"
1753504,How can it be shown that a Möbius transformation can have at most two fixed points unless it is $f(z) = z$?,"Obviously the identity fixes all points, but why do Möbius transformations fix at most two? I know that Möbius transformations map circles and lines to circles and lines, but how does that imply that no more than two points can be fixed?","['fixed-points', 'complex-analysis', 'mobius-transformation']"
1753556,Why is the group of unit upper triangular matrices solvable?,"Let $GL_n(k)$ be the $n$ by $n$ general linear group over $k$ , $B_n(k)$ be the subgroup of $GL_n(k)$ consisting of all upper triangular matrices, and $U_n(k)$ be the subgroup of $B_n(k)$ whose diagonal elements are all $1$ . To show $B_n(k)$ is solvable, I'm proving it now by following steps: $U_n(k)$ is a subgroup of $B_n(k)$ . (done) $U_n(k)$ is normal in $B_n(k)$ . (done) $U_n(k)$ is solvable. (question) $B_n(k) / U_n(k)$ is also solvable. (not yet) $B_n(k)$ is solvable. (by the below thm) I'll use a theorem to verify $B_n(k)$ is solvable. $G$ is solvable if and only if $H$ and $G/H$ are solvable for some normal subgroup $H$ of $G$ . So, I have to prove both step 3 and step 4. But I have no idea about them. How to prove them? Since my knowledge is not enough, I don't want to show them using Lie theory. Thanks in advance.","['matrices', 'solvable-groups', 'group-theory']"
1753563,Integral of a Gradient function (and another function?),"I'm aware of line integrals around planes and curves, but I cannot make up how to approach this question: $$\int_C f∇f \cdot \,d\mathbf{r} $$ where $f(x,y,z)=xz\cos(x^2+y^2)$ and C is the intersection of the cylinder $x^2+y^2=1$ and $x+y+2z=2$ Suppose I find the intersection, does the integral constitute of a product of a gradient function and its function? If so, how do I approach this?","['multivariable-calculus', 'integration']"
1753566,Application of Banach-Steinhaus theorem,"Let  $(x_n)$ be a sequence in a Banach space $E$ such that $\sum_{j=1}^{\infty} |\varphi (x_j) |<\infty$, $\forall \varphi \in E'.$ Then $\sup \limits_{\|\varphi\| \leq 1} \sum_{j=1}^{\infty}|\varphi (x_j)| <\infty $. My attempt: For all $n \in \mathbb{N}$, define $f_n: E' \to \mathbb{K}$, $f_n(\varphi) = \sum_{j=1}^{n} \varphi (x_j)$. ($\mathbb{K} = \mathbb{R}$ or $\mathbb{C}$) Each $f_n$ is a continuous linear functional, since: $$|f_n(\varphi)| = \bigg| \sum_{j=1}^{n} \varphi (x_j) \bigg| \leq 
\sum_{j=1}^{n} |\varphi (x_j)| \leq \sum_{j=1}^{n} \|\varphi\|\|x_j\| = 
(n \max \{\|x_j\|\} ) \|\varphi\|$$ For each $\varphi \in E'$, $(|f_n(\varphi)|)$ is bounded since
$$|f_n(\varphi)|  \leq \sum_{j=1}^{n} |\varphi (x_j)| \leq 
\sum_{j=1}^{\infty} |\varphi (x_j)| = M_\varphi \in \mathbb{R}  $$ By Banach-Steinhaus theorem, there exists $M>0$ such that $\sup \limits_{n \in \mathbb{N}}  \|f_n\|<M$. For all $n \in \mathbb{N}$, we have:
$$M> \|f_n\| = \sup \limits_{\|\varphi\| \leq 1} |f_n(\varphi)| = 
\sup \limits_{\|\varphi\| \leq 1} \bigg|\sum_{j=1}^{n}\varphi (x_j)\bigg| $$ Then
$$\sup \limits_{\|\varphi\| \leq 1} \bigg|\sum_{j=1}^{\infty}\varphi (x_j)\bigg| \leq M $$ Unfortunately, this is not  what we want to prove. How can I fix it?",['functional-analysis']
1753593,Why are Del Pezzo surfaces rational?,"Let $X$ be a Del Pezzo surface over an algebraically closed field $k$, i.e. a projective surface with $-K_X$ ample. I'm chiefly interested in the case that $k$ has characteristic $0$ and $X$ is smooth, so feel free to assume these if it simplifies things. Here's the idea I have so far: From the adjunction formula, we can see that any $-1$ curve is rational, since $K_X \cdot C < 0$ for any curve $C$. In fact, if $-K_X$ is very ample and $X$ is embedded in $\mathbb{P}^n$ via $-K_X$, this can be extended to show that lines are exactly the $-1$ curves, since $K_X \cdot C = \deg C$. In addition, $K_X^2 = \deg X$ as a subvariety of $\mathbb{P}^n$. EDIT: As Pooh Bear pointed out in the comments, the idea below is not correct: blowing down a $-1$-curve increases $K_X^2$ by $1$. I'd still love to see a correct proof. Now, I know Castelnuovo's Criterion shows that we can contract any of the $-1$ curves on $X$ to a point and that this decreases $K_X^2$ by $1$. So if I knew: $-K_X$ is always very ample. The blow-down of a Del Pezzo surface is still Del Pezzo. Any Del Pezzo surface other than $\mathbb{P}^2$ has a $-1$ curve, or, any Del Pezzo surface has a rational curve. Then I could keep contracting $-1$ curves and decreasing the degree until I find a Del Pezzo surface embedded in $\mathbb{P}^n$ as a degree $1$ subvariety, which should be a linear subspace (right??). Does this work?","['algebraic-geometry', 'birational-geometry']"
1753599,Diophantine equation: choosing the right modulus to prove an equation cannot be satisfied,"I was looking at this problem , which asks to show that there are no $m,n \in \mathbb Z$ such that
$$3n^2+3n+7 = m^3.$$
The result follows immediately from considering the equation modulo $9$ and enumerating the possibilities. But what if we somehow missed $9$ and instead considered other moduli? In other words, we would be aiming to find a $k \in \mathbb Z$ for which the sets $\{3n^2 + 3n + 7:n\in\mathbb Z\}$ and $\{m^3:m\in\mathbb Z\}$ are disjoint modulo $k$, thereby proving the impossibility of finding such $m,n$. I've looked through all $k \in [2, 5000]$, and the winners are precisely the multiples of $9$. Why is this so? The main question I have is this: Is it true that $\{3n^2 + 3n + 7:n\in\mathbb Z\}$ and $\{m^3:m\in\mathbb Z\}$ are disjoint modulo $k$ if and only if $k \equiv 0 \pmod 9$?","['number-theory', 'diophantine-equations']"
1753606,SVD - Decomposed Matrix Sizes,"I had a question about SVD. Specifically about the size of matrices $U$, $\Sigma$ and $V$ decomposed from the $m\times n$ matrix $X$ using the formula $$X = U \Sigma V^T $$ Most of the the tutorial literature says that the resulting sizes are $U$ is $m \times m$ $\Sigma$ is $m \times n$ $V$ is $n \times n$ However, there have been quite few times when the sizes given are $U$ is $m \times n$ $\Sigma$ is $n \times n$ $V$ is $n \times n$ In other words instead of $\Sigma$ being the matrix with possibly different number of rows and columns, its $U$ with the different number of rows and columns. The math works out, so why (and in what cases) is this less frequent version used?","['eigenvalues-eigenvectors', 'svd', 'linear-algebra']"
1753645,Evaluation of limit at infinity: $\lim_{x\to\infty} x^2 \sin(\ln(\cos(\frac{\pi}{x})^{1/2}))$,$$\lim_{x\to\infty} x^2 \sin(\ln(\cos(\frac{\pi}{x})^{1/2}))$$ What I tried was writing $1/x=t$ and making the limit tend to zero and writing the cos term in the form of sin,"['logarithms', 'trigonometry', 'limits']"
1753669,a set $X$ is infinite iff there isn't a bijection from $I_n\subset N$ to it,"Consider the set: $$I_n = \{p\in \mathbb{N}; 1<p\le n\}$$ My book says that a set is finite when it's not empty or when there exists, for some $n\in \mathbb{N}$, a bijection:
$$\phi: I_n\to X$$ Then, it defines an infinite set as a set that is not finite. This can be understood as: a set $X$ is infinite when it's not empty and for all $n\in \mathbb{N}$, there isn't any bijection $\phi: I_n \to X$ Later, my teacher gave a list of exercises, in which one of them is: Prove that a set $X$ is infinite iff it's no empty neither has a bijection $f: I_n \to X$ no matter which $n\in \mathbb{N}$ Well, I know that: $\to$
if there isn't a surjective function $f: I_n \to X$, then there isn't also a bijective function $f: I_n\to X$, because bijectivity is surjectivity with injectivity, and by the definition, this set is not finite, that is, infinite. $\leftarrow$
well, since $X$ is infinite, then it's not finite, which means that there isn't any bijective function from $I_n$ to $X$, so I know that this function must be either only injective, only surjective, or none of them. I must prove now that surjectivity must always fail , but I can't see anymore assumptions to use here. Intuitively I know that there can't possibly exist a surjective function from $I_n$ to an infinite set because all members of $I_n$ must be mapped to a unique element in $X$, but there are infinite elements in $X$. Could someone help me?","['elementary-set-theory', 'functions']"
1753671,$G_{\mathfrak a}(A)$ integral domain and $\bigcap \mathfrak a^n = 0$ implies $A$ is integral domain,"This is Lemma 11.23 in Atiyah: For an ideal $\mathfrak a \subseteq A$ , define $G_{\mathfrak a} (A) = \bigoplus _{n=0} ^\infty \mathfrak a^n / \mathfrak a^{n+1}$ . The statement of the Lemma: Let $A$ be a ring, $\mathfrak a$ an ideal of $A$ such that $\bigcap_n \mathfrak a^n = 0$ . Suppose that $G_{\mathfrak a} (A)$ is an integral domain. Then $A$ is an integral domain. Proof: Let $x,y$ be nonzero elements of $A$ . Then since $\bigcap \mathfrak a^n = 0$ , there exist nonnegative integers $r,s$ such that $x \in \mathfrak a^r - \mathfrak a^{r+1}$ and $y \in \mathfrak a^s - \mathfrak a^{s+1}$ . My question is why should the powers of $\mathfrak a$ cover everything in $A$ to begin with? Certainly if this were true, and the intersection of all the powers is zero, then the above follows. So it seems that this depends on something along the lines of $$\bigcup \mathfrak a^n = A.$$ Does it have to do with the $\mathfrak a$ -adic topology where these powers of $\mathfrak a$ are the neighborhoods of $0$ ?","['abstract-algebra', 'graded-rings', 'proof-explanation', 'commutative-algebra']"
1753678,Radius of convergence of product of two power series [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question let the radius of convergence of $\sum_{n=0}^\infty a_nz^n$ be $R_1$ and that of $\sum_{n=0}^\infty b_nz^n$ be $R_2$ then what about radius of
 $\sum_{n=0}^\infty a_nb_nz^n$
if $$
\lim\sup\left\vert \frac{a_{n+1}b_{n+1}}{a_{n}b_{n}}\right\vert =\lim
\sup\left\vert \frac{a_{n+1}}{a_{n}}\right\vert \lim\sup\left\vert
\frac{b_{n+1}}{b_{n}}\right\vert,
$$
then why is $R_1R_2\leq R$. The same thing happens to sum and division, WHY?",['complex-analysis']
1753718,When is the intersection of $k$ sets non-empty?,"Suppose, given a ground set $S$, we have two subsets $A,B \subseteq S$. If we know that $|A|, |B| > \frac{|S|}{2}$, then we know that $A \cap B \neq \emptyset$. Can this be generalized to $k$ sets? Does it hold that when $|A_1|, \dotsc, |A_k| > \frac{|S|}{k}$ for subsets $A_i \subseteq S$, then $\bigcap_{i=1}^k A_i \neq \emptyset$? My guess is no, that it only hold that there are $i,j$ such that $A_i \cap A_j \neq \emptyset$. But I would like to have a sufficient condition to conclude that the overall intersection is not empty. Do you know an easy one? Thanks a lot!","['combinatorics', 'elementary-set-theory']"
1753751,Why can't we define intersection with all the elements of a filter?,"Let's see the definition of a filter with the ""$\subset$"" order. Let $X$ be a set. We say a non-empty family $\mathcal{F}$ of subsets
  of $X$ is a filter if: $\emptyset \notin \mathcal{F}$ if $A, B \in \mathcal{F}$ then $A \cap B \in \mathcal{F}$ if $A \in \mathcal{F}$ and $A \subset B$, then $B \in \mathcal{F}$ You can't say
$$\bigcap_{F \in \mathcal{F}} F \subset \mathcal{F}$$ because not every filter is a principal filter, i.e. the interception above can be the empty set. A principal filter is a filter generated by a single element. By filter's statement 2 we know the finite intersection is defined and is an element of the filter. But why can't the interception of all elements of the filter be defined as is the finite intersection? Maybe my question could be: why there are non-principal filters?","['filters', 'elementary-set-theory']"
1753765,Finding large deviation bound for binomial distribution,"$S \sim Binomial(n, p)$. $\forall a > p$, find large deviation bound for $P( S \geq an)$ In the book, the large deviation bound definition is as follows: $\phi(t)$ is finite for some $t > 0$, $\forall a > \mu$, $P(S_n \geq an) \leq e^{-nI(a)}$, where $I(a) = sup\left \{at - log \phi(t): t > 0\right \} > 0$ My attempt at solving the problem: $P( S\geq an)$ $= P(e^{tS} \geq e^{tan})$ $ \leq \frac{E(e^{tS})}{e^{tan}}$ (by Markov inequality) $= \frac{e^{n log\phi_x(t))}}{e^{tan}}$ $= e^{[-n(at - log \phi_x(t)]}$ I know that $I(a) = sup\left \{at - log \phi_x(t): t > 0\right \} > 0$, and the answer key says that $I(a) = a log \frac{a}{p} + (1-a) log \frac{1-a}{1-p}$, but I have no idea how it arrived at that. I think I have to do something with the bernoulli mgf?","['stochastic-processes', 'large-deviation-theory', 'probability', 'probability-distributions']"
1753771,Rational Expression equivalent form,"EDIT: I know how to find the answer, but does anyone know why plugging in numbers for x does not work? The Question: If the rational expression $\frac {3x^2}{3x-1}$ is rewritten in the equivalent form $\frac {\frac 13}{3x-1}+A$, what must expression A be in terms of x ? The four answer choices : A) $x+ \frac 13$ B) $x+1$ C) $x-1$ D) $x-3$ The Answer: A) $x+ \frac 13$ I don't really know the way this question is ""supposed"" to be solved, so I just tried plugging in some numbers. Plugging in 0 for x , I would get $0=\frac {-1}3+ A$ Plugging in 1 for x , I would get $\frac 32=\frac 23 + A$. Plugging in 2 for x , I would get $\frac {12}{5}=\frac 53 +A$. The equation for $x=0$ makes it seem like $A$ really is the correct answer, but plugging in 1 and 2, I wasn't sure which one, if any were correct. Pretty sure I'm just missing something really obvious here, but asking anyway. Answer Key contains the answer listed above.","['algebra-precalculus', 'rational-numbers']"
1753779,What is wrong with my combinatorics method?,"Suppose I want to select a team of $7$ from a pool of $10$ from $A$, $8$ from $B$ and $5$ from $C$. However, I want to make sure that I have at least one from each group. My idea was to do the following $$10 \times 8 \times 5 \times \binom{20}{4}$$ the $10$, $8$ and $5$ are to choose the one member from each team, who is 'guaranteed' a spot. The $\binom{20}{4}$ is to choose the remaining $4$ team members from the remaining pool of $20$ people. The problem is that this value is larger than $\binom{23}{7}$, which should be the largest possible value.",['combinatorics']
1753855,Integrating $\int \frac{\sqrt{x^2-x+1}}{x^2}dx$,"Evaluate $$I=\int\frac{\sqrt{x^2-x+1}}{x^2}dx$$  I first Rationalized the numerator and got as $$I=\int\frac{(x^2-x+1)dx}{x^2\sqrt{x^2-x+1}}$$ and splitting we get $$I=\int\frac{dx}{\sqrt{x^2-x+1}}+\int\frac{\frac{1}{x^2}-\frac{1}{x}}{\sqrt{x^2-x+1}}dx$$ i.e., $$I=\int\frac{dx}{\sqrt{x^2-x+1}}+\int\frac{\frac{1}{x^3}-\frac{1}{x^2}}{\sqrt{1-\frac{1}{x}+\frac{1}{x^2}}}dx$$ First Integral can be evaluated using standard integral. But second one i am not able to do since numerator is not differential of expression inside square root in denominator.","['algebra-precalculus', 'integration']"
1753874,f(x) is a function such that $\lim_{x\to0} f(x)/x=1$,$f(x)$ is a function such that $$\lim_{x\to0} \frac{f(x)}{x}=1$$ if $$\lim_{x \to 0} \frac{x(1+a\cos(x))-b\sin(x)}{f(x)^3}=1$$ Find $a$ and $b$ Can I assume $f(x)$ to be $\sin(x)$ since $\sin$ satisfies the given condition?,['limits']
1753921,"We have sums, series and integrals. What's next?","We know how to sum or average a finite number of terms: sums. We know how to sum a countable infinite number ${\beth_0}$ of terms: series. We know how to sum ${\beth_1}$ terms: integrals. How to sum ${\beth_2}$ terms: ??? One ""concrete"" example please. Let ${\mathbb{R}^\mathbb{R}}$ be the set of all functions $f:\mathbb{R} \to \mathbb{R}$. Let ${x_0} \in \mathbb{R}$. Let ${\mathbb{R}^\mathbb{R}}\left( {{x_0}} \right)$ be the subset of all functions in ${\mathbb{R}^\mathbb{R}}$ having ${{x_0}}$ in their domains of definition. There are still ${\beth_2}$ of them. Is the ""functional mean image"" of ${x_0}$ under all functions in ${\mathbb{R}^\mathbb{R}}\left( {{x_0}} \right)$ that we can formally write as $\int\limits_{{\mathbb{R}^\mathbb{R}}\left( {{x_0}} \right)} {{\text{D}}f\,f\left( {{x_0}} \right)\,\,} $ (well) defined? If not, why? Same question in the set of all bijections from $\mathbb{R}$ to $\mathbb{R}$. This hypothetical ""functional mean image"", to be compared to the usual ${\beth_1}$ integral $\int\limits_\mathbb{R} {{\text{d}}xf\left( x \right)} $ may be (well) defined in some branch of mathematics I (or you) do not know; may be an unidentified mathematical object; may not exist. Anything welcome. My apologies if it is trivial but I sincerely do not know. 
Thanks.","['integration', 'measure-theory']"
1753926,"If $\{f_n\}$ is Cauchy in measure, then there is a measurable function $f$, such that $\{f_n\}$ converges in measure to $f$","The theorem is from Real Analysis (Carothers).
Let $\{f_n\}$ be a sequence of real valued measurable functions, all defined on a common measurable domain $D$. If $\{f_n\}$ is Cauchy in measure, then there is a measurable function $f:D\rightarrow \mathbb{R}$, such that $\{f_n\}$ converges in measure to $f$. Moreover, there is a subsequence $\{f_{n_{k}}\}$ that converges pointwise a.e. to $f$.
And the proof is shown in the picture Can someone explain the last line to me? The stuff in the red box. I don't quite understand how they get that inequality.","['real-analysis', 'lebesgue-measure', 'measure-theory', 'convergence-divergence']"
1753930,Solve given equation $4^{(x-2)(x+3)} - 64^{(x-3)} = 0?$,"Solve given equation $4^{(x-2)(x+3)} - 64^{(x-3)} = 0?$ My attempt: I've attempted to solve this question, but isn't it impossible to solve, i.e has already been simplified completely? $4^{(x-2)(x+3)} - 64^{(x-3)} = 0$ Because $(x-2)(x+3) = x^2 +x - 6?$ So adding those two powers together would mean that they are unable to be solved for $x$? Thanks","['algebra-precalculus', 'functions']"
1754031,sampling distribution question,"Need clarification on a binomial sample example: we drew a sample of size $100$ from a binomial($m = 2$,$p = 0.2$) distribution
and observed $76$ of the $x_i = 0$, $20$ of the $x_i = 1$ and $4$ of the $x_i = 2$ now as  $n → ∞$ the empirical distribution will look more and more like the binomial$(2,0.2)$ it was drawn from (why?). I am guessing this is because of Law of Large No. (not sure if WLLN or SLLN). 
The empirical dist mean and variance will approach Binomial dist mean and variance?? my class notes has the following explanation: For example, if $X_i ∼ Binom(2, 0.2)$ then for
$n = 100$, $$  P \bigg( \frac{1}{n} \sum_{i=1}^{n} 1_{[X_i=0]}  \geq 0.76 \bigg) \simeq	0.007   $$
but for $n = 1000$
$$  P \bigg( \frac{1}{n} \sum_{i=1}^{n} 1_{[X_i=0]}  \geq 0.76 \bigg) \simeq	2.3\times(10^{-16})   $$ I understand the LHS in the probability function, that is the total proportion of $x_i = 0$, but I dont get why RHS is $0.76$, shouldn't it be $0.64\,$ (which is the probability of $x_i = 0$, from R code dbinom(0,2,0.2)) .","['statistics', 'sampling']"
1754033,Integration and differentiation of Fourier series,"I am interested in the properties of Fourier series under integration and differentiation, and I've noticed a ""strange"" phenomenon.
Suppose I have a Fourier series which I Integrate, and suppose that I can integrate it term by term. $$f(x)=a_0+\sum_{n=1}^\infty a_n \cos(nx)+\sum_{n=1}^\infty b_n \sin(nx)$$ $$\int f(x)dx=C+a_0 x+\sum_{n=1}^\infty \frac{a_n}{n} \sin(nx)-\sum_{n=1}^\infty \frac{b_n}{n} \cos(nx) $$ Now because I want the integral to be represented as a Fourier series as well, I use the Fourier series for $x$: $$x=\sum_{n=1}^\infty \frac{2}{\pi}\frac{(-1)^{n+1}}{n}\sin(nx)$$ So I have: $$\int f(x)dx=C+\sum_{n=1}^\infty \left(\frac{2a_0}{\pi}\frac{(-1)^{n+1}}{n}+\frac{a_n}{n}\right)\sin(nx)+\sum_{n=1}^\infty \frac{b_n}{n}\cos(nx)$$ My problem is that upon differentiation (term by term), this series does not appear to give back the original Fourier series, as the constant term has been shifted into the cosine terms. I think my mistake is the step I take converting the $x$ to a Fourier series, because it can't then be differentiated term by term (see this question ).
Am I correct? And does this mean the integral of a Fourier series cannot be another Fourier series?","['derivatives', 'indefinite-integrals', 'fourier-analysis']"
1754059,Contour Integration of $\sin^2(x)/(1+x^2)$,"How should I calculate this integral $$\int\limits_{-\infty}^\infty\frac{\sin^2x}{(1+x^2)}\,dx\quad?$$ I have tried forming an indented semicircle in the upper half complex plane using the residue theorem and I tried to integrate along a curve that went around the complex plane and circled the positive real axis (since the integrand is even). Nothing has worked out for me.","['residue-calculus', 'complex-analysis', 'improper-integrals', 'integration', 'contour-integration']"
1754111,Which role does the $\frac{1}{24}$ in the Dedekind $\eta$-function play?,"The Dedekind $\eta$-function is defined as $$\eta(z) = q^{\frac{1}{24}} \prod_{n = 1}^\infty (1 - q^n)^{-1}$$
where $q = e^{2 \pi i z}$. My question is: If I start with the Euler-product $\prod_{n = 1}^\infty (1 - q^n)^{-1}$, how do I come to the point where multiplication with $q^{\frac{1}{24}}$ makes sense? Thanks!","['number-theory', 'complex-analysis', 'dedekind-eta-function']"
1754117,Geometrical Interpretation of Tensors (intuition),How would one describe to a non-mathematician (an undergraduate physicist actually- so please do use mathematics-i am not even sure if they can be described without mathematics!) what do tensors represent geometrically?,"['tensor-products', 'tensors', 'differential-geometry', 'geometry']"
1754128,Modular forms and the Roger-Ramanujan identities: How??,"I've been studying Bressoud's paper ""An easy proof of the Rogers-Ramanujan Identities"" where he proves the R.R. identity: $$\sum_{n \geq0}\frac{q^{n^2}}{(1-q) \cdots(1-q^n)}= \prod_{n \geq0}\frac{1}{(1-q^{5n+1})(1-q^{5n+4})}$$ Apparently the right hand side becomes modular when corrected by the term $q^{{-1}/{60}}$. This seems rather arbitrary to me so I'm seeking an explanation where this may come from. Is there something more behind it? A greater context for instance?","['number-theory', 'complex-analysis', 'calculus']"
1754145,"For Ito diffusion, what is the difference between two measures $Q^x$ and $P$? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 months ago . Improve this question I am confused about the difference between $Q^{s,x}$ and $P$ for the following ito diffusion:
$$dX_t=b(X_t)dt+\sigma(X_t)dB_t,\quad t\ge s;\quad X_s=x.$$ Followings are from most books: given the unique time homogenous weak solution to the above problem. Denote $\mathcal{M}_{s,x}$ be a $\sigma$-algebra on $\Omega$ generated by random variables $X_t:\omega\in \Omega \to X_t^{s,x}(\omega)\in \mathbb{R}^n$ with $t\ge s$. Define a transition probability measure $Q^{s,x}$ on $\mathcal{M}_{s,x}$ such that
$$Q^{s,x}(\{X_{t_1}\in A_1,\cdots,X_{t_k}\in A_k\})= P(\{X_{t_1}^{s,x}\in A_1,\cdots,X_{t_k}^{s,x}\in A_k\}),$$
for $A_i\in \mathcal{B}(\mathbb{R}^n)$ with $i=1,2,\cdots,k$ and $k\ge 1$. And let $E^{s,x}$ be the expectation operator with respect to $Q^{s,x}$. Then $E^{s,x}[f(X_t)]=E[f(X_t^{s,x})]$. My question is, is $X_{t_1}(\omega)=X_{t_1}^{s,x}(\omega)$ in the above paragraph? Then if it's true, isn't it $Q^{s,x}$ just a restriction of $P$ on $\mathcal{M}_{s,x}$? Then why do we need another notation for it instead of just using $P$?","['stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-analysis']"
1754158,"Triangle inequality fails in $L^{1,\infty}$","It can be proved that $\forall\varepsilon>0$
there exists $C(\epsilon)>0$ such that
 for all $f,g\in L^{1,\infty}(\Bbb R^n)$ we have that
$$
||f+g||_{1,\infty}\le(1+\varepsilon)||f||_{1,\infty}+C(\varepsilon)||g||_{1,\infty}$$
for example $C(\epsilon)=1+\frac1{\varepsilon}$ works. I have some problem in proving this inequality fails for $\varepsilon=0$. I should prove that for every $C>0$ there exist $f_C,g_C\in L^{1,\infty}(\Bbb R^n)$ such that $$
||f+g||_{1,\infty}>||f||_{1,\infty}+C||g||_{1,\infty}
$$
 but it seems really hard. I thought I can take some sequence of functions but I can't put this idea into concrete computations. Can someone help me? EDIT :
Given $f:\Bbb R^n\to\Bbb R$ measurable we define, for $\alpha>0$, $\lambda_f(\alpha):=|\{x\in\Bbb R^n\;:\;|f(x)|>\alpha\}|$ (given a subset $E\subseteq\Bbb R^n$, we set |E| to denote its Lebesgue measure). Then we set
$$
L^{1,\infty}(\Bbb R^n):=\{f:\Bbb R^n\to\Bbb R\;\mbox{measurable}\;:\exists C>0\;\;\mbox{s. t.}\;\; \lambda_f(\alpha)\le\frac C{\alpha}\;\forall \alpha>0\}\;\;\;.
$$ Finally we set $||f||_{1,\infty}$ as the infimum of such $C$'s.","['harmonic-analysis', 'real-analysis', 'lp-spaces', 'weak-lp-spaces']"
1754171,Why is this proof that a circular cone is not a surface not rigorous?,"In example $4.1.5$, page $73$ of Pressley's Elementary Differential Geometry, a ""heuristic"" argument is given to prove that the circular cone with vertex the origin and angle $\pi/4$, is not a surface. Here are the exact words: To see that it is not a surface, suppose that $\sigma: U \to S \cap W$ is a surface patch containing the vertex $(0,0,0)$ of the cone, and let $a\in U$ correspond to the vertex. We can assume that $U$ is an open ball with center $a$, since any open set $U$ containing $a$ must contain such an open ball. The open set $W$ must obviously contain a point $p$ in the lower half $S_{-}$ of $S$ where $z < 0$ and a point $q$ in the upper half $S^{+}$ where $z>0$; let $b$ and $c$ be the corresponding points in $U$. It is clear that there's a curve $\pi$ in $U$ passing through $b$ and $c$, but not passing through $a$. This is mapped by $\sigma$ into a curve $\gamma = \sigma \circ \pi$ lying entirely in $S$, passing through $p$ and $q$, and not passing through the vertex. (It is true that $\gamma$ will, in general, only be continuous, and not smooth, but this does not affect the argument.) This is clearly impossible. (Readers familiar with point set topology will be able to make this heuristic argument rigorous). Why is this argument not considered rigorous? Can someone give an outline of how a rigorous argument should be?","['proof-writing', 'differential-geometry']"
1754179,Extract coefficients for a formal power series using Lagrange Inversion Formula,"Given $f(x)$ is a formal power series that satisfies $f(0) = 0$ $(f(x))^{3} + 2(f(x))^{2} + f(x) - x = 0$ I know that the Lagrange inversion formula states given f(u) & $\varphi(u)$ are formal power series with respect to u, and $\varphi(0) = 1$ then the following is true. $[x^{n}](f(u(x))) = \frac{1}{n}[u^{n-1}](f'(u)\varphi(u)^{n})$ How do I find the coefficient of $x^{n}$ in $f(x)$ using Lagrange inversion formula?","['generating-functions', 'combinatorics']"
1754187,Frank Warner's definition of the Hodge star,"Frank Warner's book, chapter 2, excercise 13 states the following: If $V$ is an oriented inner product space ($n$ dimensional) there is a linear map $\ast \colon \Lambda (V) \to \Lambda (V)$, called star, which is well-defined by the requirement that for any orthonormal basis $e_1,\dots,e_n$ of $V$ (in particular, for any re-ordering of a given basis),
  $\ast(1) = \pm e_1 \wedge \dots \wedge e_n$, $\ast(e_1 \wedge \dots \wedge e_n) = \pm 1$, $\ast(e_1 \wedge \dots \wedge e_k) = \pm e_{k+1} \wedge \dots \wedge e_n$, where one takes ""+"" if $e_1 \wedge \dots \wedge e_n$ lies on the component of $\Lambda^n(V)-\{0\}$ determined by the orientation and ""-"" otherwise. Observe that $\ast\colon \Lambda^k(V) \to \Lambda^{n-k}(V)$. Prove that on $\Lambda^k(V)$, $\ast\ast = (-1)^{k(n-k)}$. A prior part of that exercise was to show that if we have an ONB $e_1,\dots,e_n$ of $V$ then the corresponding basis of $\Lambda(V)$ is orthonormal (after extending the inner product to $\Lambda(V)$ in the usual way), which I already accomplished. But now I'm having trouble with his definition of $\ast$. How can I calculate $\ast$ on all other basis elements, i.e. $\ast(e_{i_1}\wedge \dots \wedge e_{i_k})$? What exactly does he mean with ""for any re-ordering of a given basis""? Or is there any other way to prove the claim? Clarification: I know how the $\ast$ acts on general elements from many other sources, my question here is, if the claim follows by only knowing $\ast(e_1 \wedge \dots \wedge e_k) = e_{k+1} \wedge \dots \wedge e_n$ if my basis has the ""right"" orientation. Edit: For a solution see below.","['exterior-algebra', 'differential-forms', 'inner-products', 'hodge-theory', 'differential-geometry']"
1754198,FUNCTIONS : Theoretical doubt on functions 2,"In the functional mathematics language , if i represent function by $$f$$ . What is the theoretical difference between$$f$$ and $$f(x)$$ ? Please provide a lucid explanation.Thanks.","['functional-analysis', 'calculus', 'functions']"
1754245,"""Local"" functional central limit theorem for the empirical distribution function","Assume $(X_i)_{i=1}^{\infty}$ is a sequence of i.i.d. real-valued random variables such that $\mathbb E[X^2]<\infty$. Denote by $F_X(t) := \mathbb P(X\leq t)$ their common distribution function. The regular Donsker's Theorem  states that \begin{equation}\tag{1}
n^{1/2}\left(\frac{\sum_{i=1}^n \mathbb 1_{\{X_i\leq t\}}}{n}-F_X(t)\right)\stackrel{\mathrm d}{\rightarrow} B_0(F(t)),
\end{equation} where $\stackrel{\mathrm d}{\rightarrow}$ denotes convergence in distribution, and $B_0(\cdot)$ is a Brownian Bridge on $[0,1]$. My question is related to a previous one , which dealt with a generalization of the above result. Let us replace $t$ by $\bar t + t n^{-\alpha}$ in $(1)$, for some fixed $\bar t >0$ and $0<\alpha <1$. Accordingly, we replace the scaling constant $n^{1/2}$ by $n^{(1+\alpha)/2}$. We obtain \begin{equation}\tag{2}
n^{(1+\alpha)/2}\left(\frac{\sum_{i=1}^n \mathbb 1_{\{X_i\leq \bar t + t n^{-\alpha}\}}}{n}-F_X(\bar t + t n^{-\alpha})\right).
\end{equation} My question is: does $(2)$ converge in distribution to some process (necessarily defined on $(-\infty,\infty)$)? If so, is there an explicit characterization? Has this been studied before?","['stochastic-processes', 'probability-limit-theorems', 'probability-theory', 'central-limit-theorem']"
1754264,Do these two permutations generate $A_n$?,"Let $n$ be odd  and not a multiple of $3$. Do the cycle $\sigma:=(1, 2, \dots, n)$ and any cycle of length $3$ generate $A_n$?","['finite-groups', 'permutations', 'group-theory', 'group-presentation']"
1754266,"Show that $x^2 + y^2$ and $x^2 - y^2$ cannot both be perfect squares at the same time where $x, y \in \mathbb{Z}^+$.","Show that $x^2 + y^2$ and $x^2 - y^2$ cannot both be perfect squares at the same time where $x, y \in \mathbb{Z}^+$. I think that $x^2 + 2xy + y^2$ and $x^2 + y^2$ are not consecutive squares since the difference is even. I think it has some relation with other squares like $(x+y)^2$ and $(x-y)^2$. How should I proceed? I would love some hints.","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
1754286,open\closed and disjoint sets under R2,"I am stuck with the following question: Consider the sets in $\mathbb{R}^2$ defined by
$A = \{(x,1/x)| x > 0 \}$, $B = \{(x, −1/x)| x < 0\}$.
Prove that the sets are closed and disjoint, and that $d(A, B) = 0$ and that
there are no $a_0 \in A$, $b_0 \in B$ for which $d(a_0, b_0) = d(A, B)$. i have a couple of issues here. for close sets i thought of proving $A$'s and $B$'s commentaries are open. but i don't know how to represent their commentaries. is this a good approach? what is the mining of disjoints and how do i prove it? I am new to metric spaces in general so please don't assume i posses any prior knowledge on the subject. Thanks","['functional-analysis', 'general-topology', 'metric-spaces']"
1754289,"Assuming $0 \leq a_{n+1} \leq c_n a_n + b_n$ (+ other conditions), show $a_n \to 0$","In the paper ""A primal-dual splitting method for convex optimization ..."" (see here https://www.gipsa-lab.grenoble-inp.fr/~laurent.condat/publis/Condat-optim-JOTA-2013.pdf ), Lemma 4.6 states the following: Let $(a_n)_n, (b_n)_n, (c_n)_n$ be sequences of nonnegative real numbers such that $0 \leq c_n < 1$ for all $n$, $a_{n+1} \leq c_n a_n + b_n$ for all $n$, $\sum_n (1 - c_n) = \infty$, $b_n / (1-c_n) \to 0$. Then $a_n \to 0$. The paper cites the book ""Introduction to Optimization"" by Polyak as the source. It just quotes ""Lemma 3"" from that book, which is actually Lemma 3 of Section 2.2, page 45 (there are multiple Lemmas named ""Lemma 3""). Nevertheless, the book provides no proof . Does anyone see how this can be obtained? It seems to be a (more or less) well-known lemma, so I could also imagine that there is some other source where this is proved. As to my own input: A few lemmas before the current one are proved by considering a ""transformed"" sequence (something like $(u_k - \alpha_k /(1 - q_k))_k$ comes to mind) which then satisfy a more ""friendly"" estimate. However, I don't see which transformation would be helpful here.","['optimization', 'recurrence-relations', 'real-analysis', 'sequences-and-series']"
1754358,Combination and Probability,"There are n students and n+2 different gifts.  Each student have to receive 1 gift package. How many ways can we give out all the gifts. Scenario 1. A gift package has 3 gifts.

Ways to choose 3 gifts: (n+2)Choose(3)


Scenario 2: 2 gift packages has 2 gifts in it.

Dont know how to do..

Finally: These gift packages can be permutated and given

(Scenario 1 + Scenario 2) * n!","['combinatorics', 'probability', 'discrete-mathematics']"
1754391,Considering the power series $\sum_{n=1}^\infty(-1)^{n-1}\frac{x^{2n+1}}{(2n+1)(2n-1)}$.,"Consider the power series $\sum_{n=1}^\infty(-1)^{n-1}\frac{x^{2n+1}}{(2n+1)(2n-1)}$. Find a closed form expression for all x which converge and hence evaluate $\sum_{n=1}^\infty\frac{(-1)^{n-1}}{(2n+1)(2n-1)}$. Attempt at the solution: The radius of convergence is 1. We can rewrite the summands by: \begin{eqnarray}
\sum_{n=1}^\infty(-1)^n\frac{x^{2n+1}}{(2n+1)(2n-1)} &=& \frac{1}{2}\Big[(x^3-\frac{x^3}{3})  - (\frac{x^5}{3} - \frac{x^5}{5}) + (\frac{x^7}{5} - \frac{x^7}{7}) + \dots \Big]\\
&=& \frac{1}{2}\Big[(x^3  -\frac{x^5}{3}+ \frac{x^7}{5} + \dots ) + (\frac{-x^3}{3} + \frac{x^5}{5}-\frac{x^7}{7} +...)\Big]\\
&=& \frac{1}{2}\Big[x^2\int\frac{1}{1+x^2}dx + \int\frac{1}{1+x^2}dx-x\Big]\\
&=& \frac{x^2}{2}\arctan(x)+\frac{1}{2}\arctan(x) -\frac{x}{2}
\end{eqnarray} Substituting $x=1$ then gives $\frac{\pi}{4}-\frac{1}{2}$ . The issue I have is two fold. Firstly, when dealing with evaluations at the boundary, term by term differentiation may not be valid. In particular, we used the fact that $\arctan(x) = x-\frac{x^3}{3} + ...$ by integrating power series for $\frac{1}{1+x^2}$, valid for |x|<1. This means that the arctan formula can only be guaranteed to hold within the interior (-1,1). What are the conditions needed to talk about power series validity at boundary points? (Abelian/Tauberian theorems came to mind at first, but the conditions in this problem weren't strong enough. Alternatively, I noted that uniform convergence of the terms meant that the limit function of $x-\frac{x^3}{3} + ...$ had to be continuous. So $\arctan(1) = \frac{\pi}{4}$ by continuous extension. Do correct me if I'm wrong. The other issue that I have not been able to justify is that of conditional convergence. Clearly, the arctan series is conditionally convergent at $x=1$. How do we justify the rearrangments carried out above then?","['power-series', 'sequences-and-series', 'analysis', 'limits']"
1754402,Integral $\int \frac{x}{\left(1-x^3\right)\sqrt{1-x^2}}dx$,"I thought this integral was simple, but it turns out it's not. $$I=\int \frac{x}{\left(1-x^3\right)\sqrt{1-x^2}}\ dx$$ I tried the substitution $1-x^3=\frac{1}{t}$ , but that leaves me again with $x = \sqrt{1-t^2}$ , which doesn't simplify things. Then I tried $x = \sin(t)$ , which gets me to: $$\int \frac{\sin t}{\left(1-\sin t\right)\left(1+\sin t+\sin^2 t\right)}dt$$ This looked solvable so I did another substitution: $\tan(\frac{t}{2})=u$ , after a few steps I got this: $$\int \:\frac{\left(u+u^3\right)du}{\left(u^2-2u+1\right)\left(2u^3+3u^2+4u+1\right)}$$ which is still too complicated. What am I missing? Any ideas? Update : After splitting this function into $$I=\frac{1}{3}\int \frac{1}{\left(1-x\right)\sqrt{1-x^2}}dx + \frac{1}{3}\int \frac{x-1}{\left(1+x+x^2\right)\sqrt{1-x^2}}dx$$ It gets a bit easier. I think I can solve the first integral. But, how do I solve the second? Thank you!","['indefinite-integrals', 'integration', 'trigonometry']"
1754406,Prove $\log u > \frac{u - 1}{u}$ for $u > 1$,"How to prove that for $u > 1$ $$\log u > \frac{u - 1}{u}$$ without using integrals? I think I'm supposed to use derivatives or Taylor's theorem, as the exercise comes from a lecture about these subjects.","['derivatives', 'logarithms', 'taylor-expansion']"
1754428,Prove that $6(\sin^{10}A+\cos^{10}A)-15(\sin^8A+\cos^8A)+10(\sin^6A+\cos^6A)-1=0​$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Prove that
​$$6(\sin^{10}A+\cos^{10}A)-15(\sin^8A+\cos^8A)+10(\sin^6A+\cos^6A)-1=0​$$
Expression can be verified for different values of $A$ such as $\frac\pi4,\frac\pi2$ etc. But to prove it for general value of A?",['trigonometry']
1754430,Proving the Ricci identity,"I'm trying to prove the Ricci identity Let $Z^a$ be a vector field, $R^a_{\,bcd}$ the Riemann curvature tensor and $\nabla$ a torsion-free connection. Then: $\nabla_c\nabla_dZ^a-\nabla_d\nabla_cZ^a=R^a_{\,bcd}Z^b$. In particular, I want to start from the RHS. To do so, I've multiplied it by two arbitrary vector fields $X^c$ and $Y^d$, and used the definition of the Riemann tensor: $$\begin{align}R^a_{\,bcd}Z^bX^cY^d&=\left(R(X,Y)Z\right)^a\\
&=\left(\nabla_X\nabla_YZ-\nabla_Y\nabla_XZ-\nabla_{[X,Y]}Z\right)^a\\
&=\left(\nabla_X\nabla_YZ-\nabla_Y\nabla_XZ-\partial_{[X,Y]}Z\right)^a
\end{align}$$ where in the last line I use the fact that the connection is torsion-free to move from covariant to partial derivative. Obviously, the next step is showing that the partial derivative of $Z$ with respect to $[X,Y]$ vanishes. If they were basis vectors, I know that $[e_\mu,e_\nu]=0$, but what about the case of general vector fields? EDIT: second attempt $$\begin{align}R^a_{\,bcd}Z^bX^cY^d&=\left(R(X,Y)Z\right)^a\\
&=\left(\nabla_X\nabla_YZ-\nabla_Y\nabla_XZ-\nabla_{[X,Y]}Z\right)^a\\
&=\left(X^c\nabla_c\left(Y^d\nabla_dZ\right)-Y^d\nabla_d\left(X^c\nabla_cZ\right)-\nabla_{X^c\nabla_cY}Z+\nabla_{Y^d\nabla_dX}Z\right)^a\\
&=\left(\left(X^c\nabla_cY\right)^d\nabla_dZ+X^cY^d\nabla_c\nabla_dZ-\left(Y^d\nabla_dX\right)^c\nabla_cZ-Y^dX^c\nabla_d\nabla_cZ\right.\\
&\left.\qquad-\left(X^c\nabla_cY\right)^d\nabla_dZ+\left(Y^d\nabla_dX\right)^c\nabla_cZ\right)^a\\
&=\left(X^cY^d\nabla_c\nabla_dZ-Y^dX^c\nabla_d\nabla_cZ\right)^a\\
&=X^cY^d\left(\nabla_c\nabla_d-\nabla_d\nabla_c\right)Z^a
\end{align}$$","['connections', 'riemannian-geometry', 'general-relativity', 'tensors', 'differential-geometry']"
1754512,Techniques/Heuristics for choosing multipliers in method of characteristics for solving PDE,"I am trying to solve this linear partial differential equation via method of multipliers. $$(2y^2+z)z_x+(y+2x)z_y=4xy-z$$ The auxilary Equations are given by:
$$\dfrac{dx}{2y^2+z}=\dfrac{dy}{y+2x}=\dfrac{dz}{4xy-z}=\dfrac{l dx+m dy+n dz}{l(2y^2+z)+m(y+2x)+n (4xy-z)}$$ Letting $(l, m, n)=(1, -2y, 1)$ solves the first part of our problem; giving us $dx-2ydy+dz=0\implies x-y^2+z=c$. This is where I am stuck. I tried first to find another triplet of $l, m, n$ such that the denominator vanishes on R.H.S. On failing to find such a triplet, I expressed $x$ in terms of $ y $ and $z$ by using $x-y^2+z=c$ and tried to use $dy/..=dz/..$ in order to get the second solution. This ODE I got was very messy and I couldn't continue. Question: 1) How can another triplet $l, m, n$ be found s.t. denominator on R.H.S. is zero? 2) If not, is there a way to get a simpler ODE? 3) Are there any clever ways to get the multipliers i) in general case ii) When the coefficients of $z_x$ and $z_y$ are polynomials in $x$ and $y$? 4) Is there a deterministic way to solve Linear First Order Partial Differential Equation ? (We have to guess the multipliers in method of multipliers)","['ordinary-differential-equations', 'partial-differential-equations']"
1754558,"In how many ways can four integers be selected from $1, 2, 3, \ldots, 35$ so that the difference of any pair of the four numbers is at least $3$? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I want to choose $4$ integers from the numbers $1$ to $35$.
Condition: The difference of any pair of the $4$ numbers should be $\geq 3$. How do I model this problem?","['combinatorics', 'discrete-mathematics']"
1754572,On solution to simple ODE,Consider the ODE $$\frac{dx}{dt} = ax + b$$ where $a$ and $b$ are two parameters.  The way to solve this is to divide both sides by $ax+b$ and integrate: $$\int \frac{\dot x}{ax+b}dt = t+C \\ \frac{\log|ax+b|}{a} = t+C \\ x(t) = Ke^{at}-\frac ba$$ Easy enough.  But I'm not sure why we're not excluding some possible solutions in the first step of this approach.  Doesn't dividing by $ax+b$ immediately rule out any solution where $x(t)=-\frac ba$ anywhere in the interval over which the function is defined?  That seems like we might be losing a lot of potential solutions.  So why is the above solution the general solution ?,['ordinary-differential-equations']
1754621,Definition of hypersurface singularity,"I am really confused about this notion. Suppose $X$ is an arbitrary variety over an algebraically closed field $k$ (if you like, let the characteristic be $0$), and $p$ is a $k$-valued point. If $p$ is an isolated singularity, what is the definition that $p$ is a hypersurface singularity ? I guess we do not need $X$ to be a hypersurface for this definition. More generally, for an arbitrary variety, not necessarily projective, how do we define the hypersurface singularity on it?","['algebraic-geometry', 'commutative-algebra']"
1754651,Show that 1 + $\lambda$ is an eigenvalue of $I + A$,"Show that if $\lambda$ is an eigenvalue of $A$, then 1+$\lambda$ is an eigenvalue of $I+A$. What is the corresponding eigenvector? What I have done so far (if it is correct at all...): $(I+A)x=Ix+Ax=1x+Ax=1x+{\lambda}x=(1+\lambda)x$, thus, $1+{\lambda}$ is an eigenvector of $I+A$ But how do I find the corresponding eigenvector just from the information above?","['matrices', 'eigenvalues-eigenvectors']"
1754675,Is it true for $n > 2$ then there always exists a prime $\le n$ that does not divide $n$?,"I was thinking of how to prove $\frac{n^n}{n!}$ is never an integer for $n > 2$. I think if I prove the above question, then this follows immediately.","['number-theory', 'factorial']"
1754707,When Borel functions and Baire functions are equal?,"Suppose $X$ is compact metric space. Let $A$ be the smallest set of complex functions containing all continuous functions such that: If $f_n \in A$ are uniformly bounded and $f_n \to f$ pointwise then $f \in A$. Is it true that $A$ is equal to the set of all Borel functions? If not, what is relation between these two? What assumptions are needed to get this? Answers as well as references will be appreciated.","['descriptive-set-theory', 'real-analysis', 'measure-theory', 'analysis']"
1754710,Finding the Gradient of a Tensor Field,"Finding the Gradient of a Scalar Field I understand that you can find the gradient of a scalar field, in an arbitrary number of dimensions like so : $$grad(f) = \vec{\nabla}f = \left<\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{1}},...,\frac{\partial f}{\partial x_{n}}\right> = \begin{bmatrix}
    \frac{\partial f}{\partial x_{1}}  \\     
    \frac{\partial f}{\partial x_{2}}   \\   
    ... \\
    \frac{\partial f}{\partial x_{n}}       
\end{bmatrix}$$ where $f$ is a scalar function, $f: \mathbb{R^n} \to \mathbb{R}$. And as you can see this generalizes consistently to higher-dimensional scalar fields. All of this is straight out of Multivariable Calculus. Finding the Gradient of a Vector Field Furthermore finding the gradient of a Vector Field, is given by a Tensor i.e. given $f$ to be a vector function, $f : \mathbb{R^m} \to \mathbb{R^n}$ : $$grad(\vec{f}) = \displaystyle
 \nabla \vec{f}  = T =
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &
\frac{\partial f_1}{\partial x_2} &
... &
\frac{\partial f_1}{\partial x_m} \\
\frac{\partial f_2}{\partial x_1} &
\frac{\partial f_2}{\partial x_2} &
... &
\frac{\partial f_2}{\partial x_m} \\
... &
... &
... &
... \\
\frac{\partial f_n}{\partial x_1} &
\frac{\partial f_n}{\partial x_2} &
... &
\frac{\partial f_n}{\partial x_m} \\
\end{bmatrix}$$ with $T$ denoting the tensor (a $n\ $x$\ n$ matrix of partial derivatives of $\vec{f}$'s scalar components, i.e. rank-$0$ tensor components, correct me if what I said in these brackets is wrong )  which tells us how the vector field changes in any direction. How to find the Gradient of a Tensor Field? But how do you find the gradient of a Tensor Field? I understand that to answer this question we may need to generalize the concept of a tensor field a bit further. If I understand correctly, a scalar is a tensor of rank-$0$, a vector is a tensor of rank-$1$. Is it then fine to generalize scalar fields as tensor fields of rank-$0$, and vector fields as tensor fields of rank-$1$? If so then it means we've been finding the gradient of tensor fields (albeit of rank-0 being scalar fields) in our Multivariable courses all along, we just didn't know it. By extending the logic behind the leap between taking the gradient of a Scalar Field, to taking the gradient of a Vector Field, is it then correct to say that : The gradient of a Tensor field of rank-$n$ is a Tensor field of rank-($n+1$) ?","['multivariable-calculus', 'tensors']"
1754712,Orthogonal matrix norm,"If $H$ is an orthogonal matrix, then $||H||=1$ and $||HA||=||A||, \forall A$-matrix (such that we can writ $H \cdot A$). What norm is this about?","['matrices', 'orthogonality']"
1754729,Prove the positive definiteness of Hilbert matrix,"This is so called Hilbert matrix which is known as a poorly conditioned matrix. 
$$
A =        \left(\begin{matrix}
        1 & \frac{1}{2} & \frac{1}{3} & ... & \frac{1}{n} \\
        \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & ... & \frac{1}{n + 1} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        \frac{1}{n} & \frac{1}{n + 1} & \frac{1}{n + 2} & ... & \frac{1}{2n - 1} \\
        \end{matrix}\right)
$$ The task is to prove that matrix A is positively definite. A possible way to go is to look at the scalar product: $$(f, g) = \int_0^1f(x)g(x)dx$$ within the space of polynomials of degree not higher than $(n - 1)$. There our form $(\cdot, \cdot)$ is bilinear, symmetric and positively defined thus our polynomials' space with that form is, in fact, an euclidean space of degree $n$. Now let's take the following basis: ${1, x, x^2, ..., x^\left(n - 1\right)}$. It's easy to see that matrix A is a Gram matrix for this basis. But is there another approach?","['matrices', 'positive-definite', 'bilinear-form', 'linear-algebra']"
1754752,"""Every open cover admits an open locally finite refinement"" - can this refinement always be realized in terms of basis sets?","Let $X$ be a paracompact topological space, let $C$ be an open cover, and let $\mathscr B$ be a basis for the topology. Does there always exist a locally finite refinement that consists of basis sets? The reason this is trickier than it sounds is that, while every set in the refinement is a union of basis sets, this is definitely not guaranteed to be a locally-finite sort of union, and taking a locally finite refinement of this lands you outside the world of basis sets... Now, let me add that I don't quite remember the context that led me to consider this question, but I'm still interested in finding out the answer.",['general-topology']
1754768,Find the value of $ab+ 2cb+\sqrt3 ac$?,"Three positive real numbers $a,b,c$ satisfy the equations $a^2+\sqrt3 ab+b^2=25$,  $b^2+c^2=9$  and $a^2+ac+c^2=16$ .Then find the value of $ab+ 2cb+\sqrt3 ac$? Is there some way to find the desired value without actually finding values of $a,b,c$ or any other smart method to find $a,b,c$",['algebra-precalculus']
1754852,Evaluation of $\int_{0}^{10 \pi} ([\sec ^{-1}x]+[\cot^{-1} x])~\mathrm dx$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find the value of the integral $$\int_{0}^{10 \pi} (\lfloor\sec ^{-1}x\rfloor+\lfloor\cot^{-1} x\rfloor)~\mathrm dx$$ where $\lfloor . \rfloor$ denotes greatest integer function. Could some help me with this? I cannot break greatest integer function here into different interval. Please provide some insight.","['integration', 'definite-integrals', 'calculus']"
1754860,Does every ring of integers sit inside a ring of integers that has a power basis?,"Given a finite extension of the rationals, $K$ , we know that $K=\mathbb{Q}[\alpha]$ by the primitive element theorem, so every $x \in K$ has the form $$x = a_0 + a_1 \alpha + \cdots + a_n \alpha^n,$$ with $a_i \in \mathbb{Q}$ . However, the ring of integers, $\mathcal{O}_K$ , of $K$ need not have a basis over $\mathbb{Z}$ which consists of $1$ and powers of a single element (a power basis). In fact, there exist number fields which require an arbitrarily large number of elements to form such a basis. Question: Can every ring of integers $\mathcal{O}_K$ that does not have a power basis be extended to a ring of integers $\mathcal{O}_L$ which does have a power basis, for some finite $L/K$ ?","['number-theory', 'abstract-algebra', 'ring-theory', 'algebraic-number-theory']"
1754875,Counting number of relations that are symmetric and reflexive.,"I've looked at the other two problems similair to mine but I'm having a bit of an issue understanding as their solutions seems a bit more complex. While I for the most part understand my professors logic I am a bit confused when it comes to combinatorics. So for a finite set, he says that let $n = |A|$ $n$ is also the number of reflexive ordered pairs. I understand this part. So next since we are crossing the set $|A x A|$ = $n^2$ which is the number of total ordered pairs from the cross. So where I am getting a bit lost is he says that there are $2^{n^2-n}$ relations that are both reflexive and symmetric. So I hope I am understanding this part right. But is that because if we have all ordered pairs, and we want to include them or not include them
then thats 2 choices so we have $2$ well we have $n^2$ pairs, and we don't want to include the reflexive pairs in our choice so we must subtract them which is why we have $2^{n^2-n}$ So later he proves that for  the # of relations that are both reflexive/symmetric we have $(2^n)(2^{\frac{n^2-n}2})$
Here I am loss,why are we giving the reflexive pairs a choice if we are trying to find # of relations that are both reflexive/symmetric then wont not including one mean that relation is not reflexive? and is the above expression equivalent to $(2^{\frac{n^2+n}2})$","['relations', 'combinatorics', 'elementary-set-theory']"
1754878,Example of a Continuous-Time Markov Process which does NOT have Independent Increments,"1. Given a discrete-time Markov chain without independent increments, is the embedding of it into a continuous time Markov chain (i.e. via the use of exponential waiting times) an example of a continuous time Markov process without independent increments? 2. Does there exist a continuous-time Markov process with continuous sample paths which does not have independent increments? 3. Does there exist a continuous-time Markov process for which the increments have an infinitely divisible distribution but not independent increments? 4. Does there exist a continuous-time Markov process with a semi-group/generator but which does not have independent increments? An answer to any one of these questions would be greatly appreciated. Context: 1. As an answer to this related question , user @madprob gave an example of a discrete time Markov chain that does not have independent increments. Specifically, let a discrete time process with state space $\mathbb{R}$ be defined as follows: $X_{n+1} = X_n + Z$, where $Z|(X_n,X_{n-1},...,X_0) \sim N(-X_n, 1)$. In general, any Markov process in discrete time can be written as $X_{n+1}=X_n+Z_{n+1}$ where $Z_n = f(X_n,U_n)$ for some suitable $f$ and a random variable $U_n$ that is independent of $(X_{n-1},...,X_1,X_0)$ -- thus the problem of finding a discrete time Markov process that does not have independent increments reduces to the problem of choosing an appropriate $f$. Such a counterexample presumably exists given the answer to this question . 2. In volume 2 of Feller, p. 305, section IX.5 (2nd ed), Feller states that ""the paths are continuous with probability 1 if and only if the process is normal"". Is this really true? Clearly not every normal process has independent increments , but that counter-example is not a Markov process. Moreover, I think I proved for one of my homework assignments that a Gaussian process is Markov if and only if it has independent increments, so that would seem to imply that the answer to 2. is no, but I'm not certain of this. 3. On p. 318 of section IX.10 in Feller volume 2, he states that ""the distributions associated with continuous processes [with independent but not necessarily stationary increments] are infinitely divisible"". But does the other direction mean anything? I.e. if we have a process that is Markov and infinitely divisible increments, does that say anything about the stationarity or independence of those increments? 4. Also on p. 353, X.9, Feller states that ""practically all Markov processes represent limiting forms of pseudo-Poisson processes"". Moreover, many Markov processes can be formed by the subordination of a Levy process (a process with infinitely divisible increments). Since infinitely divisible increments, semigroups, and independent increments are all related one way or another, this seems to suggest that there ought to be some pattern to when a Markov process does or does not have independent increments.","['stochastic-processes', 'probability-theory', 'markov-chains', 'markov-process', 'stochastic-analysis']"
1754905,Why is continuity permissible at endpoints but not differentiability? [duplicate],"This question already has answers here : Differentiablility over closed intervals (3 answers) Closed 5 years ago . Differentiable at endpoints? Does differentiation only work on open sets? Admittedly, there are some questions and answers as to why a function defined on a closed interval is not differentiable on the interval's endpoints. But I find no answer as to why, in spite of this, continuity can be defined on an interval's endpoints. For differentiability, the intuition is that the neighborhood of $x$ that allows $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c} = L\quad$ must (or is this our definition) be populated from both the left ($x < c\;$) and the right ($x > c\;$). Hence, differentiability isn't defined on endpoints of an interval. Why does such a requirement not exist for continuity? A function $f$ is continuous at $c$ if $$ \forall \epsilon, \exists \delta, |x - c| < \delta \rightarrow |f(x) - f(c)| < \epsilon$$ Continuity seems not to care whether points appear left or right of $c$ -- just that, if they exist in some chosen neighborhood, they satisfy the condition above. Here is an example of this ""dichotomy""","['derivatives', 'real-analysis', 'continuity']"
1754912,If $f(x) = -2\sin(x)$ then $f′(x)$ equals what?,If $f(x) = -2\sin(x)$ then $f′(x)$ equals what? A: $2\cos x$ If $f(x) = (15)^x$ then $f′(x)$ = ? A: $(15)^x \ln (15)^x$ Are my solutions correct?,"['derivatives', 'calculus']"
1754930,Lebesgue measure-preserving differentiable function,"Let $\lambda$ denote Lebesgue measure and let $f: [0,1] \rightarrow [0,1]$ be a differentiable function such that for every Lebesgue measurable set $A \subset [0,1]$ one has $\lambda(f^{-1}(A)) = \lambda (A)$. Prove that either $f(x) = x$ or $f(x) = 1- x$. I will appreciate a hint or a solution that doesn't use ergodic theory as this is an old qual problem in measure theory. $f$ is bounded and continuous and so it is integrable and hence the hypothesis of Lebesgue's Differentiation Theorem are satisfied, but I haven't seen a way to use this or if it is even applicable.",['measure-theory']
1754942,"Let $f(x)=x^5$. For $x_1>0$, let $p_1=(x_1,f(x_1))$.Draw a tangent at the point $p_1$","Let $f(x)=x^5$. For $x_1>0$, let $p_1=(x_1,f(x_1))$. Draw a tangent at the point $p_1$ and let it meet the graph again at point $p_2$. Then draw a tangent at $p_2$ and so on . Show that , the ratio  $\frac{A(\bigtriangleup p_np_{n+1}p_{n+2})}{A( \bigtriangleup p_{n+1}p_{n+2}p_{n+3} )}$ is constant. I don't think just taking the ratio of two triangles will help, something more need to be used.","['functions', 'analytic-geometry']"
1754954,Any good books for Infinite Series?,"I wanted to know about some good, free books on Infinite Series.","['reference-request', 'sequences-and-series']"
1754987,Is this stronger statement of the squeeze theorem valid?,"The squeeze theorem formally states that if $f,g$ and $h$ are real functions defined on an interval $I$ containing $c$ as a limit point and satisfy $g(x) \leq f(x) \leq h(x)$ for all $x \in I$ except possibly $c$, and furthermore, $\lim_{x \to c} g(x) = \lim_{x \to c} h(x) = L$, then  $\lim_{x \to c} f(x) = L$. This is proven by choosing, for given $\epsilon$, the minimum of the two corresponding $\delta$s of $h$ and $g$ as the $\delta$ for $f$. This statement, however, isn't as strong as can be. In particular, suppose $g$ and $h$, about every neighbourhood of $c$, ""interchange"" roles of being the lower and upper bounds of $f$. A concrete example would be $f(x) = 0$, $h(x) = x^2\sin\frac{1}{x}$ and $g = -h$ as $x \to 0$. I think that the conclusion of the squeeze theorem should be valid in this case regardless, because so long as we choose $\delta$ such that $g,h$ are within $\epsilon$ of $L$, $f$ will be within $\epsilon$ of $L$. Essentially, I want to weaken the hypothesis so that rather than $\forall x \in I, c \neq x\left[g(x) \leq f(x) \leq h(x)\right]$, it states $\forall x \in I, c \neq x \left[g(x) \leq f(x) \leq h(x) \  \lor h(x) \leq f(x) \leq g(x)\right]$. Questions: Is my reasoning correct? If not, what mistake am I making? If so, why is the stronger statement not more common in calculus/analysis texts?","['real-analysis', 'calculus', 'limits']"
1755009,Is a basin of attraction necessarily an open set?,"Definition: The basin of attraction is the defined as the set of all initial conditions $x_{0}$ such that $x(t$ ) tends to an attracting fixed point $x^{\ast}$ as time $t$ tends to $\infty$ . Is this basin of attraction necessarily an open set? My text mentioned nothing about the basin of attraction being an open set-Of course this could imply that the audience is meant to think on a deeper level about the said properties of it being an open set. It is in a given example that I concluded that the author implicitly claimed that the basin of attraction is an open set. I would like to know if it is indeed true that the basin of attraction is an open set and if it is how can it be shown on a heuristic level.
Thanks in advance.","['general-topology', 'basins-of-attraction', 'dynamical-systems']"
1755022,Let $A$ be an $n \times n$ matrix over $\mathbb{C}$ or $\mathbb{R}$. Does $\det(e^A) = e^{\mathrm{tr}(A)}$ always hold?,"Let $A$ be an $n \times n$ matrix over a field $\mathbb{K}$ where $\mathbb{K} = \mathbb{C}$ or $\mathbb{K} = \mathbb{R}$. Does $\det(e^A) = e^{\mathrm{tr}(A)}$ always hold? If the field is $\mathbb{C}$, this can be easily proven using the Jordan canonical form. Now, if the field is $\mathbb{R}$, the Jordan canonical form may not exist for $A$. So does $\det(e^A) = e^{\mathrm{tr}(A)}$ hold for any square matrix in $\mathbb{R}$ or only when $A$ meets certain conditions (e.g. $A$ has a Jordan canonical form)?",['linear-algebra']
1755083,How do you prove this without using induction?,How do you prove this without using induction $$\frac{1}{n}+\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{2n-1}=1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots+\frac{1}{2n-1}$$,"['algebra-precalculus', 'alternative-proof']"
1755099,Induce measure between topological spaces.,"Let $X$ and $Y$ be topological spaces, suppose that the function $f:X\rightarrow  Y$ is a continuous surjection. Let $\mu$ be a regular measure on $X$, and $M_X$ the set of $\mu$-measurable sets in $X$. We can induce a measure on $Y$ define in the $\sigma$-algebra $M_Y=\{A\subset Y: f^{-1}(A)\in M_X\}$ as 
$$\nu(A)=\mu(f^{-1}[A])$$ What conditions do we have to impose in order that the measure $\nu$ be regular? I supposed that $\mu$ were regular but I got nothing. Any clue?","['general-topology', 'measure-theory']"
1755112,Solve equation $8\cos x - 6\sin x = 5$ where $0 \le x \le 360$,"Solve equation $8\cos x - 6\sin x = 5$ where $0 \le x \le 360$. I am not asked to use any form, so I am going to use $k\cos(x-\alpha)$. $8\cos x - 6\sin x = k\cos(x-\alpha)$ $$=k(\cos x\cos\alpha + \sin x\sin\alpha)$$
$$=k\cos\alpha\cos x + k\sin\alpha\sin x$$ equating coefficients: $k\cos\alpha = 8$ $k\sin\alpha = 6$, or is it $k\sin\alpha = -6$? I find this really confusing $k = \sqrt{8^2 + 6^2} = 10$ $\alpha$ is in the 1st quadrant where both sin and cos are positive. $\alpha = \arctan\frac{6}8 = 36.8^{\circ}$ $\therefore10\cos(x - 36.8) = 5$ I have a maximum and minimum value of 10 From here I do not know how to finish solving for x.",['trigonometry']
1755149,derivative of error function,"How can I calculate the derivatives $$\frac{\partial \mbox{erf}\left(\frac{\ln(t)-\mu}{\sqrt{2}\sigma}\right)}{\partial \mu}$$
and
$$\frac{\partial \mbox{erf}\left(\frac{\ln(t)-\mu}{\sqrt{2}\sigma}\right)}{\partial \sigma}$$ where $\mbox{erf}$ denotes the error function can be given by $$\mbox{erf}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}\exp(-t^2)\,dt$$ I have tried it using WA derivative calculator but I am not able to understand the steps.","['derivatives', 'error-function']"
1755150,Is this an adapted process?(deterministic integrator in Itô-process),"Assume you have a probability space with a filtration, $(\Omega,\mathcal{F},P,\{\mathcal{F}_t\})$. Assume that the stochastic process $X_t$ is adapted to this filtration, and is jointly measurable with respect to $\mathcal{B}(\mathbb{R})\otimes \mathcal{F}$. Also assume that $P(\int_0^t|X_s|ds<\infty, \forall t)=1$. Then the integral $\int_0^tX_sds$ makes sense for almost all $\omega$. But I am wondering if this is an adapted process? In order to find the answer I assume we must look at simple functions approximating $\int_0^tX_s ds$. I am able to do this pointwise for each $\omega$, but then I don't know how to use it for measurability with respect to $\mathcal{F}_t$. Maybe it is not the case that it is measurable? The reason I am asking about this, is that when an Itô-process is defined, this is one part that comes up in the process. The Itô-process consist of two parts an integral with respect to a Brownian motion and this part, and integral with respect to the lebesgue measure. I assume that an ito-process is adapted, and hence this part which is an integral with respect to the lebesgue measure should be adapted? This integral is described on page 43 and 44 here: http://th.if.uj.edu.pl/~gudowska/dydaktyka/Oksendal.pdf Update: I have gotten closer to a solution: If I can show that $X(t,\omega)$ is measurable with respect to $\mathcal{B}([0,t])\otimes \mathcal{F}_t$, then I will be done, because then I can look at the positive and negative part, and by Tonelli's theorem $\int_0^t X_s ds$ will be $\mathcal{F}_t$-measurable. What I have is that $X(t,\omega)$ is measurable with respect to $\mathcal{B}(\mathbb{R})\otimes \mathcal{F}$. If I restict the t-values it is easy to see that $X(s,\omega)$ is measurable with respect to $\mathcal{B}([0,t])\otimes \mathcal{F}$ when only considering the relevant values in the integral $s \le t$. So I end up with that for $s\le t$, $X_s$ is $\mathcal{F}_t$ measurable when holding s fixed, and as a joint function $X(s,\omega)$ is $\mathcal{B}([0,t])\otimes \mathcal{F}$-measurable. Is there then any way to conclude that it is also $\mathcal{B}([0,t])\otimes \mathcal{F}_t$ measurable?","['stochastic-processes', 'probability-theory', 'stochastic-calculus', 'stochastic-analysis']"
1755165,A semisimple Lie group has no character; Am I right?,"Let $G$ be a compact connected Lie group with semisimple Lie algebra ${\frak g}$. With the following reasoning, I show that there is no non-trivial Lie group homomorphism
$$\chi:G\to S^1.$$
Is that true? Proof. Let $\chi:G\to S^1\subseteq\Bbb C$ be a character. Then, $d\chi:{\frak g}\to i\Bbb R$ so $id\chi:{\frak g}\to \Bbb R$, i.e. $id\chi\in{\frak g}^*$. But since $\chi$ is a homomorphism, and $S^1$ is abelian, we have for all $g\in G$ and $X\in{\frak g}$ that
$$({\operatorname{Ad}}_g^*id\chi)(X)=i\frac{d}{dt}\Big|_{t=0}\chi(\exp(t\operatorname{Ad}_{g^{-1}}(X)))=i\frac{d}{dt}\Big|_{t=0}\chi(g^{-1}\exp(tX)g)=i\frac{d}{dt}\chi(\exp(tX))=id\chi(X).$$
Thus, $\operatorname{Ad}_g^*id\chi=id\chi$ for all $g\in G$. Using the Killing form to identify $id\chi\in{\frak g}^*$ with an element $X_{\chi}\in{\frak g}$, we find $\operatorname{Ad}_g(X_{\chi})=X_{\chi}$ for all $g\in G$. Thus, $X_{\chi}\in\operatorname{Lie}(Z_G)=Z_{{\frak g}}$ (where $Z$ means ""center""). But a semisimple Lie algebra has zero center and hence $X_{\chi}=0$. Thus, $id\chi=0$ and hence $\chi=1$. $$\tag{Q.E.D.}$$","['differential-geometry', 'lie-algebras', 'lie-groups']"
1755166,Linearity of the right inverse of a surjective linear map,"Suppose we have a surjective linear map $f:V\to V$ on an infinite-dimensional vector space $V$. We know that every surjective map has at least one right inverse. So I was wondering... I know not all right inverses are linear, for example, on the space of real sequences
$$(a_0,a_1,a_2,\cdots)\mapsto(1,a_0,a_1,a_2,\cdots)$$
is a right inverse of the surjective linear map
$$(a_0,a_1,a_2,\cdots)\mapsto(a_1,a_2,\cdots)$$ but obviously isn't a linear map itself, but on the other hand $$(a_0,a_1,a_2,\cdots)\mapsto(0,a_0,a_1,a_2,\cdots)$$ is a linear right inverse.  So I was wondering... does there always exist a linear right inverse of any linear surjective map on an infinite-dimensional vector space over an arbitrary field $K$?","['linear-algebra', 'linear-transformations', 'vector-spaces']"
1755194,"If one angle of a triangle be $60^\circ,$ the area $10\sqrt3$ sq cm.,and the perimeter $20$ cm,find the lengths of the sides.","If one angle of a triangle be $60^\circ$, the area $10\sqrt3\ \mbox{cm}^2$, and the perimeter $20\ \mbox{cm}$, find the lengths of the sides. Let $\angle A=60^\circ$ and $\frac{1}{2}bc\sin A=10\sqrt3$ $$bc=40,\quad a+b+c=20$$ $$2R\sin A+2R\sin B+2R\sin C=20$$ I am stuck here.",['trigonometry']
1755198,"Positive integers $<100000$, how many contain exactly one $3$, one $4$ and one $5$","So I use $5$ positions for range $00000$ to $99999$ Choose $3$, choose $4$ and choose $5$ as follows: $5C1 \cdot 4C1 \cdot 3C1$ Remaining $2$ digits have $7$ possible digits as input
Ans: $5C1 \cdot 4C1 \cdot 3C1 \cdot 7 \cdot 7$. My question is, what if the question asks contain exactly one $3$ OR one $4$ OR one $5$? Let $A$ be the set which contains a number with a single $3$. Let $B$ be the set which contains a number with a single $4$. Let $C$ be the set which contains a number with a single $5$. $|A| = (9^4) \cdot 5$ $|B| = (9^4) \cdot 5$ $|C| = (9^4) \cdot 5$ $|A \cup B \cup C| = |A| + |B| + |C| - |A \cap B| - |A \cap C| - |B \cap C| + |A \cap B \cap C|$ How do I proceed? My further attempt: $|A \cap B| = 5C1 \cdot 4C1 \cdot 8 \cdot 8 \cdot 8 = 10240$ $|A \cap B| = |A \cap C| = |B \cap C| = 10240$ $|A \cap B \cap C| = 5C1 \cdot 4C1 \cdot 3C1 \cdot 7 \cdot 7 = 2940$ Answer $= 32805 + 32805 + 32805 - 10240 - 10240 - 10240 + 2940 = 70635$","['permutations', 'combinatorics', 'discrete-mathematics']"
1755209,Determining the presentation matrix for a module,"I am trying to study some module theory using the book ""Algebra"" by Michael Artin (2nd Edition, to be precise), and I can't really fathom what is written in Section 14.5. Left multiplication by an $m \times n$ matrix defines a homomorphism
  of $R$-modules $A: R^n \rightarrow R^m$. Its image consists of all
  linear combinations of the columns of $A$ with coefficients in the
  ring, and we may denote the image by $AR^n$. We say that the quotient
  module $V=R^m/AR^n$ is presented by the matrix $A$. More generally,
  we call any isomorphism $\sigma: R^m/AR^n \rightarrow V$ a presentation of a module $V$, and we say that the matrix $A$ is a presentation matrix for $V$ if there is such an isomorphism. For example, the cyclic group $C_5$ of order $5$ is presented as a
  $\mathbb{Z}-$module by the $1\times 1$ integer matrix $[5]$, because
  $C_5$ is isomorphic to $\mathbb{Z}/5\mathbb{Z}$. The above two paragraphs seem to be clear. We use the canonical map $\pi: R^m \rightarrow V=R^m/AR^n$ to
  interpret the quotient module $V=R^m/AR^n$, as follows: Proposition 14.5.1 (a) $V$ is generated by a set of elements B $=(v_1, \cdots, v_m)$, the images of the standard basis elements of $R^m$. (b) If $Y=(y_1, \cdots, y_m)^t$ is a column vector in $R^m$, the element B $Y=v_1y_1+\cdots + v_my_m$ of $V$ is zero if and only if
  $Y$ is a linear combination of the columns of $A$, with coefficients
  in $R$ - if and only if there exists a column vector $X$ with entries
  in $R$ such that $Y=AX$. Proof . The images of the standard basis generate $V$ because the map $\pi$ is surjective. Its kernel is the submodule $AR^n$. This
  submodule consists precisely of the linear combinations of the columns
  of $A$. First, could you expound on the proof please? The proposition is obvious when applied to the above example with $R^m=R^n=\mathbb{Z}$, $V=C_5$, and $A=[5]$; but it doesn't help me to carry out more detailed proof. If a module $V$ is generated by a set B $=(v_1, \cdots, v_m)$, we
  call an element $Y$ of $R^m$ such that B $Y=0$ a relation
  vector , or simply a relation among the generators. We may also
  refer to the equation $v_1y_1+\cdots+v_my_m=0$ as a relation, meaning
  that the left side yields $0$ when it is evaluated in $V$. A set $S$
  of relations is a complete set if every relation is a linear
  combination of $S$ with coefficients in the ring. Example 14.5.2 The $\mathbb{Z}-$module or an abelian group $V$ that is generated by three elements $v_1, v_2, v_3$ with the compete
  set of relations $$ 3v_1+2v_2+v_3=0\\ 8v_1+4v_2+2v_3=0\\ 7v_1+6v_2+2v_3=0\\
 9v_1+6v_2+v_3=0 $$ is presented by the matrix $$
         A=\begin{bmatrix}
         3 & 8 & 7 & 9 \\
         2 & 4 & 6 & 6 \\
         1 & 2 & 2 & 1 \\
          \end{bmatrix}. $$ Its columns are the coefficients of the (above) relations: $(v_1, v_2,
 v_3)A=(0, 0, 0, 0)$. Here comes the second question. How did he find out immediately what matrix presents the group $V$? Artin gives an algorithm for determining the presentation matrix below this example, but since this example precedes it, the reader should be able to determine this matrix without the algorithm. So, in this case $R=\mathbb{Z}$, but what are $m$ and $n$? How do I find this matrix $A$ neatly?","['abstract-algebra', 'modules', 'group-presentation']"
1755216,Maximum Likelihood of single observation,"I'm stumped on this problem... I keep getting an undefined answer of having to solve -20 = 0. The likelihood function I get is $e^{-20\alpha}$. So I have $y_i=$ $ \begin{cases} 
      1& w/probability \ p_i \\
      0& w/probability \ 1-p_i 
   \end{cases}$ And $p_i=1-e^{-\alpha x_i}$, i=1,2,... $\alpha>=0$   and $x_i>=0$ If I have a sample of a single observation $(y_1,x_1)=(0,20)$, I am looking for the maximum likelihood of this sample. Thank you for any help :)","['statistical-inference', 'maximum-likelihood', 'statistics', 'probability', 'parameter-estimation']"
1755293,Correcting Gauss's proof of FTA. Need verification,"In his doctoral thesis, Gauss gave a proof of fundamental theorem of algebra for real polynomials, based on geometric arguents. Later in his life he expanded the proof to complex polynomials. A nice account can be found here . The problem with Gauss's proof was that he assumed some topological arguments, which were not proven rigorously till a century later. A proof is presented here which starts in Gauss's lines, then replaces his topological arguments with least upper bound for real numbers and continuity considerations only. Gauss was aware of the least upper bound property ( LUB ) for real numbers (in fact he co-discovered it). So his topological arguments could have been replaced by the following method, which is why I am calling it ""Correction"". Any help with verification will be greatly appreciated. Theorem: Every complex polynomial has a complex root. Background: We only consider polynomials which are not factorizable into lower degree polynomials of degree >1. For factorizable polynomials, the proof can be applied on the factors. Complex numbers are assumed to exists and it is known that they form a field (So that basic algebraic operations and distributive law are well defined). Modulus function is also defined. Gauss's proof : Rewrite $z=re^{i\theta}$. Then $z^k=r^{k} (cos(k\theta) +i sin(k\theta))$ Then $\sum_{k=0}^{n}c_{k}z^{k}=\sum_{k=0}^{n}  | c_{k} | r^{k}(cos(k\theta+\phi_{n}) +i sin(k\theta+\phi_{k}))$ So real part of $R(r,\theta)=\sum_{k=0}^{n}  | c_{k} | r^{k} cos(k\theta+\phi_{k} )$ and the imaginary part is $I(r,\theta)=\sum_{k=0}^{n}  | c_{k} | r^{k}  sin(k\theta+\phi_{k})$. Taking $c_{n}=1$, Gauss showed that for sufficiently large $r=L$, $R(L,\theta)$ changes sign in the interval $\theta \in (\frac{\pi k'}{n}-cos^{-1}\epsilon,\frac{\pi k'}{n}+cos^{-1}\epsilon), \quad k=0,1,2,...,2(n-1)$. Applying intermediate value theorem , this means that $R(L,\theta)$ has roots in these intervals of $\theta$, for any $r$ large enough. On the other hand, for L sufficiently large, $I(L,\theta)$ changes sign in the interval $\theta \in (\frac{\pi}{2n}+ \frac{\pi k'}{n}-cos^{-1}\epsilon,\frac{\pi}{2n}+\frac{\pi k'}{n}+cos^{-1}\epsilon)$ and thus has a root in every interval. If $\epsilon>0$ is taken sufficiently small, al these intervals have zero overlap. So on the circle $r=L$, we have $2n$ roots of $R$ and $I$ interlacing. All roots are exhausted. Gauss then deduced the existence of a root based on this fact and the following assumption: Lemma 1: If two continuous bivariate function have interlacing roots on a closed curve, they must have a simultaneous roots inside the curve. The proof of this is quite hard, based on topological arguments (connected curves, Jordan curve theorem etc). Gauss speculated that this could be proven (his speculations were often correct and groundbreaking) but he did not have a proof. My proof: Using the same $R$ and $I$ as Gauss, only least upper bound and continuity arguments can be used to prove FTA. $R$ and $I$ can also be written in terms of $(x,y),z=x+iy$ in the following way. $$\left[\begin{array}{l}R(x,y)\\I(x,y)\end{array}\right]= \sum_{k=0}^{n} \left[\begin{array}{l}Re(c_{k})&-Im(c_{k})\\Im(c_{k})&Re(c_{k})\end{array}\right] \left[\begin{array}{l}x&-y\\y&x\end{array}\right]^{k} \left[\begin{array}{l}1\\0\end{array}\right]$$ If we take $\bar{y}$ large enough, $r$ is also large enough for any $x,\bar{y}$. For all 
the disjoint $\theta$ intervals Gauss created in this proof, $x=\bar{y} tan(\theta)$ also generate disjoint intervals. Usings intermediate value theorem on $x$, it is starigtforward to deduce that $\exists y_{1}<0$, such that $R(x,\bar{y})$ and $I(x,\bar{y})$  have interlacing roots $\forall \ \bar{y}<y_{1}$. Looking at the matrix formulation of $R$ and $I$, it can be seen directly that the maximum power of $x$ in $R(x,\bar{y})$ and $I(x,\bar{y})$ are $n$ and $n-1$ respectively. So all real roots exist and interlace $\forall \ \bar{y}<y_{1}$. Now comes LUB based 'Correction': 1. $R(x,0)$ does not have any real root. Wrong, see correction at  {*} in the end of this article 2. $\exists y_{1}<0$, such that $R(x,\bar{y})$ and $I(x,\bar{y})$  have maximum possible number of real roots, which interlace $\forall \ \bar{y}<y_{1}$ 3. LUB applied on 1. and 2. says that $\exists y_{sup}<0$ such that both $R(x,\bar{y})$ and $I(x,\bar{y})$ have maximum number of real roots $\forall \bar{y}<y_{sup}$ and at least one of them has less number of real roots for $y_{sup}<\bar{y}<y_{sup}+\delta$, for some $\delta >0$. 4. Using intermediate value theorem and due to the finite number of roots, it can be shown that the roots of both $R(x,\bar{y})$ and $I(x,\bar{y})$ vary continuously with $\bar{y}, \quad \forall \bar{y}<y_{sup}$. Question is what happens at $\bar{y}=y_{sup}$. 5. $R(x,\bar{y})$ and $I(x,\bar{y})$ are both bivariate polynomials of $x$ and $y$. Thus the roots on x are clearly bounded $\forall \bar{y}<y_{sup}<0$. Thus $liminf_{\bar{y} \rightarrow y_{sup}}$ and $limsup_{\bar{y} \rightarrow y_{sup}}$  of the roots exist for any indexed root. If the $limsup$ and $liminf$ are different for any indexed root, $R=0$ or $I=0$ crosses any value between these two infinite times. For fixed $x$, $R$ or $I$ are polynomials in $y$ and cannot have infinite number of roots. So the $liminf=limsup$ (implying that the roots approach limits as $\bar{y} \rightarrow y_{sup}$). Thus $R(x,y_{sup})$ and $I(x,y_{sup})$  have maximum possible real roots. At least one of the roots do not exist for slightly large $\bar{y}$. If all roots of $R(x,y_{sup})$ and $I(x,y_{sup})$ are distinct,  intermediate value theorem can be applied on the larger side of $\bar{y}$ to have a contradiction. Thus the only possibility is that there is a root on $x$ which has same sign on both sides on the line $\bar{y}=y_{sup}$. This is clearly a root with multiplicity, implying that two of the root lines meet. It is easy to show that two consecutive roots meet at $\bar{y}=y_{sup}$ Without loss of generality, let us say two consecutive roots of $R$ meet at $\bar{y}=y_{sup}$. $\forall \bar{y}<y_{1}$, there is a root of $I$ between these two roots. Again, straighforward application of LUB gives that $\exists y_{sup}<y<y_{1}$ and the corresponding root of $I$ mentioned in the last line such that $R(x,y)=I(x,y)$. This proves FTA. I did not want this to be a very long post. So Sorry for being terse in the derivations. {*} Update 24 April 2016 : In my 'Correction', point 1 only works for real polynomials. Slightly more work needs to be done for complex polynomials. If either the real or complex part of the polynomial ($R$ and $I$)has less than $n$ or $n-1$ roots at $\bar{y}=0$, the rest or the arguments go through. If not, since maximum possible real roots exists for $R(x,\bar{y})$ and $I(x,\bar{y})$ for all $\bar{y}$, the roots vary continuity according to point 4.  Then following possibilities exists Case 1: For some $\bar{y}$, the roots of $R(x,\bar{y})$ and $I(x,\bar{y})$ do not interlace. LUB and continuity arguments can be directly applied on this to deduce the existence of a simultaneous solution of $R(x,\bar{y})$ and $I(x,\bar{y})$. Case 2: The interlacing is always preserved. Writing $R(x,\bar{y})$ as a function of $x$ and applying the Fujiwara bound for the upper bound of the roots, it can be seen that there cannot exist a roots of $R(x,\bar{y})$ for large enough $x$ in the region $\bar{y}<mx$, for some $m\in \mathbb{R}$. However, this is impossible since rewriting $R$ as $R(r,\theta)$ gives that for small, say $\theta^{*}=tan^{-1}\frac{m}{2}$, $\exists \bar{r}$ and $\theta(r)$ such that $R(r>\bar{r},\theta(r)))=0$ satisfying $|\theta(r)|<|\theta^{*}|$. If anyone follows up in details to this point and wishes to have a full explanation of this, I will be glad to provide one.","['real-analysis', 'polynomials', 'roots', 'proof-verification', 'complex-analysis']"
1755352,The number of zeros in the expansion of $n!$ in base $12$,"During an interview last year I was asked the following question: How many zeros appear at the end of $n!$ in base $12$, where $n$ is a positive integer? I applied the known Legendre formula for $p=3$ and then for $p=2$ (in the last case followed by a division by $2$). Then I realised that I couldn't compare the two series produced. It turned out that the interviewer was an applied mathematician and considered the two series to be ""equal"". So the question remained unanswered: Which of the produced series is greater for each value of $n$?","['factorial', 'sequences-and-series', 'elementary-number-theory']"
1755360,Derivative of $x^x$ and the chain rule,"Rewriting $x^x$ as $e^{x\ln{x}}$ we can then easily calculte the ${\frac{x}{dx}}$ derivative as ${x^x}(1 + \ln{x})$. We need to use chain rule in form $\frac{de^u}{du}\frac{du}{dx}$. The question is why cannot we use the chain rule skipping the 1st step of rewriting $x^x$ as $e^{xlnx}$? The idea would be to write $\frac{u^x}{du}\frac{du}{dx}$ which simply would be $x^x\ln{x} \cdot 1$? The answer is incorrect, here we are missing the $+x^x$ term.","['derivatives', 'chain-rule']"
1755363,"Does ""the functions agree at infinity"" mean anything?","I want a way to describe how two continuous functions $f,g \colon (X-x) \to Y$ might ""share a limit"" at the point $x$ when unfortunately neither of $\displaystyle \lim _{y \to x}f(x)$ or $\displaystyle \lim _{y \to x}g$ happen to exist.. Let me give an example. Suppose $x = 0$ and $f,g \colon (0,1] \to \mathbb R$ are given by $f = \sin(1/x)$ and $g = \sin(1/x) +x$. Then for any prescribed $\epsilon > 0$ there is an $\delta > 0$ such that, when I restrict the functions to the ball $B(0,\delta)$ their graphs are within distance $\epsilon$ of each other. However in this case it's a lot easier. We can just take the difference of the functions $(f-g)(x) =x$ and say that function tends to zero at $x$. But this relies on how the real line has an additive structure. If we replace the codomain with something more exotic this definition might not apply. So how might I formalise this notion of ""agreeing at infinity"" for more general topological spaces $Y$?","['limits', 'reference-request', 'asymptotics', 'functions', 'general-topology']"
1755384,Homotopy connectedness,"I am trying to prove the following statement: Let $X,Y$ be two homotopy equivalent topological spaces. If $X$ is connected then $Y$ is connected. So far, this is my attempt: If $X,Y$ are homotopy equivalent then exist a continuous function $f: X \rightarrow Y$ (homotopy equivalence). For the principal theorem of connectedness then $f(X)$ is connected in $Y$. Suppose $Y$ is not connected, then exists a separation (namely a couple of nonempty open sets $ \{U,V \}$ of $Y$) such that $Y=U \cup V$ (disjoint union). $f(X)$ is connected in $Y$ means that $f(X) \subset U$ or $f(X) \subset V$. Now I want to use the fact that exist unique map called homotopy inverse $g:Y \rightarrow X$ such that $ g \circ f \sim id_X$ (where $\sim$ stands for homotopy equivalent) and I want to find a contradiction, but nothing comes into my mind at the moment. Any hint? thank you in advance","['algebraic-topology', 'general-topology']"
1755478,How many ways are there to prove Cayley-Hamilton Theorem?,"I see many proofs for the Cayley-Hamilton Theorem in textbooks and net, so I want to know how many proofs are there for this important and applicable theorem?","['big-list', 'cayley-hamilton', 'reference-request', 'abstract-algebra', 'linear-algebra']"
1755483,Why can this cosine sum function show all primes less than $N^2$?,"I constructed this cosine sum that puts all primes within N on line y=1, and its zeros show the sieve by primes less than N. For $x<N^2$, they are all primes.
$$
P(N,x)=\sum_{n=2}^{N}\frac{1}{n}\left(1+2\sum_{k=1}^{n-1}(1-\frac{k}{n})\cos\frac{2k\pi}{n}x\right)
$$
And here's an example of $P(50,x)$, and I've created a live graph for you to play as experiment. Questions: Why this function has this property? Is it a periodic function? What's the period?","['analytic-number-theory', 'number-theory', 'divisor-sum', 'prime-numbers', 'primality-test']"
1755501,"Real Analysis, Folland Problem 6.1.16 $L^p$ spaces","Problem 6.1.16 - If $0 < p < 1$, the formula $\rho(f,g) = \int |f-g|^p$ defines a metric on $L^p$ that makes $L^p$ into a complete topological vector space. Attempted proof - Suppose $a,b > 0$ and $0 < p < 1$. For $t > 0$ we have $t^{p-1} > (a + t)^{p-1}$, and by integrating from $0$ to $b$ we obtain $a^p + b^p > (a+b)^p$. Let $E$ and $F$ be disjoint sets of positive finite measure in $X$ and set $a = \mu(E)^{1/p}$ and $b = \mu(F)^{1/p}$ we see that $$\|1_{E} + 1_{F}\|_{p} = (a^p + b^p) > a + b = \|1_{E}\|_{p} + \|1_{F}\|_{p}$$ I am not sure where to go from here, any suggestions is greatly appreciated.","['lp-spaces', 'analysis']"
1755502,Finding a function based on its Derivative without Integrating,"My question revolves around finding a function based on its derivative of the type below : Problem :
The limit below represents the derivative of some real-valued function $f$ at some real-number $a$. State such an $f$ and $a$ in this case. $$\lim_{h \ \to \  0} \frac{\sqrt{9+h}-3}{h}$$ Now this type of problem is slightly unique. In general we can always find a function based on it's derivative by taking the indefinite integral of the derivative, however in this case we don't have the derivative in a general form, we only have the value for the derivative function at some point $a$, and there are a large number of $f$'s which can produce the value for the derivative at that point. Am I correct in saying this? This problem above is easily solvable, anyone can see already that the function is obviously going to be $f(x) = \sqrt{x}$, but the way I seem to solve it is a very heuristic method, which bothers me greatly (i.e. similar to guessing a function and working from there). I'm trying to find a methodical way of solving problems of this type, as the way I solve it (shown below) will definitely break down for harder examples. This is my solution : By the definition of a derivative : $$f'(x) = \lim_{h \to\ 0} \frac{f(x+h)-f(x)}{h}$$ In the above case we can see that $\ f(a+h) = \sqrt{9+h}\ $ and $\ f(a)=3 = \sqrt{9}$ This implies that $a = 9$ and $f(x) = \sqrt{x}$ or written more formally ($f : x \to \sqrt{x}, \ \forall x\in\mathbb{R^{+}}$) As you can see that is a very hap-hazard solution, and I would like to find a better way to solve problems of these types, but it eludes me. Is there a more methodical approach (or formal approach) to solving problems of this type, where we are given a limit representing the derivative of some real-valued function $f$ at some real-number $a$, and asked to find $f$ and $a$? If you have any other suggestions for solving these types of problems, please comment below.","['derivatives', 'real-analysis', 'limits', 'calculus', 'integration']"
1755510,"If $H$ is a Hilbert space, are we able to identify the derivative ${\rm D}f(x)$ at some $x\in H$ of a differentiable $f\in H'$ with an element of $H$?","I'm confused about some equation I've seen in a book and want to write down some thoughts. I would appreciate, if somebody could tell me whether I'm terribly mistaken or not: Let $(H,\langle\;\cdot\;,\;\cdot\;\rangle)$ be a Hilbert space over $\mathbb R$ and $f:H\to\mathbb R$ be Fréchet differentiable . By definition, the Fréchet derivative ${\rm D}f$ of $f$ is a mapping $H\to H'$$^1$. By Riesz' representation theorem , for each $L\in H'$ we can find a unique $v\in H$ with $$Lu=\langle u,v\rangle\;\;\;\text{for all }u\in H\;.$$ So, we should be able to identify ${\rm D}f$ with a mapping $H\to H$ such that for all $x\in H$ $${\rm D}f(x)u=\langle u,{\rm D}f(x)\rangle\;,\tag 1$$ where ${\rm D}f(x)$ is considered as an element of $H'$ and $H$ on the left-handed and right-handed side, respectively. How would we state $(1)$ for the second derivative ${\rm D}^2f$? $^1$ Let $H'$ denote the topological dual space of $H$.","['hilbert-spaces', 'duality-theorems', 'operator-theory', 'functional-analysis', 'frechet-derivative']"
1755521,"If a divisor $D$ satisfies that $D^{2}=1$, is it true that the morphism induced by $|D|$ is birational?","Let $X\subset \mathbb{P}^{5}$ be a non-degenerate algebraic surface. Let us suppose that $D\subset X$ is a curve such that $D^{2}=1$. I would like to know if the rational map induced by the complete linear system $|D|$
$$
X\dashrightarrow \mathbb{P}(H^{0}(X,\mathcal{L}(D))),
$$
where $\mathcal{L}(D)$ is the sheaf of modules associated to $D$, is birational. If I am not wrong, under these hypothesis
$$
\mathcal{L}(D)\otimes \mathcal{L}(D)\simeq \mathcal{O}_{X}(1).
$$
Does it help? Do we need any more hypothesis? Do we need less?","['divisors-algebraic-geometry', 'coherent-sheaves', 'algebraic-geometry']"
1755560,Explain Why ${21 \choose 2}^2 - {21 \choose 2} = 3!{22 \choose 4}$,"I was given this little problem for precalc homework after a class discussion on series and sigma notation, and applying combinatorial approaches to them. We happened upon the equation in a larger problem we were doing, and the teacher was surprised that it was true and suspected that there might be an underlying combinatorial explanation for it, which he challenged us to find. I suspect it might have something to do with the Hockey Stick Theorem , which we had explored earlier in class.",['combinatorics']
1755616,Why is $v/\|v\|$ not a unit vector?,"I have a homework question that is seriously stumping me. It was a true/false statement.
The statement is ""If $v$ is any vector in an inner product space $V$, then $v/\|v\|$ is a unit vector"" and according to my professor, the statement is false.
Isn't dividing a vector by its length the very definition of normalizing a vector? Edit: Thanks everyone! I didn't even consider the null vector.","['linear-algebra', 'vectors']"
