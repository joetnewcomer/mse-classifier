question_id,title,body,tags
3834499,On locating inflection points,"From what I have learnt, a point of inflection of a curve is, by definition, a point where the curve changes concavity. The Simple Case Thus, if, for a point, $c$ , on a given function, $f(x)$ , $f'(c) = f''(c) = 0$ and $f'''(c) \neq 0$ , then $c$ is a point of inflection. I believe I understand the explanation for this, as, by definition, the second derivative describes concavity, so the third would necessarily describe the rate of change of concavity. Then, since $f'(c) = f''(c) = 0$ and $f'''(c) \neq 0$ , we can conclude that the rate of change of the second derivative is non-zero, so concavity is changing and $c$ is a point of inflection. Please feel free to correct me if my explanation for this is wrong! The General Case However, on doing a little more research, I found out that this phenomenon can actually be generalised as follows: If $f(x)$ is $k$ times continuously differentiable in a certain neighborhood of a point $x$ with $k$ odd and $k ≥ 3$ , while $f^{(n)}(x_0) = 0$ for $n = 2, …, k − 1$ and $f^{(k)}(x_0) ≠ 0$ , then $f(x)$ has a point of inflection at $x_0$ . Questions I do not understand how to explain this, since I thought that only the third derivative (and not other higher-order derivatives) describes rate of change of concavity, so I have the following four questions: How do we generalise my observation about the feature of the third derivative to any odd-numbered derivative (below the second)? Why does this generalisation only apply to odd-numbered derivatives (below the second)? In other words, why does it not apply to even-numbered derivatives (below the second)? I also know that there can be inflection points where the second derivative is undefined. How, then, do we confirm that there is an inflection point there? Is the fact that the second derivative is undefined a sufficient condition? As an extension to my third question, what if the second derivative is defined and equals zero at the particular point, but the third derivative is undefined? How, then, do we confirm that there is an inflection point there? Background Perhaps I might add that I am currently taking a introductory module in calculus at university level, so my level of knowledge about calculus at present may not be in-depth enough to understand the sophisticated explanations that I expect would be coming my way. I have learnt IVT, EVT, Rolle's Theorem, MVT, Cauchy's MVT and L'Hopital's Rule, but that is about it as far as theorems are concerned, so I would greatly appreciate it if there are any intuitive/""lower-level"" explanations to this :)","['calculus', 'derivatives', 'stationary-point', 'real-analysis']"
3834513,"Three externally touching circles have their centers on the same line and have radii $a$,$b$ and $c$ (where $a<b<c$).Prove that $b^2=ac$.","Three circles have their centers on the same line and have radii $a$ , $b$ and $c$ (where $a<b<c$ ).The circle with radius $b$ touches the other two circles but circles with radii $a$ and $c$ do not touch each other.The three circles also have a common tangent.Prove that $b^2=ac$ . I solved it with good deal of calculations. Is there an elementary way to do this problem","['circles', 'geometry']"
3834561,A Complex Function is Constant if It Satisfies One of These Properties,"I am given that a complex function $f(z) = u(x,y) + iv(x,y)$ is analytic in a region $\Omega$ . I am also given a list of conditions and must show that if $f$ satisfies any one of those conditions in $\Omega$ , then $f$ is constant in $\Omega$ . These are the conditions that I am having trouble with: a. $f(\Omega) = \{f(z) : z \in \Omega\}$ is a subset of a circle. b. $u^n(x,y) = v(x,y)$ for some $n \in \mathbb{N}$ . c. $Re(f)$ is analytic on $\Omega$ . My thoughts for part a: is this property saying that f is bounded by the subset of the circle? If so, can I apply Liouville's Theorem and say that since f is bounded, it must be constant? For part b., my approach was to substitute $u^n(x,y)$ in for $v(x,y)$ in the function, and then use the Cauchy Riemann equations (since $f(z)$ is analytic). Calculating the partial derivatives gave me $u_x = nu^{n-1}u_y$ and $u_y = -nu^{n-1}u_x$ . I am not sure if I did the partial differentiation correctly, though, and I'm not sure where to go from here. For part c., I understand what it means for a complex function to be analytic, but what does it mean for its real part to be analytic? And how does that show that $f$ is constant on the region $\Omega$ ?",['complex-analysis']
3834590,logarithm of complex number,"Generally for logarithms
If I have $2^4=16$ then it means $\log_2(16)=4$ (Here 2 is the base) So the value of logarithm basically tells us  how many times to multiply the base for the number. When we take ln it simply means base is e Now begins my question What is the logarithm of complex number? I thought since logarithm tells us how many times to multiply the base. If I would take logarithm I would get real numbers Because no matter the number if real numbers are multiplied the answer is real. But the book I have says its complex.It even has an derivation for it. Can someone explain logarithms of complex number relating real or at least share some resources? Thank you","['complex-analysis', 'complex-numbers', 'logarithms']"
3834613,Number of possible graphs of N vertices connected/disconnected.,"Motivated by this question here Number of Connected simple graphs with n vertices , I referred many webpages to find an apt proof for it But I am now confused because some of the links mention Number of Simple Graphs with $N$ vertices as $2^{\frac{N(N-1)}{2}} $ but For $N=4$ we get 11 graphs in total( Link ) and the answer is different for complete series , can anyone Answer it that what is the right answer for number of graphs ? Some Links mentioned about isomorphic and non -isomorphic graphs , Please explain them too because I didn't get a good resource to read about them.","['graph-theory', 'graph-connectivity', 'combinatorics', 'discrete-mathematics']"
3834656,Proof of Extended Law of the Mean (Taylor's Formula),"I started reading ""The Mathematics of nonlinear Programming - AL peressini"" recently. Theorem : Suppose that $f(x) , f'(x) ,f''(x)$ exist on the closed interval $[a,b]$. If $x^* ,x $ are two different points of $[a,b]$ , then there exists a point $z$ strictly between $x^*$ and $x$ such that $$f(x)=f(x^*)+f'(x^*)(x-x^*)+\frac{f''(z)}{2}(x-x^*)^2$$ I tried to prove this theorem using The Fundamental theorem of calculus. But I can not stop it at the second derivative . Do I have to use mean value theorem ? or is there a simple proof for this ?","['calculus', 'nonlinear-optimization', 'taylor-expansion']"
3834740,Prove the inequality without using the concept of Arithmetic and Geometric mean inequality,"$\displaystyle ( 2m+1) r^{m}( 1-r) < 1-r^{2m+1}$ where $r<1$ and m is positive integer.
I can prove it by concept of arithmetic and geometric mean inequality.
But I am curious to know whether there is any other method to prove it as in the book nothing has been mentioned about arithmetic-geometric mean inequality throughout the chapter Geometric progression (Higher Algebra by Hall and Knight).Can this be solved only by theorem of summation of series etc.","['arithmetic-progressions', 'a.m.-g.m.-inequality', 'sequences-and-series', 'inequality', 'geometric-progressions']"
3834752,Basic Confusion on Push-Forward of a Measure,"Let $\rho:\mathbb{R}^d\to \mathbb{R}$ be a probability density on $\mathbb{R}^d$ . Let $f:\mathbb{R}^d\to \mathbb{R}^d $ be invertible. Consider the push-forward of $\rho$ by $f$ , denoted $f_{\#}\rho$ , see Wikipedia . My question is the integral of $f_{\#}\rho$ always 1 ? And is $f_{\#}\rho$ guaranteed to be a density (i.e absolutely continuous with respect to Lebesgue measure ?","['pushforward', 'measure-theory', 'change-of-variable', 'density-function']"
3834764,"If $f: \mathbb{N} \to \mathbb{N}$ and $\forall x \in \mathbb{N},\,(f \circ f )(x) = x^{4}$, is $f$ polynomial?","If $f: \mathbb{N} \to \mathbb{N}$ and $\forall x \in \mathbb{N},\,(f \circ f )(x) = x^{4}$ , is $f$ polynomial? Of course if we forget about the first hypothesis and we take $f$ such as $\forall x \in \mathbb{R^*}, \, f(x)=\frac{1}{x}$ we have $\forall x \in \mathbb{R^*}, \, f(f(x))=x$ and $f$ isn't polynomial. But I don't see how this could help.","['algebra-precalculus', 'polynomials']"
3834777,Determine a in the system such that the system is consistent.,"The system is: $x_1 + 2x_2 - x_3 = 2, $ $2x_1 - x_2 + x_3 = 1 $ , $-x_1 + 4x_2 -2x_3 = a $ To begin solving the system, I did Row 2 - 2(Row 1). Row 3 + Row 1, then Row 3 + Row 2. This left me with \begin{bmatrix}1&2&-1&2\\0&-5&3&-3\\0&0&0&a-1\end{bmatrix} If all I am trying to find is if the system is inconsistent, do I need to continue reducing it completely to reduced echelon form? Or is it that since I solved the row with a, I can stop there leaving the system as consistent when a = 1.","['matrices', 'systems-of-equations', 'linear-algebra']"
3834779,Derivative of matrix-valued function with respect to scalar,"Given vector $\mu \in \Bbb R^n$ and $n \times n$ matrices $A$ and $\Sigma$ , let matrix-valued function $F : \Bbb R \to \Bbb R^{n \times n}$ be defined by $$F(t) := |(I-2tA\Sigma)|^{1/2} \exp \left(\frac{1}{2}\mu'[I-(I-2tA\Sigma)^{-1})\Sigma^{-1}\mu \right)$$ How can I get the second derivative of $F$ with respect to $t$ ? Could someone please give me a hint?","['matrices', 'calculus', 'matrix-calculus', 'linear-algebra', 'derivatives']"
3834783,"""There are only 5 non-linear differential equations with fixed singularities.""","In the essay the pernicious influence of mathematics on philosophy Gian Carlo Rota makes an offhand remark: it is a fact that there are only five non-linear differential equations with fixed singularities What does this remark mean? In particular what does it mean for a differential equation to have ""fixed singularities"" and what theorem states that there only 5 such differential equations? (Is the context of this statement ordinary or partial differential equations?)","['soft-question', 'reference-request', 'ordinary-differential-equations', 'partial-differential-equations']"
3834796,summing this binomial series,"I found a really interesting question which is as follows:
Prove that the value of $$\sum^{7}_{k=0}[({7\choose k}/{14\choose k})*\sum^{14}_{r=k}{r\choose k}{14\choose r}] = 6^7$$ my approach: I tried to simplify the innermost sigma as well as  trying to simplify by using ${n\choose k}=n!/k!(n-k)!$ however I am can't get hold of this one. My guess is that the summation simplifies into a standard series but I can't say for sure.
Kindly help me out.","['summation', 'binomial-coefficients', 'combinatorics', 'sequences-and-series', 'binomial-theorem']"
3834801,"Prove that $\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0$","I am trying to prove that $\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0$ using the $\epsilon$ - $\delta$ limit definition. So to prove that $\lim_{(x,y)\to(2,0)}{\frac{xy^2}{x+y^4+3}}=0$ it would be enough to show that $\forall \epsilon>0 \; \exists \delta>0 : \; [\sqrt{(x-2)^2+y^2}<\delta] \implies \Big| \frac{xy^2}{x+y^2+3}\Big| < \epsilon $ . Now I can't figure out anything reasonable to do to find what $\delta$ should be. I also tried using a rectangle instead of a sphere, but didn't get anywhere with this approach either. I've been struggling for quite a while with this and I appreciate all the help I can get.","['limits', 'multivariable-calculus', 'epsilon-delta']"
3834881,Show $X=\{(x_n)_{n\in\mathbb{N}}\in \mathbb{Z}^{\mathbb{N}}:x_{2i-1}<x_{2i+1} \land x_{2(i+1)}<x_{2i}\ \forall \ i \in \mathbb{N}\}$ is uncountable.,"I intend to show that $X=\{(x_n)_{n\in\mathbb{N}}\in \mathbb{Z}^{\mathbb{N}}:x_{2i-1}<x_{2i+1}\ \text{and} \ x_{2(i+1)}<x_{2i}\ \forall \ i \in \mathbb{N}\}$ , the set of all integer sequences that have increasing odd subsequences and decreasing even subsequences, is uncountable. Proof. Proof by contradiction using Cantor's diagonal argument. We'll show that there's at least one element in $X$ that is not hit by a function $\varphi\colon \mathbb{N}\to X$ . Suppose that $X$ is countable. Then we can enumerate its sequences $(x_n)_{n\in \mathbb{N}}$ as: \begin{align*}
    1\mapsto \varphi(1)&=s_1=(s_{11},s_{12},s_{13},\dots)\\
    2\mapsto \varphi(2)&=s_2=(s_{21},s_{22},s_{23},\dots)\\
    3\mapsto \varphi(3)&=s_3=(s_{31},s_{32},s_{33},\dots)\\
    &\vdots\\
    n\mapsto \varphi(n)&=s_n=(s_{n1},s_{n2},s_{n3},\dots)\\
    &\vdots
\end{align*} Define $s_n(m) := s_{nm}$ . Example: $s_1(2)=s_{12}$ . We now build the sequence $s$ , defined inductively by: \begin{align*}
    &s(1)=s_1(1)+1\\
    &s(2)=s_2(2)-1\\
    &s(2n+1)=\max{(s_{2n+1}(2n+1),s(2n-1))}+1\\
    &s(2n)=\min{(s_{2n}(2n),s(2(n-1))}-1
\end{align*} We see that although $s\in X$ , it is not hit by any $n\in\mathbb{N}$ . Therefore $\varphi$ is not surjective and $X$ is uncountable. Is this correct? Any correction or proof-writing tip is obviously appreciated.","['elementary-set-theory', 'solution-verification']"
3834919,Showing the inequality $f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1$ for $0<x<1$,"My proof is not really natural but I think it works. We want to show 1 : Let $0<x<1$ such that  then we have : $$f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1$$ Case $0<x\leq 0.25$ The proof of this case is due to user Batominovski: By Bernoulli's inequality we have: $$(1-x)^{2x}\leq 1-2x^2\quad (1)$$ We have also: $$x^{2-2x}\leq 2x^2\quad (2)$$ Summing $(1)$ and $(2)$ we get the desired inequality. Case $0.25\leq x\leq 0.49$ : I shall prove it later but we have : Let $0.25\leq x\leq 0.49$ then we have : $$p(x)=2^{2x}(1-x)x^{2}2\geq x^{2(1-x)}$$ And $$h(x)=\cos^2\Big(x\frac{\pi}{2}\Big)(1+\frac{195}{100}(1-x)(0.5-x)x^2)\geq (1-x)^{2x}\quad (3)$$ With my work we have : $$f^2(x)\leq \Big(\cos^2\Big(x\frac{\pi}{2}\Big)(1+\frac{195}{100}(1-x)(0.5-x)x^2)\Big)^2+\Big(2^{2x}(1-x)x^{2}2\Big)^2+4^{1.95}(x(1-x))^{2.95}2\leq 1$$ To show it we can use power series see here Case $0.49\leq x \leq 0.5$ On the domain $[0.49,0.51]$ the function $g(x)=x^{2(1-x)}$ is concave or: $$g''(x)\leq 0\quad (4)$$ So we have by Jensen's inequality: $$g(x)+g(1-x)\leq 2g(0.5)=1$$ As $f(x)=f(1-x)$ it's proved for $0.5\leq x<1$ Question: How to show $(3)$ ? Thanks in advance! 1 Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"",
The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938","['exponentiation', 'inequality', 'jensen-inequality', 'derivatives']"
3834978,"Q: Prove that if $f(a)g(b) = f(b)g(a),$ then there exists $x\in (a,b)$ such that $f'(x)/f(x)=g'(x)/g(x)$.","I have to prove the following statement holds: Suppose $a,b\in\mathbb{R}$ and $a<b$ . Let $f,g$ be continuous on $[a,b]$ and differentiable on $(a,b)$ , and $f(x)\ne0$ , $g(x)\ne0$ for all $x\in[a,b]$ . If $f(a)g(b)=f(b)g(a)$ , then there exists some $x_{0}\in(a,b)$ such that $\frac{f'(x_{0})}{f(x_{0})}=\frac{g'(x_{0})}{g(x_{0})}$ . I have thought about using both the intermediate value theorem and the mean value theorem, since the functions $f$ and $g$ are continuous and differentiable. For example, we have by the mean value theorem that $\exists x_{0}\in(a,b)$ such that $$
f(b)-f(a)=f'(x_{0})(b-a)
$$ and $$
g(b)-g(a)=g'(x_{0})(b-a)
$$ whereby $$
\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(x_{0})}{g'(x_{0})}
$$ I'm not entirely sure how to use the hypothesis that $f(a)g(b)=f(b)g(a)$ from here. Any suggestions would be very helpful! Edit: This is the proof I have come up with: Let $a,b\in\mathbb{R}$ and $a<b$ . Let $f$ and $g$ be continuous on $[a,b]$ , differentiable on $(a,b)$ , and $f(x)\ne 0$ , $g(x)\ne0$ for all $x\in[a,b]$ . Suppose $f(a)g(b)=f(b)g(a)$ . Then $\frac{f(a)}{f(b)}=\frac{g(a)}{g(b)}$ . Let $p(x)=\ln(|f(x)|)$ and $q(x)=\ln(|g(x)|)$ , then $$
p'(x) = \frac{f'(x)}{f(x)} \text{ for all } x\in(a,b)
$$ while $$
q'(x)=\frac{g'(x)}{g(x)} \text{ for all } x\in(a,b)
$$ Furthermore, given some $x_{0}\in(a,b)$ , according to the mean value theorem we obtain $$
p(b)-p(a)=p'(x_{0})(b-a)\Longrightarrow \ln(\left|\frac{f(b)}{f(a)}\right|)=\frac{f'(x_{0})}{f(x_{0})}(b-a)
$$ similarly, if $x_{0}\in(a,b)$ then $$
q(b)-q(a)=q'(x_{0})(b-a)\Longrightarrow \ln(\left|\frac{g(b)}{g(a)}\right|)=\frac{g'(x_{0})}{g(x_{0})}(b-a)
$$ By hypothesis, we have $$
\ln(\left|\frac{g(b)}{g(a)}\right|)=\ln(\left|\frac{f(b)}{f(a)}\right|)
$$ and thus $$
\frac{f'(x_{0})}{f(x_{0})}(b-a)=\frac{g'(x_{0})}{g(x_{0})}(b-a)\Longrightarrow
\frac{f'(x_{0})}{f(x_{0})}=\frac{g'(x_{0})}{g(x_{0})}
$$ From the comments below there is an issue with choosing $x_{0}\in(a,b)$ for both functions using the mean-value theorem. This is the only part of the proof I am struggling with now.","['continuity', 'derivatives', 'real-analysis']"
3834998,"Ellipsoid in $L^p([0,1],\lambda)$ spaces?","Let us consider $L^p([0,1],\lambda)$ spaces, were $\lambda$ is simply the lebesgue measure.
These are Banach spaces for $p\ge1$ (of course). It is well known (see for example here: Embedding of Lp spaces ), that for $ 1\leq p < q \leq +\infty$ we have an inclusion (embedding) $$ T^q_p \ : \ L^q([0,1],\lambda) \rightarrow L^p([0,1], \lambda),$$ which is linear (thus also affine). One thing I know about affine transformations of real $\mathbb{R}^{n}$ spaces is, that it can transform an ellipsoid into a sphere (and vice versa). This is why I wonder what can we say about this fact in the more abstract setting of $L^p$ spaces. Let as take $p=2$ and any $q>2$ . Define $$S^q_2=\{f\in L^q:||f||_2=1\}.$$ Then $S_2^q$ considered in the $L^2$ space is a subset of the unit sphere.
The question is As $T_2^q(S_2^q)=S_2^q$ , and $T_2^q$ is affine, does it hold true, that $S_{2}^q$ is ""some kind"" of an ellipsoid in the $L^q$ space? If so, how is an appropriate notion of such ellipsoid defined? The only thing that comes to my mind would possibly  be as a set of points such that a sum of their distances from some given $f$ and $g$ functions is fixed. But this is only a childish guess... I would be glad for any insight.","['lp-spaces', 'functional-analysis', 'affine-geometry']"
3835032,Closed form of $\frac{e^{-\frac{\pi}{5}}}{1+\frac{e^{-\pi}}{1+\frac{e^{-2\pi}}{1+\frac{e^{-3\pi}}{1+\ddots}}}}$,"It is well known that $$\operatorname{R}(-e^{-\pi})=-\cfrac{e^{-\frac{\pi}{5}}}{1-\cfrac{e^{-\pi}}{1+\cfrac{e^{-2\pi}}{1-\cfrac{e^{-3\pi}}{1+\ddots}}}}=\frac{\sqrt{5}-1}{2}-\sqrt{\frac{5-\sqrt{5}}{2}}$$ where $\operatorname{R}$ is the Rogers-Ramanujan continued fraction: $$\operatorname{R}(q)=\cfrac{q^{\frac{1}{5}}}{1+\cfrac{q}{1+\cfrac{q^2}{1+\cfrac{q^3}{1+\ddots}}}},\, q=e^{\pi i\tau}.$$ But I'm interested in $\operatorname{R}(e^{-\pi})$ . Numerically, I checked that it agrees with the root of the following octic equation near $x=\frac{1}{2}$ to at least $16$ decimal places: $$x^8+14x^7+22x^6+22x^5+30x^4-22 x^3+22 x^2-14x+1=0;$$ the root turns out to be equal to $$\frac{\sqrt{5}-1}{2}\frac{\sqrt[4]{5}+\sqrt{2+\sqrt{5}}}{\sqrt{5}+\sqrt{2+\sqrt{5}}}.$$ So is it true that $$\frac{e^{-\frac{\pi}{5}}}{1+\cfrac{e^{-\pi}}{1+\cfrac{e^{-2\pi}}{1+\cfrac{e^{-3\pi}}{1+\ddots}}}}=\frac{\sqrt{5}-1}{2}\frac{\sqrt[4]{5}+\sqrt{2+\sqrt{5}}}{\sqrt{5}+\sqrt{2+\sqrt{5}}}?$$ The $2$ 's and $5$ 's under the square roots seem very suggestive of the nature of $\operatorname{R}$ . Also, how could it be proved in that case? Using $$\frac{1}{\operatorname{R}(q)}-\operatorname{R}(q)=\frac{\left(q^{\frac{1}{5}};q^{\frac{1}{5}}\right)_{\infty}}{q^{\frac{1}{5}}(q^5;q^5)_{\infty}}+1$$ and $$\frac{\eta (e^{-\pi\sqrt{n}})}{\eta \left(e^{-\frac{\pi}{\sqrt{n}}}\right)}=n^{-\frac{1}{4}},\, n\gt 0$$ (where $\eta (q)=q^{\frac{1}{12}}\prod_{n\ge 1}(1-q^{2n})$ is the Dedekind eta function),
I've been able to evaluate $\operatorname{R}(-e^{-\pi})$ and $\operatorname{R}(e^{-2\pi})$ , but I don't know how could it be used to evaluate $\operatorname{R}(e^{-\pi})$ . Perhaps something else is necessary. I was inspired by Ramanujan's first letter to Hardy, where Ramanujan's $7\text{th}$ theorem states that $$\cfrac{1}{1+\cfrac{e^{-\pi\sqrt{n}}}{1+\cfrac{e^{-2\pi\sqrt{n}}}{1+\cfrac{e^{-3\pi\sqrt{n}}}{1+\ddots}}}}$$ can be exactly found for any $n\in\mathbb{Q}^{+}$ .","['complex-analysis', 'modular-function', 'continued-fractions', 'closed-form']"
3835103,Find limit at 0 of cosine function with embedded sine,"I'm getting stuck on the following exercise where I have to find the limit as x approaches zero, for this cosine function: $$\lim_{x \to 0}\cos\left(\frac{\pi\sin^2(x)}{x^2}\right)$$ The graph shows that there should be a limit of $-1$ at $0$ , but I can't find a nice trigonometric identity that allows me to rewrite this such that the $x^2$ in the denominator disappears. Any indication on how to solve this?","['limits', 'trigonometry']"
3835146,L1 norm of difference between two probability distributions over a finite set,"Let $P$ and $Q$ be two probability distributions over a finite set $\mathcal{A} = \lbrace 1,2,\dots, a\rbrace$ , show that $\|Q-P\|_1 = 2 \max_{A \in \mathcal{A}} (Q(A) - P(A))$ , where $\|Q-P\|_1 = \sum_{k=1}^{a} |Q(k) - P(k)|$ Could someone provide a proof of this identity? Does it go by a certain name? I found it in this publicly available paper (equation 14).","['statistics', 'probability-theory', 'probability']"
3835189,The Frobenius norm,"Let P be a non zero projector. Is ( $||P||_F\geq 1$ with equality if and only if P is an orthogonal projector) true? My question is about the the Frobenius norm of P, is this true $||P||_F\geq 1$ with equality if and only if P is an orthogonal projector.
I know it is happen in $2$ -norm.","['statistics', 'linear-algebra']"
3835272,Question about the application of the Mean value Theorem [duplicate],"This question already has an answer here : Proving a different version of MVT (1 answer) Closed 3 years ago . Question: Let $f$ be a real-valued differentiable function in $(a, b)$ . Suppose that there exist $x, y ∈ (a, b)$ such
that $x < y$ , $f'(x) = 0 = f'(y)$ and $(f(y)−f(x))/
(y−x) > 0$ . Prove that there exist a $ζ ∈ (x, y)$ such that $(f(ζ) − f(x))/(ζ − x)
= f'(ζ)$ My Approach: I thought this is an easy question because the condition says that $f$ is differentiable, which means it is continuous. So it satisfies the condition for applying the mean value Theorem. Since $(x,y) ∈ (a,b)$ , the differentiability and continuity still holds for domain $(x,y)$ which means we can still apply the mean value theorem, and thus proving the statement true. However, according to this approach, I am not fully using the conditions giving in the question: $x < y$ , $f'(x) = 0 = f'(y)$ and $(f(y)−f(x))/
(y−x )> 0$ Am I missing something about proving this statement?","['derivatives', 'real-analysis']"
3835302,Why does solving this equation with differential equations and related rates yield different results?,"Sorry for the figure being so large I was unsure how to shrink it. The question asked is about a beam anchored at angle $\theta$ to two perpendicular axes (at points a and b). The beam slides along them at a constant speed $-V$ in the $x$ direction and $V_b$ in the $y$ direction. The goal is to solve for $V_b$ in terms of $\theta$ and $V$ . I solved this equation in two different ways, the first using related rates and the second using a differential equation, and don't understand why they yield different results, and I was hoping someone could shed some light on it for me, as I think I violated some mathematical rule when solving with related rates. Solve attempt 1 using related rates: \begin{align*}
\frac{\mathrm{d}x}{\mathrm{d}t} &= -v\\
\frac{\mathrm{d}y}{\mathrm{d}t} &= v_b\\
y &=x\tan\theta\\
\frac{\mathrm{d}y}{\mathrm{d}x} &=\tan\theta\\
\frac{\mathrm{d}y}{\mathrm{d}t} &=\frac{\mathrm{d}y}{\mathrm{d}x}\frac{\mathrm{d}x}{\mathrm{d}t}\\
\frac{\mathrm{d}y}{\mathrm{d}t} &=-v\tan\theta\\
v_b &=-v\tan\theta
\end{align*} Solve attempt 2 using differential equations (where $L$ is the beam) \begin{align*}
x^2+y^2 &=L^2\\
2x\frac{\mathrm{d}x}{\mathrm{d}t}+2y\frac{\mathrm{d}y}{\mathrm{d}t} &=0\\
-2xv+2yv_b &=0\\
v_b &=\frac{x}{y}v\\
v_b &=\frac{v}{\tan\theta}
\end{align*}","['related-rates', 'derivatives', 'ordinary-differential-equations']"
3835339,Existence of primitive permutation group one of whose arc stabilisers is normal in the point stabiliser,"Let $G$ be a primitive permutation group with point stabiliser $G_\alpha$ for some $\alpha$ . For $\beta\ne\alpha$ , by an arc stabiliser we mean $G_{\alpha\beta}=G_\alpha\cap G_\beta$ and an edge stabiliser we mean $G_{\{\alpha,\beta\}}$ , the stabiliser of the set $\{\alpha,\beta\}$ . Note that $G_{\{\alpha,\beta\}}\ge G_{\alpha\beta}$ . My question is: can an arc stabiliser $G_{\alpha\beta}\ne 1$ be normal in $G_\alpha$ ? This is impossible if $G_{\{\alpha,\beta\}}> G_{\alpha\beta}$ . In this case, there exists $g\in G$ such that $\alpha^g=\beta$ and $\beta^g=\alpha$ . It follows that $g\in N_G(G_{\alpha\beta})$ . Note that $G_\alpha$ is maximal in $G$ and $G_{\alpha\beta}$ cannot be normal in $G$ (otherwise $G_{\alpha\beta}$ will be in the kernel of this action), the normaliser $N_G(G_{\alpha\beta})=G_\alpha$ . This gives a contradiction: $g\in G_\alpha$ but $\alpha^g=\beta\ne \alpha$ . I think there might exist an example in the case when $G_{\{\alpha,\beta\}}=G_{\alpha\beta}$ . However, by a quick check with Magma there is no such primitive permutation group $G$ with $|G|\le 300$ . Added: I asked the same question here in MO one day after this was asked.","['permutations', 'group-theory', 'abstract-algebra', 'finite-groups']"
3835396,Finding the angle of two congruent isosceles triangles inscribed in a semi circle.,"Question This question comes from Question 10 of the 2020 AIMO A circle with centre $O$ has diameter $AD$ . With $X$ on $AO$ and points $B$ and $C$ on the circle, triangles $ABX$ and $XCO$ are similar isosceles with base angle $\alpha$ as shown. Find, with proof, the value of $\alpha$ . My Attempt I knew we probably have to use the fact that the isosceles triangles have a base on the diameter and vertex on the circle. Then $\angle ACD=\angle ABD=90^{\circ}$ . Since $\angle ABX=180-2\alpha=\angle XBC$ , then $AB||XC$ . Now, when constructing $CD$ and $BD$ , I notice that $CD$ looked very much like the angle bisector of $\angle XCD$ and $BD$ looked very much like the angle bisector of $\angle CDA$ . Let $CD$ and $BD$ intersect at $I$ . I would need to prove that $I$ is in fact the incenter of $XCD$ to give a rigorous proof. Note that $\angle XCO=180-2\alpha$ and since $CO=OD$ (both a radius), then $\angle ODC=\angle COD=\frac{\alpha}{2}$ . If $I$ were to be the incenter of $XCD$ , then $\angle XCO=\angle OCD\implies 180-2\alpha=\frac{\alpha}{2}\implies \alpha=72^{\circ}$ . This looked to be the right answer (could confirm with geogebra). However, this is also the step that I am struggling to prove. I tried constructing the altitude of $\triangle XID$ , $\triangle CIX$ and $\triangle DIC$ to be $IF,IG$ and $IH$ repsetively. Note that since $AB||XC$ , $\angle ABD=\angle XGD=90^{\circ}$ and thus the points $B,G,I,D$ are colinear. Refer to the following figure: It would suffice to prove that $\triangle CGI\cong\triangle CHI$ and $\triangle XGI\cong\triangle XFI$ . Any hints or solutions would be appreciated. It would be nice if you would be able to show how to prove the last step in my attempt. Otherwise, giving me a hint to a better approach would also be much appreciated. Thank you in advance!","['contest-math', 'euclidean-geometry', 'geometry']"
3835407,"Prove that 5 lines are concurrent, and find the expression for the position vector of the point they all go through.","Pentagon $ABCDE$ is inscribed in a circle centered at the origin. Define the lines \begin{align*}
\ell_{ABC} &= \text{Line through the centroid of $\triangle ABC$ perpendicular to $\overline{DE}$},\\
\ell_{BCD} &= \text{Line through the centroid of $\triangle BCD$ perpendicular to $\overline{AE}$}, \\
\ell_{CDE} &= \text{Line through the centroid of $\triangle CDE$ perpendicular to $\overline{AB}$}, \\
\ell_{DEA} &= \text{Line through the centroid of $\triangle DEA$ perpendicular to $\overline{BC}$}, \\
\ell_{EAB} &= \text{Line through the centroid of $\triangle EAB$ perpendicular to $\overline{CD}$}. \\
\end{align*} These are lines going through the centroid of a triangle formed by three consecutive vertices, perpendicular to the line segment formed by the other two vertices. Here's $\ell_{ABC}$ in the picture: Prove that $\ell_{ABC}, \ell_{BCD}, \ell_{CDE},\ell_{DEA}$ and $\ell_{EAB}$ are concurrent, and find the expression for the position vector of the point they all go through. I truly have no idea how to approach this problem. Please help!","['euclidean-geometry', 'vectors', 'circles', 'geometry']"
3835555,Limit evaluation.,In N. Piskunov he explained differential equations by taking up the example of air resistance acting on a falling body. After evaluating the differential equation he gets an equation for the velocity as: $$v = \left(v_o - \frac{mg}{k}\right)e^{-\frac{kt}{m}} + \frac{mg}{k}.$$ He then states that if $k = 0$ then the equation turns to the basic equation: $$v = v_o + gt.$$ Now I understand this statement because when air resistance is zero this velocity equation holds. However I am not able to prove this statement when I'm evaluating the limit: $$\lim_{k \rightarrow 0} \left[\left(v_o - \frac{mg}{k}\right)e^{-\frac{kt}{m}} + \frac{mg}{k} \right]$$ Any help evaluating the limit would be appreciated!,['limits']
3835558,$\partial_xF+F\partial_yF=0$ implies that $F$=const,"Let $F\in \mathit{C}^2(\mathbb{R^2},\mathbb{R})$ and $$\partial_xF+F\partial_yF=0.$$ How do I prove that $F$ is a constant on $\mathbb{R^2}$ ? I am trying to restrict $F$ to a line passing through the origin and prove that the values of $F$ on the line equal $F(0,0)$ , but failed.","['linear-pde', 'multivariable-calculus', 'partial-differential-equations']"
3835592,subscheme where two morphisms agree is points where they agree on residue fields,"Let $X, Y, Z$ be schemes, where $X, Y$ are $Z$ schemes.  I know the definition of ""the locally closed subscheme of $X$ where two $Z$ -  morphisms $\pi, \pi': X\rightarrow Y$ agree"" from its universal property.  Also I can define it as the fiber product of the diagonal $$\delta :  Y\rightarrow Y\times_Z Y$$ with $$(\pi, \pi'): X\rightarrow Y\times_Z Y.$$ My question: how to prove that the underlying set of ""the locally closed subscheme where the two morphisms agree"" is the same as the set of points where the two morphism agree on the residue field. It is probably clear thatthe former is contained in the latter, but why is it all of them?  That is, why is a point where $\pi, \pi'$ agree on the residue field necessarily contained in ""the subscheme where $\pi, \pi'$ agree""?","['algebraic-geometry', 'schemes']"
3835595,The maximum value of the smaller root of given quadratic function,"Consider the quadratic expression : $$ f(x) = x^2 +(a+2)x + (a^2 - a +2 ) $$ given is that $a , p, q , (p<q)$ are real numbers and p and q are the roots of the equation $f(x)=0$ .
Q1) find the maximum value of q.
Q2) find the maximum value of p. Solution Q1) since q is a root of f(x), we have $f(q) = 0$ . This gives: $$ q^2 + (a+2)q + a^2 - a + 2  = 0 $$ Writing this as a quadratic in a we have $$ a^2 + (q-1)a + q^2 +2q +2  = 0 $$ Since a is given to be real this quadratic must have its determinant greater than or equal to zero, which gives: $$ (q-1)^2 -4(1)(q^2 +2q +2) \ge 0 $$ Which gives: $$ -3q^2 -10q -7 \ge 0 $$ which gives: $q \in [-7/3 , -1]$ giving the maximum value of $q$ (the larger root ) to be -1.
I am however stuck with how to go about formulating the maximum value of the smaller root.","['maxima-minima', 'calculus', 'quadratics', 'algebra-precalculus']"
3835599,Explaining the graph of $\sin(x^2) + \sin(y^2) = 1$,"I had to plot the graph of the implicitly defined function $\sin^2 x + \sin^2 y = 1$ in an exam. This is not particularly difficult, but it got me wondering what the graph would look like when the exponent is taken inside, viz. $$\sin(x^2) + \sin(y^2) = 1$$ I found it difficult to figure this out, so I resorted to Desmos' graphing calculator. It looks like this: I can explain some parts of this picture, but others elude me, and I think someone with more experience will do a better job of saying why this thing looks like it does. I'd be particularly interested to know whether the figure in the middle is a special case of some other function, and similarly with the curlicues on the axes. (I have a good idea of what the polka dots are.) Pre-emptive note : I had no trouble plotting the other thing mentioned ( $\sin^2 x + \sin^2 y = 1$ ), so you needn't bother including that in your answer. EDIT : After looking at @Jean Marie's answer below, I plotted the graph of $$(x^2 + y^2) - \frac{x^6 + y^6}{6} = 1,$$ reasoning that near the origin a few terms of the Taylor series might help. The resulting graph was quite similar to the strange shape near the origin in thing above. Somewhat unexpectedly (at least for a callow neophyte like me), a much closer approximation (shapewise) was offered by $$x^2 + y^2 - \frac{x^4 + y^4}{4} = 1$$ In case it is of any use, here is a picture (from Desmos) of the two plots I mentioned. In the figure above, the blue is the sextic and the red the quartic.","['implicit-function', 'algebra-precalculus', 'graphing-functions', 'trigonometry']"
3835602,Find a closest integer of form $3^x 5^y$?,"Given a arbitrary positive integer, is it possible to have a close-form solution of finding the closest (either smaller or larger) integer of form $3^x  5^y$ , where $x$ and $y$ are non-negative integers? I can only think of using computer to search but wonder if it can be done better?","['number-theory', 'prime-factorization', 'closed-form', 'prime-numbers']"
3835616,How Different is an Eigenvalue Problem from an Ordinary Differential Equation,"I have been thinking about how different an eigenvalue problem such as that of the Sturm-Liouville Equation (SL) with that of a second-order linear differential equation. It doesn't seem to be the case to be the difference between a boundary value problem vs. an initial value problem, as we may also have BVPs that do not involve eigenvalues, in which case solutions might not exist in abundance as opposed to the case with IVPs. Perhaps, the eigenvalue parameter is there to allow us to speak of to which values of the parameter there exist a solution. However, that brought me to the question as to what happens when no boundary values are imposed, such as in the case of the famous Simple Harmonic Oscillator (SHO) SL problem in Quantum Mechanics (QM), where the wavefunction, i.e. the solution of the ""eigenvalue"" problem, is defined throughout the entire real line without any restriction - except for the fact that they approach the $x$ -axis asymptotically, which actually follows from square-integrability. I put the word ""eigenvalue"" in quotation marks because I am now questioning it. Let me explain. The SHO problem is the seeking of the solution $\psi$ to the ""eigenvalue"" problem $\displaystyle-\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}+\frac{1}{2}m\omega^2x^2\psi=E\psi$ . We can clean up the equation by absorbing constants, but we will keep them. It is well-known in QM that solutions to this eigenvalue problem are products of Hermite polynomials with a Gaussian and the eigenvalues $E$ form a discrete set and are given by $E_n=\left(n+\frac{1}{2}\right)\hbar\omega$ . Note that no boundary conditions are imposed although, of course, we are seeking for square-integrable functions. Herein lies the question: If we rewrite the SHO equation as $\displaystyle-\frac{\hbar^2}{2m}\frac{d^2\psi}{dx^2}+\left(\frac{1}{2}m\omega^2x^2-E\right)\psi=0$ , it does look to me like a legit 2nd order variable-coefficient linear ODE that has a solution for any $E$ . Specifically, $E$ does not have to assume specific values that form a discrete set. What am I missing here? That not all $E$ s give square-integrable solutions? How is the SHO an eigenvalue problem and not a mundane differential equation problem? Appreciate your insights.","['ordinary-differential-equations', 'sturm-liouville', 'eigenfunctions', 'quantum-mechanics', 'boundary-value-problem']"
3835639,Proof verification: $\lim\limits_{x\to\infty}f(x)=L\iff\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$,"Problem 81(a) from Section 2.6 of James Stewart's Calculus: Early Transcendentals (8e) asks us to prove that $$\lim\limits_{x\to\infty}f(x)=\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)$$ provided that these limits exist. After thinking for a while, I convinced myself that this result can be strengthened to $$\lim\limits_{x\to\infty}f(x)=L\iff \lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$$ This does indeed strengthen the result because (1) it only requires the existence of at least one of the limits in question, and (2) the existence of either limit implies the existence of the other, and thus implies that $\lim_{x\to\infty}f(x)=\lim_{x\to 0^{+}}f(1/x)$ . Here's my attempt: Let's first prove that $\lim_{x\to\infty}f(x)=L\implies \lim_{x\to 0^{+}}f(1/x)=L$ . Since $\lim_{x\to\infty}f(x)=L$ , we have that for all $\varepsilon >0$ , there exists a $\delta$ such that $x>\delta\implies |f(x)-L|<\varepsilon$ . Since $\lim_{x\to 0^{+}}1/x=\infty$ , there exists a $\delta_1>0$ such that $0<x<\delta_1\implies 1/x>\delta$ . Therefore, $$0<x<\delta_1\implies\frac{1}{x}>\delta\implies\left|f\left(\frac{1}{x}\right)-L\right|<\varepsilon$$ This shows that $$\lim\limits_{x\to\infty}f(x)=L\implies \lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$$ Now let's prove the converse. Since $\lim_{x\to 0^{+}}f(1/x)=L$ , we have that for all $\varepsilon >0$ , there exists a $\delta >0$ such that $0<x<\delta\implies \left|f(1/x)-L\right|<\varepsilon$ . Since $\lim_{x\to\infty}1/x=0$ , there exists a $\delta_1$ such that $x>\delta_1 \implies\left|1/x-0\right|=1/x<\delta$ . Therefore, $$x>\delta_1\implies\frac{1}{x}<\delta\implies\left|f\left(\frac{1}{\frac{1}{x}}\right)-L\right|=|f(x)-L|<\varepsilon$$ This shows that $$\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L\implies\lim\limits_{x\to\infty}f(x)=L$$ Thus, $$\lim\limits_{x\to\infty}f(x)=L\iff\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$$ Let me know if I made any mistakes!","['limits', 'solution-verification', 'epsilon-delta']"
3835644,Avoiding catastrophic cancellation [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 3 years ago . Improve this question Dear computational scientists, I have the following formula that I need to rewrite in order to avoid catastrophic cancellation. $y =\sqrt{\frac{1}{2}(1-\sqrt{1-x^{2})}}$ As x becomes smaller $\sqrt{1-x^{2}}$ approaches 1 so you will get 1 - 1.000000000......1 what will result in a catastrophic cancellation. I tried to rewrite the formula myself in a few different ways, but didn't manage yet to avoid the catastrophic cancellation. The goal is to approximate pi: import numpy as np


tn = 0.5
for i in range(1,100):
    tn1 = np.sqrt(0.5*(1-np.sqrt(1-tn**2)))
    print(i, 6*2**i*tn1)
    tn = tn1 output 1 3.1058285412302498
2 3.132628613281237
3 3.139350203046872
4 3.14103195089053
5 3.1414524722853443
6 3.141557607911622
7 3.141583892148936
8 3.1415904632367617
9 3.1415921060430483
10 3.1415925165881546
11 3.1415926186407894
12 3.1415926453212157
13 3.1415926453212157
14 3.1415926453212157
15 3.1415926453212157
16 3.141593669849427
17 3.1415923038117377
18 3.1416086962248038
19 3.1415868396550413
20 3.1416742650217575
21 3.1416742650217575
22 3.1430727401700396
23 3.1598061649411346
24 3.181980515339464
25 3.3541019662496847
26 4.242640687119286
27 6.0
28 0.0
29 0.0
30 0.0
31 0.0
32 0.0 Question: How should I rewrite the formula to avoid catastrophic cancellation?","['algebra-precalculus', 'abstract-algebra', 'catastrophic-cancellation', 'computer-science']"
3835672,"prove whether functions are injective, surjective or bijective","[Task] Hello, I have a few questions regarding the above mentioned task.
One has to show whether or not these functions are injective, surjective or bijective. This seems straightforward for most of these functions: (1) Not injective since some values are hit multiple times. Also not surjective since not every y has a corresponding x. (2) Bijective, since there is a one to one correspondance. (3) Bijective, since it's only from the natural numbers, so every value will be hit exactly once (4) Don't quite understand where to start with this one. My question now is: How do I properly phrase this? Some of these seem quite intuitive but I'm struggling with the correct notation of this.","['elementary-set-theory', 'functions']"
3835691,Solving $x+\sqrt{a+\sqrt{x}}=a$ for $x$.,"I have to solve the following equation for $x$ , for all values of $a$ : $$x+\sqrt{a+\sqrt{x}}=a$$ Clearly $x\gt0$ For $a\lt0,x\in\phi$ as RHS $\lt0$ but LHS $\gt0$ . $a=0\Rightarrow x=0$ Now for $a\gt0$ $$a-x=\sqrt{a+\sqrt{x}}  ,x\in(0,a)$$ $$(a-x)^2=a+\sqrt{x}.....(1)$$ Now plotting the graphs for $$y=(a-x)^2...a\gt0,x\in(0,a)$$ $$y=a+\sqrt{x}$$ For $a\in(0,1)$ the graphs don't intersect $\Rightarrow x\in\phi$ . Also, $a=1\Rightarrow x=0$ . For $a\gt1$ the graphs intersect so $$a^2-a+x^2-2ax=\sqrt{x}...from (1)$$ Squaring and rearranging $$x^4-4ax^3+6a^2x^2-2ax^2-4a^3x+4a^2x+a^4-2a^3+a^2=x$$ which I am not able to solve further.","['algebra-precalculus', 'radicals']"
3835703,probability that first $2$ drawn balls are red,"A bag contain $9$ red and $12$ blue balls. If $4$ balls are selected randomly without replacement, then find the probability that the first $2$ balls are red. What I tried: Let $A$ be the event the first drawn ball is red and $B$ be the event that the second drawn ball is red. Then $P(A)=8/21$ and $P(B)=7/20$ . If first $2$ drawn balls are red. Then other $2$ drawn balls are both red or blue or one red one blue. But I don't understand how this helps. Please help me! Thanks!",['probability']
3835709,"Using spherical coordinates, is there an equation of a sphere not centered at the origin? If so what is it?","I am a high school teacher teaching Calculus for the first time actually, I am teaching Multivariable Calculus (Calculus 3). Its been a solid 15 years since I took Calculus 3. During a discussion of the text my class and I came up with a question none of us could answer. Each of the textbooks given examples of the spherical coordinates (of a sphere) is
centered at the origin. If the center of a given sphere is not at origin,
are there any changes in the spherical coordinates? If there is, how do I
manipulate that? Now I am not certain if this is even considered, our text says it will not in rectangular coordinates, so I assume it won't in spherical. I am very rusty on this topic and would love additional insights or possible resources.","['multivariable-calculus', 'spherical-coordinates']"
3835744,Dependent variable substitution of a differential equation.,"I am attempting to answer a question from a textbook. The question is as follows: ""Use the substitution $y = x^2$ to turn the differential equation $x\frac{d^2x}{(dt)^2} + (\frac{dx}{dt})^2 + x\frac{dx}{dt} = 0$ into a second order differential equation with constant coefficients involving y and t."" The answer according to the textbook is $\frac{d^2y}{(dt)^2} + \frac{dy}{dt} = 0$ I am unsure what to do with the $(\frac{dx}{dt})^2$ term in the original equation. Is it equivalent to $\frac{d}{dt}(x^2)$ ? If this is the case then my working gets as far as the following before I get stuck: $x\frac{d^2x}{(dt)^2} + (\frac{dx}{dt})^2 + x\frac{dx}{dt} = 0$ $\Rightarrow x\frac{d^2x}{(dt)^2} + \frac{d}{dt}(x^2) + x\frac{dx}{dt} = 0$ $\frac{dy}{dt} =\frac{dy}{dx}\frac{dx}{dt}=2x\frac{dx}{dt} \Rightarrow \frac{d^2x}{(dt)^2}=\frac{d}{dx}(\frac{1}{2x}\frac{dy}{dt})$ $\Rightarrow x\frac{d}{dx}(\frac{1}{2x}\frac{dy}{dt}) + 2x + x\frac{1}{2x}\frac{dy}{dt} = 0$ $\Rightarrow \frac{1}{2x}\frac{dy}{dt} + \frac{1}{2}\frac{d^2y}{(dt)^2} + 2x + \frac{1}{2}\frac{dy}{dt} = 0$ Please can someone show me where I have gone wrong?","['calculus', 'homogeneous-equation', 'ordinary-differential-equations']"
3835745,"Show function $f: \mathbb{N} \times \mathbb{N} \to \mathbb{N}$ defined by $f(m, n) = 2^{m}(2n + 1)$ is a bijection","Let $f: \mathbb{N} \times \mathbb{N} \to \mathbb{N} \ \{0\}$ be the map given by $f(m, n) = 2^{m}(2n + 1)$ for all $(m, n) \in \mathbb{N} \times \mathbb{N}$ .  Show that $f$ is a bijection. The above seems quite unintuitive since there needs to be a one to one correspondence. It has to be bijective but I don't know how I can prove that. My thought process was that the first part of the function is always even and the second part $(2n+1)$ is even for odd $n$ and odd for even $n$ . This means that the result can be both even and odd. How do I show that there is no value that is hit twice?","['elementary-set-theory', 'functions']"
3835802,Evaluating $ \int_{-\infty}^{t} e^{-(\tau+a)^2} \mathrm{erf}(\tau) \mathrm{d}\tau$,"I need to evaluate this integral: $$
I(t,a) = \int_{-\infty}^{t} e^{-(\tau+a)^2} \mathrm{erf}(\tau) \ \mathrm{d}\tau
$$ where the $\mathrm{erf}(\tau)$ is the error function. I can prove that this integral converges. By employing the python library import numpy as np
from scipy.special import erf
import matplotlib.pyplot as plt
dtau = 0.01;p=[]
trange = np.arange(-20,20,0.1)
for t in trange:
    tau = np.arange(-20,t,dtau)
    I = np.exp(-(tau+a)**2)* erf(tau)
    p.append(np.trapz(I,tau))
p=np.array(p)
plt.plot(trange,p);plt.show(); I got three graphs for different $a$ therefor one can speculate the behavior of the integral for $|a|\ll1$ is as a Bi-gaussian function and for $|a|\gg1$ is as an $\mathrm{erf}(t)$ function.
therefore the answer is something like this $$
I(t,a) \sim \alpha(a) \ \mathrm{erf}(t+a) + \beta(a) \ e^{-(t\pm a)^2}
$$ I would highly appreciate it if someone could help me to solve it. Edit: if $t \rightarrow \infty$ , the $I(\infty, a)$ is given by $$
I(t \rightarrow \infty,a) = \sqrt{\pi} \ \mathrm{erf} \Big(\dfrac{a}{\sqrt{2}} \Big)
$$ this might be helpful.","['integration', 'gaussian', 'error-function', 'gaussian-integral']"
3835859,Fubini's theorem and independent random variables,"Let $X_1$ and $X_2$ be two independent random variables. I want a really detailed proof of $\mathbb{E}(X_1 X_2) = \mathbb{E}(X_1)\mathbb{E}(X_2)$ using Fubini's Theorem. So the statement I have of Fubini's theorem is (special case of just two measureable spaces): Let $\nu_1$ and $\nu_2$ be $\sigma$ -finite measures on $(\Omega_1, \mathcal{F}_1)$ and $(\Omega_2, \mathcal{F}_2)$ . Let $f$ be a Borel function on $(\Omega_1, \mathcal{F}_1)\times (\Omega_2, \mathcal{F}_2)$ . Suppose that either $f \geq 0$ or $f$ is integrable with respect to $\nu_1 \times \nu_2$ . Then $$
\int_{\Omega_1 \times \Omega_2} f(\omega_1, \omega_2) d(\nu_1 \times \nu_2) = \int_{\Omega_2} \biggl( \int_{\Omega_1} f(\omega_1,\omega_2) d \nu_1 \biggr) d \nu_2
$$ Now, $$
\mathbb{E}(X_1 X_2) = \int_{\mathbb{R}^2} u t \, d P_{X_1,X_2} = \int_{\mathbb{R}} \biggl( \int_{\mathbb{R}} ut \, d P_{X_1} \biggr) d P_{X_2}
$$ I know that $P_{X_1,X_2} = P_{X_1} P_{X_2}$ by independence and that $P_{X_1}$ and $P_{X_1}$ are $\sigma$ -finite measures on $(\mathbb{R}, \mathcal{B})$ . However, I do not really understand why the independence is needed according to Fubini's theorem. Am I confusing $P_{X_1,X_2}$ with $P_{X_1} \times P_{X_2}$ ? Or does independence give you the $\sigma$ -finite condition? Basically, the question is: what does independence buy you in the theorem as stated above?","['integration', 'measure-theory', 'independence', 'fubini-tonelli-theorems', 'probability-theory']"
3835860,"Let A be $A=${$xy:x \in (0; 1/2),y \in \mathbb{Z}, |y|<3$}. Which is the value of $\inf A+\sup A$?","Let $A=\{xy:x \in (0, \frac{1}{2}):y \in \mathbb{Z}, |y|<3\}$ . Which is the value of $\inf A+\sup A$ ? a) $1$ b) $-1$ c) $\frac{1}{2}$ d) $0$ I got an answer which is none of the alternatives. I thought about it this way:
For $\sup A$ the biggest value that $x$ can take would be $\frac{1}{2}$ and $y$ can take $3.$ So it would be $\frac{1}{2}*3=\frac{3}{2}$ . On the other hand the smallest value for $x$ would be $0$ and for y would be $-3$ so the Infimum would be $0$ . Therefore the sum of the two would be $\frac{3}{2}?$ Apparently the correct answer is $d)$ $0$ . Clearly there is a flaw with my argument since it's none of the alternatives and it would be really helpful if somebody could say me which is that flaw. And also how come $d)?$ Thank you in advance for your help,
Annalisa","['elementary-set-theory', 'supremum-and-infimum', 'real-analysis']"
3835895,Limit of function equivalent to limit of a sequence?,"When I try to compute $\lim_{n\rightarrow\infty}\frac{(\ln n)^2}{n}$ in my mathematical analysis homework, I may be supposed to use Stolz theorem to compute this limit of a sequence . However, I find it much more easier to compute this limit by treating it as a limit of function , say $\lim_{x\rightarrow\infty}\frac{(\ln x)^2}{x}$ , using L'Hospital's Rule . And I got the right answer. However, I'm still worrying about if I can replace $n$ by $x$ under any circumstances. Here are my questions Can anyone tell me under what circumstances can I replace $n$ by $x$ safely when calculating the limit? What's the relationship between Stolz theorem and L'Hospital's Rule ? (The former looks like a discrete version of the latter theorem)","['limits', 'real-analysis']"
3835915,"Given the prime factorization of a number, is there an efficient way of sorting all the factors?","One way of sorting is to generate all the factors and then sort the list.  I was wondering if there was a more efficient way.  Given a number n = product of p i j i , if you sort the p i values then it is easy to find the first few factors.  We can assume that p 1 < p 2 ...< p k . The smallest factor is 1, followed by p 1 and the smaller of p 1 2 and p 2 .  Is there an algorithm to continue is this way?  Note that it is only necessary to find the first half of the factors. Going in reverse direction from n, we get the factors n/1 > n/p 1 and so on.","['discrete-mathematics', 'algorithms']"
3835937,Find the number of homomorphisms from $Z_m$ to $S_n$,The kernel of the homomorphism will be a normal subgroup and the normal subgroups of $Z_m$ are of the form $\langle\frac{m}{d}\rangle$ where $d|m$ .I can find out for particular cases but how do I generalize this? Any suggestion would be really helpful.,"['group-homomorphism', 'quotient-group', 'group-theory', 'abstract-algebra']"
3835939,Is $\mathcal{O}_K^{\times}$ a cyclic group just like $\mathbb{Z}^{\times}$?,"The $\text{units}$ in a ring of integers are those elements whose multiplicative inverse exists. That is, $u$ is unit if $u^{-1}$ also exists in the ring such that $uu^{(-1)}=u^{(-1)}u=\text{multiplicative identity}$ . For example, consider the ring of integers $\mathbb{Z}$ of the rational field $\mathbb{Q}$ , then $\mathbb{Z}^{\times}=$ units in $\mathbb{Z}=\{1,-1 \}.$ This is a cyclic group. In fact, this is trivial and $\mathbb{Z}$ is infinite cyclic group. Now consider the ring of integers $\mathcal{O}_K$ in a finite extension $K \supset \mathbb{Q}$ or the ring of integers $\mathcal{O}_K$ of $p$ -adic field $K \supset \mathbb{Q}_p$ . Now denote the units of $\mathcal{O}_K$ by $\mathcal{O}_K^{\times}$ . Is $\mathcal{O}_K^{\times}$ a cyclic group just like $\mathbb{Z}^{\times}$ ?","['number-theory', 'group-theory', 'cyclic-groups', 'algebraic-number-theory']"
3835942,"Give an example in which for a set $A \subseteq X$, the two sets $f(X \setminus A)$ and $Y \setminus f(A)$ are incomparable.","Give an example in which for a set $A ⊆ X$ , the two sets $f(X \setminus A)$ and $Y \setminus f(A)$ are incomparable (i.e., neither is a subset of the other). My example: Take $X =$ { $1,2$ }, $Y =$ { $3$ }, and $A =$ { $1$ }. So $f(X \setminus A) = \{3\}$ , and $Y \setminus f(A) = \phi$ . Hence neither is a subset of the other. Is my example correct? Any other examples?",['elementary-set-theory']
3835990,Doubt regarding assumption of domain when one isn't given,"Calculate $f(x)=\dfrac{49}{x^2}+x^2$ at the points where $\dfrac{7}{x}+x=3$ . Now if $D_f\subseteq \mathbb R$ , then this has no solutions since $\dfrac{7}{x}+x=3$ has no solution in $\mathbb R$ . But if $D_f=\mathbb C$ , then this equation yields solution: $\dfrac{49}{x^2}+x^2=\Big(\dfrac{7}{x}+x\Big)^2-14=9-14=-5$ (obviously here $x\notin \mathbb R$ ) Now my doubt may be silly (I apologize for that), but until now at the high school level, we have been advised to assume the domain of function to be $\mathbb R$ except when given otherwise. But here the domain must obviously be assumed to be $\mathbb C$ to yield a solution. So what should be assumed as the domain of any function in general when any specific domain isn't mentioned?","['algebra-precalculus', 'functions', 'soft-question']"
3835999,Difficult Asymptotic of a Recursive Sequence,"Let $x_1>1$ and $x_n$ defind by iteration by $$x_{n+1}=x_n+\frac{1}{\prod_{k=1}^nx_k^{1/n}}$$ Prove that there exists $C>0$ such that $x_n \sim C\sqrt{n}$ . This exercise seems difficult. Indeed, here is my start.
First, remark that $x_n$ is positive, hence increasing. Therefore, $x_{n+1}-x_n \geqslant \frac{1}{x_n}$ , therefore $x_{n+1}^2-x_n^2 \geqslant 2$ , and we obtain $x_n \geqslant C_1 \sqrt{n}$ . Plugging this bound we obtain similarly that $x_n \leqslant C_2\sqrt{n}$ and finally we have $x_n = O(\sqrt{n})$ which is a good start. Next, we want to prove that $x_n / \sqrt{n}$ indeed converge. In order to do that we juste have to prove that $x_{n+1}^2-x_n^2$ is a convergent sequence which I can not prove. This is equivalent to proving that $\frac{x_n}{\prod_{k=1}^nx_k^{1/n}}$ converge. Here are the details of a further attempt: as we expect $y_n:=x_n/\sqrt{n}$ to converge, we re-write everything in terms of $y_n$ and obtain: $$y_{n+1}^2-y_n^2=-\frac{y_n^2}{n+1} + \frac{2\sqrt{n}y_n}{(n+1)(n!)^{\frac{1}{2n}}\prod y_k^{1/n}}=\frac{y_n}{n+1} \left(\frac{2\sqrt{n}}{(n!)^{\frac{1}{2n}}\prod y_k^{1/n}}-y_n\right)$$ and now, because of Stirling, we expect that $\lim y_n^2 = \lim y_n \prod y_k^{1/n} = 2\sqrt{e}$ , because otherwise, at least if $2\sqrt{e}$ is not a limit value for $y_n^2$ , then we have $y_{n+1}^2 -y_n^2 \geqslant \frac{C}{n+1}$ and then $y_n$ is unbounded, which is a contradiction. Therefore, this proves at least that $2\sqrt{e}$ is a limit value of $y_n^2$ , and it remains to prove that it is the only one. Assuming $y_{\varphi(n)}^2 \to \ell \neq 2\sqrt{e}$ we deduce that $y_{\varphi(n)+1}^2-y_{\varphi(n)}^2 \geqslant \frac{C}{n}$ but from there I am not able to derive any contradiction ...","['convergence-divergence', 'sequences-and-series']"
3836003,Can I conclude that $x=f(x)$ from the assumption that $f(x)=f(f(x))$?,"If given $$f(x)=f(f(x))$$ would it be correct to cancel one function on both sides and end up with $$x=f(x)$$ if $f(x)$ is an injective function? If saying this is incorrect, please could someone explain why? Sorry if this is a stupid question but I can't find an answer elsewhere.","['elementary-set-theory', 'functions']"
3836044,Openness of holomorphic maps,"Chapter II, Theorem 1.18 of Several complex variables VII: Sheaf theoretical methods by Grauert, Peternell and Remmert states: Let $f: X \to Y$ be holomorphic, and assume that $Y$ is locally irreducible. Then the equation \begin{equation}\dim_x X = \dim_{f(x)}Y + \dim_x X_{f(x)}\end{equation} holds for all points $x \in X$ if and only if the map is open. Now consider for example a blow-up $f: X \to \mathbb{A}^2$ of $\mathbb{A}^2$ in $0$ , i.e. the special fiber over $0$ is a $\mathbb{P}^1$ , and $X \setminus \mathbb{P}^1 \to \mathbb{A}^2 \setminus 0$ is an isomorphism. Then for any point $x \in \mathbb{P}^1$ , the criterion of the theorem is not satisfied, as $\dim_x X = 2$ , $\dim_{f(x)} \mathbb{A}^2 = 2$ and $\dim_x X_{f(x)} = \dim_x \mathbb{P}^1 = 1$ . But I don't believe that $f$ is not open: If $U \subset X$ is open and does not meet $\mathbb{P}^1$ , then clearly $f(U)$ is open. So assume $U \cap \mathbb{P}^1 \not = \emptyset $ . Then $f(U \setminus \mathbb{P}^1) \subset \mathbb{A}^2 \setminus 0$ is open, and $f(U \cap \mathbb{P}^1) = \{0\}$ . So if $x \in U \cap \mathbb{P}^1$ , one can choose a small ball $V \subset U$ around $x$ , which maps to a small ball in $\mathbb{A}^2$ around $0$ . Hence $0$ is contained in the interior of $f(U)$ , and so $f$ is open. What did I not understand here?","['complex-analysis', 'complex-geometry', 'algebraic-geometry']"
3836059,How to determine a sufficient statistic for a Poisson sample and show that it has a monotone likelihood ratio.,"The following question is a last year's Statistics exam question I tried to solve (without any luck). Any help would be grateful. Thanks in advance. An Atomic Energy Agency is worried that a particular nuclear plant has leaked radio-active material. They do $5$ independent Geiger counter measurements in the direct neighbourhood of the reactor. They find the following measurements (per unit time): observation i: 1 2 3 4 5 count $x_i$ : 1 2 6 2 7 (I did not know how to implement this into a tabular) The natural background radiation has an average of $λ = 2$ (per unit time). The agency would only be worried if the radiation rate would be in the order of $λ = 5$ . They therefore decide to test: $H_0 : λ ≤ 2$ versus : $H_1 : λ > 2$ They want to device the optimal test to see if there there is any reason for alarm.
Assuming that the data are realizations of a sample from a Poisson distribution: $X_1, ..., X_5 ∼ POI(λ)$ with density: $f(x) = e^{-λ}\frac{λ^{x}}{x!}$ I have two questions I need some help with: Determine a sufficient statistic for the Possion sample and show that it has a monotone likelihood ratio. Derive the uniform most powerful test of level $α = 0.0487$ for the test problem. Because we have a Poisson distribution, I know that we can use: $\sum_{i = 1}^{5}X_i \sim Poi(5λ)$ For the first question, my attempt: $p(x_1,...,x_5|λ) = \prod_{i = 1}^{5} e^{-5λ}\frac{λ^{x_1 + x_2 + x_3 + x_4 + x_5}}{x_1!x_2!x_3!x_4!x_5!} = h(x_1 +...+ x_5|λ) * g(x_1,x_2,x_3,x_4,x_5) $ $h(x_1 +...+ x_5|λ) = e^{-5λ}λ^{x_1 + x_2 + x_3 + x_4 + x_5} $ $g(x_1,x_2,x_3,x_4,x_5) = \frac{1}{x_1!x_2!x_3!x_4!x_5!}$ It follows by the factorization theorem that $T(X_1, X_2, X_3,X_4,X_5) = X_1+X_2+X_3+X_4+X_5$ is sufficient statistic. Not sure how to construct a proof to show it has a monotone likelihood ratio.","['statistics', 'probability-distributions', 'hypothesis-testing']"
3836064,"Determine convergence of the sequence $x_0=1 , x_{n+1}=x_n (1+ 2^{-(n+1)})$","I want to check if the following sequence converges: $$x_0=1 , x_{n+1}=x_n \left(1+ \frac{1}{2^{n+1}}\right)$$ I proved the sequence is increasing : $\cfrac{x_{n+1}}{x_n}=1+ \cfrac{1}{2^{n+1}} \gt 1$ Now I should prove it is bounded above. let's write some terms of the equation: \begin{align}
x_0&=1 \\[2ex]
x_1&=1\cdot\left(1+\dfrac{1}{2^1}\right)\\[2ex]
x_2&=\left(1+\dfrac{1}{2^1}\right)\cdot\left(1+ \dfrac{1}{2^2}\right)\\[2ex]
x_3&=\left(1+\dfrac{1}{2^1}\right)\cdot\left(1+ \dfrac{1}{2^2}\right)\cdot\left(1+ \dfrac{1}{2^3}\right)\\[2ex]
\end{align} So, we can write: $$x_{n+1}=\left(1+\frac{1}{2^1}\right)\cdot\left(1+ \frac{1}{2^2}\right)\cdots\left(1+\frac{1}{2^{n}}\right)\cdot\left(1+\frac{1}{2^{n+1}}\right)$$ Here, I'm not sure how to prove it is bounded above.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3836076,Find value of $\sin x-\frac{1}{\cot x}$,"If $\sin x+\frac{1}{\cot x}=3$ , calculate the value of $\sin x-\frac{1}{\cot x}$ Please kindly help me Let $\sin x -\frac{1}{\cot x}=t$ Then, $$\sin x= \frac{3+t}{2}, \cot x= \frac{2}{3-t}$$ By using $$1+\cot ^2x= \frac{1}{\sin^2 x}$$ Then, the equation $$t^4-18t^2+48t+81=0$$","['nested-radicals', 'roots', 'polynomials', 'trigonometry', 'quartics']"
3836082,"Showing if $f$ is Borel measurable and $B$ is a Borel set, then $f^{-1}(B)$ is a Borel set.","The following problem is from Royden & Fitzpatrick (4 ed.). I am stuck on showing (ii), can someone please help me prove it? Thank you. $\def\R{{\mathbb R}}$ Page 59, problem 8. (Borel measurability) A function $f$ is said to be $\textbf{Borel measurable}$ provided its domain $E$ is a Borel set and for each $c,$ the set $\{x\in E | f(x) > c\}$ is a Borel set. Verify that Proposition 1 and Theorem 6 remain valid if we replace ""(Lebesgue) measurable set"" by ""Borel set."" Show that: (i) every Borel measurable function is Lebesgue measurable; (ii) if $f$ is Borel measurable and $B$ is a Borel set, then $f^{-1}(B)$ is a Borel set; (iii) if $f$ and $g$ are Borel measurable, so is $f\circ g;$ and (iv) if $f$ is Borel measurable and $g$ is Lebesgue measurable, then $f\circ g$ is Lebesgue measurable. $\textit{Proof.}$ Every Borel measurable set is Lebesgue measurable since $B\in B(\R),$ then $B$ is a Lebesgue measurable set except perhaps on a set of measure $0.$ For (iii), assume $g: \mathbb{R} \to \mathbb{R}$ and $f: \mathbb{R} \to \mathbb{R}.$ Then, $(f\circ g)^{-1}((c,\infty)) = g^{-1}\circ f^{-1} ((c,\infty)).$ By the hypothesis, $f^{-1}((c,\infty)) = B\in B(\R).$ By definition of Borel set, any member of $B(\R)$ is the result of countable set operations or a member of the topology on $\R.$ Any member of the topology on $\R$ may be written as the countable result of set operations on $(a,\infty)$ for some $a\in \R,$ so $g^{-1}(B) \in B(\R).$ Thus, $f\circ g$ is Borel measurable. Now to prove (iv), assume $f: (X,T) \to (\R,U)$ with $(X,T)$ a general topological space, and $U$ the standard topology on $\R.$ By definition, any Borel set $B\in B(\R)$ is a result of countable set operations as an open set. Now given that $f^{-1}((c,\infty)) \in B(x),$ any open set may be written in terms of open rays and any Borel set in $\R$ can be written in terms of these open sets. Hence, the inverse image of a Borel set in $\R$ is the countable set theoretic result of operations on $f^{-1}((c,\infty))$ which is a Borel set as $B(x)$ is a $\sigma$ -algebra.","['measure-theory', 'lebesgue-measure', 'borel-measures', 'real-analysis', 'borel-sets']"
3836112,Proving stationary points of inflection,"Edit For the purposes of proving the statement below, a stationary point of inflection of a curve shall be defined as a point on the curve where the curve changes concavity. Problem Suppose $f(x)$ is $k$ times differentiable with $k \mod 2 \equiv 1$ and $k \geq 3$ . Then, if $f^{(n)}(c) = 0$ for $n = 1, ..., k - 1$ and $f^{(k)}(c) \neq 0$ , prove that $c$ is a stationary point of inflection of $f$ . I have successfully proven the cases where $k = 3$ and $k = 5$ (or so I think) and I am currently trying to devise a proof for the general case above. I am trying to use the ideas from my two proofs (they are largely based on the second derivative test) and am thinking along the lines of induction, but I am not sure if that is wise. Any suggestions/hints/help will be greatly appreciated! As I am not so well-versed in mathematical proof-writing as I would like to be, I am also providing my proofs for the $k = 3$ and $k = 5$ , so that the community may critique them for me! Proof for $k = 3$ Suppose $f^{(3)}(c) > 0$ $\because f^{(3)}(c) = \lim \limits_{x \to c} \frac {f^{(2)}(x) - f^{(2)}(c)} {x - c} = \lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c}$ $\therefore \lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c} > 0$ When $x \rightarrow c^+$ , $x > c$ For $\lim \limits_{x \to c^+} \frac {f^{(2)}(x)} {x - c} > 0$ , $f^{(2)}(x) > 0$ When $x \rightarrow c^-$ , $x < c$ For $\lim \limits_{x \to c^-} \frac {f^{(2)}(x)} {x - c} > 0$ , $f^{(2)}(x) < 0$ $\because f^{(2)}(x)$ changes sign at $c$ $\therefore f$ changes concavity at $c$ $\implies$ By definition, $c$ is a stationary point of inflection of $f(x)$ Similarly, if $f^{(3)}(x) < 0$ , then $\lim \limits_{x \to c} \frac {f^{(2)}(x)} {x - c} < 0$ When $x \rightarrow c^+$ For $\lim \limits_{x \to c^+} \frac {f^{(2)}(x)} {x - c} < 0$ , $f^{(2)}(x) < 0$ When $x \rightarrow c^-$ For $\lim \limits_{x \to c^-} \frac {f^{(2)}(x)} {x - c} < 0$ , $f^{(2)}(x) > 0$ $\because f^{(2)}(x)$ changes sign at $c$ $\therefore f$ changes concavity at $c$ $\implies$ By definition, $c$ is a stationary point of inflection of $f(x)$ To conclude, suppose $f(x)$ is $3$ times differentiable. If $f^{(n)}(c) = 0$ for $n = 1, 2$ and $f^{(3)}(c) \neq 0$ , then $c$ is a stationary point of inflection of $f$ . Proof for $k = 5$ Suppose $f^{(5)}(c) > 0$ Let $g(x) = f^{(3)}(x)$ $\because g^{(1)}(c) = 0$ and $g^{(2)}(c) > 0$ $\therefore g(x)$ has a minimum at $c$ $\because g(c) = 0$ $\therefore$ for all $x$ near $c$ , $g(x) > 0$ $\implies f^{(2)}(x)$ is an increasing function near $c$ In particular, when $x \rightarrow c^-$ , $f^{(2)}(x) < f^{(2)}(c)$ and when $x \rightarrow c^+$ , $f^{(2)}(x) > f^{(2)}(c)$ $\because f^{(2)}(c) = 0$ $\therefore f^{(2)}(x)$ changes sign at $c$ $\implies f(x)$ changes concavity at $c$ $\therefore$ By definition, $c$ is a stationary point of inflection of $f(x)$ Similarly, if $f^{(5)}(c) < 0$ , then $g(x)$ has a maximum at $c$ $\because g(c) = 0$ $\therefore$ for all $x$ near $c$ , $g(x) < 0$ $\implies f^{(2)}(x)$ is a decreasing function near $c$ In particular, when $x \rightarrow c^-$ , $f^{(2)}(x) > f^{(2)}(c)$ and when $x \rightarrow c^+$ , $f^{(2)}(x) < f^{(2)}(c)$ $\because f^{(2)}(c) = 0$ $\therefore f^{(2)}(x)$ changes sign at $c$ $\implies f(x)$ changes concavity at $c$ $\therefore$ By definition, $c$ is a stationary point of inflection of $f(x)$ To conclude, suppose $f(x)$ is $5$ times differentiable. If $f^{(n)}(c) = 0$ for $n = 1, ..., 4$ and $f^{(5)}(c) \neq 0$ , then $c$ is a stationary point of inflection of $f$ . Having been able to come up with these two proofs largely by myself, with some help from my professor, I am actually quite excited on trying a proof for the general case where I am leaning towards induction (actually, it is the only form I can think of), but as my ideas for $k = 3$ and $k = 5$ are not exactly identical, I am not sure if induction is the way to go. I am also trying to stick to second derivative tests (or something of similar difficulty) as I am currently only taking an introductory calculus module at university, so I do not have such ""high-powered"" tools at my disposal, such as Taylor's Series/Theorem and the likes of it. Also, apologies for the lengthy post! Edit 2 Proof for the general case (Many thanks to John Hughes for the guidance) Let $g(x) = f(x + c) - f(c)$ $\implies g(0) = 0$ and $g^{(k)}(0) = f^{(k)}(c)$ Then, it suffices to prove that, if $0$ is a stationary point of inflection of $g$ , $c$ will be a stationary point of inflection of $f$ . Suppose $g^{(3)}(0) > 0$ $\because g^{(3)}(c) = \lim \limits_{x \to 0} \frac {g^{(2)}(x) - g^{(2)}(0)} {x - 0} = \lim \limits_{x \to 0} \frac {g^{(2)}(x)} {x}$ $\therefore \lim \limits_{x \to 0} \frac {g^{(2)}(0)} {x} > 0$ When $x \rightarrow 0^+$ , $x > 0$ For $\lim \limits_{x \to 0^+} \frac {g^{(2)}(x)} {x} > 0$ , $g^{(2)}(x) > 0$ $\because g^{(2)}(x) > 0$ for some $x \in (0, b)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) > 0$ for some $x \in (c, b + c)$ When $x \rightarrow 0^-$ , $x < 0$ For $\lim \limits_{x \to 0^-} \frac {g^{(2)}(x)} {x} > 0$ , $g^{(2)}(x) < 0$ $\because g^{(2)}(x) < 0$ for some $x \in (-b, 0)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) < 0$ for some $x \in (-b + c, c)$ $\implies f^{(2)}$ changes sign near $c$ $\implies f$ changes concavity at $c$ $\therefore c$ is a stationary point of inflection of $f$ Similarly, if $g^{(3)}(0) < 0$ , then $\lim \limits_{x \to 0} \frac {g^{(2)}(x)} {x} < 0$ When $x \rightarrow 0^+$ For $\lim \limits_{x \to 0^+} \frac {g^{(2)}(x)} {x} < 0$ , $g^{(2)}(x) < 0$ $\because g^{(2)}(x) < 0$ for some $x \in (0, b)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) < 0$ for some $x \in (c, b + c)$ When $x \rightarrow 0^-$ For $\lim \limits_{x \to 0^-} \frac {g^{(2)}(x)} {x} < 0$ , $g^{(2)}(x) > 0$ $\because g^{(2)}(x) > 0$ for some $x \in (-b, 0)$ and $f^{(2)}(x) = g^{(2)}(x - c)$ , $\therefore f^{(2)}(x) > 0$ for some $x \in (-b + c, c)$ $\implies f^{(2)}$ changes sign near $c$ $\implies f$ changes concavity at $c$ $\therefore c$ is a stationary point of inflection of $f$ To conclude, suppose $f(x)$ is $k$ times differentiable with $k \mod 2 \equiv 1$ and $k \geq 3$ . If $f^{(n)}(c) = 0$ for $n = 1, ..., k - 1$ and $f^{(k)}(c) \neq 0$ , then $c$ is a stationary point of inflection of $f$ .","['calculus', 'derivatives', 'stationary-point', 'real-analysis']"
3836136,Generalisation of formula for area of triangle in determinant form?,"It is well known that the area of triangle in the Euclidean plane is given by the formula $$A = \dfrac 1 2 {\left| \begin{vmatrix}
x_1 & y_1 & 1 \\
x_2 & y_2 & 1 \\
x_3 & y_3 & 1 \\
\end{vmatrix} \right|},$$ where $(x_i, y_i)$ are the coordinates of the three vertices of the triangle. I was wondering if this admits a generalisation to higher dimensions, since the standard proof of this formula ( something along the lines of this ) seems to result in a determinant almost accidentally. For example, might the volume of a tetrahedron be given by the following? $$A = \dfrac 1 2 {\left| \begin{vmatrix}
x_1 & y_1 & z_1 & 1 \\
x_2 & y_2 & z_2 & 1 \\
x_3 & y_3 & z_3 & 1 \\
x_4 & y_4 & z_4 & 1 \\
\end{vmatrix} \right|}.$$ I suspect this is too naive a generalisation, but I'd be curious how you generalise this determinant formula anyway, if possible.",['geometry']
3836138,Help finding the limit of $\lim_{n \to \infty}\prod_{k=1}^{n}\left(1+\frac{1}{n}f\left(\frac{k}{n}\right)\right)$.,"Given that $f:[0,1]\to \Bbb{R}$ is a continuous function, I need to show that $$\lim_{n \to \infty}\prod_{k=1}^{n}\left(1+\frac{1}{n}f\left(\frac{k}{n}\right)\right)=e^{\int_0^1f(x)dx}$$ Writing $$y=\lim_{n \to \infty}\prod_{k=1}^{n}\left(1+\frac{1}{n}f\left(\frac{k}{n}\right)\right)$$ and taking the logarithm of both sides and interchanging limit and logarithm(since $f$ is continuous), I get $$\log y = \lim_{n\to \infty}\sum_{k=1}^{n}\log\left(1+\frac{1}{n}f\left(\frac{k}{n}\right)\right)$$ Now I know that the limit of the Riemann sum $\lim_{n\to \infty}\sum_{k=1}^{n}\frac{b-a}{n}f\left(a+\frac{(b-a)}{n}k\right)=\int_a^bf(x)dx$ . However if I want it to read $\int_0^1f(x)dx$ in the given problem, I would need the term inside the summation to read $\frac{1}{n}f\left(\frac{k}{n}\right)$ . The way to do this it would seem is to take the series expansion of $\log(1+x)$ and ignoring the $x^2$ and higher order terms. Is it justifiable to do so?","['limits', 'calculus']"
3836178,Time derivative of heat semigroup.,"Imagine we have the heat semigroup $\{P_t\}_{0\leq t\leq T}$ and remember that we have $$\frac{d}{dt} (P_t\varphi)(x)=\frac 1 2 (P_t\varphi '')(x).$$ I want to calculate following time derivative $$\frac{d}{dt} P_t\varphi (\int_0^t f(u)du).$$ The problem is that the argument is also time dependent, and honestly I am having some difficulties to see why this should equal $$\frac 1 2 (P_t\varphi '')(\int_0^t f(u)du)+P_t\varphi'(\int_0^t f(u)du)f(t)$$ This seems pretty much a product rule for derivatives, but this is not a product but the action of an operator.
I apologize if this is rather trivial but my brain seems to refuse to understand it. I would appreciate any help. EDIT:  Could this be due the fact that $P_t$ is a linear operator, and an application of Riesz Theorem?","['semigroup-of-operators', 'derivatives', 'functional-analysis', 'heat-equation']"
3836199,"Limit $\lim_{(x,y)\to\infty} e^{-e^{xy}}$ with polar coordinates","can i use polar to solve this limit? $$\lim_{(x,y)\to\infty} e^{-e^{xy}}=$$ $$\frac{1}{e^{e^{r^2\cos\theta\sin \theta}}}=$$ but i'm quite stuck here i think the denominator goes to infinity but should i show that? or can i just write $0$ as a solution after the step above like $$\frac{1}{e^{e^{r^2\cos\theta\sin \theta}}}=0$$ is it correct?
any help or suggestion would be very helpful
Thank's in advance","['limits-without-lhopital', 'multivariable-calculus', 'calculus', 'polar-coordinates', 'limits']"
3836200,Mapping a single uniform random variable to a uniform product random variable,"I have a single random variable $X\sim \operatorname{Uniform}(0,1)$ . I would like a function $f: \mathbb{R} \to \mathbb{R}$ such that $F = f(X)$ has the distribution \begin{equation}
p_F(y) = \begin{cases}
{(-\log y)^n \over n!} & \text{if } 0 < y \leq 1 \\
0 & \text{otherwise}
\end{cases}
\end{equation} for some given $n$ (i.e., the product distribution for $n+1$ independent random variables). Does such an $f$ exist? If so what is it? I tried reverse-engineering one directly but became hopelessly tangled in incomplete gamma functions and the like. Thanks!","['statistics', 'probability-distributions', 'probability']"
3836218,Define $1+x+x^2+\cdots+x^n$ recursively,"Let $x \neq 1$ be a real number. Define the following sum recursively: $$
1+x+x^2+\cdots+x^n
$$ My attempt: Using summation notation we can write the sum as $\sum_{i=0}^{n} x^i$ . Now define $$
\sum_{i=0}^{0}x^i:=x^0=1 \quad \text{and} \quad \sum_{i=0}^{n}x^i:=\sum_{i=0}^{n-1}x^i+x^n
$$ It seems to me correct because if we use the definition for $n=3$ , then $$
\sum_{i=0}^{3}x^i=\sum_{i=0}^{2}x^i+x^3=(\sum_{i=0}^{1}x^i+x^2)+x^3=((\sum_{i=0}^{0}x^i+x)+x^2)+x^3=1+x+x^2+x^3
$$ In other words, we reach the base case. However, I'm unsure if this is correct. I might have misunderstood what a recursive definition is.","['algebra-precalculus', 'solution-verification', 'discrete-mathematics', 'recursion']"
3836296,$T:V→V$ is a linear transformation such that $T\circ T(x)$ is invertible. Prove that $T$ is also invertible.,"Let $V$ be a vector space with dimension $n\in\mathbb{N}$ and $T:V→V$ a linear transformation such that $T\circ T(x)$ is invertible. Prove that $T$ is also invertible. I'm thinking to use the Theorem that states: If $T:V→W$ is an invertible linear transformation with inverse $T^{-1}:W→V$ , then $T^{-1}$ is a linear transformation. Any tips on how I should go about this problem?","['inverse', 'linear-algebra', 'linear-transformations']"
3836337,"Given a finite set of points and a finite coloring of the plane, is there a monochromatic set of points of the plane similar with the first one?","Two sets of points of the plane $A$ and $B$ are said to be similar iff there is a bijection $f: A\rightarrow B$ and a constant $c_f\in\Bbb{R}^*_+$ such that $\forall X, Y\in A: \overline{XY} = c_f\cdot \overline{f(X)f(Y)}$ , so, for example, every two regular $n$ -agons are similar. Let $n\in\Bbb{N}$ and $S$ be a finite set of points of the plane. Every point of the plane is colored with one of $n$ colors. Is there a monochromatic set $S'$ of points of the plane similar with $S$ ? I know already that the statement is true for any $n$ if $|S|\le 3$ (if $|S| = 1$ or $2$ the statement is trivial; if $|S| = 3$ , a small adaptation of the solution to this problem along with induction in $n$ will do the job). I believe that the statement is true for any $n$ if $|S| = 4$ , but I was not able to prove it. What I'm interested in, in fact, is finding a counter example: For what finite coloring of the plane and what finite set of points of the plane is the statement false? How can we construct this set and this coloring?","['ramsey-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
3836350,Class Group of the Class Number $3$ with their elements given explicitly,"INTRODUCTION The  two-faced behavior of quadratic form $x^2 + 5y^2$ has a hidden companion - the quadratic form $2x^2 + 2xy + 3y^2$ - whose prime values are of the form $20n + 3$ or $20n +7$ (determinant $5$ , has two equivalence classes, or class number $2$ , irregular behavior). If we denote the form $x^2 +5y^2$ by $A$ and the form $2x^2 + 2xy + 3y^2$ by $B$ , then
Lagrange's results (combined with Brahmagupta's) say that the composites
of $A$ and $B$ have the following ""multiplication table"": $$A^2 =A, AB=BA =B, B^2 =A.$$ We recognize this as the multiplication table for the two-element group
with identity element $A$ . Today it is called the class group for $\mathbb Q(\sqrt-5)$ . WHAT I AM LOOKING FOR: A list of quadratic forms with their equivalence classes $A, B, C$ , i.e. the class group of the class number $3$ with their elements given explicitly. I came to know that, there are $25$ such quadratic forms from the paper ""Representation of primes by the principal form of $-D$ when the Class-number $h(- D)$ is $3$ "" . But due to my lacking of technical knowledge in this topic, I can not find the equivalence classes for a specific quadratic form. probably the following theorem says something about what I am looking - But I can not decode it, can anyone plz decode it for me, in general if there is  list of quadratic forms with their equivalence classes $A, B, C$ ,in the above paper, can anyone translate that into an elementary way (like the INTRODUCTION )? Thanks. EDIT A deleted answer: The proposition p.132 gives you the list of values of $D$ such that $h(-D)=3$ .
So for each $D$ , you have to find the corresponding list of reduced forms. Now $ax^2+bxy+cy^2$ is reduced of discriminant $-D$ if $a,b,c$ are coprime , $|b|\leq a\leq c$ and $b\geq 0\text{ if either }|b|=a\text{ or }a=c$ , and of course $b^2-4ac=-D$ . The last condition easily implies that $a\leq \sqrt{D/3}$ ,   so you just have to solve for each value of $D$ by trial and error the finitely many possible values for $a$ abd $b$ (and then $c$ )or by programming your favorite CAS.","['algebraic-number-theory', 'number-theory', 'abstract-algebra', 'class-field-theory', 'quadratic-forms']"
3836377,How to solve for unknown variables in a matrix?,"How do I solve this problem? Ideally, I think $$a^2-4=1$$ is the best option to solve for a I set the entire expression equal to 1. Then to solve for b I just put $b=1$ . Is this right? I feel like that's really wrong. So update I got this I suppose my answer is just b does not equal 0 and a does not equal +- sqrt 4",['matrices']
3836415,"Evaluating $\int_{0}^{\pi}\ln (1+b\cos x)\ \text{d}x$, $b$ is a parameter","Evaluating $\int_{0}^{\pi}\ln (1+b\cos x)\ \text{d}x$ where $b$ is a parameter I've tried Integration by parts which yield $$\int_{0}^{\pi}\ln (1+b\cos x)\ \text{d}x=\pi\ln(1-b)+b\int_{0}^{\pi}{x\sin x\over 1+b\cos x}\text{d}x$$ I cannot figure out what do next. I also tried using Leibniz integral rule by putting $I(b)=\int_{0}^{\pi}\ln (1+b\cos x)\ \text{d}x$ to form a differential equation. $${\text{d}I(b)\over \text{d}b}=\int_{0}^{\pi}{\cos x\over 1+b\cos x}\text{d}x$$ but I'm not able to solve the integral on right. I've looked similar questions like this one Evaluating $\int_{0}^{\pi}\ln (1+\cos x)\, dx$ to no avail. Also I'm high school student so I don't understand advanced calculus stuff yet.","['integration', 'definite-integrals']"
3836436,A collection of pairwise disjoint events with nonzero probability must be countable,"I'm trying to figure this problem out, and I keep getting stuck. I was hoping someone could help me get started: Let $(A)_{\beta \in B}$ be a collection of pairwise disjoint events. Show that if $P(A_{\beta}) > 0$ for each $\beta$ , then $B$ is either finite or countable. I started by assuming that $B$ was uncountable by way of contradiction. Then I took a countable subset $B_0\subseteq B$ and observed that $P(\bigcup_{\beta \in B} A_\beta) = \sum_{\beta \in B} P(A_\beta) > 0$ , but I don't know where to go from here.","['measure-theory', 'probability-theory']"
3836437,Number of integral values $\big(f'(x)\big)^2$ can take?,"For a twice differentiable function $f(x)$ , we have $\big|f(x)\big|\leq 3 \;\forall \; x \in \mathbb R$ and for some $\alpha$ , we have $f(\alpha)+\bigl(f'(\alpha)\bigr)^2=80$ . Find the number of integral values $\big(f'(x)\big)^2$ can take in $(0,77).$ Now I used Lagrange's Mean Value theorem that there exists $c\in(0,3)$ such that $\big|f'(c)\big|=\bigg|\frac{f(3)-f(0)}{3-0}\bigg|$ . Now as $\big|f(x)\big|\leq 3 \;\forall \; x \in R$ , hence maximum value of $\big|f(3)-f(0)\big|$ can be 6, hence $\big|f'(c)\big|\leq2$ but I am not able to use information $f(\alpha)+\bigl(f'(\alpha)\bigr)^2=80$ . How to proceed in this. Given answer is $76$ .","['calculus', 'derivatives', 'real-analysis']"
3836556,Can two independent events be disjoint?,"If events $A$ and $B$ both have positive probabilities, and if they are disjoint, they surely cannot be independent since: $$\text{disjoint:}\quad P(A \text{ intersection } B) = 0 \iff P(A \text{ union }B) = P(A) + P(B),$$ $$\text{independent:}\quad\quad\quad\quad P(A \text{ intersection } B) = P(A)P(B).$$ so if P(A intersection B) is 0, then $P(A)P(B)$ should be 0 too, but since they're both above 0, then this is false. However I am not sure if that is the case the other way around, I cannot put my head around the question if two independent events can be disjoint.
Can anyone help? Thanks in advance.","['independence', 'probability']"
3836576,Compute the limit $\lim\limits_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin tx} $,"I have been working on this limit for days, but I am not getting it. The question is Compute the limit $$\lim_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (tx)}$$ Note that the integral is well defined and convergent for every $t >0$ . Indeed the integrand function is a positive function for every $t >0$ since $$e^x + \sin tx > e^x-1 > x>0$$ And as $x \to + \infty$ the integrand function behaves like $e^{-x}$ . WHAT I TRIED: I consider $t=2n \pi$ a multiple of $2 \pi$ , and see what happens: $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)} = \sum_{k=0}^\infty \int_{k /n}^{(k+1) /n} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)}$$ Making the change of variables $u = 2n \pi x$ I get \begin{align}\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{u/2n \pi}+ \sin (u)} &\ge
\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(2k+2) \pi/2n \pi}+ \sin (u)} \\&=
\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(k+1)/n}+ \sin (u)}\end{align} where I write the lower bound with the minimum of the function at $u=(2k+2) \pi$ . Now I use the fact that the integrand function does is integrated over a period of $2 \pi$ , and using the result for $C>1$ $$\int_0^{2 \pi} \frac{ \mathrm d u}{C+ \sin (u)} = \frac{2 \pi}{\sqrt{C^2-1}}$$ I get the estimate \begin{align}\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(k+1)/n}+ \sin (u)} &= \sum_{k=0}^\infty \frac{1}{2n \pi} \frac{2 \pi}{\sqrt{e^{2(k+1)/n} -1 }} \\&= \frac{1}{n} \sum_{k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}\end{align} Summing all up, I got that $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)} \ge \frac{1}{n} \sum_{k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}$$ As $n \to \infty$ the series converges to the Riemann integral $$\int_0^{+ \infty} \frac{\mathrm d y}{\sqrt{e^{2y}-1}} = \frac{\pi}{2}$$ Hence the limit should be a number larger than $\pi/2$ , or $+ \infty$ . Using WA I got for large values of $t$ that the integral is between $1$ and $2$ , thus $\pi/2$ could be the actual limit.","['integration', 'limits', 'calculus', 'definite-integrals']"
3836580,Identifying the stalk of an integral scheme at the generic point,"Let $X$ be an integral scheme. Then since $X$ is irreducible it has a generic point $\eta$ . Suppose $\operatorname{Spec}A$ is an nonempty affine open subset of $X$ . Then $\eta$ is also the unique generic point of $\operatorname{Spec}A$ . I would like to understand in what way the stalk at $\eta$ , $\mathcal{O}_{X,\eta}$ , is identified with $K(A)$ , the fraction field of $A$ . Since $\operatorname{Spec}A$ is nonempty, I know that $A\neq (0)$ . Further, since $X$ is integral, $\operatorname{Spec}A$ is also integral, and it follows that $A$ is an integral domain. So $(0)$ is a prime ideal, and is contained in every other prime ideal of $A$ . I am not really sure where to go from here. What am I missing? I also know that the unique generic point of $\operatorname{Spec}A$ is the nilradical of $A$ , the intersection of all prime ideals of $A$ , of which $(0)$ is an element. Further, the nilradical is prime since $\operatorname{Spec}A$ is irreducible. Do we get that $(0)$ is the generic point of $\operatorname{Spec}A$ since $V((0))=\operatorname{Spec}A$ , and hence $(0)$ is the generic point of $X$ ?","['algebraic-geometry', 'abstract-algebra', 'schemes']"
3836634,"Proving that for any three integers $a,b,c$ there exists a positive integer $n$ such that $\sqrt{n^3+an^2+bn+c}$ is not an integer","Prove that for any three integers $a,b,c$ there exists a positive integer $n$ such that $\sqrt{n^3+an^2+bn+c}$ is not an integer. In order to solve this problem I have tried looking at the expression under the radical modulo n. Thus I want to find n such that c is a quadratic non-residue modulo n. For example, if c = 2 (mod 3), since 2 is a non-residue mod 3 we may take n to be 3 and thus the expression is never a perfect square. I need a way to do this for arbitrary c, which I could not find on my own.","['contest-math', 'number-theory', 'algebra-precalculus']"
3836654,"Proof that $|HK|=|H||K|/|H \cap K|$ for $H,K,HK$ subgroups of $G$","I've found this problem in a book and devised my own proof.
(took me like 5 hours and it seems trivial - just build a bijection).
I am not sure that I haven't made any errors though.
Perhaps some other proof would be simpler. Statement: $G$ is a group and $H,K,HK \subseteq G$ Prove that $|HK|=\frac{|H||K|}{|H \cap K|}$ Proof: The statement above is equivalent to: $\frac{|HK|}{|K|}=\frac{|H|}{|H \cap K|}$ So now we look at the cosets of $K$ in $HK$ - ie. the elements of $HK/K$ .
They are exactly $\frac{|HK|}{|K|}$ because of Lagrange's theorem. Then we look at the cosets of $H \cap K$ in $H$ - ie. the elements of $H/H \cap K$ .
They are exactly $\frac{|H|}{|H \cap K|}$ because of Lagrange's theorem. So if we could find a bijection from $HK/K$ to $H/H \cap K$ , we are done. So let's look at the elements of $HK/K$ , they are cosets of the form: $h_1k_1K$ ,
but $k_1K=K$ , so we get $h_1K$ . Then lets look at the elements of $H/H \cap K$ ,
they are cosets of the form: $h_1H \cap K$ . So let's define $f: HK/K \to H/H \cap K$ , $f(hK)=hH \cap K$ . To see that it's a function we need to show that it's well defined. Let $h_1K=h_2K$ , ie. $h_2^{-1}h_1 \in K$ which also implies $h_2^{-1}h_1 \in H \cap K$ Then we need to show that $f(h_1K)=f(h_2K)$ . So $f(h_1K)=h_1H \cap K$ and $f(h_2K)=h_2H \cap K$ To prove that $h_1H \cap K = h_2H \cap K$ we need $h_2^{-1}h_1 \in H \cap K$ . But we've already shown that. Hence f is well defined. Now we need to show that it's injective. Suppose $f(h_1K)=f(h_2K)$ , but $h1K \neq h2K$ . Ie. $h_1H \cap K = h_2H \cap K$ but $h_1K \neq h_2K$ . $h_1K \neq h_2K$ implies $h_2^{-1}h_1 \notin K$ which implies $h_2^{-1}h_1 \notin H \cap K$ . Hence $h_1H \cap K \neq h_2H \cap K$ . So we know f is injective. Now to check for surjectivity: Since $H/H \cap K$ has elements of the form $h_1H \cap K$ , for each of them we have, $f(h_1K)=h_1H \cap K$ . QED","['group-theory', 'abstract-algebra', 'solution-verification']"
3836662,Group theory: the study of symmetries?,"I understand basic group theory. I would say that I've seen most of the standard stuff up to, say, the quotient group. I feel like I've seen in more than one place the suggestion that group theory is the study of symmetries, or actions that leave something (approximately) unchanged. Unfortunately I can only find a couple sources. At 0:49 in this 3 Blue 1 Brown video , the narrator says ""[Group theory] is all about codifying the idea of symmetry."" The whole video seems to be infused with the idea that every group represents the symmetry of something. In this video about the Langlands Program, the presenter discusses symmetry as a lead-in to groups beginning around 33:00. I don't know if he actually describes group theory as being about the study of symmetry, but the general attitude seems pretty similar to that of the previous video. This doesn't jive with my intuition very well. I can see perfectly well that part of group theory has to do with symmetries: one only has to consider rotating and flipping a square to see this. But is all of group theory about symmetry? I feel like there must be plenty of groups that have nothing to do with symmetry. Am I wrong?","['group-theory', 'abstract-algebra', 'symmetry', 'intuition']"
3836693,A doubt on Tensors: Can they be 1-form valued?,"A I know a restrictive $-$ though sufficiently general in physics $-$ definition of the Tensor object: A $(p,q)-$ Tensor is a multilinear function like: $$ T: V\times\cdot\cdot\cdot\times V\times V^{*}\times\cdot\cdot\cdot\times V^{*} \to \mathbb{K} \tag{1}$$ But a guy told me something astonishing, and something that I never encountered before. He said that we can define a tensor object such that the field $\mathbb{K}$ can be replaced by, for example, $\mathbb{K}^{n}$ , or even $V^{*}$ . In the latter case he said: A $1-$ Form Valued Tensor . So I would like to know: What exactly is an object like: $$ T: V\times\cdot\cdot\cdot\times V\times V^{*}\times\cdot\cdot\cdot\times V^{*} \to V^{*} \tag{2}$$ They are tensors?","['multilinear-algebra', 'tensors', 'abstract-algebra', 'differential-geometry']"
3836694,How does $\int \frac{1}{e^x +1} dx$ become $\ln{\big( \frac{1+e^x}{2} \big)} +1 -x $?,I'm solving the equation $(1+e^x)yy'=e^{y}$ with the constraint $y(0)=0$ . I've got pretty far but I'm struggling in the last part. I got stuck solving this integral: $$\int \frac{1}{e^x +1} dx $$ which is $x-\ln{|1+e^x|}$ but in order to satisfy the constraint I need to make it $\ln{\big( \frac{1+e^x}{2} \big)} +1 -x $ . How does $\int \frac{1}{e^x +1} dx$ become $\ln{\big( \frac{1+e^x}{2} \big)} +1 -x $ ?,"['calculus', 'ordinary-differential-equations']"
3836769,"$A\cos(\theta) + B\sin(\theta)$ for complex $A,B$","Does the equation $$A\cos(\theta) + B\sin(\theta) = \sqrt{A^2+B^2}\cos(\theta + \gamma) \label{1} \tag{1}$$ with $\gamma = \arg(A-jB)$ require that $A$ and $B$ be real, or can they be complex? Consider the case $B= jA$ which results in: $$A\cos(\theta) + jA\sin(\theta) = Ae^{j\theta}$$ Using $\ref{1}$ this results in: $$=\sqrt{A^2-A^2}\cos(\theta + arg(2A)) = 0$$ Which appears to confirm the answer is no. So then, is there a unified relationship for $A\cos(\theta) + B\sin(\theta)$ that is closest to the form of \ref{1} and allows $A$ and $B$ to be real, imaginary or complex: $A, B \in \mathbb{C}$ , $\theta \in \mathbb{R}$ (and \ref{1} is just a simplification of this for A, B real)? I got this far toward a geometric solution with two cases with A and B both real and with A real and B imaginary as shown below in case this helps toward the analytic result, along with subsequent more significant progress which I provided as an answer. However I would be very interested in a more concise formulation toward a solution or comments on how the answer I provided may be further simplified (toward the form in \ref{1}). $$A\cos(\theta) + B\sin(\theta)$$ $$= \frac{A}{2}e^{j\theta} + \frac{A}{2}e^{-j\theta} + \frac{B}{2j}e^{j\theta} - \frac{B}{2j}e^{-j\theta}$$ $$= \frac{A}{2}e^{j\theta} + \frac{A}{2}e^{-j\theta} - \frac{jB}{2}e^{j\theta} + \frac{jB}{2}e^{-j\theta}$$ Case with A, B real to confirm known relationship resulting in $A\cos(\theta) + B\sin(\theta) = \sqrt{A^2+B^2}\cos(\theta + \gamma)$ : Case with real A and imaginary B resulting in $\frac{A+jB}{2}\cos(\theta) - jBe^{j\theta}$ :",['trigonometry']
3836846,Prove that $Dim(W) \leq k$,"Let be $V$ a vector space over a field $F$ with finit dimension. Let
be $T$ a lineal operator in $V$ . Supose that the characteristic
polynomial of $T$ , $p(x)$ , is of the form $p(x)=(x-c)^{k}g(x)$ with $k
> \in \mathbb{N}^{+}$ , $c \in F$ and $g(c) \neq 0$ , and consider $W$ the
space of the eigenvectors associated with $c$ . Prove that: $Dim(W) \leq k$ If $Dim(W)<k$ , then $T$ is not diagonizable I'm not sure of how to solve the problem. How can I prove it? I would really appreciate your help!",['linear-algebra']
3836910,The area of infinitely incribed polygons in circles,"What is the area of the white region if the total area is 1? I wrote a code a while ago to try to answer this, and the answer I got was that the area of the white region approached π/10 of the total area. However, to confirm that, I would have needed to let the code run indefinitely to account for all infinity polygons. I was wondering if anyone could prove (or disprove) the area is π/10 rather than relying on code to approximate it. By the way, I came up with this problem almost a year ago, and I showed a friend of mine and he posted it here, but it didn't get a concrete answer. Here's the link to his post: Area of the shaded region of a infinitely circumscribed set of polygons.","['puzzle', 'geometry']"
3836919,Finding the curve on a surface with a specific curvature,"I wish to find a curve $\langle x(t), y(t), z(t) \rangle$ on the surface $x + y + z = (x-y)^2 + (y-z)^2$ whose curvature is 1/2 for all $t$ . I am struggling with how to proceed. I initially tried expanding the right side and attempted to parametrize but because of the $yz$ and $xy$ terms, I wasn't able to do so. I would greatly appreciate any approaches to this problem. Thank you!","['multivariable-calculus', 'calculus', 'differential-geometry']"
3836958,Homotopy between idempotents of small difference,"Let $A$ be a unital $C^*$ -algebra. It is known that if $p$ and $q$ are projections in $A$ with $$\|p-q\|<1,$$ then $p$ and $q$ are homotopic through a path of projections. Question: Does a similar statement hold for idempotents? More precisely, if $e$ and $f$ are idempotents in $A$ , does there exist $\delta>0$ such that $e$ and $f$ are homotopic through idempotents whenever $$\|e-f\|<\delta?$$","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
3836982,"Looking for the Kernel, Basis, Range, and Dimension","$$
L: \mathbb{R}^4 → \mathbb{R}^4 
$$ defined by $$
L \begin{pmatrix}
\begin{bmatrix}
x\\y\\z\\w
\end{bmatrix}
\end{pmatrix}
=
\begin{bmatrix}
1&2&1&3\\
2&1&-1&2\\
1&0&0&-1\\
4&1&-1&0
\end{bmatrix}
\begin{bmatrix}
x\\y\\z\\w
\end{bmatrix}
$$ I'm interested in looking for its kernel Ker(L) , find a basis for Ker(L) and the dimension of Ker(L) . Same as for its range, range(L) and its dimension range(L) . I'm not really confident with my answer so I'm asking so that I could verify if my solution is correct. What I did is equate the matrix above to a zero matrix. $$
\begin{bmatrix}
1&2&1&3\\
2&1&-1&2\\
1&0&0&-1\\
4&1&-1&0
\end{bmatrix}
\begin{bmatrix}
x\\y\\z\\w
\end{bmatrix}
=
\begin{bmatrix}
0\\0\\0\\0
\end{bmatrix}
\Rightarrow
\left[
\begin{array}{cccc|c}
  1&2&1&3&0\\
2&1&-1&2&0\\
1&0&0&-1&0\\
4&1&-1&0&0
\end{array}
\right]
$$ Then I looked for its RREF and the result is $$
\left[
\begin{array}{cccc|c}
  1&0&0&-1&0\\
0&1&0&\frac{8}{3}&0\\
0&0&1&\frac{-4}{3}&0\\
0&0&0&0&0
\end{array}
\right]
$$ and decided the following: $$ \begin{bmatrix}
x = r\\
y = \frac{8}{3}r\\
z = \frac{-4}{3}r\\
w = r \end{bmatrix}
\Rightarrow r
\begin{bmatrix}
1\\
\frac{8}{3}\\
\frac{-4}{3}\\
1 \end{bmatrix}
\Rightarrow \ker(L)
\begin{bmatrix}
1\\
\frac{8}{3}\\
\frac{-4}{3}\\
1 \end{bmatrix}
$$ And from there I'm not sure if I did right, and I'm confused on where next I should go in finding the rest of the problem. I apologize if its a stupid question.","['matrices', 'matrix-equations']"
3837004,Why $\mathcal{S}^n_d$ is a manifold?,Let $\mathcal{S}^n_d$ be the set of all $n \times n$ real symmetric matrices of rank $d$ . How can I prove that $\mathcal{S}^n_d$ is a $dn-\binom{d}{2}$ dimensional manifold?,"['matrices', 'manifolds', 'smooth-manifolds', 'matrix-decomposition']"
3837041,Whats wrong with this argument that $\operatorname{Spec}(\prod A_i) = \bigsqcup\operatorname{Spec}(A_i)$ infinite product.,"We have the spec functor $\text{CRng}^\text{op} \rightarrow \text{Aff}$ . $\DeclareMathOperator{\Spec}{Spec}\DeclareMathOperator{\Hom}{Hom}$ Then $$\Hom _{\text{Aff}}(\Spec(\lim A_i), \Spec B) = \Hom_{\text{CRng}} (B, \lim A_i) $$ $$ = \lim \Hom_{\text{CRng}} (B,A_i) = \lim\Hom_{\text{Aff}}(\Spec A_i, \Spec B) 
$$ $$ =  \Hom_{\text{Aff}}(\text{colim} \Spec(A_i), \Spec B) $$ This means $\Spec(\lim A_i)$ and $\text{colim} \Spec(A_i)$ both represent the same object. But this clearly does not make sense when the colimit is infinite. What went wrong?","['limits-colimits', 'proof-explanation', 'category-theory', 'algebraic-geometry', 'solution-verification']"
3837060,How to compute median without storing all the values?,"I have a source of data that continuously gives me measurement results. At some point I compute the mean. Luckily to this end, I don't need to store all the values, I only store the sum and the number of results, so I can divide one by the other. It is important because of efficiency reason, as I have loads of such sources operating concurrently. The requirement has just changed and I was asked for the median instead of the mean. In order to compute the median, one needs all the values from the data set, which in my case would exhaust computer memory, thus I am thinking of an improvement. The idea I have is storing histograms and approximating medians from them. Nevertheless, creating a histogram object for each data source (which I have millions) might not pay back for sources that produce very few values. In short, I'd appreciate any comments related to median, quartiles and median absolute deviation computations optimization.","['data-analysis', 'statistics', 'median']"
3837080,Integral over decreasing sequence of open sets,"Let $G : T \subset \mathbb{R}^n \rightarrow \mathbb{R}^n$ , $T$ is an open set and suppose that $G$ is a $C^1$ -diffeomorphism in the sense that $G$ is injective and $\det DG(x) \neq 0$ for all $x \in T$ , where $DG(x)$ is the total derivative of $G$ at $x \in T$ . Let $B \subset T$ be a borel set of finite measure and suppose $U_n \subset T$ is a decreasing sequence of open sets, such that $B \subset U_n$ for all $n$ , and $m(U_n \setminus B) < \frac{1}{2^n}$ for all $n$ . In particular we have that $B \subset \cap_{n \geq 1} U_n = U$ and $m(U \setminus B) = 0$ where $m$ is the lebesgue measure. Why is it necessarily the case that dominated convergence theorem can be applied to show that $$\lim_{n \rightarrow \infty} \int_{U_n} |\det DG(x)|dx = \int_{B} |\det DG(x)|dx$$ ? This seems to imply to me that when $\int_{B} |\det DG(x)|dx < \infty$ , $\int_{U_N} |\det DG(x)| dx <\infty$ for some $N \in \mathbb{N}$ (and thus for all $n \geq N$ ) but I have been unable to see why this is true. Edit: For reference, this particular worry came from reading page 73 of Folland (Real Analysis : Modern Techniques and their Applications - 1984) and I have attached the section in that page which I did not understand the argument of : https://i.sstatic.net/a91Mt.jpg","['measure-theory', 'lebesgue-measure', 'determinant', 'lebesgue-integral', 'real-analysis']"
3837090,Interpolation Inequality (Sobolev embedding),"Let $\Omega$ be a $C^1$ domain, for any $\epsilon>0, 0<|\alpha|<k$ , there exists a $C_\epsilon$ such that $$||D^\alpha u||_{L^p}\leq \epsilon||u||_{W^{k,p}}+C_\epsilon||u||_{L^p}$$ for all $u\in W^{k,p}$ . I was trying to prove it by contradiction. $\forall n \in \mathbb{N}, \exists u_n \in W^{k,p}$ such that $||D^\alpha u_n||_{L^p}>\epsilon||u_n||_{W^{k,p}}+n||u_n||_{L^p}$ . Anyone could give me a hint about using Sobolev compactness embedding? I was trying to use the $W^{k,p}\subset \subset L^p$ . However, I do not how to show that the sequence is bounded in $sup$ . By taking normalisation, $v_n=\frac{u_n}{||u_n||_{W^{k,p}}}$ , we would have $$\frac{1}{||u_n||_{W^{k,p}}}||D^\alpha u_n||_{L^p}>\epsilon+\frac{n}{||u_n||_{W^{k,p}}}||u_n||_{L^p}$$","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
3837091,Prove that the boundary and the interior of a manifold are disjoint.,"What shown below is a reference from Analysis on Manifolds by James Munkres. So using the last lemma I ask to prove that the boundary and the interior of a manifold are disjoint. In particular I attempted to show this using reductio ad adsurdum but it seems I didn't be able to do this. Anyway reading the above definition it seems that the boundary and the interior are disjoint by them own definition. Indeed there are two possible cases: either there exist a coordinate patch $\alpha$ about $p$ with domain an open set in $\Bbb R^k$ or there not exist a such coordinate patch. Perhaps is my last argument incorrect? So could someone help me, please?","['multivariable-calculus', 'calculus', 'manifolds', 'general-topology', 'differential-geometry']"
3837104,How should I understand Kodaira dimension?,"Say we have projective variety $X$ .
Its Kodaira dimension $\kappa(X)$ is defined by the “growth exponential” of $P_d := \dim H^0(X,K_X^{\otimes d})$ with respect to $d$ , i.e. $\kappa(X) := -\infty$ if $P_d = 0$ for all $d > 0$ , $\kappa(X) := \min\{k \in \mathbb{Z}\mid \{P_d/d^k\} \text{ is bounded}\}$ I don’t understand how we are supposed to interpret this. From the literal definition, $P_d$ measures the space of sections of $K_X^{\otimes d}$ , which in turns tell you how big a projective space can $X$ effectively embed in, “via the line bundle $K_X^{\otimes d}$ .""
But what does this mean? And what does “growth of $P_d$ "" suppose to mean? Maybe my problem is, I don’t even understand the special role of $K_X$ . Because, one could easily give a definition $\kappa(X;L)$ for any (holomorphic) line bundle over $X$ in a similar manner and compute it. (Is this a thing? And if so, how to understand it? E.g. what if we look at $L$ as the holomorphic tangent (or cotangent) bundle over $X$ ) Thanks for your clarifications! I am an absolute beginner in this, so would appreciate any guidance.","['riemann-surfaces', 'algebraic-geometry', 'line-bundles', 'projective-varieties']"
3837121,Find a mistake in a computation of a determinant.,"I recently have to solve one exercise in which I need to compute the determinant of the matrix $$ A= \begin{pmatrix}
a & 1 & 1 &  1 \\
1 & a & 1 &  1 \\
1 & 1 & a &  1 \\
1 & 1 & 1 &  a \\
\end{pmatrix} ,$$ where $a\in\mathbb{R}$ . This is how I proceed using famous properties of the determinant $$\det A = \begin{vmatrix}
a & 1 & 1 &  1 \\
a-1 & 1-a & 0 &  0 \\
0 & a-1 & 1-a &  0 \\
\textbf{0} & \textbf{0} &\textbf{a-1} & \textbf{1-a}  \\
\end{vmatrix} = (1-a) \begin{vmatrix} a & 1 & 1 \\ a-1 & 1-a & 0 \\ 0 & a-1 & 0 \end{vmatrix} + (1-a)\begin{vmatrix} a & 1 & 1 \\ a-1 & 1-a & 0 \\ 0 & a-1 & 1-a \end{vmatrix} ; $$ $$ \det A =  (1-a) \begin{vmatrix} a & 1 & \textbf{1}  \\ a-1 & 1-a & \textbf{0}  \\ 0 & a-1 & \textbf{0}  \end{vmatrix} + (1-a)\begin{vmatrix} a & 1 & 2 \\ a-1 & 1-a & 1-a \\ \textbf{0}  & \textbf{a-1}  & \textbf{0}  \end{vmatrix} ; $$ $$ \det A =  (1-a) \begin{vmatrix}   a-1 & 1-a   \\ 0 & a-1   \end{vmatrix} + (1-a)^2\begin{vmatrix} a & 2 \\ a-1 & 1-a  \end{vmatrix} ; $$ $$ \det A = (1-a)(a-1)^2 + (1-a)^2[-a^2-a+2] = (1-a)(a-1)^2 - (1-a)^2[a^2 + a -2] ; $$ $$\det A = (1-a)(a-1)^2 - (1-a)^2(a+2)(a-1);$$ $$\det A = -(a-1)^3 - (a-1)^2 (a+2)(a-1) = -(a-1)^3 - (a-1)^3(a+2); $$ $$\det A = -(a-1)^3 [1 + (a+2)] =  \boxed{-(a-1)^3(a+3)} $$ I can't see any mistake in my procedure. However, calculating $\det A$ with a computer, I obtained $$\det A= (a-1)^3(a+3)$$ It is the same result except for a minus sign... EDIT This is what I am doing in the first steps, basically substracting rows as follows: $$\det A =  \begin{vmatrix}
a & 1 & 1 &  1 \\
1 & a & 1 &  1 \\
1 & 1 & a &  1 \\
1 & 1 & 1 &  a \\
\end{vmatrix} =\begin{vmatrix}
a & 1 & 1 &  1 \\
1 & a & 1 &  1 \\
1 & 1 & a &  1 \\
0 & 0 & a-1 &  1-a \\
\end{vmatrix} =\begin{vmatrix}
a & 1 & 1 &  1 \\
1 & a & 1 &  1 \\
0 & a-1 & 1-a &  0 \\
0 & 0 & a-1 &  1-a \\
\end{vmatrix} = \begin{vmatrix}
a & 1 & 1 &  1 \\
a-1 & 1-a & 0 &  0 \\
0 & a-1 & 1-a &  0 \\
0 & 0 & a-1 &  1-a \\
\end{vmatrix} $$ So where is exactly my mistake?","['algebra-precalculus', 'determinant']"
3837181,Does this system have an explicit solution?,"$x' = x/\sqrt{1+y^2}, \ y' = \sqrt{1+x^2}.$ I don't know if there was a conventional way to solve convoluted systems like this, but is there a way this system can yield an explicit solution (that isn't a straight line)? If not that system, what about $x' = y^2, \ y' = x^2$ ?",['ordinary-differential-equations']
3837305,Generalised Eigenvectors | Correct way to Approach,"I have a matrix $$A = \begin{bmatrix}1 & 1 \\ -1 & 3\end{bmatrix}$$ I want to find out the generalised Eigenvectors. The Eigen values corresponding to the characteristic equation is $\lambda = 2$ and the Eigenvector correspondig to the eigenvalue is found to be $\begin{bmatrix}1 \\ 1\end{bmatrix}$ . So how to calculate the generalised Eigen vector for this matrix. What I did is , I took $(A-\lambda\cdot I)^2 \nu = 0$ . then solving the $(A-\lambda I)^2 = \begin{bmatrix}1-\lambda & 1 \\ -1 & 3-\lambda \end{bmatrix}^2 = \begin{bmatrix}\lambda^2-2\lambda & 4 - 2\lambda \\ 2\lambda-4 & \lambda^2 - 6\lambda+8 \end{bmatrix}$ At this point I don't know whether I am doing the things correct . as finding the determinant will take this to $\lambda^4$ .","['eigenvalues-eigenvectors', 'control-theory', 'matrices', 'linear-algebra', 'generalized-eigenvector']"
3837314,How to measure points scattering in a circle,"I have circles containing points (x,y). I would like to measure the scattering of the points within the circle.
For example, in the following picture, circle A will have a higher value since the points are much more scattered across the circle. Notice that the circles have varying value - so we can have a circles with different radiuses.
For example in the following picture, although the points are the same - circle C will have a higher value because the points are scattered across the whole circle. Do you know a measurement which I can use for such purpose?
Thanks!","['statistics', 'circles', 'geometry', 'linear-algebra', 'trigonometry']"
3837335,Proving the limiting behavior of functions containing iterated trigonometric functions.,"I remember years ago coming across some seemingly non-trivial (ie. non-fixed point related) limits describing to the behavior of infinitely iterated trigonometric functions, but I can't for the life of me remember how to construct the proof. Can someone point me in the right direction? Specifically, I want to prove the following limits: $$
\lim _{\left|n\right|\to \infty }\sqrt{\frac{4n}{3}}\left(\sin ^{\left\{n\right\}}\left(\frac{1}{\sqrt{n}}\right)\right) = 1
$$ $$\textbf{and}$$ $$
\lim _{\left|n\right|\to \infty }\sqrt{\frac{5n}{3}}\left(\tanh ^{\left\{n\right\}}\left(\frac{1}{\sqrt{n}}\right)\right) = 1
$$ I.e. that is to say: $$
\sin \left(\sin \left(\sin \left(\sin \left(\sin \left(\frac{1}{\sqrt{5}}\right)\right)\right)\right)\right) \cdot \sqrt{\frac{4\cdot 5}{3}} \approx 1
$$ $$
\tanh \left(\tanh \left(\tanh \left(\tanh \left(\tanh \left(\tanh \left(\frac{1}{\sqrt{6}}\right)\right)\right)\right)\right)\right)\cdot \sqrt{\frac{5\cdot 6}{3}}\approx 1
$$ $$
\operatorname{arcsinh}\left(\operatorname{arcsinh}\left(\operatorname{arcsinh}\left(\frac{1}{\sqrt{3}}\right)\right)\right)\cdot \sqrt{\frac{4\cdot 3}{3}}\approx 1
$$ ... and so on, noting the absolute value in the limits. Note on Notation : It seems people use a variety of different notations for expressing function iteration, but I went with this one since it felt most natural: $$
f^{\left\{0\right\}}\left(x\right)=x
$$ $$
f^{\left\{1\right\}}\left(x\right)=f(x)
$$ $$
...
$$ $$
f^{\left\{k\right\}}\left(x\right)=f\left(f^{\left\{k-1\right\}}\left(x\right)\right)\text{ } \forall k\in \mathbb{Z}
$$ This has been bugging me for a while, but I can't seem to make any substantive progress (despite   several hours of unsuccessful attempts to reconstruct the proof from old notes), so I will be forever grateful if you guys can give me some guidance!","['iterated-function-system', 'sequences-and-series', 'limits', 'trigonometry', 'dynamical-systems']"
3837425,Does there exist a right triangle such that all side lengths and angles in degrees are rational?,"Note: I used degrees in the title for the sake of brevity, but am using radians in the body of the question for clarity. Sorry for any confusion this causes. Here's the question: are there any right triangles $\triangle ABC$ such that $C=\frac{\pi}{2}$ , all side lengths $(a, b, c)$ are rational, and $A/\pi, B/\pi \in \mathbb{Q}$ ? The first thing I noticed is that this is an equivalent problem to finding some $\theta$ such that $$\sin(\theta), cos(\theta), \frac{\theta}{\pi}\in \mathbb{Q}\setminus\{0\}$$ Intuitively I'd expect this to have no solutions, I have no specific reason for this, just that my gut tells me this is impossible. My only idea for how I could look into this problem would be to show that $\Re (e^{i\theta})$ and $\Im(e^{i\theta})$ would give an irrational value for $\frac{\theta}{\pi}\in \mathbb{Q}\setminus\{0\}$ and somehow algabraicly manipulating this into a form that can be proven transcendental (which would imply that it's also irrational) by the Gelfond-Schneider theorem,  however I haven't come up with anything yet. Keep in mind that this is just a little puzzle that I've been throwing around in my head, so for all I know it might not even have an answer yet. The most advanced math that I can say I understand confidentally is multivariable calculus and I kind of know the basics undergraduate level courses such as abstract algebra,  real analysis, and complex analysis. From what I understand problems to do with irrationality tend to be unexpectedly difficult, so if it does require more advanced math than that I'd be fine with a more high level description of the proof.","['number-theory', 'irrational-numbers', 'geometry', 'trigonometry', 'rational-numbers']"
3837485,Surface Integral - Better Parametrization?,"So I'm supposed to calculate $$
\iint_S (x^2+y^2)dS
$$ Where $S$ Is the part of the plane $z=2x+2y-1$ which is inside the paraboloid $z=x^2 +y^2$ . The way I proceeded was to parametrize $$
S:
\begin{cases}
x=1+r\cos(\theta),\\
 y=1+r\sin(\theta),\\
z=3+2r(\cos(\theta)+\sin(\theta))
\end{cases}
$$ where $\theta \in [0,2\pi)$ and, since the circunference projected at the XY plane is centered at (1,1), we have $r \in [\sqrt{2}-1,\sqrt{3+2(\cos(\theta)+\sin(\theta))}]$ Luckly, findind $||\vec{r}_{\theta}\times \vec{r}_r||$ wasn't so hard, and I found it to be $3r$ . The problem is the integral after that becomes painfully complicated (at least for me). I suspect there is a better solution/parametrization. Can somebody help me out? Also, I know there is a way to do it using Stokes theorem, but I'm not suposed to use it on this one.","['multivariable-calculus', 'surface-integrals', 'parametrization']"
