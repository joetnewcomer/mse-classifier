question_id,title,body,tags
3590429,Stokes Theorem application question,"Below is an excerpt from the book ""Partial Differential Equations"" by Evans.  The underlined equation confuses me.  Clearly it is an application of Stokes theorem, and the implication seems to be that if $f$ is any compactly supported smooth function (for simplicity say on all of $\mathbb{R}^n$ ) then $$\int_{\mathbb{R}^n-B_\epsilon(0)} f_{x^i} d{x}=\int_{\partial B_\epsilon(0)} f\cdot \frac{(-x^i)}{\epsilon} d{S}$$ (here i am just replacing $u\phi$ with $f$ and $\nu^i$ with $ \frac{(-x^i)}{\epsilon} $ ).  But I can't get this equation to come out of stokes theorem.  E.g. assume for simplicity $n=2$ (set $(x^1,x^2)=(x,y)$ ), and $\epsilon =1$ . Let $d\theta$ be the 1-form gotten by pulling back (via $\mathbb{R}^2-0\rightarrow S^1, v\mapsto v/|v|$ ) the volume form on $S^1$ .  Then stokes gives $$\int_{S^1}f\cdot(-x) d\theta=\int_{\mathbb{R}^2-B_1(0)}d(f\cdot(-x) d\theta)=\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial x}dx\wedge d\theta+\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial y}dy\wedge d\theta.$$ Now it seems $dx\wedge d\theta = \frac{x}{x^2+y^2}~~ dx\wedge dy$ and $dy\wedge d\theta = \frac{y}{x^2+y^2}~~ dx\wedge dy$ so this gives $$\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial x} \cdot \frac{x}{x^2+y^2} ~~dx dy+\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial y} \cdot \frac{y}{x^2+y^2}~~dx dy=$$ $$\int_{\mathbb{R}^2-B_1(0)}- \frac{x}{x^2+y^2}\cdot f~~dx dy+\int_{\mathbb{R}^2-B_1(0)}- \frac{x^2}{x^2+y^2}\cdot f_x ~~dx dy+\int_{\mathbb{R}^2-B_1(0)}- \frac{xy}{x^2+y^2}\cdot f_y~~dx dy.$$ I see no cancellations here or any way to make this look like $$\int_{\mathbb{R}^2-B_1(0)}f_x~~dx dy.$$","['stokes-theorem', 'analysis', 'differential-geometry']"
3590482,Counterexample second order mixed partial derivatives,"I know a few examples of functions $f:\mathbb{R}^2\to\mathbb{R}$ for which $\frac{\partial^2f}{\partial x\partial y}(0,0)$ and $\frac{\partial^2f}{\partial y\partial x}(0,0)$ both exist and are different but I cannot find one where one exists and the other does not. See here for the case when they both exist. counterexample One of my students asked me in class and I couldn’t come up with one. Has anyone seen one? If possible I would like one where both first order partial derivatives exist. Thanks!","['functions', 'derivatives', 'real-analysis']"
3590521,How to find an inverse of this type of functions?,"Let $F(x,y)$ and $G(x,y)$ Be functions from where $x,y$ are whole numbers. ( $Z^2 \to Z^2$ ) $F(x,y) = (x+3y,x+5y)$ $G(x,y) = (2x+3y,3x+5y)$ The question: One of these functions has an inverse, Prove it and find its inverse functions. My question: I don't really know how to decide which one is inversible. I tried to show that $f(x,y)$ is not surjective but with no success. Also, I don't know how to find the inverse of a function like this. Can someone hint me to the solution? (I want to solve it myself). Maybe through an example or something. Thank you.","['elementary-set-theory', 'calculus', 'functions']"
3590524,"Compute the expectation of the wrapped normal distribution on $[0,1)$","Let $\sigma>0$ , $$\phi(x):=\frac1{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}}\;\;\;\text{for }x\in\mathbb R$$ and $$\mathcal N(x,B):=\int_B\varphi(y-x)\:{\rm d}y\;\;\;\text{for }(x,B)\in\mathbb R\times\mathcal B(\mathbb R).$$ If $$\iota:\mathbb R\to[0,\infty)\;,\;\;\;x\mapsto x-\lfloor x\rfloor,$$ then the wrapped normal distribution kernel on $[0,1)$ with variance $\sigma^2$ is defined by $$\mathcal W(x,\;\cdot\;):=\mathcal N(x,\;\cdot\;)\circ\iota^{-1}\;\;\;\text{for }x\in[0,1).$$ If $$\psi(x):=\sum_{k\in\mathbb Z}\phi(x+k)\;\;\;\text{for }x\in\mathbb R,$$ we can show that $$\mathcal W(x,B)=\int_B\psi(y-x)\:{\rm d}y\;\;\;\text{for all }(x,B)\in[0,1)\times\mathcal B([0,1))\tag1.$$ Let $x\in[0,1)$ . How can we compute the expectation of a random variable $Y\sim\mathcal W(x,\;\cdot\;)$ ? Clearly, $$\operatorname E[Y]=\int_0^1\psi(y-x)y\:{\rm d}y\tag2.$$","['expected-value', 'measure-theory', 'probability-theory', 'normal-distribution']"
3590613,Bound on the $2$-norm of a diagonal sub-matrix,"Let $A \in \mathbb{R}^{n \times n}$ be an invertible real matrix and write $A_d$ for the sub-matrix consisting of its diagonal part only, namely $(A_d)_{ij} = A_{ij}$ if $i = j$ and $0$ otherwise. I can prove that $$\lVert A_d \rVert_2 \leq \lVert A_d \rVert_F \leq \lVert A \rVert_F \leq \sqrt{n}\lVert A \rVert_2 $$ but can this inequality be improved? In other words, can we find $A$ invertible such that $\lVert A_d \rVert_2 = \sqrt{n}\lVert A \rVert_2$ ?","['matrices', 'normed-spaces', 'linear-algebra']"
3590615,A function satisfying a series equation,"We already solved or argued differential equations, integral equations and even function equations. But I encounter the following, which I call it ""series equation"": Is there any nonzero function $f$ satisfying the series equation $$f(x)=\sum_{n=0}^{\infty}\frac{f(n)}{n!}x^n \tag{*} $$ The source: I interested to solve the equation $$f'(x)=f(x+1)$$ and I found out $$f^{(n)}(x)=f(x+n)$$ thus: $f^{(n)}(0)=f(n)$ and if we consider $f$ having a taylor series, then the expression $(*)$ will be the case. But I have no idea how $f$ would be. Is such a function exists? Thanks.","['ordinary-differential-equations', 'analysis', 'real-analysis', 'taylor-expansion', 'delay-differential-equations']"
3590674,Differential $k$-form and integrating factor,"Consider a $C^1$ differential $k$ -form $\omega$ on an open set $U \subset \mathbb R^n$ . A non-vanishing $C^1$ function $f: \mathbb R^n \to \mathbb R$ is called an integrating factor for $\omega$ if $d(f\omega) = 0$ . If $k$ is odd and an integrating factor exists, I want to show that $\omega \land d\omega = 0 $ . Here's my proof: $$d(f\omega \land \omega) = d(f\omega) \land \omega + (-1)^k f\omega \land d\omega
                          = 0 \land \omega + (-1)^k f\omega \land d\omega
                          = (-1)^k f(\omega \land d\omega)$$ And $f\omega \land \omega = f(\omega \land \omega) = 0$ , so $d(f\omega \land \omega) = 0$ and hence $(-1)^k f(\omega \land d\omega) = 0$ . If $f$ never vanishes, then $(-1)^k f$ never vanishes, so that $\omega \land d\omega = 0$ . Clearly, my proof doesn't use that $k$ must be odd. What did I do wrong here?","['multivariable-calculus', 'differential-forms', 'differential-geometry']"
3590712,"Solving $\int X_s \, ds = B_1$ where $B$ is a Brownian motion","Let $B_t$ be a standard Brownian motion. I am interested in the equation $$\int_0^1 X_s \, ds = B_1 \quad a.s$$ Besides the obvious solution $X_s=B_1$ for all $s\in [0,1]$ , how can we come up with solutions ? Can we characterize in a nice way the set of solutions ? Are all solutions of the form $B_1 f(s)$ with $\int_0^1 f(s) \, ds =1$ ? If so, how can we prove it ? With each solution $X_s$ , we associate the value $V_X= \int_0^1 E[X_s^2]\,ds$ . Among all solutions, is it true that the one that minimizes $V_X$ is $X_s = B_1 $ and is unique ?","['stochastic-analysis', 'real-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
3590715,Show that: $\sum_{k=0}^{\infty}\arctan\left(\frac{6k^2+4k}{4k^4+12k^3+13k^2+6k+5}\right)=\frac{\pi}{4}$,"I was observing this paper and by some experimental chance I got this result, but I can not prove it. $$\sum_{k=0}^{\infty}\arctan\left(\frac{6k^2+4k}{4k^4+12k^3+13k^2+6k+5}\right)=\frac{\pi}{4}\tag1$$ I tried to factorise $4k^4+12k^3+13k^2+6k+5$ but it is not factorable. I guess that $$\arctan\left(\frac{6k^2+4k}{4k^4+12k^3+13k^2+6k+5}\right)=\arctan(x)+\arctan(y)$$ Does anyone knows how to prove $(1)?$","['trigonometry', 'pi', 'inverse', 'sequences-and-series']"
3590718,Perfect Sampling - Reuse of random bits,"I'm currently studying the Perfect Sampling approach to Markov Chain Monte Carlo proposed by Propp and Wilson in 1996.Though having understood most important aspects, I'm still struggling to figure out their example used for explaining that one must reuse the random numbers generated in earlier iterations. The example reads as follows and is allegedly ""simple to check"". It is sometimes desirable to view the process as an iterative one, in which one successively starts up $n$ copies of the chain at times $-1$ , $-2$ , etc., until one has gone sufficiently far back in the past to allow the different histories to coalesce by time $0$ . However, when one adopts this point of view (and we will want to do this in the next subsection), it is important to bear in mind that the random bits that one uses in going from time $t$ to time $t+1$ must be the same for the many sweeps one might make through this time-step. If one ignores this requirement, then there will in general be bias in the samples that one generates. The curious reader may verify this by considering the Markov chain whose states are $0$ , $1$ , and $2$ , and in which transitions are implemented using a fair coin, by the rule that one moves from state $i$ to state $\min(i+1,2)$ if the coin comes up heads and to state $\max(i-1,O)$ otherwise. It is simple to check that if one runs an incorrect version of our scheme in which entirely new random bits are used every time the chain gets restarted further into the past, the samples one gets will be biased in favor of the extreme states $0$ and $2$ . So obviously the transition matrix of the described Markov Chain is $$
P = \begin{pmatrix} 
1/2 & 1/2 & 0 \\
1/2 & 0 & 1/2 \\
0 & 1/2 & 1/2
\end{pmatrix}
$$ with state space $S = \lbrace 0,1,2 \rbrace$ . So the stationary distribution is $ \pi = \begin{pmatrix} 1/3 & 1/3 & 1/3 \end{pmatrix}$ and the claim is that when applying Perfect Sampling wrongly( that is with regeneration of the random transitions) we would get states $0$ and $2$ with a higher frequency than $1/3$ . I tried modeling the dynamics of the coupled Markov-Chains started at each of the 3 different states. To do so I defined the following $2$ -dimensional Markov-Chain $$
X_t = (X_t^1, X_t^2),
$$ where $X_t^1$ denotes the number of the $3$ -chains currently in state $0$ and $X_t^2$ denotes the number of the $3$ -chains currently in state $1$ . The number of chains in state $2$ would then follow from $3 = X_t^1 + X_t^2$ .
Then the state space of $X$ is $$
S = \lbrace (0,0), (0,1), (0,2), (0,3), (1,0), (1,1), (1,2), (2,0), (2,1), (3,0) \rbrace
$$ and the respective transition matrix I came up with is (in R code) m <- matrix(c(.5, 0, 0, .5, 0, 0, 0, 0, 0, 0, 
              .25, 0, .25, 0, .25, 0, .25, 0, 0, 0, 
              .25, .25, 0, 0, 0, 0, 0, .25, .25, 0, 
              .5,  0, 0, 0, 0, 0, 0, 0, 0, .5, 
              0, .25, 0, .25, .25, 0, .25, 0, 0, 0, 
              0, .125, .125, 0, .125, .25, .125, .125, .125, 0, 
              0, .25, 0, 0, .25, 0, 0, 0, .25, .25, 
              0, 0, .25, .25, 0, 0, 0, .25, .25, 0, 
              0, 0, .25, 0, 0, 0, .25, .25, 0, .25, 
              0, 0, 0, .5, 0, 0, 0, 0, 0, .5), 
            byrow=TRUE, nrow=10) But using this transition matrix and computing the distributions for a start in the initial state $(1,1)$ suggests that the $3$ ""coalescence states"" $(3,0), (0,3) $ and $(0,0)$ have equal probability.","['statistics', 'monte-carlo', 'markov-chains']"
3590784,Order of minimun and maximum difference,"I want to prove firstly on $Fun(\mathbb{N},\mathbb{N})$ that "" $f < g$ if and only if $f(k)<g(k), k=\min\{n|f(n) \neq g(n)\}$ "" is a total order, but not a well-order. Then, on the set $\{f:\mathbb{N_0} \to \mathbb{N_0}|f \ \text{is definitely} = 0\}$ "" $f < g$ if and only if $f(k)<g(k), k=\max\{n|f(n) \neq g(n)\}""$ is a total and well-order. In the first case I can't find a counterexample to show that every non-empty subset has a least element in this ordering. And, in the other case, how can I prove that the second set is well-ordered?","['elementary-set-theory', 'relations']"
3590835,Prove that 4 points belong to the same circle by using complex numbers,"I have Z1, Z2, Z3, Z4 and they are all complex numbers. I want to prove that they belong on the same circle(C) and its center is O where O = 3 How do I do that? (They actually have equations, I just don't want to write them here because they don't matter, what matters is the way) The exercise solved by calculating the length between Z1 and O, then Z2 and O, then Z3 and O, then Z4 and O. Then they all gave the same result which means they all belong on the same circle. This is what I don't get it. I guess this would work if we said that a point of these points belongs to the same circle. But the thing is that we have to prove them all. What tells me that they are not inside or outside the circle?","['circles', 'geometry', 'calculus', 'linear-algebra', 'complex-numbers']"
3590839,Evaluating $\lim_{x\to 0}\frac{x\sin x-2+2\cos x}{x\ln(1+x)-x^2}$ using L'Hôpital,"Considering this limit, assigned to my high school students, $$\lim_{x\to 0}\frac{x\sin x-2+2\cos x}{x\ln \left(1+x\right)-x^2}=\left(\frac00\right)=\lim_{x\to 0}\frac{\frac{d}{dx}\left(x\sin \left(x\right)-2+2\cos \left(x\right)\right)}{\frac{d}{dx}\left(x\ln \left(1+x\right)-x^2\right)} \tag 1$$ After some steps, using L'Hôpital, I find: $$\lim_{x\to 0}\frac{\left(x\cos \left(x\right)-\sin \left(x\right)\right)\left(1+x\right)}{-2x^2-x+x\ln \left(x+1\right)+\ln \left(x+1\right)}=\left(\frac00\right)$$ Should I continue to apply L'Hôpital? :-(","['limits', 'limits-without-lhopital']"
3590854,Differentiability of an integral accumulation function,"Is $$H(x) = \int_0^x \left\lvert\sin\left(\frac{1}t\right)\right\rvert\,\mathrm dt$$ differentiable at $x = 0$ ? I claim that $H(x)$ is differentiable at $x=0.$ Observe that \begin{align}H(-x) &= \displaystyle\int_0^{-x}|\sin(\frac{1}t)|dt=\displaystyle\int_0^x |\sin(-\frac{1}u)|(-1)du,\text{ where $u = -t,$}\\
&=-\displaystyle\int_0^x |\sin(\frac{1}t)|dt = -H(x),\end{align} so $H(x)$ is odd. Also, $H(0) = 0.$ It suffices to evaluate $\lim\limits_{x\to 0^+}\dfrac{H(x)}x,$ since if $\lim\limits_{x\to 0^+} H(x)$ exists, it must equal $-\lim\limits_{x\to 0^-}H(x),$ which implies that $\lim\limits_{x\to 0^+}\dfrac{H(x)}x = \lim\limits_{x\to 0^-}\dfrac{H(x)}x.$ So assume $x>0.$ Since $|\sin(\frac{1}t)|$ is bounded and continuous on $(0, x], H(x) = \displaystyle\int_0^x |\sin(\frac{1}t)|dt = \lim\limits_{u\to 0^+}\displaystyle\int_u^x |\sin(\frac{1}t)|dt=\lim\limits_{n\to\infty}\displaystyle\int_{1/((n+1)\pi)}^{1/(k_x\pi)}|\sin(\frac{1}t)|dt + \displaystyle\int_{1/(k_x\pi)}^x |\sin(\frac{1}t)|dt \\
= \displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt+\displaystyle\int_{1/(k_x\pi)}^x|\sin(\frac{1}t)|dt,$ where $\frac{1}{k_x\pi} \leq x \leq \frac{1}{(k_x-1)\pi}\Rightarrow k_x\pi \geq \frac{1}{x} \geq (k_x - 1)\pi \Rightarrow k_x =  \lceil \frac{1}{x\pi} \rceil.$ Now, observe that $0 \leq |\dfrac{H(x)}x|\leq \dfrac{1}x \left|\displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt + \displaystyle\int_{1/(k_x\pi)}^x |\sin(1/t)|dt\right|\\
\leq \dfrac{1}x(\left|\displaystyle\sum_{k=k_x}^\infty \displaystyle\int_{1/((k+1)\pi)}^{1/(k\pi)}|\sin(\frac{1}t)|dt\right|+\left|\displaystyle\int_{1/(k_x\pi)}^x |\sin(1/t)|dt\right|)\leq \dfrac{1}x(\lim\limits_{n\to\infty} \dfrac{1}{k_x\pi} - \dfrac{1}{(n+1)\pi}+x-\dfrac{1}{k_x\pi})\leq 1,$ however, here I am stuck. Also, $\dfrac{H(x)}{x}$ is not monotone, so I think I should use a different approach. I know that for $t\in [\dfrac{1}{n\pi+\frac{3\pi}4}, \dfrac{1}{n\pi+\frac\pi4}], |\sin(\dfrac{1}t)| \geq \dfrac{1}2,$ but I am not sure if this is useful.",['integration']
3590905,Evaluate the following integral :$\int\limits_0^{\infty}\frac{\log (1+x^{4})}{\sqrt{x}(1+x)}dx$,"Evaluate the following integral : $$I=\int\limits_0^{\infty}\frac{\log (1+x^{4})}{\sqrt{x}(1+x)}dx$$ I was tried use change variable , If I use $x=y^2$ integral becomes : $$I=2\int\limits_0^{\infty}\frac{\log (1+x^{8})}{1+x^{2}}dx$$ From here I have one idea  the derivative under sing integral but I got I difficult integration : $$I=2\int\limits_0^{\infty}\frac{x^{8}}{(1+ax^{8})(1+x)}dx$$ I already to see you hints or solution!","['integration', 'improper-integrals', 'definite-integrals', 'closed-form']"
3590932,Understanding notation for holomorphic quadratic differentials,"I've found the notation in books to be a bit confusing (perhaps my background is inadequate). For example, in Farb and Margalit's book A Primer on Mapping Class Groups , they give the following definition: Definition ""Let $\{ z_\alpha: U_\alpha \to \mathbb{C} \}$ be an atlas for $X$ [Here $X$ is a Riemann surface]. A holomorphic quadratic differential $q$ on $X$ is specified by a collection of expressions $\{ \phi_\alpha (z_\alpha) dz_\alpha^2 \}$ with the following properties: 1. Each $\phi_\alpha : z_\alpha(U_\alpha) \to \mathbb{C}$ is a holomorphic function with a finite number of zeros. 2. For any two coordinate charts $z_\alpha$ and $z_\beta$ , we have $$ \phi_\beta(z_\beta)( \frac{dz_\beta}{dz_\alpha})^2 = \phi_\alpha(z_\alpha) $$ "" Here $\frac{dz_\beta}{dz_\alpha}$ is the derivative of the change of coordinates $z_\beta \circ z_\alpha^{-1}$ . So, my problem comes when I try to check that this definition is independent of the coordinate system that you choose. Say that you have $z_\alpha: U_\alpha \to \mathbb{C}, z_\beta: U_\beta \to \mathbb{C}$ , $w \in U_\alpha \cap U_{\beta}$ and a vector $v \in T_wX$ . Then, I would like to use property number 2 to check that $q(v)$ is well defined, i.e. that $$ \phi_\alpha \circ z_\alpha(w)  dz_\alpha^2(v) = \phi_\beta \circ z_\beta(w)  dz_\beta^2(v) $$ I'm confused about what exactly $dz_\alpha(v)$ and $dz_\beta(v)$ are. Is it the case that $dz_\alpha = z_\alpha^*(dz)$ ? I thought that could be it, but then I get $dz_\alpha(v) = dz(d_w z_\alpha(v)) $ , and I don't really get anywhere. I would like to do this in a rigorous way, not just formally manipulating $dz_\alpha$ and $dz_\beta$ (""multiplying"" by $dz_\alpha^2$ in property number 2 would give me what I want, but I would like to understand what I'm doing).","['complex-analysis', 'riemann-surfaces', 'differential-forms']"
3590945,Why in Statistics do we use R-squared when Comparing Linear Models instead of Least Squares?,"In Machine Learning we use a cost function such as least squared errors to evaluate how good the model is and if one model has a better score than the other, assuming that it does not overfit we choose said model. But in statistics, R-squared seems to be favored in model selection and not Least Squares. What's the point of R-squared/Adjusted R-squared when we have Least squared to measure performance in general? What am I missing? or am I just confused?","['statistics', 'mathematical-modeling', 'machine-learning', 'least-squares', 'linear-regression']"
3590972,A proof of Poincaré-Hurwitz Theorem in English?,"I was reading this paper by Shane Chern: https://arxiv.org/abs/1602.02844 and I found the following theorem: Let $E$ be a nonsingular cubic curve in $\mathbb{P}^2$ which is defined over $\mathbb{Q}$ . If the set $E\left (\mathbb{Q}\right )$ is infinite, then every open subset of $\mathbb{P}^2\left (\mathbb{R}\right )$ which contains one point of $E\left (\mathbb{Q}\right )$ must contain infinitely many points of $E\left (\mathbb{Q}\right )$ I could not find any proof of this which is written in a language I can understand (I only understand English and Spanish). The reference that Shane Chern provides is an article by Poincaré, which is written in French, and an article by Hurwitz, which is written in German. Reading another article which also makes use of this result ( http://archive.ymsc.tsinghua.edu.cn/pacm_download/21/129-2012A_DIOPHANTINE_PROBLEM_FROM_MATHEMATICAL_PHYSICS.pdf ) I found another source: ""T. Skolem, Diophantische gleichungen , Ergebnisse der Mathematik und ihrer Grenzgebiete, New Chelsea Publishing Company, 1950, p. 78"", which provides the following proof: but I do not understand German. So, do you know where to find a proof in English? And, if you know or attain a proof, could you please share it? Thank you in advance!","['cubics', 'elliptic-curves', 'translation-request', 'reference-request', 'algebraic-geometry']"
3591026,Evaluating $\sum_{0\leq k \leq l \leq n}\binom{k}{2}\binom{l}{k}\binom{n}{l}$,"I'm trying to understand the process of evaluating this sum. I know the above equals to: $$|\{(A,B,C)\mid A\subseteq B \subseteq C \subseteq [n] \wedge |A| = 2\}|$$ ...Yet, how can I express this using $n$ ?","['elementary-set-theory', 'summation', 'binomial-coefficients', 'combinatorics']"
3591038,Different bricks making a cube,"We want to build an $n \times n \times n$ cube using bricks that have integer sides and are all different.  As a function of $n$ , what is the maximum number of bricks we can use?  For $n=1$ or $2$ it is $1$ .  For $n=3$ we can use four, one way is $1 \times 1 \times 1, 1 \times 1 \times 2, 1 \times 2 \times 3, 2 \times 3 \times 3$ For the linked question I have shown that $a(10) \le 52$ by finding that the sum of the volumes of the smallest $53$ blocks is over $1000$ .  Are there better results available? Prompted by this question .","['discrete-optimization', 'tiling', 'geometry', '3d']"
3591047,"Count permutation pairs such that for each element, either permutation gives it a different position from its original position","How many permutation pairs $(\sigma,\tau)$ are there such that for every $1\leq i \leq n$ , $\tau (i) \neq i \vee \sigma(i) \neq i$ ? For calrification, the permutations are over $[n]$ . What I know is that the amount of derangements over $[n]$ is $\Sigma_{j=0}^n \frac{(-1)^j n!}{j!}$ , so that means the amount of pairs which allow for $\tau(i) \neq i \wedge \sigma(i) \neq i$ is the square of that sum. Yet, the condition in the question doesn't seem complementary to what I just described. So, can I proceed?","['permutations', 'derangements', 'combinatorics']"
3591058,Prove $\bigcup (F\setminus G) \subseteq (\bigcup F) \setminus (\bigcup G)$ iff $\forall A \in (F\setminus G) \forall B\in G (A\cap B = \emptyset)$,"This is an exercise from Velleman's ""How To Prove It"": Prove that $\bigcup (F \setminus G) \subseteq (\bigcup F) \setminus (\bigcup G)$ iff $\forall A \in (F \setminus G) \forall B \in G (A \cap B = \emptyset)$ . I am uncertain about the use of variables in existential instantiation. If I say something like $\exists x P(x)$ , is it okay to then continue using $x$ in the rest of the proof, or should I introduce a new variable $a$ such that $P(a)$ ? Also, when using contradiction, is it necessary to indicate that I am doing so? As I am self-studying, I would greatly appreciate other comments as well. Thanks in advance! Proof: Suppose $\bigcup (F \setminus G) \subseteq (\bigcup F) \setminus (\bigcup G)$ . Let $A \in (F \setminus G) $ and $B \in G$ be arbitrary. Now suppose $\exists x (x \in A \cap B)$ . Since $x \in A$ and $A \in (F \setminus G)$ , it follows by definition that $x \in \bigcup (F \setminus G)$ . Since $x \in \bigcup (F \setminus G)$ and $\bigcup (F \setminus G) \subseteq (\bigcup F) \setminus (\bigcup G)$ , $x \in (\bigcup F) \setminus (\bigcup G)$ . Thus, $x \in (\bigcup F) $ and $x \notin (\bigcup G) $ . But since $x \in B$ and $B \in G$ , $x \in \bigcup G$ . Thus, we have $x \in \bigcup G$ and $x \notin \bigcup G$ , which is a contradiction. So $\forall x (x \notin A \cap B)$ and $A \cap B = \emptyset$ . Since $A$ and $B$ were arbitrary, it follows that $\forall A \in (F \setminus G) \forall B \in G (A \cap B = \emptyset)$ . Suppose $\forall A \in (F \setminus G) \forall B \in G (A \cap B = \emptyset)$ . Let $x \in \bigcup (F \setminus G)$ be arbitrary. Since $x \in \bigcup (F \setminus G)$ , we can choose some $W \in (F \setminus G)$ such that $x \in W$ . Since $x \in W$ and $W \in F$ , it follows that $x \in \bigcup F$ by definition. Suppose $x \in \bigcup G$ . We can then choose a $V \in G$ such that $x \in V$ . But then we have $W \in (F \setminus G)$ , $V\in G$ , and $x \in W \cap V$ . This is a contradiction because it was given that $\forall A \in (F \setminus G) \forall B \in G (A \cap B = \emptyset)$ . Thus, $x \notin \bigcup G$ . Therefore, if $x \in \bigcup (F \setminus G)$ , then $x \in (\bigcup F) \setminus (\bigcup G)$ . Since $x$ was arbitrary, $\bigcup (F \setminus G) \subseteq (\bigcup F) \setminus (\bigcup G)$ . $\square$","['elementary-set-theory', 'proof-writing', 'logic', 'solution-verification']"
3591060,"Creating a SIIR (susceptible, infected, isolated, recovered) model using differential equations.","I wasn't too sure of where to post this since it's a mix of physics (dynamical systems), medicine, and mathematics but here it goes. I am trying to model the current outbreak of Covid 19 using a more sophisticated model than the simple SIR model, and so I added two categories: Isolated Sick people to represent sick people who self isolate after presenting symptoms, and Isolated Healthy people to represent people who isolate once isolation is ordered. I set the following set of differential equations and I could appreciate criticism or ways of improving the model, or just confirmation that it should be able to model a pandemic like the one today. I name $S(t)$ the susceptible people, $I(t)$ the infected people (not in quarantine), $Q_s(t)$ people who are sick in quarantine, $Q_h(t)$ people who are healthy in quarantine, and $R(t)$ recovered people. $$
\frac{dS}{dt}=-\beta \frac{S}{N}I-\alpha(t)\frac{S}{N}S+\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{Q_s}{N}S
$$ $$
\frac{dI}{dt}=\beta \frac{S}{N}I+\beta c\frac{S}{N}Q_s+\beta c\frac{I}{N}Q_h-\lambda I-\gamma I -\alpha(t)\frac{S}{N}I
$$ $$
\frac{dQ_s}{dt}=\lambda I-\gamma Q_s+\alpha(t)\frac{S}{N}I
$$ $$
\frac{dQ_h}{dt}=\alpha(t)\frac{S}{N}S-\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{I}{N}Q_h
$$ $$
\frac{dR}{dt}=\gamma Q_s+\gamma I
$$ $$
N = S(t)+I(t)+Q_s(t)+Q_h(t)+R(t) = constant
$$ The equations use the following parameters. $\beta$ is the transmission rate of the disease. $\alpha(t)$ is a time-dependent parameter that indicates the rate at which people go into quarantine once a compulsory quarantine is in effect. You can think of it being a pulse shaped function of a given duration (quarantine duration) and a given amplitude. $\alpha'(t)$ follows the same idea but instead describes how people get out of the quarantine once it's over. $c$ is a percentage multiplier that describes how reduced the infection rate for quarantine people is. In an ideal quarantine, it would be $0$ . $\lambda$ is the rate at which people self isolate once they get infected. $\gamma$ is the rate of recovery from the disease.
Finally, $N$ should be the total population and it should be a constant since all the time derivatives should add up to $0$ (unless I mistyped). I am currently solving it using Python and Scipy, and the main question I have apart from how relevant the coefficients like $\frac{S}{N}$ or $\frac{I}{N}$ are is how come isolation doesn't seem to affect much the amplitude of the peak of infections. Instead what happens is that it only pushes it back. Furthermore, sometimes it seems like isolation at the right moment (once the peak starts to form) can help a lot more than isolation too early there's barely any cases. Indeed, early isolation just pushes it back, but isolation at the right time seems to reduce the future peek since a lot of individuals are by that time already immune and the susceptible population is a lot smaller than just pure isolation when the susceptible population just bounces back to pre isolation levels and since the disease is just as infectious and it hasn't been eliminated, the peak happens just later in the future. Is this effect normal? Is this effect truly what happens in a real pandemic? And if so isn't the current isolation a bit counterproductive in the sense that we don't allow any herd immunity to happen? Thank you! EDIT: I have updated the equations following a comment and removed a squared dependence on S. So now: $$
\frac{dS}{dt}=-\beta \frac{S}{N}I-\alpha(t)S+\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{Q_s}{N}S
$$ $$
\frac{dQ_h}{dt}=\alpha(t)S-\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{I}{N}Q_h
$$ Also, I have been running some simulations (You can find the code on my Github ) that I'd like to discuss. Both correspond to a run with N=40 million people (the size of California), with $\beta=\frac{1}{24*4.375} \text{h}^{-1}$ (the $\beta$ was obtained doing a fit to the cumulative cases in California, and getting the time constant in days). $\lambda=\frac{1}{24*5}=\alpha_0$ (so people take around five days to fully comply with a quarantine. $c=0.5$ . The initial conditions are $I_0=1190$ cases $R_0=20$ , $Q_{h_0}=10$ . The first picture shows a quarantine that is pronounced at $t=20$ days, with a duration of 60 days, with 30 days at full swing, and 15 for rise at full level, and another 15 to come back to pre isolation levels. The first picture has a peak at 199 days, with a total number of infected at 24.1 million people. The second one gives a peak at day 86 with 24.2 million people infected. So as you can see the peak is moved back significantly but the amplitude of said peak stays about the same. Now what's interesting is if I launch the isolation when a little outbreak is happening. I will delay the full quarantine by 50 days. And leave the rest the same. Now the peak on the right is ""only"" at 15.5 million and it was still pushed back to day 243. It seems more manageable. Since probably around 30% of those cases might require hospitalization. Finally, I wonder if repeating a quarantine while the second outbreak happens will work like the first one further helping to reduce the total number of infected. I could model that by changing $\alpha(t)$ and $\alpha'(t)$ .","['nonlinear-dynamics', 'ordinary-differential-equations', 'dynamical-systems']"
3591076,Four bridge hands with no two people having 8 or more cards of the same suit between them.,I am trying to solve this problem: A 52-card deck is dealt out to 4 people (13 to each). What is the probability that no two people have 8 or more cards of the same suit between them? It seems to me that there are only 4! ways for this to be possible (when each player has four of a different suit and three of each of the others). This seems to be too small but I'm unsure of how to count all the possible satisfying combinations there are. I also can't quite figure out the denominator. Is it ${52 \choose 13}{39 \choose 13}{26 \choose 13}$ ? Or $\frac{52!}{13!^4}$ ?,"['discrete-mathematics', 'combinatorics', 'card-games', 'probability']"
3591089,Dummit and Foote's proof that $(\mathbb{Z}/p^{\alpha}\mathbb{Z})^*$ is cyclic of order $p^{\alpha - 1}(p-1)$,"Here is the proof from Dummit and Foote. I follow the proof until the part at the end where it says that all Sylow subgroups are cyclic, but I do not see how that implies that $(\mathbb{Z}/p^{\alpha}\mathbb{Z})^*$ is cyclic. This is $(3)$ of Corollary $20$ of section $9.5$ on page $314$ Help would be really appreciated.","['proof-explanation', 'cyclic-groups', 'ring-theory', 'abstract-algebra', 'group-theory']"
3591096,What is the intuition behind linearity of expectations not requiring independence?,"I am confused as to the intuition behind the linearity of expectations not requiring events to be independent. Why is this true? I read that since the proof that shows expected values are linear does not use anything regarding independence, independence is not a requirement. I don't quite follow that step. Why would we not need to show that both independent and dependent events have this property? This also leaves me confused with questions regarding this property. For example, Suppose you toss a fair coin 12 times resulting in a sequence of heads (H) and tails (T). Let N be the number of times that the sequence HTHT appears.  For example, HTHT appears twice in HTHTHTTTTTTT. Find E(N) The answer to this problem is 9/16 , which comes from the fact that there is a 1/16 probability that HTHT occurs, starting at index n, with 1 <= n <= 9 , and the answer is 9 * 1/16 . Why is it that we can add the probability that the string HTHT occurs starting at any index? I ask this because say HTHT were to appear in the first four flips, then the probability that HTHT occurs starting at the second index is zero because T was the outcome of the second index. An explanation of the intuition of this property would be appreciated.","['expected-value', 'intuition', 'probability-theory', 'probability']"
3591157,Can smooth ODE converge to its equilibrium in finite time?,"Consider the following nonliner system: \begin{align}
\dot{x}=f(x)
\end{align} where $x\in\mathbb{R}^n$ and $f(x)\in\mathbb{R}^n$ is sufficiently smooth and Lipschitz in $x$ .
Then the system is smooth and admits a unique solution.
Suppose $x^*\in\mathbb{R}^n$ is an equilibrium of the system, i.e., $f(x^*)=0$ .
Is it possible that, for some initial condition $x(0)=x_0$ , the solution of the system satisfies \begin{align}
\lim_{t\to T}x(t)=x^{*},
\end{align} that is, the solution reaches the equilibrium $x^*$ in some finite time $T$ .
If it is not possible, is there a way to show that, the solution of a smooth system will take an infinite amount of time to converge to an equilibrium? Update：Assume that $x_0\neq x^{*}$ .","['finite-duration', 'ordinary-differential-equations', 'dynamical-systems']"
3591187,Overall equal probabilities when replacing choices,"Explanation: I want to choose marbles from a bag x times. Once chosen, the marbles are not put back in the bag. After each choice, some number of new marbles are added to the bag. x is less than the total number of marbles, i.e. not all marbles will be chosen. My goal is to make it so every marble has an equal chance of being chosen overall (or as close as possible). Example 1 (x = 2): Start with marbles A and B. Choose one at random. Let's say B was chosen. Now marble C is added to the bag and the second marble is chosen. A has already had a 50% chance of being chosen, so giving A and C the same odds would mean A was more likely than C. Is there any weighting possible to make it such that A, B, and C were all equally likely in the beginning? I assume it is not possible because C was not available in the beginning. I believe the next best thing would be to make it so A and C were equally likely from the beginning. My guess is that this can be done by making C twice as likely as A for the second draw (A should have 1/3 chance and C should have 2/3 chance). However, I am not sure how to prove it. Example 2 (x = 3): Start with marbles A and B. Again let's say B is chosen. Now Marbles C and D are added and a second is chosen. Again, I want to say C and D should be twice as likely as A. Assuming I was correct before, A should have a 1/5 chance and C and D should both have a 2/5 chance. Let's say C was chosen. Now marbles E and F are added and so the odds of each marble for the third draw should be 0.0909 for A, 0.1818 for C, 0.3636 for E and 0.3636 for F. Is this correct? How can it be shown mathematically? Does my general approach of marbles added after the last draw being twice as likely as the marbles in the draw before that which are twice as likely as the marbles in the draw before that... work?","['statistics', 'probability']"
3591201,How to find the the force necessary to pull from a cylinder so it keeps in place?,"The problem is as follows: A cylinder of $\textrm{500 grams}$ in mass has a very thin flexible
  non elastic tin wire of negligible weight winded around it as shown in
  the figure from below. By how much a force must be applied to pull the
  wire so that the cylinder spins and keeps in place?. Assume that the
  coefficient of friction is $0.3$ and the acceleration due gravity is $9.8\,\frac{m}{s^2}$ . The alternatives given in my book are as follows: $\begin{array}{ll}
1.&\textrm{3 N}\\
2.&\textrm{2.5N}\\
3.&\textrm{1.5N}\\
4.&\textrm{0.15N}\\
5.&\textrm{4.5N }\\
\end{array}$ I'm not sure exactly if I'm understanding this problem correctly. What I've attempted to do here was to assume that the condition which must be met is given by: $\sum ^n_{i=1}\tau_{i}=0$ Therefore: $-F\cos 37^{\circ}\cdot R-F\sin 37^{\circ}\cdot R + f_R\cdot R = 0$ Hence: $f_R=F\cos 37^{\circ}+F\sin 37^{\circ}=F\frac{4}{5}+F\frac{3}{5}$ $f_R=\frac{7}{5}F$ But: $f_R=\mu N$ $N=mg-F\sin 37^{\circ}$ $f_R=\frac{3}{10}(mg-F\sin 37^{\circ})=\frac{3}{10}\left(0.5\times 9.8-\frac{3F}{5}\right)$ Solving this thing yield: $\frac{3}{10}\left(0.5\times 9.8-\frac{3F}{5}\right)=\frac{7}{5}F$ Hence: $F=0.93\,N$ But it doesn't check with any of the alternatives given. What did I missunderstood?. Can someone help me here please?.","['physics', 'algebra-precalculus', 'classical-mechanics']"
3591208,Why does Logistic Regression need Normalized data,"I am trying to implement logistic regression in some problem, but while using normal data gives me some nan results. When I normalize the data I get correct results, so why does Logistic Regression need normalize data? Thanks.","['machine-learning', 'statistics', 'logistic-regression']"
3591243,How many vertices can the intersection of two convex polygons have?,"Given two convex polygons $p_0$ and $p_1$ with $n_0$ and $n_1$ vertices (assume $n_0 >= n_1$ without loss of generality) on a two-dimensional plane, what is the maximum number of vertices $n_{2,max}$ of the intersection polygon $p_2$ which contains the area both polygons share? Here are my thoughts: If the polygons are disjoint, the intersection is empty. $n_{2,min} = 0$ . If one polygon is inside the other, the inner polygon coincides with the intersection. $n_{2,max} >= n_0$ . The polygon $p_0$ with more vertices can cross polygon $p_1$ at most $2n_1$ times (each edge twice). Each crossing point is a vertex of the intersection. If the remaining points of $p_0$ are inside, the maximum number of vertices is $n_{2,max} = 2 n_1 + (n_0 - n_1) = n_0 + n_1$ . Am I missing something? Is there a situation with more than $n_0 + n_1$ vertices for the intersection? As a background: I have an algorithm to compute the intersection of two polygons, but I need to allocate enough memory for the intersection before computing it. My initial value was $2 (n_0 + n_1)$ but that seems to be more than required if the above is correct.","['geometry', 'polygons']"
3591313,Can anyone solve this hard differential equation involving a derivative squared?,"I have been trying to solve this diff. equation for quite some time now but haven't been able to do it correctly. It describes the lost height of the water $h$ at a certain time $t$ of a leaking reservoir. I have obtained this equation by using Torricelli's law as well as the law of continuity. At $t=0$ , the seal is removed and the reservoir starts leaking. Moreover, $A_O > A_G$ and to be less precise it can be assumed $A_O >> A_G$ , however, I would like to solve it as precisely as possible. The equation is: $$\frac{2gA_O^2}{A_O^2 - A_ G^2}h(t)+\frac{A_O^2}{A_G^2}\bigg(\frac{dh(t)}{dt}\bigg)^2 - \frac{2gA_O^2}{A_O^2 - A_G^2}H_0=0$$ with $h(0)=0$ . Additionally, $h(t)$ is bounded above by $H_0$ , is a strictly increasing function and thus also $\lim_{t \to \inf} h(t)=H_0$ The constants: $A_O$ is the surface of the top of the reservoir. $A_G$ is the surface area of the hole. $g$ is the gravitational acceleration. $H_0$ is the height of the water at t=0. The simplified formula would be: $$\lambda h(t)+\mu\left(\frac{dh(t)}{dt}\right)^2-\lambda H_0=0$$ And just to be clear $\bigg(\frac{dh(t)}{dt}\bigg)^2$ is simply the first derivative squared not the second derivative. I really hope someone can help me solve this!
If there is anything unclear or you want more information please ask I will check this post regularly. Thanks in advance!","['physics', 'ordinary-differential-equations']"
3591355,Probability of a group being finite,"Suppose $F_m := F[x_1, … , x_m]$ is a free group on $m$ generators $x_1, … , x_m$ and lets define Cayley ball $B_m^n := \{e, x_1, x_1^{-1}, … , x_m, x_m^{-1}\}^n$ as the set of all elements with Cayley length $n$ or less. Suppose $R_1, … , R_l$ are $l$ random elements chosen uniformly from $B_m^n$ . Then we can define a random group as $G(m, l, n) := \frac{F_m}{\langle \langle \{R_1, … , R_l\} \rangle \rangle}$ . Now will suppose that $m$ is fixed and $l = l(n)$ depends on $n$ . We say that the random group $G(m, l, n)$ belongs to a class of groups $\mathfrak{U}$ almost surely iff $\lim_{n \to \infty} P(G(m, l, n) \in \mathfrak{U}) = 1$ . A following theorem was proved by Ollivier: If $\lim_{n \to \infty} \frac{\ln(l(n))}{n(\ln(2m - 1))} > \frac{1}{2}$ then $G(m, l, n)$ is almost surely finite. If $\lim_{n \to \infty} \frac{\ln(l(n))}{n(\ln(2m - 1))} < \frac{1}{2}$ then $G(m, l, n)$ is almost surely infinite My question is: Is there some sort of exact expression for limit probability $\lim_{n \to \infty} P(G(m, l(n), n) \text{ is finite})$ for arbitrary  non-decreasing $l: \mathbb{N} \to \mathbb{N}$ ? I, personally think, that it is very likely to be of the form $$\lim_{n \to \infty} P(G(m, l(n), n) \text{ is finite}) = \lim_{n \to \infty} a^{-b^{\ln(2m - 1)n - 2\ln(l(n))}}$$ for some positive real numbers $a(m)$ and $b(m)$ . However, I do not know that for sure. That is just intuition mostly based on analogies with a somewhat similar ""phase transition theorem"" from a completely different field, that states: Suppose $G(n, p(n))$ is an Erdos-Renyi random graph with $n$ vertices and edge probability $p(n)$ . Then $\lim_{n \to \infty} P(G(n, p(n)) \text{ is connected}) = \lim_{n \to \infty} e^{-e^{\ln(n) - np(n)}}$","['infinite-groups', 'finite-groups', 'combinatorial-group-theory', 'group-theory', 'probability']"
3591363,Asymptotic expansion for $a_n=\inf_{1\leq k \leq n}|\sin(k)|$,"Can we obtain an asymptotic formula for sequences $$a_n=\inf_{1\leq k \leq n}|\sin(k)|,\ b_n=\sup_{1\leq k \leq n}|\sin(k)|$$ Moreover, what will the asymptotic formula be if we replace $\sin(x)$ by other trigonometric functions like: $$\cos(x), \tan(x), \cot(x), \sec(x), \csc(x)$$ This problem is simply out of interest, I'm not sure whether it's an open problem or not. Any kind of suggestions or references are welcomed. Thank you!","['diophantine-approximation', 'number-theory', 'asymptotics', 'pi', 'trigonometry']"
3591368,Global Tate Duality Exercise in Neukirch,"For $K$ a $\mathfrak p$ -adic number field, local Tate duality yields a non-degenerate pairing $$H^1(K, \Bbb Z/n\Bbb Z) \times H^1(K, \mu_n) \longrightarrow \Bbb Z/n\Bbb Z,$$ where $\mu_n$ is the group of $n$ -th roots of unity, given by $$(\chi, a) \mapsto \chi((a, \bar K|K)).$$ Here $H^1(K, A)$ stands for $H^1(\operatorname{Gal}(\bar K|K), A)$ , $\chi$ is a character $\chi: \operatorname{Gal}(\bar K|K) \longrightarrow \Bbb Z/n\Bbb Z$ , $a$ lies in $K^\times / K^{\times n}$ , and $(a, \bar K|K)$ is the local Artin symbol. If $n$ doesn't divide the characteristic of the residue field of $K$ then the orthogonal complement of the unramified cohomology $H^1_{nr}(K, \Bbb Z/n\Bbb Z) = H^1(\operatorname{Gal}(\tilde K|K), \Bbb Z/n\Bbb Z)$ is $H^1_{nr}(K, \mu_n) = H^1(\operatorname{Gal}(\tilde K|K), \mu_n).$ Now for a number field $K$ , this gives us a non-degenerate pairing of locally compact groups $$\prod'_{\mathfrak{p}} H^1(K_{\mathfrak{p}}, \mathbb{Z}/n\mathbb{Z}) \times \prod'_{\mathfrak{p}} H^1(K_{\mathfrak{p}}, \mu_n) \rightarrow \mathbb{Z}/n\mathbb{Z}$$ given by $$(\chi, \alpha) = \sum_{\mathfrak{p}} \chi_{\mathfrak{p}}(\alpha_{\mathfrak{p}}, \bar{K_{\mathfrak{p}}}/K_{\mathfrak{p}}),$$ where the restricted products are taken with respect to the unramified cohomology groups defined above. It turns out that the images of $$H^1(K, \mathbb{Z}/n\mathbb{Z}) \rightarrow \prod'_{\mathfrak{p}} H^1(K_{\mathfrak{p}}, \mathbb{Z}/n\mathbb{Z})$$ and $$H^1(K, \mu_n) \rightarrow \prod'_{\mathfrak{p}} H^1(K_{\mathfrak{p}}, \mu_n)$$ are mutual orthogonal complements with respect to the global pairing. So far, so good. Now given the above, Neukirch set the following exercise on p. 404 of Algebraic Number Theory : If $S$ is a finite set of places of $K$ , then the map $$H^1(K, \mathbb{Z}/n\mathbb{Z}) \rightarrow \prod'_{\mathfrak{p} \in S} H^1(K_{\mathfrak{p}}, \mathbb{Z}/n\mathbb{Z})$$ is surjective if and only if the map $$H^1(K, \mu_n) \rightarrow \prod'_{\mathfrak{p} \notin S} H^1(K_{\mathfrak{p}}, \mu_n)$$ is injective. I know this can be proved using Tate-Poitou Duality as in Neukirch-Schmidt-Wingberg, but I have made no progress trying to prove this given only the facts outlined above. I would appreciate any and every hint.","['class-field-theory', 'number-theory', 'galois-cohomology', 'algebraic-number-theory']"
3591411,"Showing that $P (|X_1| < \Gamma , |X_2| < \Gamma)$ is increasing in $|\rho|$","Assume $(X_1,X_2 )^T$ is mean $0$ bivariate normal distributed with covariance matrix $\Sigma = \left (\begin{matrix} 1 & \rho \\ \rho & 1 \end{matrix} \right)$ and let $\Gamma > 0$ a positive constant. Then i would like to show that $P (|X_1| < \Gamma , |X_2| < \Gamma)$ is increasing in $|\rho|$ . Any tips? I already tried to simply use the integral representation of the probability, but could not show it.","['correlation', 'normal-distribution', 'probability']"
3591425,Limit of $\int_0^1 \frac{x^n-x^{2n}}{1-x}\text{d}x$,I have to find the value of $$\lim_{n\to\infty} \int_0^1 \frac{x^n-x^{2n}}{1-x}\text{d}x$$ with $n\in\mathbb{N}$ . The result is $\ln 2$ but i find no way to prove it. I thought that $$H_n=\int_0^1 \frac{1-x^n}{1-x}\text{d}x$$ would have helped but I came up with nothing. The fact that the result is the the same as the sum of alternating harmonic series made me think i should find a way to make a $(-1)^k$ pop out from somewhere but all my attempts failed… Any idea?,"['integration', 'improper-integrals', 'definite-integrals', 'harmonic-numbers', 'calculus']"
3591452,stalk of a direct image sheaf under a finite morphism,"Let $f: X \rightarrow Y$ be a finite surjective morphism of schemes, and $\mathscr{F}$ a coherent sheaf of $\mathscr{O}_X$ -modules on $X$ . I find it confusing to understand the stalk $(f_* \mathscr{F})_y$ at $y \in Y$ in terms of the finite number of stalks $\mathscr{F}_x$ where $f(x) = y$ . A section of $(f_* \mathscr{F})_y$ can be restricted to be in $\mathscr{F}_x$ , so if $f^{-1}(y) = \{ x_1, ..., x_n \}$ , I see that $$(f_* \mathscr{F})_y \longrightarrow \underset{i = 1, ..., n}{\oplus} \mathscr{F}_{x_i},$$ but is there a better way to understand the structure of $(f_* \mathscr{F})_y$ in terms of all $\mathscr{F}_{x_i}$ ?",['algebraic-geometry']
3591548,Representation and expected value of a certain simple r.v.,"Let $(\Omega,\mathcal{F},P)$ denote the probability space on which all of the following random variables are defined and $\omega\in\Omega$ . Let $X$ denote a non-negative random variable and $X_n,n\geq 1$ a random variable which is defined as follows: $$X_n (\omega) = \sum_{k=1}^{n2^n} \frac{k - 1}{2^n}\cdot\mathbf{1}_{X (\omega)\in\left[\frac{k - 1}{2^n}, \frac{k}{2^n}\right)} + n\cdot\mathbf{1}_{X (\omega)\in \left[n,\infty\right)}.\tag{$\ast$}$$ I see that $X_n$ is simple. I want to find its expected value. In Billingsley (1995): Probability and Measure it says on p. 68 (Equation (5.2)) that a simple random variable $Y$ has the form $$Y (\omega) = \sum_i y_i \mathbf{1}_{\omega\in A_i},\tag{$\ast\ast$}$$ where the $y_i$ are the values taken by $Y$ and the $A_i$ form a partition of $\Omega$ . On p. 76 (Equation (5.15)) the expected value of a simple random variable in the form $(\ast\ast)$ is given by $$E\left[Y\right] = \sum_i y_i P(A_i).$$ My understanding is that $(\ast)$ is not in the form $(\ast\ast)$ because of $X (\omega)$ instead of $\omega$ as argument of the indicator functions. Is that correct? How can I obtain $(\ast)$ in the form $(\ast\ast)$ ? Or is that not a path to the expected value of $(\ast)$ ?","['probability-theory', 'probability', 'random-variables']"
3591614,"A generalization of Feit–Thompson conjecture, for square-free integers","Few weeks ago I wondered about if the following conjecture is in the literature or well if it is possible to find a counterexample. I evoke a generalization of a well-known conjecture, I mean the Feit–Thompson conjecture from Wikipedia. An integer $n>1$ is square-free or squarefree if has no repeated prime factors. For example $n=15$ is squarefree, while than $12$ has repeated prime factors. Conjecture. There are no distinct square-free integers $p$ and $q$ (both greater than $1$ ) with $\gcd(p,q)=1$ such that $\frac{p^q-1}{p-1}$ divides $\frac{q^p-1}{q-1}$ . Question. What work can be done about the veracity of this conjecture? Since Feit–Thompson conjecture seems a particular case of this, then I'm asking if previous conjecture is in the literature, or well if you've some approach or heuristic about the Conjecture or well if it is possible to find a counterexample. Many thanks. I wrote a Pari/GP program and I've tested the Conjecture , let's say for the segments of integers $2\leq p,q\leq 2000$ . I hope that there were not mistakes and this post is interesting.","['divisibility', 'number-theory', 'elementary-number-theory', 'reference-request', 'group-theory']"
3591629,Proposition IV.2.3 in Hartshorne,"(2.2) Here was referring to the exact sequence $$0\to f^*\Omega_Y\to\Omega_X\to \Omega_{X/Y}\to 0.$$ My concern is: as $\Omega_{X/Y}\cong\mathscr{O}_R$ , I cannot see why tensoring with $\Omega_X^{-1}$ preserves $\mathscr{O}_R$ ONLY but not for the others. Thanks in advance for answering.","['algebraic-geometry', 'sheaf-theory']"
3591632,"${\rm Hom}(\textbf{G},\textbf{Ab})$ is the category of $G$-modules","This question is related to an exercise in Bosch's Algebraic Geometry (Chap. 4.5, Ex. 2) Fix a group $G$ and write $\textbf{G}$ for the one-point category with morphisms given by the group $G$ and $\textbf{Ab}$ for the category of abelian groups. The claim is that ${\rm Hom}(\textbf{G},\textbf{Ab})$ is the category of $G$ - modules, i.e. $\mathbb{Z}$ -modules with a $G$ - action. For clarification, given categories $C_1,C_2$ , ${\rm Hom}(C_1,C_2)$ is the category of all covariant functors $C_1\rightarrow C_2$ , where a morphism $\phi:F\rightarrow G$ between two such functors is a functorial morphism. These are my thoughts so far: A functor $F:\textbf{G}\rightarrow \textbf{Ab}$ defines an abelian group $F(G)$ . I want to define and action $g\cdot x$ for $g\in G$ and $x\in F(G)$ induced by $F$ , but I don't know how. What I can define is an action of ${\rm Hom}_G(G,G)$ on $F(G)$ by $f\cdot x:= F(f)(x)$ for a morphism $f:G\rightarrow G$ and $x\in F(G)$ . But this is not what I want. Can anyone give me a hint?","['category-theory', 'modules', 'algebraic-geometry', 'group-theory', 'commutative-algebra']"
3591699,"Show $(\vec x\cdot\nabla)\vec F=t \frac{\partial F}{\partial t}$ where $\vec F(\vec x,t)=\vec B(t \vec x)$","I keep missing the factor of $t$ . My working: $$(\vec x\cdot\nabla)\vec F=(x_i \vec e_i\cdot\vec e_j\frac{\partial}{\partial x_j})F_k\vec e_k=x_j\frac{\partial F_k}{\partial x_j}\vec e_k$$ and $$\frac{\partial\vec F}{\partial t}=\frac{\partial}{\partial t}\vec B(t\vec x)=(\vec x\cdot\nabla B_i(t\vec x))\vec e_i=(x_j\vec e_j\cdot\frac{\partial B_i(t\vec x)}{\partial x_k}\vec e_k)\vec e_i=x_k\frac{\partial B_i(t\vec x)}{\partial x_k}\vec e_i=x_k\frac{F_i}{x_k}\vec e_i$$ which would seem to suggest that the two lines are equal, when there should be a factor of $t$ between them. I'm not sure I have differentiated correctly on the second line, but I can't see a better way. What is the correct way to partial differentiate a time dependent vector field with respect to time?","['vector-fields', 'multivariable-calculus', 'vector-analysis']"
3591746,True statement with a false contrapositive? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Statement: $\forall m, n\in \mathbb{Z}$ , $m$ is even and $n$ is even $\implies m\cdot n$ is even (true) Contrapositive: $\forall m, n\in \mathbb{Z}$ , $m\cdot n$ is odd $\implies m$ is odd or $n$ is odd  (false) ??? it should be true where $m\cdot n$ is odd $\implies m$ is odd and $n$ is odd, but that's not the contrapositive. I'm confused can anyone help me?","['elementary-number-theory', 'logic', 'discrete-mathematics']"
3591796,What weights should I buy for my gym? (a case of integer partitioning),"I am trying to solve for possible combinations of weights that would be appropriate for use in my home gym. I have been told that this is a case of integer partitioning, but I am not sure how to solve it and would appreciate some guidance (or even better a solution!). Given that I am quarantined at home, I want to buy weight plates for use in a home gym. In my gym, I have a 45lb bar. I attach weight plates to the bar in pairs, so I always purchase and use plates in pairs. So, if I buy 1 pair of 2.5lb plates, I would be able to use the bar at weights of 45lb or 50lb (since plates must always be used in pairs). I want to determine how many pairs of 2.5lb, 5lb, 10lb, 25lb, and 45lb plates I need to ensure that I can use my bar at weights of 45 to 405lb in 5lb increments (so options would include 45, 50, 55, 60,..., 395,400, 405). I want to minimize the number of smaller plates that I buy (so mostly get 45s and then get some minimum number of pairs of each of the smaller plates to ensure all possible combinations are attainable). How might I approach this problem?","['integer-partitions', 'number-theory', 'combinatorics', 'applications']"
3591805,"Understanding what $\phi(f)(b,c) = f(b)(c)$ means","Consider $\phi: (A^B)^C \to A^{B \times C}$ given by $\phi(f)(b,c) = f(b)(c)$ . I'm trying to understand the definition of $\phi$ as given above. Suppose $k \in A^B$ . Then $k: B \to A.$ Now let $f \in (A^B)^C $ . Then $f: C \to \{k\}$ meaning $f(c) = k_i$ for some $i$ and so $f(c)$ is a function and not just a value. Thus the notation $f(c)(b)$ makes sense...but I still can't make sense of what's going on with the definition of $\phi.$ What's the reasoning behind this definition if it's an acceptable question? Thanks.","['elementary-set-theory', 'definition', 'notation', 'discrete-mathematics']"
3591829,"Evaluate : $I=\int_0^{\infty}\frac{\ln (1+ax+x^{2})}{1+x^2}\,dx$","Does the following integral have a closed form : $$I=\displaystyle\int\limits_0^{\infty}\frac{\ln (1+ax+x^{2})}{1+x^2}dx$$ Where $|a|≤1$ , I was trying using Feynman's trick. Define $$I(b)=\displaystyle\int\limits_0^{\infty}\frac{\ln (b(1+x^{2})+ax)}{1+x^2}dx$$ Differentiating with respect to $b$ we get : $$I'(b)=\displaystyle\int\limits_0^{\infty}\frac{1}{b+ax+bx^{2}}dx$$ $$=2\left(\frac{π}{2\sqrt{4b^{2}-a^{2}}}-\frac{\arctan \left(\frac{a}{\sqrt{4b^{2}-a^{2}}}\right)}{\sqrt{4b^{2}-a^{2}}}\right)$$ Known : $$\displaystyle\int \frac{1}{\sqrt{4b^{2}-a^{2}}}db=\frac{\log (2 x + \sqrt{-a^2 + 4 x^{2}})}{2}$$ my problem in this integral : $$\displaystyle\int\limits_0^{1}\frac{\arctan \left(\frac{a}{\sqrt{4b^{2}-a^{2}}}\right)}{\sqrt{4b^{2}-a^{2}}}db=?$$ Of course here $$I=I(1)=I(0)+\int\limits_0^{1}I'(b)db$$ $$I(0)=\frac{π\ln a}{2}$$ I already waiting your hints or solution.","['integration', 'definite-integrals', 'closed-form']"
3591844,Chain of circles internally tangent to an ellipse.,"I tried to get an answer to this question (which was hastily closed) but couldn't find a proof, so I decided to ask it again, adding some of my efforts. Suppose we have a finite sequence of $n$ circles ( $n\ge10$ , see figure below) whose centres lie on the major axis of an ellipse. All circles are internally tangent to the ellipse and each circle is also externally tangent to the preceding and following circle (if they exist). If $r_1$ , $r_2$ , ..., $r_n$ are the radii of these circles, prove that: $$
r_7(r_1 + r_7) = r_4(r_4 + r_{10}).
$$ If $x_0$ , $x_1$ , ..., $x_n$ are the abscissae of the intersection points between the circles and the major axis (taking as origin the ellipse centre, see figure above), then it is not difficult to find a recursive relation for $x_k$ . Let $a$ , $b$ be the semi-major axis and semi-minor axis of the ellipse, $A$ and $B$ its foci, $O$ its centre and $c=AO=BO=\sqrt{a^2-b^2}$ . If $C_k$ is the centre of $k$ -th circle and $P_k$ one of its tangency points with the ellipse, then radius $P_kC_k$ is the normal to the ellipse at $P_k$ and thus the bisector of $\angle AP_kB$ . It follows from the length of bisector formula that $$
P_kC_k={b^2\over a^2}\sqrt{AP_k\cdot BP_k}={b^2\over c^2}\sqrt{AC_k\cdot BC_k}=
{b^2\over c^2}\sqrt{c^2-c_k^2},
$$ where $c_k$ is the abscissa of centre $C_k$ . 
Inserting here $P_kC_k=(x_{k}-x_{k-1})/2$ and $c_k=(x_{k}+x_{k-1})/2$ , 
then squaring both sides and rearranging, one finds: $$
x_k^2+x_{k-1}^2-2(2e^2-1)x_kx_{k-1}=4e^2b^2,
$$ where $e=c/a$ is the eccentricity of the ellipse. From the above recursive equation one can find, once $x_0$ is given, all $x_k$ and thus compute $r_k=(x_{k}-x_{k-1})/2$ for all values of $k$ . I used these   results with GeoGebra to draw the first figure, and could numerically check that the formula to prove holds for any value of $x_0$ . Nonetheless, I couldn't obtain a real proof of that formula using algebra, hence I believe I'm missing a simpler way to find those radii. Any idea to prove the statement is welcome.","['conic-sections', 'circles', 'geometry', 'sangaku']"
3591896,When does one show a function is well defined?,"Sometimes in Linear Algebra, we just use linear transformations, and other times we have to check to make sure they are well defined. Since I'm not an experienced mathematician or anything, I can't seem to see the fine line between needing to show if a linear transformation is well defined or not. Could anyone who understands the nuance please explain it to me? Edit: For example, when we began working with quotient spaces we had to show addition was well defined. In problems, if we wanted to construct a linear transformation from a quotient space to a vector space, we had to show that the linear transformation was well defined and linear. In the past, however, when we constructed linear transformations between vector spaces, we just had to show that they were linear, and we didn't talk about it being well defined.","['functions', 'linear-algebra', 'linear-transformations']"
3591928,If the value of integral in the image below is π then what is the value of y?,"I could not simplify $$
\int_0^1 \sqrt{-1 + \sqrt{\frac{1+y}{x} - y}}\ dx
$$ I tried integration it in an online integrator but trust me the result is seriously daunting to be back traced to $\pi$ as a value, thereby determining $y$ .  So I am looking for a rather clever trick to get through this one.","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'partial-fractions']"
3591932,What is the derivative of $x'(A+tB)^{-1}x$ with respect to $t$?,"Can you please help me with the derivative of $x'(A+tB)^{-1}x$ with respect to $t$ ? ( $x$ is a vector, $A$ and $B$ are $n$ by $n$ matrices and $t$ is a scalar) I'm not sure, but is it $x'Fx$ where $F=-(A+tB)^{-1}(B)(A+tB)^{-1}$ ? [I applied $\frac{\delta C^{-1}}{\delta t} = -C^{-1}\frac{\delta C}{\delta t}C^{-1}$ Where here $C=(A+tB)$ , the vectors $x$ remains as it is.] Thanks in advance!","['matrices', 'matrix-calculus', 'derivatives']"
3591934,Bayes Theorem application for second testing based on first test result,"I am currently trying to answer this question and am though a bit confused on how to apply Bayes Theorem when a second test is performed based on the result of the first. A test to determine who is under the influence of a drug has a probability of 0.8 of being correct (i.e for both positive and negative results). If the test is positive, a second different test is carried out. The second test always correctly detects if the patient is in fact not under under the influence of the drug, but has a 10% error rate with drug users/under the influence. If 20% of the patients tested are actually users/under the influence we are asked to calculate: a) Proportion of patients that have to be given the second test (i.e proportion of testing positive on the first test)
b) probability patients testing positive on the first test are really under the influence/drug users
c) probability that patients testing negative on the second test are actually under the influence/drug users; For a) I am not sure I've arrived at the correct solution,as its 30% more than the population of 20% who are actually users. d - drug user/under the influence 
c - not under the influence/not a drug user $P(d|+) = \frac{P(+|d)*P(d)}{P(+)*P(+|d) + P(+|c)*P(c)}  = \frac{0.2*0.8}{(0.2*0.8 + 0.2*0.8)} = 0.50 $ i.e % 50% of patients will test positive on first test b) I am immediately confused by this question and c). I have followed the rationale of this question Conditional probability and testing twice , and found the answer below: P1 = first test
P2 = second test Then the desired probability is $$\Pr[P_2 \mid P_1] = \frac{\Pr[P_2 \cap P_1]}{\Pr[P_1]} = \frac{\Pr[P_2 \cap P_1 \mid d]\Pr[d] + \Pr[P_2 \cap P_1 \mid c]\Pr[c]}{\Pr[P_1 \mid d]\Pr[d] + \Pr[P_1 \mid c]\Pr[c]} = \frac{(0.8+0.9)*0.2 + (0.8+1)*0.8}{0.8*0.2 + 0.8*0.8} = 5.5625 $$ This is obviously not correct, I am not sure where I went wrong here. Any hints/answers are welcomed I am new to Bayesian Theory.","['statistics', 'conditional-probability', 'bayesian', 'bayes-theorem', 'probability']"
3591983,Find Coefficient in expansion of $(x-1)^k(x+1)^{d-k}$,"While thinking about a problem relating to the bilinear transform , I came across the following question: Given $p$ with $0 \leq p \leq d$ , find the coefficient of $x^p$ in the expansion $(x-1)^k(x+1)^{d-k}$ . Ideally, I'd like an answer in some closed form. Here are my thoughts so far: after taking a ""brute force"" approach via binomial expansion, I have found that $$
(x-1)^k(x+1)^{d-k} = \sum_{p=0}^d \left[\sum_{j=0}^{d-k} (-1)^{k+j-p} 
\binom{k}{p-j} \binom{d-k}{j}
\right]x^p,
$$ which I wasn't able to simplify further.  So, our coefficient should be equal to $\sum_{j=0}^{d-k} (-1)^{k+j-p} 
\binom{k}{p-j} \binom{d-k}{j}$ . Another thought is to compute the coefficient as $\frac 1{p!} f_{k,d}^{(p)}(0)$ , where $f_{k,d}(x) = (x-1)^k(x+1)^{d-k}$ .  Taking derivatives leads to the interesting recurrence $$
f_{k,d}' = kf_{k-1,d-1} + (d-k)f_{k,d-1}.
$$ So in other words, if $c^p_{k,d}$ denotes the desired coefficient, then we have $$
c_{k,d}^p = \frac 1{p!}f^{(p)}_{k,d}(0) = 
\frac{k}{p!}f_{k-1,d-1}^{(p-1)} + \frac{d-k}{p!}f_{k,d}^{(p-1)}\\
=\frac{k}{p}c_{k-1,d-1}^{p-1} + \frac{d-k}{p}c_{k,d-1}^{p-1}.
$$ I wasn't able to see a clear path forward from there. Any ideas here would be appreciated.","['algebra-precalculus', 'combinatorics', 'closed-form', 'discrete-mathematics']"
3592011,First moment of the measure and interval escaping to infinity,"Let $\mu$ be a finite Borel measure on $\mathbb{R}$ , and $$L = \lim_{t \to +\infty} \int_{t-1}^{t+1} x d \mu (x).$$ If $\displaystyle  \int_\mathbb{R} |x| d \mu (x) < \infty$ , then by Dominated Convergence $L<\infty$ . What would be an example of $\mu$ for which $L=\infty$ ? I tried $$d \mu (x) = \frac{d x}{(1+|x|)^p},$$ then the conditions $\mu$ being finite and its first moment infinite imply $p \in (1,2]$ , however for this $L<\infty$ .","['measure-theory', 'convergence-divergence', 'real-analysis']"
3592022,Mixing time of the random walk on $\mathbb{Z}_{2} \times \mathbb{Z}_{3} \times \cdots \times \mathbb{Z}_{n}$,"For an integer $k \geq 2$ , the simple random walk on the cycle $\mathbb{Z}_{k}$ proceeds by moving one step clockwise or counterclockwise, each with probability $1/2.$ I'm interested in the random walk on a product of cycles $\mathbb{Z}_{2} \times \mathbb{Z}_{3} \times \cdots \times \mathbb{Z}_{n}$ which moves by first choosing a coordinate $k \in \{2,\dots,n\}$ uniformly at random, and then taking a step according to the simple random walk in $\mathbb{Z}_{k}.$ If we make this walk lazy, it is aperiodic, and it is also easy to verify that it is irreducible and has uniform stationary distribution. I'm looking for asymptotically correct bounds on the mixing time of this random walk, where mixing is defined in terms of total variation distance as in Markov Chains and Mixing Times by Levin and Peres. A coordinate-wise coupling argument (à la that given in the solution to exercise 5.4 in MC&MT) gives that for any $\varepsilon>0$ , the mixing time $t_{\text{mix}}(\varepsilon)$ is at most $\frac{1}{\log(4)}n^{3}\log(n/\varepsilon)$ , where the logs are natural logs. This is of the right order, but I believe that the constant $1/\log(4)$ is too big. Using a slight generalization of Wilson's Method (see section 13.5 of the text), I obtained that for any $\varepsilon>0$ , the mixing time $t_{\text{mix}}(\varepsilon)$ is at least $(1-o(1))\frac{1}{2\pi^{2}}n^{3}\log(n).$ Based on the mixing times of related chains, I think the constant $\frac{1}{2\pi^{2}}$ at least has the right ""flavor,"" if it isn't already correct. At the very least, we know the mixing time is of order $n^{3}\log n.$ Of course, the question I'd like to answer is what is the correct constant? If my hunch about the upper bound being too large is correct, answering this requires obtaining an upper bound with a constant less than $\frac{1}{\log(4)}.$ What method(s) can I use to get a better upper bound? I am guessing that I need some spectral method. Note that the usual spectral upper bound $t_{\text{mix}}(\varepsilon) \leq \frac{1}{\text{spectral gap}}\log\frac{1}{\varepsilon \pi_{\text{min}}}$ (where $\pi_{\text{min}}$ is the smallest probability in the stationary distribution) is not helpful here: the spectral gap is of order $n^{-3}$ , but since $\pi_{\text{min}}=1/n!$ the bound is $O(n^{3}\log(n!/\varepsilon)).$ $\mathscr{l}^{2}$ methods like those in section 12.4 of the text seem difficult to apply, since the eigenvalues are complicated. A final question would be, can the lower bound be improved? Any insight would be appreciated!","['spectral-graph-theory', 'markov-chains', 'asymptotics', 'discrete-mathematics', 'probability']"
3592093,Interior $H^2$ regularity (proof),"The problem is related to the proof of the interior $H^2$ regularity theorem from Evans's book Partial Differential Equations (sec 6.3). In the proof, I am having difficulty in understanding how the following inequality was deduced, $$|A_2|\leq C \int_U \zeta |D_k^hDu||D_k^hu| + \zeta|D_k^hDu||Du| + \zeta|D_k^hu||Du|dx,$$ where $$
A_2 = \sum_{i,j=1}^n \int_U [a^{ij,h}D_k^hDuD_k^hu2\zeta D\zeta + (D_k^ha^{ij}) D_k^hDu\zeta^2 + (D_k^ha^{ij}) D_k^hu2\zeta D\zeta   ]dx
$$ and $D_k^h$ is the difference quotient. I thought triangular inequality for integrals was used here, but I do not understand how terms like $\zeta^2$ disappears. I am also having difficulty in understanding on how after application of Cauchy's inequality, the Integral space changes from $U$ to $W$ ? Here are snapshots to the Theorem and Problem . Thanks in advance!","['regularity-theory-of-pdes', 'partial-differential-equations', 'functional-analysis', 'real-analysis']"
3592118,How can I prove that $\varphi(t)=e^{-|t|^{\alpha}}$ is not a characteristic function for $\alpha > 2$,"I am trying to prove that for $\alpha > 2$ this is not a characteristic function $$
\varphi(t)=e^{-|t|^{\alpha}}
$$ Suppose $t > 0$ . Differentiating twice gives us $$
(e^{-t^{\alpha}})^{(2)} = \alpha^2e^{-t\alpha}t^{2\alpha -2}-\alpha e^{-t^{\alpha}}(\alpha-1)t^{\alpha-2}
$$ Now, $\mathbb{E}X^2=\frac{\varphi^{(2)}(0)}{i^2}$ = 0. Beacuse $0 \leq \text{Var}X \leq \mathbb{E}X^2 = 0$ we know that $X$ has a variance of $0$ which means X is constant and equal to $\mathbb{E}X$ . The same happens when $t<0$ . How can I proceed? Is it a contradiction already? I could not find how does a characteristic function of a constant variable look like.","['characteristic-functions', 'proof-writing', 'probability-theory']"
3592151,Riemann-Hilbert correspondence versus Simpson correspondence,"Let us assume that $X$ is a connected, smooth complex algebraic variety. Then the Riemann-Hilbert correspondence tells us that the functor which sends a flat connection with regular singularities on a vector bundle of $X$ to its asocciated monodromy representation is an equivalence of categories. Furthermore, the Simpson correspondence tells us that there is an equivalence of categories between the category of complex representations of the fundamental group of curves and the category of semi-stable Higgs bundles with trivial Chern class. Its seems to me that the Simpson correspondence should be a consequence of the Riemann-Hilbert correspondence, especially since the category of Higgs bundles is roughly the tangent category of the category of vector bundles. However, based on the number of papers written on this, and the fact that Simpson wrote a ICM note on this, this is clearly not the case. So I guess I must miss what the additional content of the Simpson correspondence is. Could someone help me?","['fundamental-groups', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
3592191,Some properties of zero-sets and cozero-sets,"Definition A subset $Y$ of a topological space is zero-set if there exist a continuous real function $f:X\rightarrow\Bbb{R}$ such that $Y=f^{-1}(\{0\})$ and so we say that a subset $Z$ of $X$ is cozero-set if $X\setminus Z$ is zero-set. So now we prove the following statements: any zero-set is closed and any conul set is open and moreover the sets $\varnothing$ and $X$ are respectively zero-set and cozero-set; the finite union and the finite intersection of zero-sets or cozero-sets is respectively zero-set or cozero-set; a $T_1$ space $X$ is completely regular iff for any $x\in X$ the collection of cozero-set neighborhoods of $x$ is a local basis for $x$ ; if $X$ is completely regular then the collection of cozero open sets is a basis. Proof . Since in the above definition $f$ is a continuous function and since $\{0\}$ is closed in the euclidean topology -indeed $(\Bbb{R},\mathcal{T}_e)$ is a $T_1$ space- it result that $f^{-1}(\{0\})$ is closed and so a zero-set is closed and thus a cozero-set is open. Then since the costant function $f:X\owns x\rightarrow 0\in\Bbb{R}$ is trivially continuous and since $f^{-1}(\{0\})=X$ it result that $X$ is zero-set and $\varnothing$ is cozero-set. So we proved 1. Now we suppose that $X$ a comletely regular space and so let be $x\in U$ a open neighborhood of $x$ : since $X$ is a Tychonoff space we can claim that there exist a continuous function $f:X\rightarrow[0,1]$ such that $f(X\setminus U)=\{0\}$ and so this means that every open sets are cozero-set and so the local basis of open neighborhood of $x$ is a local basis of cozero-sets. Therefore now we suppose that $X$ is a $T_1$ space such that any its point have a cozero-set local basis and so let be $F$ a closed set and $x\notin F$ : since $X\setminus F$ is an open neighborhood of $x$ there exist a cozero-set neighborhood $U$ of $x$ such $x\in U\subseteq X\setminus F$ , from which it result $F\subseteq X\setminus U$ ; and since $U$ is cozero-set there exist a coutinuous function $f:X\rightarrow\Bbb{R}$ such that $(X\setminus U)=f^{-1}(\{0\})$ and so it is clear that $f(F)=(\{0\})$ and $f(x)\neq 0$ and so for what I proved here -is it correct?- we can claim that $X$ is a completely regular space. So we proved 3. Now we suppose that $X$ is completely regular and so for what we prove above we know that for any $x\in X$ the collection of open cozero-set neighborhoods $\mathcal{B}(x)$ of $x$ is a local basis for $x$ : so it is clear that the collection of $\mathcal{B}:=\bigcup_{x\in X}\mathcal{B}(x)$ is a topological basis for $X$ . So we proved 4. As you can observe unfortunately I can't prove the second statement and then in the first statement it result that $X$ is cozero-set and $\varnothing$ is zero-set whereas I proved the contrary. So I ask to prove the second point and to explain if what it is written in the final part of first point is a typo. Then I ask if the proof of 3 and 4 points is correct. Could someone help me, please?",['general-topology']
3592240,A three-parameters identity involving Stirling numbers of both kinds,"Let $n, m, k $ be three natural numbers, ${n \brack k}$ and ${n \brace k}$ the Stirling numbers of first and second kind respectively. We have: $$ \tag{*} {n-1 \choose m}{n-m \brack k}= \sum_i (-1)^{i-m}{k-1+i
 \choose k-1}{i \brace m}{n \brack i+k} $$ where the bounds for $i$ in the sum on the rhs don't need to be specified as there is only a finite number of values of $i$ whose corresponding summand is non-zero and the sum is understood over all such $i$ . This identity can be verified numerically and can be derived from another three parameters identity involving the second kind of Stirling numbers only- namely  Eq. (6.28) in Concrete Mathematics Second Edition, R. L. Graham, D. E. Knuth, O. Patashnik ) $$ \tag{**} {\ell+m \choose \ell}{n \brace \ell+m}= \sum_k {k\brace \ell}{n-k \brace m}{n \choose k} $$ which is obtained rather easily via the exponential generating functions of ${n \brace l+m}$ , ${n \brace m}$ and ${n \brace l}$ . Indeed, if we replace $m$ by $-m$ and $n$ by $-n$ in (**), taking into account that ${-a \brace -b}$ = ${b \brack a}$ and ${-n \choose k}=(-1)^k{n+k-1\choose k}$ , we obtain \begin{align*} {\ell-m \choose \ell}{-n \brace \ell-m}&= \sum_k {k\brace \ell}{-n-k \brace -m}{-n \choose k} \\
(-1)^\ell{m-1 \choose \ell}{m- \ell \brack n}&= \sum_k {k\brace \ell}{m \brack n+k}(-1)^k{n+k-1 \choose k}\end{align*} which is (*) after the appropriate change of notation. But in Concrete Mathematics , the identity (**) is given under the condition $\ell,m,n \ge 0$ , so I am note sure whether it is licit to do such negation of the indices. Then my question is: how can we derive (*) directly, without resorting to (**). Maybe with generating functions, coefficient extractors or things like that?","['summation', 'combinatorics', 'stirling-numbers', 'generating-functions']"
3592272,Locus of segments of length $n$ which form an $n^\circ$ angle with the $x$-axis,"So basically, I was wondering what would happen if you take the locus of line segments of length $n$ that make a $n^\circ$ angle with respect to the $x$ axis. I did this, and this is what I got: I was wondering if there is any properties of this shape and if it is well known in any field of study?","['trigonometry', 'locus', 'algebra-precalculus', 'geometry']"
3592286,"When $\mathbb{C}[u,v]$ is a UFD, for special $u,v \in \mathbb{C}[t]$?","Let $f=f(t) \in \mathbb{C}[t]$ be a non-scalar polynomial.
Write $f=a_{2n}t^{2n}+a_{2n-1}t^{2n-1}+\cdots+a_2t^2+a_1t+a_0$ ,
where $a_j \in \mathbb{C}$ . The map $\iota: t \mapsto -t$ is an involution on $\mathbb{C}[t]$ , namely, a $\mathbb{C}$ -algebra automorphism of degree two. Denote the set of symmetric elements w.r.t. $\iota$ by $S$ and 
the set of skew-symmetric elements w.r.t. $\iota$ by $K$ .
Notice that even polynomials are symmetric elements w.r.t. $\iota$ ,
while odd polynomials are skew-symmetric elements w.r.t. $\iota$ . Write $f=s+k$ , where $s \in S$ and $k \in K$ . Clearly, $\mathbb{C}[f] \subseteq \mathbb{C}[t]$ is a UFD ( $\mathbb{C}[f]$ is isomorphic to $\mathbb{C}[t]$ ). Question 1: Is it possible to find a general form of $f$ such that $R_{f}:=\mathbb{C}[s,k]$ is a UFD? Partial answer: In the following four cases $R_{f}$ is a UFD: (i) $k=0$ , so $f=s \in S$ , and then $R_{f}=\mathbb{C}[s,0]=\mathbb{C}[s] \cong \mathbb{C}[t]$ . (ii) $s=0$ , so $f=k \in K$ , and then $R_{f}=\mathbb{C}[0,k]=\mathbb{C}[k] \cong \mathbb{C}[t]$ . (iii) $k=t$ , so $R_{f}=\mathbb{C}[s,t]=\mathbb{C}[t]$ . (iv) $s=k^{2m}$ for some $m$ ,
so $R_{f}=\mathbb{C}[k^{2m},k]=\mathbb{C}[k] \cong \mathbb{C}[t]$ .
For example, $s=\lambda t^6$ , $k=\mu t^3$ , so $R_{f}=\mathbb{C}[\lambda t^6,\mu t^3]=\mathbb{C}[t^3] \cong \mathbb{C}[t]$ . Question 1': Are there additional cases? Non-example: For $f=t^3+t^2$ , $R_{f}=\mathbb{C}[t^2,t^3]$ is not a UFD, as $t^2t^2t^2=t^3t^3$ shows. Remark: This question is relevant, especially the comments of Mohan and user26857,
saying the following: ""If $R$ is Noetherian, integrally closed and dimension one $\mathbb{C}$ -subalgebra of the polynomial ring, it is isomorphic to $\mathbb{C}[t]$ "". Therefore, here, if $R_{f}$ is not a UFD, then it is not isomorphic to $\mathbb{C}[t]$ , hence $R_{f}$ is not integrally closed. On the other hand, if $R_{f}$ is integrally closed, then it equals its integral closure, which is of the form $\mathbb{C}[g]$ for some $g \in \mathbb{C}[t]$ (= this result appears in the paper of Paul Eakin ""A note on finite dimensional subrings of polynomial rings""). Therefore, $R_{f}$ is isomorphic to $\mathbb{C}[t]$ , hence it is a UFD. Summarizing, my questions 1 and 1' are equivalent to the following question: Question 1'': Let $s \in S$ (= even polynomials), $k \in K$ (= odd polynomials). Is it possible to find general forms of $s$ and $k$ such that $R_{s+k}=\mathbb{C}[s,k]$ is integrally closed? (Are cases (i) , (ii) and (iii) the only ones?). Important remark to question 1'': This question and its answer are highly relevant. Question 2: I guess that questions 1 and 1' are not too difficult (perhaps some algebraic geometry is needed?),
  so what if we replace $\mathbb{C}$ by an arbitrary UFD $D$ ? Any comments are welcome! Thank you.","['involutions', 'unique-factorization-domains', 'algebraic-geometry', 'polynomials', 'commutative-algebra']"
3592313,"Prove that this function is continuous at $(0,0)$","I need to prove that the function is continuous at $(0,0)$ . $$f(x,y)=\frac{2x^2y^2}{x^2+y^2}, \text{ if } (x,y)\neq(0,0)$$ and $0 \text{ if } (x,y)=(0,0)$ I'm trying to prove that $$\lim_{(x,y)\to (0,0)}\frac{2x^2y^2}{x^2+y^2}=0$$ but I don't know how to relate it to $$0\leq \sqrt{x^2+y^2}\leq  \delta.$$ Finally I have $$2(x^2+y^2)\leq \epsilon.$$ Any ideas that can help me?","['limits', 'continuity']"
3592333,Limits - prove or disprove,"If $\lim_{x \to 0^+} f(x) = 0$ and $(\forall x>0)( \exists 0<c_x<x)$ s. t. $f(c_x)>f(x)$ , and $\forall x>0, f(x)>0$ do we have a contradiction?
I tried to build a sequence of x values that approaches $0$ but its f values form an ascending sequence but failed to show that it is actually ascending.","['limits', 'sequences-and-series']"
3592368,What is the area of the triangle?,"I managed to solve this question using trigonometry. But I wondered if there'd be anyway of doing it using only synthetic geometry. Here it is. Let $ABC$ be a right isosceles triangle of hypotenuse $AB$ . Let also $\Gamma$ be the semicircle whose diameter is the line segment $AC$ such that $\Gamma\cap\overline{AB} = \{A\}$ . Consider $P\in\Gamma$ with $PC = k$ , with $k \leq AC$ . Find the area of triangle $PBC$ . Here is my interpretation of the picture: I managed to get the solution via trigonometry as below. Then, the area $S$ requested is: $$\begin{align} S &= \displaystyle\frac{PC\cdot BC\cdot \sin(90^\circ + \beta)}{2}\\
&= \displaystyle\frac{k\cdot d\cdot \cos\beta}{2}\\
&= \displaystyle\frac{k\cdot d\cdot \frac{k}{d}}{2}\\
&= \displaystyle\frac{k^2}{2}.\\
\end{align}$$","['triangles', 'area', 'circles', 'geometry']"
3592382,The gradient of an element wise matrix function,Let $F = F[A]$ represent an element wise function $F$ applied to matrix $A$ . Here $F_{ij} = f(A_{ij})$ where $f$ is a scalar function. I would like to derive an expression for $\frac{\partial F}{\partial A}$ . My strategy was to use summation notation: $$\frac{\partial F_{ij}}{\partial A_{pq}} = \frac{\partial f}{\partial A_{ij}} \frac{\partial A_{ij}}{\partial A_{pq}} $$ $$\frac{\partial F_{ij}}{\partial A_{pq}} = \frac{\partial f}{\partial A_{ij}} \delta_{ip} \delta_{jq}$$ I know there should be 4th order tensor result but the implied sum is throwing me off. I am not too familiar with matrix manipulation when there are tensors of order 3 and higher so I did not try to construct a differential. Any walkthroughs/strategies would be much appreciated!,"['matrices', 'calculus', 'matrix-calculus', 'linear-algebra']"
3592398,relationship between derivative of vector valued function and gradient/partial derivatives,"From This article I understand that if I have a function defined using parametric equations like this $$
(1) \quad \quad \quad{{\mathbf{r}\left( t \right) = f\left( t \right)\mathbf{i} + g\left( t \right)\mathbf{j} + h\left( t \right)\mathbf{k}}\;\;\text{ or }\;\;}\kern0pt{\mathbf{r}\left( t \right) = \left\langle {f\left( t \right),g\left( t \right),h\left( t \right)} \right\rangle }
$$ Then it's derivative is the derivative of each function that generates each coordinate. $$
(2) \quad \quad \quad {\mathbf{r}^\prime\left( t \right) = \left\langle {f^\prime\left( t \right),g^\prime\left( t \right),h^\prime\left( t \right)} \right\rangle.}
$$ However if the same function is represented in a non parametric form (random example below) $$
(3) \quad \quad \quad r( x,y)= x+y  
$$ It seems there is no such thing as "" the derivative"", I only have partial derivatives, directional derivatives and the gradient. What is the relationship between ""the"" derivative as defined in $(2)$ and the gradient or the partial/directional derivatives? Are they equivalent in some way?","['partial-derivative', 'multivariable-calculus', 'derivatives', 'parametric']"
3592401,Proof verification : A weird identity involving sums,"I recently managed to solve a question that I asked a few days ago, but that was unfortunately closed, probably because it wasn't clear enough. Here is the question : Let $ n $ be a positive integer, prove the following identity : $$ \sum_{k=0}^{n}{\frac{1}{2k+1}\binom{n}{k}}=\left(\sum_{k=0}^{n}{\frac{1}{2^{k}}\binom{2k}{k}}\right)\left(\sum_{k=0}^{n}{\frac{\left(-1\right)^{k}}{2k+1}\binom{n}{k}}\right) $$ What I'm asking you for is to verify the following solution for me. First of all : $$ \sum_{k=0}^{n}{\frac{1}{2k+1}\binom{n}{k}}=\sum_{k=0}^{n}{\binom{n}{k}\int_{0}^{1}{x^{2k}\,\mathrm{d}x}}=\int_{0}^{1}{\left(1+x^{2}\right)^{n}\,\mathrm{d}x}\overset{\mathrm{denoted}}{=}I_{n}$$ Then : \begin{aligned} I_{n}=\int_{0}^{1}{\left(1+x^{2}\right)^{n}\,\mathrm{d}x}&=\left[x\left(1+x^{2}\right)^{n}\right]_{0}^{1}-2n\int_{0}^{1}{x^{2}\left(1+x^{2}\right)^{n}\,\mathrm{d}x}\\ &=2^{n}-2n\int_{0}^{1}{\left(\left(1+x^{2}\right)^{n}-\left(1+x^{2}\right)^{n-1}\right)\mathrm{d}x} \\ I_{n}&=2^{n}-2n\left(I_{n}-I_{n-1}\right)\\ \iff I_{n}&=\frac{2^{n}}{2n+1}+\frac{2n}{2n+1}I_{n-1}\end{aligned} In order to solve this recurrence relation, we'll multiply everything by $ \prod\limits_{k=1}^{n}{\frac{2k+1}{2k}} $ , so that we could have telescopic cancelling between the consecutive terms. Meaning, $$ \prod_{k=1}^{n}{\frac{2k+1}{2k}}I_{n}=\frac{2^{n}}{2n+1}\prod_{k=1}^{n}{\frac{2k+1}{2k}}+\prod_{k=1}^{n-1}{\frac{2k+1}{2k}}I_{n-1} $$ Since : $$ \frac{2^{n}}{2n+1}\prod_{k=1}^{n}{\frac{2k+1}{2k}}=\frac{2^{n}\prod\limits_{k=0}^{n-1}{\left(2k+1\right)}}{\prod\limits_{k=1}^{n}{\left(2k\right)}}=\frac{\left(2n\right)!}{2^{n}\left(n!\right)^{2}}=\frac{1}{2^{n}}\binom{2n}{n} $$ We get that for any $ k\geq 1 $ : \begin{aligned} \prod_{j=1}^{k}{\frac{2j+1}{2j}}I_{k}-\prod_{j=1}^{k-1}{\frac{2j+1}{2j}}I_{k-1}&=\frac{1}{2^{k}}\binom{2k}{k}\\ \Longrightarrow\sum_{k=1}^{n}{\left(\prod_{j=1}^{k}{\frac{2j+1}{2j}}I_{k}-\prod_{j=1}^{k-1}{\frac{2j+1}{2j}}I_{k-1}\right)}&=\sum_{k=1}^{n}{\frac{1}{2^{k}}\binom{2k}{k}}\\ \iff \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \prod_{k=1}^{n}{\frac{2k+1}{2k}}I_{n}-1&=\sum_{k=1}^{n}{\frac{1}{2^{k}}\binom{2k}{k}}\\ \iff \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ I_{n}&=\left(\prod_{k=1}^{n}{\frac{2k}{2k+1}}\right)\left(\sum_{k=0}^{n}{\frac{1}{2^{k}}\binom{2k}{k}}\right)\end{aligned} Secondly : $$ \sum_{k=0}^{n}{\frac{\left(-1\right)^{k}}{2k+1}\binom{n}{k}}=\sum_{k=0}^{n}{\left(-1\right)^{k}\binom{n}{k}\int_{0}^{1}{x^{2k}\,\mathrm{d}x}}=\int_{0}^{1}{\left(1-x^{2}\right)^{n}\,\mathrm{d}x}\overset{\mathrm{denoted}}{=}J_{n} $$ Then : \begin{aligned} J_{n}=\int_{0}^{1}{\left(1-x^{2}\right)^{n}\,\mathrm{d}x}&=\left[x\left(1-x^{2}\right)^{n}\right]_{0}^{1}+2n\int_{0}^{1}{x^{2}\left(1-x^{2}\right)^{n}\,\mathrm{d}x}\\&=2n\int_{0}^{1}{\left(\left(1-x^{2}\right)^{n-1}-\left(1-x^{2}\right)^{n}\right)\mathrm{d}x} \\ J_{n}&=2n\left(J_{n-1}-J_{n}\right)\\ \iff \ \ \ \ \ \ \ \ \ J_{n}&=\frac{2n}{2n+1}J_{n-1}\\ \Longrightarrow \prod_{k=1}^{n}{\frac{J_{k}}{J_{k-1}}}&=\prod_{k=1}^{n}{\frac{2k}{2k+1}}\\ \iff \ \ \ \ \ \ \ \ \ J_{n}&=\prod_{k=1}^{n}{\frac{2k}{2k+1}}\end{aligned} Thus, $$ \sum_{k=0}^{n}{\frac{1}{2k+1}\binom{n}{k}}=\left(\sum_{k=0}^{n}{\frac{\left(-1\right)^{k}}{2k+1}\binom{n}{k}}\right)\left(\sum_{k=0}^{n}{\frac{1}{2^{k}}\binom{2k}{k}}\right) $$ What do you guys think ?","['summation', 'binomial-coefficients', 'solution-verification', 'sequences-and-series']"
3592421,Poincaré duality for de Rham cohomology on non-compact manifolds,"Let $M$ be an $n$ -dimensional orientable non-compact manifold. Is there an isomorphism as follows, and if so how can we construct it? (Or can you provide a reference?) $$
 H^{n-i}_{\operatorname{dR},c}(M, \mathbb R) \cong H_i(M,\mathbb R).
 $$ On the left hand side we have de Rham cohomology with compact support and on the right hand side we have singular homology. According to [1], Poincaré duality can be stated as saying that the integration pairing between compactly supported forms and forms induces an isomorphism: $$
H^i_{\operatorname{dR}}(M,\mathbb R)\cong \left(H^{n-i}_{\operatorname{dR},c}(M, \mathbb R)\right)^\vee.
$$ Where $\bullet^\vee$ denotes the vector space dual. Now, note that $H^{n-i}_c\not\cong (H^i)^\vee$ in general, because in our situation the homology groups might be infinite dimensional. The de Rham theorem says that integration gives an isomorphism witht he dual of singular homology: $$
H^i_{\operatorname{dR}}(M,\mathbb R)\cong \left(H_{i}(M, \mathbb R)\right)^\vee.
$$ Now, knowing that two (infinite dimensional) vector spaces have the same dual doesn't seem all that helpful. I don't even see how integration can give a map $H^{n-i}_c\to H_i$ . Another attempt would be to try to follow the proof that appears in [1] and [2]. You could try to ""induct"" by showing that if Poincaré duality holds in two open sets $U$ abd $V$ then it holds in the union. But the five-lemma together with the Mayer-Vietoris exact sequences aren't enough to construct a map, much less a canonical one. I'd be happy with an answer that uses Verdier duality (bonus points for an answer with coefficients in a local system), but then the trouble is that books that talk about sheaves don't talk about homology (or they define it as the cohomology with compact support of the dual local system, in which case this is tautology). Then the question becomes: For a local system $\mathcal L$ on $M$ , are the following quasiisomorphic? $$Rp_!(\mathcal L) \cong C_\bullet(M,\mathcal L):= C_\bullet(\widetilde M, \mathbb R)\otimes_{\mathbb R[\pi_1(M)]} \mathcal L_p.$$ Here $p:M\to *$ is the map to a point, $\widetilde M$ is the universal cover, and the fundamental group acts on $\widetilde M$ by deck transformations and on the stalk $\mathcal L_p$ by the monodromy of $\mathcal L$ . I guess this would be a corollary of the question Is there a soft complex of sheaves that resolves $\mathcal L$ , and whose compactly supported sections form a complex quasiisomorphic to $C_\bullet(M, \mathcal L)$ ? Update: I've been looking at Glen Bredon's book [3], and Theorem V.9.2. is promising in that it relates homology and compactly supported cohomology. However, the definition of ""sheaf homology"" in that book doesn't seem to be related to singular homology as far as I can tell. In Chapter VI there is a relation between singular homology and Čech homology, but somehow not between these two and sheaf homology. [1]: Greub, Werner; Halperin, Stephen; Vanstone, Ray , Connections, curvature, and cohomology. Vol. I: De Rham cohomology of manifolds and vector bundles, Pure and Applied Mathematics, 47. New York-London: Academic Press. XIX, 443 p. $ 31.00 (1972). ZBL0322.58001 . [2]: Hatcher, Allen , Algebraic topology, Cambridge: Cambridge University Press (ISBN 0-521-79540-0/pbk). xii, 544 p. (2002). ZBL1044.55001 . [3]: Bredon, Glen E. , Sheaf theory., Graduate Texts in Mathematics. 170. New York, NY: Springer. xi, 502 p. (1997). ZBL0874.55001 .","['poincare-duality', 'de-rham-cohomology', 'homology-cohomology', 'algebraic-topology', 'differential-geometry']"
3592440,Computing the cdf of some Discrete Distribution...,"I'm trying to find the cdf of the following discrete distribution: $$f(k)=\frac{(1-\rho )^2 \rho ^{k-1}}{\left(1-\rho ^k\right) \left(1-\rho ^{k+1}\right)}+\frac{(1-\rho ) \rho ^{M-1}}{1-\rho ^M}$$ with domain of support $$k\in \{1,...,M-1\}, k < M, \mbox{and}\ M \in Z.\ \mbox{Also}, 0<\rho<1.$$ I know its a proper pmf because, when $M=10$ , I can compute $$\sum _{k=1}^9 \left(\frac{(1-\rho )^2 \rho ^{k-1}}{\left(1-\rho ^k\right) \left(1-\rho ^{k+1}\right)}\right)+\frac{(1-\rho ) \rho ^{9}}{1-\rho ^{10}} = 1.$$ I'd like to compute the cdf, but I simply don't know how to do this. I can compute the pmf from the cdf for other distributions, but I can't do it in reverse. Any help would be greatly appreciated.","['cumulative-distribution-functions', 'probability-distributions', 'probability']"
3592481,Means and entropy: what's the relationship?,"Economist here. In my current research I often encounter the concepts of entropy and means, and I find some connections interesting (for my purposes). The context is a set of $n$ nonnegative numbers $\{a_1,\dots,a_n\}$ , which can be normalized by the sum of the $a_i$ 's to get $\{x_1,\dots,x_n\}$ . These $x_i$ 's serve as weights in the context of means, and they serve as probabilities out of which entropy can be calculated.
I find relationships between entropy and means, particularly between \begin{align*}
\mathcal{A}\left(a_{i},\frac{1}{n}\right) & =\frac{1}{n}\sum a_{i} & \underset{\textrm{(i.e. equal weights }\frac{1}{n})}{\textrm{unweighted arithmetic mean}}\\
\mathcal{G}\left(a_{i},x_{i}\right) & =\prod a_{i}^{x_{i}} & \underset{\textrm{(with weights }x_{i}\textrm{ summing to one)}}{\textrm{weighted geometric mean}}\\
H(x_{i}) & =-\sum_{i=1}^{n}x_{i}\ln x_{i} & \underset{\textrm{(same }x_{i}\textrm{ as above, i.e. probabilities)}}{\textrm{Shannon entropy}}
\end{align*} Questions: In general, how are entropy and means related? I can't think I'm the first one to notice. The only thing I can think of is the inequality $\mathcal{A}\ge\mathcal{G}$ between the arithmetic and geometric means; if I am right, the difference or divergence between these two means relates to the Shannon entropy. But not so fast: the inequality of means is valid when both the arithmetic and geometric means have the same weights (either $\frac{1}{n}$ or $x_i$ )--which is not the case here.
Any thoughts greatly appreciated.","['statistics', 'entropy', 'means']"
3592495,"Prove that for the defined $\langle .,. \rangle$ there exist $0 < a \le b$ such that $a\|x\| \le \|x\|_\ast ≤ b\|x\|$ for all $x \in H$.","Let $H$ be a Hilbert space over $\mathbb{R}$ with an inner product $(·, ·)$ and the norm $\|x\| = \sqrt
{(x, x)}$ . Let $A$ be a bounded strictly positive definite linear operator on $H$ with $A^\ast = A$ .
For $x, y \in H$ , let $\langle x, y \rangle = (Ax, y)$ and $\|x\|_\ast = \sqrt{\langle x, x \rangle}$ . Prove that $\langle .,. \rangle$ is an inner product on the vector space $H$ , and that there exist constants $0 < a \le b$ such that $a\|x\| \le \|x\|_\ast ≤ b\|x\|$ for all $x \in H$ . $\text{My Attempt}$ : for the first part to show that it is inner product: 1- $\langle x, x \rangle = (Ax, x) \ge 0$ and $(Ax, x)=0$ iff $x=0$ 2- $\langle x, y \rangle = (Ax, y) = (x, Ay)= \langle y , x \rangle$ 3- $\langle x+z, y \rangle = (Ax+Az, y) = (Ax, y) +(Az, y) =\langle x, y \rangle +\langle z, y \rangle$ 4- $\langle \alpha x, y \rangle = (\alpha Ax, y)= \alpha (Ax, y)=\alpha \langle x, y \rangle$ For the second part 1- Since $A$ is a positive definite linear operator $\|x\|_\ast = \sqrt{\langle x, x \rangle} = \sqrt{(Ax,x)} \ge\sqrt{\beta \|x\|^2} = \sqrt{\beta} \|x\|\equiv b \|x\|$ 2- Let $\|A\| = \alpha$ . Since $A$ is symmetric $\|x\|_\ast = \sqrt{\langle x, x \rangle} = \sqrt{(Ax,x)} = \sqrt{(x,Ax)} \le\sqrt{ \|A\| \|x\|^2} = \sqrt{\alpha} \|x\|\equiv a \|x\|$","['inner-products', 'analysis', 'solution-verification', 'functional-analysis', 'adjoint-operators']"
3592514,Solution of a homogeneous system is the null solution if it converges to $0$ for $x\to\pm\infty$,"Let $A\in\mathbb R^{d\times d}$ be diagonizable with eigenvalues $\lambda_1,\dots,\lambda_d\in\mathbb R$ . Show that: If $y\in C^1(\mathbb R\to\mathbb R^d)$ is a solution of the system $y'=Ay$ satisifying $\lim_{x\to\pm\infty} y(x)=0$ , then $y(x)\equiv 0$ . Thoughts : Let $v_1,\dots,v_d$ be eigenvectors of $A$ for the eigenvalues $\lambda_1,\dots,\lambda_d$ . If $\lambda_i\neq\lambda_j$ for $i\neq j$ , then $\{x\mapsto v_1\exp(\lambda_1x),\dots,x\mapsto v_d\exp(\lambda_dx)\}$ is a basis for the vector space of solutions. The condition $y(x)\to 0$ for $x\to\pm\infty$ implies that all components $y_i$ of $y$ must go to $0$ as well. These can be written as sums of exponential functions of the form $\exp(\lambda_i x)$ in each component, which implies that all coefficients have to be $0$ (using the linear independence of these functions) and thus $y(x)\equiv 0$ . Why exactly is this the case, and how can one show this in the more general case where $\lambda_i=\lambda_j$ is allowed?","['calculus', 'linear-algebra', 'ordinary-differential-equations', 'real-analysis']"
3592566,Find $\lim\limits_{n \to \infty}\sum_{k=1}^n\left(\frac{k}{n}\right)^k$.,"Maybe, we can make use of Tannery's Theorem , or dominated convergence theorem, to exchange the order of the limit and summation: \begin{align*}
\lim_{n \to \infty}\sum_{k=1}^n\left(\frac{k}{n}\right)^k&=\lim_{n \to \infty}\sum_{k=0}^{n-1}\left[\left(1-\frac{k}{n}\right)^{n}\right]^{\frac{n-k}{n}}=\sum_{k=0}^{\infty}\lim_{n \to \infty}\left[\left(1-\frac{k}{n}\right)^{n}\right]^{\frac{n-k}{n}}=\sum_{k=0}^{\infty} e^{-k}=\frac{e}{e-1}
\end{align*} This is correct? How to verify that it satisfy the conditons of the theorem?","['limits', 'calculus', 'sequences-and-series']"
3592596,Why are these two definite integrals equal?,"How can one prove that, for $0< z<1$ , the two integrals $$\int_0^\infty \frac{u^{z-1}}{1+u}du$$ and $$\int_0^\infty \frac{u^{-z}}{1+u} du$$ are equal? From the integral representation of the beta function $$B(z,w)=\frac12\int_{0}^\infty \frac{u^{z-1}+u^{w-1}}{(1+u)^{z+w}} du$$ If we replace $w$ with $1-z$ , the left hand side equal to $\pi/\sin(\pi z)$ while the right hand side is $$\frac12\int_{0}^\infty \frac{u^{z-1}+u^{-z}}{(1+u)^{z+w}} du$$ this is the reason of my question.",['analysis']
3592615,how to use matrix to prove this identity?,"if $a_{n},b_{n}$ such $a_{0}=b_{0}=1$ $$\begin{cases}a_{n}=5a_{n-1}+7b_{n-1}\\
b_{n}=7a_{n-1}+10b_{n-1},\forall n=1,2,3,\cdots
\end{cases}$$ show that $$a_{m+n}+b_{m+n}=a_{m}a_{n}+b_{m}b_{n}$$ It's an interesting identity, and I've proved it with mathematical induction, but I feel like it's more obvious with a matrix, but I don't, so can someone please prove it with a matrix? Thank you $$\begin{bmatrix}
a_{n}\\
b_{n}\end{bmatrix}=\begin{bmatrix}
5&7\\
7&10\end{bmatrix}\cdot \begin{bmatrix}
a_{n-1}\\
b_{n-1}\end{bmatrix}$$",['matrices']
3592643,"$xy<1 \iff \text{arctan }x + \text{arctan }y \in (-\pi/2,\pi/2)$","Is this claim true? $$xy<1 \iff \text{arctan }x + \text{arctan }y \in (-\pi/2,\pi/2)$$ If so, how to prove? I was led to this while trying to figure out the Addition Formula for Arctangent. I've looked at many questions and answers about that Formula but don't seem to have come across a proof of the above claim.",['trigonometry']
3592648,"Is $Ext^1_{Lie-Gr}(\mathbb C, \mathbb C^*)=0$?","I want to know if any Lie group extension $$1\to \mathbb C^*\to A\xrightarrow{\pi} \mathbb C\to 0 \tag{1}\label{1}$$ is trivial, where the group structure is multiplicative on $\mathbb C^*$ and additive on $\mathbb C$ . I guess the answer is Yes and it suffices to find a section $s:\mathbb C\to A$ as Lie group morphism such that $\pi\circ s=Id$ . From the point of view of topology, $A$ is a principal $\mathbb C^*$ -bundle over the contractible base $\mathbb C$ . This implies $A$ is trivial as principal $\mathbb C^*$ -bundle, so there is a section $s:\mathbb C\to A$ . However, the section need not preserve the group structure. Can I modify the section $s$ so as preserve the group structure? Edited remark : As @Roland's points out, considering the extension $(1)$ purely as group extension which lives in $Ext_{Ab}^1(\mathbb C,\mathbb C^*)$ is different from an extension as Lie groups (or algebraic groups), because a section in the category of groups does not need to be even continuous. Second Edition : Here is another idea: If we pull back $A$ to its universal covering $\require{AMScd}$ \begin{CD}
\mathbb C @>>> \tilde{A}@>\tilde{\pi}>>\mathbb C\\
@VVV  @VVV  @VV=V \\
\mathbb C^* @>>> A @>\pi>> \mathbb C
\end{CD} If I can show the sequence on the first row is trivial, then a section $\tilde{s}:\mathbb C\to \tilde{A}$ will descend to a section $s:\mathbb C\to A$ , but how do we show $Ext^1_{Lie-gr}(\mathbb C,\mathbb C)=0$ (although it sounds trivial)?","['homological-algebra', 'group-extensions', 'abstract-algebra', 'algebraic-topology']"
3592684,"Galois Extension, Chinese Remainder Theorem, and Quotients of Polynomial Rings","I am trying to prove the following assertion (, which is an exercise from a section on tensor products in the book Algebra: Chapter 0 by P.Aluffi): Let $F=k(\alpha)\supset k$ be a finite simple extension such that $F\otimes_kF\cong F^{[F:k]}$ as a ring. Then the extension is Galois. A related question has been asked several times on this site, for example in here . Reading the answers to these questions, I learned that  the above assertion may be proved as follows: There is a canonical isomorphism $F\otimes_k F\cong F[x]/(m(x))$ of $k$ -algebras. Now $F^n$ is obviously reduced, so $F[x]/(m(x))$ is also reduced. It follows that $\alpha$ is separable over $k$ . (This step requires a little more argument, but I am OK with this step.) Therefore, to prove that $F/k$ is Galois, it suffices to show that the $\alpha$ splits over $F$ . Let $m(x)\in k[x]$ be the minimal polynomial of $\alpha$ over $k$ . But $F[x]/(m(x))\cong F^n$ as a ring by hypothesis. By the Chinese remainder theorem, this implies that $m(x)$ factors into linear factors over $F$ . I do not understand the bold faced part in step 3. Sure, the Chinese remainder theorem implies that if $m(x)$ splits over $F$ , then $F[x]/(m(x))\cong F^n$ as rings. However, we want to go in the other direction. How do we do this? Thanks in advance. In addition to the above question, I would greatly appreciate the answers to the following additional questions. ( But you do not have to answer these unless you want to. ) The question which I cited above actually assumed that the isomorphism $F\otimes_k F\cong F^{[F:k]}$ to be an $F$ -algebra isomorphism. In my book, this isomorphism is assumed to be merely a ring isomorphism. Maybe the author took it for granted that the isomorphism to be $F$ -linear? Or maybe we can do without the $F$ -linearity. In general, which quotient of $F[x]$ isomorphic to the direct product of $F$ as a ring (or as an $F$ -algebra)?","['galois-theory', 'abstract-algebra', 'tensor-products', 'extension-field']"
3592759,Solve the differential equation $ x^4y^3=xy' + y$,"Using the change of the dependent variable $z = y^{−2}$ , solve the differential equation: $$xy' + y = x^4y^3$$ . My attempt: $$xy' + y = x^4y^3 \tag 1$$ Now dividing $(1)$ by $y^2$ $$\frac{xy'}{y^2} + \frac{1}{y} = x^4y$$ Now put $z= \frac{1}{y^2}$ , $$\frac{dz}{dx} = \frac{-2}{y^3}\frac{dy}{dx}=\frac{-2}{y^3}y'$$ $$y'=\frac{-y^3dz}{2dx}$$ $$xy' z +\frac{1}{y} = x^4y$$ After that im not able to proceed further.",['ordinary-differential-equations']
3592788,Proving 4 points on a circle.,"Notes : I have been working on this question for a while, and I was stuck. The original question, I have already found the answer. But I wanted to try this way, and here I come. If a similar question was answered somewhere else, please link it in and close this question. Otherwise, please help me solve this Original question Let there be a rhombus $ABCD$ . $F$ is a random point on $[AD]$ . $G, I, H$ are centers of the incircles of $\triangle ABF , \triangle DCF, \triangle BCF$ . J is the tangent of the incircle of $\triangle BCF$ with BC. Prove that $JO \perp GI$ My attempts What I have been trying here, I pushed the problem back to solving the following property: Let $K, L$ be points on $BO, CO$ such that $JK \perp BO, JL \perp CO$ . Prove that $JLIG$ is inscribed in a circle ( i.e $J,L,I,G$ lies on the same circle) Any help is appreciated.","['quadrilateral', 'euclidean-geometry', 'geometry']"
3592856,Borel Cantelli liminf of independent random variables,"Let $\{X_n\}_{n=1}^\infty$ is a succession of indepedent random variables, such that for all $n\geq 1$ , $\mathbb E(X_n) =0$ and $\mathbb E(|X_n|) = 1$ , Prove or disprove that $\mathbb P(\lim \inf_{n} X_n < 0) > 0.$ I tried to handle it like this Let us consider the succession of events $A_n = \{X_n < 0\}$ . Since $(X_n)_n$ are independent then $(A_n)_n$ and $A^c_n = \{X_n \geq 0\}$ are also indepedent events. We have \begin{align*}
\mathbb P(\lim \inf_{n} X_n < 0) &= \mathbb P(\lim \inf_{n} A_n)\\
& = \mathbb P(A_n \, \text{ e.v.})\\
&= 1 - \mathbb P\big((A_n \, \text{ e.v.})^c\big)\\
&= 1 - \mathbb P(A_n^c \, \text{ i.o.}\big).
\end{align*} On the other hand, we have \begin{align*}
\mathbb P(A_n^c) &= \mathbb P(X_n \geq 0)\\	
& = ...
\end{align*} Here some recall of the notations used and my intention is to use the second BorelCantelli lemma that also I recall it here First let's recall some definitions. Let $(A_n)_n$ be a sequence of events, we define \begin{align*}
A_{n} \text{ infinitely often (i.o.) } &\equiv\left\{\omega: \omega \text { is in infinitely many } A_{n}\right\}\equiv \limsup _{n} A_{n} \equiv \bigcap_{m}^{\infty} \bigcup_{n=m}^{\infty} A_{n}
\end{align*} Note that $$
\mathbb {I}_{A_{n} \,i.o. }=\lim_{n} \sup \mathbb{I}_{A_{n}}
$$ Similarly, \begin{align*}
	A_{n}\text{ eventually (e.v.) } 
 \equiv\left\{\omega: \omega \text { is in } A_{n} \text { for all large } n\right\} 
 \equiv \liminf _{n} A_{n}
 \equiv \bigcup_{m} \bigcap_{n=m}^{\infty} A_{n}.
\end{align*} Note that $$
\mathbb{I}_{A_{n} \,e.v.} =\liminf _{n} \mathbb{I}_{A_{n}}
$$ Also we have $\left(A_{n} \text { e.v.}\right)^{c}=\left(A_{n}^{c} \text { i.o. }\right)$ . 
Moreover recall the second Borel-Cantelli Lemma: If the events $(A_n)_n$ are independent, then $\sum_{n} \mathbb{P}(A_{n})=\infty$ implies $\mathbb{P}(A_{n} \text{ i.o.})=1$","['borel-cantelli-lemmas', 'probability-theory', 'probability']"
3592861,Why are Covid-19 cases shown logarithmic?,"I think this is a really simple and stupid question, I am sorry for that, but I could not find anything while googling it. why are covid-19 cases shown also on a logarithmic scale? on worldometer for example. From my understanding, the ln(x) is the inverse function of the e(x) and so we could better see, if the curve flattens, when plotting the cases on a logarithmic scale? is this true?","['statistics', 'logarithms']"
3592864,How to derive the Nautilus Gears equation?,"First have a look at some video on Nautilus Gears, e.g. https://www.youtube.com/watch?v=5Ex_Drh6Rpo . I want to derive the formula for this curve, which I know is just the logarithmic/exponential spiral. I can think of 3 conditions: The sum $r(\theta)+r(\phi)$ should be constant, where $\theta$ and $\phi$ are the corresponding angles over which the gears are rotated. The arc length of the curve between $0$ and $\theta$ should be equal to the arc length of the curve between $\phi$ and $2\pi$ for corresponding $\theta$ and $\phi$ . The tangent lines of the two gears should be equal when touching. It's easy to check that a logarithmic spiral fulfills these 3 conditions, but I want to derive the equation as a solution. As commented below, probably many solutions exist, but the above 3 conditions will be common to all. So I'm interested in the general differential equation and solutions, out of which the logarithmic spiral is only one particular solution. What additional constraints give rise to that log-spiral? Note that the ""teeth"" on the spiral can be neglected. Sorry for the lousy drawing; I did my best...","['geometry', 'analysis', 'differential-geometry']"
3592884,Roots of trigonometric polynomial,"let $a=(a_0,a_1,...,a_n)$ and $P_a(x)= \sum_{k=0}^{n}a_k \cos (kx) $ define $b=(a_n,a_{n-1},...,a_0)$ If $Z_a$ is the number of roots of $P_a$ on $[0,2\pi[$ then $$Z_a+Z_b \geq 2n$$ I have failed to find a clever algebra trick, I've tried multiplying $P_a$ and $P_b$ and tried induction on $deg (P)$ but to no avail.","['trigonometry', 'polynomials', 'real-analysis']"
3592964,"$f: R \rightarrow R$ be a function such that $f(2-x)=f(2+x)$ and $f(4-x)=f(4+x)$ ,for all $x \in R$ , then $f(x)$ is a periodic function","If $f: R \rightarrow R$ be a function such that $f(2-x)=f(2+x)$ and $f(4-x)=f(4+x)$ ,for all $x \in R$ and $\int_0^2 f(x)dx=5$ . Then the value of $\int_{10}^{50} f(x)dx$ =? Can anyone please tell me how to mathematically prove that the function $f(x)$ is a periodic function on $\Bbb{R}$ . If I can prove that, I will be done.","['algebra-precalculus', 'functions']"
3592972,"On the integral $\int_0^{\sqrt{2}/2} \frac{\arctan \sqrt{1-2t^2}}{1+t^2} \, \mathrm{d}t$","I'm having a difficult time evaluating the integral $$\mathcal{J} = \int_0^{\sqrt{2}/2} \frac{\arctan \sqrt{1-2t^2}}{1+t^2} \, \mathrm{d}t$$ This is integral arose after simplifying the integral $\displaystyle  \int_{0}^{\pi/4 } \arctan \sqrt{\frac{1-\tan^2 x}{2}} \, \mathrm{d}x$ ; \begin{align*}
\require{cancel.js}
\int_{0}^{\pi/4} \arctan \sqrt{\frac{1-\tan^2 t}{2}}\, \mathrm{d}t &\overset{1-\tan^2 t \mapsto 2t^2}{=\! =\! =\! =\! =\! =\!=\!=\!} \int_{0}^{\sqrt{2}/2} \frac{t \arctan t}{\sqrt{1-2t^2} \left ( 1-t^2 \right )} \, \mathrm{d}t \\ 
&=\cancelto{0}{\left [ - \arctan \sqrt{1-2t^2} \arctan t \right ]_0^{\sqrt{2}/2}} + \int_{0}^{\sqrt{2}/2} \frac{\arctan \sqrt{1-2t^2}}{1+t^2} \, \mathrm{d}t 
\end{align*} My main guess is that differentiation under the integral sign is the way to go here. Any ideas?","['integration', 'real-analysis']"
3593011,Constant vector field on the torus $\mathbb{T}^{2n}$ is symplectic,"Let $\mathbb{T}^{2n}=\mathbb{R}^{2n}/\mathbb{Z^{2n}}$ be the $2n$ -torus, which we equip with the unique symplectic form $\omega$ that pulls back to the standard symplectic form on $\mathbb{R}^{2n}$ under the natural projection $\pi:\mathbb{R}^{2n}\to\mathbb{R}^{2n}/\mathbb{Z^{2n}}$ . We identify the tangent space $T_x\mathbb{T}^{2n}\cong\mathbb{R}^{2n}$ for all $x\in\mathbb{T}^{2n}$ . Fix some $v\in\mathbb{R}^{2n}$ and define the vector field $X\in\mathcal{X}(\mathbb{T}^{2n})$ by $X(x)=v$ . Then this is supposed to be an example of a vector field which is symplectic but not Hamiltonian for $v\neq 0$ . I know how to show that it is not Hamiltonian. To show that it is symplectic, we have to show that $d\iota_X\omega=d(\omega(X,\cdot))=0$ . By Cartan's magic formula and the closedness of $\omega$ , this is equivalent to showing that $$
\mathcal{L}_X\omega=\frac{d}{dt}\bigg|_{t=0}((\phi_X^t)^* \omega)=0
$$ Thus, we have to compute the flow $\phi_X^t$ . Note that $\frac{d}{dt}\phi_X^t(y)=X_{\phi_X^t(y)}=v$ for all $y$ . Thus, do we have that $\phi_X^t(y)=y+vt$ , where now we view $v\in\mathbb{T}^{2n}$ ? And do we have that $\mathcal{L}_X\omega=0$ ?","['vector-fields', 'symplectic-geometry', 'ordinary-differential-equations', 'differential-geometry']"
3593017,Approximation of positive definite functions by neural networks,"Bochner's theorem shows that probability measures $\mu$ are linked with positive definite functions via Fourier transform: $f(k) = \int_{\mathbb{R}^n} e^{-2 \pi i k x} \,d\mu(x)$ Currently, probability measures can be very well approximated by various generative models based on neural networks. What about positive definite functions? Is there any natural approximation tool, based on neural network? Udp: I can that question myself but this answer does not satisfy me. Let us assume that $f(k)$ is real-valued, i.e. $\mu$ is symmetric w.r.t. to origin. Then, it is natural to approximate $f$ by the single layer neural network with cosine activation fucntion: $f(x) = \sum_{i=1}^N \theta_i cos(\omega_i^T x)$ where $\theta_i\geq 0$ . My question is: are there any more powerfull (many-layer) networks, for this kind of functions?","['machine-learning', 'functional-analysis', 'approximation-theory', 'neural-networks']"
3593129,$\mathcal{O}_X(U)=\mathcal{O}_X(X)=A(X)$ for any open subset $U$ that is the complement of an irreducible variety of codimension at least $2$,"I am trying to solve Exercise 3.15 in Gathmann's 2014 notes.
Given an irreducible affine variety $X$ such that $A(X)$ is UFD und $U$ like in the heading I want to show that the sheaf of regular functions on $U$ is already the whole coordinate ring $A(X)$ . From a previous exercise I know that regular functions on $U$ are given as a quotient of polynomials globally on $U$ if $A(X)$ is UFD i.e. $\varphi(x) = \frac{g(x)}{f(x)}$ for all $x \in U$ and $f,g \in A(X)$ . It seems reasonable to show that $f$ is a unit using the fact that this equation kind of holds in dimension $2$ but I have no idea how to proceed at this point.","['affine-varieties', 'algebraic-geometry', 'commutative-algebra']"
3593131,Differentiation using product rule?,"I'm stuck on differentiating this: $$f(x) = \frac{4\sin(2x)}{e^\sqrt{2x-1}}$$ I thought about using the product rule here, but when I do that I get an expression that is hard to simplify, and I need to solve for when $f(x) = 0$ . Is there a simpler way of doing this? Help here would be much appreciated!",['derivatives']
3593153,Finite analog of these two infinite series with inverse tangents?,"The identity $$
 \sum_{n=1}^{\infty}\chi(n)\arctan e^{-\alpha n}+\sum_{n=1}^{\infty}\chi(n)\arctan e^{-\beta n}=\frac{\pi}{8}, \qquad \alpha\beta=\frac{\pi^2}{4},\tag{1}
$$ where $\chi(n)=\sin\frac{\pi n}{2}$ is Dirichlet character modulo $4$ , known from the theory of elliptic functions ( Ramanujan's Notebooks, part II , ch.14, entry 15) has a finite analog \begin{align}
\sum_{|j|\le n}&(-1)^{n+j}\arctan {\biggl(\!\sqrt{1+\alpha^2\cos^2\!\tfrac{\pi  j}{2n+1}}-\alpha\cos\tfrac{\pi  j}{2n+1}\!\biggr)^{2 m+1}}\\&+\sum_{|k|\le m}(-1)^{m+k}\arctan\!{\biggl(\!\sqrt{1+\beta^2\cos ^2\!\tfrac{\pi  k}{2m+1}}-\beta\cos\tfrac{\pi k}{2m+1}\!\biggr)^{2 n+1}}=\frac{\pi}{4},\tag{1a}
\end{align} where $n,m\in\mathbb{N}_0$ and $\alpha\beta=1$ , $\alpha>0$ . Namely, $(1)$ follows from $(1a)$ when $n=m\to\infty$ , after suitable redefinition of $\alpha$ and $\beta$ . (for details see https://arxiv.org/abs/2003.05306 ) The following identity was found in an unpublished manuscript by B. Cais, On the transformation of infinite series , (1999): \begin{align}
    \sum_{n=1}^{\infty} \left(\frac{n}{3}\right)&\arctan\frac{\sqrt{3}}{1+2 e^{\alpha n}}\\&+\sum _{n=1}^{\infty} \left(\frac{n}{3}\right) \arctan\frac{\sqrt{3}}{1+2 e^{\beta n}}=\frac{\pi}{18}, \qquad \alpha\beta=\frac{4\pi^2}{9},\tag{2}
\end{align} where $\left(\frac{j}{3}\right)$ is Legendre symbol. In the arxiv preprint cited above, it was found that \begin{align}
    \sum_{n=0}^{\infty} \left(\frac{n-1}{3}\right)&\arctan\frac{\sqrt{3}}{1-2 e^{\alpha (2n+1)}}\\&+\sum _{n=0}^{\infty} \left(\frac{n-1}{3}\right) \arctan\frac{\sqrt{3}}{1-2 e^{\beta (2n+1)}}=\frac{2\pi}{9}, \qquad \alpha\beta=\frac{\pi^2}{9}.\tag{3}
\end{align} Q: Do finite analogs of $(2)$ and $(3)$ exist and how to find them? A finite analog of a certain linear combination of $(2)$ and $(3)$ have been found in the arxiv preprint cited above. But no finite analog for $(2)$ or $(3)$ separately is known. It is not known how to extend the method used in the preprint to answer this question. Any ideas and comments are welcome.","['trigonometry', 'elliptic-functions', 'summation', 'sequences-and-series']"
3593173,Prove $2^{29}$ has exactly 9 distinct digits,There is a problem that say: The number $2^{29}$ has exactly 9 distinct digits. Which digit is missing? It is an Olympiad problem and I see it solved by using remainder modulo 9. $2^{29}=536870912$ interesting! My Question: Can we find any mathematically way to show $2^{29}$ has exactly 9 distinct digits?,"['modular-arithmetic', 'discrete-mathematics']"
3593338,Monotonic transformation preserves extrema,"I have heard that given a function $f$ which has certain relative extrema, if $g$ is monotonic, then $gf$ , $g$ composed with $f$ , will have the same relative extrema as $f$ . Is this true, and if so, where can I find a proof? I do not have a lot of experience with analysis so if someone could explain this intuitively I would appreciate that a lot. Specifically, I am working with likelihood functions and I think this is the reason why optimizing the log-likelihood function of a sample gives the same maximum likelihood estimator as optimizing the likelihood.","['functions', 'log-likelihood', 'maximum-likelihood']"
3593342,Solutions of $\ f(x) = \frac{1}{x-a_1} + \frac{1}{x-a_2} + \cdots + \frac{1}{x-a_n} $,"I got $\ a_1, \ldots,a_n $ as different real numbers  and: $$\ f(x) = \frac{1}{x-a_1} + \frac{1}{x-a_2} + \cdots + \frac{1}{x-a_n} $$ I have to find the number of the solutions in real numbers for $\ f(x) = 0$ and $\ f(x) = x$ . I notice, that: $$\ \frac{x^{n-1}-x^{n-2}(a_1+\cdots+a_n)-x^{n-3}(\cdots }{x^n-x^{n-1}(a_1+\cdots+a_n)\cdots} $$ In nominator therre always lacks a one real number because of the multiplication to the common denominator - in denominator there are all $\ a's$ . Here I ask you for help. I'm stuck on this, although I think of derivative and comparison to i.e. $$\  x = \frac{1}{x-a_1} + \frac{1}{x-a_2} + \cdots + \frac{1}{x-a_n} $$ I would appreciate any explanation or advice.","['summation', 'functions', 'derivatives']"
3593363,Probability of binary string prefix divisibility by 3,"What is the probability of an evenly distributed binary string of length $n$ to have a prefix that is divisible by $3$ ? I know that divisibility by $3$ of a binary string is equivalent to the divisibility of the (sum of bits on even positions)-(sum of bits on odd positions), but that doesn't help much. I've noticed that any string starting with $0$ has a divisible by $3$ prefix, and so does any string starting with $11$ . However, I cannot put enough restrictions on the $3$ rd bit, because both $101$ and $100$ aren't divisible by zero. This calls for a solution using dynamic programming, but I need to find a closed-form solution. A solution for any $p$ , not just $3$ would be also much appreciated.","['binary', 'discrete-mathematics', 'probability']"
3593458,Convert second-order ODE to first order system,I want to convert the equation $y'' + y' + \sin y = 0$ into a system of first order ODEs. I said: $$u = y\enspace v = y'$$ $$u' = y'\enspace v' = y'' = -y' - \sin y$$ Hence we have the system: $$u' = v$$ $$v' = -v -\sin u$$ Is this correct? Or should $u = \sin y$ at the beginning?,"['mathematical-modeling', 'ordinary-differential-equations']"
3593636,Counterexample: Uniqueness of fiber metric on alternating $2$-vectors,"I'm working on the following problem (Lee's ""Riemannian Manifolds"", Problem 8-33(a)). Suppose $(M,g)$ is a Riemannian manifold. Let $\Lambda^2(TM)$ be the bundle of $2$ -tensors on $M$ . Show that there is a unique fiber metric on $\Lambda^2(TM)$ whose associated norm satisfies $$|w \wedge x|^2 = |w|^2|x|^2-\langle w, x\rangle^2$$ for all tangent vectors $w, x$ at every point $q \in M$ . My question: Are we guaranteed uniqueness? Existence is straightforward by taking a local orthonormal frame $\{E_1,\ldots, E_n\}$ of $M$ and declaring $\{E_i \wedge E_j : i < j\}$ to be an orthonormal frame. One can further show using the algebra of alternating bivectors that given any local orthonormal frame $\{\tilde E_1, \ldots, \tilde E_n\}$ , the corresponding set $\{\tilde E_i \wedge \tilde E_j : i < j\}$ of contravariant $2$ -tensor fields is orthonormal in this inner product, so this fiber bundle is smooth and well-defined on all of $M$ . However, I'm not sure we have uniqueness. Consider $(M,g) = (\mathbb{R}^4, \overline g)$ , where $\overline g$ is the Euclidean metric, and let $\{E_1, E_2, E_3, E_4\}$ be the standard orthonormal coordinate frame. Define the metric $\langle \cdot, \cdot \rangle$ on $\Lambda^2(T\mathbb R^4)$ by declaring $|E_i \wedge E_j| = 1$ for $1 \leq i<j \leq 4$ , along with the relations $$
\langle E_1 \wedge E_2, E_3 \wedge E_4 \rangle = \langle E_1 \wedge E_4, E_2 \wedge E_3 \rangle = -\langle E_1 \wedge E_3, E_2 \wedge E_4 \rangle = 1,
$$ and all products of the form $\langle E_i \wedge E_j, E_i \wedge E_k \rangle = 0$ for $j \neq k$ . Noting $w \wedge x = \sum_{i<j}\left(w^i x^j - w^j x^i\right) E_i \wedge E_j$ , one can show by direct computation that in this metric, we have: \begin{align*}
|w \wedge x|^2 &= 2\bigg((w^1 x^2 - w^2 x^1)(w^3x^4-w^4x^3) - (w^1x^3-w^3x^1)(w^2x^4-w^4x^2) + (w^1x^4-w^4x^1)(w^2x^3-w^3x^2)\bigg) \\ &\quad+ \sum_{i<j}(w^i x^j - w^j x^i)^2 \\
&= \sum_{i<j}(w^i x^j - w^j x^i)^2 = \sum_{i\neq j} \left((w^i)^2(v^j)^2-w^i v^i w^j v^j\right) \\
&= |w|^2|v|^2-\langle w, v \rangle^2,
\end{align*} because the parenthetical term to the right of the $2$ in the first equation above simplifies to $0$ . This is obviously a different metric from the one generally constructed in the proof of existence, so is there a reason this metric fails the conditions of the problem, or is uniqueness indeed too much to ask for?","['tensors', 'exterior-algebra', 'riemannian-geometry', 'differential-geometry']"
3593658,Conditional expectation for multivariate normal: Independence trick,"Let $n\ge2$ be an integer, let $\Sigma$ be a positive semidefinite, symmetric $n\times n$ matrix of real numbers partitioned as $$\Sigma=\begin{pmatrix}\Sigma_{a,a}&\Sigma_{a,b}\\\Sigma_{b,a}&\Sigma_{b,b}\end{pmatrix},$$ where $\Sigma_{a,a}$ is $1\times1$ and $\Sigma_{b,b}$ is $(n-1)\times(n-1),$ assume $\Sigma_{b,b}$ is positive definite (i.e., invertible) and let $X=(X_1,\dots,X_n)$ be $N(0,\Sigma),$ normal with mean zero and covariance matrix $\Sigma.$ I wish to find $E(X_1\mid X_2,\dots,X_n).$ In addition, I am using the Radon-Nikodym-derivative definition of conditional expectation, so I would prefer not to compute conditional densities $f_{X_a\mid X_b}(x_a\mid x_b)=f_{X_a,X_b}(x_a,x_b)/f_{X_b}(x_b).$ From Conditional Expectation Multivariate Normal , I can guess that $E(X_1\mid X_2,\dots,X_n)=\Sigma_{a,b}\Sigma_{b,b}^{-1}(X_2,\dots,X_n)^T.$ To prove this result, I tried reasoning as follows, similar to user357269's answer to ""Conditional expectation of a joint normal distribution"": If $X_1-\Sigma_{a,b}\Sigma_{b,b}^{-1}(X_2,\dots,X_n)^T$ and $\sigma(X_2,\dots,X_n)$ are independent, then we have $$E(X_1\mid X_2,\dots,X_n)$$ $$=E(X_1-\Sigma_{a,b}\Sigma_{b,b}^{-1}(X_2,\dots,X_n)^T+\Sigma_{a,b}\Sigma_{b,b}^{-1}(X_2,\dots,X_n)^T\mid X_2,\dots,X_n)$$ $$=E(X_1-\Sigma_{a,b}\Sigma_{b,b}^{-1}(X_2,\dots,X_n)^T)+\Sigma_{a,b}\Sigma_{b,b}^{-1}(X_2,\dots,X_n)^T=\Sigma_{a,b}\Sigma_{b,b}^{-1}(X_2,\dots,X_n)^T,$$ where the last equality follows from $EX_1=0$ and $E((X_2,\dots,X_n))=0.$ However, I am stuck on showing independence. For the case $n=2,$ we can compute the covariance $\text{Cov}(X_1-\Sigma_{a,b}\Sigma_{b,b}^{-1}X_2,X_2)=0$ and appeal to a theorem. However, I am unsure what to do for larger $n,$ since $(X_2,\dots,X_n)$ is vector-valued rather than real-valued.","['probability-distributions', 'independence', 'conditional-expectation', 'probability-theory', 'probability']"
3593659,Places and schemes,"In my algebraic number theory, we're currently studying local fields, and we've noted that given a number field $K$ , its normalized discrete valuations correspond exactly to primes of $O_K$ , and we are supposed to think of its archimedean absolute values as ""primes at infinity"". This terminology suggests that the set of places (equivalence classes of absolute values modulo homeomorphism) is a sort of projectivization or compactification of $\operatorname{Spec}(O_K)$ . Is there a natural geometric structure, such as the structure of a scheme or locally ringed space, that can be put on this set? If so, what is its relation to the affine scheme of $O_K$ ? Finally, do we get something similar when we extend to arbitrary global fields, or even to function fields of complex algebraic curves? Any information or references would be appreciated.","['algebraic-number-theory', 'number-theory', 'algebraic-geometry', 'valuation-theory', 'arithmetic-geometry']"
