question_id,title,body,tags
3129029,"The tournament involves 2k tennis players they play the tournament, each played with each exactly once .What is the minimum number of rounds you","The tournament involves $2k$ tennis players they play the tournament, each played with each exactly once. What is the minimum number of rounds you need to play to find 3 such that everyone plays with everyone? In each round, $k$ games are played. I proved that if $k ^ 2 + 1$ edges will be drawn in a graph of $2k$ vertices, then a triangle is sure to exist, that is, if $k + 1$ round is held. And how to prove that $k$ rounds is not enough, I did not understand, I predict that this is proved by induction.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3129051,"How to restrict coefficients of polynomial, so the function is strictly monotonic?","I need to restrict the degree of freedom of the coefficients of a polynomial, so the function is always strictly monotonic in the domain $x\in\left[0, 1\right]$ and $y\in\left[0, 1\right]$ . The polynomial also has to go through the points $(0, 0)$ and $(1, 1)$ . The general formula for a polynomial of degree 3 is $$a_3\cdot x^3 + a_2\cdot x^2 + a_1\cdot x + a_0\quad .$$ The constraints $f(0) = 0$ and $f(1) = 1$ give $$a_0 = 0$$ and $$a_3 = 1 - a_2 - a_1$$ yielding the fitted polynomial $$(1 - a_2 - a_1)\cdot x^3 + a_2\cdot x^2 + a_1\cdot x\quad .$$ The number of parameters is already reduced to 2, but I also need the function to be strictly monotonic, so $f'(x) \geq 0$ . The additional constraints $f'(0) \geq 0$ and $f'(1) \geq 0$ give $$a_1 \geq 0$$ and $$a_2 \leq 3 - 2\cdot a_1$$ but this does not imply $f'(x) \geq 0$ in general. Is there a simple way to achieve what I want, even for the general case, where the degree of the polynomial is not restricted to 3 and can be any integer number? As a result, I want to choose the coefficients of the polynomial according to some constraints and the function is always strictly monotonic, includes the points $(0, 0)$ and $(1, 1)$ and is inside the unit square (see curves ). EDIT: I performed a Monte Carlo simulation to determine the constraints graphically (see monte carlo ). The black lines correspond to $$a_1 \geq 0$$ and $$a_2 \leq 3 - 2\cdot a_1\quad .$$ The rest looks elliptic to me. All dots inside the yellow area give monotonic increasing functions (see array of curves ). EDIT2: The accepted answer is correct. See the visual proof .","['functions', 'polynomials']"
3129073,Null Space of Bounded Linear Functionals,"Assume $f$ is a linear bounded functional from $\mathbb{R}^2\rightarrow\mathbb{R}$ . I understand that the null space of $f$ , denoted $\mathcal{N}(f)$ , is a subspace of $\mathbb{R}^2$ . However, I am having a difficult time figuring out why $\mathcal{N}$ must be a line through origin. I have looked at Griffel's Functional Analysis  Chapter 7 and also Zeidler's book but could not find an easy way to justify this.","['hilbert-spaces', 'vector-spaces', 'functional-analysis']"
3129074,"Are ""eigenspace"" and ""null space"" ever *not* synonymous?","Both the null space and the eigenspace are defined to be ""the set of all eigenvectors and the zero vector"". They have the same definition and are thus the same. Is there ever a scenario where the null space is not the same as the eigenspace (i.e., there is at least one vector in one but not in the other)?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3129080,Fixed points of a group action on tree,"Suppose a group $H$ acts on a tree $T$ , and this action fixes a point. Let $T_1$ be an $H$ invariant subtree of $T$ . How do I show that $H$ fixes a point in $T_1$ ?","['trees', 'graph-theory', 'combinatorial-group-theory', 'geometric-group-theory', 'group-theory']"
3129108,Combinations of students around 2 circular tables,"I have a question: Eleven students go to lunch. There are two circular tables in the dining hall, once can seat $7$ people, the other can hold $4$ . In how many ways can the seats be arranged. I have read this question multiple times to try to understand how to solve How many ways can $p+q$ people sit around $2$ circular tables - of sizes $p,q$? I get that table 1 will have $p=6!$ permutations of students and table 2 will have $q=3!$ permutations of students and then there needs to be a permutation of how the students are arranged around the two tables. But what I don't understand is how in the formula $${ùëù+ùëû\choose p}(ùëù‚àí1)!(ùëû‚àí1)!$$ it is $P(11,7)$ ? Shouldn't it be $P(11, 4)$ because it is that there are $11$ options of people seated at table 1 and then $4$ options of people remaining to seat at table 2? And is the answer then $P(11,7)\cdot6!\cdot3!$ . The answer in the book is $P(11,7)\cdot3!/7$ which is different from the one I get from the above formula.","['combinatorics', 'discrete-mathematics']"
3129122,When can we use derivative test to identify injective function?,"I have doubt regarding first derivative test for identifying whether a function is injective or not: For example: $$f(x)=\ln x$$ has domain $(0, \infty)$ . Now $$f'(x)=\frac{1}{x} \gt 0$$ hence $f(x)=\ln x$ is strictly increasing and hence injective. But if we consider: $$f(x)=\tan x$$ $$f'(x)=\sec^2 x \gt 0$$ But still $\tan x$ is not injective. So can I know the formal conditions to test whether a function is injective or not?","['calculus', 'functions', 'derivatives', 'monotone-functions']"
3129180,Find the max of $x_1 x_2 \cdots x_m$ when $x_1^2+x_2^2+ \cdots + x_m^2=1$,"Find the max of a function $$f(x_1, x_2,...,x_m)=x_1 x_2 \cdots x_m$$ when $x_1^2+x_2^2+ \cdots +x_m^2=1$ I am looking for unique solutions. So far I've tried two different ways. One of them was with Lagrange multiplier but there seem to be quite too many variables and I couldn't express $x_1,...,x_m$ separately. The second one solution is with the Inequality of arithmetic and geometric means. $$\frac{x_1^2+...+x_m^2}{m}\ge \sqrt[m]{x_1^2 \cdots x_m^2} \implies \frac{1}{m} \ge \sqrt[m]{x_1^2 \cdots x_m^2}=(x_1 \cdots x_m)^{\frac{2}{m}} \implies $$ $$x_1 \cdots x_m \le \frac{1}{m^{\frac{m}{2}}}$$ So we can assume that $$\max f(x_1, x_2,...,x_m)=\frac{1}{m^{\frac{m}{2}}} \quad \forall x_i=\frac{1}{\sqrt m}, \quad i=1,2,...,m.$$ I'd love to know if somebody solved this exercise with Langrange multiplier or in other way. Thanks in advance!","['optimization', 'multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
3129231,Find minimum of function and minimum point,"We have a function $$\frac{(x+\frac{1}{x})^6-(x^6+\frac{1}{x^6})}{(x+\frac{1}{x})^3+(x^3+\frac{1}{x^3})}$$ the task to find minimum of this function and point where is minimum. I tried to do by mark $t=x+\frac{1}{x}$ and then simplify this function and after that find derivative etc., but it is such a long solution. The professor, in the class, said that there is very short way to do this exercise, maybe somebody know it?","['optimization', 'maxima-minima', 'functions', 'real-analysis']"
3129268,Show that for any graph $G$ there is an ordering of the vertices for which the greedy algorithm requires $œá(G)$ colors.,"I'm learning about the greedy algorithm, but this question is to general for me to grasp (I ave no idea where to begin). Could someone please explain how I solve the problem? Show that for any graph $G$ there is an ordering of the vertices for which the greedy algorithm requires $œá(G)$ colors.","['graph-theory', 'discrete-mathematics', 'algorithms']"
3129281,Do automorphisms of quotient fields preserve the underlying ring?,"Suppose $R$ is an integral domain. Let $\gamma: \mathrm{Frac}(R) \rightarrow \mathrm{Frac}(R)$ be an automorphism of the quotient field. Is it true that $\gamma(R) = R?$ I don't think that this is true in general, but I cannot think of any examples. Are there examples for which this is not true?","['ring-theory', 'abstract-algebra']"
3129297,The digit at position $n$ of the number $x$ in base $m$,"As a solution to this question , we can define a function $f_b(x, n)$ which finds the digit in the $n$ th position of $x$ in base $b$ . $$
f_b(x, n) = \left\lfloor \frac{x}{b^n} \right\rfloor \bmod b
$$ It even works for decimals, for example: e = 2 . 7  1  8  2  8  1  8 2 8 ...
    0  -1 -2 -3 -4 -5 -6 -7 -8 -9 th position $$
f_{10}(e, -8) = 2 \quad \checkmark
$$ However, when using a non-integer base, all the digits are fractional. $$
f_\pi(e, [0,-1,-2,\dots]) = [2, 1.717, 0.867, 2.319, \dots]
$$ For reference, Wolfram Alpha states that $e$ in base $\pi$ is actually 2.2021201002111... Additionally, for negative bases, all the digits of $f$ are negative. Wolfram Alpha states that $e$ in base $-10$ is 3.3223222325590... Furthermore, the function also screws up if $x_b$ is negative. So I am also looking for a function to have a way of differentiating positive and negative numbers (especially important in negative bases, since for example, 21 in decimal would be -39 in negadecimal). My question is if there is a function which does this but is also valid even for non-integer and negative bases. I'm sure there would be, but I don't know what we need to modify such that this would happen. In other words, is there a function such that $$
f_b\left(\sum_{k=-\infty}^\infty a_k b^k, n \right) = a_n \qquad b\in\mathbb{R}
$$ ? I have since then found a recursive method for non-integer but not negative bases : Start by calculating $A=\lfloor\log_b n\rfloor$ , then $$ U_1 = f_b(n, A) = \left\lfloor\frac{n}{b^A}\right\rfloor $$ $$ U_2 = f_b(n, A-1) = \left\lfloor\frac{n-U_1b^A}{b^{A-1}}\right\rfloor $$ $$ U_3 = f_b(n, A-2) = \left\lfloor\frac{n-U_1b^A-U_2b^{A-1}}{b^{A-2}}\right\rfloor $$ etc. The simple problem with this one is when it deals with a negative base, it returns negative digits.","['functions', 'number-systems']"
3129307,Lie Bracket of Pushforwards,"Suppose $f(x,y)=(f_1,f_2,...,f_n)(x,y):\mathbb R^2\to\mathbb R^n$ is a smooth parameterization of a smooth surface $S$ in $\mathbb R^n$ . The pushforwards $\mathrm{d}f\frac{\partial}{\partial x}$ and $\mathrm{d}f\frac{\partial}{\partial y}$ are vector fields in $\mathbb R^n$ tangent to $S$ , so their Lie bracket $X=\left[\mathrm{d}f\frac{\partial}{\partial x},\mathrm{d}f\frac{\partial}{\partial y}\right]$ is too. Therefore $X$ should admit a representation in local coordinates, as the pushforward of some $\left(\alpha\frac{\partial}{\partial x}+\beta\frac{\partial}{\partial y}\right)\in\mathfrak{X}(\mathbb{R}^2)$ for some $\alpha,\beta:\mathbb{R}^2\to\mathbb{R}$ . Is my reasoning above correct? If so, how may we find $\alpha$ and $\beta$ ? If not, what can we say about $X$ ? An example would be extremely helpful. I have seen the formula $[Y,Z]=\left(Y^i\frac{\partial Z^j}{\partial x^i}-Z^i\frac{\partial Y^j}{\partial x^i}\right)\frac{\partial}{\partial x^j}$ , and I think this is what I need, but I do not see how to apply it in this case. I have also seen $\mathrm{d}f\left[\frac{\partial}{\partial x},\frac{\partial}{\partial y}\right]=\left[\mathrm{d}f\frac{\partial}{\partial x},\mathrm{d}f\frac{\partial}{\partial y}\right]$ when $f$ is a local diffeomorphism, but that does not appear to be the case here. Thank you! P.S. this is not homework. I am working through Lee's Introduction to Smooth Manifolds and I cannot find any examples where $S$ is not simply $\mathbb{R}^n$ to answer my question.","['vector-fields', 'multivariable-calculus', 'differential-geometry']"
3129312,Partitions of $2n$ and factorizations of $n$,"Let $n$ be any positive integer. Let $p_1,p_2,...,p_m$ be any positive integers such that no more than one of the $p_i$ s is $1$ and $\prod_{i=1}^mp_i=n$ . Finally, let $s_1,s_2,...,s_m$ be any nonnegative integers such that $\sum_{i=1}^ms_i=2n$ . Do there always exist nonnegative* integers $k_1,k_2,...,k_m$ such that: $$\sum_{i=1}^m(k_ip_i)=n$$ And, for each $i$ : $$k_ip_i\leq s_i$$ ? Initially, I tried induction on $m$ (the number of factors, $p_i$ ). I assumed the hypothesis held for all cases when $m$ was less than $t$ for a fixed $n$ and tried proving it for the case of $m=t$ (for the same $n$ ) but got nowhere. I also tried induction on the number of (not necessarily distinct) prime factors of $n$ but, again, only got past the base case. (I couldn't really get past viewing this through an induction lens) It seems as if it should be true given that the $s_i$ 's need to sum to $2n$ whereas the $k_ip_i$ 's need to sum to only $n$ (and the size of the $p_i$ terms are limited by the constraint that their product equal $n$ ) but I can't find a way to prove it (or a counterexample). Are there any hints anyone can provide for a proof (or for generating a good counterexample)? *EDIT: Originally, this question asked only about the existence of integer $k_i$ s - the nonnegativity of these values was not considered, but it was expected that nonnegativity would emerge from a proof of the original question. A solution to the original question was found, but the approach taken did not seem to provide any leads for showing nonnegativity.","['integer-partitions', 'number-theory', 'combinatorics', 'factoring']"
3129344,Condition when angle between two lines is ${\pi\over 3}$,"The whole question is- Show that if the angle between the lines whose direction cosines are
  given $l+m+n=0$ and $fmn+gnl+hlm=0$ is ${\pi\over 3}$ then ${1\over
 f}+{1\over g}+{1\over h}=0$ . I'm trying to solve the problem in the following manner- From the first equation $n=-l-m$ , substituting this value of $n$ in the second equation we get- $fm(-l-m)+g(=l-m)l+hlm=0$ $\implies g\left({l\over m}\right)^2+(f+g+h)\left({l\over m}\right)+f=0$ Now, the roots of this equation are ${l_1\over m_1}$ and ${l_2\over m_2}$ . So, product of them ${l_1\over m_1}.{l_2\over m_2}={f\over g}\implies {l_1 l_2\over f}={m_1 m_2\over g}$ . Similarly, we get ${m_1 m_2\over g}={n_1 n_2\over h}$ . Hence, ${l_1 l_2\over f}={m_1 m_2\over g}={n_1 n_2\over h}=K$ (say) Thus, $\cos {\pi\over3}=l_1 l_2+m_1 m_2+n_1n_2=K(f+g+h)\implies K=\frac{\sqrt{3}}{2(f+g+h)}$ . Now, I can't proceed further. I can't prove ${1\over f}+{1\over g}+{1\over h}=0$ . Can anybody solve the problem? Thanks for assistance in advance.","['analytic-geometry', 'vectors', 'geometry', '3d']"
3129418,"""Etymology"" of symbols for injections and surjections",Excuse me if this sounds silly. Does anybody know why injections and surjections are sometimes denoted symbolically as $f:V\hookrightarrow W$ and $g:V\twoheadrightarrow W$ ? How do the arrows $\hookrightarrow$ and $\twoheadrightarrow$ convey the meanings of injections and surjections?,"['notation', 'functions', 'terminology']"
3129428,Definitions for Norm of a Tensor,"I am reading a book on differential geometry and at one point the following statement is made: $|\text{Ric} + \text{Hess}(f)|^2 \geq \frac{1}{n}|R+\Delta f|^2.$ The justification is that 'we have estimated the norm of the symmetric tensor $\text{Ric} + \text{Hess}(f)$ in terms of its trace'.  I know a bit about matrix norms and norms of bounded linear operators, but I am curious what exactly is the norm of an arbitrary tensor. Is it just the Frobenius or $l^2$ norm for an arbitrary tensor?  So if it is rank 4, you take the tensor and contract with itself over the 4 indices and take the square root of the result, but then where is the $1/n$ coming from? I know $n$ would be the dimension of the space, but why do we need to divide by this and why is the Frobenius norm always bigger than or equal to the trace (the sum of the diagonal elements in matrix terms).","['tensors', 'inequality', 'normed-spaces', 'differential-geometry']"
3129433,"$\lim_{(x,y) \to (0,0)} {\frac{x^2y}{x+xy+y^2}}$","I was playing around with limits of two real variables, when I came up with the one on the title. I tried all directional limits and they were all $0.$ So then I tried a few relative limits like $y=ax^2$ , $y=x^3-x^2$ and $0\leq x=\sqrt{y}$ , but still got $0.$ I tried then with $y=\cos(x)-1,$ and it gave a complicated expression which I plugged into symbolab, and it gave $0$ too. So I tried to prove that it exists and is $0.$ So I'd need to find an inequality between the expression below and a norm $ \vert \vert (x,y) \vert \vert.$ I don't see how to manipulate it however, since the things in the denominator can be negative, and the only inequality I know can be applied here is $|xy| \leq x^2+y^2$ , which would get: $$\left|\frac{x^2y}{x+xy+y^2}\right| \leq \left| \frac{x^2+x^2y^2}{x+xy+y^2} \right|.$$ I don't know how to proceed from that, nor have any other ideas to try to disprove it exists. If possible give just a hint please, and I haven't yet learned polar coordinates nor partial derivatives.","['limits', 'multivariable-calculus']"
3129475,"Order Preserving Isomorphism from $(\mathbb{N},\leq)$ to $(\mathbb{N},\leq)$","Recall : Let $(X,\leq)$ , $(Y,\leq ')$ be two well ordered sets. Let $f:X\rightarrow Y$ be a function such that $a\leq b \rightarrow f(a)\leq 'f(b)$ we say $f$ preserves order relation. If $f$ is a bijection and $a\leq b \iff f(a)\leq 'f(b)$ fir any $a,b\in X$ we say $f$ is an order isomorphism. Let us define $\leq$ on $\mathbb{N}$ as follows: $x\leq y \iff$ $x$ divides $y$ Find all order preserving isomorphism from $(\mathbb{N},\leq)$ to $(\mathbb{N},\leq ')$ . I found one: since $1$ divides $1$ , $2$ divides $2$ , ..., then $1\leq 1$ , $2\leq 2$ ,...
, that is $Id_{\mathbb{N}}$ . Can you help for other order preserving isomorphisms ?","['elementary-set-theory', 'order-theory']"
3129491,"In equilateral ABC , Proving : $3(a^4 + b^4 + c^4 + s^4 ) = (a^2 + b^2 + c^2 + s^2 )^2$","My question is that: In equilateral $\triangle ABC$ , $AB= s$ There is a point P in same plane , the distance to three vertices are $a, b, c $ then $$ 3(a^4 + b^4 + c^4 + s^4 ) = (a^2 + b^2 + c^2 + s^2 )^2 $$ Is this equation true? If it is true How can I prove?","['analytic-geometry', 'geometry']"
3129560,How to calculate best center of an imperfect circle from N measurements of its radius,"Say I have many discrete measurements taken of the radius of a circle $[R_0,R_N]$ . I recently read a standard $^1$ in which the ""best center"" of the circle could be calculated as a displacement from the measurement axis as follows: $x = \frac{2}{N}\sum_{i=1}^{N}R_i\cos(\theta_i)$ $y = \frac{2}{N}\sum_{i=1}^{N}R_i\sin(\theta_i)$ Where the $i^{th}$ radius' measurement proceeds at angle $\theta_i$ , counterclockwise from the x-axis. The measurement axis to the ""best center"" is then represented by the vector ${\bf R_{BC}} = x\hat{\imath} + y\hat{\jmath}$ . My question is where do the factors of 2 come from? In the reduced case of two measurement points, where $R_1 = 0\hat{\imath} + 1\hat{\jmath}$ , and $R_2 = 0\hat{\imath} - 2\hat{\jmath}$ , gives ${\bf R_{BC}} = 0\hat{\imath} - 1\hat{\jmath}$ . This is a non-intuitive result for me. I would think it should be ${\bf R_{BC}} = 0\hat{\imath} - 0.5\hat{\jmath}$ , the point equidistant from the two measured radii (which is the result that would be obtained without the factor of 2). Has anyone seen something similar (with the factor of 2), or do you think the standard is wrong (see excerpt below for the relevant portion of the standard)? $^1$ The standard: Hydroelectric Turbine-Generator Units Guide for Erection Tolerances and Shaft System Alignment, Part I: Definitions, Section 3.2.9, www.ceati.com UPDATE I have since programmed a simulation to empirically investigate this topic further and I am now even more confused. Take the circular shape plotted below: If the x-axis is taken to be along the 90 degree mark and the y-axis is taken to be along the 0 degree mark, the equations for the x and y components of the best center are slightly modified to be: $x = \frac{2}{N}\sum_{i=1}^{N}R_i\sin(\theta_i)$ $y = \frac{2}{N}\sum_{i=1}^{N}R_i\cos(\theta_i)$ By symmetry, we know that the x components should cancel, and the best center should be somewhere on the positive y-axis. Moreover, we note that by definition, the best center should minimize circularity (i.e. the difference between the maximum and minimum radius, as referenced from the best center, should be a minimized when this location is truly ""best""). The following plot shows the best center at 0.25, where circularity is indeed minimized. This best center of $y=0.25$ makes intuitive sense, as this is the location that bisects the vertical deformation in the circular shape. The following plot shows the best center of this circular shape, for various number of measurement samples taken at equally spaced angles around the shape. The different lines show different scaling for the $y$ componant of the best center (the $x$ component is 0). The best match is when $K=2.\overline{27}$ , which was found empirically. Furthermore, this $K$ value doesn't appear to change for different deformations of the circle. Can anyone tell me why the best scaling for best center is $K=2.\overline{27}$ ? UPDATE 2 I developed a less uniformly deformed circular shape for my simulation. Specifically, I created a circular shape with a Gaussian bulge along a portion of its circumference with the following equations: $-\pi\leq\theta_i<\pi$ over 1000 equal discrete steps. $R_i(\theta_i)=R_{base}+\frac{A}{\sigma\sqrt{2\pi}}e^{-\theta_i^2/2\sigma^2}$ I then calculated circularity with respect to the best center as determined by varying the scaling $K$ . This produces the following plots, for $R_{base}=1.0$ , $\sigma=\frac{\pi}{6}$ , and $A=0.65644$ . This yielded $K=2.74$ , and shows that the optimal $K$ value is dependent on the circular deformity. I then decided to try a more realistic scenario. Specifically, this whole problem was prompted from a real world application of measuring the circularity of large generator rotors. A typical generator is 30 feet in diameter with a 1 inch air gap around its rotor. The following plots represent the empirical analysis for a 30 foot diameter rotor with a 500 mil (i.e. 500 thousandths of an inch, or half the air gap) deformity. In this case, the parameters of the Gaussian are $R_{base}=180000$ mils, $\sigma=\frac{\pi}{8}$ , and $A=492.2$ . These last plots make sense, as the deformity is 500 mils, and the circularity reduces to the circularity with respect to the origin at $K=0$ , which should yield a circularity of 500 mils. This is indeed the case. So, it is clear the $K$ depends on how non-uniform the circle is. Can anyone explain this dependence? UPDATE 3 Added relevant excerpt from standard as image above.","['trigonometry', 'geometry']"
3129567,Convergence on locally convex spaces,"I'm new on the locally convex spaces. I know that if $X$ is a vector space and $S$ an irreducible set of seminorms defined in $X$ , $(X,S)$ is a locally convex vector space. The first question is, how the convergence is defined in $(X,S)$ and how is a Cauchy sequence defined? And second, yet is not clear for me which are the elements that are in the topology induced by the seminorms in $S$ ? If you could recommend me a book about this I'll be grateful.","['locally-convex-spaces', 'general-topology', 'functional-analysis', 'analysis']"
3129570,Help on population differential equation,"$$\frac{\mathrm dr}{\mathrm dt} = kr \left(1-\frac{r}{r_*}\right) -\alpha fr,$$ where $k>0$ is a constant representing the rabbit breeding rate, $r_*>0$ is the (constant) maximun sustainable rabbit population size in the absence of predation, $f>0$ is the population of foxes, and $\alpha>0$ is the (constant) rate or predation of rabbits by foxes. I am told that the fox population is constant. I'm trying to solve this differential equation. I tried using the Bernoulli equation method, however it did not work out, which is shown here (habbit of mine is I accidently made $r_*$ that was for the maximum sustainable rabbit population size in the absence of predation later into $A$ since it was easier:) $$\frac{\mathrm dr}{\mathrm dt} = kr -\frac{kr^2}{r_*} -\alpha fr$$ Define $w(t)$ by $w=r^{1-2}=\frac{1}{r}$ , so that $$\frac{\mathrm dw}{\mathrm dt} = -\frac{1}{r^2} \frac{\mathrm dr}{\mathrm dt}$$ $$\implies \frac{\mathrm dw}{\mathrm dt} +kw = \frac{k}{r_*} +\alpha fw,$$ where this would become $$e^{kt} w(t) = \frac{e^{kt}}{A} +\frac{\alpha fwe^{kt}}{k} +c.$$ As I made $m(t)=k$ , leading to $e^{\int k} = e^{kt}$ and the left hand side was inverse product rule and the right hand side was integration $$ \implies w(t) =\frac{1}{A} +\frac{\alpha fw}{k} +ce^{-kt}$$ $$\implies r(t) = A +\frac{k}{\alpha fw} +\frac{e^{kt}}{c}$$","['calculus', 'ordinary-differential-equations']"
3129575,Calculate the number of points of an elliptic curve in medium Weierstrass form over finite field,Let $E$ be the elliptic curve over $\mathbb{F}_3$ in medium Weierstrass form $E:y^2=x^3+x^2+x+1$ . How to compute the number of points $|E(\mathbb{F}_{3^k})|$ ? I read that there are some formulas for computing number of points for short Weierstrass form by Frobenius endomorphism. But they don't work in this case.,"['number-theory', 'elliptic-curves']"
3129584,Derivation Verification for an Algebraic Expression,"This problem comes from a MathOverFlow thread . Inside the thread the user Per Alexandersson mentions how they learned a technique to ""simplify"" $\sqrt{7+\sqrt{3}}$ from the book Algebra for Beginners, by Todhunter . Greg Martin replied with the following formula: $\sqrt{a+\sqrt b} = \sqrt{\frac{a-\sqrt{a^2-b}}{{2}}}+\sqrt{\frac{a+\sqrt{a^2-b}}{{2}}}$ I have three questions around this, Question 1: Is the below derivation correct? If not, could someone show me how to patch it up? Derivation: \begin{align}
\sqrt{a+\sqrt b} &= \sqrt{\frac{a-\sqrt{a^2-b}}{{2}}}+\sqrt{\frac{a+\sqrt{a^2-b}}{{2}}} \\
\left(\sqrt{a+\sqrt b}\right)^{2} &= \bigg(\sqrt{\frac{a-\sqrt{a^2-b}}{{2}}}+\sqrt{\frac{a+\sqrt{a^2-b}}{{2}}}\bigg)^{2} \\
{a+\sqrt b} &= \frac{a-\sqrt{a^2-b}}{{2}} + 2\bigg(\sqrt{\frac{a-\sqrt{a^2-b}}{{2}}}\sqrt{\frac{a+\sqrt{a^2-b}}{{2}}}\bigg) + \frac{a+\sqrt{a^2-b}}{{2}} \\
{a+\sqrt b} &= \frac{a-\sqrt{a^2-b}}{{2}} + \frac{a+\sqrt{a^2-b}}{{2}} + 2\bigg( \sqrt{\frac{a-\sqrt{a^2-b}}{2}\cdot \frac{a+\sqrt{a^2-b}}{2}}\bigg)  \\
{a+\sqrt b} &= \frac{a-\sqrt{a^2-b}+a+\sqrt{a^2-b}}{2}+  2\bigg(\frac{\sqrt{b}}{\sqrt{4}}\bigg) \\
{a+\sqrt b} &= \frac{2a}{2} +  2\bigg(\frac{\sqrt{b}}{2}\bigg) \\
{a+\sqrt b} &= a +  \sqrt{b}
\end{align} Question 2: I assume that the above is if each step is reversible. But my question is (assuming the derivation is correct), how do we know each step is reversible when there are square roots involved? Question 3: Regardless if the above derivation is correct or not, could someone show me another derivation and include the steps/explanations of how these are equal? I figure another approach might enlighten me as to how the longer expression was derived from $\sqrt{a+\sqrt b}$ .","['algebra-precalculus', 'proof-verification', 'radicals']"
3129590,Projection onto the range of an operator in a nonseparable Hilbert space,"Let $A$ be a matrix with linearly-independent columns. Then $A(A^TA)^{-1}A^T$ is the orthogonal projection matrix onto the range/image of $A$ . This formula is legal because the linear independence of $A$ 's columns ensures that $A^TA$ is nonsingular. Now let $A$ be a linear operator in nonseparable Hilbert space $\cal H$ . We do not have an orthonormal basis for $\cal H$ and we cannot necessarily find a matrix representation of $A$ with linearly-independent columns. How do we find the orthogonal projection onto the image of $A$ ? My context is S.J. Bernau's The Square Root of a Positive Self-Adjoint Operator , just before lemma 16 on page 29. Bernau is proving the spectral theorem for unbounded self-adjoint operators. He shows how to construct the square root of any positive self-adjoint operator, and since $A^2$ is positive when $A$ is self-adjoint he defines an absolute value $|A|=\sqrt{A^2}$ from which he constructs the ""positive"" and ""negative"" parts of $A$ as $A^+ = {1\over2}(A+|A|)$ and $A^- = {1\over2}(A-|A|)$ . Then we can decompose $A = A^+ - A^-$ . If we repeat this reasoning with $A-\lambda I$ for arbitrary lambda, we get that $A - \lambda I = (A-\lambda I)^+ - (A-\lambda I)^-$ , and if we keep only the positive part, we get a function that is zero whenever (in the abstract spectral sense) $A\geq \lambda I$ . So the range of $(A-\lambda I)^+$ contains just those vectors that $A$ acts on with eigenvalue less than $\lambda$ (also in the abstract spectral sense), so if we can find a projection onto the range of $(A-\lambda I)^+$ we will have the projection matrix $E_\lambda$ that shows up in most statements of the spectral theorem. All of that reasoning makes sense. The trouble I have is that the paper just defines $E_\lambda$ as the projection onto the range of $(A-\lambda I)^+$ without constructing this projection. And we can't use the spectral theorem to do it because that is what we are trying to prove! So my precise question is: Let $B = (A-\lambda I)^+$ from the above problem.  We know that $B$ is a nonnegative self-adjoint operator (possibly unbounded) on some Hilbert space. How do I define an operator that is a projection onto the image of $B$ if I am not allowed to use the spectral theorem and I cannot just assume that my operator is a finite-dimensional matrix with linearly independent columns? We are free to use any special assumptions that apply to $B$ , even if they don't apply to arbitrary operators. Update: Riesz and Nagy make the same proof in section 108 of Functional Analysis . They just say ""Let $E_\lambda$ be the projection upon ${\frak L}_
\lambda$ "" as though this is something you can assume exists. So I'm adding the ""functional-analysis"" tag on the grounds that this question could be re-asked in the context of trying to understand the proof in a classic functional analysis book. Another couple of updates, since this was solved in the comments: Bernau is actually referring to the null space of $(A-\lambda I)^+$ , not its range. It is hard to tell because $\frak N$ and $\frak R$ look so similar. But my question is asking for the wrong projection operator! Riesz and Nagy have a proof in section 34 of Functional Analysis that any vector can be decomposed into the sum of an element of a subspace and a vector orthogonal to every element of the subspace. (This proof explicitly does not require the separability of the Hilbert space.) Then in section 105 they define the projection operator directly in terms of that decomposition. So this proof justifies the existence of orthogonal projections onto arbitrary subspaces, even for nonseparable spaces. So we at least have a proof that such an orthogonal projection exists. There is no tidy formula (like $P = A(A^TA)^{-1}A^T$ ) for the projection operator; I'm going to have to be happy with a formula-less proof that the operator is well-defined.","['hilbert-spaces', 'spectral-theory', 'functional-analysis']"
3129601,Orthogonal projection onto the closure of a subspace spanned by a set of orthonormal vectors,"Suppose $H$ is a Hilbert space and $\{e_1,\cdots\}$ is a set of infinitely many orthonormal vectors. Then let $M$ be the closure of the subspace spanned by these orthonormal vectors. I know that for $f\in H$ , the orthogonal projection of $f$ onto $M$ is given by $$\sum_i \langle f,e_i \rangle e_i$$ However, How can I justify that the infinite sum converges? My way is to extend these orthonormal vectors to an orthonormal basis. So we have $\{e_i\} \cup \{h_j\}$ as an orthonormal basis. Then $f=\sum_i \langle f,e_i \rangle e_i + \sum_j \langle f,h_j \rangle h_j$ . So that the concerned infinite sum must converge. However, extending these vectors to an orthonormal basis will use Zorn's Lemma. Is there any other argument to justify the convergence of $\sum_i \langle f,e_i \rangle e_i$ ?","['hilbert-spaces', 'functional-analysis']"
3129621,Proving a Subset - $(A‚à™B)‚à©C‚äÜA‚à™(B‚à©C).$,"I'm having a hard time with my subset proof. I think I'm skipping over some steps. Let $A$ , $B$ , and $C$ be sets. Prove that $(A ‚à™ B) ‚à© C ‚äÜ A ‚à™ (B ‚à© C).$ Theorem: $(A ‚à™ B) ‚à© C ‚äÜ A ‚à™ (B ‚à© C).$ Proof: Let $x ‚àà (A ‚à™ B) ‚à© C$ Assume: If $x ‚àà A$ or $x ‚àà B$ , then $x ‚àà C$ ; since $(A ‚à™ B) ‚à© C$ If $x ‚àà A$ , then $x ‚àà C$ If $x ‚àà B$ , then $x ‚àà C$ $‚à¥ x ‚àà A ‚à™ (B ‚à© C)$","['elementary-set-theory', 'proof-writing', 'proof-verification']"
3129639,"The notion of ""holes"" in topology","I was discussing with a friend about my very basic understanding of topology that it was ""basically about holes"" and she mentioned to me that the notion of holes was more complicated in higher dimensions. For $2$ -dimensional surfaces in $3$ -dimensional space we have the idea of genus that identifies a surface uniquely by the number of holes it has, but it occurred to me that in higher dimensions that this might not hold for higher dimensional surfaces and holes. In four dimensions, for example, is there a counterexample where the Betti numbers are the same for two topological spaces but they are not homeomorphic? I apologise if the question is not precise, but this is not my area of expertise.","['general-topology', 'soft-question', 'algebraic-topology']"
3129816,References for solid of revolution of a region which crosses the axis of revolution?,"Same question as this but for surface area instead of volume: Volume of revolution on an area crossing the axis 1. Is this correct to compute the volume of the solid of revolution? To compute the volume of the solid of revolution obtained by revolving the area of the region (just the regular use of the word 'region' and not region in topology ) between functions $f$ and $g$ and between $x=a$ and $x=b$ around $x$ -axis, where $a<b$ and where the region crosses the $x$ -axis, we use the function $h := \max\{|f|,|g|\}$ $$V = \pi \int_{a}^{b} (h(x))^2 dx = \pi \int_{a}^{b} (\max\{|f(x)|,|g(x)|\})^2 dx$$ $$= \pi \sum_{i=0}^{n-1}\int_{p_i}^{p_{i+1}} (\max\{|f(x)|,|g(x)|\})^2 dx$$ over some partition $\{p_0,...p_{n}\}$ of $[a,b]$ where $h(x)$ changes from one element of the partition to the next. 2. To compute the surface area of the solid of revolution, what $h$ do we use? To compute the surface area of the solid of revolution obtained by revolving the area of the region (just the regular use of the word 'region' and not region in topology ) between functions $f$ and $g$ and between $x=a$ and $x=b$ around $x$ -axis, where $a<b$ and where the region crosses the $x$ -axis, what $h$ do we use ? $$SA = 2 \pi \int_{a}^{b} h(x)\sqrt{1+(h'(x))^2} dx = 2 \pi \int_{a}^{b} ? \sqrt{1+(\frac{d}{dx}?)^2} dx $$ Is it still $\max$ ? Of course, I'm assuming overlap is still a problem in surface areas of solids of revolution just as it is a problem in computing volumes of solids of of revolution. If there's an answer out there, then you don't have to justify the answer: please just link to where I can find the answer, and I'll understand it on my own. 3. Where can I find examples or even definitions of these? I actually couldn't find any examples in Calculus by James Stewart for either question. To those who have used Stewart, do you happen to know if, in the book, there are any or that probably there aren't any (because you, like me, have tried looking)? To those who haven't used Stewart, where can I find examples please? Here's one example: Why does wolfram answer as such in this example for surface area and volume of revolution on an area crossing the axis?","['integration', 'geometry', 'real-analysis', 'calculus', 'limits']"
3129824,Prove that there are at least $4(p-3)(p-1)^{p-4}$ functions $f:S\to S$ satisfying $\sum \limits_{x\in T} x^{f(x)}\equiv a \pmod p$,"This question is the third round of Iranian exam questions, which has not been answered for several years now. I think there are many people here, which may be able to solve this problem. From AOPS Problem: $a$ is an integer and $p$ is a prime number and we have $p\ge 17$ . Suppose that $S=\{1,2,....,p-1\}$ and $T=\{y|1\le y\le p-1,\operatorname{ord}_p(y)<p-1\}$ . Prove that there are at least $4(p-3)(p-1)^{p-4}$ functions $f:S\longrightarrow S$ satisfying $$\sum_{x\in T} x^{f(x)}\equiv a \pmod p$$ It seems that this problem can be solved by generating function, but I don't know how to start. Thank you.","['modular-arithmetic', 'number-theory', 'elementary-number-theory', 'discrete-mathematics', 'algebra-precalculus']"
3129843,Finding lengths when circles and squares tangents. [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Should one approach by coordinates or by euclidean geometry? By pure geometry, I am not able to solve.","['tangent-line', 'circles', 'geometry']"
3129898,Can we show that $\operatorname E[(X-\operatorname E[X])^k]\le\operatorname E[X^k]$ if $X\ge 0$?,"If $X$ is a integrable nonnegative random variable and $k\in\mathbb N_0$ , are we able to show that $\operatorname E[(X-\operatorname E[X])^k]\le\operatorname E[X^k]$ ? That's clearly not true if $X$ might be negative.","['expected-value', 'probability-theory']"
3129901,Show that the function $f(x)=x/(x^2-1)$ is bijective.,"I have to show that the function $f:(-1,1)\to \mathbb{R}$ $$f(x)=\frac{x}{x^2-1}$$ is bijective. I have shown that it is injective which is pretty simple. I'll write it out here for future reference. To show that $f$ is injective, let $x_1,x_2\in(-1,1)$ . Assume that $f(x_1)=f(x_2)$ . Then $$\frac{x_1}{x_1^2-1}=\frac{x_2}{x_2^2-1}$$ Multiplying both sides by $(x_1^2-1)(x_2^2-1)$ which we know can't be $0$ . On simplifying the equation further, we get $$(x_1x_2+1)(x_2-x_1)=0$$ This means either $x_2x_1=-1$ or $x_2=x_1$ . It isn't possible that $x_2x_1=-1$ because if it were true then $x_2=\cfrac{-1}{x_1}$ and for all the value of $x_1$ in the domain, $x_2$ will go out of the domain. Thus, it must be that $x_2=x_1$ . This concludes the injectivity part. What I am confused about is the surjectivity part. I know that you usually invert the function and then show that for any value $y$ in the codomain, there exists a value $x$ in the domain of the function such that $f(x)=y$ . I can't seem to invert this function appropriately. Any hints?","['functions', 'real-analysis']"
3129947,Minimising $\sum \frac{(-1)^{a_n}}{n!}$,"Let $a_n$ be a sequence of positive integers. Then I want to find the minimum possible value of $$\Bigg|\sum_{n=0}^\infty \frac{(-1)^{a_n}}{n!}\Bigg|$$ I know that the maximum absolute value is $e$ given where all of $a_n$ are odd or even, but I am unsure how to approach minimising the summation.","['maxima-minima', 'summation', 'sequences-and-series']"
3129974,"Find $\lim\limits_{x\to0^+}x(\lfloor \frac{1}{x}\rfloor+\lfloor \frac{2}{x}\rfloor+\cdots+\lfloor \frac{k}{x}\rfloor), \, k \in \mathbb N$.","$$M:=x\left(\left\lfloor \frac{1}{x} \right\rfloor+\left\lfloor \frac{2}{x}\right\rfloor+\cdots+\left\lfloor \frac{k}{x}\right\rfloor\right),\, k \in \mathbb N.$$ Using $\lfloor y \rfloor=y-\{y\}$ , $$M=x\sum_{i=1}^{k}\left(\frac{i}{x}-\left\{\frac{i}{x}\right\}\right)=\frac{k(k+1)}{2}-x \sum_{i=1}^{k}\left\{\frac{i}{x}\right\}$$ Since $0\leq\{y\}<1$ , the coefficient of $x$ is finite and thus, $$\lim_{x\to0^+}M=\frac{k(k+1)}{2}$$ Is this correct?","['summation', 'ceiling-and-floor-functions', 'proof-verification', 'sequences-and-series', 'limits']"
3129987,"Maximum order of a finite 2-group which occur as a subgroup of $GL(k, Z)$?","Suppose $H$ is a finite 2-group and $H$ is a subgroup of $GL(k,\Bbb Z)$ . What can be the maximum order of $H$ ? We can embed $(\mathbb Z _2)^k$ in $GL(k;\Bbb Z)$ as diagonal elements. So $(\mathbb Z _2)^k$ is a subgroup of $GL(k;\Bbb Z)$ of order $2^k$ .  So maximum order of $H\ge 2^k$ . I can not proceed further. A detailed proof will be very helpful. Thank you so much in advance.","['group-theory', 'finite-groups']"
3129994,What is the meaning of $\int_{0}^{k}\pi(x)dx?$,"I saw this post about the prime counting function. Let's look at this intgeral: $$I(k)=\int_{0}^{k}\pi(x)\mathrm dx$$ where $I(k)=0,0,1,3,5,8,...$ for $k=1,2,3,4,5,6...$ What is this integral trying to show me? All these values of $I(k)$ and its relation, what does it mean? Let $$S(k)=\sum_{j=2}^{k-1}\pi(j)$$ I saw these relations: $$S(2k+1)-2S(2k+2)+S(2k+3)=0$$ $$S(6k+1)-3S(6k+2)+3S(6k+3)-S(6k+4)=0$$ $$S(6k+1)-4S(6k+2)+6S(6k+3)-4S(6k+4)+S(6k+5)=0$$ Meaning? Is there some usefulness to understand prime numbers distribution?","['number-theory', 'definite-integrals', 'prime-numbers']"
3130047,"For matrices, under what conditions can we write $AB = BC$?","If $A$ and $B$ are real matrices, when does there exist a matrix $C$ such that $AB = BC$ ? I understand that there is the case where $A$ and $B$ commute so $AB =BA$ , but is there a more general rule (ie. necessary and sufficient conditions)? Thanks!","['matrices', 'matrix-equations', 'linear-algebra']"
3130063,Where to learn about differential operators?,"I was reading the wikipedia page on Differential Operators. . It introduces the general concept of a differential operator and then it
goes through some examples. It mentions the derivative, the theta operator, the laplacian, the del operator, what is an adjoint of a linear operator, etc.. I'd like to know if there are some books,resources where these topics are treated in a general, related way.","['derivatives', 'reference-request']"
3130102,Convergence of diagonal of double sequence of random variables,"I have a double sequence of random variables $X^n_m$ , where $n, m \in \mathbb{N}$ . There exist variables $X^n$ and $X$ such that $X_m^n \overset{m\to \infty}{\longrightarrow} X^n$ almost surely and $X^n \overset{n\to \infty}{\longrightarrow} X$ in probability where $X$ is a constant . There also exist variables $X_m$ such that $X_m^n \overset{n\to \infty}{\longrightarrow} X_m$ in probability and $X_m \overset{m\to \infty}{\longrightarrow} X$ almost surely. My question : under what conditions can it be concluded that the 'diagonal' sequence $X_n^n \overset{n\to \infty}{\longrightarrow} X$ in probability? In my setting, the sequence $X^n$ are uniformly bounded and at least $0$ , i.e. $\exists K : \mathbb{P}(0 \leq X^n \leq K)=1$ for all $n$ . But I'm not sure how to use this fact or whether it is helpeful. Here's what I've tried so far: To prove that $X_n^n \overset{n\to \infty}{\longrightarrow} X$ in probability, it is suffices to prove that the double limit $X_m^n \overset{n,m\to \infty}{\longrightarrow} X$ in probability, i.e. that $\forall \epsilon, \delta>0\ \  \exists C$ such that $n, m > C \implies |X - X_m^n| > \epsilon$ with probability $\leq \delta$ . So, let $\epsilon, \delta > 0$ and try to prove existence of such a $C$ .
Observe that $|X - X_m^n| \leq |X - X^n| + |X^n - X_m^n|$ and using the fact that almost sure convergence implies convergence in probability, it follows that $\exists N(\epsilon, \delta) : n > N(\epsilon, \delta) \implies |X - X^n| > \epsilon$ with probability $\leq \delta$ $\exists M(\epsilon, \delta, n) : m > M(\epsilon, \delta, n) \implies |X^n - X^n_m| > \epsilon$ with probability $\leq \delta$ The issue here is that since $M$ depeneds on $n$ , it could be that $M(\epsilon, \delta, N(\epsilon, \delta)) > N(\epsilon, \delta)$ . If this is the case, then (informally) $m$ needs to grow faster than $n$ meaning that no such $C$ can exist. I'm not sure how to resolve this problem.","['convergence-divergence', 'probability-theory', 'random-variables']"
3130129,Filtration vs Natural Filtration,"I have been reading about natural filtrations, and I think I have a good grasp of them. However, I also found out that natural filtrations are just one type of filtration in general. Now I am not able to imagine a filtration other than a natural filtration which would serve a purpose. So the question is, how does a filtration generalize the idea of a natural filtration, and in particular, where would I use a filtration which is not a natural filtration. (For context, I am studying valuation of derivative contracts, and they are obviously riddled with natural filtrations.)","['measure-theory', 'probability']"
3130139,Neyman - Pearson - Test with Poisson distribution,"Let $X$ be a Poi( $\nu$ ) distributed rv with unknown $\nu \in (0,\infty)$ and we set $ \theta:=\nu $ . We want to test the hypotheses $ H_0:\nu_0=1 $ against $ H_1:\nu_1=\nu_0=2 $ a) We look at the (randomized) Neyman - Pearson - Test which has the form $T(k) = \begin{cases}
1 & R(k) > c \\
\gamma & R(k)=c \\
0 & R(k)<c
\end{cases}$ , with $c \in \mathbb{R} $ and $\gamma \in [0,1]$ $ R(k) := \frac{P_{\nu_1}(X=k)}{P_{\nu_0}(X=k)} $ is the Maximum Liklihood quotient. Compute $c^*$ and $ \gamma^* $ such that $T$ is a test with niveau $\alpha = 0.05$ i.e. $E_{\nu_0}[T]=\alpha$ . b) You see that $X=3$ . How do you decide in terms of the test wich you construct in a)? Values of a Poi( $\nu$ ) distributed rv for task a') $ P(X\leq 0) = 0.368$ $P(X\leq 1) = 0.736$ $P(X\leq 2) = 0.92$ $P(X\leq 3) = 0.981$ $P(X\leq 4) = 0.996$ $P(X\leq 5) = 0.999$ I have computed $ R(k) = \frac{e^{-2n} \prod_{i=1}^n \frac{2^{k_i}}{k_i !} }{e^{-n}\prod_{i=1}^n \frac{1}{k_i !}} = e^2 2^{\sum_{i=1}^n k_i} $ . But how can i now compute $c^*$ ? If i have $c^*$ then i can compute $\gamma^*$ with the fact that $ E_{\nu_0}[T] = P(R(k)>c^*) + \gamma^* P(R(k)=c^*) = 0.05 = \alpha  $","['statistics', 'probability-theory', 'probability', 'hypothesis-testing']"
3130163,Spectrum of the ring of formal power series over integers [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question Let $\mathbb{Z}[[X]]$ be the formal power series ring over $\mathbb{Z}$ . I want to understand the set of prime ideals $\rm{Spec}(\mathbb{Z}[[X]])$ maximal ideals $\rm{Spm}(\mathbb{Z}[[X]])$ and the Jacobson radical $J(\mathbb{Z}[[X]])$ . How can one write these sets explicitly?","['maximal-and-prime-ideals', 'ring-theory', 'algebraic-geometry', 'formal-power-series', 'commutative-algebra']"
3130170,Derivative with respect to vectorized inverse Kronecker product,"I am trying to derive the gradient of a function I wish to optimize, and wish to obtain the following derivative: $$
\frac{\partial}{\partial \pmb{x}} \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1} \pmb{y}
$$ with $\pmb{x} = \mathrm{vec}(\pmb{X})$ , $\pmb{X}$ being a square asymetric matrix and $\pmb{y}$ a vector that is not a function of $\pmb{x}$ , and $\otimes$ the Kronecker product. My thought was to first write: $$
\left( \pmb{y}^{\top} \otimes \pmb{I} \right)   \mathrm{vec}\left( \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right) 
$$ next to let $\pmb{f} = \mathrm{vec}\left( \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right)$ and then to express the differential of $\pmb{f}$ . I got to: $$
d\pmb{f} = \left(\left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right) \left( \mathrm{vec}\left( (d\pmb{X}) \otimes \pmb{X} \right)  + \mathrm{vec}\left( \pmb{X} \otimes (d\pmb{X})\right) \right) 
$$ in which $-\top$ is short for the transpose of an inverse. This seems close to the answer, but not quite there yet. I guess I am getting lost in trying to express $\mathrm{vec}\left( (d\pmb{X}) \otimes \pmb{X} \right)$ in terms of $d\pmb{x}$ . Edit: continuing this, I recognized there must be some permutation matrix $\pmb{P}$ such that: $$
\pmb{P}\mathrm{vec}( (d\pmb{x})\pmb{x}^{\top} ) = \mathrm{vec}((d\pmb{X})  \otimes \pmb{X})
$$ which I can use to further derive: $$
\begin{align}
d\pmb{f} &= \left(\left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-1}\right)\pmb{P}\left((\pmb{b} \otimes \pmb{I}) + (\pmb{I} \otimes \pmb{b})\right)d\pmb{b} \\
\frac{\partial \pmb{f}}{\partial \pmb{b}} &= \left(\left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-1}\right)  \pmb{P}\left((\pmb{b} \otimes \pmb{I}) + (\pmb{I} \otimes \pmb{b})\right).
\end{align}
$$ Which seems plausible. Thus, all that seems to be needed is an expression for $\pmb{P}$ . I guess that will take a similar form as this answer , but I am not sure about it.","['differential', 'vectorization', 'matrix-calculus', 'kronecker-product', 'derivatives']"
3130274,reflect a point over another point using matrix transformation,"We know that if we want to reflect any point over an origin, i.e. $ O\left(0, 0\right) $ , we can use matrix transformation like this $$ \left(\begin{matrix}x' \\ y'\end{matrix}\right) = \left(\begin{matrix}-1 & 0 \\ 0 & -1\end{matrix}\right)\left(\begin{matrix}x \\ y\end{matrix}\right) = \left(\begin{matrix}-x \\ -y\end{matrix}\right). $$ But, what if we reflect any point over another point $ M\left(a, b\right) $ with $ a, b \ne 0 $ ?","['matrices', 'coordinate-systems', 'transformation', 'reflection']"
3130278,Finding a mistake using Mayer-Vietoris,"I was computing the homology of $S^3-\coprod_{i=1}^4 I_i$ , where $I_i=[0,1]$ for all $i$ (they are being identified with an embedding). Intuitively, this should be homotopy equivalent to $S^1$ , since removing one interval gives something homotopic to $\mathbb{R}^3$ , removing another one gives an espace homotopic to $S^2$ , removing the third one results in something homotopic to $\mathbb{R}^2$ and finaly the last one ends up with a space homotopic to $S^1$ . Therefore, $H_2(S^3-\coprod_{i=1}^4 I_i)=0$ . But in other calculations this caused me some problems so I decide to do it formally using Mayer-Vietoris. I had already computed $H_*(S^3-I\sqcup I)$ , giving me a consistent result with the intuition above, i.e. $H_2(S^3-I\sqcup I)=\mathbb{Z}$ . Now I decompose $S^3=(S^3-\coprod_{i=1}^2 I_i)\cup (S^3-\coprod_{i=3}^4 I_i)$ . From Mayer-Vietoris there is a short exact sequence $$0\to H_3(S^3)\to H_2(S^3-\coprod_{i=1}^4 I_i)\to H_2(S^3-I_1\sqcup I_2)\oplus H_2(S^3-I_3\sqcup I_4)\to 0$$ From my calculations this would be $0\to\mathbb{Z}\to H_2(S^3-\coprod_{i=1}^4 I_i)\to \mathbb{Z}\oplus\mathbb{Z}\to 0$ But then $H_2(S^3-\coprod_{i=1}^4 I_i)\neq 0$ , which is inconsistent with my first reasoning. Where is the mistake?","['homological-algebra', 'proof-verification', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
3130279,Confused by Baby Rudin chapter $3$ problem $19$,"Associate to each sequence $a={a_n}$ , in which $a_n$ is $0$ or $2$ , the real number $x(a)=\sum \frac{a_n}{3^n}$ . Prove all the $x(a)$ is precisely the Cantor set. My attempt: If I let $a_n=2$ then $x(a)=1$ , $a_n=0,\text{ then }   x(a)=0$ . So, $x(a)\in [0,1]$ . That problem looks right and I need a more precise proof. I notice when I take $a_n=2$ , I do the sum from $1$ to $n$ it always the right point of interval I know a technology that proof two set are equal if I prove $A\subset B\,\text{ and }B \subset A$ then $A=B$ but it seems like I can‚Äôt use that technology.","['limits', 'calculus', 'analysis', 'real-analysis']"
3130389,"Does there exist a continuous, open, and surjective map from $f\colon\mathbb{R}^n\to\mathbb{R}^m$ for $m>n$?","My question is that from above. Here are my approaches so far: I know that there is no homeomorphism between an open set of $\mathbb R^n$ and an open set of $\mathbb R^m$ . So if there is an open set where $ f $ is injective we get a contradiction. Furthermore, since $ f $ is surjective there are right-inverses of $ f $ . If there was a continuous right-inverse, we would also get a contradiction since this right-inverse would be a continuous injection from $ \mathbb R^m $ to $ \mathbb R^n $ which cannot exist by Borsuk-Ulam. Unfortunately, I was not able to use one of these two approaches to give an answer to my question. If the answer is yes, I would also be interested in stronger assumptions on $ f $ to make the answer no. I wonder if uniform continuity does the job, since for Hoelder continuity and large enough $ m $ the answer is no even if we drop the openness of $ f $ (This one can prove using Hausdorff-Dimension and how Hoelder continuous maps preserve them.) Thanks for your help!","['continuity', 'general-topology', 'open-map', 'real-analysis']"
3130390,Maximum value of coefficient in Multinomial Expansion,"Find the maximum value of coefficient in the expansion of $(x+y+z+w)^{25}$ . Basically what the question is saying is that all term will be of type $k x^{r_1}y^{r_2}z^{r_3}w^{r_4}$ so what can be maximum value of $k$ . Well in binomial expansion, middle binomial coefficients are greatest but how to expand that thought here? I wrote it as $(a+b)^{25}$ , in this expansion the term having greatest coefficient will be $C(25,13) (x+y)^{12}(z+w)^{13}$ and then take maximum binomial coefficient of $(x+y)^{12}(z+w)^{13}$ to get answer as $C(25,13) \times C(12,6) \times C(13,7)$ but I am not sure it is correct. Could someone help me with this?","['algebra-precalculus', 'binomial-coefficients', 'binomial-theorem']"
3130391,Does $A_n\leq G\leq S_n$ imply either $G=S_n$ or $G=A_n$?,"Is it true that if $A_n\leq G\leq S_n$ implies $G=A_n$ or $S_n$ ? Here is my argument: Since $|G|=m$ divides $n!$ we have $mk=n!$ for some $k$ . But $|A_n|=n!/2\leq n!/k$ , hence $k\leq 2$ , hence $m=n!/k$ is either the full order of $S_n$ or that of $A_n$ .","['symmetric-groups', 'group-theory', 'proof-verification', 'finite-groups']"
3130437,"$10$ people ($6$ male, $4$ female) divided into $2$ equal groups: what is the probability that all females are in the same group?","This question comes from a completed, marked, and returned exam. It will not likely be reused. Problem As stated in the question above Work First, I note that there are $\binom{10}{5}$ possible groupings. Second, I note that, if all $4$ females are in the same group, then the remaining fifth member is one of the boys: there are $\binom{6}{1} = 6$ ways to choose the fifth member. So I conclude $\Pr = \frac{6}{\binom{10}{5}} = \frac{1}{42}$ . Question I was marked incorrect: the given answer is $\frac{1}{21}$ , or exactly twice my answer. What reasoning led to this conclusion? Why does it seem like some sort of symmetry argument allows us to conclude there are $12$ ways to choose the fifth member?","['statistics', 'combinatorics', 'probability']"
3130458,Algebra & Artificial Intelligence (AI),"Artificial intelligence , especially deep learning & neural networks for image processing and classfication, are related to statistics and physics e.g. as decribed in below papers. Statistics and AI Simoncelli + Olshausen.Natural images statistics and neural representation Field.What the statistics of natural images tell us about visual coding Physics and AI A relation of deep learning and physics (renormalization group) is elaborated in: Mehta + Schwab.An exact mapping between the Variational Renormalization Group and Deep Learning Also category theory is used to describe neural systems and single aspects like e.g. back propagation. Category theory and AI MSE - Category Theory & Artificial Intelligence (AI) Question . Are there interesting formulations and generalizations of (convolutional) neural networks (CNNs) / deep learning (or to machine learning in general) concepts in an algebraic context? Fernando Martin-Maroto and Gonzalo G. de Polavieja published in 2018 Martin-Maroto and Polavieja.Algebraic Machine Learning See also the following pages Martin-Maroto and Polavieja et al.Algebraic AI YouTube.ALMA - Human Centric Algebraic Machine Learning European Commission.ALMA: Human Centric Algebraic Machine Learning Clifford Neural Networks based on Clifford Algebras are subject in Hitzer et al.Applications of Clifford‚Äôs Geometric Algebra mentioning Clifford algebra neural computing . However as far as I understand, this is more a generalization of neuron's in- and output values rather than an algebraic formulation of neural network related concepts? In a similar direction(?) goes e.g. Bronstein et al.Geometric deep learning: going beyond Euclidean data Remark. I am aware that this question could also be posted at stack overflow or some other stack exchange site. However my idea is that it could be better to ask it on the mathematics site, since it might rather be mathematicians that have some knowledge about such connections than e.g. computer scientists themselves.","['big-list', 'neural-networks', 'machine-learning', 'artificial-intelligence', 'abstract-algebra']"
3130477,What is the probability of getting equal numbers of heads and tails?,"I'm doing my hw, there is a question that I am not sure if I'm correct. Here is the question A fair coin is thrown repeatedly. What is the probability that on the $n$ th throw, the numbers of heads and tails to date are equal? My answer is as follows: $ \mathrm{The\ required\ situation\ holds\ only\ when} \ n\ \mathrm{is\  even.}\\
\mathrm{Let}\ n=2k,\ \mathrm{then\ the\ required\ probability} = {2k \choose k} \left( \frac{1}{2} \right)^{k} \left( \frac{1}{2} \right)^{k} = {2k \choose k} \left( \frac{1}{2} \right)^{2k} 
\\ 
\mathrm{And\  I \ searched\ this\ question\ online\ and\ found\ that\ someone\ said\ that\ the \ probability\ is} \left( \frac{n}{2} \right)\left( \frac{1}{2} \right)^{n}  $ Here is the link: https://www.algebra.com/algebra/homework/Probability-and-statistics/Probability-and-statistics.faq.question.779221.html I also tried to expand ${2k \choose k}$ but failed to get the expression $\left( \frac{n}{2} \right)$ May I know whether I am correct or where did I do it wrongly, thanks.","['statistics', 'probability-distributions', 'binomial-distribution', 'probability']"
3130501,Sub-Exponential and its Orlicz norm,"Let $Z$ be a sub-Exponential r.v. with mean 0 and variance 1. Can we have a bound on its Orlicz norm, $\| Z \|_{\psi_1}$ , both upper and lower?","['inequality', 'probability-theory', 'random-variables']"
3130534,What makes the 'special groups' ($\det A = 1$) special?,"This is a rather basic, and open-ended question: in several branches of mathematics and physics, we make an effort to classify linear operators $A$ , especially orthogonal or unitary operators, by whether or not they have unit-determinant $\det A = 1$ , i.e. whether they are special or not. E.g., the special orthogonal group $\operatorname{SO}(N)$ is a subset of the orthogonal group $\operatorname{O}(N)$ and the special unitary group $\operatorname{SU}(N)$ is a subset of the unitary group $\operatorname{U}(N)$ . At the most basic level: Why are we often more (or especially) interested in these special operators? When acting on a vector, which quantities do the special operators leave invariant? What, if any, additional constraints are made on e.g. the spectrum of such special operators?","['group-theory', 'abstract-algebra', 'lie-groups', 'algebraic-groups']"
3130601,Integration by Substitution in $\int_0^{\infty}x^r\frac 1{\sqrt{2\pi}x}e^{-(\log x)^2/2}[\sin(2\pi\log x)]dx$,"In Casella and Berger (2002) I found an example for non-unique moments (example 2.3.10 on page 64). They are providing the following 2 pdfs: $f_1(x) = \frac 1{\sqrt{2\pi}x}e^{-(\log x)^2/2}$ , where $0 \leqslant x \leqslant\infty$ and $f_1(x) = f_1(x) [1 + \sin(2\pi\log x)]$ , where $0 \leqslant x \leqslant\infty$$ The former is lognormal with $\mu = 0$ and $\sigma^2 = 1$ . It can be shown that if $x_1 \sim f_1(x)$ , then $E(x_1^r) = e^{r^2/2}$ . Hence, $x_1$ has all the moments. However, $E(x_2^r) = \int_0^{\infty}x^rf1(x)[1+\sin(2\pi\log x)]dx = E(x_1^r)+\int_0^{\infty}x^rf1(x)[\sin(2\pi\log x)]dx.$ Since the last integral is equal to zero, we can show that $x_1$ and $x_2$ have distinct pfds but the same moments. My question is about how to show that the last integral is equal to zero. In exercise 2.35, Casella and Berger (2002, p. 81) show how to prove this (also see here on page 2-11). $\int_0^{\infty}x^rf1(x)[\sin(2\pi\log x)]dx = \int_0^{\infty}x^r\frac 1{\sqrt{2\pi}x}e^{-(\log x)^2/2}[\sin(2\pi\log x)]dx$ Now, Cassella and Berger propose to substitute $y = \log x$ so that $dy = (1/x)dx$ . Hence, $\int_0^{\infty}e^{(y+r)r}\frac 1{\sqrt{2\pi}x}e^{-(y+r)^2/2}[\sin(2\pi y + 2\pi r)]dx$ $\int_{-\infty}^{\infty}\frac 1{\sqrt{2\pi}x}e^{(r^2-y^2)/2}[\sin(2\pi y + 2\pi r)]dx$ Hence, the the integrand is an odd function. Thus, the negative integral cancels the positive one. This all makes sense. However, when I try to understand the substitution performed, I wonder if Casella and Berger meant to substitute using $y + r = \log x$ . My question may be trivial but would this substitution be allowed/ common? If so, then I would never have seen such an integration by substitution using 2 summands.","['integration', 'moment-generating-functions', 'substitution', 'probability']"
3130606,Why $E[X\mid Y] = E[X\mid\sigma(Y)]$?,"Let $X$ be an integrable random variable on $(\Omega, F, P)$ . Let $Y$ be measurable from $(\Omega, F, P)$ to $(A, G)$ . The conditional expectation of $X$ given $Y$ is defined to be $E[X\mid Y] = E[X\mid\sigma(Y)]$ I can not understand the reason for this equality, how does the expectation of a random variable ( $X$ ) given another one ( $Y$ ) is equal the expectation of that same random variable ( $X$ ) given the $\sigma$ -algebra generated from this another one ( $Y$ )?","['conditional-expectation', 'statistics', 'conditional-probability', 'probability-theory']"
3130612,Integration by substitution in $f: \mathbb{R^2} \to \mathbb{R}$ with Jacobian method proof,"I've searched everywhere on Google for a somewhat formal proof for the integration by substitution -- a.k.a $u$ -substitution or change of variables -- in double integral with the Jacobian determinant method but I can't find any. Anyone knows any sources for the proof? In case if the theorem is not familiar it goes something like: $$
\int\int_R f(x)d^2x=\int\int_Tf(G(u)) |\det DG|\, d^2u
$$","['integration', 'multivariable-calculus', 'change-of-variable']"
3130628,Mapping a round-robin tournament to prove one winning team can be selected?,"This is a question on my Graph Theory homework, and I wanted some help in solving it: If a tournament involving 2n teams has the following properties: 1.every day of the tournament involves n matches (with no teams repeated in a given day). each team plays every other team exactly once during the tournament, for a total of 2n ‚àí 1 days. there are no ties. Show that it is possible to choose one winning team from each day of the tournament, with no team chosen more than once. I tried to prove it through induction but keep getting stuck when I try to move beyond a base case.. Any idea how to build on this?","['graph-theory', 'combinatorics']"
3130634,Geometric interpretation of the second Bianchi identity?,"Assuming a torsion free Christoffel symbol, the covariant derivative can be shown to satisfy the second (differential) Bianchi identity: \begin{equation}
[[\nabla_a,\nabla_b],\nabla_c]+[[\nabla_c,\nabla_a],\nabla_b]+[[\nabla_b,\nabla_c],\nabla_a]=0
\end{equation} Question: Is there a nice geometric interpretation of this identity? One of my motivations for this question is that this condition can be interpreted as stating that the co-variant derivatives form a Lie algebra (with the algebra product given by the commutator). Thus a geometric interpretation of the second Bianchi identity may motivate why the Jacobi identity is natural/fundamental.","['riemannian-geometry', 'lie-algebras', 'lie-groups', 'differential-geometry']"
3130636,Real matrix satisfying $A^3=4I_n-3A$,"Let $A\in M_n(\mathbb{R}) $ so that $A^3=4I_n-3A$ . Prove that $\det(A+I_n) =2^n$ . My work : $A$ 's eigenvalues are the roots of $x^3+3x-4=0$ , so one of the eigenvalues is $1$ and the others are $\lambda_1$ and $\lambda_2$ , the roots of $x^2+x+4=0$ . Hence, $$\det(A+I_n) =2(\lambda_1+1)(\lambda_2+1)=2(\lambda_1  \lambda_2 +\lambda_1+\lambda_2+1) =2(4-1+1)=8$$ What is my mistake?","['matrices', 'determinant', 'linear-algebra']"
3130639,"Why is $ \cos(x, y) $ given with two arguments in this paper?","In the paper Phrase-Based & Neural Unsupervised Machine Translation on page 4 the authors give a probability equation: $$\Large{ p(t_j|s_i) = \frac{e^{\frac{1}{T}\cos(e(t_j), W e(s_i))}}{\sum_ke^{\frac{1}{T}\cos(e(t_k),W e(s_i))}} }$$ Here, twice the authors use $cos$ with two arguments, both at the top and the bottom of the fraction. There is no reference to $\cos$ indicating any other function in the paper. The paper does mention a rotation matrix, which leads me to believe that $\cos$ is used in its classical trigonometric form. I have looked online for instances where this notation is used, but could not find any. Can someone explain what the double arguments mean in this instance?","['trigonometry', 'functions']"
3130665,"$[a,b]$ as a smooth manifold with boundary has global coordinates?","Consider a compact interval $[a,b]$ . If $[a,b]$ had global coordinates, then there would be an homeomorphism $f:[a,b]\to U$ where $U$ is an open subset of $\mathbb{R}$ or an open subset of $[0,\infty)$ . But $U$ cannot be open in $\mathbb{R}$ because $U$ is compact, so $U$ must be open in $[0,\infty)$ . But also in this case I think U must be a closed interval, precisely $U=$ [min $f$ ,max $f]$ . But this set cannot be open in $[0,\infty)$ . So i deduce that $[a,b]$ does not have global coordinates. But then in John Lee's book Introduction to smooth manifolds i read a statement of the form ""Let $t$ denote the stadard coordinate on $[a,b]$ "". So how should I interpret the statement above?","['smooth-functions', 'smooth-manifolds', 'multivariable-calculus', 'general-topology', 'differential-geometry']"
3130699,Do the boundary points matter when defining the PDF of a continuous RV?,"Lets say we are given the following definition of a CDF $F(x)$ . $$
  F(x)=\begin{cases}
       0,     && x        &\lt        &\sqrt{2} \\
       x^2-2, && \sqrt{2} &\leq x \lt &\sqrt{3} \\
       1,     && \sqrt{3} &\leq x
       \end{cases}
$$ Does it even matter whether we define the respective PDF $f(x)$ as $$
  f(x)=\begin{cases}
       2x,     && x \in [\sqrt{2}, \sqrt{3}] \\
       0,      && \text {otherwise}
       \end{cases}
$$ versus the following $$
  f(x)=\begin{cases}
       2x,     && x \in [\sqrt{2}, \sqrt{3}) \\
       0,      && \text {otherwise}
       \end{cases}
$$ Basically I am asking whether or not the brackets matter. Or are they all the same, and I can just play around with any combination I want $[] = () = [) = \ ...$ ?","['notation', 'statistics', 'probability']"
3130729,"If $\mathfrak m_s$ generates $\mathfrak m_x$ and $\kappa(x)/\kappa(s)$ is finite separable, then $\Omega_{X/S,x} = 0$","I'm trying to understand the various equivalent definitions of an unramified morphism of schemes.  Let $f: X \rightarrow S$ be a morphism of schemes which is locally of finite type, $x \in X$ , and $s = f(x)$ .  Let's say that $f$ is unramified if the stalk of $\Omega_{X/S}$ at $x$ is zero.  I want to understand the following result: If the maximal ideal $\mathfrak m_x$ is generated by the maximal ideal $\mathcal m_s$ , and if $\kappa(s) \subset \kappa(x)$ is a finite separable extension, then $f$ is unramified at $x$ . Let $k = \operatorname{Spec} \kappa(s)$ , $X_s = X \times_S \operatorname{Spec} k$ be the fiber of $s$ , and $p: X_s \rightarrow X$ the ""projection.""  We have $$p^{\ast} (\Omega_{X/S}) = \Omega_{X_s/k}$$ And therefore $$(\Omega_{X_s/k})_x = (\Omega_{X/S})_x \otimes_{\mathcal O_{X,x}} \mathcal O_{X_s,x} = (\Omega_{X/S})_x \otimes_{\mathcal O_{X,x}}(\mathcal O_{X,x} \otimes_{\mathcal O_{S,s}} \kappa(s)) = (\Omega_{X/S})_x \otimes_{\mathcal O_{S,s}} \kappa(s)$$ I'm trying to reduce the problem to the special case when $S$ is the spectrum of the field.  I thought I could use Nakayama's lemma to say that if we show $(\Omega_{X_s/k})_x = 0$ , then $(\Omega_{X/S})_x = 0$ .  But I do not know that $(\mathcal O_{X/S})_x$ is finitely generated as an $\mathcal O_{S,s}$ -module.  It is only finitely generated as an $\mathcal O_{X,x}$ -module. How should I go about proving the result?  And where is the hypothesis that $\mathfrak m_s$ generates $\mathfrak m_x$ coming in?","['algebraic-geometry', 'commutative-algebra']"
3130792,Characterization of extreme points of the ball of $B(H)$,"The following is an exercise in Chapter 5, Section 7 of Conway's Functional Analysis text: If $\mathcal{H}$ is a Hilbert space, show that $T$ is an extreme point of $\text{ball }\mathcal{B}(\mathcal{H})$ if and only if either $T$ or $T^*$ is an isometry. The ( $\Leftarrow$ ) implication is due to both the fact that $\ell^2$ balls are strictly convex and the nature of the adjoint - other stackexchange posts talk about this. I have not been able to find an answer to ( $\Rightarrow$ ) that a beginning student in functional analysis could also find. Here is what I have so far: If $\ker T$ and $\ker T^*$ both happen to be non-trivial - say $0\neq x\in\ker T$ and $0\neq y\in \ker T^*$ are of norm one - then we can perturb $T$ by a rank-one operator $S$ sending $x$ to $y$ . Then if we decompose an arbitrary $z=\lambda x+\sqrt{1-\lambda^2}z_0$ for $z_0$ a norm-one element orthogonal to $x$ and $0\le \lambda\le 1$ , we get \begin{align*}\langle(T\pm S)z,(T\pm S)z\rangle&=\langle S\lambda x,S\lambda x\rangle+\langle T\sqrt{1-\lambda^2}z_0,T\sqrt{1-\lambda^2}z_0\rangle\pm2\langle x,T^*Sx\rangle\\
&\le\lambda^2+1-\lambda^2\pm2\langle x,T^*y\rangle=1.\end{align*} Hence $T=\frac{1}{2}(T+S)+\frac{1}{2}(T-S)$ , so $T$ is not an extreme point. This argument is restricted to $\mathbb{R}$ -Hilbert spaces, and even this does not seem to generalize very nicely to other such spaces: for instance, in $M_2(\mathbb{R})$ if we define $T:=\begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix}$ , then $\tilde{T}:=\frac{T}{\lVert T\rVert}$ is not an isometry and neither is its adjoint, $\tilde{T}\begin{bmatrix} 1 \\ 0\end{bmatrix}$ and $\tilde{T}^*\begin{bmatrix} 0 \\ 1\end{bmatrix}$ are both of norm less than one, but any perturbation of the operator by an operator of the form $\begin{bmatrix} 0 & 0 \\ \xi & 0\end{bmatrix}$ for $\xi\ge 0$ raises the operator norm of the matrix. Murphy mentions this fact on page 137 and references Takesaki, Theorem I.10.2, for the proof. The theorem in Takesaki is done for a general C*-algebra and the proof utilizes the theory of operator algebras. The proof is a bit lengthy and (in my opinion) too clever for a student of Conway. Kadison-Ringrose also has a similar statement to that of Takesaki near the beginning of their second volume (Theorem 7.3.1), and this proof uses partial isometries and the Gelfand transform. I am not contesting the more general proofs given above in (2) but am merely asking whether there is a simpler way to answer this problem in the specific case of our C*-algebra being $\mathcal{B}(\mathcal{H})$ - perhaps one accessible to a reader of Conway.",['functional-analysis']
3130797,Show that matrix of $T$ has at least dim range $T$ nonzero entries.,"Suppose $V$ and $W$ are finite-dimensional and $T \in \mathcal{L}(V, W)$ . Show that with respect to each choice of bases of $V$ and $W$ , the matrix of $T$ has at least dim range $T$ nonzero entries. I have no idea on how to solve this.","['matrices', 'linear-algebra', 'linear-transformations']"
3130813,Rotating a vector perpendicular to another?,"Suppose I have two unit vectors $(x_1,x_2,x_3)$ and $(y_1,y_2,y_3)$ in 3D space that are perpendicular to each other. (Their dot product is zero.) I want to rotate the $x$ vector by $\theta$ degrees such that it remains perpendicular to $y$ .  That is, I want to rotate $x$ using $y$ as the axis. What would be the formula to do this and how is it derived?","['vector-spaces', 'geometry']"
3130877,"Do parallel, angle, triangle, area etc still apply in Mobius band?","Normal geometry concepts, such as parallel, angle, area, triangle, do they still apply in Mobius band? If not, in which case will they fail to do so? For example, what would three lines on a Mobius band form? A triangle if not parallel? or it might be totally something else?","['geometry', 'mobius-band']"
3130906,"$ \lim_{(x,y)\rightarrow (0,0)} \frac{x^{5}y^{3}}{x^{6}+y^{4}}. $ Does it exist or not?","I have this limit: $$ \lim_{(x,y)\rightarrow (0,0)} \frac{x^{5}y^{3}}{x^{6}+y^{4}}. $$ I think that this limit does not exist (and wolfram|alpha agrees with me). But I can't find a way to prove it. I chose some paths and the limit was always equal to zero. I tried polar coordinates but I couldn't get an answer. Any tips? (I also have this one: $$ \lim_{(x,y)\rightarrow (0,0)} x^{y^{2}} $$ Can I get 2 different paths to show that the limit does not exist? $$x=0: \lim_{(x,y)\rightarrow (0,0)} 0^{y^{2}} = \lim_{(x,y)\rightarrow (0,0)} 0 = 0,  $$ $$y=0: \lim_{(x,y)\rightarrow (0,0)} x^{0^{2}} = \lim_{(x,y)\rightarrow (0,0)} 1= 1. $$ Am I right? )",['limits']
3130930,Proof of Hartshorne Corollary III.9.4.,"Corollary III.9.4 in Hartshorne's Algebraic Geometry : Let $f: X \to Y$ be a separated morphism of finite type of noetherian schemes with $Y$ affine, and let $\mathcal{F}$ be a quasi-coherent sheaf on $X$ . For any point $y \in Y$ , let $X_y$ denote the fiber over $y$ , and let $\mathcal{F}_y$ be the induced sheaf. On the other hand, let $k(y)$ denote the constant sheaf $k(y)$ on the closed subset $\{y \}^-$ of $Y$ . Then for all $i \ge 0$ , there exists a natural isomorphism $$H^i(X_y, \mathcal{F}_y) \cong H^i(X, \mathcal{F} \otimes k(y)) $$ $\textbf{Proof}:$ First let $Y' \subseteq Y$ be the reduced induced subscheme structure on $\{y\}^-$ , and let $X'= X \times_Y Y'$ , which is a closed subscheme of $X$ . The next line of the proof I have been struggling to understand for a while now, ""then both sides of our desired isomorphism depend only on the sheaf $\mathcal{F}'= \mathcal{F} \otimes k(y)$ on $X'$ . Thus we can replace $X,Y, \mathcal{F}$ by $X', Y', \mathcal{F'}$ .."" . Where is this sheaf $\mathcal{F}'$ appearing from?? Moreover, it seems when we have a scheme of the form $X \times Y \to Y$ it is ok (see Exercise 12.6) to describe a fiber as $X \times \{y \}$ as opposed to $X \times_Y \operatorname{Spec} k(y)$ . Confusingly, in this corollary he actually choses to replace the entire family $X \to Y$ by $X \times_Y \{y \} \to \{y\}$ .","['algebraic-geometry', 'schemes']"
3130953,An inequality derived from an olympiad one,"This problem is a kind of complement of this . Let $a,b,x,y,z\gt 0$ , prove $$\frac{x+y+z}{a+b}\ge \frac{xy^3}{ax^3+by^3}+\frac{yz^3}{ay^3+bz^3}+\frac{zx^3}{az^3+bx^3}$$ I have verified that this inequality is valid by computer and a lot of particular $(a, b, x, y, z)$ but I could not yet find a non-sophisticated proof.","['contest-math', 'algebra-precalculus', 'examples-counterexamples', 'inequality']"
3130960,a problem on composition of functions,"Let $f \colon A \to A$ be a function such that $f \circ f=f$ .  If $f$ is one-to-one then prove that $f$ is also onto. I know in my head that the func. $f$ is $f(x)=x$ , but I can't develop a proof for the above statement.",['functions']
3130974,"Give a $\delta$-$\varepsilon$ proof that $f(x,y)=\sqrt{x+y-2}$ is continuous at $(2,1)$.","I am new to multivariable limits and I am having a hard time arguing this limit: Give a $\delta$ - $\varepsilon$ proof that $f(x,y)=\sqrt{x+y-2}$ is continuous at $(2,1)$ . I can work out the overarching context of the proof as follows: Let $\epsilon>0$ be given. We need a $\delta$ so that whenever $\|(x,y)-(2,1)\|<\delta$ we have $\|f(x,y)-f(2,1)\|<\varepsilon.$ The difficulty I always have with these problems is that I know I need to rewrite $\|\sqrt{x+y-2}-1\|$ in terms of the given $\delta$ estimate but for some reason when it comes to these limit problems algebra turns into the most difficult thing in the world for me.","['multivariable-calculus', 'limits', 'calculus', 'epsilon-delta']"
3130989,What is the cardinal of the set of Uniform Continuous functions with domain in $\mathbb{R}$?,"I learned in my topology class that $\mathbb{R} \cong \mathfrak{C}(\mathbb{R})$ The latter being the set of continuous functions with domain in $\mathbb{R}$ . Since if a function is uniformly continuous then it is continuous, the set of all uniform function must have cardinal $\leq \mathfrak{c}$ . But continuous functions need not be uniformly continuous; does this show that there is no bijection from continuous functions to uniformly continuous functions? I'm kind of lost with all this cardinality stuff; it's not intuitive at all.",['elementary-set-theory']
3131011,Homeomorphism between unit circle with point removed to real line.,"Full question is: Define a map $\psi:S^1\setminus\{(0,1)\}\to\mathbb{R}$ as for each point $p\in S^1$ take the line through $(0,1)$ and $p$ and define $\psi(p)$ as the intersection of this line and the $x$ -axis. So I believe I want $\psi$ to be the map $(x,y)\to \frac{x}{1-y}$ , I took the line $y=\frac{y-1}{x} +1$ , and found the $x$ value when $y=0$ . So now I need to show that $\psi$ is a homeomorphism. I believe after showing it is bijective, showing it is continuous is simply a matter of showing $S^1\setminus\{(0,1)\}$ is open in $S^1$ . Then using that the pre image of open sets is open, thus continuous. However I'm having some problems showing this function is injective and surjective. Injective:
Suppose $f(x,y)=f(r,s)$ then $\frac{x}{1-y}=\frac{r}{1-s}$ then $x(1-s)-r(1-y)=0$ . Somehow I want to show that either $x\not=r$ or $y\not=s$ but I'm not seeing how to do this. I know that $x^2+y^2=1$ and $r^2+s^2=1$ .","['continuity', 'general-topology', 'real-analysis']"
3131053,"""Continuized"" Taylor Series? $\sin(x)=\sum \frac{(-1)^nx^{2n+1}}{(2n+1)!}=\int_{-1}^\infty \frac{\cos(\pi n) x^{2n+1}}{G(2n+1)}dn$?","~~not trying to reinvent the Laplace transform, but just an exploration into these particular series and integrals~~ Current answers don't fully address the 5 questions, so any new ideas or suggestions would be much appreciated. Thanks for the help! The Taylor Series for $e^x$ is $$\sum \frac{x^n}{n!}$$ Now isn‚Äôt this just a discrete sum of functions? What if I use integrals to make a ""continuous"" version of the Taylor series? Following that motivation, I came up with $$E(x)=\int_{-\infty}^\infty \frac{x^n}{G(n)}dn$$ where $G(n)=\Gamma(n+1)=n!$ . Since $\frac{x^n}{G(n)}$ goes to zero as $n\to -1$ , the integral just becomes $$E(x)=\int_{-1}^\infty \frac{x^n}{G(n)}dn$$ I graphed it on Desmos and it looked like this, with the green dotted line being $E(x)$ : After that, I was like ""wow, nice! I wonder if other functions work too‚Äù. Naturally, I moved on to $\sin(x)$ , which has power series $$\sum \frac{(-1)^nx^{2n+1}}{(2n+1)!}$$ Unfortunately, this power series is more complicated because of the $(-1)^n$ term. The first thought I came up with is that $\cos(\pi n)$ could be the continuous version of that term. So, one such ‚Äúcontinuized‚Äù version of the Taylor series for $\sin x$ would be $$S(x)=\int_{-1}^\infty \frac{\cos(\pi n) x^{2n+1}}{G(2n+1)}dn$$ Of course, that's kind of arbitrary, so I did use two other functions: $$c_1(x)=\cos^6\left(\frac{\pi x}{2}\right)-\sin^6\left(\frac{\pi x}{2}\right)$$ which is more ""triangular"", and $$c_2(x)=2\left(1-\sin^6\left(\frac{\pi x}{2}\right)\right)^6-1$$ which is more ""square"". You can see all three of these functions in orange. I made three integrals with each of the three functions, with the dotted green line with the one with $\cos(\pi x)$ . I had to multiply it by a factor of $2$ to get it right, interestingly enough. In both integrals, I made the lower bound a bit higher to avoid crashing my computer and the higher bound low enough to make little to no difference. (accessible here: https://www.desmos.com/calculator/eesis3ykai , though it may take a while to load) The first one has already been addressed here: The function $f(x) = \int_0^\infty \frac{x^t}{\Gamma(t+1)} \, dt$ , so my only contribution is the nice graph. However, the $\sin x$ and $\cos x$ ones I found far more fascinating. Hence, I have a few questions: (1) does $S(x)$ actually converge to $\sin(x)$ ? (2) can we expect this ""integral-Taylor series"" to work on a lot of other functions? Is there some general result? (3) why does $\cos(\pi x)$ work the best compared to the triangle and square waves? Why did the rectangular wave fail so badly? (4) why does the integral have to be stretched by a factor of two, when the $e^x$ integral didn‚Äôt have to be? (5) is this approximation for these functions useful? Can this method be applied elsewhere? Is there any use to this outside just ""ooh look at this neat graph""? More cool stuff: I did the same thing with $\cos x$ , and I got similar results (the bounds get shifted a bit though): $$C(x)=\int_{-0.5}^\infty \frac{\cos(\pi n) x^{2n}}{G(2n)}dn$$ (which is accessible here: https://www.desmos.com/calculator/ctjqdxuw0h ) which also has the strange multiplicative factor of $2$ , and the peculiar favorability to $(-1)^n \approx \cos(\pi n)$ . So my above 5 questions still stand.","['integration', 'special-functions', 'laplace-transform', 'analysis', 'calculus']"
3131066,Variance of a sub-Gaussian random variable,"For a zero mean sub-Gaussian R.V. we know that: $$ \mathbb{E}[e^{\lambda X}]\le e^{\frac{\lambda^2\sigma^2}{2}},\qquad\forall\lambda\in \mathbb{R}$$ From Taylor series expansion and equating the terms of the same power for $\lambda$ it can be easily obtained that: $$\mathbb{E}[X^2]\le \sigma^2$$ Is it possible to prove that $\mathbb{E}[X^2]=\sigma^2$ for the minimum $\sigma^2$ that the inequality $ \mathbb{E}[e^{\lambda X}]\le e^{\frac{\lambda^2\sigma^2}{2}}$ holds?","['moment-generating-functions', 'inequality', 'probability', 'random-variables']"
3131075,area under parametric curve,"I have difficulty on how to eliminate parameter especially the equation involved trigonometry equation. The question is asking for the area bounded by the curve , the 2 axes and the line $y=1$ . $x=4 \sin t$ $y=\cot t$ where $t$ is in the range $(0, \pi)$ .
I have tried to use the trigonometric formula $1+ \cot^2 t =\csc^2 t$ but I got the wrong answer.The answer given is 4ln[(square root 2)+1].","['integration', 'multivariable-calculus', 'area', 'parametric']"
3131087,Showing $\hat{\theta} = \frac{x_1 + 2x_2 + x_3}{4}$ is a not a sufficient estimator for the mean of a Bernoulli-distributed population,"Suppose $x_1$ , $x_2$ , $x_3$ are independant observations from a Bernoulli-distributed population with parameter $\theta$ . I want to show that $$\hat{\theta} = \frac{x_1 + 2x_2 + x_3}{4}$$ is not a sufficient estimator for $\theta$ . I have derived earlier that $$L(\theta) = (1 - \theta)^3 \left(\frac{\theta}{1 - \theta}\right)^{x_1 + x_2 + x_3}$$ which allowed me to show that $\overline{x}$ was a sufficient estimator for $\theta$ .
I however am not sure about how to proceed to show that $\hat{\theta}$ is not sufficient for $\theta$ (while it does seem natural to me). I have tried using the ""formal"" definition for a statistic to be sufficient but with no concrete results. How would one go about showing $\hat{\theta}$ is sufficient for estimating $\theta$ ?","['statistical-inference', 'statistics', 'probability-distributions']"
3131102,Understanding why the tangent bundle of a unit circle is diffeomorphic to $S^1\times\mathbb R$,"Wikipedia defines a tangent bundle of a manifold $M$ as $$\coprod_{x\in M}T_{x}M = \{(x,y)| x\in M, y\in T_x M\} .$$ The wiki link describes the tangent bundle of $S_1$ as being diffeomorphic to $S_1\times R$ by saying the way to make the tangent spaces at all points of the circle (which are in the plane of the circle) disjoint smoothly is by aligning them perpendicular to the plane of the circle which I don't understand completely. The tangent space at a point on the circle, is just the tangent line at that point and the line is not tangent to any other point on the circle, so aren't the tangent spaces disjoint to begin with? Thanks and appreciate a hint.","['manifolds', 'smooth-manifolds', 'differential-geometry']"
3131112,$n$th derivative of $\sin^3x$,"How do I find the general formula for the $n$ th derivative of $\sin^3x$ ? I tried to differentiate $4$ times, but I couldn't find a pattern for $n$ in general. I used Leibniz's formula for the $n$ th derivative, but I couldn't simplify that to a general formula. I would like some hints or a better way to find the $n$ th derivative for functions.","['derivatives', 'real-analysis']"
3131119,Find $f$ such that $f(a-b)+f(c-d)=f(a)+f(b+c)+f(d)$,"Denote the set of non-negative real numbers by $\mathbb R^+_0$ . Find all functions $f:\mathbb R \rightarrow \mathbb R_0^+$ s.t. $\forall a,b,c,d\in\mathbb R$ satisfying $ab+bc+cd=0$ we have $$f(a-b)+f(c-d)=f(a)+f(b+c)+f(d)$$ My attempt: set $b=d=0$ and get $f(0)=0$ . Now set $a=b=c=0$ to get $f(-d)=f(d)$ . Now we pretty much reduced this fe's domain to $\mathbb R^+_0\rightarrow\mathbb R^+_0$ . That's where I got stuck. I tried a few things but none of them gave me any progress. Any help/hints appreciated.","['functional-equations', 'algebra-precalculus']"
3131137,Is there any other number that has similar properties as $21$?,"It's my observation. Let $$n=p_1√óp_2√óp_3√ó\dots√óp_r$$ where $p_i$ are prime factors and $f$ and $g$ are the functions $$f(n)=1+2+\dots+n$$ And $$g(n)=p_1+p_2+\dots+p_r$$ If we put $n=21$ then $$g(f(21))=g(231)=21.$$ I checked it upto $n=10000$ , I did not find another number with this property $g(f(n))=n$ . Can we prove that other such numbers do not exist?","['number-theory', 'prime-factorization', 'elementary-number-theory']"
3131173,"Calculate $\sum_{|S|=k}(n-|\cup S|)^m$ where $S$ is a subset of $X=\{\{a_1,a_2\},\{a_2,a_3\},\cdots,\{a_{n-1},a_n\},\{a_n,a_1\}\}$","Let $x_i=\{a_i,a_{i+1}\}\ (1 \leq i \leq n-1)$ , $x_n=\{a_n,a_1\}$ and $X=\{x_1, \cdots, x_n\}$ . Given $n,m$ and $k$ , I'd like to ask how to calculate $\sum_{|S|=k}(n-|\cup S|)^m$ where $S$ is a subset of $X$ ? Note that $a_{1}\cdots a_n$ are different from each other. Let $f(j,l)$ be the number of subset $S$ of $X$ where $|S|=j$ and $|\cup S|=l$ . The above can be solved if all values of $f$ are known. I guess $f$ can be calculated recursively, and the inclusion-exclusion principle may help.","['elementary-set-theory', 'inclusion-exclusion', 'combinatorics', 'generating-functions']"
3131179,"Let $f: U\subseteq \mathbb{R}^n \to \mathbb{R}^m$ be $C^1$ s.t. $n \leq m$, $U$ open, $\mathrm{rank}{D_pf}=n$. Prove $f$ is locally injective at $p$.","I was trying to solve a problem in differential geometry that I realized the following statement is the core of my argument Let $f: U\subseteq \mathbb{R}^n \to \mathbb{R}^m$ be a $C^1$ function on an open set $U$ where $n \leqslant m$ such that $\mathrm{rank}{Df}=n$ at some $p\in U$ . Show that $f$ is injective in a neighborhood of $p$ . After thinking about it, I think that it can be proven using the constant rank theorem. Firstly, since $f$ is $C^1$ , we have $\mathrm{rank}Df\geq n$ in a neighborhood of $p$ . Since $n$ is the maximum possible rank, we have $\mathrm{rank}Df = n$ near $p$ . So, the constant rank theorem applies. Now the constant rank theorem says that I can find two open sets $V \subseteq U$ and $W\subseteq \mathbb{R}^m$ such that $f(V) \subseteq W$ and two diffeomorphisms $\psi:\mathbb{R}^n \to V$ and $\varphi:\mathbb{R}^m \to W$ such that $\varphi^{-1}\circ f\circ \psi: \mathbb{R}^n \to \mathbb{R}^m$ has the canonical form $(x_1,\cdots,x_n) \mapsto (x_1,\cdots,x_n,0,\cdots,0)$ . Since $\varphi^{-1}\circ f\circ \psi$ is clearly injective, and $\varphi$ and $\psi$ are diffeomorphisms, $f = \varphi \circ \big(\varphi^{-1}\circ f\circ \psi \big) \circ \psi^{-1}$ is injective on $V$ . Assuming my proof is correct (well, is it?) I still think it's overkill. Is there a proof that is more elementary? Ideally, a proof without using the Inverse Function Theorem. Or if it uses the Inverse Function Theorem, it should not be longer than this one since the constant rank theorem can be proven using the Inverse Function Theorem and hence, it's obvious that a longer proof exists.","['analysis', 'real-analysis', 'multivariable-calculus', 'manifolds', 'differential-geometry']"
3131186,"Why is ""points exist"" not an axiom in geometry?","I am not sure why ""points exist"" is not an axiom in geometry, given that the other axioms are likewise primitive and seemingly as obvious.","['geometry', 'axioms']"
3131189,Subgradient of $\|AX\|_1$,"If $f(x) = \|AX\|_1$ , where $\|.\|$ denotes the entrywise $\ell_1$ -norm, what is the subgradient of $f(x)$ ? Is there an expression similar to $A'A \ \partial \|X\|_1$ ? For example, $A = [1 \ 1;  1 \ 0; 0 \ 1]$ .","['convex-optimization', 'normed-spaces', 'subgradient', 'derivatives', 'convex-analysis']"
3131221,"What is meant by a filtration ""contains the information"" until time $t$?","I have problems understanding the concept of a filtration in stochastic calculus. I understand that for example the natural filtration $F_t$ contains only outcomes up to time $t$ , but since it is a sigma algebra it contains all possible events. For instance for $X_s$ , $s<t$ , it should contain all possible outcomes of $X_s$ , and even all subsets of possible outcomes of $X_s$ , right? How can it then contain information about which events occurred and which did not, when it contains all possible events up to time $t$ ? What really is meant when people write that ""the filtration contains the information of outcomes up to time $t$ "" and uses $|F_t$ to indicate conditional expectation values? Or have I completely misunderstood what is meant when one says that a filtration is a sigma algebra?","['stochastic-processes', 'measure-theory', 'probability-theory', 'filtrations']"
3131243,How can I prove that the Hilbert transform on the 1-torus doesn't map $L^1(\mathbb{T})$ into itself?,"Let $\mathbb{T}$ be the 1-torus. Then, it is well defined the Hilbert transform: $$\mathcal{H}:L^1(\mathbb{T})\to L^0(\mathbb{T}), \vartheta\mapsto\int_{-\pi}^\pi f(\vartheta-t)\cot\left(\frac{t}{2}\right) \frac{\operatorname{d}t}{2\pi}.$$ I know that $\mathcal{H}$ is weak-(1,1), i.e. that there exists a constant $C>0$ such that: $$\forall f\in L^1(\mathbb{T}), \forall \lambda>0, \left|\left\{\vartheta\in\mathbb{T} : |\mathcal{H}(f)(\vartheta)|>\lambda \right\}\right|\le C\frac{\|f\|_1}{\lambda}.$$ In my lecture notes it is stated without proof that $\mathcal H$ doesn't map $L^1(\mathbb{T})$ into itself. I'm thinking about how to prove this claim. I can prove that doesn't exist an operator $\mathcal{G}: L^1(\mathbb{T})\to L^1(\mathbb{T})$ such that $$\forall f\in L^1(\mathbb{T}), \forall n\in\mathbb{Z}, \mathcal{F}(\mathcal{G}(f))(n) = -i \operatorname{sgn}(n)\mathcal{F}(f)(n),$$ where $\mathcal{F}$ is the Fourier transform. Also, I know that $$\forall p\in(1,+\infty), \forall f\in L^p(\mathbb{T}), \left(\mathcal{H(f)}\in L^p(\mathbb{T})\right)\land \left(\mathcal{F}(\mathcal{H}(f))(n) = -i \operatorname{sgn}(n)\mathcal{F}(f)(n)\right).$$ Now, if I only could prove that "" $\mathcal{H}$ maps $L^1(\mathbb{T})$ into itself"" would imply "" $\mathcal{H}: L^1(\mathbb{T})\to L^1(\mathbb{T})$ with continuity"", then the claim that $\mathcal H$ doesn't map $L^1(\mathbb{T})$ in itself  would follows from a density and continuity argument. However I can't see how to prove that the only fact that $\mathcal{H}$ maps $L^1(\mathbb{T})$ into itselt would imply its continuity (I thought to use something like the closed graph theorem, but I can't see how it could apply here). Any help?","['weak-lp-spaces', 'fourier-analysis', 'harmonic-analysis', 'functional-analysis', 'integral-transforms']"
3131250,When to use $y = \frac{1 + x}{1 - x}$ when evaluating definite integrals,"When evaluating definite and indefinite integrals, there are times in which the integrand presents itself to be solvable using a definite method: be it a transformation, substitution, series, etc. For instance, the general method for integrals involving rational functions of trigonometric functions is to employ the Weirerstrauss Substitution. Now, on this site I mainly primarily focus on integrals and I've observed that the substitution of $y = \frac{1 + x}{1 - x}$ is used when the bounds of a definite integral are $0,1$ . Is this part of a generalised method? and if so, does it have name? And if not, is it possible to get some examples of integrals where this method is favourable.","['integration', 'soft-question', 'definite-integrals']"
3131300,Convergence of non-finite measures,"Consider the sequence $(\mu_n)_{n\in \mathbb{N}}$ of Borel measures on $\mathbb{R}$ given by: $$
\mu_n = \sum_{k\in \mathbb{Z}} \Delta x_{n,k} \, \delta_{x_{n,k}}
$$ where $\delta_x$ denotes the Dirac measure at $x$ and $x_{n,k}$ is a given sequence. I want to say that the sequence of measures $(\mu_n)$ converges to the Lebesgue measure $\lambda$ on $\mathbb{R}$ . The problem is that I am not sure which is the most natural / strongest notion of convergence here. Now some details.  For simplicity let's just take $x_{n,k} = \frac{k}{n}$ . I have denoted $$\Delta x_{n,k} = \frac{x_{n,k+1} - x_{n,k-1}}{2}$$ so here $\Delta x_{n,k} = \frac{1}{n}$ . (More generally, I require that $(x_{n,k})_{k\in \mathbb{Z}}$ is an increasing sequence such that $\lim_{k \to \pm\infty} x_{n,k} = \pm \infty$ , and that $\sup_k \Delta x_{n,k}$ converges to $0$ when $n \to \infty$ .) The sequence of measures that I am actually interested in is a higher dimensional analogue of this, but I think this 1D example captures the idea. I figured out a neat proof that $$
\lim_{n \to \infty}\mu_n(A) \to \lambda(A) \qquad (1)
$$ for any continuity set $A$ (that's a Borel set whose boundary has zero measure: $\lambda(\delta A) = 0$ ). EDIT : I realize thanks to the comment of Sangchul Lee that there was a problem in my proof: I also need $A$ to be bounded (or just $\delta A$ bounded would be sufficient). If we were talking about probability measures, I believe the question is settled: we have ""weak convergence"" (some would say ""weak-* convergence"") of $\mu_n$ to $\lambda$ , for any reasonable definition of ""weak convergence"". (See for example Convergence of measures in Wikipedia .) But here the measures are not finite. Surely I can take ""weak"" (more like weak-*) convergence relative to the space of compactly supported continuous functions $\mathcal{C}_c(\mathbb{R})$ . But maybe there's a stronger and more natural form of ""weak"" convergence to consider. Ideally, it should be equivalent to property $(1)$ with continuity sets. For example, I could try instead $\mathcal{C}^+(\mathbb{R})$ : nonnegative continuous functions (allowing infinite integrals), or $\mathcal{C}_b^+(\mathbb{R})$ (bounded) to mimic ""narrow convergence"", or $\mathcal{C}_0^+(\mathbb{R})$ (continuous functions converging to $0$ at infinity).","['integration', 'measure-theory', 'convergence-divergence', 'duality-theorems', 'probability-theory']"
3131326,Is the function $\displaystyle\liminf_{t\to t_0}f_t(x)$ Lebesgue measurable?,"Let $\{f_t:[0,1]\to \mathbb R \  |t\in I\}$ , where $I\subset \mathbb R$ is an interval, be a class of Lebesgue measurable functions. Then is it true that the function $\displaystyle\liminf_{t\to t_0}f_t(x):[0,1]\to \mathbb R$ also Lebesgue measurable, where $t_0\in I$ ? What I know is that if $f_1,f_2,...,f_n,...$ are measurable functions then $\displaystyle \liminf_{n\to \infty}f_n$ is also measurable. For the above question, I was thinking that Let $\{x_n:n\in \mathbb N\}\subset I$ be a sequence from $I$ such that $\displaystyle \lim_{n\to \infty}x_n=t_0$ . Then $\displaystyle\liminf_{t\to t_0}f_t(x)=\liminf_{n\to \infty}f_{x_n}(x)$ from the definition. And the function $\displaystyle\liminf_{n\to \infty}f_{x_n}(x)$ is measurable. So the function $\displaystyle\liminf_{t\to t_0}f_t(x)$ is measurable. I don't know it is true or not. It would be very helpful if anyone checks the above. Thank you.","['measure-theory', 'lebesgue-measure', 'measurable-functions']"
3131332,"Why are exterior products so much ""wigglier"" than symmetric and tensor products?","Apologies in advance that this is a somewhat soft question. Let $k$ be an infinite field. Fix a dimension $d$ and let $v_1,\dots,v_r$ , $w_1,\dots,w_r$ be two tuples of linearly independent vectors in $k^d$ . (Thus, assume $r\leq d$ .) We have that the exterior products $v_1\wedge\dots\wedge v_r$ and $w_1\wedge\dots\wedge w_r$ are equal if and only if the $w_i$ 's can be expressed in terms of the $v_i$ 's via a matrix in the special linear group $SL(r,k)$ . It follows that the map to the $r$ th exterior power $$\wedge^r:(k^d)^{\times r} \rightarrow \Lambda^rk^d$$ that carries $$(v_1,\dots,v_r)\mapsto v_1\wedge\dots\wedge v_r$$ has fibers generically the size of $SL(r,k)$ , which has dimension $r^2-1$ as an algebraic variety over $k$ . Now consider the same question with the symmetric and tensor powers in place of the exterior power. For the symmetric power, it seems to me we have $v_1\dots v_r = w_1\dots w_r$ (if and) only if there are elements $\alpha_1,\dots,\alpha_r\in k$ and a permutation $\sigma\in S_r$ (the symmetric group on $r$ elements) satisfying $\prod \alpha_i=1$ , and $w_i=\alpha_iv_{\sigma(i)}$ for all $i$ . I'll include an argument in a postscript, but if this is right, this means that the fibers of the map to the symmetric power $$ \Pi^r: (k^d)^{\times r} \rightarrow S^rk^d$$ given by $$ (v_1,\dots,v_r) \mapsto v_1\dots v_r $$ has fibers generically the size of the group $L\times S_r$ , where $L$ is the set of matrices $\operatorname{diag}(\alpha_1,\dots,\alpha_r)$ with the $\alpha_i$ 's as above, i.e. the diagonal matrices in $SL(r,k)$ . This group has dimension $r-1$ as a $k$ -variety. Aside: I don't know a standard name for this map to the symmetric power. I am calling it $\Pi^r$ because if one interprets the symmetric algebra over $k^d$ as a polynomial ring, then it is just taking a product. And then the more restrictive condition $v_1\otimes\dots\otimes v_r = w_1\otimes \dots\otimes w_r$ should only be met when $w_i = \alpha_iv_i$ for some $\alpha_1,\dots,\alpha_r\in k$ with $\prod\alpha_i=1$ , as above, so the map $$\otimes^r : (k^d)^{\times r} \rightarrow (k^d)^{\otimes r}$$ given by $$(v_1,\dots,v_r)\mapsto v_1\otimes\dots\otimes v_r$$ has fibers generically bijecting just with $L = \{\operatorname{diag}(\alpha_1,\dots,\alpha_r)\}$ , again, dimension $r-1$ . It is really bothering me that the fibers of $\Pi^r$ and $\otimes^r$ are the same dimension, and this dimension is so much smaller than the dimension of the fibers of $\wedge^r$ . Can you offer any insight about why this is happening? (Or is my reasoning incorrect and it's not actually happening?) Here is a little more about why it is bothering me. I normally think of the exterior algebra as quite a bit ""smaller"" than the symmetric algebra, but both are ""massively smaller"" than the tensor algebra. I mean this informally, but some form of it is true precisely: the dimension of $\Lambda^rk^d$ is $\binom{d}{r}$ which $\to 0$ as $r\to \infty$ ; that of $S^rk^d$ is $\binom{r+d-1}{d-1}$ , which is polynomial in $r$ of degree $d-1$ ; and that of $(k^d)^{\otimes r}$ is $d^r$ , which is obviously exponential in $r$ . In view of this, it's intuitive to me that the fibers of the map into the exterior power are the biggest (since smaller target perhaps forces bigger fibers), but it is confusing that the fibers of of the maps into the symmetric and tensor powers are the same size (well, at least same dimension), given how massively different in size the targets are. As I write this, it occurs to me that, well, the source $(k^d)^{\times r}$ only has dimension $dr$ , so $S^rk^d$ and $(k^d)^{\otimes r}$ both contain ""plenty of room"" to accommodate the source, while for some values of $r,d$ , the dimension of $\Lambda^rk^d$ is much smaller than that of the source, forcing the fibers to embiggen. But this doesn't really feel explanatory to me, especially in view of the fact that, per the calculation above, none of the generic fiber sizes ever depend on $d$ (except inasmuch as the assumption $r\leq d$ is satisfied). Can you offer any insight into what's going on? Postscript: Here is my argument that $v_1\dots v_r = w_1\dots w_r$ (if and) only if there are elements $\alpha_1,\dots,\alpha_r\in k$ and a permutation $\sigma\in S_r$ (the symmetric group on $r$ elements) satisfying $\prod \alpha_i=1$ , and $w_i=\alpha_iv_{\sigma(i)}$ for all $i$ . Extend $v_1,\dots,v_r$ to a basis of $k^d$ . Interpret $S^\star k^d$ as the polynomial algebra over $k$ in $d$ indeterminates, but choose the indeterminates to be the basis just constructed; in particular, $v_1,\dots,v_r$ will be indeterminates, while $w_1,\dots,w_r$ will be linear forms. Then $v_1\dots v_r$ is a monomial and $w_1\dots w_r$ is a homogeneous polynomial of degree $r$ . Suppose we have $v_1\dots v_r = w_1\dots w_r$ . A product of linear forms is not a monomial unless each linear form is a monomial. (One way to see this is to fix a monomial order; then the leading term and ""trailing term"" in an expanded product will be distinct unless each factor is a monomial.) Thus each $w_i$ is a coefficient times a single indeterminate. Furthermore, these indeterminates must coincide with $v_1,\dots,v_r$ in some order, or else $w_1\dots w_r$ and $v_1\dots v_r$ will be linearly independent. This establishes that there is a permutation $\sigma\in S_r$ and some numbers $\alpha_i\in k$ such that $w_i = \alpha_i v_{\sigma(i)}$ for all $i=1,\dots,r$ . Thus the assumed equality $\prod v_i = \prod w_i$ becomes $\prod v_i = \prod \alpha_i \prod v_i$ , and $\prod \alpha_i = 1$ follows.","['algebraic-geometry', 'abstract-algebra', 'multilinear-algebra']"
3131339,Is every normal subgroup the kernel of some self-homomorphism? [duplicate],"This question already has answers here : Is every normal subgroup the kernel of some endomorphism? (3 answers) Closed 5 years ago . Let $G$ be a group. If there is a homomorphism $f:G\to G$ (special case of the codomain being arbitrary group), then the kernel $f^{-1}(id)$ is a normal subgroup of $G$ . But now the other way around: Start out with the existence of a normal subgroup $H$ of $G$ . Is there necessarily a homomorphism $f:G\to G$ such that the kernel of $f$ is $H$ ?","['group-homomorphism', 'group-theory', 'normal-subgroups']"
3131375,Concluding that $\mathbb{R}P^2$ cannot be embedded in $S^3$,"I want to deduce that the projective plane $\mathbb{R}P^2$ cannot be embedded in $S^3$ using homology. My idea is to compute $H_*(S^3-h(\mathbb{R}P^2))$ for some arbitrary embedding $h:\mathbb{R}P^2\to S^3$ and obtain a contradiction. Previously I've computed $H_*(S^3-h(M))$ , where $M$ is the M√∂bius strip, which is $\mathbb{Z}$ for $*=0,1$ and $0$ otherwise, and I also have that the homomorphism $i_*:H_1(S^3-h(M))\to H_1(S^3-h(\partial M))$ induced by inclusion is multiplication by $2$ My work Now, I know that $\mathbb{R}P^2=M\cup int D^2$ , so I can write $S^3-M=(S^3-\mathbb{R}P^2)\cup intD^2$ . I have $(S^3-\mathbb{R}P^2)\cap intD^2=\emptyset$ . This set theoretic relation is preserved by $h$ , so the Mayer-Vietoris sequence tells me immediately that $H_i(S^3-h(\mathbb{R}P^2))=0$ for $i\geq 2$ . For $i=1$ it is also easy since we have $0\to H_1(S^3-h(\mathbb{R}P^2))\oplus H_1(h(intD^2))\to H_1(S^3-h(M))\to 0$ Since $h(int D^2)$ is contractible, we get $H_1(S^3-h(\mathbb{R}P^2))\cong H_1(S^3-h(M))=\mathbb{Z}$ . Similarly, for $i=0$ $0\to H_0(S^3-h(\mathbb{R}P^2))\oplus H_0(h(intD^2))\to H_0(S^3-h(M))\to 0$ Since both $h(intD^2)$ and $S^3-h(M)$ are path connected, this forces $H_0(S^3-h(\mathbb{R}P^2))=0$ . The contradiction now is that this means that $S^3-\mathbb{R}P^2$ has no path-connected components so it must be empty. This implies $S^3=h(\mathbb{R}P^2)$ , which is not possible since $S^3$ is not homeomorphic to $\mathbb{R}P^2$ . Question Is everything okay? I'm unsure whether there is a step where I missed something, especially the contradiction part. I didn't use $i_*$ , so if there is an alternative proof using it I'll appreciate it.","['general-topology', 'proof-verification', 'homology-cohomology', 'algebraic-topology']"
3131408,Saddle point or not?,"Consider the function $f(x,y)=2xy-x^3-y^2$ . One of the stationary points is $(0,0)$ . At this point, $f_{xx}f_{yy}-f_{xy}f_{yx}<0$ . According to me, this indicates that (0,0) is a saddle point. However, the text I am referring to calls this ""neither an extremum nor a saddle point"". Am I missing something? Edit
The plot (from GeoGebra) looks like this:",['multivariable-calculus']
3131517,Minimum of $x_1^2+x_2^4$,"I am asked to find a minimum of $f(x_1, x_2) = x_1^2+x_2^4$ applying the optimality conditions. I am stuck. What I have found is not conclusive. I show what I have done: Testing the First-Order Necessary Conditions I computed the gradient: $\left(\frac{\partial f}{\partial x_1}=2x_1 , \frac{\partial f}{\partial x_2}=4x_2^3\right)$ Now when doing: $\left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}\right)=0$ I obtain the stationary point $x^*=[0, 0]$ . Now, Testing the Second-Order Necessary Conditions Obtaing the Hessian matrix of the form: $[[2,0],[0, 12x_2^2]]$ . And the last eigenvalue is not constant? So now, I do not know how to proceed. By definition, it has to be positive semi-definite in order to possibly be an optimiser (or positive definite, then minimum for sure), but here... I do not know what to do. Thank you","['maxima-minima', 'multivariable-calculus', 'calculus', 'optimization', 'hessian-matrix']"
3131554,Different(?) measures on sphere,"Let us consider the real sphere $S^{2n-1}:=\{x=(x_1,\ldots,x_{2n})\in\Bbb R^{2n}\mid\lVert x\rVert=1\}$ and the complex sphere $S_\Bbb C^{n-1}:=\{z=(z_1,\ldots,z_n)\in\Bbb C^n\mid\lVert z\rVert=1\}$ . Then we have an isomorphism \begin{align*}
\Phi:S_\Bbb C^{n-1}\rightarrow S^{2n-1},\quad z\mapsto x
\end{align*} where $z_1=x_1+ix_{n+1},z_2=x_2+ix_{n+2},\ldots,z_n=x_n+ix_{2n}$ . Now consider the inner product \begin{align*}
\langle f,g\rangle_{L^2(S_\Bbb C^{n-1})}:=\int_{S_\Bbb C^{n-1}}f(z)\overline{g(z)}dz
\end{align*} of functions $f,g$ on $S_\Bbb C^{n-1}$ where $dz$ is the $U(n)$ -invariant measure on $S_\Bbb C^{n-1}$ and let \begin{align*}
\langle f,g\rangle_{L^2(S^{2n-1})}:=\int_{S^{2n-1}}f(x)\overline{g(x)}dx
\end{align*} for two functions $f,g$ on $S^{2n-1}$ where $dx$ is the $O(2n)$ -invariant measure on $S^{2n-1}$ . My question is: Is it true that, after normalizing the measures, it holds for functions $f,g$ on $S^{2n-1}$ that \begin{align*}
\langle f,g\rangle_{L^2(S^{2n-1})}=\langle f\circ\Phi, g\circ\Phi\rangle_{L^2(S_\Bbb C^{n-1})}?
\end{align*} If not, is there any correlation like $\langle f,g\rangle_{L^2(S^{2n-1})}=0\Rightarrow \langle f\circ\Phi, g\circ\Phi\rangle_{L^2(S_\Bbb C^{n-1})}=0$ ? Thanks for any help in advance.","['integration', 'measure-theory', 'haar-measure']"
