question_id,title,body,tags
1069520,"What's so special about primes $x^2+27y^2 = 31,43, 109, 157,\dots$ for cubics?","While trying to find a closed-form solution for particular cubics as sums of cosines (related to this question ), I came across this family with all roots real. Given a prime $p=6m+1$. Define,
$$F(x) = x^3+x^2-2mx+N =
\Big(x-2\sum_{k=1}^{m}\cos a\Big) \Big(x-2\sum_{k=1}^{m}\cos b\Big) \Big(x-2\sum_{k=1}^{m}\cos c\Big)=0$$
where,
$$a=2^k\times\beta,\;\;b=2^k\times3\beta,\;\;c=2^k\times m\beta,$$ and $\beta = \displaystyle\frac{2\pi}{p}$. I noticed that for certain primes, then $N$ is an integer . The complete list for small $p$, $$\begin{array}{|c|c|}
\hline
p&N\\
\hline
31&  -8\\
43& 8\\
109&  -4\\
157&  64\\
223&  -256\\
229&  -212\\
277&  236\\
283&  304\\
\hline
\end{array}$$ Questions: What is the complete list of such primes for a low bound, say $p<3000$? (My old version of Mathematica conks out at $p>2000$.) What do these primes $p$ have in common that make them distinct from other primes? (Other than that their $N$ is an integer.) The coefficients of the cubic $F(x)=0$ are simple polynomials in $m$, except the constant term. Can $N$ be expressed as a polynomial in $m$? P.S. I've checked the OEIS and it's not there, but the list I have for $p<2000$ suggests that a necessary (but not sufficient) condition is that $$p = x^2+27y^2,\quad\text{and}\quad 2^{2m} = 1\;\text{mod}\;p$$ ( A014752 ) and ( A016108 ), though it would be great if someone can prove (or disprove) that if $N$ is an integer, then these must hold.","['prime-numbers', 'trigonometry', 'cubics', 'roots-of-unity']"
1069572,How are some infinities larger than other infinities,"I heard an expressions, some infinities are larger than others recently, and they stated that it was proved to be so. I haven't been able to find this proof, and I'm not as much of a math enthusiast as I was earlier in my life, but I don't believe this statement to be true. I also do not know how to format the math symbols here, so forgive me if I'm a bit vague. Given a set of numbers S1 = [0, 1], we can say that there are an infinite amount of numbers in that set. If the previous statement, some infinities are larger than others is true then we can also say some infinities are smaller than others and also some infinities are equal in size to others . With this in mind, we could say that if S2 = [1, 2], then there are just as many numbers as there are in S1, as there are in S2, because the range is the same. If both of the previous statements are true, than we could define S3 = S1 + S2, which would yield all numbers in the set [0, 2], and we could say that S3 is larger than both S1 and S2, since it contains all numbers in S1 and S2. With this in mind, we could also say that the set [0, 0.1] has 1/10 as many numbers (but still infinite) as S1. This is where I lose understanding. What this is saying is that we can have infinity == infinity + infinity == infinity / 10. But because the law of identity states that $A = A$, then how can some infinities are larger than others be true? My understanding is that infinity will always equal infinity, and there are just as many numbers between $[0, 0.1]$ and $[0, 1000]$. If infinity does not equal infinity, then it violates the law of identity. By saying some infinities are larger than others it seems to me like we're saying infinity is the set of all numbers, and there exists a set whose size is larger than infinity . Am I wrong in this understanding?","['cardinals', 'elementary-set-theory', 'infinity']"
1069594,Find Jordan form of a $3\times 3$ matrix,"$$\left( \begin{array}{ccc} 0 & 1 & 2 \\ -5 &-3 & -7 \\  1 & 0 & 0 \end{array} \right) $$ I figured out the eigenvalues are all -1 from the characteristic polynomial, but I'm not sure how to find the 1's on the subdiagonal. I know they're 1, but I'm not sure how that's determined. I also know that the eigenspace is 0 on the matrix where we subtract -1 on the diagonal. Thank you.","['jordan-normal-form', 'linear-algebra', 'eigenvalues-eigenvectors']"
1069606,"Show that this set, defined similarly to the Cantor set, also has measure 0","The standard, middle-thirds Cantor set can be thought of as the set of all numbers on the interval $[0, 1]$ whose ternary expansions contain no 1s, that is, numbers of the form
  $$\sum_{n=1}^{\infty} a_n 3^{-n} \text{ where } (a_n) \in \{0, 2\}^\mathbb{N}$$
It is well known that this Cantor set has Lebesgue measure $0$. I am trying to show that a very similar-looking set, the set of points of the form 
  $$\sum_{n=1}^{\infty} a_n e^{-n} \text{ where } (a_n) \in \{-1, 1\}^\mathbb{N}$$
also has Lebesgue measure $0$. I initially tried to take a bijection between these sets and show this bijection is a homeomorphism, but then realized homeomorphisms don't necessarily preserve measure. Is there any way to use properties of the Cantor set to show that the latter set has measure $0$?","['general-topology', 'measure-theory']"
1069615,Existence of two unrelated pairs in a constrained relation,"Given two sets $S, T$ and a relation defined by a set of pairs $R \subset S \times T$, such that:
$$ 
\exists \, s_1, s_2  \in S : s_1 \neq s_2 \\ 
\exists \, t_1, t_2  \in T : t_1 \neq t_2 \\ 
\forall s \in S \, \exists \, t \in T : (s,t) \in R \\
\forall t \in T \, \exists \, s \in S : (s,t) \not \in R 
$$
Show that 
$$
\exists \, s, s' \in S : \exists\, t, t' \in T : \left[ (s,t) \in R \right]
\wedge \left[ (s', t') \in R \right] \wedge \left[ (s,t') \not \in R \right] 
\wedge \left[ (s',t) \not \in R \right] 
$$
For $S$ and $T$ finite, 
I can prove this by induction on the numbers of elements in $S$ and $T$. This is a statement of an old Putnam problem saying that if at a party every boy dances with at least one girl and no girl dances with every boy, then there exists a pair of couples such that $b$ danced with $g$ and $b'$ danced with $g'$ but $b$ did not dance with $g'$ nor $b'$ with $g$. Equivalent to the proof by induction, I think, is a proof by considering a minimal example of $S$ and $T$ which violates the proposition, and removing one member of $S$ or $T$, and looking at the properties of the remaining set, to show that then purported minimal violating set actually obeys the proposition.  (For example, a step in this proof would be to say that  the reduced sets cannot have a ""qualified"" duo of pairs since that would be qualified in the full sets; so either there is a universal $T$ or a no-relation $S$, and in either case adding back the removed element yields a qualified duo of pairs.) My question concerns proving the proposition when $S$ and $T$ may be infinite, and in particular, may be uncountably infinite.  It looks to me as if the same sort of proof requires at least the axiom of choice, but maybe it can be done with just transfinite induction. I'm shaky as to when a step in my proof implicitly assumes AC, so any help would be appreciated.","['logic', 'induction', 'elementary-set-theory']"
1069616,Eigenvalues of a unimodular matrix,"Let $U$ be a unimodular matrix, i.e. $U \in \mathbb{Z}^{n \times n}$, and $\text{det}(U) = \pm 1$. Do the real (or complex for that matter) eigenvalues of $U$ admit a special structure? Edit: It is not hard to show that the integral eigenvalues must necessarily be $\pm 1$, but this is not the case for all eigenvalues. In all of the examples I can think of the eigenvalues are of the form $a \pm \sqrt{a^2 \pm 1}$, and they always come in conjugated pairs. Is this the case in general?","['linear-algebra', 'number-theory']"
1069642,Finding a limit using change of variable- how come it works? [duplicate],"This question already has answers here : Formal basis for variable substitution in limits (6 answers) Closed 5 years ago . I'm a student just starting calculus in college, and my math skills are pretty stale. So... how come finding limits using change of variable works? For example:
$$\lim_{x \to 1}\frac{x\cos(x-1) -1}{x-1}$$ A way to solve this is to invent ""out of thin air"" $t = x-1$, and then the limit above is equal to:
$$\lim_{t \to 0}\frac{(t + 1)\cos(t) - 1}{t}$$ How come this works? A limit is not an algebraic equation. what about domains of definition? 
We are actually finding a different limit of a different function in a different place, how come they are equal (in general)? just to clarify I'm not asking about this specific example. I'm asking in general, when can you do this to find limits? when not? and why?","['calculus', 'limits']"
1069643,If a nilpotent group $A$ act on a group $G$ then $G$ is solvable.,"Theorem: If a nilpotent group $A$ act on $G$ by automorphism and $C_G(A)=e$ then $G$ is solvable. I hope that the statement of theorem is true. It should belong to Hartley, but when I googled it I could not find the the theorem. If someone provides the whole statement of the theorem with source, I would be thankful.","['reference-request', 'finite-groups', 'group-theory', 'abstract-algebra']"
1069664,Is indefinite integration non-linear?,"Let us consider this small problem:
$$
\int0\;dx = 0\cdot\int1\;dx = 0\cdot(x+c) = 0
\tag1
$$
$$
\frac{dc}{dx} = 0 \qquad\iff\qquad
\int 0\;dx = c, \qquad\forall c\in\mathbb{R}
\tag2
$$ These are two conflicting results. Based on this other question, Sam Dehority's answer seems to indicate:
$$
\int\alpha f(x)\;dx\neq\alpha\int f(x)\;dx,\qquad\forall\alpha\in\mathbb{R}
\tag3
$$ However, this clearly implies that indefinite integration is nonlinear, since a linear operator $P$ must satisfy $P(\alpha f) = \alpha Pf, \forall\alpha\in\mathbb{R}$, including $\alpha=0$. After all, a linear combination of elements of a vector space $V$ may have zero valued scalars: $f = \alpha g + \beta h, \forall\alpha,\beta\in\mathbb{R}$ and $g, h\in V$. This all seems to corroborate that zero is not excluded when it comes to possible scalars of linear operators. To take two examples, both matrix operators in linear algebra and derivative operators are linear, even when the scalar is zero. In a matrix case for instance, let the operator $A$ operate a vector: $A\vec{x} = \vec{y}$. Now: $A(\alpha\vec{x}) = \alpha A\vec{x} = \alpha\vec{y}$. This works even for $\alpha = 0$. Why is $(3)$ true? Can someone prove it formally? If $(3)$ is false, how do we fix $(1)$ and $(2)$? When exactly does the following equality hold (formal proof)?
$$
\int\alpha f(x)\;dx  = \alpha\int f(x)\;dx,\qquad\forall\alpha\in\mathbb{R}
$$ I would appreciate formal answers and proofs.","['indefinite-integrals', 'calculus', 'integration', 'real-analysis']"
1069667,Standard machine in measure theory,"Step 1.Prove the property for $h$ which is an indicator function. Step 2.Using linearity, extend the property to all simple positive functions. Step 3. Using Monotone property extend the property to all $h∈mF^+$. Step 4. Extend the property in question to $h∈L^1$ by writing $h=h^+−h^−$and using linearity. This is a standard approach used to prove theorems in measure and probability theory (See Defn. 1.3.6 in http://statweb.stanford.edu/~adembo/nyu-2911/lnotes.pdf ). What always confuses me is sometimes the theorem will be valid only for bounded measurable functions. My question is - when can you extend the property to all $L^1$ functions, and when to only bounded measurable functions? What theorems are in play here? I believe the pi-lambda argument is also involved somehow, but I'm not sure how.","['probability-theory', 'measure-theory']"
1069685,Showing $\gamma < \sqrt{1/3}$ without a computer,In 1735 Euler gave the value of $\gamma$ as $0.577218.$ The constant is generally defined as the limit of the difference between the harmonic series and $\log n:~\gamma= \lim_{n\to\infty}\sum_{k=1}^{n}\frac{1}{k}-\log n.$ Euler apparently relished this sort of calculation and must have taken quite a few terms to get such a good approximation. My question is whether without a computer one can now prove that $$\gamma <  \sqrt{1/3}$$ with at least some savings in terms of the type of work Euler apparently expended? I don't think there's any point in raising the bar to $\gamma < \ln 2\sqrt{\frac{23}{29}} $  because it seems to be the same sort of question. My own thought was to compare $\frac{1}{2}\int_0^{1/3}\frac{dx}{\sqrt{x}}$ to something like $(-1)\cdot\int_0^\infty e^{-u}\ln u~du$ but I expect there's a better way.,"['inequality', 'sequences-and-series', 'euler-mascheroni-constant']"
1069692,Solve $y' = \frac{1}{2}\sqrt{x} + \sqrt[3]{y}$,"Please help with this $$y' = \frac{1}{2}\sqrt{x} + \sqrt[3]{y}$$
Tried making $t=\sqrt[3]{y}$. Then $3t^{2}t'_{x} = \frac{1}{2}x^\frac{1}{2} + t$. $p=t'$. And then expressed $x$ and differentiated with respect to $t$. But can't see solution.",['ordinary-differential-equations']
1069694,Differential Equation: $\frac{\mathrm{d} y}{\mathrm{d} x} = xy + y\sin x$,"I'm trying to solve this differential equation and believe I may have solved it using the ""separable equations"" method. Here's my work: $$\frac{\mathrm{d} y}{\mathrm{d} x} = xy + y\sin x = y(x + \sin x),$$ $${\mathrm{d} y} \frac{1}{y} = {\mathrm{d} x} (x+\sin x),$$ $$\int {\mathrm{d} y} \frac{1}{y} = \int {\mathrm{d} x} (x+\sin x),$$ $$\ln y = x^2 - \cos x + C,$$ $$y = e^{x^2 - \cos x + C}.$$ Does this look right? And/or, does this ""solve"" the equation?","['ordinary-differential-equations', 'calculus', 'integration', 'proof-verification']"
1069696,Simple Equation Does my proof work?,"Its the inequality equation $|a+b| \leq |a|+|b
| $ I managed this by cases. Let $c = a$ and $d=b$ if $a>b $ let $c = b$ and $d = a$ if $b>a $ if $a=b$ let $a=c$ Hence we have $|c+d| \leq |c|+|d| $ where $c>d$ Case 1
if  $d > 0$   then $|c+d| = |c|+|d|$ $\forall c $ Case 2 
if $c < 0$ then $|c+d| = |c|+|d|$ $\forall d $ Case 3 if $ c \vee d =0$ then $|c+d| = |c|+|d|$ Case 4   $c > 0$  and $d < 0$ then $|c+d| < |c|+|d|$ $ \forall$ $c$ and $d$ But i want to solve this directly not by cases. I think that it can be done by treating a and b as vectors described by two components up down ($y$) and left right ($x$) where $|a|$ is the length of the vector $\vec a$ described as: $$|\vec a|=  \sqrt {x^2_a + y^2_a}$$
$$|\vec b|=  \sqrt {x^2_b + y^2_b}$$ And $|\vec a+\vec b|$ would be the length of $\vec a$ plus $\vec b$ or $|\vec a+\vec b|= \sqrt {(x_a +x_b)^2 + (y_a +y_b )^2 }$ thus $|\vec a+\vec b| \leq |\vec a|+|\vec b| $ = $ \sqrt {(x_a +x_b)^2 + (y_a +y_b )^2 } \leq \sqrt {x^2_a + y^2_a} + \sqrt {x^2_b + y^2_b}$ now since we cant have negative length we can square both sides. $ (x_a +x_b)^2 + (y_a +y_b )^2  \leq  x^2_a + y^2_a + x^2_b + y^2_b + 2(x^2_b + y^2_b)^{1/2} (x^2_a + y^2_a)^{1/2}$ $ ( x_a x_b + y_a y_b) \leq (x^2_b + y^2_b)^{1/2} (x^2_a + y^2_a)^{1/2}$ rhs = $(x^2_a x^2_b  + y^2_a x^2_b + x^2_a y^2_b  + y^2_a y^2_b)$ ? anyway from here it feels like im screwed on the plus side the right hand side is always $0 \geq  $ and the left side can be positive or negative. oddly whenever the x and y components of the vectors have a magnitude greater then or equal to 1 this seems to stand up fairly obviously its just when it has pieces under magnitude 1 that things get really ugly though only intuitively it feels obvious that when none of the components or either vector are $0$ the right side should be bigger then the left. any ideas on a different way to approach this? thanks. EDIT: i Originally did exactly what Rolighed has written down and came to the conclusion that $|x| |y| \geq |xy|$ as he did at the end of his proof but i tossed out the idea on the thought that i couldn't square both sides on the thought that $4 \geq -5$ but $4^2 < -5^2$ EDIT2: it turns out that this inequality can be squared because both sides are positive no matter what which makes if $ x \geq y$ and $ y \geq 0$  then $x^2 \geq y^2$  since we know both sides of our inequality must be 0 or bigger we can square them and do the easy proof as shown by Rolighed. My confusion of your explanation Rolighed how you come you could square both sides took me too long to realize why =(","['trigonometry', 'calculus', 'real-analysis']"
1069699,"A set of 19 numbers that are at most 93, and a set of 93 numbers that are at most 19, have equal sumsets","If $x_1, x_2, ..., x_{19}$ are natural numbers lower or equal than 93 and $y_1, y_2, ..., y_{93}$ are natural numbers lower or equal than 19 then there is a non zero sum of some $x_i$ which is equal to sum of some $y_j$. Any hints on how to prove this? I've been thinking about this for couple of days but still can't find a way to show that...","['elementary-number-theory', 'combinatorics']"
1069742,Does $\lim_{n\rightarrow\infty}\sin\left(\pi\sqrt[3]{n^{3}+1}\right)$ exist?,"I have this limit:
$$\lim_{n\rightarrow\infty}\sin\left(\pi\sqrt[3]{n^{3}+1}\right)$$
I don't even know if it exists. If so, what its value ?
Really don't have any idea..","['calculus', 'limits']"
1069751,Expressing n mod m in terms of floor values?,"I'm trying to prove the expression: $$\left\lceil\frac{n}m\right\rceil = \left\lfloor \frac{n+m-1}{m}\right\rfloor\;,$$ where $n,m$ are integers` So I've come across this article (PDF) which gives a nice method of proving the above expression from page $10$ onwards. I understand pretty much the entire proof, except for the definition on page $10$ . It states that $$\frac{n}m = \left\lfloor\frac{n}m\right\rfloor+\left\{\frac{n}m\right\}\;.$$ I've never encountered the $\{\}$ symbols before in maths, but given that any real quotient can be expressed as the floor of the quotient (the integer part) summed with the fractional part, I'm guessing that the $\{\}$ symbols state that this is the fractional part of the real number $\frac{n}m$ ? That's all fine by me, but what I can't understand is the next line. If I wanted to get rid of the $m$ on the L.H.S I'd multiply both sides by $m$ . But somehow, according to the pdf, it's true to say that $$m\left\{\frac{n}m\right\} = n \bmod m\;.$$ Can someone explain why this is the case? Thanks!","['fractional-part', 'discrete-mathematics', 'ceiling-and-floor-functions']"
1069783,Bounding an expected hitting time,"Consider a stochastic differential equation: $$dX_t = dW_t + \sin(X_t) dt, \, X_0 = x$$ where $W_t$ is a Wiener process. Define $$\tau_1 = \inf \{ t : X_t \in 2 \pi \mathbb{Z} \} \\
\tau_2 = \inf \left \{ t : X_t \in 2 \pi \mathbb{Z} \setminus \left \{ X_{\tau_1} \right \} \right \}.$$ My goal is to show that $\tau_1$ and $\tau_2$ both have finite expectation; this will allow me to solve a larger problem about recurrence using very standard arguments. Since the drift is $2 \pi$ periodic in space and homogeneous in time while the diffusion is homogeneous in both space and time, it is enough to consider $x \in (0,2 \pi)$ for the first part and $X_{\tau_1}=0$ for the second part. In the first case we will be done provided the problem $$\frac{1}{2} u'' + \sin(x) u' = -1, \, u(0)=u(2\pi)=0$$ has a nonnegative solution. In the second case we will be done provided the problem $$\frac{1}{2} u'' + \sin(x) u' = -1, \, u(-2\pi)=u(2\pi)=0$$ has a nonnegative solution. These equations may be solved in explicit form, but in terms of some non-elementary integrals, through reduction of order and integrating factors. For instance the solution to the first problem may be written as $$u(x) = -2 w(x) + 2 \frac{w(2 \pi) v(x)}{v(2 \pi)}$$ where $$v(x) = \int_0^x e^{2 \cos(y)} dy \\
w(x) = \int_0^x \int_0^y e^{2(\cos(y)-\cos(z))} dz dy.
$$ If I can prove $u \geq 0$ then I will be done. I'm fairly sure that any proof that this $u \geq 0$ will work on the second one. Any suggestions? Edit: a quick numerical check shows that $u \geq 0$ should indeed hold.","['probability-theory', 'stochastic-processes', 'stochastic-differential-equations', 'stopping-times']"
1069786,A real number $x$ such that $x^n$ and $(x+1)^n$ are rational is itself rational,"Let $x$ be a real number and let $n$ be a positive integer. It is known that both $x^n$ and $(x+1)^n$ are rational. Prove that $x$ is rational. What I have tried: Denote $x^n=r$ and $(x+1)^n=s$ with $r$, $s$ rationals. For each $k=0,1,\ldots, n−2$ expand $x^k\cdot(x+1)^n$ and replace $x^n$ by $r$. One thus obtains a linear system with $n−1$ equations with variables $x$, $x^2$, $x^3,\ldots x^{n−1}$. The matrix associated to this system has rational entries, and therefore if the solution is unique it must be rational (via Cramer's rule). This approach works fine if $n$ is small. The difficulty is to figure out what exactly happens in general.","['irreducible-polynomials', 'number-theory', 'polynomials', 'rationality-testing', 'galois-theory']"
1069803,Definition of conditional probabiliy as function dependent on $\sigma$-Algebra,"I know that for events $A,B$ with $P(B) > 0$ the conditional probability is defined as
$$
 P(A | B) = \frac{P(A \cap B)}{P(B)}. 
$$
Of course by regarding $A$ as constant, and varying $B$ we get a function $P(A | \cdot)$ by
$B \mapsto P(A | B)$, and so we get a function $P(A | \{ \cdot \}) : \Omega \to \mathbb [0,1]$ by $\omega \in \Omega \mapsto P(A | \{ \omega \})$ (assuming each $\omega$ has a non-zero probability). Is this function a random variable? Guess not because there is no measure space given on $[0,1]$ for a measure (like $P$). Also I stumble in the way conditional probabilities for $\sigma$-Algebra are defined. For this let $\mathcal F$ be an $\sigma$-Algebra, then the conditional probability $P(A | \mathcal F)$ is a $\mathcal F$-measurable and integrable random variable such that
$$
 \int_G P(A | \mathcal F) d P = P(A \cap G)
$$
for all $G \in \mathcal F$. This makes no sense to me, why now a function. In the classical definition I got a number, which could be interpreted as the probability of an event given another event, but here I have a collection of events I depend on, and the conditional probability is a function... makes no sense to me? Has my construction above something to do with the way conditional probabilites for $\sigma$-Algebras are defined? I just tried to come from the classical definition to the new one...","['probability-theory', 'probability', 'conditional-probability']"
1069824,Limits using Maclaurins expansion for $\lim_{x\rightarrow 0}\frac{e^{x^2}-\ln(1+x^2)-1}{\cos2x+2x\sin x-1}$,"$$\lim_{x\rightarrow 0}\frac{e^{x^2}-\ln(1+x^2)-1}{\cos2x+2x\sin x-1}$$ Using Maclaurin's expansion for the numerator gives: $$\left(1+x^2\cdots\right)-\left(x^2-\frac{x^4}{2}\cdots\right)-1$$ And the denominator: $$\left(1-2x^2\cdots\right) + \left(2x^2-\frac{x^4}{3}\cdots\right)-1$$ $$\therefore \lim_{x\rightarrow 0} f(x) = \frac{-\dfrac{x^4}{2}}{-\dfrac{x^4}{3}} = \frac{3}{2}$$ But Wolfram gives that the limit is $3$. I thought, maybe I used too few terms. What is a thumb rule for how many terms in expansion to use to calculate limits? Using three terms yielded the answer $\lim_{x\rightarrow 0}f(x) = -4$. What did I do wrong?","['taylor-expansion', 'polynomials', 'calculus', 'limits']"
1069827,How to solve the system $x y^5=8000$ and $x y^4>4100$?,I need help getting this equation solved for a website I am building. I am pretty bad at math and am only in pre-algebra. I don't know how I would go about canceling out the ^5 and ^4 because I can't square-root it. Thank you for your help. $$x y^5=8000$$ and $$x y^4>4100$$,"['algebra-precalculus', 'systems-of-equations']"
1069835,"Artin Chapter 11, Exercise 9.12, polynomials without common zeroes [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How do I show that the three polynomials $f_1 = t^2 + x^2 - 2$, $f_2 = tx - 1$, $f_3 = t^3 + 5tx^3 + 1$ generate the unit ideal in $\mathbb{C}[t, x]$? Artin mentions two approaches: by showing that they have no common zeros, and also by writing $1$ by a linear combination of $f_1$, $f_2$, $f_3$, with polynomial coefficients.","['ring-theory', 'algebraic-geometry', 'abstract-algebra', 'polynomials']"
1069845,A limit with an intuitive and wrong answer,"In my last question I asked about a limit used in my exploration of tangent circles and whatnot. I decided to come up with a more direct approach to my problem, and now I only have to evaluate the limit 
$$ \lim_{d\to x} \frac{\dfrac{f(x)-f(d)}{x-d}-f'(d)}{x-d}$$ Intuition would yield the answer is the second derivative of $f$ at $x$. However, by expanding and whatnot and then using l'Hôpital, as well as plugging in some sample $f$'s, I arrive at half of the second derivative. Why is my intuition wrong?","['derivatives', 'limits']"
1069859,Find the number of subgroups in $Z_p \times Z_p \times Z_p$,"let $p$ be a prime number ; I want to find the number of subgroups in $G = Z_p \times Z_p \times Z_p$ $(Z_p = \mathbb{Z}/p\mathbb{Z})$. I know that there is $p^2 + p + 1$ copies of $Z_p$ in $G$ for the following reason : I can generate subgroups isomorphic to $Z_p$ : $<(a,b,c)>$, I have $p^3-1$ distinct nonzero triples, but once I choose a particular triple, there are $p-1$ other triples which generate exactly the same copy of $Z_p$. so the number of isomorphic copies of $Z_p$ in $G$ is $\frac{p^3-1}{p-1}=p^2+p+1$. But I am not sure how should I find the number of isomorphic copies of $Z_p \times Z_p$ in $G$... I know that there is also $p^2+p+1$ copies, but I don't clearly see why ? I would appreciate your help in advance,
Thanks !","['cyclic-groups', 'group-theory', 'abstract-algebra']"
1069875,Solving $\frac{dy(t)}{dt} = y(t)^2-2 y(t)+2$,"How can I solve the following ODE
$$
\frac{dy(t)}{dt} = y(t)^2-2 y(t)+2
$$
I'm having a tough time because the differential is in terms of $dt$. My gut instinct is to integrate both sides, but to do, I would have to integrate the right hand side by $dt$, and would get a messy solution. How do I do this?",['ordinary-differential-equations']
1069879,Why is it differential equations exist on an interval instead of a domain?,"I understand a domain is the set of input elements a function is defined for (and can have breaks in it e.g. union of 2 sets) and a interval is a continuous range of real numbers. Why do we speak of the solution to a differential equation over an interval instead of an domain? If the solution to an ODE is just a function, and we normally speak of functions as having a domain, then why wouldn't we use domain?",['ordinary-differential-equations']
1069891,Graphing $\frac{x^2-x+1}{2(x-1)}$,"I need to graph $$\frac{x^2-x+1}{2(x-1)}$$ So I reduced it to make the derivative easy: $$f(x) = \frac{x(x-1)+1}{2(x-1)} = \frac{x}{2} + \frac{1}{2(x-1)}\\f'(x) = \frac{1}{2} - \frac{1}{2(x-1)^2}$$ which has roots $x = 0, x = 2$ But the derivative of the original function and the one I made are different. I can't see why. The two have the same roots, but I can't see what i'm doing wrong. The concavity was easy to find by the derivative I made, but i don't know what's happening","['calculus', 'derivatives', 'polynomials']"
1069909,"$L^1(X)$, delta epsilon measure proof","Let $f \in L^1(X)$ with $f \ge 0$. We know that $$\nu(E) := \int_E f\,d\mu$$defines a measure on $\Sigma$. How do I show that for every $ \epsilon > 0$ there exists $\delta > 0$ so that for any $E \in \Sigma$ the property $\mu(E) < \delta$ implies $\nu(E) < \epsilon$?","['measure-theory', 'real-analysis']"
1069918,Prove that $f\left(x\right)=\sin\left(x\right)$ is Continuous.,"The function $f\left(x\right)=\sin\left(x\right)$ is obviously continuous. But how would you prove this using the $\delta,\varepsilon$ definition of continuity? So given $x\in\mathbb{R}$ and $\varepsilon>0$, how do you determine the $\delta>0$ that guarantees that $\left|x-y\right|<\delta\Rightarrow\left|f\left(x\right)-f\left(y\right)\right|<\varepsilon$?","['trigonometry', 'continuity', 'real-analysis']"
1069926,The Island in the Miracle Sea. (Christmas edition),"To all of you who love math like me, I have this puzzling riddle that I hope you find interesting : On Christmas Eve just after midnight, Santa was riding his sleigh over the Miracle Sea when suddenly something went wrong and he crashed on a lonely Island. Although this Island wasn't connected to the outer world, it wasn't completely deserted. It was populated by elves that was both intelligent and rational. But they had one flaw. Among the elves it was considered a huge shame to have blue eyes. If an elf discovered that she/he had blue eyes, she/he would flee the island in shame the next midnight. You would think that since the elves had lived on the island for generations, there would be no blue eyed elves left, but since there didn't exist any mirrors on the island and the fact that all the elves was so polite, that they would never mention each others eye color, there was actually still some with blue eyes. Now.. Santa was invited in by the elves and his sleigh was repaired. After three Christmas hours with the elves, which for Santa on Christmas Eve is almost nothing, he was ready to move on. But when he sat up in his sleigh he made the mistake of shouting ""Ho! Ho! Ho! It's is so nice to see both blue and brown eyed elves living together. Merry Christmas to all of you!"" And then he flew away. The 43'th midnight after Santas visit some of the elves left the island. How many blue eyed elves was on the island, when Santa crashed? Merry Christmas to all of you and this exercise is from ""Kalkulus"" by Tom Lindstrøm, which is a great book :D","['puzzle', 'calculus', 'number-theory', 'combinatorics']"
1069937,Prove that the function: f: $\mathbb{N} \mapsto \mathbb{Z}$ defined as f(n)= $\frac{(-1)^n(2n-1)+1}{4}$ is bijective.,"Prove that the function: f: $\mathbb{N} \mapsto \mathbb{Z}$ defined as f(n)= $\frac{(-1)^n(2n-1)+1}{4}$ is bijective. This is rough. I've been staring at this one for a while now. I get stuck on the injective part. I set f(a)=f(b), and I'm trying to show a=b. It's straightforward up to $(-1)^a(2a-1)=(-1)^b(2b-1)$. From there I'm at a loss of how to proceed. I'd also appreciate a hand on the surjective part.",['functions']
1069940,Smallest constant of Lipschitz retraction from bounded to continuous functions,"Let $B$ be the space of all bounded functions $f:[0,1]\to\mathbb R$
equipped with the supremum norm*. It contains $C$, the space of
continuous functions on $[0,1]$, as a subspace. An $L$-Lipschitz
retraction $R:B\to C$ is, by definition, a map such that
$$R(f)=f \qquad \forall f\in C$$ $$\|R(f)-R(g)\|\le L\|f-g\| \qquad
\forall f,g\in B$$ Question : Does there exist a $2$-Lipschitz retraction from $B$ onto $C$? I expect the answer to be negative, but so far was unable to find a
suitable obstruction. Motivation It is known that there is a $20$-Lipschitz retraction; such a map
(pretty complicated) is constructed in Theorem 1.6 in Geometric
Nonlinear Functional Analysis by Benyamini and Lindenstrauss. The
constant $20$ could probably be lowered by tweaking their
construction, but I am not interested in that right now. There is no $L$-Lipschitz retraction for $L<2$. (Proof below). The constant $2$ is known to be smallest possible for Lipschitz
retraction from $\ell_\infty$ onto $c_0$ (Example 1.5 in the same
book). Proof of item 2 above . For $n\in \mathbb N$, let $f_n\in C$ be a
function such that $f_n(x)=-1$ for $x\le \frac12-\frac1n$, $f_n(x)=1$
for $x\ge \frac12+\frac1n$, and $f_n$ is linear in between. Let
$f(x)=\frac12 \operatorname{sign}(x-1/2)$. Note that $\|f-f_n\|\le
1/2$ for all $n$. Here is an illustration: $f$ in red, $f_n$ in blue. The function $R(f)$ must satisfy $\|R(f)-f_n\|\le L/2$ for all $n$.
Therefore, $R(f)(x)\ge 1-L/2$ for $x>1/2$ and $R(f)(x)\le -1+L/2$ for
$x<1/2$. Since $R(f)$ is continuous, it follows that $L\ge 2$. Remark on item 3 : a $2$-Lipschitz retraction from $\ell_\infty$ onto $c_0$ is obtained by mapping each $(x_n)\in \ell_\infty$ to $(x_n-\min(|x_n|,s)\operatorname{sign}x_n)$ where $s=\limsup|x_n|$. Since $s$ is a $1$-Lipschitz function on $\ell_\infty$, the resulting map is $2$-Lipschitz. (*) That is, $\|f\|=\sup_{[0,1]}|f|$. Note that $B$ is different
from $L^\infty[0,1]$ because the functions that are equal a.e. are not
identified, nor is there any measurability requirement.","['lipschitz-functions', 'functional-analysis', 'banach-spaces']"
1069954,Whether a $2 \times 2$ matrix of rank $1$ has a zero eigenvalue,"""Does $A = \begin{bmatrix}1&2\\2&4\end{bmatrix}$ have a zero eigenvalue?"" Well, it would be a funny question to ask if the asker didn't state that he wants us to explain without computing the characteristic polynomial. I have no idea how to do this.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
1069962,Edges of a permutohedron,"Consider a permutohedron $P_n$ (this is a polytope which is  a convex hull of $n!$ points, which are obtained from $(1,2,...,n)$ by all possible permutations of coordinates). 
I have to prove the following: 2 vertices are connected with an edge iff their coordinates differ by transposition of 2 numbers which differ by 1. I have managed to prove only one side (if they differ in such way then they are connected with an edge). Please help.","['polytopes', 'combinatorics']"
1069963,Prove the energy is constant in a PDE?,"I calculated the $$
\begin{align}
\frac{dE(t)}{2\,dt} & = \int_\Omega u_tu_{tt}+DuDu_t+u^3u_t\,dx \\
& =\int_\Omega [u_t(u_{tt}-\Delta u)+u^3u_t] \, dx+\int_{\partial \Omega} u_t \frac{\partial u}{\partial v} \, ds = \int_{\partial \Omega} u_t \frac{\partial u}{\partial v} \, ds
\end{align}
$$ However, I don't know how to prove $$\int_{\partial \Omega} u_t \frac{\partial u}{\partial v}ds$$ is zero by the conditions given in the problem? Can anyone help me about this? Thanks so much! Can anyone help me explain why $u_t\cong 0$ on $\partial \Omega$?
Thanks so much!:)","['ordinary-differential-equations', 'partial-differential-equations', 'analysis']"
1069989,Automorphism that is an Involution of a finite group,"I am studying for a final and am trying to solve this problem:
Let $G$ be a finite group with an automorphism $\sigma:G\rightarrow G$ such that $\sigma \circ \sigma=1$ and whose only fixed point is the identity element. I want to prove that $G$ is abelian and has odd order. To show $G$ is abelian, since $\sigma \circ \sigma(xy)=xy$, showing that $xy=yx$ is the same as showing $\sigma \circ \sigma(xy)=\sigma \circ \sigma(yx)$. Since $\sigma \circ \sigma(xy)=\sigma(\sigma(x)\sigma(y))$, we want to show $\sigma(\sigma(x)\sigma(y))=\sigma(\sigma(y)\sigma(x))$, so it is enough to show that $\sigma(x)\sigma(y)=\sigma(y)\sigma(x)$, but I am having trouble working out the details. To prove that $G$ has odd order, I am trying to show this by contradiction and assuming that $G$ is even. If $G$ is even, then it has at least one element $y$ of order 2, but using this, I haven't been able to get a contradiction.","['group-theory', 'abstract-algebra']"
1070003,Existence and uniqueness of a function generalizing a finite sum of powers of logarithms,"I hope to find a proof of the following conjecture: $(1)$ For every $a>0$ there is a convex analytic function $f_a:\mathbb R^+\to\mathbb R$ such that: $f(1)=0$ and $\forall x>1,\ f_a(x)=f_a(x-1)+\ln^ax$ (thus, for $n\in\mathbb N,\ f_a(n)=\sum_{k=1}^n\ln^ak$). $(2)$ For every $a>0$ such function $f_a$ is unique. Examples : for $a=1$, the function is $f_1(x)=\ln\Gamma(x+1)$. for $a=2$, the function is $f_2(x)=\gamma_1+\frac{\gamma^2}2-\frac{\pi^2}{24}-\frac{\ln^2(2\pi)}2-\zeta''(0,x+1)$. (where $\gamma_1$ denotes the first Stieltjes constant , and $\zeta''$ denotes the second derivative of the Hurwitz $\zeta$-function with respect to its first parameter) If the conjecture is true, can we find an explicit form for $f_a(x)$, e.g. an integral representation in terms of known special functions?","['convex-analysis', 'logarithms', 'functions', 'summation', 'analyticity']"
1070005,"The ""sin-cos-maximum"" function","Is there some specific notation for the function $f(x):=\max\{\cos(x),\sin(x)\}$, or maybe some equivalent compact expression? Improvement: Actually, maybe a compact equivalent expression for its squared version $g(x):=\max\{\cos^2(x),\sin^2(x)\}$, is easier.",['functions']
1070006,Calculating integral of step function,"My question is from Apostol's Vol. 1: One-variable calculus with introduction to linear algebra textbook. Page 70. Exercise 10. Given a positive integer $p$ . A step function $s$ is defined on the interval $[0,p]$ as follows: $s(x)=(-1)^nn$ if $x$ lies in the interval $n\le x<n+1$ , where $n=0,1,2,\cdots,p-1;$ $s(p)=0$ . Let $f(p)=\int_0^ps(x)\mathrm dx.$ a) Calculate $f(3),f(4)$ and $f(f(3)).$ b) For what value (or values) of $p$ is $|f(p)|=7$ ? The attempt at a solution. As I understood, $s(x)=(-1)^nn$ can also be expressed as $s(x)=(-1)^{\lfloor x\rfloor}\lfloor x\rfloor$ , and graph of that step function is: Here, $$f(3)=\int_0^3((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=2-1=1,$$ $$f(4)=\int_0^4((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=2-1-3=-2,$$ $$f(f(3))=\int_0^1((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=0,$$ But answers in the book say that $f(4)=-1$ , where am I making mistake or what am I misunderstanding?","['calculus', 'ceiling-and-floor-functions']"
1070007,"How ""sharp"" does a cusp have to be in order for the equation to be nondifferentiable?","From a mathematical standpoint, I understand the concept of cusps: for example, a cusp exists at the origin of $y=|x|$ because one cannot take the limit from both sides, and therefore the derivative does not exist. However, I have always wondered: how ""sharp"" does a cusp have to be in order to make the region nondifferentiable? To extend the previous example, intuitively I would assume that if the slopes from both sides of $y = |x|$ where to decrease (eg $y = |\frac{1}{2}x|$, etc) the point at (0,0) would stay nondifferentiable until the slopes from both sides became 0, at which point $f'(x) = 0$. Is this correct, and if so, why? Does it have something to do with the ""abruptness"" of the change between values, or is there a more basic underlying concept that I am missing?","['ordinary-differential-equations', 'calculus', 'derivatives']"
1070016,Sum $\sum\limits_{n=1}^{\infty}\frac{(-1)^n}{\sqrt{n}}$,"I have the following infinite sum:
$$
\sum\limits_{n=1}^{\infty}\frac{(-1)^n}{\sqrt{n}}
$$
Because there is a $(-1)^n$ I deduce that it is a alternating series.
Therefore I use the alternating series test:
$$
\lim_{n\to\infty} \frac{1}{\sqrt{n}}
$$
Because this limit is decreasing and approaching $0$ I thought it should therefore be convergent. However in the answer key it uses a different method to get a different answer. It instead takes the absolute value of the series: $\left|\frac{(-1)^n}{\sqrt{n}}\right| = \frac{1}{\sqrt{n}}$ and says because this is a divergent p series that the series is divergent. Why is the alternating series test not applied here?","['sequences-and-series', 'convergence-divergence', 'calculus', 'summation']"
1070034,The weak-star topology is Completely Hausdorff (in particular Hausdorff).,"Let $X$ be a normed space, $X^*$ its dual space, $(X^*, w^*)$ is completely Hausdorff. Proof: Let $f, g \in X^*$, $f\neq g$ then $\exists x\in X$ such that $f(x)\neq g(x)$ i.e. $\hat{x}(f)\neq \hat{x}(g)$ (here $\hat{x}\in X^{**}$, $\hat{x}(h)=h(x)$ for $h\in X^*$) so $(X^*, w^*)$ is completely Hausdorff. Note: A top space is Completely Hausdorff when for any given pair of points in it there exists a real valued continuous function that separates them. The whole thing seems pretty straight forward but it is always good to check. EDIT: I'm thinking, if the scalar field is complex, we can take either $\text{Re} \hat{x}$ or $\text{Im} \hat{x}$ as the separating continuous function. Also, this function is usually required to map into $[0,1]$. It is sufficient that it maps into $\mathbb{R}$ if the image is bounded though. The whole thing does not bother me because I'm only interested is proving that the unit closed ball $(B_{X^*}, w^*)$ is Hausdorff, which by Banach-Alaoglu is compact (so its image by a real cont function will be bounded). However I asked the question in a more general sense and I am wondering now.","['general-topology', 'functional-analysis']"
1070047,"conical surface, parametrization, immersion, Gaussian and mean curvatures","""Find the parametric form of a conical surface $S$ which is spanned by all rays starting $($but not including $)$ a fixed point $\gamma$ and passing through an arbitrary point on $\gamma$ and passing through an arbitrary point on $\gamma$. Formulate a condition which is necessary and sufficient for $S$ to be immersed. Find the Gaussian and mean curvatures."" My progress so far: We might as well assume the distinguished point is the origin. Then our surface is parameterized as $$X(s,t) = t\gamma(s),\text{ }t \neq 0,$$$$X_s = \dot{\gamma}(s),$$$$X_t = \gamma(s).$$What do I do next? Could I have a hint?","['multivariable-calculus', 'differential-geometry']"
1070106,Calculate $I_m = \int_{-\infty}^\infty \frac{dx}{1+x+x^2+\cdots+x^{2m}}$ using complex variables,"I have come as far as deducing that the denominator can be written as a geometric series. Hence, for $m=2$,
\begin{align*}
\int_{-\infty}^\infty \frac{1-x}{1-x^5} dx &= 2 \pi i ( B_1 + B_2 ) - \int_{C_R} \frac{1-z}{1-z^5} dz \\
&= 2 \pi i \left( \frac{1 - \exp(i \frac{2 \pi}{5})}{-5 \exp(i\frac{8 \pi}{5})} + \frac{1 - \exp(i \frac{4 \pi}{5})}{-5 \exp(i\frac{16 \pi}{5})} \right) - 0\\
&\overset{*}{=} \frac{\pi}{5} \cot\left( \frac{\pi}{10} \right) \sec \left( \frac{\pi}{5} \right).
\end{align*}
where $B_1$ and $B_2$ denote the residues in the upper-half plane. However, I can't work out why the equality $\overset{*}{=}$ should hold. I'm sure it's the result of some clever manipulation but I've been staring at it for a while without much progress. Is my solution wrong?",['complex-analysis']
1070108,Why do we first introduce the open set definition for continuity instead of the neighborhood definition?,"After (nearly) completing my course in topology, something weird just stuck out to me which I hadn't considered before. When first discussing continuity, we often use the following definition: Let $X$ and $Y$ be topological spaces. We say that $f:X\to Y$ is continuous if for every open set $V\in Y$, $f^{-1}(V)$ is open in $X$. This is a rather opaque definition and isn't quite as easily relatable to the notion we develop on $\Bbb R$ as the following definition: Let $X$ and $Y$ be topological spaces. We say that $f:X\to Y$ is continuous if for each $x\in X$ and neighborhood $V$ containing $f(x)$, there is a neighborhood $U$ of $x$ such that $f(U)\subseteq V$. These are of course equivalent definitions. However the latter is quite easy to connect to our normal intuition built up from real analysis: if our $x$-values are ""close"", then our $y$-values must be ""close."" Pedagogically, why have we somewhat cast away the latter definition as a mere equivalence and opted for the former? Clearly the latter is what led to the former and is, arguably, easier to latch on to. Is this somewhat of a byproduct of the category-theoretic nature of the former (with $f$ being a morphism of topological spaces) and math's general trend towards category-theoretic personifications? Can it also be attributed to early topologists wanting to separate topology from analysis in this way?","['general-topology', 'philosophy', 'education', 'math-history']"
1070113,Borel measurability is a local property,"I am looking at Exercise 5.2 (page 44) in ""Real Analysis for Graduate Students"" by Richard Bass. Let $f:(0, 1)\to \mathbb{R}$ be a function such that for every $x\in (0, 1)$,
  there exist $r>0$ and a Borel measurable function $g$, both depending
  on $x$, such that $f$ and $g$ agree on $(x-r, x+r)\cap (0, 1)$. Prove
  that $f$ is Borel measurable. Attempt. For each $x\in (0, 1)$, let's denote $r_{x}$ and $g_{x}$ to be the quantities given in the problem statement. By choosing $r_x$ sufficiently small, we may assume that $(x-r_{x}, x+r_{x})\subseteq (0, 1)$. Then for $a\in\mathbb{R}$, we have
$$
f^{-1}((a, \infty))=\{y: f(y)>a\}=\bigcup_{x\in (0, 1)} \{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x})
$$
Since $g_{x}$ is Borel measurable, we know that $\{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x})$ is a Borel set. But the union above is uncountable; so how do we show that $f^{-1}((a, \infty))$ is a Borel set? Attempt 2. Since $\mathbb{Q}$ is countable and dense in $(0, 1)$, we can try looking at the points $x\in\mathbb{Q}\cap (0, 1)$. But then the intervals $(x-r_{x}, x+r_{x})$ need not cover $(0, 1)$. The standard example is as follows. Suppose $\{q_1, q_2, …\}$ is enumeration of rationals in $(0, 1)$, and let $\varepsilon>0$. If we let $r_{q_{j}}$ to be $\varepsilon 2^{-j}$, then the measure of $\bigcup_{j=1}^{\infty} (q_j-r_{q_{j}}, q_{j}+r_{q_{j}})$ is at most $2\varepsilon$. Attempt 3. If each $g_{x}$ is continuous, we would be done. In that case, $$\{y: g_{x}(y)>a\}\cap (x-r_{x}, x+r_{x})$$ would be an open set; so as a union of open sets, $f^{-1}((a, \infty))$ would be an open set, and hence Borel.","['measure-theory', 'real-analysis']"
1070127,How to evaluate a double integral with two Dirac functions?,"Here I have a problem, is the solution  the same if I integrate every one? part by part? $$\int_0^Te^{-(s+\mu\lambda^2 ) t}  \int_0^l\left[\delta(x-R)\delta(t-tj)\varphi(x) \, dx\, dt\right]$$ I've already integrated but not sure about the result, I would like to corroborate it.","['multivariable-calculus', 'definite-integrals', 'dirac-delta', 'distribution-theory', 'real-analysis']"
1070141,A polynomial that annihilates two other,"While studying, I found the following problem: Let $f, g \in F[t]$. Prove that $\exists p \in F[x, y], p \neq 0 : p(f(t), g(t)) = 0$ I'd thank any hints that point me in the right direction.","['linear-algebra', 'polynomials']"
1070145,How to prove that the dimension of a hyperplane is n-1,"The hyperplane $H$ defined by $$H:=\{x\in\mathbb {R}^n:a^Tx=b\}$$ is the set that has dimension $n-1$, my question is why or how can we prove that its dimension is $n-1$? Thank you to every one who provide any help or if possible the proof for that.",['linear-algebra']
1070150,"Suppose that f is integrable on $[a,b]$. Prove there is a number $x$ in $[a,b]$ such that $\int_a^x f = \int_x^b f$","Also, show by example that it is not always possible to choose $x$ in $(a,b)$ I've proven the first part (in the title), but I can't seem to think of a scenario for the second part.  Perhaps my brain is a bit fuzzy at this point in the night, so I apologize in advance if this is an obvious answer, but any help would be appreciated.","['real-analysis', 'analysis']"
1070151,"For i.i.d. $U(0,1)$ random variables $(X_i)$, $\max\limits_{1\le i \le n/2}\{(1-2i/n)X_i\}\to1$ in probability","I want to show $\max_{1\le i \le \frac{n}{2}}\{(1-\frac{2i}{n})X_i\}$ converges in probability to $1$ as $n \to \infty$, where $X_i$ is an i.i.d sequence of $[0,1]$-uniformly distributed random variables. I didn't learn any estimate to deal with maximum except Kolmogorov's inequality, but seems it does not work here. Can anyone provide any idea? Many thanks!","['probability-theory', 'uniform-distribution', 'random-variables']"
1070154,A power series that converges for $|x| \leq 1$ and diverges otherwise.,"I need to find a power series $\sum a_n z^n$ that converges for $|x| \leq  1$ and diverges otherwise. I think I have one I just want to be sure. So, the series: $\sum \frac{z^n}{n^2}$ has radius of convergence of 1.  So it converges when $|z| <1$ and diverges when $|z| >1$, correct? And we know it converges at $z= \pm 1$ by the comparison test, correct?  This part is where I'm having trouble with.  Could someone explain in detail how to use the comparison test with this? I know the comparison test says, ""if you have two series $\sum a_n$ and $\sum b_n$ with $a_n, b_n \geq0$ and $a_n \leq b_n$, then if $\sum b_n$ converges, then $\sum a_n$ converges."" But what other series would you use in the comparison test.  I also know that $|\frac{z^n}{n^2}|= \frac{1}{n^2}$.  Can you use this fact? Please help! This series would work correct?",['real-analysis']
1070167,Munkres Section 10: How are these order types different?,"Let $\mathbb{Z}^+$ denote the set of all positive integers in the usual order, let $n$ be a positve integer, and let the following sets have the dictionary order: $\{1, \ldots, n \} \times \mathbb{Z}^+$, $\mathbb{Z}^+ \times \mathbb{Z}^+$, and $\mathbb{Z}^+ \times \left( \mathbb{Z}^+ \times \mathbb{Z}^+ \right)$. Now Munkres states that all these sets have different order types. My question is: how do we show that this is indeed the case? Each of these sets has a smallest element, and in each set, every element has an immediate successor. In $\{1, \ldots, n \} \times \mathbb{Z}^+$, only finitely many elements fail to have immediate predecessors, namely, the elements $(1,1), \ldots, (n,1)$, whereas in $\mathbb{Z}^+ \times \mathbb{Z}^+$, there is an infinite subset of elements without immediate predecessors, viz., the set $\{ (1,1), (2,1), (3,1), \ldots \}$, and the situation is similar for the set $\mathbb{Z}^+ \times \left(\mathbb{Z}^+ \times \mathbb{Z}^+ \right)$. Is this difference sufficient to distinguish the order types of $\{1, \ldots, n\} \times \mathbb{Z}^+$ from that of either $\mathbb{Z}^+ \times \mathbb{Z}^+$ or $\mathbb{Z}^+ \times \left( \mathbb{Z}^+ \times \mathbb{Z}^+ \right)$ And how do we know that the order types of $\mathbb{Z}^+ \times \mathbb{Z}^+$ and $\mathbb{Z}^+ \times \left( \mathbb{Z}^+ \times \mathbb{Z}^+ \right)$ are different?","['elementary-set-theory', 'order-theory']"
1070180,Second order differential equation with a variable coefficient. Show |f(x)| is bounded.,"Was given this question as extra credit on an ODE exam. Didn't have time during the exam to consider it, but I have since then, and I'm stumped. $$ f''(x) + f(x) = -f'(x)x^{2015}$$ $f(x)$ is twice differentiable and continuous everywhere. Show that $|f(x)| < M $ for some real $M$. Hint: compute the derivative of $f'(x)^2 + f(x)^2$. I wrote this from memory, and I hope its correct or I've waiting quite a bit of time trying to figure this out. As per the hint, I compute the derivative and found that
$$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = 2f''(x)f'(x) + 2f'(x)f(x) = 2f'(x)[f(x) + f''(x)] $$
So, that would be $2f'(x)$ multiplied by the left hand side of the given differential equation. In other words, if we multiply through by $2f'(x)$, we have
$$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = -f'(x)x^{2015}*(2f'(x)) $$ 
$$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = -2f'(x)^2x^{2015} $$ I didn't manage to produce anything fruitful from here. I tried to integrate both sides, but could not solve or draw any conclusions from the right hand side. I tried to isolate $x^{2015}$ and then integrate, but I couldn't solve the left hand integral. At this point, I was pretty stumped and tried a bunch of other ideas. I tried to solve the homogeneous system, to no avail. Then there was a fun attempt at creating a system of first order equations that left me with a, predictably, non-constant coefficient matrix. At that point, I decided to step away and ask for some help. Am I headed in the right direction with either of these attempts? If it helps, this is supposed to require only knowledge of calculus, so I'm certain the key lies above, especially considering the hint, but I'm not sure where to proceed. Solutions and hints equally appreciated! EDIT: Forgot to mention, $f(x)$ is twice differentiable and continuous everywhere. Will include at top. Update: Focused on studying for final exams but I will return to this! Still stumped.","['ordinary-differential-equations', 'calculus']"
1070188,"A compact set, which is not closed.","I'm looking for a compact set, which is not closed. I read somewhere that $Z^+$ are compact and not closed, but I don't understand why. Are there any other examples of compact sets that are not closed and could you please explain? I know that we can't look in the reals because every compact set in the reals is closed and bounded correct?","['compactness', 'analysis']"
1070213,"Evaluate $\int\frac{\sin(8x)}{9+\sin^4(4x)}\,\mathrm dx$","I have tried to evaluate $$∫\frac{\sin(8x)}{9+\sin^4(4x)}\,\mathrm d x$$ using the following identity: $$\frac{d(\sin^{-1}{u})}{du} = \frac{du}{1+u^2}$$ So I then reformed the integral to this: $$1/9\int\frac{\sin(8x)}{1+\sin^4(4x)/9}\,\mathrm dx = 1/9\int\frac{\sin(8x)}{1+(\sin^2(4x)/3)^2}\,\mathrm dx$$ My $u$ in this case would be $\sin^2(4x)/3$. But I do not know where to go from here because I don't know how to reform $\sin(8x)$ into $d(u)$. Could anyone please explain how I could turn $\sin(8x)$ into my $d(u)$?","['trigonometry', 'calculus', 'integration', 'indefinite-integrals']"
1070214,"Show that $E(X\mid Y, Z) = E(X\mid Y)$ almost surely with condition Z is independent of $(X, Y)$","$(X, Y, Z)$ is a continuous random vector and $Z$ is independent of $(X,Y)$. Prove that $E(X\mid Y, Z) = E(X\mid Y)$ almost surely. I had been thinking this question tonight but couldn't figure out how to apply the independent condition. I was trying to show $E(X\mid Y,Z)$ is another version of $E(X\mid Y)$ but came up with nothing. More detail, by the definition of conditional expectation as a random variable, 
\begin{equation*}E((X - E(X\mid Y,Z)\cdot H(Y,Z))=0\end{equation*} for every function $H$. Now I want to show $E(X\mid Y,Z)$ is another version of $E(X\mid Y)$ and then by the uniqueness of conditional expectation, I can get the desired result. Also we can write\begin{equation*}E((X - E(X\mid Y,Z)\cdot h(Y))=0\end{equation*} for every function $h$ since $h(Y)$ is included in $H(Y,Z)$. But $E(X\mid Y,Z)$ is function of $(Y,Z)$ instead of $Y$ which means it is still not a version of $E(X\mid Y)$. And it occurs to me, I didn't use the independent condition and that is where I stuck. I think it remains to show $E(X\mid Y,Z)$ is a function of $y$ alone, which should be true intuitively since $Z$ is independent of $(X,Y)$. (i.e knowing $Z$ makes no contribution to know $X$) But how to do it rigoriously?","['probability-theory', 'conditional-expectation']"
1070228,Usage of Rouche's theorem?,"I'm trying to find the number of zeros for the function $f(z) = z + 2 - e^z$ in the half plane $\{\mathscr{R}z < 0\}$. I know I'm supposed to use Rouche's theorem, which states that if both $f$ and $g$ are holomorphic inside and on some closed contour $C$, and $|f(z)| > |g(z)|$ for all $z \in C$, then $f$ and $f+g$ have the same number of zeros inside of $C$. The problem is, I'm having some trouble coming up with intuition about what to choose for $f$ and $g$. Something tells me that $f(z) = z+2$ and $g(z) = -e^z$, but how precisely do I complete the problem with Rouche's theorem? Thanks.",['complex-analysis']
1070229,What properties do isospectral Riemannian manifolds share?,"I'm studying the Laplacian on (compact) Riemannian manifolds, and it turns out that if the Laplacian operators of two such spaces share their spectrum (the spaces are then called isospectral ), then the spaces themselves must share some geometric properties. The most spectacular I've come across is that if two Riemannian manifolds are isospectral, then they share their sequence of lengths of closed geodesics. What other properties do they share? I'd really appreciate references on these, so I could look up the  proofs. So far I've looked at Buser's textbook ""Geometry and Spectra of Compact Riemann Surfaces"", and some notes by Yaiza Canzani for a course at Harvard. Between them they mention the closed geodesic lengths, as well as shared volume, dimension, and total Ricci curvature (so for surfaces, the same Euler characteristic, meaning that they are diffeomorphic assuming compactness).","['laplacian', 'riemannian-geometry', 'spectral-theory', 'differential-geometry']"
1070254,What is the importance of $\sinh(x)$?,"I stumbled across $\sinh(x)$. I am only a calculus uno student, but was wondering when this function comes into play, and what is its purpose? Last, does it have world applications, or is it a human-made concept?","['calculus', 'hyperbolic-functions']"
1070264,If $\int_E f=\int_E g$ then $f=g$ a.e.?,"Is the converse of the following statement is true? Let $f$ and $g$ be two bounded measurable functions on a set $E$. If $f(x)=g(x)$ a.e. on $E$ then
$$\int_E f=\int_E g$$ Here is my proof for converse but my textbook says give an example that the converse statement does not hold. Let $A_1=\{x|f(x)>g(x)\}$ and $A_2=\{x|f(x)<g(x)\}$ $A_1\cup A_2=\{x|f(x)\not=g(x)\}$ Suppose $m(A_1)>0, \,\,\, A_1=\bigcup_{n=1}^\infty E_n$ Therefore there exists $n, \,\,\, m(E_n)>0$ $E_n=\{x|f(x)-g(x)\geq\frac{1}{n}\} \,\,\,\, f-g$ is
measurable therefore $E_n$ is measurable. $
\int_{E_n}f-\int_{E_n}g=\int_{E_n}f-g\geq\frac{1}{n}mE_n>0\,\,\,$
contradiction. Similarly if $m(A_2)>0$ we get a contradiction.","['lebesgue-integral', 'measure-theory']"
1070275,"If A is normal, then the nullspace of A is the nullspace of A*","Suppose $A$ is a normal matrix. Prove that $x$ is in the nullspace of $A$ if and only if $x$ is in the nullspace of $A^{*}$. This isn't a homework problem. It was on a test I took recently, and I'd like to know if I was on the right track in solving it.","['matrices', 'linear-algebra']"
1070277,No sum of three numbers equals another number in set,"Consider the set $S=\{1,2,\ldots,1000\}$. What is the maximum size of a subset $S'$ such that for any distinct $a,b,c,d\in S'$, we have $a+b+c\neq d$? We can choose $S'=\{333,334,335,\ldots,1000\}$, so that $|S'|=668$. For the proof that this is optimal, I would like to use the pigeonhole principle to show that any $669$ elements will have $a+b+c=d$. But choosing the buckets iteratively like $\{1,2,3,6\},\{4,5,7,16\},\ldots$ seems difficult.","['extremal-combinatorics', 'combinatorics']"
1070293,Linear maps for $\Bbb{R}^n$ to $\Bbb{R}^m$?,"This question is related to: What is $\Bbb{R}^n$? The basis of a matrix representation I am still confused about the topics in these questions and am going to ask another question that will hopefully clarify this for me. Consider the following theorems: Theorem 1 Let $A$ be an $m\times n$ matrix with components in the field $\Bbb{F}$ then the map: $$\Bbb{F}^n\rightarrow \Bbb{F}^m$$
  $$x \mapsto  Ax$$
  is linear. Conversely, if $f:\Bbb{F}^n \rightarrow \Bbb{F}^m$ is linear , there exists a unique $m \times n$ matrix $A$  with components in the field $\Bbb{F}$  for which $f(x)=Ax$. Hence one can interoperate the $m \times n$ matrices as the linear map from $\Bbb{F}^n$ to $\Bbb{F}^n$ This was taken from ""Linear Algebra"" by Janich, Kalus (with minor changes). Now consider the similar theorem from the same book. Theorem 2 Let $f:V\rightarrow W$ be a linear map between vector spaces over $\Bbb{F}$, and let $(v_1,...,v_n)$ and $(w_1,...,w_n)$ be bases for $V$ and $W$ respectively. Then the $m \times n$ matrix $B$ determined by the commutator diagram: $$
\require{AMScd}
\begin{CD}
V @>{f}>> W\\
@VVV @VVV \\
\mathbb{F}^n @>{B}>> \mathbb{F}^n
\end{CD}
$$
  is called the matrix associated to $f$ relative to the two chosen bases. The matrix $B$ is associated with the basis $v= (v_1,...,v_n)$ and $w=(w_1,...,w_n)$, and we could write $B$ in terms of another basis which will change its components and be representing $f$ in the new basis. It is thus not theorem 2 that I have a problem understanding. It is rather theorem 1. Example Consider the linear map $$f(
        \begin{bmatrix}
        x \\ y
        \end{bmatrix})=\begin{bmatrix}
        2x+y \\ y
        \end{bmatrix}$$
  Clearly here the vector  $$\begin{bmatrix}
        x \\ y
        \end{bmatrix}$$ is in $\Bbb{R}^n$ (taking $x,y\in \Bbb{R}$) and is not a coordinate vector , but an arbitrary vector in $\Bbb{R}^n$. We could write the linear map $f$ as follows:
  $$f(
        \begin{bmatrix}
        x \\ y
        \end{bmatrix})= \begin{pmatrix}
        2 & 1\\ 0&1
        \end{pmatrix}
\begin{bmatrix}
        x \\ y
        \end{bmatrix}
$$
  In this case the matrix $A$ in theorem 1 is given by:
  $$A=\begin{pmatrix}
        2 & 1\\ 0&1
        \end{pmatrix}$$ The thing that I am confused about is simply which bias the matrix $A$ is in as described above. My view (which goes against the answers in the linked questions) is that $A$ should not be associated with a basis as described above. The reasoning behind this is that $A$ performers exactly the same operation on a vector $x$ as $f$ does with $x$ not been a coordinate vector. If we however say that $A$ is in a basis (by in a basis I mean like the matrix $B$ in theorem 2, so that is represents $f$ with respect to one (or two, if the bases of the domain and codomain are different) bases). Then it follows that $x$ also has to be a coordinate vector in this same basis (the basis of the domain). But the linear map $f$ does not act on coordinate vectors, it is not associated with a basis and therefore $x$ is not a coordiante vector. So my conclusion is that: The matrix $A$ is not a representation of $f$ in a particular basis but represents $f$ in general and performing $A$ on an arbitrary vector in $\Bbb{R}^n$ is equivalent to performing the linear map $f$ on it. And both $A$ operate on vectors in $\Bbb{R}^n$ which themselves are not coordinate vectors. So the matrix $A$ is not the same as $B$ i.e. $B$ represents its linear map in with respect to a particular set of basis and acts on coordinate vectors only. Whilst the matrix $A$ is not related to a basis and acts on any vectors, as $f$ does in $\Bbb{R}^n$. Please could you either confirm that this argument/analysis is right or wrong. I am really confused about this topic, so if you could give sources in you questions, this would be a great help. p.s. I note that this question is very similar to my other questions, I have however tried to make it slightly more detailed and asking a different question at the end. I have asked this one since I am still confused about the subject and want a detailed answer, preferably backed up by sources.","['matrices', 'linear-algebra']"
1070306,Directed Multigraph or Directed Simple Graph?,"I have the following two questions in my book: Question # 1 Determine whether the graph shown has directed or undirected edges, whether it has multiple edges, and whether it has one or more loops. Because Graph (7) has multiple edges (as the book says ""A Directed graph may have multiple directed edges from a vertex to a second (possibly the same) vertex are called as directed multigraphs"") and it also has loops at vertex c and e. Similar is the case with Graph (9). They should both be Directed Multigraphs but the book says that Graph(7) is a directed graph only and Graph (9) is a Directed Multigraph. Why Graph(7) is only a directed graph instead of a directed multigraph? Question # 2 Describe a graph model that represents whether each person at a party knows the name of each other person at the part. Should the edges be directed or undirected? Should multiple edges be allowed? Should loops be allowed? I think the graph should be directed because its not necessary that if A knows the name of B then B would also the know the name of A. Moreover, because of this reason I think that the graph should have multiple edge but the answer at the back of the book is different. The book says that the the graph should be directed but it should not have multiple edges. Why is that? Thanks!","['graph-theory', 'multigraphs', 'discrete-mathematics']"
1070318,Probabilistic implications of the existence of non-measurable sets,"Measure theory and probability theory are deeply connected through the interpretation of subset measures on the sample space as probabilities of events. A major (and somewhat disturbing) result from measure theory is the existence of non-measurable sets, even in concrete cases such as that of the Lebesgue measure on $\mathbb R$ . What is even more disturbing is that the existence of such sets appears conditional on us believing in the Axiom of Choice (see Solovay model ). Probability theory usually avoids this problem by choosing event algebras that ensure measurability (such as the Borel algebra if the sample space is a subset of $\mathbb R^n$). Does, then, the existence of non-measurable sets have any implications on probability at all? I'm especially interested in the following question: Assuming a model of set theory that does not include the Axiom of Choice (where it is not possible to prove the existence of non-measurable subsets of the reals), we should be able to explore probability spaces $(\mathbb R,2^{\mathbb R},P)$ instead of being limited to just $(\mathbb R,B(\mathbb R),P)$, that is, use the full power set as an event space in place of the Borel algebra. From a probabilistic point of view, what would that mean?","['measure-theory', 'probability-theory', 'soft-question', 'axiom-of-choice', 'lebesgue-measure']"
1070330,How many ways can seven people sit around a circular table?,"How many ways seven people can sit around a circular table? For first, I thought it was $7!$ (the number of ways of sitting in seven chairs), but the answer is $(7-1)!$. I don't understand how sitting around a circular table and sitting in seven chairs are different. Could somebody explain it please?",['combinatorics']
1070344,Using numerical methods to calculate integral,"$$
\mbox{How can I go about calculating}\quad
\int_{0}^{\infty}\,{\rm e}^{-100\,x^{2}}\,{\rm d}x\quad 
\mbox{to}\ {\sf\mbox{five}}\ \mbox{decimal places of accuracy ?.}
$$ Do I use Simpson's Rule ?. If so, wouldn't calculating the fourth derivative be a pain, and what about maximums ?.","['integration', 'numerical-methods']"
1070359,How do I prove this function doesn't exist?,"Let $g: S\rightarrow S$ be a function such that $g$ has exactly two fixed points, and $g\circ g$ has exactly four fixed points. Prove that there is no function $f:S\rightarrow S$ such that $g=f\circ f$ I have literally no idea where to start, any hints or help?",['functions']
1070361,Is Cantor's diagonal argument dependent on the base used?,"Applying Cantor's diagonal argument to irrational numbers represented in binary, one and only one irrational number can be generated that is not on the list. Wikipedia image: But if you change the base from 2 to 3 or higher, including base-10, there are an infinite number of irrational numbers that can be generated that are not on the list, through various combinations of digits at tenths, hundredths, thousandths, etc places. How can this be possible? Bases are just representations of a number, not a number in themselves. But binary produces fewer number of uncountable irrationals than ternary or quaternary. Please correct me if I've gone wrong somewhere.","['discrete-mathematics', 'fractions', 'elementary-set-theory', 'real-numbers', 'binary']"
1070382,"Probability of $m$ out of $n$ rolls of a die being among the numbers $1,2,\ldots,m-1$, for some $m$.","Suppose I have a $k$ sided die with the numbers $1,2,\ldots,k$ on each side, and that I roll it $n$ times ($n<k$). What is the probability that there exists an $m\leq n$, so that $m$ of the $n$ rolls lie in the set $\{1,2,\ldots,m-1\}$? If a closed form in terms of $k,n$ cannot be easily found, a recursion would be equally useful, so it can be more easily calculated. I have tried calculating this for specific values of $n$ and $k$, but it is difficult, because the two events corresponding to distinct values of $m$ are not mutually exclusive, so you can't just calculate the probability of the event occurring for each value of $m\leq n$, and add them up. This means that copious use of the Principle of Inclusion-Exclusion is required, and it gets messy very quickly.","['inclusion-exclusion', 'probability', 'combinatorics']"
1070399,Inequality with summation of cosine terms $\left|1 + 2\sum_{j=1}^k \cos (\frac{2\pi n}{q}j) \right| \leq 1 + 2\sum_{j=1}^k \cos (\frac{2\pi }{q}j)$,"I got stuck on the following problem:
Let $q\in \mathbb{N}$ be a fixed odd number and $k,n \in \{ 1,…,\frac{q-1}{2}\}$. 
I want to show that
$$ \left|1 + 2\sum_{j=1}^k \cos (\frac{2\pi n}{q}j) \right| \leq 1 + 2\sum_{j=1}^k \cos (\frac{2\pi }{q}j).$$
By simple manipulations one can see that this is equivalent to
$$\left| \frac{\sin(\frac{2k+1}{q}\pi n)}{\sin(\frac{\pi}{q}n)} \right| \leq \frac{\sin(\frac{2k+1}{q}\pi )}{\sin(\frac{\pi}{q})}.$$
Any ideas?","['trigonometry', 'inequality']"
1070405,Resolvent operator,"Let's consider the following operator on $L^2(\mathbb{R}^3)$
$$A(t)=\Delta+b(t,x)\cdot\nabla$$
where $\Delta$ is the Laplace operator and $b(\cdot,\cdot)$ a smooth vector field.
How to compute the resolvent operator?","['operator-theory', 'functional-analysis', 'differential-operators', 'analysis']"
1070406,On a method to compute dimension of moduli space of Riemann surfaces,"Consider genus $g$ Riemann surfaces, and thier moduli space $\mathcal M_g$. To determine dimension of $T\mathcal M_g$,
start with a complex structure, which in some coordinates can be written
$$J=\begin{pmatrix} i & 0 \\ 0 & -i \end{pmatrix}$$
and consider small deformations $J \to J+\epsilon$. The condition $(J + \epsilon)^2=-1$ yields
$$\epsilon=\begin{pmatrix} 0 & \epsilon_1 \\ \epsilon_2 & 0 \end{pmatrix}.$$ Now comes the point I don't understand: it is claimed that vanishing of Nijenhuis tensor for $J+\epsilon$ implies $\bar\partial \epsilon_1=0=\partial \epsilon_2$. Can anybody please explain why this follows from $N_{J+\epsilon}=0$ ? (The final part when one applies Riemann-Roch to study dimension of $\epsilon_1$ space called Beltrami differentials is clear to me) What I tried is the following: write
$$ \delta N = [\epsilon X,JY]+[JX,\epsilon Y]-\epsilon[ X,JY]-J[X,\epsilon Y]-\epsilon [JX,Y]-J[\epsilon X,Y]$$
and notice that $J\partial=+i\partial$, $J\bar \partial=-i\bar\partial$ and $\epsilon \partial = \epsilon_2 \bar \partial$, $\epsilon \bar\partial = \epsilon_1 \partial$. Then it is enough to evaluate the tensor for $X=\partial=Y$ and $X=\bar\partial$, $Y=\partial$, and the other two, but all these give zero, not the desired above conditions. Also the above derivation smells a bit, since we know that in $\dim_\mathbb C \Sigma=1$ there is no obstruction to the deformation of the complex structure, so I would expect that the conditions $\bar \partial \epsilon_1=0=\partial\epsilon_2$ follow from just $\epsilon$ being off diagonal. Reference: Collinucci's lectures page 25. Note this is crossposted on mo","['complex-geometry', 'differential-geometry', 'algebraic-geometry', 'moduli-space', 'riemann-surfaces']"
1070424,(Though?)Expression Rearranging,"I have the following expression $
2x+3x^2+e^{5x+x^2}=7
$ which I need rearranged in a form of the type $Ye^Y=Z$ with Y a function of x and Z some constant.
I have tried the substitution $y=5x+x^2$, that is I replaced x by the roots of that equation. For the root $x=\frac{-5+\sqrt{25+4y}}{2}$ I get $e^y+11\sqrt{25+4y}+4y+38=0$ and I don't see how to get to the above mention form from here. Any idea about how I should tackle this? I don't know if the Taylor expansion of the exponential could help, can you see any link.Thank you.","['taylor-expansion', 'quadratic-forms', 'complex-analysis', 'polynomials']"
1070433,Discrete analogue of Green's theorem,"Following formula concerning finite differences is in a way a discrete analogue of the fundamental theorem of calculus: $$\sum_{n=a}^b \Delta f(n) = f(b+1) - f(a) $$ We can think about the Green's theorem as a two-dimensional generalization of fundamental theorem of calculus, so I'm interested is there a discrete analogue of Green's theorem?","['discrete-mathematics', 'integration', 'soft-question']"
1070464,Find the fundamental period,"How do I find the fundamental period of this function? $$y = \sin x + \cos(1,01x)$$ I know that the fundamental period of  $\sin x$ is $2\pi$ and the fundamental period of $cos(1,01x)$ is $\frac{2\pi}{1,01}$, but what can I do then?","['trigonometry', 'periodic-functions']"
1070489,trying to understand what a polynomial ring is,"I don' really understand what a polynomial ring is, maybe because the lack of examples. Consider for example the polynomial ring $\mathbb{Z}[x]$. Can you please tell me how this polynomial ring (its elements) looks like? How is x defined, is $x \in \mathbb{Z}$? What are the two operations of the ring?","['ring-theory', 'abstract-algebra']"
1070493,Solving $y^{(n)}(t)=f(t); t>0$ with initial conditions,"I will use the notation $\frac{d^n y}{dt^n} \equiv y^{(n)}$. How do I solve this ODE? $$y^{(n)}(t)=f(t); t>0;\\
y(0)=y_0, y'(0)=y_1, ..., y^{(n-1)}(0)=y_{n-1}$$ What I did: The ODE is in the form $F(t,y^{(n)})=0$ so I should look for parametric solution
$\begin{cases}
t=\phi(\tau)\\
y^{(n)}=\psi(\tau)
\end{cases}$ so $y^{(n-1)}=\int\psi(\tau)\phi'(\tau)d\tau$ and the order is reduced by one. In my problem, let's choose $\phi(\tau)=\tau; \psi_n=f(\tau)$ and $y^{(k-1)}=\psi_{k-1}(\tau)=\int \psi_k(\tau)d\tau$ where $k=1,2,...,n$ How do I apply the initial conditions here? There is also an answer which reads $$y(t)=\frac{1}{(n-1)!}\int_0^t (t-\tau)^{n-1}f(\tau)d\tau + \sum_{i=0}^{n-1} \frac{y_i t^i}{i!}$$ How do I get this answer?",['ordinary-differential-equations']
1070523,Topological Hangman,"Suppose a mysterious adversary has captured me and challenged me to the following topological game. We fix some finite set, say, $X = \{1, 2, 3, \ldots, n\}$, and my adversary secretly constructs a topology $T$ on $X$. My job is to determine $T$ by asking my opponent, as many times as I wish, for the boundary $\partial S$ in $T$ of any subset $S \subset X$, but for every answer I receive, a portion of my anatomy is forcibly removed. I cannot answer until I am absolutely sure of $T$ (no guessing!), and I will be freed as soon as I answer correctly. Question: What strategy should I use to maximize the number of body parts I expect to walk away with? The case $n=1$ is trivial, as there is only one topology on a singleton. In the case $n=2$, my strategy is particularly simple: I ask for the boundary of $\{1\}$. There are four possible boundaries, each uniquely determining $T$: If $\partial \{1\} = \emptyset$, then $T$ is the discrete topology on $X$. If $\partial \{1\} = X$, then $T$ is the indiscrete topology on $X$. If $\partial \{1\} = \{1\}$, then $T = \{\emptyset, \{2\}, X\}$. If $\partial \{1\} = \{2\}$, then $T = \{\emptyset, \{1\}, X\}$. In any case, I lose just one body part. I have determined via computer search that for $n=3$, it suffices to check
$$ \{1\}, \{1,3\}, \{3\} $$
and for $n=4$, 
$$ \{1\}, \{1,2,4\}, \{1,3,4\}, \{4\} $$
though I am not sure if these are optimal, and I do not know if the pattern generalizes.",['general-topology']
1070524,Calculate $\lim_{x \to 0} (e^x-1)/x$ without using L'Hôpital's rule [duplicate],This question already has answers here : Proving that $\lim\limits_{x \to 0}\frac{e^x-1}{x} = 1$ (8 answers) Closed 8 years ago . Any ideas on how to calculate the limit of $(e^x -1)/{x}$ as $x$ goes to zero without applying L'Hôpital's rule?,"['limits-without-lhopital', 'exponential-function', 'calculus', 'limits']"
1070527,"Compute the fourier coefficients, and series for $\log(\sin(x))$","I posted a similar question with a bad response, so I am retrying with hopes of better knowledge. The fourier series is in the form: $$f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} a_n\cos(nx) + \sum_{n=1}^{\infty} b_n\sin(nx)$$ Where: $$a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\cos(nx) dx$$ $$b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\sin(nx) dx$$ The problem is computing the coefficients. The goal of trying to derive the series is to solve: $$\int_{0}^{\pi} \log(\sin(x)) dx$$ $\displaystyle a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} \log(\sin(x))\cos(nx) dx$ Which is very difficult to compute. What can be done? The series representation is: $$\log(\sin(x)) = -\log(2) - \sum_{k=1}^{\infty} \frac{\cos(2kx)}{k}$$","['fourier-series', 'sequences-and-series', 'calculus', 'integration', 'real-analysis']"
1070531,Building a 3D matrix of positive integers,"I'm trying to build a 3D matrix made up of positive integers that has very specific properties. The matrix dimensions are $N \times N \times (N+1)$ where $N$ is a positive integer. The matrix has two properties: Every one of the $(N+1)$ ""slices"" of size $N \times N$  of the matrix contains each of the numbers $1$ through $N^2$ exactly once. Assuming we look at each such slice as made up of rows and columns, then if we pick any two rows belonging to different slices, they have to have exactly one number in common (due to property 1, it might be sufficient to impose that the two rows have at most one number in common and still get property 2). Any ideas on whether this is easy or hard to achieve, and in the former case, what would be a way to achieve it?","['combinatorial-designs', 'combinatorics']"
1070539,Handling integrals of trig functions,"I'm not sure how to handle the following class of integrals: $I=\int_0^{2\pi}f(\cos(\theta))d\theta$ If I make the change of variables $x=\cos(\theta)$ the new limits of the integral are the same, giving $I=0$. I know this is incorrect, as it fails for the case $f(\cos(\theta))=\cos^2(\theta)$, for example. Can anybody tell me where the inconsistency is here, please? My guess is that it comes down to $\theta=\arccos(x)$ not being well defined, or something of that nature.","['definite-integrals', 'trigonometry', 'integration']"
1070561,"If $f(0) = f(1)=0$ and $|f'' | \leq 1$ on $[0,1]$, then $|f'(1/2)|\le 1/4$","Let $f :  [0,1] \rightarrow \mathbb{R}$ be a function whose second order derivative $f''(x)$ is continuous on $[0,1]$. Suppose that $f(0) = f(1)=0$ and that $|f''(x)| \leq 1$ for any $x  \in [0,1]$. Prove that $|f'(\frac{1}{2})| \leq \frac{1}{4}$. I think we need to use the mean value theorem, and I have proven that $|f(x)| \leq \frac{1}{8}$ for any $x \in [0,1]$. I'm not sure how to proceed, though. Could someone please help? Thanks!","['inequality', 'derivatives', 'real-analysis']"
1070565,Proving that $\left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) \lvert f(z) \rvert^2 = 4 \lvert f'(z) \rvert^2$,"Given $f$ entire show that
$$
\left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) \lvert f(z) \rvert^2 = 4 \lvert f'(z) \rvert^2
$$
I've come close to getting the exact answer by writing $f(z)$ as $u(x,y)+iv(x,y)$ and realizing the Laplacian of f is equal to zero. This leads to a lot of cancellations but I'm still making mistakes in my computations and I can't figure out where. Any help is appreciated.","['multivariable-calculus', 'laplacian', 'partial-derivative', 'complex-analysis', 'derivatives']"
1070572,Evaluate $\int_1^\infty \frac {dx}{x^3+1}$,"I would like some help with the following integral. I would like to find a contour line to evaluate $$\int_1^\infty \frac {dx}{x^3+1}$$ So one can see that on any circumference it goes to $0$, but how do I connect it to $1$? I have tried a number of things but without results. Can somebody offer any help?","['improper-integrals', 'integration', 'definite-integrals', 'complex-analysis', 'contour-integration']"
1070574,"No real $x,y$ such that $(x+y)^2+(x-2)^2+(y-2)^2=4$","Here's the context of this problem. Solve: $x^2=y^3-3y^2+2y$ $y^2=x^3-3x^2+2x$ We subtract the second equation from the first and obtain $$(x-y)(x^2+y^2+xy-2x-2y+2)=0$$ The first factor yields the solutions $0,2\pm\sqrt2$ and the second one can be rewritten as $$(x+y)^2+(x-2)^2+(y-2)^2=4$$ I have checked Wolfram Alpha and it tells me that there are no more real solution to the above system.This means that the above equation has no solution.Now I have to prove it. Obviously,none of the expressions $(x+y)^2,(x-2)^2,(y-2)^2$ can be $4$ otherwise the LHS would be larger than $4$ . Hence,we have the inequalities $$|x+y|<2$$ $$|x-2|<2$$ $$|y-2|<2$$ which(unless I am mistaken) yield $$0<x<2$$ $$0<y<2$$ I cannot proceed any further.Perhaps there is some clever way to write the whole expression solely using squares but I cannot see it immediately. I would appreciate it if the method used to solve this is elementary.Some help will be appreciated.",['algebra-precalculus']
1070575,Prove that $c_n = \frac1n \bigl(\frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}} \bigr)$ converges,"I want to show that $c_n$ converges to a value $L$ where: $$c_n = \frac{\large \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}}}{n}$$ First, it's obvious that $c_n > 0$. I was able to show using the following method that $c_n$ is bounded: $$c_n = \frac{\large \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}}}{n} < \overbrace{\frac{\large \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{2}} + \cdots + \frac{1}{\sqrt{2}}}{n}}^{n-1 \text{ times}} = \frac{\large {n - 1}}{n\sqrt{2}} < \frac{1}{\sqrt{2}}$$ So now we know that $\large {0 < c_n < \frac{1}{\sqrt{2}}}$. I know from testing for large values of $n$ that $c_n \to 0$. What's left is actually finding a way to show this. Any hints?","['convergence-divergence', 'sequences-and-series', 'limits']"
1070606,How to prove that a bijective transformation is NOT continuous,"I am having this transformation $f: \mathbb R \to \mathbb R$ $$f(x) = \begin{cases}
x  & x \in \mathbb R \setminus
 \mathbb Q
\\x+1 & x \in \mathbb Q 
\end{cases}$$ I've already prooved that this transformation is bijective. How can I proove that the transformation is NOT continous in every point $x \in \mathbb R $","['transformation', 'continuity', 'real-analysis', 'analysis']"
1070611,pullback of twisting sheaf,"Let $[k]: \mathbf{P}^n \to \mathbf{P}^n, [x_0:\ldots:x_n] \mapsto [x_0^k:\ldots:x_n^k]$ be a morphism. (Why) do we have $[k]^*\mathcal{O}_{\mathbf{P}^n}(1) \cong \mathcal{O}_{\mathbf{P}^n}(k)$?",['algebraic-geometry']
1070618,"Is this an identity: $\int_X f \, d\mu=\int_{\mathbb{R}} \mu(f^{-1}(t)) \, dt$?","Let $(X,\mu)$ be a measure space and $f:X \to \mathbb{R}$ an integrable function. Does the following always hold? $$\int_{X}f\,d\mu=\int_{\mathbb{R}}\mu(f^{-1}(t))\,dt$$","['lebesgue-measure', 'integration']"
1070645,"Extending the zeta function to semiprimes, etc.","The Riemann Zeta function is defined for $s > 1$ as \begin{align}
&\prod _{n=1}^{\infty}\dfrac{1}{1 -\ p_{n}^{\ \ -s}}\\
\end{align} It is possible to extend the zeta function to semiprimes with \begin{align}
&\prod _{n=1}^{\infty}\dfrac{1}{1 -\ q_{n}^{\ \ -s}}\\
\end{align} where $q$ runs through the semiprimes $4,6,9\ldots$ and similarly, for all almost primes. Below is a plot of the zeta functions for the k-almost primes $1\leq k\leq 6$: I suspect these functions are rather difficult to approach analytically. Interestingly though, it appears that $\zeta(2s)$ bounds the semiprime zeta function from below: and so on, though the bound gets progressively weaker ... is this likely to be a true statement? \begin{align}
&\prod _{i=1}^n \frac{1}{\left(1-p_i{}^{1-c s}\right)^{c}}
\end{align} with $c=e+k-2$ is closer (though not a lower bound). For $k=2$ (ie semiprime zeta): The reciprocals of k-almost primes have been studied by Richard J. Mathar (given in the link in the OEIS sequence here ), but I am not aware of any references to the extended zeta functions as outlined above. I would be grateful if anyone knows of any research in this area, or could shed any further light on these functions.","['semiprimes', 'riemann-zeta', 'zeta-functions', 'number-theory', 'prime-numbers']"
1070655,Decomposition of abelian varieties up to isogeny,"Let $A_1,A_2,B_1,B_2$ be simple abelian varieties over a number field $k$. Suppose that $A_1\times A_2$ is $k$-isogenous to $B_1\times B_2$. Can we deduce that (up to reordering the factors) $A_1$ is $k$-isogenous to $B_1$ and $A_2$ is $k$-isogenous to $B_2$?","['abelian-varieties', 'algebraic-geometry', 'number-theory']"
1070675,Calculating canonical divisor in product of projective spaces.,"Let $X$ be an intersection of two divisors of bidegree $(a,b)$ and $(c,d)$ in $\mathbb{P^2}\times \mathbb{P^2}$. Then how can I find the canonical divisor $K_X$? I'm asking because I have no experience in working with bidegrees.","['intersection-theory', 'algebraic-geometry']"
1070676,Arnold ODE Problem,"Problem 1 of Section 1.2.4 of Arnold's ODE book asks Can the integral curves of a smooth (continuously differentiable) equation $\frac{dx}{dt} = v(x)$ approach each other faster than exponentially as $t\rightarrow \infty$ ? It says that the answer is no when one of the curves corresponds to an equilibrium position but is yes otherwise. I interpret this to mean that for two integral curves $x_1, x_2$ defined for all values of $t$ larger than some constant, $\lim_{t\rightarrow \infty} e^t |x_1(t) - x_2(t)| = 0$ . But I can't think of an example of a ODE with solutions like that.  I tried the following ODEs $dx/dt = 1$ .  This had solutions that maintained constant distance from each other $dx/dt = x^2$ . The solutions to this ODE approach 0 for t large but their difference is on the order of $1/t^2$ . $dx/dt = -e^t$ .","['ordinary-differential-equations', 'real-analysis']"
1070727,Product of permutations of consecutive numbers yields arithmetic sequence,"Let $n\geq 3$ be an integer, and $a,b$ be positive integers. Let $c_1,\ldots,c_n$ be a permutation of $a,a+1,\ldots,a+(n-1)$, and $d_1,\ldots,d_n$ be a permutation of $b,b+1,\ldots,b+(n-1)$. Is it possible that $c_1d_1,c_2d_2,\ldots,c_nd_n$ form an arithmetic sequence? For example, for $n=3$, this is not possible. The permutations we need to check are $ab,(a+1)(b+1)=ab+a+b+1,(a+2)(b+2)=ab+2(a+b)+4$ $ab,(a+1)(b+2)=ab+2a+b+2,(a+2)(b+1)=ab+a+2b+2$ and so on. However, for general $n$ it is harder to check all permutations.","['arithmetic', 'permutations', 'sequences-and-series']"
1070728,Why does the union of all open null sets is itself a nullset for second countable space?,"On the online Encyclopedia of mathematics, it is written  ""The existence of a countable base guarantees that the union of all open μ-null sets is itself a nullset.""
See: https://www.encyclopediaofmath.org/index.php/Support_of_a_measure I do not understand this result, i.e., I do not manage to prove it. I think that I have a sketch of a proof, but I have not been able to complete it. See below. Any proof/explanation is welcome. Let $(X,\mathcal{B}, \mu)$ be a second countable measure space, and $\mathcal{O}:=\bigcup\{O \subset X: O  \text{ open and }\mu(O)=0\} $. $\mathcal{O}$ is an open set as a union of open sets. Thus, by second countability of $X$, $\mathcal{O} $ can be written as a countable union of open sets extracted from a countable base of $X$, i.e.,   $\mathcal{O}=\bigcup_{n\in \mathbf{N}} O_{\alpha(n)}$, where $\alpha: \mathbf{N} \rightarrow \mathbf{N} $ is an increasing function, and $\{O_n\}_{n \in \mathbf{N}} $ is a base of $X$. Therefore, by $\sigma$-additivity of measures, 
\begin{eqnarray*}
\mu(\mathcal{O})=\mu(\bigcup_{n\in \mathbf{N}} O_{\alpha(n)})\leqslant \sum_{n \in \mathbf{N}}\mu(O_{\alpha(n)})
\end{eqnarray*}
Then, to conclude $\mu(\mathcal{O})=0 $, I would probably like to say that, for all  $n \in \mathbf{N} $, $\mu(O_{\alpha(n)}) =0$. But I do not see why it should be the case... Answer by David Mitra (as far as I understand it): Index the set of open null sets  by $i \in I $, i.e., define $I$ s.t. $\{U_i \}_{i \in I}:=\{O \subset X: O  \text{ open and }\mu(O)=0\} $. By second countability, for all $i \in I $, $U_i=\bigcup_{n \in \mathbf{N}}  O_{\alpha_i(n)}$ where $\alpha_i: \mathbf{N} \rightarrow \mathbf{N} $ is an increasing function, and $\{O_n\}_{n \in \mathbf{N}} $ is a base of $X$.  Now, note that by monotonicity of measures,  for all $i \in I $, and $n \in \mathbf{N} $, $\mu(O_{\alpha_i(n)})\leqslant \mu(U_i)=0 $, which implies that
\begin{eqnarray}
\mu(O_{\alpha_i(n)})=0 \qquad (1)
\end{eqnarray} Moreover, the union of all open null sets can be expressed as
\begin{eqnarray}
\mathcal{O} &= &\bigcup_{i \in I}U_i=\bigcup_{i\in I} \bigcup_{n\in\mathbf{N}}O_{\alpha_i(n)}\\
& = &\bigcup_{m \in \mathbf{N}} O_{\beta(m)}
\end{eqnarray}
where for all $m \in \mathbf{N} $, there exits $n \in \mathbf{N} $ and $i \in I $ s.t. $\beta(m)=\alpha_i(n) $. Then, using (1), the completion of the proof above follows 
\begin{eqnarray*}
\mu(\mathcal{O})=\mu(\bigcup_{m\in \mathbf{N}} O_{\beta(m)})\leqslant \sum_{m \in \mathbf{N}}\mu(O_{\beta(m)})=0
\end{eqnarray*}
Thanks David Mitra!",['measure-theory']
1070731,Pretty Simple Integral,"I am trying to find the following indefinite integral:
$$
\int \sqrt{x^2+x^4}dx
$$ At first, it seems like an easy u-substitution after we factor out an $x^2$ from the square root, but when we do that we are making the implicit assumption that $x \in \mathbb{R}^+$ or zero. Would the correct way be to do the two cases, when x is positive and x is negative, or is there a way to step around that assumption and arrive at one answer for both?","['calculus', 'integration']"
