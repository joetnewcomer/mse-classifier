question_id,title,body,tags
290798,How to understand the Second covariant derivative?,"I am reading P. Li's lectures on Geometric analysis. On page 14, the author defines the second covariant derivative as follows: Let $f$ be a smooth function on $M$. $\omega_1, \cdots, \omega_n$ be a local orthonormal basis of $T^*M$ around a fixed point $p$. And $d\omega_i=\sum_j\omega_{ij}\wedge\omega_j$. Then $$df=\sum_i f_i\omega_i$$
THen the author give the definition:
$$
f_{ij}\omega_j=df_i+f_j\omega_{ji}.
$$ My question is how to understand this definition compare with the usual one, i.e.
$$
\nabla^2_{X,Y}f=XY(f)-\nabla_XY (f)
$$ Also the third order covaiant derivative defines in the similar manner:
$$
f_{ijk}\omega_k=df_{ij}+f_{kj}\omega_{ki}+f_{ik}\omega_{kj}
$$
The similar question for this expression, how to understand these indices? Thank you for any detailed answer, I am kind of afraid of this local calculations compare with the ususal global definition. However, in most papers, the local calculations are more common. So I want to figure it out clearly.","['riemannian-geometry', 'differential-geometry']"
290801,Does $a!b!$ always divide $(a+b)!$,"Hello the question is as stated above and is given to us in the context of group theory, specifically under the heading of isomorphism and products. I would write down what I have tried so far but I have made very little progress in trying to solve this over the last few hours!","['factorial', 'group-theory']"
290808,the definition of Riemann zeta function,"$\alpha^z=e^{z\log\alpha}$ is multi-valued. Now I am  confused with the definition of Riemann zeta function:
$$\zeta(s)=\sum_{n=1}^{\infty}\frac1{n^s}, s=\sigma+it$$
because 
$$n^s=e^{s(\log n+i2k\pi)}$$ 
where $\log n $ is the natural logarithm, then, Is $\zeta(s)$  a  multi-valued function? or, we should think $n^s=e^{s\log n}$ ? many thanks!",['complex-analysis']
290836,Finding an example which is Gâteaux-differentiable in one point but not continuous in this point,"Find an example for a $f\colon X\to Y$ which is Gâteaux-differentiable in a point $x_0$ but not continuous in this point $x_0$. I am not good in finding examples but I thought of
$$
f\colon\mathbb{R}\to\mathbb{R}, f(x):=\begin{cases}0, & x\leq 1\\1, & x>1\end{cases}
$$
I guess on the one hand this function is not continuous in $x_0=1$ but the Gâteaux derivative
$$
\eta=Df(1)[h]=\lim\limits_{t\to 0}\frac{f(1+th)-f(1)}{t}=0
$$
exists to my opinion. Is this an appropriate example? Edit Isn't the function
$$
F\colon\mathbb{R}\to\mathbb{R}, F(x):=\begin{cases}1, & x\neq z\\0, & x=z\end{cases}
$$
Gâteaux-differentiable in $z$ and not continuous in $z$?",['derivatives']
290853,Inverse of a diagonal matrix plus a Kronecker product?,"Given two matrices $X$ and $Y$, it's easy to take the inverse of their Kronecker product: $(X\otimes Y)^{-1} = X^{-1}\otimes Y^{-1}$ Now, suppose we have some diagonal matrix $\Lambda$ (or more generally an easily inverted matrix, or one for which we already know the inverse).  Is there a closed-form expression or efficient algorithm for computing $(\Lambda + (X\otimes Y))^{-1}$?","['tensor-products', 'linear-algebra', 'inverse']"
290872,Cardinality of the set of clopen subsets of a topological space,"Is there some way to find the cardinality of set of all clopen subsets of a topological space, say, Cantor space, Baire space?","['general-topology', 'cardinals']"
290884,Finite unions and intersections $F_\sigma$ and $G_\delta$ sets,"Why is the intersection of finitely many $F_\sigma$ sets an $F_\sigma$ set, and the union of finitely many $G_\delta$ sets a $G_\delta$ set?","['general-topology', 'real-analysis']"
290887,On a theorem on Lie derivatives,"I am a little confused proving this theorem (for $p$-forms on $M^n$, $M^n$ is a smooth manifold):
$L_X\cdot i_Y-i_Y\cdot L_X=i_{[X,Y]}$ where $X,Y$ are smooth vector fields. Now it is clear that both sides are anti-derivations, since $L_X$ is a derivation, $i_Y$ and $i_{[X,Y]}$ are anti-derivations. Now this reduces the problem to proving that the theorem is true for $f\in C^\infty$ and $df$ for such $f$. It seems to me the the L.H.S is zero for both $f$ and $df$. For $f$: L.H.S $=0$ is trivial since $i_Y$ for any zero-form is $0$ for $df$ $$i_Y \, df=df(Y)
\implies L_X(i_Y \, df)=L_X(df(Y))$$ And $$L_X(df)=dL_X(f)
\implies i_Y(L_X(df))=L_X(df)(Y)$$ Hence L.H.S. $=0$ Have I made an error here ? Thanks for any help or pointers.","['lie-groups', 'differential-geometry']"
290903,Difference between a Gradient and Tangent,"I am unable to understand the fundamental difference between a Gradient vector and a Tangent vector.
I need to understand the geometrical difference between the both. By Gradient I mean a vector $\nabla F(X)$ , where $ X \in  [X_1 X_2\cdots X_n]^T  $ Note: I saw similar questions on ""Difference between a Slope and Gradient"" but the answers didn't help me much. Appreciate any effort.","['geometry', 'calculus']"
290910,Geodesics on a polyhedron,"Which sequences of adjacent edges of a polyhedron could be considered to be a geodesic? The edges of a face most surely will not, but the ""equator"" of the octahedron eventually will. But for what reasons? How do the defining property of a geodesic - having zero geodesic curvature - apply to a sequence of edges? (One crude guess: any sequence of edges that pairwise don't share a face? What does this have to do with curvature?)","['polyhedra', 'differential-geometry']"
290912,$4494410$ and friends,"The number $4494410$ has the property that when converted to base $16$ it is $44944A_{16}$, then if the $A$ is expanded to $10$ in the string we get back the original number. $3883544142410_{10}=3883544E24A_{16}$ is another. These numbers are in OEIS A187829 . They come in blocks of $6$ or $10$, depending on whether the one's digit in hex is $A-F$ or $0-9$. I suspect the list is complete but have not proven it. The largest is $806123145829415507126939101294137128298625241370656314360169_{10}=\\806C3E58294F507C6939AC94D7C829862524D706563E360169_{16}$ If the number has $m$ hex digits and $n$ base $10$ digits, we must have $16^{m-1} \gt 10^{n-1}$ and $16^{m-2} \lt 10^{n-2}$ which leads the hunt to $m=6,n=7;\ m=11,n=13;\ m=16, n=19;\ m=50,n=60$ guided by the convergents of $\frac {\log 16}{\log 10}$. We can view finding these numbers as finding solutions to the subset-sum problem, where each hex digit contributes the difference between its value in base $16$ and base $10$ (depending on how many base $16$ digits to the right are $A-F$ and counting the two base $10$ digits coming from one hex digit together). The sum then has to be zero. My search program ran reasonably quickly even for the next convergent, $m=535, n=644$ and didn't find any.  I believe they just have too many ways to fail as the number gets long. Can we prove that there are no more, or at least that there are no more with very high probability, in the sense of ""proofs"" of Goldbach that if the primes are ""random"" the chance of any large even number having no solution is very low?","['elementary-number-theory', 'arithmetic', 'algebra-precalculus', 'recreational-mathematics']"
290917,Rabbits drinking bottles of poisoned water (combinatorics problem),"Suppose we have $n$ bottles of water, one of which is poisoned and which we want to identify. A mixture can be made by mixing some number of bottles of water. We have $k$ rabbits, and each day, each of the rabbits can  be given a mixture - a rabbit dies if the mixture contains any poison. How many days are necessary to find out which bottle is poisoned? A similar, but slightly different problem, is discussed here: Logic problem: Identifying poisoned wines out of a sample, minimizing test subjects with constraints",['combinatorics']
290948,Contest Math Geometry,"I'm currently prepping for some high school math competitions soon, and I was wondering if anyone knows any resources that are out there with an abundance of contest-math-related geometry problems. Geometry is definitely my weak point in contest math, and any input would be appreciated. Thanks!","['geometry', 'contest-math']"
290971,Left topological zero-divisors in Banach algebras.,"Let $ A $ be a unital Banach algebra. Define $ \zeta: A \longrightarrow [0,\infty) $ by
$$
\forall a \in A: \quad \zeta(a) \stackrel{\text{def}}{=} \inf_{b \in \mathbb{S}(A)} \| ab \|,
$$
where $ \mathbb{S}(A) $ denotes the unit sphere of $ A $. Definition An element $ a \in A $ is called a left topological zero-divisor iff $ \zeta(a) = 0 $, or equivalently, iff there exists a sequence $ (b_{n})_{n \in \mathbb{N}} $ in $ \mathbb{S}(A) $ such that $ \displaystyle \lim_{n \to \infty} a b_{n} = 0 $. (a) Prove that a left topological zero-divisor is not invertible. (b) Prove that $ \zeta: A \longrightarrow [0,\infty) $ is continuous. (c) Let $ a \in \partial(G(A)) $, the boundary of $ G(A) $. Prove that there exists a sequence $ (\nu_{n})_{n \in \mathbb{N}} $ of invertible elements in $ A $ such that $ \displaystyle \lim_{n \to \infty} \nu_{n} = a $ and $ \displaystyle \lim_{n \to \infty} \| \nu_{n}^{-1} \| = \infty $. Note: $ G(A) \stackrel{\text{def}}{=} \{ a \in A \mid \text{$ a $ is invertible} \} $.","['banach-algebras', 'general-topology', 'continuity', 'topological-vector-spaces', 'functional-analysis']"
290975,Estimate on the norm of a self-adjoint operator,"EDIT: thks to Martin's comment I realize the previous version was wrong. Here is the correct version of what I need to show: I am trying to show that if $A$ is a self - adjoint operator in a Hilbert space $H$ then 
$$
\|A\| \le \sup_{\|x\| = 1}   |\langle x, Ax \rangle|
$$
I am given the fact that whenever $\|x\| = \|y\| = 1$ we have
$$
|\langle x,Ay\rangle| \le \sup_{\|x\| = 1} \langle x,Ax \rangle.
$$ 
I am really stuck with this one, any bhint would be highly appreciated, many thanks !!","['operator-theory', 'hilbert-spaces', 'functional-analysis']"
290986,closest point to on $y=1/x$ to a given point,"I feel like I'm missing something basic - given a point $(a,b)$ how do I find the closest point to it on the curve $y=1/x$? I tried the direct approach of pluggin in $y=1/x$ into the distance formula but it leads to an order-4 polynomial...","['analytic-geometry', 'conic-sections', 'calculus', 'euclidean-geometry']"
290999,Improving one's calculation skills,"I am a first-year graduate student in mathematics. My undergraduate mathematics curriculum did not emphasize ""calculating""; it was a theoretical curriculum in which even a traditional course in multivariable calculus was not ""required"" (a course in differential geometry sufficed). I am training to be a ""hands-on analyst"", if that term makes any sense. For example, I know how to existence and uniqueness of solutions to PDE, but I haven't yet the ""nose"" to compute, to perform certain critical integration by parts, etc. I am starting to realize that theories are built on calculations and certain very interesting techniques in PDE--such as viscosity methods for example--arose from refining one's intuition while performing calculations. This is very inspiring for me and I want to learn to calculate! Calculating has been an acquired taste for me, and as a ""hands-on analyst"", I would like to work in PDE and variational problems where one is interested in producing sharp bounds, etc. (this is vague, I know). I am wondering if anyone can suggest any references/ workbooks where I can refine my ""computation"" skills. For example, I heard that the physicist Lev Landau gave his prospective students a preliminary test in integration. I suspect I will not pass such a test at this moment, but I would like to try to get myself to a stage where I can. Is there perhaps (a Russian?) text that emphasizes computation and serves as a good workbook for refining one's computation/calculation abilities. Much thanks in advance!","['calculus', 'reference-request']"
291020,Does Riemann integrable imply Lebesgue integrable?,"Suppose a definite integral exists in the Riemann sense. Does that mean the integral exists as a Lebesgue integral, and do we get the same result either way? ------- BTW: I have a MS in Electrical Engineering and a strong interest in math. I had one semester of real analysis 25 years ago, I tried to learn Lebesgue integration on my own by reading a book on real analysis, and that was a few years ago. Hence, I don't have a solid grasp of the subject.","['lebesgue-integral', 'analysis']"
291026,Probability of getting 5 multiple-choice questions answered correctly,"What is the probability of getting 5 multiple-choice questions answered correctly, if for each question the probability of answering it correctly is 1/3. The answer is 45/118, but I am unsure of how. Update : The book may have had the question worded incorrectly, because the answer stated is incorrect. What i know : Each question has 3 selections, and the probability of getting one wrong is 2/3. I thought it would be as simple as the multiplicative rule, but there's more to it apparently. I think it has to do with a formula as you go along in the questions, but it states that each probability is equal, as a 1/3 chance. Any suggestions?",['probability']
291029,Intersection of Cyclotomic Fields,"How would I prove that $\mathbb{Q}_m \cap \mathbb{Q}_n = \mathbb{Q}_{(m, n)}$ (here $\mathbb{Q}_n$ denotes the $n$ th cyclotomic field)? I already know of a solution involving the fact that given two normal extension fields $M, L$ of some field $K$ contained in some common extension, then $\text{Gal}(ML/L) \cong \text{Gal}(M/M \cap L)$ , but does there exist a solution that doesn't require such a theorem?","['galois-theory', 'abstract-algebra', 'field-theory']"
291038,"If $\kappa (A) > \kappa (B)$, show $\kappa (B^{-1}A) < \kappa (A)$","Let $A$ and $B$ be a toeplitz and symmetric positive definite $NxN$ matrices. If $\kappa (A)  > \kappa (B)$, how to show that:
$$\kappa (B^{-1}A)  < \kappa (A)$$ where $\kappa $(X) is condition number of matrix $X$?","['matrices', 'condition-number', 'toeplitz-matrices']"
291045,Critical paths for length cannot have kinks.,"This problem is in Spivak's Differential Geometry (Ch.9 #37), and he gives a sketch of a proof which I have been unable to finish. So let's compute $\frac{dL(\overline{\alpha}(u))}{du}\mid_{u=0}$  where $L(\overline{\alpha}(u))=L_{0}^{t_{1}}(\gamma)+\int_{t_{1}}^{t_{0}+u}F(\,,\,)dt+\int_{t_{0}+u}^{1}F(\,,\,)dt$ , and of course $F(\alpha(u,t),\frac{\partial\alpha}{\partial t}(u,t))=\sqrt{\underset{i,j}{\sum}g_{ij}(\alpha)\frac{\partial\alpha^{i}}{\partial t}\frac{\partial\alpha^{j}}{\partial t}}$  (all the ""$(u,t)$"" omitted). Well, $\frac{dL(\overline{\alpha}(u))}{du}=\frac{d}{du}\int_{t_{1}}^{t_{0}+u}F(\,,\,)dt+\frac{d}{du}\int_{t_{0}+u}^{1}F(\,,\,)dt$ . Now I apply the Leibniz integral rule, and the terms become $F(\alpha(u,t_{0}+u),\frac{\partial\alpha}{\partial t}(u,t_{0}+u))+\int_{t_{1}}^{t_{0}+u}\frac{\partial}{\partial u}F(\alpha(u,t),\frac{\partial\alpha}{\partial t}(u,t))dt$ and $\int_{t_{0}+u}^{1}\frac{\partial}{\partial u}F(\alpha(u,t),\frac{\partial\alpha}{\partial t}(u,t))dt-F(\alpha(u,t_{0}+u),\frac{\partial\alpha}{\partial t}(u,t_{0}+u))$ , respectively. Evaluating their sum at $u=0$ , we just get $\int_{t_{1}}^{1}\frac{\partial}{\partial u}F(\alpha(0,t),\frac{\partial\alpha}{\partial t}(0,t))dt$ . Note that $\alpha$  is not exactly a variation on $\gamma$  since $\alpha(0,t)$  has a piece of $\gamma$  replaced by a geodesic (*). But anyway, $\alpha$ is  a variation on the piecewise smooth curve $\alpha(0,t)$ , and the integral obtained yields the First Variation Formula for Length of $\alpha(0,t)\mid_{[t_{1},1]} .$ . For the moment ignore *, and assume this is the thing we want to show $\neq0$ . The integral term in the First Variation Formula will disappear, leaving $\left\langle \frac{\partial\alpha}{\partial u}(0,t_{0}),\frac{\partial\alpha}{\partial t}(0,t_{0}^{+})-\frac{\partial\alpha}{\partial t}(0,t_{0}^{-})\right\rangle$  . This is sort of like $\left\langle \frac{\partial\alpha}{\partial u}(0,t_{0}),\Delta_{t_{0}}\frac{d\gamma}{dt}\right\rangle$  , where we know $\Delta_{t_{0}}\frac{d\gamma}{dt}\neq0$ . Assuming everything was correct so far, I have two questions: 1) Why (where) do we need $t_{1}$  to be sufficiently close to $t_{0}$, as hinted by Spivak ? 2) How can I conclude that the inner product term is not 0?","['riemannian-geometry', 'differential-geometry']"
291063,Calculation of an expectation for the 'part' of a vector,"Let $x$ be vector in $R^n$. Let $\pi(⋅)$  be a permutation on the set $\{1,\ldots,n\}$  with a uniform distribution.  Let $|m|\leq n, m \in Z$. Calculate
$$
E\left|\sum_{i=1}^mx_{\pi(i)}\right|^q, \quad q\geq 2.
$$ Thank you.","['statistics', 'probability', 'probability-theory']"
291073,Using Logical Operators in One Line,I am looking for a function that has this meaning: f(x)= if x>10 x+1 else x-1 or f(x)=x>10 : x+1 ? x-1 Similar to the ternary operator in computer programming. What syntax should I use to express this as a mathematical function?,"['logic', 'functions']"
291074,Bounding the injectivity radius from below.,"Let $(M, g)$ be a finite-dimensional Riemannian manifold, and let $S \subseteq M$ be a compact set. I want to prove the following fact: There exists a number $\epsilon > 0$ such that the exponential map $\exp_p : B_{\epsilon}(0) \rightarrow M$ at $p$ is a diffeomorphism onto an open set, for all $p \in S$. Now, I actually have a proof that involves considering the map $E : TM \rightarrow M \times M$ given by $E(v) = (\pi(v), \exp_{\pi(v)}(v))$ (when $M$ is complete; otherwise $E$ is defined on an open subset of $TM$ containing the zero section), showing that it is a local diffeomorphism near the zero section and arguing by contradiction. However, I don't like this proof very much, and I have two related questions: Question 1: Is there an easier way to establish this fact? Question 2: What happens if $M$ is infinite-dimensional? Thanks.","['riemannian-geometry', 'differential-geometry']"
291087,"Compute the limit $\displaystyle \lim_{x\rightarrow 0}\frac{n!x^n-\sin x\sin 2x \sin 3x\cdots\sin nx}{x^{n+2}}\;\;,$","How can I calculate the given limit $$\displaystyle \lim_{x\rightarrow 0}\frac{n!x^n-\sin x\sin 2x \sin 3x\cdots\sin nx}{x^{n+2}}\,,$$ where $n\in\mathbb{N}$ .",['calculus']
291094,Find the solutions to ${ 450 }^{ { (\sin x) }^{ 3 } }+{ 273 }^{ { (\cos x) }^{ 5 } }=2$,"Find solutions to, 
$${ 450 }^{ { (\sin x) }^{ 3 } }+{ 273 }^{ { (\cos x) }^{ 5 } }=2$$
where $0≤x≤8π$ Since $\sin x$ and $\cos x$ are in powers hence $450$ and $273$ will never be zero for any $x$. So I took $$(\sin x)^{ 3 }=0$$ and $$(\cos x)^{ 5 }=0$$
and thus I got $5$ solutions in $[0, 8π]$.
Is it correct?",['trigonometry']
291102,Matrix proof using norms,I have a linear algebra question I need help with. Let $A$ be an $m\times m$ matrix with $\|A\|_2 < 1$ where $\|A\|_2$ is the $2$-norm of $A$. Show that $I - A$ is invertible where $I$ is the identity matrix. I know that $\|Ax\|_2 \leq C\|x\|_2$ for some constant $C$ and a vector $x$. However I don't know the definition of $\|x\|_2$. I also don't see how this definition can help solve this problem.,"['vector-spaces', 'eigenvalues-eigenvectors', 'matrices', 'normed-spaces', 'linear-algebra']"
291121,generating functions for pennies and nickels,"We will use generating functions to determine how many ways there are
to use pennies and nickels to give n cents change. (i) Write the sequence Pn for the number of ways to use only pennies to
change n cents. Write the generating function for that sequence. (ii) Write the sequence Nn for the number of ways to use only nickels to
change n cents. Write the generating function for that sequence. (iii) Write the generating function for the number of ways to use pennies and
nickels to change n cents. (iv) Write the generating function for the number of ways to use pennies,
nickels and dimes to change n cents. the only thing i can think of so far is that pennies will have <1,1,1,1,1,...> 1+x+x^2+x^3+... and nickels will have <1,0,0,0,1,0,0,0,1,....> or something like that... don't know how to do the rest","['generating-functions', 'discrete-mathematics', 'combinatorics']"
291127,"can $\cos(x)$ be written as a linear combination of $e^x$ and $e^{-x}$ using the interval $[0,1]$?","Consider $C[0,1]$: the vector space of all continuous functions on the interval $[0,1]$.  Let $S$ be a subspace of $C[0,1]$ where $S =$ the span of $\{e^x, e^{-x}\}$ does the following function: $\cos(x)$ belong to $S$?  In other words, can $\cos(x)$ be rewritten as a linear combination of $e^x$ and $e^{-x}$ when working with the interval $[0,1]$? My intuition is yes, since these functions arent discontinuous, there will always be some real numbers a and b such that satisfy the following equation: $a\cdot e^x + b\cdot e^{-x} = \cos(x)$ for all $x$ in $[0,1]$.  I just dont know how to prove that..","['vector-spaces', 'linear-algebra']"
291136,Proving that $\lfloor a^2/2 \rfloor$ is even for all $a\in\mathbb{Z}$.,"$\forall a \in \mathbb Z, \lfloor a^2/2 \rfloor$ is even. I am pretty sure this statement is true. The only suspicious cases to me are 0/2 and 1/2, but they also have even floors. How do I prove it though?",['discrete-mathematics']
291153,Showing that generating function satisfies function,"Suppose that the sequence $S = (s_0, s_1, \dots )$ is defined by: $s_0 = 0$, $s_1= 1$ and $s_{n+2} = s_{n+1}+2s_n$ for $n \ge 0$. Thus, $S = (0, 1, 1, 3, 5, 11, 21,...)$. Show that the generating function $S(x) = s_0 + s_1 x + s_2 x^2 + \dots$ satisfies
$S(x) = \frac{x}{1 - x - 2x^2}$. I would start this problem with $(1,1,1,1,1,1,...) = 1/(1-x)$ and manipulate it until I end up with $S(x) = x/(1-x-2x^2)$, but I don't know how to manipulate it.","['generating-functions', 'discrete-mathematics', 'combinatorics']"
291166,Prove: bounded derivative if and only if uniform continuity,"The definition of uniform continuity of a real-valued function states: A function $f\colon A\mapsto\mathbb{R}$ is uniformly continuous on $A$ iff for every $\varepsilon \gt 0$ there exists a $\delta \gt 0$ such that for every $x$ and $y$ in $A$, whenever $y \in \left(x-\delta,x+\delta\right)$, it is the case that $f\left(y\right) \in \left(f\left(x\right)-\varepsilon,f\left(x\right)+\varepsilon\right)$. Basically how my book distinguishes this from point-wise continuity is that there exists a single $\delta$ that works for every point in the domain, so once we find that $\delta$, we know it works everywhere. On the other hand, point-wise continuity says that given a $c\in A$, there exists a $\delta$ such that the function is continuous at $c$, but all these $\delta$s may be different, perhaps depending on $c$, and we might not be able to find just one $\delta$ that works for all $c$s everywhere. I interpret this definition a completely different way, and I want to see if my conjecture is correct. I think that functions which have bounded derivatives are uniformly continuous. That is, if the ""steepness"" and ""shallowness"" of a function is limited to a certain minimum and maximum, then the there's a sufficiently small enough $\delta$ that we can use, particularly at the steepest part of the function (say at $x_0$), such that the output stays within the $\varepsilon$-neighborhood of $f\left(x_0\right)$. Is that correct? How can I prove/disprove it? The converse states that the derivative of a uniformly continuous function is bounded. Is that also true?","['calculus', 'continuity', 'real-analysis']"
291168,Algorithms for approximating $\sqrt{2}$,"Well, ""Solving"" is the wrong term since I am speaking about irrational numbers. I just don't know which word is the correct word... So that can be part $1$ of my question... what is the correct word since you obviously can't ""solve"" an irrational number because it goes forever. Part $2$ (my real question) are there algorithms for figuring out the answer to a problem like the square root of $2$ other than guess-and-checking your way to infinity? Again, I'm obviously not asking for an algorithm to give me the never ending answer because that's crazy... but for example if I wanted to know what the $15^{th}$ decimal place of the square root of $2$ was, is there an algorithm for that? Thank you! (I'm new here and know nothing about how to format math questions so any help or links would be appreciated as well, thanks!)","['radicals', 'algebra-precalculus', 'irrational-numbers', 'algorithms']"
291171,counting problems when to use what,"$n$ choose $k$, $n!$, bars and stars method are all different methods you use to get the result your looking for when it comes to counting problems, my question is when do you know when to use what. i.e. $n!$ is used when there are no replacements allowed $n(n-1)(n-2)\dots$ $n$ choose $k$ is used when you need to choose $k$ subsets out of set $n$ but when do you use the bars and stars method? and are there any context clues or something i need to look out for when reading a problem, that can help me determine when i need to use the bars and stars method or the other methods","['discrete-mathematics', 'combinatorics']"
291173,Set Theory - proof using set identities,"Suppose $B, C, D, E$ are sets Prove/Disprove: $(B \backslash C)\backslash (D\backslash  E) = (B\backslash  D)\backslash  (C\backslash  E)$ Any help would be much appreciated",['elementary-set-theory']
291174,Probability distribution of the subinterval lengths from a random interval division,"For $a \in \mathbb{R}_{+}$ and $n \in \mathbb{N}_{+}$ draw $n-1$ points $X_1, \ldots, X_{n-1}$ independently, uniformly at random from the interval $I = [0, a]$. These points partition $I$ into $n$ disjoint subintervals $I_1 \, \dot\cup \ldots \dot\cup \, I_n = I$. Let $Y_i := |I_i| \in [0, a]$ denote the interval length of the $i$'th subinterval. How are the $Y_i$'s distributed? I am especially interested in the expectation and the variance. Here are my thoughts about this: I suppose that all $Y_i$'s are identically distributed, i.e., $Y_i \sim Y$ for some random variable $Y$. Further, I suppose that the expectation of $Y$ is $\mathbb{E}(Y) = \frac{a}{n}$. However, I have no clue about the variance, and I can neither prove my conjectures, nor find an answer on the internet. Can you help me on this?","['probability-distributions', 'probability']"
291180,Why do we have 360 degrees in a circle and why we need radians? [duplicate],"This question already has answers here : Why is a full turn of the circle 360°? Why not any other number? (6 answers) Closed 11 years ago . I have two related questions: 1- Why do we have 360 degrees in a circle? 2- I have seen in most of the mathematical concepts, angle is expressed in radians not 
in degrees. Why was radian introduced?","['geometry', 'math-history']"
291189,Dual space of space of all smooth function,"On the space $C^\infty(S^1,\mathbb R)$, for each $n\in \mathbb N$, define 
$$p_N(\gamma)= \max\{|f^{(k)}(t): t\in S^1, k\leq N\}$$ Topology of all norms above define a metrizable locally convex topology (in fact Frechet space) on this space [Rudin Functional analysis page 35]. How to calculate dual space to this space, For dual space, I mean set of all continuous linear functional on $C^\infty(S^1, M)$ with norms
$$p'_M(f)= \sup_{\gamma\in M\subset C^\infty(S^1,\mathbb R)}|f(\gamma)|$$ and $M$ runs through all bounded subsets of $L$. My background and others:  I do not have enough practice and knowledge of functional analysis course.. Hence i will be happy if i get reference reading for this so that i can calculate dual myself. What are the books/topic name which i should read to get comfortable
  in calculating these type questions","['reference-request', 'locally-convex-spaces', 'functional-analysis']"
291197,Vandermonde matrix,"Let ${\bf G} \in\mathbb{C}^{M\times K}$ and ${\bf H} \in\mathbb{C}^{N\times K}$ are full-rank Vandermode matrices where $MN-1=K>N\geq N$, that is, ${\bf G}$ and ${\bf H}$ are fat. Let ${\bf F}= {\bf H}\circ {\bf G}$ where $\circ$ denotes Khatri-Rao matrix product or column-wise Khatri-Rao matrix product. Is it true that there exists ${\bf g} \in\mathbb{C}^{M}$ and ${\bf h} \in\mathbb{C}^{N}$, which are different from the columns of ${\bf G}$ and ${\bf H}$, such that ${\bf h}\otimes{\bf g}$ belongs the range space of $\bf F$, that is, \begin{equation}
{\bf F}{\bf a} = {\bf h}\otimes{\bf g}?
\end{equation} Thanks.","['vector-spaces', 'matrices']"
291214,Yet another nested radical,"Consider $$F(x) = \sqrt{x -\sqrt{2x - \sqrt{3x - \cdots}}}$$ I believe I can prove (with some handwaving) that $F$ does converge everywhere in $\mathbb{C}$ $\Im F = 0$ for sufficiently large real $x$ (actually larger than $x0 \approx 0.5243601\dots$ Does this number ring a bell?) Coincidentally $F(x0) = 0$ Weird things happen in the limit to $0$. Obviously, $F(0) = 0$. However, it seems that $$\lim_{x \to +0}F(x) = \overline{\zeta} $$ $$\lim_{x \to -0}F(x) = \zeta $$
where $\zeta = \frac{1 + i\sqrt{3}}{2}$ is a usual cubic root of $-1$. Moreover, $F$ seems to reach one of those as $x$ approaches $0$ at a rational angle. I understand that this may well be a computational artifact (still making no sense to me), but proving or refuting these limits is definitely out of my league. Any help?","['nested-radicals', 'experimental-mathematics', 'sequences-and-series']"
291230,Orthogonal Projection of matrix onto subspace,"Let's say I have the subspace $$S=\{(X_1,X_2,X_3,X_4)\mid~~6X_1 - 2X_2 + 4X_3-10X_4 = 0\}$$ How do I go about finding the matrix which is the orthogonal projection onto this subspace?","['vector-spaces', 'matrices', 'linear-algebra']"
291233,Is $f(z)=\exp (-\frac{1}{z^4})$ holomorphic?,"Let $f(z)=\exp (-\frac{1}{z^4})$ for $z\neq 0$ and $f(0)=0$. Is it obvious that 
  $$\lim_{z\to z_{0}}\frac{f(z)-f(0)}{z-0}=\lim_{z\to z_{0}}\frac{\exp (-\frac{1}{z^4})}{z}=0$$ And if this limit is indeed equal to $0$; can we conclude that $f$ is holomorphic on $\mathbb{C}$ ?",['complex-analysis']
291235,Show function with infinitely many discontinuities is Riemann-integrable,"Prove that the following function is Riemann-integrable on $[0, 1]$ even though it has infinite many discontinuities on $[0, 1]$ : $$f(x) = \begin{cases} 1 & \text{if } x = \frac1n \text{ where } n = 1, 2, 3, \ldots, \\ 0 & \text{otherwise}.\end{cases}$$ I have a possible proof but don't feel too confident about it. If anyone could tell me if its valid or give me another proof, or hints to another proof, that would be very helpful! Let $\varepsilon > 0$ .  Observe that given $\frac\varepsilon2$ there exists a least natural number $N$ with $\frac{1}{N+1} < \frac\varepsilon2$ . Observe that there are $N$ numbers less than $N + 1$ . Now we want to consider the two intervals $[0, \delta]$ and $[\delta, 1]$ with $0 < \delta < 1$ and $\delta = \frac\varepsilon2$ . Lets first consider the interval $[\delta, 1]$ . Observe that there are exactly $N$ discontinuities on this interval, that is, $N$ elements of the form $\frac1n$ such that $\frac1N > \frac{1}{N+1}$ . This interval is bounded since by construction $f$ is bounded above by 1 and bounded below by 0 and thus it is bounded. We have satisfied the hypothesis of Exercise 6.6 and thus we can conclude that $f$ is Riemann-integrable on $[\delta, 1]$ . Moreover, since $f$ is Riemann-integrable on $[\delta, 1]$ , by Theorem 6.1, we are assured that there exists a partition $P_2$ such that $U(P_2, f) - L(P_2, f) < \frac\varepsilon2$ . Observe that the lower sum is 0 for any given partition, so we have $U(P_2, f) < \frac\varepsilon2$ . Now let us consider the interval $[0, \delta]$ . Let $P_1$ be any partition of this set and observe that $$U(P, f) = \sum_{i = 1}^l M_i \, \Delta x_i < 1 \cdot \big( \frac\varepsilon2 - 0\big) = \frac\varepsilon2$$ since the supremum of the entire interval is 1, it must hold that 1 is an upper bound for any smaller interval. Moreover the length of the interval $[0, \delta]$ is $\delta$ which is defined to be $\frac\varepsilon2$ . Again it still holds that the lower sum is 0. Thus we have $U(P_1, f) < \frac\varepsilon2$ . Now choose $P = P_1 \cup P_2$ to be a common refinement of $P_1$ and $P_2$ and observe that $$U(P, f) - L(P, f) = U(P, f) = U(P_1, f) + U(P_2, f) = \frac\varepsilon2 + \frac\varepsilon2 = \varepsilon$$ Thus by Theorem 6.1 $f$ is Riemann-integrable on $[0, 1]$ .","['integration', 'real-analysis']"
291240,How to prove that a given curve is actually a straight line,"Given a curve, I have to prove or disprove that it is a straight line. How do I do this? I tried by finding and comparing slopes but I can see that this will not be a very computationally efficient way (as I am implementing this on a PC) ; I will have to find slopes at different points and see if the slope is changing or not. If the slop has not changed, I will chose next  point and find the slope at this new point. And yes there are infinite points on the line, so how many slopes I am going to measure ? Another trick I adopted is Divide and Conquer; I divided the entire curve into N segments, and processed each segment using multi-threading, looking for slope changes at several different places simultaneously. This also seem not very efficient, given the kind of curves I am processing. Even I have to look for inter-segment slope changes in addition to intra-segment slope changes Any other computationally efficient way? Some of the curves that I am dealing with are as follows: (As can be seen only one or two small curves look like straight lines. ) I want to understand an efficient algorithm from two different point of views: 1- When the equations for each curve is given; Algebra point of view 2- No equations are given, only images are given; may be I can call this Geometry point of view",['geometry']
291250,Is the inverse of a linear transformation linear as well?,"A question from the field of Linear Algebra.
If I have a linear transformation $T$ that is one-to-one and onto, would that mean that the $T^{-1}$ will also be linear? If so, is there any general proof for it? Thanks",['linear-algebra']
291258,What happens when the basis vectors of an integer lattice are not linearly independent?,"The definition of a lattice requires basis vectors that are linearly independent. Why? For example, the following three vectors are linearly independent and form the basis of a lattice: \begin{array}{ccc}
0 & 0 & 1 \\
0 & 2 & -2 \\
1 & -2 & 1 \end{array} But what if we add a fourth vector such that they're not linearly independent anymore. For example: \begin{array}{ccc}
0 & 0 & 1 & 4\\
0 & 2 & -2 & 2\\
1 & -2 & 1 & 3\end{array} Are the four vectors the basis of a lattice? Why or why not? And, if it is, why does the definition require linear independence? Is there an equivalent basis that is linearly independent?","['integer-lattices', 'linear-algebra']"
291260,How do I prove convergence of $\sum\limits_{k=1}^\infty \tfrac{\sin k^2}{k}$?,How do I prove convergence of $\sum\limits_{k=1}^\infty \tfrac{\sin k^2}{k}$? I would prefer avoid using Taylor expansion...,"['convergence-divergence', 'sequences-and-series', 'real-analysis']"
291281,Diffeomorphic riemannian manifolds and volume forms,"Maybe the question will be stupid, but I'm a beginner in riemannian geometry... We have two riemannian manifolds $(M,g)$, $(\overline M,\overline g)$ and a diffeomorphism $F:M\rightarrow\overline M$ between them. If $dV_g$ and $dV_\overline g$ are the riemannian volume forms of $M$ and $\overline M$ respectively, is it true that $F^*(dV_\overline g)_p=det(DF_p)(dV_g)_p$ $\forall p\in M$  ?","['differential-geometry', 'differential-forms', 'manifolds', 'riemannian-geometry', 'differential-topology']"
291284,$X$ and $f(X)$ independent $\Longleftrightarrow$ $f(X)$ is degenerate,"Let $(\Omega, \cal{A}, \mathbb{P})$ be a probability space and $X$ a random variable on $\Omega$. Let, also, $f:\Omega\to\mathbb{R}$ be a Borel function. Then: $X$ and $f(X)$ are independent $\Longleftrightarrow$ there exists some $t\in\mathbb{R}$ such that $\mathbb{P}[f(X)=t]=1$, that is $f(X)$ is a degenerate r.v. The only thing that I could make out is that if $X$ and $f(X)$ are independent, then $\mathbb{P}[f(X)\in B]=0$ or $1$ for every Borel subset of $\mathbb{R}$, since $\sigma(f(X))\subseteq \sigma(X)$ and hence, $f(X)$ is independent of its self. Suppose, now, that $\mathbb{P}[f(X)\leq x]=0$ for all $x\in\mathbb{R}$. Then: $\mathbb{P}[f(X)\in\mathbb{R}]=\mathbb{P}[\bigcup_{n=0}^{\infty}[f(X)\leq n]]\leq\sum_{n=0}^{\infty}[f(X)\leq n]=0$ which obviously is a contradiction since $\mathbb{P}[f(x)\in\mathbb{R}]=1$. However, I don't know ow to prove this and my attempt isn't likely to become a complete solution. Any help would be appreciated. Thanks in advance!",['probability-theory']
291287,For which $a$ does the equation $f(z) = f(az) $ has a non constant solution $f$,"For which $a \in \mathbb{C} -\ \{0,1\}$ does the equation $f(z) = f(az) $ has a non constant solution $f$ with $f$ being analytical in a neighborhood of $0$. My attempt: First, we can see that any such solution must satisfy: $f(z)=f(a^kz)$ for all $k \in \mathbb{N} $. If $|a|<1$: The series $z_{k} = a^k$ converges to 0 which is an accumulation point, and $f(z_i)=f(z_j)$ for all $i, j\in \mathbb{N} $. Thus $f$ must be constant. If $|a|=1$: For all $a \neq 1$ , $f$ must be constant on any circle around $0$, so again $f$ must be constant. My quesions are: Am I correct with my consclusions? Also, I'm stuck in the case where $|a|>1$. Any ideas? Thanks",['complex-analysis']
291290,Ito Isometry and quadratic variation,"Here is a confusion regarding stochastic integrals. Let
$Y_t=\int_0^tW_sds$ where $W_t$ is a Brownian Motion. Now $dY_t=W_tdt$. So from this expression one can conclude that $dY_t \cdot dY_t=d[Y_t,Y_t]$ where $[\ ]$ is the quadratic variation process and it must be zero as $dt \cdot dt =0$. But people apply Fubini-stochastic integral exchange trick and see $Y_t=\int_0^t(t-u)dW_u$. So by applying Ito isometry one can see that $E(Y_t^2)=\int_0^t(t-u)^2du \neq 0$. Can someone tell me where I'm going wrong? Why this discrepancy?","['probability-theory', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-analysis']"
291298,non-linear ordinary differential equation,"Studying some Newtonian mechanics, I've encountered this differential equation : $y'+a y^2=b$ where $a,b$ are constants. how could we solve it ? (I trying to get an algebraic solution)",['ordinary-differential-equations']
291314,Every $T_3$ space with a countable basis is $T_4$.,"Theorem. Every regular space with a countable basis is normal. Proof. Let X be a regular space with a countable basis $\mathcal{B}$. Let $A$ and $B$ be disjoint closed subsets of $X$. Each point $x$ of $A$ has a neighborhood $U$ disjoint from $B$. Using regularity, choose a neighborhood $V$ of $x$ whose closure lies in $U \dots$ I can't follow the last step made until this point; Using regularity, choose a neighborhood $V$ of $x$ whose closure lies in $U$ . I do know that $X$ is a regular space if, given any nonempty closed set $F$ and any point $x$ that does not belong to $F$, there exists a neighbourhood $U$ of $x$ and a neighbourhood $V$ of $F$ that are disjoint. Could anyone help me with some insights? Thanks in advance.","['general-topology', 'separation-axioms', 'second-countable']"
291316,Continuous functions with a certain condition,What is the class of continuous functions $f\colon \mathbb{R}\to\mathbb{R}$ which satisfy $f(x)-f(y)\in\mathbb{Q}$ if and only if $x-y\in \mathbb{Q}$?,"['continuity', 'real-analysis']"
291318,Derivative of the $2$-norm of a multivariate function,"I've got a function $$g(x,y) = \| f(x,y) \|_2$$ and I want to calculate its derivatives with respect to $x$ and $y$ . Using Mathematica, differentiating w.r.t. $x$ gives me $ f'_x(x,y) \text{Norm}'( f(x,y))$ , where Norm is $\| \cdot \|$ . I read here that $$d\|{\bf x}\| = \frac{ {\bf x}^Td{\bf x}}{\|{\bf x}\|}$$ at least for the $2$ -norm. Point is, as inside the norm I have a multivariate function, I'm still confused on how to calculate $ f'_x(x,y) \text{Norm}'( f(x,y))$ I think it should be $f'_x(x,y) \frac{f(x,y)}{||f(x,y)||}$ , but some verification would be great :)","['multivariable-calculus', 'partial-derivative', 'derivatives']"
291322,"Books for algebraic geometry, algebraic topology [duplicate]","This question already has answers here : Closed 11 years ago . Possible Duplicate: (undergraduate) Algebraic Geometry Textbook Recomendations I am planning to self-study one of these  two subjects: Algebraic geometry , Algebraic topology. I can borrow books from library, but I don't know which books to borrow. Please suggest books for me. Thank you","['algebraic-geometry', 'self-learning', 'algebraic-topology']"
291348,Exact sequence involving the nabla operator,"Recently I noticed that $$0 \longrightarrow \Bbb R \overset{\text{const.}}\longrightarrow \mathcal{C}^\infty(\Bbb R^3,\Bbb R) \overset{\text{grad}}\longrightarrow \mathcal{C}^\infty(\Bbb R^3,\Bbb R^3) \overset{\text{rot}}\longrightarrow \mathcal{C}^\infty(\Bbb R^3,\Bbb R^3) \overset{\text{div}}\longrightarrow \mathcal{C}^\infty(\Bbb R^3,\Bbb R)\longrightarrow 0$$ is an exact sequence of $\Bbb R$-algebras, where the second arrow is given by $\text{const}:c \mapsto f(\vec x)\equiv c$ and grad, rot ,div are the gradient, rotation and divergence operators. Is the existence of such an exact sequence a mere curiosity or does it have its origins from deep results in homological algebra. If so, are there generelizations to $\Bbb R^n$ with higher $n$ or even to other smooth manifolds?","['homological-algebra', 'multivariable-calculus', 'functional-analysis', 'differential-geometry']"
291349,Looking for a simple proof that groups of order $2p$ are up to isomorphism $\Bbb{Z}_{2p}$ and $D_p$ for prime $p>2$.,"I'm looking for a simple proof that up to isomorphism every group of order $2p$ ( $p$ prime greater than two) is either $\mathbb{Z}_{2p}$ or $D_{p}$ (the Dihedral group of order $2p$ ). I should note that by simple I mean short and elegant and not necessarily elementary. So feel free to use tools like Sylow Theorems, Cauchy Theorem and similar stuff. Thanks a lot!","['dihedral-groups', 'group-isomorphism', 'cyclic-groups', 'finite-groups', 'group-theory']"
291374,Show identity of subgroup is same as identity of group,"Let $H$ be a subgroup of $G$ . Let $1_H$ and $1_G$ be the identities of $H$ and $G$ , respectively. Show that $1_H=1_G$ . My attempt is: since we know that the identity of a group is unique, and hence $1_H=1_G$ . Is my proof correct?","['group-theory', 'abstract-algebra']"
291376,differentiate with respect to a function,"Let's say I have this function $f(x)=x$. I want to differentiate with respect to $x^2$. So I want to calculate $\large\frac{df(x)}{dx^2}$. In general, how can I calculate the derivative of a function $f(x)$ with respect to a function $g(x)$, so $\large\frac{df(x)}{dg(x)}$? (I dont know whether this is a good notation)?","['calculus', 'derivatives']"
291392,Strictly monotone functions such that $x= f(\frac{x^2}{f(x)})$ [duplicate],"This question already has answers here : Find all strictly monotone $f:(0,+\infty) \to (0, +\infty)$ such that $f(\frac{x^2}{f(x)})=x.$ (2 answers) Closed 3 years ago . What are the strictly monotone functions $f\colon (0,\infty)\to (0,\infty)$ which satisfy $x= f(\tfrac{x^2}{f(x)})$ for $x>0$. I cannot find any other than $f(x)=x$.","['real-analysis', 'functional-equations']"
291411,Is every compact hypersurface contained in a sphere which it touches twice?,"Let $M\subset \mathbb{R}^{n+1}$ be a compact $n$-manifold. There exists, then, a smallest $n$-sphere containing $M$, and it must touch it in one point. Must it touch it twice? This seems quite intuitively right to me, but I've no idea how to prove it. It's easy to construct counterexamples where you can't have more than 2 (e.g. an ellipse which is not a circle).",['differential-geometry']
291416,"Nonempty subset H of group G is subgroup iff $ab^{-1} \in H $ for any $a,b \in H$","Let $G$ be a group. Show that a nonempty subset $H$ is a subgroup of $G$ if any only if $ab^{-1} \in H $ for any $a,b \in H$. The forward direction is quite easy. Suppose $H$ is a subgroup. Then by closure, $ab \in H$ for any $a,b \in H$. Every element has an inverse. Hence, if $b \in H $, then $b^{-1} \in H$. Hence, by closure again, $ab^{-1} \in H$. Backward direction, suppose $ab^{-1} \in H $ for any $a,b \in H$. Let $a=b$. Then we have $bb^{-1}=1_H\in H$. Let $a=1_H$ , we have $1_Hb^{-1}=b^{-1} \in H$. I don't know how to show closure. Reference: Fraleigh p. 58  Question 5.45 in A First Course in Abstract Algebra","['group-theory', 'abstract-algebra']"
291420,"If $x,y,z\in[-1,1]$ and $1+2xyz\geq x^2+y^2+z^2$, then can we infer $1+2(xyz)^n\geq x^{2n}+y^{2n}+z^{2n}$?","This problem was in IMC 2010. Assuming $x,y,z\in [-1,1]$ , suppose that $$1+2xyz\geqslant x^2 + y^2 + z^2$$ Can we infer from this that $$1+2(xyz)^n\geqslant x^{2n} + y^{2n} + z^{2n}$$ for any positive integer $n$ ?","['contest-math', 'inequality', 'cauchy-schwarz-inequality', 'real-analysis']"
291455,Skewness - Zero,"Why is the skewness of the samples $[ 1,\,2,\,3,\,4,\, 5,\, 6,\, 7,\, 8 ,\, 9 ]$ zero? Of course, it follows from definition, $\displaystyle E \left[\left(\frac{x_i - \text{mean}}{\text{deviation}}\right)^3\right]$. But on the other hand a skewness of zero means a symmetrical distribution which is, obviously, not the case for the linear increasing values of the samples presented above?!",['statistics']
291478,Show that there are no nonzero solutions to the equation $x^2=3y^2+3z^2$,"I am asked to show that there are no non zero integer solutions to the following equation 
$x^2=3y^2+3z^2$ I think that maybe infinite descents is the key. So I started taking the right hand side modulo 3 which gives me zero. Meaning that $X^2$ must be o modulo 3 as well and so I can write $X^2=3k$ , for some integer K and (k,3)=1. I then divided by 3 and I am now left with $k=y^2+z^2$ . Now I know that any integer can be written as sum of 2 squares if and only if each prime of it's prime factorization has an even power if it is of the form 4k+3. But yet I am stuck .
If anyone can help would be appreciated.",['number-theory']
291484,Extreme Value Theorem and Semicontinuity,"Restricting us to function of a single real variable, I was used to prove Extreme value theorem via the short way: show that continuous functions preserve compactness, and the job is done. Now, I want to prove similar results for semicontinuous functions. Wikipedia show me how to do: http://en.wikipedia.org/wiki/Extreme_value_theorem#Extension_to_semi-continuous_functions This convinces me, however is a bit long and messy. There is any way to prove the theorem more straightforwardly? For example, proving that an upper semicontinuous function has an image that owns its supremum? Thanks.","['functions', 'continuity', 'real-analysis']"
291497,Matrix is singular to working precision,"I have a problem while evaluating inverse using inv in MATLAB.
My matrix looks like this: term1 =

       29929       29756       29929           0       29756       29756
       29756       29584       29756           0       29584       29584
       29929       29756       29929           0       29756       29756
           0           0           0           0           0           0
       29756       29584       29756           0       29584       29584
       29756       29584       29756           0       29584       29584 when i try to calculate inverse, MATLAB throws a warning Matrix is singular to working precision and the result is: ans =

   Inf   Inf   Inf   Inf   Inf   Inf
   Inf   Inf   Inf   Inf   Inf   Inf
   Inf   Inf   Inf   Inf   Inf   Inf
   Inf   Inf   Inf   Inf   Inf   Inf
   Inf   Inf   Inf   Inf   Inf   Inf
   Inf   Inf   Inf   Inf   Inf   Inf Can anyone tell me why this is happening and any ways to resolve it and get the correct result?","['matrices', 'matlab', 'inverse']"
291501,Two-valued measure is a Dirac measure,"Let $(X,\mathfrak B)$ be a measurable space such that $\{x\}\in \mathfrak B$ for all $x\in X$, and let $\mu$ be a positive measure on this space such that
$$
  \mu(B) \in\{0,1\} \quad\text{for all }B\in \mathfrak B.
$$
What are the mildest conditions on $(X,\mathfrak B)$ that imply that $\mu =\delta_x$ for some $x\in X$? It is known to hold for $\Bbb R$ with a Borel $\sigma$-algebra, and I believe it fairly easy extends to $\Bbb R^n$. I wonder, though, whether it holds at least for locally compact Polish spaces, or perhaps for more general case. I am also interested in examples of spaces where such statement does not hold.","['probability-theory', 'measure-theory', 'functional-analysis']"
291504,Does $\int_0^{\infty} \left( p + q W \left( r e^{- s x + t} \right) + u x \right) e^{- x} d x$ have a closed-form expression?,"Does $\int_0^{\infty} \left( p + q W \left( r e^{- s x + t} \right) + u x \right)
e^{- x} d x$ (with 6 variables) where W is the Lambert W function (also known as ProductLog in Mathematica) have a closed-form expression? If we drop the variable $s$ from the expression Maple is able calculate 
$$
\int_0^{\infty} \left( p + q W \left( r e^{- x + t} \right) + u x \right)
e^{- x} d x = q W \left( r e^t \right) + \frac{q}{W \left( r e^t \right)}
- q + u + p - \frac{q}{r e^t}
$$ 
which agrees with numerical calculations, so my gut feeling is that such an expression should exist for the 6-variable expression as well, but Maple nor Mathematica are able to compute it. To simplify the problem a bit let's consider a simpler integral, $$\int_0^{\infty} W \left( e^{- s x} \right) e^{- x} d x$$
If $s=1$ then we have $$\begin{array}{ll}
  \int_0^{\infty} W \left( e^{- x} \right) e^{- x} d x & = \frac{1 - 2 W
  \left( 1 \right) + W \left( 1 \right)^2}{W \left( 1 \right)}\\
  & = 0.330366124761680583225170439162
\end{array}$$ which is the solution to $$\left\{ y : y - \frac{1}{W \left( 1 \right)} = W \left( 1 \right) - 2
\right\}$$ If $s=1/2$ then Maple is not able to immediately find  an answer but it is seen to be true numerically that we have(thanks to the amazing and wonderful RIES program ) $$\begin{array}{ll}
  \int_0^{\infty} W \left( e^{- \frac{x}{2}} \right) e^{- x} d x & =
  \frac{- 1 + 2 W \left( 1 \right) - W \left( 1 \right)^2 + 4 W \left( 1
  \right)^3}{4 W \left( 1 \right)^2}\\
  & = 0.421516016690748181742333199330
\end{array}$$ which is the solution to $$\left\{ y : \cos \left( \pi \sqrt{W \left( 1 \right) - y} \right) = \sin
\left( \frac{\pi}{2 W \left( 1 \right)} \right) \right\}$$ Any ideas would be greatly appreciated!","['closed-form', 'integration', 'lambert-w']"
291511,"If $f:\mathbb R\to\mathbb R$ is continuous and $f^3(x)=x$, then $ f(x)=x$ [duplicate]","This question already has answers here : Closed 11 years ago . Possible Duplicate: 3rd iterate of a continuous function equals identity function Assume $f:\mathbb R\to\mathbb R$  is  continuous and $f^3(x)=x $ $\forall x$. How can I prove that$$\forall x\in\mathbb R,\ f(x)=x$$","['functions', 'analysis']"
291515,"Space of probability measures ""complete""? (In the other sense)","I want to consider a space of probability measures on some set $X$, such that the space of measures is complete , not in the sense of complete probability measures (though probably that too), but as in the standard analysis meaning. I haven't studied far enough in probability measures, so I can't tell -- what conditions do I need on my space such that it is complete? And how should I metrize it? Thanks! Edit: A reference/citation to a text would also be super helpful!","['measure-theory', 'analysis']"
291538,"Limit of $\frac{\sin(x+y)}{x+y}$ as $(x,y) \to (0,0)$","I want to prove that $\lim_{(x,y)\to (0,0)} \frac{\sin(x+y)}{x+y} = 1$ Is it sufficient to say that if $u = x+y$ then as $(x,y) \rightarrow (0,0)$ then $u \rightarrow 0$ and then evaluate $\lim_{u\to 0} \frac{\sin u}{u}$ which is 1 by L'Hôpital's rule? I feel that this is somehow ""cheating"" and I would really like to prove it in a more rigorous way, i.e., by using an $\epsilon$ - $\delta$ argument. I have tried for a long time to use the one variable version and prove that if $0 \lt \sqrt{x^2 + y^2} \lt \delta$ then somehow $0 \lt |x+y| \lt \delta$ and it would follow from the one variable case that $| \frac{\sin(x+y)}{x+y} - 1| \lt \epsilon$. However, because $|x+y| = 0$ when $y = -x$ I am not getting anywhere with this. Any input on this would be helpful :)","['calculus', 'limits']"
291569,Holomorphic functions on unit disc,"Let $f,g$ be holomorphic on $\mathbb{D}:=\lbrace z\in\mathbb{C}:|z|<1\rbrace$, $f\neq0,g\neq0$, such that $$\frac{f^{\prime}}{f}(\frac{1}{n})=\frac{g^{\prime}}{g}(\frac{1}{n}) $$ for all natural $n\geq1$. Does it imply that $f=Cg$, where $C$ is some constant? Let $A:=\lbrace\frac{1}{n}:n\geq1\rbrace$ and $h:=\frac{f^{\prime}}{f}-\frac{g^{\prime}}{g}$. Now, $h$ is holomprphic on $\mathbb{D}$ and disappears on a subset of $\mathbb{D}$ which has a limit point. Thus $h=0$, so $\frac{f^{\prime}}{f}=\frac{g^{\prime}}{g}$ on $\mathbb{D}$. Could someone help with the next steps? Or maybe $f$ doesn't have to be in the form described above?",['complex-analysis']
291571,Paracompactness of CW complexes (rather long),"I finished reading Lee's 'introduction to topological manifolds' (2nd edition) and I'm currently tying up some loose ends. One thing I can't understand is the proof of paracompactness of CW complexes. The proof contains some mistakes I feel (perhaps wrongly), and although I get the general idea, there is one technical detail I can't work out. Lee uses a somewhat different approach (as far as I know), just to illustrate a technique of inductively building maps out of a CW complex skeleton par skeleton. He claims that any open cover $\left ( U_\alpha  \right )_{\alpha \in A}$ of a CW complex $X$ has a partition unity $\left ( \psi_\alpha  \right )_{\alpha \in A}$ subordinate to it, from which paracompactness follows (I guess because then $\left ( \psi_\alpha^{-1}\left \{ x \in X|  \psi_{\alpha }(x)\neq 0 \right \}\right )_{\alpha \in A}$ is a locally finite open refinement of $\left ( U_\alpha  \right )_{\alpha \in A}$). The aim is to inductively build a partition  of unity $\left ( \psi^n_\alpha  \right )_{\alpha \in A}$ of the n-skeleton $X_n$ subordinate to the open cover $\left ( U^n_\alpha  \right )_{\alpha \in A}$ of $X_n$, where $U^n_\alpha = U_\alpha \cap X_n$, as follows: for $n=0$, choose, if $x \in X_0$, any $\alpha(x) \in A$ such that $x \in U_{\alpha(x)}$. Then, set $\psi^0_{\alpha(x)}(x)= 1$ and $\psi^0_{\alpha}(x)= 0$ if $\alpha \neq \alpha(x)$. Then, suppose that we have found, for $k=0,\dots,n$, partitions of unity $\left ( \psi^k_\alpha  \right )_{\alpha \in A}$ of $X_k$ subordinate to $\left ( U^k_\alpha  \right )_{\alpha \in A}$, such that $ {\psi^k_\alpha}_{|X_{k-1} }=\psi^{k-1}_\alpha$ and, if $\psi^{k-1}_\alpha\equiv 0$ on an open subset $V$ of $X_{k-1}$, then there exists an open subset $V'$ of $X_k$ containing $V$ on which $\psi^{k}_\alpha \equiv 0$. Let $q:X_n\sqcup \bigsqcup_{\gamma \in \Gamma}  D^{n+1}_{\gamma}  \rightarrow X_{n+1}$be the quotient map realizing $X_{n+1}$ as an adjunction space obtained by attaching $n+1$- cells $D^{n+1}_{\gamma}$ to $X_n$ and let $\phi_\gamma : \partial D^{n+1}_{\gamma} \rightarrow X_n$ be the maps glueing $ \partial D^{n+1}_{\gamma}$ to $X_n$. First, set $\tilde{\psi}^n_{\alpha,\gamma}=\psi^n_\alpha \circ \phi_\gamma :\partial D^{n+1}_{\gamma} \rightarrow \left [ 0,1 \right ]$ and $\tilde{U}^{n+1}_{\alpha,\gamma}=q_{|D^{n+1}_\gamma}^{-1}(U^{n+1}_\alpha)$, so, in particular, $(\tilde{U}^{n+1}_{\alpha,\gamma})_{\alpha \in A}$ is an open cover of $D^{n+1}_\gamma$ and choose a partition of unity $(\eta_{\alpha,\gamma})_{\alpha \in A}$ of $D^{n+1}_\gamma$ subordinate to $(\tilde{U}^{n+1}_{\alpha,\gamma})_{\alpha \in A}$. For $\gamma \in \Gamma$ fixed, $(supp \  \psi^n_\alpha)_{\alpha \in A}$ is a locally finite family of subsets of $X_n$ by construction and since $\phi_\gamma(\partial D^{n+1}_{\gamma})$ is compact in $X_n$, it meets $supp \  \psi^n_\alpha$ for at most a finite amount of indices (I get that). Hence, only for finitely many indices $\alpha_1,\dots,\alpha_k$, we have $\tilde{\psi}^n_\alpha \not\equiv 0$ (I get that). Let, for $C\subseteq \partial D^{n+1}_{\gamma} $, and $0<\varepsilon <1$, $C(\varepsilon)=\left \{ x \in D^{n+1}_{\gamma} |\frac{x}{\left \| x \right \|}  \in C \text{ and } 1-\varepsilon < \left \| x \right \| \leqslant 1 \right \}$, norm confer some homeomorphism from $ D^{n+1}_{\gamma}$ to the closed unit ball. For any $\alpha_j$ of the indices $\alpha_1,\dots,\alpha_k$, $supp \  \tilde{\psi}^n_{\alpha_j,\gamma}$ is a non-empty compact subset of $\partial D^{n+1}_{\gamma}$ so of $D^{n+1}_{\gamma}$ and is contained in $\tilde{U}^{n+1}_{\alpha_j,\gamma}$ (I can work that out). Since $co \tilde{U}^{n+1}_{\alpha}$ is closed in $D^{n+1}_{\gamma}$ and disjoint from $supp \  \tilde{\psi}^n_{\alpha_j,\gamma}$  , one can find an $0<\varepsilon_j<1$ such that $supp \  \tilde{\psi}^n_{\alpha_j,\gamma} (\varepsilon_j)\subseteq \tilde{U}^{n+1}_{\alpha_j,\gamma}$ (I get that). Then, set $\varepsilon_\gamma =\min^k_{j=1} \varepsilon_j$ (I would feel more confident choosing $0<\varepsilon_\gamma <\min^k_{j=1} \varepsilon_j$ ) and let $\sigma: D^{n+1}_\gamma \rightarrow \left [0,1 \right ]$ be a bump function that is $1$ on $D^{n+1}_\gamma  \setminus \partial D^{n+1}_\gamma(\varepsilon_\gamma) $ and is supported in $\partial D^{n+1}_\gamma(\dfrac{\varepsilon_\gamma}{2})$. This seems wrong to me because $\partial D^{n+1}_\gamma(\dfrac{\varepsilon_\gamma}{2})$ and $D^{n+1}_\gamma  \setminus \partial D^{n+1}_\gamma(\varepsilon_\gamma) $ are disjoint. I think that $\partial D^{n+1}_\gamma(\dfrac{\varepsilon_\gamma}{2})$ should be replaced with something like $D^{n+1}_\gamma  \setminus \overline{\partial D}^{n+1}_\gamma(\dfrac{\varepsilon_\gamma}{2}) $ . So, let's do that. Then define $\tilde\psi ^{n+1}_{\alpha,\gamma}: D^{n+1}_\gamma \rightarrow \left [ 0,1 \right ]$ by $\tilde\psi ^{n+1}_{\alpha,\gamma}(x)=\sigma(x)\eta _{\alpha,\gamma}(x)+(1-\sigma(x))\tilde{\psi}^n_{\alpha,\gamma}(\frac{x}{\left \| x \right \|}) $. $\tilde\psi ^{n+1}_{\alpha,\gamma}$ is continuous, takes values in $\left [ 0,1 \right ]$, coincides with $\tilde\psi ^{n}_{\alpha,\gamma}$ on $\partial D^{n+1}_\gamma$ and $\sum_{\alpha \in A}\tilde\psi^{n+1}_{\alpha,\gamma}\equiv 1$. $supp \ \tilde{\psi}^{n+1}_{\alpha,\gamma}\subseteq \tilde{U}^{n+1}_{\alpha,\gamma}$ (I can work that out, but only if one chooses $\varepsilon_\gamma< \min^{k}_{j=1}\varepsilon_j$ ). Now, repeat this for every $\gamma \in \Gamma$. The map that coincides with $\psi^n_\alpha$ on $X_n$ and with $\tilde{\psi}^{n+1}_{\alpha,\gamma}$ on $D^{n+1}_\gamma$ passes to the quotient to yield the requested $\psi^{n+1}_\alpha$. Okay, here is my problem: I cannot relate the supports of $\psi^{n}_\alpha$ and the $\tilde{\psi}^{n+1}_{\alpha,\gamma}$ to the support of $\psi^{n+1}_\alpha$ accurately enough. I can only show that $supp \ \psi^{n+1}_\alpha \subseteq \overline{U}^{n+1}_\alpha$ . Any thoughts anyone? I can understand the rest of the proof, however, something seems not right, so, to be sure, here it is. To check that $(supp \ \psi^{n+1}_\alpha)_{\alpha \in A}$ is locally finite, suppose $x$ is in the interior of an $n+1$-cell $D^{n+1}_\gamma$. Lee claims that the cell itself is a neighbourhood of $x$ on which only finitely many of the $\psi^{n+1}_\alpha$ are non-zero. However, is this true? Identically zero $\tilde{\psi}^{n}_\alpha$ cán yield non-zero  $\tilde{\psi}^{n+1}_\alpha$. Having said that, there is an easy way to work around this, because of the use of the $\eta_{\alpha,\gamma}$ in the construction of the $\tilde{\psi}^{n+1}_\alpha$. Any comments would be grately appreciated, big thanks in advance.","['general-topology', 'paracompactness', 'algebraic-topology', 'cw-complexes']"
291586,Solve for $x$: $(x+0.6)^2 = 1.4x^2$,I am very terrible at some aspects of algebra and I would like to ask how to solve this problem (It's actually only a small part of a larger physics problem). I've looked up the laws of exponents and from what I can tell I cannot easily seperate the $x$ from the $0.6$ . At this point I am quite confused and would appreciate a step by step approach: Solve for $x$ : $$(x+0.6)^2 = 1.4x^2$$,['algebra-precalculus']
291611,"Why can we ""separate"" variables? [duplicate]","This question already has answers here : Closed 11 years ago . Possible Duplicate: What am I doing when I separate the variables of a differential equation? My school textbook has a section on differential equations. One of the tricks used is the following- $$\frac{dx}{dy}=\frac{x}{y}\implies\frac{dx}{x}=\frac{dy}{y} $$ Integration is then duly carried out.Sparation of the variables leaves an impression on me that somehow, $dy$ is ""dividing"" $dx$. Whereas,when I studied the definition of the derivative, it was like $$f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}.$$ I am not convinced how the so-called separation of variables is legal .Does it follow from the definition of the derivative ? Can anyone guide me to a proof?",['calculus']
291613,Conditional distributions of the multivariate normal,"Wikipedia gives details on the conditional distribution of the multivariate normal: If $\mu$ and $\Sigma$ are partitioned as follows $\boldsymbol\mu = \begin{bmatrix}  \boldsymbol\mu_1 \\  \boldsymbol\mu_2 \end{bmatrix}$ $\boldsymbol\Sigma = \begin{bmatrix}  \boldsymbol\Sigma_{11} & \boldsymbol\Sigma_{12} \\  \boldsymbol\Sigma_{21} & \boldsymbol\Sigma_{22} \end{bmatrix} \quad$ then, the distribution of $x_1$ conditional on $x_2= a$ is
  multivariate normal $(x_1|x_2=a) \sim N(\bar{\mu}, \bar{\Sigma})$
  where $\bar{\boldsymbol\mu} = \boldsymbol\mu_1 + \boldsymbol\Sigma_{12} \boldsymbol\Sigma_{22}^{-1} \left(  \mathbf{a} - \boldsymbol\mu_2 \right) $ and covariance matrix $\overline{\boldsymbol\Sigma} = \boldsymbol\Sigma_{11} - \boldsymbol\Sigma_{12} \boldsymbol\Sigma_{22}^{-1} \boldsymbol\Sigma_{21}.  $ How can I prove this result? Wikipedia cites Eaton, Morris L. (1983). Multivariate Statistics: a Vector Space Approach. John Wiley and Sons. pp. 116–117., but I don't have this book handy...","['normal-distribution', 'probability']"
291626,Is there a general formula for the antiderivative of rational functions?,"Some antiderivatives of rational functions involve inverse trigonometric functions, and some involve logarithms. But inverse trig functions can be expressed in terms of complex logarithms. So is there a general formula for the antiderivative of any rational function that uses complex logarithms to unite the two concepts?","['rational-functions', 'integration']"
291644,Alternative construction of the tensor product (or: pass this secret),"The paper Tensor products and bimorphisms by B. Banachewski and E. Nelson studies tensor products (defined by classifying bimorphisms) in concrete categories. It is quite interesting that their main existence theorem gives an alternative, quite explicit construction of the tensor product of two modules (or any other algebraic structures). If $M,N$ are $R$-modules with underlying sets $|M|,|N|$, consider $$P=\bigoplus_{m \in |M|} N \oplus \bigoplus_{n \in |N|} M$$ with the natural inclusions $i_m : N \to P$  for $m \in |M|$ and $j_n : M \to P$ for $n \in |N|$. Let $U=\langle i_m(n)-j_n(m) : (m,n) \in |M| \times |N| \rangle$. Then $P/U$ is a model for $M \otimes_R N$. Question 1 . Is there any other paper or book at all which mentions this construction? Or is it well-known? Question 2 . Is there a textbook introducing tensor products and gives this construction as a proof that it exists? Question 3 (subjective): Isn't this construction more explicit than the usual one (which starts with the free module on $|M| \times |N|$ and mods out bilinear relations)? It only uses direct sums, generated submodules, and quotients, no free modules are needed. What do you think, do you favor it? Is it suited for the use in textbooks and classes? If you are a teacher or professor, would you consider using this construction in your class? What are your reasons? I have found a smiliar ""free module""-free construction of the module of differentials $\Omega^1_{A/R}$ for an $R$-algebra $A$: The $R$-linear map $A \otimes_R A \to A \otimes_R A$, $a \otimes b \mapsto  ab \otimes 1 - b \otimes a - a \otimes b$ extends to an $A$-linear map $(A \otimes_R A) \otimes_R A \to A \otimes_R A$, when $A$ acts on the right. Let $\Omega^1_{A/R}$ be its cokernel, and $d(a)$ the image of $a \otimes 1$. The universal property is immediate. I would like to ask the same questions as above.","['education', 'abstract-algebra', 'tensor-products', 'reference-request', 'modules']"
291671,Open Mapping Theorem: counterexample,"The Open Mapping Theorem says that a linear continuous
surjection between Banach spaces is an open mapping.
I am writing some lecture notes on the Open Mapping Theorem.
I guess it would be nice to have some counterexamples.
After all, how can you appreciate it's meaning without a
nice counterexample showing how the conclusion could fail
and why the conclusion is not obvious at all. Let $\ell^1 \subset \mathbb{R}^\infty$ be the set of sequences
$(a_1, a_2, \dotsc)$, such that $\sum |a_j| < \infty$.
If we consider the $\ell^1$ norm $\|\cdot\|_1$ and
the supremum norm $\|\cdot\|_s$, then,
$(\ell^1, \|\cdot\|_1)$ is complete,
while $(\ell^1, \|\cdot\|_s)$ is not complete. In this case, the identity
$$
  \begin{array}{rrcl}
    \mathrm{id}:& (\ell^1, \|\cdot\|_1)& \to &(\ell^1, \|\cdot\|_s)
    \\
                & x & \mapsto & x
  \end{array}
$$
is a continuous bijection but it is not open. I want a counterexample in the opposite direction.
That is, I want a linear continuous bijection
$T: E \to F$ between normed spaces $E$ and $F$
such that $F$ is Banach but $T$ is not open.
This is equivalent to finding a vector space
$E$ with non-equivalent norms $\|\cdot\|_c$ and $\|\cdot\|_n$,
such that $E$ is complete when considered the norm $\|\cdot\|_c$,
and such that
$$
  \|\cdot\|_c
  \leq
  \|\cdot\|_n.
$$
The Open Mapping Theorem implies that $\|\cdot\|_n$ is
not complete. So, is anyone aware of such a counterexample?","['examples-counterexamples', 'normed-spaces', 'functional-analysis']"
291678,Estimating rate of blow up of an ODE,"Suppose I have a differential equation $x'=f(x)$ and $f(x)>0$ grows super-linearly. I.e., $\lim_{|x| \rightarrow \infty} |f(x)|/|x| \rightarrow \infty$. Several related questions: (1) Can I conclude blow up of a solution for some initial conditions? (2) Via taylor series, or whatever relevant approximations of $f(x)$, can we estimate the rate of blow up? And (3), again by approximation, can we estimate given an initial condition when this blow up occurs? I have in mind $x'=x^p$ which has solution $x=[(1-p)t-C]^{-1/(p-1)}$. There is finite time blow up of order $O(t^{1/(p-1)})$. Given an initial condition $x(0)=X$, we have blow up occur at $X^{1-p}/(p-1)$.","['asymptotics', 'ordinary-differential-equations', 'analysis']"
291684,Quadratic ODE systems,"Linear ODE systems $x'=Ax$ are well understood. Suppose I have a quadratic ODE system where each component satisfies $x_i'=x^T A_i x$ for given matrix $A_i$. What resources, textbooks or papers, are there that study these systems thoroughly? My guess is that they aren't completely understood, but it would be good to know more about what has been done.","['ordinary-differential-equations', 'soft-question']"
291709,Is there a general algorithm that can be implemented via a computer program that can identify the function being represented by a graph?,"It might help understanding my question to think of the hypothetical situation in which I draw a seemingly random function on a piece of paper (with an accurate coordinate axis already on the paper), and I scan my drawing into the computer.  Then I open an application that can ""look"" at the graph (as a set of data points, maybe?  I don't know how such an algorithm would work) and identify the function's corresponding equation. I know there are polynomial curve fitting methods, but I was wondering if there was a more general algorithm for identify any type of function's equation in their most used form.","['algorithms', 'graphing-functions', 'functions']"
291724,Calculus question: finding tangent line using limit (first principles for derivative),How would I solve the following? Find an equation for the tangent line to the graph $f(x)=3x^2-4x$ at the point $x=-1$ What I did is $3(-1+h)^2-4(-1+h)-7)$ $3-6h+3h^2+4-4h-7)$ As h approaches zero $(2h+3h^2)/h$ limit equal $2$ then the equation I wrote is $y-7=2(x+1)$ But I am not sure if it is correct.,"['calculus', 'derivatives', 'limits']"
291729,"Comparing $\large 3^{3^{3^3}}$, googol, googolplex",How to show that $\large 3^{3^{3^3}}$ is larger than a googol ($\large 10^{100}$) but smaller than googoplex ($\large 10^{10^{100}}$). Thanks much in advance!!!,"['exponentiation', 'big-numbers', 'number-theory']"
291731,Prove $\sum_{i=1}^n$$w_i^2\geq\frac{1}{n}$ given $\sum_{i=1}^n w_i=1$,"I was looking at my stats textbook and they claim that the sample variance of a weighted distribution involving i.i.d. $x_i$s will be smallest when each of the weights is equal. I follow this argument up to the point where I reach $\sum_{i=1}^n$$w_i^2\geq\frac{1}{n}$ given $\sum_{i=1}^n w_i=1$ (this result obtained due to the fact that $Var(\bar{x_w})= \sigma^2\sum_{i=1}^nw_i^2$ and $Var(\bar{x})=\frac{\sigma^2}{n}$, so setting them equal and cancelling the $\sigma^2$ on each side yields that inequality, where $\bar{x_w}$ is the weighted average). Trying out a few examples, it seems pretty obvious that the inequality holds, but the book offers no mathematical justification and I was hoping someone here could help put it more concretely - I'm not sure how to approach it myself. Any thoughts?","['statistics', 'inequality']"
291742,Integrate: $\int x(\arctan x)^{2}dx$,"I'm not sure how to start I think we have to use integration by parts
$$\int x(\arctan x)^{2}dx$$",['integration']
291758,Conditional independence property: weak union,"Let $(X,Y,W,Z)$ be disjoint sets of random variables each with finite space. Then prove that if $\Pr(X\mid W,Y \cup Z)=\Pr(X\mid W)$ then $\Pr(X\mid Y,Z \cup W) = \Pr(X\mid Z \cup W)$. This is sometimes referred to as weak union in conditional independence. i am having hard time to prove this. Can someone help me to prove this? Thanks",['probability']
291774,Extension of continuous function,"The question is: Let $(K,\rho)$ be compact metric space. $F\subset K$ closed. $f:F\rightarrow \mathbb{R}$ continuous. Is there a continuous extension of $f$ on $K$? Attempt: Suppose there exists neighbourhood $G$ of $F$; $G=\{x\in K : \rho(x,F)\leq \epsilon\}$. That for every $x\in G$ there exists exactly one $y\in F$ that $\rho (x,F) = \rho (x,y)$. Than define $f(x) = f(y)\frac{\epsilon - \rho(x,y)}{\epsilon}$ for $x\in G$. Where $y\in F$ and $\rho(x,F) = \rho(x,y)$. $f(x)=0$ for $x\in K \setminus G$ Function defined like this should be continuous. Problem is that not for every $F$ exists neighbourhood $G$ with desired properties. Is there a way to fix this? Like define functions $f_\epsilon$ for $\epsilon > 0$. And than somehow mix those functions.","['metric-spaces', 'continuity', 'functions']"
291803,$ y=y_1(t)+y_2(t)$ is solution of $y'+p(t)y=g(t)$ if $y=y_1(t)$ is one of $y'+p(t)y=0$ and $ y=y_2(t)$ is one of $y'+p(t)y=g(t)$,Let $y=y_1(t)$ be a solution of $y'+p(t)y=0$ and let $ y=y_2(t)$ be a solution of $y'+p(t)y=g(t)$. How can we show that $ y=y_1(t)+y_2(t)$ is also a solution of $y'+p(t)y=g(t)$? I'm not really sure how to approach the problem. It's in the section dealing with differences between linear and nonlinear equations.,['ordinary-differential-equations']
291808,Solutions of $\frac{1}{\cos \theta} = a \sin \theta - b$,"One of my math professors and I are working on a physics problem involving spinning a chain, and we decided to go as simple as possible and work out the solution explicitly for that case (a long rod hanging from a hinge rotating in a horizontal circle). Then we could hopefully work up from there. In the end, we boiled it down to the point where we had an equation of this form: $$\frac{1}{\cos \theta} = a \sin \theta - b$$ Depending on the values of $a$ and $b$, there are $0$, $1$, $2$, $3$, or $4$ solutions for $\theta$ in this equation. What I'm curious about is whether there are formulas in terms of $a$ and $b$ that will give these solutions. As an aside, this situation actually reminds me of quadratics - they have $0$, $1$, or $2$ solutions, the solutions are given by the quadratic formula, and the value of $b^2-4ac$ indicates how many real-valued solutions there are. I'm looking for something similar for the equation I've given above, and WolframAlpha is being no help (gasp!).",['algebraic-geometry']
291818,How can I evaluate $\lim_{x\to1}\frac{\sqrt{5-x}-2}{\sqrt{2-x}-1}$ without invoking l'Hôpital's rule?,"In the math clinic I work at, somebody in a Calculus 1 class asked for help with this limit problem. They have not covered basic differentiation techniques yet, let alone l'Hôpital's rule. $$\lim_{x\to1}\frac{\sqrt{5-x}-2}{\sqrt{2-x}-1}$$ We have tried various algebraic techniques, such as multiplying the top and bottom of the fraction by the conjugate of the denominator, but haven't had any success at getting rid of the indeterminate form. How can this limit be solved, using only techniques that would be available to a beginning Calculus 1 student?",['limits']
291822,Potential Munkres Error? (Impossible!),"This is problem 19.10 (d) from Munkres' topology text, the second edition: Let $A$ be a set; let $\{X_\alpha\}$ be an indexed family of spaces; and let $\{f_\alpha\}$ be an indexed family of functions, $f_\alpha\colon A \to X_\alpha$ .  Let $S_\alpha = \{f_\alpha^{-1}(U_\alpha) \ | \ U_\alpha \ \text{open in} \ X_\alpha\}$ , and let $S$ be the union of $S_\alpha$ over all $\alpha$ .  Let $T$ denote the topology on $A$ generated by the subbasis $S$ . (d) Let $X = \prod_\alpha X_\alpha$ with the product topology.  Let $f\colon A \to X$ be defined by the equation $f(a) = (f_\alpha(a))$ ; let $Z$ denote the subspace $f(A)$ of the product space $X$ .  Show that the image under $f$ of each element of $T$ is an open set of $Z$ . I cannot figure this out.  I think that I am confused with some fundamentals regarding the product topology, because I have a proposed counter example to this claim.  I would like someone to show me where my confusion lies, assuming the claim is true. Counter Example:  Let $A = \mathbb R$ .  For all positive integers $i$ , let $X_i = \mathbb R$ , with the standard topology, and let $f_i = I = \ \text{the identity map on} \ \mathbb R$ .  Then, $X = \mathbb R^\omega$ and $Z = f(A) = \mathbb R^\omega$ .  Now, an example of an open set in $T$ is the subbasis element $I^{-1}((0,1)) = (0,1)$ .  But $f((0,1))$ is $(0,1)^\omega$ .  If this image were open in $Z = \mathbb R^\omega$ with the product topology, then there would exist a basis element of $\mathbb R^\omega$ lying completely within $(0,1)^\omega$ .  This is impossible, since basis elements of the product topology on $\mathbb R^\omega$ look like the countable union of open subsets of $\mathbb R$ , $U_i$ , where $U_i$ equals $\mathbb R$ for all but finitely many $i$ .",['general-topology']
291833,"are there any ""deep"" reasons for representing linear systems as $Ax=b$ instead of $xA=b$?","Nowadays we represent the system of $m$ linear equations
$$\sum_{i=1}^na_{1i}x_i=y_1$$
$$\sum_{i=1}^na_{2i}x_i=y_2$$
$$\vdots$$
$$\sum_{i=1}^na_{mi}x_i=y_m$$
as $\mathbf{Ax}=\mathbf{y}$, where $(A)_{ij}=a_{ij}$ is an $m\times n$ matrix, $\mathbf{x}$ is an $n\times 1$ column vector, and $\mathbf{y}$ is an $m\times 1$ column vector. Call this the ""column picture."" But we could just as well have represented it by the transposed equation
$$\mathbf{y}^T=\mathbf{x}^T\mathbf{A}^T$$
where we now deal with row vectors rather than column vectors. Call this the ""row picture."" I have two questions: (1) Can anyone point to a specific historical reference in which a linear system of equations was represented using the row picture? (2) Are there any deep reasons for preferring the column picture to the row picture? Or is it fair for us to describe the column picture as totally arbitrary?",['linear-algebra']
291845,List of various vector (linear) spaces [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question The vector (linear) space is defined as a non-empty set L over a field F , where two relations (binary operations) are defined: Addition $ \oplus: L \times L \longrightarrow L $ Scalar multiplication $ \odot: F \times L \longrightarrow L $ Although we call these relations as addition and scalar multiplication, both of these relations can have arbitrary forms, which do not need to have anything in common with the traditional apprehension of the addition and the multiplication (for example the addition and the multiplication of real numbers). From the set-theoretical point of view are both operations just mappings between two sets, closed under the relations, which need to meet following conditions (axioms of linear space): Associativity of addition Commutativity of addition Identity element of addition Inverse elements of addition Distributivity of scalar multiplication with respect to vector addition Distributivity of scalar multiplication with respect to field addition Compatibility of scalar multiplication with field multiplication Identity element of scalar multiplication We are usually working with the linear space of complex numbers $\mathbb{C}^n$, because all linear spaces of dimension of $n$ are isomorphic. In other words, we can use linear transformation between two different linear spaces and equivalently solve the problem in some well known linear space (usually $\mathbb{C}^n$) and then transform it back. My question is: What kinds of linear spaces do you know? What fields and binary operations compose the linear space. If you can, please, also note the physical application of such a linear space. I will just summarize the ones I know: Vector space $\mathbb{C}^n$, field of complex numbers $\mathbb{C}$ Let $\mathbf{x},\mathbf{y} \in \mathbb{C}^n$ and $\alpha \in \mathbb{C}$, where $\mathbf{x} = (x_1, x_2, ..., x_n)$ and $\mathbf{y} = (y_1, y_2, ..., y_n)$. Operation of addition $\oplus$ and scalar multiplication $\odot$ are defined as $\mathbf{x} \oplus \mathbf{y} \triangleq (x_1 + y_1, x_2 + y_2, ..., x_n + y_n)$ $\alpha \odot \mathbf{x} \triangleq (\alpha \cdot x_1, \alpha \cdot x_2, ..., \alpha \cdot x_n)$ (Note: Plus symbol and dot symbol in the brackets denote operations of addition and multiplication of complex numbers.) Function space, field of complex numbers $\mathbb{C}$ $ f \oplus g \triangleq f(x) + g(x) $ $ \alpha \odot f \triangleq \alpha f(x)$ Vector space of possitive real numbers $\mathbb{R}^+$, field of real numbers $\mathbb{R}$ $\mathbf{x} \oplus \mathbf{y} \triangleq x \cdot y$ $\alpha \odot \mathbf{x} \triangleq x^{\alpha}$ Vector space of matrices $\mathbb{F}^{m \times n}$ over field $\mathbb{F}$ $ (\mathbf{X} \oplus \mathbf{Y})_{i,j} \triangleq (\mathbf{X})_{i,j} + (\mathbf{Y})_{i,j}$ $ (\alpha \odot \mathbf{X})_{i,j} \triangleq \alpha(\mathbf{X})_{i,j} $","['vector-spaces', 'linear-algebra']"
291849,Uniqueness of Bounded Solutions to Dirichlet's Problem in the Half-Space,"Title basically says everything.  Prove that if $u\in C^{2}(\mathbb{R}^{n}_{+})\cap C(\bar{\mathbb{R}^{n}_{+}})$ is a bounded solution of the BVP $$\left\{\begin{array}
-\Delta u=0&\text{in}\;\mathbb{R}^{n}_{+}\\
u=g&\;\text{on}\;\partial\mathbb{R}^{n}_{+},
\end{array}\right.$$
then it is unique. Various tools I have in mind are maximum principle, mean value formulas, Liouville's theorem, ""energy"" functionals, and Harnack's inequality, uniqueness of Green's function, Hopf's lemma, etc....but in all my scratch work to prove the problem, I keep running into technical difficulties in working with the boundary at infinity. I arrived at a proof by using a result from Evans exercise #2.5.10 (Schwarz reflection principle): Proof. Consider the ball $B_{R}(0)$ and suppose $u_{R}$ is a bounded solution to the above problem, but posed on the domain $B_{R}^{+}(0):=\{x:x\in B_{R}(0), x_{n}>0\}.$  Let $v_{R}$ be another bounded solution and define $w_{R}:v_{R}-u_{R}.$  Then $w_{R}=0$ on $\partial B^{+}_{R}(0)\cap\{x_{n}=0\}$, and the Schwarz reflection principle states that the odd extension of $w_{R}$ to $B^{-}_{R}(0)$ is harmonic in all of $B_{R}(0)$ (the proof of this is trivial if we assume $w\in C^{2}(\bar{B^{+}_{R}(0)})$ by using the mean-value formulas, and only a little more difficult under the present assumptions by using Poisson's formula for the ball).  Now, $u_{R},v_{R}$ both being bounded implies $w_{R}$ is also bounded.  Sending $R\to\infty$, we find that $w:=\lim_{R\to\infty}w_{R}$ is a bounded and harmonic in $\mathbb{R}^{n}$, from which it follows $w_{R}\equiv0$ by Liouville's theorem and the fact that $w=0$ on the hyperplane $x_{n}=0.$ Let $u_{1}$ be a bounded solution to the problem above.  Suppose $u_{2}$ is another solution and define $$w:=u_{1}-u_{2}.$$  Then $w=0$ on $\partial\mathbb{R}^{n}_{+}.$","['partial-differential-equations', 'analysis']"
291850,Prove that $\int_{0}^{\infty }\frac{x^{a-3/2}dx}{[ x^2+( b^2-2)x+1]^a}=b^{1-2a}\frac{\Gamma(1/2)\Gamma(a-1/2)}{\Gamma(a)}$,"How can one prove that
$$I\left( a,b \right)=
\int_{0}^{\infty }\frac{x^{a-\frac{3}{2}}dx}{\left[ x^2+\left( b^2-2 \right)x+1 \right]^a}=b^{1-2a}\frac{\Gamma \left( \frac{1}{2} \right)\Gamma \left( a-\frac{1}{2} \right)}{\Gamma \left( a \right)},\ $$
where $a>\frac12,\ b\in \mathbb{R}^+$?","['improper-integrals', 'calculus', 'integration', 'real-analysis', 'gamma-function']"
291884,"Why $f(x + x^3, y + y^3) \in L^1(\mathbb{R}^2)$, when $f(x, y) \in L^2(\mathbb{R}^2)$?","How show that $f(x + x^3, y + y^3) \in L^1(\mathbb{R}^2)$, when  $f(x, y) \in L^2(\mathbb{R}^2)$? Can someone help me? Thank you!","['lebesgue-integral', 'measure-theory', 'lp-spaces', 'real-analysis']"
