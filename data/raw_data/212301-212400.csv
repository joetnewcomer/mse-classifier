question_id,title,body,tags
4281492,Ringed space but not locally ringed space,Give an example of a ringed space which is not locally ringed space.Why we need to go to locally ringed space if all ringed spaces are locally ringed spaces before defining schemes in Hartshone?Please clarify the difference in locally ringed spaces and ringed spaces?,"['commutative-algebra', 'coherent-sheaves', 'category-theory', 'algebraic-geometry', 'schemes']"
4281511,Is this function of partitions one-to-one?,"Suppose we have a set of integers $H=\{1,2, ...n\}$ . Let $A$ a set of partitions of H into $n/2$ pairs $\{\{x_1,y_1\},\{x_2,y_2\}, ...,\{x_{n/2},y_{n/2}\}\}$ and
function $f:A \rightarrow Z^n$ where $f(\{\{x_1,y_1\},\{x_2,y_2\}, ...,\{x_{n/2},y_{n/2}\}\})=\cup \{x_i*y_i\}$ . For a set of integers $\{1,2, ...8\}$ and the pairing $\{\{3,6\},\{1,7\},\{2,4\},\{5,8\}\}$ $f(\{\{3,6\},\{1,7\},\{2,4\},\{5,8\}\})=\{18,7,8,40\}$ Is this function injective?",['functions']
4281514,generalization of 2nd derivative test to multi dimensional when the hessian is inonclusive,"We all know the 2nd derivative test in its original form, if $f'(x)=0$ , then if $f''(x)<0$ the point is max, and if $f''(x)>0$ the point is min.
We also know the generalization for the case is inconclusive with one variable:
(I) https://en.wikipedia.org/wiki/Derivative_test#Higher-order_derivative_test We also know the generalization for multi variable:(II) https://en.wikipedia.org/wiki/Second_partial_derivative_test#Functions_of_many_variables The question is if there is a generalization of the two,
say for multivariable if none of the conditions are met, then we can take the next derivative until we find a derivative which is not zero and use the conditions in (II) to decide whether it's a max or min. So prove or disprove, one can just take nth derivative and check using the II conditions, that if an extremum is a max or min, or disprove via a counterexample .","['maxima-minima', 'multivariable-calculus', 'partial-derivative', 'optimization', 'derivatives']"
4281528,"Prove or disprove: $\bigcup_{n\ge1}\left[0,\frac n{n+1}\right]=[0,1)$ and $\bigcap_{n\ge1}\left[0,\frac n{n+1}\right]=\{0\}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question For integers $ n \ge 1 $ , let $ A _ n = \left \{ x \in \mathbb R \Biggm | 0 \le x \le \frac n { n + 1 } \right \} = \left [ 0 , \frac n { n + 1 } \right ] $ . Prove or disprove the statements below: $ \bigcup _ { n \ge 1 } A _ n = [ 0 , 1 ) $ $ \bigcap _ { n \ge 1 } A _ n = \{ 0 \} $ My textbook for my class does not contain any notes on how to solve things like this. Any help solving these or links to sites that can show me how to solve things like this would be much appreciated.",['elementary-set-theory']
4281621,Finding perimeter with a grid based polygon,"I have been banging my head against this question for a while trying to come up with an answer that isn't going to be either weird to implement, not give me what I want all the time or just take too much time to get right. I have A grid of squares and I need to make a polygon out of the perimeter of them. I know: The centers of the squares that are inside the polygon (Those are the blue circles) The vertices of the squares (Those are the White dots) The edge vertices (if they are needed since I have that as well)
Since I am making a polygon out of this grid, I need the vertex order to be either clockwise or counter-clockwise since I can just reverse the list. Starting at the top right was only a choice I made since it's probably going to make my life the simplest starting from the outside most point then working around but it can be changed if need be. I have thought about getting angles from the center like this: but the problem was there was no way I could see about getting around intersections between the perimeter and the middle since they could be closer and wouldn't work. Then I thought about Taking the closest point and weighting the distance from the point we are on to each point left, with the distance from the middle to the points while still making the distance from point to point the closest it could be. The Minimum Weight approach Then I thought about making A tree and getting each connection but I would have to know the connections by walking the perimeter anyways. Using a tree and connecting triangles but that would require re-writing a lot of code Then I thought about some version of the postman problem, but it wouldn't guarantee going around, only going through the closest points and all of them ruling out maps with equal distances from each point to the neighboring centers like one in the form of an H Since it can go inside itself I can't use any convex hull methods, alpha hull is parameterized and could give me something I don't want, and a step by step walk around with signed fields deciding which way is the best in A situation where going two different directions are both good was too convoluted to work with. Any ideas would be gratefully appreciated; I have been working on this for a little too long and I'm probably overthinking something.","['functions', 'geometry', 'algorithms']"
4281624,An example about Lindelöf spaces,"Let $X$ a Lindelöf space and $A \subset X$ a closed subspace, we have seen here Closed subsets of Lindelöf spaces are Lindelöf that $A$ need to be Lindelöf.  My question is about the reciprocal : if $A$ is a Lindelöf subspace of a Lindelöf space $X$ , then $A$ need to be closed?? I guess not, but I cannot find an example, cause everithing I need is about compact sets. Can you give me some example?","['general-topology', 'lindelof-spaces']"
4281644,"""Pseudovertices"" of a triangle","Now asked at MO . Let $\mathbb{T}\subseteq(\mathbb{R}^2)^3$ be the set of ordered noncollinear triples of points in the plane. Say that a pseudovertex is a  function $\mu:C\rightarrow\mathbb{R}^2$ such that: $C$ is a dense open subset of $\mathbb{T}$ . $\mu$ is order-invariant and is continuous with respect to the usual topologies on domain and codomain. $\mu$ respects ""distortion-free"" maps: if $\alpha$ is a composition of translations, rotations, reflections, and scalings $(x,y)\mapsto(\lambda x,\lambda y)$ for nonzero $\lambda$ , then $\alpha(\mu(a,b,c))=\mu(\alpha(a),\alpha(b),\alpha(c))$ in the strong sense (if either side is defined, they're both defined and equal). If $a,b,c$ are non-collinear and $(a,b,c)\in C$ then $(\mu(a,b,c),b,c)\in C$ as well and $\mu(\mu(a,b,c),b,c)=a$ (so given $\{a,b,c,\mu(a,b,c)\}$ we cannot tell which three points were the original vertices). It is, as far as I can tell, not immediately obvious that pseudovertices exist at all. However, there is at least one very natural example: the orthocenter , modulo appropriate domain-tweaking (e.g. we need to throw out all right triangles) . This is in fact the only example I know, but I strongly suspect I'm missing an easy proof that there are infinitely many: Are there infinitely many pseudovertices? (I am prepared to be highly embarrassed.) I am more ambivalent however about the prospects for additional natural pseudovertices: Are there any pseudovertices, besides the (map corresponding to the) orthocenter, which are ""natural"" - e.g. have been given a name in the existing literature? (Incidentally, unless I'm missing something neither of the ""Shermanesque"" fourth-vertex candidates suggested at MO are pseudovertices due to the third requirement. That said, see Blue's comment below.) EDIT: user runway44 observed below that I'm essentially asking about continuous functions $\mu:\subseteq_{\mbox{dense open}}\mathbb{C}\setminus\mathbb{R}\rightarrow\mathbb{C}\setminus\mathbb{R}$ satisfying the following properties: $\mu$ commutes with conjugation and with the map $z\mapsto 1-z$ . $\mu({1\over z})={\mu(z)\over z}$ . $\mu\circ\mu=\mathsf{id}.$ (I'm including this observation here since I think it's worth keeping around, and comments are temporary.) SECOND EDIT: I should have also required that $G$ be connected : if we allow disconnected $G$ , the symmetry requirement becomes vacuous since the set of scalene triangles is dense open. The iterability requirement then needs to be weakened to only hold on a dense open subset of $G$ (otherwise the orthocenter doesn't count). That said, I may be having a silly moment but it's not entirely obvious to me that the question is trivial without these modifications, even if omitting them was obviously a mistake. For now: I prefer answers addressing the modified case, but I'm also interested in the looser question.","['euclidean-geometry', 'triangles', 'geometry']"
4281654,"A question about $SL(2,\mathbb{R})$","My professor gave me an exercise where I had to show that the special linear group $SL(2,\mathbb{R})$ is a lie subgroup of $GL(2,\mathbb{R})$ . I was able to do this part. However, I was then asked to do the following: All real $2\times 2$ matrices $\begin{pmatrix} a & b \\ c & d\end{pmatrix}$ can be identified with $(a,b,c,d) \in \mathbb{R}^4$ . In this way, $SL(2,\mathbb{R})$ can be thought of as a subset of $\mathbb{R}^4$ . In this correspondence, find all matrices in $SL(2,\mathbb{R})$ that are closest to the origin. I really don't have any idea how to approach this problem. The only things I have ever seen like this are Lagrange multipliers, but those don't seem to apply here. For reference, though this exercise is not in the text, our course is using Introduction to Smooth Manifolds by Lee.","['optimization', 'manifolds', 'lie-groups', 'differential-geometry']"
4281658,Inconsistency when solving IVP using Laplace Transform with Dirac Delta,"solving $\dot{x}(t) + x(t) = \delta (t) $ Using Laplace transform for $x(0) = 1$ , we get: $sX(s)-1 + X(s) = 1$ $X(s) = \frac{2}{s+1}$ so, $x(t) = 2e^{-t}$ However, evaluating at t=0, $x(0) = 2 \neq 1$ This disagrees with the initial condition. What went wrong here?","['initial-value-problems', 'laplace-transform', 'ordinary-differential-equations', 'dirac-delta']"
4281663,Cyclicity of the trace for operators,"I know that if I have two operators $A$ and $B$ and one is bounded and the other is trace class, then $$
\mathrm{Tr}(AB) = \mathrm{Tr}(BA).
$$ Another case where this works is when $A$ and $B$ are both Hilbert-Schmidt operators. But I heard that it is actually sufficient to have $\mathrm{Tr}(|AB|)<\infty$ and $\mathrm{Tr}(|BA|)<\infty$ . Has anyone a reference about that? Are there other cases where the cyclicity of the trace still works, or at least where it works ""in a certain sense""?
In particular I am interested in the case where $A$ is a really nice operator and $B$ is unbounded. For example, say $B=x$ is the unbounded operator of multiplication by $x\in\Bbb R$ and $A$ is a compact positive operator acting on $L^2$ functions $\varphi$ through the formula $$A\varphi(x) = \sum_j \lambda_j\, \psi_j(x) \int_{\Bbb R} \psi_j\,\varphi$$ with $\sum_j \lambda_j\int_{\Bbb R} |\psi_j(x)|^2\,(1+|x|)\,\mathrm d x< C$ . Then $$
\mathrm{Tr}(AB) = \mathrm{Tr}(BA) = \sum_j \lambda_j \int_{\Bbb R} |\psi_j(x)|^2\,x\,\mathrm d x
$$ Remark: Another quite borderline case where I know how to do the proof is if $A$ and $B$ are positive operators and $\sqrt A\sqrt B$ is an Hilbert-Schmidt operator and one defines $\mathrm{Tr}(AB) := \mathrm{Tr}(\sqrt B\,A\,\sqrt B) = \|\sqrt A\sqrt B\|_2^2$ . Then by invariance of the Hilbert-Schmidt norm by taking the adjoint, $$
\mathrm{Tr}(AB) = \|\sqrt A\sqrt B\|_2^2 = \|\sqrt B\sqrt A\|_2^2 = \mathrm{Tr}(BA).
$$","['operator-theory', 'trace', 'functional-analysis']"
4281680,How to solve $y''(t)=|y(t)|$,"Solve differential equation: $y''(t) = |y(t)|$ My attempt to solution: I could not find all solutions possibles to the problem. And I do not know how to continue. $1)$ We know that $y''(t) = |y(t)| \Rightarrow (y''(t))^2=(|y(t)|)^2 \Rightarrow (y''(t))^2=(y(t))^2$ $ \Rightarrow(y''(t) - y(t))(y''(t) + y(t))=0$ $2)$ Define: $y_1''(t)=y_1(t)$ and $y_2''(t)=-y_2(t)$ $3)$ So, we have three possibilites, First is: For all real $t$ , $y_1(t)$ is solution of $y''(t) = |y(t)|$ Second is:  For all real $t$ , $y_2(t)$ is solution  of $y''(t) = |y(t)|$ Third is: $y_1(t)$ is solution  of $y''(t) = |y(t)|$ for some values of $t$ , and $y_2(t)$ is solutions  of $y''(t) = |y(t)|$ for the remaining values of t. $4)$ I test the First situacion, and I find the condition for it: The general solution of $y_1''(t)=y_1(t)$ is $y_1(t) = C_1.e^{t}+C_2.e^{-t}$ So, if $y_1(t)$ is solution for all t, implies that $y_1''(t)=|y_1(t)| \ge 0$ $y_1''(t)= C_1.e^{t}+C_2.e^{-t} \ge 0$ for all real t The condition of $C_1, C_2$ is that $C_1 \ge 0$ and $C_2 \ge 0$ $5)$ I test the Second situation, and I find that is impossible: The general solution of $y_2''(t)=-y_2(t)$ is $y_2(t) = C_3.\cos{t}+C_4.\sin{t}$ So, if $y_2(t)$ is solution for all t, implies that $y_2''(t)=|y_2(t)| \ge 0$ $y_2''(t)= -C_3.\cos{t}-C_4.\sin{t} \ge 0$ for all real t It is impossible, unless $C_1 = C_2 = 0$ that implies $y(t) = 0$ for all real t. $6)$ The last possibility is the Third situation, and I could not to find this As $y(t)$ is a continuous function, when one of the solutions ceases to be valid and passes to another solution, both must be equal at these exact instants and differenciable. And I don't know how to find this. Also, I suspect we will find a multitude of solutions for the third case.",['ordinary-differential-equations']
4281692,How can I give an CW complex structure to the space $X$ using a covering map $p:X\rightarrow E$ where $E$ has structure of CW complex?,"I think that idea is to take an n-cell $X_{n}$ of space $E$ a proof $p^{-1}(X_{n})$ is a union of n-cells in $X$ . I found in other post the idea is right, but I can't prove that, in my case n-cells are open, therefore I can't use compactness argument of cells.
I really appreciate your help.","['general-topology', 'cw-complexes', 'algebraic-topology', 'covering-spaces']"
4281753,Is this idea for solving multivariable recurrences close to correct?,"According to Wikipedia , the binomial coefficient identity offers a good example of a multivariable recurrence relation.  I believe the idea is to treat the top and bottom slots of the choose operator each as an input to an unknown function, leading to the equation $$C[n,\ k] = C[n - 1,\ k - 1] + C[n - 1,\ k]$$ where we pretend not to know in advance that $C$ is the choose operator.  My goal is to try to solve this analytically by extrapolating from the patterns relating discrete and continuous equations in the single variable case. First, I shifted some things to get $C[n + 1,\ k + 1] - C[n,\ k + 1] - C[n,\ k] = 0$ , and considered the most similar partial differential equation, $c_{_{NK}} - c_{_K} - c = 0$ , where $c$ , $N$ , and $K$ are the continuous versions of $C$ , $n$ , and $k$ , respectively.  This was based on the observation that incrementing inputs and taking derivatives have the same effect on the characteristic equation in linear, single variable equations. Next, I tried to solve the partial differential equation using separation of variables.  This led to $c = (Ae^{\frac{1}{\lambda} N})(Be^{\lambda K + K})$ , where $A$ and $B$ are the arbitrary constants from each separated ODE. Finally, I want to get rid of the exponential nature of the solution and write the solution to the discrete equation.  The most natural thing for me to write based on the single variable pattern is $$C = a(\frac{1}{\Lambda})^n b(\Lambda + 1)^k$$ I just yeeted the $e$ from each factor and pulled the coefficients of the independent variables down (and switched the case of each letter again for clarity of notation).  I'm not sure if $a$ and $b$ can be lumped, since they may have been affected by two different discretizations. Actually, I'm not sure if any of this is valid.  Is this process or something similar a valid way to solve multivariable recurrence relations?  If not, why is there a nice relationship between analytically solving discrete and continuous equations in the single variable case but not in the multivariable case?","['recurrence-relations', 'solution-verification', 'combinatorics', 'discrete-mathematics', 'partial-differential-equations']"
4281811,"Proof verification: if $a_n, b_n>0$ and $\lim\limits_{n \to\infty} \frac{a_n}{b_n}=L_1$ with $L_1>0$, then if $\sum a_n$ converges, so does $\sum b_n$","I'm trying a proof technique I'm not used to for limits on fractions, which attempts to avoid an epsilon-delta approach similarly to how the single variable chain rule is proved in baby Rudin, and I was wondering if it works. Any help or tips are very welcome! Statement: If $a_n, b_n>0$ and $\lim \limits_{n \to \infty} \frac{a_n}{b_n} = L_1$ with $L_1 > 0$ , then if $\sum_{n \in \mathbb N} a_n$ converges, so does $\sum_{n \in \Bbb N} b_n$ Proof: Suppose $\sum_{n \in \Bbb N} a_n$ converges to $L_a$ . Since $\lim \limits_{n \to \infty} \frac{a_n}{b_n} = L_1$ , we have $$a_n = b_n(L_1 + \varepsilon(n))$$ with $\varepsilon(n) \to 0$ as $n \to \infty$ . Therefore, $$ \sum_{n \in \Bbb N} a_n = \sum_{n \in \Bbb N} b_n(L_1 + \varepsilon(n)) = L_a, $$ so $$\lim \limits_{n \to \infty} \sum_{i=1}^n b_i(L_1 + \varepsilon(n)) = L_a,$$ and therefore, $$\sum_{i=1}^nb_i=\frac{L_a+\mu(n)}{L_1 + \varepsilon(n) } ,$$ where $\mu(n) \to 0$ as $n \to \infty$ . Therefore, letting $n \to \infty$ results in $\sum_{i=1}^nb_i = \frac{L_a}{L_1}$ , so $\sum_{n \in \mathbb N}b_n$ converges. $\blacksquare$","['limits', 'summation', 'solution-verification', 'real-analysis']"
4281836,Intuition behind Legendre convex function,"I came across the definition of Legendre functions and Legendre transformations in my studies (in the sense of convex analysis) and I started searching about it. I found a definition in Rockefellar's 1996 ""Convex analysis"" book. So let $\Psi$ be a proper and closed convex function, meaning $\text{dom}(\Psi) \neq \emptyset$ and $\forall \boldsymbol{x} \in\text{dom}(\Psi), \Psi(\boldsymbol{x}) >-\infty$ and $\Psi$ is semi-continuous. Let $\Theta=\text{int}(\text{dom}(\Psi))$ , where $\text{int}(\text{dom}(f))$ denotes the interior domain of $f$ . Then $(\Theta, \Psi)$ is said to be a Legendre-type convex function or simply Legendre if and only if: $\Theta \neq \emptyset$ . $\Psi$ is strictly convex and diferentiable in $\Theta$ . $\forall \theta_b \in bd(\Theta), \lim_{\theta\to\theta_b} || \nabla \Psi(\theta)|| = \infty $ , where $\boldsymbol{\theta} \in \Theta$ and $bd$ denotes the boundary. While the first two conditions are clear to me, the meaning of $\lim_{\theta\to\theta_b} || \nabla \Psi(\boldsymbol{\Theta})|| \to \infty $ is a bit obscure because I'm having trouble visualizing this to grasp the intuition behind it. Why is this precisely required or useful?","['legendre-transformation', 'analysis', 'functions', 'legendre-functions', 'convex-analysis']"
4281837,How can I prove that $\sum_{n=0}^\infty\frac{(2n)!}{2^{2n}(n!)^2(2n+1)}$ converges?,"Note : If this is a duplicate question, and I'm pretty sure it is, I have not been able to find a post that it duplicates. The closest one I could find asks about the divergent series $$\sum_{n=0}^\infty \frac{(2n)!}{2^{2n}(n!)^2}$$ Now for the problem: I'd like to prove that the series $$\sum_{n=0}^\infty\frac{(2n)!}{2^{2n}(n!)^2(2n+1)}$$ converges. This series is the formal result when one evaluates the Maclaurin series of $\sin^{-1}$ at $1$ . Appealing to Abel's theorem and consulting WolframAlpha, the series, if convergent, sums to $\pi/2$ . This is to be expected because $\sin^{-1}(1)=\pi/2$ . Given the presence of factorials and exponentials, my first idea was to try and apply the ratio test to the terms of the series. Unfortunately, the relevant limit evaluates to $1$ : \begin{align}
\frac{\frac{(2(n+1))!}{2^{2(n+1)}((n+1)!)^2(2(n+1)+1)}}{\frac{(2n)!}{2^{2n}(n!)^2(2n+1)}} &= \frac{(2n+2)!}{2^{2n+2}((n+1)n!)^2(2n+3)}\cdot\frac{2^{2n}(n!)^2(2n+1)}{(2n)!}\\
&= \frac{(2n)!(2n+1)(2n+2)}{2^2(n+1)^2(n!)^2(2n+3)}\cdot\frac{(n!)^2(2n+1)}{(2n)!}\\
&= \frac{(2n+1)^2\cdot 2(n+1)}{2^2(n+1)^2(2n+3)}\\
&= \frac{4n^2+4n+1}{2(n+1)(2n+3)}\\
&= \frac{4n^2+4n+1}{4n^2+10n+6}\to 1\text{ as }n\to\infty\\
\end{align} It is pointless to apply the root test since it is inconclusive whenever the ratio test is. I would consider trying to leverage the identity $$\frac{(2n)!}{2^{2n}(n!)^2}=\frac{2}{\pi}\int_0^\frac{\pi}{2}\sin^{2n}(x)dx$$ but this requires knowledge of the decay rate of $\int_0^\frac{\pi}{2}\sin^{2n}(x)dx$ , something I do not yet possess. I doubt integration by parts will be useful, since this is how you prove the integral identity, and obvious substitutions like $x=\sin^{-1}(t)$ yield seemingly non-fruitful expressions like $$\frac{2}{\pi}\int_0^1\frac{x^{2n}}{\sqrt{1-x^2}}dx$$ I don't have any other ideas. Could I get some assistance?","['calculus', 'convergence-divergence', 'sequences-and-series']"
4281849,"Why do we say -$11\div 3$ is $-4$ with remainder $1$, instead of $-3$ with remainder $-2$?","In the book Discrete Mathematics and Its Applications, 8e, Kenneth Rosen the quotient and remainder when $-11$ is divided by $3$ are specified as $-4$ and $1$ respectively.  I would appreciate some help in understanding how we got there.  It doesn't gel well with what I was taught in my elementary school. Specifically, since $-11=(-3)\times 3+(-2)$ , why do we not say that the quotient is $-3$ and the remainder is $-2$ ?","['arithmetic', 'discrete-mathematics']"
4281904,Boundedness of a subset using linear functionals,"Let $S$ be a subset of the $\mathbb K-$ normed vector space $V$ such that $$\sup_{x\in S}|T(x)|<\infty$$ for any $T\in\mathcal L(V,\mathbb K)$ . Prove that $S$ is a limited subset of $V$ . Hello everyone, I found this exercise but I can't have ideas to start with, I have assumed that $S$ is not bounded, therefore for all $M>0$ exists $x_0\in S$ such that $\|x_0\|_V>M$ . Or I can say that there is a sequence that converges to infinity. But since $\sup_{x\in S}|T(x)|<\infty$ , then there is a sub-succession $(x_{n_j})$ such that $T(x_{n_j})$ is convergent. Then I don't know what to do. Any ideas please. I found this but I don't understand it well.","['normed-spaces', 'functional-analysis', 'dual-spaces']"
4281905,Probability of Random Variable Question | Probability Error,"I had this question in my past paper: Bags of sugar are packed in boxes, each box containing 20 bags. The masses of the boxes, when
empty, are normally distributed with mean 0.4 kg and standard deviation 0.01 kg. The masses of the
bags are normally distributed with mean 1.02 kg and standard deviation 0.03 kg. i) Two full boxes are chosen at random. Find the probability that they differ in mass by less than 0.02 kg I did the calculations and I got my probability as 2 x 0.4582 however in the marking scheme they've deducted the final answer from 1. Why is that so?","['statistics', 'probability', 'random-variables']"
4281954,help with $\lim_{n\to\infty}\left(\frac{1}{n^{2}}+\frac{2}{n^{2}}+\cdots+\frac{n-1}{n^{2}}\right)$,"Is my solution right? \begin{align}
\lim_{n\to\infty}\left(\frac{1}{n^{2}}+\frac{2}{n^{2}}+\cdots+\frac{n-1}{n^{2}}\right)	&=\lim_{n\to\infty}\left(\frac{1}{n^{2}}\left(1+2+3+\cdots+n-1\right)\right)\\
&=\lim_{n\to\infty}\left(\frac{1}{n^{2}}\left(\frac{(n-1)(1+n-1)}{2}\right)\right)\\
&=\lim_{n\to\infty}\left(\frac{1}{n^{2}}\left(\frac{n(n-1)}{2}\right)\right)\\
&=\lim_{n\to\infty}\left(\frac{n-1}{2n}\right)\\
&=\lim_{n\to\infty}\left(\frac{1}{2}\right)\\
&=\boxed{\frac{1}{2}}
\end{align}","['limits', 'sequences-and-series']"
4281963,How to tell if a complex function has this property,"Suppose we have a complex function of a complex variable $f(z)$ . How can we know if it satisfies (I don't know how to name this or type it in latex) $$
\overline{f(z)}=f(\bar z)
$$ I tried linear functions, polynomials, exponentials and I got yes for some and no for others. For polynomials, all the coefficients must be real, for exponentials, the power has to be real and so on. So I came up with this idea: If you can write down the formula of the function without having to type $i$ then you get this property. If not, the LHS and the RHS of the equation above disagree. Any one have the answer to that? I guess It should be some kind of theorem, no? All help greatly appreciated.","['complex-analysis', 'functions']"
4281980,General Geodesic Curvature Formula,"Suppose $(M, g)$ is a Riemannian manifold, and $\gamma: I \rightarrow M$ is a regular (but not necessarily unit-speed) curve in $M$ . Show that the geodesic curvature of $\gamma$ at $t\in I$ is $$\kappa(t)=\frac{\sqrt{|\gamma'(t)|^2|D_t\gamma'(t)|^2 - \langle \gamma'(t), D_t \gamma'(t) \rangle^2}}{|\gamma'(t)|^3}.$$ I've been searching for an error in my solution for hours but I can't find it. Here's my solution: Let $\tilde\gamma = \gamma \circ\varphi$ be a unit-speed parametrization of $\gamma$ . We start by listing identities. By the product rule, $D_t \tilde\gamma'(\varphi^{-1}(t))= D_t \frac{\gamma'(t)}{|\gamma'(t)|}\implies\\
D_t \tilde\gamma'(\varphi^{-1}(t))= -\frac{|\gamma'(t)|'}{|\gamma'(t)|^2}\gamma'(t) + \frac{1}{|\gamma'(t)|}D_t \gamma'(t)\tag{i}$ Since $|\tilde\gamma'|$ is constant, $\langle \tilde\gamma'(\varphi^{-1}(t))\, , \, D_t \tilde\gamma'(\varphi^{-1}(t))\rangle = 0 \implies \\ \langle \frac{1}{|\gamma'(t)|}\gamma'(t)\, , \, D_t \tilde\gamma'(\varphi^{-1}(t))\rangle =0\tag{ii}$ By combining (i) and (ii), we have $\langle \frac{1}{|\gamma'(t)|}\gamma'(t)\, , \, -\frac{|\gamma'(t)|'}{|\gamma'(t)|^2}\gamma'(t) + \frac{1}{|\gamma'(t)|}D_t \gamma'(t)\rangle =0 \implies \\
-\frac{|\gamma'(t)|'}{|\gamma'(t)|^3}\langle\gamma'(t), \gamma'(t) \rangle + \frac{1}{|\gamma'(t)|^2} \langle\gamma'(t), D_t\gamma'(t) \rangle=0
\implies\\
\frac{\langle\gamma'(t), D_t\gamma'(t) \rangle}{|\gamma'(t)|}=|\gamma'(t)|'
\tag{iii}$ Finally we compute: $(\kappa(t))^2=\langle D_t \tilde\gamma'(\varphi^{-1}(t))\, , \, D_t \tilde\gamma'(\varphi^{-1}(t)) \rangle \\
=\langle -\frac{|\gamma'(t)|'}{|\gamma'(t)|^2}\gamma'(t) + \frac{1}{|\gamma'(t)|}D_t \gamma'(t)\, , \, D_t \tilde\gamma'(\varphi^{-1}(t))\rangle \quad\quad \text{by (i)}\\
=\langle \frac{1}{|\gamma'(t)|}D_t \gamma'(t), \, -\frac{|\gamma'(t)|'}{|\gamma'(t)|^2}\gamma'(t) + \frac{1}{|\gamma'(t)|}D_t \gamma'(t)\rangle \quad\quad \text{by (i) and (ii)}\\
= -\frac{|\gamma'(t)|'}{|\gamma'(t)|^3}\langle \gamma'(t), D_t\gamma'(t)\rangle + \frac{|D_t\gamma'(t)|^2}{|\gamma'(t)|^2}\\
= -\frac{\langle \gamma'(t), D_t\gamma'(t)\rangle^2}{|\gamma'(t)|^4}+\frac{|D_t\gamma'(t)|^2}{|\gamma'(t)|^2} \quad\quad \text{by (iii)}\\
=\frac{|\gamma'(t)|^2|D_t\gamma'(t)|^2 - \langle \gamma'(t), D_t \gamma'(t) \rangle^2}{|\gamma'(t)|^4}$ But that final denominator is clearly supposed to be $|\gamma'(t)|^6$ . Any help would be greatly appreciated.","['curvature', 'riemannian-geometry', 'differential-geometry']"
4281982,Determining the period of $ \frac{\sin(2x)}{\cos(3x)}$,"I would like to compute the period of this function which is a fraction of two trigonometric functions. $$ \frac{\sin(2x)}{\cos(3x)}$$ Is there a theorem for this? what trick to use to easily find the period?
I started by reducing the fraction but I'm stuck on the rest. For example, let $T$ be the period to be calculated: $$\frac{\sin(2x)}{\cos(3x)}  =\frac{\sin(2x + 2 T)}{\cos(3x+3T)} =  \frac{\sin(2x) \cos(2T)+\sin(2T) \cos(2x)}{\cos(3x) \cos(3T)-\sin(3T)\sin(3x)}$$ Thanks for your help.","['periodic-functions', 'trigonometry']"
4281989,How to solve $\\e^{-u}+\frac{u}{5}=1\\$ for $\\u\\$ ? What is the method to solve it without using graph.,"Solving $\\e^{-u}+\frac{u}{5}=1\\$ without using graph. From graphing line $y(u)=1$ intersects with $\\e^{-u}+\frac{u}{5}\\$ at two points (0,1) &  (4.97,1) so that gives $\\u = 0\\$ or $\\4.97\\$ . But how can I solve it analytically ? [ [graph]",['algebra-precalculus']
4282003,Number of relations on (A cross B)?,"Is the number of relations on $A \times B$ the same as the number of relations from $A$ to $B$ ? Can anybody clear this doubt with some examples? In my notes, I have written the number of relations on $A \times B$ as $2^{(mn)^2}$ and number of relations from $A$ to $B$ as $2^{mn}$ . But some friends are arguing both are same and answer is $2^{mn}$ .","['combinatorics', 'relations', 'function-and-relation-composition']"
4282024,"Solve the Diophantine equation $ 12^x + y^4 = 56^z $ where $x, y, z$ are non-negative integers.","I need help to solve the following Diophantine equation: $$ 12^x + y^4 = 56^z $$ where $x,y,z$ are non-negative integers. My attempt: We can see immediately that $x=y=z=0$ is a solution. By taking mod $3$ we have: $$y^4 \equiv 2^z \text{ (mod 3)}$$ But we already know that $y^4 \equiv 0 \text{ or } 1 \text{ (mod 3)}$ and $2^z$ is not divisible by $3$ , therefore $2^z \equiv 1 \text{ (mod 3)}$ , so $z$ is even. Let $z=2t$ , we then subtract $y^4$ from both sides to get $$12^x = \left( 56^t-y^2 \right) \left( 56^t+y^2 \right)$$ So we have $56^t-y^2= 2^a3^b$ and $56^t+y^2 = 2^{2x-a}3^{x-b}$ for some non-negative intgers $a,b$ . But since $\left( 56^t-y^2 \right) + \left( 56^t+y^2 \right)$ is not divisible by $3$ , only one of the two terms is divisible by $3$ , which means $b = 0 \text{ or } b=x$ . This is where I got stuck, I don't know what to do next. Am I going the wrong direction? Please help me.","['exponential-diophantine-equations', 'number-theory', 'diophantine-equations']"
4282034,Can't find particular integral of a differential equation,"I have been trying to solve this differential equation using Method of undetermined coefficients and all my guesses are incorrect and not working : $$y''' + 3y'' + 3y' + y = 30e^{-x}$$ When I try to find the Particular Integral, the whole LHS is becoming 0. So I can't get any value for 'A'. Can anyone solve this differential equation for me? Or just the particular integral is enough I stared with the guess that it could be $Ae^{-x}$ . But it didn't work.  When i looked online, i came across an article, it said that i have to add an extra term x^s, where s is the smallest positive integer that renders all summands of a solution independent of the homogeneous solutions. Now i didn't exactly understand what they meant. But like in few examples they gave, i tried with just t first, then i tried $t^2$ . That also didn't work. Then I tried $t^3$ , and then got $\frac{5}{x-1}$ as a value for A. I don't think that's the right answer. Is there any shortcut to guessing the correct one? I know that I can try doing it with the method of variation of parameters but in our exams sometimes they specifically say that you have to use this specific method..Can anyone help?","['integration', 'calculus', 'ordinary-differential-equations']"
4282041,Holomorphic function with Taylor series coefficients sum of reciprocal factorials,"I am trying to find a holomorphic function $f(z)$ with Taylor series $$ f(z) = \sum_{n=0}^{\infty}c_n z^n$$ where the Taylor series coefficients are given by $$ c_n = \sum_{\text{even } i}^n \frac{1}{(i+1)!}$$ The partial sums should be related to the incomplete Gamma function by the relation $$ \sum_{k=0}^n \frac{x^k}{k!} = \frac{\Gamma(n+1,x)e^x}{n!}$$ and setting $x=1$ . The question is then roughly can the incomplete Gamma function $\Gamma(n,x)$ be realised as an nth derivative! I'm dreaming of ultimately something like an integral expression for $f(z)$ . Thanks","['complex-analysis', 'taylor-expansion', 'gamma-function']"
4282068,Why is my process of differentiation (trigonometric substitution) not working?,"Prologue (you can skip straight to the ""Problem"" section (bolded) if you want) : First, to show you what way (let's call it trigonometric substitution method) I'm talking about and to show that this way works, I'll describe the tenets and then do a math using that way: Basic tenets of trigonometric substitution method: It is applicable when we are differentiating inverse trigonometric functions. $x$ should be substituted with a trig ratio that can hold all the possible values of $x$ and that will make differentiation easier. For example, in $\cos^{-1}(\sqrt{\frac{1+x}{2}})$ , $-1\leq x\leq1$ , so it can be substituted with $\cos\theta$ or $\sin\theta$ ; substituting with $\sin\theta$ doesn't make our life easier, so we have to substitute with $\cos\theta$ . Similarly, in $\tan^{-1}\left(\sqrt{\frac{1-x}{1+x}}\right)$ ( $-1<x\leq1$ ) and $\sin^{-1}\left(\frac{1-x^{2}}{1+x^{2}}\right)$ $(x\in(\infty,-\infty))$ , $x$ has to be substituted with $\cos\theta$ & $\tan\theta$ respectively. All of the maths can also be done exclusively using the chain rule. However, the maths might get tedious in that way. Example Differentiate with respect to $x$ : $\tan^{-1}\frac{4x}{\sqrt{1-4x^2}}.$ Differentiation using trigonometric substitution: Let, $y=\tan^{-1}\frac{4x}{\sqrt{1-4x^2}}$ and $2x=\cos\theta\implies\theta=\cos^{-1}2x\ [\text{Assuming $\theta$ is within the principal range of $\arccos$}]$ Now, $$y=\tan^{-1}\frac{4x}{\sqrt{1-4x^2}}$$ $$y=\tan^{-1}\frac{2\cos\theta}{\sqrt{1-\cos^2\theta}}$$ $$y=\tan^{-1}2\cot\theta$$ $$\frac{dy}{dx}=\frac{d}{d(2\cot\theta)}(\tan^{-1}2\cot\theta).\frac{d}{d(\cot\theta)}(2\cot\theta).\frac{d}{d\theta}(\cot\theta).\frac{d}{dx}\theta$$ $$...$$ $$\frac{dy}{dx}=\frac{4}{(12x^2+1)(\sqrt{1-4x^2)}}$$ This is the correct answer. We could've taken $2x=\sin\theta$ as well and the answer would've been the same. We could've done the math exclusively using the chain rule as well. Problem Differentiate with respect to $x$ : $\sin^{-1}(2x\sqrt{1-x^2}).$ Attempt 1 Let $y=\sin^{-1}(2x\sqrt{1-x^2})$ and $x=\sin\theta\implies\theta=\sin^{-1}x\ [\text{assuming $\theta$ is within the principal range of $\arcsin$}]$ $$y=\sin^{-1}(2x\sqrt{1-x^2})$$ $$y=\sin^{-1}(2\sin\theta\cos\theta)$$ $$y=\sin^{-1}(\sin2\theta)$$ $$y=2\theta\tag{1}$$ $$y=2\sin^{-1}x$$ $$\frac{dy}{dx}=2\frac{1}{\sqrt{1-x^2}}$$ Attempt 2 Let $y=\sin^{-1}(2x\sqrt{1-x^2})$ and $x=\cos\theta\implies\theta=\cos^{-1}x\ [\text{assuming $\theta$ is within the principal range of $\arccos$}]$ $$y=\sin^{-1}(2x\sqrt{1-x^2})$$ $$y=\sin^{-1}(2\sin\theta\cos\theta)$$ $$y=\sin^{-1}(\sin2\theta)$$ $$y=2\theta\tag{2}$$ $$y=2\cos^{-1}x$$ $$\frac{dy}{dx}=-2\frac{1}{\sqrt{1-x^2}}$$ Interestingly enough, we get two different answers using $x=\cos\theta$ & $x=\sin\theta$ , which shouldn't have been the case. More importantly, both of the answers are wrong . Questions: Why am I not able to differentiate correctly using the trigonometric substitution method? In the graph of the correct derivative and the incorrect derivative found using $x=\sin\theta$ , there is an overlap between the two from $x=-0.707$ and $x=0.707$ . What is the significance of the number $0.707$ , and why is the overlap happening? In the graph of the correct derivative and the incorrect derivative found using $x=\cos\theta$ , there is an overlap between the two from $x=-0.707$ to $x=-1$ in the negative y-axis and from $x=0.707$ to $x=1$ in the positive y-axis. What is the significance of the number $0.707$ , and why is the overlap happening? My observations: My hunch is that lines $(1)$ & $(2)$ are wrong. However, I don't want to explain my hunch because I fear that it might complicate matters unnecessarily. This might help you in answering the question: it contains the graphs of the original problem, the incorrect derivative found using $x=\sin\theta$ , the incorrect derivative found using $x=\cos\theta$ & the correct derivative that can be found by differentiating exclusively using the chain rule.","['calculus', 'derivatives', 'trigonometry']"
4282082,Monotone approximation of elements in AF-algebras and $C^*$-algebras,"Suppose that we are given an AF-algebra $A$ and a sequence of finite-dimensional subalgebras $\mathbb{C}=A_0\subset A_1\subset A_2\subset\ldots$ such that $A=\overline{\bigcup\limits_{n\geq 0}A_n}$ . Let me denote this dense subalgebra of $A$ by $A^{LS}$ , i.e. $A^{LS}= \bigcup\limits_{n\geq 0}A_n$ . Next, we define the positive elements as $$A^+=\left\{h^2\ \middle|\ h\in A,\ h^*=h \right\}$$ and $$(A^{LS})^+=\left\{h^2\ \middle|\ h\in A^{LS},\ h^*=h \right\}.$$ Then for any $y\in A^+$ we can find a sequence $\{y_n\}_{n\geq 1}\subset (A^{LS})^+$ such that $\lim\limits_{n\to\infty}y_n=y$ . Questions: Is it true that we can pick this $\{y_n\}$ in such a way that $y_n\leq y_{n+1}$ for any $n$ with respect to the partial order $\leq$ on $A^{LS}$ defined by $(A^{LS})^+$ ? More generally. Suppose we are given a unital separable $C^*$ -algebra $A$ with a unital dense $*$ -subalgebra $\widetilde{A}\subset A$ . Is it true that any $y\in A^+$ can be approximated by a monotone (in the sence of the partial order defined by positive elements) sequence of elements from $\left(\widetilde{A}\right)^+$ ? Is it possible at least for any $y\in A^+$ and any open neighbourhood of $y$ find an element $z\in \left(\widetilde{A}\right)^+$ such that $z$ belongs to that neighbourhood and $z\leq y$ ? Thanks in advance!","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4282120,Trigonometric equation in quadratic form,"If $\cos^4α+k$ and $\sin^4α+k$ are the roots of $x^2+\lambda \  (2x+1 )=0$ and $\sin^2α +\ell$ and $\cos^2 α +\ell$ are the roots of $x^2+8x+4=0$ , then the sum of the possible values of $λ$ is _________. My approach is as follow ${\sin ^2}\alpha  + \ell  + {\cos ^2}\alpha  + \ell  =  - 8;1 + 2\ell  =  - 8;\ell  =  - \frac{9}{2}$ $\left( {{{\sin }^2}\alpha  + \ell } \right)\left( {{{\cos }^2}\alpha  + \ell } \right) = 4 \\ \Rightarrow {\sin ^2}\alpha {\cos ^2}\alpha  + \ell \left( {{{\sin }^2}\alpha  + {{\cos }^2}\alpha } \right) + {\ell ^2} = 4 \\ \Rightarrow {\sin ^2}\alpha {\cos ^2}\alpha  + \ell  + {\ell ^2} = 4$ $ \Rightarrow \dfrac{{4{{\sin }^2}\alpha {{\cos }^2}\alpha }}{4} + \ell  + {\ell ^2} = 4 \\ \Rightarrow \dfrac{{{{\sin }^2}2\alpha }}{4} - \dfrac{9}{2} + {\left( { - \dfrac{9}{2}} \right)^2} = 4\\ \Rightarrow \dfrac{{{{\sin }^2}2\alpha }}{4} - \dfrac{9}{2} + \dfrac{{81}}{4} = 4 \\ \Rightarrow \dfrac{{{{\sin }^2}2\alpha }}{4} + \dfrac{{63}}{4} = 4\\ \Rightarrow {\sin ^2}2\alpha  =  - 47$ which is not possible hence how do I proceed?",['trigonometry']
4282152,Inverse of two variable function,"I do not have much experience in finding the inverse of a multivariable function, so any help is appreciated. I have function $f(\theta_a,\theta_b) =\begin{pmatrix}\frac{\cos(\theta_a)+\cos(\theta_b)}{1+\cos(\theta_a)\cos(\theta_b)}\\ \frac{\sin(\theta_a)+\sin(\theta_b)}{1+\sin(\theta_a)\sin(\theta_b)}\end{pmatrix} $ The right hand side above is a vector, so the function takes two real numbers $\theta_a,\theta_b \in [0,2\pi)$ in a vector and outputs a vector. My strategy was to try and solve the system of equations: $u = \frac{\cos(\theta_a)+\cos(\theta_b)}{1+\cos(\theta_a)\cos(\theta_b)}, \quad v = \frac{\sin(\theta_a)+\sin(\theta_b)}{1+\sin(\theta_a)\sin(\theta_b)} $ . by first isolating $\theta_a$ in the first equation, and inserting this expression into the second equation, then isolating $\theta_b$ and reinserting to get $\theta_a$ and $\theta_b$ as functions of $u$ and $v$ . This quickly becomes some very complex expressions however, and I have not been able to complete the computations, leading me to wondering if there is a different way. Thanks in advance!","['multivariable-calculus', 'calculus', 'trigonometry']"
4282160,Compactness in sequence space,"We have the vector space $X=\{\vec{x}=(x_1, x_2,\cdots) | x_n\in\mathbb{R} (n\in\mathbb{N}), \sum_{n=1}^{\infty}\frac{1}{n}|x_n|<\infty \}$ , and the norm $\|\vec{x}\|=\sum_{n=1}^{\infty}\frac{1}{n}|x_n|\ (\vec{x}=(x_1, x_2, \cdots))$ on it. This normed space is complete. The question is about the compactness of the following subset $A\subset X$ , $A=\{\vec{x}\in X|\sum_{n=1}^{\infty} |x_n|^2\leq 1\}$ . I know this subset $A$ is bounded and closed (the boundedness comes from the fact $|x_n|\leq\frac{1}{\sqrt{n}}$ assuming $\{|x_n|\}$ is monotonically decreasing). What I cannot tell is whether this subset $A$ is compact or not (in the Banach space $(X, \|\cdot\|)$ ). Can you help me with this problem?",['functional-analysis']
4282188,Intuition behind equation of line in Vector form,"I'm new to Vector and 3D Geometry and today I was taught the equation of a line passing through two points $A(\vec{a})$ and $B(\vec{b})$ as $$\vec{r}=\vec{a} + λ(\vec{b}-\vec{a})$$ It is difficult for me to imagine how this works. Why do we have a $λ$ here? What I can think of is that $\vec{r}=\vec{a} + λ(\vec{b}-\vec{a}$ )
means that if we draw a line passing through $A$ and $B$ and we plug in different values of $λ$ , we will get the position vector of different points that lie on that line. Am i right in thinking so? So this is a bit different from the stuff we use in $2D$ geometry or other forms like expressing the line in terms of $x,y,z$ ?","['vectors', 'geometry', 'intuition', '3d']"
4282201,How many books are in a library?,"My cousin is at elementary school and every week is given a book by his teacher.  He then reads it and returns it in time to get another one the next week. After a while we started noticing that he was getting books he had read before and this became gradually more common over time.   Naturally, I started to wonder how one could estimate the total number of books in their library. Say the true number of books in the library is $N$ and the teacher picks one uniformly at random (with replacement) to give to you each week.  If at week $t$ you have received a book you have read before $x$ times, is there an unbiased estimator for the total number of books in the library and what is the variance of this estimator? Is there another biased estimator with lower variance? In my cousin's case, in the first $30$ weeks he received a book he had received before $3$ times.","['statistics', 'probability']"
4282208,I'm having a problem with parallel transport which suggests the dimensionality is incorrect,"I'm working in spherical coordinates and I want to transport a vector for a radial velocity over an interval with $dr\ne 0$ but all other increments zero. The formula I have found for parallel transport is $$v_{r + dr}^\mu  \approx v_r^\mu  - \Gamma _{\nu \alpha }^\mu v_r^\nu d{x^\alpha }$$ I understand the four vector has elements $\gamma(c, u, 0, 0)$ where $u$ is my radial velocity. Looking at the equation for $v_{r+dr}^1$ I get $$v_{r + dr}^1 \approx v_r^1 - \Gamma _{01}^1v_r^0d{x^1} - \Gamma _{11}^1v_r^1d{x^1}$$ where, clearly, $dx^1=dr$ (and no other Christoffel symbols apply because other $v^\nu$ are zero).
Here's my problem. Terms $v_r^0dx^1$ and $v_r^1dx^1$ have the same dimensionality but the two Christoffel symbols do not. What have I misunderstood? (Dimensionality of $\Gamma _{01}^1$ is $T^{-1}$ but of $ \Gamma _{11}^1$ is $L^{-1}$ )",['differential-geometry']
4282211,Solve inverse trig system,"The problem is: Solve for $x,y$ in $$\sin^{-1} x-\sin^{-1} y=120^{\circ}$$ $$\cos^{-1} x-\cos^{-1} y=60^{\circ}$$ My problem is that if $$\sin^{-1} x+\cos^{-1} x=90^{\circ}$$ holds the system is impossible to solve.
By moving to the other side one can easily deduce that $x=\pm \frac{\sqrt{3}}{2}$ or possibly, $x=0$ , same with $y$ . But what is the correct solution ? I assume $\sin^{-1}$ is chosen between $-90^{\circ}$ and $90^{\circ}$ , and that $\cos^{-1}$ between $0^{\circ}$ and $180^{\circ}$ .","['algebra-precalculus', 'trigonometry']"
4282297,$f^{\ast}$ surjective $\iff$ $f$ is an isomorphism into a closed set,"The result on the title is what I'm trying to prove, so I'll write the whole question, with all the hypothesis. Here $k$ is an algebraically closed field. Let $f : X \rightarrow Y$ be a morphism and $f^{\ast} : k[Y] \rightarrow k[X]$ its corresponding pullback homomorphism. Show that $f^{\ast}$ is surjective if, and only if, $f$ is an isomorphism into a closed set. I can only get into the basic initial structure of the ""if"" and the ""only if"" parts: if $f^{\ast}$ is surjective, then for all $g \in k[X]$ there exists $\phi \in k[Y]$ such that $g = \phi \circ f$ , as shown by the diagram below. And here I don't proceed further with much effectivity. I've tried some routes, but was not quite there for me. The closest one was to take $x_i \in k[X]$ defined by $x_i = \phi_i \circ f$ for $i \in \{1,\dots,m\}$ and then construct $x = (x_1,\dots,x_m)$ , and here is where I hit a wall again. if $f$ is an isomorphism into a closed set, $f(X)$ is closed so I can look to $(\phi \circ f)(X)$ . And that's it, I didn't advance further because I was trying the other part. It is really bad, but I have some ideas: There is a corollary of the nullstellensatz that gives us equivalence between algebraic sets and radical ideals. It could be useful. There is a result that I've shown previously that says: Let $\varphi : A \rightarrow B$ be a ring homomorphism and consider the corresponding spectra map $\mathrm{spec}(\varphi) : \mathrm{spec}(B) \rightarrow \mathrm{spec}(A)$ . If $J$ is an ideal of $B$ , then $\overline{\mathrm{spec}(\varphi)(V(J))} = V(\varphi^{-1}(J))$ . The second one I know for a fact that it is equivalent to the problem I am trying to solve. Finally, I need some help to connect these dots, and I prefer to use the result (2.) to do so. Thanks in advance. PS.: I did read other similar questions, but they didn't satisfy my need to get a better grasp on the subject.","['algebraic-geometry', 'commutative-algebra']"
4282333,"Solution verification of flux integral $\iint_DF\cdot\hat{n}dS$ with $F=(2x,y,z)$","Compute the flux of $F=(2x,y,z)$ through the surface $$
r = u^2v\,\hat{\imath} + uv^2\,\hat{\jmath} + v^3\,\hat{k},
\quad 0\leq u \leq 1, \quad 0 \leq v \leq 1
$$ My approach: is it correct?","['multivariable-calculus', 'solution-verification', 'surface-integrals', 'vector-analysis']"
4282339,What is the meaning of this notation for a function?,"A contour or path is a continuous mapping $\gamma:[a,b]\rightarrow \mathbb{C}$ which is piecewise continuously differentiable, i.e., there exist $a=a_0<a_1<...<a_n=b$ such that $\gamma_{|[a_{j-1},a_j]}$ is continuously differentiable for each $j$ What is the meaning of "" $\gamma_{|[a_{j-1},a_j]}$ "" in this text?","['complex-analysis', 'notation', 'contour-integration', 'functions', 'piecewise-continuity']"
4282350,True definition of a function $f$ [duplicate],"This question already has answers here : What is the actual definition of a function? (3 answers) Closed 2 years ago . Thomas Calculus gives the following definition for a function: A function $f$ from a set $D$ to a set $Y$ is a rule that assigns a unique
value $f(x)$ in $Y$ to each $x$ in $D$ . I have my misgivings as far as this definition for a mathematical function is concerned. Every $x$ needn't be assigned a unique $f(x)$ from $Y$ . If $x_1, x_2 \in D$ and $f(x_1)=f(x_2)=y \in Y$ which is perfectly valid and $y$ is not unique since the same $y$ gets assigned to both $x_1$ and $x_2$ . Am I wrong in my interpretation?","['calculus', 'functions', 'definition', 'relations']"
4282379,Maximizing $a^2+b^2+c^2+d^2$ with given constraints,"The following problem is from a local contest which ended today: Let $a,b,c,d$ be positive real numbers such that $$(a+b)(c+d)=143\\ (a+c)(b+d)=150\\ (a+d)(b+c)=169$$ Find the maximum value of $a^2+b^2+c^2+d^2$ . Here are my workings: We have $$\tag{1}ac+bc+bd+ad=143$$ $$\tag{2}{ab+bc+cd+ad=150}$$ $$\tag{3}{ab+bd+ac+cd=169}$$ Summing $(1)$ , $(2)$ and $(3)$ , we have $$2(ab+bc+cd+ac+ad+bd)=462$$ We have $$\begin{align} a^2+b^2+c^2+d^2 &=(a+b+c+d)^2-2(ab+bc+cd+ac+ad+bd)\\ &= (a+b+c+d)^2-462\end{align}$$ So we have to maximize $a+b+c+d$ . I can't proceed from here. Minimizing the expression seems easy by Cauchy-Schwarz or AM-GM but I don't know how to maximize this.","['contest-math', 'inequality', 'systems-of-equations', 'optimization', 'algebra-precalculus']"
4282410,Is there a test for convexity?,"This is a very heterodox question. But here is the context. I'm programming a computational package, and the user may write/define a cost function freely, e.g. $$
cost(x,y) = e^{|x-y|} (x-y)^2.
$$ Now, the algorithm programmed only works if the cost function is convex. Here is where my question comes in. Would there be some kind of test to verify if the function is indeed convex? Mathematically, we can try to manipulate the function in order to verify whether it satisfies the convexity definition, but this scenario does not allow for such approaches.
I was thinking for something like ""sample some points, calculate the function and verify if the mid point is above the linear interpolation"". How many points would be necessary to correctly guess that the function is convex with a certain probability? Any references on this kind of odd question (the probability that a function is convex)?","['convex-analysis', 'probability', 'computability']"
4282424,"If $\frac{(1+x)^2}{1+x^2}=\frac{13}{37}$, then find the value of $\frac{(1+x)^3}{1+x^3}$","Let $x$ be a real number such that $\frac{(1+x)^2}{1+x^2}=\frac{13}{37}$ . What is the value of $\frac{(1+x)^3}{1+x^3}$ ? Of course, one way is that to solve for $x$ from the quadratic $37(1+x)^2=13(1+x^2)$ which gives the value $x=\frac{-37\pm\sqrt{793}}{24}$ . Thus we can compute $\frac{(1+x)^3}{1+x^3}=\frac{13}{49}$ . But I am trying to find the result from the equation $\frac{(1+x)^2}{1+x^2}=\frac{13}{37}$ without solving for $x$ using some algebra. Here are my attempts to do that: We have $$\begin{align} & \frac{(1+x)^2}{1+x^2}=\frac{13}{37}\\ &\implies \frac{(1+x)^2}{(1+x)^2-2x}=\frac{13}{37}\\ &\implies \frac{(1+x)^3}{(1+x)^3-2x(1+x)}=\frac{13}{37}\\ &\implies \frac{(1+x)^3}{1+x^3+3x(2+x)-2x(1+x)}=\frac{13}{37}\\ &\implies \frac{(1+x)^3}{1+x^3+x(4+x)}=\frac{13}{37}
\end{align}$$ I can't seem to proceed from here.","['contest-math', 'systems-of-equations', 'recreational-mathematics', 'algebra-precalculus', 'quadratic-forms']"
4282478,"To compute $\lim\limits_{n\to +\infty} \log_2\left(\frac{n^3}{n^4+2}\right)=-\infty, \lim\limits_{x\to+\infty}\log_2\left(\frac{x^3}{x^4+2}\right)$","On my textbook of Maths for students of my high school (there are really only two such limits) I have found, for example, a limit of a succession with this solution: $$\color{magenta}{\lim_{n\to +\infty} \log_2\left(\frac{n^3}{n^4+2}\right)=-\infty} \tag 1$$ I am a bit perplexed about the result. In fact it is true that $\forall n\in \Bbb N\smallsetminus \{0\}$ we have: $$\frac{n^3}{n^4+2}>0$$ and the domain of the logarithm is guaranteed ( $n\ne 0$ ). Being $$\lim_{n\to +\infty} \log_2\left(\frac{n^3}{n^4+2}\right)=\log_2\left(\lim_{n\to +\infty} \frac{n^3}{n^4+2}\right)$$ but $$\lim_{n\to +\infty} \frac{n^3}{n^4+2}=0$$ hence it has no sense to calculate $$\require{cancel} \color{red}{\cancel{\log_2 0}}$$ Considering the function $$y=f(x)=\log_2\left(\frac{x^3}{x^4+2}\right) \tag 2$$ we know that the domain is $]0,+\infty[$ . The allowable limits are for $x\to 0^+$ and $x\to +\infty$ . It is true, instead, that $$\color{green}{\lim_{x\to 0^+}\log_2\left(\frac{x^3}{x^4+2}\right)=-\infty}$$ but if I plot the $(2)$ with Desmos, when I compute the $\lim\limits_{x\to +\infty}\log_2\left(\frac{x^3}{x^4+2}\right)$ I have precisely $-\infty$ . So why must the limit of $(1)$ be $-\infty$ ? What am I doing wrong and why? So it must be $\lim_n \log_c a_n=-\infty$ ( $c>0 \wedge c\ne 1$ ), when $\{a_n\}\to 0$ , i.e. infinitesimal?","['logarithms', 'calculus', 'sequences-and-series', 'limits', 'algebra-precalculus']"
4282529,"Find the derivative of $h(x) = \min_{u \in [x,b]} f(u)$","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a differentiable function. And let $b\in \mathbb{R}$ be a fixed number, consider the function $h:(-\infty,b]\rightarrow \mathbb{R} $ given by \begin{equation} 
h(x) = \min_{u \in [x,b]} f(u) \end{equation} I want to find $h'(x)$ . Following the definition \begin{equation}
h'(x) = \lim_{\epsilon \to 0} \frac{h(x+\epsilon)-h(x)}{\epsilon}
\end{equation} then \begin{equation}
h'(x) = \lim_{ \epsilon \to 0} \frac{\min_{u \in [x+\epsilon,b]} f(u)-\min_{u \in [x,b]} f(u)}{\epsilon}
\end{equation} But at this point I don't know how to continue. Do you think is possible to find $h'(x)$ without knowing an expression for $f(u)$ ? or Do you know how to find $h'(x)$ using other technique? Thanks in advance.","['calculus', 'derivatives']"
4282619,compute the integral $\int_0^1 \int_0^1 \int_0^1 \frac{1}{(1+x^2+y^2+z^2)^2} dxdydz$ [duplicate],"This question already has answers here : Doubt in solution for evaluating $\int_0^1\int_0^1\int_0^1(1+u^2+v^2+w^2)^{-2}du~dv~dw$. (2 answers) Closed 2 years ago . Determine, with justification, the value of the integral $\int_0^1 \int_0^1 \int_0^1 \frac{1}{(1+x^2+y^2+z^2)^2} dxdydz$ . I tried converting this integral to cylindrical coordinates with $r = \sqrt{x^2 + y^2}$ ranging from $0$ to $\sqrt{2}$ , $0\leq \theta \leq \pi/2, 0\leq z \leq 1,$ where $\theta $ is such that $x= r\cos\theta, y = r\sin\theta.$ However, this seems to lead to an incorrect result. Which bounds have I gotten wrong? Also, it seems that the integral over the unit cube equals twice the integral over the region defined by $0\leq z\leq 1, 0\leq x\leq 1, 0\leq y\leq x,$ but I'm not sure why. The result should be $\frac{\pi^2}{32},$ which is basically what WolframAlpha outputs. Using spherical coordinates seems to make the integration more complicated due to the integration factor.","['integration', 'calculus', 'real-analysis']"
4282639,Difference between increasing function and strictly increasing function in terms of derivatives?,"Consider the following theorem and remarks from the Application of Derivatives chapter in p. 201 of the NCERT textbook . Theorem: Let $f$ be continuous on $[a, b]$ and differentiable on the open
interval $(a,b)$ . Then (a) $f$ is increasing in $[a,b]$ if $f′(x) > 0$ for each $x \in (a,
 b)$ (b) $f$ is decreasing in $[a,b]$ if $f′(x) < 0$ for each $x \in (a,
 b)$ (c) $f$ is a constant function in $[a,b]$ if $f′(x) = 0$ for each $x
 \in (a, b)$ Remarks: (i) $f$ is strictly increasing in $(a, b)$ if $f′(x) > 0$ for each $x
 \in (a, b)$ (ii) $f$ is strictly decreasing in $(a, b)$ if $f′(x) < 0$ for each $x
 \in (a, b)$ (iii) A function will be increasing (decreasing) in R if it is so in
every interval of R. I am thinking that the theorem is incomplete and the remarks are correct. That theorem has to be as follows: (a) $f$ is increasing in $[a,b]$ if $f′(x) \ge 0$ for each $x \in (a,
 b)$ (b) $f$ is decreasing in $[a,b]$ if $f′(x) \le 0$ for each $x \in (a,
 b)$ Since there is a difference between increasing function and strictly increasing function, I am feeling that the theorem given in the textbook is faulty. Am I correct, or where am I going wrong?","['calculus', 'functions', 'derivatives']"
4282680,$E[(Y_1-f(x))^2] \leq E[(Y_2-Y_1)^2]$,"Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous bounded function, $(X_k)_k$ a sequence of i.i.d random variables such that $$\forall x \in \mathbb{R},f(x)=\int_{\mathbb{R}}f(x+y)dP_{X_1}(y).$$ Let $x \in \mathbb{R},Y_k=f\left(x+\sum_{q=1}^kX_q\right),\mathcal{F}_k=\sigma(X_1,...,X_k).$ Verify that: $$\tag{1}E\left[(Y_1-f(x))^2\right] \leq E\left[(Y_2-Y_1)^2\right]  $$ and deduce that for every $y \in \text{supp} P_{X_1}:=\{u,\forall r>0,P_{X_1}([y-r,y+r])>0\},f(x+y)=f(x).$ One way to prove the first question is to note that $$\int_{\mathbb{R}}(f(x+y)-f(x))^2dP_{X_1}(y)=\int_{\mathbb{R}}\left(\int_{\mathbb{R}}(f(x+y+u)-f(x+u))dP_{X_2}(u)\right)^2dP_{X_1}(y) \leq \int_{\mathbb{R}}\left(\int_{\mathbb{R}}(f(x+y+u)-f(x+u))^2dP_{X_2}(u)\right)dP_{X_1}(y) \leq E[(Y_2-Y_1)^2]$$ For part 2 we can deduce that for $P_{X_1}$ -almost every $y \in \mathbb{R},f(x+y)=f(x),$ how to conclude using continuity that $f(x+y)=f(x)$ for every $y \in \text{supp}P_{X_1}$ ? How to deduce part 2 ?","['measure-theory', 'stochastic-processes', 'martingales', 'inequality', 'probability-theory']"
4282705,What is the area of $y^2=\sqrt x-x$ (Guitar Pick),"I made a typo while experimenting on Desmos and typed $y^2=\sqrt x-x$ . It drew a shape, one that I've never seen before: With my very limited knowledge of calculus, I know the area would be equal to: $$2\int_0^1 \sqrt{\sqrt x - x} \,dx$$ However, I have no clue how to evaluate this integral. Using Desmos, I can get a decimal approximation (it's about 0.785), and Wolfram Alpha can give me the final result ( $\pi/4$ ). No site I can think of has the solution and steps to solve it, so I figured I'd ask it here. How would you evaluate this integral?","['integration', 'calculus']"
4282721,"Is there a ""natural"" example of a probability space where the singletons aren't measurable?","Is there a ""natural"" example of a probability space $(\Omega, \mathcal{F}, P)$ such that $\bigcup_{\omega \in \Omega} \{\{\omega\}\} \not\subset \mathcal{F}$ ? Here by ""natural"" I mean something which might be of independent interest to probabilists, or has an intuitive probabilistic interpretation. Of course, there are many natural examples where we have a sub - $\sigma$ -algebra which does not contain the singletons, and often (e.g., in the context of conditional expectation) this is of interest. However, in this question, I'm referring to the ambient probability space. If there is such an example, how should we interpret this? It would seem very strange for the elementary outcomes $\{\omega\}$ to not be measurable. This would mean, for instance, the natural ""random variable"" $\mathbf{1}_{\{\omega\}}$ isn't even a random variable!",['probability-theory']
4282732,"I'm trying to find where a ""3D logarithmic spiral"" converges.","Background and problem It's in quotes because I don't know what I'm talking about. I'm more of an artist who likes to dabble in math so bare with me. I know the title is a little confusing so I have made a pretty animation to illustrate what I mean. I assume this is some type of discrete logarithmic spiral. It always starts the origin with no translations, rotations or scale applied. You'll notice in the video, the 2nd cube has 3 axes coming out of it. It's transform is driving the rest of the cubes as seen in this clip . I can translate, rotate, and scale and as long as the translation and rotation are not 0, and the scale is between 0 and 1 it'll make a spiral. Anything else won't produce a spiral as far as I'm aware. I've searched Google for about 5 hours today and haven't found anything so I either don't know the proper terms, it's never been done before which I doubt, or it's impossible which I don't know but highly doubt. I'm trying to find the $x, y,$ and $z$ for where this would converge in non-polar/spherical form so I can plug them into my software. Why I'm here Some hindrances I'm having are that I've never taken anything on spherical coordinates which I assume would be useful (or even polar coordinates), I don't have a background in linear algebra at all, and I've been out of school about 8 years at this point so I'm very rusty. I've asked this question on Reddit and was given an answer but I don't understand it. They suggested, If your initial vector is v and your linear transformation is T, then
the point it converges to is $(1+T+T^2 +T^3...)v=\dfrac v{1-T}.$ For the purpose of computation, you need to write T as a matrix, v as a column vector. Can come one break that down a bit more? Is $v$ my 2nd cube with the axes poking out? I don't understand what $T$ means at all. Is it even right? I'm ok with not being spoon-fed the answer but I need a little bit more than this. If the first answer is any indication, then is way above my ability. I'm not even sure which branch of math I need. I know it's some sort of geometric series because it's decreasing and I can see it converging to some 3d point in space. That reminds me of calculus. I've taken Calc 2 at University but that was a long time ago and we never talked about 3d space. Any help would be useful. Motivation And in case you're wondering why I'm interested in this. I've been interested in these scaling looping animations lately and I wanted to understand the math behind them. I found some videos by the people who make them (one of the guys mentioned in this video did an animation for Justin Bieber so they're legit) and even they don't understand the math. They're using their artistic skills to get close but that can be time-consuming. I want to know the math behind it. And I refuse to believe the math hasn't been worked out already. The point where I'm trying to find is where if you scale the whole spiral from, it has this unique property of looping perfectly. And if it's scaled at an exponential rate $\bigg(\dfrac{1}{s}\bigg)^{\frac{n(f - 1)}{a}}$ where $s$ is the scaling factor, $n$ is the number of cycles you want, $f$ is the current frame and $a$ is the length of the animation in frames, then it ends up looking like it's scaling linear. In this example , on the left you get the illusion of it scaling forever but on the right you can see what's really happening. If it doesn't scale at an exponential rate, then it starts slow and gets faster which also breaks the illusion. It's a 1m square that gets shrunk in half each time $(1 + 1/2 + 1/4 + ...)$ so I know the limit is $2$ . I'm then scaling the whole spiral from $(2, 0, 0)$ by a factor of $2$ over $50$ frames. It only scales perfectly from $(2, 0, 0)$ . This is what happens if I don't use the right point . It slowly drifts away and snaps back each cycle breaking the illusion. So that's why it's important for me for find this location. Updates A user on Reddit found a solution that works in 2D and I can confirm that it works. Their solution is as follows: Let $x'$ and $y'$ be the point where it converges. Then it converges at: $x' = Ax - By$ $y' = Bx + Ay$ where $A$ is the function $\frac{1 - k cos θ}{1 + k^2 - 2k cos θ}$ $B$ is $\frac{k sin θ}{1 + k^2 - 2k cos θ}$ and where $k$ is the scaling factor and $θ$ is the angle. Update 2 I think I'm getting closer. I've been told that: In two dimensions, the transformation T is given by $T = k R(θ) = k [ cos θ , -sin θ ; sin θ , cos θ ]$ , and therefore $T^n = k^n [ cos(nθ) , -sin(nθ) ; sin(nθ) , cos(nθ) ]$ . However, this cannot be easily generalized to three dimensions. You would need to explain more precisely what you mean by a rotation in three dimensions. Do you want a single rotation with respect to a given axis? If you want to apply three rotations with respect to the three coordinate axes, in what order do you apply them? How would I know what order to apply them since I can edit any one of them at any time?","['matrices', 'calculus', 'linear-transformations', 'rotations']"
4282744,Why is Trace function in $\mathbb{F}_{2^n}$ a map $\mathbb{F}_{2^n} \rightarrow \mathbb{F}_2$?,"Except from a paper: ( screenshot ) A function $F$ from $\mathbb{F}_{2^{n}}$ to $\mathbb{F}_{2^{n}}$ admits a unique representation, called Univariate Polynomial Representation , over $\mathbb{F}_{2^{n}}$ of degree at most $2^{n}-1$ : $$
F(x)=\sum_{j=0}^{2^{n}-1} \delta_{j} x^{j}, \text { with } \delta_{j} \in \mathbb{F}_{2^{n}}
$$ For every integer $j$ consider its binary expansion $\sum_{s=0}^{n-1} j_{s} 2^{s}$ and denote with $w_{2}(j)$ the number of nonzero coefficients (i.e. $\sum_{s=0}^{n-1} j_{s}$ ). The algebraic degree of the function $F$ is the $\max _{j=0, \ldots, 2^{n}-1 / \delta_{j} \neq 0} w_{2}(j)$ . Functions of algebraic degree 1 are called affine and of degree 2 quadratic . Linear functions are affine functions without the constant term and they can be represented as $L(x)=\sum_{j=0}^{n-1} \gamma_{j} x^{2^{j}}$ . A known example of a linear function defined over any dimension $n$ is the Trace function $\operatorname{Tr}(x)=\operatorname{Tr}_{n}(x)=\sum_{i=0}^{n-1} x^{2^{i}}$ , In particular the trace is a Boolean function, i.e. $\operatorname{Tr}: \mathbb{F}_{2^{n}} \rightarrow \mathbb{F}_{2}$ . For $m$ positive divisor of $n$ we use the notation $\operatorname{Tr}^{m}(x)=\sum_{i=0}^{n / m-1} x^{2^{i m}}$ . As you can see, in the paper the trace function $Tr$ is $\mathbb{F}_{2^n} \rightarrow \mathbb{F}_2$ . Shouldn't it be $\mathbb{F}_{2^n} \rightarrow \mathbb{F}_{2^n}\;$ ? I searched a lot over the internet, but couldn't find an explanation. Please help.","['finite-fields', 'functions']"
4282763,"What is a $\sigma$-algebra, really?","I know that a $\sigma$ -algebra is a suitable generalization of the notion of sample space , in the following sense: Consider a sample space $\Omega$ and a collection $\mathscr{F}$ of subsets of $\Omega$ .  Then $\mathscr{F}$ is called a $\sigma$ -algebra on $\Omega$ if the following are satisfied:
(1) $\phi \in \mathscr{F}$ .  (2)  If $A \in \mathscr{F}$ , then $A^C \in \mathscr{F}$ , that is, complementation is closed.  (3)  If $\{A_i\}$ is a sequence in $\mathscr{F}$ , then $\bigcup_i{A_i} \in \mathscr{F}$ , that is, countable union is closed. I would like to know what motivated this definition for a $\sigma$ -algebra, in simple terms.","['motivation', 'probability-theory', 'intuition']"
4282765,How to prove that every set in a sigma ring is a subset of a countable union of sets in it generator?,"One of the ""Remarks"" in our lecture notes says that if $\mathcal G$ is non-empty family of sets and $\sigma_{\mathcal R}(\mathcal G)$ is the sigma ring generated by $\mathcal G$ , then every set in $\sigma_{\mathcal R}(\mathcal G)$ is a subset of countable union of sets in $\mathcal G$ . Why is this remark true.","['measure-theory', 'functional-analysis']"
4282796,Almost sure convergence of maximum of sequence of random variables,"Let  $X_1, X_2, \dots$ be a sequence of i.i.d random variables from distribution $F$ with exponential tails. Denote $Y_n = \max (X_1, \dots , X_n)$. How can we prove the following:
$$ \lim_{n \rightarrow \infty} \frac{Y_n}{\log n} = c $$ 
almost surely for some constant $c$. And also, how can we determine what that value of $c$ is? What if we knew that the distribution $F$ is, say, a Gamma distribution (or another common distribution)? This result seems standard, as indicated in the question here , but I could not discover how to prove it.","['convergence-divergence', 'probability-theory']"
4282808,Dimension of solution space and adjoint solution space are equal,"Consider the system of linear ODE's and its linear adjoint system given by \eqref{1} and \eqref{2} respectively ( $x\in \mathbb{R}^n)$ . $$
\begin{align}
&x'(t)=M(t)x(t) \label{1} \tag{1}\\
&y'(t)=-M^\ast(t)y(t) \label{2}\tag{2}
\end{align}
$$ where $M$ is a continuous $n\times n$ matrix and $\omega>0$ periodic. If we denote say $S$ and $S^*$ to be the space of $\omega$ -periodic solutions to \eqref{1} and \eqref{2} respectively, is it true that their dimensions are equal? I was thinking if we can show an element of $S^*$ is in the dual of $S$ and by using the fact that the dimension of a vector space and its dual must be equal, the proof then follows trivially. I am not too sure how to solve this. Any help would be much appreciated. Thank you!",['ordinary-differential-equations']
4282814,cobordism for every spin structure on a boundary?,"Let's consider two n-dimensional closed Riemannan manifolds N and M that are cobordant to an undetermined (n+1) dimensional manifold W (That is: N and M are the boundary of W).
If all considered manifolds admit spin structures, then does each class of inequivalent spin structures on N,M determine a different cobordism (and hence W)?  An answer to this general question may be to long so I'll be more spedific: An example in 1-d is easiest. Choosing two circles as our 1-dimensional N and M, We have two spin structures (the connected or unconnected double cover over each). Trivially the connected double cover gives N and M each as the boundary of a disk $D^{2}$ (see for example here ). Then our W is the disjoint union of two 2-disks. Similarly, choosing the unconnected double cover gives N and M as the boundary of a cylinder. (Note here, that the connected sum of these surfaces is the topological sphere, does a similar result happen in higher dimensions...?) How much does this generalize? If I have g different spin structures on my boundaries, does each choice of spin structure correspond to a different W?  If so, what is the relation between these different W, are they homeomorphic (or diffeomorphic for $n\geq3$ )? My specific case: I am working with closed three-manifolds of genus g which each admit $2^{g}$ spin structures. How do I know which spin structures are a subset of a 4-manifold resulting from a cobordism of these manifolds? The last question is what I'm trying to answer, though an answer covering the general case would be amazing!","['geometric-topology', 'general-topology', 'spin-geometry', 'riemannian-geometry']"
4282861,Proof that there is a finite number of rationals within a given range of an irrational number?,"When studying the continuity of the Riemann function, the whole proof lies on the basis that are are finitely many rationals within a given range of an irrational. Such as part of the proof below:
Once you're given an irrational $x$ and an $\epsilon>0$ , there is an integer $n>1/\epsilon$ , and there are only finitely many rational numbers in, say, $(x-1,x+1)$ having denominator smaller than $n$ is lowest terms.  Thus there is a closest one to $x$ , and you can use this to find your $\delta$ . Is there an intuitive way to explain or prove this basis?","['epsilon-delta', 'continuity', 'functions', 'riemann-zeta', 'limits']"
4282937,Set of singular points of a normal variety.,"In my algebraic geometry course these result appeared: If X is a normal variety, then the set of singular points $S$ have codimension $\geq 2$ . (Here normal means that $\mathcal{O}_x$ is integrally closed for all $x\in X$ ) I've read many proofs of this fact in books and I understood them. However my teacher said that it was a ''fairly easy exercise'' and gave us three steps to do it: Consider the case when $S$ has an irreducible component $Y$ of codimension 1, defined by an equation in $X$ Let $Y$ an afine subvariety of $X$ with $\text{codim}(Y)=1$ . Prove that $\mathcal{O}_Y$ is a discrete valuation ring, and hence the maximal ideal of $\mathcal{O}_Y$ is principal. Prove the general case using 1. and 2. I'm trying to find a conection with these and the proofs I've found in the books, but I don't see it. If I can get some help on how to aproach this exercise in this particular proof I'll be very grateful. (Although my teacher said it was not a dificult exercise, it does not look easy for me) Edit: My problem is that most books solve it in a short way but using plenty of previous results. What I am looking (if possible, I don't know if this exists) is a complete proof without refering to any strong result. I have a nice background on commutative algebra and abstract algebra in general, but very little knowledge about algebraic geometry (I've recently started to study it). So I am looking for a proof using commutative algebra methods. Of course, I dont want you to do all the work, I will be very happy with just an sketch of the proof, to make me think a bit and complete the gaps. Edit: This is my attempt, if you can help me to complete/correct it: Let $Y=V(f)$ and $y\in Y$ a point which is non-sigular as a point of $Y$ (this is posible since the set of singular points of a variety is proper). If we show that $y$ is non-singular as a point of $X$ we are done. Let us denote by $m_{X,y}$ , the maximal ideal of the local ring of $X$ at $y$ (same with $m_{Y,y}$ ). If $f_1,\dots,f_n$ is a basis for $m_{Y,y}/m_{Y,y}^2$ , then $f,f_1,\dots,f_n$ is a basis for $m_{X,y}/m_{X,y}^2$ and this proves that $y$ is non-singular, contradiction. The definition I'll use for DVR is Noetherian, dim 1, local and integrally closed. $\mathcal{O}_Y$ is known to be local, and it is integrally closed because $X$ is normal. Also, since it is the localization of a noetherian ring it is noetherian. But I don't know how to prove that it has dimension 1. No ideas for this step. I guess that using 2. I should be able to prove the existence of a component $Y$ as in 1, but don't know how to start.","['algebraic-geometry', 'dimension-theory-algebra']"
4282980,"Evaluate $\int_0^t e^{-\lambda s} \,\text{erf}(\ln(t-s))\,\text{d}s$?","I'm trying to efficiently graph the function $I(t)$ for $t>0$ where $$I(t) :=\int_0^t e^{-\lambda s} \,\text{erf}(\ln(t-s))\,\text{d}s,\qquad \lambda>0$$ but its evaluation is beyond my powers of integration. I can use a numerical integration package to plot it, but it is pretty slow. Happy to compute a decent approximation if it's available. Mathematica was not able to provide an answer, and I've tried some integral substitutions like $u=\ln(t-s)$ , as well as looked through the fantastic table of erf integrals , but no solution seems to be clear. My current strategy is to approximate the exponential by a quartic polynomial $$\exp(-\lambda x)\approx \sum_{k=0}^4 c_k (\lambda x)^k$$ in some region $x\in[0,R]$ , and set to zero when $x>R$ . Then I can compute $\int s^k \text{erf}(\ln(t-s))\,\text{d}s$ , but there are a painful number of terms when using a quartic - are there any better suggestions? Perhaps some other approximation of the $\exp$ , or nice series expansion of the function? Thanks! Edit : Just to be clear, imagine I am given ten thousand different values of $\lambda$ , and I want to graph $I(t)$ for each of them in the region $t\in [0,100]$ . How can I do this efficiently?","['integration', 'definite-integrals', 'special-functions', 'convolution', 'error-function']"
4282981,Doubt on a method used in an article ( extreme value problem of trig function),"This might be a straightforward problem but I couldn't figure it out on my own. start $$
\begin{aligned} \mathcal{W}_{\mathbf{p}, \theta} &=\frac{1}{2}\left[\cos \alpha+\cos (\theta-\alpha)+2 \cos \frac{\theta}{2}+4 \sin \frac{\theta}{2}\right] \\ &=(2 \mathbf{p}-1)\left(\cos \frac{\theta}{2}\right)^{2}+2 \sqrt{\mathbf{p}(1-\mathbf{p})} \cos \frac{\theta}{2} \sqrt{1-\left(\cos \frac{\theta}{2}\right)^{2}}+\cos \frac{\theta}{2}+2 \sqrt{1-\left(\cos \frac{\theta}{2}\right)^{2}} \end{aligned}
$$ where $\mathbf{p}=\frac{1+\cos \alpha}{2}$ . Furthermore, in order to obtain the maximal value of $\mathcal{W}$ expression only about the maximal guessing probability $\mathbf{p}$ (i.e., the angle of $\alpha$ ), we use the method of the extreme-value problem of function and let $x=\cos \frac{\theta}{2} .$ Applying to the equation above, we get $$
\mathcal{W}_{\mathbf{p}}^{\max }=\max _{\{r\}}\left\{r+(2 \mathbf{p}-1) r^{2}+2 \sqrt{1-r^{2}}+2 \sqrt{\mathbf{p}(1-\mathbf{p})} r \sqrt{1-r^{2}}\right\}
$$ where $r$ is one of the real roots of $4 x^{4}+4[(2 \mathbf{p}-1)+4 \sqrt{\mathbf{p}(1-\mathbf{p})}] x^{3}+x^{2}-4[(2 \mathbf{p}-1)+2 \sqrt{\mathbf{p}(1-\mathbf{p})}] x+(2 \mathbf{p}-1)^{2}=$ $0 .$ end How do they get the polynomial equation? What method is this? I needed to find the extrema of a multivariable trigonometric function. Can I apply the same method there? Kindly help in any way possible. This was written in the supplementary of this article .","['extreme-value-theorem', 'maxima-minima', 'polynomials', 'optimization', 'trigonometry']"
4282982,Growth function $\tau_{\mathcal{H}}(m)$ lower bound,"I have been working on this problem for a long time and I would like some help. They ask me to find for each $ n $ a hypothesis class $ \mathcal {H} \subset \{\pm 1 \}^{\mathbb {N}} $ with $ n $ elements such that the growth function is $ \tau_{\mathcal{H}}(m) =\min\{ m+1,n \}$ . And they ask me to prove that in general $\tau_{\mathcal{H}}(m) \geqslant \min\{ m+1,n \}$ if $|\mathcal{H}|=n$ . I have worked with the indicator functions of the sets $\{1\}, \{2\},\ldots, \{n\}$ and $\{1\}, \{1,2\},\ldots, \{1,2,\ldots,n\}$ but in no case have I obtained results. I would appreciate if you could help me. A hypothesis class is just a designation for that subset of functions of $ \{\pm 1 \}^{\mathbb{N}}$ (in this case). And the growth function is $$\tau_{\mathcal{H}}(m)=\sup |\mathcal{H}_{C}|$$ where the supreme is taken over all subsets $ C \subset \mathbb{N}$ such that $|C|=m$ . Here, $$\mathcal{H}_{C}=\{ h|_{C}: h\in \mathcal{H} \}$$","['machine-learning', 'statistics', 'probability-theory']"
4283000,Difference between EPE and MSE,"In the book ESL ( Element of Statistical Learning ), the author introduces the EPE ( Expected prediction Error ) and the MSE ( Mean Squared Error ).
I know that the EPE is defined as: $$EPE(f)=E(Y-f(X))^2$$ which is the expected value generated on all the different training data set. But what abount the MSE? The author defined the MSE like: $$MSE(x_0) = E_T[f(x_0)-\widehat{y_0})]^2$$ and i really don't get the difference...
The question is: basically speaking, what's the difference between EPE and MSE?","['machine-learning', 'statistics']"
4283027,Defining derivatives in Real/Complex Analysis?,"When studying Real Analysis, I was given the following definition for derivatives: Let $f:D \rightarrow \mathbb{R}$ , where $D$ is some subset of $\mathbb{R}$ and $a \in D$ be a cluster point of $D$ (For each $\epsilon > 0$ there is an $x \in D$ with $0<|x-a|<\epsilon$ ). Then $f$ is differentiable at $a$ if $\lim_{x\rightarrow a} \frac{f(x)-f(a)}{x-a}$ exists and in this case we denote the limit $f'(a)$ And then in Complex Analysis, I have the following definition: Take $U \subseteq \mathbb{C}$ open, a function $f:U \rightarrow \mathbb{C}$ and $c \in U$ . Then $f$ is complex differentiable at $c$ if $\lim_{z\rightarrow c} \frac{f(z)-f(c)}{z-c}$ exists and we write $f'(c)$ for this limit My question is, why is $U$ required to be open in the complex case, but $D$ is not required to be open in the real case? I can see that if $U$ is an open set, then each $c \in U$ is a cluster point of $U$ , but it seems that having the domain $U$ be an open set in the complex case is a much stricter condition than in the real case where you only require that the individual point in question $a \in D$ is a cluster point of $D$ . Why is this condition necessary in the complex case?","['real-analysis', 'complex-analysis', 'functions', 'limits', 'derivatives']"
4283101,How can I intuitively predict the shape of a solution to ordinary differential equations?,"I am new to differential equations, and I realized that I don't have even an intuition as to what solutions to ordinary differential equations would roughly look like. For example, given the governing equation: $$ \frac{\partial^2 y}{\partial x^2} -\frac{\partial y}{\partial x} + y = 0 $$ for the range $$0<x<10$$ given the initial conditions $$y(0) = 1$$ $$y(10) = 5$$ The plotted graph for the function y looks like: How can you intuitively predict that y will be mostly flat in the range 0<x<6? If so, how would you intuitively picture the solution for this function y without solving for it analytically?","['calculus', 'ordinary-differential-equations']"
4283111,Proving (non)existence of a measure space and sequence satisfying two properties,"Setting Let $\mathbb N_0 = \mathbb Z_{\geq 0}$ be the natural numbers including zero. Let $(X, \Sigma, \mu)$ be a measure space and $(p_i)_{i=0}^\infty$ be a sequence of measurable maps $p_i\colon X \to [0,1]$ satisfying $$\forall x \in X\colon ~\sum_{i=0}^\infty p_i(x) = 1, \tag{1}$$ and $$\forall i,j\in \mathbb N_0\colon~ \int_X p_i(x)p_j(x) \mathop{}\!\mathrm{d}\mu(x) = \begin{cases} 
  1/2&\text{if } i \neq j\\
  1&\text{if } i = j.
\end{cases}\tag{2}$$ Questions Can we prove that a pair $\left[(X, \Sigma, \mu), (p_i)_{i=0}^\infty \right]$ satisfying (1) and (2) can or cannot exist? If it can exist, can we construct an example? If it can exist, can we prove that the space could be $\sigma$ -finite or can we prove that it must not be $\sigma$ -finite? If it can exist, can we say anything about the uniform integrability of $p_i$ ? Any other interesting properties we can say about the pair? More generally, is this question even well-posed? Hopefully it's not trivial :) Attempts By summing over $i, j$ in (2), we find that $\mu(X)$ cannot be finite. Using the $L^p$ Dominated Convergence Theorem, we can prove that, for example, for a fixed $i$ , $$\lim_{j \to \infty}\int_X \left(p_i(x) p_j(x)\right)^2 \mathop{}\!\mathrm{d}\mu(x) = 0,$$ but this doesn't (at least I don't think) tell us that $$\lim_{j \to \infty} \int_X p_i(x) p_j(x) \mathop{}\!\mathrm{d}\mu(x) = 0,$$ which would prove nonexistence. This theorem can almost be applied, but not quite because I see no reason why $p_i(x)p_j(x)$ cannot be zero for some $x$ . I'm very unsure about this one, but looking at Vitali's convergence theorem and this paper , it seems like we can possibly apply them to say that either the measure space is not $\sigma$ -finite or $p_i$ is not uniformly integrable? Thanks! Any solutions or suggestions for how to proceed would be greatly appreciated :) Even a list of potentially relevant theorems and lemmas in measure theory would also be helpful.","['sequence-of-function', 'measure-theory']"
4283115,Proving $\lim_{x \to +\infty}\frac{\ln(x)}{2\cdot \ln(x+1)} = \frac{1}{2}$ with $\epsilon- \delta$,"Today, I spent some hours trying to solve this problem but I can't reach the conclusion: Show with $\epsilon-\delta $ definition the following limit: $$\lim_{x \to +\infty}\frac{\ln(x)}{2\cdot \ln(x+1)} = \frac{1}{2}$$ So, first of all, I set up the inequality: $$\left|\frac{\ln(x)}{2\cdot \ln(x+1)}-\frac{1}{2}\right|<\epsilon$$ Then: $$\frac{1}{2}-\epsilon<\frac{\ln(x)}{2\cdot \ln(x+1)}<\frac{1}{2}+\epsilon \implies (1-2\epsilon)\cdot \ln(x+1) < \ln(x) < (1+2\epsilon)\cdot \ln(x+1)$$ And elevating with base $e$ to cancel the $\ln(x)$ : $$(x+1)^{1-2\epsilon}<x<(x+1)^{1+2\epsilon}$$ I want to apply the limit $\lim_{x \to +\infty}\frac{(1+f(x))^\alpha-1}{\alpha\cdot f(x)}=1$ with $f(x) \to_{x \to 0} 0$ . So, I have: $$x^{1-2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1-2\epsilon}<x<x^{1+2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1+2\epsilon}$$ Then: $$
\left\{\begin{matrix}
x>x^{1-2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1-2\epsilon}
\\ x<x^{1+2\epsilon}\cdot\left(1+\frac{1}{x} \right )^{1+2\epsilon}
\end{matrix}\right.
$$ So: $$
\left\{\begin{matrix}
x^{2\epsilon} - 1>\left(1+\frac{1}{x}\right)^{1-2\epsilon} - 1
\\ \frac{1}{x^{2\epsilon}}-1<\left(1+\frac{1}{x} \right )^{1+2\epsilon}-1
\end{matrix}\right.
$$ When $x$ approaches $+\infty$ : $$
\sim
\left\{\begin{matrix}
x^{2\epsilon}-1>\frac{1-2\epsilon}{x}
\\\frac{1}{x^{2\epsilon}}-1<\frac{1+2\epsilon}{x}
\end{matrix}\right.
$$ Notice that the second inequality is always true because $\frac{1}{x^{2\epsilon}}\to 0^+$ when $x \to +\infty$ , so that left side is negative while right side $\frac{1-2\epsilon}{x}$ is positive. Now, in some way I have to find $\delta = N$ . I noticed that: $$x^{2\epsilon}-1\sim x^{2\epsilon} \;\;\; x \to +\infty$$ and then for the first inequality: $$x^{2\epsilon}>\frac{1-2\epsilon}{x} \implies x> \delta(\epsilon)= N =(1-2\epsilon)^{1+2\epsilon}$$ Here, tere is a problem because when $\epsilon \to 0^+$ , $\delta = N \to 1$ and not to $+\infty$ . Is this anyway correct? Or, I can't apply asympothics reduction with $\epsilon - \delta$ ? Edit: I  all three solutions proposed, $\delta = N$ goes to infinity when $\epsilon \to 0^+$ . In my answer, $N$ tends to $1$ , so is my solution not corret? If it's yes, why? Thanks.","['limits', 'calculus', 'solution-verification', 'epsilon-delta']"
4283149,If $\mu(A\triangle B)=0$ then $\mu(A)=\mu(B)$,Let $\mu$ is finite measure and $\mu(A\triangle B)=0$ . Show that $\mu(A)=\mu(B)$ My work: $\mu(A\triangle B)=0=\mu(A|B)+\mu(B|A)=(\mu(A)-\mu(A\cap B))+(\mu(B)-\mu(B\cap A))$ So $\mu(A)=-\mu(B)+2\mu(B\cap A)$ How is $\mu(A)=\mu(B)$ ?,"['elementary-set-theory', 'measure-theory', 'functional-analysis', 'real-analysis']"
4283209,How to get the maximum value of the sum in two sides in two right triangles?,"The problem is as follows: The figure from below shows two triangles intersected on point $E$ . Assume $AE=3\,m$ and $ED=1\,m$ . Find the maximum value of $AB+EC$ The given choices in my book are as follows: $\begin{array}{ll}
1.&\sqrt{10}\,m\\
2.&\4\,m\\
3.&\sqrt{5}\,m\\
4.&\4\,m\\
\end{array}$ The official solution for this problem according to my book is shown below: Let $M=AB+EC$ Notice on the figure: $AB=3\sin x$ and $EC=\cos x$ $M=3\sin x+\cos x$ $M=\sqrt{10}\left(\frac{3}{\sqrt{10}}\sin x+\frac{1}{\sqrt{10}}\cos x\right)$ $\tan \alpha =\frac{1}{3}$ $M=\sqrt{10}\left(\sin (x+\alpha)\right)$ Also notice that $x$ and $\alpha$ are acute angles. Therefore: $0<x+\alpha<\pi$ $\sin (x+\alpha)\leq 1$ Thus the maximum $M$ is $\sqrt{10}$ . There is where it ends the official solution. But I am confused, where exactly is that alpha that is being talking about?, how did the author came up with the idea of $\sqrt{10}$ and the use of $\frac{1}{3}$ . Could someone please explain me, the logic that the author used and offer an alternate solution , perhaps easier to understand and less to guess what it was meant?. I appreciate a step by step solution so I can understand.","['maxima-minima', 'algebra-precalculus', 'trigonometry']"
4283292,Is there an elementary way to understand this trigonometric identity?,"Let $\{k_r\}$ be the $N$ solutions between $0$ and $\pi$ of $\cos(N k) = h$ , where $\lvert h \rvert <1$ . I have come across the following identity in a physics research problem: \begin{align}
 \Bigg(\prod_{r=1}^N 2 \sin(k_r) \prod_{1\leq r<s\leq N}  
\Bigg(\frac{\sin^2 (\frac{k_r-k_s}{2})}{\sin^2 (\frac{k_r+k_s}{2})} \Bigg)^{(-1)^{r+s+1}}\Bigg)^{1/N}=\frac{2\sqrt{1-h^2}}{N}.
\end{align} I am able to prove it by calculating the same quantity in two different ways, but I was wondering if there is an elementary way to understand it? If anyone knows a reference that contains such an identity I would be interested.",['trigonometry']
4283332,"Prove that $(A_0,A_1)_{\theta,q}$ is Banach.","Let $A_0$ , $A_1$ be two Banach spaces, both embedded continuously in a Hausdorff topological vector space $\mathcal{A}$ . Then we can consider the normed spaces $A_0 \cap A_1$ and $A_0 + A_1$ with the norms $$
||a||_{A_0 \cap A_1} = \max\{ ||a||_{A_0}, ||a||_{A_1}\},
$$ $$
||a||_{A_0 + A_1} = \inf \{ ||a_0||_{A_0} + ||a_1||_{A_1} : a=a_0+a_1, a_j \in A_j\},
$$ and following this, define the real interpolation space $(A_0,A_1)_{\theta,q}$ consisting of those elements $a \in A_0 + A_1$ with finite norm: $$
||a||_{\theta,q} = \left( \int_0^{\infty} \left(t^{-\theta}K(t,a) \right)^q \frac{dt}{t} \right)^{\frac{1}{q}}, 1\leq q <\infty,
$$ where $K(t,a) = \inf\{ ||a_0||_{A_0} + t||a_1||_{A_1}: a=a_0+a_1, a_j \in A_j\}$ is Peetre K-functional. My task now is to prove that $(A_0,A_1)_{\theta,q}$ is a Banach space. I have already proven that, with this construction, $A_0 + A_1$ is a Banach space, as well as the fact that $(A_0,A_1)_{\theta,q}$ is continuously embedded in $A_0 + A_1$ with the norms defined above. So, if I'm not mistaken, it would be enough to prove that the interpolation space $(A_0,A_1)_{\theta,q}$ is a closed subspace of $A_0+A_1$ and using the fact that the latter is Banach, the former would be Banach as well. However, my efforts have been unfruitful so far. I'd like to show that if I take a succession $\{x_n\} \subset (A_0,A_1)_{\theta,q}$ that converges to $x\in A_0+A_1$ , then: $$
||x||_{\theta,q} = \left( \int_0^{\infty} \left(t^{-\theta}K(t,x) \right)^q \frac{dt}{t} \right)^{\frac{1}{q}} <\infty,
$$ so $x \in (A_0,A_1)_{\theta,q}$ , proving it's a closed subspace. Any hints on how to proceed towards my goal would be appreciated.","['banach-spaces', 'interpolation-theory', 'functional-analysis']"
4283339,A limit involving powers and sums of primes,"This limit is one of three from a question that was closed for lacking focus and context. I can't speak to the source of the problem but I can include my own attempts and thoughts in an effort to resuscitate it. Let $p_n$ denote the $n^{th}$ prime number. Claim: $$
 \lim_{n\to\infty}\frac{\sqrt[np_n]{p_n^{p_1+\cdots+p_n}}}{\sqrt[np_n]{p_1^{p_1}\cdots p_n^{p_n}}}=e^{1/4}
$$ Taking the logarithm and simplifying, it amounts to showing $$
\lim_{n\to\infty} \frac{1}{n p_n} \sum_{k=1}^{n} p_k \log\left(\frac{p_n}{p_k}\right)=\frac{1}{4}
$$ Using the very crass approximation $p_n\approx n$ and asymptotics of the hyperfactorial function $H(n) = \prod_{k=1}^n k^k$ (here $A$ is the Glashier constant, but it doesn't matter), it works out: $$
\lim_{n\to\infty} \frac{1}{n^2}\left(\log(n)\cdot \frac{n^2+n}{2}-\log(H(n))\right)
$$ $$
=\lim_{n\to\infty} \frac{1}{2}\log(n)  + \frac{\log(n)}{2n}- \frac{1}{n^2}\log\left(e^{-n^2/4} n^{(6n^2+6n+1)/12}(A+O(n^{-2}))\right)
$$ $$
=\frac{1}{4}+\lim_{n\to\infty} \frac{1}{2}\log(n)  + \frac{\log(n)}{2n}- \frac{6n^2+6n+1}{12n^2}\log(n)+\frac{o(n)}{n^2}= \frac{1}{4}
$$ Of course, this is quite crass and it's not even clear to me that replacing $p_n$ with $n$ provides a lower bound. By the Prime Number Theorem, the first asymptotic we should be using is $p_n\approx n \log(n)$ , but the calculation becomes formidable at this point and I'm not sure how many terms to include in the asymptotic. Still, I believe the claim is true and would like to see a proof.","['limits', 'asymptotics', 'elementary-number-theory', 'prime-numbers']"
4283367,Solving $\int_{0}^{2\pi}\frac{d\phi}{2+\sin(\phi)}$ with complex exponentials,"I’m working through the textbook A First Course in Complex Analysis and I’m trying to solve problem 4.30: Compute the real integral $$\int_{0}^{2\pi}\frac{d\varphi}{2+\sin(\varphi)}$$ by writing the sine function in terms of the complex exponential and making the substitution $z=e^{i\varphi}$ to turn the real integral into a complex integral. I started by rewriting the sine function as $\frac{e^{i\varphi}-e^{-i\varphi}}{2i}$ : $$=\int_{0}^{2\pi} \frac{d\varphi}{2+\tfrac{e^{i\varphi}-e^{-i\varphi}}{2i}}=2i\int_{0}^{2\pi} \frac{d\varphi}{4i+e^{i\varphi}-e^{-i\varphi}}$$ Then I substituted $z=e^{i\varphi}$ , as the problem asked, and got $dz=izd\varphi$ , so $d\varphi=\tfrac{-idz}{z}$ . I also defined my path as $\gamma:[0,2\pi]\rightarrow \mathbb{C}$ as $\gamma(\varphi)=e^{i\varphi}$ $$=2i\int_{\gamma} \frac{1}{4i+z-\tfrac{1}{z}} \frac{-idz}{z}=2\int_{\gamma} \frac{dz}{z^2+4iz-1}$$ . We can see that the inside of the integral is a rational function, so it should be holomorphic everywhere where it’s defined. The poles can then be calculated by the quadratic equation as $\pm \sqrt{3}i-2i$ . However, these poles clearly lie outside of the region enclosed by $\gamma$ , the unit circle. Since the inside of the integral is defined and holomorphic on all points on $\gamma$ and inside $\gamma$ , and $\gamma$ is a closed curve, we have: $$2\int_{\gamma} \frac{dz}{z^2+4iz-1}=2\cdot0=0$$ However, this is very obviously wrong, because the original integral is always positive and hence can’t be zero. So my questions are: Where is my mistake? How would you solve this correctly?","['integration', 'complex-analysis', 'cauchy-integral-formula']"
4283392,What is a reasonable bound for integrating the standard normal distribution with a single tail?,This is a question I'm trying to figure out how to solve because my calculator does not allow me to evaluate improper integrals. I am trying to find the z that produces a small enough p to be negligible. In general I think I will need about 5 digits of accuracy maximum but I would also be interested in the general solution for some natural n. The problem boils down to solving this inequality: Find the least value of $a$ for which: $$\int_a^\infty \frac{\exp(-x^2/2)}{\sqrt{2\pi}} < 10^{-n}$$ Any insights on how I might go about this? This function doesn't even have an elementary antiderivative so this looks like a pretty tricky problem.,['statistics']
4283399,"Given a r.v. $X$, can we choose a r.v. $Y$ independent of $X$ and with the same distribution?","The question originates in the following problem: Given that $\phi$ is a characteristic function, show that so is $| \phi |^2$ . The solution given by a lecturer was as follows: Suppose $\phi$ is the characteristic function of a random variable $X$ . Choose another random variable $Y$ which is independent of $X$ and has the same distribution. Then $| \phi |^2$ is the characteristic function of $X-Y$ . I agree with the last part once $Y$ is chosen, but it does not seem completely obvious to me that you can necessarily choose such a $Y$ . $Y$ has to be defined on the same probability space as $X$ in order for $X-Y$ to be defined. So my question is whether it is so that given any random variable $X$ defined on the space $( \Omega, \mathcal{F}, P)$ , we may choose another random variable $Y$ defined on $( \Omega, \mathcal{F}, P)$ such that $X$ and $Y$ are independent and have the same distribution? If so, how can we show that?","['characteristic-functions', 'independence', 'probability-theory', 'random-variables']"
4283421,How to compute the dual of an optimization problem defined on a function space?,"I am interested in one result in the first version of the paper titled ""On the Margin Theory of Feedforward Neural Networks"" by Colin Wei, Jason D. Lee, Qiang Liu and Tengyu Ma. In Equation B.1 of the appendix (page 17), the authors introduce the following optimization problem (which relates to the maximum margin of an $\ell_1$ -SVM classifier) : $$\begin{align}
\min_{\alpha\in\mathcal L^1(\mathbb S^{d-1})} &||\alpha||_1 = \int_{u\in\mathbb S^{d-1}} |\alpha(u)|du \\
\text{subject to }\, &y_i\langle \alpha,\varphi(x_i)\rangle \ge 1 \end{align} \tag1$$ Here, $\mathbb S^{d-1}$ is the unit sphere in $\mathbb R^d$ , and $\mathcal L^1(\mathbb S^{d-1})$ is the set of real-valued functions defined on $\mathbb S^{d-1}$ with Lebesgue integrable absolute value. For all $i \in [\![ 1;n ]\!]$ , the pair $(x_i,y_i)\in\mathbb R^d \times \{-1,1\} $ represents datapoints and their associated labels, while $\varphi$ is a ""lifting function"" mapping any datapoint $x_i$ to the infinite dimensional space $\mathcal L^\infty(\mathbb S^{d-1}) $ , and thus $\varphi(x_i)\in\mathcal L^\infty(\mathbb S^{d-1})$ . Lastly, $\langle\cdot,\cdot\rangle$ represents the dot product : $\langle f,g\rangle =\int_{u\in\mathbb S^{d-1}} f(u)g(u)du $ . In claim B.3 of the paper (page 17), the authors claim that the dual of equation $(1)$ has the following form : $$ 
\begin{align}
\max_{\lambda\in\mathbb R^n} &\sum_{i=1}^n \lambda_i \\
\text{subject to } &\left\lvert\sum_{i=1}^n \lambda_i y_i \phi(u^Tx_i)\right\rvert\le 1\; \forall u\in\mathbb S^{d-1}\\
&\lambda_i \ge 0 \; \forall i \in [\![ 1;n ]\!]
\end{align} \tag2
$$ Where $\phi$ is such that $\forall u \in \mathbb S^{d-1}, \forall x \in \mathbb R^d, \varphi(x)[u] = \phi(u^Tx)$ . The details on how to prove that $(2)$ is the dual of $(1)$ are however not given and I fail to prove it myself, being quite unfamiliar with Lagrangian optimization on function spaces. My question is thus : How to prove that $\mathbf{(2)}$ is the dual of $\mathbf{(1)}$ ? I attempted to do the proof by proceeding like in the more common ""subset of $\mathbb R^d$ "" case, i.e. minimizing the Lagrangian, which according to my calculations has the following expression : $$L(\alpha,\lambda) = ||\alpha||_1 + \sum_{i=1}^n\lambda_i\left(1-y_i\left\langle\alpha,\varphi(x_i)\right\rangle\right) $$ However, I can't get much further than this, as I don't know how to find the minimum of $L(\alpha,\lambda)$ for $\alpha\in\mathcal L^1(\mathbb S^{d-1})$ . Even though the paper suggests to take $L^*(\lambda) = \sum_i \lambda_i $ , I don't know how to prove that it is indeed the solution, and I have no idea where that second constraint with the absolute value in $(2)$ comes from. I'm afraid that my approach is totally wrong. Any help with this problem will be much appreciated.","['machine-learning', 'optimization', 'statistics']"
4283545,Finite $\sigma$-algebras are generated by unique minimal finite partitions,"I am trying to prove the following statement: Let $(X,\mathcal A)$ be a measure space s.t. the $\sigma$ -algebra $\mathcal A$ is finite, then there exists a unique minimal finite partition $\mathcal P=\{P_1,\dots,P_n\}$ of $X$ s.t. $\sigma(\mathcal P)=\mathcal A$ and for all $A\in\mathcal A$ holds $$(*)\qquad A\cap P_k\in\{\emptyset,P_k\}.$$ Here's what I have so far: Let $\mathcal A=\{\emptyset,X,A_1,\dots,A_m\}$ , where $A_i$ are the nontrivial elements of the algebra (assume $m>0$ , there is nothing to prove for $\mathcal A=\{\emptyset, X\}$ ), then $\bigcup_iA_i=X$ (otherwise $\left(\bigcup_iA_i\right)^c=A_j$ for some $j$ , which is a contradiction). Disjointise the $A_i$ and get $B_i\in\mathcal A$ s.t. $\bigcup_iB_i=X$ and $B_i\cap B_j=\emptyset$ for $i\neq j$ (standard measure theoretic trick). Each $B_i$ is either empty or equal to $A_{k_i}$ for some $k_i$ , so discard the empty ones and get $P_j:=A_{k_j},1\leq j\leq n$ , which parition $X$ . For $A\in\mathcal P$ the condition (*) is trivial [for $A\in\mathcal A$ arbitrary I'm having trouble showing this]. Now, assuming (*), we get for an arbitrary $A\in\mathcal A$ : $$A=A\cap X=\bigcup_jA\cap P_j=\bigcup_kP_k,$$ so $A\in\sigma(\mathcal P)$ and therefore $\mathcal A=\sigma(\mathcal P)$ . I'm having trouble showing that the partition I get this way is both unique and minimal and that it has property (*). Any tips on how to proceed? Also, if $\mathcal A,\mathcal P$ are as above, would it be correct to say that $|\mathcal A|=2^{|\mathcal P|}$ ? It seems plausible to me that intersection and complementation in this situation do not produce anything other than (disjoint) unions of elements of $\mathcal P$ , so the number of elements of $\mathcal A$ should be given by the number of subsets of $\mathcal P$ (each subset $\{P_{k_1},\dots,P_{k_n}\}$ determines the element $\bigcup_iP_{k_i}$ of $\mathcal A$ ). In particular, the cardinality of a finite $\sigma$ -algebra is always a power of $2$ . Would this be a correct argument?","['elementary-set-theory', 'measure-theory', 'solution-verification']"
4283565,Joint distribution is absolute continuous with respect to product of marginal distribution.,"Consider the following: Suppose $X,Y$ are random variables with joint distribution $Q$ (For simplicity, assume $Q$ is a probability measure on $\mathbb{R}\times \mathbb{R}$ ). Let $\mu$ , $\nu$ be marginal distribution of $X,Y$ respectively. Is it true $Q$ absolute continuous with respect to $\mu\times \nu$ ? For rectangles, $\mu\times \nu(A\times B)=0$ certainly implies $Q(A\times B)=0$ . However, I have difficulty generalize this to all measurable sets. (Personally I believe this is true.) I tried to use $\pi-\lambda$ theorem. Let $\mathcal{G}$ denote the class of sets satisfying the statement: If $\mu\times \nu(A)=0$ , then $Q(A)=0$ . Then try to proof $\mathcal{G}$ includes all measurable sets. However, $\mathcal{G}$ is not a $\lambda$ system. i.e. It is not closed under complement. Is there any other ways  to prove this? Thanks in advance !","['measure-theory', 'probability-theory']"
4283571,Local max of ratio of elementary symmetric polynomials,"Let $e_k(x_1,\cdots, x_n) := \sum_{1\leq j_1 < \cdots < j_k \leq n}x_{j_1}\cdots x_{j_k}$ be elementary symmetric polynomials (see e.g. https://en.wikipedia.org/wiki/Elementary_symmetric_polynomial ). For any fixed $0<x_1\leq x_2 \leq \cdots \leq x_n \in \mathbb{R}$ let's define the following: \begin{equation}
s_k := \frac{1}{k}\left(1 + \frac{e_{n-k+1}(x_1,\cdots,x_n)}{e_{n-k}(x_1,\cdots,x_n)}\right)
\end{equation} for $k = 1,\cdots,n$ . I would like to know whether the following is true: There exists no $k = 2,\cdots,n-1$ such that $s_k > s_{k-1}$ and $s_k > s_{k+1}$ . (Prove or Disprove) In other words, the sequence $s_k$ has no 'local maximum', for any given fixed $0<x_1\leq x_2 \leq \cdots \leq x_n$ . I could verify in the simplest case where $x_1 = x_2 = \cdots = x_n =: x$ that this statement holds, we have $e_k(x,\cdots,x) = (n!/k!(n-k)!)x^k$ , and so \begin{equation}
s_k = \frac{1}{k}\left(1 + \frac{(n-k)!k!}{(n-k+1)!(k-1)!}x\right) = \frac{1}{k}\left(1 + \frac{k}{n-k+1}x\right) = \frac{1}{k} + \frac{x}{n-k+1}.
\end{equation} In this case, for any $k = 2,\cdots,n-1$ , we will either have $s_{k-1} \leq s_k \leq s_{k+1}$ , or $s_{k-1} \geq s_k \geq s_{k+1}$ , or $s_k \leq s_{k-1}, s_{k+1}$ , but not $s_k > s_{k-1}, s_{k+1}$ . I'm not sure how to prove (or disprove) the statement in general, but I numerically compute $s_k$ for randomized $0 < x_1 \leq x_2 \leq \cdots \leq x_n$ many times and the statement appears to be true so far. Below is the plot of an example computed $s_k$ with $n = 20$ for a sample set of $0<x_1\leq x_2\leq \cdots\leq x_n$ : as you can see there is no local maximum. If someone could help or suggest ways to either prove or disprove the statement I would really be appreciated!","['sequences-and-series', 'symmetric-polynomials', 'combinatorics', 'real-analysis']"
4283612,Maximizing expression under assumption that similar expression is maximized.,"I'm trying to maximize the following expression: $$
\left(\frac{a_{2}}{4^{k_{1}}}-4 a_{2}^{3}\right) \cdot 1+\sum_{j=2}^{m}\left(\frac{a_{j+1}}{4^{k_{j}}}-4 a_{j+1}^{3}\right) \prod_{i=2}^{j}\left(\frac{4^{k_{i}}}{4^{k_{i-1}}}-1\right)
$$ where $k_i$ is a positive integer such that $a_i \in (2^{-k_i-1}, 2^{-k_i}]$ for $i = 1,...,m$ . We can assume that $a_i \in [0,1]$ and that the values are decreasing i.e if $a_i \in (2^{-k_i-1}, 2^{-k_i}]$ then $a_{i+1} \leq 2^{-k_i-1}$ . Based on some experimentation I have a strong suspicion that the expression is maximized
for $a_{i+1}=1 / 2^{k_{i}+2}$ . I figured that induction
and local optimality will probably do the trick: Assume the result for limited $i$ , apply the assumption, then optimize the terms with $i+1$ under the assumption
that the smaller terms must also be chosen optimally wrt each other. However, I can't really get anywhere with this. It is stated in this [Page 63] that a similar expression, namely $$
\left(\frac{a_{2}}{2^{k_{1}}}-2 a_{2}^{2}\right) \cdot 1+\sum_{j=2}^{m}\left(\frac{a_{j+1}}{2^{k_{j}}}-2 a_{j+1}^{2}\right) \prod_{i=2}^{j}\left(\frac{2^{k_{i}}}{2^{k_{i-1}}}-1\right)
$$ where $a_i$ is subject to the same constraints is maximized if indeed we choose $a_{i+1}=1 / 2^{k_{i}+2}$ , for $i=1, \ldots, m$ , i.e., $k_{i}=k_{1}+2(i-1)$ . So. Any help with a solution from the scratch would be greatly appreciated or alternatively, a solution using the result of the second expression would be more than sufficient. Edit: The $k_i$ 's are determined by the $a_i$ 's (as it should show in the second line). Providing some more context $2^{-k_i}$ is the smallest number greater than or equal to $a_i$ or alternatively $a_i$ is the side length of a square and we round $a_i$ up to the closest power of 1/2.","['optimization', 'induction', 'analysis']"
4283700,Derivative of area integral w.r.t. parameter (Stokes' theorem?),"Let $$g\left(p\right) = \iint_{R\left(p\right)} f\left(x\right)dA,$$ where $f$ is some continuous function on $\mathbb{R}^d$ , and $R\left(p\right)$ is some subset of $\mathbb{R}^d$ parameterized by $p$ . To give some examples, we could have $R\left(p\right) = \left\{x:\|x\|\leq p\right\}$ , or perhaps an ellipse $\left\{x:x^\top B x\leq p\right\}$ , or a parameter that can be changed to translate the shape (like moving the unit sphere in some direction), etc., but the problem is not restricted to these examples; however, we can assume that $R$ changes ""smoothly"" with $p$ . Note that the function $f$ being integrated has no dependence on $p$ . The parameter only affects the set over which $f$ is being integrated. I would like to understand $\frac{dg}{dp}$ . It seems like this should be related to Stokes' theorem or similar results, because if I change $p$ to $p+\Delta p$ , most of the $x$ values in $R\left(p\right)$ will continue to be in the modified set; the change will only occur for $x$ around the boundary. So, the ""net"" change in $g$ should be some sort of integral over the boundary of $f$ combined with the ""direction"" in which the boundary is moving. But I am a bit lost as to how to formalize this -- the usual formulations of Stokes' theorem, Green's theorem etc. use a different setting. Would appreciate any pointers.","['area', 'contour-integration', 'stokes-theorem', 'derivatives', 'differential-forms']"
4283703,Existence of real analytic diffeomorphisms with prescribed values on a finite set,"My question is the following: given a finite set $ F = \{ x_1, \dots, x_k \} \subset \mathbb{R} $ such that $x_i < x_{i+1}$ for all $i = 1, \dots, k-1$ and numbers $a_1, \dots, a_k \in \mathbb{R}$ such that $ a_i < a_{i+1}$ for $i=1,\dots,k-1 $ , is it possible to find a real analytic diffeomorphism $f: \mathbb{R} \to \mathbb{R}$ such that $f(x_i) = a_i$ ? I know that it's easy to find real analytic functions satisfying these conditions (for instance, using polynomial interpolation), but is there a  way to get at least one that is a diffeomorphism (i.e. with $f'>0$ and surjective)? Thanks!","['analytic-functions', 'lagrange-interpolation', 'analysis', 'real-analysis']"
4283704,Did I solve this probability question (bag of coloured balls) on sampling with replacement using combinatorics correctly?,"Suppose a bag has $3$ red balls, $3$ green balls and $3$ blue balls. If $2$ balls are picked at random without replacement, what is the probability that they are the same colour? My solution is in $2$ distinct ways: For the first ball I have $9$ choices, while for second ball I have $8 $ choices. So $2$ balls can be picked in $9 \times 8 = 72$ ways, but since here the order of the result doesn't matter, the sample space only has $\frac{72}{2} = 36$ outcomes. Now let $A$ be the event ""I pick $2$ red balls"". This can be done in $3 \times 2$ ways but because order doesn't matter, in fact only $3\choose 2$ outcomes correspond to $A$ . Similarly, I have $3\choose 2$ outcomes for picking $2$ blue balls, and $3\choose 2$ outcomes for picking $2$ green balls. Therefore the required probability $P$ is $P = 3 \times \frac{3\choose 2}{36}  = \frac{1}{4}$ . My second argument is simply that: suppose the first ball is drawn, of any colour. Then the probability of picking a second ball the same colour as first must be $\frac{2}{8}$ , since no matter what colour was picked first, there are only $2$ balls remaining of that same colour, out of  a total of $8$ . Hence we again get the required probability $P = \frac{1}{4}$ . Are both of my solutions entirely correct?","['conditional-probability', 'solution-verification', 'combinatorics', 'probability-theory', 'probability']"
4283735,"Without superior math, can we evaluate this limit?","We all knew, with $$\lim_{x\to 0}\frac{\sin x - x}{x(1-\cos x)}$$ we can use L'Hôpital's rule or Taylor series to eliminate undefined form. But without all tools, by only using high school knowledge, how can we evaluate this limit? It seems difficult to transform numerator, any idea?
Thank you!","['limits', 'limits-without-lhopital']"
4283742,Determining solution of Riccati equations,"Given are a $\mathbb{C}$ -valued function $\phi$ , $\mathbb{C}^d$ -valued function $\psi$ , $\mathcal{X}\subseteq \mathbb{R}^d$ and the following differential equation, which holds for all $x \in \mathcal{X},\ T \geq 0,\ u \in \mathrm{i}\mathbb{R}^{d}$ : $$
\begin{aligned}
\partial_{T} \phi(T, u)+\partial_{T} \psi(T, u)^{\top} x&=\psi(T, u)^{\top} b(x)+\frac{1}{2} \psi(T, u)^{\top} a(x) \psi(T, u)\\
\psi(0,u)&=u\\
\phi(0,u)&=0
\end{aligned}
$$ Now, the author states the following: Since $\psi(0, u)=u$ , this implies that $a$ and $b$ are of the form $$
\begin{aligned}
&a(x)=A+\sum_{i=1}^{d} x_{i} \alpha_{i} \\
&b(x)=B+\sum_{i=1}^{d} x_{i} \beta_{i}
\end{aligned}
$$ for some $d \times d$ -matrices $A$ and $\alpha_{i}$ , and $d$ -vectors $B$ and $\beta_{i}$ . Plugging this back into the above equation yields the Riccati equations \begin{aligned}
\partial_{t} \phi(t, u) &=\frac{1}{2} \psi(t, u)^{\top} A\  \psi(t, u)+B^{\top} \psi(t, u) \\
\phi(0, u) &=0 \\
\partial_{t} \psi_{i}(t, u) &=\frac{1}{2} \psi(t, u)^{\top} \alpha_{i} \psi(t, u)+\beta_{i}^{\top} \psi(t, u), \quad 1 \leq i \leq d \\
\psi(0, u) &=u
\end{aligned} For $T=0$ , I find $$
\begin{aligned}
\partial_{T} \phi(0, u)+\partial_{T} \psi(0, u)^{\top} x&=u^{\top} b(x)+\frac{1}{2} u^{\top} a(x) u\\
\end{aligned}
$$ But how do I find the form of $a(x)$ and $b(x)$ from that? How do I find the Riccati equations? The equation $$
\begin{aligned}
\partial_{T} \phi(T, u)+\partial_{T} \psi(T, u)^{\top} x&=\psi(T, u)^{\top} b(x)+\frac{1}{2} \psi(T, u)^{\top} a(x) \psi(T, u)\\
\end{aligned}
$$ reminds me on some sort of Taylor expansion. Maybe this could be used? Thank you in advance!","['ordinary-differential-equations', 'real-analysis', 'linear-algebra', 'partial-differential-equations', 'partial-derivative']"
4283745,What is the explicit ${\rm GL}_n$-action on the Weyl algebra?,"This is a typical classical invariant theory setup.  Let $V = \mathbb C^n$ , and let $\mathbb C[V] \cong \mathbb C[x_1,\ldots,x_n]$ be the space of polynomial functions on $V$ .  The group $G={\rm GL}(n,\mathbb C)$ acts on $V$ in the obvious way, inducing the usual action on $\mathbb C[V]$ via $(g \cdot f)(v) = f(g^{-1}v)$ for $g \in G$ , $f \in \mathbb C[V]$ , and $v \in V$ .  Hence $G \subset {\rm End}(\mathbb C[V])$ . Now consider the Weyl algebra $\mathbb D(V)$ consisting of polynomial-coefficient differential operators on $\mathbb C[V]$ .  Clearly $\mathbb D(V) \subset {\rm End}(\mathbb C[V])$ as well, and in fact $G$ normalizes $\mathbb D(V)$ ; that is, we have a group action of $G$ on $\mathbb D(V)$ given by $$
g \cdot D = g \circ D \circ g^{-1} \in \mathbb D(V),
$$ for $g \in G$ and $D \in \mathbb D(V)$ .  Given the action above, I have seen several sources state without proof that the $G$ -action on the span of the differential operators $\partial_i = \frac{\partial}{\partial x_i} \in \mathbb D(V)$ is equivalent to the defining representation $V$ , whereas the $G$ -action on the span of the multiplication operators $x_i \in \mathbb D(V)$ is equivalent to the dual $V^*$ . This has been driving me insane; below is my attempt to convince myself of this fact, using a concrete example.  Is this the correct way to think about the actions here?  Or, is there a more obvious way to see it? Example. Let $n=2$ , so that $\mathbb C[V] = \mathbb C[x,y]$ , and $\mathbb D(V)$ is generated by the set $\{x,y,\partial_x, \partial_y\}$ .  Consider $g=\left[ \begin{smallmatrix} 3 & 5\\1 & 2 \end{smallmatrix}\right] \in G$ , so that $g^{-1} = \left[ \begin{smallmatrix} \phantom{-}2 & -5\\-1 & \phantom{-}3 \end{smallmatrix}\right]$ .  If my interpretation above is correct, then we should have the following: \begin{align*}
g \cdot \partial_x &= 3 \partial_x + \partial_y\\
g \cdot \partial_y &= 5 \partial_x + 2 \partial_y\\
g \cdot x &= 2x - 5y\\
g \cdot y &= -x + 3y.
\end{align*} (Note that I have $G$ acting via inverse transpose on $x$ and $y$ , since those are suppposed to play the role of the basis vectors of $V^*$ .)  Here is my attempted justification for the first equation above, using $u$ and $v$ to substitute in the third line: \begin{align*}
(g\cdot \partial_x) \cdot f(x,y) &= (g \circ \partial_x \circ g^{-1})\cdot f(x,y)\\
&= (g \circ \partial_x) \cdot f(3x+5y, \: x+2y)\\
&= (g \circ \partial_x) \cdot f(u,v)\\
&= g \cdot f_x(u,v)\\
&= g \cdot (f_u u_x + f_v v_x) & \text{(chain rule)}\\
&= g \cdot (3 f_u(u,v) + f_v(u,v))\\
&= 3 g \cdot f_u(u,v) + g \cdot f_v(u,v)\\
&= 3 f_x(x,y) + f_y(x,y)\\
&= (3 \partial_x + \partial_y)\cdot f(x,y).
\end{align*} This is the ""correct"" anticipated result, but am I applying all the transformations in the correct order? Likewise, for the third equation above (the action of $g$ on $x$ ), I have this so far: \begin{align*}
(g \cdot x) \cdot f(x,y) &= (g \circ x \circ g^{-1}) \cdot f(x,y)\\
&= (g \circ x) \cdot f (u,v) \\
&= g \cdot x f(u,v).
\end{align*} This is where I'm not sure how to finish rigorously; one one hand, $g$ should transform $f(u,v)$ back into $f(x,y)$ , and on the other hand, since $g^{-1} \cdot \left[\begin{smallmatrix} x \\ y \end{smallmatrix}\right] = \left[\begin{smallmatrix} 2x - 5y \\ -x + 3y \end{smallmatrix}\right]$ , I suppose that the $g$ -action here transforms the ""extra"" $x$ into $2x-5y$ ?  So the final result is to multiply the original $f(x,y)$ by $2x-5y$ , just as anticipated? Is there a better way to understand the $G$ -action here?","['differential-operators', 'invariant-theory', 'group-theory', 'group-actions', 'lie-groups']"
4283778,Given product and convolution of pair of functions can you find original pair?,"Suppose you have two functions $f,g: \mathbb{R} \rightarrow \mathbb{C}$ and you have their product and convolution $$ h_1(t) = fg$$ $$ h_2(t) = \int_{-\infty}^{\infty} f(t-\tau)g(\tau) d\tau $$ Is it possible to express $f,g$ solely in terms of $h_1$ , $h_2$ ? I have reason to believe no, but it should be possible up to a constant factor that is on the unit circle. I would like to ask: how to go about doing so? Why I think it is possible: I believe if you know the entire momentum probability distribution $|\phi(p,t)|^2$ and position probability distribution $|\psi(x,t)|^2$ of an object you should be able to recover the wave function of the object up to a multiplicative factor of a root of unity. If we define the physicists ""fourier transform"" as $$ \mathcal{F}_x[f] = \frac{1}{\sqrt{2\pi\hbar}} \int_{-\infty}^{\infty}f(x) e^{\frac{ipx}{\hbar}} dx$$ Then we can state the famous momentum position relationship as $$ \psi(x,t) = F_x[\phi(p,t)]$$ $$ \phi(p,t) = F_p[\psi(x,t)]$$ It then follows that: $$ \sqrt{2\pi\hbar} \mathcal{F}_x^{-1} [|\phi(p,t)|^2] = \psi(x,t) \star \psi^*(x,t)$$ $$ |\psi(x,t)|^2 = \psi(x,t)\psi^*(x,t) $$ Where $\star$ indicates convolution.  I'm taking a bet that it should be possible to solve for $\psi, \psi^*$ here up to a multiplicative root of unity, although I am not clear how to do so.","['fourier-analysis', 'probability-distributions', 'convolution', 'complex-analysis', 'quantum-mechanics']"
4283796,Smallest $\mathbb R^n$ into which $SO(3)$ can be topologically embedded?,"I think there are theorems in differential geometry guaranteeing that $SO(3)$ can be embedded into $\mathbb R^n$ for some $n$ . But do we know what particular $n$ this is for $SO(3)$ , and do we know of a particular embedding? This would be nice to know if $n$ happens to be small enough for us to ""think about"" the global topology of $SO(3)$ in a somewhat geometrical way.","['topological-groups', 'manifolds', 'differential-topology', 'lie-groups', 'differential-geometry']"
4283812,What exactly am I doing wrong in this problem?,"There are two local factories that produce microwaves. Each microwave produced at factory A is defective with probability $.05$ , whereas each one produced at factory B is defective with probability $.01$ . Suppose you purchase two microwaves that were produced at the same factory, which is equally likely to have been either factory A or factory B. If the first microwave that you check is defective, what is the conditional probability that the other one is also defective? Ok so below is my attempts to this problem. Obviously what I got is wrong, but I'm confused on what I did that was wrong. I'm trying to figure out what is wrong with my understanding.
Note: I didn't know how to include fancy symbols like I see on other post so I just installed a picture instead. My work:",['statistics']
4283861,"$f(0,p)$ for $f(x,p) = \int \frac{\sin(2x)}{x} C_i(x+p) dx$","How do i find $f(0,p)$ for $$f(x,p) = \int \frac{\sin(2x)}{x} C_i(x+p) dx$$ $C_i(x)$ is Cosine Integral . Obviously, if I could solve the integral, I would have substituted $x$ with $0$ . But I don't know how to solve it. I wonder if there is a general technique for finding the value of this function at a particular point, even if I can't solve it generally for all values of $x$ . Ignore the constant of integration (the usual +C term for indefinite integrals)",['integration']
4283936,Probable error in the proof of the remainder theorem in my grade 10 book,"The following is given in my book as a proof of the remainder theorem: If $p(x)$ is the dividened, $q(x)$ is the quotient, $r(x)$ is the remainder and $(x-a)$ is the divisor, then: $$p(x) = q(x)(x-a) + r(x)$$ If $x=a$ , $$\implies p(a) = q(a)(0) + r(a)$$ Hence, $$r(a) = p(a) \tag{i} $$ Therefore, $$ p(x) = q(x)(x-a) + p(a) \tag{ii}$$ Hence, we can say $p(a)$ is the remainder. Isn't it wrong? In the last equation $(ii)$ , I assume they have tried to substitute the value of $r(a)$ from equation $(i)$ , but that makes equation $(ii)$ before the substitution $$p(x) = q(x)(x-a) + r(a)$$ But that is wrong right? The correct equation should be $$p(x) = q(x)(x-a) + r(x)$$ and I don't think $r(x)\neq(a)$ . I think the problem can be avoided easily by not using a function of $x$ in the first place and just using a variable like $c$ (suppose), where $p(a)$ will be proved equal to $c$ , and hence the theorem will also be satisfied and proved. I am still in grade $10$ , hence I might be wrong in some aspects, please go easy on me :)","['proof-explanation', 'algebra-precalculus', 'proof-writing', 'solution-verification']"
4284060,Showing Differentiability/Continuity at endpoints of closed interval?,"I am given the function $\gamma:[-1,\frac{\pi}{2}] \rightarrow \mathbb{C}$ $\gamma(t) = 
\begin{cases}
 t+1 & \text{for $-1 \leq t \leq0$} \\
 e^{it} & \text{for  $0 \leq t \leq\frac{\pi}{2}$} \\
\end{cases}$ And I want to show this function is continuous and piecewise continuously differentiable. To show the function is continuous at the endpoints $-1$ and $\frac{\pi}{2}$ of the interval $[-1,\frac{\pi}{2}]$ , is it sufficient to show that $\lim_{t\rightarrow-1^{+}} \gamma(t) = \gamma(-1)$ and $\lim_{t\rightarrow\frac{\pi}{2}^{-}} \gamma(t) = \gamma(\frac{\pi}{2})$ ? Then, to show the function is piecewise differentiable, would I need to show that $\gamma_{|[-1,0]}$ is differentiable at all interior points of the interval $[-1,0]$ , left differentiable at $0$ and right differentiable at $-1$ ? (And then do the equivalent for $\gamma_{|[0,\frac{\pi}{2}]}$ ) To show continuity at the endpoints of the interval for this derivative, would I need to show that $\lim_{t\rightarrow-1^{+}} \gamma_{|[-1,0]}'(t) = \gamma_{|[-1,0]}'(-1^{+})$ and $\lim_{t\rightarrow0^{-}} \gamma_{|[-1,0]}'(t) = \gamma_{|[-1,0]}'(0^{-})$ ? (And then do the equivalent for $\gamma_{|[0,\frac{\pi}{2}]}$ )","['complex-analysis', 'continuity', 'limits', 'derivatives', 'piecewise-continuity']"
4284067,Infinite convolution of a smooth compactly supported function converges uniformly,"$\newcommand{\R}{\mathbb{R}}$$\newcommand{\diff}{\mathrm{d}}$ Let $\rho\in C^\infty_c(\mathbb R^d;\mathbb R)$ be an even function, i.e. $\rho(-x)=\rho(x)$ for all $x\in\mathbb R^d$ that furthermore satisfies $\operatorname{supp}\rho\subset B(0,1)$ and $$\int_{\mathbb R^d}\rho(x)\,\mathrm{d} x=1.$$ The function is not necessarily non-negative (which then implies $\int |\rho(x)|\, \diff x\geq 1$ ). We define $$\rho^{(n)}(x)=2^{2d}\rho(2^nx).$$ Basically $\rho^{(n)}$ is somehow an approximation to the identity. We also define $$\eta^{(n)}=\rho^{(0)}*\rho^{(1)}*\rho^{(2)}*\cdots * \rho^{(n)},$$ where $*$ is the convolution operator. Problem. Show that $\eta^{(n)}$ converges in $C(\mathbb R^d)$ uniformly. Attempt. The book (see Motivation section below) gives the hint to prove the limit exists in Fourier space. I imagine that they mean that I have to prove $\mathcal F\eta^{(n)}\to \hat\eta$ in the Schwartz space $\mathscr S(\mathbb R^d)$ and use continuity of the Fourier transform (-/inversion) from $\mathscr S(\mathbb R^d)$ to $\mathscr S(\mathbb R^d)$ .  So I did that and in particular I wanted to show that $\eta^{(n)}$ is Cauchy. So let $m\geq n$ and look at $$(\mathcal F(\eta^{(m)}-\eta^{(n)}))(\xi)=\prod_{i=0}^m (\mathcal F\rho^{(i)})(\xi)-\prod_{i=0}^n (\mathcal F\rho^{(i)})(\xi)=\prod_{i=0}^n (\mathcal F\rho^{(i)})(\xi)\left(\left[\prod_{i=n+1}^m (\mathcal F\rho^{(i)})(\xi)\right]-1 \right).$$ It is not difficult to see that $$(\mathcal F\rho^{(i)})(\xi)=(\mathcal F\rho)(2^{-i}\xi).$$ So in particular as $i\to\infty$ we easily see that $$\mathcal F\rho^{(i)}\to \mathcal F\rho(0)=\int\rho(x)\,dx= 1$$ in $\mathcal S'(\mathbb R^d)$ , i.e. convergence as in tempered distribution. After this I was wishing the product written above also would converge to $1$ , so I wrote $$(\mathcal F(\eta^{(m)}-\eta^{(n)}))(\xi)=\prod_{i=0}^n(\mathcal F\rho)(2^{-i}\xi)\left( \left[\prod_{i=n+1}^m(\mathcal F\rho)(2^{-i}\xi)\right]-1\right).$$ I felt that this is just as stiff, because I do not know the rate for which the convergence happens. I was guessing through Taylor approximation $$(\mathcal F\rho^{(i)})(\xi)-1=(\mathcal F\rho)(2^{-i}\xi)-1\approx \xi 2^{-i},$$ but then $\xi$ is a polynomial so it does not belong in $\mathscr S(\mathbb R^d)$ . After this I tried showing it directly, something like $$|\eta^{(n)}(x)-\eta^{(n+1)}(x)|\leq C 2^{-n}$$ uniformly, so that it is Cauchy in $C(\mathbb R^d)$ . One thing one easily sees is that $\rho^{(n)}$ has support inside $B(0,2^{-n})$ and therefore $\eta^{(n)}$ has support inside $$B(0,1)+B(0,2^{-1})+B(0,2^{-2})+\cdots + B(0,2^{-n})\subset B(0,3) $$ for all $n\in\mathbb N$ (by the support property of the convolution). In this case it is actually enough to show uniform convergence in say $C(\overline{B(0,3};\mathbb R)$ . I could not show that either. I appreciate any hint, or any methods to attack this problem. Motivation. I am reading the book ""A course on Rough Paths"" by Friz and Hairer and I am struggeling in proving the previous result (Exercise 13.7, second edition) which is needed for the proof of the so-called reconstruction theorem (Theorem 13.26). This question is somehow independent of the context, because it serves as a useful tool from basic analysis. Edit (Added attempt). Let us use the notation $$\rho^{(n,m)}=\rho^{(n)}*\rho^{(n+1)}*\cdots *\rho^{(m)}.$$ Basically $\eta^{(n)}=\rho^{(0,n)}$ . This notation is somehow easier for the things we will do soon. We want to show that $(\eta^{(n)})_{n\in\mathbb N}$ is Cauchy in $C_b(\R^d)$ . To that end we want to control $$\eta^{(n+1)}-\eta^{(n)}=\rho^{(0,n+1)}-\rho^{(0,n)}=\rho^{(0,n)}*(\rho^{(n+1)}-\delta_0)=\rho^{(1,n)}*(\rho^{(n+1)}-\delta_0)*\rho.$$ Let us focus on the term coming after $\rho^{(1,n)}$ , we have $$((\rho^{(n+1)}-\delta_0)*\rho)(x)=\int \rho^{(n+1)}(y)(\rho(x-y)-\rho(x))\,\diff y.$$ We have by the Mean value theorem $$|\rho(x-y)-\rho(x)|\leq \sup_{z\in \mathbb R^d} |D\rho(z)||y|$$ so that uniformly over $y\in\R^d$ $$|\rho(x-y)-\rho(x)|\leq C|y|.$$ Hence $$|((\rho^{(n+1)}-\delta_0)*\rho)(x)|\leq C\int_{\R^d} |\rho^{(n+1)}(y)||y|\,\diff y=C2^{-n-1}\int_{\R^d} 2^{(n+1)d}|\rho(2^{(n+1)}y)||2^{(n+1)}y|\,\diff y. $$ We change variables to get $$|((\rho^{(n+1)}-\delta)*\rho)(x)|\leq C2^{-n-1}\int_{\R^d} |\rho(y)||y|\,\diff y \leq \tilde C 2^{-n}.$$ Let us back to what we had $$(\eta^{(n+1)}-\eta^{(n)})(z)=\int_{\R^d}\rho^{(1,n)}(z-x) \int_{\R^d} \rho^{(n+1)}(y)(\rho(x-y)-\rho(x))\,\diff y\,\diff x.$$ Bounding all this $$|(\eta^{(n+1)}-\eta^{(n)})(z)|\leq \tilde C2^{-n} \int_{\R^d}|\rho^{(1,n)}(x)|\,\diff x.$$ Life would be much easier if we could bound the integral on the RHS. It is not difficult to see (through induction) that $$\int_{\R^d}|\rho^{(1,n)}(x)|\,\diff x=\int_{\R^d}|\rho^{(0,n-1)}(x)|\,\diff x=\int_{\R^d}|\eta^{(n)}(x)|\,\diff x.$$ So it is enough to show that $\eta^{(n)}$ is bounded in $L^1(\R^d)$ . I could not show that... According to the exercise it actually says that the integral is actually bounded. It is also enough to show that the Fourier transform $\mathcal F\eta^{(n)}$ is bounded in $L^1(\R^d)$ . So here is where I am stuck now. Note. that $\rho$ is not necessarily non-negative, otherwise it was easy. Edit 2. Since every little piece helps, let me add this. Define $$A_n:=\int_{\R^d}|\rho^{(0,n)}(x)|\,\diff x=\int_{\R^d}|\eta^{(n)}(x)|\,\diff x.$$ Recall we ended the previous edit with the observation that it is enough to show $\sup_n A_n<\infty$ . It is not difficult to find a recurrence relation for $A_n$ , namely $$A_{n+1}\leq \int_{\R^d}|\eta^{(n+1)}(x)-\eta^{(n)}(x)|\,\diff x+\int_{\R^d}|\eta^{(n)}(x)|\,\diff x\leq  C' 2^{-n} A_{n-1}+A_n,  $$ where $C'$ is some constant depending on $\tilde C$ above and the volume of $B(0,3)$ . This recurrence is promising except that it doesn't lead to anywhere if we bound $A_n\leq A_0A_{n-1}$ through Young's inequality because $A_0=\int |\rho(x)|\,\diff x\geq 1$ . Remark. On the other hand if $\rho(x)\geq 0$ then necessarily $A_0=1$ and the above argument works. This is why the case $\rho(x)\geq 0$ is easy.","['stochastic-pde', 'fourier-analysis', 'approximation-theory', 'functional-analysis', 'partial-differential-equations']"
4284133,"$x(t)= a\cos(t)$ , $y(t)= b\sin(t)$ in terms of the arc length $S$","I'm trying to parameterize the ellipse $x(t)= a\cos(t)$ , $y(t)= b\sin(t)$ in terms of the arc length $S$ but I don't know how to do it. Supposing that $\gamma:[a,b]\to \mathbb{R}$ is a smooth curve  with $\gamma'(t)\neq 0$ for $t\in [a,b]$ , I know that $s(t)$ = $\int_{a}^{t}\left\|  \gamma'(\psi)\right\|d\psi$ for $t\in [a,b]$ then I find the inverse funtion of $s$ . Can anybody help me find a way to express the parameterization for the ellipse? I’m looking for a solution in terms of sine amplitude and cosine amplitude.","['multivariable-calculus', 'differential-geometry', 'parametrization', 'real-analysis']"
4284143,About a claim in limits,"Let $a_n\geq 0$ . If $\displaystyle \lim_{n\to +\infty} a_n=L$ then $\displaystyle\lim_{n\to +\infty} \sqrt[k]{a_n}=\sqrt[k]{L}$ , $(k\in N)$ I am trying to show this using that: $a^k-b^k=(a-b)(a^{k-1}+a^{k-2}b+...+b^{k-1})$ .","['limits', 'calculus', 'proof-writing']"
4284151,General solution of $\dot{x} = (t+x) / (t-x)$,"I have the following question: I need to find the general solution of $\dot{x} = (t+x) / (t-x)$ . I tried to use the substitution $x = y \cdot t$ to get: \begin{align*}
\dot{(y \cdot t)} &= \dot{y}t + y = \frac{t + ty}{t - ty} = \frac{1 + y}{1 - y} \\
\dot{y}t &= \frac{1 + y}{1 - y} - y \\
\dot{y}t &= \frac{1 + y - y + y^2}{1 - y} \\
\dot{y} &= \frac{1 + y^2}{(1 - y)t} \\
\int_{y_0}^y \frac{1-\xi}{1 + \xi^2}d\xi &= \int_{t_0}^t 1/\eta \; d \eta \\\end{align*} $$
\arctan(y) - \arctan(y_0) - \frac{\ln(1+ y^2)}{2} + \frac{\ln(1 + y_0^2)}{2} = \ln |t| - \ln |t_0|
$$ However, I have no idea how to solve this equation for $y$ . There is probably a much better way to do it than what I did, could someone please help me? Thanks!","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4284195,distance between sorted arrays,"Assume we have two arrays of real numbers: $$
X = \{x_{1}, x_{2}, \dots, x_{n} \}
$$ and $$
Y = \{y_{1}, y_{2}, \dots, y_{n} \}
$$ Next, assume that $d = \max(|x_{i} - y_{i}|)$ . Next let us sort both arrays in increasing order: $$
X'=\{x_{1}', x_{2}', \dots, x_{n}' \} = sort(X)
$$ and $$
Y'=\{y_{1}', y_{2}', \dots, y_{n}' \} = sort(Y)
$$ Then, let $d' = \max(|x_{i}' - y_{i}'|)$ . Is it always $d' \leq d$ ?","['sorting', 'geometry', 'rearrangement-inequality', 'combinatorics', 'sequences-and-series']"
4284210,Prove a limit by definition given another limit,"I'm trying to prove the following statement: If $\lim_{x\rightarrow a}{f(x)\over x-a}=L$ , for some $L\neq \pm\infty$ , then $\lim_{x\rightarrow a}{f(x)}=0$ My proof goes as follows: Since $\lim_{x\rightarrow a}{f(x)\over x-a}=L$ , by the definition: Given $\varepsilon>0, ~\exists ~\delta>0 ~||x-a|<\delta \implies \left|{f(x)\over x-a}-L\right|<\varepsilon $ Then: $\left|{f(x)\over x-a}-L\right|<\varepsilon$ $\left|{f(x)}-L(x-a)\right|<\varepsilon|x-a|$ $\left|{f(x)}-L(x-a)\right|<\varepsilon|x-a|<\varepsilon\delta$ $\left|{f(x)}-L(x-a)\right|<\varepsilon\delta$ By using the triangle inequality $|u|-|v|\leq |u-v|$ : $|{f(x)}|-|L(x-a)|\leq\left|{f(x)}-L(x-a)\right|<\varepsilon\delta$ $|{f(x)}|-|L(x-a)|<\varepsilon\delta$ $|{f(x)}|<\varepsilon\delta+|L(x-a)|$ $|{f(x)}|<\varepsilon\delta+|L(x-a)|<\varepsilon\delta+|L|\delta$ $|{f(x)}|<\varepsilon\delta+|L|\delta$ $|{f(x)}|<(\varepsilon+|L|)\delta$ Then, given $\varepsilon'>0$ , choose $\delta ={\varepsilon'\over\varepsilon+|L|}$ , we have: $|{f(x)}|<\varepsilon'$ Therefore, since $|x-a|<\delta\implies |f(x)-0|<\varepsilon'$ , then $\lim_{x\rightarrow a}{f(x)}=0$","['limits', 'calculus']"
4284218,Construct a collection of pairwise disjoint measurable sets from a set of rectangles,"In proving the Fubini's theorem, I've come across below statement for which I'm not sure if it's indeed correct. Could you please confirm the validity of this statement and if my proof is correct? Let $(X, \mathcal A)$ and $(Y, \mathcal B)$ be measure spaces. $[n] \triangleq \{1, \ldots, n\}$ and $A \times B \in \mathcal A \times \mathcal B$ . $\{X_k \times Y_k\}_{k=1}^n$ a finite collection of sets in $\mathcal A \times \mathcal B$ . $(X_k \times Y_k) \cap (X_h \times Y_h) = \emptyset$ , either $X_k = X_h$ or $X_k \cap X_h = \emptyset$ , and either $Y_k = Y_h$ or $Y_k \cap Y_h = \emptyset$ for all $k \neq h$ . Then there is a finite collection $\Delta = \{A_k \times B_k\}_{k=1}^m$ of sets in $\mathcal A \times \mathcal B$ such that $\{X_k \times Y_k\}_{k=1}^n \subseteq \Delta$ . $\bigcup \Delta = (A \times B) \bigcup \cup_{k=1}^n X_k \times Y_k$ . $(A_k \times B_k) \cap (A_h \times B_h) = \emptyset$ , either $A_k = A_h$ or $A_k \cap A_h = \emptyset$ , and either $B_k = B_h$ or $B_k \cap B_h = \emptyset$ for all $k \neq h$ . Proof: We define $A_k \triangleq A \cap X_k$ and $B_k \triangleq B \cap Y_k$ for $k \in [n]$ . Also, $A' \triangleq A \setminus \cup_{k=1}^n A_k$ and $B' \triangleq B \setminus \cup_{k=1}^n B_k$ . Then $A_k, A' \in \mathcal A$ and $B_k, B' \in \mathcal B$ for $k \in [n]$ . Moreover, $$A \times B = (A' \cup A_1 \cup \cdots \cup A_n) \times (B' \cup B_1 \cup \cdots \cup B_n).$$ Clearly, $(A_i \times B_j) \cap (X_k \times Y_k) \neq \emptyset$ if and only if $A_i \cap X_k \neq \emptyset$ and $B_j \cap Y_k \neq \emptyset$ if and only if $A_i \subseteq X_k$ and $B_j \subseteq Y_k$ . Then it's either $(A_i \times B_j) \cap (X_k \times Y_k) = \emptyset$ or $(A_i \times B_j) \subseteq (X_k \times Y_k)$ for all $i,j,k \in [n]$ . Let $$I \triangleq \{(i, j) \in [n]^2 \mid \forall k \in [n], (A_i \times B_j) \cap (X_k \times Y_k) = \emptyset\}$$ and $$\Delta \triangleq \{X_k \times Y_k, A' \times B_k, A_k \times B'\}_{k=1}^n \cup \{A' \times B'\} \cup \{A_i \times B_j\}_{(i, j) \in I}.$$ Then $\Delta$ satisfies the required conditions. Update: After looking closer to the problem, I found that the ""theorem"" can be simplified. The price to pay for this simplification is that we no longer have $\{X_k \times Y_k\}_{k=1}^n \subseteq \Delta$ . Let $(X, \mathcal A)$ and $(Y, \mathcal B)$ be measure spaces. $[n] \triangleq \{1, \ldots, n\}$ and $A \times B \in \mathcal A \times \mathcal B$ . $\{X_k \times Y_k\}_{k=1}^n$ a finite collection of sets in $\mathcal A \times \mathcal B$ such that $X_k \cap X_h = \emptyset$ for all $k \neq h$ . Then there is a finite collection $\Delta = \{A_k \times B_k\}_{k=1}^m$ of sets in $\mathcal A \times \mathcal B$ such that $\bigcup \Delta = (A \times B) \bigcup \cup_{k=1}^n X_k \times Y_k$ . $A_k \cap A_h = \emptyset$ for all $k \neq h$ . Update 2: I apply above theorem to build a countable partition of a measurable set from one of its countable cover containing measurable subsets. Let $\Delta = \{X_k \times Y_k\}_k$ be a sequence in $\mathcal A \times \mathcal B$ . We define a sequence $(\Lambda_k)$ of collections with $\Lambda_k \subseteq \mathcal A \times \mathcal B$ as follows. First, $\Lambda_1 = \{X_1 \times Y_1\}$ . We construct $\Lambda_{k+1}$ by applying above theorem to the finite collection $\Lambda_k$ and an element $X_{k+1} \times Y_{k+1}$ . It follows that $\Lambda_{k} \subseteq \Lambda_{k+1}$ and that $\Lambda_k$ contains only pairwise disjoints sets. Let $\Lambda = \bigcup_k \Lambda_k$ . Then $\bigcup \Lambda = \bigcup \Delta$ and that $\Lambda$ contains only pairwise disjoint sets. We notice that $\Lambda_1$ contains only $1$ element and thus satisfies the condition of the theorem. Hence the recursive construction by the theorem is valid.","['elementary-set-theory', 'measure-theory', 'solution-verification', 'fubini-tonelli-theorems']"
