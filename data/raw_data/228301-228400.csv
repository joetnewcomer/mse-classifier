question_id,title,body,tags
4724060,Binomial identity reference request,"Math Overflow answer https://mathoverflow.net/a/297916/113033 references the binomial identity \begin{equation}
    \sum_{t} \binom{r}{t} \frac{(-1)^t}{r+t+1} \binom{r+t+1}{j}
    =\begin{cases}
         \frac{1}{(2r+1) \binom{2r}r}, & \text{if } j=0;\\
         \frac{(-1)^r}{j} \binom{r}{2r-j+1}, & \text{if } j>0.
    \end{cases}
\end{equation} However, I did not succeed in search of literature reference of it. Is there any literature mention of it or proof?","['binomial-coefficients', 'combinatorics', 'reference-works', 'reference-request']"
4724067,"How can I compute every possible tuple of lists from the elements V, V, V, M, M, M?","I have two lists, and six elements: V V V M M M I would like to compute every possible combination of those elements into two lists (left and right): So, five possible solutions could be: ([V, V, V], [M, M, M])
([], [V, V, V, M, M, M])
([V, V, V, M, M, M], [])
([V, M, V], [V, M, M])
([V], [V, M, V, M, M]) My first approach was reducing the problem to that which had less elements (two). So, all possible solutions in that case are: ([], [V, M])
([], [M, V])
([V, M], [])
([M, V], [])
([V], [M])
([M], [V]) I can't seem to fit this problem into a basic standard combinatorics one, but perhaps there is something I am missing. How can I approach this problem ?","['combinatorics', 'problem-solving']"
4724080,Coefficients of Chebyshev polynomials,"Not long ago, I derived the formula for Chebyshev polynomials $$T_{n}\left( x\right)= \sum_{k=0}^{\lfloor  \frac{n}{2} \rfloor}{n \choose 2k}x^{n-2k}\left( x^2-1\right)^{k}$$ How to extract the coefficients of this polynomial of degree $n$ ? I tried using Newton's binomial but got a double sum $$\sum_{k=0}^{\lfloor  \frac{n}{2} \rfloor}{n \choose 2k}x^{n-2k}\left(  \sum_{m=0}^{k} {k \choose m} x^{2m}\left( -1\right)^{k-m}  \right) \\
\sum_{k=0}^{\lfloor  \frac{n}{2} \rfloor}\sum_{m=0}^{k}\left( -1\right)^{k-m} {n \choose 2k}  \cdot {k \choose m} x^{n+2m-2k}\\
 $$ Now how to continue counting this sum ?
What would it look like to change the order of summation and would it do anything ? What else did I try ? Well, I worked out the sum $$\sum_{k=0}^{\lfloor  \frac{n}{2} \rfloor}{n \choose 2k}x^{n-2k}\left( x^2-1\right)^{k} $$ for $n=8$ and I hypothesized that $$T_{n}\left( x\right)= \sum_{k=0}^{\lfloor \frac{n}{2} \rfloor}  \sum_{m=k}^{\lfloor \frac{n}{2} \rfloor}\left( -1\right)^{k}{n \choose 2m}  \cdot {m \choose k} x^{n-2k} $$ However, it would be useful to demonstrate the correctness of this hypothesis and count the sum of $$\sum_{m=k}^{\lfloor \frac{n}{2} \rfloor}{n \choose 2m}  \cdot {m \choose k}$$ Here I would like to point out that Wolfram Alpha counts this sum incorrectly $$\sum_{m=k}^{\lfloor \frac{n}{2}\rfloor}{{n \choose 2m} \cdot  {m \choose k}}  = \frac{n}{2n-2k}  \cdot 2^{n-2k} \cdot {n - k \choose k}$$ And it would even be a nice result but
first of all it is not quite correct ( Have you noticed why ?)
and secondly it comes from a hypothesis I made after dissecting the formula for $n=8$ . It seems to me that this hypothesis of mine would be enough to prove by induction after n but how would it look? How to calculate the coefficients of the Chebyshev polynomial ?
Are you sure they will be the sum of products of Bernoulli numbers and Newton symbol
since, after working out the formula, I came out that it would be $$T_{n}\left( x\right)= \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}  \sum_{m=k}^{\lfloor\frac{n}{2}\rfloor}\left( -1\right)^{k} {n \choose 2m} \cdot  {m \choose k}  x^{n-2k}$$ Please help.","['elementary-functions', 'functions', 'chebyshev-polynomials']"
4724091,$L^2$ Correspondence Between Dirichlet and Neumann Domains,"Let $\Omega\subset \mathbb R^k$ be compact with smooth boundary, $T$ denote the trace operator to its boundary (i.e. a continuous extension of the restriction operator), and define the following subspaces of $L^2(\Omega)$ : $$\mathcal{D}_D=\{u \in H^2 \textrm{ such that }Tu=0\}\quad \textrm{and}\quad \mathcal{D}_N=\{u \in H^2 \textrm{ such that }T(\partial_nu)=0\},$$ the Dirichlet and Neumann domains, respectively. Does there exist a continuous bijection from $L^2$ to itself which restricts to a bijection from $\mathcal{D}_D$ to $\mathcal{D}_N$ ? Some comments: I know that such a map exists if, instead of the Neumann domain, we consider functions which satisfy some APS boundary condition, see for example section 3 of [BL] . However, the Neumann boundary condition requires information in a neighborhood of the boundary, so the methods presented in that paper don’t work. I can’t prove that no other way works though, but this example shows that we need to look at information which is more refined than simply considering the functional analysis perspective.","['laplacian', 'functional-analysis', 'partial-differential-equations']"
4724108,Does $\sqrt a + \sqrt b$ have a four way conjugate?,"Let $a, b$ be rational numbers that are not perfect squares.  Consider the set $S = \{\sqrt a + \sqrt b, \sqrt a - \sqrt b, - \sqrt a + \sqrt b, -\sqrt a - \sqrt b\}$ . If $p$ is a polynomial with rational coefficients, is it correct that if any element of $S$ is a root of $p$ , then every element of $S$ is? That is implied by the excellent post defining conjugates , and I want to confirm my understanding.","['field-theory', 'abstract-algebra', 'irreducible-polynomials', 'polynomials']"
4724139,"Supremum of the function $f(x,y,z,u)=\frac{x(1-x)y(1-y)z(1-u)}{1-(1-xy)z}$","Consider a function $$f(x,y,z,u)=\frac{x(1-x)y(1-y)z(1-u)}{1-(1-xy)z} $$ where $x,y,z,u\in (0,1)$ I need the supremum of $f(x,y,z,u)$ for $x,y,z,u\in (0,1)$ We have $1-(1-xy)z=1-z+xyz$ Now since $x,y,z\in(0,1)$ , so we have $1-z>0$ and by AM-GM inequality $$1-(1-xy)z=1-z+xyz\geq 2\sqrt{(1-z)xyz}$$ So we have $$f(x,y,z,u)\leq \frac{1}{2} \frac{\sqrt{x}(1-x)\sqrt{y}(1-y)z(1-u)}{\sqrt{z(1-z)}} $$ Now if $G(x)=\sqrt{x}(1-x)$ and we put $t=\sqrt{x}$ then the supremum of $G(x)=t(1-t^2)$ occurs when $1-3t^3=0$ so at $t=\frac{1}{\sqrt{3}}$ so we have $$f(x,y,z,u)<\frac{1}{2}(\frac{\frac{1}{\sqrt{3}}(1-\frac{1}{3})\frac{1}{\sqrt{3}}(1-\frac{1}{3})z(1-u)}{\sqrt{z(1-z)}}) $$ So $$f(x,y,z,u)<\frac{2}{27}\frac{z(1-u)}{\sqrt{z(1-z)}}$$ Please solve this question.","['real-analysis', 'multivariable-calculus', 'calculus', 'functions', 'supremum-and-infimum']"
4724157,Expected number of connected components on a line,"This problem is from a quant interview. Consider a line of $n$ adjacent colorless squares. Color in each individual square black with probability 3/4 or white with probability 1/4, independent of all other squares. A connected component is a maximal sequence of adjacent squares all with the same color. For example, for $n=10$ BBWBWWWBBW has 6 connected components. Find the expected number of connected components in our line. Here's my attempt. Let $X_1,\ldots,X_{n+1}$ be iid random variables that model the colors of each square. Let $S_n$ be the random variable that counts the number of connected components. I'm trying to find a recurrence relation on $E[S_n]$ . $\begin{align}
E[S_{n+1}] 
&= E[S_{n+1} 1_{X_{n+1}= X_{n}}] + E[S_{n+1} 1_{X_{n+1}\neq X_{n}}]
\\
&= E[S_{n} 1_{X_{n+1}= X_{n}}] + E[(1+S_{n}) 1_{X_{n+1}\neq X_{n}}]
\\
&= E[E[S_{n} 1_{X_{n+1}= X_{n}}|X_1,\ldots,X_n]] + E[E[(1+S_{n}) 1_{X_{n+1}\neq X_{n}}|X_1,\ldots,X_n]]
\\
&= E[S_{n}\big(\frac 34 1_{X_n=B} + \frac 14 1_{X_n=W}\big)] + E[(1+S_{n}) \big(1-\frac 34 1_{X_n=B} - \frac 14 1_{X_n=W}\big)].
\end{align}$ I cannot proceed further because $S_n$ and $X_n$ are not independent...","['expected-value', 'probability-theory', 'probability']"
4724183,Are there any aperiodic tilings of an infinite cylinder?,"Aperiodic tilings for the plane are quite popular. I can't find any papers on aperiodic tilings on infinite surfaces like a cylinder. One reason might be that these are no longer called tiles but something else (a curved finite surface). Do references for this exist? Periodic tilings of cylinder can easily be accomplished with a machine: https://en.wikipedia.org/wiki/Knurling A lathe machine has parts that rotate and easily create a periodic pattern that has one tile type. Not sure if there exists a machine that would do it with multiple tile types. My end goal is to find a similar method to aperiodically tile a cylinder. At least one part of rotation would have to exhibit aperiodic behavior, but I leave that outside of the scope of this question.","['combinatorics', 'tiling', 'reference-request']"
4724208,Find the maximum value of $\chi(G)$ on all simple graphs $G$ with $30$ nodes and a girth of at least $6$.,"Problem statement: Let $k$ be the maximum value of $\chi(G)$ on all $30$ nodes graphs with a girth of at least $6$ . Find two numbers $a$ and $b$ such that $a \leq \chi(G) \leq b$ and $b - a \leq 1$ . Using the fact that for all simple graphs $G$ , $\chi(G) \leq \Delta(G) + 1$ , I proved that $k \leq 9$ . It is also obvious that $k \geq 3$ . How did I prove that $k \leq 9$ ? Let's say a graph $G$ exist with $30$ nodes, girth of at least $6$ and $\chi(G) = 10$ . We will find a contradiction. We know that $\chi(G) \leq \Delta(G) + 1$ so $G$ needs have a node $v$ with at least $9$ neighbors. since the girth of $G$ is at least 6, none of the neighbors of $v$ are connected to each other. we will color $v$ with $1$ and it's neighbors with $2$ ( $1$ and $2$ are color indexes). now if we remove $v$ and it's neighbors and call the remaining graph $H$ , we know that $\chi(H) \geq 8$ , $n(H) \leq 20$ . now we do the exact same thing we did with $G$ , with $H$ . we keep doing this until there is one or two nodes left. If you keep track of the number of colors used you will see that when we arrive at this situation we have used $8$ colors. If there is one node left or the nodes left aren't connected then we get contradiction that $\chi(G) \leq 9$ , if not these two need be connected to the first node we removed because if they're not then we can color one with $1$ and get the last contradiction. but if these nodes are connected to each other and the first node we remove then they form triangle which is again a contradiction. But now I have not made any progress for a day, I would be thankful if you could give a solution or a hint (mostly for how can I find useful a lower bound for $k$ ).","['graph-theory', 'coloring', 'combinatorics', 'contest-math']"
4724294,Sequence of integers without 3-terms AP in it.,"In a school there are $2023$ students, numbered $1$ to $2023$ . The teacher met the students one by one in the order of their numbers, and gave a candy to each student except if that would mean three students whose numbers form an arithmetic sequence all got candies. In this way, the teacher would give a candy to students 1 and 2, but not student 3 (as 1, 2, 3 form an arithmetic sequence), then to students 4 and 5, but not to students 6 and 7 (as both 4, 5, 6 and 1, 4, 7 are arithmetic sequences), and so on. How many students got candies in the end? I tried to list the number and found the number 1,2,4,5,10,11,13,14,28,29,31,32,37,38,40,41 is available In the first 50 numbers. The thing I could only found is that everytime it start getting a candy, it would be in the form of ✅✅❌✅✅ (for example: In number 10,11,12,13,14, 12 is not available)","['word-problem', 'combinatorics']"
4724301,M-dependency and ergodicity,"Let $X_n, n \in \mathbb{N}$ be a strictly stationnary m-dependent sequence of real-valued random variables on an underlying probability space $(\Omega,\mathcal{A},\mathbb{P})$ , i.e. $X_n$ and $X_{n'}$ are independent if $\vert n-n' \vert\geq m$ . Strictly stationnary here means that $\{X_n,n \in \mathbb{N}\} =\{X_{n+k},n \in \mathbb{N}\} $ in distribution for all $k \in \mathbb{N}$ . This already implies that $X_n, n \in \mathbb{N}$ is a measure-preserving dynamical system with respect to $\tau: \Omega \rightarrow \Omega$ with $X_{n+1}(\omega) = X_n(\tau(\omega))$ . Can we also conclude ergodicity, i.e. that any $A \in \mathcal{A}$ with $\tau(A)=A$ is $\mathbb{P}$ -trivial?","['ergodic-theory', 'stochastic-processes', 'markov-process', 'probability-theory', 'probability']"
4724339,Proving that: $\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$,"Let $a$ and $b$ be positive reals. Show that
$$\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$$","['real-numbers', 'means', 'real-analysis', 'limits', 'radicals']"
4724346,Uniqueness of the extension in Hahn-Banach,"Thinking about when we can talk about uniqueness in Hahn-Banach theorem i came up with the following conjecture: Let $H$ a Hilbert space and $M$ a closed subspace of it. Take $f\in M'$ . The there exist an unique $F\in H'$ such as $F_{|M}=f$ and $||F||=||f||$ . Here is my reasoning: since $M$ is a closed subspace of a Hilbert space, it's itself Hilbert, so by Riesz's theorem there exist an unique $u\in M$ such as $f(z)=\langle z,u\rangle,$ for every $z\in M$ and $||f||=||u||$ . On the other way, by Hahn-Banach theorem, there exist $F\in H'$ such as $||F||=||f||$ and $F_{|M}=f$ . Again, by Riesz's theorem, $F$ is uniquely determined by some vector $v\in H$ such as $||F||=||v||$ . Since they are equal in $M$ and have the same norm I think that we can conclude that $u=v$ and that the extension is uniquely. Although I think it fails, the intuition says that it seems true. What do you think? Any help will be very appreciated.","['hilbert-spaces', 'normed-spaces', 'functional-analysis']"
4724407,If $F$ is closed then $A = \{a\in\mathbb{R}^{n}: \text{there exists}\quad x \in F\quad\text{with}\quad ||x - a|| = \delta\}$ is closed.,"I'm trying to prove the next: Let $F\subset\mathbb{R}^{n}$ be a closed set and $\delta>0.$ Show that A is closed in $\mathbb{R}^{n}$ if $$A = \{a\in\mathbb{R}^{n}: \text{there exists}\quad x \in F\quad\text{with}\quad ||x - a|| = \delta\}.$$ I was attempting to prove that each convergent sequence on $A$ must have its limit in $A$ : If $(a_{m})_{m\in\mathbb{N}}\subset A$ such that $a_{m}\rightarrow a,$ then for each $m$ we have there exists $x_{m}\in F$ with property $||a_{m} - x_{m}|| = \delta.$ So I was trying to prove that the sequence $(x_{m})_{m\in\mathbb{N}}$ must be convergent in $F$ to some $x$ to utilize closedness of $F$ and have the desired property but I do not know how to do this. Other attempt was writting A as a union of circumferences of closed balls with center in each point of $F$ and trying to prove the limit of a sequence must be in such union but seems not a good way... Any kind of help is thanked in advanced.","['multivariable-calculus', 'real-analysis']"
4724424,Proof of $(A\cup B) \times X = (A \times X) \cup (B \times X)$ [duplicate],"This question already has an answer here : Proof verification: Cartesian Product of Sets (1 answer) Closed last year . An exercise given in Halmos' Naive Set Theory : required to prove that $$(A\cup B) \times X = (A \times X) \cup (B \times X).$$ My approach to this was as follows. Assume $z$ belongs to $(A\cup B) \times X$ , then, since $z$ is a member of a cartesian product, it must be of the form $z = (x,y)$ with $x$ in $A\cup B$ and $y$ in $X$ . Then $x$ is either in $A$ , or in $B$ . If $x$ is in $A$ , then since $y$ is also in $X$ , $(x,y)$ is, by definition, in $A \times X$ . However, if $x$ is in $B$ , then, again, by definition, $(x,y)$ must be in $B \times X$ . So then $(x,y)$ is in $A \times X$ , or it is in $B \times X$ . From this it follows that $z$ is in $(A \times X) \cup (B \times X)$ . We have just proved the ""only if"" part of the initial statement we set out to prove. Now, to prove the ""if"" part: Assume $z$ is in $(A \times X) \cup (B \times X)$ , then $(x,y)$ is either in $A\times X$ or $B \times X$ . If the former case is true, then $x$ is in $A$ , and $y$ is in $X$ , however, if the latter case is true, then $x$ is in $B$ , and $y$ is in $X$ . Since in both cases $y$ is in $X$ , it follows from disjunction elimination that $y$ is in $X$ . Then $x$ is either in $A$ or $B$ , and $y$ is in $X$ ; hence, $x$ is in $A\cup B$ and $y$ is in $X$ . Then, by definition, $(x,y)$ is in $(A\cup B) \times X$ . I have never done a proof using cartesian products before, so I must ask for guidance from some more professional. Is there any fault in my reasoning, moreover, is it ok for a proof to be formated like this? I am still fairly new to proving set-theoretic facts so I require constant affirmation from the more professional. Thank you in advance.","['elementary-set-theory', 'proof-writing', 'solution-verification']"
4724433,Find the inverse of the map,"I am trying to compute the inverse of $$F(x,y)=\left(\sqrt{x^2+y^2+1},xy+\frac{1}{2}\log{\left(\frac{x+y}{y-x}\right)}\right)$$ in a suitable domain $U$ of $\mathbb{R}^2$ . I have checked that $F$ is a local diffeomorphism by the Implicit function theorem, however I completely failed finding the inverse. I appreciate any help.","['multivariable-calculus', 'real-analysis']"
4724436,Does anyone have good logarithmic integrals? And logarithmic integral identities?,"I have recently taken an interest in evaluating logarithmic integrals and would really love practice problems and especially, theorems, series expansions, and identities that have helped any of ya’ll in evaluating integrals like these, especially ones with products of logarithms with different argument combined with ratios of polynomials. To hopefully give a sense as to the level I am at in the evaluation of these, I can evaluate integrals like: $$\int_{0}^{\frac{\pi}{4}}\log(\cos(x))\,dx$$ $$\int_{0}^{1}\frac{\log^n(x)}{x^2+1}\,dx$$ $$\int_{0}^{1}\log(x)\log(1\pm x)\,dx$$ $$\int_{0}^{1}\frac{\log(x)}{1-x^2}\,dx$$ $$\int_{0}^{\infty}\frac{\log(x^4+x^2+1)}{x^2+1}\,dx$$ $$\int_{0}^{\infty}\frac{\log(x)\sin(x)}{x}\,dx$$ $$\int_{0}^{\infty}\log(x)e^{-x^2}\,dx$$ $$\int_{0}^{1}\log(x)\arctan(x)\,dx$$ Anything is appreciated!","['integration', 'harmonic-numbers', 'definite-integrals', 'logarithms']"
4724460,On an interesting infinite summation... from a chemistry problem!,"In my first year of high school, I was given a chemistry problem with roots in pure math (it had to do with equilibrium, but the details aren't important here.) A common tactic in complicated mass-charge balance questions of that caliber is to express all unknown concentrations as an expression of one, then get an equation of the form $x=f(x).$ You then enter $f(Ans)$ into your calculator, make an initial guess, and then press enter until the answer converges. Since this problem was much more math than chemistry, though, you actually end up with an infinite sum! (The original phrasing of the problem involved adding infinite acids to a bottle.) The equation I arrived at was this: $$[H^+]=\sum^{\infty}_{i=1}\frac{1\times 10^{-6}\cdot 0.5^i}{i[H^+]+1\times 10^{-6}}.$$ I tested the value of the RHS for several guesses of $[H^+]$ using the first 10 terms (notably, this was on a TI calculator with 2 lines of LCD display.) Most telling was a sum of $6.93 \times 10^{-4}$ for $[H^+]=1 \times 10^{-3}.$ Thus, I conjectured that the RHS simplified to $\ln{(2)}\cdot 1\times 10^{-6}/[H^+].$ From there, solving the equation was trivial (no iteration needed, thank god.) Then, substituting the result of $8.33 \times 10^{-4}$ back into the expression gave the same number back, confirming the conjecture. My question is, how would you figure out that summation through math and not bs approximations? I'm noting a superficial similarity to the power series expansion of $\ln{x}$ but can't quite work it out. An answer would be greatly appreciated to this problem which has been living rent-free in my head for a while.","['power-series', 'taylor-expansion', 'sequences-and-series']"
4724507,A conic section formed by tracing the point of intersection of tangents to a circle,"About two years ago, I came across this wonderful property of conic sections while using Geogibra. I am not sure whether it is a new discovery or if it was previously discovered. If it was previously discovered, put a source mentioning it, and in any case, how can we prove that? If we have a moving circle that touches a conic section at two points and its tangents are perpendicular to the line of the motion path and we draw the tangents of the segments at the points of their intersection with it, then the path of movement of the two points of intersection of the two tangents located outside the straight line of the motion path will make a conic section of the same type and with the same ratio between the large diameter and the distance between the focus and the center  The two main vertices of the first conic section are considered as its focus. In the case of an ellipse you can see the following link $OF/OA=OA/OB$ In the case of an parabola you can see the following link $FQ=QT=TV$ There are two cases of hyperbola: The first case The second case $OF/OA=OA/OH$ amendment:
Here is a link to a file in Arabic with some additional details on the subject that might be useful https://drive.google.com/file/d/1YxStcytgtbCc7YlVKEF0ZCBdIhB9p6Cu/view?usp=drivesdk","['curves', 'conic-sections', 'geometry']"
4724571,Why is $d(xdx) = 0$?,"If the idea behind the exterior derivative $d$ is that it tells us how quickly a $k$ -form changes along every possible direction, why is $d(xdx)=0$ even though $xdx$ varies with $x$ ? I understand the mechanics of exterior calculus but I'm trying to understand the intuition behind why it's the right tool for ""differentiating"" vector fields, rather than say working with vector-differences like a standard derivative (e.g. $\frac {\partial\overrightarrow v}{\partial x}=\lim\limits_{h \to 0} \frac{\overrightarrow v(x+h,y)-\overrightarrow v(x,y)}{h}$ ).","['exterior-derivative', 'differential-geometry', 'vector-fields', 'differential-forms', 'exterior-algebra']"
4724576,$6x^2-12x+3=p(x-2)$ where $p$ is a prime number. find values of $p$ such that the equation has at least one integer solution,"Hello I have a question: $6x^2-12x+3=p(x-2)$ where $p$ is a prime number. find values of $p$ such that the equation has at least one integer root. What I did: $${6x^2-12x+3}=p(x-2)$$ $$6x^2-(12+p)x+3+2p=0$$ then discriminant must be greater or eqault to $0$ for it to have a solution for $x$ . $$D\ge0$$ $$(12+p)^2-24(3+2p)\ge0$$ $$p^2-24p+72\ge0$$ $$p\in(-\infty;12-6\sqrt2)]\cup[12+6\sqrt2 ;\infty)$$ and because $p$ is prime it can be those values: $$p\in\{3, 23, 29, 31, 37... \}$$ then I checked for $3,23,29,31$ and $37$ if the above equation has at least one integer root and for only $p=3$ and $p=31$ works so these are the answers but how to know that max is $31$ and after that there is no prime number that works, and I tried to check that, I ran a program that checked values for like $80000$ prime numbers but only $3$ and $31$ works. I also thought that: $$x=\frac{(12+p)\pm\sqrt{p^2-24p+72}}{12}$$ and for it to have an integer solution we need $p^2-24p+72$ to be perfect square of something but I don't know if it is worth checking. Thanks everyone in advance.","['functions', 'integers', 'prime-numbers']"
4724581,An integral related to Gamma value.,"We have: $$\int_{0}^{\pi/2}\frac{\sin{x}\log{(\tan{(x/2))}+x}}{\sqrt{\sin{x}}(\sin{x}+1)}dx=\pi-\frac{\sqrt{2\pi}\Gamma{(1/4)}^{2}}{16}-\frac{\sqrt{2}\pi^{5/2}}{2\Gamma{(1/4)}^{2}}\tag{1}.$$ As other integrals, Mathametica and Wolfram Alpha are unable to deal with it
(see Wolfram Alpha response ).
The proof I have is not elementary and it is based on the following identity: $$\Im\left(K(\sqrt{\frac{2k}{1+k}})\right)=\Im \left(K(\sqrt{\frac{1-k}{1+k}})-K(\sqrt{\frac{1+k}{1-k}})\cdot\sqrt{\frac{1+k}{1-k}}\right),\tag{2}$$ where: $$K(k)=\int_{0}^{\pi/2}\frac{dx}{\sqrt{1-k^2\sin^{2}{x}}},\tag{3}$$ is the complete elliptic integral of the first kind. The proof follows from $(2)$ integrating both sides carefully. Question: Can we prove $(1)$ using only Feynman's trick and Beta function? Thanks for your cooperation.","['integration', 'definite-integrals', 'calculus', 'closed-form', 'elliptic-integrals']"
4724604,Find the probability that no students from country B are in classroom 2,"We have a school with students from countries A and B, 155 and 35 respectively.
We have 4 classrooms that can accommodate 80, 35, 35, 40 students.
If each student is assigned to a class independently and uniformly, compute the probability that classroom 2 has no students from country B. I have in my mind two possible solutions that reach completely different answers and I don't know which one holds. Each student has probability $3/4$ that he/she does not end up in classroom 2.
So in total $$p = (3/4)^{35}=0.0000423$$ There are $\binom{190}{35}$ in which we can assign students from country B in total and there are $\binom{155}{35}$ ways that we can assign we can assign students from country B in classes except classroom 2 (so in 1, 3, 4). $$ p = \frac{C(155, 35)}{C(190, 35)} \approx 0.00035$$ There are $C(35+3-1, 35)$ ways to distribute student's of country B to the 3 other classrooms and $C(35+4-1, 35)$ way to distribute them in the 4 classrooms in total. $$ p = \frac{C(37, 35)}{C(38, 35)}=0.07894 $$","['combinations', 'combinatorics', 'probability-theory', 'probability']"
4724621,Is Pappus's theorem really a particular case of Pascal's theorem?,"I am currently studying Bézout's theorem for projective plane curves and its applications. Amid this study, I came across the very famous Pascal's and Pappus's theorems. I was able to prove Pascal's theorem; as for Pappus's theorem, most of the authors I've read/watched state that this is just a particular case of Pascal's theorem, but I can't understand how. The formulation of Pascal's theorem (according to this class notes from Andreas Gathmann ) is: Pascal's theorem. Let $F$ be an irreducible conic over an algebraically closed field. Pick six distinct points $P_1,\dots,P_6$ on $F$ (that can be thought of as the vertices of a hexagon inscribed in $F$ ). Then the three intersection points of the opposite edges of the hexagon (i.e., $P = \overline{P_1P_2} \cap \overline{P_4P_5}$ , $Q = \overline{P_2P_3} \cap \overline{P_5P_6}$ and $R = \overline{P_3P_4} \cap \overline{P_6P_1}$ ) are colinear (all of this lie on a line). Notice that the author restricts this to irreducible conics and this fact is indeed used in the proof (every other proof that I found using Bézout's theorem also uses this fact, even though some don't mention it (?)). As for Pappus's theorem, it is formulated as follows: Pappus's theorem. Let $A,B,C$ be three points on a straight line and let $X,Y,Z$ be three points on another line. If the lines $\overline{AY},\overline{BZ},\overline{CX}$ intersect the lines $\overline{BX},\overline{CY},\overline{AZ}$ respectively, then the three points of the intersection are colinear. To prove this theorem, I was watching this really cool video from Zvi Rosen . Here, he says that Pappus's theorem is just a particular case of Pascal's theorem because obviously the union of two lines form a conic and then one can apply Pascal's theorem. Note that in this video Rosen stated Pascal's theorem without the word irreducible so it made sense to him to apply it directly, but in the proof of Pascal's theorem, he used the fact that the conic $F$ is irreducible. In fact, his proof follows exactly the same logic as the class notes linked above, from Gathmann. My concern is that although the union of two lines forms a conic, this conic is reducible and thus one can't apply Pascal's theorem so directly (at least, in my point of view). So, am I missing something here or we really can't apply the Pascal Theorem to prove the Pappus's theorem? If we can't apply it, how would one prove Pappus's Theorem using Bézout's theorem? Thanks for any help in advance.","['proof-explanation', 'algebraic-geometry', 'solution-verification', 'reference-request']"
4724634,Mean reversion and ergodic processes.,"Let $\vartheta(t)$ be a real ergodic continuous Markov chain solution to the stochastic differential equation $$
\mathrm{d}\vartheta(t)=A_1(\vartheta(t),t)\mathrm{d}t+A_2(\vartheta(t),t)\mathrm{d}W(t)
$$ for $A_1,A_2$ unknown $\sigma$ -finite functions and $W(t)$ the standard Brownian motion satisfying the usual conditions. Define the transition operator $H$ as $$
(H_tg)(a)=\mathbb{E}(g(\vartheta(t))\mid \vartheta_0=a)
$$ where $g$ is an arbitrary Borel-measurable function in $\mathbb{R}$ . Assume that $H_t$ satisfies $$
|H_s|_{2}=\sup_{g,\mathbb{E}g(X)=0}\frac{\mathbb{E}^{1/2}(H_sg)^2(X)}{\mathbb{E}^{1/2}g^2(X)}<1
$$ for some $s>0$ . At last, assume that the conditional density $p_{\ell}(x\mid y)$ of $\vartheta_{t+\ell}$ given $\vartheta_t$ is continuous in the arguments $(x,y)$ and bounded and that the density $p(x)$ is bounded and time-invariant. Then is the process $\vartheta(t)$ necessary mean-reverting, or a similar type of characteristic that pulls the process to its unconditional mean? And if not, does there exists conditions on the operators $A_1$ and $A_2$ that achieve this? For some additional information, the operator $|\cdot|_2$ is called the Rosenblatt condition in the book nonparametric techniques in statistical inference . The condition is used in the paper by Fan, Fan and Lv Aggregation of Nonparametric Estimators for Volatility Matrix for their aggregated estimator for the diffusion operator using a kernel estimator, which requires the above conditions, among others, to be satisfied.","['stochastic-differential-equations', 'ergodic-theory', 'probability-theory']"
4724663,Prove that $-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx=\frac{2}{5}\zeta(3)$,"Prove that $$-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx=\frac{2}{5}\zeta(3)$$ where ${\rm Li}_2(x)$ is the Poly Logarithm function and $\zeta(s)$ is the Riemann zeta function Let $$I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx $$ Substituting $x=1-y$ gives $$I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{1-x}\ dx $$ Adding the above two equations we have $$2I=-\int_{0}^{1}{\rm Li}_2(-x(1-x))\left(\frac{1}{x}+\frac{1}{1-x}\right)\ dx $$ $$2I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x(1-x)}\ dx \tag{1}$$ Now we have the integral representation of ${\rm Li}_2(z)$ as $${\rm Li}_2(z)=-\int_{0}^{1} \frac{\log(1-tz)}{t} \ dt$$ For $z=-x(1-x)$ we have $${\rm Li}_2(-x(1-x))=-\int_{0}^{1} \frac{\log(1+tx(1-x))}{t} \ dt\tag{2}$$ So from $(1)$ and $(2)$ $$2I=\int_{0}^{1}\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x(1-x)}\ \frac{\log(1+tx(1-x))}{t} \ dt\ dx $$ Now I am unable to apply integration by parts in the last equation. Edit I came across this question on MSE click here . I am unable to understand the following equality $$-\frac{1}{2}\int _{\frac{1}{\phi ^2}}^1\frac{\ln ^2\left(x\right)}{x}\:dx-\int _{\frac{1}{\phi ^2}}^1\frac{\ln ^2\left(x\right)}{1-x}\:dx=-\frac{4}{3}\ln ^3\left(\phi \right)-2\sum _{k=1}^{\infty }\frac{1}{k^3}+2\sum _{k=1}^{\infty }\frac{1}{k^3\:\phi ^{2k}}+4\ln \left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k^2\:\phi ^{2k}}+4\ln ^2\left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k\:\phi ^{2k}}$$ The first integral I am able to evaluate. I am having problem in the second integral.
Also how do we get the following? $$2\sum _{k=1}^{\infty }\frac{1}{k^3\:\phi ^{2k}}+4\ln \left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k^2\:\phi ^{2k}}+4\ln ^2\left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k\:\phi ^{2k}}=\frac{8}{5}\zeta \left(3\right)+\frac{4}{3}\ln ^3\left(\phi \right)-\frac{8}{5}\ln \left(\phi \right)\zeta \left(2\right)+\frac{8}{5}\ln \left(\phi \right)\zeta \left(2\right)$$","['number-theory', 'analysis', 'complex-analysis', 'analytic-number-theory', 'polylogarithm']"
4724673,Regularity of functions in weighted Sobolev spaces,"I am interested in a version of Morrey's inequality but for a weighted Sobolev space. The classical Morrey inequality states, Let $p>N$ , then for all $u \in W^{1,p}(\mathbb{R}^N)$ , $$
|u(x)-u(y)|\leq C|x-y|^{\alpha} \cdot\|\nabla u\|_{L_p},
$$ with $\alpha=1-\frac{N}{p}$ . My question is does a similar result hold if we are working in a weighted Sobolev space, i.e let, $$
W_\omega^{1,p}(\mathbb{R}^N)=\{ u:\int_\mathbb{R}|u|^p\omega_1dx + \int_{\mathbb{R}}|\nabla u|^p\omega_2dx < \infty \},
$$ with $\omega_1$ and $\omega_2$ are positive weight functions. I have tried to prove a result like this on my own but with no avail, approximating the function $u$ by it's derivative becomes more delicate. Any references would be greatly appreciated.","['lp-spaces', 'sobolev-spaces', 'functional-analysis']"
4724769,"Any cool ways to solve $\int_{0}^{\frac{\pi}{2}}\log(1+4\sin^2x)\,dx$","As per the title, I have solved the following integral $$\int_{0}^{\frac{\pi}{2}}\log(1+4\sin^2x)\,dx$$ I would love to see any insights, and solution processes anyone may have in solving it as well. For anyone who may be interested here is my solution process. $$I=\int_{0}^{\frac{\pi}{2}}\log(1+4\sin^2(x))\,dx$$ $$I=\int_{0}^{\frac{\pi}{2}}\log((1+4\sin^2(x))\sec^2(x)\cos^2(x))\,dx$$ $$=\int_{0}^{\frac{\pi}{2}}\log(1+5\tan^2(x))\,dx+2\int_{0}^{\frac{\pi}{2}}\log(\cos(x))\,dx$$ The second integral is a well known result easily provable with symmetry, and so we have. $$I=-\pi\log(2)+\int_{0}^{\frac{\pi}{2}}\log(1+5\tan^2(x))\,dx$$ Now let $$\tan(x)\longrightarrow{x}$$ This gives $$I=-\pi\log(2)+\int_{0}^{\infty}\frac{\log(1+5x^2)}{1+x^2}\,dx$$ Now let $$I(t)=\int_{0}^{\infty}\frac{\log(1+tx^2)}{1+x^2}\,dx$$ Differentiating with respect to t yields $$I’(t)=\int_{0}^{\infty}\frac{x^2}{(1+x^2)(1+tx^2)}\,dx$$ $$=\frac{1}{t-1}[\int_{0}^{\infty}\frac{1}{1+x^2}\,dx-\int_{0}^{\infty}\frac{1}{1+tx^2}]\,dx$$ So we get $$I’(t)=\frac{\pi}{2}\frac{1}{t-1}(1-\frac{1}{\sqrt{t}})$$ Since we know that $$I=I(5)-I(0)-\pi\log(2)
=-\pi\log(2)+\int_{0}^{5}I’(t)\,dt$$ $$I=-\pi\log(2)+\frac{\pi}{2}\int_{0}^{5}\frac{1}{t-1}(1-\frac{1}{\sqrt{t}})\,dt$$ Let $$t=x^2$$ and so $$I=-\pi\log(2)+\pi\int_{0}^{\sqrt{5}}\frac{1}{x+1}\,dx$$ $$=\pi\log(\phi)$$ And so we achieve the desired result, namely: $$\int_{0}^{\frac{\pi}{2}}\log(1+4\sin^2(x))\,dx=\pi\log(\phi)$$","['integration', 'golden-ratio', 'definite-integrals', 'alternative-proof', 'trigonometric-integrals']"
4724802,"Showing that $\|f-g\|_{L^1}=\int_{-\infty}^\infty \mu\left[\left(F(t) \setminus G(t)\right) \cup \left(G(t) \setminus F(t)\right)\right] \,dt.$","Let $f,g$ be measurable and integrable functions on a measure space $(X, \mu)$ and let us define the following sets, $$F(t)=\{x\in X: f(x)> t\}\quad G(t)=\{x\in X: g(x)> t\}.$$ I would like to show that $$\|f-g\|_{L^1}=\int_{-\infty}^\infty \mu\left[\left(F(t) \setminus G(t)\right) \cup \left(G(t) \setminus F(t)\right)\right] \,dt.$$ I first thought of trying to use the same technique to show that $$\|f\|_{L^1}=\int_X|f|\,dx=\int_X\int_0^\infty 1_{t<|f|}\,dt\,dx=\int_0^\infty\mu\{x\in X: |f|>t\},$$ but got quickly got stuck trying to break down $|f-g|>t$ into cases.","['integration', 'measure-theory']"
4724813,Relationship between sum of squares and square of sums in a recursive way,"In the proof of Theorem 2 of Sparse projections onto the simplex authors mention the following equality for any $\mathbf{b} \in \mathbb{R}^k$ and $\lambda \in \mathbb{R}$ : $$
\begin{aligned}
&\sum_{i=1}^k b_i^2 - \frac{((\sum_{i=1}^k b_i) - \lambda)^2}{k}
\\
&=
\\
&\lambda(2b_1-\lambda)+\sum_{j=2}^k
\frac{j-1}{j}
\Big(
b_j - \frac{(\sum_{i=1}^{j-1}b_i)-\lambda}{j-1}
\Big)^2.
\end{aligned}
$$ Question Is there an elegant way of showing the above equality? Probably it is possible to show it using induction but I am looking to find a way other than induction. My thoughts If we look at the way terms are given on the left hand side, one can think of sampling $k$ number of $b_i$ 's with uniform probability of $\frac{1}{k}$ where each $b_i$ has a fix distribution $\mathcal{D}$ . Then the sums can be written as expected value of k samples of $b_i$ and $b_i^2$ . Looking in this way, can we give another proof to that other than algebraic proof? Another observation is that, the right hand side some how tries to relate sum of squares and square of sums in a recursive way.","['optimization', 'statistics', 'linear-algebra', 'sum-of-squares-method']"
4724848,"How to solve the differential equation $dx/dy = x^2 y\,$?","How to solve? I have gotten $$ 
y\,dy = \frac{dx}{x^2} 
$$ then taking integrals $$ 
\int y\, dy = \int \frac{dx}{x^2} 
$$ and solving: $$ 
y = \sqrt{-2x^{-1} + C} 
$$ but it's not the correct answer.","['integration', 'calculus', 'analysis', 'ordinary-differential-equations']"
4724855,Is there a well-defined notion of size where the set of rationals is “bigger” than the set of integers?,"I want to rephrase my question: is there a definition of size that retains order for strict subsets but still allows you to compare or equate sets that are mutually exclusive from each other/contain mutually exclusive elements? The main examples I would like this definition to satisfy are: The set of positive integers is equivalent to the set of negative integers. Natural numbers < integers < rationals < reals Even positive integers = odd positive integers My original question:
The main ways to quantify “size” of a set that I’ve come across is cardinality and measure, both of which equate rationals to integers (bijection/measure 0). Is there any other metric for evaluating size of a set that, for instance, preserves order for sets that are strictly subsets of other sets? For instance, some metric that denotes the size of evens equal to the size of odds but denotes both as less than the size of all integers.",['measure-theory']
4724879,If two matrices have the same characteristic polynomial then do they have the same determinant?,"There is a similar question here , but it's asking that if two matrices have the same
characteristic polynomial then are they similar. If the answer were positive then the answer to my question will also be positive, but it's not. Now, if two matrices have the same characteristic polynomial then they are of the same order. Suppose $A$ and $B$ are matrices of order 2 and they both have a characteristic polynomial of $t^2+at+b$ . We know that $t^2+at+b=t^2-\text{trace}(A)t+\text{det}(A)=t^2-\text{trace}(B)t+\text{det}(B)$ , so in this case the answer to my question is affirmative. However, I don't think the answer is ""yes"" in general because then I probably will have a theorem which says this in my book. I looked for counterexamples but couldn't find any. Basically, I made up random matrices and never got a counterexample. So if the answer to the question is ""no"", can you please explain how you arrived at the counterexample.","['determinant', 'trace', 'matrices', 'linear-algebra', 'characteristic-polynomial']"
4724880,Closed Linear Subspace of BFS $X$ .,"This is the definition which would be useful: Let X be a Bannach Function Space on $[0;1]$ .
Assume that the space $C([0;1])$ of continuous functions on $[0;1]$ is a closed linear subspace of $X$ . Then for every $f \in C([0;1])$ we have $$||f|||_X \leq ||f||_{C([0;1])} $$ . As I read It's because $||f||_X \leq ||\chi_{\Omega}||_X||f||_{C(I)}$ , assuming that $||\chi_{\Omega}||_X = 1 $ . I don't understand why do we have this inequality : $||f||_X \leq ||\chi_{\Omega}||_X||f||_{C(I)}$ . ? Any help would be appreciated.","['banach-spaces', 'normed-spaces', 'analysis', 'real-analysis', 'functional-analysis']"
4724901,Lee Smooth Manifolds: Fundamental Theorem of Flows (9.12) Details,"Theorem 9.12 For the second paragraph of the proof, I do not see how it is obvious that $\theta^{(p)}$ is the unique, maximal extension of all such integral curves. Without appealing to a Zorn's Lemma argument. Edit March 28: The Zorn's Lemma argument is unnecessary because we can view the integral curves as a subset of the product space $\mathbb{R}\times M$ . Taking the union over all the graphs gives us a maximal extension. Zorn's Lemma Fix $p\in M$ , define $$
\mathcal{I}_p = \Biggl\{\gamma:J\to M,\quad \substack{J\text{ is an open interval containing 0, and}\\ \gamma\text{ is an integral curve of } V \text{ starting at } p}\Biggr\}
$$ If $\gamma_1$ , and $\gamma_2$ are in $\mathcal{I}_p$ , we define $$
\gamma_1\lesssim\gamma_2\iff \operatorname{domain}\gamma_1\subseteq\operatorname{domain}\gamma_2
$$ Through a 'union of connected open sets with a common point is again connected and open' argument, we obtain a maximal curve denoted by $\theta^{(p)}$ with domain $\mathcal{D}^{(p)}$ , similar to the proof for Tychonoff's Theorem. Smoothness of $\theta$ Once we have these unique maximal integral curves, on the 6th paragraph of the proof (where he introduces some $t_1>t_0-\varepsilon$ ), I fail to see why $$
t_1<t_0\implies (t,p_0)\in W
$$ without the added restriction $t_1>0$ . Later in the paragraph, we define $\widetilde{\theta}: [0, t_1+\varepsilon)\times U_1\to M$ , with our original flow $\theta$ being smooth on $(t_1-\delta,\: t_1+\delta)\times U_1$ . Shouldn't $\widetilde{\theta}$ be defined on some $(-\delta, t_1+\varepsilon)$ (by shrinking $\delta$ if necessary) instead? By showing $\widetilde{\theta}(\cdot,p)$ is again an integral curve starting at $p$ for any $p\in U_1$ . $$\operatorname{domain}\widetilde{\theta}(\cdot,p)=(-\delta,\: t_1+\varepsilon)\implies \widetilde{\theta}(\cdot,p)\lesssim \theta^{(p)}$$ It has to be pointwise equal to $\theta$ . $$\widetilde{\theta}=\theta\vert_{(-\delta,\:t_1+\varepsilon)\times U_1}$$ Since $\widetilde{\theta}$ is smooth on $(-\delta, t_1+\varepsilon)\times U_1$ , this means $\theta$ is smooth on the same neighbourhood as well, breaking the supremum. Excerpt from book Errata can be found here .","['smooth-manifolds', 'ordinary-differential-equations', 'differential-geometry']"
4724917,$f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x$ and $\lim_{n \to \infty} \frac{f(n)}{\pi(n)} = 1$?,"$$f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x$$ $$f(n) < \pi(n+1)$$ $$\lim_{n \to \infty} \frac{f(n)}{\pi(n)} = 1$$ where $\pi(n)$ is the prime counting function. My mentor more or less wrote that when he was around $12$ yo. If you use the Riemann zeta function $\zeta(s)$ to prove the PNT and you know the easy $f(x) < \frac{x}{\ln(x)-1}$ (easy to show for large $x$ at least) then with some little extra work you can prove the two statements at least for large $x$ or large $n$ . But how about proving it directly ?
What if we did not know that $\pi(n)$ is about $= \frac{x}{\ln(x)-1}$ ?
What if we do not use the zeta function nor complex dynamics ? Is this related to one of those elementary proofs of the prime number theorem ? How does one even come up with the idea of the equation above ??
What is the logic behind it ? Do not confuse with the similar looking $$Li(x) + Li(x^{1/2}) + Li(x^{1/3}) + ...$$ you often see in number theory, this is not the one. And it is also not this one $$g(x) + g(x)/2 + g(x)/3 + g(x)/4 + ... = x$$ Because that one gives about $g(n) = \frac{n}{H_n}$ where H_n is the n th harmonic number, what is a weak version of the PNT. It also does not follow from the 3 theorems of Mertens nor does it resemble anything I ever read. I am not looking for sharp asymptotics for $f(x)$ here, that is not the question here.
(although I will probably start a new topic about that soon) Any ideas ? Btw this function is even more mysterious when you think about it : For example $$f(x) + f(x/2) + f(x/3) + f(x/4) + ... = x$$ substitute $x$ to $x/2$ $$f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ... = x/2$$ This implies that $$f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ... = x/2$$ and thus $$f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ... = f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ... $$ and $$f(f(x) + f(x/3) + f(x/5) + f(x/7) + f(x/9) + ...) = f(f(x/2) + f(x/4) + f(x/6) + f(x/8) + f(x/10) + ...) = f(x/2)$$ and similar identities can be constructed.
Imo very much like series multisection, fractals and self-referential things. But getting series expansions for it that are defined everywhere is hard. Does $f$ need to be continu ? Maybe we should relax the equation and write $$f(n) + f(n/2) + f(n/3) + f(n/4) + ...= n$$ and the divisions get rounded or so. Or maybe we should truncate the ellipsis (...) somehow. But that still does not answer my questions. edit My mentor told me this $$\pi(x) + \pi(x/2) + \pi(x/3) + ... > x$$ if we take enough terms and if the following is true $$\pi(x/a) > \frac{\pi(x)}{a}$$ (conjecture A) It then follows that $$f(x) + f(x/2) + f(x/3) + ... = x$$ implies that $f(x) < \pi(x)$ Conjecture A might have a name, it is a weaker version of the second hardy-littlewood conjecture : $$\pi(x+y) - \pi(x) < \pi(y) + 1$$ Conjecture A makes sense for large $x$ and small $a$ for sure if we use a sharp version of the prime number theorem somewhat like the good asymptotic $$\frac{x}{\ln(x) - 1 + \frac{1}{\ln(x)}}$$ But I am unsure if conjecture A has been proven. Again this might be overkill but it is another way of thinking about it. see : $f(x) + f(x/2) + f(x/3) + ... = x$ and conjecture A : $\pi(x/a) > \frac{\pi(x)}{a}$ edit A similar looking idea seems to have occured here on page 8 : http://www.math.columbia.edu/~goldfeld/ErdosSelbergDispute.pdf although that is an integral instead of a sum. I wanted to mention it.
Maybe it has value to someone.","['functional-equations', 'number-theory', 'alternative-proof', 'limits', 'prime-numbers']"
4724919,Tightness of Bernstein's inequality and Hoeffding's inequality,"Let $\{ X_t \}$ be independent zero mean random variables, and $|X_t|<R$ .
Bernstein's inequality yields: $$\mathbb{P} \left( \sum_{t=1}^{T}X_{i} \geq \varepsilon \right)
\leq \exp \left( 
\frac{ -\frac{1}{2}\varepsilon^{2} }{
\sum_{t=1}^{T} \mathbb{E} \left[ X_{t}^{2} \right] +\frac{1}{3}R\varepsilon }
\right).$$ Azuma's inequality (Hoeffding's inequality) yields: $$\mathbb{P} \left( \sum_{t=1}^{T}X_{t} \geq \varepsilon \right)
\leq \exp \left(
\frac{-\frac12 \varepsilon^{2}}{ TR^2 }
\right).$$ Apparently, for large $\varepsilon$ , Hoeffding's inequality is better; while for $\mathrm{var}(X_t)<R^2$ and $\varepsilon<TR$ , Bernstein's inequality is better. So Question 1: Is Bernstein/Hoeffding tight? Question 2: Is there any inequality that combines Bernstein+Hoeffding? Q3: Is there any inequality that is tight in both regimes?","['distribution-tails', 'statistics', 'probability-theory', 'inequality']"
4724969,Differential Equations: What is the difference between a linear and nonlinear centre?,"In 2nd year differential equations, we define a centre to be the graphical phenomena we witness around a critical point with complex eigenvalues in form $\pm bi$ . But, we also throw around this term ""non-linear centre,"" saying that for a conservative system, a linear centre is often a non-linear centre. I'm having a tough time differentiating between a ""non-linear centre"" and a ""linear centre,"" being only familiar with what a ""centre"" is based on my earlier definition. What's the difference? Could you give an example?","['differential', 'ordinary-differential-equations']"
4724998,"Are there formal definitions of ""state"" and ""state variable"" in the context of state space models in control theory?","I'm taking a class on control theory and I thought I understood the state space representation of linear systems -- it seemed like essentially just extra syntax (or, ""syntactic sugar"" as programmers would call it) used in the process of a turning an nth order linear differential equation into a system of n first order linear DE's.  It seemed the ""state variables"" were additional syntax to ""hide"" the additional derivatives. e.g. If I have the DE $$v''(t) = v'(t) + 2u(t) + 3v(t)$$ I can define the state variables $$x_1 = v(t), x_2 = x_1'(t) = v'(t)$$ and rewrite the single 2nd order DE as the set of equations $$ x_2' = x_2 + 2u + 3x_1
$$ $$y = x_1
$$ where the first equation is considered the ""state"" equation and the second is the output equation.  And since this is a set of linear equations we could of course represent it in matrix form.  The process is simple enough, at least for the sorts of problems we're dealing with in my class. But the thing I realized today that I don't get is, what makes the first equation the ""state"" equation?  What even is a state, mathematically, in the context of state spaces? I've seen informal definitions stuff like, A state variable is one of the set of variables that are used to describe the mathematical ""state"" of a dynamical system. Intuitively, the state of a system describes enough about the system to determine its future behaviour in the absence of any external forces affecting the system.
( Wikipedia ) and The state variables represent values from inside the system that can change over time. ( Wikibooks ) And I've seen specific examples where it intuitively makes sense to call one set of variables the state variables: e.g. if we're modeling an RLC circuit and we just care about the voltage on the resistor and are controlling the voltage of the power source, it makes sense intuitively to call the resistor voltage the output, the power source voltage the input, and the voltages through the inductor and capacitor the state variables. But surely there has to be a more formal and less hand-wavy definition than that?  What I'm really looking for is something in terms of linear and/or abstract algebra that doesn't rely on vague and nonmathematical terms like ""inside the system"".","['ordinary-differential-equations', 'control-theory', 'definition', 'linear-algebra', 'linear-control']"
4725016,Symbolising the set of pupils who do not like both subjects,"If $A= \{\text{pupils who like Science}\}$ , $B= \{\text{pupils who like History}\},$ then is the set of pupils who do not like both subjects $$(A\cap B)^\complement$$ or $$(A\cup B)^\complement\,?$$","['elementary-set-theory', 'discrete-mathematics', 'logic-translation']"
4725076,Integral using Bessel functions $\int_0^{\pi}\cos⁡(z\sin x)e^{\cos (x)}\ dx$,"I want to evaluate the following, $$I(z)=\int_0^{\pi}\cos⁡(z\sin x)e^{\cos (x)}\ dx,\quad z\in\mathbb{N}$$ using Bessel functions . My attempt where I ended up with a divergent series is below. I think the error comes from the switching of integration and summation, but I hope there is a way around this. Using the known expansion in terms of the Bessel function $J_\alpha(z)$ , $$\cos(z \sin x)=J_0(z)+2\sum_{k=1}^\infty J_{2k}(z)\cos(2kx)$$ $$\begin{align*}I(z)=\int_0^{\pi}\cos⁡(z\sin x)e^{\cos (x)}\ dx&=\int_0^\pi J_0(z)e^{\cos(x)}\ dx+2\int_0^\pi \sum_{k=1}^\infty J_{2k}(z)\cos(2kx)e^{\cos(x)}\ dx\\&=\underbrace{J_0(z)\int_0^\pi e^{\cos(x)}\ dx}_{T_1}+\underbrace{2\sum_{k=1}^\infty J_{2k}(z)\int_0^\pi\cos(2kx)e^{\cos(x)}\ dx}_{T_2}.\end{align*}$$ We evaluate $T_1$ by subbing $z=e^{ix}$ and integrating over a circle contour $\gamma$ surrounding the origin in a counter clockwise sense, $$\begin{align*}J_0(z)\int_0^\pi e^{\cos(x)}\ dx&=\frac{J_0(z)}{2}\int_{-\pi}^\pi e^{\cos(x)}\ dx 
\\&=\frac{J_0(z)}{2}\int_{-\pi}^\pi \exp\left(\frac{e^{ix}+e^{-ix}}{2}\right)\frac{ie^{ix}}{ie^{ix}}\ dx \\&=\frac{J_0(z)}{2i}\oint_\gamma\frac{1}{z}\exp\left(\frac{z+z^{-1}}{2}\right)\ dz\end{align*}$$ the only singularity is at $z=0$ , and we find the residue at the point by extracting the coefficient of the $z^{-1}$ term in the laurent expansion of the integrand, $$[z^{-1}]\left(\frac{1}{z}\exp\left(\frac{z+z^{-1}}{2}\right)\right)=[z^{-1}]\left(\frac{1}{z}\sum_{n=0}^\infty\frac{z^n}{n!2^n}\sum_{n=0}^\infty\frac{z^{-n}}{n!2^n}\right)=\sum_{n=0}^\infty\frac{1}{(n!2^{n})^2}=I_0(1)$$ where $I_0(z)$ is the Modified Bessel function and $[z^n]f(z)$ is the coefficient of $z^n$ in $f$ . Thus by Cauchy's formula , $$T_1=\pi J_0(z)I_0(1).$$ For $T_2$ we can evaluate the integral in terms of the Modified Bessel function again, given the representation , $$I_\alpha(z)=\frac{1}{\pi}\int_0^\pi e^{z\cos(x)}\cos(\alpha x)\ dx-\frac{\sin(\alpha \pi)}{\pi}\int_0^{+\infty} e^{-z\cosh(x)-\alpha x}\ dx$$ we have, $$2\sum_{k=1}^\infty J_{2k}(z)\int_0^\pi\cos(2kx)e^{\cos(x)}\ dx=2\sum_{k=1}^\infty J_{2k}(z)\left(\pi I_{2k}(1)+\sin(2\pi k)\int_0^{+\infty} e^{-\cosh(x)-2kx}\ dx\right)$$ the integral on the RHS vanishes due to the $\sin(2\pi k)$ factor, $$T_2=2\pi\sum_{k=1}^\infty J_{2k}(z)I_{2k}(1)$$ hence, $$I(z)=\pi J_0(z)I_0(1)+2\pi\sum_{k=1}^\infty J_{2k}(z)I_{2k}(1)$$ Edit : The convergence of the series above is as follows. For fixed $z$ and large $k$ , $$J_{2k}(z)\sim\frac{1}{\sqrt{4\pi k}}\left(\frac{ez}{4k}\right)^{2k},\quad I_{2k}(1)\sim\frac{1}{\sqrt{4\pi k}}\left(\frac{e}{4k}\right)^{2k}\\
J_{2k}(z)I_{2k}(1)\sim\frac{1}{4\pi k}\left(\frac{e}{4k}\right)^{4k}z^{2k}\sim\frac{1}{k^k}$$ which is convergent. Here we have made use of asymptotic expansions $(10.19.1)$ and $(10.14.1)$ as suggested in the comments.","['special-functions', 'laurent-series', 'complex-analysis', 'sequences-and-series', 'bessel-functions']"
4725148,Can we use series expansion to evaluate $\lim_{x\to\infty}\frac{\cos x+\sin x}{x^2}?$,"Evaluate $\lim_{x\to\infty}\frac{\cos x+\sin x}{x^2}$ Method $1$ : Numerator is small. Denominator is much bigger. So, limit is zero. Method $2$ : Using series expansion, $\lim_{x\to\infty}\frac{\left(1-\frac{x^2}{2!}+\frac{x^4}{4!}+...\right)+\left(x-\frac{x^3}{3!}+\frac{x^6}{6!}+...\right)}{x^2}$ Now, the numerator is a polynomial. And its degree is higher than that of denominator. So, the answer is $\infty?$ What's wrong in this method? I first thought the series expansion can be used only when $x$ tends to zero. But perhaps this is not the case. These expansions are valid for all $x?$ Or, maybe the numerator can be manipulated in some way so that the final answer is zero?","['contest-math', 'limits-without-lhopital', 'calculus', 'solution-verification', 'limits']"
4725149,Why does the TI-30X Pro calculator seem to get the derivative of $\sin(x)$ at $x=0$ wrong?,"I typed the derivative of $\sin(x)$ at $x=0$ into my calculator, and it outputted $0.0174...$ I was expecting the answer $1$ . What went wrong?","['calculus', 'derivatives', 'calculator']"
4725188,Convergence of series of uniform random variables,"I am working of following problem. Let $\{X_n\}$ be sequence of independent random variables, $X_n$ ~ $U[-n, n]$ . For what values of $p>0$ series $$\sum_{n=1}^{\infty}\frac{X_n}{n^p}$$ is convergent? Let $Y_n = \frac{X_n}{n^p}$ . We have that $\mathbb{E}[X_n] = 0$ and $VarX_n = \frac{n^2}{3}$ , so $\mathbb{E}[Y_n] = 0$ and $VarY_n = \frac{1}{3n^{2p-2}}$ . From independence of $\{X_n\}$ , independece of $\{Y_n\}$ follows. $\sum_{n=1}^{\infty}\mathbb{E}[Y_n] = 0$ . $\sum_{n=1}^{\infty}VarY_n  < \infty$ if and only if $p > \frac{3}{2}$ , so by Kolmogorov two series theorem, sum converges a.e for $p > \frac{3}{2}$ . How can I determine convergence if $p \in (0, \frac{3}{2})$ ?","['convergence-divergence', 'random-variables', 'probability-theory', 'sequences-and-series']"
4725195,A question about a third-order homogenous linear ODE,"Consider the ODE $$
xy'''-2xy''+(x-1)y'+y=0
$$ wolframalpha returns the following solution: $$
y(x) = c_2 e^x x^2 + c_3 e^x (1 - x^2) + c_1 e^x
$$ My Question : It seems that the solutions are spanned by $$
\{y_1(x)=e^xx^2\ ,\ y_2(x)=e^x(x^2-1)\ ,\ y_3(x)=e^x\}
$$ But this set is linearly dependent since $y_2=y_1-y_3$ . Shouldn't we suppose to have 3 linearly independent  in the basis?",['ordinary-differential-equations']
4725206,Why topological spaces in Baire category theorem are required to be Hausdorff?,"In Baire category theorem it says  a locally compact Hausdorff space $S$ is second category. In the proof, it choose $V_1,V_2\cdots$ are dense open subset of $S$ . $B_0$ is an arbitrary nonempty open set in $S$ . Then we choose $B_n\not = \emptyset$ , $\bar B_n \subset V_n \cap B_{n-1}$ . We can choose $\bar B_n$ is compact. Put $K=\cap_{n=1}^\infty \bar B_n$ . Because in compact space, every family of closed subsets having the finite intersection property has non-empty intersection. So $K$ is nonempty. And $K\subset B_0,K\subset V_n$ . Hence $B_0$ intersects $\cap V_n$ So where the Hausdorffness is used. It seems that it should only be regular space. So where it use that it should be regular?","['baire-category', 'functional-analysis']"
4725209,Stuck On Understanding a Binomial Coefficients Proof,"While reading a proof on the nature of binomial coefficients, the author of the text makes a leap I can't understand. Namely, he states that if $k \lt \frac{n+1}{2}$ , such that $k$ is an integer, and $n$ is even; then the equation is equivalent to $k \le \frac{n}{2}$ . I don't fully understand how this is justified. I tried working the reasoning out on my own. What I reasoned is as follows: since $k$ is an integer, if we add $1$ to the RHS of the equation we should have $k + 1 \le \frac{n + 1}{2}$ . From here, $k \le \frac{n – 1}{2}$ . I don't know what else I could add. Another related thing he does is that he states that, if $n$ is odd, and $k$ is, again, an integer, then $k \lt \frac{n + 1}{2}$ is equivalent to $k \le \frac{n – 1}{2}$ . Again, I am only further puzzled. Help would be of use. Thank you in advance.","['algebra-precalculus', 'inequality']"
4725286,Understanding Terence Tao's proof of Cayley-Bacharach's theorem.,"I am studying Cayley-Bacharach's theorem, which Tao states as (here) Theorem (Cayley-Bacharach). Let $\gamma_0 = \{ P_0(x,y) = 0\}$ and $\gamma_\infty = \{ P_\infty(x,y) =0 \}$ be two cubic curves that
intersect (over an algebrically closed field $k$ ) in precisely nine
distinct points $A_1,\dots,A_9.$ Let $P$ be a cubic polynomial that
vanishes on eight of those points (say $A_1,\dots,A_8).$ Then $P$ is a
linear combination of $P_0,P_\infty$ and in particular vanishes on the
ninth point $A_9.$ After this, Tao presents a proof of this theorem which I believe I understand almost totally, there is just one small detail that I am not getting. Below I will show an outline of the proof presented in the link above (just the topics) and be more detailed in the part that I didn't get. Outline of proof. Essentialy, we can split the proof Tao did in the following steps: Let $P$ be a cubic polynomial that vanishes on the points $A_1,\dots,A_8$ such that $P$ is not a linear combination of $P_0,P_\infty.$ Some observation on the $9$ original points: no four of these can be colinear and no seven of these can lie on the same quadric. A consequence of the observations above: any five points from $\{A_1,\dots,A_9\}$ determine a unique quadric curve $\sigma$ . Some further conclusions: no three points from $\{ A_1,\dots,A_8\}$ are colinear and no six points from $\{A_1,\dots,A_8\}$ lie on the same quadric curve. After this, Tao continues: Finally, let $l$ be the line that goes through the points $A_1,A_2$ and let $\sigma$ be the quadric curve that goes through $A_3,\dots,A_7.$ Note that the quadric curve must be a conic section based on $4.$ Even more, from $4.$ It also follows that $A_8 \notin l$ and $A_8 \notin \sigma.$ Now let us consider two points $B,C$ that are in $l$ but not in $\sigma.$ We can construct $Q = aP + bP_0 + cP_\infty$ such that $Q$ vanishes in $B$ and $C$ . Furthermore, $Q$ vanishes in $4$ points of $l$ $(B,C,A_1,A_2)$ and thus $l$ must be a component of $Q$ (Bézout). The other component must be a quadric curve (degree comparasion). Also, this quadric curve must contain the points $A_3,\dots,A_7$ and thus it must be $\sigma$ , based on $3.$ But then $Q$ doesn't vanish at $A_8$ , which is a contradiction, because $A_8$ vanishes at the three polynomials $P$ , $P_0$ and $P_\infty$ . My doubt. I believe I understand almost every argument that Tao used: I just don't see where we use the fact that $P$ is not a linear combination of $P_0,P_\infty.$ Basically, I understand that we find a contradiction and how we find it, I just don't see how the fact that $P$ is not a linear combination of $P_0$ and $P_\infty$ influences this same contradiction. As far as my mind can think, we would reach this same contradiction if $P$ was a linear combination of $P_0,P_\infty$ . I don't see what I am missing but it must be something really trivial. Thanks for any help in advance.","['algebraic-curves', 'plane-curves', 'proof-explanation', 'curves', 'algebraic-geometry']"
4725288,Find $f(1)$ and also find monotonicity of $f(x)$ and $f\left(f(x)\right)$,"Consider $\alpha>1$ and $f:\left[\dfrac{1}{\alpha}, \alpha\right]\to\left[\dfrac{1}{\alpha}, \alpha\right]$ be a bijective function. Suppose that $f^{-1}(x)=\dfrac{1}{f(x)}$ for all $x\in \left[\dfrac{1}{\alpha}, \alpha\right]$ . Then find $f(1)$ and also find monotonicity of $f(x)$ and $f\left(f(x)\right)$ Given solution in Book for $f(1)$ : Author of book replaced $x$ by $f(x)$ in given equation and then he obtained $x=\dfrac{1}{f\left(f(x)\right)}\implies f(f(x))=\dfrac{1}{x}$ $\implies f^{-1}(x)=f\left(\dfrac{1}{x}\right).......(1)$ and from $(1)$ and given equation in question he obtained $f(x)f\left(\dfrac{1}{x}\right)=1$ Now set $x=1$ and he obtained $f(1)=1$ Doubts: (1) As I studied in my classroom that when $f(x)$ is increasing then $f^{-1}(x)$ will be increasing but according to given function in question as $f(x)$ increases $f^{-1}(x)$ decreases. So it is possible for continuous or discontinuous function. (2) How to obtain the proof of monotonicity of $f(x)$ and $f(f(x))$ ?","['calculus', 'functions', 'algebra-precalculus']"
4725292,"Does there exist a sequence $a_1, a_2, . . . \in \{−1, 1\}$ such that $\sum_n \frac{a_n a_{n+1} \cdots \,a_{n+k}}{n}$ converges for all $k\geq0$?","Does there exist a sequence $a_1, a_2, . . . \in \{−1, 1\}$ such that $\sum_n \frac{a_n a_{n+1} \cdots \, a_{n+k}}{n}$ converges for all $k \geq 0$ ? I got this problem in my probability course and couldn't get to a solution. I want to start by asking why would a probability approach work here? If I set $X_1 ,...$ i.i.d that can get $-1$ or $1$ with probability $1/2$ . Even if I prove the chance to get such a sequence is zero, there still might be a very specific sequence that would will hold for the problem. Just the chance of getting such sequence is zero. This thought made me think I need to prove such a sequence exists. I tried using Kolmogorov's three-series theorem but havn't gotten much progress. Thank you,","['probability-theory', 'sequences-and-series']"
4725499,A graph looks flatter and flatter as we zoom in,"Let $f : \mathbb{R}^n \to \mathbb{R}^m$ be a function of class $C^1$ such that $f(0) = 0$ and $Df(0) = 0$ . Let $$M = \{ (z, f(z)) \in \mathbb{R}^{n} \times \mathbb{R}^m: z \in \mathbb{R}^n \}$$ be the graph of $f$ ; it is an $n$ -dimensional submanifold of $\mathbb{R}^{n+m}$ . By hypothesis, the tangent plane of $M$ at the origin is the subspace $\mathbb{R}^n \times 0 \subset \mathbb{R}^n \times \mathbb{R}^m$ . Let $S$ be the unit sphere of $\mathbb{R}^n \times 0$ . I would like to prove that for every $\varepsilon > 0$ there exists $\delta > 0$ such that $$0 < \vert x \vert < \delta \text{ and } x\in M \implies d\left( \frac{x}{\vert x \vert}, S \right) < \varepsilon.$$ Intuitively, as we zoom in at the origin, $M$ looks flatter and flatter, so eventually the directions generated by points in $M$ get closer to directions in $S$ . How do I formalize this?","['multivariable-calculus', 'smooth-manifolds']"
4725562,Partial derivative with respect to a matrix in RNN backpropagation,"I have an issue with the following problem.
I am trying to derive the gradients with respect to $x_t, h_{t-1}, W_x, W_h$ . $x_t$ is a $N*D$ vector. $h_t$ is a $N*H$ vector. $W_h$ is a $H*H$ matrix. $W_x$ is a $D*H$ matrix. The function is $h_t=tanh(h_{t-1}W_h+x_tW_x+b)=tanh(O_t)$ I am struggling with the derivative with respect to $W_h, W_x$ The answer said that the derivative with respect to $W_h$ is $h_{t-1}^T⋅dh_t⋅(1-h_t⋅h_t)$ . $dh_t$ means the derivative obtained in the previous step.
Also, $dh_t⋅(1-h_t⋅h_t)$ is just inner product in my guess. $h_{t-1}^T⋅dh_t$ is done by matrix multiplication.
The most confusing part is how $h_{t-1}^T$ moves to the first term when the chain rule is applied. I will appreciate if anyone could answer this question.","['neural-networks', 'backpropagation', 'matrices', 'linear-algebra', 'derivatives']"
4725564,Geometric Interpretation of $\sum^\infty_{k=1} \frac{k}{x^k}$,"The series $\sum^\infty_{k=1} \frac{k}{2^k}$ can be solved by taking the derivative of the formula for the geometric series, multiplying by $x$ , and then evaluating it at $x = \frac{1}{2}$ . $$\sum^\infty_{k=1} \frac{k}{2^k} = x \cdot \frac{d}{dx}\left[\frac{1}{1-x}\right] \left|_{x=\frac{1}{2}}\right. = 2$$ However, I am interested if this, like the geometric series, has a nifty geometric argument to go with it such as a $1$ by $2$ rectangle being divided in half infinitely many times for $\sum^\infty_{k=0}\frac{1}{2^k}$ . Basically, what I am asking is if the series can be solved without relying on calculus and sticking to only geometry. If the series above has a geometric interpretation, I am also curious if $\sum^\infty_{k=1}\frac{k^n}{2^k}$ for $n \in \mathbb{N}$ all have geometric interpretations too (note that the sum of each of these can be found by taking the derivative of $\frac{1}{1-x}$ and multiplying by $x$ $n$ times and then plugging in $x=\frac{1}{2}$ ). Any ideas for where to even start looking for geometric arguments are appreciated!","['calculus', 'geometry']"
4725584,Business rules to propositional logic and truth tables,"First I'm not a mathematician, (Ux designer), so please excuse the lack of technical terms. I'm trying to convert the set of potentially conflicted business rules to propositional logic formulas then represent them in a truth table to identify the conflicted literals. In a business production rules change all the time, to simulate this, I created the following conflicted rules: R1: (if clients choose sport package then add Leather Seats and Black Console).
This means if clients choose sport package then the leather seats and black console MUSt be true for this rule to true. Similarly, to get the Leather Seats and Black Console clients must select the sport package!. R2: (if sport package is chosen don't add Leather Seats).
This means if the client chooses sport package then the leather seats should be false!. This rule should be only true when the leather seats are false or not selected! Here is what I did: Truth table with equation I'm not allowed to embed images yet :/ so sorry for the link. (SportPackage -> (BlackConsole and LeatherSeats)) and (SportPackage -> (BlackConsole and not LeatherSeats)) Because the evaluation of rules is linear I put a logical ""&"" between R1 and R2. Yet, Somehow the truth table generated from this equation is not making any sense to me.
I read somewhere that the ""if...then"" construction is supposed to be translated to implication -> yet the table shows the equation to be true even if the ""Sport package"" is not true! :( The purpose: I'm trying to show that the literal not LeatherSeats let's call it L2 from R1 and L2 from R1 are conflicted in a truth table.
What I expected to see is a total ""falsum"" or ""contradiction"" because rule one says the sport package provide the Leather Seats and rule two says clients can't have it. :( As I have limited knowledge of this topic I wanted to ask for your help :) Edit: Is the formula that I wrote optimal? Or can it be shortened? In attempt to isolate the conflict causing literal, I played around with the formula and came up with two versions please let me know if these are semantically as well syntactically correct: i tried to simplify the above mentioned formula to this (SportPackage -> ((BlackConsole and LeatherSeats) and (BlackConsole and not LeatherSeats))) then (SportPackage -> (BlackConsole and (LeatherSeats and not LeatherSeats))) To my naive eye the table of these looked the same as above yet im not sure if this formula transformation change the semantic meaning. Could you please validate it? Thanks in advance 😊.","['propositional-calculus', 'predicate-logic', 'logic', 'discrete-mathematics']"
4725592,I can't comprehend Stewart's explanation for the precise definition of limit and it's driving me nuts,"I'm going through a Calculus class for the third time, and I understand everything very basically. I'm able to solve simple limits, derivatives, and integrals, but I can never really comprehend what I'm doing and the logic behind it. I'm on my third Calculus book, and I chose Stewart because I heard it's simpler. I couldn't get past the properties of numbers in the previous ones. I can (now) understand the entire logic that's going on here. However, I can't understand why he makes some assumptions that allow the logic to happen (not sure if that's the right way to put it). They're not really assumptions, I just don't know why these things appear. For example, he writes: Notice that if $0 < |x - 3| < \frac{0.1}{2} = 0.05$ , then $|f(x) - 5| = |(2x - 1) - 5| = |2x - 6| = 2|x - 3| < 2(0.05) = 0.1$ How did he know delta is 0.05? How does that prove anything about that bunch of equations and inequalities? When I first read this paragraph I though it meant something like Notice that if (I assume that delta is half of epsilon) then (this string of equations and inequalities is true) But a friend explained to me that said string is actually how you arrive at delta. As in, it's a proof of the previous assumption. She rewrote it to me like this and it made sense: $$|f(x) - 5| < 0.1$$ $$|f(x) - 5| = |2x - 1 - 5| = |2x - 6| = 2|x - 3|$$ $$2|x - 3| < 0.1$$ $$|x - 3| < \frac{0.1}{2} = 0.05$$ Now I'm reading Stewart's statement as Notice that if (I assume that delta is half of epsilon) then (this string of equations and inequalities proves that I magically made the correct assumption) I don't know if I'm right to think this statement is contrived. My friend wasn't able to help me past here because she doesn't understand what I'm asking anymore :( Then I got another question, which is how $|x - 3| < \delta$ and $|x - 3| < 0.05$ being true prove that $\delta = 0.05$ . It feels like that merely puts a roof on delta, as if it could be anything below 0.05. What if it's actually 0.005? I believe it's justified by saying that we obtained that number from $\varepsilon = 0.1$ , so their relation are proven enough that for $\delta = 0.05$ it'll never result in an out of bounds epsilon. I just can't make the synapses to visualize that.","['limits', 'calculus']"
4725600,Probability that the next card is the ace of spades,"Say I have a standard $52$ count deck of cards in random order, and that I start flipping cards from the deck over until a king appears, which is card # $19$ . What's the probability the next card is the ace of spades? It's possible to brute force this using Bayes' theorem and slogging through a laborious calculation, but I was wondering if there were any tricks to arrive at the answer in a cleverer way.","['conditional-probability', 'card-games', 'bayes-theorem', 'probability']"
4725605,"$a,b,c\ge 0: a+b+c+abc=4$. Prove that: $\sum_{cyc}\sqrt{\frac{a+b}{c+ab}}\ge 3.$","Problem. Let $a,b,c\ge 0: a+b+c+abc=4$ . Prove that: $$\sqrt{\frac{a+b}{c+ab}}+\sqrt{\frac{b+c}{a+bc}}+\sqrt{\frac{c+a}{b+ca}}\ge 3.$$ I think the problem is nice and very hard. Equality holds iff $a=b=c=1; (0,2,2).$ The idea of using AM-GM directly doesn't work, because $$(a+b)(b+c)(c+a)\ge (a+bc)(b+ca)(c+ab),$$ is wrong when $a=b=\dfrac{3}{2}.$ Also: $$\sum_{cyc}\sqrt{\frac{a+b}{c+ab}}\ge \sum_{cyc}\frac{2(a+b)}{a+b+c+ab},$$ leads to wrong inequality. Is Holder inequality is a good way in this case ? I tried $$\left(\sum_{cyc}\sqrt{\frac{a+b}{c+ab}}\right)^2.\sum_{cyc}(a+b)^2(c+ab)\ge 8(a+b+c)^3,$$ which is not good enough. I think Mixing variables is a good approach but I am not good at using this method. A fact: $$\sqrt[3]{\frac{a+b}{c+ab}}+\sqrt[3]{\frac{b+c}{a+bc}}+\sqrt[3]{\frac{c+a}{b+ca}}\ge 3,$$ is already wrong when $a=b=1.874$ . Hope to see more ideas. Thank you.","['algebra-precalculus', 'uvw', 'inequality']"
4725619,Stokes' Theorem and perimeter,"As a consequence of Stokes' Theorem it seems that the perimeter of a closed curve $C$ can be obtained by choosing $F$ to be the vector field formed by rewriting the unit tangent $T$ of $C$ as a function of position. Let $T$ be rewritten as a vector field on $\Bbb R^3$ : $$
T =  \langle f(x,y,z), g(x,y,z), h(x,y,z) \rangle.
$$ This vector field obtained from $T$ is defined on the surface patch bounded by $C$ if this vector field for $T$ is defined on a subset of $\Bbb R^3$ containing the surface patch bounded by $C$ . Integrating the curl of this vector field over the surface patch whose boundary is $C$ , you get the perimeter (length) of the curve. For example, suppose the surface is the paraboloid $z=x^2+y^2$ and the curve $C$ is the circle of radius $R$ in the $z=R^2$ plane. The unit tangent of $C$ in terms of $x, y$ and $z$ would be $\langle  -y/R , x/R , 0 \rangle$ . Integrating the curl of this vector field over the surface patch whose boundary is $C$ , you get $2\pi R$ . I've verified it works on a lot of examples.
Analogously, Green's theorem can also be used to find perimeter. Simply find $f$ and $g$ for $T = \langle f(x,y), g(x,y) \rangle $ . Is this a common application of Stokes' Theorem / Green's Theorem? Can anyone attest to ever having heard of it before? The only reasons I have for believing this actually works are that the math loosely seems to make intuitive sense and that it didn't produce any contradictions when I tested it on examples. And when I say that the math loosely seems to make intuitive sense, I mean that the length of a curve is equal to the line integral of the unit tangent over the curve.","['greens-theorem', 'multivariable-calculus', 'stokes-theorem', 'vector-analysis', 'differential-geometry']"
4725638,Metric space arc-connectedness,"I need hints to prove the implication “path-connected implies arc-connected” for metric spaces? [from Pugh chapter $2$ exercise $75$ ] For simplicity, let me just call, given a path $f : [0,1] \to M$ , a pair of points $(a,b)$ such that $f(a)=f(b)$ for distinct $a$ and $b$ an intersection point. This exercise problem seems trivial enough if the number of intersection points is only finite. We can just remove all the inbetween image of intersection points $f( (a,b) )$ and get a homeomorphism. However, I cannot seem to tackle the case where there are an infinite number of intersection points. I tried doing stuff like considering the supremum and infimum of the set of intersection points but that led nowhere because of some strange cases where the path is just very ugly/not convenient. Pugh says this is only $1$ star difficulty which is why I feel like I’m just missing something. A hint in the right direction ( or even a complete proof ) would be very much appreciated. Thanks","['general-topology', 'path-connected', 'metric-spaces', 'real-analysis']"
4725645,What is the Bernoulli product measure's Radon-Nikodym derivative wrt Lebesgue measure?,"The Bernoulli product measure $\mu$ can be defined for each $p\in (0,1)$ on $\Omega = \{0,1\}^\mathbb N=\{\omega=(\omega_i)|\omega_i\in\{0,1\}, i\in\mathbb N\}=\Pi_{i=1}^\infty \{0,1\}$ .  The measure $\mu$ is essentially defined by a collection of Bernoulli random variables $X_i:\Omega\to\{0,1\}$ which take the value $1$ with probability $p$ and $0$ with probability $1-p$ depending only on the $i$ th coordinate of $\omega\in\Omega$ .  The pre-images $X_i^{-1}(0)$ and $X_i^{-1}(1)$ are examples of cylinder sets in the product space and generate the $\sigma$ -algebra that the product measure $\mu$ is defined on. We can then define a map $\Omega\to [0;1]\subset\mathbb R$ by $\omega=(\omega_i)\mapsto \sum_{i=1}^\infty \omega_i 2^{-i}$ .  Up to a set of Lebesgue measure $0$ having to do with duplicate  binary expansions of the dyadic rationals $k/2^n$ ( $k,n\in\mathbb N$ ), this map identifies $\Omega$ with the unit interval $[0,1]\subset\mathbb R$ .  The duplicate issue is that dyadic rationals have both an infinite and a terminating binary expansion (in binary, $1/2=0.1=0.0\bar 1$ ), so the map is not a perfect bijection on that countable set. We now have two measures on $[0;1]$ , $\mu$ and Lebesgue measure $\cal L$ .  They are defined on slightly different $\sigma$ -algebras since $\cal L$ is defined on the Borel sets and $\mu$ is defined on the $\sigma$ -algebra generated by the sets that make the random variables $X_i$ measurable (generated by the cylinder sets). Is there anything like a Radon-Nikodym derivative of $\mu$ with respect to $\cal L$ even though they are not defined on the same $\sigma$ -algebras?  For example, is it possible to extend or intersect the $\sigma$ -algebras to obtain a useful integral formula with an actual density function so that we could write $$\mu=\nu + \int f d\cal L$$ on some meaningful collection of sets? ( $\nu$ a measure singular wrt $\cal L$ ) I've looked in a number of books including Billingsley, Chung, Folland, Rudin, Durrett, Cohn, and others.  They discuss the construction of $\mu$ , the differences of the $\sigma$ -algebras that $\mu$ and $\cal L$ are defined on, but I haven't been able to find any kind of expression of one with respect to the other, if that's possible in some way. My motivation is to get a better understanding of $\mu$ in more standard analytical terms. PS: I originally posted this question over in https://stats.stackexchange.com/questions/619732/what-is-the-bernoulli-product-measures-radon-nikodym-derivative-wrt-lebesgue-me but it was recommended by a commenter that I post it here instead.","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'bernoulli-distribution', 'radon-nikodym']"
4725688,Approximation of $ \frac{1 - a x^2} {1 - b x^2} $ for small $x$,"I am working with the following function: $$\frac{1 - a x^2} {1 - b x^2} $$ I am studying it for small values of $x$ , which are between $0$ and $0.25$ and $b$ sligtly smaller (or slightly larger) than $a$ . Plotting this function, I see clearly that it can be approximated by a quadratic form that is dependent on $a - b$ . Is there a known approximation for such a function?","['taylor-expansion', 'functions', 'approximation', 'polynomials']"
4725712,Doubt on the definition of heat kernel,"The heat kernel of a compact Riemannian manifold is the only smooth function $k=k(t,x,y):\mathbb{R}_{>0}\times M \times M\to \mathbb{R} $ such that $k(\cdot,x,\cdot)$ is a solution to the heat equation with initial distribution $k(0,x,\cdot)$ is the Dirac delta centered in $x$ . (By $k(0,x,\cdot)$ , I mean the limit in the sense of distribution of $k(t,x,\cdot)$ as $t\to 0^+$ ). Since distributions act on the space of test functions $C^\infty_c(M)=C^\infty(M)$ , the condition $k(0,x,\cdot)=\delta_x$ should mean that $$\lim_{t\to 0^+}\int_M k(t,x,y)f(y)=f(x)$$ for any $f\in C^\infty(M)$ . However, many important references like this one and this one , require an (apparently) stronger condition. The first reference requires the equality above to hold for any $f\in L^2(M)$ and the second one for any $f\in C^0(M)$ . Are these three requests all equivalent?","['riemannian-geometry', 'heat-equation', 'analysis', 'distribution-theory', 'lp-spaces']"
4725811,Estimate unknown parameter by maximum likelihood method and moment's method,"There is a random sample $X_1, X_2, ..., X_n$ distributed with: $ x $ -3 0 3 $ P(X = x) $ $ \frac{1}{3} - \theta $ $ \frac{1}{3} + 2 \theta $ $ \frac{1}{3} - \theta $ Estimate unknown parameter by maximum likelihood method and moment's method. Are these estimates unbiased and consistent? Moments: $ L(\theta) = (\frac{1}{3} - \theta)(\frac{1}{3} + 2\theta)(\frac{1}{3} - \theta) = \frac{1}{27} + 2\theta^3-\theta^2 $ $ L'(\theta)=6\theta^2-2\theta=0$ $ \theta = 0, \theta = \frac{1}{3}$ Maximum likelihood: $ EX = -3(\frac{1}{3} - \theta)+0(\frac{1}{3}+2\theta)+3(\frac{1}{3}-\theta)=0 $ $ EX^2 = (-3)^2(\frac{1}{3} - \theta)+0^2(\frac{1}{3}+2\theta)+3^2(\frac{1}{3}-\theta) = 6 - 18\theta $ $ DX = (EX)^2 - EX^2 = 6-18\theta $ $ \sum_{i=1}^{3} (x_{avg}-x_{i})^2 = (\frac{1}{3}-\frac{1}{3}+\theta)^2 + (\frac{1}{3}-\frac{1}{3}+2\theta)^2+(\frac{1}{3}-\frac{1}{3}+\theta)^2=6\theta^2 $ $ 6\theta^2 = 6 - 18\theta$ $\theta = -3.3$ or $ \theta = 0.3 $ If I'm correct, what next should be in testing unbiasedness and consistency?","['statistics', 'probability-distributions', 'maximum-likelihood', 'probability-theory', 'probability']"
4725836,question on integer polynomial sequences,"Let $f:{\mathbb Z}\to {\mathbb Z}$ be a degree $d$ polynomial, and for $c\in{\mathbb Z}$ set $S_c=|\{n\in{\mathbb Z}:f(n)=c\}|$ . It is easy that $S_c\le 2$ for all but finitely many $c$ (but not more, as $f$ could be a translated even polynomial), also clearly $S_c\le d$ for all $c$ . Can one say something more, like, $\sum_{S_c>2}S_c=O(d)$ , at least can the number of $c$ with $S_c>2$ be bounded in terms of $d$ ? Has there been any related research? Thanks a lot.","['number-theory', 'polynomials', 'integer-sequences']"
4725841,A bead is moved 6 times from vertex to vertex on a steel-wire cube ; how many different ways to come back to the starting vertex?,"Consider an cubical arrangement of steel wire (edges made up steel wire) and a bead is on this wire (one can slide this bead across wire).A person performing a game in which there are 6 steps, in each step the bead is moved from one vertex to another adjacent vertex. In how many different games the final and initial positions of the beads are same. I tried to make cases, that $x$ denotes displacement in $x$ axis, respectively $y$ , $z$ . $$\sum^a{x}+\sum^b{y}+\sum^c{z}=0$$ where, $$a+b+c=6$$ And took $28$ cases, since it was symmetric it doesn't took more than half a hour but and gave answer 191 (my accuracy isn't good my answer might not be true). I am completely exhausted after this counting? Can someone help me if finding a good solution for this?","['permutations', 'combinations', 'combinatorics', 'contest-math']"
4725845,"Proving $\mu=\nu$ or $\mu\perp\nu$ if $\mu,\nu$ are two ergodic probability measures on $(X,B)$ with dynamics $T$ without Birkhoff's Ergodic Theorem","Let $(X,B)$ be a measure space, $T:X\to X$ be a measurable transformation and $\mu,\nu$ be two ergodic probability measures on $(X,B)$ . I am trying to prove that either $\mu=\nu$ or $\mu\perp\nu$ without the help of the Birkhoff Ergodic Theorem. Currently I am stuck at concluding that if we assume that $\mu(A)=\nu(A)$ for all such $A\in B$ that $A = T^{-1}(A)$ , then either $\mu=\nu$ or there exists some $A\in B$ such that $A\neq T^{-1}(A)$ and $\mu(A)=1,\nu(A)=0$ or the other way around. To be precise, by defining $\eta(A)=\frac{1}{2}(\mu+\nu)$ , if $\eta(A)=\frac{1}{2}$ for some $T$ -invariant $A\in B$ , then $\mu$ and $\nu$ are singular. Currently I am dealing with the case that for all $T$ -invariant sets $A\in B$ , $\eta(A)=0$ or $\eta(A)=1$ . But as of writing I am not seeing how to either construct such a concentrated set $A$ or to extend the equality between $\mu$ and $\nu$ on the $T$ -invariant sets $A\in B$ .","['measure-theory', 'ergodic-theory', 'real-analysis', 'probability-theory', 'dynamical-systems']"
4725869,Convergence of multiple iterated series,"I am searching for sources or references for this proposition. Can anyone help me? Thank you in advance. Let $A_{n_1,n_2,...,n_k}$ be multiple sequence on $\mathbb{C}$ and $\phi: \mathbb{N} \rightarrow \mathbb{N}^k$ be bijection. If $\sum_{n=1}^\infty|A_{\phi(n)}|$ converge, then $\sum_{n_1=1}^\infty\sum_{n_2=1}^\infty\cdots\sum_{n_k=1}^\infty A_{n_1,n_2,...,n_k}=\sum_{n=1}^\infty A_{\phi(n)} $ In fact, I have already proven it using my own method. However, I would like to reference this proposition in my article. If there are existing documents that have presented the proof of this proposition, I would be able to reference them without wasting space in the article. Do you happen to have any references? Thank you in advance.","['multivariable-calculus', 'sequences-and-series']"
4725883,Show by induction that a function is bounded,"My question is about the proof of the corollary 3.3 page 21 of this paper . Let $(E,d,\mu)$ be a measured metric space with $\mu$ doubling.
We write $\mathcal{M}$ the Hardy-Littlewood maximal operator.
Let $1<q\leq\infty$ and $a>1$ be fixed.
Let $F,G\in L^1_{loc}(E)$ be non-negative. Definition : We says $(F,G)\in\mathcal{E}_{q,a}$ if for every ball $B$ we can find
non-negative measurable functions $G_B, H_B$ defined on $B$ with $$ F
 \leq G_B + H_B ~ \text{a.e. on}\, B $$ such that $$ \begin{split}
 \left( \frac{1}{\mu(B)}\int_B (H_B)^q\, d\mu \right)^{1/q} &\leq a
 \inf_{x\in B} \mathcal{M} F(x) + \inf_{x\in B} G(x), \\
 \frac{1}{\mu(B)}\int_B G_B\, d\mu &\leq \inf_{x\in B} G(x).
 \end{split} $$ Let $1<\rho$ and let $F,G\in\mathcal{E}_{q,a}$ such that $\|F\|_1<\infty$ and $\|G\|_\rho<\infty$ .
In this proof we define a function $$
\Phi(t) = \rho\int_0^t \lambda^{\rho-1}\mu\{\mathcal{M}F>\lambda\}\,d\lambda
$$ for $t\geq 0$ .
Also after using a precedent proposition we show the innequality $$
\Phi(Kt) \leq
\frac{1}{2} \Phi(t) + \left( \frac{K}{\gamma} \right)^\rho \lVert G \rVert_\rho^\rho
$$ where $K$ and $\gamma\leq 1$ are some constants.
And then the author says that ""An easy iteration shows that $\Phi$ is bounded"".
I don't see what kind of iteration we have to do.","['proof-explanation', 'induction', 'functional-analysis', 'real-analysis']"
4725889,Intuition of quotient rings of polynomials,"Let $k$ be a field, $R=k[x_1,\ldots, x_n]$ , $V$ an algebraic set and $I:=I(V)$ the ideal formed by the polynomials that vanish on all points of $V$ . I am trying to get a firm intuition of what the quotient $R/I$ is. I got told that this quotient is exactly the set of polynomial functions on $V$ . To start with, I don't see why this is true. I mean, if I write down some examples I kind of see how this can be true but I don't have an intuition of why it happens; I hope someone can help me with this. Now let's write down a couple of examples. For the first one, I'll choose $k=\mathbb{C}$ and $n=2$ . I am therefore working in $\mathbb{A}_\mathbb{C}^2$ . I will take $V= \{y=0\} \cong \mathbb{A}_{\mathbb{C}}^1$ . The quotient $\mathbb{C}[x,y]/I\cong \mathbb{C}[x]$ and I can understand that the set $\mathbb{C}[x]$ is exactly the set of polynomial functions on $V=\mathbb{A}_{\mathbb{C}}^1$ . However, if now I write $V'= \{y=1, y=-1\}$ this is like two instances of $\mathbb{A}_{\mathbb{C}}^1$ . However, when taking the quotient $\mathbb{C}[x,y]/I(V')$ where $I(V')=(y^2-1)$ , I get that this is again $\mathbb{C}[x]$ , and I don't see that in this case this is the set of polynomial functions on two instances of the affine space. Where am I going wrong?","['algebraic-geometry', 'commutative-algebra', 'quotient-spaces']"
4725892,growth rate of the sides of a triangle,"I got into this problem: In an isosceles triangle $ABC$ the vertex $C$ moves perpendicular to the base $AB$ so that the area of the triangle grows at a speed of $4 cm^2/s$ . The base $AB$ is $3 cm$ long. At what rate does the height $CH$ grow? What about the $CB$ side? I solved the first question using derivatives, knowing that: the area of a triangle measures $A =\frac{1}{2} \cdot AB \cdot CH$ the base stays constant in time So we get that: $\frac{dA}{dt} = \frac{3 \ cm}{2} \cdot \frac{dCH}{dt}$ $\frac{dCH}{dt} = 4 \frac{cm^2}{s} \cdot \frac{2}{3 \ cm}$ $\frac{dCH}{dt} = \frac{8 \ cm}{3 \ s}$ For the second question i thought i could use pythogorean theorem on the triangle $CHB$ , getting: $CB = \sqrt{\frac{1}{4} {AB}^2 + {CH}^2}$ but im not sure on how to get $\frac{dCB}{dt}$ .","['calculus', 'derivatives']"
4725927,Prove $\frac{Z_1+Z_2+..Z_n}{\log(n+1)}\rightarrow1$ a.s when $Z_n\sim \mathrm{Poi}(\lambda _n)$,"Let $Z_1,Z_2,..$ be independent r.v s.t $Z_n \sim \mathrm{Poi}(\lambda _n)$ and $\lambda _n = \log (n+1)-\log n$ (log with base 2). Let $T_n=Z_1+Z_2+...+Z_n$ Prove $\frac{T_n}{\log(n+1)}\rightarrow1$ a.s hint: first prove convergence in probability. This is a question from a test in probability theory. I havn't found the definition for convergence in probability in the net so I hope I translated it right. This is the definition: $X_n\overset{\mathbb{P}}{\rightarrow}X$ if for all $\epsilon>0 :$$\ \ \ \mathbb{P}(|X_n-X|>\epsilon)\rightarrow 0$ I know there is a solution using Kronecker’s lemma and the strong law under condition of second moment. But I want to know what's the solution using the hint. Thank you,","['convergence-divergence', 'probability-theory', 'poisson-distribution']"
4725934,Show that $\operatorname{int}{(A \times B)} = \operatorname{int}(A) \times \operatorname{int}(B)$,"The question goes as follows: Let $(X, \mathcal{T})$ and $(Y, \mathcal{U})$ be topological spaces. Let $A$ and $B$ be subsets of $X$ and $Y$ respectively. Show that $\operatorname{int} (A \times B) = \operatorname{int}(A) \times \operatorname{int}(B)$ . Here's my proof of the above: The interior of a set $G$ is defined as $$
x \in \operatorname{int}(G) \iff \exists \text{ an open set $U$ containing $x$ such that $U\subseteq G$}
$$ Let $(x,y) \in \operatorname{int}(A \times B)$ . This means there exists an open set $A_1 \times B_1$ containing $(x,y)$ such that $A_1 \times B_1 \subseteq A \times B \Rightarrow A_1 \subseteq A$ containing $x$ and $B_1 \subseteq B$ containing $y$ . Thus $(x, y) \in \operatorname{int}(A) \times \operatorname{int}(B)$ . Conversely, if $(x, y) \in \operatorname{int}(A) \times \operatorname{int}(B)$ , then there exists open sets $A_1$ and $B_1$ containing $x$ and $y$ such that $A_1 \subseteq A$ and $B_1 \subseteq B$ . So, $A_1 \times B_1$ is a open set in $A \times B$ containing $(x, y)$ . Thus, $(x, y) \in \operatorname{int}(A \times B)$ . I am not quite sure about the correctness of my proof. So I thought I would check it. Thanks in advance.","['general-topology', 'solution-verification', 'product-space']"
4726008,Sum of normal densities,"Suppose $Y$ is a random variable whose distribution is completely known. For $i=1,\dots,d$ , $\beta_i$ are positive constants, $g_i(\cdot)$ are exponential functions with the form $g_i(x) = \exp(a_i(x-b_i)^2 + c_i)$ , where $a_i,b_i,c_i$ are all constants (can be positive or negative). We also have $\sum_{i}\beta_i = 1$ . I am wondering if there is any way to give an analytical form (in terms of distribution of $Y$ ) for the probability of the event $\{\sum_{i=1}^d \beta_i g_i(Y)\geq\beta_1g_1(Y)\}?$ Say $Y$ follows some normal distribution. We can easily give a closed form for the probability of the event above for $d\leq2$ . But I don't know how to do so for $d=3$ ; discussing the convexity/monotonicity and searching for equilibriums seem to be too complicated. I would greatly appreciate it if any advice can be given.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4726017,An integral identity: $\int_{0}^{\large\frac{π}{2}}\dfrac{\cos^{a}x\sin(a+1)x}{\sin x}\ \mathrm{d}x$,"I recently came across a parametric definite integral about Chebyshev polynomials: $$ f(a)= \begin{aligned}
\begin{gather*}
\int_{0}^{\frac{\pi}{2}}\dfrac{\cos^{a}x\sin(a+1)x}{\sin x}\ \mathrm{d}x
\end{gather*}
\end{aligned} $$ Easily, I found out the form of Chebyshev polynomials of the Second kind and guessed that the integration result might be a constant or something. We know that Chebyshev polynomials of the Second kind satisfies the following differential equation: $$ \begin{aligned}
\begin{gather*}
(1-x^2)\ U''_{a}(x)-3x\ U_{a}'(x)+a(a+2)\ U_{a}(x)=0
\end{gather*}
\end{aligned} $$ Let $\ \displaystyle U_{a}(x)=\sum_{k=0}^{\infty}{u_{k}\ x^k}$ , we can get that $\displaystyle u_{k+2}=\dfrac{(k-a)(k+a+2)}{(k+1)(k+2)}u_{k}$ , which means that $$ \begin{aligned}
\begin{gather*}
\displaystyle U_{a}(x)=\sum_{k=0}^{\infty}{\dfrac{(-2)^k\ \Gamma\left({\large\frac{k+a}{2}}+1\right)\Gamma\left({\large\frac{k-a}{2}}\right)}{\Gamma(k+1)}x^k}
\end{gather*}
\end{aligned} $$ MeanWhile， $$ \begin{aligned}
\begin{gather*}
\displaystyle \int_{0}^{1}{\dfrac{x^u}{\sqrt{1-x^2}}\ \mathrm{d}x}=\dfrac{\sqrt{π}\ \Gamma({\large\frac{u+1}{2}})}{2\ \Gamma({\large\frac{u}{2}}+1)},\ u>-1
\end{gather*}
\end{aligned} $$ Finally, it ends up with one series following: $$ \begin{aligned}
\begin{gather*}
-\dfrac{\sin(πa)}{2π}\sum_{k=0}^{\infty}{\dfrac{\sqrt{π}\ (-2)^k\ \Gamma\left({\large\frac{k-a}{2}}\right)\Gamma\left({\large\frac{k+a+1}{2}}\right)}{2\ \Gamma(k+1)}}
\end{gather*}
\end{aligned} $$ I have been going at this Series all day and I am confused on where to begin.Thoughts and several Solution ideas are greatly appreciated !","['definite-integrals', 'chebyshev-polynomials', 'special-functions', 'complex-analysis', 'contour-integration']"
4726051,"How to get probability of $X \le Y + Z$ for $X, Y, Z$ all from the same specific distribution?","I'm struggling to get the probability of $\mathbb{P}[X\le Y+Z]$ where all three are from the same distribution with the following pdf: $$f_X (x) = \frac{2(x-a)}{(b-a)^2}$$ where $0<a<b$ .  (Notice the pdf above comes from taking the maximum of two random variables uniformly distributed between $a,b$ ). However, I know that I need to use convolution to compute $\mathbb{P}[X\le Y+Z]$ where $X,Y,Z$ are i.i.d.  But I admit that I'm stuck on this convolution step.  How do I convolve these and get $\mathbb{P}[X\le Y+Z]$ ?","['statistics', 'convolution', 'probability']"
4726061,Cardinalities and bijections (naive set theory),"Note that a set $A$ is said to be extremely infinite iff for each set $a$ : $|A\cup\{a\}| = |A|$ . Let $\alpha$ be a set. Prove that if $\alpha$ is extremely
infinite, then there exists some $\beta \subset \alpha$ (in particular $\alpha \neq \beta$ ) s.t $|\alpha| \leq |\beta|$ Since we should define such an injective and we only know that one property about $\alpha$ , I assume that I just don't see it. I tried to use a diagram to simplify the problem however it just made it more vague. Wish to get some orientation, thanks.","['elementary-set-theory', 'cardinals']"
4726074,A group is profinite if and only if it is a Stone space,"A profinite group is an inverse limit of an inverse system of discrete finite groups. Alternatively, a profinite group is a topological group that is also a Stone space. Under the second, axiomatic definition it's obvious that a profinite group $G$ is a Stone space. My professor made the claim that a group $G$ is profinite if and only if $G$ is a Stone space. The forward implication, as I mentioned, seems trivial. I need some help figuring out what is meant to be shown in the opposite direction. It is true that a topological space $X$ being a Stone space is equivalent to $X$ being homeomorphic to a projective limit of an inverse system of finite discrete spaces. My thought is that the reverse implication means to show that if $G$ is a Stone space, then the finite discrete spaces in the projective limit may be taken to be finite discrete groups. If this is the case, how would someone go about proving such a thing? Otherwise, can someone lay out this duality in a more sensible way, and indicate the direction to go for studying profinite groups, and more generally pro-objects in a category?","['infinite-groups', 'profinite-groups', 'category-theory', 'group-theory', 'general-topology']"
4726086,Existence and uniqueness of the solution of a control system,"Let $T>0$ , $(U,d)$ be a metric space and $\mathcal{V}:=\{u:[0,T]\to U\,|\, u\text{ is measurable}\}$ . Consider the control system $$\begin{cases}\dot{x}(t)=f(t,x(t),u(t)),\quad \text{a.e. }t\in[0,T],\\x(0)=x_0,\end{cases}$$ where $f:[0,T]\times\mathbb{R}^n\times U\to \mathbb{R}^n$ is measurable. According to [1, P. 102], for any $u\in\mathcal{V}$ , the above control system has a unique solution $x:[0,T]\to\mathbb{R}^n$ if the following conditions hold: $U$ is a separable metric space. There exist a constant $L>0$ and a modulus of continuity $\omega: [0,\infty) \to [0,\infty)$ such that 2.1) $|f(t,x,u) - f(t,\hat{x},\hat{u})|\leq L|x-\hat{x}|+\omega(d(u,\hat{u})),\quad \forall t\in[0,T],x,\hat{x}\in \mathbb{R}^n,u,\hat{u}\in U,$ 2.2) $|f(t,0,u)|\leq L,\quad \forall (t,u)\in[0,T]\times U.$ My question is about the proof of this statement ([1] does not provide the proof). Can one extend the standard Picard–Lindelöf theorem to prove it? specially, since condition 2.1 is basically a Lipschitz condition for $f$ . I cannot also see what role separability of $U$ and the condition 2.2 play in the existence and uniqueness of the solution. Any help or hint with the proof is appreciated. [1] Yong, Jiongmin; Zhou, Xun Yu , Stochastic controls. Hamiltonian systems and HJB equations, Applications of Mathematics. 43. New York, NY: Springer. xx, 438 p. (1999). ZBL0943.93002 .","['lipschitz-functions', 'control-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4726096,Is it possible to prove that the polynomial $x^4 - 2x^3 + 16$ doesn't have a real zero by writing it as a sum of squares?,"To be able to apply the argument principle to a certain problem, I have to prove that the polynomial $x^4-2x^3+16$ doesn't have a real zero. I think that I can do this by a non-satisfactory, derivative technique: The value of the polynomial seems to go to plus infinity as x approaches plus or minus infinity. Therefore there must be a globally minimal value at a critical point. Critical points are 0 and 3/2, where the value is positive. Therefore the polynomial can't have a real zero. A more satisfactory proof would be writing the polynomial as a sum of squares, but that seems difficult. I've tried to get rid of the disturbing $-2x^3$ term by writing the polynomial as $(x^2-x)^2 - x^2 + 16$ , but then an equally disturbing $-x^2$ term poppes up. What to do?","['calculus', 'polynomials']"
4726127,Matrix of rotation in $\mathbb R^{3}$,"I'm attempting to teach myself basic multivariable calculus and so far finding some of the 3d visualizations a bit difficult. Here's an example: Let $f: \mathbb R^{3} \longrightarrow \mathbb R^{3}$ be the rotation by $\pi$ about the line $x_1 = x_2$ and $x_3 = 0$ in the $x_1x_2$ plane. What is the matrix of $f$ with respect to the standard bases? I suppose the line is the intersection of the plane $x_1 = x_2$ with plane $x_3 = 0$ . However, it's unclear to me just what thing (for lack of a better term) is being rotated when we say ""rotated about such-and-such in the so-and-so (plane, etc)"". Silly question, but what would be the difference from just rotating the axes $x_1$ and $x_2$ themselves by $\pi$ , vs. what is specifically asked to be rotated in this problem? The more images and visual intuitions one can offer, the better. Thank you.","['multivariable-calculus', 'linear-algebra', 'linear-transformations', 'rotations']"
4726142,Prove or disprove $\det(6(A^3 + B^3 + C^3)+(A+B+C)^3) \ge 5^2\det(A^2 + B^2 + C^2)\cdot\det(A + B+ C)$,"Let $A, B, C$ be $2\times 2$ Hermitian positive semidefinite (PSD) matrices. Prove or disprove that $$\det(6(A^3 + B^3 + C^3)+(A+B+C)^3)
\ge 5^2\det(A^2 + B^2 + C^2)\det(A + B+ C).$$ This problem is inspired by these two problems: P1 , P2 .
When I tried to deal with P2, I want to
eliminate the constraint $A+B+C = I_2$ .
So I propose this problem. I just do some numerical experiments and do not find an counterexample yet. We can use the fact that for $2\times 2$ matrix $S$ , $$\det S = \frac12 (\mathrm{tr}(S))^2 - \frac12\mathrm{tr}(S^2).$$ An idea is to use $A = u_1u_1^H + u_2u_2^H, B = v_1v_1^H + v_2v_2^H,
C = w_1w_1^H + w_2w_2^H$ .
Then we have a unconstrained optimization
of $f(u_1, u_2, v_1, v_2, w_1, w_2)$ . I also consider the following: Let $A, B$ be $2\times 2$ Hermitian positive semidefinite (PSD) matrices. Prove or disprove that $$\det(6(A^3 + B^3 + I_2)+(A+B+I_2)^3)
\ge 5^2\det(A^2 + B^2 + I_2)\det(A + B+ I_2).$$","['determinant', 'matrices', 'hermitian-matrices', 'linear-algebra', 'inequality']"
4726212,Strange distribution of movie ratings,"I like math but I also like movies. I have been collecting movies all my life. My collection is rather huge: almost 25.000 movies. Being also a developer I was able to create my own online catalogue and pull various statistics from the database. There is one thing that puzzles me. Movies have ratings and I did not invent mine: I have copied them from IMDb. As you probably already know, IMDb ratings go from 1 to 10, with 1 being the lowest. I have created a histogram representing ratings distribution and it looks like this: I expected to see something like normal distribution, but my histogram has a funny dip around rating 7.0. Is this a known phenomenon in statistics? Has anyone seen something like this in other data?","['statistical-inference', 'statistics', 'descriptive-statistics']"
4726241,How to calculate the eigenvalues of an operator?,"I am trying to determine the eigenvalues of the following operator. $T:L^2[0,1] \rightarrow L^2[0,1]$ , $Tf(x)=\int_0^1(2xy-x-y+1)f(y)dy$ The Eigenvalues are $\rho_p(T):=\{\lambda: \lambda-T \text{ is not injective}\}$ My approach:
Let $\lambda \in \mathbb{R}$ , $(\lambda-T)f(x)=\lambda f(x)- Tf(x)=\lambda f(x)-\int_0^1(2xy-x-y+1)f(y)dy$ My idea would be not to set $\lambda f(x)- Tf(x)=\lambda f(x)-\int_0^1(2xy-x-y+1)f(y)dy=a$ for $a \neq 0$ and trying to solve it for $\lambda$ . But there comes my next problem, that I don't know how to handle the Integral. Do I just set $F'=f$ , and do formal calculation?",['functional-analysis']
4726243,multiple integral on domain,"The Question : Calculate $$\int_A z \ dx \ dy \ dz$$ Where $A$ is the cone : $\ A=\{0\leq z \leq 1,\ z^2\geq x^2+y^2 \}$ My try : I first try to establish the bounds. $z$ is from $0$ to $1$ . Once $z$ is fixed, I changed for polar coordinates and so: $z\geq r$ . We also know that a radius is positive so I deduced $r$ is from $0$ to $z$ . I finally set $\theta$ from $0$ to $2\pi$ because my cone is a full one I guess. It gives me : $$\int_0^1\left(\int_0^{2\pi} \left( \int_0^z zr\ dr \right) d\theta \right)dz$$ $$=\int_0^1\left(\int_0^{2\pi} \frac{z^3}{2} d\theta \right)dz$$ $$=2\pi \int_0^1 \frac{z^3}{2} dz$$ $$=\frac{2\pi}{8}$$ I have the feeling something is wrong, mostly when I find the bounds (the deductions about the bounds of $\theta$ and $r$ ). The difficulty for me in this case is that it's very difficult to have a visual interpretation. My question: Is my bounds searching process good or am I in the wrong direction ?","['integration', 'multivariable-calculus', 'multiple-integral']"
4726255,Resource request on algebraic number theory,"I studied number theory during my master's degree; however, after pursuing my PhD in mathematical logic, I found that my knowledge of number theory has significantly faded. Now, I am interested in rekindling my passion for this field, specifically in diophantine equations rather than prime numbers. Could you please provide me with recommendations for reference books, seminal works, or any other resources that would be beneficial for someone looking to delve back into number theory, particularly focusing on diophantine equations?","['number-theory', 'diophantine-equations', 'reference-request']"
4726266,"6 children are sitting on a merry-go-round, in how many ways can you switch seats so that no one sits opposite the person who is opposite to them now?","I'm preparing for an exam and I would appreciate your help if you can tell me if there's a mistake in my solution. Question $6$ children are sitting on a merry-go-round: Now, we need to change their seats s.t kid $1$ doesn't sit opposite kid $4$ , kid $2$ doesn't sit opposite kid $5$ and kid $3$ doesn't sit opposite kid $6$ . $A_1 $ is the set of all their ways to sit in a circle s.t kid $1$ sits opposite kid $4$ . $A_2 $ is the set of all their ways to sit in a circle s.t kid $2$ sits opposite kid $5$ . $A_3 $ is the set of all their ways to sit in a circle s.t kid $3$ sits opposite kid $6$ . $U$ is the set of all their ways to sit in a circle. We need to count $U\setminus (A_1 \cup A_2 \cup A_3)$ . $$U\setminus (A_1 \cup A_2 \cup A_3)= 5! - 3 \cdot 4! -3 \binom{3}{2}\cdot 2\cdot 2-2^3=5!-28 = 92$$ Explanation: The $6$ children are sitting on a merry-go-round. That defines an order on them. I use symmetric inclusion and exclusion to solve the problem. I get confused while trying to calculate $A_1$ for exapmle (I think the first way is correct)","['inclusion-exclusion', 'combinatorics']"
4726273,Coefficient of a falling factorial,"Let $a_0,\dots, a_{15} $ be such that $\sum_{i=0}^{15} a_ix^{\underline{i}} = \sum_{i=0}^{15} x^i$ . Note that underline means that it's a falling factorial. What is $a_{12}$ ? My attempt: obviously we can just compute $x^{\underline{12}}, \dots, x^{\underline{15}}$ and then find $a_{12}$ but that would take a lot of time without a computer. Is there any other way?","['summation', 'combinatorics', 'discrete-mathematics']"
4726285,Can one find an injective-matrix-valued approximation of a given matrix-valued continuous function?,"This question is basically an extension of a previous question by replacing a matrix with a continuous matrix-valued function: Given a matrix $A \in \mathbb{R}^{m \times n}$ ($m > n$), can one always find an injective matrix $B$ close to $A$? Suppose that $X \subset \mathbb{R}^{n}$ is compact and $U \subset \mathbb{R}^{m}$ is convex compact. A continuous matrix-valued function $A: X \to \mathbb{R}^{l \times m}$ is given with $l > m$ .
Given $\epsilon > 0$ , can one always find an injective-matrix-valued continuous function $B: X \to \mathbb{R}^{l \times m}$ such that for any $x \in X$ , $\lVert A(x)-B(x) \rVert < \epsilon$ and $u_{1} \neq u_{2}$ $\Rightarrow$ $B(x)u_{1} \neq B(x)u_{2}$ ? Note from the comments of the previous question that one can construct an injective matrix $B$ for a given matrix $A$ by singular value decomposition. My understanding is something like this: SVD of $A \in \mathbb{R}^{l \times m}$ ; $A = U \Sigma V^{*}$ .
Define $B := U \left( \Sigma + t\begin{bmatrix} I \\ 0 \end{bmatrix} \right) V^{*}$ , which satisfies the requirement of the previous question with arbitrarily small $t \in \mathbb{R}_{>0}$ . However, this approach is not easily extended for this question as the SVD of $A(x)$ is not uniquely determined in general for given $x \in X$ and it seems hard to guarantee that there exists $U(x)$ , $V^{*}(x)$ , and $\Sigma(x)$ continuous w.r.t. $x$ .","['matrices', 'svd', 'approximation']"
4726296,Can we apply $\dfrac{ \sin ( u ) }{ u } = 1$ when ${ u }$ approaches $0$ even when there are other variables in the sin function?,"I have an equation: $$
  \displaystyle\lim_{ δx   \rightarrow  0  }   \dfrac{  \sin \left(\left(  x+ \dfrac{ δx  }{ 2  } \right)      δx \right) }{  \left( x+ \dfrac{ δx  }{ 2  }    \right)  δx  }      
$$ The solution says that it is equal to $1$ because of the $ \dfrac{  \sin (  u   )    }{ u  }   =  1 $ when ${ u }$ approaches $0$ limit theorem. But I don't understand how that can be applied here, since the equation has both $δx$ and $x$ , so how can the theorem still be applicable?","['limits', 'calculus', 'derivatives', 'trigonometry']"
4726348,Two juxtaposed equilateral triangles with joined lines,"My old high school teacher has been posting some math problems online and I just couldn't solve this one. Let triangles $\triangle ABC$ and $\triangle BDE$ be equilateral. Prove $$\overline{FB}=\sqrt{\overline{CF}\cdot \overline{FE}}$$ This seems just plain awful using analytic geometry, but it is doable, because we know the coordinates of every point taking $A$ as the origin and $AD$ as the x-axis and the y-axis perpendicular to it and pointing upwards. $C=(l_1/2,l_1\sqrt{3}/2)$ , for example, so it is just a matter of doing computations. This former teacher of mine usually uses clever tricks, but I can't seem to think of any.",['geometry']
4726367,How do people create trigonometric questions from scratch?,"I'm interested in how people just generate a question from scratch,(i'm preparing for indian highschool level competitive exam ""iitjee""), i think people don't just start writing random stuff and label that as a question but, there are only four things i can think of they are using geometry some sort of inequalities complex numbers somehow algebra
But i don't know how to generate trig questions from it, if anyone could help in giving some examples and thought processes it would be much appreciated. (Seen my teacher creating questions from Jensens inequality, mth power theorem,am-gm-hm-qm inequality,complex number ) (It was my first time asking questions please ignore if any mistakes are made) Thank you!","['algebra-precalculus', 'trigonometry', 'complex-numbers', 'inequality']"
4726424,Show that $\sigma_n^2$ is an unbiased estimator for $\sigma^2$,"Let $X_n$ be a sequence of i.i.d. random variables with $E(X_i)=\mu,Var(X_i)=\sigma^2$ and define $\mu_n:=\frac{1}{n} \sum_{i=1}^n X_i,\sigma_n^2:=\sum_{i=1}^n(X_i-\mu_n)^2.$ I have already shown that $Var(\mu_n)=\frac{\sigma^2}{n}$ . Now to show that $\sigma_n^2$ is an unbiased estimator for $\sigma^2$ I need to show $E(\sigma_n^2)=\sigma^2$ . What I have so far is: \begin{align}
E(\sum_{i=1}^n(X_i-\mu_n)^2)&=\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=\sum_{i=1}^nVar(X_i-\mu_n)-(E(X_i-\mu_n))^2\\&=\sum_{i=1}^nVar(X_i)+Var(\mu_n)-\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=\sum_{i=1}^n \sigma^2+\sigma^2/n-\sum_{i=1}^nE((X_i-\mu_n)^2)\\&=n\sigma^2+\sigma^2-E(\sum_{i=1}^n(X_i-\mu_n)^2)
\end{align} Here I have used the fact that the variance of the sum of two independent variables is the sum of the single variances, the product of expectations of independent variables is the expectation of the product and the sum of the expectations is the expectation of the sum. Adding the identical sums on both sides togehter I get $2E(\sum_{i=1}^n(X_i-\mu_n)^2)=n\sigma^2+\sigma^2$ . Dividing by $2$ unfortunately doesn't give me $\sigma^2$ due to the $n$ on the right side which makes me think that some of my conclusions were wrong. Any help would be appreciated!","['expected-value', 'statistics', 'variance', 'probability']"
4726454,First order elliptic operator with compact resolvent.,"I am working on Omar Mohsen's article Witten deformation using Lie groupoids and having trouble with Proposition 2.1, which says Let $W$ be a complete Riemannian manifold, $\sharp : T^*W\to TW$ the musical isomorphism given by the Riemannian metric, $\alpha$ a 1-form on $W$ such that the form $\mathrm{d}\alpha$ is bounded, the section of $\mathrm{End}(\Lambda_{\mathbb{C}}T^*W)$ given by $\mathcal{L}_{\alpha^\sharp}+\mathcal{L}^*_{\alpha^\sharp}$ is bounded, $\lVert\alpha\rVert$ is a proper function, then the operator $d+d^*+c(\alpha)$ on $L^2(\Lambda_{\mathbb{C}}T^*W)$ is a self-adjoint elliptic operator with compact resolvent, where $L^2(\Lambda_{\mathbb{C}}T^*W)$ is the Hilbert space of $L^2$ sections on $\Lambda_{\mathbb{C}}T^*W$ , $d$ is the exterior derivative and $d^*$ its formal adjoint, $c(\alpha)=\alpha\wedge\cdot+i_{\alpha^\sharp}(\cdot)$ is the Clifford action. I can prove that it is self-adjoint and elliptic, but I have no idea of the compactness of its resolvent. Any help would be grateful!","['operator-theory', 'functional-analysis', 'differential-geometry']"
4726466,Why is this function of bounded variation?,"Let $g=[0,1]\rightarrow \mathbb R$ be a function such that for any sequence $(f_n)$ of left-continuous step functions on $[0,1]$ decreasing to $0$ , we have that $\int_0^1 f_n(x) dg(x)$ converges to $0$ . I read that this implies that $g$ is of bounded variation. How to show that? I imagine this has something to do with Carathéodory extension theorem?","['integration', 'convergence-divergence', 'functional-analysis', 'real-analysis']"
4726473,"Why is not $\,f(x)=\arcsin\left(2x\sqrt{1 - x^2}\right)$ a one to one function?","Why is not $\,f(x)=\arcsin\left(2x\sqrt{1 - x^2}\right)$ a one to one function? I've always knew inverse trig functions to be one to one on a specific range. But from the graph of the above inverse trig function it shows that it’s not one to one. My question is why? Also what are the necessary restrictions for the function to act as one to one?","['inverse-function', 'analysis', 'continuity', 'functions', 'trigonometry']"
4726504,A clean way to identify $\nabla(|f|^2)$ with $2f\overline{f'}$ for a holomorphic $f$.,"Let $\Omega \subset \Bbb C$ be an open domain and $f\colon \Omega \to \Bbb C$ be a holomorphic function. By identifying the point $z = x+iy$ with $(x,y) \in \Bbb R^2$ , we may write $f(z) = u(x,y) + i v(x,y)$ where $u,v$ are real-valued functions on $\Omega$ (considered as a subset of $\Bbb R^2$ ). We can then define a real-valued function $$
W(x,y) = |f(z)|^2 = u(x,y)^2 + v(x,y)^2.
$$ It is not hard to verify that $W$ is differentiable and its gradient is $$
\nabla W = 2\begin{bmatrix}
uu_x+vv_x \\
uu_y + vv_y
\end{bmatrix}
=2\begin{bmatrix}
uu_x+vv_x \\
-uv_x + vu_x
\end{bmatrix},
$$ where we used the Cauchy-Riemann equation to substitute $u_x = v_y$ and $u_y = -v_x$ . Again, by the Cauchy-Riemann equation, we know that $f'(z) = u_x(z) + iv_x(z)$ and hence $$
f\overline{f'} = (u+iv)(u_x-iv_x) = (uu_x + vv_x) + i(-uv_x + vu_x).
$$ One would be tempted to identify the real gradient of the real-valued function $W$ with the complex number $2f\overline{f'}$ and write $$
\nabla W(x,y) = 2 f(z)\overline{f'(z)}.
$$ Is there a neat way to derive this ""identity"" $\nabla W = 2 f\overline{f'}$ ? Of course, this looks suspiciously like the identity $(g^2)' = 2gg'$ for a differentiable $g\colon\Bbb R\to\Bbb R$ , but with a complex conjugation sign, so I think it is reasonable to expect that one should be able to derive it using a product rule or chain rule of some kind. I tried writing $W = f\bar f$ , thinking of $f,\bar f$ as $\Bbb R^2$ -valued functions, and then differentiated it. Alas, I am quite bad at complex analysis and couldn't recover the desired identity. One of the problems I encountered is that the $2\times2$ Jacobian matrix $D(\bar f)$ cannot be identified with $\overline {f'}$ . In fact, $D(\bar f)$ doesn't even represent any complex number (since $f$ is holomorphic). By that I meant the identification of $\Bbb C$ as isomorphic to the subalgebra of $M^{2\times 2}(\Bbb R)$ $$
\left\{ \begin{bmatrix}
a &-b \\
b &a
\end{bmatrix}  : a,b \in \Bbb R \right\} \cong \left\{ z = a+ib : a,b \in \Bbb R  \right\} \cong \Bbb C. 
$$","['complex-analysis', 'multivariable-calculus', 'partial-differential-equations', 'real-analysis']"
4726532,Is convergence of vectors equivalent to convergence of inner products [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 12 months ago . The community reviewed whether to reopen this question 12 months ago and left it closed: Original close reason(s) were not resolved Improve this question Consider a net $(x_\alpha)_{\alpha\in A}$ and $x$ in $\mathbb{R}^n$ . Is the following true? $$x_\alpha\rightarrow x ~\iff~ \langle x_\alpha,y\rangle\rightarrow\langle x,y\rangle,~\forall y\in\mathbb{R}^n.$$ I can prove ( $\Rightarrow$ ) but get stuck on ( $\Leftarrow$ ). (Convergence on the left is meant as in euclidean norm.)","['functional-analysis', 'analysis', 'real-analysis']"
4726537,How can we find Lambert W solution to $\dfrac {x\ln x}{\ln x+1}=\dfrac{e}{2}$?,"Find all real solutions: $$\frac {x\ln x}{\ln x+1}=\frac{e}{2}$$ Cross multiplication gives $$2x\ln x=\ln (x^e)+e$$ I didn't see any useful thing here. I tried solving this equation in WA.  The interesting thing is that Wolfram provides $2$ real approximate solutions. $$x\approx 0.282858...$$ $$x\approx 2.71828...$$ When I looked at one of these solutions, I easily saw that the second solution was the Euler Number and immediately tried it in the equation. $$\frac {e\ln e}{\ln e+1}=\frac {e}{1+1}=\frac {e}{2}$$ Indeed $x=e$ is a correct solution.  One of the solutions is correct, but reaching that solution doesn't actually give us an idea of ​​the equation. Also I have no idea about the first solution. Can we find Lambert W solution to $\dfrac {x\ln x}{\ln x+1}=\dfrac{e}{2}$ ?","['lambert-w', 'algebra-precalculus', 'transcendental-equations', 'logarithms']"
4726568,Showing that $\exp(rN_t-\lambda t(e^r-1))$ is a Martingale,"Let $(N_t)_t$ be a homogeneous Poisson process with intensity $\lambda>0$ . How can I prove that $Y_t=\exp(rN_t-\lambda t(e^r-1))$ is
a Martingal w.r.t. the canonical filtration, $r\in \mathbb{R}$ , $t\geq
> 0$ . My attempt $Y_t$ is adapted to $\mathcal{F}_t$ since $N_t$ is adapted to $\mathcal{F}_t$ and $Y_t=f(N_t)$ is a measurable function of $N_t$ . Integrability: $E(|Y_t|)<\infty$ ( I don't really see at first glance why that is the case) For $0\leq s\leq t$ , one has to show: $$E(\exp(rN_t-\lambda t(e^r-1))|\mathcal{F}_s)=\cdots = \exp(rN_s-\lambda s(e^r-1))$$ However, I don't know how to deal with the $\exp$ in $E(\cdot)$ . It might be noteworthy that I know that $N_t-\lambda t$ (i. e. the compensated poisson process) and $(N_t-\lambda t)^2-\lambda t$ are martingales. Can someone help?","['stochastic-processes', 'martingales', 'poisson-process', 'probability']"
4726631,Is every algebraic structure of this sort embeddable in a vector space?,"A convex space is a set $X$ equipped with a function $X \times X \times [0, 1] \to X$ , denoted as $(a, b, t) \mapsto a \overset{t}{+} b$ , such that for all $a, b, c \in X$ and $t, t' \in (0, 1)$ , the following axioms hold: (""identity"") $a \overset{0}{+} b = a$ , $a \overset{1}{+} b = b$ , and $a \overset{t}{+} a = a$ . (""associativity"") $a \overset{t}{+} (b \overset{t'}{+} c) = (a \overset{\frac{t(1-t')}{1-tt'}}{+} b) \overset{tt'}{+} c$ . (""commutativity"") $a \overset{t}{+} b = b \overset{1-t}{+} a$ . (There is a short Wikipedia entry about this type of algebraic structure. I was originally curious about it as a simple way to model real-life paint mixing, and funny enough, independently even gave it the same name.) Let's say that a convex space $X$ is embeddable in a vector space if there exists a vector space $V$ over $\mathbb{R}$ with $X \subseteq V$ and $a \overset{t}{+} b = (1-t)a + tb$ for all $a, b \in X$ and $t \in [0, 1]$ . Now my initial question was whether every convex space is embeddable in a vector space - it turns out the answer is no, and here is a counterexample: Let $X$ be the two-point set $\{x, y\}$ and define $$a \overset{t}{+} b = \begin{cases}
x & \text{if ($a = b = x$) or ($a = x$ and $t = 0$) or ($b = x$ and $t = 1$)},\\
y & \text{otherwise.}
\end{cases}$$ Then $X$ is a convex space that cannot be embeddable in a vector space, for then the equation $\frac{1}{2}x + \frac{1}{2}y = x \overset{\frac{1}{2}}{+} y = y$ gives $x = y$ . This motivates an additional axiom: A convex space $X$ is faithful if the function $t \mapsto a \overset{t}{+} b$ is injective for each pair of distinct $a, b \in X$ . Now, is this enough? That is to say, is every faithful convex space embeddable in a vector space ?","['convex-analysis', 'abstract-algebra', 'linear-algebra', 'vector-spaces']"
