question_id,title,body,tags
3806216,Solve $x_1+x_2+x_3+...+x_6<10$ if $x_i $ are non -negative integers,"So I've got to solve $x_1+x_2+...+x_6<10$ , if $x_i$ are non-negative integers. In class, we learned to do it as such: Since $(x_1+x_2+..+x_6)$ is a positive quantity there must be a positive integer $x_7$ such as $(x_1+x_2+..+x_6) + x_7 = 10$ . Then, we say that this equation is equivalent to this equation : $y_1+ y_2+..+y_6+y_7=9$ where $y_i \mapsto x_i for i  to  [0,6]$ and $y_7=x_7-1$ . My question is about the mapping of $y_7$ . Did we choose it to be $y_7=x_7-1$ because we know for sure that x7 must be at least 1? So we said, $x_7 - 1>=0 \rightarrow y_7 \mapsto x_7-1$ ?
If for example we put that $y_7 = x_7-2$ and we solve the equation $y_1+ y_2+..+y_6+y_7=8$ shouldn't we get the same result?
I am trying to figure out for sure , which is the the technique behind those mappings.","['inequality', 'combinatorics', 'problem-solving', 'discrete-mathematics']"
3806230,Does a curve (differentiable manifold of dim 1) always have a parametrization?,"I don't know much about this field, so this is a basic question. I think there are 2 similar basic concepts referring to curve: A $C^k$ differential curve , can be defined as a differentiable $C^k$ manifold of dimension 1. This is what one can read here: https://en.wikipedia.org/wiki/Curve#Differentiable_curve . A $C^k$ - parametric curve is roughly a $C^k$ function $f$ from an interval $I$ of $\mathbb{R}$ , to a normed vector space $E$ .
This concept is studied here https://en.wikipedia.org/wiki/Differentiable_curve , surprisingly I haven't seen this page mentioning the first definition despite its title. Now I think it is obvious that the trajectory of a $C^k$ -parametric curve (ie $f(I)$ ) is a $C^k$ differential curve, because one of the equivalent definitions of manifold says that, each point of it needs to have some local parametrization, so we can use the global parametrization provided by the parametric curve for it. It naturally raises the converse question:
If i am given a set of points that is a manifold of dimension 1, is it the trajectory of at least one parametric arc? A more accurate question: if I have a $C^k$ differential curve, is it the trajectory of at least one $C^k$ parametric arc? I thought that maybe one can use the local parametrizations provided by the manifold and somehow stick them together as a chain, but I feel like there is no guarantee that I can cover the whole curve, because maybe the paramatrizations are getting smaller and smaller...","['curves', 'parametrization', 'differential-geometry']"
3806255,Determinant of Frobenius action on the exterior power on cohomology,"Given an elliptic curve $E$ , the Frobenius action $\mathrm{Frob}$ , and $\mathrm{det}(1-\mathrm{Frob}_E T | H^1(E))$ , how do we find an expression for: $$\mathrm{det}(1-\mathrm{Frob}_X T | \wedge^2 (H^1(E) \otimes H^1(E)))$$ for $X=(E \times E)/(\mathbb{Z}/2\mathbb{Z})$ , the abelian surface with 16 singular points which correspond to the 2-torsion points of $E$ . This seems to be an exercise in linear algebra but I am somewhat muddled with how operations work over exterior powers!","['determinant', 'elliptic-curves', 'number-theory', 'algebraic-geometry', 'linear-algebra']"
3806282,Geometric interpretation of Divergence of $\vec{f} = \frac{1}{r^2} \hat{r}$,"I know that the mathematics tells me that the divergence is zero for the below vector field: $\vec{f} = \frac{1}{r^2} \hat{r}$ But I am more interested in the geometric intuition of it. Here is what I am looking at. The vector length is decreasing as I am increasing the radii of the sphere around origin in 3D space.  Now divergence is defined as $\partial {v_x}/\partial x +\partial {v_y}/\partial y+\partial {v_z}/\partial z$ in cartesian coordinates. Now lets think about a point other than the origin. Lets take $\partial {v_x}/\partial x$ . Now as the vector is decreasing in length as we are increasing the radii, this slope must be less than zero, i.e., $\partial {v_x}/\partial x < 0$ as the value is decreasing as we are increasing the $x$ .  The same logic can be applied to other dimensions, i.e., $\partial {v_y}/\partial y < 0$ $\partial {v_z}/\partial z < 0$ Now given all these inequalities, how can $\partial {v_x}/\partial x +\partial {v_y}/\partial y+\partial {v_z}/\partial z=0 $ ?",['multivariable-calculus']
3806321,Unable to get the same curve as question with parametric equations,"my given task is to write a parametric equation to create this spiral curve with the parameters, $t \in [0, 1]$ . my answer is $x = 0.8\cdot t\cdot\cos(-3.5\pi t)$ $y = 0.8 \cdot t\cdot\sin(-3.5\pi t)\ +\ 0.2$ However, I am unable to get the same curve. As you can see, the question's curve goes into the negative region of y-axis. But my curve doesn't. Also, I don't seem to touch the 0.6 point in y-axis. I am using Desmos to test with my equations, but I can't seem to increase the ""inner radius"" of the spiral curve. Could someone please point me out on what I am missing out please? https://www.desmos.com/calculator/odaxzim8uc","['curves', 'trigonometry', 'graphing-functions', 'parametric']"
3806327,"Showing that if $G$ is abelian of exponent $n$, then $|\operatorname{Hom}(G,\mu_n)|= (G:1)$","I encountered this result in the following context ( p.72 of Milne's Fields and Galois Theory ): Could you please help by explaining to me why this result is true? In particular, I am interested why we need that $G$ is abelian. Also, I am wondering why one would write $(G:1)$ instead of $|G|$ . I know what it means that $G$ has exponent $n$ (it is mentioned in the text) and $\operatorname{Hom}(G,\mu_n)$ is the set of group homomorphism between the abelian group $G$ and the sets of all $n$ -th roots of unity $\mu_n$ . The issue is that I cannot plug all arguments together.","['group-homomorphism', 'galois-theory', 'abstract-algebra', 'group-theory', 'abelian-groups']"
3806329,Show that $\mathcal F=\{X\subseteq\Bbb R|X\neq\emptyset\land\forall x\forall y((x\in X\land x<y)\rightarrow y\in X)\}$ has no minimal element.,"Not a duplicate of this or this . This is exercise $4.4.12$ from the book How to Prove it by Velleman $($$2^{nd}$ edition $)$ : Show that $\mathcal F=\Bigr\{X\subseteq\Bbb R|X\neq\emptyset\ \text{and}\ \forall x\forall y\bigr((x\in X\land x<y)\rightarrow y\in X\bigr)\Bigr\}$ has no minimal element. Here is my proof: Suppose $\mathcal F$ has a minimal element called $M$ . So by definition $M\neq\emptyset$ , $\forall x\forall y\bigr((x\in M\land x<y)\rightarrow y\in M\bigr)$ , and $\forall X\in\mathcal F(X\subseteq M\rightarrow X=M)$ . Since $M$ is not empty we can choose some $m_0\in M$ . Since $\forall x\forall y\bigr((x\in M\land x<y)\rightarrow y\in M\bigr)$ we can write $\{y\in\Bbb R|y\geq m_0\}\subseteq M$ . Now consider the set $W$ as follows $W=\{x\in\Bbb R|x>m_0\}$ . Clearly $W\in\mathcal F$ and also $W=\{x\in\Bbb R|x>m_0\}\subseteq\{y\in\Bbb R|y\geq m_0\}\subseteq M$ . Since $\forall X\in\mathcal F(X\subseteq M\rightarrow X=M)$ , $W=M$ which contradicts the fact that $m_0\in M$ but $m_0\notin W$ and ergo $\mathcal F$ has no minimal elements. $Q.E.D.$ Is my proof valid $?$ Thanks for your attention.","['proof-writing', 'solution-verification', 'discrete-mathematics']"
3806354,Evaluation Of Iterated Limit,"Let $f: \textrm{dom}(f) \rightarrow \mathbb{R}$ . Let $a \in \textrm{dom}(f)$ . Assume $f''(a)$ exists. Want To Prove $f''(a) = \displaystyle\lim_{h \rightarrow 0} \frac{f(a + h) + f(a - h) - 2f(a)}{h^2}$ . $f''(a)$ $= \displaystyle\lim_{h \rightarrow 0} \frac{f'(a + h) - f'(a)}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h)}{k} - \lim_{k \rightarrow 0} \frac{f(a + k) - f(a)}{k}}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \bigg( \frac{f(a + h + k) - f(a + h)}{k} - \frac{f(a + k) - f(a)}{k} \bigg)}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \frac{\displaystyle\lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h) - f(a + k) + f(a)}{k}}{h}$ $= \displaystyle\lim_{h \rightarrow 0} \Bigg( \! \lim_{k \rightarrow 0} \frac{f(a + h + k) - f(a + h) - f(a + k) + f(a)}{hk} \Bigg)$ In order to get the desired result, I would like to replace $k$ with $-h$ . I did this because as $h \rightarrow 0$ and $k \rightarrow 0$ , $k \rightarrow -h$ . However, this reasoning of "" $h \rightarrow 0$ and $k \rightarrow 0$ "" would be more applicable in the case of a two-variable limit as $(h, k) \rightarrow (0, 0)$ rather than an iterated limit as seen in the expression, which may not be equal: Link . So how can I justify replacing $k$ with $-h$ ? I thought about using the Moore-Osgood Theorem, which says the iterated limit is equal to the two-variable limit under a sufficient condition. However, I am unfamiliar with multivariable calculus and I am not sure how I would use it in this case. Unfortunately, I also cannot find a proof that follows Wikipedia's description: Link .","['multivariable-calculus', 'limits', 'calculus', 'derivatives']"
3806367,Is every natural number the truncation of a prime?,"E.g. $30$ can be ""extended"" to a prime, namely 30 $19$ , and $575$ can be extended to a prime, namely 575 $4853343$ . Is this true for every natural number? To make things precise: Let $n\in\mathbb{N}$ . Does there always exist a prime $p=\sum_{i=0}^k d_i\cdot 10^i$ , where $k\in\mathbb{N}_0$ and $d_i\in\{0,\ldots, 9\}$ , $d_k\neq 0$ , $i=0,\ldots, k$ , such that $n=\sum_{i=l}^k d_i\cdot 10^{i-l}$ for some $l\leq k$ ? I thought of this problem randomly and find it interesting. My guess is that it is known, studied, but perhaps unsolved. Or trivial in a non-trivial way.","['number-theory', 'elementary-number-theory', 'reference-request', 'recreational-mathematics', 'prime-numbers']"
3806394,How to calculate the $q$-extent of round sphere?,"By definition the $q$ -extent of a metric
space $X$ is the maximum average distance between $q$ points; i.e. $$xt_q(X):=\max_{x_1,\dots,x_q}xt_q(x_1 ,\dots ,x_q)$$ where $$xt_q(x_1 ,\dots,x_q) = {q\choose 2}^{-1}\sum_{i<j}  \operatorname{dist}(x_i , x_j) .$$ The following image is a screenshot of a lecture. There the lecturer calculated the $q$ -extent of round sphere which I think it is wrong. because for $q=4$ it fails. What is the correct solution? I think it should be $2\pi/4=\pi/2$ but how? Update: After a few search, I found the following example which shows that my claim is wrong. But I don't understand the author argument. (Or I misunderstood the concept of $q$ -extent!! For example, my thought of $4$ -extent of $\Bbb S^1$ is that to reach the maximum amount we should put them pair of antipodal points, but this is wrong.) Q: What does $q$ -extent of $\Bbb S^1$ represent  exactly? I know that for $q=2$ it is ""diameter"".","['metric-spaces', 'riemannian-geometry', 'differential-geometry']"
3806419,Does there exist a closed form for $\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx$?,"I am not sure if there exists a closed form for $$I=\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx$$ which seems non-trivial. I used the reflection and landen's identity, didn't help much. In case you are curious how I came up with this integral: From here we have $$\arcsin^3(x)=6\sum_{k=1}^\infty\left[\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}\right]\frac{{2k\choose k}}{4^k}\frac{x^{2k+1}}{2k+1},\quad |x|<1$$ differentiate both sides with respect to $x$ we get $$\frac{\arcsin^2(x)}{\sqrt{1-x^2}}=2\sum_{k=1}^\infty\left[\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}\right]\frac{{2k\choose k}}{4^k}x^{2k}$$ use $\sum_{m=0}^{k-1}\frac{1}{(2m-1)^2}=H_{2k}^{(2)}-\frac14H_k^{(2)}$ and replace $x$ by $\sqrt{x}$ we get the form $$\frac{\arcsin^2(\sqrt{x})}{\sqrt{1-x}}=2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}x^{k}$$ Divide both sides by $x$ then $\int_0^y$ we have $$\int_0^y\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}dx=2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{y^{k}}{k}$$ Next, multiply both sides by $-\frac{\ln(1-y)}{y}$ then $\int_0^1$ and use $-\int_0^1 y^{k-1}\ln(1-y)dy=\frac{H_k}{k}$ $$2\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{H_k}{k^2}=-\int_0^1\int_0^y\frac{\arcsin^2(\sqrt{x})\ln(1-y)}{xy\sqrt{1-x}}dxdy$$ $$=\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}\left(-\int_x^1\frac{\ln(1-y)}{y}dy\right)dx$$ $$=\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}\left(\zeta(2)-\text{Li}_2(x)\right)dx$$ $$\overset{\sqrt{x}=\sin\theta}{=}2\int_0^{\pi/2}\frac{x^2}{\sin x}(\zeta(2)-\text{Li}_2(\sin^2x))dx$$ So we have $$\sum_{k=1}^\infty\left[H_{2k}^{(2)}-\frac14H_k^{(2)}\right]\frac{{2k\choose k}}{4^k}\frac{H_k}{k^2}=\zeta(2)\int_0^{\pi/2}\frac{x^2}{\sin x}dx-\int_0^{\pi/2}\frac{x^2\ \text{Li}_2(\sin^2x)}{\sin x}dx$$ The first integral can be calculated by applying integration by parts then using the fourier series of $\ln(\tan\frac x2)$ . Another question is, clearly the two sums on the LHS are convergent as the denominator blows to infinity much faster than the numerator. But does there exist a closed form for each? All methods are welcome. Thank you","['integration', 'harmonic-numbers', 'polylogarithm', 'closed-form', 'sequences-and-series']"
3806442,Invariant subspaces to the permuting action of the alternating group.,"This question arose while researching Galois groups of random reciprocal polynomials. Throughout $A_n$ is the alternating group. Let us define the following group action of $A_n$ on the vector space $\mathbb{C}^n$ $$\sigma \circlearrowright (x_1,\dots,x_n)=(x_{\sigma^{-1}(1)},\dots,x_{\sigma^{-1}(n)})$$ My interest is to classify all the subspaces invariant to this action. My idea was to classify them in terms of linear systems. the invariant spaces I found are the following ones: $\{0\}\subseteq \mathbb{C}^n$ which corresponds to $x_1 = \dots = x_n = 0$ , $V_1=\operatorname{span}\{(1,\dots, 1)\}\subseteq \mathbb{C}^n$ which corresponds to $x_1 = \dots = x_n$ , $V_2\subseteq \mathbb{C}^n$ which corresponds to $\sum_{i=1}^nx_i=0$ , $\mathbb{C}^n \space$ , the whole space. In case we extend the action to the whole symmetric group $S_n$ it can be shown that those truly are all the invariant subspaces, however, for $A_n$ I suspect I missed a few. Any Help would be much appreciated!","['permutation-matrices', 'representation-theory', 'linear-algebra', 'group-theory', 'group-actions']"
3806446,Show that every group of order 15 is cyclic using class equation.,"I am willing to show that every group of order 15 is cyclic, using class equation. Let $G$ be a group of order 15. If $G$ is abelian, then $G=Z(G)$ and so for each $a\in G=Z(G)$ we have $cl(a)=\{a\}$ . Hence only possible class equation is $$15=1+1+1+\cdots+1+1(15~\text{times})$$ In this case, $G$ is isomorphic to either $\mathbb{Z}_{15}$ or the external direct product $\mathbb{Z}_3\times \mathbb{Z}_5$ . But since $\mathbb{Z}_{15}\simeq \mathbb{Z}_3\times \mathbb{Z}_5$ , it follows that $G$ is cyclic. We now show if $G$ is non-abelian then contradiction will appear. When $G$ is non-abelian, then $G\neq Z(G)$ . Here $|Z(G)|\in \{1,3,5\}$ . But if $|Z(G)|=3$ then $|G/Z(G)|=5$ a prime, so $G/Z(G)$ is cyclic and hence $G$ becomes Abelian, contradiction. Similalry $|Z(G)|\neq 5$ as well and so only possibility is $|Z(G)|=1$ . Then the class equation reads $$15=|G|=|Z(G)|+\sum |cl(a)|=1+\sum |cl(a)|$$ where the sum is taken over the orders of all non-singleton conjugacy classes $cl(a)$ in $G$ . Let there be $x_3$ and $x_5$ number of conjugacy classes of order 3 and 5 respectively in $G$ . Then we must have $$15=1+3x_3+5x_5\Rightarrow 14=3x_3+5x_5$$ which is satisfied by $x_3=3, x_5=1$ so that ultimately the class equation  becomes $$15=1+(3+3+3)+5$$ I do not know how to bring contradiction here. Any help? Thanks in well advance.","['group-theory', 'abstract-algebra', 'finite-groups', 'cyclic-groups']"
3806460,Morrie's law with sines,"While it's trivial to prove $\prod_{k=0}^{n-1}\cos(2^kx)=\frac{\sin(2^nx)}{2^n\sin x}$ , Wikipedia refers to a ""similar"" identity $\sin\tfrac{\pi}{9}\sin\tfrac{2\pi}{9}\sin\tfrac{4\pi}{9}=\frac{\sqrt{3}}{8}$ . How does this generalize to a result for $\prod_{k=0}^{n-1}\sin(2^kx)$ ? Failing that, how do we prove this special case?","['trigonometry', 'products']"
3806475,"Differences between matrices, bivectors and rank-2 tensors","I recently came across bivectors while looking into spacetime algebra, but couldn't understand their differences from the matrices, and from rank-2 tensors. While looking into bivectors, I found that they also follow the same distributivity laws as that of matrices. While being on the same question, I know that tensors are defined in vector spaces, but is it that matrices are defined on some other space, which makes it different from a rank-2 tensor? And if these three are completely different from one another, then why do we represent one with the other?","['matrices', 'tensors']"
3806512,"Bound $\sum_{k=1}^n P(B<k) P(B\ge k) $ where $B$ is Binomial $(n,p)$","Let $B$ be Binomial $(n,p)$ . Is there a simple but tight bound on \begin{align}
\sum_{k=1}^n P(B<k) P(B\ge k) 
\end{align} as function of $n$ ? Here is my attempt, which I think is suboptimal, \begin{align}
\sum_{k=1}^n P(B<k) P(B\ge k)  &\le  \sum_{k=1}^n  P(B\ge k) \text{           use $P(B<k) \le 1$ }\\
&= \sum_{k=1}^n  e^{-2n(p-\frac{k}{n} )^2},
\end{align} where in the last step we have use a tail bound for Binomial (see wiki ). There are some other better tail bounds that can improve this inequality.
However, I think we still lose a lot in step. Is there a better way of approaching this? That would result in a tight bound for large $n$ ?","['binomial-distribution', 'probability-theory']"
3806516,Proof verification: Any countable subset of $\Bbb R$ is disconnected,"In an exercise I'm asked to prove the following: Prove that every countable subset of $\Bbb R$ with more than one point is disconnected. I did My proof but there's one set on the proof that I'm not sure how to prove. This is my proof: Let $S\subset \Bbb R$ such that $S$ is countable. This means that we can list the elements of the set $S$ as following: $$S = \{ s_1,...,s_n\}$$ Where $n$ can be a natural number if the set is finite, or $n$ can go infinitely high if the set $S$ is countably infinite. Now let's define the following interval: $$I = (\inf S,\sup S)$$ Because $\text{card } I > \text{card } S$ we can assume that $S \subset I$ . So there are element of $I$ that do not belong to $S$ . So let $y\in I\setminus S$ . This means that there exists $s_i,s_j\in S$ such that: $$s_i < y < s_j\ \ \ (1)$$ We have that $s_i,s_j\in S$ but $y \notin S$ . This means that $S$ is not an interval and therefore it's not connected. First of all: Is this proof correct? I'm not quite sure how to prove statement (1). How can I prove that such $s_i,s_j \in S$ do in fact exists?","['real-numbers', 'general-topology', 'solution-verification', 'connectedness']"
3806518,Replacement only when 'prize' is found,"There are $a$ balls in a jar. One is gold, the rest are black. Balls are taken from the jar. If a black ball is taken, it is not replaced. If a gold ball is taken, all balls are replaced. I am interested in finding a closed form solution for the expected number of gold balls taken after $k$ balls have been taken in total. My approach so far is to form a recurrence relation with $E_n =$ the expected number of gold balls taken after $n$ balls have been taken in total. The starting condition is $$E_0 = 0$$ Conditioning on getting the gold ball on the $i$ th try (probability $\frac{1}{a}$ ), For $0 < k < a$ , $$E_k = \left(\frac{1}{a}\right)\left(\sum^{k}_{i=1}1+E_{k-i}\right)$$ For $k \geq a$ , $$E_k = \left(\frac{1}{a}\right)\left(\sum^{a}_{i=1}1+E_{k-i}\right)$$ The relation can be simplified to $$E_{<0} = -1, E_0 = 0$$ $$E_{k>0} = 1+\left(\frac{1}{a}\right)\left(\sum^{a}_{i=1}E_{k-i}\right)$$ I'm not sure where to go from here, or if this is the best approach. Another thing I'm a bit confused on is some intuition. When $a=2$ and the jar is full, the expected number of balls it takes to get the golden ball is $\frac{1}{2}+\frac{2}{2}=\frac{3}{2}=1.5$ , I believe. On the other hand, if the above recurrence is correct, then for $a=2$ , $E_3=\frac{15}{8}=1.875$ . So you expect $1.875$ gold balls after taking $3$ balls. I can't figure out why this is less than $2$ , intuitively, which leads me to be unsure about my recurrence. On the other hand, I notice as $k$ grows large (from plotting the recurrence), the difference between the expected number of gold balls every $3$ balls taken tends to $2$ from below. What I've said above seems to generalise for all $a$ . Table of recurrence for $a=2$ . Left column is $k$ , right is $E_k$ .",['probability']
3806529,Question for the function $f(x)=\log\left(\frac{x^2}{x-2}\right)$,"Let the function defined by $$f(x)=\log\left(\frac{x^2}{x-2}\right)$$ which of the following affirmations is FALSE ? $\fbox{A}\, f\,$ has an absolute minimum; $\fbox{B}\, f(x)>0,\quad  \forall x\in\operatorname{dom} f$ ; $\fbox{C}\, f\,$ have not inflected; $\fbox{D}\, f\,$ restricted to $]2; 4[$ is invertible; $\fbox{E}\, \nexists\, c\in \operatorname{dom} f \mid f(c)=0$ . My attempt to resolve it. Being the $\operatorname{dom} f=]2,+\infty[$ and the $\lim_{x\to2+}f(x)>0$ , and hence $\fbox{B}$ is true . It is true the $\fbox{D}$ , because if I switch $x\leftrightarrow y$ , I think that I will have an exponential function of base $e$ . If $f(x)=\log\left(\frac{x^2}{x-2}\right)=0 \iff \frac{x^2}{x-2}=1$ and this have not real roots. Hence it is true the $\fbox{E}$ . For the inflected I  think that $f$ it has any so it is true the $\fbox{C}$ . Definitively I think that the false is the $\fbox{A}$ (see Weiestrass's theorem: $]2,+\infty[$ is not closed and limited). Dear users, I ask you if there are any errors and I wait your observations/answers.","['calculus', 'functions', 'algebra-precalculus']"
3806533,"Does $\{ 0, \{ 0 \} \}$ contradict the axiom of regularity?","I'm trying to understand the ZFC axioms, and I understand most of them except the axiom of regularity. $$\forall x[\exists a(a\in x) \Rightarrow \exists y(y\in x \wedge \neg\exists z(z\in y \wedge z\in x))]$$ From what I understand, it is saying that, for all non-empty sets that have a set for an element, the set does not share any elements with the original set and the element of the set. However, wouldn't that make the natural number construction of 2 impossible because: $$2 = \{ 0, 1 \} = \{ 0, \{ 0 \} \}$$ If we choose $x = 2$ and we choose the element $\{ 0 \}$ to be our $y$ . There does exist an element $z$ which is in both sets, more precisely $0$ . $$0 \in 2 \, \land \, 0 \in \{ 0 \}$$ What am I missing? Thanks!","['elementary-set-theory', 'natural-numbers', 'axioms']"
3806534,"If $\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2,$ show that $\lim_{x\to 0}f(x)=1$.","Question: Suppose $f:(-\delta,\delta)\to (0,\infty)$ has the property that $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2.$$ Show that $\lim_{x\to 0}f(x)=1$ . My approach: Let $h:(-\delta,\delta)\to(-1,\infty)$ be such that $h(x)=f(x)-1, \forall x\in(-\delta,\delta).$ Note that if we can show that $\lim_{x\to 0}h(x)=0$ , then we will be done. Now since we have $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2\implies \lim_{x\to 0}\frac{(f(x)-1)^2}{f(x)}=0\implies \lim_{x\to 0}\frac{h^2(x)}{h(x)+1}=0.$$ Next I tried to come up with some bounds in order to use Sandwich theorem to show that $\lim_{x\to 0} h(x)=0,$ but the bounds didn't quite work out. The bounds were the following: $$\begin{cases}h(x)\ge \frac{h^2(x)}{h(x)+1},\text{when }h(x)\ge 0,\\h(x)<\frac{h^2(x)}{h(x)+1},\text{when }h(x)<0.\end{cases}$$ How to proceed after this?","['limits', 'calculus', 'real-analysis']"
3806536,Relation between universal properties and topological invariants.,"I don't really have any experience working with universal maps or adjoint functors (well, neither with categories, really. Only some basic definitions and intuitions). However, I have the feeling that maybe it's not a crazy idea to think of multiplicative properties in topology as topological invariants preserved when formal categorical products (or coproducts, or quotients... ) are done . I guess this topological products, coproducts and quotients have to do with universal maps, that's why I picked that title for my question, but I don't know how to apply this language yet so I'm sorry in advance for any possible misunderstanding. Is it possible to achieve some statement like ""Because of these categorical properties of compactness, compactness is a multiplicative property""?","['product-space', 'universal-property', 'general-topology', 'category-theory']"
3806548,Simple space-filling curve,Here are six iterations of Hilbert space-filling curve . Isn't there a simpler space-filling curve? For example $16$ iterations of this curve: Isn't it also a space-filling curve? If it is not then why? If it is then why they (mathematicians) did not use the simplest possible curve? Or what are advantages of Hilbert curve over mine?,"['general-topology', 'geometry']"
3806575,"The ""co-small"" topology on the naturals?","Consider the set of all subsets of the naturals, $2^\Bbb{N}$ . We call a subset $A \subseteq \Bbb{N}$ small if $\sum_{a \in A} \frac{1}{a} < \infty$ , and large otherwise. The set of small subsets of $\Bbb{N}$ , $$X := \{ A \in 2^\Bbb{N}: A \text{ is small} \},$$ is closed under arbitrary intersection and finite union. This suggests that we can define a topology on $\Bbb{N}$ as follows: Call an element $U \in 2^{\Bbb{N}}$ co-small if $U^c := \Bbb{N} \setminus U$ is small. Then the co-small topology on $\Bbb{N}$ is the topology where the nontrivial open sets (i.e. besides $\Bbb{N}$ and the empty set) are the co-small sets. This is a topology because the set of co-small sets is closed under arbitrary union and finite intersection. Under this topology, $\Bbb{N}$ is $T_1$ (for any two distinct points $a, b$ there is a neighborhood of $a$ disjoint from $b$ and vice versa) but not Hausdorff (since any two co-small sets have co-small intersection, any two neighborhoods of distinct points $a, b$ will overlap). The only compact sets in $\Bbb{N}$ under this topology are the finite sets; however, $\Bbb{N}$ is not discrete in this topology (since one-point sets cannot be co-small). Questions: Is there a formal name for this topology, and is it studied in the literature at all? Is $\Bbb{N}$ in the co-small topology homeomorphic to another, better known or understood space? What are the continuous functions from $\Bbb{N}$ to itself under the co-small topology, aside from trivial examples like the constant function or the identity function? ( The only continuous maps from $\Bbb{N}$ in the co-small topology to $\Bbb{R}$ in the usual topology are the constant functions. ) We can also think of $\Bbb{N}$ as a discrete measure space with the obvious measure $$\mu(A) := \sum_{a \in A} \frac{1}{a}.$$ Every open set has infinite measure, and sets with finite measure are closed. Also, every element of $2^\Bbb{N}$ is measurable, from which it trivially follows that every function $f: \Bbb{N} \to \Bbb{R}$ or $\Bbb{C}$ is measurable. The measure also scales in a nice way: we have $\mu(kA) = \frac{\mu(A)}{k},$ where $kA := \{ ka: a \in A \}$ . Can this measure be applied to any interesting problems in number theory or combinatorics? Is it used to prove ergodicity of any maps?","['measure-theory', 'number-theory', 'combinatorics', 'general-topology', 'convergence-divergence']"
3806608,"$\sum_n(-1)^n\int_Xf_n \, d\mu=\int_X\sum_n(-1)^nf_n \, d\mu$","Let $(X,\mathcal{F},\mu)$ be measure space, $(f_n)_n$ be a non-increasing sequence of functions in $L^1(\mu)$ , converging $\mu$ -a.e to $0.$ Prove that $$\sum_n(-1)^n\int_Xf_n \, d\mu=\int_X\sum_n(-1)^nf_n \, d\mu.$$ To prove it, it's evident that we should use the dominated convergence theorem on $\sum_{k=1}^n(-1)^kf_k:$ $$\sum_{k=1}^n(-1)^k\int_Xf_k \, d\mu=\int_X\sum_{k=1}^n(-1)^kf_k \, d\mu$$ Since $(f_n)_n$ is a non-increasing sequence converging $\mu$ -a.e to $0$ (which means $f_n$ are non-negative $\mu$ -a.e), this shows that $(\sum_{k=1}^n(-1)^kf_k)_n$ converges $\mu$ -a.e (to $\sum_n(-1)^nf_n$ ), it remains to prove that $\sum_{k=1}^n(-1)^kf_k$ is dominated by a function $\phi \in L^1.$ Do you know how to have a verification?","['sequence-of-function', 'measure-theory', 'real-analysis']"
3806609,Closed Form of Universal formal group law,"There is Lazard's theorem which states that the universal formal group ring $L$ is isomorphic to $\mathbb{Z}[x_1,x_2,\ldots]$ . If we let formally $\exp(t)=t+\sum\limits_{i>0}b_i t^i$ and $\log(t)$ s.t. $\exp(\log(t))=t$ then we obtain group law $F$ over $\mathbb{Z}[b_1,b_2,\ldots]$ by putting $F(t_1,t_2)=\exp(\log(t_1)+\log(t_2))$ .
Then the universal map $L\to \mathbb{Z}[b_1,\ldots]$ identify $L$ with a subring of $\mathbb{Z}[b_1,\ldots]$ . This is basic construction behind a proof and after some work we identify $L$ as free polynomial algebra. I have three questions: Is there are any expression for a universal group law over $\mathbb{Z}[x_1,\ldots]$ ? Is there are any expression for a generators of $L\subset \mathbb{Z}[b_1,\ldots]$ in terms of $b_i$ ? Is there are explicit and closed expression for a generators of complex cobordism ring $\pi_*(MU)\sim L$ ?","['number-theory', 'algebraic-topology']"
3806618,What are the research areas in differential geometry involving Lie Groups action?,"I love Lie groups actions! My contact with this topic occurred in the context of differentiable manifolds and riemannian geometry courses, and while studying Klein geometries. My knowledge is still very limited, though. I would like to specialize on a research area in which I could work a lot with Lie groups action from a geometric point of view. It would be very nice if I could find some suggestions/possibilities here. I'm not connected to the university now and I couldn't find pithy information on the internet. Thank you all, in advance.","['lie-groups', 'differential-geometry']"
3806631,Having extreme difficulty understanding conditional statements. [duplicate],"This question already has answers here : In classical logic, why is $(p\Rightarrow q)$ True if both $p$ and $q$ are False? (25 answers) Closed 3 years ago . everyone. I'm having a very difficult time understanding conditional statements, and was hoping someone could help me understand them. I took discrete math this last spring and remember struggling with them, but at some point had to let go and just say ""it is what it is, I guess."" I'm now taking a new course and conditionals are being thrown back in my face. I've watched several YouTube videos, read back over my discrete math textbook, did a google search and read the websites that came up in the first page and a half. Anything I do, I can't seem to grasp the meaning as a whole. Simply saying ""if p, then q"" or ""p implies q,"" or ""p is sufficient for q,"" or ""q if p"" does not make things clear. I understand a conditional statement is a compound statement made of individual propositions that are either true or they aren't true. I can also see how compound statements such as ""It's raining outside AND it's cloudy"" could, as a whole statement, be true. I've seen different definitions for what ""if p, then q"" mean, and they sometimes seem to contradict one another. So I'm going to ask it here. What is a conditional statement, and what does it mean? Further, in a conditional statement: Why does a true hypothesis and a true conclusion make the conditional true? Why does a True hypothesis and a false conclusion make the conditional false? Why does a false hypothesis and a true conclusion make the conditional true? Why does a false hypothesis and a false conclusion make the conditional true? Right now, I don't exactly know what questions I need to specifically ask to make it clear - all I know is I'm not getting it, and I'm exhausted with the run-around, lack of clarity, and simply not understanding. Hoping someone can help me. Thank you and hope you're all well.
5.",['discrete-mathematics']
3806667,How to use the extended version of Weierstrass's theorem?,"After my question Question for the function $f(x)=\log\left(\frac{x^2}{x-2}\right)$ , I have obtain a very good answer and I remember that I have never studied this theorem during my period at my university (1993) "" extended version of Weierstrass's theorem"" . Obviously there are several pages on the internet that discuss the proof of the theorem. I would like to understand well the hypotheses and when and how to apply the theorem in practical cases especially for the students of an high school . I have seen that are used the sequences and the minimum and maximum limits ( $\lim \sup$ , $\lim \min$ )  and I do not remember their use and the meaning of this concepts.","['extreme-value-theorem', 'examples-counterexamples', 'analysis', 'real-analysis']"
3806747,Determine if a number is algebraic.,"I recall there is an algorithm involving lattices that would tell me if a given number(approximation) is an algebraic number or not, along with the exact algebraic form of the number. Is there any tool (Gap, Mathematica, python package, etc) that has an implementation of this? I have numbers which I can approximate to any degree of accuracy I want and I would like to know if they are algebraic or not.","['gap', 'algebraic-number-theory', 'python', 'mathematica', 'abstract-algebra']"
3806783,"How to evaluate $ \int_0^1 \frac{\ln(x+\sqrt{1-x^2})}{\sqrt{1+x^2}} \, \mathrm{d}x $","How can I evaluate $$ \int_{0}^{1} \frac{\ln(x+\sqrt{1-x^2})}{\sqrt{1+x^2}} \, \mathrm{d}x $$ U-substitution has not worked for me. Integration by parts, Differentiation under integral sign, Mathematica is not coming up with a solution either. Is there a closed form for this integral? Thank you kindly for your help and time.","['integration', 'definite-integrals']"
3806838,Space of Matrices up to Congruence.,"I am trying to understand congruent matrices and Sylvester's Law of Inertia, and came upon the following space. Let $X$ be the space $M_n(\mathbb{R})$ mod congruence of matrices. Question 1. : Is this space studied in any part of mathematics (if so, does it have a name)? I assume it must be important since it is the space of quadratic forms classified by their indices. Question 2. Is it a manifold? Are there notable topological properties about this space? (Anything interesting about its homology, cohomology, fundamental group etc.)?","['matrices', 'manifolds', 'algebraic-geometry', 'linear-algebra']"
3806848,Symmetric difference of symmetric differences,"I was looking around wikipedia for the symmetric difference when I stumbled across this fact. From the property of the inverses in a Boolean group, it follows that the symmetric difference of two repeated symmetric differences is equivalent to the repeated symmetric difference of the join of the two multisets, where for each double set both can be removed. In particular: $$A△C = (A△B)△(B△C)$$ where $A△C$ is the symmetric difference of A and C. Is $A△C = (A△B)△(B△C)$ true in the general case? I tried proving it, but the amount of terms grew unwieldly for me when I tried reducing everything to unions, complements, and intersections. Is there any proofs available at hand that anyone knows for this identity?",['elementary-set-theory']
3806857,Show that if $3^x+4^y=12^z \Rightarrow z=\frac{xy}{x+y}$,Show that if $3^x+4^y=12^z \Rightarrow z=\frac{xy}{x+y}$ $\textbf{My Attempt}$ $$3^x+4^y=12^z \Rightarrow z=\frac{xy}{x+y}$$ $$\Rightarrow \log(3^x4^y)=\log(12^z)\Rightarrow \log \big(\frac{3^x4^y}{12^z}\big)=0\Rightarrow \frac{3^x4^y}{12^z}=1 \Rightarrow 3^x4^y=12^z$$ $$\Rightarrow 3^x4^y=3^x+4^y$$ How do you find what $z$ equals from this? Any help would be appreciated.,"['algebra-precalculus', 'exponential-function', 'examples-counterexamples']"
3807002,Paradox? law of large numbers vs option theory,"Flip a fair coin: if you get heads you win $120$ , if you get tails you get $0$ .
How much would you pay to play this game? $60$ right ? Now let's consider the following situation:
You have an asset S that has value $S_0= 100$ at $t=0$ . At time $t=1$ , $S$ changes value to $S_1=200$ with probability $\frac{1}{2}$ or $S_1=40$ with probability $\frac{1}{2}$ . How much would you pay for an option that pays $max(S_1-80, 0)$ ? If you work out the math, you will find that the options value is $45$ . But if you take a step back, this situation is identical to the first one (flipping the coin), there is a $50-50$ chance of winning $120$ or $0$ . So how much should someone pay to play any of these two games, $45$ or $60$ ?
According to option theory, if you pay $50$ you will lose money but that not really the case, especially if we repeat this game a big number of times.  How to reconcile these two situations? does it really make sense to price options the way we do in practice? If the price is $50$ , it seems that everyone is happy, the option dealer who can sell an option worth $45$ at $50$ and make $5$ as a profit and the gambler who on average is winning an extra $10$ per game. How can this be?","['gambling', 'martingales', 'finance', 'probability-theory']"
3807022,Solve ODE $yy'y''=(y')^3+(y'')^2$,"Solve ODE $yy'y''=(y')^3+(y'')^2$ . Attempt . $y=0$ is clearly a solution. If $y\neq 0$ , then ODE becomes $$y=\frac{(y')^2}{y''}+\frac{y''}{y'}.$$ Let $y'=u$ , so by differentiating with respect to variable $x$ we derive an equation of the form $y'=f(y',y'',y''')$ , i.e. $u=f(u,u',u'')$ (non linear, i guess). Am I on the right path? Is there an algorithm for solving this category (if so) of equations? Thanks in advance for the help.",['ordinary-differential-equations']
3807027,Integrate $ \int \frac{1}{\sin^{4}x+\cos^{4}x}dx $,Show that $$ \int \frac{1}{\sin^{4}(x)+\cos^{4}(x)}dx \ = \frac{1}{\sqrt{2}}\arctan\left(\frac{\tan2x}{\sqrt{2}}\right)+C$$ I have tried using Weierstrass substitution but I can't seem to get to the answer... Should I be using the said method or is there another way I can approach the question? Since the integrand evaluates into an arctangent function I am assuming there is some trickery in the manipulation that can get me there. But I just can't seem to see it...,"['integration', 'indefinite-integrals', 'calculus', 'trigonometric-integrals']"
3807067,"What is the largest number of elements in a subset of $\{1,2,3, \ldots, N\}$ such that the sum of every pair of distinct elements in it is different?","The original question from my textbook was What is the largest number of elements in a subset of $\{1,2,3, \ldots, 9\}$ such that the sum of every pair of distinct elements in the subset is different? I got the answer $5$ purely by trying out a few combinations. I would like to know how to solve it mathematically, and also how we can generalise this up till the $N$ th natural number.",['combinatorics']
3807102,"In quadrilateral $ABCD$, $\angle BAC=\angle CAD=2\,\angle ACD=40^\circ$ and $\angle ACB=70^\circ$. Find $\angle ADB$.","Let quadrilateral $ABCD$ satisfy $\angle BAC = \angle CAD = 2\,\angle ACD = 40^\circ$ and $\angle ACB = 70^\circ$ . Find $\angle ADB$ . What I tried Ceva’s Theorem (Trigonometry version) Try to construct some equilateral triangle. Which both failed. Any hints or solutions please. Thanks in advance!","['contest-math', 'euclidean-geometry', 'quadrilateral', 'geometry', 'trigonometry']"
3807208,How to show that any polytope $P$ is spanned by the neighboring edges of any vertex $x$?,"Definitions: A subset $P \subset \mathbb R^n$ is a polytope if it is the convex hull of finitely many points. Let $P \subset \mathbb R^n$ be a polytope. A face is a subset $F\subset P$ of the form $$F=\arg\max\{cx : x \in P\}$$ for some $c \in \mathbb R^n$ .
The dimension of a face is the dimension of its affine hull. A vertex is a zero dimensional face and an edge a one dimensional face. Two vertices $v, w$ are neighbors if their connecting line $\operatorname{conv}(\{v,w\})$ is an edge. Given a vertex $x$ define $$N(x) = \{y \in P: \text{ $y$ is a vertex neighboring $x$}\}$$ as the set of vertices that are neighbors of $x$ , and define $$E(x) = \{y-x: y \in N(x)\}$$ as the set of edge vectors pointing from $x$ to its neighbors. Question: Let $P \subset \mathbb R^n$ be a polytope and let $x$ be a vertex. Let $$E(x) = \{y-x: \text{ $y$ is a vertex neighboring $x$}\}$$ be the set
of vectors that point from $x$ to its neighboring vertices. How can we
show that for any $z \in P$ there exist coefficients $\lambda_v\ge 0$ such that $$ z = x + \sum_{v \in E(x)}\lambda_v v$$ The question can also be phrased as: How to show that the conical hull of $P-\{x\}$ , $$K=\operatorname{cone}(P-\{x\}):=\{\sum_{i=1}^k \alpha_i (z_i-x): z_i \in P, \alpha_i\ge0, k =1,2\dots, \}$$ is generated by the edge vectors $E(x)$ ? That is, show that $$K=\{\sum_{y \in N(x)} \alpha_y (y-x): \alpha_i\ge0 \}.$$ See also example and images below. I think Farkas' Lemma should lead to the answer somehow, but so far I've had no success in my proof attempts. Example: Consider $\mathbb R^2$ and let $P$ be the polytope that is the convex hull of the points $(0,0), (0,1), (1,0)$ . If we take the vertex $x=(0,0)$ then $N(x) = \{(0,1), (1,0)\} = E(x)$ and the set of vectors that are nonnegative linear combinations of elements of $E(x)$ is $\mathbb R^2$ . In particular, any $z \in P$ can be expressed as a nonnegative linear combinations of elements of $E(x)$ . Here is an image (the shaded region is the set of points $z = x + \sum_{v \in E(x)}\lambda_v v$ for some nonnegative $\lambda_v$ ): Here are two more images showing the idea for different polytopes:
A polytope in $\mathbb R^2$ : A polytope in $\mathbb R^3$ :","['polytopes', 'geometry', 'linear-programming', 'linear-algebra', 'discrete-mathematics']"
3807214,Why is the Stone space of a Boolean algebra compact?,"Quick introduction:  I left mathematics about $30$ years ago to begin law school and pursue a career as a lawyer.  I've reacquired the itch, and I've been very slowly going through Set Theory:  An Introduction to Independence Proofs by Kenneth Kunen ( $1980$ edition). After about $18$ months, I'm working my way through Chapter $2$ and I've reached the discussion of Martin's Axiom.  I'm looking specifically at Theorem $3.4$ , which asserts a number of equivalent conditions to Martin's Axiom. Kunen sets up the proof by using the Stone space of a complete Boolean algebra $\mathscr B$ .  The Stone space is a new concept for me.  I think the definition Kunen states is standard -- the points of the space are the algebra's ultrafilters, and a basis for the topology is $\{ U_p \mid p \in \mathscr B \setminus \{ \mathbb 0 \} \}$ , where $U_p$ is the set of all ultrafilters containing $p \in \mathscr B$ . I see why this set is a basis for a topology.  Kunen asserts that it's a compact, totally disconnected Hausdorff space.  I see why it's Hausdorff and that each set in the basis is clopen.  I'm having trouble seeing why the Stone space is compact.  (Kunen asserts that we don't need the algebra to be complete but I'm willing to use the fact if it helps.) I'm trying to prove that a collection of closed sets with the finite intersection property has non-empty intersection.  I think that any closed set turns out to be an arbitrary intersection of basic open sets, so we can assume without loss of generality that the closed sets in our collection are in fact basic open sets, $U_{p_\alpha}$ .  Therefore, if the collection has the f.i.p., the various $p_\alpha$ must be pairwise compatible.  But I don't think that gets me anywhere because the infimum of the $p_\alpha$ still can be $\mathbb 0$ . So why does the Stone space of a (complete) Boolean algebra have to be compact?  Thanks for helping me understand this.","['boolean-algebra', 'general-topology', 'compactness']"
3807240,For $\alpha\in(0^\circ;90^\circ)$ simplify $\sin^2\alpha+\tan^2\alpha+\sin^2\alpha.\cos^2\alpha+\cos^4\alpha$,For $\alpha\in(0^\circ;90^\circ)$ simplify $\sin^2\alpha+\tan^2\alpha+\sin^2\alpha\cdot\cos^2\alpha+\cos^4\alpha.$ My try: $\sin^2\alpha+\tan^2\alpha+\sin^2\alpha\cdot\cos^2\alpha+\cos^4\alpha=\sin^2\alpha+\dfrac{\sin^2\alpha}{\cos^2\alpha}+\sin^2\alpha\cdot\cos^2\alpha+\cos^4\alpha=\dfrac{\sin^2\alpha\cdot\cos^2\alpha+\sin^2\alpha+\sin^2\alpha\cdot\cos^4\alpha+\cos^6\alpha}{\cos^2\alpha}.$ This doesn't seem to help much.,['trigonometry']
3807263,"Prove that for n ≥ 3, $K_n$ is the union of cycles $C_3,\ldots,C_{n − 1}$, a path of length 2 and an edge.","We managed to prove that $K_n$ and the union of cycles $C_3$ to $C_{n − 1}$ have same number of edges: $ n \choose 2 $ = $\sum_{1}^{n-1}k = \frac{n(n-1)}{2}$ We know we need to prove it by induction, but the priority right now is to understand whether this condition (same number of edges) is sufficient to reply the questions. How can we arrange the cycles in a specific way without ""breaking the rings"", to make the complete graph $K_n$ ? I read the following post but I don't really understand the proof: $K_n$ for odd $n$ $ϵ$ $Z_+$ is a disjoint union (of edges) of collections of Hamiltonian cycles","['graph-theory', 'discrete-mathematics']"
3807267,"If $(x_i^2-x_{i+2}x_{i-1})(x_{i+1}^2-x_{i+2}x_{i-1})\leq 0$ for all $i=1,2,3,4,5$ (indices considered mod $5$), then $x_1=x_2=x_3=x_4=x_5 $.","Question. Let $x_1,x_2,x_3,x_4,x_5>0$ . If $$(x_1^2-x_3x_5)(x_2^2-x_3x_5)≤0,$$ $$(x_2^2-x_4x_1)(x_3^2-x_4x_1)≤0,$$ $$(x_3^2-x_5x_2)(x_4^2-x_5x_2)≤0,$$ $$(x_4^2-x_1x_3)(x_5^2-x_1x_3)≤0,$$ and $$(x_5^2-x_2x_4)(x_1^2-x_2x_4)≤0,$$ then prove that the only solution to these inequalities is $x_1=x_2=x_3=x_4=x_5=a $ where $a$ is some real number. What I tried Initially I assumed that $0<x_1≤x_2≤x_3≤x_4≤x_5$ Using the assumption that $0<x_1≤x_2≤x_3≤x_4≤x_5$ and the given inequality $$(x_1^2-x_3x_5)(x_2^2-x_3x_5)≤0$$ We get $$x_1^2≤x_3x_5$$ and $$x_2^2≥x_3x_5$$ Upon using all inequalities in a similar way, it becomes clear that there is a contradiction unless $x_1=x_2=x_3=x_4=x_5=a $ is true So initially I thought I had proved it. Since all the inequalities seemed to be symmetric for $x_1,x_2,x_3,x_4$ and $x_5$ , it did not matter that I had assumed $0<x_1≤x_2≤x_3≤x_4≤x_5$ to prove it. However on closer inspection, I realised that the equations aren't truly symmetric. For example we have $(x_1^2-x_3x_5)$ and $(x_1^2-x_2x_4)$ but not $(x_1^2-x_3x_2)$ and other such terms used in the inequalities. I wasn't able to figure out if this means that I cannot assume $0<x_1≤x_2≤x_3≤x_4≤x_5$ . So I would like to know 1)Is my assumption is valid? 2)If it isnt valid (or even if it is), what is an alternate solution to this problem? Thank you so much in advance Regards","['contest-math', 'algebra-precalculus', 'systems-of-equations', 'inequality']"
3807268,Proving that inequality holds under condition.,"Let $a$ and $b$ be positive numbers. Prove that inequality $$\frac{ax+by}{2} \leqslant \sqrt{\frac{ax^2+by^2}{2}}$$ holds for all real $x$ and $y$ only and only if $a+b \leqslant2$ Problem needs to be done using ""basic"" algebraic methods. I tried expanding this into form $$2ax^2+2by^2-a^2x^2-b^2y^2-2abxy \geqslant 0$$ and then take oustide parenthesis $2-a-b$ . Inequalities between means did not help either.
Can you give me some clues?","['algebra-precalculus', 'inequality']"
3807352,Integration of functions with norm,"Let $\|\cdot\|$ be a norm on $\mathbb{R}^2$ . Prove that $\exists c>0$ such that for every borelian function $f: \mathbb{R} \rightarrow \mathbb{R},$ $$\int_{\mathbb{R}^2}f(\|x\|)dx=c\int_{\mathbb{R}^2}f(|x|)dx,$$ where $|\cdot |$ is the Euclidian norm. Prove the following extension to $\mathbb{R}^d$ : $$\exists c'>0;\int_{\mathbb{R}^d}f(\|x\|)dx=c'\int_{0}^{+\infty}x^{d-1}f(x)dx$$ Trying the change $x=r\cos(\theta),y=r\sin(\theta)$ (from $]0,2\pi[ \times ]0,+\infty[ \to \mathbb{R}^2-([0,+\infty[ \times$ { $0$ } $)$ , so $$\int_{0}^{2\pi}\int_{0}^{+\infty}rf(r\|(\cos(\theta),\sin(\theta))\|)drd\theta=\int_{\mathbb{R}^2}f(\|x\|)dx,$$ $$\int_{\mathbb{R}^2}f(|x|)dx=2\pi\int_0^{+\infty}rf(r)dr,$$ So how to obtain $c$ ? Is there another way to prove it? How can we prove the extension for this result?","['integration', 'measure-theory', 'real-analysis']"
3807356,Radon Nikodym property (Banach valued) is separably determined $\sigma-$finite case,"Im reading Analysis in Banach spaces ( https://www.springer.com/gp/book/9783319485195 ) and I was trying to provide a different demostration of lemma 1.3.17, which is used in order to prove that the Radon Nikodym property is separably determined (theorem 1.3.18). The lemma says:
Suppose that X has the RNP with respect to a $\sigma-$ finite measure space $(S,\mathcal{A},\mu)$ , and let $\mathcal{B}$ be a sub $-\sigma-$ algebra of $\mathcal{A}$ . Then X has the RNP
with respect to $(S,\mathcal{B},\mu_{\restriction{\mathcal{B}}})$ .
They prove it using results of other section, so I would like to prove it in a self-contained form.
Modifying a finite version from these lectures notes ( http://wwwarchive.math.psu.edu/huff/ , chapter 7, lemma 7.5.1) it's easy to prove it if the sub $-\sigma-$ algebra is $\sigma-$ finite:
Lemma:
Let $(\Omega,\Sigma,\mu)$ be a finite measure space. For every sub $-\sigma-$ algebra $\Sigma_0$ of $\Sigma$ there exists a unique function $E_0: \mathcal{L}^{1}(\mu,X)\longmapsto \mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}},X)$ such that $$\displaystyle\int_{A}f d\mu = \displaystyle\int_{A} (E_0 f) d\mu$$ for every $A\in\Sigma_0$ and every $f\in \mathcal{L}^{1}(\mu,X).$ In addition, $E_0$ it's a  bounded and linear operator of $||E_0||\leq1.$ Unicity follows from previous result. Let $A\in\Sigma$ be fixed, then the map $E \longmapsto \mu(A\cap E)$ with $E\in\Sigma_0,$ defines a finite positive measure $\Sigma_0$ which is absolute continuous with respect $\mu_{\restriction{\Sigma_0}}.$ By Radon-Nikodym theorem there exists a $g_A \in \mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}})$ such that $$\mu(A\cap E)=\displaystyle\int_{E}g_A d\mu \hspace{0.2cm} \text{ for all } E\in\Sigma_0.$$ For a simple function $f=\displaystyle\sum_{j=1}^{n}\alpha_j \chi_{A_j},$ we define $$E_0(f)=\displaystyle\sum_{j=1}^{n}\alpha_j g_{A_j}.$$ It's easy to see that $E_0$ it's linear and continuous over the simple functions with $||E_0||\leq1$ and that $\displaystyle\int_{E}f d\mu = \displaystyle\int_{E} (E_0 f) d\mu$ for all $E\in\Sigma_0.$ Therefore it admits a unique externsion to a bounded and linear operator $E_0:\mathcal{L}^{1}(\mu,X) \longmapsto\mathcal{L}^{1}(\mu_{\restriction{\Sigma_0}},X)$ with $||E_0||\leq1.$ If $f_n$ it's a sequence of simple functions such that $||f_n-f||_1 \xrightarrow[n\to\infty]{}0$ for $f\in \mathcal{L}^{1}(\mu,X),$ thens $||E_0f_n-E_0f||_1 \xrightarrow[n\to\infty]{}0$ and for all $E\in\Sigma_0$ $$\displaystyle\int_{E}(E_0 f)d\mu = \displaystyle\lim_{n\to\infty}\displaystyle\int_{E}(E_0 f_n)d\mu=\displaystyle\lim_{n\to\infty}\displaystyle\int_{E}f_n d\mu=\displaystyle\int_{E}f d\mu.$$ Also I know that if $(\Omega,\Sigma,\mu)$ is a $\sigma-$ finite measure space and $\mathcal{B}$ is a sub $-\sigma-$ algebra, we can find a disjoint decomposition $\Omega=S_0 \cup S_1$ such that the restriction of $\mu$ to $\mathcal{B}_{\restriction{S_0}}$ is $\sigma-$ finite and the restriction of $\mu$ to $\mathcal{B}_{\restriction{S_1}}$ is purely infinite.
I doubt that it's possible, because the authors would have done it, but I have to prove theorem 1.3.18, and this lemma is used, so I need to prove it. Other solution would be see the way it's used in the theorem and see if I need to prove something easier only; in fact i think we can assume the sub $-\sigma-$ algebra is countably generated, but cant see how that helps. Anyway, any comments would help. Thanks a lot in advance.","['integration', 'banach-spaces', 'measure-theory', 'real-analysis', 'functional-analysis']"
3807376,Eigenvalues of Complex Stucture in a Complexified Vector Space,"For context, this question comes from my reading of Kobayashi+Nomizu's differential geometry book, volume 2 (pages 116-117). Given a real vector space $V$ with $\mathrm{dim}(V)=2n$ , a complex structure is a linear endomorphism satisfying $J^2=-1$ . One can also define the complexification of a vector space as $V^{\mathbb{C}} = V \otimes_{\mathbb{R}} \mathbb{C}$ . Now, $J$ extends naturally to a complex endomorphism on the complexified space, and (Kobayashi and Nomizu claim that) it has eigenvalues $\pm i$ . I have some confusions about this: Is it true that the natural extension of $J$ is just defined by $J(V\otimes z) = J(V)\otimes z$ ? More crucially, the complexified vector space is treated as a tensor product over the reals, so isn't the coefficient field of this new space also the reals? Isn't, then, the characteristic polynomial viewed as a polynomial over $\mathbb{R}$ (and thus cannot have imaginary roots)? My intuition is to treat application of $J$ as multiplication by $i$ in the original space, but the complexified space offers a bona fide multiplication by $i$ , and I'm not sure how these two interact. What would be an example of a $v\in V^{\mathbb{C}}$ such that $Jv = -iv$ ? What does $Jv=iv$ actually mean? I've tried to answer these questions for $V=\mathbb{R}^2$ with the canonical structure but I'm coming up empty handed. I greatly appreciate your time and help!","['almost-complex', 'complex-geometry', 'linear-algebra', 'differential-geometry']"
3807399,Two inequalities for proving that there are no odd perfect numbers?,"Let $n$ be a natural number. Let $U_n = \{d \in \mathbb{N}| d|n \text{ and } \gcd(d,n/d)=1 \}$ be the set of unitary divisors, $D_n$ be the set of divisors and $S_n=\{d \in \mathbb{N}|d^2 | n\}$ be the set of square divisors of $n$ . The set $U_n$ is a group with $a\oplus b  := \frac{ab}{\gcd(a,b)^2}$ . It operates on $D_n$ via: $$ u \oplus d := \frac{ud}{\gcd(u,d)^2}$$ The orbits of this operation ""seem"" to be $$ U_n \oplus d = d \cdot U_{\frac{n}{d^2}} \text{ for each } d \in S_n$$ From this conjecture it follows (also one can prove this directly since both sides are multiplicative and equal on prime powers): $$\sigma(n) = \sum_{d\in S_n} d\sigma^*(\frac{n}{d^2})$$ where $\sigma^*$ denotes the sum of unitary divisors. Since $\sigma^*(k)$ is divisible by $2^{\omega(k)}$ if $k$ is odd, where $\omega=$ counts the number of distinct prime divisors of $k$ , for an odd perfect number $n$ we get (Let now $n$ be an odd perfect number): $$2n = \sigma(n) = \sum_{d \in S_n} d \sigma^*(\frac{n}{d^2}) = \sum_{d \in S_n} d 2^{\omega(n/d^2)} k_d $$ where $k_d = \frac{\sigma^*(n/d^2)}{2^{\omega(n/d^2)}}$ are natural numbers.
Let $\hat{d}$ be the largest square divisor of $n$ . Then: $\omega(n/d^2)\ge \omega(n/\hat{d}^2)$ . Hence we get: $$2n = 2^{\omega(n/\hat{d}^2)} \sum_{d \in S_n} d l_d$$ for some natural numbers $l_d$ . If the prime $2$ divides not the prime power $2^{\omega(n/\hat{d}^2})$ , we must have $\omega(n/\hat{d}^2)=0$ hence $n=\hat{d}^2$ is a square number, which is in contradiction to Eulers theorem on odd perfect numbers. So the prime $2$ must divide the prime power $2^{\omega(n/\hat{d}^2})$ and we get: $$n = 2^{\omega(n/\hat{d}^2)-1} \sum_{d \in S_n} d l_d$$ with $l_d = \frac{\sigma^*(n/d^2)}{2^{\omega(n/d^2)}}$ . Hence the odd perfect number, satisifies: $$n = \sum_{d^2|n} d \frac{\sigma^*(n/d^2)}{2^{\omega(n/d^2)}}=:a(n)$$ Hence an odd perfect number satisifies: $$n = a(n)$$ Edit :
This equation is wrong for odd perfect numbers. So my idea was to study the function $a(n)$ , which is multiplicative on odd numbers, on the right hand side and what properties it has to maybe derive insights into odd perfect numbers. Conjecture: For all odd $n \ge 3$ we have $a(n)<n$ . This would prove that there exists no odd perfect number. This conjecture could be proved as follows:
Since $a(n)$ is multiplicative, it is enough to show that for an odd prime power $p^k$ we have $$a(p^k) < p^k$$ The values of $a$ at prime powers are not difficult to compute and they are: $$a(p^{2k+1})= \frac{p^{2(k+1)}-1}{2(p-1)}$$ and $$a(p^{2k}) = \frac{p^{2k+1}+p^{k+1}-p^k-1}{2(p-1)}$$ However, I am not very good at proving inequalities, so: If someone has an idea how to prove the following inequalities for odd primes $p$ that would be very nice: $$p^{2k+1} > \frac{p^{2(k+1)}-1}{2(p-1)}, \text{ for all } k \ge 0$$ and $$p^{2k} > \frac{p^{2k+1}+p^{k+1}-p^k-1}{2(p-1)}, \text{ for all } k \ge 1$$ Thanks for your help!","['number-theory', 'inequality', 'perfect-numbers']"
3807464,Show that $|α(t)|$ is a nonzero constant if and only if $α(t)$ is orthogonal to $α'(t)$ for all $t ∈ I$ .,"Let $\alpha: I → \mathbb{R^3}$ be a parametrized curve, with $α'(t) \neq 0$ for all $t \in I$ . Show
that $|α(t)|$ is a nonzero constant if and only if $α(t)$ is orthogonal to $α'(t)$ for all $t ∈ I$ . my attempt: For the second implication: Suppose that $ \alpha (t) \cdot \alpha'(t) = 0 $ . I should show that $ | \alpha (t) | = C \neq 0 $ , for all $ t \in I $ , where $ C $ is a constant. Now $( | \alpha (t) |^2)' = 2 \alpha (t) \alpha'(t) = 0 $ . This implies that $ | \alpha (t)| = C$ for some constant $C$ . However I could not show that that $ C \neq 0 $ . I need suggestions for the other implication please.",['differential-geometry']
3807488,Finding the limit: $\lim_{x\to \infty}\frac{1}{2}x\sin {\frac{180(x-2)}{x}}$,"While investigating a problem, I came across a function: $$f(x) = \frac{1}{2}x\sin {\frac{180(x-2)}{x}}$$ When looking at the function in Desmos (I was checking my proof), I discovered that $$\lim_{x\to \infty}\frac{1}{2}x\sin {\frac{180(x-2)}{x}} = \pi$$ I double-checked Wolframalpha, and this limit is true. The only issue is that I can't seem to prove it by hand, and I'm really interested as to how $\pi$ pops out of nowhere. Please note I am working in degrees, so it is not 180 radians in the sin function. I would really appreciate it if someone could explain a solution.","['limits', 'calculus', 'pi', 'limits-without-lhopital']"
3807550,"In a finite commutative ring , every prime ideal is maximal?","I am stuck in a true/false question. It is In a finite commutative ring, every prime ideal is maximal. The answer says it's false. Well what I can say is (Supposing the answer is right) $(1)$ The ring can't be Integral domain since finite integral domain is a field. $(2)$ There can't be unity in the ring since in that case the result would be true.(By the Theorem that if $R$ is a commutative ring with unity then an ideal $I$ is prime iff $R/I$ is Integral Domain) $(3)$ All the elements are zero divisors since if there is at least one non- zero divisor, there will be a unity and so $(2)$ would follow. So at the end, I am in search of a finite commutative with all elements as zero -divisors, having no unity and obviously a prime ideal in it which is not maximal. What kind of strange looking ring is this (if possible) ? Any hints??","['finite-fields', 'rngs', 'maximal-and-prime-ideals', 'ring-theory', 'abstract-algebra']"
3807571,Why use random variables instead of probability spaces,"When talking about various ways to model something probabilistically, many authors prefer to use random variables, instead of probability distributions.  Of course, this difference is more of a point of view, than of actual mathematical substance - yet I'm very much interested in why the random variables point-of-view is assumed? Let me elaborate below on this. It seems to me that this comes from not being fully explicit and formal, when building your model - since if you would be, that you would see that using random variables is actually very artificial and using the probability distribution is actually much more natural. Consider the following problem: Suppose we have a vector $x\in\mathbb{R}^{p}$ that we interpret as
the visible attributes of individual. For example, $x$ might represent
a loan applicants age, gender, race, and credit history.
We consider the problem of modeling whether we should give a person
represented by $x$ a loan; let $y\in\{0,1\}$ represent the target
of this prediction, i.e. whether an individual will have defaulted
on a loan he received ( $y=0$ ) or repaid it according to his contract
( $y=1$ ). To formalize this problem, we can  define random variables $X$ and $Y$ that take on values $X=x$ and $Y=y$ for an individual drawn randomly
from the population of interest (e.g., the population of ).
We define the true risk \begin{equation}
r(x)=Pr(Y=1|X=x)\ \ (1).
\end{equation} Then the problem is how to estimate this risk from data, yadda, yadda. The issue I mention above is related to the formulation (not the solution
or theoretical framework) of this problem. Usually the above description
is all that you get! Let us investigate how we can make it even more precise: If we begin to be more explicit, in order to even introduce random
variables $X,Y$ we need a sample space. Because these random variables
appear in the expression (1), which explicitly is $$
r(x)=Pr(\{\omega\in\Omega:Y(\omega)=1\}|\{\omega\in\Omega:X(\omega)=x\}),
$$ the random variables furthermore need to be defined on the same sample
space. We could pick $\Omega:=\mathbb{R}^{p}\times\{0,1\}$ as a suitable
candidate, where a distribution $\mathcal{D}$ on it models how likely
it is that a certain individual is drawn from it. We could then define $X:\Omega\rightarrow\mathbb{R}^{p}$ as the projection onto the first $p$ components and $Y:\Omega\rightarrow\{0,1\}$ as the projection
onto the last component. By doing so, we have given (1) a concrete
meaning. But defining the random variables like this is rather cumbersome;
since we already needed to introduce $\Omega$ and $\mathcal{D}$ to even talk about random variables, we could just use these two ingredients
to define the true risk by \begin{equation}
r(x)=Pr(\{\omega\in\Omega:\omega_{p+1}=1\}|\{\omega\in\Omega:\omega_{1,\ldots,p}=x\}) \ \ (2),
\end{equation} where subscripts indicate the $p$ -th coordinate. But somehow a formulation as in (2) is very rarely used. My question
is: Why does the community tend to prefer a vague way of defining
random variables, that, if made precise, is actually more tedious
to set up(as I have just shown) than using the formulation (2) ?","['convention', 'soft-question', 'probability-theory', 'random-variables']"
3807577,"If $f$ is continuous on $[1,8]$ and some values of $f$ are given, which of the following statement concerning the existence of solutions must be true?","I have graphed this function, and it seems to be a parabola like figure. However, I am not sure what I am supposed to do. Am i supposed to construct the function? Or am I missing the point here? A small hint to guide me in the right direction would be appreciated! :D","['algebra-precalculus', 'graphing-functions']"
3807626,"Solving the system $\cos^2x+\sin^2y=1$, $\cos y\sin y=\cos x\sin x$","I have a set of trigonometric equations as follows: $$\cos^2(x)+\sin^2(y)=1$$ $$\cos(y)\sin(y)=\cos(x)\sin(x)$$ I have tried to plot these two graphs on desmos and it seems that two functions agree on the line $x=y+n\pi$ . However, I don't see any clue in getting this relation and I am hard stuck right now. Could anyone give me some hint on this? Thanks!","['trigonometry', 'systems-of-equations']"
3807648,Deriving a Möbius transformation specified by three points,"A Möbius transformation is given by $$f(z)=\frac{az+b}{cz+d}$$ with parameters $a$ , $b$ , $c$ , and $d$ . The Wikipedia article provides rules for finding these parameters based on three points $z_1$ , $z_2$ , and $z_3$ and their images $w_1$ , $w_2$ , and $w_3$ . It is my goal to understand how we can derive the equations which yield the parameters. Möbius transformations preserve the cross-ratio , so I assume we start with the cross-ratios of the original points and their images: $$(z,z_1;z_2,z_3)=(f(z),w_1;w_2,w_3)$$ which can be reformulated as $$\frac{(z-z_2)(z_1-z_3)}{(z_1-z_2)(z-z_3)}=\frac{(f(z)-w_2)(w_1-w_3)}{(w_1-w_2)(f(z)-w_3)}$$ I imagine the solution is obtained by reformulating this equation above somehow to solve for $f(z)$ . But how is this done? I could not find a proper tutorial for this online - most tutorials I find plug in specific points at this stage, but I would like to learn how the general approach is derived.","['complex-analysis', 'mobius-transformation']"
3807695,"How many parallelepipeds can be constructed with vertices $(0,0,0), (0,0,1), (0,1,0), (1,0,0)?$","My friend showed this problem to me after her professor asked this question as a warmup. Most of my issues come with double- or triple-counting the parallelepipeds. The simplest example of this is the cube, you can take a square in the XY, YZ, or XZ planes and translate them $1$ unit, but they're all the same parallelopiped. I grouped the parallelepipeds into those with square bases (I counted ten), $\sqrt 2$ x $1$ bases (I counted twelve, but I'm not sure about double/triple counting here), and what I call ""slanted"" bases with the base parallelogram spanned by $(0,0,1), (0,1,0), (1,0,0)$ , and a fourth vertex. I've found at least two of this kind which are unique. I'm not sure how to multiply by axes in this case because again repeats are annoying. Not too sure how to proceed from here. I have a feeling this is a graph theory question under the hood somehow. I thought the problem was really interesting though. Here's most of my working out so far:","['combinatorics', 'geometry']"
3807708,"Is it possible to solve this identity by ""inspection""?","I was asked to prove the following identity (starting from the left-hand side): $$(a+b)³(a⁵+b⁵)+5ab(a+b)²(a⁴+b⁴)+15a²b²(a+b)(a³+b³)+35a³b³(a²+b²)+70a⁴b⁴=(a+b)^8.$$ I'm trying to solve it by a sort of ""inspection"", but I haven't made it yet. Of course I could try to expand the left-hand polynomial and come to a more recognizable form of $(a+b)^8$ , but of course that would be the hard way (assuming that there is an easy one). As an example of why I am talking of ""inspection"" I can state a similar problem: Show that $$(x+\frac{5}{2}a)⁴-10a(x+\frac{5}{2}a)³+35a²(x+\frac{5}{2}a)²-50a³(x+\frac{5}{2}a)+24a⁴=(x²-\frac{1}{4}a²)(x²-\frac{9}{4}a²).$$ Here by ""inspection"" we can deduce that the left-hand side of the identity is equivalent to $$[(x+\frac{5}{2}a)-a][(x+\frac{5}{2}a)-2a][(x+\frac{5}{2}a)-3a][(x+\frac{5}{2}a)-4a]$$ and then after a few steps come to the the desire result. I would appreciate any help you could give me.","['algebra-precalculus', 'polynomials']"
3807715,A continuous map $f: S^1 \to S^2$ is homotopic to a point,"$f: S^1 \to S^2$ is a continuous map. Prove that $f$ is homotopic to a constant function. Can someone give me a dircetion for proving this result? I think I should use the fact that $S^2$ is simply connected. I was able to prove the result given that $Im(f) \neq S^2$ , but this is not generally true.","['continuity', 'general-topology', 'homotopy-theory', 'algebraic-topology']"
3807744,An Optimal Strategy for a Coin Flipping Game,"Consider a fair coin, tossed 100 times to create a sequence of $H$ s and $T$ s. A participant is allowed to ask 1 yes or no question (e.g. was the first coin flip heads?), then plays a game where he tries to guess all 100 coins. The participant is awarded $\$1$ for every coin guessed correctly, and loses $\$1$ for each incorrect guess. Find and prove an optimal strategy for the player. I have a hunch that the optimal strategy may be to ask ""Were there more heads than tails?"" and then, depending on the answer, proceed to guess either all $H$ s or all $T$ s. With this strategy, the player is guaranteed nonnegative earnings, and I believe the expected value is $$\sum_{i=0}^{50}{\binom{100}{i}\left(\frac{1}{2}\right)^{99}(100-2i)} \approx \$7.96$$ I've confirmed the expected value with a Monte-Carlo simulation in Python, but I'm having trouble proving that this is optimal. My best attempt to translate this into more rigorous mathematics is to consider the yes/no question as a partition. Let $X$ be the set of $2^{100}$ possible sequences and $x$ be the sequence rolled. A yes/no question will always partition the set into two. Suppose that set $A$ is the set of all sequences in which the answer to our question is ""yes"", then the expected value of our game would be $$E[G] = \frac{|A|}{2^{100}}E[G|x\in A]\space + \left(1-\frac{|A|}{2^{100}}\right)E[G|x \notin A],$$ where G is the expected value of the game, playing with some optimal strategy. I've also made the note that given any specific set $A$ , $x \in A$ implies there is an optimal (but not necessarily unique) guess. For instance, if we know that there are more heads than tails, a sequence of 100 $H$ s is an optimal guess.","['expected-value', 'probability']"
3807766,Is there any guarantee of maximum distance to next prime?,"When watching the Numberphile video about Highly Composite Number , I spotted something that aroused some of my doubts. One of properties suggested by Ramanujan was that the highly composite number's powers of prime factors are in order of decreasing order, with the highest prime factor almost always (with exactly 2 exceptions: 4 and 36.) appearing with power of 1. It seems to me this assertion hinges upon the next prime after the last being lower than the square of the previous one. While π(N) shows the average distance between the consecutive primes would be significantly lower than between the prime and it's square, as I understand it's more of a probabilistic thing, and while very unlikely, it's not guaranteed next prime will be found within pretty much any finite distance of the prior one. So is this property of highly composite numbers just a conjecture based on dwindling probability of such a gap between primes ever appearing, or is there some solid proof to it?","['number-theory', 'prime-numbers']"
3807792,Applying CLT to random variable made up of two sequences of iid random variables,"2nd year stats hw Q: Suppose you have a sequence $X_1, X_2, ...$ of iid random variables with mean $E(X_1)=\mu_X$ and variance $Var(X_1)=\sigma^2_X$ and another sequence $Y_1, Y_2, ...$ of iid random variables with mean $E(Y_1)=\mu_Y$ and variance $Var(Y_1)=\sigma^2_Y$ . For each $n=1,2,...$ let $A_n$ be the random variable $$\frac{\sqrt n}{\sqrt {\sigma^2_X+\sigma^2_Y}}[\bar X_n - \bar Y_n - (\mu_X - \mu_Y)]$$ where $\bar X_n = \sum_{i=1}^n \frac{X_i}{n}$ and $\bar Y_n = \sum_{i=1}^n \frac{Y_i}{n}$ . Show that, in distribution, $A_n$ converges to $N(0,1)$ as $n \to \infty$ . I know that this will require use of the central limit theorem and when I asked my lecturer for help he just reminded me that the $X$ variables are independent to the $Y$ variables, but I don't know how to apply this. Please help - even if its just pointing me in the right direction!","['statistics', 'central-limit-theorem', 'probability-theory', 'weak-convergence']"
3807798,Question about paragraph on differentiation in Spivak's Calculus Chapter 10,"I was finishing up Chapter 10 of Spivak's Calculus when I stumbled upon the following remark which I have trouble wrapping my head around. The paragraph in question went something like this: lt is often tempting, and seems more elegant, to write some of
the theorems in this chapter as equations about functions, rather than about their
values. Thus Theorem 3 might be written $$
(f+g)' = f' + g'
$$ For which he follows up with: Strictly speaking, these equations may be false, because the functions on the left-
hand side might have a larger domain than those on the right. I feel like I am missing the main message here, and do not quite understand what he means by thinking about the theorems (mainly, the rules for differentiation) in terms of values instead of functions. Does he mean that such an 'expansion' only makes sense if a specific value is attached to a function? e.g. $(f(a) + g(a))' = f'(a) + g'(a)$ as illustrated earlier in the chapter. I have also been unsuccessful in generating some examples to help me break down this point better. Any pointers would be greatly appreciated!","['calculus', 'functions', 'real-analysis']"
3807824,Is this discrete analogue of Fatou's lemma valid,"I was curious if this discrete analog of Fatou's lemma is valid: \begin{align}
\sum_{j=1}^\infty \liminf_{k \rightarrow\infty} a_j(k) \leq \liminf_{k \rightarrow\infty} \sum_{j=1}^\infty a_j(k) ,
\end{align} where $a_j(k)$ is a doubly indexed sequence of real numbers. Does it hold in the general real case? What if $a_j(k) \geq 0 $ , does it hold then? Thanks to all helpers.","['measure-theory', 'lebesgue-integral', 'sequences-and-series']"
3807863,Every complex matrix can be written as the sum of a Hermitian and a skew-Hermitian matrix. What is this called?,"Given $T\in\mathrm M_n(\mathbb C)$ , there exists $A$ and $B$ such that $A$ is Hermitian and $B$ is skew-Hermitian such that $T=A+B$ . Or, equivalently, $T=A+iB$ where $A$ and $B$ are both Hermitian matrices. I wonder what is this decomposition called. My friend recalls it is the Toeplitz decomposition, but I barely find any information on that. EDIT: The decomposition described above appears in many texts. To illustrate, here is an excerpt from page 145 of this article : In [52], Kaluznin and Havidi approach the problem of unitary similarity for $m$ -tuples, $(A_1,A_2,\ldots,A_n)$ and $(B_1,B_2,\ldots,B_n)$ , of square matrices from a more geometric point of view. First note that when we decompose each matrix into its Hermitian and skew Hermitian components , $A_j = H_j + iK$ , and $B_j = L_j + iM_j$ , where $H_j$ , $K_j$ , $L_j$ , and $M_j$ are all Hermitian, a unitary matrix $U$ satisfies $U^*A$ , $U = B_j$ if and only if $U^*H_jU = L_j$ and $U^*K_jU = M_j$ . Thus, we may replace each $A_j$ and $B_j$ with a pair of Hermitian matices and study the $2m$ -tuples in which every entry is Hermitian.","['matrices', 'linear-algebra', 'terminology']"
3807975,If two lines make an angle $\alpha$ on their intersection. Prove that $\cos\alpha = \frac{a_1a_2+b_1b_2}{\sqrt{a_1^2+b_1^2}\sqrt{a_2^2+b_2^2}}$,"If two lines $a_1x+b_1y+c_1=0$ and $a_2x+b_2y+c_2=0$ make an angle $\alpha$ on their intersection. Prove that $$\cos\alpha = \frac{a_1a_2+b_1b_2}{\sqrt{a_1^2+b_1^2}\sqrt{a_2^2+b_2^2}}$$ I have seen a question with $\sin \alpha$ instead, but the answer uses the $\tan \alpha$ , if the easier way of proving this is using $$m_1=-\frac{a_1}{b_1},m_2=-\frac{a_2}{b_2}$$ $$\tan\alpha=\left|\frac{m_1-m_2}{1+m_1m_2}\right|$$ I would like to have the logic/demonstration of it too, as I can't see it, I am trying to prove this using geometry and trigonometry, not vectors","['trigonometry', 'geometry']"
3808014,"$f(x),g(x)$,2 quadratic polynomials:$|f(x)|тЙе|g(x)|тИАx тИИ R$. Find the number of distinct roots of equation $h(x)h''(x)+(h'(x))^2=0$ if $h(x)=f(x)g(x)$","Question: If $f(x)$ and $g(x)$ are two distinct quadratic polynomials and $|f(x)|тЙе|g(x)|$ $тИА$ $x тИИ R$ .
Also $f(x)=0$ has real roots.
Find the number of distinct roots of equation $$h(x)h''(x)+(h'(x))^2=0$$ where $h(x)=f(x)g(x)$ What I tried: I attempted to find $h(x)h''(x)+(h'(x))^2=0$ in terms of $f(x)$ and $g(x)$ using $h(x)=f(x)g(x)$ , upon which I got the following equation, $$g(x)^2[f(x)f''(x)+(f'(x))^2]+f(x)^2[g(x)g''(x)+g'(x)^2]+4f(x)f'(x)g(x)g'(x)=0$$ I don't know how to proceed, or where to use the fact that $|f(x)|тЙе|g(x)|$ $тИА$ $x тИИ R$ and that $f(x)=0$ has real roots. I also tried actually using general equations for the quadratic polynomials $f(x)$ and $g(x)$ $$f(x)=a_1x^2+b_1x+c_1$$ $$g(x)=a_2x^2+b_2x+c_2$$ I then tried deducing some information from $|f(x)|тЙе|g(x)|$ , coming to the conclusion that $|a_1|<|a_2|$ , and that $$|\frac{b_1^2}{4a_1}-c_1|>|\frac{b_2^2}{4a_2}-c_2|$$ I found the expressions for $f'(x)$ , $f''(x)$ , $g'(x)$ and $g''(x)$ and plugged them into the equation I had obtained. This lead to a rather complicated degree 6 equation, as one would expect. I've no idea what to do next. Any help or hints are appreciated... Thanks in advance! Regards","['calculus', 'quadratics', 'roots', 'polynomials']"
3808019,Does there exist a non-commutative algebraic structure with the following properties?,"A magnium is a set M with a binary operation $\cdot$ satisfying: $|M| \ge 2$ For all $a$ , $b$ , $c$ $\in M$ , $(a \cdot b) \cdot c = a \cdot (b \cdot c)$ . For all $a$ , $b$ $\in M$ with $a \ne b$ , exactly one of the equations $a \cdot x = b$ and $b \cdot x = a$ has a solution for $x$ in $M$ . For all $a$ , $b$ $\in M$ , the equation $a \cdot x = b$ has a solution for $x$ in $M$ if and only if the equation $y \cdot a = b$ has a solution for $y$ in $M$ . Examples of magniums are the positive real numbers and the non-negative integers under addition. Another example is the set $\{1, 2, 3, ..., 120\}$ under the operation $x \cdot y = \min\{x + y, 120\}$ , which shows that magniums generally do not have the cancellation property. So the question is, is there a non-commutative magnium? Currently I'm trying to think of some two-valued function $f(x, y)$ on $\Bbb{R}$ satisfying $f(x, y) \ge \max\{x, y\}$ that's associative but not commutative, and I'm not coming up with anything good.","['abstract-algebra', 'semigroups']"
3808029,How to handle the integral $\int_{0}^{\infty} \frac{\log(x)}{\sqrt{x^{3}}} e^{-\frac{1}{2b} \frac{(x-a)^{2}}{ax} } dx$,"I have two integrals as parts of a bigger problem. One is, $$
\int_{0}^{\infty}\frac{\log(x)}{\sqrt{x^{3}}}
\exp\left(-\frac{1}{2b} \frac{\left[x - a\right]^{\,2}}{ax}\right)\mathrm{d}x
$$ and another is similar, $$
\int_{0}^{\infty}\frac{\left(x - a\right)^{2}}{x\sqrt{\,{x^{3}}\,}}
\exp\left(-\frac{1}{2b} \frac{\left[x - a\right]^{2}}{ax}\right) dx
$$ I have gotten only as far as simplifying the first one to, $$
\mathrm{e}^{1/b}\int_{0}^{\infty}
\frac{\log(x)}{\sqrt{\,{x^{3}}\,}}
\exp\left(-\left[\frac{x}{2ab}+\frac{a}{2bx}\right]\right) dx
$$ which would also apply to the second one, but I am struggling to go any further.  Any hint on how to start about these would be greatly appreciated. If it helps, the exponential part of the integrals stems from the formula of differential entropy of the inverse Gaussian distribution.","['integration', 'improper-integrals', 'definite-integrals', 'exponential-function']"
3808070,"How to prove that $f: \mathbb{R} \to \mathbb{R}$ is continuous, given that $xy-y^2 \leq f(x)-f(y) \leq x^2-xy$?","Let $f: \mathbb{R} \to \mathbb{R}$ and let $$xy-y^2 \leq f(x)-f(y) \leq x^2-xy$$ I am trying to prove that f is continuous i.e: $$\lim_{x \to x_0}f(x) = f(x_0)$$ To be honest, I barely understand the problem. What I can extract from it is: $$ xy-y^2 \leq x^2-xy \iff \boxed{x^2+y^2 \geq 1} $$ Hence, the inequality holds only $\forall$ outter points of the unit circle, centered at $(0,0)$ . More than that, I can't understand how to show continuity for $f$ , since I don't even know what $f$ is. In order for a function to be continuous the following condition must be satisfied: $$\lim_{x \to x_0} f(x) = f(x_0)$$ Any tips that will help me understand the problem would be greatly appreciated","['limits-without-lhopital', 'continuity', 'calculus', 'functions', 'limits']"
3808111,Prove that if the series is convergent then the law of large numbers hold.,Let $(X_n)_{n \geq 1}$ be a sequence of pairwise independent random variables such that : $$\sum_{n=1}^{\infty} n^{-1} P\left\{\max _{1 \leq m \leq n}\left|\sum_{k=1}^{m}\left(X_{k}-E X_{k}\right)\right|>\varepsilon n\right\}<\infty$$ show that $n^{-1} \sum_{k=1}^{n}\left(X_{k}-E X_{k}\right) \rightarrow 0$ almost surely. I'm fairly certain Borel Cantelli Lemma for pairwise independent random variables is to be used here but I dont know how to get rid of the $n^{-1}$ inside the series.,"['borel-cantelli-lemmas', 'law-of-large-numbers', 'probability-theory']"
3808135,Bochner's theorem using Lévy's theorem,"Bochner's Theorem: If $\varphi : \mathbb{R}^d \to \mathbb{C}$ is positive definite, continuos and $\varphi(0)=1$ then it is the characteristic function of a probability measure, i.e. the Bochner's theorem. Proof: We can prove that if $f \in L^1(\mathbb{R}^d,\mathbb{C})$ then $\int_{\mathbb{R}^d \times \mathbb{R}^d}\varphi(t-s)f(t)\overline{f(s)}dtds \geq 0$ using the fact that $\varphi$ is positive definite. Thus if $f(t)=e^{i \langle t,x \rangle-\varepsilon \|t\|^2}$ we can get that $h_{\varepsilon}(x)=\int_{\mathbb{R}^d}\varphi(u)e^{i \langle u,x \rangle -\frac {\varepsilon}{2}\|u\|^2}du \geq 0$ . If I could compute $\int_{\mathbb{R}^d}h_{\varepsilon}(x)dx$ I would modify $h_{\varepsilon}(x)$ in such a way it is a probability density. Then I would compute the characteristic function of a random variable with this density and I will find that it is $\varphi_{\varepsilon}(u)=\varphi(u)e^{\frac{-\|t\|^2\varepsilon}{2}}$ and I will conclude thanks to Lévy's theorem with $\varepsilon \to 0$ that $\varepsilon$ is a characteristic fucntion of some probability. My question is how can we evaluate $\int_{\mathbb{R}^d}h_{\varepsilon}(x)dx$ ? It is true that $0\leq \int_{\mathbb{R}^d \times \mathbb{R}^d}\varphi(t-s)f(t)\overline{f(s)}dtds =\int_{\mathbb{R}^d}\varphi(u)e^{i \langle u,x \rangle - \frac{\varepsilon \|u\|^2}{2}}(\frac{\pi}{2\varepsilon})^{\frac d 2 }du$ ?","['integration', 'characteristic-functions', 'definite-integrals', 'probability-theory']"
3808138,Limit of sequence of Riemann integrable functions is not Riemann integrable.,"This post is to clarify if the following example works. Let ${q_n}$ be an enumeration of $\mathbb{Q}$ . For each n define $f_n(x)=1$ if $x\in\{q_1,...,q_n\}$ and $f_n(x)=0$ otherwise.
Then for all n $\int f_n dx=0$ while $\int f dx$ does not exist where $lim_{n}f_n=f$ and $f$ is the Dirichlet function. Is this valid?","['integration', 'riemann-integration', 'real-analysis']"
3808144,Find the value of $a^{2020}+b^{2020}+c^{2020}$. [duplicate],"This question already has answers here : Find $a^{2013} + b^{2013} + c^{2013}$ (2 answers) Closed 3 years ago . Question: Let $f(x)=x^3+ax^2+bx+c$ and $g(x)=x^3+bx^2+cx+a$ where $a,b,c\in\mathbb{Z}$ , $c\neq 0$ . Suppose $f(1)=0$ and roots of $g(x)=0$ are squares of the roots of $f(x)=0$ . Find the value of $a^{2020}+b^{2020}+c^{2020}$ . My approach: Let the other roots of $f(x)$ except $1$ be $\alpha$ and $\beta$ . This implies that the roots of $g(x)$ are $1,\alpha^2$ and $\beta^2$ . Thus, we have $1+\alpha+\beta=-a,$ $\alpha+\beta+\alpha\beta=b,$ $\alpha\beta=-c$ and $1+\alpha^2+\beta^2=-b,$ $\alpha^2+\beta^2+\alpha^2\beta^2=c,$ $\alpha^2\beta^2=-a.$ Note that thus we have $c^2=-a$ . Again, $$a^2=(1+\alpha+\beta)^2=1+\alpha^2+\beta^2+2(\alpha+\beta+\alpha\beta)=-b+2b=b.$$ Also, since $f(1)=0\implies a+b+c=-1$ . Now since we have $a=-c^2$ and $b=a^2\implies b=c^4,$ which in turn implies that $$-c^2+c^4+c+1=0\implies c^4-c^3+c^3-c^2+c-1=-2\\\implies (c^3+c^2+1)(c-1)=-2.$$ Thus, we either have $$\begin{cases}c^3+c^2+1=1\\ c-1=-2\end{cases} \text{ or }\begin{cases}c^3+c^2+1=-1\\ c-1=2\end{cases} \text{ or }\begin{cases}c^3+c^2+1=2\\ c-1=-1\end{cases} \text{ or }\begin{cases}c^3+c^2+1=-2\\ c-1=1\end{cases}.$$ Note that only the first diophantine equation yields a solution, that is $c=-1$ . Thus, $a=-1$ and $b=1$ . Therefore, $$a^{2020}+b^{2020}+c^{2020}=1+1+1=3.$$ Is this solution correct and rigorous enough and is there any other way to solve the problem? Also, does anyone know the original source of this problem?","['number-theory', 'solution-verification', 'polynomials', 'diophantine-equations']"
3808149,"If $\mathbb{P}(A_n)\to 0$, prove that $\int\limits_{A_n}X\mathrm{d}\mathbb{P}\to 0.$","Let $(\varOmega,\mathcal{F},\mathbb{P})$ be a probability space. If $X:\varOmega\to \mathbb{R}$ is integrable and $(A_n) \subset\mathcal{F}$ such that $\mathbb{P}(A_n)\to 0$ , then prove that $\int\limits_{A_n}X\mathrm{d}\mathbb{P}\to 0.$ Attempt. If $X=1_A$ for some $A\in \mathcal{F}$ , then $$\int\limits_{A_n}X\mathrm{d}\mathbb{P}=\int1_{A_n}1_A\mathrm{d}\mathbb{P}=\int1_{AA_n}\mathrm{d}\mathbb{P}=\mathbb{P}(AA_n)\to 0,~~n\to \infty$$ since $\mathbb{P}(A_n)\to 0$ . If $X=\sum_{k=1}^{m}a_k1_{A_k}$ is simple, then from linearity of integral: $$\int\limits_{A_n}X\mathrm{d}\mathbb{P}=\sum_{k=1}^{m}a_k\int\limits_{A_n}1_{A_k}\mathrm{d}\mathbb{P}\to \sum_{k=1}^{m}a_k\cdot 0=0,~~n\to \infty.$$ If $X$ is non negative, then for some increasing sequence of simple rv's,
say $(X_k)$ , $X_k\nearrow X$ . $$\lim_{n\to \infty}\int\limits_{A_n}X\mathrm{d}\mathbb{P}=\lim_{n\to \infty}\int\limits_{A_n} \lim_{k\to \infty} X_k\mathrm{d}\mathbb{P}=\lim_{n\to \infty}\int  \lim_{k\to \infty} X_k1_{A_n}\mathrm{d}\mathbb{P}\overset{!}{=}$$ $$  \lim_{n\to \infty}\lim_{k\to \infty} \int   X_k1_{A_n}\mathrm{d}\mathbb{P}\overset{?}{=}  \lim_{k\to \infty}\lim_{n\to \infty} \int   X_k1_{A_n}\mathrm{d}\mathbb{P}=\lim_{k\to \infty}0=0,$$ where the $!$ holds by the MCT. I am not sure about the $?$ equality, how the change of limits is justified. (the case where $X$ takes values on $(-\infty,\infty)$ follows easily from the above, since $X=X^+-X^-$ ). Thanks in advance for the help. EDIT. According to the comment by @TheSilverDoe, we may use DCT. Indeed, $|X1_{A_n}|\leqslant |X|$ and $X$ is integrable. Also, is it true that $X1_{A_n}\to 0$ with prob. $1$ , since $\mathbb{P}(A_n)\to 0$ ?","['measure-theory', 'probability-theory', 'probability']"
3808170,The Effect of Small Variations in the Coefficients of a Linear Differential System,"In E.L. Ince's Ordinary Differential Equations , page $219$ , the effect of small variations in the coefficients of a linear differential system is examined. In particular, the main result of the section is that the ""index"" of the system is not raised by any variation of the coefficients which is ""uniformly small."" The result above concerns the ""index"" of the system, but in general, the question of the effect of small variations in the coefficients of a differential equation sounds interesting and, despite going over a couple of texts, modern and old, I don't see anyone discussing about it. I have one specific question in mind: Suppose we focus on just ordinary linear differential equations and one (or maybe all) of the terms is differentiably parametrized (i.e. the partial derivative of the term w.r.t. the parameter exists - or any other way you want to interpret it). Is the solution (now a two-variable function) differentiable w.r.t. the parameter as well? Maybe the answer to this question can be found in PDE's, but I hardly know any PDEs sensibly. In general, does anyone know of a good text (preferably a modern one) that addresses such a question? Thanks! Note : By the way, I am not sure if this kind of question is allowed here, but judging from how this topic evades internet searches, if anyone is interested to look into the problem deeper and from other angles (e.g. computational, applied), I would like to do the work together - preferably under a formal position such as a postdoc.","['stability-in-odes', 'ordinary-differential-equations', 'partial-differential-equations']"
3808185,Limit over one variable for two variable function,"For function $f(x,y)$ I want to find out $\displaystyle \lim_{y \to +0} f(x,y) $ . By substituting $y=0$ , I get $\displaystyle f(x,0) = \frac{0}{0}$ . Is it valid to use L'Hospital's rule.
In my case I have arrived at $f(x,y)$ as a power series expression derived from a complicated function involving Bessel function. Problem is after applying L'Hospital's rule I am getting required expression with all the required terms involving log functions etc but with one extra constant term. Final expression is very unintuitive from original function.
Additional point is $f(x,y)$ is monotonically increasing of $x$ for all values of $y$ . Also for non-zero values of $y$ , $f(x,y)$ is bounded function of $x$ . Slope of $f(x,y)$ with respect to $x$ is steeper for low values of $y$ . $f(0,y) = 0$ for all positive values of $y$ according to original expression. I am not able to get any discrepancy in my derivation. Derivation is based on first principles without application any advanced theorems etc.","['power-series', 'limits', 'calculus', 'functions']"
3808189,Does the unit generate the additive group in a unital ring with cyclic additive group?,"Let $R$ be a unital ring with cyclic additve group $(R, +,0)$ . Is it the case that $1$ generates the additive group $(R,+,0)$ ? Thoughts: Maybe classifying the unital rings with cyclic subgroups is possible.","['ring-theory', 'group-theory', 'abstract-algebra']"
3808191,Proof $f$ has a minimum if $f$ is continuous and $\lim_{x \rightarrow \infty}(f(x))=\infty=\lim_{x \rightarrow -\infty}(f(x))$,"I have a question about the validity of this proof I wrote: Claim : If $f$ is continuous on $\mathbb{R}$ and $\lim_{x \rightarrow \infty}(f(x))=\infty=\lim_{x \rightarrow -\infty}(f(x))$ , then $f$ has a minimum. My proof : Lower Bound Theorem) If $g$ is continuous on $[a,b]$ , then $\exists z \in [a,b]: \forall x \in [a,b]: f(z)\leq f(x)$ (1) $\forall N_1: \exists n_1>0: \text{if } x>n_1 \text{ then } f(x)>N_1$ (Definition) (2) $\forall N_2: \exists n_2<0: \text{if } x<n_2 \text{ then } f(x)>N_2$ (Definition) (3) Given $N_1, N_2$ : Let $I = [n_2, n_1]$ (4) $\therefore f$ is continuous on $I$ (5) $\therefore \exists y \in I: \forall x \in I:f(y) \leq f(x)$ (Using Lower Bound Theorem) (6) If $x \in \mathbb{R}$ but $x \notin I$ , then $x < n_2$ or $x>n_1$ . (7) $\therefore f(x) > N_1$ or $f(x) > N_2$ (8) Choose $N_1, N_2: f(y)<N_1$ and $f(y)<N_2$ (9) $\therefore \forall x \in \mathbb{R}:f(y) \leq f(x)$ Thanks for reading. I'm not sure about line 8. I already defined $N_1$ and $N_2$ in line 3, and constructed $y$ based on that in line 5. So if I redefine $N_1$ and $N_2$ based on $y$ in line 8, that could potentially redefine $y$ in line 5, which could redefine $N_1$ and $N_2$ in line 8, which could redefine $y$ in line 5, etc. etc. I'm pretty sure either this is just a little technicality that can be fixed easily but I'm not sure how, or it's not a problem at all and I'm overthinking. Can someone help?","['proof-writing', 'real-analysis', 'continuity', 'solution-verification', 'limits']"
3808221,Solve $ay''+by+c=0$ using separation of variables,"How could be $$ay''+by+c=0$$ (for constants $a,b,c$ ) solved using separation of variables? I tried using the method on an easier equation, namely $ay''+by'+c=0$ : $$\begin{align}a\frac{d^2y}{dx^2}+b\frac{dy}{dx}+c&=0,\, u=\frac{dy}{dx}\\a\frac{du}{dx}+bu+c&=0\\ \frac{1}{a}\frac{dx}{du}&=-\frac{1}{bu+c}\\x&=-a\int \frac{du}{bu+c}=-a\left(\frac{\ln |bu+c|}{b}+C\right)\\-\frac{b}{a}(x+aC)&=\ln |bu+c|\\|bu+c|&=e^{-\frac{b}{a}(x+aC)}\\bu+c=b\frac{dy}{dx}+c&=C'e^{iC''}e^{-\frac{b}{a}(x+aC)},\, \color{red}{C'\ge 0,\, C''\in\mathbb{R}}\\y&=\frac{1}{b}\left(C_0\int e^{-\frac{b}{a}(x+aC)}\, dx-c\int dx\right)\\&=\frac{1}{b}\left(C_0\left(-\frac{ae^{-\frac{b}{a}(x+aC)}}{b}+C'''\right)-c(x+C'''')\right).\end{align}$$ When I tried to apply this method on $ay''+by+c=0$ , I ran into problems, as the degrees of the derivatives differ by $2$ , not $1$ : $$\begin{align}a\frac{d^2 y}{dx}+by+c&=0,\, \frac{dy}{dx}=u\\a\frac{du}{dx}+b\int u\, dx+c&=0\\ \frac{dx}{du}&=-\frac{a}{b\int u\, dx+c}\\x&=-a\int\frac{du}{b\int u\, dx+c}.\end{align}$$ I'm stuck here. The solution should be $$y=Ce^{x\sqrt{\frac{b}{a}}}+C'e^{-x\sqrt{\frac{b}{a}}}-\frac{c}{b}.$$","['calculus', 'ordinary-differential-equations']"
3808230,Find the ratio $\frac{AF}{FC}$,"In the figure below, $AD$ is the median on $BC$ . The point $E$ divides $A$ and $D$ in the ratio $1:2$ . $BE$ produced meets $AC$ at $F$ . Find the value of $AF:FC$ My try: I joined $E,C$ .Let area of $\Delta ABC=x$ Then we get $$ar(BED):ar(ABE)=2:1$$ Also $$ar(ABD)=\frac{x}{2}$$ $\implies$ $$ar(BED)=\frac{2x}{6}$$ $$ar(AEB)=\frac{x}{6}$$ So $$ar(ECD)=\frac{2x}{6}$$ $\implies$ $$ar(AEC)=\frac{x}{6}$$ Any way from here?","['contest-math', 'euclidean-geometry', 'median', 'geometry', 'geometric-transformation']"
3808269,Almost $\pi$: a closed form expression for $\int_0^\infty \left[ \left(1+\frac 1{x!}\right)^x-1\right] dx$,"A slight variation of the famous limit definition of $e$ yields $$ \lim_{n\to\infty} \left(1+\frac 1 {n!}\right)^n =1.$$ By the comparison test, you can show that the series $$ S= \sum_{n=0}^\infty \left[ \left(1+\frac 1 {n!}\right)^n -1\right]$$ converges, and by the integral test, the corresponding integral $$ I= \int_{0}^\infty \left[ \left(1+\frac 1 {\Gamma(x+1)}\right)^x-1\right] dx $$ converges as well. Are there closed-form expressions for the values of $S$ and $I$ ? Approximate values are: $$
\begin{split}
S &\approx 3.06768391298,\\
I &\approx 3.12609693980.
\end{split}
$$ This reminds me of the Fransén-Robinson constant , the integral of $1/\Gamma$ over $[0,\infty)$ , being ""almost"" $e$ , in the following way: $$ F=\int_0^\infty \frac{1}{\Gamma(x)}dx = \sum_{n=0}^\infty \frac{1}{n!} + \int_0^\infty \frac{e^{-x}}{\pi^2+\ln^2(x)}dx.$$ I wonder whether $I-S \approx 0.05841302682$ can also be quantified as an improper integral of some sort.","['definite-integrals', 'closed-form', 'sequences-and-series']"
3808295,"Generalizing $a^ab^b>a^bb^a$, $a^ab^bc^c>a^bb^cc^a$","Title Generalizing $a^ab^b>a^bb^a$ , $a^ab^bc^c>a^bb^cc^a$ . Background We consider two distinct numbers $a,b\in\mathbb{R}^+$ , no matter $a<b$ or $b<a$ , we have: $$\frac{a^ab^b}{a^bb^a}=\Big(\frac{a}{b}\Big)^{a-b}>1$$ and hence $a^ab^b>a^bb^a$ . I am wondering about possible generalization(s). First of all, for the case of three distinct numbers $a,b,c\in\mathbb{R}^+$ , $a^ab^bc^c>a^bb^cc^a$ and $a^ab^bc^c>a^cb^ac^b$ , or simply $a^ab^bc^c>\max\{a^bb^cc^a,a^cb^ac^b\}$ . To prove this, assume $a>b>c$ , and notice that $$\frac{a^ab^bc^c}{a^bb^cc^a}>\frac{a^bb^ac^c}{a^bb^cc^a}=\frac{b^ac^c}{b^cc^a}=\Big(\frac{b}{c}\Big)^{a-c}>1$$ and $$\frac{a^ab^bc^c}{a^cb^ac^b}>\frac{a^bb^ac^c}{a^cb^ac^b}=\frac{a^bc^c}{a^cc^b}=\Big(\frac{a}{c}\Big)^{b-c}>1.$$ If mutually distinct $a,b,c\in\mathbb{R}^+$ in general, denoted by $a_0,a_1,a_2$ respectively, we could let bijective mapping $\sigma:\{0,1,2\}\to\{0,1,2\}$ be a permutation of 0,1,2 such that $a_{\sigma(0)}>a_{\sigma(1)}>a_{\sigma(2)}$ so that now $$a_0^{a_0}a_1^{a_1}a_2^{a_2}=a_{\sigma(0)}^{a_{\sigma(0)}}a_{\sigma(1)}^{a_{\sigma(1)}}a_{\sigma(2)}^{a_{\sigma(2)}}>a_0^{a_{(0\space+_3\space k)}}a_1^{a_{(1\space+_3\space k)}}a_2^{a_{(2\space+_3\space k)}}$$ where $k=1$ or $2$ and $+_3$ denotes the addition in $\mathbb{Z}/3\mathbb{Z}$ . Notice that this is possible because at the rightmost expression the exponent of $a_i$ is $a_j$ for some $j\neq i$ and notice that for $n=3$ , if a bijective mapping $\sigma:\{0,1,2\}\to\{0,1,2\}$ has the property $\sigma(i)\neq i$ for $i=0,1,2$ , then (by listing out all permutations of $(1,2,3)$ ) we can lead to the conclusion that $\sigma(i)=(i\space+_3\space k)\space\forall i=0,1,2$ for $k=1$ or $2$ . My conjecture For $n\in\mathbb{N}$ , for $a_0,\cdots,a_{n-1}$ , must we have $$\prod_{i=0}^{n-1}a_i^{a_i}>\prod_{i=0}^{n-1}a_i^{a_{\sigma(i)}}$$ where $\sigma:\{0,\cdots,n-1\}\to\{0,\cdots,n-1\}$ is any bijective map such that $\exists i: \sigma(i)\neq i$ . Also, if we relax the condition that $\exists i: \sigma(i)\neq i$ , maybe we can replace the $>$ by $\ge$ and the inequality still holds?
Notice that the conjecture says that product of at least 2 power of mutually distinct positive real numbers to itself will be strictly reduced when the exponents are re-arranged, which is quite interesting to my point of view. More about it I have tried to generalize the case for $n>3$ by considering a sequence of swapping so that $(0,\cdots,n-1)$ is changed to $\big(\sigma(0),\cdots,\sigma(n-1)\big)$ , and hoping that we can use $a^ab^b>a^bb^a$ to prove the conjecture step by step. However, consider the following example: Example making me frustrated: For $n=6$ , let $\sigma(i)=5,2,3,1,0,4$ for $i=0,1,\cdots,5$ respectively. Denote mutually distinct positive real numbers $a_0,\cdots,a_{5}$ by $a,b,c,d,e,f$ respectively, and we hope that we can prove: $$a^ab^bc^cd^de^ef^f>a^fb^cc^dd^be^af^e.$$ We may first swap the 4-th and 5-th terms so that $a^ab^bc^cd^de^ef^f>a^ab^bc^cd^de^ff^e$ . We may then swap the 2-nd and 3-rd terms so that $a^ab^bc^cd^de^ef^f>a^ab^bc^dd^ce^ff^e$ now. We may then swap the 1-st and 3-rd terms so that $a^ab^bc^cd^de^ef^f>\cdots(?)$ : The problem is that I thought I could have $b^bd^d>b^dd^b$ to use but the power of $d$ in the previous step is already changed. So I can't prove the conjecture I proposed. Conclusion Is it possible to prove my conjecture easily? Of course, things may not always be that easy, but I am not experienced enough in every subject in Mathematics, so please state where I can find the knowledge required clearly (something like permutation?) if possible. Thank you a lot! At the end, I will appreciate any kind of help!","['permutations', 'exponentiation', 'algebra-precalculus', 'inequality']"
3808303,Faulty proof of Lagrange Multipliers?,"i am currently reading the book Convex Analysis . At ""Theorem 7.8.1"" there is given a sketch of a proof which seems faulty to me. I will briefly explain the Theorem and the proof and then my problem i have. Maybe you can hint me where i missed something. We start out as usual with continuous differerentiable functions $f_0,\dots,f_m : U\to \mathbb R$ with $U\subset \mathbb R^n$ open. If the following Problem has a minimum at $\hat x$ : $\min f_0(x), x\in \mathbb R^n,\ f_1(x) = \dots = f_m(x) =0$ Then there exists $\lambda = \lambda_0,\dots,\lambda_m$ , not all $0$ with: $\mathcal L(\hat x,\lambda) = \lambda_0 f'_0(\hat x) + \dots + \lambda_m f'_m(\hat x) = 0$ The proof thats shown in the book goes like this: (you may skip this part at first glance) Let $\hat x$ be a minimal point of the Problem. Define $F(x,y) = f_0(x) \text{ if } f_1(x) +y_1 = \dots = f_m(x) + y_m = 0$ and $\infty$ otherwise. For $$\min f(x) := F(x,0)$$ we get our original Problem then. Taking the solutionset of $(z,y) = (f_0(x),\dots, f_m(x))$ we can define a function $g(x)=(z,y)$ which is continuously differnentiable and its Graph ,,can be seen'' as the Graph $\Gamma_F$ of $F$ (This is sort of wrong already but since its a sketch i get what he means). Now the author says the Tangent space $T_{(\hat x, 0_Y, f(\hat x))}$ at the $(\hat x, 0 , f(\hat x))(\Gamma_F)$ then is the Graph of the derivative $g'(\hat x)x = (f_0'(\hat x)x, \dots, f_m'(\hat x)x)$ so we can describe it by an equation $$
\langle (\lambda_1,\dots,\lambda_m),y\rangle -\langle \lambda_0,z\rangle = 0
$$ And like this we get the multiplicators. Now comes the part where it seems wrong to me :
We want $y$ to be $0$ to solve our problem $\min f(x) = F(x,0)$ and also $z$ to be $0$ for minimality sake which translates into $$
(\mathbb R^n \times 0_Y \times 0) + T_{(\hat x, 0_Y, f(\hat x))} \subsetneq \mathbb R^n \times Y\times \mathbb R
$$ note the proper inclusion is equivalent to $(\mathbb R^n \times 0_Y \times 0)\subset T_{(\hat x, 0_Y, f(\hat x))}$ . BUT why would $z$ be $0$ ?. The Author says because Tangent space of the Graph of $f$ at the point $(\hat x, f(\hat x))$ is contained in $\mathbb R^n\times 0$ , ,,which follows immediatly from the local minimality of $f$ ''. But that thing is not even differentiable by definition. I mean okay one could go with the subgradient instead or something. But still its wrong in my opinion. I mean imagine $n=2$ and $f_0$ to be  an inverse Paraboloid and $f_1 = 0$ is our only restriction which is $0$ on the unit ball, smooth and $>0$ outside the unit ball. Then the subgradient $\partial f(x)$ will never have an element $0$ inside its subgradient . Even in this more general case its wrong right?","['lagrange-multiplier', 'real-analysis', 'optimization', 'convex-analysis', 'differential-geometry']"
3808363,"$f$ is 1-periodic prove $\lim_{n \to \infty} \int_{0}^{1} \sin^2(πx)f(nx)\,dx = \frac{1}{2} \int_{0}^{1} f(x)\,dx$","I came across this very nice Question so thought of sharing it! Let $f(x)$ be a continuous function $f:R \to R$ with period $1$ . Prove that $\displaystyle \lim_{n \to \infty} \int_{0}^{1} \sin^2(\pi x)f(nx)\,\mathrm{d}x = \frac{1}{2} \int_{0}^{1} f(x)\,\mathrm{d}x$ . Added the solution as an answer.","['integration', 'real-analysis', 'calculus', 'functions', 'riemann-sum']"
3808388,Algebra of upper triangular matrices,"Let $k$ be a field and $R$ the algebra of $3\times3$ upper triangular matrices $(a_{ij})$ st $a_{11}=a_{22}=a_{33}$ . Find the Jacobson radical $J(R)$ of $R$ Attempt : Using the characterization $y\in J(R)\iff 1-xy\in U(R),\ \forall x\in R$ I found that $$J(R)=\left\{\begin{pmatrix} 0 \ a \ b\\ 0\ 0 \ c \\ 0\ 0 \ 0\end{pmatrix}:a,b,c\in k\right\}$$ Show that every two simple $R-$ modules are isomorphic. Attempt : The left simple $R-$ modules coincide with left simple $R/J(R)-$ modules and $R/J(R)=k$ which is a simple Artinian ring. Hence all simple $R-$ modules are isomorphic. Let $M,N$ two semisimple $R-$ modules with $\dim_kM=\dim_kN<\infty$ . Is it true that $M$ and $N$ are isomorphic? Attempt : Yes. It is $M=V^m,\ N=V^n$ where $V$ is the unique up to isomorphism simple $R-$ module. So $n\dim_kV=\dim_kN=\dim_kM=m\dim_kV<\infty \Rightarrow m=n \Rightarrow M\cong N$ . Let $M,N$ be two $R-$ modules with $\dim_kM=\dim_kN<\infty$ .  Is it true that $M$ and $N$ are isomorphic? Attempt : I suppose they are not since $R$ is not a simple algebra ( $\left\{\begin{pmatrix} 0 \ 0 \ b\\ 0\ 0 \ 0 \\ 0\ 0 \ 0\end{pmatrix}:b\in k\right\}$ is an ideal) but I can't find a counter-example. Is my approach correct? What about $4$ ? Can you give a general example where $R$ is a $k-$ algebra of finite dimension (not simple) and the equivalence $M\cong N\iff \dim_kM=\dim_kN<\infty$ does not hold? Thanks in advance!","['matrices', 'ring-theory', 'solution-verification', 'noncommutative-algebra']"
3808412,Inverse of a matrix and matrix relation,"Let $A$ be the matrix $$ A= \begin{pmatrix}
2 & -1 & -1\\
0 & -2 & -1 \\
0 & 3 & 2 \\
\end{pmatrix} $$ I am trying to find $A^{-1}$ as a relation of $I_{3}, A$ and $A^{2}$ and also to prove that $A^{2006}-2A^{2005}=A^{2}-2A$ .  For the first one I noticed that $$A^{n}= \begin{pmatrix}
2^{n} & -(2^{n}-1) & -(2^{n}-1)\\
0 & -2 & -1 \\
0 & 3 & 2 \\ \end{pmatrix} , A^{k}= \begin{pmatrix}
2^{k} & -(2^{k}-1) & -(2^{k}-1)\\
0 & 1 & 0 \\
0 & 0 &  1\\ \end{pmatrix}$$ for $n$ odd and $k$ even but I don't know how to proceed from here. Also the the equality $A^{2006}-2A^{2005}=A^{2}-2A$ leads to $A^{2004}(A-2I_{3})=A-2I_{3}$ but again I don't know what to do next since $(A-2I_{3})$ is not invertible. Any help?","['matrices', 'linear-algebra', 'inverse']"
3808433,Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function that satisfies $f(q+1/n)=f(q)$ for every $q\in\mathbb{Q}$ and for every $n\in\mathbb{N}$.,"Question: Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function that satisfies $f(q+\frac{1}{n})=f(q)$ for every $q\in\mathbb{Q}$ and for every $n\in\mathbb{N}$ . Show that $f$ must be a constant function. Solution: Substituting $q-\frac{1}{n}$ for $q$ in $$f(q+\frac{1}{n})=f(q),$$ we have $$f(q)=f(q-\frac{1}{n})$$ for all $q\in\mathbb{Q}$ and for all $n\in\mathbb{N}$ . Let the original relation be denoted by $(*)$ and the derived relation be denoted by $(**)$ . Now select any positive rational number $q=\frac{r}{s},$ where $r,s\in\mathbb{N}$ . Thus, by repeated application of $(**)$ , we have $$f\left(\frac{r}{s}\right)=f\left(\frac{r-1}{s}\right)=f\left(\frac{r-2}{s}\right)=\cdots=f\left(\frac{0}{s}\right)=f(0).$$ Next select any negative rational number $q=\frac{r}{s}$ , where $r\in\mathbb{Z}_{\le 1}$ and $s\in\mathbb{N}.$ Thus, by repeated application of $(*),$ we have $$f\left(\frac{r}{s}\right)=f\left(\frac{r+1}{s}\right)=f\left(\frac{r+2}{s}\right)=\cdots=f\left(\frac{0}{s}\right)=f(0).$$ Thus, for all $q\in\mathbb{Q}$ , we have $f(q)=f(0)$ . Next select any irrational number $q'$ . We know that there exists a convergent sequence $(x_n)_{n\ge 1}$ of rational numbers such that it converges to $q'$ . Now since $f$ is continuous on $\mathbb{R}$ , implies that it is continuous at $q'$ . Thus, by the sequential definition of limit we can conclude that the sequence $f(x_n)$ converges to $f(q').$ Now note that $f(x_n)=f(0)$ , for all $n\in\mathbb{N}$ . This implies that $f(x_n)$ converges to $f(0)$ , which in turn implies that $f(q')=f(0)$ . Now since $q'$ is arbitrary, therefore, $f(q')=f(0)$ for all irrational numbers $q'$ . Thus, we can conclude that $f(x)=f(0)$ for all $x\in\mathbb{R}$ , that is $f$ is a constant function. Is this solution correct and rigorous enough and is there any other way to solve the problem?","['solution-verification', 'sequences-and-series', 'real-analysis']"
3808453,When is $3 x^2 + 2 x$ a square,"Let $x, y$ be a positive integers. I want to know when $3 x^2 + 2 x = y^2$ has a solution. Through some enumeration of all $x$ , and trial and error, I have found the following recursion which appears to include all the solutions: Initial conditions are: $$\begin{array}{l}
x_0 = 0, x_1 = 2\\
y_0 = 0, y_1 = 4
\end{array}$$ Recursion is: $$\begin{array}{l}
  x_n = 8 y_{n - 1} + x_{n - 2}\\
  y_n = 14 y_{n - 1} - y_{n - 2}
\end{array}$$ This appears to be similar to Pell's equation, and here it seems that $x / y$ is some continued fraction approximation to $1 / \sqrt{3}$ . I'm not quite sure how to find all solutions mathematically though, and see that this indeed produces all solutions.","['number-theory', 'quadratic-forms', 'diophantine-equations']"
3808474,IMO $2002$ Problem $1$ (C$1$) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Hi! I don't understand this solution. Specifically, when it says ""with all the blue rows and columns unchanged except that the values of $a_x$ and $b_y$ DECREASE by $1$ "", could someone explain why this happens? why does $x + y$ needs to be maximal? When the point is changed to blue, doesn't $a_x$ and $b_y$ are added $1$ ?","['contest-math', 'induction', 'combinatorics']"
3808485,Do I understand the difference between $\implies$ and $\to$? [duplicate],"This question already has answers here : What is the difference between implication symbols: $\rightarrow$ and $\Rightarrow$? [duplicate] (5 answers) Closed 3 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I am taking a course in Discrete Mathematics. In the course we are using $\to$ for implication and have been discussing truth tables and the like. But something was said about this being the same as $\implies$ . It seemed strange to me that if they are the same, why not just use one of the symbols. I dug around and find that there is a difference. I know that in the day to day life of a mathematician, whatever difference there is, it isn't really much to worry about. But there is supposedly a difference. I know that there are a number of other questions/answers on this site that discuss this, but I am still a bit confused. Here is my current understanding. Please tell me if I am thinking about it the right way First my understanding: A proposition is the same as a statement. When $A$ and $B$ are propositions, then $A \to B$ is the proposition with the truth table that is only false when $p$ is true and $q$ is false. When proving a theorem something is assumed to be true. From this one makes arguments that lead to the conclusios. We then use $A \implies B$ to say that since we know $A$ is indeed true, then $B$ must also be true. To $\implies$ is not a strict logical symbol with a truth table. We only use this to say that something is true because of something else. If I know that $x$ is equal to $1$ and I want to say that from this follows that $x^2 = 1$ , then I would use $\implies$ . So I may say ""We know that $x=1 \implies x^2 = 1$ "". So far so good. Let's say I want to define a set. If I consider the two sets $$
A = \{x\in \mathbb{R}: x^2 =1 \to x\geq 0\} \\
B = \{x\in \mathbb{R}: x^2 =1 \implies x\geq 0\} 
$$ Here then $A = \mathbb{R}\setminus \{-1\}$ because for these numbers the proposition/statement $(x^2 =1 \to x\geq 0)$ is true. And $\implies$ in $B$ doesn't make sense because I am not asserting anything. This would be the same reason that if I make the theorem that: for all real numbers $x$ , $x^2 = 1 \implies x = 1$ , then this is an incorrect theorem. If I make the definition saying that a real number $x$ is foo if $x^2 = 1 \implies x =1$ , then the only number that is foo is $1$ . Is all this correct? I understand that mathematicians use $\implies$ when maybe they ""should"" use $\to$ and this doesn't bother me. I am just trying to understand. (You should have a ""did-I-understand-this-correctly tag.)","['propositional-calculus', 'logic', 'notation', 'definition', 'discrete-mathematics']"
3808489,Values such that piecewise function is differentiable everywhere,"A small discussion on calculus. I have a function $$f(x)=\begin{cases}3-x, & \text{$x<1$}\\ ax^2 +bx, &\text{$x\geq1$}\end{cases}$$ I need to find $a,b$ such that this function is differentiable everywhere. Usually we proceed like this (correct me if I am wrong): We must have continuity, so that $2=a+b$ . We also must have equal left and right derivative at 1, so by calculating each one-sided derivative at 1, we have $-1=2a+b$ . Solving the system, we get $a,b$ . By the way, I am not studying this as new student in Calculus. This is just something that bothers me even though I have passed Calculus class. My two questions here are: What is the theory behind equating both one-sided derivatives here? Are we arguing this way: at any $x<1$ , the derivative is $-1$ , and at any $x>1$ , the derivative is $2ax + b$ . Limiting to $1$ , they have to be equal, hence $-1=2a+b$ . However, aren't we limiting $f'(x)$ (for $x\neq 1$ ) here? Is one-sided limit of $f'(x)$ the definition of one-sided derivative? Still related, I thought one-sided derivative comes from the definition $\frac{f(x)-f(1)}{x-1}\rightarrow \text{(some number)}$ as $x\rightarrow 1^-$ (for left side, and similarly for right side) instead? If it is the case, then I tried to calculate by definition and wanted to equate the result, but it does not look easy at all, and I could not obtain $-1=2a+b$ . Thanks a lot for clearing my misunderstanding.","['limits', 'calculus', 'derivatives']"
3808515,"If $A \times B$ is a Lie group, are both $A$ and $B$ Lie groups?","Suppose $A,B$ are smooth manifolds and there exists a binary operation on the product manifold $A \times B$ making it into a Lie group. Does this guarantee that there exist binary operations on both $A$ and $B$ making them each into Lie groups? If the answer is yes, can it be done in such a way that the product group $A \times B$ is equal to the original Lie group? If the answer to that is yes, is it necessary? I tried briefly to find a counterexample browsing a table of Lie groups but was unsuccessful.","['abstract-algebra', 'lie-groups', 'differential-geometry']"
3808593,Narrow equals wide topology on probability measures: reference?,"The following page says that the narrow (sometimes ""weak"") topology (induced by bounded continuous functions) equals the wide [or weak* topology] (induced by compactly-supported continuous functions) the set $P(X)$ of Radon probability measures on X, provided that X is a Hausdorff space, or I think that X is required to be locally compact Hausdorff. https://encyclopediaofmath.org/wiki/Convergence_of_measures Could somebody provide an exact reference to this? I am also interested in exact references on similar results. Moreover, I'd like to know [Edit:] (A) if the result is correct (and contained or clearly implied by the some reference); i.e., the wide topology on $P(X)$ equals the narrow topology on $P(X)$ ; or (B) if the reference only says that the two topologies have the same convergent sequences; i.e.,
if $\{\mu_n\}_{n \geq 1}\subset P(X)$ , $\mu\in P(X)$ , and $$\lim_{n\rightarrow\infty}\int_X f(x) \mu_n(dx) = \int_X f(x) \mu(dx), \tag{1}\label{eq1}$$ for all compactly-supported continuous functions $f:X\to\mathbb K$ (where $\mathbb K$ is $\mathbb R$ or $\mathbb C$ ), then \eqref{eq1} holds for all bounded continuous functions $f:X\to\mathbb K$ . Of course, I'd like to know the exact conditions required on $X$ and $P(X)$ in the reference. My belief is that $X$ is required to be ""any locally compact Hausdorff space"" and $P(X)$ is required to be Radon but not necessarily more; i.e., "" $P(X)$ = all Radon probability measures"" or equivalently, all regular Borel probability measures (see below). Background information: Rudin: RCA, Theorem 6.19 says that if $X$ is a locally compact Hausdorff space, then the dual of $C_0(X)$ (hence of $C_c(X)$ too) is exactly the space of regular measures , so then the word ""weak*"" for the wide topology is justified. Indeed, for locally compact Hausdorff spaces, Radon measures (i.e., inner regular or tight measures) are the same as regular measures , by https://encyclopediaofmath.org/wiki/Radon_measure (A sufficient criterion is Rudin: 2.18.)","['measure-theory', 'weak-convergence']"
3808600,Definition of differentiation in context of abstract algebra.,"In any regular calculus or real analysis course, we learn the definition of the derivative of a function $f(x)$ as $$f^\prime (x)=\lim\limits_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h}$$ However while studying abstract algebra we come to know that differentiation is just like any operation (like addition, multiplication etc.) but on functions. So I want to know that is there a way to define an algebraic structure with the underlying set as the set of all differentiable functions and the operation of differentiation. And also if it's possible to define differentiation in such a manner, how to connect it with the analytical definition of differentiation.","['calculus', 'abstract-algebra']"
3808605,Alternative ways to evaluate $\int_0^1 x^{2n-1}\ln(1+x)dx =\frac{H_{2n}-H_n}{2n}$,"For all, $n\geq 1$ , prove that $$\int_0^1 x^{2n-1}\ln(1+x)\,dx =\frac{H_{2n}-H_n}{2n}\tag1$$ This identity I came across to know from here, YouTube which is proved in elementary way. Trying to make alternate efforts to prove  it, we can observe  that $$\ln(1+x)=\ln\left(\frac{1-x^2}{1-x}\right)=\ln(1-x^2)-\ln(1-x)$$ On multiplying by $x^{2n-1}$ and integrating from $0$ to $1$ , we have first order partial derivatives of beta function . How can we prove that identity $(1)$ without the use of derivatives of beta function ?","['integration', 'definite-integrals', 'alternative-proof', 'harmonic-numbers', 'calculus']"
3808637,Is there a differentiable smooth max and min function that respects the distributive law?,"I'm looking for functions $S_{max}, S_{min}$ that is similar (converges to?) to a smooth max, but respects the distributive law and is differentiable (or is polynomial). Is there such a known function? So it should at least satisfy the following: $S_{max}(x, y)'$ exists assuming that $x', y'$ do. $min(max(x, y), z) = max(min(x, z), min(y, z))$ and vice versa (distributive)","['optimization', 'maxima-minima', 'derivatives', 'smooth-functions']"
3808638,Group $G$ of order $24$ that is either $S_4$ or $G/Z(G)$ is $A_4$.,"A group is $p$ - closed if it has only one Sylow $p$ -subgroup. Theorem : Let $G$ be a group of order 24 that is not 3-closed. Then either $G\cong S_4$ or $G/Z(G)\cong A_4$ . Proof. $G$ acts on $$ \Omega:=\operatorname{Syl}_3 G$$ by conjugation. Since $G$ is not 3-closed Sylow's Theorem gives $|\Omega|=4$ . Thus, there exists a homomorphism $\varphi:G\to S_4$ such that $$\operatorname{Ker}\varphi =\bigcap_{S\in\Omega} N_G(S)=: N.$$ $G/N$ is a subgroup of $S_4$ and $|N|$ a divisor of $\frac{24}{4}=6$ . If $|N|\in\{3,6\}$ , then $N$ and thus also $G$ is 3-closed, a contradiction. The case $N=1$ yields $G\cong S_4$ , and the case $|N|=2$ implies $N=Z(G)$ and $G/N\cong A_4$ . $\square$ I don't understand the case $|N|=2$ . Obviously $Z(G)\leq N_G(S)$ for every $S\in Syl_3 G$ and so $Z(G)\leq N$ . Is it that $Z(G)=2$ ? And why is $G/N$ isomorphic to $A_4$ ? This seems even more difficult to prove!","['group-theory', 'sylow-theory']"
3808668,If $\tan\theta =\cos2\alpha\tan\phi$ then prove that $\tan(\phi-\theta)=\frac{\tan^2\alpha\sin2\phi}{1+\tan^2\alpha\cos2\phi}$,"If $\tan\theta =\cos2\alpha\tan\phi$ then prove that $\tan(\phi-\theta)=\frac{\tan^2\alpha\sin2\phi}{1+\tan^2\alpha\cos2\phi}$ I have tried in a few different ways applying the formula $\tan(\phi - \theta) = \frac{\tan \phi - \tan \theta} {1+\tan \phi \tan \theta}$ then substituting $\tan \theta = \cos 2\alpha \tan \phi$ . But all in vain. Assuming the problem to be correct, my highest achievement was getting the numerator correct, but the denominator really gives me a lot of headache. Please guide me. Thanks in advance.",['trigonometry']
3808684,Why do engineers use derivatives in discontinuous functions? Is it correct?,"I am a Software Engineering student and this year I learned about how CPUs work, it turns out that electronic engineers and I also see it a lot in my field, we do use derivatives with discontinuous functions. For instance in order to calculate the optimal amount of ripple adders so as to minimise the execution time of the addition process: $$\text{ExecutionTime}(n, k) = \Delta(4k+\frac{2n}{k}-4)$$ $$\frac{d\,\text{ExecutionTime}(n, k)}{dk}=4\Delta-\frac{2n\Delta}{k^2}=0$$ $$k= \sqrt{\frac{n}{2}}$$ where $n$ is the number of bits in the numbers to add, $k$ is the amount of adders in ripple and $\Delta$ is the ""delta gate"" (the time that takes to a gate to operate). Clearly you can see that the execution time function is not continuous at all because $k$ is a natural number and so is $n$ .
This is driving me crazy because on the one hand I understand that I can analyse the function as a continuous one and get results in that way, and indeed I think that's what we do (""I think"", that's why I am asking), but my intuition and knowledge about mathematical analysis tells me that this is completely wrong, because the truth is that the function is not continuous and will never be and because of that, the derivative with respect to $k$ or $n$ does not exist because there is no rate of change. If someone could explain me if my first guess is correct or not and why, I'd appreciate it a lot, thanks for reading and helping!","['real-analysis', 'calculus', 'functions', 'optimization', 'derivatives']"
