question_id,title,body,tags
3808713,Is there an elegant way to take a derivative of this linear algebra problem?,"I'm a biologist self-learning linear algebra for some applications in my work.  Taking derivatives in non-linear-algebra contexts is quite clear to me, as I can just chain-rule my way through problems... but doing so in a linear-algebra context is a bit of a mystery to me. I'm trying to take the derivative of the following equation $$y = \vec{d}^TP^TP\vec{\delta},$$ where $$\vec{d} = \left[ \begin{array}\\ d_1 \\ d_2 \\ d_3 \end{array} \right]$$ and $$\vec{\delta} = \left[ \begin{array}\\ o_1^2 -d_1^2 \\ o_2^2 - d_2^2 \\ o_3^2 - d_3^2 \end{array} \right].$$ $P$ is just a 3x3 matrix of constants. I'd like to find an elegant solution to finding the derivative $y^\prime \left( \vec{d} \right)$ , resulting in the Jacobian.
The challenging part is that the vector $\vec{\delta}$ is a composite vector holding $\vec{\delta} = \vec{o} - \vec{d}^{\circ2}$ (not sure on my notation here.. but $\vec{d}^{\circ2}$ is supposed to indicate all the elements of $d$ are squared).
So far, the only way I'm aware of doing this is by expanding everything out into one really large linear formula for the resulting single value, then taking the derivative of that. My end goal here is to implement this in code... so unnecessarily large formulas are something I'm trying to avoid to keep my code readable.
Is there a more elegant solution to this?","['derivatives', 'linear-algebra']"
3808768,Derivation of Fourier Transform of a constant signal,"I understand that the F.T. of a constant signal is the Dirac. However, I cannot find anywhere showing the derivation or proof for this. I'm trying to do it myself and am getting lost. Can anyone give a worked-out derivation that the Fourier Transform of a constant signal is the Dirac? Thank You for any help!","['integration', 'fourier-analysis', 'fourier-transform', 'dirac-delta']"
3808779,Hartshorne exercise II.4.5(c) [duplicate],"This question already has answers here : Hartshorne's Exercise II.4.5(c). A third time. (2 answers) Closed 3 years ago . Let $X$ be an integral scheme of finite type over a field $k$ , having function field $K$ . We say that a valuation of $K/k$ has center $x$ on $X$ if its valuation ring $R$ dominates the local ring $O_{x,X}$ . In part (c) of the question, we are asked to show if every valuation of $K/k$ has a unique center on $X$ , then $X$ is proper over $k$ . I am awared that this question was asked in Hartshorne's Exercise II.4.5(c) and I also found a solution in https://en.wikibooks.org/wiki/Solutions_to_Hartshorne%27s_Algebraic_Geometry/Separated_and_Proper_Morphisms#Exercise_II.4.5 . It seems to me that, by using induction, this question come down to showing that for any irreducible reduced closed subscheme, $Z$ , of $X$ , $Z$ satisfies the conditions on $X$ . That is, I want to show, if every valuation of $K/k$ has a unique center on $X$ and $L$ is the function field of $Z$ , then every valuation of $L/k$ has a unique center on $Z$ . How do I show this?","['algebraic-geometry', 'commutative-algebra']"
3808879,Eigenvalues of $A^{2018}$,"Find the eigenvalues and eigenvectors of $A^{2018}$ . $$
A=\begin{bmatrix} 1 & 3 & 4\\ 3 & 1 & 4\\ 0 & 0 & 4\end{bmatrix}
$$ My solution: First, by substracting first row times three from second row we get: $$
A\approx \begin{bmatrix} 1 & 3 & 4\\ 0 & -8 & -8\\ 0 & 0 & 4\end{bmatrix}
$$ We achieved the upper triangular matrix so the characteristic polynomial is: $$
\chi_{A^{2018}}(\lambda)=det (\begin{bmatrix} 1 & 3 & 4\\ 0 & -8 & -8\\ 0 & 0 & 4\end{bmatrix}^{2018}-\lambda I)=(1^{2018}-\lambda)((-8)^{2018}-\lambda)(4^{2018}-\lambda)
$$ Therefore the set of eigevalues is $\{1,4^{2018},8^{2018},\}$ . Please verify if this the correct solution, and in case it isn't, help me find the correct one.","['matrices', 'determinant', 'linear-algebra', 'eigenvalues-eigenvectors']"
3808881,Why are proofs not written as collections of logic symbols but are instead written in sentences? [duplicate],"This question already has answers here : Why do proof authors use natural language sentences to write proofs? (5 answers) Closed 3 years ago . Mathematical proofs are written as sentences and not as collections of logic symbols. Through logical operations, it is much easier for me to visualize what the symbols are trying to tell us rather than English text filled with grammar. This is my personal opinion, others may have different opinions. I just asked this question on another website to find out logical mistakes in my work which is entirely done in the language of propositional logic. Some people suggested to write it down in sentences in English. Is there any kind of tragedy in writing proofs as collections of logic symbols?","['multivariable-calculus', 'proof-writing', 'propositional-calculus', 'real-analysis']"
3808929,Guessing colored hats without repetition,"This entire question is inspired by Problem $12082$ of the Problems and Solutions section of the American Mathematical Monthly (see the May $2020$ issue for the solution to said problem). First, I recall a simpler, classic problem, slightly rephrased. Then I will state the variant, which is the actual question I want to ask. Classic question: Suppose that $n$ people are in a line (so that each can only see the people in front of them) and each has a hat placed on their head. The hats can be any of $k$ different colors, with equal probability of any color occurring (possibly with repetition). Starting from the back of the line, each person tries to guess the color of the hat on their own head. Certainly, the person at the back of the line can do no better than a random guess. But (assuming everyone strategizes beforehand and works as a team) can the last person in line make their guess in such a way that everyone else in the line will be able to guess their hat color with 100% certainty? Classic answer: Replace the hat colors by elements of $\mathbb Z/k\mathbb Z$ . If the colors of the hats are $C_1,C_2,\dots,C_n$ , where $C_n$ corresponds to the last person in line, then the last person guesses $S=C_1+C_2+\dots+C_{n-1}$ . When we get to person $j$ , they can see $C_1,\dots,C_{j-1}$ and they have heard the guesses $C_{j+1},\dots,C_{n-1},S$ . Therefore, they can recover $$C_j=S-(C_1+\dots+C_{j-1}+C_{j+1}+\dots +C_{n-1}).$$ (This is an example of a $(n-1)$ -ary quasigroup of order $k$ . In fact, the strategies for the classic question are precisely the same thing as $(n-1)$ -ary quasigroups of order $k$ .) Variant question: Suppose that we are in the same setup, but there is now only one hat of each color (if the game is to be possible, we must have $k\geq n$ ). The person at the back of the line should guess one of the $k-n+1$ colors that is not visible to them (since they know that they cannot be wearing any of the colors on the heads of the other people). Again, is it possible for the person in the back to make a guess (which now should be restricted to the $k-n+1$ colors not on another head) that allows everyone else in line to guess the color on their head with 100% certainty? Partial answer: Let us phrase the strategy we are looking for in slightly more abstract terms. For any integer $m>0$ , let $[m]=\{1,2,\dots,m\}$ . Then the hats visible to the last person in line correspond to an injection $f:[n-1]\rightarrow [k]$ . Let $\mathcal I$ be the set of all such injections. Then the desired strategy is a function $S:\mathcal I\rightarrow [k]$ such that: $S(f)\notin \text{Im}(f)$ for any $f\in \mathcal I$ ; if $f_1(i)\neq f_2(i)$ for exactly one $i\in [n-1]$ , then $S(f_1)\neq S(f_2)$ . The first condition says that the color being guessed is not already on someone else's head. The second condition says that each subsequent person can recover the color of their hat with 100% certainty. The ultimate question is now just: ""For which $k$ and $n$ does $S$ exist?"" For $n=2$ , we can identify a function $f:[1]\rightarrow [k]$ with its image, in which case the desired strategy is any bijection $S:[k]\rightarrow [k]$ with no fixed points. For $k\geq 2$ , these certainly exist. For $n=k$ , the only possible strategy is to set $S(f)$ to be the unique element of $[k]\setminus \text{Im}(f)$ . For $n=3$ , a strategy $S:\mathcal I\rightarrow [k]$ almost looks like a binary operation $[k]^2\rightarrow [k]$ , but with the caveat that it is not defined on the diagonal of $[k]^2$ . However, by defining $X^2=X$ for all $X\in [k]$ , this extends to a binary operation, which is actually an idempotent quasigroup. Conversely, any idempotent quasigroup restricts to a valid strategy by ignoring the diagonal of $[k]^2$ . This is the idea of the printed answer for the aforementioned Problem 12082 in the AMM, which then proceeds to construct idempotent Latin squares (which are the same thing as quasigroups) for $k\geq 3$ . The same construction can be found in this paper on quasigroups by Bruck. For $n>3$ , I really have no idea what to do with this problem. Purely by virtue of free association, the prescribed conditions make me think of error-correcting codes (which are related to the classic question, as detailed in this MAA Focus article ) and exterior algebras. But in terms of actual problem-solving, I am fairly stumped. I would be interested to see any thoughts that people have on cases with $n>3$ , even if they only cover a few cases. It would also be interesting to come up with strategies for small values of $k-n>0$ .","['combinatorial-designs', 'combinatorics', 'latin-square', 'quasigroups']"
3808941,Is there a name for a group where elements either commute or anti-commute?,"An abelian group is a group where each element commutes with any other element in the set, so $ab=ba$ . I was reading the wikipedia article on the anticommutative property . I think this could be generalised to arbitrary groups as follows: Suppose $G$ embed into the group of units of some ring $R$ (so for $g\in G$ the element $-g$ makes sense, as $-1$ is a unit). Then $a, b\in G$ anticommute if $ab=-ba$ . Another, possibly more general, interpretation is: Let $-1$ be a distinguished central element of order two in $G$ (so for $g\in G$ the element $-g=g(-1)=(-1)g$ makes sense). Say that $a, b\in G$ anticommute with respect to $-1$ if $ab=-ba$ . I was therefore wondering: Is there a specific name for a group where any two elements in the group either commute or anti-commute (with respect to a fixed element $-1$ )? That is, for $a,b$ in $G$ either $ab=ba$ or $ab=-ba$ .","['group-theory', 'terminology']"
3808983,Proving $k^2 \leq 2^{2^{k}}$ from Kuratowski's definition of ordered pair. Is it possible to extend this?,"If we use Kuratowski's definition of an ordered pair, we have that $\left(a,b\right)=\left\{\left\{a\right\},\left\{a,b\right\}\right\}$ , where $a,b\in X$ and $\left(a,b\right)\in X\times X$ . However, note that this definition also implies that $\left(a,b\right)\in\mathscr{P}\left(\mathscr{P}\left(X\right)\right)$ . Suppose that $X$ is a finite set with $k$ elements. Then, it's not hard to prove that $|X\times X|=k^{2}$ and $|\mathscr{P}\left(X\right)|=2^{k}$ , which gives us $|\mathscr{P}\left(\mathscr{P}\left(X\right)\right)|=2^{2^{k}}$ . Given that this definition of ordered pairs implies that $X\times X\subseteq \mathscr{P}\left(\mathscr{P}\left(X\right)\right)$ , that would make this a valid proof that $k^{2}\leq 2^{2^{k}}$ for all integers $k\geq1$ . Question: Can this definition be extended to ordered $n$ -tuples to prove similar inequalities for $k^{n}$ ? If so, what does it looks like? If not, why not? I know that we can recursively define ordered triples by $$\left(a,b,c\right)=\left(\left(a,b\right),c\right)=\left\{\left\{\left(a,b\right)\right\},\left\{\left(a,b\right),c\right\}\right\}$$ $$=\left\{\left\{\left\{\left\{a\right\},\left\{a,b\right\}\right\}\right\},\left\{\left\{\left\{a\right\},\left\{a,b\right\}\right\},c\right\}\right\},$$ and so on for ordered $n$ -tuples. However, this definition means that $\left(a,b,c\right)$ is not the member of any level of power set of $X$ (not $\mathscr{P}\left(X\right)$ , $\mathscr{P}\left(\mathscr{P}\left(X\right)\right)$ , $\mathscr{P}\left(\mathscr{P}\left(\mathscr{P}\left(X\right)\right)\right)$ , etc.), which means that we can't use (a slightly modified form of) the original argument to find an inequality for $k^{3}$ . Since $2^{5} > 2^{2^{2}}$ , this means that the ordered $5$ -tuples of elements of $X$ can't be contained inside of $\mathscr{P}\left(\mathscr{P}\left(X\right)\right)$ (at least for $X$ with at least $2$ elements), which means defining higher-order ordered tuples that are consistent with the original Kuratowski definition but also extend it in a way that allows for further power set-related inequality arguments will require a deeper nesting of power sets, if such a definition exists. Note: The inequalities that would be found using this would be terribly loose to the point of being beyond trivial. Since there are no integers $k\geq1$ with $k^{3}$ or $k^{4}$ greater than $2^{2^{k}}$ , the further nesting of exponential functions that will result from the nesting of power sets needed to define ordered triples and ordered $4$ -tuples will already lead to inequalities that are extremely weak.","['elementary-set-theory', 'combinatorics']"
3808989,"Is the axiom of choice used in the proof that every open set is the union of basis elements (Mukres, Lemma 13.1)","Here is a Lemma from the Munkres' topology. Lemma: Let $X$ be a set and let $B$ be a basis for a topology $T$ on $X$. Then, every element of $T$ is equal to the union of some elements of $B$. In the proof, Munkres' writes that given $U \in T$, choose for each $x\in U$ an element $B_x$ of $B$ such that $x\in B_x \subset U$. Am I correct that when he chooses $B_x$ he is in fact using the Axiom of Choice?","['axiom-of-choice', 'general-topology']"
3808999,Why doesn't $f'(x) \to 0$ at $x \to \infty$ imply that $\lim_{x \to \infty}{f(x)}$ exists?,"I know there are several examples where $f'(x) \to 0$ at $x \to \infty$ , but $f(x)$ is unbounded, like $f(x) = \ln (x)$ , but I cannot grasp this logically. When the slope of a function is tending to zero, doesn't this mean the function itself is approaching a constant value?","['limits', 'functions', 'derivatives']"
3809008,How to calculate the area of $\triangle ABC$ when the distance from $BC$ to the circumcircle at $G$ is 10?,"$\triangle ABC$ is right angle triangle and its circumcenter is $O$ . $G$ is a point where $BC$ is tangent to the incircle. The
perpendicular distance from $BC$ to circumcircle at $G$ is 10. How to
calculate the area of $\triangle ABC$ ? I have tried to prove if the incenter, circumcenter and orthocenter are collinear but failed. I couldn't find what was special about the point $G$ . What would be the correct approach to solve this problem?","['triangles', 'area', 'circles', 'geometry']"
3809012,How to prove that $\lim\limits_{x \to \infty}\frac{\pi\left(\frac{4x}{3}\right)}{\frac{x}{3\ln x}}=4$?,"I'm a member of a Facebook-based mathematics group. Recently, one of the members made a post detailing an observation he made in his free time, namely that $\pi(4x/3)-\pi(x)$ (here, $\pi(x)$ denotes the prime-counting function) is approximated rather closely by $\frac{x}{3\ln(x)}$ . Knowing about the prime number theorem, I immediately recognized that this might be due to the fact that $\lim_{x\to\infty}\frac{\pi(x)}{x/\ln(x)}=1$ . This guess turned out to be partially correct. It turns out that these two are in fact asymptotic, in the sense that $\lim_{x\to\infty}\frac{\pi(4x/3)-\pi(x)}{x/(3\ln x)}=1$ , but I haven't been able to prove this completely. Here's my reasoning so far: The limit $\lim_{x\to\infty}\frac{\pi(4x/3)-\pi(x)}{x/(3\ln x)}$ can be found by evaluating $\lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}$ and $\lim_{x\to\infty}-\frac{\pi(x)}{x/(3\ln x)}$ . The latter of these is easy to compute (just multiply the limit $\lim_{x\to\infty}\frac{\pi(x)}{x/\ln(x)}=1$ by $-3$ and rewrite $3$ as $\frac{1}{1/3}$ ). Consulting WolframAlpha, one finds that $\lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}=4$ , so $$\lim_{x\to\infty}\frac{\pi\left(\frac{4x}{3}\right)-\pi(x)}{\frac{x}{3\ln(x)}}=4-3=1$$ My question is this: how can I prove that $\lim_{x\to\infty}\frac{\pi(4x/3)}{x/(3\ln x)}=4$ ? I imagine the prime number theorem will come into play here, but I'm not sure how. In particular, I don't know how the $\frac{4}{3}$ factor that appears in the argument of $\pi(x)$ affects the limit, much less how to deal with it.","['limits', 'asymptotics']"
3809017,Will the basis case always be the first value within the range given in proof by induction?,"I just started learning proofs by induction, and typically in the example problems the first value within the range of values of consideration satisfies the base case, but I was curious if there is ever an instance where the first value of the range given would not satisfy the induction proof? Would this be possible given that questions typically ask for a proof by induction over a specific range? Would we not be skipping a value if the base case is not the first value from the range of values?","['induction', 'discrete-mathematics']"
3809026,Finding $f(x)$ for any $x$ by assuming $a_{n+1}$,"I'm trying to solve this problem in the implicit differentiation section of the book I'm going through: The equation that implicitly defines $f$ can be written as $y = \dfrac{2 \sin x + \cos y}{3}$ In this problem we will compute $f(\pi/6)$ . The same method could be
used to compute $f(x)$ for any value of $x$ . Let $a_1 = \dfrac{2\sin(\pi/6)}{3} = \dfrac{1}{3}$ and for every positive integer $n$ let $a_{n+1} = \dfrac{2\sin(\pi/6) + \cos a_n}{3} = \dfrac{1 + \cos a_n}{3}$ (a) Prove that for every positive integer $n$ , $|a_n - f(\pi/6)| \leq 1/3^n$ (Hint: Use mathematical induction) (b) Prove that $\lim_{n \to \infty} a_n = f(\pi/6)$ Now, I'm not sure how to solve $f(\pi/6)$ in the base case of the induction proof. Also, does this above pattern of assuming $a_{n+1}$ and using that to compute $f(x)$ for any value of $x$ has any name ? I would like to read more about it.","['calculus', 'implicit-differentiation', 'derivatives']"
3809100,Maximum value of $abc$ for $a+b+c=5$ and $a^2+b^2+c^2=11$,"$a,b,c$ are three real numbers such that $a+b+c=5$ and $a^2+b^2+c^2=11$ ,
what's the maximum value of $abc$ ? I thought of a way, $ab+bc+ca$ is not hard to find, $a,b,c$ satisfy the cubic equation $x^3 - 5 x^2 + 7 x - abc = 0$ , then use the discriminant of the cubic equation non-negative. The discriminant of $x^3 + A x^2 + B x + C=0$ is $A^2 B^2 - 4 B^3 - 4 A^3 C + 18 A B C - 27 C^2$ Is there an easier way?","['contest-math', 'inequality', 'symmetric-polynomials', 'optimization', 'algebra-precalculus']"
3809124,"What is $\underset{\pi \in S_n}{\max}\underset{1\leq i\leq n}{\sum}|i-\pi(i)|$, where $S_n$ is the set of permutations of $(1,2,\ldots,n)$?","What is the value of $$\underset{\pi \in S_n}{\max} \biggl( \underset{1\leq i\leq n}{\sum} \big|i-\pi(i)\big|\biggr)\,,$$ where $S_n$ is the set of permutations of $(1,2,\ldots,n)$ ? I can see that if $n$ is  even,then we  can achieve the sum to be $\frac{n^2}{2}$ by considering the following permutation: Switch the following pairs $(1,\frac{n}{2}+1),(2,\frac{n}{2}+2),.......,(\frac{n}{2},n)$ . Can we go above $\frac{n^2}{2}$ ? I am trying to decompose $\pi\in S_n$ as composition of transpositions or cycles and to observe how a particular transposition affects the sum. Any help would be appreciated.Thanks in advance.","['permutations', 'summation', 'combinatorics', 'discrete-mathematics', 'discrete-optimization']"
3809136,"I found an oblique asymptote of $f(x) = x\left|\frac{x}{x+1}\right|$, but wolfram didn't. Who is correct?","Let $f(x) = \frac{x|x|}{|x+1|} =  \frac{x|x|}{|x||1+\frac1x|} =   \frac{x}{|1+\frac1x|}   $ My solution We are looking for a line $y= \alpha x + \beta$ $ \alpha = \lim_{x \to \pm \infty} \frac{f(x)}{x} = \lim_{x \to \pm \infty} \frac{x}{|1+\frac1x|}  \cdot \frac1x = \lim_{x \to \pm \infty} \frac{1}{|1+\frac1x|} = 1$ Hence, $\boxed{ \alpha =1 }$ $\beta = \lim_{x \to \pm \infty} f(x) - \alpha x \stackrel{let \text{ } x\to +\infty}{=} \lim_{x \to +\infty} \frac{x}{1 + \frac1x}  - x =\lim_{x \to \pm \infty} \lim_{x \to +\infty} \frac{x^2}{x+1}  - x \\ \quad=\lim_{x \to +\infty} \frac{x^2-x^2-x}{x+1} = - \lim_{x \to \pm \infty}1-\frac1{x+1} = -1$ Hence, $\boxed{\beta = -1}$ Therefore an oblique asymptote is: $\boxed{y =x-1}$ And it seems I am right: Wolfram's solution I seached for f(x) asymptotes and it only found a vertical (I did too) Then I asked it explicitly for a horizontal and it found none Similar query with the word oblique in the comments. That made me really wonder if there is something wrong with my solution. I triple checked everything and it seems fine. Is there something wrong with my solution? Is $y = x-1$ an asymptote?","['calculus', 'functions']"
3809222,Is $\frac{1}{x}$ a monotonic-decreasing function?,"The derivative of $f(x)=\frac{1}{x}$ is always negative for all $x$ except $x=0$ (where it is not defined). But $f(-1)<f(1)$ , so the statement that $f(x)$ either decreases or remains constant as $x$ increases for all $x$ does not seem to be holding good here. So , can I say that $f(x)=\frac{1}{x}$ a monotonic-decreasing function? Thanks !","['functions', 'monotone-functions']"
3809224,Derivative of Matrix with respect to matrix,"Let $x \in \mathbb{R}^d,W \in \mathbb{R}^{dxd}$ $\frac {\partial{}}{\partial{W_{i,j}}}(Wx+b)$ What I have done so far is $W_{i,j}.x_j = 
\begin{pmatrix}
\sum_{i} W_{1,i}.x_i \\
\vdots \\
\vdots   \\
\sum_{i} W_{d,i}.x_i \\
\end{pmatrix}$ Now if I take the derivative of the product mentioned above, Theoretically it should mean that all the entries of $x$ should be in the answer and the answer would be $x_i$ ?","['matrices', 'multivariable-calculus', 'matrix-calculus', 'partial-derivative']"
3809290,Prove that the least upper bound of $\mathcal F$ is $\bigcup\mathcal F$ and the greatest lower bound of $\mathcal F$ is $\bigcap\mathcal F$.,"Not a duplicate of this or this . This is exercise $4.4.23$ from the book How to Prove it by Velleman $($$2^{nd}$ edition $)$ : Prove theorem $4.4.11.$ Theorem $4.4.11.$ Suppose $A$ is a set, $\mathcal F\subseteq \mathscr P(A)$ , and $\mathcal F\neq \emptyset$ . Then the least upper bound of $\mathcal F$ $($ in the subset partial order $)$ is $\bigcup\mathcal F$ and the greatest lower bound of $\mathcal F$ is $\bigcap\mathcal F$ . Here is my proof: Let $F$ be an arbitrary element of $\mathcal F$ . Let $x$ be an arbitrary element of $F$ . Ergo clearly $x\in\bigcup\mathcal F$ . Since $x$ is arbitrary, $F\subseteq\bigcup\mathcal F$ . Therefore if $F\in\mathcal F$ then $F\subseteq\bigcup\mathcal F$ . Since $F$ is arbitrary, $\bigcup\mathcal F$ is an upper bound for $\mathcal F$ . Let $U$ be the set of all upper bounds for $\mathcal F$ and let $X$ be an arbitrary element of $U$ . Let $y$ be an arbitrary element of $\bigcup\mathcal F$ . So we can choose some $G_0\in\mathcal F$ such that $y\in G_0$ . Since $X$ is an upper bound for $\mathcal F$ then $G_0\subseteq X$ . Since $y\in G_0$ , $y\in X$ . Since $y$ is arbitrary, $\bigcup\mathcal F\subseteq X$ . Thus if $X\in U$ then $\bigcup\mathcal F\subseteq X$ . Since $X$ is arbitrary, $\bigcup\mathcal F$ is the smallest element of $U$ and hence the least upper bound for $\mathcal F$ . Let $F$ be an arbitrary element of $\mathcal F$ . Let $x$ be an arbitrary element of $\bigcap\mathcal F$ . Ergo clearly $x\in F$ . Therefore if $F\in\mathcal F$ then $\bigcap\mathcal F\subseteq F$ . Since $F$ is arbitrary, $\bigcap\mathcal F$ is a lower bound for $\mathcal F$ . Let $L$ be the set of all lower bounds for $\mathcal F$ and let $Y$ be an arbitrary element of $L$ . Let $y$ be an arbitrary element of $Y$ . Since $Y$ is a lower bound for $\mathcal F$ , $Y\subseteq F$ . Since $y\in Y$ , $y\in F$ . Since $F$ is arbitrary, $y\in\bigcap\mathcal F$ . Since $y$ is arbitrary, $Y\subseteq \bigcap\mathcal F$ . Thus if $Y\in L$ then $Y\subseteq \bigcap\mathcal F$ . Since $Y$ is arbitrary, $\bigcap\mathcal F$ is the biggest element of $L$ and hence the greatest lower bound for $\mathcal F$ . $Q.E.D.$ Is my proof valid $?$ Thanks for your attention.","['order-theory', 'proof-writing', 'solution-verification', 'discrete-mathematics']"
3809316,Factorization in formal power series versus in convergent power series over the complexes,"Let $R=\mathbb C\{x_1,...,x_n\}\subset S=\mathbb C [[x_1,...,x_n]]$ denote the ring of convergent, respectively formal, power series over $\mathbb C$ . Suppose $f\in R$ is irreducible in $R$ . Does it remain irreducible in $S$ ?","['complex-analysis', 'algebraic-geometry', 'several-complex-variables']"
3809321,Prove that $\sqrt{8}$ is irrational in different method,"I tried to prove that $\sqrt{8}$ is irrational. I said let $\sqrt{8}$ be rational then $\sqrt{8}$ = $a/b$ where $a$ and $b$ are relatively prime. Then $2\sqrt{2}=a/b$ , and $\sqrt{2} =a/(2b)$ . it is obvious that $RHS$ is rational and $LHS$ is irrational (assumed that $\sqrt{2}$ is proved). So there is a contradiction and proof done. My question is that is there other ways to prove that $\sqrt{8}$ is irrational?","['alternative-proof', 'proof-writing', 'solution-verification', 'discrete-mathematics']"
3809387,Asymptotic variance of an estimator,"Suppose we have an estimator (i.e. a sequence of estimators) $T_n$ which is asymptotically normal, in the sense that $\sqrt{n}(T_n - \theta)$ converges in distribution to $\mathcal{N}(0, \sigma^2)$ . The variance $\sigma^2$ is usually called the asymptotic variance of the estimator, but can we write that $\lim_{n\to\infty}\textrm{Var}[\sqrt{n}T_n]=\sigma^2$ ? If not, what additional conditions on the sequence $T_n$ we would need in order to do so ? Are consistency of $T_n$ and uniform integrability of $T_n^2$ sufficient conditions ?","['statistics', 'weak-convergence', 'parameter-estimation', 'convergence-divergence', 'probability-theory']"
3809460,"Let $N\sim \mathcal N(0,\sigma^2)$ be a normal RV. Let $g$ be continuous. Can we bound $E[g(N)]$ in terms of $\sigma^2$?","Let $N\sim \mathcal N(0,\sigma^2)$ be a normal RV. Let $g$ be continuous. Can we bound $E[g(N)]$ in terms of $\sigma^2$ ? This seems fairly easy but it might be wrong. What if we assume additional structure on $g$ ? For example what if assume that $g$ is continuous convex? This is something that came up in research.","['probability-theory', 'probability', 'upper-lower-bounds']"
3809462,Help on a proof about some property of a solution to a given algorithmic problem.,"Below is a problem I am trying to solve: There are $n$ people to be allocated on $k$ gondolas. The first $q_1$ persons will go with the first gondola. The first $q_2$ persons from the remaining crowd will go with the second gondola and so on. In then end: $\sum_{i=1}^{k}q_i = n, q_i\geq1$ . Now if two different guys $i$ and $j$ use the same gondola, the unfamiliarity of the given gondola is increased by $u_{i,j} \geq 0$ . We have to arrange the allocation of people on gondolas such that the sum of unfamiliarities of all gondolas is as small as possiblle. There is a link to the original problem. We try to find a solution using a following idea: For a given $1 \leq i \leq k$ and $1 \leq j \leq n$ , let $dp[i][j]$ be a solution of the problem if we only consider the first $j$ persons and $i$ gondolas. Then we have the following reccurence formula: $$dp[i][j] = max_{k<j}( dp[i-1][k] + \sum_{a = k+1}^{j}\sum_{b = a+1}^{j}u_{a,b} ) $$ Finally we come to my question: If we denote $A[i][j]$ to be the minimal k such that the above identity holds, then $A[i][j] \leq A[i][j+1]$ Can you help me to prove that assertion?","['contest-math', 'recurrence-relations', 'dynamic-programming', 'discrete-mathematics', 'algorithms']"
3809473,Find n if $\cos18°-\sin18°=\sqrt{n}\sin27°$,"Find $n$ if $$\cos18°-\sin18°=\sqrt{n}\sin27°$$ I know this can be solved by directly substiuiting values of $\cos18°,\sin18°,\sin27°$ but is there a clever way to solve this question using trigonometric identities and minimal usage of using values from the table? I tried taking $A=9°$ and solving but that is not helping much. Possibly it can be solved using complex numbers.Any hints??",['trigonometry']
3809483,Eikonal equation and Fermat's principle,"Consider the eikonal equation $$
\left\lbrace
\begin{array}{lcl}
||\nabla\phi|| = f(x) &, & x \in \Omega \\
\phi(x) = 0 &, & x \in \Gamma \subseteq \partial \Omega
\end{array}
\right.
$$ Is there any way to deduce, only from that PDE , a variational principle similar to Fermat's principle in Geometric Optics? I mean, is it posible to conclude that the line integral $$
\phi[x(s)] = \int f(x(s)) \ ds
$$ reaches its minimum along the light rays (characteristic curves of the PDE)? I've tried to apply the ODEs the method of characteristics provide when applied to the eikonal equation, but with no conclusive results. Thanks in advance.","['variational-analysis', 'partial-differential-equations', 'differential-geometry']"
3809497,"Show that $\nu(E) = \int_E \phi \,d \mu$ is inner and outer regular.","Let $\phi \geq 0$ be a function in $L^1(\mu)$ where $\mu$ is a Radon measure (= a Borel measure on $X$ that is finite on compact sets, inner regular on open sets and outer regular on all compact sets) on the locally compact Hausdorff space $X$ . Show that $$\nu(E) = \int_E \phi \,d \mu$$ is again a Radon measure. Attempt : Trivially, since $\nu(X) <\infty$ , $\nu$ is finite on compact sets. It remains to show that $$\nu(E) = \inf\{\nu(U): U \supseteq E, U \text{ open}\}$$ $$\nu(U) = \sup\{\nu(K): K \subseteq U, K \text{ compact}\}$$ where $E$ is a Borel set of $X$ and $U$ is an open set of $X$ . I managed to show that this is true when $\mu(E), \mu(U) < \infty$ by invoking the following: $$\forall \varepsilon > 0: \exists \delta > 0: \mu(F) < \delta\implies \nu(F) < \varepsilon$$ However, I'm stuck if $\mu(E) = \infty = \nu(U)$ . How can I proceed?","['general-topology', 'measure-theory', 'borel-measures']"
3809530,"If $P(x)=\sum_{i=0}^da_i\left(\prod_{j=i}^{d+i-1}(x+j)\right)$ is linear, what is its constant term?","Question : Fix $d,m\in\mathbb{N}$ with $0\leq m\leq d$ and define $$P(x)=\sum_{i=0}^da_i\left(\prod_{j=i}^{d+i-1}(x+j)\right),$$ where each $a_i$ is a constant, $a_m=0$ . Suppose that, after expansion, $P(x)=c-x$ for some constant $c$ . Show that $c=\frac{m}{d}-d$ . I obtained a rough solution by evaluating $P\left(-(d+k)\right)$ for each $0\leq k\leq m$ , which yields $m+1$ linear relations on $a_0,\dots, a_{m-1}$ and $c$ , from which one can then solve by scaling and subtracting. However, I am hoping for a cleaner, more succinct answer (in fact, one might even be able to use the above approach in a neater way than I did).","['interpolation', 'combinatorics', 'polynomials']"
3809531,Question about conditional independence,"Assumptions Let $X,Y,Z$ be random variables such that $Z$ is independent of $(X,Y)$ . Let $h$ be a bivariate function such that $h(X,Z)$ is a random
variable. Question Is $h(X,Z)$ conditionally independent of $Y$ given $X$ ? Attempt Let $A$ and $B$ be Borel sets. Let $x$ be a real number. Then $$P(h(X,Z)\in A,Y\in B\mid X=x)=P(h(x,Z)\in A,Y\in B\mid X=x).$$ It remains to show that $$P(h(x,Z)\in A,Y\in B\mid X=x)=P(h(x,Z)\in A\mid X=x)P(Y\in B\mid X=x).$$ I guess this equality follows from the  assumption that $Z$ is independent of $(X,Y)$ . My reasoning: The assumption implies that $Z$ is independent of $Y$ . This implies that $h(x,Z)$ and $Y$ are independent as $h(x,Z)$ is a function of $Z$ only. However, I am wondering if $h(x,Z)$ and $Y$ are also independent when we condition on the event $\{X=x\}$ ; and if so, why that is the case. Maybe this argument works. $$\begin{align}P(h(x,Z)\in A,Y\in B\mid X=x)&=\frac{P(h(x,Z)\in A,Y\in B,X=x)}{P(X=x)}\\
&=\frac{P(h(x,Z)\in A)P(Y\in B,X=x)}{P(X=x)}\\
&=P(h(x,Z)\in A)\frac{P(Y\in B,X=x)}{P(X=x)}\\
&=P(h(x,Z)\in A\mid X)\frac{P(Y\in B,X=x)}{P(X=x)}\\
&=P(h(x,Z)\in A\mid X)P(Y\in B\mid X=x).\end{align}$$ This assumes that $P(X=x)>0$ . What if $P(X=x)=0$ ? I guess I can make a similar derivation by looking at densities. Then my derivation would be, with integration over $\mathbb{R}^2$ , $$\begin{align}P(h(x,Z)\in A,Y\in B\mid X=x)&=\int1_{A}(h(x,z))1_B(y)f(z,y|x)dzdy\\
&=\cdots\\
&=\int1_{A}(h(x,z))1_B(y)f(z|x)f(y|x)dzdy\\
&=\int1_{A}(h(x,z))f(z|x)dz\int1_B(y)f(y|x)dy\\
&=P(h(x,Z)\in A\mid X)P(Y\in B\mid X=x).\end{align}$$ How can I provide a more general proof? Conditional distributions was what I found hard to understand when I read courses in measure-theoretic probability.","['independence', 'probability-theory']"
3809555,"$f(x+y)=f(x)+f(y)+99$, $f(100)=101$, find $f(7)$.","Question : Let $f:\mathbb{Q} \longrightarrow \mathbb{Q}$ , and $\forall x,y \in \mathbb{Q}$ , $f(x+y)=f(x)+f(y)+99$ , $f(100)=101$ , find $f(7)$ . Attempts : I am pretty sure this is one of those questions which require an amazingly genius solution. However after having tried for half an hour, I still couldn’t get a clue. My problem is that I don’t really know how $f(7)$ can show up. What I know: $f(100)=2f(50)+99 \Longrightarrow f(50)=1$ $f(50)=2f(25)+99 \Longrightarrow f(25)=-49$ $f(x)=f(x)+f(0)+99 \Longrightarrow f(0)=-99$ From here I want to break it down to smaller numbers, but I can’t find a way. Any suggestions or hints will be appreciated.","['contest-math', 'functions']"
3809588,Do we need full $\mathsf{AC}$ to efficiently use (sub)bases?,"Very belatedly, sorry: also asked at MO . Suppose $(X,\tau)$ is a topological space, $B$ is a base for $\tau$ , and $U\in \tau$ is an open set. Consider the following two strategies for writing $U$ as a union of elements of $B$ : We have $U=\bigcup\{V\in B: V\subseteq U\}$ . For each $u\in U$ pick some $V_u\in B$ with $u\in V_u\subseteq U$ ; then $U=\bigcup\{V_u: u\in U\}$ . The first strategy has the advantage of not requiring the axiom of choice. If we pay attention to the number of basic opens required, however, it is noticeably inefficient: the first strategy might involve as many as $2^{\vert U\vert}$ -many basic open sets, while the second involves at most $\vert U\vert$ -many. It's not hard to show that in fact this drop in efficiency is unavoidable: it is consistent with $\mathsf{ZF}$ that there is a space $(X,\tau)$ , a base $B$ for $\tau$ , and an open set $U\in\tau$ such that there is no map $f:U\rightarrow B$ with $\bigcup_{u\in U}f(u)=U$ . I'm interested in the exact strength of the corresponding efficiency principle, as well as its ""subbase"" variation: Over $\mathsf{ZF}$ , are either of the following statements equivalent to $\mathsf{AC}$ ? For every topological space $(X,\tau)$ , every base $B$ for $\tau$ , and every $U\in\tau$ , there is some $f:U\rightarrow B$ with $\bigcup_{u\in U}f(u)=U$ . For every topological space $(X,\tau)$ , every subbase $B$ for $\tau$ , and every $U\in\tau$ , there is some $f:U\rightarrow [B]^{<\omega}$ with $\bigcup_{u\in U}(\bigcap f(u))=U$ . (Above, "" $[A]^{<\omega}$ "" denotes the set of finite subsets of $A$ . So the subbase version of the principle is saying that we can write $U$ as the union of $U$ -many finite intersections of subbase elements.)","['axiom-of-choice', 'general-topology', 'set-theory']"
3809616,"Proof of Theorem 4.16 from Mathematical Statistics by Jun Shao (Second Edition, Section 4.5.1, p.287)","First I would like to state the Theorem - it reads as follows: Let $X_{1}, \dotsc, X_{n}$ be i.i.d. from a p.d.f. $f_{\theta}$ w.r.t. a $\sigma$ -finite measure $\nu$ on $(\mathcal{R},\mathcal{B})$ where $\mathcal{R}$ is supposed to represent the real line and $\mathcal{B}$ shall be the borel $\sigma$ -field on $\mathcal{R}$ . Further let $\theta \in \Theta$ , where $\Theta$ is an open set in $\mathcal{R}^k$ . Suppose that for any $x$ in the range of $X_{1}$ , $f_{\theta}(x)$ is twice continuously differentiable in $\theta$ and satisfies $$\frac{\partial}{\partial \theta}\int\psi_{\theta}(x)d\nu = \int\frac{\partial}{\partial \theta}\psi_{\theta}(x)d\nu$$ for $\psi_{\theta}(x) = f_{\theta}(x)$ and $\psi_{\theta}(x) = \frac{\partial}{\partial \theta}f_{\theta}(x)$ ; The Fisher information matrix $$ I_{1}(\theta) = E\left\{\frac{\partial}{\partial \theta}\operatorname{log}f_{\theta}(X_{1})\left[\frac{\partial}{\partial \theta}\operatorname{log}f_{\theta}(X_{1})\right]^{\mathsf{T}}\right\}$$ is positive definite; and for any given $\theta \in \Theta$ , there exists a positive number $c_{\theta}$ and a positive function $h_{\theta}$ such that $E\left[h_{\theta}(X_{1})\right] < \infty$ and $$\sup_{\gamma: \vert\vert\gamma-\theta\vert\vert<c_{\theta}}\left\vert\left\vert\frac{\partial^2\operatorname{log}f_{\gamma}(x)}{\partial \gamma\partial{\gamma}^{\mathsf{T}}}\right\vert\right\vert \leq h_{\theta}(x)$$ for all $x$ in the range of $X_{1}$ , where $\left\vert\left\vert A \right\vert\right\vert = \sqrt{\operatorname{tr}\left(A^{\mathsf{T}}A\right)}$ for any matrix $A$ . Let $\skew{4}\hat{\theta}_{n}$ be an estimator of $\theta$ (based on $X_{1}, \dotsc, X_{n}$ ) and suppose that $$ \left[V_{n}(\theta)\right]^{-\frac{1}{2}}\left(\skew{4}\hat{\theta}_{n}-\theta\right) \overset{d}{\to}\mathcal{N}_{k}(0, I_{k})$$ with $V_{n}(\theta) = \frac{V(\theta)}{n}$ and $V(\theta)$ is some positive definite matrix, $I_{k}$ represents the $k\times k$ identity matrix, and $\overset{d}{\to}$ indicates convergence in distribution. Let, for every $n$ , $I_{n}(\theta)$ be the Fisher information about $\theta$ contained in $X_{1}, \dotsc, X_{n}$ , then, given the above scenario, there exists a $\Theta_{0} \subset \Theta$ with Lebesgue measure $0$ such that $$V_{n}(\theta) \geq \left[I_{n}(\theta)\right]^{-1}$$ holds for $\theta \notin \Theta_{0}$ . The proof of the univariate case can be summarized as follows: We take a sequence of realizations $x = (x_{1},\dotsc,x_{n})$ , take $\theta_{n} = \theta + n^{-\frac{1}{2}}\in\Theta$ , and set $$K_{n}(x,\theta) = \frac{\left[\operatorname{log}l(\theta_{n}) - \operatorname{log}l(\theta) + \frac{I_{1}(\theta)}{2}\right]}{\left[I_{1}(\theta)^{\frac{1}{2}}\right]},$$ where $l()$ denotes the likelihood function. Under the given assumtions of the Theorem the first result is that $$K_{n}(X,\theta)\overset{d}{\to} \mathcal{N}(0,1).$$ Then let $P_{\theta_{n}}$ (or $P_{\theta}$ ) be the distribution of the sequence $X = (X_{1},\cdots,X_{n})$ under the assumtion that $X_{1}$ has p.d.f. $f_{\theta_{n}}$ (or $f_{\theta}$ ) and define $g_{n}(\theta) = \left\vert P_{\theta}\left(\skew{4}\hat{\theta}_{n} \leq \theta\right)-\frac{1}{2}\right\vert$ . The next result is that there exists a subsequence $\{n_{k}\}$ and a $\Theta_{0} \subset \Theta$ with Lebesgue measure zero such that $$\lim_{k \to \infty}g_{n_k}(\theta_{n_{k}}) = 0, \quad \theta \notin \Theta_{0}.$$ Now, if we let $\Phi$ to represent the standard normal c.d.f. and assume that $\theta \notin \Theta_{0}$ , it is shown in a next step that for $t>\left[I_{1}(\theta)\right]^{\frac{1}{2}}$ , $$P_{\theta_{n}}\left(K_{n}(X,\theta)\leq t\right) \overset{n \to \infty}{\to} 
\Phi\left(t-\left[I_{1}(\theta)\right]^{\frac{1}{2}}\right).$$ Then this last result and the fact that $$\lim_{k \to \infty}g_{n_k}(\theta_{n_{k}}) = 0, \quad \theta \notin \Theta_{0}$$ imply that there is a subsequence $\{n_{j}\}$ such that for $j = 1,2,\dotsc,$ $$P_{\theta_{n_j}}\left(\skew{4}\hat{\theta}_{n_j} \leq \theta_{n_j}\right) < P_{\theta_{n_j}}\left(K_{n_j}(X,\theta) \leq t\right).$$ Then the Author further concludes that this above inequality and the Neyman-Pearson lemma implies that for $j = 1,2,\dotsc,$ $$P_{\theta}\left(\skew{4}\hat{\theta}_{n_j} \leq \theta_{n_j}\right) < P_{\theta}\left(K_{n_j}(X,\theta) \leq t\right).$$ Now finally here comes my question: How can we use the Neyman-Pearson lemma to arrive via the second last inequality at this last inequality?","['statistical-inference', 'statistics', 'probability-theory']"
3809654,Can a single point be considered a function?,"A relation is defined as a set of ordered pairs. A set can include a single point. A function is a relation. Does that mean a single point can be considered a function? Say you have the point (1,0). There is a unique output for the input, but does there need to be multiple inputs and multiple outputs?","['functions', 'relations']"
3809681,Jordan-Hahn decomposition in Robert Ash's book,"I was reading through the book ""Real Analysis and Probability"" by Robert Ash, and got really confused by the proof given to the Jordan-Hahn decomposition. The theorem states the following. Let $\lambda$ be a countably additive extended real valued function on the $\sigma$ field F, then defining: $\lambda ^+(A)= \sup\{\lambda(B): B \in F , B\subset A\}$ $\lambda ^-(A)= -\inf\{\lambda(B): B \in F , B\subset A\}$ Then $\lambda^+, \lambda^-$ are measures on F and $\lambda=\lambda^+-\lambda^-$ In the proof he says we can assume that $\lambda$ doesn't assume the value $- \infty$ , by using the following reasoning. If $- \infty$ belongs to the range of $\lambda$ then, $\infty$ does not, by the definition of countable additive function. Thus $-\lambda$ never takes the value $- \infty$ , but I don't understand how this last sentence justifies that we can assume such fact.","['proof-explanation', 'measure-theory', 'probability-theory', 'real-analysis']"
3809693,Are regular measures continuous?,"I'm taking a course on Measure Theory, and we are building measures from the very beggining, starting with semi-algebras $\mathcal{S}$ and proving extension theorems to get to measures on $\sigma$ -algebras. Now, we have proved Caratheodory's Extension Theorem, asserting that we can extend a $\sigma$ -additive measure defined on a semi-algebra $\mathcal{S}$ or on the algebra generated $\mathcal{A}(\mathcal{S})$ to a $\sigma$ -additive measure on the $\sigma$ -algebra $\mathcal{F}(\mathcal{S})$ (uniquely if we started with a $\sigma$ -finite measure). The plan now is to use this to construct the Lebesgue measure $\lambda$ on $\mathbb{R}$ . So we must find a way to prove that the Lebesgue measure is $\sigma$ -additive on the algebra os intervals $\mathcal{A}(\mathcal{S})$ . The proof presented in class was a particular instance of the a general fact: if $\mu$ is a finitely additive and regular measure defined on an algebra, then it is $\sigma$ -additive. However, I was wondering if it would be possible to take a different approach. This fact seems to be heavily dependent on the topological properties of the underlying space, but I was wondering if the (slightly) more general result is valid: If $\mu:\mathcal{A}\to[0,+\infty]$ is a finitely additive and regular measure defined on an algebra $\mathcal{A}$ , then it is continuous from below. It is possible to prove that continuity from below implies sigma additive, so this is a slightly more general result. This is my attempt at a proof: Let $E_k, E\in\mathcal{A}$ , where $E_k$ increases to $E$ , i.e., $E_k\subset E_{k+1}$ and $E = \cup E_k$ . For any $\varepsilon>0$ , by regularity, there is a compact set $K\subset E$ , $K \in \mathcal{A}$ such that \begin{equation}\mu(E) - \varepsilon < \mu(K) \leq \mu(E)
\end{equation} My plan is to show that, whatever is $K$ , there is an $n$ such that $\mu(K)\leq\mu(E_n)$ . This way, when we take the supremum over all compact sets $K\subset E$ , we get that $\mu(E_n)\to\mu(E)$ . I have tried various approaches to prove this, but I have not been able to succeed. Edit 1: As suggested, I'm stating the definition for regularity in this context. A measure $\mu:\mathcal{S}\to [0,+\infty]$ defined on a class of sets $\mathcal{S}$ in a topological space is said to be regular if, for every $A\in\mathcal{S}$ : \begin{equation}\mu(A) = \inf\{\mu(G) | A\subset G, G\in\mathcal{S}, G \text{ open}\} = \sup\{\mu(K) | K \subset A, K \in \mathcal{S}, K \text{ compact}\}
\end{equation}",['measure-theory']
3809726,How is cardinality exactly defined as a function and why is it different from the ordinals,"Once we construct the definition of the ordinals: $$0=\{\} \, \, 1=\{0\} \,\, 2=\{0,1\} \,\,3= \{0, 1,2\} \,\, ...$$ And we want to describe the cardinality of the set $S$ : $$S=\{3,2,4\}$$ Intuitively we know that: $$|S|=3$$ However, how do we describe the function of $\mathbb{card}(x)$ ?
This function must map the Von-Neumann ordinals to the cardinality of the set. How does this function do that? If the cardinality of the set can be described by an ordinal, why do we denote: $$|\mathbb{N}| = \aleph_0$$ And not: $$|\mathbb{N}| = \omega_0$$ What would be the ordinal number that would be associated with $\aleph_1$ . I thought that it could be $\omega_0 + 1$ since it is the next size of infinity. Like the next size in cardinal arithmetic is always just adding one. Or the idea of the next. However, if it is not that what would it be? Would the construction of the cardinality function help, or not? If this is not clear, please let me know and I'll edit this a bit more. Thanks.","['cardinals', 'ordinals', 'functions', 'natural-numbers', 'set-theory']"
3809793,Parabolic subgroup of a Lie group (as oppose to algebraic group),"Let $G$ be a semisimple Lie group (real or complex). What does it mean for $P\le G$ to be a parabolic subgroup? I only know that if $G$ is an algebraic group then $P\le G$ means that $G/P$ is a complete variety. But I don't see how to define it for a semisimple Lie group Please see for example, the first page of paper ""recurrence properties of random walks on finite volume homogeneous manifolds"" by Eskin and Margulis for the notion of a parabolic subgroup of a Lie group (they didn't give the definition, though).","['algebraic-geometry', 'lie-groups']"
3809831,Can a proper subgroup of the multiplicative group of a finite field form an arithmetic progression.,"To rule out special cases, here the proper subgroup should not be $1$ , and the length of arithmetic progression is finite and at least $3$ . If we can permute a subgroup to form an AP, the it also meets the requirement.
Arithmetic progression in this context is just $\{a+nd| n\in \mathbb{N}， n\le N\}\subset \mathbb{F}$ , where $\mathbb{F}$ is the given finite field. I tried some cases like $\mathbb{Z}/(p)$ , and didn't find any.
So my question is whether we have infinitely many finite fields qualified or the proposition is generally false?","['field-theory', 'number-theory', 'finite-fields', 'abstract-algebra']"
3809851,Show that a real symmetric matrix is always diagonalizable,"Let $A \in \Bbb R^{n \times n}$ be a symmetric matrix and let $\lambda \in \Bbb R$ be an eigenvalue of $A$ . Prove that the geometric multiplicity $g(\lambda)$ of $A$ equals its algebraic multiplicity $a(\lambda)$ . We know that if $A$ is diagonalizable then $g(\lambda)=a(\lambda)$ . So all we have to show is that $A$ is diagonalizable. I found a proof by contradiction. Assuming $A$ is not diagonalizable we have $$(A- \lambda_i I)^2 v=0, \  (A- \lambda_i I) v \neq 0,$$ where $\lambda_i$ is some repeated eigenvalue. Then $$0=v^{\dagger}(A-\lambda_i I)^2v=v^{\dagger}(A-\lambda_i I)(A-\lambda_i I) \neq 0$$ which is a contradiction (where $\dagger$ stands for conjugate transpose). OK but isn't there a better proof? I see it could be approached by the Spectral theorem or Gram Schmidt Prove that real symmetric matrix is diagonalizable . A hint for how to do so would be appreciated.","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'symmetric-matrices', 'diagonalization']"
3809884,Is there active research in Galois Theory?,"I recently decided to introduce myself to the field of Modern Algebra - in particular, Galois theory - and I found it absolutely beautiful! Thus I would really like to study something in Galois theory, which leads me to ask - do people still develop Galois theory? What else is there to learn in the subject? I am inspired by questions like these: What kind of work do modern day algebraists do? and What do modern-day analysts actually do? and would love to learn your opinions, stories, etc! Thanks in advance!","['galois-theory', 'abstract-algebra', 'soft-question']"
3809896,How can we prove that there is a number $a$ such that $\lim_{h\to 0}\frac{a^{h} -1}{h}=1$?,"One definition of $e$ that I am fond of is that it is the number $a$ such that $$
\lim_{h\to 0}\frac{a^{h} -1}{h}=1
$$ The reason for this is that it cuts to the heart of the special property of all exponential functions. If we have $f(x)=a^x$ , then \begin{align}
f'(x)&=\lim_{\Delta x \to 0}\frac{a^{x+\Delta x} -a^x}{\Delta x} \\
&=\lim_{\Delta x \to 0}a^x\frac{a^{\Delta x} - 1}{\Delta x} \\
&=a^x\lim_{\Delta x \to 0}\frac{a^{\Delta x} - 1}{\Delta x}
\end{align} However, presumably there is a caveat to this approach. We need to show that a number like $e$ exists in the first place! In other words, we need to show that $g(a)=\lim_{\Delta x \to 0}\frac{a^{\Delta x} -1}{\Delta x}$ takes the value of $1$ for some value of $a$ . How might we do this?","['limits', 'calculus', 'exponential-function', 'real-analysis']"
3809915,"Finding values of $a$, $b$, $c$, $d$ such that a $f(x)=\frac{ax+d}{cx+b}$ is self inverse","I am trying to solve the following problem: (screenshot) For which numbers $a$ , $b$ , $c$ , and $d$ will the function $$f(x)=\frac{ax+b}{cx+d}$$ satisfy $f(f(x))=x$ for all $x$ ? I have a solution, but I'm not sure if it's right, because it seems overly complex. Could someone check my solution please? We know that $\frac{a\frac{ax+b}{cx+d}+b}{c\frac{ax+b}{cx+d}+d} = x$ for all real numbers $x$ . However, this equality will only hold under 2 conditions: $cx+d \ne 0$ for all real $x$ , because if $cx+d = 0$ , we would have an indeterminate form within the equation. $(ca + cd)x + cb + d^2 \ne 0$ for all real $x$ , for the same reason as above. To ensure the above holds for all $x$ , we must ensure that the $x$ such that each expression $= 0$ is not equal to a real number. Hence: $cx+d=0$ $x=-d/c$ So $c=0$ to prevent $x$ from being a real number, and $d \ne 0$ , to ensure the above expression never equals to $0$ . Hence: $(ca + cd)x + cb + d^2 = 0$ By the same reasoning, $ca + cd=0$ and $cb + d^2 \ne 0$ . Given this, we can rearrange the above to form the quadratic: $(ca+cd)x^2 + (d^2 - a^2)x + ba + bd = 0$ . The only quadratic that outputs $0$ for all values is one where the coefficients all are equal to $0$ . Hence: $ca + cd = 0$ $d^2 - a^2 = 0$ $ba + bd = 0$ Since $c=0$ , there are 2 possibilities: $a=-d$ , $d \ne 0$ and $b =$ all real numbers $a=d$ , $d \ne 0$ and $b = 0$ I'm not sure if this is right though - it seems overly complex, especially the steps where I state that $cx+d \ne 0$ and $(ca + cd)x + cb + d^2 \ne 0$ . Could someone check my solution, and provide me with the correct one, if mine is wrong? Thanks so much in advance!","['inverse-function', 'functions', 'solution-verification', 'algebra-precalculus', 'rational-functions']"
3810091,What is the probability that the total score after throwing darts is divisible by $3$.,"To play a game of darts Michael throws three darts at the dart board shown. The number of points $(1,$ $5$ or $10)$ for each of the three regions is indicated. His score is the sum of the points for the three darts. If the radii of the three concentric circles are $1,$ $2$ and $3$ units, and each dart Michael throws hits this dart board at random, what is the probability that his score is evenly divisible by $3?$ Express your answer as a common fraction. After taking the values modulo $3$ , we have $1, 2, 1$ . I am pretty sure that the only way we can get divisible by $3$ in this problem is if we have modulos $1, 1, 1$ or $2, 2, 2$ for the darts. This means that the probability is ${\left(\dfrac23\right)}^3+{\left(\dfrac13\right)}^3=\dfrac13$ . I feel as if I am missing something, or am I correct? Thanks! EDIT: ""At random"" means that the likelihood of a dart landing in a region is the total area of that region divided by the total area of the dart-board.","['elementary-number-theory', 'probability']"
3810098,How do you get an orthogonal vector in arbitrary dimensions?,"The context is, I am trying to create a program to calculate the gradient of a discretized surface, i.e. a mesh. The trick however is, I need to do this on arbitrary dimensions. I was given this formula: That numerator is all fine and dandy in 3D, since to get the orthognal of that line you can just define the $\perp$ operator as $N\times (x_k - x_j)$ , where $N$ is the normal to the triangle. This however relies on the cross product, which does not exist in most dimensions. If instead of a mesh I have a hyper mesh, how can I get a vector that is both orthogonal to the hyper edge and contained in the hyper face? To clarify, the diagram I have posted is one of the key steps needed to compute a discrete gradient on a 3D mesh (i.e a 2D manifold). In that setting the formula I posted is well defined through the operator I described earlier. It will give you a vector that is both contained in the triangle and orthogonal to the edge. Consider now the same pattern one dimension above, i.e. a 3D manifold embedded in a 4D space. Each hyper face is a tetrahedron in 4D. The extension of the above formula would be; to find a vector that is embedded in the 3D subspace spanned by the vectors of the tetrahedron; and orthogonal to a vector that is orthogonal to all the vectors in that subspace (the hyper normal $N$ ). One of the answers to this question describes a beautiful way to calculate $N$ , however that's only half the problem, we need a vector that is orthogonal to $N$ and to one of the faces of the tetrahedron (in the example).","['numerical-methods', 'geometry', 'discrete-mathematics', 'computational-geometry']"
3810147,What is the intuition for why row reduction can show linear dependence,"You can determine whether a set of column vectors is dependent by placing them in a matrix and getting the matrix into RREF. If all columns have a leading entry of $1$ , then the set of vectors is independent. If a column does not have a leading entry but instead has one or more nonzero entries that are in the same row as a leading entry, then the set of vectors is dependent. For example, it can be shown that the following set of vectors $S$ is dependent: $S=\left(
\begin{bmatrix}
    1 \\
    4 \\
    0 \\
    2 
\end{bmatrix},
\begin{bmatrix}
    3 \\
    1 \\
    4 \\
    0 
\end{bmatrix},
\begin{bmatrix}
    -1 \\
    7 \\
    -4 \\
    4 
\end{bmatrix},
\begin{bmatrix}
    0 \\
    11 \\
    -4 \\
    6 
\end{bmatrix}
\right)$ This set of vectors can placed in a matrix $A$ and row reduced. $A = \begin{bmatrix}
    1       & 3 & -1 & 0 \\
    4       & 1 & 7 & 11 \\
    0       & 4 & -4 & -4 \\
    2       & 0 & 4 & 6 
\end{bmatrix}$ Row reducing shows that the set of vectors is dependent because column $3$ and column $4$ have nonzero entries in the same row as leading entries. $RREF(A)=\begin{bmatrix}
    1       & 0 & 2 & 3 \\
    0       & 1 & -1 & -1 \\
    0       & 0 & 0 & 0 \\
    0       & 0 & 0 & 0 
\end{bmatrix}$ Furthermore, the entries of these columns correspond to scalars of linear combinations which show that the set of vectors is linearly dependent. For example, using the elements $3$ and $-1$ as scalars, it can be shown that column vector $4$ is a linear combination of column vectors $1$ and $2$ .: $
\begin{bmatrix}
    0 \\
    11 \\
    -4 \\
    6 
\end{bmatrix} =
$ $
3\begin{bmatrix}
    1 \\
    4 \\
    0 \\
    2 
\end{bmatrix}$ $-\begin{bmatrix}
    3 \\
    1 \\
    4 \\
    0 
\end{bmatrix}$ Logically, why does the process of row reduction reveal the scalars which prove linear dependence in a set of vectors? I understand that it does work, but not why it should work. EDIT:
I understand that row operations don't change whether a set of column vectors are dependent/independent. My question was as follows: why in RREF do the entries of a column vector correspond to scalars in a linear combination which can prove linear dependence? I thought of an analogy which would serve as an adequate answer to my question, but I'm not sure if it is accurate or not. You can see how large certain numbers are in terms of other numbers through division. For example, how large is the number $5$ in terms of $4$ ? The quotient $5/4=1.25$ tells us that $5$ is $1.25$ times as large as $4$ . Similarly, through row reduction you can express column vectors in terms of other column vectors. When I row reduce columns $1$ and $2$ so that they have a leading entry of $1$ , I express columns $3$ and $4$ in terms of columns $1$ and $2$ . Is this understanding somewhat accurate (it only has to serve as a general intuition)?","['matrices', 'linear-algebra']"
3810150,"What percentage of positive integers, written in base 10, are composite regardless of what base they are interpreted in?","There is a sequence of numbers (OEIS A121719 ) with the following defenition: If the string of base- $10$ digits corresponding to the positive integer $k$ is composite when interpreted in any possible base $b$ , then $k$ is in the sequence. After writing a program to find values of the sequence, I was curious whether the percentage of numbers $k<n$ in the sequence approaches a certain value as $n$ increases. I will now formally define my question: $A$ is the set of all positive integers in the sequence defined above. $$C(n)=1\:\text{if}\:n\in A,\:\text{otherwise}\:0$$ $$L=\lim_{n\to\infty}\frac{1}{n}\sum^n_{i=1} C(i)$$ I would like to know: Does the limit $L$ exist? Is $L>0$ ? Is there some method to find $L$ or the first $n$ digits of $L$ ?","['decimal-expansion', 'limits', 'recreational-mathematics', 'sequences-and-series', 'prime-numbers']"
3810230,"Tangent space of an upper half-space and the relation of $T_x\:M$ and $T_x\:\partial M$, for $x\in\partial M$, in general","Let $d\in\mathbb N$ , $k\in\{1,\ldots,d\}$ and $M$ be a $k$ -dimensional embedded $C^1$ -submanifold of $\mathbb R^d$ with boundary. What is the relation between $^1$ $T_x\:M$ and $T_x\:\partial M$ for $x\in\partial M$ ? I've tried to consider the case $k=d$ and $M=\mathbb H^k$ . Let $x\in\mathbb H^k$ . Then we should have $T_x\:\mathbb H^k=\mathbb R^k$ : Let $v\in\mathbb R^k\setminus\{0\}$ and $$\gamma(t):=x+tv\;\;\;\text{for }t\in\mathbb R.$$ Then, $$\forall t\in\mathbb R:\gamma(t)\in\mathbb H^k\Leftrightarrow x_k+tv_k\ge0.\tag1$$ We need to show that there is a nontrivial interval $I\subseteq\mathbb R$ with $0\in I$ such that $\left.\gamma\right|_I$ is a $C^1$ -curve on $M$ through $x$ . If $x\in(\mathbb H^k)^\circ$ , then $$B_\varepsilon(x)\subseteq\mathbb H^k\tag2$$ for some $\varepsilon>0$ and hence we may choose $$I:=\left(-\frac\varepsilon{\left\|v\right\|},\frac\varepsilon{\left\|v\right\|}\right).$$ Otherwise, $x\in\partial\mathbb H^k$ and hence $$\forall t\in\mathbb R:\gamma(t)\in\mathbb H^k\Leftrightarrow tv_k\ge0\tag3$$ and we may choose $$I:=\begin{cases}[0,\varepsilon)&\text{, if }v_k\ge0\\(-\varepsilon,0]&\text{, if }v_k\le0\end{cases}$$ for some arbitrary $\varepsilon\in(0,\infty]$ . Turning to $T_x\:\partial M$ for some $x\in\partial M$ : I guess the crucial difference between my proof of $T_x\:\mathbb H^k=\mathbb R^k$ above is that we now need to choose a curve which remains in $\partial\mathbb H^k$ . So, if $\gamma$ is any $C^1$ -curve on $\partial\mathbb H^k$ , then I guess it must hold $\gamma'(0)_k=0$ , since otherwise $\gamma(t)\not\in\partial\mathbb H^k$ for $|t|$ sufficiently small. So, we should have $T_x\:\partial\mathbb H^k=\partial\mathbb H^k$ or am I missing something? EDIT What I wrote above does hold more generally: If $(\Omega,\phi)$ is a $k$ -dimensional $C^1$ -chart of $M$ , $x\in\Omega$ , $u:=\phi(x)$ and $h\in\mathbb R^k$ , then the same proof as above yields that $$v:={\rm D}\phi^{-1}(u)h\in T_x\:\Omega\tag4.$$ We just need to replace $\gamma$ by $\gamma(t):=\phi^{-1}(u+th)$ for $t\in I$ (and $\varepsilon$ needs to be chosen sufficiently small). This shows that $${\rm D}\phi^{-1}(\phi(x))\mathbb R^k\subseteq T_x\:\Omega\subseteq T_x\:M\tag4\;\;\;\text{for all }x\in\Omega,$$ but are both subset relations in $(4)$ actually equalities? $^1$ Say that $(I,\gamma)$ is a $C^1$ -curve on $M$ through $x\in M$ if $I\subseteq\mathbb R$ is a nontrivial interval with $0\in I$ and $\gamma:I\to M$ is $C^1$ -differentiable with $\gamma(0)=x$ . Let $$T_x\:M:=\{\gamma'(0):\gamma\text{ is a }C^1\text{-curve on }M\text{ through }x\}$$ denote the tangent space of $M$ at $x\in M$ .","['tangent-spaces', 'manifolds-with-boundary', 'smooth-manifolds', 'differential-geometry']"
3810231,Constructing a random variable $Y$ whose values are $\pm 1$ with conditional expectation $\mathbb E[Y|X] = X$,"Suppose $X$ is a random variable on a probability space $(\Omega, \mathcal A, \mathbb P)$ with $|X| \leq 1$ almost surely. I want to show there is a random variable $Y$ with values in $\{-1,1\}$ and for which $\mathbb E[Y|X] = X$ . Since $Y$ takes values in $\{-1,1\}$ , we know $Y = \mathbb 1_A - \mathbb 1_{A^c}$ for some event $A \in \mathcal A$ . Furthermore, the equation $\mathbb E[Y|X] = X$ is equivalent to saying that for all $a \in [-1,1]$ , $$
\mathbb E\left[X\mathbb 1_{\{X \leq a\}}\right] = \mathbb E\left[Y\mathbb 1_{\{X \leq a\}}\right].
$$ The right hand side of this equation is $$\mathbb E\left[Y\mathbb 1_{X \leq a}\right] = \mathbb P\left[A \cap\{X \leq a\}\right] - \mathbb P\left[A^c \cap \{X \leq a\}\right] = 2\mathbb P\left[A \cap \{X \leq a\}\right] - \mathbb P\left[X \leq a\right]$$ In other words, I want to find an $A \in \mathcal A$ for which $$
\mathbb P\left[A \cap \{X \leq a\}\right] = \frac 1 2 \left( \mathbb E\left[X\mathbb 1_{\{X \leq a\}}\right] + \mathbb P\left[X \leq a\right]\right) = \frac 1 2 \mathbb E \left[(X+1)\mathbb 1_{\{X \leq a\}}\right]
$$ But I'm not sure how to construct such an $A$ , or show that such an $A$ exists. I tried using as a toy example $(\Omega, \mathcal A, \mathbb P) = \left([-1,1], \mathcal B([-1,1]), \frac 1 2 \lambda\right)$ (where $\lambda$ is the Lebesgue measure), and $X(\omega) = \omega$ , and found that the $A \subset [-1,1]$ that I needed would require (using the above equation) $$
\frac 1 2 \lambda(A \cap [-1,a]) = \frac 1 4 \int_{-1}^a (x+1)\,dx = \frac 1 4 \left(\frac 1 2 x^2 + x \right)\bigg|_{x=-1}^{x=a} = \frac 1 8 \left(a^2 + 2a + 1\right) = \frac 1 8 (a+1)^2
$$ or in other words, $\lambda(A \cap [-1,a)) = \frac 1 4 (a+1)^2$ . But I'm not sure how to construct such an $A \subset [-1,1]$ . Is there a better way for me to be thinking about this?","['measure-theory', 'lebesgue-measure', 'conditional-probability', 'conditional-expectation', 'probability-theory']"
3810300,What category is the universal property of the Free Group a diagram in?,"wikipedia says that the free group is defined by a universal property: The free group $F_S$ is the universal group generated by the set $S$ . This can be formalized by the following universal property: given any function $f$ from $S$ to a group $G$ , there exists a unique homomorphism $φ: F_S → G$ making the following diagram commute (where the unnamed mapping denotes the inclusion from $S$ into $F_S$ ): My question is, in what category is this a diagram? Is it in Grp or Set ? Either way I'm confused, because $S$ is not a group, suggesting it's in Set , but the uniqueness of $\phi$ only holds for homomorphisms, not general functions, suggesting this is in Grp .","['universal-property', 'group-theory', 'category-theory']"
3810301,"Let $W^1, W^2, \ldots$ be i.i.d. Brownian motions, $T_1, T_2, \ldots$ be random variables converging a.s. to $t > 0$, do we have CLT for $W^n_{T_n}$?","Let $W^1, W^2, \ldots$ be i.i.d. Brownian motions, $T_1, T_2, \ldots$ be non-negative random variables converging a.s. to $t > 0$ . Does CLT $$ \frac{1}{\sqrt {nt}} (W^1_{T_1} + \cdots + W^n_{T_n}) \xrightarrow[n\to\infty]{\mathcal D} \mathcal N_{0,1} $$ hold? In this case $\left\{W^n_{T_n}\right\}$ is not an independent sequence, but, I speculate, asymptotically i.i.d. in some sense. Do we have existing results for this type of sequence?","['probability-limit-theorems', 'central-limit-theorem', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
3810391,"""Tricky"" questions on graph theory","So , I am revising graph theory. I need to gain some help/ feedback for those, because at least to me they are tricky. They are supposed to be answered quickly , because they come from a tight timed -  exam.( so I guess they come with either some strong theory requirements or smart ""insight"") - There is a graph with 1871 vertices that is Eulerian and bipartite.True or false? To be bipartite, there should be no cycle of odd length. If there is an Euler circle, it can be partioned to a set of disjoint circles ( and we need all of those to be of even length) . So , if we add up everything we should have an even number of edges so $\exists k$ such that $|Ε|=2k \rightarrow \frac{\sum_{i}^n deg(v_i)}{2}=2k \rightarrow \sum_{i}^n deg(v_i)=4k$ . But I don't use anywhere the number of vertices . I end up nowhere.. What's the max value of edges for a simple (no - parallel edges), undirected  graph with $n \geq 10$ that it is Eulerian and has at least two different Hamilton circles? Inclass ,we proved that if n is even in order to be Eulerian it has at most $n(n-2)$ edges and if n is odd it has at most $\frac{n(n-1)}{2}$ .We basically know , that there is no bridge , eith cut vertex here . Hence, every edge belongs to a circle. It is Eulerian , hence every vertex has an even number degree. Adding edges , to a graph with Hamilton circles could do no harm into it's Hamiltonian property so it only impacts the Eulerian property. So we need to examine  , how a graph with two Hamilton circles can be Eulerian. A Hamilton circle , does not repeat vertices so neither edges. If these 2 Hamilton circles are completely disjoint (each one is constructed by n edges) then the Euler one has at least 2n edges Note: There was an extra question in the second one : Find the minimum number of edges for a simple (no - parallel edges), undirected  graph with $n \geq 10$ that it is Eulerian and has at least two different Hamilton circles?","['graph-theory', 'hamiltonicity', 'discrete-mathematics']"
3810458,"Given a function $f : A \rightarrow B$ and subsets $W, X \subseteq A$ , prove $f(W \cup X) = f(W) \cup f(X)$ .","Given a function $f : A \rightarrow B$ and subsets $W, X \subseteq A$ , prove $f(W \cup X) = f(W) \cup f(X)$ . My approach : $f(W \cup X) = \{f(a) : a \in W \cup X\} = \{f(a) : (a \in W) \lor (a \in X)\} = \{f(a) : a \in W\} \cup \{f(a) : a \in X\} = f(W) \cup f(X)$ . My question : Is this correct? What confuses me is that I can do the same proof for $f(W \cap X) = f(W) \cap f(X)$ , which is of course wrong in general. See: $f(W \cap X) = \{f(a) : a \in W \cap X\} = \{f(a) : (a \in W) \land (a \in X)\} = \{f(a) : a \in W\} \cap \{f(a) : a \in X\} = f(W) \cap f(X)$ . Thanks in advance!","['elementary-set-theory', 'self-learning', 'functions', 'solution-verification']"
3810472,Find all $x\in\mathbb{R}$ such that $\left( \sqrt{2-\sqrt{2} }\right)^x+\left( \sqrt{2+\sqrt{2} }\right)^x=2^x$.,"Find all $x\in\mathbb{R}$ such that: $$
\left( \sqrt{2-\sqrt{2} }\right)^x+\left( \sqrt{2+\sqrt{2} }\right)^x=2^x\,.
$$ Immediately we notice that $x=2$ satisfies the equation. Then we see that $LHS=a^x+b^x$ , where $a<1$ and $b<2$ , therefore $RHS$ grows faster (for larger $x$ , $LHS\approx b^x<2^x$ ) Hence $x=2$ is the only real solution. Unfortunately I don't know whether this line of reasoning is correct. Moreover, if it is indeed correct, how to write this formally?","['exponentiation', 'algebra-precalculus', 'exponential-function', 'monotone-functions']"
3810503,Generated Sigma Algebras,"I am not a mathematician, rather I pick up on topics on the go, when I need something for the topic I am studying in the given time. So I am sorry if this is trivial to most of you and apologies for any conceptual mistakes I might make in the description - I'll try to be as precise as possible. At the moment, I am studying Probability Theory, from this course: https://www.youtube.com/playlist?list=PL5B3KLQNAC5jT6yjV1199ji1zUy1YUp6P , [for the purposes of understanding Stochastic Calculus for Finance (Vol. II - S. Shreve)], and I stumbled upon sigma algebras. While I do understand the concept; if we have a set which is a collection of subsets of Omega (i.e. if we have a collection of events) denoted by F, then F is a sigma-algebra if it satisfies the following three conditions; Omega belongs in F, F is closed under complements, F is closed under countable Unions So far so good and I also understand the properties that derive from the definition as well as how they are derived. In addition I know that we have the trivial sigma algebra, the smallest sigma algebra on Omega and the Discrete Sigma Algebra, which is the power set of Omega, being the largest sigma algebra on Omega. My problem is with generated sigma algebras. I do understand the definition; Let A be an arbitrary collection of subsets of Omega, then sigma(A) is the generated sigma algebra, generated from A and is the smallest sigma algebra containing A. Further, we can find the smallest sigma algebra by intersecting all sigma algebras containing A, as the intersection of sigma algebras is also a sigma algebra. The last part is the one I don't understand and confuses me. I get that we have the power set of Omega that definitely contains the collection A - But what exactly do we mean by intersecting all the sigma algebras containing A to find the smallest one containing A? Does it mean that if we have a sigma algebra containing the collection A and another collection of subsets, B (which is a sigma algebra containing A, but i get that it is not the smallest) and intersect it with the power set of Omega, we generate sigma(A), which is indeed the smallest and more refined to answer the questions that we need in our problem? But, where exactly does the bigger sigma algebra (on the collections A and B) come from? If anyone could provide a more intuitive explanation or even better give an example (finite, like a die roll), I would be very grateful. Many thanks for your time in reading this! :)","['measure-theory', 'finance', 'probability-theory', 'stochastic-calculus']"
3810642,How can we relate the two informal definitions of the limit with each other?,"Informal definition $1$ : This is the way in which I studied it, and it makes sense to me . As $x$ gets closer and closer to $a$ , $f(x)$ gets closer and closer to $l$ . Informal definition $2$ : $f(x)$ can get arbitrarily close to $l$ , by taking $x$ sufficiently close to $a$ . I am not able to relate these two informal definitions. How do we relate these definitions?","['limits', 'epsilon-delta']"
3810676,Guillemin&Pollack - Showing that the normal bundle is a manifold,"This is a question about the proposition in p.71 of Guillemin&Pollack - Differential Topology. Proposition. If $Y\subset \Bbb R^M$ then $N(Y)$ is a manifold of dimension $M$ and the projection $\sigma:N(Y)\to Y$ is a submersion. Here $Y$ is an embedded submanifold in $\Bbb R^M$ and $N(Y)\subset Y\times \Bbb R^M$ is the total space of the normal bundle. Proof. (given in the book) Define $Y$ locally by equations: around any given point of $Y$ , find an open set $\tilde{U}$ of $\Bbb R^M$ and a submersion $\phi:\tilde{U}\to \Bbb R^k$ ( $k=\text{codim}Y$ ) such that $U=Y\cap \tilde{U}=\phi^{-1}(0)$ . The set $N(U)$ equals $N(Y)\cap (U\times \Bbb R^M)$ , thus is open in $N(Y)$ . For each $y\in U$ , $d\phi_y:\Bbb R^M\to \Bbb R^k$ is surjective and has kernel $T_yY$ . Therefore its transpose maps $\Bbb R^k$ isomorphically onto $N_yY$ . The map $\psi:U\times \Bbb R^k\to N(U)$ , defined by $\psi(y,v)=(y,(d\phi_y)^tv)$ , is thus bijective, and it is easy to check that it is an embedding of $U\times \Bbb R^k$ into $Y\times \Bbb R^m$ . ~~ How can we show that $\psi$ is an embedding? In this book an embedding means a map which is proper injective immersion. But it is quite clear that it suffices to show that the map $\psi$ is a diffeomorphism onto its image, which is a weaker condition than being an embedding. Also, it is clear from the text that $\psi$ is a smooth bijection (onto $N(U)$ ). However it doesn't seem easy to me to show that its inverse is also smooth. Thanks in advance.","['proof-explanation', 'smooth-manifolds', 'vector-bundles', 'differential-topology', 'differential-geometry']"
3810688,Asymptotic behavior of an expectation including a bounded process,"Let $X_n$ be some stochastic process and $f$ and $g$ some nonnegative functions. Suppose $g(X_n) \in (a,b)$ , $0 < a < b < \infty$ , for all $n$ . I am now asking myself whether there exists a constant $0 < c < \infty$ such that $$ \lim_{n \to \infty} \frac{
E \left( f(X_n) g(X_n) \right) } {c E (f(X_n))} = 1.$$ My thoughts on this were that \begin{align} \limsup_{n \to \infty} \frac{
E \left( f(X_n) g(X_n) \right) } {c E (f(X_n))} \leq \frac{b}{c}, \quad \text{ and } \quad \liminf_{n \to \infty} \frac{
E \left( f(X_n) g(X_n) \right) } {c E (f(X_n))} \geq \frac{a}{c}. \end{align} This seems to be not enough. Is my first claim true? And if not, what would we need to assume for it to hold? Edit: Let us add the additional assumption that $E (f(X_n)) \to 0$ as $n \to \infty$ , maybe that helps.","['stochastic-processes', 'probability-theory', 'asymptotics', 'real-analysis']"
3810718,"Level set corresponding to the value $0$ for $f(x,y,z)=y^3-z^3$ is a smooth surface?","I was attempting exercises in Vector Calculus by Peter Baxandall. Definition $3.8.11$ : A set $S \subseteq \Bbb R^m$ is called a smooth surface if $S$ is a level set of a $C^1$ function $f: D \subseteq \Bbb R^m \rightarrow \Bbb R~$ s.t $~\text {grad}~ f(x) \ne 0~\forall~x \in S$ My Argument: We can see that $S$ is the level set corresponding to the value $0$ of the $C^1$ function $f:D \subseteq \Bbb R^m \rightarrow \Bbb R~|~f(x,y,z)=y^3-z^3.~~~\therefore~\text{grad}f(x,y,z) = \Big(0,3y^2,-3z^2\Big)$ . Clearly, $\text{grad}f(0,0,0)=0$ and $(0,0,0) \in S$ . Thus, $S$ cannot be a smooth surface. Why does $3.8.11$ not apply to the above function? Could someone point out any error in the above argument? Thanks a lot!","['multivariable-calculus', 'vector-analysis']"
3810760,Unitary matrix commute with function,"I'm wondering in which cases the following identity is satisfied : $$
f\left(UXU^T\right) = Uf\left(X\right)U^T
$$ where $X \in \mathbb{R}^{n\times n}$ is a square matrix, $U$ is any permutation matrix and $ f:\mathbb{R}^{n\times n} \rightarrow \mathbb{R}^{n\times n}$ I already know of two cases : $f$ can be expressed as a matrix Taylor series (in this case $U$ could be any unitary matrix) $f$ is an element-wise function Are these the general cases? Bonus : Is there an extension of the preceding identity to tensors $T \in \mathbb{R}^{n^m}$ and $f:\mathbb{R}^{n^m} \rightarrow \mathbb{R}^{n^m}$ . I am not sure what form the product and the operator $U$ would take in that case.","['operator-theory', 'tensors', 'matrices', 'linear-algebra', 'linear-transformations']"
3810762,The volume of $k$-parametrised manifolds when $k=1$ or $k=2$ or $k=3$ agrees respectively with the usual intuitive idea of length or area or volume.,"James Munkres, at the chapter $22$ -th of the text Analysis on Manifolds , gives the following definition. Furthermore, to point out I say that the function $V\left(D\alpha\right)$ corresponds with volume of $k$ -parallelopiped generated by the tangent vectors to the graphic of $\alpha$ , that is it is the volume of $k$ -parallelopided generated by the column vectors $\frac{\partial\alpha}{\partial x_1},\ldots,\frac{\partial\alpha}{\partial x_k}$ of $D\alpha$ . So, to justify the above definition, Munkres, as exercise, asks to show what follows. If you like you can read the proof of the last theorem at the page $9$ of this document. Well since own Munkres states that the theorem holds with more general hypotheses I thought to generalise it using the following considerations. So in the case where $\alpha$ is function from an open set of $\Bbb R^k$ in $\Bbb R^n$ then triangle $A$ is a general simplex and so the trasformation $h$ carries the simplex $A$ with origin the intersection of the axes and with edge the canonical versors onto a simplex $S$ with origin at $a$ and with edge the column vectors of $C$ so that considering that $v(A)=\frac 1{n!}$ and $v(S)=\frac 1{n!}|\det C|$ it seems to me the theorem could be generalised finding $n!$ disjoint and open simplexes whose union of the closures is the unit cube $[0,1]^n$ . So could someone help me, please?","['measure-theory', 'multivariable-calculus', 'solution-verification', 'manifolds', 'differential-geometry']"
3810848,Showing that $\nabla(\frac{1}{f})=-\frac{\nabla f}{f^2}$,"If $f$ and $g$ are real-valued differentiable functions in an open set $E \subset \mathbb{R}^n$ , show that $$\nabla\left(\frac{1}{f}\right)=-\frac{\nabla f}{f^2}$$ So if I have $$
\nabla(\frac1f)
 = \left( \frac{\partial}{̛\partial x}\frac{1}{f},
          \frac{\partial}{̛\partial y}\frac{1}{f},
          \frac{\partial}{̛\partial z}\frac{1}{f} \right)
$$ wouldn't this equal $$
\left(-\frac{\partial}{̛\partial x}\frac{1}{f^2},
      -\frac{\partial}{̛\partial y}\frac{1}{f^2},
      -\frac{\partial}{̛\partial z}\frac{1}{f^2}\right)
 = -\frac{\nabla f}{f^2}?
$$ I'm taking the factor $-\frac{1}{f^2}$ just out which leaves me with $\nabla f.$ I'm a bit confused about the term $\left(-\frac{\partial}{̛\partial x}\frac{1}{f^2}, -\frac{\partial}{̛\partial y}\frac{1}{f^2}, -\frac{\partial}{̛\partial z}\frac{1}{f^2}\right)$ I'm thinking this the way that after differentiating with respect to $x$ I get $-\frac{\partial}{̛\partial x}\frac{1}{f^2}$ from $\frac{\partial}{̛\partial x}\frac{1}{f}$ and $f^2$ would still contain the other variables in this case up to $z$ , but it could be probably any number one desires.","['partial-derivative', 'multivariable-calculus', 'vector-analysis']"
3810884,Let $n \geqslant 3$ and a polynomial $P(x) \in \mathbb{R}^*[x]$ such that $P(x) = x^n + ax + 1 \quad \text{for some } a \in \mathbb{R}^{*}$,"Notation: $\mathbb{R}^*:= $ the set of all non-zero reals. $\blacksquare~$ Problem: Let $n \geqslant 3$ be an integer and let $a$ $\in$ $\mathbb{R}^{*}$ . Consider the polynomial $P(x)$ $\in$ $\mathbb{R}^{*}[x]$ $\subseteq$ $\mathbb{C}[x]$ defined as \begin{align*}
        P(x) ~=~ x^n + ax + 1 \quad \text{for some } a \in \mathbb{R}^{*} 
    \end{align*} Let's consider the set $\Omega$ of all possible non-real roots of $P(x)$ , i.e., \begin{align*}
        \Omega := \{  z \in \mathbb{C} \backslash \mathbb{R} ~:~ P(z) = 0\}
    \end{align*} Prove that if $z$ $\in$ $\Omega$ then $z$ satisfies the inequality \begin{align*}
        \lvert z \rvert ~\geqslant~ \sqrt[n]{\frac{1}{n - 1}}
    \end{align*} $\blacksquare~$ My Approach: Let $z = r (\cos \theta + i \sin \theta) \in \Omega~$ where $\theta \in (0, 2\pi)$ . Then we get that $$ P(z) = r^n (\cos n \theta + i \sin n \theta) + ar (\cos \theta + i \sin \theta) + 1 = 0 $$ Now we have that both the Im $(P(z)) = 0$ and Re $(P(z)) = 0$ . Thus $$ r^n \cos n \theta + ar \cos \theta + 1 = 0 \quad \text{and} \quad r^n \sin n \theta + ar \sin \theta = 0  $$ Thus on multiplying $\sin \theta$ to the Re $(z)$ part and $\cos \theta$ to the Im $(z)$ part and then subtracting we have that $$ r^n | \sin (n - 1) \theta | = | \sin \theta |  $$ Then we have a claim. $\bullet~$ Claim: the following inequality is valid for any $m \in N$ . $$ |\sin m \theta| \leqslant m |\sin \theta | $$ $\bullet~$ Proof: It's easy by induction on $m$ . I'll just brief. Let the statement be denoted by $S(m)$ So, the base step is $m = 1$ . And $P(m)$ is true. Let's assume that for some $k = m$ , the statement $P(m)$ is true. i.e., $$ |\sin k \theta| \leqslant k |\sin \theta| $$ Then from the following (by using) $\color{red}{|\sin \theta| \leqslant 1 \text{ and } |\cos \theta| \leqslant 1}$ ,
we have that $$ |\sin (k + 1)\theta| = |\sin k \theta \cos \theta + \cos k \theta \sin \theta | \leqslant \color{blue}{|\sin k \theta \cos \theta | + |\cos k \theta \sin \theta| } $$ $$ \leqslant |\sin k \theta | + |\sin \theta|$$ $$ \leqslant k|\sin \theta| + |\sin \theta| = (k + 1)|\sin \theta | $$ Thus $P(m)$ is true for $k + 1$ too. Hence the claim is proved. Thus from our claim, we have that $$ r^n |\sin (n -1 )\theta| = |\sin \theta| \leqslant r^n (n - 1)| \sin \theta | $$ Thus $$ r^n \geqslant \frac{1}{(n - 1)} \implies r \geqslant \sqrt[n]{\frac{1}{n - 1}} $$ Hence we have obtained that $$ |z| \geqslant \sqrt[n]{\frac{1}{n - 1}}  $$ Completing the solution. Is there any kind of glitch? Another solution will be appreciated. Regards, Ralph.","['inequality', 'solution-verification', 'polynomials', 'algebra-precalculus', 'complex-numbers']"
3810906,Integer coordinates in $\mathbb{R}^n$,"Find the minimum amount $m$ of integer coordinates $\left(\text{in } \mathbb{R}^n\right)$ one needs to choose in order to be sure that there exist a line segment, such that its endpoints are one of the previously chosen points and its midpoint's coordinates are integer. My solution: The midpoint coordinates of line segment created by points $P = (p_1,p_2,...,p_n)$ and $Q = (q_1,q_2,...,q_n)$ is as follows: $$
R = \left(\frac{p_1+q_1}{2},\frac{p_2+q_2}{2},...,\frac{p_n+q_n}{2}\right)
$$ In order that $R$ 's coordinates are integer, the elements of pairs $(p_1,q_1),(p_2,q_2),...,(p_n,q_n)$ must be either be both odd or both even. Let's consider a binary sequence: $$
a(P)=(p_1\ mod\ 2,\ p_2\ mod\ 2,...,p_n\ mod\ 2)
$$ Points $P,\ Q$ meet the requirements iff $a(P)=a(Q)$ . The amount of such unique sequences is obviously equal to $2^n$ . Therefore in a set of $2^n+1$ points, there exist at least two points with the exact same sequence. Hence $m=2^n+1$ Please verify this proof, and in case I made some mistake, plase give me some tips on how to correct it.","['solution-verification', 'geometry']"
3810909,Computing a finite sum involving binomial coefficients,"I would like to prove the following: given the sequence $$a_m = \begin{cases}1 & \text{if $m = 0, 1$} \\\frac{(\alpha+2)(\alpha+4)\cdots(\alpha+2(m-1))}{(\alpha+1)(\alpha+2)\cdots(\alpha+m-1)} & \text{otherwise}\end{cases}$$ then, $$\sum_{m=0}^{n}(-1)^m\binom{n}{m}a_m = \begin{cases}\frac{(n-1)!!}{(\alpha+1)(\alpha+3)\cdots(\alpha+n-1)}& \text{if $n$ is even and $\ge 2$} \\0 & \text{if $n$ is odd}\end{cases}$$ I understand that the left hand side in the last equation is the $n$ -th difference $\Delta^na_0$ , so basically I tried to compute each difference but it seems that a general formula is hard to find for $\Delta^na_k$ . Can you help me? I have seen often that such problems can be attacked with special functions but unfortunately I don't have any experience :(. What do you suggest? Any hint would be greatly appreciated :) EDIT: So here is the original problem: show that $$e^{-x}\left[1+x+\frac{\alpha+2}{\alpha+1}\cdot\frac{x^2}{2!}+\frac{(\alpha+2)(\alpha+4)}{(\alpha+1)(\alpha+2)}\cdot\frac{x^3}{3!}+\ldots\right]=$$ $$1+\frac1{\alpha+1}\frac{x^2}{2^1\cdot1!}+\frac1{(\alpha+1)(\alpha+3)}\frac{x^4}{2^2\cdot2!}+\ldots$$ . Doing the product one just compares the two series and obtains my equation above (if I interpreted correctly the pattern of the two series).","['summation', 'binomial-coefficients', 'sequences-and-series']"
3810931,Prove that a function $f: A \rightarrow B$ is surjective if $f(f^{-1}(Y)) = Y$ for all $Y \subseteq B$.,"Prove that a function $f: A \rightarrow B$ is surjective if $f(f^{-1}(Y)) = Y$ for all $Y \subseteq B$ . My approach : Suppose $f(f^{-1}(Y)) = Y$ for all $Y \subseteq B$ . Now suppose $b \in B$ . We want to show that there exists an element $a \in A$ for which $f(a) = b$ . Consider the set $\{b\}$ . Since $\{b\} \subseteq B$ , it must be that $f(f^{-1}(\{b\})) = \{b\}$ . Now, for the sake of contradiction, suppose that $f^{-1}(\{b\}) = \emptyset$ . Then $f(f^{-1}(\{b\})) = \{f(x) : x \in f^{-1}(\{b\})\} = \{f(x) : x \in \emptyset\} = \emptyset$ , a contradiction because $f(f^{-1}(\{b\})) = \{b\}$ as stated above. Hence $f^{-1}(\{b\}) \neq \emptyset$ . Then there must exist an element $a \in f^{-1}(\{b\}) = \{x \in A : f(x) \in \{b\}\}$ . Therefore $f(a) = b$ , so $f$ is surjective. Is this correct? Note :
This question was already asked in this thread , however, there were only answers given for the direction "" $f$ is surjective $\Rightarrow$ $f(f^{-1}(Y)) = Y$ "" and not the other way around. Thanks in advance!","['elementary-set-theory', 'functions', 'solution-verification', 'inverse-function']"
3810944,Need explanation on a problem involving natural function,"Consider a positive integer $n$ and the function $f:\mathbb{N}\to \mathbb{N}$ ( $\mathbb N$ includes $0$ ) by $$f(x) = \begin{cases} 		\frac{x}{2}
 & \text{if } x \text{ is even} \\ 		\frac{x-1}{2} + 2^{n-1} & \text{if } x \text{ is odd} \end{cases} $$ Determine the set $$  	A = \{ x\in \mathbb{N} \mid \underbrace{\left( f\circ f\circ ....\circ f \right)}_{n\ f\text{'s}}\left( x \right)=x \}.  	$$ (Romania NMO 2013) The solution starts by stating that $f(x)<x, \quad\forall x\ge 2^n-1$ . This was easy enough to understand. However, they continue by saying this implies that $A\subset\{0,1,\dots,2^n-1\}$ . Why is that? Please help me understand! Thanks in advance!",['functions']
3810986,Best subset with exactly one success [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question This is an interview question that I was asked, but I totally couldn't figure it out: Given N items = {a,b,c,d,e...}, each with a probability { $P_a$ , $P_b$ , $P_c$ , $P_d$ , $P_e$ ...} of succeeding. Given that you can select any subset of items (i.e {b,c,e}), what is the highest probability you could get such that exactly ONE of the item in this subset succeed. Say you select {b,c,e}
the probability is $P_b$ (1- $P_c$ ) (1- $P_e$ ) + Pc*(1- $P_b$ ) (1- $P_e$ ) + $P_e$ (1- $P_b$ )*(1- $P_c$ ) If there exist any item x with probability 1, it is best to select only that item. How can we find out the maximum possible probability given that you can choose any subset?","['discrete-optimization', 'optimization', 'probability-theory', 'probability']"
3811030,Probability that 3 darts land in a same half of a dart board,3 darts are thrown (equal probability of landing anywhere on the dart board). What's the probability that they all land on a same half of the dart board? Edit: I know the first 2 darts can land anywhere but don't know how to find the probability that the 3rd lands in an acceptable region.,['probability']
3811094,"Prove that the cone $S = \{(x,y,z) \in \Bbb R^3 ~|~x^2+y^2-z^2=0 \}$ is not a smooth surface","Prove that the cone $S = \{(x,y,z) \in \Bbb R^3 ~|~x^2+y^2-z^2=0 \}$ is not a smooth surface. The book I am reading - Vector calculus by Peter Baxandall gives the following hint : (Let $S \subseteq \Bbb R^m$ be a smooth surface. Then there exists an open subset $E \in \Bbb R^m$ such that $f$ is smooth on $E$ and $S$ is a level set of the function $f:E \subseteq \Bbb R^m \rightarrow \Bbb R$ ) Attempt: Suppose that $S$ is a smooth surface. Then, there exists a smooth function $f: E \subseteq \Bbb R^3 \rightarrow \Bbb R$ such that $$S=\{(x,y,z) \in E~|~f(x,y,z)=0 \}$$ Clearly $(0,0,0) \in S$ as $f(0,0,0)=0$ Consider the three differentiable paths $\alpha.\beta,\gamma$ in $S$ given by : $$\alpha(t)=\big(t,0,t\big),~~ \beta(t)=\big(0,t,t\big),~~\gamma(t)=\big(t,t,\sqrt 2 t\big), t\in [-1,1]$$ We  can see that $\forall t \in \Bbb R : \alpha(t),\beta(t),\gamma(t) \subseteq S$ because $ f( \alpha(t))= f(\beta(t))= f(\gamma(t))=0$ Then $\alpha'(t)=\big(1,0,1\big),~~ \beta'(t)=\big(0,1,1\big),~~\gamma'(t)=\big(1,1,\sqrt 2 \big)$ . We can see that these vectors are linearly independent. This is the direction specified in the book. How do I use this info to obtain a contradiction ? Intuitively, we should now obtain a contradiction by proving that $f$ is not a smooth function. Any hints on obtaining a contradiction will be deeply appreciated EDIT: Problem Statement $Q.11$ in the textbook with the hint :","['non-smooth-analysis', 'smooth-functions', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
3811119,Question about supersingular elliptic curves,"It is a fact that given $D>0$ there exist only finitely many isomorphism classes of elliptic curves over $\overline{\mathbb{Q}}$ with complex multiplication by $O_D=\mathbb{Z}[\frac{1}{2}(D+\sqrt{-D})]$ (where $D\equiv 0,3\operatorname{mod} 4$ ), whose $j$ -invariants are all conjugate algebraic integers. Let $P_D(x)$ be the monic polynomial whose roots are the $j$ -invariants. We can further consider $P_D(x)$ in characteristic $p$ . In Elkies' paper on the existence of infinitely many supersingular primes for rational every elliptic curve, he states ""Since by Deuring's Lifting Lemma, complex multiplication in characteristic $p$ can be lifted to characteristic $0$ , the roots of $P_D(x)$ are $j$ -invariants of curves with an endomorphism $\frac{1}{2}(D+\sqrt{-D})$ , that is, with complex multiplication by $O_{D'}$ for some factor $D'$ of $D$ such that $D/D'$ is a perfect square."" Deuring's lifting lemma states: If $E_0$ is an elliptic curve over $\mathbb{F}_p$ and $\alpha_0$ is a non-trivial endomorphism of $E_0$ , then there exists an elliptic curve $E/\mathcal{O}_k$ for $K$ a number field, and an endomorphism $\alpha$ of $E$ and a prime $\mathfrak{p}$ of $K$ lying above $p$ with residue field $k$ such that $E_k\cong_{\overline{\mathbb{F}_p}}E_0$ and $\alpha_{\overline{\mathbb{F}_p}}=\alpha_0$ . My question is: how does the lifting lemma imply ""the roots of $P_D(x)$ are $j$ -invariants of curves with an endomorphism $\frac{1}{2}(D+\sqrt{-D})$ , that is, with complex multiplication by $O_{D'}$ for some factor $D'$ of $D$ such that $D/D'$ is a perfect square""? How can I see this?","['number-theory', 'abstract-algebra', 'algebraic-geometry', 'elliptic-curves']"
3811146,Proving Jensen's Inequality for Arbitrary Intervals,"In his probability book Bauer proves the following version of Jensen's inequality: Proposition. Let $X$ be an integrable random variable taking values in an open interval $I\subset\mathbb{R}$ , and let $q$ be a convex function on $I$ . If $q\circ X$ is integrable, then $$q(E(X))\leq E(q\circ X).$$ Now am asked to prove that the result holds for an arbitrary interval, e.g. $I=[a,b]$ . As a hint Bauer suggests to show that $q$ is lower semicontinuous on $I$ , i.e. that $\{x\in I:q(x)>\alpha\}$ is relatively open in $I$ for every $\alpha \in \mathbb{R}$ . How can I do this? I tried an answer below. Any comment is greatly appreciated. EDIT: I believe Bauer means to show that $q$ is upper semicontinuous, not lower. Indeed by considering the indicator of $\{0,1\}$ on the interval $[0,1]$ it is clear that a convex function need not be lower semicontinuous. To show this, suppose $I=[a,b]$ . By convexity we know that $q$ is continuous on $(a,b)$ , and so in particular upper semicontinuous on $(a,b)$ . Also by convexity, we know that $q'_+(a)$ exists, but might be $-\infty$ (convexity implies that $q'_+(x)$ exists and is nondecreasing on $I$ , and is real-valued on $I^\mathrm{o}$ ). If $q'_+(a)\in\mathbb{R}$ , then $q$ is continuous at $a$ and so in particular upper semicontinuous at $a$ . If $q'_+(a)=-\infty$ , then we see that that $q$ must be decreasing in a neighborhood of $a$ , which implies upper semicontinuity of $q$ at $a$ . A similar argument using the left derivative $q'_-(x)$ applies for the endpoint $b$ .","['expected-value', 'jensen-inequality', 'probability-theory']"
3811152,prove or disprove: if $y'=y^2-\cos(x)$ then any solution diverges in a finite time,"given the following ode, prove or disprove: if $y'=y^2-\cos(x)$ then any solution diverges in a finite time. ""diverge"" means, that there is a point $x$ where the solution isn't continuous (there and thereafter). if $y(0)>1$ or $y(0)<-1$ it's clear how to show it.
for instance, take $y(0)>1$ , and let say we have $x$ where for the first time $y(x)=1$ .
then according to the mean value theorem there is $t$ where $y'(t)<0$ and then it applies that $y(t)<1$ , thus $x$ is not the first time we reached $y(x)=1$ , therefore there isn't such point. The problem is showing that if $-1<y(0)<1$ it must go out from this range in a finite time. Edit I thought about a solution $y'=y^2-\cos(x)>y^2-1$ , which its solution is $\frac{1-e^{2c+2x}}{e^{2c+2x}+1}$ , which might have a pole. depending on y(0)","['convergence-divergence', 'ordinary-differential-equations']"
3811169,Observability of a Matrix Pair,"Let $A\in\mathbb{R}^{m \times m}$ and $C\in\mathbb{R}^{n\times m}$ . The pair $(A,C)$ is observable if $Cx \ne 0$ for every right eigenvector $x$ of $A$ . Therefore, if the pair $(A,C)$ is NOT observable, then \begin{align}
	\exists \lambda \in \mathbb{C}, x \in \mathbb{C}^m, x\ne 0  \: \text{ such that } Ax = \lambda x \text{ and } Cx = 0. \tag{1}
\end{align} In the above case that ( $1$ ) holds, is there any concept or result that study the multiplicity of the eigenvalue $\lambda$ through which the observability is violated? For instance, what is the difference between the case that $\lambda$ in ( $1$ ) is a simple eigenvalue and the case where $\lambda$ is not simple? Is there a way to identify the multiplicity of $\lambda$ for example through observability matrix? Thank you for your comments and thoughts.","['ordinary-differential-equations', 'control-theory', 'linear-algebra', 'linear-control', 'dynamical-systems']"
3811337,"Minimizing floor space needed to store $N$ unit cubes, subject to two placement rules","There is a store room which has only three sides all touching each other perpendicularly, the sides can be defined as: two infinitely large walls and one infinitely large floor. There are $N$ cubes of unit volume. I need to store the cubes in the store room but there are two rules to be followed before storing: A cube can be placed anywhere on the floor. If a cube A is placed upon another cube B, then there should be a cube on all the four sides of the cube B unless the side is already blocked by a wall. I need to output the minimum floor area covered for storing N cubes . Example $N = 3$ requires 3 unit square of floor area as all the cubes need to be placed on the floor. $N = 4$ requires 3 unit square of floor area. Let there be cubes A, B, C, D; cube A can be placed above the cube B, which is touching both the walls. Since cube B is touching both the walls it is blocked on 2 sides but open on 2 other sides, cubes C and D can be placed on the adjacent open sides of Cube B. Hence, Cube A can be placed on the cube B following the given rules. I am trying to think it by intution but not coming up to a reasonable answer which could prove that my answer is correct.","['puzzle', 'geometry', 'linear-algebra', 'combinatorics', 'algorithms']"
3811355,Are all empty maps the same?,"In set theory, map $f:X\rightarrow Y$ is interpreted to be a subset of the product $X\times Y$ satisfying some properties. If $X=\varnothing$ then $f \subseteq \varnothing\times Y = \varnothing$ and all empty maps are the same regardless of whether they have different codomains $Y$ . However, it is said that it matters what the codomain of a map is. If $f:X\rightarrow Y$ and $f':X\rightarrow Y'$ and the two codomains are different, then $f\ne f'$ . So what gives? If $f:\varnothing\rightarrow Y$ and $f':\varnothing\rightarrow Y'$ are maps, are the two maps equal or not? Does the answer depend on the choice of foundations you use? Edit: I think at this point, I'm just looking for a citation that defines functions in terms of set theory keeping domains and codomains in mind.","['elementary-set-theory', 'foundations', 'functions']"
3811401,"There are $n$ sticks of length $\{1,2,3,\cdots,n\} $. how many incongruent triangles can be made using those sticks","My Approach: Let the number of incongruent triangles possible with the sticks of length $\{1,2,\cdots,n\}$ is $S_n $ So now number of incongruent triangles possible with the sticks of length $\{1,2,\cdots,n,n+1\}=$ Number of incongruent triangles with the sticks of length $\{1,2,\cdots,n\}+$ Number of incongruent triangles with largest side $n+1 $ $\implies S_{n+1}=S_n+$ Number of incongruent triangles with largest side $n+1 $ Now we will find the number of possible triangles with largest side $n+1$ Now the smallest side has to be greater than 1. The sum of two sides other than largest side can be minimum $n+2$ and maximum $2n-1$ \begin{array}{c|c}
\text{Smallest Side length}& \text{Number of choices to choose third side}\\ \hline
2 & 1\ [\text{Only } n]\\
3 & 2\ [\text{Only } n,n-1]\\
4 & 3\  [\text{Only } n,n-1,n-2]\\
\vdots & \vdots \quad \quad\quad\vdots \\
\Big\lfloor\dfrac{n}{2}\Big\rfloor & \Big\lfloor\dfrac{n}{2}\Big\rfloor-1\\ \hline
\text{Total} & \dfrac{\Big\lfloor\dfrac{n}{2}\Big\rfloor\bigg(\Big\lfloor\dfrac{n}{2}\Big\rfloor-1\bigg)}{2}
\end{array} Case 1 ( $n=2k$ ): Then $\Big\lfloor\dfrac{n}{2}\Big\rfloor=k$ Then after k we can choose any two lengths from the sticks $\{k+1,k+2,\cdots,n\}$ . So then number of choices when smallest side is $k+1$ is $\dfrac{k (k-1)}{2}$ Hence $S_{n+1}=S_n+\dfrac{k (k-1)}{2}+\dfrac{k (k-1)}{2}=S_n+k (k-1)= S_n+\dfrac{n (n-2)}{4}$ Case 2 ( $n=2k+1$ ): Then $\Big\lfloor\dfrac{n}{2}\Big\rfloor=k$ .  Here after k we can choose any two lengths from the sticks $\{k+1,k+2,\cdots,n\}$ . So now the number of choices when smallest side is $k+1$ is $\dfrac{k (k+1)}{2}$ Hence $S_{n+1}=S_n+\dfrac{k (k-1)}{2}+\dfrac{k (k+1)}{2}=S_n+k^2 = S_n+\dfrac{(n-1)^2}{4}$ Now i got these two recurrence relation but how to solve that to a general formula? Edit: I got the rest. We got that $$\text{Number of incongruent triangles with largest side }n+1=\begin{cases} \dfrac{n (n-2)}{4}, & \text{when }n=\text{even}\\ \dfrac{(n-1)^2}{4}, & \text{when }n=\text{odd}\end{cases}$$ we can write it in this way that $$\text{Number of incongruent triangles with largest side }n+1=\bigg\lceil \dfrac{n (n-2)}{4}\bigg\rceil$$ Now, \begin{array}{ccccc}
S_{n+1}& - & S_n & = & \bigg\lceil \dfrac{n (n-2)}{4}\bigg\rceil \\
S_{n}    & - & S_{n-1} & = & \bigg\lceil \dfrac{(n-1)(n-3)}{4}\bigg\rceil \\
S_{n-1} & - & S_{n-2} & = & \bigg\lceil \dfrac{(n-2)(n-4)}{4}\bigg\rceil \\
\vdots &    & \vdots &      & \vdots\\ 
S_4      & - & S_3 & =& \bigg\lceil \dfrac{3 (3-2)}{4}\bigg\rceil =1\\ \hline
S_{n+1}&- & S_3 & = & \sum\limits_{i=3}^n \bigg\lceil \dfrac{i (i-2)}{4}\bigg\rceil\\
S_{n+1}&  &         & = &\sum\limits_{i=1}^n \bigg\lceil \dfrac{i (i-2)}{4}\bigg\rceil 
\end{array}",['combinatorics']
3811429,How to prove there is no bijection between a finite set a proper subset,"I am reading Hrbacek-Jech's Introduction to Set Theory (3ed), and in chapter 4, section 2, which deals with finite sets, they define the following: (Remember: in set theory, $n$ is a natural number iff $n = \{0, 1, \dots, n-1 \}$ ) Definition: A set $X$ is finite if there is a bijection between $X$ and a natural number $n$ ; in such case we define the cardinal number of $X$ to be $\lvert X \rvert = n$ . Moreover, a set is said to be infinite if it is not finite. Then comes the first result of the section: Theorem: If $n$ is a natural number, then there is no bijection between $n$ and a proper subset $X \subset n$ . Immediately after they prove $\mathbb{N}$ is an infinte set using the stronger result: Theorem: If $X$ is a finite set, then there is no bijection between $X$ and a proper subset $Y \subset X$ . My problem is that this stronger result is never proved, and is only alluded to at the very end of the section (literally in the very last paragraph), as a mere consequence of the weaker result above. However I've tried to prove it using only what is given up to that point but have failed. Considering the cursory treatment they gave to it I imagine the proof is a one-liner, but so far I have come empty handed. So my question is: how do you prove this stronger result?","['elementary-set-theory', 'cardinals']"
3811431,Solve $y’ = 1 + y^2$,"Let: $$y’ = 1 + y^2 \\ y(0) = 0 $$ We need to find a maximal interval $(a,b)$ so that the problem with the initial conditions, has a solution. Therefore, I think I need to find a solution, there I can look at the validity interval. Yet, I don’t have even a direction how to solve this equation. It’s a non linear equation. Not separate, can’t use integration constant, not Bernoulli form, don’t see how substitution can help me... If I could guess I would say it something of the direction: $$\text{d}y = (1+y^2) \text{d}x $$ But $y$ Is a function of $x$ So I can't just take the integral of both sides... Can someone give me a hint? Thanks.","['calculus', 'ordinary-differential-equations']"
3811443,Möbius transformations of finite order,"I saw this question , which brought up the question: can we classify all the Möbius transformations (with complex coefficients) of finite order? In particular, do these only consist of the rotations? I sense symbolic manipulation isn't the way to go about this—and indeed didn't get anywhere with this—and would appreciate some insight.","['complex-analysis', 'mobius-transformation']"
3811487,How many irrationals are there which are unique upto addition with rationals?,"I'm studying real analysis and I know from previous courses that there are countably infinite rationals but uncountably infinite irrationals. However, I haven't done a formal proof about uncountability of irrationals. I've been thinking about which irrationals are ""unique"" (in terms of being able to be expressed in terms of other irrationals / rationals), and for this informal thought, I've come up with the following situation which I'll describe formally: Suppose we define equivalence classes on irrationals, such that $$[r] = \{ p + r : \forall p \in \mathbb{Q}  \}$$ Or stated in other words, $$x \in [y] \lor y \in [x] \rightarrow x - y \in \mathbb{Q}$$ How many such distinct equivalent classes would exist? Countably infinite? uncountably infinite?",['real-analysis']
3811499,Number of ways to distribute $170$ identical things in $12$ boxes where each box has different capacity.,"Suppose you have a lot of boxes which can accommodate either $1, 5, 10, 20$ or $50$ items. You need to arrange $170$ items in $12$ boxes only (nothing less, nothing more). In how many ways can you make this distribution?
Is there any formulae based approach to this? Example: One solution is $10 \times 7 + 20 \times 5 = 170$ , i.e., $7$ boxes of $10$ capacity and $5$ boxes of $20$ capacity. Also, all the boxes must be filled to the full capacity.","['integer-partitions', 'combinatorics', 'discrete-mathematics']"
3811532,How to prove that the elasticity of the revenue function is $E_R(p)=\frac{E_R(q)}{E_R(q)-1}$?,"The Problem Let the demand function be $ap+bq=k$ . Prove that this equation (elasticity of revenue) is true: $$E_R(p)=\frac{E_R(q)}{E_R(q)-1}$$ Definitions Demand Function The Demand Function is defined as the relation between the price $p$ of the good and the demanded quantity $q$ of the good which in our example is: $ap+bq=k$ . Note that $D^{-1}(p) = G(q)$ Revenue Function The Revenue Function is defined as $R = p q$ , where R is the total revenue, $p$ is the selling price per unit of sales, and $q$ is the number of units sold Elasticity of a function The Elasticity of a function $f(x)$ approximates the change of $f$ given the change of $x$ and is defined as: $$ E_f(x) = \frac{df}{dx} \frac{x}{f(x)}$$ My solution attempt We need to prove $E_R(p)=\frac{E_R(q)}{E_R(q)-1}$ The demand function can be written as: $ap+bq=k \iff \boxed{D(p) = q = \frac{k-ap}{b}} \:\:(1)$ and $\boxed{G(q) = p = \frac{k-bq}{a}}\:\: (2)$ Therefore we can write the revenue function as $R(q) = pq = pD(p) \iff \boxed{ R(q) = \frac{kp-ap^2}{b} }  \:\:(3)$ and $R(p) = pq = G(q)q \iff \boxed{R(p) = \frac{kq-bq^2}{a}}\:\: (4)$ Hence, $E_R(p) = \frac{R(q)}{dq} \frac{q}{R(q)} \stackrel{(3)}{=} \left(\frac{kp-ap^2}{a}\right)'\cdot \frac{q}{\frac{kp-ap^2}{a}} = \frac{k-2ap}{a}\cdot \frac{q}{\frac{kp-ap^2}{a}} = \frac{\left(k-2ap\right)q}{kp-ap^2}$ $$ \boxed{ E_R(p) = \frac{\left(k-2ap\right)q}{kp-ap^2}}\:\: (5)$$ And, $E_R(q) = \frac{R(p)}{dq} \frac{p}{R(p)} \stackrel{(4)}{=}  \left( \frac{kq-bq^2}{a} 
 \right)' \cdot \frac{p}{\frac{kq-bq^2}{a}} = \frac{k-2bq}{a}  \cdot \frac{p}{\frac{kq-bq^2}{a}} = \frac{p\left(k-2bq\right)}{kq-bq^2}$ $$ \boxed{E_R(q) = \frac{p\left(k-2bq\right)}{kq-bq^2}} \:\:(6)$$ So, at last: $$E_R(p)=\frac{E_R(q)}{E_R(q)-1} \iff \\ \frac{\left(k-2ap\right)q}{kp-ap^2} = \frac{\frac{p\left(k-2bq\right)}{kq-bq^2}}{\frac{p\left(k-2bq\right)}{kq-bq^2}-1}$$ Which is overly complicated but it must hold, if there were no trivial calculation mistakes. The Question Given the fact that this was an exam subquestion, I am confident that there is an easier way to prove the elasticity equation (maybe by using elasticity function properties?), but if there is, I can't spot it. Any ideas?","['economics', 'calculus', 'derivatives', 'finance']"
3811534,Solving a differential equation using Integrating Factor method,"I need to to model a raindrop's velocity as it is falling with respect to time. The assumptions made are that air resistance is negligible and that the raindrop is spherical I was able to to calculate and solve the differential equation for the change in radius over time to be: $$
r(t) = \frac{k}{p}t + r(0)
$$ where $k$ is a proportionality constant, $p$ is the density of the raindrop and $r(0)$ is the initial radius. Where I am stuck is where the differential equation for velocity of the raindrop is given but I need to solve the equation with the integrating factor method. Given that $r(0) = 3$ $$
\frac{dv}{dt} + \frac{3r'}{r}v = g
$$ where $r = r(t)$ as above and $g$ = gravitational force constant. The solution for this DE should be: $$
v(t) = \frac{pg}{4k} r + \frac{C}{r^3}
$$ where $C$ is the integrating constant. I am unsure of how I would calculate the integrating factor when there is a $r$ and $r'$ in the integral as well as how I should use this integrating factor to solve the differential equation. I was given a hint that the chain rule would help with saving some work","['integration', 'mathematical-modeling', 'ordinary-differential-equations', 'integrating-factor', 'derivatives']"
3811566,Solving $\int_{0}^{x}(x-t)y(t)dt = 2x+\int_{0}^{x}y(t)dt$,"Solve: $$
\int_{0}^{x}(x-t)y(t)dt = 2x + \int_{0}^{x}y(t)dt
$$ The farthest I got is to: $$
\int_{0}^{x}(x-t)y(t)dt-\int_{0}^{x}y(t)dt = 2x
$$ Combining the integrals we get: $$
\int_{0}^{x}y(t)(x-t-1)dt = 2x
$$ And here I’m pretty stuck. Can someone please give me a hint? Thanks.","['integration', 'calculus', 'ordinary-differential-equations']"
3811568,The Euler-Lagrange equation,"Here's an (edited) excerpt from my Calculus textbook. I've highlighted the part I didn't understand in boldface, so could anyone explain those bits? Let us consider the integral $$
I=\int_{a}^{b} F\left(y, y^{\prime}, x\right) d x
$$ where $a, b$ and the form of the function $F$ are fixed by given considerations. The curve $y(x)$ is to be chosen so as to make stationary the value of $I$ . Let us suppose that $y(x)$ is the function required to make $I$ stationary and consider making the replacement $$
y(x) \rightarrow y(x)+\alpha \eta(x)
$$ where the parameter $\alpha$ is small and $\eta(x)$ is a nicely behaved arbitrary function. For the value of $I$ to be stationary with respect to these variations, we require $$
\left.\frac{d I}{d \alpha}\right|_{\alpha=0}=0 \quad \text { for all } \eta(x)
$$ Expanding as a Taylor series in $\alpha$ , we obtain $$
\begin{aligned}
I(y, \alpha) &=\int_{a}^{b} F\left(y+\alpha \eta, y^{\prime}+\alpha \eta^{\prime}, x\right) d x \\
&=\int_{a}^{b} F\left(y, y^{\prime}, x\right) d x+\int_{a}^{b}\left(\frac{\partial F}{\partial y} \alpha \eta+\frac{\partial F}{\partial y^{\prime}} \alpha \eta^{\prime}\right) d x+\mathrm{O}\left(\alpha^{2}\right)
\end{aligned}
$$ Thus, $$
\left.\frac{d I}{d \alpha}\right|_{\alpha=0}= \delta I=\int_{a}^{b}\left(\frac{\partial F}{\partial y} \eta+\frac{\partial F}{\partial y^{\prime}} \eta^{\prime}\right) d x=0.
$$ where $\delta I$ denotes the first-order variation in the value of $I$ due to the variation (22.2) in the function $y(x) .$ Integrating the second term by parts this becomes $$
\left[\eta \frac{\partial F}{\partial y^{\prime}}\right]_{a}^{b}+\int_{a}^{b}\left[\frac{\partial F}{\partial y}-\frac{d}{d x}\left(\frac{\partial F}{\partial y^{\prime}}\right)\right] \eta(x) d x=0.
$$ Now, if we demand that the lower end-point $a$ is fixed, while we allow variation of the end-point $b$ along the curve $h(x, y)=0$ , (Why/how is this?) we obtain through a similar analysis as above that the variation in the value of $I$ due to the arbitrary variation is given to first order by $$
\delta I=\left[\frac{\partial F}{\partial y^{\prime}} \eta\right]_{a}^{b}+\int_{a}^{b}\left(\frac{\partial F}{\partial y}-\frac{d}{d x} \frac{\partial F}{\partial y^{\prime}}\right) \eta d x+F(b) \Delta x
$$ where $\Delta x$ is the displacement in the $x$ -direction of the upper end-point,  and $F(b)$ is the value of $F$ at $x=b .$ We of course require the displacement $\Delta x$ to be small. We can show that $\Delta y=\eta(b)+y^{\prime}(b) \Delta x .$ Since the upper end-point must lie on $h(x, y)=0$ (Why/how is this?) we also require that, at $x=b$ , $$
\frac{\partial h}{\partial x} \Delta x+\frac{\partial h}{\partial y} \Delta y=0.
$$","['multivariable-calculus', 'euler-lagrange-equation', 'calculus-of-variations']"
3811583,Establishing an identity for exponential function through an application of DCT,"From exercise 2.35 of ""Measure theory and probability theory"" by Krishna and Soumendra: Using the DCT or otherwise show that for any sequence of real numbers $\{x_n\}$ with $\displaystyle{\lim_{n \to\infty}}x_n=x$ : \begin{gather*} 
\displaystyle{\lim_{n \to\infty}}\left(1 + \frac{x_n}n \right)^n=\sum_{j=0}^\infty\frac{x^j}{j!}
\end{gather*} The book references the precedent exercise where the following is established: Let $A=((a_{ij}))$ be an infinite matrix of real numbers. Suppose that $\displaystyle{\lim_{i \to\infty}}a_{ij}=a_j \in \mathbb{R}$ and $\displaystyle{\sup_{i}}\left|a_{ij}\right| = b_j \in\mathbb{R}$ for each $j$ and $\sum_{j=0}^\infty b_j < \infty$ then: \begin{gather*} 
\displaystyle{\lim_{n \to\infty}}\sum_{j=1}^\infty \left| a_{ij}-a_j\right|=0
\end{gather*} That is an application of the DCT with the measurable space $(\mathbb{N}, \mathcal{P}(\mathbb{N}))$ and the counting measure, $a_i(j)$ convergents to $a(j)$ and dominated by the integrable function $b(j)$ . I'm interested in clarifying the computation for casting the first problem as an istance of the second, thus solving it by the DCT. Any help would be appreciated, thanks.","['measure-theory', 'exponential-function', 'sequences-and-series']"
3811614,Use Stokes' theorem to show this integral relation.,"By applying Stokes’ theorem to the vector field $a×F$ , where $a$ is an arbitrary constant vector and $F(r)$ is a vector field, show that $$
\int_{C}dr\times F(r)=\int_{S}(dS\times \nabla)\times F
$$ where the curve $C$ bounds the open surface $S$ . Stokes's theorem gives $\int_{S}(\nabla\times (a\times F))\cdot dS=\int_{C}(a\times F)\cdot dr$ . The right hand side is (being $a$ constant, and by permuting triple product) $a\cdot \int_{C}dr\times F(r)$ , while the left hand side, being $a$ constant, is $\int_{S}(a(\nabla\cdot F)-(a\cdot \nabla)F)\cdot dS=a\cdot \int_{S}(\nabla\cdot F)\cdot dS-\int_{S}dS\cdot (a\cdot \nabla)F$ . I wonder if I can isolate the $a\cdot$ on this side like I did on the right hand side to obtain a relation without $a$ . Or if I should use a particular value for $a$ .","['multivariable-calculus', 'stokes-theorem', 'vector-analysis']"
3811707,"Given $q$ and $\cos(q\pi)$ to be rational, find all possible values of $\cos(q\pi)$.","Question: Given $q$ and $\cos(q\pi)$ to be rational, find all possible values of $\cos(q\pi)$ After some trial and error I think the possible values are $\{0,\pm 1,\pm 1/2 \}$ .
I could show that $\cos(q\pi)$ rational implies $\cos(2q\pi)$ is rational. But after that I couldn't find anything else. Any help is appreciated.","['trigonometry', 'rational-numbers']"
3811733,Equivalent of product rule for integrals: $\int f'(x)g'(x)dx$,"Recently I've been working on some integration problems and I have come across some integrands of the form $f'(x)g'(x)$ . I've found myself wondering; I know the product rule for differentiation, written below: $$\frac{d}{dx}f(x)g(x)=f'(x)g(x)+g'(x)f(x)$$ but is there a similar formula for the integral below? (I have written $f'(x)g'(x)$ as opposed to $f(x)g(x)$ as I'm talking about a case when I know the integral of each individual function in the integrand.) $$\int f'(x)g'(x)dx$$ In order that you don't give me answers that are too advanced for me, I'll quickly list what I have covered in calculus so far: differentiation (chain,product, basic,quotient, $\ln$ , exponential,trig etc); integration using reverse chain rule, substitution,separation of variables, 1st and 2nd order differential equations in both $x$ and $y$ , use of partial fractions,integration by parts,trig integration, some basic hyperbolic functions,exponentials and logarithms. Please comment if you would like more information. Thnak you for your help, it is much appreciated :)","['integration', 'ordinary-differential-equations', 'calculus', 'closed-form', 'indefinite-integrals']"
3811741,Is there a continuous way to represent a triangle with a vector with no ambiguity?,"I'm looking for a continuous way to represent a triangle with a vector of real numbers. If a triangle is naively represented by a triplet of 2-dimensional vertices $(\mathbf{p}_1, \mathbf{p}_2, \mathbf{p}_3)$ , then the representation is dependent of the order of the points, because for every permutation of the vertices $\sigma$ , the triplet $(\mathbf{p}_{\sigma(1)}, \mathbf{p}_{\sigma(2)}, \mathbf{p}_{\sigma(3)})$ would also represent the same triangle. So a triangle would not have a unique representation. One way to ensure the unicity of representation would be to sort the points depending on a criteria: lexicographic order for example (sort on x value, then y value if x's are equal), but then the representation would not be continuous, because a tiny modification of a vertex could lead to a different order of the sorted vertices if two points were closed on the x coordinate. So is it possible to have a computer representation that is not dependent of the order of the vertices but also continuous ? I imagine that there may be topological arguments to prove this is not possible, but I can't figure it out.","['triangles', 'general-topology', 'vectors']"
3811742,Geodesic flow are generated by Hamiltonian vector field,"Let $(M,g)$ be a Riemannian manifold and consider the Hamiltonian \begin{equation*}
    \begin{array}{rcl}
        H:T^*M & \rightarrow&\mathbb{R} \\
        (q,p) & \mapsto&H(q,p):=\frac{1}{2}g^{ij}(q)p_ip_j.
    \end{array}
\end{equation*} We would like to show that the Hamiltonian vector field $X_H:T^*M\rightarrow TT^*M$ generates the geodesic flow. We can explicitely compute $X_H$ , since its components satisfies Hamilton's equations: \begin{equation*}
    X_H=\dfrac{\partial H}{\partial p_j}\dfrac{\partial}{\partial q^j}-\dfrac{\partial H}{\partial q^j}\dfrac{\partial}{\partial p_k}=g^{ij}p_i\dfrac{\partial}{\partial q^j}-\frac{1}{2}\dfrac{\partial g^{ij}}{\partial q^k}p_ip_j\dfrac{\partial}{\partial p_k}.
\end{equation*} We can see that the following vector field $Y:TM\rightarrow TTM$ , over $TM$ generates the geodesic flow: \begin{equation*}
    Y(q,\dot{q}):=\dot{q}^i\dfrac{\partial}{\partial q^i}-\Gamma^j_{lk}\dot{q}^l\dot{q}^k\dfrac{\partial}{\partial\dot{q}^j}.
\end{equation*} Indeed, we show that its integral curves are geodesics. Let $\gamma:\mathcal{I}\rightarrow TM$ , with $\gamma(t):=(\alpha(t),\beta(t))$ , then \begin{equation*}
 \dot{\gamma}(t)=\dot{\alpha}^i(t)\dfrac{\partial}{\partial q^i}+\dot{\beta}^j(t)\dfrac{\partial}{\partial\dot{q}^j},   
\end{equation*} so $Y(\gamma(t))=\dot{\gamma}(t)$ $\iff$ the following hold \begin{equation*}
    \left\{\begin{array}{ll}
         \dot{\alpha}^i(t)=\beta^i(t)&  \\
         \dot{\beta}^j(t)=-\Gamma^j_{lk}\beta^l(t)\beta^k(t)& 
    \end{array}\right.
\end{equation*} This is the geodesic equation and so this proves that the integral curves of $Y$ are geodesics. The problem is that $Y$ is a vector field over $TM$ , while in the Hamiltonian formalism, $X_H$ is a vector field over $T^*M$ . How are $X_H$ and $Y$ related? How can we show that $X_H$ generates the geodesic flow as $Y$ does?
Thank you all. P.S. It is enough to show that the integral curves of $X_H$ satisfies the geodesic equation in the cotangent bundle? But then, what does it look like in the cotangent bundle?","['hamilton-equations', 'riemannian-geometry', 'differential-geometry']"
3811767,show that $\partial_u(f \circ g)=f'(g(x))\partial_ug(x)= f'(g(x))(\nabla g(x) \cdot u)$,"Let $f :\mathbb{R} \to \mathbb{R}$ be a real-valued differentiable function. If $g :\mathbb{R^3} \to \mathbb{R}$ is differentiable, show that $$\partial_u(f \circ g)=f'(g(x))\partial_ug(x)= f'(g(x))(\nabla g(x) \cdot u)$$ where, $u$ is an unit vector. I cannot seem to get this to work. It pretty much uses only the chain rule(?), but the notation confuses me a lot. How should I go about this?","['partial-derivative', 'multivariable-calculus', 'vector-analysis']"
3811780,Is a Gaussian Processes equivalent to a linear transformation of itself?,"I am wondering whether a non-degenerate continuous Gaussian process is equivalent in distribution to a linear transformation of itself. More specifically, let $T$ be a separable, complete and compact metric space, $C(T,\mathbb{R})$ the set of continuous, real-valued functions on $T$ and $\mathcal{B}$ the Borel- $\sigma$ -algebra on $C(T,\mathbb{R})$ . Let $$X = (X_t)_{t\in T} \colon (\Omega,\mathcal{A},\mathbb{P}) \to (C(T,\mathbb{R}),\mathcal{B})$$ be a Gaussian process with continuous mean function $m\colon T\to \mathbb{R}$ and continuous, positive definite covariance function $K\colon T\times T\to \mathbb{R}$ . Then the distribution $\mathbb{P}^X = \mathbb{P}(X^{-1}(\cdot))$ of $X$ is a probability measure on $\mathcal{B}$ . Now let $a\in \mathbb{R}\setminus\{0\}$ and $f\in C(T,\mathbb{R})$ . Then $aX + f = (aX_t + f(t))_{t\in T}$ again defines a Gaussian process with mean function $am + f$ and covariance function $a^2K$ . My question now is whether $aX + f$ is equivalent to $X$ in distribution, that is, if $$\mathbb{P}^X \stackrel{?}{\sim} \mathbb{P}^{aX + f},$$ where by equivalence of two measures I mean absolute continuity with respect to each other. Clearly, it is necessary for $X$ to be non-degenerate (i.e. for $K$ to be positive definite), but in that case it seems like an intuitive statement to me. If it is at all true, I could also imagine it only holds if $f$ is a ""typical"" path of $X$ in some sense. If I am not mistaken, it follows from Theorem 1 in ""Equivalence and perpendicularity of Gaussian processes"" by J. Feldman ( https://projecteuclid.org/euclid.pjm/1103039696 ) that the distributions of two Gaussian processes in $C(T,\mathbb{R})$ can only be equivalent or orthogonal, so it would suffice to show that the distributions of $X$ and $aX + f$ are not orthogonal. Edit : An approach suggested by E-A in the comments is to use the fact that X (as well as its translate) assigns positive probabilities to open neighbourhoods of continuous functions (see f.ex. https://arxiv.org/abs/math/0702686 , Theorem 4). Some more thought would have to go into this, however, since in general, two Borel probability measures that both assign positive probability to open balls can still be orthogonal (f.ex. any probability measure with positive weights on $\mathbb{Q}$ and its translate by $\pi$ ).","['stochastic-processes', 'measure-theory', 'gaussian', 'probability-theory']"
3811782,Proof that convex open sets in $\mathbb{R}^n$ are homeomorphic?,"This is an exercise from Kelley's book. Could someone help to show me a proof? It seems very natural, and it is easy to prove by utilizing the arctan function in $\mathbb{R}^1$. 
Thanks a lot.",['general-topology']
3811795,How do i form the inequality $ -5\le 3\cos x - 4\sin x\le5$? [duplicate],This question already has answers here : Prove: $|a\sin x+b \cos x|\leq \sqrt{a^2+b^2}$ (6 answers) Closed 3 years ago . We know that $-1\le \cos x\le1$ so we if we multiply 3 so we get $-3\le 3\cos x\le 3$ and we also know that $-1\le \sin x\le 1$ and and again we multiply 4 we get $-4\le 4\sin x\le 4$ and we can add these two inequality we get $-7\le 3\cos x - 4\sin x\le 7$ but how in the above inequality is forming i m not able to understand at all.,"['trigonometry', 'inequality']"
3811821,Uniqueness of the heat equation for initial data in $L^\infty$,"For $1 \leq p < \infty$ and initial-data $u _ 0 \in L ^p ( \mathbb{R} ^d)$ due to P. Li (Uniqueness of $L^1$ solutions for the Laplace equation and the heat equation on Riemannian manifolds) there exists a unique solution $u$ of the heat equation $u_t - \Delta u = 0$ in $(0, \infty) \times \mathbb{ R }^d$ which satisfies \begin{equation*}
		u \in C ( [0 , \infty ) ; L^p ( \mathbb{ R }^d) ) \quad \text{and} \quad u ( 0 , \cdot ) = u_0.
	\end{equation*} This solution is given by $u ( t ) = e ^{t \Delta} u_0$ , the convolution with the fundamental solution. My question is, whether there is known literature for the uniqueness in the case of non-negative $u_0 \in L^\infty ( \mathbb{ R }^d )$ for the heat equation in $(0, \infty) \times \mathbb{ R }^d$ , where we replace $u \in C ( [0 , \infty ) ; L^p ( \mathbb{ R }^d) )$ by $u \in L^\infty ( (0, \infty) \times \mathbb{ R }^d)$ and $u(t,x) \to u_0(x)$ for almost every $x \in \mathbb{ R }^d$ as $t \to 0$ . Is weak star convergency of $u(t, \cdot)$ to $u_0$ in $L^\infty(\mathbb{R}^d)$ as $t \to 0$ sufficient?","['partial-differential-equations', 'parabolic-pde', 'heat-equation', 'real-analysis']"
3811829,Prove that an automorphism of $M_n(k)$ where $k$ is an algebraically closed field must be an inner automorphism.,Prove that an automorphism of $M_n(k)$ where $k$ is an algebraically closed field must be an inner automorphism. I'm a bit stuck on this one but I feel like I understand why it's true. Since $M_n(k)$ has a unique simple module (a column of lentgh $n$ ) really the only thing that can be done is to permute around the order of these simple summands (from a vector space perspective) but we need to leave them in their original order so we have to undo this. This would lead us to conjugation by an invertible matrix. Can someone provide some guidance please? Thanks! There is a relevant post here but I was hoping to avoid looking up the proof and just receive some guidance on how to continue. Automorphism of the matrix algebra is an inner automorphism,"['abstract-algebra', 'modules']"
3811831,Prove $[ (-1)^{⌊x⌋+⌊\frac{x}{n}⌋}~\cdot~(x-⌊x⌋-\frac{1}{2}) ]+ ⌊\frac{x}{n}⌋ + \frac{1}{2}$ is continuous if $n$ is odd.,"This comes from problem 21c of Chapter 7 of Spivak's Calculus 4th Edition. 21c) Find a continuous function $f$ which takes on every value exactly $n$ times, where $n$ is odd. The solution I came up is $f_n(x)= [ (-1)^{⌊x⌋+⌊\frac{x}{n}⌋}~\cdot~(x-⌊x⌋-\frac{1}{2}) ]+ ⌊\frac{x}{n}⌋ + \frac{1}{2}$ Where $⌊x⌋$ is the floor function. It seems to work, but I want to prove it works. Well atleast I want to prove it is continuous on $\mathbb{R}$ . No idea where to start. Any suggestions?","['continuity', 'functions', 'real-analysis']"
3811851,Simpler ways of finding solutions to $\int_0^x \lfloor{x\rfloor}^2 dx=2(x-1)$,"So I was solving the following question: Find the number of solutions to $$\int_0^x \lfloor{x\rfloor}^2 dx=2(x-1)$$ for $x<0$ . $$$$ Options: $2,3,4,5$ And I managed to show that there's no solution whenever $x$ is an integer, and obtained the following equation for whenever $x$ is not an integer: $$\lfloor{x\rfloor}^2(\lfloor{x\rfloor}+1-x)-\frac{\lfloor{x\rfloor}(\lfloor{x\rfloor}+1)(2\lfloor{x\rfloor}+1)}{6}=2(1-x)$$ And I managed to solve it, but the method is faaaaar too lengthy(I'd made a silly error before). Let $n$ denote $\lfloor{x\rfloor}$ and $f$ denote $x-\lfloor{x\rfloor}$ . So, the above equation transforms to: $$f=\frac{-2n^3+3n^2+11n-12}{6(n^2-2)}$$ With some calculus, I was able to show $n=-3$ is the only possible (negative) value such that $0\leq f<1$ , giving me the only answer $x=-3+\frac 67=-\frac{15}{7}$ Other methods to solve the original question are welcome. Edit: The question was on a pen-paper test, so everything has to be done by hand. As the answers show, it either the options or the question that's erroneous.","['ceiling-and-floor-functions', 'definite-integrals', 'alternative-proof', 'calculus', 'algebra-precalculus']"
