question_id,title,body,tags
4118889,Correspondence between ideals of a valuation ring and value group,"I'm reading number theory seminar notes from Stanford . I found the following exercise, posting the abridged version: Let $R$ be a valuation ring with fraction field $K$ . Let $\Gamma := \Gamma_R = K^{\times}/R^{\times}$ be the
value group of $R$ , and $v$ the valuation on $K$ given by reduction
modulo $R^\times$ . Consider the mapping $M_I:= I \mapsto v(I \backslash \{0\})$ We leave to the reader as an exercise to prove that this is an
inclusion-preserving bijection from the set of ideals of $R$ onto the
set of submonoids $M \subset Γ_{\le 1}$ such that $m \in M, \gamma \le m \Rightarrow γ \in M$ This exercise has defeated me: how should I think about proving it?","['field-theory', 'order-theory', 'ring-theory', 'abstract-algebra', 'group-theory']"
4118933,What is wrong with this proof that L'Hospital works both ways?,"Our professor proved L'Hôpital's rule through Cauchy's mean value theorem. He proved it in multiple parts, I have trouble following the first, most basic example. Theorem: Let $f, g$ be continuous functions on $(a,b)$ where $a < b$ . Let: $g(x) \neq 0$ and $g'(x) \neq 0$ , $\forall x \in (a,b)$ $ \lim_{x \downarrow a} f(x) = \lim_{x \downarrow a} g(x) = 0  $ If limit $B = \lim_{x \downarrow a} \frac{f'(x)}{g'(x)}$ exists, then $A = \lim_{x \downarrow a} \frac{f(x)}{g(x)}$ also exists and $A = B$ . Proof: We define $f(a)=0$ and $g(a)=0$ . Then, by condition $(2)$ , we see that $f, g$ are continuous on $[a,b)$ . Let $x \in (a,b)$ . Then on $[a,x]$ all necessary conditions for Cauchy's mean value theorem are fulfilled. Therefore, there exists a $c_x \in (a,x)$ , such that: $$\frac{f'(c_x)}{g'(c_x)} = \frac{f(x)-f(a)}{g(x)-g(a)} = \frac{f(x)}{g(x)}$$ When $x \downarrow a$ , also $c_x \downarrow a$ . Therefore: if $B= \lim_{x \downarrow a} \frac{f'(c_x)}{g'(c_x)}$ exists, $A= \lim_{x \downarrow a} \frac{f(x)}{g(x)}$ exists as well and $A=B$ . Question: Now, my main question is, why does this last conclusion work? Why couldn't we say the same thing in the other direction: if $A= \lim_{x \downarrow a} \frac{f(x)}{g(x)}$ exists, $B= \lim_{x \downarrow a} \frac{f'(c_x)}{g'(c_x)}$ exists as well and $A=B$ . What would be wrong with that conclusion? Everyone says that L'Hopital only works one way, but I don't see why that is true from the proof.","['analysis', 'real-analysis', 'calculus', 'limits', 'derivatives']"
4118945,Inner and Outer Group Automorphisms,"I have a question about something I believe the naming convention for group automorphisms suggests. From my understanding the inner automorphisms defined on a group $G$ are those automorphisms $\varphi: G \to G$ where there is some $g \in G$ where $\varphi$ is equivalent to the action of $g$ on $G$ by conjugation. Those automorphisms which are not inner automorphisms ( $\text{Inn }G$ ), are outer automorphisms. To me, this suggests as if (informally) there is a larger group $A$ where $G \trianglelefteq A$ , and those ""outer"" automorphisms are simply equivalent to the action of some element $a \in A/G$ on $G$ by conjugation (It's called outer because $a$ is outside of $G$ ). Note that I'm stating $G$ is a normal subgroup of $A$ since we want $G$ to be normalized by all elements of $A$ so that the action of $a$ by conjugation limited to $G$ becomes an automorphism. This idea is not discussed in the textbook I am studying. So my question is if the following theorem is valid, and how one goes around proving it. Proposition: For every group $G$ there exists groups $G'$ and $A'$ such that $G \cong G'$ and $G' \trianglelefteq A'$ where the following condition holds: $$ \text{Aut }G \cong \text{Inn}\ A' $$","['automorphism-group', 'group-theory', 'abstract-algebra']"
4118994,$D \overline{D}= \mu$ for a complex matrix $D$ and a scalar $\mu$ implies that $\mu$ is real,"The following question arose while reading Kevin Buzzard's notes on ""Forms of reductive algebraic groups"", available here . Let $D \in GL(n,\mathbb{C})$ be a matrix such that $D \overline{D} = \mu$ for a scalar $\mu \in \mathbb{C}$ .  Here the right hand side denotes the scalar matrix obtained by multiplying the identity by $\mu$ .  Buzzard claims that $\mu$ must be real.  His reasoning is that $\overline{\mu} = \mu$ , but I don't see why this is true.  If you apply complex conjugation to both sides of $D \overline{D} = \mu$ , you get $\overline{D} D = \overline{\mu}$ , but I don't see why we must have $D \overline{D} = \overline{D} D$ .","['matrices', 'linear-algebra']"
4119115,Closed sets in a cofinite topology,"I'm reading a set theory text book and I've come upon the section on topologies. (An Introduction to Set Theory and Topology by Ronald Freiwald, p. 105). The author defines the cofinite topology in the usual way: T = { O ⊆ X : O = ∅ or X - O is finite} Then he writes: ""In ( X , T ), a set F is closed iff F = ∅ or F is finite."" This seems wrong to me. What if X is infinite? Then it is closed but not finite. Am I missing something? If this is wrong, what do you think he was trying to get at?",['general-topology']
4119207,Are random variables equal almost surely if their expectation are interchangable?,"Suppose you have $X$ and $Y$ on a common probability space Let $h,g$ be arbitrary bounded Borel-measurable functions. If you have: $$
\mathbb{E}(h(X)g(Y)) = \mathbb{E}(h(X)g(X))
$$ Do you also have X=Y a.s.? My intuition is yes, using $h=1,g(\cdot)=|\cdot-X|$ (is this allowed?), and then LHS=RHS=0 meaning $|X-Y|=0$ a.s.",['probability-theory']
4119234,Maximize $x^2y$ where $x^2+y^2=100$. (Looking for other approaches),"For positive numbers $x,y$ we have $x^2+y^2=100$ . For which ratio of $x$ to $y$ , the value of $x^2y$ will be maximum? $1)\ 2\qquad\qquad2)\ \sqrt3\qquad\qquad3)\ \frac32\qquad\qquad4)\ \sqrt2$ It is a problem from a timed exam so I'm looking for alternative approaches to solve this problem quickly. Here is my approach: We have $x^2=100-y^2$ , so $x^2y=y(100-y^2)$ . By differentiating with respect to $y$ and putting it equal to zero, we have: $$(100-y^2)-2y^2=0\qquad\qquad y^2=\frac{100}3$$ Hence $x^2=\frac{200}3$ and $\frac{x^2}{y^2}=2$ therefore $\frac{x}{y}=\sqrt2$ . Is there another quick approach to solve it? (like AM-GM inequality or others)","['optimization', 'calculus', 'algebra-precalculus']"
4119277,what does $Φ ^ {-1}$ mean?,This is a part of a statistics question I am given $Φ ^ {-1}(0.9377)=1.536$ but I don't know what this $Φ ^ {-1}$ means and how he got 1.536 and from where. any help is much appreciated thank you.,"['statistics', 'probability']"
4119290,Is the set of words generated by a non-empty subset $S$ of a finite group $G$ of order $n$ of length $n$ always be a (normal) subgroup?,"Let S be a non-empty subset of a group $\mathbf G$ , and $\mathbf \vert G \vert$ = n. For each $k$ , let $S^{k}$ be the set with elements of form $\{s_1 \dotsm s_k \mid s_i \in S\}$ . Prove that $S^{n}$ is a subgroup G. Or going more further, a normal subgroup of G? Let $H$ be the group generated by $S$ . I have two observations: For any $h ∈ H$ , $h \in S^{m}$ for some $m$ with $1\le m\le n$ . That is, each element of $H$ is a word of length less than $n$ . For $1\le i\le j\le n$ , we have $\vert S^{i} \vert\le\vert S^{j} \vert$ . That is, the cardinality of the chain $S^{i}$ is non-decreasing. So from above, if $\vert S^{n}\vert = \vert H\vert$ , we are done. But there still is a case when for some $i$ , the case $\vert S^{i}\vert = \vert S^{i+1}\vert$ . Anyone can help?","['group-theory', 'abstract-algebra', 'finite-groups']"
4119298,Solve $ f'(x)f(-x)=f(x)$,"I want to find all differentiable functions $ f$ from $ \mathbb{R}$ to $ \mathbb{R}$ such that $$ 
 f'(x)f(-x)=f(x)
$$ for all $ x \in \mathbb{R}$ . It's easy to check that this equation admits $f(x)=0$ and $ f(x)=1+e^x$ as solutions. The main difficulty comes from the appearance of $f(-x) $ . For the case in which $f(x) \neq 0 $ for all $ x$ , I've tried differentiating both sides of the equation and using the relation $ f'(-x) f'(x) = 1 $ to get rid of the term $ f'(-x) $ and obtained $$
f'' f = f'^2 + f'
$$ But this equation seems to be more complicated.",['ordinary-differential-equations']
4119315,conditional expectation wrt sigma field identity,"I'm reading a proof which seems to make use of an identity, below, that I can't prove. Let $f$ be a Borel measurable function, assume that $X$ is measurable with respect to $\mathcal{F}$ and $Y$ is independent of $\mathcal{F}$ .
Then $\mathbb{E}[f(X+Y) |\mathcal{F}] = \int f(y+X) P_Y(dy)$ . Can some one verify if this is true? Thanks. attempt: take limit of indicator functions. in the case where $f$ is the indicator function on the set $(\infty,z]$ this reduces to showing $\mathbb{P} [X+Y \leq z | \mathcal{F}] = \int_{-\infty}^{z-X} P_Y(dy)$ but I cannot prove this.",['probability-theory']
4119317,"Let $f(x)=\frac{x\ln x}{1+x}$,evaluate $\lim _{m\to f\left(x_0\right)+}\frac{x_1+x_2-2x_0}{m+x_0}$.","Let $f(x)=\frac{x\ln x}{1+x}$ ,suppose $x_0$ is the extreme point of $f$ .For $m\in \mathbb{R}	$ ,it's easy to see  that if $m>f(x_0)=-x_0$ ,then the equation $f(x)=m$ has exactly two real roots $x_1,x_2$ , $x_1<x_0<x_2$ .The question is how to evaluate the limit \begin{align*}
\lim _{m\to f\left(x_0\right)+}\frac{x_1+x_2-2x_0}{m+x_0}？
\end{align*} To this problem,we can see that if $m\to f(x_0)+$ ,then $x_1\to x_0^-$ ,and $x_2\to x_0^+$ .By Taylor's formula we have $$f(x_1)-f(x_0)=f'(x_0)(x_1-x_0)+\frac12f''(\xi)(x_1-x_0)^2=\frac12f''(\xi)(x_1-x_0)^2,$$ where $\xi \in (x_1,x_0)$ .So $$m+x_0\sim \frac{f''(x_0)}{2}(x_1-x_0)^2.$$ But I can't handle $x_1+x_2-2x_0$ .If we can expand $x_1+x_2-2x_0$ as  terms of power of $(x_1-x_0)$ ,then it must be $$x_1+x_2-2x_0\sim k(x_1-x_0)^2.$$ But how can I get it?Maybe there are other ways.","['limits', 'mathematical-physics']"
4119369,Function of at most polynomial growth and its derivative,"I consider a function $f:\mathbb{R}^{+} \rightarrow \mathbb{R}$ which is continuous and differentiable. I know moreover that both $f$ and $f'$ are of at most polynomial growth, i.e.
I use the following definition: The function $g$ is of at most polynomial growth if there exist constants $m$ and $C$ such that $$
|g(x)|\leq C (x^{-m} + x^m)
$$ for all $x \in \mathbb{R}^{+}$ . My question is: Is there any relation in the degree of polynomial $m$ occuring in the above equation for $f$ and $f'$ if we know that that both are of at most polynomial growth? My intuition is that if $f$ is bounded by a polynomial of degree $m$ , then $f'$ is bounded by a polynomial of degree $m-1$ . Is this statement true?","['functions', 'polynomials', 'asymptotics']"
4119389,"If $p^2=a^2(\cos x)^2+b^2(\sin x)^2$, prove that $p +\frac{d^2 p}{dx^2}=\frac{a^2 b^2}{p^3}$",How to prove this? It seems simple enough to start but the at the end I cannot prove the expression $p +\frac{d^2 p}{dx^2}=\frac{a^2 b^2}{p^3}$ . Is there a particular trick I am missing or is this just an ordinary sum with lots of manipulation. I did try modifying the question slightly by multiplying both the LHS and RHS by $p^3$ . That seems to lessen the manipulation somewhat but still the answer does not come.,['derivatives']
4119452,Proof of $\int_Mf\nabla_i\nabla_jh_{ij}\mathsf{dvol}_g=\int_M(\nabla_i\nabla_j f)h_{ij}\ \mathsf{dvol}_g.$,"Suppose $f$ is a scalar function and $h$ a symmetric $(0,2)$ tensor on a closed Riemannian manifold $(M,g)$ . Then is the following equality true? $$\int_Mf\nabla_i\nabla_jh_{ij}\mathsf{dvol}_g=\int_M(\nabla_i\nabla_j f)h_{ij}\ \mathsf{dvol}_g.$$ I know that $\nabla_i\nabla_jh_{ij}=\mathsf{div}(\nabla_jh_{ij})$ and $f\mathsf{div}(\nabla_jh_{ij})=\mathsf{div}(f\nabla_jh_{ij}) -\langle \nabla f , \nabla_jh_{ij} \rangle$ then integrating over $M$ and using divergence theorem gives: $$\int_Mf\nabla_i\nabla_jh_{ij}\ \mathsf{dvol}_g=\int_M-\langle \nabla f , \nabla_jh_{ij} \rangle\ \mathsf{dvol}_g$$ The other equation that can help is $\mathsf{div}(\nabla_jfh_{ij})=\nabla_i\nabla_j(fh_{ij})=(\nabla_i\nabla_jf)h_{ij}+f\nabla_i\nabla_jh_{ij}+2\nabla_if\nabla_j h_{ij}$ but I don't know how to relate this to the last equation and deduce the wanted equality.","['riemannian-geometry', 'differential-geometry']"
4119454,The Gamma function is holomorphic on V,"From the Weierstrass theorem we know that if $(f_n)$ is a sequence of holomorphic functions on an open set $V\subset\mathbb C$ such that $(f_n)$ uniformly converges to $f$ on every compact $K\subset V$ , then $f$ is holomorphic on $V.$ I am planing to use this idea to show that $$ \Gamma(z)=\int_0^{\infty}e^{-t}t^{z-1}\,\mathrm dt $$ is holomorphic on the open right half plane of $\mathbb{C.}$ Let's denote this set by $V.$ The following is the plan: Let $$\Gamma_n(z):=\int_{1/n}^ne^{-t}t^{z-1}\,\mathrm dt$$ for any $z \in V.$ Then I tried with DCT and spent my whole day to show that $\Gamma_n$ converges to $\Gamma,$ but I failed. Could you help me prove this uniform convergence? Also, on this site, a solution suggested by Disintegrating-By-Parts used Fubini's theorem. How do we actually know that $\Gamma_n$ is $L^1 ((0, \infty) \times V$ )?  Here V is the open right halfplane in $\mathbb{C}.$ NB: This problem was asked two/three times. Unfortunately, I do not completely understand any of the solutions. So, I am posting this question one more time. Thank you for your time.","['complex-analysis', 'gamma-function']"
4119494,Abstract Wiener Space for $\ell^2(\mathbb{R})$,"Let $\ell^2(\mathbb{R})$ denote the space of square-summable real-valued sequences equipped with the inner product $$ \langle x, y \rangle = \sum_{n = 1}^{\infty} x_n y_n $$ Let $\nu$ denote its canonical cylinder measure i.e. a cylinder measure defined on the weak sigma algebra whose Fourier transform has the form $$ \exp( - \frac{1}{2} \langle x, x \rangle_{\ell^2}) ~~.$$ Does there exist a measurable norm associated to $\ell^2$ and $\nu$ ? If so, is there a nice description of its associated abstract Wiener space? If not, is there a proof that there is not? I do know that $\ell^2$ is the Cameron-Martin space of $\mathbb{R}^{\infty}$ with the product topology and the distribution of an iid sequence of random variables $(X_n)_{n \in \mathbb{N}}$ with $X_1 \sim \mathcal{N}(0,1)$ , and also that $\ell^2$ lies dense in that space w.r.t. the topology of point-wise convergence. So it ""should be"" $\mathbb{R}^{\infty}$ , but it is not since $\mathbb{R}^{\infty}$ is only separable Frechet and not Banach.","['measure-theory', 'gaussian-measure', 'functional-analysis']"
4119504,Ordering sets of integers based on their elements: can we always achieve the perfect balance?,"Consider a family $\{S_i\}_{i=1}^n$ of finite, pairwise disjoint sets of positive integers, i.e., $S_i\subset \mathbb{N}$ and $S_i\cap S_j=\emptyset$ , for any $1\le i,j \le n$ and $i\neq j$ . Denote by $\succ$ a linear order over $\{S_i\}_{i=1}^n$ and, if $\overline{s} = (s_1, \dots, s_n)$ is a tuple with $s_i\in S_i$ , say that $\overline{s}$ induces $\succ$ if it holds that $s_i > s_j$ if and only if $S_i \succ S_j$ . Example ( $n=2$ ). If $S_1 = \{0,3\}$ and $S_2 = \{1,2\}$ , the tuple $(3,1)$ induces the order $S_1\succ S_2$ ,
whereas the tuple $(0,1)$ induces the order $S_2 \succ S_1$ . Note that different tuples of integers can induce the same ordering, e.g., the tuples $(3,1)$ and $(3,2)$ both induce the order $S_1\succ S_2$ . Intuitively, I am drawing a number $s_i$ from each $S_i$ and using the
ordered sequence thus obtained to determine the ordering over the $S_i$ 's. The question I am interested in is whether, for any $n\geq 2$ , we can choose the sets $\{S_i\}_{i=1}^n$ such that every possible linear order over $\{S_i\}_{i=1}^n$ is induced equally often. Example ( $n=2$ , continued). With $S_1$ and $S_2$ as above, there are two possible linear orders over $\{S_1, S_2\}$ , i.e., $S_1 \succ S_2$ and $S_2\succ S_1$ , and four possible tuples,
i.e., $(0,1)$ , $(0,2)$ , $(3,1)$ , $(3,2)$ . Note that $(0,1)$ , $(0,2)$ induce $S_2\succ S_1$ and $(3,1)$ , $(3,2)$ induce $S_1 \succ S_2$ , so in this case each of the two possible linear orders over $\{S_1,S_2\}$ appears exactly twice. Example ( $n=3$ ). The following assignment works: $S_1 = \{2, 5, 7, 12, 15, 16\}$ , $S_2 = \{1, 6, 8, 11, 14, 17\}$ ,
and $S_3 = \{3, 4, 9, 10, 13, 18\}$ .
There are $3!=6$ linear orders over $\{S_1,S_2,S_3\}$ ,
the sets $S_1$ , $S_2$ and $S_3$ each have $6$ elements,
there are $6^3=216$ triples consisting of one element from each,
and each linear order over $\{S_1,S_2,S_3\}$ is induced exactly $\frac{216}{6}=36$ times by these triples. The example for $n=3$ was found with the help of a computer,
and it is not unique,
but for $n \ge 4$ the search space already gets too big to handle
and the code timed out before finding any solution. Of course, the $S_i$ 's do not need to have the same cardinality. I am not necessarily looking for a solution,
but would appreciate any insight that could help me think about the general case.
Has the problem been studied before?
Does it ring any bells?","['combinatorics', 'discrete-mathematics']"
4119506,Integrate $\int_{0}^{\infty} \big( |y + x|^{\lambda - 1} - |y|^{\lambda - 1} \big) y^{-\beta} dy$,"Let $0< \lambda < 1$ and $0< \beta < \lambda/2$ . I would like to integrate, $$\int_{0}^{\infty} \big( |y + 1|^{\lambda - 1} - |y|^{\lambda - 1} \big) y^{-\beta} dy,$$ this integral is finite because for $y \rightarrow \infty,~(|x+y|^{\lambda - 1} - |y|^{\lambda - 1})y^{-\beta} \approx y^{\lambda - \beta - 2}.$ Integral of $|y + 1|^{\lambda - 1}y^{-\beta}$ can be calculated and  it's equal to Beta function that is, $$ \int_{0}^{\infty} |y + 1|^{\lambda - 1}y^{-\beta} dy = B(1 - \beta, \beta - \lambda).$$ I would really appreciate any hints or tips.","['integration', 'calculus', 'definite-integrals', 'analysis']"
4119585,Ito isometry for different time interval,"Without throwing too much technical details, The Ito isometry says $$
\mathbb{E}\bigg[\Big(\int^t_0 X_s d W_s\Big) \Big(\int^t_0 X_s d W_s\Big)^T \bigg] = \int^t_0 X_s\,X_s^T ds,
$$ for some suitable Brownian motion $W_t$ . How about in the above two integrals $t$ are not the same? That is, $$
\mathbb{E}\bigg[\Big(\int^{t_1}_0 X_s d W_s\Big) \Big(\int^{t_2}_0 X_s d W_s\Big)^T \bigg] = ?
$$ Is it equal to $\int^{\mathrm{min}(t_1, t_2)}_0 X_s\,X_s^T ds$ ? If yes how to prove?","['stochastic-integrals', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4119589,Show that the positive solution of $a = \tanh(ax)$ satisfies $\frac{1}{1-a^2} > x$,"Let $x>1$ and $a = \tanh(ax)$ be positive (out of the three roots). How can I conclude that $$
\frac{1}{1-a^2} > x?
$$ Context of how this popped up: I was solving the Ising model in the mean field theory for zero magnetic field case. But all this can be ignored if one is not familiar. The following is the relevant math. I had to minimize $F(M)$ , and it turned out that \begin{align}
F'(M) &= -(NqJ)M + \frac{N}{2\beta}\ln\left( \frac{1+M}{1-M} \right)\quad\text{and,}\\
F''(M) &= \frac{N}{\beta}\left( -\beta qJ + \frac{1}{1-M^2} \right),
\end{align} where $\beta$ , $N$ , $q$ , $J$ are constants of the theory. $M$ is the variable. It is easily seen that $F'(M) = 0$ corresponds to the transcendental equation $M = \tanh(\beta qJM)$ , which splits into two cases: $\beta qJ \le 1$ . Then we have the only solution to be $M = 0$ , which also happens to be a minimum. $\beta qJ> 1$ . Then we have three solutions to the transcendental equation, say $-M_s, 0, M_s$ (with $M_s > 0$ ). Then $M = 0$ is not a minimum. To show that $M_s$ (and hence $-M_s$ ) is a local minimum, I needed the inequality to hold.","['inequality', 'analysis', 'real-analysis']"
4119623,How can a markov transition matrix have eigenvalues other than 1?,"A Markov transition matrix has all nonnegative entries and so by the Perron-Frobenius theorem has real, positive eigenvalues. In particular the largest eigenvalue is 1 by property 11 here . Furthermore in these notes (sec 10.3) it says that the eigenvalues of $P$ are $1 = \lambda_1 > \lambda_2 \geq \dots \geq \lambda_N \geq -1$ . But how can a transition matrix $P$ have eigenvalues less than 1? Since the matrix is acting on probability distributions $v$ , which have to have $\sum_i v_i = 1$ , we cannot have $Pv = cv$ with $c\neq 1$ since $\sum_i cv_i = c$ .","['transition-matrix', 'markov-chains', 'probability']"
4119627,"Reason for $(0,1)$ tensor as a vector?","Assume, we have a finite-dimensional vector space $V = (R^3,+,-)$ over the field $(R,+,-)$ and the dual space is the set of all the linear maps, $V^* = \{f:R^3 \to R| f \text{ is linear}\}$ . We define $(r,s)$ tensor as multilinear map: $V \times ... \times V(\text{r times})\times V^* \times ... \times V^*(\text{s times}) \to R$ .   I understand that we can view the dual space, $V^{*}$ as a $(1,0)$ tensor since it maps $R^3 \to R$ . But how can we view $(0,1)$ tensor as a vector since a $(0,1)$ tensor will be a map between $V^* \to R$ .","['tensors', 'differential-geometry']"
4119636,Upper bound for a conditional variance (and expectation),"Let $V(x,y)\in C^\infty$ be a uniformly convex function with polynomial growth, i.e. there exist $\alpha>0$ , $n\in\mathbb N$ , $k>0$ such that $$Hess V(x,y) \,\geq\, \alpha\, I \quad\textrm{for all }(x,y)\in\mathbb R^{d_1}\times\mathbb R^{d_2}$$ $$V(x,y) \,\leq\, k\,(|x|^{2n}+|y|^{2n}) \quad\textrm{as }|x|^2+|y|^2\to\infty \;.$$ We may assume w.l.o.g. $\int\!\int e^{-V(x,y)} dx\,dy=1$ , so that $e^{-V(x,y)}dx\,dy\,$ is a log-concave probability measure. I am interested in the conditional probabily measure.
In particular I wonder if -maybe under additional assumption on $V$ and its derivatives- we have $$ \frac{\int |x|^{2p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx}  \,-\,  \left(\frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx}\right)^2 \,\leq\, h_1\,|y|^{2p} + h_0$$ for any $p \in \mathbb N$ and for suitable constants $h_1,h_0\geq0$ (that depend on $p$ and $V$ ). In other terms: is the variance of $|x|^{p}$ given $y$ bounded by a polynomial in $|y|$ ? Example. The answer is yes in the case of Gaussian measure. Indeed if $e^{-V(x,y)}dx\,dy\,$ is the Gaussian measure with mean $0$ and positive-definite covariance matrix $A$ , then the conditional measure given $y$ turns out to be the Gaussian measure with mean $A_{12}A_{22}^{-1}y$ and covariance matrix $A_{11}-A_{12}A_{22}^{-1}A_{12}^T$ . Then a simple change of variable $x\mapsto x+A_{12}A_{22}^{-1}y$ allows to conclude. Edit. After computations in the case of Gaussian measure, and after some simple numerics for a non-Gaussian measure ( $2n=4$ ), I suspect the same inequality could hold true also for expectations. Namely: $$ \frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx} \,\leq\, h_1\,|y|^p + h_0$$ for every $p\in\mathbb N$ and for suitable constants $h_0,h_1\geq0$ which depend of $p$ and $V$ . Of course this would imply the previous inequality for variances. Edit 2. By continuity of the functions involved, an equivalent formulation of the previous inequality is: $$ L\equiv \limsup_{|y|\to\infty} \frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{|y|^p \int e^{-V(x,y)} dx} <\infty \;.$$ Edit 3. Conjecture : if $V(x,y)$ is a uniformly convex polynomial of degree $2n\geq 2$ and $p\in\mathbb N$ , there exists a positive-definite covariance matrix $A$ such that $$ \frac{\int |x|^{2p} e^{-V(x,y)} dx}{\int e^{-V(x,y)} dx} \,\leq\, \frac{\int |x|^{2p} e^{-G(x,y)} dx}{\int e^{-G(x,y)} dx}$$ where $G(x,y) = \frac{1}{2}\langle A^{-1}(x-x_0,y-y_0) , (x-x_0,y-y_0)\rangle $ and $(x_0,y_0)$ is the unique minimum point of $V$ .
Now in the Gaussian case we can perform computations explcitly, finding $$ \frac{\int |x|^{2p} e^{-G(x,y)} dx}{\int e^{-G(x,y)} dx} \leq h_1\,|y|^{2p} + h_0 $$ for suitable constants $h_0,h_1$ . Thus if the conjecture holds true, it would prove the desired bound for conditional moments, at least in the case $V$ is a polynomial.","['conditional-probability', 'polynomials', 'functional-analysis', 'convex-analysis', 'probability-theory']"
4119650,Why should we believe the abc conjecture?,"To fix notation and check that my definitions are correct I will first state: abc conjecture: Let $a,b\in\mathbb{N}$ be coprime, $c:=a+b$ , and define the quality of the triple $(a,b,c)$ to equal $q(a,b,c):=\log(c)/\log(\text{rad}(abc))$ .  Then for each $\varepsilon>0$ , the number of such $(a,b,c)$ with $q>1+\varepsilon$ is finite. However, looking at this table on Wikipedia of qualities of triples with $c<10^{18}$ , it would seem possible that for $q\in[1,3/2]$ we have that the number $N(q,n)$ of triples with quality $>q$ and $c<n$ satisfies $N(q,n)=\Omega_q(\log n)$ . This would contradict the abc conjecture, so is there a reason that this behaviour should eventually stop for large $c$ ?  Are there any other reasons (aside from its consequences) to believe that it is true? My understanding of the conjecture is very superficial - I understand (from Silverman's Arithmetic of elliptic curves , section VIII) that abc implies Szpiro's conjecture, and conversely Szpiro's conjecture implies abc with the 1 replaced with 3/2, but this is the extent of my knowledge.  I would also be interested in reasons why one might believe Szpiro's conjecture, which I currently do not have any intuition about.","['number-theory', 'abc-conjecture', 'elliptic-curves', 'arithmetic-geometry']"
4119658,Hairy Ball Theorem on $\mathbb{S}^2$: a counter-example?,"Let $(U, (\theta, \varphi))$ be the spherical coordinate chart on the sphere $\mathbb{S}^2$ , and consider the vector field on $U$ defined by $\dfrac{\partial}{\partial \varphi}$ . Then in this chart, the metric tensor $g_{ij}$ can be expressed as follows: $$g_{ij} = d\varphi^2 + \sin^2\varphi \, d\theta^2. $$ This means that the vector $\dfrac{\partial}{\partial \varphi} \Bigr|_{p} \in T_p\mathbb{S}^2$ has norm $1$ for any point $p \in U$ . But this seems to contradict the Hairy Ball theorem, which says that every smooth vector field on $\mathbb{S}^2$ has to vanish somewhere. What's going on? What is wrong with this logic? I think the problem is that the spherical coordinate chart on $\mathbb{S}^2$ is not a global chart. So perhaps this vector field defined on $U$ cannot be extended to a smooth vector field on the whole manifold. My question is: is there an intuitive geometric reason why we would expect that this vector field cannot be smoothly extended? Something involving pictures of vector fields would be helpful. Thanks.","['vector-fields', 'riemannian-geometry', 'differential-geometry']"
4119663,Primes in shifted geometric progressions,"Are there integers $a,b,r\in\mathbb Z$ such that there are infinitely many primes of the form $ar^n+b$ with $n\in\mathbb N$ ? (I added the reference-request tag because I suspect that the answer to the above question is contained in the literature.)","['number-theory', 'examples-counterexamples', 'prime-numbers', 'reference-request']"
4119676,Will the limit exist?,"If $$f(x) = \begin{cases} \frac{\sin([x]+x)}{[x]+x} &\textrm{if } x \neq 0, \\ 1 &\textrm{if } x = 0, \end{cases}$$ where [.] is the greatest integer function, then does $\lim_{x \to 0}f(x)$ exist or not? I thought of using the property that $\lim_{x \to 0} \frac{\sin g(x)}{g(x)}=1$ by which I thought that the limit should exist but my book said otherwise. What did I miss? Is there anything else that should be applied?","['limits', 'calculus', 'functions', 'ceiling-and-floor-functions']"
4119720,Computing trace of matrix,Let A be a n×n matrix such that $$[a_{ij}]_{n×n}=\frac{((-1)^i)(2i^2+1)}{4j^4+1}$$ then what is $$1+ \lim_{n\to {\infty}}\left(tr(A^n)^{1/n}\right)$$ I cannot figure out how to calculate trace of $$A^n$$,"['matrices', 'limits', 'trace', 'matrix-calculus']"
4119766,Number of elements of order coprime to $p$ in a finite group,"Let $G$ be a finite group. It is easy to prove that the elements of $G$ with odd order are in odd number. Indeed, if, for every divisor $d$ of the order of $G$ , $r_{d}$ denotes the number of elements of $G$ with order $d$ , then the number of  elements of $G$ with odd order is $1 + \sum_{d} r_{d}$ , where $d$ ranges over the odd divisors of $\vert G \vert$ such that $d \geq 3$ ; but $r_{d}$ is divisible by $\varphi (d)$ (Euler function) and, for a naturel number $n \geq 3$ , $\varphi (n)$ is even, thus it is well true that the elements of $G$ with odd order are in odd number. So, my question is : let $G$ be a finite group and $p$ a prime number; is it known if it is necessarily true that the elements of $G$ with order coprime to $p$ are in number coprime to $p$ ? I found some lemmas and, if I am not wrong, these lemmas make it possible to prove that the answer is ""yes"" for every group of order $\leq 431$ . Since my proof of this meager result is very long, I will only sketch it if I don't get a better answer. Edit (August 8, 2021). I discover that this question is Exercise 28 b) in Bourbaki, Algèbre, ch. I (Paris, 1970), § 6, p. I.139.","['group-theory', 'finite-groups']"
4119790,Solve the equation $\frac{x^2-10x+15}{x^2-6x+15}=\frac{4x}{x^2-12x+15}$,Solve the equation $$\dfrac{x^2-10x+15}{x^2-6x+15}=\dfrac{4x}{x^2-12x+15}.$$ First we have $$x^2-6x+15\ne0$$ which is true for every $x$ ( $D_1=k^2-ac=9-15<0$ ) and $$x^2-12x+15\ne0\Rightarrow x\ne6\pm\sqrt{21}.$$ Now $$(x^2-10x+15)(x^2-12x+15)=4x(x^2-6x+15)\\x^4-12x^3+15x^2-10x^3+120x-150x+15x^2-180x+225=\\=4x^3-24x^2+60x$$ which is an equation I can't solve. I tried to simplify the LHS by $$\dfrac{x^2-10x+15}{x^2-6x+15}=\dfrac{(x^2-6x+15)-4x}{x^2-6x+15}=1-\dfrac{4x}{x^2-6x+15}$$ but this isn't helpful at all. Any help would be appreciated! :) Thank you in advance!,['algebra-precalculus']
4119820,Calculus prove that $F(x)= \int_{-\infty}^xf(t)dt$ is continuous,"Let $f: \mathbb{R} \to \mathbb{R}$ be a function for which $\int_{-\infty}^xf(t)dt$ converges for every $x \in \mathbb{R}$ . Consider the function $F$ defined as $F(x)= \int_{-\infty}^xf(t)dt$ for every $x \in \mathbb{R}$ . Prove that $F$ is continuous in $\mathbb{R}$ . My attempt: Assuming towards contradiction that $F$ is not continuous, so there exists a $x_0$ such that $F(x_0) \ne \lim_{x \to x_0}F(x_0)$ . But: $F(x_0)=\int_{-\infty}^{x_0}f(t)dt=L$ $\lim_{x \to x_0} F(x_0)=\lim_{x \to x_0}\int_{-\infty}^{x_0}f(t)dt=\lim_{x \to x_0}L=L$ So $F(x_0) = \lim_{x \to x_0}F(x_0)$ - contradiction. Is that correct? Thanks!","['integration', 'calculus', 'functions', 'solution-verification']"
4119841,Prove by induction that $3^{n-2} \geq n^5$ for $n\geq20$,"I am new to induction proofs and wanted to know if my reasoning behind the following proof is correct. Here are my steps : $n=18 \Rightarrow 3^{18} \geq 20^5$ Given that $3^{n-2} \geq n^5$ is true for n, show $3^{n-1} \geq (n+1)^5$ $3^{n-1} = 3\times(3)^{n-2} \geq 3n^5$ If $3n^5 \geq (n+1)^5$ , then $3\times (3)^{n-2} \geq 3n^5 \geq (n+1)^5 \Leftrightarrow 3^{n-1} \geq (n+1)^5$ I show this by induction for all $n \geq 20$ . $n=18 \Rightarrow 3(20)^{5} \geq 21^5$ $3(n+1)^{5} = 3n^5+15n^4+30n^3+30n^2+15n+3 \geq (n+1)^5+15n^4+30n^3+30n^2+15n+3$ $3n^5+15n^4+30n^3+30n^2+15n+3 \geq (n+2)^5-10n^4-40n^2-60n-28$ $(3n^5+25n^4+70n^3+30n^2+75n+31) = (n+2)^5 + (2n^5+15n^4+30n^3+10n^2+10n+1)\geq (n+2)^5$ Since $(2n^5+15n^4+30n^3+10n^2+10n+1) > 0$ for $n\geq20$ , it follows that $(n+2)^5+(2n^5+...+1) \geq (n+2)^5$ . Therefore, $3\times (3)^{n-2} \geq 3n^5 \geq (n+1)^5 \Leftrightarrow 3^{n-1} \geq (n+1)^5$","['discrete-mathematics', 'real-analysis']"
4119869,Vector-valued integral: Equivalence of Riemann sums and Rudin's definition,"Definition/Notation: In his book ""Functional Analysis"", Rudin defines the integral of a vector-valued function as follows (Definition 3.26). Let $(Q, \mu)$ be a measure space, $X$ a topological vector space on which the dual $X^*$ separates points, and $f:Q\to X$ such that $\lambda f:Q\to\mathbb{R}$ is integrable w.r.t. $\mu$ for any $\lambda\in X^*$ . If there exists some $y\in X$ such that $$
\lambda(y)=\int_Q \lambda f d\mu \qquad (*)
$$ for all $\lambda \in X^*$ , define $\int_Qfd\mu=y$ . Question: I would like to show that this is equivalent to defining it as the strong limit of Riemann sums in the following way (Chapter 3, Exercise 23): Suppose $Q$ is a compact Hausdorff space, $\mu$ a Borel probability measure, $X$ a Fréchet space [for my purposes we could even assume Banach, if that makes it easier] and $f:Q\to X$ continuous. (The existence of the integral under these circumstances is shown in Theorem 3.27.) Then for any neighborhood $V\subseteq X$ of $0$ , there exists a partition $E_1, ..., E_n$ of $Q$ (i.e. $E_i\cap E_j = \emptyset$ for $i\neq j$ and $\bigcup_{i=1}^n E_i=Q$ ) such that $$
z:=\int_Q fd\mu - \sum_{i=1}^n \mu(E_i)f(s_i) \in V
$$ for any choice of $s_i\in E_i$ . Own work: As mentioned, I assume $(X, \lVert \cdot \rVert_X)$ is Banach. Given some neighborhood $0\in V\subseteq X$ , let $B(0,\varepsilon)\subseteq V$ for some $\varepsilon>0$ . By the Hahn-Banach theorem, there exists $\lambda \in X^*$ with $\lambda(z)=\lVert z \rVert_X$ and $\lVert \lambda \rVert=1$ , so $$
\lVert z \rVert_X = |\lambda(z)| = \left|\int_Q \lambda fd\mu - \sum_{i=1}^n \mu(E_i)\lambda(f(s_i))\right| \le \sum_{i=1}^n \int_{E_i} |\lambda(f(t)-f(s_i))|\mu(dt) \le \varepsilon \qquad (**)
$$ given that we define the $E_i$ such that $f(t)-f(s) \in B(0,\varepsilon)$ whenever $t,s \in E_i$ .
Since $Q$ is compact, $f$ is uniformly continuous, which I assume helps with finding the partition. I struggle, since it has to be a finite partition (which would be easy if $X$ were totally bounded, but it doesn't have to be). Could someone help me?",['functional-analysis']
4119918,Showing an equality of complex numbers,"Let $e_k \in \mathbb{C}$ such that $$e_1+e_2+e_3 =0$$ and $$g =4(e_1 e_2 + e_2 e_3 + e_1 e_3)$$ Prove that $$
g^2=16(e_1^2e_2 ^2 + e_2^2 e_3^2+e_1^2 e_3^2)
$$ What I got : $$
g^2 =16(e_1^2e_2 ^2 + e_2^2 e_3^2+e_1^2 e_3^2+2e_1^2e_2e_3+2e_1e_2^2e_3+2e_1e_2e_3^2)
$$ However, I fail to see how $2e_1^2e_2e_3+2e_1e_2^2e_3+2e_1e_2e_3^2 = 0$ Would appreciate any help","['algebra-precalculus', 'complex-numbers']"
4119919,Question in a proof from Gathmann's notes on Algebraic Geometry: The tangent space.,"I am studying chapter 10 from Gathmann's notes about algebraic geometry and there is something I don't understand in the following proof. I don't really understand the equality $\frac{g}{f} = c g$ . If I understand well, the aim of this part is to show that $S^{-1}(I(a)/I(a)^2) \subseteq I(a)/ I(a)^2$ so that we can deduce the ismorphism since the other inclusion is trivial. So this equality has to be understood as an equality in $S^{-1}(I(a)/I(a)^2)$ , i.e. $$\frac{g}{f} = \frac{cg}{1} \quad \Leftrightarrow \quad \exists t \in S, ~t(g - cf g) = 0 \text{ in }A(X).$$ But I really don't see how we deduce from the first part that $\exists t \in S, ~t(g - cf g) = 0 $ since $f$ is invertible in $A(X)/I(a)$ , not in $A(X)$ . Any help ?","['proof-explanation', 'localization', 'algebraic-geometry', 'local-rings', 'tangent-bundle']"
4119998,Proof regarding conditional expectation of sum of minima,"Assume $X_1, X_2$ are continuous bounded random variables, where the dependency between them is unknown, where the joint and marginal density functions are well-defined. Let \begin{align}
I_1 := \min(X_1, \overset{\sim}{a_1}), \quad I_2 := \min(X_2,\overset{\sim}{a_2}), 
\end{align} where $\overset{\sim}{a_1}, \overset{\sim}{a_2}$ are non-negative numbers. Let $\alpha \in (0, 1)$ be fixed, and let $F_{I_1+I_1}^{-1}(\alpha)$ denote the $\alpha$ -quantile of $I_1+I_2$ . Moreover, define $[t]^+ := \max(t, 0)$ . I am trying to show that given $a_1, a_2, \overset{\sim}{a_1}, \overset{\sim}{a_2}$ such that \begin{align}
a_1+a_2 = F_{I_1+I_2}^{-1}(\alpha)
\end{align} and \begin{align}
 \overset{\sim}{a_1}+ \overset{\sim}{a_2} > F_{I_1+I_2}^{-1}(\alpha), 
\end{align} with $a_1 < \sup X_1, a_2 < \sup X_2$ , then \begin{align}
& E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I_2}^{-1}(\alpha)] + E\left[ [X_1 -\overset{\sim}{a_1}]^+\right] + E\left[ [X_1 -\overset{\sim}{a_2}]^+\right]\\ & \leq a_1 + a_2 + E\left[ [X_1 -a_1]^+\right] + E\left[ [X_1 -a_2^+\right].
\end{align} I have not been able to contradict this using Monte Carlo simulation, but I cannot seem to prove it formally. I have considered \begin{align}
E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I
_2}^{-1}(\alpha)] &= \frac{1}{1-\alpha} \int_{\mathbb{R}} x f_{I_1+I_2}(x) \chi_{\{x \geq F_{X_1+X_2}^{-1}(\alpha)\}}\ dx\\
&= \frac{1}{1-\alpha} \left( \int_{\mathbb{R}} x f_{I_1+I_2}(x) \chi_{ \{\overset{\sim}{a_1}+\overset{\sim}{a_2} > x \geq F_{I_1+I_2}^{-1}(\alpha)\}}\ dx + (\overset{\sim}{a_1} +\overset{\sim}{a_2})( 1-F_{I_1+I_2}(\overset{\sim}{a_1} +\overset{\sim}{a_2})) \right).
\end{align} The integrand is clearly larger than $a_1+a_2$ everywhere, so it evaluates to a larger value than $a_1+a_2$ . At the same time, $E[ [X_1-a_1]^+]$ and $E[ [X_2-a_2]^+]$ are decreasing in $a_1$ and $a_2$ , but as we don't make assumptions on $a_1, a_2$ but on the sum $a_1+a_2$ , the change in these expectations would intuitively depend on the tails of the distributions of $X_1$ , $X_2$ . Is there a way forward with this problem? Or is my setting too general? Do I need to assume independence between $X_1$ and $X_2$ to get anywhere? Edit: Another consideration - The terms \begin{align}
E[[X_1-\overset{\sim}{a_1} ]^+] + E[X_2-\overset{\sim}{a_2}]^+] & \geq E[[X_1 + X_2 - (\overset{\sim}{a_1} +\overset{\sim}{a_2})]^+],
\end{align} by convexity of the function $[\cdot]^+$ . On the other hand, \begin{align}
 & E[[X_1-a_1 ]^+] + E[X_2-a_2]^+] - E[[X_1-\overset{\sim}{a_1} ]^+] + E[X_2-\overset{\sim}{a_2}]^+]\\
&= \int_{a_1}^{\overset{\sim}{a_1}} xf_{X_1}(x) \ dx + \int_{a_2}^{\overset{\sim}{a_2}} xf_{X_2}(x) \ dx, 
\end{align} so I need to show that \begin{align}
 E[I_1+I_2 \ | \ I_1 + I_2 \geq F_{I_1+I_2}^{-1}(\alpha)] - F^{-1}_{I_1+I_2}(\alpha) \geq \int_{a_1}^{\overset{\sim}{a_1}} xf_{X_1}(x)\ dx + \int_{a_2}^{\overset{\sim}{a_2}} xf_{X_2}(x) \ dx, 
\end{align}","['integration', 'statistics', 'stochastic-analysis', 'conditional-expectation', 'probability']"
4120039,Bounding integrals on a `moving' domain,"I've been reading through Schoen-Yau's proof of the positive mass theorem and find myself stuck on a particular estimate they prove. It seems to be simple multivariable calculus, but I haven't been able to prove it. Here's a simplified setup: Let $S\subset\mathbb{R}^3$ be a non-compact surface and consider the set of exhautions $S_{\sigma}=S\cap B_\sigma(0)$ , where $B_\sigma(0)$ is the ball of radius $\sigma$ with centre $0$ . Clearly, $S_{\sigma_1}\subset S_{\sigma_2}$ if $\sigma_1<\sigma_2$ and $\{S_\sigma\}$ exhausts $S$ as $\sigma\rightarrow\infty$ . They use the following argument to bound the integral, $$
\int_S \frac{1}{1+r^a}=\int_{S_{\sigma}}\frac{1}{1+r^a}+\int_{\sigma_0}^\infty\left(\frac{d}{dt}\int_{S_t}\frac{1}{1+r^a}\right)dt\leq Area(S_{\sigma_0})+\int_{\sigma_0}^\infty\frac{1}{1+t^a}\left(\frac{d}{dt}Area(S_t)\right)dt.
$$ Here $a>2$ and $\sigma_0>0$ are constants. The first equality is just an application of the fundamental theorem of calculus. The $Area(S_\sigma)$ is clear. The term that is bothering me is the second term involving $\frac{d}{dt}Area(S_t)$ . I'm not sure how they got that bound. Are they using some kind of Leibniz rule type formula?","['multivariable-calculus', 'differential-geometry', 'riemannian-geometry', 'real-analysis']"
4120041,"Proof that the trigonometric functions form a basis for $L^2[0, 2\pi]$","My math teacher has recently talked about the Fourier series; any periodic function can be written as a sum of trigonometric functions. That's cool and stuff, but he didn't prove it. We only derived a ""formula"" for the coefficients and without proof, I will feel unsatisfied. I started to dig around the internet and found stuff about Hilbert spaces, Schauder bases and $L^2$ spaces (which is a Hilbert space). I then read that the set $\{\sin(nx), \cos(nx)\}_{n \in \mathbb{N}}$ is a Schauder basis for the space $L^p[0, 2 \pi]$ and, if I have not misunderstood, will prove the thing that I want to prove. I also read about orthonormal spaces and their connection to Hilbert spaces. So in this case I want to see proof that the set of trigonometric functions form an orthonormal basis for $L^2[0, 2\pi]$ . The orthogonal part is quite easy and I have searched for proof of the rest for a while and I can't find anything. Maybe some kind person will give me proof of this? However, I don't know how complicated that proof actually is and considering that I have almost no knowledge in this field, a handwavy argument would satisfy me. If not that, then at least guidance. For example, what I need to learn to understand the proof etc.","['fourier-series', 'trigonometry', 'functional-analysis']"
4120059,Continuity of characteristic functions of probability measures with respect to the weak* topology and the topology of compact convergence,"Let $E$ be a normed $\mathbb R$ -vector space, $\sigma(E',E)$ and $c(E',E)$ denote the weak* topology and the topology of compact convergence on $E'$ , respectively, $\mu$ be a probability measure on $E$ and $$\hat\mu:E'\to\mathbb C\setminus\{0\}\;,\;\;\varphi\mapsto\int\mu({\rm d}x)e^{{\rm i}\langle x,\:\varphi\rangle}$$ denote the characteristic function of $\mu$ . As I have shown in this answer , $\hat\mu$ is $c(E',E)$ -continuous as long as we assume that $\mu$ is sufficiently regular $^1$ . However, isn't it $\sigma(E',E)$ -continuous as well? In fact, if $\varphi_n\to\varphi$ wrt $\sigma(E',E)$ it holds $$\forall x\in E:\langle x,\varphi_n\rangle\to\langle x,\varphi\rangle\tag1$$ and hence $$\int\mu({\rm d}x)e^{{\rm i}\langle x,\:\varphi_n\rangle}\to\int\mu({\rm d}x)e^{{\rm i}\langle x,\:\varphi\rangle}\tag2$$ by the dominated convergence theorem. Now, since $\sigma(E',E)\subseteq c(E',E)$ , couldn't we immediately infer from this result that $\hat\mu$ is $c(E',E)$ -continuous? (And hence the $c(E',E)$ -continuity holds without any regularity assumption on $\mu$ .) $^1$ I've assumed thightness of $\mu$ in my answer. So, $E$ being complete and separable would be sufficient.","['measure-theory', 'weak-topology', 'general-topology', 'probability-theory', 'compactness']"
4120075,Some Geometric intuition behind self-adjoint operators,"In my Functional Analysis class, we have been studying self-adjoint compact operators for the past week or so (more specifically their spectrums). I have a geometric idea of what it means for an operator to be compact (the image of any subset is relatively compact in the codomain) and I have a vague geometric notion of the adjoint in my head (define a new operator whose image is the complement of the original operator's kernel and vice versa). I'd like some intuition for what exactly it means for an operator in a Hilbert space to be self-adjoint . Obviously this isn't a question with a right answer; I'd just like to know how people think about/visualize self-adjointness, and I'd appreciate the thoughts of anyone who's tried to do so!","['adjoint-operators', 'soft-question', 'functional-analysis', 'intuition']"
4120096,Finding moment estimator and its asymptotic distribution,"I got a question:
We let $X$ and $Y$ be independent random variables with $X$ Poisson distributed with mean $\lambda$ and $Y$ exponentially distributed with rate $\lambda>0$ and we let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be a sample from this distribution. I have to find the moment estimator $\hat{\lambda}$ based on the statistic $t(x,y)=x-y$ and the sample $(X_1,Y_1),\ldots,(X_n,Y_n)$ and find the moment estimator's asymptotic distribution. Can anyone help me? My thoughts so far is that: To find the asymtotic distribution I think I can use that $V(t(x,y))/m'(\lambda)^2$ , but how do I find $m(\lambda)$ ? And can I then find moment estimator by solving $\lambda$ in $m(\lambda)$ ?","['moment-problem', 'statistics', 'probability-theory', 'poisson-distribution']"
4120099,When I can divide and multiply in a limit by $x$,"I have formulated an old question but in that case I have committed many errors in limit calculus and  I understand my doubt was not absolutely clear so I have decided to remove it (it had not answer) and now I try to formulate it better. If I have a limit as $x$ goes to $\infty$ or $x\to 0$ I can multiply and divide by $x$ in order for instance to regain a notable limit. For instance $$\lim_{x\to 0}\frac{\sin^2{x}}{x}=\lim_{x\to 0}x\frac{\sin^2{x}}{x^2}=0$$ Now:
if I have for istance a function $f(x)$ s.t $\lim_{x\to \infty}\frac{f(x)}{x}=1$ and I have to compute $\lim_{x\to \infty}f(x)-x$ , if I apply the previous argument ONLY on the term $f(x)$ I will obtain: $\lim_{x\to \infty}x-x=0$ . $\textbf{My doubt is: }$ is it allowed what i have done in the last?","['limits', 'calculus', 'real-analysis']"
4120114,Stability of Explicit midpoint method,"I am trying to determine the stability region of the well known explicit midpoint method $$y_{i+1} = y_i + h f\left( t_i + \frac h 2, \ y_i + \frac h 2 f(t_i, y_i)\right)$$ and after following the links Determine a stability region? and Calculating stability and order of implicit midpoint scheme , I managed to apply the numerical method on the test equation and got $$\begin{align*}y_{i+1} & = y_i + h f\left( t_i + \frac h 2, \ y_i + \frac h 2 f(t_i, y_i)\right) \\ & = 
 y_i + \left( \lambda h +\frac{\lambda^2h^2}{2}  \right)y_i \\ 
& = \left( 1 + \lambda h +\frac{\lambda^2h^2}{2}  \right)y_i\end{align*}$$ Stability: $\big| 1 + \lambda h +\frac{\lambda^2h^2}{2}\big|<1$ and simplifying, $-2 < (1+ \lambda h)^2<0$ is the stability. I don't know if the stability region is correct. How do I find it?","['ordinary-differential-equations', 'stability-in-odes', 'runge-kutta-methods', 'stability-theory', 'numerical-methods']"
4120129,Show that $\cup_{x\in\Omega}\mathcal{F}_x$ is a $\sigma$-algebra,"Let $X$ be an arbitrary set and  let $\Omega$ denote the minimal uncountable well-ordered set. Given $\mathcal{E}\subset \mathcal{P}(X) $ with $\emptyset\in \mathcal{E}$ we define a collection of subsets of $\mathcal{P}(X)$ by transfinite recursion as follows: Define $\mathcal{E}^c:=\{E^c:E\in\mathcal{E}\}$ and $\mathcal{E}_{\sigma}:=\{\cup_{n=1}^{\infty}E_n:(E_n)\subset\mathcal{E}\}$ . Define $1:=\min \Omega$ and set $\mathcal{F}_1:=\mathcal{E}\cup \mathcal{E}^c$ . If $x\in\Omega$ and if $x$ has an immediate predecessor $y$ then we set $\mathcal{F}_x:=(\mathcal{F}_y)_{\sigma}\cup ((\mathcal{F}_y)_{\sigma})^c$ , and if $x$ has no immediate predecessor then we set $\mathcal{F}_x:=\cup_{y<x}\mathcal{F_y}$ . The goal is to show that $\cup_{x\in\Omega}\mathcal{F}_x$ is a $\sigma$ -algebra in $X$ . Am stuck on proving closure under countable unions: Let $(E_n)\subset \cup_{x\in\Omega}\mathcal{F}_x$ . For each $n$ choose $x_n\in\Omega$ such that $E_n\in \mathcal{F}_{x_n}$ (axiom of countable choice). Since the set $\{x_n:n\geq 1\}$ is a countable subset of $\Omega$ , it has an upper bound. Let $x$ denote the smallest upper bound. If $x\neq x_n$ for each $n$ , then $x$ has no immediate predecessor $y<x$ , for otherwise such $y$ would be a strictly smaller upper bound for $\{x_n:n\geq 1\}$ . Hence $\mathcal{F}_x=\cup_{y<x}\mathcal{F_y}\supset \cup_{n\geq 1}\mathcal{F}_{x_n}$ , and since $\Omega$ has no largest element it follows that $\cup_{n\geq 1} E_n \in (\mathcal{F_x})_{\sigma}\subset \mathcal{F_{x+1}} $ , where $x+1$ denotes the immediate sucessor of $x$ in $\Omega$ . But what about the case where $x=x_n$ for some $n$ ? EDIT: I tried to follow Troposhere's approach below. Any feedback is very appreciated.","['elementary-set-theory', 'measure-theory', 'well-orders', 'real-analysis']"
4120132,"Finding values of $\theta$ for which $(\cos\theta,\sin\theta)$ lies inside the triangle formed by $x+y=2$, $x−y=1$, and $6x+2y−\sqrt{10}​=0$","Find the range of values of $\theta$ , such that $\theta\in[0,2\pi]$ for which $(\cos \theta,\sin \theta)$ lies inside the triangle formed by $x + y = 2, x − y = 1$ and $6x + 2y − \sqrt{10} ​=0$ . Not getting any hint of how can I get the range of those points is there any short method of doing so. I have plotted the graph and then I found out that the line $6x + 2y − \sqrt{10} ​=0$ which forms one of the sides of the triangle has $O(0,0)$ and $P(\cos \theta, \sin\theta)$ on opposite sides so applied the formula of power of point. So $L_3(O)\cdot L_3(P)<0$ and then solved to find out the range. If there is any short method please tell me.","['analytic-geometry', 'trigonometry', 'geometry']"
4120134,Why are the most likelihood estimators often very intuitive?,"This is likely a very vague and ""wish-washy"" question but I wanted to ask it on here. I have recently began studying mathematical statistics and have been working with the MLE estimators for a while now. There are of course a few exceptions of course but the MLE always seem to be the empirical average, empirical variance or another obvious and intuitive sum that one would guess for the parameter in question. For example when considering a sequence of IID normal random variables we have the MLE for the mean = $ \hat{\mu} =\frac{1}{n}\sum\limits_{i=0}^nX_i$ and $\hat{\sigma} = \frac{1}{2}\sum\limits_{i=0}^n(X_i - \hat{\mu})^2$ Would someone be able to provide some intuition as to this ""rule of thumb"". Maybe I have just been looking too closely into this and its actually very obvious. It does feel natural I just cant seem to explain it at least somewhat rigorously. Thanks :)","['statistical-inference', 'statistics', 'probability-distributions', 'maximum-likelihood', 'probability']"
4120143,Solve the equation $\frac{1}{x}+\frac{1}{x+1}+\frac{1}{x+2}+\frac{1}{x+3}+\frac{1}{x+4}=0$,Solve the equation $$\dfrac{1}{x}+\dfrac{1}{x+1}+\dfrac{1}{x+2}+\dfrac{1}{x+3}+\dfrac{1}{x+4}=0.$$ For $x\ne -4;-3;-2;-1;0$ we have $$(x+1)(x+2)(x+3)(x+4)+x(x+2)(x+3)(x+4)+x(x+1)(x+3)(x+4)+\text{...}=0$$ Most likely that's not the author's intention. I have tried to substitute $t=x+2$ to get $$\dfrac{1}{t-2}+\dfrac{1}{t-1}+\dfrac{1}{t}+\dfrac{1}{t+1}+\dfrac{1}{t+2}=0$$ which actually isn't easier to work with than the original problem.,"['algebra-precalculus', 'rational-functions']"
4120218,2019 China National Olympiad day 1 P3,"Let $O$ be the circumcenter of $\triangle ABC$ ( $AB<AC$ ), and $D$ be a point on the internal angle bisector of $\angle BAC$ . Point $E$ lies on $BC$ , satisfying $OE\parallel AD$ , $DE\perp BC$ . Point $K$ lies on $EB$ extended such that $EK=EA$ . The circumcircle of $\triangle ADK$ meets $BC$ at $P\neq K$ , and meets the circumcircle of $\triangle ABC$ at $Q\neq A$ . Prove that $PQ$ is tangent to the circumcircle of $\triangle ABC$ . my progress:
Let $M$ be the midpoint of arc ${BAC}$ . Let $K'$ be the reflection of $K$ over $E$ and denote $L=AM\cap BC$ . Note that $OE\perp AM$ , thus $OE$ is the perpendicular bisector of $AM \implies E$ is the circumcenter of $\odot(KAMK')$ . Let $X=MK\cap \Gamma$ and $Y=MK'\cap \Gamma$ . Clearly $XY$ passes through $O$ , by radical axis we have that $XY$ also passes through $L$ .","['contest-math', 'geometry']"
4120222,Probability of recurrence for a random walk in $\mathbb Z^3$,"Let $\mathbf X(n)$ be a random walk in $\mathbb Z^3$ in the following sense: We start at the point $\mathbf 0=(0,0,0)$ and for each step, we randomly decide in which of the three directions we move by $\pm 1$ step (i.e., there are $6$ possibilities for each step, each with probability $1/6$ ). It is well-known that this random walk is transient, i.e. $\mathbf P(\mathbf X(n)=\mathbf 0\text{ for some }n\geq 1) \neq 1$ . My question is if this probability can actually be calculated; or if there is some result on what this value is. All sources I found only mention the $\neq 1$ part, but do not comment on the actual value.","['random-walk', 'stochastic-processes', 'markov-chains', 'probability']"
4120259,China Mathematical Olympiad 2016 P5,"Let $ABCD$ be a convex quadrilateral. Show that there exists a square $A'B'C'D'$ (Vertices maybe ordered clockwise or counter-clockwise) such that $A \not = A', B \not = B', C \not = C', D \not = D'$ and $AA',BB',CC',DD'$ are all concurrent. my progress: If $AC \perp BD$ , take $P \equiv AC \cap BD$ , and it is trivial to construct a square $A'B'C'D'$ with center $P$ such that $AA', BB', CC', DD'$ are concurrent at $P.$ Thus, suppose that $AC \not\perp BD.$ We will construct a point $P \in BD$ distinct from $B, D$ such that $\angle APB = \angle CPB.$ Let the perpendicular bisector of $\overline{AC}$ meet $BD$ at $Q$ and let $P$ be the second intersection of $BD$ with $\odot(ACQ).$ Note that $Q$ is a midpoint of arc $\widehat{AC}$ on $\odot(ACQ)$ because $QA = QC.$ Therefore, $BD$ bisects $\angle APC.$ If $P$ coincides with $B$ , repeat above construction about vertex $D$ instead of vertex $B.$ If $P$ then coincides with $D$ , we have $\angle ABD = \angle CBD$ and $\angle ADB = \angle ACB.$ Therefore, $ABCD$ is a kite, implying that $AC \perp BD$ , impossible. Thus, we may assume assume that $P \not\equiv B.$","['contest-math', 'geometry']"
4120287,"show this $e^{x_{n}-2n\pi-\frac{\pi}{2}}-\frac{x_{n}}{2}+n\pi>\frac{1}{5},\forall n\in N^{+}$","as this: problem ,I continue to consider this let $f(x)=e^x\cos{x}-\sin{x}-1$ ,and $n$ be postive integer,such $x_{n}$ be a root of $f(x)=0$ ,and $\dfrac{\pi}{3}+2n\pi<x_{n}<\dfrac{\pi}{2}+2n\pi$ ,show that $$e^{x_{n}-2n\pi-\frac{\pi}{2}}-\dfrac{x_{n}}{2}+n\pi>\dfrac{1}{5},\forall n\in N^{+}\tag{1}$$ First we note $e^x\ge 1+x$ ,so we have $$e^{x_{n}-2n\pi-\frac{x_{n}}{2}}-\dfrac{x_{n}}{2}+n\pi\ge x_{n}-2n\pi-\dfrac{\pi}{2}-\dfrac{x_{n}}{2}+n\pi=\dfrac{x_{n}}{2}-n\pi-\dfrac{\pi}{2}$$ since $$x_{n}>\dfrac{\pi}{3}+2n\pi\Longrightarrow \dfrac{x_{n}}{2}-n\pi-\dfrac{\pi}{2}>-\dfrac{\pi}{3}$$ but last no great than $\frac{1}{5}$","['contest-math', 'trigonometry', 'functions', 'inequality']"
4120334,depicting blowups,"Is there a way to make nice images of blowups? For instance you can always find a picture of the blowup of $y^2 = x^2 +x^3$ for example, https://www.math.purdue.edu/~arapura/graph/nodal.html But I what if I want to blowup $x^2 - y^5=0$ , something that requires multiple blowups? How do I make a nice drawing of it? I'm looking for something that works nice in my browser, some matlab code, or an easy to use javascript library.","['algebraic-curves', 'algebraic-geometry', 'blowup']"
4120343,How to visualize the double cover of $SO(4)$ as two copies of $S^3$?,"How to visualize the double cover of the rotational symmetry group of $S^3$ (which is $Spin(4)$ , namely the double cover of $SO(4)$ ) as two copies of $S^3$ ? This is due to $Spin(4) = SU(2) \times SU(2) = S^3 \times S^3$ . But how to visualize the above  via the perspective of $S^3$ ? Related question: Implications of $\text{Spin}(3)\times \text{Spin}(3) \cong \text{Spin}(4)$","['spin-geometry', 'smooth-manifolds', 'geometric-topology', 'lie-groups', 'differential-geometry']"
4120408,What is the expected value of the number of factors of a number?,"I recently learnt that the expected number of prime factors for an integer $n$ is on the order of $\log\log n$ . This was apparently proven by Hardy and Ramanujan. This led to me wondering what the expected number of (total) factors for a given number is. I am aware that, if $n=p_1^{\nu_1}p_2^{\nu_2}p_3^{\nu_3}\cdots p_k^{\nu_k}$ , then the number of factors of $n$ is $\prod_{i=1}^k\left(\nu_i+1\right)$ . I also know that $\displaystyle P(\nu_i=a)=\frac1{p_i^{~a}}$ . 
Not sure how to proceed, though.","['expected-value', 'number-theory', 'factoring', 'prime-numbers']"
4120409,How the right term of the derivative is gained?,This deduction is one of the typical ones I think. What I want to deduce is the right term from the left term of the below equation. $$\frac{d}{dx}\left(\log\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)\right)=\frac{-a}{x\sqrt{a^{2}+x^{2}}}$$ $\frac{d}{dx}\left(\log\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\frac{d}{dx}\left(\frac{a+\sqrt{a^{2}+x^{2}}}{x}\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\frac{d}{dx}\left(x^{-1}\left(a+\sqrt{a^{2}+x^{2}}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left((x^{-1})'\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(a+\sqrt{a^{2}+x^{2}}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left((-1\cdot x^{-2})\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(\sqrt{a^{2}+x^{2}}\right)\right)$ $=\left(\frac{x}{a+\sqrt{a^{2}+x^{2}}}\right)\left(-x^{-2}\left(a+\sqrt{a^{2}+x^{2}}\right)\right)\left(x^{-1}\frac{d}{dx}\left(\left(a^{2}+x^{2}\right)^{\frac{1}{2}}\right)\right)$ Anyone deduced it in someday? ps. I have to go to work. Back after about 7hours.,"['systems-of-equations', 'derivatives']"
4120445,"A matrix equation, are there solutions?","I try to prove that: there are no matrices $A, B \in M_{3 \times 3}(\mathbb{R})$ such that: $ \begin{pmatrix} 0&1&0\\ 0&0&1\\ 0&0&0 \end{pmatrix} = A^2 + B^2 $ How can I do that? I have no idea.","['matrices', 'linear-algebra']"
4120479,On the proof about the dimension of the conformal group of a manifold,"I have been reading the book ""Transformation Groups in Differential Geometry"" by S. Kobayashi. More concretely, I am trying to understand the proof of the Theorem 6.1 of Chapter IV. Theorem 6.1 (S. Kobayashi, 1954) : Let $M$ be a manifold of dimension $n \geq 3$ , and $P$ a conformal structure on $M$ . Then, the conformal transformation group  Conf $(M)$ of $M$ is a Lie group with dimension $\leq \frac{1}{2}(n+1)(n+2)$ . I think I understood the essence of the proof, which is the following: a conformal structure $P$ on $M$ is determinated by a (unique) Cartan connection on $P$ that ""comes from"" the restriction of the canonical form of second frame bundle $P^2(M)$ to $P$ (that connection is constructed in the section 5 of the same chapter and extended to a Cartan connection on $P$ thanks to Theorem 4.2).
The uniqueness of that Cartan connection holds since $n\geq 3$ . Now, by using the Theorem 3.1 (of the same book), that states that the group $U(P,\omega)$ of automorphisms of $P$ that preserve the Cartan connection $\omega$ , is a Lie group and has dimension $\leq \dim P = \frac{1}{2}(n+1)(n+2)$ , we obtain the result. Unfortunately, there are two details that I cannot understand. That is why I am posting this question. (1) In the page 143, it is written ""Assume that $n\geq 3$ , so that the normal Cartan connection is unique . Then, for each automorphism $f$ of $P$ , $f_*$ (restricted to $P$ ), preserves the normal connection."" I suppose that this means that for any conformal transformation $f$ on $M$ , the induced map $f_*: P^2(M) \rightarrow P^2(M)$ that sends $P$ into $P$ preserves the Cartan connection $\omega$ on $P$ . Thus, we have that any conformal transformation is an automorphism of $U(P,\omega)$ and then Conf $(M) \subset U(P, \omega)$ . However this affirmation is not clear for me. In other words, how do we know that if the Cartan connection is unique, then any automorphism of $P$ preserves that connection? (2) The other detail that intrigues me is the following: Do we have the equality Conf $(M) = U(P, \omega)$ in general or just Conf $(M) \subset U(P, \omega)$ ? By reading the book it seems that the equality holds; that should justify the fact that Conf $(M)$ is a Lie group by using Theorem 3.1, but I still can't see how to prove this fact. Any comment is highly appreciated.","['connections', 'fiber-bundles', 'conformal-geometry', 'geometric-transformation', 'differential-geometry']"
4120487,"Example of compact manifold with bdd such that there exists geodesic whose first time of hitting boundary is different form exit time of geodesic,","Let M be compact Riemann manifold with boundary. The unit sphere bundle $SM$ is given by $$SM=\{(x, v)||v|_{g}=1,x \in M\}$$ where $g$ is the Riemannian metric in the tangent space at $x$ . Given $(x, v) \in S M$ , let $\gamma_{x, v}$ denote the unique geodesic determined by $(x, v)$ so that $\gamma_{x, v}(0)=x$ and $\dot{\gamma}_{x, v}(0)=v$ . For any $(x, v) \in S M$ the geodesic $\gamma_{x, v}$ is defined on a maximal interval of existence that we denote by $\left[-\tau_{-}(x, v), \tau_{+}(x, v)\right]$ where $\tau_{\pm}(x, v) \in[0, \infty]$ , so that $$
\gamma_{x, v}:\left[-\tau_{-}(x, v), \tau_{+}(x, v)\right] \rightarrow M
$$ is a smooth curve that cannot be extended to any larger interval as a smooth curve in $M$ . We let $$
\tau(x, v):=\tau_{+}(x, v)
$$ Thus $\tau(x, v)$ is the exit time when the geodesic $\gamma_{x, v}$ exits $M$ . I am interested in finding an example of manifold such that there exists geodesic at point $x$ and direction $v$ such that first time geodesic to hit the boundary is different from exit time of geodesic $\tau_{(x,v)}$ . I could not imagine an example where geodesic reach boundary but come back again inside the manifold. Any help or hint will be appreciated.","['examples-counterexamples', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
4120488,How can we make an assumption on something undefined?,"I have been learning generating functions lately, and I love the concept that when we represent a sequence in the form of a polynomial, the coefficient of $x$ has some meaning, the power of $x$ has some meaning, but $x$ itself does not carry any meaning.. the abstraction is beautiful. But here is where I have a doubt - say we have a generating function of the form - $$1+x+x^2+x^3+\cdots$$ Now, in almost every text I have read, this generating function is written as - $$1 + x^2+x^3+ \cdots = \sum_{n=0}^{\infty}x^n =\frac{1}{1-x}$$ I understand that the generating function here is actually a geometric series. And the summation of geometric series is - $$\frac{a(r^n-1)}{r-1}$$ where $a$ is the first element and $r$ is the common ratio. Now, when $n \rightarrow \infty$ , we can say - $$\frac{a(r^{\infty}-1)}{r-1} \approx \frac{a}{1-r}$$ iff $r \in (0,1)$ . Now, my question is, in case of the generating function stated above, $r=x$ and we have already seen that $x$ itself is not defined. So, how can we write that - $$1+x+x^2+x^3+\cdots =\frac{1}{1-x}$$ without knowing anything about $x$ and obviously without making sure that it lies between $(0,1)$ . How can we make assumptions on a variable that is itself undefined? and even if we did make an assumption, where did we prove that our assumption holds good? Can someone please clarify this to me?.. thank you so much..😊","['generating-functions', 'geometric-series', 'polynomials', 'sequences-and-series']"
4120549,We choose 3 numbers from a set. Find probability that the multiple is lower than 8192,"We randomly select three numbers from the set $A = \{1, 2, 4, 8, .., 8192 \}$ . Find the probability that the multiple of chosen numbers is not higher than $8192$ . My understanding: The given set consists of $14$ numbers, each of which is the result of multiplying the previous number by $2$ . Thus the number of ways to choose three numbers from that set is $\dbinom{14}{3}={364}$ Now I need to find the number of favorable ways to choose 3 numbers so that $a \times b\times c \le 8192$ . My question: Is there a way to do this elegantly? My only current solution would be to go manually through all the $ 1\times 2\times 4$ etc possiblities to see which expressions are less than $8192$ . But that does not seem efficient at all especially if we had more numbers.","['combinations', 'combinatorics', 'probability-theory', 'discrete-mathematics']"
4120560,Bounding rows of random matrices,"Consider i.i.d. random variables $\delta_{ij}\sim\text{Ber}(p)$ , where $i,j\in[n]$ . Assuming that $pn\geq \log(n)$ , show that $$\mathbb{E}\max_{i\in[n]}\sum_{j=1}^n(\delta_{ij}-p)^2 \leq Cpn.$$ This is Exercise 6.6.2 from High-Dimensional Probability Book by Roman Vershynin. And here is my try: Since $\sum_{j=1}^n(\delta_{ij}-p)^2$ are subgaussian (they are bounded), $$\mathbb{E}\max_{i\in[n]}\sum_{j=1}^n(\delta_{ij}-p)^2 \lesssim \max_{i\in[n]}\|\sum_{j=1}^n(\delta_{ij}-p)^2\|_{\psi_2}\log^{1/2}(n).$$ Now it boils down to estimating $\|\sum_{j=1}^n(\delta_{ij}-p)^2\|_{\psi_2}$ for each fixed $i$ .   Simple calculation shows $$\sum_{j=1}^n(\delta_{ij}-p)^2 = np^2 + (1-2p)\sum_{j=1}^n \delta_{ij}.$$ And I would like to have the bound for $\|\sum_{j=1}^n(\delta_{ij}-p)^2\|_{\psi_2}$ to be of order $\sqrt{np}$ . However, if I use the inequality $$\|\sum_{j=1}^n(\delta_{ij}-p)^2\|_{\psi_2}^2 \lesssim \sum_{j=1}^n\|(\delta_{ij}-p)^2\|_{\psi_2}^2,$$ then the bound is only of $O(\sqrt{n})$ . So my question is, are the inequalities I use too loose?","['concentration-of-measure', 'probability-distributions', 'probability-theory', 'probability']"
4120569,Is true that a closed $k$-form defined on an open connected set is exact iff its integral on a compact $k$-manifold without boundary is zero?,"So let be $U$ an open connected set of $\Bbb R^n$ and thus let be $\omega$ a $k$ -form there defined. So if $\omega$ is exact than by the Stoke's theorem $$
\int_M\omega=0
$$ where $M$ is a compact $k$ -manifold contained in $U$ . However if $\omega$ is such that the last identity holds for any compact $k$ -manifold contained in $U$ then is it exact? How prove it? Is it much complicate? What I have to study to prove it? Where can I find the proof? So could someone help me, please?","['de-rham-cohomology', 'multivariable-calculus', 'manifolds', 'differential-forms', 'differential-geometry']"
4120594,Is there an ordinal number $x$ such that $\omega^\omega \cdot x=\omega_1$?,"Is there an ordinal number $x$ such that $\omega^\omega \cdot x =\omega_1$ ? My work: After googling, I found this which states the lemma below: Lemma. If $\square$ is any ordinal arithmetic operation, and at least one of $\alpha,\beta$ is infinite, then $|\alpha\square\beta|=\max\{|\alpha|,|\beta|\}$ . I assumed $||$ here means cardinality. So according to this, if $x$ satisfies $\omega^\omega \cdot x =\omega_1$ , then we have $\max\{|\omega^\omega|,|x|\}=|\omega_1|$ , and since $|\omega^\omega|$ is countable, $|x|$ must be uncountable. So I thought substituting $x=\omega_1$ would be a nice start, but I don't know how to calculate $\omega^\omega \cdot \omega_1$ . How can I proceed from here?","['elementary-set-theory', 'ordinals']"
4120617,Finding minima from simultaneous equations,"We are given that a point $(x,y,z)$ in $\mathbb{R}^3$ satisfies the following equations $x\cos\alpha-y\sin\alpha+z =1+\cos\beta$ $x\sin\alpha+y\cos\alpha+z =1-\sin\beta$ $x\cos(\alpha+\beta)-y\sin(\alpha+\beta)+z=2$ Where $\alpha,\beta\in\mathbb(0,2\pi)$ We need to find the Minimum value , $M$ of $x^2+y^2+z^2$ . \begin{align}M&=2\end{align} My attempt: By using Cramers rule, we find that the equations have an unique solution in the angle range mentioned. Solving for $x,y,z$ and then squaring and adding them and then finding the minimum value would take so much time. Is there any easier alternative way to do this?","['inequality', 'systems-of-equations', 'determinant', 'matrices', 'trigonometry']"
4120644,The diameter of Voronoi cells in Euclidean spaces,"Let $A \subset \mathbb{R}^d$ and let $(x_n)_{n \in \mathbb{N}} \subset \mathbb{R}^d$ be a sequence dense in $A$ .
For each $n \in \mathbb{N}$ , let $V_{1,n},\dots,V_{n,n}$ the sequence of Voronoi cells associated to the points $x_1, \dots, x_n$ , where the ties are broken lexicographically, i.e.: $$V_{1,n} =\{x \in \mathbb{R}^d \mid \forall k\in\{2,\dots,n\}, |x-x_1|\le |x-x_k|\} \\
V_{2,n} =\{x \in \mathbb{R}^d \mid |x-x_2|<|x-x_1| \land \forall k\in\{3,\dots,n\}, |x-x_2|\le |x-x_k|\} \\
\vdots \\
V_{n,n} =\{x \in \mathbb{R}^d \mid \forall k\in\{1,\dots,n-1\}, |x-x_n| < |x-x_k|\}
$$ If $x \in \mathbb{R}^d$ and $n \in \mathbb{N}$ , define $V_n(x)$ as the unique element in $V_{1,n},\dots, V_{n,n}$ that contains $x$ . Is it true that $$\forall x \in A, \operatorname{diam}\big(A \cap V_n(x)\big) \to 0, n \to \infty?$$ I suspect that this result should hold due to the finite dimensionality of $\mathbb{R}^d$ , since at least in this case we can obtain bounded sets using a finite number of intersections of half-spaces.
However, it seems quite involved from a geometric point of view to obtain this claim. Has anyone any idea? EDIT: note that we can WLOG assume that $A$ is the closure of the set whose points are those of the sequence $(x_n)_{n \in \mathbb{N}}$ . Some context: I'm trying to prove the aforementioned result to obtain that if $g \colon A \to \mathbb{R}$ is a continuous function, then $$\forall x \in A, \sup_{y \in A \cap V_n(x)} |g(x) - g(y)| \to 0, n \to \infty.$$","['euclidean-geometry', 'geometry', 'geometric-measure-theory', 'real-analysis']"
4120665,Show that the tangent plane to the surface $\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1$,"Show that the tangent plane to the surface $$\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}=1$$ At $(x_0,y_0,z_0)$ is given by $$\frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}=1$$ Then assume $a=b=c=1$ and show the tangent lines to the surface in the intersection with $x=x_0$ construct a cone. ( $x_0 >1$ ). Define $$F(x,y,z)=\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}-\frac{z^{2}}{c^{2}}-1$$ Then the tangent plane to the the surface at the given point is: $$\frac{2x_{0}}{a^{2}}\left(x-x_{0}\right)-\frac{2y_{0}}{b^{2}}\left(y-y_{0}\right)-\frac{2z_{0}}{c^{2}}\left(z-z_{0}\right)=0$$ On the other hand the point is also on the surface so the equation is: $$\frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}-\left(\frac{2x_{0}^{2}}{a^{2}}-\frac{2y_{0}^{2}}{b^{2}}-\frac{2z_{0}^{2}}{c^{2}}\right)=0=0$$ $$\frac{x_{0}x}{a^{2}}-\frac{y_{0}y}{b^{2}}-\frac{z_{0}z}{c^{2}}=1$$ Also if $a=b=c=1$ then the points on the intersection of the surface with $x=x_0$ satisfy the following relation: $$x_0^2-y^2-z^2=1$$ $$z^2+y^2=x_0^2-1\tag{1}$$ Now should I find the tangent lines to $(1)$ ? And show that they construct a cone?","['multivariable-calculus', 'surfaces']"
4120702,Possible proof of Cauchy's Integral Formula for derivatives - completion and verification,"First, let me state Cauchy's Integral formula: Let $U$ be an open region in the complex plane and $D = \{z : |z-z_0| \leq R\}$ a disk in $U$ . If $f : U \to \mathbb C$ is holomorphic and $\gamma$ is the boundary of $D$ , then for all points $a$ inside the disk $$ f(a) = \frac 1{2πi}\oint_\gamma \frac{f(z)}{z-a}\ dz$$ and Cauchy's Integral Formula for the derivatives: $$f^{(n)}(a) = \frac{n!}{2πi}\oint_\gamma \frac{f(z)}{(z-a)^{n+1}}\ dz$$ The majority of books prove the second formula by differentiation of the first formula and then proceeding by induction. However, it seems like Wikipedia wants to suggest a different proof that I couldn't find anywhere else, but they only give hints: Since ${\displaystyle 1/(z-a)}$ can be expanded as a power series in the variable ${\displaystyle a}$ : ${\displaystyle {\frac {1}{z-a}}={\frac {1+{\frac {a}{z}}+\left({\frac {a}{z}}\right)^{2}+\cdots }{z}}}$ — it follows that holomorphic functions are analytic, i.e. they can be expanded as convergent power series. In particular f is actually infinitely differentiable So I took the time to try to understand what they are actually suggesting. What follows is my attempt to turn Wikipedia's hint into a proof, however, it's incomplete because I don't know how to justify the red-marked interchange of integral and series: Let $a$ be a point inside $D$ and $w$ a point in the largest open disk around $a$ contained in $D$ . Then, $|w-a| < |z-a|$ for all $z$ on $\gamma$ , so $\left|\frac{w-a}{z-a}\right|<1$ and then $$\sum_{n=0}^\infty \frac{(w-a)^n}{(z-a)^{n+1}} = \frac{1}{z-a} \frac{1}{1-\frac{w-a}{z-a}} = \frac 1{(z-a)-(w-a)} = \frac 1{z-w} $$ Applying Cauchy's Integral formula: $$\begin{align} f(w) &= \frac 1{2πi}\oint_\gamma \frac{f(z)}{z-w}\ dz \\ &= \frac 1{2πi}\oint_\gamma \sum_{n=0}^\infty f(z)\frac{(w-a)^n}{(z-a)^{n+1}}\ dz \\ &\color{red}{=} \sum_{n=0}^\infty \frac 1{2πi}\oint_\gamma f(z)\frac{(w-a)^n}{(z-a)^{n+1}}\ dz \\ &= \sum_{n=0}^\infty \left(\frac 1{2πi}\oint_\gamma \frac{f(z)}{(z-a)^{n+1}}\ dz\right)(w-a)^n \end{align}$$ but this is a power series in $w$ around $a$ . And we know that power series are infinitely-differentiable in their radius of convergence with $$ f(w) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}(w-a)^n $$ therefore $$ f^{(n)}(a) = \frac{n!}{2πi}\oint_\gamma \frac{f(z)}{(z-a)^{n+1}}\ dz $$ Is the red-marked interchange valid? Apart from the interchange, is everything alright?","['complex-analysis', 'contour-integration', 'solution-verification', 'cauchy-integral-formula']"
4120740,Is a finite measure tight iff it is a Radon measure?,"Let $E$ be a Hausdorff space and $\mu$ be a finite measure on $E$ . Remember that $\mu$ is called tight if for all $\varepsilon>0$ , there is a compact $K\subseteq E$ with $$\mu(K^c)<\varepsilon\tag1;$$ Radon if for all $B\in\mathcal B(E)$ and $\varepsilon>0$ , there is a compact $K\subseteq E$ with $$\mu(B\setminus K)<\varepsilon\tag2.$$ It is easy to verify that $\mu$ is tight iff $$\mu(E)=\sup_{K\subseteq E\text{ is compact}}\mu(K)\tag{1'}$$ and $\mu$ is Radon iff $$\forall B\in\mathcal B(E):\mu(B)=\sup_{\substack{K\subseteq E\text{ is compact}\\ K\subseteq B}}\mu(K)\tag{2'}.$$ Now, on page 11 of Probability in Banach Spaces - Stable and Infinitely Divisible Distributions by Werner Linde , we can find the following paragraph: So, on the one hand he is calling $\mu$ Radon if $(1')$ hold - which is equivalent to my definition of $\mu$ being tight. On the other hand, he is claiming that this already implies $(2')$ - which is equivalent to my definition of $\mu$ being Radon. In summary, it seems like he's claiming that both notions coincide. So, my question is: How do we show that? I don't understand why $(1')$ implies $(2')$ . Let me note that he's assuming that $E$ is a metric space. If that's important, feel free to show the desired claim under this additional assumption. And if the claim is indeed true: Does this extend to families $\mathcal F$ of finite measures on $E$ ? (Remember that in that case $\mu$ in $(1)$ and $(2)$ is replaced by the corresponding supremum over all $\mu\in\mathcal F$ .)","['measure-theory', 'probability-theory']"
4120770,Applying Leibniz's Rule to an Integral With Multiple Parameters,"I was given the following exercise and wasn't really able to make heads-or-tails out of it. It goes like so: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be twice differentiable continuously , and statisfy the following: \begin{align*}
    \quad & \forall (x,y,z) \in \mathbb{R}^3 \\
    (*) \quad & \frac{\partial f}{\partial y}(x,y,z) = \frac{\partial^2 f}{\partial x^2}(x,y,z)
\end{align*} Then define: \begin{gather*}
    g: \mathbb{R}^2 \rightarrow \mathbb{R} \\
    (x,y) \rightarrow \int_0^y f(x,y-z,z)dz
\end{gather*} Prove for all $(x,y) \in \mathbb{R}^2$ \begin{gather*}
\frac{\partial g}{\partial y}(x,y) = \frac{\partial^2 g}{\partial x^2}(x,y) + f(x,0,y)
\end{gather*} I do know this question must involve Leibniz's Rule of Integration Under The Integral Sign , yet I am not sure how this is applicable here.
The only sensible step to take is to fix $g$ for both variables and derive using Leibniz's rule (as we know $g$ is continuously differentiable twice in $\mathbb{R}^3$ ).
So for the second variable: \begin{gather*}
    \frac{\partial g}{\partial y} =  f(y,y,z)\cdot\frac{d}{dy}y + \int_0^y\frac{\partial f}{\partial y}(x,y-z,z)dz
\end{gather*} For the first variable it seemed to me that I should derive twice using Leibniz's rule so I can apply $(*)$ , and I got: \begin{gather*}
\frac{\partial^2 g}{\partial x^2} = \int_0^y\frac{\partial^2 f}{\partial x^2}(x,y-z,z)dz
\end{gather*} We can easily see that by applying $(*)$ we get close to the required term, yet it is not quite there.
I assume my evaluation of $\dfrac{\partial g}{\partial y}$ is incorrect. So first and foremost, how would one comprehensively justify using Leibniz's rule here in the preceding manner - it is not your usual case, there are two parameters in the integral represented by $g$ . Is it even justifiable here? And secondly, did I evaluate $\dfrac{\partial g}{\partial y}$ wrongly, or have I just missed something and the evaluation is incomplete? Any hint or explanation would be extremely appreciated. Thank you so much, and have a great day!","['integration', 'multivariable-calculus', 'leibniz-integral-rule', 'real-analysis']"
4120778,Expressing diagonal matrix using elementary matrices as generators in $\operatorname{SL}(\mathcal O_K \oplus \mathfrak a)$,"Let $K$ be a real quadratic number field, $\mathcal O_K$ its ring of integers and an $\mathfrak a \subset K$ a fractional ideal. I've read in van der Geer that the group $$\operatorname{SL}(\mathcal O_K \oplus \mathfrak a) := \left\{ \begin{pmatrix}a & b \\ c & d\end{pmatrix} \in \operatorname{SL}_2(K) : a,d \in  \mathcal O_K, b \in \mathfrak a^{-1}, c \in \mathfrak a \right\}$$ is generated by matricies of the form $$\pm \begin{pmatrix}1 & b \\ 0 & 1\end{pmatrix},\quad \pm \begin{pmatrix}1 & 0 \\ c & 1\end{pmatrix}.$$ So I tried to express \begin{pmatrix}\varepsilon & 0 \\ 0 & \varepsilon^{-1}\end{pmatrix} for $\varepsilon \in \mathcal O_K^\times$ with those matrices and was successful with an approach I developed using four matrices. This approach relied on a condition on the number field and I was not sure if it's always satisfied so I wrote a program to check that. It turned out that in $K=\mathbb Q(\sqrt{146})$ my approach doesn't work (for all squarefree $d<146$ it works in $K=\mathbb Q(\sqrt{d})$ ). Can anyone find a decomposition for $\varepsilon = 145 + 12\sqrt{146}$ being the fundamental unit and $\mathfrak a$ being any ideal which is not principal (for example $\mathfrak a:=(5,1+\sqrt{146})$ which is a prime ideal over $5$ ). By the way, the class number of $K=\mathbb Q(\sqrt{146})$ is $2$ . If you have a general approach this would help me even more. Edit: A few days after posting this question I could show that there is no way for $d \in \{ 146, 170, 194, 221, 226, 254, 290, 291, 323, 326, 365, 386, 399, 410, 434, 439, 442, 445, 485, 499, 506, 514, 530, 533, 574, 579, 582, 646, 674, 706, 723, 730, 731, 785, 786, 791, 799, 839, 842, 866, 870, 890, 898, 899, 901, 910, 914, 959, 962, 965, 970, 982, 986 \}$ to do it for all units with only four matrices. This is due to how $(\varepsilon \pm 1)$ decomposes into prime ideals (and to which ideal classes they belong). So if van der Geer is right we need here at least five matrices and I'm very curious to see how this works.","['matrices', 'number-theory', 'algebraic-number-theory', 'matrix-decomposition']"
4120781,$\int_0^1\frac{\ln x\ln^2(1-x^2)}{\sqrt{1-x^2}}dx=\frac{\pi}{2}\zeta(3)-2\pi\ln^32$,"I'm looking for proof of the following identity $$\int_0^1\frac{\ln x\ln^2(1-x^2)}{\sqrt{1-x^2}}dx=\frac{\pi}{2}\zeta(3)-2\pi\ln^32$$ I have worked on this problem for quite some time, however since I'm not much comfortable with beta functions and stuff, I was unable to prove the required. I'm looking for an elementary approach, however, any detailed method (including beta function) is most welcomed. Thanks.","['integration', 'definite-integrals', 'logarithms', 'sequences-and-series', 'riemann-zeta']"
4120834,A die was thrown three times. Find probability that first throw is less than the second and second is less than the third throw.,"My proposed solution follows. Is this correct? If yes, I'd like to know if there is a better way to solve this problem. We can see that the value of the first throw could only be $1, 2, 3$ or $4$ . Throws are labeled as $T_1, T_2, T_3$ . If $T_1$ is $1$ then $T_2$ , $T_3 \in \{2,3,4,5,6\}$ If $T_1$ is $2$ then $T_2$ , $T_3 \in \{3,4,5,6\}$ If $T_1$ is $3$ then $T_2$ , $T_3 \in \{4,5,6\}$ If $T_1$ is $4$ then $T_2$ , $T_3 \in \{5,6\}$ Hence the number of favorable outcomes is $\dbinom{2}{2}+\dbinom{3}{2}+\dbinom{4}{2}+\dbinom{5}{2} = 20$ And $P(A) = \frac{20}{6\times6\times6}=\frac{5}{54}$","['combinatorics', 'probability-theory']"
4120844,How did nuclear spaces come about?,"I researched a lot what the point of nuclear spaces is. From what I understand they were invented by Grothendieck to make a more general statement for the Kernel Theorem by Schwartz. He figured out that the Theorem holds more generally if the respective spaces are nuclear. Now a nuclear space can be defined by saying that the projective tensor product coincides with the injective tensor product. I wanted to figure out where exactly in the proof or which part of it really depends on the nuclearity. More generally I wanted to understand how Grothendieck came up with the idea that the spaces have to be nuclear. I also can't really find the proof of the theorem, I looked in Grothendiecks doctoral thesis but it is french so I am not sure which part it is. I hope there is someone here that could shed some light on this. Thanks you!","['topological-vector-spaces', 'functional-analysis', 'distribution-theory']"
4120935,Modification of Lagrange’s Remainder Theorem to calculate $\ln 2$,"The following question is from Stephen Abbott's ""Understanding Analysis."" Question: Explain how Lagrange’s Remainder Theorem can be modified to prove $$
1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4} \cdots = \ln2.
$$ A question close to mine was asked here but does not address my concern. The usual procedure used to solve such problems is to first show that the power series $f(x) = \sum_{k=1}^\infty \frac{(-1)^{k-1}x^{k}}{k}$ can be derived from that of $1/(1+x)$ so that the region of convergence for $f(x)$ would be $(-1,1)$ , then to show that $f(x)$ converges at $x=1$ (using alternating series test in this case).  Abel's theorem would imply that $f(x)$ converges uniformly on $[0,1]$ and continuous limit theorem would imply that $f(x)$ is continuous at $x=1$ . Finally, as $f(x) = \ln(1+x)$ on $(-1,1)$ , we can conclude that $\ln(2) = 1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4} \cdots$ The question however asks us to modify Lagrange’s Remainder Theorem. Following are two statements, the first of which is Lagrange’s Remainder Theorem as given in the text, and the second is the one that I have modified. Please let me know if second statement is correct and if not, please give me a hint as to where I am going wrong. Lagrange’s Remainder Theorem (as given in the text): Let $f$ be differentiable $N + 1$ times on $(−R,R)$ . Define $a_n = f^{(n)}(0)/n!$ for $n = 0,1,\cdots,N$ , and let $$
S_N(x) = a_0 +a_1x+a_2x^2 +\cdots+a_Nx^N.
$$ Given $x\neq 0$ in $(−R,R)$ , there exists a point $c$ satisfying $|c| < |x|$ where the error function $E_N(x) = f(x) − S_N(x)$ satisfies $$
E_N(x) = \frac{f^{(N+1)}(c)x^{N+1}}{(N + 1)!}.
$$ Modified Lagrange’s Remainder Theorem: Let $f$ be differentiable $N + 1$ times on $(−R,R)$ , be continuous at $x=R$ , and let the Taylor series of $f$ be convergent at $x=R$ ( $R>0$ ). Define $a_n = f^{(n)}(0)/n!$ for $n = 0,1,\cdots,N$ , and let $$S_N(x) = a_0 +a_1x+a_2x^2 +\cdots+a_Nx^N.$$ There exists a point $c$ satisfying $|c| < R$ where the error function $E_N(x) = f(x) − S_N(x)$ satisfies $$
E_N(R) = \frac{f^{(N+1)}(c)R^{N+1}}{(N + 1)!}.
$$ The proof basically runs along the same lines as the first approach that I have outlined above which uses Abel's Theorem, along with the standard proof for Lagrange’s Remainder Theorem.","['continuity', 'derivatives', 'taylor-expansion', 'real-analysis']"
4120937,Pre-dual of the measure space $\mathcal{M}(X)$,"I have to find the 'pre-dual of the measure space $\mathcal{M}(X)$ '. $X$ can be assumed to be Polish and equipped with the Borel $\sigma$ algebra. This is all I'm given and it's a bit vague. What I found so far is this:
if $X$ is locally compact and Hausdorff, then any element in the dual of $C_c(X)$ (the space of continuous compactly supported complex-valued functions on $X$ ) corresponds to a unique regular Borel measure on $X$ , i.e. for $\psi \in (C_c(X))'$ , we have $$\psi(f)=\int_X f(x) \,d\mu(x)$$ for such a measure $\mu$ . But do we 'hit' all such measures, i.e. can we really identify the dual of $C_c(X)$ with the space of regular Borel measures? Would the mapping $\psi\mapsto\mu$ only be bijective, or can we make it, say, into an isomorphism (using the total variation norm)? Is there another space whose dual is the space of all Borel measures on $X$ , without making the assumptions that $X$ is locally compact and Hausdorff? (Also, if you know good literature on this please let me know, I was having a hard time finding something.)
(Edit: for anyone interested (and German speaking): Funktionalanalysis by Dirk Werner seems to cover the topic well.)","['borel-measures', 'measure-theory', 'functional-analysis', 'dual-spaces']"
4120960,"Solving for the derivative of absolute value, gone wrong","The absolute value $|x|$ can be represented as $\sqrt{x^{2}}$ , as per this question . Let $f(x) = |x| = \sqrt{x^{2}}$ . Solving for $f'(x)$ , let $u = x^{2}$ . Then, \begin{align*}\frac{df}{dx} &= \frac{df}{du}\cdot\frac{du}{dx} \\ &= \frac{d}{du}(\sqrt{u})\cdot\frac{d}{dx}(x^{2}) \\ &=\frac{1}{2\sqrt{u}}\cdot2x \\ &= \frac{x}{\sqrt{x^{2}}} \\ &= \frac{x}{|x|}\end{align*} We can also see that $\sqrt{x^{2}} = \left(\sqrt{x}\right)^{2}$ . Then, let $u = \sqrt{x}$ . Solving for $f'(x)$ , \begin{align*}\frac{df}{dx} &= \frac{df}{du}\cdot\frac{du}{dx} \\ &= \frac{d}{du}(u^{2})\cdot\frac{d}{dx}(\sqrt{x}) \\ &= 2u\cdot \frac{1}{2\sqrt{x}} \\ &= \frac{\sqrt{x}}{\sqrt{x}} \\ &= 1\end{align*} I think the problem here is by letting $u = \sqrt{x}$ . What seems to be the problem?","['calculus', 'derivatives', 'absolute-value']"
4120965,Does smoothness of left and right multiplication imply smoothness of multiplication?,"A smooth manifold $G$ (of dimension $m$ ) with a group structure given by a multiplication map $\mu: G \times G \to G, \, (g,h) \mapsto gh$ is called a Lie group if both the multiplication $\mu$ and the inversion $\iota:G \to G, \, g \mapsto g^{-1}$ are smooth maps. It is well-known that smoothness of the multiplication already implies the smoothness of the inversion, which is proven via the inverse function theorem. Smoothness of $\mu$ moreover implies that the left and right multiplication maps $$L_g: G \to G, \, h \mapsto gh  \, \text{ and } \, R_g: G \to G, \, h \mapsto hg^{-1} \quad (g \in G)$$ are smooth by pre-composing $\mu$ with suitable smooth maps $G \to G \times G$ . My question is about the converse of the previous statement: Does smoothness of the maps $L_g$ and $R_g$ for all $g \in G$ also imply that $G$ is a Lie group, i.e. that the multiplication $\mu$ is smooth as well? If yes, does it also suffice to assume that $L_g$ is smooth for every $g \in G$ ?","['topological-groups', 'smooth-manifolds', 'differential-topology', 'lie-groups', 'differential-geometry']"
4121050,"Holder-continuity of the fractional Brownian motion in a compact $[0,T]$ and dependence in $T$","I know that the fractional Brownian motion has a Holder-continuous version (thanks to Kolmogorov's continuity theorem for example). Basically, for any $T>0$ , $\epsilon>0$ and $t,s \in [0,T]$ , there exists a random variable, say $C_T$ such that: $$ \mid B_t - B_s \mid \leq C_T \mid t - s \mid^{H-\epsilon}  \text{        a.s}$$ Say $H$ lives in a compact set of $(0,1)$ and therefore we can make the constant $C_T$ independent of $H$ . I am interested in the dependence of $C_T$ with respect to $T$ . I realize that it shouldn't grow too fast with $T$ but I couldn't find somewhere where it is explicitly given. On the other hand, I am also interested in the regularity of $B$ with respect to $H$ . That is with similar arguments (Kolmogorov), one can show that for $t \in [0,T]$ and $H_1,H_2$ in a compact set of $(0,1)$ , we have: $$\mid B_t^{H_1}- B_t^{H_2} \mid \leq C_t \mid H_1 - H_2 \mid^\alpha \text{        a.s}$$ But again, how does $C_t$ depends on $T$ ?","['stochastic-processes', 'gaussian-measure', 'brownian-motion', 'probability-theory']"
4121052,subgroup contains normal subgroup as normal subgroup and is also normal,"I'm currently working on the following problem and I'm stuck: Let $K \trianglelefteq G$ and let $\bar H \leq G/K$ . Let $\pi: G \rightarrow G/K$ denote the quotient map $g \mapsto gK$ . Show that $$H=\pi^{-1}(\bar H)=\{g \in G:gK \in \bar H\}$$ is a subgroup of $G$ , containing $K$ as a normal subgroup, with $H/K=\bar H$ . Show further that if $\bar H \trianglelefteq G/K$ then $H \trianglelefteq G$ . I managed the first parts (please tell me if I made a mistake): Claim: $H$ is a subgroup. Proof: $H$ is clearly a subset of $G$ , for a subgroup we also need: identity: $e \in G$ and $eK=K\in \bar H$ thus $e \in H$ . inverses: take $g \in H$ . Then $gK \in \bar H$ and $(gK)^{-1}=K^{-1}g^{-1}=Kg^{-1}=g^{-1}K \in \bar H$ thus $g^{-1} \in H$ . closure: take $g_1,g_2 \in H$ . Then $g_1K, g_2K \in \bar H$ and $g_1Kg_2K=g_1g_2K \in \bar H$ thus $g_1g_2 \in H$ . Claim: $K$ is contained in $H$ . Proof: Since $e \in H$ , we have $k \in K: kK=K=eK \in \bar H$ . Thus $k \in H$ for all $k$ . Now I'm stuck. Thank you for your help!","['quotient-group', 'normal-subgroups', 'group-theory', 'abstract-algebra']"
4121082,"Maximizing $x^2y$ given $x^2+y^2=100$, without using the AM-GM inequality and calculus tools","Problem says: Let $x^2+y^2=100$ , where $x,y>0$ . For which ratio of $x$ to $y$ , the value of $x^2y$ will be maximum? I know these possible tools: AM-GM inequality Calculus tools Here, I want to escape from all of the tools I mentioned above. I will try to explain my attempts in the simplest sentences. (my english is not enough, unfortunately). I will not prove any strong theorem and also I'm not sure what I'm doing exactly matches the math, rigorously. Solution I made: First, it is not necessary to make these substitutions.  I'm just doing this to work with smaller numbers. Let, $x=10m, ~y=10n$ , where $0<m<1,~ 0<n<1$ , then we have $$ x^2+y^2=100 \iff m^2+n^2=1$$ $$x^2y=1000m^2n$$ This means, $$\max\left\{x^2y\right\}=10^3\max\left\{m^2n\right\}$$ $$m^2n=n(1-n^2)=n-n^3$$ Then suppose that, $$\begin{align}\max\left\{n-n^3 \mid 0<n<1\right\}&=a, a>0&\end{align}$$ This implies $$n-n^3-a≤0,~ \forall n\in\mathbb (0,1)$$ $$n^3-n+a≥0,~\forall n\in\mathbb (0,1)$$ Then, we observe that $$\begin{align}n^3-n+a≥0, \forall n\in (0,1) ~ \text{and} ~ \forall n≥1\end{align}$$ This follows $$ n^3-n+a≥0, ~ \forall n>0.$$ Using the last conclusion, I assume  that there exist $u,v>0$ , such that $$n^3-n+a=(n-u)^2(n+v)≥0.$$ If $n>0$ , then the equality occurs, if and only if $$n=u>0$$ Based on these, we have: $$\begin{align}n^3-n+a= (n-u)^2(n+v)≥0 \end{align}$$ $$\begin{align}n^3-n+a = & n^3 - n^2(2u-v)+ n(u^2 - 2 u v ) + u^2v & \end{align}$$ $$\begin{align} \begin{cases} 2u-v=0 \\ u^2-2uv=-1 \\u^2v=a \\u,v>0 \end{cases} &\implies \begin{cases} v=2u \\ u^2-4u^2=-1 \\ 2u^3=a \\ u,v>0 \end{cases}\\
&\implies \begin{cases}  u=\frac{\sqrt 3}{3} \\ v=\frac{2\sqrt 3}{3}\\ a=2\left(\frac{\sqrt 3}{3} \right)^3=\frac{2\sqrt 3}{9} \end{cases} \end{align}$$ $$\begin{align}n^3-n+\frac{2\sqrt 3}{9} &=\left(n-\frac{\sqrt 3}{3} \right)^2\left(n+\frac{2\sqrt 3}{3}\right)≥0.&\end{align}$$ As a result, we deduce that $$\begin{align}n-n^3-\frac{2\sqrt 3}{9} &=-\left(n-\frac{\sqrt 3}{3} \right)^2\left(n+\frac{2\sqrt 3}{3}\right)≤0, &\forall n\in (0,1).&\end{align}$$ $$\begin{align}\max\left\{n-n^3 \mid 0<n<1\right\}&=\frac{2\sqrt 3}{9}, ~ \text{at }~ n=\frac{\sqrt 3}{3}&\end{align}$$ Finally, we obtain $$m=\sqrt{1-n^2}=\sqrt{1-\frac 13}=\frac{\sqrt 6}{3}$$ $$\frac xy=\frac mn=\sqrt 2.$$ Question: How much of the things I've done here are correct?","['proof-writing', 'maxima-minima', 'solution-verification', 'optimization', 'algebra-precalculus']"
4121084,Help Understanding a Lemma: if $\exists X$ such that $X=A \backslash g(B \backslash f(X))$ then there exists a bijection from $A$ to $B$,"I had a Lemma in one of my algebraic structure class that I was never really able to grasp the use of.
We used it to prove Cantor-Schröder-Bernstein 's theorem, but I never used it after that, nor remember the teacher using it.
I'd love to have some help. If anyone could provide an example (intuitive or use), I'd be very grateful. The Lemma: Let $f:A\rightarrow B$ and $g:B\rightarrow A$ two injections. If there exists a subset $X\subseteq A$ such that $X=A \backslash g(B \backslash f(X))$ , then there exists a bijection $A\rightarrow B$ . Thank you.","['elementary-set-theory', 'proof-explanation']"
4121086,Two variables limit that confused my intuition and can't figure out why my intuition is wrong.,"Background on how I got the intuition: I have recently learnt about two variable limits, and my professor gave us a tip that whenever we have two homogenous polynomials in the denominator and numerator, with $(0,0)$ being the only problematic point, then we can decide if the limit exists and equal to zero or DNE, based on the powers. and we got some examples: $\lim_{(x,y)\to (0,0)}\frac{xy^2}{x^2+y^4}$ , DNE because the power in the denominator is $4$ and numerator $3$ . $\lim_{(x,y)\to (0,0)}\frac{xy^2}{x^2+y^2}=0$ , because the power in the numerator is bigger than the denominator. Basically if the power of the numerator is higher than the power of the denominator, the limit exists and equal zero, else it DNE. The Limit that confused me: $\lim_{(x,y)\to (0,0)}\frac{x^4y^13}{x^8+y^{18}}=0$ , but power in denominator is $18$ and power in numerator is $17$ , so based on the tip he told us, the limit shouldn't exist.. (and I can see that $(0,0)$ is the only problematic point in the denominator). What am I missing?","['limits', 'multivariable-calculus']"
4121089,To prove it is a Schauder basis?,"In Classical Banach Spaces I and II by Lindenstrauss and Tzafriri
proposition 1.a.3
Let $\{x_n\}_{n=1}^\infty$ be a sequence of vectors in $X.$ Then $\{x_n\}_{n=1}^\infty$ is a Schauder basis of $X$ if and only if the following three conditions hold. $x_n \neq 0$ for all $n.$ There is a constant $K$ so that, for every choice of scalars $\{a_i\}_{i=1}^\infty$ and integers $n <m,$ we have $$\|\sum_{i=1}^n a_i x_i\| \leq K \|\sum_{i=1}^m a_i x_i\|.$$ The closed linear span of $\{x_n\}_{n=1}^\infty$ is all of $X.$ Then in the rest of the book to show that say $\{x_i\}_{i=1}^\infty$ is a Schauder basis of a Banach space X, he always say by proposition 1.a.3 we have to show that the operators $\{P_n\}_{n=1}^\infty,$ defined by $P_n x = \sum_{i=1}^n x_i^*(x) x_i,$ are uniformly bounded Q1 How proposition 1.a.3 enables us to do that?
Any help will be appreciated","['schauder-basis', 'analysis', 'real-analysis']"
4121090,Deriving the closed form for $\sum_{n=-\infty}^{\infty} \tan^{-1} (an+b) $,"I saw this amazing identity elsewhere : $$ \bbox[5px,border:2px solid #C0A000]{\sum_{n\in\mathbb Z} \tan^{-1} (an+b) =\lim_{N\to \infty} \sum_{-N}^N \tan^{-1} (an+b)  = \tan^{-1} \left( \tan\frac{b\pi}{a} \cdot \coth \frac{\pi}{a} \right) +\pi\text{sgn}(a)\bigg( \text{ceil}\left(\frac ba+\frac 12\right) -1 \bigg)}$$ for $a\ne 0$ . Here, the principal branch of $\tan^{-1} x$ is taken, i.e. $\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ . I feel like proving this involves the use of Euler’s sin product in some way. I tried integrating w.r.t $r$ , the identity $$\sum_{\mathbb Z} \frac{1}{(n+r)^2 +x^2}=\frac{\pi\sinh(2\pi x)}{x(\cosh(2\pi x) -\cos(2\pi r))} $$ using the Weierstrass substitution to get $$\sum_{\mathbb Z}\tan^{-1} \left( \frac{n+r}{x} \right) =\tan^{-1} \left( \tan\pi r \cdot \coth \pi x \right) +C $$ I could then change variables, $\frac 1x \mapsto a$ and $\frac rx \mapsto b $ . The problem is with the $+C$ : it should equal the second term of the RHS, but I don’t know how I could show that. Any ideas for a proof?","['integration', 'inverse-function', 'calculus', 'sequences-and-series', 'trigonometry']"
4121129,Squaring a complex exponential that represents a real number,"Often, complex exponential functions are used to represent trigonometric functions, since $$
e^{i\theta} \equiv \cos\theta + i\sin\theta .
$$ Thus, if for example I want to express the quantity $\cos x$ , I might write: $$
\cos x \equiv \Re\left\{e^{i x}\right\} .
$$ I'm told that I can manipulate the LHS just like I would the RHS, and at the end just take the real part to get the same answer as other methods, but I have come across some trouble. Let's say I wanted to square the LHS to get $\cos^2 x$ . On the RHS, this would give me: $$
\begin{align}
e^{2ix} &= (\cos x + i \sin x)^2 \\
         &= (\cos^2x - \sin^2 x + 2i\cos x \sin x) \\
\implies \Re\{e^{2ix}\}         &= \cos^2 x - \sin^2 x
\end{align}
$$ Now, of course I recognise that the RHS is the identity for $\cos 2x$ , which makes complete sense since $e^{2ix} \equiv e^{i(2x)}$ . My question then is, why do the rules suddenly break down as soon as I attempt to square my complex exponential as I would my trig function? And what are the most conventional steps to take to work around this? Many thanks.","['trigonometry', 'exponential-function', 'complex-numbers']"
4121146,Difficulty in computing the full conditional for a Probit model (Hoff 6.3),"In a course of Bayesian statistics, I have been given an excercise from Hoff, about the computations of some full conditional for the Probit model. I have not great confidence with conditional probability computation.
So I would like to ask if the following resoning is correct. Thanks in advance. Consider the following Probit model: $$Z_i=\beta x_i+\epsilon_i\qquad\epsilon_1,\dots,\epsilon_n\sim_{iid}\mathcal{N}(0,1)$$ $$Y_i=\mathbb{1}_{(c,+\infty)}(Z_i)$$ where: $\beta$ , $c$ unknown parameters, for which we assume priors $\beta\sim\mathcal{N}(0,\sigma_\beta^2)$ , $c\sim\mathcal{N}(0,\sigma_c^2)$ the covariates $x_i$ are known and hence tretated as constants We have to derive $p(\beta\mid z_{1:n}, y_{1:n}, c)$ . Clearly, $c$ and $y_{1:n}$ bring no information on $\beta$ . Then: $$p(\beta\mid z_{1:n},y_{1:n}, c)=p(\beta\mid z_{1:n})\propto p(z_{1:n}\mid\beta)p(\beta)$$ $$\propto\mathtt{exp}(-\frac{1}{2}\{\sum_{i=1}^{n}(z_i-\beta x_i)^2+(\beta/\sigma_\beta)^2\})
                \propto \mathtt{exp}[-\frac{1}{2}(\frac{\beta-\tilde{\mu}_\beta}{\tilde{\sigma}_\beta})^2]$$ where: $$\tilde{\sigma}_\beta^2=\left(\sum_{i=1}^n x_i^2+\frac{1}{\sigma^2_\beta}\right)^{-1}\qquad\tilde{\mu}_\beta=\tilde{\sigma}_\beta\left(\sum_{i=1}^n x_iz_i\right)$$ Hence, we conclude $p(\beta\mid z_{1:n}, y_{1:n}, c)\sim\mathcal{N}(\tilde{\mu}_\beta,\tilde{\sigma}_\beta)$ Question 1 Is this computation correct? We have to derive $p(c\mid z_{1:n}, y_{1:n}, \beta)$ Clearly, $\beta$ brings no information on $c$ . Moreover, observe that $p(c\mid z_{1:n})=p(c)$ , since $z_{1:n}$ alone brings no information on $c$ .
Then: $$ p(c\mid\beta,z_{1:n},y_{1:n})=p(c\mid z_{1:n},y_{1:n})\propto p(y_{1:n}\mid c,z_{1:n})p(c\mid z_{1:n})=p(y_{1:n}\mid c,z_{1:n})p(c)$$ Now, observe that $y_i\mid c,z_i$ is not random any more. In particular it is one if $z_i>c$ and it is $0$ if $z_i\leq c$ . Then we may write, observing that $z_{-i}$ brings no information about $y_i$ : $$p(y_i\mid c,z_{1:n})=p(y_i\mid  c,z_i)=\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}$$ Then: $$p(y_{1:n}\mid c,z_{1:n})=\prod_{i=1}^{n}\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}$$ And in turn: $$p(c\mid\beta,z_{1:n},y_{1:n})\propto\left(\prod_{i=1}^{n}\mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}\right)\mathcal{N}(0,\sigma_c^2)$$ Question 2 According to a given hint,parenthesis should reduce to the charachterstic function of an interval, so that we have a truncated gaussian. But I cannot see how it can. can someone help? We have to derive $p(z_i\mid z_{-i}, y_{1:n}, \beta, c)$ . Clearly, $z_{-i}$ brings no information on $z_{-i}$ , similarly for $y_{-i}$ . $$p(z_i\mid y_{i},\beta,c)\propto p(y_i\mid z_{i},\beta,c)p(c)
                    \propto p(y_i\mid z_{i},c)p(c)
                    \propto \mathbb{1}_{(z_i>c)}^{y_i}\mathbb{1}_{(z_i\leq c)}^{1-y_i}\mathcal{N}(0,\sigma_c^2)$$ Question 3 Is this computation correct?","['conditional-probability', 'statistics', 'bayesian', 'probability']"
4121160,Measure $\mu$ is 0.,"I am reading the paper of Balazard ,Saias and Yor. Let, $$f(z)=(s-1)\zeta(s) $$ where $s=\frac{1}{1-z}$ and $\zeta(s)$ denotes the Riemann zeta function. Denote by $$\exp\left[\int_{-\pi}^{\pi}\frac{e^{i\theta}+z}{e^{i\theta}-z}d\mu(\theta)\right]$$ the singular interior factor of $f.$ Denote by $$f^*(e^{i\theta})=\lim_{r\to 1^-}f(re^{i\theta})$$ Then by Generalised Jensen's formula $$\frac{1}{2\pi}\int_{-\pi}^{\pi} \log|f^*(e^{i\theta})|d\theta=\log|f(0)|+\sum_{|\alpha|<1,f(\alpha)=0}\log\frac{1}{|\alpha|}+\int_{-\pi}^{\pi}d\mu(\theta)$$ The measure $\mu$ associated to the singular interior factor of $f$ is 0. For this it suffices to reuse the argument developed by Bercovici and Foias for the interior factor of the functions $(\theta-\theta^s)\zeta(s)\frac{(s+\frac{1}{2})}{s}$ Question How does the measure $\mu$ associated to the singular interior factor of $f$ is $0$ ?.","['measure-theory', 'complex-analysis', 'analytic-number-theory', 'riemann-zeta', 'complex-numbers']"
4121168,Stirling number relation,"Let $S(k,n)$ denote the Stirling number, i.e. the number of ways to partition $k$ distinguishable objects into $n$ indistinguishable blocks. Then consider $S(n+r,n)$ for some fixed $r$ , how can I show that $S(n+r,n)$ is a polynomial in $n$ of degree $2r$ ?","['combinatorics', 'stirling-numbers']"
4121213,Fulton's Algebraic Curves Exercise 8.13,"This exercise is from Fulton's Algebraic Curves, exercise 8.13, page 101. The exercise reads as follows Suppose $l(D)>0$ and let $f \neq 0, f \in L(D)$ . Show that $f \notin L(D-P)$ for all but a finite number of $P$ . So $l(D-P)=l(D)-1$ for all but a finite number of $P$ . Here $L(D)$ represents the linear system (or series if you prefer) of some divisor $D$ , while $l(D)$ denotes the dimension of this vector space. My Attempt As $l(D) > 0$ , without loss of generality we may assume that $D$ is effective, so we can write $D = \sum_{Q \in I} n_QQ $ where $I$ is some finite set of points and all of the $n_Q > 0$ . My guess is that we require $P \in I$ for the proposition to hold. Suppose instead that $P \notin I$ . Now for $f \in L(D)$ we see that $val_P(f) \geq 0$ , while for $g \in L(D-P)$ , we need $val_P(g) \geq 1$ . Additionally, if $P \in I$ , we have $val_P(f) \geq -n_P$ and for any $g \in L(D-P)$ we have $val_P(g) \geq 1-n_P$ . In both of these cases it seems to me that there could be some $f$ which could be an element of $L(D-P)$ for any chosen $P$ . Could someone show me what I'm missing with this problem, I feel like it should be straightforward but I can't figure out what to try next.","['algebraic-curves', 'divisors-algebraic-geometry', 'algebraic-geometry']"
4121228,"Given $|f(x,y)|\le \sin^2(x^2+y^2)$. Is $f$ differentiable at $(0,0)$?","Knowing that $|f(x,y)|\le \sin^2(x^2+y^2)$ in all $\mathbb{R}^2$ , then $f$ is differentiable in $(0,0)$ . I don't know how to approach this question, I have learnt recently the definition of differentiability for two variable functions, which is (for point $(0,0)$ ): $f(0+\Delta x,0+ \Delta y)-f(0,0)=f_x(0,0)\Delta x+f_y(0,0)\Delta y+\epsilon\sqrt{(\Delta x)^2+(\Delta y)^2}$ , and if $\epsilon \to 0$ when $\Delta x,\Delta y \to0$ then we say $f$ is differentiable. will the definition of differentiability help me here? I'm finding it extremely hard to find counter examples and check them, so if theres any trick I would love to hear. I would appreciate any hints and pushes in the right direction.","['multivariable-calculus', 'derivatives']"
4121229,$\int_0^1 e^{g(s)-1}ds $ where $g(s) = \sum\limits_{i\geq 1} \frac{ s^i i^{i-1} e^{-i}}{i!}.$,"I am trying to evaluate the following integral $$ I:= \int_0^1  e^{g(s)-1}ds $$ where $$g(s) = \sum\limits_{i\geq 1} \frac{ s^i i^{i-1} e^{-i}}{i!}.$$ I am pretty much stuck completely... In particular, I am not used to working with sums that include the nasty $i^{i-1}$ term. I did check that the sum $g(s)$ indeed converges for $|s|\leq 1$ , but I am not sure what it converges to. From other considerations, I know that $I = \int_0^1 \frac{g(s)}{s} ds$ . Now, modulo some arguments for the exchanging of sums and integrations, we then have $$  I = \sum\limits_{i\geq 1} \int_0^1  \frac{s^{i-1} i^{i-1}e^{-i}}{i!}ds = \sum\limits_{i\geq 1} \left [ \frac{s^{i} i^{i-2}e^{-i}}{i!} \right]_0^1 = \sum\limits_{i\geq 1} \frac{s^{i} i^{i-2}e^{-i}}{i!}  .$$ But I still have no idea how so rewrite the right hand side. I did numerically approximate both expressions for the integral and the last sum, and they all roughly equal $\frac{1}{2}$ , so I think that is what the answer should be. Any help will be appreciated. Note: I have shown the recurrence $g(s)=s e^{g(s)-1}$ , which is how I obtained the different integral forms. Perhaps repeated applications of this relation might be useful, but I am not so sure how. Especially because the relation $x=e^x$ is already a tricky one. Potential progress: Let $w = g(s)-1 = se^{w} - 1$ , so that $s=(w+1)e^{-w}$ . Then $\frac{dw}{ds} =e^w + s \frac{dw}{ds} e^w = e^w +(w+1) \frac{dw}{ds} $ giving $\frac{dw}{ds} = -e^{w}/w$ . Then the integral becomes something like $$ - \int \frac{e^{2w}}{w} dw $$","['integration', 'calculus']"
4121236,An étale morphism that restricts to an isomorphism on a closed subvariety.,"If an étale morphism $f:X\rightarrow Y$ induces an isomorphism $f:f^{-1}(Z)\rightarrow Z$ for some closed subvariety $Z$ of $Y$ . Doesn't it imply that $f$ is an isomorphism (I believe the answer should be negative because it is assumed to be so in a paper, but cannot come up with an example.)","['etale-cohomology', 'algebraic-geometry', 'schemes']"
4121260,Prove that $v=vv^*v$ for a partial isometry $v$.,"Let $A$ be a $C^*$ -algebra and let $v$ be a partial isometry so that $v^*v$ is a projection. Show that $v=vv^*v$ . The hint in the book recommended setting $z=(1-vv^*)v$ and then computing $z^*z$ . I recognise that we are likely trying to show $z^*z=0$ to prove that $z=0$ , however I am not sure how to show this. edit: Using a suggestion bellow I was able to get this \begin{align*}
    z^*z&=v^*(1-vv^*)^*(1-vv^*)v\\
    &=v^*(1-v^*v)(1-vv^*)v\\
    &=v^*(1-vv^*-v^*v+v^*vvv^*)v\\
    &=(v^*-v^*vv^*-v^*v^*v+v^*v^*vvv^*)v\\
    &=v^*v-v^*vv^*v-v^*v^*vv+v^*v^*vvv^*v\\
    &=v^*v-(v^*v)^2-v^*v^*vv+v^*v^*vvv^*v\\
    &=-v^*v^*vv+v^*v^*vvv^*v\\
    &=v^*v^*v(vv^*v-v)
\end{align*} But I am unsure of how to proceed.","['k-theory', 'c-star-algebras', 'functional-analysis', 'operator-algebras']"
4121265,Show that $0$ is the only distribution solution of $u''-u=0$ in $\mathcal{S}'(\mathbb{R})$,"Show that $0$ is the only distribution in $\mathcal{S}'(\mathbb{R})$ that satisfies $u''-u=0$ . So, I have that for each $\varphi\in\mathcal{S}(\mathbb{R})$ $$0=\langle u''-u,\varphi\rangle = \langle u,\varphi''-\varphi\rangle.$$ Any hints on how to proceed?","['schwartz-space', 'ordinary-differential-equations', 'distribution-theory']"
4121305,Convergence of Stationary random variables,"We have a stationary sequence of random variables $X_{j}:j\geq 0$ and let $D$ be a Borel subset of $\mathbb{R}^{d}$ . For each n, let $Y_{n}$ be the number of indices $i \in \{0,1, \ldots, n-d\}$ such that $(X_{i+1}, X_{i+2}, \ldots X_{i+d}) \in D$ . Show that $\frac{Y_{n}}{n}$ converges $as$ . My attempt: Attempt 1: $Y_{n}=i$ has equal probability to be anything from the set ${0,1, \ldots, n-d}$ so we can say that $Y_{n}$ follows a uniform distribution with $P(Y_{n}=i) = \frac{1}{n-d+1}$ . Now, I know that I can if I can show that $Y_{n}$ is a subadditive process, stationary, and integrable, then I can use the sub-additive ergodic theorem to prove that it converges $as$ . I don't know to prove that $Y_{n}$ is integrable because nothing has been given about the random variables $X_{j}$ except that they form a stationary sequence. Attempt 2: Since nothing's given about the integrability, if I can prove that the transformation is ergodic then I can use the Birkhoff ergodic theorem to show that $Y_{n}/n$ converges $as$ . For a simple example, if I fix d=1, then for different values of n, the different values of $Y_{n}$ correspond to shifted set of random variables belong to $A$ . For $n=1$ , $i = {0,1}$ which corresponds to $(X_{1}) \in D$ or $(X_{2}) \in D$ but both of them have the same distribution because $X_{j}$ is a stationary sequence. But I cannot go further. Any help would be great.","['measure-theory', 'ergodic-theory', 'stationary-processes', 'almost-everywhere']"
4121326,Modular forms of weight 0 mod p-1,"I'm studying Serre's ""Formes modulaires et fonctions zêta p-adiques"". There is a point which is not at all clear to me. He says that the algebra $\widetilde{M}^0$ of modular forms mod $p$ of weight congruent to $0$ mod $p-1$ is ""l'algebre affine d'une courbe lisse"". I have not found the definition of affine algebra of a curve, but I think - meaning I'm not sure - he means the coordinate ring, over $\mathbb{F}_p$ , of such curve. Even so, it is not clear to me which curve he is talking about, although I recall you that Swinnerton-Dyer showed that $\widetilde{M}\cong \mathbb{F}_p[X,Y]/(\tilde{A}-1)$ , with $A$ the unique polynomial such that $A(E_4,E_6)=E_{p-1}$ . In ""Congruences et formes modulaires"" he makes a more precise description: he states that $\widetilde{M}^0$ is isomorphic to l'algebre affine over $\mathbb{F}_p$ of the curve $X$ obtained from the projective line by removing the values of the modular function $j$ mod $p$ corresponding to curves with zero Hasse invariant, i.e., supersingular curves. But then, how would this isomorphism be defined? Why would this curve $X$ be smooth? Consider that I am not at all familiar with Katz's approach, and scheme theory in general, so keep it elementary please!","['number-theory', 'p-adic-number-theory', 'algebraic-geometry', 'modular-forms']"
4121406,Continuity of a piecewise integral function,"I have a problem in evaluating the limit as $x$ goes to $0$ of: $$F(x)=\int_1^x f(t)\, dt\,\,\,\ \text{with }f(t)\begin{cases}t^3\ln{t}\, \text{ for }t>0\\ \arctan{t} \, \text{ for } t\leq 0\end{cases}$$ Now I have remarked that since $\lim_{t\to 0^+}f(t)=0=f(0)$ then the function $f$ can be continously extended in $0$ and so the integral function will be continous and defined in $\mathbb{R}$ . If it is continous in $0$ this means that $F(x)\to F(0)=\int_1^0t^3\ln{t}\,dx=-\int_0^1t^3\ln{t}\,dx=\frac{1}{16}$ . $\textbf{Problem}$ $$\lim_{x\to 0^-}\int_{1}^x f(t)=-\lim_{x\to 0^-}\int_{x}^1 f(t)=???$$ In fact $x\to 0^-$ so I have that the extreme $x$ tends to $0^-<0$ but the second is $1$ ...how can I write this limit and check that it is equal to $\frac{1}{16}$ ?","['integration', 'improper-integrals', 'graphing-functions', 'real-analysis', 'limits']"
4121434,When does the inequality $|x+y| \le |x|+|y|$ not hold?,"I was trying to find the least value of $|\sin x|+ |\cos x|$ and applied the above inequality as : $|\sin x + \cos x| \le |\sin x| + |\cos x|\\
\Longrightarrow  \sqrt{2} \le |\sin x| + |\cos x|$ But the range of the given function is $[1,\sqrt2]$ . Any idea what might have went wrong here?","['trigonometry', 'inequality']"
4121437,Can the Knight go through all spaces on a $7\times7$ chessboard?,On a $8\times8$ chessboard if the Knight starts at one of the corners it can move through all $64$ squares only once. But on a $7\times7$ board can the Knight go through all $49$ squares only once (need the route taken) If not why?,"['chessboard', 'discrete-mathematics']"
