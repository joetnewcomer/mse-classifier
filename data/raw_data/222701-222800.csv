question_id,title,body,tags
4571080,Polar Coordinates ODE,"I have the following ODE: $$\dot x=y+ax(x^2+y^2), \dot y=-x+ay(x^2+y^2)$$ and I want to prove that the system is equivalent with the following $$\dot r=ar^3, \dot \theta=-1$$ I start by taking polar coordinates but when I make the change I get $$ r \dot r=ax^4+2ax^2y^2+ay^4$$ Any suggestions?","['polar-coordinates', 'systems-of-equations', 'ordinary-differential-equations', 'dynamical-systems']"
4571143,Confusion about Zorn's lemma.,"I'm reading the book Notes on Lattice Theory , J. B. Nation and am confused about the proof of two equivalent axioms of AC (in Appendix 2: The Axiom of Choice, p137), the two axioms are: Zorn's Lemma If every chain in an ordered set $P$ has an upper bound in $P$ , then $P$ contains an maximal element. Hausdorff Maximality Principle Every chain in an ordered set $P$ can be embedded in a maximal chain. I have only basic knowledge about set theory and get confused about the proof from Zorn's Lemma to Hausdorff Maximality Principle: Given an ordered set $P$ , let $Q$ be the set of all chains in $P$ , ordered by set containment. If $\{C_{\alpha}: \alpha \in A\}$ is a chain in $Q$ , then $\bigcup C_{\alpha}$ is a chain in $P$ that is the least upper bound of $\{C_{\alpha}: \alpha \in A\}$ . Thus $Q$ satisfies the hypothesis of Zorn's Lemma, and hence it contains a maximal element $C$ , which is a maximal chain in $P$ . Isn't it possible that $\{C_{\alpha}: \alpha \in A\}$ is an infinite set? If possible, is there any constraint on the use of a union of infinite many sets? Like my proof below: For any chain $C$ of $\mathbb{N}$ , the sum of every element of $C$ is an upper bound of $C$ , can I use Zorn's Lemma here and say $\mathbb{N}$ has a maximal element? And I always feel lost when proofs have something to do with infinite stuff, is there any book about infinity? Many thanks!","['elementary-set-theory', 'order-theory']"
4571175,computing a connection coefficient over a framed open set,"It is well known that given a connection on a $\mathbb{K}$ vector bundle $E$ over some manifold $M$ with rank $r$ , there is a $U\subset M$ is a framed open set with frame $e= \{ e_i \}_{i=1}^r $ , such that we have a connection form matrix $\omega \in \Omega ^1 (U, \text{Mat}_{r\times r}(\mathbb{K})$ with respect to this frame. It satisfies $$
\nabla_X(\tilde{e}_i)=\sum_j \omega^i_j(X)e_j
$$ if $\tilde{e}_i$ is a globally defined section that agrees with $e_i$ on $U$ . I understand that given a vector bundle, we can always find framed open sets in which we can define the connection locally. Now, my question is given a connection and a framed open set, can we always have a connection form with respect to this frame (we of course still have connection via restriction of $\nabla$ )? I think that this does not hold when the section cannot be extended. Usually, when given a connection, we can compute the connection form by evaluating the expression $\nabla_X(\tilde{e}_i)$ since most cases I have seen, $e_i$ can be extended globally for the given frame so this question never occured to me. But considering my question, does one have to be careful with a choosing a frame in which we wish to find a connection over?","['manifolds', 'vector-bundles', 'differential-geometry']"
4571179,Existence of A Smooth Function Under a Positive Continuous Function,"Let $M$ be a smooth manifold. Let $f : M \to \mathbb{R}$ be a positive continuous function. Then is it true that there exists a positive smooth function $g : M \to \mathbb{R}$ such that $0 < g \le f$ ? This seems obvious, but I do not know how to approach. Thank you. (Edit) I think I proved it. If $M$ is compact, then $f$ has minimum, say $m > 0$ , so we can consider the constant function $g = m$ . If $M$ is noncompact, then consider some nice partition $\{ K_i \}_i$ of $M$ by compact sets, and let $m_i > 0$ be the minimum of $f|_{K_i}$ . Now we may take some nice smooth function $g$ such that $g \le m_i$ on each $K_i$ .","['continuity', 'differential-geometry', 'smooth-functions', 'real-analysis']"
4571192,Can a self-map of a vector space preserve a nondegenerate bilinear form without being linear?,"Let $V$ be an infinite-dimensional vector space over $\mathbb{R}$ . Let $B:V\times V\rightarrow \mathbb{R}$ be a nondegenerate bilinear form, i.e., a bilinear form satisfying $$ B(x,y)=0,\;\forall y\in V \Rightarrow x=0.$$ Suppose $\phi:V\rightarrow V$ is a self-map satisfying $B(x,y)=B(\phi x, \phi y)$ for all $x,y\in V$ . My question is this: Is $\phi$ necessarily linear? If ""no"", what's an example? The answer is yes if $\phi$ is surjective, by an argument I will share momentarily. What I want to know is if there exists a non-surjective nonlinear $\phi$ and a $B$ satisfying the above. The question is motivated by a passage in Peter Lax's book Functional Analysis , which gives a proof that relies on this assumption. On p. 59, Lax writes the following. (Hopefully the meaning of Lax's notation can be inferred from context, but just in case: $H$ is a Hilbert space and there is a fixed self-map of $H$ for which the image of $x\in H$ is denoted by $x'$ , and similarly for other variables.) ""We stated in chapter 5 that every isometry of a Banach space onto itself that maps $0$ into $0$ is linear. We give now a new proof of this in Hilbert space: [... here Lax verifies that an isometry fixing the origin preserves the Hilbert space inner product...] Now denote $x+y$ by $z$ , and let $u$ be any vector in $H$ . Using [the preservation of the inner product] we have $$ (z',u')=(z,u) = (x+y,u) = (x,u) + (y,u) = (x', u') + (y',u') = (x'+y',u'). $$ Thus $$(z'-x'-y',u')=0$$ for all $u'$ . This can be only if $z'=x'+y'$ . The virtue of this proof is that it applies even when the scalar product is not positive, as long as it is nondegenerate, meaning that no $u$ is orthogonal to all points. My question arose from making sense of Lax's final comment here. If the scalar product is not positive, then it doesn't induce a metric, so it becomes unclear to me what it should mean for the metric to be complete, and therefore unclear to me how to interpret the notion of ""Hilbert space"" in this broader context. On the other hand, the argument makes no use of completeness, and the metric structure only enters through the inner product, so presumably the scope of Lax's final comment above is really just spaces equipped with a nondegenerate bilinear form (as in the question above). This led me to a question about what Lax wants to replace the isometry with in this broader context. He defines an isometry as (distance-preserving and) surjective, and implicitly uses this in the argument above because prima facie, ""for all $u'$ "" means ""for all $\phi u$ with $u\in H$ ""; this only ranges across all of $H$ if $\phi$ surjects. So the claim that results from a straightforward generalization of the above argument is that if $\phi$ preserves $B$ and is surjective onto $V$ , then $\phi$ is linear. So this made me curious: what happens if $\phi$ is not surjective?","['linear-algebra', 'functional-analysis']"
4571232,Estimation: An integral from MIT Integration bee 2022 (QF),"We need to determine the value of: $$\lim_{n\to\infty}\sqrt n\cdot \int_{-1/2}^{1/2} (1-3x^2+x^4)^n\mathrm{d}x$$ Since a limit has been given, one suspects that, as usual, the integral is not easy to evaluate in general and some estimates have to be made. Some promising starts (and observations) include: The polynomial always lies between $5/16$ and $1$ in the given range. Perhaps we can ignore some higher order terms (even $x$ has modulus less than one)? But the problem is that exponent $n$ . Reduction formula and integration by parts can give a recursion. But all reasonable ways of splitting the integrand seem to complicate matters rather than simplify them. We can now take some hints from the limits. Perhaps some trigonometric substitution can aid. A reasonable one is $x^2=\frac 14\sin \theta$ or $x=\frac 12 \cos\theta$ but there is not much one can do due to the quadratic polynomial. Factorize the polynomial as $x^2-3x+1$ has all real roots? But I still get stuck. But I am unable to make any of these work. Does anyone have any suggestions?","['limits', 'definite-integrals']"
4571236,Why is the rank of an element of a null space less than the dimension of that null space?,"I'm trying to understand a statement I came across in a paper. Say $A$ and $B$ are $n \times n $ -dimensional real matrices, and $B$ is in the null subspace of $A$ i.e. $AB=0$ (in fact $BA=0$ is also true in this case but I´m not sure if this is relevant here). The paper states that, since $B$ is in the null subspace of $A$ , $$ \textrm{rank}(B) \leq \textrm{dim}\big(\mathcal{N}(A)\big)\,,    $$ where $\mathcal{N(.)}$ returns the null subspace of the argument. How can one prove this inequality? I tried to show it with the rank-nullity theorem, to no success. I suspect the proof may be even simpler than this.","['matrices', 'matrix-rank', 'linear-algebra', 'linear-transformations']"
4571242,"Find all the real numbers an expression might take when $a, b, c$ are complex numbers of the same modulus.","Let $a, b, c$ be three complex numbers of the same modulus. Find all real numbers that might be equal to: $$x = \frac{a^3+b^3+c^3}{abc}$$ It is obvious that when all three numbers are equal, we might write that: $$x = \frac{3a^3}{a^3} = 3 \in \mathbb{R}$$ I would write, in order to obtain real values: $$x = \overline{x}$$ So: $$\overline{a}\overline{b}\overline{c} (a^3 + b^3 + c^3) = abc (\overline{a}^3 + \overline{b}^3 + \overline{c}^3)$$ But the calculations are quite harsh, the only thing I might use to solve them is that: $$a\overline{a} = b\overline{b} = c\overline{c} = m^2$$ where $m$ is the complex modulus of the three numbers.","['algebra-precalculus', 'complex-numbers']"
4571303,Determine the parameters so that the function is continuous in R,I got this problem i've tried to solve but i don't know how to proceed. $$\begin{cases} 3\sin(4x)&&\text{if }x\leq 0 \\ mx+q&&\text{if }x>0 \end{cases}$$ Find the value of $m$ and $q$ so that the function is continuos in R. I've tried this $$\lim_{x\to 0^-} 3\sin(4x) = \lim_{x\to 0^+}mx+q$$ So that the $q$ value equals to $0$ but I don't know.,"['limits', 'functions', 'analysis']"
4571329,How to find matrix multiplications like AB = 10A+B? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question saw this one. How do we find others (not necessarily 2 by 2)?
How do we generalize it?","['matrices', 'matrix-decomposition']"
4571342,Computing Theta Characteristics of Hyperelliptic Curves,"Take a hyperelliptic curve $C$ (nonsingular, algebraically closed field) of genus $g$ . The theta characteristic(s) of the curve are defined to be the line bundles $\{\theta\in Pic^{g-1}(C)|\theta^{\otimes 2} = \omega_C\}$ , where $\omega_C$ is the canonical sheaf. We can (after fixing some $\theta$ ) write $\theta=\mathscr{O}_C(\Theta)$ and $\omega_C=\mathscr{O}_C(K)$ for the canonical divisor $K$ and corresponding theta divisor $\Theta$ . It is well-known that there are $2^{2g}$ theta characteristics for a genus $g$ hyperelliptic curve. Since we are in the nonsingular case we can consider $\Theta,K$ as Weil divisors. In particular we must have $[2\Theta]=[K]$ . Naively I hoped to simply formally divide the coefficients in $K$ by $2$ to find a $\Theta$ , but although the degree of $K$ is even, the coefficients of it are all not even (for example the coefficients at the Weierstrass points are $1$ ). Question: How does one calculate $\theta,\Theta$ in practice? Are there any examples of this for low-genus curves? Any references?","['algebraic-curves', 'vector-bundles', 'algebraic-geometry', 'divisors-algebraic-geometry']"
4571343,"Solving for $x$ such that $x^2 \log(\alpha/x) \leq \beta$ where $\alpha, \beta >0$.","I am trying to solve the following inequality for $x$ . Essentially it suffices even if I can find one value of $x$ which satisfies this: $$x^2 \log\left(\frac{\alpha}{x}\right) \leq \beta,$$ where $\alpha, \beta >0$ . How can I approach it? Some background on where the problem is coming from:
Basically, I have the following inequality: $$\beta\left(cH\sqrt{d\log\left(\frac{Hkd}{\beta\delta\delta_1}\right)}+\frac{4}{3}\right) \leq \frac{1}{4}$$ and I need to find a positive value for $\beta$ for which the inequality satisfies. Here $c,H,d, k,\delta, \delta_1$ are positive constants.","['logarithms', 'inequality', 'functions', 'real-analysis']"
4571451,EV of a strictly increasing sequence of n-sided dice rolls,"You roll a fair, $n$ -sided die. If its value is higher than that of your previous roll (or this is your first roll), you add it to your score and roll again. If not, you keep any banked points and your turn is over. I'd like to know the expected score for each turn. Example playthrough with a D20: Rolls : 7, 12, 16, 13 (turn ends) Score : $7 + 12 + 16 = 35$ This process was inspired by the ball flight mechanic from the dice game Roll In One . Simulating this in Python yields the surprising (to me, at least) conclusion that the expected score is $n$ . Looking forward to any insights you have to offer!","['expected-value', 'dice', 'probability']"
4571510,Is it possible to show that an absolutely continuous function $I\to \mathbb{R}$ is differentiable a.e. without invoking Lebesgue theory?,"Theorem: let $f:I\to \mathbb{R}$ be absolutely continuous , then $f'$ exists a.e. As mentioned here , Rudin's Real and Complex Analysis proves the theorem using a fair bit of measure theory. The theorem, however, is very easily stated with little measure theory; one would only need said theory to define the meaning of ""a.e."", which could be done as follows: Definition: for an arbitrary $x\in\mathbb{R}$ , let $P(x)$ be a statement about $x$ (e.g. "" $x$ is rational"", "" $x$ is larger than 2"", etc.). We write $$¬P:=\{x\in\mathbb{R} : P(x) \text{ is false}\}$$ We say that $P$ holds almost everywhere iff $$\inf\left\{\sum_{k=1}^{\infty}(b_k-a_k) : ¬P\subseteq \bigcup_{k=1}^{\infty}[a_k,b_k] \right\} = 0.$$ Considering that the above is the only bit of measure theory we need to state the theorem, is it possible to prove the theorem without invoking any more measure theoretical machinery?","['analysis', 'real-analysis', 'alternative-proof', 'absolute-continuity', 'derivatives']"
4571527,Find a completion of the following spaces,"Find a completion of the incomplete metric space $(X,d)$ , where a) $X=\Bbb{Q}$ , where $\forall{q,r}\in{X}$ $d(q,r) = \lvert {\arctan(q) - \arctan(r)}\rvert$ b) $X=(0,1)\setminus \mathbb{Q}$ , where $\forall{x,y}\in{X}$ $d(x,y)=|x-1|+|y-1|$ for $x\neq y$ , $d(x,x)=0$ For part a), I suspect that the metric $d$ is equivalent to the usual Euclidean metric $\lvert {q-r}\rvert$ . If this is correct, then the completion of $X$ would be $\Bbb{R}$ I guess. To show the equivalence, I thought that we can use the sequential characterization of continuity along with the fact that the function $arctan$ is continuous, but I am not sure about this as the domain of $arctan$ is $\Bbb{Q}$ , any idea is highly appreciated. For part b), If $X$ was $(0,1)$ with the same metric, I would argue that any convergent sequence in $X$ converges to $1$ , so the completion might be $(0,1]$ . But $X=(0,1)\setminus \mathbb{Q}$ , so should I still argue that the completion is $(0,1]$ ? My mind is very confused about this part.","['complete-spaces', 'metric-spaces', 'analysis', 'real-analysis']"
4571532,Writing proofs and solutions completely but concisely [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 months ago . Improve this question I have a university lecturer that puts a lot of emphasis on the writing quality of an answer, not just if its correct. He wants our answers to be 5 things, Clear, Complete, Concise, Coherent and Correct. He says he doesn't believe in perfect answers so can be very nit-picky over our solutions, but it also means he can be contradictory. For example in one of my solutions he said i had been repetitive but firstly, the repeated line (which was something like, ""by the law of total variance"") was there to help the narrative of the answer - which he puts so much emphasis on - and it was repeated from part 1 of a question after quite a few lines of integration. But secondly and more importantly, in his very own solution, he had the exact same repetition. And he said my answer was very good and he was being really nitpicky but that i could at some point write more detail, like write ""the pdf of X is f(x)=..."" rather than just go straight into ""f(x)=..."" So my question is how do you strike a balance between explaining enough and becoming too laborious? Also what are some good connecting words in proofs and 'show me' questions and when do you use which? Because i just tend to stick in 'and', 'because', 'so' 'thus' etc, when prehaps sometimes it isn't strictly true (but does get the point across) An example of such a question: Let X~Gamma( $\alpha,\beta$ ) where $\beta$ is a rate parameter. Find the MGF of X and
use this to show that $\Bbb{E}(X)=\frac{\alpha}{\beta}$ and $var(X)=\frac{\alpha}{\beta^2}$ Let X~Gamma( $\alpha , \beta$ ) Then the PDF of X is: $$f(x)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{(\alpha-1)}e^{-\beta x}dx $$ ,for $x>0$ and 0 otherwise, where $$\Gamma(\alpha)=\int_0^\inf t^{\alpha-1}e^{-t}dt$$ $$M_X(t)=\mathbb{E}(e^{tX})=\int_0^\inf e^{tx}f(x)dx$$ $$=\frac{\beta^\alpha}{\Gamma(\alpha)}\int_0^\inf e^{tx}x^{\alpha-1}e^{-\beta x}dx$$ $$=\frac{\beta^\alpha}{\Gamma(\alpha)}int_0^\inf x^{\alpha-1}e^{-(\beta-t) x}dx$$ Let $$u=(\beta-t)x$$ $$\iff x=\frac{u}{\beta-t}$$ $$\implies dx=(\beta-t)du$$ So $$M_X(t)=\frac{\beta^\alpha}{\Gamma(\alpha)}\int_0^\inf (\frac{u}{\beta-t})^{\alpha-1}e^-u(\beta-t)du$$ $$=\frac{\beta^\alpha}{\Gamma(\alpha)}(\frac{1}{\beta-t})^{\alpha}\int_0^\inf u^{\alpha-1}e^{-u}du$$ $$\frac{\beta^\alpha}{(\beta-t)^\alpha}$$ To show that $\Bbb{E}(X)=\frac{\alpha}{\beta}$ and $var(X)=\frac{\alpha}{\beta^2}$ :
Using differentiation gives us: $$M'(t)=\frac{\alpha\beta^\alpha}{(\beta-t)^{\alpha+1}}$$ and $$ M''(t)=\frac{\alpha(\alpha+1)\beta^\alpha}{(\beta-t)^{\alpha+21}}$$ It follows that $$\Bbb{E}(X)=M'(0)=\frac{\alpha\beta^\alpha}{\beta^{\alpha+1}}=\frac{\alpha}{\beta}$$ and $$\Bbb{E}(X^2)=M''(0)=\frac{\alpha(\alpha+1)\beta^\alpha}{\beta^{\alpha+21}}=\frac{\alpha(\alpha+1)}{\beta^2}$$ Hence $$ var(X)=\Bbb{E}(X^2)-\Bbb{E}(X)^2$$ $$=\frac{\alpha(\alpha+1)}{\beta^2}-\frac{\alpha^2}{\beta^2}$$ $$=\frac{\alpha}{\beta^2}$$ as required I know i need connecting words before i calculate the M(t) but i have no idea what to put, plus I'm not sure if where I've put ""it follows that"" is strictly true, also i feel like where i've used ""So"" there might be a better word to use but i can't think what","['proof-writing', 'soft-question', 'probability']"
4571552,Simple Riemann Integrability Question,"I would like to show that for $a\leq s < t\leq b$ , the function $$f(x) = \begin{cases}
1,\,&\mbox{ if }s<x<t;\\
0,\,&\mbox{ otherwise}
\end{cases}$$ is Riemann-integrable on $[a,b]$ and that this integral has value the $t-s$ . So far, my line of thinking has been to let $P$ be a general partition $a = x_{0}<x_{1}<\ldots<x_{n} = b$ , take $M,N$ to be such that $s\in [x_{M-1},x_{M}]$ and $t\in [x_{N-1},x_{N}]$ , and then write down the lower- and upper-Riemann sums of the function $f$ on $[a,b]$ depending on whether $s\in (x_{M-1},x_{M})$ or $s \in \left\{x_{M-1},x_{M}\right\}$ (and similarly for $t$ ). My question is this: is my line of thinking a good way to approach the problem, and if so, where do I go from here? If my line of thinking is not likely to be productive, please point me in a better direction. Thanks!","['integration', 'analysis', 'real-analysis', 'calculus', 'riemann-integration']"
4571557,Show that the arc length is a limit of lengths of inscribed polygons.,"In Do Carmo's book (page 11), asked to find given $\epsilon>0$ , there exists $\delta>0$ such that if $|P|<\delta$ then $\left|\int_a^b |\alpha'(t)|~dt-l(\alpha,P)\right|<\epsilon$ . Here $P$ is a partition with $|P|=\max(t_i,t_{i-1}),i=1, \dots, n$ , $\alpha:I \to \mathbb{R}^3$ for an interval $I$ , $l(\alpha, P)= \sum_{i=1}^n|\alpha(t_i)-\alpha(t_{i-1})|$ . Any ideas to prove this?","['differential-geometry', 'real-analysis']"
4571589,Finding an asymptotic equivalent of a series,"$\forall n \in \mathbb N^*$ , let $f_n(x) = n^x\exp(-nx)$ . Find an asymptotic equivalent of the series when $x \to 0^+$ : $$\sum_{n=1}^{+\infty}f_n(x)$$ Let $f(x) = \sum_{n=1}^{+\infty}f_n(x) $ , I found that $f$ is define on $\mathbb R^+_*$ . But after that, it is difficult to find something. I have though one hint given : Use $n \ln(n)\sim n$ . I want to know if there is any documentation about ways to find an asymptotic equivalent of a series of functions. Thanks for your help ! Edit 1 : I don't know if it is helpful but I noticed that $f$ is convex on $\mathbb R^+_*$ Edit 2 : I looked over the integral $\int_{1}^{+\infty}t^x\exp(-tx)dt$ . I found that it is equal to $\dfrac{1}{x^{x+1}}(x^x \exp(-x)+x\Gamma(x)-x\int_{0}^x u^{x-1}\exp(-u)du)$ . I don't know how $x\int_{0}^x u^{x-1}\exp(-u)du$ goes for $x\to0^+$ . And, I want to prove it (if possible) without the $\Gamma$ function.","['functions', 'asymptotics', 'sequences-and-series']"
4571604,Remainder Theorem vs. Factor Theorem,"Is it right to say that the remainder theorem and the factor theorem are similar, but not the same processes? For context, this was a question on a homework sheet provided to me by my advanced functions teacher. The question asked for a way other than remainder theorem to determine if a binomial was a factor of a polynomial. I argued that factor theorem was a way, but she said that the two were basically the same and that synthetic division would be the correct answer. My train of thought was: Remainder Theorem - for any polynomial f(x), if you divide it by the binomial x−a, the remainder is equal to the value of f(a). Factor Theorem - if a is a zero of a polynomial f(x), then (x−a) is a factor of f(x) I concluded, with the help of an online resource, that the remainder theorem links the remainder of division by a binomial with the value of a function at a point, while the factor theorem links the factors of a polynomial to its zeros. While they may seem the same, mathematically, they are different, right? That's why they are two distinct theorems and not one. I also thought that if the ""remainder theorem and factor theorem are basically the same"" then the remainder theorem and synthetic division are also the same because you are using the rational zeroes theorem in both cases. Any mathematical or logical help to prove the difference between the remainder theorem and the factor theorem would be greatly appreciated. Thanks","['algebra-precalculus', 'functions', 'factoring']"
4571660,"Finding $x+y$, given $xy= 1$, $x^2+y^2=5$, $x^3+y^3=8$","This problem is from a math competition, but I think is wrong: Find the value of $x+y$ if: $$\begin{align}
xy &= 1 \\ 
x^2 + y^2 &= 5 \\ 
x^3+y^3 &= 8
\end{align}$$ Solution (I think is wrong): $x^3 + y^3 = (x + y)(x^2-xy+y^2) = (x+y)(5-1) = 4(x+y)$ So we have: $x^3+y^3 = 4(x+y)$ $x^3+y^3 = 8$ Then: $8 = 4(x+y)$ $x+y = 2$ However if we replace that value in $(x+y)^2$ we have: $(x+y)^2 = 2^2 =4$ $(x+y)^2 = x^2+y^2+2xy = 5 + 2 = 7$ As you can see $4 \neq 7$ , what is happening?","['solution-verification', 'factoring', 'polynomials', 'nonlinear-system', 'algebra-precalculus']"
4571704,Projective Limits of Compact Groups: Exact or Not?,"I am reading the following lemma from Washington's book ""Introduction to Cyclotomic Fields"": On the other hand, there is a counterexample, given by this answer . The comments below this answer indicate that for projective limits to be exact, one should add Hausdorff condition, whose necessity I don't really understand. So now I get very confused because I have a counterexample and a proof, and I am not able to locate a potential flaw in the proof, which does not assume (at least explicitly) Hausdorff-ness. I would appreciate it very much if anyone can help me understand the subtle issue here.","['limits-colimits', 'homological-algebra', 'topological-groups', 'separation-axioms', 'general-topology']"
4571756,"If $f$ is analytic defined on $D:=\{z:|z|<1\}$ and $|f(z)|\le 1$, can $f$ be extended continuously to $\overline D$?","If $f$ is analytic defined on $D:=\{z:|z|<1\}$ and $|f(z)|\le 1$ , can $f$ always be extended continuously to $\overline D$ ? In other words, can all analytic functions $f:D\to\overline D$ be extended to continuous functions $\tilde f:\overline D\to\overline D$ (such that $\tilde f|_D=f$ )? Ideally, for $|z_0|=1$ , we could simply define $f(z_0)=\lim_{z\to z_0}f(z)$ . It seems like a reasonable assumption that this extended version of $f$ exists and is continuous, but it's not obvious to me that these limits must exist. This feels like it should be a standard result in complex analysis, but I haven't been able to find a good reference. (Unfortunately, the hope that $f$ can be extended to an analytic function is dashed by the example of $\sqrt{z-1}$ .)","['complex-analysis', 'analyticity', 'analytic-functions']"
4571783,Compute the gradient of polar basis vectors in tensor calculus,"$(1)$ Compute the gradient of polar basis vectors, \begin{equation}
\tilde{\nabla} e_\rho=\frac{1}{\rho} \widetilde{e}_\rho \otimes e_\theta \text { and } \tilde{\nabla} e_\theta=\frac{1}{\rho} \tilde{e}^\rho \otimes e_\theta-\tilde{\rho} \tilde{e}^\theta \otimes e_\rho
\end{equation} $(2)$ What are the gradients of the vectors $e_\rho$ and $e_\theta$ on a cylinder? (Hints: A cylinder is a surface of constant $\rho$ in cylindrical coordinates in $\mathbb{R}$ ) I know, \begin{align}
[g_{ij}] = 
\begin{pmatrix}
g_{rr} & g_{r\theta}\\
g_{\theta r} & g_{\theta \theta}
\end{pmatrix} = 
\begin{pmatrix}
1 & 0 \\
0 & r^2
\end{pmatrix}
\end{align} \begin{align}
[g^{ij}] = 
\begin{pmatrix}
g^{rr} & g^{r\theta}\\
g^{\theta r} & g^{\theta \theta}
\end{pmatrix} = 
\begin{pmatrix}
1 & 0 \\
0 & 1/r^2
\end{pmatrix}
\end{align} and gradient in polar coordinates, \begin{align}
\text{grad}(f) &= g^{rr}\dfrac{\partial f}{\partial r}\dfrac{\partial }{\partial r} + g^{\theta \theta}\dfrac{\partial f}{\partial \theta}\dfrac{\partial }{\partial \theta} \\
&= \dfrac{\partial f}{\partial r}\dfrac{\partial }{\partial r} + \dfrac{1}{r^2}\dfrac{\partial f}{\partial \theta}\dfrac{\partial }{\partial \theta} \tag{$**$}
\end{align} But I didn't see from these information how can I compute the given one? From here I knew that there are two kind of basis. Covariant basis and orthonormal basis (which we mostly familiar with linear algebra). From the question context I couldn't understand how to interpret which basis was used to compute the gradient. Any help will be appreciated. TIA. As @Paul Sinclair suggested, I need to use gradient for vector field. but what I get was scalar gradient in my book only. I have the following definitions in my book . $$
\begin{align}
\text{grad}(\phi) &= \phi_{,i} = \frac{\partial \phi}{\partial x^{i}}\\
\text{div}(A^{i}) &= A^{i}_{,i} = \frac{1}{\sqrt g}\frac{\partial }{\partial x^{k}}(\sqrt g A^k)\\
\text{curl}(A_i) &= A_{i,j} - A_{j,i}
\end{align}
$$ Where I see any of them can't help me to get the solution. And I was wondering why $\text{curl}$ operator only defined for covariant tensor? Where $\text{div}$ defined for both covariant and contravarint tensor $\text{div}(A^{i})=\text{div}(A_{i})$ What bother me a lot that from vector calculus we taught the operations grad (which maps functions to vector fields), curl (which maps vector fields to vector fields), and div (which maps vector fields to functions). But in tensor calculus we are talking about gradient of vector field, isn't that seem wrong?","['polar-coordinates', 'tensors', 'differential-geometry']"
4571793,Why do we require a function to be measurable in order to define its Lebesgue Integral?,"Let be $f : X \to [0; + \infty)$ measurable . We define the Lebesgue integral for $f$ as follow: $$ \int_X f(x) \ d\lambda(x) := sup \{ \ \int_X s(x) \ d\lambda(x) : s(x) \le f(x) \ \forall x \in X \ \} $$ where s(x) is a non-negative simple function. I cannot understand why do we require $f$ to be measurable . My thought is: we don't really need f to be measurable (I never mentioned it in the definition of the integral, I only said that $f(x)$ should be always not-smaller than any simple function $s(x)$ ). We never use it in the definition of the Lebesgue Integral . What we really need and indeed use is that the any simple function $s(x)$ be measurable (otherwise I cannot integrate it). Now, my guess is: do we require $f(x)$ to be measureable in order to be sure that all its approximation (i.e. the simple function ) are indeed measurable ? If I am wrong, where exactly in the definition of Lebesge Integral do we use the measurability of $f(x)$ ? Thank you so much to you all, I really appreciate it.","['measure-theory', 'simple-functions', 'lebesgue-integral', 'measurable-functions']"
4571815,"Prove that there exists $x \le 4^n$ such that $x \equiv i \mod p_i$ for all $1 \le i \le t$, given all prime $p_i \le n$","Let $p_1, \cdots, p_t$ be all prime numbers between $2$ and $n$ . Prove that there exists $x \le 4^n$ such that $x \equiv i \mod p_i$ for all $1 \le i \le t$ . My attempts: From Chinese remainder theorem $$x = \sum_{i=1}^t i M_i N_i \mod N$$ with $N = \prod_{j=1}^t p_j$ , $N_i = \frac{N}{p_i}$ and $M_i$ is the inverse element of $N_i$ in $\mathbb{Z}_{p_i}^*$ . $$x \le N \le \prod_{i=1}^t p_i = e^{\sum_{i=0}^t \log p_i} \le e^{\pi(n) * \log n} \sim e^{n} \le 4^n$$ In the last $\sim$ step, I used the prime number theorem . However, this is only correct when $n \rightarrow \infty$ . I haven't proven in any $n$ and $t$ . Another method is that $$x \le N \le \prod_{i=1}^t p_i = e^{\sum_{i=0}^t \log p_i} \le e^{\theta(n)} \le e^{1.000028 n} < 2.8^n < 4^n$$ with $\theta(n)$ 1st Chebyshev function , and upper bound $\theta(n) < 1.000028 n$ . However, I haven't used the property: $x = i \mod p_i$ in this problem. Since it's only homework for undergraduate abstract algebra, I'm curious whether there is any fundamental solution without the use of the Chebyshev function ?","['number-theory', 'elementary-number-theory', 'abstract-algebra', 'discrete-mathematics', 'prime-numbers']"
4571842,"Can any continues differentiated h(x,y) function be represented by f1(x)*g1(y) + f2(x)*g2(y)?","I'm trying to prove mathematically if this is correct or incorrect. (Sorry for lack of an accurate tag - I don't know what is the correct one for this question) Given a continues differentiated 2d function $$h(x,y)$$ then there must be four 1d differentiated functions: $$f1(x),\ f2(x),\ g1(x),\ g2(x) $$ that fulfill: $$h(x,y) = f1(x)\cdot g1(y) + f2(x)\cdot g2(y)$$ Note: it is easy to show that not all h(x,y) can be represented by only f(x)*g(y)","['continuity', 'functions']"
4571864,"Equilateral triangles on two adjacent sides of a rectangle, and the rectangle's fourth vertex, determine another equilateral triangle","Recently I bumped into a $7^{th}$ -grade problem.
Shamefully I can't find any elementary solution for it. The problem is as follows: There is given a rectangle $ABCD$ with shorter sides $AD=BC$ .
Let $BCE$ , $ABF~$ be two equilateral triangles with $E~$ inside $ABF$ . We are being asked to prove that $DEF$ is a equilateral triangle as well. Any clue how to defeat it $7th$ -graders with elementary methods?","['euclidean-geometry', 'geometry']"
4571897,Differentiating $(\frac{\sqrt{x+1}}{\sqrt{x}})^3$ using chain rule,"We have, $$y = \left(\frac{\sqrt{x+1}}{\sqrt{x}}\right)^3$$ $$ \implies y = \left(1 + \frac{1}{x}\right)^{3/2}$$ Differentiating both sides w.r.t. $x$ (Using chain rule) $$\implies \frac{dy}{dx} = \frac32\left(1 + \frac{1}{x}\right)^{3/2 -1 }\cdot \dfrac{d}{dx}\left(1 + \frac{1}{x}\right)$$ $$ \implies \frac{dy}{dx} = \frac32\left(1 + \frac{1}{x}\right)^{1/2}\cdot \frac{-1}{x^2}$$ $$ \implies \boxed{\frac{dy}{dx} = \frac{-3}{2x^2}\sqrt{1 + \frac{1}{x}}}$$ What is the mistake in my work? Graphs of $f'(x)$ and $\frac{-3}{2x^2}\sqrt{1 + \frac{1}{x}}$ are not same .","['calculus', 'derivatives', 'chain-rule']"
4571900,"If two matrices have same characteristic polynomial, then if square root for one exists, it also exists for the other one.","This is True/False question from the recent exam. Statement: Suppose $A$ and $B$ are two elements of $M_n(\mathbb{R})$ such that their characteristic polynomials are equal. If $A=C^2$ for some $C\in M_n(\mathbb{R})$ , then $B=D^2$ for some $D\in M_n(\mathbb{R})$ . I haven't reach the conclusion yet. But here's my thought. Since $A=C^2$ , eigenvalues of $A$ are non-negative. Since $C_A(x)=C_B(x)$ , eigenvalues of $A$ and $B$ are same, which means $B$ also have non-negative eigenvalues. But will this guarantee square root for $B$ ?","['linear-algebra', 'characteristic-polynomial']"
4571936,The result of a standard form line equation [duplicate],"This question already has answers here : What does distance of a point from line being negative signify? (4 answers) Closed 1 year ago . If we substituted a point in the standard form equation, what the resulting number indicates? For example: We have the line: $$3x + 6y + 12 = 0$$ And we have the point $I(1,2)$ Substituting I in the line equation: $$3(1) + 6(2) + 12 = 0$$ $$=> 27 = 0$$ Does this only means that the point does not belong to the line? And what the $27$ indicates in this example? (I know that if the resulted number is positive that means that the point is above the line and if it is negative the point is below the line). But my question is that if the number $27$ indicates something, apart from the sign","['algebra-precalculus', 'linear-algebra', 'geometry']"
4572000,Proving that $ X$ is homeomorphic to $ Y / A $,"I want to solve the following exercise: Exercise Suppose that $ X $ and $ Y $ are compact Hausdorff topological spaces and $ A \subset Y $ is non-empty and closed.
Let $ x_0 \in X $ be a point.
Prove that if $ \varphi \colon X \setminus \{ x_0 \} \to Y \setminus A $ is a homeomorphism, then there exists a homeomorphism $ f \colon X \to Y / A $ . I think that I can intuitively see why this is true, but I cannot formally prove this statement. My attempt Let $ f $ be given by $$
f(x) = 
\begin{cases}
    (q \circ \varphi)(x) &\text{ if } x \neq x_0 \, , \\
    a &\text{ otherwise} \, ,
\end{cases}
$$ where $ q \colon Y \to Y / A $ is the quotient map and $ a \in q(A) \subset Y / A $ is a fixed point.
Since $ \varphi $ and $ q \colon Y \setminus A \to Y / A $ are homeomorphisms (we restricted $ q $ to $ Y \setminus A $ - this is a homeomorphism onto its image), we know that $ f $ is a bijection.
We need to prove that $ f(U) $ is open for any open $ U \subset X $ and $ f^{-1}(O) $ is open for any open $ O \subset Y / A $ .
Let $ U \subset X $ .
If $ x_0 \not \in U $ , then $ f(U) $ is clearly open in $ Y / A $ , because $ f(U) = (q \circ \varphi)(U) $ .
If $ x_0 \in U $ , then $$
f(U) 
            = f(U \setminus \{ x_0 \}) \cup f(\{ x_0 \}) 
            = (q \circ \varphi)(U \setminus \{ x_0 \}) \cup \{ a \} \, .
$$ Let $ f(U) = W $ .
We know that $ W $ is open if and only if $ q^{-1}(W) = \varphi(U \setminus \{ x_0 \}) \cup A $ is open. I don't know how to proceed from here. Similarly, I do not know how to prove $ f^{-1}(O) $ is open given an open $ O \subset Y / A $ such that $ a \in O $ , but it seems to me that the proof must use a very similar argument.
I imagine we now need to use the compactness of $ X $ , $ Y $ , and $ A $ somehow, but I've no idea how.","['general-topology', 'quotient-spaces']"
4572007,When is a sequentially closed subspace the kernel of a sequentially continuous linear functional?,"Let $E$ be a Banach space with continuous dual $E'$ , endowed with the weak $^*$ topology. Let $V\subseteq E'$ be a (weak $^*$ -)dense subspace of $E'$ . If $V$ is also sequentially closed (with respect to the weak $^*$ topology on $E'$ ), does there exist a weak $^*$ sequentially continuous linear functional $\phi:E'\longrightarrow\mathbb{R}$ , such that $\textsf{ker}(\phi)=V$ ?","['continuity', 'banach-spaces', 'functional-analysis', 'weak-topology']"
4572019,"Arrangements of $a,a,b,b,b,c,c,c,c$ in which no two consecutive letters are the same","I have the following question: We are going to generate permutations from a,a,b,b,b,c,c,c,c. Please compute the number of permutations such that: (a) for any consecutive 4 elements, they are not all the same; (b) for any consecutive 3 elements, they are not all the same; (c) for any consecutive 2 elements, they are not all the same. See Arrangements of $a,a,b,b,b,c,c,c,c$ in which no four/three/two consecutive letters are the same for the first two parts. I got an answer for part c but I am not sure if it is correct or not.
My step is the following. I found that there are six pairs containing adjacent identical letters. I will define them as $S_1, S_2,..., S_6$ the small number is the number of pairs of adjacent identical letters. $$S_0 =\binom{9}{2,3,4}$$ $$S_1 =\binom{8}{3,4} + \binom{8}{2,4} + \binom{8}{2,3,2}$$ $$S_2 = \frac{7!}{4!}+\binom{7}{3,2}+\binom{7}{4,2} + \binom{7}{3,2} + \binom{6}{3,2}+\binom{7}{2,2}$$ $$S_3 = \frac{6!}{4!}+\frac{6!}{3!}+\frac{5!}{3!} + \binom{6}{2,2} + \frac{6!}{2!} + \frac{5!}{2!}$$ $$S_4 = \frac{5!}{2!}+\frac{5!}{3!}+\frac{5!}{2!} + \frac{5!}{2!} + {5!}+{4!}$$ $$S_5 = {4!}+{3!}+\frac{4!}{2!} $$ $$S_6 = 3!$$ I sum these up by the inclusion-exclusion principle, and I got the answer of the following: $$N = S_0 - S_1+ S_2- S_3 +S_4 -S_5+ S_6 = 473$$ I want to know if there is any miscalculation or if the wrong steps exist. Could anyone please help?","['permutations', 'combinations', 'combinatorics', 'inclusion-exclusion']"
4572035,Grothendieck group of a smooth curve,"Let $X$ be a smooth projective and geometrically irreducible curve over a field $k$ . It an exercise in Hartshorne's book on ""Algebraic Geometry"" (p.149) that in the case that $k$ is algebraically closed the Grothendieck group $K(X)$ is isomorphic to $\textrm{Pic}(X)\oplus\mathbb{Z}$ (and the isomorphism is given by determinant and rank). Is this also the case when $X$ is not algebraically closed? It seems to me on the first glimpse that the same proof would work but then I am a bit insecure because why does Hartshorne then not state it for arbitrary fields?","['algebraic-curves', 'algebraic-geometry', 'sheaf-theory']"
4572068,"Show that contact manifolds are orientable, or why $\lambda\alpha\wedge(\lambda d\alpha+d\lambda\wedge\alpha)^n=\lambda^{n+1}\alpha\wedge(d\alpha)^n$","While reading to Geige's An Introduction to Contact Topology, I got stuck after the following statement: Observe that $\alpha$ is a contact form precisely if $\alpha\wedge(d\alpha)^n$ is a volume form on $M$ (i.e. a nowhere vanishing top-dimensional differential form); in particular, $M$ needs to be orientable. The condition $\alpha\wedge(d\alpha)^n\not=0$ is independent of the specific choice of $\alpha$ and thus is indeed a property of $\xi=\ker\alpha$ : any other 1–form defining the same hyperplane field must be of the form $\lambda\alpha$ for some smooth function $\lambda:M\rightarrow\mathbb R\setminus\{0\}$ , and we have $$(\lambda\alpha)\wedge d((\lambda\alpha))^n=\lambda\alpha\wedge(\lambda d\alpha+d\lambda\wedge\alpha)^n=\lambda^{n+1}\alpha\wedge(d\alpha)^n\not=0\text{.}$$ For context, $\alpha$ is a 1-form that doesn't vanish at any point of $M$ , where $M$ is a $(2n+1)$ -differentiable manifold, and $(d\alpha)^n$ denotes the $n$ -fold wedge product $d\alpha\wedge\dots\wedge d\alpha$ . I don't know how to prove the second equality, that is, why do we have that $$\lambda\alpha\wedge(\lambda d\alpha+d\lambda\wedge\alpha)^n=\lambda^{n+1}\alpha\wedge(d\alpha)^n?$$ This would be trivial if we somehow had that $d\lambda\wedge\alpha=0$ , which certainly doesn't have to be the case. Thanks in advance for your answer.","['differential-forms', 'orientation', 'smooth-manifolds', 'differential-geometry']"
4572071,Calculating angles for wood joinery,"This is an actual problem I have faced in woodworking and am now facing again, and figure I ought to understand how to think about this problem geometrically, which is the problem I'm having. I have some wood boards that are initially similar to those in the first picture. It would be simple to join them by cutting each end at a 45° angle. Boards while flat However, they will not be laying flat (that is, Z=0 for all bottom edges). The bottom face will be tilted up A° from the outside such that the inside edges are raised above the Z=0 plane. (A = 30° in the picture.) My goal is to cut the ends of the boards with the same angles such that the cut edges will join face to face. After the boards are tilted up, B cannot have been cut at 45° if the faces are the join. Also, the cut edge is no longer 90° from the face (like the other 3 edges on each board). How do I calculate B and C given an angle A? Essentially, the cut faces need to be parallel to the Z-axis. The purpose of this calculation is because the boards can only be held steady when laying flat and the saw blade must have its angles adjusted—but to what? Thanks for any help orienting my thinking about the steps for solving this problem! In other situations there would be more than 4 boards being joined, so the 45° would actually be another number (like 22.5° with 8 boards). Boards where the inside edges have been rotated up 30°",['geometry']
4572076,Are the functions linear?,"I want to check of the following functions are linear or not. $g:\mathbb{R}^2\to \mathbb{R}, \ (x,y)\mapsto x+3y$ $h:\mathbb{R}^3\to \mathbb{R}^2, \ (x,y,z)\mapsto (x+y+z, 2y-z)$ I have done the following : Let $(x_1,y_1), (x_2,y_2)\in \mathbb{R}^2$ . Then we have \begin{align*}g\left ((x_1,y_1)+(x_2,y_2)\right )&=g(x_1+x_2,y_1+y_2)\\ & =(x_1+x_2)+3(y_1+y_2)=(x_1+3y_1)+(x_2+3y_2) \\ & =g(x_1,y_1)+g(x_2,y_2)\end{align*} Let $\lambda\in \mathbb{R}$ . Then we have \begin{align*}g\left (\lambda (x,y)\right )&=g(\lambda x, \lambda y)\\ & =\lambda x+3\lambda y\\ & =\lambda (x+3y)\\ & =\lambda g(x,y)\end{align*} Therefore $g$ is linear. Let $(x_1,y_1,z_1), (x_2,y_2,z_2)\in \mathbb{R}^3$ . Then we have \begin{align*}h\left (c+(x_2,y_2,z_2)\right )&=h(x_1+x_2,y_1+y_2,z_1+z_2)\\ &=\left ((x_1+x_2)+(y_1+y_2)+(z_1+z_2), 2(y_1+y_2)-(z_1+z_2)\right )\\ & =\left ((x_1+y_1+z_1)+(x_2+y_2+z_2), (2y_1-z_1)+(2y_2-z_2)\right )\\ & =(x_1+y_1+z_1,2y_1-z_1)+(x_2+y_2+z_2,2y_2-z_2)\\ & =h(x_1,y_1,z_1)+h(x_2,y_2,z_2)\end{align*} Let $\lambda\in \mathbb{R}$ . Then we have \begin{align*}h\left (\lambda (x,y,z)\right ) & =h(\lambda x, \lambda y,\lambda h)\\ & =(\lambda x+\lambda y+\lambda z, 2\lambda y-\lambda z)\\ & =\lambda (x+y+z,2y-z)\\ & =\lambda h(x,y,z)\end{align*} Therefore $h$ is linear. Is everything correct? Or am I missing something?","['calculus', 'functions', 'solution-verification', 'homogeneous-equation']"
4572088,"Proofs that the sine function is strictly increasing on $[-\pi/2,\pi/2]$.","Here on the forum there are some proofs that the sine function is strictly increasing on $[-\pi/2,\pi/2]$ , but they all use the fact that $\sin '(x)=\cos(x)$ . Is there any rigorous proof without going through the derivative? (Define $\sin$ like a ratio between sides of a right triangle) Thanks in advance.","['calculus', 'trigonometry', 'real-analysis']"
4572091,"Evaluate $\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz$ where $\gamma = C(0,3)$ with positive orientation.","I need to evaluate $\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz$ where $\gamma = C(0,3)$ with positive orientation. We know that: $$\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz = 2 \int_{\gamma} \frac{ \sin(z)}{z^2 - 2z} dz + \int_{\gamma} \frac{e^z}{z^2 - 2z} dz = 2 \int_{\gamma} \frac{ \sin(z)}{z(z - 2)} dz + \int_{\gamma} \frac{e^z}{z(z - 2)} dz$$ To evaluate $2\int_{\gamma} \frac{ \sin(z)}{z(z-2)} dz$ we take: $$2\int_{\gamma} \frac{ \sin(z)}{z(z-2)} dz = 2 \int_{\gamma_1} \dfrac{ \sin(z)/z}{(z-2)}dz +  2\int_{\gamma_2} \dfrac{ \sin(z)/\{z-2\}}{z}dz $$ Where $\gamma_1$ is a circle around pole $z = 2$ and $\gamma_2$ is a circle around pole $z = 0$ .
Using Cauchy's Integral Formula: $$2 \int_{\gamma_1} \dfrac{ \sin(z)/z}{(z-2)}dz = 2 * 2 \pi i \frac{ \sin(2)}{2} = 2 \pi i \sin(2)$$ $$2\int_{\gamma_2} \dfrac{ \sin(z)/\{z-2\}}{z}dz = 2 * 2 \pi i \frac{ \sin(0)}{-2} = 0$$ To evaluate $\int_{\gamma} \frac{e^z}{z(z - 2)} dz$ we take: $$\int_{\gamma} \frac{e^z}{z(z - 2)} dz = \int_{\gamma_1} \dfrac{e^z/z}{(z-2)}dz + \int_{\gamma_2} \dfrac{e^z/\{z-2\}}{z}dz $$ Where $\gamma_1$ is a circle around pole $z = 2$ and $\gamma_2$ is a circle around pole $z = 0$ . Using Cauchy's Integral Formula: $$\int_{\gamma_1} \dfrac{e^z/z}{(z-2)}dz = 2 \pi i \frac{e^2}{2} = \pi i e^2$$ $$\int_{\gamma_2} \dfrac{e^z/\{z-2\}}{z}dz = 2 \pi i \frac{e^0}{-2} = -\pi i$$ Taking everything together we get: $$\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz = 2 \pi i \sin(2) + \pi i e^2 -\pi i$$ Is that correct? I did it just by looking in the textbook and examples so it may be all wrong.","['complex-analysis', 'complex-integration']"
4572099,Use boolean algebra to proof $(A\cap B)\Delta (C\cap D)\subseteq (A\Delta C)\cup (B\Delta D)$,"I'm trying to prove that $(A\cap B)\Delta  (C\cap D)\subseteq (A\Delta C)\cup (B\Delta D)$ and using the laws in set theory might be complicated. I think this can be somehow simplified by the boolean algebra like $1_A\oplus 1_B\leq 1_A \cdot 1_B$ and $1_{A\bigtriangleup B}=1_A\oplus 1_B$ but I'm stuck here. Similarly, can $(A\cup B)\Delta (C\cup D)\subseteq (A\Delta C)\cup (B\Delta D)$ reduced by boolean algebra, too? I can reduce them to $(1_A\cdot 1_B)\oplus (1_C\cdot 1_D)\leq (1_A\cdot 1_B)\cdot (1_C\cdot 1_D)$ and $(1_A\oplus 1_C)\cdot (1_B\oplus 1_D)\leq (1_A\cdot 1_C)\cdot (1_B \cdot 1_D)$ , but that doesn't help.","['elementary-set-theory', 'boolean-algebra']"
4572102,Changing summation order,"Let $N,M,R \geq 0$ be positive integers with $N \leq M$ . I want to rewrite a summation of the form $$\sum_{i=0}^R \sum_{j=N}^M \sum_{\ell=0}^{M+i-j} \lambda_{i,j,\ell} $$ as $$\sum_{\ell =0}^{M+R-N} \sum_{?=?}^? \sum_{?=?}^? \lambda_{i,j,\ell}.$$ I have only been able to do so by splitting the sum in four blocks, namely by rewriting the sum as something like $$\sum_{\ell =?}^{?} \sum_{?=?}^? \sum_{?=?}^? \lambda_{i,j,\ell} + \sum_{\ell =?}^{?} \sum_{?=?}^? \sum_{?=?}^? \lambda_{i,j,\ell} + \sum_{\ell =?}^{?} \sum_{?=?}^? \sum_{?=?}^? \lambda_{i,j,\ell} + \sum_{\ell =?}^{?} \sum_{?=?}^? \sum_{?=?}^? \lambda_{i,j,\ell}$$ but this is far from being useful for my purposes.","['summation', 'combinatorics']"
4572106,If $f$ is a function from $X$ to $Y$ then does the equality $f[X]\setminus f[X\setminus U]=f[U]$ holds for any $U\in\mathcal P(X)$?,"Given a function $f$ from $X$ to $Y$ I am try to prove or to disprove the equality $$
\tag{1}\label{1}f[X]\setminus f[X\setminus U]=f[U]
$$ where $U\in\mathcal P(X)$ . So I surely know thath for any $U,V\in\mathcal P(X)$ the euquality $$
f[U]\setminus f[V]\subseteq f[U\setminus V]
$$ holds so that clearly the inclusion $$
f[X]\setminus f[X\setminus U]\subseteq f\big[X\setminus(X\setminus U)\big]=f[U]
$$ holds. Now I am not to able to prove because if $f$ is not injective it is possibile that $f[U]$ and $f[X\setminus U]$ are not disjoint so that I suspect that $\eqref{1}$ does not holds but unfortunately I was not able to find a counterexample so that I thought to put a specific question where I ask to prove or disprove the equality. So could someone help me, please?","['elementary-set-theory', 'examples-counterexamples']"
4572164,Why is the union of all proper subgroups of G equal to G?,"""Let G be a non-trivial and non-cyclic Group and let T be the union of all proper subgroups of G. Prove that G=T."" I'm a bit stuck on this problem... My first idea was that : $\forall g \in G$ , we can search a subgroup $H \subseteq G$ such that $g \in H$ . But I don't really know how to get this done, or even if this is a good way of starting the problem... Note to clarify : G can be infinite and a proper subgroup cannot be equal to G. (I'm specifying this because I saw on wikipedia that some people use ""proper"" the same way as ""non-trivial"".)","['group-theory', 'cyclic-groups']"
4572180,If $f$ is a function from $X$ to $Y$ then does the inclusion $f^{-1}\big[Y\setminus f[X\setminus U]\big]\subseteq U$ hold for any $U\in\mathcal P(X)$?,"Given a function $f$ from $X$ to $Y$ I am trying to prove or to disprove the inclusion $$
\tag{1}\label{1}f^{-1}\big[Y\setminus f[X\setminus U]\big]\subseteq U
$$ where $U\in\mathcal P(X)$ . So I observe that if $\eqref{1}$ did not hold then by the inclusion $$
f\Big[f^{-1}\big[Y\setminus f[X\setminus U]\big]\Big]\subseteq Y\setminus f[X\setminus U]
$$ there would be exists $x\in X\setminus U$ such that $f(x)$ lies in $Y\setminus  f[X\setminus U]$ but obviously $f(x)$ lies even in $f[X\setminus U]$ so that we conclude $\eqref{1}$ must holds. I understand that this question could seem trivial but I need $\eqref{1}$ to prove another result and I decided to not put directely a question about that result because I want try to prove it by my self as exercise. So does $\eqref{1}$ holds? could someone help me, please?","['elementary-set-theory', 'examples-counterexamples']"
4572181,inner products on polynomials,"So, I was studying about orthogonal polynomials and saw general examples of inner products on $\mathbb{R}[x]$ , mostly of the forms $$\langle f,g\rangle=\int f(x)g(x)q(x)dx,$$ for some kind of density $q$ ; and an inner product of the form $$\langle f,g\rangle=\sum_{k=1}^n f(x_k)g(x_k),$$ for some fixed set of $x_1,...,x_n$ (mostly the eigenvalues of some matrix). As it happens, in both cases we have that the product is of the form $$\langle f,g\rangle=\int fgd\mu,$$ for some measure $\mu$ defined on the borelians (the first case the absolut continuous $d\mu =qdm$ and the second the dirac measure supported on $x_j$ ) My intuition is that for all cases there should be some measure, which we can define from the inner product. This intuition comes from Riez-Markov theorem, but there is no topological structure to be used here, so I cannot use it. so the question is: Is it true that every positive semi-definide inner product on $\mathbb{R}[x]$ is of the kind $$\langle f,g\rangle=\int fgd\mu,$$ for some measure $\mu$ that can be determined by the inner product? If not I would really apreciate a counterexample, as I am new to this area...","['measure-theory', 'linear-algebra', 'functional-analysis', 'polynomials', 'orthogonal-polynomials']"
4572184,Lagrange Multipliers. What do I do if the Lagrange multiplier is 0 or the gradient of the constraint is 0?,"Consider the example below. Here, We found that $\nabla g$ is only zero at $(0, 0)$ but we see that the point $(0,0)$ does not satisfy $g(x,y) = 0$ . But I don't understand why we need to ensure that that the $\nabla g \ne 0$ . What would it mean if $\nabla g = 0$ satisfies the constraint? What would I need to do in this case? How would my steps change? Also, later they mention that $\lambda$ is not equal to $0$ since it would lead to a contradiction if it did. Once again, I don't understand why we need to make sure that $\lambda \ne 0$ . Why do we need to make that step? What happens if $\lambda$ can be zero? What would that mean? What measures should I take if that happens? In brief, I'm asking what do I do if $\nabla g = 0$ satisfies the constraint or $\lambda = 0$ .","['lagrange-multiplier', 'multivariable-calculus', 'calculus', 'optimization', 'constraints']"
4572215,The number of functions $f: \mathcal{P}(A) \to \mathcal{P}(B)$ for which $f(X) \cap f(Y) = f(X \cap Y)$,"Let $A$ and $B$ two finite sets of natural numbers and by $\mathcal{P}(X)$ we denote the set of all subsets of a given set $X$ . Find the number of functions $f: \mathcal{P}(A) \to \mathcal{P}(B)$ , for which $f(X) \cap f(Y) = f(X \cap Y)$ . I think that I might associate each subset of $A$ with its product of elements, while considering, without loss of generality, that each of the elements are prime numbers. This equivalence might be seen as the number of injections (from the identity consideration) between a set of $n$ prime numbers and the set of all possible products of these prime numbers, which are at most $2^n - 1$ elements. So, the number would be: $$N = \prod_{i = 1}^n (2^n - i)$$ Is this a correct reasoning?","['elementary-set-theory', 'functions', 'combinatorics', 'products']"
4572221,"Conjecture: There always exist $k\in \Bbb N$, such that $m^k\equiv k\pmod n$, where $m,n\in\Bbb N.$","Conjecture: Let $m,n\in\Bbb N$ . Then there always exists $k\in \Bbb N$ , such that $m^k\equiv k\pmod n$ holds. This question comes from here. Let $m,n\in\Bbb Z_{>0}$ are fixed numbers, such that $$m^{k_0}-k_0\equiv 0\pmod n$$ for some $k_0\in \Bbb N$ .
Then, there exist infinitely many $k$ , such that $$m^k-k\equiv 0\pmod n.$$ We proved that, if there exist such $k$ , then there exist infinitely many $k$ . But here, the question is: Does there always exist the smallest $k$ ? Numerical results support the conjecture. The question seems beyond of my elementary knowledge, so I don't have a non-trivial attempt, unfortunately.","['number-theory', 'conjectures']"
4572269,Version of Helly theorem in the plane,"This is a question from the book lectures on Discrete Geometry: Let $C_1, \dots, C_n$ be convex sets in the plane such that each 4-tuple of them contains a ray in the intersection. Prove that $\bigcap_{i=1}^n C_i contains a ray. My (wrong I think) idea how to solve it is the following: We proceed by induction on $n\ge 4$ . Base case is obvious so we will show that $n\implies n+1$ . Let $a_i + tr_i, t\ge0$ be the ray contained in $\bigcap_{j \neq i}C_j$ . This gives us a set of  at least 5 vectors in $\mathbb{R}^4$ $\{(a_i, r_i): i\ge 0\}$ . These vectors are linearly independent so we can split $[n]$ into disjoint sets $A,B$ such that $\sum_{i\in A}a_i \lambda_i = \sum_{j \in B}a_j\mu_j$ and similarly for $r_i$ From here my idea was to show that ray $\sum_{i\in A}a_i \lambda_i + t\sum_{i\in A}r_i \lambda_i$ will be in the intersection but I am not sure if this works, particularly if the sums above are zero which can happen it shouldn't work. Thus I am interested in if my solution can be fixed or finding alternate solutions to this problem.","['discrete-geometry', 'geometry', 'combinatorial-geometry', 'combinatorics', 'discrete-mathematics']"
4572312,Sum of an irreducible representation.,"I am trying to prove the following statement: Let $\mathfrak{X}$ be an irreducible $F$ -representation of $G$ over an arbitrary field. Show that $\sum\limits_{g\in G}\mathfrak{X}(g) = 0$ unless $\mathfrak{X}$ is the principal representation. I know that there are many different proofs of this fact, but I wanted to prove this statement on my own, and I have not yet seen a proof similar to mine (although I may be wrong). But is this proof correct? Unfortunately, as it has already turned out, this proof doesn't work for an arbitrary field, but it is probably true for a field with characteristic $0$ !!! Proof: Let $\mathfrak{X}$ be the principal representation, then by definition $\forall g\in G\Rightarrow \mathfrak{X}(g) = 1_{F}$ . Then $$\sum\limits_{g\in G}\mathfrak{X}(g) = \sum\limits_{g\in G}1_{F} =|G|\cdot 1_{F}$$ Now let $\mathfrak{X}$ not be the principal representation. Let $T = \sum\limits_{g\in G}\mathfrak{X}(g)$ . Note that $\forall a \in G$ $$\mathfrak{X}(a)T = \mathfrak{X}(a)\sum\limits_{g\in G}\mathfrak{X}(g) = \sum\limits_{g\in G}\mathfrak{X}(ag) = T.$$ Then it follows that $T^2 = |G|\cdot T$ . Then the polynomial $x^2 -|G|x$ is annulling for $T$ . Then the Jordan form of $T$ (up to permutations of Jordan cells) can take the following forms: Either $|G|\cdot I$ ; or zero matrix; or a matrix with several units diagonally (not completely filling it), and the rest of all values are zero, and multiplied by $|G|$ . Note that only the zero matrix among all these types of matrices has a trace equal to zero! Let $\chi$ be the character associated with $\mathfrak{X}$ . Consider the following expression $$S = \sum\limits_{g\in G}\chi(g) = [\chi, 1_G] = 0.$$ The last equality holds because $\chi\in Irr(G)$ and $\chi \neq 1_G$ . Since $\chi(g) = tr(\mathfrak{X}(g))$ and $tr(A + B) = tr(A) + tr(B)$ for any matrices $A$ and $B$ , we get that $S = tr(T) = 0$ . But only the zero matrix among the possible candidates for the role of $T$ had a zero trace. So $T = 0$ , which was required to be proved. $\blacksquare$","['representation-theory', 'group-theory', 'abstract-algebra', 'solution-verification']"
4572321,change variable integral over a ball,"If I have the integral $$
\int_{\partial B(x_0,r)}f(z)\,d\sigma(z)
$$ over a open ball of center $x_0$ and radius $r$ , how can 'move' this integral to the open ball $B(x,\tilde{r})$ , where $\tilde{r}>0$ , I mean $$
\int_{\partial B(x_0,r)}f(z)\,d\sigma(z)\to\int_{\partial B(x,\tilde{r})}f(\tilde{z})\,d\sigma(\tilde{z})
$$ My approach was take the change of variable $z\to z-x_0+x$ , then $$
\int_{\partial B(x_0,r)}f(z)\,d\sigma(z)\to\int_{\partial B(x,r)}f(z-x_0+x)\,d\sigma(z)
$$ this change the center of the ball, but I don't know this is correct. Any hint will be appreciated. Thanks!","['multivariable-calculus', 'calculus', 'change-of-variable']"
4572392,"Spivak, Ch. 23, ""Infinite Series"", Problem 21c","The following problem is from Chapter 23 ""Infinite Series"" of Spivak's Calculus . Note that there are questions about convergence of the binomial series such as this one , but the current question concerns a specific proof. Namely, the one presented in the problem below. In this problem we will establish the ""binomial
series"" $$(1+x)^\alpha=\sum\limits_{k=0}^\infty \binom{\alpha}{k}x^k, |x|<1$$ for any $\alpha$ , by showing that $R_{n,0}(x)=0$ . The proof is in
several steps, and uses the Cauchy and Lagrange forms as found in
problem 10-21. (a) Use the ratio test to show that the series $\sum\limits_{k=0}^\infty \binom{\alpha}{k} r^k$ does indeed converge for  > $|r|<1$ . (b) Suppose first that $0\leq x<1$ . Show that $\lim\limits_{n\to\infty}
 R_{n,0}(x)=0$ , but using Lagrange's form of the remainder, noticing
that $(1+t)^{\alpha-n-1}\leq 1$ for $n+1>\alpha$ . (c) Now suppose that $-1<x<0$ ; the number $t$ in Cauchy's form of the
remainder satisfies $-1<x_t\leq 0$ . Show that $$|x(1+t)^{\alpha-1}|\leq |x|M, \text{ where }
 M=\max(1,(1+x)^{\alpha-1}),$$ and $$\left | \frac{x-t}{1+t} \right |=|x|\left ( \frac{1-t/x}{1+t} \right
 )\leq |x|\tag{1}$$ Using Cauchy's form of the remainder, and the fact that $$(n+1)\binom{\alpha}{n+1} = \alpha \binom{\alpha-1}{n}$$ show that $\lim\limits_{n\to\infty} R_{n,0}(x)=0$ I was able to prove (a) and (b) and most of (c), up to (1). The issue is proving the last part, namely that $\lim\limits_{n\to\infty} R_{n,0}(x)=0$ . The solution manual has the following $$|R_{n,0}(0)|=\left | (n+1)\binom{\alpha}{n+1}x(1+t)^{\alpha-1} \left(\frac{x-t}{1+t}\right )^n \right |\tag{2}$$ $$\leq |x\alpha M|\cdot \left |\binom{\alpha-1}{n}x^n\right | \to 0
 \text{ by part } a$$ First off, the $R_{n,0}(0)$ seems to be a typo. This would be the n-th order remainder evaluated at $0$ . What I have is the following $$R_{n,0}(x)=\binom{\alpha}{n+1}(1+t)^{\alpha-(n+1)}x^{n+1}$$ $$=\binom{\alpha}{n+1}\cdot x\cdot (1+t)^{\alpha-1}\cdot \left ( \frac{x}{1+t}\right )^n$$ How is (2) obtained?","['proof-explanation', 'calculus', 'sequences-and-series']"
4572405,"Generalise a function, $f\left(x\right)=x^4+ax^3+bx^2$ such that $f$ and $f'$ only has integer roots.","Consider a function, $f:\mathbb{R}\rightarrow \mathbb{R},\:f\left(x\right)=x^4+ax^3+bx^2$ where $a,b\in \mathbb{Z}\setminus \left\{0\right\}$ and $b\ne \left(\frac{a}{2}\right)^2$ with three distinct real roots whose roots and stationary points have integer $x$ coordinates. Generalise such a function, $f(x)$ . My initial thoughts were to factorise to enforce integer requirement such that $f\left(x\right)=x^2\left(x^2+ax+b\right)$ and finding $x^2+ax+b=\left(x-c\right)\left(x-d\right)$ for some $c,d\in \mathbb{Z}$ but that leads to complications when trying to enforce an integer requirement for $f'(x)$ Any help would be greatly appreciated.","['calculus', 'functions', 'algebra-precalculus']"
4572458,Distributional expression for $\lim_{\epsilon\to 0^+}\frac{1}{(\epsilon-it)^n}$?,"I have computationally verified that the following equation holds $$\lim_{\epsilon\to 0^+}\int_{-\infty}^{\infty}dt\frac{\phi(t)}{(\epsilon-it)^2}=i\pi\phi'(0)+PV\int_{-\infty}^\infty\frac{\phi(t)-\phi(0)}{t^2}dt$$ for various $\phi$ , where $\phi$ is a test function. I thought this would follow as a consequence of the well known distributional relation $$\lim_{\epsilon\to 0^+}\frac{1}{\epsilon-it}=\pi\delta(t)+iPV\frac{1}{t}$$ but since the two principal value prescriptions are different from each other, taking a derivative  of the last relation doesn't seem to yield the answer immediately. What is a good way to show this? I have a feeling that complex analysis could work here, and even help with showing the more general conjecture (I verified with Mathematica up to $n=4$ as well) $$i^{n-1}(n-1)!\lim_{\epsilon\to 0^+}\int_{-\infty}^{\infty}dt\frac{\phi(t)}{(\epsilon-it)^n}=(-1)^{n-1}\pi\phi^{(n-1)}(0)+iPV\int_{-\infty}^\infty\frac{\phi(t)-\sum_{k=0}^{n-2}\frac{\phi^{(k)}(0)}{k!}t^k}{t^n}dt$$","['integration', 'limits', 'improper-integrals', 'distribution-theory']"
4572471,Functionally complete set of a particular trivalued logic,"Consider the V-shaped partial order $\mathbf V$ . Denote the three elements $\mathsf F > \bot < \mathsf T$ . $\mathbb S_n$ is the set of n-ary connectives, that is, the set of monotonous functions $\mathbf V^n \to \mathbf V$ . (They do not need to preserve $\bot$ .)
Some of the connectives include: The ""strict"" $\dot\lor$ , that agrees with the boolean $\lor$ , and takes the value $\bot$ everywhere else. The ""parallel"" $\tilde\lor$ , where $x \mathop{\tilde\lor} \mathsf T = \mathsf T \mathop{\tilde\lor} x = \mathsf T$ , even if $x = \bot$ . The ""left-biased"" $\stackrel \leftarrow \lor$ , where $\bot \overleftarrow\lor \mathsf{T} = \bot$ , but $\mathsf T \overleftarrow\lor \bot = \mathsf T$ . This appears in many programming languages. The parallel majority function $\mathrm{maj}(x,y,z)$ , on boolean inputs, it takes the value of the majority. This is the source of some important counterexamples. There cannot be a function $f(x)$ so that $f(\bot) = \mathsf{T}$ , and otherwise $f(x) = \mathsf{F}$ , because this is not monotone. Is there a finite set of connectives such that all the other connectives can be expressed in terms of them? With some research it seems that Kleene's trivalued logic is close in spirit, but I haven't found any work that considers the monotone restriction. Some works on paraconsistent logic also has similar truth values, but they are sometimes too restrictive because of the need to eliminate the principle of explosion. If there is no such finite set, an alternative is to allow some higher-order connectives, i.e. the least-fixpoint operation, or the strictification operation. Is there any relevant paper on this matter?","['propositional-calculus', 'logic', 'combinatorics']"
4572509,Is the hypothesis that $V$ be finite dimensional needed in this exercise?,"I am confused if the hypothesis that $V$ be finite dimensional is required in this exercise as I never use that hypothesis in my proof. I have typed the exercise and my own attempted proof below. Exercise: Suppose $V$ is finite-dimensional, $T \in L(V)$ , and $v \in V$ with $v \ne 0$ .
Let $p$ be a nonzero polynomial of smallest degree such that $p(T)v = 0$ .
Prove that every zero of $p$ is an eigenvalue of $T$ . Proof: Let $\lambda$ be a zero of $p$ . Then there exists a polynomial $q$ such that $$p(z)=(z-\lambda)q(z)$$ Evaluating $p(T)$ for $v$ we get $$p(T)v=((T-\lambda I)q(T))v=0$$ Given that $p$ is the polynomial of the smallest degree that satisfies $p(T)v=0$ , we have that $q(T)v\ne 0$ as $\deg q <\deg p$ . Then the above equation implies that $$(T-\lambda I)(q(T)v)=0$$ Because $q(T)v\ne 0$ , the above equation implies that $T-\lambda I$ is not injective. Which is equivalent to $\lambda$ being an eigenvalue of $T$ . I am not sure where I use the hypothesis that $V$ is finite dimensional. The condition that $$\text{$\lambda$ is an eigenvalue of $T\iff T-\lambda I$ is not injective}$$ is true on any vector space and not just for finite dimensional vector spaces. Am I correct in believing that the hypothesis that $V$ be finite dimensional is not needed?","['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
4572517,Commutative matrix multiplication,"Given two 3x3 matrix: $$
V=
\begin{bmatrix}
1 & 0 & 9 \cr
6 & 4 & -18 \cr
-3 & 0 & 13 \cr
\end{bmatrix}\quad
W=
\begin{bmatrix}
13 & 9 & 3 \cr
-14 & -8 & 2 \cr
5 & 3 & -1 \cr
\end{bmatrix} 
$$ Is there any way to predict that $ V * W = W * V $ without actually calculating both multiplications",['matrices']
4572539,Odd Polynomial approximation of $\frac{1}{x}$ as a polynomial around $0$,"The taylor approximation of $\frac{1}{x}$ does not exist centered at $x = 0$ , as the function is not continuous there. However, there does exist a formula: $$
\frac{1}{1-x} = \sum_{n=0}^\infty x^n\\
\Rightarrow \frac{1}{1 - (1 - x)} = \sum_{n=0}^\infty (1 - x)^n = \frac{1}{x}
$$ However, this function is not of a definite parity (i.e.) $f(-x) \neq -f(x)$ and $f(x) \neq f(-x)$ . This is a characteristic of the original function $\frac{1}{x}$ . Is there a polynomial approximation, that maybe looks something like the below sketch? This functions is odd and seems plausible to make. I just cannot find it online. Is there any function like what I drew? Again, I am not looking for a taylor approximation, as it won't exist. But perhaps an ad-hoc solution has been made?","['approximation', 'approximation-theory', 'continuity', 'functions', 'polynomials']"
4572581,Evaluate $\lim_{n\to \infty}{1+{1\over 2}+\cdots +{1\over n}\over (\pi ^n +e^n)^{1\over n}\log_e n}$,Evaluate $$\lim_{n\to \infty}{1+{1\over 2}+\cdots +{1\over n}\over (\pi ^n +e^n)^{1\over n}\log_e n}.$$ My attempt: $${1\over \pi}\lim_{n\to \infty}{1+{1\over 2}+\cdots +{1\over n}\over (1 +({e\over \pi})^n)^{1\over n}\log_e n}={1\over \pi}\lim_{n\to \infty}{1+{1\over 2}+\cdots +{1\over n}\over \log_e n}={1\over \pi}\times 1.$$ As $\lim_{n\to \infty}(1 +({e\over \pi})^n)^{1\over n}=1$ . Is there any mistake in this. If so please rectify this. Also any other way to solve this will be appreciated. Thanks in advance.,"['limits', 'solution-verification']"
4572610,"Follow-up to ""Prove $\mathbb{P}(\sup_{t \geq 0} M_t > x \mid \mathcal{F}_0)= 1 \wedge \frac{M_0}{x}$ for...""","I don't understand the accepted answer to the following question: Let $M$ be a positive, continuous martingale that converges a.s. to zero as $t$ tends to infinity. I now want to prove that for every $x>0$ $$ P\left( \sup_{t \geq 0 } M_t > x \mid \mathcal{F}_0 \right) = 1 \wedge \frac{M_0}{x}. $$ The accepted answer shows that $$\forall k:M_0 1_{\{M_0>x\}} + 1_{\{M_0 \leq x\}} x P(\tau < k \mid F_0) + E(M_k 1_{\{\tau \geq k\}} \mid F_0) = M_0$$ with $\tau := \inf\{t\geq 0 : M_t>x\}$ . It then says that we obtain the desired equation by considering the limit $k\to\infty$ and this is the part I don't understand. Edit : I think that I solved the problem thanks to a hint in the comments. It looks like the problem is much less trivial than I expected, maybe someone can confirm this: On the one hand, we have $$\lim_{k\to\infty}E(M_k 1_{\{\tau \geq k\}} \mid F_0)=0$$ since ""M converges to zero"" (DCT). Thus we obtain: $$M_0 1_{\{x<M_0\}} + 1_{\{M_0 \leq x\}} x P(\tau < \infty \mid F_0)= M_0$$ Since $0<x$ , this is equivalent to: $$ 1_{\{x<M_0\}}\frac{M_0}{x} + 1_{\{M_0 \leq x\}} P(\tau < \infty \mid F_0)=\frac{ M_0}{x}$$ And this is equivalent to: \begin{equation}\tag{1}
1_{\{M_0 \leq x\}} P(\tau < \infty \mid F_0)=\frac{ M_0}{x}-1_{\{M_0>x\}}\frac{M_0}{x} =1_{\{M_0\leq x\}}\frac{M_0}{x}
\end{equation} On the other hand, we can use $$1\wedge\frac{M_0}{x}=1_{\{x<M_0\}}+1_{\{M_0\leq x\}}\frac{M_0}{x}$$ and $$P(x<\sup_{t}M_t\mid F_0)=P(\tau<\infty\mid F_0)$$ (see the comments) to rewrite the desired result: $$P(\tau<\infty\mid F_0)=1_{\{x<M_0\}}+1_{\{M_0\leq x\}}\frac{M_0}{x}$$ But this equation follows from $(1)$ and the fact that $1_{\{x<M_0 \}} P(\tau < \infty \mid F_0)=1_{\{x<M_0 \}}$ : \begin{equation}
P(\tau < \infty \mid F_0)=1_{\{M_0 \leq x\}} P(\tau < \infty \mid F_0)+1_{\{x<M_0 \}} P(\tau < \infty \mid F_0)=1_{\{M_0 \leq x\}} \frac{M_0}{x}+1_{\{x<M_0 \}} 
\end{equation}","['conditional-expectation', 'martingales', 'solution-verification', 'probability-theory']"
4572626,"If $f$ is real-valued bounded measurable and $\mu$ a complex measure, then $\left | \int_X f \mathrm d \mu \right | \le \int_X |f| \mathrm d |\mu|$","Let $(X, \mathcal X)$ be a measurable space. Let $\mu$ be a complex measure on $X$ and $|\mu|$ its variation . Then $|\mu|$ is a non-negative finite measure. By definition, $|\mu(B)| \le |\mu| (B)$ for all $B \in \mathcal X$ . Let $\mu_1, \mu_2$ be the real and imaginary parts of $\mu$ . Then $\mu_1, \mu_2$ are finite signed measures such that $\mu = \mu_1 + i \mu_2$ . Let $(\mu_1^+, \mu_1^-)$ and $(\mu_2^+, \mu_2^-)$ be the Jordan decompositions of $\mu_1, \mu_2$ respectively. Then $\mu_1^+, \mu_1^-, \mu_2^+, \mu_2^-$ are non-negative finite measures such that $\mu_1 = \mu_1^+ - \mu_1^-$ and $\mu_2 = \mu_2^+ - \mu_2^-$ . Integration w.r.t. $\mu$ is defined as follows. If $f:X \to \mathbb R$ is measurable then $$
\begin{align}
\int_X f \mathrm d \mu &:= \int_X f \mathrm d \mu_1 + i \int_X f \mathrm d \mu_2 \\
&:= \left [ \int_X f \mathrm d \mu_1^+ - \int_X f \mathrm d \mu_1^-  \right ] + \left [ \int_X f \mathrm d \mu_2^+ - \int_X f \mathrm d \mu_2^- \right ], \quad (\star)
\end{align}
$$ provided that each integral in $(\star)$ is well-defined. If $f:X \to \mathbb C$ is measurable then $$
\int_X f \mathrm d \mu := \int_X (\operatorname{Re} f) \mathrm d \mu  + i \int_X (\operatorname{Im} f) \mathrm d \mu.
$$ In a proof of this result , I appealed to below inequality many times. Theorem: If $f:X \to \mathbb R$ measurable bounded, then $$
\left | \int_X f \mathrm d  \mu \right | \le \int_X |f| \mathrm d |\mu|
$$ As such, I would like to prove it. Could you have a check on my attempt? Proof: For convenience, let $\alpha$ be the value of the LHS and $\beta$ that of the RHS. Let $f = 1_B$ with $B \in \mathcal X$ . So $f$ is a characteristic function. We have $\alpha = |\mu(B)|$ and $\beta = |\mu| (B)$ . The claim then holds. Let $f = \sum_{i=1}^m b_i 1_{B_1}$ with $b_i \in \mathbb R$ and $B_i \in \mathcal X$ such that $B_i \cap B_j \neq \emptyset \iff i=j$ . So $f$ is a simple function. We have $\alpha = |\sum_{i=1}^m b_i \mu(B_i)|$ and $\beta = \sum_{i=1}^m |b_i| \cdot |\mu| (B_i)$ . The claim then holds thanks to triangle inequality and (1.) Let $f:X \to \mathbb R$ be measurable bounded. There is a sequence $(f_n)$ of simple functions such that $(f_n)$ is uniformly bounded and that $f_n \to f$ pointwise everywhere . By applying DCT for each term in $(\star)$ , we have $$
\alpha = \left | \lim_n \int_X f_n \mathrm d  \mu \right | = \lim_n \left |  \int_X f_n \mathrm d  \mu \right | .
$$ By (2.), we get $$
\alpha \le \lim_n \int_X |f_n| \mathrm d  |\mu|.
$$ By DCT again, we have $$
\lim_n \int_X |f_n| \mathrm d  |\mu| = \int_X |f| \mathrm d  |\mu| = \beta.
$$ This completes the proof.","['integration', 'measure-theory', 'inequality']"
4572645,"When can a double limit $\lim_{k\to\infty}\lim_{m\to\infty}f_{k,m}(x)$ be combined into $\lim_{k\to\infty}f_{k,k}(x)$?","Let $f_{k,m}:\mathbb{R}^n\to \mathbb{R}, \forall k, m \in \mathbb{N}$ and suppose that $\lim_{m\to\infty}f_{k,m} = f_{k,-}$ uniformly in $m$ and $\lim_{k\to\infty}f_{k,m} = f_{-,m}$ at least pointwise. Can I then combine the double limit $\lim_{k\to\infty}\lim_{m\to\infty}f_{k,m}$ to $\lim_{k\to\infty}\lim_{m\to\infty}f_{k,m} = \lim_{k\to\infty}f_{k,k}$ ? I already tried to search my analysis notes and books, but as this sort of limit combining is quite rare at least in standard curriculum (I cannot reside any that important theorem that combines limits), so I could not find anything useful. I am aware that under these assumptions conditions one can change the order of the limits of $\lim_{n\to\infty}\lim_{m\to\infty}f_{k,m}$ to $\lim_{m\to\infty}\lim_{k\to\infty}f_{k,m}$ . Edit: A guiding motivation to my question relates to convergence discussed in a problem such as this: Dense subspace of the space of functions vanishing at infinity , where one acceptable approach is this . If I understood that answer correctly, we are showing that we can approximate a continuous function $\tilde{f}_k$ with a compact support in the infinity norm with a sequence of smooth compactly supported functions $g_{k,m}$ . We are taking it as granted that we can approximate a continuous function $f$ vanishing at infinity with such $\tilde{f}_k$ s, so in the end we are approximating $f$ with a sequence that itself is approximated with some other functions. Therefore once wrote out, the entire expression is something like $$\lim_{k\to \infty}||f - \tilde{f}_k||_\infty = \lim_{k\to \infty}||f - \lim_{m\to\infty}g_{k,m}||_\infty$$","['real-analysis', 'functions', 'uniform-convergence', 'sequences-and-series', 'limits']"
4572680,Form the differential equations of the family of circles touching two given parallel lines $y=\pm a$,"Form the differential equations of the family of circles touching two given parallel lines $y=\pm a$ My Attempt: Let the family of circles be $(x-h)^2+y^2=a^2$ , with $(h,0)$ being the centre and $a$ being the radius. Differentiating w.r.t. x, we get, $$2(x-h)+2yy'=0\\ \implies yy'+x=h$$ Differentiating again, I get, $$yy''+(y')^2+1=0$$ But the answer given is $$y^2(1+(y')^2)=a^2$$","['calculus', 'derivatives', 'ordinary-differential-equations']"
4572764,Definition of upper semi-continuous functions: limsup or liminf?,"Notation: $\{f\geq c\}$ stands for $\{x\in x: fx\geq c\}$ . The standard definition of an upper semi-continuous function $f:X\to \bar{ \mathbb R}$ is: For each $c$ in $\mathbb R, \{f\geq c\}$ is closed Or equivalently For each net $x_a\to x, \limsup_a fx_a\leq fx$ It seems to me that the $ \limsup_a$ here can be substituted by $\liminf_a$ : For each net $x_a\to x, \liminf_a fx_a\leq fx$ or even by: For any $x_a\to x$ s.t. $fx_a$ converges, $\lim fx_a\leq fx$ . Of course 2) implies 3) and 4); FOR THE CONVERSE: Let $x_a$ be any net in $\{f\geq c\}$ with $x_a\to x$ , we have $fx\geq \liminf_afx_a\geq c$ , hence $\{f\geq c\}$ is closed. Is this true?","['limsup-and-liminf', 'semicontinuous-functions', 'continuity', 'functional-analysis', 'nets']"
4572793,If $\left | \frac{1}{\mu(A)} \int_A f \mathrm d \mu \right | \in G$ for all $A \in \mathcal A$ then $|f(x)| \in G$ for almost all $x \in X$,"Let $(X, \mathcal F, \mu)$ be a $\sigma$ -finite measure space and $(E, |\cdot|)$ a Banach space. Here we use the Bochner integral . I would like to prove the following result, i.e., Theorem Let $f \in L_1(X, \mu, E)$ , $\mathcal A := \{A \in \mathcal F :  \mu(A) \in (0, \infty)\}$ , and $$
\varphi(A) := \frac{1}{\mu(A)} \int_A f \mathrm d \mu \quad \forall A \in \mathcal A.
$$ Let $G$ be a non-empty closed subset of $\mathbb R$ . If $|\varphi (A)| \in G$ for all $A \in \mathcal A$ then $|f(x)| \in G$ for almost all $x \in X$ . Could you have a check on my attempt? Proof: We need the following related result, i.e., Lemma Let $f \in L_1(X, \mu, E)$ and $F$ be a non-empty closed subset of $E$ . If $$
\varphi(A) :=\frac{1}{\mu(A)} \int_A f \mathrm d \mu \in F \quad \forall A \in \mathcal F \text{ s.t. } \mu(A) \in (0, \infty),
$$ then $f(x) \in F$ for almost all $x \in X$ . Let $F := \{e \in E : |e| \in G\}$ . Because the norm is continuous and $F$ is closed, we get $G$ is closed. Also, $\varphi (A) \in F$ for all $A \in \mathcal A$ . By our Lemma , $f(x) \in F$ and thus $|f(x)| \in G$ for almost all $x \in X$ . This completes the proof.","['banach-spaces', 'measure-theory', 'bochner-spaces', 'functional-analysis']"
4572807,A proof of the polar decomposition of a complex measure,"Let $(X, \mathcal X)$ be a measurable space. Let $\mu$ be a complex measure on $X$ and $|\mu|$ its variation . Then $|\mu|$ is a non-negative finite measure. By definition, $|\mu(B)| \le |\mu| (B)$ for all $B \in \mathcal X$ , so $\mu \ll |\mu|$ . By Radon-Nikodym for complex measures, there is a measurable $f:X \to \mathbb C$ such that $$
\mu (B) = \int_B f \mathrm d |\mu| \quad \forall B \in \mathcal X.
$$ Now I would like the prove the polar decomposition of $\mu$ , i.e., Theorem: $|f(x)|=1$ for $|\mu|$ -a.e. $x \in X$ . Could you have a check on my attempt? Proof: For $B \in \mathcal X$ , let $\Pi(B)$ be the collection of all finite measurable partitions of $B$ . For $B \in \mathcal X$ , we have $$
\begin{align}
|\mu| (B) &= \sup \bigg  \{ \sum_{i=1}^m |\mu (B_i)| : (B_i)_{i=1}^m \in \Pi(B) \bigg\} \\
&= \sup \bigg  \{ \sum_{i=1}^m \left | \int_{B_i} f \mathrm d |\mu|  \right | : (B_i)_{i=1}^m \in \Pi(B) \bigg\} \\
&\le \sup \bigg  \{ \sum_{i=1}^m \int_{B_i} |f| \mathrm d |\mu| : (B_i)_{i=1}^m \in \Pi(B) \bigg\} \\
&= \int_B |f| \mathrm d |\mu|.
\end{align}
$$ This implies $$
\int_B \mathrm d |\mu| \le \int_B |f| \mathrm d |\mu| \quad \forall B \in \mathcal X,
$$ and thus $|f(x)| \ge 1$ for $|\mu|$ -a.e. $x \in X$ . For the reverse inequality, we need the following lemma, i.e., Lemma Let $(X, \mathcal F, \mu)$ be a $\sigma$ -finite measure space and $(E, |\cdot|_E)$ a Banach space. Here we use the Bochner integral . Let $f \in L_1(X, \mu, E)$ , $\mathcal A := \{A \in \mathcal F :  \mu(A) \in (0, \infty)\}$ , and $$
\varphi(A) := \frac{1}{\mu(A)} \int_A f \mathrm d \mu \quad \forall A \in \mathcal A.
$$ Let $G$ be a non-empty closed subset of $\mathbb R$ . If $|\varphi (A)|_E \in G$ for all $A \in \mathcal A$ then $|f(x)|_E \in G$ for almost all $x \in X$ . Notice that we have $|\mu(B)| \le |\mu| (B)$ and thus $$
\left | \frac{1}{|\mu| (B)}\int_B f \mathrm d |\mu| \right | \le 1
$$ for all $B \in \mathcal X$ such that $|\mu|(B) \neq 0$ . By our Lemma , $|f(x)| \le 1$ for $|\mu|$ -a.e. $x \in X$ . This completes the proof.","['measure-theory', 'radon-nikodym']"
4572922,"Trigonometry, how to order law of sine/cosine operations to get a minimally convoluted term","Given is this seemingly trivial problem: Two elevations (""mountains"") on a sphere with radius r, height normals H1, H2 with ""summit points"" G1 and G2, respectively. Their distance D is given as the length of the great circle arc length between their base points. How tall can a third elevation H (which is located on the arc between the basepoints of H1 and H2) at an arc length distance d from H1 become, before it obstructs G2 as seen from G1 or vice-versa? Assumptions made: Simplified the problem to two dimensions Heights are surface normals so they originate from, and intersect at, the circle center 0 < d < D No obstruction between G1 and G2 by the surface arc alone (no negative values for H) Drawing: This is how far I got before I realized that this approach must be somehow flawed: The central angle between H1 and H2 in radians is φ = D / r The central angle between H1 and H in radians is β = d / r The radius is just a constant, thus we assume h1 = H1 + r h2 = H2 + r Distance G between summit points G1 and G2 with law of cosines: G² = h1² + h2² - 2 * h1 * h2 * cos(φ) Angle α as difference to 180° (π rad) of the angle between line g and H2 with law of sine α = π - φ - arcsin((h1 * sin(φ)) / G) Angle γ similarly from the triangle h1_h_g γ = π - α - β Missing length h (Hx+r) with law of sines: h = (h1 * sin(α)) / sin(γ) Finally, subtract the constant radius H(d) = h(d) – r When I attempted to find a formula for the unknown height H as a function H = f(H1, H2, D, d), somewhere between steps 7 and 8, I must have painted myself in a corner. In particular 4, the G² term looks like it would fit without taking the square root, but does not. The term that I get from resubstituting the parts is utterly convoluted. There has to be an easier way to simplify the law-of-cosine term. Or I'm on the wrong track altogether?","['trigonometry', 'arc-length']"
4572929,"Random walk's leading, asymptotic haven't-returned-to-origin probability","In $1d$ and $2d$ , the probability that a simple random walk of length $n$ never returns to the origin asymptotes to $0$ as $n \to \infty$ . What is the leading asymptotic behavior of this decay at large $n$ ? A nice proof of recurrence in $1d$ and $2d$ is given here , where it's shown that the expected number of returns to the origin is, up to multiplicative constants, $\sim \sqrt{n}$ and $\sim \log(n)$ in $1d$ and $2d$ respectively. With an asymptotically infinite number of returns, the probability of returning must be $1$ . I'm tempted that these expected number of returns roughly point to the asymptotic probability of never returning to the origin in $n$ steps. That is, I anticipate that, up to multiplicative constants, the probability of never returning in a walk of $n$ steps in $1d$ and $2d$ is $\sim \frac{1}{\sqrt{n}}$ and $\sim \frac{1}{\log(n)}$ respectively. This is just a guess, but I show below the $1d$ guess is in fact correct. One can show that in $1d$ , the guess of $\sim \frac{1}{\sqrt{n}}$ up to multiplicative factors is correct for the probability of no returns in $n$ steps. One can find the entire distribution of the number of returns in time $2n$ in $1d$ ; the probability of $k$ returns is $\frac{\binom{2n-k}{n-k}}{2^{2n}}2^k$ . The probability of no returns in $2n$ steps is then $\frac{\binom{2n}{n}}{2^{2n}}$ (which is curiously the probability of returning on exactly the $2n$ th step); this probability goes as $\sim \frac{1}{\sqrt{\pi n}}$ , confirming the guess. Thus, only the $2d$ case is left to investigate. At one point, I found a somewhat complicated recursion relation for the exact probability of never returning to the origin in $2n$ steps; I'm going to try to dig that up. However, I anticipate there are simpler methods of estimating the asymptotic probability of no return up to multiplicative constants that won't need to first pass through calculating the exact probability of never returning.","['random-walk', 'stochastic-processes', 'asymptotics', 'probability']"
4572941,Number of $5$-digit sequences with at least two $4$s.,"I am trying to figure out how many five-digit sequences (0-9)  have at least two fours in them. ex: 41234 or 44086 .....
Here's my understanding: Total number of possible sequences as $10^5 = 100000$ Total number of sequences without any 4: $9^5 = 59049$ . Total number of sequences with only one 4: $9^4 = 6561$ . Total number of sequences with at least two fours is : $100000 - (59049+6561)$ Is this correct?
Thanks in advance.","['combinatorics', 'discrete-mathematics']"
4572951,"Trouble with ""very simple"" lemma from Kenig.","In Carlos Kenig's 1986 paper Elliptic Boundary Value Problems on Lipschitz Domains , he uses (but does not prove) the following result which he says is ""very simple"". Fix $n\geq 2$ . Let $\lambda$ be the function satisfying $\lambda(0)=0$ and $\lambda'(t) = \frac{1}{(1+t^2)^{n/2}}$ . Let $\varphi:\mathbb{R}^{n-1}\to \mathbb{R}$ be a Lipschitz function and let $f\in C^\infty_c(\mathbb{R}^{n-1})$ . Then he claims that \begin{align*} 
&\lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{(\|x-y\|^2+|\varphi(x)-\varphi(y)|^2)^{n/2}} f(x)  \, dx\\ 
&= - \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}\lambda\left( \frac{\varphi(x)-\varphi(y)}{\|x-y\|} \right) \nabla f(x)\cdot(y-x) \, dx \qquad(*)
\end{align*} for each point $y\in \mathbb{R}^{n-1}$ where $\varphi$ is differentiable. This looks scary, not simple really, but I gave it a shot before asking here. Let $\epsilon>0$ and let $y\in \mathbb{R}^{n-1}$ be such that $\nabla\varphi(y)$ exists. Then \begin{align*}
&\int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{(\|x-y\|^2+|\varphi(x)-\varphi(y)|^2)^{n/2}} f(x)  \, dx \\
&= 
\int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|^n\left(1+\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right)^2\right)^{n/2}} f(x)  \, dx \\
&= \int_{\|x-y\|>\epsilon} \frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|}\frac{1}{\|x-y\|^{n-1}}\lambda'\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right) f(x)  \, dx.
\end{align*} With $y$ fixed, we define a function $g$ on $\mathbb{R}^{n-1}\setminus\{y\}$ by $$ g(x) = \frac{\varphi(x)-\varphi(y)}{\|x-y\|}. $$ Notice that for almost every $x\in \mathbb{R}^{n-1}$ we have $$ \partial _jg(x) = \frac{\partial_j\varphi(x)\|x-y\|-(\varphi(x)-\varphi(y))\frac{x_j-y_j}{\|x-y\|}}{\|x-y\|^2} = \frac{\partial_i\varphi(x)}{\|x-y\|} - (\varphi(x)-\varphi(y))\frac{x_j-y_j}{\|x-y\|^3} $$ which implies $$ \nabla g(x)\cdot(x-y) = \frac{\nabla\varphi(x)}{\|x-y\|}\cdot(x-y) - (\varphi(x)-\varphi(y))\frac{\|x-y\|^2}{\|x-y\|^3} = \frac{\nabla \varphi(x)(x-y) - \varphi(x) + \varphi(y)}{\|x-y\|}. $$ Hence $$ \nabla(\lambda\circ g)(x)\cdot(y-x) = \lambda'\left(\frac{\varphi(x)-\varphi(y)}{\|x-y\|}\right)\frac{\varphi(y)-\varphi(x)-\nabla\varphi(x)\cdot(y-x)}{\|x-y\|} $$ by Chain Rule. Thus the left hand side of $(*)$ is $$ \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}\nabla(\lambda\circ g)(x)\cdot(y-x) f(x)  \, dx  $$ and the right hand side is $$ - \lim_{\epsilon\to 0 } \int_{\|x-y\|>\epsilon} \frac{1}{\|x-y\|^{n-1}}(\lambda\circ g)(x)  \nabla f(x)\cdot(y-x) \, dx. $$ However, I am now stuck. I am guessing there should be an integration by parts to make the derivative on $\lambda\circ g$ disappear, get the negative sign and get a derivative on $f$ . I can't make this work. Also I don't think I have used the fact that $\lambda(0)=0$ or that $\varphi$ is differentiable at $y$ yet.","['harmonic-analysis', 'singular-integrals', 'real-analysis']"
4573002,Smallest group containing $G$ with every automorphism of $G$ becomes inner,"What is the smallest group $\widetilde{G}$ containing $G$ and such that every automorphism of $G$ is induced by an inner automorphism of $\widetilde{G}$ ? Here are some of my thoughts.  For notation, let $G$ be a group and denote $\operatorname{Inn}(G)$ the group of inner automorphisms (i.e. those which are conjugation in $G$ ) and define $\operatorname{Out}(G) = \operatorname{Aut}(G)/ \operatorname{Inn}(G)$ . The holomorph $\operatorname{Hol}(G)$ is $G \rtimes_{\operatorname{id}} \operatorname{Aut}(G)$ has the desired property, but doesn't seem to be the smallest such group.  For instance, $\operatorname{Aut}(D_4) \cong D_4$ with $\operatorname{Out}(D_4) \cong C_2$ .  Then $\operatorname{Hol}(D_4)$ has order 64, while $D_8 \cong D_4 \rtimes \operatorname{Out}(D_4)$ is smaller, and every automorphism of $D_4$ is inner when we consider it as a subgroup of $D_8$ . Maybe the general construction should be $\widetilde{G} \cong \operatorname{Hol}(G) / \operatorname{Inn}(G)$ . Does it have a name?  Is there a reason the holomorph is the canonical choice, even if its bigger?","['automorphism-group', 'group-theory']"
4573004,Reduction of Order to Solve $y'' =-y^{3}$,"I want to solve $y'' +y^3 = 0$ with the boundary conditions $y(0) = a$ and $y(k) = b$ . My goal is to reduce this problem to $y' +y^2 = 0$ while solving but I'm not sure it can be done. I tried reduction of order substitutions (ie. taking $y' = w$ and $y'' = \frac{dw}{dy}y'$ ) but that did not work. Then I tried to solve in the following way $y'' y' = -y^3 y'$ $\frac{1}{2}[(y')^2]' = -[\frac{1}{4} y^4]'$ $\frac{1}{2}(y')^2 = -\frac{1}{4} y^4+C$ $(y')^2 = -\frac{1}{2} y^4+C$ $y' = \pm \sqrt{-\frac{1}{2} y^4+C}$ It seems to me if I take my original problem to be $y'' - 2y^3 = 0$ instead, I get $y' = \pm \sqrt{y^4+C}$ . If $C=0$ , this would reduce to $y' - y^2 = 0$ , which is close enough to what I want for my purposes. But I'm not sure how to get $C=0$ without a condition on the derivative, so maybe this was the wrong way to go. Can I reduce my original problem, $y'' +y^3 = 0$ , to $y' +y^2 = 0$ ? Where do my boundary conditions come into play?","['reduction-of-order-ode', 'boundary-value-problem', 'ordinary-differential-equations']"
4573020,Changing a r.v. on a set of measure zero leaves it measurable,"This is a claim that I thought and wanted to ask for a check. Claim. Take $X$ a real r.v. on a measure space $(\Omega,\Sigma,P)$ , with $\Sigma$ complete. Take a set of measure zero $U$ and define $Y$ s.t.: $Y(\omega)=X(\omega)$ if $\omega \in U^{c}$ $Y(\omega)$ arbitrary if $\omega \in U$ Than $Y$ is also a r.v.. Proof. We just have to check that $Y$ is measurable. To do this we fix $B$ a borel set and check if $Y^{-1}(B) \in \Sigma$ . We note that : $Y^{-1}(B)=\{ Y^{-1}(B) \cap U\} \cup \{ Y^{-1}(B) \cap U^{c}\}=\{ Y^{-1}(B) \cap U\} \cup \{ X^{-1}(B) \cap U^{c}\}$ Now: $X^{-1}(B)$ is measurable $U$ is measurable, otherwise in the claim we could not even say Take a set of measure zero $U$ . Therefore also $U^c$ is measurable. $\{ Y^{-1}(B) \cap U\} \subset$ U and since the sigma algebra is complete this is measurable. Therefore $Y$ is measurable. Is my proof/claim correct, and is the completeness condition necessary ?","['self-learning', 'measure-theory', 'solution-verification', 'random-variables']"
4573114,"If $X$ is infinite-dimensional normed vector space, then $\mathcal C_c(X) = \{0\}$","Let $(X, |\cdot|)$ be a normed vector space. For $f:X \to \mathbb R$ , the support of $f$ is defined as the closure of $\{x \in X : f(x) \neq 0\}$ . Let $\mathcal C_c(X)$ be the space of all real-valued continuous functions on $X$ with compact supports. Theorem If $X$ is infinite-dimensional, then $\mathcal C_c(X) = \{0\}$ . I'm trying to prove this well-known result. Could you have a check on my attempt? My attempt: Fix $f \in \mathcal C_b(X)$ . If $\{x \in X : f(x) \neq 0\} = \emptyset$ then we are done. Assume the contrary that there is $a \in X$ such that $f(a) \neq 0$ . By continuity of $f$ , there is an open ball $B(a, r)$ centered at $a$ with radius $r>0$ such that $f(x) \neq 0$ for all $x \in B(a, r)$ . We have $\overline{B(a, r)} \subset \operatorname{supp} f$ , so $\overline{B(a, r)}$ is compact. Lemma Let $(X, |\cdot|)$ be a normed vectpr space and $B:= \{x \in X \mid |x|\le1\}$ the closed unit ball. Then $B$ is compact if and only if $X$ is finite-dimensional. Notice that the closed unit ball $B$ is homeomorphic to a closed subset of $\overline{B(a, r)}$ , so $B$ is compact. By our Lemma , $X$ is finite-dimensional, which is a contradiction. This completes the proof.","['normed-spaces', 'functional-analysis']"
4573119,How to prove a bitstring structural induction problem,"A bitstring is a string consisting of only 0s and 1s. Define “·” to be the operation of concatenation, and let $\epsilon$ be the empty
bitstring. Consider the following recursive definition of the function
“count”, which counts the number of 1’s in the bitstring: • count $(\epsilon) = 0$ , • count $(s \cdot 1) = 1 +$ count( $s$ ), • count $(s \cdot 0) =$ count( $s$ ). How would I use structural induction to prove that count $(s \cdot t) =$ count( $s$ )+count( $t$ )? I know that the base case is $t = \epsilon$ , but have no idea what to do for the inductive step.","['induction', 'discrete-mathematics', 'bit-strings']"
4573167,"Given a closed set A, construct a continuously differentiable function that has A as its set of zeroes.","Edit: I answered my own question. Please let me know if there's an error, and I will fix it. If it's right, I'd appreciate it if you let me know that. First of all, does this function - $f(x) = inf\{(x-y)^2 : y \in A\}$ - work? I hope it does. Second, I'm trying to show this function is well-defined. Let $f(x) = inf\{(x-y)^2 : y \in A\}$ . We prove this function is well-defined. Grab an arbitrary $x$ . It is either in $A$ or not. If it's in $A$ , then $f(x) = 0$ because $f(x) = inf\{(x-y)^2 : y \in A\}$ , and we can let $y = x$ , so $f(x) = 0$ . If it's not in $A$ , then note that $(x-y)^2$ is smallest when $y$ is near $x$ , so take the $y$ such $|x-y|$ is the smallest. We know that such a $y$ exists because if not ... This is where I get stuck. Intuitively, I know that for closed sets, the endpoint is included in the set, and so we don't need to worry about points that get arbitrarily close to an endpoint but are not equal to an endpoint. Regarding the continuously differentiable part, I'm intuitively imagining that we have a bunch of zeros on the line with these half parabolas connecting them?","['general-topology', 'real-analysis']"
4573181,How can i proof by Mathematical Induction? $\sum_{i=1}^{n} (2i-1) = n²-1$ to n>=2,$\sum_{i=1}^{n} (2i-1) = n²-1$ to n>=2 Proof: first step: $\sum_{i=2}^{n=2} (2i-1) = (2*2)-1 = 3$ and $n²-1 = (2*2)-1=3$ so $\sum_{i=2}^{n=2} (2i-1) = n²-1$ $\sum_{i=1}^{k+1} (2i-1) = \sum_{i=1}^{k} (2i-1)+ 2(k+1)-1$ $=k²-1+2k+1$ $=k²+2k$ $=(k+1)²-1$ That its correctly?,"['induction', 'solution-verification', 'discrete-mathematics']"
4573213,Prove that $\dim \ker (A-I_{n}) = \frac{1}{d} \sum\limits_{i= 1}^{d}\mbox{Tr} \left(A^{i}\right)$,Let $A \in M_{n}(\mathbb{C})$ be a matrix such that $A^{d} = I_{n}$ for some positive integer $d$ . Prove that $$\dim \ker (A-I_{n}) = \frac{1}{d} \sum\limits_{i= 1}^{d}\mbox{Tr} \left(A^{i}\right)$$ My Attempt: $A$ is diagonalizable as minimal polynomial of $A$ is product of distinct linear factor. I think diagonalizability of $A$ can help to prove this identity. Can anyone help me for further approach?,"['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4573214,Fourier transform of the dipolar interaction,"I am not sure how to compute the following integral: $$
I=\int_{-\infty}^{+\infty} dx \, \frac{e^{ikx}}{(x^2 + a^2)^{\frac{3}{2}}}
$$ where $a$ is a real and positive parameter. I tried to use complex integration. However there are no simple poles due to the $3/2$ exponent.
At the moment I don't know how to proceed, so if you could help me I would really appreciate it. Thank you","['integration', 'complex-analysis', 'fourier-transform']"
4573237,Stein complex analysis exercise 4.12,"Let $f$ be a function on $\Bbb R$ that satisfies $$f(x) = O(e^{-\pi x^2})\quad\text{and}\quad\hat{f}(\xi) = O(e^{-\pi\xi^2}).$$ Then if $f$ is even then $\hat{f}$ extends to an even entire function. Moreover, if $g(z) = \hat{f}(z^{1/2})$ , then g satisfies $$|g(x)|\leq ce^{-\pi x}\quad\text{and}\quad |g(z)|\leq ce^{\pi R\sin^2(\theta/2)}\leq ce^{\pi|z|}$$ when $x\in\Bbb R$ and $z = Re^{i\theta}$ with $R\geq 0$ and $\theta\in\Bbb R$ . Question. Apply the Phragmen-Lindelof principle to the function $$F(z) = g(z)e^{\gamma z}\quad\text{where}\ \gamma = i\pi{e^{-i\pi/(2\beta)}\over \sin\pi/(2\beta)}$$ and the sector $0\leq\theta\leq\pi/\beta<\pi$ , and let $\beta\to\color{red}{1}$ to deduce that $e^{\pi z}g(z)$ is bounded in the closed upper half-plane. My attempt: We apply the Phragmen-Lindelof on the given sector (we don't rotate). First, if $z = re^{i\theta}$ then \begin{align*}
e^{\gamma z} &= \exp\left(i\pi{e^{-i\pi/(2\beta)}\over \sin\pi/(2\beta)} z\right)\\
i\pi{e^{-i\pi/(2\beta)}\over \sin\pi/(2\beta)}r(\cos\theta+i\sin\theta) & = i\pi{\cos(\pi/2\beta)-i\sin(\pi/2\beta)\over\sin(\pi/2\beta)}r(\cos\theta+i\sin\theta)\\
& \overset{\mathrm{\operatorname{Re}}}{=} {-\pi r\over\sin(\pi/2\beta)}(\cos(\pi/2\beta)\sin\theta-\sin(\pi/2\beta)\cos\theta)\\
& = -\pi r{\sin(\theta-\pi/2\beta)\over\sin(\pi/2\beta)}\\
\end{align*} Note that $0<\theta<\pi/\beta$ so $$-{\pi\over 2}<-{\pi\over 2\beta}<\theta-{\pi\over 2\beta}<{\pi\over 2\beta}<{\pi\over 2}$$ and $\sin$ is strictly increasing function on $(-\pi/2,\pi/2)$ . This implies ${\sin(\theta-\pi/2\beta)\over\sin(\pi/2\beta)}<1$ . But this is not good. $$|F(z)| = |g(z)e^{\gamma z}|=|g(z)|e^{\operatorname{Re}(\gamma z)} = ce^{\pi r(1-\epsilon)},\quad\epsilon<1,$$ by the above argument. So we can't apply P-L principle. Even if I can apply, I'm not sure how the function is bounded on the boundary of the given sector. I'm stuck here. Could you help?","['complex-analysis', 'fourier-analysis']"
4573267,Largest number of maximal chains in a finite poset,"What's the largest number of maximal chains contained within any poset with $n$ elements? My best guess is http://oeis.org/A000792 (take a partition $$\coprod A_i = P$$ and let $a < b$ if $a \in A_i, b \in A_j, i < j$ , then the number of maximal chains is the product of partition sizes) but I cannot prove it. This question was mistakenly posed in a module test without solution.","['graph-theory', 'relations', 'order-theory', 'combinatorics', 'discrete-mathematics']"
4573280,"The open embedding of topoi $\operatorname{Mod}^{a}_{R,I}\to\operatorname{Mod}_R$","All of the following comes from these notes by Bhatt . I apologize for my inexperience with topoi. Fix flat ideal $I\subset R$ such that $I^2=I$ . Define the category $\operatorname{Mod}^{a}_{R,I}$ of almost modules to be the Serre quotient of $\operatorname{Mod}_R$ by those $I$ -torsion $\operatorname{Mod}_{R/I}$ . Define the subcategory inclusion functor $j_!:\operatorname{Mod}^{a}_{R,I}\to\operatorname{Mod}_R$ ; the Serre quotient functor $j^*:\operatorname{Mod}_R\to\operatorname{Mod}^{a}_{R,I}$ ; and the coëxtension functor $j_*:\operatorname{Mod}^{a}_{R,I}\to\operatorname{Mod}_R:M\mapsto\operatorname{Hom}_R(I,M)$ . One notices two significant facts: There's an adjunction triple $j_!\dashv j^*\dashv j_*$ ; and The counit $f^* f_* \Rightarrow\operatorname{id}$ is an isomorphism; which induces one's wonder on if there's an open geometric embedding of (ringed) topoi for which objects of $\operatorname{Mod}_R$ and $\operatorname{Mod}^{a}_{R,I}$ are quasi-coherent modules. Also, recall (from SGA 4 or something, if someone can recall where it is please comment) that if one decomposes say a scheme $X$ into a pair of mutually complementary components $U,Z$ , one open, open closed, one has where the composition of any two consecutive morphisms in the same direction results in the zero functor. Surprisingly, one can ""decompose"" a topos over $\operatorname{Spec}R$ in a similar fashion: where the three functors on the last are given by the usual functors for change of rings . Moreover, Bhatt (pp. 21-22) and Gabber-Ramero (at p. 11) both mentioned that there supposes to exist an ""almost topos"" explaining these geometric phenomena; i.e., according to Bhatt, there supposed to be a ""open subscheme"" complementary to the closed $Z$ , the localization on which results in $\operatorname{Mod}_{R/I}$ . What is the ""almost topos""? Actually, localizing a topos should result in another topos? But how do I get a handle on the site, or specifically on what $\bar{U}$ is?","['arithmetic-geometry', 'topos-theory', 'algebraic-geometry', 'commutative-algebra']"
4573342,Cartier Divisors and Ideal sheaves (Huybrechts Complex Geometry),"I'm reading Complex Geometry-an introduction by Daniel Huybrechts . In section 2.3, he gives a group homomorphism $\mathcal{O}: \operatorname{Div}(X)\to \operatorname{Pic}(X)$ , and he proves that for any effective divisor $D\in \operatorname{Div}(X)$ , there exists a nonzero global section $s$ of $\mathcal{O}(Y)$ such that $Z(s)=D$ . Now let $Y$ be a irreducible hypersurface, then the section $s\in H^0(X,\mathcal{O}(Y))$ with $Z(s)=Y$ gives rise to a sheaf homomorphism $\mathcal{O}(X)\to \mathcal{O}(Y)$ and dually to $\mathcal{O}(-Y)\to \mathcal{O}_X$ . I think that this two homomorphisms are $f\mapsto fs$ and $t^*\mapsto t^*(S)$ locally. Then he gives Lemma 2.3.22: Lemma 2.3.22 The induced map $\mathcal{O}(-Y) \rightarrow \mathcal{O}_X$ is injective and the image is the ideal sheaf $\mathcal{I}$ of $Y \subset X$ of holomorphic functions vanishing on $Y$ . In the proof, he says that locally the map is given by multiplication with the equation defining $Y$ . I can't understand this claim and why it's injective.","['complex-geometry', 'algebraic-geometry', 'sheaf-theory']"
4573362,catenary equation with additional weigth,"Is there a trivial solution for the catenary equation with an additional weight attached to a given position on the rope? Like you would hang something from a rope at some given point and the cable weight can not be neglected. For example: Pole distance w: 10m Cable length l: 12m Distributed load Fl = 1N/mm Additional weight Fg = 300N Weights position x: 8.5m x is measured in the horizontal between the poles. My goal is not to find the exact position of the rope at every position, but to find the exact position of the added weight. As you can see in the image. The blue line from D to E is the rope, hanging without any attached weight with the length $l$ , the gray lines $i$ and $j$ represent a stage, where a weight is added at the point B assuming, that the rope is fully stretched, and the cable weight can be neglected. $l = i + j $ Now in the real world, the rope will not be fully stretched, if the weight is not big enough. So I would like to now how much this will reposition B with some given values. The blue line is given by the catenary equation $f(x) = \frac{e^{ax} + e^{-ax}}{2a}$",['geometry']
4573413,When can we continuously extend a linearly independent set to a basis?,"As we all know, any linearly independent subset of $\Bbb{R}^n$ (or vector spaces in general) can be extended to a (Hamel) basis. I'm wondering when this can be done continuously. To put it precisely: For which $m, n \in \Bbb{N}$ , where $m < n$ , does there exist a continuous map $\phi$ from the set of real $m \times n$ rectangular matrices of full row rank, to $\operatorname{GL}_n(\Bbb{R})$ , such that the submatrix made from the first $m$ rows of $\phi(A)$ is always $A$ ? My work so far: This is a generalisation of an older question that someone else asked, which I answered (twice). One of my answers shows that this cannot be done when $n$ is odd and $m = 1$ , using the hairy ball theorem. If such a map existed, then restriction to any row but the first would produce a continuous function mapping a non-zero vector to a linearly independent vector, which contradicts the hairy ball theorem. Of course, this doesn't work so well when $n$ is even. If $n$ is even, we can simply map $$(v_1, v_2, \ldots, v_{n-1}, v_n) \mapsto (v_2, -v_1, \ldots, v_n, -v_{n-1}),$$ i.e. swapping pairs of coordinates and negating one. As such, we always get a vector orthogonal to the original vector, and the map is clearly continuous. And, of course, one could simply pick other pairings as well, to get other vectors orthogonal to the original vector. However, there's no guarantee that we could build a whole basis from these maps! If $m = n - 1$ , then such a map exists. Indeed, all we need is a continuous map that takes $A$ and produces a vector linearly independent of all the rows. We can do that by generalising the cross product. We define: $$\psi(A) = \det\begin{pmatrix} \matrix{e_1 & e_2 & \ldots & e_n} \\ A \end{pmatrix},$$ where $e_1, \ldots, e_n$ are the standard basis vectors for $\Bbb{R}^n$ . We compute the ""determinant"" in much the same way as the usual mnemonic for the cross product: $$u \times v = \det \begin{pmatrix} \hat{i} & \hat{j} & \hat{k} \\ u_1 & u_2 & u_3 \\ v_1 & v_2 & v_3 \end{pmatrix} = \det\pmatrix{u_2 & u_3 \\ v_2 & v_3}\hat{i} - \det\pmatrix{u_1 & u_3 \\ v_1 & v_3}\hat{j} + \det\pmatrix{u_1 & u_2 \\ v_1 & v_2}\hat{k}.$$ This $\psi$ works in much the same way: taking $\psi(A) \cdot v$ for some $v \in \Bbb{R}^n$ , the result becomes $\det \pmatrix{v \\ A}$ , so if $v$ is already a row of $A$ , the result is $0$ . Thus, $\psi(A)$ is orthogonal to the rowspace of $A$ , and hence adding it as a row to $A$ will produce an invertible square matrix. Given the determinant is continuous, this is also a continuous choice. Searching yielded some other related posts , but both start with a one parameter family of vectors/rectnagular matrices, rather than considering a function on the full set of matrices. As such, they are asking something weaker. So, is there some characterisation of when such a map exists?","['matrices', 'linear-algebra']"
4573486,Prove that $\frac{\ln x}{x}+\frac{1}{e^x}<\frac{1}{2}$ for $x > 0$,"Given $x>0$ , prove that $$\frac{\ln x}{x}+\frac{1}{e^x}<\frac{1}{2}$$ I have tried to construct $F(x)=\frac{\ln x}{x}+\frac{1}{e^x}$ and find the derivative function of $F(x)$ to find the maximun value, but I can't solve the transcendental equation. So I tried another way.I tried to use the inequality $e^{-x}\le \frac{1}{x+1}$ (when $x>0$ ) to prove the inequality $\frac{\ln x}{x}<\frac{1}{2}-\frac{1}{e^x}$ , but I can't connect these two inequalities well, and I did't solve the problem in the end. How can I do?","['calculus', 'inequality', 'real-analysis']"
4573516,Quotient of algebraic group by the intersection of the kernels of all characters is diagonalisable,"Let $G$ be an algebraic group and let $H= \bigcap_{\chi \in X(G)} \ker{\chi} $ where $X(G)$ denotes the character group of $G$ . Now it is claimed in Humphreys Linear algebraic Groups exercise  16.12 that $G / H$ is diagonalisable. I am struggling to prove this statement. What I do know is that $G/H$ is necessarily abelian as commutators will be in the kernel of an arbitrary character and therefore it would be enough to show that any element of $G/H$ is diagonalisable. Also if I could show that $X(G/H)$ is finitely generated by $\chi_1,\dots , \chi_n $ say then $(G/H) $ is diagonalisable as I can then find an injective map  from $G/H$ to a Torus $T$ given by $\phi: G/H \mapsto T \ \phi(g)=(\chi_1(g), \dots , \chi_n(g))$ . However I cannot assume the statement that the character group of any algebraic group is finitely generated as I am trying to use this statement to get to that conclusion. Any help would be greatly appreciated.","['group-theory', 'algebraic-geometry', 'lie-algebras', 'algebraic-groups']"
4573539,What is the product of the areas of every regular polygon inscribed in a circle of area $1$?,"What is a closed form of $P=\prod\limits_{k=3}^{\infty}\frac{k}{2\pi}\sin{\left(\frac{2\pi}{k}\right)}\approx 0.05934871...$ ? This is the product of the areas of every regular polygon inscribed in a circle of area $1$ . I do not know how to evaluate this product. I have been trying to use complex numbers, to no avail. Remarks: $P$ is remarkably close to $\frac{1}{6\pi-2}\approx 0.0593487\color{red}{45}...$ But this is not a closed form, because the product decreases as the number of factors increases, and according to desmos, $$\prod\limits_{k=3}^{10^8}\frac{k}{2\pi}\sin{\left(\frac{2\pi}{k}\right)}<\frac{1}{6\pi-2}$$ There is a related question: What is the product of the circumferences of every regular polygon inscribed in a circle of circumference $1$ ? This is $P'=\prod\limits_{k=3}^{\infty}\frac{k}{\pi}\sin{\left(\frac{\pi}{k}\right)}\approx 0.51633595...$ The ratio of $P$ to $P'$ is $\prod\limits_{k=3}^{\infty}\cos{\left(\frac{\pi}{k}\right)}\approx 0.11494204...$ which is the Kepler-Boukamp constant and has no known closed form. But is there a closed form of $P$ ? EDIT The product of the areas of every odd -gon inscribed in a circle of area $1$ , equals $\frac{\pi}{2}\times$ the Kepler-Boukamp constant , as shown here .","['area', 'polygons', 'closed-form', 'infinite-product', 'limits']"
4573546,Angles in Isosceles Triangles,"Find the value of $x$ in the triangle. My attempt, I do know that because it is an isosceles triangle, so that $\angle ABC=\angle ACB$ Of course we can see that $y=3x$ and $z=5x$ so that $x=9$ by adding $12x+3x+5x=180$ But how do we know that that's the unique solution? Basically we can see that $$5x+y=3x+z$$ But how to know $y=3x$ and $z=5x$ are the only possible solution? Thanks in advance.","['triangles', 'algebra-precalculus', 'geometry']"
4573557,Link between sum of squares and sum of absolutes,"If I have a real-valued vector $\mathbf{x} = (x_1, \cdots, x_p)$ . Is there a way I can obtain $\sum_{i=1}^p \left|x_i \right|$ from $\sum_{i=1}^p \left|x_i \right|^2$ ?","['statistics', 'vectors', 'vector-spaces', 'real-analysis', 'linear-algebra']"
4573608,Do real quadratic number fields with prime discriminants have odd class numbers? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question Using the software SageMath I confirmed that there is no real quadratic number field of prime discriminant $D<10^6$ with an even class number. Is this only true for small discriminants or a general truth? Edit: Why was this question closed?","['algebraic-number-theory', 'discriminant', 'number-theory', 'class-field-theory', 'prime-numbers']"
4573615,Why do the ideas in Sheaf theory seem analogous to concepts of Manifold theory?,"I had this thought when studying ""Sheaf Theory through Examples"" by Daniel Rosiak. In the introduction, there is the exploration of how one can fit together data of different sensors observing the same space. Here is a picture from page-11, This system of mutually compatible local data assignments or “measurements” of the happenings on the space—where the various data assignments are, piece by piece, constrained by one another, and thereby patched together to supply an assignment over the entire space covered by the individual regions—is, in essence, what constitutes our sheaf. Hmm this feels like the motif we have in Differential Geometry where we chart Manifolds. And now consider this section about the motivation behind the name of Sheaf: Here one thinks of various regions as the parcels of an overall space covered by those pieces, the collection of which then serves as a site where certain happenings are held to take place, and the abstract sensors capturing local snapshots or measurements of all that is going on in each parcel are then regarded as being collected together into “stalks” of data, regarded as sitting over (or growing out of) the various parts of the ground space to which they are attached. A selection of a particular snapshot made from each of the intersecting regions) and collation (along unions of regions) of these sections captures how the various stalks of data are bound together. pg-12,13 And this seems like the concept of a bundle in Differential Geometry! What exactly is going on here? Is there a deep connection between Sheaf Theory and manifold Theory?","['manifolds', 'sheaf-theory', 'differential-geometry']"
4573628,Closed form of series with factorial-squared denominator?,"Does the following series have a closed-form expression: $$\sum_{k=0}^{\infty} \frac{z^k}{(k!)^2}$$ I know that it must converge because: $$\sum_{k=0}^{\infty} \frac{z^k}{k!} = e^z$$ and the $(k!)^2$ denominator obviously increases more quickly than the $k!$ denominator. This problem came up in computing the probability of a draw in a football match with each team's goal scoring modeled as a Poisson process. Thanks, John","['summation', 'factorial', 'poisson-distribution', 'closed-form', 'sequences-and-series']"
4573630,"If $A$ is a normed space, then ball $A$ is $\sigma(A^{**},A^{*})$ is dense in ball $A^{**}$.","I am reading Functional Analysis by Conway. The location of that theorem is Chapter V, Prop 4.1. $\sigma(A^{**},A^{*})$ means weak star topology on $A^{**}$ . The prove is as follows: Let $B$ be the $\sigma(A^{**},A^{*})$ -closure of ball $A$ in $A^{**}$ ; clearly, $B \subseteq \text{ball}\, A^{**}$ {because ball $A^{**}$ is weak-* compact by banach Alaoglu's theorem}. If there is an $x_0^{**}$ in ball $A^{**} \text{\B}$ , then the Hahn-Banach theorem implies there is an $x^*$ in $A$ , and $\alpha$ in $\mathbb R$ , and an $\epsilon >0$ such that $$\operatorname{Re}⟨x,x^*⟩<\alpha<\alpha+\epsilon< \operatorname{Re}⟨x^*,x_0^{**}⟩$$ for all $x$ in ball $A$ . And here I am unable to understand how to use Hahn-Banach theorem. What I understood:- First of all what is the meaning of $x^{**}$ belongs to B in the topology of $A^{**}$ . Here $\sigma(A^{**},A^*)$ is the topology such that the collection of evaluation maps $${\{\hat{x^*}:A^{**} \rightarrow \mathbb{C}:\hat{x^*}(x^{**})=x^{**}(x^{*}) \}}$$ are continuous.  Since $x_0^{**}$ not in B so there is no sequence $x_n$ in ball $A$ which converges to $x_0^{**}$ in $A^{**}$ with respect to $\sigma(A^{**},A^{*})$ topology i.e. for any sequence $x_n$ in $A$ there is some $x^*$ in $A^*$ such that $\hat{x^*}(\hat{x_n})$ does not converges to $x_0^{**}$ . That I am able to understand but unable to understand how to get such $x^*$ as in book.","['operator-theory', 'operator-algebras', 'functional-analysis', 'real-analysis']"
4573677,"Let $A_1, A_2, ..., A_k\in M_n(\mathbb{R})$ be symmetric matrices such that $A^2_1+A^2_2+...+A^2_k=0$. Prove that $A_i=0$ for every $i=1,2,...,k$.","Let $A_1, A_2, ..., A_k\in M_n(\mathbb{R})$ be symmetric matrices  such that $A^2_1+A^2_2+...+A^2_k=0$ . Prove that $A_i=0$ for every $i=1,2,...,k$ . My attempt: Let $B$ be a ortonormal base of $M_n(\mathbb{R})$ (this base exists becasue $M_n(\mathbb{R}$ ) has finite dimension), $v\in\mathbb{R}^n$ and $[T_i]_B=A_i$ for every $i=1,2,...,k$ , then $0=\left< (T^2_1+...+T^2_k)(v),v\right>=\left< T^2_1(v),v\right>+\left< T^2_2(v),v\right>+...+\left< T^2_k(v),v\right>=\left< T_1(v),T^*_1(v)\right>+\left<T_2(v),T^*_2(v) \right>+...+\left< T_k(v),T^*_k(v)\right>=\left< T_1(v),T^t_1(v)\right>+\left<T_2(v),T^t_2(v) \right>+...+\left< T_k(v),T^t_k(v)\right>=\left< T_1(v),T_1(v)\right>+\left<T_2(v),T_2(v) \right>+...+\left< T_k(v),T_k(v)\right>=||T_1(v)||^2+||T_2(v)||^2+...+||T_k(v)||^2$ . So, we need to have $||T_i(v)||^2=0$ for every $i=1,2,...,k$ ans so $T_i=0$ for every $i=1,2,...,k$ and $A_i=0$ for every $i=1,2,...,k$ . Is this correct? Thanks","['matrices', 'solution-verification', 'linear-algebra', 'vector-spaces']"
4573706,Showing that $\prod_{k=1}^{n} \left( 3 + 2\cos\left(\frac{2\pi}{n+1}k\right) \right)$ is the square of a Fibonacci number,"I was experimenting with products of the form $$\prod_{k=1}^{n} \left( a + b\cos(ck) \right)$$ when I found that the expression $$\prod_{k=1}^{n} \left( 3 + 2\cos\left(\frac{2\pi}{n+1}k\right) \right)$$ seems to always return a perfect square. (verified numerically). $$\left(3 + 2\cos\left(\frac{2\pi}{2}\right)\right) = 1 = F_2^2$$ $$\left(3 + 2\cos\left(\frac{2\pi}{3}\right)\right) \left(3 + 2\cos\left(\frac{4\pi}{3}\right)\right) = 4 = F_3^2$$ $$\left(3 + 2\cos\left(\frac{2\pi}{4}\right)\right)\left(3 + 2\cos\left(\frac{4\pi}{4}\right)\right) \left(3 + 2\cos\left(\frac{6\pi}{4}\right)\right) = 9 = F_4^2$$ $$\dots$$ Furthermore, it appears these squares are the squares of Fibonacci numbers. ( $F_1 = F_2 = 1$ , $F_n = F_{n-1} + F_{n-2})$ I've tried to prove that this expression always gives a perfect square by considering the identity (Cassini's identity) $$F_{n}^2 = F_{n+1}F_{n-1} + (-1)^{n-1}$$ and using induction to show that the above product satisfies this relation, and since the base cases ( $F_1, F_2...$ ) are that of the Fibonacci sequence then this would prove that the expression is the square of Fibonacci numbers. However, this approach led nowhere. Is there a clever way to show that the above expression is related to Fibonacci numbers?","['number-theory', 'trigonometry', 'square-numbers', 'products']"
4573746,Boundary conditions for differential forms,"I am trying to understand differential forms on manifolds with boundaries, and I am a bit confused with the boundary conditions. For the following, let $(M,g_M)$ be a smooth Riemannian manifold with boundary $\partial M$ , define $\iota:\partial M\hookrightarrow M$ the inclusion of the boundary, and $\star$ is the Hodge-star with respect to $g_M$ . In [1] the authors define the space of $p$ -forms satisfying Dirichlet $(\Omega^p_{\text{D}}(M))$ and Neumann $(\Omega^p_{\text{N}}(M))$ boundary conditions. Their definition, towards the end of page 3, is equivalent to \begin{align}
\Omega^p_{\text{D}}(M) &:= \Big\lbrace\omega\in\Omega^p(M)\ \Big\vert\ \iota^*\omega = 0\Big\rbrace \\
\Omega^p_{\text{N}}(M) &:= \Big\lbrace\omega\in\Omega^p(M)\ \Big\vert\ \iota^*\star\omega = 0\Big\rbrace .\end{align} Verbosely, this defines Dirichlet $p$ -forms , as $p$ -forms that vanish upon eating vectors that are tangent to $\partial M$ , and Neumann $p$ -forms as $p$ -forms that vanish upon eating vectors normal to $\partial M$ . Had I not seen that definition I would have defined Dirichlet $p$ -forms in the same way, but the Neumann ones I would define by demanding that their derivative along the normal directions to $\partial M$ vanishes, in the same fashion as one does for functions. This translates to demanding that $$\iota^*\star\mathrm{d}\omega = 0.$$ My question is, is my definition of Neumann $p$ -forms the same as [1]'s ? I would expect not, as having vanishing $\star\mathrm{d}\omega$ is clearly less restrictive than having vanishing $\star\omega$ . But then, if not, why is their definition justified as Neumann boundary conditions ? I.e. how does it generalize the usual Neumann boundary conditions for functions in a natural way , and why is my naive definition wrong ? Moreover, a space that is important is the space $$\Omega^p_{\text{important}}(M) := \Big\lbrace\omega\in\Omega^p(M)\ \Big\vert\ \iota^*\mathrm{d}\omega = 0\Big\rbrace.$$ This space arises naturally in gauge theories as the space of gauge transformations that vanish on the boundary. How is this space related to $\Omega^p_{\text{D}}(M)$ or $\Omega^p_{\text{N}}(M)$ ? (apart from the obvious $\Omega^p_{\text{D}}(M)\subset\Omega^p_{\text{important}}(M)$ ) References: [1] Sylvain Cappell, Dennis DeTurck, Herman Gluck, Edward Y. Miller, Cohomology of Harmonic Forms on Riemannian Manifolds With Boundary , https://arxiv.org/abs/math/0508372","['riemannian-geometry', 'manifolds-with-boundary', 'gauge-theory', 'differential-forms', 'differential-geometry']"
4573810,Orthocenter of a triangle collinear with two points in the circumcircle.,"This difficult elementary geometry problem was proposed by @Nyafh54 and receiving no answer was deleted twice despite several upvotes. We republish it here mainly for the information of the O.P. who showed be interested in the subject, and those who gave their opinion in favor of the problem.
The statement is true even for shapes of the triangle such that, for example, the segment $DOE$ is not contained inside the triangle. We have this, in particular, for the triangle of vertices $A=(3,4),B=(0,0), C=(8,0)$ in the attached figure. Heading HINT.- $(1)$ Vertices $A;B;C$ given, we know how to determine the circumcenter $O$ , the orthocencer $H$ and the circumcircle. $(2)$ $P$ and $S$ points are determined by the bisector of $BC$ side and $D$ and $E$ points by the bisector of segment $AS$ so we have got three points $D,E,S$ . $(3)$ The statement can lead people to the construction of the circumcircle of the triangle $\triangle{DES}$ then by intersection with the first circumcircle to determine the $X$ point and this is the difficulty of the problem for beginners. $(4)$ This difficulty can be avoided by intersecting the line $PH$ with the first circumcircle so we get a point $X$ . We don’t know yet that $X$ belongs to the circumcircle of new triangle $\triangle {DES}$ but we can show this proving that the quadrilateral $DESX$ is cyclic. For this, already having the coordinates $(x_i,y_i)$ of the points $D,E,S,X$ we can verify the Ptolemy's theorem: $$\overline{DE}\cdot\overline{XS}+\overline{DX}\cdot\overline{ES}=\overline{DS}\cdot\overline{XE}$$ Another way is using the equation of the circumcircle of $\triangle {DES}$ $$\det\begin{vmatrix} x^2+y^2&x&y&1\\x_1^2+y_1^2&x_1&y_1&1\\ x_2^2+y_2^2&x_2&y_2&1\\ x_3^2+y_3^2&x_3&y_3&1\end{vmatrix}=0$$ and verify this equality  putting the fourth point $X$ instead of the generic $(x.y)$ .","['euclidean-geometry', 'proof-writing', 'circles', 'geometry', 'triangles']"
4573894,Can we design a geometry where the angle between two lines can increase infinitely?,"As a contrast, the maximum angle between two lines is 90° in Euclidean geometry. I am just thinking people living in the universe with unlimited line angles will be curious that there exists a world where line angles are limited to a certain value, just like people are curious about the fact that in Minkowski geometry there is a maximum speed, i.e., the speed of light.","['geometry', 'differential-geometry']"
