question_id,title,body,tags
3438365,Compact set $K_\epsilon\subset B$ such that $\ell^*(B\setminus K_\epsilon) \leq \epsilon$,"This is from Bartle's book: If B is Lebesgue measurable in $\mathbb R$ , if $\epsilon >0$ and $B\subset I_n=(n,n+1] $ , then there exists a compact set $K_\epsilon\subset B$ such that $\ell^*(K_\epsilon)\leq \ell^*(B) \leq \ell^*(K_\epsilon)+\epsilon.$ Here $\ell^*$ denotes the Lebesgue measure on the line and $F^*$ the family of lebesgue measurable sets. I tried the following proof: We note that $\bar I_n = [n,n+1] \in F^*$ . Hence, $\bar I_n\setminus B \in F^*$ and given $\epsilon >0$ , I know from a previous exercise that exists $G_\epsilon$ open such that $\bar I_n\setminus B \subset G_\epsilon$ and $\ell^*(\bar I_n\setminus B) \leq \ell^*(G_\epsilon) \leq \ell^*(\bar I_n \setminus B) + \epsilon$ . Since $\bar I_n \setminus B \subset G_{\epsilon}$ we guarantee that $I_n\setminus G_\epsilon \subset B$ . Define $K = \bar I_n\setminus G_\epsilon$ which is compact, since its closed and bounded. Hence: $$\ell^*(\bar I_n\setminus B) \leq \ell^*(G_\epsilon) \leq (\bar I_n\setminus B) \implies \ell^*(I_n) \leq \ell^*(G_{\epsilon}) + \ell^*(B) \leq \ell^*(\bar I_n) \implies $$ $$\ell^*(\bar I_n) - \ell^*(G_\epsilon) \leq \ell^*(B) \leq \ell^*(\bar I_n) - \ell^*(G_\epsilon)$$ but I can't ensure that $\ell^*(\bar I_n) - \ell^*(G_\epsilon) = \ell^*(\bar I_n\setminus G_\epsilon) = \ell^*(K),$ since we don't know wheter $G_\epsilon \subset \bar I_n$ or not. Any help on how to solve this? Thank you",['measure-theory']
3438396,Evaluating $\int_0^{\pi/2} \frac{t \ln (1-\sin{t})}{\sin t} dt$,"In a problem in scattering theory, this integral arises: $$\displaystyle{\int\limits_0^{\pi/2} \frac{t \ln (1-\sin{t})}{\sin t} dt}$$ I have tried a number of approaches to evaluating the integral, which I suspect has a closed form solution. The reason is that I generated a numerical value for the integral, $-3.87578458503\ldots$ and after a bit of numerical exploration I found this to agree with $-\pi^3/8$ .","['integration', 'definite-integrals', 'harmonic-numbers', 'calculus', 'closed-form']"
3438417,A question about a positive continuous function,"Suppose $f \in C[0, 1]$ satisfies following properties: 1) $f(0) = f(1) = 0$ 2) $\forall x \in (0, 1)$ $f(x) > 0$ Do there always exist such $c$ and $d$ in $(0, 1)$ that $f(c) = f(d) = d - c$ ? I  have tried to consider the function $g(x) = \max\{t \in [0, 1]|f(t) = f(x)\} - x - f(x)$ which is strictly positive in $0$ and negative in the point of largest point of maximum. Thus if that function were always continuous, our problem would have been solved. Unfortunately, it isn’t.","['continuity', 'calculus', 'real-analysis']"
3438438,"Let $X_1...X_n \sim \Gamma(\alpha,\beta)$, what unbiased estimator of $\frac{1}{\beta}$ has minimum variance?","Given that $\alpha$ is known and $\beta$ is an unknown rate parameter, how do I find the unbiased estimator of $\frac{1}{\beta}$ ? Does it involve finding $\hat\theta_{\text{MLE}}$ first? I honestly don't know where to start.","['statistics', 'parameter-estimation']"
3438460,Find the domain of $f(x)=\sec^{-1}\dfrac{x}{\sqrt{x-[x]}}$,"Domain of $f(x)=\sec^{-1}\dfrac{x}{\sqrt{x-[x]}}$ is: $$
x-[x]\neq0\implies x\notin \mathcal{Z}\implies x\in\mathcal{R}-\mathcal{Z}\\
\sec^{-1}:\mathcal{R}-(-1,1)\to(0,\pi)\implies \dfrac{x}{\sqrt{x-[x]}}\in\mathcal{R}-(-1,1)\\
\dfrac{x}{\sqrt{x-[x]}}\notin(-1,1)\\
x-[x]\in(0,1)\implies\sqrt{x-[x]}\in(0,1)
$$ The solution given in my reference is $\mathcal{R}-\{(-1,1)\cup\mathcal{Z}\}$ , but how do I prove the part $x\notin (-1,1)$ ?","['functions', 'inverse-function']"
3438474,"Find $\lim_{(x,y) \to (0,0)} \frac{xy-\sin(x)\sin(y)}{x^2+y^2}$","Find $$\lim_{(x,y) \to (0,0)} \frac{xy-\sin(x)\sin(y)}{x^2+y^2}$$ What I did was: Find the second order Taylor expansion for $xy-\sin(x)\sin(y)$ at $(0,0)$ : $P_2 (0,0)=f(0,0)+df_{(0,0)}+d^2f_{(0,0)}+r_2(0,0)$ $f(0,0)=0$ $df_{(0,0)}=\frac{\partial f}{\partial x} (0,0) \Delta x + \frac{\partial f}{\partial y} (0,0) \Delta y = 0$ $d^2f_{(0,0)}=\frac{\partial f}{\partial x} (0,0) \Delta x^2 + \frac{\partial f}{\partial x \partial y} (0,0) \Delta x \Delta y + \frac{\partial f}{\partial y} (0,0) \Delta y^2 = 0$ $\implies$ $P_2(0,0)=r_2(0,0)$ Now I have $\lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2}$ $\implies  \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2} = \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{x^2+y^2} \cdot \frac{\sqrt{x^2+y^2}}{\sqrt{x^2+y^2}} = \lim_{(x,y) \to (0,0)} \frac{r_2(0,0)}{\sqrt{x^2+y^2}} \cdot \frac{\sqrt{x^2+y^2}}{x^2+y^2}= 0$ However, wolframalpha is telling me the limit does not exist...","['multivariable-calculus', 'limits', 'calculus', 'taylor-expansion']"
3438483,"$f=x+\phi(y,z)=y+\psi(x,z)=z+\varphi(x,y)$ so $f(x,y,z)=x+y+z$?","This seems to be a very easy problem. Given that: $$f=x+\phi(y,z)$$ $$f=y+\psi(x,z)$$ $$f=z+\varphi(x,y)$$ We want to solve for $f$ , which is a real function with three variables. First without loss of generality, we write $\varphi=h(x)+g(y)+k(x,y)$ , by the first equation, we have $h=x$ , by the second equation, we have $g=y$ . So repeating this method, I can conclude that $f=x+y+z$ but I think my proof is wrong... Is it true? What are some strategy to solve this kind of equations with a function as unknown?","['systems-of-equations', 'functions', 'problem-solving']"
3438498,How to find the second derivative of y in $y^2 = x^2 + 2x$?,"I have a problem to solve: use implicit differentiation to find $\frac{dy}{dx}$ and then $\frac{d^2y}{dx^2}$ . Write the solutions in terms of x and y only It means that I need to differentiate the equation one time to find $y'$ and then once more to find $y''$ . The correct answer from the textbook is $y' = \frac{x + 1}{y}$ and $y'' = \frac{x^2 + 2x}{y^3}$ . I got the first derivative right, but I can't understand how did they get the second one, or is it a typo (unlikely), since I have $y'' = \frac{1}{y} - \frac{(x + 1)^2}{y^3}$ I did this: $$
y^2 = x^2 + 2x\\
2yy' = 2x + 2\\
yy' = x + 1\\
y' = \frac{x + 1}{y}\\
$$ I tried to get to the second derivative from both $yy' = x + 1$ , $y' = \frac{x + 1}{y}$ and $2yy' = 2x + 2$ . But every time I had that dangling constant (1 or 2), which lead to the dangling $\frac{1}{y}$ in my answer. Like here: $$
yy' = x + 1\\
y'y' + yy'' = 1\\
yy'' = 1 - (y')^2\\
y'' = \frac{1 - (y')^2}{y}\\
y'' = \frac{1}{y} - \frac{(y')^2}{y}\\
y'' = \frac{1}{y} - \frac{(\frac{x + 1}{y})^2}{y}\\
y'' = \frac{1}{y} - \frac{(x + 1)^2}{y^3}
$$ I don't see any way to get from my answer to the textbook's one with a transformation, no way to get rid from y in the numerator. And the correct answer doesn't have a ""y"" there. Could someone either point to an error in my solution, or corroborate the suspicion that it indeed may be a typo.","['calculus', 'implicit-differentiation', 'derivatives']"
3438535,Dimension of algebraic variety if we know dimension of irreducible components,"I have a bit of trouble with finding the dimension of an algebraic set. In general I know that this is the supremum of the integers $n$ such that there exists a chain $Z_0 \subset Z_1 \subset ... \subset Z_n$ of distinct irreducible closed subsets. In the exercise that I'm making, I know that the algebraic set that I'm working with is reducible. I also know that the dimension of every irreducible component of the algebraic set is $2$ . Does this imply that the dimension of the algebraic set is $2$ ? This doesn't seem right to me. Hopefully someone can help me out.",['algebraic-geometry']
3438544,Confusion about genus-degree formula,"My question regards a perplexity I have on how to apply the genus-degree formula for irreducible, projective, complex plane curves. Consider first the affine complex plane curve given by the equation $$ C: \ (x-2)(x-1)(x+1)(x+2) -y^2 = x^4-5x^2+4 -y^2=0.$$ The Jacobian is given by $(x(2x+\sqrt{10})(2x-\sqrt{10}), -2y)$ , so it never vanishes on $C$ . Let us now look at the compactification $\hat{C}$ , which has the same points as $C$ plus a point at infinity with coordinates $[x:y:z]=[0:1:0]$ . This point lies in the affine chart with $y=1$ , and the affine equation for $\hat{C}$ in terms of $x$ and $z$ in that chart is $$ x^4-5z^2x^2+4z^4-z^2 = 0.$$ The differential $(4x^3-10xz^2, -10x^2z+16z^3-2z)$ vanishes at $(x,z)=(0,0)$ , hence $\hat{C}$ has one singularity: the point at infinity. If we pretend for a moment that it doesn't (i.e., that it is smooth), one can do the ""usual construction"" to see that it is topologically a torus: one can draw two cuts along $[-2,-1]$ and $[1,2]$ on two copies of the Riemann sphere and glue them together with the right orientation. So if $\hat{C}$ were regular, it would be an elliptic curve, and in particular have genus $1$ . However, the genus-degree formula for projective plane curves predicts genus $3$ , since the equation of $\hat{C}$ has degree $4$ . But the Wikipedia article on the genus-degree formula also mentions that the formula actually gives the arithmetic genus and that for every ordinary singularity of multiplicity $r$ , the geometric genus is smaller than the arithmetic genus by $\frac{1}{2}r(r-1)$ . Now, I am not really sure about how to measure the multiplicity of a singularity, but in this case it seems that for any value of $r \ge 0$ , we never have that $3-\frac{1}{2}r(r-1)=1$ . So the geometric intuition and the formula seem to disagree. The only other thing that comes to my mind is that I have not checked yet that $\hat{C}$ is irreducible, but this can be checked on $C$ using Eisentein's criterion applied to the polynomial ring $(\mathbb{C}[x])[y]$ using the prime ideal $\mathfrak{p}=(x+1)$ . Reassuming, my question is: what is the genus of $\hat{C}$ ? If it is $1$ , why is the genus-degree formula wrong? If it is $3$ , why is the geometric intuition wrong? After all, also the article of Wikipedia on elliptic curves seems to confirm that $\hat{C}$ should have genus $1$ .","['complex-geometry', 'algebraic-geometry', 'elliptic-curves']"
3438621,Prove that $4\tan^{-1}\left(\frac{1}{5}\right) - \tan^{-1}\left(\frac{1}{239}\right)= \frac{\pi}{4}$,"Prove that $4\tan^{-1} \left(\dfrac{1}{5}\right) - \tan^{-1}\left(\dfrac{1}{239}\right)=\dfrac{\pi}{4}.$ I was wondering if there was a shorter solution than the method below? Below is my attempt using what I would call the standard approach to these kinds of problems. The expression on the left hand side is equivalent to $$\tan^{-1}\left[\tan \left(4\tan^{-1}\left(\dfrac{1}{5}\right)\right)-\tan^{-1}\left(\dfrac{1}{239}\right)\right]\\
=\tan^{-1}\left(\dfrac{\tan(4\tan^{-1}(\frac{1}{5}))-\frac{1}{239}}{1+\frac{1}{239}\tan(4\tan^{-1}(\frac{1}{5}))}\right)\tag{1}.$$ We have that $$\tan\left(4\tan^{-1}\left(\frac{1}{5}\right)\right)=\dfrac{2\tan(2\tan^{-1}(\frac{1}{5}))}{1-\tan^2(2\tan^{-1}(\frac{1}{5})}\tag{2}$$ and that $$\tan\left(2\tan^{-1}\left(\frac{1}{5}\right)\right)=\dfrac{2\cdot \frac{1}{5}}{1-(\frac{1}{5})^2}=\dfrac{5}{12}\tag{3}.$$ Plugging in the result of $(3)$ into $(2)$ gives $$\tan\left(4\tan^{-1}\left(\frac{1}{5}\right)\right) = \dfrac{2\cdot \frac{5}{12}}{1-(\frac{5}{12})^2}=\dfrac{120}{119}\tag{4}.$$ Pluggin in the result of $(4)$ into $(1)$ gives that the original expression is equivalent to $$\tan^{-1}\left(\dfrac{\frac{120}{119}-\frac{1}{239}}{1+\frac{1}{239}\cdot\frac{120}{119}}\right)=\tan^{-1}\left(\dfrac{\frac{119\cdot 239 + 239-119}{239\cdot 119}}{\frac{119\cdot 239+120}{119\cdot 239}}\right)=\tan^{-1}(1)=\dfrac\pi4,$$ as desired.","['calculus', 'trigonometry']"
3438627,An example for a seminorm on $\mathbb{R}^n$,"Can any one come up with an example of a seminorm that is not a norm on $\mathbb{R}^n$ ? A seminorm on a real vector space $V$ is a function $N:V\rightarrow \mathbb{R}$ that satisfies that 1) $N(x)\geq 0$ , $x\in V$ 2) $N(\alpha x)=|\alpha|N(x)$ , $x\in V$ , $\alpha\in \mathbb{R}$ 3) $N(x+y)\leq N(x)+N(y)$ , $x,y\in V$ So a seminorm generalizes a norm as it does not require the condition $$N(x)=0\Longrightarrow x=0$$ .","['linear-algebra', 'functional-analysis', 'approximation-theory']"
3438635,Does removing finitely many points from an open set yield an open set?,"Removing finitely many point from an open set in $\mathbb{R}^n$ gives an open set.  Is this true in general for any space? My intuition is that this is the case, however, how does one (dis)prove this? The only idea that comes in mind is that, since singletons are closed and unions of closed sets are closed, a union of singletons is closed. Now, let $U$ be an open set.  Then the complement of $U$ is closed, and any point $x$ removed from $U$ is a singleton that is ""unioned"" with the closed complement of $U$ to form a bigger closed set $C$ . $C$ is the complement of $U\setminus\{x\},$ so $U\setminus\{x\}$ is open. One can thus repeat this argument inductively for any finite number of removed points. Does this make sense?","['general-topology', 'separation-axioms']"
3438653,"Why $\{ \lambda x + (1-\lambda){y}\;\lvert\; \lambda \in [0,1] \}$ represents the line segment between $x,y$?","I have this thing written on my notes: let ${x}, {y}\in\mathbb R^n$ be two distinct points, then the set $$\{ \lambda x + (1-\lambda){y}\;\lvert\; \lambda \in [0,1] \}$$ contains all the points on the line segment that connects $x$ to $y$ .
I can't seem to understand why it is so, and why we need to require that $\lambda \in [0,1]$ . 
Thanks for any clarification.","['euclidean-geometry', 'vectors', 'convex-analysis', 'geometry']"
3438672,An algorithm to find every induced subgraph that is a tree,"I've been taking discrete math classes for a few months now and I was recently tasked to with a challenging exercise. I have to write a program that finds every induced subgraph that is a tree. The induced graph must have $m$ number of vertices ( $m < n$ , where $n$ is the number of vertices in the given graph). So I've been digging around the internet for the last two days and the best things that I've found are an algorithm to check whether the graph is a tree: https://www.geeksforgeeks.org/check-given-graph-tree/ and these mathematical proofs: https://reader.elsevier.com/reader/sd/pii/S0166218X09000663?token=63213A4B2D548D315FF37E74896FD371A61C08FBDEC7C79C89D1C3E919C631ABBF6C048E5DB1600A3622E12758D51E49 . It's quite frustrating as we were only given basic theory about trees and no programming knowledge at all so any help would really be appreciated!","['graph-theory', 'programming', 'trees', 'discrete-mathematics']"
3438673,variance of maximum,"Let $(X_i)_{i\leq n}$ be random variables, which may be dependent. Is it true that \begin{align*}
\text{Var}(\max X_i) \leq \sum_{i=1}^n \text{Var}(X_i).
\end{align*} I have tried integrating the tail probability, \begin{align*}
\text{Var}(\max X_i) &= E\left( \max X_i - E \max X_i \right)^2 \\
&= \int_0^\infty P\left((\max X_i - E \max X_j)^2 > t \right) dt \\
&\leq \int_0^\infty P\left(\bigcup_{i=1}^n \{(X_i - E \max X_j)^2 > t\} \right) dt \\
&\leq \sum_{i=1}^n \int_0^\infty P((X_i-E \max X_j)^2 > t) dt \\
&= \sum_{i=1}^n E (X_i - E \max X_j)^2,
\end{align*} but this is not quite what we need. It seems that if the $X_i$ are independent, then I shouldn't have lost anything in the union bound. Does anyone know of a counter example or a proof?","['probability-theory', 'probability']"
3438750,Understanding Conjugacy classes of the Unitary group over finite fields,"Consider the General linear group $GL(n,q)$ over the finite field $\mathbb{F}_q$ of $q$ elements. The unitary group $U(n,q)$ is described as a subgroup of $GL(n,q^2)$ which is the set of linear maps which are invariant under a $c$ -Hermitian form on the $n-$ dimension vector space $V$ over $\mathbb{F}_{q^2}$ , where $c:\mathbb{F}_{q^2}\to \mathbb{F}_{q^2}$ is the Frobenius automorphism $\alpha\mapsto \alpha^{q}$ . Since, all the $c-$ Hermitian forms over $\mathbb{F}_q$ are equivalent, there is a unique isometry group upto isomorphism, which is $U(n,q)$ . Now, G.E Wall had described the conjugacy classes of these groups(over any field though). Now, I need to verify my understanding of conjugacy class of $U(n,q)$ . This is what I understand: (1) One defines a certain set of monic polynomials in $\mathbb{F}_{q^2}[x]$ as follows: If $f\in \mathbb{F}_{q^2}[x]$ , then define the dual of $f$ as $\tilde{f}(x)=\frac{1}{f(0)^c}x^{deg(f)}f^{c}(x^{-1})$ . In other words, $\tilde{f}$ is such that: $\alpha$ is a root of $f$ iff $\alpha^{-q}$ is a root of $\tilde{f}$ . We call $f$ self-dual if $f=\tilde{f}$ . (2) Now, $K=\bigcup\limits_{g\in GL(n,q^2)} gU(n,q^2)g^{-1}$ is a normal subset of $GL(n,q^2)$ , and hence a union of conjugacy classes of $GL(n,q^2)$ . Now, by the theory of rational canonical forms, one can associate a combinatorial data to an element $T\in GL(n,q^2)$ , as follows: If $\Phi$ is the set of all irreducible polynomials in $\mathbb{F}_{q^2}$ except $x$ , then the combinatorial data of $T$ denoted by $\Delta_T$ is: for $\phi \in \Phi$ , (a) a partition $\lambda_{\phi}$ of $|\lambda_{\phi}|$ (b) $\sum\limits_{\phi \in \Phi} deg(\phi)|\lambda_{\phi}|=n$ . Now, By the theory of Wall, a conjugacy class $C$ of $GL(n,q^2)$ belongs to $K$ iff the combinatorial data is as follows: for $\phi \in \Phi$ , (a) a partition $\lambda_{\phi}$ of $|\lambda_{\phi}|$ (b) $\lambda_{\phi}=\lambda_{\tilde{\phi}}$ (c) $\sum\limits_{\phi \in \Phi} deg(\phi)|\lambda_{\phi}|=n$ . It is clear that $\phi$ is irreducible iff $\tilde{\phi}$ is irreducible. (3) Finally, it also asserts the following: Suppose $C$ be a conjugacy class of $GL(n,q^2)$ residing in $K$ . Then $C$ is as described in (2). Suppose $\emptyset \neq L\subset C$ is the part of $C$ contained in $U(n,q)$ , then $L$ is a complete conjugacy class in $U(n,q)$ , that is, it doesn't split further as $U(n,q)$ -conjugacy class. It is implicit that for each conjugacy class $C$ as described above, there always exists a non-empty $L$ as above. This completely describes the conjugacy class in $U(n,q)$ . This kind of description can be found in several papers. For Example: In several papers of J.Fulman and others. But since the writing is much compact, I had expanded it in my own way. It is a big post and I will be very thankful if anyone can patiently read this and clarify me if my understanding is correct. Thank you!","['group-theory', 'finite-groups', 'classical-groups']"
3438898,Sidon sets in finite groups,"Suppose $G$ is a group, $S \subset G$ . Let’s call $S$ a Sidon subset iff $\forall$ quadruples $(a, b, c, d)$ of distinct elements of $S$ we have $ab \neq cd$ (named after Simon Sidon who studied such subsets in $C_\infty$ ). Let’s define $Sid(G)$ as the maximal possible cardinality of a Sidon subset in $G$ . Do there exist such $0<c<C<+\infty$ , such that $c \leq \frac{Sid(G)}{\sqrt{|G|}} \leq C$ for any finite group $G$ ? The existence of $C$ can be proved the following way: If $S$ is a Sidon subset of $G$ , then $|G| \geq |S^2| \geq \frac{|S|(|S| - 1)}{2}$ , thus $C = \sqrt{2} + 1$ will work. The only thing I know about $c$ is that Erdos proved such constant exist for finite cyclic groups $G$ . But does it exist for all finite $G$ ?","['additive-combinatorics', 'finite-groups', 'combinatorics', 'sumset', 'group-theory']"
3438909,Proving a subring of $\Bbb Z[\zeta_{11}]$ is PID,Check if $\Bbb Z[\zeta_{11}+\zeta_{11}^{-1}]$ is PID where $\zeta_{11}$ is primitive $11$ -th root of unity. I don't know how to proceed? Are there any techniques in algebraic number theory to tackle this kind of problem? Thank you for your help.,"['algebraic-number-theory', 'modular-arithmetic', 'number-theory', 'principal-ideal-domains', 'abstract-algebra']"
3438966,Estimate $\sum_{n = 0}^N \cos (\alpha n^2)$,"I want to estimate this sum $$\sum_{n = 0}^N \cos (\alpha n^2)$$ where $\alpha$ is a constant less than $1$ and $N$ is an integer. One of the things that I tried was using Taylor expansion for cosine and then using Stirling's approximation for the factorial in it but summing over the powers of integers involves Bernoulli numbers and it gets tricky quite quickly. Is there a way to convert this sum into an integral (with an appropriate error term maybe)? If not, is there any other way to estimate this sum? If you could point me to the relevant literature, that would be useful as well.","['reference-request', 'trigonometric-integrals', 'taylor-expansion', 'bernoulli-numbers', 'trigonometry']"
3439049,Trying to understand why Cech cohomology computes derived functor cohomology,"I am trying to get a better understanding of what Cech cohomology actually is. In particular, I want to see why it computes derived functor cohomology.  I am familiar with the orthodox proof of this fact via showing that the derived functor cohomology agrees with the Cech cohomology in degree $0$ , and then showing that the Cech cohomology is effaceable. But I feel that abstract proofs like that don't really show me what is going on. So I tried to find a more concrete proof and I've been running into problems. I wanted to do this with an explicit description of the injective hull of a quasi-coherent sheaf. Hartshorne's Residues and Duality gives such a description: For each $p \in X$ , let $J(p)$ denote the injective hull of the residue field $\kappa(p)$ as an $\mathcal{O}_{X, p}$ -module. Then let $\mathcal{J}_{p}$ denote the skyscraper sheaf at $p$ which associates the module $J(p)$ at the point $p$ . Then $\mathcal{F}$ has an injective hull of the form, $$
\mathcal{F} \hookrightarrow \bigoplus_{p \in X} \mathcal{J}_{p}
$$ where each $p$ may be repeated. Let $\mathfrak{U} = \{ U_{\alpha}  \}_{\alpha \in I}$ be an affine cover of $X$ . I want to construct a morphism in degree $0$ from the Cech complex to the global sections of the injective resolution of $\mathcal{F}$ , $$
\check{C}^{0}(\mathfrak{U}, \mathcal{F}) \longrightarrow \Gamma \left(X , \bigoplus_{p \in X} \mathcal{J}_{p}  \right).
$$ By quasi-compactness, this is equivalent to finding a morphism, $$
\bigoplus_{\alpha} \Gamma(U_{\alpha}, \mathcal{F}) \longrightarrow \bigoplus_{p \in X} \Gamma \left(X ,  \mathcal{J}_{p}  \right).
$$ Can anyone point me in the right direction? Or perhaps if this is completely the wrong way to look at things, give me some idea on how to see explicitly how the Cech cohomology maps to the derived functor cohomology on the level of cocycles and coboundaries?","['algebraic-geometry', 'homology-cohomology', 'sheaf-cohomology', 'schemes']"
3439056,Main properties of smooth functions that vanish outside the certain interval,"In my textbook, there is a problem in which I'm given $V$ , a vector space of infinitely differentiable (smooth) functions vanishing outside the certain interval. Formally, I'm aware that $\exists a, b \in \mathbb{R}, I=[a, b]$ (closed interval) such that $\forall f \in V, f(x)=0, \forall x \notin I$ . But besides this straightforward definition, what are other general properties of such functions? Examples Are there any constraints for vanishing intervals? If $I=[a, b]$ and $f \in  V$ is a smooth function, are there any constraints for choice of $a$ and $b$ on which $f$ vanishes? Is there any way such bounds can be found? Shall such interval $I$ contain the origin (0)? Are there any constraints for derivatives and integrals of such functions? For example, if $f(x)=0, \forall x \notin I$ , what does this imply about $D(f(x)), \forall x \notin I$ or $D(f(x)), \forall x \in I$ ? May it imply that all of its derivatives vanish at origin? Furthermore, does this imply something about $\int_a^b f(t) \, dt, \forall t \in I$ besides the fact that it is not zero? If smooth functions vanish outside the certain interval, are they necessarily non-analytic? Concordantly, are they considered smooth transition functions? This very interesting Wikipedia article , shows examples of smooth functions that can not be approximated by convergent power series - but that example contains a function which has derivative that contains the origin its vanishing interval (as stated in my second example). In this case, where the definition of $f$ is not explicit, does this say something about its analytic/non-analytic property? The reason for my interest in this property is because such functions seem very similar to smooth transition functions . In Short Is there any explicit name for smooth functions that vanish outside the interval? If not, what are the properties that make them ""special""? Note : To be more explicit about the definition of ""special"", this is the problem from my textbook (Serge Lang, Linear Algebra): Let $V$ be a finite dimensional space over $\mathbb{R}$ of infinitely
  differentiable functions vanishing outside some interval . Let
  the scalar product be defined as usual by: $$\langle f, g \rangle = \int_0^1 {f(t)g(t)} \, dt$$ Let $D$ be the derivative. Show that one can define $D^T$ as
  before, and $D^T=-D$ .","['calculus', 'functions', 'real-analysis']"
3439096,Coupon collectors problem: variance calculation missing a term.,"EDIT: for a consolidated answer to the general variance in a coupon collectors problem with unequal probabilities, see here: https://math.stackexchange.com/a/3454032/155881 In example 5.17 of the book on Introduction to probability models by Ross , he solves the coupon collector's problem, where there are $n$ coupons, each with probability $p_j$ of being collected per draw (with $\sum_{j=1}^n p_j=1$ ). He uses the Poisson process to come up with the following expression for the expected value of $X$ , the number of coupons to be collected for completing the collection: $$E(X) = \int\limits_0^\infty P(X>t)dt = \int\limits_0^\infty \left(1-\prod\limits_{j=1}^n (1-e^{-p_j t})\right)dt$$ Using the fact that $\int_0^\infty e^{-pt}=\frac 1 p$ , $$E(X)=\sum \frac 1 p_j -\sum_{i<j} \frac{1}{p_i+p_j}+\dots +(-1)^{n-1}\frac{1}{p_1+\dots+p_n}$$ Now, I want to use the same approach to calculate the variance. Per comment by @BGM here and also this question , we can use the following expression to get $E(X^2)$ : $$E(X^2) = \int\limits_0^\infty 2tP(X>t)dt = \int\limits_0^\infty 2t\left(1-\prod\limits_{j=1}^n(1-e^{-p_j t})\right)dt$$ Using the fact that $\int\limits_0^\infty te^{-pt}=\frac{1}{p^2}$ and the same algebra as for $E(X)$ we get: $$\frac{E(X^2)}{2} = \sum \frac {1} {p_j^2} -\sum_{i<j} \frac{1}{(p_i+p_j)^2}+\dots +(-1)^{n-1}\frac{1}{(p_1+\dots+p_n)^2} $$ Now, let's consider the special case where all coupons have an equal probability of being selected. In other words, $p_j=\frac 1 n \; \forall \; j$ . Approach-1 We get: $$\frac{E(X^2)}{2} = n^2\left(\sum\limits_{k=1}^n (-1)^{k-1}\frac{n\choose k}{k^2}\right)$$ Per my answer to the question here , this summation yields: $$E(X^2) = 2n^2\left( \sum_{j=1}^n\sum_{k=1}^j\frac{1}{jk}\right)\tag{1}$$ Approach-2 But per this paper , the variance for this special case is: $$V(X) = n^2\sum_{j=1}^m\frac{1}{j^2}-n\sum_{j=1}^m\frac{1}{j} $$ and this would mean that: $$E(X^2) = V(X)+E(X)^2 = n^2\sum_{j=1}^m\frac{1}{j^2}-n\sum_{j=1}^m\frac{1}{j}+\left(n\sum_{j=1}^m\frac{1}{j}\right)^2$$ If we visualize a $j-k$ grid, it's easy to see that this is the same as: $$E(X^2) = 2n^2\left( \sum_{j=1}^n\sum_{k=1}^j\frac{1}{jk}\right)-n\sum_{j=1}^m\frac{1}{j}\tag{2}$$ If we compare equation (1) from approach-1 and equation (2) from approach-2, it's clear that equation (1) has a missing $-n\sum_{j=1}^m\frac{1}{j}$ term. And equation (2) has been verified using other methods. This indicates that there is some small mistake with approach-1 that is making us miss this term. I haven't been able to spot what this issue is. Hoping someone else might.","['summation', 'coupon-collector', 'probability']"
3439117,How to show the double negation law in Boolean algebra,"I want to show the double negation law $\lnot \lnot s = s \tag{0}$ where $s$ is an element of Boolean algebra.
And $\lnot$ is defined as $\lnot s := s \rightarrow 0$ . Boolean algebra is a Heyting algebra with following two laws. $s \lor \lnot s = 1 \tag{1}$ $s \land \lnot s = 0 \tag{2}$ And Heyting algebra is a lattice with $0$ , $1$ , and $\rightarrow$ . I want to show (0) using only (1), (2), and the nature of Heyting algebra. I could show the following law using the nature of Heyting algebra. $s \le \lnot \lnot s \tag{3}$ To prove (0), the following law need to be proved. But I feel it is difficult. $s \ge \lnot \lnot s \tag{4}$","['boolean-algebra', 'abstract-algebra', 'heyting-algebra']"
3439146,Primitive system is ergodic if original system is ergodic,"Suppose that $(X,\mathscr{A},\mu)$ is a probability space. Let $T\colon X\to X$ be a measure preserving ergodic transformation . Let $f\in L^{1}(X,\mathscr{A},\mu)$ be such that $f(X)\subset\mathbb{N}$ . We define the primitive (or integral) system of $T$ under $f$ by: set: $X_{f}:=\{(x,k) \ | \ x\in X, \ k\in\mathbb{N}, \ k\leq f(x)\}$ , $\sigma$ -algebra: $\mathscr{A}_{f}$ generated by sets of the form $A_{f}^{k}:=\{(x,k) \ | \ x\in A, \ k\leq f(x)\}$ , where $A\in\mathscr{A}$ and $k\in\mathbb{N}$ . measure: $$\mu_{f}(A_{f}^{k}):=\frac{\mu(A)}{\int_{X}f \ \text{d}\mu},$$ and extend $\mu_{f}$ to all of $\mathscr{A}_{f}$ . transformation: $T_{f}\colon X_{f}\to X_{f}$ defined by $$T_{f}(x,k):=\begin{cases}(x,k+1)&k<f(x)\\(Tx,1)&k=f(x)\end{cases}.$$ I have been able to prove that $T_{f}$ is measure preserving. But how do I show that $T_{f}$ is ergodic? Any suggestions are greatly appreciated. For instance, it enough to prove it for generators of the $\sigma$ -algebra $\mathscr{A}_{f}$ ?","['measure-theory', 'ergodic-theory', 'lebesgue-integral', 'transformation', 'dynamical-systems']"
3439148,A Theorem from Jacobson: $U^G\otimes_F V\cong (U\otimes_F V_H)^G$,"Theorem: Let $G$ be a finite group, $H\le G$ , $F$ a field. Let $U$ be an $F[H]$ -module and $V$ an $F[G]$ -module; $U^G$ is induced $F[G]$ -module. Then, $$U^G\otimes_F V\cong (U\otimes_F V_H)^G \,\,\,\, \mbox{(isomorphic as $F[G]$-modules)}$$ This theorem is mentioned in Jacobson's volume 2 (see $\S 5.10$ ,Theorem 5.17(3), p. 292-293). But I didn't get clue for the proof by Jacobson. He points out the following in beginning of proof of this theorem:  we have two $F[G]$ -modules $(F[G]\otimes_{F[H]} U)\otimes_F V$ and $F[G]\otimes_{F[H]} (U\otimes_F V)$ , and the natural $F$ -vector space isomorphism of these spaces, $(a\otimes x)\otimes y\mapsto a\otimes (x\otimes y)$ is not $F[G]$ -module homomorphism. (Then he constructs new $F[G]$ -modue isomorphism of them.) Q. I tried to write alternate proof in following way; I wanted to know, is it correct? (i) Let $t=[G:H]$ and $G=\cup_{i=1}^t g_iH$ ( $g_1=1$ ). Then W.L.G, we can take $U^G=\oplus_i g_iU$ . (ii) So $U^G\otimes_F V\cong (U\otimes_F V) \oplus \cdots \oplus (g_tU\otimes_F V)$ as $F[G]$ -module. (iii) The decomposition of $F[G]$ -module in (ii) satisfies following properties: $G$ permutes the $t$ many components (subspaces) transitively. stabilizer of subspace $U\otimes_F V$ is precisely $H$ (so $U\otimes_F V=U\otimes_F V_H$ ). So the $F[G]$ -module in RHS of $(ii)$ is isomorphic to $(U\otimes_F V_H)^G$ as $F[G]$ -module. q.e.d.","['representation-theory', 'group-theory', 'finite-groups']"
3439205,Integral of derivative of function with constant support is zero: intuitive descriptioin sought,"I am looking for an intuitive argument why the integral of the derivative of a function with compact support is zero. More formally, let $\phi:\mathbb{R}\to\mathbb{R}$ be a function with compact support and at least one derivative. For the integral of the derivative we easily find: \begin{align}
\int_{-\infty}^\infty \frac{d\phi(t)}{dt} dt&=\lim_{a\to\infty}\phi(a)-\phi(-a)\\
&= 0
\end{align} Now I try to understand this more intuitive. In some limited interval, $\phi$ is some non zero bump or jitter, lets say $\geq 0$ for simplicity. In this interval, the derivative is some differently formed non zero curve. The mean value theorem (intuitive enough, lets say) tells me that the derivative must change sign, so there are negative parts and positive parts. The latter tells me that there is indeed the chance that the negative and positive parts cancel out over the integral. What I am looking for is an eye-opener explaining why the negative and positive parts need to cancel out exactly. Something based on the two graphs of the function and its derivative would be nice.","['integration', 'derivatives']"
3439271,Measurable function with parameters,"Let $\mu_{y}$ a Borel probability on $\mathbb{R}^{n}$ indexed by a parameter $y \in \mathbb{R}^{d}$ . Let $A$ a borel set of $\mathbb{R}^{n}$ and suppose that the map $$
F \text{ : } y \rightarrow \mu_{y}(A)
$$ is measurable (we equiped $\mathbb{R}$ with the borel algebra). Let $T_{y} \text{ : } \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ a family of measurable functions indexed by a parameter $y \in \mathbb{R}^{d}$ . My question is : Is the map $$ y \rightarrow \mu_{y}( T_{y} \in A) $$ measurable ? Thanks and regards. EDIT : We can suppose that $(x,y) \rightarrow T_{y}(x)$ is measurable if it helps. Maybe without this assumption we can say anything.","['measure-theory', 'lebesgue-measure', 'borel-measures', 'measurable-functions', 'borel-sets']"
3439316,Topology with only one open proper set,"What is the name of the Topologies that contains only one proper open set?
That is if X<>{ } and T={{ }, O,X} ,where O ⊂ X   ..I looking for a 
terminology  that represent this topology in general.For me I call them 
Non-Trivial minimal Topologies .But this is not scientific terminology?
please I f there an answer must be supported by a reference ?","['general-topology', 'terminology', 'reference-request']"
3439377,Maximization of quotient of quadratic forms,"I have a problem with finding: $$\max_{a \in \mathbb{R}^p} \frac{a’Ca}{a’Ba},$$ where $C$ and $B$ are $p \times p$ symmetric matrices. After differentiation I get the following result: $$\frac{2a’C(a’Ba) - 2a’B(a’Ca)}{(a’Ba)(a’B’a)} =0$$ I can’t solve the above equation for $a$ . How to find it?","['analysis', 'linear-algebra', 'optimization', 'derivatives', 'quadratic-forms']"
3439391,Evaluating a definite Integral with a constant $n$,"I am trying to evaluate the following integral: $$ \int_0^1 \frac{n(n-1)(n-2)}{2} y^{3} \left( 1 - y \right)^{n-3}  \, dy $$ Here is my solution. Let $I$ be the integral we are trying to evaluate. \begin{align*}
I &= \int_0^1 \frac{n(n-1)(n-2)}{2} y^{3} \left( 1 - y \right)^{n-3}  \, dy
\end{align*} Now to evaluate this integral we use the substitution $u = 1 - y$ which gives us $y = 1 - u$ and $du = -dy$ . \begin{align*}
I &= - \int_0^1  \frac{n(n-1)(n-2)}{2}  (1-u)^3 u^{n-3} \, du \\
I &= \int_1^0  \frac{n(n-1)(n-2)}{2}  (1-u)^3 u^{n-3} \, du \\
\end{align*} Observe that: \begin{align*}
(1-u)^3 &= (1-u)(1-u)^2 = (u-1)^2(1-u) = (u^2 - 2u + 1)(1-u) \\
(1-u)^3 &= -u^3 + 3u^2 - 3u + 1
\end{align*} Now back to the integral: \begin{align*}
I &= \int_1^0  \frac{n(n-1)(n-2)}{2}  (  -u^3 + 3u^2 - 3u + 1) u^{n-3} \, du \\
I &= \int_0^1  \frac{n(n-1)(n-2)}{2}  ( u^3 - 3u^2 + 3u - 1) u^{n-3} \, du \\
I &= \int_0^1  \frac{n(n-1)(n-2)}{2}  ( u^n - 3u^{n-1} + 3u^{n-2} - u^{n-3} ) \, du \\
I &=
	\frac{n(n-1)(n-2)}{2}  \left( \frac{u^{n+1}}{n+1} - \frac{3u^n}{n} + \frac{3u^{n-1}}{n-1} - \frac{u^{n-2}}{n_2} \right) \bigg|_0^1 \\
I &= \frac{n(n-1)(n-2)}{2}  \left( \frac{1}{n+1} - \frac{3}{n} + \frac{3}{n-1} - \frac{1}{n-2} \right) \\
I &= \frac{n(n-1)(n-2) -3(n+1)(n-1)(n-2) + 3(n+1)(n)(n-2) - (n+1)(n)(n-1)}{2(n+1)} \\
I &= \frac{n^3 - 3n^2 + 2n -3(n^2-1)(n-2) + 3(n+1)(n)(n-2) - (n+1)(n)(n-1)}{2(n+1)} \\
I  &= \frac{n^3 - 3n^2 + 2n -3(n^3-2n^2-n+2) + 3(n+1)(n)(n-2) - (n+1)(n)(n-1)}{2(n+1)} \\
I &= \frac{-2n^3 + 3n^2 + 5n - 6  + 3(n^3-n^2 - 2n) - (n+1)(n)(n-1)}{2(n+1)} \\
I &= \frac{-2n^3 + 3n^2 + 5n - 6  + 3n^3-3n^2 - 6n - (n+1)(n)(n-1)}{2(n+1)} \\
I &= \frac{n^3 + 3n^2 + 5n - 6 - 3n^2 - 6n - (n+1)(n^2-n)}{2(n+1)} \\
I &= \frac{n^3 + 5n - 6 - 6n - (n+1)(n^2-n)}{2(n+1)} \\
I  &= \frac{n^3 - n - 6 - (n^3 -n^2 + n^2 - n)}{2(n+1)} \\
I &= \frac{n^3 - n - 6 - n^3 + n}{2(n+1)} = \frac{-6}{2(n+1)} \\
I &= \frac{-3}{n+1} \\
\end{align*} I have reason to believe that the value of this integral is: $$  \frac{3}{n+1} $$ Where did I go wrong in evaluating this integral?","['integration', 'calculus']"
3439419,Evaluating $\int_0^\infty e^{-ax}\frac{\sin{(bx)}}{x}dx $,"From the formula $\displaystyle\int_0^\infty e^{-tx}\frac{\sin{(x)}}{x}dx=\frac{\pi}{2}-\arctan{(t)}$ for $t>0$ , how to use change of variables to obtain a formula for $\displaystyle\int_0^\infty e^{-ax}\frac{\sin{(bx)}}{x}dx$ , when $a$ and $b$ are positive? Then how to use differentiation under integral sign with respect to b to find a formula for $\displaystyle\int_0^\infty e^{-ax}\cos{(bx)}dx$ when a and b are positive. My attempt:
 I know the result $\displaystyle\int_0^\infty e^{ax}\cos{(bx)}dx= \frac{e^{ax}}{a^2+b^2}(a\cos{(bx)}+b\sin{(bx)})$ . Is this result useful here?
Secondly integral calculator gives me this answer $$-\frac{i(Ei((ib-a)x)-Ei(-(ib+a)x))}{2}.$$ How to evaluate this answer  involving exponential integrals if a=4 and b=5?
Note=It was assumed that $(ib+a)\not=0$","['integration', 'improper-integrals', 'trigonometric-integrals', 'derivatives', 'exponential-function']"
3439436,Generalize Clairaut-Schwarz theorem to arbitrary order of mixed partial derivatives,"After reading the answer here to understand how to apply difference operator, I've figured out how to generalize my proof of Clairaut-Schwarz theorem here to arbitrary order of mixed partial derivatives. Could you please verify if my proof looks fine or contains logical gaps/errors? Thank you so much for your help! $\textbf{Generalized Clairaut-Schwarz Theorem:}$ Let $X$ be open in $\mathbb R^n$ , $f:X \to F$ , and $m \in \mathbb N$ . Suppose $j_1, j_2, \ldots, j_m \in\{1,\ldots,n\}$ and $\sigma$ is a permutation of $\{1, \ldots, m\}$ . If $\partial_{j_1} \partial_{j_2} \cdots \partial_{j_m} f$ is continuous at $a$ and $\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f$ exists in a neighborhood of $a$ , then $$\partial_{j_1} \cdots \partial_{j_m} f (a)= \partial_{j_{\sigma(1)}}  \cdots \partial_{j_{\sigma(m)}} f(a)$$ In my proof, I utilize two below lemmas: Let $\{e_1,\ldots, e_n\}$ be the standard basis of $\mathbb R^n$ . For $h \in \mathbb  R$ and $j \in \{1,\ldots,n\}$ , we define a map $\Delta_j^h f$ by $$\Delta_j^h f: X \to  F, \quad x \mapsto f(x+he_j)-f(x)$$ $\textbf{Lemma 1:}$ $$\partial_{j_1} \cdots \partial_{j_m} f (a) = \lim_{h_1 \to 0} \left ( \lim_{h_2 \to 0} \left( \cdots \left ( \lim_{h_m \to 0} \left( \frac{ \Delta_{j_1}^{h_1}  \cdots\Delta_{j_m}^{h_m} f (a)}{h_1 \cdots h_m} \right ) \right ) \cdots \right ) \right)$$ $\textbf{Lemma 2:}$ The finite difference operator is commutative, i.e. $$ \Delta_{j_1}^{h_1} \cdots\Delta_{j_m}^{h_m} f (a) = \Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)$$ $\textbf{My attempt:}$ By Mean Value Theorem, we have $$\begin{aligned} & \quad \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}} \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}}\\
=& \quad \frac{\Delta_{j_1}^{h_1} \cdots\Delta_{j_{m}}^{h_{m}} f (a)}{h_1 \cdots h_m} \quad \text{by} \,\, \textbf{Lemma 2} \\
=& \quad \frac{\partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) h_1 \cdots h_{m}}{h_1 \cdots h_m} \quad \text{by} \,\, \textbf{MVT} \\ =& \quad\partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) \end{aligned}$$ in which $$\begin{aligned} \min\{0,h_1\} < t_1 < \max\{0,h_1\} \\  \vdots\quad\quad\quad\quad\quad\quad \,\,\, \\ \min\{0,h_m\} < t_1 < \max\{0,h_m\} \end{aligned}$$ Hence $$\begin{aligned} & \quad \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \|\\
=& \quad \left \| \partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \|\end{aligned}$$ Let $t = |t_1| + \cdots+|t_{m}|$ and $h= |h_1| + \cdots+|h_{m}|$ . It follows from the continuity of $\partial_{j_1} \partial_{j_2} \cdots \partial_{j_m} f$ at $a$ that for all $\delta > 0$ there is $\epsilon > 0$ such that $$\left \| \partial_{j_{1}} \cdots \partial_{j_{m}} f (a + t_1 e_{j_1} + \cdots + t_{m} e_{j_{m}}) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| <\ \delta$$ for all $t < \epsilon$ . As such, for all $h <\ \epsilon$ , we have $$ \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| < \delta$$ Take the limit $h_{\sigma(m)} \to 0$ , we have $$\lim_{h_{\sigma(m)} \to 0} \left \|\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ and consequently $$ \left \| \lim_{h_{\sigma(m)} \to 0} \left (\frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m)}}^{h_{\sigma(m)}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m)}} \right ) - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ and consequently $$\left \| \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}}  \cdots\Delta_{j_{\sigma(m-1)}}^{h_{\sigma(m-1)}} \partial_{j_{m}} f (a)}{h_{\sigma(1)} \cdots h_{\sigma(m-1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta  \quad \text{by} \,\, \textbf{Lemma 1}$$ Iterating this process of taking limit, we get $$\left \| \lim_{h_{\sigma(1)} \to 0} \frac{\Delta_{j_{\sigma(1)}}^{h_{\sigma(1)}} \left ( \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right) (a)}{h_{\sigma(1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ or equivalently $$\left \| \lim_{h_{\sigma(1)} \to 0} \frac{ \left ( \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right) (a + h_{\sigma(1)} e_{\sigma(1)}) - \left (\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right)(a)}{h_{\sigma(1)}} - \partial_{j_{1}} \cdots \partial_{j_{m}} f (a) \right \| \le \delta$$ For all $\delta >0$ , there is $\epsilon >0$ such that for all $|h_{\sigma(1)}| < \epsilon$ , the last inequality holds. It follows that $$\partial_{j_{\sigma(1)}}\left (\partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f \right)(a) =  \partial_{j_{1}} \cdots \partial_{j_{m}} f (a)$$ and consequently $$\partial_{j_{\sigma(1)}} \partial_{j_{\sigma(2)}}  \cdots \partial_{j_{\sigma(m)}} f (a) =  \partial_{j_{1}} \cdots \partial_{j_{m}} f (a)$$ This completes the proof.","['partial-derivative', 'multivariable-calculus', 'real-analysis']"
3439455,Prove that the equation has only one real root.,"Prove that $(x-1)^3+(x-2)^3+(x-3)^3+(x-4)^3=0$ has only one real root.
It's easy to show that the equation has a real root using Rolle's theorem. But how to show that the real root is unique? By Descartes' rule of sign, it can be shown that it has 3 or 1 real root. But it doesn't guarantee that the real root is unique. If we calculate the root then it can be shown that it has only one real root.",['real-analysis']
3439475,$\mathbb{Q}(\sqrt{n})$ is Contained in the Cyclotomic Field of the $4n$'th Primitive Root of Unity.,"From Silverman and Tate, Rational Points on Elliptic Curves . Exercise 6.1 Let $\zeta'$ be the $4n$ 'th primitive root of unity. Use (c) to prove that $\mathbb{Q}(\sqrt{n})$ is contained in the cyclotomic field of the $4n$ 'th primitive root of unity. From previous exercises I know that (b) If $f(X) = X^n-1$ . Then $ \text{Disc}(f) =(-1)^{(n-1)(n-2)/2}n^n.$ (c) Let $\zeta$ be a primitive $n$ 'th root of unit. Then the cyclotomic field $\mathbb{Q}(\zeta)$ contains $\sqrt{\text{Disc}(f)}.$ We need to show that $\sqrt{n} \in \mathbb{Q}(\zeta')$ . Since $\zeta'$ is a $4n$ 'th primitive root of unity, it is a solution to $X^{4n} = 1$ . (so it is a root of $f(X) = X^{4n} -1$ ). By (b), $ \text{Disc}(f) =(-1)^{(4n-1)(4n-2)/2}(4n)^{(4n)}.$ By (c), $\sqrt{\text{Disc}} \in \mathbb{Q}(\zeta)$ . Since fields are closed under inverses and multiplication by integers, then $(n)^{(4n)}\in \mathbb{Q}(\zeta)$ . I don't see how to get from $\mathbb{Q}(\zeta)$ to $\mathbb{Q}(\zeta')$ .","['number-theory', 'cyclotomic-fields', 'roots-of-unity', 'discriminant']"
3439478,Clarification of definitions of function being a contraction in $L_{p}$ space,"This question arose from Durrett Theorem 4.1.4, as follows: Conditional expectation is a contraction in $L^{p}$ space for $p\geq 1$ . This question has been addressed here: Contraction Property of Conditional Expectation , and it seems that a conditional expectation being a contraction in $L_{p}$ means that it satisfies $$\|\mathbb{E}(X|\mathcal{F}))\|_{p}\leq\|X\|_{p}.$$ but this definition seems not working in other cases. For instance, what it means by a random variable being a contraction in $L_{p}$ ? From https://en.wikipedia.org/wiki/Contraction_(operator_theory) , a bounded operator $T:X\longrightarrow Y$ between normed linear spaces $X$ and $Y$ means it satisfies $\|T\|_{Y}\leq 1$ . So a random variable $X\in L_{p}(\Omega,\mathcal{F},\mathbb{P})$ being a contraction means it satisfies $\|X\|_{p}\leq 1$ ? But from here: https://en.wikipedia.org/wiki/Contraction_mapping it seems that a map being contraction has another meaning. Are these definitions equivalent or sort of related to each other or they are totally independent among each other? The word contraction appears only once in Durrett, which is in this theorem. So I do need a general clarification of the definition of ""contraction"" Please point it out if my question is not clear. Thank you so much! Edit 1: Thanks to ""kimchi lover"", now it is really clear. I edit this post to simply prove this statement and clarify everything for further readers. [Contraction]. Given two normed linear space $V$ and $W$ , a linear map $T:V\longrightarrow W$ is continuous if and only if there exists a real number $c$ such that $$\|T(v)\|_{W}\leq c\|v\|_{V},\ \text{for all}v\ \in V.$$ Then we define the operator norm of $T$ as $$\|T\|_{op}:=\inf\{c\geq 0:\|T(v)\|_{W}\leq c\|v\|_{V}\ \text{for all}v\ \in V\}.$$ An operator is said to be a contraction if $\|T\|_{op}\leq 1$ . [Refined Statement]. The operator of obtaining conditional expectation is a contraction in $L_{p}$ for $p\geq 1$ Proof: Let $(\Omega,\mathcal{F}_{0},\mathbb{P})$ be a probability space, $\mathcal{F}\subset\mathcal{F}_{0}$ a $\sigma-$ algebra, we show that the operator $$T:L_{p}(\Omega,\mathcal{F},\mathbb{P})\longrightarrow L_{p}(\Omega,\mathcal{F},\mathbb{P})\ \text{defined by}$$ $$X\mapsto\mathbb{E}(X|\mathcal{F}),$$ is a contraction. Firstly, the absolute value function $|\ \cdot\ |^{p}$ is convex for $p\geq 1$ . Therefore, by Jensen we have $$\Big|\mathbb{E}(X|\mathcal{F})\Big|^{p}\leq\mathbb{E}(|X|^{p}|\mathcal{F}),$$ so taking expectation we have $$\mathbb{E}\Big|\mathbb{E}(X|\mathcal{F})\Big|^{p}\leq\mathbb{E}\Big(\mathbb{E}(|X|^{p}|\mathcal{F})\Big).$$ Now, $\mathcal{F}$ is a $\sigma-$ algebra (in fact a sub- $\sigma-$ algebra), so $\Omega\in\mathcal{F}$ , then by the second criterion of conditional probability, we have $$\mathbb{E}\Big(\mathbb{E}(|X|^{p}|\mathcal{F})\Big)=\mathbb{E}|X|^{p}.$$ These together imply that $$\|\mathbb{E}(X|\mathcal{F})\|_{p}\leq\|X\|_{p},$$ which is saying that for all $X\in L_{p}(\Omega,\mathcal{F},\mathbb{P})$ , $$\|T(X)\|_{p}\leq\|X\|_{p}.$$ Therefore, the operator norm of $T$ , being the infimum, must be $\leq 1$ . Thus, $T$ is a contraction.","['conditional-expectation', 'definition', 'lp-spaces', 'probability-theory']"
3439492,How to show the cardinality $0$ is less than the cardinality $1$ [duplicate],"This question already has an answer here : Empty function, what is it? (1 answer) Closed 4 years ago . I know that to show this I have to show there’s an injection from the $\varnothing$ to $\{\varnothing\}$ .
But how can one even define a function on the $\varnothing$ ?","['elementary-set-theory', 'cardinals']"
3439540,Linking the intuition of topology with its axiomatic definition [duplicate],"This question already has answers here : How to get intuition in topology concerning the definitions? (6 answers) Closed 4 years ago . For a long time I have had an intuition on what topology is, without ever formally studying it. i.e the classic example of a mug being the same, topologically, as a doughnut, by stretching and reshaping while retaining ""holes"". I have recently started studying it formally, with the idea of a topology of a set being closed under unions and finite intersections etc. I can understand these definitions, but in no way can I relate these to my initial intuitive understanding of what topology is; they seem like separate subjects altogether. Could someone please help bridge this gap in my understanding?","['general-topology', 'intuition']"
3439589,Are conjugate vectors unique?,"A set of nonzero vectors $\{p_0,p_1,\ldots,p_{n-1}\}$ is said to be conjugate with respect to a symmetric positive definite matrix $A$ if $$
p_i^{\mathrm T}Ap_j=0
$$ for all $i\ne j$ . Such vectors are used in the conjugate gradient method . It follows that conjugate vectors are also linearly independent. An example of conjugate vectors are the eigenvectors of the matrix $A$ . Are the conjugate vectors unique up to a scalar multiple? In other words, if there are two sets of conjugate vectors with respect to the same symmetric positive definite matrix $A$ , are the vectors going to be the same up to a scalar multiple? I would guess that that they are unique up to a scalar multiple, but I am not sure. Any help is much appreciated!","['linear-algebra', 'vectors', 'positive-definite', 'symmetric-matrices']"
3439658,Give $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $|f'(x)|<1$ $ f(x) \neq x$ for all $x \in \mathbb{R}$,"Problem Give a function $f:\mathbb{R} \rightarrow \mathbb{R}$ , $C^\infty$ such that $1) |f'(x)|<1$ $2) f(x) \neq x$ for all $x \in \mathbb{R}$ My ideia The idea is to get a function that tends asymptotically to $y=x$ . If you reduce the condition to $|f'(x)|\le 1$ , it's easy but I don't know how to proceed with the condition $|f'(x)|<1$ .","['analysis', 'real-analysis', 'calculus', 'functions', 'limits']"
3439749,Showing that a first countable space is Hausdorff if and only if every sequence converging in $X$ has a unique limit,"Show that a first countable space $X$ is Hausdorff if and only if every sequence converging in $X$ has a unique limit. My Proof $X$ is Hausdorff, then it is trivial that every convergent sequence has a unique limit... Now assume that $X$ is not Hausdorff then $\exists x \neq y$ such that they cannot be seperated by disjoint open neighborhoods of $x$ and $y$ Now because $X$ is first-countable then there exists a sequence of decreasing neighborhoods $U_i$ of $x$ and $V_i$ of $y$ such that $U_i \cap V_j \neq \phi$ , in particular $U_i \cap V_i \neq \phi$ , choose $z_i\in U_i \cap V_i$ , then the sequence $( z_i) $ converges to both $x$ snd $y$ .
Is it correct? (note that the fact that $U_i$ and $V_i$ is decreasing sequence of neighborhoods is necessary to prove the convergence)
When $X$ is not Hausdorff, then there is a sequence whose limit is not unique. Is this proof correct.","['limits', 'general-topology', 'proof-verification', 'first-countable']"
3439760,"Let $X_1, X_2,X_3\sim\rm{ Bernoulli}(\theta)$. Show that $I_{X_1+X_2>X_3}$ is an unbiased estimator of $h(\theta)$ and find UMVUE of $h(\theta)$","Let $X_1, X_2,X_3\sim\rm{ Bernoulli}(\theta)$ . Show that $I_{X_1+X_2>X_3}$ is an unbiased estimator of $h(\theta)=P_{\theta}(X_1+X_2>X_3)$ and find UMVUE of $h(\theta)$ . Assuming $A = (X_1+X_2 > X_3)$ , is it correct that $E[I_A]= P(A)$ in this situation? If this is correct, then I know that for the estimator to be unbiased, $E[I_A] = P(A) = h(\theta)$ , which checks out. I also know that $I_A$ is the best unbiased estimator of $h(\theta)$ if it attains the Cramer-Rao lower bound: $$\operatorname{Var}\left(I_A\right) \geq \frac{\left(\frac{d}{d\theta}E[I_A]\right)^2}{E\left[\left(\frac{d}{d\theta} \ln\left(f(X\mid\theta)\right)\right)^2\right]}$$ When I try solving for $\operatorname{Var}\left(I_A\right)$ : \begin{align}
\operatorname{Var}\left(I_A\right) &= E[(I_A - E[I_A])^2] \\
&= E[(I_A - h(\theta))^2] \\
\end{align} I get stuck because I don't know how to simplify further (i.e. I don't know how to evaluate $h(\theta)$ ). Trying to solve for the left-hand side, I am also stuck because I don't know the joint pmf $f(X\mid\theta)$ that gives rise to $h(\theta)$ when integrated. Edit: Because $I_A$ is not the UMVUE, I need to find the best unbiased estimator of $h(\theta)$ . Toward this end, I figured out that the joint pmf is $$f(X\mid\theta) = \theta^{(X_1+X_2+X_3)}(1-\theta)^{(3-(X_1+X_2+X_3))}$$ and $$h(\theta) = \theta^2 + 2 \theta (1-\theta)^2$$ but how do I use this information to obtain an estimator $\delta$ such that $E[\delta] = h(\theta)$ ?","['statistics', 'parameter-estimation']"
3439806,cauchy's first theorem on limits of sequences,"Cauchy's first theorem on limits goes like this If $\ <f_n> $ be a sequence of positive terms and $$ \lim_{\ n\to\infty}\ f_n=l$$ Then $$ \lim_{ n\to\infty}\ [\ \frac{f_1+f_2+\dots+f_n}{n}]=l$$ Now this is an example of its application. $Q)$ Find the value of $$ \lim_{\ n \to \infty}\frac{1}{n}[1+\frac{1}{2}+\frac{1}{3}\dots+\frac{1}{n}] $$ $A)$ By cauchy's theorem if $$ f_n=\frac{1}{n} \ and \lim_{\ n\to\infty}\frac{1}{n}=0 $$ Then $$ \lim_{\ n\to\infty}\frac{1}{n}[1+\frac{1}{2}+\frac{1}{3}\dots+\frac{1}{n}]=0$$ I understand this example. Now here is another example and its solution which I found in various texts but I don't understand how can we apply cauchy's theorem to it $Q)$ Find the value of $$ \lim_{\ n\to\infty}[\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}+\dots+\frac{1}{2n^2}] $$ $A)$ Multiply and divide by n $$ \lim_{\ n\to\infty}\frac{1}{n}[\frac{n}{(n+1)^2}+\frac{n}{(n+2)^2}+\dots+\frac{n}{2n^2}] $$ Let $$ <f_n>=\frac{n}{(n+n)^2}=\frac{n}{4n^2}$$ Then, $$ \lim_{\ n\to\infty}f_n=\lim_{n\to\infty}\frac{1}{4n}=0$$ By cauchy's theorem $$ \lim_{\ n\to\infty}[\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}+\dots+\frac{1}{2n^2}]=0 $$ As you can see there is clearly a distinction between the first and second question In th first we have $ 1+\frac{1}{2}+\frac{1}{3}+\dots+\frac{1}{n}$ while in the second one we have $\frac{n}{(n+1)^2}+\frac{n}{(n+2)^2}+\dots+\frac{n}{(2n)^2} $ I see how the first one can be written as the sum of sequence $\frac{1}{n}$ But how can the second one written as the sum of the sequence $\frac{n}{(2n)^2}$ Please help....","['limits', 'sequences-and-series']"
3439843,Find $g$(3) if $g(x)g(y)=g(x)+g(y)+g(xy)-2$ and $g(2)=5$,"If $g(x)$ is a polynomial function satisfying $g(x)g(y)=g(x)+g(y)+g(xy)-2$ for all real $x$ and $y$ and $g(2)=5$ , then find $g$ (3) Putting $x=1,y=2$ $$
5g(1)=g(1)+5+5-2\implies \boxed{g(1)=2}
$$ And I think I can evaluate for $g(1/2)$ also. Solution given in my reference is $g(3)=10$ , but I think I am stuck with this.","['functions', 'polynomials']"
3439850,Contour integral of rational function to some power,"I am interested in the following integral $$\int_{0}^{2\pi}d\theta \left(\frac{1-|x|^2}{|1-x e^{-i\theta}|^2}\right)^{y}$$ Does it has a nice closed form expression?
I know that $$\int_{0}^{2\pi}d\theta \left(\frac{1-|x|^2}{|1-x e^{-i\theta}|^2}\right)=2\pi$$ by contour integration.
Any hints are welcome.","['trigonometry', 'definite-integrals']"
3439861,"Orthogonal projections on $M$, $L$ and $M\cap L$ in Hilbert space","I'm dealing with this functional analysis exercise: Let $H$ be a Hilbert space and $M,L$ are two closed subspaces of $H$ . $P_M$ is the orthogonal projection operator. Prove $P_MP_L=P_{M\cap L} \iff P_MP_L=P_LP_M$ . "" $\Longrightarrow$ "" part is easy: $P_LP_M=P_{L\cap M}=P_{M\cap L}=P_MP_L$ . But for the "" $\Longleftarrow$ "" part, denote $P=P_MP_L=P_LP_M$ . Then $Px=P_M(P_Lx)\in M$ and similarly in $L$ . Therefore $Px\in L\cap M$ . But how can I prove it is an orthogonal projection on $L\cap M$ ?","['hilbert-spaces', 'functional-analysis']"
3439864,"Show $f(x)=x-x^2/2$ is not a contracting map on $[0,1]$.","We define a contracting map as a function such that $|f(x)-f(y)|\leq \alpha|x-y|$ for some $\alpha\in(0,1)$ . Consider $f:[0,1]\to[0,1]$ given by $f(x)=x-x^2/2$ . I wish to show that this map is not contracting on $[0,1]$ . Suppose that such a constant $\alpha$ exists. By the mean value theorem, $$f'(c)=\frac{|f(x)-f(y)|}{|x-y|}\leq \alpha$$ for some constant $c\in (0,1)$ , and thus $f'(c)=1-c\leq \alpha$ . I don't see how this helps, since it implies that $1\leq c+\alpha$ , which is not yet a contradiction.","['calculus', 'derivatives']"
3439953,Prove that the segment $MN$ intersects the segment $BD$ at its own midpoint.,"Let $ABCD$ be a trapezium such that the side $AB$ and $CD$ are parallel and the side $AB$ is longer than the side $CD$ . Let $M$ and $N$ be on segments $AB$ and $BC$ respectively,such that each of the segments $CM$ and $AN$ divides the trapezium in two parts of equal area. Prove that the segment $MN$ intersects the segment $BD$ at its own midpoint. I worked out that $BM=AM+CD$ and if you let the midpoint be $O$ and you have $MN$ meet $CD$ at $K$ then $OKD$ must be congruent to $OMB$ for $O$ to be the midpoint But Im not sure if that helps or what to do next, solutions would be appreciated Taken from the 2016 Pan African Math Olympiad http://pamo-official.org/problemes/PAMO_2016_Problems_En.pdf","['contest-math', 'euclidean-geometry', 'geometry']"
3439966,"If $y$ simplifies to $a\pi + $b then find ($a – b$), for the following condition.","Let $y=\sin^{-1}(\sin 8)–\tan^{-1}(\tan 10)+\cos^{-1}(\cos 12)–\sec^{-1}(\sec 9) +\cot^{-1}(\cot 6)– \mathrm{cosec}^{-1}(\mathrm{cosec} 7)$ . If $y$ simplifies to $a\pi + $ b  then find ( $a – b$ ) My attempt is as follows:- $$\sin^{-1}(\sin 8)=3\pi-8$$ $$\tan^{-1}(\tan 10)=-3\pi+10$$ $$\cos^{-1}(\cos 12)=4\pi-12$$ $$\sec^{-1}(\sec 9)=3\pi-9$$ $$\cot^{-1}(\cot 6)=2\pi-6$$ $$\mathrm{cosec}^{-1}(\mathrm{cosec} 7)=7-2\pi$$ $$y=(3\pi-8)+(3\pi-10)+(4\pi-12)-(3\pi-9)+(2\pi-6)-(7-2\pi)$$ $$y=11\pi-34$$ so $a=11, b=-34$ , hence $a-b=11+34=45$ But actual answer is $53$ , I don't know where I am going wrong. I checked the solution lot of times but didn't get any breakthrough.",['trigonometry']
3439988,What is the solid angle in higher dimensions?,"I'm struggling with understanding the generalization of a solid angle to higher dimensions. What is the intuition behind a solid angle in general and how does it generalize in higher dimensions? Edit: One thing that is not very clear to me: For 3D space $d\Omega = \dfrac{dS}{r^2}$ so the solid angle is the ratio of the area subtended to the square of the radius, how can I write the n dimensional infinitesimal solid angle?","['geometry', 'differential-geometry']"
3440036,$\underset{n=1}{\overset{\infty}{\sum}}\frac{a_n}{n!}\in\mathbb{Q}\Longleftrightarrow\exists N\in\mathbb{N}\backepsilon a_n=n-1\forall n\ge N$,"Given integers $a_1,a_2,...$ such that $1\le a_n\le n-1$ , $n=2,3,...$ Show that the sum of the series $\underset{n=1}{\overset{\infty}{\sum}}\frac{a_n}{n!}$ is rational if and only if there exists integer $N$ such that $a_n=n-1$ for all $n\ge N$ . My attempt If part: $a_n=n-1$ $\forall n\ge N$ $\Rightarrow\frac{a_n}{n!}=\frac{n-1}{n!}=\frac{1}{(n-1)!}-\frac{1}{n!}$ $\forall n\ge N$ $\Rightarrow\underset{k=N}{\overset{n}{\sum}}\frac{a_k}{k!}=\frac{1}{(N-1)!}-\frac{1}{n!}$ $\Rightarrow\underset{n=1}{\overset{\infty}{\sum}}\frac{a_n}{n!}=\underset{n\rightarrow\infty}{\lim}\underset{n=N}{\overset{n}{\sum}}\frac{a_n}{n!}=\frac{1}{(N-1)!}\in\mathbb{Q}$ I need help with the only if part.","['limits', 'convergence-divergence', 'sequences-and-series']"
3440090,Normal functional equation: $ f \big( x f ( y ) + f ( x ) \big) + f \left( y ^ 2 \right) = f ( x ) + y f ( x + y ) $,"Find all functions $ f : \mathbb R \to \mathbb R $ such that for any real numbers $ x $ and $ y $ we have $$ f \big( x f ( y ) + f ( x ) \big) + f \left( y ^ 2 \right) = f ( x ) + y f ( x + y ) $$ What I tried: $$ P ( 0 , - x ) \implies f \big( f ( 0 ) \big) + f \left( x ^ 2 \right) = f ( 0 ) - x f ( - x ) = f \left( x ^ 2 \right) $$ which implies that $ f $ is odd, because $$ P ( 0 , x ) \implies f \big( f ( 0 ) \big) + f \left( x ^ 2 \right) = f ( 0 ) + x f ( x ) = f \left( x ^ 2 \right) $$ Also, considering $$ P ( 1 , 1 ) \implies f \big( 2 f ( 1 ) \big) = f ( 2 ) \implies f ( 1 ) = 1 $$ Let $ a , b \in \mathbb R $ such that $ a ^ 2 = b $ . $$ P \left( a ^ 2 , a \right) \implies f \Big( a ^ 2 f ( a ) + f \left( a ^ 2 \right) \Big) = a f \left( a ^ 2 + a \right) \text . $$ Now with the observation that $ f ( - x ) = - f ( x ) $ , we can deduce that $ f ( x ) = x $ . How to proceed?","['contest-math', 'functions', 'solution-verification']"
3440102,Prove that $( R \circ S ) \cap T = \varnothing$ iff $(\mathrm{R}^{-1} \circ T) \cap S= \varnothing$.,"I am having a bit of a hard time proving the following statement: Show that $( R \circ S ) \cap T = \varnothing$ iff $(\mathrm{R}^{-1} \circ T) \cap S= \varnothing$ . I sort of understand composition of functions, inverse functions, and set theory individually, but when put together I seem to get confused. It would be wonderful if anyone could be of help, as I'm not even sure how to begin. Thank you! EDIT: See attempt at the question in the answers","['elementary-set-theory', 'inverse-function', 'discrete-mathematics', 'function-and-relation-composition']"
3440119,Homomorphisms with the same kernel has the same semidirect product?,"Suppose I am trying to construct a semidirect product and I have two homomorphisms $\varphi_1:K\rightarrow\text{Aut}(H)$ and $\varphi_2:K\rightarrow\text{Aut}(H)$ . Furthermore, suppose that $\ker\varphi_1\cong\ker\varphi_2$ . Are the semidirect products constructed by these homomorphisms equivalent? For an explicit example, suppose I wanted to construct $Z_7\rtimes (Z_2\times Z_2)$ , Aut( $Z_7)\cong Z_6$ . If we have $Z_2\times Z_2=\langle a\rangle\times\langle b\rangle$ , the following homomorphisms both work (defined on the generators): $$\varphi_1(a)=1\quad \varphi_1(b)=x$$ $$\varphi_1(a)=x\quad \varphi_1(b)=x$$ where $x$ is the automorphism of order $2$ in Aut $(H)$ . In both of these cases, the kernel is isomorphic to $Z_4$ . Is it true that $Z_7\rtimes_{\varphi_1} (Z_2\times Z_2)\cong Z_7\rtimes_{\varphi_2} (Z_2\times Z_2)$ ?",['group-theory']
3440139,Is it possible for the intersection of nonempty subset to be empty?,"The question is: Give an example of nonempty closed sets $$C_1  \supset C_2  \supset \ ... $$ such that the intersection $$\bigcap U_i$$ is empty. I think that this is not possible. If the set $A$ is a subset of the set $B$ then all elements in $A$ are contained in $B$ . In the above case, unless $U_n$ is empty, the intersection $U_i$ will be nonempty. Is this reasoning correct? Thanks!","['elementary-set-theory', 'general-topology']"
3440187,How should trig functions be pronounced?,"For $\sin$ , I think that it is pretty obvious to just say the full name. However, for the others do you sound out the abbreviation or do you say the full name? I have also heard some people pronounce $\csc$ as ""cosec"". Is there a general consensus or convention?","['trigonometry', 'terminology']"
3440204,Prove that the product of two continuous functions is continuous using the sequential definition of continuity,"Using the following definition, prove that the product of two continuous functions is continuous. Let $f$ be a funtion from $\mathbb{R}^n$ to $\mathbb{R}$ . The function
  f is continuous if for every point $p$ , for every sequence $x_n\to p$ , $f(x_n)\to f(p)$ Here is my attempt at the proof: Let $f,g:\mathbb{R}^n\to\mathbb{R}$ be continuous functions that are given. Therefore: For every point $p$ , for every sequence $a_n\to p$ , $f(b_n)\to f(p)$ . For every point $q$ , for every sequence $b_n\to p$ , $g(b_n)\to g(q)$ . Let $F:\mathbb{R}^n\to\mathbb{R}$ be a function defined by $f\cdot g$ . We wish to show that $F$ is continuous. That is, for every point $p$ , for every sequence $x_n\to p_o$ , one has $F(x_n)\to F(p_o)$ . Take the point $p_o$ to be the product of $p$ and $q$ , and the sequence $x_n$ to be the defined as $a_n b_n$ . Therefore $x_n \to p_o$ . The function $F(x_n) \to F(p_o)$ if given $\epsilon >0,$ there is some $N$ such that whenever $n>N$ , $|F(x_n) - F(p_o)| < \epsilon$ This is where I'm stuck. I think I'm on the right track with taking the product of the two sequences but it seems like I might need to take the proof by contradiction and start with $$|F(x_n) - F(p_o)| \geq \epsilon$$ Is this the correct approach to be taking? Thanks!","['limits', 'functions', 'continuity']"
3440223,"Solid region $S$ bounded by parabolic cylinder $z+x^2 = 2$ and planes $z=0$, $y=0$ and $y=x$.","The problem There exists a (unique, I guess) solid region $S$ bounded by parabolic cylinder $z+x^2 = 2$ and planes $z=0$ , $y=0$ and $y=x$ . Sketch the part of $S$ in the first octant of $\mathbb R^3$ . Set up the bounds for the triple integral $$I_S = \int\int\int _S f dz dy dx$$ for any suitable $f: \mathbb R^3 \to \mathbb R$ (uniformly continuous, Riemann integrable, Lebesgue integrable or whatever you need). Questions : Question : For 1, how is $S$ not completely in the first octant (except possibly for some of its boundary)? I think $S$ is a cut out of an infinite loaf of bread, with the bottom as $z=0$ cut and the top as the $z+x^2=2$ , where the infinite loaf is cut by $y=0$ and $y=x$ . Looks like first octant to me. Question : For 2, this is what I got at least for $S$ in the first octant, denoted $S_1$ . Is this correct? $z$ is from $0$ to $2-x^2$ . So we get $$I_{S_1} = \int\int_{R_1} \int_{z=0}^{z=2-x^2} f dy dx$$ for some region $R_1$ . Afterwards, I let $g = \int_{z=0}^{z=2-x^2} f dz$ , $g: \mathbb R^2 \to \mathbb R$ and pretend we're in the $xy$ -plane ( $z=0$ ) and then set up the bounds for the double integral $$I_{S_1} = \int\int_{R_1} g dy dx$$ over $R_1$ which appears to be a triangle bounded by $y=x$ , $x=\sqrt{2}$ and $y=0$ . Therefore, $$I_{S_1} = \int_{x=0}^{x=\sqrt{2}} \int_{y=0}^{y=x} g dy dx = \int_{x=0}^{x=\sqrt{2}} \int_{y=0}^{y=x} \int_{z=0}^{z=2-x^2} f dz dy dx$$ Question : If $S$ is not completely in the first octant (except possibly for some of its boundary), then why, and what's the integral? See below. Update Based on Quanto's answer, in particular the term 'third octant' (which I hope means negative y, negative x and positive z based on the '2' in the 'gray code' here ), I realised my (implicit) mistake in extending my answer to Question (2). Here's what I think to do: Forget $y=x$ and $y=0$ for now. The solid region bounded by $z=0$ and $z+x^2=2$ is $S^\# = \{(x,y,z) \in \mathbb R^3| z \ge 0, z=2-x^2\}$ . I see this as an infinite hollow loaf of bread where the bread is shaped parabolically and where the $y$ -axis is the centre of the base of the infinite loaf. Observe $S^\#$ can also be written $S^\# = \{(x,y,z) \in \mathbb R^3| z=2-x^2, x^2 \le 2 \}$ , i.e. the condition ' $z \ge 0$ ' is equivalent to the condition ' $x^2 \le 2$ '. Flattening $S^\#$ , in the sense of projecting $S^\#$ onto $z=0$ , gives us $\tilde A = \{(x,y,z) \in \mathbb R^3| z=0, x^2 \le 2 \}$ which is pretty much the same as (In higher maths: 'pretty much the same as' means 'homeomorphic, as topological manifolds with boundary, to' or something) the set $A = \{(x,y) \in \mathbb R^2|x^2 \le 2 \}$ of 2 parallel vertical lines and the space in between them. Now we bring back $y=x$ and $y=0$ . It appears my mistake was that I implicitly assumed 'bounded by $y=0$ ' meant 'is contained in $y \ge 0$ '. Observe that $A$ is cut up by $y=x$ and $y=0$ into 4 spaces (Let these spaces include the boundaries), 2 of which are bounded. Denote these bounded spaces as $A_1$ and $A_2$ . One of $A_1$ and $A_2$ without its boundary is completely in the first quadrant. The other without its boundary is completely in the third quadrant. Let $A_1$ be the former and $A_2$ be the latter. The part $S_1$ (without boundary) of $S$ in the first octant projects to $A_1$ (without boundary). $A_2$ (without boundary) indeed corresponds to a non-empty part $S_2$ (without boundary) of $S$ not in the first octant. Just as $A_2$ (without boundary) is in the third quadrant, so do we have $S_2$ (without boundary) to be in the third octant. Now, the bounds for $z$ in $I_{S_2}$ are the same as in $I_{S_1}$ , I think: $$I_{S_2} = \int\int_{R_2}\int_{z=0}^{z=2-x^2} f dy dx = \int\int_{R_2} g dy dx$$ Now, $R_2$ is the region in $\mathbb R^2$ which appears to be a triangle bounded by $y=x$ , $x=-\sqrt{2}$ (rather than $x=\sqrt{2}$ !) and $y=0$ . Therefore, $$I_{S_2} = \int_{x=-\sqrt{2}}^{x=0} \int_{y=x}^{y=0} g dy dx = \int_{x=-\sqrt{2}}^{x=0} \int_{y=x}^{y=0} \int_{z=0}^{z=2-x^2} f dz dy dx$$ Finally, I believe that $S = S_1 \cup S_2$ and that $S_1 \cap S_2$ is empty except for boundary points. Therefore, $$I_S = I_{S_1} + I_{S_2} = \int \int \int_{S_1} f dz dy dx + \int \int \int_{S_2} f dz dy dx$$","['integration', 'multivariable-calculus', 'calculus']"
3440257,"How to prove that if $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous and $|f(x) - f(y)| > |x-y|$ for all $x,y$,then range of $f$ is $\mathbb{R}$ [duplicate]","This question already has answers here : Proving that a certain continuous function is surjective. (2 answers) Closed 4 years ago . Question: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be continuous and $|f(x) - f(y)| > |x-y|$ for all $x,y \in \mathbb{R}$ and $x \neq y$ . Prove that the range of $f$ is $\mathbb{R}$ Answer: I have a hard time writing this. For any [ $x_0, x_1$ ], it must be that the function within this interval is strictly increasing or decreasing. Otherwise, there is a max (or min) value $f(x_2)$ such that $x_2 \in (x_0,x_1)$ . Applying the Intermediate Value Theorem on $[x_0,x_2]$ and $[x_2,x_1]$ , then there exists a point $a \in [x_0,x_2]$ and $b \in [x_2,x_1]$ such that $f(a)=f(b)$ , contradicting $|f(x) - f(y)| > |x-y|$ for all $x,y \in \mathbb{R}$ . Furthermore, for arbitrary $y_0, y_1, y_2$ , if $|f(y_2)-f(y_1)| > |y_2-y_1|$ and $|f(y_1)-f(y_0)| > |y_1-y_0|$ then we have $|f(y_2)-f(y_0)| > |y_2-y_0|$ . Otherwise it means that the function on $[y_0,y_1]$ and $[y_1,y_2]$ were not both strictly increasing or strictly decreasing, which leads to the same contradiction, that there exist a $c \in [y_0,y_1]$ , $d\in [y_1,y_2]$ on each interval respectively where $f(c)=f(d)$ . Would I be able to then conclude that the range is $\mathbb{R}$ ? Logically it seems like I can, but it also doesn't seem to be based on any ""theorem"". I'm essentially just saying ""dot dot dot and so on"". I don't know if I'm missing anything, or if the arguments are not sound. Is there a more rigorous way to prove this?","['continuity', 'real-analysis']"
3440311,How are the logarithmic integrals $\int_{-\pi}^{\pi} \ln^n(2\operatorname{cos}(x/2))dx$ related to $\zeta(n)$?,"Suppose we have defined the ""cochord"" of an angle $\theta \in (-\pi,\pi)$ as $$\operatorname{coc}(\theta) := 2\cos\left(\frac \theta 2\right),$$ and set $$c_n := \frac 1 \pi \int_{-\pi}^{\pi} \ln^n (\operatorname{coc} x)\ dx. $$ I have employed the Fourier expansion $$\tag 1 \ln(\operatorname{coc}x) = -\sum_{k=1}^\infty \frac {(-1)^k} k \cos(kx) $$ to obtain, via Parseval's identity w.r.t. to the usual inner product on $L^2(-\pi,\pi)$ , the interesting result $$ \tag2 \boxed{c_2 = \zeta(2)} = \frac{\pi^2}6. $$ Even more miraculously, it seems that $$\boxed{-\frac 1 3 c_3 = \zeta(3)} = 1.2020569... \tag3$$ Now I cannot seem to find any similarly satisfying relationship between $c_n$ and $\zeta(n)$ , for integer $n \geq 4$ . (It does also look like $c_1 = 0$ , but I haven't bothered to try to prove it.) Does anyone have any idea as to what the general relation might be, and why it should be the case that these logarithmic integrals are related to $\zeta$ at all? [Wolfram Mathworld has an article on a similar set of integrals, where $\operatorname{coc}(x)$ has been substituted by the usual cosine, but so far I have not been successful at transforming these into something closer to $c_n$ .] Edit. Here is my follow-up question concerning the closed form for $c_n$ .","['integration', 'fourier-series', 'riemann-zeta']"
3440351,When is a surface integral equal to double integral over projection? A verification of Stokes' Theorem. Intuition and relation to Green's Theorem.,"I'm helping a calculus student. It's been awhile since I've done some vector calculus. I know Stokes' Theorem in terms of differential forms and manifolds with boundary, but I've forgotten much of Stokes' Theorem in the undergraduate setting. The problem is (paraphrased) Consider the vector field $F(x,y,z) = [2z-y, x+z, 3x-2y]^T$ . Let $\Sigma$ be the surface of the paraboloid defined by $z \ge 0$ and $z = 9-x^2-y^2$ . Let $\Sigma$ 's unique outward-pointing normal vector be $n$ . Verify Stokes' Theorem $$\int_C F^T dr = \int \int_{\Sigma} (\nabla \times F)^Tn dS \tag{A}$$ Some notes : I think we have $F: \mathbb R^3 \to \mathbb R^3$ (or $F: \mathbb R^3 \to \mathbb R^6$ if you think of tangent bundle), but no range is given. The domain isn't given either. It could be $\mathbb R^3 \setminus 0$ for all we know, but I think, in undergraduate, a map with 3 variables usually has domain $\mathbb R^3$ unless the problem says otherwise. I think paraboloids are surfaces, so I guess 'the surface of the paraboloid' is in a similar meaning as 'game of tennis' rather than 'colour of the phone'. Or 'the surface of the paraboloid defined ... $-y^2$ ' means 'the section of the surface, which is the paraboloid $z=9-x^2-y^2$ , defined by intersecting the paraboloid with $z \ge 0$ ' No $C$ is mentioned. I guess this is up to the students to determine. Questions Question : Is this a correct computation for the line integral? For C: The boundary of $\Sigma$ is $B = \{x^2+y^2=9\} \times \{0\}$ . Therefore, we choose $C$ to be the projection of $B$ onto $z=0$ , which is the circle $C = \{x^2+y^2=9\}$ . Equivalently, I think, we choose $C$ to be the boundary of the projection of $S$ onto $z=0$ , where the projection is, I think, the disc $D = \{x^2+y^2 \le 9 \}$ . I guess $B$ is the manifold boundary of $\Sigma$ . I mostly just pretend that I remember what is meant by 'boundary' in undergraduate. For F: For $z=0$ , $F=F(x,y,z)$ becomes the projection of $F=F(x,y,0) = [-y,x,3x-2y]^T$ onto $z=0$ which is $F=[-y,x]^T$ (I think this new $F$ is the pullback $\iota^{*}F$ for $\iota: C \to S$ ) For the bounds: I think $0$ to $2 \pi$ , where we parametrise $C$ as the image of the curve $r: [0, 2 \pi] \to \mathbb R^2$ , $r(t)=[x(t),y(t)]^T=[3\cos(t),3\sin(t)]^T$ Hence, the line integral is $$\int_C F^T dr = \int_0^{2 \pi} [-y(t),x(t)] [dx(t), dy(t)]^T$$ with $x(t) = 3 \cos(t)$ , $y(t)= 3 \sin(t)$ , $dx(t) = -3 \sin(t) dt$ , $dy(t) = 3 \cos(t)$ , $dr(t) = [dx(t),dy(t)]^T$ . Therefore, the line integral is $18 \pi$ , as computed here . Question : Is this another correct computation for the line integral? I use Green's theorem $$\int_C F^T dr = \int \int_{R} (\nabla \times F)^T[0,0,1]^T dA$$ The $F$ in the line integral is $F=[-y,x]^T$ . The $F$ on the double integral, I think, can be either $[-y,x,0]^T$ or $[-y,x,3x-2y]^T$ because, regardless, $(\nabla \times F)^T[0,0,1]^T = 2$ . The $C$ is the still the circle. I choose $R=D$ from Question (1). Hence, the line integral is equal to the double integral $$\int_C F^T dr = \int \int_{D} 2 dx dy = 2 \int \int_{D} dx dy = 2 \ \text{Area}(D)$$ Therefore, the line integral is again $18 \pi$ . Question : Which of the two computations for the surface integral are correct? For both computations: For curl: $curl F = \nabla \times F = [-3,-1,2]^T$ , as computed here . For n ( First method ): Form the explicit surface $\{z=f\}$ , where $f$ is $f: \mathbb R^2 \to \mathbb R$ , $f(x,y) = x^2+y^2-9$ . Let $\nabla f = [f_x, f_y]^T$ . Then $n$ is normalised version of either $[-f_x, -f_y, 1]^T = [-2x,-2y,1]^T$ or $[f_x, f_y, -1]^T = [2x,2y,-1]^T$ . I guess $[-f_x, -f_y, 1]^T$ is outward-pointing. For n ( Second method ): Alternatively, form the implicit surface $\{h=0\}$ , where $h$ is $h: \mathbb R^3 \to \mathbb R$ , $h(x,y,z) = x^2+y^2-9-z$ . Then $n$ is normalised version of either $\nabla h = [h_x, h_y, h_z]^T = [2x,2y,-1]^T$ or $-\nabla h$ . I guess $-\nabla h$ is outward-pointing. The first computation : For the bounds and for $dS$ : I'm actually not really sure of this. Off the top of my head , I think I kind of just project $\Sigma$ onto $z=0$ to get $D$ from Question (1). ( See Question (4). ) Therefore, I think $dS = |\nabla h| dA = |\nabla h| dx dy$ , where we use the notation ' $dS$ ' (of course in differential geometry, $dS$ and $dA$ are no longer just notation) for $\Sigma$ and ' $dA$ ' for $D$ , though I think we should have instead 'dA = du dv' instead of 'dA = dx dy' ( see the 'However, upon...' later a few bullet points down ). Hence, the surface integral does not need any of the $\nabla h$ , $\nabla f$ above and just uses $n = [0,0,1]^T$ : $$\int \int_{\Sigma} (\nabla \times F)^Tn dS = \int \int_{D} (\nabla \times F)^T[0,0,1]^T dA = \int \int_{D} (\nabla \times F)^T [0,0,1]^T dx dy$$ $$= \int \int_{D} [-3,-1,2]^T [0,0,1]^T dx dy = \int \int_{D} 2 dx dy = \int_{0}^{2 \pi} \int_{0}^{3} 2 dx dy$$ Therefore, the surface integral is $18 \pi$ , with precisely the same computation as in Question (2). The second computation : However, upon closer inspection of my undergraduate calculus textbook ( no longer off the top of my head ), what is supposed to be done, I think, is something like: $$\int \int_{\Sigma} (\nabla \times F)^Tn dS = \int \int_{P} (\nabla \times F)^Tn |\nabla h| dA$$ where $P$ is our 'parameter domain', so our $dA$ becomes not $dx dy$ but actually $du dv$ (which makes more sense actually). Our surface is given explicitly as $\Sigma = \{z=f(x,y)=9-x^2-y^2 | z \ge 0\}$ , but we don't really have an explicit parameter domain $P=\{(u,v)\}$ here to have $\Sigma$ as the image of some $r(u,v)$ , $r: P \to \mathbb R^3$ . The book says we can $x,y$ as our parameters $u,v$ , when our surface is given explicitly, so, for some parameter domain $P$ , we have $\Sigma = \{z=f(x,y)=9-x^2-y^2 | (x,y) \in P\}$ . Converting ' $z \ge 0$ ' into ' $(x,y) \in P$ ' seems to be done in taking $P=D$ , i.e. we take our parameter domain $P$ to be the projection $D$ of the surface $\Sigma$ , i.e. the surface $\Sigma$ is parametrised by its projection $D$ . Edited to add : Oh, I guess this is the 3D version of the same idea for curves: This part of a parabola $L = \{y=x^2 | y \in [0,1] \}$ is explicit and so although one might parametrise $L$ as $L = \{(x,y)=(t,t^2) | t \in [-1,1]\}$ with a new variable $t$ , one can also do $L = \{(x,y)=(x,x^2) | x \in [-1,1] =: P_L\}$ , and indeed $P_L := [-1,1]$ is both the parameter domain of the map $r: [-1,1] \to \mathbb R^2$ , $r(t) = (t,t^2)$ with image $L$ and the projection of $L$ into a dimension one lower than the dimension of $L$ . Therefore, it appears that the intuition of the idea of flattening, a.k.a. projecting, $\Sigma$ onto $z=0$ to get $D$ is justified by the fact that the flattening, a.k.a. projection, $D$ can be used to parametrise $\Sigma$ . Anyway, the surface integral should instead be computed as follows $$\int \int_{\Sigma} (\nabla \times F)^Tn dS = \int \int_{D} (\nabla \times F)^Tn |\nabla h| dA = \int \int_{D} (\nabla \times F)^T \frac{-\nabla h}{|\nabla h|} |\nabla h| dx dy$$ $$= \int \int_{D} [-3,-1,2]^T [-2x,-2y,1]^T dx dy = \int \int_{D} 6x +2y + 2 dx dy = \int_{0}^{2 \pi} \int_{0}^{3} [6x(r,\theta) +2y(r,\theta) + 2] (r dr d\theta)$$ Therefore, the surface integral is $18 \pi$ , as computed here . Question : It appears in this case we can convert a surface integral into a double integral $$\int \int_{D} (\nabla \times F)^T[0,0,1]^T dA = \int \int_{\Sigma} [-3,-1,2]^T  dS \tag{B}$$ where $D$ is the projection of $\Sigma$ onto $z=0$ (or $D$ is the region in $z=0$ that is diffeomorphic to the boundary of $\Sigma$ or something). What's going on? It appears we have the fact that for an explicit surface $\Sigma$ , we can take $P=D$ to parametrise $\Sigma$ . This fact suggests, if not outright implies, that we can compute a surface integral over $\Sigma$ as the double integral over $\Sigma$ 's projection $(P=)D$ . I know Stokes' theorem is a generalisation of not only the fundamental theorem of calculus and the fundamental theorem of line integrals but also Green's theorem and that the theme of all of these is that, whenever applicable, the integral of something depends only on the values of the 'antidervative' ( $f(x)$ for $f'(x)$ , $f(x)$ for $\nabla f(x)$ , $F(x)$ for $curl F(x)$ , etc) values at the boundary points. However, it seems that whenever Stokes' theorem is applicable for a surface integral over $\Sigma$ , we can convert the integral into a double integral over $D$ , a flattening of the $\Sigma$ , bypassing any need to obtain the 'boundary' $B$ of $\Sigma$ or 'boundary $C$ of $\Sigma$ 's projection $D$ . It actually seems more practical to compute the double integral over $D$ than the line integral over $B$ or $C$ . ( See Question (5). ) I think Stokes' theorem is actually more of something with an equation looks like $(B)$ and then $(A)$ is a corollary of $(B)$ . (You might end up proving $(A)$ and $(B)$ are equivalent forms of Stokes' theorem.) Something that would have an equation like $(B)$ might be: Under the same assumptions of Stokes' theorem in $(A)$ , you can evaluate the surface integral over $\Sigma$ into the double integral over the projection $D$ of $\Sigma$ . The fact that Green's Theorem is used in proving the Stokes' Theorem in (A) suggests there's indeed a form of Stokes' Theorem like in $(B)$ that is simply converting a surface integral into a double integral. Of course, this form $(B)$ would not be in line with the theme mentioned in the preceding paragraph. I think $$\int_C F^T dr = \int_B F^T dr = \int \int_D (curl F)^T [0,0,1]^T dA = \int \int_{\Sigma} (curl F)^T n dS$$ or something. Here, $\Sigma$ and $D$ have respective boundaries $B$ and $C$ . The projections of $\Sigma$ and $B$ are respectively $D$ and $C$ . Question : ( A follow-up to Question (4) ) If I'm right about the existence of some kind of form $(B)$ for Stokes' theorem or right about something in my bullet points in Question (4), then while I see the theoretical point of Stokes' Theorem in form $(A)$ , I don't see the practical point. What is the practical point ? I don't think form $(A)$ is practical for computing line integrals over a boundary curve $B$ of a surface $\Sigma$ because we could instead compute line integral over projection $C$ of the boundary curve $B$ of the surface $\Sigma$ , a.k.a. the boundary curve $C$ of the projection $D$ of the surface $\Sigma$ , which we would do using Green's theorem. I don't think form $(A)$ is practical for computing surface integrals over a surface $\Sigma$ because if I convert the surface integral, under form $(A)$ into a to line integral over the boundary curve $B$ , why wouldn't I then just use Green's Theorem to further convert the line integral over $B$ (into a line integral over $C$ ) into a double integral over $D$ ? Edited to add : EuYu might not have explicitly said this in the comments, but maybe my mistake here is assuming is assuming Green's Theorem is applicable. In general, $B$ and $C$ are not curves in $\mathbb R^2$ or even curves that lie on a single plane. Also, in general, $B \ne C$ , which was kinda unlike the case above, where $B = \{x^2+y^2 = 9\} \times \{0\} = \{x^2+y^2=9\} = C$ and furthermore even if $B \ne C$ but $C$ is the projection of $B$ , I don't believe the line integral is the same. A simple example (for non-closed curves and 1-dimension lower): $C=[0,1]$ , the closed (in the sense of containing $\{0,1\}$ rather than in the sense of having the same start and end points) unit interval in $\mathbb R^1$ and $B = ([0,\frac12] \times \{0\}) \cup ([\frac12,1] \times \{1\})$ as a union of intervals from different $\mathbb R^1$ 's (Actually $B$ isn't smooth or even continuous, but $B$ is at least piecewise continuous). We have $C$ as the projection of $B$ onto $\mathbb R^1$ (assuming we treat $[0,1]$ as identical to $[0,1] \times \{0\}$ ), but the line integral of the scalar constant function $f:D \to \mathbb R$ , $f(D)=\{2\}$ , where $D$ is either $B$ or $C$ is surely different over $B$ (I think it's 1.5) from the one over $C$ (I think it's 2).","['integration', 'surface-integrals', 'multivariable-calculus', 'stokes-theorem', 'differential-geometry']"
3440354,How many terms are there in expansion of $(x_1 +x_2+\ldots +x_r)^n$,"How many terms are there in expansion of $(a)(x_1 +x_2+\ldots+ x_r)^n$ $(b)(1+x_1+x_2+\ldots+ x_{r-1})^n$ for $(a)$ , a term in its expansion will look like $$C\cdot x_{1}^{k_1}\cdot x_2^{k_2}\cdots x_{r}^{k_r}$$ where $k_1 + k_2 +\ldots +k_r = n\;\; \; \;| k_i \geq 0$ So number of terms will be equal to the solutions of above $=C(n+r-1,r-1)$ I think answer for (b) should be same as well?","['number-theory', 'combinatorics', 'elementary-number-theory', 'multinomial-theorem']"
3440423,Uniform Convergence Weierstrass' M-Test,"Could you correct my following proof for the uniform convergence of the Weierstrass' M-Test? At first, I'm going to write the preconditions: We have a sequence of complex functions $(a_n(z))_{n \in \mathbb{N}_0}$ with $|a_n(z)|\leq M_n$ for $z \in  K \subset \mathbb{C}$ and $\sum_{n=0}^\infty M_n < \infty$ . This implies that $L(z)= \sum_{n=0}^{\infty} a_n(z)$ is uniformly convergent (which means that for all $z \in K$ there is an $\epsilon >0$ and $m_0>0$ such that $|L(z)-\sum_{n=0}^{m} a_n(z)|<\epsilon$ , $m\geq m_0$ , $z \in K$ .) I have to show this following two steps: 1) The series is uniformly cauchy, i.e. there is an $\epsilon >0$ and $m_0$ such that $|\sum_{n=0}^{M_2} a_n(z) - \sum_{n=0}^{M_1} a_j(z)|<\epsilon$ , $M_2>M_1\geq m_0$ , $z \in K$ . 2) Given the existence of the Limit of the partial sums, show that we get the uniform convergence of the partial sums. Now I am going to present my ideas: For step 1):
I already proved that the series $\sum_{n=0}^{\infty} a_n(z)$ converges absolutely. And therefore, it converges.
My idea is: Let $s(z)$ be the limit of the series.
Then for all $\epsilon>0$ there is an $m_0 \in \mathbb{N}$ such that $|\sum_{n=0}^{M_1}a_n(z)-s(z)|<\frac{\epsilon}{2}$ and $|\sum_{n=0}^{M_2}a_n(z)-s(z)|<\frac{\epsilon}{2}$ for all $M_2,M_1\geq m_0$ .
And then it follows that $|\sum_{n=0}^{M_2} a_n(z) - \sum_{n=0}^{M_1} a_j(z)|=|\sum_{n=0}^{M_1}a_n(z)-s(z)+s(z)-\sum_{n=0}^{M_2}a_n(z)|\leq  |\sum_{n=0}^{M_1}a_n(z)-s(z)|+|\sum_{n=0}^{M_2}a_n(z)-s(z)|< \frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$ . Is this correct? For step 2):
By convergence of the series we know that for all $\epsilon>0$ there is an $m_0 \in \mathbb{N}$ such that $|\sum_{n=0}^{M_1}a_n(z)-s(z)|<\frac{\epsilon}{2}$ for all $M_1>m_0$ .
By step 1) we know that for all $\epsilon$ is an $m_0$ gibt such that $|\sum_{n=0}^{M_2} a_n(z) - \sum_{n=0}^{M_1} a_j(z)|<\epsilon$ for all $M_2>M_1\geq m_0$ .
Hence, we get: $|s(z)-\sum_{n=0}^{M_1}a_n(z)|\leq |s(z)-\sum_{n=0}^{M_2}a_n(z)|+|\sum_{n=0}^{M_2}a_n(z)-\sum_{n=0}^{M_1}a_n(z)|<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$ Is this correct? If it's not correct, how does the proof work? Thank you!!","['laurent-series', 'complex-analysis', 'uniform-convergence', 'sequences-and-series', 'convergence-divergence']"
3440455,Why does Lagranges equations of motion yield minimum?,"While calculus of variation from which Lagranges equations of motion arises states integral to be an extremum, it seems almost always to yield the minimum. Is it simply way things work or is there a mathematical reason to it? Also, is it possible for a solution to Euler's equation for a given system to yield expressions for both minimum and maximum? If not why is that?","['calculus', 'derivatives', 'euler-lagrange-equation', 'calculus-of-variations']"
3440498,Finding $\int_{0}^{\infty} \frac{e^{3x}-e^x}{x(e^{3x}+1)(e^x+1)} dx$,"I want to find the following integral: $$\int_{0}^{\infty} \frac{e^{3x}-e^x}{x(e^{3x}+1)(e^x+1)} dx$$ I have been trying to apply simple substitutions like $u=e^x$ , to simplify the integral without any luck.
I simplified it to $$\int_{0}^{\infty}\left[ \frac{1}{x(e^x+1)}-\frac{1}{x(e^{3x}+1)}\right]dx$$ which seems to have increased my problems. Wolfram Alpha gives the answer to be $0.5493\ldots$ and the graph looks like a Gaussian distribution. Somehow, I feel that this integral is very non-trivial due to the 'troublesome' $x$ in the denominator.","['integration', 'calculus', 'improper-integrals']"
3440521,"$SL(2,\mathbb R)$ and $SO(2,1)$ isomorphic - or not?","In this wikipedia article about $SU(1,1)$ , it is stated that This group [ $SU(1,1)$ ] is isomorphic to $SO(2,1)$ and $SL(2,\mathbb ℝ)^{[17]}$ I'm confused by the relation of $SO(2,1)$ and $SL(2,\mathbb R)$ . I know that they are locally isomorphic as Lie groups (as their Lie algebras are isomorphic). Topologically, they are different - $SL(2,\mathbb R)$ is connected (by row-echelon-reduction), where $SO(2,1)$ is not connected (there is the component fixing the upper hyperboloid and the component interchanging the hyperboloids). Therefore, they are not isomorphic as Lie groups. So, are $SO(2,1)$ and $SL(2,\mathbb R)$ isomorphic as abstract groups ? Why (not)? The source $^{[17]}$ (Gilmore's Lie Groups, Lie Algebras and some of their Applications , p.201-205) from wikipedia seems only to show an isomorphism between the Lie algebras. This question does not show that there is no isomorphism, but gives a 2-1-map, which is not surjective.","['matrices', 'group-theory', 'lie-groups']"
3440541,"Is this question valid? If $\angle A$ and $\angle B$ are complements, and the supplement of $\angle B$ is $78^\circ$, then find $\angle A$.",Question: Angle A and B are complementary. If the supplement of angle B is 78°. What is the measure of angle A? I am confused since A + B = 90 B + 78 = 180 B = 102 A + 102 = 90 A = -12 I think the question should be the supplement of angle B is 102°. Is the question right or wrong?,"['angle', 'geometry']"
3440623,When is a non-archimedean prime $v$ of a number field unramified in a Kummer extension?,"I have been working on the exercises of the book ""Algebraic number theory"" published by Frölich and Cassels. In one exercise, the following statement is given as a fact: Let $K$ be a number field and $m\geq 2$ . Let $v$ be a non-archimedean prime of $K$ which does not divide $m$ , and let $a\in K^*$ . Then the extension $K(a^{\frac{1}{m}})/K$ is unramified at $v$ if and only if $v(a)$ is divisible by $m$ . I have trouble writing down a proof for this statement, even after reviewing the chapter on Kummer theory inside the same book. In this chapter, it is proven that the discriminant of the extension $K(a^{\frac{1}{m}})/K$ divides $m^ma^{m-1}$ , and hence a prime $v$ is unramified in particular if it does not divide $ma$ (ie in our case if $v(a)=0$ , given that $v(m)$ is already $0$ ), which makes sense to me. 
However I fail to see how to prove/justify the more general statement above. Could someone give me a hint or a reference for this ?","['number-theory', 'algebraic-number-theory', 'ramification', 'kummer-theory']"
3440647,Why does this process map every fraction to the golden ratio?,"Start with any positive fraction $\frac{a}{b}$ .
First add the denominator to the numerator: $$\frac{a}{b} \rightarrow \frac{a+b}{b}$$ Then add the (new) numerator to the denominator: $$\frac{a+b}{b} \rightarrow \frac{a+b}{a+2b}$$ So $\frac{2}{5} \rightarrow \frac{7}{5} \rightarrow \frac{7}{12}$ . Repeating this process appears to map every fraction to $\phi$ and $\frac{1}{\phi}$ : $$
\begin{array}{ccccccccccc}
 \frac{2}{5} & \frac{7}{5} & \frac{7}{12} & \frac{19}{12} & \frac{19}{31} & \frac{50}{31} & \frac{50}{81} & \frac{131}{81} & \frac{131}{212} & \frac{343}{212} & \frac{343}{555} \\
 0.4 & 1.40 & 0.583 & 1.58 & 0.613 & 1.61 & 0.617 & 1.62 & 0.618 & 1.62 & 0.618 \\
\end{array}
$$ Another example: $$
\begin{array}{ccccccccccc}
 \frac{11}{7} & \frac{18}{7} & \frac{18}{25} & \frac{43}{25} & \frac{43}{68} & \frac{111}{68} & \frac{111}{179} & \frac{290}{179} & \frac{290}{469} & \frac{759}{469} & \frac{759}{1228} \\
 1.57143 & 2.57 & 0.720 & 1.72 & 0.632 & 1.63 & 0.620 & 1.62 & 0.618 & 1.62 & 0.618 \\
\end{array}
$$ Q . Why?","['golden-ratio', 'sequences-and-series']"
3440660,"""Indexed"" version of compactness and Axiom of Choice","When thinking about some problems related to compactness, I thought of this notion which, at the first glance, seems similar to the usual definition of compactness. A topological space is compact iff every open cover has a finite subcover. In the other words, if $\{U_i; i\in I\}$ is an open cover of $X$ then we have $U_{i_1},\dots, U_{i_n}$ such that $U_{i_1}\cup\dots\cup U_{i_n}=X$ . However, one should be a bit careful with the notation like this. An important thing to notice is that if the open cover is indexed by a set $I$ , the index $i\in I$ needn't be uniquely determined by $U$ . I.e., it is possible that $U_i=U_j$ for some $i\ne j$ . Indexed compactness. Let $(X,\mathcal T)$ be a topological space and $f\colon I\to\mathcal T$ be a function such that $f[I]=\{f(i);i\in I\}$ is an open cover of $X$ . Then there is a finite set $F\subseteq I$ such that $f[F]$ is a cover of $X$ . The difference from the usual definition of compactness is that here when working with the finite subcover $\{U_{i_1},\dots,U_{i_n}\}$ we also have the indexes $i_1,\dots,i_n$ . In the other words, if we have an open cover $\{U_i; i\in I\}$ (or a subcover $\{U_i; i\in F\}$ , for an open set $U$ there might be several $i$ 's such that $U_i=U$ . This version of compactness selects one of them. Of course, if we are working in ZFC, then using Axiom of Choice we can choose $i\in I$ for each open set in the cover (subcover). So in ZFC this is equivalent to the usual notion of compactness. However, when we are are not allowed to use AC, my guess would be that this is no longer equivalent to the usual notion of compactness. (Or at least I do not see a straightforward way to prove the equivalence in ZF.) Question. Is the ""indexed-compactness"" defined above equivalent to the usual definition of compactness (in ZF)? Was this version of compactness studied somewhere? Does equivalence between indexed-compactness and compactness imply AC? I am aware that various versions of compactness are studied under ZF. Herrlich 's book Axiom of Choice (Lecture Notes in Mathematics 1876) mentions filter-compact, ultrafilter-compact and Alexandroff-Urysohn-compact spaces. I haven't seen there something which would be equivalent to the above. (At least for none of the types of compactness defined there the relationship to ""indexed-compactness"" is immediately clear.)","['axiom-of-choice', 'reference-request', 'general-topology', 'set-theory', 'compactness']"
3440734,Functions near any Lipschitz,"Let $\operatorname{Lip}\subseteq C([0,1]^d,\mathbb{R}^d)$ be the set of all Lipschitz functions from $[0,1]^d$ to $\mathbb{R}^d$ . Which non-affine functions $f \in C(\mathbb{R}^d,\mathbb{R}^d)$ satisfy: There exists some $\epsilon\geq \delta\triangleq K\epsilon>0$ such that, for every ${g} \in Lip$ and every $g_1,g_2 \in Ball_{\epsilon}({g})$ $$
\inf_{
         \underset{B,A\in Mat_{d\times d}(\mathbb{R})}{a,b \in \mathbb{R}^d}
}
\sup_{x \in [0,1]^d} \big\|(a + A\, f (B\, g_1(x)+b)) - g_2(x)\big\|_2 < \delta.
$$ Intuitively, I would expect that it's necessary for $f$ to be Lipschitz, maybe contractive. Note: $Ball_{\epsilon}(g)\triangleq \left\{h \in C([0,1]^d;\mathbb{R}^d):\, 
\sup_{x \in [0,1]^d} \big\|h(x)- g(x)\big\|_2<\epsilon
\right\}$ (for $\epsilon >0$ and $g \in C([0,1]^d;\mathbb{R}^d)$ ).","['real-analysis', 'functional-analysis', 'optimization', 'inequality', 'supremum-and-infimum']"
3440789,How to find $I=\int_0^1\frac{\arctan^2x}{1+x}\left(\frac{\ln x}{1-x}+\ln(1+x)\right)dx$,"$$I=\int_0^1\frac{\arctan^2x}{1+x}\left(\frac{\ln x}{1-x}+\ln(1+x)\right)dx=-\frac{\pi^4}{512}+\frac{3\pi^2}{128}\ln^22+\frac{\pi}{8}G\ln2-\frac{21}{64}\zeta(3)\ln2$$ This integral was proposed to me by a friend, but without the solution.
 I tried the integration by parts, but too complex.
 I tried to find a closed form but without result. $$I=\int_0^1\frac{\arctan^2x}{1±x}\ln{x}dx$$","['integration', 'definite-integrals', 'harmonic-numbers', 'polylogarithm', 'riemann-zeta']"
3440799,"Lagrangian submanifolds, symplectomorphism","I want to show the following: Let $L_0,L_1 \subset \mathbb{R}^{2n}$ be Lagrangian submanifolds (standard symplectic structure on $\mathbb{R}^{2n}$ . Let $p=(x,y) \in \mathbb{R}^{2n}$ , s.t. $p \in L_0 \cap L_1$ and $T_pL_0 \cap T_p L_1= \{0\}$ . 
Then there exists a symplectomorphism $\varphi: \mathbb{R}^{2n} \rightarrow \mathbb{R}^{2n}$ such that $\varphi(L_0 \cup L_1)\cap B_{\epsilon}(p)= (\mathbb{R}^{n} \times\{y\} \cup \{x\}\times \mathbb{R}^{n}) \cap B_{\epsilon}(p)$ for sufficient small $\epsilon$ . My idea is the following: There are neighborhoods $U_0$ of $p$ in $L_0$ and $U_1$ of $p$ in $L_1$ , and charts $\phi:U_0 \rightarrow B_{\epsilon}(x) \times \{y\}$ for $L_0$ and $\psi:U_1 \rightarrow \{x\} \times B_{\epsilon}(y)$ for $L_1$ (also $U_0 \cap U_1=\{p\}$ ). (Such neighborhoods exist, because $T_pL_0 \cap T_p L_1= \{0\}$ ) W.l.o.g. $\phi(p)=p, \psi(p)=p$ . Then set $\varphi(q)=\begin{cases} \phi(q) & q \in U_0 \\
\psi(q)& q \in U_1
\end{cases}$ Now I'm not quite sure about my statements here and also I don't show how to extend this $\varphi$ and also how to show that it is a symplectomorphism. Does somebody have any hints for me?","['submanifold', 'symplectic-geometry', 'tangent-spaces', 'tangent-bundle', 'differential-geometry']"
3440838,Extending surface independence of Stokes' theorem in elementary calculus to the Stokes' theorem in differential geometry,"Okay so I discovered, here When is a surface integral equal to double integral over projection? A verification of Stokes' Theorem. Intuition and relation to Green's Theorem. (and also here Applying Stokes' theorem - what surface? ), a view of Stokes' Theorem, at least in elementary calculus, as not only Given a surface $\Sigma$ , let $C$ be its boundary curve. If (insert assumptions), then $\int_C = \int \int_{\Sigma}$ . but also Given a curve $C$ , we have for any surface $\Sigma$ with $C$ as its boundary curve that if (insert assumptions), then $\int_C = \int \int_{\Sigma}$ . Thus, Given a surface $\Sigma$ , if (insert assumptions), then $\int \int_{\Sigma} = \int \int_{\Sigma'}$ for any $\Sigma'$ that has the same boundary curve as $\Sigma$ as long as (insert assumptions). Question : How can we extend this view to Stokes' theorem in differential geometry? If I have 2 distinct smooth oriented $n$ -manifolds with boundary $M$ and $N$ that actually turn out have the same manifold boundary $\partial M=\partial N$ , then, under assumptions (insert here), I want to use Stokes' Theorem to say something like $$\int_M d \omega=\int_{\partial M} \omega = \int_{\partial N} \omega=\int_N d \omega \tag{A}$$ Not sure what $\omega$ would be though. It's has to be a smooth differential $(n−1)$ -form with compact support on both $N$ and $M$ . Also, if we're going to be strict about it, then $(A)$ should look more like $$\int_M d \omega=\int_{\partial M} \iota_M^{*}\omega = \int_{\partial N} \iota_N^{*}\omega=\int_N d \omega \tag{B}$$ where $(\cdot)^{*}$ denotes pullback and $\iota_M: \partial M \to M$ and $\iota_N: \partial N \to N$ are inclusion maps in which case I'm not sure we can have the same ' $\omega$ ' since we can't exactly have $\omega: M \to \wedge(T^{*}M)$ to be also $\omega: N \to \wedge(T^{*}N)$ . Some examples that might make sense out of $\omega$ are when $M$ is a submanifold with boundary of $N$ (in which case I guess $M$ is open in $N$ extending from the case for submanifolds without boundary of codimension zero) or when they have a submanifold with boundary in common or something. In the former example (assuming it works), we could have the ' $\omega$ ' on $N$ to be just $\omega$ and then the ' $\omega$ ' on $M$ is $\omega|_{M}$ , in which case I hope that $\omega(M) = \omega|_{M}(M)$ , a subset of $\wedge(T^{*}N)$ , is a vector subbundle that is bundle isomorphic to the cotangent bundle $\wedge(T^{*}M)$ . I think the latter example can work similarly.","['multivariable-calculus', 'stokes-theorem', 'differential-geometry']"
3440880,"There exists a prime $p$ such that $p \mid n$ for all $n \in\mathbb N$, $n > 1$","My textbook asks these following true or false questions but provides two different answers even though, in my opinion, the questions are asking the exact same thing. Could someone explain how the questions are different? True or False? (a) For all $n \in\mathbb N$ , $n > 1$ , there exists a prime $p$ such that $p \mid n$ . (b) There exists a prime $p$ such that $p \mid n$ for all $n \in\mathbb N$ , $n > 1$ . Part (a) can be proven using the Lemma, but part (b) is apparently false because the prime can't be $2$ , since $2 \nmid 3$ , and can't be odd since if $p$ is an odd prime, $p \nmid 2$ . But why is that the case for part (b) and not part (a)? And why is it trying to do $2 \nmid 3$ , instead of $2 \mid4$ or $2 \mid 6$ ?",['discrete-mathematics']
3440889,Circumcentral midpoints,"Let $ABC$ be a triangle with $AB = AC$ . Let $M$ be the midpoint of $BC$ . Let the circles with diameters $AC$ and $BM$ intersect at points $M$ and $P$ . Let $MP$ intersect $AB$ at $Q$ . Let $R$ be a point on $AP$ such that $QR \parallel BP$ . Prove that $CP$ bisects $\angle RCB$ . Attempt: Let $AP \cap BC=E$ and Let $D$ be orthocenter WRT $\Delta AEC$ $\implies$ $PEMD$ is cyclic. Also, $\angle EPM$ $=$ $\angle ACB$ $=$ $\angle QBE$ $\implies$ $QBEP$ & $AQPD$ are cyclic.","['euclidean-geometry', 'geometry']"
3440893,When are semidirect products isomorphic?,"I have proved this proposition, which has been stated on other StackExchange pages. Proposition: Let $N$ and $H$ be groups and let $\phi_1,\phi_2: H\to\newcommand{\Aut}{\operatorname{Aut}} \Aut(N)$ be group homomorphisms s.t.  for some $g\in \Aut(N)$ , $\phi_2(h)=g\phi_1(h)g^{-1}$ for all $h\in H$ . Then $N \rtimes_{\phi_1} H \cong N\rtimes_{\phi_2} H$ . The map \begin{align*}
		f: N \rtimes_{\phi_1} H &\to N\rtimes_{\phi_2} H, 
		(n,h)\mapsto (g(n),h).
	\end{align*} is an isomorphism. Proof: This map is clearly well-defined and bijective since $g$ is a group automorphism. So it suffices to check the group homomorphism properties: $f((n,h)(n',h'))=f((n\phi_1(h)(n'),hh'))= (g(n\phi_1(h)(n')),hh')= (g(n) g\phi_1(h)(n'),hh')\\
			=(g(n) g \phi_1(h)g^{-1}g(n'),hh')\\
			=(g(n) \phi_2(h) g(n'),hh')\\
			=(g(n),h)(g(n'),h')\\
			= f(n,h)f(n',h').$ $	f((n,h)^{-1})=f((\phi_1(h^{-1})(n^{-1})),h^{-1})=(g\phi_1(h^{-1})(n^{-1}),h^{-1})\\
			=((g\phi_1(h^{-1})g^{-1})(g(n^{-1})),h^{-1})\\
			=(\phi_2(h^{-1})g(n^{-1}),h^{-1})\\
			=(g(n),h)^{-1}\\
			=f((n,h))^{-1}.$ Interestingly, when I do my homework, classifying groups of order 18,70,75, the converse is true. For example, group of order 70. ""By Sylow Theorem, $n_5=n_7=1$ . Then $N=N_5N_7 \cong Z/35Z$ is a normal subgroup since it is a product of two normal ones. Let $H$ be the $2-$ Sylow subgroup, then $H=Z/2Z$ . So we need to determine all the maps $\phi:Z/2Z \to \Aut(N) =\Aut(Z/5Z \times Z/7Z) \cong Z/4Z \times Z/6Z$ . So we have the possibilities $\phi(1)=(0,0),(0,3),(2,0),(2,3)$ . So there arev at most 4 groups of order $70$ ."" And according to the wiki, there are 4 groups of order 70, so the result follows. My questions: 1) Is the converse true? Is there a counterexample?
2) Is my proof correct? THis is a ridiculously strong proposition, but I think my proof is not wrong.","['semidirect-product', 'group-theory', 'abstract-algebra', 'abelian-groups']"
3441001,Uniform stability of equilibrium,"I have been given the following definition for uniform stability: the equilibrium state $x_e$ is uniformly stable, if for any $\epsilon > 0$ there is a $\delta > 0$ such that $$\|x(0)-x_e\|<\delta~~\Rightarrow ~~\|x(t)-x_e\|<\epsilon, ~~\forall t\geq 0$$ In my opinion this definition does not have anything to do with stability. Imagine a system with $x(t)$ going to infinity and $x(t) \geq 10^{100} ~~\forall t \geq 0$ and $x_e = 0$ . Then the system would be uniformly stable following the above definition, as for any $\epsilon$ I can use $\delta = 10^{100}$ and that would fulfill the implication. The reason is that $x(0) - x_e$ is never smaller than $10^{100}$ so the left side of the implication is always false and therefore the implication always true. What am I missing here?","['ordinary-differential-equations', 'lyapunov-functions', 'neural-networks', 'stability-in-odes', 'dynamical-systems']"
3441032,Composition of almost-everywhere differentiable functions $\mathbb{R} \rightarrow \mathbb{R}$,"Suppose $g, f : \mathbb{R} \to \mathbb{R}$ are almost-everywhere differentiable. Is $g \circ f$ almost-everywhere differentiable? ( This question deals with a-e. continuity, but the answer does not apply. This question deals with a-e. differentiability but in higher dimensions; the answers do not apply either.)","['measure-theory', 'real-analysis']"
3441093,"If $|f(x)|\leq1$ and $|f''(x)|\leq1$, then $|f'(x)|\leq2$.","Let $f:\mathbb{R}\to\mathbb{R}$ be twice differentiable, and supose that for all $x\in\mathbb{R},\,|f(x)|\leq1$ and $|f''(x)|\leq1$ . Prove that $|f'(x)|\leq2$ for all $x\in\mathbb{R}.$ I tried working with Taylor polynomials but had no successful idea. 
This question is from the book Berkley Problems in Mathematics .","['derivatives', 'real-analysis']"
3441136,Hilbert's 90 Theorem proof clarification (Milne's Étale Cohomology),"So I'm reading Milne's Étale Cohomology and stuck in trouble to understand the proof of Proposition 4.9 Hilbert's Theorem 90), from §4; page 124: The canonical maps $$ H^1(X_{Zar},  \mathcal{O}_X^*) \to H^1(X_{et},  \mathbb{G}_m)  \to H^1(X_{fl},  \mathbb{G}_m)$$ are isomorphisms. $X_x$ with $x = Zar,et, fl$ denotes the ""topology"" of $X$ . the proof only shows $ H^1(X_{Zar},  \mathcal{O}_X^*) \to H^1(X_{fl})$ . according to the proof by Theroem 1.18 it suffice to show that $R^1f_*\mathbb{G}_m=0$ . for this, let $U$ be a Zariski open subset of $X$ and we must show that $H^1(U_{fl},  \mathbb{G}_m)$ becomes zero. if we consider a Zariski open affine cover of $U$ by a family $U_i$ , we can assume that $U$ is affine, since we can the $U_i$ choose to be affine, i.e. $U=spec(A)$ . this reduction is clearly can be done with sheaf axiom in mind. The step that confuses me that futhermore is told that is can be also be assumed that $A$ is a local ring. why this assumption can be done here?","['etale-cohomology', 'algebraic-geometry', 'sheaf-cohomology']"
3441151,Subset and bijection implies cardinal equality?,"I hope I have not hereby created a duplicate, please perdon me if I did, but I had this question for a while now: Let $A \& B $ be two sets such that $A   \subseteq B$ . Suppose there exist a one to one (bijective) function $f : A \to B $ . Then have we got $|A|   = |B|$ ? I know that if these sets are finite, it works, but what about the infinite case Thank you T. D","['elementary-set-theory', 'cardinals']"
3441213,How to show the difference of max and min of exponential random variables is exponential?,"Let $X \sim \exp(\lambda_1)$ and $Y \sim \exp(\lambda_2)$ be two exponential random variables. Let $M= \max(X,Y)$ and $L= \min(X, Y)$ . We know that $M -L = |X-Y|$ . How to show $M -L$ is distributed exponentially? My try: $$
P(M-L \leq t) = P( |X-Y| \leq t)=\int_{-\infty}^\infty \int_{x = y- t}^{x= y+t} \lambda_1 e^{-\lambda_1x}\lambda_2e^{-\lambda_2y} \, dx \, dy
$$ First I do not know if this is the double integral that leads to the solution. Second, when I try to solve it does not a convergent integral. Can you help me on that?","['statistics', 'exponential-distribution']"
3441225,Sum of alternating series is non-zero?,"Let $S=1-1/3+1/5-1/7+\cdots$ . As each term in the series is decreasing and tends to $0$ , it is known that their sum exists and is finite by alternating series test. And by considering $\int_0^11/(1+x^2)dx$ , it is known that $S=\pi/4$ . I am wondering is there a fast way to see that $S\neq 0$ ? In general, is it true that for $T=\sum_{n=1}^\infty (-1)^n a_n$ , where $a_n>0$ are strictly decreasing and tend to $0$ , $T\neq 0$ ?","['analysis', 'sequences-and-series']"
3441262,How does one find the axis of rotation for a pure rotation matrix when said matrix is also symmetric?,"I'm a programmer working on an advanced C++ 3D mathematics library. Now, things have been going well, in fact, basically everything about the library has been fully implemented, but one final bit of code still alludes me: finding the axis of rotation for a pure rotation matrix when said matrix is also symmetric. I've got a nice bit of math going when it comes to non-symmetric matrices Given a non-symmetric 3x3 pure rotation matrix [M]

M = { { a, b, c },
      { d, e, f },
      { g, h, i } }

det(M) = 1

M * T(M) = T(M) * M = I

M =/= T(M)

an eigenvector [u] which sits along the axis of rotation can be found 

u = { h - f,
      c - g,
      d - b }

such that its normal is axis of rotation [r] of the matrix

r = u / |u| but this math breaks the moment you give it a symmetric matrix, as the 'h - f', 'c - g', and 'd - b' portions will all resolve to zero, which obviously isn't the normal vector I want. Now, I do understand linear algebra, but only a little bit. I've been researching this problem for a few days now, and while there are resources that talk about this, most of them either don't address the problem I'm having, or explain it in a way that my scrub brain simply can't keep up with. They tell me to do things like 'diagonalize M and solve for u', but I've not a clue what that actually entails doing, let alone in a generalized way, let alone (even more) teaching my C++ library to do it in a generalized way given any symmetric pure rotation matrix. So ya, that's my plight. Hoping that one of y'all could help bail me out on this one and show me how to solve this problem. :D Btw, again, this is needed for writing code, so if your answer could be written in a way that addresses that need and also the fact that I'm a linear algebra noobie, that would be awesome. Thanks in advance!","['matrices', 'linear-algebra', 'symmetric-matrices', 'eigenvalues-eigenvectors']"
3441280,Finding a generating function for a formal power series containing $n$ and $a_n$,"Given the sequence $(a_n)_{n\geq0}$ , the elements of which are recursively defined as follows: $$a_0 := 0; a_{n+1} := n \cdot a_n + 1$$ We can define a formal power series $A(z) = \sum_{n\geq0}a_nz^n$ and after a few simplification steps we obtain: $$A(z) = a_0 + z \cdot \sum_{n\geq0}n \cdot a_n \cdot z^n + z \cdot A(z)$$ My goal is to obtain a generating function by eliminating the power series but I am not sure how to proceed. None of the ""simple"" power series known to me fit the pattern of the power series above. Is there some Identity I can use or is there another trick? Thanks for the help! :)","['formal-power-series', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
3441309,Why can't we integrate functions on a manifold?,"Most books about manifolds say that ""there is no way to integrate real-valued functions in a coordinate-independent way on a manifold"". I've read the usual reasons but they don't seem to differ from the usual theory of integration in $\mathbb{R}^n$ . I explain below what I mean. Let $M$ be an orientable smooth $n$ -manifold. Since it is orientable, we have a nonvanishing $n$ -form $\omega$ . This allows us to define the following linear functional: \begin{align*}
\Lambda:C_c^{\infty}(M)&\to \mathbb{R}\\
f &\mapsto \int_M f\omega.
\end{align*} By continuity, we can extend its domain to obtain a positive linear functional $C_c(M)\to \mathbb{R}$ . Then, the Riesz-Markov-Kakutani representation theorem implies the existence of a regular Borel measure $\mu$ such that $$\Lambda(f)=\int_M f \:\mathrm{d}\mu,$$ for all $f\in C_c^\infty(M)$ . Why isn't this a good notion of integral of functions on a manifold? It doesn't seem to depend on a choice of chart and while it depends of a choice of $\omega$ , I would argue that the same thing happens in $\mathbb{R}^n$ since the Lebesgue measure depends of a normalization factor.","['integration', 'manifolds', 'measure-theory']"
3441322,Proof that $\Phi(G) \le N$ implies $\Phi(G) \le \Phi(N)$ for normal subgroup $N \unlhd G$,"Let $G$ be some finite group and $p$ some prime divisor of the order of $G$ . Suppose $O_p(G) = 1$ , i.e. the largest normal subgroup with order $p$ is trivial, and $G / N$ is a $p$ -group for some normal subgroup $N$ . Then $\Phi(N) = \Phi(G)$ , i.e. the Frattini subgroups of $N$ and $G$ are equal. This is an exercise from The Theory of Finite Groups by Kurzweil and Stellmacher. By an application of the Frattini argument we find that every Sylow subgroup of $\Phi(G)$ is normal in $G$ , hence as $O_p(G) = 1$ we know that $p$ does not divide the order of $\Phi(G)$ . Also as $G/N$ is a $p$ -group we can deduce $\Phi(G) \le N$ . Now maybe it is true that if the Frattini subgroup of the whole group is contained in some normal subgroup $N$ , then $\Phi(G) \le \Phi(N)$ might hold. I tried to show that $\Phi(G) \le M$ for every maximal subgroup $M$ of $N$ , for if not we have some $x \in \Phi(G)$ such that $\langle x, M \rangle = N$ . But that is all I can deduce, I do not see how to get a contradiction from that... The other inclusion is a previous exercise, which I solved. For if $N \unlhd G$ , then $\Phi(N) \unlhd G$ . Let $M \le G$ be some maximal subgroup, if $\Phi(N)$ is not contained in $M$ then $M\Phi(N) = G$ , hence $N = N \cap M\Phi(N) = (N\cap M)\Phi(N)$ by Dedekinds modular law, which implies $N = N\cap M$ , or $N \le M$ , which gives $\Phi(N) \le M$ contrary to our assumption. Hence $\Phi(N) \le M$ for every maximal subgroup $M$ of $G$ . So any suggestions how to show the implication $\Phi(G) \le \Phi(N)$ ?","['group-theory', 'abstract-algebra', 'finite-groups']"
3441418,"Is there an isomorphism $\text{Hom}_R(R \otimes_k V, R \otimes_k W) \cong R \otimes_k W \otimes_k V^*$?","Suppose that $R = k[x_1, \dots, x_n]$ , $G$ is a finite group acting on $R$ , and $V$ and $W$ are finite dimensional $G$ -modules (perhaps one-dimensional?). Is there an isomorphism (of $kG$ -modules?) $$
\text{Hom}_R(R \otimes_k V, R \otimes_k W) \cong R \otimes_k W \otimes_k V^*?
$$ Here I'm viewing $R \otimes_k V$ and $R \otimes_k W$ as left $R$ -modules in the usual ring-theoretic way. Whenever $kG$ acts on a tensor product, the action splits over the tensor. Unless I'm getting confused, $\text{Hom}_R(R \otimes_k V, R \otimes_k W)$ is a left $kG$ -module via $$
(g \cdot \phi)(r \otimes v) = g \cdot \phi(g^{-1} \cdot(r \otimes v)) = g \cdot \phi((g^{-1} \cdot r) \otimes (g^{-1} \cdot v)).
$$ I tried using hom-tensor adjunction, but it's not clear to me if that holds here (tensoring $kG$ -modules seems slightly different to tensoring modules over rings).","['ring-theory', 'group-theory', 'abstract-algebra', 'representation-theory']"
3441447,Summations from negative to positive infinity,"What does the series $\sum_{k=-\infty}^\infty a_k$ mean as appose to $\sum_{k=0}^\infty a_k$ Where $a_n$ is a complex sequence. And how can one take the product of two such series (in a way similar to the cauchy product), for example the product of two
complex valued fourier series.","['complex-analysis', 'fourier-series']"
3441480,Is there an exact value of $\cos^{-1}(4/5)?$,"This is about the $3,4,5$ Pythagorean triangle. Question in the title. I think the answer is ""no"", but if not, then why not? What about the $5, 12, 13$ triangle, or even non-Pythagorean triangles like $1, 4,  \sqrt {17} $ ? Can you write the angles in these triangles in a ""nice way""?",['trigonometry']
3441513,How to prove that the Möbius band has geodesics?,"In my class of Differential Geometry, the teacher defined geodesics as follows: A regular curve on a regular surface, denoted as $\gamma:I\subset\Bbb{R}\to S$ , ( $S$ is the surface) is a geodesic if, $\forall t\in I$ , the vector $\gamma""(t)$ is a normal vector to $S$ at the point $\gamma(t)$ . With this definition, I must prove for an exposition project that the möbius band can have geodesics. The problem is that saying that a vector is normal to a surface implies orientation, and the möbius band is un-orientable. So this is my question: How can i define geodesics on an un-orientable surface like the möbius band, and with that, how do i calculate them? Can't the möbius band have any geodesics at all, because of its un-orientability? If you can provide me a reference, i would apreciate it.","['mobius-band', 'differential-geometry']"
3441586,Hessian squared and Laplacian on a Riemannian manifold,"I have been trying to understand more about the norm squared of the Hessian on a Riemannian manifold, that is $$
|\nabla^2f|^2
$$ This quantity shows up in the Bochner formula, for instance. On $\mathbb{R}^n$ , the induced fiber metric on the 2-tensor bundle is just a dot product of matrices (viewed as $\mathbb{R}^{n\times n}$ in this case), so viewing the contribution from the diagonal terms, we get $$
\sum_{i=1}^n\frac{\partial^2 f}{\partial x_i^2}
$$ These can be used to estimate the square of the Laplacian via the Cauchy-Schwartz inequality. I'm wondering how to show that we have an analogous property in the more general case, that is, showing that the norm of the Hessian squared is equal to a sum of terms that resemble the Laplacian in coordinates squared plus some other non-negative terms. So far, I have tried the following: take a symmetric two-tensor $T$ on a manifold $(M^n,g)$ ; it induces a self-adjoint operator $A$ satisfying $T(v,w)=\langle A(v),w\rangle$ . Fix a point $p\in M$ and choose a local frame about $p$ , call it $E_j$ , which diagonalizes $A$ . Using the canonical association between endomorphisms and $(1,1)$ tensors, we can write $$
A=\lambda_i \epsilon^i\otimes E_i
$$ where there is a sum in $i$ and $\epsilon^i$ is the coframe dual to $E_i$ . Then $$
\langle A,A\rangle =\sum_{i,j}\lambda_i\lambda_j\langle\epsilon^i,\epsilon^j\rangle\langle E_i,E_j\rangle
$$ Here, I want to claim that the last quantity is a sum of squares of eigenvalues plus a non-negative error term, but I think I am missing some relevant identity from linear algebra. The sum of squares of eigenvalues would then specialize in our Hessian case to be related to the Laplacian terms (I think). I'm interested in advice either for how to make this approach work, or for a different approach to thinking about this that is simpler.","['riemannian-geometry', 'tensors', 'multilinear-algebra', 'hessian-matrix', 'differential-geometry']"
3441600,Are these two functions describing spring motion in the same way?,"Suppose I have a differential equation in the form $m\ddot{x} + kx = 0$ then the solution is in the form $x(t)=C_1cos\left(\sqrt{\dfrac{k}{m}}t\right) + C_2sin\left(\sqrt{\dfrac{k}{m}}t\right)$ Prior to learning differential equations, when  I did physics I learned that the equation of motion of the spring was $x(t) = Acos\left(\omega t + \phi\right)$ . Both of these functions seem to be able to handle the same set of functions. The first equation seems to use the fact that it has an additional term to shift the function and avoid the need of $\phi$ . Can it be shown that they are equivalent solutions or do they differ in some way?",['ordinary-differential-equations']
3441626,Algebraic degree of $\cos\left(\frac{p\pi}{q}\right)$,"How can we find the algebraic degree of $\cos\left(\frac{p\pi}{q}\right)$ for $p$ , $q$ coprime integers? I know that the algebraic degree of $e^{\frac{p\pi i}{q}}$ is $\phi(q)$ , since cyclotomic polynomials are irreducible. I also know that $$\cos(x)=\frac{e^{ix}+e^{-ix}}{2}.$$ But my knowledge of abstract algebra is quite slim, and I don't know how to relate these two facts. I've checked on Google, and I haven't been able to find anything related. Any result or reference would be appreciated.","['trigonometry', 'algebraic-numbers']"
3441685,Question on Krull domains.,"I'm stuck on a detail in Luther Claborn's paper Every Abelian Group is a Class Group . Recall that we say an integral domain $A$ is a Krull domain if \begin{align*}
&\text{(1) }A = \bigcap_{P \in S} A_P, \text{ where $S$ is the set of minimal primes of $A$.}
\\
&\text{(2) } A_P \text{ is a DVR for all $P \in S$.}
\\
&\text{(3) } \text{For all $a \in A \setminus \{0\}$, $a$ is in only finitely many $P \in S$.}
\end{align*} We also say that $\{A_P \,:\, P \in S\}$ is the set of essential valuation rings. In the proof of Proposition 6, he defines a Krull domain \begin{align*}
B = F[X_1,Y_1,Z_1, \dots, X_i, Y_i, Z_i, \dots ]
\end{align*} ( $F$ is a field and $i \in J$ , where $J$ is an arbitrary index set), and \begin{align*}
R_i = F[\dots,X_i,Y_i,W_i,\dots]
\end{align*} where $W_i = X_iZ_i$ .  Form the ideals $(X_i,Y_i)$ in $R_i$ , from which we get some valuations $v_i(r) = t$ if $r \in Q_i^t$ and $r \notin Q_i^{t+1}$ .  These can be extended to the quotient field of $R_i$ which is the same as the quotient field for $B$ , and we denote $V_i$ to be the corresponding valuation rings.  Now, $B$ is a UFD, so $B = \bigcap_P B_P$ , where $P$ ranges over all minimal primes of $B$ .  The goal is to show that $A = \left(\bigcap_i V_i \right)\cap B$ is a Krull domain with essential valuation rings $\Sigma = \{V_i\} \cup \{B_P\}$ . I can see that $A = \bigcap_i V_i \cap \bigcap_P B_P$ and that the elements of $A$ can only appear in finitely many of the minimal primes corresponding to the rings in $\Sigma$ . I don't see why $\Sigma$ is the set of essential valuation rings. In the paper, Claborn claims: ""To see this, we need only produce an element of the quotient field which is not in a particular ring of $\Sigma$ but is in all other rings of $\Sigma$ ."" In my mind, we need to check that if $P_0$ is a minimal prime such that $A_{P_0} \notin \Sigma$ , then somehow the claim above should contradict the minimality of $P_0$ .  Is this right?  Also, is showing the claim for a given set of valuation rings enough to show that set is essential in any Krull domain, or is it just true in this specific example? Thanks in advance for any help!","['number-theory', 'ring-theory', 'ideal-class-group']"
3441692,"Prove X is banach iff $S(0,1)$ is complete. [duplicate]","This question already has an answer here : Banach spaces and their unit sphere (1 answer) Closed 4 years ago . Let $X$ a normed space. Let $S(0,1)=\{x\in X: ||x||=1\}$ Prove that $X$ is banach iff $S(0,1)$ is complete. My attempt: $(\implies)$ Note $S(0,1)\subset X$ and we have $X$ is banach space. Let $\{x_n\}$ a cauchy sequence in $S(0,1)$ . We need prove that $\{x_n\}$ converge. As $\{x_n\}$ is cauchy then exists $N$ such that if $n,m>N$ then $||x_n-x_m||<\epsilon$ Here i'm stuck. $(<-)$ Let $\{x_n\}$ a cauchy sequence in $X$ . Note for $n,m>N$ we have $||x_n-x_m||<\epsilon$ .","['banach-spaces', 'functional-analysis']"
3441704,Equational theory of sedenions and beyond,"Consider a Cayley-Dickson algebra $(X,+,*,0,1)$ , that is an algebra generated from the reals by the Cayley-Dickson construction. From complexes to quaternions, we lose commutativity of multiplication, from quaternions to octonions, we lose associativity of multiplication, and from octonions to sedenions we lose alternativity of multiplication. I conjecture that, in a sense, the sedenions are the final stop. More precisely, for any Cayley-Dickson algebra $X$ that is sedenion or beyond, is the equational theory in the signature $(+,*,0,1)$ for $X$ the same as the equational theory of the sedenions?",['abstract-algebra']
3441706,Is the range of the Radon-Nikodym derivative always measurable?,"I am a student majoring applied mathematics. While writing my thesis paper, I found out that I need to prove or disprove the following statement: Suppose that $P$ and $Q$ are probability measures on the same measurable space, and $Q$ is absolutely continuous with respect to $P$ . Then, the range of $dQ/dP$ is a measurable subset of $\mathbb{R}$ . I made several attempts to prove the statement or find a counterexample, but I am having difficulties. Is there anyone who knows an answer to this question? Thank you.","['measure-theory', 'probability-theory', 'radon-nikodym']"
3441725,What's the cleanest way to show that the only polynomials that map powers of $10$ to powers of $10$ are those of the form $p(x) = 10^mx^n$?,"My current idea is to say that we know that $p(x)$ takes the following form: $$p(x) = a_nx^n + \dots + a_0$$ Let $x = 10^r$ . As $x$ tends to infinity, the first term will dominate the rest. If $a_n$ is not a power of $10$ , then eventually the first term will be so far off a power of $10$ that it is impossible for any of the other terms to ""make up for it"". This implies that $a_n = 10^m$ for some $m$ . Since this first term will eventually be larger than the rest of the series, the rest of the polynomial must equal $0$ for any $x = 10^r$ . However, this implies that $a_{n-1}x^{n-1} + \dots + a_0$ is a polynomial with infinitely many roots, but this is only possible if it equals $0$ . Hence $p(x) = 10^mx^n$ for some integers $m,n$ . How can I make this more rigorous, and how would you solve this most elegantly?","['contest-math', 'algebra-precalculus', 'polynomials']"
