question_id,title,body,tags
4444269,"For $n\in\mathbb{Z^+}$ unique $f(n)\in\mathbb{Z^+}$ is mapped with $f(1)=1$, $f(f(n))=n$ and $f(2n)=2f(n)+1$. Find $f(2020)$.","Problem For every positive integer $n$ , a unique positive integer $f(n)$ is assigned in the following manner: $f(1)=1$ and for every positive integer $n$ , $f(f(n))=n$ and $f(2n)=2f(n)+1$ . Find the value of $f(2020)$ with a proof. My Approach Given : $f(1)=1$ and $f(2)=2[f(1)]+1=3$ . Claim $1$ : if $f(a)=b$ , then $f(b)=a$ for all positive integers $a$ and $b$ . Proof : $f(a)=b$ , given $f(f(n))=n$ , then $f[f(a)]=a$ , therefore $f(b)=a$ . We know that $f(2)=3$ , therefore $f(4)=7, f(8)=15, f(16)=31, f(32)=63$ . From Claim $1$ $f(63)=32$ . Then, $f(126)=65, f(252)=131$ . $f(131)=252$ , therefore $f(262)=505$ . $f(505)=262$ , therefore $f(1010)=525$ , thus $f(2020)=1051$ . I want to know whether my proof given below is correct and whether there's a better proof than mine.","['contest-math', 'functional-equations', 'functions', 'integers']"
4444281,"let curve $y=\dfrac{x^2}{4}$ and point $F(0,1)$. Let points $A_1(x_1,y_1),A_2(x_2,y_2),...,A_n(x_n,y_n)$ be n points on curve such that","let curve $y=\dfrac{x^2}{4}$ and point $F(0,1)$ . Let points $A_1(x_1,y_1),A_2(x_2,y_2),...,A_n(x_n,y_n)$ be n points on curve such that $x_k>0$ and $\angle{OFA_k}=\dfrac{k\pi}{2n}, (k=1,2,3....,n)$ then find $\lim_{n\to \infty}\dfrac{1}{n} \sum_{k=1}^{n} FA_k$ I am not able to get how to solve. Can anyone help. Thanks","['limits', 'conic-sections']"
4444369,Is $S_2 \wr S_k$ contained in $(S_a \times S_b) \wr S_{k-1}$?,"I'm working on my thesis and I want to prove a theorem but I need the following to be true: $S_2 \wr S_k$ is not isomorphic to a subgroup of $(S_a \times S_b) \wr S_{k-1}$ where $a,b < 2k$ . Does anybody have an idea if this is true? EDIT:I am using this as a lemma to prove that every graph $G$ with automorphismgroup equal to $S_2 \wr S_k$ has at least $k$ disjunct edges. It is not hard to prove that this holds for $k=2$ but for $k=3$ there is a counterexapmle since $S_2 \wr S_3 \cong S_2 \times S_4$ , it seems to me that it probably would hold if $k$ is high enough, though I don't know how to proof that. (P.S. The counterexample for $k=3$ also gives a counterexample to the graph theorem I'm trying to prove since you can just take an empty graph on 4 vertices with the disjunct union of a single edge.)","['symmetric-groups', 'wreath-product', 'group-theory', 'abstract-algebra']"
4444389,Confusion about a conditional expectation,"Suppose $X$ is an integrable, positive random variable and define the filtration $(\mathcal{F}_t)_{t\ge 0}$ with $\mathcal{F}_t=\sigma(\{X\le s\} :s\le t)$ . I want to calculate $\mathbb{E}[X|\mathcal{F}_t]$ , but I seem to get two very different results depending on how I calculate it.
On one hand, I would write $X=\mathbf{1}_{\{X>t\}}X+\mathbf{1}_{\{X \le t\}}X$ and say that both of these terms are $\mathcal{F}_t$ -measurable. But then $X$ itself is $\mathcal{F}_t$ -measurable, which I think is really strange since that would mean $\sigma(X) \subseteq \mathcal{F}_t $ for every $t \ge 0$ which can't be true. On the other hand, I have shown that for example $$\mathbf{1}_{\{X >t\}}\mathbb{E}[X|\mathcal{F}_t] =\mathbb{E}[\mathbf{1}_{\{X >t\}}X|\mathcal{F}_t] = \mathbf{1}_{\{X>t\}}\frac{\mathbb{E}[\mathbf{1}_{\{X>t\}}X]}{\mathbb{E}[\mathbf{1}_{\{X>t\}}]}$$ And similariy on the complement.
I showed this by showing that the right hand side integrates like $\mathbf{1}_{\{X>t\}}X$ on every set from $\{\{X\le s\}:s\le t\}\cup \{\Omega\}$ which is stable under intersections and contains the underlying set $\Omega$ , so this should suffice. But both results seem weird to me since in ""Introduction to the theory of point processses"" by Dayley and Vere-Jones, they claim that $$\mathbb{E}[X|\mathcal{F}_t]=\mathbf{1}_{\{X\le t\}}X + \mathbf{1}_{\{X>t\}}\frac{\mathbb{E}[\mathbf{1}_{\{X>t\}}X]}{\mathbb{E}[\mathbf{1}_{\{X>t\}}]}$$ Clearly something has gone wrong somewhere, but I cannot see where - which, if any, of the above results are correct?","['conditional-expectation', 'measure-theory', 'probability-theory']"
4444415,"If $f:[a,b]\to\mathbb{R}$ continuous at all but countably infinitely many points, $g:[0,1]\to[a,b]$ continuous, can $f\circ g$ be a.e. discontinuous?","Let $f:[a,b]\to\mathbb{R}$ be continuous except at $\{x_1,x_2,\cdots,x_n,\cdots\}$ , $g:[0,1]\to[a,b]$ be a continuous function. My question is: can the points of discontinuity of $f\circ g$ have full measure? The set of points of discontinuity of $f\circ g$ is included in $\bigcup^\infty_{n=1} \partial g^{-1}(x_n)$ , where $\{\partial g^{-1}(x_n)\}_{n\ge 1}$ is a countably infinite collection of disjoint closed sets with empty interior. I'm well aware that a subset of $[0,1]$ with full measure can be written as a countable union of closed sets with empty interior (there are many simple constructions), but I don't know if there is a subset of $[0,1]$ with full measure that is a countable union of disjoint closed sets with empty interior. Also, even there is a subset $E$ of $[0,1]$ with full measure such that $E = \bigcup^\infty_{n=1} C_n$ , where $\{C_n\}$ are disjoint closed sets with empty interior, can we find $\{x_1,x_2,\cdots,x_n,\cdots\}$ and define a continuous function $g$ such that $g^{-1}(x_n) = C_n$ ? Edit : Note that $f\circ g$ cannot be a.e. discontinuous if $f$ has only finitely many points of discontinuity, since $\bigcup^{N}_{n=1} \partial g^{-1}(x_n)$ would be closed and, by Baire category theorem, have empty interior, so it cannot have full measure. On the other hand, the set of points of discontinuity of $f\circ g$ can have measure arbitrarily close to 1, even if $f$ is just discontinuous at one point.","['continuity', 'measure-theory', 'real-analysis']"
4444439,Minimize trace of quadratic inverse + LASSO,"Given a symmetric positive definite matrix $S \in \mathbb R^{d\times d}$ and $\lambda > 0$ , I would like to find $$X^\star := \underset{{X\in\mathbb R^{d\times d}}}{\operatorname{argmin}} \operatorname{tr}\left(X^{-T}SX^{-1}\right) + \lambda \|X\|_1.$$ where $$\|X\|_1 := \sum_{i=1}^d\sum_{j=1}^d\left\vert X_{ij}\right\vert$$ Has anyone seen this kind of objective function? In particular, it has proven to be quite tricky as it seems to be locally convex.","['matrices', 'optimization', 'regularization']"
4444479,Minimize $\mathrm{tr}(B^TXB)$ subject to $X=A^TX(I+BB^TX)^{-1}A$,"For a given $A=\begin{bmatrix}a_1&&&\\&a_2&1&\\&&a_2&\\&&&a_2\end{bmatrix}$ and $B=\begin{bmatrix}b_{11}&b_{12}\\b_{21}&b_{22}\\b_{31}&b_{32}\\0&b_{42}\end{bmatrix}$ , where $a_i>1$ , $a_1\neq a_2$ , $a_i$ and $b_{ij}$ are real, \begin{array}{ll} \underset{X \in \mathbb{R}^{4\times 4}}{\text{minimize}} & \mathrm{tr} \left( B^T X B \right)\\ \text{subject to} & X=A^TX(I+BB^TX)^{-1}A.\end{array} Here $X$ is the unique solution to DARE (discrete-time algebraic Riccati equation). For a specific $A$ and $B$ , I can use Matlab to solve the DARE and then insert the resulting $X$ to find $\mathrm{tr}(B^TXB)$ . Is it possible to get the general answer in terms of $A$ and $B$ ? I have asked this question for general $A$ and $B$ here , but now trying to solve it for simpler case. Currently I have the following bounds: $$a_1^2a_2^4+a_2^2-2\geq \mathrm{tr}(B^TXB) \geq 2a_1a_2^3-2.$$ Matlab code to calculate the $\mathrm{tr}(B^TXB)$ when $A$ is fixed for various $B'$ s: A=[5 0 0 0; 0 2 1 0; 0 0 2 0; 0 0 0 2];
Q=zeros(4,4);
for i=1:100
    B=2.*rand(4,2)-1*ones(4,2);
    [X,K,L] = dare(A,B,Q);
    t(i)=trace(B'*X*B);
end","['optimization', 'trace', 'linear-algebra']"
4444509,Is this question solvable using the law of sines and cosines?,"Is it possible to (analytically) calculate the area of the following triangle using the rules of sines and cosines ? Is it possible to calculate it using only the rule of sines? The data given is: $BD$ is a median to $AC$ , $\angle ABD = 50^\circ$ , $\angle DBC = 18^\circ$ and $BD = 4.2~\text{cm}$ . It seems to me that some data is missing. Am I right?","['trigonometry', 'geometry', 'plane-geometry']"
4444532,Range of the function $f(x) = 4^x + 4^{-x} + 2^x + 2^{-x} + 3$,"I tried to form a quadratic equation by taking $2^x = u$ $$u^2 + \frac{1}{u^2}+u + \frac{1}{u}+3$$ $$\left(u + \frac{1}{u}\right)^2+u + \frac{1}{u}+1$$ taking $u+1/u = t$ $$t^2+t+1$$ From this quadratic equation, range should be $[3/4, \infty)$ . But this is wrong, correct answer is $[7, \infty)$ which I got using AM-GM inequality. My question is, why is the above quadratic method wrong?","['functions', 'quadratics']"
4444579,A very important measure.,"Let $f\in L^1(X,\mathcal{A},\mu)$ ; we define an application $\nu\colon\mathcal{A}\to \mathbb{R}$ placing $$\large \nu(E):=\int_E f\;d\mu\quad (E\in\mathcal{A}).$$ Let $\{E_k\}_k\subseteq \mathcal{A}$ be a sequence of disjoint sets and define $E=\bigcup_{k=1}^\infty E_k$ . Claim. $\large \nu(E)=\sum_{k=1}^\infty\nu(E_k)$ For definition, $\nu(E)=\nu_+(E)-\nu_-(E)$ , where $\nu_\pm\colon\mathcal{A}\to\mathbb{R}_+$ and $\large \nu_\pm=\int_E f_\pm\; d\mu$ , since $\nu_\pm$ are measures we have $$
\begin{aligned}
\large \nu(E)=\nu_+(E)-\nu_-(E)&=\left(\sum_{k=1}^\infty\nu_+(E_k)\right)-\left(\sum_{k=1}^\infty\nu_-(E_k)\right)\\
&\color{RED}{=}\sum_{k=1}^\infty \left(\nu_+(E_k)-\nu_-(E_k)\right)\\
&=\sum_{k=1}^\infty \nu(E_k)
\end{aligned}
$$ I know that the red equality derives from the fact that the series $$\large \sum_{k=1}^\infty \nu_\pm(E_k)=\int_E f_\pm\,d\mu<\infty.$$ My text also states that in addition to series convergence, known theorems on absolutely convergent series have been used. However, I do not understand where these theorems have been used. Could you give me some explanation about it? Thanks.","['proof-explanation', 'measure-theory', 'real-analysis']"
4444592,EM Algorithm vs Gradient Descent,"I was reading about the EM algorithm ( https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm ) - this algorithm is used for optimizing functions (e.g. the Likelihood Functions belonging to Statistical Models). I have heard that in the context of optimizing the Likelihood Function for Mixture Models in Statistics ( https://en.wikipedia.org/wiki/Mixture_model ), the EM algorithm is preferred to more common algorithms such as Gradient Descent. Apparently, this is because the Likelihood Function of Mixture Models is usually ""multi-modal"" (e.g. a Mixture Model is a mixture of several Normal Distributions and each Normal Distribution has a ""mode"" - therefore, a Mixture Model is almost guaranteed to be ""multi-modal""). What I am having difficulty in understand is the following point : Why should the EM algorithm be any more suited for optimizing ""Multi-Modal"" functions compared to Gradient Descent? That is, by considering the mathematical properties of ""Multi-Modal"" functions, the EM algorithm and Gradient Descent - how can we use these mathematical properties to rationalize why the EM algorithm is more suited for optimizing ""Multi-Modal"" functions compared to Gradient Descent? Thanks! Note: My guess is that perhaps the EM algorithm might be ""less computationally expensive"" compared to Gradient Descent? Do either of these algorithms have any theoretical convergence properties that could be used to explain the traditional preference of EM over Gradient Descent in ""Multi-Modal"" FUnctions? References: https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch20.pdf Thanks!","['statistics', 'maximum-likelihood', 'optimization', 'gradient-descent', 'probability']"
4444621,Help with $\frac{\sin\theta+\cos\theta}{-\sin\theta+\cos\theta}=?$,"Assume that $0<\theta<\frac{\pi}{4}$ . If $\sin2\theta=\frac{1}{4}$ , then $\frac{\sin\theta+\cos\theta}{-\sin\theta+\cos\theta}=?$ My approach with this problem is to rationalize the denominator in the fraction to get this: $$
\frac{\sin2\theta+1}{\cos2\theta}=\frac{\frac{1}{4}+1}{\cos2\theta}
$$ After that i got stuck with $\cos2\theta$ any advice how to attack the problem?",['trigonometry']
4444722,When is $d^{2} x$ =0?,"We want to find $d^{2} z$ : $$
(4 x-3 z-16) d x+(8 y-24) d y+(6 z-3 x+27) d z =0
$$ So, in my book, we apply the differential operator d to the above equation : We use the product rule ; $$
(4 d x-3 d z) d x+(8 d y) d y+(6 d z-3 d x) d z+(6 z-3 x+27) d^{2} z=0
$$ So my question is : Where did $d^{2} x$ and $d^{2} y$ go? Are they equal to 0? If so why $d^{2} z$ is not 0? Edit:
That's the problem : So above $d^{2} z$ isn't asked in the question but it's needed to find the asked ones ; Problem 1 (65 points) We consider the function $$
F(x, y, z)=2 x^{2}+4 y^{2}+3 z^{2}-3 x z-16 x-24 y+27 z+94
$$ and let $z=z(x, y)$ be an implicit function defined by the equality $F(x, y, z)=-1$ . 1.1. Calculate $\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}, \frac{\partial^{2} z}{\partial x^{2}}, \frac{\partial^{2} z}{\partial y^{2}}$ and $\frac{\partial^{2} z}{\partial x \partial y}$ at point $(x=1, y=3, z=-5)$ .","['partial-derivative', 'multivariable-calculus', 'derivatives']"
4444771,Need help with Euclidean geometry,"Here's the problem: Consider triangle ABC inscribed in circle C(O, r). The angle bisector of ∠A (resp. ∠B,
∠C) intersects the circle C(O, r) in the points A and A' (resp. B and B', C and C'). Prove that m(∠A'B'C') = 90 − 0.5m(∠ABC). I wasn't given a diagram, but I drew a rough sketch of what I think it looks like. I don't know where to start with this. Any advice would be appreciated. This is for a college-level course, by the way.","['euclidean-geometry', 'geometry']"
4444831,"Proving the existence of a weak limit given the convergence of $\lim_{n\to\infty}(x_n,v) \forall v\in H$","I've got a problem I'm a little stuck on and was curious as to a way to prove this. I've always sort of just assumed that when defining weak convergence, the weak limit automatically exists but now I cant prove it. More formally, if for a sequence $\{x_n\}$ where $\lim_{n\to\infty}(v,x_n)$ exists for all $v\in H
$ , prove that there exists an $x\in H$ such that $\lim_{n\to\infty}(x_n,v)=(x,v)\quad \forall v\in H$ . My first approach was to consider the $\sup_{v\in H} \lim_{n\to\infty} \frac{1}{\Vert x_n\Vert} (x_n,\frac{v}{\Vert v\Vert})$ as something that would give me the weak limit $x$ that I desired, but after thinking about the case where $H=\ell^2$ and $x_n=e_n=(0,...,1,0,...)$ I'm uncertain, as $e_n\overset{w}{\to} 0$ .","['weak-convergence', 'functional-analysis', 'analysis']"
4444878,Prove that there exist primes $p_{i}$ such that $\prod_{i=1}^{k}p_{i} \mid \sum_{i=1}^{k}(p_{i})^{a_{i}}$,"If $k\geq 3$ is a given positive integer, prove that there exist  prime numbers $p_{1}<p_{2}<\cdots<p_{k}$ and positive integers $a_{1},a_{2},\cdots,a_{k}$ , such that $$p_{1}p_{2}\cdots p_{k} \mid (p_{1})^{a_{1}}+(p_{2})^{a_{2}}+\cdots+(p_{k})^{a_{k}}$$ I need to prove $$p_{i} \mid (p_{1})^{a_{1}}+(p_{2})^{a_{2}}+\cdots+(p_{i-1})^{a_{i-1}}+(p_{i+1})^{a_{i+1}}+\cdots+(p_{k})^{a_{k}}\quad\forall i=1,2,\ldots,k$$ from Fermat's little theorem, we need prove there exist $a_{1},a_{2},\ldots,a_{n}$ s.t. \begin{align} 
a_{1} &= 0 \pmod {(p_{i}-1)} &(i &\neq 1) \\
a_{2} &= 0 \pmod {(p_{i}-1)} &(i &\neq 2) \\
&\;\;\vdots \\
a_{k} & = 0 \pmod {(p_{i}-1)} & (i &\neq k)
\end{align} it seem use CRT, but in fact we can't use this theorem, because this problem not  such Chinese remainder theorem condition, so How to prove this problem","['contest-math', 'divisibility', 'number-theory', 'elementary-number-theory', 'chinese-remainder-theorem']"
4444884,Why is the function $\|\mathbf{J}\|_{\infty}$ $1$-Lipschitz w.r.t to the Euclidean norm?,"Assume that $\mathbf{J} = \{ J \}_{ij}$ are centered independent standard Gaussian with variance $1/n$ random variables for $i, j = 1, \dots, n$ . Why is the function $\|\mathbf{J}\|_{\infty}$ then $1/n$ -Lipschitz w.r.t to the Euclidean norm, that is, $$
\left|
  \sup_{\|u\| = 1} \langle u, \mathbf{J}u \rangle
- \sup_{\|v\| = 1} \langle v, \mathbf{J}v \rangle
\right|
\leq
\frac{1}{n} \|u - v\|_2 .
$$ Thus, it has a sub-Gaussian tail which is $$
P(\|\mathbf{J}\|_{\infty} - E[\|\mathbf{J}\|_{\infty}] \geq x)
\leq
e^{-n x^2 / 2}.
$$ The original statement is as follows. Moreover, it is easy to see that $$
\|\mathbf{J}\|_\infty = \sup_{\|u\| = 1} \langle u, \mathbf{J} u \rangle
$$ has sub-Gaussian tail.
Indeed, is is a Lipschitz function of the Gaussian entries of $\mathbf{J}$ (with respect to the Euclidean norm) with constant bounded by $N^{-1/2}$ so that by Herbst argument (see [24, 25]), we have the concentration inequality $$
  P(\|\mathbf{J}\|_\infty \geq E[\|\mathbf{J}\|_\infty] + x)
  \leq
  e^{-\frac{1}{2} N x^2} .
$$","['lipschitz-functions', 'statistics', 'analysis', 'probability']"
4444935,Why are these three intersection points collinear?,"This is what I found several years ago when I was in middle school: Suppose we have a circle on a plane and arbitrarily choose four different points on the circle, say $P,A,B,C$ . Then draw three circles with center $A,B,C$ and radius $|PA|, |PB|, |PC|$ , respectively. The three new circles meet at three crossover points besides $P$ , say $D, E, F$ . I was amazed that $D, E, F$ are on the same line. Could someone prove this?","['euclidean-geometry', 'analytic-geometry', 'geometry']"
4444946,Prove that for all natural numbers $\neg\big(S(a)+b=a\big)$,"The natural numbers are defined as per the Peano axioms . Addition is defined recursively as follows: $$
\begin{cases}
a + 0 &= a,\\
a + S(b) &= S(a+b).
\end{cases}
$$ Prove that $\forall a\forall b\Big(\neg\big(S(a)+b=a\big)\Big)$ . My attempts at a solution I tried using induction on $b$ , but I am struggling to prove the inductive step. Please do not use any laws regarding inequality, as I have yet to formally define them.","['peano-axioms', 'induction', 'natural-numbers', 'discrete-mathematics']"
4444972,Sum of series in the form $\sum^{\infty}_{n=0}\frac{1}{(n+k)\cdot n!}$ for some k>0,"I was recently given the series $$\sum^{\infty}_{n=0}\frac{1}{(n+3)\cdot n!}$$ and was told that it evaluated to a very nice number of $e-2$ . However, I do not know how to arrive at that answer using any of the tricks I know. It is neither a geometric series, a p series, or a known Taylor series. Neither can I find any way to form an integral representation of $\frac{1}{(n+3)\cdot n!}$ so I can swap the integral and summation. So how would I go about evaluating this? More generally, how would I evaluate series with the form $$\sum^{\infty}_{n=0}\frac{1}{(n+k)\cdot n!}$$ for a positive nonzero k?","['summation', 'factorial', 'sequences-and-series']"
4444981,Proving that $\mathbb{R}/\mathbb{Z} \cong S^1$ as groups,"I am trying to prove that $\mathbb{R}/\mathbb{Z}$ under addition is isomorphic to $S^1$ . The definitions I have to work with are: \begin{align*} 
\mathbb{R}/\mathbb{Z} & = [0,1) \text{ with addition mod $1$} \\ 
S^1 & =  \{z \in \mathbb{C}^{*} \mid |z| = 1\} \text{ with multiplication.} 
\end{align*} Though it seems natural to use the first isomorphism theorem, I don't yet have access to it, so I need to use the bijection $t \mapsto \exp(2 \pi i t)$ . Here is my attempt at a proof. Define $\varphi: \mathbb{R}/\mathbb{Z} \to S^1$ by $\varphi(t) = \exp(2 \pi i t)$ . We first show that $\varphi$ is a homomorphism. Homomorphism. Let $a,b \in \mathbb{R}/\mathbb{Z}$ . We denote addition in $\mathbb{R}/\mathbb{Z}$ by $*$ and addition in $\mathbb{R}$ by $+$ . If $a + b < 1$ , then $a * b = a + b$ , and we have \begin{align*}
\varphi(a * b) & = \varphi(a + b) \\
& = \exp^{2 \pi (a + b) i} \\
& = \exp(2 a \pi i + 2 b \pi i) \\
& = \exp(2 a \pi i) \exp(2 b \pi i) \\
& = \varphi(a) \varphi(b).
\end{align*} Now suppose that $a + b \geq 1$ . Then $a*b = a + b - 1$ , and we have \begin{align*}
\varphi(a * b) & = \varphi(a + b - 1) \\
& = \exp(2 \pi (a + b - 1) i) \\
& = \exp(2 \pi a i + 2 \pi b i - 2 \pi i) \\
& = \exp(2 \pi a i) \exp(2 \pi b i) \exp(-2 \pi i) \\
& = \varphi(a) \cdot \varphi(b) \cdot 1 \\
& = \varphi(a) \varphi(b), 
\end{align*} so $\varphi$ is a bijective homomorphism and therefore an isomorphism. We next show that $\varphi$ is a bijection of sets. Injectivity. For injectivity, we show that $\mathrm{ker}(\varphi) = \{0\}$ . For any $t \in [0,1)$ , we have \begin{align*}
t \in \mathrm{ker}(\varphi) & \iff \varphi(t) = \exp(2 \pi i t) = 1 \\
& \iff \cos(2\pi) + i \sin(2\pi t) = 1 + 0i \\
& \iff \sin(2 \pi t) = 0 \\
& \iff t \in \mathbb{Z} \cap [0,1) \\
& \iff t = 0,
\end{align*} so $\mathrm{ker}(\varphi) = \{0\}$ , so $\varphi$ is injective. Surjective. Let $z \in S^1$ , and write $z$ in its exponential form $z = \exp^{i \theta}$ where $\theta \in [0,2\pi)$ . As $0 \leq \theta < 2\pi$ , we have $0 \leq \frac{1}{2\pi} \theta < 1$ , so $\frac{1}{2\pi} \theta \in \mathbb{R}/\mathbb{Z}$ . We then have $$
\varphi\left(\frac{1}{2\pi} \theta\right) = \exp\left(2\pi \cdot \frac{\theta}{2\pi} i \right) = \exp(i \theta) = z,
$$ so $\varphi$ is surjective and therefore bijective.","['quotient-group', 'group-theory', 'solution-verification']"
4445000,There are 11 plates. Each plate weight is a natural number and no two weights are equal... Prove that one plate has to weigh more than 35,"There are 11 plates of which each weighs a natural number of grams and no two plates have the same weight. We know that no matter how we put the plates (it doesn't have to be all of them) on a scale with two sides, that the side with more plates is always heavier (we don't know which side is heavier if they have an equal number of plates). Prove that one plate has to weigh more than 35 grams. I think looking at the case where the plates weigh $35,34,33,32,31,30,29,28,27,26,25$ is worth it. We can notice that the sum of the last $6$ numbers is $165$ while the sum of our first $5$ numbers is also $165$ , a contradiction. We can also notice if we were to add any smaller plate (as we don't want to use any plate that weighs more than $35$ ) that we can yet again sort the plates in descending order, take the first $5$ and they will never weigh less than the last $6$ . My question is, this seems like a far reach for a proof. Does anyone else have any decent ideas that might help me out? Thanks in advance!",['combinatorics']
4445132,The “parentheses” of the composite functions,"The question requires me to show that the following statement is true or false. Let $A,B$ be sets, and $f:A\to B$ , $g:B\to A$ be functions. Suppose $g\circ f\circ g$ is surjective, and $f\circ g\circ f$ is injective. Then $f\circ g$ is bijective. So basically I apply the fact that “if $g\circ f$ is surjective, then $g$ is surjective” and “if $g\circ f$ is injective, then $f$ is injective”. But the problem is, can I read $f\circ g\circ f$ as both $f\circ (g\circ f)$ and $(f\circ g)\circ f$ ? The purpose of doing this is to get “ $g\circ f$ is surjective” and “ $g\circ f$ is injective”. But I am not sure if it is legal.",['functions']
4445269,"Submersion, diffeomorphism, and embedding",Let $\pi:M \rightarrow N$ a submersion and $f:S \rightarrow M$ a smooth map such that $\pi \circ f : S \rightarrow N$ is a diffeomorphism. Show that $f:S \rightarrow M $ is an embedding. I'm having trouble showing that $f$ is a homeomorphism onto its image with the subspace topology. I already showed that $f$ is an immersion and I tried to show that is an open map using the fact that submersions are open but got stuck. Any help is appreciated.,"['manifolds', 'general-topology', 'smooth-manifolds', 'differential-geometry']"
4445283,Question regarding Solutions to the equation $y'' + y = y^3$,"A while ago I had a question given to me by a tutoring student that read as follows: A solution to the differential equation $y'' + y = y^3$ is: A) $y = \tanh(x/\sqrt{2})$ B) $y = \tanh(x\sqrt{2})$ C) $y = \coth(x/\sqrt{2})$ D) $y = \coth(x\sqrt{2})$ E) None of these. [3 Marks] Given the low marks available, I presumed that the desired method would be to simply 'plug-in' the given solutions to see which were satisfactory. Interestingly, I found that both A) and C) satisfied this equation.
I was curious about this, since the formatting to me implied that only one of these should be correct... I decided to scout around online a bit to see if there was an actual method to solving a DE like this to see if I could narrow it down, or pull these solutions out directly (for my own interest). The results I obtained this way were interesting. My method is as below $$y''+y=y^3$$ $$y''y'+yy'=y^3y'$$ $$\frac{1}{2}((y')^2)'+\frac{1}{2}(y^2)'=\frac{1}{4}(y^4)'$$ $$(y')^2+y^2=\frac{1}{2}y^4$$ $$y'=\frac{1}{\sqrt{2}}\sqrt{y^4-2y^2}$$ Separating variables and integrating both sides: $$\int \frac{dy}{y\sqrt{y^2-2}} = \frac{x}{\sqrt{2}} + C$$ Answers I got from here then vary slightly; in my initial attempt I made the substitution $y = \sqrt{2}\cosh{u} \rightarrow dy=\sqrt{2}\sinh{u}du$ , leading to: $$\frac{1}{\sqrt{2}}\int{\frac{du}{\cosh{u}}} = \frac{x}{\sqrt{2}}+C$$ Using a further substitution of $v = e^u$ I finally arrived at a solution: $$\frac{2}{\sqrt{2}}\arctan{e^{\cosh^{-1}{\frac{y}{\sqrt{2}}}}} = \frac{x}{\sqrt{2}}+C$$ After doing some rearranging, I arrived at: $$y = \sqrt{2}\cosh{\left(\ln\left({\tan{\left(\frac{x}{2}+K_1\right)}}\right)\right)}$$ With $K_1 = C/\sqrt{2}$ Sticking this into wolfram alpha seems to show that this is another satisfactory solution to my original equation. Further; when I checked an integral calculator for the result of $$\int\frac{dy}{y\sqrt{y^2-2}}$$ I instead managed to acquire: $$\frac{1}{\sqrt{2}}\arctan{\left(\frac{1}{\sqrt{2}}\sqrt{y^2-2}\right)}=\frac{x}{\sqrt{2}}+C$$ leading to $$y = \sqrt{2}\sec(x+K_2)$$ where $K_2 = C\sqrt{2}$ Again, putting this into wolfram alpha seems to indicate that it is too another solution to the equation. What I'm wondering is How to extract the two solutions indicated in the question out of the DE, as I've clearly been unable to do so, What's the deal with the solutions I've found? I wouldn't be as surprised or intrigued if they both simplified to one (or both) of the results expected from the question, but plotting them indicates that they are not equivalent. Would be greatful to hear some insight from the wider community.
Thanks!","['calculus', 'trigonometry', 'ordinary-differential-equations', 'hyperbolic-functions']"
4445288,How we can solve this differential equation $y'=x^2 e^{x^3}-\ln(y^2)$,"What is the way to solve this differential equation : $$y'=x^2 e^{x^3}-\ln(y^2)$$ I thought at start to make it exact but I got no luck there for the factor , Does it need series or Laplace? May anyone please tell me how to solve it ?",['ordinary-differential-equations']
4445307,Composite function bijectivity,"The problem requires me to show that the following statement is true or false. Let $A,B$ be sets, and $f:A\to B$ , $g:B\to A$ be functions. Suppose $g\circ f\circ g$ is surjective, and $f\circ g\circ f$ is injective. Then $f\circ g$ is bijective. So I get: “ $g\circ f\circ g$ is surjective and $f\circ g\circ f$ is injective“ $\implies$ “ $g\circ f$ is bijective“ $\implies$ “ $g$ is surjective and $f$ is injective” But since it does not imply that “ $f\circ g$ is bijective”, I try to find some counter examples. But it is quite difficult. Normally, if the question turns to be: Let $A,B$ be sets, and $f:A\to B$ , $g:B\to C$ be functions. Suppose $g$ is surjective, and $f$ is injective. Then $f\circ g$ is bijective. I can easily disprove it since $A={0,1}, B=C={2}, f(2)=2,g(0)=2,g(1)=2$ is already a counter example. But in this case, the set $B$ has to map back to $A$ by $g$ . Then seems I can never find a counter example? Is it actually bijective? Or what else did I miss?",['functions']
4445319,number of permutations maximizing a sum,"Let $n$ be an odd integer greater than $1$ . Find the number of permutations $\sigma$ of the set $\{1,\cdots, n\}$ for which $|\sigma(1) - 1| + |\sigma(2) - 2|+\cdots + |\sigma(n) - n| = \frac{n^2 - 1}2$ . I found the solution below from a book, but I have some questions: Why is $\frac{n^2 - 1}2$ the maximum possible value of $\sum_{i=1}^n |\sigma(i) - i|$ ? I find it hard to prove this formally as $\sigma(i)$ is a bijection. For instance, it might not be true that $|\sigma(i) - i|\leq \frac{1}n (n^2 - 1)/2$ for all $i$ . Why must $\{\sigma((n+2)/2),\sigma((n+5)/2),\cdots, \sigma(n)\}\subset \{1,2,\cdots, (n+1)/2\}$ and why must $\{\sigma(1),\cdots, \sigma((n-1) / 2)\}\subset \{(n+1) / 2, \cdots n\}$ ? If $\sigma((n+1)/2) = k\leq (n+1)/2$ , then how can one verify that $\sum_{i=1}^{n} |\sigma(i) - i|$ indeed achieves the maximum value? I'm not sure if $\sum_{i=1}^{(n-1)/2} |\sigma(i) - i|$ and $\sum_{i=(n+3)/2}^{n} |\sigma(i) - i|$ have values only dependent on $k$ . I've also read this post but I'm still unsure how to answer my questions.","['permutations', 'combinatorics', 'discrete-mathematics', 'contest-math']"
4445331,Probability that a pencil is blunt,"Suppose a student has $n$ pencils, each of them sharpened. He has his final exams lined up. Each day, before the exam, he randomly chooses a pencil to write the test. What is the probability that on the $k$ -th day, the pencil he picks up is (still) sharp? Note that once a pencil is used, it becomes blunt and a blunt pencil is never re-sharpened. The answer that's given is $\left(1-\frac{1}{n}\right)^{k-1}$ . My understanding is that if $P(k)$ is the probability that the pencil chosen on the $k$ th day is sharp: Then, $P(k=1) = 1$ since every pencil is sharp. For $k>1,$ $\displaystyle P(k) = \sum_{i=1}^{k-1} \frac{n-i}{n}$ since we can have $i=1,2, \cdots k-1$ blunt pencils. But this is clearly wrong as probability can not exceed $1$ which it does here. Questions: Where did I go wrong? I see the recursion $P(k) = P(k-1) \left(1-\frac{1}{n}\right)$ which may be of use. Can I solve it using this recursion?","['discrete-mathematics', 'combinatorics', 'probability']"
4445342,"Probability that out of $n$ bags, at least one contains no black ball, if $n$ of $n^2$ balls are black","I am working on SL Parsonson's Pure Mathematics and I haven't been able to solve this problem: $n^2$ balls, of which $n$ are black and the rest white, are distributed at random into $n$ bags, so that each bag contains $n$ balls. Determine the probability that at least one bag contains no black ball. The answer given in the book is $1-\frac{(n-1)!(n^2-n)!n^{n-1}}{(n^2-1)!}$ . I thought I might start with the fact that there are $\frac{(n^2)!}{n!(n^2-n)!}$ unique arrangements of the balls, and $n-1$ partitions to be placed at intervals of n to divide them into $n$ bags, but I am stuck a little after here.","['combinatorics', 'probability']"
4445344,Question about Differential Geometry (Surfaces and Differentiability,"I have a doubt about differentiability.
Why if I have two regular surface, $S_1$ and $S_2$ $\in \mathbb{R^3}$ and a function $f: \mathbb{R^3} \to \mathbb{R^3}$ such that $f(S_1) \subset S_2$ differentiable, then we can define another function $g=f|_{S_1}$ it's also differentiable? This it's from a proposition of Do Carmo but I can't understand it. And even less can I understand why if it is differentiable we can say that $dg_p(v)=Df(v)$ $\forall p \in S_1$ and $v \in T_p (S_1)$","['surfaces', 'geometry', 'calculus', 'derivatives', 'differential-geometry']"
4445346,Complete intersection Calabi-Yau of dimension $3$ with anti-holomorphic map such that each component of fixed point set has $b^1 \neq 0$,"I am looking for a complete intersection Calabi-Yau manifold $X$ of complex dimension $3$ that admits an anti-holomorphic involution $\sigma: X \rightarrow X$ such that $L:=\operatorname{fix}(\sigma)$ is smooth and each of its connected components has first Betti number $b^1 \neq 0$ . By ""complete intersection Calabi-Yau"" I mean an algebraic variety of $\mathbb{CP}^{k+3}$ cut out by $k$ polynomials that is smooth and has trivial canonical bundle.
I think the only possibilities for such complete intersection Calabi-Yau manifolds are: A quintic in $\mathbb{CP}^4$ Intersection of a quadric and quartic in $\mathbb{CP}^5$ Intersection of two cubics in $\mathbb{CP}^5$ Intersection of two quadrics and a cubic in $\mathbb{CP}^6$ Intersection of four quadrics $\mathbb{CP}^7$ For example, take the Fermat quintic $F=\{[x_0:x_1:x_2:x_3:x_4] \in \mathbb{CP}^4: x_0^5+x_1^5+x_2^5+x_3^5+x_4^5\}$ in $\mathbb{CP}^4$ .
It admits many anti-holomorphic involutions, such as $\sigma: [x_0:x_1:x_2:x_3:x_4] \mapsto [\overline{x_0}:\overline{x_1}:\overline{x_2}:\overline{x_3}:\overline{x_4}]$ and $\sigma': [x_0:x_1:x_2:x_3:x_4] \mapsto [\overline{x_1}:\overline{x_0}:\overline{x_2}:\overline{x_3}:\overline{x_4}]$ (the map $\sigma'$ interchanges the first two coordinates).
Unfortunately, I was unable to compute their fixed point sets.
(In a previous version of the question I wrote that $L:=\operatorname{fix}(\sigma)$ seems to be singular, but I realised it probably is smooth.) Comment:
often ""complete intersection Calabi-Yau"" means an algebraic variety in a product of projective spaces.
My definition in this question is more restrictive.
If I allowed products of projective spaces, then an example in $\mathbb{CP}^1 \times \mathbb{CP}^1 \times \mathbb{CP}^1 \times \mathbb{CP}^1$ would be given in Example 7.6 of Joyce, Karigiannis: A new construction of compact torsion-free G2-manifolds by gluing families of Eguchi–Hanson spaces .
This question is motivated by their article.","['involutions', 'algebraic-geometry', 'differential-geometry']"
4445357,Question about Hille Yosida Theorem proof,"I'm working on Hille-Yosida theorem on Vrabie's book. Here is the statement: In order to prove the sufficiency two lemmas are needed: and Here comes my question, is highlighted in yellow: Why can we deduce that there exists such operator $S(t)$ ? How can we justify that such limit exist? Because of boundedness? Once we know that there exits is easy to show that $$\|e^{tA_\lambda}x-S(t)x\|\to 0$$ since we have $(3.2.5)$ and $(3.2.3)$ . On the other hand, about the words that are highlighted in blue, the convergence in norm implies uniform convergence, but can we choose that convergence just for compact subsets of $R_+$ ?.","['semigroup-of-operators', 'operator-theory', 'functional-analysis', 'analysis']"
4445362,Trying to prove a proposition about the nth order derivative of a polynomial by induction - is this correct?,"Recently, I decided to try and create a formula for the $n$ th order derivative of a polynomial, and I believe I succeeded! I tried to do a proof by induction to confirm this for myself, but since I haven't done one before, I'm not really sure if this is correct. Any insight would be greatly appreciated! So, here's my (attempted) proof: Let $P(x)$ be a polynomial of degree $s$ with integer coefficients $c_0,c_1,c_2,...c_s$ of the form $$P(x)=c_0+c_1x+c_2x^2+c_3x^3+\cdots+c_sx^s$$ Then, I propose that $$P^{(n)}(x)=\sum_{i=0}^{s-n}c_{n+i}\frac{(n+i)!}{i!}x^i$$ Base case for $n=0$ : $$P^{(0)}(x)=\sum_{i=0}^{s}c_{i}x^i$$ $$=c_0+c_1x^1+c_2x^2+c_3x^3+\cdots+c_sx^s$$ $$=P(x)$$ Inductive step: Assume that for a particular natural number $k$ , the case $n=k$ is true (induction hypothesis): $$P^{(k)}(x)=\sum_{i=0}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i$$ Expanding the first term gives: $$P^{(k)}(x)=c_kk!+\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i$$ Differentiating both sides with respect to $x$ gives: $$\frac{d}{dx}P^{(k)}(x)=\frac{d}{dx}(c_kk!+\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{i!}x^i)$$ $$P^{(k+1)}(x)=\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{(i-1)!}x^{i-1}$$ The right side can be simplified as: $$\sum_{i=1}^{s-k}c_{k+i}\frac{(k+i)!}{(i-1)!}x^{i-1}=\sum_{i=0}^{s-k-1}c_{k+i+1}\frac{(k+i+1)!}{i!}x^i$$ $$=\sum_{i=0}^{s-(k+1)}c_{(k+1)+i}\frac{((k+1)+i)!}{i!}x^i$$ Equating both sides once more gives: $$P^{(k+1)}(x)=\sum_{i=0}^{s-(k+1)}c_{(k+1)+i}\frac{((k+1)+i)!}{i!}x^i$$ Therefore, the statement is true for $n=k+1$ , proving the inductive step. Since the base case and the inductive step have been proven true, the statement holds for all positive integers $n$ . Hopefully that was all correct! Let me know if I messed up somewhere, it'd be appreciated! Edit: As Aman Kushwaha described, the inductive step has an assumption that $n\leq s$ , and that for $n>s$ , we can just define $P^{(n)}(x)=0$ . Edit 2:
Ryszard Szwarc showed me that there is actually a formula on Wikipedia analogous to the power rule for regular derivatives, but for fractional derivatives instead. Whereas the normal power rule for the nth order derivative is $$f^{(n)}(x)=\frac{d^n}{dx^n}cx^k=\frac{k!}{k-n}cx^{k-n}$$ the generalized power rule for derivatives of a real number order (excluding the negative integers) is $$f^{(n)}(x)=\frac{d^n}{dx^n}cx^k=\frac{\Gamma (k+1)}{\Gamma (k-n+1)}cx^{k-n}$$ Using this, a new formula that encompasses fractional derivatives can be made that avoids the ambiguity associated with the non integer subscript polynomial coefficients. The only remaining issue is the ambiguity with the upper bound of the summation, however, when analyzing the generalized power rule, it becomes clear that constant terms in the polynomial only evaluate to 0 after each whole number iteration of the differential operator. Therefore the issue can actually be avoided entirely by using the floor function! So, overall, this is what I came up with: $$P^{(n)}(x)=\sum_{i=0}^{s-\lfloor n \rfloor}c_{\lfloor n \rfloor +i}\frac{\Gamma (\lfloor n \rfloor +i+1)}{\Gamma (\lfloor n \rfloor -n+i+1)}x^{\lfloor n \rfloor -n+i},\: n \neq \mathbb{Z}^-$$ As far as I can tell, it seems to work, but I have absolutely no idea how to prove it! The formula isn't exactly pretty though, so if anyone has any idea how to simplify it, or how to prove it of course, I'd love to hear about it.","['calculus', 'solution-verification', 'induction', 'proof-writing']"
4445370,Relating the Lie derivative to inner product of 2-tensors,"Let $(M,g)$ be a Riemannian manifold. Let $f \in C^\infty(M)$ , $X$ be a vector field and $h$ be a (symmetric) covariant $2$ -tensor. Denote by $\langle \cdot, \cdot \rangle$ the inner product induced by the metric $g$ on the space of covariant $2$ -tensors. I would like to know why the following formula holds: $$\langle df \otimes df, \mathcal{L}_X(h) 
\rangle = (\mathcal{L}_X(h))(\nabla f, \nabla f).$$ Here, $\mathcal{L}_X$ is the Lie derivative in the direction of $X$ and $\nabla f$ is the gradient of $f$ . I am sorry if this is a silly question, but I have never seen such identity.","['tensors', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4445395,Prove that R is an integral domain,"I'm studying for my qualifying exam and I came across the following question in one of the old question bank. Consider the affine space given by four $2\times 2$ matrices, i.e., $\mathbb{A}^{16}\cong M(\mathbb{C})_{2\times 2}^4$ . Now, consider the algebraic set $V$ given by the vanishing of the relation $AB-CD=0$ , where the matrices are as follows: $A=(a_{ij}), B=(b_{ij}), C=(c_{ij})$ and $D=(d_{ij})$ . Prove that $V$ is irreducible in $\mathbb{A}^{16}$ . In other words, I want to prove that the following ring $R=\mathbb{C}[a_{11},a_{12},a_{21},a_{22}, b_{11},\dotsc, d_{21},d_{22}]/I$ , where $I=(a_{11}b_{11}+a_{12}b_{21}−c_{11}d_{11}−c_{12}d_{21},\,a_{11}b_{12}+
a_{12}b_{22}−c_{11}d_{12}−c_{12}d_{22},\,a_{21}b_{11}+a_{22}b_{21}−c_{21}d_{11}−c_{22}d_{21},\,a_{21}b_{12}+a_{22}b_{22}−c_{21}d_{12}−c_{22}d_{22})$ $R$ is an integral domain. I've been trying to follow the same idea as in this post ( https://math.stackexchange.com/a/4303220/884739 ), but I'm having hard time trying to figure out what the correct change of coordinate should be, so that I can embed this ring $R$ inside some field and hence, conclude that $R$ is an integral domain.","['quiver', 'representation-theory', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4445424,Directional derivative dotted with function,"I appreciate your help. In this problem, a function is dotted with its own derivative. I'm not sure if this is a case of directional derivative or just scaling. The problem is as follows: Let $\vec{x}: \mathbb{R} \to \mathbb{R}^3$ be a differentiable function and $r: \mathbb{R} \to \mathbb{R}$ be the function $r(t) = \lVert \vec{x}(t) \rVert$ denote the $l_2$ length. Let $t_0$ be a real number and $r(t_0) \neq 0$ , then $r$ is differentiable at $t_0$ and $$
r'(t_0) = \frac{\vec{x}'(t_0) \cdot \vec{x}(t_0)}{r(t_0)}
$$ I appreciate your help.","['multivariable-calculus', 'calculus', 'derivatives', 'vectors']"
4445453,Why are degree zero line bundles on an Abelian Variety translation invariant?,"I am reading about Abelian varieties from Huybrecht's 'Fourier-Mukai Transforms in Algebraic Geometry' and in here $\operatorname{Pic}^0(A)$ is defined to be the group of translation invariant line bundles (ie. $\{L \in \operatorname{Pic} A\;|\;t_a^*L \cong L \text{ for all } a \in A\}$ where $t_a: A \to A$ is translation by $a$ ). In other contexts, this is defined to be the line bundles associated to divisors of degree zero. As such, I expect these to be equivalent notions. In the usual references (Mumford, Milne) I had trouble finding this fact. Does anyone know of one? Otherwise, is there an easy way to see why this is true? The two facts which I thought might be useful in proving this are as follows. If $A$ is an abelian variety, $L$ is translation invariant iff $m^*L \cong p^*L \otimes q^*L$ on $A \times A$ , where $m$ is multiplication and $p,q$ the projections. If $L$ is translation invariant on $A$ , then $H^i(A, L) = 0$ for all $i> 0$ . Beyond this I am not really sure how to go about it. Any reference or help is greatly appreciated. Edit: Apparently degree does not make sense in dimensions higher than 1, so my definition of $\operatorname{Pic}^0(A)$ is not a right definition in general. The Huybrechts book mentioned before proves that (over $\mathbb{C}$ ) translation invariance of $L$ is equivalent to $c_1(L) = 0$ . Perhaps I should correct my definition of $\operatorname{Pic}^0(L)$ to be the connected component of the identity, as suggested by Cranium Clamp in the comments.","['algebraic-geometry', 'reference-request']"
4445456,Historical Mistake of Assuming Measurability,"I am recently reading about Fourier transforms and convolutions. It was a surprise to me that it takes quite several paragraphs to prove the measurability of innocent looking $f(x-y)$ (reference: proof that $\hat{f}(x,y)=f(x-y)$ is measurable if $f$ is measurable, Stein & Shakarchi Prop 3.9 ) . It makes me wonder : Does it ever happen  in history that mathematicians publish wrong results because they assumed measurability of some (innocent looking) sets or functions ?","['measure-theory', 'fake-proofs', 'analysis', 'real-analysis', 'math-history']"
4445472,Levi Civita Connections vs. Ehresmann connection,"Forgive me if I mess some of these concepts up or say something incorrect, I am still figuring out all the details of an Ehresmann connection in an associated vector bundle. So, how do we relate these two practically? Like for a two sphere I can very easily write down the levi civita connection in the standard way for the usual round metric, but if I wanted to get something equivalent by starting with a connection in the frame bundle I don't really know where to begin. Specifically, it seems we would  have a natural choice of a horizontal distribution on the frame bundle if we have imposed a metric on it (i.e. the horizontal vector fields are those such that $g(\mathfrak{gl},H)=0$ , for a metric $g$ ), but I'm not sure what other conditions I would need to get the equivalent of the levi civita connection in the tangent bundle. Or would we want to look at an orthonormal frame bundle? I don't see why we would necessitate that other than that maybe the structure group is smaller as it would be $O(n)$ instead of $GL(n)$ , but at that point why not consider the bundle of orthonormal frames with the same orientation and use $SO(n)$ ? I think I am missing something","['principal-bundles', 'connections', 'differential-geometry']"
4445473,Maximum number of different road lengths between 1000 cities,"Problem: 1000﻿ cities are connected by roads, and between any two cities there is a road. Also, for any two cities ﻿ $A$ ﻿ and $﻿B﻿$ , on any path from ﻿ $A$ ﻿ to ﻿ $B$ ﻿ passing through other cities, there is a road that is equal or shorter in length than the road from $﻿A$ ﻿ to $﻿B$ ﻿. What is the largest number of roads that can have pairwise different lengths? My attempt at solution: Let's choose any two cities $A$ and $B$ . Then let's take any other city $C$ and look at the path $A\to C\to B$ . The second sentence of a problem specifies that at least one of the roads $AC$ and $CB$ is equal in length or shorter than the direct road $AB$ . By swapping letters $A, B, C$ we get that any three cities form an isosceles triangle, in which legs are shorter or equal in length to the base (1). The first sentence of a problem suggests that cities and roads between them form a complete (weighted) graph $K_n,$ where $n=1000$ , and the question essentially asks what is the maximum number of different road lengths (or edge weights) can exist under these constraints. By considering complete graphs $K_n$ with small $n$ ( $n=3,4,5$ ) we may see that the maximum number of different edge weights (let's denote it by $L$ ) is $L=n-1$ . Let's then make a hypothesis: Hypothesis: For any $n\in\mathbb{N}, n>2$ we may choose weights for a complete graph $K_n$ that meets our conditions in a way that the maximum number of different edge weights is $L=n-1$ . Proof: Let's pick an edge that will have a minimal length and name corresponding vertices $A$ and $B$ . Since it has to satisfy condition (1), there must be at least $n-2$ edges of minimal length connected to vertices $A$ and $B$ (because there are $n-2$ vertices left). That means that there are $n-1$ edges of minimal length in total. Let's say, for simplicity's sake, that all of these edges are connected to vertex $A$ , and the only edge of minimal length connected to $B$ is coming from $A$ . (Otherwise, if there exists an edge $BC$ of minimal length  between $B$ and $C$ , in order to satisfy condition (1) there has to be at least one more edge of minimal length in triangle $BCD$ for any other vertex D.) It means that there is at least one unique length of edges. Let's then remove vertex A and all its edges form the initial graph. Now we get a complete graph $K_{n-1}$ that still meets our condition (1). By repeating all of the previous steps until no edges remain, increasing the minimal length on each iteration, we create a graph that satisfies all our requirements and has $L=n-1$ different edge lengths. OK, that means that for my initial problem the answer is at least $1000-1=999$ , however, now I am running into a problem. I cannot prove that this is, indeed, a maximum possible number of road lengths, nor can I think of the way to increase this number. However, I have a feeling that $n-1$ is actually the answer to the problem, since, again, by brute forcing small numbers of $n$ you come to the answer $n-1$ . I would really appreciate some help in proving or disproving this statement. Thanks!","['graph-theory', 'discrete-mathematics']"
4445492,Prove the following property for Euler's totient function,"I have been asked to prove that $\phi(n) > \dfrac{9n}{50}$ for all $n$ that have at most seven prime factors. I'm trying to think of how this relates to any theorems/ properties I have learnt regarding the totient function, but I have drawn a blank. Could anyone help to explain how the property holds?","['totient-function', 'discrete-mathematics']"
4445500,Norm of vector in directional derivate,"Reading about the derivatives according to a direction, I found a definition saying that the norm of the vector v in the formula $\nabla_{\mathbf{v}}{f}(\mathbf{x}) = \lim_{h \to 0}{\frac{f(\mathbf{x} + h\mathbf{v}) - f(\mathbf{x})}{h}}$ must necessarily be 1. Why is this needed? If the vector norm is different, what happens?","['partial-derivative', 'derivatives']"
4445523,Does $x_1^{a_1}+\dots+x_k^{a_k}\equiv0$ always have no solution modulo some $m$.,"Let $x_1,\dots,x_k\geq1$ be fixed integers. Does there always exist an integer $m \ge 1$ so that $$x_1^{a_1}+\dots+x_k^{a_k}\equiv 0 \pmod{m}$$ has no solution for integers $a_1,\dots,a_k\geq0$ ? The following proves the affirmative if $x_1=\ldots=x_k=:x$ . Assume without loss of generality $x\geq2$ . Consider the modulus $m=x^\ell-1$ . Let $k\geq1$ be the minimum number such that $x^{a_1}+\dots+x^{a_k}\equiv0$ has a solution modulo $m$ . Since $x^\ell\equiv1\ (\mbox{mod}\ m)$ , we can take the $a_i$ values modulo $\ell$ . If $x$ of the $a_i$ values are equal, then they could have been replaced with a single $a_i$ value that is one larger. So every $a_i$ value appears at most $x-1$ times, giving $x^{a_1}+\dots+x^{a_k}\equiv c_0+c_1x+\dots+c_{\ell-1}x^{\ell-1}\equiv0$ for some $c_0,\dots,c_{\ell-1}\leq x-1$ summing to $k\geq1$ . We find the only solution is $c_0=\ldots=c_{\ell-1}=x-1$ giving $k=\ell(x-1)$ . Thus, if $k<\ell(x-1)$ then there is no solution. If there is some prime $p$ that divides all $x_i$ but one, then there is no solution modulo $p$ . So if $k=2$ then we can assume without loss of generality that $\mbox{rad}(x_1)=\mbox{rad}(x_2)=:r$ . The following uses this to prove the affirmative if $k=2$ . If we assume $m\equiv7\ (\mbox{mod}\ 8)$ , then $2$ is a quadratic residue modulo $m$ and $-1$ is not. Due to the Chinese remainder theorem, we can furthermore assume that $m$ is not a quadratic residue modulo all odd primes $p|r$ . Due to Dirichlet's prime number theorem on arithmetic progression we can furthermore assume that $m$ is prime. By the law of quadratic reciprocity, it follows that all odd primes $p|r$ are quadratic residues modulo $m$ . It follows that $x_1^{a_1}\equiv-x_2^{a_2}$ has no solution modulo $m$ , having a quadratic residue on the left hand side but not the right hand side. To give a bit of context, I saw a video about the equation $2^a+2^b=n!$ . I decided to generalize this to $x^{a_1}+\dots+x^{a_k}=n!$ and proved it always has finitely many solutions by showing there exists $m$ such that $x^{a_1}+\dots+x^{a_k}$ is never divisible by $m$ . As a bonus, I was also wondering if the equation $$1^{a_1}+\dots+n^{a_n}=n!$$ has a solution for infinitely many values of $n$ . This is probably highly non-trivial, for example with $n=6$ we find $1^1+2^3+4^1+3^4+5^4+6^0=6!$ . Solving this is not necessary to get your answer accepted, but I am thinking of giving a bounty if you do solve it, depending on how hard it was.","['number-theory', 'abstract-algebra', 'modular-arithmetic']"
4445536,How to prove that a conserved quantity does not exist in the system $\ddot{x}=-kx-c\dot{x}$,"I would like to know how to prove that there is no conserved quantity in a system. Let $k$ and $c$ be real numbers. Below is the ODE for the damped harmonic oscillator. $$\ddot{x}=-kx-c\dot{x}$$ From a physics point of view, I would expect that no conserved quantities exist in this system, since energy is not conserved in it. The definition of a conservative quantity is a non-constant function $f:\mathbb{R}^2\to\mathbb{R}$ that satisfies $\frac{\mathrm{d}f(x(t),y(t))}{\mathrm{d}t}=0$ for all solutions $(x,y)$ of the following system. $$
\begin{aligned}
\dot{x}&=y\\
\dot{y}&=-kx-cy
\end{aligned}$$","['calculus', 'dynamical-systems', 'ordinary-differential-equations', 'real-analysis']"
4445539,"Is there an open subset $A$ of $[0,1]^2$ with measure $>\frac{1}{100}$ that satisfies this property?","Can we find for any given $\varepsilon>0$ an open subset $A\subseteq[0,1]^2$ with measure $>\frac{1}{100}$ such that, for any smooth curve $\gamma:[0,1]\to\mathbb{R}^2$ of length $1$ , the set $\gamma+A=\{\gamma(t)+a;t\in[0,1],a\in A\}$ does not contain any balls of radius $\varepsilon$ ? I wouldn't mind changing the $\frac{1}{100}$ for any other positive constant. Also, I ask about smooth curves but it may make more sense to consider in general $1$ -Lipschitz functions $\gamma:[0,1]\to[0,1]^2$ . For context, a positive answer to this question could be useful for this other question . But the question is also interesting in itself, of course.","['multivariable-calculus', 'measure-theory', 'geometric-measure-theory']"
4445546,"Generating exercises about extrema of $f(x,y)$","I can generate many examples of functions $f(x,y)$ for which finding local extrema is a good exercise for students, for example $f(x,y)=x^3+y^3-3xy$ , $f(x, y) = x^3 + 3xy^2 − 51x − 24y$ , $f (x, y) = xy + \ln y + x^2$ . The general idea of solving them is to obtain a polynomial of a small degree, for which the roots are easy to obtain, because, e.g., some of them are integer or rational. However, I do not know how to obtain nontrivial, i.e., not of the type $f(x,y)=(x-a)^2+(y-b)^2$ , examples of functions with previously known extrema. A long time ago I found a simple rule for memorizing the values of $\sin x$ for $x\in\{\pi/6,\pi/4,\pi/3,\pi/2\}$ . I did not believe that it is not generally known. Indeed, many years later I found this observation in a written source. In this case, I also believe that the mathematical society knows the general rules of generating many examples of easy but not trivial exercises, only I do not know them. Are such rules known to you?","['calculus', 'derivatives', 'polynomials']"
4445567,How to calculate this Integral which function is defined by parts,"Let $k:[0,1]\times [0,1] \to \mathbb{R}$ defined as: $k(x,y) =
\left\{
	\begin{array}{ll}
		y(1-x)  & \mbox{if } 0\leq y\leq x \le 1,\\
		x(1-y)  & \mbox{if } 0\leq x\leq y \le 1.
	\end{array}
\right.$ If we denote $u_n(x):=\sin(n\pi x), \forall n\in \mathbb{N}$ . Prove that: $\int_0^1k(x,y)u_n(y) dy=\dfrac{\sin(n\pi x)}{n^2\pi^2}.$ My attempt was integrating by parts, and see that $k(x,0)=k(x,1)=0$ and $k'(x,y)=f(y)$ i.e the derivate of $f$ doesn't deppends of the value of $x$ at all, but is defined by parts. But what I got using the previous facts was that: $\int_0^1k(x,y)u_n(y) dy= \dfrac{-x\sin(n\pi)}{n^2\pi^2}=0, $ since $n\in \mathbb{N}.$ So... What I'm doing wrong, maybe Integrating By Parts in a function defined my parts, or maybe I may use theorems about compactness or convergent sub-sequences because $(u_n)$ is bounded and k is uniformly continuous. Any idea will be appreciated.",['integration']
4445580,Show $-\sin(x)$ is a solution to the same ODE,"Let $\psi:\mathbb{R}\rightarrow\mathbb{R}$ be any function. Suppose $\cos(x)$ is a solution to the ODE $y'=\psi(y)$ over $\mathbb{R}$ . Show $-\sin(x)$ is a solution to the same ODE. Attempt: It is true that $-\sin(x)=\cos(x+\pi/2)$ , for all $x\in\mathbb{R}$ . Therefore, $$\frac{d}{dx}(-\sin(x))=\cos'(x+\pi/2)=\psi(\cos(x+\pi/2))=\psi(-\sin(x)).$$ Since $\frac{d}{dx}(-\sin(x))=\psi(-\sin(x))$ , $-\sin(x)$ must be a solution to the same ODE. There is an answer for this question at Show that satisfy the ODE . I'm wondering if my solution is 1. correct and 2. equivalent to the previous answer. The previous answer was: You could observe that $$-\sin(x) = \cos\left(\frac{\pi}{2}+x\right)$$ and that if $$y(x) = \cos(x)$$ then $y'(x) =-\sin(x) = f(\cos(x))$ and so we can say that $$y(x+\pi/2) = \cos(x+\pi/2)$$ which implies that $$y'(x+\pi/2) =-\sin(x+\pi/2) = f(\cos(x+\pi/2)).$$","['solution-verification', 'ordinary-differential-equations']"
4445607,"Evaluate $\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2}$","Evaluate the following limit: $$\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2}$$ My try: Using polar substitution - putting $x =r\cos(\theta)$ , $y = r\sin(\theta)$ , $$\implies\lim_{r \to 0} \frac{-r^2\cos(\theta) \sin(\theta)}{r^2(\sin^2\theta+\cos^2\theta)} =\lim_{r \to 0} -\cos(\theta) \sin(\theta) = -\cos(\theta) \sin(\theta)$$ Does this make any sense? Also, approaching from two different sides, $$\begin{align}\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2} & = \lim_{x\to 0}\frac{-x(0)}{x^2 + (0)^2} = 0\\\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2} & = \lim_{y\to 0}\frac{-(0)y}{(0)^2 + y^2} = 0 \end{align}$$ Both are equal... but the limit doesn't exist which I verified from Symbolab . What should I do to evaluate this?","['limits', 'multivariable-calculus']"
4445681,When are two space curves with same image equivalent?,"Let $I_1, I_2$ are two non-degenerating intervals of $\mathbb{R}$ and, let $\gamma_j : I_j \to \mathbb{R}^n,\quad j=1,2$ be two parametrized regular $\mathcal{C}^r$ -curves with same trace (image in $\mathbb{R}^n$ ) $$\gamma_1(I_1)=\gamma_2(I_2).$$ What are the necessary and sufficient conditions that guarantee the existence of a $\mathcal{C}^r$ -function $\varphi: I_1\to I_2$ such that $\varphi'(t)\neq 0$ $\gamma_1(t)=\gamma_2(\varphi(t))$ for all $t\in I_1$ ? I am trying to understand this notion of equivalent curves.","['curves', 'geometry', 'parametric', 'parametrization', 'differential-geometry']"
4445719,Combinatorics explanation of Inclusion-Exclusion Principle Exactly-$m$ Properties Formula $E_m=\sum_{j=m}^n(-1)^{j-m}\color{blue}{{j\choose m}}S_j$,"I'm trying to ""explain"" (I think this would not be a formal proof because I use a special case of the formula itself when I was ""proving"" it. So the tag I put might need to be edited.) this formula: ( $E_m$ counts those elements with exactly $m$ properties. $S_j$ is explained in the quoted text below) $$
E_m=\sum_{j=m}^n(-1)^{j-m}\color{blue}{{j\choose m}}S_j
$$ The following is the final version I just finished editing on one of my old answers. Let me cite the text below. My main focus is on the correctness of steps 3. and 4. (but any error identified is welcome!) because it's still very counterintuitive for me. Link to the original answer: https://math.stackexchange.com/a/3804796/390226 If my argument would work, I would also like to know how to turn this ""proof"" into a formal one. Thanks in advance! Terms used in the answer: $A_k$ : The set of elements has (at least) property indexed $k$ . IEP for Inclusion-Exclusion Principle. Exactly-none IEP: exactly none of the indexed properties got selected via IEP. i.e. Long answer to understand the two coefficients: Let's define $S_m$ : It's a shorthand to simplify the formula of IEP. Assume there are $n$ properties in total, and $I$ is an index set, then we can pick any $m\le n$ : $$\begin{align}
    S_m&=\sum\left|\textrm{an intersection of }m\textrm{ sets}\right|\\
    &=\sum_{|I|=m}\left|\bigcap_{j\in I}A_j\right|
    \end{align}$$ $E_0$ gives exactly-none IEP: (Let $S_0=U$ ): $$\begin{align}E_0 &=\sum_{j=0}^n(-1)^{j-0}{j\choose 0}S_j\\
&=\sum_{j=0}^{n}(-1)^{j}S_j\\
&=\left|\bigcap_{j=1}^n\bar{A_j}\right|.\\
\end{align}$$ for each $S_j$ , the coefficient is $(-1)^j$ : $$\begin{align}
    E_0&=\sum_{j=m}^n(-1)^{j-m}{j\choose m}S_j, m=0\\
       &=\sum_{j=0}^n(-1)^{j-0}{j\choose 0}S_j\\
       &=\sum_{j=0}^{n}(-1)^{j}S_j.
    \end{align}
    $$ The ${j\choose m}$ disappears in $E_0$ . But for $E_m$ , we got more than that. So maybe there are more than one exactly-none IEPs in $E_m$ ? My attempt to explain $E_m$ : $$
    E_0=\sum_{j=m}^n(-1)^{j-m}{j\choose m}S_j\\
    $$ 3.1. Observe the first term, $j=m$ . Now $S_m$ gives: $$\begin{align}
    S_m&=\sum_{|I|=m}\left|\bigcap_{j\in I}A_j\right|\\
       &=\left|A_{1,2,...,m}\right| + \left|A_{1,2,...,(m-1),(m+1)}\right| + ... + \left|A_{(n-m+1),(n-m+2),...,n}\right|\\
    \end{align}\\
    $$ 3.2. Now we calculate $E_0$ once for each term $A_{...}$ as the universe. So we have: (The notation $E_{0,U}$ where $U$ means the universe defined for the calculation) $$\begin{align}
    E_{0,A_{1,2,...,m}}&=\color{blue}{\left|A_{1,2,...,m}\right|} + \sum_{j=m+1}^n(-1)^{j-m}S_j\\
    E_{0,A_{1,2,...,(m-1),(m+1)}}&=\color{blue}{\left|A_{1,2,...,(m-1),(m+1)}\right|} + \sum_{j=m+1}^n(-1)^{j-m}S_j\\
    \vdots\\
    E_{0,A_{(n-m+1),(n-m+2),...,n}}&=\color{blue}{\left|A_{(n-m+1),(n-m+2),...,n}\right|} + \sum_{j=m+1}^n(-1)^{j-m}S_j\\
    \end{align}
    $$ Notice that these $\left|A_{...}\right|$ have been provided by $E_0$ . And the sum is $\color{blue}{S_m}$ , i.e. the (common) first term of $E_m$ . you might have many questions at this stage: Q1: Those $A_{...}$ might overlap some of the others? Yes. But if each $E_{0, A_{\dots}}$ work, no overlapping. Because $E_m$ means exactly-m properties when the calculation is complete. Q2: Each $E_0$ , ignoring those $(-1)^{j-m}$ , have $1$ for each term. Now you're trying to convince me that applying it many times will create ${j\choose m}$ , a variable for each term in the resulting $E_m$ ? By intuition, if you apply it, say $5$ times, you should have a $5$ for each term? Yes. I don't know how to explain this either for now. Now, the strangest step, I cannot believe it too: Let's try to calculate how many $S_j$ are needed when $j$ is given. This is equivalent to finding how many universes include each of them . So here is the term: $$
    {j\choose m}
    $$ given any $j$ properties, you choose $m$ of them, you find one universe, i.e. one left-hand-side on the list of 3.2. that includes it on the right-hand-side. This explains the mysterious ${j\choose m}$ of $E_m$ .","['inclusion-exclusion', 'solution-verification', 'combinatorics', 'combinatorial-proofs']"
4445750,"Existence of a minimal continuous map on $S^1 \times [0, 1]$","First please see the question: Let $X = S^1 \times [0, 1]$ . Does there exist a continuous map $f: X \rightarrow X$ satisfying, for any point $x \in X$ , $\{f^{(n)}(x)\}_{n=1}^\infty$ is dense in $X$ ? Here we denote $f^{(n)}=f \circ f \circ ... \circ f$ by the composition of $f$ for $n$ times. (EDIT: Here ""for any point"" means ""for all points"", "" $\forall$ "") Note that if $X = S^1 \times S^1$ is the torus, then the answer is ""yes"": just let $f(a, b)=(ae^{2\pi ip}, be^{2 \pi iq})$ where $p, q$ are rationally independent irrational numbers and we see that for any $x \in X$ , $\{f^{(n)}(x)\}_{n=1}^\infty$ is dense in $X$ , by the property of irrational numbers. Note also that if $X=[0, 1] \times [0, 1]$ , then the answer is ""no"". From the Brouwer's fixed point theorem we see that given any continuous $f : X \rightarrow X$ we can find a fixed point $t \in X$ , and $f^{(n)}(t)=t$ for all $n$ . I am trying to show that given a continuous $f: S^1\times [0, 1] \rightarrow S^1 \times [0, 1]$ ,  there always exists some $a \in X$ such that the closure of $\{f^{(n)}(a)\}_{n=1}^\infty$ is a closed curve but not the whole of $X$ . I don't know if this is true for all $f$ , but it works for many examples. The simplest example is as follows: let $f(r, b)=(re^{2\pi ip}, b)$ , where $p$ is an irrational number. I have not found any other clues. Maybe this question requires some knowledge in dynamical systems or algebraic topology, which is not what I am familiar with. Any hint would be greatly appreciated!","['topological-dynamics', 'ergodic-theory', 'real-analysis', 'algebraic-topology', 'dynamical-systems']"
4445759,It's true that $\mathrm{D}_{X\times Y}(M^\bullet\boxtimes N^\bullet)=\mathrm{D}_X(M^\bullet)\boxtimes \mathrm{D}_Y(N^\bullet)$ for D-modules?,"Let $X,Y$ be smooth algebraic varieties over $\mathbb{C}$ , $M^\bullet\in\mathsf{D}^b_{\text{coh}}(\mathcal{D}_X)$ , $N^\bullet\in\mathsf{D}^b_{\text{coh}}(\mathcal{D}_Y)$ , and denote by $\mathrm{D}_X$ the Verdier duality functor over $X$ . I wonder if it's true that $\mathrm{D}_{X\times Y}(M^\bullet\boxtimes N^\bullet)=\mathrm{D}_X(M^\bullet)\boxtimes \mathrm{D}_Y(N^\bullet)$ . This is true for constructible sheaves, so Riemann-Hilbert implies it at least for regular holonomic modules. (I can explain it if someone wants.) But there should be a direct proof in this generality. (I've put complex geometry as a tag because the same result should hold for D-modules over complex varieties as well and, I hope, with the same proof.)","['complex-geometry', 'algebraic-geometry', 'd-modules']"
4445770,Convergence in probability implies a.s. convergence in a countable space,"Let be $(\Omega, \mathcal{F}, \mathcal{P})$ a probability space, $(X_{n})$ a sequence of randiom variables and $X$ a random variable. Let be $\Omega$ countable and $\mathcal{F}$ a power set of $\Omega$ . Show that $X_n\xrightarrow{p} X, n \rightarrow \infty \ \ \implies X_n \xrightarrow{a.s.} X, n \rightarrow \infty$ I have done some research and I have found out that the statement is not true in general but I have no idea how to prove it for this special case. I appreciate your help in advance a lot.","['probability-limit-theorems', 'probability-theory', 'probability']"
4445778,Geodesics on a pseudosphere,"I am trying to show the following: Let \begin{equation}
\gamma(t) = \begin{pmatrix}
\frac{1}{t} \\
\sqrt{1 - \frac{1}{t^2}} + \cosh^{-1}(t)
\end{pmatrix}
\end{equation} for $t \geq 1$ be the unit-speed parametrization of the tractrix and \begin{equation}
\sigma(u,v) = \begin{pmatrix}
\frac{1}{u} \cos v \\
\frac{1}{u} \sin v \\
\sqrt{1 - \frac{1}{u^2}} + \cosh^{-1}(u)
\end{pmatrix}
\end{equation} the surface of revolution obtained by rotating the tractrix around the z-axis. Now I want to show that the geodesics on the pseudosphere (tractroid in this case) are represented in the $(u,v)$ -coordinates by arcs of circles centered on the $v$ -axis. The first thing that I noted is that the curve is not unit-speed parametrized.
So I calculated the coefficients of the first fundamental form (for a surface patches) \begin{align*}
  E(u,v) &= \dot{f}^2(u) + \dot{g}^2(u) = \frac{3 + u^2}{u^2(u^2 - 1)}, \\
  F(u,v) &= \langle \sigma_u(u,v), \sigma_v(u,v) \rangle = 0, \\
  G(u,v) &= \frac{1}{u^2} = f^2(u)
\end{align*} and now the geodesic equations are \begin{align*}
\frac{d}{dt}\left( E \dot{u} + F \dot{v} \right)  &= \frac{1}{2}\left( E_u \dot{u}^2 + 2 F_u \dot{u} \dot{v} + G_u \dot{v}^2 \right), \\
\frac{d}{dt}\left( F \dot{u} + G \dot{v} \right) &= \frac{1}{2}\left( E_v \dot{u}^2 + 2 F_v \dot{u} \dot{v} + G_v \dot{v}^2 \right)
\end{align*} therefore \begin{align*}
 \frac{d}{dt} \left(\frac{3 + u^2}{u^2(u^2 - 1)}  \dot{u} \right) &= -\frac{4 u \dot{u}^2 }{(u^2 - 1)^2} - \frac{\dot{v}^2 - 3 \dot{u}^2}{u^3}, \\
\frac{d}{dt}\left( \frac{1}{u^2}\dot{v} \right) &= 0
\end{align*} solving the second gives \begin{equation*}
  \dot{v} = C u^2
\end{equation*} which can also be obtained by clairauts theorem
where $\psi$ is the angle between $\dot{\gamma}$ and the meridians of the surface \begin{equation*}
  \dot{v} = \frac{\sin \psi}{f} = u \sin \psi
\end{equation*} and $f \sin \psi$ being constant gives \begin{equation*}
  f^2 \dot{v} = \frac{1}{u^2} \dot{v} = f \sin \psi \stackrel{!}{=} \mbox{const}.
\end{equation*} Plugging this $\dot{v}$ into the first geodesic equation leaves us with \begin{equation*}
  \frac{d}{dt} \left(\frac{3 + u^2}{u^2(u^2 - 1)}  \dot{u} \right) = \frac{\dot{u}\left(3 - \frac{4 u^4}{(u^2 - 1)^2} \right)}{u^3} - C^2 u.
\end{equation*} From this I can't figure out how the geodesics are arcs of circles centered on the $v$ -axis. So I tried using the fact that \begin{equation}
I(\dot{\gamma},\dot{\gamma}) \stackrel{!}{=} 1
\end{equation} which gives \begin{equation}
E \dot{u}^2 +  G \dot{v}^2 = 1
\end{equation} or \begin{equation}
\dot{u}^2 \frac{3 + u^2}{u^2 - 1} +  C^2 u^4 = u^2
\end{equation} thus \begin{equation}
\dot{u} = \pm \frac{\sqrt{u^2(u^2C^2 - 1)}}{\sqrt{ \frac{3 + u^2}{1 - u^2} }}.
\end{equation} From this I would get an expression for $\frac{\dot{v}}{\dot{u}}$ hence by separation of variables $(v - v_0) = (\ldots)$ where the right hand side involves an elliptic integral.
I would expect that from $\frac{\dot{v}}{\dot{u}} = F(u)$ where $F$ is some function depending only on $u$ , I can use separation of variables to get something like \begin{equation}
(v-v_0)^2 + u^2 = \frac{1}{C}
\end{equation} but from the geodesic equation and the ODE it is not possible to obtain such a form. If I would switch the sign from $\cosh^{-1}(t)$ to $-\cosh^{-1}(t)$ this would work.
I would be thankful if anybody can give me a hint.","['differential-geometry', 'ordinary-differential-equations', 'real-analysis']"
4445869,Integrating by integrating under the integral sign — the other Feynman trick?,"Having been introduced to the Feynman technique of integration , it seemed natural to wonder if it could be done the other way: Introduce a new parameter $a$ Integrate with respect to $a$ Integrate with respect to the variable $x$ Differentiate with respect to $a$ Set $a=1$ and add a constant by hand For instance, \begin{align}
\int x \cos(x) dx =& \frac{d}{da}\int \int x \cos(ax) da dx\\
=& \frac{d}{da}\int \frac{x \sin(ax)}{x}dx\\
=& \frac{d}{da}\int \sin(ax)dx\\
=& \frac{d}{da}\frac{-\cos(ax)}{a}\\
=& \frac{x \sin(ax)}{a} + \frac{\cos(ax)}{a^2}\\
=& x \sin(x) + \cos(x) + C
\end{align} And that is the right answer. But I couldn't think of any problems to solve with this method that I couldn't have done with integration by parts. So I wonder if this is actually an integration technique, and whether there are problems that are best solved with that method.","['integration', 'calculus', 'derivatives']"
4445874,Prove that $AG \parallel BE$,"As shown in the picture, $E$ is a point outside the square $ABCD$ Connect $BE,CE,DE$ , point $F$ is on line $DE$ Connect $AF$ to intersect line $DB$ at point $G$ Suppose that $DE=DB, CE=CF, AG=EB$ Prove that $AG \parallel BE$ . My thinking: It seems very difficult and complicated to prove without any analytic method. But I believe there must be a proof through pure geometric methods.Unfortunately, I have no idea about how to use the giving conditions still. P.S. This question comes from Zhihu (there no one could solve it), and according to a comment, the original source is the junior high third grade research of the junior part of Nanjing Xuanwu Senior High School. The original Chinese version:","['euclidean-geometry', 'geometry']"
4445884,Word problem in residually finite groups - enumerating normal subgroups,"I am trying to prove that the word problem is solvable for residually finite, finitely presented groups. It is known that one runs two algorithms in parallel. One algorithm stops when the word $w$ is trivial and the other when it is not. I am trying to reconstuct the second algorithm.
Literature says that the algorithm works as follows: Enumerate all finite index normal subgroups $N$ / all homomorphism to finite groups $\phi:G\to Q$ ; For every normal subgroup, check whether $w$ lies in $N$ , or check whether $w$ becomes trivial under the homomorphism $\phi$ ; If $w$ does not lie in $N$ or $\phi(w)\neq 1$ then $w$ is not trivial in $G$ . (End the algorithm.) My question is as follows: How does one enumerate all the normal subgroups or all homomorphism to all finite groups? So far I succeeded in doing these things: I can algorithmically enumerate all subgroups of index $n$ as $\text{Stab}_G(1)$ for homomorphism $G\to S_n$ , where $G$ transitively on $\{1,2,\dots n\}$ . How does one decide whether the subgroup is normal in $G$ ? It would suffice to find generators of $\text{Stab}_G(1)$ , but how does one do so? Or is there another method? It is quite important for my work that the enumeration goes from small to larger indices.","['combinatorial-group-theory', 'group-theory', 'normal-subgroups']"
4445893,Inequality involving a sequence of events in a probability space,"Given the probability space $(\Omega,\mathfrak{A},P)$ and sequence of events $A_1,A_2...A_n\in \mathfrak{A}$ how do I go about proving the inequality: $$\max \bigg\{0, \sum_{j=1}^{n}P(A_j) -n+1  \bigg\}\leq P\bigg( \bigcap_{j=1}^{n} A_j\bigg)\leq \min_{1 \leq j \leq n} P(A_j).$$ The inequality on the right-hand side makes some sense to me. If we consider $A_j$ a decreasing sequence the intersection of all the events is bound to be less than or equal to the smallest $A_j$ and so the inequality of the probability would follow from that but I'm not sure if such an assumption is too strong. We're not given whether the sequence is decreasing so I'm not sure how to proceed. As for the left hand side I'm just not sure where to begin.","['measure-theory', 'stochastic-processes', 'elementary-set-theory', 'probability-theory', 'probability']"
4445899,Terence Tao' analysis 3.4.6. Needed help with exercise specifications,"I've been reading Terence Tao's Real Analysis. Sometimes I don't understand what exactly I am supposed to do with exercise.
Exercise 3.4.6. Prove Lemma 3.4.9. (Hint: start with the set {0, 1} $^X$ and apply
the replacement axiom, replacing each function f with the object $f^{−1}($ {1} $)$ .)
See also Exercise 3.5.11. Lemma 3.4.9. Let $X$ be a set. Then the set
{ $Y$ : $Y$ is a subset of $X$ }
is a set. Axiom 3.6 (Replacement). Let $A$ be a set. For any object $x ∈ A$ , and
any object $y$ , suppose we have a statement $P(x, y)$ pertaining to x and $y$ , such that for each $x ∈ A$ there is at most one $y$ for which $P(x, y)$ is
true. Then there exists a set { $y : P(x, y)$ is true for some $x ∈ A$ }, such
that for any object $z$ , $z ∈$ { $y : P(x, y)$ is true for some $x ∈ A$ } $
⇐⇒ P(x, z)$ is true for some $x ∈ A$ . How can I prove that { $Y$ : $Y$ is a subset of $X$ } is a set? What does it even mean to prove that something is a set?","['elementary-set-theory', 'set-theory', 'real-analysis']"
4445979,How does $\frac{{\rm d}u}{{\rm d} x}\Big{|}_{x+dx} = \frac{{\rm d} u}{{\rm d} x}\Big{|}_{x} + \frac{{\rm d}^2 u}{{\rm d}x^2}{{\rm d} x}$?,"I am currently studying some equations related to a flexible beam bending problem, but I don't fully comprehend how this relation is being derived below: $$\frac{{\rm d}u}{{\rm d} x}\Big{|}_{x+dx} = \frac{{\rm d} u}{{\rm d} x}\Big{|}_{x} + \frac{{\rm d}^2 u}{{\rm d}x^2}{{\rm d} x}$$ I don't think its derived as a result of something specific to the beam bending problem, but I've attached an image below (from some lecture notes) of what the diagram shows, just in case. Below, $Q(x)$ is just the tension of the beam during an applied distributed load. . Can anyone explain to me how the differential on the left hand side is transformed into the right hand side? Maybe I'm missing something fundamental, I haven't looked at calculus in a while.","['calculus', 'derivatives']"
4445992,Cooking French Fries as a Stochastic Process?,"I always had this question since I was a kid: Suppose you place 100 french fries on to a pan over a stove For this problem, let's assume that each french fry can only have 2 ""states"" : ""face up"" or ""face down"" Each french fry needs to be cooked for 1 minute on each side - if a french fry is cooked for more than 1 minute on any side, it is considered as burnt You place the french fries on the pan and after one minute you shake the pan - some of the french fries get flipped in the air and land on the pan either ""face up"" or ""face down"", but some of the french fries never got flipped at all. After another minute has passed, you shake the pan again. For the sake of this question, let's assume that each time you shake the pan, each individual french fry has a 50% chance of getting flipped in the air, and the french fries that were flipped in the air have a 50% chance of landing ""face down"" or ""face up"". Here is the question: After 2 minutes, how many of the 100 french fries are perfectly cooked and how many french fries are burnt? How many minutes need to pass until all french fries are guaranteed to have been cooked on both sides (even though many of them will be burnt)? I tried writing some computer simulations to simulate how many french fries are burnt/perfectly cooked after ""n"" minutes, then repeat the simulation many times to try and take the average proportion of burnt/perfectly cooked fries for all these simulations ... but I was looking for a more ""mathematical way"" to solve this problem (e.g. some equation). Can this problem somehow be modelled as a Stochastic Process or using Markov Chains, and then we can derive a general formula that shows how many fries are burnt/cooked as the Markov Chain is raised to the power of ""n""? Thanks!","['stochastic-processes', 'markov-chains', 'probability']"
4446079,Proving $\lim_{x \to \infty} \frac{\sum_{n=1}^{\infty}\frac{x^n}{n!}\sqrt{x}}{\sum_{n=1}^{\infty}\frac{x^n}{n!}\sqrt{n}} = 1$ [duplicate],"This question already has answers here : How does this series scale ? (A fractional Touchard polynomial) (3 answers) Elementary asymptotics of $\sum_{k=0}^\infty \sqrt k \frac{x^k}{k!}$ as $x\to \infty$ (1 answer) Closed 2 years ago . As part of solving a problem, I am trying to prove the following identity: $$\lim_{x \to \infty }\dfrac{\sum_{n=1}^{\infty}\dfrac{x^n}{n!}\sqrt{x}}{\sum_{n=1}^{\infty}\dfrac{x^n}{n!}\sqrt{n}} = 1$$ Intuitively, I can understand why this is true. But I don't know how to make a rigorous argument to prove it. For small $n \ll x$ , $\frac{x^n}{n!}$ does not contribute to the sums as the terms with a higher power in $x$ dominate. As well, for $n \gg x$ , the denominator $n!$ dominates and the fractions become far too small. So the main contributors to the sum are the ones with $n$ at $n\approx \text{ceil}\left[x\right]$ or $n \approx \text{floor}\left[x\right]$ , at which $\dfrac{x^n}{n!}$ is maximized. Say the width of this region is $\Delta W$ . My guess at making this argument concrete, is that we have to prove that the contribution to the sum outside this interval $[x-\Delta W/2, x+\Delta W/2]$ is bounded above by an arbitrary $\epsilon/2$ , and inside the interval $\sqrt{x/n}$ differs from $1$ at most by $\epsilon/\Delta W$ .","['limits', 'convergence-divergence', 'real-analysis']"
4446085,How to show $\frac{1}{\delta} \mathbb{P}[ \sup_{t\leq s \leq t+\delta} |X(s)-X(t)|\geq \epsilon] \leq \eta$?,"If a random element $X$ of $D[0,1]$ has the property that $$\lim_{\delta\to 0} \sup_{0\leq t\leq 1-\delta} \frac{1}{\delta}\mathbb{P}(|X(t+\delta)-X(t)|\geq \epsilon) = 0.$$ for every $\epsilon >0$ , then $\mathbb{P}(X\in C[0,1])=1$ . $D[0,1]$ be the space of real functions $f$ on $[0,1]$ that are right continuous and have left limit. That is space of cadlag functions. $C[0,1]$ is set of continuous function. My solution: $$\lim_{\delta\to 0} \sup_{0\leq t\leq 1-\delta} \frac{1}{\delta}\mathbb{P}(|X(t+\delta)-X(t)|\geq \epsilon) = 0.$$ implies $\forall \ \epsilon >0$ and $\forall \ \eta>0$ , $\exists \delta\in(0,1)$ such that $$\sup_{0\leq t\leq 1-\delta} \frac{1}{\delta}\mathbb{P}(|X(t+\delta)-X(t)|\geq \epsilon) \leq \eta.......(*)$$ I have to show that 1.For each $\eta >0$ $\exists a$ such that $\mathbb{P}(|X(0)|>a)\leq \eta$ 2.For each $\epsilon >0$ and $\eta >0$ $\exists \delta \in (0,1)$ such that $\mathbb{P}(w(X,\delta) \geq \epsilon) \leq \eta$ . where $w(x,\delta) = \sup_{0\leq t \leq 1-\delta} |x(t+\delta) - x(t)|$ . First claim is trivial since $X\in D[0,1]$ hence $||x||_{\infty} < \infty$ hence the first claim. For the second claim, I want to used the theorem 7.4 from the Billingsley's book Convergence of probability measures (2nd edition) . Theorem 7.4: Suppose that $0=t_0 <t_1<....<t_v=1$ and $$\min_{i=1,...,v}(t_i - t_{i-1}) \geq \delta.$$ Then, for arbitrary x, $$w(x,\delta) \leq 3\max_{i=1,...,v}\sup_{t_{i-1}\leq s \leq t_i} |x(s)-x(t_i)|$$ and, for arbitrary $\mathbb{P}$ , $$\mathbb{P}(x| w(x,\delta)\geq 3\epsilon) \leq \sum_{i=1}^v \mathbb{P}[x| \sup_{t_{i-1}\leq s \leq t_i} |x(s)-x(t_i)|\geq \epsilon]$$ Using this theorem and $$\frac{1}{\delta} \mathbb{P}[x| \sup_{t\leq s \leq t+\delta} |x(s)-x(t)|\geq \epsilon] \leq \eta$$ . We can prove second claim. If I prove the second claim then we are able to show our desired proof using the theorem stated in the same book 1st edition (stated below). Theorem: Let $\{\mathbb{P}_n\}$ be defined on $(D[0,1], \sigma(D[0,1]))$ . Then $\{\mathbb{P}_n\}$ is tight if
1.For each $\eta >0$ $\exists a$ such that $\mathbb{P}_n(|X(0)|>a)\leq \eta \ \forall n$ and,
2.For each $\epsilon >0$ and $\eta >0$ $\exists \delta \in (0,1)$ and $n_0$ such that $\mathbb{P}_n(w(X,\delta) \geq \epsilon) \leq \eta \ \forall n>n_0$ . If a sub-sequence $\{\mathbb{P}_{n'}\}$ converges weakly to $\mathbb{P}$ , then $\mathbb{P}(C[0,1])=1$ . But I don't know how to show the $$\frac{1}{\delta} \mathbb{P}[ \sup_{t\leq s \leq t+\delta} |X(s)-X(t)|\geq \epsilon] \leq \eta$$ form $(*)$ . Other ingenious approaches are also appreciated.","['measure-theory', 'weak-convergence', 'skorohod-space', 'probability-theory', 'random-variables']"
4446145,Volumen of the intersection of a cone and a tangent cone.,"the professor of our integration class gave us the following exercise as an assignment: Determine the measure in $\mathbb{R}^3$ of $A=\{(x,y,z) \in \mathbb{R}^3 ,x^2+y^2+z^2-2x+2z\leq0\leq x^2-y^2-z^2 \}$ That is, the volumen resulting of the solid bounded by the sphere of center $(1,0,-1)$ and radius $\sqrt{2}$ and the cone $x^2=y^2+z^2$ . Due to the lack of spherical/cylindrical symmetry (at least at a first glance), I have tried to come up with a solution using cartesian coordinates. First of all, plotting the section for $y=0$ , it can be geometrically argued that $x \in [0,1+\sqrt{2}]$ . The following idea is to study the transversal sections for each value of $x \in [0,1+\sqrt{2}]$ . Fix $x \in [0,1+\sqrt{2}]$ , then the resulting section is the surface bounded by the equations $y^2+(z+1)^2=2-(z-1)^2$ and $y^2+z^2=x^2$ . Solving this system of equations on $(y,z)$ we deduce that the solution exists for $0\leq x \leq 2$ and for each such $x$ there are exactly two solutions: $(-\sqrt{2x^3-x^4},x-x^2),(\sqrt{2x^3-x^4},x-x^2)$ . Therefore, for a fixed $x \in [0,2]$ we have that $y \in [-\sqrt{2x^3-x^4},\sqrt{2x^3-x^4}]$ as the intersetion points of the circunferences are the extreme possible values for the $y$ . Then, be must set the integration limits for $z$ : it's easy to see that $z \in [-\sqrt{x^2-y^2},-1+\sqrt{2-(x-1)^2-y^2}]$ as it's bounded below by the cone and bounded above by the sphere. On the other hand, for $x \in [2,1+\sqrt{2}]$ we have no intersections in the transversal sections. Therefore, the circle corresponding to a cut of the sphere is completely inside the cut corresponding the cone. Therefore, in this case we have to integrate for each $x$ a circle of radius $\sqrt{2-(x-1)^2}$ . Summing it all up, we conclude that $$ m_3(A) = \int_0^{2} dx \int_{-\sqrt{2x^3-x^4}}^{\sqrt{2x^3-x^4}}dy \int_{-\sqrt{x^2-y^2}}^{-1+\sqrt{2-(x-1)^2-y^2}} dz + \int_2^{1+\sqrt{2}} dx \pi (2-(x-1)^2) $$ I can't find any incongruence with my reasoning and my (limited) geometric intuition makes me think I'm correct. However, as the resulting integral seems to be hard to compute, makes me think that either there's an error in my procedure or that there's a simpler/straightforward way of handling the problem. Any suggestion or objection is always welcome.","['integration', 'measure-theory', 'volume', 'iterated-integrals', 'lebesgue-integral']"
4446185,non linear differential equation that i cant solve,"Can anyone help me to solve this differential equation. Obviously this is not a linear differential equation. So the only way that I know to solve non linear differential equations is by ""manipulating"" the $y$ variable in order to become a linear differential equation.
But in this case I cant find a way. My main problem is that there is a $dy/dx$ inside and an $\ln$ outside . \begin{align}
\ y - \ln(\dot y) = x\dot y \
     \end{align}",['ordinary-differential-equations']
4446200,How many distinct ways to flatten a cube?,"Think of cutting open a cubical box with the smallest possible cuts to lay it flat. A cube has 12 edges and it seems in all the possible meshes, you have to cut along 7 edges. So, the most possible number of distinct ways to lay a cube flat should be ${12 \choose 7} = 792$ . But some of these are ruled out. For instance, you can't cut one face completely off (by choosing all 4 of its edges for cuts). So, what are the total ways to make the cuts and lay the cube flat? Some of them are shown below (the meshes with ticks are possible and the one with the cross isn't). EDIT: Per @OscarLanzi's comment, the number of distinct meshes is $11$ . But there are multiple cuts that lead to each of those meshes. I'm looking for the number of ways to make the cuts. But also interested in how one would count the distinct meshes.","['solid-geometry', 'geometry', '3d', 'meshing']"
4446247,Using SDE uniqueness in law to show that two processes have the same distribution,"Let $B$ and $\tilde{B}$ be independent standard Brownian motions defined on the same probability space with $B_0=\tilde{B}_0=0$ . Let $$X_t=e^{B_t}\int_0^te^{-B_s}d\tilde{B}_s,\hspace{1cm}Y_t=\sinh(B_t).$$ I am required to show that $X$ and $Y$ have the same law, with the question giving that I can use that SDEs with Lipschitz coefficients satisfy the uniqueness in law property. With the implication that the SDE hint gives, I used Ito's fomula (hopefully without making any computational errors) with $f(x,y)=e^xy$ to derive that $X_t$ satisfies the SDE $$dX_t=\frac{1}{2}X_tdt+X_tdB_t+d\tilde{B}_t.$$ Then the coefficients $b(x)=x/2$ and $\sigma(x)=(x,1)$ are Lipschitz and so this SDE will have uniqueness in law. I think then that I am supposed to show that $Y$ satisfies the SDE also. Using Ito's formula with $g(x)=\sinh(x)$ and the fact that $\cosh(B_t)=\sinh(B_t)+e^{-B_t}$ , I obtained that $$dY_t=\frac{1}{2}Y_tdt+(Y_t+e^{-B_t})dB_t.$$ But, defining $Z_t$ by $dZ_t=e^{-B_t}dB_t$ , we do not have that $Z$ is a BM independent of $B$ , so it does not appear that these SDEs are the same. I would greatly appreciate if someone could point me to where I've gone wrong with my efforts.","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4446264,How to solve the $p$th power of matrix $C$?,"if $p\equiv1(mod3)$ , $C=\left[
  \begin{array}{ccc}
    0 & 0 & 1 \\
    1 & 0 & -3 \\
    0 & 1 & -3 \\
  \end{array}
\right]\in M_3(\mathbb{F}_p).$ How to calculate $C^p$ ? I have tried some conventional methods, such as splitting the matrix, binomial theorem, etc. But got no result. Do you have other good solutions? Thanks for your answer.","['matrices', 'number-theory', 'finite-fields', 'minimal-polynomials']"
4446271,Fractional part and greatest integer function,"Here are a few questions on the fractional part and the greatest integer function. Find out $[\sqrt[3]{2022^2}-12\sqrt[3]{2022}]$ If $\{x\}=x-[x],$ find out $[255\cdot x\{x\}]$ for $x=\sqrt[3]{15015}.$ For question 1, using a calculator, I know that the answer is $8.$ But I really do not know how to proceed in a mathematical way. For question 2, $$255\cdot x\{x\}=255\cdot x( x-[x])=255\cdot \sqrt[3]{15015}(\sqrt[3]{15015}-24)$$ As $25^3>15015>24^3.$ So we have to find $$[255\cdot \sqrt[3]{15015}(\sqrt[3]{15015}-24)].$$ We can bound it from below as $[xy]\ge [x]\cdot [y].$ So $$[255\cdot \sqrt[3]{15015}(\sqrt[3]{15015}-24)]\ge 255[\sqrt[3]{15015^2}-\sqrt[3]{15015}\cdot 24].$$ This again looks like question 1 but I do not know how to solve it. We can upper bound it too using $[x-y]\le [x]-[y].$ Now, $$[255\cdot \sqrt[3]{15015}(\sqrt[3]{15015}-24)]=[255\cdot\sqrt[3]{15015^2}-255\cdot\sqrt[3]{15015}\cdot 24]=[255\cdot {15015^2}]-[255\cdot\sqrt[3]{15015}\cdot 24].$$ Any solutions?","['contest-math', 'elementary-number-theory', 'algebra-precalculus', 'fractional-part']"
4446296,"A question about the ring of continuous functions on $[0,1]$","I am reading Andreas Gathmann's Commutative Algebra notes. I am reading Chapter 2, and I am struggling with Exercise 2.11. ""Let $R$ be the ring of all continuous real-valued functions on the unit interval $[0,1]$ .
Similarly to Definition 0.3 (c), for any subset $S$ of $R$ we denote by $$V(S) := \{a \in [0,1] : f(a) = 0 \text{ for all } f \in S\} \subset [0,1]$$ the zero locus of S. Prove: (a) For all $a \in [0,1]$ the ideal $I_a := \{f \in R : f(a) = 0\}$ is maximal. (b) If $f_1,...,f_n \in R$ with $V(f_1,...,f_n)=\varnothing$ , then $f_1^2+···+f_n^2$ is invertible in $R$ . (c) For any ideal $I \trianglelefteq R$ with $I \neq R$ we have $V(I) \neq \varnothing$ ."" (This exercise has part (d), but I will not write it here because I haven't done it yet) I solved successfully part (a) and (b), however I am stuck  at part (c). At first, I think of proving the equivalent statement of part (c), which is ""If $V(I)=\varnothing$ then $I=R$ "". And I actually proved for a specific form of the ideal $I$ , that is $I=(f_1,\dots,f_n)$ where the functions $f_k$ have no roots over $[0,1]$ . However, since $C[0,1]$ is not a PID, or even not all of its ideals are finitely generated, I have no idea what to do to generalize my proof or to prove in some other ways. Thanks for your help.","['algebraic-geometry', 'commutative-algebra']"
4446302,"Can you, given any semigroup, define an identity element to make it a monoid","I'm wondering if I can ""make up"" an identity element, like so: I can define an element I such that any element x + I is equal to x, i.e.: I can redefine my set as [the old set] union with {I}, and redefine my binary operation such that if one of the operands is I, then the result is the other operand. Then I will have an identity element. For example, imagine the set of integers greater than 100 and addition; this forms a semigroup but has no identity. I can define a new element I (it doesn't have any ""numerical value"" but it doesn't need to). Then I say 100 + I = 100, 101 + I = 101, etc. Of course, I wouldn't be able to do multiplication or take the square root of I, but those operations aren't a part of my semigroup anyways.","['monoid', 'group-theory', 'semigroups']"
4446446,FIFO (M/D/1/N queue) Overflow probability,"I'm an engineer, trying to figure out the optimal FIFO depth for a particular application. The events I usually work with follow a Poisson distribution, and a fixed readout rate (thus the M/D/1/N naming). In an example, events arrive with an expected rate of 1MHz, and they are to be read out with a fixed rate of 1Mevents/s. If the incoming events had a fixed timing distribution (1 event every 1us) there would be no need for event storage at all, but the Poissonian nature of the process dictates that it's possible to receive multiple events in any 1-us interval: it is necessary to store (buffer) these excess events (where the local event rate exceeds 1MHz) for later readout. In this example, data is lost when the FIFO is full and I have a new event that needs to be written. Until now, I've designed FIFOs by using simulations, and adjusting their size according to the data loss that I experience. But I wanted to be able to make educated guesses to the appropriate FIFO size beforehand. I've found the following resources online, which however turned out to be of little help to me, given my poor knowledge of the topic: https://en.wikipedia.org/wiki/M/D/1_queue#Stationary_distribution https://www.physicsforums.com/threads/m-d-1-n-queueing-theory-help.600325/ Most often posts online focus on average waiting time or busy period, which I don't think apply to my problem. What I thought would work The probability of event loss can be described as (1) the probability that the FIFO is full, and (2) that I receive another event before the readout. $P_{overflow} = P_{full} * P_{1\,event\,in\,\mu}$ where $\mu$ is the readout time (1us) Now, $P_{1 event in \mu}$ is trivial, and given by the PDF of the Poisson distribution. $P_{full}$ ... is another matter. I think it can be evaluated as: $P_{full} = P_{full\,-\,1} * P_{1\,event\,in\,\mu}$ . And, thus, recursively: $P_{full} = \prod_i^{FIFO\,Depth} P_{1\,event\,in\,\mu} * P_{empty}$ Although I don't think this is actually right, and I don't know how to evaluate $P_{empty}$ so... I'm at loss. Is this the right way to proceed? I'm guessing that the matter is far more complex than I anticipated. Which road should I pursue in studying it? PS: I would eventually like to embed this in a small FIFO depth calculator in Python, so I can use SciPy primitives.","['queueing-theory', 'statistics', 'probability-theory', 'probability']"
4446451,Logic behind need for injectivity in $f(A_0\setminus A_1)=f(A_0)\setminus f(A_1)$,"Let $f:A\to B$ be a function with $A_i\subset A, i=0,1$ . I want to understand the precise logic behind the need for injectivity for $f(A_0\setminus A_1)=f(A_0)\setminus f(A_1)$ to be true. First we prove $f(A_0)\setminus f(A_1)\subset f(A_0\setminus A_1)$ since this direction does not require injectivity: Let $b\in f(A_0)\setminus f(A_1)$ . This is equivalent to $b\in f(A_0)$ and $b\notin f(A_1)\iff f^{-1}(b)\in A_0$ and $f^{-1}(b)\notin A_1$ . The latter implies, but is not equivalent to, $f^{-1}(b)\in(A_0\setminus A_1)\iff b\in f(A_0\setminus A_1).$ Thus $f(A_0)\setminus f(A_1)\subset f(A_0\setminus A_1)$ . Note here $f^{-1}(b)$ does not refer to a necessarily unique element, just an $a\in A$ such that $f(a)=b$ . So therefore $f^{-1}(b)\in A_0$ and $f^{-1}(b)\notin A_1$ only implies $f^{-1}(b)\in(A_0\setminus A_1)$ since if $f^{-1}(b)\in(A_0\setminus A_1)$ were to imply $f^{-1}(b)\in A_0$ and $f^{-1}(b)\notin A_1$ we would find in case $f(a_0)=f(a_1)=b$ with $A_0=\{a_0\}, A_1=\{a_1\}$ that $$a_0=f^{-1}(b)\in(A_0\setminus A_1)\implies a_1=f^{-1}(b)\in A_0 \text{ and } a_1=f^{-1}(b)\notin A_1$$ which is false. Now we prove $f(A_0\setminus A_1)\subset f(A_0)\setminus f(A_1)$ if $f$ is injective: Let $b\in f(A_0\setminus A_1).$ This is equivalent to $f^{-1}(b)\in (A_0\setminus A_1).$ But since $f$ is injective, the latter $f^{-1}(b)\in (A_0\setminus A_1)$ is equivalent to $f^{-1}(b)\in A_0$ and $f^{-1}(b)\notin A_1$ since $f^{-1}(b)$ is the unique $a\in A$ such that $f(a)=b.$ So we then have $b\in f(A_0)$ and $b\notin f(A_1)$ , or $b\in f(A_0)\setminus f(A_1).$ Thus $f(A_0\setminus A_1)\subset f(A_0)\setminus f(A_1)$ . So I guess my question is, is my invocation of injection for a unique $a$ really ""enough"" per se without that middle explanatory paragraph, or should I be do something like Robert Cardona does here to more directly use the injectivity condition? I use the exact same minimal justification of uniqueness of the inverse in my proof of $f(A_0\cap A_1)=f(A_0)\cap f(A_1)$ too so I'm wanting to get this right. The thing that's interesting is that injectivity turns $f^{-1}(b)$ into an element, whereas generally for non injective functions $f^{-1}(b)$ denotes a set. So therefore, you can do regular set theory containment without worrying about quantifiers when $f$ is injective.",['elementary-set-theory']
4446465,Can we control the distance between the empirical and theoretical mean on the whole trajectory any better than using Hoeffding and a union bound?,"Suppose $X,X_1,X_2,X_3\dots$ is a $\mathbb{P}$ -i.i.d. family of $[-1,1]$ -valued random variables with $\mathbb{E}[X] = 0$ . Hoeffding's inequality implies that \begin{equation*}
    \forall T \in \mathbb{N}, \forall \varepsilon > 0, \qquad \mathbb{P}\bigg[\frac{1}{T} \sum_{s=1}^T X_s \ge \varepsilon\bigg] \le \exp\Big(-\frac{T}{2}\varepsilon^2\Big),
\end{equation*} from which it follows that \begin{equation*}
    \forall T \in \mathbb{N}, \forall \delta \in (0,1), \qquad \mathbb{P}\bigg[\frac{1}{T} \sum_{s=1}^T X_s \ge \sqrt{\frac{2}{T}\log\Big(\frac{1}{\delta}\Big)}\bigg] \le \delta.
\end{equation*} Basically, this result states that the trajectory of the process $Y_t := \frac{1}{t}\sum_{s=1}^t X_s$ is below the threshold $\sqrt{\frac{2}{T}\log\Big(\frac{1}{\delta}\Big)}$ with probability at least $1-\delta$ , at any prescribed time $T$ . Instead, suppose that we wanted the whole trajectory $Y_1, Y_2,\dots,Y_T$ under the thresholds $\sqrt{\frac{2}{1}\log\Big(\frac{1}{\delta}\Big)}, \sqrt{\frac{2}{2}\log\Big(\frac{1}{\delta}\Big)}, \dots, \sqrt{\frac{2}{T}\log\Big(\frac{1}{\delta}\Big)}$ , where $T \in \mathbb{N}$ is fixed. Can we do any better than using the following union bound \begin{equation*}
    \mathbb{P}\bigg[\bigcup_{t\in\{1,\dots,T\}} \bigg\{\frac{1}{t} \sum_{s=1}^t X_s \ge \sqrt{\frac{2}{t}\log\Big(\frac{1}{\delta}\Big)}\bigg\}\bigg] \le \sum_{t=1}^T \mathbb{P}\bigg[\frac{1}{t} \sum_{s=1}^t X_s \ge \sqrt{\frac{2}{t}\log\Big(\frac{1}{\delta}\Big)}\bigg] \le \delta\cdot T ?
\end{equation*} The problem with this last inequality is that a multiplicative factor $T$ appears in the bound, and I feel that maybe, due to something in the spirit of maximal martingale inequalities, we can do much better. Any suggestion or pointer to the literature is very welcome. EDIT : I tried to perform some simulations. Taking $X_1,X_2,\dots$ as a $\mathbb{P}$ -i.i.d. family of Rademacher random variables seems to suggest that at least something of the form \begin{equation*}
    \forall T \in \mathbb{N}, \forall \delta \in \Big(0,\frac{1}{10}\Big), \qquad \mathbb{P}\bigg[\bigcup_{t\in\{1,\dots,T\}} \bigg\{\frac{1}{t} \sum_{s=1}^t X_s \ge \sqrt{\frac{2}{t}\log\Big(\frac{1}{\delta}\Big)}\bigg\}\bigg] \le \delta\cdot \log(T)
\end{equation*} holds, and even something better than this (maybe, even an upper bound of the form $O(\delta \cdot \log\log T)$ ).","['statistics', 'concentration-of-measure', 'large-deviation-theory', 'stochastic-processes', 'probability-theory']"
4446469,Mayer-Vietoris Sequence and Poincare dual,"Given a $4$ -dimensional simply connected manifold $M$ and open sets $U,V\subseteq M$ such that $U\cup V=M$ we can compute the deRham cohomology in terms of the Mayer-Vietoris sequence: \begin{align*}
0\rightarrow H^1(U)\oplus H^1(V) \rightarrow H^1(U\cap V) \xrightarrow{\delta} H^2(X) \rightarrow H^2(U)\oplus H^2(V)
\end{align*} Now assume that the intersection $U\cap V$ is homotopy equivalent to a $2$ -dimensional surface $S\subseteq M$ . My Question: Given the Poincare dual $\omega_S\in H^2(X)$ of $S$ , is there a relation between $\omega_S$ and the image of $\delta:H^1(U\cap V)\rightarrow H^2(X)$ ?","['mayer-vietoris-sequence', 'poincare-duality', 'algebraic-topology', 'differential-geometry']"
4446495,"Partial derivative, show that problem. L.H.S to R.H.S","This is a question from Advanced Calculus by David Wider. If $u=f(x,y),x=r\cos(\theta)$ and $y=r\sin(\theta)$ show that $$\frac{\partial u}{\partial x}^2+ \frac{\partial u}{\partial y}^2 = \frac{\partial u}{\partial r}^2 + \frac{\partial
 u}{\partial \theta}^2\frac{1}{r^2}$$ So far I have said the following \begin{align}&\frac{\partial u}{\partial x}=\frac{\partial u}{\partial r}\frac{\partial r}{\partial x}=
\frac{\partial u}{\partial r}\frac{1}{(\frac{\partial x}{\partial r})}=\frac{\partial u}{\partial r}\frac{1}{\cos\theta} \\
\Rightarrow 
&\frac{\partial u}{\partial x}^2=\frac{\partial u}{\partial r}^2\frac{1}{\cos^2\theta}\text{, and } \frac{\partial u}{\partial y}=\frac{\partial u}{\partial \theta}\frac{\partial \theta}{\partial y}=\frac{\partial u}{\partial \theta}\frac{1}{(\frac{\partial y}{\partial \theta})}=\frac{\partial u}{\partial \theta}\frac{1}{-r\cos\theta} \\
 \Rightarrow
&\frac{\partial u}{\partial y}^2=\frac{\partial u}{\partial \theta}^2\frac{1}{r^2\cos^2\theta}  \\
&\text{ taking the sum ( the L. H .S) we get the following }\\
&\frac{\partial u}{\partial r}^2\frac{1}{\cos^2\theta}+\frac{\partial u}{\partial \theta}^2\frac{1}{r^2\cos^2\theta}=\frac{1}{\cos^2 \theta}(\frac{\partial u}{\partial r}^2+ \frac{\partial
 u}{\partial \theta}^2\frac{1}{r^2})
\end{align} given my certainty about the fact that $\frac{1}{\cos^2 \theta}$ not equalling 1 can say I have made a mistake could someone please point it out or provide a complete solution that would be apricated. I suspect my mistake might be around my inversion of the partial derivatives but i am not sure.","['partial-derivative', 'derivatives', 'partial-differential-equations']"
4446513,Example of linear algebraic group which has unique rational point,"Sorry for my bad English. For a field $k$ , I want to find examples of linear algebraic groups over $k$ , which has a unique rational point (it is just unit element $e$ ). Of course  trivial group is one such example, so I want to know non-trivial such groups especially over $k=\mathbb{R}$ .","['group-schemes', 'algebraic-geometry']"
4446515,A gamma summation: $\sum_{n=0}^{\infty} \frac{2}{\Gamma ( a + n) \Gamma ( a - n )} = \frac{2^{2a-2}}{\Gamma ( 2a - 1 )} + \frac{1}{\Gamma^2 (a)}$,"Let $a \notin \mathbb{Z}$ and $a \neq \frac{1}{2}$ . Prove that $$\sum_{n=0}^{\infty} \frac{2}{\Gamma \left ( a + n \right ) \Gamma \left ( a - n \right )} = \frac{2^{2a-2}}{\Gamma \left ( 2a - 1 \right )} + \frac{1}{\Gamma^2 (a)}$$ Attempt Using the fact that \begin{align*} 
\frac{1}{\Gamma\left ( a+x \right ) \Gamma \left ( \beta - x \right )} &= \frac{1}{\left ( a+x-1 \right )! \left ( \beta-x-1 \right )!} \\ &=\frac{1}{\Gamma \left ( a + \beta - 1 \right )} \frac{\left ( a + \beta-2 \right )!}{\left ( a + x -1 \right )! \left ( \beta - x -1 \right )!} \\ &=\frac{1}{\Gamma \left ( a + \beta - 1 \right )} \binom{a + \beta - 2}{a + x -1} 
\end{align*} the question really boils down to the sum $$\mathcal{S} = \sum_{n=0}^{\infty} \binom{2a-2}{a+n-1}$$ To this end, \begin{align*}
 \sum_{n=0}^{\infty} \binom{2a-1}{a+n-1} &=\frac{1}{2\pi i} \sum_{n=0}^{\infty} \oint \limits_{|z|=1} \frac{\left ( 1+z \right )^{2a-1}}{z^{a+n}}\, \mathrm{d}z \\ 
 &= \frac{1}{2\pi i} \oint \limits_{|z|=1} \frac{\left ( 1 + z \right )^{2a-1}}{z^a} \sum_{n=0}^{\infty} \frac{1}{z^n} \, \mathrm{d}z  \\ 
 &= \frac{1}{2\pi i} \oint \limits_{|z|=1} \frac{\left ( 1+z \right )^{2a-1}}{z^{a-1} \left ( z-1 \right )} \, \mathrm{d}z
\end{align*} using the handy identity $\displaystyle \binom{n}{k} = \frac{1}{2\pi i } \oint \limits_{\gamma} \frac{\left ( 1+z \right )^n}{z^{k+1}} \, \mathrm{d}z$ . I think I'm on the right track, but I'm having a difficult time evaluating the last contour integral. Any help?","['complex-analysis', 'sequences-and-series', 'special-functions', 'real-analysis']"
4446520,Asymptotic integration of a function,"The function $$\int_0^{\frac{\pi }{2}} \exp \left\{-\frac{1}{2}\left [ \sigma ^2 \left(\cos ^2(\theta )+\frac{1}{\cos ^2(\theta )}-2\right)+\frac{x^2 \cos ^2(\theta )}{\sigma ^2}+2 x \left(1-\cos ^2(\theta )\right) \right ] \right\} \, d\theta=\frac{\sqrt{2 \pi } \sigma }{2  R }$$ can be interpreted as the implicit definition of the function $x=f(R,\sigma )$ , where $R$ and $\sigma$ are positive parameters. Limiting the attention to $x>0$ , is it possible to find an explicit expression for $x$ valid near $\sigma=0$ . Thanks in advance.","['integration', 'implicit-function', 'functions']"
4446559,Density of CLT functions for ergodic dynamical systems,"Assume that $ ( X , \mathcal A , \mu ) $ is a probability space, $ T : X \to X $ is an ergodic transformation, and $ f \in L ^ 2 ( \mu ) $ is such that $ \int f \ \mathrm d \mu = 0 $ and $ \frac { S _ m ( f ) } { \| S _ m ( f ) \| } $ converges in distribution to $ \mathcal N ( 0 , 1 ) $ . Here, $ \mathcal N ( 0 , 1 ) $ is the standard normal distribution, $ \| . \| $ is the $ L ^ 2 ( \mu ) $ norm and $ S _ m ( f ) = \sum _ { i = 0 } ^ { m - 1 } f \circ T ^ m $ . Prove that for any $ \phi \in L ^ 2 ( \mu ) $ , $ \frac { S _ m ( f + \phi \circ T - \phi ) } { \| S _ m ( f + \phi \circ T - \phi ) \| } $ also converges in distribution to $ \mathcal N ( 0 , 1 ) $ . This problem comes from studying the Central Limit Theorem (CLT) for dynamical systems. In "" On the Central Limit Theorem for Dynamical Systems "" by Burton and Denker, the authors claim without proof that if the function $ f \in L ^ 2 ( \mu ) $ satisfies CLT, then for any functions of the form $ \phi \circ T - \phi $ for some ergodic $ T $ , $ f + \phi \circ T - \phi $ also satisfies CLT. Consequently, the functions satisfying CLT are dense in the subspace of the functions in $ L ^ 2 ( \mu ) $ with zero integral, because functions of the form $ \phi \circ T - \phi $ are dense in that subspace. I can observe that $ S _ m ( f + \phi \circ T - \phi ) = S _ m ( f ) + \phi \circ T ^ m - \phi $ . As $ T $ is ergodic, the fraction $ \frac { S _ m ( f ) + \phi \circ T ^ m - \phi } m $ converges almost sure to $ 0 $ , by Birkhoff ergodic theorem. But I can't replace the $ m $ in the denominator by $ \| S _ m ( f ) + \phi \circ T ^ m - \phi \| $ . I'm familiar with the proof of CLT in case of an i.i.d.; but in this case $ ( f \circ T ^ m ) _ m $ is not an i.i.d. sequence. I can't figure out how to approach this problem. Burton, Robert; Denker, Manfred , On the central limit theorem for dynamical systems , Trans. Am. Math. Soc. 302, 715-726 (1987). ZBL0628.60030 .","['ergodic-theory', 'probability-theory', 'central-limit-theorem', 'dynamical-systems']"
4446587,Understanding difference between a distinguished boundary and 'normal' boundary in several complex variables,"I am reading through Tasty Bits of Several Complex Variables and I come across the term distinguished boundary . It seems a distinguished boundary is different from a normal boundary as the author explains the latter has fewer dimensions than the latter. What is also confusing me is the image of the bidisk where I can't tell the difference between the distinguished boundary and the normal boundary? Is it that the distinguished boundary is illustrated as the bold lines/edges of the image and while the boundary cannot be fully represented on a 2D paper? Does the boundary include the shaded grey area whilst the distinguished boundary does not? Also the union of inside and outside of the donut confuses more. I feel i'm being short sighted here, can someone correct my vision?","['complex-analysis', 'several-complex-variables', 'manifolds-with-boundary']"
4446594,Finding points on a parametric curve where curvature changes,"I am a engineer working on Wankel motors, where a simil-Reuleaux triangle rotates eccentrically in a 8-shaped form: (from Wikipedia) Fascinated by this mechanism, I was studying the meaningful points out of sheer curiosity. The parametric equations to define the epitroch (the external 8-shape) are for eccentricity $f$ and $0\leq t < 2 \pi$ : $$x = \cos(t)+f \cos(3t)$$ $$y = \sin(t)+f \sin(3t)$$ First and second derivatives of the parametric equations return respectively the red and fuchsia points ( $f=0.2$ in my drawing): QUESTION: Is it possible to calculate analytically the 4 blue points where the curve changes curvature? I was wrongly assuming they were given by the second derivative, but it is not the case. I am in a cul-de-sac and perhaps I only need to find another hobby.","['calculus', 'parametrization', 'geometry', 'curvature']"
4446631,Does every element in a well-ordered set have a successor element?,"I am currently reading Faticoni's The Mathematics of Infinity (2nd edition), and I think there is an error in his writing. He defines a well-ordered set by the following: Definition 6.1.1 Let $A$ be a nonempty set. We say that $A$ is a well-ordered set if it satisfies the following two properties: $A$ satisfies the Trichotomy Property. That is, given $x, y \in A$ then $x$ and $y$ satisfy exactly one of the following options, $x < y$ , $y < x$ or $x = y$ . $A$ satisfies the Minimum Property. That is, each nonempty subset of $A$ contains a unique least element. Equivalently, to each element $x \in A$ , there is a unique element $x^+ \in A$ such that (given $y \in A$ such that $x < y$ , then $x^+ \leq y$ .) We call $x^+$ the successor of $x$ . The problem is that in condition 2, I don't think the two properties are equivalent; there can be a greatest element in the set which has no successor. Later, he argues that the set $A = \{\emptyset, \{a\}, \{a, b\}, \{a, b, c\}\}$ is not a well-ordered set because $\{a, b, c\}$ does not have a successor element. But I think this is wrong since $A$ satisfies both the Trichotomy Property and the Minimum Property so $A$ should be well-ordered? I feel like this is such a big error that reading this book further won't be possible without resolving it. Am I missing something obvious?",['elementary-set-theory']
4446755,$\mathbf{E} |\xi_n - \xi|^a \to 0 \Rightarrow \mathbf{E} \xi_n^a \to \mathbf{E} \xi^a$.,"Let $\xi_n$ , $\xi$ be nonnegative random variables. Prove or disprove that if $$\mathbf{E} \lvert \xi_n - \xi \rvert^a \to 0 \quad \text{as } n\to \infty$$ for all $a \in (0, \infty)$ , then $$\mathbf{E} \xi_n^a \to   \mathbf{E} \xi^a$$ I know that if $a \in \mathbb{N}$ then the statement is true, but I don't know how to prove it and I don't know anything about the case when $a$ is arbitrary. The following is my attempt. If $a=m \in \mathbb{N}$ we have \begin{align}
\lvert \mathbf{E} \xi_n^m - \mathbf{E}  \xi^m \rvert &= \lvert \mathbf{E} (\xi_n - \xi)(\xi_n^{m-1} - \xi_n^{m-2}\xi + \ldots ) \rvert \\
&\le \sqrt{\mathbf{E} (\xi_n - \xi)^2 \mathbf{E}(\xi_n^{m-1} - \xi_n^{m-2}\xi + \ldots )^2} \\
&\to 0
\end{align} because $\mathbf{E}(\xi_n^{m-1} - \xi_n^{m-2}\xi + \ldots )^2$ is bounded. It is bounded because it is a finite sum of terms $\mathbf{E}\xi_n^i \xi^j$ and $$\lvert \mathbf{E}\xi_n^i \xi^j \rvert \le \sqrt{\mathbf{E}\xi_n^{2i} \mathbf{E}\xi^{2j} }$$ So it's sufficient to show that $\mathbf{E}\xi_n^{2i}$ is bounded for every $i$ .","['probability-theory', 'probability', 'sequences-and-series']"
4446783,Are characteristic functions always differentiable?,"I am thinking about the statement if the characteristic function of a random variable $X$ , $\Phi_X$ , is always differentiable. By definition, $$\Phi_X(t)=\int_{\Bbb{R}^d}e^{i\langle t,y \rangle}P_X(dy)$$ Hence, I think it has something to do with changing the integral and the derivative right?
But my intuition tells me that there is a counterexample but I can't find one.","['characteristic-functions', 'stochastic-calculus', 'derivatives', 'probability-theory', 'probability']"
4446859,What are the speeds of a bus and bike if they are heading in opposite directions? Unsure on how to go about solving for speed here,"Cities $A$ and $B$ are $70$ miles apart.  A biker leaves City $A$ at the same time that a bus leaves City $B$ .  They travel toward each other and meet $84$ minutes after their departure at a point between $A$ and $B$ .  The bus arrives at City $A$ , stays there for $20$ minutes, and then heads back to City $B$ .  The bus meets the biker again $2$ hours and $41$ minutes after their first meeting.  What are the speeds of the bus and the biker? So far, I’ve created 2 variables $V_{\text{bus}}=$ bus speed and $V_{\text{bike}}=$ bike speed.  Where they meet for the first time, I’ve marked that as distance $C$ .  From this I got $1.4 \text{ hrs} = C/{V_{\text{bike}}} = (70–C)/V_{\text{bus}}$ . To describe the time in which the bus begins heading back to City $B$ , I did $(70/V_\text{bus})+1/3$ hrs which means the bike has traveled $V_\text{bike}((70/V_\text{bus})+1/3)$ by the time the bus leaves again.  This is all I have so far but I’m still unsure if I’m even heading in the right direction. If it helps at all, the textbook I’m working out of has the answers as $35$ m/h and $15$ m/h but has no work.","['algebra-precalculus', 'arithmetic']"
4446879,Definition of Schauder basis,"I have a definition of a Schauder basis but I’m unsure of it. The definition I have is A sequence $\{e_k : k \in \mathbb{N} \}$ in a normed space $(V, \| \cdot \| )$ is a Schauder basis if $\sum_{k=1}^{\infty } \alpha _k e_k =0 $ implies $\alpha _k =0 $ for all $k$ . every $x \in V$ can be written in the form $x=\sum_{k=1}^{\infty } \alpha _k e_k $ (i.e. $ \lim_{n \to \infty }\| (\sum_{k=1}^{n} \alpha _k e_k ) -x\|=0$ This definition to me seems to mean that the Schauder basis is countable. However a theorem is ‘an infinite dimensional Hilbert space is separable if and only if it has a countable orthonormal basis’ . This theorem seems to contradict that a Schauder basis has to be countable. What’s the deal here?","['hilbert-spaces', 'schauder-basis', 'functional-analysis']"
4446882,Can germs be defined as a quotient of vector spaces?,"Summary: Let $M$ be a smooth manifold and $p\in M$ . I know of two notions of germs of functions at $p$ , the more restrictive of which can be written as a quotient vector space. I am curious whether the more general notion can also be written as a quotient vector space. Let $U$ be a neighborhood of $p$ , let $\mathscr C^\infty(U)$ denote the set of smooth functions from $U$ to $\mathbb R$ , and let $I_p(U)\subseteq\mathscr C^\infty(U)$ denote the subspace of functions that vanish in a neighborhood of $p$ . Then, as is noted here , the space of germs of functions on $U$ at $p$ can be defined as $$C^\infty_p(U)=\frac{\mathscr C^\infty(U)}{I_p(U)}.$$ This is a nice definition because the resulting space is automatically a vector space. However, the restriction to $U$ is undesirable. If $M$ were an analytic manifold, we might want smooth functions on some neighborhood of $p$ to have an associated germ, regardless of whether the function extends to the whole manifold. Let $\mathscr C^\infty_p$ denote the set of pairs $(f,U)$ , where $U$ is a neighborhood of $p$ and $f:U\to\mathbb R$ is smooth. Define the equivalence relation $(f,U)\sim_p(g,V)$ if $f$ and $g$ are identical on a common neighborhood of $p$ . Then we can define $$C^\infty_p=\mathscr C^\infty_p\big/\sim_p.$$ It turns out that $C^\infty_p$ is a vector space when we define addition and scalar multiplication in a natural way: $[(f,U)]+[(g,V)]=[f|_{U\cap V}+g|_{U\cap V},U\cap V]$ and $a[(f,U)]=[(af,U)]$ . But it takes some extra work to show that these operations are well defined and that the vector space axioms are satisfied. Moreover, this seems like a coincidence, because $\mathscr C^\infty_p$ does not have an obvious vector space structure. Can we define $C^\infty_p$ as a quotient of two vector spaces, in a similar way to $C^\infty_p(U)$ ?","['smooth-functions', 'smooth-manifolds', 'germs', 'quotient-spaces', 'differential-geometry']"
4446927,Image of the exponential map on the 2-Sphere at a point,Take the north pole of the standard 2-sphere in $\mathbb{R^3}$ equipped with the riemannian metric induced by the standard metric on $ \mathbb{R^3}$ . What is the image of the exponential map of the tangent space at the north pole? I am having trouble with the definition of the exponential map making it hard for me to compute this. Thanks for any help!,"['riemannian-geometry', 'differential-geometry']"
4446953,Help with understanding the Gram-Schmidt Process,"Let $U=\langle x_1,x_2,x_3\rangle \subseteq \mathbb{R^4},$ where $$x_1=\begin {pmatrix} 3\\4 \\0\\0
\end {pmatrix}, \ x_2=\begin {pmatrix} 1\\3 \\1\\1 \end {pmatrix},\ x_3=\begin {pmatrix} 0\\5 \\5\\7 \end {pmatrix}.$$ Use the Gram-Schmidt Process to determine an orthogonal basis of $U$ with respect to the bilinear form given by the identity $I_4\in \mathbb{R^{4\times4}}$ . I think I understand the process at hand ok enough but I'm unsure of my work and need someone to check it. First I look at the Gram Matrix which is just the $4\times4 $ identity matrix. Then based on the definition of the Gram matrix I turn it into $ \mathfrak{b}(v_i,y_i)= v_1y_1+v_2y_2+v_3y_3+v_4y_4.$ Then I start looking at the actual process: $$B'=\langle w_1,w_2,w_3\rangle \text{ where } w_1= \begin {pmatrix} 3\\4\\0\\0 \end {pmatrix} $$ $$w_2=x_2-\frac{\mathfrak{b}(x_2,w_1)}{\mathfrak{b}(w_1,w_1)}w_1 \implies w_2= \begin {pmatrix} 1\\3\\1\\1 \end {pmatrix} - \frac{(1 \times3)+(3\times4)+(1\times0)+(1\times0)}{(3\times3)+(4\times4)+(0\times0)+(0\times0)}\begin {pmatrix} 3\\4\\0\\0 \end {pmatrix}.$$ $$w_2= \begin {pmatrix} -4/5\\3/5\\1\\1 \end {pmatrix} $$ $$
w_3= \begin {pmatrix} 0\\5\\5\\7 \end {pmatrix} - \frac{(5\times4)}{25}\begin {pmatrix} 3\\4\\0\\0 \end {pmatrix}- \frac{(0)+(3)+(5)+(7)}{(16/25)+(9/25)+1+1} \begin {pmatrix} -4/5\\3/5\\1\\1 \end {pmatrix}$$ $$w_3= \begin {pmatrix} 8/5\\-6/5\\0\\2 \end {pmatrix},\ B'= \langle w_1,w_2,w_3\rangle.$$ What I'm most particularly unsure about is whether or not I had to turn the Gram matrix into a bilinear form and also I'm just generally unsure about what I've turned it into as well. I suspect I should actually be doing vector-matrix multiplication or something but I don't really know.","['orthogonality', 'solution-verification', 'linear-algebra', 'gram-schmidt']"
4446954,Connecting $\sqrt{i \sqrt{i \sqrt{i \sqrt{i \dots}}}}$ to an infinite process?,"I just watched this YouTube video by Michael Penn about the expression $$\sqrt{i \sqrt{i \sqrt{i \sqrt{i \dots}}}}$$ and how to evaluate it. At the end of the video, he mentions that if we do not interpret square roots as just being principal square roots, then the solutions take the form $$
e^{\frac{i \pi}{2} + i \pi \bigl( n_0 + \frac{n_1}{2} + \frac{n_2}{4} + \frac{n_3}{8} + \cdots \bigr)}
$$ where $n_0, n_1, n_2, ... \in \{0, 1\}$ . Because all real numbers in $[0, 2)$ can be expressed as binary numbers of the form $b_0.b_1b_2b_3b_4b_5\ldots\,$ , this means that solutions to the original nested radical work out to ""any complex number of modulus $1$ ."" (However, if we do restrict ourselves to the principal square root, we'd get back a single answer .) I'm trying to interpret exactly what this claim means and was wondering if the following line of reasoning works. Let's start with $\sqrt{i}$ . This means ""a number that, if squared and divided by $i$ , is equal to $1$ ."" There are two such numbers. Now, let's try $\sqrt{i\sqrt{i}}$ . This means ""some number that, if squared and divided by $i$ , then squared and divided by $i$ again, gives $1$ ."" Let's then try $\sqrt{i\sqrt{i\sqrt{i}}}$ . That would mean ""some number that if squared and divided by $i$ , then squared again and divided by $i$ , then squared again and divided by $i$ , you get $1$ ."" And more generally, any finite expansion of the nested radical can be interpreted as ""some number that, if repeated squared and divided by $i$ a total of $n$ times, gives $1$ ."" For starters - is this a correct way of thinking about the partial terms of the series? Assuming that this is the case, I'm struggling to make sense of what the infinitely nested radical would mean. The above process-based formulation breaks down if you repeat this process infinitely many times, since at a first reading it would mean ""a number where, if you square and divide by $i$ infinitely many times, yields $i$ ."" That doesn't make sense to me, nor does it feel like a legal strategy for extending the finite case to a limit. Rather, it seems like what's going on here is that if you repeatedly increase the number of iterations of ""square and divide by $i$ ,"" you start filling up more and more of the complex unit circle. And the infinite limit working out to ""all complex numbers of modulus $1$ "" then would mean something to the effect of the following: Let $z$ be an arbitrary complex number of modulus $1$ . Then by repeatedly squaring $z$ and dividing by $i$ , we can make the number get as close to $1$ as we'd like. Is this a reasonable conclusion to draw? Or is this not an appropriate way of thinking about the infinitely nested radical? Thanks!","['limits', 'recreational-mathematics', 'complex-numbers']"
4446963,Weak Compactness Theorem for Borel Measures,"Weak convergence is defined as follows: The sequence $\{\mu_j\}$ of Borel measures on $\Bbb R^n$ converges weakly to a Borel measure $\mu$ if for all $f\in C_0(\Bbb R^n)$ , $$\int f d\mu_j \to \int f d\mu$$ After this, they state a certain weak compactness theorem , which is supposed to follow from the separability of $C_0(\Bbb R^n)$ . Theorem. Any sequence $\{\mu_j\}$ of Borel measures on $\Bbb R^n$ satisfying $$\sup_j \mu_j(\Bbb R^n) < \infty$$ has a weakly converging subsequence. I'm trying to prove the above theorem. Suppose $\{\mu_j\}$ is a sequence of Borel measures on $\Bbb R^n$ satisfying $\sup_j \mu_j(\Bbb R^n) < \infty$ . We must find a weakly convergent subsequence $\{\mu_{j_k}\}$ , i.e., $$\int fd\mu_{j_k} \xrightarrow{k\to\infty} \int f d\mu$$ for some Borel measure $\mu$ , and all $f\in C_0(\Bbb R^n)$ . I don't have much clue where to begin; could I please get any suggestions? Thanks a lot! Reference: Fourier Analysis and Hausdorff Dimension by Pertti Mattila.","['borel-sets', 'measure-theory', 'borel-measures']"
4446974,Speed of Convergence for some series (double sum),"For $\alpha>2$ , i want to find the speed of convergence of $$S_n=\sum_{i=1}^n\sum_{j=n-i+1}^n i^{-\alpha}j^{-\alpha}.$$ In particular, i want $\sum_{i=1}^n\sum_{j=n-i+1}^n i^{-\alpha}j^{-\alpha}\leq Cn^{-\alpha+1-\epsilon}$ for some $C,\epsilon>0$ . My first attempt was to estimate by integrals $$\sum_{i=1}^n\sum_{j=n-i+1}^n i^{-\alpha}j^{-\alpha}\leq \int_{x=1}^n\int_{y=n-x+1}^n x^{-\alpha}y^{-\alpha}dxdy\\=\int_{x=1}^nx^{-\alpha}(n^{-\alpha+1})dx+\int_{x=1}^nx^{-\alpha}(n-x+1)^{-\alpha+1}dx$$ but the second integral on the left is not an easy one. My second attempt was to show that $$n^{\alpha-1+\epsilon}\sum_{i=1}^n\sum_{j=n-i+1}^n i^{-\alpha}j^{-\alpha}$$ is bounded.","['asymptotics', 'analysis', 'real-analysis', 'sequences-and-series', 'riemann-zeta']"
4447032,"Exercise 6.A.17 in ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. I am worried if my solution is ok.","I am reading ""Linear Algebra Done Right 3rd Edition"" by Sheldon Axler. 6.A.17 Prove or disprove: there is an inner product on $\mathbb{R}^2$ such that the associated norm is given by $$||(x,y)||=\max\{x,y\}$$ for all $(x,y)\in\mathbb{R}^2$ . I solved this exercise but I am worried if my solution is ok because this exercise appears to be unnaturally too easy. My solution is the following: If there is an inner product on $\mathbb{R}^2$ such that the associated norm is given by $$||(x,y)||=\max\{x,y\}$$ for all $(x,y)\in\mathbb{R}^2$ , then $0<||(-1,-1)||=\max\{-1,-1\}=-1$ . This is a contradiction.","['inner-products', 'solution-verification', 'linear-algebra', 'normed-spaces']"
4447033,How to make $\int_0^\infty \delta(x) dx = \frac12 $ rigorous using generalized functions?,"There are many versions of the theory on generalized functions. The most famous one is the distribution theory of Schwartz, where test functions are smooth and compactly supported. In the Schwartz's theory, the expression $$\int_0^\infty \delta(x) dx = \langle\delta, \chi_{[0,\infty)}\rangle,$$ where $\chi$ is the indicator function, is undefined since $\chi_{[0,\infty)}$ is not smooth. However, the prescription(?) of setting $\int_0^\infty \delta(x) dx = \frac12 $ is very useful in physics, and I want to make this expression rigorous. Another famous theory of generalized function is the hyperfunction theory of Sato. Hyperfunctions can be regarded as differences of boundary values of two holomorphic functions on upper and lower half plane. As far as I know, the integral $$\int_a^b f(x) dx$$ of a hyperfunction $f$ is defined only if $f$ is real analytic on two endpoints $a$ and $b$ . However, the hyperfunction $\delta(x)$ is not real analytic at 0, and the integral $\int_0^\infty \delta(x) dx$ is not defined. Is there any way to make rigorous meaning to $\int_0^\infty \delta(x) dx$ ?","['complex-analysis', 'distribution-theory']"
4447104,Volume of a triangular prism with 2 different bases,"How do I arrive at a formula to calculate the volume of the following 3D shape? Does this shape have a proper name? It kind of looks like an irregular triangular prism with 2 similar triangles as bases. The base edf is bigger in this example, but could be smaller, too. All edges are different sizes and their lengths are known. Angles α and β are known too. Bases are parallel to each other. ab , df , gf , ga are all right angles. Couldn't find a formula for a shape like this. I tried subdividing ot into other 3D shapes with known volume formulas, but I wasn't able to infer the sizes of all the necessary dimensions to plug into their volume formulas. Any ideas? Can / should this be solved using integral calculus or algorithmically, since this can be viewed as the abc triangle riding on the g edge and linearly increasing / decreasing in size until it becomes bfe ?..","['volume', 'triangles', 'geometry', '3d']"
4447109,Upper bound of $\sum_{n=1}^N |1-z^n|$ where $|z| \leq 1$,How to derive an upper bound of $$\sum_{n=1}^N |1-z^n|$$ where $z\in\mathbb{C}$ and $|z| \leq 1$ ? A trivial upper bound would be $2N$ since each $|1-z^n| \leq 2$ . But I am hoping for tighter bounds. I ran some numerical experiments and believe the bound should be $\frac{3}{2}N$ but don't know how to prove it.,"['complex-analysis', 'power-series', 'complex-numbers', 'sequences-and-series']"
4447142,How the Frobenius behaves under the change of base field,"Let $k'/k$ be an extension of finite fields, $X$ be a scheme over $k'$ and thus over $k$ . Then $X\otimes_{k} \bar{k}$ is $n = [k':k]$ disjoint union of $X\otimes_{k'} \bar{k}$ . How the Frobenius of the former can be expressed in the latter? Milne's book on etale cohomology claims that:
for a sheaf $E$ on $X_{et}$ , denote $W = H_c^r(X\otimes_k \bar k,\bar E)$ , $V = H_c^r(X\otimes_{k'} \bar {k},\bar E)$ , then $(W,F) = (V^n,F_n')$ , where \begin{equation}
  F_n'(v_1,\dots, v_n) = (F'v_n,v_1,\dots,v_{n-1}),
\end{equation} and $F'$ is the Frobenius map on $X/k'$ . I don't know why this is true. I want to express $\mathrm{id}_X \otimes_k F_{\bar k/k}$ on $X\otimes_{k} \bar{k}$ in terms of $X\otimes_{k'} \bar{k}$ under the identification \begin{align*}
    X \times_{k'} G_{k'}
    = X\times_{k'} k' \times_k k'
    = X\times_k k'
\end{align*} where $G = \operatorname{Gal}(k'/k)$ . But it is just hard to make it clear why a single $F'$ shows up.","['galois-theory', 'algebraic-geometry', 'etale-cohomology']"
