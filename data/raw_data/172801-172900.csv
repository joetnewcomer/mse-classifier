question_id,title,body,tags
3071218,calculate flux through surface,"I need to calculate the flux of the vector field $\vec{F}$ through the surface $D$ , where $$\vec{F} = \left<z, \, y \sqrt{x^2 + z^2}, \, -x \right> \\
D = \{x^2+6x+z^2\le 0 \,| -1\le y \le 0\}.$$ So there should be a cylinder (height on $y$ axis) shifted by 3 units (center $x=-3$ ), with cut on $ -1\le y \le 0$ . Cylindrical parametrization: $$\begin{cases}
x=r\cos\theta -3\\
y=y\\
z=r\sin\theta
\end{cases}
$$ I think I can do this problem in two ways: The first one by calculating the flux for each of the 3 surfaces (1 cylinder, 2 disks), and the second one by using the divergence theorem. $$\operatorname{div}F = \sqrt{x^2+y^2} \overset{\text{cylindrical}}{\underset{\text{coordinates}}{=}} \sqrt{r^2-9-6r\cos\theta},$$ so with the divergence theorem, $$\int_{0}^{2\pi}\int_{0}^{3\sqrt{3}}\int_{-1}^{0}{r\sqrt{r^2-9-6r\cos\theta} \, dy \, dr \, d\theta}.$$ Is this correct?","['integration', 'multivariable-calculus', 'surface-integrals', 'vector-analysis']"
3071234,Why does the support must be closed?,"Apparently it is important that the support is defined as the closure of $\{f \neq 0\}$ . Because of that condition globalization is allowed as the exercise below indicates. However, I have no idea why it is so important that the support is defined as the closure of $\{f \neq 0\}$ ? The exercise: Let ( X, $\mathcal{T}$ ) be a topological space, U $\subset$ X open and $\eta$ $\in$ C(X) , ( C(X) is the space of continuous functions on X ) supported in U . Then, for any continuous map $g:U \rightarrow \mathbb{R} $ , $(\eta \cdot g): X \rightarrow \mathbb{R}$ , $(\eta \cdot g)(x) = \eta (x)g(x)$ if $x \in U$ and $(\eta \cdot g)(x) = 0$ if $x \notin U$ is continous. Show that this statement fails if we only assume that $\{f \neq 0\} \subset U$ . I have been able to show that the map $g : U \rightarrow \mathbb{R} $ is continuous. However, I still don't understand the importance of the closure and why the map otherwise isn't continuous. Can anyone help me?",['general-topology']
3071292,When is the blow up morphism flat?,"The question is as in the title: given a scheme $X$ and a closed subscheme $Z$ when is it true that the blow up morphism $Bl_ZX \rightarrow X$ is flat? I’m mainly concerned with $X$ being a smooth projective variety but a general answer would be appreciated. My idea was to work affine locally and recover something from $Bl_0 \mathbb{A}^n \rightarrow \mathbb{A}^n$ which it seems to me to be flat (but I might be wrong), but I wasn’t able to get far.","['algebraic-geometry', 'blowup', 'flatness']"
3071307,A simple model for the card game Dominion,"This problem is inspired by the card game Dominion . We play a card game. At the beginning, we get one card ""Copper"". At each round, we draw a card uniformly at random form cards that we already have. If we draw a copper, we can obtain a new card, either another copper card, or an estate card. If we draw an estate card, then we cannot do anything. The aim of the game is to maximize the number of estate card after $n$ rounds. What would be the optimal strategy in this game? How do we balance the number of copper cards and estate cards?","['card-games', 'optimization', 'probability-theory']"
3071343,Expression for sum of $n$ exponentials,"So I have this sum of exponentials and I would like to find an expression for it. $$\sum^n_{i=1} e^{\mu(i-1)} $$ Note that $i$ is not an imaginary indicator. I am aware there is a formula for summing a purely exponential sum, but I am not clear as to what happens to the $\mu$ .","['summation', 'geometric-series', 'sequences-and-series']"
3071367,What's so special about standard deviation?,"Equivalently, about variance? I realize it measures the spread of a distribution, but many other metrics could do the same (e.g., the average absolute deviation). What is its deeper significance? Does it have a particular geometric interpretation (in the sense, e.g., that the mean is the balancing point of a distribution)? any other intuitive interpretation that differentiates it from other possible measures of spread? What's so special about it that makes it act as a normalizing factor in all sorts of situations (for example, convert covariance to correlation)?",['statistics']
3071383,Finding functions $f: \Bbb R_*^+ \to \Bbb R_*^+$ with certain properties,Find the functions $f:\Bbb R_*^+ \to \Bbb R_*^+$ such that: $$f(x)f \left(\frac{1}{x}\right)=1$$,"['functional-equations', 'functions']"
3071423,What is the probability that no cup is on a saucer of the same colour?,"A tea set comprises four cups and saucers in four distinct colours. 
If the cups are placed at random on the saucers, what is the probability that no cup is on a saucer of the same colour? MY ATTEMPT As far as I understand, the given event's probability is given by \begin{align*}
\textbf{P}(C^{c}_{1}\cap C^{c}_{2}\cap C^{c}_{3}\cap C^{c}_{4}) = 1 - \textbf{P}(C_{1}\cup C_{2}\cup C_{3}\cup C_{4})
\end{align*} where the last expression can be calculated using the inclusion-exclusion principle based on the knowledge of the probabilities \begin{cases}
\textbf{P}(C_{i})\quad\text{for}\quad1\leq i \leq 4\\
\textbf{P}(C_{i}\cap C_{j})\quad\text{for}\quad 1\leq i < j \leq 4\\
\textbf{P}(C_{i}\cap C_{j}\cap C_{k})\quad\text{for}\quad 1 \leq i < j < k \leq 4\\
\textbf{P}(C_{1}\cap C_{2}\cap C_{3}\cap C_{4})\\
\end{cases} which can be easily obtained.","['conditional-probability', 'inclusion-exclusion', 'combinatorics', 'probability-theory', 'probability']"
3071478,Automorphism Group of a Variety acts on Local Sections,"The motivation/background of my question arises from following thread: Galois morphism - group acting on the variety The original setting is that we have a finite Galois morphism $f: X \to S$ , where $X$ and $S$ are non-singular and connected projective varieties over $\mathbb{C}$ . Galois means here that if we denote with $G$ the automorphism group of $X$ over $S$ then the quotient $X/G$ exists and the natural morphism $X/G \to S $ is an isomorphism. For futher discussion I would like to use the language of schemes to consider the topic from more general point of view by taking into account that basically a variety is a scheme $X$ over $k$ such that $X$ is
integral and the structure morphism $X \to Spec(k)$ is separated and of finite type. While the discussion @user52991 mentioned following statement: Let $$0 \to F′ \to F \to F′′\to 0$$ be a short exact sequence of coherent sheaves on X. Let the push forward $f_*$ of the short exact sequence to $S=X/G$ be exact (since $f$ finite this holds automatically). The aim is to consider the functor which associates to any coherent sheaf $E$ on $S$ , its subsheaf $E^G$ of $G$ invariants. I don't know why it is well defined. Here occure some understanding problems to me: First of all: How does $G$ act on local sections of $X$ ? Namely, for $U \subset X$ the rings $\Gamma(U, \mathcal{O}_X)$ ? So if we take $g \in G= Aut(X)$ then $g: X \to X$ as morphism of schemes induce ring maps $$g_U ^{\#}:\Gamma(U, \mathcal{O}_X) \to \Gamma(U, g_*\mathcal{O}_X)= \Gamma(g^{-1}(U), \mathcal{O}_X)$$ And here occurs to me the core question how $G$ can act on the local sections $\Gamma(U, \mathcal{O}_X)$ ? This action only make sense if $g^{-1}(U)=U$ for all $g \in G$ and $ U \subset X$ , right? Therefore I don't know why it make sense to define the invaraint subring $\Gamma(U, \mathcal{O}_X)^G$ . On the other hand since $g^{-1}(X)=X$ it make sence to define the action on global sections $\Gamma(X, \mathcal{O}_X)$ . Therefore only in this special case it make sense to set $\Gamma(X, \mathcal{O}_X)^G := \Gamma(X/G, \mathcal{O}_{X/G})$ . But what about local sections? Since we expect that $X/G$ has a variety structure (especially a scheme structure) it should be possible to talk about local sections of $X/G$ . What are they? If we can solve this question then the next problem would be how the $G$ - action is extended the the coherent sheaves $f_*F$ as given in the exact sequence after applying $p_*$ ? or more generally any coherent sheaves $E$ on $S$ ?","['algebraic-geometry', 'projective-schemes', 'projective-varieties']"
3071602,Is there always a positive integer $a$ such that both $a^2-4$ and $a^2+4$ are quadratic non-residues $\bmod p$?,"I would like to prove (disprove if wrong) the following statement: For all odd prime numbers except for $p=3,5$ or $13$ , there exists an
  integer $a>0$ such that both $a^2-4$ and $a^2+4$ are quadratic
  non-residue $\bmod p$ . Here is what I have done: suppose that $p \notin \{3,5,13\}$ and consider different cases: Case I: $p \equiv 3 \bmod 4$ .  We have $q$ is quadratic residue (QR) iff $-q$ is quadratic non-residue (QNR). Since $p \neq 5$ , then either $5$ is QNR (case I-i)  or $5$ is QR (case I-ii). In case (I-i) there exists $b$ such that $b^2 \equiv -5  \bmod p$ and obviously $-b^2+4 \equiv 3^2 \bmod p$ and $-b^2-4 \equiv 1 \bmod p$ , then both $b^2-4$ and $b^2+4$ are QNR. It suffices to take $a=b$ . Consider now case (I-ii).  Since $p \neq 13$ , then either (I-ii-1) $13$ is quadratic non-residue (QNR) mod $p$ (I-ii-2) $13$ is quadratic residue (QR) mod $p$ In case (I-ii-1), $65=5\cdot 13$ is QNR and there exists $c$ such that $c^2 \equiv -65  \bmod p$ and obviously $-c^2-16 \equiv 7^2 \bmod p$ and $-c^2+16 \equiv 9^2 \bmod p$ . Let $d=\frac{c}{2}$ , $e=\frac{7}{2}$ and $f=\frac{9}{2}$ in $ \mathbb{Z}/p\mathbb{Z}$ . Then $-d^2-4 \equiv e^2 \bmod p$ and $-d^2+4 \equiv f^2 \bmod p$ and both $d^2+4$ and $d^2-4$ are QNR. It suffices to take $a=d$ . In the case (I-ii-2) where both $5$ and $13$ are QR, I don't see how to proceed. In the case (II) where $p \equiv 1 \bmod 4$ , we have $q$ is quadratic residue (QR) iff $-q$ is quadratic residue (QNR) and here again the above method does not seem to work. I am also interested in any sharp upper bound for the smallest $a$ , in terms of $p$ . Thanks for help.","['number-theory', 'quadratic-residues', 'elementary-number-theory']"
3071605,Optimal set of rectangle sizes to pack arbitrary rectangle?,"I'm looking to build a set of wooden storage boxes of various standard sizes for storing small objects. I would like to choose a set of ""optimal"" box sizes (outside dimensions) for filling arbitrary rectangular spaces. I define ""optimal"" here as: minimize wasted space after packing; minimize different box sizes; and minimize the number of boxes required to fill the space, assuming the definition of some largest box dimension. In 1 dimension this is straightforward: a set of ""boxes"" of sizes { 16, 8, 4, 2, 1 } can fill any 1-dimensional space with less than 1 unit of waste with my definition of ""optimal"" as above. For 2-dimensional spaces, I don't know how to solve this.  It seems like there should be some geometric answer to this, but I'm not versed enough in packing problems to know where to start looking. I was wondering if anybody on here could help solve this, or point the way to an answer? Thanks!","['geometry', 'packing-problem']"
3071624,Green function for solving ODE: continuity and discontinuity of derivatives,"I am studying Green functions and I want to find the Green function $G(x,x_0)$ of the linear operator \begin{equation}
\mathcal{L}=\left(\frac{d^n}{dx^n}+a_1(x)\frac{d^{n-1}}{dx^{n-1}}+\dots+a_n(x)  \right)
\end{equation} This means solving the following problem \begin{equation}
\frac{d^n G}{dx^n}+a_1(x)\frac{d^{n-1}G}{dx^{n-1}}+\dots+a_n(x)G=\delta(x-x_0)
\tag{1}
\end{equation} I understand that I have to impose continuity in $x_0$ for all the derivatives, up to $n-2$ , and impose the jump discontinuity in $x_0$ for the $(n-1)$ -th derivative. I don't exactly get why this is the case. I have searched in some lecture notes and found the following clues: First approach They proceed by integrating equation $(1)$ , and they say ""we obtain $G^{(n-1)}=H(x-x_0)$ + some continuous functions. The $(n-1)$ -th derivative is not continuous, but suffers a discontinuous jump there. Integrating again shows that $G^{(n−2)}$ is continuous"". My problem here is the following: 
how can I say that the result of integrating equation $(1)$ gets me ""some continuous functions"" if I don't know anything about the continuity of $G(x, x_0)$ and its derivatives? Second approach I found it for a $2^{nd}$ order ODE but I assume it can be generalized. It proceeds by saying ""Suppose first that $G(x,x_0)$ was discontinuous at $x=x_0$ , with the discontinuity modelled by a step function. Then $G' \propto \delta(x-x_0)$ and consequently $G''\propto \delta(x−x_0)$ . However, the form of the ODE shows that $\mathcal{L}G$ involves no generalized functions beyond $\delta(x-x_0)$ , thus $G(x, ξ)$ must be continuous."" He THEN proceeds in integrating the ODE in a small neighborhood of $x_0$ , and  he asserts that ""Also, since $G$ is continuous, $G'$ must be bounded so the term $G'$ also cannot contribute as the integration region shrinks to zero size"". He then concludes that it's only the second derivative that contributes to the magnitude 1 jump. My problems here: 1) we started by modeling the discontinuity of $G$ by a step function, noticed that there was a contradiction and consequently stated that ""so the function must be continuous"". But isn't it a bit restrictive, to just assume a jump discontinuity? 2) Can this be generalized for a generic $n$ -order equation? My questions To resume, I would like to know: 1) What is the correct way to proceed to demonstrate that $G$ and its derivatives up to the $n-2$ -th are continuous in $x_0$ and its $(n-1)$ -th derivative has a jump discontinuity 2) As a side question: if the $(n-1)$ -th derivative of the function is discontinuous in $x_0$ how can $G^{n}$ exist in $x_0$ ? I'm a physicist not a mathematician, so please be kind =)","['greens-function', 'ordinary-differential-equations']"
3071630,Cardinality of sets and usage of Cantor-Bernstein theorem,"I have recently been studying cardinality of finite and infinite sets and the 'theory part' was not hard to understand, but it is pretty problematic for me to apply any of the definitions to solve problems. The first problem is following and I am not sure whether I solved it correctly: Does $A_1\sim A_2$ , $B_1\sim B_2$ and $|A_1|\leq|B_1|$ implies that $|A_2|\leq|B_2|$ ? My answer is yes, as $A_1$ is equinumerous to $A_2$ and $B_1$ is equinumerous to $B_2$ , we may assume that $|A_1|=|A_2|=n$ and $|B_1|=|B_2|=m$ . Then, $|A_1|\leq|B_1| \equiv n\leq m$ . As $n=|A_2|$ and $m=|B_2|$ , we get that $|A_1|\leq|B_1| \Rightarrow |A_2|\leq|B_2|$ . The second problem is connected with Cantor-Bernstein theorem , which says that if $|A|\leq|B|$ and $|B|\leq|A|$ , then $|A|=|B|$ . Using the theorem we can determine the cardinality of sets $A$ and $B$ by constructing two injective functions $f:A\rightarrow B$ and $g: B\rightarrow A$ . My job is to construct injective functions to show that $\{0,1\}^\mathbb{N}$ and $\mathbb{N}^\mathbb{N}$ have the same cardinality, but I do not know how do I construct such functions: $f: \{0,1\}^\mathbb{N} \rightarrow \mathbb{N}^\mathbb{N}$ , $g: \mathbb{N}^\mathbb{N} \rightarrow \{0,1\}^\mathbb{N}$ . Creating e.g. function $f$ requires to map $\big(\mathbb{N}\rightarrow\{0,1\}\big)\rightarrow \big(\mathbb{N}\rightarrow\mathbb{N}\big)$ , which confuses me a lot and I have no idea how should it work.",['elementary-set-theory']
3071694,Infinite product of Haar measures,"Suppose that $(G_n,\mu_n)$ is a sequence of compact metrizable groups such that each $\mu_n$ is a probability (right-invariant) Haar measure on $G_n$ . Is it true that the product measure $\otimes_n \mu_n$ on $\prod_n G_n$ is also the unique probability (right-invariant) Haar measure on it ? Edit : we consider compact metrizable groups so the Borel sigma algebra of the product is the product sigma-algebra.","['group-theory', 'measure-theory', 'probability']"
3071705,"Counting, Probability and Binomial Coefficients","If $$P_{2n+2}=\sum_{k=n+2}^{2n+2}{2n+2 \choose k}p^kq^{2n+2-k}$$ and, $$P_{2n}=\sum_{k=n+1}^{2n}{2n \choose k}p^kq^{2n-k}$$ where $0<p<q<1$ and $q=1-p$ Prove that $$P_{2n+2}=P_{2n}+{2n \choose n}p^{n+2}q^n-{2n \choose {n+1}}p^{n+1}q^{n+1}$$ $\mathbf {Inspiration:}$ A and B play a series of games where the probability of winning $\mathit p$ for A is kept less than 0.5. However A gets to choose in advance the total no. of plays. To win the game one must score more than half the games . If the total no. of games is to be even, How many plays should A choose? $\mathbf {Here}$ $P_{2n}$ and $P_{2n+2}$ represents the probability of A winning the play in $2n$ and $2n+2$ games where $2n$ is considered the optimum number of games","['binomial-coefficients', 'combinatorics', 'sequences-and-series', 'probability-theory', 'probability']"
3071714,If $x \geq y>x/2$ then is it true that $x \pmod y < x/2$?,"I'm not sure how to prove this statement, which I believe is true: given $x,y \in Z$ such that $x \geq y > \frac {x}{2}$ then $x$ (mod $y$ ) < $\frac {x}{2}.$ Edit: Would this be an acceptable sketch of the proof? Suppose that $x \geq y \geq \frac{x}{2};$ consider the bounds where $y = x$ or $y = \frac{x}{2}$ . 
In the case in which $x=y,$ then $x$ (mod $y$ ) = $x$ (mod $x) = 0$ .
On the other hand, if $y = \frac{x}{2},$ then $x$ (mod $y$ ) = $x$ (mod $\frac {x}{2}) = \frac {x}{2}$ .
Therefore, $0 \leq x$ (mod $y$ ) < $\frac {x}{2}$ .","['modular-arithmetic', 'discrete-mathematics']"
3071718,Non-Wide-Sense-Stationary process with autocorrelation that depends on time shift,"My question consists of two parts. 1)
 A wide sense stationary process has two properties: a) it's mean is constant. b) it's autocorrelation function depends on the time shift only. My problem is that I can't think of a non wide sense stationary process that has the second condition(b) but not the first(a) 2)
Is the limit of the autocorrelation as the time shift approaches infinity is the square of the mean of the process? Is there a proof for this?","['stochastic-processes', 'signal-processing', 'probability-theory']"
3071765,A collection of lines drawn between points in a regular 13-gon - how to determine where the points sit relative to each other?,"So I have 4 collections of lines drawn between points each making a path. The angles are measured. The problem I am attempting to solve is to determine whether or not each of the collections of points are a valid path between some subset of points on a regular 13-gon. So for instance, for the second shape are there 3 vertices in a regular 13 gon such that drawing a line between them gives a 60 degree angle (within 5 degrees of error, see context). I'm having a hard time figuring this out myself. I know the geometry involved but I'm just having a hard time applying it to this problem. Some Context The context of this problem is actually a clue given in the hunt for the secret zoo level of the video game Accounting Plus VR and this was my attempt to solve what these diagrams mean. So because I measured the angles with an actual protractor they have 5 degrees of precision. So if there are valid points with approximate angles that is also a valid answer. I hope this doesn't make the question unanswerable but this is what I have to work with geometry wise. This is effectively an actual ""real"" problem with imperfect diagrams, rather than an ideal theoretical problem coming out of a book. And yes the images are the original images I obtained for reference blown up in size. So if one wants to remeasure the angles in case my protractor skill is flawed, they are welcome to.","['geometry', 'applications']"
3071826,Can a finite group have 2D and 3D faithful irreducible representations?,I am looking for finite groups that have a 2D (complex matrix) faithful irreducible representation and a 3D faithful irreducible representation. Up to order 1023 GAP found none. Other combinations of dimensions seem to occur. Are groups with 2D and 3D faithful irreps non-existing?,"['group-theory', 'representation-theory']"
3071838,How many $g$ in a finite group are such that $b=g^{-1}ag$ for given $a\ne b$ in the group?,"Given a group $G$ , we know that we can set up an equivalence relation among its elements by defining $a \equiv b \Leftrightarrow \exists g \in G|b=g^{-1}ag$ (conjugacy). Let's define $\mathcal{F}_b^{(a)}:=\lbrace g \in G|b=g^{-1}ag \rbrace$ and denote by $C_G(a)$ the centralizer of $a$ in $G$ . If $b \ne a$ , then $\mathcal{F}_b^{(a)} \cap C_G(a)=\emptyset$ ; in fact, $g \in \mathcal{F}_b^{(a)} \cap C_G(a) \Rightarrow$ $ag=gb \wedge ag=ga \Rightarrow gb=ga \Rightarrow b=a$ . We can therefore state that $b \ne a \Rightarrow$ $\mathcal{F}_b^{(a)}=\lbrace g \in \complement_G(C_G(a))|b=g^{-1}ag \rbrace$ , $\complement_G(X)$ being the complement in $G$ of any subset $X \subseteq G$ . My question is the following. Let's take $G$ finite and $a,b \in G$ with $b \ne a$ : how can we express the cardinality of $\mathcal{F}_b^{(a)}$ ?","['equivalence-relations', 'group-theory', 'finite-groups']"
3071840,"Lebesgue Integral, an example","I have been trying to studying the construction of Lebesgue integral for a while now. I am following the Princeton Lectures on Analysis and I am stuck at the part where it defines the integral of non-negative functions. I find the definition to be quite clear but I cannot understand the examples given here. With the above definition of the integral, there are only two possible cases; the supremum is either finite, or infinite. In the first case, when $\int f(x)dx<+\infty$ , we shall say that $f$ is Lebesgue integrable or simply integrable . Clearly, if $E$ is any measurable subset of $\mathbb R^d$ , and $f\ge0$ , then $f_{\chi_E}$ is also positive, and we define $$\int f(x)dx=\int f(x)\chi_E(x)dx.$$ Simple examples of functions on $\mathbb R^d$ that are integrable (or non-integrable) are given by $$f_a(x)=\begin{cases}|x|^{-a}&\text{ if }|x|\le1,\\0&\text{ if }|x|>1.\end{cases}$$ $$F_a(x)=\frac1{1+|x|^a}, \text{ all }x\in\mathbb R^d.$$ Then $f_a$ is integrable exactly when $a<d$ , while $F_a$ is integrable exactly when $a>d$ . How are the values of ""a"" here in these two examples making the function integrable or not !? and what does the value of ""a"" has to do with ""d"" ?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
3071867,Prove that a function is smooth if it is smooth in almost all directions,"Question So suppose we have a function $f:\mathbb R^2\to \mathbb R$ for which it is given that $x\mapsto f(x,g(x))$ is smooth (i.e., $C^\infty$ ) for all smooth functions $g:\mathbb R\to\mathbb R$ . Can we prove that $f$ is smooth as well? I don't know whether this statement is true and honestly I wouldn't be surprised either way. What I've tried already Fix a point $(x_0,y_0)$ . Intuitively, by taking $g(x) = \lambda x$ with $\lambda\in\mathbb R$ we see that $f$ should be at least differentiable along all directions $(1,\lambda)$ at $(x_0,y_0)$ . This follows for instance by considering the curve $t\mapsto (x_0+t, y_0+\lambda t)$ . Thus the only direction that is non-trivial is the vertical direction $(0,1)$ . If we can show that $f$ is also differentiable in that direction then I'm confident that it will be possible to show that $f$ is differentiable. But how can we show whether $f$ is differentiable along $(0,1)$ ? We cannot do it directly from the fact that $f(x,g(x))$ is smooth, but perhaps we can use a limiting argument, letting the slope of the curve $(t,g(t))$ tend to infinity? When we know that $f$ is differentiable, it will probably be possible using an inductive argument to prove that $f$ is smooth (i.e., $C^\infty$ ). Any help is appreciated. EDIT. If found a closely related result, namely Boman's theorem , which says basically says that $f$ is smooth if and only if $f\circ\gamma$ is smooth for all smooth curves $\gamma:\mathbb R\to\mathbb R^2$ . I feel like the statement of my question should probably be reducible to this theorem. The only difficulty is that we don't necessarily know if our $f$ is differentiable along vertical curves, but perhaps this follows in some way.",['real-analysis']
3071952,Determining range of a function,"I was trying to determine the domain and range of a function. The function is: $$y = \frac{x}{x^2 + 1}$$ I determined the domain which is $\mathbb{R}$ . In this equation, when the value of $x$ is 0, the value of $y$ is $0$ .  Then, I tried to determine the range by this equation: $$x = \frac{1 \pm \sqrt{1 - 4y^2}}{2y}$$ I derived the equation from previous equation using quadratic equation formula. In this method we can see, the range is $\{-0.5 \leq y < 0\}$ and $\{0 < y \leq 0.5\}$ . Zero cannot be a range because the denominator cannot be zero. 
But if $0$ is not in the range, then we do not find any value for $x$ when $x$ is $0$ which is a contradiction of the definition of a function. Where is the fault?",['functions']
3071967,Schwarz inequality in linear algebra and probability theory,"Linear algebra states Schwarz inequality as $$\lvert\mathbf x^\mathrm T\mathbf y\rvert\le\lVert\mathbf x\rVert\lVert\mathbf y\rVert\tag 1$$ However, probability theory states it as $$(\mathbf E[XY])^2\le\mathbf E[X^2]\mathbf E[Y^2]\tag 2$$ By comparing $\lvert\sum_i x_iy_i\rvert\le\sqrt{\sum_i x_i^2\sum_i y_i^2}$ with $\lvert\sum_y\sum_x xyp_{X,Y}(x,y)\rvert\le\sqrt{\sum_x x^2p_X(x)\sum_y y^2p_Y(y)}$ , we see that $(1)$ and $(2)$ are equivalent when $p_{X,Y}(x,y)=\begin{cases}\frac1n&\text{if $x=x_i$ and $y=y_i$ for $i\in\{1,2,\cdots,n\}$}\\0&\text{otherwise}\end{cases}$ . Thus, $(2)$ can be thought of as a more general form of the inequality. Another way to think about this is to compare $\lvert\cos\theta\rvert=\frac{\lvert\mathbf x^\mathrm T\mathbf y\rvert}{\lVert\mathbf x\rVert\lVert\mathbf y\rVert}\le1$ with $\lvert\rho\rvert=\frac{\lvert\mathbf{cov}(X,Y)\rvert}{\sqrt{\mathbf{var}(X)\mathbf{var}(Y)}}\le1$ . The former is exactly $(1)$ , while the latter becomes $(2)$ only when $\mathbf E[X]=\mathbf E[Y]=0$ . In some sense, we can view $\mathbf x^\mathrm T\mathbf y$ as a special form of $\mathbf{cov}(X,Y)$ . Then, it follows that $\mathbf x^\mathrm T\mathbf x$ is a form of $\mathbf{var}(X)$ and $\lVert\mathbf x\rVert$ is a form of $\sqrt{\mathbf{var}(X)}$ . What is the special form of $\mathbf E[X]$ and how do we understand $\mathbf E[X]=\mathbf E[Y]=0$ in linear algebra? With $p_{X,Y}$ defined above, we have $\mathbf E[XY]=\frac{\mathbf x^\mathrm T\mathbf y}n$ , but $\mathbf{cov}(X,Y)\ne\mathbf E[XY]$ unless $\mathbf E[X]=0$ or $\mathbf E[Y]=0$ . How can we obtain a relation between $\mathbf{cov}(X,Y)$ and $\mathbf x^\mathrm T\mathbf y$ ?","['cauchy-schwarz-inequality', 'linear-algebra', 'probability-theory']"
3072094,Understanding a remark for the Hausdorff measure in Wolff's lecture notes,"In the chapter of Hausdorff measures in Wolff's notes on harmonic analysis, I'm trying to understand a piece of remark. Fix $\alpha>0$ , and let $E\subset\mathbb{R}^n$ . For $\epsilon>0$ , one defines $$
H_\alpha^\epsilon(E)=\inf \sum_{j=1}^\infty r_j^\alpha,
$$ where the infimum is taken over all countable coverings of $E$ by discs $D(x_j,r_j)$ with $r_j<\epsilon$ . It is clear that $H_\alpha^\epsilon(E)$ increases 1 as $\epsilon$ decreases, and we define $$
H_\alpha(E)=\lim_{\epsilon\to 0} H_\alpha^\epsilon(E).
$$ 1. Note: I believe this should be understood as ""not decrease"" since if $H_\alpha^1(E)=0$ , then $H_\alpha(E)=0$ . Remark . It is clear that $H_\alpha(E)=0$ for all $E$ if $\alpha>n$ , since one can then cover $\mathbb{R}^n$ by discs $D(x_j,r_j)$ with $\sum_jr_j^\alpha$ arbitrarily small. I don't understand the remark. Could anyone elaborate it? By applying the inequality $$
\sum_j r_j^\alpha \le \delta^{\alpha-n}\sum_jr_j^n,\quad r_j<\delta,
$$ one can show that if $H_n(E)<\infty$ , then $H_\alpha(E)=0$ for $\alpha>n$ . But the remark says something stronger.","['harmonic-analysis', 'measure-theory', 'hausdorff-measure', 'real-analysis']"
3072147,Explanation of the steps on calculating the length of the catenary,"Can someone please explain to me what identities have been used in the calculation of the below example and why the limits of integration change from $0$ to $a$ ? I have tried $\sinh^2{ t} = [-1 + \cosh (2 t)]/2$ and end up with nothing like the form below. Example : The catenary, is the shape of a wire hanging under its own weight and is given by $$
x(t) = t{\bf{e}}_1 + \cosh{t} {\bf{e}}_2,$$ with $I = [−a,a]$ . The total length of this catenary is \begin{align}
L &=\int_ {a}^{−a} {\sqrt{1 + \sinh^2{ t}}} \quad dt \\
&=2\int_0^a \cosh{t} \quad dt \\
&=2\sinh{a}.
\end{align}","['integration', 'proof-explanation', 'trigonometry']"
3072158,"Is the set $\{\delta_x\}_{x \in [a, \ b]}$ a basis for the set of distributions on $C^{\infty}_c([a, \ b])$?","$\def\braket#1#2{\langle#1|#2\rangle}\def\bra#1{\langle#1}\def\ket#1{#1\rangle}$ Is the set $\{\delta_x\}_{x \in [a,  b]}$ a basis for the set of distributions on $C^{\infty}_c([a, \ b])$ ?
Below are p.59 and p.60 from Shankar's book Principles of Quantum Mechanics. According to Shankar, the set $\{\delta_x\}_{x \in [a, \ b]}$ is a basis for  the space of functions on $[a, \ b]$ that vanishes on the set $\{ a, b\}$ .
Here, $\delta_x(y) = \delta(x-y) = \braket{x}{y}$ and $\delta_x = |\ket{x}$ .
However, Dirac deltas are not functions. Is the set $\{\delta_x\}_{x \in (a, \ b)}$ a basis for the set of distributions on $C^{\infty}_c((a, \ b))$ ? $\varphi:= \int_a^b | \ket{y} \bra{y} | dy $ $\varphi [f] = \int_a^b | \ket{y} \bra{y} | \ket{f} dy = \int_a^b | \ket{y} f(y) dy $ $ \braket{x}{\varphi(f)} =  \int_a^b \braket{x}{y} f(y) dy = \int_a^b \delta(x-y) f(y) dy = f(x) = \braket{x}{f}$ Hence $\varphi(f) = f$ , and $\varphi = id$ . $ \int_a^b | \ket{y} \bra{y} | dy  = id \in \operatorname{Hom}(C_c^{\infty}((a, \ b), C_c^{\infty}((a, \ b))$ (This is equation (1.10.11) in Shankar) $\varphi_g :=\int_a^b \ dy \ g(y) \bra{y}|$ Then $ \varphi_g (|\ket{f}) = \int_a^b \ dy \ g(y) \braket{y}{f} = \int_a^b \ dy \ g(y) f(y) = \int_a^b g f \ dy = \braket{g}{f}$ Hence $g = \int_a^b \ dy \ g(y) \bra{y}|$ in $\operatorname{Hom}(C^{\infty}_c((a, \ b), \mathbb{R})$ ? My questions are: Is the set $\{\delta_x\}_{x \in (a, \ b)}$ a basis for the set of distributions on $C^{\infty}_c((a, \ b))$ ? If 1 is true, then in what sense (e.g. finite sum , countable sum ...) does $\{\delta_x\}_{x \in (a, \ b)}$ span the space of distributions on $C^{\infty}_c((a, \ b))$ ? $g = \int_a^b \ dy \ g(y) \bra{y}|$ in $\operatorname{Hom}(C^{\infty}_c((a, \ b), \mathbb{R})$ ?","['distribution-theory', 'real-analysis', 'functional-analysis', 'quantum-mechanics', 'mathematical-physics']"
3072173,Check my proof that a monotone function has one-sided limits at any point,"Here $f:\mathbb{R} \rightarrow \mathbb{R} $ . We consider the case when $f$ is an increasing function. Let $A=\{f(x) | x<x_0\}$ . $A$ is bounded from above by $f(x_0)$ ,so there exists $\alpha \in \mathbb{R} $ so that $\sup A= \alpha$ . Let $\epsilon >0$ . Since $\alpha - \epsilon < \alpha => \exists x_1 \in \mathbb{R}, x_1<x_0$ so that $f(x_1)>\alpha - \epsilon$ . $=>\forall x\in (x_1, x_0), \alpha - \epsilon < f(x_1) \le f(x) <\alpha + \epsilon$ Let $\delta_1=x_0 - x_1$ .Then $\forall x\in \mathbb{R}, |x-x_0|<\delta_1,|f(x)-\alpha|<\epsilon$ ,so $\lim_{x\to x_0^{-}} f(x) =\alpha$ . Let $B=\{f(x) | x>x_0\}$ . $B$ is bounded from below by $f(x_0)$ ,so there exists $\beta \in \mathbb{R} $ so that $\inf B= \beta$ . Let $\epsilon_1 >0$ . Since $\beta + \epsilon_1 > \beta => \exists x_2 \in \mathbb{R}, x_2>x_0$ so that $f(x_2)<\beta + \epsilon_1$ . $=>\forall x\in (x_0, x_2), \beta + \epsilon_1 > f(x_2) \ge f(x) >\beta - \epsilon_1$ Let $\delta_2=x_2 - x_0$ .Then $\forall x\in \mathbb{R}, |x-x_0|<\delta_2,|f(x)-\beta|<\epsilon_1$ ,so $\lim_{x\to x_0^{+}} f(x) =\beta$ .","['limits', 'functions', 'proof-verification']"
3072198,Fundamental Theorem of Calculus with Different Variables,"How can one explain that $$\frac{d}{dx}\left(\int_0^x{\cos(t^2+t)dt}\right) = \cos(x^2+x)$$ Without solving the integral? I know it's related to the fundamental theorem of calculus, but here we have a derivative with respect to $x$ , while the antiderivative is with respect to $t$ . Thank you.","['integration', 'calculus', 'definite-integrals', 'derivatives']"
3072234,Why do we divide by $ \sqrt{A^2 + B^2} $ to convert a function from the standard form to the normal form?,"To convert an equation in the standard form, $ Ax + By + C = 0 $ to the normal form $ \cos \omega \ x + \sin \omega \ y = p $ , we first divide by $ \sqrt{A^2 + B^2} $ . Why? For example, to convert the equation $ \sqrt 3 \ x + y - 8 = 0 $ , we divide by $ \sqrt{A^2 + B^2} = 2 $ to get $ \frac{\sqrt 3}{2} \ x + \frac 12 \ y = 4 $ and then solve for $ \cos \omega = \frac{\sqrt 3}{2} $ and $ \sin \omega = \frac 12 $ to find the value of $ \omega $ . I understand that we can't possibly convert this directly, since there is no $ \omega $ for which $ \cos \omega = \sqrt 3 $ --but why do we specifically divide by $ \sqrt{A^2 + B^2}?$ Why not any other number to bring the values of $ \cos \omega $ and $ \sin \omega $ between 0 and 1 and then solve it?","['algebra-precalculus', 'linear-algebra']"
3072241,Prove that the estimators are biased.,"Given the following sentences: Let $X_1,..., X_n$ be a random sample from a $Pois(\mu)$ distribution. Consider the following estimator for $e^{-\mu}=P(X_i=0)$ : $T=e^{-\overline{X_n}}$ . The independent random variables $X_1;...;X_n$ have a geometric distribution with parameter $p$ . Look at the following estimator for $p$ : $S=\frac{1}{\overline{X_n}}$ . Prove that the estimators are biased. In my opinion both estimators are unbiased: $E[T]=e^{E[\overline{X_n}]}=e^{-\mu}$ that is unbiased for the parameter $e^{-\mu}$ . $E[S]=\frac{1}{E[\overline{X_n}]}=\frac{1}{1/p}=p$ that is unbiased for the parameter $p$ . Why I'm wrong in both cases? Where are my mistakes? Thanks.",['statistics']
3072350,What is the probability that a random function from $\mathbb{N} \to \mathbb{N}$ is surjective?,"Here $[n]$ denotes the first $n$ positive integers $\{1,2,...,n\}$ and $\mathbb{N}$ is the set of natural numbers. Let $f:[n] \to [n]$ be a random function chosen uniformly from all possible functions from $[n] \to [n]$ .  The probability that $f$ is surjective is $\frac{n!}{n^n}$ which tends to zero as $n \to \infty$ .  This makes me want to believe that a function from $\mathbb{N} \to \mathbb{N}$ has probability zero of being surjective. On the other hand, let $g:\mathbb{N} \to [n]$ be a random function chosen uniformly from all possible functions from $\mathbb{N} \to [n]$ . For each $m \in [n]$ , the probability that $g^{-1}(m) = \emptyset$ is equal to the limit as $k \to \infty$ of $(\frac{n-1}{n})^k$ which equals zero.  This makes me think that the desired probability is 1.","['elementary-set-theory', 'combinatorics', 'probability']"
3072360,"$C[0,1], 0<p<1$ is not a norm linear space. What about $p = \infty$ (i.e. supremum norm)? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question To Prove - $C[0,1], 0<p<1$ (with usual p-norm) is not a norm linear space. My thought process - I know that the triangle inequality is violated. But I am not able to produce a counterexample because every time I try to construct an example, I violate that it must be continuous on $[0,1]$ . 
One more subtle point, most other similar proofs that I've done usually are with the $f = g$ (or $x = y$ for the two points of Triangle Inequality). But here, taking $f = g$ will not provide a counterexample since for such a case, Triangle Inequality holds with equality. To prove/disprove - $C[0,1], $ p =  inifinity $ $ (i.e. the norm defined the supremum of the absolute value of the function on the closed interval $[0,1]$ .) is not a norm linear space. My thought process - The proof for p>=1 uses Holder Inequality, Minkowski Inequality. I wish to know if that thing can simply be used for p = infinity also. Then, that's done.","['calculus', 'normed-spaces', 'functional-analysis', 'real-analysis']"
3072363,Show that X is a sub-gaussian random vector with dependent sub-gaussian coordinates,"Let $X \in R^n$ be a zero mean, random vector with sub-gaussian coordinates $X_i$ . prove that X is a sub-gaussian random vector no matter if coordinates are independent or dependent. It is easy to prove the result in the case of independent coordinates. When it comes to the case of dependent coordinates, I think of the definition of multivariate normal distribution but don't know if it works for sub-gaussian family. Assume a random vector Z $\in R^n$ has independent zero mean, unit variance, sub-gaussian coordinates and denote $\Sigma_X$ as the covariance matrix of X, we can find a Z such that $\Sigma_X^{1/2} Z$ has the same distribution as X. Because for $\forall a \in R^n$ $a^{T}\Sigma_X^{1/2} \in R^n$ , given the case of independent coordinates, we can say for $\forall a \in R^n, a^{T}\Sigma_X^{1/2}Z = a^{T}X$ is sub-gaussion distributed. So X is a sub-gaussian random vector. I am not sure if the proof is right for the whole sub-gaussian family, because I am not sure we can find a Z that $\Sigma_X^{1/2} Z$ is distributed same as X. Any suggestions and ideas?","['statistics', 'normal-distribution', 'probability']"
3072405,An abelian group $G$ of order $35$ with $g^{35}=e$ for all $g\in G$ is cyclic.,"I'm reading ""Contemporary Abstract Algebra,"" by Gallian . This is Exercise 4.20. Suppose that $G$ is an Abelian group of order $35$ and every element of $G$ satisfies the equation $x^{35}=e$ . Prove that $G$ is cyclic. Does your argument work if $35$ is replaced with $33$ ? There's definitely something I don't understand here. I am/was under the impression that, for any group $H$ and any $h\in H$ , we have $h^{\lvert H\rvert}=e$ ; indeed: the cyclic subgroup $\langle h\rangle$ of $H$ has the same order as a group as the order $\lvert h\rvert$ of $h$ as an element of $H$ ; Lagrange's theorem then gives that $\lvert h\rvert$ divides $\lvert H\rvert$ , so that then $h^{\lvert H\rvert}=e$ . So what gives? I get the feeling that it's something obvious.","['abelian-groups', 'group-theory', 'cyclic-groups', 'finite-groups']"
3072427,Proof verification that the sequence $x_n = \frac{1}{n}$ converges to every point of $\mathbb{R}$ on the cofinite topology,"Let $a \in \mathbb{R}$ . Then any open set $U$ in the cofinite topology containing $a$ is of the form $U = \mathbb{R} - \{\alpha_1, \alpha_2, \cdots, \alpha_p\}$ for some $p \in \mathbb{N}$ (and of course $\alpha_i \neq a \ \forall 1 \leq i \leq p$ ). Now, take $N = c \left(\displaystyle{\frac{1}{\min \alpha_i}}\right) + 1$ , where $c(x)$ stands for the ceiling of $x$ (i.e, $c(2.1) = 3$ ). Then it's clear that $x_n \in U \ \forall n \geq N$ and we're done. Is this alright? I actually think I could do away with all of this and just make the argument that the tail of $x_n$ is eventually contained in $U$ since $\mathbb{R} - U$ has only finitely many elements and $\{x_n\}_{n \in \mathbb{N}}$ is infinite, but I'm not sure if that's fine too.","['general-topology', 'proof-verification', 'real-analysis']"
3072458,How does a gradient allow the calculation of the directional derivative?,"If the gradient only results in a vector telling you the steepest direction to travel, how can the ""slope"" in any direction be calculated? If the gradient is: $\nabla f(x,y,z) = \left[\begin{array}{c}\frac{\partial f}{\partial x}\\\frac{\partial f}{\partial y}\\\frac{\partial f}{\partial z}
\end{array}\right]$ How can the directional derivative simply be a dot product between the gradient and the vector? $\nabla_{\vec{v}}\,f(x,y,z) = \nabla f(x,y,z) \cdot \vec{v} = \vec{v}_x\frac{\partial f}{\partial x}+\vec{v}_y\frac{\partial f}{\partial y}+\vec{v}_z\frac{\partial f}{\partial z}$","['partial-derivative', 'multivariable-calculus', 'vectors', 'vector-analysis']"
3072514,Nigerian Olympiad [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Suppose $a,b,c,d$ are integers satisfying $ab + cd = 44,ad - bc = 9.$ Find the minimum possible value of $a² + b² + c² + d².$","['elementary-number-theory', 'algebra-precalculus']"
3072544,Let $X$ be Poisson r.v. with $\lambda$ find $f(x)$ such that $E[f(X)]=\lambda \log (\lambda)$,"I am looking for a function $f(x)$ such that \begin{align}
E[f(X)]=\lambda \log (\lambda) \quad  \text{ for all } \lambda \ge 0 \tag{$*$}
\end{align} where $X$ is a Poisson random varaible with parameter $\lambda$ . Note, we are looking for a function idenpendent of $\lambda$ . Here are some thoughts: \begin{align}
\lambda \log (\lambda) = E[f(X)]= \sum_{k=0}^\infty f(k) \frac{\lambda^k e^{-\lambda}}{k!} 
\end{align} Therefore, we have that \begin{align}
e^{\lambda} \lambda \log (\lambda) =  \sum_{k=0}^\infty f(k) \frac{\lambda^k }{k!}= \sum_{k=0}^\infty a_k \frac{\lambda^k }{k!} \tag{$**$}
\end{align} where in the last step I defined $f(k)=a_k$ . Now, this is the point where I am a bit stuck. It doesn't look like the expression in $(**)$ can hold as I don't think the function $g(x)=e^x x \log(x)$ can be written as a Maclaurin series. 
These lets me to conclude that there is no function $f(x)$ such that $(*)$ holds.","['poisson-distribution', 'probability-theory', 'probability', 'real-analysis']"
3072589,How can I calculate $\int\frac{x-2}{-x^2+2x-5}dx$?,"I'm completely stuck on solving this indefinite integral: $$\int\frac{x-2}{-x^2+2x-5}dx$$ By completing the square in the denominator and separating the original into two integrals, I get: $$-\int\frac{x}{x^2-2x+5}dx -\int\frac{2}{(x-1)^2 + 4}dx$$ The second one is trivial, but the first one has me stuck. Whatever substitution I apply or form I put it in, I just can't figure it out. They're meant to be solved without partial integration, by the way.","['integration', 'indefinite-integrals', 'partial-fractions']"
3072705,Derive Group Law on Elliptic Curve with Riemann Roch,"Consider $E$ be an elliptic curve and $k$ a field. I read that one way to show that $E(k)$ has an abelian group structure can be derived using Riemann Roch. Could anybody explain how it concretely provides the desired result? My considerations: We have a group of divisors $Div(E)$ where the divisors are formal sums $\sum_{P \in E(k)} n_P (P)$ with $n_P \in \mathbb{Z}$ and the principal divisors $div(f) = \sum_P ord_P(f) (P)$ form a subgroup of $Div(E)$ ; denote it by $PrDiv(E)$ . The divisor class group is the quotient $Cl(E)= Div(E)/PrDiv(E)$ . We can define canonically a map $E(k) \to Cl(E), p \to (P)-(O)$ where $O$ is the special point (=neutral element). Obviously it suffice to show that every divisor $D$ obtained from intersection of a line $L$ with $E$ has three points (counted we multiplicies). Or in language of divisors: For $D:= L \cap E$ we have to show that $deg(D)=3$ , so $D= (P) + (Q) +(R) + div(f)$ for some principal divisor $div(f)$ . This would settle $P+Q=-R$ . But I have some problems to derive it with Riemann Roch: The RR-formula is: $$l(D)-l(K-D) = deg(D) +g-1$$ Since $E$ elliptic $g=1$ so it suffice to show $l(D)-l(K-D)=3$ . And here I stuck. I know that $l(D) := dim_kH^0(D, \mathcal{O}_D)$ but this doesn't help me. Futhermore what to do with $l(K-D)$ ? Remark: I know that there are a lot of other ways to derive the group law but the point of this question is to derive it using Riemann Roch. Background on my question: @Awenshi's comment in https://mathoverflow.net/questions/6870/why-is-an-elliptic-curve-a-group","['divisors-algebraic-geometry', 'algebraic-geometry', 'elliptic-curves']"
3072721,Find an implicit solution to $y ^ { \prime } ( x ) = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } }$,"$y ^ { \prime } ( x ) = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } }$ $\frac { d y } { d x } = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } }$ $\frac { d x } { d y } = \frac { 2 y \sqrt { 1 + y ^ { 2 } } } { x ^ { 3 } }$ $\int x ^ { 3 } d x = 2 \int y \left( 1 + y ^ { 2 } \right) ^ { 1 / 2 } d y$ R.H.S. $\int y \sqrt { 1 + y ^ { 2 } } d y$ let $u = 1 + y ^ { 2 }$ $ d u = 2 y d y $ $ \frac { d u } { 2 } = y d y $ $\int y \sqrt { 1 + y ^ { 2 } } d y$ $\int \sqrt { 1 + y ^ { 2 } } y d y$ $\int \frac { \sqrt { u } } { 2 } d u$ $\frac { 1 } { 2 } \int \sqrt { u } d u$ $= \frac { 1 } { 2 } \left[ \frac { 2 u ^ { 3 / 2 } } { 3 } \right]$ $= \frac { u ^ { 3 / 2 } } { 3 } = \frac { 1 } { 3 } \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 } \ldots 1$ $\int x ^ { 3 } d y = 2 \int y \left( 1 + y ^ { 2 } \right) ^ { 1 / 2 } d y$ $\frac { x ^ { 4 } } { 4 } + c = 2 \left[ 1 / 3 \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 } \right]$ $\frac { x ^ { 4 } } { 4 } + c = \frac { 2 } { 3 } \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 }$ cant finish it , or maybe my working is wrong ?",['integration']
3072765,"If $\tan 2\alpha \cdot \tan \alpha = 1$, then what is $\alpha$? Different methods give different answers.","If $\tan 2\alpha\cdot\tan \alpha = 1$ , then what is $\alpha$ ? I tried two methods but got two different answers. Method 1: $$\begin{align}
\tan 2\alpha\cdot\tan \alpha = 1
&\implies
\frac{2\tan \alpha}{1 - \tan ^2 \alpha}\;\tan \alpha = 1 \tag{1a}\\[6pt]
&\implies 2\tan ^2\alpha = 1 - \tan ^2\alpha \tag{1b}\\[6pt]
&\implies \tan ^2 \alpha = \frac{1}{3} \tag{1c}\\[6pt]
&\implies \tan \alpha = \pm\frac{1}{\sqrt 3} \tag{1d}\\[6pt]
&\implies \tan \alpha = \tan\left(\pm\frac{\pi}{6}\right) \tag{1e}\\[6pt]
&\implies \alpha = n\pi \pm \frac{\pi}{6} \;\text{where}\; n \in \mathbb{Z} \tag{1f}
\end{align}$$ Method 2: $$\begin{align}
\tan 2\alpha \cdot \tan \alpha = 1
&\implies \tan 2\alpha = \frac{1}{\tan \alpha} \tag{2a}\\[6pt]
&\implies \tan 2\alpha = \cot \alpha \tag{2b}\\[6pt]
&\implies \tan 2\alpha = \tan\left(\frac{\pi}{2} - \alpha\right) \tag{2c}\\[6pt]
&\implies 2\alpha = n\pi + \frac{\pi}{2} - \alpha \text{?} \tag{2d}\\[6pt]
&\implies \alpha = \frac{1}{3}\left(n\pi + \frac{\pi}{2}\right)\;\text{where}\; n \in \mathbb{Z} \tag{2e}
\end{align}$$ Which one is correct? Is there any mistake in the above solutions?",['trigonometry']
3072784,Using the R method for finding all solutions to $\sin(2a) - \cos(2a) = \frac{\sqrt{6}}{2}$. My solution differs from official answer.,"How many solutions does $$\sin(2a) - \cos(2a) = \frac{\sqrt{6}}{2}$$ have between $-90^\circ$ and $90^\circ$ ? I used the R method and got $$2a-45^\circ = \arcsin\left(\frac{\sqrt{3}}{2}\right).$$ Since $a$ is between $-90^\circ$ and $90^\circ$ , then $2a$ is between $-180^\circ$ and $180^\circ$ . The RHS can be $60^\circ$ , $120^\circ$ , $-240^\circ$ , and $-300^\circ$ . Only $60^\circ$ and $120^\circ$ fit the criteria, but the answer is 4 solutions. Where did I go wrong?",['trigonometry']
3072798,Prove that $(1/n^n)\leq(1/n!)$ for every $n\geq1$,"I want to know if my answer is correct: 1) For n=1 : $ 1=1$ Correct! 2) Let n=k is an inductive assumption which is correct: $$\frac{1}{k^k}\leq \frac{1}{k!}$$ 3) For n=k+1 , we should prove that: $$\frac{1}{(k+1)^{k+1}}\leq \frac{1}{(k+1)!}$$ So, $$\frac{1}{(k+1)!}=
\frac{1}{(k+1)k!}\geq 
\frac{1}{(k+1)k^k}\geq 
\frac{1}{(k+1)(k+1)^k}=
\frac{1}{(k+1)^{k+1}}$$ It's correct also for $n=k+1$ , so the inequality $1/(k+1)^{k+1}\leq1/(k+1)!$ is correct for every number $n\geq1$","['induction', 'proof-verification', 'discrete-mathematics']"
3072803,"Prove $E[X/Y]\ge1$ for X,Y iid positive random variables.","Prove $E[X/Y]\ge1$ for X,Y iid positive random variables. My attempt: Let $Z=X/Y$ $Z\in(0,\infty)$ . $$E[Z]=\int_0^\infty P(Z\gt z)dz=\int_0^\infty P(Y\lt{1\over z}X)dz$$ $$=\int_0^\infty\int_0^\infty\int_0^{{1\over z}X}f(x)f(y)dydxdz$$ $$=\int_0^\infty\int_0^\infty f(x)F({1\over z}X)dxdz$$ $$=\int_0^\infty (F(x)F({1\over z}X)|_0^{\infty}-\int_0^\infty F(x)f({1\over z}X){1\over z}dx)dz$$ $$=\int_0^\infty(1-\int_0^\infty F(x)f({1\over z}X){1\over z}dx)dz$$ This is how I get 1 in the equation. I hope it has something to do with the question. I know its a mess. Can anyone help me out?",['probability']
3072817,Solving $8x-3+\sqrt{x+2}-\sqrt{x-1}=7 \sqrt{x^2+x-2}$,"Solve the equation $$8x-3+\sqrt{x+2}-\sqrt{x-1}=7 \sqrt{x^2+x-2}$$ I have this idea: set $$\sqrt{x+2}=a , x+2=a^2 , \sqrt{x-1}=b.$$ So $$x-1=b^2 , 2a^2+6b^2 =8b-4$$ and $$x^2+x-2 =a^2b^2$$ and then I'd simplify, but it's still very hard to solve. 
Any hint is appreciated.","['substitution', 'problem-solving', 'algebraic-equations', 'radicals', 'algebra-precalculus']"
3072820,Limit point alternative definition,"I have a question, why isn't an alternative definition of a $p \in X$ being a limit point of a set $E$ : $\forall r>0 N_r(p) \cap E \neq \emptyset$ , why does it have to be the punctured neighborhood?
Note that $E$ is a subset of $X$ and $X$ is a metric space.
Note that the definition of a limit point of a set E is a point $p \in X$ such that every neighborhood contains a point $q$ such that $q\neq p$ and $ q\in E$ .","['proof-verification', 'analysis', 'real-analysis', 'definition', 'general-topology']"
3072828,Why didn't they simplify $x^y=y^x$ to $x=y$?,"Solving $x^y = y^x$ analytically in terms of the Lambert $W$ function This ""solution"" for $x^y=y^x$ should simplify to $y=x$ , but for some reason no pointed that out in the OP. According to the stack exchange, the answer is $y= \frac{-xW(-\frac{ln(x)}{x})}{ln(x)}$ . However, the term $\frac{-ln(x)}{x}$ itself can be rewritten as $$\frac{-ln(x)}{x}=-ln(x)e^{-ln(x)}$$ Therefore, the productlog of that expression should simplify as follows, $y= \frac{-xW(-\frac{ln(x)}{x})}{ln(x)}, \ \ \ \ \ $ $y= \frac{-xW(-ln(x)e^{-ln(x)})}{ln(x)}, \ \ \ \ \ $ $y=\frac{-x(-ln(x))}{ln(x)}=x$ Did this simplification just slip past everyone or is there something wrong about my algebra?",['algebra-precalculus']
3072830,Elliptic Curve has Canonical Bundle $K_E = \mathcal{O}_E$,Let $E$ be an elliptic curve over a field $k$ . I'm looking for a proof that for the canonic divisor $K_E$ we have $K_E = \mathcal{O}_E$ ? RR says $h^0(K_E) = h^0(\mathcal{O}_E)+deg(K_E) +g-1= h^0(\mathcal{O}_E)=1$ since $E$ elliptic. I don't see wy this already imply $K_E = \mathcal{O}_E$ .,"['algebraic-geometry', 'elliptic-curves']"
3072870,About two functions whose Lebesgue integral on all sets of a $\sigma-$algebra are equal,"Let $X$ be an infinite set and $\mathcal{F}$ be a $\sigma-$ algrbra with infinite sets on $X$ . Given on $X$ a measure $\mu$ . Let $f$ and $g$ be two $\mathcal{F}-$ measurable functions. Is it necessary that if $$ \int_A f d\mu = \int_A g d\mu, \forall A \in \mathcal{F}$$ then $f = g$ ( $\mu-$ a.e)? I feel that the answer for this is positive, but I can't prove the statement above nor give a counter-example. Please give me a hint. Thank you.",['measure-theory']
3072893,What is the profinite completion of a free abelian group of infinite rank?,"By definition, profinite completion of a group $G$ is $\widehat{G}=\varprojlim_N G/N$ where $N$ runs through every subgroup of finite index in $G$ .
Let $M=\bigoplus_{n\ge1} \Bbb{Z}$ be a free abelian group of countably infinite rank. $1$ . What is $\widehat{M}$ ? My guess is $\widehat{M}=\prod_{n\ge1}\Bbb{\widehat{Z}}$ . 
Am I right? How can I prove? $2$ . More generally, what is $\widehat{\oplus_{n\ge1}{ C_n}}$ where $C_n$ is cyclic group? Is it ${\prod_{n\ge1}{\widehat{ C_n}}}$ ? Similarly what is pro- $p$ -completions? My questioins are originated from the profinite completion of $\Bbb{Q}^{\times}$ , the multiplicative group of the rational number field. It is known that $\Bbb{Q}^{\times}\cong {\{\pm1\}}\times \bigoplus_{n\ge1} \Bbb{Z} $","['group-theory', 'profinite-groups', 'topological-groups']"
3072954,Proof that the following map $\Phi:\ell^1\to(\ell^\infty)'$ is not surjective,"I am working on the dual spaces of sequence spaces, and I want to show that the map $$
\Phi:\ell^1\to(\ell^\infty)',\qquad(\Phi y)(x)=\sum_{i\in\mathbb{N}}y_ix_i
$$ is not surjective. I have already shown it is a linear isometry. Can I use the Hahn-Banach theorem to find $y'\in (\ell^\infty)'$ such that there is no $y\in\ell^1$ with $\Phi  y=y'$ . I have written my own answer the following way (corollary 4.14 is a corollary in my lecture notes stating that $X$ is seperable if $X'$ is. I have proven earlier that $\ell^\infty$ is inseperable) Proposition 2: The map $$\Phi_\infty:\ell^1\to(\ell^\infty)',\qquad(\Phi_\infty y)(x)=\sum_{i\in\mathbb{N}}x^iy^i$$ is not surjective. The proof is based on the following lemma, which consists of two parts.
Lemma 1. Let $X$ and $Y$ be normed spaces. Then the following claims are true: If $f:X\to Y$ is a surjective linear isometry, then $f$ is a homeomorphism. Let $f: X\to Y$ be a homeomorphism. If $X$ is a seperable space, then $Y$ is a seperable space. Proof of 1: We remark that every isometry is automatically injective. Since $f$ is also surjective, $f$ is a bijection. Now, we calculate $|f|_{op}=\sup\{||f(x)||_Y\,\big|\,||x||_X\leq 1\}=\sup\{||x||_X\,\big|\,||x||_X\leq 1 \}=1<\infty$ , hence $f$ is bounded. Since $f$ is linear, $f$ is continuous. We will now show that $f^{-1}$ is continuous. We remark that $f^{-1}$ is linear. We also remark that for all $y\in Y$ , $||y||_Y=||f(f^{-1}(y))||_Y=||f^{-1}(y)||_X$ , hence $f^{-1}$ is an isometry, too. Then, $|f^{-1}|_{op}=\sup\{||f^{-1}(y)||_X\,\big|\,||y||_Y\leq 1\}=\sup\{||y||_Y\,\big|\,||y||_Y\leq 1\}=1<\infty$ , hence $f^{-1}$ is bounded. It follows that $f^{-1}$ is continuous, hence $f$ is a homeomorphism. \
\
Proof of 2: Let $X$ be a seperable normed space and let $f:X\to Y$ be a homeomorphism. Since $X$ is seperable, there exists a countable dense subset $A\subseteq X$ . Then, $f(A)$ is a countable subset of $Y$ . We will show that $f(A)$ is dense in $Y$ . \
\
Let $V$ be an open set in $Y$ . By continuity of $f$ , $f^{-1}(V)$ is open in $X$ , so $A\cap f^{-1}(V)\neq\emptyset$ . By bijectivity, we see that $\emptyset\neq f(A\cap f^{-1}(V))=f(A)\cap f(f^{-1}(V))=f(A)\cap V$ . This holds for all open sets in $Y$ , hence $f(A)$ is dense in $Y$ . It follows that $Y$ is seperable. We can now prove proposition 2. By theorem 4.6, $\Phi_\infty$ is a well-defined linear isometry. We remark that $\ell^1$ is seperable. It follows by corollary 4.14 that $(\ell^\infty)'$ is inseperable since $\ell^\infty$ is inseperable.\
\
We will give a proof by contradiction. Suppose that $\Phi_\infty$ is surjective. Then, $\Phi_\infty$ is a surjective linear isometry and by lemma 1 part \textit{i}, a homeomorphism. Since $\ell^{1}$ is seperable, it follows by lemma 1 part \textit{ii} that $\Phi_\infty(\ell^{1})=(\ell^\infty)'$ is seperable, but this is a contradiction since we know by corollary 4.14 that $(\ell^\infty)'$ is inseperable. Therefore, our assumption that $\Phi_\infty$ was surjective is false, hence $\Phi_\infty$ is not surjective","['separable-spaces', 'functional-analysis', 'linear-transformations', 'dual-spaces', 'sequences-and-series']"
3073005,throwing a $k$-sided dice until you get every face the same number of times,"You throw a fair $k$ -sided dice repeatedly and keep track of all the results. You stop if you have seen every face exactly the same number of times. What is the expected number of times you have to throw the dice? Is that number even finite? Start with the simplest example of a $2$ -sided dice, ie a fair coin. There is a 50% chance that after 2 flips you will have one head and one tail so you stop. There is a 12.5% chance that after 2 flips you will be at 2:0 and continue but after 4 flips you will be at 2:2, so you stop. One can compute a few more terms but I don't see enough of a pattern to get a summable series. It is not too hard to compute the probability that after $k\cdot m$ throws of a $k$ -sided dice you get every face exactly $m$ times but these probabilities are not independent for different $m$ so I'm not sure whether that is helpful. The relation to random walks looks useful. One can map the results of throwing a coin repeatedly to a random work on the integers. Seeing the same number of heads and tails is equivalent to coming back to the starting point (at zero). This is a well studied problem and it is known that you come back to zero with probability one in a finite time. The number I'm looking for would be the expected time when you get back to zero for the first time but I haven't found any results on that. For $k$ bigger than $2$ (and even) the mapping to random walks is not bijective anymore. If you map the outcome of a $6$ -sided dice throw to a random walk on the $3$ -dimensional integer lattice with each face corresponding to a step in one of the 6 directions that a return to the origin is necessary but not sufficient for having seen each face the same number of times. However, it is known that for a random walk on the integer lattice in dimension of at least $3$ , the chance to return to the origin in finite time is strictly smaller than $1$ . So this should prove that for a dice with at least $6$ faces, the expected number of throws is not finite. Is this a finite number for $k=2$ and $k$ between $3$ and $5$ ? Are there estimates for it?","['elementary-number-theory', 'probability']"
3073059,"$\operatorname{rank}(A) = \operatorname{rank}(B)$, Prove there exist $U, V$ invertible matrices such that: $A = UBV$","$\DeclareMathOperator{\rank}{rank}$$\DeclareMathOperator{\Mat}{Mat}$ Given two matrices $A, B \in \Mat_{m \times n}$ , as $\rank(A) = \rank(B)$ . Prove there exist two invertible matrices: $$U \in \Mat_{m \times m}, V \in \Mat_{n \times n}$$ such that: $$A = UBV$$ My attempt: 
this question essentially is to prove that multiplying a matrix of the left side is equivalent to be preforming operations on the rows, and multiplying a matrix to the right side is  equivalent to be preforming operations on the columns. I don't know how to prove this - so I tried using Linear maps and to prove that using linear mapps, which was so effective - as this does not ""proves"" that for every $A, B$ with an equal rank, there exist $U, V$ so that $A = UBV$ .","['matrices', 'matrix-rank', 'linear-algebra']"
3073075,"What's the difference between ""relation"", ""mapping"", and ""function""?",I think that a mapping and function are the same; there's only a difference between a mapping and relation . But I'm confused. What's the difference between a relation and a mapping and a function ?,"['definition', 'functions', 'terminology']"
3073077,Seeking methods to solve: $\int_0^\infty \frac{\ln(t)}{t^n + 1}\:dt$,"Seeking Methods to solve the following two definite integrals: \begin{equation}
I_(n) = \int_0^\infty \frac{\ln(t)}{t^n + 1}\:dt \qquad J(n) = \int_0^\infty \frac{\ln^2(t)}{t^n + 1}\:dt
\end{equation} For $n \in \mathbb{R},\:n \gt 1$ The method I took was to take the following integral: \begin{equation}
 \int_0^\infty \frac{t^k}{\left(t^n + 1\right)^m}\:dt = \frac{1}{n}B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n} \right)
\end{equation} Where: $0 \leq k \lt n$ . Here let $m = 1$ . Differentiate under the curve with respect to $k$ and taking the limit as $k \rightarrow 0^+$ (via the Dominated Convergence Theorem ), i.e. \begin{align}
\lim_{k \rightarrow 0+} \frac{d}{dk} \left[ \int_0^\infty \frac{t^k}{t^n + 1}\:dt \right] &= \lim_{k \rightarrow 0+} \frac{d}{dk} \left[\frac{1}{n}B\left(1 - \frac{k + 1}{n}, \frac{k + 1}{n} \right) \right] \\
\lim_{k \rightarrow 0+}  \int_0^\infty \frac{t^k \ln(t)}{t^n + 1}\:dt  &= \lim_{k \rightarrow 0+}  \left[\frac{1}{n^2} B\left(1 - \frac{k + 1}{n}, \frac{k + 1}{n}\right)\left[\psi^{(0)}\left(\frac{k + 1}{n} \right) - \psi^{(0)}\left(1 - \frac{k + 1}{n} \right)\right] \right] \\
\int_0^\infty \frac{\ln(t)}{t^n + 1}\:dt&= \frac{1}{n^2} B\left(1 - \frac{1}{n}, \frac{1}{n}\right)\left[\psi^{(0)}\left(\frac{1}{n} \right) - \psi^{(0)}\left(1 - \frac{1}{n} \right)\right] \\
&=- \frac{\pi^2}{n^2}\operatorname{cosec}\left(\frac{\pi}{n}\right)\cot\left(\frac{\pi}{n} \right)
\end{align} Which is our expression for $I_n$ . Taking the same approach but differentiating twice with respect to $k$ we arrive at our expression for $J_n$ : \begin{equation}
 J(n) = \int_0^\infty \frac{\ln^2(t)}{t^n + 1}\:dt = \frac{\pi^3}{n^3}\operatorname{cosec}\left(\frac{\pi}{n} \right)\left[\operatorname{cosec}^2\left(\frac{\pi}{n}\right) + \cot^2\left(\frac{\pi}{n}\right) \right]
\end{equation} And in fact we may generalise: \begin{equation}
    \int_0^\infty \frac{\ln^p(t)}{\left(t^n + 1\right)^m}\:dt = \lim_{k\rightarrow 0}\frac{d^p}{dk^p}\left[\frac{1}{n} B\left(m - \frac{k + 1}{n}, \frac{k + 1}{n}\right)\right]
\end{equation} Where $p \in \mathbb{N}$ This method however was just an extension of another integral. I'm curious, if I had just started with $I_n, J_n$ what alternative methods could be used?","['integration', 'definite-integrals', 'polygamma', 'beta-function', 'limits']"
3073083,How to reduce matrix into row echelon form in NumPy?,"I'm working on a linear algebra homework for a data science class. I'm suppose to make this matrix into row echelon form but I'm stuck. Here's the current output I would like to get rid of -0.75, 0.777, and 1.333 in A[2,0], A[3,0], and A[3,1] respectively; they should be zeroed out. Below is my current code... can anybody please nudge me in the right direction and tell me what step I'm missing? import numpy as np

def fixRowTwo(A) :

    # Sets the sub-diagonal elements of row two to zero
    A[2] = A[2] - A[2,0] * A[1]
    A[2] = A[2] - A[2,1] * A[1]

    # Test if diagonal element is not zero.
    if A[2,2] == 0 :
        # Add a lower row to row two.
        A[2] = A[2] + A[3]

        # Sets the sub-diagonal elements to zero again ???
        A[2] = A[2] - A[2,0] * A[1]
        A[2] = A[2] - A[2,1] * A[1]

    if A[2,2] == 0 :
        print(""S I N G U L A R"")
        sys.Exit()

    # Set the diagonal element to one
    A[2] = A[2] / A[2,2]

    return A

def fixRowThree(A) :

    # Sets the sub-diagonal elements of row two to zero
    A[3] = A[3] - A[3,0] * A[2]
    A[3] = A[3] - A[3,1] * A[2]
    A[3] = A[3] - A[3,2] * A[2]    

    # Test if diagonal element is not zero.
    if A[3,3] == 0:
        print(""S I N G U L A R"")
        sys.Exit()

    # Set the diagonal element to one
    A[3] = A[3] / A[3,3]

    return A

A = np.array([
        [1, 7, 4, 3],
        [0, 1, 2, 3],
        [3, 2, 0, 3],
        [1, 3, 1, 3]
    ], dtype=np.float_)

fixRowTwo(A)
print("""")
print(""Row Two:"")
print(A)

fixRowThree(A)
print("""")
print(""Row Three:"")
print(A)","['matrices', 'gaussian-elimination', 'python', 'linear-algebra']"
3073112,Quadratics: Intuitive relation between discriminant and derivative at roots,"While working with quadratics that have real roots, I realized an interesting fact: The slope of a quadratic at its roots is equal to $\pm \sqrt{D}$ where $D=b^2-4ac$ Proof: $$f(x) = ax^2 + bx +c$$ $$f'(x) = 2ax+b$$ Roots: $$x = \frac{-b\pm\sqrt{b^2-4ac}}{2a}$$ So, if we try to find the slope at any root ( $r$ ): $$f’(r) = \pm \sqrt D$$ where the sign ( $\pm$ ) can be determined by whether the root is on the right of the vertex or the left. If the quadratic has only $1$ root (or $2$ roots that are the same) then it means the quadratic is at a stationary point so the slope must be $0$ . This is backed by the fact that quadratics have only 1 distinct root when $b^2 - 4ac = 0$ . What geometric/intuitive approach can be applied to explain this interesting phenomenon?","['roots', 'slope', 'polynomials', 'derivatives', 'quadratics']"
3073173,Is $ f(x):= \frac{x}{ |x|^n }$ a gradient field?,"for $ 2 \leq n $ let be $ f: \mathbb{R}^n \backslash \{ 0 \} \rightarrow \mathbb{R}^n $ $ f(x):= \frac{x}{|x|^n} $ , $ x\in \mathbb{R}^n \backslash \{ 0 \} $ Is f a gradient field? A vectorfield $ f $ ist a gradient field, if there is a function $ f= \nabla v $ $ \left[ \nabla f (x) = (\frac{ \partial f}{ \partial x_1} (x),...,\frac{ \partial f}{ \partial x_n} (x)) \right] $ So, I need to find a function $v$ so that $ f= \nabla v $ right? If I look for the case $n=2 $ so $ f(x):= \frac{x}{|x|^2} \in \mathbb{R}^n \backslash \{ 0 \} $ let be $ \partial_j v(x)= \frac{x_j}{|x|}$ then $v(x)= ln(|x|) $ and $$ \partial_j v(x)= \frac{1}{|x|} \frac{x_j}{|x|} = \frac{x_j}{|x^2|} $$ Is this the right idea?
If yes, how can I proceed for larger $n$ ?
If no, what would you suggest? Appreciate any help !","['multivariable-calculus', 'vector-analysis', 'real-analysis']"
3073225,Proof of first Fundamental theorem of calculus,"Can you please, check if my proof is correct? Suppose that $f:[a,b]\to \Bbb{R}$ is continuous and $F(x)=\int^{x}_{a}f(t)dt$ , then $F\in C^{1}[a,b]$ and $$\dfrac{d}{dx}\int^{x}_{a}f(t)dt:=F'(x)=f(x)$$ MY PROOF: Credits to Aweygan for the correction Let $x_0\in[a,b]$ and $\epsilon>0$ be given. Since $f$ is continuous at $x_0$ then, there exists $\delta>0$ such that $|t-x_0|<\delta$ implies $$|f(t)-f(x_0)|<\epsilon.$$ Thus, $$f(x_0)=\dfrac{1}{x-x_0}\int^{x}_{x_0}f(x_0)dt,\;\;\text{where}\;\;x\neq x_0.$$ For any $x\in (a,b),$ with $0<|x-x_0|<\delta,$ such that $x_1=\min\{x,x_0\}$ and $x_2=\max\{x,x_0\}$ . So, we have \begin{align}\left| \dfrac{F(x)-F(x_0)}{x-x_0}-f(x_0) \right|&=  \left| \dfrac{1}{x-x_0}\int^{x}_{x_0}(f(t)-f(x_0))dt \right|  \\&\leq  \dfrac{1}{|x-x_0|}\int^{x}_{x_0} \left|f(t)-f(x_0) \right|dt\\&\leq  \dfrac{1}{|x-x_0|}\int^{x_2}_{x_1} \left|f(t)-f(x_0) \right|dt\\&< \dfrac{1}{|x-x_0|}\epsilon|x_1-x_2| \\&\leq \dfrac{1}{|x-x_0|}\epsilon|x-x_0| =\epsilon  \end{align} Hence, $$F\in C^{1}[a,b]\;\;\text{and}\;\;\dfrac{d}{dx}\int^{x}_{a}f(t)dt:=F'(x)=f(x)$$","['proof-verification', 'analysis', 'real-analysis', 'calculus', 'riemann-integration']"
3073235,Is a Rational Rotation Algebra a Cutdown of a Matrix Algebra?,"Let $\theta=m/n$ and let $A_{\theta}$ be the rational rotation C $^{*}$ -algebra with rotation angle $\theta$ . I.e., $A_{\theta}=C^{*}(u,v)$ , where $u$ and $v$ are unitaries such that $vu=e^{2\pi i \theta}uv$ . I know from here that $A_{\theta}$ is not a full matrix algebra. Given that $A_{\theta}$ has irreducible representations only of degree $n$ , is it true that $A_{\theta}$ is a cutdown of a full matrix algebra? I.e., Is there a space $C(X,M_{n}(\mathbb{C}))$ and a projection $p\in C(X,M_{n}(\mathbb{C}))$ , such that $A_{\theta}$ is isomorphic to $pC(X,M_{n}(\mathbb{C}))p$ ?","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
3073255,"Find all non-isomorphic abelian groups s.t. $|G| \leq 30$ and $g^{12}=1, \, \forall g\in G$","Assume the prime factorization of the order of $G$ : $$
|G|=p_1^{a_1}p_2^{a_2}\dots p_r^{a_r}
$$ The condition $$
g^{12}=1,\, \forall g\in G\tag{1}
$$ in other words means that we must find those groups whose elements satisfy: $$
\text{ord} (g) \big|12, \, \forall g \in G
$$ Intuitively, I can see that Condition $(1)$ holds for every $g\in G$ if and only if each $p_i^{a_i}$ divides $12$ , obtaining the cases: $$
\begin{align*}
&\bullet |G|=1 \longrightarrow G=\{0\} \\
&\bullet |G|=2 \longrightarrow G=\mathbb{Z_2} \\
&... \\
&\bullet|G|=24 \longrightarrow G= \mathbb{Z}_2 \times \mathbb{Z}_{12}\bigg|\mathbb{Z}_4 \times \mathbb{Z}_6\bigg|\mathbb{Z}_2 \times \mathbb{Z}_2 \times \mathbb{Z}_2 \times \mathbb{Z}_3 \\
\end{align*}
$$ How could I verify the correctness and prove the sentence in bold? Is it a derivation of Cauchy's theorem ?","['group-isomorphism', 'cyclic-groups', 'abstract-algebra', 'group-theory', 'abelian-groups']"
3073299,Is the $p$-norm ever a norm for $0<p<1$?,"I wonder: Is there a measure space $(X,\Sigma,\mu)$ such that $L^p(\mu)$ form a normed space w.r.t the $p$ -norm, for some $0<p<1$ ?(assuming that $X$ contains more than point). I know that in general the "" $p$ -norm"" is not really a norm for $0<p<1$ . 
(it violates the triangle inequality). I am asking if there are (non-trivial) special cases where it does form a norm.","['lp-spaces', 'normed-spaces', 'functional-analysis']"
3073343,Reverse Littlewood-Offord problem: lower bound for the number of choices of signs such that $|\pm a_1\dots\pm a_n| \leq \max|a_i|.$,"Let $n$ be a positive integer. For $\mathbf{a}\in\mathbb R^n$ let $N(\mathbf{a})$ be the number of choices of $\epsilon\in\{-1,1\}^n$ such that $$\Biggl|\sum_{i=1}^n\epsilon_ia_i\Biggr|\leq \max_{1\leq i\leq n}|a_i|.$$ Example. for $n$ even and $a=(1,\dots,1),$ we are counting $\epsilon$ with $|\sum_{i=1}^n\epsilon_i|\leq 1,$ which forces $\epsilon$ to have exactly $n/2$ entries of $-1,$ so $N(1,\dots,1)$ is the central binomial coefficient $\binom{n}{n/2}.$ Is it true that $N(\mathbf a)\geq \binom{n}{n/2}$ for even $n$ ? I also have a conjectured lower bound for odd $n$ if you're interested. Define $$B_n=N(1,\dots,1,\tfrac12)=2\binom{n-1}{\lfloor (n-1)/2\rfloor}=
\begin{cases}
\binom{n}{n/2}&\text{ for $n$ even}\\
2\binom{n-1}{(n-1)/2}&\text{ for $n$ odd.}
\end{cases}
$$ I've confirmed $N(\mathbf a)\geq B_n$ by picking random vectors $\mathbf{a}$ uniformly at random, but these tests aren't very convincing since it doesn't even find $B_n$ for $n\geq 9.$ For $n=3$ we always have $N(\mathbf{a})\geq B_3=4$ : assume $1=a_1\geq a_2\geq a_3\geq 0,$ then $a_1-a_2-a_3$ and $a_1-a_2+a_3$ and their negations all lie in $[-1,1].$ This would be a reversed form of the Littlewood-Offord problem , where Erdős showed there are at most $\binom{n}{\lfloor n/2\rfloor}$ vectors $\epsilon\in\{-1,1\}^n$ such that $\Bigl|\sum_{i=1}^n\epsilon_ia_i\Bigr|\leq \color{red}{\min}_{1\leq i\leq n}|a_i|.$ It's a version of Tomaszewski's Problem with the $\ell^2$ norm replaced by $\ell^\infty.$ I believe a positive answer would also provide a lower bound for the Minimum number of balanced partitions .","['inequality', 'combinatorics']"
3073361,"Building Intuition for Differential forms, exterior derivative, wedge [duplicate]","This question already has answers here : What's the geometrical intuition behind differential forms? (2 answers) Closed 3 years ago . I think I understood 1-forms fairly well with the help of these two sources. They are dual to vectors, so they measure them which can be visualized with planes the vectors pierce. Gravitation 1973 On the Visualisation of Differential Forms - Dan Piponi But I struggle with the explanations for higher order forms. The goal is to answer and understand these questions with drawings: How can I visualize the wedge between two 1-forms $\alpha\wedge\beta$ ?
I think I understood the wedge between two vectors, as the parallelogram created by the two in a ""area sense"". The determinant comes in to make it only about the area which is why $v\wedge w = \frac{1}{2}v\wedge 2w$ since stretching the parallelogram by two in the w direction is compensated by squishing it in the v direction, so the area stays constant.
So the wedge between two vectors is the area it spans with its vectors.
But how does that translate to the dual space? Where 1-forms measure the length of the component of its dual vector. And can be visualized as planes the vectors pierce through.
What is the visualization between two of these 1-forms as a wedge? Gravitation has this picture: Dan-Piponi drew it like this: Now these pictures make some sense as they are generated by intersecting the 1-forms. But I am not quite getting how the result is evaluated. The result (2-form) should map two vectors as input to a number. And I don't see how these intersections do that. While for 1-forms you count the numbers of planes a vector pierced. Why does it make sense that $d(d\alpha)=0$ for every differential form $\alpha$ What does Dan Piponi mean by saying: ""exterior derivative is none other than finding the boundary of the picture"" (4 Exterior Derivatives) Understand part 5 about Stokes' theorem from Dan Piponi's paper Note: I should maybe add that I have no background in physics, so I didn't understand a lot of the stuff in Gravitation. I just tried to read it after I couldn't quite understand other source since it was cited there. Similar questions: What's the geometrical intuition behind differential forms? Edit (since someone voted ""close"", based on this question as a duplicate):
This question indicates not grasping how 1-forms work (""families of surfaces [...] Why do this interpretation makes any sense?"") not only does that invite explanations for 1-forms and hand-waving away the rest with ""it works similarly in higher dimensions"" but it in particular does not mention specific visualizations for 2-forms which kind of show that there should be an intuition for 2-forms (and maybe higher). And while this question accepted an answer already, this answer does not help to answer the (enumerated) questions above. So this is absolutely not a duplicate. Geometric understanding of differential forms. Visualizing Exterior Derivative","['exterior-derivative', 'intuition', 'soft-question', 'differential-forms', 'differential-geometry']"
3073386,Proof of second Fundamental theorem of calculus,"Is there any other proof of this? Second fundamental Theorem of Calculus: If $f$ is differentiable on $[a,b]$ and $f'$ is integrable on $[a,b]$ , then $$\int^{b}_a f'(t)dt=f(b)-f(a)$$ My proof Since $f$ is differentiable on [a,b], then $f'(t)$ exists for all $t\in[a,b]$ . For each $n\in \Bbb{N}$ , let $P_n$ be an arbitrary partition such that $$a=x_0<x_1<\cdots<x_n=b.$$ Since $f$ is differentiable on $[a,b]$ , then $f$ is continuous on $(a,b).$ So, by Mean Value Theorem, there exists $t_i\in [x_{i-1},x_i]$ , for $1\leq i\leq n$ such that $$ f(x_i)-f(x_{i-1})=f'(t_i)(x_{i-1}-x_i).$$ Summing these up, we have $$ f(b)-f(a)=\sum^{n}_{i=1}\left[f(x_i)-f(x_{i-1})\right]=\sum^{n}_{i=1}f'(t_i)(x_{i-1}-x_i).$$ Integrability of $f'$ on $[a,b]$ implies that $$ \sum^{n}_{i=1}m^{f'}_i(x_{i-1}-x_i)\leq \sum^{n}_{i=1}f'(t_i)(x_{i-1}-x_i)\leq \sum^{n}_{i=1}M^{f'}_i(x_{i-1}-x_i),$$ which implies that $$ L(f',P_n)=\sum^{n}_{i=1}m^{f'}_i(x_{i-1}-x_i)\leq f(b)-f(a)\leq \sum^{n}_{i=1}M^{f'}_i(x_{i-1}-x_i)=U(f',P_n),$$ As $n\to\infty,$ $$\lim\limits_{n\to\infty} L(f',P_n)=\lim\limits_{n\to\infty} U(f',P_n)=\int^{b}_a f'(t)dt.$$ Therefore, $$\int^{b}_a f'(t)dt=f(b)-f(a)$$","['proof-verification', 'analysis', 'real-analysis', 'calculus', 'riemann-integration']"
3073414,"Suppose $a, b\in G$ such that $\lvert a\rvert$ is odd and $aba^{-1}=b^{-1}$. Show that $b^2=e$.","I'm reading ""Contemporary Abstract Algebra,"" by Gallian . This is Exercise 4.30. Suppose $a$ and $b$ belong to a group, $a$ has odd order, and $aba^{-1}=b^{-1}$ . Show that $b^2=e$ . My Attempt: Let $\lvert a\rvert=2n+1$ , $n\in\Bbb N\cup\{0\}$ . We have $$\begin{align}
b&=(b^{-1})^{-1} \\
&=(aba^{-1})^{-1} \\
&=ab^{-1}a^{-1},
\end{align}$$ so, by induction, $b=a(\underbrace{aba^{-1}}_{=b^{-1}})a^{-1}=\dots =a^{2n}ba^{-2n}$ , so that $$\begin{align}
b&=a^{2n}ba^{-2n} \\
&=a^{2n}(ab^{-1}a^{-1})a^{-2n} \\
&=a^{2n+1}b^{-1}a^{-(2n+1)} \\
&=b^{-1},
\end{align}$$ so $b=b^{-1}$ , i.e. , $b^2=e$ . $\square$ Is the above proof okay? I think it is. However , I'd be interested to see how properties of Baumslag-Solitar groups could be used here, since $aba^{-1}=b^{-1}$ brings them to mind.","['alternative-proof', 'group-theory', 'proof-verification']"
3073425,What does the integral of a delta distribution even mean?,"Formally, we define $\delta(\phi)=\phi(0)$ where $\phi$ comes from a suitable class of test function. Based on this, the expression $\int_{-\infty}^{\infty} \delta(x) dx$ seems completely meaningless and I'm unsure how to attribute meaning to integrals involving $\delta$ . I feel like I'm missing something important here - help!!",['functional-analysis']
3073492,Boundary of the image of a continuous function contained in image of the boundary of the domain,"I am having trouble solving the following exercise: Let $U$ be a bounded domain, and let $f(z)$ be a continuous function on $U \cup \partial U$ that is holomorphic on $U$ . Show that $\partial\left( f(U)\right)\subseteq f(\partial U)$ , that is, the boundary of the open set $f(U)$ is contained in the image under $f(z)$ of the boundary of $U$ . One of the main issues I have with this exercise is that my teacher has defined the open mapping theorem as: Open mapping theorem: if $\Omega \subseteq \mathbb{C}$ is a connected open set and $f$ is holomorphic and not constant on $\Omega$ , then $f(\Omega)$ is an open set in $\mathbb{C}$ . The additional hypothesis that the domain be connected means that we cannot apply the theorem directly. This is my approach: Suppose $f$ is not constant (if it is the result is clear). Since $\stackrel{\circ}{U}$ is open, for each $z\in\stackrel{\circ}{U}$ there exists $r_z>0$ such that the open disk of center $z$ and radius $r_z$ , $D(z, r_z)$ , is contained in $\stackrel{\circ}{U}$ . Then we can write $\stackrel{\circ}{U}$ as: $\stackrel{\circ}{U} = \bigcup_{z\in\stackrel{\circ}{U}} D(z, r_z)$ Hence: $f(\stackrel{\circ}{U}) = f\left(\bigcup_{z\in\stackrel{\circ}{U}} D(z, r_z)\right) = \bigcup_{z\in\stackrel{\circ}{U}} f\left(D(z, r_z)\right)$ Since $f$ is holomorphic in $\stackrel{\circ}{U}$ , disks are open connected sets and the arbitrary union of open sets is open, we can apply the Open mapping theorem to conclude that $f(\stackrel{\circ}{U})$ is an open set. This means that $f$ maps interior points of $U$ to interior points of $f(U)$ . Let $w$ be a point of $\partial f(U)$ , then there exists a sequence $w_n \subseteq f(U)$ that converges to $w$ , for $\partial f(U)$ is a compact set (1). By definition of $f(U)$ , for all $k\in\mathbb{N}$ there exists $z_k\in\stackrel{\circ}{U}$ such that $f(z_k) = w_k$ . By hypothesis $U$ is bounded, and therefore $z_k\rightarrow z\in\partial U$ and $f(z_k) \rightarrow f(z) = w\in\partial f(U)$ , which implies: $\forall w \in\partial f(U) : \exists z\in\partial U \mid f(z) = f(w) \implies \partial f(U) \subseteq f(\partial U)$ . One of my concerns is that $U$ is an infinite union of open sets, and I am not sure that $f(\bigcup_{k=0}^{\infty} U_k) = \bigcup_{k=0}^{\infty} f(U_k)$ still holds in such conditions. My other issue is that I am not sure that $(1)$ is true. Thank you.","['complex-analysis', 'analysis']"
3073504,Do we really need $X$ to be compact Hausdorff?,"Let $R= C(X,\mathbb{R})$ be the ring of all continuous real-valued functions on a topological space $X$ , where $\mathbb{R}$ is equipped with the standard Euclidean topology. Suppose $|X|>1$ and $X$ is connected and compact Hausdorff. Then the only idempotents in $R$ are the functions $0$ and $1$ . Prove also that if $R$ is Von Neumann regular then $R$ is a field. Proof: We clearly have that $0^2=0$ and $1^2=1$ . Now take $f \in R$ such that $f$ is idempotent. Then, for all $x \in X$ , we have that $f^2(x)-f(x)=0$ . This equation has only two solutions in $\mathbb{R}$ , namely $f(x)=0$ or $f(x)=1$ . Now define \begin{align*}
A:=\lbrace x \in X \mid f(x)=0 \rbrace=f^{-1}(\lbrace 0 \rbrace) \\
B:= \lbrace x \in X \mid f(x)=1 \rbrace=f^{-1}(\lbrace 1 \rbrace)\end{align*} Since $\mathbb{R}$ is equipped with the standard Euclidean topology, we know that all the singletons are closed subsets of $\mathbb{R}$ . The continuity of $f$ implies that $A$ and $B$ are closed subsets of $X$ . Furthermore, we have that $A \cap B = \emptyset$ and $A \cup B =X$ . Assume that both $A$ and $B$ are non-empty (which is possible since $|X|>1$ ), then connectedness of $X$ gives us a contradiction. So we must have that $A=X$ and $B= \emptyset$ (and thus $f=0$ ), or $A= \emptyset$ and $B=X$ (and thus $f=1$ ). Assume that $R$ is Von Neumann regular. Then, for every non-zero $f \in R$ , we have that $(f)=(e)$ , where $e \in R$ is idempotent. It follows that $(f)=R$ and thus $f$ is invertible. Hence, $R$ is a field. Now, my question is the following: why did we need $X$ to be compact Hausdorff?","['ring-theory', 'general-topology', 'proof-verification']"
3073601,Use an appropriate statistical test to test the manufacturer’s claim at significance level $α=0.05$.,"The manufacturer of the resistors claims that the expected maximum power at which the resistor fails is $0.25$ . You suspect this threshold is lower and therefore decide to measure the current and resistance at which the resistor breaks down. By an ingenious application of Ohm’s Law this results in a sample mean of $\overline{x}_{20} = 0.21$ and a sample variance of $s^2_{20} = 0.01$ for the power. You may assume that the measurements for the power are also normally distributed. Use an appropriate statistical test to test the manufacturer’s claim at significance level $α=0.05$ . I assume that $H_0$ : $\mu=0.25$ $H_1$ : $\mu<0.25$ $P(T\geq0.21|H_0)\leq\alpha$ $Z=\frac{0.21-0.25}{\frac{\sqrt{0.01}}{\sqrt{20}}}=-1.788$ $P(Z\leq-1.788)=P(Z\geq1.788)= 0.0368$ (from the N(0,1) table) I think that what I did is correct but I didn't use the value of $0.05$ for the significance level, how can I use it in this question? What I assumed is correct? Thanks for the help.",['statistics']
3073660,Prove that a semigroup satisfying $a^pb^q=ba$ is commutative,"Let $(S, \cdot)$ be a semigroup. There are natural numbers $p,q \geq 2$ such that $a^pb^q=ba$ for all $a,b \in S$ . Prove that $S$ is commutative. I wrote $$\begin{align}
a^{p+1}b^{q+1} &=b^{(q+1)p}a^{(p+1)q} \\
 &=b^{p}\cdot(b^q)^p \cdot (a^p)^q\cdot a^q \\
 &=b^p\cdot a^p \cdot b^q \cdot a^q \\
 &= b^p\cdot b \cdot a \cdot a^q \\
 &=b^{p+1}a^{q+1}.
\end{align}$$ From the given identity I also got $a^{p+1}b^{q+1}=abab$ . Using $a^{p+1}b^{q+1}=b^{p+1}a^{q+1}$ I then got $abab=baba$ . Making $a=b$ in the statement gives $a^{p+q}=a^2$ . I don't know what to do from there.","['binary-operations', 'group-theory', 'abstract-algebra', 'semigroups']"
3073677,Is there an algebraic function near every smooth function?,"For every real number, you can create rational numbers that are arbitrarily close to it. As a result, rational numbers are a sufficient field for every real-world problem involving real numbers, and as a result of that computers are useful for scientific purposes. Is something similar true of algebraic functions? When I conduct an experiment I can constrain the values of a function to an arbitrary but finite precision. Will I ever be able to rule out the theory that the function I am measuring is algebraic, or are there algebraic functions arbitrarily close to every other function? (I'm excluding, of course, pathological functions like the ones that are continuous nowhere.)","['functions', 'polynomials']"
3073730,Is this (1.11716..) a known/named constant?,"While looking at the Wikipedia entry for the Supergolden ratio aka Narayana's cow constant (cf. https://en.wikipedia.org/wiki/Supergolden_ratio ), I noticed the following construction involving that constant and a 120-degree ( $2\pi/3$ radian) triangle on that page: I then began to wonder about similar constructions with other fractional angles: $2\pi/2$ , $2\pi/4$ , ... I found that an angle of $2\pi/2$ gives a degenerate triangle based on the golden ratio: $2\pi/4$ gives a right triangle based on the square-root of the golden ratio: And $2\pi/6$ gives an equilateral triangle based on unity: Nothing too surprising or new in the results so far, but an angle of $2\pi/5$ (72-degrees) produced the following: So, the positive solution to $-2 - x + \sqrt 5 x - 2 x^2 + 2 x^4=0$ , approximate value $1.11716..$ , minimal polynomial $1
 + x + x^2 + x^3 - x^4 - x^5 - 2 x^6 + x^8$ Is this a known/named constant?","['constants', 'geometry']"
3073776,If $z=\cos\theta + i\sin \theta$ prove $\frac{z^2-1}{z^2+1}=i\tan\theta$,"If $z=\cos\theta + i\sin \theta$ prove $$\frac{z^2-1}{z^2+1}=i\tan\theta$$ Here is my workings, I'm not sure if I've made a mistake or I'm just not spotting what to do next. Any help would be appreciated. $$\frac{(\cos\theta + i\sin \theta)^2-1}{(\cos\theta + i\sin \theta)^2+1}$$ $$\frac{(\cos^2\theta + 2i\sin \theta \cos\theta - \sin^2\theta)-1}{(\cos^2\theta + 2i\sin \theta \cos\theta - \sin^2\theta)+1}$$ $$\frac{(\cos^2\theta - \sin^2\theta)+( 2i\sin \theta \cos\theta) -1}{(\cos^2\theta - \sin^2\theta)+( 2i\sin \theta \cos\theta)+1}$$ $$\frac{\cos2\theta + i\sin 2\theta  -1}{\cos2\theta + i\sin 2\theta +1}$$ I understand how I can do it with using $z=e^{i \theta}$ , however I want to solve it using double angle identities.","['trigonometry', 'complex-numbers']"
3073785,Changing order of integration:$\int_0^\infty\int_{-\infty}^{-y}f(x)\mathrm dx\mathrm dy\Rightarrow\int_{-\infty}^0\int_0^{-x}f(x)\mathrm dy\mathrm dx$,"Why does $$\int_{0}^{\infty} \int_{-\infty}^{-y} f(x)\mathrm dx \mathrm dy \Rightarrow \int_{-\infty}^{0} \int_{0}^{-x} f(x) \mathrm dy \mathrm dx$$ The title is pretty self explanatory. I couldn't see how to properly change the order of the left integeral to the right one. I'd love to hear your thoughts, thanks.","['integration', 'multivariable-calculus', 'change-of-variable']"
3073794,Questions related to the Dirichlet series for $\frac{\zeta'(s)}{\zeta(s)^2}$,"This question is related to the following two functions evaluated with the coefficient function $a(n)=\mu(n)\log(n)$ . (1) $\quad f(x)=\sum\limits_{n=1}^x a(n)$ (2) $\quad\frac{\zeta'(s)}{\zeta(s)^2}=\sum\limits_{n=1}^\infty a(n)\,n^{-s},\quad\Re(s)>1?$ The following plot illustrates $f(x)$ defined in formula (1) above. Figure (1) : Illustration of $f(x)$ defined in formula (1) Question (1) : Is it true $f(x)$ has an infinite number of zero crossings? Question (2) : What are the limits on $f(x)$ predicted by the Prime Number Theorem and the Riemann Hypothesis? The following figure illustrates the Dirichlet series for $\frac{\zeta'(s)}{\zeta(s)^2}$ defined in (2) above in orange where formula (2) is evaluated over the first $10,000$ terms. The underlying blue reference function is $\frac{\zeta'(s)}{\zeta(s)^2}$ . Figure (2) : Illustration of formula (2) for $\frac{\zeta'(s)}{\zeta(s)^2}$ (orange curve) and reference function (blue curve) The following four figures illustrate formula (2) for $\frac{\zeta'(s)}{\zeta(s)^2}$ evaluated along the line $\Re(s)=1$ in orange where formula (2) is evaluated over the first $1,000$ terms. The underlying blue reference function is $\frac{\zeta'(s)}{\zeta(s)^2}$ . The red discrete portions of the plots illustrate the evaluation of formula (2) for $\frac{\zeta'(1+i\,t)}{\zeta(1+i\,t)^2}$ where $t$ equals the imaginary part of a non-trivial zeta zero. Figure (3) : Illustration of formula (2) for $\left|\frac{\zeta'(1+i\,t)}{\zeta(1+i\,t)^2}\right|$ Figure (4) : Illustration of formula (2) for $\Re\left(\frac{\zeta'(1+i\,t)}{\zeta(1+i\,t)^2}\right)$ Figure (5) : Illustration of formula (2) for $\Im\left(\frac{\zeta'(1+i\,t)}{\zeta(1+i\,t)^2}\right)$ Figure (6) : Illustration of formula (2) for $Arg\left(\frac{\zeta'(1+i\,t)}{\zeta(1+i\,t)^2}\right)$ Question (3) : What is the range of convergence of the Dirichlet series for $\frac{\zeta'(s)}{\zeta(s)^2}$ defined in (2) above? Does it converge only for $\Re(s)>1$ , or does it also converge for $\Re(s)=1\land\Im(s)\ne 0$ ? Question (4) : Are there explicit formulas for $f(x)$ and $\frac{\zeta'(s)}{\zeta(s)^2}$ expressed in terms of the non-trivial zeta zeros?","['number-theory', 'dirichlet-series', 'riemann-zeta', 'mellin-transform', 'prime-numbers']"
3073801,Equating the coefficients in $\sum_{n=1}^\infty C_{n-1} \frac{t^n}{n!}=\sum_{n=0}^\infty C_n \frac{t^n}{n!} \sum_{m=0}^\infty \frac{t^m}{m!}$,$$\sum_{n=1}^\infty C_{n-1} \frac{t^n}{n!}=\sum_{n=0}^\infty C_n \frac{t^n}{n!} \sum_{m=0}^\infty \frac{t^m}{m!}$$ In the above equation is it possible to equate the coefficients of $\frac{t^k}{k!}$ ? Or do we need to consider $(\frac{t^k}{k!})^2$ ? Could you please give me an idea for this?,"['power-series', 'number-theory', 'sequences-and-series']"
3073816,Number of solutions $X$ to $AX=XB$ in $\mathbb F_2$,"It is a well-known theorem that in an arbitrary field $F$ , if $A$ is an $m\times m$ square matrix and $B$ is an $n\times n$ square matrix, then there is a unique $m\times n$ solution $X$ to the equation $$AX=XB$$ if and only if $A$ and $B$ share no eigenvalues. My question is this: what can be said about the number of solutions $X\in \mathbb F_2$ to the above equation? Is the number of nonzero solutions equal to the number of common eigenvalues? If so, how can one prove this?","['matrices', 'matrix-equations', 'linear-algebra']"
3073821,Studying on this sum interesting $\sum_{n=1}^{\infty}\frac{{2n \choose n }}{4^n n}$,"I was studying this particular sum $$\sum_{n=1}^{\infty}\frac{{2n \choose n}}{4^n n}$$ and eventually I ended up with is sum $(1)$ $$\sum_{n=1}^{\infty}\frac{{2n \choose n}}{(-4\phi)^n}\cdot\frac{1}{(2n-1)(2n-3)\cdots[2n-(2k+1)]}=F(k)\tag1$$ Where $k\ge 0$ and $\phi=\frac{1+\sqrt{5}}{2}$ $$F(k)=\frac{(-1)^k}{(2k+1)!!\sqrt{\phi^{2k+1}}}\left[\sqrt{\phi^{2k+1}}-\sum_{j=0}^{k}{k \choose j}\phi^{1+j}\right]$$ Here is a few first value of $k=0,1,2,3,...$ $$F(0)=1-\sqrt{\phi}$$ $$F(1)=\frac{\phi+\phi^2-\phi^{3/2}}{3\phi^{3/2}}$$ $$F(2)=-\frac{\phi+2\phi^2+\phi^3-\phi^{5/2}}{15\phi^{5/2}}$$ How do we show that $$(1)=F(k)=\frac{(-1)^k}{(2k+1)!!\sqrt{\phi^{2k+1}}}\left[\sqrt{\phi^{2k+1}}-\sum_{j=0}^{k}{k \choose j}\phi^{1+j}\right]?$$","['golden-ratio', 'closed-form', 'sequences-and-series']"
3073822,Optimization exercise regarding a circle and a function,"A circle $\omega$ centered at $K$ , $(0.5,0.5)$ is tangent to the $x$ - and $y$ -axes. Consider the function $$f(x):=\frac{8}{4+x^2} \; \forall x\in\mathbb{R}$$ and a point $P\in f$ such that the distance between $\omega$ and $P$ is minimal. Determinate the coordinates of $P$ . My attempt so far: It's trivial that the radius of $\omega$ is $\frac{1}{2}$ which might be proven for instance through reductio ad absurdum and the Pythagorean theorem . Now, it is a well-known fact, that the Euclidean distance $d$ between $\omega$ and some point $Q(x_q |f(x_q))$ is $$d=\rvert \sqrt{\big(x_q-\frac{1}{2}\bigr)^2+\big(f(x_q)-\frac{1}{2}\bigr)^2}-\frac{1}{2}\lvert$$ This looks something like this I've tried to simplify the expression and tried to determinate de zeros of the derivative without success since the expression I came up with was to ugly to work with. So, if you've achieved to solve the problem this way 'nicer', I would appreciate if you could show me how. Anyhow, I was wondering if there is a nicer way to approach this problem, rather than the analytical one, which might be the case since this exercise is though for 15-years-old students... Thanks in advanced","['optimization', 'functions', 'circles']"
3073842,Possible spectra of singular Sturm-Liouville problems,"A Sturm–Liouville (SL) eigenvalue problem with separated boundary condition on $[a,b]$ $$(py')'-qy=-\lambda^2wy$$ is regular if $p(x),w(x)>0$ and $p(x),p'(x),q(x),w(x)$ are continuous in the finite interval $[a,b]$ . Otherwise, it becomes singular . Let's just take the homogeneous Dirichlet b.c. if necessary. The regular case exhibits an infinite sequence of discrete eigenvalues and corresponding orthonormal eigenfunctions. But for the singular case, is there any conclusion and condition on the possibilities of the spectrum? By skimming over this wiki page and other materials, I only vaguely know it's possible to have the following only an inifinite sequence of discrete eigenvalues only a continuous spectrum a finite sequence of discrete eigenvalues a discrete/continuous mixture I'm confused by case 3 and case 4. The questions arise from another post that seems to have a finite sequence of discrete eigenvalues. Is only a finite sequence of discrete eigenvalues possible? Or case
3 always implies a complementary continuous spectrum, i.e., case 4? In case 4 with a finite sequence of discrete eigenvalues, can the ranges (I mean the interval between the upper and lower bounds) of discrete and continuous spectra have overlap? Or always exclusive? If exclusive, could there be region(s) not covered by either of the two? how about case 4 with an infinite sequence of discrete eigenvalues? (optional) Examples are also welcome.","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'sturm-liouville', 'functional-analysis', 'spectral-theory']"
3073853,Double conditional expectation of martingale,"Assume $X$ a martingale. A common exercise is showing that every martingale has uncorrelated increments. That is, with $a < b < c< d$ , $$ Cov( X_a - X_b, X_c - X_d) = 0 $$ While there are a few straightforward ways of showing this result, I was curious whether we could reach the same result through noting that, say, $X_a = \mathbb{E}(X_n | \mathcal{F}_a)$ , for some $n$ such, that $\mathcal{F}_a \subset \mathcal{F}_n$ . This way, $$ Cov( X_a - X_b, X_c - X_d) = Cov( \mathbb{E}(X_n | \mathcal{F}_a) - \mathbb{E}(X_n | \mathcal{F}_b), \mathbb{E}(X_n | \mathcal{F}_c) - \mathbb{E}(X_n | \mathcal{F}_d))  $$ And since $\mathbb{E}(\mathbb{E}(X_n | \mathcal{F}_c) - \mathbb{E}(X_n | \mathcal{F}_d))=0$ , this should reduce to four sums, where I'm guessing every member should equate to zero. Namely, $$ \mathbb{E}(\mathbb{E}(X_n | \mathcal{F}_i)\mathbb{E}(X_n | \mathcal{F}_j)) = 0$$ would seem to hold for every $i \neq j$ such, that $\mathcal{F}_{i,j} \subset \mathcal{F}_n$ Is this true, or am I messing things ups? How should we be dealing with double expectation of a multiplication of moments? It's not clear that the two expectations are independent (since $\mathcal{F}_i \subset \mathcal{F}_j$ might hold, or the other way round).","['martingales', 'probability-theory', 'probability']"
3073884,a non-zero continuous function with compact support and its derivatives are zero at zero,"I am looking for a continuous function with support compact, which its derivates are zero at zero, it means $f'(0)=0$ , $f''(0)=0$ ..., I have seen this question All derivatives zero at a point $\implies$ constant function? . So, I know that one answers is a flat function but the difference is that I want a function such that $f(0)\neq0$ Maybe, that function is piecewise function or something different. I've tried to build it but I couldn't, any help, please?","['continuity', 'derivatives', 'real-analysis']"
3073929,Do I have something wrong when solving $y'+2y=6$?,"Solve $$y'+2y=6.$$ When I do $$y'=2(3-y)\implies\int\frac{\mathrm dy}{3-y}=2\int\mathrm dx\implies-\ln{|3-y|}=2x+c\implies3-y=ke^{-2x}\therefore y=\boxed{3-ke^{-2x}},\quad c,k\in\Bbb R.$$ It satisfies the ODE because $$2ke^{-2x}+6-2ke^{-2x}=6=6.$$ However, when I try another solution, namely first solve the homogeneous equation: $$y'+2y=0\implies\int\frac{\mathrm dy}y=-2\int\mathrm dx\implies\ln{|y|}=-2x+c\implies y=ke^{-2x},\quad c,k\in\Bbb R$$ then $y_P=k(x)e^{-2x}$ , so then $$y'_P=k'(x)e^{-2x}-2k(x)e^{-2x}\implies k'(x)e^{-2x}-2k(x)e^{-2x}+2k(x)e^{-2x}=6\implies k'(x)=6e^{2x}\implies k(x)=3e^{2x}\implies y_P=3e^{2x}e^{-2x}=3\therefore y=y_H+y_P=\boxed{3+ke^{-2x}},$$ where this solution also satisfies $y'+2y=6$ , because $$-2ke^{-2x}+6+2ke^{-2x}=6=6.$$ My question is, how can we express both solutions with the same expression of $y$ ? I would like both solutions to be identical, but how? Thanks!",['ordinary-differential-equations']
3073952,How do you differentiate with respect to y?,"Find the gradient of $$z=x^y$$ I understand how to get it with respect to $x$ since $y$ is treated as a constant. But when trying to solve it with respect to $y$ , why is it incorrect to implicitly differentiate and use the product rule: $$\ln(z)=y\cdot ln(x)$$ $$\frac{(z_y)}{z}=\Bigr(y\cdot \frac{1}{x}\Bigr)+(1\cdot \ln(x))$$ $$z_y=z\Bigr(\frac{y}{x}+\ln(x)\Bigr)$$ $$z_y=x^y\Bigr(\frac{y}{x}+\ln(x)\Bigr)$$",['derivatives']
3074001,Proof verification for the equivalence of two sets.,"I have constructed two identical draft proofs for the following question using implications and words. Can you please verify whether they are logically correct. Should I have used De Morgan's Laws? Exercise: Suppose that $C,D$ are subsets of a set $X$ . Prove that $$(X\setminus C){\,}\cap{\,}D =D\setminus C.$$ Proof 1: Suppose that $x\in{(X\setminus C){\,}\cap{\,}D}$ . Then $x\in{(X\setminus C)}$ and $x\in{D}$ . Then ( $x\in{X}$ and $x\notin{C}$ ) and $x\in{D}$ . Then ( $x\in{X}$ and $x\in{D}$ ) and ( $x\in{D}$ and $x\notin {C}$ ). Then $x\in(X \cap D) \cap (D \setminus C)$ . Thus, $x\in(D \setminus C)$ . So, $(X \setminus C) {\,} \cap D \subseteq (D \setminus C)$ . Conversely, suppose that $x\in (D\setminus C)$ . Then $(x\in{D}$ and $x\notin{C})$ . Then $(x\in X$ and $x\in{D}$ ) and $x\notin{C}$ . Then $(x\in{X}$ and $x\notin{C})$ and $x\in{D}$ . Thus $x\in(X\setminus {C})\cap{D}$ . So, $(D\setminus{C}) \subseteq{(X\setminus{C}})\cap{D}$ . Since $(X \setminus C) {\,} \cap D \subseteq (D \setminus C)$ and $(D\setminus{C}) \subseteq{(X\setminus{C}})\cap{D}$ , we have that $(X \setminus C) {\,} \cap D = (D \setminus C)\\$ . Proof 2: Suppose that $x\in{(X\setminus C){\,}\cap{\,}D}$ . Then, \begin{align}
&\implies x\in{(X\setminus C)}{\,}{\,}\text{and}{\,}D \\
&\implies(x\in{X} {\,}\text{and} {\,}x\notin{C}) {\,}\text{and}{\,} x\in{D} \\
&\implies (x\in{X} {\,}\text{and}{\,} x\in{D}) {\,}\text{and} {\,}(x\in{D} {\,}\text{and}{\,} x\notin {C})\\
&\implies x\in(X \cap D) \cap (D \setminus C)\\
&\implies  x\in(D \setminus C).\\ \\
\text{Thus}, (X \setminus C) {\,} \cap D \subseteq (D \setminus C).\\ \\
\end{align} Conversely, suppose $x\in (D\setminus C)$ . Then, \begin{align}
&\implies (x\in{D} {\,}\text{and}{\,} x\notin{C}) \\
&\implies (x\in X {\,}\text{and}{\,} x\in{D}){\,}\text{and}{\,} x\notin{C} \\
&\implies (x\in{X}{\,}\text{and}{\,} x\notin{C}){\,}\text{and}{\,} x\in{D} \\
&\implies x\in(X\setminus {C})\cap{D}. \\ \\
\text{Thus,}{\,}(D\setminus{C}) \subseteq{(X\setminus{C}})\cap{D}.
\end{align} Since $(X \setminus C) {\,} \cap D \subseteq (D \setminus C)$ and $(D\setminus{C}) \subseteq{(X\setminus{C}})\cap{D}$ , we have that $(X \setminus C) {\,} \cap D = (D \setminus C)$ .","['elementary-set-theory', 'proof-verification', 'logic']"
3074068,$\cos\frac\pi{n}$ Analytic expression,"I recently found out that $$\sin\frac\pi5=\frac12\sqrt{\frac{5-\sqrt5}2}$$ Which means that $$\cos\frac\pi5=\frac{1+\sqrt5}4$$ I also recently found that if $n\in\Bbb N$ , $$\sin nx=\sin x\,U_{n-1}(\cos x)\\ \cos nx=T_n(\cos x)$$ Where $T_n$ and $U_n$ are the Chebyshev polynomials of the first and second kinds respectively. They are defined as $$T_n(x)=\frac{n}2\sum_{r=0}^{\lfloor n/2\rfloor}\frac{(-1)^n}{n-r}{n-r\choose r}(2x)^{n-2r}$$ $$U_n(x)=\sum_{r=0}^{\lfloor n/2\rfloor}(-1)^r{n-r\choose r}(2x)^{n-2r}$$ Using these definitions, I am attempting to find a general analytic expression for $\cos\frac\pi n$ . To do so I started with $$\cos nx=T_n(\cos x)$$ $$\cos n\cdot\frac{\pi}{n}=T_n\bigg(\cos\frac{\pi}n\bigg)$$ then setting $w=\cos(\pi/n)$ , $$T_n(w)+1=0$$ So our task is to solve for $w$ . I'm fairly certain that $x=\cos(\pi/n)$ is always the largest real root of $T_n(x)+1=0$ . So I guess that's my question: What Is the largest real root of $T_n(x)+1=0$ ? But as far as I know there are no really simple ways of going about this (but I really don't know much). Could I have a bit of help? Thanks.","['trigonometry', 'roots', 'chebyshev-polynomials']"
3074118,Entire function $f(z)$ grows like $\exp(x^\pi)$ as $x\to+\infty$,"Does there exists an entire function $f(z)$ such that $\lim_{x\to+\infty}f(x)/\exp(x^\pi)=1$ (along the real axis)? I have successfully constructed $f(z)$ when $\pi$ is replaced by a rational number $\frac pq$ . For $\lim_{x\to+\infty}f(x)/\exp(x^{p/q})=1$ , take $$f(z)=\exp(z^{p/q})+\exp(z^{p/q}e^{2/q\pi i})+\exp(z^{p/q}e^{4/q\pi i})+\cdots+\exp(z^{p/q}e^{2(q-1)/q\pi i})$$ It is easy to verify $\lim_{x\to+\infty}f(x)/\exp(x^{p/q})=1$ . Proof of $f(z)$ is entire It is easy to see $f(z^q)$ is entire.   Denote $$g(z)=\exp(z^p)+\exp(z^pe^{2/q\pi i})+\exp(z^pe^{4/q\pi i})+\cdots+\exp(z^pe^{2(q-1)/q\pi i}),$$ $g$ has property $g(z)=g(ze^{2/q\pi i})$ and $f(z)=g(z^{1/q})$ . Let $g(x)=a_0+a_1x+\cdots$ , substituting $g(z)=g(ze^{2/q\pi i})$ repeatedly and solving the simultaneous equation gives $g(x)=a_0+a_qx^q+a_{2q}x^{2q}+\cdots$ . Hence the entirety of $f$ . But for $\pi$ ? I can't take the limit with respect to $p/q$ . I have no idea how to proceed.","['complex-analysis', 'limits', 'entire-functions']"
3074138,Matrices with all non-zero entries.,"I am reading a paper and it uses one of these facts, I would like to know if it has  a simple proof: Let $F$ be an infinite field and $n\ge2$ and integer. Then for any non-scalar matrices $A_1,A_2,...,A_k$ in $M_{n}(F)$ , there exist some invertible matrix $Q \in M_{n}(F)$ such that each matrix $QA_1Q^{-1}, QA_2Q^{-1},..., QA_{k}Q^{-1}$ have all non-zero entries. I just don't know where to start at the first place, could have used diagonalizability but not all non-scalar matrices are diagonalizable. Maybe its too simple, please help. Thanks in advance.","['matrices', 'ring-theory', 'linear-algebra']"
3074164,Complex version of the Fermat last problem,"A complex integer is a complex number $x=m+ni$ where $m,n\in \mathbb{Z}$ . Are there complex integers $x,y,z$ with $x^3+y^3=z^3$ ?","['number-theory', 'complex-numbers']"
3074167,Is the differential of left multiplication still left multiplication?,"Let $A$ be a matrix in the real n-dimensional general linear group $G$ . Let $l: G \to G$ , $l(B)=AB$ be left multiplication by $A$ . Consider the differential of $l$ at the identity matrix $I$ of $G$ , $$l_{\{*,I\}}: T_I(G) \to T_{l(I)}(G)$$ Obviously, $l(I)=A$ . Therefore, the differential is $$l_{\{*,I\}}: T_I(G) \to T_A(G)$$ Is the differential $l_{*,I}$ of $l$ , left multiplication by $A$ , still left multiplication by $A$ ? I think it is, and here is what I've tried: Let $X_I \in T_I(G)$ . We must show $l_{*,I}(X_I)(f) = AX_I(f)$ for all $f \in C^{\infty}_I(G)$ . (Strictly, $f: U \to \mathbb R$ is a smooth function for some $U$ containing $I$ and open in $G$ while the members of $C^{\infty}_I(G)$ are equivalence classes. I think the $C^{\infty}_I(G)$ here is treated as a collection of the representatives of the equivalence classes). First, there exists a smooth a curve $s:(-\varepsilon,\varepsilon) \to G$ that starts at $I$ and whose velocity vector is $X_I$ for some $\varepsilon > 0$ . This means $s(0)=I$ and for each $f$ , we have $$X_I(f) = s'(0)(f) := s_{*,0}[\frac{d}{dt} \mid_0 f] := \frac{d}{dt} \mid_0 f(s(t))$$ Then, for each $f$ , $$l_{*,I}(X_I f) = \frac{d}{dt} \mid_0 f(l(s(t)))$$ Now, $s(t)$ is a matrix for every $t$ , so $l(s(t))=As(t)$ . Therefore, the left hand side is: $$l_{*,I}(X_I f) = \frac{d}{dt} \mid_0 f(As(t))$$ For the right hand side, $$AX_I(f) = As'(0)(f) = A s_{*,0}[\frac{d}{dt} \mid_0 f] = A \frac{d}{dt} \mid_0 f(s(t))$$ By linearity of $X_I$ , $AX_I(f) = X_I(Af)$ . Hence, $$A \frac{d}{dt} \mid_0 f(s(t)) = AX_I(f) = X_I(Af) = s'(0)(Af) =  s_{*,0}[\frac{d}{dt} \mid_0 Af] = \frac{d}{dt} \mid_0 Af(s(t))$$ Therefore, the right hand side is $$AX_I(f) = A \frac{d}{dt} \mid_0 f(s(t)) = \frac{d}{dt} \mid_0 Af(s(t))$$ If $Af(s(t)) = f(As(t))$ , then why? I think $f(s(t)$ and $f(As(t))$ are real numbers while $Af(s(t))$ is a real invertible matrix. I might have said something meaningless above. If everything is meaningful, then (3) below might be helpful, but please explain how exactly (3) is helpful. If $Af(s(t)) \ne f(As(t))$ , then how do I proceed instead, or where have I gone wrong? I think the following are supposed to be helpful, if they are right. Please explain how any of them are helpful or wrong and if I have implicitly used any of them already. $G$ is open in $\mathbb R^{n \times n}$ . Then $T_A(G) \cong \mathbb R^{n \times n}$ . By (1), the differential is $$l_{\{*,I\}}: T_I(G) \to \mathbb R^{n \times n}$$ Also by (1), the tangent vector $X_I$ , which is supposedly a function $X_I: C^{\infty}_I(G) \to \mathbb R$ , is now (the isomorphic pre-image or image of) a constant real n by n matrix. $\mathbb R^{n \times n}$ is isomorphic to $\mathbb R^{n^2}$ . By (4), we have for the coordinate chart $(\mathbb R^{n^2}, x^1, ..., x^{n^2})$ about $s(0)=I$ , where $x^1, ..., x^{n^2}$ are the standard coordinates on $\mathbb R^{n^2}$ , that a basis for $T_I(G)$ is $\{\frac{\partial}{\partial x^1}, ..., \frac{\partial}{\partial x^{n^2}}\}$ , and hence, $$X_I = s'(0) = \sum_{i=1}^{n^2} \dot{s}^i(t) \frac{\partial}{\partial x^i} \mid_{s(0)} = \sum_{i=1}^{n^2} \dot{s}^i(t) \frac{\partial}{\partial x^i} \mid_{I},$$ where $s^i = x^i \circ s$ is the ith component of $s$ Thanks in advance! I think I have an answer: $$Af(s(t)) \ne f(As(t))$$ but $$\frac{d}{dt} \mid_0 f(As(t)) = \frac{d}{dt} \mid_0 Af(s(t))$$ For the left hand side, we have $$\frac{d}{dt} \mid_0 f(As(t)) = (As)_{*,0}[\frac{d}{dt} \mid_0](f)$$ For the right hand side, we have $$\frac{d}{dt} \mid_0 Af(s(t)) = A \frac{d}{dt} \mid_0 f(s(t)) = As'(0)(f) = A(s_{*,0}[\frac{d}{dt} \mid_0])(f)$$","['matrices', 'differential-geometry']"
3074168,Find out a general expression for the coordinates of a point in a square based on certain distances and an angle,"my problem appears to be a very simple one, but I just can't seem to figure it out, maybe I am just overlooking something… The problem is defined as the following (refer to the figure for better understanding): I have a square of side L and I have a variable point O inside the square. The distances d1, d2, d3, and d4 are known and the angle between d1 and the horizontal, which I call $\alpha$ , is also known. The line segments d1, d3 and d2, d4 make the lines AB and CD respectively, which are perpendicular to each other. My aim is to find the coordinates of the point O when the origin of the 2d plane is the bottom left corner of the square (denoted as P) and the X axis is defined by the bottom side of the square, and the Y axis is defined by the left side of the square. In the figure 1, this can be calculated very easily: the coordinate $X=L-d1*cos(\alpha)$ and $Y=L-d4*cos(\alpha)$ . The issue arises when I try and find a general equation to give the coordinates of O. For example in figure 2: $X$ is no longer $L-d1*cos(\alpha)$ but rather is $X=d4*cos(\alpha)$ and $Y$ is no longer $L-d4*cos(\alpha)$ but is $Y=L- d1*sin(\alpha)$ . In other words, the expressions for X and Y keep changing based on certain conditions. Ideally, I would want to have a general expression for X and Y regardless of the situation, but I am okay with having a few conditions. To put this math problem into context, my point O is actually a robotic car, the square represents a boundary that the car moves about in and the distances d1,d2,d3 and d4 are distances to the boundary that are measured from distance sensors that are attached to the car, the angle is measured using an onboard accelerometer and gyro. And I am trying to use these sensor inputs to position the robot on a 2d cartesian plane. Update: I added another figure (figure 3) to clarify certain doubts.","['trigonometry', 'geometry', 'robotics']"
3074189,"Is there a right triangle with angles $A$, $B$, $C$ such that $A^2+B^2=C^2$?","A right angle triangle with vertices $A,B,C$ ( $C$ is the right angle), and the sides opposite to the vertices are $a,b,c$ , respectively. We know that this triangle (and any right angle triangle) has the following properties: $a^2+b^2=c^2$ $a+b>c$ $a+c>b$ $b+c>a$ $A+B+C=\pi$ Can we add the property that $A^2+B^2=C^2$ such that this triangle can be formed? If yes, how to find an example for such triangle, finding $A,B,C,a,b,c$ ?","['triangles', 'trigonometry', 'geometric-inequalities', 'geometry']"
3074221,How to solve a second order partial differential equation involving a delta Dirac function?,"In a mathematical physical problem, I came across the following partial differential equation involving a delta Dirac function: $$
a \, \frac{\partial^2 w}{\partial x^2}
+ b \, \frac{\partial^2 w}{\partial y^2}
+ \delta^2(x,y) = 0 \, , 
$$ subject to the boundary conditions $w(x = \pm 1, y) = w(x, y = \pm 1) = 0$ .
Here $a, b \in \mathbb{R}_+$ and $\delta^2(x,y) = \delta(x)\delta(y)$ is the two-dimensional delta Dirac function. While solutions for ODEs with delta Dirac functions can readily be obtained using the standard approach, I am not aware of any resolution recipe for PDEs with delta Dirac functions. Any help or hint is highly desirable and appreciated. Thank you","['dirac-delta', 'elliptic-equations', 'calculus', 'partial-differential-equations', 'greens-function']"
3074283,What happens to inequalities when $\epsilon$ $\to$ $0$?,"I recently came across a proof in topology. There was an inequality in which $A$$<$$B$ + $\epsilon$ (strictly less than),But when $\epsilon$ $\to$ $0$ , then it was inferred that $A$ <= $B$ . Here $B$ is the infimum of a sequence and we are using the basic definition of infimum.(There is always a number between infimum and infimum + $\epsilon$ ) How is this possible?","['general-topology', 'measure-theory', 'real-analysis']"
3074336,Evaluating Dirichlet $L$-functions at $s=1$,"I'm trying to find references on general methods for evaluating Dirichlet $L$ -functions at $s=1$ , but it's proving a little harder to google than I'd hoped. Specifically I'm looking for any books or papers that go through methods for solving the following question: suppose I have a (possibly primitive) Dirichlet character $\chi$ of conductor $q$ , what is the value of $L(1,\chi)$ ? Any insights anyone has about this that aren't a book or paper would also be very much appreciated. This question has a good amount of detail on how to approach the problem, but it only covers odd characters. The Wikipedia page for the analytic class number formula also has this section , which gives closed forms for $L$ -functions at $s=1$ , but this is only for primitive characters with prime conductors (and possibly only for quadratic characters?) This is as much as I could find, and as for the more general cases I'm at a bit of a loss.","['number-theory', 'l-functions', 'closed-form']"
3074357,Axioms independence in Rosser system,"In textbook, I see a method to judge axiom independence: Axiom Systems .
One example is in the link, and in this interpretation, R.S1 is not always A, while the other two are A-tautology, so R.S 1 is independence. 
And I hope to know if there is some method to find the specific interpretation to make an axiom not always A. I tried to make it for R.S.3 but failed, there are too many choices and when I change one, I need to change many other items.","['logic', 'discrete-mathematics']"
3074386,Cardinal exponentiation without generalized continuum hypothesis,"First I have to confess that I don't know about set theory language. Let $A$ and $B$ be infinite cardinals with $A>B$ . My question is: $A^B=A$ ? (without assuming generalized continuum hypothesis) Remark: assuming generalized continuum hypothesis (GCH briefly), 
this can be proved by the following (at least for unlimit cardinal). Sps $A$ is a unlimit cardinal.
Then $A=2^C$ for some $C\ge B$ by GCH. Therefore $A^B = (2^C)^B=2^{CB}=2^C=A$ . Unfortunately, I don't know how to prove for limit cardinal case. Please somebody help me!","['elementary-set-theory', 'cardinals']"
