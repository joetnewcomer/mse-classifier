question_id,title,body,tags
4019837,"This (rather long) implicit equation has a short explicit solution, but how can it be found?","I am curious if a method exists for solving for $k$ or $h$ in this implicit equation: $$\frac{k^2}{h} \mathrm{sech}^2(k) \sqrt{1 + \left(\frac{k}{h} \tanh(k)\right)^2} = \ln\left( \frac{k}{h} \tanh(k) + \sqrt{1 + \left(\frac{k}{h} \tanh(k)\right)^2}\right)$$ Both $k=0$ and $h=\pm k\, \mathrm{sech}(k)$ are solutions. I can easily check that these work, but I have been unable to find a way to manipulate the implicit equation to get these solutions. I have tried writing the hyperbolic functions in terms of their equivalent exponential functions (like $\tanh(x)=\frac{e^{2x}-1}{e^{2x}+1}$ , etc.), raising $e$ to the power of both sides (to remove the log), and (unsuccessfully) factoring the part under the root. Even Wolfram Alpha says, ""Standard computation time exceeded...""! Again, my question is about what techniques could be used to find these explicit solutions (especially $h=k\, \mathrm{sech}(k)$ ) if I did not already know them. Background I am currently taking Calculus 2. A couple weeks ago, for fun, I decided to try to calculate the shape of a soap film between two parallel rings and the maximum distance between the rings before the bubble would pop. Since the soap tries to minimize surface area, it forms the shape of a catenoid (the surface of revolution of hyperbolic cosine). I let the radius of the rings equal $1$ , $h$ equal half the distance between the rings, and $r$ equal the radius halfway between the rings (the minimum of $cosh$ ). I found that letting $k = \mathrm{sech}^{-1}(r)$ simplified things. Using integration, I found the surface area of $y=\mathrm{sech}(k)\cosh\left(\frac{k}{h}x\right)$ revolved around the X-axis between $-h$ and $h$ . The bubble should then converge to the nearest local minimum of this area. Taking the derivative of this with respect to $k$ and setting it equal to zero gave me the above equation. After trying to solve the equation for a while, I found this site that says the maximum separation between the rings was the maximum height of $\frac{x}{\cosh(x)}$ . I tried this expression in my equation, and, to my amazement, it worked.","['calculus', 'minimal-surfaces', 'problem-solving', 'hyperbolic-functions']"
4019856,How can one justify if the graph is planar from adjacency matrix?,"Given an adjacency matrix of a graph $G$ , I was asked to do the following without drawing the graph: A) Find the vertex of largest degree. B) Does the graph have an Euler Circuit? C) Is the graph Planar? I did A and B. How can one justify if the graph is planar from adjacency matrix?","['graph-theory', 'eulerian-path', 'combinatorics', 'discrete-mathematics', 'planar-graphs']"
4019900,Riemann-Stieltjes integral of a continuous function w.r.t. a step function,"$
\newcommand{\para}[1]{\left( #1 \right)}
\newcommand{\abs}[1]{\left| \, #1 \, \right|}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\ds}{\displaystyle}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\norm}[1]{\left \Vert #1 \right \Vert}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\vp}{\varphi}
\newcommand{\ve}{\varepsilon}
\newcommand{\a}{\alpha}
\newcommand{\g}{\Gamma} 
\newcommand{\og}{\overline{\Gamma}}
$ Definition: I define the notion of an ""increasing partition"" $P$ of $[a,b]$ to be a finite partition $\set{z_k}_{k=0}^n \subseteq [a,b]$ where $a = z_0 < z_1 < \cdots < z_n = b$ . It's a minor thing that helps reduce the repetition in the wording herein. Problem Statement: Let $\vp : [a,b] \to \Bbb R$ be a step function with $\set{\a_i}_{i=1}^m$ its discontinuities forming an increasing partition of $[a,b]$ . Thus $\vp$ is constant on each interval $(\alpha_{i-1},\alpha_i)$ . Let $f : [a,b] \to \Bbb R$ be continuous, and let \begin{alignat*}{99}
\vp(\alpha_i^+) &:=\;&& \lim_{x \to  \a_i^+} \vp(x) &&\quad \forall i \in \set{0,1,\cdots,m-1} \\
\vp(\a_i^-) &:=\;&& \lim_{x \to \alpha_i^-} \vp(x) &&\quad \forall i \in \set{0,1,\cdots,m-1} \\
d_i &:=\;&& \vp(\a_i^+) - \vp(\a_i^-) &&\quad \forall i \in \set{1,2,\cdots,m-1} \\
d_0 &:=\;&& \vp(\a_0^+) - \vp(\a_0) \\
d_m &:=\;&& \vp(\a_m) - \vp(\a_m^-)
\end{alignat*} Show $$
\int_a^b f \, \dd \vp = \sum_{i=1}^m f(\a_i) d_i
$$ Context: This ultimately comes up as a homework assignment for me, and I just wanted to check my solution, because I have some doubts on it. I'll express those at the end, though, since they tie more intimately into my proof. Attempted Proof: Let the following be: Let $\g_\a := \set{\a_i}_{i=0}^m$ be an increasing partition of $[a,b]$ . (This is the set of discontinuities of $\vp$ , i.e. $\vp$ is constant on $(\a_{i-1},\a_i)$ $\forall i$ .) Let the definitions in the problem statement hold. Let $\ve > 0$ . Let $\ds \delta := \min_{1 \le i \le m} \left| \a_i - \a_{i-1} \right|$ . Let $\g := \set{x_i}_{i=0}^n$ be an increasing partition of $[a,b]$ with $\norm \g < \delta$ . Let $\ol \g := \g \cup \g_\a$ , and let it be written $\ol \g := \set{\ol{x_i}}_{i=0}^p$ . Let $\ol{\xi_i} \in [\ol{x_i}, \ol{x_{i-1}}]$ for each $i$ . (These are our representative valuation points in the definition of the integral.) By the merit of $\ol \g$ being a refinement of $\g$ , and by merit of definition, we have $$
\norm{ \ol \g } \le \norm \g < \delta \le \norm{\g_\a}
$$ By the Cauchy criterion for Riemann-Stieljes integration, then, where $R_\g$ denotes the usual Riemann-Stieltjes sum, $$
\abs{ R_\g - R_{\ol \g} } < \ve
$$ and so $R_{\ol \g}$ equals the desired integral. It remains to see if this equals the desired sum. Write $$
R_{\og} = \sum_{j=1}^p f \para{ \ol{ \xi_j } } \Big( \vp \para{ \ol{x_j} } - \vp \para{ \ol{x_{j-1}} } \Big)
$$ From definition, we may have either $\ol{x_i} = \alpha_j \in \g_\a$ for some $i$ (i.e. is a discontinuity of $\vp$ ), or $\ol{x_i} = x_i \in \g$ for some $i$ . Note that for any consecutive pairing $\ol{x_{i-1}},\ol{x_{i}}$ that not both are in $\g_\a$ . (This is because each are in $\og$ for which $\norm \og < \delta$ , and thus the smallest distance between any two discontinuities of $\vp$ is bigger than the distance between any two members of $\og$ .) Thus, we only have three cases to consider, based on how many of a given pair are, indeed, discontinuities: Case 1 (neither): In the case neither $\ol{x_{i-1}},\ol{x_{i}}$ are discontinuities of $\vp$ , then trivially we will have $$
\vp \para{ \ol{x_j} } - \vp \para{ \ol{x_{j-1}} } = 0
$$ Case 2 (right discontinuity): Suppose $\ol{x_{i}} = \a_j$ for some $j$ (but, again, that means $\ol{x_{i-1}}$ is not a discontinuity). Then in such a case, using the piecewise continuity and constantness of $\vp$ , $$
\vp \para{ \ol{x_j} } - \vp \para{ \ol{x_{j-1}} } = \vp\para{ \a_j } - \vp \para{ \a_{j}^- }
$$ Case 3 (left discontinuity): Similar to the previous, we have $\ol{x_{i-1}} = \a_j$ for some $j$ , and take $$
\vp \para{ \ol{x_j} } - \vp \para{ \ol{x_{j-1}} } = \vp\para{ \a_j^+ } - \vp \para{ \a_{j} }
$$ Let $\ol{\xi_{i,j}}$ be the $\ol{\xi_i}$ whose terms are not eliminated by merit of Case 1 but lie in Case 2; similarly, $\ol{\xi'_{i,j}}$ for those whose terms lie in Case 3. Then we have, once the terms have negated, $$
R_{\og} = \underbrace{\sum_{j=1}^m f \para{ \ol{\xi_{i,j}} } \Big( \vp\para{ \a_j } - \vp \para{ \a_{j}^- } \Big) }_{\text{case 2 terms}} + \underbrace{\sum_{j=1}^m f \para{ \ol{\xi'_{i,j}} } \Big( \vp\para{ \a_j^+ } - \vp \para{ \a_{j} } \Big)}_{\text{case 3 terms}}
$$ Note that, for the Case 2 terms, each $\ol{\xi_{i,j}}$ is within $\delta$ of $\a_j$ ; specifically, $\ol{\xi_{i,j}} \in (\a_j - \delta,\a_j)$ . Likewise, $\ol{\xi'_{i,j}} \in (\a_j,\a_j+\delta)$ . Since $f$ is continuous, we may take each to be its corresponding $\a_j$ , and write $$
R_{\og} = \sum_{j=1}^m f \para{ \a_j } \Big( \vp\para{ \a_j } - \vp \para{ \a_{j}^- } \Big)  +\sum_{j=1}^m f \para{ \a_j } \Big( \vp\para{ \a_j^+ } - \vp \para{ \a_{j} } \Big)
$$ Some clear cancellation results, giving $$
R_{\og} = \sum_{j=1}^m f \para{ \a_j } \Big( \vp\para{ \a_j^+ } - \vp \para{ \a_{j}^- } \Big) 
$$ Taking the usual convention for what occurs for the $\a_j$ which are $a,b$ , we see the $\vp$ difference here is indeed $d_i$ . So we see $$
\int_a^b f \, \dd \vp  = R_{\og} = \sum_{j=1}^m f \para{ \a_j } d_i
$$ as desired. My Questions / Issues: While I feel like this proof is on the right path, I have some anxieties about some steps. Some in particular I feel aren't quite ""there"" yet. I was wondering if anyone would be able to help me flesh these ideas uot. The use of the Cauchy criterion for Riemann-Stieltjes integrals (stated here) feels far too un-fleshed out, and I doubt it actually lets me conclude that $R_\og$ equals the integral in question. I imagine it only lets me conclude the integral exists at best. I do see that this proof seems to use something similar towards the end of their proof, but their motivation, choice of bound, and the weird $\delta$ 's all escape me. The second chunk of the proof (verifying the alternate form for $R_\og$ ) feels right, but is there any issue there? Something feels sketchy about how I waited until then to define the $\ol{\xi_{i,j}}$ terms to be $\a_j$ in particular, but I don't see how it would be problematic either... I feel like I barely used the continuity of $f$ at all, which means its purpose is lurking elsewhere. Where would that be? Thanks for the insights and thoughts you guys can give!","['integration', 'stieltjes-integral', 'real-analysis', 'solution-verification', 'riemann-integration']"
4020015,Get wrong answer on $\frac{\partial \mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}$ when using graph,"I can use the product rule to obtain $\frac{\partial \mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{x}^{\top} \frac{\partial \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}+(\mathbf{A} \mathbf{x})^{\top} \frac{\partial \mathbf{x}}{\partial \mathbf{x}}$ , and get the right result $\mathbf{x}^{\top}\left(\mathbf{A}+\mathbf{A}^{\top}\right)$ . Here $x$ is a vector, $A$ is a matrix that is not a function of $x$ . However, when I try to calculate by using a graph, the result is different. I am not sure at which step I made the mistake. Please point out where I made it wrong. I used the graph attached ( red numbers are the derivatives) and the equations as follows. Let $
q_{2}=q_{1}x$ , $q_{1}=(A^{T} x)^{T}$ ,  and $k=A^Tx$ . I want to find $\frac{d q_{2}}{d x}$ $\frac{d q_{2}}{d x}=\frac{\partial q_{2}}{\partial x}+\frac{\partial q_{2}}{\partial q_{1}} \frac{\partial q_{1}}{\partial x}= q_{1}+x \frac{\partial q_{1}}{\partial x}= q_{1}+x \frac{\partial q_{1}}{\partial k} \frac{\partial k}{\partial x}  =x^TA+xIA^T = x^TA+xA^T \ne x^{\top}\left({A}+{A}^{\top}\right)$ , $I$ is the NxN identity matrix. The result I got here is different. I know it is not correct, but I couldn't figure out where I lost it. Thank you!","['scalar-fields', 'inner-products', 'multivariable-calculus', 'derivatives', 'quadratic-forms']"
4020035,determine whether a point of a set is in the interior point,"Let $A \subseteq \Bbb R^2$ with $A = \{(x,y): 1<x<4, 1<y<3 \}$ .
Is the point $(2,2)$ be an interior point of $A$ ?
Is the point $(4,2)$ be an interior point of $A$ ?
Justify! Attempt: Edit: I know that the answer is yes and no, respectively.
But, I'm not sure how to show it. To show: $(2,2)$ is an interior point of $A$ .
Let $w \in B(u,r)$ with $u=(2,2)$ . Let $r=\frac{1}{2}>0$ and $k=\frac{1}{2} - ||w-u||$ . Then, $k > 0$ since $||w-u|| < \frac{1}{2}$ . Let $x \in B(w,k)$ . Then, by the triangle inequality, \begin{equation*}
||x-u|| \le ||x-w|| + ||w-u|| \le k + ||w-u|| = \frac{1}{2}.
\end{equation*} Thus, $x \in B(u,\frac{1}{2})$ so that $B(w,k) \subseteq B(u,\frac{1}{2}) \subseteq A$ .
Hence, $w$ is an interior point of $A$ such that $B(u,\frac{1}{2}) \subseteq int(A)$ and so, $u$ is an interior point of $A$ . How to show it? On the other hand, a point $x \in A$ is an interior point of $A$ if there exists $r>0$ such that $B(x,r) \subseteq A$ i.e.
there exists $r>0$ such that for all $y \in \Bbb R^2, ||y-x||<r \implies y \in A$ .","['multivariable-calculus', 'general-topology']"
4020044,"Counting the number of ways to get a sum of $2012$, given that any number of cards can be chosen","A person has $32$ cards out of which $10$ cards, each of blue, green and red colours, have
denominations as $\{2^1, 2^2,\cdots , 2^{10}\}$ and one black and one white each of whose value is unity. What is the number of ways in which the person can get a sum of $2012$ if he can choose any number of cards? I couldn't think much in it, except that number of ways of choosing $\text{(blue,black,white)}$ is equal to if he chooses $\text{(green,black,white) or (red,black,white)}$ . So if suppose he chooses $\text{(blue,black,white)}$ , the total sum of the cards would be $$1+1+2+2^2+\cdots+2^{10}=2^{11}=2048$$ So, if we remove the cards whose sum equals $36$ , we will get $2012$ . Thus, we need to find the number of ways in which the sum on cards equals $36$ . I tried it, but there were so many cases, that I gave up. I also haven't taken the case if he chooses for instance $\text{(blue,green,white)}$ etc. The answer to this is $1007^2$ , which suggests some easy method must be there. Any hints would be appreciated!","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
4020101,Matrix equation with diagonal matrix,"Let $A \in\mathbb{R}^{n\times n}$ be a given Positive-Semi Definite matrix , $\theta\in\mathbb{R}^n$ a vector, $x\in\mathbb{R}$ an unknown variable, and $a\in\mathbb{R}^+$ a positive constant. I have the following equation $$
\theta^TA(A+xI)^{-1}(A+xI)^{-1}A\theta  = a
$$ where $I$ is the identity matrix. Is it possible to solve this equation for $x?$ What I have tried: $$
\text{trace}(\theta^TA(A+xI)^{-1}(A+xI)^{-1}A\theta)  = \text{trace}(a)
\\\theta^TAA\theta  \cdot\text{trace}((A+xI)^{-1}(A+xI)^{-1})  = a
\\ \sum_{i = 1}^n\frac{1}{(x+s_i)^2}=\frac{a}{\|\theta\|_{A^2}^2}
$$ where $s_i$ are the singular values of the $A$ . Is it correct? How can i proceed from here?","['matrices', 'matrix-equations', 'linear-algebra', 'matrix-calculus']"
4020158,Find the sum: $\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n$ [duplicate],"This question already has an answer here : How to find sum of this series $\sum_{0}^{\infty} \frac{(n!)^{2}x^{n}}{(2n)!}$ (1 answer) Closed 8 months ago . The community reviewed whether to reopen this question 8 months ago and left it closed: Original close reason(s) were not resolved Find the sum: $$\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n$$ My try: I played a bit with the coefficient to make it look easier/familiar: First attempt: $$\begin{align}
\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n
&= \sum_{n=0}^\infty \frac{n!}{2^n(2n-1)!!}x^n \\
&= \sum_{n=0}^\infty \frac{n!}{(2n-1)!!}\left(\frac x2\right)^n
\end{align}$$ Second attempt: $$\begin{align}
\sum_{n=0}^\infty \frac{(n!)^2}{(2n)!}x^n
&= \sum_{n=0}^\infty \frac{n!\cdot n!}{(n+n)!}x^n \\
&= \sum_{n=0}^\infty \frac{1}{{2n \choose n}}x^n
\end{align}$$ However, I could not proceed with any of them. Also, I have figured out that the convergence radius is $4$ . My research: I have also found the same sum has been discussed at AoPS , which unfortunately uses Beta function that my course has not covered yet. Entering the sum to Wolphram Alpha , I got the following output for the partial sum : $$\sum_{n=0}^k\frac{(n!)^2x^n}{(2n)!}=\frac{4\sqrt{x}\left(\sin^{-1}\left(\frac{\sqrt{x}}{2}\right)-\frac{2^{2k}k!(k+1)!B_\frac{x}{4}\left(k+\frac{1}{2},\frac{3}{2}\right)}{(2k)!}\right)}{(4-x)^{3/2}}+\frac{4}{4-x}.$$ My background: As I have already mentioned, I cannot use Gamma, Beta or similar functions. I only know about the convergence theorems on functional series and operations on them. So, I'm looking for some method that uses quite elementary tricks. Thanks in advance.","['summation', 'real-analysis', 'calculus', 'sequences-and-series', 'power-series']"
4020205,Continuous version of Kingman's subadditive ergodic theorem.,"Let $T$ be a measure-preserving transformation on the probability space $(\Omega, \mathcal{A},\mathbb{P})$ . K ingsman's subadditive ergodic theorem states the following : Let $(g_n)_n$ a sequence of random variables in $L^1(\Omega)$ such that $$g_{n+m}(\omega) \leq g_n(\omega) + g_m(T^n \omega), \quad \forall n,m>0$$ for almost every $w \in \Omega$ . Then there exists $g : \Omega \rightarrow \mathbb{R}$ a $T$ -invariant function such that $$\underset{n \rightarrow + \infty}{\lim} \frac{g_n(\omega) }{n} = g(\omega)$$ for almost every $w \in \Omega$ . Moreover, if $T$ verify the ergodic property (every $T$ -invariant function is constant almost everywhere on $\Omega$ ), then $g$ is a constant function. I am looking for a continuous version of this theorem. I would like a theorem that works on the following framework : Let $(\Omega, \mathcal{A}, \mathbb{P})$ a probability space equipped with a group of transformation $(\tau_x)_{x \in \mathbb{R}^d}$ such that $(\Omega, \mathcal{A}, \mathbb{P},(\tau_x)_{x \in \mathbb{R}^d} )$ forms a $d$ -dimension dynamic system. Let $(w \mapsto g(x,\omega))_{x \in \mathbb{R}^d}$ a family of random variables in $L^1(\Omega)$ indexed by $x \in \mathbb{R}^d$ and assume that $$g(x+z,\omega) \leq g(x,\omega) + g(x,\tau_z \omega), \quad \forall x,z \in \mathbb{R}^d$$ and for a.e $w \in \Omega$ . Is it true that we can find $h$ a $\tau$ -invariant function (i.e $h(\tau_x \omega)=\omega, \ \forall x \in \mathbb{R}^d, \ a.e \ \omega \in \Omega$ ) such that $$\underset{R \rightarrow + \infty}{\lim} \frac{1}{|B_R|} \int_{B_R} g(x,\omega) \ \mathrm{d}x = h(\omega)$$ for almost $w \in \Omega$ , where $B_R$ stands for the ball in $\mathbb{R}^d$ of radius $R$ and center $0$ . Moreover, if I assume the following : the field $g : \mathbb{R}^d \times \Omega$ is stationary (i.e there exists a random variable $\tilde{g} \in L^1(\Omega)$ such that $g(x,\omega)=\tilde{g}(\tau_x \omega)$ for all $x \in \mathbb{R}^d$ and almost all $w \in \Omega$ ) the system $(\Omega, \mathcal{A}, \mathbb{P},(\tau_x)_{x \in \mathbb{R}^d} )$ is ergodic can I say that $h$ is a constant function (yes thanks to ergodicity) and do we have $h=\mathbb{E}[\tilde{g}]$ ?","['measure-theory', 'ergodic-theory', 'probability-theory', 'dynamical-systems']"
4020230,Asymptotic solution to the differential equation $y''+(\pi^2+\varepsilon)y+y^2=0$ with $y'(0)=y'(1)=0$,"In studying perturbation methods, I am stuck with the following problem. Let $\varepsilon>0$ be a small parameter. Find the asymptotic approximation to the non-constant solution of the differential equation $$
y''+(\pi^2+\varepsilon)y+y^2=0,\\
y'(0)=y'(1)=0.
$$ I tried to solving it by perturbation method. Assume that we have the expansion $$
y(x)=\varepsilon^\alpha y_1(x)+\varepsilon^\beta y_2(x)+\cdots, \quad (*)$$ where $0<\alpha<\beta$ . Substituting it into the original differential equation, at leading order $O(\varepsilon^\alpha)$ , we obtain $$
y''_1+\pi^2 y_1=0,\\
y'_1(0)=y'_1(1)=0,
$$ which yields $y_1(x)=A\cos(\pi x)$ , where $A$ is a constant. To find the value of $A$ , we need to look at the next order, which is $$
\varepsilon^\beta y''_2+\varepsilon^\beta y_2+\varepsilon^{1+\alpha }y_1+\varepsilon^{2\alpha} y_1^2=0,\\
y'_2(0)=y'_2(0)=0$$ I tried to balance the terms by choosing $\beta=1+\alpha$ or $\beta=2\alpha$ or $1+\alpha=2\alpha$ or $\beta=1+\alpha=2\alpha$ . However, in each of the cases,  I ended up with $A=0$ when substituting the solution of $y_2$ into the boundary conditions. On the other hand, integrating the original differential equation once, we have $$
y'^2+(\pi^2+\varepsilon)y^2+\frac{2}{3}y^3=C,$$ where $C$ is a constant. Using the boundary conditions $y'(0)=y'(1)=0$ , we can find the values of $y(0)$ and $y(1)$ once $C$ is specified. By separating variables, we see that $$
\int_{y(0}^{y(1)}\frac{dy}{\sqrt{C-(\pi+\varepsilon^2)y^2-\frac{2}{3}y^3}}=\int_{0}^1 dx=1,$$ which can be used to find the constant $C$ . Numerical calculations show that for small $\varepsilon>0$ , $C$ exists, so the differential equation has non-constant solutions for small $\varepsilon>0$ . For instance, when $\varepsilon=0.01$ , $C\approx 1.1699$ and the graph of a non-constant solution plotted by Mathematica is I think that problem is that the asymptotic expansion of the non-constant solution may not be of form (*), but I don't know how what the correct form is.","['asymptotics', 'ordinary-differential-equations', 'perturbation-theory']"
4020247,What are the chances of rain on particular days,"I study maths as a hobby and am on to elementary theory of probability. I have come across this problem: Find the probability of events A, B and C given it rained on exactly 2 days last week. A: it rained Monday and Tuesday B: it rained on 2 consecutive days C: it rained neither Monday or Tuesday. My first thought was to work from the assumption that there was  a 2/7 chance of rain over the week and work out the probability for any particular day.
But then I thought I should start by working out the total number of outcomes for 2 days rain in one week, which I take to  be $\binom {7}{2} = 21$ Now the answers given in the book are $\frac{1}{21}, \frac{2}{7}, \frac{10}{21}$ I can see that the probability of any particular 2 days having rain is $\frac{1}{21}$ so can see how the the first answer is correct, but even then I have doubts. As for the 2nd and 3rd answers, I cannot see where these come from.",['probability']
4020338,How to find $dT$? Thermodynamics (equation of state),"I recently had my first lesson thermodynamics, and there is an exercises that I don't quite understand how you have to get the answer. It goes as follows: $$PV^{2}=nRe^{3T}$$ we are given that n en R are both constants. The question is what is $dT$ ? I don't think I have the right mathematical background to solve this question. I really would love to know where I could find the name of such kind problems. I now how to derive (and partial derivation), but I'm used to working with $\frac{dy}{dx}$ , but know it's only $dy$ or in my case $dT$ . If someone could help, solve this problem and where I can find theory to practice such mathematical problems that would be very helpful Thanks in advance","['derivatives', 'mathematical-physics']"
4020407,"If $\sin x=\frac{2 t}{1+t^{2}}$ and $\cot y=\frac{1-t^{2}}{2 t}$, then the value of $\frac{d^{2} x}{d y^{2}}$","If $\sin x=\frac{2 t}{1+t^{2}}$ and $\cot y=\frac{1-t^{2}}{2 t}$ , then the value of $\frac{d^{2} x}{d y^{2}}$ If we take $t=\tan \theta$ then $x=2\theta$ and for $y$ ,I am getting two values as follows , if we apply identity $\tan 2 A=\frac{2 \tan A}{1-\tan ^{2} A}$ then we get $y=2\theta$ and if we take $\tan \theta $ common from $\dfrac{1-\tan^2 \theta}{2\tan\theta}$ and simplify then we get $y= \dfrac{\pi}{4} + \theta$ ? how we are getting two values of $y$ ? thankyou","['calculus', 'derivatives']"
4020450,Normal forms of differential equations and vector fields,"INTRODUCTION Consider an equation $$
\dot{x} = Ax + \varphi(x),
$$ where $x \in \mathbb{R}^n$ , $A \neq 0$ is constant matrix, $\varphi(x)$ is a germ of smooth vector field in $0$ s.t. $$
\left\{
\begin{align*}
&\varphi(0) = 0\\
&\frac{d\varphi}{dx}(0) = 0
\end{align*}\right. .
$$ There are plenty of methods to prove that this equation is smoothly equivalent to its linearisation $\dot{y} = Ay$ in some neighbourhood of zero. For example, there is method of homological equation described in Arnold's book. Or we can just substitute some unknown function  and derive an ODE for this function. The first question is, what other methods are known and which of them can be generalised to the case of parametric families of vector fields? MY APPROACH I've also conducted an experiment, and came up with the following method. We can introduce a parameter $\varepsilon$ : $$
\dot{x}(t,\varepsilon) = Ax(t,\varepsilon) + \varepsilon\varphi(x(t,\varepsilon)).
$$ When $\varepsilon = 0$ we get the linearised system, and when $\varepsilon = 1$ we have the original system. Now denote $$
y = \frac{\partial x(t,\varepsilon)}{\partial \varepsilon}.
$$ Then we can differentiate the whole equation w.r.t. $\varepsilon$ : $$
\frac{\partial}{\partial \varepsilon}\dot{x}(t,\varepsilon) = \frac{\partial}{\partial \varepsilon}Ax(t,\varepsilon) + \frac{\partial}{\partial \varepsilon}\varepsilon\varphi(x(t,\varepsilon)),
$$ $$
\dot{y}(t,\varepsilon) = A y(t,\varepsilon) + \varphi(x(t,\varepsilon)) + \varepsilon\varphi'(x(t,\varepsilon))y(t,\varepsilon).
$$ We know that the solution $x(t,\varepsilon)$ exists, is unique and smooth in some neighbourhood of zero $U$ for all $\varepsilon \in [0,1]$ , so everything is alright and we can define smooth functions $F(t,\varepsilon) = \varphi(x(t,\varepsilon))$ , $f(t,\varepsilon) = \varepsilon\varphi'(x(t,\varepsilon)) + A$ in $U \times [0,1]$ . Then our equation $$
\dot{y}(t,\varepsilon) = f(t,\varepsilon)y(t,\varepsilon) + F(t,\varepsilon)
$$ has a unique smooth solution which satisfies a condition $y(0,\varepsilon) = 0$ maybe in some smaller neighbourhood of zero $W$ for all $\varepsilon \in [0,1]$ . Finally, if we have two systems $$
\dot{x_0}(t) = A x_0(t)
$$ and $$
\dot{x_1}(t) = A x_1(t) + \varphi(x_1(t)),
$$ then the substitution $$
x_1(t) = x_0(t) + \int_{0}^{1} y(t, \xi) \, d\xi
$$ turns the second system into the first one in some neighbourhood of zero $W$ . Is my approach correct? If not, how can it be fixed? Can we improve it? Can we extend it to the case of parametric families of vector fields?","['vector-fields', 'ordinary-differential-equations', 'dynamical-systems']"
4020469,On the notation of the likelihood function,"Let $X$ be a random variable realized as the event $(X=x)$ .
The corresponding likelihood function is given by $$\mathcal{L}_x:\Theta\rightarrow[0,1]$$ $$\theta\mapsto P(X=x|\theta)$$ for a space $\Theta$ of parameter configurations $\theta$ . In the literature, $\mathcal{L}_x(\theta)$ is sometimes written as $\mathcal{L}(\theta|X=x)$ . I assume this is done to emphasize that the event $(X=x)$ is 'given'.
However, this notation leads to confusion, since
it suggests that $\mathcal{L}_x$ is a probability density ('conditioning' on the event $X=x$ ), which appears to not be true in general (cf. second answer in this thread on math.overflow ). So my questions are: Is $\mathcal{L}(\theta|X=x)$ just 'overloading' the notation $f(\cdot|\cdot)$ , or is there some hidden meaning/analogy to conditional probability $P(\cdot|\cdot)$ which I am missing? Are there other areas in mathematics where $f(\cdot|\cdot)$ is used? Could you provide an example? Currently, I think $\mathcal{L}(\theta|X=x)$ is a bad notational choice because it caused confusion for me when trying to understand the likelihood function.
Especially since at any point $\theta$ , one has $\mathcal{L}(\theta|X=x)=P(X=x|\theta)$","['notation', 'statistics', 'conditional-probability', 'log-likelihood']"
4020472,K-Theory Equivalence Classes,"Let $M$ be a finite dimensional compact manifold and $(Vect(M),\oplus)$ be the abelian monoid of complex vector bundles on $M$ . I just read that it is possible to construct an equivalence relation on $Vect(M)^2$ by $$(E_1,F_1)\sim (E_2,F_2) \iff \exists N\in \mathbb N \, s.t. \, E_1\oplus F_2\oplus \mathbb C^N\cong E_2\oplus F_1\oplus \mathbb C^N$$ In the book i'm reading (Salomon's notes on SW theory), it says that ""the additional summand $\mathbb C^N$ is needed to obtain an equivalence relation"". It seems to me that the only problem of not having it could be the transitivity property: is it not necessarily true that if $E_1\oplus F_2\cong E_2\oplus F_1$ and $E_2\oplus F_3\cong E_3\oplus F_2$ , then $E_1\oplus F_3\cong E_3\oplus F_1$ ? I'm not able to prove the transitivity property also for the adjusted relation above. Can someone help me with that?","['k-theory', 'topological-k-theory', 'differential-geometry']"
4020518,Reference request: Commutator estimate,"I am wondering if the following commutator estimate is true, and in such case, where can I find a proof for it. Let $N\in 2^{\mathbb{Z}}$ dyadic, and let's denote by $P_N$ the standard Littlewood-Paley projectors, that is, $P_N$ projects into frequencies $\{\vert\xi\vert\sim N\}$ . Finally, consider two functions in the Schwartz class $f,g\in \mathcal{S}(\mathbb{R})$ . Then, there exists $C>0$ such that $$
\big\Vert [P_N\partial_x,g]f\big\Vert_{L^2(\mathbb{R})}\leq C\Vert g_x\Vert_{L^\infty(\mathbb{R})}\Vert f\Vert_{L^2(\mathbb{R})},
$$ where $[\cdot,\cdot]$ stands for the commutator operator $[A,B]=AB-BA$ . Does anyone knows if such inequality holds and where can I find it?","['harmonic-analysis', 'analysis', 'reference-request', 'littlewood-paley-theory', 'partial-differential-equations']"
4020550,A version of Borel-Cantelli lemma without independence,"Let $(X,\mathcal{B},\mathbb P)$ be a probability space and $(A_n)_{n=1}^{\infty}$ be a sequence of measurable sets with $\sum_{n=1}^{\infty}\mathbb P(A_n)=\infty$ . I learnt from a talk that there is a version of Borel-Cantelli lemma saying that $$\mathbb P(\limsup_{N\to \infty} A_N)\ge \limsup_{N\to \infty} \frac{(\sum_{n=1}^{N}\mathbb P(A_n))^2}{\sum \sum_{m,n\le N} \mathbb P(A_m\cap A_n)}.$$ Are there any references for the proof this version of Borel-Cantelli lemma? (or a direct proof if it is not too complicated?) Let me just copy the proof given by saz (thanks!) from the link below just for the convenience of future readers. Let consider the following application of Cauchy-Schwartz inequality: Let $X_n=1_{A_n}$ for each $n$ . $$\mathbb [E(\sum_{n=1}^{N} X_n)]^2=[E((\sum_{n=1}^{N} X_n) \cdot(1_{\cup_{n=1}^{N} A_n}))]^2 \le E(1_{\cup_{n=1}^{N} A_n})^2 E(\sum_{n=1}^NX_n)^2= \mathbb P(\cup_{n=1}^{N} A_n) {\sum_{m,n\le N} \mathbb P(A_m\cap A_n)}$$ This gives us the Chung-Erdos inequality: $$ \mathbb P(\cup_{n=1}^{N} A_n) \ge \frac{(\sum_{n=1}^{N}\mathbb P(A_n))^2}{\sum \sum_{m,n\le N} \mathbb P(A_m\cap A_n)}$$ Now one can apply the above inequality to the (truncated) tail union and use the hypothesis to get the inequality we want to prove.","['probability-theory', 'reference-request']"
4020561,Using $\sec(x)$ for integral.,"Find the undefined integral $\int \frac{\sqrt{x^2+4x}}{x^2}\mathrm{dx}$ $$\displaystyle\int \dfrac{\displaystyle\sqrt{x^2+4x}}{x^2}\mathrm{dx}$$ I tried to rearrange the square root and I got: $$\sqrt{(x+2)^2-4}$$ and I substitute the $x+2$ with $2\sec(u)$ so indeed I got these two: $$x+2=2\sec(u) \\
\sqrt{x^2+4x}=2\tan(u) \\
\mathrm{dx}=2\tan(u)\sec(u)\mathrm{du}$$ And the integral turns out like this: $$\displaystyle \int \dfrac{\tan^2(u)\sec(u)}{(\sec(u)-1)^2}\mathrm{du}$$ And I continued to rearrange the integral: $$\displaystyle\int \dfrac{1-\cos^2(u)}{\cos(u)(1-\cos(u))^2}\mathrm{du}$$ I apply partial fractions method, saying $\cos(u)=u$ without integral sign and I ended up with: $$\displaystyle\int \dfrac{1}{\cos(u)}\mathrm{du}+2\displaystyle\int \dfrac{1}{1-\cos(u)}\mathrm{du}$$ One can easily integrate those integrals se I skip the calculating part. In the end I get: $$\ln\left|\sec(u)+\tan(u)\right|+2(-\cot(u)-\csc(u))+\mathrm{C}$$ I drew a triangle: Knowing $\sec(u)=\dfrac{x+2}{2}$ says us $\cos(u)=\dfrac{2}{x+2}$ As I calculate I got: $$\ln\left|x+2+\sqrt{x^2+4x}\right|-\dfrac{2x-8}{\sqrt{x^2+4x}}+\mathrm{C}$$ Bu the answer key is: $$\ln\left|x+2+\sqrt{x^2+4x}\right|-\dfrac{8}{x+\sqrt{x^2+4x}}+\mathrm{C}$$ I have been thinking where my wrong is for five hours but I couldn't find anything. Please if you see any gap tell me. Thanks.","['integration', 'solution-verification', 'analysis']"
4020635,"Justifying: $\frac{\mathrm d}{\mathrm d c}{_2F_1}(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm d c}\frac{(a)_k(b)_k}{(c)_k}\frac{z^k}{k!}$","I was reading this paper on derivatives of the hypergeometric function $F(a,b;c;x)$ w.r.t. the parameters $a$ , $b$ , and $c$ . In the paper the authors simply state without justification $$
\tag{1}
\frac{\mathrm d^n}{\mathrm dc^n}F(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d^n}{\mathrm dc^n}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}.
$$ Now I am willing to accept that this is just a formal representation of the derivatives; however, from a rigorous point of view this certain does not hold in general. For example, we know the series expansion for the hypergeometric function diverges outside the unit disk (with the exception of when $a$ or $b$ are nonpositive integers). So for $|x|>1$ , $(1)$ certainly does not hold. After a little digging I found that we can differentiate functional series so long as we can justify a few criteria. In particular one needs to justify uniform convergence of the differentiated series. So my question is this: Let $x\in[0,1]$ , $a,b\in\Bbb R$ , and $c>0$ with $c-a-b>0$ so that the hypergeometric series converges absolutely for all $x\in[0,1]$ . How can we rigorously justify the differentiation of the hypergeometric series: $$
\frac{\mathrm d}{\mathrm dc}F(a,b;c;x)=\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm dc}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}?
$$ Note that $$
\sum_{k=1}^\infty\frac{\mathrm d}{\mathrm dc}\frac{(a)_k(b)_k}{(c)_k}\frac{x^k}{k!}=-\sum_{k=1}^\infty\underbrace{\frac{(a)_k(b)_k}{(c)_k}(\psi(c+k)-\psi(c))\frac{x^k}{k!}}_{f^\prime(c)},
$$ where $\psi(z)$ is the digamma function. So we need to show that $\sum_{k=1}^\infty f^\prime(c)$ converges uniformly for all $c>0$ .","['sequences-and-series', 'hypergeometric-function', 'uniform-convergence', 'real-analysis']"
4020724,Non-standard five-point formula for second derivative used in Tracker,"Tracker is an open-source program used to analyze object trajectories from video. The typical data sets Tracker produces are time series of object positions at various times, i.e., data pairs (t_i,x_i) for $i=1$ to $N$ . The time samples are assumed to have uniform time step $t_{i}-t_{i-1}=\Delta t$ . Tracker then uses this data to generates estimates for the first and second derivatives, corresponding to the velocity $v=dx/dt$ and acceleration $a=d^2x/dt^2$ respectively. By default, it does this using the following finite difference schemes: $$v_i = \frac{x_{i+1}-x_{i-1}}{\Delta t},\qquad  a_i = \frac{1}{7(\Delta t)^2}(2x_{i+2} - x_{i+1} - 2x_i - x_{i-1} + 2x_{i-2}) $$ The first is a standard two-point formula which requires no comment. However, the standard 5-point formula for the second derivative is $$f''(x)\approx \frac{-f(x+2h)+16 f(x+h)-30 f(x)+16f(x-h)-f(x-2h)}{12h^2} \tag{1}$$ which has error of order $O(h^4)$ . (Note that this formula is exact for quartic polynomials but not quintic.) The formula used by Tracker, by contrast, would correspond to $$f''(x)\approx \frac{2f(x+2h)- f(x+h)-2 f(x)-f(x-h)+2f(x-2h)}{12h^2}. \tag{2}$$ Both sides do match in the limit $h\to 0$ , but the error is instead $O(h^2)$ . So Tracker uses a scheme which, on the face of it, is not as precise. Tracker's documentation doesn't address this point in detail, but does state the following: ""Note: there are many other finite difference algorithms. Tracker's algorithms define the velocity for a step to
be the average velocity over a 2-step interval, and the acceleration to be the second derivative of a parabolic fit over a 4-step interval, with the step at the center. Tracker's acceleration algorithm is less sensitive to position uncertainties than others."" The last sentence is especially notable. So my question as a whole is: What's the precise motivation behind Tracker's use of approximation (2), and in what sense (if any) is it ""less sensitive to position uncertainties?""","['calculus', 'derivatives', 'approximation', 'numerical-methods']"
4020733,Check that a surface over $\mathbb Z$ is regular,"Let $X$ be an arithmetic surface over $\mathbb Z$ with smooth generic fibre (which is a curve over $\mathbb Q$ ). Moreover assume that $X$ is given by one equation $F=0$ . added later: In other words, the generic fibre is expressed as a plane curve over $\mathbb Q$ . Now let $x\in X$ be a closed point. How do I check if $X$ is regular at $X$ ? Ok, formally I must show that $\mathcal O_{X,x}$ is a regular local ring (of Krull dimension 2). But in more concrete terms what should I do with my $F$ ? I mean, if $X$ was a variety over a field, I could use the Jacobian criterion, but what about this particular case where the base is $\operatorname{Spec} \mathbb Z$ ? Remark: With the term arithmetic surface I mean an integral projective scheme of dimension $2$ which is flat over $\mathbb Z$ .","['algebraic-geometry', 'surfaces', 'schemes']"
4020744,Expectation Value with exponential function,"I've got a short question about the existence of the expectation value. Let $X$ be a random variable without further knowledge about it (e.g. if it has a density or not).
If I know $\mathbb{E}(e^X)<\infty$ can I conclude that $\mathbb{E}(|X|)<\infty$ , hence the expectation value of X exists? Thanks!","['probability-theory', 'probability']"
4020817,Derivative of a summation with variable in the upper limit,"I wish to derive the following equation with respect to its upper limit, but i'm not sure how to proceed. $${ d \over dv} \sum_{t=1}^{T-t_0(v)} f(t)$$ This link offered some insight, as well as this link , altough i did not quite understand the solution of this second one. I thought of a solution like this. Create a function using the following integral $$
G(v,T,t) = 
\int_1^{T-t_0(v)} f(t) \, dt
$$ Then take the derivative of this function with respect to v : $${ d \over dv} G(v,T,t)$$ This is correct?","['integration', 'summation', 'derivatives']"
4020823,What is the logic behind the balls colours in MarkSix lottery?,"I am not familiar with this kind of problem and I am trying to find what could be the general term or pattern used for the color assignment in the Mark Six lottery ( This is the matrix of colors ). There are three sequences: Red balls: [1, 2, 7, 8, 12, 13, 18, 19, 23, 24, 29, 30, 34, 35, 40, 45, 46]; Red(n) = ? Blue balls: [3, 4, 9, 10, 14, 15, 20, 25, 26, 31, 36, 37, 41, 42, 47, 48]; Blue(n) = ? Green balls: [5, 6, 11, 16, 17, 21, 22, 27, 28, 32, 33, 38, 39, 43, 44, 49]; Green(n) = ? How do you think would be the best approach to find the pattern followed there? What would be the general term for those number sequences? Thanks in advance!",['sequences-and-series']
4020850,Relation between Ax=0 and Ax=b,I'm not sure how to solve this. My teacher told me that if $A\vec{x}=0$ has one unique solution (the trivial) then $A\vec{x}=\vec{b}$ has only one unique solution. But I don't know how to prove this.,['linear-algebra']
4020860,Limit involving a Differential Equation,"Problem: Consider the differential equation $$\dfrac{dy}{dx} + 10y = f(x) , \
> x>0,$$ where $f(x)$ is a continuous function such that $\displaystyle \lim_{x \to \infty} f(x) = 1$ . Find the value of $\displaystyle \lim_{x \to \infty} y(x)$ . My attempt: Since this is a Linear Differential Equation, we can solve for $y(x)$ to get, $$ y(x) = \dfrac{C + \int e^{10x} f(x)}{e^{10x}} $$ which gives, $$ \require{cancel} \lim_{x \to \infty} y(x) = \cancelto{0}{\lim_{x \to \infty} \dfrac{C}{e^{10x}}} + \lim_{x \to \infty}\dfrac{{\int e^{10x} f(x)}}{e^{10x}} =  \lim_{x \to \infty}\dfrac{{\int e^{10x} f(x)}}{e^{10x}} $$ But I don't know how to solve for the last limit involving the integral.","['limits', 'calculus', 'ordinary-differential-equations']"
4020863,Application of Hahn-Banach Theorem in $\Bbb{R}^2$,"I'm trying to solve Let consider the space $\Bbb{R}^2$ with the norm $\|(x,y)\|_p=(|x|^p+|y|^p)^{1/p}$ and let $Z$ be the subspace $$Z=\{(t,mt);t\in\Bbb{R}\},$$ for some $m\in\Bbb{R}$ . Let $f\colon Z\to\Bbb{R}$ be defined by $f(t,mt)=t$ . (a) For $1<p<\infty$ find the unique linear extension $\tilde{f}$ of $f$ defined over $\Bbb{R}^2$ such that $\|\tilde{f}\|=\|f\|$ . (b) For $p=1$ , find two distincts extensions of $f$ . Do the same for $p=\infty$ . I'm stucked in (a), and neither try (b) yet. I was thinking that the extension which works in (a) is $\tilde{f}(x,y)=y/m$ . But I'm with big difficult to see what to do with the norms. For any $(t,mt)\in Z$ , we have that $$\|(t,mt)\|_p=|t|(1+|m|^p)^{1/p},$$ and $$|f(t,mt)|=|t|\leqslant|t|(1+|m|^p)^{1/p}=\|(t,mt)\|_p,$$ hence $\|f\|\leqslant1$ . So the candidate to be the norm of $f$ is $1$ . But I can't find the vector in $Z$ that does the work, neither usying some sequence in $Z$ that makes the norm be $1$ . So, I'm thankful for any help.","['hahn-banach-theorem', 'functional-analysis']"
4020877,Confusion on why 2 equivalence classes are either equal or disjoint,So I've just started trying to teach myself some topology and in the book I'm reading there is a proof that 2 equivalence classes are either equal or disjoint. However I'm a bit confused on why the author just randomly states any 2 equivalence classes surely it should be any 2 equivalence classes defined by the same equivalence relation or is it that any 2 equivalence classes regardless whether or not the equivalence relations on them is equal or not are either equal or disjoint? Thanks in advance.,"['elementary-set-theory', 'equivalence-relations', 'general-topology']"
4020884,One-point gradient estimator and Stokes' theorem.,"In bandit convex optimization, we are only given access to zeroth-order oracle of a function but not first-order (gradient) oracle. Hence, people often use some one-point gradient estimator to approximate the gradient of a function at a certain point. More specifically, let $f:\mathbb{R}^d\mapsto \mathbb{R}$ be a function in $\mathbb{R}^d$ , $\mathbb{S}$ be the unit sphere in $\mathbb{R}^d$ , $\mathbb{B}$ be the unit ball in $\mathbb{R}^d$ . For a point $\mathbb{x}\in\mathbb{R}^d$ , if we want to estimate $\nabla f(\mathbf{x})$ , we randomly sample a $\mathbf{u}$ from $\mathbb{S}$ . Let $\delta>0$ be a small positive scalar. We query the zeroth-order oracle and obtain $f(\mathbf{x}+\delta \mathbf{u})$ . We then use $\frac{f(\mathbf{x}+\delta \mathbf{u})d\mathbf{u}}{\delta}$ as an approximation of $\nabla f(\mathbf{x})$ . Lemma 1 in this paper proved that the expectation of the approximation is the gradient of a smoothed version of $f$ , that is let $\hat{f}(\mathbf{x})=\mathbb{E}_{\mathbf{v}\in\mathbb{B}}[f(\mathbf{x}+\delta \mathbf{v})]$ , then $\nabla \hat{f}(x)=\mathbb{E}_{\mathbf{u}\in \mathbb{S}}\big[\frac{f(\mathbf{x}+\delta \mathbf{u})d\mathbf{u}}{\delta}\big]$ .
The proof invoked Stokes's theorem to show the following (equation (15) in the paper): \begin{align}
\nabla \int_{\delta \mathbb{B}}f(\mathbf{x}+\mathbf{v})\mathrm{d}\mathbf{v}=\int_{\delta \mathbb{S}}f(\mathbf{x}+\mathbf{u})\frac{\mathbf{u}}{\|\mathbf{u}\|}\mathrm{d}\mathbf{u}.
\end{align} I'm trying to understand how exactly does the above follow from the Stokes's theorem. Is it that the curl of the vector field defined as $f(\mathbf{x}+\mathbf{u})\frac{\mathbf{u}}{\|\mathbf{u}\|}$ somehow is related to the left-hand-side?","['convex-optimization', 'multivariable-calculus', 'gradient-descent', 'stokes-theorem']"
4020885,The center of the circumcircle lies on a side of a triangle,"Consider a triangle $ABC$ . Let the angle bisector of angle $A$ be $AP,P\in BC$ . $BP=16,CP=20$ and the center of the circumcircle of $\triangle ABP$ lies on the segment $AC$ . Find $AB$ . $$AB=\dfrac{144\sqrt5}{5}$$ By Triangle-Angle-Bisector Theorem $$\dfrac{BP}{PC}=\dfrac{AB}{AC}=\dfrac{16}{20}=\dfrac{4}{5}\\ \Rightarrow AB=4x, AC=5x.$$ The cosine rule on $ABC$ gives $$BC^2=AB^2+AC^2-2\cdot AB\cdot AC\cdot\cos\alpha \\ \iff 1296=41x^2-40x^2\cos\alpha,$$ where $\measuredangle A=\alpha.$ Is any of this helpful for the solution? Any help would be appreciated. Thank you in advance!","['euclidean-geometry', 'triangles', 'angle', 'geometry']"
4020903,Find all functions such that $f(xf(y)+y)=f(xy)+f(y)$,"Find all functions, $f:\mathbb{R}\to \mathbb{R}$ such that $$f(xf(y)+y)=f(xy)+f(y)$$ What i have got till now is Let $$P(x,y): f(xf(y)+y)=f(xy)+f(y)$$ Then \begin{align*}
&P(0,0):  f(0)=f(0)+f(0)\implies f(0)=0\\
&P\Bigg(\frac{y}{y-f(y)},y\Bigg): f(y)=0
\end{align*} Hence $f(x)=0$ is a solution. But we can see $f(x)=x$ is also a solution but I can't find that.",['functions']
4020906,Find the gradient and hessian of $g(x)=f(Ax)$,"Let $f(z):\mathbb R^m\rightarrow \mathbb R$ be a real-valued function
from $\mathbb R^m$ to $\mathbb R$ . Let $A^{m\times n}, x^{n\times 1}$ ,
and let $g(x)=f(Ax)$ . Find the gradient and hessian of g(x) in terms of A, $\nabla f(x)$ ,
and the hessian $H(f(x))$ I tried using the chain rule $\underbrace{\nabla g(x)}_{n\times 1}=\underbrace{A}_{m\times n} \underbrace{\nabla f(Ax)}_{n\times 1}$ But it seems as if the dimensions don't work out. Is the dimension of $\nabla g(x)$ actually mx1? I thought it should be nx1 because there are n  elements of x. If that is the case, my application of chain rule is probably wrong.","['multivariable-calculus', 'chain-rule']"
4020955,Every path in $S^n$ is homotopic to a non-surjective path,I want to prove that the fundamental group of $S^n$ is trivial. I found this demonstration by Ners√©s Aramian which can be found in this pdf . It is essentially demonstrated that every path in $S^n$ is homotopic to a path that is not surjective. I would like to understand this demonstration. My problem with this proof is at the end of it. Why is the comment made about the $\sigma$ and $\tilde{\sigma}$ maps? And the last paragraph was very confusing for me.,"['fundamental-groups', 'general-topology', 'homotopy-theory', 'algebraic-topology']"
4020971,$x^3+y^3-3xy+C=0$ is a smooth curve for $C\neq0$ or $1$,"Show that $x^3+y^3-3xy+C=0$ describes a smooth curve for $C\neq0$ and $C\neq1$ . For $C=0$ , the curve self-intersects at the origin, so cannot be smooth. For $C=1$ , the curve contains an isolated point at $(x,y)=(1,1)$ . However, I'm not sure how to show that the curve is smooth for all other values of $C$ . I guess I'd be done if I could find some smooth parametrisation for the curve (that is injective and has non-zero derivative).","['calculus', 'differential-geometry']"
4020972,Find the number of roots of $z^2 - \cos z=0$ for $|z| < 2$ using Rouche's theorem,"Find the number of roots for the following equation for $|z| < 2$ , $z\in \Bbb C$ : $$
z^2 - \cos z=0
$$ The reasoning below is based on using Rouche's theorem. So basically I picked two functions $f(z)$ and $\phi(z)$ such that: $$
f(z) = z^2\\
\phi(z) = \cos z
$$ Now I need to show that $|f(z)| > |\phi(z)|$ for $|z| = 2$ . My main issue here is proving that statement. Consider $|z^2|$ , clearly the absolute value is $4$ . For $|\cos z|$ and $y\in[-2,2]$ : $$
\begin{align}
|\cos z| &= \left|\frac{e^{iz} + e^{-iz}}{2}\right| \\
&= {1\over 2}\left|e^{ix - y} + e^{-ix+y}\right| \\
&\le {1\over 2}\left(\left|e^{ix}\right|\left|e^{-y}\right| + \left|e^{-ix}\right|\left|e^{y}\right|\right)\\
&={1\over 2}\left(e^{-y}+e^y\right)
\end{align}
$$ Now $|\cos z|$ is symmetric with respect to $x = 0$ so we might consider only one case: $y \in [0;2]$ . Take a look at the following equation: $$
\begin{align}
{1\over 2}\left(e^{-y} + e^y\right) &= 4 \\
e^{-y} + e^y &= 8 \ \ | \times e^y \\
e^{2y} - 8e^y + 1 &= 0
\end{align}
$$ Solve for $e^y$ : $$
e^y = 4\pm \sqrt{15}
$$ Approximate calculations show that: $$
\begin{align}
y = \ln(4+\sqrt{15}) &\approx 2,06343... > 2\\
y = \ln(4-\sqrt{15}) &\approx -2,06343... < -2
\end{align}
$$ Finally since ${1\over 2}\left(e^{-y}+e^y\right)$ is incresing for $y \in [0,2]$ we have that: $$
|\cos z| = {1\over 2}\left(e^{-y}+e^y\right) < 4, \forall y\in[0,2]
$$ This means the equation has two roots (with multiplicities) in $|z| < 2$ . The question here is how do I show $$
\ln(4+\sqrt{15}) > 2\\
\ln(4-\sqrt{15}) < -2
$$ Also, I would appreciate it if someone could show a simpler solution.","['complex-analysis', 'rouches-theorem', 'calculus', 'inequality']"
4021053,Transform Limit definition of $e$,"The limit definition of $e$ can be written as: $$ \lim_{x \to \infty} {\left(1 + \frac{1}{x}\right)^x} . $$ This is equal to another similar form: $$ \lim_{x \to 0} {(1 + x)^{\frac{1}{x}}} . $$ I am wondering how to prove these 2 forms are equivalent, any help is appreciated thank you.","['limits', 'calculus']"
4021115,How to find the probability of drawing colored marbles without replacement?,"There are $2$ red marbles, $3$ white marbles and $5$ black marbles in a bag.
What is the probability of drawing $1$ red marble, $2$ white marbles and $3$ black marbles, if $6$ marbles are drawn without replacement? Answer: $$\frac{\binom{2}{1}\binom{3}{2}\binom{5}{3}}{\binom{10}{6}}=\frac{2}{7}$$ My question is why is the numerator of the answer $\binom{2}{1}\binom{3}{2}\binom{5}{3}$ instead of $1$ ? Below is my answer: Total number of ways to choose $6$ marbles from $10$ marbles is $\binom{10}{6}$ , so combinations such as $BBBBBW$ , $RWWBBB$ , $WWWRBB$ , ... So the probabilty I calculated is $\frac{1}{\binom{10}{6}}$ because $RWWBBB$ is one of the possible combinations out of the $\binom{10}{6}$ total combinations. However this is incorrect, and is supposed to be $\frac{\binom{2}{1}\binom{3}{2}\binom{5}{3}}{\binom{10}{6}}$ . I do not understand why the numerator is the product combinations because it does not make sense to multiply combinations of nondistinct objects. For example, there are $5$ black marbles $BBBBB$ and so the number of ways to select $3$ black marbles is $1$ , therefore it does not make sense to me that it is written as $\binom{5}{3}$ as above. To me this would make sense if it were asking to arrange $3$ letters at a time from $ABCDE$ where the order does not matter. Can anyone explain why the numerator is the product of combinations instead of $1*1*1=1$ ? My observation: I noticed that using subscripts are necessary when there are fewer distinct objects than the total number of objects. See below: The combination formula is $\binom{n}{r}$ where $n$ is total number of objects and $r$ is number of selected objects. Let $k$ be the number of distinct objects where if $k<n$ , then subscripts are needed if $k=n$ , then subscripts are not needed Case $1$ . $k<n$ Suppose I want to select $r=2$ marbles from a bag with $1$ white marble and $2$ black marbles, so $k=2$ and $n=3$ , so $k<n$ So $\binom{n}{r}=3$ with combinations: { $W_1,B_1$ }, { $W_1,B_2$ }, { $B_1,B_2$ } Case $2$ . $k=n$ Suppose I want to select $r=2$ marbles from a bag with $1$ white marble, $1$ black marble and $1$ red marble, so $k=3$ and $n=3$ , so $k=n$ So $\binom{n}{r}=3$ with combinations: { $W,B$ }, { $W,R$ }, { $B,R$ }","['combinatorics', 'discrete-mathematics']"
4021177,"Calculate the integral $ \iint_R\sqrt{y/x}\,e^{\sqrt{xy}}\,dA$","The $R$ is the region of integration, described in the following image. To solve this integral we make a change of coordinates with $u=\sqrt{xy}$ and $v=\sqrt{\frac{y}{x}}$ . Furthermore, in the region of integration we have $x>0$ and $y>0$ . Then $u^2=xy$ y $v^2=\frac{y}{x}$ , so we have: \begin{align*}
    v^2 &=\frac{y}{x}\\
    x^2v^2 &= xy\\
    x^2v^2 &= u^2.
\end{align*} \begin{align*}
    x^2 &= \frac{u^2}{v^2}\\
    x &= \frac{u}{v}.
\end{align*} Observation: $x=-\frac{u}{v}$ is not taken, because $x>0$ in the $R$ region. With the above we can solve for $y$ . \begin{align*}
    v^2& = \frac{y}{x}\\
    v^2& = \frac{y}{\frac{u}{v}}\\
    uv & = y.
\end{align*} On the other hand, in the $R$ region we have the following equations $xy = 1$ , $y = x$ and $y = 2$ . Now with all of the above, let's pass these equations to the coordinates $u$ and $v$ . \begin{align*}
    xy = 1 &\Rightarrow u  = \sqrt{1} \Rightarrow u = 1\\
    y = x  &\Rightarrow uv = \frac{u}{v} \Rightarrow v^2 = 1 \Rightarrow v = 1\\
    y = 2  &\Rightarrow 2 = uv.
\end{align*} Observation: By definition of $u$ and $v$ , we have to be positive about the values ‚Äã‚Äãtaken by the $R$ region, therefore $v = -1$ is not taken. Using the above equations, we have that the region $R$ is transformed into the region $S$ , shown below. Now using $x = \frac{u}{v}$ and $y = uv$ we calculate the Jacobian to use the change of variable in the original integral. \begin{align*}
    \frac{\partial x}{\partial u}&=\frac{1}{v} &\frac{\partial y}{\partial u} =v\\
    \frac{\partial x}{\partial v}&=-\frac{u}{v^2} &\frac{\partial y}{\partial v} =u
\end{align*} Thus, the Jacobian is: \begin{align*}
    \begin{vmatrix}
    \frac{1}{v}    &v\\
    -\frac{u}{v^2} &u
    \end{vmatrix}
    =\frac{u}{v}+\frac{u}{v}=\frac{2u}{v}.
\end{align*} So using all the information developed we have: \begin{align*}
    \iint\limits_R\sqrt{\frac{y}{x}}e^{\sqrt{xy}}dA &= \iint\limits_Sve^u\left(\frac{2u}{v}\right)dA\\
    &= 2\int_{1}^{2}\int_{1}^{2/u}ue^u\,dv\,du\\
    &= 2\int_{1}^{2}ue^uv\biggr\rvert_{1}^{2/u}\,du\\
    &= 2\int_{1}^{2}ue^u\left(\frac{2}{u}-1\right)du\\
    &= 2\int_{1}^{2}e^u(2-u)du=2(e^u(3-u))\biggr\rvert_{1}^{2}=2e(e-2).
\end{align*} I think this is the correct solution, I await your comments. If anyone has a different solution or correction of my work I will be grateful.","['integration', 'multivariable-calculus', 'multiple-integral']"
4021202,Is there hope for Einstein tensor notation in Quantum Mechanics?,"The Core Question How would one define and justify the tensor notation for vectors and operators on a separable Hilbert space? Motivation Whenever I work with finite-dimensional linear algebra, I find it very illuminating to think about vectors, linear maps, bilinear forms etc. collectively as just various types of tensors. I like how the tensor formalism puts emphasis on representation-free formulations and how even a very complicated multi-linear map can be understood in terms of its individual ‚Äûindices‚Äú. When I started to learn Quantum Mechanics, I immediately tried to use the tensor notation. At a surface level it seemed to work just fine. One can formally rewrite the bra-ket expressions on left with a ‚Äûtensor-like‚Äú notation on the right: $$
\begin{aligned}
  \left< \psi \mid \varphi \right>
  \qquad &\leftrightsquigarrow \qquad
  \overline{\psi^{\,\mu}} \; g_{\overline{\mu} \nu} \; \varphi^\nu
  \\
  \left| \psi \right> = \hat A \hat B \left| \varphi \right>
  \qquad &\leftrightsquigarrow \qquad
  \psi^{\,\mu} = A^\mu_\nu \; B^\nu_\kappa \; \varphi^\kappa
  \\
  W = \left| \psi \right>\left< \psi\right|, \;
  \operatorname{Tr} W = 1
  \qquad &\leftrightsquigarrow \qquad
  W^\mu_\nu = \psi^{\,\mu} \; g_{\overline{\kappa}\nu} \; \overline{\psi^\kappa}, \;\;
  W^\mu_\mu = 1
\end{aligned}
$$ Furthermore, the tensor notation feels more natural when dealing with composite systems, where the state is a tensor product of states of the underlying systems. However, when I tried to put the notation on a more rigorous ground, I learned that the topic is much more difficult and nuanced than I thought. The Problem In finite-dimensional spaces, the tensor notation is made possible by several circumstances: \begin{gather}
V^* \otimes V \simeq \operatorname{Hom}(V, V)
\label{homomorphism} \tag{1} \\[5pt]
T(\mathbf{v}) = T( \; \sum_k v^k \mathbf{e}_k \;) = \sum_k v^k \; T(\mathbf{e}_k)
\label{continuity} \tag{2} \\
\big| \operatorname{Tr}(T) \, \big| < \infty
\label{trace} \tag{3}
\end{gather} While \eqref{homomorphism} essentially makes sure that all $k$ -covariant $l$ -contravariant tensors are from the same space, \eqref{continuity} lets us describe all tensors using their coefficients wrt. some basis and \eqref{trace} lets us express all operations on tensors using just tensor product and contraction. In a sense, neither of these is true for the separable Hilbert space $\mathcal H$ where QM is done. By a basis on a Hilbert space one usually means the Schauder basis , which describes vectors with an infinite series of coefficients. This, combined with the fact that most interesting operators in QM aren't continuous, gives us: $$
  T(\mathbf{v})
  = T( \; \sum_{k=1}^\infty v^k \mathbf{e}_k \;)
  = T( \; \lim_{N\to\infty}\sum_{k=1}^N v^k \mathbf{e}_k \;)
  \neq \lim_{N\to\infty} T( \; \sum_{k=1}^N v^k \mathbf{e}_k \;)
  = \sum_{k=1}^\infty v^k \; T(\mathbf{e}_k)
$$ The sequence on the RHS might either diverge, or even converge to a different value than LHS (although this is pathological and usually not considered in practise). Condition \eqref{trace} only holds for the so-called trace-class operators (not even the identity is trace-class) and the space $\mathcal H^* \widehat{\otimes} \mathcal H$ ( tensor product of Hilbert spaces ) is isomorphic to finite-rank operators, a proper subset of $\operatorname{Hom}(\mathcal H, \mathcal H)$ . A Possible Solution? (and more questions) These unfortunate circumstances mean, that if it's even possible to justify the tensor notation in infinite-dimensional spaces, it can't be by a simple generalization of the finite-dimensional case. So, is it possible to make it work? The condition \eqref{continuity} seems relatively easy to fix ‚Äì one just needs to replace the Schauder basis with a Hamel basis . A serious downside to this would be that a typical operator would have an uncountable number of coefficients. For \eqref{homomorphism} it would be great if one could find a space $\mathcal G$ , such that the Hilbert space $\mathcal H$ is embedded in it $\mathcal H \subset \mathcal G$ , with the property $\mathcal G \otimes \mathcal G \simeq \operatorname{Hom}(\mathcal H^*, \mathcal H)$ . Vectors of the bigger space could then be used to construct operators and bilinear forms. Here, one immediately thinks about the Gelfand triple $\Phi \subset \mathcal H \subset \Phi^*$ ‚Äì could it be that $\mathcal G = \Phi^*$ for an appropriate choice of rigging? Sadly it appears that no, as the Schwartz kernel theorem says that $\Phi^* \otimes \Phi^* \simeq \operatorname{Hom}(\Phi, \Phi^*)$ . Is such a space $\mathcal G$ even possible? If yes, would this approach generalize to tensors of higher degree? Finally, the problem with \eqref{trace} seems to be there to stay. There is no reasonable way to define the trace of identity, for example. What I'm interested in is whether, given a complicated expression, one can tell a priori which indices are contractible and which will inevitably diverge. Are these proposed ‚Äúsolutions‚Äù any good? Is there any literature that would investigate this topic? Or am I on the wrong path and should I stop wasting my time with tensors in the infinite dimension?","['hilbert-spaces', 'tensors', 'functional-analysis']"
4021298,Expressing a indefinite integral using a definite integral with the same function,"I was wondering how we can express a indefinite integral $\int f(x) \, dx$ with the function $F(x) = \int_a^x f(t)\,dt$ . I was experimenting with some functions, for example, if $f(x) = 3x^2$ , then \begin{equation}
\int f(x) \, dx = \int 3x^2 \, dx = x^3 + C.
\end{equation} On the other hand, \begin{equation}
F(x) = \int_a^x f(t) \, dt = \int_a^x 3t^2 \, dt = [t^3]^x_a = x^3 - a^3.
\end{equation} That means what $F(x)$ and $\int f(x) \,dx$ differs is only by a constant. I was wondering if there is a proof for $F(x) + C = \int f(t)\,dt$ and the proof works for integrals that do not cannot be expressed as an elementary function (e.g. the error function (maybe)) P.S. I was thinking if this has to do with the derivatives of $F$ and integral of $f$ , but if there is no upper and lower limit in the indefinite integral, we cannot use FTC. That's why I do not know how to proceed. Thanks you! Edit: Yes. I have forgotten to mention that $f$ should be a continuous function, in order to make things more neat and to apply FTC. However, I very much thank you all for the responses, especially those who talked about discontinuous functions as well. Cheers.","['integration', 'definite-integrals', 'calculus', 'indefinite-integrals', 'derivatives']"
4021304,"Dimension of the Invariant Subspace $((\mathbb{C}^2)^{\otimes 2n})^{\mathfrak{sl}(2,\mathbb{C})}$","This was posed as a challenging exercise by my professor. The exercise is to compute the dimension of the invariant subspace of $(\mathbb{C}^2)^{\otimes 2n}$ viewed as an $\mathfrak{sl}(2,\mathbb{C})$ representation. For some background, $\mathfrak{sl}(2,\mathbb{C})$ can be viewed as traceless $2 \times 2$ matrices with complex entries. So one common basis we see is $e = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, f = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, h = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$ . The standard representation of $\mathfrak{sl}(2,\mathbb{C})$ on $\mathbb{C}^2$ is just by matrix multiplication. Let $v_1 = (1,0)$ and $v_0=(0,1)$ . Observe that $ev_1 = fv_0 = 0$ , $ev_0 = v_1,fv_1 = v_0$ , and $hv_1 = v_1,hv_0 = -v_0$ . In general, if we have a Lie algebra $\mathfrak{g}$ acting on two representations $V,W$ , then we can define a representation on their tensor product by a Leibniz rule: $a(v \otimes w) = (av) \otimes w + v \otimes (aw)$ . An invariant subspace of a representation $V$ is really all vectors that are annihilated by every element of $\mathfrak{g}$ : $V^\mathfrak{g} := \{v \in V: av = 0, \forall a \in \mathfrak{g}\}$ . Okay, with that set up, some knowledge of $\mathfrak{sl}(2,\mathbb{C})$ tells us that the Lie algebra is completely reducible and all the irreducible complex representations are of the form $V(n) = \langle v_n, v_{n-2}, v_{n-4},...,v_{2-n},v_{-n} \rangle$ , a $n+1$ dimensional vector space. The subscripts here represent the weight which really means that they are eigenvectors of $h$ with their eigenvalue being the subscript. So $h v_{n-6} = (n-6) v_{n-6}$ . I didn't write it this way above but it's mostly due to bookkeeping. Moreover, $e$ adds $2$ to the weight of vectors while $f$ subtracts $2$ from their weight. $h$ doesn't shift the weight. Anyways, the relevance of this is that the invariant subspace must lie within the weight $0$ subspace because otherwise, $h$ cannot annihilate. Also, under tensor products, weights add. For example, $\mathbb{C}^2 \otimes \mathbb{C}^2$ has basis: $v_1 \otimes v_1, v_1 \otimes v_0, v_0 \otimes v_1, v_0 \otimes v_0$ . The first one has weight $2$ , the middle two have weight $0$ , the last one has weight $-2$ . We can show that $e(v_1 \otimes v_0 - v_0 \otimes v_1) = 0$ and $f$ and $h$ also annihilate this vector. Well, $h$ acts by multiplication by $0$ on the weight $0$ subspace so we don't really need to check $h$ . Note also that if we take $(\mathbb{C}^2)^{\otimes n}$ where $n$ is odd, then there are no weight $0$ vectors other than the zero vector. In this case, there cannot be any nontrivial invariant subspace. That's why we take even number of copies. Now the bookkeeping: I chose $v_1,v_0$ to denote the basis vectors of $\mathbb{C}^2$ because then we can denote the basis elements of $W_{2n} := (\mathbb{C}^2)^{\otimes 2n}$ by strings of zeros and ones. So $\dim W_{2n} = 2^{2n}$ . Among these, the weight $0$ strings are precisely those with half being zeros, the other half being ones. So the weight zero subspace has dimension ${2n \choose n}$ . The invariant subspace has to have dimension smaller than this. I could have chosen $+,-$ instead but it might get confusing to read things like $(+-)-(-+)$ . Also, as an example, in $W_6$ , $e(101001) = (111001) + (101101) + (101011)$ . Basically, $e$ flips $0$ to $1$ like in computers and but kills $1$ ; that's why we see only three terms instead of six which we expect from Leibniz's rule. $f$ does the opposite. I suspect that we really only need to study $e$ but I'm not sure. Anyways, with that lengthy prologue (if you read till here, thank you), I admit that I don't see any combinatorial patterns. I thought I had found something when $n=2$ but it fell apart in $n=3$ . For $n=2$ , I think the invariant subspace has dimension $2$ and is spanned by $(1100)-(1010)-(0101)+(0011)$ and $(1001)-(1010)-(0101)+(0110)$ .","['representation-theory', 'combinatorics', 'lie-algebras', 'invariant-theory']"
4021326,Is it wrong to use limits to calculate an integral of a Piecewise Function?,"In my test I used limits when I calculated the integral of a piecewise function. In my head the reasoning was that it was an improper integral in one piece of the function because it never reached that point although the limit is easily solved with simple substitution. $f(x)=\begin{cases}x+3 & x\in\mathbb{[-2;1]}\\ (x-2)^2 & x\in\mathbb{(1;3]} \\ \end{cases}$ This was the function, so what I did is to calculate the integral of the function I divided the function in two different named function lets say $ g(x)=x+3 $ and $ h(x)=(x-2)^2 $ . As we can see both functions have limited domains for g(x) the domain is $[-2;1]$ and for h(x) the domain is $(1;3]$ . So what I did to calculate the integral of f(x) was: $ \int_{-2}^3 f(x) = \int_{-2}^1g(x) + \lim_{a \to 1^+}\int_a^3h(x)$ I did this because $h(x)$ is never really $1$ , so when calculating the integral of a function that does not contain the bound because it never really reaches the bound I use improper integrals, and that is why I decided to use limit. To solve it I just used simple substitution. I cant understand why that would be wrong, my math teacher took points of my test score because of this but he wasn't able to explain why this was wrong, and frankly I can't understand what is wrong either. So my question is: is this really wrong? And if it is, why? Thanks :)","['integration', 'definite-integrals', 'improper-integrals', 'limits', 'piecewise-continuity']"
4021341,Number of doublings in a bounded submartingale,"Let $1\leq X_n \leq N$ be a submartingale with respect to a filtration $(\mathcal{F}_n)_{n\geq 1}$ , i.e., $\mathbb{E}[X_{n+1}\mid\mathcal{F}_{n}] \geq X_n$ and $N \geq 0$ is a scalar. Suppose $X_0 = 1$ and let $n_k$ denote the index at which the process doubles for the $k$ -th time, i.e., $n_0 = 0$ and for $k \geq 1$ , $$
n_k = \min\{n > n_{k-1}: X_n > 2X_{n-1}\}.
$$ Let $\kappa_N$ be the set of all these indices until time $N$ and $K_N = |\kappa_N|$ be the total number of doublings until time $N$ . Can we bound $\mathbb{E}[K_N]$ ? My Approach: In the special case where $X_{n+1} \geq X_n$ monotonically, we claim that $K_N \leq 1 + \log N$ . because otherwise, $$
X_{n_{K_N}} = \prod_{2 \leq n \leq n_{K_T}}\frac{X_{n}}{X_{n-1}} \cdot X_1 \geq \prod_{n \in \kappa_N, \\n \neq 1}\frac{X_{n}}{X_{n-1}} > 2^{K_N-1} > N,
$$ which is a contradiction with $X_{n_{K_N}} \leq N$ . Can we prove something similar in the more general case of submartingales? Edit: Originally the question was with $0 \leq X_i \leq N$ . But, now it is changed to $1 \leq X_i \leq N$ to avoid the counter example provided by @Peter Morfe.","['expected-value', 'martingales', 'probability-theory', 'upper-lower-bounds']"
4021348,Proving an absolute inequality,"No answers please, just hints! I'm asked to prove that, on $ 0\leq x < \pi$ , where $n\in\mathbb N$ and $|\sin(nx)|\leq n\sin x$ , that $|\sin((n+1)x)|\leq(n+1)\sin x$ suing the fact that $\sin(x+y)=\sin x \cos y+\cos x \sin y$ . What I've done so far is $\begin{align*} |\sin(nx+x)|&=|\sin(nx)\cos x + \cos(nx)\sin x| \\ &\leq |\sin(nx)\cos x|+|\cos(nx)\sin x| \\ &\leq |\sin(nx)*1|+|\cos(nx)*1| \\ &\leq n\sin x+|\cos(nx)|\end{align*}$ and here I'm stuck","['proof-writing', 'discrete-mathematics']"
4021354,Bound on steepest descent,"I'm learning about the method of steepest descent for approximating the solution of $Ax=b$ where A is an invertible matrix.
Here is the part I understand:
We do this by minimizing the function $f(x)=\frac{1}{2}\|Ax-b\|_2^2$ . The $\nabla f(x)=A^TAx-A^Tb$ . Let $M=A^TA$ and $c=A^Tb$ . I came across this bound $$f(x^{(i)}) \leq f(x^{(0)})\left(1-\frac{1}{k(A)^2}\right)^i$$ . Where $x^{(i)}$ ist the $i^{th}$ gradient descent step and is defined as follows. $$r^{(i)}=c-Mx^{(i)}\\
\alpha^{(i)}=\frac{(r^{(i)})^T r_n^{(i)}}{(r^{(i)})^T M r^{(i)}}\\
x^{(i+1)}=x^{(i)}+\alpha r^{(i)}$$ and $k(A)$ is the condition number of a matrix. I was able to derive some other bounds such as $$
\frac{f(x^{(i)})-f(x^{(*)})}{f(x^{(0)})-f(x^{(*)})}<\left(\frac{k(M)-1}{k(M)-1}\right)^{2i}
$$ but I can't wrap my head around the one mentioned above. Any pointers or suggestions would be great. Thanks!","['condition-number', 'matrices', 'upper-lower-bounds', 'convergence-divergence', 'gradient-descent']"
4021365,4D Left Isoclinic Rotations as double rotations,"According to this page, 4D left-isoclinic rotation matrices are double rotations of the same angle (and same sign). But in the same page, in the context of quaternions, it is mentioned that left-isoclinic matrices have the form: $$R=\begin{bmatrix}a&-b&-c&-d \\ b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{bmatrix}$$ where $a,b,c,d\in\mathbb{R}$ such that $$a^2+b^2+c^2+d^2=1.$$ My question is, how can we view $R$ as a double rotation? I mean, how can we find the common angle? If two of $b,c,d$ are $0$ then it is easy to see. But what if this is not the case?","['matrices', 'orthogonal-matrices', 'quaternions', 'rotations']"
4021401,How to conclude truth value of elements from true propositions,"suppose that these propositions are true $\qquad \neg (\neg d \lor a) \ , \quad (\neg a \land c) \lor d \ , \quad a \lor \neg b \ , \quad c \rightarrow b $ What are the value(s) of Truth of a, b, c and d? I got an exercice like that and I'm not sure if my method is correct mainly because the question states that an element could have more than one value of truth Answer... $a$ : False, $b$ : False, $c$ : False, $d$ : True We have: $ \neg (\neg d \lor a)  \Leftrightarrow d \land \neg a $ (Morgan law) Therefore $a$ is strictely false and $d$ is  strictly True so that the $\neg (\neg d \lor a)$ proposition is True. Also in: $a\lor \neg b$ We conclude that $\neg b$ is True so $b$ is False (knowing that $a$ is False) so that the proposition $a \lor \neg b$ is True. In: $c \rightarrow b$ .
We have $b$ False, so for the $c \rightarrow b$ proposal to be True $c$ must be strictly False to satisfy the proposal. The statement $(\neg a \land c) \lor d$ is also True with all our results.","['logic', 'discrete-mathematics']"
4021420,What is intuition behind total derivative of a function,"I know that a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is called totally differentiable at a point $a\in \mathbb{R^n}$ if $\exists$ a linear transformation $T_a$ such that \begin{equation}
\|{\lim_{h\rightarrow 0} \frac{f(a+hv)-f(a) - T_a(h,v)}{h}}\| = 0
\end{equation} I also know that this expression is derived from Taylor's theorem. My question is that if this definition is supposed to be analogous to differentiablity of a function in a single variable setup. Then how does this expression justifies it. In short, my question is how does this function justify differentiablity as we know. Also, here we have used linear transformation $T_a$ as derivative of that function. How do we know that the derivative of this function will be a linear transformation?","['multivariable-calculus', 'functions', 'derivatives']"
4021433,Estimations of some new recurrence sequences,"Problem 1 . Given the recursion $a_{n+ 1}= \sqrt{a_{n}^{2}+ a_{n}}$ with $a_{1}= 1.$ Prove that $$a_{n}\sim\frac{n}{2}+ \frac{1}{2n}\,{\rm as}\,n\rightarrow\infty$$ For problem 1 , I only can prove $a_{n}\sim\left ( n+ 1 \right )/2$ by using the Laurent series for the increasing $a_{n} :$ $$a_{n+ 1}= a_{n}+ \dfrac{1}{2}+ \mathcal{O}\left ( \dfrac{1}{a_{n}} \right )$$ Problem 2 . Given the recursion $a_{n+ 1}= \sqrt[3]{a_{n}^{3}+ a_{n}}$ with $a_{1}= 1.$ Prove that $$a_{n+ 1}\sim\sqrt{\dfrac{2n}{3}}\,{\rm as}\,n\rightarrow\infty$$ What if we changed the given one by $a_{n+ 1}= \sqrt[3]{a_{n}^{3}+ 3a_{n}^{2}+ a_{n}}$ ? For problem 2 , similarly, I saw that $a_{n+ 1}\fallingdotseq a_{n}+ \dfrac{1}{3a_{n}}$ , then how can we find the relative equations like ${y}'= \dfrac{1}{3y}$ by $n$ times ?","['laurent-series', 'recurrence-relations', 'recursion', 'alternative-proof', 'limits']"
4021465,How to know whether a set of points can be rotated to lie in positive orthant?,"I have a set of points in N-dimensional space. I want to find out whether they could, in principle, be rotated to lie solely in the positive orthant of space. Is there a property of these points that would guarantee this rotation exists? If this question proves too difficult, perhaps another related one will be easier. I have the linear dynamical system and starting point that generated these points. Is there a property of linear dynamical systems which, if satisfied, ensure that under some rotation the trajectory will lie solely in the positive orthant? To give an idea of the kind of thing I'm looking for (if it exists!): in 2D if the dot products between the set of points are all positive you can guarantee that there's some rotation that could rotate the set of points into the positive quadrant, and if a dot product is negative you know there is no such rotation. This is a nice easy test I can apply. However, this approach doesn't generalise (in 3D points can have all positive dot products, but cannot be rotated into the positive octant), can you think of one that will generalise? Later edit - a few things I've been considering: People study positive linear systems: linear dynamical systems, $\dot{\boldsymbol{x}}(t) = \boldsymbol{Ax}(t)$ , in which the trajectory, $\boldsymbol{x}(t)$ , never leaves the positive orthant. It turns out if the matrix $\boldsymbol{A}$ is Metzler, meaning all off-diagonal elements are non-negative, then a trajectory starting in the positive orthant will stay there. This is related to the Perron-Frobenius Theorem which I don't know so much about. So it seems another way of framing my question would be: you have your linear dynamical system in an arbitrary basis. Can you tell whether the dynamics matrix $\boldsymbol{A}$ is Metzler under some rotation? The Perron-Frobenius theorem talks about rotation invariant things like eigenvalue spectra which gives me hope that this could be a constructive route, but I haven't been able to fit things together yet... Maybe the clever people on this site will be able to! (Which is similar to this old question: Converse of Perron Frobenius Theorem: Necessary and Sufficient Conditions for positivity (or non negativity) )","['vectors', 'dynamical-systems', 'linear-algebra', 'linear-transformations', 'rotations']"
4021473,How to prove this lower bound on $\log(1+x)$ for $x \geq 0$? How about $x > -1$?,"I want to show that for all real numbers $x \geq 0$ : $$
\log(1+x) \geq \frac{x(5x+6)}{2(x+3)(x+1)}.
$$ I'd like to understand each step necessary to prove this so that I can apply it to future problems. Amazingly, a popular online calculator immediately spits out that this inequality is true for all $x > -1$ . Is there an easy way to prove this for $x \geq 0$ ? How about $x >-1$ ? Attempt (and more info on how I'm stuck) :
My guess is to use the Taylor series expansion of both sides at $x=0$ (but why is that sufficient for all $x \geq 0$ ?). Expanding both sides: $$
\log(1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \ldots \geq x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{5x^4}{18} + \frac{7x^5}{27} - \ldots = \frac{x(5x+6)}{2(x+3)(x+1)}.
$$ I see that the early order terms match and then they don't. Not sure how to proceed from there. The signs are alternating on both sides, which I'm not sure how to deal with. Though, it seems the coefficients are converging to $0$ on both sides. It also seems the magnitude of the coefficients on the right-hand side is larger than the left-hand side (if you go term-wise), but how do I prove that? And do I need that fact to prove the inequality?","['logarithms', 'real-analysis', 'calculus', 'taylor-expansion', 'inequality']"
4021486,Calculation of probability for random variable,"The number of the daily car accidents in Luna-land is a random variable with mean 50 and standard deviation 5. Find the probability for the total accidents in the following 25 days to be $\leq 1300$ . I am only familiar with combinatorics, not distributions.
I did a bit of reading around and found that probably this must be approached by the normal distribution.
So we want $P (X\leq 1300)$ ?
(this is not homework!) Thank you! EDIT: I found the formula for the central limit theorem. So, it seems I must calculate $\frac {(1300-25*50)}{\frac{5}{\sqrt25}}$ ?","['probability-distributions', 'probability']"
4021523,Finding the limit for a rational expression involving factorials,"The task is to find $$L=\lim_{n\rightarrow \infty} \frac{1}{2n}\sqrt[n]{\frac{(2n+1)!}{n!}}.$$ I say we resort to Sirling's approximation $k!\approx \sqrt{2\pi k} (\frac{k}{e})^k$ : $$ L \approx \lim_n \frac{1}{2n}\frac{\bigg(\sqrt{2\pi (2n+1)}(\frac{2n+1}{e})^{2n+1}\bigg)^{1/n}}{\bigg(\sqrt{2\pi n}(\frac{n}{e})^{n}\bigg)^{1/n}}\approx \frac {4n^2/e^2}{2n^2/e} \approx \frac {2}{e}.$$ however, as usual there is a solution manual that disagrees and says $L=2$ . I'd appreciate it if you could help me figure out what's wrong.","['factorial', 'approximation', 'calculus', 'sequences-and-series', 'limits']"
4021524,"A different music ""shuffle"" feature","Suppose every friday night you like going in a lounge bar and drink some soft drinks. Usually you spend $t$ minutes (say, $120$ ) there and talking to the bartender you know that there are exactly $N$ songs (say, $100$ ), of $3$ minutes each, in the playlist that keeps the typical atmosphere. The algorithm of the player is made such that once a song is played, it can't be played until other $n$ (say, $20$ ) different songs are played, then the probability of the choice of that song returns uniform as before. What is the probability that you'll listen to the same song twice? Honestly I do not know how to attack this problem, I saw that this kind of problems can have solutions that are ""very strange"" but still beautiful.","['probability-theory', 'probability']"
4021536,Optimal transport as a metric between two color images,"I am trying to characterize a distance between two images in relation to the colors present in these images. Therefore I would like to solve Earth mover's distance/1-st Wasserstein distance with entropic regularization (for a quick approximate solution) between the 3D histograms (RGB) of the two images. We resolve $\min_P \langle P,C \rangle - \epsilon H(P)$ s.t $P1=a, P^T1=b$ where $P$ is the optimal transport plan, $H$ is the entropy, $a$ and $b$ are the histograms of the two images and $\epsilon$ is the regularization term. Each entry $C_{ij}$ in this matrix contains the cost of moving point $i$ in the support of $a$ to point $j$ in the support of $b$ . It is said that the 1-st Wasserstein distance is given by the solution of the optimisation problem above if $C_{ij}=||X_i-Y_j||_2$ . So I understand this as the distance between two pixels of the two images in RGB space. Only since in dimension > 1 there is no more order relation, can I order my points $X_i$ and $Y_j$ anyhow?","['entropy', 'probability-distributions', 'optimal-transport', 'image-processing', 'probability']"
4021566,How to perform a push forward change of measure.,"Im a bit confused with the push forward formula https://en.wikipedia.org/wiki/Pushforward_measure Let $\rho$ be a probability density associated to a probability measure $\mu$ on $\mathbb{R}^d$ , i.e "" $\rho(x)dx=d\mu(x)$ "". Let $f:\mathbb{R}^d\to\mathbb{R}^d$ and denote $f_{\#}\mu$ be the push forward of $\mu$ by $f$ , and $f_{\#}\rho$ the associated density pushed forward. Then $\textbf{is it true that}$ for any integral $$
\int_{\mathbb{R}^d} \big( g \circ f_{\#}\rho \big) (x) df_{\#}\mu(x)  =\int_{\mathbb{R}^d} \big( g\circ \rho \big) \rho(x)dx
$$","['integration', 'measure-theory', 'pushforward', 'real-analysis', 'change-of-variable']"
4021586,Tangent bundle of the connected sum of two smooth manifolds,"Let $M,N$ be two smooth connected real $n$ -dimensional manifolds. Suppose they are both orientable for simplicity. Let $i:D_1\hookrightarrow M$ and $j:D_2\hookrightarrow N$ two embedded disks so that $i$ preserves the orientation and $j$ reverses the orientation. Consider their connected sum given by \begin{equation*}M\#N=(M\setminus\overset{\circ}D_1)\sqcup_\phi(N\setminus\overset{\circ}D_2)
\end{equation*} where $\phi:\partial D_1\rightarrow\partial D_2$ is a diffeomorphism glueing the boundary of the two disks. The hypothesis on $j$ to reverse the orientation is given in order to ensure that $M\#N$ is orientable. At this point I would like to know if there is a canonical approach to understand how the tangent space at a point of the connected sum looks like and if this can help us to have global information on the tangent bundle. Why Am I asking that? I want to understand some characteristic classes associated to the connected sum. For example we can consider the total Stiefel-Whitney class $w(M\#N)\in H^*(M\#N, \mathbb{Z}_2)$ or the Euler class $e(M\#N)\in H^n(M\#N,\mathbb{Z})$ . I was thinking about the case when the tangent bundle splits as a direct sum (product of manifold $M\times N$ ) and so by using Whitney product formula for the Stiefel-Whitney class we can recover $w(M\times N)$ from $w(M)$ and $w(N)$ . In particular this seems to me no longer true for the connected sum even in the simplest case of connected compact and orientable surface. Question Is there a canonical approach to understand how $T(M\#N)$ looks like and to get information on its characteristic classes? Or better, is there a way to understand how for example $w(M\# N)$ looks like by  using only the axioms and using only the construction of the connected sum?","['characteristic-classes', 'vector-bundles', 'smooth-manifolds', 'differential-geometry']"
4021589,Does the family of compact subsets of a Polish space belong to the Effros Borel structure?,"Since the answer to my previous question (see Effros Borel structure ) turned out to be negative, I don't know how to solve the following problem: Let $X$ be a Polish space and let $\mathcal{F}(X)$ be the space of all closed subsets of $X$ equipped with the Effros Borel structure. Is it necessary true that $\mathcal{K}(X)$ , the set of all compact subsets of $X$ , is Borel as a subset of $\mathcal{F}(X)$ ? Trivially, the answer is positive if $X$ is compact. However, the case I am interested in is $X = \mathbb{U}$ , where $\mathbb{U}$ stands for the universal Urysohn space (which is not compact).","['borel-sets', 'general-topology', 'descriptive-set-theory', 'compactness']"
4021592,"Prove that $\sup_{0\leq s\leq t}\mathbb E[|X_s|^2]<\infty $ where $dX_s=\mu(X_s,s)ds+\sigma (X_s,s)dW_s$","Let $$|\mu(x,t)|+|\sigma (x,t)|\leq K(1+|x|),$$ for all $x\in\mathbb R$ and some $K>0$ . Let $$\begin{cases}d X_t=\mu(X_t,t)\,\mathrm d t+\sigma (X_t,t)d W_t\\X_0=x_0\in\mathbb R\end{cases}.$$ I would like to prove that $\sup_{0\leq s\leq t}\mathbb E(|X_s|^2)<\infty $ Using It√¥ formulae gives $$X_t^2=x_0^2+2\int_0^tX_s\mu(X_s,s) d s+\underbrace{\int_0^t \sigma (X_s,s)d W_s}_{=:M_t}+\int_0^t\sigma (X_s,s)^2 d s.$$ Let $\tau_n\nearrow \infty $ s.t. $(M_{\tau_n\wedge t})$ is a martingale. Then $$\mathbb E[X_t^2]\leq x_0^2+2\mathbb E\int_0^{t\wedge \tau_n} X_s\mu(X_s,s) d s+2\mathbb E\int_0^{t\wedge \tau_n}(1+X_s^2)\,\mathrm d s.$$ Q1) How can I conclude ? Moreover, I have to deduce that $\mathbb E(|X_t|^2)\leq Ce^{Dt}$ for some $C,D>0$ . So, I could easily prove that $$\mathbb E(|X_t|^2)\leq C+D\int_0^t\mathbb E(|X_s|^2)ds,$$ for some $C,D>0$ , and thus the result follows from Gronwall's inequality. Q2) Nevertheless, I wonder why $\sup_{0\leq s\leq t}\mathbb E(|X_s|^2)<\infty $ is required here ? Q3) Moreover, in my Gronwall inequality, continuity of $s\mapsto \mathbb E(|X_s|^2)$ is continuous, and I failed to prove it. Does the continuity of $s\mapsto \mathbb E(|X_s|^2)$ comes from $\sup_{0\leq s\leq t}\mathbb E(|X_s|^2)<\infty $ ?","['probability-theory', 'stochastic-calculus']"
4021595,Question about smooth functions and their signs with given initial conditions,"Problem Statement Suppose $f:\mathbb{R}\to\mathbb{R}$ is a smooth function (infinitely differentiable) where $f(x)\geq 0$ for all $x\in\mathbb{R}$ , $f(0)=0$ , and $f(1)=1$ . Show that there is some $n\in\mathbb{N}$ and an $x_0\in\mathbb{R}$ such that $f^{(n)}(x_0)<0$ . Proof Attempt We can consider two cases: Case (1): There exists a $z<0$ such that $f(z)>0$ . By the MVT, there exists a point $z_0\in(z,0)$ such that $$f'(z_0)=\frac{f(z)-f(0)}{z-0}=\frac{f(z)}{z}<0 $$ so we are done. Case (2): For all $x<0$ , $f(x)=0$ . In this case, $f'(x)=0$ for all $x<0$ , which implies that, by left sided derivatives and the fact that $f'$ is smooth, $f'(0)=0$ . It then follows that,  since $f'(x)=0$ on $(-\infty,0]$ , we have $f^{(n)}(x)=0$ for all $x\in(-\infty,0]$ . For $n=1$ , define $z_1$ as the real number in $(0,1)$ such that $$f'(z_1)=\frac{f(1)-f(0)}{1-0}=1$$ For $n\in\mathbb{N}$ , define $z_{n+1}$ by the real number in $(0,z_n)$ such that $$f^{(n+1)}(z_{n+1})=\frac{f^{(n)}(z_n)-f^{(n)}(0)}{z_n-0}=\frac{f^{(n)}(z_{n})}{z_n}>0$$ whose existence is given by the mean value theorem. This defines the sequence $(z_n)$ , which converges to $0$ and is strictly decreasing. Edit 2: Continuing, we see that by definition, we have $$f^{(n+1)}(z_{n+1})=\frac{f^{(n)}(z_n)}{z_n}>f^{(n)}(z_n)$$ for any $n\in\mathbb{N}$ with $n\geq 2$ , since $0<z_n<1$ , thus the sequence $f^{(n)}(z_n)$ is strictly increasing and always greater than or equal to $1$ . Now, $f^{(n)}(0)=0$ , but since $(z_n)$ can get arbitrarily close to $0$ and $f^{(n)}(z_n)\geq 1$ for all $n\in\mathbb{N}$ , we have $f^{(n)}(0)=0<1\leq f^{(n)}(z_n)$ for large enough $n$ . This is has almost shown that $f$ is not differentiable for some value of $n$ , but I am not quite sure that it has exactly shown that. If it has, then we are done, as Case (2) cannot happen. I would like help either formalizing this or fixing this if it is not true. This is where my argument, to me, seems to break down. Intuitively, the slope of the secant lines between $f^{(n)}(z_n)$ and $f^{(n)}(0)$ is increasing without bound as $n$ increases. My idea was to create a sequence of points which converged to $0$ , but broke the differentiability at $0$ . I would greatly appreciate any tips in proceeding, critiques of my proof outline, or alternative proof methods. Thank you in advance. Edit 1: In Case (2) , I am not explicitly making any assumptions that would lead to a contradiction, but why is this proof going in that direction? Either Case (2) cannot happen, which I find intuitively true, but my intuition often fails me, or one of my assumptions in the beginning of Case (2) is false. Edit: Just to close this off, one of my faulty assumptions was that I assumed the sequence I constructed converged to $0$ , which it doesn't have to.","['smooth-functions', 'real-analysis', 'alternative-proof', 'solution-verification', 'derivatives']"
4021630,Set theory task,"Let F be a set of sets.
We say a set is n-pretty  (it's writen in my text book, I don't know the exact name)(n is fixed nat. number) if $$\forall X (X \in F \iff \forall Y (Y \subseteq X \land |Y| <= n \implies Y \in F)$$ .
And we say that a set is finite-pretty if: $$\forall X (X \in F \iff \forall Y (Fin(Y) \land Y \subseteq X \implies Y \in F)$$ So the task is to find if every n-pretty set is finite-pretty and if every finite-pretty set is n-pretty . I think that every finite-pretty set is n-pretty. Since it's true for every finite set, it will be true for every set with cardinality n. But I can't seem to  prove it in terms of set theory . And what about the other direction ? Any tips?",['elementary-set-theory']
4021697,Confusion in the solution for this question,"Given 4 flags of different colours, how many different signals can be generated. If a signal requires the use of 2 flags one below the other? Le the colours be R , B , G , O. My book answers it as  = 4 *3 = 12 which is right answer but not conceptually right . 4 means 4 choices I.e R,B,G,O. Then for 3 choices , we have B,G,O. Now , if you see . Total combinations it will give  is RB RG RO , BB BG BO , GB GG GO , OB OG OO. Here , OO GG BB are wrong combinations . So , if I remove them there are total 9 now. But we need 3 more . Those are BR , OR and GR. Please tell if i am right till here ? How to consider them then ? Now , all this cutting and then adding made the solution way also long . Is there a shorter way of doing and how to revive those BR , OR , GR without counting it knowing and also to cut BB , GG ,  OO without counting.","['permutations', 'combinations', 'combinatorics']"
4021718,Chain rule for functional derivates with the same variables,"I am following an online course where we should sometimes compute functional derivatives. The scope of the course is density functional theory, so we have the density $n(r)$ and we have functionals of the form $F[n(r)]$ . $$n(r) = \sum_i \Psi_i^*(r)\Psi_i(r)$$ In the course we need to perform derivatives with respect to $\Psi_i^*(r)$ not with respect to $n(r)$ . One quick result is that: $$\frac{\delta n(r)}{\delta \Psi_i^*(r)} = \frac{\delta \Psi_i^*(r)}{\delta \Psi_i^*(r)} \Psi_i(r) = \Psi_i(r)$$ One of the students used the chain rules for functional derivatives: $$\frac{\delta F[n(r)]}{\delta \Psi_i^*(r')} = \int \frac{\delta F[n(r)]}{\delta n(r)}\frac{\delta n(r)}{\delta \Psi_i^*(r')}dr = \int \frac{\delta F[n(r)]}{\delta n(r)}\delta(r - r')\Psi_i(r) dr = \frac{\delta F[n(r)]}{\delta n(r')}\Psi_i(r')$$ Which is a really helpful result because it allows to compute derivatives directly with respect to $n(r)$ and still get what we want by just multiplying by $\Psi_i(r')$ . Now the problem I have, is that, in this line he introduced a new variable $r'$ while it shouldn't, the derivative should be with one variable $r$ everywhere, and I believe that this is not valid when the variables are the same (the integral doesn't vanish.). What I did is that I used the functional derivative definition of composed function $F[g(f(r))]$ and adapted to each functional I needed to derive which lead to the same results with more work While I acknowledge that the first solution is faster, I wouldn't be able to find it by myself because it doesn't make so much sense to me. Any thoughts?","['functional-equations', 'functional-calculus', 'functional-analysis', 'chain-rule']"
4021719,Computing a difficult integral,"This is actually a follow-up question to what I posted here: Computing double integral for expected value . I think that the result in the previous post may be helpful in computing this integral. The result in the previous post was: $$\frac{\int_{\mathbb{R}^{2}}\frac{p^2}{2} e^{-\beta (V(x) + p^2/2)}\mathop{dx}\mathop{dp}}{\int_{\mathbb{R}^{2}} e^{-\beta (V(x) + p^2/2)} \mathop{dx}\mathop{dp}} = (k_BT)^{-1},$$ where $\beta = (k_BT)^{-1}$ . So we can use this result if it makes the following integral easier. I would now like to compute the following: $$\frac{\int_{\mathbb{R}^{6n}} \frac{p^2}{2} e^{-\beta(V(x) + \frac{1}{2} \sum_{i = 1}^{3n} p_i^2)} \mathop{dx dp}}{\int_{\mathbb{R}^{6n}} e^{-\beta(V(x) + \frac{1}{2}\sum_{i = 1}^{3n} p_i^2)} \mathop{dx dp}},$$ where $V$ is some unknown function of $x$ . The answer should be $3nk_BT/2 = \frac{3n}{2\beta}$ , but I'm really not sure about how to show this result. Any help is appreciated","['integration', 'multivariable-calculus', 'calculus']"
4021726,Writing a group as a product of its generators in MAGMA,"Let $G \subseteq S_8$ be generated by the elements $s=\begin{pmatrix} 1 & 2 \end{pmatrix}\begin{pmatrix} 3 & 5 \end{pmatrix}\begin{pmatrix} 4 & 6 \end{pmatrix}\begin{pmatrix} 7 & 8 \end{pmatrix}$ and $t=\begin{pmatrix} 1 & 3 & 7 & 4 \end{pmatrix}\begin{pmatrix} 2 & 5 & 8 & 6 \end{pmatrix}$ . Now I would like to see how to write the element $$u = \begin{pmatrix} 1 & 8 \end{pmatrix}\begin{pmatrix} 2 & 7 \end{pmatrix}\begin{pmatrix} 3 & 6 \end{pmatrix}\begin{pmatrix} 4 & 5 \end{pmatrix} \in S_8$$ can be written as a product of $s,s^{-1}$ and $t,t^{-1}$ in MAGMA (assuming that $u \in G$ ). I wrote the following code G<s,t> := PermutationGroup< 8 | (1, 2)(3, 5)(4, 6)(7, 8) , (1, 3, 7, 4)(2, 5, 8, 6) >;
u := G ! (1, 8)(2, 7)(3, 6)(4, 5); Are there any commands to give me my desired result?","['permutations', 'abstract-algebra', 'magma-cas', 'math-software', 'group-theory']"
4021886,Show $\int_0^\infty \frac{\tan^{-1}x^2}{1+x^2} dx= \int_0^\infty \frac{\tan^{-1}x^{1/2} }{1+x^2}dx$,"I accidentally found out that the two integrals below $$I_1=\int_0^\infty \frac{\tan^{-1}x^2}{1+x^2} dx,\>\>\>\>\>\>\>I_2=\int_0^\infty \frac{\tan^{-1}x^{1/2} }{1+x^2}dx$$ are equal in value. In fact, they can be evaluated explicitly. For example, the first one can be carried out via double integration,  as sketched below. \begin{align}
I_1&=\int_0^\infty \left(\int_0^1 \frac{x^2}{1+y^2x^4}dy\right)\frac{1}{1+x^2}dx\\
&= \frac\pi2\int_0^1 \left( \sqrt{\frac y2}+ \frac1{\sqrt{2y} }-1\right)\frac{1}{1+y^2}dy=\frac{\pi^2}8
\end{align} Similarly, the second one yields $I_2=\frac{\pi^2}8$ as well.
The evaluations are a bit involved, though, and it seems an overreach to prove their equality this way, if only the following needs to be shown $$\int_0^\infty \frac{\tan^{-1}x^2-\tan^{-1}x^{1/2} }{1+x^2}dx=0$$ The question, then, is whether there is a shortcut to show that the above integral vanishes.","['integration', 'improper-integrals']"
4021970,Simplify this statistical average,"I have a quantity $\tau$ given by: $$
\frac{1}{\tau} = \frac{1}{\tau_1}+\frac{1}{\tau_2}+\frac{1}{\tau_3}
$$ where $\tau_1$ , $\tau_2$ and $\tau_3$ are some constituent quantities. Now these $\tau$ 's are function of a variable $x$ , but the ensemble averages of $\tau_i$ are known, given by: $$
\frac{\langle \tau_i^2\rangle}{\langle \tau_i\rangle^2} = \alpha_i
$$ Is there a way to express or simplify the ensemble average of total $\tau$ in terms of $\alpha_i$ 's: $$
\frac{\langle \tau^2\rangle}{\langle \tau\rangle^2} 
$$ If not, what would be the necessary information needed w.r.t. the actual distribution of $\tau_i$ with $x$ .","['average', 'statistics', 'random-variables']"
4021980,An onto function $f: \mathbb{R}^2 \rightarrow \mathbb{R}^4$,"Does there exist an onto function $f: \mathbb{R}^2 \rightarrow \mathbb{R}^4$ ? If $f$ is required to be linear then the answer is no, this follows from basic ideas of linear algebra. In the case where we allow for $f$ to be an arbitray function, I believe the answer is that yes, such a function $f$ does exist, simply because the domain and codomain both have the same cardinality (uncountably infinite) and thus there is a bijection between the two. How far does this generalize? If it is true then it must also be true that for each $n$ there exists a function $f$ such that $f: \mathbb{R} \rightarrow \mathbb{R}^n$ is onto. What about $f: \mathbb{R} \rightarrow \mathbb{R}^{\mathbb{N}}$ ? What about $f: \mathbb{R} \rightarrow \mathbb{R}^I$ where $I$ is an uncountable infinite set? Thank you.","['elementary-set-theory', 'set-theory']"
4022002,Impossible integral?,"$$\int_{0}^{x^2-1} f(t) dt= x^6+x^4+3x^2$$ I saw this problem in a calculus exam. $f$ is assumed to be continuous. Using the Fundamental Theorem of Calculus, I calculated $f(t)= 3t^2+8t+8 $ . But when I integrate $f$ it gives me $\int_{0}^{x^2-1} f(t) dt= x^6+x^4+3x^2-5$ : $$\frac {d}{dx}\int_{0}^{x^2-1} f(t) dt= \frac {d}{dx}[x^6+x^4+3x^2]$$ $$f(x^2-1)2x = 6x^5+4x^3+6x$$ $$f(x^2-1) = 3x^4+2x^2+3$$ $$f(x^2-1) = 3(x^2-1)^2+8(x^2-1)+8$$ $$f(t) = 3t^2+8t+8$$ $$\int_{0}^{x^2-1} 3t^2+8t+8 dt= x^6+x^4+3x^2-5$$ This means that there is no continuous function that satisfies the equation of the problem?","['integration', 'calculus', 'definite-integrals']"
4022027,On the definition of a smooth manifold,"I am self studying vector calculus from Hubbard's book [I have no advanced background in topology, differential geometry and related subjects]. Here is the definition of a smooth manifold as per the text. A subset M of R^n is a smooth k dimensional manifold if it is the graph of a C1 [continuously differentiable] function f expressing n-k variables as functions of the other k variables. Here are my questions about this definition. Why force the function f to be C1? Specifically, why do we need the derivative of f to be continuous? What happens if we allow differentiable functions whose derivative is not continuous? As a follow up, a one dimensional smooth manifold is also called a smooth curve. For a (parameterised) curve to be smooth, is it really true that we require its tangent vector to be non-zero at all points? Do we lose the smoothness property even if the tangent vector at (at least) a single point is zero? Why or why not? Thanks a lot for reading my question. I understand that I have asked a lot, but these things are bothering me. Regards,
Madhav","['manifolds', 'curves', 'smooth-manifolds', 'differential-geometry']"
4022060,Probability of lines in a pentagon intersecting internally or at vertices.,"I'm studying maths as a hobby. The points A,B,C,D & E are the vertices of a regular pentagon. All possible lines joining the pairs of these points are drawn. If two of these lines are chosen at random, what is the probability that their point of intersection is (a) inside the pentagon, (b) one of the points A,B,C,D,E? I start out by saying the number of ways of choosing 2 lines from 10 is $\binom{10}{2} = 45$ Then I started getting confused. I can see there are 5 internal points of intersection, which I mark in red, so that would give me $\frac{5}{45} = \frac{1}{9}$ probability of choosing lines which meet internally. And this is indeed the answer my text book gives. But I'm not sure if this is just coincidence. As for (b), I'm not sure how to proceed. The book gives the answer as $\frac{2}{3}$","['combinatorics', 'probability']"
4022134,Show that $2^n<2^{\lceil n \log_23\rceil}-3^n<3^n-2^n$,"Is there a way to show that (with integer $n>2$ ) $$2^{\lceil n \log_23\rceil}-3^n<3^n-2^n$$ I tried to figure a way with derivative or by looking how both side are growing, but I have some trouble with the ceiling function.","['collatz-conjecture', 'limits', 'inequality', 'ceiling-and-floor-functions']"
4022168,Questions about domain and range of composite functions,"Let $f: \mathbb R \to \mathbb Z, \ g: \mathbb R \setminus \{0\} \to \mathbb  R$ be defined by $f(x) = \lfloor x\rfloor, g(x) = \displaystyle{\frac 1x}$ . Note that in this case, $g \circ f$ is not defined ( $\color{red}{\text{why?}}$ ). However, $f \circ g: \mathbb R \setminus \{0\} \to \mathbb Z$ is defined. You can $\color{blue}{\text{verify}}$ that the composite is given by $$(f \circ g)(x) = \left\lfloor\frac 1x\right\rfloor =\begin{cases}0 \text{ if } x \in (1, \infty) \\  n \text{ if } x \in \color{orange}{\left(\frac{1}{n + 1},\frac 1n\right]_{n \in \mathbb N}} \\  -(n + 1) \text{ if } x \in \color{orange}{\left(-\frac 1n, -\frac{1}{n + 1}\right]_{n \in \mathbb N}} \\ -1 \text{ if } x \in (-\infty, -1)\end{cases}$$ How do we know that we have defined $(f \circ g)(x)$ for all $x \in \mathbb R \setminus \{0\}$ ? $\color{green}{\text{Recall }\bigcup_n [1/n,1) = (0,1)}.$ My questions: Answer to the $\color{red}{\text{question in red}}$ above: $g \circ f$ is not defined as $0$ in the range of $f$ , but not in the domain of $g$ . Is that correct? $\color{blue}{\text{Suggestion in blue}}$ above. I'll only consider one branch to see if I can do it. Suppose $\displaystyle{-\frac 1n < x \le -\frac{1}{n + 1}}$ . Then $\displaystyle{-n > \frac 1x \ge  -(n + 1)} \iff \left\lfloor \frac 1x \right\rfloor = -(n + 1)$ by definition of $\lfloor \cdot \rfloor?$ $\color{green}{\text{Hint in green}}$ above. If $x \in \mathbb Q$ , then $g(x) \in \mathbb N.$ If $x 
\in \mathbb N$ , then $g(x) \ne 0$ . Is this what the mysterious(?) $\color{green}{\text{hint in green}}$ above alluding to? How do they know the given partition of the domain of $f \circ g$ covers all the necessary cases? In particular, how did they know to choose the $\color{orange}{\text{intervals in orange}}$ ? Thanks.","['algebra-precalculus', 'discrete-mathematics']"
4022181,Does eigenvalues of matrix change after multiplication by unitary matrix?,"For a matrix $A \in \mathbb{C}^{n \times n}$ , does multiplication by a unitary matrix $U$ change the eigenvalues of $A$ ? So for: $$Ax = \lambda x \qquad \mathrm{and} \qquad AUy = \mu y $$ does $\lambda = \mu$ for some $x,y \in \mathbb{C}^n$ ? I know the above is true for doing left and right multiplication by $U$ : $$ UAU^*y = \mu y \\ 
AU^*y = \mu U^* y \\
Az = \mu z \\
\therefore \mu = \lambda$$ (defining $z = U^* y$ ) Under the guise that unitary matrices are simply rotations, it logically makes sense to me that $\mu$ and $\lambda$ should be identical, and only the the eigenvectors should be different. The statement is true for singular values (see here ), but I'm having trouble proving it for eigenvalues (if it even is true). Edit After a quick example in python, I understand that the above is not true. So instead: where is my thought process going wrong with regard to how unitary matrices/rotations effect eigenvalues? Edit 2 What I was really going for, but did not state correctly was that: $$AX = \Lambda X \qquad \mathrm{and} \qquad AUY = MY \\$$ such that $ \lambda \in M, \forall \lambda \in \Lambda$ , where $M$ and $\Lambda$ are diagonal matrices.","['linear-algebra', 'eigenvalues-eigenvectors']"
4022228,Writing $\mathbb{R}$ as a countable union of closed intervals,"This is probably trivial, but I need to know if my reasoning is correct. $\mathbb{R} = \displaystyle \bigcup_{i=1}^{\infty} [ -i,i]$ If $x \in \mathbb{R}$ , then there is $\epsilon>0$ such that $x \in (x-\epsilon, x+\epsilon) \subseteq (-i,i) \subseteq[-i,i]$ for some natural $i$ , so $x \in \displaystyle \bigcup_{i=1}^{\infty}[-i,i]$ . If $x \in \displaystyle \bigcup_{i=1}^{\infty}[-i,i]$ , then there is a natural such that $x \in [-i,i] \subseteq \mathbb{R}$ . Hence the sets are equal. Is everything ok with this? Thanks.","['elementary-set-theory', 'solution-verification']"
4022272,Studying set theory using polynomials?,"I came up with a way to do set theory using polynomial arthimetic. To explain, consider two polynomials $P(x)$ and $Q(x)$ , then we can observe the following analogies: 1. In Polynomial addition $$ |P(x)|+|Q(x)|$$ Is analogous to set intersection of sets because the above equation is only true for $x$ values which make both $P$ and $Q$ zero at the same time. 2. In Polynomial multiplication $$|P(x)| \cdot |Q(x)|$$ Can be thought of as set union because the above equation is true for the numbers which satisfy either $P$ or $Q$ 3. In polynomial division $$ \frac{|P|}{|Q|} $$ The above expression can be thought of a set difference, we remove the zeros of Q which exist also in P. Analogies to set complements: Polynomial reciprocal is similar to set inverses, for example $$ |P(x)^{-1}| = \frac{1}{|P(x)|}$$ Is defined for all values of $x$ except where $P(x)=0$ where it is undefined. To get the whole universal set back, we multiply inverse with regular one: $$|P(x)| \cdot \frac{1}{|P(x)| }=1$$ Proving set identities using polynomials $ A \cup (B \cup C)= (A \cup B) \cup C$ This is directly analogous to associativity of polynomial multiplication , considering three polynomials $P,Q,R$ : $$ ( |P| \cdot |Q|) |R|  = |P|( |Q| \cdot |R|)$$ $A \cup B  = B \cup A, A \cap B , B \cap A$ This is directly analogous to communality of polynomial multiplication. Demorgan's laws $$ A^c \cup B^c = (A \cup B)^c$$ Easy relation to understand with polynomial way, consider two polynomials $P(x),Q(x)$ , then it is trivial that: $$ \frac{1}{|P|} \cdot \frac{1}{|Q|} = (|PQ|)^{-1}$$ $$ (A \cap B)^c= A^c \cup B^c$$ This one just turns out weird, but it works well for most identities where there isn't addition and multiplication on both sides My question: Has this idea been studied before? Where there any new insights from thinking of sets like this? [Note: I  know of generating functions already]","['elementary-set-theory', 'polynomials']"
4022275,"Is the set made up of all points that satisfy $g_{1}(x, y, z)=x^2 + 2y^2=1 $, $\:g_{2}(x, y, z)= xy + xz=2$ compact?","I want to find the least value of a function whose domain is made up of all points that satisfy the following: $g_1(x, y, z)= x^2 + 2y^2=1$ $g_2(x, y, z)= xy + xz=2$ In my book it is written that ""a minimum exists since the domain is a closed set"". However, previously there is a theorem that one can only be certain that function obtains a max and min value if the domain is compact, i.e. both closed and bounded. From what I see, the domain is not bounded, how can we then be certain that ""a minimum exists""? Any help would be appreciated. $\textbf{Edit:}$ : I want to find the least distance form the origin, so my function is $f(x, y,z)=ùë•^2+ùë¶^2+ùëß^2$","['optimization', 'multivariable-calculus', 'maxima-minima', 'compactness']"
4022308,Adding water with constraints,"There are $n$ cups of capacity $1$ each. A game proceeds in $n$ rounds. In each round, you can add water to some cups subject to the following rules. For each $2\le i\le n$ , you cannot add more water into cup $i$ than the water that was already in cup $i-1$ in the previous round. You cannot add more water than the water you added into the first round. Your score for adding to cup $i$ is the amount of water you added into this cup times $1/i$ , and your score for the round is the sum over all cups you added to. Your final score is then the minimum among the scores in the $n$ rounds. Is it possible for you to score $2/n$ or more for some $n$ ? Or asymptotically larger than $O(1/n)$ ? Scoring $1/n$ is easy, by adding $1/n$ to the first cup $n$ times, or by adding $1$ to cup $i$ in round $i$ . You may be able to score above $1/n$ , for instance with $n=3$ , you can add $1/2$ to the first cup (round 1), $1/4$ to each of the first and second cup (round 2), and again $1/4$ to each of the first and second cup (round 3), for a score of $3/8$ .","['optimization', 'combinatorics']"
4022312,Calculate $\int_0^{\infty} \frac{\cos 3x}{x^4+x^2+1} dx$,"Calculate $$\int_0^{\infty} \frac{\cos 3x}{x^4+x^2+1} dx$$ I think that firstly I should use Taylor's theorem, so I have: $$\int_0^\infty \frac{1-\frac{x^2}{2!}+\frac{x^4}{4!}-\dots}{(x^2+1)^2}dx$$ However I don't know what I can do the next.","['integration', 'complex-analysis']"
4022364,Eigenvalues of a particular block circulant matrix,"I need to compute all the eigenvalues of the following block-circulant matrix for a research. Can anyone help me compute the eigenvalues of the following matrix? $$\left[\begin{array}{l}2I&-I&0&0&0&...&0&0&-I\\-I&2I&-I&0&0&...&0&0&0\\0&-I&2I&-I&0&..&0&0&0\\.\\.\\.\\0&0&0&0&0&...&-I&2I&-I\\-I&0&0&0&0&...&0&-I&2I\end{array}\right]$$ In above, $I$ denotes the identity matrix of dimension $a$ and there are $r$ block-rows and block-columns, making the entire matrix have dimension $ar \times ar$ . Any help will be greatly appreciated!","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'circulant-matrices', 'block-matrices']"
4022398,Limit of $E(X_n-X\vert\mathcal F_n)$ or how to handle the fact that the conditional changes,"Consider real RVs $X_n$ that converge to $X$ a.s. and are dominated by an integrable $Y$ , so $\lvert X_n\rvert\le Y$ a.s. for all $n$ and $E(\lvert Y\rvert)<\infty$ . Now let $\mathcal F_n$ be a filtration. I want to prove that $$\lim_{n\rightarrow\infty}E(X_n-X\vert\mathcal F_n)=0$$ both in $L^1$ and almost surely. I have no clue how to handle the fact that the $\sigma$ -Algebra in the conditional expectation changes as well over $n$ . I'm not even sure I can pull in the limit at all in this case. Is there a tool/trick to work around the fact that the conditional changes as well?","['conditional-expectation', 'limits', 'convergence-divergence', 'probability-theory']"
4022415,Is it possible to split the natural numbers into a finite number of sets so that no pair of numbers within a set adds up to a square?,"My attempt: $\lbrace6,19,30\rbrace$ is sufficient to show that two sets are impossible. Using a computer program with a brute force method I found that separating the numbers $1$ through $85$ into three sets is possible as shown below: $\lbrace1,4,6,9,13,14,17,18,20,26,28,33,34,37,41,42,49,54,56,57,62,69,70,73,76,78,81,85\rbrace$ $\lbrace2,5,8,10,12,21,22,25,29,30,32,38,40,45,46,48,50,53,58,61,64,65,66,72,74,77,82,84\rbrace$ $\lbrace3,7,11,15,16,19,23,24,27,31,35,36,39,43,44,47,51,52,55,59,60,63,67,68,71,75,79,80,83\rbrace$ but $1$ through $86$ is impossible. Edit: WhatsUp in the answer below provides
the set of four numbers: $\lbrace1058, 6338, 10823, 13826\rbrace$ with an explanation of how he got them.  This is a alternative non-brute force way of showing that separating the natural numbers into three sets is impossible.
In a comment of this question a set of five numbers $\lbrace 7442, 28658,148583,177458,763442\rbrace$ is provided by the user Bob Kadylo. This shows that four sets are impossible. Edit 2: In a previous version of my post I made a proof showing that the number of sets needed for Natural numbers $1$ to $N$ so that no pair of numbers in the same set sums to a square is no more than $\lfloor\sqrt{2N-1}\rfloor$ . I realized that I can do significantly better than this. In order to explain the method that has a smaller upper bound I have to transform the problem into graph theory. An equivalent formulation is to have $N$ vertices labeled from $1$ to $N$ . A pair of points are connected iff the two points add up to a square. Then our goal is to color the vertices using the least number of colors so that no two vertices with the same color are connected. The first step in the greedy algorithm for coloring vertices is to make a list of colors with numbers. (ex. RED-1, BLUE-2, GREEN-3, YELLOW-4, etc.) If during the process more colors are required than are on the coloring list, add more colors to the list. The next step is to pick an uncolored vertex and use the lowest color number that isn't connected that the chosen vertex. Repeat the last step until all vertices are colored. The worst case scenario is to use one more color than the degree value of the vertex with the greatest degree (or tied with the greatest degree). If each vertex that is connected to the greatest degree vertex is a different color then the greatest degree vertex has to be a different color from all of those. The vertex with the greatest degree (or tied with the greatest) is $3$ . It has degree $\lfloor\sqrt{N+3}\rfloor-1$ . Therefore the Number of sets (or colors) required is no more than $\lfloor\sqrt{N+3}\rfloor$ . We can do slightly better by using brooke's theorem which states that if a graph is simple, connected, not complete, and not an odd cycle, then the upper bound of the number of colors is equal to the degree of the greatest degree vertex. This means that the new upper bound is $\lfloor\sqrt{N+3}\rfloor-1$ sets. This is the significant improvment from $\lfloor\sqrt{2N-1}\rfloor$ I mentioned at the beginning. End edits For each natural number $X$ there are $\lfloor\sqrt{2X-1}\rfloor-\lfloor\sqrt{X}\rfloor$ numbers that are less than $X$ that when summed to $X$ results in a square. The expression: $\lfloor\sqrt{2X-1}\rfloor-\lfloor\sqrt{X}\rfloor$ increases as $X$ gets larger, because of this fact my guess is that separating the natural numbers into a finite number of sets so that no pair of numbers in a set doesn't sum to a square is impossible.","['elementary-set-theory', 'square-numbers']"
4022450,A monotone sequence of sets is convergent,"I'm having trouble understanding the following part of a proof from Leadbetter's measure and probability book: THEOREM 1.4.2: A monotone increasing (decreasing) sequence { $E_n$ } is convergent and lim $E_n=\bigcup_{n=1}^‚àû E_n (\bigcap_{n=1}^‚àû E_n)$ . Proof: $$\bar{lim}E_n=\bigcap_{n=1}^‚àû (\bigcup_{m=n}^‚àû E_m )=\bigcap_{n=1}^‚àû (\bigcup_{m=1}^‚àû E_m )$$ But $\bigcup_{m=1}^‚àû E_m$ does not depend on n and thus, $$\bar{lim}E_n=\bigcup_{m=1}^‚àû E_m $$ In particular, I don't understand the logic behind moving from $\bar{lim}E_n=\bigcap_{n=1}^‚àû (\bigcup_{m=1}^‚àû E_m )$ to $\bar{lim}E_n=\bigcup_{m=1}^‚àû E_m $ . I understand that $\bigcap_{n=1}^‚àû (\bigcup_{m=1}^‚àû E_m )$ does not depend on n, but I don't see how removing the intersection wouldn't also change the union of all members of { $E_n$ }. Since a monotone increasing sequence of sequences contains more elements per $E_n$ as n increases, wouldn't the union of all subsequences of { $E_n$ } also be increasing, whereas the intersection wouldn't?","['elementary-set-theory', 'measure-theory', 'probability-theory']"
4022531,How to prove that a hexagon is the regular polygon with the most sides that can tile a plane [duplicate],"This question already has answers here : Why a tesselation of the plane by a convex polygon of 7 or more sides is not possible? (5 answers) Closed 3 years ago . I need to know the answer to this question to find out why bees use hexagonal cells in hives. I know that a circle takes up the most area using the least perimeter, so bees would try to make shapes as close to a circle as possible to use the most space without wasting too much material on walls. However, bees don't use circles because using circles creates a lot of waste space between cells, so bees use a shape that can tile a plane without overlap. The shape that meets the conditions of having a lot of sides and being regular to look like a circle, and being able to tile a plane  is the hexagon, so bees use this in their hives. However, I want to know whether or not this is the polygon with the most sides that fits this requirement. I try to prove there is no bigger polygon like this. First, I note that an integer number of interior angles must meet at a site and add up to $360$ degrees to tile a plane. For example, squares can tiles a plane because they all have $90$ degree angles, and $4$ of these make $360$ . All polygons have exterior angles adding up to $360$ , so a polygon with $n$ sides has $\frac{360}n$ degrees per side. However, interior and exterior angles are supplementary, thus the interior angle of an $n$ side polygon measures $180- \frac{360}n$ degrees. Now, let's test whether this quantity divides $360$ degrees. We get $360 \over 180- \frac{360}n$ , which simplifies down to $\frac {2n}{n-2}$ . Now, I need to prove that $n=6$ is the biggest number such that this quanitity is an integer, but I am not sure how I would go about  doing that.","['divisibility', 'elementary-number-theory', 'angle', 'geometry', 'tiling']"
4022544,"If $|E| = 0$, then there exists $h \in \mathbb{R}$ such that $E+h$ does not contain a rational point.","Here we are trying to show that if a set $E$ has zero (Lebesgue) measure, then there is a point $h \in \mathbb{R}$ such that $E+h$ does not contain a rational point. I tried to solve this using contradiction. The argument goes as follows: Suppose that for every $h \in \mathbb{R}$ we can find a rational point in $E+h$ . From here I am trying to show that if this is the case, then the measure of $E$ would not be zero, thus proving our claim. However, I am not sure how to formulate the argument to get to the end. I already looked at the previous questions posted on here for this type of problem, but it did not make much sense to me. Any help would be appreciated!","['measure-theory', 'real-analysis']"
4022548,Generating Pythagorean Triples Using a New Method?,"Using a right triangle with side lengths $(a,b,c)$ where $a , b < c$ ,
I was thinking about how the area of a Pythagorean triple can be found using the Pythagorean triple right before it and I came across something that worked for a large number of Pythagorean triples, $12r^2 + a_{k- 1}b_{k - 1} = a_kb_k$ , a recursive formula where $k$ represents the $k$ th term in a sequence. This seemingly generates a sequence of Pythagorean triples that I could not find used in any other formula. Its important to note that $12r^2$ is  twice the area of Pythagorean triples that stem from side lengths $(3,4,5)$ . Using this formula we can find the $1st$ term of sets where the inradius of each Pythagorean triple is $r + r^2k$ and the relationship between the side lengths are still defined by our recursive formula. These $1st$ terms are triplets with an even value of $a$ where $r$ increases by $2$ : $(8,15,17),(12,35,37),(16,63,65)...$ Note: We find this using $(8,15,17)$ as we have a recursive formula as well as the knowledge that $r = \frac{a + b - \sqrt{a^2 + b^2}}2$ ,  which lets us find the side lengths of each Pythagorean triple. Here is a sample of what they generate: $$\begin{array}{c|c|c|c} 
set_1&15,8,17&33,56,65&51,140,149&69,260,269 \\ \hline
set_2&35,12,37&85,132,157&135,352,377&185,672,697 \\ \hline
set_3 &63,16,65&161,240,289&259,660,709&357,1276,1325& \\ \hline
set_4&99,20,101&261,380,461&423,1064,1145&585,2072,2153  \\ \hline
\end{array}$$ I couldn't seem to find any similar formulas to this one or any method of generating Pythagorean triples that follow this sequence, I am looking for a proof.","['number-theory', 'solution-verification', 'geometry', 'pythagorean-triples']"
4022568,How to minimize exposed surface for half a pie?,"I bought a large pie (of radius $R$ ). I cut off a half and gave it to my friend. This exposed an area or $2Rh$ --  where $h$ is the pie's thickness -- to air.  I watched one Numberphile video too many , and now I want to rearrange my half of the pie so that the exposed area is minimal. I can get $\sqrt{2}Rh$ like this: Main question: Can I do better? Alternative question:  What if I had this in mind already when I was cutting off my half? That is, can I divide a pie into two equivalent parts such that one (and thus also the other) of the remaining parts can be rearranged to have even smaller exposed area?
(Here ""equivalent"" means ""such that they can be in turn subdivided  into finite number of subparts which can be matched into equal pairs"", equal, in turn,  being ""movable to each other, with the motion matching up the pieces inherited from the original boundary of the pie"".) My Osgood - Denjoy‚ÄìRiesz pie-cutting knife is being resharpened, so piecewise-smooth cuts only, please.","['plane-curves', 'dissection', 'geometry', 'plane-geometry']"
4022570,Showing $\sum_{k=1}^{n}k(k+1)(k+2)\cdots(k+r) = \frac{n(n+1)(n+2)\cdots(n+r+1)}{r+2}$,"I was solving a question and saw a pattern. Can someone prove it, please? We know $$\sum_{k=1}^{n}k = \frac{n(n+1)}{2}$$ $$\sum_{k=1}^{n}k(k+1) = \frac{n(n+1)(n+2)}{3}$$ $$\sum_{k=1}^{n}k(k+1)(k+2) = \frac{n(n+1)(n+2)(n+3)}{4}$$ So we see the pattern... Can anyone give a proof of why: $$\sum_{k=1}^{n}k(k+1)(k+2)\cdots(k+r) = \frac{n(n+1)(n+2)\cdots(n+r+1)}{r+2}$$","['algebra-precalculus', 'summation', 'sequences-and-series']"
4022607,Why does this Infinite Series have contradictory convergences?,"In my Calculus II homework, I encountered the following exercise: If the $n$ th partial sum of a series $\sum_{n=1}^\infty a_n$ is $$s_n = \frac {n-1}{n+1}$$ find $a_n$ and $\sum_{n=1}^\infty a_n$ . I solved the exercise this way: ( I took $S_n=(n-1)/(n+1)$ to be equation (1)) Equations (2) and (3) answer the exercise's questions. However, when I tried to corroborate my answer in equation (3) by using the found $a_n$ in equation (2), I encountered an inconsistency that I haven't yet been able to harmonize. This is what I tried to do: Equations (3) and (4) should have yielded the same answer but, this disparity I have been so far unable to harmonize. Your kind comments on what to do will be greatly appreciated.","['calculus', 'sequences-and-series']"
4022646,Upper bound on expected value of $\frac{1}{(1+X)^2}$ where $X$ is binomial,"As observed in Find the expected value of $E[\frac{1}{\left(X+1\right)^2}]$ where X is binomial , there exists a closed form solution to $$\mathbb{E}\left[\frac{1}{(1+X)^2}\right]$$ where $X$ is binomial random variable i.e $X \sim \textrm{Bin}(n,p)$ . I am interested in proving an upper bound which I expect to be $$\mathbb{E}\left[\frac{1}{(1+X)^2}\right] \leq \frac{c(p)}{n^2}$$ for $n>1$ where $c(p)$ is some constant dependent on $p$ . Any ideas on how to approach this? EDIT: After @NN2's answer, I changed the question from $O(1/n^2)$ to $c(p)/n^2$ for $n>1$ .","['expected-value', 'statistics', 'binomial-distribution', 'probability']"
4022657,Expectation of a betting game involving generalized Catalan numbers,"I've recently become interested in a type of biased random walk. In the original context, the simplest problem I cannot answer is as follows: Suppose you enter a simple but lousy betting game. The price to enter
is \$3, and a fair coin is flipped. If it lands heads you win \$4,
otherwise you win nothing. So, each game either increases your wealth by
\$1 or decreases it by \$3. If you begin with just \$3 and repeatedly
enter the game until you can no longer pay the fee, what is the
expected number of games you will play? Actually, I'd like to be able to answer this question for any game where the cost is $m$ , the prize is $m+1$ , and we have any given amount of starting wealth $w$ . This similar question+answer shows that the expectation diverges when $m=1$ for any starting wealth. Another way to phrase the question would be as a random walk. If you consider our position on the number line to be our total wealth after paying the fee, the case $m=w=1$ is a random walk from the origin that ends whenever the position becomes negative. The general case $m$ is a sort of random walk, but really it's more of a ""stumble"", since we either advance one step or reverse $m$ steps. I am interested in the expected length of these $m$ -stumbles. Here is what I know so far: The expected number of games, $g_{m,w}$ , satisfies an easy recurrence relating your starting wealth: $$ g_{m,w}  = 1 + \frac{1}{2}g_{m,w+1} + \frac{1}{2}g_{m,w-m}$$ Which I find has generating function: $$\sum_{w \ge 0} g_{m,w}z^n = \left(\alpha_m - \frac{2z}{1-z}\right)\left(\frac{1}{1 - 2z + z^{m+1}}\right)z^{m}$$ where $\alpha_m = g_{m,m}$ represents the average length of an $m$ -stumble from the origin. The $g$ terms are related to the Fibonacci n-step numbers by $A_{w}\alpha_m - 2B_{w}$ where A is a partial sum of the first $w$ n-step numbers $\sum_{k \le w}F_{k}^{(n)}$ , and B is a double convolution, i.e. the sum of the first $w-1$ partial sums $\sum\sum_{k < w}F^{(n)}_{k}$ . The easiest way to find $g_{m,w}$ given some $g_{m,s}$ that I know of is to use the recurrence directly. I have found the following double sum formula for general $\alpha_m$ : $$\alpha_m = \sum_{n = 0}^{\infty} \sum_{k=1}^{m} k\binom{mn + n + k}{n} / 2^{mn + n + k}$$ which more-or-less follows from the definition of the Fuss-Catalan numbers. Each term represents the contribution to the expectation of games ending at flip number $(m+1)n+k$ . The inner sum does not include the term $k=0$ because it is not possible to lose after flip numbers divisible by $m+1$ . (After flip $n$ your total wealth must be congruent to $n+m \mod{(m+1)}$ , since it increases by exactly $1$ or $-m \equiv +1 \mod{(m+1)}$ each flip, and it begins at $w = m \equiv -1 \mod{(m+1)}$ , so if you haven't already lost your wealth must be at least $m$ meaning you are guaranteed to have enough to pay the fee for these games.) I don't know how to evaluate this sum, but I do know at least one value and have good guesses for others. We know from the linked question $\alpha_1 = \infty$ , but for $m > 1$ , $\alpha_m$ converges. The one value I know is: $$\alpha_2 = \sum_{n=0}^{\infty} \binom{3n+1}{n}/2^{3n+1} + 2\sum_{n=0}^{\infty}\binom{3n+2}{n}/2^{3n+2} = 1 + \sqrt{5}$$ Which is the value Mathematica gives me. I don't how how it arrives at this value, but it appears correct and matches my simulations. Unfortunately, Mathematica chokes on $\alpha_3$ and higher. I guessed that $\alpha_m$ is the root of an $m$ 'th order polynomial, so $\alpha_2 = 1 + \sqrt{5} = (x^2 - 2x - 4)_{+}$ , meaning $\alpha_2$ is the unique positive real root of $x^2 - 2x - 4 = 0$ . A value for $\alpha_3$ would answer the initial problem statement in this question. I find the following guesses match the value of my sum with very high accuracy, to at least 100 digits: $$
\begin{align}
\alpha_3 &= (x^3 - 4x - 4)_{+} \\
\alpha_4 &= (3x^4 + 4x^3 - 8x^2 - 24x - 16)_{+} \\
\alpha_5 &= (2x^5 + 5x^4 - 20x^2 - 32x - 16)_{+}
\end{align}
$$ but even knowing these values I have no idea how to prove them, even for the apparently simple cases of $\alpha_2$ and $\alpha_3$ . Does anyone know how to find $\alpha_3$ or general $\alpha_m$ ?","['summation', 'catalan-numbers', 'binomial-coefficients', 'discrete-mathematics', 'probability']"
4022666,Is the image of this function open?,"Suppose $f: \mathbb{R}^n \to \mathbb{R}^n$ is continuous and for some $\lambda>0$ , we have $\|f(x)-f(y)\|\geq \lambda \|x-y\|$ for all $x,y\in\mathbb{R}^{n}$ . Is $f$ surjective? I can show the image is closed by showing it contains its limit points. So, if I can show it is also open, then I am done (I want to avoid using the Invariance of Domain theorem). Obviously, $f$ is one-to-one. By considering the the inverse map defined from $E=f(\mathbb{R}^n)$ back to $\mathbb{R}^n$ we get the following equivalent formulation of the problem: Suppose $E \subset \mathbb{R}^n$ is closed and $f\colon E \to \mathbb{R}^n$ is Lipschitz, one-to-one, and onto. Is $E$ necessarily equal to all of $\mathbb{R}^n$ ?","['multivariable-calculus', 'calculus', 'lipschitz-functions', 'real-analysis']"
4022675,How does Ian Stewart's Complex Analysis Textbook compare to the more commonly used text?,"I am trying to find a good complex analysis text for self study and I was wondering what people think of Ian Stewart's complex analysis text. I was trying to find some reviews on this forum but most people recommend other books and it seems like most people have not actually tried Stewart's text. Since I haven't really studied complex analysis, I can't tell exactly how good the text does at covering each topic. Has anyone tried self learning with this text?","['complex-analysis', 'soft-question', 'book-recommendation']"
4022824,Find the general solution to the ODE:,"I was asked to find the general solution to these ODEs. $$\frac{dy}{dx}=\frac{(-8x+3y-31)}{(-3x+y-11)}$$ I tried by rearranging to the form $M(x,y)+N(x,y)\frac{dy}{dx}=0$ and let $y=vx$ , but what I got was $$(v^2x-6vx-11v+8x+31)dx+x(-3x+vx-11)dv=0$$ and I could not separate $x$ and $v$ for integration. $$\frac{dy}{dx}=\frac{y}{x}+\frac{9}{2}xexp(-\frac{2y}{x})+\frac{9}{2}xexp(\frac{2y}{x})$$ I have totally no idea which method to be used in Question 2. May I know if I'm on the wrong track, and how should I solve them? Thanks.","['homogeneous-equation', 'ordinary-differential-equations']"
4022895,Number of ways to distribute 15 indistinguishable balls to 7 kids such that each kid gets a maximum of 3 toys?,"I want to find the number of ways to distribute 15 indistinguishable balls to 7 kids such that each kid gets a maximum of 3 toys. I tried splitting to different cases: first determining how many kids get 3 balls, then the number or kids that get 2, than the kids that get 1. But I got lost and it seemed to be too long anyway (this question was taken from an exam). The answer is 728. Any help would be appreciated.",['combinatorics']
4022997,Prove that the sum of derivatives of a non-negative function is non-negative [duplicate],This question already has answers here : Sum of derivatives of a polynomial (3 answers) Closed 3 years ago . Let $f(x)$ be a polynomial of degree $n$ with real coefficients such that $f(x)$ is non-negative for all real $x$ . Let $g(x)=f(x)+f'(x)+f''(x)+\dots$ be the sum of $f(x)$ and the first $n$ derivatives of $f(x)$ . Show that $g(x)$ is non-negative for all real $x$ . Thanks for any comments/answers in advance!,"['derivatives', 'polynomials']"
4023057,About the infinitude of the primes of the form: $x^2+y^2+d$ where $d‚â•1$ is a positive integer,When I read this page: https://en.wikipedia.org/wiki/Pythagorean_prime about the infinitude of Pythagorean primes. I am asking about the results about the infinitude of the primes of the form: $x^2+y^2+d$ where $d‚â•1$ is a fixed positive integer.,"['number-theory', 'prime-numbers', 'reference-request']"
4023081,Show that a matrix has full column rank,"Consider a matrix $A$ of size $T\times 2K$ . Consider the collection of numbers $k_1,k_2,...,k_T$ , where each $k_t\in \{1,...,K\}$ . Each $t$ -th row of the matrix $A$ is structured as follows: its $k_t$ -th element is equal to $1$ its $(k_t+K)$ -th element is equal to some scalar $\delta_t>0$ all other elements are zero. Further, $A$ does not contain zero columns . I believe that the following holds: $A$ has full column rank $2K$ if and only if there exists $t\in \{1,...,T\}$ and $\tau\in \{1,...,T\}$ with $t\neq \tau$ such that $$
k_t=k_{\tau} \quad \delta_t\neq \delta_{\tau}
$$ Question: I wrote a proof for this claim (below), which looks to me a bit ""naive"". Is there anything more formal and, perhaps, simpler that you can suggest? My proof: $A$ has full column rank $2K$ if and only if the following system admits as unique solution $x=0_{2K\times 1}$ : $$
(*)\hspace{1cm} x_{k_t} + \delta_{t} x_{k_t+K}=0 \quad \text{ for each $t\in \{1,...,T\}$}.
$$ First, I prove sufficiency.
If there exist $t,\tau$ such that $k_t=k_{\tau}\equiv k$ with $\delta_{t}\neq\delta_{\tau}$ , then system $(*)$ contains the equations $$
\begin{aligned}
& x_{k}+\delta_t x_{k+K}=0,\\
& x_{k}+\delta_{\tau} x_{k+K}=0,\\
\end{aligned}
$$ from which it follows that $x_{k}=x_{k+K}=0$ . If the above is true for every $k\in \{1,...,K\}$ , then $x_{k}=x_{k+K}=0$ for each $k\in \{1,...,K\}$ . Hence, the unique solution of system $(*)$ is $x=0_{2K\times 1}$ . Now, I prove necessity.
Suppose there exist only a $t$ such that $k_t\equiv k$ . Then, system $(*)$ contains only one equation involving  any of $x_k,x_{k+K}$ , which is $$
x_{k}+\delta_t x_{k+K}=0,
$$ which is satisfied for any values of $x_k, x_{k+K}$ such that $x_k=-\delta_t x_{k+K}$ . Therefore, system $(*)$ has not a unique solution.","['matrices', 'matrix-rank', 'linear-algebra']"
4023082,On positive dimensional Polish spaces in which every compact set has empty interior,"A standard characterization of the Baire space is that is the only nonempty, zero dimensional, Polish space in which every compact set has empty interior (up to homeomorphism of course). I'm interested in what happens when the zero-dimensional hypothesis is dropped: does there exist, for every $n\in\{1,2,3,\ldots,\infty\}$ , an $n$ -dimensional Polish space in which all compact sets have empty interior? How many such spaces up to homeomorphism are there if I insist that I only want spaces where the local dimension at every point is $n$ (this is to avoid producing many boring examples by taking disjoint unions with smaller dimensional spaces)? I know that every infinite dimensional, separable Banach space is an example for $n=\infty$ , and that we have the complete Erd≈ës space for $n=1$ , but I'm already having troubles finding more examples for finite $n\geq 2$ . Edit: Following discussion in the comments, for every $n$ the space $X_n=\Bbb R^n\setminus \Bbb Q^n$ satisfies $\dim X_n=n-1$ and all of its compact subspaces have empty interior (and it is clearly $G_\delta$ in $\Bbb R^n$ hence Polish). Note that for $n=1$ we recover the Baire space, while for $n=2$ we get an example distinct from the complete Erd≈ës space ( $X_2$ is connected, unlike the complete Erd≈ës space). I now suspect that there is an easy construction of infinite families in all dimensions starting from $X_n$ .","['general-topology', 'descriptive-set-theory', 'dimension-theory-analysis', 'polish-spaces']"
