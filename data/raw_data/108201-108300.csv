question_id,title,body,tags
1551410,Simplifying $2\cos(t)\cos(2t)-\sin(t)\sin(2t)$,"How do I simplify $2\cos(t)\cos(2t)-\sin(t)\sin(2t)$? I know this should be possible, but I don't know how. I have tried the $\cos(t)\cos(u)-\sin(t)\sin(u)=\cos(t+u)$, but I don't know what to do with the $2$ in front of $\cos(t)$.",['trigonometry']
1551426,Given $f(z)$ entire function and $\left| f(z) \right| \le 1 + \left| z \right|^3$ for all $z \in \mathbb{C}$ show that $f$ is a polynomial,"I'm learning about complex analysis and need some help with this problem : Given $f(z)$ entire function and $\left| f(z) \right| \le 1 + \left| z \right|^3$ for all $z \in \mathbb{C}$ show that $f$ is a polynomial. What is the degree of the polynomial? Here's my attempt so far : By Cauchy's Integral Formula for Derivatives we have: $$f^{(n)}(z) = \frac{n!}{2 \pi i} \int_{C} \frac{f(\zeta)}{(\zeta - z)^{n+1}} \, d\zeta $$ Let $C_R = \{z \in \mathbb{C} : \left| z \right| = R \}$. For $\left| z \right| < R$ we have: $$\left| f^{(n)}(z) \right| = \frac{n!}{2 \pi} \left| \int_{\left| \zeta \right| = R} \frac{f(\zeta)}{(\zeta - z)^{n+1}} \, d\zeta \right| = \frac{n!}{2 \pi} \left| \int_{0}^{2 \pi}\frac{\left|f(\zeta_t) \right|}{\left|\zeta_t - z\right|^{n+1}} \, \left| \zeta'_t \right| d\zeta \right| \le$$ $$\le \frac{n!}{2 \pi} \int_{0}^{2 \pi}\frac{1 + \left| \zeta(t) \right|^3}{(\left|\zeta(t) \right| - \left| z \right|)^{n+1}} \, \left| \zeta'(t) \right| dt = \frac{n!}{2\pi} \frac{1+R^3}{(R - \left| z \right|)^{n+1}} 2\pi R =$$ $$= n! \frac{1+R^3}{(R - \left| z \right|)^{n+1}} R$$ We want $\left| f^{(n)}(z) \right| \rightarrow 0$ when $R \rightarrow \infty$. This is where I'm stuck. Is my work correct so far and how do I continue from here to find the degree of the polynomial?","['cauchy-integral-formula', 'complex-analysis', 'complex-integration']"
1551437,Probability concerning summation of independent trials,"I am have a very hard time solving this particular question. If anyone can help my understanding by providing a helpful solution, it would be much appreciated. Thank you. Consider n independent trials, each of which results in one of the outcomes $1, \dots , k$ with respective probabilities $p_1,\dots,p_k$, $$\sum_{i=1}^{k}p_i = 1$$
Show that if all the $p_i$ are small, then the probability that no trial outcome occurs more than once is approximately equal to $$\exp\left(\frac{-n(n-1)}{2}\left(\sum_{i}^{}p_i^2\right)\right) $$","['summation', 'probability']"
1551486,"Decomposing a composite function: $f(g(x))=\frac{x^4+x^2}{1+x^2}$, then $f$=?","We have a composite function $f(g(x))=((x^4+x^2)/(1+x^2))$ and a single function $g(x)=1-x^2$. We want to know $f(1/2)$. From theory we know that $g(x)$ had been plugged in $f(x)$, but how would i find standalone $f(x)$ from here? I thought of dividing the result by $g(x)$, but that isn't true.","['algebra-precalculus', 'functions']"
1551538,"If $\mathbf{ABC}$ non-singular prove that $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ non-singular too","I am interested in the following exercise, and I have tried to solve it with the following way. Firstly , could you please check the correctness of the given answer. Secondly , can you give an alternative answer? $\textbf{Exercise}$: If the product $\mathbf{M} = \mathbf{ABC}$ of three square matrices is invertible, then $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ are invertible. $\textbf{Answer}$: Part 1 : If $\mathbf{C}$ is singular, there is $\mathbf{x} \neq 0$ such that $\mathbf{Cx} = 0 \iff \mathbf{ABCx} = 0 \iff \mathbf{Mx} = 0$. This comes in contradiction with the fact the $\mathbf{M}$ is non-singlular by the exercises definition. Thus $\mathbf{C}^{-1}$ exists. Using the last statement, we may write $\mathbf{MC}^{-1} = \mathbf{AB}$. Knowing that $\mathbf{M}$ and $\mathbf{C}^{-1}$ are invertible, we are interested to prove the invertibily of $\mathbf{MC}^{-1}$, so as to continue with similar way with the prof of matrix $\mathbf{B}$ invertibility. We may have: $$\mathbf{J} = \mathbf{MC}^{-1} \iff \mathbf{M}^{-1}\mathbf{J} = \mathbf{C}^{-1} \iff \mathbf{CM}^{-1}\mathbf{J} = \mathbf{I}$$ This means that matrix $\mathbf{MC}^{-1}$ has a left inverse given by $\mathbf{J}^{-1} = \mathbf{CM}^{-1}$. Part 2 : Based on the last statement, and similarly with the invertibility prof we followed for $\mathbf{C}$, If $\mathbf{B}$ is singular, there is $\mathbf{x} \neq 0$ such that $\mathbf{Bx} = 0 \implies \mathbf{ABx} = 0 \implies \mathbf{MC}^{-1}\mathbf{x} = 0 \implies \mathbf{J}\mathbf{x} = 0$. This comes in contradiction with the fact the $\mathbf{J}$ is non-singlular. Thus $\mathbf{B}^{-1}$ is invertible. Part 3 : Finally, we may write $\mathbf{A} = \mathbf{MC}^{-1}\mathbf{B}^{-1} = \mathbf{J}\mathbf{B}^{-1}$. Matrix $\mathbf{A}$ is non-singular, because: $$(\mathbf{JB}^{-1})^{-1}\mathbf{JB}^{-1} = \mathbf{I}~~\text{and}~~\mathbf{JB}^{-1}(\mathbf{JB}^{-1})^{-1} = \mathbf{I}$$ Thank you! PS: Changes have been made taking into account users comments. I hope the post is improved.",['linear-algebra']
1551562,Are Bezier curves invariant under conformal mapping?,"I've spent quite a bit of time on google trying to find information on whether or not Bezier curves are invariant under conformal mapping (i.e. a conformal mapping of all points on the curve is the same as a curve formed with just the conformal mapped control points for the curve). It feels like this should be true, but I can't seem to find anything that either proves or disproves this. Does anyone know whether Bezier curves are invariant under conformal mapping? Edit : based on Hagen's observation of a straight line becoming a circle, it no longer feels like this ""should"" be true! Although that does raise the question ""which conformal transforms are also affine transforms"", which I'm also having a bit of trouble googling (although I did find http://www.leptonica.com/affine.html )","['invariance', 'bezier-curve', 'conformal-geometry', 'geometry']"
1551591,Generalized pigeonhole principle: 15 workstations and 10 servers,"Q: Suppose that a computer science laboratory has 15 workstations and 10 servers. A cable can be used to directly connect a workstation
  to a server. For each server, only one direct connection to that
  server can be active at any time. We want to guarantee that at any
  time any set of 10 or fewer workstations can simultaneously access
  different servers via direct connections. What is the minimum number
  of direct connections needed to achieve this goal? This is an example from Rosen's Discrete Mathematics and Its Applications . The given solution to this example consists of 2 parts if I understood correctly. The first part includes finding the number of connections like the following: Suppose that we label the workstations $W_1, W_2, ..., W_{15}$ and the
  servers $S_1, S_2, ..., S_{10}$. Furthermore, suppose that we connect
  $W_k$ to $S_k$ for $k = 1,2, ..., 10$ and each of $W_{11}, W_{12}$,
  $W_{13}, W_{14}$, and $W_{15}$ to all 10 servers. We have a total of 60
  direct connections. Then I guess some kind of proof by contradiction is given to show that less than 60 connections is impossible. Now suppose there are fewer than 60 direct connections between
  workstations and servers. Then some server would be connected to at
  most $\lfloor 59/10\rfloor= 5 $ workstations.(If all servers were
  connected to at least six workstations, there would be at least $6.10$
  $= 60$ direct connections.) This means that the remaining nine servers are not enough to allow the other 10 workstations to simultaneously
  access different servers. Consequently, at least 60 direct connections
  are needed. But I can't understand this part. How is it concluded that remaining nine servers are insufficient when at most 59 connections are used?","['combinatorics', 'pigeonhole-principle', 'discrete-mathematics']"
1551592,Prove two polynomials are equal,"define$$f^{-1}(a)=\{z\in \mathbb{C}:f(z)=a\}$$
$f$ and $g$ are polynomials,
$$f^{-1}(0)=g^{-1}(0) ,f^{-1}(1)=g^{-1}(1)$$ prove $f=g$ I wonder if there is a complex analysis way to solve it?","['complex-analysis', 'polynomials']"
1551623,Continuity of min function,"I would like to ask a question about the continuity of this min function. Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be continuous. Define
$$F(x):=\min\{f(t):t \in [-x,x]\}.$$ I believe that the function $F$ is continuous on $\mathbb{R}.$
However, I do not know how to use the $\delta$-$\epsilon$ definition to show its continuity. Could somebody help me? Thank you.
Masih","['real-analysis', 'calculus', 'analysis']"
1551640,Prove H is in the center Z(G),"Exercise from Artin's 2nd edition of Algebra. Let $H$ be a normal subroup of prime order $p$ in a finite group $G$. Suppose that $p$ is the smallest prime that divides the order of $G$. Prove that $H$ is in the center $Z(G)$. My attempt:
 Choose any $e\neq h \in H$, since $h$ is normal, the conjugacy class including $h$ lies in $H$, so this class has size less than $p$. The only positive integer less than $p$ which divides $|G|$ is one. So the class containing $h$ has size one, and $h \in Z(G)$. Correct?","['abstract-algebra', 'group-theory', 'proof-verification']"
1551658,Showing a statistic is not complete,"Show that the order statistics from an i.i.d sample from the family of
  all symmetric distributions on the line with known center of symmetry
  are not complete. So without loss of generality, I can assume that the center of symmetry is along the origin by just reshifting the distribution. So let $f_\theta(x)$ be a distribution with symmetry along $x=0$. Let $X_1, \cdots, X_n$ be an i.i.d sample from $f_\theta$ and $X_{(1)}, \cdots, X_{(n)}$ be the order statistics I need to show that there exists some nonzero $\delta(X_{(1)}, \cdots, X_{(n)})$ so that $E_\theta\left[\delta(X_{(1)}, \cdots, X_{(n)}) \right] = 0$ But I'm unsure how to proceed. I would imagine that when integrating $\delta$ over a symmetric distribution, I should be able to exploit the symmetry to have $\delta \neq 0$ but the integral will be.","['statistics', 'order-statistics', 'probability-distributions']"
1551666,Confusion about differentials,"If we have some differentiable function $f(x)$, then by the definition, we have: $$\mathrm{d}y = f'(x) \mathrm{d}x$$ This makes sense; the differential $\mathrm{d}y$ can be thought of as a function that expresses the linear change in $y$ given a finite change $\mathrm{d}x$ in $x$. Now, what confuses me is why we can simply do this: $$\int \mathrm{d}y = \int f'(x) \mathrm{d}x$$ and just magically append integrals to both sides of the equation. Some people tell me that the ""differentials"" in the integrand aren't really being multiplied by the integrand, and others tell me that they are. If they are not multiplied by the integrand, then I don't see why that above step is justified. $\mathrm{d}y$ has changed from a finite quantity to a ""closing parentheses"" on the integral once we performed the integration. But somehow, this method works, and it is essentially how most students are taught (including myself) to solve very simple differential equations. To complicate matters further, my physics teacher explained that $\frac{dy}{dx}$ is a quotient of infinitesimal changes, and therefore $\mathrm{d}y = f'(x) \mathrm{d} x$ is quite easy to see. The integration symbols are just summing these infinitesimals over a given region (whether it be an area, a volume, or in the above case, a simple one dimensional segment). From this viewpoint, we are actually multiplying the integrand by $\mathrm{d}x$. Although this ""definition"" of differentials disagrees with their definition as finite quantities (as described at the top of the question) , it does seem to satisfy my intuition quite nicely. For instance, it solves the mystery of ""magically appending"" integrals to both sides of an equation. For example, consider the integral: $$\int x \cos\left(x^2\right) \mathrm{d}x = \frac{1}{2} \int \cos(u) \mathrm{d}u$$ where I have simply replaced $x \times \mathrm{d}x$ with $\frac{1}{2} \mathrm{d}u$. If the differentials weren't part of the product, and were just ""closing parentheses"" (as some people have told me), then why does this work? Viewing the differential as an infinitesimal part of a product therefore has nice properties; not only is it dimensionally accurate, but it also makes these $u$-substitutions quite intuitive. If $\mathrm{d}x$ were just a closing parentheses on the integral, I don't see how we can substitute $x$ times a closing parentheses with another closing parentheses. But recently, in my multivariable class, we learned that: $$\mathrm{d}x \mathrm{d}y = \left| \frac{\partial(x, y)}{\partial(u, v)}\right| \mathrm{d}u \mathrm{d}v$$. and we learned that the differential element $\mathrm{d}A = \mathrm{d}x \mathrm{d}y$ is the area of an infinitesimally small rectangle. Under the transformation of the jacobian, an infinitesimally small rectangle area in $uv$-space differs from that of a rectangle in $xy$-space by a factor of the jacobian's determinant. This matrix is essentially transforming $uv$-space areas into $xy$-space areas. This makes sense, but when coupled with the definition of a differential, I see an apparent contradiction. Consider the polar coordinate transformation, in which we have $x = r \cos{\theta}$ and $y = r\sin{\theta}$. By the above equation, we have that $\mathrm{d}x \mathrm{d}y = r \mathrm{d}r \mathrm{d}\theta$. But if we were to compute the differentials of $x$ and $y$ and multiply them together, expanding the product would not give us $r \mathrm{d}r \mathrm{d}\theta$. It would give us a bunch of ugly stuff, and I'm not even sure this ""stuff"" is meaningful. If the differentials were truly being multiplied by one other, then after expanding their product (which I am not going to do here), the result would be $r \mathrm{d}r \mathrm{d}\theta$. But this is not the case. So in the end, neither the viewpoint of differentials as ""infinitesimals"" multiplied to the integrand, nor the viewpoint of differentials as linear changes makes sense to me. If differentials were infinitesimals multiplied to an integrand, then why does expanding the product $\mathrm{d} x(r, \theta) \times \mathrm{d}y(r, \theta)$ not yield $r \mathrm{d}r \mathrm{d}\theta$? If differentials were finite, linear changes, why can we just magically ""slap on"" integrals on both sides of the equation $\mathrm{d}y = f'(x) \mathrm{d}x$? And probably most importantly, are differentials just closing parantheses, or are they actually multiplied by the integrand? If we consider them as part of an infinitesimal product, then this makes appending integrals to both sides justified. But if they are just ""closing parantheses"", then why is appending integrals to both sides of an equation justified? A finite quantity ($\mathrm{d}x$) cannot magically ""transform"" into a piece of notation by simply writing an integral symbol on both sides of the equation. I am very confused about differentials, and I would appreciate if someone could clarify exacty why multiplying them doesn't work in the intuitive way described above, and why we are allowed to just append integrals to both sides of an equation.","['multivariable-calculus', 'ordinary-differential-equations', 'calculus']"
1551684,"In a proof of ""weakly measurable implies measurable""","This is a follow-up question to the following ones: How can this theorem about weakly measurable functions on $\sigma$-finite measure spaces be deduced from the finite measure space case? properties of a separable metrizable locally convex space I'm reading the proof the following theorem: If $X$ is a separable, metrizable locally convex space, $(\Omega,\Sigma,\mu)$ is a $\sigma$-finite measure space, and $f:\Omega\to X$ is weakly measurable, then $f$ is strongly measurable. The first part of the proof shows that $f$ is measurable: We can restrict ourselves to the case of finite measures. We prove first that $f$ is measurable. To do this it suffices to show that 
  $$
f^{-1}(x+V)\in\Sigma
$$
  whenever $x\in X$ and $V$ is a neighborhood of $0$ and a barrel.  Replacing $f$ by $f-x$, we may also assume that $x=0$. Choose points $y_n\in X\setminus V$, and open convex neighborhoods $V_n$ of $0$ so that
  $$
X\setminus V=\bigcup_{n=1}^\infty(y_n+V_n).
$$
  Choose next functional $\varphi_n\in X^*$ such that
  $$
\Re\varphi_n(y_n+v_n)>\Re\varphi_n(v)\tag{*}
$$
  whenever $v_n\in V_n$ and $v\in V$. ($\color{red}{Question:}$ Why $\varphi_n$ exists? Are we using some separation theorem here?) Then the set
  $$
f^{-1}(x+V)=\bigcap_{n=1}^\infty\{\omega\in\Omega:\Re \varphi_n(f(\omega))\leq\sup_{v\in V}|\varphi_n(v)|\}\tag{**}
$$ 
  is a countable intersection of sets in $\Sigma$. ($\color{red}{Question:}$ I'm quite sure that $\Re f(\omega)$ is a typo, which might be $\Re \varphi_n(f(\omega))$. Could anyone explain how to get (**)? I don't see it has anything to do with (*). ) Could somebody help me with the questions indicated in the proof? Also, I'm wondering if anyone could come up with a cited reference for the proof of this theorem. [Updated:] One more typo fixed. It is quite clear now that one can use (*) to show (**).","['functional-analysis', 'reference-request', 'locally-convex-spaces', 'measure-theory']"
1551712,Functions satisfying the functional equation $ \big( 1 - f ( x ) f ( y ) \big) f ( x + y ) = f ( x ) + f ( y ) $,"How can I prove that there is no real function defined on $ \mathbb R $ , continuous at $ 0 $ and not always vanishing satisfying the functional equation $$ \big( 1 - f ( x ) f ( y ) \big) f ( x + y ) = f ( x ) + f ( y ) \text ? \tag E $$","['continuity', 'functions', 'functional-equations']"
1551768,Geometrical meaning of orientation on vector space,"Can any one explain, the geometrical meaning of orientation in a vector space?","['orientation', 'linear-algebra', 'vector-spaces']"
1551791,How to deal with this boundary condition when using separation of variables?,"Consider Laplace's equation $\nabla^2u = 0$ on the region $A = [0,a]\times [0,b]\times [0,c]\subset \mathbb{R}^3$ and suppose we impose the boundary conditions: $$u(0,y,z) = \sin \frac{\pi y}{b} \sin \frac{\pi z}{c}, \quad u(a,y,z)=0 \\ u(x,0,z)=u(x,b,z)=u(x,y,0)=u(x,y,c)=0.$$ In that case if we use separation of variables we set $u(x,y,z)=X(x)Y(y)Z(z)$ and the equation reduces to $$\frac{X''}{X}+\frac{Y''}{Y}+\frac{Z''}{Z}=0.$$ In that case reorganizing gives $$\frac{X''}{X}+\frac{Y''}{Y}=-\frac{Z''}{Z}=\lambda$$ for some constant $\lambda$. Thus we have two new equations, one of which is $$\frac{X''}{X} = \lambda-\frac{Y''}{Y} = \mu,$$ for some constant $\mu$. Finally we end up having three equations $$\begin{cases}Z''+ \lambda Z &= 0, \\ Y'' + (\mu+\lambda)Y&=0, \\ X'' - \mu X &=0\end{cases}.$$ The boundary conditions which are zero on the boundaries transfer nicely to the functions $X,Y,Z$. That is, except the first condition, all the other becomes $$X(a)=0, Y(0)=Y(b)=0, Z(0)=Z(c)=0.$$ With this the equations for $Y$ and $Z$ with those boundary conditions becomes Sturm Liouville problems. On the other hand the first condition does not separate nicely. We have $$X(0)Y(y)Z(z) = \sin \frac{\pi y}{b} \sin \frac{\pi z}{c}.$$ This boundary condition doesn't seem to transfer nicely to $X$. So how do we deal with this boundary condition? How do we use it to solve the problem in question?","['sturm-liouville', 'partial-differential-equations', 'mathematical-physics', 'multivariable-calculus', 'boundary-value-problem']"
1551832,Why there are no brackets in $e^x$ or $\ln x$?,"I have seen that one uses often brackets when one evaluates functions, like $f(3)=5$, never $f3=5$. But why some textbooks makes it clear that it is okay to write $\ln 3$ or $e^3$ instead of $\ln (3)$ or $e^{(3)}$? Why do $\ln, e, \sin,\cos,\tan\ldots$ uses different rules of bracketing compared to $f,g,h\ldots$?","['notation', 'functions']"
1551868,Does the binomial theorem hold by default for the Cartesian Product of intervals?,"I was wondering if the binomial theorem holds for Cartesian products of intervals?
For example, does this inequality hold: $$[(a,b) + (c,d)] ^2=(a,b) \times (a,b)+2\cdot(a,b) \times (c,d)+(c,d) \times (c,d)\ \text{?}$$ I somehow feel that this is a dumb question but still hope that I get a clarifying answer. Thank you in advance...","['elementary-set-theory', 'real-analysis', 'analysis']"
1551873,Differential and Rank of $XAX^{-1}-A$,"I have a map: $F_{A} (X) :GL\left(2n,\mathbb{R}\right) \longrightarrow\mathbb{\mathfrak{M}_{\mathit{2n\times2n}}\left(\mathbb{R}\right)}$
such as \begin{eqnarray}
 & F_{A}(X) & =XAX^{-1}-A
\end{eqnarray} where $X\in GL(2n,\mathbb{R})$ and $A=\left(\begin{array}{cc}
0 & -I_{n}\\
I_{n} & 0
\end{array}\right)$ And I need to find the rank of this map I tried to develop in for $n=1$ just to see what's going. 
What I obtained
$$F_{A}(X)= \frac{1}{\det X} \left(\begin{array}{cc}
X_{1} & X_{2}\\
X_{3} & X_{4}
\end{array}\right) \left(\begin{array}{cc}
0 & -I_{n}\\
I_{n} & 0
\end{array}\right)\left(\begin{array}{cc}
-X_{4} & +X_{2}\\
+X_{3} & -X_{1}
\end{array}\right)-\left(\begin{array}{cc}
0 & -I_{n}\\
I_{n} & 0
\end{array}\right)$$ $$F_{A}(X)=\frac{1}{\det X}\left(\begin{array}{cc}
\left(-X_{1}X_{3}-X_{2}X_{4}\right) & \left(X_{1}X_{1}+X_{2}X_{2}\right)\\
\left(-X_{3}X_{3}-X_{4}X_{4}\right) & \left(X_{3}X_{1}+X_{4}X_{2}\right)
\end{array}\right)-\left(\begin{array}{cc}
0 & -I_{n}\\
I_{n} & 0
\end{array}\right)$$
but I'm quite stuck. I could go on with brute force and resolve the $n=1$ case, but I'm pretty sure that since it seems a really natural application there should be a known formalism or even a known formula where this kind of things are evident. Can You give me an hint?","['multivariable-calculus', 'differential-geometry', 'multilinear-algebra']"
1551913,About the second-order linear ODEs with polynomial coefficients,"I know that for $p(x)y''+q(x)y'+r(x)y=0$ : When $p(x)$ is an incomplete-square quadratic polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Confluent Equation . When $p(x)$ is a complete-square quadratic polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Doubly-Confluent Equation . When $p(x)$ is a linear polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Biconfluent Equation . When $p(x)$ is a constant, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Triconfluent Equation . How about the types of the ODEs belonging when $p(x)$ is either an incomplete-square quadratic polynomial or a complete-square quadratic polynomial, $q(x)$ is a linear polynomial, or a constant, or even the $y'$ term vanishes instead?",['ordinary-differential-equations']
1551928,Is there a $\nabla^3 f(x)$?,"In multivariable calculus we have so far seen the gradient, and the hessian, So it is natural to ask whether if $\nabla^3 f(x)$ exists Can anyone let me know what comes after the hessian?","['multivariable-calculus', 'hessian-matrix']"
1551952,Does the idempotent law for union of a set imply that the measure of a set is zero?!!!,"Sorry in advance for this question!!! The idempotent law for union of a set is : A= A $\cup$ A But it seems to imply that for the measure we get: m(A)=m(A)+m(A)=2$\cdot$m(A), which, then would imply that m(A)=$0$ And so,the measure of every set would be zero!!!!
What I am doing wrong????","['measure-theory', 'elementary-set-theory']"
1551962,Expected number of remaining pairs of animals,"Suppose that Noah started with $n$ pairs of animals on the ark and that $m$ animals died. If the $m$ animals were chosen randomly, what is the expected number of complete pairs left? My attempt: Total animals: $2n$ Probability of dying: $\frac{m}{2n}$ The probability that a pair $X_i$ is left intact is $$\left(1-\frac{m}{2n}\right)^2,$$ so the expected number of remaining pairs is $$n\left(1-\frac{m}{2n}\right)^2$$ Am I correct?","['expectation', 'probability']"
1551976,When exponential of a matrix is diagonal?,"$\newcommand{\C}{\mathbb{C}}$
$\newcommand{\R}{\mathbb{R}}$
$\newcommand{\ga}{\gamma}$
$\newcommand{\al}{\alpha}$ Let $A$ be an $n \times n$ real matrix. Assume $e^A$ is a diagonal matrix. Does this imply $A$ is diagonal? If not, then for which matrices, their exponential is diagonal? Can we obtain some nice characterization? (The complex case might also be interesting) Update: Define $G=\C^* = \C \setminus \{0\}$ to be the group of nonzero comlplex numbers, with multiplication. Let $H=\{\begin{pmatrix}a&b\\-b&a\end{pmatrix}|a,b \in \R \, , \, a^2+b^2 \neq 0 \}$ be a group of $2 \times 2$ real matrices (with the operation of matrix multiplication). Look at the following group isomorphism: $\phi:G \to H, \phi(a+ib)=aI+bJ$, where $I=\begin{pmatrix}1&0\\0&1\end{pmatrix} \, , \, J = \begin{pmatrix}0&1\\-1&0\end{pmatrix}$ By the theory of Lie groups, we know: $$(1): \, \,  \phi(\exp^G(v))=\exp^H(\phi_*(v))$$ Since $H$ is a subgroup of $GL_2(\R)$ $\exp^H$ is just the usual matrix exponential. Note that $T_eG \cong \C$, We claim, $\exp^G(z)=e^z$ (where $e^z$  is the standard complex exponential). Proof: Let $v \in T_eG = \C$. Define $\ga:I \to \C^*$, $\ga(t)=e^{tv}$. Then $\ga$ satisfies $\ga(0)=1,\dot \ga (0) = v, \ga(t+s)=\ga(t)\cdot\ga(s)$, so $\ga$ is a one-parameter subgroup in $G=\C^*$ with initial velocity $v$.
By definition, $\exp^G(v)=\ga(1)=e^v$, as required. Hence, equation $(1)$ becomes $(1'):$ $$(1'): \, \,  \phi(e^v)=\exp(\phi_*(v))$$ Let us calculate $\phi^*=(d\phi)_e:T_eG \to T_IH \subset M_2$.
Let $v=x+iy \in T_eG=\C$. Define $\al(t)=1+tv=(1+tx)+i(ty),\dot \al(0)=v$, and $$\phi^*(v)=(d\phi)_e(v)=\frac{d}{dt}\big(\phi(\al(t))\big)|_{t=0}= \frac{d}{dt}\big( \begin{pmatrix}1+tx&ty\\-ty&1+tx\end{pmatrix}\big)|_{t=0}=\begin{pmatrix}x&y\\-y&x\end{pmatrix}$$ So, Hence, equation $(1')$ becomes $(1''):$ $$(1''): \, \,  \phi(e^{(x+iy)})=\exp(\begin{pmatrix}x&y\\-y&x\end{pmatrix})$$, Finally, since $\phi(e^{(x+iy)})=\phi(e^x\cos y+e^x\sin y)=\begin{pmatrix}e^x\cos y&e^x\sin y\\-e^x\sin y&e^x\cos y\end{pmatrix} $ we get the following formula: $$\exp(\begin{pmatrix}x&y\\-y&x\end{pmatrix}) = \begin{pmatrix}e^x\cos y&e^x\sin y\\-e^x\sin y&e^x\cos y\end{pmatrix} $$ In particualr, taking $x=0,y=t$ we get: $$\exp(\begin{pmatrix}0&t\\-t&0\end{pmatrix}) = \begin{pmatrix}\cos t&\sin t\\-\sin t&\cos t\end{pmatrix} $$","['matrices', 'linear-algebra']"
1551994,Combinatorics and trigonometry identity,"Prove the following: $\displaystyle\prod_{n=1}^{180}\left(\cos{\left(\dfrac{n\pi}{180}\right)}+2\right)=\displaystyle\sum_{n=0}^{89}\binom{180}{2n+1}\left(\dfrac{3}{4}\right)^n$. We can state this as $\displaystyle \binom{180}{1}\left(\dfrac{3}{4}\right)^0+ \binom{180}{3}\left(\dfrac{3}{4}\right)^1+\cdots+\binom{180}{179}\left(\dfrac{3}{4}\right)^{89} = \left(\cos\left(\dfrac{\pi}{180}\right)+2\right)\left(\cos\left(\dfrac{2\pi}{180}\right)+2\right) \cdots \left(\cos\left(\dfrac{180\pi}{180}\right)+2\right)$ The problem I am encountering is that there is no general result we can prove about this for $n$ it seems. I tried using induction on $n$ to prove this but it fails for the base case. Also, combinations and cosines seem unrelated, so how could I prove the equation is true?","['combinatorics', 'trigonometry']"
1552002,How do you show the following is diagonalizable?,"So if $A$ is diagonalizable, how would you show the following is diagonalizable? a)$p(A),p(x)$ (any polynomial) b)$kI+A$ for any scalar $k$ c)$U^{-1}AU$ for any invertible matrix $U$. So for the first one, I have no idea how polynomials associate with diagonalization. And neither can think of anything for the third. For the second one, I assume that $P^{-1}(kI+A)P=P^{-1}kIP+P^{-1}AP=kI+D$, since $kI$ and$D$ each is diagonalizable, $kI + D$ is diagonalizable. But I felt I have a wrong idea about this, someone help?","['matrices', 'diagonalization', 'linear-algebra']"
1552015,Cauchy Sequence in Normed Space,"Let $(E, ||\cdot ||)$ be a normed space and let $(x_n)$ be a sequence in $E$. Show that the following
conditions are equivalent: (a) $(x_n)$ is a Cauchy sequence. (b) For every increasing function $p: \mathbb{N} \to \mathbb{N}$ we have $||x_{p(n+1)} - x_{p(n)}||\to 0$ as $n \to \infty$. I've encountered this problem in my graduate level analysis class.
The definition I have for Cauchy sequence is: $\forall  \varepsilon >0$  $\exists \, n_0$ such that $\forall m,n>n_0$, $||x_n-x_m||< \varepsilon$ (or $||x_n-x_m||\to 0$ as $n,m\to \infty$). Thank you for any assistance here.","['normed-spaces', 'real-analysis', 'cauchy-sequences', 'analysis']"
1552069,Justifying the result of $\lim_{x\to\infty} \tfrac{3^x}{4^x}$,"I am working with the next limit: $$\lim_{x\to\infty} \frac{3^x}{4^x}$$ I intuitively know that since $$3^x< 4^x$$ when $x$ tends to infinite,
the result of the limit is: $$\lim_{x\to\infty} \frac{3^x}{4^x}=0$$ However, I need a some more mathematical justification rather than the intuitive justification, I would appreciate any help or hint to justify this result, the l'Hospital's rule doesn't work for this limit, since if I apply the rule, the limit remains similar.",['limits']
1552136,How to prove $\sum_p p^{-2} < \frac{1}{2}$?,"I am trying to prove $\sum_p p^{-2} < \frac{1}{2}$, where $p$ ranges over all primes. I think this should be doable by elementary methods but a proof evades me. Questions already asked here (eg. What is the value of $\sum_{p\le x} 1/p^2$? and Rate of convergence of series of squared prime reciprocals ) deal with the exact value of the above sum, and so require some non-elementary math.","['number-theory', 'prime-numbers', 'sequences-and-series']"
1552185,Showing truncated Poisson process has no unbiased estimator,"Suppose that $X$ has the Poisson distribution truncated on the right at $a$, so that it has the conditional distribution of $Y$ given $Y \leq a$, where $Y$ is distributed as $P(\lambda)$. Show that $\lambda$ does not have an unbiased estimator. I'm trying to prove this using a proof by contradiction. Suppose $\delta(X)$ is an unbiased estimator for $g(\lambda) = \lambda$. Then: $$\sum_{k=0}^{\infty} \delta(k)\frac{e^{-\lambda} \lambda^k}{k!} 1_{k \leq a} = \lambda $$ Rearranging I get: $$\sum_{k=0}^{\infty} \delta(k)\frac{\lambda^{k-1}}{k!} 1_{k \leq a} = e^{\lambda}  $$ Is this a contradiction because the finite sum will never equal $e^\lambda$? Or do I need another argument?","['parameter-estimation', 'statistics', 'poisson-distribution']"
1552209,Noncommuting complex matrices: Existence of a simultaneous eigenvector,Let $A$ and $B$ be $n\times n$ matrices with complex entries such that $AB - BA$ is a linear combination of $A$ and $B$ . I'd like to prove that there exists a non-zero vector $v$ that is an eigenvector of both $A$ and $B$ .,"['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'lie-algebras']"
1552283,Primes on the form $p^2-1$,Prove that there exists a unique prime number of the form $p^2 − 1$ where $p\geq 2$ is an integer. I have no idea how to approach the question. any hints will be greatly appreciated,"['discrete-mathematics', 'prime-numbers', 'elementary-number-theory']"
1552352,Expected Value for $n$ values,"I've been thinking about this for a while and I can't seem to figure it out. 
In a class, a teacher will call a student to answer a questions and the number of questions $n$ is equal to the number of students. A question is asked to one student at random. Using linearity of expectation, if there are $n$ students and $n$ questions, what is the expected number of students that don't get called? I decided to try it out with $n=2$. Using $1$ = called, and $0$=not called, there are 4 possibilities $(11, 10, 01, 00)$. Then, we would have to multiply the probability with the value. Now, I am totally lost. There is a probability of $1/4$ is for $00$, and $1/4$ for both $01$ and $10$ each. Then is the expected value just $3/4$? And how would I apply it to $n$ cases, instead of just $n=2$?","['statistics', 'expectation', 'discrete-mathematics']"
1552390,"MGF, Calculate the standard deviation of $X + Y$ . Method.","Two claimants place calls simultaneously to an insurers claims call
  center. The times X and Y , in minutes, that elapse before the
  respective claimants get to speak with call center representatives are
  independently and identically distributed. The moment generating
  function for both $X$ and $Y$ is
  $$ M(t) = \left( \frac{1}{1-1.5t}\right)^2, \quad t< \frac{2}{3}.$$
  Calculate the standard deviation of $X + Y$ . I would like to know what approaches I need to use to solve this exercise. I am practicing for an actuarial exam and I am looking for the method that will minimize the amount of time spent on these types of problems. Is it correct this answer, (I used the coment below my post): X and Y are Erlang(2, 2/3) and independent, then var(x+y)=var(x)+var(y)+2 cov(x,y)
because are independent, Var(x+y)= 2 var(x)= 2 n * (alpha)^2  =2 (2) (2/3)^2 Then, satandar deviation= 2 * (alpha) = 4/3","['statistics', 'probability', 'actuarial-science']"
1552426,Are uniformly continuous function $f:\mathbb{R}^m \to \mathbb{R}^m$ bounded?,"I'm trying to understand how uniform continuity relates to the solutions of ODEs, and I have a hunch that because uniformly continuous functions are 'well-behaved' functions, the solutions to uniformly continuous ODEs do not escape to infinity in a finite time. One attempt at proving this required the condition that the range of a function $f:\mathbb{R}^m \to \mathbb{R}^m$ given to be uniformly continuous is bounded (i.e. it takes on some maximum value $K$, and $f(x) \leq K$ for all $x$ in its domain); is this a valid assumption to make? My current rationale for making this assumption is this: since $f$ is given to be uniformly continuous, it can't tend toward infinity on its range, because this would allow us to choose larger and larger $\epsilon$ values that contradict the uniform continuity hypothesis for a given $\delta$. Any clarification would be much appreciated!","['uniform-continuity', 'real-analysis', 'ordinary-differential-equations']"
1552432,Implicit differentiation to find derivatives of a function whose only defined by its derivative,"A question I have asks if $K(x)$ satisfies $K(1)=0$ and $K'(x)={1\over{x}}$ then show: If $f(x)=K(10x)$ then $f'(x)={1\over{x}}$ So Im not sure if I have to prove it or do something else but this is what I did.
$$
f'(x)=K(10x)^0\cdot K'(10x) \cdot 10
\\
f'(x)={1\over{10x}}\cdot 10={1\over{x}}
$$
The other questions are similar to this so if this is okay then im sure I did the rest okay as well. Then it asks:
$$
Let \ g(x)=K(x^2+1)\ \ \ \ find \ g'(x)
$$
So I do:
$$
g'(x)=K'(x^2+1)\cdot 2x = {2x\over{x^2+1}}
$$
At this point I havent used the $K(1)=0$ so im not sure if im doing this right. The last sub question asks: If $a,b >0$ show $K(ab) = K(a)+K(b)$. Which I have no idea where to start. Any feedback appreciated, thanks.","['derivatives', 'implicit-differentiation', 'real-analysis', 'calculus']"
1552444,"From actuarial exam: Calculate the variance of the retirement package for a new employee, > given that the value is at least 10.","The distribution of values of the retirement package offered by a
  company to new employees is modeled by the probability density
  function, Calculate the variance of the retirement package for a new employee,
  given that the value is at least 10. I know that, Var(Y)= E(V(Y|X>10)+ V(E(Y|X>10)), is an equation that show a relationship between both. But I have no idea how to do this exercise. Thanks, any advise it will be apreciated.","['probability-theory', 'probability', 'statistics']"
1552447,Fundamental group of real projective plane minus one point,"I understand that the $\mathbb{R}P^2$ is homeomorphic to the unit disc with boundary points identified with their antipodes. But even if we puncture the disc and stretch it from the origin to let it be retracted to the boundary, how are we gonna justify that the identified boundary is still homeomorphic to $S^1$(or is it?)?","['algebraic-topology', 'general-topology']"
1552453,Expected Value of edges,"I've been thinking about this for a while and I couldn't understand how we could apply this to a bigger case. There's a graph with x vertices and y edges. The vertices are randomly put into set A or set B, with a 1/2 chance for getting put in A, and a 1/2 chance for getting put in B. The cut is a set of edges where there is an endpoint in A and one in B. What is the expected number of edges that will be in the cut? I looked at this starting from a 2 vertices graph. In this case, there could be either 2 vertices in A or 2 vertices in B, or 1 in B and 1 in A (and vice versa). It would be 1/2 (1). But I don't know how this could be applied to a generic x vertices and y edges? Could this involve a binomial distribution?","['graph-theory', 'expectation', 'discrete-mathematics']"
1552460,Derivative of a nonsingular matrix,Show that : $$\frac{d}{dt} A^{-1}(t) = -A^{-1}(t) (\frac{d}{dt} A(t) ) A^{-1}(t) $$ A(t) is a matrix.,"['derivatives', 'matrices', 'normed-spaces', 'matrix-decomposition', 'linear-algebra']"
1552485,Symmetric closure of the reflexive closure of the transitive closure of a relation,"Give an example to show that when the symmetric closure of the reflexive closure of the transitive closure of a relation is formed, the result is not necessarily an equivalence relation. My attempt at a solution: $R = \{(2,1),(2,3)\}$ . Transitive closure: $\{(2,1),(2,3)\}$ . Reflexive closure: $\{(1,1),(2,1),(2,2),(2,3),(3,3)\}$ . Symmetric closure: $\{(1,1),(1,2),(2,1),(2,2),(2,3),(3,2),(3,3)\}$ . Since the set is missing $(1,3)$ and $(3,1)$ to be transitive, it is not an equivalence relation. I am not sure if this is correct.","['equivalence-relations', 'discrete-mathematics']"
1552593,Union of infinite sets,"Let $A = \cup_{i \ge 1} A_i$, $i = 1, 2, \cdots$. This is union of countably infinite sets. Also, $A_i \subsetneq B$ for all $i$, i.e. for every $A_i$, there exists at least one element in $B$ that is not in $A_i$. It is also the case that $A_{i-1} \subsetneq A_{i}$ for all $i > 1$. Then is it true that $A \subsetneq B$? Intuitively, it seems true because for every $A_i$, I can point to an element that is in $B$ but not in $A_i$. However, it is not clear to me what happens after you take union of countably infinite sets.",['elementary-set-theory']
1552622,Finding extrema of $\frac{\sin (x) \sin (y)}{x y}$,"I need to find the extrema of the following function in the range $-2\pi$ and $2\pi$ for both $x$ and $y$, but I don't know how to go about doing it since it's a bit weird and not similar to other functions I've seen: $$f(x,y)=\frac{\sin (x) \sin (y)}{xy}$$ I've evaluated the gradient function as below: $\nabla f = <\frac{\sin (y) (x \cos (x)-\sin (x))}{x^2 y}, \frac{\sin (x) (y \cos (y)-\sin (y))}{y^2 x}>$ but setting it to zero gives a few answers for $x$ and $y$, none of which seem to be the right answer. The following is the sketch of the graph in the aforementioned range. According to WolframAlpha, it should have it's local extrema at $\{0, 4.49\}$, $\{0, -4.49\}$, $\{4.49, 0\}$, $\{-4.49, 0\}$.","['derivatives', 'optimization', 'calculus', 'graphing-functions']"
1552627,Riemann Surface of $w^{2}=\sqrt{1-z^{2}}$,"I'm working in the problem of finding branch points and build the Riemann Surface of the following complex function: $$
w(z)=\sqrt{1-z^{2}} \,\, .
$$ I'm reading lots of texts about how to do this, but I'm not able to do it. How to indentify the branch points and how to build the Riemann surfaces step-by-step of this function? Greetings!","['complex-geometry', 'complex-analysis', 'complex-numbers']"
1552649,$\sum_{n=1}^{50}\arctan\left(\frac{2n}{n^4-n^2+1}\right)$,"Find the value of 
$$\sum_{n=1}^{50}\arctan\left(\frac{2n}{n^4-n^2+1}\right)$$ $$\frac{2n}{n^4-n^2+1}=\frac{2n}{1-n^2(1-n^2)}$$
I am not able to split it into sum or difference of two $\arctan$s.Please help me.",['trigonometry']
1552654,Closed form for $\int_0^{\infty}\sin(x^n)\mathbb{d}x$,"I was wondering if anyone knows a closed form for $$\mathrm{I} = \int_0^{\infty}\sin(x^n)\mathbb{d}x$$ Preliminary evaluations on Wolfram Alpha seem to yield something like this: $$\mathrm{I} = k\sin\left(\frac{\pi}{2n}\right)\Gamma\left(\frac an \right)$$ where $k$ is a proportionality constant, normally $1$, and $a$ is a constant, normally $1$ as well.","['integration', 'closed-form']"
1552668,Are all quintic polynomials of this type not solvable by radicals?,The author of my textbook argues that the quintic polynomial $3x^5-15x+5$ is not solvable by radicals over $\mathbb{Q}$ by showing that the Galois group of $3x^5-15x+5$ over $\mathbb{Q}$ is isomorphic to $S_{5}$ (which is not solvable). But the argument given would seemingly apply to any quintic polynomial with integer coefficients that is irreducible over $\mathbb{Q}$ and has 3 distinct real roots and 2 non-real complex roots. Are all quintic polynomials of this type not solvable by radicals?,"['abstract-algebra', 'galois-theory', 'polynomials', 'field-theory']"
1552696,A continuous function defined by Lebesgue measure,"Let $A,B\in\mathcal A_{\Bbb R}^*$ given with $\overline{\lambda}(A)<\infty$ and $\overline{\lambda}(B)<\infty$. Lets define $\; \overline{\lambda}_{A,B}:\Bbb R\to\Bbb R$ as follows: $$\overline{\lambda}_{A,B}(x)=\overline{\lambda}(A\cap(B+x))$$ where $B+x=\{b+x:b\in B\}$. So what I want to prove is that $\overline{\lambda}_{A,B}$ is continuous. Proof: Let $c\in\Bbb R$ fixed and let $x_n=c-\frac{1}{n}\;\forall n\in\Bbb N$. Clearly $x_n\in\Bbb R\;\forall n\in\Bbb N$. Then, let $B_n=B+x_n\;\forall n\in\Bbb N\Rightarrow\ B_n=\{b+c-\frac{1}{n}:b\in B\}\;\forall n\in\Bbb N$ Lemma 1 : $$B_n\subseteq B_{n+1}\;\forall n\in\Bbb N$$ Let $y\in B_n\Rightarrow\ y=b+c-\frac{1}{n}=b+c-\big(\frac{1}{n(n+1)}+\frac{1}{n+1}\big)=b+c-\frac{1}{n(n+1)}-\frac{1}{n+1}\;\forall n\in\Bbb N$. Thus $ y=b+c'-\frac{1}{n+1}\;\forall n\in\Bbb N$ with $c'=c-\frac{1}{n(n+1)}\Rightarrow\ y\in B_{n+1}$. Lemma 2 : $$\lim_{n\to\infty}(A\cap B_n)=A\cap (B+c)$$ Since $B_n\subseteq B_{n+1}\;\forall n\in\Bbb N$ by Lemma 1, the limit of $(B_n)_{n\in\Bbb N}$ exists and $\lim_{n\to\infty}(B_n)=\bigcup_{n=1}^\infty B_n,\ $but $\bigcup_{n=1}^\infty B_n=\bigcup_{n=1}^\infty \{b+c-\frac{1}{n}:b\in B\}=\{b+c:b\in B\}=B+c$. Now, clearly $A\cap B_n\subseteq A\cap B_{n+1}\;\forall n\in\Bbb N\Rightarrow\ \lim_{n\to\infty}(A\cap B_n)=\bigcup_{n=1}^\infty (A\cap B_n)=A\cap\bigcup_{n=1}^\infty B_n=A\cap (B+c)$. So by Lemma 2 we get that:
$$\lim_{n\to\infty}\overline{\lambda}_{A,B}(x_n)=\lim_{n\to\infty}\overline{\lambda}(A\cap(B+x_n))=\lim_{n\to\infty}\overline{\lambda}(A\cap B_n)=\overline{\lambda}\big(\lim_{n\to\infty}(A\cap B_n)\big)=\overline{\lambda}(A\cap (B+c))=\overline{\lambda}_{A,B}(c)$$ And clearly $\lim_{n\to\infty}x_n=c$, thus $\overline{\lambda}_{A,B}$ is continuous. Did I miss something?","['lebesgue-measure', 'measure-theory']"
1552737,$\tan\frac{\pi}{16}+\tan\frac{5\pi}{16}+\tan\frac{9\pi}{16}+\tan\frac{13\pi}{16}$ [duplicate],This question already has an answer here : Sum of tangent functions where arguments are in specific arithmetic series (1 answer) Closed 8 years ago . Find the value of the expression $\tan\frac{\pi}{16}+\tan\frac{5\pi}{16}+\tan\frac{9\pi}{16}+\tan\frac{13\pi}{16}$ I identified that $\frac{\pi}{16}+\frac{13\pi}{16}=\frac{5\pi}{16}+\frac{9\pi}{16}=\frac{14\pi}{16}$ $\tan(\frac{\pi}{16}+\frac{13\pi}{16})=\tan(\frac{5\pi}{16}+\frac{9\pi}{16})$ $\frac{\tan\frac{\pi}{16}+\tan\frac{13\pi}{16}}{1-\tan\frac{\pi}{16}\tan\frac{13\pi}{16}}=\frac{\tan\frac{5\pi}{16}+\tan\frac{9\pi}{16}}{1-\tan\frac{5\pi}{16}\tan\frac{9\pi}{16}}$ But i am stuck here.Please help me.Thanks.,['trigonometry']
1552760,How to prove $\sinh^{-1} (\tan x)=\log \tan (\frac{\pi}{4}+\frac{x}{2})$,"Like the question says How to prove $$\sinh^{-1} (\tan x)=\log \tan (\frac{\pi}{4}+\frac{x}{2})$$
I have tried using many identity but in vain For reference
$$\tanh ^{-1} x=\frac{1}{2} \log \frac{1+x}{1-x}$$
and $$\sinh^{-1} x=\log (x+\sqrt{x^2+1})$$","['hyperbolic-functions', 'trigonometry']"
1552761,What is the Benefit If a Stochastic Process is a Submartingale?,"Suppose if I have a stochastic process which is a submartingale. What is the ""practical"" benefit from this property? I have a roughly idea that this submartingale property suggests a favorable game scenario which is better than the fair game scenario (e.g., if a stochastic process is a martingale, then it can be used to model a fair game). But I'm pretty interested to know if there are any other practical benefits if a stochastic process is a submartingale process?","['martingales', 'probability-theory', 'soft-question']"
1552762,Does this proof make use of the Axiom of Choice?,"Theorem Let $X$ be uncountable. Let $A$ be countable. Then $|X\cup A| = |X|$ . Proof As $|X|>\aleph_0 \rightarrow \exists f: \Bbb N \to X$ injective. Note that $\operatorname {Im}(f)=\{f(0)=x_0,f(1)=x_1,...\}= X' \subsetneq X$ and $|X'|=\aleph_0$ . Now $X=(X\setminus X')\cup X'$ . (proof $|X'|=|X'\cup A|$ ). Then we define some bijection $G$ sending $X\setminus X' \to X\setminus X'$ and $X' \to X'\cup A_\square$ . My question is: Is choice being used when saying $\operatorname{Im}(f)=\{x_0,...\}$ ? My professor mentioned something about this, but I didn't really understand it as this was just a very preliminary set theory class for another subject and we never ended learning about choice.","['elementary-set-theory', 'axiom-of-choice']"
1552791,Meromorphic Function on Extended Plane,How do I prove that every meromorphic function on the extended plane is a rational function?,"['taylor-expansion', 'singularity-theory', 'laurent-series', 'complex-analysis', 'power-series']"
1552795,"If $G$ is a finite group of order $n$, then $n$ is the minimal such that $g^n=1$ for all $g \in G$?","I know that if the order of a finite group $G$ is $n$ then $g^n=1$ for all $g\in G$. But, is $n$ the smaller integer that satisfies that property? There isn't another $m<n$ such that $g^m=1$ for all $g\in G$?
Thanks","['finite-groups', 'abstract-algebra', 'group-theory']"
1552809,"Is the (anti)derivative of an even complex valued function odd, and vice versa?","I'm not sure how much content I can put for a relatively straightforward question, haha. But, I'm attempting to prove something about even and odd functions and integrals in Complex Analysis, and I was curious as to if it is true that the (anti)derivative of an even complex valued function is odd, and vice versa, as it is for reals (to my knowledge).","['derivatives', 'complex-analysis', 'integration']"
1552821,"Give an example of a function which is in $L^2 (\mathbb{R})$ but not in $L^p(\mathbb{R})$ for any $p \in [1, 2) \cup (2, \infty]$.","This question was on a problem set regarding $L^p$ spaces in an undergraduate-level real analysis course. I actually used an answer on StackExchange to help me provide an example, but I couldn't provide adequate justification for why it works. The answer I used is linked here . I modified this function a little to make it so that it only converges if $p = 2$, namely I defined my function as follows: $$f(x)=\frac{1}{x^{2/p} \ln^2(x^{2/p})}$$ But I can't really explain why it works (and to be honest, I'm not really sure it does work). My professor hinted in class that there is a solution that involves a continuous piecewise function agreeing at $x=1$ that is basically a generalization of the observation that for the following functions: $$g(x)=\begin{cases} \frac{1}{\sqrt{x}} & 0<x<1 \\ 0 & \text{otherwise}\end{cases}$$ $$h(x)=\begin{cases} \frac{1}{x} & x>1 \\ 0 & \text{otherwise}\end{cases}$$
we can easily show that $g \in L^1(\mathbb{R}), \, g \not\in L^2(\mathbb{R})$ and $h \in L^2(\mathbb{R}), \, h \not\in L^1(\mathbb{R})$. So I would imagine that the solution based on this hint would involve some sort of clever manipulation of powers to incorporate all $p \in [1, 2) \cup (2, \infty]$ somehow, but I have no idea how to do that. I'm pretty lost with this problem. What is the best way to go about it?","['functional-analysis', 'real-analysis', 'lp-spaces']"
1552867,A metrisable compact space that is non isometric to any compact subset of $\mathbb{R}^{n}$,"I'm looking for the metrisable compact $M$ so that there is no isometry $f:M \rightarrow \mathbb{R}^{n}$ with $im(f) = K$ $-$ compact subspace. First, isometry preserves completeness, separability, and boundedness. Then, let's see how do the compact subspaces of $\mathbb{R}^{n}$ look like. First, they are complete (this works not only in the case of $\mathbb{R}^{n}$ but is the cases of metrisable space), then they are bounded (Heine-Borel states that compact set is bounded and closed). Moreover, any compact space in metric space is separable. So, it seems reasonable to find a metric space that is compact but not bounded. But, since a metric space is compact iff it's closed and totally bounded (which implies boundedness) then it's impossible. So, the basic approach did not help much. How to construct such space or to prove that it doesn't exist? Any help would be much appreciated.","['functional-analysis', 'real-analysis', 'metric-spaces']"
1552897,Solving recursions with max,"The question is general, but I'll first give a simple example. Suppose you have a candy machine with $N$ candies. The machine is weird, when you give it a quarter it gives you $1$ to $N$ candies (all numbers equally probable), this is called one ""buy"". You may assume you have more than $N$ quarters, so you can exhaust the machine completely. Moreover, when you see how many candies it gave you, you can decide whether you'll keep them all to yourself or give them all to a friend. You'll keep buying candies until the machine is empty, and you wish to maximize the expected number of candies you have at the end. The restriction here is that if you decide to keep a buy to yourself, you must give the next buy to your friend. So the obvious recurrence is as follows: $E[n] = \frac{1}{n}\sum_{k=0}^{n-1}\max\{n-k+Z[k], E[k]\}$ $Z[n] = \frac{1}{n}\sum_{k=0}^{n-1}E[k]$ $E[n]$ represents the maximum expected number of candies if played optimally. $Z[n]$ represents the same thing if you have to skip a buy. My question is, are there any established methods for getting a more/less closed form for such recurrences? I could guess that there might be a constant $\alpha$ such that if I get below $n\alpha$ candies I would give it to the friend (haven't checked this on a computer yet), but even then the math gets hairy.","['combinatorics', 'recurrence-relations', 'probability', 'discrete-mathematics']"
1552901,What is the motivation behind the study of sequences?,"I was discussing some ideas with my professor and he always says that before you work on something in mathematics, you need to know the motivation for studying/working on it.
A better way to put this would be, Why were sequences studied and sought after?","['sequences-and-series', 'analysis', 'motivation']"
1552916,"Solve the equation,$\sqrt{\log(-x)}=\log{\sqrt{x^2}}$","Solve the equation,$\sqrt{\log(-x)}=\log{\sqrt{x^2}}$(base of log is 10) $\sqrt{\log(-x)}=\log{\sqrt{x^2}}$ $\sqrt{\log(-x)}=\log{|x|}$ Now two cases arise,when $x>0$ and when $x<0$ When $x<0$, $\sqrt{\log(-x)}=\log(-x)$ I found $x=-1,-10$ When $x>0$ $\sqrt{\log(-x)}=\log x$ $\log(-x)=\log x\times\log x$ I could not solve it further.Please help me.","['algebra-precalculus', 'logarithms']"
1552926,Proving inequality with constraint $abc=1$,"For positive reals $a,b,c$, with $abc=1$, prove that $$\frac{a^3+1}{b^2+1}+ \frac{b^3+1}{c^2+1}+\frac{c^3+1}{a^2+1} \geq 3$$ I tried the substitution $x/y,y/z,z/x$, but it didn't give me anything. What else to do? Thanks.","['algebra-precalculus', 'inequality']"
1552927,How to obtain Laplace transform of {f(t-a)U(t-b)},"$f(t)=g(t-10)U(t-15)-g(t-10)U(t-20)$ The above $f(t)$ contains terms of the form $g(t-a)U(t-b)$, where $a$ doesn't equal $b$. Describe the form that $L\{f(t-a)U(t-b)\}$ takes. [Hint: The formula for $L\{g(t)U(t-a)\}=e^{-as}L\{g(t+a)\}$","['laplace-transform', 'differential', 'ordinary-differential-equations', 'functions']"
1552990,Is it possible to characterize completeness of a normed vector space by convergence of Neumann series?,"If $X$ is a normed vector space and if for each bounded operator $T \in B(X)$ with $\| T\| < 1$, the operator ${\rm id} - T$ is boundedly invertible, does it follow that $X$ is complete? Context: It is well known that if $X$ is a Banach space and if $T \in B(X) = B(X,X)$ is a bounded linear operator on $X$ with $\| T \|  <1$, then the Neumann series $\sum_{n=0}^\infty T^n$ converges (in the operator norm) to $({\rm id} - T)^{-1}$. In particular, ${\rm id} - T$ is invertible. There are counterexample to this fact if we do not assume $X$ to be complete. For example, we can take $X = \ell_0 (\Bbb{N})$ (the finitely supported sequences) and $T = \frac{1}{2} S$, where $S$ is the right shift operator. In this case, it is easy to see that $\sum_{n=0}^\infty T^n$ does not converge to a well-defined operator from $X$ to $X$. After I came up with the above counterexample, I wondered if we can characterize completeness of the normed vector space $X$ by the above property, as in the question stated above. Thoughts on the problem: Equivalently, we could require that $\sum_{n=0}^\infty T^n$ converges to a well-defined operator from $X\to X$ as soon as $\|T\|<1$, since in the completion $\overline{X}$, we still know that $T$ extends to a contiuous linear operator $\overline{T} : \overline{X} \to \overline{X}$ with $\| \overline{T} \| = \| T\|<1$, so that $S := {\rm id_{\overline{X}}} - \overline{T}$ is invertible with $S^{-1} = \sum_{n=0}^\infty \overline{T}^n$ and the restriction of $S^{-1}$ to $X$ is the inverse of ${\rm id} - T$, so that $({\rm id}_X - T)^{-1} = \sum_{n=0}^\infty T^n$. I know that $X$ is complete iff $B(X)$ is, so that it would suffice to show that $B(X)$ is complete. To show that a normed vector space $Y$ is complete, it suffices to show that ""absolute convergence"" of a series implies convergence, or even more restrictive that if $\|x_n\|\leq 2^{-n}$ for all $n$, then the series $\sum_{n=1}^\infty x_n$ converges in $Y$. My problem with applying observation 3 to $Y = B(X)$ is that we only know that the statement for 3 is true for $x_n = T^n$ with suitable $T$, which seems to be too restrictive. In fact, I don't know how to construct any kind of nontrivial bounded operators on a general normed vector space $X$, apart from operators of the form $x \mapsto \varphi(x) \cdot x_0$ (and linear combinations of those), where $\varphi $ is a bounded functional on $Y$ and $x_0 \in Y$. But for operators as above (i.e. with finite dimensional range), convergence of the series $\sum_{n=0}^\infty T^n$ is always true, since in fact we only need to consider a finite dimensional subspace, which certainly is complete.","['functional-analysis', 'normed-spaces', 'banach-spaces', 'operator-theory']"
1552994,Question on indefinite integrals,I have to integrate: $$I_2 = \int \frac{e^{2x} - e^{x} + 1}{(e^x\cos(x) - \sin(x))\cdot \left(e^x\sin(x) + \cos(x)\right)} \text{d}x$$ I simply can't understand from where to begin with. Please help me in solving this problem.,"['indefinite-integrals', 'integration']"
1553005,"Integrate a function over a contour including infinitely many poles, such as $\int_{|z|=1}1/\sin(1/z)\,dz$","We can find complex integration of a function over a closed contour by residue theorem if there are only finite many singularity inside the contour. But my question is how to find  the integration if there are infinite many singularity inside the contour? Please help me solve this type of problem mention below.
$$\int_{|z|=1} \frac{1}{\sin(\frac{1}{z})} dz$$","['complex-analysis', 'complex-integration']"
1553036,How big are regular (hyperbolic) polygons?,"Given a hyperbolic surface of constant curvature $K=-1/a^2$ embedded in $\mathbb{R}^3$, is there a known formula for the length of the edges of a  regular polygon? I know that the Gauss–Bonnet theorem tells us that the area of an $n$-gon is given by:
$$A=(n-2)\pi-\sum_{i=1}^n \alpha_i = n(\pi-\alpha) - 2\pi$$
But i'm not sure how one would go about calculating the edge length without explicitly specifying the surface. Is an explicit formula even possible?","['hyperbolic-geometry', 'differential-geometry', 'surfaces', 'geodesic']"
1553046,What is the maximum value of $x^T Ax$?,"Let $$A = \begin{bmatrix}3 &1 \\  1&2\end{bmatrix}$$ What is the maximum value of $x^T Ax$ where the maximum is taken over all $x$ that are the unit eigenvectors of $A$ ? $5$ $\frac{(5 + \sqrt{5})}{2}$ $3$ $\frac{(5 - \sqrt{5})}{2}$ The eigenvalues of $A$ are $\frac{5 \pm \sqrt{5}}{2}$ . What is $x^T Ax$ ? Can you explain a little bit, please?","['matrices', 'eigenvalues-eigenvectors', 'quadratic-forms', 'linear-algebra']"
1553047,Express $\frac{\sin 7\theta}{\sin \theta}$ in powers of $\sin \theta$ only,"By using DeMoivre's theorm express
$$\frac{\sin 7\theta}{\sin \theta}$$
in the powers of Sine only answer given in the book is
$$7-56\sin ^2\theta+112\sin ^4 \theta-64\sin^6 \theta$$
can any one help to solve the question","['trigonometry', 'complex-numbers']"
1553064,Derivation of a function over $\frac{1}{\sinh(t)}\frac{d }{dt}$,"I can not calculate the next derivative, someone has an idea
$$\left( \frac{1}{\sinh(t)}\frac{d }{dt} \right)^n \left( e^{z t} \right)$$
Where $n\in \mathbb N$, $t>0$ and $z\in \mathbb C$. Thanks in advance","['derivatives', 'special-functions', 'real-analysis', 'calculus']"
1553106,"Example of quasi-compact, non-quasi seperated scheme where qcqs fails?","The qcqs lemma (in Ravi Vakil's notes) says that if $X$ is a quasi-compact (qc) and quasi-separated (qs) scheme, for any global section $f$, the natural map from $\Gamma(X, O_X)_f \to \Gamma(X_f, O_X)$ is an isomorphism.  (We are defining $X_f$ to be the complement of the set where $f(p) = 0$, i.e. where $f = 0$ in $O_{X,x} / m_x$. So the natural map is the one guaranteed by the universal property of localization as $f$ becomes a unit over $X_f$.) The proof of this lemma uses in a crucial way these finiteness assumptions in order to take advantage of the willingness of localization to commute with finite products. (One builds the exact sequence describing the global sections via the sheaf axioms, plugs a finite affine cover with a finite affine cover of the overlaps into it. Then localizing this sequence expresses $\Gamma(X, O_X)_s$ as the kernel of the exact sequence describing $\Gamma(X_s, O_X)$, bootstrapping off of the case of this theorem in the affine case.) I want counter examples in the situation when this hypothesis has been dropped. Here is what I have been thinking so far: I tried the example of the only qc but not qs scheme that I know: the infinite affine space with a doubled origin. The functions on this space and on any open subscheme are the same as functions on the corresponding open $\mathbb{A}^{\infty}$, since the gluing identifies the variables on a dense open set. So I don't think this will provide an example (but I am not sure - now after thinking about this I am more convinced that this example would provide a counter example, but I don't see how). Here one would (obviously) need to use that infinitely many affines are required to cover the intersection of the two $\mathbb{A}^n$ in order to produce something... I think I found an example of a quasi-seperated non-quasi compact scheme that provides a counter example. Essentially the point is that localization doesn't commute with infinite products in general, so one can take the infinite disjoint union of $Spec Z / (2^i)$, for all $i \geq 0$, and try to invert the element $2$. $X_2 = \emptyset$, but $(\Pi Z / 2^i)[2^{-1}] \not = 0$, since for instance $(1,1,1,1, \ldots)$ is not 2-torsion.","['examples-counterexamples', 'algebraic-geometry']"
1553138,if $F(x)=\ln{x}\ln{(1-x)}$ prove $ F'(x)>0$,"an anyone please help me with the following proof: Let $$F(x)=\ln{x}\ln{(1-x)},0<x\le\dfrac{1}{2}$$ show that
$$F'(x)>0$$ because 
$$F'(x)=\dfrac{(1-x)\ln{(1-x)}-x\ln{x}}{x(1-x)}$$
It suffices to show that
$$G(x)=(1-x)\ln{(1-x)}-x\ln{x}>0,0<x\le\dfrac{1}{2}$$","['derivatives', 'functions']"
1553151,absolute minimum of function,"Let $p$ and $q$ be positive numbers satisfying $\dfrac1p+\dfrac1q= 1$; and let $f ∶
[0,+∞) \to \mathbb{R}$ be the function
$f(x) =\dfrac1p x^p− x +\dfrac1q$ Show that $f$ has an absolute minimum at $x = 1$ and hence deduce the
inequality
$ab \le \dfrac1p a^p + \dfrac1q b^q$
for any positive $a$ and $b$.
Hint. What can you say on the magnitude of $p$ and $q$? I can find the absolute minimum at $x=1$ but I can't establish the inequality. Please help!!","['young-inequality', 'inequality', 'functions']"
1553173,limit of $n \left(e-\sum_{k=0}^{n-1} \frac{1}{k!}\right) = ?$,"As in the title:
$$
\lim_{n\to\infty} n \left(e-\sum_{k=0}^{n-1} \frac{1}{k!}\right) = ?
$$
Numerically, it seems 0, but how to prove/disprove it?
I tried to show that the speed of convergence of the sum to e is faster than $1/n$ but with no success.","['real-analysis', 'sequences-and-series', 'limits']"
1553209,"If $A$ is similar to $B$, then $A$ is invertible $\implies$ B is invertible?","I think that in order to achieve the invertibility, $\det(A)\ne 0$. But for similarity, we only have that $B=P^{-1}AP$ associates with. If we have $-1$ as power to both sides the equality would establish, but how do you prove the invertibility?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
1553270,Are all Lie groups Matrix Lie groups?,"I have beard a bit about so-called matrix Lie groups. From what I understand (and I don't understand it well) a matrix Lie group is a closed subgroup of $GL_n(\mathbb{C})$. There is also the notion of a Lie group. It is something about a smooth manifold of the manifold $M_n(\mathbb{C})$. I have also hear something saying that all Lie groups are in fact isomorphic to a matrix Lie group. Is this correct? Could someone give me a bit more detail about this? What, for example, is the isomorphism? Is it of abstract groups, manifolds, or ...?","['matrices', 'group-theory', 'manifolds', 'lie-groups']"
1553306,"Is there a sequence such that $a_n\to0$, $na_n\to\infty$ and $\left(na_{n+1}-\sum_{k=1}^n a_k\right)$ is convergent?","Is there a positive decreasing sequence $(a_n)_{n\ge 0}$ such that ${\it i.}$  $\lim\limits_{n\to\infty} a_n=0$. ${\it ii.}$  $\lim\limits_{n\to\infty} na_n=\infty$. ${\it iii.}$  there is a real $\ell$ such that $\lim\limits_{n\to\infty} \left(na_{n+1}-\sum_{k=1}^n a_k\right)=\ell.$ Of course without condition ${\it i.}$ constant sequences are non-increasing examples that satisfy the other two conditions, but the requirement that the sequence must be decreasing to zero makes it hard to find!.","['real-analysis', 'sequences-and-series']"
1553322,Law of Total (Conditional) Probability and Filtration,"The law of total probability establishes that $$
\mathbb{P}[A\mid C] = \sum_{n}\mathbb{P}[A\mid C\cap B_n]\,\mathbb{P}.[B_n\mid C] 
$$ Suppose that I have a filtration $\mathcal{F}_t$ and $A_t$ and $B_t$ are  $\mathcal{F}_t$-adapted stochastic processes. In particular $B_t$ can be either $1$ or $0$. Let $I\subset\mathbb{R}$ be an interval. I am wondering if I can apply the law of total probability and write $$
\begin{eqnarray}
\mathbb{P}[A_t\in I\mid\mathcal{F}_{t-1}]&=&\mathbb{P}[A_t\in I\mid \mathcal{F}_{t-1}\cap \{B_t=1\}]\,\mathbb{P}[B_n=1\mid\mathcal{F}_{t-1}]+\\
& & +\mathbb{P}[A_t\in I\mid \mathcal{F}_{t-1}\cap \{B_t=0\}]\,\mathbb{P}[B_n=0\mid\mathcal{F}_{t-1}] 
\end{eqnarray}.
$$ 
Is the writing above formally correct?","['stochastic-processes', 'probability-theory', 'probability', 'measure-theory']"
1553329,"Absolute max and min, inverse of function $f(x) = \sin x − x \cos x$","Let $f(x) = \sin x − x \cos x$, $0 \le x \le \pi$. Find the absolute maximum and
the absolute minimum of f. Hence, or otherwise, determine the range
of f. Finally, determine whether f has an inverse or not. You need not
find the formula of the inverse function if exists. I can find that the critical points are at $x=0$ and $x=\pi$ but when I do the sign test the abs max and min are also =0 What does this mean?",['functions']
1553332,Is this $X_T$ if the stopping time is $T \le \infty$?,"Is this $X_T$ if the stopping time is $T \le \infty$? Let $(\Omega, \mathscr{F}, \{\mathscr{F_n}\}_{n \in \mathbb{N}}, \mathbb{P})$ be a filtered probability space, and let $X = ({X_n})_{n \in \mathbb{N}}$ be a stochastic process adapted to $(\{\mathscr{F_n}\}_{n \in \mathbb{N}})$ If $T$ is an a.s. finite stopping time, we have: $$X_T = X_1 1_{T=1} + X_2 1_{T=2} + ... $$ right? What if $T \le \infty$ and $[\lim X_n]$ exists? Do we have $$X_T = [\lim X_n] 1_{T=\infty} + X_1 1_{T=1} + X_2 1_{T=2} + ... $$ What if $[\lim X_n]$ dn exist?","['stochastic-processes', 'probability-theory', 'martingales', 'stopping-times']"
1553337,If $f$ is analytic in $|z|<1$ then prove that $f(z^n)=f(0)+(g(z))^n$,"If $f(z)$ is analytic in $|z|<1$ and $f'(0)\not =0$ prove that there exists an analytic function $g(z)$ such that $f(z^n)=f(0)+(g(z))^n$ in the nbd. of origin. Since $f$ is analytic so Taylor's series expansion of $f$ about $z=0$ is $\displaystyle f(z)=\sum_{k=0}^{\infty}a_kz^k$. Also , $f(0)=a_0$. Then $\displaystyle f(z^n)=f(0)+\sum_{k=1}^{\infty}a_kz^{nk}=f(0)+z^nh(z^n)$ , where $h$ is analytic. But from here how I can prove the required result ?",['complex-analysis']
1553343,$\phi:\mathcal F\rightarrow \mathcal G$ is an isomorphism if there is an open cover on which $\phi(U)$ is an isomorphism.,Let $\mathcal F$ and $\mathcal G$ be sheaves of abelian groups over a topological space $X$ and let $\phi : \mathcal F\rightarrow \mathcal G$ be a morphism of sheaves. If $\{U_{\alpha}\}$ is an open cover of $X$ such that $\phi(U_{\alpha}):\mathcal F(U_{\alpha})\rightarrow\mathcal G(U_{\alpha})$ is an isomorphism of abelian groups for every $\alpha$ then is it true that $\phi$ is an isomorphism of sheaves? While trying to prove this  I had to assume that for an open set $U\subseteq X$ the map $\phi(U_{\alpha}\cap U)$ is an isomorphism. However I can't show this unless the restriction map $\mathcal G(U_{\alpha})\rightarrow\mathcal G(U_{\alpha}\cap U)$ is surjective. So is there another way to prove this or is it false? Can someone give me a counter example? Thank you.,"['sheaf-theory', 'algebraic-geometry']"
1553358,If $|f(z)|\le |f(z^2)|$ then prove that $f$ is constant,"Let $\Bbb D$ be the open unit disc and $f:\Bbb D\to\Bbb C$ be an analytic function such that $|f(z)|\le |f(z^2)|$, for all $z\in\Bbb D$. Prove that $f$ is constant. Here is my proof: For any $0<r<1$, consider the closed disc $B=\{z\in \Bbb D:|z|\le r\}$. Then the given condition implies that $|f(z)|\le |f(z^{2^n})|$, for all $n\in\Bbb N$ and for $z\in B$. Since for $z\in B$, $|z^{2^n}|\le r^{2^n}\to 0\implies z^{2^n}\to 0$. So by continuity, $|f(z)|\le |f(0)|$, for all $z\in B$. So by the Maximum modulus theorem, $f$ is constant on $B$. By the uniqueness theorem, $f$ is constant on $\Bbb D$. Is this correct? Let me know if there is any fallacy.",['complex-analysis']
1553366,To find the $n$th derivative of this function.,Let $f(x)$ be smooth and continuous for $|x|<1$. I am interested in the $n$th derivative of: $$g(x) = f(x) e^{af(x)}$$ for some $a>0$. Is it possible to write this in a neat form? Thanks.,['derivatives']
1553369,Derivative of Heaviside Function and Equivalence,"The derivative of the Heaviside function $\theta(x - a)$ is normally taken to be the delta function $\delta(x - a)$ . This question has two parts, the first is whether a constant coefficient is introduced when differentiating the Heaviside function. According to Wolframalpha (I know it is not the best reference site but unable to find information elsewhere), $$\frac{\partial}{\partial x}\theta(x/a - 1) = \frac{1}{a}\delta(x/a - 1),$$ whereas, $$\frac{\partial}{\partial x}\theta(x-a) = \delta(x - a),$$ although these two Heaviside functions are essentially the same. Wolframalpha: first equation second equation . The second part is whether is is correct to say that, $$\theta(x - a) = \theta(x/a - 1),$$ which the above seems to imply is not true but I would normally freely switch between the two. I guess a similar question would be if, $$\delta(x - a) = \delta(x/a - 1).$$ This could be a mistake by me in either formulating the problem or not really a proper mathematical question (I am a physicist and so skirt the boundaries of mathematical rigour). Many thanks","['derivatives', 'dirac-delta']"
1553497,What is coefficient of $x^k$ in $ n! (x/1! + x^2/2! + x^3/3! + ... )^n$?,"Given 
$n![\frac{x}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+...\frac{x^n}{n!} ]^n$, how do I find coefficient of $x^k$ in it ? How to find coefficient in case of above series having infinite terms i.e. $n! [\frac{x}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+... ]^n$ ?","['permutations', 'combinatorics', 'binomial-theorem']"
1553532,Prove that $y_1(x)=\sin(x^2)$ can't be a solution for a linear homogeneous second order differential equation.,"let $y_1(x)=\sin(x^2)$, $y_2(x)=\cos(x^2)$. Prove that the function $y_2$ can't be a solution for a linear homogeneous second order equation which fulfills the conditions of Existence and Uniqueness Theorem. Prove that the function $y_1$ can't be a solution for a linear homogeneous second order equation which fulfills the conditions of Existence and Uniqueness Theorem. My solution attempt: I thought about using the Wronskian property for the proof. but I didn't have and direction for how to do it. any kind of help would be appreciated.",['ordinary-differential-equations']
1553537,Why $\mathbb{R}$ is a subset of $\mathbb{C}$? [duplicate],"This question already has answers here : Is ""$a + 0i$"" in every way equal to just ""$a$""? (10 answers) Closed 8 years ago . Let us define $\mathbb{C}$ by
$$\mathbb{C} = \mathbb{R}^2.$$
Real elements of $\mathbb{C}$ are tuples $(x, 0)$. But $x \neq (x, 0)$, so $\mathbb{R}$ cannot be a subset of $\mathbb{C}$. I also read the following definition. Let
$$\mathbb{C} = \{ a + bi \mid a, b \in \mathbb{R},\ i^2 = -1 \}.$$
But there is no $i \in \mathbb{R}$ such that $i^2 = -1$, so where does $i$ come from? If $i \in \mathbb{C}$ then we are using $\mathbb{C}$ in definition of $\mathbb{C}$. NOTE: Apparently, there are similar questions on MSE but I think that lhf's answer is the best and most concise. Also, title ""R is a subset of C?"" is much easier to google than ""Is a+0i in every way equal to just a?"" I don't want readers to be redirected.",['elementary-set-theory']
1553616,Solve the differential equation for obtaining $x$ as a relation of $t$: $\frac{d^2x}{dt^2}=\alpha\sqrt{x}$,"Question: Solve the differential equation for obtaining $x$ as a relation of $t$:
$$\frac{d^2x}{dt^2}=\alpha\sqrt{x}$$ My attempt: $$\frac{d^2x}{dt^2}=\alpha\sqrt{x}$$
$$\Rightarrow 2\frac{dx}{dt}\frac{d^2x}{dt^2}=\alpha\sqrt{x}\cdot 2\frac{dx}{dt}$$
$$\Rightarrow \frac{d}{dt}\left[\left(\frac{dx}{dt}\right)^2\right]=\frac{4}{3}\alpha\cdot \frac{d}{dt}\left(x^\frac{3}{2}\right)$$
$$\Rightarrow \left(\frac{dx}{dt}\right)^2=\frac{4}{3}\alpha x^\frac{3}{2}+c_1$$ Now we have $$\frac{dx}{dt}=\sqrt{\frac{4}{3}\alpha x^\frac{3}{2}+c_1}=\sqrt{k x^\frac{3}{2}+c_1}$$
$$\Rightarrow \frac{dx}{\sqrt{k x^\frac{3}{2}+c_1}}=dt$$ Can anyone suggest how to proceed? Any substitutions?","['kinematics', 'integration', 'ordinary-differential-equations', 'calculus']"
1553656,"Is this alternative notion of continuity in metric spaces weaker than, or equivalent to the usual one?","I will try to be as clear as possible. For simplicity I will assume that the function $f$ for which we define continuity at some point is real function of a real variable $f: \mathbb R \to \mathbb R$, although the same line of reasoning should be the same even if we talk about continuity of a function at a point that is defined at some metric space and that has values in some metric space. To define continuity of a function $f$ at some point $x_0$ from its domain we have the following standard and famous definition, the $\varepsilon$-$\delta$ definition of continuity, which goes like this: Definition 1: $f$ is continuous at the point $x_0$ from its domain if for every $\varepsilon>0$ there exists $\delta>0$ such that when $|x-x_0|<\delta$ then $|f(x)-f(x_0)|<\varepsilon$. It is clear that we could write $\delta (\varepsilon)$ instead of $\delta$ because there really is dependence of $\delta$ on $\varepsilon$. Now, I was thinking about alternative definition of continuity that would go like this: Definition 2: $f$ is continuous at the point $x_0$ from its domain if there exist two sequences $\varepsilon_n$ and $\delta_n$ such that for every $n \in \mathbb N$ we have $\varepsilon_n>0$ and $\delta_n>0$ and $\lim_{n\to\infty}\varepsilon_n=\lim_{n\to\infty}\delta_n=0$ and when $|x-x_0|<\delta_n$ then $|f(x)-f(x_0)|<\varepsilon_n$. We could also write here $\delta_n(\varepsilon_n)$ instead of $\delta_n$ because obviously there is dependence of $\delta_n$ on $\varepsilon_n$. It is clear that second definition does not require that for every $\varepsilon$ there exist $\delta(\varepsilon)$ (which includes in itself an uncountable number of choices for $\varepsilon$) but instead requires that for every member of the sequence $\varepsilon_n$ there is some $\delta_n$ (and this includes in itself only countable number of choices because the set $\{\varepsilon_n : n \in \mathbb N\})$ is countable). It is clearly obvious that definition 1 implies definition 2, and the real question is is the converse true, in other words: If the function is continuous at some point according to definition 2, is it also continuous at the same point according to definition 1?","['continuity', 'real-analysis', 'analysis', 'definition']"
1553662,"Does this type of ""cyclic"" matrix have a name?","Let $\{a_1, a_2, ..., a_n\} \subset \mathbb{C}$ and consider the matrix of the form
$$
\begin{bmatrix}
  a_1 & a_2 & ... & a_{n-1} & a_n\\
  a_2 & a_3 & ... & a_n & a_1\\
  .\\
  .\\
  .\\
  a_n & a_1 & ... & a_{n-2} & a_{n-1}
\end{bmatrix}
$$ Does this type of matrix have a specific name?","['matrices', 'linear-algebra']"
1553673,How to solve this initial value problem?,"How do I solve this initial value problem? $y'=2x-2\sqrt{\max(y,0)}$ $y(0)=0$ I've never seen a problem of this type with the ""max"" and I don't know how to begin. It must have a unique solution $y:[0,\infty)\rightarrow\mathbb{R}$ I know the solution must be of the type $y(x)=cx^2$.","['ordinary-differential-equations', 'initial-value-problems']"
1553678,Lie bracket on a product manifold,"Suppose we have two manifolds $M,N$ with Lie brackets $\chi(M)\times\chi(M)\ni (X,Y) \longmapsto [X,Y]_M \in \chi(M)$ and 
$\chi(N)\times\chi(N)\ni (X,Y) \longmapsto [X,Y]_N \in \chi(N)$. My question is what is the relation between $[\cdot,\cdot]_M$, $[\cdot,\cdot]_N$ and Lie bracket $[\cdot,\cdot]_{M \times N}:=[\cdot,\cdot]$ on $M\times N$? Especially is it true that $[(X,0),(Y,0)]=([X,Y]_M,0)$ or $[(X,0),(0,Y)]=0$ or perhaps there is a formula for $[(X,Y),(Z,T)]$? I have tried to google but without any result, and trying to compute looks too complicated for me. Edit: Actually I would be gratefoul if someone gave at least an answere because I realised that for example in formula: $$[(X,0),(Y,0)]=([X,Y]_M,0)$$ one has a problem even with such an issue that $X,Y$ are not a vector fields on $M$. So are there any relation for those brackets? Answere: (for decendants) Generally there is no nice answere, yet if $X,Z$ are lifts of vector fields $X,Z$ on $M$ and the same for $Y,T$ than $$[(X,Y),(Z,T)]=([X,Z]_M,[Y,T]_N)$$","['lie-derivative', 'smooth-manifolds', 'differential-geometry']"
1553693,Algorithmic way to check if a power-conjugate presentation is consistent?,"Is there an algorithmic way to check if a power-conjugate presentation (for a finite polycyclic group) is consistent? Background: A finite solvable group $G$ has a subnormal series $$ G=G_0 \triangleright G_1\triangleright \dots G_n=1$$ with each $G_{k-1}/G_k$ cyclic of prime order $p_k$. A power-conjugate presentation is, informally, a description of $G$ in terms of this series. Specifically, for each $k$, one picks an element $a_k$ of $G_{k-1}$ whose image in the quotient $G_{k-1}/G_k$ is a generator; then $a_k^{p_k}\in G_k$ and as $G_k$ is normal in $G_{k-1}$, $a_k$ acts on $G_k$ by conjugation. One can construct $G_{k-1}$ from $G_k$ via these two pieces of data: the element of $G_k$ equal to $a_k^{p_k}$, and the conjugation action of $a_k$ on $G_k$. A power-conjugate presentation specifies exactly this data for each $a_k$ and thus has the form
$$\langle a_1,\dots,a_n\mid \{a_k^{p_k} = \mu_k\}_{1\leq k\leq n}, \{a_j^{a_k} = \mu_{kj}\}_{1\leq k<j\leq n}\rangle $$
where the $\mu_k$ and the $\mu_{kj}$ are words on $a_{k+1},\dots,a_n$ (specifying elements of $G_k$). Any group presentation having this form is said to be a power-conjugate presentation (whether or not it was arrived at as described above, beginning with some already-existing group $G$). It is said to be consistent if each element of the group presented has a unique representation in the form $a_1^{e_1}\dots a_n^{e_n}$ with $0\leq e_k<p_k$ for each $k$. Clearly if I begin with a group $G$ and a subnormal series $G=G_0\triangleright \dots \triangleright G_n=1$ as above, and build the power-conjugate presentation as described above, by picking a set of $a_k$'s whose images generate $G_{k-1}/G_k$ and then specifying the values $a_k^{p_k}\in G_k$ and the conjugation action of $a_k$ on $G_k$, the presentation I arrive at will be consistent, as each element $x$ of $G$ has a unique representation as $a_1^{e_1}\dots a_n^{e_n}$ with $0\leq e_k<p_k$ for each $k$. (First look at the image of $x$ in $G_0/G_1$ to determine $e_1$; then $a_1^{-e_1}x$ will be in $G_1$ so look at its image in $G_1/G_2$ to determine $e_2$; etc.) On the other hand, if I start trying to build a presentation outside of any reference to a preexisting group, just specifying the $\mu_k, \mu_{kj}$ as arbitrary words on $a_{k+1},\dots,a_n$, I may arrive at a presentation that is not consistent. My question is: is there an algorithmic way to recognize a consistent presentation? A followup question: if there is, are the computations reasonable to do by hand in small cases?","['finite-groups', 'computational-algebra', 'combinatorial-group-theory', 'group-presentation', 'group-theory']"
1553720,Showing that similar matrices have the same minimal polynomial,"I am in the process of proving the title. The hint says, for any polynomial $f$, we have $$f(P^{-1}AP) = P^{-1}f(A)P.$$ A is an $n \times n$ matrix over $F$ while $P$ is an invertible matrix such that the above matrix multiplication $P^{-1}AP$ makes sense. Why is this true? Thank you.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'minimal-polynomials']"
1553754,Free Product of Groups with Presentations,"There is a highly believable theorem: Let $A, B$ be disjoint sets of generators and let $F(A), F(B)$ be the corresponding free groups.  Let $R_1 \subset F(A)$, $R_2 \subset F(B)$ be sets of relations and consider the quotient groups $\langle A | R_1 \rangle$ and $\langle B | R_2 \rangle$. Then $\langle A \cup B | R_1 \cup R_2 \rangle \simeq \langle A | R_1 \rangle \ast \langle B | R_2 \rangle$, where $\ast$ is the free product. I can supply a horrible proof using normal closures and elements and such, but surely there is a reasonable diagram-chase proof using the universal properties of free groups and the free product.  However, I have been unable to construct it; I think there is probably some intermediary but basic categorical statement needed about presentations. There is a hint given for the problem (it is coming from Lee, Topological Manifolds, problem 9-4) that is: If $f_1 : G_1 \rightarrow H_1$ and $f_2 : G_2 \rightarrow H_2$ are homomorphisms and for $j = 1,2$ we have $i_j, i_j'$ the injections of $G_j, H_j$ into $G_1 \ast G_2$ and $H_1 \ast H_2$ respectively, there is a unique homomorphism $f_1 \ast f_2 : G_1 \ast G_2 \rightarrow H_1 \ast H_2$ making the square $(i_j, f_j, i_j', f_1 \ast f_2)$ commute for $j = 1,2$. This is almost trivial to prove with the UMP for free products, but I can't wrangle it into a proof of the top statement.  Also, anything legitimately trivial like $F(A) \ast F(B) \simeq F(A \cup B)$ or $F(F(S)) \simeq F(S)$ is fair game. I can get a whole lot of arrows, but I am having trouble conjuring up any arrows with $\langle A | R_1 \rangle \ast \langle B | R_2 \rangle$ as their domain.  We also know that the free product is the coproduct in $\textbf{Grp}$ but admittedly I haven't tried exploiting this yet. Can anyone help with a hint?  Thanks a lot!","['homotopy-theory', 'category-theory', 'combinatorics', 'general-topology', 'group-theory']"
1553802,"About the sum $ s_n:=\sum_{k=1}^{n} \text{lcm}(k,n) $","I'm currently studying the following sum:
$$
s_n:=\sum_{k=1}^{n} \text{lcm}(k,n)
$$
I manipulated it as follows:
$$\begin{align}
s_n&=\sum_{k=1}^{n} \text{lcm}(k,n)\\
&=\sum_{d|n}{\sum_{(k,n)=d,\space 1≤k≤n}\frac{kn}{d}}\\
&=n\sum_{d|n}{\sum_{\left(k,\frac{n}{d}\right)=1,\space 1≤k≤\frac{n}{d}}k}\\
&=n\sum_{d|n}\frac{n}{2d}\varphi\left(\frac{n}{d}\right)\\
&=\frac{n}{2}\sum_{d|n}d\varphi(d)
\end{align}
$$
I don't now if the can be broken down further. Is it possible to obtain a closed form expression? Or at least an asymptote? Any help is highly appreciated.","['number-theory', 'totient-function', 'elementary-number-theory']"
1553824,"Convergence in distribution, $X_n \xrightarrow{d} X$ and $|X_n-Y_n| \xrightarrow{P} 0$ implies $Y_n \xrightarrow{d} X$","I find this problem and I'd like to know if my answer is correct. Thank you Let $(X, \mathscr{A}, P)$ a probability space. Suppose that $X$ is a r.v. and $\{ X_n \}$ is a sequence of r.v.'s such that $X_n \xrightarrow{d} X$ (convergence in distribution) and $\{Y_n\}$ a sequence of r.v.'s such that $|X_n-Y_n| \xrightarrow{P} 0$. Then $Y_n \xrightarrow{d} X$ Proof: Let $f: \mathbb R \to \mathbb R$ be bounded and uniformly continous function. Let $K$ a constant such that $|f|\le K$ and let $\epsilon>0$ given, so there is a $\delta>0$ such that for $|x-y|<\delta$ then $|f(x)-f(y)|<\epsilon$. Thus \begin{align*} |E f(X_n)-& Ef(Y_n)|\le  E |f(X_n)- f(Y_n)|\\
&=\int_{\{|X_n- Y_n|<\delta\}} |f(X_n)- f(Y_n)| dP+E |f(X_n)- f(Y_n)|+\int_{\{|X_n- Y_n|\ge \delta\}} |f(X_n)- f(Y_n)| dP\\
&\le \epsilon P \{|X_n- Y_n|<\delta\} +2K P\{|X_n- Y_n|\ge\delta\}\\
&\le \epsilon + 2K P\{|X_n- Y_n|\ge\delta\} \end{align*} Letting $n\to \infty$ we have $|E f(X_n)- Ef(Y_n)|\le \epsilon$ and since $\epsilon$ was arbitrary thus $\{Ef(y_n)\}$ converges at the same value that $\{Ef(X_n)\}$, that is, $\{Ef(y_n)\}\to Ef(X)$ and since this holds for all uniformly continuous and bounded function thus $Y_n \xrightarrow{d} X$","['probability', 'probability-distributions']"
1553853,Recurrence relation for the determinant of a tridiagonal matrix,"Let $$f_n := \begin{vmatrix}
a_1 & b_1 \\
c_1 & a_2 & b_2 \\
& c_2 & \ddots & \ddots \\
& & \ddots & \ddots & b_{n-1} \\
& & & c_{n-1} & a_n
\end{vmatrix}$$ Apparently, the determinant of the tridiagional matrix above is given by the recurrence relation $$f_n = a_n f_{n-1} - c_{n-1} b_{n-1}f_{n-2}$$ with initial values $f_0 = 1$ and $f_{-1} = 0$ (according to Wikipedia). Can anyone please explain to me how they came to this recurrence relation?
I don’t really understand how to derive it.","['recurrence-relations', 'tridiagonal-matrices', 'matrices', 'determinant', 'linear-algebra']"
1553888,How to find $\sup \Pi_{i=0}^{n} (\sin(i)^2 - \frac{25}{16})$?,"Let $\sup,\inf,{\rm dif}$ denote resp supremum , infimum and $\rm dif$ = supremum - infimum. Does any of the 3 below have a closed form ? $\sup \Pi_{i=0}^{n} (\sin(i)^2 - \frac{25}{16})$ $\inf \Pi_{i=0}^{n} (\sin(i)^2 - \frac{25}{16})$ ${\rm dif} \Pi_{i=0}^{n} (\sin(i)^2 - \frac{25}{16})$","['products', 'limsup-and-liminf', 'trigonometry']"
1553908,Factorial in power series; intuitive/combinatorial interpretation?,"It is well known that the terms of the power series of exponential and trigonometric functions often involve the factorial function, essentially as a consequence of iterating the power rule. My question is whether there is another way to view the occurrence of factorials so often; factorials are often involved in counting/combinatorics, so is there a combinatorial interpretation of why this happens, or some other interesting interpretation? More generally, is there any way I can look at the power series of $e^x$ or some other function and intuitively understand why factorials are involved, rather than just thinking of the power rule and derivatives?","['intuition', 'combinatorics', 'power-series', 'calculus']"
