question_id,title,body,tags
4354582,Koszul formula applied to divergence,"On a Riemannian manifold $(M, g)$ of dimension $n$ , for a point $x$ on $M$ and a basis $\{U_i\}$ for $T_xM$ which is orthonormal with respect to $g$ , we have the formula $$\operatorname{div} V = \sum_{i=1}^n g(\nabla_{U_i} V, U_i)$$ for the divergence of vector field $V$ at $x$ , where $\nabla$ here is the Levi-Civita connection (see proof of Proposition 3.1.1 in Hsu's Stochastic Analysis on Manifolds ). The Koszul formula allows us to write \begin{align*} g(\nabla_{U_i} V, U_i) = \frac{1}{2}&\left[U_i(g(V, U_i))+V(g(U_i, U_i))-U_i(g(U_i, V))\right. \\ &\left.+g([U_i, V], U_i)-g([U_i, U_i], V)-g([V, U_i], U_i)\right] \end{align*} Given that $g(U_i, U_i)$ is constant and that $[U_i, U_i] = 0$ , this seems like it should simplify down to $$g(\nabla_{U_i} V, U_i) = g([U_i, V], U_i)$$ so we could equivalently write $$\operatorname{div} V = \sum_{i=1}^n g([U_i, V], U_i)$$ Firstly, are these in fact equivalent? If so, are there any reasons for preferring one form of this definition to another?","['manifolds', 'riemannian-geometry', 'differential-geometry']"
4354616,Is the union of the intersection the same as the intersection of the union?,"If $A_{nm}$ is a subset of $A$ for $n = 1,2, \dots$ and $m = 1,2, \dots$ is it necessarily true that $$\displaystyle\bigcup_{n=1}^\infty \Big[ \displaystyle\bigcap_{m=1}^\infty A_{nm} \Big] = \displaystyle\bigcap_{m=1}^\infty \Big[ \displaystyle\bigcup_{n=1}^\infty A_{nm} \Big] \, \, ? $$ I claim that this is the case. So I attempt to prove it by showing double inclusion which implies equality. Proof . Let $x \in \displaystyle\bigcup_{n=1}^\infty \Big[ \displaystyle\bigcap_{m=1}^\infty A_{nm} \Big]$ . By definition of union, there exists $\displaystyle\bigcap_{m=1}^\infty A_{n_0 m}$ so that $x \in \displaystyle\bigcap_{m=1}^\infty A_{n_0 m}$ . By definition of intersection, we have $$\forall \, m \in \mathbb{N}, x \in A_{n_0 m}$$ Now notice that  for any $m \in \mathbb{N}$ , we also have $x \in A_{n_0 m} \subset \displaystyle\bigcup_{n=1}^\infty A_{nm}$ . Now from here, I want to somehow get an intersection on the outside of that union to show inclusion the first direction, but I'm not sure how to do so. Maybe union and intersection aren't interchangeable. Maybe there's a counterexample to this claim. Any advice?",['elementary-set-theory']
4354619,Epsilon-delta proof of a limit of a quadratic function,"I found the following example: picture Is the reason for getting an upper bound $|x+2|\leq 3+2=5$ (and not $|x-2|\leq 5$ [I've used the triangle inqeuality here]) that we want a bound on $|x-2|$ to depend on $\epsilon$ but we don't care about a bound on $|x+2|$ ? So if the problem were asking to prove $\lim_ {x\to -2}=4$ , I would have to say $|x-2|\leq 5$ and so $|x+2|\leq \epsilon/5$ ? If we had chosen the max $\delta$ value to be $1/2$ instead of $1$ , would we have to set $\delta=\min\{1/2,\epsilon/4.5\}$ ? I'm also not sure if I fully understand the point of setting $\delta=\min\{1,\epsilon/5\}$ . Let's say $\epsilon=10$ . Then if I take delta to be $\epsilon/5$ , I'll have $|x-2||x+2|\leq |2||5|=10$ . Is the only reason why I should take $\delta=1$ in this case that the inequality $|x-2||x+2| < \epsilon$ must be strict?","['limits', 'calculus', 'epsilon-delta']"
4354708,How soon can we represent a number as the sum of two primes?,"Update 27-Apr-2022 : Posted in MO Goldbach's conjecture says that every even number can be represented as the sum of two primes. But how soon can we find such a representation. Taking $20 = 3 + 17 = 7 + 13$ if we start searching from the smallest prime we will encounter the pair $(3,17)$ first and say that $20$ satisfied Goldbach's conjecture. We do not have to go all the way to the pair $(7,13)$ . Definition 1 : The smallest Goldbach representation of an even number $n$ is defined as the pair $(p_n,n-p_n)$ where $p_n$ is the smallest prime such that $n-p_n$ is also a prime. I found that $p_n$ is actually much smaller than $n$ . Experimental data for $n \le 230751000000 $ suggests an asymptotic relation of the form $$
\frac{1}{n}\sum_{k \le n} p_k \sim \log n + c
$$ that for $c < 1.3054$ and $c$ is decreasing as $n$ increases. So given $n$ , we can ask how large can $p_n$ . For even $n \le 3.6 \times 10^{10}$ , the largest value of $p_n$ occurs as the following values of $n$ as shown below by maximal $(n, p_n)$ pairs. (n,p_n)
(4,2)
(6,3)
(30,7)
(98,19)
(220,23)
(308,31)
(556,47)
(992,73)
(2642,103)
(5372,139)
(7426,173)
(43532,211)
(54244,233)
(63274,293)
(113672,313)
(128168,331)
(194428,359)
(194470,383)
(413572,389)
(503222,523)
(1077422,601)
(3526958,727)
(3807404,751)
(10759922,829)
(24106882,929)
(27789878,997)
(37998938,1039)
(60119912,1093)
(113632822,1163)
(187852862,1321)
(335070838,1427)
(419911924,1583)
(721013438,1789)
(1847133842,1861)
(7473202036,1877)
(11001080372,1879)
(12703943222,2029)
(21248558888,2089)
(35884080836,2803) Question : Assuming Goldbach's conjecture or otherwise, is there any heuristic upper bound on $p_n$ in terms of $n$ ? Trivially Goldbach's conjecture says $p_n \le n/2$ but can we do better than this?","['number-theory', 'elementary-number-theory', 'asymptotics', 'analytic-number-theory', 'prime-numbers']"
4354717,Understanding the counterexamples to L'Hopital's Rule,"Wiki provides a counterexample limit that fails with L'Hopital's Rule, because it doesn't satisfy the $g'(x) \ne 0$ rule they provide earlier. Let $f(x)=x+\sin x \cos x$ and $g(x)=f(x)e^{\sin x}.$ Then there is no limit for $f(x)/g(x)$ as $x\to\infty$ However, $$
\begin{align}
   \frac{f'(x)}{g'(x)} &= \frac{2\cos^2 x}{(2 \cos^2 x) e^{\sin x} + (x+\sin x \cos x) e^{\sin x} \cos x} \\[4pt]
   &= \frac{2\cos x}{2 \cos x +x+\sin x \cos x} e^{-\sin x},
 \end{align}
$$ which tends to 0 as $x\to\infty$ . My question: is it possible to construct a counterexample where $x$ goes to a real number, rather than infinity? Or does approaching a real number protect you from failing in the $g'(x) = 0$ case.","['limits', 'calculus', 'derivatives']"
4354719,"Finding the order of an element in $GL(2,\mathbb F_p)$.","Let $\mathbb F_p=\mathbb{Z}/p\mathbb{Z}$ . I am looking for a general method of finding the order of an element in $GL(2,\mathbb F_p)$ . Suppose $p=7$ and I am given the element $\begin{pmatrix} 5 &1 \\ 1& 1\\\end{pmatrix}$ , and I am asked to find the order, then how should I proceed. One way to proceed is to calculate $A^k$ for each $k$ and see when it takes the value $I_2$ for the first time. But this becomes a tedious task when the order is large. I think there is some other way to figure out the order by some group representation technique, but I do not know that topic very well. Can someone help me?","['general-linear-group', 'group-theory', 'representation-theory', 'problem-solving']"
4354725,Three circles internaly tangent to an equilateral triangle,"The diagram shows an equilateral triangle of side length 1 with 3 identical circles.  Find the radius of the circle. The correct answer for the length of the right triangle in red should be $\sqrt{3}r$ .
In the image, OP is found to be $\frac{\sqrt{3}}{2}$ , so the length in red should be $\frac{\sqrt{3}}{2} - 3x$ . But why the length is not equal to the one I found? $x$ cm is the radius of the circle.
By the way, trigonometry is not allowed to be used.","['pythagorean-triples', 'solution-verification', 'geometry']"
4354854,Finding the derivative of $f(x)=x/||x||$ for $x \in \mathbb{R}^3$ [duplicate],"This question already has an answer here : Find the gradient of $f:\Bbb R^m\setminus\{0\}\to\Bbb R,\; x\mapsto |x|$ (1 answer) Closed 4 months ago . I'm trying to find the derivative of the function $f:\mathbb{R}^3 \to \mathbb{R}^3$ given by $f(x)=x/||x||$ for $x \in \mathbb{R}^3 \setminus \{0\}$ and $f(0)=0$ . I've actually already been given the following expression $$f'(x)(h)=\frac{h}{||x||} - \frac{\langle x,h \rangle x}{||x||^3}$$ but cannot find a nice way to prove that this is in fact the derivative of $f$ on $\mathbb{R}^3 \setminus \{0\}$ . What I've tried is to use the definition $$f(x+h)=f(x) + T(h) + ||h|| \varepsilon(h)$$ where $T(h)$ is a linear map that ends up being the derivative if it exists, and $\varepsilon(0)=0$ and is continuous at $0$ . Plugging the function and the given derivative in, it seems you have to show that for $x,h \in \mathbb{R}^3$ , $$\lim_{h \to 0} \frac{1}{||h||} \left(\frac{x+h}{||x+h||} - \frac{(x+h)}{||x||} + \frac{\langle x,h \rangle x}{||x||^3} \right)=0$$ and I can't see a nice way of showing this. I don't know if doing this is the intended way or if there is a much easier way. I've tried looking at the components of $f$ since $f$ is differentiable at $a$ if and only if its component functions are also differentiable at $a$ , but those expressions aren't any nicer than the one above. How do you show that the given function is the derivative of $f$ ?","['multivariable-calculus', 'real-analysis']"
4354898,"How can you measure how ""shuffled"" a deck of cards is?","A few days ago I asked for some methods of measuring how shuffled a deck of cards was. Predictably there were a lot of suggested methods, which got me thinking, which is the best one? I think it'd be a cool experiment to test a bunch of methods, and then compare them. My idea is to start with a sorted deck of cards, and then progressively shuffle it, and after each shuffle calculate how ""shuffled"" the deck is using a number of methods. I plan to test many different types of shuffles as well, but that's getting into unnecessary detail. So my question is, what ideas do you have for measuring how shuffled the deck is? I've got a lot of ideas from my previous post, and I'll be throwing in some more basic methods. Here's what I have so far: Calculating the entropy of the deck as described in this wonderful post. Counting the number of pairs in the deck. Counting the number of inversions in the deck after a shuffle. Counting the number of rising or falling sequences in the deck. Tracking the position of each card in the deck and analyzing the change in position after shuffling. Then calculate the entropy of this information. This may have near identical results to the first method, but that's part of the experiment! Recording the number of high and low cards in a series of fixed ranges, and determining the randomness of the shuffle using this data (I don't think I need to go into full detail here but if requested I can give it). Those are the main methods I have at the moment, but I'm looking for as many as possible. I will be using a program to run all of these, so I can check a nearly unlimited number of methods, of any complexity. So go for it! Give me whatever methods you'd like to see. They can be as easy and flawed as you like (like the 2nd example I gave), or as complicated as you want. The whole idea is to test as many as I can. Also I'm very interested in using entropy, but it's pretty new to me, so any links to methods involving entropy would be fantastic. Once I run the experiment and get the results (in maybe a week or so) I'll update this post with the results I found (lots of pretty graphs to be expected!). That's the main gist of it, but for some of the more astute of you I'll give a quick explanation for a key issue I found when researching this. A big idea I kept running across is that you don't want to measure the randomness of the shuffled deck, but rather the randomness of the shuffling method itself, checking for a uniform distribution of final deck permutations. However, my objective is to look at a shuffled deck of cards that was originally sorted, and using some method, calculate a quantitative value that represents how shuffled the deck of cards is. The shuffling method itself will be a mystery to the program calculating the randomness (in reality I'll be using a lot of methods from a riffle shuffle to cutting the deck to the Fisher-Yates method). I'm going to look into a lot of statistics on each method like finding the standard deviation or consistency of each one. In the end I'll have a lot of conclusions, but the one I most want to answer is: if handed a shuffled deck that was once sorted, what would be the best method for calculating how well shuffled the deck is? I probably have misunderstood something here so feel free to comment any advice or changes you'd make. EDIT:
I completely forgot to post the results of this test from almost a year ago. The final paper had some very interesting results, and I'd like to show them here. Here is a PDF of the full paper: https://drive.google.com/file/d/1EoJhtHAO5iFjikkH35KDVrmmJQpXVb5q/view?usp=sharing","['statistics', 'entropy', 'random', 'card-games', 'probability']"
4354901,"Show that $S=\mathbb{Q} \cap [0,1]$.","Can you give a hint to show the following exercise?
Let $S \subset [0,1]$ such that $0, 1\in S$ and for all $n\in \mathbb{N}$ , $s_1,...,s_n \in S$ distinct, then $\dfrac{s_1+...+s_n}{n}\in S$ . Show that $S=\mathbb{Q} \cap [0,1]$ . Thank you","['rational-numbers', 'real-analysis']"
4354932,When is $X/R(X)$ an integer where $R(X)$ is the reverse of an integer $X$?,"My question concerns reverse numbers (e.g. $1234 → 4321$ ).
Is it possible to find integer solutions greater than $1$ for such numbers when you take their ratio? I am not interested in trivial solutions such as powers of ten and their multiples ( $0, 100, 20, 1100$ etc.).
Let's say you have a number $X$ and $R(X)$ so that $X/R(X) = n$ .
I've done some testing and have not found any solutions for $n = 2,3$ up to $X = 10^7$ . I'm fairly certain that I have proved that such a number cannot exist if it has an odd number of digits. Any ideas or solutions? (This is just an idea that occurred to me, nothing I really have to solve) Thanks!","['elementary-number-theory', 'recreational-mathematics', 'decimal-expansion', 'sequences-and-series']"
4355006,Absolute continuity implies the limit goes to zero,"If $f=f(x)$ is absolutely continuous on $[0,1]$ such that $f' \in L^2([0,1])$ and $f(0)=0$ . Prove then that $$\lim_{x \rightarrow 0^+}\frac{f(x)}{x^{\frac{1}{2}}}=0$$ The hint says to use Fundamental theorem of calculus but I don't see where to use that. I know if $f' \in L^2([0,1])$ then $$\bigg(\int_{[0,1]} \vert f'(x) \vert^{2} dx\bigg)^{\frac{1}{2}} < + \infty.$$ I don't know what to do from here though. Any hints would be appreciated. Am I merely using L'Hôpital's rule, since I get $0/0$ ?","['absolute-continuity', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
4355007,Solution to this differential equation that does not diverge at $x=0$,"I have the differential equation $$\tan x \; \frac{\text{d} y}{\text{d} x} + y = {\rm e}^x \tan x$$ By using the integrating factor $\mu(x)=\cos{x}$ , I solved it as an equation in full differentials and got the solution (verified with WolframAlpha). $$y(x) = A\csc{x} + \frac{1}{2}{\rm e}^x(1-\cot{x})$$ However, the question I am solving asks for a solution that does not diverge at $x=0$ , which this solution clearly does because of the $\cot{x}$ . How can I get a solution that converges?","['calculus', 'ordinary-differential-equations']"
4355012,"$f(x)=3x-\sin(\frac{\pi x}{2})$, $\quad$ $g(x)=x^3+2x-\sin(\frac{\pi x}{2})$","$f:\mathbb R \rightarrow\mathbb R$ and $g:\mathbb R \rightarrow\mathbb R$ are two functions such that $f(x)=3x-\sin(\frac{\pi x}{2})$ , $\quad$ $g(x)=x^3+2x-\sin(\frac{\pi x}{2})$ then choose the correct statements (A) $\frac{d}{dx}(f^{-1}(g^{-1}(x)))$ at $x=-12$ is $\frac{2}{3(28+\pi)}$ (B) $\frac{d}{dx}(f^{-1}(g^{-1}(x)))$ at $x=-12$ is $\frac{2}{3(28-\pi)}$ (C) Area bounded by $y=f^{-1}(x)$ and $y=g^{-1}(x)$ is $1$ (D) Area bounded by $y=f^{-1}(x)$ and $y=g^{-1}(x)$ is $\frac{1}{2}$ My Method: $(f^{-1}(g^{-1}(x)))=(g(f(x)))^{-1}$ $\implies$ $\frac{d}{dx}(f^{-1}(g^{-1}(x)))=\frac{d}{dx}(g(f(x)))^{-1}$ $\implies$ $\frac{d}{dx}(f^{-1}(g^{-1}(x)))=\frac{-g'(f(x))f'(x)}{(g(f(x))^{2}}$ But now I'm stuck. For Option (C) Because intersection points are $x=-1,x=0,x=1$ So area will be $\int_{-1}^{1}|g(x)-f(x)|dx$ Solution to Option (A) given in my Solution Image is as follow: $\frac{d}{dx}(f^{-1}(g^{-1}(x)))=\frac{1}{\frac{d}{dx}(g(f(x)))_{x=-1}}$ It seem to me that they must have used $f(g(x))=x$ if $f(x)$ and $g(x)$ are Inverse of each other. But I am not sure how did they use in above problem Please Help me in this question.","['integration', 'area', 'inverse-function', 'functions', 'derivatives']"
4355096,What progress has been made on the Collatz conjecture since Crandall's 1978 paper?,"I have recently read the famous paper by Crandall (1978) on the $3x+1$ problem, and I wonder what progress has been made since then. The paper claims that: If a cycle exists, then the minimum number of $3x+1$ steps for the number to reach itself is $17985$ . Is there a better lower bound nowadays? It is not known whether a positive density of odd integers satisfy the conjecture. Is that still unknown?","['collatz-conjecture', 'number-theory', 'soft-question']"
4355185,"If $D_1 f$ exists and is bounded, and $y \mapsto f(x,y)$ is continuous, then $f$ is continuous","Suppose that $U$ is an open subset of $\mathbb{R}^2$ and $f:U \to \mathbb{R}$ is such that the map $y \mapsto f(x,y)$ is continuous. Moreover, assume that the partial derivative in the first coordinate, $D_1 f$ , exists and is bounded. I'm trying to show that $f$ must be continuous. What I have so far: The condition that $D_1f$ exists and is bounded is equivalent to saying that for all $(a_1,a_2) \in U$ , $$\lim_{t \to 0} \frac{1}{t} (f(a_1+t,a_2)-f(a_1,a_2))$$ exists and is bounded. The condition that $y \to f(x,y)$ is continuous is equivalent to $$\lim_{(x,y) \to (x_0,y_0)} f(x,y) = f(x_0,y_0)$$ but I can't really see a sensible way of linking these together straight away. (I know the $D_1f$ condition must be important because it's not necessarily true that $x \mapsto f(x,y)$ and $y \mapsto f(x,y)$ both being componentwise continuous implies $f$ continuous). How do you go about showing the result?","['multivariable-calculus', 'real-analysis']"
4355199,Classification of quadratic rational Bézier curves,"My teacher months ago gave me a few hints on a method that can classify quadratic rational Bézier curves as different conic sections (arcs of those). As I recall, it starts with such a curve given three points. Then via a reparametrization it makes the first and last coefficients the same (I remember it being a mobius transformation), in this way one can simplify them leaving only the middle one $\beta$ . After that, it uses some affine geometry, defines an affine coordinate system using the three points. In the end finds out the standard cartesian form of a conic and studies the coefficients as functions of the previous middle coefficient $\beta$ . Can someone help me reconstruct the whole method? Thanks PS: I built myself a similar method. I start by computing the parametric coordinates of a Bézier curve of second degree(not a rational one). Then I regroup the powers of the parameter and I apply an ""implicitization"", but this process is neither elegant or quick and not the one the teacher gave me.","['conic-sections', 'bezier-curve', 'linear-algebra', 'geometry']"
4355267,"Apply Ito formula to $f(B_t,S_t)=(S_t-B_t)^2$ for $t\geq0$","Let $S_t=\max_{0\leq s\leq t}B_s$ for $t\geq0$ and apply Ito formula to $f(B_t,S_t)=(S_t-B_t)^2$ for $t\geq0$ . Determine the continuous local martingale  part and the bounded variation part in the resulting semi-martingale. First, I apply Ito formula to $f(B_t,S_t)$ : $$
\begin{align}
df&=-2(S_t-B_t)dB_t+2(S_t-B_t)dS_t+(dB_t)^2+(dS_t)^2-2dB_tdS_t\\
&=-2(S_t-B_t)dB_t+2(S_t-B_t)dS_t+dt+(dS_t)^2-2dB_tdS_t
\end{align}
$$ I couldn't get how to simplify $(dS_t)^2$ and $dB_tdS_t$ , because I haven't introduced max/sup/inf $B_t$ related process. It will be great help if anyone explain that.","['stochastic-integrals', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4355285,Understanding the solution of a second order differential equation by combination of solutions,"I am currently reading the paper The dielectric lamellar diffraction grating , in which the following ODE is solved for $u$ : $$u^{''}+\zeta^{2}S(x-c)u=-\beta^{2}u,$$ where $\zeta^{2}=k_2^2-k_1^2$ , $\beta^2=k_1^2-\mu^2$ , $S(x)=0$ for $x < 0$ and $S(x)=1$ for $x\geq0$ . They do this the following way, which I don't understand at all: Let $\theta$ and $\psi$ be two linearly independent solutions which are continuous and continuously differentiable at $x = c$ , such that $\theta(0) = 1$ , $\psi(0) = 0$ , $\theta^{'}(0) = 0$ , $\psi^{'}(0) = 1$ . Then for $0\leq x\leq c$ , we have that $$\theta = \text{cos}(\beta x),\;\;\;\psi=\frac{1}{\beta}\text{sin}(\beta x)$$ and for $c<x$ we have that $$\theta=\text{cos}(\beta c)\text{cos}(\gamma(x-c))-\frac{\beta}{\gamma}\text{sin}(\beta c)\text{sin}(\gamma(x-c)),$$ $$\psi=\frac{1}{\beta}\text{sin}(\beta c)\text{cos}(\gamma(x-c))+\frac{1}{\gamma}\text{cos}(\beta c)\text{sin}(\gamma(x-c)),$$ where $\gamma^{2}=\beta^{2}+\zeta^{2}$ . Can somebody please explain to me what is going on here? Why these two solutions? Where do all the sin/cos come from? EDIT (based on answers) The ODE is a piecewise defined function, so let's look at it piece by piece. $x < c$ in this case: $x-c< 0$ , hence $S(x-c) = 0$ , and the ODE becomes: $$u^{''}=-\beta^{2}u,$$ for which the answer is: $$u(x) = c_1 \cos(\beta x) + c_2 \sin(\beta x) $$ $x \geq c$ in this case: $x-c \geq 0$ , hence $S(x-c) = 1$ , and the ODE becomes: $$u^{''}+\zeta^{2}u=-\beta^{2}u$$ which can be rewritten as: $$u^{''}=-(\beta^{2}+\zeta^{2})u$$ substituting $m^2 = \beta^{2}+\zeta^{2}$ we obtain again the same kind of equation as for the first piece ( $x < c$ ) of the function: $$u^{''}=-m^2u$$ for which the answer is: $$u(x) = c_1 \cos(m x) + c_2 \sin(m x) $$ resubstituting: $$u(x) = c_1 \cos(\sqrt{\beta^{2}+\zeta^{2}}x) + c_2 \sin(\sqrt{\beta^{2}+\zeta^{2}} x) $$ So, I still don't understand: The use of $\theta$ and $\psi$ ? Their solution compared to the one posted here? Thank you.",['ordinary-differential-equations']
4355290,Method of moments estimator for lognormal distribution,"Let $X_1,\cdots X_n$ be identically and independently distributed lognormally. I want to find the method of moments estimators for $\mu,\sigma^2$ . We know that $E[X] = e^{\mu+\frac{\sigma^2}{2}}$ , $E[X^2] = e^{2\mu + 2\sigma^2}$ . Then we have $\mu + \frac{\sigma^2}{2} = \log\left( \frac{\sum X_i}{n} \right)$ . Hence, $\mu = \log(\sum X_i) - \log(n) - \frac{\sigma^2}{2}$ . Further, $e^{2\mu + 2\sigma^2} = \frac{\sum X_i^2}{n}$ which gives $2\mu + 2\sigma^2 = \log \left( \frac{\sum X_i}{n} \right)$ , and after some algebraic manipulation we have $\mu = \log \left( \frac{\log(\sum X_i^2)}{2} \right) - \frac{\log(n)}{2} - \sigma^2$ . Equating these gives $\sigma^2_{MM} = \log(\sum{X_i^2}) - 2\log(\sum X_i) + \log(n)$ , and in the exact same way $\mu_{MM} = -\frac{\log(\sum X_i^2)}{2} + 2\log(\sum X_i) - \frac{3}{2}\log (n)$ . However the wikipedia page gives $\mu_{MM} = \log \left( \frac{E(X)^2}{\sqrt{\text{Var}(X) + E(X)^2}} \right), \sigma^2_{MM} = \log \left( \frac{\text{Var}(X)}{E(X)^2} + 1 \right)$ . Have I done it wrong and if so where is the mistake? Thanks:)","['statistics', 'solution-verification', 'parameter-estimation', 'probability']"
4355346,Question about universal derivation $\Omega_{A/k}$,"Let $k$ be a ring, let $A$ be a $k$ -algebra. The universal derivation $\Omega_{A/k}$ is the (unique) $k$ -module representing the functor of the $k$ -derivations of $A$ ; suppose that $\Omega_{A/k}=0$ . If $k$ is an algebraically closed field, we can deduce that, for every maximal ideal $m$ , holds $m/m^2=0$ . In what settings this is this true? I would say yes for $k$ a generic field, but I don't understand if the arguments we used in the course to get this result are valid when $k$ is just a ring. Thanks in advance for any clarify; also, any reference that could help is welcome","['modules', 'algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
4355355,Deducing closed form of series,"In a past exam question we prove that the following function is well-defined and holomorphic on $\mathbb{C}$ \ $\mathbb{Z}$ , and then we are asked to find the closed form. Let $$
f(z)=\sum_{n=-\infty}^{\infty}\frac{1}{(z+n)^2}.
$$ The mark scheme says: We have that $f$ is periodic as $f(z) = f(z+2) \forall z$ . See that $f(z)$ has double poles at every integer with residue $(-1)^k$ . $(*)$ Note that $f(z) = −g'(z)$ , where $g(z)$ has single poles with residue $(−1)^k$ at each integer.
Then by periodicity it follows that $g(z) = \frac{\pi}{sin(\pi z)}$ and we obtain $f$ by differentiating. I honestly have no idea why this argument is right. I can see why $f$ is periodic and its residues are as given but everything from $(*)$ is not resonating. Any help in understanding this would be great! Thanks",['complex-analysis']
4355380,"Is there an interesting example of a chaotic dynamical system on $\widehat{\mathbb{C}}^n$, where $\widehat{\mathbb{C}}$ is the Riemann sphere?","This is for a mathematical visualization project I'm interested in. I am looking for an example of a chaotic, continuous-time dynamical system on $\widehat{\mathbb{C}}^n$ . Here $n$ is arbitrary, but I would also like to hear about examples for specific $n$ . I am particularly interested in systems that are highly chaotic: i. e. they generally do not converge to a stable orbit or limit, and are quite ""irregular"". Topological mixing might be a desirable property. Ideally such a system will be specified by a differential equation and make some use of the complex structure.  I am not very well-versed in dynamical systems, so please excuse me if this comes off as obtuse. Can anyone please provide an example of such a system, or direct me to a reference where I can read about explicit examples of complex dynamical systems along these lines? Thank you.","['complex-analysis', 'dynamical-systems']"
4355423,Example of incomplete metric on $\mathbb{R}$?,"I was wondering if the following is a valid example of an incomplete metric on $\mathbb{R}$ . Let $d(x,y)$ be defined as follows: if $x,y\in\mathbb{Q}$ then $d(x,y) = |x-y|$ ; if $x,y\in\mathbb{R}\setminus\mathbb{Q}$ then $d(x,y) = |x-y|$ ; otherwise (if one is rational and the other is not), let $d(x,y) = |x-y|+1$ . We verify this is a metric. The only concern is triangle inequality. Let $x,y,z\in\mathbb{R}$ , if $x,z\in\mathbb{Q}$ or $x,z\in\mathbb{R}\setminus\mathbb{Q}$ , then we have $$d(x,z)\leq d(x,y) + d(y,z)$$ regardless of if $y\in\mathbb{Q}$ or $y\in\mathbb{R}\setminus \mathbb{Q}$ since either case can only increase the RHS. On the other hand, if $x\in\mathbb{Q}$ and $z\in\mathbb{R}\setminus \mathbb{Q}$ , then the LHS gets 1 added to it but the RHS does as well, so the triangle inequality holds. For incompleteness, consider a sequence of rationals converging to $\pi$ in the Euclidean norm. Since the metric restricted to rationals is simply the Euclidean metric, this is a Cauchy sequence. However, for each $x_n$ in the sequence is rational, it cannot converge to an irrational number as $d(x_n,y)\geq 1$ for all $y\in\mathbb{R}\setminus \mathbb{Q}$ . On the other hand, $x_n$ cannot converge to a rational as well: if it does, then it must converge to that rational (say $q$ ) in the Euclidean norm, which is a contradiction since it cannot converge to both $\pi$ and $q$ in the Euclidean norm.","['general-topology', 'metric-spaces', 'real-analysis']"
4355426,"$X \geq 0$, $E[X] =\infty$. What's a lower bound for speed at which sample mean $\overline X_n$ diverges toward $\infty$?","Question Let $X_1, X_2, \dots \sim F$ be an i.i.d. sequence of observations such that $X_1 \geq 0$ and $E X_1 = \infty$ . Set the sample mean $\overline X_n := \frac 1n \sum_{i=1}^n X_i$ . We know that $\overline X_n \overset{\text{a.s.}}{\longrightarrow} \infty$ . But I'm looking for a result on how fast this occurs. (Not necessarily almost surely.) For example, a sequence $w_n \to \infty$ such that $P(\overline X_n > w_n) \to 1$ for $n \to \infty$ . This could be some function of $F$ . So maybe we'd say a sequence $\big(w_n(F)\big)_{n \in \mathbb N}$ . Thoughts Since $P(\overline X_n \leq w_n) = P(\sum_i X_i \leq nw_n) \leq P(\max_i X_i \leq nw_n) = P(X_1 \leq nw_n)^n = F(nw_n)^n$ . So if $w_n$ is slow enough that $F(nw_n)^n \to 0$ as $n \to \infty$ , then $(w_n)$ works as long as $w_n \to \infty$ . The problem is that if $1-F(x) \approx 1/x$ for large $x$ , then there does not exist such a sequence $(w_n)$ , since $F(nw_n)^n$ would then only work if $w_n \to 0$ : Suppose $w_n \to \infty$ . Then $$
\begin{align}
F(nw_n)^n \approx \Big[ 1 - \frac{1}{nw_n} \Big]^n &= \bigg( \Big[ 1 - \frac{1}{nw_n} \Big]^{nw_n} \bigg)^{n/(nw_n)}\\
&= \exp \left( \frac{n}{nw_n}\log \bigg( \Big[ 1 - \frac{1}{nw_n} \Big]^{nw_n} \bigg) \right) \\
&= \exp \left( \underbrace{\frac{1}{w_n}}_{\to 0} \underbrace{ \log \bigg( \Big[ 1 - \frac{1}{nw_n} \Big]^{nw_n} \bigg)}_{ \to -1} \right) \to \exp(0) = 1.
\end{align}
$$ I'd like to find a $(w_n)$ that would work for any distribution such that $E X_1 = \infty$ .","['statistics', 'probability-limit-theorems', 'convergence-divergence', 'probability-theory']"
4355454,On bounding an integral from below.,"I am trying to read Exploring the toolkit of Jean Bourgain , a beautiful article by Terrence Tao, that can be found here . At page 5 Tao considers the integral $$ \int_{|\xi|\le \delta/ t_j}| \hat{1}_B(\xi)  |^2 \hat{\sigma}(t_j\xi) d \xi   \tag{1}  $$ where $0< \delta= \delta(\epsilon) \le 1/2$ is a small quantity depending on $0<\epsilon< 1/2$ to be chosen later, $B \subset [-1,1]^2$ , $0<t_j \le 1$ , $\hat{1}_B$ is the Fourier transform of the indicator function $1_B$ , i.e., $$  \hat{1}_B(\xi) = \int_{\mathbb{R}^d} 1_B(x) e^{2\pi i x \xi} dx , $$ and $\hat{\sigma}(\xi) = \int_{S^1} e^{-2\pi i \omega \xi} d \sigma( \omega) $ is the Fourier transform of the surface measure $d \sigma$ on the unit circle $S^1$ . Tao states that, for $\delta$ small enough the factor $\hat{\sigma}(t_j\xi)$ is close to $1$ and thus it is not difficult to show that the integral in $(1)$ is approximately $\gtrsim|B|$ , thus $$ \int_{|\xi|\le \delta/ t_j}| \hat{1}_B(\xi)  |^2 \hat{\sigma}(t_j\xi) d \xi  \gtrsim |B| \ge \epsilon^2  . $$ How does one rigorously show this?","['integration', 'improper-integrals', 'fourier-analysis', 'real-analysis']"
4355562,"Showing $\frac{3\sqrt{3}}{2\pi}\sum_{z\in\Lambda}\frac1{1-\left(\frac{z}{\sqrt3}-1\right)^3}=1$, with $\Lambda$ a lattice","Consider the sum $$\frac{3 \sqrt{3}}{2\pi}\sum_{z\in \Lambda} \frac{1}{1-\left(\dfrac{z}{\sqrt{3}}-1\right)^3} \overset{?}=1$$ with $\Lambda=3\mathbb Ze^{\pi i/6}+3\mathbb Ze^{-\pi i/6}$ , then it is numerically not too difficult to see that this sum is equal to $1.$ I am however looking for an analytic argument of this nice fact. I assume it must rely on some subtle symmetries? Please let me know if you have any questions.","['complex-analysis', 'sequences-and-series']"
4355598,Can someone please explain the joke in this picture?,"I came across the following picture: I tried to do some research online to figure out why this picture was funny - but I couldn't really figure it out. Am I missing something, can someone please tell me what the intended joke in this picture is supposed to be? Thanks!","['calculus', 'soft-question', 'derivatives']"
4355626,If $M=M^{\perp\perp}$ for every closed subspace $M$ of a pre-Hilbert space then $H$ is complete,It is well-known that if $H$ is a Hilbert space then for every closed subspace $M$ we have $M=M^{\perp\perp}$ see here I would like to prove the converse by showing that: if $M=M^{\perp\perp}$ for every closed subspace $M$ of a pre-Hilbert space  then $H$ is complete,"['hilbert-spaces', 'operator-theory', 'orthogonality', 'functional-analysis']"
4355658,Tails of the normal distribution,"I'm currently reading Roman Vershynin's High-Dimensional Probability. For Proposition $2.1.2$ , I wonder how the lower bound is obtained. I understand that the lower bound is correct, but I don't know where the term $-3x^{-4}$ comes from. Thanks.",['probability']
4355704,Derivative of Mahalanobis pairwise distance matrix respect to transformation matrix,"For a set of vectorial observations $x_i$ stored in the matrix $X$ , I would like to obtain the gradient of the pairwise Mahalanobis distance matrix $D$ with respect to the Mahalanobis transformation matrix $M$ ( $\frac{\partial D}{\partial M}$ ), given that: $D_{ij} = (x_i - x_j)^TM(x_i - x_j)$ edit: M should be positive semi-definite, or alternatively $M = A^T A$ I have found how the euclidean pairwise distance matrix can be expressed as in a vectorized  form ( Pairwise distance matrix ), and how the Mahalanobis distance between two elements can be derived with respect to the matrix M ( Differentiating mahalanobis distance ), but I have not been able to put everything together.","['mahalanobis-distance', 'matrices', 'machine-learning', 'matrix-calculus', 'optimization']"
4355708,Differential equations: Shouldn't the method of separation of variables miss the solutions when x=0?,"This question is regarding ""Example 6"" from the book ""Elementary differential equations with boundary value problems"" by Edwards and Penney. Consider the ODE: $x \frac{d y}{d x}=2 y$ (1) We apply the separation of variables technique to obtain: $\frac{1}{y}dy=\frac{2}{x}dx$ (2) which provides the general solution: $y(x)=C x^{2}$ (3) The book mentions that ""the general solution (3) satisfies (1) for any value of the constant C and for all values of the variable x"". However, I believe the statement is wrong when $x=0$ . By using the separation of variables, we divided by $x$ and $y$ to obtain (2). Thus, we make the implicit assumption that $x \neq 0$ , and the separation of variables will generate solutions where $x \neq 0$ . How can $y$ be defined for any $x$ , when we implicitly assume that $x \neq 0$ ?",['ordinary-differential-equations']
4355953,Show that $\int_0^\frac{\pi}{2}\sqrt{\sin x} dx \times \int_0^\frac{\pi}{2}\frac{1}{\sqrt{\sin x}} dx =\pi $,"Show that $\int_0^\frac{\pi}{2}\sqrt{\sin x} dx \times \int_0^\frac{\pi}{2}\frac{1}{\sqrt{\sin x}} dx =\pi $ My teacher gave this question to solve but I was unable to solve it. I think there is surely any property of definite integral which I'm missing.
I'm trying not to use exponential integral or any other special function. I tried the following method: $$\int_0^\frac{\pi}{2}\sqrt{\sin x} dx \times \int_0^\frac{\pi}{2}\frac{1}{\sqrt{\sin x}} dx$$ $$\int_0^\frac{\pi}{2}\sqrt{\sin x} \times \frac{1}{\sqrt{\sin x}} dx$$ $$\int_0^\frac{\pi}{2}dx = \frac{\pi}{2} $$ This is of course  not true. What should I do with this? Kindly help me.","['integration', 'calculus']"
4355961,Meaning of $\frac{P(X\cap Y)}{P(X)P(Y)}$,"Imagine that we have a set $\Omega$ and $X$ and $Y$ are events that can happen, I mean, $P(X),P(Y)>0$ . Then, what does it mean the ratio $\frac{P(X\cap Y)}{P(X)P(Y)}$ ? I know that $\frac{P(X\cap Y)}{P(X)P(Y)}=\frac{P(X|Y)}{P(X)}=\frac{P(Y|X)}{P(Y)}$ and if that ratio is equal to 1 then $X,Y$ are independent events, but I can't figure out what exactly it means... please give simple examples. I found this when reading about lift-data mining.","['statistics', 'probability']"
4355999,Approximating a line integral over a rectifiable path by a line integral over a polygonal path in an infinite dimensional space,"Let $E$ be a complex Banach algebra and $U\subseteq E$ an open subset. Suppose we are given a continuous map $f:U\to E$ and a rectifiable path $\gamma:[a,b]\to U$ . For each tagged partition $\{a=t_0<t_1<\dots<t_n=b\}$ with tags $\tau_k\in [t_{k-1},t_k]$ , we consider the Riemann sum $$
\sum_{k=1}^n f(\gamma(\tau_k))\mkern2mu[\gamma(t_k)-\gamma(t_{k-1})].
$$ Analagously to the case $E=\mathbb{C}$ , we may define the line integral of $f$ along $\gamma$ by the limit of the net of Riemann sums as the mesh of the partitions tends to $0$ . (This limit exists, since $f$ is continuous and $\gamma$ is rectifiable.) I want to prove the following lemma from Conway's book on Complex Analysis (cf. page 65) in this more general setting. 1.19 Lemma. For every $\epsilon>0$ there exists a polygonal path $\Gamma$ in $U$ such that $\Gamma(a)=\gamma(a)$ , $\Gamma(b)=\gamma(b)$ and $$\Bigl\lvert\int_{\gamma} f-\int_{\Gamma}f\mkern2mu\Bigr\rvert<\epsilon.$$ By this old paper on Cauchy's theorem in Banach spaces, the above lemma is easily seen to be true (cf. page 77). On the other hand, an essential part of Conway's proof uses the fact that $\mathbb{C}$ is locally compact and hence $f$ uniformly continuous in a neighbourhood of $\gamma([a,b])$ , which is not the case anymore if $E$ is infinite dimensional. Do I miss something?","['integration', 'complex-analysis', 'banach-spaces', 'functional-analysis']"
4356009,Pendulum with Dirac Comb excitation,"There is a pendulum that is excited by a Dirac Comb. $l \ddot\theta+b\dot \theta+g\theta=G\,\sum_{-\infty}^\infty\delta(t-nT)$ where $l, b, g, G$ are constants and $T=\dfrac{2\pi}{\omega}$ . Show that the resulting motion is given by $\theta(t)=\dfrac{G}{Tl\omega^2}+\dfrac{2G\cos(\omega t-\frac{\pi}{2})}{Tb\omega}$ +[terms with frequencies $\ge$ 2 $\omega$ ] and explain why the higher frequency terms are supressed. My first take was to rearrange to $ \ddot\theta+\dfrac{b}{l}\dot \theta+\omega^2\theta=\frac{G}{l}\,\sum_{-\infty}^\infty\delta(t-nT)$ where $\omega=\sqrt{\dfrac{g}{l}}$ Then, taking the Laplace transform of both sides I got $\Theta(s)=\dfrac{G}{l\,\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}\, \dfrac{\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}{\left(s+\frac{b}{2l} \right)^2-\left(\left(\frac{b}{2l}\right)^2-\omega^2 \right)}\, \dfrac{1}{1-e^{-sT}}$ which, as far as I'm concerned transforms to $\theta(t)=\dfrac{G}{l\,\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}}\sum_{n=0}^\infty\,H(t-nT)\,e^{-\frac{b}{2l}(t-nT)}\sin\left(\sqrt{\omega^2-\left(\frac{b}{2l}\right)^2}(t-nT) \right)$ And, assuming that this is a correct form of the solution, I can't see how that is equivalent with the function given in the question. I reckon it has something to do with using Fourier series/transform instead? If so, I'm not sure how to do that. Or, is there a way to convert my solution into the given one? I've been struggling with this for a good few days now, so any help would be much appreciated.","['dirac-delta', 'ordinary-differential-equations', 'periodic-functions', 'laplace-transform', 'fourier-series']"
4356043,Number of k-APs in Z/NZ?,"Let $\mathbb{Z}/N\mathbb{Z}$ be the module class of an integer $N$ . Let $k$ be an integer and we may assume $N$ is much larger than $k$ to avoid some trivial case. If necessary, we can also assume $N$ is a prime. A $k$ -term arithmetic progression ( $k$ -AP) $K$ in $\mathbb{Z}/N\mathbb{Z}$ is a set of $k$ distinct numbers in $\mathbb{Z}/N\mathbb{Z}$ so that you can name them $a_1,a_2,...,a_k$ with $a_i\equiv a_1+(i-1)d$ (mod $N$ ) for some $d\in \mathbb{Z}/N\mathbb{Z}$ and all $i=1,…,k$ . Question: how many $k$ -APs in $\mathbb{Z}/N\mathbb{Z}$ ? To solve this question, my idea is to first consider how many $k$ -AP contains a fix number $x$ : there are $k$ ways to determine the place of $x$ in a $k$ -AP, and then there are $N-1$ ways to determine the ""common difference"" $d$ . Then we can determine a $k$ -AP containing $x$ . So by a double-counting argument, there are $k(N-1)N/k=N(N-1)$ many $k$ -APs in total. While I am not sure if $k(N-1)$ overcounts.","['arithmetic-progressions', 'combinatorics']"
4356079,Maximum-likelihood estimator resulting in complex estimator,"I'm trying to find the maximum likelihood estimator of a random sample $X=(X_1,\dots,X_n)$ from a distribution with pdf $$f(x;\lambda)=\frac{2}{\lambda x \sqrt{2 \pi}} \exp\left({\frac{[ -\log(x)]^2}{2 \lambda^2}}\right), \lambda >0, 0 \leq x \leq 1$$ Until now i have calculated the joint distribution function $$f_X(\underline{x};\lambda)=\left(\frac{2}{\lambda \sqrt{2 \pi}}\right)^n \prod^n_{i=1}\frac{1}{x_i} \exp \left({\frac{\sum_{i=1}^n[-\log(x_i)]^2}{2 \lambda^2}}\right)$$ The likelihood function $$L(\lambda,\underline{x}) \propto \left(\frac{2}{\lambda \sqrt{2 \pi}}\right)^n\exp \left({\frac{\sum_{i=1}^n[-\log(x_i)]^2}{2 \lambda^2}}\right)$$ The log-likelihood function $$l(\lambda,\underline{x})=(n \log(2) - n\log(\lambda)-\frac{1}{2}\log(2 \pi) + \frac{1}{2 \lambda^2} \sum_{i=1}^n[-\log(x_i)]^2$$ To find the maximum of the log-likelihood function i want to find the point $\hat{\lambda}$ where $l'(\lambda,\underline{x})=0$ and then prove that $l''(\lambda,\underline{x})|_{\lambda = \hat{\lambda}}<0$ .
I have calculated the following derivatives: $$l'(\lambda;\underline{x}) = -\frac{n}{\lambda} - \frac{1}{\lambda^3}\sum_{i=1}^n[-\log(x_i)]^2$$ $$l''(\lambda,\underline{x})=\frac{1}{\lambda^2}\left( n + \frac{1}{\lambda^2} \sum_{i=1}^n[-\log(x_i)]^2\right)$$ I find $$\hat{\lambda}= \pm \sqrt{-\frac{1}{n}\sum_{i=1}^n[-\log(x_i)]^2}$$ Which would result in a complex estimator.
Are my calculations off in any way?","['statistics', 'probability-distributions', 'maximum-likelihood']"
4356173,Are the solutions of a system of real polynomial equations continuous in the coefficients?,"Let $f_1(x,c_1),\ldots,f_n(x,c_n)$ be $n$ real polynomials in $n$ variables $x=(x_1,\ldots,x_n)$ of degree at most $d$ with coefficients $c=(c_1,\ldots,c_n)$ . Thus, for each $i=1,\ldots,n$ , we have $c_i\in\mathbb{R}^{{{n+d+1}\choose{d}}} $ . Let $$
\Gamma(c)=\{x\in\mathbb{R}^n\mid f_1(x,c_1)=0,\ldots,f_n(x,c_n)=0\}
$$ denote the set of real solutions of the given system of polynomial equations with coefficients $c=(c_1,\ldots,c_n)$ . Moreover, define $$
C=\left\{c\in\mathbb{R}^{n\times{{n+d+1}\choose{d}}} \ \middle|\ \Gamma(c)\neq \emptyset\right\}
$$ to be the set of coefficients for which there exists a real solution to the associated system of equations. Let us understand the solution set $\Gamma(c)$ as a set-valued function $\Gamma: C \to 2^{\mathbb{R}^n}$ over $C$ . A (single-valued) continuous function $\gamma:C \to\mathbb{R^n}$ is said to be a continuous selection of the set-valued function $\Gamma: C \to 2^{\mathbb{R}^n}$ if $\gamma(c)\in\Gamma(c)$ for all $c\in C$ . Question. Suppose $C$ is ``nice'', say open and connected (if more topological structure is needed, please feel free to assume so). Does $\Gamma$ admit a continuous selection? (A reference is much appreciated.) Thoughts: Of course, if there is the same number of real solutions over $C$ , then this follows from an implicit function theorem. However, if solution paths intersect or the polynomials are not in general position, then I am not sure how to formally proceed. Related questions are here and here , but they only concern a single polynomial and the question is not quite the same.","['systems-of-equations', 'algebraic-geometry', 'polynomials', 'analysis']"
4356275,"Given $\mathfrak{q}$ in $L/K$ unramified, can we find $\alpha$ such that $L=K(\alpha)$ and $f'(\alpha)$ not divisible by $\mathfrak{q}$?","Consider the following situation: Let $L/K$ be a finite extension of number fields and $\mathfrak{q}$ a prime of $L$ that is unramified. I am looking for a proof of the fact that one can always find some primitive element $\alpha$ of the field extension with the following property: If $f$ is the minimal polynomial of $\alpha$ , then $f'(\alpha)\mathcal{O}_L$ is not divisible by $\mathfrak{q}$ - in other words, reducing everything mod $\mathfrak{q}$ , $\overline{\alpha}$ is not a repeated root of $\overline{f}$ . I know that this is true ""most of the time"". Namely, if $\mathfrak{p}$ is the prime of $K$ below $\mathfrak{q}$ , in the case where we can find some $\alpha$ with $L=K(\alpha)$ such that the conductor $\mathfrak{f}$ of $\mathcal{O}_K[\alpha]$ in $\mathcal{O}_L$ is coprime to $\mathfrak{p}\mathcal{O}_L$ . However, this might not always be possible. If it is indeed true, I would like to have a ""global"" proof of this fact, i.e. without localization. (Background: I am trying to give a fairly simple proof that unramified primes $\mathfrak{q}$ do not divide the different $\mathcal{D}_{L/K}$ . If one can always find an $\alpha$ as above, the argument is simple: We know $f'(\alpha)\in\mathcal{D}_{L/K}$ and hence $\mathcal{D}_{L/K}|f'(\alpha)\mathcal{O}_L$ , which shows that $\mathcal{D}_{L/K}$ is not divisible by $\mathfrak{q}$ .)","['number-theory', 'abstract-algebra', 'algebraic-number-theory']"
4356285,"Understanding what $\displaystyle \lim\sup_{x \to a, x \in E} f(x)$ means","I recently learned of the following extension to the definition of a limit: Let $S \subset \mathbb{R}$ , let $f: S \rightarrow \mathbb{R}$ , let $a$ be a limit point of $S$ , and let $L \in \mathbb{R}$ . Then $$\lim_{x \to a \\ x \in S } f(x) = L$$ means the following: For each $\epsilon > 0$ there exists some $\delta > 0$ such that $$x \in S \text{ and } |x-a| < \delta \implies |f(x) - L| < \epsilon.$$ (I assume this definition extends similarly for general metric spaces.) I am now trying to understand the analogous extension for limit sup's and lim inf's. To quote Wikipedia : There is a notion of lim sup and lim inf for functions defined on a metric space whose relationship to limits of real-valued functions mirrors that of the relation between the lim sup, lim inf, and the limit of a real sequence. Take a metric space $X$ , a subspace $E$ contained in $X$ , and a function $f : E \rightarrow R$ . Define, for any limit point a $a$ of $E$ , \begin{align} \limsup_{x \to a} f(x) := \lim_{\epsilon \to 0} \left(\sup \big\{f(x):x \in E \cap B (a;\epsilon) \setminus \{a\} \big\}\right) \hspace{3cm} (1) \end{align} and $$ \liminf_{x \to a} f(x) := \lim_{\epsilon \to 0} \left(\inf \big\{f(x):x \in E \cap B (a;\epsilon) \setminus \{a\} \big\}\right) \hspace{3.3cm} (2) $$ where $B(a;\epsilon)$ denotes the metric ball of radius $\epsilon$ about $a$ . My first question is: In the context of the setting described above, do $\displaystyle \lim\sup_{x \to a \\ x \in E} f(x) = L$ and $\displaystyle \lim\sup_{x \to a} f(x) = L$ mean the same thing? (And similarly for $\lim\inf$ ?). My question comes from my reading of Dini numbers in Stein and Shakarchi's Real Analysis . They define $$D^{+}(F)(x) := \lim \sup_{h \to 0, \\ h > 0} \Delta_h(F)(x),  \hspace{3cm} (3) $$ where $\Delta_h(F)(x) := \frac{F(x + h) - F(x)}{h}$ (where $F:[a,b] \rightarrow \mathbb{R}$ ), and similarly for the other three Dini numbers. I just want to ensure that I fully understand what this means. Does $(3)$ translate to the following? \begin{align*}
    \lim \sup_{h \to 0, \\ h > 0} \Delta_h(F)(x) &:=  \lim_{\epsilon \to 0^{+}} \left(\sup \left\{\Delta_h(F)(x): x \in (0,\infty) \cap (-\epsilon, \epsilon) \setminus \{0\}  \right\}\right) \\[4pt]
&= \lim_{\epsilon \to 0^{+}} \left(\sup \left\{\Delta_h(F)(x): x \in (0,\epsilon) \right\}\right) \quad ?
\end{align*} (I take it that $h > 0$ means that $E = (0,\infty)$ in this case?)","['epsilon-delta', 'limsup-and-liminf', 'analysis', 'real-analysis', 'limits']"
4356296,Does a sequence of $C^1$ functions converge to a $C^1$ function if the derivatives are bounded and equicontinuous?,"My question is: If $f_n\to f$ pointwise in $(0,1)$ where $f_n\in C^1$ (continuously differentiable), AND the family $\{f_n'\}$ is uniformly bounded and equicontinuous, then is $f\in C^1$ as well? Ie, is $f$ also continuously differentiable? My intuition tells me that $|f_n'(x)|\leq M$ and equicontinuity should be enough to force $f'(x)$ to exist and be continuous but I'm not sure where to start a rigorous proof. Any help is welcome.","['equicontinuity', 'sequences-and-series', 'real-analysis']"
4356331,Would this Interpretation prove this argument to be deductively invalid (FOL) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I want to show: $$∀x(Ax → ∃y(By ∧ Rxy)) ∴ ∀y(By → ∃x(Ax ∧ Rxy))$$ Is deductively invalid.
Would the following interpretation show this? Domain: all natural numbers Let $Ax$ be $x=2$ Let $By$ be $y=0$ Let $Rxy$ be $x$ is less than $y$ Under this interpretation, would the premise be true and the conclusion false?","['first-order-logic', 'predicate-logic', 'logic', 'discrete-mathematics']"
4356342,Is differentiability the same as continuity of the derivative?,"I've seen an answer or two on this, but they don't fully make sense with me. One example is that $sin\frac{1}{x}$ is not differentiable at $x=0$ , but then isn't the derivative: $$-\frac{cos\frac{1}{x}}{x^2}$$ This is not continuous at $x=0$ as well. Can someone explain this, or give an example of when the differentiability of a function is not the same as the continuity of the derivative?","['continuity', 'calculus', 'derivatives']"
4356457,Conjugacy Classes in matrix groups $\mathrm{GL}_n(k)$ and rings $M_n(k)$,"Let $k$ be a finite field of size $q$ , let $n\ge1$ , and let $\mathrm{GL}_n(k)$ and $M_n(k)$ be the group of invertible matrices and ring of $n\times n$ matrices, respectively. Now, I have calculated (by grouping by the characteristic polynomial) that the number of conjugacy classes are: $q-1$ in $\mathrm{GL}_1(k)=k^\times$ , $q$ in $M_1(k)=k$ ; $q^2-1$ in $\mathrm{GL}_2(k)$ , $q^2+q$ in $M_2(k)$ ; and $q^3-q$ in $\mathrm{GL}_3(k)$ , $q^3+q^2+q$ in $M_3(k)$ . I conjecture that the pattern continues on the $M_n(k)$ -side, i.e., that there are always $q^n+\dots+q$ conjugacy classes in $M_n(k)$ . However, I cannot seem to prove this. How should I proceed? Also, is there an analogous formula for the number of conjugacy classes in $\mathrm{GL}_n(k)$ ? Note: By conjugacy class in $M_n(k)$ , I mean the equivalence classes under the relation, for $a,b\in M_n(k)$ , of $a\sim b$ iff there exists a $u\in\mathrm{GL}_n(k)$ such that $a=ubu^{-1}$ .","['finite-fields', 'group-theory', 'linear-algebra']"
4356540,Identity for $\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d}$,"I am looking for an identity for the following derivative: $$\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d},$$ where $\mathbf{a}$ , $\mathbf{b}$ , $\mathbf{c}$ , and $\mathbf{d}$ are column vectors. My approach to expand the expression is as follows: \begin{align*}
\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d}
&= [ ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} + \mathbf{c} \otimes\nabla(\mathbf{a} \cdot \mathbf{b}) ] \cdot \mathbf{d} \\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ \mathbf{c} \otimes [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \} \cdot \mathbf{d}\\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ \mathbf{c} \otimes [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \}^\top \mathbf{d}\\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \otimes \mathbf{c} \} \mathbf{d}\\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + (\mathbf{c} \cdot \mathbf{d}) [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}].
\end{align*} Here, $D \equiv \nabla^\top$ , i.e., $\nabla$ stands for the gradient operator and $D$ denotes the Jacobian. My question is, is this expansion correct? Moreover, in my understanding, the resulting expression is again a column vector. Is it right?","['partial-derivative', 'multivariable-calculus', 'derivatives']"
4356543,How do I prove this statement about trivial covering spaces?,"I have the following problem about covering spaces, but first of all I write our definitions: Definiton isomorphism of coverings An isomorphism between covering maps $p:Y\rightarrow X$ and $p':Y'\rightarrow X'$ is a homeomorphism $\phi:Y\rightarrow Y'$ such that $p'\circ \phi=p$ Definition of a trivial covering We say that a covering $p:Y\rightarrow X$ is trivial if $\forall x\in X$ we can take $X\in \mathfrak{U}(x)$ s.t. $p^{-1}(X)=\dot\bigcup_\alpha N_\alpha$ where $N_\alpha$ are open forall $\alpha$ $N_\alpha\cap N_\beta =\emptyset$ forall $\alpha \neq \beta$ $p|_{N_\alpha}:N_\alpha \rightarrow X$ us a homeomorphism Now I have the following statement: A covering $p:Y\rightarrow X$ is trivial iff it is isomorphic to a covering of the form $$p':X\times Z\rightarrow X;\,\,(x,z)\mapsto x$$ Where $Z$ is any set with the discrete topology. I wanted to prove this statement, but I somehow struggle a bit. My Idea was the following. Consider $$\phi:Y\rightarrow X\times Z;\,\,\,y\mapsto (p(y),z)$$ Then clearly $p'\circ\phi(y)=p'(p(y),z)=p(y)$ for all $y\in Y$ but now I remark that for $\phi$ to be homeomorphic it needs to be homeomorphic in each component. At the moment it is only true for the first component since in the second component it is not continuous. I think if I can find another second component I'm done right? Could someone help me finding this second component? Thanks","['general-topology', 'algebraic-topology', 'covering-spaces']"
4356625,Symplectic trivialization along path,"Let $(M,\omega)$ be a (symplectic) manifold. I want to compute the Maslov index of a loop $\gamma:\mathbb{R}\to M$ directly. In order to do that I have to find a (symplectic) trivialization of $\gamma^*TM$ but I can't see how to do this in general. Many references say to use a (symplectic) trivialization of $u^*TM$ for $u$ a map from the disk to $M$ agreeing with $\gamma$ on the boundary. Here below what I've done. The example I tried to work out is $M=S^2$ and $\gamma(t)=(\cos(t),\sin(t),0)$ . The $u$ one could consider is $$
u: D\to S^2\\
(x,y)\mapsto (x,y,\sqrt{1-x^2-y^2})
$$ In this case, as $M$ is two dimensional I would be tempted to use a coordinate chart $\chi:U\to \mathbb{R}^2$ (e.g. stereographic projection from south pole), which would give a map $$
D\times \mathbb{R}^2\to u^*TM\\
$$ but this works only for $\dim M =2$ so it's probably the wrong approach. It would be very helpful to have (a reference to) an example of such a concrete computation.","['vector-bundles', 'symplectic-geometry', 'differential-geometry']"
4356627,Expected number of rolls until all dice are removed,"There are $n$ fair dice. They are all tossed every time except the dice that are removed. A dice is removed if $3$ is rolled. What is the expected number of rolls? Any help would be appreciated. My attempt: Consider the case of two dice. The process ends on step one with probability $\frac{1}{36}$ , if one of the dice rolls up $6$ and other doesn't the on average $\frac{10}{36}(1+6)$ more steps are needed. Finally if $6$ doesn't roll up on both of the rolls then $\frac{25}{36}(z+1)$ rolls will be needed where $z$ is the expected number of rolls until both dice are removed. Thus $$z= \frac{1}{36}+\frac{10}{36}(1+6)+\frac{25}{36}(z+1) $$ Is this correct? If it is I can extend it recursively for $n$ dice.","['expected-value', 'combinatorics', 'probability']"
4356649,"Is this inequality valid, or did the authors make a mistake?","My question relates to Boucheron et al. (1999) . I paraphrase from their proof of Theorem 6 (p.15-16). Let $\phi(u) = e^u - u - 1$ , then for all $\lambda\in \mathbb{R}$ the inequality, \begin{align}
(1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) \leq v \phi(-\lambda), \qquad (1) 
\end{align} holds true. Now, considering equality in $(1)$ , we obtain a ordinary differential equation, \begin{align}
(1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) = v \phi(-\lambda), \qquad (2)
\end{align} which has as a solution $\Phi_0 = v\phi(\lambda)$ . We want to show that $\Psi \leq \Psi_0$ . In fact if $\Psi_1 = \Psi - \Psi_0$ , inequality (1) can be written as, \begin{align}
(1 - e^{-\lambda})\Psi_{1}^{\prime} - \Psi_1(\lambda) \leq 0 \qquad(3)
\end{align} That $\Psi_0(\lambda) = v\phi(\lambda)$ is a solution to the posed ODE, is, I believe, clear. By noting that, \begin{align}
\Psi_1(\lambda) &= \Psi(\lambda) - v(e^{\lambda} - \lambda - 1), \\
\Psi_{1}^{\prime}(\lambda) &= \Psi^{\prime}(\lambda) - v(e^{\lambda} - 1),
\end{align} inequality $(3)$ follows from $(1)$ , since \begin{align}
(1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) - v (e^{-\lambda} + \lambda - 1) &= (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) + v(e^{\lambda} - \lambda - 1) - v(e^{-\lambda} + e^{\lambda} - 2),  \\[1em]
&= (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - (\Psi(\lambda) - v\phi(\lambda)) - v(e^{\lambda} - 1)(1 - e^{-\lambda}),\\[1em]
&=  (1 - e^{-\lambda})(\Psi^{\prime}(\lambda) - v(e^{\lambda}-1) - (\Psi(\lambda) - \Psi_{0}(\lambda)), \\[1em]
&= (1 - e^{-\lambda})\Psi_{1}^{\prime}(\lambda)- \Psi_1(\lambda).
\end{align} Continuing with the paper, Hence defining $f(\lambda) = \ln(e^{\lambda} - 1)$ and $g(\lambda) = e^{-f(\lambda)}\Psi_1(\lambda)$ we have, \begin{align}
(1 - e^{-\lambda})[f^{\prime}(\lambda)g(\lambda) + g^{\prime}(\lambda)] - g(\lambda) \leq 0, \qquad (4)
\end{align} which yields since $f^{\prime}(\lambda)(1 - e^{-\lambda}) = 1$ \begin{align}
(1 - e^{-\lambda})g^{\prime}(\lambda) \leq 0. \qquad (5)
\end{align} Now if I work this out for myself, i.e., substitute the functions $f(\lambda)$ and $g(\lambda)$ , I obtain, \begin{align}
\Psi_1(\lambda) &= e^{f(\lambda)}g(\lambda), \\
\Psi_{1}^{\prime}(\lambda) &= e^{f(\lambda)}f^{\prime}(\lambda)g(\lambda) + g^{\prime}(\lambda)e^{f(\lambda)}.
\end{align} Plugging this into $(3)$ , I obtain, \begin{align}
(1-e^{-\lambda})e^{f(\lambda)}\Big(f^{\prime}g(\lambda) + g^{\prime}(\lambda)\Big) &- e^{f(\lambda)}g(\lambda) \leq 0 \\[1em]
(1 -e^{-\lambda})(e^{\lambda} - 1)g^{\prime}(\lambda) &\leq 0. \qquad (6)
\end{align} Now this extra factor is important, since the authors go on and argue that $g(\lambda)$ is non decreasing on $(-\infty,0)$ and non increasing on $(0,\infty)$ . This claim makes sense for the inequality $(5)$ , but not so much for $(6)$ . What am I missing? Is inequality $(5)$ valid? Or did the authors make a mistake? Edit Not surprisingly, there are multiple versions of this paper. In Boucheron et al. (1999) - v2 the authors caught their error. The corresponding proof is still a bit 'hand wavy', but I can follow it just fine. Addition is just for full disclosure.","['inequality', 'solution-verification', 'ordinary-differential-equations']"
4356655,"$2xy''+y'+xy=0,\ x>0$","$2xy''+y'+xy=0, x>0$ I have to find a power series solution and the  radius of convergence. My solution: I know that ${y=\sum_{n=0}^{\infty}a_{n}X^{\lambda+n}=a_{0}X^{\lambda}+a_{1}X^{\lambda+1}+a_{2}X^{\lambda+2}+\cdots}.$ ${y'=\sum_{n=0}^{\infty}(\lambda+n)a_{n}x^{\lambda+n-1}=\lambda a_{0}X^{\lambda-1}+(\lambda+1)a_{1}X^{\lambda}+(\lambda+2)a_{2}X^{\lambda+1}+\cdots}.$ ${y''=\sum_{n=0}^{\infty}(\lambda+n)(\lambda+n-1)a_{n}X^{\lambda+n-2}=\lambda(\lambda-1)a_{0}X^{\lambda-2}+\lambda(\lambda+1)a_{1}X^{\lambda-1}+(\lambda+2)(\lambda+1)a_{2}X^{\lambda}+\cdots}$ Then, $2x(\lambda(\lambda-1)a_{0}X^{\lambda-2}+\lambda(\lambda+1)a_{1}X^{\lambda-1}+\cdots+(\lambda+n+1)(\lambda+n)a_{n+1}X^{\lambda+n-1})+1(\lambda a_{0}X^{\lambda-1}+\cdots+(\lambda+n+1)a_{n+1}x^{\lambda+n})+x(a_{0}X^{\lambda}+\cdots+a_{n}X^{\lambda+n})=0.$ Find the indicial equation: $2\lambda(\lambda+1)+(\lambda+1)+0=0\impliesλ=-\frac{1}{2},\:λ=-1.$ Then the recurrence equation: $a_{n+1}=\frac{-a_{n-1}}{2λ^{2}+2n^{2}+3λ+4nλ+3n+1}\implies a_{n+2}=\frac{-a_{n}}{2n^{2}+2λ^{2}+7λ+7n+4nλ+6},$ with $λ=-\frac{1}{2} \implies a_{n+2}=\frac{-a_{n}}{2n^{2}+5n+3}.$ Then, $y_{1}(x)=x^{-\frac{1}{2}}(\sum_{n=0}^{\infty}\frac{-a_{n}}{2n^{2}+5n+3}),$ and $λ=-1 \implies a_{n+2}=\frac{-a_{n}}{2n^{2}+3n+1}$ Then, $$y_{2}(x)=x^{-1}(\sum_{n=0}^{\infty}\frac{-a_{n}}{2n^{2}+3n+1}).$$ I have to find the  radius of convergence; how can I do that?",['ordinary-differential-equations']
4356695,The largest number of intersection points we can obtain from $10$ distinct points on a circle.,"Suppose we pick $10$ distinct points on a circle and connect all pairs of points with line segments, what is the largest number of intersection points we can obtain? I have tried with $4$ points and got $1$ intersection point. Next, I have tried with $5$ points and got $5$ intersection points. Finally, I have tried with $6$ points and got $15$ intersection points. After that I am lost. Can someone give an argument how to find it?","['algebra-precalculus', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4356699,Line subbundles of maximal degree,"I want to know whether it is possible to bound the degree of line subbundles of certain holomorphic line bundles over Riemann surfaces. Even more concretely, consider the complex projective line $\mathbb{P}_{\mathbb{C}}^1$ and the rank-2 vector bundle $E=\mathcal{O}(a)\oplus \mathcal{O}(b)$ for $a,b\in\mathbb{Z}$ . Let $L\subset E$ be a holomorphic line subbundle. Thanks to the classification of line bundles over the Riemann sphere, this is some $\mathcal{O}(d)$ for $d\in\mathbb{Z}$ . Question : is necessarily $d$ bounded by $a,b$ or $a+b=deg(E)$ ? How can I prove that? Is there a maximal-degree subbundle of $E$ ?","['differential-geometry', 'complex-geometry', 'vector-bundles', 'algebraic-geometry', 'line-bundles']"
4356713,Why is $\frac{d}{dx}(x^3)$ not $3x^2 + 3x$?,In 3Blue1Brown's 'The Essence of Calculus' chapter 3 he shows a geometric analogy of why the derivative of $f(x)=x^3$ is $3x^2$ . I understand why we can ignore the tiny cube in the corner. But why do we also ignore the three lines along the edges of the cube? Each has a volume of $x$ as $dx$ approaches zero.,"['integration', 'calculus', 'derivatives']"
4356780,Find $\lim_{n\to\infty}\frac{1}{n}\left(n+\frac{n-1}{2}+\frac{n-2}{3}+\dots+\frac{2}{n-1}+\frac{1}{n}-\log(n!)\right)$,"I want to determine the limit of the following sequence $$x_n=\frac{1}{n}\left(n+\frac{n-1}{2}+\frac{n-2}{3}+\dots+\frac{2}{n-1}+\frac{1}{n}-\log(n!)\right)$$ From the foregoing, consider $$n+\frac{n-1}{2}+\frac{n-2}{3}+\dots+\frac{2}{n-1}+\frac{1}{n}=\sum_{k=1}^n\frac{n+1-k}{k}$$ Also try to consider Stirling's approximation, so you would have to find the limit of $$x_n=\frac{1}{n}\left(\sum_{k=1}^n\frac{n+1-k}{k}-\log(\sqrt{2πn}\left(\frac{n}{e}\right)^n\right)$$ I don't know if my previous statement is completely true, besides, from this expression it is difficult for me to find the requested limit. Any help please?","['limits', 'calculus', 'sequences-and-series']"
4356792,Numbers arranged in a circle are painted blue or red. Prove that the sum of the red numbers is 0.,"Numbers are arranged around a circle and are painted either blue or red. Every red number is equal to the sum of its two neighbors (left and right) and every blue number is equal to half the sum of its two neighbors (left and right). Prove that the sum of the red numbers is equal to $0$ . It's easy to prove that all numbers can't be red if they're not all $0$ and that if all numbers are blue they must all be equal, but I'm having difficulty going from here. I can't even find an example that fits our criteria. Any help is appreciated.",['combinatorics']
4356821,"Calculate power of the test $H_0$: $\sigma^2 \leq \sigma_0^2$ vs. $H_1$: $\sigma^2 > \sigma_0^2$ for $\mathcal{N}(0, \sigma^2)$ data.","Let $X_1, \dots, X_n \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma^2)$ . We consider the testing problem $H_0$ : $\sigma^2 \leq \sigma_0^2$ vs. $H_1$ : $\sigma^2 > \sigma_0^2$ and the statistical test $\delta:\mathbb{R}^n \rightarrow \{0, 1\}$ defined as follows: $$\delta(x) = \begin{cases}
1 &\mbox{if } \ \frac{1}{\sigma_0^2}\sum_{i=1}^n x_i^2 \geq \chi^{2-}_{n, \, 1 - \alpha}\\
0 &\mbox{else}
\end{cases}$$ where $\chi^{2-}_{n, \, 1 - \alpha}$ denotes the $1-\alpha$ quantile of the Chi-squared distribution with $n$ degrees of freedom. (I've heard that this is the uniformly most powerful test for our situation, is that correct?). We now want to calculate the power of this test. I.e., given $X = (X_1, \dots, X_n)$ , we evaluate for $\sigma^2 \in (0, \infty)$ : \begin{align}
\mathbb{E}_{\sigma^2}(\delta(X)) &= \mathbb{P}_{\sigma^2}(\delta(X) = 1)\\
&= \mathbb{P}_{\sigma^2} \left( \frac{1}{\sigma_0^2}\sum_{i=1}^n X_i^2 \geq \chi^{2-}_{n, \, 1 - \alpha} \right)\\
&= \mathbb{P}_{\sigma^2} \left( \frac{1}{\sigma^2}\sum_{i=1}^n X_i^2 \geq \frac{\sigma_0^2}{\sigma^2}\chi^{2-}_{n, \, 1 - \alpha} \right)\\
&= 1 - \mathbb{P}_{\sigma^2} \left( \frac{1}{\sigma^2}\sum_{i=1}^n X_i^2 \leq \frac{\sigma_0^2}{\sigma^2}\chi^{2-}_{n, \, 1 - \alpha} \right)
\end{align} Note that $\left(\frac{1}{\sigma^2}\sum_{i=1}^n X_i^2 \right) \sim \chi^{2}_{n}$ . Hence, when $\sigma^2 = \sigma_0^2$ we have \begin{align}\mathbb{E}_{\sigma^2}(\delta(X)) &= 1 - \mathbb{P}_{\sigma^2} \left( \frac{1}{\sigma^2}\sum_{i=1}^n X_i^2 \leq \chi^{2-}_{n, \, 1 - \alpha} \right)\\
&= 1 - (1 - \alpha)\\
&= \alpha
\end{align} My question: can we explicitly calculate $\mathbb{E}_{\sigma^2}(\delta(X))$ when $\sigma^2 \neq \sigma_0^2$ ?","['statistics', 'probability-theory']"
4356836,Union-Closed Families form a subcategory of a functor-structured category: can we describe it through categorical closure operators?,"Let us consider the functor-structured category $S(\mathcal{P})$ induced by the powerset functor. As it is well-known it is the category having the pairs $(X,\mathcal{F})$ (with $\mathcal{F}$ any set system on $X$ ) as objects and with structure-preserving maps as arrows. One full subcategory of $S(\mathcal{P})$ is given for example by the category of union-closed families. Observe now that the objects $(X,\mathcal{P}(X)$ , for any set $X$ , are always union-closed, and arbitrary intersections of union-closed families on the same ground set is again a union-closed family on such a ground set. Therefore, given $(X,\mathcal{F}) \in Obj(S(\mathcal{P}))$ , we can always find the smallest union-closed family on $X$ generated by $\mathcal{F}$ . In other terms, we're defining a closure operator on the lattice $\mathcal{P}(X)$ . Recent researches on categorical closure operators define a closure operator on a concrete category $(\mathcal{C},\mathcal{X})$ , with $\mathcal{X}$ finitely complete and with suitable $(E,M)$ -factorizations ( $E \subseteq Epi(\mathcal{C})$ and $M \subseteq Mono(\mathcal{C})$ ). Given an $\mathcal{X}$ -object $C$ , we will always mean a subobject of $U(C)$ (here $U: \mathcal{C} \longrightarrow \mathcal{X}$ stands for the forgetful functor). Recall that if $X \in Obj(\mathcal{X})$ , each $M$ -morphism with codomain $X$ is called a $M$ -subobject of $X$ . A closure operator on $\mathcal{C}$ is a family of maps $c=(c_C: $ M $-SubC \longrightarrow $ M $-SubC)_{C \in \mathcal{C}}$ satisfying some suitable conditions (it is a ""pre-closure"" operator, in general idempotence it is not satisfied). Well, my question is: it is possible to use categorical closure operators to describe the category of union-closed families? Similarly, if I consider the functor-structured category $S(Q_2)$ (with $Q_2(\Omega):=\Omega \times \Omega$ ) and the category of preorders, it is possible to do the same thing as above?","['elementary-set-theory', 'general-topology', 'category-theory']"
4356848,Wandering domains of $z + \sin(2\pi z)$,"I was recently working through Sullivan's Non-Wandering Theorem when I came across this counter-example to the theorem holding for functions $\mathbb{C} \to \mathbb{C}$ . It appears that the entire function $f(z) = z + \sin(2\pi z)$ is something of a standard example of a transcendental function that exhibits wandering domains. However, I cannot seem to be able to show that this is indeed the case, even after reading through a number of Baker's papers on the subject. My problem is that I cannot seem to be able to determine explicitly what these Fatou components actually are. In other examples, a strategy would be to find (super)attracting fixed points and hence use their immediate basin of attraction as the required Fatou components. Then, one would employ the periodicity of $\sin$ to show that these Fatou components are wandering. This strategy would appear not to work in this case, since every fixed point of $f$ is repelling and hence contained in the Julia set. So the only chance that this could work is to find a (super)attracting periodic orbit of period $\geq 2$ , which is a bit of an issue seeing how unwieldy the iterates of $f$ can be. Another strategy that I have tried is to work with covering spaces. Using the universal covering map $\mathbb{C} \to \mathbb{C} \setminus \{0\}$ , $z \mapsto e^{2\pi i z}$ , I get a commuting diagram $$\require{AMScd}
\begin{CD}
\mathbb{C} @>{f}>> \mathbb{C}\\
@VVV @VVV \\
\mathbb{C} \setminus \{0\} @>{g}>> \mathbb{C} \setminus \{0\}
\end{CD}$$ where $g(z) = z\exp(\pi(z - 1/z))$ . I would then apply the same strategy as described above to $g$ , but this gets just as unwieldy as working with $f$ . Now at my wits end, could I get some help on this problem?","['complex-analysis', 'complex-dynamics']"
4356876,Is there a natural connection on $TM$,"The Sasaki metric gives a natural way to equip $TM$ with a Riemannian metric in case $M$ is already equipped with a Riemanian metric. Question: Let $M$ be manifold equipped with a connection, is there a known natural way to equip $TM$ with a connection ?","['connections', 'tangent-bundle', 'differential-geometry']"
4356881,How should I have known that $x^4-2x^3-7x^2+10x+10=(x^2-2x-2)(x^2-5)$?,"How should I have known that $$x^4-2x^3-7x^2+10x+10=(x^2-2x-2)(x^2-5)$$ ? I was asked to find the splitting field of $f(x)=x^4-2x^3-7x^2+10x+10$ . The solution that I was given starts off by noting the given factorization of $f(x)$ into quadratics. Hw should I have seen this factorization? I tried writing $f(x)$ as the product of two arbitrary monic quadratics and matching coefficients but things got messy quite quickly. In general, to find the splitting field of a quartic polynomial, if all else fails, I believe I could find the roots of the quartic using the general method for solving a quartic by radicals (although I have not learned about this method, or the method for cubic's, I know they exist). Is using the general method for solving quartics a common approach to finding splitting fields of quartics?","['algebra-precalculus', 'splitting-field', 'factoring', 'polynomials']"
4356918,What is the volume of the largest truncated octahedron that can be inscribed in the unit sphere?,"If you were to maximize the volume of a truncated octahedron while keeping it in completely inside a given sphere, what percentage of the sphere's volume would it take up? This question is an extension of a larger one I've been wondering. What is the largest space-filling polyhedron of any kind that can be inscribed in a sphere? Space-filling meaning, can perfectly tile the 3D plane in Euclidean space. Would, for example, a rhombic dodecahedron maximized in a sphere take up more space than the maximized truncated octahedron? More concretely, imagine you have spheres of a valuable material you need to pack. You can pack them more densely by cutting them in a more packable shape, thus saving more of the material, but you lose whatever you cut off. Is there a space-filling polyhedron you could cut each sphere into that contains more than ~74.048% (maximum sphere packing density) of the original sphere?","['solid-geometry', 'polyhedra', 'geometry']"
4356947,"Simple graph with $G$ with $n$ vertices, satisfying $d(u)+d(v)\ge n-2$ for every two non-adjacent vertices $u,v$, wtih no Hamiltonian path","A simple graph $G$ with $n$ vertices in which the sum of degrees of every two non-adjacent vertices is at least $n-1$ has a Hamiltonian path. As described here , one can add a new vertex $w$ and connect it to other $n$ vertices, so that for each two non-adjacent $u,v\in V(G)$ , $$\deg(v)+1+\deg(u)+1\ge n-1+2\\=n+1,$$ which, according to Ore's theorem, means $G+w$ is Hamiltonian, that is, there is a Hamiltonian cycle- a cycle passing through all the vertices of $G+w$ . Now, if we remove $w$ and all the added edges connecting it to the initial $n$ vertices, we break a Hamiltonian cycle in $G+w$ and get a Hamiltonian path in $G$ . But, if we change the hypothesis to sum of degrees of every two non-adjacent vertices being at least $n-\color{red}2,$ the statement doesn't hold.
E. g., $n=6$ and $G$ consisting of $2$ triangles: $d(u)+d(v)=4$ for each non-adjacent $u$ and $v$ and there is no Hamiltonian path. Question: Is there any example with $|V(G)|>6$ where $d(u)+d(v)=n-2$ for at least one pair of non-adjacent vertices $u,v\in V(G)$ and for other non-adjacent pairs $u,w\in V(G), d(u)+d(w)\ge n-1$ and $G$ doesn't have a Hamiltonian path?","['graph-theory', 'examples-counterexamples', 'discrete-mathematics', 'hamiltonian-path']"
4356972,On topological definitions of zero-dimensionality,"There are many different, and not necessarily equivalent, definitions of zero-dimensionality for a topological space. Here are two examples: Def. 1 : A topological space is zero-dimensional if every open cover of the space has a refinement which is itself a cover consisting only of disjoint open sets. (This implies the Lebesgue covering dimension is zero.) Def. 2 : A topological space is zero-dimensional if it has a base consisting exclusively of clopen sets. (This implies the small inductive dimension is zero.) Suppose I were to create the following, perhaps more geometrically intuitive definition of a zero-dimensional topological space: Def. 3 : A topological space is zero-dimensional if no subspace of the original space is homeomorphic to an interval equipped with the Euclidean topology. What statements can we make about the connections, if any, between Def. 3 and Defs. 1 and 2? For example, does Def. 3 imply Def. 2 or 1? Does it imply it only in separable metrizable spaces? (And so on.)","['general-topology', 'dimension-theory-analysis']"
4356982,"Reference Book - Analysis with a ""General View""","When studying Analysis (Real, $\mathbb R^n$ , Metric ...) it's common to see similar (or even the same) theorem, sometimes with the same (or similar) proof. I was wondering if there is any book out there that tries to present ""Analysis"" with results from a more general spaces to more specific spaces. For example, if a result is valid for every complete metric space, then it's presented and proved with this generic view. While, if the result is specific to $\mathbb R$ , then the result is shown for that case. I understand that this might be perhaps too much to ask, but you never know. Maybe someone wrote such book already. Just to make clear where this comes from. I'm writing some notes on courses I've taken as a graduate student, and I was trying to organize my notes from ""more generic to more specific"". Hence, I started with stuff like Topological Spaces, Metric Spaces, and moved to Complex and Euclidean Spaces. I realized that many of the results I proved for Euclidean Spaces were pretty much the same in more general spaces, and I was repeating a bunch of my theorems, with slight changes.","['analysis', 'reference-request']"
4356994,Faster way to find the first four non-zero terms of the Maclaurin series for $\frac{1-x}{1+x}\cosh\sqrt{x}$,"I want to find the first 4 non-zero terms for : $$\frac{1-x}{1+x}\cosh\sqrt{x}$$ Before expanding, I rewrite this as $$(1-x)\left(\frac{1}{1+x}\right)\cosh\sqrt{x}$$ Then I expand to get $$(1-x)\left(1-x+x^2-x^3+\cdots\right)\left(1+\frac x {2!}+\frac {x^2} {4!}+\frac{x^3}{6!}+\cdots\right)$$ Now multiplying these brackets and simplifying takes a while (pretty long for my exam time), so is there a faster method to do this or to multiply these brackets?","['hyperbolic-functions', 'calculus', 'taylor-expansion', 'power-series', 'algebra-precalculus']"
4357005,$AA^T=J_n-I_n$ has a solution over $\mathbb{F}_2$ iff $n$ is odd.,"I came across a combinatorics problem, which I turned into a Linear Algebra problem. In the end, it boiled down to the following problem: \begin{align*}
 AA^T=J_n-I_n\pmod 2
\end{align*} has a solution iff $n$ is odd. Here we are working over the field $\mathbb{F}_2$ $J_n$ denotes the matrix with all elements $1$ . $A$ is a $n\times n$ matrix. My attempt: If $n$ is odd, it is not hard to see that $A=J_n-I_n$ itself is a solution. I am not sure if my ""proof"" for the even $n$ part is correct. Throughout the rest of the solution we will be working over $\mathbb{F}_2$ . Let us denotes the $i$ -th row vector of $A$ as $v_i$ . Then the equation basically translates to \begin{equation*}
v_i\cdot v_j=
\begin{cases}
0 & i=j \\
1 & \text{otherwise} 
\end{cases}
\end{equation*} For even $n$ , $\det(J_n-I_n)=1\pmod 2$ (see circulant matrix ). Thus we conclude that $\det(A)=1\pmod 2\implies A$ is invertible $\implies$ $v_i$ 's $(1\le i\le n)$ are linearly independent $\implies$ $v_i$ 's form a basis pf $\mathbb{F}^n_2$ . Now consider \begin{equation}
v=\sum_{i=1}^{n}\lambda_iv_i\implies v^2= \sum \lambda_i^2v_i^2 + 2 \sum \lambda_i\lambda_j(v_i\cdot v_j )=0\pmod  2
\end{equation} But this is a contradiction, because $e_1\cdot e_1=1$ , here $e_1$ is the first standard basis vector of $\mathbb{F}_2^n$ . This whole proof seems a little bit weak to me, in the sense there seems to be no key observation/key step in the proof. Also defining the dot product seems a bit iffy to me. After all, this does not make $\mathbb{F}_2^n$ into a inner product space.","['finite-fields', 'matrices', 'solution-verification', 'linear-algebra', 'combinatorics']"
4357006,Why do we approximate sample stdev using sample stdev / sqrt(n) when population stdev is unknown? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question In this stackoverflow stdev estimation question , we are discussing about when to use z-distribution and t-distribution. Now, I'm getting confused here why are we taking the indirect & apparently longer path for finding sample z-score? For a given set of sample and population, $sample \ z_{score}= \frac{x - \mu_{sample}}{\sigma_{sample}}$ . Now, we go around to estimate $\sigma_{sample} = \frac{\sigma_{population}}{\sqrt{n}} \ \ \ \ \ \ \  ... [1]\ $ where $n$ = sample size. Now, my question is following: If we want $\sigma_{sample}$ , why not simply find it using formula: $$\sigma = \frac{\sum_{i=1}^{n}(x_{i}-\mu)^2}{N-1}$$ Also, this will be having a different answer from our estimation [1]'s $\sqrt{n}$ factor. Please help me if I'm missing something major over here.","['statistics', 'standard-deviation', 'probability']"
4357040,Estimating the length of a curve from below by a bound on its curvature.,"I got stuck on a problem. Let $\alpha(s)$ be smooth simple closed curve  with curvature $k(s)$ and $0 <k(s) \leq c $ . Then show that the length of the curve is limited from below by $2\pi \frac{1}{c}$ . I can't formalize the solution very well: in a neighborhood of each point $\alpha(s)$ the curve $\alpha$ has length very near to the length of the osculating circle tangent at $\alpha(s),$ and the osculating circle has radius at least $\frac{1}{c},$ then we should be able to conclude by summing up all the local contributions.","['curves', 'multivariable-calculus', 'curvature', 'differential-geometry']"
4357081,How many solutions does the ODE have?,"The question : Given ODE : $$
\begin{cases}
y'-a^2(y')^3-\frac{\sin(x)}{x+y}=0 \\
y(0)=1
\end{cases}
$$ Write how many solutions does the system have for $a=0$ and $a \ne 0$ . My try : for $a=0$ we have a unique solution by Picard's theorem. In the case where $a \ne 0$ I'm stuck and can't understand...",['ordinary-differential-equations']
4357087,Stokes' theorem in higher dimensions,"I'm trying to follow along with Stokes's theorem on page 46 and I'm not quite understanding where this comes from: $$ V=\int_{B} dx \wedge dy \wedge dz \wedge dt = - \int_{S} t dx \wedge dy \wedge dz $$ I can understand the negative due to the anticommutivity, $\int_{B} dx \wedge dy \wedge dz \wedge dt = - \int_{B} dt \wedge dx \wedge dy \wedge dz$ due to how there would be three interchanges But where does the $d$ for the $t$ go? Is it as simple as integrating with respect to $t$ but leaving all the rest the same? (So far, I've only applied $d$ to various forms and so I'm not sure how it works in reverse. -- I can understand the first line is Stokes' theorem, but how would you know that α is that? I'd know that the volume is $\int_{B} dx \wedge dy \wedge dz \wedge dt = \int_{domain} d\alpha $ and then I'd need α itself to integrate over the surface like $\int_{surface} \alpha$ . But how would we get $\alpha$ if given $d\alpha$ ?","['multivariable-calculus', 'stokes-theorem', 'vector-analysis', 'differential-geometry']"
4357111,"A chain of well ordered sets defines a ""unique"" well order on the union of chains?","Halmos in his Naïve Set Theory , states the following result in Section 17, Well Ordering : If a collection $\mathscr C$ of well ordered sets is a chain with respect to continuation, and if $U$ is the union of the sets of $\mathscr C$ , then there is a unique well ordering of $U$ such that $U$ is a continuation of each set (distinct from $U$ itself) in the collection $\mathscr C$ . I think that this is not quite correct. Consider this. $\mathscr C = \{ \{0, 2\}, \{0, 2, 3\}, \{0, 2, 3, 6, 8\} \}$ with the usual ordering of $\mathbb N$ in each of the sets. Now, I can order $U = \{0, 2, 3, 6, 8\}$ as $0 < 2 < 3 < 8 < 6$ . Then $U$ is a continuation of each $X\in\mathscr C\setminus \{U\}$ . But the usual order of $\mathbb N$ also satisfies this requirement, albeit being a different order. Question: What has gone wrong here? He defines continuation as follows. A well ordered set $A$ is a continuation of a well ordered set $B$ iff $B$ is an initial segment (the set of all strict predecessors) of some element of $A$ and if the order on $B$ can be inherited from that on $A$ . (Hence, a well ordered set can't be a continuation of itself.)","['elementary-set-theory', 'order-theory', 'well-orders', 'set-theory']"
4357115,Is $f^{-1}\big( \sqrt{xf(x)} \big)$ convex (for large $x$) when $f(x) = o(x)$ is concave and strictly increasing?,"Question Let $f : (N, \infty) \to \mathbb (0,\infty)$ be a strictly increasing, concave function such that $f(x) \to \infty$ and $f(x) = o(x)$ for $x \to \infty$ , where $N > 0$ . Also, let $f$ be twice differentiable (or more if needed). Define $g(x):= f^{-1}\big(\sqrt{xf(x)} \big)$ where the definition makes sense (i.e. for all $x$ such that $\sqrt{x f(x)} > f(N)$ ). Does it follow that $g$ is convex for large $x$ ? That is, if the restriction $g|_{[K, \infty)}$ is convex for some $K>0$ . Also: I'd like to know if $h(x):= g(x)/x$ is increasing for large $x$ . Thoughts The idea is that the geometric mean $\sqrt{x f(x)}$ sort of pulls $f$ up toward the 45 degree line, straighening it out a bit. This is still concave (which isn't too hard to show, knowing the geometric mean is a concave function), but ""less"" so. Applying the convex $f^{-1}$ to $f(x)$ yields $x$ , sort of cancelling out the concavity. But applying $f^{-1}$ to $[xf(x)]^{1/2}$ should yield a convex function, since we already brought $f(x)$ a bit in this direction with the geometric mean.","['convex-analysis', 'asymptotics', 'real-analysis']"
4357122,How would one describe $k$ iterations of $\cos(n)$?,"What function would one use to describe $k$ iterations of $\cos(n)$ ? I'm pretty sure that the function would be a damped sine wave (as can be seen in the curve fit equation I wrote in the third row), however the actual formula is probably quite complicated as it involves the Dottie number , which, to my knowledge, cannot be expressed in terms of $e$ , $\pi$ , or polynomial roots. Below is an attempt of curve-fitting on $n=1$ . Some of the deficiencies of this fit I've noticed while experimenting with Desmos are that the lines are far too steep (even without the scale factor or with a smaller scale factor), and the fit seems to be weaker for even $n$ (although I presume that this is simply an artifact of approximation). Note that the y-axis has been scaled by a factor of 5 for the sake of graph readability.","['desmos', 'fixed-point-theorems', 'wave-equation', 'limits', 'trigonometry']"
4357128,Are the Besicovitch Covering Constant known exactly?,"My question above summarizes my question. I have been studying geometric measure theory and encountered the Besicovitch covering lemma. Also an auxiliary result ""Fix an arbitrary set $A \subset X$ and associate a ball $B(a, r(a))$ to each $a \in A$ so that $sup_{a \in A} r(a) < \infty$ . The theorem states that there exists a constant,
ß = ß (N), depending only on the normed space, such that for some m < ß ,
one can find m disjoint subsets $A_i \subset A$ with the property that for each set $A¡$ ,
the associated balls are pairwise disjoint and the union $\cup_{1<i<m} \cup_{a \in A_i} B(a, r(a))$ still covers A."" I found online that for R^1, ß = 2 (easy), R^2, ß = 18, and via the paper ""On the Besicovitch constant in small dimensions"" that there are upper bounds for small n on euclidean space R^n. But I couldn't find any results for exact numbers. Since this result was published in 1998, I reckon there must have been some progress on these upper bounds but I could not find any results in the literature. Do we known the exact numbers? And up to which n? P.S. Note that it has been proven that ß(N) is the same as the amount of n-dimensional unit balls that can be packed in a n-dimensional ball of radius 5.","['euclidean-geometry', 'measure-theory', 'geometry', 'geometric-measure-theory']"
4357189,Calculate the determinant of $a_{ij} = \frac{(1+x)^{i+j-1}-1}{i+j-1}$,"There is a question asked by my classmate. Looking forward to some ideas, thanks. Set $A=\{a_{ij}\}_{n\times n}$ , where $$a_{ij}=\frac{(1+x)^{i+j-1}-1}{i+j-1}.$$ Prove that $\det A=cx^{n^2}$ for some $c$ . I have tried to calculate it, but failed. I computed $$\frac{(1+x)^{i+j-1}-1}{i+j-1}=\sum_{k=1}^{i+j-1}\frac{(i+j-2)!}{k!(i+j-1-k)!}x^k,$$ but I have no idea how to continue.
I know when $a_{ij}=\frac{1}{i+j-1}$ , it is the Hilbert matrix, and we can get its determinant, but I don’t know how to calculate the above determinant. Are there some hints? Looking forward to your answer. Thanks!","['matrices', 'determinant', 'linear-algebra']"
4357221,Rick Durrett Probability Theory 5th edition exercise 4.6.1,"Ex 4.6.1: Let $Z_1,Z_2, ...,$ be independent and identically distributed with $E|Z_i|<\infty$ , let $\theta$ be an independent random variables with finite mean, and let $Y_i=Z_i+\theta$ . If $Z_i$ is normal $(0,1)$ then in statistical terms we have a sample from a normal population with variance 1 and unknown mean. The distribution of $\theta$ is called the prior distribution, and $P(\theta \in \dot\mid Y_1,\ldots,Y_n)$ is called the posterior distribution after n observations. Show $E(\theta \mid Y_1,\ldots,Y_n) \rightarrow \theta$ almost surely. The problem is from Rick Durrett's Probability Theory 5th edition. Theorem 4.6.8: Suppose $\mathcal{F}_n \uparrow \mathcal{F}_{\infty}$ , i.e., $\mathcal{F}_n$ is an increasing sequence of $\sigma$ -fields and $\mathcal{F}_{\infty}=\sigma(\cup_n \mathcal{F}_n)$ . As $n \rightarrow \infty,$ $E(X\mid \mathcal{F}_n) \rightarrow E(X\mid \mathcal{F}_{\infty})$ almost surely and in $L^1$ . Attempt: Let $\mathcal{F}_n=\sigma(Y_1,\ldots,Y_n)$ , then by theorem 4.6.8, we have $E(\theta \mid \mathcal{F}_n) \rightarrow E(\theta\mid \mathcal{F}_{\infty})$ . Then I am told to show $\theta \in \mathcal{F}_{\infty}$ using strong law of large number. My question lies in using the strong law of large number. Since $Z_i \sim  \text{normal}(0,1)$ , $E Z_i=0$ , then $EY_i=E \theta$ . By strong law of large number, $\frac{Y_1+\cdots+Y_n}{n} \rightarrow E\theta $ almost surely, not $\rightarrow \theta$ . That's what got me here. Thanks in advance! Update: https-//
math.stackexchange.com/questions/264198/convergence-of-empirical-distribution?rq=1 Based on the link above and Eric (Thanks!)'s comment, I think I got it. Let $EZ_i=0$ , by strong law of large number, $\begin{align} \lim_{n \rightarrow \infty} \frac{Z_1+...+Z_n}{n}=EZ_i=0  \\  \Rightarrow \theta+\lim_{n \rightarrow \infty} \frac{Z_1+...+Z_n}{n}=\theta \\ \Rightarrow \frac{n\theta}{n}+\lim_{n \rightarrow \infty} \frac{Z_1+...+Z_n}{n}=\theta \\ \Rightarrow \lim_{n \rightarrow \infty} \frac{n\theta+Z_1+...+Z_n}{n}=\theta \\ \Rightarrow \lim_{n \rightarrow \infty} \frac{(Z_1+\theta)+...+(Z_n+\theta)}{n}=\theta \\ \Rightarrow \lim_{n \rightarrow \infty} \frac{Y_1+...+Y_n}{n}=\theta  \end{align} $ .","['martingales', 'measure-theory', 'law-of-large-numbers', 'probability-theory']"
4357254,"How to prove or disprove $ |r'(a)| \leq c_1 \sup_{x \in [a-1/2,a+1/2]} |r(x)|$?","Let matrix $ A  = \begin{bmatrix}
		a & 1 \\ 0 & a
	\end{bmatrix}$ is a Jordan matrix with $ -1 < a < 1 $ . Let $ r(z) = \frac{p(z)}{q(z)} $ is any an irreducible rational function, where $p(z)$ and $q(z)$ are polynomials of degree $k$ with real coefficients , and $p(z)$ and $q(z)$ are not zero on the interval $E = \{v^\top Av | v \in \mathbb{R}^2, \|v\| = 1\} = [a-1/2, a+1/2]$ . My problem is finding a positive number $c$ (possibly depends on $k$ ) such that \begin{equation}\notag
		\|r(A)\|_2 \leq c \sup_{x \in E} |r(x)|.
\end{equation} Now I have get that \begin{equation}\notag
		r(A) = \begin{bmatrix}
			r(a) & r'(a) \\ 0 & r(a)
		\end{bmatrix}
\end{equation} and $\|r(A)\|_2 \leq \sqrt{2}\|r(A)\|_1 = \sqrt{2}(|r(a)| + |r'(a)|)$ . Thus I guess that there may exist a positive number $c_1$ (possibly depends on $k$ ) such that \begin{equation}\notag
		|r'(a)|  \leq c_1 \sup_{x \in E} |r(x)|.
\end{equation} However, I don't know how to prove or disprove this conjecture.","['analysis', 'real-analysis', 'complex-analysis', 'functional-analysis', 'rational-functions']"
4357300,How to prove this identity involving Lerch function?,"Trying to find another way to answer this question , I arrived at a point where I cannot simplify anymore. The solution being $$I_1=\int_0^{\frac \pi 2}\csc (x) \log \left(\frac{1+b \sin (x)}{1-b \sin (x)}\right)=\pi  \sin ^{-1}(b)$$ I am stuck with $$I_2=\frac 12\Bigg[a_+ \Phi
   \left(a_+^2,2,\frac{1}{2}\right)+a_- \Phi
   \left(a_-^2,2,\frac{1}{2}\right)\Bigg]\quad \text{where}\quad a_\pm=b\pm\sqrt{b^2-1}$$ $\Phi(.)$ being the Lerch transcendent function. How to prove that, for $\color{red}{-1 \le b \le 1}$ , $\color{red}{I_2=I_1}$ ?","['trigonometry', 'definite-integrals', 'special-functions']"
4357304,An 8th grade contest-math puzzle,"In a competitive sort of exam, the following question was aimed at 8th graders and above- Sophie had written the numbers 1 to 22 in the 22 discs in the figure, but Adelaide, her big annoying sister, erased fourteen. Find the positions of the numbers erased knowing that Each number written in the centre of a hexagon represents the sum of numbers placed at the vertices of this hexagon Two discs directly connected by a line never contain consecutive numbers. On the answer sheet, write the values of the numbers $a, b, c, d$ and $e$ . On top of this, the question may have multiple answers (and the student is expected to report all of them) as well. I don't have any idea about how to attack this problem. Of course, brute forcing is not a solution since in that case you have to check through $\sim 22^{14}\approx 6.2\times 10^{18}$ cases which is an impossibility even for a computer programme. After doing some Google search recently, I found out something similar called Hexagonal Tortoise Problem and a paper about it. But, these couldn't take me anywhere. One random observation is that if we indeed need to brute force the solution with some educated guesses, then maybe the disc connecting $4,7,12$ is the one to start with (since that's the one which is most restricted ). But, even in that case, there are too many choices to deal with. All ideas are welcome. This question had an extremely elegant solution which was closed down because of the complaints of the question being a contest problem. The reality is that- I had already submitted my answers for the contest I posted the question only after the initial contest deadline But unfortunately, I wasn't aware that the contest had extended their deadlines and hence I made this mistake. I can't clearly remember the answer (except that it dealt with adding up the numbers in the boxes somehow). If someone can solve it, or someone here remembers it, or the actual original answerer sees it once more, please consider putting the answer here again.","['contest-math', 'algebra-precalculus', 'puzzle', 'recreational-mathematics']"
4357305,Periodic order-reversing homeomorphism in the unit interval,"I'm studying the theorem of Kerekjarto and it says: ""Let $f : I \to I$ be a periodic homeomorphism of the unit interval. If $f$ preserves the endpoints then $f$ is the identity map. If $f$ exchanges the endpoints then $f^2 = Id$ and $f$ is conjugate to the reflection map $x \to 1 − x$ ."" Note the meaning of periodic here: ""We say that $f$ is periodic if there is an integer $n>0$ such that $f^n=Id$ . The period of $f$ is the smallest positive integer $n$ with this property"" I can prove that if $f$ preserves the endpoints, then $f = Id$ , but I can't find a solution to the other statement, that is, if $f$ is a periodic homeomorphism, $f(0)=1$ and $f(1)=0$ , then it $f(x) = 1 - x$ . We know that $f$ is an involution and $f^2 = Id$ For example, it's easy to prove that if $f$ is linear, then $f$ must be $1-x$ , but why should $f$ be linear?","['general-topology', 'dynamical-systems']"
4357310,"Prove or disprove : In a topological space $(X,\tau)$ if every compact subsets $K\subset X$ are closed then $(X, \tau) $ is hausdorff.","$(X, \tau) $ be a topological space. $K\subset X$ is compact. I can prove if $X$ is hausdorff space then $K\subset X$ is closed. I know that the proof strongly requires the $T_2$ - property of $X$ . But if $X$ is not hausdorff then a compact subset is not necessarily closed. The simplest example, $X=\{0, 1\}$ and $\tau=\{\emptyset ,\{0\},X\}$ Then, $K=\{0\}\subset X$ is compact but not closed. My question : In a topological space $(X,\tau)$ if every compact subsets $K\subset X$ are closed then $(X, \tau) $ is hausdorff. This may be a simplest question, I mean there may be some trivial example but i am not able to solve it.","['separation-axioms', 'general-topology', 'examples-counterexamples', 'compactness']"
4357345,Find the area of ​the shaded region in the figure below,"For reference: In the figure $O$ is the center of the circle and its radius measures $a$ and $AQ = QB$ . Calculate the area of ​​the shaded region.(Answer: $\frac{a^2}{4}(\pi-2)$ ) correct figure My progress: If $AQ = BQ \implies \angle AQB=90^\circ$ Complete the square $AQBD$ . incorrect figure. incorrect figure, please do not consider it for any effect $OC = r$ and $QC =R = AC.$ $O$ is centre of square. $QO$ is angle bisector, therefore $\angle AQO$ is $45^\circ.$ $QD = R\sqrt2$ Considering $\triangle OCQ$ , $\displaystyle r^2+\left(\frac{R}{2}\right)^2=OQ^2\implies r^2+\frac{R^2}{4}=(R\sqrt2)^2$ $\therefore R = \dfrac{2r\sqrt7}{7}$ I don't see a solution...is it missing some information? The book has another similar question but in this question $a = 2$ and answers match if we replace $a$ with $2$ . Diagram below -","['euclidean-geometry', 'area', 'geometry', 'plane-geometry']"
4357350,"If every finite quotient group of a finitely generated linear group G is solvable, then G is solvable","For this question, I was able to show that each finite quotient is polycyclic: Suppose $N$ is a normal subgroup of finite index. Then, all subgroups of $G/N$ are finite, so $G/N$ is Noetherian. A solvable group is polycyclic iff it is Noetherian. At this point, I don't know how to continue or if I'm even going in the right direction. I am familiar with Theorems by Mal'cev, Milnor, Wolf etc. I would appreciate any help!","['infinite-groups', 'linear-groups', 'finitely-generated', 'group-theory', 'solvable-groups']"
4357363,Coin Flipping Riddle,"2 criminals A and B, were recently captured and brought to prison. They were then locked in two separate rooms. Known for being exceedingly smart, the prison warden set a test for them. They flip a fair coin an infinite number of times and told outcomes of odd trials to A and even trials to B. Now A and B are separately told to pick a trial whose outcome they don't know, i.e., A is supposed to pick an even number, and B is supposed to pick an odd number. If the outcomes of the trials A and B picked are the same, the prison warden will release them. If they are different, they will spend life in jail. The Warden told them what they were going to do and let them agree upon a common strategy in advance but after that they can't communicate. Find a strategy such that the chance of winning is higher than $0.5$ . I recently tried this problem in university and I am interested to see other people's solutions, and perhaps the optimal solution. Edit: My strategy is that both players agree that as soon as they see 2 tails in a row for themselves, they pick the number in the middle, i.e. A sees a tail on coin flip 2 and 4 so he picks 3, B does the same. After running this on a computer simulation I get a 60% winrate. Although I don't fully understand why. Edit: a 70% chance of winning has been found on my Puzzling StackExchange duplicate post! Strategies so Far 70% chance of winning by @Jaap Scherphuis 2/3 probability of winning by @Mike Earnest 62.5% chance of winning by @Teo Miklethun","['puzzle', 'probability']"
4357371,Why is $P(|x|\ge\varepsilon) = P(x^2\ge \varepsilon^2)$?,I am trying to understand the Chebyshev inequality. The step I cannot really follow is the identity: $P(|x-\mu|\ge\varepsilon) = P((x-\mu)^2 \ge \varepsilon^2)$ . Can anyone explain the intuition behind this or give some other form of insight? That would be really helpful! Thanks!,"['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4357421,Intuition of local connection forms,"In the past I had the feeling that I understood the mathematics and ideas behind principal connections and connection one-forms rather well. However, while trying to explain these ideas in simple terms to a nonmathematical audience, I noticed that my understanding might not extend flawlessly beyond the pure formulas. When you have a principal $G$ -bundle $\pi:P\rightarrow M$ , you can canonically define the vertical subbundle as the kernel of the projection $\pi_*$ . A connection is then equivalent to a choice of complement inside the tangent bundle $TP$ . Locally, where $P|_U\cong U\times G$ , one can identify the vertical spaces with the Lie algebra $\mathfrak{g}$ . A complement can then be defined by assigning to every basis $\partial_i$ of a tangent space on $M$ a ''horizontal lift'' $$\widetilde{\partial}_i:=\partial_i+\chi_i,$$ where $\chi_i\in\mathfrak{g}$ (these two terms should be mapped in the right way to a tangent space of $P|_U$ and extended to a local frame). The associated connection one-form $\omega$ on $P$ is then, as far as I understand, the form that assigns to any vector field on $P$ , at any point, the contribution in $\mathfrak{g}$ that does not arise from these $\chi_i$ 's: $$X = \sum_i\lambda_i(\partial_i+\chi_i) + \omega(X),$$ for some scalars numbers $\lambda_i$ . It measures the change in the fibres that does not arise from a mere change on the base manifold. When we then, locally, pullback the connection one-form $\omega$ to a one-form on the patch $U$ , we get $$A = s^*\omega.$$ By definition of the pullback I would then think that this evaluates to the following formula on any vector field: $A(Y) = \omega(s_*Y)$ . By then combining the above statements, I would expect that $A$ assigns to any vector field on $M$ the difference between its pushforward (along a section) to the bundle and its horizontal lift, resulting in a linear combination of the $\chi_i$ 's. Is this correct?","['principal-bundles', 'connections', 'differential-geometry']"
4357422,Prove $\int_0^1 \frac{\ln x\ +\ \ln(\sqrt x\ +\sqrt {1+x})}{\sqrt {1-x^2}} dx=0$,"If a simple way exists , I am looking to show that $$\boxed{K=\int_0^1 \frac{\ln(x)+\ln(\sqrt x+\sqrt {1+x})}{\sqrt {1-x^2}} dx=0}$$ say, with  symmetry, a clever change of  variables, or  integrations by parts, without evaluating  integrals separately. It is similar to @Zacky question , of proving $$\boxed{\int_0^\frac{\pi}{2}\left(\frac{\pi}{3}-x\right)\frac{\ln(1-\sin x)}{\sin x}dx=0}$$ without calculating separately integrals. If we  take separately $$I=\int_0^1 \frac{\ln(x)}{\sqrt {1-x^2}}dx,\>\>\>\>J=\int_0^1 \frac{\ln(\sqrt x+\sqrt {1+x})}{\sqrt {1-x^2}}dx$$ the integrals $I$ and $J$ define the same series to a sign (two series of opposite sums) $$ I=-\sum_{n=0}^{\infty}\frac{{2n \choose n}}{4^{n}(2n+1)^{2}},\>\>\>\>\>J=\sum_{n=0}^{\infty}\frac{{2n \choose n}}{4^{n}(2n+1)^{2}}$$ Explanation By Fourier series $\ln\left(\sqrt{1+\sin x}+\sqrt{\sin x}\right)=\sum_{k=0}^\infty\frac{(2k)!}{4^k(2k+1)(k!)^2}\sin((2k+1)x)$ then $J=\int_0^{\frac{\pi}{2}} \ln\left(\sqrt{\sin t}+\sqrt{1+\sin t}\right)dt=\sum_{n=0}^{\infty}\frac{{2n \choose n}}{4^{n}(2n+1)^{2}}$ $ I=\int_{0}^{1}\frac{\log(t)}{\sqrt{1-t^{2}}}dt$ We know that $\frac{1}{\sqrt{1-t^{2}}}=\sum_{n=0}^{\infty}\frac{{2n \choose n}}{4^{n}}t^{2n} $ and $ \int_{0}^{1}\log(t)t^{2n}dt=-\frac{1}{(2n+1)^{2}}$ then $I=-\sum_{n=0}^{\infty}\frac{{2n \choose n}}{4^{n}(2n+1)^{2}} $ We can write K as $$K=\int_0^{\dfrac{\pi}{2}} \Big(\ln(\sin t )+\ln\left(\sqrt{\sin t}+\sqrt{1+\sin t}\right)\Big)dt=0 $$ Remark : Wolframalpha can calculate K , but do not know how to calculate $$\int_0^1 \frac{\ln(\sqrt x+\sqrt {1+x})}{\sqrt {1-x^2}} dx$$ I believe that Wolframe uses a simple way to see that $K$ is zero . same observation for the Zacky’s integral","['integration', 'calculus', 'alternative-proof', 'definite-integrals']"
4357460,Does $y = f(x)$ and $y=g(x)$ imply $f(x)=g(x)$?,"Okay, this maybe a very dumb question but I can't seem to find a ""proper"" reason to show why this isn't true. In some calculus books or notes, I have come across questions where they sometimes begin by saying, ""If $y =f(x)$ and $y =g(x)$ are two functions and blah-blah..."" I'm curious, is this some ""abuse of notation"" thing because it should be clear from the context? Because should it not mean that $y =f(x) = g(x)$ ?. There is also another context where this is used— While making graphs. The question usually says graph $y=f(x)$ and $y=g(x)$ etc. Surely it is not a substitution, otherwise the two functions would be same. So what does equating (possibly multiple) functions to $y$ mean? Or what is the intended meaning when we say $y =f(x)$ and $y =g(x)$ ?","['calculus', 'functions', 'algebra-precalculus']"
4357517,Cumulative Multinomial Distribution does not seem to add up to 1.00 for parameters >3,"For a multinomial distribution $P = ( n! / (\prod_{i=1}^k {n_i!}) ) * ( \prod_{i=1}^k {{p_i}^{n_i}})$ If you sum the resulting multinomial distribution for every possible unique frequency for k=2 for any integer n that is n > 1 (this being equivalent to the binomial distribution) this will equal 1.00.
The same is true for k=3 But for integers k>3 this does not seem to hold. It will always be less than 1.00. Try it yourself for probabilities <0.25,0.25,0.25,0.25> and n=4. You’ll get $0.09375 + (0.046875*12) + (0.01563*12) + (0.00391*4) = 0.85951$ <1,1,1,1> -> 1 unique permutation adding up to 4 <0,1,1,2> -> 12 unique permutations adding up to 4 <0,0,1,3> -> 12 unique permutations adding up to 4 <0,0,0,4> -> 4 unique permutations adding up to 4 I multiplied by 12,12,and 4 because they have the same multinomial distribution as you’ll see when you try it. What’s going on here? I’ve made a mistake right? —-please excuse the informalities I’m not a mathematician","['probability-theory', 'probability-distributions', 'binomial-distribution', 'multinomial-distribution', 'probability']"
4357587,Necessary and sufficient condition on the cardinal number of quotient set,"I'm studying for an exam, and I have found question that I'm not sure about how to solve it. The question is: Let A be finite set. Let R be equivalence relation on A. 1.Write necessary and sufficient condition on the cardinal number of quotient set so R will be equality relation (equality relation is the relation R: xRy iff x=y. 2.Is the necessary and sufficient condition from (1.) true for infinite set A? My solution for 1 is: |A/R| = |A| iff R is the equality relation. I think its true
because the number of all equivalence classes equal to the the number of elements in A. For example A = {1,2,3} [1]R = {1} [2]R = {2} [3]R = {3} My solution for 2 is: Yes, The cardinal of A/R is still equal to the cardinal of A. I would like to know where my mistakes are.
Tell me please if the absence of Latex notations is a problem.","['elementary-set-theory', 'cardinals', 'relations']"
4357593,"If $f(x)=\frac{1}{3} \biggl ( f(x+1)+\frac{5}{f(x+2)}\biggl)$ and $f(x)>0$, $\forall$ $x\in \mathbb R$ then $\lim_{x \rightarrow \infty} f(x)$ is?","If $f(x)=\frac{1}{3} \biggl ( f(x+1)+\frac{5}{f(x+2)}\biggl)$ and $f(x)>0$ , $\forall$ $x\in \mathbb R$ then $\lim_{x \rightarrow \infty} f(x)$ is? Doubt: In solution provided in book they assumed that $\displaystyle \lim_{x \rightarrow \infty} f(x)=\displaystyle \lim_{x \rightarrow \infty}f(x+1)=\displaystyle \lim_{x \rightarrow \infty}f(x+2)=l$ . How can they all be equal?","['limits', 'calculus', 'limits-without-lhopital']"
4357633,$\lim_{n \to \infty}\frac{\sum_{i=0}^{n}{\sqrt{i}}}{n^2}=0 \; ?$ [duplicate],"This question already has answers here : $ \lim_{n \to \infty} \frac{\sqrt{n}(\sqrt{1} + \sqrt{2} + ... + \sqrt{n})}{n^2} $ (4 answers) Closed 2 years ago . I was wondering whether this limit converges to zero: $$
\lim_{n \to \infty}\frac{\sum_{i=0}^{n}{\sqrt{i}}}{n^2}=0
$$ And i'm pretty sure it is. First, by intuition. I know that $\sum_{i=0}^n{i} = \frac{n(n+1)}{2}$ ~ $O(n^2)$ , so i guess that $\sum_{i=0}^n{\sqrt{i}}$ is ""less powerful"", but i don't really know how much ""lesser"" So, the thing that really interest me was: what is the ""cardinality"" of $\sum_{i=0}^{\infty}{\sqrt{i}} \;? \quad$ Is it ""equal ~"" to $O(n)$ ? (I'm not sure i translated the words correctly. Does 'cardinality' is the right word for my description? I'm not familiar with these words in english, sorry. hope you understood what i meant). Here is my thought: $
{\sum_{i=0}^{n}{\sqrt{i}}} = 
{\sqrt{1}+\sqrt{2}+\sqrt{3}+...+\sqrt{n-1}+\sqrt{n}} =
{\sqrt{n} \big( \frac{\sqrt{1}}{\sqrt{n}} +\frac{\sqrt{2}}{\sqrt{2}} +...+ \frac{\sqrt{n-1}}{\sqrt{n}} + \frac{\sqrt{n}}{\sqrt{n}} \big)} =
{{\sqrt{n} \big( \sqrt{\frac{1}{n}} + \sqrt{\frac{2}{n}} +...+ \sqrt{\frac{n-1}{n}} + \sqrt{\frac{n}{n}} \big)}}
$ and so: $$
\lim_{n \to \infty} \frac{\sum_{i=0}^{n}{\sqrt{i}}}{n^2} = 
\lim_{n \to \infty} \frac{{\sqrt{n} \big( \sqrt{\frac{1}{n}} + \sqrt{\frac{2}{n}} +...+ \sqrt{\frac{n-1}{n}} + \sqrt{\frac{n}{n}} \big)}}{n^2} \overbrace{<}^{(*)}
\lim_{n \to \infty} \frac{{\sqrt{n} \big( \sqrt{\frac{n}{n}} + \sqrt{\frac{n}{n}} +...+ \sqrt{\frac{n}{n}} + \sqrt{\frac{n}{n}} \big)}}{n^2} =
\lim_{n \to \infty} \frac{{\sqrt{n} \big( \overbrace{1 + 1 +...+ 1 + 1}^{n \, times} \big)}}{n^2} =
\lim_{n \to \infty} \frac{ \sqrt{n} *n }{n^2} =
\lim_{n \to \infty} \frac{ \sqrt{n} }{n} =
\lim_{n \to \infty} \frac{1}{ \sqrt{n} } = 0
$$ But my enlargement in $(*)$ above was too big. I was wondering if you can suggest me some better way :)","['power-series', 'limits']"
4357656,Counterexample to Jensen's Inequality when the convex function admits values in the extended real set,"Let $(X,A,\mu)$ be a set, a $\sigma$ -algebra and a measure. Suppose that $\mu(X) = 1$ . Let $u : X \rightarrow {\mathbb{R}}$ and $f : \mathbb{R} \rightarrow \mathbb{R} \, \cup \, \{+\infty \} $ be an integrable function and a convex, lower semi-continous function respectively. Then $$\int_{X} f \circ u \, d\mu \geq f \left( \int_{X} u \, d\mu \right)$$ This is the version of Jensen's Inequality I'm working with (note that convex functions with values in $\mathbb{R} \cup {+\infty}$ are not automatically continous and that LSC is required in order to have the lower affine approximants required to prove the theorem). I'd like to find a counterexample to this inequality when $f$ is convex but not necessarily lower semi-continous. I expected it to be quite easy to find, but the fact that $E = \{ x \in \mathbb{R} | f(x) < +\infty \}$ has to be an interval and that $u(x)$ has to be in $E$ for almost every $x \in X$ for the left-hand side of the equation not to be $+\infty$ is kinda annoying!","['jensen-inequality', 'analysis', 'probability']"
4357682,Why does $\text{arctanh}(2^{-k})$ approach powers of $2$?,"This is from a piece of verilog code I generated for $\text{arctanh}(2^{-k})$ : localparam bit [31:0][47:0] arctanhTable = {
        48'b100011001001111101010011110101010110100000011000,
        48'b010000010110001010111011111010100000010001010001,
        48'b001000000010101100010010001110010011110101011101,
        48'b000100000000010101011000100010101101001101110101,
        48'b000010000000000010101010110001000100100011010111,
        48'b000001000000000000010101010101100010001000101011,
        48'b000000100000000000000010101010101011000100010001,
        48'b000000010000000000000000010101010101010110001000,
        48'b000000001000000000000000000010101010101010101100,
        48'b000000000100000000000000000000010101010101010101,
        48'b000000000010000000000000000000000010101010101010,
        48'b000000000001000000000000000000000000010101010101,
        48'b000000000000100000000000000000000000000010101010,
        48'b000000000000010000000000000000000000000000010101,
        48'b000000000000001000000000000000000000000000000010,
        48'b000000000000000100000000000000000000000000000000,
        48'b000000000000000010000000000000000000000000000000,
        48'b000000000000000001000000000000000000000000000000,
        48'b000000000000000000100000000000000000000000000000,
        48'b000000000000000000010000000000000000000000000000,
        48'b000000000000000000001000000000000000000000000000,
        48'b000000000000000000000100000000000000000000000000,
        48'b000000000000000000000010000000000000000000000000,
        48'b000000000000000000000001000000000000000000000000,
        48'b000000000000000000000000100000000000000000000000,
        48'b000000000000000000000000010000000000000000000000,
        48'b000000000000000000000000001000000000000000000000,
        48'b000000000000000000000000000100000000000000000000,
        48'b000000000000000000000000000010000000000000000000,
        48'b000000000000000000000000000001000000000000000000,
        48'b000000000000000000000000000000100000000000000000
    }; The $\text{arctanh}(2^{-k})$ values start converging to successive powers of $2$ .  It's apparent that the number of zeroes between the first $1$ and the second $1$ keeps increasing, so the form $\text{arctanh}(2^{-k}) = 2^{-1} + q(k)$ for values of $q(k)$ decreasing much more quickly than values of $2^{-k}$ .  Wolfram Alpha tells me $q(k) = -2^{-k} - \dfrac{\ln(1-2^{-k}) + \ln(1+2^{-k})}{2}$ Why does this divergence happen?  I don't think I can leverage this to cheaply calculate $\text{arctanh}$ , although it does give me an interesting way to compress the table by storing a floating-point $q(k)$ , not that that's actually useful either.","['limits', 'calculus', 'asymptotics', 'hyperbolic-functions']"
4357699,$|W_{t_n}| \to \infty$ a.s. as $n \to \infty$ for Brownian $(W_t)_{t \in \mathbb{R}_+}$ with $\sum_{n=1}^\infty t_n^{-0.5} < \infty$?,"Let $(W_t)_{t \in \mathbb{R}_+}$ be a Brownian motion. Let $(t_n)_{n \in \mathbb{N}} \subset \mathbb{R}_+$ be a sequence of time points, such that $\sum_{n=1}^\infty t_n^{-0.5} < \infty$ (so, for example, the sequences $t_n = n$ or $t_n = n^2 $ are not increasing sufficiently fast, but the sequence $t_n = n^3$ is increasing fast enough). Prove that \begin{align*}
|W_{t_n}| \to + \infty \quad \text{a.s. as } n \to +\infty 
\end{align*} I stumbled across this problem in a relatively unknown stochastic process textbook and I have no idea how to solve it. Thank you in advance for your help.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4357707,Proving $\nu_{t/n} \to \delta_0$ weakly as $n \to \infty$ for convolution semigroup,"A convolution semigroup is a family of probability measures $(\nu_t)_{t \in I}$ on $\mathbb R^d$ with $I \subset [0,\infty)$ and $0 \in I$ , for which $\nu_s * \nu_t = \nu_{s+t}$ for $s,t \in I$ . I want to prove the following (from Achim Klenke's ""Introduction to Probability Theory"", Exercise. 14.4.2): Let $\{\nu_t : t \geq 0\}$ be a convolution semigroup. Show that for $t \geq 0$ , $\nu_{t/n}$ converges weakly to the Dirac measure $\delta_0$ as $n \to \infty$ . Idea 1: Write $\nu_t = \nu_{t/n}^{*n}$ so that $\nu_t((-\epsilon,\epsilon)^d) = \left(\nu_{t/n} * \cdots * \nu_{t/n}\right)((-\epsilon, \epsilon)^d)$ for every $\epsilon > 0$ , and somehow use this to show $\lim_{n \to \infty} \nu_{t/n}(G) = 0$ for every open $G \subset \mathbb R^d$ not containing $1$ , and $=1$ otherwise. But I’m having trouble doing that. I have an inductive formula that for $f : \mathbb R^d \to \mathbb R$ $\nu_{t/n}$ -integrable, $$
\int f \, d\left(\nu_{t/n}^{*n}\right) = \int \cdots \int f(x_1 + \cdots + x_n) \,d\nu_{t/n}(x_n) \cdots d\nu_{t/n}(x_1)
$$ but this doesn't seem to help. Idea 2: Show that if $A \subset \mathbb R^d$ is measurable with $0 \not\in \overline A$ , so that $\delta_0(\partial A) = \delta_0(A) = 0$ , then $\lim_{n \to \infty} \nu_{t/n}(A) = 0$ . For this, I thought I could show that if $\nu_t(A)$ increases as $t$ increases, then $\lim_{n \to \infty} \nu_{t/n}(A)$ exists since $\nu_{t/n}(A)$ decreases with $n$ , and then I could try arriving at a contradiction if $\lim_{n \to \infty} \nu_{t/n}(A) > 0$ . But the claim that $\nu_t(A)$ increases with $t$ is not true in general, e.g. if $\nu_t$ is a Gaussian normal distribution with mean $0$ and standard deviation $t$ . Then for fixed $0 < a < b$ and values of $t$ sufficiently large, $\nu_t((a,b))$ decreases as $t$ increases. Any thoughts on how this result can be proven?","['weak-convergence', 'lebesgue-integral', 'convolution', 'semigroups', 'probability-theory']"
4357712,Higher derivatives of the log-partition function,"I need higher derivatives of the log-partition function $Z(z)=\log \sum_i \exp(z_i)$ , has anyone derived the formula? Looking at concrete values of derivatives up to order 8, evaluated at $z=(1,1,1)$ makes me suspect there's a nice formula in terms of $p_i=\frac{\exp z_i}{Z}$ $$H_2=
\left(
\begin{array}{ccc}
 \frac{2}{9} & -\frac{1}{9} & -\frac{1}{9} \\
 -\frac{1}{9} & \frac{2}{9} & -\frac{1}{9} \\
 -\frac{1}{9} & -\frac{1}{9} & \frac{2}{9} \\
\end{array}
\right)=\text{diag}(p)-pp'
$$ $$H_3=\left[\left(
\begin{array}{ccc}
 \frac{2}{27} & -\frac{1}{27} & -\frac{1}{27} \\
 -\frac{1}{27} & -\frac{1}{27} & \frac{2}{27} \\
 -\frac{1}{27} & \frac{2}{27} & -\frac{1}{27} \\
\end{array}
\right),\left(
\begin{array}{ccc}
 -\frac{1}{27} & -\frac{1}{27} & \frac{2}{27} \\
 -\frac{1}{27} & \frac{2}{27} & -\frac{1}{27} \\
 \frac{2}{27} & -\frac{1}{27} & -\frac{1}{27} \\
\end{array}
\right),\left(
\begin{array}{ccc}
 -\frac{1}{27} & \frac{2}{27} & -\frac{1}{27} \\
 \frac{2}{27} & -\frac{1}{27} & -\frac{1}{27} \\
 -\frac{1}{27} & -\frac{1}{27} & \frac{2}{27} \\
\end{array}\right)
\right]$$ ( crossposted on physics.SE)","['statistics', 'probability', 'statistical-mechanics']"
4357727,Weak convergence and convergence in measure,"Let $\Omega \subset \mathbb{R}^n$ be an open set and let $1 < p < \infty$ .
Consider a sequence $(f_n)_n \subset L^p(\Omega)$ and $f \in L^p(\Omega)$ such that $f_n$ converges to $f$ weakly in $L^p(\Omega)$ .
Let $g_n: \Omega \rightarrow \mathbb{R}$ and $g: \Omega \rightarrow \mathbb{R}$ be measurable functions such that $g_n \rightarrow g$ in measure and $||g_n||_{L^\infty} \le C$ , $\forall n$ and for some constant $C > 0$ .
Prove that $g \in L^\infty (\Omega)$ and $f_n g_n \rightharpoonup fg$ in $L^p(\Omega)$ . $\textbf{Note}$ : $g_n \rightarrow g$ in measure if $\forall \epsilon > 0$ , $\mu\{x \in \Omega : |g_n(x) - g(x)| > \epsilon\} \rightarrow 0$ , where $\mu$ in this case denotes Lebesgue measure. Here is my attempt. Since $g_n \rightarrow g$ in measure, there exists a subsequence $(g_{n_k})_k$ such that $g_{n_k} \rightarrow g$ a.e. in $\Omega$ . It follows that $|g_{n_k}(x)| \rightarrow |g(x)|$ for a.e. $x \in \Omega$ . By hypothesis, $|g_{n_k}(x)|\le ||g_{n_k}||_{L^{\infty}} \le C$ for all $k \in \mathbb{N}$ $\Rightarrow$ $|g(x)| \le C$ for a.e. $x \in \Omega$ . This implies that $g \in L^{\infty}(\Omega)$ . We aim to prove that $f_n g_n \rightharpoonup fg$ in $L^p(\Omega)$ $\Longleftrightarrow$ $\forall \phi \in L^q(\Omega)$ (where $\frac{1}{p} + \frac{1}{q} = 1$ ) , $\int_{\Omega} f_n g_n \phi dx \rightarrow \int_{\Omega} fg \phi dx$ .
First of all, let us check $f_n g_n \in L^p(\Omega)$ . $||f_n g_n||_{L^p}^p = \int_{\Omega} |f_n g_n|^p dx \le ||g_n||_{L^{\infty}}^p ||f_n||_{L^p}^p < \infty$ since, by hypothesis, $f_n \in L^p(\Omega), g_n \in L^\infty(\Omega), \, \forall n$ . (The same argument holds true also for $fg$ ). Note that $f_n g_n - fg = (f_n -f)g_n + f (g_n - g)$ . Then $|\int_{\Omega} f_n g_n \phi dx - \int_{\Omega} fg \phi dx| = |\int_{\Omega} (f_n g_n -fg) \phi dx| \le |\int_{\Omega} (f_n-f) g_n \phi dx| + |\int_{\Omega} f(g_n -g) \phi dx|$ . Let us focus on the first integral on the right hand-side of the expression above. $|\int_{\Omega} (f_n-f) g_n \phi dx| \le ||g_n||_{L^{\infty}} |\int_{\Omega} (f_n-f) \phi dx| \le C |\int_{\Omega} (f_n-f) \phi dx| \rightarrow 0$ , since $f_n \rightharpoonup f $ . Let $\epsilon > 0$ and define $A_{\epsilon} := \{x \in \Omega: |g_n(x) - g(x)| < \epsilon\}$ . Convergence in measure implies that $|\int_{\Omega} f(g_n -g) \phi dx| \le |\int_{A_{\epsilon}} f(g_n - g) \phi dx| + |\int_{\Omega \setminus A_{\epsilon}} f(g_n -g) \phi dx| < \epsilon \int_{A_{\epsilon}} |f| \phi dx + \int_{\Omega \setminus A_{\epsilon}} |f(g_n -g) \phi| dx$ $< \epsilon \int_{\Omega} |f| \phi dx + \int_{\Omega \setminus A_{\epsilon}} |f(g_n -g) \phi| dx$ . The second integral tends to zero for $n \rightarrow \infty$ because $g_n \rightarrow g$ in measure (which means that $\mu(\Omega \setminus A_{\epsilon}) \rightarrow 0$ , the thesis is a consequence of the absolute continuity of the integral), while $\int_{\Omega} |f| \phi dx$ is merely a constant. If anyone could check the above reasoning, it would be greatly appreciated.","['sequence-of-function', 'measure-theory', 'functional-analysis', 'weak-convergence']"
4357744,Integral curves of a vector field not tangent to an embedded submanifold $S$,"I got stuck on a problem. Let $M$ be a smooth $n-$ dimensional manifold and let $S$ be a compact embedded submanifold. Suppose $V$ is nowhere tangent to $S.$ Prove that there exists $\epsilon>0$ such that the flow of $V$ restrict to a smooth embedding $$\Phi:(-\epsilon,\epsilon)\times S \to M.$$ Firstly, by the compactness of $S,$ we find a positive $\epsilon>0$ for which all the integral curves with initial conditions in $S$ are defined on $(-\epsilon,\epsilon).$ The differential of $\Phi$ if I am not mistaken should be of this form: $$(\frac{\partial}{\partial t}, di_{S}),$$ where $i_{S}$ is the inclusion of $S$ in $M,$ practically by definition this should have maximum rank.
The only part I'm not sure is on how to prove that $\Phi$ is open.
Since $S$ is embedded for every point $y \in S$ there is a chart $(U,\varphi)$ in $M$ in which $U\cap S$ is diffeomorphic under $\varphi$ to a set of the form $$\{ x \in \mathbb{R}^{N} : (x^1,\dots,x^{k},\,0, \dots,\,0) \}.$$ Since $V$ is nowhere tangent to $S$ it's not restrictive to suppose the differential of $\varphi$ sends $V$ to the constant vector field $e_{k+1}.$ So the integral curves with initial conditions on $S$ should all be of the form $$\gamma(t)=(x^1,\dots,x^{k},t,\dots,0).$$ So the set $\Phi(S\times (-\epsilon,\epsilon))$ is diffeomorphic to the following relative open set of $\mathbb{R}^n$ $$ \{(x^1,\dots,x^{k},t,\dots,0): ((x^1,\dots,x^{k}) \in \mathbb{R}^{k+1}, \, t \in (-\epsilon,\epsilon) \},$$ so the map $\Phi$ is open on is image and we are done.
Is this reasoning correct? Where should I be more precise?","['submanifold', 'smooth-manifolds', 'differential-geometry']"
