question_id,title,body,tags
2045516,"If $\mathbb{E}\left(g(X)\mid\mathscr{G}\right) = g(Y)$ a.s. for each bounded measurable function $g$, then $X=Y$ a.s.?","I have this problem: Let $(\Omega, \mathscr{F}, \mathbb{P}$ be a probability space, $\mathscr{G}$ be a sub-$\sigma$-algebra. Let $X,Y$ be two real random variables such that for every bounded Borel-measurable function $g: \mathbb{R} \rightarrow \mathbb{R}^+$  we have $$\mathbb{E}\left(g(X)\mid\mathscr{G}\right) = g(Y)  \text{ a.s.}$$ The question is if that implies $X=Y  \text{ a.s}$ . I guess the answer is yes, so I tried to prove that $\{X<Y\}$ is a nullset. If this set is in $\mathscr{G}$ it is obviously a nullset. But otherwise i get stuck. Does anyone have an idea?","['almost-everywhere', 'probability-theory', 'conditional-expectation', 'random-variables']"
2045519,Rolle's Theorem: $f(x)=3-\left\lvert x-3 \right\rvert$,"Determine whether Rolle's Theorem can be applied to the function on the closed interval of $[a,b]$ .  If Rolle's Theorem can be applied, find all values of c in the open interval $(a,b)$ such that $f'(c)=0$ . If Rolle's Theorem can not be applied explain why. $f(x)=3-\left\lvert x-3 \right\rvert$ in the interval of $[0,6]$ I began the problem by finding the derivative, and looking for the critical numbers and got that the critical number was at $x=3$ .  I also saw that the function was not differentiable in the interval of $[0,6]$ How would I word my answer, would I state that the function is not differentiable in the open interval of $(0,6)$ leading to Rolle's Theorem not be applicable in this certain scenario?","['derivatives', 'calculus']"
2045611,Quotients of varieties by finite group which are also flat maps,Let $A$ be a ring with an action of a finite group $G$. Is there a special name for an action that satisfies that $A$ is a projective module over the ring of invariants $A^G$? More generally is there a special name for an action of a finite group $G$ on a projective variety $X$ which satisfies that the quotient map $X \to X/G$ is flat?,"['abstract-algebra', 'representation-theory', 'algebraic-geometry']"
2045683,Integer solutions question simple,"Suppose I have:
$$x_1 + x_2 + x_3 + x_4 = 12$$ and I want to find all solutions $x_i \geq 1$ Well firstly I can give 1 to each x, so that leaves me with:
$$x_1 + x_2 + x_3 + x_4 = 8$$
Then, I can just make 3 dividers, like such:
$$x_1 | x_2 | x_3 | x_4$$
and so my answer is $$\binom{8+3}{3} = \binom{11}{3}$$
My sheet says: $\textit{Theorem}$: The number of ways to distribute $r$ identical objects into $n$ distinct boxes with at least $one$ object in each box is:
$$\binom{r-1}{n-1}$$
As you can see, this formula works, cause $\binom{12-1}{4-1} = \binom{11}{3}$ But I'm not sure where they got this formula, and why it works?","['combinatorics', 'probability']"
2045692,What is the idea to show this property of a function based on its Fourier transform?,"Let $f\in \mathcal{M}(\mathbb{R})$ be a function of moderate decrease such that the Fourier transform $\mathfrak{F}(f)$ is continuous and satisfies: $$\mathfrak{F}(f)(\xi) = O\left(\dfrac{1}{|\xi|^{1+\alpha}}\right), \quad \text{as $|\xi|\to\infty$}$$ where $\alpha \in (0,1)$. I want to show that $f$ satisfies the HÃ¶lder condition of order $\alpha$: $$|f(x+h)-f(x)|\leq M|h|^\alpha, \quad \text{with $M> 0$ and for all $x,h\in \mathbb{R}$}.$$ Now the condition on $\mathfrak{F}(f)$ says that it is continuous and furthermore, there's $K\in \mathbb{R}$ such that if $|\xi|> K$ we have $$\mathfrak{F}(f)(\xi)\leq \dfrac{A}{|\xi|^{1+\alpha}}, \quad \text{for all $|\xi|> K$ and for some $A\in \mathbb{R}$}.$$ Now, I really have no idea on what to do here. The obvious initial step is to write: $$|f(x+h)-f(x)| = \left|\int_{-\infty}^{\infty} \mathfrak{F}(f)(\xi)e^{2\pi i (x+h)\xi}d\xi-\int_{-\infty}^{\infty}\mathfrak{F}(f)(\xi)e^{2\pi i x\xi}d\xi\right|$$ So that we have $$|f(x+h)-f(x)| \leq \int_{-\infty}^{\infty} |\mathfrak{F}(f)(\xi)| | e^{2\pi i h\xi}-1|d\xi.$$ Now one suggestion is to break this integral as follows: $$|f(x+h)-f(x)|\leq \int_{|\xi|\leq 1/|h|} |\mathfrak{F}(f)(\xi)||e^{2\pi ih\xi -1}|d\xi + \int_{|\xi|\geq 1/|h|} |\mathfrak{F}(f)(\xi)||e^{2\pi ih\xi -1}|d\xi. $$ And now I'm completely stuck. First of all, I really have no idea or intuition whatsoever on what has to be done . My only guess would be to use continuity on the compact $[-1/|h|,1/|h|]$ to bound $\mathfrak{F}(f)$ there, but this would not produce the desired $|h|^\alpha$. On the other integral I can't also use the other condition over $\mathfrak{F}(f)$ because the integral is for $|\xi| \geq 1/|h|$ not $|\xi| > K$. Anyway, I'm quite lost here. So how can I show this? But much more importantly than how to prove this , how can I actually arrive at the proof? How can I have the idea on what has to be done? What is the right way to think about this problem, in order to solve it? I'm much more interested here in how should I reason about this, and how can I have the idea of how to prove it.","['real-analysis', 'fourier-analysis', 'problem-solving', 'functional-analysis', 'integration']"
2045708,Find the derivative of $f^{-1}(x)$ at $x=2$ if $f(x)=x^2 + x + \ln x$,I'm fairly confused with this question (or I guess the concept of inverse functions and taking their derivative). I know that the general rule for taking the derivative of an inverse function is: $$f^{-1}{'}(x) = \frac{1}{f'(f^{-1}(x))}$$ But I'm not really sure where to go from here. A nudge in the right direction would be greatly appreciated.,"['derivatives', 'inverse-function', 'calculus']"
2045722,Why is the upper Riemann integral the infimum of all upper sums?,"I was reading the theory of Riemann integration when I cam across the following , If $f$ is bounded on $[a,b]$ , and $P = \{x_0,x_1,x_2.......x_n\}$ is a partition of $[a,b]$ , let $$M_j = \sup_{x_{j-1}\leq x\leq x_j}f(x)$$ The upper sum of f over P is $$S(P) = \sum_{j=1}^{n} M_j(x_j-x_{j-1})$$ and the upper integral of $f$ over $[a,b]$ , denoted by $$\int_{a}^{b^-} f(x)dx$$ is the infimum of all upper sums. The theorem similarly goes on to state the result for lower sums. My doubt is : I do not understand how is $$\int_{a}^{b^-} f(x)dx$$ the infimum of all upper sums. I understand that if we refine the partition P, then the upper sum would decrease, so it may be a lower limit for all the upper sums computed on the refinements of P (but still being the lower limit does not prove that it is the infimum) and what about those partitions for which P itself is the refinement of? 
How do I know that it will be a lower limit for those, let alone a infimum?","['real-analysis', 'riemann-integration']"
2045724,Proving limit using precise limit definition,"I would like to prove that $$
\lim_{x \rightarrow 2}\frac{1}{x^2}=\frac{1}{4}
$$ using the epsilon-delta definition of a limit. I start with the formal definition: For every $\varepsilon$ > 0 there exist a $\delta$ > 0 such that if $|x - a| < \delta$, then $|f(x) - a|$ < $\varepsilon$. So I need to prove that if $|x - 2| < \delta$ then $|\frac{1}{x^2} - \frac{1}{4}| < \varepsilon$ . I start by doing scratch-work to turn the left hand side of $|\frac{1}{x^2} - \frac{1}{4}| < \varepsilon$ into $|x-2|$
$$
|\frac{1}{x^2} - \frac{1}{4}| < \varepsilon
$$
$$
|\frac{4-x^2}{4x^2}| < \varepsilon
$$
$$
|4-x^2| < 4x^2\varepsilon
$$
$$
|(x+2)(x-2)| < 4x^2\varepsilon
$$
$$
|x-2| < \frac{4x^2}{|x+2|}\varepsilon
$$
I would be able to complete the proof if the right-hand side were simply $\frac{\varepsilon}{|x+2|}$, but the $4x^2$ is throwing me off. Normally I would pick: $\delta \leq 1$ then use $|x-2|<\delta=1$, or $-1<x-2<1$ to turn $x-2$ into the denominator with $\varepsilon$, then pick the lowest bound of the inequality, giving me $\delta = min:\{1, \frac{\varepsilon}{<lowest bound>}\}$ I don't know how to handle the extra $4x^2$ however. Any help would be appreciated.","['epsilon-delta', 'proof-writing', 'calculus', 'limits']"
2045783,Prove that if $n \mid 1 + \sum_{k=0}^{n-1} k^{n-1}$ then $n$ is prime.,"How can I show that if $$n \mid 1 + \sum_{k=0}^{n-1} k^{n-1}$$ then $n$ is prime? I was able to prove that $n$ must be squarefree as follows: since $n \mid 1 + \sum_\limits{k=0}^{n-1} k^{n-1}$ then $\sum_\limits{k=0}^{n-1} k^{n-1} \equiv -1 \pmod n.$ Suppose some prime $p \mid n$, and let $q = n/p$. Now $\sum_\limits{k=0}^{n-1} k^{n-1} \equiv -1 \pmod p$, so
$$\sum_{k=0}^{n-1}k^{n-1} = \sum_{m=0}^{q-1}\sum_{k=0}^{p-1}(mp+k)^{n-1} \equiv \sum_{m=0}^{q-1}\sum_{k=0}^{p-1}k^{n-1} = q\sum_{k=0}^{p-1}k^{n-1} \equiv -1 \pmod p.$$
Since $-1$ is a unit in $\mathbb Z/p\mathbb Z$, then so must be $q$, so since $p$ is prime we have $p \nmid q$, and thus $p \mid n$ but $p^2 \nmid n$. I have not been able to show that $n$ must be prime. By reducing exponents using Fermat's Little Theorem, I can show that for every prime $p \mid n$ (again $q = n/p$) we have
$$q \sum_{k=0}^{p-1} k^{q-1} \equiv -1 \pmod p$$
but I am not sure how to deduce (if it is even possible) that for some $p \mid n$ we must have $q = 1$ (which would immediately imply $n = p$).","['number-theory', 'conjectures', 'open-problem']"
2045785,Is mapping from a countable set to an uncountable set never surjective?,"Suppose $f: X \rightarrow \mathbb{R}$, where $X$ is a countable set. Does it mean that it is always not surjective? (Sorry for such a basic question, but we never dealt with this question in elementary real analysis class).","['real-analysis', 'elementary-set-theory']"
2045808,Point Guard Problem Derivative,"I am currently taking calculus course and is in derivative section. I have this problem i currently am struggling a lot on how to solve... A point guard for an NBA team averages 15 free-throw opportunities per
  game. He currently hits 72% of his free throws. As he improves, the
  number of free-throws opportunities decreases by 1 free throw per
  game. and his percentage of free throws made increases by 0.5
  percentage point per game. When his decreasing free throws and
  increasing percentage are taken into account, what is the rate of
  change in the number of free-throw points that this point guard makes
  per game? What does it mean when it says what is the rate of change in the number of free throw point guard makes per game ?","['derivatives', 'calculus']"
2045859,Relationship between properties of linear transformations algebraically and visually,"I learned from 3Blue1Brown's Linear Algebra videos that a 2-D transformation is linear if it follows these rules: lines remain lines without getting curved the origin remains fixed in place grid lines remain parallel and evenly spaced I'm now going through linear algebra from a textbook, which lays out this definition of a linear transformation: T(u+v) = T(u) + T(v) T(cu) = cT(u) I'm wondering, is there a connection between these two ways of thinking of linear transformations? Do the visual ways of seeing 2-D linear transformations correspond to the formal definition when in 2-D?",['linear-algebra']
2045959,To find a singular integral to $(xy^{\prime}-y)^{2}=x^{2}(x^{2}-y^{2})$,I am doing a self study in differential equations and came across the following problem in this post $$(xy^{\prime}-y)^{2}=x^{2}(x^{2}-y^{2})$$ to find singular integral of this ODE. The solution given there isn't helpful at all. Is there a standard method to solve these type of equations. Any help would be great!,['ordinary-differential-equations']
2045983,Prove probability of event i.o. equal to 0 with given conditions,"Suppose events $\left\{A_n\right\}_{n=1}^\infty$ are from a common probability space. And we have $$\lim_{n \rightarrow \infty}P(A_n)=0,\qquad\sum_{n=1}^\infty P(A_nA_{n+1}^c)< \infty$$ Prove: $P(A_n \text{ i.o.})=0$. I think it can be proved using Borel-Cantelli lemma. But not sure how to show $\sum_{n=1}^\infty P(A_n)< \infty$","['borel-cantelli-lemmas', 'real-analysis', 'measure-theory', 'probability-theory']"
2046004,find the maximum of sum of roots without calculus,"A question asks me to evaluate the maximum and minimum value of the following function: $$y=\sqrt{x}+\sqrt{27+x}+\sqrt{13-x}, 0\le x\le13$$ I have tried to compute y using $$x=0,1,2,...,13$$ I believe such maximum occurs at x=9, and the minimum value lies on boundary points, but how can I prove that without calculus? Thanks!","['algebra-precalculus', 'radicals', 'optimization']"
2046007,Converting ODE solution to Bessel function,"I have this second-order differential equation: $$x''(t) + \frac{1}{(\tau + t)}x'(t) + k^2x(t) = 0$$ I want to make the solution to this ODE amenable to a closed form Bessel function, and so a suggested way is to make a change of variables so that we can compare the differential equation above to the transformation equation below: (where this $x$ is analogous to my $t$, and this $y$ is analogous to my $x(t)$) Transformation equation: The goal (or atleast the way I did it for a simple function) was to compare and identify what values the parameters $\alpha, \beta, C, m$ must have so that the form of differential equation is captured by a Bessel function that makes use of these parameters (such as a linear combination of $x^{\alpha}J_m(Cx^{\beta})$ and $x^{\alpha}Y_m(Cx^{\beta})$). This method allowed me to solve a simple equation like the Airy equation. But if I try to do that in this case, the moment I divide the boxed equation on both sides by $x^2$, you get a $\frac{1}{x}$ as the co-efficient for the first-derivative term, which doesn't represent the form of my differential equation's 2nd term (which has $\frac{1}{\tau + t}$ as its coefficient). I am wondering if I am missing something here, or perhaps there's an intermediary step that's required before I can use this method. Ultimately, I just need a solution to that differential equation that is represented as a Bessel function.","['bessel-functions', 'ordinary-differential-equations', 'transformation', 'functions']"
2046043,Definition of submanifold.,"In my textbook, submanifold is defined as follows: If $X$ and $Y$ are both manifolds in $\mathbb{R}^n$ and $Y\subset X,$ then $Y$ is a submanifold of $X.$ I think that the topology of $X$ and $Y$ should not be irrelevant and some sort of condition such as ""the topology of $Y$ is the subspace topology"" is needed. In addition, I think that not only such a topological condition, but also a condition which is relevant to the smooth structures of $X$ and $Y$ is needed. What is the correct definition of submanifold?","['differential-geometry', 'differential-topology']"
2046080,Rewriting $\sin(a+b) = c$,Given $$\sin(a+b) = c$$ Could it be rewritten as $$a = \arcsin(c) - b$$ For all reals $a$ and $b$? Sine over reals isn't one-to-one though. Is the above valid?,['trigonometry']
2046084,Union of Balls around Rationals,"Let $(r_n)_{n \ge 1}$ be an enumeration of the rationals. Consider the union $A := \cup_n (r_n-\frac{1}{n^2},r_n+\frac{1}{n^2})$ . It is unclear a-priori whether $A$ covers the real line, since although the rationals are dense in the reals, the $\frac{1}{n^2}$ 's might shrink too fast. However, using measure theory, it is very easy to see $A$ does not cover much: indeed, $m(A) \le \sum_n m((r_n-\frac{1}{n^2},r_n+\frac{1}{n^2})) = \sum_n \frac{2}{n^2} = \frac{\pi^2}{3}$ . Since this argument relies much on the convergence of $\sum_n \frac{1}{n^2}$ , I am wondering whether $B := \cup_n (r_n-\frac{1}{n},r_n+\frac{1}{n})$ covers the whole real line, or what portion of it? Does the amount covered depend on the enumeration we choose?","['real-analysis', 'lebesgue-measure', 'measure-theory', 'analysis']"
2046098,Are certain integer functions well-defined modulo different primes necessarily polynomials?,"Call a function $f : \mathbb Z \to \mathbb Z$ consistent if for every prime $p$ and integer $a, b$, when $a \equiv b \pmod p$ then $f(a) \equiv f(b) \pmod p$. The set $C$ of consistent functions is closed under addition, subtraction, composition, translation, and finite difference, and contains all univariate polynomials. Does $C$ contain only univariate polynomials, i.e. $C = \mathbb Z[x]$? My intuition is that this must be the case. Since $f$ is well-defined $\mod p$ for every prime $p$, then I feel that $f$ must be defined based only on ring operations generically, so that the same definition of $f$ (with ring operations) works for any ring $\mathbb Z / p\mathbb Z$. Since the ring operations include only using 0, 1, and the variable $x$, addition, multiplication, that would mean that $f$ must be a polynomial in $x$ with integer coefficients. Is this indeed the case?",['abstract-algebra']
2046123,First order nonlinear differential equation,"I have problem determining the type of this first order differential equation can someone help me how to start solving it?
The equation is:
$$2x^2y\ln(y)y'=y+xy'.$$
Thank you!",['ordinary-differential-equations']
2046163,"Does there exist a normal matrix which is not diagonal, nor Hermitian nor Unitary nor Skew Hermitian?","Well, we know that if a matrix is diagonal, Hermitian, Unitary or Skew-hermitian, then the matrix is Normal. But is the converse true? If not, can anyone give an example?",['linear-algebra']
2046164,Elementary Algebra Problem (in 8th grade),"The exercise is to prove that $$ \forall x \in [0,3] $$ : $$ f(x)=\sqrt{18 + 3x -x^{2}} + \sqrt{9-x^{2}} + \sqrt{9-6x+x^{2}} + \sqrt{9x-3x^{2}} \le 12 $$ I notice that when $$ x=0 \implies f(x) = 3\sqrt{2} + 3 + 3 \le 12 $$ 
and when $$ x=3 \implies f(x) = 3\sqrt{2} \le 12 $$, but how further?",['algebra-precalculus']
2046176,Show $\lim\limits_{n \to \infty}{\sqrt[n]{n}-1 \over {\ln{n} \over n}}=1$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How can I show without the use of derivatives that $$\lim\limits_{n \to \infty}{\sqrt[n]{n}-1 \over {\ln{n} \over n}}=1$$
?","['real-analysis', 'limits']"
2046185,Relation between moduli of curves and TeichmÃ¼ller theory,"Both the theory of moduli of curves and TeichmÃ¼ller theory seem to be concerned with the moduli of Riemann surfaces. However, they appear to belong to different fields within mathematics. Could someone explain in which ways these approaches to studying Riemann surfaces differ and wherein their similarities lie? Also, I have seen that in both theories compactifications appear to play an important role and I would be interested to hear if these are somehow related. I have heard that to compactify, one has to add curves of a different genus. Does this apply to both theories?","['riemann-surfaces', 'algebraic-geometry', 'teichmueller-theory', 'complex-analysis', 'algebraic-curves']"
2046195,About the Moment Generating Function,"Let $X \sim \exp(\lambda)$. Prove that : $E[X^n] = n!/\lambda n$ The hint in (Fundamentals of probability) gives the hint use the Moment Generating Function, but I can't see how I should use that to prove that this is the respective expression that follows.",['probability']
2046219,Finding $ \int^1_0 \frac{\ln(1+x)}{x}dx$,"There is supposed to be a clean solution to the integral below, maybe involving some symmetry $$ \int^1_0 \frac{\ln(1+x)}{x}dx$$ I have tried integration by parts as followed: $\ln(x+1)=u$ , $\frac{1}{x+1} dx = du$ and also $\frac{1}{x}dx=dv$ , $\ln(x)=v$ . Then, the integral becomes $$\int^1_0 \frac{\ln(1+x)}{x}dx=- \int^1_0 \frac{\ln x}{1+x}dx$$ which does not make this easier. I have also tried using the identity $\int_a^bf(x)~dx=\int_a^bf(a+b-x)~dx$ . So let $I = \int^1_0 \frac{\ln(1+x)}{x}dx$ and also $I = \int^1_0 \frac{\ln(2-x)}{1-x}  $ . Then $$2I= \int^1_0 \ln(1+x)+\ln(2-x)$$ which is not any easier, either. Any ideas? :)","['improper-integrals', 'integration', 'definite-integrals']"
2046272,The Law of Cosines in Hadamard manifolds,"Let $M$ be a Hadamard manifold. Let $\Delta(p_1,p_2,p_3)$ be a geodesic triangle and for $i=1,2,3\pmod 3$ , $\gamma_i:[0,\ell_i]\to M$ be the geodesic joining $p_i$ to $p_{i+1}$ where
$\ell_i=\ell(\gamma_i)$. If $\alpha_i$ is the angle between $\dot\gamma_i(0)$ and $-\dot\gamma_{i-1}(\ell_{i-1})$, then how to prove that
$$\alpha_1+\alpha_2+\alpha_3\le\pi\tag{1}$$
and
$$\ell_i^2+\ell_{i+1}^2-2\ell_i\ell_{i+1}\cos(\alpha_{i+1})\le\ell_{i-1}^2\tag{2}$$
I'm looked some books on Riemannian geometry but did not find a clear proof. Some references introduce this as a result of Rauch's comparison theorem but I don't have enough information and experience on Riemannian geometry, while I can understand Rauch's comparison theorem's statement but I don't know how to obtain $(1)$ and $(2)$ from this theorem, so it would be appropriate if someone here guide me and sketch a proof.","['riemannian-geometry', 'differential-topology', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
2046295,Matrix Multiplication and Rank?,"I'm having trouble with this question: Is it possible for $A$ and $B$ to be $3\times3$ rank $2$ matrices with $AB = 0$? Prove your
      answer. I searched the internet and found this: Rank (A) = Rank (AB) So since Rank (A) = Rank (B) = 2 it follows that this is false since Rank (AB) has to be 2 as well? And a 0 matrix won't have a rank? Somehow I feel this is not the correct explanation, and the answer to this question talks about some column space and null space of the matrix but I can't figure out what it means. Any help is greatly apreciated!","['matrices', 'linear-algebra']"
2046362,Consecutive Numbers are coprime,"Can anyone help me in proving that two consecutive numbers are co-primes? My approach : Let two consecutive numbers are $n$ and $n+1$. Assume they are not co-primes. Then $\gcd(n,n+1)=x$, because it can not equal to $1$, $x$ is natural and $x\gt1$ So $x$ divides $n$ as well as $n+1$. Then $x$ also divides $n+1-n$, by general understanding. Hence $x$ divides $1$ or $x=1$. But we have assumed $x\gt 1$. So by contradiction $n$ & $n+1$ are co-prime. Is it right or is there any better way to prove that two consecutive numbers are co-prime?",['number-theory']
2046383,Representability of functors: $\operatorname{Sch} \to \operatorname{Set}$,Suppose that we have a functor $$F:\operatorname{Sch} \to \operatorname{Set}$$ from the categorie of schemes to  the category of sets. And suppose that when we restrict it to $\operatorname{AffSch}$ the category of affine schemes we obtain that this functor is representable. What else do we need to check in order to be able to say that $F$ is itself representable? It seems to me that the representing object that we get when we restrict the functor should tell you the affine pieces you need to glue to form the global representing functor. However I don't know how to make this precise.,"['category-theory', 'sheaf-theory', 'algebraic-geometry']"
2046470,Details in applying the Barr-Beck monadicity theorem to Tannakian reconstruction,"The Barr-Beck monadicity theorem gives necessary and sufficient conditions for a category $\mathcal{C}$ to be equivalent to a category of (co)algebras over a (co)monad. A functor $F:\mathcal{C}\to\mathcal{D}$  is said to be comonadic if it has a right adjoint $G$ and the ""enhanced functor"" $F': \mathcal{C}\to\mathsf{Coalg}_{FG} (\mathcal{D})$ into colagebras over the comonad $FG$ on $\mathcal{D}$ is an equivalence. The comonadic version of Barr-Beck says: A functor $F:\mathcal{C}\to\mathcal{D}$ is comonadic if and only if: F has a right adjoint; F reflects isomorphisms; Every $F$-cosplit pair in $\mathcal{C}$ admits an equaliser in $\mathcal{C}$ and it is preserved by $F$. I have recently learned that this substantially simplifies the proof of Tannaka duality for a (neutral) Tannakian category $\mathcal{C}$ with fiber functor $\omega:\mathcal{C}\to\mathsf{Vecf}_k$ to be equivalent to the category of finite-dimensional representations of an affine group scheme $G$. Namely, the Barr-Beck theorem applied to $\omega$ establishes that $\mathcal{C}$ is equivalent to the category of comodules over a coalgebra $A$. Then the rigid tensor structure on $\mathcal{C}$ can be used to show that $A$ is a commutative Hopf algebra and hence is the coordinate ring of an affine group scheme. I would like some tips towards the Barr-Beck part of this proof. Firstly, note that by definition $\omega$ is exact and faithful, so conditions (2) and (3) in Barr-Beck are satisfied. I am not sure how to easily show condition (1), that $\omega$ has a right adjoint $R$ (adjoint functor theorems seem too general, but maybe I have missed something). Can anyone help with this? Secondly, assume that Barr-Beck applies and that $\omega'$ induces an equivalence $\mathcal{C}\simeq\mathsf{Coalg}_{C} (\mathsf{Vecf}_k)$, where $C = \omega\circ R$ is the comonad induced on $\mathsf{Vecf}_k$ by the adjunction $\omega\dashv R$. Then $A=C(k)$ is a $k$-coalgebra, and I believe that $\mathsf{Coalg}_C (\mathsf{Vecf}_k)$ is equivalent to the category $\mathsf{Comodf}_A$ of finitely-generated comodules over the coalgebra $A$. However, I am not sure how to do this because I don't know whether $k$ can be made into a $C$-coalgebra $a_k: k\to C(k)$. If it can, I would want to define the $A$-comodule structure on a $C$-coalgebra $V$ as $$V\xrightarrow{\sim} k\otimes V\xrightarrow{a_k\otimes \text{id}} A\otimes V,$$ but I don't know if this is correct. Can anyone confirm or correct me on this, and possibly sketch out the details of the equivalence $$\mathsf{Coalg}_C (\mathsf{Vecf}_k) \simeq \mathsf{Comodf}_A$$ that would complete this part of the proof? Thanks for your help!","['category-theory', 'monoidal-categories', 'monads', 'algebraic-geometry']"
2046492,How likely are two events to occur at the same time?,"Let's think of two events $1$ and $2$. Both events happen randomly $n_1$/$n_2$-times during a given time $T$ and last for a time of $t_1$/$t_2$. What is probability $P$, that both events happen simultaneously at some moment? EXAMPLE 1: $T = 60$ min Event $1$ - looking out of the office window: $n_1 = 8$  and $t_1 = 1$ min Event $2$ - a green car is on the street visible:  $n_2 = 20$  and $t_2 = 0.5$ min $P$: How likely do I see a green car during these $60$ min?","['random', 'probability-theory', 'statistics', 'probability', 'stochastic-approximation']"
2046628,If $A$ is independent of $B$ and $C$ then why is it not necessarally independent of $B\cap C$?,"I'm attempting to acquire an intuitive understanding of why the content in the question of the title is correct, however I am unable to do so. Is there way of thinking about the result that makes sense?",['probability']
2046696,"Show that $f(x) = 0$ for all $x \in [a,b]$ given $|f'(x)| \leq C|f(x)| $","Suppose for real numbers $a<b$ one has a function with continuous derivative $f:[a,b] \to \mathbb{R}$ such that $f(a) = 0$ and there exists a real number $C$ with $$|f'(x)| \leq C|f(x)|$$ for all $x \in [a,b]$ . Show that $f(x) = 0$ for all $x \in [a,b]$ . Well, since $f(a) = 0$ , we have that $|f'(a)| \leq C|0|$ , so $|f'(a)| = 0$ . Since $f$ has a continuous derivative, we also know that $f$ is continuous. Since $f$ is continuous on a compact interval, $f$ obtains a maximum, say at $\xi \in [a,b]$ . So, $|f'(x)| \leq C|f(\xi)|$ . Since the derivative is bounded, we obtain that $f$ is Lipschitz, so $f$ is also uniformly continuous. Suppose to the contrary that $f(\xi) > 0$ ?
The above is pretty much everything I could figure out about $f$ , so I'm not sure what to try next. This one is also from an old qual and possibly uses methods from beyond our course. I think maybe I should try to show that $|f'(x)| = 0$ for all $x$ , but I don't know how.","['real-analysis', 'analysis']"
2046704,Find number of solutions of $x_1 \wedge x_2 \oplus x_2 \wedge x_3 \oplus ... \oplus x_{n-1} \wedge x_n = 1$,"I'm trying to find number of solutions of boolean equation $$x_1 \wedge x_2 \oplus x_2 \wedge x_3 \oplus ... \oplus x_{n-1} \wedge x_n = 1$$
where $\oplus$ is Exclusive or and $\wedge$ is Logical conjunction My knowledge of combinatorics is not good enough, so my first idea was to code a program which produces correct output for further investigation. And the output was something like this n = 2   counter: 1
n = 3   counter: 2
n = 4   counter: 6
n = 5   counter: 12
n = 6   counter: 28
n = 7   counter: 56
n = 8   counter: 120
n = 9   counter: 240
n = 10  counter: 496
n = 11  counter: 992
n = 12  counter: 2016
n = 13  counter: 4032
n = 14  counter: 8128
n = 15  counter: 16256
n = 16  counter: 32640
n = 17  counter: 65280
n = 18  counter: 130816
n = 19  counter: 261632
n = 20  counter: 523776 After some googling, I came out with this sequence I'm really interested about how can I solve this problem by some ""counting"" methods. Here's the program I wrote in case you're interested about the output (hope it is correct). Thanks for your help.","['boolean-algebra', 'combinatorics', 'discrete-mathematics']"
2046720,Continuity of $f^{(n)}$ in one point implies n-th differentiability of $f$ in that point?,"Is the following implication true? Let $f:\mathbb{R} \to \mathbb{R}$ be a continous function,
  differentiable in all $\mathbb{R}$, besides at most one point $x_0$. $$f^{(n)}(x) \,\,\mathrm{is \,\,\, continous \,\,\, in \,\,\,} x_0
 \implies f \,\,\,\mathrm{is \,\,\, differentiable \,\,\, in }\,\,\,
 x_0  \,\,\, \mathrm{and} \,\,\, f^{n}(x_0)=\lim_{x \to x_0}f^{n}(x)$$ Where $f^{(n)}$ denotes the n-th derivative of $f$. I know that such theorem is valid for the first derivative $f'$, but is it valid (as stated) in the case of the n-th derivative?","['derivatives', 'real-analysis', 'calculus', 'continuity', 'integration']"
2046742,Find the area of the part of the paraboloid $z = x^2 + y^2$ that lies under the plane $z=4-x$,"So the first thing I did was to try and parametrize the parabloid as: $$r(\theta,z)=\sqrt{z}\cos(\theta)i+\sqrt{z}\sin(\theta)j+zk$$ Then I found $||r_\theta\times r_z||=\frac{1}{2}\sqrt{4z+1}$. Hence the surface area is $\int\int _SdS=\int \int_D \frac{1}{2}\sqrt{4z+1}dA$. Here, $D$ is the region with $0\leq \theta \leq 2\pi$ and $0 \leq z\leq 4-x=4-\sqrt{z}\cos(\theta)$. But Here is my problem, I can't find the upper limit in of z in this integral, the lower limit is 0. But I can't find an upper limit $g(\theta)$. How do I get rid of the $z$ from the limit? Any help would be appreciated I also tried as suggested below to change to a new parametrization but I always go back to the same integral, any hints here?","['calculus', 'multivariable-calculus', 'parametric', 'integration', 'surfaces']"
2046757,How to approach construction of ring homomorphisms?,"As a first year math undergrad I'm taking my first abstract algebra course and I've been struggling with one aspect of the course in particular. How do you approach the construction of certain ring homomorphisms / isomorphisms, in general, given certain restrictions? Given a function, it is easy for me to evaluate whether or not it is a homomorphism. However, when asked, for example: GIVE EXAMPLES OF -commutative rings R, S and a ring homomorphism f : R â S such that Kerf is a maximal ideal, and S is not a field. -commutative rings R, S and a ring homomorphism f : R â S such that Kerf is not a
maximal ideal, and S is a field. I never really know where to start. How do you ""incorporate"" different conditions to find homomorphisms? Is it just a matter of knowing a lot of examples, or is there a more solid approach to these problems? Any and all help is appreciated!","['abstract-algebra', 'ring-theory', 'field-theory', 'ideals']"
2046773,Is the derivative of an integral always continuous?,"Given a function $f$ that is differentiable at a point $x_0$, if we define (using the Riemann integral) $$F(x) = \int_a^x f$$ Can we necessarily say that $F^{\prime}(x)$ is continuous at $x_0$? Going back and forth between $f$ and $F$ confuses me a bit. I think that the Fundamental Theorem of Calculus gives us some relation between $F^{\prime}(x_0)$ and $f(x_0)$, but I'm not sure.","['derivatives', 'real-analysis', 'calculus', 'continuity', 'integration']"
2046836,Prove that $\cos(x+y)=\cos(x)\cos(y)-\sin(x)\sin(y)$ using Cauchy product.,"Show the correctness of the equality by making use of the Cauchy product of the series $\cos(x)=\sum_{n=0}^{\infty}(-1)^n.\frac{x^{2n}}{(2n)!}$ and $\sin(x)=\sum_{n=0}^{\infty}(-1)^n.\frac{x^{2n+1}}{(2n+1)!}$. I've reached $$\cos(x).\cos(y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{n}\frac{1}{(2k)!(2(n-k))!}.x^{2k}.y^{2(n-k)}$$ and $$\sin(x).\sin(y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{n}\frac{1}{(2k+1)!(2(n-k)+1)!}.x^{2k+1}.y^{2(n-k)+1}$$
and $$\cos(x+y)=\sum_{n=0}^{\infty}(-1)^n.\sum_{k=0}^{2n}\frac{1}{k!(2n-k)!}.x^{k}.y^{2n-k}$$ but I guess I'm missing some steps or got something wrong.","['real-analysis', 'trigonometry', 'analysis']"
2046981,Is $f(X)=(2+i)X^4+5X^2-3i$ irreducible over $\mathbb{Z}[i]$?,Is $f(X)=(2+i)X^4+5X^2-3i$ irreducible over $\mathbb{Z}[i]$? I would be pleased if you could tell me how to start. Thanks.,"['irreducible-polynomials', 'abstract-algebra', 'polynomials']"
2046990,How can we evaluate this $\prod_{k=1}^n(1+kx)$,"$\displaystyle\prod_{k=1}^n(1+kx)=\underbrace{\displaystyle\sum_{k=0}^n a_k x^k}_{\text{I assumed this,it don't have to be like this}}$ I'm  investigating what this means, how we can analyse this and get     generalized formula. In fact ,I thought $n-$degree equaliton's formulas. For instance ,assume this $\displaystyle\prod_{k=1}^n(1+kx)=a_0+a_1x+....+a_{n-1}x^{n-1}+a_nx^n$ And I think we know $\displaystyle\sum \left(\dfrac{-1}{k_i}\right)=-\dfrac{a_{n-1}}{a_n}$ It's like , when  ($ ax^2+bx+c=0 $)  , $x_1+x_2=\dfrac{-b}{a}$ And I kept doing this , but this was gonna last to eternity...",['sequences-and-series']
2046996,Subnet vs Cofinal subnet,"I was reading the wikipedia page on subnets , and it says: A more natural definition of a subnet would be to require B to be a cofinal subset of A and that h be the identity map. This concept, known as a cofinal subnet, turns out to be inadequate. For example, the second theorem above fails for the Tychonoff plank if we restrict ourselves to cofinal subnets. I don't see why in the Tychonoff plank a cluster point of a net is not equal to a limit of a subnet. Does anyone know why, or a reference where to find out why this is true?
Thanks.","['general-topology', 'nets', 'convergence-divergence', 'limits']"
2047013,"Is this ""combinatorial"" sum equal to $1$ for every natural $m$?","I did some computations and it seems to me that this holds: $\dfrac {m}{m+1} \cdot \sum_{k=1}^{\infty} \dfrac {1}{m+k \choose m+1}=1$ for every $m \in \mathbb N$. How to prove this beautiful identity, if it is really true?","['binomial-coefficients', 'sequences-and-series']"
2047014,"Lagrange multiplier on unit sphere for $f(x,y,z)=x^2+2y^2+3z^2$","Find the maximum and minimum value of $f(x,y,z)=x^2+2y^2+3z^2$ in the region $D=\{(x,y,z)\in \mathbb R^3| x^2+y^2+z^2=1\}$. And find a unit vector at which the maximum and minimum are attained respectively. Attempt:
I know I need to proceed by Lagrange multiplier method, but I am not sure how to proceed after a step we will get the equations as $$x=\lambda x$$$$y=\frac{\lambda}{2} y$$$$z=\frac{\lambda}{3} z$$$$x^2+y^2+z^2=1$$ Now how to solve them?","['multivariable-calculus', 'lagrange-multiplier', 'optimization', 'calculus']"
2047055,Isomorphic Graph Proof (Degrees of Vertices),"I need to prove that two graphs (say, H and G) which are isomorphic and their vertices have the same degree. So far, I have the summation of the degrees across all vertives is 2|E| and since if two graphs are isomorphic, then they must have the same number of edges. Therefore, each corresponding vertex in H and G must have the same degree. Is this sufficient? Thanks!","['graph-theory', 'discrete-mathematics']"
2047073,the sum of consecutive odd numbers,"If the sum of consecutive odd numbers starting with $-3$ until $2k+1$ equals $21$ What is the value of $k$ ? I can solve this by trying the numbers $-3-1+1+3+5+7+9=21$ , so the last term is $7th$ so the $k$ value is $3$ But I could not solve this with formula, I know the odd numbers come in the form of $2k+1$ but could not get much further.","['algebra-precalculus', 'sequences-and-series']"
2047076,Derivative of Integral with Function in Bound,"I need to evaluate the following derivative in the scope of a PDE class $\frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx$ For some reason I am not able to obtain the right answer using a simple chain rule with the Fundamental theorem of Calculus. Here is what I tried (where $\frac {\partial\mu}{\partial x} =\rho$). Please note the function has a jump discontinuity at $x_s(t)$. $\begin {align}
\frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx &= \frac{d}{dt} [\mu(x_s(t),t)-\mu(a,t)]\\
&= \frac{dx_s}{dt}\frac {\partial\mu}{\partial x}+\frac{\partial\mu}{\partial t}-(\frac{da}{dt}\frac {\partial\mu}{\partial x}+\frac{\partial\mu}{\partial t})\\
&= \frac{dx_s}{dt}\frac {\partial\mu}{\partial x}\\
&= \frac{dx_s}{dt}\rho
\end{align}$ Which is only one term of what I should get. Any hint of where I did something wrong is appreciated, this is making me feel like I forgot calculus... EDIT: Here is what I should get $\frac{d}{dt}\int_a^{x_s(t)} \rho(x,t) \text dx = \int_a^{x_s(t)} \frac{\partial\rho}{\partial t} \text dx + \frac{dx_s}{dt}\rho$","['multivariable-calculus', 'chain-rule', 'partial-differential-equations']"
2047101,"Prove that $A,B$ have a common eigenvector","Let $A,B$ be $2\times2$ real matrices satisfying $\det(A)=\det(B)=1$ and $$\text{tr}(A)>2 , \text{tr}(B)>2, \text{tr}(ABA^{-1}B^{-1})=2$$
Prove that A,B have a common eigenvector.","['matrices', 'linear-algebra', 'soft-question', 'education']"
2047134,Explicit solution of Initial Value Problem for inviscid Burgers equation,"I've been working on solving PDEs with conditions recently and have come across this question: Obtain the explicit solution $u(x,t)$ to the IVP:
  $$u_t+uu_x=2t,   \quad -\infty < x <\infty, \quad t>0,$$
  $$u(x,0)=x, \quad -\infty<x<\infty.$$
  Casting your answer as a function of $x$ and $t$ only. I have had issues getting a solution in just terms of $x$ and $t$, and have been worried that I am misunderstanding how to tackle this. Any guidance is appreciated!","['ordinary-differential-equations', 'initial-value-problems', 'partial-differential-equations']"
2047178,Proving that a limit of a piecewise constant function does not exist using $\epsilon$-$\delta$,"How would one go about proving that the limit of
$$f:[0,1)\cup(1,2]\to\{1,2\}\\
		x\mapsto
		\begin{cases}
			1&\text{if }x\in[0,1)\\
			2&\text{if }x\in(1,2]
		\end{cases}$$
as $x\to 1$ does not exist? I attempted to proceed by contradiction. Suppose $\displaystyle\lim_{x\to 1} f(x)=L$ exists. Then $\forall\,\epsilon>0$, $\exists\,\delta>0$ such that 
$$0 < |x-1|<\delta\Longrightarrow |f(x)-L|<\epsilon$$ Now I know that it is impossible to find an appropriate $\delta$ if we take $\epsilon=\frac{1}{3}$ for example, since the leap $f$ at $x=1$ is of length $1$. So so far I wish to show that $$0 < |x-1|<\delta\Longrightarrow |f(x)-L|<\frac{1}{3}$$
is impossible. But how can I proceed from here? I have to substitute $f(x)$ somehow, but I'm not sure how to go about it. Also, note that I do not wish to involve Left-hand/Right-hand limits at this point (we haven't covered those yet).","['epsilon-delta', 'real-analysis', 'limits']"
2047182,Professor leaves Princeton to go to Stanford and increases the quality of both departments,An old joke is that a certain professor left Princeton to go to Stanford and thereby improved the average quality of both departments. Is this mathematically possible? I found it in statistics course while I am studying. It sounds like ordered statistics. How it is possible? Thanks.,['statistics']
2047202,Geometric intepretation of Holder continuous functions?,"I've started working with Holder spaces recently and I'm wondering how I should think of them intuitively? I really have no idea what a function $f$ that is Holder continuous with exponent $\alpha$ is supposed to look like whereas I do have a good idea for other function spaces. $L^{\infty}$: Say we had a function $f$ such that $\Vert f \Vert_{L^\infty([0, 2])} \le 1$. Then I can visualize $f$ as being some function in the box $[0, 2] \times [-1, 1]$. Lipschitz: If we take another function $g$ and say it is Lipschitz continuous with some fixed constant $C$ I know that the slope of the $g$ will always be less than $C$ at any point in its domain which agrees with the vizualization stated on the Wikipedia page regarding a double cone that can be translated along the graph such that the graph always remains outside the cone. Holder: What is the best way to visualize a Holder continuous function with exponent $\alpha$ on a given domain such as, say, $[0, 5]$? Does such a function have a clear geometric interpretation like $L^{\infty}$ functions? What would be an example of such a function if the exponent was $\alpha = 0.2$ for example? Would a function with an exponent of $\alpha = 0.3$ be 'nicer' than one with $\alpha= 0.2$?","['functional-analysis', 'continuity', 'holder-spaces', 'partial-differential-equations']"
2047216,Proof regarding function giving the length of a curve restricted to an interval,"Consider a parametric curve $\gamma(t): [a,b] \subset \mathbb{R} \to \mathbb{R}^n$. Let the interval $[a,b]$ be divided by a partition $\mathscr{P}=\{ t_1=a, t_2,..,t_{N-1},t_N=b \}$. The lenght of the curve is defined as 
$$L(\gamma):=\sup_{\mathscr{P}} \sum_{i=1}^{N}|\gamma(t_i)-\gamma(t_{i-1})|\tag{1}$$ Where $\mathscr{P}$ may vary among all the possible partition of $[a,b]$. Now consider a function $S(t):[a,b] \to [0,L(\gamma)]$ defined as $$S(t) := L(\gamma |_{[a,t]})$$ ($\gamma |_{[a,t]}$ is the restriction of $\gamma$ to the interval $[a,t]$). Given an $h>0$ how can I prove the following using the definition of lenght given above? $$S(t+h)-S(t)=L(\gamma|_{[t,t+h]}) \tag{2}$$ ($\gamma |_{[t,t+h]}$ is the restriction of $\gamma$ to the interval $[t,t+h]$). Edit : As said  the proof of $(2)$ should make use only of the definition of lenght as given above, that is $(1)$. In particular it should not use the formula 
$$L(\gamma)=\int_{a}^{b} ||\gamma'(t)|| dt$$","['multivariable-calculus', 'calculus']"
2047227,How many ways can 20 different diplomats be assigned to 5 different continenets?,"Well, there are 20 diplomats, and $any$ of the 5 continents can be assigned to them, so this is $5 * 5 * 5 * 5 ... * 5 = 5^{20}$ (any of the five for the first diplomat, any of the five for the second,  etc...) The main question here is: What if each continent needs to have 4 diplomats each? How would I do this then?",['combinatorics']
2047253,If best unbiased estimator exists then it's maximum likelihood estimator?,"Our teacher proved in class that if the best unbiased estimator exists, then it is an MLE using a theorem that if $\hat{\theta}-\theta$ is proportional to the score of $\theta$ with probability $1$ , then $\hat{\theta}$ is the best unbiased estimator since it attains the CR bound. In general, I know that MLE attains the CR bound asymptotically. So I'm in doubt whether the statement in the title holds for a finite sample. Could anyone provide some insight about relationship between the  best unbiased estimator and MLE in finite sample case (and proof)?","['maximum-likelihood', 'statistics', 'estimation', 'parameter-estimation']"
2047299,"Why isn't $\int\sin^2(â¡x)\,dx$ equal to $\frac{\sin^3(x)}3\frac{-1}{\cos(x)}+c$?","Using the trigonometry ""half-angle"" identity $\sin^2(x)=\frac12(1-\cos(2x))$: $$\int\sin^2(â¡x)\,dx
=\int\left(\frac12-\frac12\cos(2x)\right)\,dx
=\frac12x-\frac14\sin(2x)+c$$ I know below method is wrong. But following the basic steps of integral that can be used for other functions such as $(2x+1)^2$, why doesn't this step work?
$$\int\sinâ¡^2(x)\,dx
=\frac{\sin^3(x)}3\frac{-1}{\cos(x)}+c$$","['integration', 'trigonometry']"
2047410,Finding the Units Digit to 7 to the 2945,"How would I go about finding the Units Digit to 7^(2945)? I know that:
7^0 = 1 7^1 = 7 7^2 = 49 7^3 = 343 7^4 = 2401 ... 7^9 = 40353607",['discrete-mathematics']
2047493,"Suppose A, B and C are sets such that #A=#B. Is it true that #(A x C) =#(B x C)? If it is, prove that. [duplicate]","This question already has an answer here : Arithmetics of cardinalities: if $|A|=|C|$ and $|B|=|D|$ then $|A\times B|=|D\times C|$ (1 answer) Closed 7 years ago . I'm sure that this is true, because A and B have the same cardinality (number of elements in the set). So there has to be the same number of ordered pairs with C in both cases. But I'm not sure how to exactly prove it since we just started with cardinality.","['cardinals', 'elementary-set-theory']"
2047500,Prove that $\lim_{n \to \infty}\bigg[\int_0^1f(t)^n \text{dt}\bigg]^{1/n}=M$,"Suppose that $f$ is a continuous, non-negative function on the interval $[0,1]$ . Let $M$ be the maximum of $f$ on the interval. Prove that $$\lim_{n \to \infty}\bigg[\int_0^1f(t)^n \text{dt}\bigg]^{1/n}=M$$ We wrote out some simple examples to show it worked for functions such as $x^2$ . We are having trouble finding how to create a general proof. Thanks for any help!","['riemann-integration', 'analysis', 'limits']"
2047535,Is it possible to have $2^{2015}$ balls in a box according to these rules?,"I have the following exercise, about counting, especially. Exercise Deeds has a big sack of balls and three empty boxes, $A$, $B$ & $C$. He will put the balls on the boxes according to the next rules (in any order, and how many times he wants): (a) He can take out a certain amount of balls of box $A$, and add the same amount of balls, squared,  on box $B$. (b) He can take out a certain amount of balls of box $B$ and add the double of the amount on the box $C$ (c) He can take out all the balls on box $C$ and add that amount on box $A$ and box $B$ (Example, if he had $9$ balls on $C$, he will add $9$ to $A$ and $9$ to $B$, and will remain $0$ on $C$ Initially he has $1$ ball, and he can put it in any box: A) Is it possible to get $2^{2015}$ balls in box $C$, and that the other two boxes remain empty? B) And if the target were $2^{2014}$ balls? What I have so far -It doesn't matter where do you put the first ball, you will always have a pair numbers of balls in box $C$ at second movement. -if we start from the end, the penultimate movement will be $2^{2014}$ on $B$. Next to that you should find a way, i'd appreciate any help! Thanks!","['combinatorics', 'algorithms']"
2047541,Find inverse of multivariable function,"How do I find the inverse of $f(x_{1}, x_{2}, x_{3}) = ( \frac{ x_{1} }{1 + x_{1} + x_{2} + x_{3}} , \frac{ x_{2} }{1 + x_{1} + x_{2} + x_{3}} , \frac{ x_{3} }{1 + x_{1} + x_{2} + x_{3}})$ I already managed to prove that the function is inyective but I have no clue on how to find the inverse. Any help is very appreciated.",['multivariable-calculus']
2047553,Show that there exists no strictly increasing function $f:\mathbb{N}\rightarrow\mathbb{N}$ with $f(2)=3$...,"Full exercise: Show that there exists no strictly increasing function $f:\mathbb{N}\rightarrow\mathbb{N}$ with $f(2)=3$ which has the property that $f(mn)=f(m)f(n)$. This is one of the first exercises in Putnam and Beyond , in the section dedicated to the proof by contradiction. I am quite familiar with the proof technique and feel comfortable with the mechanics of the problem but I find a nice trick here quite elusive. If anyone sees the elephant in the room a bit of subtle guidance would be much appreciated. I have the solution on hand but I would rather not look at it (where's the fun?). If you have any general suggestions that come to mind on how to handle proofs of this nature, especially those involving multiplicative homomorphisms between subsets of $\mathbb{R}$, I am all ears. Many thanks! P.S. I forgot to mention that I have tried using the fact that $f$ increasing implies $f(n+1)>f(n)$ for all $n\in\mathbb{N}$ in several ways to produce a contradiction of the $f(2)=3$ condition. Mainly I used the factorization of $n^2-1$ to get $f(n+1)f(n-1)<f(n)f(n)$ from the inequality $f(n^2)>f(n^2-1)$ but did not find anything very helpful in this approach.","['contest-math', 'real-analysis', 'proof-writing']"
2047611,Combinatorial identity $\sum_{k=0}^{n}\binom{n}{k}\binom{n}{k -1} = \binom{2n}{n+1}$,"LetÂ $n$Â andÂ $k$Â be integers with $1Â \leqÂ kÂ \leqÂ n$.Â Use proof by double counting to show that 
$$\sum_{k=0}^{n}\binom{n}{k}\binom{n}{k -1} = \binom{2n}{n+1}.$$ I have this so far: Suppose we want to form a committee.Â We can choose members from a group ofÂ $n$Â math professors and $n$Â computer science professors, and we must have one more math professor than computer science professors. The committee can be of any size, as long as there is one more math professor than there are computer science professors. LetÂ $k$Â be the number of math professors we choose for the committee.Â There areÂ $\binom{n}{k}$ ways to choose them. Because we must have one more math professor than computer science professors let $k-1$ be the number of computer science professors. There are $\binom{n}{k-1}$ ways to choose them. This gives us $\binom{n}{k}\binom{n}{k-1}$ by the product rule and we sum over all choices of $k$. I get stuck when I try to prove the right side.","['binomial-coefficients', 'combinatorial-proofs', 'combinatorics', 'summation', 'discrete-mathematics']"
2047614,ODE how to check if the kernel is empty,"In class we are working on a type of problems called green function $G$, and we use it to solve second order ODE. What I have an issue is that for problems where the kernel of L is empty we use one approach to get to $G$, and if the kernel is non-empty we have a more complicated way. My problem in particular is that I do not know how to inspect when the kernel is empty. Example. $y''+y=1$ with $y(0)=0$, $y(\frac{\pi}{2})=0$ has an empty kernel Example $y''+y=f(x)$ with $y(0)=0$ $y(\pi)=0$ has a kernel spanned by $\sin(x)$. From what I understand what we do is we take the homogeneous equation and solve with the boundary conditions. Can someone explain to me how we see when the kernel is empty and how we do it in general. Thank you for the time guys","['ordinary-differential-equations', 'linear-algebra']"
2047616,"Find three Poisson-distributed random variables, pairwise independent but not mutually independent","I am asked to give an example of three Poisson-distributed random variables which are pairwise independent, but are not mutually independent. I thought of the example about the Intersection where cars are coming from one side and they can go to one of three directions left, right or keep straight, with probabilities $p \ $, $q \ $ and $1-p-q$, respectively. However, I find it hard to prove.","['independence', 'probability-theory', 'poisson-distribution']"
2047628,What's the variance of an odds ratio?,"I know that 
$$\log(\hat{\psi}) \sim N\left(\log(\psi), \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}\right),$$ so what's the distribution of $\hat{\psi}$? Note that $\displaystyle\hat{\psi} = \frac{n_{11}n_{22}}{n_{12}n_{21}}$ I tried using the $\delta$-method, and found that 
$$\hat{\psi} \sim N\left(\psi, \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}\right).$$
Is this correct?",['statistics']
2047721,how to show $a_{n}=[\frac{(2n)!!}{(2n-1)!!}]^2 \frac{1}{2n+1}$ converges?,"Question:Â $\displaystyle{a_{n} =
\left[{\left(2n\right)!! \over \left(2n - 1\right)!!}\,\right]^{2}
{1 \over 2n + 1}\,,\quad\mbox{prove}\ a_{n}}$ converges. My thought:Â I want to prove {$a_{n}$} is an increasing sequence and it has an upper bound. I've figured out $$a_{n} = (\frac{2\cdot 2 }{1\cdot 3})\cdot(\frac{4\cdot4}{3\cdot5})\dots (\frac{(2n-2)(2n-2)}{(2n-3)(2n-1)})(\frac{(2n)^2}{2n-1})(\frac{1}{2n+1})$$ And, $$\frac{a_{n+1}}{a_{n}}=\frac{(2n)^2}{(2n-1)(2n+1)}\frac{(2n+2)^2}{(2n+1)}\frac{(2n-1)}{(2n)^2}(\frac{2n+1}{2n+3}) = \frac{(2n+2)^2}{(2n+1)(2n+3)} \gt 1$$ So it is increasing. My problem: I'm stuck at proving it has a upper bound. Maybe there are some inequalities that can be used here? In addition, I've searched this question and find this sequence converges to $\pi/2$ and it is known as Wallis Formula .But here, I only want to show it converges and I'm not going to find its limit. Thanks for your time!","['sequences-and-series', 'calculus']"
2047811,Why does the second order delta method approximation to the variance of Bernoulli r.v. result in a negative chi-square?,"Let $Y_1, \ldots, Y_n$ be iid Bernoulli random variables with parameter $p$. Suppose we estimate the variance $Var(Y_1) = p(1-p)$ using $\hat{p}(1-\hat{p})$ where $\hat{p}=\bar{Y}_n$. I am trying to obtain the asymptotic distribution for this estimator specifically in the case where $p=\frac{1}{2}$. In the $p = \frac{1}{2}$ case, the first derivative is zero, hence we use the second order delta method: Suppose that $\sqrt{n}(T_n-\theta_0) \to_D Z$ and that we are interested in $g(T_n)$ where $g'(\theta_0) = 0$ but $g''$ is continuous and nonzero in a neighborhood of $\theta_0$. Then $\sqrt{n}(g(T_n)-g(\theta_0))$ has a degenerate limiting distribution, but: $$
$\sqrt{n}(g(T_n)-g(\theta_0)) \to_D \frac{g''(\theta_0)}{2}Z^2
$$ So, using this I have that for $p =\frac{1}{2}$, $g'(p) = 0$, but $g''(p) = -2 \neq 0$, so: $$
n\left(\hat{p}(1-\hat{p})-\frac{1}{4}\right) \to_D -\frac{1}{4}\chi^2_1
$$ I am wondering why this negative limit makes sense. I read in a footnote somewhere that this makes sense since $p(1-p)$ is maximized at $p=\frac{1}{2}$, but this doesn't really register with me. Can anyone help me here? thanks.","['probability-theory', 'probability', 'statistics']"
2047812,Solutions of $x^3 + (x+4)^2 = y^2$,"I want to solve the above equation in integers. I'm pretty sure the only solutions are $(x,y) = (0, \pm 4)$ but I'm not sure how to prove it.","['number-theory', 'cubics', 'diophantine-equations', 'elementary-number-theory']"
2047817,Spectral decomposition of $L^2(\Gamma \setminus \mathbb{H})$,"Let $\Gamma \leq SL(2,\mathbb{R})$ be a discrete subgoup such that the quotient $\Gamma \setminus \mathbb{H}$ is compact (i.e. $\Gamma$ has a compact fundamental domain in the upper half plane $\mathbb{H} \subset \mathbb{C})$. My question concerns the spectral decomposition of the hyperbolic Laplacian 
$$\Delta=-y^2(\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2})$$
viewed as an operator in the Hilbert space $L^2=L^2(\Gamma \setminus \mathbb{H})$. I am studying some lecture notes about this topic and I found there a theorem which is confusing to me: $\textbf{Theorem.}$ If $\Gamma \setminus \mathbb{H}$ is compact, then there exists an orthonormal basis of $L^2$ consisting of eigenfunctions of $\Delta$. My problem is that $\Delta$ seems not to be defined on the whole space $L^2$ but only for functions in $L^2$ which are at least $C^2$. My question is whether it is a nontrivial part of the theorem that $L^2 \subset C^2$ or whether the formulation of the theorem is just a bit unprecise. (A more precise formulation I have in mind might be something like this: ... there exists a dense subspace of $L^2$ which has an orthonormal basis consisting of $\Delta$-eigenfunctions. I am not sure about a possible candidate for the dense subspace, but some other references I found on the web suggest it might be the space 
$$\{f \in C^{\infty}(\Gamma \setminus \mathbb{H}):f\text{ bounded and }\Delta f\text{ bounded}\}$$ 
where ""bounded"" probably means ""bounded w.r.t. the $L^2$-norm"".) Any help is appreciated.","['automorphic-forms', 'complex-analysis', 'laplacian', 'spectral-theory', 'hyperbolic-geometry']"
2047853,About the Cantor-Schroeder-Bernstein theorem.,If $A$ and $B$ are finite sets it is true that $|A|\leq |B|$ iff there is an injection from $A$ to $B$ and $|A|\geq |B|$ iff there is an surjection from $A$ to $B$. This motivates me to define these inequalities for arbitrary sets. But doesn't this definition makes the Cantor-Schroeder-Bernstein theorem trivial? What is wrong here? I should have to prove the Cantor-Schroeder-Bernstein theorem before defining these inequalities for arbitrary sets? Or they don't make sense at all?,"['elementary-set-theory', 'functions']"
2047859,Algebraists' Intuition on Center and Centralizer,"I would like to ask of some intuitive ways which algebraists use to picture the centers, centralizers and their relation. I understand that the center is the intersection of all centralizers. But this doesn't really help me if I cannot picture the centers and the centralizers individually. Any other hint or an intuitive window through which I can see the two would be helpful I guess. Thanks.","['finite-groups', 'abstract-algebra', 'group-theory']"
2047879,Matrix generated by prime numbers,"Let $p$ be the vector of dimension $n^2$ consisting of ordered prime numbers
i.e. $p= [ 1 \ 2 \ 3 \ 5 \ 7   \ldots]^T$ and $A$ be the matrix of dimension $n\times{n}$ constructed with this vector by the following way: the first  column of $A$ is the first  $n$ prime numbers from 
vector $v$ the second column of $A$ is the next   $n$ prime numbers from 
vector $v$ etc. Examples of such matrices: $\begin{bmatrix}
1 & 3 \\
2 & 5 \\
\end{bmatrix}$, $\begin{bmatrix}
 1 &  5 &  13 \\ 
 2 &  7 &  17   \\
 3  & 11 &  19   
   \end{bmatrix}$, $\begin{bmatrix}
 1 &   7 &  19 &  37 \\ 
 2 &  11 &  23 &  41 \\
 3  & 13 &  29 &  43 \\
 5 &  17  & 31 &  47 
\end{bmatrix}$,  $\dots$ Question: is it true that for any $n$ $ \ $ rank$(A)=n$ (i.e. columns of $A$ are linearly independent) or for some $n$ the statement above is not true?","['matrices', 'number-theory', 'prime-numbers', 'linear-algebra']"
2047890,Value of the integral $\int_0^{2\pi} \log|re^{it}-\zeta| dt$,"I am trying to evaluate the integral
$$ \frac{1}{2\pi}\int_0^{2\pi} \log|re^{it}-\zeta| dt$$
for $\zeta\in\mathbb{C}$. My approach so far has been to first assume $r\leq\zeta$, since this implies the integrand is the real part of a holomorphic branch of the logarithm, hence harmonic, which gives
$$ \frac{1}{2\pi}\int_0^{2\pi} \log|re^{it}-\zeta| dt = \log|\zeta| $$ For the case $r > \zeta$ I would like to conclude
$$\frac{1}{2\pi}\int_0^{2\pi} \log|re^{it}-\zeta| dt =
\frac{1}{2\pi}\int_0^{2\pi} \log|re^{it}| dt = \log r $$
but I don't quite see why the first equality in this equation should be true. Any help and hints are appreciated! :)","['logarithms', 'complex-analysis', 'integration']"
2047947,Prove that $a_n=1+\frac{1}{1!} + \frac{1}{2!} +...+ \frac{1}{n!}$ converges using the Cauchy criterion,"Any tips on how to approach these kind of proof problems when a factorial is included? Here is what I've tried, By the Cauchy criterion the sequence converges if for every $\varepsilon>0$ there exists $N$ such that for all $n\ge N$ satisfies 
  \begin{align}|a_{n+p}-a_n|&=\frac{1}{(n+1)!}+\frac{1}{(n+2)!}+\frac{1}{(n+3)!}+...+\frac{1}{(n+p)!}\\&<\frac{1}{n}+\frac{1}{(n+1)}+\frac{1}{(n+2)}+...+\frac{1}{(n+p-1)}\end{align}
  any ideas on how to continue from here if what i did is correct at all?","['real-analysis', 'cauchy-sequences', 'limits', 'calculus', 'convergence-divergence']"
2048072,What is the meaning of log moment,"In the basic probability and statistics, we are familiar with the meanings of the first moment and the second moment: $$\mathbb{E}[X]\mathrm{: the~average~of~}X$$ $$\mathbb{E}[X^2]-\mathbb{E}[X]^2\mathrm{: the~variance~of~}X$$ However, for the log moment, $$\mathbb{E}[\log X]$$ it is somehow esoteric to figure out its meaning. Actually, I come up with this question because I learned that the maximum entropy distribution subject to the following constraints is the gamma distribution. $$\mathbb{E}[X]=g_1$$ $$\mathbb{E}[\log X]=g_2$$ where $g_1$ and $g_2$ are some constant. Maybe some realization of the random variable $X$ has specific meanings in both of its first moment and log moment so that the gamma distribution can be its maximum entropy distribution. For more information about the maximum entropy distribution, please refers to http://www.mtm.ufsc.br/~taneja/book/node14.html Thanks","['statistics', 'probability', 'probability-distributions']"
2048078,Why keeping a disctinction between almost surely equal elements in probability theory?,"[[This is a rephrasing of this question , intended to be more general.]] One of the fundamental rules of the mathematical language is that one is allowed to give a name only to a unique object. Now, in probability theory, we can consider examples like conditional expectation and conditional probabilities (wrt to a $\sigma$-algebra) that are defined as any random variable that respect some given statements but that are given names $E(X|\mathcal{F})$ and $P(B|\mathcal{F})$. Another example is the RadonâNikodym $\frac{d\mu}{d\nu}$ (although this is more generally a concept of measure theory) and more generally probability theory generates a lot of objects that are almost surely unique . Similarly, many statements are ""only"" almost surely true and it seems to me that the Borel Kolmogorov paradox implies that what happens in a null set can only be consider meaningfull / interesting if one specifies how to reach this null set (so if one specifies sequences / nets / filters of non null sets that in some sense ""converge"" to it). When mathematicians have multiple objects satisfying a given property but do not want to care about this variability, they generally resort to a powerful and amazing concept : quotient set and equivalent classes. This is to my knowledge the canonical tool used every time one wants to forget irrelevant variability. This can also bring unexpected structures out. $L^p$ are only Banach spaces (and $L^2$ is only a Hilbert space) because one does want to make a difference between almost everywhere equal functions. A $S$-valued random element is traditionally described as a measurable function  $X : (\Omega, \sigma_\Omega, P) \mapsto (S, \sigma_S)$. Here I consider with $S$ all type of spaces, including functional spaces. So the definition here is intended to encompasses all types of random variables, stochastic processes, random fields, point processes, random measures, random set, etc. In the end, why hasn't any attempt to simplify the entire theory of probability by considering equivalent classes wherever possible (which seems everywhere) and then defining any kind of $S$-valued random element as an equivalent class of almost surely equal random function on $S$? What could be a fundamental obstacle (mathematically speaking, I'm not considering social traditions) to such an approach, or what relevant information might be lost with it ? It seems to me that most of the current phrasing of probability theory (and I don't even mention statistics) is laborious. (As if contrary to Algebraic geometry for example, it had just not found its Grothendieck yet)","['probability-theory', 'quotient-spaces', 'random']"
2048114,The lebesgue integral equals the Riemann integral,"Suppose that $f$ is integrable on $[a, \infty)$ for some $a \in \mathbb{R}$. Let $\lambda$ denote the restriction of the Lebesgue measure to $[a, \infty)$. Suppose furthermore that $f$ is Riemann integrable over every interval $[a, b]$ with $b > a$. Prove that
$$ \int_{[a, \infty)} f ~ d\lambda = \int_a^{\infty} f(x) ~ dx $$
My attempt: we know that there is an increasing sequence $(u_n)_{n \in \mathbb{N}}$ of step functions that converges uniformly to $f$ (from below) and
$$ \int_{[a, \infty)} f ~ d\lambda = \sup_{n \in \mathbb{N}} \int_{[a, \infty)} u_n ~ d \lambda $$
However, since $u_n$ is a step function on $[a, \infty)$, it assumes only finite different values on $[a, \infty)$, say the values $r_{n, 1}, \ldots, r_{n, k_n}$. Writing $A_{n, i} = u^{-1}(r_{n, i})$, the disjointness of the $A_{n, i}$ yields
$$ \int_{[a, \infty)} f ~ d\lambda = \sup_{n \in \mathbb{N}} \sum_{i = 1}^{k_n} \int_{A_{n, i}} u_n ~ d\lambda = \sup_{n \in \mathbb{N}} \sum_{i = 1}^{k_n} r_{n, i} \cdot \mu(A_{n, i}) $$
But now, I am stuck. What I see is that the right hand side is basically just the interval $[a, \infty)$ split into a finite ($k_n$) amount of pieces and then some lower sum of the integral is computed. Intuitively, this lower sum converges to the integral, but I am unable to prove this. I have also read something about the Lebesgue and Riemann integral of step functions are identical. However, this would only give me
$$ \int_{[a, \infty)} f ~ d\lambda = \sup_{n \in \mathbb{N}} \int_a^{\infty} u_n(x) ~ dx $$
but then again, I am unable to finish the proof. Any help is greatly appreciated, and solutions involving different methods are welcome as well.","['lebesgue-integral', 'measure-theory', 'riemann-integration']"
2048118,How to prove that the series $\sum\limits_{n=1}^\infty \left(\left(n+\frac12\right)\ln\left(1+\frac1n\right) - 1 \right)$ converges,"How would you show that the following series is convergent:
$$\sum_{n=1}^\infty  \left(\left(n+\frac12\right)\ln\left(1+\frac1n\right) - 1 \right)$$ Here is what I have tried so far: I can show that the terms tend to zero, but have been unsuccessful in showing that the series is Cauchy as this seems to amount to just showing that the series as a whole converges (i.e. it does no to simplify the problem) The ratio test does not give an answer The integral test might work, but I have been unable to evaluate the integral Wolfram Alpha suggests the comparison test, so I have tried the following: Comparison with $\frac1{n^2}$ (plotting the graphs, I know that this is a 'correct' comparison) but I cannot get the answer out) Comparison by using the inequality $\ln(1+\frac1n) \le \frac1n$ but this only shows that the series is less than the divergent harmonic series, so has not helped Taylor series expansion for log... though this initially seemed promising, it got a bit messy, and crucially, I wanted to avoid this method as this has not been covered in our analysis course thus far, so the question should not require it I have also tried rewriting it by collecting together the log terms in the sum to get that the series sum is equal to $\lim\limits_{n\to \infty}\ln(\frac{n^n e^{-n}}{n!})$, which seems reminiscent of Stirling's approximation - however, our lecturer said that this could be used to show that Stirling's approximation converges (I can see how to do this part), so I would rather avoid a proof that directly makes use of Stirling's approximation (the implication seems to be that there is a more 'elementary' way to show convergence). I wanted to work out the answer myself, but have got to the point where I feel I am staring at the question but am unable to make much progress.
For this reason, with any solutions, could you possibly include the 'steps'/'clues' that led you to the solution, and how similar questions could be approached. Thanks... Edit: Initially I was a bit wary of using anything involving Taylor polynomials as it is not something that has been remotely touched on in lectures (only the 'obvious' properties of functions such as log have been used/manipulated in inequalities), but now that I think about it, log is (usually) just defined in terms of its series expansion (correct me if I'm wrong here... I know it is also sometimes defined in terms of an integral). Hence, I feel that answers involving Taylor polynomial are good too... nevertheless, given the fact that none of this has been used in any of the other questions or examples from lectures, I am still wondering if there is another way of proving convergence just using inequality manipulation and only 'basic' logarithm inequalities. ... Also, whilst I am very grateful for all answers, I am interested to see how the problem may be solved in different ways so as to give me more tools examples to help solve further problems","['real-analysis', 'inequality', 'sequences-and-series', 'convergence-divergence']"
2048141,Is $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\cdots=\ln 2$ true?,"This might be an extremely stupid question but I stuck on two different definition of series expansion of $\ln(1+x)$. In this Resonance article, the author assert that -
$$1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\cdots=\ln 2$$
based on the series expansion $\ln(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\frac{x^5}{5}-\cdots  $ for $-1<x\leq1$. While the Wikipedia article says $\ln(1+x)=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\frac{x^5}{5}-\cdots  $ for $-1<x<1$. Now these two definition looks confusing to me, as one definition includes the $1$ while other excludes it. If the first definition is true , then the sum of the series is correct, while other definition says it's not. Am I missing something obvious? Please can somebody explain this to me ? Thanks.","['logarithms', 'power-series', 'taylor-expansion', 'sequences-and-series']"
2048194,If $f$ continuous and $[f(x_1)+...+f(x_n)]/n=f([x_1+...+x_n]/n)$ then $f$ linear,"Let $f$ be a continuous function $\mathbb{R}\to 
\mathbb{R}$ such that for all $(x_1,...,x_n) \in \mathbb{R}^\mathbb{N}$, $[f(x_1)+...+f(x_n)]/n=f([x_1+...+x_n]/n)$ I conjecture that $f$ must be linear. I have found a weird geometric proof of this for $n=2$ and $n=3$, but it's not very elegant and I'm not sure if it generalizes easily to higher $n$. Essentially, if $S$ denotes the set of points on the graph of $f$, then for any $n$ points in $S$, their barycenter also lies in $S$; I have shown (for $n=2,3$) that unless $S$ is a line,then $S$ will fail the ""vertical line test"", contradicting the fact that $S$ is the graph of a function . I have also found a cute counterexample for $n=2$ to the problem statement if $f$ is not required to be continuous. Anyways, I'm looking for an easy proof of the problem for general $n$.","['functional-equations', 'real-analysis', 'functions', 'geometry']"
2048219,Intuition for Conditional Expectation,"It seems like NNT aka Nero in The Black Swan (2007) is giving the law of iterated expectations that involve filtrations in a heuristic way by matching the everyday usage of the word 'expect' with the mathematical definition of expectation (a Riemann integral or sum in elementary probability theory; a Lebesgue or Riemann-Stieltjes integral in advanced probability theory). I'm guessing the correspondence between the precise and the heuristic is as follows: Heuristic: $\text{If I expect to expect} \ \color{green}{\text{something}} \ \text{at} \ \color{red}{\text{some date in the future}},$ $\text{then I already expect that} \ \color{green}{\text{something}} \ \text{at} \ \color{purple}{\text{present}}.$ Precise in the case of one non-trivial $\sigma-$ algebra, $$E[E[\color{green}{X}|\color{red}{\mathscr F_t}]] = E[\color{green}{X}|\color{purple}{\mathscr F_0}] (= E[\color{green}{X}])$$ Or Precise in the case of two non-trivial $\sigma-$ algebras, $$E[E[\color{green}{X}|\color{red}{\mathscr F_{t+1}}]|\color{purple}{\mathscr F_t}] = E[\color{green}{X}|\color{purple}{\mathscr F_t}]$$ where $\color{green}{X}$ is a random variable in $(\Omega, \mathscr F, \mathbb P)$ with filtration $\{\mathscr F_t\}_{t\in I}$ where $I \subseteq \mathbb R$ An example I thought of for second case I currently expect to expect tomorrow at 1pm that someone will try to prank me tomorrow at 3pm if and only if I currently expect someone to prank me tomorrow at 3pm Where 3pm refers to the larger $\mathscr F_{.}$ and 1pm refers to the smaller $\mathscr F_{.}$ . 1. Anything wrong? If so, please explain why, and suggest how it may be improved. 2. How to similarly heuristically explain law of iterated expectation when we don't have filtrations? For example $$E[E[\color{green}{X}|\color{blue}{Y}]] = E[\color{green}{X}]$$ $\text{If I expect to expect} \ \color{green}{\text{something}}$ _____ $\color{blue}{(?)}$ _____, $\text{then I (?)expect that} \ \color{green}{\text{something}} $ _____ $(?)$ _____ What I tried: I guess we can consider X as payoff of playing one game out of Y possible games. So the amount we expect to win is equal to the (probabilistically) weighted average of the amounts we expect to win in each of the Y games. But I wanted to use similar language to the one with filtrations so I'm looking for something like If I expect to expect to win 5 dollars (something something) then I expect to win 5 dollars Of course without the something something we have simply $E[E[X]] = E[X]$","['real-analysis', 'probability-theory', 'probability', 'measure-theory', 'conditional-expectation']"
2048269,The origin of the name homological equation,"Let $$\dot{y} = Ay + \cdots \, ,$$ where the dots represent higher order terms in $y$. Make the change of variables $y \mapsto x - h(y)$, where $h$ is a vector valued polynomial of order $r$ in $y$. If the eigenvalues of $A$ are non-resonant, that is, if none of them can be expressed as a linear combination of the others, then the system above can be brought to the form $$ \dot{x} = Ax + v(x) + \cdots \, ,$$ where the dots express terms of order higher than $r$ and $$v(x) = \frac{\partial h(x)}{\partial x} Ax - Ah(x) := L_A h(x) \, .$$ Then equation $L_A h = v$ is called the homological equation associated to $h$ ($L_A$ can be seen as a Lie derivative). The possibility for such a linearisation is called the PoincarÃ© theorem . The context is the one of normal forms for dynamical systems. My question is: I know some objects in mathematics are named sort of randomly, but homological is a very, very specific name. I simply cannot associate the homological equation with homology (the one with classes, exact sequences, cohomology and Betti numbers). Is there a connection or is this name simply unfortunate?","['homology-cohomology', 'lie-derivative', 'ordinary-differential-equations', 'dynamical-systems']"
2048277,Construction of tensor products of Hilbert spaces with different scalar fields.,"Let $H_1$ be a $\mathbb{C}$-Hilbert space and $H_2$ be a $\mathbb{R}$-Hilbert space, both assumed separable. For any $\phi\in H_1,\psi\in H_2$ we may define the (bilinear? by the wikipedia definition this can't be bilinear since $H_1$ and $H_2$ has different scalar fields) mapping $\phi\otimes \psi:H_1\times H_2 \to \mathbb{C}$ by
$$
\phi\otimes \psi(x,y) = \langle x,\phi \rangle_1 \langle y,\psi \rangle_2
$$
We note that since only $H_1$ has a complex inner product, we can in general for $\lambda\in \mathbb{C}$ only say that
$$
\lambda (\phi \otimes \psi) = (\lambda \psi)\otimes \psi \quad \quad \text{and not}\quad \quad  \lambda (\phi \otimes \psi) =  \psi\otimes (\lambda\psi).
$$
Anyways I have proven (I can add the proof, but i think it is correct) that we can create an inner product of the space $\mathcal{E}$ of all finite linear combinations of the bilinear mappings considered above, by letting 
$$
\langle \phi_1 \otimes \psi_1 , \phi_2\otimes \psi_2 \rangle = \langle\phi_1,\phi_2 \rangle_1 \langle\psi_1,\psi_2 \rangle_2
$$
and extending it to finite linear combinations in the following way
$$
\Big\langle \sum_{i=1}^n a_i ( \phi_i\otimes \psi_i), \sum_{i=1}^m b_i (\beta_i \otimes \gamma_i) \Big\rangle  = \sum_{i=1}^n \sum_{j=1}^m a_i \bar{b}_j \langle \phi_i\otimes \psi_i, \beta_j \otimes \gamma_j \rangle
$$
We now define $H_1\otimes H_2$ as the completion of the space of finite linear combinations with respect to the metric induced by the above inner product. Furhtermore it is well-known that the above inner product can be extended to $H_1\otimes H_2$, such that it satisfies
$$
\langle \iota(\phi\otimes \psi), \iota(\gamma\otimes \beta) \rangle_{H_1\otimes H_2} = \langle\phi_1,\gamma\rangle_1 \langle \psi,\beta\rangle_2
$$
for any $\phi,\gamma\in H_1$ and $\psi,\beta\in H_2$, where $\iota$ is the linear isometric embedding into the completion. Question : Is this a valid construction of the tensor product of two Hilbert spaces with different fields? I could not find any mistake, but the ideas are taken from some notes that considers two real Hilbert spaces. Also every source of the tensor product of Hilbert spaces that I have encountered considers either two real or two complex Hilbert spaces, which is why I'm worried I have made a mistake. Edited to reflect only the above question remains.","['tensor-products', 'hilbert-spaces', 'functional-analysis', 'measure-theory', 'analysis']"
2048292,Using the Axiom of Regularity to show $x \in y \in x$ is impossible,"Trying to prove the claim: $$\forall x \; \forall y \; \lnot (x \in y \in x)$$ I know we should apply the axiom of regularity to the set $\{x, y\}$. Can anyone help?","['logic', 'elementary-set-theory']"
2048303,"prove that all lines $ax+y=b$ such that coefficients $a, 1, b$ constitute arithmetic sequence have one common point","Prove that all lines $ax+y=b$ such that coefficients $a,$$1,$$b$ constitute arithmetic sequence have one common point. We know that $1-a=b-1$
and solving for b we get $b=$$2-a$
replacing b in the equation $y = 2-a-ax$ but I am not sure where to go from here that will help me prove my claim.",['algebra-precalculus']
2048313,What is the number of integer solutions of the following equation ? Find the solution without computing 9 combinations.,"The equation is:
$$x_1+x_2+x_3+x_4+x_5+x_6<10$$
with $x_i\geq 0$ for $i=1,2,\dots,6$. What is the number of integer solutions of the following equation ? Find the solution without computing 9 combinations.",['discrete-mathematics']
2048351,What is the intuition behind signed measures?,"I have a rather basic question, and I cannot really find an answer around. What is the intuition behind the introduction of signed measures? I am not referring to the mathematical properties of these objects. Rather, the problem is to get what we capture with them (concrete examples, etc). Any feedback is most welcome. Thank you for your time.","['probability-theory', 'measure-theory']"
2048372,Evaluating a probability density function using polar coordinates,"I'm looking at the following probability density function: $$\sqrt{5+x^2+y^2}$$ where $$ 1 \le x^2+y^2 \le 4 $$ I'm trying to find the $P(0<X<3Y)$ and to do this I've converted this to polar coordinates and am trying to take the integral to get the probability: $$ \int\int \sqrt{1+r^2}*r*drd\theta $$ I'm really unsure how to assign the bounds for this, and if anyone could point me in the right direction I'd appreciate it","['statistics', 'probability']"
2048389,How to find linear function if two points are given?,"Determine the parameterization of $x_1 (t)$ of curve $C_1$ with $tâ [0,1]$ From the picture I see two points $(0,0)$ and $(1,1)$ (red dot). I calculated that function is $y=2t$ but solution is $y=\frac{3}{2}\sqrt{2}t$. Can my solution also be correct or only the the solution from book? P.S. $C_1$ is this linear function on the picture, there is also $C_2$ which curve is from $\frac{\Pi}{4}$ to $\Pi $ but $C_2$ is not important for my question!","['calculus', 'functions']"
2048396,Relationship between Cauchy-Goursat Theorem and conservative vector fields,"In vector calculus we know that if $\,\,\underline{F}\,\,$ is a $\,\,\underline{conservative}\,\,$ vector field, then: $\oint_\gamma{\textbf{F}\cdot d{\textbf{r}}} = 0$, where $\gamma$ is a closed curve. $\nabla\times \textbf{F} = 0$, i.e. the Curl Vanishes $\oint_{\gamma_1}{\textbf{F}\cdot d{\textbf{r}}}= \oint_{\gamma_2}{\textbf{F}\cdot d{\textbf{r}}}$, where $\gamma_i$ starts at $A$ and ends at $B$, Path Independent $\textbf{F} = \nabla\phi$, Existence of Potential, $\phi$ is a scalar field. However in Complex Analysis we know that if $f$ is holomorphic along and inside a simple closed contour $C$, then: $$\int_Cf(z)dz = 0$$ where $z\in\mathbb{C}$. I wanted to know what is the relationship between these two properties in $\mathbb{R}$ and in $\mathbb{C}$. Indeed in both we have a closed path that does not intersect itself (simple). However, in $\mathbb{R}$ we require that the vector field is conservative, whereas in $\mathbb{C}$ we require that the function is holomorphic inside and into the contour. My idea was that the property of the conservative vector field that the curl vanishes, somehow connects to $f$ being holomorphic. Indeed by the sufficiency theorem we know that $f$ is holomorphic if the four partial derivatives $u_x,u_y,v_x,v_y$ exists and are continuous on and in the contour, furthermore the Cauchy Riemann equations hold, i.e. $u_x = v_y$ and $v_x = -u_y$. The Cauchy-Riemann equations look like they could come out of the cross product between $\nabla$ and $\mathbf{F}$ somehow. Even though I couldn't figure it out. I tried to make some kind of function that transforms $f: \mathbb{C}\to\mathbb{C}$ to a function $g:\mathbb{R}^2\to\mathbb{R}^2$, where $u(x,y)\to x'$ and $v(x,y) \to y'$, where $g(x',y') = (u(x,y),v(x,y))$. However I have no idea if what I am doing even makes sense. So I would like to know if you could explain to me how the conditions of a conservative vector field translate to those in $\mathbb{C}$ and vice versa. Indeed in both cases we have that the integral around the path is zero, so there could be some relationship. Please let me know how you would go about showing the link between these two apparently separate properties.","['complex-analysis', 'vector-fields', 'vector-analysis']"
2048410,"In expectation, does conditioning reduce moments beyond the second?","I know that conditioning on a random variable, in expectation, reduces variance. This is a consequence of the law of total variance. Formally, I refer to the following:
$$E[Var(x|y)]=Var(x)-Var[E(x|y)]\le Var(x)$$
where equivalence holds only in the case of independence. 
My question is whether this type of argumentation extends to higher moments. In particular, is it the case that: $$E[(x-E(x|y))^k]\le E[(x-E(x))^k]$$
for $k>2$ and arbitrary distributions of $x$ and $y$? Intuitively, I believe it must hold for $k$ even, but haven't thought of a suitable proof technique. I'm not sure whether or not it will hold for $k$ odd.","['statistics', 'probability', 'probability-distributions']"
2048416,Which would you rather have?,"Q.  Which would you rather have, a piece of an 8-inch pie that's been cut into sixths or a piece of a 10-inch pie that's been cut into eights? A.  This is a problem involving sectors. One-sixth of a pie is $\frac {1}{6}$ of $ 2\pi$ radians. The measure of the central angle is $\frac{1}{6} \cdot 2\pi = \frac{\pi}{3}$. An 8-inch pie has a 4-inch radius. Putting the angle and radius into the formula, for the area of a sector, $A= \frac{1}{2} \cdot \frac{\pi}{3} \cdot 4^2 = \frac{16\pi}{6} = \frac{8\pi}{3} \approx 8.38$ square inches. One-eighth of a pie is $\frac{1}{8}$ of $2\pi$ radians. The measure of the angle is $\frac{1}{8} \cdot 2\pi = \frac{2\pi}{8} = \frac{\pi}{4}$. A 10-inch pie has a 5-inch radius. Putting the angle and radius into the formula, for the area of a sector, $A= \frac{1}{2} \cdot \frac{\pi}{4} \cdot 5^2 = \frac{25\pi}{8} \approx 9.82$ square inches. There isn't too much difference, but it looks like the smaller part of the bigger pie has the larger piece, in terms of area. My question: Is how do one, get, $2\pi$ radians?",['trigonometry']
2048469,How to prove that a function is affine?,"I am trying to understand the concept of affinity of functions. First, I thought that every affine function has to be a linear function, too, because my teacher's notes define linear and affine functions as follows: $$ T(\sum_{i=0}^n \alpha_iu_i) = \sum_{i=0}^n\alpha_iT(u_i) $$
is a linear function. An affine function is defined as $ T(\sum_{i=0}^n \alpha_iu_i) $ with $ \sum_{i=0}^n \alpha_i = 1 $ and the above condition of a linear function. Then, I found the example of $ f(x) = 2x + 3 $ which is an affine function but not linear which is pretty confusing to me (I understand why it is not linear, but have no clue as to why it is affine according to the definitions). I also have to solve a problem such as:
$$ T: \mathbb{R} \to \mathbb{R},  T(x,y,z) := (x â z + 1, y - 5, z - y, 2) $$
but I really have no idea how to proof if it's affine or not. There are no $ \alpha $ and that function is not linear, so I am kind of stuck here. I appreciate any sort of help, like links to websites or anything that helps me to understand this because I have no strategy to solve this problem.","['linear-algebra', 'vectors', 'vector-spaces']"
2048490,Anyone knows a transformation that can lump non-zero values in a matrix together?,"Suppose you have a matrix that contains a lot of zeros (like spare matrix). Is there any known transformation that can gather non-zero values (e.g., into a corner of the matrix)? I am not sure but I think such a transformation is very likely to be non-linear. There is a naÃ¯ve transformation can be used as an example of the intention (however it does not well achieve the above objective). You can count the number of zeros of each row and rearrange rows by those counts in an ascending order. This can be done by multiplying a corresponding permutation matrix on the left. Then you can count the number of zeros of each column and do the same thing -- rearrange the columns by those counts in an ascending order. After rearrangement of both rows and columns, a sparse matrix would approximate a diagonal block matrix, as shown below, with blue color meaning 0. However, I am asking if there is any transformation that can lump most non-zero values into approximately one block , rather than a diagonal block. It would be greatly appreciated if someone could help provide some hint or reference.","['numerical-linear-algebra', 'matrices', 'sparse-matrices', 'numerical-methods', 'linear-algebra']"
2048539,Using the commutator to show unit vectors in polar coordinates are a noncoordinate basis?,"I'm having trouble getting the algebra right here and don't know where I'm going wrong: Show that the unit basis vector fields for polar coordinates in the Euclidean plane, $$\hat{\mathbf{r}} = \cos\theta \hat{\mathbf{x}} + \sin\theta \hat{\mathbf{y}} \\
\hat{\mathbf{\theta}} = -\sin\theta \hat{\mathbf{x}} + \cos\theta \hat{\mathbf{y}}
$$ where $\hat{\mathbf{x}}  = \partial/\partial x $, $\hat{\mathbf{y}}  = \partial/\partial y $, are a noncoordinate basis. I started by noting that $x=r\cos\theta$, $y=r\sin\theta$, therefore $\dfrac{\partial}{\partial x}\cos\theta = \dfrac{1}{r}$ and $\dfrac{\partial}{\partial x}\sin\theta= \dfrac{1}{r}$. Then I wrote out the commutator components: $$ [\hat{\mathbf{r}}, \hat{\mathbf{\theta}}] = \left[r^i {\partial\over\partial x^i}, \theta^j {\partial\over\partial x^j}\right] \\
 = \left(r^i {\partial\theta^j\over\partial x^i}- \theta^i {\partial r^j\over\partial x^i}\right){\partial\over\partial x^j} \\
=-cos\theta{\partial \sin\theta \over \partial x}{\partial  \over \partial x} + \cos\theta{\partial \cos\theta \over \partial x}{\partial  \over \partial y} 
-\sin\theta{\partial \sin\theta \over \partial y}{\partial  \over \partial x}
+ \sin\theta{\partial \cos\theta \over \partial y}{\partial  \over \partial y} \\
+ \sin\theta{\partial cos\theta \over \partial x}{\partial  \over \partial x}
+ \sin\theta{\partial sin\theta \over \partial x}{\partial  \over \partial y}
- \cos\theta{\partial cos\theta \over \partial y}{\partial  \over \partial x}
- \cos\theta{\partial sin\theta \over \partial y}{\partial  \over \partial y} \\
= 0 + \cos\theta {1\over r}{\partial \over \partial y} - \sin\theta {1\over r}{\partial \over \partial x} + 0 + \sin\theta {1\over r}{\partial \over \partial x}  +0+0-\cos\theta {1\over r}{\partial \over \partial y} = 0
$$ but the answer is supposed to be non-zero, so I've included terms that I shouldn't have somewhere.","['multivariable-calculus', 'mathematical-physics', 'differential-geometry']"
2048577,"Why does $\operatorname{null}(A) = \operatorname{null}(A^TA)$, intuitively?","It's easy to show that the nullspace of $A$ and the nullspace of $A^TA$ are the same. But intuitively what does that mean?  Or maybe the better question to ask first is, intuitively how does $A^TA$ relate to $A$?","['matrices', 'intuition', 'linear-algebra']"
2048590,How do we know what the integral of $\sin (x)$ is?,"Since I started more or less formally learning the foundations of calculus, I naturally came across the Riemann definition of the integral. At first I considered this definition intuitive but not really useful since to obtain a closed form expression, one needed to add an infinite amount of values. Later on, an exercise prompted me to calculate a Riemann Integral, and from the definition and the expression for the sum of squares, I was able to calculate the limit with nothing more than I had learned at school. This was a revelation for me, since so far I had considered the definition a mere formalism. Now I knew how it gave results. The next integral I tried to calculate this way was, for obvious reasons $\sqrt {1-x^2}$. Unfortunately, I found the sum intractable and gave up. I started to question the usefulness of the definition again. If it only works for simple functions like polynomials, how did we ever find out that the integral of $\sin (x)$ is $-\cos (x)$? Did we use the Riemann definition or did we just say ""the derivative of $-\cos (x)$ is $\sin (x)$ and therefore its integral must be $-\cos (x)$? I would like to get some insight into the theory as well as the history that led to the tables of integrals we have today","['math-history', 'calculus']"
