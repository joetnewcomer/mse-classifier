question_id,title,body,tags
4542702,How to solve an ODE containing the integral of the unknown [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Is there a way to get an analytical solution for $\psi(y)$ for this second order ODE that contains the integral of $\psi$ over the domain: $$\frac{d^2\psi}{dy^2}=a+b\int^{L}_{0}{\psi dy}$$ where $a$ and $b$ are constants, and with boundary conditions: $\psi(0)=\psi_1$ $\dfrac{d\psi}{dy}(L)=0$ If not, how would you do it numerically? Do Matlab or Maple handle this?","['integration', 'ordinary-differential-equations']"
4542715,Polya's urn martingale proof,"At time $0$ , an urn contains $1$ black ball and $1$ white ball. At each time $1,2,3,...,$ a ball is chosen at random from the urn and is replaced together with a new ball of the same colour. Just after time $n$ , there are therefore $n+2$ balls in the urn, of which $B_n+1$ are black, where $B_n$ is the number of black balls chosen by time $n$ . Let $M_n = \frac{B_n + 1}{(n + 2)}$ , the proportion of black balls in the urn just after time $n$ . Prove that (relative to a natural filtration which you should specify) $M$ is a martingale. There are several ways of solving the problem. One proof starts off by letting $\mathcal{F}_n=\sigma(B_1,...,B_n)$ and then stating $$E[M_n|\mathcal{F}_{n-1}]=P(B_n=B_{n-1})\frac{B_{n-1}+1}{n+2}+P(B_n=B_{n-1}+1)\frac{B_{n-1}+2}{n+2}$$ Intuitively speaking, I agree completely with the equation. It reminds of the basic definition of conditional expectation for discrete rv $X$ given $Y=y$ : $$E(X|Y=y)=\sum_xx\cdot P(x|y)$$ How can one prove the equation rigorously? There are similar problems like De Moivre's martingale ( https://en.wikipedia.org/wiki/Martingale_(probability_theory) ) which make use of the same property of conditional expectation to state: $$E(Y_{n+1}|X_1,...,X_n)=p\left(\frac{q}{p}\right)^{X_n+1}q\left(\frac{q}{p}\right)^{X_n-1}$$ from which it is easy to then show that $(Y_n)_{n\in\mathbb{N}}$ is a martingale.
What is the general formula that I am missing which applies to both of these problems?","['measure-theory', 'statistics', 'conditional-probability', 'martingales', 'probability-theory']"
4542727,How many expected flips before my sausage patties are all face up?,"I was in front of the stove making breakfast sausages this morning and I thought of a problem. If I have 3 patties that need to be flipped, and after every pan flick somewhere between 1 and 3 random patties flip, how many flicks should I expect before all of the patties are flipped to the right orientation? I'm sure this problem has been abstracted for coins or something, but I can't seem to think of how I put them together (especially because AT LEAST 1 patty is always flipped). I see that logically about 1/3 of the time I get it first try (not my personal experience but lets assume I'm a better chef than that). but after that it looks like a Markov chain problem (right?). My programatic trials seem to suggest at mean of around 6, but how would I work this out on pen and paper? Here is my simulation code import random as rd
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
total_trials = 1000000
hist_data = []
for _ in tqdm(range(total_trials)):
    patty_list = [False, False, False]
    num_attempts = 0
    while not all(patty_list):
        num_flips = rd.choice([1, 2, 3])
        rd.shuffle(patty_list)
        for i in range(num_flips):
            patty_list[i] = not patty_list[i]
        num_attempts += 1
    hist_data.append(num_attempts)
    
sns.histplot(hist_data, bins=range(1, 100)).set(title=f'trials: {total_trials:,}\nmean: {sum(hist_data)/len(hist_data)}')
plt.show() resulting in: Edit: I think my code is wrong, as it does not account for the different permutations of flips. TTF = TFT = FTT despite those being 3 scenarios. Does that check out? Edit 2: Does it make sense for me to assume N flips and then assign those flips randomly to pattys? Or do I have to assume that each patty has some probability of flipping? If I do the latter, is it just magic that they never all-not-flip? Edit 3: Thanks all for the thoughtful approach to my breakfast problem. I've learned a lot about the consequences of not defining my problem well! There are a lot of great answers here and since they're all right from a different point of view (I'll define better next time) I'm going to mark the first answer given as the correct. Thanks again all!","['probability-distributions', 'binomial-distribution', 'probability']"
4542734,"Find all bounded infinite sequences of positive integers $(a_n)$ with $a_n = (a_{n-1}+a_{n-2})/(\gcd(a_{n-1},a_{n-2}))$ for $n\ge 2$.","Find all bounded infinite sequences of positive integers $(a_n)$ with $a_n = (a_{n-1}+a_{n-2})/(\gcd(a_{n-1},a_{n-2}))$ for $n\ge 2$ . I think the only possible sequence is the constant sequence with each term equal to $2$ . Let $g_n = \gcd(a_{n},a_{n+1})$ . One can show that $(g_n)$ is eventually constant by proving that it is nonincreasing, since it is bounded below by zero and a convergent sequence of integers is eventually constant (the sequence $g_n$ converges by the Monotone Convergence theorem).  If $d$ divides $a_{n+1}$ and $a_{n+2}$ , then it divides $g_n a_{n+2} - a_{n+1} = a_n,$ so $g_{n+1} | g_n$ , proving the claim.  Clearly one cannot have $g=1$ , since then for all $n$ sufficiently large, $a_n = a_{n-1}+a_{n-2}$ so $(a_n)$ has a strictly increasing, unbounded subsequence. One should also be able to eliminate the case $g\ge 3$ . Now I think once one proves $g=2$ , one can use backwards induction to prove that the sequence $(a_n)$ must be as claimed, though I'm not sure about the details. Here's some additional justification of the third case of the given answer by acreativename: For the third case, I think they're assuming $A_{n} = 2$ for $n\ge k-2$ (even if not, the proof should still work if you instead assume this). We have that for all $n\ge k-2, A_n = 2.$ So in particular, $a_n$ and $a_{n+1}$ cannot both be divisible by $4$ for any $n\ge k-2.$ We have $a_{k+1} = \dfrac{a_{k-1}+a_{k}}2.$ So $a_k$ is even. If $a_k\equiv 0\mod 4,$ then $a_{k-1} + a_k\equiv 0\mod 4$ (as $a_{k+1}$ is even) implies $a_{k-1}\equiv 0\mod 4,$ contradicting $\gcd(a_k, a_{k-1}) = 2.$ Hence $a_k\equiv 2\mod 4,$ and so $a_{k-1}\equiv 2\mod 4.$ But from this point on, I don't know why $a_{k-1}=a_{k-2} = 2$ must hold. Once we show that $a_{k-1} = a_{k-2} = 2,$ we can then conclude the result by backwards induction ( $a_{k-j-1} =a_{k-j} = 2\Rightarrow  a_{k-j-2}=2$ ).","['contest-math', 'number-theory', 'divisibility', 'elementary-number-theory']"
4542776,Ellipse bounded between two lines and a circle,"Given two circles with radii $\beta$ and $\beta^{-1}$ , where $\beta\geq1$ . Also, given two lines $y=x\tan\alpha$ and $y=-x\tan\alpha$ , where $\pi/2>\alpha\geq0$ . I am interested in all ellipses with center at $(k,0)$ that are bound between these two lines and the larger circle (LHS of Figure). I also assume that ellipses are tangent to points $(\beta^{-1}\cos\alpha, \beta^{-1}\sin\alpha)$ and $(\beta^{-1}\cos\alpha, -\beta^{-1}\sin\alpha)$ , and has the following equation: $$\frac{(x-k)^2}{a^2}+\frac{y^2}{b^2}=1.$$ I assume that $b\geq a$ . I found that all such ellipses can be obtained by the following equations: $$a=\sqrt{\frac{k(k\beta-\cos\alpha)}{\beta}}, \quad b=\sqrt{\frac{k\sin^2\alpha}{\beta\cos\alpha}}.$$ So, by changing $k$ in the range $[\beta^{-1}\cos\alpha, \beta^{-1}\frac{1}{\cos\alpha}]$ I can obtain all such ellipses: desmos . However, for some combination of $\beta$ and $\alpha$ , $k$ cannot reach $\beta^{-1}\frac{1}{\cos\alpha}$ , since ellipse will touch larger circle before that (RHS of Figure). Given $\alpha,\beta$ pair, I want to find the largest $k$ allowed, before an ellipse touches the larger circle.","['desmos', 'conic-sections', 'geometry']"
4542796,Give an example of a function $h:\mathbb{R}\backslash\mathbb{Q}\rightarrow\ \mathbb{Q}$,"The question is $h:\mathbb{R}\backslash\mathbb{Q}\rightarrow\ \mathbb{Q}$ so that the
image of $h$ is the same as the codomain of $h$ . I couldn't really think of a function that maps irrational numbers to rational numbers. Can anyone give me some hints?","['irrational-numbers', 'functions', 'rational-numbers']"
4542814,Is it possible that an infinite group has exactly one infinite nontrivial proper subgroup that has a certain order?,"Can there be an infinite group $G$ and a nontrivial proper subgroup $1<H<G$ such that if $1<K<G$ and $|K|=|H|$ , then $K=H$ ? Although I'm particularly interested in when $|H|$ is infinite, some examples for finite $H$ have been given in the comment: $(G,H)=(\mathbb{Z}\times\mathbb{Z}/2\mathbb{Z},\{0\}\times\mathbb{Z}/2\mathbb{Z})$ $(G,H)=(\mathbb{R}^*,\{1,-1\})$ Prüfer groups (I do not know about this) Note: Originally I allowed $K=G$ . I wasn't aware of this distinction but Qiaochu Yuan's answer shows that  if $K=G$ is allowed, it is impossible to find such $(G,H)$ where $H$ is infinite.","['group-theory', 'abstract-algebra', 'infinite-groups', 'examples-counterexamples']"
4542817,Area moment of inertia of trapezium about y-axis,"Good day! As stated in the title, I need to obtain the area moment of inertia, also sometimes called the second moment of area of a trapezium as shown in the figure below, about the y-axis: \begin{align}
I_y&=\int x^2 dA \\
   &=\int_a^b x^2y \ dx + \int_b^c x^2y \ dx +\int_c^d x^2y \ dx \\
   &=\int_a^b x^2 \frac{(x-a)}{(b-a)}h \ dx + \int_b^c x^2 h \ dx +\int_c^d x^2\frac{(x-d)}{(c-d)}h \ dx \\
   &=\frac{h(b-a)(3b^2+2ab+a^2)}{12} + \frac{h(c^3-b^3)}{3} + \frac{h(d-c)(d^2+2cd+3c^2)}{12} \\
   &=\frac{h}{12}[(d^3+c^3)-(a^3+b^3)+ cd(c+d) -ab(a+b)] \tag{1} \\
\end{align} I also referred to this wikipedia page which gives that the second moment of area about the y-axis for any simple polygon assumed to have $n$ vertices, numbered in counter-clockwise fashion can be calculated as: $$I_y =  \frac{1}{12}\sum\limits_{i = 1}^n {(x_i \ y_{i+1} - x_{i+1} \ y_{i})({x_i}^2 + {x_{i+1}}^2} + x_i \ x_{i+1}) $$ where $ \ x_{i}, \ y_{i} \ $ are the coordinates of the $i-th$ polygon vertex, for $1\leq i\leq n$ . Also, $ \ x_{n+1}, \ y_{n+1} \ $ are assumed to be equal to the coordinates of the first vertex, i.e., $ \ x_{n+1}=x_{1} \ $ and $ \ y_{n+1}=y_{1} \ $ . On calculating $I_y$ using above formula for trapezium $ABCD$ as in the figure, I got back the same answer as (1). However a different result shows up here as: $$I_y = h \frac {(a+b)(a^2 + 7b^2))} {48} $$ I am stuck as to what the actual output should be, kindly help. Thanks in advance.","['integration', 'area', 'geometry', 'polygons', 'calculus']"
4542835,Kernel Induced Almost Sure Upper Bound,"Let $\kappa(x,E)\in[0,1]$ for $x\in\mathbb R$ and events $E\subseteq\mathbb R$ be a Markov kernel such that $u(x)=\inf\{u\in\mathbb R:\kappa(x,(-\infty,u])=1\}<\infty$ for all $x\in\mathbb R$ .
Under what conditions is the map $u:\mathbb R\rightarrow\mathbb R$ measurable?
And where can I find references (that also cover more general cases)? Background: For any $X\in\mathbb R$ , and $Y\in\mathbb R$ given by $\kappa$ (cf. the semidirect product on Wikipedia) we have $Y\le u(X)$ almost surely whenever $u$ is well-defined. In this sense $u$ is the canonical (tight) almost sure upper bound for $\kappa$ .",['probability-theory']
4542844,Lyapunov CLT for dependent random variables,"Suppose $\{X_{1},\ldots ,X_{d}\}$ is a sequence of independent random variables, each with finite expected value $ E[X_{i}]$ and variance $ \operatorname{Var}[X_{i}]$ . We define $$s_{d}^2 = \sum_{i=1}^{d} \operatorname{Var}[X_{i}].$$ If for some $\delta >0$ , the following Lyapunov’s condition holds true \begin{align*}
    &\lim_{d \to \infty} \frac{1}{s_{d}^{2+\delta}} \sum_{i=1}^{d} E\left[ |X_{i} - E[X_i]|^{2+\delta}\right] = 0, \qquad \text{then},\\
    &\frac{1}{s_{d}} \sum_{i=1}^{d} (X_{i} - E[X_{i}]) \overset{\mathcal{D}}{\to} \mathcal{N}(0,1),  
\end{align*} as $d$ tends to infinity. Here $\overset{\mathcal{D}}{\to}$ signifies the convergence in distribution. Is there any similar kind of Lyapunov's result that exists for dependent random variables $\{X_1,…, X_d\}$ ? Any help or lead would be highly appreciated.","['statistics', 'probability-distributions', 'probability-theory', 'central-limit-theorem']"
4542870,Independent and identically distributed random variable,"It is my first time learning probability theory, and if I understand correctly, the following is the meaning/motivation behind the definition of a random variable: "" A function's output is uniquely determined by its input. Random variable $f$ is a function defined on a sample space of a random phenomenon. Only after a realisation of the random phenomenon, we can know what is $x$ and hence $f(x).$ Additionaly, if the function is measurable, knowing the distribution of the underlying random phenomenon(on the sample space $\Omega$ ), one can understand the probability distribution of $f(\Omega).$ Thus, the so-called measurable functions are used to model this and are referred to as random variables. Now let $f$ and $g$ two measurable functions defined on a measurable sample space $(\Omega,\mu).$ We say that $f$ and $g$ are independent random variables if the elements of sigma algebra generated by them are mutually independent. What is the motivation behind this definition. ? I understand that by independence of two events, we mean that occurrence of one does not change the  chance of occurrence of others. Also, I am looking for some explicit real-world examples of a few functions that are all defined on the same sample space $\Omega$ to better understand the following: Random variables are independent but not identically distributed. Identically distributed but not independent. Practical ones are appreciated. Thanks in advance.","['probability-theory', 'probability', 'random-variables']"
4542871,"Proving an equation for the number of ""sentences"" using $k$ letters","Suppose there is a language of three words $W_1=a$ , $W_2=ba$ , $W_3=bb$ . Let $N(k)$ be the number of sentences using exactly $k$ letters. Then there is only one sentence with a single letter, $(a)\,$ so $N(1) = 1$ . Furthermore, $N(2) = 3$ since there are three possible sentences with two letters: ( $a.a/ ba/ bb$ ). Also $N(3) = 5$ , with the possible sentences being $(a.a.a/ a.ba/ ba.a/ a.bb/ bb.a)$ . No space is allowed between words. I'm trying to show that $$(*)\quad\quad  N(k) = N(k-1) + 2N(k-2),\,\,k=2,3\ldots,$$ where $N(0)=1$ . I'm going to use a combinatorial argument, but before that, I just want to check $(*)$ is correct for some $k$ . For instance, if $k=4$ , $(*)$ implies $N(4) = N(3)+2N(2)=5+2\cdot 3=11$ , i.e. there are 11 four-letter sentences. However, as far as I can see, the possible four-letter sentences are $$(a.a.a.a/ a.a.ba/ a.ba.a/ ba.a.a/ a.a.bb/ a.bb.a/ bb.a.a/ ba.bb/ bb.ba),$$ 9 and not 11 as $(*)$ predicts. I must be missing something...","['combinatorics-on-words', 'combinatorics']"
4542881,Distance between antipodal points in a geodesic ball,"Let $(M^n,g)$ be a complete Riemannian manifold. For fixed $p \in M$ and small $r > 0$ (smaller than the injectivity radius of $M$ at $p$ ), let $q$ and $q'$ be antipodal points in the sphere $S_R(p)$ of radius $R$ centered at $p$ . This means that if $\gamma : \mathbb{R} \to M$ is the unique unit speed geodesic with $\gamma(0) = p$ and $\gamma(R) = q$ , then $\gamma(-R) = q'$ . My question : is it true that the distance between $q$ and $q'$ is equal to $2 R$ ? Since there is a curve of length $2R$ joining $q$ and $q'$ , we know that $d(q, q') \leq 2R$ . Can it be strictly smaller than $2R$ ?","['geodesic', 'riemannian-geometry', 'geometry', 'smooth-manifolds', 'metric-spaces']"
4542911,Weil restriction of quasi-projective varieties,"I have the following definition of the Weil restriction: For a quasi-projective variety $X$ over a number field $K$ and any $\mathbb{Q}$ -algebra $A$ we set $Res_{K/\mathbb{Q}}X(A):=X(A\otimes_\mathbb{Q}K)$ .
I was wondering how one can see that this gives a quasi-projective variety, lets call it, $Res_{K/\mathbb{Q}}X$ over $\mathbb{Q}$ without using the language of schemes. I was also wondering how one can see that if we take $X=\mathbb{G}_m$ then the weil restriction is a torus over $\mathbb
{Q}$ . I'm just trying to make sense of all these without much knowledge of schemes. Is this possible or am I just waisting my time?","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'algebraic-geometry']"
4542927,Derivation of $\frac{d}{dx} \arccos{\frac{1}{x}}$,"I am trying to derive $\frac{d}{dx} \arccos{\frac{1}{x}}$ and found a (rather longer than necessary) method below. I realise there are more concise ways, but I wanted to check the method below is technically correct, and interpretation of the $\frac{d}{dx}$ and $dx$ variables. Starting with $\cos{y} = \frac{1}{x}$ $y = \arccos{\frac{1}{x}}$ taking the derivative of both sides and expanding $y$ to $\arccos{\cos{y}}$ $\frac{d}{dx} \arccos{(\cos{y})} = \frac{d}{dx} \arccos{\frac{1}{x}}$ $\frac{d}{\cos{y}}\arccos{(\cos{y})} \cdot (-\sin{y}) \cdot \frac{dy}{dx} = \frac{dy}{dx} $ $\frac{d}{\cos{y}} y = \frac{-1}{\sin{y}}$ $\frac{d}{\cos{y}}y = \frac{-1}{\sqrt{1 - \cos^2{y}}}$ $\frac{d\cos{y}}{dx} = \frac{d(\frac{1}{x})}{dx}$ $\frac{d}{\frac{d(\frac{1}{x})}{dx}dx}y = \frac{-1}{\sqrt{1 - \cos^2{y}}}$ $\frac{d}{dx}y \frac{dx}{d(\frac{1}{x})} = \frac{-1}{\sqrt{1 - \cos^2{y}}}$ $\frac{d}{dx}\arccos{\frac{1}{x}} = \frac{-1}{\sqrt{1 - \frac{1}{x^2}}}  \frac{d(\frac{1}{x})}{dx}$ Which is the correct answer. My questions are: Is this valid? For some reason I thought we could not manipulate $\frac{dy}{dx}$ like that, but have since seen it done in my text book. Interpreting $dx$ , this is the arbitary choice of $\delta{x}$ , and when we differentiate we take the limit to 0. But before we differentiate we can read this as some arbitary segment along x. Is this correct? Similar, $\frac{dy}{dx}$ is the change in y w.r.t change in x when we make a small $\delta{x}$ . As such $\frac{dy}{dx}dx$ can be interpreted as $dy$ because we are multiplying $\frac{dy}{dx}$ with are arbitary change in x, giving an arbitary change in y. Is this correct, is there anything further to consider? Thank for your help","['calculus', 'trigonometry']"
4542978,How can I show that the following sigma algebras are equal?,"Let $\Omega$ be a set and $(B_n;~n\in \Bbb{N})$ it's partition. We define $G=\sigma(B_n)=\{\bigsqcup_{n\in L} B_n: L\subset \Bbb{N}\}$ . Consider a random variable $V=\sum_{n\in \Bbb{N}}n1_{B_n}$ where $V:(\Omega,G)\rightarrow (E,\epsilon)$ . I need to show that $\sigma(V)=G$ . I know that by definition $\sigma(V)=\{\Lambda \subset \Omega: V^{-1}(B)=\Lambda~~\text{for some}~~B\in \epsilon\}$ . Now I thought that I can take $\Lambda\in \sigma(V)$ which means that $V^{-1}(B)=\Lambda$ . But then clearly $\Lambda=\bigsqcup_{n\in L} B_n$ for some $L\subset \Bbb{N}$ . Now let me take $C\in G$ which means that $G=\bigsqcup_{n\in L} B_n$ . But then I somehow do not see how to continue to show that $C\in \sigma(V)$ . I think in this direction we need to use the definition of $V$ but I somehow do not see how I should do it. Maybe my idea is also totally wrong. Can maybe someone help me further?","['measure-theory', 'probability-theory', 'probability', 'stochastic-calculus']"
4542989,"centraliser of an element in $\mathrm{GL}(n,\mathbf Z)$","Given a matrix $M \in \mathrm{GL}(n,\mathbf Z)$ , is there a description of what the centraliser of $M$ looks like? Here are some related facts: We know that two matrices commute implying they have at least one
common eigenvector. The power of $M$ is an element in the centraliser of $M$ , but the converse is not true, e.g. take $M$ as the identity matrix. This post gives us a description of all rational    matrices that
commute with $M$ , but not all of such matrices are in $\mathrm{GL}(n,\mathbf Z)$ . Any reference/ideas would be really appreciated.","['matrices', 'group-theory', 'abstract-algebra', 'linear-algebra']"
4542996,Prove that $\lim\limits_{n\to\infty} a_n$ exists.,"Let $a_1,\cdots, a_{2022}$ be real numbers in $(0,1)$ . Define for $n\ge 2023$ , $a_n = (a_{n-1}+\cdots + a_{n-2022})^{1/2023}$ . Prove that $\lim\limits_{n\to\infty} a_n$ exists. Clearly it suffices to show that $\lim\limits_{n\to\infty} a_n^{2023}$ exists since $x\mapsto x^{1/2023}$ is a continuous function (on $\mathbb{R}$ ). I initially thought of using the Cesaro stolz theorem, but it seems hard to prove that the limit $\lim\limits_{n\to\infty} \dfrac{a_{n+1}^{2023}-a_n^{2023}}{1} = \lim\limits_{n\to\infty} a_n - a_{n-2022}$ exists. But  it should be possible to prove it indeed exists and is equal to zero. The Squeeze theorem or some sort of telescoping sum might help. I'm not sure if inequalities like the AM-GM inequality could be useful for this problem.","['contest-math', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4543009,$\limsup (A_{n+1}-A_n)=\limsup (A_{n}-A_{n+1})$?,"Let $\{A_n\}$ be a sequence of sets. Note that "" $-$ "" here means "" $\backslash$ "", the set operator, $A-B=A\cap B^c$ . I think $\limsup (A_{n+1}-A_n)=\limsup (A_{n}-A_{n+1})$ . The reason is that they both equal to $\limsup A_n - \liminf A_n$ . Here is my ""proof"" (please help me check it). First $\limsup A_n - \liminf A_n = \limsup (A_{n+1}-A_n)$ . 1.1 For $x\in \limsup A_n\cap (\liminf A_n)^c=\limsup A_n\cap \limsup A_n^c$ , given any $N$ , there exists $n_1>N$ such that $x\in A_{n_1}^c$ . There exists $n_2>n_1$ such that $x\in A_{n_2}$ . By well-ordering principle, there exists a least $n_2>n_1$ satisfies $x\in A_{n_2}$ . Then $x\in A_{n_2}-A_{n_2-1}$ , which shows $\limsup A_n - \liminf A_n \subset \limsup (A_{n+1}-A_n)$ . 1.2 on the other side, if $x\in \limsup (A_{n+1}-A_n)$ , then for any $N$ , there exists $n>N$ such that $x\in A_{n+1}$ and $x\in A_n^c$ . From the definition, $x\in \limsup A_{n}$ and $x\in \limsup A_n^c=(\liminf A_n)^c$ . Therefore $x\in \limsup A_n - \liminf A_n$ . From 1, take $A_n$ be $A_n^c$ (change the notation), then $\text{LHS}=\limsup A_n^c-\liminf A_n^c=(\liminf A_n)^c-(\limsup A_n)^c=(\liminf A_n)^c\cap (\limsup A_n)=\limsup A_n - \liminf A_n$ , stays the same. While $\text{RHS}=\limsup (A_{n+1}^c-A_{n}^c)=\limsup (A_n-A_{n+1})$ . Since LHS=RHS, we have $\limsup A_n - \liminf A_n=\limsup (A_n-A_{n+1})$ . 2*. The same proof in 1 also could prove 2. Therefore, I think in general $\limsup (A_n-A_{n+1})=\limsup (A_{n+1}-A_n)$ . I am not so sure because I couldn't find anyone state this on the internet (google). Actually the 1. is from this question 1 and I need the latter because of this question 2 . Any ideas or references of the equation might help!! If this is true, could someone prove it directly?",['elementary-set-theory']
4543015,"Does there exist a unique solution for $y' = -y^{1/3}, y(0) = 0$?","The given problem is to determine if the following initial value problems have a unique solution : $y' = y^{\frac13}, y(0) = 0,$ $y' = -y^{\frac13}, y(0) = 0,$ We can not apply Picard's theorem here. For the first one, the answer in this post tells that solutions are infinite. For the second one, I tried to construct a similar function given in the answer but was not successful. Since only the sign is different in the second one than the first one, I feel that the solutions are infinite but unable to prove or disprove them. Any help is higly appreciated.","['initial-value-problems', 'calculus', 'ordinary-differential-equations']"
4543042,"The equation $x^2-3ax+b=0$ does not have distinct real roots, find the least possible value of $\frac{b}{a-2}$, where $a\gt2$.","The following question is taken from JEE practice set. The equation $x^2-3ax+b=0$ does not have distinct real roots, find the least possible value of $\frac{b}{a-2}$ , where $a\gt2$ . My Attempt: $$D\le0\\ \implies 9a^2-4b\le0\\ \implies9a^2-36\le4b-36\\ \implies9(a-2)(a+2)\le4(b-9)$$ Now, $a+2\gt4\implies 9(a-2)(a+2)\gt36(a-2)$ Also, $b\gt b-9\implies4b\gt4(b-9)$ Thus, $4b\gt9(a-2)+a+2)\gt36(a-2)$ Therefore, $\frac{b}{a-2}\gt9$ The answer given is $18$ . How to do this?","['contest-math', 'algebra-precalculus', 'quadratics', 'inequality']"
4543087,Arithmetic-Geometric limit $\lim\limits_{n\to\infty} x_n = \lim\limits_{n\to\infty} y_n$,"For $x,y>0$ , define two sequences $(x_n)$ and $(y_n)$ by $x_1=x,y_1=y$ and $x_{n+1}=(x_n+y_n)/2$ and $y_{n+1}=\sqrt{x_ny_n}$ . Prove that $\lim\limits_{n\to\infty} x_n = \lim\limits_{n\to\infty} y_n= \dfrac{\pi}{\int_0^\pi \dfrac{d\theta}{\sqrt{x^2 \cos^2\theta + y^2\sin^2\theta}}}.$ I think it might be easier to prove $\lim\limits_{n\to\infty} x_n = \lim\limits_{n\to\infty} y_n.$ Let the LHS of this equation be denoted $L$ and let the RHS be denoted $M$ . By the AM-GM inequality and induction, $x_{n}\leq y_n$ for all $n\ge 2$ . It could be useful to define a new sequence with a limit that's easier to evaluate. We have $L\leq M$ by limit properties, so we just need to show $L\ge M$ to get $L=M$ . Suppose for a contradiction that $L < M.$ Then by definition, there exists $N$ so that for all $n\ge N, x_n < \frac{L+M}2$ and $y_n > \dfrac{L+M}2.$ How can I proceed from here? As for showing it equals an expression involving a given integral, I think the integral is actually fairly hard to compute explicitly, so one should use some properties of the sequences to show the desired equality.","['integration', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4543103,"Spivak, Chapter 20, Problem 11: Using Taylor's Theorem Instead of L'Hôpital´s Rule to compute limit.","The following is a problem from Chapter 20 of Spivak's Calculus Calculations of this sort may be used to evaluate limits that we might otherwise try to find through laborious use of l'Hopital's Rule.
Find the following (a) $\lim\limits_{x\to 0}
 \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0}
 \frac{N(x)}{D(x)}$ Hint: First find $P_{3,0,N}(x)$ and $P_{3,0,D}(x)$ for the numerator
and denominator $N(x)$ and $D(x)$ . ""Calculations of this sort"" refers to the previous problem $10$ in which we compute Taylor polynomials of functions $f+g$ , $f\cdot g$ , and $f\circ g$ , using formulas derived in problem $9$ . Here is the solution manual solution $$\lim\limits_{x\to 0} \frac{N(x)}{D(x)}=\lim\limits_{x\to
 0}\frac{\frac{1}{6}x^3+R_{3,0,N}(x)}{\frac{1}{6}x^3+R_{3,0,D}(x)}=\lim\limits_{x\to
 0}\frac{\frac{1}{6}+\frac{R_{3,0,N}(x)}{x^3}}{\frac{1}{6}+\frac{R_{3,0,D}(x)}{x^3}}=1$$ since the limits involving $R$ terms are $0$ My question is about the intermediate steps in this proof. Here is my attempt at writing them out. Using Taylor's Theorem, we can easily see that $$N(x)=P_{3,0,N}(x)+R_{3,0,N}(x)$$ $$=\frac{x^3}{3!}+\frac{e^t}{4!}x^4, t\in (0,x)$$ $$D(x)=P_{3,0,D}(x)+R_{3,0,N}(x)$$ $$=\frac{x^3}{3!}+\frac{(-\sin{u})}{4!}x^4,u\in (0,x)$$ Therefore, we have $$\lim\limits_{x\to 0}
 \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0}
 \frac{N(x)}{D(x)}$$ $$=\lim\limits_{x\to 0} \frac{\frac{x^3}{3!}+\frac{e^t}{4!}x^4}{\frac{x^3}{3!}+\frac{(-\sin{u})}{4!}x^4}$$ $$=\lim\limits_{x\to 0} \frac{\frac{1}{6}+\frac{e^t x}{4!}}{\frac{1}{6}-\frac{x\sin{(u)}}{4!}}$$ $$=1$$ Are these intermediate steps correct? Seems like this was all a lot more work than applying L'Hopital directly to the original limit. $$\lim\limits_{x\to 0} \frac{e^x-1-x-\frac{1}{2}x^2}{x-\sin{x}}=\lim\limits_{x\to 0} \frac{e^x-1}{\sin{x}}=\lim\limits_{x\to 0} \frac{e^x}{\cos{x}}=1$$","['integration', 'calculus', 'solution-verification', 'taylor-expansion', 'derivatives']"
4543113,Is $\|\cdot\|_{\pi} = \|\cdot\|$?,"Let $\ell^{1}(\mathbb{N})$ be the space of complex-valued sequences such as: $$\|a\|_{\ell^{1}} = \sum_{n\in \mathbb{N}}|a_{n}| < +\infty$$ and set $\ell^{1}(\mathbb{R})\otimes \ell^{1}(\mathbb{R})$ to be the set of all finite sums of sequences of the form $a\otimes b = \{(a\otimes b)_{n,m}\}_{n,m \in \mathbb{N}}$ , with $(a\otimes b)_{n,m} = a_{n}b_{m}$ . If $c = \sum_{k}a_{k}\otimes b_{k}$ is an element of this space, define: $$\|c\| := \sum_{n,m}|\sum_{k}(a_{k}\otimes b_{k})_{n,m}|$$ In addition, define: $$\|c\|_{\pi} := \inf \sum_{k}\|a\|_{\ell^{1}}\|b\|_{\ell^{1}}$$ where the infimum is over all possible representations of $c$ as a sum of finite elements of $a_{k}\otimes b_{k}$ . Do $\|\cdot\|$ and $\|\cdot\|_{\pi}$ agree? I proved $\|\cdot\| \le \|\cdot\|_{\pi}$ but not the converse. I believe the $\|\cdot\|_{\pi}$ is called projective norm in the literature.","['normed-spaces', 'functional-analysis', 'analysis', 'sequences-and-series']"
4543117,How can I find/plot $f(t)$ if I only have $f(t+\Delta t)$ and $f(0)$?,"So, I am working on a game, and I need to come up with some math formulas for a proof of concept (and tweak them to make sure it's balanced). In particular, I have a quantity $y$ that should decrease over time with a specific pattern. As is often the case in games, this quantity will be stored in a variable, and the new value of this variable will be re-computed every frame starting from the current value and the time passed since last frame $\Delta t$ . Through trial and error, I came up with this formula to compute the new value of $y$ : $$
y_{new}=\frac{y_{old}}{((k)^{y_{old}})^{\Delta t}}
$$ where $k$ is a constant parameter such that $k>1$ , $y$ will always be $>1$ (meaning that if it goes below 1 I will discard the value and consider $y=1$ instead), and $\Delta t$ is a value in seconds always $>0$ .
Now, this ""formula"" is okay from a coding perspective, it's ready to be implemented, but I would like to be able to plot it on Geogebra or Wolfram Alpha in order to analyze and tweak it as needed, and for that I need to transform it in a ""traditional"" function $y=f(t, y_0)$ .
How do I do that? I don't even know what to Google (I guess it's something in the realm of differential equations?), so it would be nice if you could explain the reasoning/point me to a general explanation so I can further research the topic, as I expect that I will face similar problems multiple times over the course of the development.","['functions', 'graphing-functions']"
4543121,Partition of a family of sets with lower bound on cardinality,"Let $X_1, \ldots, X_n$ be sets such that $|\cup_{i = 1}^n X_i| = \sum_{i = 1}^n s_i$ and $|X_i| \ge s_i$ . Is there, in general, a partition $P_1,\ldots,P_n$ such that $P_i \subseteq X_i$ and $|P_i| = s_i$ for each $i$ ?","['elementary-set-theory', 'combinatorics']"
4543126,Simple combinatorics problem - assigning people to groups,"$20$ people that A, B and C belong to are to be randomly seperated in groups of 4. I have a few questions about this problem: Q1: What is the probability, that A, B and C place in the same group? My intuition would be, that the person A has a $4/20=1/5$ chance to land in group 1 and then person B has a $3/19$ chance to land in the same group and finally person C has a $2/18$ chance to do the same. Because there are $5$ groups, this means that $\mathbb{P}(Q1)=5\frac{1}{5}\frac{3}{19}\frac{2}{18}=\frac{1}{57}$ . Q2: What is the probability that they all are placed in different groups? Place person A in group $a$ (probability $1/5$ ). Then place person B in group $b\neq a$ - probability $4/19$ . Finally place person C in group $c\notin\{a,b\}$ - probability $4/18$ . There are $\binom{5}{3}$ ways to choose $a,b,c$ , so $\mathbb{P}(Q2)=\binom{5}{3}\frac{1}{5}\frac{4}{19}\frac{4}{18}$ This somehow seems wrong to me, because it does not account for the other $17$ people. Q3: What is the conditional probability, that A,B,C land in different groups if it is already known that two of them are in different groups? This one I really struggle with. My attempt: Because there are three groups to choose from and $18$ people to match, I think that $\mathbb{P}(Q3)=3\frac{4}{18}$ . Please check my solutions and tell me what is wrong. EDIT: Thank you to everyone for the discussion and solutions, especially to @chrslg for even throwing together a little simulation. Also, Q3 is for sure related to conditional probability, so $\mathbb{P}(Q3)=\frac{\mathbb{P}(Q1)}{1-\mathbb{P}(Q2)}$ , as pointed out by you all.","['conditional-probability', 'solution-verification', 'combinatorics', 'probability']"
4543148,Bounded variation on $\mathbb{R}$,"We define for $v \in L^1(\mathbb{R})$ $$
Var_{\mathbb{R}}(v) := \mathrm{sup}\left \lbrace\int_{\mathbb{R}} u(x)g'(x)~\mathrm{d}x: g \in C_0^\infty(\mathbb{R}),~ \lvert g(x) \rvert \leq 1\text{ for all }x \in \mathbb{R} \right \rbrace.
$$ If $u \in L^1(\mathbb{R}) \cap C^1(\mathbb{R})$ and $Var_{\mathbb{R}}(u) < \infty$ , is it true that $$
\int_{\mathbb{R}} \lvert v'(x) \rvert~\mathrm{d}x < \infty \quad ?
$$ I am sure that it is, but I can not quite find a rigorous proof. I also know that this holds for bounded domains. I also know that $$
\int_{\mathbb{R}} v(x) g'(x)~\mathrm{d}x = - \int_{\mathbb{R}} v'(x) g(x)~\mathrm{d}x.
$$ Maybe approximating $-\mathrm{sign}(u')$ could help, but I do not see how. Mabye Riesz Representation Theorem can be helpful... I am grateful for help or a reference. I have found some posts on SE that could not really help me. Maybe it is not even true...","['reference-request', 'bounded-variation', 'real-analysis', 'lp-spaces', 'functional-analysis']"
4543154,Inverse function theorem: Lemma 5 from Terence Tao's blog,"I'm trying to understand the proof of Lemma 5 from Terence Tao's blog , i.e., Let $X$ be an open subset of $\mathbb R^n$ and $f:X \to \mathbb R^n$ be differentiable such that $\partial f (x)$ is invertible for all $x\in X$ . Fix $x_0 \in X$ and $y_0 := f(x_0)$ and let $$
K := \{x \in X: f(x)=y_0\}.
$$ Lemma 5: Let $H$ be the connected component of $K$ that contains $x_0$ . Then $H = \{x_0\}$ . Could you have a check on my attempt? Proof: Assume the contrary that $H \neq \{x_0\}$ . Then there is a path $\gamma:[0,1] \to H$ such that $\gamma(0) = x_0$ and $\gamma(1) \in H \setminus \{x_0\}$ . We have $f \circ \gamma = y_0$ . Then $$
\begin{align}
\lim_{t \to 0^+} \frac{f \circ \gamma (t) - f(x_0) - \partial f (x_0)(\gamma(t)-x_0)}{|\gamma(t)-x_0|} &= -\lim_{t \to 0^+} \frac{ \partial f (x_0)(\gamma(t)-x_0)}{|\gamma(t)-x_0|} \\
&= - \partial f (x_0) \left (\lim_{t \to 0^+}\frac{\gamma(t)-x_0}{|\gamma(t)-x_0|} \right ).
\end{align}
$$ Because $\partial f (x_0)$ is invertible, it is bijective. It follows that $$
\lim_{t \to 0^+}\frac{\gamma(t)-x_0}{|\gamma(t)-x_0|} = 0,
$$ which is a contradiction.","['multivariable-calculus', 'inverse-function', 'derivatives']"
4543157,How do I compute $\Bbb{E}\left(\sum_{k=1}^n X_i|X_1\right)$?,"I have $(X_i)$ an i.i.d sequence of random variables in $L^1$ . Now I want to compute $\Bbb{E}\left(\sum_{k=1}^n X_k|X_1\right)$ . I know that this is equal to $\sum_{k=1}^n\Bbb{E}\left( X_k|X_1\right)$ i.e. the problem reduces to compute $\Bbb{E}(X_k|X_1)$ . But somehow I don't see how I can compute this conditional expectation. Could maybe someone help me? We have the following definition: If $X\in L^1$ and $B\subset A$ a sub sigma algebra, then $\Bbb{E}(X|B)$ is the unique r.v. $\xi\in L^1$ such that for all $Q\in L^\infty$ $$\Bbb{E}(XQ)=\Bbb{E}(\xi Q)$$","['conditional-expectation', 'probability-theory', 'probability', 'stochastic-calculus']"
4543171,Tensor product plus (identity times) its trace,"Let $\xi$ and $\zeta$ be two covectors in $V^*$ . What can we say about the $(1,1)$ tensor $$
\xi \otimes \zeta^\sharp + \text{tr}(\xi\otimes \zeta^\sharp)I?
$$ Here $\zeta^\sharp$ is the metric dual to $\zeta$ (assume we have some inner product - which is Lorentzian in my case - so that this is defined).
More concretely I am wondering if the resulting matrix representation is a nonsingular matrix. The context is that I am trying to prove existence and uniqueness of solutions to the system of equations $$
(a b^T + b^T a I)x = c,
$$ where $a, b, c$ are known vectors. Here $I$ is the $n \times n$ identity matrix. More context/conditions: The covectors $\xi, \zeta$ are both null with respect to an ambient Lorentzian inner product $g$ on $V$ . Furthermore, $g(\xi, \zeta) = -2$ . Therefore the covectors are linearly independent.","['linear-algebra', 'tensor-products']"
4543183,"If $d$ is a metric, for what class of functions $f$ is $f(d)$ also a metric?","One example is that if $d$ is a metric, then so is $\frac{d}{1+d}.$ I'm wondering if there are broader generalities than this, that if we can broaden and classify the set of all functions $f$ for which $f(d)$ is also a metric.",['real-analysis']
4543189,"There exists an edge coloring of $K_{10,10}$ with two colors such that there are at most $56$ monochromatic copies of $K_{3,3}$.","Show that there exists an edge coloring of $K_{10,10}$ with two colors such that there are at most $56$ monochromatic copies of $K_{3,3}$ . There are $100$ edges in $K_{10,10}$ and $9$ edges in $K_{3,3}$ . If we consider $56$ copies of $K_{3,3}$ , then there are a total of $56*9 = 504$ edges which is much more than $100$ . So there is certainly intersections between the monochromatic copies of $K_{3,3}$ . I am not getting any idea how to approach the problem. Any hints will be appreciable. Thanks.","['graph-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
4543217,Unique solution of the differential equation with the initial value,"exercise: Let $g$ be a continuous function on the domain $\mathbb{R}$ and suppose that for each $c$ the differential equation $y^{\prime}=g(y)$ has a unique solution, that satisfies the initial condition $y(0)=c$ , and is defined on the domain $\mathbb{R}$ . We can define the function $\phi(x, c)$ of the two variables $x$ and $c$ , so that, as a function of $x$ , it is the solution that satisfies the condition $y(0)=c$ . (a) Show that the translate of a solution of $y^{\prime}=g(y)$ is again a solution; that is, if $y=f(x)$ is a solution and $\alpha$ a number then $f(x-\alpha)$ is also a solution. (b) Show that there is a unique solution that satisfies $y^{\prime}(\alpha)=c$ , and that it is $\phi(x-\alpha, c)$ My draft work: a) Let $y=f(x)$ it means $f'(x)=g(f(x))\Rightarrow f'(x-a)=g(f(x-a))\Rightarrow h'(s)=g(h(s))$ and $h(0)=c$ b)  for simplicity denote $h(s)=t$ . $\,$ $h'(s)=g(h(s))\Rightarrow [h'(s)]'=g'(t)h'(s)$ from here if we get the derivative of $q(s)=\dfrac{h'(s)}{e^{g(h(s))}}\Rightarrow q'(s)=\dfrac{[h'(s)]'e^{g(h(s))}-[e^{g(h(s))}]'h'(s)}{(e^{g(h(s))})^2}=0$ we conclude that $h'(s)=le^{g(h(s))}$ for some number l. However, I couldn't to show the uniqueness. Can anybody check my work if I am on the right track?? Please. Thanks to @Martin R comments I am very suspicious about this may be typo. It should be $y(\alpha)=c$ not $y'(\alpha)=c$","['calculus', 'logarithms', 'derivatives', 'real-analysis']"
4543236,Question about covariant derivative on manifolds.,"I am currently learning about the covariant derivative on smooth manifolds with the following definition: Definition. A covariant derivative of vector fields is an operation that associates to two vector fields $X$ and $Y$ a new vector field $\nabla_XY$ and satisfies the following properties. $\mathcal{C}^{\infty}(M)$ -linearity with respect to $X$ : $$
\nabla_{X_1+X_2}Y = \nabla_{X_1}Y + \nabla_{X_2}Y,\quad \nabla_{fX}Y = f\nabla_XY;
$$ additivity and the Leibniz rule with respect to $Y$ : $$
\nabla_X(Y_1+y_2) = \nabla_XY_1+\nabla_XY_2,\quad \nabla_X(fY) = D_Xf\cdot Y + f\nabla_XY.
$$ Then there is this theorem, for which I do not understand the proof. How am I supposed to understand the transfer the vectorfield via $\varphi$ part? Where does the covariant derivative on $U_i$ come from? Can you please explain me, what the first part with the chart means in exact terms? Thank you",['differential-geometry']
4543313,"Question about Relations - Reflexive, Symmetric, Transitive","Here is the question: Given A = {1, 2, 3, 4, 5, 6, 7}. Determine which of the following
relations from A to A are reflexive, symmetric, and transitive, respectively: • R1 = {(1, 1),(2, 2),(1, 2),(2, 3),(3, 2),(3, 3),(4, 4),(5, 5),(7, 6),(6, 7),(6, 6),(2, 1),(7, 7)}; • R2 = {(1, 2),(2, 3),(1, 3),(1, 4),(2, 4)}; • R3 = {(1, 4),(2, 5),(3, 7),(6, 6)}; • R4 = {(2, 4),(4, 2),(4, 6),(2, 6),(6, 4),(6, 2),(2, 2),(4, 4),(6, 6)}. I have some questions about R2 to R4. For R1, this is reflexive, symmetric, and not transitive. But I am a little bit confused about R2 - R4. Since some of the vertexes have never been visited(like 5,6,7 in R2). If that kind of vertex exists, then it will be not reflexive, not symmetric, and not transitive right? I am confused about wheater I need to exclude them or not. If I include them as a vertex in the graph. Then R2 - R4 will come up with the same result, which is not reflexive, not symmetric, and not transitive. If not, the result will be totally different. Can anyone explain to me this? Thanks","['elementary-set-theory', 'relations', 'discrete-mathematics']"
4543324,"Let $[G:H]=p$ be prime. If there is an element $g\notin H$ satisfies $gH=Hg$, then $H$ is a normal subgroup of $G$","Suppose $G$ is a group and $H\leq G$ with $[G:H]=p$ is a prime. Then prove if there is an element $g\notin H$ satisfies $gH=Hg$ , then $H$ is a normal subgroup of $G$ . My attempt: I tried to consider the normalizer of $H$ . Notice that $H\leq N_G(H)$ , and as a set, $|G/H|=p>|G/N_G(H)|$ . From these I have a feeling that I can show that $N_G(H)=G$ thus $H$ is normal, but I can't get any further... Any help and hints will be appreciated! Best regards!","['normal-subgroups', 'group-theory', 'abstract-algebra']"
4543335,Introductory Holography Resources,"I'm looking for mathematically rigorous introductory resources (as rigorous as an introduction can be) on the subject of Holographic Duality in physics. I require something that adequately covers Anti-de Sitter geometry and the conjectured correspondence with Conformal Field Theory with good physical explanations. I have a background in Differential Geometry at the level of O'Neill's Semi-Riemannian Geometry . My background in Algebra is upto Gorodentsev's Algebra I and II . Lots of diagrams and visualisations to help capture the intuition would be welcome! It would also be great if the resource was comprehensive and covers advanced topics as well. Any help will be greatly appreciated. I do not mind a bunch of different sources as well, so long as they're not all too lengthy.","['reference-request', 'conformal-field-theory', 'conformal-geometry', 'mathematical-physics', 'differential-geometry']"
4543395,Proving the existence of MLE for logistic distribution,"Let $X_{1},\cdots,X_{n}\overset{IID}{\sim}\operatorname{Logis}(\theta,\sigma),\theta\in\mathbb{R},\sigma>0$ . Prove that there exists an MLE of $\eta = (\theta,\sigma)^T$ . I want to prove this by the following theorem: For the log-likelihood function $l(\theta)$ whose second partial derivatives are all continous, if the Hessian matrix is negative-definite and limiting $\theta$ to the boundary of the parameter space makes $l(\theta)$ go to negative infinity, there exists a unique solution to $\dot{l}(\theta) = 0$ , which is the MLE of $\theta$ . Since the pdf for Logistic distribution is $$f(x;\theta,\sigma)=\frac{1}{\sigma}\frac{e^{-\frac{x-\theta}{\sigma}}}{(1+e^{-\frac{x-\theta}{\sigma}})^2}$$ , the log-likelihood function is $$l(\theta,\sigma) = -n\log{\sigma} -n\frac{\bar{x}-\theta}{\sigma} -2 \sum_{i=1}^{n}{\log{(1+e^{-\frac{x_{i}-\theta}{\sigma}})}}       $$ and the Hessian matrix is $\begin{pmatrix}
\frac{\partial^2 l}{\partial \theta^2} & \frac{\partial^2 l}{\partial \theta \partial\sigma} \\
\frac{\partial^2 l}{\partial \sigma \partial\theta} & \frac{\partial^2 l}{\partial \sigma^2}
\end{pmatrix}$ where $$
\begin{aligned}
&\frac{\partial^2 l}{\partial \theta^2}=-\frac{2}{\sigma^2} \sum_{i=1}^n \frac{e^{-\frac{x_i-\theta}{\sigma}}}{\left(1+e^{-\frac{x_i-\theta}{\sigma}}\right)^2} \\
&\frac{\partial^2 l}{\partial \theta \partial \sigma} =\frac{\partial^2 l}{\partial \sigma \partial \theta} =-\frac{n}{\sigma^2}+\frac{2}{\sigma^3} \sum_{i=1}^n \frac{e^{-\frac{x_i-\theta}{\sigma}}\left(\sigma\left(1+e^{-\frac{x_i-\theta}{\sigma}}\right)-\left(x_i-\theta\right)\right)}{\left(1+e^{-\frac{x_i-\theta}{\sigma}}\right)^2} \\
&\frac{\partial^2 l}{\partial \sigma^2} =\frac{n}{\sigma^2}-\frac{2}{\sigma^3} \sum_{i=1}^n x_i+\frac{2 n \theta}{\sigma^3}+\frac{2}{\sigma^4} \sum_{i=1}^n \frac{e^{-\frac{x_i-\theta}{\sigma}}\left(x_i-\theta\right)\left(2 \sigma\left(1+e^{-\frac{x_i-\theta}{\sigma}}\right)-\left(x_i-\theta\right)\right)}{\left(1+e^{-\frac{x_i-\theta}{\sigma}}\right)^2}
\end{aligned}
$$ I could show that $\lim_{\theta \rightarrow \infty}{l(\theta,\sigma^*)}=\lim_{\theta \rightarrow -\infty}{l(\theta,\sigma^*)}=\lim_{\sigma \rightarrow 0+}{l(\theta^*,\sigma)}=\lim_{\sigma \rightarrow \infty}{l(\theta^*,\sigma)}=-\infty$ somehow, but I don't know how to show that the hessian matrix is negative-definite.","['statistical-inference', 'statistics', 'matrices', 'linear-algebra', 'maximum-likelihood']"
4543407,"Correlation, Linear Regression, and Minimizing Cost","If $X$ and $Y$ are random variables with correlation coefficient $\rho$ , then linear regression tells us that, is we wish to minimize the mean square error, then the best linear approximation $\hat{Y}$ (resp $\hat{X}$ ) of $Y$ ( $X$ ) as a function of $X$ ( $Y$ ) satisfies $$\frac{\hat{Y}-\mu_Y}{\sigma_Y} = \rho\frac{X-\mu_X}{\sigma_X},$$ $$\frac{\hat{X}-\mu_X}{\sigma_X} = \rho\frac{Y-\mu_Y}{\sigma_Y}.$$ I want to highlight here that in BOTH cases, the slope (as a function of the normalized variables) is $\rho$ , which reflects the fact that $\rho$ is a symmetric function of $X$ and $Y$ . Now, if someone were to ask me for the equation of the line of best fit between $X$ and $Y$ , WITHOUT mentioning that we want to minimize mean squared error, my naive guess would be that whatever slope the line of best fit has for $Y$ vs $X$ , the line of best fit for $X$ vs $Y$ should have the inverse slope, that is (assuming $X$ and $Y$ are normalized for simplicity): $$\hat{Y} = aX \quad \iff \quad \hat{X} = \frac{1}{a}Y,$$ with the case $a=0$ being left undetermined.
This was my first naive attempt, but after some reflection I realized why this doesn't need to be the case, both on a conceptual level and on an algebraic level. Basically, my confusion was resting on the following logic: $$(\hat{Y} = \rho X \quad \& \quad \hat{Y} \approx Y) \implies  Y \approx \rho X \iff X \approx \frac{1}{\rho}Y \implies \hat{X} = \frac{1}{\rho}Y,$$ and written this way it is clear that the mistake is at the $\hat{Y} \approx Y$ step. Now, I'm saying all of this to finally get to the following question: is there a (natural, useful) way of defining $\hat{X}$ and $\hat{Y}$ so that they do satisfy the naive logic I presented? Perhaps by changing the cost function from mean squared error to something else, and/or by replacing the correlation coefficient by some other quantity $\rho'$ satisfying $\rho'(X,Y) = \frac{1}{\rho'(Y,X)}$ ? $\textbf{tldr:}$ Can we reformulate linear regression to make $\hat{Y} = \rho{X} \iff \hat{X} = \frac{1}{\rho}Y$ ?","['statistics', 'mean-square-error', 'correlation', 'linear-regression', 'probability']"
4543435,Motivation behind Fatou's lemma.,"I am a graduate student of Mathematics and currently studying measure theory.There is a lemma which is used to prove the Lebesgue dominated convergence theorem called Fatou's lemma .It states that: If $(X,\mathcal S,\mu)$ is a measure space and $f_n:X\to [0,\infty]$ be measurable functions,then $\int_X \liminf\limits_{n \to \infty} f_n d\mu\leq \liminf\limits_{n\to \infty}\int_Xf_nd\mu$ . I am looking for some motivation behind this theorem.One motivation may be this: We know that $\liminf$ is finitely super-additive i.e. $\liminf a_n+\liminf b_n\leq \liminf(a_n+b_n)$ for two sequences $(a_n)$ and $(b_n)$ in $\overline{\mathbb R}$ .Now this can be generalized to $\liminf a_{1,n}+...+\liminf a_{N,n}\leq \liminf (a_{1,n}+...+a_{N,n})$ for $N$ number of sequences $(a_{1,n}),...,(a_{N,n})$ in $\overline{\mathbb R}$ .Now define a sequence of functions $f_n:\{1,2,...,N\}\to \overline{\mathbb R}$ by, $f_n(k)=a_{k,n}$ then the above can be rewritten as $\sum\limits_{k=1}^N\liminf f_n(k)\leq \liminf\sum\limits_{k=1}^N f_n(k)$ .So,it is natural to ask whether it is still valid if the sum is on a countably infinite index set say $\mathbb N$ .Our lemma implies that if $X=\mathbb N$ and $\mathcal S=\mathcal P(\mathbb N)$ and $\mu$ is the counting measure,then for any sequence of non-negative functions $f_n:\mathbb N\to [0,\infty]$ ,which is measurable as $\mathcal S=\mathcal P(\mathbb N)$ we can write $\int_{\mathbb N}\liminf f_nd\mu\leq \liminf\int_{\mathbb N} f_nd\mu$ which is nothing but summation.So,we have, $\sum\limits_{k=1}^\infty \liminf f_n(k)\leq \liminf\sum\limits_{k=1}^\infty f_n(k)$ .So,our initial result can be generalized to a countably infinite index set if the sequence is non-negative. I like to think in this way.Is my intuition correct?","['motivation', 'measure-theory', 'lebesgue-integral', 'intuition']"
4543474,What makes Cousin's theorem remarkable?,"I've stumbled upon a mention of Cousin's theorem in the context of Henstock–Kurzweil integral and got confused. I do not understand why this fact is called a theorem and what makes it any remarkable, when it seems to be a more or less trivial consequence of compactness (in the sens that every open cover has a finite subcover). Here is the statement taken from Wikipedia (slightly simplified): Let $\mathcal{C}$ be a full cover of $[a, b]$ , that is, a collection of closed subintervals of $[a, b]$ with the property that for every $x\in[a, b]$ , there exists a $\delta > 0$ so that $\mathcal{C}$ contain all subintervals of $[a, b]$ which contain $x$ and are of length smaller than $\delta$ . Then there exists a partition $a = x_0 < x_1 <\dotsb < x_n = b$ of $[a, b]$ such that $[x_{i-1},x_{i}]\in{\mathcal{C}}$ for all $i$ . Is there some historical context that makes this obvious consequence of compactness deserve to be called a theorem? If so, why is this theorem still so often mentioned in the context of Henstock–Kurzweil integral nowadays instead of just referring to the compactness of the interval? Am I missing something? Clearly, the interval $[a,b]$ is covered by the interiors (the interiors relative to $[a,b]$ ) of closed intervals in $\mathcal{C}$ such that all their closed subintervals containing their midpoint are also in $\mathcal{C}$ , so there is a finite set $\mathcal{D}$ of such closed intervals in $\mathcal{C}$ that covers $[a,b]$ . Is the hard part supposed to be to prove that if $[a,b]$ is covered by a finite set $\mathcal{D}$ of closed intervals, then there is a partition $a = x_0 < x_1 <\dotsb < x_n = b$ of $[a, b]$ such that each $[x_{i-1},x_{i}]$ is contained in an element of $\mathcal{D}$ and contains that element's midpoint?","['integration', 'real-analysis', 'gauge-integral', 'math-history', 'soft-question']"
4543477,Old Maid Card Game Probability Question,"I am not a math person but was shocked by something that happened when I played Old Maid for the first time ever with with my two young sons tonight. We were playing with a deck that had 37 cards in total, which means 18 pairs plus the Old Maid. When I dealt the cards into three groups (12+12+13), not one of us had a single pair in our hands. What is the statistical likelihood that this could happen? Can anyone figure out the math for me? I doubt this will ever happen again in our lifetimes, but maybe I'm wrong..","['graph-theory', 'combinatorics', 'card-games', 'probability']"
4543480,How to factorize $\frac{\cos(3x)-\cos(x)}{\tan(2x)-\tan(x)}$?,"How to factorize $\dfrac{\cos(3x)-\cos(x)}{\tan(2x)-\tan(x)}$ ? Which trigonometric identities to use? I'm stuck when it comes to $\tan(2x)+\tan(x)$ . I don't know which identity to use to turn it into the product. I was thinking of just transforming them to sines and cosines, but also doesn't get me anywhere. Thanks for help in advance.",['trigonometry']
4543484,Examples of non-constant functions $f(x)$ such that $f(x)=f(\pi+x)=f(\pi-x)$,"I am trying to find a non-constant functions $f(x)$ satisfying $f(x)=f(\pi+x)=f(\pi-x)$ . I tried to use $f(x)=\sin(x)$ , $f(x)=\cos(x)$ , $f(x)=\sin(x/2)$ , $f(x)=\cos(x/2)$ , $f(x)=\sin(2x)$ , $f(x)=\cos(2x)$ , and some other functions involving other trigonometric ratios, also tried to combine some of them together, but I failed. Your help would be appreciated. THANKS!","['periodic-functions', 'examples-counterexamples', 'functions', 'pi', 'trigonometry']"
4543529,Minimum possible sum of weights of the heaviest and the $2^{nd}$ heaviest kid,"Question : The average weight of any $2$ kids in a group of $6$ kids is a distinct natural number. The minimum weight of any kid is $11$ kg. Find the minimum possible sum of the weights of the heaviest and the $2^{nd}$ heaviest kid in the group. Solution : By hit and try we get the values as $11,13,15,19,25,35$ kgs. As after taking $11,13,15$ we can't take $17$ , so next take $19$ and so on. Is there any other (better, that is, more logical) way to do this question (without hit and try)? Because the hit and try approach is too time taking.","['elementary-set-theory', 'elementary-number-theory', 'average', 'contest-math']"
4543578,Radius of curvature at a point on a Lorentzian manifold,"I recently asked this question on the physics stack exchange. After some discussion in the comments, I didn't really arrive at a satisfactory conclusion, so I thought it might be worth it to ask it here as well. I'm new to posting questions here, so if it goes against guidelines, then apologies and a mod can close this. Anyway, here is the question. For context, I am reading this paper. The authors speak of the radius of curvature of a spacetime at a particular point. However, they don't give an explicit definition of this term. So more precisely, let $(M,g)$ be a $D$ -dimensional Lorentzian manifold with Riemann tensor $R$ , which is non-vanishing, and consider an arbitrary point $p\in M$ . Then how is the radius of curvature at $p$ defined? In the comments to my original post, user Prahar provided the following definition: \begin{equation}
\tag{1}
r^2(x)=\frac{D(D-1)}{R(x)}
\end{equation} where $r(x)$ is the radius of curvature at $x$ and $R(x)$ is the Ricci scalar at $x$ . The problem is, the authors of the article mentioned above, have to be using a definition where the radius of curvature is finite, even in a Ricci-flat region of $M$ . This is clearly not the case for this definition, since the Ricci scalar would be identically zero in a Ricci flat region and hence $r$ would diverge. My question can thus be summarized: Is there a definition of the radius of curvature at a point on a Lorentzian manifold such that the radius of curvature is finite even in a Ricci-flat region of the manifold? One candidate I could think of would be to replace $R(x)$ with the square-root of the Kretschmann scalar in $(1)$ . However I don't know if this even makes sense as a definition of radius of curvature. Any answers or reference suggestions are greatly appreciated.","['definition', 'general-relativity', 'differential-geometry']"
4543594,Eigenvectors spanning closed subspace in a Banach space,"I'm looking for an example of a (bounded) linear operator $T$ on a Banach space $X$ with infinitely many eigenvalues such that $\sum_{\lambda\in\mathbb C}\ker(T-\lambda)$ is closed where the sum denotes the algebraic sum as vector spaces, i.e. the smallest vector space containing $\ker(T-\lambda)$ for all $\lambda\in\mathbb C$ . So far in the examples I've tried, it seems like this fails to be the case, at best this sum is dense in $X$ , for instance any normal operators on a Hilbert space. Since $\sum_{\lambda\in\mathbb C}\ker(T-\lambda)$ is closed, we know that it must have an uncountable dimension, so either there are uncountably many eigenvalues or the eigenspace has uncountable dimension. However this doesn't seem to help much. My intuition is that $X$ being complete would prevent this from happening but I can't seem to show that there must be an element in the closure that is not an eigenvector. So far my current attempt to show that $X$ need to have uncountably many eigenvalues is as follows: Let $\lambda_i$ be the set of eigenvalues of $T$ , i.e. $\{\lambda\in\mathbb C:\ker(T-\lambda)\neq\{0\}\}=\{\lambda_i\}_{i=0}^\infty$ . Then we want to show that $$\bigoplus_{i=0}^\infty\ker(T-\lambda_i)$$ is not closed. Choose arbitrary $x_i\in\ker(T-\lambda_i)$ such that $\left\lvert x_i\right\rvert<i^{-2}$ , then $$\sum_{i=0}^\infty x_i\in\overline{\bigoplus_{i=0}^\infty\ker(T-\lambda_i)}$$ However $$\alpha\sum_{i=0}^\infty x_i=\sum_{i=0}^\infty\lambda_ix_i$$ From here I'm hoping to show that $\sum_{i=0}^\infty x_i$ cannot be a finite sum of eigenvectors, but I'm not sure if the $\lambda_i$ on the RHS is the only way of expressing the LHS as a limit of scalar multiples of $x_i$ .","['banach-spaces', 'operator-theory', 'functional-analysis', 'eigenvalues-eigenvectors']"
4543632,Geometry - Solve a system of equations (maybe using WA or another CAS) [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question I found this problem on the web, and it seemed like an interesting exercise to me. So I am trying to solve this as a system of equations. I tried using for example Wolfram Alpha. That means e.g. I am trying to find the radius $R$ of the circle, and the point $x$ , where the two curves touch each other. I was hoping at least for a numeric solution. But I don't get anything  useful. Could anyone help to get this working in WA? I am not sure why WA doesn't understand this syntax. Solve[Exp(-x^2) * (-2*x) == (R-x) / sqrt(x * (2*R-x)) && Exp(-x^2) == R + sqrt(x * (2*R-x)) && x > R && x < 2*R, {x, R}, Reals] How did I get these equations? Well, I think they should give me the solution to this problem above. Why? Because they express that $f(x) = g(x)$ and also $f'(x) = g'(x)$ where the radius is denoted by $R$ . Here I have denoted $$f(x) = e^{-x^2}$$ $$g(x) = R + \sqrt{x(2R-x)}$$ ( $g$ is the top semi-circle basically) and I am looking for a solution $x$ , where $x \in (R, 2R)$ I hope I didn't mess any calculations... but I think I didn't.
The two derivatives are easy to calculate.
So I am not sure why WA doesn't understand this. EDIT : I am not sure if any of the solutions below is actually an exact/explicit solution. I think I didn't get any formula for the blue area or for the touch point. This is not a Mathematica stack exchange question actually. Mathematica was used just as a means to solve the problem. I am actually looking for a formula for the blue area (or for the X value of the touch point). I constructed a system of equations but solving by hand wasn't possible. Is there such formula, or do we have to use numeric approximations here? I think both answers here provide just a numeric approximation.","['calculus', 'systems-of-equations', 'area', 'geometry']"
4543684,"Convolution derivative, $(f*g)'=f*g'$","Let $f,g \in L^1$ and $g$ is differentiable and $g' \in L^1$ , then $(f*g)'=f*g'$ PROOF : For this proof I am asked to use the dominated convergence theorem It can be quickly arrived at that $$\begin{align}
(f*g)'(x) &= \lim\limits_{h \to 0}  \int_\mathbb{R} f(t)\frac{g(x+h-t)-g(x-t)}{h}dt\\
\end{align}$$ For the following, it is necessary to interchange the limit and the integral and it is here where the theorem mentioned above is used. To do so, it is sufficient to find a $k\in L^1$ such that $|f(t)\frac{g(x+h-t)-g(x-t)}{h}|\leq k(t)$ However, I am having trouble finding such a function $k$","['measure-theory', 'lebesgue-integral', 'convolution', 'analysis']"
4543762,Transformation of Curvature 2-Form Under Global Bundle Autormphism,"Let $F^A$ be a curvature two form on a principal bundle $P$ corresponding to the connection one form $A$ . Let $f:P\rightarrow P$ be a global bundle automorphism; there then exists a map $\sigma_f:P\rightarrow G$ such that: $$f(p)=p\cdot \sigma_f(p)$$ Finally let $\mu_G$ be the maurer cartan form, given by: $$\mu_G(X_g)=L_{g^{-1}*}X_g$$ where $X_g$ is a vector field on some Lie group $G$ . Under the pullback of $A$ by $f$ we have:"" $$f^*A=\text{Ad}_{\sigma_{f^{-1}} } \circ A+\sigma_f^*\mu_G$$ I am trying to show that: $$F^{f^*A}=\text{Ad}_{\sigma_{f^{-1}}}\circ F^A$$ where: $$F^A=dA+\frac{1}{2}[A,A]$$ and for two Lie algebra valued one forms $\eta,\omega$ : $$[\eta,\omega](X,Y)=[\eta(X),\omega(Y)]-[\eta(Y),\omega(X)]$$ which comes from the definition of the wedge product for Lie algebra valued $k$ and $l$ forms. When replacing $A$ with $f^*A$ , we see that: $$F^{f^*A}=d(\text{Ad}_{\sigma_{f^{-1}}}\circ A)+\sigma_f^*d\mu_G+\frac{1}{2}\text{Ad}_{\sigma_{f^{-1}}}\circ [A,A]+\frac{1}{2}\sigma_f^*[\mu_G,\mu_G]+\frac{1}{2}[\text{Ad}_{\sigma_{f^{-1}}}\circ A,\sigma_f^*\mu_G]+\frac{1}{2}[\sigma_f^*\mu_G,\text{Ad}_{\sigma_{f^{-1}}}\circ A]$$ By the Maurer-Cartan equation we have that: $$\sigma_f^*d\mu_G=-\frac{1}{2}\sigma^*_f[\mu_G,\mu_G]$$ hence: $$F^{f^*A}=d(\text{Ad}_{\sigma_{f^{-1}}}\circ A)+\frac{1}{2}\text{Ad}_{\sigma_{f^{-1}}}\circ [A,A]+\frac{1}{2}[\text{Ad}_{\sigma_{f^{-1}}}\circ A,\sigma_f^*\mu_G]+\frac{1}{2}[\sigma_f^*\mu_G,\text{Ad}_{\sigma_{f^{-1}}}\circ A]$$ My problem is now that I am unsure of how to calculate the exterior derivative of $\text{Ad}_{\sigma_{f^{-1}}}\circ A$ . By our previous formula for two Lie algebra valued one forms we have that for $\eta=\text{Ad}_{\sigma_{f^{-1}}}\circ A$ ,
and $\omega=\sigma_f^*\mu_G$ : $$\frac{1}{2}\left([\eta,\omega]+[\omega,\eta]\right)(X,Y)=\frac{1}{2}\left([\eta(X),\omega(Y)]-[\eta(Y),\omega(X)]+[\omega(X),\eta(Y)]-[\omega(Y),\eta(X)]\right)$$ $$=[\eta(X),\omega(Y)]-[\eta(Y),\omega(X]$$ $$=[\eta,\omega](X,Y)$$ So I feel that: $$d(\text{Ad}_{\sigma_{f^{-1}}}\circ A)=\text{Ad}_{\sigma_{f^{-1}}}\circ dA-[\eta,\omega]$$ but I don't see why this should be true. In fact, since $\text{Ad}$ is a representation of $G$ on $\mathfrak{g}$ I would think that: $$d(\text{Ad}_{\sigma_{f^{-1}}}\circ A)=\text{Ad}_{\sigma_{f^{-1}}}\circ dA$$ but then I have this extra factor of $[\eta,\omega]$ , so I am at a loss. Any hints on how to calculate this exterior derivative, or on how that wedge product may actually be zero would be greatly beneficial. Edit: Employing Professor Shifrin's notation, I write that: $$f^*A=\sigma_{f^{-1}}A\sigma_f+\sigma_{f^{-1}}d\sigma_f$$ I have already calculated the exterior derivative of the second term, albeit in a different notation, so I move to the first term: $$d(\sigma_{f^{-1}}A\sigma_f)=d(\sigma_{f^{-1}})\wedge A\sigma_f+
\text{Ad}_{\sigma_f^{-1}}\circ dA+\sigma_{f^{-1}}A\wedge d(\sigma_f) $$ Note that: $$\sigma_{f^{-1}}\sigma_f=e$$ hence: $$d(\sigma_{f^{-1}})\sigma_f=-\sigma_{f^{-1}}d\sigma_f\Rightarrow d\sigma_{f^{-1}}=-\sigma_f^{-1}d\sigma_f\sigma_{f^{-1}}$$ Therefore we obtain: $$d(\sigma_{f^{-1}}A\sigma_f)=-\sigma_{f^{-1}}d\sigma_f\wedge \text{Ad}_{\sigma_f^{-1}}A+
\text{Ad}_{\sigma_f^{-1}}\circ dA-\text{Ad}_{\sigma_{f^{-1}}}\circ A \wedge d(\sigma_{f^{-1}})\sigma_f$$ The final term in this sum can be rewritten as: $$\text{Ad}_{\sigma_{f^{-1}}}\circ A \wedge d(\sigma_{f^{-1}})\sigma_f=-\text{Ad}_{\sigma_{f^{-1}}}\circ A \wedge \sigma_f^{-1}d\sigma_{f}$$ Thus: $$d(\sigma_{f^{-1}}A\sigma_f)=-\sigma_{f^{-1}}d\sigma_f\wedge \text{Ad}_{\sigma_f^{-1}}A+
\text{Ad}_{\sigma_f^{-1}}\circ dA+\text{Ad}_{\sigma_{f^{-1}}}\circ A \wedge \sigma_f^{-1}d\sigma_{f}$$ From here, I can kind of see how the pieces should move, but I am unsure why. Reconciling the two notations is proving troublesome to me. To be clearer, it seems that from the definition for the wedge product of twisted forms, this product is actually symmetric for one forms, so I feel like the first term and the last term would cancel, which doesn't make sense. I am then left to conclude that there is something different about the wedge product I have used in this edit, and the wedge product I used in the original post, but I am unsure what that difference actually is, or how to make it precise. Edit 2: The definition of wedge product for Lie algebra valued one forms is
as follows: Given a $\omega\in \Omega^k(P,\mathfrak{g})$ and $\eta\in \Omega^k(P,\mathfrak{g})$ , then their wedge product is given by: $$[\omega,\eta](X_1,\dots,X_{k+l})=\frac{1}{k!l!}\sum_{\sigma\in S}\text{sign}(\sigma)\left[\omega(X_{\sigma(1)},\dots,X_{\sigma(k)}), \eta(X_{\sigma(k+1)},\dots,X_{\sigma(k+l)} ) \right]$$ When choosing a basis $\{T_i\}$ for the Lie algebra $\mathfrak{g}$ , we have that $\eta$ and $\omega$ can be written as: $$\eta=\sum_{i=1}^n \eta^i\otimes T_i$$ $$\omega=\sum_{i=1}^n \omega^i\otimes T_i$$ where each $\omega^i$ and $\eta^i$ are $k$ and $l$ forms on $P$ respectively. Then the wedge product can be written as $$\omega \wedge \eta =\sum_{i,j=1}^n \omega^i\wedge\eta^j\otimes[T_i, T_J]  $$ Hence: $$[\eta,\omega] =\sum_{i,j=1}^n \eta^j\wedge\omega^i\otimes[T_j,T_i] $$ $$=(-1)^{lk}\sum_{i,j=1}^n\eta^i \wedge\omega^j\otimes[T_j,T_i]$$ $$=(-1)^{lk+1}\sum_{i,j=1}^n\eta^i \wedge\omega^j\otimes[T_i,T_j]$$ $$=(-1)^{lk+1}[\omega,\eta]$$ So for two one forms, $\eta$ and $\omega$ : $$[\eta,\omega]=(-1)^2[\omega,\eta]=[\omega,\eta]$$ Hence the wedge product of Lie algebra valued forms is symmetric for one forms.","['connections', 'curvature', 'gauge-theory', 'differential-topology', 'differential-geometry']"
4543778,Find a side using similarity of triangles,"I was given this problem by a professor. I think this is a straightfoward problem: by AA, triangles ABD and CBA are similar. After identifying corresponding sides one can see that $x=4$ Question: I was told this is incorrect by the professor. I cannot see what is wrong with my reasoning. What is the correct answer to this problem? Thank you!","['euclidean-geometry', 'geometry']"
4543781,Why does this strategy to pick the largest envelope work?,"Game: You have two envelopes. Each one contains a real number. Choose one envelope and you get to see the number on it. Now you have two choices: select the same envelope, or swap. At the end, if you have chosen the greater number, You win. Problem: Find a strategy that gives you a probability of winning that is more than $0.5$ Solution: Choose one real number $x$ . If envelope one contains $y>x$ , keep it, else swap. My Question: Why does the strategy work? Can we prove that probability-theoretically?","['probability-distributions', 'probability-theory', 'probability']"
4543799,$X \cap A = X\setminus B \iff X \cap (M\setminus A) = B$,"Suppose $A \subseteq M, X \subseteq M \text{ and } B\subseteq X$ I want to prove the following biconditional statement: $X \cap A = X\setminus B \iff X \cap (M\setminus A) = B$ I did as follows, but I'm not sure if I did it right $X \cap A = X\setminus B$ $\Leftrightarrow X \cap A = X \cap B^\complement$ $\Leftrightarrow X \setminus (X \cap A) = X \setminus (X \cap B^\complement)$ $\Leftrightarrow X \setminus A = X \setminus B^\complement$ $\Leftrightarrow X \cap A^\complement = X \cap B$ $\Leftrightarrow (X \cap A^\complement) \cap M = B$ $\Leftrightarrow X \cap (M \cap A^\complement) = B$ $\Leftrightarrow X \cap (M \setminus A) = B$ Is this correct?",['elementary-set-theory']
4543825,Proving there is no solution to an ODE,"Question: Show that, for any a > 0, there exists no solution u : $[0, \frac{1}{a}] \rightarrow \mathbb{R}$ to the problem $$
\begin{cases}
\frac{du}{dt} = u^{2}+e^{-u}\\
u(0) = a
\end{cases}
$$ Hint: Check that $u(t) > 0$ for all $t$ and that $\frac{du}{dt} \geq u^{2}$ ; deduce that $\frac{1}{u}$ decreases at least linearly My work so far: Given that the first derivative of $u$ is strictly greater than $0$ , then $u$ is strictly increasing from $a>0$ . Thus, $u(t) > 0$ for all $t$ . Secondly, since $e^{-u}$ is non-zero and non-negative, $\frac{du}{dt}>u^{2}$ Thirdly, since it was established that $u$ is an increasing function, then $\frac{1}{u}$ is a decreasing function.
To show that it decreases at least linearly, consider the following $$
\begin{aligned}
\frac{du}{dt} &\geq u^{2}\\
\int \frac{du}{u^{2}} &\geq \int dt\\
\frac{-1}{u} &\geq t + c\\
-t - c &\geq \frac{1}{u} 
\end{aligned}
$$ Thus, $\frac{1}{u}$ decreases at least linearly. My obstacle: How can I deduce from that that there are no solutions to the ODE?
Any help or guidance is appreciated.",['ordinary-differential-equations']
4543844,General solution to $f^{\prime\prime} (x) + g(x) f(x) = 0$,"I am trying to find the general solution to the following ODE with a variable coefficient: $$ f^{\prime\prime} (x) + g(x) f(x) = 0 \tag{1}$$ where the function $f$ is not known and $g$ is known. By doing research I only managed to find solutions of equation $(1)$ for specific cases of $g$ , but I did not find a general case where $g$ can be defined in any way. Is there a general solution for equation $(1)$ where $g$ is a general function, not a specific one? If a general solution does exist, what is it?",['ordinary-differential-equations']
4543895,Quantify the similarity between a polynomial roots and the roots of its derivatives,"On $\mathbb{C}[X]$ , many theorems and conjectures deal with relations between a polynomial roots and the roots of its derivatives. When looking at a graph, the derivative roots distribution somewhat mimics the distribution of the polynomial roots. It is this ""somewhat mimics"" that I would like to look at in this question. Examples. Let $P$ be a degree $n$ polynomial in $\mathbb{C}[X]$ , then: (Well-known) The mean of $P$ roots is also the mean of $P$ successive derivatives roots. And so it is the only root of $P^{(n-1)}$ which has degree $1$ . The mean being the first cumulant, what about the other cumulants of the roots? The second cumulant is the variance $\sigma^2$ , same as second central moment. We find that $\sigma^2/(n-1)$ is conserved: if $\sigma'^2$ is the variance of $P'$ roots, $\sigma'^2 = \frac {n-2} {n-1} \sigma^2$ (proof at the end). Similarly, for the third cumulant $\kappa_3$ , which is also the third central moment $\mu_3$ : if $\kappa_3'$ is the third cumulant of $P'$ roots, $\kappa_3'= \frac {n-3} {n-1} \kappa_3$ (proof at the end). However this does not extend to $\kappa_4$ nor to $\mu_4$ (which by the way are different). $\sigma'^2 = \frac {n-2} {n-1} \sigma^2$ has the following consequence (proof at the end): distance between the two roots of $P^{(n-2)}$ is proportional to $\sigma$ : $\frac 2 {\sqrt{n-1}} \sigma$ . Question :
Are there other quantities that characterize the roots distribution of a polynomial, that are conserved (possibly with a factor only depending upon $n$ ) in the polynomial derivative? These quantities should have an established statistical meaning, or a geometric interpretation. E.g. what about the PCA (principal component analysis) of the roots? Proofs : Use the following relations between cumulants $\kappa_i$ , elementary symmetrical polynomials $e_i$ , elementary symmetric polynomials for the derivative $e'_i$ , central moments $\mu_i$ , raw moments $\mu'_i$ (sorry for the notation clash: not the central moments of the derivative), power sums $p_i$ , polynomial roots $a_i$ : $\mu'_i=\frac 1 n p_i=\frac 1 n \sum_{k=1}^n a_k^i$ $p_1=e_1$ $p_2=e_1^2-2e_2$ $p_3=e_1^3-3e_1e_2+3e_3$ $\kappa_2=\mu'_2-\mu_1'^2$ $\kappa_3=\mu'_3-3\mu'_2\mu'_1+2\mu_1'^3$ $e'_i = \frac {n-i} n e_i$ Second cumulant (variance): $\kappa_2=\mu'_2-\mu_1'^2=\frac 1 n p_2 - \frac 1 {n^2} p_1^2$ $=\frac {n-1} {n^2} e_1^2 - \frac 2 n e_2$ $\kappa'_2=\frac {n-2} {(n-1)^2} e_1'^2 - \frac 2 {n-1} e'_2$ then replace $e'_i$ with $\frac {n-i} n e_i$ , gives $\kappa'_2=\frac {n-2} {n-1} \kappa_2$ . Third cumulant: $\kappa_3=\mu'_3-3\mu'_2\mu'_1+2\mu_1'^3$ $=\frac 1 n p_3 - \frac 3 {n^2} p_2p_1 + \frac 2 {n^3} p_1^3$ $= \frac 1 n (e_1^3-3e_1e_2+3e_3) - \frac 3 {n^2}(e_1^2-2e_2)e_1 + \frac 2 {n^3}e_1^3$ $=\frac {(n-1)(n-2)} {n^3} e_1^3 - 3 \frac {n-2} {n^2}e_1e_2 + \frac 3 n e_3$ $\kappa'_3= \frac {(n-2)(n-3)} {(n-1)^3} e_1'^3 - 3 \frac {n-3} {(n-1)^2}e'_1e'_2 + \frac 3 {n-1} e'_3$ then replace $e'_i$ with $\frac {n-i} n e_i$ , gives $\kappa'_3=\frac {n-3} {n-1} \kappa_3$ . Distance between roots of the $(n-2)$ th derivative: this can be proven using the variance conservation relation, or directly: Let $P(Z) = \sum_{j=0}^n (-1)^j e_j \; Z^{n-j}$ . $P^{(n-2)}(Z)=\frac {n!} 2 Z^2-(n-1)! \; e_1 \; Z+(n-2)! \; e_2.$ Its two roots are $\frac 1 n {e_1} \pm \frac 1 {n!} \sqrt{(n-1)!(n-2)!((n-1)e_1^2-2ne_2)}$ . The quantity $(n-1)e_1^2 - 2n e_2 = (n-1)(\sum a_j)^2 - 2n \sum_{j<k}a_j a_k$ $= n (\sum a_j)^2 - 2n \sum_{j<k}a_j a_k - (\sum a_j)^2 = n \sum a_j^2 - (\sum a_j)^2$ $= n^2 (\frac 1 n \sum a_j^2 - (\frac {\sum a_j} n)^2) = n^2 \sigma^2$ , with $\sigma^2$ the variance of $P$ roots. So distance between the two roots $= \frac 2 {n!} \sqrt{(n-1)!(n-2)!n^2 \sigma^2} = \frac 2 {\sqrt{n-1}} \sigma$","['cumulants', 'derivatives', 'polynomials']"
4543918,A direct computation on Dirac delta function,"For the two dimensional vectors $x=(x_1,x_2)$ and $\xi=(\xi_1,\xi_2)$ , define the following operator $$e^{it\Delta}f(x):=\int_{\mathbb{R}^2}e^{i x\cdot\xi+it|\xi|^2}\hat{f}(\xi)\mathrm{d}\xi, \quad x\cdot\xi:=x_1\xi_1+x_2\xi_2.$$ For the functions ${\psi}_1$ and ${\psi}_2$ , assume that $$\mathrm{supp}(\widehat{\psi}_1) \subset\{\xi\in\mathbb{R}^2:|\xi|\sim M\}, \quad \mathrm{supp}(\widehat{\psi}_2) \subset\{\xi\in\mathbb{R}^2:|\xi|\sim N\}.$$ Here $\widehat{\psi}_1$ denotes the Fourier transform of $\psi_1$ and the notation $|\xi|\sim M$ means that $$c_1 M\leq |\xi|\leq c_2 M$$ for some universal constants $c_1$ and $c_2$ . Furthermore, let $\delta_0$ be the Dirac delta function. Then my question is how to use Cauchy-Schwarz to conclude the following estimate: \begin{align}
\left\|e^{it\Delta}\psi_1 e^{it\Delta}\psi_2\right\|_{L_{t,x}^2(\mathbb{R}^3)}&=\int_{\mathbb{R}_{\tau,\xi}^3} \left|\int_{\mathbb{R}_{\xi'}^2}\widehat{\psi}_1(\xi')\widehat{\psi}_2(\xi-\xi') \delta_0(\tau-|\xi'|^2-|\xi-\xi'|^2)\mathrm{d}\xi'\right|^2\mathrm{d}\tau\mathrm{d}\xi \\
&\leq \left\|\psi_1\right\|_{L^2}^2 \left\|\psi_2\right\|_{L^2}^2 \left[\sup_{\tau,|\xi|\sim N} \left|\left\{\xi'\in\mathbb{R}^2: |\xi'|\sim M, |\xi'|^2+|\xi-\xi'|^2=\tau\right\}\right|\right] \\
&\leq C \frac{M}{N}.
\end{align} This estimate appears in Bourgian's paper Refinements of Strichartz' inequality and applications to 2D-NLS with critical nonlinearity in the proof of Lemma 111. Further details can be seen there. As far as I know, up to a constant, there holds $$\widehat{e^{it\Delta}\psi}(\tau,\xi)=\delta_0(\tau-|\xi|^2)\widehat{\psi}(\xi),$$ where $\widehat{e^{it\Delta}\psi}(\tau,\xi)$ is the space-time Fourier transform of $e^{it\Delta}\psi(t,x)$ . Therefore, Plancherel theorem can give the identity $$\left\|e^{it\Delta}\psi_1 e^{it\Delta}\psi_2\right\|_{L_{t,x}^2(\mathbb{R}^3)}=\int_{\mathbb{R}_{\tau,\xi}^3} \left|\int_{\mathbb{R}_{\xi'}^2}\widehat{\psi}_1(\xi')\widehat{\psi}_2(\xi-\xi') \delta_0(\tau-|\xi'|^2-|\xi-\xi'|^2)\mathrm{d}\xi'\right|^2\mathrm{d}\tau\mathrm{d}\xi.$$ However, I am not very familar with the computations for delta functions and confused about using Cauchy-Schwarz to compute further. In fact, the last inequality $$\left\|\psi_1\right\|_{L^2}^2 \left\|\psi_2\right\|_{L^2}^2 \left[\sup_{\tau,|\xi|\sim N} \left|\left\{\xi'\in\mathbb{R}^2: |\xi'|\sim M, |\xi'|^2+|\xi-\xi'|^2=\tau\right\}\right|\right] \leq C \frac{M}{N}$$ also confuse me very much. Any comments would be helpful. Thanks in advance for your comments and answers!","['measure-theory', 'harmonic-analysis', 'distribution-theory', 'real-analysis', 'partial-differential-equations']"
4543973,Question from P1 exam book - joint continuous gamma distribution. This is a question from the actuary P1 book,"If X and Y are independent gamma random variables with parameters (α, λ)
and (β, λ) respectively, compute the joint density of U = X + Y and $$ V =
\frac{X}{X+Y} $$ I was stuck on how to find $$ f_U $$ and $$ f_V $$ for a while but was able to figure it out. I had seen some questions pertaining to parts of this problem or slightly different problems with people struggling and thought I should post it. The answer with my work is below. I hope it helps!","['multivariable-calculus', 'probability-distributions', 'probability-theory', 'probability']"
4544007,Squeeze Theorem for $(2^n + 3^n + 4^n)^{\frac{1}{n}}$,"How to find this limit when $n \rightarrow \infty$ . $$(2^n + 3^n + 4^n)^{\frac{1}{n}}.$$ My Idea: We need to squeeze this sequence. $$ (4^n)^{\frac{1}{n}}\le (2^n + 3^n + 4^n)^{\frac{1}{n}} \le (3\cdot4^n)^{\frac{1}{n}}$$ So that we obtain $$ 4\le (2^n + 3^n + 4^n)^{\frac{1}{n}} \le (3)^{\frac{1}{n}} 4$$ By taking the limit $n \rightarrow \infty$ $$4\le (2^n + 3^n + 4^n)^{\frac{1}{n}} \le 4 $$ So, by squeeze theorem $$\lim_{n \to \infty} (2^n + 3^n + 4^n)^{\frac{1}{n}} = 4.$$ Actually, I still doubt about my idea.
Is my idea correct? Thanks in advance.","['calculus', 'sequences-and-series', 'real-analysis']"
4544050,$\cos(105°)$ using sum and difference identities,"I was trying to solve $\cos(105°)$ using sum and difference identities. My solution: $\cos(105°) = \cos(60°+45°)$ $\cos(60°)\cos(45°) - \sin(60°)\sin(45°)$ so, $\frac{1}{2} \cdot \frac{\sqrt{2}}{2}$ - $\frac{\sqrt{3}}{2} \cdot \frac{\sqrt{2}}{2}$ then, $\frac{\sqrt{2}}{4} - \frac{\sqrt{6}}{4}$ last, $\frac{\sqrt{2}-\sqrt{6}}{4}$ but when I try to use calculator it says, $\frac{-\sqrt{6}+\sqrt{2}}{4}$ What part of the solution I am doing it wrong?","['algebra-precalculus', 'trigonometry']"
4544087,Is it possible that $y'(x) = x^2 - y^2$ has a second solution?,"While testing Physics-informed neural networks I discovered that IVP $$ y'(x) = x^2 - y^2 $$ $$ y(0) = 2.5 $$ possibly has a second solution. I'm quite sure that my model once found a very good approximation (I discarded it because I wasn't looking for it) of this ""new"" solution. But I cannot recreate it since, so I started wondering whether this IVP actually has a second solution. My best attempt so far at finding the second solution is shown on this graph as a blue curve. Curves on the graph are: Blue curve - best neural network solution Red curve - known solution found with a RK45 numerical method Green curve - actual derivative of NN solution Orange curve - value of $f(x, y) = x^2 - y^2$ computed from the NN solution Is it possible to check if that potential second solution actually exists or does not exist, without finding it? I know that this IVP is a form of Riccati equation and its known solution has a (not simple) analytical form . So if the potential second solution actually exists, I am wondering if it has maybe been already found or the existing analytical solution can be adapted to fit the potential ""second"" solution.","['initial-value-problems', 'numerical-methods', 'ordinary-differential-equations']"
4544094,"Solving the false ""theorem"" $\int\frac1{f(x)}dx=\frac1{\int f(x)dx}$","For fun, I am trying to solve the false ""theorem"" $$\int\frac1{f(x)}dx=\frac1{\int f(x)dx}$$ for all functions $f$ that satisfy the above equation. I got a solution but it seems to be wrong, and I was wondering where it went wrong. Here's how I did it: I first started by differentiating both sides, using Chain Rule on the RHS: $$\frac1{f(x)}=-\frac{f(x)}{(\int f(x)dx)^2}$$ Cross multiplying, I get: $$-(f(x))^2=\left(\int f(x)dx\right)^2$$ So: $$\int f(x)dx=\sqrt{-(f(x))^2}$$ Then, differentiating both sides again: $$f(x)=\frac12(-(f(x))^2)^{-1/2}\cdot(-2f(x))\cdot f'(x)$$ Which simplifies to (I switch from $f(x)$ to $y$ from here to make it easier to keep track of the variables): $$y=-(-y^2)^{-1/2}\cdot\frac{dy}{dx}$$ Separating the variables: $$dx=\frac{-(-y^2)^{-1/2}}ydy$$ At this point I'm not sure what else to do so I introduce the imaginary numbers into the equation (this is where it starts getting iffy for me, because I never mixed imaginary numbers with integration before): $$dx=\frac{-(-1)^{-1/2}\cdot(y^2)^{-1/2}}ydy$$ $$dx=\frac{-1/i}{y^2}dy$$ $$\int dx=-\frac1i\int\frac1{y^2}dy$$ $$x+C_1=-\frac1i\cdot\left(-\frac1y+C_2\right)$$ $$x+C_1=\frac1{yi}+C_2$$ $$x+C_3=\frac1{yi}$$ $$yi=\frac1{x+C_3}$$ $$\boxed{f(x)=\frac1{ix+C}}$$ where $C$ is an arbitrary complex number. But the problem with this solution is when I try to plug this back into the original equation, it doesn't seem to make the equation true. For example, let's say I choose $C=0$ , so that $f(x)=\frac1{ix}$ . Then the LHS becomes: $$\int\frac1{f(x)}dx=\int ixdx=i\frac{x^2}2+C_4$$ But the RHS becomes: $$\frac1{\int f(x)dx}=\frac1{\int\frac1{ix}dx}=\frac1{\frac1i\ln|x|+C_5}$$ which obviously don't match each other at all.","['integration', 'ordinary-differential-equations', 'calculus', 'derivatives', 'complex-numbers']"
4544117,Inutitive explanation for why my crude approximation of a probability approaches exactly half of the exact value,"First, an introductory question: There are $3n$ couples. The $6n$ people are each randomly allocated to one of three rooms, so that each room has $2n$ people. What is the probability that every couple is separated? I made a crude approximation for large $n$ : $\left(\frac{2}{3}\right)^{3n}$ , because (assuming for the sake of simplicity that each couple consists of a husband and a wife) for each husband, there is an approximately $\frac{2}{3}$ chance that his wife is in a different room. My question is: It turns out that my approximation approaches exactly half of the exact value, as $n\to\infty$ . Is there an intuitive explanation for this? In case you're interested, the exact value is $\dfrac{\binom{3n}{2n}\binom{2n}{n}2^{3n}}{\binom{6n}{2n}\binom{4n}{2n}}$ . Explanation: The denominator is the total number of ways to divide the $6n$ people
into three rooms. First, among the $6n$ people, we choose $2n$ people
to go to the first room, so $\binom{6n}{2n}$ . Then among the
remaining $4n$ people we choose $2n$ people to go to the second room, so $\binom{4n}{2n}$ . Then the remaining $2n$ people go to the third
room. The numerator is the total number of ways in which each couple is
separated. First, among the $3n$ couples, we choose $2n$ couples to
each be represented in the first room (one person from each couple),
so $\binom{3n}{2n}$ . Then we allocate the $n$ remaining couples into
the second and third rooms (each room getting one member of each of
these couples), so the second room must now get an additional $n$ people, and these are chosen among the $2n$ people whose spouse went
to the first room, so $\binom{2n}{n}$ . Then the remaining $n$ people
go to the third room. Then each couple has two ways of being separated among two rooms, so $2^{3n}$ . It can be shown algebraically or by Wolfram that my approximation, $\left(\frac{2}{3}\right)^{3n}$ , approaches exactly half of the exact value, as $n\to\infty$ . I'm looking for an intuitive explanation. (The introductory question was inspired by this question .)","['intuition', 'combinatorics', 'approximation', 'probability']"
4544181,MVUE problem related to splitting joint variables into independent ones.,"Hi please help me with this problem. With the random samples $X_1,\dots,X_n$ from $\operatorname{Exp}(\mu, \sigma)$ , I need to attain the MVUE of $\eta = \mathbb{P}(X_1>a)$ . I used the Lehmann-Scheffe and Basu theorem to get close to my goal. However, I can't attain the distribution of ancillary statistic. My solution so far is as below. We have CSS of $(\mu, \sigma)$ , $T = (X_{(1)}, \sum^n_{i=1}(X_i - X_{(1)})$ and since $\eta = \mathbb{E}_\theta\mathbb{I}(X_1>a)$ } by Lehmann-Scheffe, $$\eta^{MVUE} = \mathbb{P}(X_1>a|X_{(1)}=x, S=s) \text{ where 
$S=\sum^n_{i=1}(X_i - X_{(1)})$}\\
=\mathbb{P}\Big(\frac{X_1-X_{(1)}}{S}>\frac{a-x}{s}\Big)\qquad (\because \text{Basu})\\
=\begin{cases}
1 \ (a < x) \\
\mathbb{P}(\frac{X_1-X_{(1)}}{S}>\frac{a-x}{s}) \ (a\geq x)
\end{cases}\\
=\begin{cases}
1 \ (a < x) \\
\mathbb{P}(\frac{X_1-X_{(1)}}{S}>\frac{a-x}{s}, X_1 > X_{(1)}) \ (a\geq x)
\end{cases}\\
$$ Let us consider the distribution of the ancillary statistic from now. WLOG take $\mu=0, \sigma=1$ , and since $X_1 > X_{(1)}$ , $X_{(1)} = \min_{2\leq i \leq n}X_i = \min_{1\leq i \leq m}Z_i$ ,  where $Z_1,\dots,Z_m \sim \text{i.i.d. } \operatorname{Exp}(1)$ . Observe that $X_1, Z_{(1)}, \sum^m_{i=1}(Z_i-Z_{(i)})$ are independent by exponential spacing. Then we have $$
\frac{X_1-X_{(1)}}{S}=\frac{X_1 - Z_{(1)}}{X_1 - Z_{(1)} + \sum^m_{i=1}(Z_i-Z_{(1)})}
$$ As you can see by implementing this new $Z$ variable, the joint distribution we had which was dependent is now jointly independent. With the transformation of variables we also have $$
\operatorname{pdf}_{X_1-Z_{(1)}}(y) = \frac{m}{m+1}e^{-y}\mathbb{I}(y>0),\sum^m_{i=1}(Z_i-Z_{(1)}) \sim \Gamma(m-1,1)
$$ My gut is telling me that this is Beta Distribution. However, I can't figure out its parameters. Can anyone help me through this? Also, if anyone can think of some easier way please let me know. @StubbornAtoms answered the same question but I want to attain the ""specific"" beta distribution. But his solution explains why I split the variable of $X$ 's into $X$ 's and $Z$ 's.","['statistical-inference', 'statistics', 'probability-distributions', 'exponential-distribution', 'order-statistics']"
4544197,Shortest paths from A to B in a grid,"My task is to count number of different paths from A to B, moving only down or right so that path does not pass through walls on picture below. So, in both of these walls we have $4$ segments. For the top right one I tried to find all paths that pass through segment $i(i=1,2,3,4)$ . Let $A_i$ be number of paths that pass through segment $i$ for the top wall. Now, I've found cardinality of those $4$ sets, and got following results: $|A_1|$ = $5 \choose 4$ $9 \choose 4$ $|A_2|$ = $7 \choose 4$ $8 \choose 3$ $|A_3|$ = $8 \choose 5$ $7 \choose 3$ $|A_4|$ = $9\choose 5$ $5 \choose 4$ For the next part, using inclusion exclusion formula, I should find intersection of all combinations of $2,3$ and $4$ sets, but not sure how to do that. For the cardinalities above I first found path from A to starting point of segment, and then from end point of segment to B(without that segment). Any idea how to continue?","['combinatorics', 'discrete-mathematics']"
4544207,What is the distribution of the product of three random variables?,"From Wikipedia : If $X$ and $Y$ are two independent, continuous random variables, described by probability density functions $f_{X}$ and $f_{Y}$ then the probability density function of $Z=XY$ is \begin{equation}
f_{Z}(z)=\int_{-\infty}^{\infty}f_{X}(x)f_{Y}(z/x)\frac{1}{|x|}dx
\end{equation} Suppose now that we have another independent, continuous random variable $W$ , described by probability density function $f_{W}$ , what is the probability density function of $Z=XYW$ ? Is the following expression correct \begin{equation}
f_{Z}(z)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X}(x)f_{Y}\left(\frac{y}{x}\right)f_{W}\left(\frac{z}{y}\right)\frac{1}{|xy|}dxdy
\end{equation} ?","['statistics', 'probability-distributions', 'probability']"
4544270,"Show $\mu(A\cap B \cap C)\leq \min\{\mu(A\cap B),\mu(A\cap C),\mu(B\cap C)\}$","Problem: Let $(X,\mathscr{A},\mu)$ be a probability space. Let $A,B,C \in \mathscr{A}$ be sets which fulfills $$\mu(A\cap B)\geq \frac{3}{5}, \mu(A\cap C)\geq \frac{3}{5}, \mu(B\cap C)\geq \frac{3}{5}$$ Show that $$\frac{2}{5}\leq \mu(A\cap B \cap C)\leq \min\left\{\mu(A\cap B),\mu(A\cap C),\mu(B\cap C)\right\}.$$ Attempt: I have shown $$\frac{2}{5}\leq \mu(A\cap B \cap C).$$ But i'm having a real hard time starting on the other inequality. Any hints?","['measure-theory', 'probability-theory', 'inequality']"
4544326,Differentiating curves given by curves in a Lie group acting on a point,This is a problem I keep running into and I feel it is likely a really basic fact that I am forgetting/missing but I haven't been able to see a way through. Suppose I have a curve in a Lie group: $\varphi:I\to G$ and I let that act on a point $v$ in some representation $V$ of $G$ to make a curve there. $$\sigma := \varphi \cdot v : I \to V.$$ How can I write the derivative $\sigma'$ in terms of $v$ and $\varphi$ ? For simplicity I am happy if that representation is the adjoint one $V = \mathfrak{g}$ and conversely for more complexity I would love an answer for when $v$ is replaced by a map $I \to V$ . One guess might be to use the logarithmic derivative $\varphi^{-1} \cdot \varphi'$ in some way so we can get a curve in the Lie algebra to act on $v$ : $$ \varphi^{-1}(\varphi \cdot v)' = (\varphi^{-1} \cdot \varphi')\cdot v.$$ Both sides here are curves in $V$ but I can't see why such a thing would be true. Is there some approachable way of computing $(\varphi \cdot v)'$ here or is this a foolish endeavour?,"['lie-algebras', 'lie-groups', 'differential-geometry']"
4544360,Surface area of intersection of sphere and off-angle cone (Footprint of satellite),"A satellite has a telescope looking down at a section of the Earth, so its field of view is a cone with some angle φ. The telescope can slew any direction θ (non-perpendicular to Earth), and doing so will change the surface area in its field of view. What is the equation for that surface area? You can assume that the earth is a sphere of radius $r$ and the satellite is $h$ above the ground. Here is an illustration of the parameters as well as the surface area in light blue.","['integration', 'calculus', 'area', 'differential-geometry']"
4544384,How to solve this phasor question without some sort of approximation?,"Question: Write $2\cos(100t + \frac{1}{3}) - \sin(100t-1)$ in the form $A\cos(\omega t + \phi)$ . Find A, $\omega$ , $\phi$ . (Hint: Phasor approach
may simplify your task.) (Remark: Leave your answers exact as real numbers maybe in the form of
mathematical expressions, and do not attempt to approximate them using decimal representations.) My attempt: $$-\sin(100t-1)=\cos\left(100t - 1 + \frac{\pi}{2}\right)$$ Rewrite using this identity: $$\underbrace{2\cos\left(100t + \frac{1}{3}\right)}_{=z_1} + \underbrace{\cos\left(100t - 1 + \frac{\pi}{2}\right)}_{=z_2} $$ $z_1$ in phasor is $z_1 = 2 \angle{\frac{1}{3}rad}$ which is $z_1 = 2 \angle{19.0986^{\circ}}$ $z_2$ in phasor is $z_2 = 1 \angle{-1 + \frac{\pi}{2}rad}$ which is $z_2 = 1 \angle{32.7042^{\circ}}$ This is where I am struggling because in order to proceed I have to take the approximations otherwise how can I convert it to rectangular form in order to perform addition over these two complex numbers?","['trigonometry', 'approximation', 'complex-numbers']"
4544390,Limit with Arithmetico-geometric sequence,"It is given that $$ 
L=\lim _{k \rightarrow \infty}\left\{\frac{e^{\frac{1}{k}}+2 e^{\frac{2}{k}}+3 e^{\frac{3}{k}}+\cdots+k e^{\frac{k}{k}}}{k^2}\right\}
$$ I tried solving it but I am stuck on this, but it seems to be that numerator is an arithmetico-geometric sequence.
Solution to this problem was given something like this: $$s=-\dfrac{e^{\frac{1}{k}}(e-1)}{(e^{\frac{1}{k}}-1)^2}+\dfrac{ke^{1+\frac{1}{k}}}{e^{\frac{1}{k}}-1} \tag{1}\label{1}$$ where $s$ is sum of AGP series in the numerator.
So $$\begin{align}
   L &= \displaystyle\lim_{k \to \infty}  \dfrac{s}{k^2} \\
     &= -(e+1)+e \tag{2}\label{2}
\end{align}$$ So I am having difficulty in understanding \eqref{1} and \eqref{2}
Any other aliter solution and help is appreciated.","['limits', 'limits-without-lhopital', 'sequences-and-series']"
4544411,A Weighted Gaussian Inequality: $E[\frac{\sigma_n^2 x_n^2}{\sum_{i=1}^n \sigma_i^2x_i^2} ] \ge \frac{\sigma_n^2}{\sum_{i=1}^n \sigma_i^2}$,"Given $\sigma_1 \ge \dots \ge \sigma_n \ge 0$ ,
and independent random gaussian variables $x_1, \dots, x_n \sim \mathcal N(0,1)$ ,
I want to show: $$
\mathbb E\left[
\frac{\sigma_n^2 x_n^2}{\sum_{i=1}^n \sigma_i^2 x_i^2}
\right]
\ge \frac{\sigma_n^2}{\sum_{i=1}^n \sigma_i^2}.
$$ Note that this corresponds to taking the expectation of the numerator and denominator individually. Using Jensen's inequality I can show \begin{align}
\mathbb E\left[
\frac{x^2}{x^2 + z}
\right]
&=
\mathbb E\left[
\mathbb E\left[
\frac{x^2}{x^2 + z}
\mid x
\right]
\right]
\\&\ge
\mathbb E\left[
\frac{x^2}{x^2 + \mathbb E[z]}
\right]
\\&\approx
\frac{1}{1 + \sqrt{\mathbb E[z]} + \mathbb E[z]}.
\end{align} However, what I would need to be true is $
\mathbb E\left[
\frac{x^2}{x^2 + z}
\right] \ge \frac{1}{1 + \mathbb E[z]}
$ , and that certainly doesn't hold in general.
In particular it seems I need to somehow use that it's the smallest $\sigma_n$ that's in the numerator. The equivalent result with an arbitrary $\sigma_i$ doesn't seem to be true in general. It's also interesting to notice that in the simple case $n=2$ we get $$
\mathbb E\left[
\frac{\sigma_2^2 x_2^2}{\sigma_1^2 x_1^2 + \sigma_2^2 x_2^2}
\right]
= \frac{\sigma_2}{\sigma_1 + \sigma_2}.
$$ (At least Mathematica says this is true, I'd be interested in knowing a proof.)
Though that definitely doesn't hold for $n > 2$ . I suppose the equation $\sum_{i=1}^n \sigma_i^2 x_i^2 = 1$ corresponds to integrating over an ellipse, but I haven't found a nice geometric way to make use of that. I tried something else.
In the case $\sigma_1 = 1$ and $\sigma_2=\sigma_3$ , Mathematica can evaluate the expectation as $$
\frac{x_1^2}{x_1^2 + \sigma_2^2 (x_2^2 + x_3^2)}
=
\frac{1}{1-\sigma_2^2}+\frac{\sigma_2 \sinh ^{-1}\left(\sqrt{\sigma_2^2-1}\right)}{\left(\sigma_2^2-1\right)^{3/2}}.
$$ As expected this is below $1/(1+2\sigma_2^2)$ for $\sigma_2 > 1$ : Mathematica even finds an expression for the general case $E[\frac{x_n^2}{x_n^2+a \chi^2}]$ where $\chi^2$ is Chi-squared distributed with $n-1$ degrees of freedom.
So maybe there's a proof works by ""evening out"" the larger $\sigma$ values...
The bound with chi-squared isn't particularly pretty though... A statement equivalent to my inequality is that $$
\mathbb E\left[
\frac{x_n^2}{\sum_{i=1}^n p_i x_i^2}
\right]
\ge E\left[
\frac{x_n^2}{\frac{1}{n} \sum_{i=1}^n x_i^2}
\right],
$$ where $\sum_i p_i=1$ , and $p_1 \ge p_2 \ge \dots \ge p_n \ge 0$ .
Since $E\left[
\frac{x_n^2}{\sum_{i=1}^n x_i^2}
\right]=\frac1n$ by symmetry.
It might even be that all of this is true independent of $x_i$ being Gaussian, as long as they are IID.","['geometric-probability', 'conic-sections', 'normal-distribution', 'inequality', 'probability']"
4544455,"Finding pseudoinverse of ""non linearly-independent"" matrix","If: If $A$ has linearly independent columns, $A^+=\left(A^*A\right)^{-1}A^*$ If $A$ has linearly independent rows, $A^+=A^*\left(AA^*\right)^{-1}$ Otherwise, use the SVD decomposition. Is it possible to avoid using SVD decomposition? I've found the following method ( https://www.omnicalculator.com/math/pseudoinverse#how-to-calculate-the-pseudoinverse ): Start by calculating $AA^T$ and row reduce it to reduced row echelon form. Take the non-zero rows of the result and make them the columns of a new matrix $P$ . Similarly, row-reduce $A^TA$ and use its non-zero rows for the columns of the new matrix $Q$ . With your newly found $P$ and $Q$ , calculate $M=P^TAQ$ . Finally, calculate the pseudoinverse $A^+=QM^{-1}P^T$ . It works fine for most cases, but it doesn't work for $A=\left [ \begin{matrix}
    0&1&0&-i\\0&0&1&0\\0&0&0&0\end{matrix} \right ]$ . Is the method wrong?","['matrices', 'pseudoinverse']"
4544458,How to show there is an $S^4$ included in a simplicial complex?,"A $\mathbb{Z}_2$ -space is a pair $(T, \nu)$ , where $T$ is a topological space and $\nu: T \rightarrow T$ , called the $\mathbb{Z}_2$ -action, is a
homeomorphism such that $\nu \circ \nu= id_{T}$ . If $(T_1,\nu_1)$ and $(T_2, \nu_2)$ are $\mathbb{Z}_2$ -spaces, a $\mathbb{Z}_2$ -map
between them is a continuous mapping $f : T_1 \rightarrow T_2$ such that $f \circ \nu_1 = \nu_2 \circ f$ . The sphere $S^n$ is considered as a $\mathbb{Z}_2$ -space with the antipodal homeomorphism $x \rightarrow -x$ . A simplicial $\mathbb{Z}_2$ -complex is a simplicial complex $K$ with a simplicial map $\nu$ of $K$ into itself such that $\nu$ is a $\mathbb{Z}_2$ -action on $K$ . I am working on a huge 35-dimensional simplicial $\mathbb{Z}_2$ -complex that is very complicated. Let $B$ be this simplicial $\mathbb{Z}_2$ -complex with the $\mathbb{Z}_2$ -action $\nu$ . I want to show there is no (continuous) $\mathbb{Z}_2$ -map from $B$ to $S^3$ . In this text when we talk about an $S^4$ , we mean a ""hollow"" $S^4$ such that $\nu(S^4)=S^4$ . If a copy of $S^4$ (or any structure homeomorphic to an $S^4$ ) were included in $B$ , then by Borsuk-Ulam Theorem, there would not be any $\mathbb{Z}_2$ -map from $B$ to $S^3$ . I applied a discrete Morse function on $B$ and got $B'$ which is simpler than $B$ , and homotopy equivalent to $B$ . Since $B$ and $B'$ are homotopy equivalent, their homology groups $H_k$ are isomorphic and their Betti numbers $b_k$ are equal: $$ H_k(B') \simeq H_k(B) \quad b_k(B')=b_k(B).$$ I am thinking to compute the homology group $H_4(B')$ and the Betti number $b_4(B')$ to get $H_4(B)$ and $b_4(B)$ , and prove there exists a copy of $S^4$ included in $B$ . I am not sure what homology group or Betti number show there exists an $S^4$ in $B$ . I guess that if I get $b_4(B)\neq 0$ , I could conclude there is an $S^4$ included in $B$ ; or if $H_{4}(B)\neq 0$ and $b_4(B)= 0$ , I could conclude there is no $S^4$ included in $B$ but still there is no $\mathbb{Z}_2$ -map from $B$ to $S^3$ . Am I right? Are there any other ideas? My question is mostly about the homology groups. How do I realize that there is an $S^4$ in $B$ from its homology group (or Bettie number)?","['morse-theory', 'algebraic-topology', 'discrete-mathematics', 'simplicial-complex']"
4544481,Probability that a random element of a group has order $2$ [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question If $G$ is a finite group, let $\tau(G)$ be equal to probability that a random element $x \in G$ satisfies $x^2=1$ , ie. $\tau(G) = \frac{|\{x \in G :\ x^2=1\}|}{|G|}$ . Can we say something about $\tau$ ? My intuition about the range of possible results we could possibly obtain here comes from another problem: taking two elements of $G$ at random, let $c(G)$ be the probability that they commute. It is quite widely known that if $c(G) > \frac{5}{8}$ , then $G$ is abelian and $c(G)=1$ . More recently it was proven that the set of possible values of $c(\cdot)$ is nowhere dense, has no irrational limit points, and is well-ordered by $>$ . We can state more or less the same problems in our case: a) Is there a constant $c<1$ such that $\tau(G)>c$ implies $\tau(G)=1$ ? b) Is the set of values of $\tau$ nowhere dense? Does it have any infinite increasing sequence (okay, this is essentially the same as well-order)? c) Is the set of values of $\tau$ closed? Parts b, c) look like they might be hard, and I don't expect anyone to write a serious paper in answer to this post, so two more questions, less specific and hopefully easier to answer. d) Are there any sources of examples of groups (or families of groups) with high value of $\tau$ ? Of course $\tau(G)=1$ if and only if $G \simeq \mathbb{Z}_2^k$ . We have $\tau(G \times H) = \tau(G) \cdot \tau(H)$ , and $\tau(\mathbb{Z}_n) = \frac{1}{n}$ for odd $n$ , $\frac{2}{n}$ for even; in particular for abelian groups other than $\mathbb{Z}_2^k$ we have $\tau(G) \leqslant \frac{1}{2}$ . We get better efficiency with dihedral groups: we always have $\tau(D_n) > \frac{1}{2}$ , although half is the limit with $n \to \infty$ . Groups such as $\operatorname{Aut}(D_8)$ also give a half. Therefore it seems like a good bound for a conjecture of the following kind: e) For every $\varepsilon > 0$ there is an integer $N$ , such that $\tau(G) \geqslant \frac{1}{2} + \varepsilon$ and $|G| \geqslant N$ imply $G \simeq H \times \mathbb{Z}_2$ for some group $H$ . (As $-\times \mathbb{Z}_2$ does not change $\tau$ , this means we have essentially finitely many examples of $G$ giving values bigger than $1/2 + \varepsilon$ , and in particular there are finitely many such values.)","['combinatorial-group-theory', 'group-theory', 'probability']"
4544497,"Is $p(x) = \max(p_1(x), p_2(x))$ a seminorm when $p_1, p_2$ are seminorms?","To start: I've shown that yes, $p(x) = \max(p_1(x), p_2(x))$ is a seminorm if $p_1, p_2$ are seminorms. But I did that by proving absolute homogeneity and the triangle inequality directly. What I'd like to know: Is there some broader theoretical result that could be used instead? By way of comparison: if $\pi_i(x_1, x_2) = x_i$ are projection operators and $T$ is a linear transformation, then I know that $q(x) = p_1(\pi_1(Tx)) + p_2(\pi_2(Tx)))$ is a seminorm without directly proving the seminorm axioms because each $p_i(\pi_1(Tx))$ is a seminorm (as the composition of a seminorm and a linear operator), so that $q$ is the sum of seminorms. Is there some similar theorem(s) I could use for $p$ ?","['hilbert-spaces', 'normed-spaces', 'functional-analysis']"
4544525,Smallest constant $C$ to bound $\mathbb{E}[\tfrac{1}{\overline{X}_n + 1}] \leq C~\tfrac{1}{\mathbb{E}[X] + 1}$?,"Let $X_1, \dots, X_n$ be random real-valued random variables in the interval $[0, a]$ . Assume they are independently and identically distributed. Let $\mu = \mathbb{E}[X_i]$ denote their common mean. Define their average $\overline{X}_{n} = n^{-1} S_n$ where $S_n = \sum_{i=1}^n X_i$ . Let $f(t) := (t + 1)^{-1}$ . Question: What is the smallest constant $C = C(a, n) \geq 1$ such that we have $$
\mathbb{E}[f(\overline{X}_n)] \leq C~f(\mu)?
$$ It should be emphasized that the constant $C$ is universal: it is valid for any law of $X_i$ , supported on $[0, a]$ . It should be dependent only on $a, n$ . Comments: Necessarily $C \geq 1$ . Note that by Jensen's inequality, we have the following inequality, $
\mathbb{E}[f(\overline{X}_n)] \geq f(\mu), 
$ since $f$ is a convex function on the nonnegative reals. Additionally, I claim that $C \leq 1 + \tfrac{a}{n}$ . I have a proof of this below. (I would also be interested if there is a simpler way to establish this. I tried, thinking I could possibly improve the constant, to obtain it by Taylor expansion of the function $f$ around $t = \mu$ , but failed to recover it.) Let $X_{n+1}$ be independent of $\{X_i\}_{i=1}^n$ , but identically distributed. Then \begin{multline*}
\mathbb{E} [f(\overline{X}_n)] = \frac{n}{\mu} 
\mathbb{E} \Big[\frac{X_{n+1}}{S_n + n}\Big] =
\frac{n}{\mu} 
\mathbb{E} \Big[\frac{X_{n+1}}{S_{n+1} + n} \frac{S_{n} + n + X_{n+1}}{S_{n} + n}\Big]
\\\leq \Big(1 + \frac{a}{n}\Big) \frac{n}{\mu} 
\mathbb{E} \Big[\frac{X_{n+1}}{S_{n+1} + n} \Big].\qquad \mbox{(1)}
\end{multline*} Since $X_{j}$ , $j \leq n + 1$ are exchangeable, we also have $$
\mathbb{E} \Big[\frac{X_{n+1}}{S_{n+1} + n} \Big]
= \frac{1}{n+1} 
\mathbb{E} \Big[\frac{\overline{X}_{n+1}}{\overline{X}_{n+1} + n/(n+1)} \Big]
\stackrel{{\rm (*)}}{\leq}  \frac{\mu}{(n+1)\mu + n} \leq \frac{\mu}{n} \frac{1}{\mu + 1}. \quad \mbox{(2)}
$$ Above, we have used Jensen's inequality in (*) with mapping $z \mapsto z/(z + n/(n+1))$ which is concave on the nonnegative reals. Combining bounds (1) and (2), we get a bound $C(a, n) \leq 1 + a/n$ .","['expected-value', 'inequality', 'probability']"
4544549,Are X and Y independent if the $MGF_{X+Y} = MGF_X MGF_Y$?,"I know that when two random variables $X$ and $Y$ are independent, then the MGF of $X+Y$ is $$
MGF_{X+Y}(t) = \mathbb{E}[e^{t(X + Y)}] = \mathbb{E}[e^{tX}e^{tY}] = MGF_{X}(t)MGF_{Y}(t)
$$ However, is the reverse true? If we know that $MGF_{X+Y}(t) = MGF_{X}(t)MGF_{Y}(t)$ , then are $X$ and $Y$ necessarily independent?","['independence', 'moment-generating-functions', 'probability-theory', 'probability', 'random-variables']"
4544561,Check if this series is convergent or not,"I've been going crazy for a long time in determining if this series converges or diverges. Most likely it converges. $\displaystyle \sum_{n=1}^\infty \left(\frac{n}{2}\, \sin\frac{1}{n}\right)^\frac{n^2+1}{n+2}$ I am stuck in the necessary condition. I tried to solve in this way : $\displaystyle \frac{1}{2}\lim_{n \to \infty} \left(\frac{\sin\frac{1}{n}}{\frac{1}{n}}\right)^\frac{n^2+1}{n+2} = \frac{1}{2}\lim_{n \to \infty} (1)^\infty$ To solve this indeterminate form, I tried to use the exponential in this way $\displaystyle  \frac{1}{2}\lim_{n \to \infty} \left(e^{\frac{n^2+1}{n+2}log \left(\frac{sin \frac{1}{n}}{\frac{1}{n}}\right)}\right)$ In this way I always get an indeterminate form $\displaystyle \frac{1}{2}\lim_{n \to \infty} (e^{\infty *0})$ could you kindly give me support for the resolution of the character of this series?","['limits', 'convergence-divergence', 'summation', 'sequences-and-series']"
4544567,Intuition behind writing the Laplacian operator in different ways: $\nabla^2 u$ versus $\nabla \cdot \nabla u$ [duplicate],"This question already has answers here : Intuitive interpretation of the Laplacian Operator (7 answers) Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved I was looking over Langtangen's book on finite volume methods, and he mentions in section 5.5.1 that the second order or Laplacian term in a PDE can be written either as: $$
\nabla^2 u \quad \text{or} \quad \nabla \cdot \nabla u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}
$$ Both of these notations are analytically equivalent, but the $\nabla \cdot \nabla u$ approach seems to focus on this idea of the divergence of the gradient. I was just trying to understand the physical intuition behind defining the Laplacian as the divergence of the gradient? I understand that the Laplacian operator represents diffusion and that it looks to reduce the deviation between a point and its neighboring points. But I was not clear on how this idea relates to the ideas of divergence--which relates to flux, and gradient--which describes how a scalar valued function is changing at a point given its variables.","['divergence-operator', 'notation', 'multivariable-calculus', 'intuition', 'partial-differential-equations']"
4544575,Maximize disc in polygon,"The question is as follows:
Four coordinates $(0,0)$ , $(1,4)$ , $(4,2)$ and $(3,0)$ create a polygon. What is the largest disc that can fit within the polygon? I have a formula for calculating the maximum distance ( $d$ ) from the center point $(x_1,x_2)$ , but I´m not sure if it is correct or how to get a reliable answer. $d=\left|\frac{x_2-ax_1-b}{\sqrt{a^2+1}}\right|$ Where $y=ax+b$ are the four sides of the polygon described as lines.","['optimization', 'geometry']"
4544628,Explicitly Computing Structure Sheaves For Quotients of Polynomial Rings,"I'm working through an introductory problem in scheme theory and just want to make sure I'm understanding the concepts correctly.  The problem states to consider the following two rings, specify their Zariski open sets, as well as the restriction maps: $A = \mathbb{C}[t]/\left (t^2-t\right) = \{a+bt \; | \; a,b\in\mathbb{C}\}$ $B = \mathbb{C}[t]/\left (t^3-t^2\right) = \{a+bt+ct^2 \; | \; a,b,c\in\mathbb{C}\}$ For ring $A$ we can compute $X_A = \text{Spec} A = \left \{\widetilde{(t)}, \widetilde{(t-1)}\right \}$ where the $\sim$ is to signify that we are really dealing with the quotients $\widetilde{(t)} = (t)/\left (t^2-t\right )$ .  I arrived at this since prime ideals in a ring $R/I$ are projections of prime ideals of $R$ containing $I$ .  Furthermore, I concluded that the Zariski topology on $X_A$ is discrete since both prime ideals can be written as the zero set of an ideal (namely itself) in $A$ .  At the risk of overdoing the notation I'll call the open sets $U_t$ and $U_{t-1}$ , then the structure sheaf elements should be \begin{eqnarray*}
\mathcal{O}(U_t) & = & A_{\widetilde{(t)}} \;\; =\;\; \left \{\left .\frac{a+bt}{c+dt} \;\right | \; c+dt \not \in \widetilde{(t)}\right \} \\
\mathcal{O}(U_{t-1}) & = & A_{\widetilde{(t-1)}} \;\; =\;\; \left \{\left .\frac{a+bt}{c+dt} \;\right | \; c+dt \not \in \widetilde{(t-1)}\right \}.
\end{eqnarray*} Is my logic more or less correct?  We as well have that $U_t \cap U_{t-1} = \emptyset$ so we need not worry about the sheaf element in that case. For ring $B$ much of my logic remains the same in that $X_B = \text{Spec} B = \left \{\widetilde{(t)}, \widetilde{(t-1)}\right \}$ , this is discrete as well for similar reasons, and the only difference is in the forms of the structure sheaf elements: \begin{eqnarray*}
\mathcal{O}(U_t) & = & B_{\widetilde{(t)}} \;\; =\;\; \left \{\left .\frac{a+bt+ct^2}{d+ft+gt^2} \;\right | \; d+ft+gt^2 \not \in \widetilde{(t)}\right \} \\
\mathcal{O}(U_{t-1}) & = & B_{\widetilde{(t-1)}} \;\; =\;\; \left \{\left .\frac{a+bt+ct^2}{d+ft+gt^2} \;\right | \; d+ft+gt^2 \not \in \widetilde{(t-1)}\right \}.
\end{eqnarray*} I'm studying algebraic geometry for the first time out of Bosch's Algebraic Geometry and Commutative Algebra , and I also don't have much of an algebra background aside from groups, Lie groups, and basic ring theory. Response to Comments I think I caught KReiser's comment about the denominators in ring $B$ being incorrect.  In response to the comments about rationalization, for ring $A$ we can take $$
\frac{a+bt}{c+dt} \;\; =\;\; \frac{at+bt^2}{ct+dt^2} \;\; =\;\; \frac{at+bt}{ct+dt} \;\; =\;\; \frac{a+b}{c+d}
$$ which means that both sheaf elements in ring $A$ are just copies of $\mathbb{C}$ .  So can we conclude that $\mathcal{O}(U_t) \cong \mathbb{C}$ as well as $\mathcal{O}(U_{t-1}) \cong \mathbb{C}$ ? Moving on to ring $B$ we can similarly compute $$
\frac{a+bt+ct^2}{d+ft+gt^2} \;\; =\;\; \frac{at^2 + bt^3 +gt^4}{dt^2+ft^3+gt^4} \;\; =\;\; \frac{a+b+c}{d+f+g}
$$ so, do we get the same result for ring $B$ ?","['algebraic-geometry', 'solution-verification', 'schemes', 'sheaf-theory']"
4544637,If $f(z) = \sum_{n=0}^{\infty} a_n(z-z_0)^n$ has radius convergence $ R > 0$. if $f(z) = 0$ for all $z$ $|z-z_o| < R$ show that $a_0 = a_1 = ... =0$.,"If $f(z) = \sum_{n=0}^{\infty} a_n(z-z_0)^n$ has a radius of convergence $R > 0$ and if $f(z) = 0$ for all $z$ $|z-z_o| < R$ show that $a_0 = a_1 = ... =0$ . Proof Attempt: If it has a radius of convergence $R > 0$ then I know $\frac{1}{R} = \lim_{n \rightarrow \infty} |\frac{a_{n+1}}{a_n}| > 0$ . Pick $z^*$ to be in the radius of convergence, then we know that : $$ 0 = a_0 + a_1(z^*-z_0) + ...+a_n(z^*-z_0)^n + ...$$ Which means that for some $a_i \rightarrow 0 \leq i \leq n$ we can write: $$a_i = (z^* - z_0)^{-i} (-a_0 - a_1(z^* - z_0) - ...-a_{i+1}(z^* - z_0)^{i+1} -....-a_n(z^*-z_0)^n) + ....$$ $$a_{i+1} = (z^* - z_0)^{-(i+1)} (-a_0 - a_1(z^* - z_0) - ...-a_{i+2}(z^* - z_0)^{(i+2} -....-a_n(z^*-z_0)^n). + ...$$ I want to setup now a contradiction, by using the fact that  as $i \rightarrow 0$ then $  | a_{i+1}/a_i | > 0 $ : $$ \lim_{i \rightarrow \infty}| \frac{a_{i+1}}{a_i} |  = (z^* - z_0)^{-1}\frac{(-a_0 - a_1(z^* - z_0) - ...-a_{i+2}(z^* - z_0)^{i+2} -....-a_n(z^*-z_0)^n). + ...)}{(-a_0 - a_1(z^* - z_0) - ...-a_{i+1}(z^* - z_0)^{i+1} -....-a_n(z^*-z_0)^n) + ....)} $$ and I am having troubles concluding anything from this ... can someone suggest maybe another way? Attempt II I am attempting a solution based on $f(z) = f'(z) = ... = f^k(z) = 0$ $$(1) \ a_0 + a_1(z-z_0) + a_2(z-z_0)^2 + a_3(z-z_0)^3 + ... = 0 $$ $$(2) \ a_1 + 2a_2(z-z_0) + 3a_3(z-z_0)^2 + ... = 0 $$ $$ (3) \ 2a_2 + 3\times 2a_3(z-z_0) + ... = 0 $$ Take $(2) \times (z-z_0)$ : $$(2) \ a_1(z-z_0) + 2a_2(z-z_0)^2 + 3a_3(z-z_0)^3 + ... = 0 $$ Subtract it from (1)? $$a_0 - [a_2(z-z)^2 + 2a_3(z-z_0)^3 + ... ] = 0 $$ Not sure where to go with this...",['complex-analysis']
4544656,Demonstrating non-differentiability with absolute value equations.,"I have a function $f(x)=|x^2-4|$ . I am able to use the following definition of absolute value to show that $f(x)$ is non-differentiable at $x=\pm2$ $|x| =
\begin{cases}
x, & x\geq0 \\
-x, & x<0
\end{cases} $ . When I use that definition I find that $\lim \limits_{x \to -2-} f'(x)=-4$ and $\lim \limits_{x \to -2+} f'(x)=4$ However, I am not able to show that with the following definition for the absolute value: $|x|=\sqrt{x^2}$ . $|x^2-4|=\sqrt{(x^2-4)^2}$ $\lim \limits_{x \to -2} \frac{\sqrt{(x^2-4)^2}-\sqrt{((-2)^2-4)^2}}{x-(-2)}$ $=\lim \limits_{x \to -2} \frac{\sqrt{(x^2-4)^2}-0}{x+2}$ $=\lim \limits_{x \to -2} \frac{(x^2-4)^2}{(x+2)^2}$ $=\lim \limits_{x \to -2} \frac{(x+2)^2(x-2)^2}{(x+2)^2}$ $=\lim \limits_{x \to -2} (x-2)^2$ $=((-2)-2)^2=16$ I have bad fundamentals. So, I apologize for missing something basic. Thank you for your help in advance.","['limits', 'calculus', 'derivatives', 'absolute-value']"
4544669,Integration by parts on dot products of two vectors,"How can we take the integration by parts of the following, $$ \int_{\mathcal{V}} (\nabla a) \cdot (\nabla b) \;d\tau$$ Where one can also consider $a$ as a scalar over $\mathcal{V}$ . I'm not sure how to approach doing integration by parts for a dot product of two gradient vectors. How does one approach this integral?","['multivariable-calculus', 'vector-analysis']"
4544682,"Given n squares, in how many ways can they be contiguously arranged into a single shape?","Context: At school today, a friend of mine ripped up a piece of paper he had written on. I was considering the number of possible ways I could rearrange these individual pieces to try to reconstruct the whole, and the runtime complexity of such a de-rearrangement. However, that’s a question for the theoretical CS StackExchange, not here. What I’m interested in is a more simplified problem: If some polygon in a Euclidean space which only contains right-angles is cut into equally-sized individual squares, into how many possible configurations can these squares be rearranged, allowing only rotation and translation, and no reflections (since a reflection of an actual piece of paper would imply flipping over whatever text is written on it)? I thought of using graph theory to analyze this–that is, each square node can be in 4 possible states, because it can be rotated into 4 separate states ( $4^{n}$ possible rotational states overall) and each node can have at most 4 edges extending from it (since equally-sized squares can touch at most 4 others). However, note that if you have square-nodes A, B, and C, and they are connected in a line from left to right like ABC, that is a different configuration from CBA even though each individual letter has an edge connecting it to the same other nodes, which doesn’t align with the way that graph theory equates any two such graphs regardless of their physical representation. ABC is the same thing, however, to CBA with each node rotated twice if I’m not mistaken, if you wish to think of all rotations of the overall structure as the same thing. I’m thinking that maybe each edge can also be labeled with a number 1-4 that represents the direction in which the contact between the two squares occurs, but at this point I’m stuck because I have no formal training in graph theory.
This certainly seems like a problem within combinatorics, in which I have very minor experience (I learned a little bit for Academic Decathlon last year).
I think the problem will be more easily represented by looking at a few cases. Case 1: 1 square Result: Either 1 or 4 ways, depending on whether you wish to consider rotations of the entire structure to be the same thing. From now on, for simplicity’s sake, I will say that every rotation of the entire structure is a separate structure. Case 2: 2 squares Result: Let’s call these squares A and B. A can connect to B’s left, right, top, or bottom side. A being on B’s left is the same as B being on A’s right. There are thus 4 connection states, and for each square, there are 4 rotation states. That’s $4^2 = 16$ rotation states overall, multiplied by the 4 connection states, giving $4^3 = 64$ states overall. Case 3: 3 squares Result: Let’s call these squares A, B, and C. I think that each outcome might be able to be represented by a tree, where at each point I choose whether to include or exclude a square from the next side. Each time I make a choice to include another square, I have one fewer squares to choose from. I must, however, choose every square, because I’m not making a discontinuous or incomplete shape here.
Note that if two squares touch the same other square, they do not touch each other, so contact is never transitive. However, if the difference between the sides chosen on a given square (A) for two other squares (B and C) has an absolute value of 1, then a fourth square (D) is able to touch both B and C. This makes a tree very confusing to deal with. So, at square A, I can choose either B or C to go on either side 1, 2, 3, or 4. Once I place 1 of those 2 squares, I have only 1 square left to choose from that can go on 1 of the 3 remaining sides of the other two squares, giving me 6 choices. If I choose B the first time around, then that gives me 4 possible position-configurations, and then after placing C I can actualize the next 6 possible configurations one step down the line. This gives $4*6 = 24$ possible position configurations when choosing B first. The same follows with C, meaning that there are actually $4*6*2 = 48$ possible position-configurations. With the 4 possible rotations for each square, I have $4^3 = 64$ rotation-states for the figure, giving $64*48 = 3,072$ possible configuration states for the three squares in the image. If, hypothetically (though not applicable in my usage case), we were allowed to reflect each of these squares as well, you would have to consider the 4 axes of symmetry on each of the squares, giving another $4^3 = 64$ reflection states for the shape, which when multiplied by our previous $3,072$ states gives $196,608$ possible states for the image. I’m not sure which sets of rotations or reflections would be equivalent (I’d have to learn some group theory to predict that), but that’s a lot for 3 squares. Case 4: 4 squares Results:
This one is a bit harder. Since the first three squares can be placed in an L-shape, the fourth square can be placed to fill in the corner, which can be reached two separate ways by placing the square on one side of two separate squares respectively. I don’t know how to calculate this one. Overall:
I think it would be nice if anyone could tell me of a more general view of this problem and its solution (for example, using any polygon as the decomposed shape which can be arranged into a gapless tiling like a triangle, square, or hexagon, and considering the options based on its number of sides and its number of symmetries). Edit: I just realized that in the case of the three squares, I can also perform a similar process of choice starting with each of the three squares individually rather than the others. This means that I have a separate set of $3,072$ configurations starting with each square, giving me $3*3072 = 9,216$ arrangements with rotation, and $9,216*64 = 589,824$ reflection-allowing states.","['puzzle', 'mathematical-modeling', 'graph-theory', 'geometry', 'combinatorics']"
4544694,A basic question about concave function.,"Question: Let $f: \mathbb{R}_{+}^{n} \rightarrow \mathbb{R}$ be a concave
function satisfying $f(0)=0 $ . Show that for all $k \geq 1 $ we have $k f(x) \geq f(k x) $ . What happens if $ k \in[0,1)  ?$ I know that when $ k \in [0,1)$ , by the definition of concave function and the condition given, $f(0) = 0$ , we can show that $kf(x) \leq f(kx)$ . But when $k \geq 1,$ I don't know how to prove this conclusion.",['functions']
4544710,Combinatorics Problem: Basketball Team with Players that Can Play any Position,"Problem. Suppose you are to choose a basketball team of 5 players from 12 available athletes. How many ways can you choose a team composed of 2 guards, 2 forwards, and a center? Solution. The way I understood the problem is that any player can be delegated to either of the three positions. There is no restriction to how many players we can only delegate to a certain position. So I assumed that the only concern I have is not over-counting the players I have already chosen for a certain role in a team of 5. Thus $$\binom{12}{2}\binom{10}{2}\binom{8}{1}=23,760 \:\text{ways}$$ Is this correct?","['solution-verification', 'combinatorics']"
4544738,Normalizing Flow Penalization,"I am looking to fit a normalizing flow, specifically a Masked Autoregressive Flow model. However, this model leads to high variance on lower dimensional, less complex data. I am using a neural network to parameterize the scale and shift components of the MAF. I think we need to smooth the loss surface, i.e., the log likelihood of the normalizing flow. Is there any literature outlining good ideas or what would be a good starting point? More generally, are there any papers that have a solid theoretical foundation to defining a penalty that can smooth something like a normalizing flow?","['machine-learning', 'optimization', 'statistics']"
4544742,Is it possible to prove the derivative of $e^x$ is $e^x$ using the limit definition of $e$ without using binomial expansion?,"I am teaching my students about the derivative of $e^x$ . I have walked through what is in our textbook and they have all happily believed me, but I would like to have a better explanation for why the derivative is $e^x$ . I know several other proofs exist like those that define $e$ using the slope at $0$ and those that use the natural log, but I'd like my proof to closely follow the method used in the textbook.  That is why this question and this question have not answered all of my questions. Here is what the textbook says. I understand everything until they decide to let $e^{\Delta x}\approx 1 + \Delta x$ . My question is why do they not directly use the limit definition of $e$ . Is it because they can't without introducing students to binomial expansion? I put $e^{x}$ outside the limit and then tried to substitute the definition of $e$ . But I think I made a mistake. Here is my work: $$e^{ x} \lim_{\Delta x\to\ 0} \frac{(\lim_{\Delta x\to\ 0}( 1+ \Delta x)^{\frac{1}{\Delta x}}) ^{\Delta x}-1}{\Delta x}$$ Using limit properties I know I can rewrite this as: $$e^{x} \lim_{\Delta x\to\ 0} \frac{(\lim_{\Delta x\to\ 0}( 1+ \Delta x))-1}{\Delta x}$$ This is where I get confused/stuck. If I resolve the inner limit first, I get: $$e^{x} \lim_{\Delta x\to\ 0} \frac{1-1}{\Delta x}$$ $$e^{x}\lim_{\Delta x\to\ 0} \frac{0}{\Delta x}$$ $$e^{x} \times 0 = 0$$ Is there a property of limits that would allow me to ""get rid"" of that inner limit?","['proof-explanation', 'calculus', 'derivatives', 'exponential-function']"
4544756,How to construct the normalized set of random variables?,"I have read the paper Narrow Reliability Bounds for Structural Systems . The author stated on page 458 that there exists a transformation such that a $n$ -set of random variables $X=(X_1,...,X_n)$ can be normalized. Here, the term 'normalized' means transforming $X$ into a set of variables $Y=(Y_1,...,Y_n)$ with all means equal to zero, all standard deviations equal to one, and all covariances equal to zero. The author further stated that It is well known from the general theory of random variables that such a transformation exists if the covariance matrix of $X$ is regular. $X$ in the above quotation is the original $n$ -set of random variables before normalization. I was wondering how to construct such a transformation to make all covariances equal to $0$ . Besides, I was also wondering what the term ' regular ' means in the above quotation. Thank you for your kind help.","['probability-theory', 'probability', 'random-variables']"
4544760,Help needed in finding the area of the square (tricky geometry question),"I saw this question posted on Twitter , and I can't seem to find the answer to this - I am stuck with this puzzle for over 2 hours now. How would one go about trying to solve this and can anyone explain it in a simple way that anyone can understand.","['recreational-mathematics', 'geometry']"
4544765,How to find the value of the given limit with summation?,"It was asked to find the value of the following limit: $$
\lim _{n \rightarrow \infty} \frac{96}{n^4}\left[1\left(\sum_{k=1}^n k\right)+2\left(\sum_{k=1}^{n-1} k\right)+3\left(\sum_{k=1}^{n-2} k\right)+\cdots+n \cdot 1\right]
$$ Here's my try on it: Let the general $m^{th}$ term of the series be $$\begin{align}t_m &= m \left( \displaystyle \sum_{k=1}^{n-m+1}k \right)\\
                   &= m\ .\dfrac{(n-m+1)(n-m+2)}{2}
\end{align}$$ So, $$  \begin{align}
\sum_{n=1}^n t_m &=\sum_{m=1}^n \frac{m(n-m+1)(n-m+2)}{2} \\
                 &=\sum_{s=0}^{n-1} \frac{(s+1)(n-s)(n-s+1)}{2} \end{align}
$$ From here I have no clue how to proceed further with this series. Thanks for any help.","['limits', 'sequences-and-series']"
4544777,How to show states are recurrent/transient?,"Let $\{X_n\}$ be a time-homogeneous Markov chain on $\mathbb{N}$ with $P(X_{2}=2 \,|\, X_1 = 1) =  1$ , $P(X_2 = 1 \,|\, X_1 = n) = \frac{1}{n^\alpha}$ , and $P(X_2 = n+1 \,|\, X_1 = n) = 1-\frac{1}{n^\alpha}$ for $n \geq 2$ . How can show that all states are transient when $\alpha > 1$ and all states are recurrent when $\alpha = 1$ ? I would think maybe you need an additional requirement that maybe we need irreducible chain but maybe this is not needed. I am told to conisder $\tau = \inf\{n:X_n=1\}$ , but do not know how to use $\tau$ . Edit: I can show $P_n(\tau = k) = f_k(n,1)$ with $f_k(n,1) = P(X_1 \neq 1, \dots, X_{k-1} \neq 1,X_k=1 \,|\, X_0 = n)$ . This can show that $$\sum_{k=1}^\infty kf_k(n,1) = \sum_{k=1}^\infty P_n(\tau \geq k).$$","['markov-chains', 'real-analysis', 'markov-process', 'probability-theory', 'probability']"
4544787,Is there a closed form expression for $\sum_{1\le i_1 \le i_2 \le \dots \le i_k \le n} i_1 i_2 \dots i_k$?,These sums showed up in a probability problem I was working on. They're not quite the Stirling numbers of the first kind since it's possible to have e.g. $i_1 = i_2$ . Denoting the sum by $(k\mid n)$ we have the recurrence relation $(k\mid n) = n(k-1\mid n) + (k\mid n-1)$ I thought maybe the average term of the sum would be similar to the average product over a random list of $k$ numbers from $1$ to $n$ . But it seems to always be a few times greater. I'm beginning to despair of there being a closed form expression for $(k\mid n)$ in terms of factorials and powers. Does anyone know how to analyze the sum further and perhaps find a good approximation?,"['summation', 'combinatorics', 'stirling-numbers']"
4544790,"Showing that $H_n G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)$ if $H_n H_n' = I$ and $G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)$","Suppose $A_n,B_n$ are invertible symmetric square matrices. Define $C_n := A_nB_nA_n$ so $C_n$ is symmetric and invertible as well. Define $C_n^{-1/2}$ to be such that $C_n^{-1} = C_n^{-1/2} C_n^{-1/2}$ , where $C_n^{-1/2}$ is symmetric (note that this is always possible by diagonalizing $C_n^{-1}$ since it is symmetric). Define $H_n:=C_n^{-1/2} A_n B_n^{1/2}$ , noting that $H_n H_n' = C_n^{-1/2} A_n B_n^{1/2} B_n^{-1/2}A_nC_n^{-1/2} = C_n^{-1/2} C_n C_n^{-1/2} = I$ , where $I$ is the identity matrix. Suppose $G_n \overset{d}{\rightarrow} \mathcal{N}(0,I),$ $\textbf{How do I show that $H_n G_n \overset{d}{\rightarrow} \mathcal{N}(0,I)$?}$","['matrices', 'probability-theory', 'symmetric-matrices']"
4544902,"Spivak, Ch. 20, Problem 15: Prove that if $x\leq 0$, then the remainder term $R_{n,0}$ for $e^x$ satisfies $|R_{n,0}|\leq \frac{|x|^{n+1}}{(n+1)!}$.","The following is a problem from Chapter 20 of Spivak's Calculus Prove that if $x\leq 0$ , then the remainder term $R_{n,0}$ for $e^x$ satisfies $$|R_{n,0}|\leq \frac{|x|^{n+1}}{(n+1)!}$$ My question is about the solution in the solution manual, which I show below. First let me show my own attempt at solving this problem. If $x=0$ , both sides are $0$ . Assume $x\lt0$ . $$e^x=\sum\limits_{i=0}^n \frac{x^i}{i!}+\frac{e^t}{(n+1)!}x^{n+1}, \quad t\in (x,0)$$ We know that for $t<0$ we have $0<e^t<1$ . Thus, $$|R_{n,0,e^x}(x)|=\frac{e^t}{(n+1)!}|x|^{n+1}<\frac{|x|^{n+1}}{(n+1)!}$$ Is my attempt correct? When I looked at the solution manual, however, I was slightly bewildered. Here is what it has $$\left | \int_0^x \frac{e^t}{n!}(x-t)^n dt \right |= \int_x^0 \frac{e^t}{n!} |x-t|^n dt $$ $$\leq \int_x^0 \frac{|x-t|^n}{n!} dt, \text{since } e^x\leq 1 \text{ for } x\leq 0$$ $$=\frac{|x|^{n+1}}{(n+1)!}$$ Why is the solution manual using an integral, and what is the expression $\frac{e^t}{n!}(x-t)^n$ ? It looks like a remainder, but I don't understand the $t$ in the $x-t$ factor that is the same as the exponent in $e^t$ .","['integration', 'proof-explanation', 'calculus', 'taylor-expansion', 'derivatives']"
4544906,How to calculate infinite limit for function and its derivative? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question How to do this question? Tried separating the limit to $$\lim_{x \rightarrow +\infty} f(x) + 2 \lim_{x \rightarrow +\infty} f'(x) + \lim_{x \rightarrow +\infty} f""(x) = k$$ but seems not working","['limits', 'calculus']"
4544959,Prerequisites for Basic Mathematics by Serge Lang,"I am trying to self learn mathematics and I found out about Basic Mathematics. What are the prerequisites for this book? Should I read a book on mathematical proofs? I know some very basic math like arithmetic, exponents and fractions.","['self-learning', 'algebra-precalculus']"
