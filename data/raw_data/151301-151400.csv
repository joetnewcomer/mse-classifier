question_id,title,body,tags
2525547,Find the domain and range of $y=-x^2+4x-3$,"Find the domain and range of $y=-x^2+4x-3$ My Attempt:
$$y=f(x)=-x^2+4x-3$$
The given function is a polynomial of degree $2$ in $x$. $f(x)$ is defined for all $x\in R$, so the domain of $f=R$.
Again,
$$y=-x^2+4x-3$$
$$y=-(x^2-4x+3)$$
$$y=-(x-1)(x-3)$$
$$y=(x+1)(x-3)$$.",['functions']
2525573,Find the domain and range of $y=\sqrt {x-2}$,"Find the domain and range of $y=\sqrt {x-2}$ My Attempt:
$$y=\sqrt {x-2}$$
For $y$ to be defined,
$$(x-2)\geq 0$$
$$x\geq 2$$
So $dom(f)=[2,\infty)$.",['functions']
2525586,What is the distribution of the results of a round robin tournament? What is the distribution of the number of winners?,"With the ATP tour finals tennis tournament going on right now, I got to thinking about round robin tournaments. I learned upon searching that in graph theory the tournament models a round robin, in which each player plays every other player exactly once, and either wins or loses (no draws). But after some searching I was not able to find answers to the questions I posed in the title. Suppose there are $n$ players in a tournament. The result of a tournament is described as a graph in which each pair of vertices is connected by exactly one directed edge, where an edge going from vertex $i$ to vertex $j$ indicates that player $i$ beat player $j$. Call $g$ the number of edges in the graph, which reflects the number of games played in the tournament. Of course, $g = n(n-1)/2$, the triangular number of $n-1$ (imagine the cells above the diagonal of a tournament crosstable). Clearly, there are $2^g$ possible results of a tournament. Suppose every player in the tournament is equally skilled, so that every result is equally likely. For a given result, we can count the wins of each player and rank the win counts in a non-decreasing sequence $s = (s_1,\ldots,s_n)$ where $0 \leq s_1 \leq \cdots \leq s_n$. My first question is: what in general would be the probability distribution for $s$? Relatedly, what would be the expectation for each $s_i$? To get an intuition for this last question, I simulated 10,000 instances of a round robin tournament of 100 equally skilled players. The plot below shows the average of $s$, where the x axis reflects the index $i$ and the y axis the number of wins $s_i$. Now let the number of winners of the tournament be denoted $ w = |\{s_i : s_i = \max s\}|$. What is the probability that $w=1$, that is, that the tournament has an outright winner? In general, what is the probability distribution of $w$? Using the data from the same simulation as above, I plotted the relative frequencies of $w$ values: . Does anyone know of any results that bear on these questions? When I thought of these questions, I figured the answers would be standard results, but after a moderate amount of looking I haven't come across them.","['graph-theory', 'statistics', 'probability']"
2525592,Write formula for matrix in terms of Fibonacci numbers,"How could I express this matrix in terms of Fibonacci numbers?  It seems like I'd have to use induction once I have a candidate for a formula but I'm unsure of where to start with expressing the matrix in terms of Fibonacci numbers. Thanks in advance! Let $T:\mathbb{R^2}\rightarrow \mathbb{R^2}$ be a linear map such that $$T\left(
\begin{array}{c}
x\\
y\\
\end{array}
\right)=\left(
\begin{array}{c}
y\\
x+ y\\
\end{array}
\right)$$ using the basis $\beta=\{e_1,e_2\}$ $$e_1=\left(
\begin{array}{c}
1\\
0\\
\end{array}
\right),\quad e_2=\left(
\begin{array}{c}
0\\
1\\
\end{array}
\right)$$ Write a formula for the matrix$$ [T^n]_\beta, \forall n\in\mathbb{N}$$ in terms of Fibonacci numbers.","['matrices', 'matrix-equations', 'number-theory', 'linear-algebra']"
2525597,How to apply the definition of complex conjugate to a partial derivative,"In my book on complex variables and applications, the concept of a complex conjugate is simply stated as a reflection about the real axis in the complex plane. Mathematically speaking, the complex conjugate of $x+iy$ is $x-iy$. They then go on to describe some properties of the conjugate and provide some rather ""simple"" examples. My book is likely an introduction to the subject, for which I'm not complaining, but I've now encountered a more difficult problem and lack the confidence to move forward. I'm working on a problem, beyond the scope of examples given in the book, where an expression is given as $\displaystyle \frac{1}{\frac{\partial f(z)}{\partial z}} \frac{f(z)}{z}$ in which $z$ is complex. Can someone help step me through how to find the complex conjugate of the expression? Can you instruct me such that I can carry the knowledge and apply it to other practical/applied problems. In other words, instead of just giving the answer, tell me how/why you obtain what you did. For example, if $f(z)$ is real, I could possibly get one answer, whereas if $f(z)$ is complex, I'd possibly get a different one. I don't know, but I'd like to know how to determine the conjugate in either case. EDIT: So here's my ""stab"" at it following the rules presented in my book $\displaystyle \overline{\frac{1}{\frac{\partial f(z)}{\partial z}} \frac{f(z)}{z}} =\overline{xy} =\overline{x}\:\overline{y}$ where $\displaystyle x = \frac{1}{\frac{\partial f(z)}{\partial z}} \quad,\quad y = \frac{f(z)}{z}$ Looking first at the y-term, the conjugate of a rational expression is equal to a rational of the conjugates which gives $\displaystyle \overline{y} = \frac{\overline{f(z)}}{\overline{z}}$ Applying the same logic to the x-term gives $\displaystyle \overline{x} = \frac{\overline{1}}{\overline{\frac{\partial f(z)}{\partial z}}}$ The complex conjugate of 1 is just itself, so bringing it all together, I get $\displaystyle \frac{1}{\overline{\frac{\partial f(z)}{\partial z}}} \frac{\overline{f(z)}}{\overline{z}}$ While the expression above may be correct, I think I can say more about the conjugate of the partial derivative term $\displaystyle \overline{\frac{\partial f(z)}{\partial z}}$ Under what condition does the conjugate expression further simplify? Can it reduce to something like $\frac{\partial \overline{f(z)}}{\partial \overline{z}}$? Under some conditions, I think I've seen people pull the conjugate into the function as well i.e. $\overline{f(z)} = f(\overline{z})$. Basically, I want to know if I can say more about $\overline{\frac{\partial f(z)}{\partial z}}$ and what conditions must be satisfied to perform such manipulations.","['complex-analysis', 'complex-numbers']"
2525607,"What is a good way to explain why the graph of polynomials do not exhibit ripples, even in an arbitrarily small interval?","I was showing someone the graph of $0.1x^9+0.6x^5+0.5x^2 + x$ on Wolframalpha (for this question, any real valued polynomial will do) Someone asked me why the graphs of polynomials are smooth no matter what interval on $\mathbb{R}$ we look. More precisely, the person asked me why there isn't random ripples like or as we travel along the graph of the polynomial. Or using another example, imagine $x^2$, why isn't there a sinusoid like perturbation for this function in the range $(10000000018.3, 10000000019.1)$? A tinie tiny sinusoid? My answer was basically that we can check the derivative and see that it is always going up or down. However, I am not totally satisfied with this answer. So, what would be a good way to explain to someone why there isn't random oscillation (or ripples) in the interval (using another example, say) $(-398, -386)$ for the polynomial $x^{340} + 0.5x^{238} + 0.4x^{77} + 4$?","['intuition', 'elementary-functions', 'polynomials', 'calculus', 'algebra-precalculus']"
2525626,Show that a real solution goes to infinity as t goes to infinty,"Let $\phi(t)$ be a real, continuous, pi-periodic function. Show that there exists a real solution goes to infinity as t goes to infinity of: $x''-(cos^2t)x'+\phi(t)x=0$ I am having a hard time finding the solutions to this ode. I tried wolfram alpha with no luck. Am I missing a way to solve this by hand? Thanks.",['ordinary-differential-equations']
2525631,Proving that $ ^\mathbb{Q}\mathbb{R}$~$\mathbb{R}$ using Cantor-Bernstein [duplicate],"This question already has answers here : Bijection from $\mathbb R$ to $\mathbb {R^N}$ (2 answers) Closed 6 years ago . I am trying to prove that $ ^\mathbb{Q}\mathbb{R}$~$\mathbb{R}$ . I want to use Cantor Schroder Bernstein Theorem rather than coming up with a bijection. Any suggestions on how to get started with using this theorem? Should I begin with a function from  $ ^\mathbb{Q}\mathbb{R}$ to $\mathbb{R}$ or the other way around first? I was also thinking maybe we do not need to explictly find an injection for one direction, instead, we could justify an injection by saying if we take $f\in  ^\mathbb{Q}\mathbb{R}$. Then $f\subset\mathbb{Q}$ x $\mathbb{R}$ and continuing from here somehow. Would it be best to consider  $ ^\mathbb{N}\mathbb{R}$ instead?","['proof-writing', 'real-numbers', 'elementary-set-theory']"
2525639,Every $\text{Spec} R \rightarrow X$ factors through a open affine subscheme,"For what kind of commutative rings $R$, the following property holds? For any scheme $X$ and any morphism $f:\text{Spec} R \rightarrow X$, there exists  a open affine subscheme $U \hookrightarrow X$ such that $f$ factors through $U$. For example this is true when $R$ is local: choose a $U$ containing the image of the closed point, then $U$ will contain the whole image as it's stable under generalization.",['algebraic-geometry']
2525644,General solution of $(\sqrt3 - 1)\sin\theta + (\sqrt3 + 1)\cos\theta =2 $,"Find general solution of the equation $(\sqrt3 - 1)\sin\theta + (\sqrt3 + 1)\cos\theta =2 $. My approach:
Squared on both sides, formed a quadratic equation in $\cos\theta$ and finally got two solutions for theta,
$$\theta = 2n\pi \pm \frac{\pi}{6}$$
$$\theta = 2n\pi \pm \frac{\pi}{3}$$
But the answer given in my book is $$2n\pi \pm \frac{\pi}{4} + \frac{\pi}{12}$$
Pretty strange can anyone help me?",['trigonometry']
2525668,Doubt in the proof of Poincaire's theorem using Gauss-Bonnet theorem (local).,"I'm studying differential geometry through the book ""Differential Geometry of Curves and Surfaces - Manfredo P. do Carmo"", and I have a doubt in the demonstration of Poincare's Theorem. POINCARE'S THEOREM. The sum of the indices of a diferentiable vector field $v$ with isolated singular points on a compact surface $S$ is
  equal to the Euler-Poincaré characteristic of the surface $S$. The proof uses Gauss-Bonnet Theorem (local) GAUSS-BONNET THEOREM (Local). Let $x$: $U\subset \mathbb{R}^2$ $\rightarrow$ $S$ be an orthogonal parametrization (that is, $\langle x_u,x_v\rangle = 0$), of an oriented surface $S$, where $U$ $\subset$ $\mathbb{R}^2$ is homeomorphic to an open disk and $x$ is compatible with the orientation of $S$.  Let $R$ $\subset$ $x(U)$ be a simple region of $S$ and let $\alpha$: $I$ $\rightarrow$ $S$ be such that $\partial R$ $=$ $\alpha(I)$. Assume that a is positively oriented, parametrized by arc length $s$, and let $\alpha(s_0)$, . . . , $\alpha(s_k)$ and $\theta_0$, . . . ,$\theta_k$,  be, respectively, the vertices and the external angles of $\alpha$. Then 
  $$\sum_{i=0}^{k} \int_{s_{i}}^{s_{i+1}}k_g + \int \int_R K dS + \sum_{i=0}^{k} \theta_i  = 2\pi$$
  where $k_g(s)$ is the geodesic curvature of the regular arcs of a and $K$ is the Gaussian curvature of $S$. To prove this Manfredo proposes the following solution: Let $S$ $\subset$ $\mathbb{R}^3$ be an oriented, compact surface and $v$ a differentiable vector field with only isolated singular points. We remark that they are finite in number. Otherwise, by compactness, they have a limit point which is a nonisolated singular point. Let $\{x_{\alpha}\}$ be a family of orthogonal parametrizations compatible with the orientation of $S$. Let $\mathcal{T}$ be a triangulation of $S$ such that Every triangle $T$ $\in$ $\mathcal{T}$ is contained in some coordinate neighborhood of the family $\{x_\alpha\}$. Every $T$ $\in$ $\mathcal{T}$ contains at most one singular point. The boundary of every $T$ $\in$ $\mathcal{T}$ contains no singular points and is positively oriented. If we apply GAUSS-BONNET THEOREM (Local) , sum up the results, and take into account that the edge of each $T$ $\in$ $\mathcal{T}$ appears twice with opposite orientations, we obtain $$\int \int_S K d\sigma - 2\pi\sum_{i=0}^{k} I_i = 0  $$ where $I_i$ is the index of the singular point $p_i$, $i = 1$, . . . , $k.$  Joining this with the Gauss-Bonnet theorem (Global), we finally arrive at $$ \sum_{i=0}^{k} I_i = \frac{1}{2\pi} \int \int_S K d\sigma = \chi (S). \quad \mbox{Q.E.D.} $$ My problem is in the part marked in bold above. If we use Gauss-Bonnet on $T$ $\in$ $\mathcal{T}$ we get $$ \int_{\partial T} k_g + \int \int_{T} K dS + \sum_{i=0}^{2} \theta_{iT}  = 2\pi$$ If we sum up the equation above for all $T$ $\in$ $\mathcal{T}$. \begin{align*} 
\sum_{T \in \mathcal{T}}\left(\int_{\partial T} k_g + \int \int_{T} K dS + \sum_{i=0}^{2} \theta_{iT} \right) =& \sum_{T \in \mathcal{T}} 2\pi \\
\sum_{T \in \mathcal{T}} \int_{\partial T} k_g + \sum_{T \in \mathcal{T}} \int \int_{T} K dS + \sum_{T \in \mathcal{T}} \sum_{i=0}^{2} \theta_{iT} =& 2\pi \hspace{0.1cm}\# \mathcal{T} \\
0+ \int \int_S K dS + \sum_{T \in \mathcal{T}}\sum_{i=0}^{2} \theta_{iT}  - 2\pi \hspace{0.1cm}\# \mathcal{T} &= 0. \\
\end{align*}
Where $\# \mathcal{T}$ $=$ number of elements of the set $\mathcal{T}$. My Question: Why
  $$ \sum_{T \in \mathcal{T}}\sum_{i=0}^{2} \theta_{iT}  - 2\pi \hspace{0.1cm}\# \mathcal{T} = 2 \pi \sum_{i=0}^{k} I_i \hspace{0.1 cm} ? 
 $$","['riemannian-geometry', 'triangulation', 'curvature', 'proof-explanation', 'differential-geometry']"
2525772,Motivation for using a prior distribution with the same functional form as the likelihood,"When finding the functional form of the posterior distribution my textbook has suggested that when deciding on a prior distribution it should have the same functional form as the likelihood. 
Wondering why exactly this is the case ? Is it just for ease of analysis and interpretation or are there any other reasons ?","['bayesian', 'statistics']"
2525773,Solution Form to Differential Inequality,I have the following differential inequality: $$2\frac{\partial g(x)}{\partial x}+\left[\frac{\partial g(x)}{\partial x}\right]^2+f(x)\cdot[g(x)]^2 < 0$$ Can you recommend a solution for $g(x)$ in terms of $f(x)$?,['ordinary-differential-equations']
2525786,Is this a necessary and sufficient condition of a continuous linear functional to be injective?,"Is it true that a continuous linear functional $f$ is injective if and only if there exist a constant $M\gt 0$, and for any given $x \in (X,||\cdot||)$,  such that $|f(x)|\ge M||x||$? I know it is obviously from the right hand side we can get the left hand side. But I want to know is the opposite direction still true？ Many thanks for your help.","['real-analysis', 'analysis', 'functions']"
2525802,What is the mathematical meaning of the 'construction'?,"People say that we can construct a line segment by collecting (uncountably many) points. And they say it's because $[0,1] = \{ x \in R \mid 0 \leq x \leq 1\}$. Similarly we can construct a plane segment by collecting line segments because $[0,1]\times[0,1] = \{(x,y) \in R^2 \mid x, y \in [0,1]\}$. But I don't agree with that - because we cannot create new things with 'set building using predicates.' I mean by defining $[0,1]\times[0,1]$ as above, we used the predefined set $R^2$, which means we didn't actually construct a new thing but just made a subset. What is the meaning of the construction and what is wrong with my statements?",['elementary-set-theory']
2525846,"If $f$ has a zero and $|f''|\leq M$, then $f$ is monotone on $(-h,h)$, where $h=\sqrt{2|f(0)|/3M}$","Let $f$ be twice differentiable on $\mathbb R$ and let $M$ be a bound
  of $f''$, $|f''|\leq M$ on $\mathbb R$. Assume $f(0)\neq0$ and define
  $h=\sqrt{\frac{2|f(0)|}{3M}}$. Prove that if $f$ has a zero in $(-h,h)$, it's
  monotone in $(-h,h)$. So let's assume there's a zero $a\in (-h,h)$ such that $f(a)=0$ and also there's $b\in (-h,h)$ such that $f'(b)=0,\ f''(b)\neq0$. We need to get a contradiction. This question is under the Taylor Expansion chapter although I can't really get anything out of $f(x)=f(0)+f'(0)x+\frac{f''(\xi_x)}{2}x^2$ nor by the expansions near $a,b$. I get that in $(-h,h)$ we have $\frac{M}{2}x^2<\frac{|f(0)|}{3}$ which may be related to the 2nd derivative term though.","['derivatives', 'real-analysis', 'taylor-expansion']"
2525874,"If I know my current $v_x$ and $v_y$ and I supply a force and a direction, how do I update the velocities?","Known quantities: Initial position $x_i, y_i$, initial velocity components $V_{ix}$ and $V_{iy}$, and mass $m$. I can apply a force $F$ pushing me towards arbitrary point $P$, and then I must update the velocities accordingly the next time unit. (This is all for a game AI). My attempt: If I choose some point $P$ with coordinates $x_f$ and $y_f$ then the angle $\theta$ formed is part of the expression $\displaystyle \cos(\theta) = \frac{\Delta x}{d} = \frac{x_f-x_i}{d}$ where $d = \sqrt{(x_f-x_i)^2 + (y_f-y_i)^2}$. The force applied relates to $F = ma$ so I can solve for $a = \frac{F}{m}$. Now somehow I have to update the velocity components. We have $\displaystyle \cos(\theta) = \frac{V_{fx}}{V_f}$ (velocity in the $x$ direction over velocity in the diagonal pointing to $P$). And in general we have $V_i + a = V_f$. I'm a little bit lost for how to update the velocities, for example solving for $V_{fx}$. If I set the cosine stuff equal to each other we get $$\frac{\Delta x}{d} = \frac{V_{fx}}{V_f}$$ So that means $\displaystyle V_{fx} = \frac{V_f \cdot \Delta x}{d} = \frac{(V_i + a) \cdot \Delta x}{d}$ At this point I feel like I am getting way off track and I could use some help getting it right.","['vectors', 'physics', 'trigonometry', 'calculus']"
2525922,Motivation behind matrix diagonalisation,"I'm going to give a 50 minutes lecture about matrix diagonalization for first year college students and I would like to give some applications of it. I've been thinking about saying the calculation of matrix exponential become simpler in a diagonal matrix, but this subject is a higher level for them and I have only 10-15 minutes to talk about it. So what do you think I can say to motivate them to study matrix diagonalization?","['diagonalization', 'linear-algebra']"
2525989,"recurrence relation, all terms of the sequence positive","Let $a_1=a$ , $a_2=\frac{1}{a}-a$ , $a_{n+1}=\frac{n}{a_n}-a_n-a_{n-1}$ for $n=2,3,4,...$ . Find all $a$ such that $(a_n)$ is a sequence of positive reals. My attempt was to look at $a_3=\frac{3a^2-1}{a-a^3}$ , $a_4=\frac{8a^3-4a}{3a^4-4a^2+1}$ and a few more, $a_1>0$ gives $a>0$ , $a_2>0$ gives $a\in(0,1)$ , $a_3>0$ gives $a\in(\frac{1}{\sqrt{3}},1)$ , but this probably doesn't give important information and further terms are nasty.","['recurrence-relations', 'sequences-and-series']"
2525991,prove that $a^{25}-a$ is divisible by 30,"I'm trying to prove that $30|a^{25}-a$. First of all I said that it equals
$a(a^3-1)(a^3+1)(a^6+1)(a^{12}+1)$, so that it must be divisible by 2. Now I want to show it can be divided by $3,6$ using Fermat's little theorem. I need a bit of direction since I'm kind of lost.","['number-theory', 'divisibility', 'modular-arithmetic', 'elementary-number-theory']"
2525995,Trigonometric identity of finite terms,"Prove that:
$$\dfrac{1}{\cos x+\cos {3x}} + \dfrac{1}{\cos x+ \cos {5x}}+\dots+\dfrac{1}{\cos x+ \cos {(2n+1)x}}  \\= \frac{1}{2}\csc x \,[ \tan{(n+1)x}-\tan{x}]$$ I tried to prove this using the regular formulas. But failed. Please help me.","['trigonometry', 'telescopic-series', 'induction', 'summation', 'fractions']"
2526024,$\mathbb R $ is uncountable,"The uncountability of $\mathbb R $ can be proved by two
beautiful methods.. One is by proving the sequence of 0and 1 are uncountable using Cantor's diagonal process in which we choose any countable subset of the set of all  sequence of 0 and 1. And then by altering the i'th components of i'th element of the countable subset we get a sequence of 0 and 1 which lie outside the countable set..And considering the binary representation of all reals. The another method ,as described in munkres' Topology ,is to prove any nonempty compact Hausdorff space with no isolated point is uncountable. This would prove intervals of $\mathbb R $ is uncountable and hence $\mathbb R $  is uncountable. Both the methods are beautiful but is there any relation between the two?? Two argument proving the same thing has no Linc at all, is it possible?",['general-topology']
2526037,$C^1$ function $f$ with bounded gradient such that $\frac{f(x)}{1+|x|}$ is unbounded,"Consider the following proposition: Proposition 1: Let $\Omega$ be an open, convex subset of $\mathbb R^2$ and let $f:\Omega \to \mathbb R$ be a $C^1$ function with bounded gradient. Then  $ \frac{f(x)}{1+|x|} $ is bounded. Proof of proposition 1: A $C^1$ function with bounded gradient over a convex domain is Lipschitz
(by the Mean Value Theorem). Hence, $\forall x,y\in \Omega,$
$$|f(x)-f(y)|\leq M |x-y|,$$ where $M>0$ is the Lipschitz constant.
If we fix an $y_0\in \Omega$ we have, $\forall x \in\Omega$
$$|f(x)|\leq |f(y_0)|+|f(x)-f(y_0)|\leq |f(y_0)|+ M|x-y_0|\leq |f(y_0)|+ M|x|+M|y_0|\leq C(1+|x|),$$ where $C=max\{|f(y_0)|+M|y_0|;M\}.$ Now consider another similar proposition: Proposition 2: Let $\Omega$ be an open, convex subset of $\mathbb R^2$ and let $f:\Omega \to \mathbb R$ be an uniformly continuous function. Then  $ \frac{f(x)}{1+|x|} $ is bounded. Proof of proposition 2: $f$ is an uniformly continuous function over a convex domain. Hence, $\forall \epsilon>0\; \exists K>0 $ such that
$$|f(x)-f(y)|\leq K|x-y|+\epsilon\quad\quad\quad \forall x,y\in \Omega.$$
(See http://orfe.princeton.edu/~rvdb/tex/unif_cont/uc3.pdf for a proof of this fact.) Reasoning as in the proof of proposition 1, we can conclude that $|f(x)|\leq C(1+|x|).$ Note also that the two hypotesis ""uniformly continuous"" and ""$C^1$ with bounded gradient"" are, in general, independent. If the domain is convex the bounded gradient implies that the function is Lipschitz and hence also uniformly continuous. In general, however, it is possible to find uniformly continuous functions with unbounded derivative ($\sqrt x$ over $(0, +\infty)$) or functions with bounded gradient but not uniformly continuous ($f(x,y)$= ""argument of $(x,y)$"" over the annulus with a radius removed). My questions are: Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ $C^1$ with bounded gradient such that $ \frac{f(x)}{1+|x|} $ is unbounded? Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ uniformly continuous such that $ \frac{f(x)}{1+|x|} $ is unbounded? Is it possible to find a non-convex domain $\Omega$ and a function $f:\Omega\to \mathbb R$ uniformly continuous, $C^1$ with bounded gradient such that $ \frac{f(x)}{1+|x|} $ is unbounded?","['real-analysis', 'convex-analysis', 'multivariable-calculus', 'uniform-continuity', 'lipschitz-functions']"
2526101,"Infinite matrices, integral operators and absolute values.","If $k: X \times X \to \mathbb{C}$ (or $\mathbb{R}$) is the kernel of a bounded integral operator $K: L^2 (X, \Omega, \mu) \to L^2 (X, \Omega, \mu)$, is $\lvert k \rvert$ also the kernel of a bounded integral operator? ($\lvert k \rvert (x, y) = \lvert k (x, y) \rvert$) Depending on one's taste, this can also be formulated in the special case where $X = \mathbb{N}$ and $\mu$ is the counting measure: If $A_{ij}$ is an infinite matrix that determines a bounded operator on $\ell^2$, does the matrix $\lvert A_{ij} \rvert$ also determine a bounded operator?","['functional-analysis', 'real-analysis', 'integration', 'hilbert-spaces']"
2526164,holomorphic vector fields tangent to a hypersurface,"Let $(V,0)\subset (\mathbb{C}^n,0)$, $n\geq 3$ a (germ of) hypersurface given by $\{f=0\}$, $f$ holomorphic. A (germ of) holomorphic vector field $X$ leaves $V$ invariant if ${\rm d}f(X)$ belongs to the ideal generated by $f$. My question is: For any such $V$, does there exist a vector field tangent to $V$ with an isolated singularity? When $V$ is smooth or $V$ has an isolated singularity (at the origin) then one can readily exhibit such vector fields. However, for germs of non-isolated singularities I could neither prove this nor find a counterexample.","['singularity-theory', 'holomorphic-foliations', 'several-complex-variables', 'algebraic-geometry']"
2526182,For each $a \in \mathbb{R}$ evaluate $ \lim\limits_{n \to \infty}\left(\begin{smallmatrix}1&\frac{a}{n}\\\frac{-a}{n}&1\end{smallmatrix}\right)^n$,"If $a \in \mathbb{R}$ , evaluate $$ \lim_{n \to \infty}\left(\begin{matrix} 1&\frac{a}{n}\\\frac{-a}{n}&1\end{matrix}\right)^{n}$$ My attempt: Let $$A = \left(\begin{matrix} 0&a\\-a&0\end{matrix}\right) = -a\left(\begin{matrix} \cos(\frac{\pi}{2})&-\sin(\frac{\pi}{2})\\\sin(\frac{\pi}{2})&\cos(\frac{\pi}{2})\end{matrix}\right)$$ so that $$A^k = (-a)^k \left(\begin{matrix} \cos(\frac{k\pi}{2})&-\sin(\frac{k\pi}{2})\\\sin(\frac{k\pi}{2})&\cos(\frac{k\pi}{2})\end{matrix}\right)$$ Thus, \begin{align}\displaystyle \lim_{n \to \infty}\left(\begin{matrix} 1&\dfrac{a}{n}\\\dfrac{-a}{n}&1\end{matrix}\right)^{n} &=\displaystyle \lim_{n \to \infty} \left(I+\dfrac{A}{n}\right)^n =e^A=\displaystyle \sum_{k=0}^{\infty}\dfrac{A^k}{k!}\\&= \sum_{k=0}^{\infty} \left(\begin{matrix} \dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}&-\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}\\\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}&\dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}\end{matrix}\right) \end{align} and since $\displaystyle \sum_{k=0}^{\infty}\dfrac{(-a)^k\cos(\frac{k\pi}{2})}{k!}=1+0-\dfrac{a^2}{2!}+0+\dfrac{a^4}{4!}+\cdots= \cos a$ and $\displaystyle \sum_{k=0}^{\infty}\dfrac{(-a)^k\sin(\frac{k\pi}{2})}{k!}=0-a+0+\dfrac{a^3}{3!}+0-\dfrac{a^5}{5!}+\cdots= -\sin a$ therefore the required answer is $\left(\begin{matrix} \cos a&\sin a\\-\sin a&\cos a\end{matrix}\right).$ However the above answer does not match the choices provided which are $I, 0$ and none of the above. So my question is: Is my answer correct?","['real-analysis', 'limits', 'matrices', 'calculus', 'analysis']"
2526233,"$f(x) = x$ when x is rational, $f(x) = 0$ when x is irrational. Find all points at which $f$ is continous.","Let $f:\mathbf{R} \to \mathbf{R}$ defined as : $f(x) = x$ when x is
  rational, $f(x) = 0$ when x is irrational. Find all points at which $f$ is continous. Let $c \in Q - \{0\}$ $ \exists \langle x_n\rangle$ in $R \setminus Q $ such that $x_n \to c $ $f(x_n) = 0 \to 0 $ now $ f(c) = c \neq 0$ Therefore, $ f(x_n)$ does not converges to $f(c)$ Hence $f$ is discontinuous on $Q - \{0\}$ Let $c \in R \setminus Q$ $\exists \langle y_n\rangle$ in $Q$ such that $y_n \to c$ $f(y_n) = y_n \to c$ now $f(c) = 0$ because $c \in R\setminus Q.$ $\; \;$ also $c \neq 0$ (same reason) therefore, $f$ is discontinuous on $R\setminus Q$ Finally for $c=0$ we have $|f(x) - f(0)| \leq ||x| - 0| \leq |x|$ Therefore $\forall \; \; \epsilon>0 \; \; $$\exists \; \; \delta  = \epsilon> 0$ such that if $|x|< \delta \Rightarrow |f(x)-f(0)|<\epsilon$ Therefore $f$ is continuous at $c=0$ , and it is the only point at which it is continuous Is my proof correct ? (particularly for c=0 part)","['continuity', 'real-analysis', 'proof-verification']"
2526253,Calculating infinite sums of power series.,"This past week I have been studying power series, first I studied how to determinate the intervals of convergence and I have no problem doing that (usually I just have to apply the root or ratio test of convergence). However, now I am asked to calculate the sums of: I know some results of infinite series, like the geometric or telescopic series, however this is not enough to calculate any of those infinite sums. Is there any general procedure to calculate this sums? Or any differentation/integration theorems of power series I could use?","['power-series', 'sequences-and-series', 'calculus']"
2526259,"Express the set $\{3, \{3\}, \{\{3\}\}, \{\{\{3\}\}\}, \dots \}$ in set-builder notation.","Is it possible to express the set $\{3, \{3\}, \{\{3\}\}, \{\{\{3\}\}\}, \dots \}$ containing all sets of the form $$\underbrace{\{\dots\{}_{n} 3 \underbrace{\}\cdots\}}_{n}$$ for finite $n\geq 0$, in set-builder notation? Furthermore, is it possible to do it in a non-recursive way?",['elementary-set-theory']
2526266,"""Pivot step"" that Donald Knuth mentioned in TAOCP","I stumbled upon the following operation on matrices in section 2.2.6 Arrays and Orthogonal Lists in the first volume of The Art of Computer Programming , in an example of working with sparse matrices:
$$
\begin{pmatrix}
&\vdots&&\vdots&\\
\dots&a&\dots&b&\dots\\
&\vdots&&\vdots&\\
\dots&c&\dots&d&\dots\\
&\vdots&&\vdots&
\end{pmatrix}
\leadsto
\begin{pmatrix}
&\vdots&&\vdots&\\
\dots&1/a&\dots&b/a&\dots\\
&\vdots&&\vdots&\\
\dots&-c/a&\dots&d-bc/a&\dots\\
&\vdots&&\vdots&
\end{pmatrix}
$$
(see it on Google Books ).
This operation is called there a ""pivot step,"" the pivot in this case beeing the ""$a$"" entry in the first matrix.
This operation is stated to be used in algorithms for inverting matrices and solving [systems of] linear equations, and in the simplex method.
It is also cited here on page 141. I do not know the simplex method, but I know how to solve systems of linear equations, and usually all operations involved are elementary row transformations.
However, this ""pivot step"" operation clearly cannot be obtained as a composition of elementary row transformations, neither does it preserve the rank of the matrix (consider the case of a $2\times 2$ matrix with $a = b = c = d$), and I do not understand its ""meaning."" What is this operation?","['matrices', 'gaussian-elimination', 'numerical-linear-algebra', 'sparse-matrices']"
2526269,Extending Function to Make it Continuous,"I've been struggling with extending this function to make it continuous: $$f(x,y)=\frac{1}{x}\sin\left(\frac{x^3}{x^2+y^2}\right)$$ So, the domain is $\mathbb{R}^2\backslash\{(0,y):y\in\mathbb{R}\}$. Set of limit points of the domain is obviously $\mathbb{R}^2$. I'm not sure how to approach this problem because none of the methods we've done in class work.
I'm not even sure how to a priori assume whether I can or can't extend this function to be continuous (or even in which points is it possible to do so).
This is all new to me so I would appreciate any help or advice on how to predict the behaviour of this function. Thanks in advance.","['multivariable-calculus', 'continuity']"
2526286,Summation involving Golden ratio,Find the value of $$S=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{m^2n}{\phi^m\left(n\phi^m+m\phi^n\right)}$$ I have written it as $$S=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{\frac{m}{\phi^m}}{\frac{\phi^m}{m}+\frac{\phi^n}{n}}$$ Let $f_k=\frac{\phi^k}{k}$  Then $$S=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}\frac{1}{f_m\left(f_m+f_n\right)} $$ so we get $$S=\sum_{m=1}^{\infty}\frac{1}{f_m}\sum_{n=1}^{\infty}\frac{1}{\left(f_m+f_n\right)} $$ Can we proceed from here?,"['algebra-precalculus', 'summation', 'sequences-and-series']"
2526292,"Global minimum of $f(x)=\langle Ax,x \rangle +2 \langle x,b \rangle+c$","Given that $A\in M_n(\Bbb R)$ is a symmetric positive-definite matrix, $b \in \Bbb R^n , c\in \Bbb R$ I need to find the global minimum of the function 
$$ f(x) =\langle Ax,x \rangle +2 \langle x,b \rangle + c.$$ To solve this I first wanted to solve $D_f(a)=0$ ($D_f(a)$ is the differential of $f$ at point $a$). In order to find a local minimum, then show it is also global. But it seems like a lot of work if I define $A=(a)_{ij}$. Is there a shorter/better way?","['matrix-equations', 'matrix-calculus', 'positive-definite', 'multivariable-calculus', 'maxima-minima']"
2526404,Why does a solution of a linear homogenous ODE with constant coefficient must be infinitely differentiable?,"Let $y_0$ be a solution to the ODE: $$y^{(n)} + a_{n-1} y^{(n-1)} + \dots + a_0 y = 0$$ In this note (part 2), it is claimed that $y_0$ must be infinitely differentiable because we can write $y^{(n)}$ in terms of derivatives with lower order. My understanding is that since $y^{(n)} = -a_{n-1} y^{(n-1)} - \dots - a_0 y$, differentiating both sides will give you a formula of $y^{(n + 1)}$ in terms of derivatives with lower order. Continuing this process gives us formula for derivative of arbitrarily high order. My problem with this is that why are allowed to differentiate on both sides? Aren't we allow to differentiate only if $y^{(n)}$ is differentiable?","['ordinary-differential-equations', 'proof-explanation']"
2526454,Supremum of integrable random variables and pointwise convergence,Suppose $X:\Omega\to\mathbb{R}$ is an integrable random variable and the functions $f:\mathbb{R}\to\mathbb{R}$ and $f_i:\mathbb{R}\to\mathbb{R}$ are such that $f(X)$ and $f_i(X)$ is integrable for all $i\in\mathbb{N}$. Assume that $f_i$ converges to $f$ pointwise as $i\to\infty$. What conditions do I need to impose on $\sup_{i\in\mathbb{N}} f_i$ to ensure that $\sup_{i\in\mathbb{N}} f_i(X)$ is also integrable? Would imposing $|\sup_{i\in\mathbb{N}} f_i(x)| < \infty$ for all $x\in\mathbb{R}$ ensure this?,"['real-analysis', 'integration', 'convergence-divergence', 'probability-theory']"
2526512,Possible to solve Algebraic Riccati Equation through ODE45?,"The Algebraic Riccati Equation is: $$A'X + XA - XBR^{-1}B'X + Q = 0$$ And the Algebraic Riccati Difference Equation is: $$A'X + XA - XBR^{-1}B'X + Q = \frac{dX}{dt}$$ Some times $\frac{dX}{dt}$ is just called $S$ in books. But is it not possible to solve the Algebraic Riccati Difference Equation through ODE45 and break the simulation when $-tol <= \frac{dX}{dt} <= tol$ ? You may wonder ""Why not use Schur's method to solve the Riccati equations?"". Well, Schur's method is a very bad method and there is a reason why MATLAB and Octave are not using it. Sure, the method find the solution $X$ to the riccati equation, but build a control law $L$ for the LQR controller by using that solution is not what I recommending. Right now I using fsolve, but that's a built in library in Octave and a function in Optimization package for MATLAB.","['optimization', 'optimal-control', 'control-theory', 'linear-control', 'ordinary-differential-equations']"
2526519,Solving differential equation with the Dirac Delta Function,"I've got a differential equation to solve with the Dirac Delta Function and I'm not really sure how to handle it. I have been instructed to use the Laplace Transform as my method of solution. Here is the equation: $y''+8y'+41y=δ(t-\pi)+δ(t-3\pi)$, $y(0)=1, y'(0)=0$ I have no idea where to begin here. I took the Laplace transform but at this point I'm unsure exactly how to decompose the function after I solved for $y$. Thanks for any help.",['ordinary-differential-equations']
2526541,"If $f(x)$ satisfies $2f (x) = f(xy) + f(x/y)$, find $f(x)$","If $f(x)$ is a continuous and differentiable function which satisfies the function equation If $$2f (x) = f(xy) + f\left(\frac xy\right)\quad \forall x,y \in \mathbb{R}^{+}$$ and $f'(1)=1$ then find $f(x)$ I can see that $f(x)=\ln(x)$ is one function which satisfies all the properties but how can it be proved? I tried using first principle of differentiation but wasn't able to obtain the function. Could someone help me with this?","['functional-equations', 'calculus', 'functions']"
2526559,Does the definition of the linear span of a subset of a vector space require that the set be countable?,"My book gives this definition of linear span of a subset $S$ of a vector space $V$ : Now if $S$ is any subset of $V$, we let $L(S)$ be the set of all finite linear combinations of elements of $S$. Thus
  $$L(S) = \left\{\sum_{i=1}^k{\alpha_i v_i \mid k \in \mathbb{N}, v_i \in S, \alpha_i \in \mathbb{R}}\right\}.$$ But when we write $k \in \mathbb{N}$, aren't we implying that $S$ is at most countable?
That is, $S=\{v_i:i=1(1)k,k \in \mathbb{N}\}$. But $S$ may very well be an uncountable subset.","['linear-algebra', 'vector-spaces']"
2526564,"Does $\frac{\langle a_k,b_k\rangle}{\|a_k\|}$ converge, if $(a_k)_k$ and $(b_k)_k$ tend to $0$ and $b\neq 0$ respectively?","If $(a_k)_k$ and $(b_k)_k$ both convergent sequences in $\mathbb{R}^2$ such that their limits are $0$ and $b\neq0$ respectively. Does the sequence. $$\frac{\langle a_k,b_k \rangle}{\|a_k\|}$$ converge? (where $\langle a_k,b_k \rangle$ is the scalar product). I'm not really sure how to go about this, since the numerator and the denominator both go to $0$( all norms are equivalent so it doesn't matter which norm we choose for the denominator). I figured I might try using the sandwich theorem to approximate this but I'm not sure how, since the numerator is basically $a^1_kb^1_k * a^2_kb^2_k$ and if $(a_k)_k$ goes to $(0,0)$ obviously the component sequences both tend to $0$ and i can't separate them from the component sequences of $b_k$. So I'm not sure how to go about this!
Any hints at all would be appreciated
Thanks in advance!","['multivariable-calculus', 'limits']"
2526636,Proving $ \lim\limits_{n\to\infty} \frac{8n^2-5}{4n^2+7} = 2$ using $\epsilon-\delta-$ definition.,"I'm trying to prove the limit of this sequence using the formal definition. I've looked at other questions on the site but from the ones I've seen, the $n^2$ term always seems to cancel out, making it simpler. Show that $$ \lim_{n\to\infty} \frac{8n^2-5}{4n^2+7} = 2$$ So this is how I started: $$ \left|\frac{8n^2-5}{4n^2+7} -2 \right| = \left|\frac{-19}{4n^2+7}\right| = \frac{19}{4n^2+7} \leq \frac{19}{4n^2} = \epsilon$$ Let $\epsilon > 0 \implies n = \sqrt{\frac{19}{4\epsilon}}$ By Archimedian Property: $ N > \sqrt{\frac{19}{4\epsilon}}  $ If $ n \geq N \geq \sqrt{\frac{19}{4\epsilon}} $ $$ \left|\frac{8n^2-5}{4n^2+7} -2 \right| \leq \frac{19}{4n^2} < \frac{19}{4(\frac{19}{4\epsilon})} = \epsilon $$","['real-analysis', 'limits', 'proof-verification', 'epsilon-delta', 'sequences-and-series']"
2526657,Every open set has a proper open subset. What spaces satisfy this property?,"Suppose $X$ is a topological space satisfying the following property: every (nonempty) open set of $X$ has a (nonempty) proper open subset. Does this property have a name? What are some spaces with this property? What are some properties that imply this property (eg. Hausdorff, separable, second countable, etc.)? The intuition is that, if you have an open set $U \subseteq X$ , you can ""zoom in"" at any point of $U$ , forever. Example. If $X$ has the discrete topology, then it doesn't have this property, because singletons $\{x\} \subseteq X$ are open sets that don't have (nonempty) proper open subsets. Example. If $X$ is ${\mathbb R}^n$ with the Euclidean topology, then it does have this property, because, for any open ball $B$ of radius $\epsilon$ around $x \in X$ , you can take the open ball of radius $\epsilon/2$ , which is a (nonempty) proper open subset of $B$ .",['general-topology']
2526748,Solving derivative of squared error where the predictor is a sigmoid function,"$\newcommand{\sigmoid}{\operatorname{sigmoid}}$In the book ""Make your own neural network"" by Tariq Rashid, I have to take the derivative of my cost function which is: $$
\left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)^2
$$ where $t$ is the true value and thus is a constant. $o_j$ is the value of the previous node and $w_{jk}$ are the weights that connect $o_j$ to the error node. Trying to work out the derivative of the function myself I get the following result: $$
2\left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times \left(\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\left(1-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)
$$ The problem is that the derivative in the book is a bit different and I have no idea why or what I did wrong. The answer in the book has $-2$ and is multiplied by $o_j$ at the end. Where does the $-2$ and $o_j$ in the equation come from? what step of the chain rule did I miss?
$$
-2\left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times \left(\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\left(1-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times o_j
$$",['derivatives']
2526750,Question about modulo of prime numbers (or prime factorization (I guess)) [duplicate],"This question already has answers here : Show $pq\mid a^{pq} -a^q -a^p +a$ if $p\neq q$ are prime (4 answers) Closed 4 years ago . I would like to prove the following: Let $p$ and $q$ different prime numbers and let $a$ be an integer such that $a < p$ and $a < q$. I want to prove that 
$$pq \; | \; a^{pq} - a^p - a^q + a$$ i.e. I would like to prove:
$$ \frac{a^{pq} - a^p - a^q + a}{pq} \in \mathbb{Z}$$. Can any body help me to prove this?","['prime-factorization', 'prime-numbers', 'discrete-mathematics']"
2526761,"$\int_{\Bbb R ^2} e^{-x^2-y^2} \, dx \, dy$","I got this integral: $\int_{\Bbb R ^2} e^{-x^2-y^2} \, dx \, dy$ The first thing that came to my mind was the Fubini theorem. But then I tried to calculate it via substitution theorem because of that $e^{v^2}$. But I am not sure about the boundaries. As the substitution I used the polar coordinates: $x = r\cos\phi,$ $y = r\sin\phi$ $$\int_a^b \int_0^\infty e^{-r^2} r\,dr\,d\phi$$ Then I used the classic substitution: $u = r^2, dr=\frac{du}{2r}$ EDIT So $\displaystyle \frac{1}{2} \int_0^{2\pi} \left(\int_0^{\infty}e^{-u} \, du\right) \, d\phi = \frac{1}{2} \int_0^{2\pi} \Big[ -e^{-u}\Big]_0^{\infty} \, d\phi = \frac{1}{2} \int_0^{2\pi} \, d\phi=\pi$ I am not sure what I am doing, this is new for me so I will be happy for any 
hint.","['real-analysis', 'substitution', 'calculus', 'integration', 'lebesgue-integral']"
2526782,Sequence notation where a has a subscript and a superscript,"So from what I've seen of sequences a common format would be something like $a_{n+1} = 2a_n + 5 $ I was doing some review for sequences and come across a format that looks something like $a_{n+1}=a_n^2-1$ given $a_1=1$ According to my solution manual, $a_1=1,a_2=0,a_3=-1,a_4=0$ I initially thought that maybe it was just a weird notation and I could write it as $a_n$ but that would just make is a continually decreasing sequences so $a_1=1,a_2=0,a_3=-1,a_4=-2,a_5=-3$ and so on. So this doesnt fit the answer. I'd like to know how this $a_n^c$ where c is just some number actually works in case it comes up on a test. Any example at all or just an explanation would be great, I can't actually find this notation in the section before the question. For reference, this is in Briggs, Cochran Calculus Early Transcendentals 2nd edition, 8.1 #20 Edit: Thanks to u/Rob Arthan for this Since  $a_n^2 = a_n \times a_n=(a_n)^2$ this makes the answers from above as $a_2=(a_1)^2-1=1^2-1=0$ $a_3=(a_2)^2-1=0^2-1=-1$ $a_4=(a_3)^2-1=(-1)^2-1=1-1=0$ and so on and so forth","['notation', 'sequences-and-series']"
2526825,"If a function f has no jump discontinuities, does it have the intermediate value theorem property?","I realized I was confused by this concept (while preparing for my exam). If a function f has no jump discontinuities, does it have the intermediate value theorem property? Facts, I know:
I know that continuity implies intermediate value theorem property.  However, intermediate value theorem property does not imply continuity.  All derivatives have the intermediate value theorem property, but we can have discontinuous derivatives.
Derivatives do not have jump discontinuities, or discontinuities of the first kind. I don't know if a function has no jump discontinuities then it necessarily has the intermediate value theorem property.  I know this works for derivatives.  I was trying to construct a discontinuous function with discontinuities of the second kind, (one of left or right does not exist), with no jump discontinuities, that doesn't have intermediate value theorem property, but I couldn't think of any. Thanks","['derivatives', 'real-analysis', 'limits', 'continuity', 'analysis']"
2526846,Show that the following function is continuous on Q:,"Let $h : \mathbb{Q}\rightarrow\mathbb{R}$ be given by 
$$
h(x)= \begin{cases}
0,
&\ x^2<2\\ \ \\ 1,&\ x^2>2
\end{cases}
$$ see why this function is continuous on $\mathbb{Q}$, but I can't show it. Maybe we could work with the epsilon-delta criterion? So for every $\epsilon> 0$ we find a $\delta> 0$  with $| x - x_0 | <\delta$ implying $ | h(x) - h(x_0) | < \epsilon$ ? Am I right? How do I have to choose my $\delta$ then?","['continuity', 'real-analysis', 'functions']"
2526851,Refined Bezout Lemma,"Fix positive integers $a,b,c$ such that $\mathrm{gcd}(a,b,c)=1$. Is it true that if $n$ is sufficiently large then there exist positive integers $x,y,z$ such that 
$$
ax+by+cz=n\,\,\,\text{ and }\,\,\,x+y+z \equiv 1\bmod{3}\,\,?
$$","['number-theory', 'combinatorics', 'elementary-number-theory']"
2526865,Expected sum of cards until first Ace,"I came across this question while preparing for an interview. You draw cards from a $52$-card deck until you get first Ace. After each card drawn, you discard three cards from the deck. What's the expected sum of cards until you get the first Ace? Note J, Q, K have point value 11, 12 and 13, and Ace has point value 1 discarded cards don't count towards the sum and if we don't get an Ace we shuffle the deck and continue when you shuffle, you shuffle all cards but you keep the sum, and when you draw a new card you add it to that sum (you don't start from zero after each shuffle) My thought so far: the expected sum is definitely between $73$ and $91$. $73$ is the expected sum if we don't discard any cards, so the problem simply becomes the expected sum until first Ace, that is, $(2+\dots+13) \cdot 4 \cdot \frac{1}{5}+1$. $91$ is the expected sum if we discard all $51$ remaining cards (shuffle the deck after each draw). In this case the number of draws needed to see the first Ace follows a Geometric distribution, so the answer is $(\frac{52}{4}-1) \cdot 7.5+1$ Any help is appreciated!!!","['probability-theory', 'probability', 'statistics', 'card-games']"
2526880,Finding a solution to a PDE arising from using Feynman's trick of differentiating under the integral sign,"When considering possible generalisations of the improper integral
$$\int^\infty_0 e^{-x^2} \cos (x^2) \, dx,$$
(for its evaluation see here ) one that immediately comes to mind is
$$u(a,b) = \int^\infty_0 e^{-ax^2} \cos (bx^2) \, dx, \quad a > 0, b \in \mathbb{R}.$$ This integral can be readily evaluated using complex methods. But if we confine our attention to using only real methods, employing a method reminiscent of the most common way used to evaluate the Gaussian integral, by squaring the integral before converting it to polar coordinates similar to what was done here , one finds
$$u(a,b) = \frac{\sqrt{\pi}}{2 \sqrt{2} \sqrt{a^2 + b^2}} \sqrt{a + \sqrt{a^2 + b^2}}. \tag1$$ As an alternative (real) method to squaring the integral one can try Feynman's trick by differentiating under the integral sign. Differentiating with respect to $a$ one has
$$u_a(a,b) = - \int^\infty_0 x^2 e^{-ax^2} \cos (bx^2) \, dx.$$
Differentiating with respect to $b$
$$u_b(a,b) = - \int^\infty_0 x e^{-ax^2} \cdot x \sin (bx^2) \, dx,$$
followed by a single integration by parts gives
$$u_b (a,b) = -\frac{1}{2b} u(a,b) + \frac{a}{b} \int^\infty_0 x^2 e^{-ax^2} \cos (b x^2) \, dx = -\frac{1}{2b} u(a,b) - \frac{a}{b} u_a(a,b),$$
which after rearranging becomes
$$2a u_a + 2b u_b + u = 0. \tag2$$ Two boundary conditions for the above first-order semi-linear partial differential equation can be found. They are
$$u(a,0) = \int^\infty_0 e^{-ax^2} \, dx = \sqrt{\frac{\pi}{4a}},$$
and
$$u(0,b) = \int^\infty_0 \cos (b x^2) \, dx = \sqrt{\frac{\pi}{8b}}.$$ I understand the method of characteristics may be used to find a solution to (2). If this is correct can someone show me how this can be done so we arrive at (1). If not, what other method(s) could be used to find a solution to (2)? I should add if the limits of integration are extended to all $\mathbb{R}$, a further generalisation of the improper integral can be found here where it was evaluated using a very slick and simple complex method.","['improper-integrals', 'integration', 'partial-differential-equations']"
2526892,Basis of the image of the product of two matrices,"We have  $A \in \mathbb R ^{\mathrm {mxn} }$ and $B \in \mathbb R ^{\mathrm {nxp}}$ which are two matrices. It is said that $\{ b_1, b_2, ..., b_k\}$, where $k \leq q$, is a basis for $\mathrm{Im}(B)$ and that $\mathrm{Ker}(A)\cap   \mathrm{Im}(B) = \{ \vec{0}\}$. We have to show  that $\{ Ab_1, Ab_2, ..., Ab_k \}$ form a basis for $\mathrm{Im}(AB)$. I've tried fiddling with the rank nullity theorem, leading me to this: $$\dim (\mathrm{Im}(AB)) = \dim (\mathrm{Im}(B)) - \dim(\mathrm{Ker}(A)\cap   \mathrm{Im}(B)) $$ which makes sense since $\dim(\mathrm{Ker}(A)\cap   \mathrm{Im}(B)) = 0 $ (since it only contains the null vector) and since we are asked to show that the basis for  $\mathrm{Im}(AB)$ is $\{ Ab_1, Ab_2, ..., Ab_k \}$ (i.e a set with $k$ vectors). But, unfortunately I don't where to go from here. The only thing I've shown is that  $\dim (\mathrm{Im}(AB)) = k$ Another approach I tried to have to solve this problem was the following: $$AB\vec{x} = \vec{0} \\ \Leftrightarrow A(B\vec{x}) = \vec{0} \\ \Rightarrow B\vec{x}\in \mathrm{Ker}(A) \\ \text{and}\ B\vec{x}\in \mathrm{Im}(B)$$ but again I don't know where to go from here.","['matrices', 'matrix-rank', 'linear-algebra', 'vector-spaces']"
2526909,Canadian Senior Math Contest Question,"I was trying to solve the problem below but was completely unable to. My question is that which general category do questions like these belong to? . Once, I know that I would go learn the specific math concept and attempt at solving the question. For each positive integer $n$ , define $a_n$ and $b_n$ to be the positive integers such that $$\left(\sqrt 3 + \sqrt 2\right)^{2n} = a_n + b_n\sqrt 6$$ and $$\left(\sqrt 3 - \sqrt 2\right)^{2n} = a_n - b_n\sqrt 6$$ (a) Determine the values of $a_2$ and $b_2$ . (b) Prove that $2a_n - 1 < \left(\sqrt 3 + \sqrt 2\right)^{2n}<2a_n$ for all positive integers $n$ . (c) Let $d_n$ be the ones (units) digit of the number $\left(\sqrt 3 + \sqrt 2\right)^{2n}$ when it is written in decimal form. Determine, with justification, the value of $d_1+d_2+d_3+\cdots+d_{1865}+d_{1866}+d_{1867}$ (the given sum has 1867 terms.)","['algebra-precalculus', 'contest-math', 'recreational-mathematics', 'exponentiation']"
2526923,Is my professor's explanation of direct sum accurate?,"This is coming from a graduate-level abstract algebra class, for reference. My professor says that given two groups $G,H$ we say $D$ is the direct sum of $G$ and $H$ and write $C = G \oplus H$ if $G$ and $H$ are disjoint except for zero and $C = G+H = \{g+h | g \in G, h \in H\}$. My issue is that this isn't really a definition, since for arbitrary groups $G$ and $H$ that aren't necessarily disjoint , this isn't defined. For example, I have an assignment question to show that the direct sum of two modules with a certain property still has that property, but I don't see how the direct sum is defined for arbitrary modules. For example, what would $\mathbb{Z}_3 \oplus \mathbb{Z_2}$ be?","['abstract-algebra', 'direct-sum']"
2526943,Boundary conditions for spherical harmonics,"How does the constraint that the solution to $ \\ $
 $$\left((1-x^2)y'\right)' - \frac{m^2}{1-x^2}y = \lambda y$$ $ \\ $ be square integrable on $[-1,1]$, force the solution to be bounded at $\pm 1$?","['functional-analysis', 'spherical-harmonics', 'ordinary-differential-equations']"
2526945,How to find a form whose exterior derivative is an exact differential form?,"I think it will help me if look at examples. So, in other words, let's say we have an exact differential form: $$\alpha = (yz-z)dx + (xz+z)dy + (xy-x+y)dz$$ How can I find a differential form $\omega$ such that the exterior derivative of $\omega$ is $\alpha$? Is it just integration? And if so, how would that look? I don't yet have a good understanding of integrating differential forms. And is it a different or more difficult process if we have an exact 2-form? For instance, say we have the 2-form: $$\beta = 2xy^2 dxdy +z dydz$$ Thanks very much!","['multivariable-calculus', 'differential-forms']"
2526959,Gaussian type integral $\int_{-\infty}^{\infty} \frac{\mathrm{e}^{-a^2 x^2}}{1 + x^2} \mathrm{d}x$,"When working a proof, I reached an expression similar to this: $$\int_{-\infty}^{\infty} \frac{\mathrm{e}^{-a^2 x^2}}{1 + x^2} \mathrm{d}x$$ I've tried the following: 1. I tried squaring and combining and converting to polar coordinates, like one would solve a standard Gaussian.  However, this yielded something which seems no more amenable to a solution: $$\int_{\theta=0}^{\theta=2\pi} \int_{0}^{\infty} \frac{r \mathrm{e}^{-a^2 r^2}}{(1 + r^2 \sin^2(\theta))(1 + r^2 \cos^2(\theta))} \mathrm{d}r \mathrm{d}\theta$$ 2. I tried doing a trig substitution, t = tan u, and I have no idea what to do from there. $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \mathrm{e}^{-a^2 \tan^2(u)} \mathrm{d}u$$ 3. I looked into doing $u^2 = 1 + x^2$ but this gives us a ugly dx that I don't know how to handle, and moreover, I think I'm breaking my limits of integration (because Mathematica no longer solves it.): $$u^2 = 1 + x^2$$ $$2 u \mathrm{d}u = 2 x \mathrm{d}x$$ $$\mathrm{d}x = \frac{u}{\sqrt{u^2 - 1}}$$
$$\mathrm{e}^{a^2} \int_{-\infty}^{\infty} \frac{\mathrm{e}^{-a^2 u^2}}{u \sqrt{u^2 - 1}} \mathrm{d}u$$ 4. I looked into some form of differentiation under the integral, but that didn't seem to yield anything that looked promising.  (I checked parameterizing x^2 to x^b in both places, and in either place, and nothing canceled cleanly.) I have a solution from Mathematica, it's: $$\pi  e^{a^2} \text{erfc}(a)$$ But I'd like to know how to arrive at this.  I'm sure it's something simple I'm missing.","['gaussian-integral', 'error-function', 'integration', 'calculus']"
2526973,"Visual resources like ""better explained"", but for higher math?","I've always been a very, very visual thinker (to the point that I suspect I have a mild autism) and basically only visual explanations of math make any sense. Of course, it is not taught that way in school! I stumbled across this site https://betterexplained.com/ and it basically changed my life. All the math I've ever truly understood was stored in my brain like one of Kalid Azad's articles.  I'm now furiously gobbling up anything like it. If anybody else shares this thinking style, what other resources helped you? I know you always have to sit and ""play with"" problems yourself, but I'm looking for books, videos, etc to assist this. Also to clarify, by ""higher math"" I mean basically anything you'd see in a typical mechanical engineering undergrad series (calc 3, differential equations, linear algebra etc). Geometry is fun, but it's also self explanatory for me. I'm looking for visual explanations of not explicitly visual math. The ""art of problem solving"" by Richard Rusczyk and the rest of that textbook series phrase things in a way that helps me make my own visuals/analogies even though there arent many diagrams. Also ""proofs without words"" by Roger Nelsen was helpful.","['multivariable-calculus', 'linear-algebra', 'soft-question']"
2526995,Holomorphic functions and Wirtinger operators,"Why do so many textbooks define holomorphic functions $f: \mathbb{C}^n \to \mathbb{C}$ as $\mathbb{R}$-differentiable functions satisfying $$\frac{\partial f}{\partial \overline{z_i}}=0 $$ where we use Wirtinger operators $$\frac{\partial}{\partial z_i} := \frac{1}{2} (\frac{\partial}{\partial x_i} - i \frac{\partial}{\partial y_i}) \ \ \text{and} \ \ \frac{\partial}{\partial \overline{z_i}} := \frac{1}{2} (\frac{\partial}{\partial x_i} + i \frac{\partial}{\partial y_i}) \ ?$$ To me defining $\frac{\partial}{\partial z_i}$ in this manner looks very artificial. A much better approach seems to be using the usual definition of differentiability of maps between affine spaces over $\mathbb{C}$, that is requiring $$f(z-z_0)=L_{z_0} \cdot (z-z_0) + o( \| z - z_0 \|)$$hold for all $z_0$ in the domain we are interested in and where $L_{z_0}: \mathbb{C}^n \to \mathbb{C}$ is complex linear. Then we have the usual Jacobi matrix where $\frac{\partial f}{\partial z_i}$ has its usual meaning as $$\frac{\partial f}{\partial z_i} |_{z_0} = \lim_{h \to 0} \frac{f(z_0 + h\cdot \vec{e_i}) - f(z_0)}{h}, \ \ h \in \mathbb{C}.$$ Then we of course observe that if $f$ is $\mathbb{C}$-differentiable then it is also $\mathbb{R}$-differentiable and we just have the equality:
$$\frac{\partial f}{\partial z_i} =  \frac{1}{2} (\frac{\partial f}{\partial x_i} - i \frac{\partial f}{\partial y_i}).$$ This way both sides of the equation have their natural meaning and we are not defining the LHS via the RHS. An advantage of this approach is that we no longer have to perform ad hoc checks of chain rules for Wirtinger operators and the composition of holomorphic functions being again holomorphic becomes a triviality. As another advantage it seems that if one is not interested in Hodge theory and one just wants to define holomorphic tangent bundle and holomorphic bundle of one-forms for a complex manifold there is no longer need to go through the yoga of defining $(1,0)$ and $(0,1)$ splittings and one can proceed in the usual differential-geometric way, defining the holomorphic tangent sheaf as the sheaf of $\mathbb{C}$ derivations of the structure sheaf with the local holonomic basis consisting of $\{ \frac{\partial}{\partial z_i} \}$. To sum it up, is that just a historical way to deal with holomorphic functions as real ones and use Wirtinger operators or is there some important point I am missing here? One possible explanation I can think of is that for a complex manifold those $(p,q)$ decompositions (or more generally the interplay between real and holomorphic) are so much more interesting that people care much less about proper holomorphic objects and hence don't bother laying out a streamlined analytic exposition of them.","['complex-analysis', 'several-complex-variables', 'complex-geometry']"
2527021,How to figure out if there is an actual horizontal tangent without a graph,"There is this practice problem that asks to determine the points at which the graph of $y^4=y^2-x^2$ has a horizontal tangent. So I did implicit differentiation to find that $$\displaystyle\frac{dy}{dx} = \frac{-x}{2y^3-y}$$ To find the horizontal tangent, I set $\frac{dy}{dx}=0$ and solved for $x$: $$\begin{align}
\displaystyle\frac{dy}{dx} &= 0 \\
\frac{-x}{2y^3-y} &= 0 \\
-x &= 0 \\
x &= 0
\end{align}$$ Then I substituted $x=0$ into the equation of the curve: $$\begin{align}
y^4&=y^2-(0)^2 \\
0 &= y^4 - y^2\\
0 &= y^2(y+1)(y-1) \\
y&=-1,\,0,\,1
\end{align}$$ I concluded that the points $(0,0)$, $(0,-1)$, and $(0,1)$ were the points with a horizontal tangent. However, when I graphed this using Desmos, it turns out that the point at $(0,0)$ did not look like it has horizontal tangent. Graph of y^4=y^2-x^2 How would I have been able to figure this out without graphing it?","['derivatives', 'implicit-differentiation', 'calculus']"
2527028,The Internal Rate of Return,"The internal rate of return is the rate of return ""r"" for which future cash flows of an investment equals the price of an investment. As a formula, this would be: 100 = 10/(1+r)+10/(1+r)^2+100/(1+r)^3 Assuming this was an investment which paid $10 in years 1 and 2, then $100 in year 3. It's usually solved in excel. My question is: is there a good way to estimate (without excel) what the rough change in IRR would be if we moved the cash flows around? i.e. how much would r increase if we receive $50 of the $100 cash flow in year 1 instead of year 3?: 100 = 60/(1+r)+10/(1+r)^2+50/(1+r)^3 Don't have a good understanding of mathematics at all, but is there a very simple arithmetic way of estimate the IRR change?",['functions']
2527061,Is it possible to uniquely identify a shape in $\mathbb{R}^3$ knowing its surface area and volume?,"More precisely, can an orientable closed compact 2-manifold embedded in $\mathbb{R}^3$ be, up to an isometry, uniquely identified by its surface area and the volume it encloses? Conversely, can a counter example be given to disproof this claim? In the answer to an earlier question related to the isoperimetric inequality a proof for the statement that ""sphere is the only closed surface in $\mathbb{R}^3$ that minimizes the surface area to volume ratio"" has been outlined. That and the beginning of this post on Brunn-Minkowski inequality on Terence Tao's What's New made me curious about the existence (or lack thereof) more stringent constraints akin to the isoperimetric inequality. As a first step towards constructing a counterexample, I examined the simple case of rectangular cube having the same surface area and volume as a cube. I ended up with the following equation:
$$ 
(\underbrace{a b c}_{volume})^\frac{2}{3} = \frac{1}{6}\times\underbrace{(2ab+2bc+2ac)}_{\text{surface area}}\,, 
$$
where $a$, $b$, and $c$ are the dimensions of the rectangular cube. (For a cube of the sidelength $d$, $(d^3)^{2/3}=\frac{1}{6}\,6\,d^2$; hence the above equation.) I have no idea how to proceed with the search for any positive definite solutions to the above problem. I suppose it can be simplified by invoking (isotropic) scaling to set one of the dimensions of the rectangular cube to $1$ and the following equation:
$$ 
(a b)^2 = \left(\frac{2}{6}\right)^3(ab+b+a)^3\,. 
$$","['euclidean-geometry', 'differential-geometry']"
2527078,Computing limit of integral [duplicate],"This question already has an answer here : Let $b>0.$ Evaluate $\lim_{n \to \infty} \int^{b}_{0} \frac{\sin nx}{nx}dx$ (1 answer) Closed 4 years ago . We are supposed to calculate the limit of the integral $\lim_{n\to\infty}\int_0^\pi \frac{\sin nx}{nx}dx$. What I am currently thinking is this. Since $|\frac{\sin nx}{nx}|<\frac{1}{n}$, we can prove that this converges uniformly and can then use the dominated convergence theorem to shift the limit inside the integral, as in 
$\int_0^\pi \lim_{n\to\infty}\frac{\sin nx}{nx}dx = \int_0^\pi \frac{1}{n} \,dx$. However, I don't think that this is correct. Does someone know the correct way to solve this?","['real-analysis', 'convergence-divergence', 'calculus', 'limits']"
2527083,What is the significance of deciding the convention of $1 \text{ radian} = 180 \text{ degrees}$ over $\pi$?,"Let's say we alter the definition of the radian, for example 1 radian = 1 degree and there are 360 radians in a circle, then one consequence that I can think of is that the Taylor expansions of trigonometric functions no longer work unless altered accordingly. $$\sin x = x-{x^3\over3!}+{x^5\over5!}-{x^7\over7!}+\cdots $$ 
should instead, in order to work, become
$$\sin x= \left({\pi\over 180}\right)x-\left({\pi\over 180}\right)^3\left({x^3\over3!}\right)+\left({\pi\over 180}\right)^5\left({x^5\over5!}\right)-\cdots$$ My question is: did mathematicians recognize the elegance of defining $1 \operatorname{radian}={180^\circ/\pi}$ in Taylor expansions or also in other stuff (that it simplifies things nicely), or did the said definition come before anything else, or is there other significance to that? It is not important to question an already well-defined structure but I am just curious, thank you.","['math-history', 'trigonometry', 'convention']"
2527108,"What does $\{0,1\}*\mathbb{N}$ mean?","The question is to define a bijection 
$$ f: \{0,1\}*\mathbb{N} \rightarrow \mathbb{N} $$
I can't interpret what does $\{0,1\}*\mathbb{N}$ mean? Q. Two sets $P$ and $Q$ are given. Question is to define bijection from $P \rightarrow Q$",['elementary-set-theory']
2527152,How to get the Jacobian for Double Integrals,"So in my textbook, it has the following equation for when you are changing the variables in a double integral where $x=g(u, v)$ and $y=h(u, v)$ . \begin{align}
\int \int_R f(x,y) dA = \int \int_S f(g(u, v), h(u, v)) \left|\frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial y}{\partial u} \frac{\partial x}{\partial v}\right| du dv
\end{align} I understand that the following is the Jacobian, $\displaystyle \left|\frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial y}{\partial u} \frac{\partial x}{\partial v}\right|.$ The textbook also says that we get the above Jacobian from this determinant: $\begin{array}{|ccc|} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{array}.$ Why is the Jacobian this determinant? I don't understand where this determinant is coming from.","['multivariable-calculus', 'jacobian', 'determinant']"
2527155,Conditional probability of gates.,"A hydraulic stucture has four gates which operate independently . The probability of failure of each gate is 0.1. Given that gate 1 has failed, what is the probability that gate 2 and gate 3 will fail ? I think the answer is $P(G_2 \cap G_3) / P(G_1) = (0.1*0.1)/0.1 = 0.1$ Am I correct?","['permutations', 'combinatorics', 'probability', 'combinations']"
2527165,Calculating distance using polar coordinate metric,"I've just come across metrics that give you the distance between two points in an orthogonal curvilinear system. I have a couple of questions I was hoping someone could answer. Does distance, or $\sqrt {ds^2}$, in $ds^2=dx^2+dy^2$ and $ds^2=dr^2+r^2d\theta^2$, have the same meaning? I tried applying these to an example and I know I've gone wrong somewhere along the way so I was also hoping someone could tell me what I've done wrong. Example: Find the distance between points $P(1,0)$ and $Q(0,1)$. In Cartesian coordinates I think $dx$ and $dy$ is the difference between the $x$ and $y$ components respectively, of $P$ and $Q$. (Is that right?) So $ds^2=(1-0)^2+(0-1)^2=2 \Rightarrow ds=\sqrt2$ Converting the Cartesian coordinates into polar coordinates $(r,\theta)$ we get $P(1,0)$ and $Q(1,\frac{\pi}{2})$. I was thinking maybe $dr$ was the difference in radii and since $P$ and $Q$ are the same distance from the origin the difference is zero so $dr=0$. As for $d\theta$ I thought it was the difference in angles. I'm not sure what $r$ in $r^2$ is in the polar coordinate metric just before $d\theta^2$. I just took it as $1$ because $P$ and $Q$ are both on the unit circle. I wonder though what would happen if $P$ was on the unit circle and $Q$ was on the circle of radius $2$ for example...then I wouldn't know what value to give $r$. Anyway using my terrible logic above I get $ds^2=dr^2 +r^2d\theta^2=(1-1)^2+1^2(\frac{\pi}{2}-0)^2=\frac{\pi^2}{4} \Rightarrow ds=\frac{\pi}{2} \neq \sqrt2$ Can someone please explain what I'm doing wrong?","['multivariable-calculus', 'mathematical-physics', 'differential-geometry', 'polar-coordinates']"
2527210,Product of Two Functions is Riemann Integrable (Multivariable Calculus),"If $S$ is a non-empty bounded set in $\mathbb{R}^N$, and $f,g:S\longrightarrow\mathbb{R}$ are Riemann integrable, show $fg$ is Riemann integrable. I am not too sure with the $N$ dimensional proof of this, and have only worked it out in $1$-dimension. My work so far is: Suppose $f$ is bounded and Riemann integrable on $[a,b]$, then clearly $|f(x)|<a$ for some bound $a\geq 0$ So, $$|f^2(x)-f^2(y)|=|f(x)+f(y)||f(x)-f(y)|\leq2a|f(x)-f(y)|$$ By letting $M(f):=\sup\{f:x_{i-1}\leq x\leq x_i\}$ and $m(f):=\inf\{f:x_{i-1}\leq x \leq x_i\}$, We see, $M(f^2)-m(f^2)\leq 2a(M(f)-m(f))$ By Cauchy Criterion, because $f$ is integrable on $[a,b]$, $\forall \epsilon > 0$, $\exists$ a partition $P$, such that $$U(P,f)-L(P,f)< \frac{\epsilon}{2a}$$ $$\therefore U(P,f^2)-L(P,f^2)< \epsilon$$ Thus $f^2$ is Riemann integrable. The claim then follows by noting $fg=\frac 12((f+g)^2-f^2-g^2)$ Is this the right way to proceed for a multivariable proof of the statement in n-dimensions? Or am I not approaching this correctly? How would my proof change for a multivariable version? Any help would be appreciated. Thanks in advance!","['multivariable-calculus', 'riemann-integration']"
2527212,Direct sum of non-orientable bundles is orientable?,"Let $M$ be a smooth manifold, and let $E_1,E_2$ be two non-orientable vector bundles over $M$. Is $E_1 \oplus E_2$ orientable? I am sure there is an easy answer, but somehow my search didn't result with anything useful.","['differential-topology', 'orientation', 'direct-sum', 'vector-bundles', 'differential-geometry']"
2527231,How to evaulate $\int_0^{\infty}\frac{\ln x}{\sqrt{x}(x^2+a^2)^2}$ using contour integration,"I'm asked to calculate the following integral for which $0 \neq a \in \mathbb{R}$: $$\int_0^{\infty}\frac{\ln x}{\sqrt{x}(x^2+a^2)^2}$$ I'm confused about which contour I should use, whether it should be a semi-circle deformed to avoid the origin, or a keyhole contour based on a similar question I found here: Calculating $\int_0^{\infty } \frac{\ln (x)}{\sqrt{x} \left(a^2+x^2\right)^2} \, \mathrm{d}x$ using contour integration (but this question did not go into detail about how the integration was carried out using the keyhole contour) Also, after deciding on which contour to use, how do I proceed from there to evaluate this integral? I know that I would eventually have to use the Residue Theorem but how do I isolate the part of the contour only going from $0$ to $\infty$?","['complex-analysis', 'complex-integration']"
2527299,Abstract Proof that Exponential Map is Surjective onto $\mathrm{GL}_n(\mathbb{C})$,"It is well-known that the exponential map associated to any compact connected Lie group is surjective (the proof is a simple application of the Lefschetz fixed point theorem). As it happens, the exponential map associated to $\mathrm{GL}_n(\mathbb{C})$ is also surjective, although $\mathrm{GL}_n(\mathbb{C})$ fails to be compact. The only way I know how to prove this latter claim is by expressing each element of $\mathrm{GL}_n(\mathbb{C})$ in its Jordan canonical form and then showing that any Jordan block is the exponential of some matrix. My question is this: is there a wider class of Lie groups (more general than just compact and connected, and perhaps including examples like $\mathrm{GL}_n(\mathbb{C})$) for which we can say that the exponential map is surjective?","['matrices', 'matrix-exponential', 'group-theory', 'lie-algebras', 'lie-groups']"
2527300,If the norm of an ideal is prime then the ideal is prime,"Let $K$ be a number field and $D = \mathcal{O}_K$ its ring of integers. Let $I$ be a non-zero integral ideal of $D$ and suppose that the norm $N(I)$ is a (rational) prime. I want to prove that $I$ is a prime ideal in $D$. Since $D$ is a Dedekind domain, $I$ has a unique factorisation into prime ideals, so write $I = \mathfrak{p}_1\cdots\mathfrak{p}_n$. Then, since the norm is completely multiplicative, we have $$N(I) = N(\mathfrak{p}_1)\cdots N(\mathfrak{p}_n) = p$$ from some prime $p \in \Bbb Z$. In particular, this means that $p \mid N(\mathfrak{p}_i)$ for some $i \in \lbrace 1, \dots, n\rbrace$, and the product of the remaining norms is equal to $1$. Does this mean that $N(I) = N(\mathfrak{p}_i)$ for some $i$ and hence that $I = \mathfrak{p}_i$?","['abstract-algebra', 'ring-theory', 'dedekind-domain', 'algebraic-number-theory']"
2527302,What's the motivation for Runge-Kutta methods?,"Recently, I have been taking a course on ODEs and learning Runge-Kutta methods. To be specific, the 4th order Runge-Kutta method on solving initial value problems. My instructor and the textbook told me the formula but didn't say anything about the thoughts behind the method. I wrote some code and found that the Runge-Kutta method does perform better than the Euler method, but I can't understand why. Is anyone willing to give me a hand on how to get the formula of Runge-Kutta? Thanks!","['numerical-methods', 'ordinary-differential-equations', 'runge-kutta-methods']"
2527312,Are dihedral groups indecomposable?,"This is Exercise 6.29 from An Introduction to the Theory of Groups by J. J. Rotman: Show that the following groups are indecomposable: $\mathbb{Z}$; $\mathbb{Z}^{p^n}$; $\mathbb{Q}$; $S_n$; $D_{2n}$; $\mathbf{Q}_n$; simple groups; nonabelian groups of order $p^3$; $A_4$; the group $T$ of order $12$. But is $D_{2n}$ really indecomposable? I think $D_{4}\cong\mathbb{Z}_2\times\mathbb{Z}_2$ and $D_{12}\cong S_3\times\mathbb{Z}_2$. Is this a typo? This leads to the question: For which $n\geq3$ is $D_{2n}$ indecomposable? I think this happens iff $n\not\equiv 2\pmod4$. Here is my proof. Let $D_{2n}=\left<a,b\mid a^n=b^2=1,bab=a^{-1}\right>$ be decomposable. The normal subgroups of $D_{2n}$ are of the form $\left<a^k\right>$, $\left<b,a^2\right>$, $\left<ba,a^2\right>$, where the latter two subgroups are proper iff $2\mid n$. Since $D_{2n}$ is not abelian, it cannot be the direct product of two subgroups of the form $\left<a^k\right>$, therefore $2\mid n$ and $|\left<b,a^2\right>|=|\left<ba,a^2\right>|=n$. But $\left<b,a^2\right>\cap\left<ba,a^2\right>\neq1$, so $D_{2n}$ must be $\left<b,a^2\right>\times\left<a^k\right>$ or $\left<ba,a^2\right>\times\left<a^k\right>$ for some $k$. By order considerations we see $k=n/2$. Then $\left<b,a^2\right>\cap\left<a^{n/2}\right>=1$ or $\left<ba,a^2\right>\times\left<a^{n/2}\right>=1$ iff $2\nmid n/2$, i.e., $n\not\equiv 2\pmod4$. On the other hand, we have $D_{2n}\cong D_{n}\times\mathbb{Z}_2$ when $n\equiv2\pmod4$, and the proof is complete. Is my proof correct?","['direct-product', 'dihedral-groups', 'group-theory', 'solution-verification']"
2527376,Trigonometric sum of type $\sum^{n}_{k=1}\cos^{n}(k\theta)$,"Finding sum of $$\sum^{1007}_{k=1}\left(\cos \left(\frac{k\pi}{2007}\right)\right)^{2014}$$ $\bf{Attempt:}$ With the help of $\displaystyle \cos x = \frac{e^{ix}+e^{-ix}}{2}$ and substitute $\displaystyle \frac{\pi}{2007} = \theta$ $$\sum^{1007}_{k=1}\left(\cos k\theta\right)^{2014} = \sum^{1007}_{k=1}\bigg(\frac{e^{ix}+e^{-ix}}{2}\bigg)^{2014}$$ Could some help me to solve it, thanks",['trigonometry']
2527394,"General approach to solving problems with extending multivariable functions to be continuous, e.g. $f(x,y,z) = \frac{x^2-y^2+z^2}{x+y}$","I'm having issues with the said title. In general, I get some functions,determine the domain $D_f$ and additionally the limit points of your domain, since $f$ is said to be continuous at $x$ if $x$ isn't a limit point of the set $D_f$. The problem is always to find points in which the function can be extended so it is continuous . So I can't say oh it's not continous at one specific point therefore it can't be extended. E.g. if you had a function that can be extended to $(0,0)$ but not points of the form$(0,x)$ you have to prove the statement and that's a valid answer. Some of the tools I've been given to prove the function is continous at a point are Heine, which proved to be not useful whenever you have a function that is a polynomial and more than one variable is in the denominator, and the usual $\epsilon -\delta$ which is pretty complicated to work out most of the time. An additional problem is that I've been taught that at uni just a few days ago, and next week I'm already being tested in solving some functions that are even harder. It's far easier to prove that a function can't be extended at a point, because then all you have to do is find two restrictions of the given function with different limit points, or two specific sequences that converge to the same point, but the limit point of the function value of the said sequences is not the same. So basically, I'm more interested in figuring out a general approach to these problems, rather than specific solutions. It's like I still haven't got the slightest feeling beforehand, whether it will or wil not be extendable to certain points . And considering I'll get about 20 minutes to solve such a problem I really want to figure it out. For example, consider the function: $$f:R^3\rightarrow R,$$defined as $$f(x,y,z) = \frac{x^2-y^2+z^2}{x+y}$$ Now, $D_f = R^3\backslash\{(x,-x,z):x,z\in R\}$ and obviously every point in $R^3$ is a limit point of the set $D_f$. First let's talk about $(0,0,0)$
After testing out some limits of restriction of this function, e.g. $lim_{x\rightarrow0}f(x,0,0) = 0 $ whichever such restriction  I choose, e.g. $y = 0, x = 0, z= 0, x=y=z$ etc. the limit is either $0$ or doesn't exist. So my intuition tells me I either can't extend it or I can extend it to 0, for that point. 
But I'm not sure what to do here? $\epsilon - \delta$ for $0$ didn't yield any results and taking a sequence $(x_k,y_k,z_k)$ and plugging it into the function didn't work out well either. Not to mention proving anything for $\{(x,-x,z):x,z\in R\}$. Other examples include functions such as :
$f:R^2 \rightarrow R, f(x,y) = \frac{e^{xy-1}}{x(x^2-y)}$","['multivariable-calculus', 'limits']"
2527441,Show that on $\mathbb R^2$ the vector field $y^2\frac{\partial}{\partial x}+x^2\frac{\partial}{\partial y}$ is not complete,"How to prove that on $\mathbb R^2$ the vector field  $$y^2\frac \partial{\partial x}+x^2\frac \partial{\partial y}$$ is not complete? I tried to solve the simultaneous ODE, but I realized that it is much complicated...","['vector-fields', 'differential-geometry']"
2527472,"Square board with $N\times N$ squares, at least $N(\sqrt N + {1\over 2})$ colored","In a square board made of $N\times N$ equal squares, at least $N(\sqrt N + {1\over 2})$ squares are colored (all other squares are, say, white). Prove that there are four colored squares that form a rectangle (the four colored squares are at the corners of some rectangle found on the board; for example, in a chess board, squares b2, b7, e2 and e7 form a rectangle while squares a1, a5, b1 and b6 do not). I have tried to count all rectangles in the board, which is pretty easy, and sum the number of corners found in it, then to count to number a corners defined by coloured squares and make same comparison between the two but I fell short. Any ideas?","['graph-theory', 'chessboard', 'combinatorics', 'contest-math', 'discrete-mathematics']"
2527495,Fundamental matrix solution and Floquet theory,"Let the following matrix problem be given
$$y'=M(t)y$$
where $y\in\mathbb{R}^n$ and $M(t+a)=M(t)$ for some $a>0$. So $M$ is periodic. A fundamental matrix solution can be written as $X(t)=f(t)e^{At}$, where $f(t)$ is a $a$-periodic $n\times n$-matrix and $A$ a constant $n\times n$-matrix. For $n=1$ and $n=2$: 1. How do I find $A$? 2. What must hold for $M(t)$ such that all solutions remain bounded when $t\rightarrow \pm \infty$? 3.  What must hold for $M(t)$ such that all solutions are $a$-periodic? Here $M(t)=m(t)\begin{pmatrix}a&b\\c&d\end{pmatrix}$ for $n=2$ with $m(t)$ again $a$-periodic and $a,b,c,d$ constants. What I thought: For $n=1$ you want to find 
$$X'=M(t)X$$
where $X(t)=f(t)e^{At}$. So this gives us
$$f'(t)+Af(t)=M(t)f(t)$$
where $A$ is just a real number. How do I find it? And what must I do for the conditions in 2. and 3.? For $n=2$ I would appreciate any hints.","['matrix-equations', 'ordinary-differential-equations', 'dynamical-systems']"
2527544,Converting an expression to a trigonometric form,"I have this expression to convert in trig form:
$z=-\cos\frac{\pi}{7}+ i\sin\frac{\pi}{7}$ Tried to move the minus sine inside the $\cos$ function, but that just wouldn't work,
then with the help of $\sin x=\cos(90-x))$ tried to change the expression to:
$$
z=-\sin\frac{5\pi}{14}+i\cos\frac{5\pi}{14}
$$ Here I moved the minus sine inside the sine function since it's allowed:
$$
z=\sin\dfrac{-5\pi}{14}+i\cos\frac{5\pi}{14}
$$ Again using $\cos x=\sin(90-x))$ to convert the real part to $\cos$: 
$$
\sin\frac{-5\pi}{14}=\cos\left(90+\frac{5\pi}{14}\right)
$$ $$
\sin\frac{-5\pi}{14}=\cos\left(\frac{6\pi}{7}\right)
$$ and the imaginary part to:
$$
\cos\frac{5\pi}{14}=\sin\left(90-\frac{5\pi}{14}\right)
$$ $$
\cos\frac{5\pi}{14}=\sin\left(\frac{\pi}{7}\right)
$$ now I would get different angles for sine and cosine, which I think doesn't fit the general trig form:
$z=r(\cos x+i\sin x)$ I would really appreciate the help for this problem!","['trigonometry', 'complex-numbers']"
2527565,"Determine $\frac{dy}{dx}$ for $y=\csc(x)$ for $x(0,p)$","Determine $\frac{dy}{dx}$ for $y=\csc(x)$ for $x(0,p)$.
Find the points where $\frac{dy}{dx}$ exists; and where $\frac{dy}{dx} = 0$. I obtained $$\frac{dy}{dx}=-\csc(x)\cot(x)$$ Then I used limits to find where $\frac{dy}{dx}$ exists and I get $-\infty$ to $+\infty$. I am stuck as they asked for points. Also, I can't find where $\frac{dy}{dx}=-\csc(x)\cot(x)$ is $0$.","['derivatives', 'calculus']"
2527579,An entire function whose imaginary part is bounded is constant [duplicate],"This question already has answers here : An entire function whose real part is bounded above must be constant. (3 answers) Closed 6 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I need to prove the following question Question Let $g(z)$ be an entire function such that there exists an $\alpha > 0$ such that $\left|\operatorname{Im}(g(z))\right| \le \alpha$. Prove that $g(z)$ is a constant function. My first thought was to use Liouville's theorem. As that states, for an entire function $g$, if $g$ is bounded , then $g$ is constant. So if we could prove that $g$ is bounded, then by Liouville's theorem, $g$ is constant. I have attempted to prove that $g$ is bounded, but I am struggling. Some help would be much appreciated.","['complex-analysis', 'entire-functions', 'proof-explanation']"
2527641,Proving a local minimum is a global minimum.,"Let $f(x,y)=xy+ \frac{50}{x}+\frac{20}{y}$, Find the global minimum / maximum of the function for $x>0,y>0$ Clearly the function has no global maximum since $f$ is not bounded.
I have found that the point $(5,2)$ is a local minimum of $f$. It seems pretty obvious that this point is a global minimum, but I'm struggling with a formal proof.","['a.m.-g.m.-inequality', 'optimization', 'multivariable-calculus', 'maxima-minima', 'fractions']"
2527648,"Left invariant vector fields, example","I am trying to increase my understanding of Lie Algebra, but I got stuck on a example. I have a Lie group corresponding to the set of matrices \begin{bmatrix}
       x & y          \\[0.3em]
       0 & 1         \\
     \end{bmatrix} and I am seeking to determine the basis for the left invariant vector fields. I know that the equation for the fields is 
 $$ X_{|g}=L_{g*}V=\frac{d}{dt}(g\gamma(t)) $$
where $V=\frac{d}{dt}(\gamma(t)) $ is a vetor on $TG$. Now the matrix is the element $g\in G$. What I have been doing is that I use that $X_{|g}\in TG$ and just take a derivative of $g$ with respect to $t$ and then plug into the equation, and then solve for $\gamma$. But I know what the solution is suppose to be and I don't get the corret result  by doing that. So I don't know what I am doing wrong here. I would really appriciate some guidance.","['vector-fields', 'differential-geometry', 'lie-algebras', 'lie-groups']"
2527753,Derivative of bounded function when $x \to \infty$ [duplicate],"This question already has answers here : If a function has a finite limit at infinity, does that imply its derivative goes to zero? (6 answers) Closed 6 years ago . Can it be said for a differentiable bounded function $f(x)$ that if $\lim_{x \to \infty} f(x)$ exists then $\lim_{x \to \infty} f'(x)=0$ ? This holds for functions like $\arctan(x)$ and arccot$(x)$ but will it always be true and can it be proved?","['calculus', 'functions']"
2527754,Rotation invariance of the Lebesgue measure,"Denote the Borel sets in $\mathbb R^d$ as $\mathcal B^d$.
Is there a proof for the rotation invariance of the Lebesgue measure that doesn't use already that one has 
$$ \lambda(A^{-1}(B)) = \vert \operatorname{det} A \vert ^{-1} \lambda (B) \qquad \text{for all } A \in \operatorname{GL}(\mathbb R^d), \ B \in \mathcal B^d?$$
For example one can show easily that $\lambda$ is invariant under translation just using that intervals are invariant under translation and a $\cap$-closed generator of the Borel sets. This the first step in the proof of the above statement. Hence I am interested to see a proof for the rotation invariance likewise without the above statement.","['real-analysis', 'rotations', 'reference-request', 'lebesgue-measure', 'measure-theory']"
2527765,How does a metric tensor describe geometry on a manifold?,"I’m fairly new to the subject to the area of differential geometry, but as I understand it, the metric tensor $g$ is a tensor field that acts on the tangent space $T_{p}M$ to each point $p$ on a (Riemannian) manifold $M$. Specifically, it is defined as a mapping $g:T_{p}M\times T_{p}M\rightarrow\mathbb{R}$ such that $(v,w)\mapsto g(v,w)$ where $v,w \in T_{p}M$. The metric tensor intuitively gives the inner product of two vectors in a vector space, and thus can be used to determine magnitudes of vectors as well as the angle between intersecting curves tangent to two tangent vectors at a given point. What I’m really unsure about is how the metric actually describes geometry on the manifold $M$? I know that one can choose a set of basis vectors adapted to a given set of coordinates on the manifold, a coordinate basis, such that the metric takes the form $$g=g_{\mu\nu}(x)dx^{\mu}\otimes dx^{\nu}\equiv g_{\mu\nu}(x)dx^{\mu}dx^{\nu}$$ where $g_{\mu\nu}(x)=g\left(\frac{\partial}{\partial x^{\mu}}, \frac{\partial}{\partial x^{\nu}}\right)$. But this just gives the metric tensor at a point. How can one use $g_{\mu\nu}(x)$ in the entire coordinate chart? Or is the point that one evaluates $g_{\mu\nu}(x)$ at each point lying within the domain of the coordinate chart?","['metric-spaces', 'riemannian-geometry', 'differential-geometry']"
2527836,"Deriving a relationship between curvature, torsion and the curvature of tangent vector","Let $\alpha(s)$ be a regular and biregular curve in $\mathbb{R}^3$ parametrized in arc length with its curvature $k(s)$ and torsion $\tau(s)$. Then we can think of it's tangent vector $T(s)$ as another curve which range is contained on the unit sphere, let's call it $\beta(s)=\alpha'(s)$ with its curvature, say $k_1(s)$. I have to prove the following relation: $$k_1^2=\frac {k^2+\tau^2}{k^2}$$ I've tried different approaches and I've derived some relations (maybe true as long as my calculation were right) using Frenet-Serres formula but I still cannot conclude.","['curves', 'frenet-frame', 'differential-geometry']"
2527871,Is $\mathbb{R}$ isomorphic to $\mathbb{R}(x)$? [duplicate],This question already has an answer here : Isomorphism between $\Bbb R$ and $\Bbb R(X)$? (1 answer) Closed 6 years ago . Is $\mathbb{R}$ isomorphic (as a field) to $\mathbb{R}(x)$? (where $x$ is an indeterminate on $\mathbb{R}$),"['abstract-algebra', 'field-theory']"
2527894,Coefficient of $x^{n-2}$ in $(x-1)(x-2)(x-3)\dotsm(x-n)$,Question Find the coefficient of $x^{n-2}$ in the expression $$(x-1)(x-2)(x-3)\dots(x-n)~~.$$ My approach The coefficient of $x^n$ is $1$ . The coefficient of $x^{n-1}$ is $- \frac{n(n+1)}{2}$ But I cannot proceed from here. I would appreciate any help.,"['combinatorics', 'binomial-theorem']"
2527897,"For any prime numbers $p$, any integers $a$, any natural numbers $k$, if $p|a^k$ then $p|a$","Does this make sense? Proof (induction on $k$) Base case: Let $k=1$, then trivially $p|a$ Induction hypothesis: Let $p\mid a^k \implies p \mid a$ hold for any arbitrary $k$ in the natural numbers Induction step: $p\mid a^{k+1} \implies p\mid a^k a^1 \implies p\mid a^k$, which holds true via the inductive hypothesis. Thus $p \mid a$","['discrete-mathematics', 'induction', 'elementary-number-theory']"
2527901,If $M\geq0$. Why $\overline{\text{Im}(M)}=\text{Im}(M)\Leftrightarrow \text{Im}(M)=\text{Im}(M^{1/2}) $?,"Let $E$ be a complex Hilbert space. Let $M\in \mathcal{L}(E)^+$ (i.e. $M^*=M$ and $\langle Mx\;,x\;\rangle\geq 0,\,\forall x\in E$). Why
$$\overline{\operatorname{Im}(M)}=\operatorname{Im}(M)\Leftrightarrow \operatorname{Im}(M)=\operatorname{Im}(M^{1/2})\; ?$$
Thank you","['hilbert-spaces', 'operator-theory', 'functional-analysis', 'operator-algebras', 'linear-algebra']"
2527936,Recurrence for the number of ternary strings of length $n$ that contain either two consecutive $0$s or two consecutive $1$s,"I attempted this problem and this is what I have so far: First, I considered the possible ""cases"". If the string starts with $00$ or $11$, then the rest can be anything so there are  $2\cdot 3^{n-2}$ such strings. If the string starts with $2$, then there are $n-1$ strings that contain two consecutive $0$s or $1$s. If the string starts with $22$, then there are $n-2$ strings that contain two consecutive $0$s or $1$s. I came up with the recurrence relation:
$$a_n=a_{n-1}+a_{n-2}+2\cdot 3^{n-1}.$$
However, the solution in my textbook says it is actually
$$a_n=2a_{n-1}+a_{n-2}+2\cdot 3^{n-2}.$$
I can't seem to figure out why $a_{n-1}$ is multiplied by two.","['combinatorics', 'recursion', 'recurrence-relations', 'discrete-mathematics']"
2527941,"If $f\in L^1(\Bbb R,dx)$ then prove that for almost every $x\in\Bbb R$ $\lim\limits_{n\to \infty} f(nx) = 0.$","If $f\in L^1(\Bbb R,dx)$ Then prove that for almost every $x\in\Bbb R$ $$\lim_{n\to \infty} f(nx) = 0$$ Be aware this statement is different from the following: $$ f\in L^1(\Bbb R,dx)\implies  \lim_{|x|\to \infty} f(x) = 0$$ which is a false statement. Indeed,
We set the functionn $f$ ([see its construction here}[1]) \begin{equation}
f(x)= \begin{cases}
 2^{n/2}\cdot \underbrace{-2^{2n+2}(x-n)^2+2^{n+2}(x-n)}_{P_n}& \text{if} ~~ n\le x\le n+\frac{1}{2^n} ~~\text{for some $n\in\mathbb{N}$}\\
0 &\text{ if}~~~j+\frac{1}{2^j}\le x\le j+1~~\text{for some $j\in\mathbb{N}$}\\ f(-x)& \text{if }~~x\lt  0.
\end{cases}
\end{equation} ( $h =1$ , $ \varepsilon_n  =\frac{1}{2^n}$ , $a_n = n$ and $b_n =n+\frac{1}{2^n}$ ) One can check that $f$ is an even and continuous function. Further, we have $$ \int_\mathbb{R} f(x)\,dx= 2\sum_{n=0}^{\infty}\int_{n}^{n+\frac{1}{2^n}}2^{n/2} P_n(x) dx = 2\sum_{n=0}^{\infty}-4.2^{n/2}
\left[ \frac{2^{2n}}{3}(x-n)^3-\frac{2^n}{2}(x-n)^2\right]_{n}^{n+\frac{1}{2^n}}\\
= \frac43\sum_{n=0}^{\infty} \frac{1}{2^{n/2}}<\infty $$ Morerover, for every $n\in\mathbb{N}$ one has $$f(n) = 0~~~~\text{and} ~~~f(n+\frac{1}{2^{n+1}}) = 2^{n/2} $$ Therefore $$\lim_{|x| \to \infty}|f(x)|\not \to 0 $$ further, $\lim_{|x| \to \infty}|f(x)|$ does not exist. Whereas in this case for all $x\not \in \{n+\frac{1}{2^{n+1}}, n \in \Bbb N\} $ $$\lim_{j\to \infty} f(jx) = 0$$ I also tried to construct a contradiction with this function Does this integral converge or diverge ? $\int_\Bbb R \left (\frac{2+\cos x}{3}\right )^{x^4}dx?$ . Sill we have $$\lim_{j\to \infty} f(jx) = 0~~~~x\not\in \{2n\pi:~~ n \in \Bbb N\}$$ Also notice that this question is differ from Let $f\in L^1(R)$ set $f_n(x)=\dfrac{f(nx)}{n}$ prove that $\lim_{n \to \infty}f_n=0$ for a.e x Prove that for almost every $x\in$ $\mathbb{R}$ , $\lim_{n\to\infty}n^{-p}f(nx)=0$ . where the situation is easy to manage.
[1]: https://math.stackexchange.com/q/2143818","['real-analysis', 'functional-analysis', 'baire-category', 'lebesgue-integral', 'measure-theory']"
2527954,Distribution of all point to point distances on a square $L \times L$ lattice?,"Is it possible to derive a probability density function $P(d,L)$ which gives probability of having a point to point distance $d$ on a $2D$ square lattice of size $L$ or at least it's asymptotic behavior ?","['combinatorics', 'combinatorial-geometry', 'euclidean-geometry']"
2527976,How do we convert a generating function that counts to a generating function of probabilities?,"I am learning about the binomial coefficient and counting. We define: 
$$
(1+x)^m = \sum^m_{n=0} \begin{pmatrix}
 m \\ n
\end{pmatrix} x^n
$$
the coefficients of the powers of $x^n$ represent the number of ways of choosing $n$ objects from a set of $m$. Is there a way to convert this into a probability distribution by dividing each coefficient by 
$$
 \sum^m_{n=0} \begin{pmatrix}
 m \\ n
\end{pmatrix}. 
$$ Example We have a set of three objects, $\{a, b , c\}$, we can draw 1 object three ways $(a + b + c)$, two objects three ways $(ab + ac + bc)$ and three objects one way $(abc)$. Then, $$
(1+x)^3 = 1x^0 + 3x^1 + 3x^2 + 1x^3 
$$
If we divide each coefficient by the sum of coefficients (in this case 8) then they represent probabilities(?). Can we then say that $(1+x)^n$ is the generator for a distribution function
$$
p_n = \frac{1}{\sum^m_{n=0} \begin{pmatrix}m \\ n
\end{pmatrix}}
\begin{pmatrix}
 m \\ n
\end{pmatrix} 
$$
Such that $$
(1+x)^n = \sum_n p_n x^n
$$","['binomial-coefficients', 'probability-distributions', 'generating-functions', 'probability', 'combinatorics']"
2527983,Finding the area inside an implicitly defined curve $x^2 + (y + \sqrt[3]{|x|})^2=1$,"Need help finding the area inside an implicitly defined curve $x^2 + (y + \sqrt[3]{|x|})^2=1$ . (I think it is a heart shape). I've been trying to parameterize it with no luck. I also tried to restrict my attention to $0 \leq x \leq 1$ and do an integral against the $y$ axis from $y=-1$ to $y=1$ (since $x$ is a function of $y$ in that interval) and then add that to the area of the curve in $1 \leq y \leq 1.7$ , which I would do with another integral but against $x$ this time. None of these have worked, because it's difficult to rearrange well enough to get $y$ or $x$ alone on one side. How would you go about solving this? Is there a neat parameterization I'm missing? Also, I believe the area is $\pi$ but I don't know how they figured that.","['integration', 'parametrization', 'calculus']"
2528080,Find $\lim\limits_{x\to0}\frac 1x(x^{-\sin x}-(\sin x)^{-x})$,"The question is to evaluate this limit:$$\lim_{x\to0}\frac{\big(\frac{1}{x}\big)^{\sin x}-\big(\frac{1}{\sin x}\big)^x}{x}$$
I tried using l'Hospital's rule, taking the logarithm, doing some manipulations using known limits, but without success.",['limits']
2528123,Particular solution of non-homogeneous recurrence relation,"can somebody help me with my homework, please? I have to solve this: $a_{n}$ = $-a_{n-1}$ + $12a_{n-2}$ - 10n + 13 + $7.3^{n}$ $a_{0} = 3$, $a_{1}=24$. I know to solve this (homogeneous equation):
$a_{n}$ = $-a_{n-1}$ + $12a_{n-2}$ $a_{n}$ = $x^{n}$ $x^{n}$ = -$x^{n-1}$ + $12.x^{n-2}$ $x^{n}$ + $x^{n-1}$ - $12.x^{n-2}$ = 0 (x + 4)(x - 3) = 0 $t_n$ = $(-4)^{n}.\alpha$ + $3^{n} \beta$ How can I find particular solution? ....using A,B,C... because i don't know solve this with sum (I found here something with sum, but we did't use sum on course). Thanks everyone for your help :)","['recurrence-relations', 'discrete-mathematics']"
