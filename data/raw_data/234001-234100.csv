question_id,title,body,tags
4885769,Analytical solution to a nonlinear system of coupled ODEs,"I think I'm being rather silly but I'm trying to see if the system of nonlinear coupled ODEs given by $$x'=-xy$$ $$y'=-xy$$ has any non-trivial solutions. After some googling I have found that a solution to this problem is given by $$ y(t)=c_{1}-\frac{c_{1}e^{c_{1}c_{2}}}{e^{c_{1}c_{2}}-e^{c_{1}t}}$$ $$ x(t)=-\frac{c_{1}e^{c_{1}c_{2}}}{e^{c_{1}c_{2}}-e^{c_{1}t}}$$ but I have no idea how this solution was obtained? Any insight into the problem or this solution would be a great help. This all started as I was trying to see if the system given by $$x'=-k_{1}xy-k_{2}xz$$ $$y'=-k_{1}xy$$ $$z'=-k_{2}xz$$ where $k_{1}$ and $k_{2}$ are constants, has an analytical solution. I simplified the problem and obtained what I presented at the start of this post. Anyway, any insight into any of this is appreciated.","['nonlinear-system', 'ordinary-differential-equations', 'dynamical-systems']"
4885774,Simplification of $ \sum_{n=0}^{\infty} x^{n q} \prod_{k=1}^{t} \left(\sum _{m=0}^n \frac{1}{(x^{a_k})^m}\right) $,"$$\sum_{n=0}^{\infty} x^{n q} \prod_{k=1}^{t} \left(\sum _{m=0}^n \frac{1}{(x^{a_k})^m}\right)$$ For $t=1$ : \begin{align*}
\sum_{n=0}^{\infty} x^{n q} \left(\sum _{m=0}^n \frac{1}{x^{a_1 m}}\right) &= \sum_{n=0}^{\infty} x^{n q} \left( \frac{\frac{1}{x^{(n+1)a_1}}-1}{\frac{1}{x^{a_1}}-1}\right)\\
&= \frac{1}{ \left(\frac{1}{x^{a_1}}-1\right)} \sum_{n=0}^{\infty} x^{n q} \left( \frac{1}{x^{(n+1)a_1}}-1\right)\\
&= \frac{1}{(1-x^q)(1-x^{q-a_1})}
\end{align*} For $t=2$ : \begin{align*}
&\mathrel{\phantom=} \sum_{n=0}^{\infty} x^{n q} \left(\sum _{m=0}^n \frac{1}{x^{a_1 m}}\right) \left(\sum _{m=0}^n \frac{1}{x^{a_2 m}}\right )\\
&= \sum_{n=0}^{\infty} x^{n q} \left( \frac{\frac{1}{x^{(n+1)a_1}}-1}{\frac{1}{x^{a_1}}-1}\right)\left( \frac{\frac{1}{x^{(n+1)a_2}}-1}{\frac{1}{x^{a_2}}-1}\right)\\
&= \frac{1}{ \left(\frac{1}{x^{a_1}}-1\right) \left(\frac{1}{x^{a_2}}-1\right)} \sum_{n=0}^{\infty} x^{n q} \left( \frac{1}{x^{(n+1)a_1}}-1\right)\left( \frac{1}{x^{(n+1)a_2}}-1\right)\\
&= \frac{1}{ \left(\frac{1}{x^{a_1}}-1\right) \left(\frac{1}{x^{a_2}}-1\right)} \sum_{n=0}^{\infty}  \left( \frac{x^{n(q-a_1-a_2)} }{x^{a_1+a_2}}- \frac{x^{n(q-a_1)} }{x^{a_1}} - \frac{x^{n(q-a_2)} }{x^{a_2}} + x^{q n}\right)\\
&= \frac{1}{ \left(\frac{1}{x^{a_1}}-1\right) \left(\frac{1}{x^{a_2}}-1\right)} \left ( \frac{1}{x^{a_1+a_2}-x^q} + \frac{1}{x^{a_1}-x^q} + \frac{1}{x^{a_2}-x^q} + \frac{1}{1-x^q}\right)\\
&= \frac{1-x^{2q-a_1-a_2}}{(1-x^q)(1-x^{q-a_1})(1-x^{q-a_2})(1-x^{q-a_1-a_2})}
\end{align*} However, for $t=3$ the solution becomes unwieldy. Mathematica gives $\dfrac{P}{Q}$ , where: \begin{gather*}
P = x^{a_1+a_2+a_3} \left(-x^{4 q} \left(x^{a_1+a_2+2 a_3}+x^{a_3} \left(x^{a_1+2 a_2}+\left(x^{a_1}+3\right)\
x^{a_1+a_2}+x^{a_1}+x^{a_2}\right)+x^{a_1+a_2}\right)-\left(x^{a_1+a_2+2 a_3}+x^{a_3} \left(x^{a_1+2 a_2}+\left(x^{a_1}+3\right)
x^{a_1+a_2}+x^{a_1}+x^{a_2}\right)+x^{a_1+a_2}\right) x^{a_1+a_2+a_3+2 q}+2 \left(x^{a_1}+1\right) \left(x^{a_2}+1\right) \left(x^{a_3}+1\right) x^{a_1+a_2+a_3+3 q}+x^{3 (a_1+a_2+a_3)}+x^{6 q}\right)\\
Q = \left(x^q-1\right) \left(x^q-x^{a_1}\right) \left(x^q-x^{a_2}\right) \left(x^{a_3}-x^q\right) \left(x^q-x^{a_1+a_2}\right) \left(x^{a_1+a_3}-x^q\right) \left(x^{a_2+a_3}-x^q\right) \left(x^{a_1+a_2+a_3}-x^q\right)
\end{gather*} So my two questions are as follows: Can the resulting sum always be put into a product of $(1-\sum_{m=0}x^{q-p_m})$ where $p_m$ is a combination of $a_t$ ? and Is there a convenient form (i.e. less messy) of the solutions for all $t$ ? Thanks. Edit: Just realized I messed up the subscript of $a_k$ in the title. Sorry.","['algebra-precalculus', 'sequences-and-series']"
4885821,"Question About Proving The Countable Subadditivity of $\mu^*:\mathcal{P}(\mathbb{R})\to[0,+\infty]$","Let $F:\mathbb{R}\to\mathbb{R}$ be a bounded, nondecreasing, and right-continuous function that satisfies $\lim_{x\to-\infty}F(x)=0$ . Define a function $\mu^*:\mathcal{P}(\mathbb{R})\to[0,+\infty]$ by letting $\mu^*(A)$ be the infimum of the set of sums $\sum_{n=1}^{\infty}(F(b_n)-F(a_n))$ , where $\{(a_n,b_n]\}$ rangers over the set of sequences of half-open intervals that cover $A$ , in the sense that $A \subseteq \bigcup_{n=1}^{\infty}(a_n,b_n]$ . I need to prove the following: If $\{A_n\}$ is an infinite sequence of subsets of $\mathbb{R}$ , then $\mu^*(\bigcup_nA_n)\leq\sum_n\mu^*(A_n)$ . I would like to precede in the following way: Let $\{A_n\}_{n=1}^{\infty}$ be an arbitrary sequence of subsets of $\mathbb{R}$ . If $\sum_n\mu^*(A_n)=+\infty$ , then $\mu^*(\bigcup_nA_n) \leq \sum_n\mu^*(a_n)$ certainly holds. So suppose that $\sum_n\mu^*(A_n)<+\infty$ , and let $\epsilon$ be an arbitrary positive number. For each $n$ choose a sequence $\{(a_{n,i},b_{n,i}]\}_{i=1}^{\infty}$ that covers $A_n$ and satisfies \begin{align*}
    \sum_{i=1}^{\infty}(F(b_{n,i}) - F(a_{n,i})) < \mu^*(A_n) + \frac{\epsilon}{2^n}.
\end{align*} If we combine these sequences into one sequence $\{(a_j,b_j]\}$ , then the combined sequence satisfies \begin{align*}
    \bigcup_nA_n \subseteq \bigcup_j(a_j,b_j)
\end{align*} and \begin{align*}
    \sum_j(F(b_j) - F(a_j)) < \sum_n\left(\mu^*(A_n) + \frac{\epsilon}{2^n}\right) = \sum_n\mu^*(A_n) + \epsilon.
\end{align*} These relations, together with the fact that $\epsilon$ is arbitrary, imply that $\mu^*(\bigcup_nA_n) \leq \sum_n\mu^*(A_n)$ . However, I feel that my description is not rigorous enough. Especially, I claimed the existence of a sequence $\{(a_{n,i},b_{n,i}]\}_{i=1}^{\infty}$ that covers $A_n$ and satisfies $\sum_{i=1}^{\infty}(F(b_{n,i}) - F(a_{n,i})) < \mu^*(A_n) + \frac{\epsilon}{2^n}$ . However, I have difficulties construct a concrete sequence to show that this is indeed true. Here is some of my thought: Define a sequence $\{(c_{n,k},d_{n,k}]\}$ of half-open intervals such that $\bigcup_k(c_{n,k},d_{n,k}] = \bigcup_i(a_{n,i},b_{n,i}]\backslash A_n$ and that $d_{n,k} = c_{n,k}+p_{n,k}$ , where $\{p_{n,k}\}_{k=1}^{\infty}$ is a sequence of positive number converging to $0$ such that for every $\epsilon>0$ we have $F(d_{n,k}) - F(c_{n,k}) = F(c_{n,k}+p_{n,k}) - F(c_{n,k}) < \frac{\epsilon}{2^k}\cdot\frac{1}{2^n}$ . Note that such a sequence $\{p_{n,k}\}_{k=1}^{\infty}$ exists due to the right-continuity of $F$ . Then \begin{align*}
\sum_{i=1}^{\infty}(F(b_{n,i}) - F(a_{n,i})) &= \sum_{k=1}^{\infty}(F(d_{n,k}) - F(c_{n,k})) + \inf\left(\sum_{i=1}^{\infty}(F(b_{n,i}) - F(a_{n,i}))\right)\\
&< \frac{\epsilon}{2^n} + \mu^*(A_n).
\end{align*} I seriously doubt that my attempt is entirely correct and rigorous. But this is really the best I can get. I would really appreciate it if someone could help me check my construction and correct my proof if there is any flaws! Reference: Measure Theory 2nd Edition by Donald L. Cohn Proposition 1.3.10.","['measure-theory', 'proof-writing', 'real-analysis', 'solution-verification', 'sequences-and-series']"
4885829,Solve $\| X A - B \|$ subject to $X C = C X$,"Given $A, B \in \mathbb{R}^{n \times k}$ and S.P.D. $C \in \mathbb{R}^{n \times n}$ , I would like to find an analytical solution for the matrix $X \in \mathbb{R}^{n \times n}$ that minimizes \begin{align}
\lVert X A - B  \rVert^2_{F}
\end{align} subject to the hard constraint $$
C X = X C.
$$ Given the eigendecomposition of $C$ with $C = R \Lambda R^T$ , from Nearest commuting matrix it appears that a reasonable projection operator onto the space of matrices commuting with $C$ is $P_C(X) = R P_\Lambda(R^T X R) R^T$ with $$
[P_\Lambda(X)]_{ij} = \begin{cases} X_{ij}, & \lambda_i = \lambda_j \\ 0, & \textrm{otherwise} 
\end{cases}
$$ . With this in mind, my guess at solving this would be to apply this projection operator to the unconstrained minimal solution to the least squares system, e.g. $$
X = P_C(B A^{\dagger})
$$ with $A^{\dagger}$ denoting the M.P. pseudoinverse of $A$ . Alternatively, it seems like the solution $$
X = P_C(B A^T)
$$ is also reasonable (similar to the orthogonal procrustes solution). That said, I don't know which one of these (if either) is the minimal solution that commutes with $C$ . EDIT: Building on top of user1551's excellent answer below, the solution to the case where $X$ is also constrained to be orthogonal can be expressed concisely as $$
X = R U V^T R^T
$$ where $U, V^T$ are recovered from the SVD $$
U \Sigma  V^T = P_{\Lambda}(R^T B A^T R).
$$ Pretty nifty.","['matrices', 'least-squares', 'symmetric-matrices', 'matrix-equations', 'matrix-decomposition']"
4885840,"One cannot place the numbers 1, 2, 3, 4, 5, 6, and 7 to make the sum of the numbers on the hexagon equal to 7 times the number in the center.","Question Prove by contradiction the following statement: One cannot place the numbers 1, 2, 3, 4, 5, 6, and 7 to make the sum of the numbers on the hexagon equal to 7 times the number in the center. Let a, b, c, d, e, f be the numbers on the hexagon, and g the number in the center. Write down an equation that corresponds to the negation of the statement, then try to reach a contradiction. Note: This statement can be easily proved by explicitly trying all possible values for g. Such proof is not going to get credit. What I did I am confused on the equation that this is asking for. I think it would have been possible with brute force, though it is not allowed in the question. Does it mean x=7p where x is the sum of the outside corners and p the number in the middle?","['linear-algebra', 'geometry', 'discrete-mathematics']"
4885912,Numerical approach of integral equation,"If I have a equation $$\int_0^{t} \int_{s(\tau)}^D f(x, \tau) d x d \tau=0$$ and I want to solve $s(t)$ for any given $t$ . Here $D$ is a constant and $f(x,\tau)$ is a known function, is there a numerical method for solving $s(t)$ ? I was thinking a method like Runge–Kutta method for ODE, but I'm not sure. Any help is appreciated.","['integration', 'numerical-methods', 'ordinary-differential-equations']"
4885927,"Find all entire functions such that $|f(z+z')|\leq |f(z)| + |f(z')|$, for all $z,z'\in\mathbb{C}$","Find all entire functions such that $|f(z+z')|\leq |f(z)| + |f(z')|$ , for all $z,z'\in\mathbb{C}$ In particular, let $z=z'$ yields $|f(2z)|\leq2|f(z)|$ . This gives that $\frac{f(2z)}{f(z)}=c, $ for some $c\in\mathbb{C}$ . By considering continuity at 0, we have $f(0)=\lim_{n\to\infty}\frac{1}{c^n} f(1)$ . Then $c\in\mathbb{R}$ and $\frac{1}{c}\leq1$ . If $c=1$ , then $f(z)$ is the constant equating $f(0)$ . But if $\frac{1}{c}<1$ we just have that $f(0)=0$ . This does not seem to lead to anywhere. Is there another thing to be considered? Hints will be appreciated. Thank you.","['complex-analysis', 'entire-functions', 'inequality', 'functions']"
4885975,Semantics of the angle between velocity vector and the positive $x$-axis,"Let's say a particle moves in plane with curvature equal to $\kappa(t) = 2t$ , with constant speed of $\|v(t)\| = 5$ , such that $v(0) = 5\textbf{i}$ , and the particle never goes to the left of the $y$ -axis. The angle that the velocity vector makes with the positive $x$ -axis at time $t$ is $\alpha(t)=\frac{\pi}{2} - 5t^2$ . $\alpha(t)=\frac{\pi}{2} - 5t^2 \implies \frac{d\alpha}{dt} = -10t$ , which is always negative because $t \ge 0$ . That would normally imply that the angle is decreasing without bound, but intuitively I don't see how it can go below $-\pi$ , so I'm not sure how to interpret that angle. Note: This is the exercise $14.15.11$ in Tom Apostol's Calculus vol. $1$ . Clarification: We know the curvature is $\kappa(t) = |\frac{d \alpha}{ds}|$ , where $s$ is arc length. To compute $\alpha$ , I would like to know the sign of the $\frac{d \alpha}{ds}$ . If we take $\alpha$ is always decreasing, we know $\kappa(t) = -\frac{d \alpha}{ds}$ , and I can integrate to find $\alpha$ . However, if the angle is constrained to be in the range $[-\pi, \pi]$ , then I'm not sure how to assign the sign to the absolute value (and even the derivative is not continuous).","['plane-curves', 'angle', 'multivariable-calculus', 'intuition', 'plane-geometry']"
4886043,Working of the Chain Rule in Calculus,"I came across a proof of the chain rule in a book called ""Calculus Made Easy"" by Silvanus P. Thompson. It said that the rule works because we essentially multiply and divide by another small change in another function (usually represented as du for a change in function u(x) ). I.e. $$\frac {dy}{dx}=\frac {dy}{du}.\frac {du}{dx}$$ But when I tried to confirm this with my Physics teacher, he said that we can't actually explain it in that way because, du separated from dy or dx has no meaning as $\frac {d}{dx}$ is considered as a whole entity or an operator rather than a fraction . Please help me out with a clear explanation regarding this. Also, please say whether there is any more direct approach other than the indirect (I suppose, it's indirect because we make up an intermediate function) chain rule to differentiate function compositions such as $$y=e^{sin x}$$ P.S. I'm just a beginner in the context of Calculus.","['calculus', 'derivatives']"
4886052,List of geometric theorems linked by two squares,"I'm trying to create a classification for geometric theorems that relate to two squares As a type of organization and classification And the curiosity to explore I have collected some theorems of this type that I will put in an answer/answers. I hope you can help me expand my list. It's important to note that I'm not looking for theorems about squares because it would become too extensive a list, I'm looking for theorems about a number of squares equal to exactly twoTherefore, theorems such as Van Opel's theorem are not accepted in the answers","['euclidean-geometry', 'big-list', 'geometry']"
4886081,"Range and domain of $x\mapsto 1-f(x+1)$, knowing those of $f$","Problem: Let $f$ be a function which has domain $D_f=[-1,2]$ and range $=[0,1]$ . What are the domain and range of the function $g$ defined by $g(x) = 1-f(x+1)$ ? My thinking: If the domain of $f$ is $[-1,2]$ , then the domain of $x\mapsto f(x+1)$ is $[0,3]$ (adding 1 to both the extreme limits of domain of $f$ ). And because the range of $f$ is $[0,1]$ , the range of $1- f(x+1)$ is $[-1,0]$ (subtracting 1 from both the extreme limits of range of $f$ ). But this is not the range and domain. I have been taught that when a change occurs inside the function, only the domain changes. And when the change is outside the function, the range changes. Is this incorrect? Question: What would be the range and domain of $g$ , and how is my thinking incorrect?",['functions']
4886110,expected winning for a lottery,"You decide to make a lottery with n tickets, where each ticket is numbered between 1 and n, and each ticket is unique. Each ticket costs $5, and the lottery
works in the following manner. Once all n tickets have been purchased, a number x is selected
at random between 1 and n and all the money is divided equally between people
with tickets less than x. That way, if 1 is selected, you(as the organizer) get to keep the
prize pool. Everyone's number is randomized, the only case where the organizer wins the prize pool is if x=1, as no ticket number is less than 1, so no person wins anything and so the organizer automatically wins the prize pool. Also, the organizer does not hold the number 1, and does not win any money if the number is greater than 1.The only way where they earn the prize pool is if the chosen number is 1, other than that, there is no way they can win any money. The first part is to calculate the expected winnings per lottery as the organizer. So there is only one case where you can win the prize pool, I multiplied the case where you win and the prize pool, so I get: $$\frac{1}{n} \cdot 5n = 5$$ Now, the second part is to calculate the expected winnings if somebody purchases a ticket. I am not able to figure out the probability where somebody wins. For the prize pool, I am confused if it is 5n or 5p (where p is the person's ticket number).","['probability-distributions', 'discrete-mathematics', 'lotteries', 'probability']"
4886121,Checking if a ring is Artinian.,"I am reviewing some commutative algebra notes for an upcoming course, and I found an exercise I wanted to solve, but I found out I absolutely have no idea how. Let $K$ be a field (I think $\operatorname{char}K\neq 2$ , but the exercise doesn't outright state it), find whether $$R = k[x,y]/(2x^2 + 3y^2 - 11, x^2 - y^2 - 3)$$ and $$S = k[x,y]/(x^2 - xy + x, xy - y^2 + y)$$ are artinian or not. I mean, they are of course noetherian, should I check if its Krull dimension is $0$ , but I don't know how to check it. Could someone please help me? Thank you!","['artinian', 'algebraic-geometry', 'commutative-algebra', 'noetherian']"
4886131,"Total derivative of f(x, g(x, y)) and its approximation","I understand the steps to calculate the total derivative of f(x, g(x)) Related: Derivative of $f(x, g(x))$ with respect to $x$ I have three sub-questions related to calculating the total derivative of f(x, g(x, y)), (1) How do I calculate its total derivative, here's my attempt: $$
df=\Big(\frac{\partial{f}}{\partial x}+\frac{\partial{g}}{\partial x}\Big)dx+\frac{\partial{f}}{\partial y}dy
$$ So applying a simple example of f(x, x+y) where g(x,y)=x+y $$
df=\Big(\frac{\partial{f}}{\partial x}+1\Big)dx+\frac{\partial{f}}{\partial y}dy
$$ (2) Why do I not need to consider higher order terms? Looking at Taylor Series would it make it more accurate? (3) In terms of approximating the total derivative, is this logic correct? $$
df(x, x+y) = f(x+\Delta x, y+\Delta y) - f(x,y) \approx \Big(\frac{f(x+\Delta x, x+\Delta x + y)-f(x, x+y)}{\Delta x}+1\Big)\Delta x + \Big(\frac{f(x, x + y + \Delta y)-f(x, x+y)}{\Delta y}\Big)\Delta y
$$","['multivariable-calculus', 'calculus', 'implicit-differentiation', 'partial-derivative', 'derivatives']"
4886159,Why are closed balls not a topology in a metric space?,"Define a closed ball in a metric space $X$ with the metric $\rho$ that has centre a $x$ and radius $r$ as the set $\{y:\rho(x,y)\le r\}$ for $r\ge0$ and $x,y\in X$ . Why are arbitrary unions of closed balls not a topology in $X$ but arbitrary unions of open balls are? An open ball is defined as the set $\{y:\rho(x,y)\lt r\}$ .","['general-topology', 'metric-spaces', 'analysis', 'real-analysis']"
4886168,Find prime numbers satisfying an equation,"Find all triplets $(m, n, p)$ , where $p$ is a prime number and $m, n ∈ \Bbb N$ , such that $p=\frac{m}{4}\sqrt{{2n-m \over 2n+m}}$ My procedure is as follows: $p=\frac{m}{4}{\sqrt{(2n)^2-m^2}\over 2n+m}$ It can be shown that m cannot be odd, so if $m=2k$ , $p=\frac{k}{2}{\sqrt{n^2-k^2}\over n+k}$ If $l=\sqrt{n^2-k^2}$ then $(k,l,n)$ form a Pythagorean triplet giving the two equations $p=\frac{kl}{2(n+k)}, k^2+l^2=n^2$ I do not know how to proceed after this. Using a python script, I think the only solutions are $(24,15,2),(24,20,3)$ and $(30,39,5)$ . How do I prove this? Any help would be appreciated, thanks.","['number-theory', 'pythagorean-triples', 'diophantine-equations', 'discrete-mathematics', 'prime-numbers']"
4886196,"Why is $[X,Y]_{\mathfrak g} = -[X,Y]$?","Let $G$ be a Lie group that is a closed subgroup of the isometry group of a manifold $M$ . Identify each $X$ in the Lie algebra $\mathfrak g$ of $G$ with the vector field on $M$ that is generated by the one-parameter group of diffeomorphisms $\phi_t(y) = \exp(tX)y$ . Then Arthur Besse’s book “Einstein Manifolds” warns on p.182 that $$[X,Y]_{\mathfrak g} = -[X,Y],$$ where $[\cdot,\cdot]$ is the bracket of vector fields on $M$ and $[\cdot,\cdot]_{\mathfrak g}$ is the bracket on $\mathfrak g$ . Attempt: By the definition of the commutator on $M$ , $[X,Y]f$ , where $f$ is a function on $M$ , is $(XY - YX)f$ . The first term, $XY(f)(p)$ , equals $$\left.\frac{d}{dt}\right\vert_{t=0}(\exp tX)^*\left.\frac{d}{ds}\right\vert_{s=0}(\exp sY)^* f(p) = \left.\frac{d}{dt}\right\vert_{t=0}\left.\frac{d}{ds}\right\vert_{s=0}(\exp tX)^*(\exp sY)^*f(p) = \left.\frac{d}{dt}\right\vert_{t=0}\left.\frac{d}{ds}\right\vert_{s=0}(\exp tX)^*f((\exp sY)(p)) = \left.\frac{d}{dt}\right\vert_{t=0}\left.\frac{d}{ds}\right\vert_{s=0}f\big((\exp sY)((\exp tX)(p))\big) = \left.\frac{d}{dt}\right\vert_{t=0}\left.\frac{d}{ds}\right\vert_{s=0} f\left(\exp \left(sY + tX +\frac{st}{2}[Y,X]_{\mathfrak g} + \dots\right)p\right),$$ by the BCH formula. So far it looks promising that the result of these two derivatives combined with the $-YXf$ term should yield $[X,Y]f = [Y,X]_\mathfrak g f$ . Unfortunately, I’m unsure how to apply the chain rule correctly when taking these two derivatives. In my attempt, I got $XY(f)(p) = f’’(p)XpYp +f’(p)\left(\frac 12 [Y,X]_\mathfrak g + YXp\right).$ Note: Besse gives two references for this warning, but unfortunately, I couldn’t find them. The first is N.R. Wallach’s “Compact homogeneous Riemannian manifolds with strictly positive curvature” and the second is Kobayashi and Nomizu’s “Foundations of differentiable Geometry Vol. II” p.469 (this reference doesn’t even have this many pages).","['lie-algebras', 'differential-geometry']"
4886272,Hodge decomposition on vector bundle-valued differential forms,"Let $M$ be a compact Reimannian manifold and let $(E,h)$ be a Hermitian vector bundle over $M$ . Let $A^k(M,E)$ denote the space of $E$ -valued $k$ -forms, i.e. smooth section of the bundle $\bigwedge^kT^*M\otimes E$ . Let $D$ be a flat Hermitian connection on $(E,h)$ (where 'Hermitian' means $D$ is compatible with the metric $h$ : $dh(s,t)=h(Ds,t)+h(s,Dt)$ , $\forall s,t\in A^0(M,E)$ ), and let $D^*$ denote its formal adjoint (i.e. $(D\alpha,\beta)=(\alpha,D^*\beta)$ where $(-,-)$ is the $L^2$ -inner product of $A^*(M,E)=\bigoplus_kA^k(M,E)$ induced by $h$ ). Then one can define the Laplacian w.r.t. to $D$ : $\Delta_D=-D^*D-DD^*$ . A Hodge decomposition theorem on $A^*(M,E)$ : $$A^*(M,E)=ker\Delta_D\oplus im D\oplus im D^*$$ is demonstrated in Demailly's $L^2$ Hodge Theory and Vanishing Theorems (the first chapter of the book 'Introduction to Hodge Theory'), Section 4. The proof is actually the same as the proof in the case without $E$ : to show the formally self-adjoint operator $\Delta_D$ is elliptic and to use linear elliptic PDE theory. Now I wonder wether the condition that $D$ is Hermitian can be removed. It seems to me that the only place where this condition is used is to calculate $D^*=(-1)^{...}\star D \star$ . This formula implies the principal symbol $\sigma_{D^*}(\xi)=-\xi_{\sharp}\lrcorner$ , then it follows that $\sigma_{\Delta_D}(\xi)=|\xi|^2$ : the differential operator $\Delta_D$ is elliptic. But using antilinear Hodge operators $\bar\star:A^{p,q}(M,E)\leftrightarrow A^{n-p,n-q}(M,E^*):\bar\star$ , we have $D^*=(-1)^{...}\bar\star D_{E^*}\bar\star$ even $D$ is not Hermitian. Then the same consequences still follow.","['hodge-theory', 'vector-bundles', 'differential-geometry']"
4886279,Concentration of measure on spheres with respect to a unitary of trace approximately zero,"This question arose out of my attempt to understand how a unitary of trace approximately zero acts on the unit sphere of a $n$ -dimensional Hilbert space. First, some context: We note that, by concentration of measures for spheres, we have the following: Let $S^{n-1} = \{x \in \mathbb{R}^n: \|x\|_2 = 1\}$ denote the unit sphere of $\mathbb{R}^n$ . Let $v_n \in S^{n-1}$ be randomly chosen according to the canonical probability measure on $S^{n-1}$ . For any fixed $\epsilon > 0$ , as $n \to \infty$ , the probability that $v_n$ lies $\epsilon$ -close to any given equator goes to $1$ . See, for example, here . Reframing this in terms of orthogonal matrices, we see that an equator is exactly the the intersection of the unit sphere with the invariant subspace of an orthogonal matrix whose eigenvalues are all $1$ except one $-1$ with multiplicity $1$ . That is to say, we have the following: Let $v_n \in S^{n-1}$ be randomly chosen according to the canonical probability measure on $S^{n-1}$ . Fix $\epsilon > 0$ , and fix a sequence of matrices $T_n \in \mathbb{M}_n(\mathbb{R})$ s.t. $T_n$ is an orthogonal matrix with eigenvalue $1$ of multiplicity $n-1$ and eigenvalue $-1$ of multiplicity $1$ for all $n$ . Then the probability that $\|T_nv_n - v_n\|_2 < \epsilon$ goes to $1$ as $n \to \infty$ . By essentially the same proof, the complex version of this also holds, namely, the above result still holds if $v_n$ is instead randomly chosen on the unit sphere of $\mathbb{C}^n$ and $T_n \in \mathbb{M}_n(\mathbb{C})$ are unitary matrices with the same condition on eigenvalues. Now, I’m interested in analogues of this for unitary matrices with (normalized) trace close to $0$ . Drawing from the intuition from real Hilbert spaces and orthogonal matrices, it would seem that such matrices should have a large region on the unit sphere where the vectors in the region are sent to vectors approximately orthogonal to the original one. Then by a concentration of measure style argument, it should be the case that the probability that a uniformly random vector on the unit sphere will be sent to a vector orthogonal to it goes to $1$ as the dimension goes to $\infty$ . I’ve been unable to prove it, however. The following is the precise question statement: Let $v_n \in S^{2n-1} \subset \mathbb{C}^n$ be randomly chosen according to the canonical probability measure on $S^{2n-1}$ . Fix $\epsilon > 0$ , and fix a sequence of matrices $T_n \in \mathbb{M}_n(\mathbb{C})$ s.t. $T_n$ is a unitary matrix with $\frac{1}{n}\mathrm{Tr}(T_n) < \frac{\epsilon}{2}$ for all $n$ . Is it the case then that the probability that $|\langle T_nv_n, v_n \rangle| < \epsilon$ goes to $1$ as $n \to \infty$ ? Some thoughts on the matter: Again, this seems intuitively plausible. We also observe that the stronger condition that $\langle T_nv_n, v_n \rangle = 0$ is equivalent to (after diagonalizing $T_n$ ) $\sum_{i=1}^n \lambda_{ni}|(v_n)_i|^2 = 0$ , where $\lambda_{ni}$ are the eigenvalues of $T_n$ and $(v_n)_i$ is the $i$ -th coordinate of $v_n$ . This is a complex co-dimension $1$ condition, same as the equator example in the complex case. Furthermore, this condition is certainly satisfiable. Indeed, $\frac{1}{n}\mathrm{Tr}(T_n) < \frac{\epsilon}{2}$ implies that any $v_n$ with $|(v_n)_i| = \frac{1}{\sqrt{n}}$ for all $i$ satisfies the desired condition. And furthermore any $v$ that is $\frac{\epsilon}{4}$ -close to such an $v_n$ would satisfy the desired condition as well, again similar to the equator example. However, I’ve been unable to make much progress on this as I’m not quite familiar with the intricacies of the concentration of measure, and the same method as in the equator example doesn’t exactly apply. Any help on this is highly appreciated!","['measure-theory', 'concentration-of-measure', 'operator-theory', 'linear-algebra', 'probability']"
4886316,"Curvature of ""music-scale""","Lately I am thinking about music from a mathematical point of view. Let us consider $\mathbb{H} =\mathcal{L}^2([0,T])$ as the Hilbert space of functions that represents the time-intensity plot of a sound. Quite tautologically, playing together sounds amount to sum them. So, up to scale volumes, the chords one can produce with $n$ sounds $f_1, \ldots, f_n \in \mathbb{H}$ lives in the convex envelope of $f_1, \ldots, f_n$ . Now let us restrict to simple wave sounds $f_{\lambda}(t) := e^{i \lambda t} \in \mathcal{L}^2([0,T])$ . If I consider $n$ different frequencies $\lambda_1, \ldots, \lambda_n$ , the functions $f_{\lambda_i}$ will be linearly independent (and orthogonal actually), so that their convex envelope is just the simplex $\Delta^{n-1}$ . This approach unfortunately does not shed light on the underlying geometry. My second approach is more differential. Let us consider $f: (0,\Lambda) \to \mathbb{H}$ defined as $f \mapsto f_{\lambda}(\bullet)$ for some big $\Lambda$ . This is a curve in $\mathbb{H}$ , and I'd like to understand if there is some way to compute its ""curvature"" even though we have infinite dimensions. Trying to remember differential geometry from University and reading off Wikipedia, there should be infinitely many ""curvatures"" associated to a ""Frenet orthonormal system"", but I have not been able to generalize the latter. On the other hand, the simple definition for a curvature in two dimensions generalize easily. We can compute the norm of the ""second-derivative"" along $\lambda$ , obtaining $$ || \partial_{\lambda}^2 f_{\lambda} || = \frac{1}{T} \int_0^{T} ((it)^2 e^{i \lambda t}) (-it)^2 e^{-i \lambda t } dt = \frac{1}{T} \frac{T^5}{5} = \frac{T^4}{5}  $$ Quite nicely, it does not depend on the frequency (but unfortunately it does depend on the length of the interval that is arbitrary). However, I am not really sure about how I should interpret a measure of curvature in $(\textrm{seconds})^4$ . Does any of the latter make sense? Is there a way to compute the curvature in infinite dimensions? One could then look for a curve in the space that has similar features and get a grasp of the geometry of $f_{\lambda}$ ! As a last intuition, note that at each fixed time $t$ the function $f_{\bullet}(t)$ draws a circle at a speed proportional to $t$ . It does make sense then that the curvature does not depend on the frequency since the circle has constant curvature. The method based on the inner product integrates all the contributes at fixed time.","['riemannian-geometry', 'curvature', 'hilbert-spaces', 'functional-analysis', 'music-theory']"
4886349,"Closedness of the Relation given by an Involution of $[0,1]$","If $I=[0,1]$ , let $f:I\to I$ be an involution. Then $\sim_f:=\Delta_I\cup \text{Gr}(f)\subseteq I^2$ defines an equivalence relation on $I$ where $\Delta_I=\{(t,t):t\in I\}$ is the diagonal of $I$ and $\text{Gr}(f)=\{(t,f(t)):t\in I\}$ is the graph of the function $f$ . My question then is Q: What involutions $f$ are such that $\sim_f$ is closed as a subspace of $[0,1]^2$ ? For example, if $f$ is continuous, then $\text{Gr}(f)$ is closed and so is $\sim_f$ . But this is clearly not a necessary condition. Also, if $\text{Fix}(f)$ is open and $f\vert_{I\setminus\text{Fix}(f)}$ continuous, then $\sim_f=\Delta_{I}\cup\text{Gr}(f\vert_{I\setminus\text{Fix}(f)})$ is again close. But, again, this is not a necessary condition. So are there stronger sufficient/necessary conditions? Or even a more intrinsic characterization for such functions? Note that an equivalence relation is given by an involution as above $\iff\forall t\in [0,1]:|[t]|\leq2$ . This is because, if $\sim$ is such that $\forall t\in I:|[t]|\leq2$ , we can define $f:I\to I$ as $$f(t)=\begin{cases}t & \text{if } [t]=\{t\}\\ t'&\text{if }[t]=\{t,t'\}\end{cases}$$ which is an involution such that $\sim_f=\sim$ .","['equivalence-relations', 'general-topology', 'functions']"
4886378,Unbounded operator whose spectrum is the entire complex plane?,"The question is simple: how to find an unbounded operator $T:H\to H$ where $H$ is a Hilbert space such that $\text{Sp} T = \mathbb C$ ? This seems a very basic thing, but I have not found an example in the literature. In some proofs, we need to consider this case separately. This example should be quite important.","['spectral-theory', 'functional-analysis']"
4886383,Extreme value theory: asymptotic of the least-rolled number out of a series of rolls,"Choose positive integers $D$ and $N$ . Roll a fair $D$ -sided die $N$ times, recording the number of times each of the $D$ outcomes are rolled, say $r_1, r_2, \ldots, r_D.$ What are the asymptotics of $\min(r_1, r_2, \ldots, r_D)$ ? They're not independent, so I can't directly apply the Fisher–Tippett–Gnedenko theorem but that's probably a good starting point. If it makes it easier you can assume $N \gg D.$ Of course the leading term is $N/D$ but what’s the second-order term? Maybe $\asymp \sqrt{N/D}$ ?","['probability-distributions', 'asymptotics', 'dice', 'probability', 'extreme-value-analysis']"
4886465,Torsion elements of $SL_3(\mathbb{F}_p[x])$?,"Is every element of $SL_3(\mathbb{F}_p[x])$ a torsion element? Here are my thoughts: First of all, the group is noncommutative, so a torsion element is an element of finite order. I'm thinking of examples of torsion elements:
Any generator (since a generator p times is the identity.)
Any element that has a rotation matrix as one of its factors.
And there are a bunch more examples. Then I can't think of any element that does not have torsion in this group. Could someone please refer me to a result that makes this more precise? Your help will be very much appreciated!","['torsion-groups', 'algebraic-geometry', 'abstract-algebra', 'geometric-group-theory', 'group-theory']"
4886484,Divisors Sum Related Interesting Approximate Relation,"Working on Divisors Sum Efficient calulcation topic. Accidentaly discovered one interesting relation which is accurate up to $10^{17}$ order. $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{i}}}\approx\frac{\pi^2}{6}-\frac{1}{2}+\frac{1}{24}$$ To get things more clear look at the below numbers: $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{i}}}=1.1866007335148928206...$$ $$\frac{\pi^2}{6}-\frac{1}{2}+\frac{1}{24}=1.1866007335148931031...$$ Just would like to share this nice relation, check if you know some paper about this and wondering if there are some similar known relations for other number theoretical functions:) Just to clarify things this is not the only realation, but one of many, for example: $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{\sqrt{e^i}}}\approx\frac{2\pi^2}{3}-1+\frac{1}{24}$$ EDITED Accorging to @Greg Martin comment just realized that this is Lambert Series example. Lets assume now $s>0$ . So the general rule is $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{si}}}=\sum_{i=1}^{\infty}{\frac{i}{e^{si}-1}}$$ Using Euler-Maclaurin summation: $$\sum_{i=1}^{\infty}{\frac{i}{e^{si}-1}}=\int_{0}^{\infty}\frac{x}{e^{sx}-1}dx - \frac{1}{2}\big(\lim_{x\to0}\frac{x}{e^{sx}-1}+\lim_{x\to\infty}\frac{x}{e^{sx}-1}\big)+\frac{1}{12}\big(\lim_{x\to\infty}(\frac{x}{e^{sx}-1})'-\lim_{x\to0}(\frac{x}{e^{sx}-1})'\big)-\frac{1}{720}\big(\lim_{x\to\infty}(\frac{x}{e^{sx}-1})'''-\lim_{x\to0}(\frac{x}{e^{sx}-1})'''\big)+...$$ Here all the higher order derivatives are odd. If we look at the $\frac{x}{e^{sx}-1}$ all the odd derivatives $(\frac{x}{e^{sx}-1})^{(2k-1)}$ at $x\to0$ and $\infty$ are equal $0$ except the first derivative. Here is the list of few odd derivatives: $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)=\frac{1}{s}; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)=0$$ $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)'=-\frac{1}{2}; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)'=0$$ $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)'''=0; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)'''=0$$ $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)^{(5)}=0; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)^{(5)}=0$$ and integral equals to $$\int_{0}^{\infty}\frac{x}{e^{sx}-1}dx=\frac{\pi^2}{6s^2}$$ So finally we have: $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{si}}}=\frac{\pi^2}{6s^2}-\frac{1}{2s}+\frac{1}{24}$$ Conclusion Even after above formulas, the calculation shows that the real values are not matching. For example case for $s=1$ . Any idea why this formula does not work?","['number-theory', 'divisor-sum', 'approximation', 'real-analysis']"
4886503,"Generator of the joint process $(X_t,Y_t)$ where $Y_t= e^{-t}W(e^{2t})$ and $X_t = \int^t_0 Y_sds$.","Let $(W_t)_{t\geq 0}$ be a standard one-dimensional Brownian motion and let $$ Y_t := e^{-t}W(e^{2t}), \qquad X_t := \int^t_0 Y_s ds $$ Show that the joint process $(X_t,Y_t)$ is Markovian and find the generator of the process. This is an exercise that I came across while reading a book. I can show the first part where the joint process is indeed Markovian. However, I don't know how to find the generator $$ L[f](0,x) := \lim_{t \rightarrow 0}\frac{E_{(0,x)}[f(X_t,Y_t)] -f(0,x)}{t}. $$ for $f$ sufficiently regular. My first thought is to use Ito, but I only know the form of $f(t,X_t)=...$ not the form $f(X_t,Y_t)=...$ (if there is any) and my second thought is to use the joint density but don't know how to proceed to find the joint density for now.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4886507,Distance between triangle incenter and vertices,"after many researches on the subject, I can't find any convincing argument anywhere, so I come to you about this problem which has been brought by some of my high school students. Let $ABC$ be a random triangle and $I$ his incenter. It is known that $AB=c$ , $AC=b$ and $BC=a$ . I'm looking for a clean way to express the distance AI only from $a$ $b$ and $c$ parameters (no angles). When I searched on the internet, I found the formula : $$ AI^{2}=\frac{p-a}{p}bc$$ Where $p=\frac{a+b+c}{2}$ is the semi-perimeter.
The formula is quite nice, but I can't find a proof. I tried with law of cosines, heron's formula, but I can't quite catch the idea which will bring me this particular formula. Any idea ?","['euclidean-geometry', 'triangles', 'triangle-centres', 'geometry']"
4886571,"why is $f(x,y)=\begin{cases}\frac{4(x^2-y)(2y-x^2)}{y^2} & y>0\\ 0 & y\leq 0.\end{cases}$ continuous on $\mathbb{R}^2\setminus\{(0,0)\}$?","I am reading a real analysis book which states that the function $f:\mathbb{R}^2\to\mathbb{R}$ defined by $$f(x,y)=\begin{cases}
    \frac{4(x^2-y)(2y-x^2)}{y^2} & \text{if } y>0\\
    0 & \text{if }y\leq 0.
\end{cases}$$ is continuous on $\mathbb{R}^2\setminus\{(0,0)\}$ , is not continuous at $(0,0)$ and if we fix any $y\in\mathbb{R}$ then $f(\cdot,y)$ is continuous on $\mathbb{R}$ and if we fix any $x\in\mathbb{R}$ then $f(x,\cdot)$ is continuous on $\mathbb{R}.$ Now, it is easy to see that $f$ is not continuous at $(0,0)$ since if we set $x=0$ then $f(0,y)=\begin{cases}-8 &\text{ if }y>0\\ 0&\text{ if }y\leq 0\end{cases}$ and it is also easy to see that for any fixed $y\in\mathbb{R}$ the function $f(\cdot,y)$ is continuous because this last function is a polynomial in the $x$ variable. What I don't understand is why this function (and also $f(x,\cdot)$ ) is continuous on $\mathbb{R}\setminus\{(0,0)\}$ ; it seems to me that it should be continuous on $\mathbb{R}^2\setminus\{(x,0):x\in\mathbb{R}\}=\mathbb{R}^2\setminus\{x-\text{axis}\}$ infact if we consider $f(1,y)=\begin{cases}\frac{4(1-y)(2y-1)}{y^2}=\frac{-8y^2+12y-4}{y^2} & \text{if }y>0\\ 0&\text{if }y\leq 0\end{cases}$ it evaluates to $0$ for $y\leq 0$ but it goes to $-\infty$ for $y\to 0^+$ . Could someone please explain this to me? Why is $f$ continuous on $\mathbb{R}^2\setminus\{(0,0)\}$ and why is $f(x,\cdot)$ continuous on $\mathbb{R}$ for any fixed $x$ ? Thanks.","['continuity', 'multivariable-calculus', 'real-analysis']"
4886591,The need for independence in random sums when using law of total expectation,"Let $(X_n : n\in \mathbb{N})$ be a sequence of i.i.d. random variables with mean $\mu$ and variance $\sigma^2$ . Let $S_0 = 0$ and $S_n = \sum _{i=1}^{n}X_i$ for $n \geq 1$ . Let $N$ be a non-negative integer-valued random variable. The question I am working on asks to assume further that $N$ is independent of the random variables $X_i$ , and to show that $\mathbb{E}(S_N)=\mu\mathbb{E}(N) $ . I know how to show this in the standard way using total expectation, by conditioning on $N$ : \begin{align*}\mathbb{E}(S_N)&=\sum_{n=0}^\infty \mathbb{E}(S_N|N=n)\mathbb{P}(N=n)\\
&=\sum_{n=0}^\infty \mathbb{E}(S_n)\mathbb{P}(N=n)\\
&=\sum_{n=0}^\infty n\mu \mathbb{P}(N=n) \\
&=  \mu\mathbb{E}(N). \end{align*} My question is the following: Do we need $N$ to be independent of the random variables $X_i$ for the above argument to follow? I do not see where I used the independence of $N$ in the above working. On trying to find an answer to this, I found the following document concerning the proof of Wald's equation. In remark 1.2 on page 4, it talks about the case where the stopping time is independent as a special case, so it seems that independence is indeed important. I would like to understand why independence is needed.","['stochastic-processes', 'probability']"
4886650,Can an ellipse roll down a tilted sine curve without jumping?,"Background Assume that we have a solid ellipse with uniform density, and that it rolls along a curve. In the following MO question , I asked along what curve an ellipse rolls down fastest. It was pointed out that one first needs to find any curve the ellipse rolls down from, without losing contact from the curve and jumping up and down. One can consider this as a variation of the classical Brachistochrone problem . Now, it turns out that there are various curves along which an ellipse can roll horizontally . These are described in this video by Morphocular. It turns out that, if one wants to describe the equation of the curve along which an ellipse with width $2a$ and height $2b$ rolls down horizontally with the center as its axle point, the x-coordinate of this equation comes down to an elliptic integral (see 5:04 of the video): $$x = \int \frac{b}{\sqrt{1-\epsilon^{2} \cos^{2}(\theta)}} d \theta \label{1}\tag{1} $$ This is not an easy expression to deal with. However, there is a way out if one chooses the foci as the axle points, instead of the center of the ellipse. Assume that the ellipse has width $1$ and height $\sqrt{2}$ . In this case, we can describe the curve with a simple sine wave, with period $\pi$ and amplitude $1/2$ . Here is a picture (a snapshot from the video at 7:39): The equation of this curve can thus be described succinctly as $$y = \frac{1}{2} \sin(2x) .\label{2}\tag{2}$$ Question Now, suppose that we tilt this sine curve and the ellipse by an angle of $\theta$ . In the image below, I have visualized the setting. (Note that the shape of the sine curve is different from the one above. However, this is merely due to my mediocre illustration skills. It is supposed to describe the same curve as the one shown in the image above. The same applies to the ellipse.) I have the following questions: For what values of $\theta$ does the curve roll down without losing contact with the tilted sine curve - if any? If there are multiple values of $\theta$ for which the ellipse rolls down properly, I wonder: for what value does the ellipse roll down the fastest?","['trigonometry', 'mathematical-physics', 'calculus-of-variations']"
4886675,Bound on convolution: $ | (h * f^2) (x)| \leq \| f\|^2_2 g(h)$,"I am trying to find bounds for the following quantity. Take two functions $f,h \in L^{1} \cap L^2$ but $\|h \|_{\infty} = \infty$ . Is there a way to obtain a bound of the following type: $$ | (h * f^2) (x)| \leq \| f\|^2_2 g(h)$$ where $g$ is some function of $h$ ? This would be the case if we could take the sup norm of $h$ out of the integral by using Holder's inequality, but in this case is not allowed as $\|h \|_{\infty} = \infty$ . PS: you can also assume that $(h * f^2) (x)$ is everywhere well-defined.","['measure-theory', 'convolution', 'real-analysis', 'functional-analysis', 'holder-inequality']"
4886692,Shuffled image of set,"A permutation $\psi: S \rightarrow S$ , where $S = \{1,2,\dots,n\}$ , is considered $\textit{descriptive}$ if for every $k < n$ , the image under $\psi$ of $\{1,2,\dots,k\}$ is not simply $\{1,2,\dots,k\}$ . Determine the total number of $\textit{descriptive}$ permutations for the set $\{1,2,3,4,5,6\}$ . $\textbf{Analytical Journey:}$ $\textbf{Initial Observation:}$ It was identified that a critical condition for a permutation to be well-shuffled is $\psi(a_1) \neq a_1$ . This observation suggested a strategy of partitioning based on the placement of elements, leading to an exploration of permutations excluding fixed points. $\textbf{Principle of Inclusion-Exclusion Strategy:}$ The Principle of Inclusion-Exclusion was considered to remove permutations that are not well-shuffled from the total count of $15!$ . The complexity arose in correctly identifying and subtracting permutations by their fixed points to avoid double-counting. $\textbf{Recurrence Relation Formulation:}$ A recurrence relation approach was investigated to express the number of well-shuffled permutations for $n$ based on smaller instances. The challenge was in developing a relationship that incorporates the increasing complexity of the well-shuffled condition as $n$ grows. $\textbf{Investigation of Generating Functions:}$ Generating functions were explored as a potential method to encapsulate the enumeration problem. The main hurdle was devising a function that naturally excluded permutations that did not meet the well-shuffled criterion, and translating this condition into mathematical terms was particularly challenging. $\textbf{Pattern Identification through Smaller Cases:}$ By manually enumerating cases for smaller $n$ , an effort was made to uncover patterns that might lead to a method applicable to larger instances. Although this process provided insight, it did not lead to an easily scaled method.","['permutations', 'combinatorics', 'recurrence-relations', 'generating-functions']"
4886724,Chain rule and differentiability of $|x|^2$,"Going through Thomas Calculus, question 90 in the chapter on Chain Rule: Suppose that $f(x) =x^{2}$ and $g(x) =|x|$ . Then the composites $$
( f\circ g)( x) =|x|^{2} =x^{2} \ \ \ \ \ \ and\ \ \ \ \ ( g\circ f)( x) =|x^{2} |=x^{2}
$$ are both differentiable at $x=0$ even though $g$ itself is not differentiable at $x=0$ . Does this contradict the Chain Rule? I understand that $|x^{2}|$ is differentiable at $x=0$ because the inner function is differentiable. And the outer function receives only non-negative values, so it also turns out to be differentiable. But I don't believe that $f\circ g$ is differentiable. While the expression $|x|^{2}$ can be simplified to $x^{2}$ , these are in fact different functions. And because the inner $g$ isn't differentiable at $x=0$ , the whole composition isn't differentiable at this point. But strictly speaking the question itself doesn't sound correct - it says the composites are both differentiable. But what the author meant is the 3d function $c(x)=x^{2}$ is differentiable. Composites and $c(x)$ are not the same function - they just happen to give the same result. Do I understand the solution correctly? Is the question posed incorrectly?","['derivatives', 'chain-rule']"
4886730,"Among 101 dalmatian dogs, each dog has a unique number of black spots, Addition property","Among 101 dalmatian dogs, each dog has a unique number of black spots from the set {1, 2, 3, . . . , 101}. We choose any 52 of the 101 dogs. We want to prove that any set of 52 dogs satisfies the following addition property: Addition property: The numbers of spots on two of the dogs add up to exactly the number of spots on some other dog. (a) Which proof technique is most appropriate and why? (b) Prove that any set of 52 dogs has the addition property. Hint: First, prove the statement assuming that one of the 52 chosen dogs has 101 spots. Then generalize. (c) How many among the 101 dogs have an odd number of spots that is not divisible by 3 and not divisible by 5. Show your work. What I did a) I am thinking it is the pigeonhole principle; however, I am confused on the potential setup. b) 52+49 = 101 so this is two of the chosen dogs to 101. This can be generalized in that the second number can be decreased to get to 52+1= 53, this is the set of all dogs {53,54,..,101}, then 52 decreased in the same manner to 1+1=2 to get {2,3,...,52}, and some form of this to find the set of all dogs. However, I don't know how to formalize this proof nor what proof type it is. c) This is inclusion-exclusion principle. I started off with 51 dogs, which have an odd number of spots, and proceeded to find A and B where A = Divisible by 3 and B = Divisible by 5, used the formula |A+B| = |A| + |B| - |A ∩ B|. To get 24 that are divisible by 3 and 5, subtract that from 51 to get 27 that aren't, so 27 is the final answer. Question : I am confused on what proof to use for B and how to set up A with pigeonhole principle.","['algebra-precalculus', 'cardinals', 'discrete-mathematics']"
4886755,"Using derivatives to prove an inequality and, as an application, computing a limit","Show that for every $x \in \mathbb{R}^{+}$ $$x-\frac{x^{2}}{2} < \log (x+1) < x,$$ and as an application, compute the limit $$\lim_{n\to \infty}\prod_{k=1}^{n}\left(1+\frac{k}{n^{2}}\right).$$ My attempt: Consider the map $f:\mathbb{R}_{0}^{+} \to \mathbb{R}$ defined by $f(x) = \log (x+1) -x+\frac{x^{2}}{2}$ . Then $f$ is differentiable and $$f'(x) = \frac{1}{1+x} - 1 + x = \frac{x^{2}}{x+1}.$$ Therefore, $f'(x) = 0$ if and only if $x = 0$ and $f'(x) > 0$ for all $x>0$ . This means that $f$ attains its absolute minimum at 0 and so we have $$\log (x+1) -x+\frac{x^{2}}{2} = f(x) > f(0) = 0$$ for every $x>0$ , which implies that for all $x>0$ $$x-\frac{x^{2}}{2} < \log (x+1).$$ Similarly, define $g: \mathbb{R}_{0}^{+} \to \mathbb{R}$ by $g(x) = x- \log (x+1)$ . Then $g$ is differentiable and $$g'(x) = \frac{x}{x+1}.$$ Hence, a similar argument shows that $g$ attains its absolute minimum at 0 and, for every $x>0$ $$\log(x+1) < x.$$ This shows that $$x-\frac{x^{2}}{2} < \log (x+1) < x,\ \ \text{for all}\ x > 0.$$ Now note that, for every $1 \leq k \leq n$ , we have $$\log \left(1+\frac{1}{n^{2}}\right) + \log \left(1+\frac{2}{n^{2}}\right) + \ldots + \log\left(1+\frac{n}{n^{2}}\right)\\ < \frac{1}{n^{2}} + \frac{2}{n^{2}} + \ldots + \frac{n}{n^{2}}\\ = \frac{1+2+\ldots+n}{n^{2}} \\ = \frac{n(n+1)}{2n^{2}} $$ , and $$\log \left(1+\frac{1}{n^{2}}\right) + \log \left(1+\frac{2}{n^{2}}\right) + \ldots + \log\left(1+\frac{n}{n^{2}}\right)\\ > \frac{1}{n^{2}}-\frac{1}{2n^{4}} + \frac{2}{n^{2}}-\frac{4}{2n^{4}} + \ldots + \frac{n}{n^{2}}-\frac{n^{2}}{2n^{4}}\\ =  \frac{1+\ldots+n}{n^{2}} - \frac{1+4+\ldots+n^{2}}{4n^{4}}\\ = \frac{n(n+1)}{2n^{2}} - \frac{1+4+\ldots+n^{2}}{4n^{4}}.$$ This gives $$\lim_{n \to \infty} \frac{n(n+1)}{2n^{2}} - \frac{1+4+\ldots+n^{2}}{4n^{4}} = \frac{1}{2} - 0 = \frac{1}{2},$$ and $$\lim_{n \to \infty} \frac{n(n+1)}{2n^{2}} = \frac{1}{2},$$ which, by the squeeze theorem, implies that $$\lim_{n\to\infty} \log \left(1+\frac{1}{n^{2}}\right) + \log \left(1+\frac{2}{n^{2}}\right) + \ldots + \log\left(1+\frac{n}{n^{2}}\right) = \frac{1}{2}.$$ But then $$\lim_{n\to \infty}\prod_{k=1}^{n}\left(1+\frac{k}{n^{2}}\right) = e^{\lim_{n\to \infty}\log \left(\prod_{k=1}^{n}\left(1+\frac{k}{n^{2}}\right)\right)} = e^{\frac{1}{2}}.$$ I think the first part is correct but I don´t know if I´ve made some mistake with the arithmetic in the second part. Is it correct?","['limits', 'derivatives', 'analysis', 'sequences-and-series']"
4886789,$\int_{-\infty}^\infty \frac{1}{x^5+1}dx$ using contour integration.,"I am wondering if I have correctly computed this integral, which I see in a lot of posts as being really hard. $\int_{-\infty}^\infty \frac{1}{x^5+1}dx$ . Consider the following contour: The poles of $\frac{1}{z^5+1}$ occur at $e^{i\left(\frac{\pi}{5} + \frac{2\pi}{5} n\right)}$ where $n=0,1,2,3,4$ . Our contour only includes the poles $n=0,1$ . First, we compute the outermost circle. We parameterize the integral with $\gamma_1(t) = Re^{it}-1$ where $0\leq t \leq \pi$ . $$\int_{\gamma_1}\frac{1}{z^5+1}dz = \int_0^{\pi}\frac{Rie^{it}}{(Re^{it}-1)^5+1}dt.$$ Then taking $R \to \infty$ , since the $R$ term in the denominator dominates, $$\lim_{R\to \infty} \left|\int_0^{\pi}\frac{Rie^{it}}{(Re^{it}-1)^5+1}dt \right|\leq \lim_{R\to\infty}\pi \sup_{t}\left|\frac{Rie^{it}}{(Re^{it}-1)^5+1}\right|= 0$$ and now for the inner circle $\gamma_2(t) = \epsilon e^{-i\theta}-1$ $0\leq \theta \leq \pi$ , by the binomial formula, $$\lim_{\epsilon\to 0} \int_0^{\pi}\frac{-\epsilon ie^{it}}{(\epsilon e^{it} -1)^5+1}dt \\
=\lim_{\epsilon \to 0}\int_{0}^\pi \frac{-\epsilon i e^{it}}{\epsilon^5 e^{5 i t}-5 \epsilon^4 e^{4 i t}+10 \epsilon^3 e^{3 i t}-10 \epsilon^2 e^{2 i t}+5 \epsilon e^{i t}-1 + 1}dt\\
=\lim_{\epsilon \to 0}\int_{0}^\pi \frac{-i}{\epsilon^4 e^{4 i t}-5 \epsilon^3 e^{3 i t}+10 \epsilon^2 e^{2 i t}-10 \epsilon e^{ i t}+5}dt\\
=-\frac{i\pi}{5}.$$ Now it remains to just compute the residues inside the contour. Both poles are simple, so by the residue formula, $$\lim_{z\to e^{i\frac{\pi}{5}}} \frac{(z-e^{i\frac{\pi}{5}})}{z^5+1}= \frac{1}{5e^{i\frac{4\pi}{5}}}$$ and $$\lim_{z\to e^{i\frac{3\pi}{5}}} \frac{(z-e^{i(\frac{3\pi}{5})})}{z^5+1}= \frac{1}{5e^{i\frac{12\pi}{5}}}$$ Then by the residue theorem, $$\int_{-\infty}^\infty \frac{1}{x^5+1}dx  = 2\pi i \left[\frac{1}{5e^{i\frac{4\pi}{5}}} + \frac{1}{5e^{i\frac{12\pi}{5}}}\right]+\frac{i\pi}{5}= \frac{1}{5} \sqrt{5 + 2 \sqrt{5}}\pi.$$","['complex-analysis', 'calculus', 'contour-integration', 'complex-integration', 'complex-numbers']"
4886800,Prove that the following function is one-to-one,"Define a function $g$ from the set of real numbers to $S$ by the following formula: $$
g(x) = \frac12\biggl( \frac x{1+|x|} \biggr) + \frac12,\quad x\in\mathbb{R}.
$$ Prove that $g$ is a one-to-one correspondence. (It is possible to prove this statement either with calculus or without it.) What conclusion can you draw from this fact? My question is that what is the conclusion we can draw after we decide that it is a one-to-one correspondence? I would prove its one-to-one correspondence through its graph, which is one-to-one in that no two $x$ 's are mapped to the same $y$ .","['algebra-precalculus', 'functions', 'discrete-mathematics']"
4886832,Evaluate the series which looks like a telescopic series but isn't one?,Evaluate $$\sum_{r=0}^\infty\frac{1}{(3r+1)(3r+2)}$$ Wolfram alpha gives the answer $\pi/(3\sqrt{3})$ so I know for sure that this requires multiple mathematical concepts like the taylor series which isn't the telescopic series. I think complex numbers and the cube root of unity may have something to do with it due to the tricyclic nature of the series but I can't lay my finger on the approach.,"['taylor-expansion', 'complex-numbers', 'sequences-and-series']"
4886849,Finding if an ideal is the radical of another one...,"suppose we have, in the ring $\mathbb{Z} [x,y,z,w,v]$ , the following polynomials: $f=xw-yz$ , $g=x^2z-y^3$ , $h=yw^2-z^3$ , $k=xz^2-y^2w$ . The question is to prove that $I=(f,g,h,k)$ is the radical ideal of $J=(f,g,h)$ . I tried several things to no avail: by imposing $x>y>z>w$ I tried to divide $k^2$ by $J$ , but since I don't know whether $f,g,h$ is a Grobner base I stopped since having the residual being non zero wouldn't have said anything about it (also, the residual is not zero in this case). I then remembered that if $J$ is primary, the radical is the smallest prime who contains it. But checking that $J$ is primary seems like hell honestly. Since those are not simple polynomials, checking if any power of $k$ is contained in $J$ seems crazy. I could use some kind of software to find a Grobner base of $J$ , but I'd like to do it by my own hand, and since my professor didn't explain how to find Grobner bases, I don't think it's the intended method to solve the exercise. Am I comitting some judgement errors here? How do I prove the thesis? Can anybody help? Thank you in advance.","['maximal-and-prime-ideals', 'algebraic-geometry', 'polynomials', 'commutative-algebra']"
4886861,Least square derivatives,"Let $X_1, \ldots, X_N \in \mathbb{R}^p$ and $Y_1, \ldots, Y_N \in \mathbb{R}$ . Define $$
X=\left[\begin{array}{c}
X_1^{\top} \\
\vdots \\
X_N^{\top}
\end{array}\right] \in \mathbb{R}^{N \times p}, \quad Y=\left[\begin{array}{c}
Y_1 \\
\vdots \\
Y_N
\end{array}\right] \in \mathbb{R}^N
$$ Let $$
\ell_i(\theta)=\frac{1}{2}\left(X_i^{\top} \theta-Y_i\right)^2 \quad \text { for } i=1, \ldots, N, \quad \mathcal{L}(\theta)=\frac{1}{2}\|X \theta-Y\|^2 .
$$ Show (a) $\nabla_\theta \ell_i(\theta)=\left(X_i^{\top} \theta-Y_i\right) X_i$ and (b) $\nabla_\theta \mathcal{L}(\theta)=X^{\top}(X \theta-Y)$ . Hint. For part (a), start by computing $\frac{\partial}{\partial \theta_j} \ell_i(\theta)$ . For part (b), use the fact that $$
M v=\sum_{i=1}^N M_{:, i} v_i \in \mathbb{R}^p
$$ for any $M \in \mathbb{R}^{p \times N}, v \in \mathbb{R}^N$ , where $M_{:, i}$ is the $i$ th column of $M$ for $i=1, \ldots, N$ . My Preliminary Progress:
For part (a): I started with the definition of $\ell_i(\theta)$ and applied the chain rule to compute the gradient, arriving at: $$
\nabla_\theta \ell_i(\theta)=\left(X_i^{\top} \theta-Y_i\right) X_i
$$ For part (b): Using the overall loss $\mathcal{L}(\theta)$ , I leveraged matrix differentiation rules and the hint provided regarding matrix-vector multiplication to deduce that: $$
\nabla_\theta \mathcal{L}(\theta)=X^{\top}(X \theta-Y)
$$ My Questions: Rigorous Derivation: Could someone provide a step-by-step, rigorous derivation of both (a) and (b), possibly highlighting any subtleties in matrix calculus that I might have glossed over? Matrix Calculus Techniques: Are there specific matrix calculus techniques or identities that are particularly useful in simplifying these derivations? Generalization: How might these results generalize to other loss functions or optimization problems within machine learning? I believe that understanding these derivations in depth will greatly enhance my comprehension of optimization in statistical learning. Thank you in advance for your time and help!","['derivatives', 'gradient-descent', 'least-squares', 'linear-algebra']"
4886875,Classical Nullstellensatz implies Hilbert Nullstellensatz,"In artin's Algebra book (1st edition) the following theorems are stated: (Hilbert's Nullstellensatz) In $\mathbb{C}[x_1,...,x_n]$ maximal ideals are of the form $\langle x_1-a_1,...,x_n-a_n\rangle$ . (Classical Nullstellensatz) $f_1,...,f_n\in \mathbb{C}[x_1,...,x_n] $ . If $g=0$ in the variety defined by zeros of $f_1,...,f_n$ , then there is a power $g^m\in \langle f_1,...,f_n\rangle$ He then asks us to prove that the Hilbert Nullstellensatz is a consequence of the Classical Nullstellensatz. I think I was able to prove this, but it is not exactly pretty, I am looking for nicer solutions and for any possible flaw in my argument. Take $\mathcal{M}$ a maximal ideal and suppose $\mathcal{M}\not= \langle x_1-a_1,...,x_n-a_n\rangle$ for any $(a_1,...,a_n)\in \mathbb{C}^n$ . Because $\mathcal{M}$ is maximal and $\langle x_1-a_1,...,x_n-a_n\rangle$ are proper ideals of our ring this means that: $$\mathcal{M}\subset \langle x_1-a_1,...,x_n-a_n\rangle \quad \text{cannot hold.} $$ In other words, for every $(a_1,...,a_n)\in \mathbb{C}^n$ there is $m_{(a_1,...,a_n)}\in \mathcal{M}$ but not in $ \langle x_1-a_1,...,x_n-a_n\rangle $ . This means that $m_{(a_1,...,a_n)}(a_1,...,a_n)\not=0$ . Similarly, because $\langle x-a_1,...,x-a_n\rangle$ are maximal ideals, if it were contained in $\mathcal{M}$ it would be $\mathcal{M}$ , so we also need $m^{(a_1,...,a_n)}\in \langle x-a_1,...x-a_n\rangle$ but not in $ \mathcal{M}$ . Consider the following variety where $f_1,...,f_k$ are drawn from $\mathcal{M}$ , which has $d\geq 1$ zeros given by $k_1,...,k_d\in \mathbb{C^n}$ . $$\begin{cases}
f_1=0\\
....\\
f_k=0\end{cases}$$ $m^{k_1}\cdot...\cdot m^{k_d}$ is zero in the variety, so $(m^{k_1}\cdot...\cdot m^{k_d})^n\in \langle f_1,...,f_k\rangle\subset \mathcal{M}$ by the Nullstellensatz. Because $\mathcal{M}$ is maximal it is prime which implies that $m^{k_1}\cdot...\cdot m^{k_d}\in \mathcal{M}$ and $m^{k_j}\in \mathcal{M}$ for some $j$ which is absurd! Hence a variety drawn from $k$ polynomials of $\mathcal{M}$ has no zeros. But this implies by a proposition from Artin: $$g_1f_1+...+g_k f_k=1\in \mathcal{M}$$ So $\mathcal{M}=\mathbb{C}[x_1,...,x_n]$ which is absurd.","['ring-theory', 'algebraic-geometry', 'solution-verification']"
4886985,$n$-tuples as functions - why do they need to be surjective?,"The idea of using n-tuples to represent functions is introduced in Terence Tao's Analysis I ex 3.5.2: Suppose we define an ordered $n$ -tuple to be a surjective function $x
> : \{i \in N : 1 \leq i \leq n\} \to X$ whose codomain is some
arbitrary set $X$ (so different ordered n-tuples are allowed to have
different ranges); we then write $x_i$ for $x(i)$ and also write $x$ as $(x_i)_{1 \leq i \leq n}$ . Using this definition, verify that we
have $(x_i)_{1 \leq i \leq n} = (y_i)_{1 \leq i \leq n}$ if and only
if $x_i = y_i$ for all $1 \leq i \leq n$ . Question: Why does the function need to be surjective ? My Thoughts Initially I thought this was to ensure the range of the function was fully accounted for by the n-tuple. However, Tao is quite precise in using range and codomain precisely, so I don't think this is an avenue worth pursuing. Then I thought about the traditional textbook approach of considering the definition of surjective - that every element of the codomain has an element in the domain which is mapped to it. But that line of thinking doesn't seem to help here. Exploring this site, I can see that surjectivity is only needed for the later part of the exercise: Also, show that if $(X_i)_{1 \leq i \leq n}$ are an ordered n-tuple of
sets, then the Cartesian product, as defined in Definition 3.5.6, is
indeed a set. (Hint: use Exercise 3.4.7 and the axiom of
specification.) I can't even begin to attempt this (I'm a beginner) - so I suspect understanding the need for surjectivity may help?","['elementary-set-theory', 'functions']"
4887002,Three Notions of Small,"I’m currently learning about the history of the development of the Lebesgue integral in Thomas Hawkins’s book “Lebesgue’s Theory of Integration; It’s Origins and Development” Hawkins is stressing how much early confusion about the Riemann integral came from conflating three different notions of “small” subsets of reals: two topological notions and one measure theoretic notion. The three notions are: Nowhere dense : A set $X \subseteq \mathbb{R}$ is nowhere dense if and only if $\text{int}(\text{cl}(X)) = \emptyset$ . First species: For $X \subseteq \mathbb{R}$ , let $X’$ denote the set of limit points of $X$ . Let $X^{(n)} = ( \cdots ((X’)’)’ \cdots )’$ , where limit points are taken $n$ times. $X$ is first species if and only if $X^{(n)} = \emptyset$ for some $n$ . Zero Jordan Outer Content: Let $X \subseteq \mathbb{R}$ , and let $\mathcal{I} = \{I_1, \cdots, I_n\}$ be a finite collection of intervals such that $X \subseteq \bigcup_j I_j$ . The Jordan outer content of $X$ is $c_o(X) = \inf_{\mathcal{I}} \sum_{I \in \mathcal{I}} \ell(I)$ , where $\ell(I)$ is the length of the interval $I$ . A set has zero Jordan outer content if and only if $c_o(X) = 0$ . Hawkins’s book gives the following results: Dini proved that every first species set has zero Jordan outer content. Volterra prove that there are nowhere dense sets with positive Jordan outer content. du Bois Reymond independently proved that there are nowhere dense sets that aren’t of first species (this is implied by Volterra + Dini, of course). This leaves open two possible containments. (A) Is every first species set nowhere dense? (B) Is every set of zero Jordan content nowhere dense? I think both of these should be true, but I’d like to have my understanding checked. Thanks!","['measure-theory', 'real-analysis', 'descriptive-set-theory', 'math-history', 'set-theory']"
4887099,Geodesics in Hyperbolic Disk,"Let $\Gamma \le \operatorname{PSL}(2,\mathbb{R})$ be a discrete subgroup and $\Sigma:=\mathbb{D}^2/\Gamma$ be the quotient. Then $\Sigma$ is a hyperbolic surface whose universal cover is $\mathbb{D}^2$ and the fundamental group $\pi_1(\Sigma,x)$ ( $x\in\Sigma$ ) is isomorphic to $\Gamma$ . Fix a point $\tilde{x}\in\mathbb{D}^2$ over $x$ . For two closed geodesics $\alpha, \beta:S^1(\cong \mathbb{R}/\mathbb{Z})\rightarrow \Sigma$ ,
which are elements of $\pi_1(\Sigma,x)$ (so $\alpha(0)=\alpha(1)=\beta(0)=\beta(1)=x$ ), let $\tilde{\alpha}_0, \tilde{\beta}_0:\mathbb{R}\rightarrow\mathbb{D}^2$ be the liftings of $\alpha, \beta$ , respectively, with $\tilde{\alpha}_0(0)=\tilde{\beta}_0(0)=\tilde{x}$ . Then let $\tilde{\alpha}_1:\mathbb{R}\rightarrow \mathbb{D}^2$ be the lifting of $\alpha$ with $\tilde{\alpha}_1(0)=\tilde{\beta}_0(1)$ , and $\tilde{\beta}_1:\mathbb{R}\rightarrow \mathbb{D}^2$ the lifting of $\beta$ with $\tilde{\beta}_1(0)=\tilde{\alpha}_0(1)$ . My claim is that $\tilde{\alpha}_0$ , $\tilde{\beta}_0$ , $
\tilde{\alpha}_1$ and $\tilde{\beta}_1$ do not make an embedded quadrangle in $\mathbb{D}^2$ . In the case the loops are not assumed to be geodesics,
I already know that there are counterexamples,
see Paths in the hyperbolic disk . I tried prove the claim with geodesics (or minimally meeting curves) with Gauss-Bonnet theorem, but it gave me only a restriction between angles.
Can I show this using another techniques in hyperbolic geometry or find some counterexamples? Any help or hint will be appreciated.","['geometric-group-theory', 'hyperbolic-geometry', 'differential-geometry']"
4887132,"Prove that every root of $P(z)$ in the closed unit disc has multiplicity at most $c \cdot \sqrt{n}$, where $c = c(M) > 0$ is constant depending on $M$","Problem statement: Let $P(z)$ be a polynomial of degree $n$ with complex coefficients, $P(0) = 1$ , and $|P(z)| \leq M$ for $|z| \leq 1$ . Prove that every root of $P(z)$ in the closed unit disc has multiplicity at most $c \cdot \sqrt{n}$ , where $c = c(M) > 0$ is a constant depending only on $M$ . attempt (this is my friends attempt because I don’t know where to start) It is sufficient to examine the multiplicity of the number 1. In fact, if we prove something for 1, then we may apply the result to the polynomial $p(z) = P(\alpha z)$ with $|\alpha| \leq 1$ , and in this way, we obtain the same estimate for all roots lying in the unit disc. The idea of the solution is the following. We consider the integral $$F(P) = \int_{0}^{2\pi} \log(|P(e^{i\phi})|) \, d\phi$$ and show that it exists and is nonnegative. Then we estimate it from above, once in the neighborhood of 1 with the aid of the multiplicity of 1 and the degree of $P$ , and once at other points using the condition $|P(z)| \leq M$ . It is sufficient to prove the existence of the integral for polynomials of the form $z - z_0$ . If $$P(z) = c \cdot \prod_{i=1}^{n} (z - z_i)$$ then $$\log(|P(z)|) = \log(|c|) + \sum_{i=1}^{n} \log(|z - z_{i}|).$$ The existence of $$\int_{0}^{2\pi} \log(|e^{i\phi} - z_{0}|) \, d\phi$$ is ensured since $|e^{i\phi} - z_{0}| \geq 0$ for all $\phi$ , and the integrand is continuous on the interval $[0, 2\pi]$ .","['complex-analysis', 'analytic-geometry', 'polynomials', 'complex-numbers']"
4887164,QuantGuide Busted 6 II,"This question is from QuantGuide(Busted 6 II): Suppose you play a game where you continually roll a die until you obtain either a 5 or a
6. If you receive a
5, then you cash out the sum of all of your previous rolls (excluding the
5). If you receive a
6, then you receive no payout. You have the decision to cash out mid-game. What is your expected payout following the optimal strategy? My Approach: First I look into the case when we can't cash out mid-game. The expected value is 2.5 in this case.
Now for the additional option of stopping midgame, we calculate the expected value at each stage of the dice throw.
For the $i^{th}$ throw the expected value will be: \begin{equation}
\frac{2}{3}^i(2.5i)+\frac{2}{3}^{i-1}(2.5(i-1))\frac{1}{6}
\end{equation} The 2.5 value is due to each throw having the average value of the dice roll to be $\frac{1+2+3+4}{4}$ (the first term is for when all the throws till now don't have 5 or 6 and the second term is for the case of landing with a 5 in the $i^{th}$ throw). The value I am getting is approximately 2.59. The case without the option to cash out was solved at Roll until 5 or 6 is obtained on die without mid-game cash out .","['expected-value', 'probability-theory']"
4887165,Are Strong- and weak Operator topologies on separable Hilbert spaces sequential?,"If I am not mistaken, the norm operator topology should make the set of bounded operators into a sequential space, since the norm defines a metric. I was wondering if the Weak and Strong Operator topologies also turn the bounded operators into a sequential space, i.e. is a sequentially closed set in those topologies automatically closed? If that makes things easier, I am only interested in bounded operators on a separable Hilbert space. Best
Lev","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4887187,Why is maximum number of joints of 6 lines is 4?,"The following is considered in Lary Guth's Polynomial Methods in Combinatorics, page 14.
Let $L$ be a set of lines in $\mathbb R^3$ . A point $x$ which lies in some set of three non-co-planar lines of $L$ is called a joint of $L$ . Suppose $L$ has $6$ lines. Then, why is it that $L$ has at most $4$ joints? This has been my approach so far: Note that the tetrahedron has 6 edges and 4 vertices. If we take our $L$ to be the set of 6 lines containing each of the six edges of a tetrahedron, we get that each of its vertex is a joint as the three lines intersecting any vertex are non-co-planar. Now, I want to argue that if one wants to maximize the number of joints possible for any set of six lines, the configuration of a tetrahedron is the best possible one. The problem is that I don't know why or how to prove 3.
Any suggestions will be really helpful :)","['euclidean-geometry', 'geometry', 'affine-geometry']"
4887210,"In complex geometry, is an holomorphic function continuous by definition?","I am attending a course on Introduction to Complex Geometry, and the definition they have given me of a holomorphic function between complex manifolds is as follows: Let $X,Y$ be complex manifolds. A map $f: X \rightarrow Y$ is $\textbf{holomorphic}$ if 1. f is continuous and 2. $\forall p\in X$ , there exist charts $(U,\phi)$ in $X$ , $(V,\psi)$ in $Y$ so that $\psi \circ f \circ \phi^{-1} : \phi(U\cap f^{-1}(V)) \rightarrow \psi(V)$ is holomorphic. My question is: Is the first hypothesis (f continuous) necessary? Does not the second one imply the first one? This question also applies to Differential Geometry in the real case. Does anyone know if there is a counterexample if the first hypothesis is removed? Thanks.","['complex-geometry', 'differential-topology', 'differential-geometry']"
4887211,"Prove that $\sum_{i=1}^{k} \lambda_i f(g_i x) \geq 0$ holds for all $x \in G$, then $\sum_{i=1}^{k} \lambda_i \geq 0.$","Problem statement: Let $f(x) \geq 0$ be a nonzero, bounded, real function on an Abelian group $G$ , $g_1, \ldots, g_k$ are given elements of $G$ , and $\lambda_1, \ldots, \lambda_k$ are real numbers. Prove that if $$\sum_{i=1}^{k} \lambda_i f(g_i \cdot x) \geq 0$$ holds for all $x \in G$ , then $$\sum_{i=1}^{k} \lambda_i \geq 0.$$ My failed attempt We can suppose that $f(g_{1}) \geq 0$ . Denote by $A_{n}$ the set of those elements that can be written in the form $g_{1}^{\alpha_{1}}, \ldots, g_{k}^{\alpha_{k}}$ , where the maximum absolute value of the numbers $\alpha_{1}, \ldots, \alpha_{k}$ is $n$ , where $n > 0$ is an integer. Denote by $S(H)$ the sum $\sum_{x \in H} f(x)$ where $H$ is a finite set. In $$\lim_{n \to \infty} \frac{S(A_{n+1}) - S(A_{n-1})}{S(A_{n})} = 0$$ holds, since if for some $\epsilon > 0$ and for all $n > 0$ , $$\frac{S(A_{n+1}) - S(A_{n-1})}{S(A_{n})} > \epsilon$$ . would hold, then $$S(A_{n+1}) > S(A_{n-1}) + \epsilon S(A_{n}) \geq (1 + \epsilon) S(A_{n-1})$$ and so $$S(A_{2n+1}) \geq (1+ \epsilon)^{n} S(A_{1})$$","['group-theory', 'abelian-groups', 'real-analysis']"
4887239,Solve $dy/dx = \sin(x+y)$,"Let $u(x)=x+y(x)$ , which implies $\frac{du(x)}{dx}=1+\frac{dy(x)}{dx}$ . The equation becomes separable : $$ \frac{du}{\sin(u)+1}=dx \iff \tan(u)-\frac{1}{\cos(u)}=x+C $$ This last equation seems to be a transcendental equation with no direct solution. However, the solutions are $y=-x-\pi /2+2k\pi$ , or $\sin(x+y)-1=(x+C)\cos(x+y)$ or $y=-x-2\arctan(1+\frac{2}{x+C})+2k\pi$ . Except for the first solution, I cannot manage to find those.",['ordinary-differential-equations']
4887242,A group isomorphic to $\mathbb{Z}_2\times\mathbb{Z}_2\times\mathbb{Z}_3$,"I am taking a first course on group theory. I understand why $\mathbb{Z}_2\times\mathbb{Z}_3 \cong \mathbb{Z}_6$ . How can I use this fact to show that $\mathbb{Z}_2\times\mathbb{Z}_2\times\mathbb{Z}_3 \cong \mathbb{Z}_2\times\mathbb{Z}_6$ ? More in general, it looks like (but I am not sure if this is true) that given $\mathbb{G}_1\times\mathbb{G}_2\times\mathbb{G}_3$ , I am allowed to take the right most ""pair"" (i.e. $\mathbb{G}_2\times\mathbb{G}_3$ ) and replace it with a group isomorphic to it. A sort of associativity rule for the external direct product.","['group-theory', 'abstract-algebra', 'finite-groups', 'group-isomorphism']"
4887247,"Figuring out if $\lim_{(x,y)\to(0,0)}\frac{-x^6y^1(x^2+1)}{(x^6+y^2)\sqrt{x^2+y^2}}$ exists","I need to find out if the limit exists. $$\lim_{(x,y)\to(0,0)}\frac{-x^6y^1(x^2+1)}{(x^6+y^2)\sqrt{x^2+y^2}}$$ First, I approached the limit from $y=0$ , and the result was $\frac{0}{x^7}$ . Then, I approached from $x = 0$ , and the result was $\frac{0}{y^3}$ . This made me assume that the limit does not exist. However, Wolfram Alpha calculated the limit as zero. What is the solution to this question?","['limits', 'multivariable-calculus']"
4887248,"If $f(X)=AX-XA$ is diagonalizable, show that $A$ is diagonalizable","Let $f:M_n(F)\rightarrow M_n(F), X\mapsto AX-XA$ . If $f$ is diagonalizable, I want to show that $A$ is diagonalizable.  I'd prefer to avoid Jordan Blocks. I know that $f$ is diagonalizable if and only if: its minimal polynomial is square-free, or there exist $d$ linearly independent eigenvectors where $d = \dim M_n(F)$ , or the characteristic polynomial of $f$ factors into linear terms and each geometric multiplicity equals the corresponding algebraic multiplicity.","['matrices', 'diagonalization', 'linear-algebra']"
4887295,Finding and proofing a closed formula for $\sum_{n=1}^k\sqrt{1+\frac{1}{n^2}+\frac{1}{(n+1)^2}}$,"I want to find and proof a closed formula for the following sum $$\sum_{n=1}^k\sqrt{1+\frac{1}{n^2}+\frac{1}{(n+1)^2}}=\sqrt{1+\frac{1}{1^2}+\frac{1}{2^2}}+\sqrt{1+\frac{1}{2^2}+\frac{1}{3^2}}+\dots +\sqrt{1+\frac{1}{k^2}+\frac{1}{(k+1)^2}}$$ I have found a closed formula but I have problems with proofing it and proofing my steps in between. First I simplified the sum. To do that I calculated the radicands. After that I calculated the sum for $k=1$ to $k=4$ to see a pattern. Lastly I concluded from the pattern the closed formula: $$s_k=\frac{(k+1)^2-1}{(k+1)}=(k+1)-\frac{1}{(k+1)}$$ With $s_k$ I betitle the sum up to $k$ . This closed formula holds with the solutions. But now I want to proof it and my steps in between. Simplifying the sum I first calculated the following three radicands: \begin{alignat*}{3}
&1+\frac{1}{1^2}+\frac{1}{2^2}&&=\frac{9}{4}&&&=\frac{3^2}{2^2} \newline\newline
&1+\frac{1}{2^2}+\frac{1}{3^2}&&=\frac{49}{36}&&&=\frac{7^2}{6^2} \newline\newline
&1+\frac{1}{3^2}+\frac{1}{4^2}&&=\frac{169}{144}&&&=\frac{13^2}{12^2}
\end{alignat*} After calculating the radicands I saw the following pattern so that I could rewrite the sum like that: $$\sum_{n=1}^k\sqrt{1+\frac{1}{n^2}+\frac{1}{(n+1)^2}}=\sum_{n=1}^k\sqrt{\frac{(n\cdot(n+1)+1)^2}{(n\cdot(n+1))^2}}=\sum_{n=1}^k\frac{n^2+n+1}{n^2+n}$$ I have problems with proofing that this holds, I can't use induction to show this. Searching for a pattern $$
\begin{array}{c|c|c|c|c}
 \style{font-family:inherit}{k} &\style{font-family:inherit}{1} & \style{font-family:inherit}{2}
& \style{font-family:inherit}{3} & \style{font-family:inherit}{4}\newline \hline
  s_k & \frac{3}{2}                                       & \frac{8}{3}    & \frac{15}{4}   & \frac{24}{5} 
\end{array}
$$ Here I calculated the sum for different $k$ 's to see a pattern. I saw the correct closed formula: $$s_k=\frac{(k+1)^2-1}{(k+1)}=(k+1)-\frac{1}{(k+1)}$$ But here I am also inable to proof that that holds, as before I can't use induction . Proofing the simplification of the sum Because I can't use induction I tried to show that with simple transformations. I thought that I should start with showing that $1+\frac{1}{n^2}+\frac{1}{(n+1)^2}=\frac{(n^2+n+1)^2}{(n^2+n)^2}$ holds, because the removal of the square root seems trivial. \begin{align*}
    &1+\frac{1}{n^2}+\frac{1}{(n+1)^2} &= \newline\newline
    &\frac{n^2+1}{n^2}+\frac{1}{(n+1)^2} &= \newline\newline
    &\frac{(n^2+1)\cdot(n+1)^2}{n^2\cdot(n+1)^2}+\frac{n^2}{n^2\cdot(n+1)^2} &= \newline\newline
    &\frac{(n^2+1)\cdot(n+1)^2+n^2}{n^2\cdot(n+1)^2} &= \newline\newline
    &\frac{n^4+2n^3+3n^2+2n+1}{n^4+2n^3+n^2} &\overset{?}{=}\newline\newline
    &\frac{(n^2+n+1)^2}{(n^2+n)^2}
\end{align*} I have problems in seeing the last step which is marked with the questionmark. I know that denominator is given due to the binomial theorem. And I think that also the numerator is given about the binomial theorem. But I have problems in seeing how I could transform that. You could help me with giving more smaller steps in in between the equation with the question mark. I would be able to see that this holds the other way around because that is just factorization but I have problems to see it in this way. Proofing the closed formula Now I want to proof the following: $$\sqrt{1+\frac{1}{1^2}+\frac{1}{2^2}}+\sqrt{1+\frac{1}{2^2}+\frac{1}{3^2}}+\dots +\sqrt{1+\frac{1}{k^2}+\frac{1}{(k+1)^2}}=(k+1)-\frac{1}{(k+1)}$$ I know from a hint that I could use the fact that this is a telescoping sum. So I rewrote the sum into the following (I can do this because of the simplification): $$\frac{3}{2}+\frac{7}{6}+\frac{13}{12}+\dots+\frac{k^2+k+1}{k^2+k}$$ From here I tried different expansions, but didn't came up with one where the terms cancel each other. I tried to expand it like this: $$(\frac{4}{2}-\frac{1}{2})+(\frac{8}{6}-\frac{1}{6})+(\frac{14}{12}-\frac{1}{12})+\dots+(\frac{k^2+k+2}{k^2+k}-\frac{1}{k^2+k})$$ and like this: $$(2-\frac{1}{2})+(2-\frac{5}{6})+(2-\frac{11}{12})+\dots+(2-\frac{k^2+k-1}{k^2+k})$$ But in both of these tries nothing cancels. In conclusion I would appreciate smaller steps which help at my proof of the simplification of the sum, and I would appreciate help in finding the expansion such that the terms cancel.","['algebra-precalculus', 'telescopic-series', 'summation']"
4887304,Is there a 9×9 Sudoku Room Square?,"The following is an order 9 Room square . Copying from Wikipedia, Each cell of the array is either empty or contains an unordered pair from the set of symbols. Each symbol occurs exactly once in each row and column of the array. Every unordered pair of symbols occurs in exactly one cell of the array. This square meets a few additional requirements, each $3\times3$ square has five pairs, and a few of the squares have all ten symbols. Is an order 9 Room square possible where all nine $3\times3$ squares have all ten symbols? Note: I don't think row/column permutations for this particular Room square will yield an answer, due to trying a few million cases.","['sudoku', 'recreational-mathematics', 'combinatorics', 'latin-square']"
4887334,Is my solution to count the integer solutions for a linear inequality with bounds correct?,"I'm working on a problem where I need to count the number of integer solutions for the inequality $10 \leq x_1 + x_2 + x_3 + x_4 \leq 19$ , given that each $x_i$ (for $i = 1$ to $4$ ) is bounded by $-5 \leq x_i \leq 10$ . I approached the problem in two main steps, transforming the inequalities to work with non-negative integers by adjusting the bounds and then applying combinatorial reasoning. However, I'm unsure if my methodology and calculations are correct. Here's my approach: Step 1: Dealing with the upper bound First, I transformed the original inequality $x_1 + x_2 + x_3 + x_4 \leq 19$ to a new form where I could deal with non-negative integers by introducing a new variable $B$ and adjusting the bounds of $x_i$ : $x_1 + x_2 + x_3 + x_4 + B = 19$ , with $-5 \leq x_i \leq 10$ Then, I adjusted the variables to work within a non-negative range, leading to: $x_1 + x_2 + x_3 + x_4 + B = 39$ , with $0 \leq x_i \leq 15$ , $B \geq 0$ From there, I calculated the total number of solutions without restrictions, subtracted the cases where one variable exceeds its bound, and added the cases where two variables exceed their bounds. The solution for step 1 was given by: $CR(5,39) - (CR(5, 39-16) \times 4) + (CR(5, 39-32) \times C(4,2))$ Step 2: Dealing with the lower bound Similarly, for the lower bound $10 \leq x_1 + x_2 + x_3 + x_4$ , I considered cases where the sum is less than or equal to 9 (to subtract them from the total count): $x_1 + x_2 + x_3 + x_4 + B = 9$ , with $-5 \leq x_i \leq 10$ This was transformed to: $x_1 + x_2 + x_3 + x_4 + B = 29$ , with $0 \leq x_i \leq 15$ , $B \geq 0$ I then calculated the total number of solutions without restrictions for this case and subtracted the cases where one variable exceeds its bound. There were no cases for exceeding two variables. The solution for step 2 was: $CR(5, 29) - (CR(5, 29-16) \times 4)$ Final Solution: Combining the solutions from both steps, I derived the complete solution for the exercise as: $CR(5,39) - (CR(5, 39-16) \times 4) + (CR(5, 39-32) \times C(4,2)) - (CR(5, 29) - (CR(5, 29-16) \times 4))$ I'm seeking feedback on whether my approach and the steps I've taken are correct for solving this problem, particularly my use of combinatorial reasoning and the transformation of inequalities. Any insights into errors in my methodology or calculations would be greatly appreciated.","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
4887391,Limit of lacunar power series in $1^-$.,"Let $\sigma:\mathbb{N}\longrightarrow\mathbb{N}$ be strictly increasing, and consider the power series $$ S_{\sigma}(x)=\sum_{n=0}^{+\infty}(-1)^nx^{\sigma(n)}. $$ Can any real number in $[0,1]$ be obtained as the limit $\lim\limits_{x\rightarrow 1^-}S_{\sigma}(x)$ for some $\sigma$ ? According to this answer, the limit always is $\frac{1}{2}$ when $\sigma$ is a polynomial, WolframAlpha suggests that the limit is also $\frac{1}{2}$ with $\sigma(n)=n\log n$ (think of $\sigma(n)$ as the $n$ -th prime number). Therefore my question can also be : Is the limit $\lim\limits_{x\rightarrow 1^-}S_{\sigma}(x)$ always $\frac{1}{2}$ ? if not, can any rational number in $[0,1]$ be obtained this way for some $\sigma$ ?","['real-analysis', 'analytic-number-theory', 'lacunary-series', 'power-series', 'limits']"
4887434,Solve the second order equation $u^{\prime \prime}(t)=\frac{16 t\left(\beta t^3-27\right)}{81 \beta} u(t)$,"I need to solve the second order equation $$u^{\prime \prime}(t)=\frac{16 t\left(\beta t^3-27\right)}{81 \beta} u(t)$$ or alternatively, $$u''(t) = \frac{16}{81}t^4u(t) - \frac{432}{81 \beta }tu(t)$$ $\beta > 0$ . I'm not sure how to approach this as it is second order. The equation is in normal/regular form. Wolfram gives me a series solution, but I would like something closed form. The original equation was $$9f''-10t^2f'+\left(t^4+\left(\frac{48}{\beta}-10\right)t\right)f =0 $$ and I used the integrating factor $$f(t) = \exp\left({\frac{5}{27}t^3}\right)u(t)$$ to obtain the form seen above. EDIT: Here is a simpler related case which is solved: $$9 f^{\prime \prime}-10 t^2 f^{\prime}+\left(t^4-10 t\right) f=0$$ which tranforms into regular form by $$
f(t)=\exp \left(\frac{5}{27} t^3\right) u(t)
$$ where $u(t)$ then satisfies the equation $$
u^{\prime \prime}-\frac{16}{81} t^4 u=0
$$ the solutions of which are $$
t^{1 / 2} I_{ \pm 1 / 6}(T) \text { and } t^{1 / 2} K_{1 / 6}(T)
$$ where $T=\frac{4}{27} t^3$ and $I_{ \pm 1 / 6}(T)$ and $K_{1 / 6}(T)$ are the modified Bessel functions of order $\frac{1}{6}$ . In this way we see that the solutions of Eq. (1.4) are given by $$
t^{1 / 2} \exp \left(\frac{5}{27} t^3\right)\left\{I_{ \pm 1 / 6}(T), K_{1 / 6}(T)\right\}.
$$","['calculus', 'special-functions', 'ordinary-differential-equations', 'bessel-functions']"
4887465,Help Antie evaluate Gauss curvature of a smooth surface using ruler and a protractor,"Antie, a smart ant living on a smooth surface $S$ of $\mathbb{R}^3$ , would like to evaluate the Gauss curvature $K$ at a certain point $P\in S$ . Antie is aware of Gauss Theorema Egregium, according to which Gauss curvature may be evaluated using the first fundamental form of the surface. So, Antie grabs a ruler and a  protractor, determines a neighbourhood  around $P$ and is ready to start calculating, since it has heard that the first fundamental form is related to lengths and angles around $P$ (words like tangent space do not really make sense to Antie). Αntie knows that if he knew some quantities, called $E$ , $F$ , $G$ , then it would be able to evaluate $K$ . But of course Antie is not aware of the way the surface is embedded in space $\mathbb{R}^3$ and its local parametrization $\sigma(u,v)=( \sigma_1(u,v),   \sigma_2(u,v),\sigma_3(u,v))$ around $P$ (in order to evaluate $E=\sigma_u\cdot \sigma_u$ etc around $P$ ). How would we help Antie calculate all the quantities needed for Gauss curvature $K$ at $P$ ?","['surfaces', 'curvature', 'differential-geometry']"
4887487,Improving my way of showing $\sin^212^\circ+\sin^221^\circ+\sin^239^\circ+\sin^248^\circ=1+\sin^29^\circ+\sin^218^\circ$,"This problem is from 1904 and was given to students studying for the Cambridge and Oxford entry examinations. My solution is presented below, but I am of the opinion that it can be improved. All ideas welcome. Show that $$\sin^{2}{12^{\circ}}+\sin^{2}{21^{\circ}}+\sin^{2}{39^{\circ}}+\sin^{2}{48^{\circ}}=1+\sin^{2}{9^{\circ}}+\sin^{2}{18^{\circ}}$$ A solution $$\begin{align}
\sin^{2}{12^{\circ}}=\sin^{2}{(30^{\circ}-18^{\circ})} 
&=(\sin{30^{\circ}}\cos{18^{\circ}}-\cos{30^{\circ}}\sin{18^{\circ}})^{2} \tag1\\
&=\left(\frac{1}{2}\cos{18^{\circ}}-\frac{\sqrt{3}}{2}\sin{18^{\circ}}\right)^{2} \tag2\\
&=\frac{1}{4}\cos^{2}{18^{\circ}}+\frac{3}{4}\sin^{2}{18^{\circ}}-\frac{\sqrt{3}}{2}\cos{18^{\circ}}\sin{18^{\circ}} \tag3
\\ \\ \\
\sin^{2}{48^{\circ}}
&=\sin^{2}{(30^{\circ}+18^{\circ})} \tag4 \\
&= (\sin{30^{\circ}}\cos{18^{\circ}}+\cos{30^{\circ}}\sin{18^{\circ}})^{2} \tag5 \\
&=\left(\frac{1}{2}\cos{18^{\circ}}+\frac{\sqrt{3}}{2}\sin{18^{\circ}}\right)^{2} \tag6 \\
&=\frac{1}{4}\cos^{2}{18^{\circ}}+\frac{3}{4}\sin^{2}{18^{\circ}}+\frac{\sqrt{3}}{2}\cos{18^{\circ}}\sin{18^{\circ}} \tag7
\\ \\ \\
\sin^{2}{21^{\circ}} &=\sin^{2}{(30^{\circ}-9^{\circ})} \tag8\\
&=\left(\sin{30^{\circ}}\cos{9^{\circ}}-\cos{30^{\circ}}\sin{9^{\circ}}\right)^{2} \tag9\\ 
&=\left(\frac{1}{2}\cos{9^{\circ}}-\frac{\sqrt{3}}{2}\sin{9^{\circ}}\right)^{2} \tag{10}\\
&=\frac{1}{4}\cos^{2}{9^{\circ}}+\frac{3}{4}\sin^{2}{9^{\circ}}-\frac{\sqrt{3}}{2}\cos{9^{\circ}}\sin{9^{\circ}} \tag{11}
\\ \\ \\
\sin^{2}{39^{\circ}}=\sin^{2}{(30^{\circ}+9^{\circ})}  
&=(\sin{30^{\circ}}\cos{9^{\circ}}+\cos{30^{\circ}}\sin{9^{\circ}})^{2} \tag{12} \\
&=(\frac{1}{2}\cos{9^{\circ}}+\frac{\sqrt{3}}{2}\sin{9^{\circ}})^{2} \tag{13}\\
&=\frac{1}{4}\cos^{2}{9^{\circ}}+\frac{3}{4}\sin^{2}{9^{\circ}}+\frac{\sqrt{3}}{2}\cos{9^{\circ}}\sin{9^{\circ}} \tag{14}
\end{align}$$ So, $$\begin{align}
\text{LHS} &=\frac{3}{2}\sin^{2}{18^{\circ}}+\frac{1}{2}\cos^{2}{^18{\circ}}+\frac{3}{2}\sin^{2}{9^{\circ}}+\frac{1}{2}\cos^{2}{9^{\circ}} \tag{15}\\  
&=\sin^{2}{18^{\circ}}+sin^{2}{9^{\circ}}+\frac{1}{2}(sin^{2}{18^{\circ}}+\cos^{2}{18^{\circ}})+\frac{1}{2}(sin^{2}{9^{\circ}}+\cos^{2}{9^{\circ}}) \tag{16} \\  
&=\sin^{2}{9^{\circ}}+\sin^{2}{18^{\circ}}+\frac{1}{2}+\frac{1}{2} \tag{17} \\   
&=1+\sin^{2}{9^{\circ}}+\sin^{2}{18^{\circ}} \tag{18} \\
&=RHS \tag{19}
\end{align}$$",['trigonometry']
4887510,Uniqueness and continuous dependence on the data of Heat equation.,"Let two smooth $v_1$ and $v_2$ both satisfy the system $$\partial_t{v}-\Delta v=f \quad \text{in} \quad U \times (0,\infty), $$ $$v = g \quad \text{on} \quad \partial U \times (0,\infty),$$ for some fixed given smooth $f: \bar{U}\times (0,\infty) \rightarrow \mathbb{R}$ and $g: \partial U \times (0,\infty).$ $U$ is open, bounded and $U \subset \mathbb{R}^n.$ Show that $$\sup_{x \in U}  |v_1(t, x) − v_2(t, x)| \rightarrow 0,$$ as $t \rightarrow \infty.$ This is my work: Let $ u =v_1 -v_2,$ it is sufficient to prove $\sup_{x \in U}  |u(x,t)| \rightarrow 0,$ as $t \rightarrow \infty. (1)$ $u$ obeys the system $$\partial_t{u}-\Delta u=0 \quad \text{in} \quad U \times (0,\infty), $$ $$u = 0 \quad \text{on} \quad \partial U \times (0,\infty).$$ Multiply both sides by $u.|u|^{2(m-1)},$ note that $\partial_t(|u|^{2m})=2m\partial_tu.u.|u|^{2(m-1)}$ then $$\dfrac{1}{2m}\partial_t\int_{U}|u|^{2m}dx=\int_{U}\Delta u.u.|u|^{2(m-1)}dx$$ Apply integration by part for the RHS, we get $$\dfrac{1}{2m}\partial_t\int_{U}|u|^{2m}dx=-(2m-1)\int_{U}|\nabla u|^2|u|^{2(m-1)}dx.$$ By the generalize Poincare's inequality, we obtain $$\partial_t\int_{U}|u|^{2m}dx \leq -2C\left(2-\dfrac{1}{m}\right)\int_{U}|u|^{2m}dx$$ or $$\partial_t\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)} \leq -2C\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)}$$ $\Rightarrow 2m \Vert u(t,\cdot)\Vert^{2m-1}.\partial_t\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq  -2C\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert^{2m}_{L^{2m}(U)}$ $\Rightarrow \partial_t\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq -2\dfrac{C}{m}\left(2-\dfrac{1}{m}\right)\Vert u(t,\cdot)\Vert_{L^{2m}(U)}$ Applying Gronwall's inequality, we get $\Vert u(t,\cdot)\Vert_{L^{2m}(U)} \leq e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t}\Vert u(0,\cdot)\Vert_{L^{2m}(U)}.$ I am planing to let $m \rightarrow \infty$ to obtain $\Vert u(t,\cdot)\Vert_{L^{\infty}(U)}$ and let $t \rightarrow \infty$ then $e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t} \rightarrow 0$ to obtain (1). But the problem is as $m \rightarrow \infty,$ $e^{-2\frac{C}{m}\left(2-\frac{1}{m}\right)t} \rightarrow 1 \neq 0.$ Am I on the right track or did I make some wrong steps? Could you provide any ideas to improve my work?","['heat-equation', 'gronwall-type-inequality', 'analysis', 'partial-differential-equations']"
4887529,Characterizing functions that satisfy the reflection $f(z)f(-z)=1$,"Context: During a mathematical discussion with a good friend, I was brought to think about the functional equation $q(t)q(1-t) = 1$ for $q$ with some regularity. (To be honest the exact context doesn't really matter, since I am not trying to solve the original problem that this stemmed from). Upon brainstorming a few solutions ( $exp(t-1/2)$ , $\Gamma(t)\sqrt{\sin \pi t}$ up to a constant)), I came to realize that the solutions are plentiful. By reparameterizing the functions and choosing the regularity to be meromorphic (motivated by the fact that all the pretty solutions were meromorphic), I set myself to the following challenge: Characterise/Classify all the functions $f \in \mathcal{M}(\mathbb{D})$ which satisfy the reflection $f(z)f(-z)=1$ My attempt goes as follows: Denote by $G$ the set of such functions. After some verifications, it becomes clear that $G$ is actually a group with respect to the operation of pointwise multiplication* of functions (*one has to take care of removable singularities). Moreover, define the function \begin{align} K : \mathcal{M}(\mathbb{D})  &\to \mathcal{M}(\mathbb{D}) \\\\ f(z) &\to K(f)(z) = f(z)/f(-z) \end{align} Easy computations show the following three facts: The image of $K$ is inside $G$ , ie $Im(K) \subset G$ K is in fact a morphism of groups with respect to multiplication The kernel of $K$ is the subgroup of even meromorphic functions on the unit disk, let's call it $\mathcal{E}(\mathbb{D})$ Now using the first isomorphism theorem, one obtains the following isomorphism of groups: $\mathcal{M}(\mathbb{D})/\mathcal{E}(\mathbb{D}) \cong Im(K)$ Obviously for my purpose, I'd like $K$ to be onto $G$ , in other words, I want to be able to express every $g \in G$ as $g(z) = f(z) / f(-z)$ for some meromorphic $f$ , obtaining the isomorphism of groups: $\mathcal{M}(\mathbb{D})/\mathcal{E}(\mathbb{D}) \cong G$ There is some non formal evidence for that being the case, but I'd like to formalize it. The path I took was the following:
if $g \in G$ is without any zeroes, then it is also without poles, so there exists a holomorphic branch of its square root on $\mathbb{D}$ . Then $f(z) = \sqrt{g(z)}$ satisfies the following: $f(z) / f(-z) = \sqrt{g(z)}/\sqrt{g(-z)} = g(z) / (\sqrt{g(z)}\sqrt{g(-z)})$ , where the last denominators square is equal to one (since $g \in G$ ), so that it itself must be equal to one (if one takes the correct branch?). Thus any $g\in G$ without zeroes can be expressed as $f(z) / f(-z)$ for some meromorphic $f$ . To tackle the case where $g$ is allowed to be zero, we'll show a little lemma: lemma: $g\in G$ admits the desired decomposition iff there exists an even meromorphic $e$ such that there exists a meromorphic branch of $\sqrt{e(z)g(z)}$ proof: if such an $e$ exists then one can set $f(z) = \sqrt{e(z)g(z)}$ and redo the previous computations. For the converse, consider $f^2(z) / g(z) \blacksquare$ In our case, the poles and zeros of $g$ come in pairs: if $z$ is a zero of order $n$ then $-z$ is a pole of order $n$ and vice versa. By multiplying by our magic $e$ , we want to make all the orders of the zeroes/poles even, then $eg$ will admit a square root. An easy example is if $g$ has a finite number of zeroes of odd order, say $\alpha_1, ..., \alpha_n$ : one considers: $e(z) = \prod_k (z^2 - \alpha_k^2)$ . This finite product is an even function that has simple zeroes at all the points where $g$ has an odd ordered zero, thus $eg$ will only have even ordered zeroes and hence admits a square root. (and obviously the problem of the poles is also taken care of, since for any odd ordered pole $-\alpha_k$ , the product $eg$ will have the pole's order decreased by one, giving an even ordered pole) This is where the problem comes: what if $g$ has an infinite number of odd ordered zeroes? In that case the aforementioned product has no reason to converge (that I'm aware of). Is there a workaround with more well chosen functions than $(z^2 - \alpha^2)$ to make the infinite product converge? Or is this approach just not good enough to obtain the full result?","['complex-analysis', 'meromorphic-functions']"
4887673,Why is this function almost Lipschitz?,"We are still in the saga of solving the 2002 qualifier. This question 6b has stumped me and I am mostly clueless about it: Say $f:\mathbb{R}\rightarrow \mathbb{R}$ is bounded with a finite constant $B$ such that: $$\frac{|f(x+y)+f(x-y)-2f(x)|}{|y|}\leq B$$ Prove there exists $M(\lVert f \rVert_\infty, B)$ such that for all $x\not=y$ : $$|f(x)-f(y)|\leq M |x-y|\left(1+\ln_+(\frac{1}{|x-y|})\right)$$ Where $\ln_+(x)=\max \{0,\ln(x)\}$ Intuitively this means that away from $y=x$ , $f(x+y)\rightarrow f(x)$ linearly. Close to $x$ , it is still true $f(x+y)\rightarrow f(x)$ but it is slightly perturbed by $\ln_+$ . Here are a couple of facts which have gotten me nowhere: Fact 0. The inequality in $B$ would be an approximation for $f''(x)$ if it were divided by $y^2$ instead of $y$ . This is particularly useless, because we have no regularity associated with $f$ . Even if we did $|f''(x)|\leq \lim M/|y|=\infty$ so this observation cannot be of any help. Fact 1. $\lim_{y\rightarrow 0} |f(x-y)-f(x)|$ exists. One has by inverted triangle inequality that: $$\lim_{y\rightarrow 0}||f(x+y)-f(x)|-|f(x-y)-f(x)||\leq \lim_{y\rightarrow 0 }B|y|=0$$ But taking away the modulus and changing $\lim$ to $\limsup$ : $$\lim_{y\rightarrow 0}\sup_{y\in[-a,a]}(|f(x+y)-f(x)|-|f(x-y)-f(x)|)=0$$ $$\lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|\leq\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x-y)-f(x)|$$ But as we are taking the infimum in a symetric interval, hence we may write: $$\lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|\leq\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x+y)-f(x)|\leq \lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|$$ $$\lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|=\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x+y)-f(x)|$$ This means that $\lim |f(x+y)-f(x)|$ exists. Fact 2. There is the obvious $y=y-x$ substitution $$\frac{|f(y)+f(2x-y)-2f(x)|}{|y-x|}\leq B$$ $$|f(y)-f(x)|\leq B|y-x|+|f(2x-y)-f(x)|$$","['functional-equations', 'calculus', 'inequality', 'real-analysis']"
4887682,Neighboring property irrespective of continuity.,"Given, $f(x,y),g(x,y)$ functions with $f(x,y)\gt g(x,y)\gt 1$ , where $(x, y)\in [0,1]\times[0,1]$ . Now, irrespective of continuity of $f(x,y),g(x,y)$ , can we say that there is an open ball $B$ about $(1,1)$ such that $\left(g^{-\frac{x+y}{x+xy}}-f^{-\frac{x+y}{y+xy}}\right)-(f-g)\lt0$ for $(x,y)\in B\cap ([0,1]\times[0,1])$ ? At $(1,1)$ , clearly, $\left(g^{-1}-f^{-1}\right)-(f-g)\lt0$ . Then, without continuity, can I say $\left(g^{-\frac{x+y}{x+xy}}-f^{-\frac{x+y}{y+xy}}\right)-(f-g)\lt0$ is true for $(x,y)\in B\cap ([0,1]\times[0,1])$ ?","['multivariable-calculus', 'real-analysis']"
4887710,Restriction of global sections to the fiber without semicontinuity,"Let $f:X\to Y$ be a projective morphism of noetherian schemes and let $\mathcal{F}$ be a coherent sheaf on $X$ which is flat over $Y$ . If you read Hartshorne chapter 3 section 12 or Vakil chapter 25, you can see the development of a wonderful theory which leads to Grauert's theorem: If $Y$ is integral and $\dim_{k(y)} H^i(X_y,\mathcal{F}_y)$ is constant, then $R^if_*\mathcal{F}$ is locally free on $Y$ and the natural map $R^if_*\mathcal{F} \otimes k(y) \to H^i(X_y,\mathcal{F}_y)$ is an isomorphism. This is great. Can the case $i=0$ (i.e. global sections) be done without quite so much of the machinery? What I mean is, if in the situation above, we know $f_*\mathcal{F}$ is locally free, is there an argument which shows that $f_*\mathcal{F}\otimes k(y) \to H^0(X_y,\mathcal{F}_y)$ is an isomorphism without introducing the Mumford complex (theorem 25.2.1 in Vakil or lemma 3.12.3 in Hartshorne).","['semicontinuous-functions', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
4887731,"Proving Existence of Sequence and Series Satisfying Hardy-Littlewood Convergence Condition Prove that for every $\vartheta, 0 < \vartheta < 1$,","Prove that for every $\vartheta, 0 < \vartheta < 1$ , there exists a sequence $\lambda_{n}$ of positive integers and a series $\sum_{n=1}^{\infty} a_{n}$ such that: \begin{align*}
    (i) & \quad \lambda_{n+1} - \lambda_{n} > (\lambda_{n})^{\vartheta}, \\
    (ii) & \quad \lim_{r \to 1-0} \sum_{n=1}^{\infty} a_{n} r^{\lambda_{n}} \text{ exists}, \\
    (iii) & \quad \sum_{n=1}^{\infty} a_{n} \text{ is divergent}.
\end{align*} Background: András Simonovits noted that $\lambda_{n+1} > c\lambda_{n}$ ( $c > 1$ ) and the $(C,1)$ summability together guarantee convergence. Moreover, if we substitute $x = e^{-y}$ in G. H. Hardy, $\textit{Divergent Series}$ , Clarendon Press, Oxford, 1949, p. 87, Theorem 114 can be stated as follows: Given $c > 1$ constant and a sequence of natural numbers, such that $\lambda_{n+1} > c\lambda_{n}$ ( $c > 1$ ), $\sum_{n=1}^{\infty} a_{n} r^{\lambda_{n}}$ converges in the unit circle, and its limit exists as $r \to 1^{+}$ , then $\sum_{n=1}^{\infty} a_{n}$ is convergent. So, this theorem, conjectured by Littlewood and proved by Hardy and Littlewood, states that Abel summability and convergence are equivalent notions in the case of sequences satisfying the Hadamard gap condition $\lambda_{n+1} > c\lambda_{n}$ ( $c > 1$ ). Our problem states that the Hadamard gap condition cannot be substituted with a much weaker condition. The participants gave two kinds of generalizations for the problem. Some of them substitute Abel summability with the stronger $(C,1)$ summability; others showed that for any sequence $\lambda_{n}$ satisfying $\liminf (\lambda_{n+1} - \lambda_{n})/\lambda_{n} = 0$ , there exists a sequence $\sum_{n=1}^{\infty} a_{n}$ that satisfies the conditions of the problem. This latter statement is also a generalization of the theorem, since if $\lambda_{n} = e^{\sqrt{n}}$ , then... $$\lambda_{n+1} - \lambda_n = e^{\sqrt{n+1}} - e^{\sqrt{n}} = (e^{\sqrt{n+1} -  \sqrt{n}}  - 1)e^{\sqrt{n}}$$ $$= e^{\sqrt{n}} \left(e^{\frac{1}{\sqrt{n} + \sqrt{n+1}}} - 1\right) = \left(1 + o(1)\right) \frac{e^{\sqrt{n}}}{2\sqrt{n}}$$ And therefore $$\frac{\lambda_{n+1} - \lambda_{n}}{\lambda_{n}} \rightarrow 0, \quad \frac{\lambda_{n+1} - \lambda_{n}}{\lambda_{n}^{\vartheta}} \rightarrow \infty \quad \text{for } \vartheta < 1$$","['summation', 'analysis', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
4887777,why cant i integrate the derivative of a cylinders volume to find it again?,"For a cylinder, the volume formula is given by $V = \pi r^2 H $ . To find the differential ( dV ), we differentiate ( V ) with respect to ( r ) and ( H ), yielding $\left( dV = 2 \pi r H dr + \pi r^2 dH \right)$ . Upon integrating ( dV ) again, we obtain $( 2 \pi r^2 H )$ . However, I'm confused about where the factor of two originates. Additionally, in calculus, it's taught that to derive the formula for the volume of a cylinder, we perform a double integral of ( da ) over the cylinder's surface area ( A ) and ( dh ) over the height ( H ). This suggests that $( V = \iint 2 \pi r dr dh )$ , or equivalently $( V = \int \pi r^2 dH )$ , which only includes the second term of the earlier ( dV ) expression. Does this imply that the first term is zero?","['calculus', 'geometry', 'volume']"
4887789,"If $A$ is normal with $\sigma(A)\subseteq \mathbb{R}\cup\mathbb{T}$, does $\text{dim ker}(AB-BA)=\text{dim ker}(A^*B-BA^*)$?","This clearly holds if $A$ is self-adjoint, and also if $A$ is unitary, because then $A(\text{ker}(AB-BA))=\text{ker}(A^*B-BA^*)$ . To prove this, if $w\in\text{ker}(AB-BA)$ , then $A^*B(Aw)=A^*ABw=Bw=BA^*(Aw)$ , so $Aw\in\text{ker}(A^*B-BA^*)$ and if $v\in\text{ker}(A^*B-BA^*)$ , then $v=AA^*v$ and with $w=A^*v$ we have $ABw=ABA^*v=AA^*Bv=Bv=BAA^*v=BAw$ , so $w\in\text{ker}(AB-BA)$ . If $A$ is normal with spectrum contained in the union of the real line and the unit circle, then there several things one can try. On the one hand, if we diagonalize $A=UDU^*$ , then $D$ can be split into the sum of two diagonal matrices, one with the real entries and $0$ 's else, and one with the entries on the unit circle and $0$ 's else, $D=D_1+D_2$ . Moreover, let $J$ be the diagonal matrix which has $1$ 's where $D$ has real entries and $0$ 's else, then $D=(D_1-J)+(D_2+J)$ , so $A=U(D_1-J)U^*+U(D_2+J)U^*=:A_1+A_2$ , where $A_1$ is self-adjoint and $A_2$ is unitary. In this case, $A^*=A_1+A_2^*$ Another possibility is to split up $D$ as the product of two diagonal matrices, one with the real entries and $1$ 's where the entries from the unit circle were, and one with the entries from the unit circle and $1$ 's where the real entries were, $D=D_1D_2$ , which leads to $A=(UD_1U^*)(UD_2U^*)=:A_1A_2$ where $A_1$ is self-adjoint and $A_2$ is unitary again. In this case $A^*=A_1A_2^*$ . I couldn't prove the statement in both cases. It would be good to express $\text{dim ker}(A_1B-BA_1)$ and $\text{dim ker}(A_2B-BA_2)$ in terms of $\text{dim ker}(AB-BA)$ .","['matrices', 'matrix-rank', 'linear-algebra']"
4887813,Probability that the centroid of a triangle is inside its incircle,"Question : The vertices of triangles are uniformly distributed on the circumference of a circle. What is the probability that the centroid is inside the incricle. Simulations with $10^{10}$ trails give a value of $0.457982$ . It is interesting to note that this agrees with $\displaystyle \frac{G}{2}$ to six decimal places where $G$ is the Catalan's constant . Julia source code: using Random

inside = 0
step = 10^7
target = step
count = 0

function rand_triangle()
    angles = sort(2π * rand(3))
    cos_angles = cos.(angles)
    sin_angles = sin.(angles)
    x_vertices = cos_angles
    y_vertices = sin_angles
    return x_vertices, y_vertices
end

function incenter(xv, yv)
    a = sqrt((xv[2] - xv[3])^2 + (yv[2] - yv[3])^2)
    b = sqrt((xv[1] - xv[3])^2 + (yv[1] - yv[3])^2)
    c = sqrt((xv[1] - xv[2])^2 + (yv[1] - yv[2])^2)
    s = (a + b + c) / 2
    incenter_x = (a * xv[1] + b * xv[2] + c * xv[3]) / (a + b + c)
    incenter_y = (a * yv[1] + b * yv[2] + c * yv[3]) / (a + b + c)
    incircle_radius = sqrt(s * (s - a) * (s - b) * (s - c)) / s
    return incenter_x, incenter_y, incircle_radius
end

while true
    count += 1
    x_vertices, y_vertices = rand_triangle()
    centroid_x = sum(x_vertices) / 3
    centroid_y = sum(y_vertices) / 3
    incenter_x, incenter_y, incircle_radius = incenter(x_vertices, y_vertices)
    centroid_inside = sqrt((centroid_x - incenter_x)^2 + (centroid_y - incenter_y)^2) <= incircle_radius
    inside += centroid_inside
    if count == target
        println(count, "" "", inside, "" "", inside / count)
        target += step
    end
end","['integration', 'geometric-probability', 'geometry', 'triangles', 'probability']"
4887825,Maximizing area of the triangle in a quarter circle,"The radius of the quarter circle is $6\sqrt 5$ and we assume that $OA= 5$ and $OC=10$ . What is the maximum area of the blue triangle? Interpreting the problem statement, I believe that points $A$ and $C$ are fixed and point $B$ can move on the arc. To solve this problem, I assumed that the coordinate of $O$ is $(0,0)$ and then assigned coordinates for each vertex of the triangle: $A(5,0), C(0,10), B(x,\sqrt{180-x^2})$ where $x \in [0, 6\sqrt5]$ . Then I applied the formula for the area of the triangle given its vertices, and the problem is reduced to maximizing $$A(x)= \left|25-(\frac52\sqrt{180-x^2}+5x)\right|\quad \text{for}\quad x \in [0, 6\sqrt5]$$ Which is easy to continue and I got $50$ as the answer. I'm looking for other approaches to solve this problem. I'm particularly interested in geometric approaches.","['euclidean-geometry', 'triangles', 'area', 'geometry']"
4887836,Solve the IVP $y'+2y = \frac{1}{1+x^2}$,"Find the solution of the DE $$y'+2y = \frac{1}{1+x^2}\,\,\,\,\,\,\forall x \in \mathbb R$$ satisfying $y(0) = a$ where $a \in \mathbb R$ is a constant. My attempt: Since it's a linear ODE, therefore the Integration factor (I.F.) is $ e^{\int 2\,dx} = e^{2x}$ .And the solution is $$ ye^{2x} = \int \frac{e^{2x}}{1+x^2} dx$$ I'm facing trouble in solving the integral. I tried using some online integral calculator but the solutions over there tends to include imaginary expressions. I'm not sure if my approach was incorrect or if I'm missing something while solving the ODE. Edit:
The question furter required us to find the value $$\lim_{x \to \infty} y_a(x)$$ . Can we find the limit for $$y(x) = e^{-2x}\int \frac{e^{2x}}{1+x^2} dx + e^{-2x}C$$ where C is the constant of integration. I'm not sure how to use the initial value in this case.
The solution is: $$\lim_{x \to \infty} y_a(x) = 0\,\,\,\,,\forall\,a\in\mathbb R$$ Note: The question was asked in a maths competition where the syllabus doesn't include ODE with complex functions as a solution or complex analysis.","['integration', 'indefinite-integrals', 'calculus', 'ordinary-differential-equations']"
4887880,Steenrod squares and higher cup products for differential forms?,"I am physicist, so I am sorry if I am not too rigorous in the following. I have two (closely related I guess) questions: Let me consider a triangulated manifold $M$ and its simplicial cohomology. Here the Steenrod square is an operation $Sq^q: H^p (M,Z_2) \to H^{p+q}(M,Z_2)$ . My manifold is smooth and I have also a differential structure on it: is there an analogous operation also on the de Rham cohomology? (eventually by considering forms in $H_{dR}^p (M)$ mod 2 for example) Now, perhaps more important for me, the Steenrod squares can be written on a element $\alpha_p \in H^p(M,Z_2)$ by introducing the so called higher cup products, so that $Sq^q \alpha_p = \alpha_p \cup_{p-q} \alpha_p$ (where $\cup_0 = \cup$ is the standard cup product and $\cup_p$ actually makes sense for $\alpha_p \in H^p(M,G)$ for some Abelian group $G$ ). Is there a generalization of these products also for differential forms? In my understanding these higher cup products basically come from the fact that the standard cup product is not graded commutative at the level of cochains (and its failure to be commutative is measured by $\cup_1$ ). For the differential forms the wedge product ('which is the cup product for forms') has no this kind of problem, so I cannot see how similar products could arise for the forms. Let me end with an intuitive physical picture that I have in mind (just to explain my questions). I start with a manifold $M$ with a triangulation. This is a discretization of my physical space, where I work with cochains on simplexes. In my view this is an approximation: I then take the continuum limit to $M$ and I represent the cochains as forms. As long as I am concerned just with the ordinary cup product I somewhat know how to handle it (physically). But higher cup products? I really do not know how to make sense of expressions like $\alpha \cup_1 \alpha$ for example (they somewhat resemble 'discretization errors' in this intuitive picture). I have tried to look (and I am looking) in the mathematical literature but I have not found anything for now, especially for the part regarding the higher cup products (at least anything for what I could understand).","['differential-geometry', 'de-rham-cohomology', 'homology-cohomology', 'algebraic-topology', 'simplicial-complex']"
4887930,Trigonometry inequality for sine of 5 angles,"Given $5$ positive angles $x_1,x_2,x_3,x_4,x_5$ such that $x_1+x_2+x_3+x_4+x_5=2\pi$ , show that: \begin{align}
0 &\le \sin(x_1+x_2) + \sin(x_2+x_3) + \sin(x_3+x_4) + \sin(x_4+x_5)+\sin(x_5+x_1) \\
&\le \sin(x_1)+\sin(x_2)+\sin(x_3)+\sin(x_4)+\sin(x_5).
\end{align} I was able to show it for $2$ and $3$ angles, but then I got stuck. Any help would be appreciated! $\sin (x_1+x_2) = \sin x_1 \cos x_2 + \cos x_1 \sin x_2 \le \sin x_1 + \sin x_2$","['trigonometry', 'inequality', 'geometry']"
4887935,Uniform Boundedness of a $C_0$-group,"Let $p\in [1,\infty)$ . On $X=L^p(\mathbb{R}; \mathbb{C}^2)$ we consider the operator $D=A+B$ , where $A=\begin{pmatrix}-\partial_x & 0 \\ 0 & \partial_x \end{pmatrix}\quad{and}\quad  B=c(x)\begin{pmatrix}0 & 1 \\ -1 & 0\end{pmatrix}$ (with domain $\mathsf{Dom}(D)=W^{1,p}(\mathbb{R};\mathbb{C}^2)$ ). Here, $c\colon \mathbb{R}\to \mathbb{R}$ is a real-valued $L^\infty$ -function. My question is: Does $D$ generate a bounded $C_0$ -group on $X$ ? My thoughts so far: It is clear that $A$ generates a bounded $C_0$ -group on $X$ . In fact, $(e^{tA}f)(x)=\begin{pmatrix} f_1(x-t)\\ f_2(x+t)\end{pmatrix}$ for a.e. $x\in \mathbb{R}$ , all $t\in \mathbb{R}$ and all $f=(f_1,f_2)\in X$ , from which we immediately get that $(e^{tA})_{t\in \mathbb{R}}$ is a $C_0$ -group of contractions on $X$ . Now, as $B$ is bounded linear operator on $X$ , it follows from standard pertubation theory that $D$ is the generator of a $C_0$ -group $(e^{tD})_{t\in \mathbb{R}}$ . However, pertubation theory only guarantees a growth rate $\omega\geq 0$ and and $M\geq 1$ such that $\|e^{tD}\|\leq Me^{\omega |t|}$ for all $t\in \mathbb{R}$ and this $\omega$ can be strictly positive in general (consider for example the case where $B$ was the identity matrix). So, the question is if $\omega$ can be chosen to be zero, and this seems to be a much more delicate matter. If $f=(f_1,f_2)\in \mathsf{Dom}(D)$ , then $(u,v)(t):=e^{tD}f$ for $t\in \mathbb{R}$ solves the abstract Cauchy problem $(\ast) \begin{cases}
u'(t)=-\partial_x u(t)+cv(t) \quad (t\in \mathbb{R}), \quad u(0)=f_1,\\
v'(t)=\hphantom{-}\partial_x u(t)-cu(t) \quad (t\in \mathbb{R}), \quad v(0)=f_2.
\end{cases}
$ The boundedness of $(e^{tD})_{t\in \mathbb{R}}$ is then equivalent to the existence of $M\geq 1$ such that the estimate $\|(u,v)(t)\|_{X}\leq M \|(f_1,f_2)\|_X$ for all $t\in \mathbb{R}$ holds. The problem is that I am not able to find any reasonable representation of the solution of $(\ast)$ which allows me estimate its $X$ -norm in an effective way. Here, the problem is that $(\ast)$ is coupled (it is a coupled system of transport equations) and that $c$ is only asssumed to be $L^\infty$ which makes taking one additional time derivative in order to obtain a decoupled wave equation problematic. My hope is that $(e^{tD})_{t\in \mathbb{R}}$ is indeed bounded, and if not I am curious to know if it is possible to give (smallness-)conditions on $c$ that make the $C_0$ -group bounded. Any hints would be greatly appreciated.","['transport-equation', 'analysis', 'functional-analysis', 'partial-differential-equations', 'semigroup-of-operators']"
4887937,Papa Rudin $6.16$ theorem.,"There is the theorem:
Suppose $1\leq p \lt \infty $ , $\mu$ is a $\sigma$ -finite positive measure on $X$ , and $\phi$ is a bounded linear functional on $L^{p}(\mu)$ . Then there is a unique $g \in L^{q}(\mu)$ , where $q$ is the exponent conjugate to $p$ , such that $$\phi(f) = \int_{X} fg \ d\mu \ \ (f \in L^{p}(\mu)). $$ Moreover, if $\Phi$ and $g$ are related as mentioned above, we have $$ ||\phi|| = ||g||_{q} . $$ There is the proof: The uniqueness of $g$ is clear, for if $g$ and $g’$ satisfy the relation with $\phi$ , then the integral of $g-g’$ over any measurable set $E$ of finite measure is $0$ ( as we see by taking $\chi_{E}$ for $f$ ), and the $\sigma$ -finiteness of $\mu$ implies therefore that $g - g’ = 0$ a.e. Next, if the relation between $g$ and $\phi$ holds, Hölder‘s inequality implies $$ ||\phi|| \leq ||g||_{q} . $$ I don’t understand how does Hölder’s inequality imply the last inequality. Any help would be appreciated.","['measure-theory', 'normed-spaces', 'analysis', 'real-analysis', 'functional-analysis']"
4887945,Prove that $g(x) = \sum_{n=0}^{+\infty}\frac{1}{2^n+x^2}$ ($x\in\mathbb{R}$) is differentiable and check whether $g'(x)$ is continuous.,"The function $g(x)$ is a function series, so it is differentiable when $g'(x)$ converges uniformly. So I should just check uniform convergence of $g'(x)$ by using the Weierstrass M-test: $$g'(x) = \left(\sum_{n=0}^{+\infty}\frac{1}{2^n+x^2}\right)' = \sum_{n=0}^{+\infty}\left(\frac{1}{2^n+x^2}\right)',$$ then $$\left|-\frac{2x}{(2^n+x^2)^2}\right| = \frac{2|x|}{(2^n+x^2)^2} \leq \frac{2|x|}{(2^n)^2} =  \frac{2|x|}{4^n}.$$ But now I can't find a sequence that is bigger than $\frac{2|x|}{4^n}$ to use. For checking whether function $g'(x)$ is continuous or not, I think I will use the same argument: If $g''(x)$ converges uniformly, then $g'(x)$ is differentiable $\Longrightarrow$ continuous. Am I solving this problem in a correct way? Any help would be much appreciated.","['calculus', 'functions', 'analysis']"
4887988,"In an art museum, there are $n$ paintings, $n \ge 33$, ...","In an art museum, there are $n$ paintings, $n \ge 33$ , for which there are
used a total of $15$ different colors so that any two paintings have at least one common color and there are no two paintings that have exactly the same colors. Determine all possible values ​​of $n \ge 33 $ so that anyway we color the paintings with the above properties we can choose four distinct paintings $T_1$ , $T_2$ , $T_3$ and $T_4$ , so that any color that is used in both $T_1$ and $T_2$ , it can be found in $T_3$ or $T_4$ . I've been trying to solve this combinatorics problem for some time, but I can't think of what the result could be (probably a big number). I don't know if it will be useful, I will put my attempts below. Let $T_1, T_2, T_3, ... , T_n$ be the n sets representing the ""paintings"", each having at least one element and at most 15 elements and let those elements be $c_1,c_2,...,c_{15}$ (the $15$ colors).From the first sentence we have that $T_i \ne T_j , \forall i,j = \{1,2,...,n\} , i \ne j$ and $T_i \cap T_j \ne \emptyset$ for every $i \ne j$ .From the second sentence $\implies \exists i,j,k,l \in \{1,2,...,n\}$ with $i \ne j \ne k \ne l $ such that $(T_i \cap T_j) \subset (T_k \cup T_l)$ ( both and or in the second sentence were the key words from which I got this ) . The total number of paintings with no conditions is $2^{15}-1$ but we can't have 2 subsets which don't have any color in common so we would have $2^{14}$ paintings (for example we take the color $c_i$ and the subset $C=\{c_1,c_2,...,c_{15} \} \setminus \{c_i\}$ . For every subset of $C$ we put $c_i$ in it so we get the $2^{14}$ ).Now maybe we could show that for every $n\in \{33,34,...,2^{14}\}  $ we can the find the 4 paintings with the second condition.I think if by absurd , let's say that $\forall i,j,k,l \in \{1,2,...,n\}$ with $i \ne j \ne k \ne l $ we have that $(T_i \cap T_j) \not\subset (T_k \cup T_l)$ .Prety much  I denied the second condition.So if we prove that the condition $|(T_i \cap T_j) \setminus (T_k \cup T_l)| \ge 1$ (equivalent to what I wrote previously) $\implies$ contradiction , we are done. I don't really know what to do next, I think the answer would have to do with that ""15 colors"". I think that for a very large n we can no longer form these sets. What do you think ? I am open to any suggestion, comment or solution. Thank you !","['permutations', 'combinations', 'combinatorics']"
4888000,Non-decreasing functions satisfying functional inequalities $f(1+ax)\leq a f(1+x)$ and $f(xy)\leq f(x)+f(y)$,"Can we exactly determine a class of non-decreasing functions defined on the set of non-negative real numbers satisfying: $$f(1+ax)\leq a f(1+x)$$ and $$f(xy)\leq f(x)+f(y)$$ for any $x,y\in [0,\infty)$ , assuming that $a\in[0,1)$ can vary?
Par example, log fulfills second inequality, but not the first one...
Looking for any kind of input or advice. (or book to look for some results on this topic)","['functions', 'functional-inequalities']"
4888010,Is geodesic distance locally approximated by euclidean distance?,"Let $(M,g)$ be a Riemannian manifold. Let $p\in M$ . Is it possible to find a chart $\varphi: p\in U \to \mathbb R^d$ such that the geodesic distance $d_M$ on $U$ is close to the euclidean distance $d_E$ on $\phi_i(U_i)$ ? More precisely, if I take $\phi_i = \exp^{-1}_p: \{x\in M: d_M(x,p)\le \delta\} \to \{v\in \mathbb R^d: d_E(v,0 )\le \delta\}$ , can I find bounds $$A(p, M ,\delta) \le \frac{d_M(x,y)}{d_E(\exp^{-1}_p(x), \exp^{-1}_p(y))}\le B(p, M ,\delta)\enspace,$$ such that $A(p, M ,\delta)\small{\nearrow} \normalsize1$ and $B(p, M ,\delta)\small\searrow \normalsize1$ as $\delta\to 0$ ? Presumably, $A$ and $B$ will depend only on the curvature at $p$ and on $\delta$ . Of course, if $x=p$ , then $A=B=1$ by the definition of the exponential map. The difficult part is $x,y\neq p$ . I think I know how to derive $B$ . I would consider $\delta$ sufficiently small such that the geodesic ball is convex. A path in $\{v\in \mathbb R^d: d_E(v,0 )\le \delta\}$ from $\exp^{-1}_p(x)$ to $\exp^{-1}_p(y)$ induces a path $\gamma$ in $M$ of length $$l(\gamma) = \int_0^1 \sqrt{g(\gamma'(t), \gamma'(t))} dt$$ with $g(\gamma', \gamma') = \Vert \gamma' \Vert^2 - \frac 1 3 R_{\mu \sigma \nu \tau}(p) ~ \gamma^\sigma \gamma^\tau \gamma'^\mu \gamma'^\nu+ \mathcal O(|\gamma|^3)$ . So if $\gamma$ is a straight line, then $\gamma'(t) = \exp^{-1}_p(x) - \exp^{-1}_p(y)$ and $l(\gamma)$ depends directly on $d_E(\exp^{-1}_p(x), \exp^{-1}_q(x))$ . However, I don't know how to find the lower bound $A$ .","['geometry', 'riemannian-geometry', 'differential-geometry']"
4888031,Prove that if a smooth manifold $M$ is contractible then every vector bundle over $M$ is trivial,"I've seen that this can be proved by using that if two functions are homotopic then the pullbacks of such functions are isomorphic, but the only ""easy"" proof of this I found is in Hatcher's vector bundles book and I don't find this proof very clear.
This was left to me as a homework exercise and all we've seen of vector bundles are the basic definitions, constrictions by cocycles and that every vector bundle has a riemmanian metric so I don't know how to proceed with only this, any help would be appreciated","['riemannian-geometry', 'vector-bundles', 'homotopy-theory', 'differential-topology', 'differential-geometry']"
4888053,Proof that $\sin(z-w)=\sin(z) \cos(w)-\sin(w) \cos(z)$,"Note that \begin{aligned}
\sin(z-w) &= \frac{e^{i(z-w)}-e^{-i(z-w)}}{2i} \\
&= \frac{e^{iz}e^{-iw}-e^{-iz}e^{iw}+e^{iz}e^{iw}-e^{iz}e^{iw}}{2i} \\
&= \frac{e^{iw}(e^{iz}-e^{-iz})}{2i}-\frac{e^{iz}(e^{iw}-e^{-iw})}{2i} \\
&= e^{iw}\sin(z)-e^{iz}\sin(w) \\
&= 2(\cos(w) \sin(z)-\cos(z) \sin(w))+e^{-iz}\sin(w)-e^{iw} \sin(z)
\end{aligned} I have omitted to add $e^{-iw} \sin(z)-e^{-iw} \sin (z)$ The idea was to achieve that $e^{-iz} \sin(w)-e^{iw} \sin (z) = -\sin (z-w)$ to clear this value and easily obtain the equality, but using the sin parity is not given, any suggestions?",['complex-analysis']
4888056,Question About Half-Open Cubes on $\mathbb{R}^d$,"I am self-studying real analysis. I encountered the following assertion without a proof: For each positive integer $k$ , let $\mathcal{C}_k$ be the collection of all cubes of the form \begin{align*}
\{(x_1,\dots,x_d):j_i2^{-k} \leq x_i < (j_i+1)2^{-k}\ \text{for}\ i = 1,\dots,d\},
\end{align*} where $j_1,\dots,j_d$ are arbitrary integers. Then (1) each $\mathcal{C}_k$ is a countable partition of $\mathbb{R}^d$ , and (2) if $k_1<k_2$ , then each cube in $\mathcal{C}_{k_2}$ is included in some cube in $\mathcal{C}_{k_1}$ . I tried a couple of examples, for instance, I tried $k=1$ and $j_i=i$ , $k=1$ and $j_1=0$ , $j_2=-5$ , $j_3=10$ , and so on. It does show that the claim is correct. However, I am really having a hard time proving it rigorously. I really appreciate it if someone could help me out! I do not have the definition of a partition of $\mathbb{R}^d$ . But I think it means a collection of nonempty disjoint sets whose union is $\mathbb{R}^d$ . Reference: Lemma 1.4.2 from Measure Theory by Donald Cohn","['measure-theory', 'analysis', 'real-analysis', 'elementary-set-theory', 'problem-solving']"
4888067,"The $\infty$-dimensional cube, $\infty$-dimensional cross-polytope and their pseudo-boundaries","Let $A = \mathbb{R}^\mathbb{N}$ with the product topology. Let $B = \{ (x_n)_n \in A \mid \forall n : |x_n| \leq 1\} = [-1, 1]^\mathbb{N}$ be the $\infty$ -dimensional cube. Let $C = \{ (x_n)_n \in A \mid \sum_{n=1}^\infty |x_n| \leq 1 \}$ be the $\infty$ -dimensional cross-polytope. Let $\operatorname{d}B = \{(x_n)_n \in B \mid \exists n : |x_n| = 1\}$ be the pseudo-boundary of $B$ . Let $\operatorname{d}C = \{(x_n)_n \in C \mid \sum_{n=1}^\infty |x_n| = 1 \}$ be the pseudo-boundary of $C$ . Are $B$ and $C$ homeomorphic? Are $\operatorname{d}B$ and $\operatorname{d}C$ homeomorphic? Does there exist a homeomorphism of $B$ and $C$ that restricts to a homeomorphism of $\operatorname{d}B$ and $\operatorname{d}C$ ? A natural map would be $\varphi: \operatorname{d}C \to \operatorname{d}B, (x_n)_n \mapsto \frac{1}{\underset{m}{\operatorname{max}}|x_m|} \cdot (x_n)_n$ . However, it is unclear to me whether this map is even continuous. Even more, the map is clearly not surjective. So this is not a homeomorphism of $\operatorname{d}B$ and $\operatorname{d}C$ .",['general-topology']
4888075,Tricky Algebraic Reduction,"So I'm trying to work my way through Ernst Kummer's De Numeris Complexis , and I've reached a point where I keep stumbling over something that should be very, very simple. After almost an hour of playing around with this, I have been unable to solve it, and so appeal to the community for help. The basic proposition can be boiled down to what follows: Let's say that $a,b$ are some numbers such that $1+a+b=0$ , and consider the product $p = (ax+by)(ay+bx)$ where $x$ and $y$ are independent variables. The claim is that $p$ can then be expressed as a form in $x,y$ wherein neither $a$ nor $b$ appear. What is this form, and how does one find it? EDIT: To give some idea of my own failed attempts, one idea I had was that you write out the product as follows: $$(x^2+y^2)ab + xy(a^2+b^2).$$ Then you use that $$0=0^2=(1+a+b)^2 = 1 + 2(a+b)+ 2 ab + a^2+b^2$$ to get that $$a^2+b^2=-1-2(a+b)-2ab.$$ I then inserted that into $(x^2+y^2)ab + xy(a^2+b^2)$ to get $$(x^2+y^2)ab - xy(1 + 2(a+b)+ 2 ab).$$ Since $2(a+b)=2(1+a+b)-2$ , I would then write this as $$(x^2+y^2)ab - xy(-1 + 2 ab).$$ This, I then re-arranged as $$ab(x^2+y^2+xy)-2xyab=ab(x+y)^2-2abxy.$$ At this point I got stuck, not knowing where to go next. The rest of my attempts look similar in nature, the sort of stuff where you play around with the terms and products, and in the end, you come back to where you started.",['abstract-algebra']
4888087,On the definition of a differential equation with an initial value ON the boundary of an open intervall,"In a textbook that I am reading the author defines a differential equation for values of $t>0$ and an initial condition for $t=0$ . As an example consider $$
y'(t) = 0 \enspace\text{for all } t\in\left]0,\infty\right[, \quad y(0)=1.
$$ I don't fully understand how solutions to differential equations are even defined in this case. Clearly, the solution that one thinks of is $y(t)\equiv 1$ . But wouldn't the function $$
y(t)=\begin{cases}1, & \text{if }t=0, \\ 0, & \text{if }t>0 \end{cases}
$$ satisfy this initial value problem, too? After all, it clearly has both required properties, right? In my eyes this kind of solution function should be excluded. Shouldn't the function and its derivative be defined on the same set? The way I see it the correct way to phrase the initial value problem  (i.e., the way that the author actually had in mind) should be $$
y'(t) = 0 \enspace\text{for all } t\in\left[0,\infty\right[, \quad y(0)=1.
$$ This would force the solution to be continuous in $t=0$ because $y$ is then necessarily differentiable in $t=0$ . The derivative $y'(0)$ is simply, by definition*, the one-sided derivative. And if $y$ isn't continuous in $t=0$ , then this one-sided derivative does not exist. Am I correct? I have not really seen any textbooks that use this notation comment on this issue. Does this mean that for any differential equation on an open intervall one should demand that its initial value is given at a point inside of this domain? Any input is appreciated. * Many textbooks define differentiablity only for open sets to begin with. Some others (e.g. Rudin) choose the way that I have described above where the one-sided derivative is implemented ad hoc whenever a boundary point is part of the function's domain.","['derivatives', 'ordinary-differential-equations', 'partial-differential-equations']"
4888118,"In what sense are similar matrices ""the same,"" and how can this be generalized?","I sort of intuitively see why we care about similar matrices, i.e., when $A=S^{-1}BS$ for some invertible matrix $S$ . But I want to make this intuition more precise and abstract. Matrices: First of all, as mentioned here , Because matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator. This is followed by a long list of shared properties. However, I feel this doesn't give the full story. For example, we also care about when two different operators are similar, in which case (conversely), they can be represented by the same matrix with appropriate bases. In this case, the long list of properties is still shared by both operators. How can one precisely say what type of properties are shared by similar operators, and what's the most abstract way to understand this? Generalizations: If $A, B, S$ are elements of a group $G$ , then $A = S^{-1}BS$ is described by saying $A$ is the conjugation of $B$ by $S$ . For any $S \in G$ , conjugation by $S$ is an endomorphism of $G$ , and hence preserves all group-theoretic properties of any element. Matrix similarity is an extension of this idea, where we conjugate elements of an algebra $L(V)$ (operators) with the group of units in the algebra $GL(V)$ (invertible matrices). This type of conjugation then provides an algebra endomorphism, so we should expect the properties of some $T \in L(V)$ as an element of the algebra $L(V)$ to be preserved by conjugation—but some of the properties in the list linked above (e.g., determinant) are specific to $L(V)$ and cannot be generalized to an arbitrary algebra. The most general framework I can think of is as follows: We have some group $G$ which acts on some structure $X$ from the left and right. Thus conjugation makes sense. Can we say anything about what properties of an arbitrary $x \in X$ are preserved under conjugation, in a way that includes matrix similarity as a special case?","['abstract-algebra', 'linear-algebra', 'similar-matrices', 'group-actions', 'matrix-decomposition']"
4888122,Does the sum of two convex functions with global minimizers have a global minimizer?,"If $f(x)$ and $g(x)$ are two convex functions defined in $\mathbb{R}^n \to \mathbb{R}$ . Suppose $f(x)$ has a global minimizer $x_1 \in \mathbb{R}^n$ but not necessarily the unique global minimizer.
Similarly, $g(x)$ has a global minimizer $x_2 \in \mathbb{R}^n$ but not necessarily the unique global minimizer.
Does there exist a global minimizer in $\mathbb{R}^n$ for the function $f(x) + g(x)$ ?","['optimization', 'calculus', 'functions', 'convex-analysis']"
4888158,Is $S_5$ generated by (1 3) and (1 2 3 4 5)? [duplicate],"This question already has answers here : How do i prove how $S_5$ is generated by a two cycle and a five cycle? (3 answers) Closed 3 months ago . I proved that $S_4$ is not generated by $(1 \ 3)$ and $(1 \ 2 \ 3 \ 4)$ , using that the partition $A = \{ \{ 1, 3 \}, \{ 2, 4 \} \}$ is invariant under the action of the elements of $\left \langle (1 \ 3), (1 \ 2 \ 3 \ 4) \right \rangle$ , but if we take a permutation $\sigma \notin \left \langle (1 \ 3), (1 \ 2 \ 3 \ 4) \right \rangle$ , for example $\sigma = (1 \ 2)$ , then $\sigma A \ne A$ . I tried to use the same reasoning for $S_5$ , taking a partition $P = \{ \{1, 3 \}, \{ 2, 4, 5 \} \}$ and I got that $P$ is invariant under the action of $(1 \ 3)$ , but isn't invariant under the action of $(1 \ 2 \ 3 \ 4 \ 5)$ , that is, $(1 \ 2 \ 3 \ 4 \ 5)P \ne P$ . Any tips to help me?","['permutations', 'permutation-cycles', 'finite-groups', 'abstract-algebra', 'group-theory']"
4888209,"The ""turning-point fraction"" of a random sample from a discrete distribution must have expectation less than 2/3?","A sequence of reals $x_1,...,x_n$ is said to have a turning point at index-value $i$ ( $1\lt i\lt n$ ) iff $x_{i-1}\lt x_{i}\gt x_{i+1}$ or $x_{i-1}\gt x_{i}\lt x_{i+1}$ .  The number of turning points in the sequence is denoted $T(x_1,...,x_n)$ , and we define the turning-point fraction as $$R(x_1,...,x_n)={\text{number of turning points}\over\text{number of potential turning points}}={T(x_1,...,x_n)\over n-2}$$ so $0\le R(x_1,...,x_n)\le 1.$ If $X_1,...,X_n$ are random variables, we define the corresponding r.v.s $T_n=T(X_1,...,X_n)$ and $R_n=R(X_1,...,X_n).$ Conjecture: If $X_1,...,X_n$ are i.i.d. r.v.s with any discrete distribution, then $E[R_n]\lt{2\over 3}$ . (It's easy to show that $E[R_n]={2\over 3}$ when the $X_i$ are i.i.d. with any continuous distribution.) Supposing the $X_i$ are i.i.d. with a discrete distribution having p.m.f. $p()$ and c.d.f. $F()$ , we have the following: $$\begin{align*}E[R_n]
&={1\over n-2}E\left[ \sum_{i=2}^{n-1}\mathbb{1}_{(X_{i-1}<X_i>X_{i+1}) \text{ or } (X_{i-1}>X_i<X_{i+1})}\right]\\
&=P[(X_1<X_2>X_3) \text{ or } (X_1>X_2<X_3)]\\
&=P[X_1<X_2>X_3] + P[X_1>X_2<X_3]\\
&=\sum_{x: p(x)>0} p(x)\left(P[X_1<x>X_3\mid X_2=x] + P[X_1>x<X_3\mid X_2=x]\right)\\
&=\sum_{x: p(x)>0} p(x)\left(P[X_1<x]^2 + P[X_1>x]^2\right)\\
&=\sum_{x: p(x)>0} p(x)\left(\left(F(x)-p(x)\right)^2 + \left(1-F(x)\right)^2\right)\\
\end{align*}$$ Question : How to show that this must be less than 2/3, if such is the case? Or find a counterexample if not? NB : Simulations suggest that for a given finite support,  a uniform distribution might maximize $E[R_n]$ .  When the $X_i$ are i.i.d. DiscreteUniform $\{1,...,m\}$ , we have $p(x)={1\over m}$ and $F(x)={x\over m}$ for $x=1,...,m$ ,  and the above summation reduces to $$E[R_n]={(2m-1)(m-1)\over 3m^2}\lt{2\over 3}$$ with $E[R_n]$ approaching ${2\over 3}$ from below as $m\to\infty.$","['statistics', 'expected-value', 'time-series', 'probability', 'random-variables']"
4888230,Can we make this subspace $\aleph_0$-dimensional?,"Let $X$ be a compact Hausdorff space and $A\subseteq X$ a subspace of $X$ . Is it possible for the space $\{f\vert_A:f\in\mathcal{C}(X,\mathbb{R})\}$ to be $\aleph_0$ -dimensional as a $\mathbb{R}$ -vector subspace of $\mathcal{C}(X,\mathbb{R})$ ? I know that in this case $\mathcal{C}(X,\mathbb{R})$ is, at least, $\mathfrak{c}$ -dimensional as an infinite-dimensional Banach algebra. But I don't know if we can restrict it to make it $\aleph_0$ -dimensional somehow. Thanks!","['banach-spaces', 'topological-vector-spaces', 'continuity', 'functional-analysis', 'general-topology']"
4888295,How to represent $x^n$ as a sum of $P_k:= (x)(x-1)\dots(x-k+1)$?,"Just for curiosity  I want to  represent $x^n$ as a sum of $P_k:= (x)(x-1)\dots(x-k+1)$ . Since $x=P_1,\ xP_n= P_{n+1} +nP_n$ , this proves that it is possible for any $x^n$ to be represented as a sum of $P_k:= (x)(x-1)\dots(x-k+1)$ where $1\le k \le n$ , $ \ n \in \mathbb{N}$ . We have $$x^n = \sum\limits_{k=1}^n C_{(n,k)} P_k$$ where $C_{(n,k)}$ is just some constant that depends on $n, \ k$ , This also give us a way to calculate the first $C_{(n,k)}$ for $1\le n\le 6$ . $$x=P_1$$ $$x^2 = P_1+ P_2  $$ $$x^3 = P_1 + 3P_2 +P_3  $$ $$x^4=P_1  +7 P_2 + 6P_3  +P_4$$ $$x^5=P_1+15P_2  +25P_3+10P_4 +P_5$$ $$x^6=P_1+31P_2 +90P_3 +65P_4+15P_5+ P_6$$ The question is How to represent $C_{(n,k)}$ in a ""nice"" closed from? I expected at first that $C_{(n,k)}$ have some relation to the binomial coefficients $x, \ x^2, \ x^3$ I believed that  such simple relation could be found, but after I calculated $x^4, \ x^5, \ x^6$ it turns out that $C_{(n,k)}$ is more complicated that I initially thought . After some thinking representing $x^n$ as a sum of $P_k$ can be useful ,For example: $\sum\limits_{r=1}^m r^n = \sum\limits_{r=1}^m\sum\limits_{k=1}^n C_{(n,k)} P_k(r)$ where $P_k(r)=(r)(r-1)\dots(r-k+1)= k!\dbinom{r}{k} $ $$ \sum\limits_{r=1}^m \sum\limits_{k=1}^n C_{(n,k)} P_k(r)=  \sum\limits_{k=1}^n C_{(n,k)} \sum\limits_{r=1}^m  P_k(r)=\sum\limits_{k=1}^n C_{(n,k)} \sum\limits_{r=1}^m  P_k(r) $$ $$= \sum\limits_{k=1}^n C_{(n,k)} \sum\limits_{r=1}^m k!\dbinom{r}{k}$$ $$ \sum\limits_{r=1}^m\dbinom{r}{k} =\dbinom{m+1}{k+1}  $$ So $$\sum\limits_{r=1}^m r^n =\sum\limits_{k=1}^n k!C_{(n,k)} \dbinom{m+1}{k+1}$$ This gives us a general formula for $\sum\limits_{r=1}^m r^n $ . Another example is evaluating $\sum\limits_{r=1}^\infty\frac{r^n x^r}{r!} $ where $n$ is a natural number. $$\sum\limits_{r=1}^\infty\frac{r^n x^r}{r!}  = \sum\limits_{r=1}^\infty\frac{\sum\limits_{k=1}^n C_{(n,k)} P_k(r) }{r!}x^r = \sum\limits_{k=1}^n C_{(n,k)}  \sum\limits_{r=1}^\infty \frac{P_k(r)x^r}{r!} $$ $$=\sum\limits_{k=1}^n C_{(n,k)}  \sum\limits_{r=1}^\infty \frac{x^r}{(r-k)!}=e^x\sum\limits_{k=1}^n C_{(n,k)} x^k      $$","['algebra-precalculus', 'binomial-coefficients', 'closed-form', 'analysis']"
4888296,A condition for matrices to commute,"Recently found an exercise for high school students as follows: Let $A,B$ be $2\times 2$ matrices with real entries and suppose $A^2=B^2$ , $\operatorname{tr}(A)\neq0$ , $\operatorname{tr}(B)\neq0$ . Show that $AB=BA$ . Since it only ask the case for $2\times 2$ matrix and hence we can simply bash out all the equations. My problem. Does this hold for $n\times n$ matrices, or does it have a generalization?","['matrices', 'linear-algebra']"
4888381,What is the Asymptotic Order of the Sum of Random Variables with non-finite Moments?,"Suppose $X_i$ is independently and identically distributed over $i$ , and $E(|X_i|^d)$ with $d>0$ is not finite or undefined: I am wondering whether $\sum_{i=1}^N |X_i|^d$ is of any asymptotic order. Take Cauchy distribution as an example. If $d=1$ , then $N^{-1}\sum_{i=1}^N X_i$ is still Cauchy and hence $$
\sum_{i=1}^N |X_i|\ \mbox{is}\ o_p(N^{1+\delta})\ \mbox{for any}\ \delta>0
$$ That is $N^{-(1+\delta)}\sum_{i=1}^N |X_i|$ converges in probability to $0$ . If $d=2$ , $E(|X_i|^2)$ is infinite. I am not sure whether there exists $\delta>0$ such that $N^{-(1+\delta)}\sum_{i=1}^N |X_i|^2$ converges in probability to $0$ . Another interesting example is related to Central Limit Theorem (CLT) : if $X_i$ follows student t distribution with $3$ degrees of freedom independently over $i$ , then $E(X_i)=0$ , $E(X_i^2)=3$ and any moments of order higher than 3 do not exist. Due to CLT, $N^{-\frac{1}{2}}\sum_{i=1}^N X_i$ converge in distribution to $N(0,3)$ . Hence, $(N^{-\frac{1}{2}}\sum_{i=1}^N X_i)^3\leq 3!N^{-\frac{3}{2}}\sum_{i=1}\sum_{m=1}\sum_{j=0}X_iX_{i+m}X_{i+m+j}+N^{-\frac{3}{2}}\sum_{i=1}^NX_i^3=O_p(1)$ . Since the third moment of normal distribution with zero mean is $0$ , one could conjecture $E[(N^{-\frac{1}{2}}\sum_{i=1}^N X_i)^3]$ tends to $0$ (notation abuse due to the non-existence of $E(X_i^3)$ ). One should have $N^{-\frac{3}{2}}\sum_{i=1}^NX_i^3=o_p(1)$ or $\sum_{i=1}^NX_i^3=o_p(N^{\frac{3}{2}})$ . Similarly, $\sum_{i=1}^NX_i^d=o_p(N^{\frac{d}{2}})$ with $d\geq3$ if the sequence $\lbrace X_i\rbrace$ satisfies the CLT conditions, though such asymptotic orders may not be on the boundary.","['central-limit-theorem', 'probability-distributions', 'law-of-large-numbers', 'probability-theory', 'probability']"
4888386,Asymptotic Gambler's Ruin Probability with Unequal Gain/Loss with Zero-Mean Payoff Distribution,"The gambler's ruin problem with unequal gain/loss with a payoff distribution whose support is a finite subset of $\mathbb Z$ is an old problem; for example, see Feller (1968, Vol.1, Section XIV.8) and this old MSE question . The simple case of the problem where the payoff distribution takes $-1$ and $1$ with probabilities $p$ and $1-p$ , has been studied in many introductory text books (it can be easily adjusted for the case the payoff distribution takes $-1$ , $1$ , and $0$ with probabilities $p$ , $q$ , and $1-p-q>0$ ). The ruin probability for the general case is a linear combination of the roots of the following equation: $$P_X(z)=1 \Leftrightarrow M_X(t)=1$$ where $P_X(z)=\mathbb E(z^X)$ and $M_X(t)=\mathbb E(e^{tX})$ are the generating function and moment generating function of $X$ , respectively. Inspired by the heuristic method used in this answer , I guess if $\mathbb E(X)=0$ and the range of $X$ is fixed, we have $$\color{blue}{\lim_{N,M \to \infty} \mathbb P_\text{ruin}(M,N)= \lim_{N,M \to \infty} \frac{N-M}{N}}. $$ Here, $M$ is the gambler's initial wealth, and he is ruined if he loses all of it. If the gambler’s fortune exceeds $N>M$ , the gambler wins and his opponent or the casino is ruined. For any $M$ and $N$ , the relation holds for the simple case of the problem described earlier. This is also supported by the method used to adjust the simple case of the problem for the case where $0$ payoff appears with non-zero probability. The above can be a useful result (if it is proven generally), because in practice the initial moneys of the players ( $M$ and $N-M$ ) can be relatively large compared to the payoff $X$ gained in each round of the game, and the condition $\mathbb E(X)=0$ is somehow shows that the game is balanced and fair. I am seeking a counterexample, or a rigorous proof for the above conjecture. This can be first studied for the simple payoff distribution that takes two integer values $-r$ and $k$ with probabilities $\frac{k}{r+k}$ and $\frac{r}{r+k}$ .","['stochastic-processes', 'statistics', 'probability', 'real-analysis']"
4888392,Quadratic form with an absolute lower bound on integer vectors: Conditions for semidefinite matrices,"Assume that $X\in \mathrm{PSD}(n)$ is a symmetric, positive semi-definite matrix with real entries and $C>0$ is a positive real number. If $X$ satisfies $$
\forall \alpha\in\mathbb{Z}^n\setminus\{0\},\qquad \alpha^T X \alpha \geq C,
$$ then is it true that $X$ is positive definite? That is, if $X$ is lower bounded on integer vectors, is it true that it is lower bounded on whole $\mathbb{R}^n$ ? Can we give a concrete lower bound for $\lambda_{\min}(X)$ in terms of $C,n$ and $\lambda_{\max}(X)$ ? The way I understand, the problem is closely related to approximations of real vectors with rational numbers: Simultaneous version of the Dirichlet's approximation theorem implies that if $v\in\mathbb{R}^n$ is a real vector and $N>0$ is a natural number, then there exists a rational vector $\beta\in\mathbb{Q}^n$ such that $$
\beta=\Big(\,\frac{p_1}{q},\, \frac{p_2}{q},\,\dots,\,\frac{p_n}{q}\,\Big)\; \text{ with }\;1\leq q\leq N\qquad\text{and}\qquad \Vert v-\beta\Vert <\frac{\sqrt{n}}{q N^{1/n}}.
$$ Note that $\beta^T X \beta \geq \frac{C}{q^2}\geq \frac{C}{N^2}$ . Since $\Vert v-\beta\Vert < \frac{\sqrt{n}}{q N^{1/n}}$ and $\beta^T X\beta\geq \frac{C}{N^2}$ , I guess that choosing a large enough $N$ (relative to $\lambda_{\max}(X)$ ?) should guarantee that $v^T Xv> 0$ . Above, I am not using the sharpest bound so maybe Dirichlet's theorem might be an overkill.","['number-theory', 'linear-algebra', 'quadratic-forms']"
4888435,"If $m^2+n^2=1$, find the maximum of $\dfrac{5-4m}{5-4n}$","If $m^2+n^2=1$ , find the maxmum of $\dfrac{5-4m}{5-4n}$ . The original question is to find the maximum of $\dfrac{BD}{CD}$ . By simplifying the formula through cosine theorem, I get the above formula. The value should be equal to $\sqrt{\dfrac{5-4m}{5-4n}}$ . How to find the value? Any elegant geometric solutions are also welcomed.","['triangles', 'geometry']"
4888442,From algebra of smooth functions to a smooth manifold,"Suppose that the $\mathbb{R}$ -algebras $\mathcal{F}_1$ and $\mathcal{F}_2$ , as vector spaces, are isomorphic to the plane $\mathbb{R}^2$ . Let the multiplication in $\mathcal{F}_1$ and $\mathcal{F}_2$ be respectively given by the relations $(x_1, y_1) \cdot (x_2, y_2) = (x_1x_2, y_1y_2) $ $(x_1, y_1) \cdot (x_2, y_2) = (x_1x_2 + y_1y_2, x_1y_2 + x_2y_1) $ Find the manifold $M_i$ for which the algebra $\mathcal{F}_i$ , $i =1, 2$ , is the algebra of smooth functions, explicitly indicating what
function on $M_i$ corresponds to the element $(x, y) \in \mathcal{F}_i$ . Are the algebras $\mathcal{F}_1$ and $\mathcal{F}_2$ isomorphic? This question has boggled me for a while. We are looking for manifolds $M_i$ for which $\mathcal{F}_i \cong C^\infty(M_i)$ . The problem I see here is that $C^\infty(M_i)$ is infinite-dimensional while $\mathcal{F}_i$ 's are both $2$ -dimensional so how can there exists such manifolds? The second thing is that the latter one looks like its almost the product we have with complex numbers except for the first $+$ sign which should probably be a $-$ sign? How should one go about solving this kinda problem?","['smooth-manifolds', 'differential-geometry']"
4888488,"Convergence Rate of ""Infinite Monkey""-type-probability","Let $S$ be a finite set and $n,m\in\mathbb N$ . Consider a random variable $R$ uniformly distributed on $S^{n+m}$ . I am trying to tackle the probability of the event \begin{equation}
A_m(n):=\{\exists_{1\leq i<j\leq m}: (R_i,\dots,R_{i+n-1}) = (R_j,\dots,R_{j+n-1})\}
\end{equation} as $m\to\infty$ . I managed to prove $\lim_{m\to\infty}\mathbb P(A_m(n)) = 1$ , since the event "" $(R_i,\dots,R_{i+n-1})=r$ infinitely often"" constitutes a lower bound for every $r\in S^n$ and has probability $1$ using Borel-Cantelli (note that here we extend the process $R$ to be $(R_i)_{i\in\mathbb N}$ with all $R_i$ iid and uniformly distributed on $S$ ). I am now trying to find the rate of convergence of $\mathbb P(A_m(n))$ depending on $n$ . My initial idea was to find a suitable subset of $A_m(n)$ to at least get a lower bound. However I am struggling to do so. This question seems to be related to the Infinite Monkey Theorem for which formulas for the expected hitting time of a certain word exist. But here I do not need to hit a specific word. Rather one is interested in hitting any word of length $n$ twice when hitting $n+m$ keys on the keyboard. For this I couldn't find a source.","['stochastic-processes', 'probability-theory', 'probability']"
4888518,Calculating the Distance from a Point on the Tangent to an Ellipse to the Center,"In the following figure, the line that touches the ellipse at only one point, called A, is the tangent line to the ellipse at that point. C is the center of the ellipse. Point $L'$ is the point where the perpendicular passing through C to the tangent line intersects the tangent line. Point L, instead, is the intersection between this perpendicular and the ellipse. I know that the ratio $\frac{AL'}{L'C}$ is given. I would like to calculate the length of $AC$ . I thought I could calculate $L'C$ (and then $AC$ ) using question Distance point on ellipse to centre ; however, by doing so, I can only calculate $LC$ . Any suggestions?","['conic-sections', 'geometry']"
