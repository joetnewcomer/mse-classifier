question_id,title,body,tags
4865789,Gradient estimates of linear elliptic PDE,"Let $\Omega \subset \mathbb{R}^n$ be a bounded smooth domain. Assume that $u(x)$ is the classical solution solving $$a_{ij}(x)\partial_{ij}u(x)+b_i(x)\partial_iu(x)+c(x)u(x)=f(x)$$ $$u(x)\Big|_{\partial \Omega}=g(x)$$ for some smooth enough coefficients and uniformly elliptic $a_{ij}$ . I found that in Gilbarg and Trudinger's PDE book, Theorem 8.33 states that $$ |u|_{1,\alpha,\Omega'}\leq C(|u|_{0}+|g|_{0}+|f|_{0,\alpha}) $$ for $\Omega' \subset \Omega$ and some constant $C$ depending on the $C^{0,\alpha}$ -norms of $a_{ij}$ , $b_i$ and $c$ (and some other parameters). Now, I am looking for a upper bound of $|\nabla u|$ on $\Omega'$ , not the holder norm, can this upper be something like $$ \sup_{\Omega'}|u|\leq C(|u|_{0}+|g|_{0}+|f|_{0}) $$ for some constant $C$ depending on the $C^{0}$ -norms of $a_{ij}$ , $b_i$ and $c$ , but not their holder norms? If this is to complicated, we can deal with the ODE case or equation in divergence form.","['elliptic-equations', 'reference-request', 'ordinary-differential-equations', 'partial-differential-equations']"
4865816,An exotic integral,"Good evening, We were playing with a friend on Desmos, and we came to $x \longmapsto \arctan\left(\exp\left(-\displaystyle\frac{1}{\sqrt{1-x^2}}\right) \right)$ . Here is the graph : And we have : $$\int_{-1}^1 \arctan\left(\exp\left(-\displaystyle\frac{1}{\sqrt{1-x^2}}\right) \right) \hspace{0.1cm} \mathrm{d}x \approx 0.529797566526076$$ We were wondering if it was calculable ? There is very few chance but hey, we never know. Best regards.",['integration']
4865899,Why is $AA^TAA^TAA^TAA^T$ larger than $AAAAA^TA^TA^TA^T$ for Gaussian $A$?,"Suppose $A$ is a $d\times d$ matrix with IID standard normal entries. Plots below compare value of $f(AA^TAA^TAA^TAA^T)$ and $f(AAAAA^TA^TA^TA^T)$ using 3 standard Schatten norms for $f$ and the former is consistently larger, why? Furthermore, we can compute expected values of $Tr(A\ldots)$ for $d=2$ and various permutations of $A$ , and these two forms appear to be the extreme values. What's the easiest way to explain this? getVal[d_, f_, sampler_] := (
   A = sampler[d];
   {Norm[
     A . A\[Transpose] . A . A\[Transpose] . A . A\[Transpose] . A . 
      A\[Transpose]], 
    Norm[A . A . A . A . A\[Transpose] . A\[Transpose] . 
      A\[Transpose] . A\[Transpose]]}
   );
dvals = Range[100, 200, 10];
funcs = {Norm[#, ""Frobenius""] &, Norm, Tr};
sf = ""Log"";
dec[pairSeq_] := {{dvals, pairSeq[[All, 1]]}\[Transpose], {dvals, 
     pairSeq[[All, 2]]}\[Transpose]};
f = Norm;
plotFunc[f_, fname_, sampler_] := (
   ListLinePlot[dec[getVal[#, f, sampler] & /@ dvals], 
    PlotLabel -> fname, AxesLabel -> {""d"", ""value""}, 
    PlotLegends -> {""f(AA'AA'AA'AA')"", ""f(AAAAA'A'A'A')""}, 
    ScalingFunctions -> ""Log""]
   );

randNormal[d_] := RandomVariate[NormalDistribution[], {d, d}];
randUniform[d_] := RandomVariate[UniformDistribution[], {d, d}];
randBernoulli[d_] := 
  N@RandomVariate[BernoulliDistribution[0.5], {d, d}];

sampler = randNormal;
TableForm[{{plotFunc[Norm, ""f(A)=||A||"", sampler],
    plotFunc[Tr, ""f(A)=Tr(A)"", sampler],
    plotFunc[Norm[#, ""Frobenius""] &, 
     ""f(A)=||A\!\(\*SubscriptBox[\(||\), \(F\)]\)"", 
     sampler]}}\[Transpose]]","['random-matrices', 'combinatorics', 'probability-theory', 'random-variables']"
4865914,$L^p_{loc}(\Omega)$ is completely metrizable,"Let $\Omega \subset \mathbb{R}^n$ be a (not necessarily bounded) domain and $1 \leq p \leq \infty$ . Then define $L^p_{loc}(\Omega)$ to be the set of functions $f: \Omega \rightarrow \mathbb{R}$ such that $$\Big(\int_K |f|^p\Big)^{1/p} < \infty, \quad \forall K \text{ compact.}$$ I have read that this space is a completely metrizable space but cannot find a proof so I would like to prove it myself but I am having some trouble. My approach is to find a metric such that convergence with respect to that metric agrees with convergence in $L^p_{loc}$ . I am not sure if this is the right approach since constructing such a metric is not obvious. Is there a better way to prove this?","['metrizability', 'lp-spaces', 'functional-analysis', 'analysis']"
4865958,Lattice Point Combinatorics Question,"""100 people are arranged in a 10×10 square grid. Person 1 can see Person 2 if the line drawn between Person 1 and 2 passes through no other people. For example, a person standing at (0, 0) can see a person standing at (2, 1), but they can't see a person standing at (2, 0), since the person at (1, 0) is in the way of their line of sight. You must prove that there are no five people who can all see each other."" It's clear to me that there are at least four people that can see each other, if you just select a 2x2 rectangle of points. However, proving that another person cannot see all four of those people is where I fall apart. I began by recognizing that two people can see if each other if the differences in their coordinates are coprime. For example, if a person is at (0, 2), and a person is at (1, 4), they can see each other since their difference (1, 2) is coprime. If a person was at (0,2) and another was at (2, 4), then they could not see each other as their difference (2, 2), is not coprime: there is a person at (1, 1) that blocks them I think I'm on the right track, but I'm unsure how to proceed with this information in order to prove that no five people can all see each other. Any help is appreciated.",['combinatorics']
4865959,Ratio of cubic and quadratic form is approximately normal?,"Let be $x_{1},x_{2},x_{3}$ i.i.d. random variables following a normal distribution with $\mu=0$ and $\sigma=1$ . I'm intrigued by the following random variable, which is a ratio of a cubic form and a quadratic form: $$\frac{1}{\sqrt 2}\frac{x_1 x_2^2 + x_1 x_3^2 + x_2 x_1^2 +x_2 x_3^2 + x_3 x_1^2 + x_3 x_2^2 - 6x_1 x_2 x_3}{x_1^2  + x_2^2 + x_3^2 - x_1 x_2 - x_1 x_3 - x_2 x_3 }$$ When sampling the variable, it looks very close to normal (1m samples, 100 bins): I'm trying to understand how it can be so close, but the distribution of cubic forms in random variables isn't well documented or developed, and exact calculations for the ratio of forms seem to be very complicated, even in the case of two quadratic forms. Is there any simple approach to see why the variable distribution might be close to a normal? Cross with stats.SE","['statistics', 'probability-distributions', 'normal-distribution', 'probability', 'quadratic-forms']"
4865977,what more can I say using Sturm's comparison theorem?,"Can I use Sturm's comparison theorem to say something about the average? Let $f(t)$ be a continuous function such that $\lim_{t \to \infty } f(t) = \infty$ . Let us consider the following function $$ \ddot{x}(t) + t^2 x(t) = 0. $$ Let $C$ be a constant. Then, we can find $T$ such that $C^2 < t^2$ for all $t > T$ . By Sturm's comparison theorem, exist $t^*$ in $(\frac{2\pi k}{C},\frac{2\pi (k+1)}{C})$ such that $x(t^*) = 0$ for every $k$ big enough. Can we concluded the following? $$ \lim_{t \to \infty} x(t) =0 $$ or $$ \lim_{t \to \infty} \frac{1}{t}\int_0^t s x(s) ds =0 $$","['average', 'sturm-liouville', 'ordinary-differential-equations']"
4866029,What makes Itô processes special?,"As I understand it, Itô processes are those semi-martingales whose finite variation part is an integral (against Lebesgue measure) and whose (continuous local) martingale part is a stochastic integral against a Brownian motion. But I thought all (continuous local) martingales could be written as integrals against a Brownian motion, which would mean the only thing that distinguishes an Itô process among the semi-martingales is that its finite variation part is absolutely continuous. What am I missing?","['stochastic-analysis', 'stochastic-processes', 'math-history', 'probability-theory']"
4866052,$\dfrac{1}{a-b \cos x}$ format integral,"Looking at solving: $$\int_{0}^{2\pi}\frac{1}{\sqrt{2}-\cos x}\,dx \\$$ According to integral tables (such as this one ) the solution is: $$\frac{2}{\sqrt{a^2 - b^2}}\arctan\frac{\sqrt{a^2 - b^2}\tan(x/2) }{a+b}$$ $$a=\sqrt{2}, b=-1$$ $$\left.2\arctan\frac{2\tan(x/2) }{\sqrt{2}-1}\right\rvert_0^{2\pi}$$ Solving this equation for $x={2\pi}$ results in zero, and for $x=0$ results in zero, so the result is zero.  Except I'm missing something here because the original function to be integrated is never less than zero, it oscillates between 0.414 and 2.414, so there must be net positive area under that curve. Another way to solve this integral is to transform it to a complex line integral, using residues and getting a result of ${2\pi}$ $$\int_{0}^{2\pi}\frac{1}{\sqrt{2}-\cos x}\,dx = {2\pi}\\$$ Why doesn't the table of integrals get the same result?","['integration', 'complex-analysis']"
4866068,Extension of Borel map from a separable metric space to a Polish space,"Suppose that $f:X\to Y$ is a Borel map from separable metric space $X$ to a $T_3$ space $Y$ . Does there always exist a Polish space $\tilde X \supseteq X$ and $T_3$ space $\tilde Y\supseteq Y$ and an extension $\tilde f:\tilde X\to \tilde Y$ of $f$ in the sense that $\tilde f$ is Borel and $\tilde f(x) = f(x)$ for $x\in X$ ? Context: I'm trying to consider spaces which are Borel images of separable metric spaces. However, the theory seems to be available for Polish spaces and not separable metric spaces. That's why I'm trying to show that a Borel image of separable metric space is a subspace of a Borel image of a Polish space, for which I know what to do. I've tried to do this by exhibiting a Borel map $g:\tilde X\to X$ , however, thats only really available if $X$ is an analytic set. Thus I think this might be approachable by extending both the domain and codomain, hence the question.","['measure-theory', 'descriptive-set-theory', 'polish-spaces', 'borel-sets', 'general-topology']"
4866093,Is galois cohomology invariant under inner forms and not just pure inner forms?,"Let $G, G'$ be smooth algebraic groups over $k$ (absolute Galois group $\Gamma$ ) which are etale inner forms of each other, that is, there exists an isomorphism $G_{k_s} \cong G'_{k_s}$ and the associated torsor for the automorphism group is in the image of the natural map $H^1(k, G/Z_G) \to H^1(k, \operatorname{Aut}_G)$ . My question is about the truth of the following proposition: Proposition: Each inner isomorphism $G_{k_s} \cong G'_{k_s}$ induces a bijection $H^1(k, G) \xrightarrow{\sim} H^1(k, G')$ . Is this true as stated, or only for pure inner forms? Does it follow if we make any further assumptions on $G$ (affine, reductive, semisimple . . .)? This is asserted a few places, e.g. the answer here . However, the linked proof here (as in Serre's Cohomologie Galoisienne ), only works for pure inner forms, i.e. those corresponding to cocycles that factor through $H^1(k, G) \to H^1(k, G/Z_G) \to H^1(k, \operatorname{Aut}_G)$ . Proof for pure inner forms The argument, as I understand it, goes as follows. Proof for pure inner forms: Let $s \in \Gamma \mapsto g_s \in G(k_s)$ be a Galois cocycle in $H^1(k, G)$ , with associated cocycle valued in $\operatorname{Aut}_G$ given by conjugation, $s \mapsto \left(h \mapsto g_s h g_s^{-1}\right)$ . Let $G'$ be the pure inner form of $G$ defined by this latter cocycle. In particular, $G'(k_s) = G(k_s)$ , and the two galois actions are related by $s \cdot'(g) = g_s(s \cdot g) g_s^{-1}$ . We define a map $H^1(k, G) \to H^1(k, G')$ on cocycles now as follows: given $s \mapsto a_s \in G(k_s) = G'(k_s)$ a cocycle for the $G$ -Galois action, we can define a cocycle for the $G'$ -Galois action by $s \mapsto b_s := a_s g_s^{-1}$ . This is indeed a cocycle: $$
b_{s} s \cdot'(b_t) = a_s g_s^{-1} g_s(s \cdot(a_t g_t^{-1})) g_s^{-1} = a_s s \cdot(a_t) \; g_{st}^{-1} = a_{st} g_{st}^{-1},
$$ easily seen to descend to $H^1(k, G)$ , and has inverse given by right multiplication by $g_s$ instead. QED. My Thoughts In the proof above, if $s \mapsto g_s \in (G/Z_G)(k_s) = G(k_s)/Z_G(k_s)$ is valued in the inner automorphism group, then the expression $s \mapsto b_s = a_s g_s^{-1}$ is only well-defined up to an element of the center, and the failure of the identity $g_s s(g_t) = g_{st}$ at the level of $G$ (which was used in checking $b_s$ was a cocycle) is measured by a 2-cocycle valued in $Z_G$ . If this 2-cocycle is not a coboundary, we should not expect to be able to choose a cocycle lifting the $g_s$ to $G(k_s)$ . In other words, we should run into the difficulty that the map $H^1(k, G) \to H^1(k, G/Z_G)$ is rarely surjective, fitting into the LES $$
H^1(k, Z_G) \longrightarrow H^1(k, G) \longrightarrow H^1(k, G/Z_G) \longrightarrow H^2(k, Z_G).
$$ For $\operatorname{GL}_n$ , for example, $Z_G = \mathbb{G}_m$ , and so the Brauer group provides an obstruction to lifting cocycles from $\operatorname{PGL}_n$ to $\operatorname{GL}_n$ . I haven't been able to construct a counterexample among the forms of $\operatorname{GL}_n$ , however.","['algebraic-groups', 'number-theory', 'galois-theory', 'algebraic-geometry', 'group-cohomology']"
4866123,The Jacobian of $g(\vec{x}) = f(A\vec{x} + \vec{b})\vec{x}$.,"Let $A = \mathbb{R}^{n \times n}$ and $f: \mathbb{R^{n}} \mapsto \mathbb{R}$ I can compute Jacobians of simple functions, but this question obliterated me, and I have spent days trying to understand it. Within the solution they derive that $[D(\vec{g}(\vec{x}))]_{jk} = f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})\frac{\partial \vec{x}_j}{\partial x_k} + \vec{x}_j \frac{\partial f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})}{\partial x_k}$ This is fine as it is just chain rule, but where they lose me is when they change to summation: $f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})\frac{\partial \vec{x}_j}{\partial x_k} + \vec{x}_j \sum_{\ell=1}^{n} \frac{\partial f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})}{\partial (\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})_{\ell}} \cdot \frac{\partial (\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})_{\ell}}{\partial x_k}$ I've tried coming up with a simple example using the 1-norm of an A $\mathbb{R}^{2 \times 2}$ , and the accompanying x and b vectors, but it doesn't help because it is too specific compared to how general this solution is. If anyone can explain the change to summation, I'd be greatly appreciative.","['optimization', 'multivariable-calculus', 'jacobian', 'linear-algebra']"
4866132,Grouplike Hopf algebras are group rings?,"Let $H$ be a commutative and cocommutative Hopf algebra over an algebraically closed field $k$ . I've read that if $H$ is grouplike in the sense that it has no nonzero primitive elements, then $H$ is isomorphic to the group ring on its grouplike elements,but no reference or proof was given. What is a proof of this fact? What is a good reference? Is there differing terminology I should be aware of? Are the commutativity and cocommutativity hypotheses necessary? My source is the discussion after remark 1.0.7 in Hopkins and Lurie .","['algebraic-geometry', 'abstract-algebra', 'hopf-algebras', 'group-schemes']"
4866157,"Find all $(a,b)\in\Bbb{N},$ such that $5^a +2^b +8$ is a perfect square.","Find all $(a,b)\in\Bbb{N},$ such that $5^a +2^b +8$ is a perfect square. My approach:
Let $5^a +2^b +8=k^2\implies \bigg(k+5^{\frac{a}{2}}\bigg)\bigg(k-5^{\frac{a}{2}}\bigg)=8\bigg(1+2^{b-3}\bigg).$ The RHS is greater than zero and even,so each of $\bigg(k+5^{\frac{a}{2}}\bigg)$ and $\bigg(k-5^{\frac{a}{2}}\bigg)$ is even and $k$ is odd. $1+2^{b-3}$ is even only when $b=3.$ Case(1): When $b=3,k^2 =5^a +16;$ Let $k=2m+1\implies 4m^2 +4m+1=5^a +16$ or $4m(m+1)=5^a +15=5\bigg(5^{a-1}+3\bigg).$ If $m$ is odd, $m+1$ is even and vice-versa.
Equating odd and even parts ,
Sub-case(a): $m+1=5\implies m=4,$ and $4m=16=5^{a-1}+3\implies 5^{a-1}=13\implies$ no natural number exist.
Sub-case (b): $m=5$ and $4(m+1)=24=5^{a-1}+3\implies 5^{a-1}=21\implies$ again no natural number exist.
So,there is no natural number solution set exist for $b=3.$ This implies that $1+2^{b-3}$ is odd.
This means that even components: $k+5^{\frac{a}{2}}=c\bigg(1+2^{b-3}\bigg).$ And $8=8\bigg(k-5^{\frac{a}{2}}\bigg)......(A).$ where $c$ is even .
From equation $(A),$ $c$ has to be non-zero even factor of $8,$ but not $8$ because otherwise $k-5^{\frac{a}{2}}=1$ and odd.
So, $c=(2,4).$ Case(2): When $c=2$ $k-5^{\frac{a}{2}}=4$ $k+5^{\frac{a}{2}}=2\bigg(1+2^{b-3})\implies 5^{\frac{a}{2}}=2^{b-3}-1$ Or $2^{b-3}-5^{\frac{a}{2}}=1.$ The only $a$ and $b$ for which this works is $a=2$ and $b=4\implies k=7.$ There are other solutions too! And I have no clue how to proceed from here,also if any other way is possible,then please try to show here.
Thank you for each and everyone trying to indulge in solving it! Is there any complete solution?","['number-theory', 'diophantine-equations']"
4866168,Minimize $\frac{1}{2}\sum_{k=1}^m (x_{k+1}-x_{k})^2$,"Given sequence: $$
\begin{cases}
x_{n+1}(2\cos(\frac{\pi}{m})-x_n)=1,\forall n\geq 1\\
x_1=x\in\mathbb R,m\in\mathbb N,m\geq 2
\end{cases}
$$ Minimize $$A=\frac{1}{2}\sum_{k=1}^m (x_{k+1}-x_{k})^2$$ Edit I've proved that $\frac{dA}{dx}|_{x=-1}=0$ (see my answer). The problem now is to show that $A_{\min}=A|_{x=-1}$ is the global minimum. The part below belongs to the original post $A$ is actually the area bounded by the zig-zag loop between $y=\frac{1}{2\cos(\frac{\pi}{m})-x}$ and $y=x$ . I tried it on a computer and observed that $A$ minimizes at $x=-1,\forall m$ , and other $m-1$ values of $x$ , resulting in the same minimal value of $A$ (the only minimum of $A$ ). I did try to prove the necessary condition $\frac{dA}{dx} |_{x=-1}=0$ , after some algebra, the equivalent equation is: $$\sum_{k=1}^m \left(x_k-2\cos\left(\frac{\pi}{m}\right)x_{k+1}^2+x_{k+1}^3\right)\prod_{i=1}^k x_i^2|_{x=-1}=0$$ I recognized a sort of symmetry inside the sum (sum of two terms (of the sum above) with indices adding up to $m$ is $0$ and the last term is $0$ as well) but I couldn't figure out how to deal with the product in the sum. How do I proceed from here? Also, with the necessary condition proved, how can I show that there is exactly one minimal value of $A$ for every $m$ ? [Feel free to check out this graph ]","['convex-optimization', 'recurrence-relations', 'real-analysis', 'sequences-and-series', 'optimization']"
4866177,How can I calculate position of a point after it moved towards another point?,"I'm working on a text-based game involving a spaceship travelling between planets. Space is represented as a 2D plane. The ship has a current position, a destination and a speed. Every minute, the game engine updates the current position of the ship, according to its current position, current destination and speed. Knowing the ship speed (in unit per minute), I know the distance traveled by the ship in one minute. But I'm struggling to calculate the new position of the ship after it traveled that distance. Let's say the starting position of the ship is (1,1), its destination position is (3,4) and the ship moving towards it at the speed of 1 unit per minute. What would be the new position at the ship after 1 minute, having traveled 1 unit? Furthermore, what is a formula to calculate the new position (xP, yP) of the ship knowing its starting position (xA, yA), its current destination (xB, yB), and the distance traveled in 1 minute ( D )? PS: I should say that I've never learned mathematics so this might be very basic but I can't wrap my head around it and the problem might not be phrased well. But I'd be happy to provide more information if necessary.",['geometry']
4866180,Combinatorial problem about finding equidistant words,"Assume I have an alphabet $\{A,B,C...\}$ with a total of $K$ symbols. For words of same length, I define the distance $d$ between them as the number of positions in which they have differing symbols (reduces to Hamming distance for $K=2$ ). Starting from a fixed word $w$ , I am interested in finding sets of words such that they 1) have distance $d$ to $w$ , 2) have distance $D$ between each other. How many of such sets are there, how big are they, and what would be a smart way to generate them? Consider a solution attempt for $K=4$ , $w_0=ABCDABCD$ , $d=2$ , and $D=4$ . One can choose $\displaystyle\binom{8}{2} =\dfrac{8!}{2!6!}=28$ combinations of the two letters to be alternated in $w_0$ . For each of these two positions, we have $4-1=3$ options, which gives us the total of $28\times3\times3=252$ words $w_1$ of distance $2$ from $w_0$ . For each $w_1$ of these 252 words, there are $\displaystyle\binom{6}{2}\times3\times3 =135$ words $w_2$ which are distance 2 from $w_0$ and distance 4 from $w_1$ . To find such words, we start from $w_0$ and alternate any two other letters except those two which we alternated to get $w_1$ . For each of the 135 words in the previous step, there are $\displaystyle\binom{4}{2}\times3\times3 =54$ of the words $w_3$ which are distance 4 from each other. $\displaystyle\binom{2}{2}\times3\times3 =9$ options for $w_4$ . So it seems that one can find at most 4 words with the desired property, and there are $252\times135\times54\times9=16533720$ ways to do so.","['combinatorics-on-words', 'combinatorics', 'extremal-combinatorics']"
4866183,Prove that the Riemann curvature tensor is a tensor,"How would I prove that the Riemann curvature tensor $R: \scr X(M)^3 → \scr X(M)$ , $R(X, Y )Z := \nabla_X\nabla_Y Z − \nabla_Y \nabla_XZ − \nabla_{[X,Y ]}Z$ , is indeed a tensor ? I thought I could use  this: (pag 41 in https://radbouduniversitypress.nl/site/books/m/10.54195/EFVF4478/ ) and simply prove that $R(fX_1+gX_2, Y )Z=fR(X_1, Y )Z+gR(X_2, Y )Z$ ...(1) $R(X, fY_1+gY_2 )Z=fR(X,Y_1)Z+gR(X, Y_2 )Z$ ...(2) and $R(X, Y )(fZ_1+gZ_2)=fR(X,Y)Z_1+gR(X, Y )Z_2$ ...(3) Am I on the right track?Isn't doing this indeed using proposition 2.7? My T.A said I cannot use 2.7 it as it is stated, but I don't see why And then how do I use proposition 2.7 then?","['tensors', 'general-relativity', 'riemannian-geometry', 'differential-geometry']"
4866189,Matrix involving reciprocal factorials,"Let $m$ and $n$ be two integers and $m \le n$ . There are a matrix $A$ of $m$ -by- $m$ with $A(i,j) = 1/(2n+2j-2i)!$ and a vector $r$ of $m$ entries with $r(i) = 2/(2n+2i)!$ . Is there a formula for the inner product of $r$ and the first column of the inverse of $A$ ? Actually, I do not even know whether $A$ is invertible (a proof of this is helpful). Or, can it be shown that the inner product aforementioned is bounded from above by a constant strictly less than one? The background of this question is as follows. There is a linear system $$
   \begin{pmatrix}1 & \frac{2}{(2n+2)!} & \frac{2}{(2n+4)!} & \ldots & \frac{2}{(2n+2m)!} \\1 & \frac{1}{(2n)!} & \frac{1}{(2n+2)!} & \ldots & \frac{1}{(2n+2m-2)!}\\ 0 & \frac{1}{(2n-2)!} & \frac{1}{(2n)!} & \ldots & \frac{1}{(2n+2m-4)!}\\ 0 & \frac{1}{(2n-4)!} & \frac{1}{(2n-2)!} & \ldots & \frac{1}{(2n+2m-6)!}\\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 0 & \frac{1}{(2n+2-2m)!} & \frac{1}{(2n+4-2m)!} & \ldots & \frac{1}{(2n)!}\end{pmatrix}
\begin{pmatrix}\vphantom{\frac{1}{(2n+2)!}}v_0\\ \vphantom{\frac{1}{(2n+2)!}}v_1\\ \vphantom{\frac{1}{(2n+2)!}}v_2\\ \vphantom{\frac{1}{(2n+2)!}}v_3\\ \vdots\\ \vphantom{\frac{1}{(2n+2)!}}v_m\end{pmatrix} = 
\begin{pmatrix}\vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}}O(\epsilon)\\ \vphantom{\frac{1}{(2n+2)!}} O(\epsilon) \\ \vdots \\ \vphantom{\frac{1}{(2n+2)!}} O(\epsilon)\end{pmatrix},
$$ where $\epsilon>0$ , and $O(\epsilon)$ is the big $O$ notation for any quantity $Q$ for which there exists a constant $C>0$ such that $|Q|< C \epsilon$ . So $O(\epsilon)$ can be different values at different places. It can be found the previous matrix $A$ is just the the bottom right part of the above coefficient matrix, from the 2nd column to the last and from the 2nd row to the last. The previous vector $r$ is just the top right part of the above coefficient matrix, from the 2nd column to the last in the 1st row. It is wanted to have an estimate of $v_0$ . Can it be shown that $v_0=O(\epsilon)$ independent of $m$ and $n$ ? A less but still wanted goal is just to show the above matrix is invertible.","['schur-complement', 'factorial', 'toeplitz-matrices', 'matrices', 'linear-algebra']"
4866204,What's wrong with this derivation of the volume of a hemisphere?,"My idea to calculate the volume of the hemisphere is to sum up the area of circles of all radii up to the radius of the hemisphere we are interested in: $$\int_0^r \pi x^2 dx$$ This gives $\frac{1}{3}\pi r^3 \neq \frac{2}{3}\pi r^3$ , so deviating by a factor of $2$ from the known formula for the volume of a hemisphere. Can you explain where my reasoning is wrong and why it deviates by a factor of $2$ ?","['integration', 'solid-geometry', 'solution-verification', 'geometry']"
4866216,Eliminating $\theta$ from $x^2+y^2=\frac{x\cos3\theta+y\sin3\theta}{\cos^3\theta}$ and $x^2+y^2=\frac{y\cos3\theta-x\sin3\theta}{\cos^3\theta}$,"An interesting problem from a 1913 university entrance examination (Melbourne, Australia): Eliminate $\theta$ from the expressions $$x^{2}+y^{2}=\frac{x \cos{3\theta}+y \sin{3\theta}}{\cos^{3}\theta} \tag1$$ $$x^{2}+y^{2}=\frac{y \cos{3\theta}-x \sin{3\theta}}{\cos^{3}\theta} \tag2$$ Find expressions for $x$ and $y$ in terms of $\theta$ As with many of these historic problems, I'm sure it has been discussed somewhere at length... Solutions involving complex numbers would have been acceptable at the time. (EDIT 20/Feb Australian time) I thought I had a solution but upon review I don't think it works (I found an expression for y, then substituted. It was very messy and I don't think the algebra was totally correct.) so editing the post to say: any solutions or suggestions appreciated. The wording of similar questions on the paper suggests that a ""simplified"" expression is possible. (Edit 21/Feb) Thanks everyone for the excellent suggestions. Really appreciated. I am wondering if the substitution $t=\tan{(\frac{\theta}{2})}$ works here. If $t=\tan{(\frac{\theta}{2})}$ then $\sin{\theta}=\frac{2t}{t^{2}+1}$ and $\cos{\theta}=\frac{1-t^{2}}{1+t^{2}}$ . After some basic manipulation, this seems to have promise, but I'm yet to finish the problem using this approach.","['trigonometry', 'math-history']"
4866259,"If $X_n=Y_n$ in distribution and $X_n \to 0$ almost surely, then does $Y_n \to 0$ almost surely ? (For particular $X_n$ and $Y_n$)","Of course in general the answer to my question is NO, however I am interested in two sequences of random variables that have very specific structure. Let $a_n,b_n$ be deterministic (real-valued) sequences and let $\xi_n$ be an i.i.d sequence of random variables. Then let $$ X_n=a_n+b_n\xi_n, \quad Y_n=a_n+b_n\xi,$$ where $\xi=\xi_n$ in distribution. If we suppose $X_n \to 0$ almost surely, can we conclude that $Y_n \to 0$ almost surely? An idea of a possible method of proof (or a counter example) would be much appreciated. Note we also assume that the sequence $b_n$ is non-degenerate i.e not the zero sequence (otherwise the question would not be very interesting).","['limits', 'probability-theory', 'probability', 'sequences-and-series']"
4866309,Show $\root{-e}\of{e}<\ln2$ without a calculator,I tried manipulating the expression to come up with inequalities such as $1<e^{\frac{1}{e}}\ln2$ . One idea I have is to show that $\lim_{x\to\infty}\left(\frac{1}{x}+\ln2\right)\left(\frac{1}{x}+1\right)^{\frac{x}{e}}>1$ . $$\lim_{x\to\infty}\frac{\left(1+x\ln2\right)\left(1+x\right)^{\frac{x}{e}}}{x^{1+\frac{x}{e}}}>1$$ I don't know how I could progress.,"['limits', 'algebra-precalculus', 'number-comparison', 'inequality']"
4866357,Proving solutions of $y''+p(x)y'+q(x)y=0$ to be linearly independent,"When studying Elementary Differential Equations by William, I found trouble understanding Theorem 5.1.5 It says the two solutions are linearly independent iff their Wronskian is never zero, but I think they can still be linearly independent even if the Wronskian is zero for some $x$ . In the proof, when $W(x_0)=0$ , Theorem 5.1.4 is used to show $W\equiv 0$ . This theorem is the Abel's identity . It seems flawless, until I saw this answer . So $p(x)$ must be continuous on $(a,b)$ , but it is not as long as $W(x_0)=0$ , so we shouldn't use Abel's identity. This is because $$
    y_1''+p(x)y_1'+q(x)y_1,\quad
    y_2''+p(x)y_2'+q(x)y_2
$$ $$
    y_1''y_2+p(x)y_1'y_2+q(x)y_1y_2,\quad
    y_2''y_1+p(x)y_2'y_1+q(x)y_2y_1
$$ $$
p(x) = \frac{y_1''y_2-y_2''y_1}{y_2'y_1-y_1'y_2} = \frac{y_1''y_2-y_2''y_1}{W[y_1,y_2](x)},
$$ $p(x)$ is undefined at $x_0\in(a,b)$ . Also, I noticed all this by considering an example: for two solutions $y_1=1+x$ and $y_2=1+x^2$ , $W(x) = x^2+2x-1$ , on interval $(0,\infty)$ . So although $W(\sqrt{2}-1)=0$ , I think the two functions are still linearly independent on the interval, at least according to the definitions here . But using Theorem 5.1.5 they should be linearly dependent, because $W$ is zero for a point in the interval? Now it really confuses me despite thinking about it for hours. Which part did I miss? I am sorry if the question is too dumb, still not accustomed to the linearly independence of functions.","['wronskian', 'ordinary-differential-equations']"
4866371,algebraic equation with powers,"The original statement of the problem : Solve in $\mathbb R$ the following equation : $$ a^{log_b x^2 } + a^{log_x b^2 } = a^{1+log_b x } + b^{1+log_x b } $$ where $a,b>0$ and $b \neq 1$ A more general statement of the problem : Find all real solutions t , to the equation $ a^{2t} + a^{\frac{2}{t}} = a^{1+t} + a^{1+ \frac{1}{t}} $ where a is a strictly positive real constant. EDIT I hope I did not complicate the problem by making these transformations to the second equation, I am especially interested in solving the original one, so that's more important. My approach : First of all, if a = 1 , it is clear that any real t verifies the given equation. In the following, I assumed that a > 1 to simplify, and then take the case when 0<a<1 , analogously. I tried to prove that t = 1 is the only solution of the equation for any a different from 1. I transformed the equation into this $a^{t}(a^{t}-a) + a^{\frac{1}{t}}(a^{\frac{1}{t}}-a)=0$ where I was going to use some inequalities and prove that the left side is strictly greater for t different from 1 but I got stuck. Any and all proofs will be helpful. Thanks a lot!",['algebra-precalculus']
4866422,"For a vector-valued $f,$ is there a convex $\varphi$ such that $\nabla \varphi = f?$","Let $U \subset \mathbb{R}^d$ be a convex open set.
Suppose a continuously differentiable vector-valued function $f : U \to \mathbb{R}^d$ satisfies that its derivative $D f : U \to \mathbb{R}^{d \times d}$ is symmetric and positive semidefinite.
Is there a convex function $\varphi : U \to \mathbb{R}$ such that $\nabla \varphi = f$ ? I guess the answer is yes.
I attempted to show that $f$ is cyclically monotone, but I could show only the monotonicity as follows.
Let $x_1, \dots, x_k \in U.$ Then, Taylor's theorem says $$
f(x_{i+1})
=
f(x_i)
+
\left(
\int_0^1
D f ((1 - s) x_i + s x_{i+1}) 
d s
\right)
(x_{i+1} - x_i)
.
$$ Thus, we have $$
(x_{i+1} - x_i)^\prime
(f(x_{i+1}) - f(x_i))
=
(x_{i+1} - x_i)^\prime
\left(
\int_0^1
D f ((1 - s) x_i + s x_{i+1}) 
d s
\right)
(x_{i+1} - x_i)
\geq
0
$$ by the positive semidefiniteness of $D f.$ In this proof, the symmetry of $D f$ is not used, but I am not sure how it helps. EDIT: If $f$ is the gradient of some function, the monotonicity implies that the antiderivative is convex, but I am not assuming that $f$ is a gradient.
Also, if $f$ is linear, p.240 of Rockafellar's book claims the answer is yes.
I would like to know what happens if $f$ is nonlinear.","['analysis', 'real-analysis', 'functional-analysis', 'partial-differential-equations', 'convex-analysis']"
4866423,$\int_{-\infty}^{\infty}\exp\left(-\frac{\beta}{4}\left(y^2+x^2-1\right)^2\right)dy$,"I'm interested in the following integral: \begin{equation}
P_\beta(x) = \int_{-\infty}^{\infty}\exp\left(-\frac{\beta}{4}\left(y^2+x^2-1\right)^2\right)dy,
\end{equation} where $\beta>0$ . For context, this integral comes from the probability density function of the displacement (or velocity) of a Van der Pol-Rayleigh oscillator, as shown by Talmadge 1991 (eqn. 9). This integral is very similar in form to this question which has already been answered on here, however Did's method was only correct for $a>0$ , $b >0$ . For my specific case, $b<0$ ( $x^2-1<0$ ) forms an important solution case. Through some trial and error with Mathematica/MATLAB, I was able to find the general solution for my case: \begin{equation}
P_\beta(x) = \sqrt{|x^2-1|}\exp\left(-\frac{\beta}{4}\left(x^2-1\right)^2\right) F(x^2-1,\beta),
\end{equation} where \begin{equation}
F(x^2-1,\beta) = \sqrt{2}{K}_{1/4}\left(\frac{\beta}{4}\left(x^2-1\right)^2\right)+2 H\left(-(x^2-1)\right){I}_{1/4}\left(\frac{\beta}{4}\left(x^2-1\right)^2\right),
\end{equation} and where $I_{\nu}(z)$ , $K_{\nu}(z)$ are the modified Bessel functions of the first and second kind, respectively, and where $H(x)$ is the Heaviside function. When $x^2-1>0$ , I can see how the solution is formed, but I'm struggling with the $x^2-1<0$ case. Any insights or hints would be greatly appreciated.","['integration', 'calculus', 'probability', 'bessel-functions']"
4866477,"Is there a sequence, defined in terms of integers and the basic arithmetic operations such that $\lim\limits_{n\rightarrow\infty} s_n = \pi$?","It's a well-known formula that $$
e = \lim\limits_{n\rightarrow\infty} (1+1/n)^n
$$ and I'm wondering if there exists a similar formula for $\pi$ . All the formulas I know for $\pi$ involve series or products or special functions (such as inverse trig functions). Or one can use Stirling's approximation if factorials are allowed. I'm wondering if there exists a function $F:\mathbb{Z}^m\rightarrow \mathbb{R}$ such that $F$ can be written in terms of a finite number of additions, subtractions, multiplications, divisions, and exponentiations, and $$
\pi = \lim\limits_{n\rightarrow\infty} F(\underbrace{n,n,\cdots,n}_{r \text{ times}},k_1,k_2,k_3,\dots,k_{m-r})
$$ for some fixed integers $m$ , $r$ , and $k_1,k_2,\dots,k_{m-r}$ . In particular, a formula that is defined by a series, infinite product, recursive formula, or continued fraction would not satisfy these conditions. To be perfectly clear about what I mean, let me formalize. Define a set of elementary sequences $S\subset \mathbb{R}^\mathbb{Z}$ as follows: Let $S_0=\{n\}\cup\{\text{all constant, integer-valued sequences}\}$ and define $S_i$ recursively by $$
S_i = \left\{ s_n+t_n,\, s_nt_n,\, (s_n)^{t_n},\, s_n-t_n,\, \frac{s_n}{t_n}\, \mid\, (s_n),(t_n)\in S_{i-1} \right\}.
$$ Then let $S = \bigcup\limits_{i=0}^\infty S_i$ . Does there exist a sequence $(s_n)\in S$ such that $\lim\limits_{n\rightarrow\infty} s_n =\pi$ ? Note that $(s_n)\in S$ implies $(s_n)\in S_i$ for some finite $i$ , which means that each term in the sequence $s_n$ has at most $i$ operations in it. So for example, the sequence $(1+1/n)^n$ is in $S_3$ because it involves 3 operations (one addition, one division, and one exponentiation). A series, such as $s_n = \sum\limits_{k=1}^n \frac{(-1)^k 4}{2k-1}$ is not in $S$ because the number of operations in the $n$ th term is unbounded.","['limits', 'number-theory', 'sequences-and-series']"
4866488,In what time $\tau$ will the particle reach the point $x=0$?,"A particle of mass $m$ capable of moving along the $x$ -axis, is acted upon by a force $F(x) = -\frac{k}{x^3}$ . At the initial time moment $t=0$ , the particle is at the point $x=x_0>0$ , and its velocity is $\dot{x}(0)=0$ . In what time $\tau$ will the particle reach the point $x=0$ ? My attempt: We basically have to solve this second order nonlinear ODE $$\frac{d^{2}x}{dt^{2}}=-\frac{k}{mx^{3}}.$$ Using substitution $v=\frac{dx}{dt}$ , we get $\frac{dv}{dt}=-\frac{k}{mx^3}$ , using chain rule to rewrite $\frac{dv}{dt}=v\frac{dv}{dx}$ , then separate ant integrate $$\frac{1}{2}v^2 = \frac{k}{2mx^2} + A \implies v = \sqrt{\frac{k}{mx^2} + A}.$$ The result I got is $v = \sqrt{\frac{k}{mx^2} + A}$ , where $A$ is a constant of integration. After plugging in the initial conditions we find the value of $A=-\frac{k}{2mx_0^2}$ . Next, applying the initial conditions and keeping in mind that velocity is negative due to given conditions, $$$$ \begin{aligned}
&\int_0^\tau dt=-\int_{x_0}^0\frac{dx}{\sqrt{\frac k{mx^2}-\frac k{mx_0^2}}} \\
&\tau=-\int_{x_0}^0\frac{dx}{\sqrt{\frac k{mx^2}-\frac k{mx_0^2}}}
\end{aligned} Are there any ways to simplify it further? I suppose we can make use of some integrals with similar form like $\int \frac{dx}{\sqrt{a^2-x^2}}=\arcsin(\frac{x}{a})$","['integration', 'ordinary-differential-equations', 'calculus', 'solution-verification', 'physics']"
4866556,"Is $SL(2,\Bbb R)$ generated by $SO(2)$ and a single upper triangular element?","Consider the subgroup $\Gamma < \operatorname{SL}(2,\mathbb{R})$ generated by the element $$
\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}
$$ and all elements of $\operatorname{SO}(2)$ . Is $\Gamma = \operatorname{SL}(2,\mathbb{R})$ ? Some calculations show that $$
\begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \in \Gamma
$$ and hence $\operatorname{SL}(2,\mathbb{Z}) \subseteq \Gamma$ .","['matrices', 'group-theory', 'lie-groups', 'matrix-decomposition']"
4866564,Is it possible for the dihedral angles of a polyhedron to all grow simultaneously?,"Suppose $P$ and $Q$ are combinatorially equivalent non-self-intersecting polyhedra in $\mathbb{R}^3$ , with $f$ a map from edges of $P$ to edges of $Q$ under said combinatorial equivalence. Let $\theta(e)$ be the dihedral angle of edge $e$ . If it is the case that for all $e\in D$ , $\theta(e)\le \theta(f(e))$ , must we have $\theta(e)=\theta(f(e))$ for all $e$ ? That is, is it possible to distort the geometry of $P$ such that no dihedral angle decreases, and at least one dihedral angle increases? My intuition is that this should not be possible, that there is some conserved notion of ""total curvature"" which would be violated by such an operation. But I can't seem to prove any such invariant (the sum certainly isn't one, for instance). I'm also interested in the restriction to the convex case, ie where every dihedral angle is less than $\pi$ .","['polyhedra', 'geometry']"
4866634,How to calculate the area of a projected path onto a plane? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question How do you ""orthogonally project"" the shape? After I drew multiple graphs, I still had no idea. The correct answer is $12$ . I guess it is $3 \sqrt 2 \cdot 2 \sqrt 2$ according to the gist of the graph that I drew (a plane with two parallel sides through four mid points each, and two shorter sides a little out of shape) The problem: ""A bee travels in a series of steps of length $1$ : north, west, north, west, up, south, east, south, east, down.
(The bee can move in three dimensions, so north is distinct from up.)
There exists a plane $P$ that passes through the midpoints of each step. Suppose we orthogonally project the bee’s path onto the plane $P$ , and let $A$ be the area of the resulting figure. What is $A^2$ ?""","['area', 'geometry', '3d']"
4866650,Smallest group acting transitively on projective space,"Let $ K $ be a field. Let $ K^n $ be an $ n $ dimensional vector space over $ K $ . Let $ KP^{n-1} $ be the projective space of lines in $ K^n $ . Let $ GL(n,K) $ be the group of invertible $ n \times n $ matrices over $ K $ . What is the smallest subgroup of $ GL(n,K) $ that acts transitively on $ KP^{n-1} $ ? Context: When $ K=\mathbb{C} $ then I think the smallest subgroup of $ GL(n,\mathbb{C}) $ acting transitively on $ \mathbb{C}P^{n-1} $ is $ SU(n) $ . When $ K=\mathbb{R} $ then I think the smallest subgroup of $ GL(n,\mathbb{R}) $ acting transitively on $ \mathbb{R}P^{n-1} $ is $ SO(n) $ . I was wondering if this is true and also what the corresponding group is for other choices of $ K $ .","['representation-theory', 'finite-groups', 'algebraic-geometry', 'group-theory', 'lie-groups']"
4866703,Is every group isomorphic to a set of isomorphisms?,"Informally: Every group is representable (up to an isomorhism) as a group of isomorphisms. Formally: For every group $G$ there exists a binary relation $f$ on some set $U$ such that $G$ is isomorphic to group of all bijections $g$ on $U$ such that $g^{-1}\circ f\circ g=f$ . In other words, is every group $G$ isomorphic to the group of isomorphisms of some (possibly infinite) digraph? Moreover, is this digraph $f$ unique up to an isomorphism, for each group $G$ ?","['graph-isomorphism', 'group-theory', 'group-isomorphism']"
4866712,On the isomorphism $\mathbb{R} \cong \mathbb{R}^2$,"It is a well known pathological counterexample in group theory that (assuming the axiom of choice), we have an isomorphism $$\mathbb{R} \cong \mathbb{R}^2,$$ where these are additive groups. This somewhat unsettling result generally is shown by appealing to fact that both of these groups, when viewed as $\mathbb{Q}$ -vector spaces, have the same dimension. Naturally, showing this relies on the axiom of choice. As with most results coming from the axiom of choice, it is strongly insinuated that one cannot explicitly construct an isomorphism $\phi : \mathbb{R} \rightarrow \mathbb{R}^2$ . This is very believable, but I'm wondering if it has actually been shown that this fact is equivalent to choice (i.e. without choice, the two are not isomorphic). If so, what would a proof of that look like? All comments are appreciated.","['axiom-of-choice', 'group-theory', 'abstract-algebra']"
4866733,Prove that Wallis' product and Euler's formula directly imply that $(-1/2)!=\sqrt{\pi}$,"(This occured to me recently,
and I was pretty sure that
it was true,
so I was pleased that
it really was.
This has almost certainly
been published
many times before,
but I didn't see it
in either of the Wikipedia articles
on the Wallis product
and Euler's limit formula
for factorial
so I thought that
I would propose it here.) Euler's formula for general factorial is $$z!
=\lim_{n \to \infty} \frac{n!n^z}{\displaystyle\prod_{k=1}^n (z+k)}
. $$ Wallis' product is $$\frac{\pi}{2}
=\prod_{k=1}^{\infty} \frac{4k^2}{4k^2-1}
$$ Prove that
Wallis' product and
Euler's formula
directly imply that $(-1/2)!
=\sqrt{\pi}
$ . I'll post my answer
in a few days
if no one else does.","['infinite-product', 'limits', 'pi', 'factorial']"
4866760,"How do I triangulate a point from 3 segments, rather than from 3 points?","I have a real-world acoustics problem that I'm trying to solve for some work related R&D. Hopefully you big brained folks can explain down to me. :) I'm good at visualizing math concepts, but I lack any substantial experience, save perhaps g/FooBar. So, sans elegance: Edit: Using the below descriptions, I'm attempting to model a room with people talking into mics while facing the center of the room. The segment variability models the potential distance to a mic that the respective person might be, as people in this environment will often change their position from leaning into a mic to leaning back in their chair. I realize these constraints limit beyond actual potential for where a person might be physically located relative to their mic. However, I picked the constraints because 1. they cover the vast majority of cases 2. I am actually interested in learning how to solve the problem as described 3. the output I am trying to achieve is for estimating time delays to compare to cross-correlegrams, so imperfection is quite alright. Let's start with a basic open acoustic space, and represent its length and width using a 2d grid. For simplicity, let's assume our origin (0, 0) marks the center of the room. Place 3 microphones (a,b,c) at random in the room.
Imagine onto our 2d space 3 rays (a1,b1,c1) that start at the origin, and cross each of the microphone coordinates. Now let's add 3 sound sources (A,B,C) to the space with the following constraints: A,B,C respectively fall on rays a1, b1, and c1 the distance to each sound source A,B,C from 0,0 is > the distance to the respective mics apply some arbitrary limit (L) to the distance between respective sources and mics So, based on the title of this question, the 3 segments start at the mic coords a,b,c, and overlap rays a1,b1,c1 each for a length of L. The sound sources A,B,C could fall anywhere on their respective segments. How do I use the (precise) varying sound arrival times at the mics a,b,c to determine the relative coordinates of the mics when I don't know exactly where the sound sources A,B,C are located, only that the sources A,B,C are constrained to fall on the segments a1,b1,c1? To be overtly concise, we do not start with exact knowledge of the distances A-a, B-b, C-c, rather the data we have is (A:a-b,a-c)(B:b-a,b-c)(C:c-a,c-b). Edit: ""(A:a-b,a-c)"" is a made-up convention. Neither math nor physics. In this example, I just mean to indicate that for source A signal, we know the difference in arrival time between a-b, and a-c. Thanks for the help!
-Sobel","['trigonometry', 'geometry']"
4866765,Metric circle on a sphere,"Consider $\mathbb S_R^2 \subseteq \mathbb R^3$ be a sphere of radius $R$ . Let $N$ denote the north pole and $0 < r < R \pi$ . Define the metric circle with center N to be all points, $x \in \mathbb S_R^2$ that $$d_{\mathbb S_R^2}(x,N) = r$$ Here $d_{\mathbb S_R^2}$ is the induced metric on the sphere. Parameterize $\mathbb S_R^2$ with the usual spherical coordinates: $$\Gamma:[0,2\pi) \times [0,\pi]\to\mathbb{R}^3 \quad \quad  (\phi, \theta)\mapsto (R\sin\theta\cos\phi,R\sin\theta\sin\phi, R\cos\theta),$$ My hunch is that the metric circle with center N is spherical circle: $$\Gamma ( [0,2\pi) \times \{ r / R \} ) \subseteq \mathbb S_R^2$$ However, I cannot justify this claim using only elementary arguments using only elementary techniques. Edit: Bump!","['spheres', 'differential-geometry']"
4866838,Biased random walk with unequal step size,"Consider an asymmetric random walk $(X_n)$ in which the initial point is one ( $X_0 = 1$ ). It increases by $a$ with a probability of 0.3, remains the same with a probability of 0.4, and decreases by $b$ with a probability of 0.3, where $a<b$ and $a,b \in \mathbb{R}$ . Also, suppose this process stops once $X_n \geq \lambda$ . For example, $a=0.3$ , $b=0.5$ , and $\lambda=1.4$ . How can I calculate the probability of this process stopping? That is, what would be the probability of $X_n$ ever getting bigger than $\lambda=1.4$ ?
Since the probability of stopping at the second period is $0.3\cdot 0.3$ , it would be greater than zero. Also, since it is supermartingale, by the maximal inequality for supermartingale, \begin{equation*}
P(\sup X_n \geq 1.4) \leq \frac{\mathbb{E} X_0 }{1.4} = \frac{1}{1.4}.
\end{equation*} Thus, it would not be one. However, I would like to derive precise predictions about them, which is where I am stuck now. I found other problems with asymmetric random walks with equal integer step sizes. However, I am unsure if I can still use the same ways in this environment. Also, would it be a completely different problem in an environment where the step sizes are irrational numbers?","['martingales', 'random-walk', 'probability-theory', 'probability']"
4866841,Explain the continuity of the function $f$ defined by $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} (0\leq x\leq \pi/\sqrt2)$ at $x = 1$. [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . Improve this question Explain for continuity the function $f$ defined by $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} (0\leq x\leq \pi/ \sqrt2)$ at $x = 1$ . Explain why the function $f$ does not vanish anywhere in $[0, \pi/\sqrt2]$ although $f(0)f(\pi/\sqrt2) <0$ . My attemt If $0 \leq x < 1$ then $x^n$ $\to$ 0 therefore $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} = e^x$ . If $ x > 1$ then $x^n \to ∞$ therefore $f(x) = \lim_{n\to \infty} \frac{e^x-x^n\sin(nx)}{1+x^n} = -\sin(nx)$ I don't know how to proceed further please help me","['limits', 'continuity']"
4866854,"Let G be a topological group that is T2,N2 and locally Euclidean and suppose G admits an open lie subgroup H. Prove that G is Lie","I found this exercise in Tao’s book “Note on Hilbert fifth problem” . Note that if $G$ is connected then $H=G$ since $H$ is also closed and $H\neq\emptyset$ since the neutral element lies in $H$ . But if $G$ is not connected I’m a little bit stuck. Tao suggest to take a coordinate chart $(U,\phi:U \to V)$ from the atlas on $H$ and translate it around to create an atlas on $G$ which makes $G$ a Lie group so i was trying to take the atlas $\{(gU,\psi_{g})\}_{g\in G}$ where $\psi_{g}:gU\to V , gu \mapsto \phi(u)$ for all $u$ in $U$ . Unfortunately i don’t know how to prove the compatibility of the charts. Sorry for the bad English I’m from Italy :)",['differential-geometry']
4866873,The triangle $ABC$ is right-angled in $A$. Prove that the inequality $(1-\sin B)(1-\sin C)\leq \frac{{(\sqrt{2}-1)}^2}{2}$ holds.,"The question The triangle $ABC$ is right-angled in $A$ . Prove that the inequality $(1-\sin B)(1-\sin C)\leq \frac{{(\sqrt{2}-1)}^2}{2}$ holds The idea Let's note the sides of the triangle as $a,b,c$ This means that according to the Pythagoras $a^2=b^2+c^2$ . I also tried replacing $\sin B =\frac{AC}{BC}$ and $\sin C= \frac{AB}{BC}$ , but got to nothing useful. I thought of using the sine rule...but again, I got to nothing... I hope one of you can help me! Thank you!","['trigonometry', 'inequality', 'geometry']"
4866894,"Can an infinite sum of non-computable numbers be computable, such that all finite sums of subsets of terms are non-computable?","Background In the following question , User1 asks whether an infinite sum of irrational numbers can be rational. Multiple answers 1 indicate the answer to this question is 'yes'. For instance, Rasmus Erlemann notes that $$\tan \frac{\pi}4=\sum_{n=0}^\infty \frac{(-1)^n 2^{2n+2}(2^{2n+2}-1)B_{2n+2}}{(2n+2)!}\left(\frac{\pi}4\right)^{2n+1}=1, $$ by the Maclaurin series of $\tan(\cdot)$ . The Question This question is asked in a similar spirit. I wonder whether there are any infinite series consisting solely of non-computable terms, that amount to a computable number. Examples of such non-computable numbers can found over here . They include Chaitin's constants . To make the question more precise, define $$ C = \sum_{k=1}^{\infty} u_{k} . $$ Here, all $u_{k}$ are non-computable numbers. I am looking for explicit examples of infinite series such that $C$ is a computable number, and all $u_{k}$ are both positive and linearly independent over $\mathbb{Q}$ . Added Although user TonyK answers my initial question correctly, I would like to add another restriction to make the question more interesting. I furthermore require that every sum of a finite subset of summands of the infinite series is also non-computable. This makes TonyK's example unworkable, since, for instance, the sum of the first two terms is computable: $$ \alpha u_{1} + (1-\alpha)u_{1} = u_{1}, $$ where $\alpha$ is non-computable and $u_{1}$ is a positive computable term. Notes [1] If you're interested in such series, I encourage you to also take a look at my recent answer . It involves a class of representations of numbers, called rational zeta series , that offers a fruitful way to approach this problem. For instance, the answer includes the series identity $$\sum_{n=1}^{\infty} \left[ \zeta(2n)-1 \right] = \frac{3}{4} . $$","['computability', 'examples-counterexamples', 'sequences-and-series']"
4866897,Ito integral independence of time increments,"When looking at an Ito integral are $\int_{0}^t f(s) dW_s $ and $\int_{t}^x f(s) dW_s $ independent under what conditions for $f$ . I was trying to solve the exercise  where I need to prove that: $\int_{0}^t f(s) dWs$ is a Gaussian random variable if we look at this as a stochastic process by varying $t$ . If we have independence of the time increments i can construct any collection of $\int_{0}^{t_k} f(s) dWs$ as a linear transformation of a vector with components $\int^{t_{k+1}}_{t_k} f(s) dWs$ and a linear transformation of a normally distributed vector is normally distributed. It is clear for me, that if $f$ is a deterministic elementary function that we have independece of time increments for the ito integral. I also know that I can write my Ito integral of the deterministic function as an ${L}^2$ limit of the ito integral of elementary functions. My proof would be complete, if I knew that independence of random variables carries over via $L^2$ limits. This feels wrong, but I am not sure. Thanks for the help!","['stochastic-integrals', 'probability-theory']"
4866908,Show that $\int_{-\infty}^\infty \frac{e^x}{e^{2x}+e^{2a}}\frac{1}{x^2+\pi^2}dx = \frac{2\pi e^{-a}}{4a^2+\pi^2}-\frac{1}{1+e^{2a}}$,"Show that \begin{align*}
\int_{-\infty}^\infty \frac{e^x}{e^{2x}+e^{2a}}\frac{1}{x^2+\pi^2}dx = \frac{2\pi e^{-a}}{4a^2+\pi^2}-\frac{1}{1+e^{2a}}
\end{align*} where $a\in \mathbb{R}$ . My SOLUTION Let $\displaystyle f(z) = \frac{e^z}{(e^{2z}+e^{2a})z}$ . Let $\Gamma$ be the positively oriented rectangle in the complex plane with vertices $R-i\pi$ , $R+i\pi$ , $-R+i\pi$ and $-R-i\pi$ where $R>|a|$ . There are three first order poles of $f(z)$ lying inside $\Gamma$ at $z= 0, \frac{i\pi}{2}+a$ and $-\frac{i\pi}{2}+a$ . Then, by the residue theorem, we have \begin{align*}
\int_\Gamma f(z)\; dz &= 2\pi i \left( \mathop{\text{Res}}_{z=0} \; f(z) +  \mathop{\text{Res}}_{z=\frac{i\pi}{2}+a} \; f(z) +  \mathop{\text{Res}}_{z=-\frac{i\pi}{2}+a} \; f(z)\right) \\
&= 2\pi i \left( \frac{1}{1+e^{2a}} - \frac{i e^{-a}}{2a+i\pi} + \frac{i e^{-a}}{2a-i\pi}\right) \\
&= 2\pi i \left( \frac{1}{1+e^{2a}} - \frac{2\pi e^{-a}}{4a^2 +\pi^2}\right) \tag{1} 
\end{align*} By change of variables, \begin{align*}
 \int_\Gamma f(z) \; dz &= \int_{-R-i\pi}^{R-i\pi} f(z)\; dz + \int_{R-i\pi}^{R+i\pi} f(z)\; dz + \int_{R+i\pi}^{-R+i\pi} f(z)\; dz + \int_{-R+i\pi}^{-R-i\pi} f(z)\; dz \\
&= \int_{-R}^{R} \left(f(x-i\pi)-f(x+i\pi) \right) dx + \int_{R-i\pi}^{R+i\pi} f(z)\; dz + \int_{-R+i\pi}^{-R-i\pi} f(z)\; dz \\
&= -2\pi i \int_{-R}^{R} \frac{e^x}{e^{2x}+e^{2a}}\frac{1}{x^2+\pi^2} dx + \int_{R-i\pi}^{R+i\pi} f(z)\; dz + \int_{-R+i\pi}^{-R-i\pi} f(z)\; dz \tag{2}
\end{align*} Note that \begin{align*}
\left| \int_{R-i\pi}^{R+i\pi} f(z)\; dz\right| &= \left|\int_{-\pi}^\pi f(R+iy)\; dy \right| \\
&\leq e^R \int_{-\pi}^\pi \frac{dy}{|iy+R|\cdot |e^{2iy+2R}+e^{2a}|}dy \\
&\leq \frac{e^{-R}}{R} \int_{-\pi}^\pi \frac{dy}{|e^{2iy}+e^{2a-2R}|} \\
&\leq \frac{e^{-R}}{R} \frac{2\pi}{|1-e^{2a-2R}|} \tag{3}
\end{align*} Similarly, \begin{align*}
\left|\int_{-R+i\pi}^{-R-i\pi} f(z)\; dz \right| \leq \frac{e^{-R}}{R}\frac{2\pi}{|e^{2a}-e^{-2R}|}\tag{4}
\end{align*} From equations (3) and (4), we see that the vertical integrals tend to 0 as $R\to \infty$ . Therefore, by equations (1) and (2): \begin{align*}
\int_{-\infty}^\infty \frac{e^x}{e^{2x}+e^{2a}}\frac{1}{x^2+\pi^2}dx &= \frac{2\pi e^{-a}}{4a^2+\pi^2}-\frac{1}{1+e^{2a}}
\end{align*}","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'closed-form']"
4866945,"Is a subgroup of $\operatorname{GL}(n,\mathbb{R})$ semialgebraic if and only if all its orbits are?","A subset $X \subseteq \mathbb{R}^n$ is called semialgebraic if it is of the form $$
X = \bigcup_{finite} \bigcap_{finite} \{ x \in \mathbb{R}^n \colon f_{i,j}(x) \star 0 \}
$$ where $\star$ represents any of the symbols $=, \leq , \geq, <, >$ and $f_{i,j} \in \mathbb{R}[X_{11}, \ldots , X_{nn}]$ . A subgroup $G < \operatorname{GL}(n,\mathbb{R}) \subseteq \mathbb{R}^{n\times n}$ is called a semialgebraic group if $G$ is a semialgebraic subset of $\mathbb{R}^{n\times n}$ . Is the following true? Conjecture: A subgroup $G < \operatorname{GL}(n,\mathbb{R})$ is semialgebraic if and only if for every $p \in \mathbb{R}^n$ , the orbit $G.p \subseteq \mathbb{R}^n$ is semialgebraic. One direction follows from the Tarski-Seidenberg transfer principle: If $G$ is semialgebraic, orbits $$
G.p = \{x \in \mathbb{R}^n \colon \exists g \in G , x=g.p\}
$$ are semialgebraic. What about the other direction?","['group-theory', 'algebraic-groups', 'semialgebraic-geometry']"
4866993,"How to evaluate double integral: $\iint \frac{y}{x} \, dx \, dy$ if it is in the first quadrant and is bounded by: $y=0$, $y=x$, and $x^2 + 4y^2 = 4$","I want to evaluate this double integral: $$
\iint \frac{y}{x} \, dx \, dy \quad
$$ which is bounded by functions: $$
y = 0 \quad
$$ $$
y = x \quad
$$ $$
x^2 + 4y^2 = 4 \quad
$$ And is in the first quadrant. The thing is that I don't quite get how to determine if the domain is x-simple, y-simple, or both exactly. I can determine it in easy graphs, but most of the time I'm stuck in there. After determining it, which variable should I take as constant and how should I place upper and lower bounds in the integral? These are the main things that stuck me on this problem. I would love any kind of help here and thank you in advance. We say that the domain D in the xy -plane is y-simple if it is bounded by two vertical lines x = a and x = b and two continuous graphs y = c(x) and y = d(x) between these lines. Lines parallel to the y-axis intersect a y-simple domain in an interval (possibly a single point) if at all. Similarly, D is x-simple if it is bounded by horizontal lines y = c and y = d and two continuous graphs x = a(y) and x = b(y) between these lines. Some of the domains are both x-simple and y-simple , for example, rectangles, triangles, and disks.","['integration', 'functions', 'analysis']"
4867023,Is it possible to adjust the sine function to pass through specific points?,"I have a set of points $\left\{ \left(−\frac{6}{4}, 0\right), \left(−\frac{4}{4}, −\frac{\sqrt{6}}{3}\right), \left(−\frac{3}{4}, −1\right), \left(−\frac{2}{4}, −\frac{\sqrt{6}}{3}\right), \left(\frac{0}{4}, 0\right), \left(\frac{2}{4}, \frac{\sqrt{6}}{3}\right), \left(\frac{3}{4}, 1\right), \left(\frac{4}{4}, \frac{\sqrt{6}}{3}\right), \left(\frac{6}{4}, 0\right) \right\}$ . I want to find a sinusoidal-esque wave that fits these points. The function $f(x) = \sin\left(\frac{2 π}{3} x\right)$ has the correct period/wavelength and amplitude, but is too wide at the wave peaks/troughs. Is there a simple way to modify the sine function to be a little less steep at the midpoints between extrema and a little narrower at the extrema? As an additional constraint, I'd like the derivative of $f(x)$ to not have extra twists/peaks/troughs in it. I believe a more mathematical way of stating that goal is that the second derivative should have no more zeros than the function itself.",['trigonometry']
4867094,When is the Cayley embedding for infinite groups optimal?,"By Cayley's theorem, any group $G$ naturally injects into the symmetric group $\mathrm{Sym}(G)$ of its underlying set via the Cayley embedding. Let's say this embedding is optimal for $G$ , if there is no embeding $G \hookrightarrow \mathrm{Sym}(I)$ for some smaller index set $I$ . The finite groups with this property are completely understood , they are just the primary cyclic groups, the generalized quaternion groups, or $C_2\times C_2$ . For infinite groups, this phenomenon is a lot more common: In fact, if $|G|$ is a strong limit cardinal (that is, if $\lambda<|G|$ implies $2^\lambda<|G|$ ), it can have no smaller faithful permutation representation (as $|\mathrm{Sym}(\lambda)|=2^\lambda$ for any infinite $\lambda$ ). But if $|G|$ is not a strong limit, I do not know what happens in general. Are there any groups of these sizes with no smaller embeddings? Is there even a characterization of them? Some thoughts: With a bit of work, I was able to show that any infinite abelian group $A$ with $|A|$ not a strong limit always has a more efficient embedding. I will reproduce the proof in case it could lead to any insights into the general case: Suppose $A$ is such a group. Fix some $\lambda<|A|$ for which we have $|A|\leq 2^\lambda$ . We will show that $A$ then embeds into $\mathrm{Sym}(\lambda)$ . To that end, we will show that there is an embedding $\iota: A \hookrightarrow \left(\mathbb{Q}/\mathbb{Z}\times \mathbb{Q}\right)^\lambda$ . This will suffice: The subgroups $$A_\alpha= \ker(\pi_\alpha\circ \iota) $$ (where $\pi_\alpha$ is the projection onto the $\alpha$ -th coordinate) are each of (at most) countable index in $A$ and altogether intersect trivially (both clear by definition). Hence, the action of $A$ on the disjoint union $\coprod_{\alpha<\lambda}A/A_\alpha$ is faithful on a set of cardinality $\lambda\times \omega=\lambda$ . It thus remains to construct this embedding $\iota$ . First, we deal with the $\mathbb{Q}/\mathbb{Z}$ -part: Fix any prime $p$ and consider the $\mathbb{F}_p$ -vector space $V_p$ of the $p$ -torsion elements. Since $|V_p|\leq |A|\leq 2^\lambda$ , we have an embedding $V_p \hookrightarrow \mathbb{F}_p^\lambda$ which further embeds into $\left(\mathbb{Q}/\mathbb{Z}\right)^\lambda$ (just coordinatewise). Since this latter group is injective, this embedding extends to a homomorphism $\iota_p:A\to \left(\mathbb{Q}/\mathbb{Z}\right)^\lambda$ which is then injective on the $p$ -component of $A$ . If we put all these homomorphisms for each prime $p$ together, we obtain a map $$\iota_T:A\to \left(\mathbb{Q}/\mathbb{Z}\right)^{\lambda\times \omega}\simeq \left( \mathbb{Q}/\mathbb{Z} \right)^{\lambda}$$ that is injective on the torsion subgroup $T$ of $A$ . Finally, the torsion free quotient $A/T$ likewise embeds into $\mathbb{Q}^\lambda$ , which we can then amalgamate with $\iota_T$ into a map into $\left(\mathbb{Q}/\mathbb{Z}\times \mathbb{Q} \right)^\lambda$ which is then finally guaranteed to be injective.","['symmetric-groups', 'group-theory', 'set-theory']"
4867113,Limiting behaviour of state vector under delayed (open-loop) inputs,"Given a discrete-time LTI system as $$x_{i+1} = A x_{i} + B u_{i}$$ and suppose the feedback law $u_{i} = \mathcal{K}(x_{i})$ assymtotically stabilizes the closed-loop system. Now, consider the modified (i.e. delayed) input law $u_{i} = \tilde{\mathcal{K}}(x_{i-I})$ for some constant (but possibly unknown) $I \gg 1$ (e.g. 10). Given an initial condition $x_{0}$ , what can I see about whether there exists some $i$ such that $\Vert x_{i} \Vert \geq \rho $ ? Does the state norm grow beyond some bound, given enough time? And does this depend on $A$ being (Schur) stable? My intuition (and simulation) says that the state trajectory will eventually leave any finite set (regardless of $A$ being Schur ), as we are basically injecting an arbitrary input into the system, which is not related to the current state. Thus, the system is running in open loop. However, how do I prove this, or at least make this more mathematically rigorous? Any help would be appreciated! P.S. This is related to replay attacks in control systems which I'm investigating.","['limits', 'convergence-divergence', 'linear-control', 'control-theory']"
4867170,Definition of a one point subscheme and fibre at a point,"The fibre of an $S$ -scheme $X$ over $s\in S$ is defined as the fibre product $X\times_S \{s\}$ where the one point subscheme $\{s\}$ is given a local ring $k(s)$ equal to the residue field at $s$ . This makes $\{s\}$ into an $S$ -scheme $Spec(k(s))$ . My question is: is there any special reason for choosing this particular structure $(s,k(s))$ ? Are there any other local rings we could attach to $\{s\}$ to make it into a scheme and so that the fibre product makes sense? I get that there is a morphism of schemes $Spec(k(s))\rightarrow S$ which is induced by a homomorphism of rings which is just reduction at the maximal ideal of the local ring $\mathcal{O}_{S,s}\rightarrow \mathcal{O}_{S,s}/\mathcal{M}_{S,s}$ , at least in the affine case. In this way I suppose one could  think of the resulting $S$ -scheme $Spec(k(s))$ as the most natural one.","['algebraic-geometry', 'schemes']"
4867248,Integral from MIT Integration Bee 2024 $\int_{-\infty}^{\infty} \frac{1}{x^4+x^3+x^2+x+1} {d}x$,"Good evening, I was interested in the third integral from the finals of the MIT Integration Bee 2024 : $$I = \int_{-\infty}^{\infty} \frac{1}{x^4+x^3+x^2+x+1} \hspace{0.1cm} \mathrm{d}x$$ One way to solve this is to identify that the denominator is the fifth cyclotomic polynomial, so we can write : $$I = \displaystyle\int_{-\infty}^{\infty} \frac{1-x}{(1-x)(x^4+x^3+x^2+x+1)} \hspace{0.1cm} \mathrm{d}x = \displaystyle\int_{-\infty}^{\infty} \frac{1-x}{1-x^5} \hspace{0.1cm} \mathrm{d}x$$ Then, by Chasles : $$I = \displaystyle\int_{-\infty}^{0} \frac{1-x}{1-x^5} \hspace{0.1cm} \mathrm{d}x + \displaystyle\int_{0}^{\infty} \frac{1-x}{1-x^5} \hspace{0.1cm} \mathrm{d}x$$ Let $x\to -x$ in the first integral, we obtain : $$I = \displaystyle\int_{0}^{\infty} \frac{1+x}{1+x^5} \hspace{0.1cm} \mathrm{d}x + \displaystyle\int_{0}^{\infty} \frac{1-x}{1-x^5} \hspace{0.1cm} \mathrm{d}x$$ And we use these two identities : $$ \displaystyle\int_{0}^{\infty} \frac{x^{s-1}}{1+x^k} \hspace{0.1cm} \mathrm{d}x = \frac{\pi}{k} \csc\left( \frac{\pi s}{k} \right)$$ $$ \mathrm{PV} \displaystyle\int_{0}^{\infty} \frac{x^{s-1}}{1-x^k} \hspace{0.1cm} \mathrm{d}x = \frac{\pi}{k} \cot\left( \frac{\pi s}{k} \right)$$ As a consequence : $$ I = \frac{\pi}{5} \left( \csc\left( \frac{\pi}{5} \right) + \csc\left( \frac{2\pi}{5} \right) + \cot\left( \frac{\pi}{5} \right) - \cot\left( \frac{2\pi}{5} \right)\right) = \frac{\pi}{5} \sqrt{10 + 2 \sqrt{5}}\approx 2.39026573179$$ I have several questions : 1) I have never seen these two identities, apparently it comes from beta/gamma functions, does anyone have a reference with a proof ? 2) Is it possible to take another path ? With a contour integral for example ? I'm curious.",['integration']
4867310,Is a map that preserves torsion and curvature of all smooth curves an isometry?,"In my introductory differential geometry class, we learnt that isometries preserve the torsion $\tau$ and curvature $\kappa$ of curves. I was wondering if the converse of this statement is true: if a map $F$ preserves $\kappa$ and $\tau$ for all curves, is $F$ an isometry? Thanks for any help.","['manifolds', 'isometry', 'differential-geometry']"
4867316,Some questions on derived pull-back and push-forward functors of proper birational morphism of Noetherian quasi-separated schemes,"Let $f: X \to Y$ be a proper birational morphism of Noetherian quasi-separated schemes. We have the derived pull-back $Lf^*: D(QCoh(Y))\to D(QCoh(X))$ ( https://stacks.math.columbia.edu/tag/06YI ) and similarly the derived push-forward $Rf_*: D(QCoh(X))\to D(QCoh(Y))$ functor. I have the following questions: (1) Is $Rf_*$ a right-adjoint to $Lf^*$ ? (2) Is $Lf^* \mathcal O_Y\cong O_X$ ? (3) Does $Rf_*$ map $D(Coh(X))$ to $D(Coh(Y))$ and $D^b(Coh(X))$ to $D^b(Coh(Y))$ ? (4) Does $Lf^*$ map $D(Coh(Y))$ to $D(Coh(X))$ and $D^b(Coh(Y))$ to $D^b(Coh(X))$ ? Perhaps all these are standard and well-known, in which case, I would highly appreciate some references as well. Thank you.","['derived-categories', 'reference-request', 'algebraic-geometry', 'adjoint-functors', 'schemes']"
4867338,Can a triangle up to isometry shatter seven points?,"From Wikipedia : The class [of sets] $C$ shatters the set $A$ if for each subset $a$ of $A$ , there is some element $c \in C$ such that $a = c\cap A$ . In other words, $C$ shatters $A$ if for every subset $S\subset A$ , it's possible to find  a set $c\in C$ which covers the points in $S$ but none of the points in $A\setminus S$ . For instance, if $C$ is the set of closed half-planes, then $C$ shatters any non-colinear set of three points, but does not shatter a unit square, because it isn't possible to cover one diagonal while omitting the other. Given a triangle $T$ , let $C$ be the set of all isometric copies of $T$ - all translations, rotations, and reflections of $T$ in the plane, not permitting scaling. What is the maximum size of a set $A$ such that for some triangle $T$ , $C(T)$ shatters $A$ ? It's possible to attain $|A|=6$ by taking $A$ to be the vertices of a regular hexagon of unit side length and $T$ to be an isosceles triangle with side lengths $(2.8,4.48797,4.48797)$ . Below are diagrams of every configuration up to dihedral symmetry: On the other hand, it's impossible to have $|A|=8$ . Note that every point of $A$ must lie on a vertex of its convex hull, or else taking $S$ to be the convex hull would prove impossible (since a triangle is convex). So $A$ must be the vertices of a convex polygon. But then take $S$ to be an alternating set of four points around the perimeter of $A$ : this forces a side of our triangle to lie in four different places, none of which are compatible, which is a contradiction since we only have three sides available. (To see that the side restrictions are incompatible, observe that the permissible oriented angles enforced by each region are disjoint intervals.) This only leaves the case $|A|=7$ . Is it possible? I've tried some approaches where $A$ is a regular heptagon, and wasn't able to find something that worked, but I'm not confident I've ruled out even that case (I think an isosceles $T$ at least is not possible with such an $A$ ).","['triangles', 'geometry']"
4867342,"Existence of $1/(e-1)$ value of a continous and differentiable function $f:[0,1]\to \mathbb R$ given $f(1)=1, f(0)=0$","Let $f:[0,1]\to \mathbb R$ be a continous function on $[0,1]$ and differentiable on $(0,1)$ , $f(0)=0, f(1)=1$ . Prove that there exists $c\in (0,1)$ so that $f(c)+\frac{1}{e-1}=f'(c)$ where $e$ is the Euler number. I tried to subs $f(x)=e^{x}g(x)$ so that $f'(x)-f(x)=e^{x}g'(x)$ and define $e^{x}g'(x)=w(x)$ . Then I tried to use the mean value theorem (to ensure the existence of $r \in (0,1)$ s.t. $f'(r)-f(r)=e^{r-1}$ ) however then I have no idea how to continue. My guess is we use intermediate value theorem on $w(x)$ however I don't have any idea on how to do it. Is there any construction to solve this?","['analysis', 'real-analysis', 'calculus', 'functions', 'mean-value-theorem']"
4867398,If $f$ is measurable and $f=g$ almost everywhere is $g$ measurable,"We were told in lecture, that if two functions $f,g$ are equal almost everywhere. Meaning that for a measure space $(X,A, \mu)$ sucht that the set $\{x \in X | g(x) \neq f(x)\}$ is a measurable nullset, then if $f$ is measurable so is $g$ . I understand that this holds for complete measures. But I am stuck showing it for arbitrary measures.
Any help would be appreciated.","['measure-theory', 'lebesgue-measure']"
4867402,Did my TA make a mistake regarding probabilities?,"So I had an assignment due yesterday, regarding the probability of the attackers second highest dice roll, being strictly bigger than the defenders second highest dice roll, in the game Risk. In the game, the attacker has three dices and the defender has two. The highest dice of the attacker is compared with the highest dice of the defender, and the second-highest dice of the attacker is compared with the second highest dice of the defender. For the attacker to win, their dice must be strictly larger than the defender. The dice of the defender are $C_1$ and $C_2$ , ordered as $C_{(1)} \leq C_{(2)}$ , and the dice of the attacker are $B_1$ , $B_2$ and $B_3$ , ordered as $B_{(1)} \leq B_{(2)} \leq B_{(3)}$ I, with the help of this board wrote my answer as P(A>D) = $B_{(2)}>C_{(1)}$ : $$\sum_{k=2}^{6} \left(\sum_{n=1}^{k-1}\frac{13-2n}{36}\right) \cdot \frac{-6k^2+42k-20}{216} =\frac{1181}{1944} =0.6075 $$ Yet my TA had another answer.
We both had the same probability for $B_{(2)}$ , ie. $P(B_{(2)}=k) = \frac{-6^2+42k-20}{216}$ but he used the distribution function for $B_{(2)},  P(B_{(2)}$ ≤k) $ = \frac{9k^2-k^3}{108}$ , instead of the probability function for $C_{(1)}$ which I used. He then multiplied the possibilities and took the sum, and got close to the same answer, but not the exact. $$\sum_{k=1}^{6} \frac{21k-3k^2-10}{108} \cdot \frac{9k^2-k^3}{108} = 0.598 $$ My question is, which of these are the correct way to find the probability of the attackers second dice being strictly larger than the defenders second dice roll?",['probability']
4867492,Doubt about sampling without replacement,"A problem consists of five numbers $2,3,6,8 and 11$ . Consider all possible sample space of size 2 that can be drawn with and without replacement from this population then find
(a) mean of the population
(b) the std deviation of the population
(c) the mean of  the sampling distributon of means I was dealing with this problem.. Here for  possible sample with replacement of size 2, they have considered both {4,3} and {3,4}.. My question is, aren't they both same sample set? If I believe in first set we got '4' first and then '3' nd. And in second set they got '3' first and then '4'... in that sense both sets has to be different so different sample space.But in that case, for  sampling without replacement also they should have considered {4,3} and {3,4} both...like for first sample they got '4' first and then '3' and for the second sample they got '3' first and then '4'. Please clear my this doubt... May be  I am wrong with my definitions of with and without replacement... for me with replacement means after selecting one element for the sample you have put it again in the population set to choose the other, so there is chance of getting the same element again...so repetition is allowed in samples...But this is not the case with without replacement thanks in advance... I dont know why I am  not getting answer, may be I used some pic instead of writing... so I written the question myself and reposting","['statistics', 'sampling-theory']"
4867681,If $g$ is contraction and $f+f'=g(f)$ then f(t) has a limit as $t\to\infty$,"I would like to prove the following statement: Let $g:\mathbb{R}^n\to\mathbb{R}^n$ be contraction mapping. If function $f:\mathbb{R}\to\mathbb{R}^n$ satisfies: $$f(t)+f'(t)=g(f(t))$$ then the limit $\lim_{t\to\infty}f(t)$ exists. Below I write a sketch of my solution to this problem. My question is: is there some simpler way to prove the theorem and is my proof correct? Let $L<1$ be Lipschitz constant of $g$ . It's not hard to show that if $\alpha\in\mathbb{R}^n$ is a fixed point of $g$ then $|f(t)-\alpha|\leq\frac{1}{1-L}|f'(t)|$ so it's enough to show that $|f'(t)|\to 0$ . With some elementary calculation it can be proven that if $a,b\in\mathbb{R}^n$ satisfy $|a+b|\leq L|a|$ then $$\langle a,b\rangle\leq \frac{c}{2}|a|^2$$ where $c=\frac{L-1}{2-L}<0$ .
Let $h\neq 0$ . Substituting $a=f(t+h)-f(t)$ , $b=f'(t+h)-f'(t)$ in the formula above we get: $$\langle f(t+h)-f(t),f'(t+h)-f'(t)\rangle\leq \frac{c}{2}|f(t+h)-f(t)|^2$$ Since $\frac{d}{dt}|f(t+h)-f(t)|^2=2\langle f(t+h)-f(t),f'(t+h)-f'(t)\rangle$ we obtain: $$\frac{d}{dt}|f(t+h)-f(t)|^2\leq c|f(t+h)-f(t)|^2$$ $$\frac{d}{dt}\ln\left(|f(t+h)-f(t)|^2\right)\leq c$$ Integrating from t to $0$ we get: $$\ln\left(|f(t+h)-f(t)|^2\right)-\ln\left(|f(h)-f(0)|^2\right)\leq ct$$ $$|f(t+h)-f(t)|^2\leq e^{ct}|f(h)-f(0)|^2$$ $$\left|\frac{f(t+h)-f(t)}{h}\right|^2\leq e^{ct}\left|\frac{f(h)-f(0)}{h}\right|^2$$ Since $h$ was arbitrary we can take $h\to 0$ obtaining: $$\left|f'(t)\right|^2\leq e^{ct}\left|f'(0)\right|^2$$ Having $c<0$ we see that $\lim_{t\to \infty}e^{ct}=0$ which implies that $\lim_{t\to \infty}|f'(t)|^2=0$ and this is what we wanted to prove.","['contraction-mapping', 'multivariable-calculus', 'solution-verification', 'ordinary-differential-equations']"
4867697,Solve functional equation $f(x+y) + f(x-y) = 2f(x)f(y)$,"The statement of the problem : Let $f: \mathbb R \rightarrow \mathbb R $ with the 2 properties : $f(x+y) + f(x-y) = 2f(x)f(y) ,   \forall x , y \in \mathbb R$ There exists a ""the smallest"" strictly positive number a , with the property that f(a) is the maximum of the function and f(a)>0 . Prove that f is a periodic function with period a . My approach : For $x = y = 0 \implies 2f(0) = 2f(0)^2$ so $f(0)\in\{0,1\}$ ; If $f(0) = 0 \implies 2f(x)=0 \implies f(x)=0 , \forall x \in \mathbb R$ , so it's clearly periodic.( EDIT actually it is false because if the function was 0 then it would not correspond with 2) so f(0) is necessarily 1). Now if $f(0) = 1$ , for $x = 0$ we get $f(y) + f(-y) = 2f(y) \implies f(y)=f(-y) , \forall x \in \mathbb R $ . So it is enough to determine $f$ on the interval $[0,\infty)$ . EDIT : so I managed to prove that f(a)=1 and that the final answer might be something related to a trigonometric function , probably cosine , since $cos(-x)=cos(x)$ as above.I don't really have any ideas about how I should continue, but I'm waiting for suggestions or solutions. Any and all proofs will be helpful. Thanks a lot!","['functional-equations', 'functions']"
4867745,Show $nx_n \to 1$ for sequence defined by $x_{n+1}=\frac{x_n}{1+nx_n^2}$,"Let $a>0$ . Consider the sequence $x_{n+1}=\frac{x_n}{1+nx_n^2},~x_1=a,~n\in\mathbb{N}^*$ . Study the convergence of $(x_n)_{n\ge1}$ and $(nx_n)_{n\ge1}$ . It was easy for me to prove that $(x_n)_n$ is convergent with the limit equal to $0$ , but couldn't find a straightforward approach for $(nx_n)_n$ . I first proved that $x_n\le\frac{1}{n}$ , or equivalently, $nx_n\le1$ , for $n\ge2$ , using induction. Then I showed that $(nx_n)_n$ is a nondecreasing sequence: $$\frac{(n+1)x_{n+1}}{nx_n}=\frac{n+1}{n+n^2x_n^2}\ge 1$$ Thus $(nx_n)_n$ is nondecreasing and upper bounded, so it is convergent with positive finite limit $l$ . Now consider $a_n=n$ and $b_n=\frac{1}{x_n}$ , so $(b_n)_n$ is a positive, increasing sequence with infinite limit. We have that $$\frac{a_{n+1}-a_n}{b_{n+1}-b_n}=\bigg(\frac{1}{x_{n+1}}-\frac{1}{x_n}\bigg)^{-1}=\frac{1}{nx_n}\to\frac{1}{l}$$ By the Stolz–Cesàro theorem, we obtain the identity $l=\frac{1}{l}$ , so the only possible value for $l$ is $l=1$ . The hardest part was to prove the convergence of $(nx_n)_n$ , which took me a long time. I feel like I am missing an easier solution. Please let me know if there's an elegant proof for $nx_n\to1$ . Thanks in advance.","['limits', 'convergence-divergence', 'recurrence-relations', 'sequences-and-series']"
4867755,Calculate Riemann integral,"Calculate Riemann integral: $$\int_{\frac{1}{3}}^{3} \frac{\arctan(x)}{x^2-x+1}dx $$ In this assignment, the integrand function does not have an antiderivative.  I used the substitution of $x=\frac{1}{t}$ and the fact that $\arctan(x) +\arctan \left( \frac{1}{x} \right) = \frac{\pi}{2} (x \neq 0, x \in \mathbb{R})$ and reduced the integral from the task to the following form: $$
\int_{\frac{1}{3}}^{3} \frac{\arctan(x)}{x^2-x+1}dx = \int_{3}^{\frac{1}{3}} \frac{\arctan(\frac{1}{t})}{\frac{1}{t^2}-\frac{1}{t}+1} \left(-\frac{1}{t^2} \right)
dt
= \frac{\pi}{2} \cdot \int_{3}^{\frac{1}{3}} \frac{1}{\frac{1}{t^2}-\frac{1}{t}+1} \left(-\frac{1}{t^2} \right)
dt - 
\int_{3}^{\frac{1}{3}} \frac{\arctan(t)}{\frac{1}{t^2}-\frac{1}{t}+1} \left(-\frac{1}{t^2} \right)
dt=
$$ $$
= \frac{\pi}{2} \cdot \int_{3}^{\frac{1}{3}} \frac{1}{\frac{1}{t^2}-\frac{1}{t}+1} \left(-\frac{1}{t^2} \right)
dt - \int_{\frac{1}{3}}^{3} \frac{\arctan(\frac{1}{x})}{x^2-x+1}dx \ \left( * \right)
$$ I'm stuck on this. I calculated this integral on a calculator in Wolframalpha and it gives out that the value of the integral $$\int_{\frac{1}{3}}^{3} \frac{\arctan(x)}{x^2-x+1}dx 
= \frac{\pi}{4} \cdot \int_{3}^{\frac{1}{3}} \frac{1}{\frac{1}{t^2}-\frac{1}{t}+1} \left(-\frac{1}{t^2} \right)
dt
= \frac{\pi}{4} \cdot \int_{\frac{1}{3}}^{3} \frac{1}{x^2-x+1} dx
$$ But I don't understand how to get this result from the step $\left( * \right)$ where I left off. I will be glad if you tell me what to do after $\left( * \right)$ or suggest another way to calculate the integral.","['integration', 'calculus', 'riemann-integration']"
4867775,Harmonic oscillator differential equation question,"Consider a harmonic oscillator subject to a frictional force proportional to velocity: $$\ddot{x}+2\gamma\dot{x}+\omega^2x=0.$$ Here $\dot{x}$ and $\ddot{x}$ are $\frac{dx}{dt}$ and $\frac{d^2x}{dt^2}.$ Assume $\omega>\gamma.$ Interpreting this equation as a system of equations for two functions $x\left ( t\right ) $ and $\nu\left ( t\right ) = \dot{x} \left ( t\right ) $ , solve this system of equations and show that the solutions are: $$\begin{gathered}
x\left(t\right)=\mathrm{e}^{-\gamma t}\left[x_0\cos\left(\Omega t\right)+x_0\frac\gamma\Omega\sin\left(\Omega t\right)+\nu_0\frac1\Omega\sin\left(\Omega t\right)\right]; \\
\nu\left(t\right)=\mathrm{e}^{-\gamma t}\left[-x_{0}\frac{\omega^{2}}\Omega\sin\left(\Omega t\right)+\nu_{0}\cos\left(\Omega t\right)-\nu_{0}\frac\gamma\Omega\sin\left(\Omega t\right)\right]. 
\end{gathered}$$ Here, $x_0= x\left ( 0\right ) $ and $\nu_0= \nu\left ( 0\right ) $ are initial conditions. What is the frequency $\Omega$ of these oscillations? My attempt: Let's introduce a new variable $v(t) = \dot{x}(t)$ so that we have $$\begin{cases}\dot{x}&=v\\\dot{v}&=-2\gamma v-\omega^2x&\end{cases}$$ Now, we have a system of first-order ordinary differential equations. We can rewrite this in matrix form as: $$\frac d{dt}\begin{bmatrix}x\\v\end{bmatrix}=\begin{bmatrix}0\times x+1\times v\\-\omega^2\times x+(-2\gamma)\times v\end{bmatrix}=\begin{bmatrix}0&1\\-\omega^2&-2\gamma\end{bmatrix}\begin{bmatrix}x\\v\end{bmatrix}$$ The solution to this system can be found by diagonalizing the coefficient matrix. Let's denote the coefficient matrix by $A$ : $$A=\begin{bmatrix}0&1\\-\omega^2&-2\gamma\end{bmatrix}$$ The eigenvalues $\lambda$ and eigenvectors $\mathbf{v}$ of $A$ satisfy the equation: $\det(A-\lambda I)=0$ , so we have $$\det\begin{pmatrix}-\lambda&1\\-\omega^2&-2\gamma-\lambda\end{pmatrix}=0 \implies\lambda=-\gamma\pm\sqrt{\gamma^2-\omega^2}$$ Since $\omega > \gamma$ , the term under the square root is negative, resulting in complex conjugate eigenvalues. Let's denote $\Omega = \sqrt{\omega^2 - \gamma^2}$ , so the eigenvalues become: $$\lambda=-\gamma\pm i\Omega.$$ Now, we can write down the general solution using these eigenvalues. The general solution has the form: $$\begin{bmatrix}x(t)\\v(t)\end{bmatrix}=c_1\begin{bmatrix}1\\-\gamma+i\Omega\end{bmatrix}e^{(-\gamma+i\Omega)t}+c_2\begin{bmatrix}1\\-\gamma-i\Omega\end{bmatrix}e^{(-\gamma-i\Omega)t}$$ $$\begin{aligned}
&x\left(t\right) =c_1e^{(-\gamma+i\Omega)t}+c_2e^{(-\gamma-i\Omega)t}  \\
&v(t) =(-\gamma+i\Omega)c_1e^{(-\gamma+i\Omega)t}+(-\gamma-i\Omega)c_2e^{(-\gamma-i\Omega)t} 
\end{aligned}$$ Then, finding the constants $c_1$ and $c_2$ in terms of $x_0$ and $v_0$ I get $$\frac{x_0 i \Omega+v_0+\gamma x_0}{2i \Omega}=c_1$$ and $$-\frac{v_0+\gamma x_0-i \Omega x_0}{2i \Omega}=c_2,$$ if I haven't made any errors. Thus plugging in and applying Euler's formula, $$\begin{gathered}
x(t)=\left(\frac{x_0i\Omega+v_0+\gamma x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)+i\sin(\Omega t)) \\
+\left(-\frac{v_0+\gamma x_0-i\Omega x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)-i\sin(\Omega t)) 
\end{gathered}$$ and $$\begin{gathered}
\begin{aligned}v(t)=(-\gamma+i\Omega)\left(\frac{x_0i\Omega+v_0+\gamma x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)+i\sin(\Omega t))\end{aligned} \\
+(-\gamma-i\Omega)\left(-\frac{v_0+\gamma x_0-i\Omega x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)-i\sin(\Omega t)) 
\end{gathered}$$ But I am having trouble simplifying it to get to the solutions as written. Btw, am I correct in assuming $\Omega$ as frequency in this physical context? Furthermore, how do we calculate the frequency $Ω$ of these oscillations?","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'classical-mechanics', 'solution-verification', 'physics']"
4867789,Show that the general solution to $y^{\prime\prime}-4xy^{\prime}+\left(4x^2-2\right)y=0$ is $y\left(x\right)=C_1\mathrm{e}^{x^2}+C_2x\mathrm{e}^{x^2}$,"By solving the differential equation $$y^{\prime\prime}-4xy^{\prime}+\left(4x^2-2\right)y=0$$ using the series method, demonstrate that its general solution is $$y\left(x\right)=C_1\mathrm{e}^{x^2}+C_2x\mathrm{e}^{x^2}$$ Here, $C_1$ and $C_2$ are arbitrary constants of integration. My attempt and hint : I've been given a hint that by obtaining recursive relationships for the coefficients of the series, you can calculate several initial terms, and then ""guessing"" the expression for the $n$ -th coefficient (since you know what you should get) you can verify whether it holds true in all cases. I am not really familiar with this method and would appreciate your help.","['ordinary-differential-equations', 'sequences-and-series']"
4867810,Probability of rain after an amount of time in Minecraft game,"Minecraft time is measured in ticks. When a world is loaded, the game waits anywhere from 12,000 to 180,000 ticks to start raining. After it starts raining, the rain lasts anywhere from 12,000 to 24,000 ticks. When the rain ends, clear weather lasts for another 12,000 to 180,000 ticks, etc. This cycle repeats forever. I want to know if there is a function that can determine the chance that it's raining on tick t . From ticks 0 to 12,000 the chance is 0%. From ticks 12,000 to 24,000 the chance is (t - 12,000) / 168,000 (I believe) After this, I got confused. What would the probabilities be past this point? I created a simulation to brute force it and this is what the graph looked like (x is ticks in thousands, y is the percentage chance of rain). But I'm not actually sure what the function for this would be. Thank you.",['probability']
4867875,Find all functions $f: \mathbb R \to \mathbb R$ satisfying $f(xf(y) + f(x+y)) = y(f(x)+1)+f(x)$,"Can you tell me if my solution is so far correct and help me complete the second case? The statement of the problem : Find all functions $f: \mathbb R \to \mathbb R$ with the following property: $$
f(xf(y) + f(x+y)) = y(f(x)+1) + f(x) , \forall x,y \in \mathbb R.
$$ My approach : For $x = y = 0$ we get $$
f(f(0)) = f(0).
$$ For $x = 0$ we get $$
f(f(y))=y(f(0)+1)+f(0) , \forall y \in \mathbb R.
$$ Now the function $y \mapsto ay+c$ is bijective , where $a$ and $c$ real constants and $a \neq 0$ . So , for $f(0) \neq -1$ we get $$
\begin{align}
&y \mapsto y(f(0)+1)+f(0) \text{ is bijective,}\\ 
\implies &y \mapsto f(f(y)) \text{ is bijective,}\\ 
\implies &f \text{ is injective and $f$ is surjective,}\\
\implies &f \text{ is bijective}.
\end{align}
$$ Making $y = 0$ in the main equation we get $$f(f(x))=f(x) ,\forall x \in \mathbb R. 
$$ And since $f$ is bijective we obtain that $$f(x)=x ,\forall x \in \mathbb R,
$$ which satisfies the initial equation. Now we have left the case where $y \mapsto y(f(0)+1)+f(0)$ is not bijective , more precisely when $$
\begin{align}
f(0) &= -1 \\
\implies f(f(y)) &= f(0) = -1 ,\forall y \in \mathbb R. 
\end{align}
$$ Making $y=0$ we get $$f(-x+f(x))=f(x),\forall x \in \mathbb R. 
$$ From here I didn't know what to do. I think that we either have to show that $f \equiv -1$ or reach a contradiction. I hope that at least the first part is correct. I am open to all your suggestions and solutions. Thank you!","['functional-equations', 'functions']"
4867910,Why do so many solutions to $n+1\mid3^n+1$ satisfy $n\equiv27\pmod{72}$?,"Here are the first $40$ integers for which $\dfrac{3^n+1}{n+1}$ is an integer. I've denoted their residue mod $72$ by color and a symbol. $$\begin{array}{r}\dagger\,\color{violet}0,&\bullet\,\color{brown}1,&*\,\color{red}3,&27,&531,\\
1\,035,&4\,635,&6\,363,&11\,475,&19\,683,\\
4\,131,&80\,955,&*\,\color{red}{266\,475},&280\,755,&307\,395,\\
356\,643,&\circ\,\color{blue}{490\,371},&544\,347,&557\,955,&565\,515,\\
572\,715,&808\,227,&1\,256\,355,&1\,695\,483,&1\,959\,075,\\
1\,995\,075,&2\,771\,595,&2\,837\,835,&3\,004\,155,&3\,208\,491,\\
*\,\color{red}{3\,337\,635},&3\,886\,443,&4\,670\,955,&5\,619\,411,&6\,434\,595,\\
\bullet\,\color{brown}{6\,942\,817},&*\,\color{red}{7\,631\,715},&*\,\color{red}{9\,274\,755},&9\,436\,923,&9\,586\,107,\end{array}$$ $\dagger\,\color{violet}{\rm Violet}$ means $0$ mod $72$ , $\circ\,\color{blue}{\rm blue}$ means $51$ mod $72$ , $\bullet\,\color{brown}{\rm brown}$ means $1$ mod $72$ , $*\,\color{red}{\rm red}$ means $3$ mod $72$ . All the rest, ${\rm black}$ , are $27$ mod $72$ . Why are so many solutions ( $31$ of the first $40$ ) equivalent to $27$ mod $72$ ? It can't be a coincidence that (beyond $n=3$ ) the counterexamples are so few and take so long to show up. Some other observations. Of the first $40$ solutions: $32$ of them are multiples of $9$ . $37$ of them are $3$ mod $8$ . $38$ of them are multiples of $3$ . I was honestly ready to conjecture that (other than $1$ ) all solutions were multiples of $3$ , until $6\,942\,817$ showed up. Why is the first non-multiple of $3$ so large?? That same number is also the first one after $1$ that's not $3$ mod $8$ . What's going on here? Python code: for n in range(10_000_000):
  if (pow(3,n,n+1)==n):
    print(n) EDIT: Here's a graph of ""What percentage of the first $x$ nonzero values are $27$ mod $72$ ?"" as $x$ ranges from $1$ to $213$ : https://www.desmos.com/calculator/o1agwn8feo (Thanks to Zubin Mukerjee for the list of the first 213 nonzero values) The sequence is now on OEIS. https://oeis.org/A370578","['number-theory', 'modular-arithmetic', 'divisibility']"
4867977,Convergence in Distribution of a Particular Sample Average,"Suppose $g_{n}(\cdot)$ defined on $[0,1]$ converges in distribution to a continuous Gaussian process. Let $U_{1},...,U_{n}$ be i.i.d. random variables following $\text{Unif}[0,1]$ .  Allow $g_{n}$ to be correlated with the $U_{i}$ s; for instance, $g_{n}(x)=\frac{1}{\sqrt{n}}\sum_{i}1(U_{i}\leq x)$ . Is the following statement correct? $$\frac{1}{n}\sum_{i}g_{n}(U_{i}) \text{ has the same asymptotic distribution as } \int_{[0,1]} g_{n}(u)du.$$ I feel the statement is true but I'm not sure if my proof is correct: Let $\mu_{n}$ be the empirical c.d.f. for $U_{i}$ . We can rewrite the average $\frac{1}{n}\sum_{i}g_{n}(U_{i})$ as $\int_{[0,1]}g_{n}(u)\mu_{n}(du)$ .Hence, the difference of the considered two terms is $$\int_{[0,1]} g_{n}(u)(\mu_{n}-\mu)(du)\equiv f(g_{n},\mu_{n}-\mu).$$ By Glivenko-Cantelli, $\mu_{n}\to \mu$ almost surely where $\mu$ is the c.d.f. of $\text{Unif}[0,1]$ . By convergence in distribution of $g_{n}(u)$ and by continuity of the functional $f$ , the continuous mapping theorem implies that $f(g_{n},\mu_{n}-\mu)$ converges to 0 in probability. Is there any error in my proof? Thank you very much.","['weak-convergence', 'statistics', 'probability-theory', 'asymptotics']"
4867979,"Find $f(x)$ wher $f$ is an infinitely differentiable function such that $f(1) = 0, f(5) = \ln 4, f'(1) = 2$ and $f'(5) = −2$","The given equation is: $$x^2\left(f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^2\right)=1$$ The full question actually requires you to find f(x) and then use it to evaluate $$\int_1^5 e^{f(x)} dx$$ However I am stuck on finding f(x) first. First I attempted to guess what it could be using the boundaries given but that of course failed, so I am trying to get f(x) by manipulating the given equation by integrating both sides.
Here is what I tried: $$
\begin{aligned}
& x^{2}\left(f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^{2}\right)=1 . \\
& f^{\prime \prime}(x)+\left(f^{\prime}(x)\right)^{2}=\frac{1}{x^{2}}
\end{aligned}
$$ Integrate both sides: $$
\int f^{\prime \prime}(x) d x+\int\left(f^{\prime}(x)\right)^{2} d x=\int \frac{1}{x^{2}}
$$ let $u=f^{\prime}(x)$ $$
d u=f(x) d x \Rightarrow \frac{d u}{f(x)}=d x
$$ then, $$
\int \frac{u^{\prime}}{f(x)} d u+\int \frac{u^{2}}{f(x)} d u=\int \frac{1}{x^{2}}
$$ Reaching here I am understanding that I am headed in the wrong direction however I tried a few other ways using the equation, but I cant seem to get it in a form that is more manageable.
All help and hints are appreciated, Thank You.","['calculus', 'definite-integrals', 'ordinary-differential-equations']"
4867981,Regular conditional distribution of $Y$ given $X=x$ in Klenke's book,"In his book ""Probability theory"", Klenke uses the following definition of transition kernel: and if in $ii)$ the measure is a probability measure for all $\omega_1$ then $K$ is called a stochastic kernel. Later on he defines regular conditional distribution of $Y$ given $\mathcal F$ as follows: After the last line in the image above, he added ""(the function from the factorization lemma with an arbitrary value for $x \notin X(\Omega)$ ) is called a regular conditional distribution of $Y$ given X."", which perplexes me a lot. My attempt to understand this line: First, I think he forgot to assume that the map $ \omega \mapsto K_{Y|\sigma(X)}(\omega, B)$ should be $\mathcal F$ -measurable for any fixed $B \in \mathcal E$ . Next, I assume the previous statement holds and fix one $B \in \mathcal E$ . Since $K_{Y|\sigma(X)}(\cdot, B)$ is a version of $ P(Y \in B | \sigma(X)) $ and is $\sigma(X)$ -measurable too, by factorization theorem, there is a measurable function $\kappa(x, B)$ from $(E',\mathcal E')$ to $\mathbb R$ such that $\kappa(X(\omega), B) =  K_{Y|\sigma(X)}(\omega, B) $ for all $\omega$ . This shows, for every fixed $x \in X(\Omega))$ , $B \mapsto \kappa(x,B)$ is a measure since it equals to $ K_{Y|\sigma(X)}(\omega_x, B) $ for any $B \in \mathcal E$ for some $\omega_x \in X^{-1}(x)$ . We still have to check if it is a measure for a fixed $x \notin X(\Omega)$ . If I understand correctly his last line, we need to modify the map $x \mapsto \kappa(x,B)$ in the following way. Fix one probability measure $\mu$ on $\mathcal E$ ,  for any fixed $B$ , define $$ \forall x \in E', \qquad \kappa'(x,B) := \kappa(x,B) 1_{x \in X(\Omega)} + \mu(B)1_{x \notin X(\Omega)}. $$ Now we can easily check that for any fixed $x \in E'$ , $B \mapsto \kappa'(x,B)$ is a probability measure. HOWEVER, measurability is now an issue! Unless $X(\Omega) \in \mathcal E'$ , which rarely happens, then the map $x \mapsto \kappa'(x,B)$ is likely not $\mathcal E'$ measurable. I think he missed this in this book. Is there anyway to resolve this issue? My attempt to fix this issue: Suppose there is a set $N \in \mathcal E'$ such that $P^X(N)=1$ ( $P^X$ is the distribution of $X$ on $(E',\mathcal E')$ ) and that $ N \subset X(\Omega)$ then we can define $$ \forall x \in E', \qquad \kappa'(x,B) := \kappa(x,B) 1_{x \in N} + \mu(B)1_{x \notin N}. $$ The measurability is now resolved and it still holds that \begin{align}
\int_{E} \kappa'(x,B) P^X(dx) &= \int_{N} \kappa(x,B) P^X(dx) \\\\
                &= \int_{E} \kappa(x,B) P^X(dx) \\\\
                &= \int_{\Omega} \kappa(X(\omega),B) P(d\omega) \\\\
                &= \int_{\Omega} K_{Y|\sigma(X)}(\omega,B) P(d\omega) \\\\
                &= \int_{\Omega} P(Y \in B | \sigma(X)) P(d\omega) \\\\
                &= P(Y \in B)
\end{align} and so this new $\kappa'(\cdot,\cdot)$ is indeed a stochastic kernel from $(E',\mathcal E')$ to $(E, \mathcal E)$ . However, I believe such a set $N$ does not always exists.... My question is: Is there a way to fix this issue? or is there other approach to this regular conditional distribution? I borrow the images from this post . And consulted several related questions such as this , or this (Fabio forgot the check measurability here!), and this one too. However, they give no answers to my question. Thank you for your suggestions and comments.","['conditional-probability', 'measure-theory', 'probability-theory']"
4867995,Limit of measurable functions in a general metric space is measurable?,"Let $(f_n)_{n \geq 1}$ be a sequence of measurable functions from $(\Omega, \mathcal F)$ to a metric space $(X, d)$ . Suppose that $f$ is such that $$ \lim_{n \rightarrow +\infty} d(f_n(\omega),f(\omega)) = 0, \ \forall \omega \in \Omega. $$ Is the function $f$ $\mathcal F$ -measurable? What I know: if the space $X$ is separable (not necessarily complete) then this is true. I wonder why separability is needed here because I found an attempt that does not use separability. However, I still have a little bit of doubt so I would like to ask where did I go wrong... My attempt: Let $C$ be a closed set then the map $x \mapsto d(x,C)$ is continuous. Call this map $\Psi_C$ then the composition maps $ \Psi_C \circ f_n$ is measurable. $\Psi_C \circ f_n \rightarrow \Psi_C \circ f$ pointwise so $\Psi_C \circ f$ is measurable too. Now I observe $$ f^{-1}(C)= \{ \omega \in \Omega: d(f(\omega),C) = 0 \} = (\Psi_C \circ f)^{-1} \{ 0 \} \in \mathcal F $$ for every closed set $C$ , which generates the Borel sets $\mathcal B(X)$ induced by $d$ , and so $f$ is measurable. A similar argument is given here . However, I don't understand why in most probability/ analysis books, they usually assume the metric space is separable to have the limit measurable. Thanks in advance!","['limits', 'measure-theory', 'measurable-functions', 'real-analysis']"
4868003,Proving $\sum_{i=0}^n (-1)^i\binom{n}{i}\binom{m+i}{m}=(-1)^n\binom{m}{m-n}$,"I am trying to prove the following binomial identity: $$\sum_{i=0}^n (-1)^i\binom{n}{i}\binom{m+i}{m}=(-1)^n\binom{m}{m-n}$$ My idea was to use the identity $$\binom{m}{m-n}=\binom{m}{n}=\sum_{i=0}^n(-1)^i \binom{m+1}{i}$$ but the coefficients of the two sums don't seem to be equal. I also know there is a combinatorial argument that proves this, but I am trying to find an algebraic proof.","['summation', 'binomial-coefficients', 'combinatorics', 'algebra-precalculus', 'binomial-theorem']"
4868037,Limits of triple integral over a tetrahedron.,"The tetrahedron has vertices $O(0,0,0); A(0,0,2); B(0,2,0); C(1,0,0)$ I was thinking that the plane $ABC$ has equation $2x+y+z=2$ since: $\vec{BA}= \langle 0,-2,2 \rangle$ and $\vec{CA}= \langle -1,0,2 \rangle$ with a cross product $\langle 4,2,2 \rangle$ hence $4x+2y+2z=4 \cdot (1) + 2 \cdot (0) +2 \cdot (0) =4 $ So $$2x+y+z=2 \tag{1}$$ The question asked to follow the order $dz\,dy\,dx$ so I got: $$I=\int^1_0\int^{2-2x}_0\int^{2-2x-y}_0 \; dz\,dy\,dx \tag{2}$$ Is there anything wrong with my work? I'm particularly skeptical of the $z$ limit. Also, Is there a more straightforward way of doing it? **edited for $dy$ limits.","['volume', 'iterated-integrals', 'multivariable-calculus', 'solution-verification', 'vector-analysis']"
4868111,Why does $\sum_{n=1}^{\infty} \frac{1}{n}$ not converge and yet $\sum_{n=1}^{\infty} \frac{1}{n^2}$ does converge? Looking for an intuition,"It was proven to us that $\sum_{n=1}^{\infty} \frac{1}{n}$ does not converge and yet $\sum_{n=1}^{\infty} \frac{1}{n^2}$ does converge, I don't doubt the proof, and yet I would like to have acquired an 'intuition' to it. What I don't understand is what exactly is the difference between $\sum_{n=1}^{\infty} \frac{1}{n}$ and $\sum_{n=1}^{\infty} \frac{1}{n^2}$ . My intuition leads me to believe that the reason either of them converges is because the terms that are summed are gradually decreasing until the addition of the next term is meaningless, because both: $$\lim_{n \to \infty} \frac{1}{n} = 0$$ And $$\lim_{n \to \infty} \frac{1}{n^2} = 0$$ I would appreciate it if someone could debunk this in plain terms which refer to intuition, as I do understand the formality of the proof.","['calculus', 'sequences-and-series']"
4868113,A trigonometric integral and algebraic integral limit of the same kind,"Evaluate the value of: $$(i)\lim_{n \to ∞} n^2 \left(\int_0^{\pi/2} (\sin^n(x)+\cos^n(x))^{1/n} dx-\sqrt2 \right)$$ $$(ii)\lim_{n \to ∞} n^2 \left(\int_0^1 (x^n+(1-x)^n)^{1/n} dx-3/4 \right)$$ Both of these limits have a similar form and it's easy to see that it is a $0$ times $∞$ form; it is easy to evaluate both integrals by using the sandwich theorem. In (i), for example, first I broke it into two integrals from $0$ to $\pi/4$ and $\pi/4$ to $\pi/2$ and noted that $\sin(x)>\cos(x)$ in the first interval and vice versa in the second interval. Using these facts, I put a lower bound and upper bound on the integral; $$\int_0^{\pi/4}2^{1/n} \sin x dx+\int_{\pi/4}^{\pi/2} 2^{1/n} \cos x dx<\int_0^{\pi/2} (\sin^n(x)+\cos^n(x))^{1/n} dx <\int_0^{\pi/4}2^{1/n} \cos x dx+\int_{\pi/4}^{\pi/2} 2^{1/n} \sin x dx $$ Upon passing the limit, the value of the integral came out to be $\sqrt2$ . Similarly, for the second integral, I broke it down into an interval from $0$ to $1/2$ and $1/2$ to $1$ to obtain $3/4$ . But, how do I evaluate these infinity times zero indeterminate forms using the sandwich theorem or anything?","['limits', 'trigonometric-integrals', 'definite-integrals', 'real-analysis']"
4868191,"Convergence in Probability, a question","Supposing $X_{t_i}$ is a real-valued stochastic process defined for $i=1,\ldots, N$ , where $N=N_n\rightarrow\infty$ , and $a_n\rightarrow 0$ as $n\rightarrow\infty$ , I am trying to see if the following holds: \begin{equation}
\sum_{i=1}^{N}(X^2_{t_i}-a_n^2)=o_p(a_n)\tag{Result}
\end{equation} when we have \begin{equation}
\max_{1\leq i\leq N}\frac{X_{t_i}^2}{a_n^2}\overset{p}\rightarrow c\tag{$*$}
\end{equation} as $n\rightarrow\infty$ . Open to amend the condition ( $*$ ) if necessary. Some thoughts : obviously, since we have \begin{align*}
&\frac{\sum_{i=1}^N(X_{t_i}^2-a_n^2)}{a_n}\\
\qquad&\leq N\cdot \max_{1\leq i\leq N}\left(\frac{X_{t_i}^2-a_n^2}{a_n}\right)=Na_n\cdot \max_{1\leq i\leq N}\left(\frac{X_{t_i}^2-a_n^2}{a_n^2}\right)\\
&\leq Na_n\cdot \max_{1\leq i\leq N}\left(\frac{X_{t_i}^2}{a_n^2}-1\right),
\end{align*} if $c=1$ and $N=O(a_n^{-1})$ , we have the desired result by ( $*$ ). But I am not sure if we really need to restrict $c=1$ . Also, I am looking for a way to relax $N=O(a_n^{-1})$ as well. Perhaps the bounds above in my solution is not tight enough somehow","['stochastic-processes', 'probability-theory', 'asymptotics']"
4868201,Proof of Doob's Maximal Inequality for Positive Supermartingales,"I am working through a course on Stochastic Processes and am looking for a proof verification for an alternative to the one that I have been presented. My proof, as noted below, currently contains a gap. However, it is unclear to me whether or not this is an easily resolvable gap or if I have to refer to an alternative proof. Let $(X_n)_{n \ge 0}$ be a positive supermartingale. Then Doob's Maximal Inequality states that for any $\lambda \ge 0$ , we have the following: $$ \lambda \mathbb{P} \big{(} \max _{k \ge 0} X_k \ge \lambda \big{)} \leq \mathbb E(X_0)$$ My proof attempt uses the fact that by Doob's Optional Stopping Theorem, if we can find a stopping time $T$ , then for any martingale, $\mathbb{E}(X_T) \leq \mathbb E (X_0)$ under the assumption that one of the following holds: $T$ is bounded $X$ is bounded and $T$ is finite $\mathbb E (T) < \infty$ and for some $K > 0$ , we have that $|X_n - X_{(n-1)} | \leq K$ The gap in my proof is that I can't see how any of these conditions apply in this case. However, I suspect that the fact that we assume that $X$ is not just a supermartingale, but a positive one may allow us to relax these assumptions. After that, inspired by the inequality in question, I let our stopping time $T$ be defined: $$T := \inf \{ n \ge 0 : X_n \ge \lambda \}$$ After this we can perform some fairly routine calculations to show the desired result: $$ \mathbb E (X_T) = \mathbb E (X_T | T < \infty ) \mathbb{P}(T \leq \infty) + \mathbb E (X_T | T = \infty ) \mathbb{P} (T = \infty) $$ $$ \ge \mathbb E (X_T | T < \infty ) \mathbb{P}\big{(}T \leq \infty)$$ $$ \ge \lambda \mathbb{P}(T \leq \infty)$$ $$ = \lambda \mathbb{P}\big{(}\max _{k \ge 0} X_k \ge \lambda \big{)}$$ $$ \text{So: } \space \lambda \mathbb{P}\big{(}\max _{k \ge 0} X_k \ge \lambda \big{)} \leq \mathbb E (X_T) \leq \mathbb E (X_0) \space \text{ (as required) }\quad \square$$ This intuitively feels close to a complete proof, however, I am aware that this could be misleading and I may need to take a completely different avenue given the fact that I cannot currently invoke the Optional Stopping Theorem in the way that I have done. I would be grateful if this could be resolved for me.","['stochastic-processes', 'solution-verification', 'martingales', 'inequality', 'probability-theory']"
4868208,Understanding $dx/df$ (not $df/dx$),"I have encountered here that sometimes people write $dx/df$ . For example, here , the author of the answer writes down the chain rule as $$\frac{dg}{df}=\frac{dg}{dx}\frac{dx}{df},$$ which seems to be an abuse of notation. And $dx/df$ is found to be just $\frac{1}{df/dx}$ . Here , I learned that when someone writes ""differentiate $f$ with respect to $g$ "", what they mean is to differentiate some hidden composition, which seems to rely upon an inverse of what is said to be a quantity with respect to which we want to differentiate (there, they have $\phi(t)=2t$ , whereas they differentiate $f(x)=x^2$ with respect to $x/2$ , and the composition is $f\circ\phi$ ), but I may be wrong here. So, if the goal is to ""differentiate $x$ with respect to $f$ "", do we actually have $$(x\circ h)'(u)=x'(h(u))h'(u),$$ where $h(u):=f^{-1}(u)$ and $x(u):=u$ ?
This seems to agree with what $dx/df$ is to be found: $$(x\circ h)'(u)=\frac{1}{f'(f^{-1}(u))}=\frac{dx}{df}.$$","['calculus', 'derivatives']"
4868251,Dual of completion and weak$^*$-topology,"Let $X$ be a dense subspace of a Banach space $Y$ . The restriction map $$r: Y^* \to X^*:\omega \mapsto  \omega \vert_X$$ is then an isometric isomorphism that is weak $^*$ -continuous. I am wondering if it is true if the map $r$ is a weak $^*$ -homeomorphism? My intuition tells me that this is not true. Of course, on norm-bounded subsets, the inverse map is weak $^*$ -continuous, so the problem (if any) exists because of the unbounded nature of things. Any insight is appreciated!","['weak-convergence', 'normed-spaces', 'topological-vector-spaces', 'functional-analysis', 'general-topology']"
4868303,"Calculate the range of the function $f(x)=-x^2+6x$ with the domain $[2,5]$","Consider the function $f(x)=-x^2+6x$ with the domain $[2,5]$ . Now calculate the range of the function according to its domain. Here is my solution: The domain is $2\leq x\leq5$ so I can wrote: $2\leq x\leq5\implies4\leq x^2\leq25\implies-4\geq-x^2\geq-25\implies\boxed{-25\leq-x^2\leq-4}\ (\text{1})$ $2\leq x\leq5\implies\boxed{12\leq6x\leq30}\ (\text{2})$ Then I calculated the sum of the two inequalities $(\text{1})$ and $(\text{2})$ . So the range is $-13\leq-x^2+6x\leq26$ . But since this was a question from a calculus book, I know that the correct answer is $[5,9]$ and not $[-13,26]$ . So what is the problem of my solution?","['calculus', 'algebra-precalculus']"
4868327,Prove the existence of $g$ such that $\mathbb E (e^{aX_t} ) = e^{g(a) t}$ for a Compound Poisson Process $X_t$,"I have been studying a course on Stochastic Processes and recently encountered the following result with the proof listed as an exercise. Let $X_t$ be a Compound Poisson Process defined by the sum of iid random variables $(Y_n)$ independent of $N$ , written explicitly as: $$ X_t := Y_1 + Y_2 + \cdots + Y_{N_t}$$ where $N$ is a Poisson Process. Show that there exists a function $g(a)$ such that for any $a \in \mathbb{R}$ we have that $$\mathbb E (e^{aX_t} ) = e^{g(a) t}$$ By the independence of $(Y_n)$ , we can write the following: $$\mathbb E (e^{a X_t}) = \mathbb E (e^{aY_1}e^{aY_2} \cdots e^{aY_{N_t}}) = \mathbb E (e^{aY_1}) \mathbb E (e^{aY_2}) \cdots \mathbb E (e^{aY_{N_t}})$$ Observation: this is the product of the MGFs for each of the $Y_i$ , however, as the distribution of $Y_i$ is unknown but iid, this means that all the MGFs will be the same for each $Y_i$ . Which simplifies the above to: $$\mathbb E (e^{a X_t}) = \mathbb E (e^{aY_1})^{N_t}$$ Now I believe there is some merit to the next approach that I take, but I don't believe that I am formalising it correctly or perhaps applying the lemma in the wrong way. But first, observe the lemma itself: Lemma: if $f$ is a continuous function such that $f(t+s) = f(t)f(s)$ , then either $f =0$ or $f = e^{bt}$ for some constant $b$ . If I define the set $A := \{ Y_1, \space \cdots , \space Y_{N_t} \}$ , then we can define the function $f$ with domain $A$ as $f(Y) = \mathbb{E}(e^{a Y})$ . This function satisfies the second part of the Lemma , however, I am unsure about continuity. Is this approach to the problem along the right lines, or should I try something else? Edit: Snoop has provided a proof below which has been helpful. However, the question of how to solve this using the method above remains.","['independence', 'continuity', 'stochastic-processes', 'functions', 'probability-theory']"
4868330,"Why does Wolfram Alpha give two different, inconsistent answers for sum of a matrix?","I would like to find out what $\sum_{r=1}^{n} \pmatrix{1 & -1 \\ 0 & 2}^r$ is equal to. I have typed this expression into WolramAlpha twice and got a different answer each time, both inconsistent. Attempt 1 Attempt 2 We can see the entry in the top right is not the same in both answers and those expressions are definitely not equal. My question is: why is WolframAlpha giving different answers?","['matrices', 'wolfram-alpha', 'sequences-and-series']"
4868427,Can I determine the angles of a quadrilateral if I know the lengths of the sides and the difference between the diagonals?,"I know the lengths of the four sides of a quadrilateral and the difference between the diagonals (but I do not know the actual lengths of the diagonals). My instinct is that this information ought to be sufficient to determine the angles of the quadrilateral, because a specific difference between the diagonals constrains it to a single, fixed shape. example measurements:
L side length = 326mm;
R side length = 325mm;
bottom length = 677mm;
top length = 675mm;
diagonal from bottom L to top R is 7mm longer than from bottom R to top L",['geometry']
4868452,$\frac{\sin3x + 2}{\sin x+2} = 2023^{\sin x-\sin3x}$,"The statement of the problem : Solve in $\mathbb R$ the following equation : $$\frac{\sin3x + 2}{\sin x+2} = 2023^{\sin x-\sin3x}$$ My approach : To simplify, we will use the fact that $\sin3x = 3*\sin x -4*\sin^3 x$ . So, if we make the notation $t=\sin x$ , where $t \in [ -1 , 1 ]$ we get the following : $$\frac{-4t^3+3t+2}{t+2} = 2023^{4t^3 - 2t}$$ Now I tried for some time to find some solutions and below you can see a graph, but it seems to me that it is getting super complicated, I managed to find some solutions, but it is difficult for me to prove that they are the only ones. If possible, I would prefer a solution that provides the use of inequalities or convexity/concavity or inverse trigonometric functions rather than the use of limits or derivatives because I have to present it to a group of students who have not yet learned about these. The problem is from an old textbook and it is before the lesson on limits and derivatives, so there is probably another solution that don't implies them which I'm looking forward to. All proofs will be helpful. Thanks a lot!","['inverse-trigonometric-functions', 'exponentiation', 'trigonometry']"
4868501,Failure of translation-invariance of an integral?,"Let $u$ be a smooth compactly supported function on $\mathbb{C}$ . The $\overline{\partial}$ -Poincaré lemma is the formula $$\frac{{\partial}}{{\partial} \overline{z}}\frac{1}{\pi} \int_{\mathbb{C}} \frac{u(w)}{w - z} dx \wedge dy = u(z)$$ where $dx \wedge dy$ is the standard Lebesgue measure on $\mathbb{C}$ ( $w = x + iy$ ). In proving this, it is not immediately justified to pass the derivative through the integral, since for any fixed $w$ the integrand is an unbounded function of $z$ , so, e.g., the Dominated Convergence Theorem fails. But one can make the change of variables $w \to w + z$ to remedy this, so the above equals $$\frac{1}{\pi} \int_{\mathbb{C}} \frac{{\partial}}{{\partial} \overline{z}}\frac{u(w + z)}{w } dx \wedge dy.$$ From here one can make a standard limit argument using Stokes' theorem to finish the proof of the formula. My confusion is that the integral $\frac{1}{\pi} \int_{\mathbb{C}} \frac{{\partial}}{{\partial}\overline{ z}}\frac{u(w + z)}{w } dx \wedge dy$ appears not be translation-invariant! Indeed, if we undo the substitution we made before, then the integrand $\frac{{\partial}}{{\partial} \overline{z}}\frac{u(w)}{w - z}$ is zero almost everywhere, since $\frac{u(w)}{w - z}$ is holomorphic as a function of $z$ away from its pole. Clearly something is wrong here, but why exactly is this substitution is unjustified? What exactly fails when the derivative is introduced to the integral?","['complex-analysis', 'measure-theory', 'real-analysis']"
4868579,Remainders of $(1-3i)^{2009}$ when divided by $13+2i$ in $\Bbb{Z}[i]$,"Find the possible remainders of $(1-3i)^{2009}$ when divided by $13+2i$ in $\mathbb{Z}[i]$ . I'm having a hard time understanding remainders in $\mathbb{Z}[i]$ . I'm gonna write my solution to the problem above just to give some context, but you can skip this part and read the question below if you want. We start by noticing that $N(13+2i) = 173$ , which is a prime number. Therefore, $13+2i$ is irreducible in $\mathbb{Z}[i]$ . This allows us to quickly calculate the number of elements of the multiplicative group $(\mathbb{Z}[i]/(13+2i))^{\times}$ , namely $N(13+2i) - 1 = 172$ . By Lagrange's Theorem, since $1-3i$ and $13+2i$ are coprime, $$(1-3i)^{11\cdot 172} \equiv 1\mod(13+2i).$$ We now reduced the problem to finding $(1-3i)^{117} \mod (13+2i)$ . Note that $117 = 3^2\cdot 13$ , so by a tedious but direct calculation (once we figure out $(1-3i)^{3^2}$ modulo $13+2i$ we get a nice expression, so the $13$ th power is relatively straightforward to compute), we arrive at $6-2i$ . Question: Remainders in $\mathbb{Z}[i]$ are not uniquely determined and, from my understanding, there are $4$ possible choices. It is not difficult to find different remainders when we are working with small numbers and can test various quotients. In this case, however, how would I determine the other possible choices? Are they somehow related? For instance, when I was calculating $(1-3i)^{3} = -26+18i$ , I found that $$(1-3i)^{3} = (-2+i)(13+2i) + (2+9i) = (-1+2i)(13+2i) + (-9-6i).$$ The remainders above have norms $N(2+9i) = 85$ and $N(-9-6i) = 117$ . I thought they would have to be associates, but the example above shows they are not and, even worse, $\operatorname{gcd}(85, 117) = 1$ .","['number-theory', 'gaussian-integers', 'algebraic-number-theory', 'elementary-number-theory']"
4868657,Limit of a sequence of the real root of a family of polynomials,"Consider a sequence of polynomials $P(x)=x^n+x^{n-1}+x^{n-2}+...+x^2+x-1, n>2$ . (i) Prove that it has a unique positive real root $x_n$ (ii) Find $$\lim_{n \to ∞} 2^n (x_n - 1/2)$$ The first part was relatively simple; upon differentiating $P(x)$ , it is easy to see that for all $x>0$ , the function is strictly increasing, so it can at most have one root. Secondly, $P(0)=-1$ and $P(1)=n-1$ i.e. the root must lie in $(0,1)$ . The third observation is that, as we apply the limit of n tending to infinity on the polynomial, $x_n$ tends to $1/2$ (it just becomes an infinite geometric series) which explains why the limit is an indeterminate form of $∞.0$ In the concise solution of the book, it was written that $x_n = \dfrac{1}{2}+\dfrac{\theta_n}{2^{n+1}}$ where $\theta_n$ lies in $[0,1]$ and left at that. Firstly, this step seems quite unmotivated to me and I also couldn't use it to solve the limit. Secondly, I failed to prove this equality as well (it resembled the form of LMVT so that was my starting point but it led nowhere). Does anyone have a more intuitively well-motivated answer for the second part of this problem or any observation that I missed?","['limits', 'mean-value-theorem', 'polynomials', 'sequences-and-series']"
4868664,"How to evaluate $\int_{0}^{1} \int_{0}^{1} \frac{{(1 + x) \cdot \log(x) - (1 + y) \cdot \log(y)}}{{x - y}} \cdot (1 + \log(xy)) \,dy \,dx$","Question: How to evaluate this integral $$\int_{0}^{1} \int_{0}^{1} \frac{{(1 + x) \cdot \log(x) - (1 + y) \cdot \log(y)}}{{x - y}} \cdot (1 + \log(xy)) \,dy \,dx$$ My messy try $$\int_{0}^{1} \int_{0}^{1} \frac{{(1 + x) \cdot \log(x) - (1 + y) \cdot \log(y)}}{{x - y}} \cdot (1 + \log(xy)) \,dy \,dx$$ $$
\begin{array}{r}
I=\int_0^1 \int_0^1 \frac{(1+x) \ln (x)-(1+y) \ln (y)}{x-y}(1+\ln (x y)) d y d x \\
I=\int_0^1 \int_0^{\frac{1}{x}} \frac{(1+x) \ln (x)-(1+x t)(\ln (x)+\ln (t))}{1-t}(1+2 \ln (x)+\ln (t)) d t d x \\
=\int_0^1 \int_0^{\frac{1}{x}} x \ln (x)(1+2 \ln (x)+\ln (t)) d t d x- \\
-\int_0^1 \int_0^{\frac{1}{x}} \frac{(1+x t) \ln (t)}{(1-t)}(1+2 \ln (x)+\ln (t)) d t d x \\
=\int_0^1 x \ln (x)\left(\frac{1+2 \ln (x)-\ln (x)-1}{x}\right) d x-\int_0^1 \int_0^1 \frac{(1+x t) \ln (t)}{(1-t)}(1+2 \ln (x) \ln (t)) d x d t + 
\end{array}
$$ $$- \int_{0}^{\infty} \int_{0}^{\frac{1}{t}} \frac{{(1 + xt) \cdot \ln(t)}}{{1 - t}} \cdot (1 + 2 \cdot \ln(x) + \ln(t)) \,dx \,dt$$ $$I_1 = \int_{0}^{1} x \ln(x) \left( \frac{{1 + 2 \ln(x) - \ln(x) - 1}}{{x}} \right) \,dx - \int_{0}^{1} \int_{0}^{1} \frac{{(1 + xt) \ln(t)}}{{1 - t}} \cdot (1 + 2 \ln(x) + \ln(t)) \,dx \,dt $$ $$I_2 = -\int_{0}^{\infty} \int_{0}^{1/t} \frac{{(1 + xt) \ln(t)}}{{1 - t}} \cdot (1 + 2 \ln(x) + \ln(t)) \,dx \,dt$$ I need hints for figuring out $I_1$ and $I_2$ .","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'closed-form']"
4868667,binomial coefficient identity $\sum_{k=0}^n {n\choose k}\sum_{j=0}^{k-1}{{n-1}\choose j}=2^{2n-2}$,"While trying to solve a probability question about a coin flip game, I arrived at the expression $\sum_{k=0}^n {n\choose k}\sum_{j=0}^{k-1}{{n-1}\choose j}.$ Computing this sum for several low values of $n$ suggested that it summed to $2^{2n-2}$ , suggesting there is an identity: $$\sum_{k=0}^n {n\choose k}\sum_{j=0}^{k-1}{{n-1}\choose j}=2^{2n-2}$$ However I was not able to prove the identity by means of the Pascal recurrence relation, the Chu-Vandermonde identity $\sum_{k=0}^r {m\choose k}{n\choose {r-k}}={{m+n}\choose r}$ , nor the $\sum_{k=0}^n{n\choose k} = (1+1)^n=2^n$ identity from the binomial theorem. Can I get a hint how to proceed?","['summation', 'binomial-coefficients', 'combinatorics']"
4868733,Sufficient condition for being a topological line [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 4 months ago . Improve this question Let $S\subset \mathbb{R}^n$ a (path-)connected subspace in the usual topology with the property that, for all $x\in S$ , $S-\{x\}$ is disconnected and has two connected components. Is $S$ homeomorphic to $R$ with the usual topology? My attempts at (dis)proving this have been unfruitful, with the tools that I now have. I kindly ask you to provide me with slight hints on how to tackle this problem. Note : I'm not sure if path-connectedness is necessary or plain connectedness suffices for this result.","['general-topology', 'algebraic-topology']"
4868756,"How many natural numbers $a\le100$ are there such that $a=[\frac a2]+[\frac a3]+[\frac a5]$, where [.] represents the greatest integer function?","A natural number $a$ is selected from the first $100$ natural numbers. The probability that $a=[\frac a2]+[\frac a3]+[\frac a5]$ , where [.] represents greatest integer function, is $\frac mn$ where $m,n$ are coprime then $(m+n)$ is equal to My Attempt: Let $a=30n+\gamma$ , where $0\le\gamma\lt30$ Putting this in the given equation, I get, $n=\gamma-[\frac{\gamma}{2}]-[\frac{\gamma}{3}]-[\frac{\gamma}{5}]$ $\gamma=0$ doesn't satisfy but $\gamma=1, 2, ..., 29$ satisfy. So, the probability is $\frac{29}{100}$ . Is this correct?","['contest-math', 'solution-verification', 'combinatorics', 'discrete-mathematics', 'probability']"
4868779,Easiest argument for showing that a holomorphic map with finite fibres has no essential singularity,"Let $D$ be the open unit disc in $\mathbb{C}$ and let $D^* = D\setminus\{0\}$ . Now, Picard's Great Theorem implies the following fact. If $f\colon D^\ast \to \mathbb{C}$ is a holomorphic function with finite fibres, then $f$ does not have an essential singularity at $0$ . Here is the simple argument (using Picard's Great Theorem). The fibre $f^{-1}(0)$ is finite and so is the fibre $f^{-1}(1)$ . Thus, upon replacing $D$ by a disc of radius $r$ for some suitable $0<r<1$ and $D^*$ accordingly, we may assume that $f$ takes values in $\mathbb{C}\setminus \{0,1\}$ . Then Picard's Great Theorem says that $f$ has no essential singularity. QED Is there a proof of the above fact that does not use Picard's Great Theorem? I'm hoping to find  an argument which uses simpler results (e.g., Casorati-Weierstrass, existence of logarithm after shrinking $D^*$ , etc.)",['complex-analysis']
4868845,Why would the triangles join up to a rhombus?,"The question I am going to present may as well sound very dumb. But this is becoming a hell of a confusing thing for me. The question is from ISI B.Math-B.Stat entrance exam 2022 UGA question paper. It is problem 29. The question included pictures, but I am going to rephrase the question in a way such that the pictures are not needed but I will still include the pictures. If $\triangle{APB}$ has area $4$ , $\triangle{BPC}$ has area $5$ , $\triangle{CPD}$ has area $x$ and $\triangle{APD}$ has area $13$ , where $AB=BC=CD=DA=6$ Then find the value of $x$ . Here is the picture: Now. Here's my question. Apparently if we join up the triangles we get a rhombus. Like this: Then we can simply draw two perpendicular going through the point $P$ . Then considering the areas of the triangles we will get that $\triangle{APB}+\triangle{CPD}=\triangle{BPC}+\triangle{APD}$ Meaning that, $x=13+5-4=14$ But why would the the triangle add up to a rhombus. I mean it intuitively makes sense but can we give a logical reason. I mean it could've been the case that when we join up the triangles then two of the triangles just don't meet. Like this: Now, after we connect $C$ and $E$ (shown by the dotted line), if we can show that $CE=6$ then we can simply show that such a picture is a contradiction because then all the sides of $\triangle{PCE}$ and $\triangle{PCD}$ will be equal which will mean that they are similar triangles but clearly $\angle{PCE}\neq\angle{PCD}$ which is a contradiction. The problem became when my friend asked me how can I say that $CE=6$ which caught me off guard. And now, after spending a few hours using trignometry to show that $CE=6$ , I am clueless. The expressions are becoming too complicated. So at the end I have two questions- $1)$ How to show that $CE=6$ ? $2)$ Is there any other way to show that the four will join up to a rhombus?",['geometry']
4868859,Periodic solutions to a first order linear ODE with incommensurable periods,"I'm studying for an admission test for the PhD in Mathematical Analysis. I'm stuck to solve this exercise: Consider the ODE $x'(t)+a(t)x(t)=b(t)$ where $a(t)$ and $b(t)$ are continuous functions from $\mathbb{R}$ to itself. Prove that this Equation cannot have 3 periodic non-constant solutions with pairwise incommensurable periods. I recall that if $k\in\mathbb{R}$ and $q\in\mathbb{R}-\{0\}$ , then they are called incommensurable when $\frac{k}{q}\notin\mathbb{Q}$ . I solved the first part of the exercise that asked to prove that if a continuous function on $\mathbb{R}$ is periodic and has two periods that are incommensurable then it's a constant function. But I can't see how the first part of the exercise can help with the second part. I also tried to do some computations with the solutions (subtracting and adding them etc.) and to write the general integral of the ODE and see if it could help.  In both cases all I did seemed useless.","['periodic-functions', 'analysis', 'ordinary-differential-equations', 'real-analysis']"
4869861,"Lower bound $\sum_{\{ s_{1},..,s_{d}\}\subset [n]} d!\frac{1}{s_{1}s_{2}\dots s_{d}} $","I want to lower bound the following expression of choosing $d$ numbers from $[n]:=\{ 1,2,\dots,n \}$ . $$
\sum_{\{ s_{1},..,s_{d}\}\subset [n]} d!\frac{1}{s_{1}s_{2}\dots s_{d}}
$$ Here $s_{1},\dots,s_{d}$ are all distincts. I know I can upper bound this by $\left( 1+\frac{1}{2}+\dots+\frac{1}{n} \right)^{d} = H_{n}^{d} < (\ln n+1)^{d}$ . Is there a way to lower bound this quantity, perhaps to $\Theta((\ln n)^d)$ ? Or is a bound of $\Theta((\ln n)^d)$ even possible? I don't really know much about combinatorics, so not sure what tools I can utilize.","['graph-theory', 'combinatorics', 'probability']"
4869894,Multivariate Normal Distributions and the Uniform Distribution on the Sphere,"Given a multivariate normal vector $X \sim N(0,I_d)$ (identity covariance matrix), it is well known that : $$\frac{X}{\|X\|_2}
$$ is uniformly distributed on the sphere of radius $\sqrt{d}$ in $\mathbb{R}^d$ , where here $\|\cdot\|_2$ denotes the usual Euclidean norm. Question : Is this is not the case for any rotationally invariant $X$ ? Roughly speaking, $X$ being rotationally invariant in my mind corresponds to the distribution of $X$ depending only $\|X\|_2$ (radius) and so any normalized rotationally invariant distribution should also be uniformly distributed on the sphere (of course possibly with a different choice of radius). Is there something special about normality being invoked above? I know that the multivariate normal is the only rotationally invariant distribution with independent components $X_i$ (this is the so-called Maxwell's theorem) but I'm not sure if/where this fact is invoked when showing that normalized versions of $X$ are uniformly distributed on the sphere.","['geometric-probability', 'probability-distributions', 'probability-theory', 'probability']"
4869913,"Roots of quartic equation - given product of two roots, find missing coefficient","The quartic equation $ax^4 + bx^3 + cx^2 + dx + e = 0$ has roots $\alpha, \beta, \gamma, \delta$ . Given that $\alpha \beta = p$ find the value of $k$ So I have deduced that $\gamma \delta = \frac{e}{ap}$ using product of roots $=-\frac{e}{a}$ but I am not sure how to proceed from here. I have written out Vieta's formulae, but can't seem to manipulate to get $k$ . Is there an efficient way to do this?",['algebra-precalculus']
4869914,Covariant derivative/connection of a local frame $\nabla \bar e = \nabla(e)g + edg$,"Suppose $e$ and $\bar e$ are two frames for a vector bundle $E \to M$ over $U \subset M$ such that $\bar e = eg$ for some $g: U \to \text{GL}(r,\Bbb R)$ . Then $$\nabla \bar e = \nabla(e)g + edg.$$ I'm trying to understand this which is seemingly just the Leibniz rule, but I have some trouble with the definitions. A frame $e = (e_1,\dots,e_r)$ is an $r$ -tuple of sections $e_i : U \to E$ such that $e(p)=(e_1(p),\dots,e_r(p))$ forms a basis for $E_{p}$ . There are some slight ambiguities here, first off is $e$ a map $e : U \to E$ also? I mean it has to be since otherwise $\nabla e$ would not make sense.  Second, could someone help me understand how the above equation is the Leibniz rule? In general for a connection $\nabla$ on a vector bundle the Leibniz rule gives $$\nabla (fs)=df \otimes s + f\nabla s$$ where $s$ is a section $M \to E$ , but $f :M \to \Bbb R$ is a smooth function not a map with codomain $\text{GL}(r,\Bbb R)$ so what gives?",['differential-geometry']
4869915,Solve $x\left(x-1\right)y^{\prime\prime}+3xy^{\prime}+y=0$ for $0≤x<1$ with series,"Solve $x\left(x-1\right)y^{\prime\prime}+3xy^{\prime}+y=0$ for $0≤x< 1$ I am supposed to solve via the series method. The answer should look like this: $\begin{aligned}y\left(x\right)=A\frac{x}{\left(1-x\right)^{2}}+B\frac{x}{\left(1-x\right)^{2}}\left(\ln\left(x\right)+\frac{1}{x}\right)\end{aligned},$ where $A, B$ are constants. My attempt: $\begin{array}{l}y'(x)=\sum_{n=1}^\infty na_nx^{n-1}=\sum_{n=0}^\infty(n+1)a_{n+1}x^n\\ y''(x)=\sum_{n=2}^\infty n(n-1)a_nx^{n-2}=\sum_{n=0}^\infty(n+2)(n+1)a_{n+2}x^n\end{array}$ Substituting, $$x(x-1)\sum_{n=0}^\infty(n+2)(n+1)a_{n+2}x^n+3x\sum_{n=0}^\infty(n+1)a_{n+1}x^n+\sum_{n=0}^\infty a_nx^n=0$$ This is where I am stuck with the algebra. And I have no idea how and where to apply the condition $0≤x<1$","['calculus', 'sequences-and-series', 'ordinary-differential-equations', 'real-analysis']"
