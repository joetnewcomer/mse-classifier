question_id,title,body,tags
461853,"Given a fixed perimeter, which shape will have the maximum area?","I think the answer is a circle. If so, then what is the rigorous prove?",['geometry']
461890,Free Graded Commutative Algebra on a Graded Vector Space,"Let $V$ be a graded vector space, thought of as a collection $\{ V^n \}_{n \ge 0}$ of vector spaces. Let $V_{odd} = \bigoplus_{n \text{ odd}} V^n$ and $V_{even} = \bigoplus_{n \text{ even}} V^n$. I am trying to show that there is a functor
$$
\Lambda \colon \mathsf{GVec} \longrightarrow \mathsf{CGAlg}
$$
between graded vector spaces, and commutative graded algebras which is left adjoint to the forgetful functor - i.e. $\Lambda V$ should be the free commutative graded algebra on $V$. One way to define this is $\Lambda V = E( V_{odd} ) \otimes S(V_{even} )$ where $E$ denotes the exterior algebra and $S$ the symmetric algebra.
However I am having trouble showing that it is commutative. We can think of the exterior and symmetric algebras as polynomial algebras on basis elements of each of $V_{odd}$, $V_{even}$ respectively, subject to some relations. Let $\underline{x} \otimes \underline{y}, \underline{z} \otimes \underline{w}$ be elements in $\Lambda V \otimes \Lambda V$, where the underline means a monic monomial. Multiplying these gives
$$
(\underline{x} \otimes \underline{y} ) \cdot (\underline{z} \otimes \underline{w}) = (-1)^{|\underline{y}| | \underline{z}|} \underline{x} \underline{z} \otimes \underline{y} \underline{w} \ \ \ \ \ \ \ \ \ \ \ \ \ (1)
$$
by definition of the tensor product of graded algebras. However if we apply the braid to the element $\underline{x} \otimes \underline{y}\otimes \underline{z} \otimes \underline{w}$ this gives
$$
(-1)^{| \underline{x} \otimes \underline{y}||\underline{z}  \otimes \underline{w}| } \underline{z} \otimes \underline{w} \otimes \underline{x} \otimes \underline{y} = (-1)^{(| \underline{x}| +  |\underline{y}|)(|\underline{z}\ + |\underline{w}| )} \underline{z} \otimes \underline{w} \otimes \underline{x} \otimes \underline{y}
$$
then multiply we get
$$
(-1)^{(| \underline{x}| +  |\underline{y}|)(|\underline{z}| + |\underline{w}| )} (-1)^{|\underline{w}| |\underline{x}|}\underline{z} \underline{x}\otimes \underline{w} \underline{y}
$$
We can swap $\underline w$ and $\underline y$ since they live in the symmetric algebra, and can swap $\underline z$ and $\underline x$ after multiplying by $(-1)^{| \underline x| |\underline z | }$ so the above expression simplifies to
$$
(-1)^{|\underline y| |\underline z| + |\underline y| |\underline w |} \underline{x} \underline{z}\otimes \underline{y} \underline{w}  \ \ \ \ \ \ \ \ \ \ \ \ \ (2)
$$ (1) and (2) should be the same. What have I done wrong? EDIT: Just to be clear I am trying to show that $\mu \sigma = \mu$ where $\sigma$ in $\mathsf{GVec}$ is the braiding $a \otimes b \mapsto (-1)^{|a||b|}b \otimes a$ and $\mu$ is the multiplication.","['exterior-algebra', 'category-theory', 'abstract-algebra']"
461901,"Differentiable function, with $f'(x)=[f(x)]^2$","Let $f: \mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(0)=0$ and $\forall x \in \mathbb{R}$, we have $f'(x)=[f(x)]^2$. Show that $f(x)=0, \forall x \in \mathbb{R}$.","['calculus', 'derivatives', 'real-analysis']"
461944,recurrence relation Homework question 1,"This is a HW question Find the recurrent relations for $a_n, n\geq 0$ where $a_n$ is the number of $n$character upper case words that contain exactly one $A$ We are only required to find the relation and not solve it. I believe its: $a_0 = 1$ $a_1 = 26$ $a_2 = 25*26$ $a_3 = 25^2*26$ $a_4 = 25^3*26$ I guess I am wondering if I am right as Recurrence relation is a  very new topic for me.","['recurrence-relations', 'discrete-mathematics']"
461950,Limit of certain recurrence relation,"So given this recurrence relation (not how it was presented, but equivalent and much nicer) $$ x_{n+1} = \dfrac{x_n + nx_{n-1}}{n+1}; \ x_0 = 0,\ x_1 = 1 $$ I just can't find what the limit as $n$ goes to infinity is. It was straightforward to show it was convergent, and oscillatory about its limit, and I know the limit is going to be between $0.6$ and $0.75$ (through repeated calculation). The next step of finding the exact limit, though, I'm completely lost on.","['recurrence-relations', 'sequences-and-series', 'limits']"
461956,SLLN of Markov chains .,"Let $X_1$, $X_2$,... be a finite state, irreducible and aperiodic Markov chain with initial state $X_0=i$. It is known that 
\begin{equation}
\mathrm{P}\Big\{\lim_{n\rightarrow\infty} \frac{1}{n}\sum_{t=1}^n I\{X_t=j\}=\pi(j) \Big\} = 1,
\end{equation}
where $I(\cdot)$ is the indicator function and $\pi(j)$ is the stationary distribution of state $j$. Question: Is the above also true if the state of the Markov chain is observed at arbitrary time instances (for example recording only every second state transition of the chain $X_1, X_3, X_5,...$)?
To put more formally, I'd like to know if the following is also true :
\begin{equation}
\mathrm{P}\Big\{\lim_{n\rightarrow\infty} \frac{1}{n}\sum_{k=1}^n I\{X_{\tau(k)}=j\}=\pi(j) \Big\} = 1,
\end{equation}
where $\{\tau(k)\}$ is any infinite strictly increasing sequence of natural numbers.","['statistics', 'markov-chains', 'law-of-large-numbers', 'probability']"
461958,Show that $\lambda$ is eigenvalue of a normal $A$ if and only if $\bar \lambda$ is eigenvalue of $A^*$,I want to show that $\lambda$ is an eigenvalue of a normal matrix $A$ if and only if $\overline{\lambda}$ is an eigenvalue of $A^{*}$. I am trying to show it for a while and I guess there are some simple ideas to deal with it but I am just stuck. I know that a matrix is normal if and only if it is unitarily diagonalizable but I don't want to use this property cause I believe there must be other easier solution. Any advice?,"['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
461964,Proving a formula related with zeta function,"Could you show me how to prove the following formula?$$\sum_{n=1}^\infty\frac{\zeta (2n)}{2n(2n+1)2^{2n}}=\frac12\left(\log \pi-1\right).$$ In the 18th century, Leonhard Euler proved the following expression: $$\zeta (3)=\frac{2}{7}\pi^2\log 2+\frac{16}{7}\int_{0}^{\frac{\pi}{2}}x\log \left(\sin x\right)dx.$$ Note that $$\zeta (s)=\frac{1}{1^s}+\frac{1}{2^s}+\frac{1}{3^s}+\cdots=\sum_{n=1}^\infty\frac{1}{n^s}.$$
However, as far as I know, no one has been able to calculate this definite integral. By the way, I've known the following expression:
$$\int_{0}^{\frac{\pi}{2}}x\log \left(\sin x\right)dx=\frac{\pi^2}{8}\left(\log {\frac{\pi}{2}}-\frac12-\sum_{n=1}^\infty\frac{\zeta (2n)}{n(n+1)2^{2n}}\right).$$ I got interested in this infinite series, and I've just known the following similar formula without any proof:
$$\sum_{n=1}^\infty\frac{\zeta (2n)}{2n(2n+1)2^{2n}}=\frac12\left(\log \pi-1\right).$$ Then, my question is how to prove this formula. I suspect that the following expression might be used:$$\sin {\pi x}=\pi x\prod_{n=1}^\infty\left(1-\frac{x^2}{n^2}\right).$$
Though I've tried to prove this, I'm facing difficulty. I need your help.","['riemann-zeta', 'zeta-functions', 'number-theory']"
461967,Recurrence Relation Homework question 2,"This is a HW question. Find a recurrence relation for $b_n, n \geq 0 $where $b_n$ is the number of ways to partition $s= {1,2,3,...n}$ into exactly 2 subsets. I am looking for a hint on how to go about this question.","['recurrence-relations', 'discrete-mathematics']"
461990,Solving Recurrences Using Annihilators,"I've recently learnt how to use ""annihilators"" to find closed form solutions to recurrence relations. For instance, if I have the recurrence:
$$\begin{align}
f(0) &= 10 \\
f(n) &= 4 \, f(n-1), \; n \geq 1
\end{align}$$
I can use the operator $(E-4)$ which transforms $f(n)$ into the zero function $\forall \, n \geq 0$. I then use the operator's form to find the generic solution in a static table, which in this case is $\alpha \, 4^n$. Plugging this into the base case I get $\alpha=10$ which yields the closed form $f(n) = 10 \times 4^n$. This is great, but I have no idea why it works. Wikipedia turns this up, which is seemingly impenetrable to the uninitiated in abstract algebra. I can see that you treat the operator as a polynomial, and that its roots are what get plugged into the generic solution. But why?","['recurrence-relations', 'discrete-mathematics']"
462006,Essential Supremum vs. Uniform norm,"I just went to check something about the $||\cdot||_\infty$ norm and realized that it can perhaps refer to two quite different things. I'm coming at this from an Analysis class so I am use to having $||f||_\infty = ess sup |f(t)|$. But, according to wikipedia ( http://en.wikipedia.org/wiki/Supremum_norm ) the uniform norm is the same notation but is defined as $||f||_\infty = sup |f(t)|$. Unless I am really missing something, these are not equivalent. I'm a little uncomfortable with the possible ambiguity. How does one know which definition is actually being used in any one case? It seems like it is safe to say it is ess sup if anything explicitly references the $L^\infty$ space. I guesss I'm not sure what to ask. Can you verify that these are not necessarily equivalent? How do you differentiate which is being referred to?",['analysis']
462024,Recurrence Relation Homework Question 3,"This is a HW question Consider the set $T={A,B,C,1,2,3,4}$. For $ n\geq 0$ let $c_n$ be the number of n-character sequences of elements of T that contain no consecutive letters (distinct or identical) I believe there has to be two cases here. eg.. One where $a_{10}$ ends in a letter then $a_{11}$ has 4 choices for the 11th digit ${1,2,3 or 4}$ else if $a_{10}$ ends with a number then we can pick any element from the set T for the 11th character in $a_{11}$ Is my understanding of the question correct ? If it is could I get a hint on how to proceed.","['recurrence-relations', 'discrete-mathematics']"
462045,How to find the sum of this : $\sqrt{1+\frac{1}{1^2}+\frac{1}{2^2}}+ \sqrt{1+\frac{1}{2^2}+\frac{1}{3^2}}+\sqrt{1+\frac{1}{3^2}+\frac{1}{4^2}}+.....$,How to find the sum of the following : $\sqrt{1+\frac{1}{1^2}+\frac{1}{2^2}}+ \sqrt{1+\frac{1}{2^2}+\frac{1}{3^2}}+\sqrt{1+\frac{1}{3^2}+\frac{1}{4^2}}+.....+\sqrt{1+\frac{1}{1999^2}+\frac{1}{2000^2}}$ Please suggest as getting no clue on this... thanks..,"['sequences-and-series', 'algebra-precalculus']"
462047,injective endomorphisms of finite modules need not be surjective,"In the case of finite-dimensional vector spaces, an endomorphism is injective if and only if it is surjective. In the case of finitely generated modules over a commutative ring, if an endomorphism is surjective, then it is injective. I am wondering about the converse, which i suspect is not true. I am interested in counterexamples that give insight into why an injective endomorphism of a finite module is not necessarily surjective. Edit: at several occasions in abstract algebra injectivity and surjectivity appear to be dual notions. At other cases this duality breaks down. The case i am referring to seems to be one of these and i am interested in understanding what it is that makes one notion (surjectivity) ""more difficult"" to achieve than the other (injectivity).","['commutative-algebra', 'abstract-algebra']"
462075,How to solve the Riccati's differential equation,"I found this question in a differential equation textbook as a question The equation
 $$             
               \frac{dy}{dx} =A(x)y^2 + B(x)y +C(x)
$$
is called Riccati's equation show that if $f$ is any solution of the equation, then the transformation 
$$
                       y = f + \frac{1}{v}
$$
reduces it to a linear equation in $v$. I am not understanding this, what does $f$ mean? How can we find the solution with the help of the solution itself. I hope anyone could help me to solve this differential equation.",['ordinary-differential-equations']
462082,How to compute $\prod\limits^{\infty}_{n=2} \frac{n^3-1}{n^3+1}$,"How to compute
  $$\prod^{\infty}_{n=2} \frac{n^3-1}{n^3+1}\ ?$$ My Working : $$\prod^{\infty}_{n=2} \frac{n^3-1}{n^3+1}= 1 - \prod^{\infty}_{n=2}\frac{2}{n^3+1}
=  1-0 = 1$$ Is it correct","['infinite-product', 'sequences-and-series', 'algebra-precalculus']"
462118,"Prove the inequality $\,\frac{1}{\sqrt{1}+ \sqrt{3}} +\frac{1}{\sqrt{5}+ \sqrt{7} }+\ldots+\frac{1}{\sqrt{9997}+\sqrt{9999}}\gt 24$","Prove the inequality $$\frac{1}{\sqrt{1}+ \sqrt{3}}
 +\frac{1}{\sqrt{5}+ \sqrt{7} }+......... +\frac{1}{\sqrt{9997}+\sqrt{9999}} > 24$$ My work: Rationalizing the denominator gives $$\frac{\sqrt{3}-1}{2}  +\frac{\sqrt{7}-\sqrt{5}}{2}+......+\frac{\sqrt{9999}-\sqrt{9997}}{2} .$$ Now by taking two as common and separating the positive and negative terms gives $$\frac{1}{2} [ \{\sqrt{3} +\sqrt{7}+\dots +\sqrt{9999}\} - \{1+\sqrt{5} +\dots+\sqrt{9997}\}].$$ Can we do like this please suggest. Thanks.","['sequences-and-series', 'algebra-precalculus']"
462124,Is the MLE strongly consistent and asymptotically efficient for exponential families?,"It is known that the Maximum Likelihood Estimator (MLE) is strongly consistent and asymptotically efficient under certain regularity conditions. By strongly consistent I mean that $\hat{\theta}_{MLE} \rightarrow \theta$ almost surely. By asymptotically efficient I mean that $\sqrt{n}(\hat{\theta}_{MLE}-\theta)\rightarrow N(0,I^{-1}(\theta))$ in distribution. These regularity conditions are cumbersome to check so I was wondering if there is a general and easy to check case for when the regularity conditions hold. For example, do these regularity conditions always hold for exponential families? I am not asking anyone to prove this, I am just wondering if someone knows the answer. Regularity Conditions for Asymptotic Efficiency: http://en.wikipedia.org/wiki/Maximum_likelihood#Asymptotic_normality Regularity Conditions for Strong Consistency: http://en.wikipedia.org/wiki/Maximum_likelihood#Consistency","['statistics', 'convergence-divergence', 'probability']"
462148,Integration formula with wedge product.,"On $\mathbb{R}^3$, consider a compactly supported $2$-form
$$\omega = f_1 \, dx_2 \wedge dx_3 + f_2 \, dx_3 \wedge dx_1 + f_3 \, dx_1 \wedge dx_2.$$ Choose for $S$ the parametrization $h: \mathbb{R}^2 \to S$ defined by
$$h(x_1, x_2) = (x_1, x_2, G(x_1, x_2)).$$ The I start to lost at why we are doing this: Compute
$$h^* dx_1 \wedge dx_2 = dx_1 \wedge dx_2$$
$$h^* dx_2 \wedge dx_3 = -\frac{\partial G}{\partial x_1} dx_1 \wedge dx_2$$
$$h^* dx_3 \wedge dx_1 = -\frac{\partial G}{\partial x_2} dx_1 \wedge dx_2$$ So I get totally lost here And we emerge with the formula
  $$\int_s \omega = \int_{\mathbb{R}^2}(n_1 f_1 + n_2 f_2 + n_3 f_3) dx_1 dx_2,$$
  where
  $$\vec{n} = (n_1, n_2, n_3) = (-\frac{\partial G}{\partial x_1},-\frac{\partial G}{\partial x_2}, 1).$$ Can someone give me some explanation/intuition here? Thank you.","['differential-topology', 'multivariable-calculus']"
462151,recurrence relation homework question,"This is a homework question let $a_n$ number of n digit quaternary $(0,1,2,3)$ sequences in which there is never a$ 0 $anywhere to the right of a $3$. Solve for $a_n$ bot sure how to go about this. Could use help.","['recurrence-relations', 'discrete-mathematics', 'recursion']"
462152,to clear doubt about basic definition in graph theory,"Can anybody help me in clearing the doubt about hierarchical product of graphs.
Its quite different from other graph products. Here is the screenshot and link how it is done. I know the rooted graph, but its applied here by using 0 in definition and in adjacency relation is out of my mind.  I will be thankful if a little hint is provided. Thanks a lot. http://www.sciencedirect.com/science/article/pii/S0166218X08001960","['graph-theory', 'discrete-mathematics', 'algebraic-graph-theory', 'definition']"
462159,Is there an unbounded uniformly continuous function with a bounded domain?,"I tried to solve it by cases: domain is a set of numbers; domain is an interval,;domain is a union of numbers and some intervals. For the first case, I thought about arctanh is unbounded but its domain is bounded. To make it uniformly continuous, I can let Z be the domain. For the second case, I think there does not exist a function like this. For the third case, I am not sure if there exists a function satisfying all these conditions.. Did I think anything wrong for this question? or Could you give some idea or hint about that? Thanks!","['functions', 'continuity', 'real-analysis', 'uniform-continuity']"
462199,Why does factoring eliminate a hole in the limit?,"$$\lim _{x\rightarrow 5}\frac{x^2-25}{x-5}  = \lim_{x\rightarrow 5} (x+5)$$ I understand that to evaluate a limit that has a zero (""hole"") in the denominator we have to factor and cancel terms, and that the original limit is equal to the new and simplified limit. I understand how to do this procedurally, but I'd like to know why this works. I've only been told the methodology of expanding the $x^2-25$ into $(x-5)(x+5)$, but I don't just want to understand the methodology which my teacher tells me to ""just memorize"", I really want to know what's going on. I've read about factoring in abstract algebra, and about irreducible polynomials (just an example...), and I'd like to get a bigger picture of the abstract algebra in order to see why we factor the limit and why the simplified is equal to the original if it's missing the $(x-5)$, which has been cancelled. I don't want to just memorize things, I would really like to understand, but I've been told that this is ""just how we do it"" and that I should ""practice to just memorize the procedure."" I really want to understand this in abstract algebra terms, please elaborate. Thank you very much.","['factoring', 'calculus', 'intuition', 'limits']"
462216,What are applications of number theory in physics?,"I was reading Goro Shimura's The Map of My Life.
He wrote the following quote in the book.
It made me come up with the title question.
In particular, is there any application of the theory of modular forms in physics? A well known math-physicist Eugene Wigner was in our department,
  and so I occasionally talked with him. He was pompous
  and took himself very seriously. That is the impression shared by
  all those who talked with him. At a departmental party, he asked
  me what kind of mathematics I was doing. He asked that question
  as if he met me for the first time. At that time I had been a full
  professor and his colleague at least for six years, perhaps more. I
  told him vaguely, “Well, mainly things related to modular forms.”
  Then he said, “Oh, modular forms; we physicists don’t need such”
  in a very contemptuous tone.
  I can add that there are some physicists who are interested
  in modular forms. Once Edward Witten attended my graduate
  course on Siegel modular forms, and often asked sensible questions
  in class.","['physics', 'modular-forms', 'soft-question', 'number-theory']"
462239,"What is the smallest $n$ for which the usual ""counting sizes of conjugacy classes"" proof of simplicity fails for $A_n$?","A common proof of the simplicity of $A_5$ proceeds as follows. First, note that a normal subgroup is always a union of conjugacy classes. Also, a subgroup has order dividing the size of the group by Lagrange's theorem. Any normal subgroup will contain the conjugacy class with the identity. Now if we look at unions of conjugacy classes of $A_5$ where we include $\{(1)\}$ in the union, there only two cases where the size of the union divides the order $A_5$. These are the cases where the union is all of $A_5$ or just $\{(1)\}$. Hence $A_5$ must be simple. This same proof also works for $A_6$. But in general we cannot deduce the simplicity of $A_n$ for $n \geq 5$ with the same proof. According to this article it turns out that for example,  $1 + \frac{n(n-1)(n-2)}{3}$ divides $n!/2$ when $n = 68$. Note that $\frac{n(n-1)(n-2)}{3}$ is the size of the conjugacy class of $3$-cycles in $A_n$ when $n \geq 5$. Question: Is $n = 68$ the smallest positive integer where the proof fails? In the article they say that this ""seems to be the smallest counterexample"". However, counting all the possible sums of conjugacy classes of $A_n$ becomes computationally difficult very quickly (as is noted in the article). Hence actually demonstrating this might be hard.","['symmetric-groups', 'finite-groups', 'group-theory', 'abstract-algebra']"
462242,Prove the complement of a point in $\mathbb{A}^n$ is compact.,I am learning algebraic geometry and I came across the folowing question.  Prove that the complement of a point is compact in $\mathbb{A}^n$.  Does anyone know how to do this?,['algebraic-geometry']
462251,Regarding probability bound of flip coins,"Suppose you flip a fair coin 10,000 time how can you characterize the distribution of the occurrence of head? From the textbook, it says that $P[head>\frac{n}2 + k\sqrt{n}]$ < $e^{-{k^2/2}}$, why is that and what is the derivation? What theorem is this, we had only learn Bernoulli distribution and Chebyshev so far, it seem odd that the textbook would jump to such a conclusion without rigorous proof. Thanks in advance","['discrete-mathematics', 'statistics', 'probability-theory', 'probability-distributions', 'probability']"
462269,What is a directional derivative?,"I have encountered this in an online PDE course I'm following but I've never really been exposed to it. I've looked for the 'formal' definitions but I've never really understood any concept by looking at the formal, mathematical definition so can anyone elucidate this concept?",['multivariable-calculus']
462275,"What does ""topological dual of a Banach space"" mean?","I am not sure what does the ""topological"" imply. Thanks.","['functional-analysis', 'definition']"
462298,Determine the degree of the splitting field for $f(x)=x^{15}-1$.,"Let $f(x)= x^{15} - 1$ . Let $L$ be the splitting field of $f(x)$ over the field $K$ . Determine the extension degree $[L:K]$ in each case. a) $K= \Bbb{R}$ b) $K= \Bbb{Q}$ c) $K = \Bbb{F}_2$ d) $K = \Bbb{F}_{31}$ a) If we let $\Psi_d(x)$ denote the dth cyclotomic polynomial, then we have, $$x^{15}-1= \Psi_{d|15}(x) = \Psi(x) \Psi_3(x) \Psi_5(x) = (x-1)(x^2+x+1)(x^4+x^3+x^2+x+1)$$ So the roots of $f(x)$ are $1, \zeta_3, \zeta_3^2, \zeta_5, \zeta_5^2, \zeta_5^3, \zeta_5^4$ where $\zeta_k$ denotes the kth primitive root of unity. So the splitting field is $L=\Bbb{R}(\zeta_3, \zeta_5)$ . We know that $$[L:\Bbb{R}] = [L: \Bbb{R}(\zeta_5)][\Bbb{R}(\zeta_5): \Bbb{R}]$$ Since $x^4+x^3+x^2+x+1 \in \Bbb{R}[x]$ , we just need to check if we can factorize it into polynomials of lower degree. Of course, we cannot reduce it into polynomials of degrees 1 and 3, since we would then have a complex number in the reals. So we just need to check if we can reduce it into quadratic polynomials. Since $x^4+x^3+x^2+1=(x-\zeta_5)(x-\zeta_5^2)(x-\zeta_5^3)(x-\zeta_5^4)$ . By multiplying $(x-\zeta_5^t)$ and $(x-\zeta_5^w)$ for $w,t=0,1,2,3,4$ and $w \not= t$ , we find that we can reduce it into quadratics $(x-\zeta_5)(x-\zeta_5^4) = x^2 - (\zeta_5 + \zeta_5^4)x + 1$ and $(x-\zeta_5^2)(x - \zeta_5^3) = x^2 - (\zeta_5^3 + \zeta_5^2) + 1$ . We know that $\zeta_5 + \zeta_5^4$ and $\zeta_5^3 + \zeta_5^2$ are real but looking at the unit circle, because it is symmetric with respect to the horizontal axis, and so we end up on that axis. So $[L: \Bbb{R}(\zeta_5):\Bbb{R}]=2$ since the minimal polynomial over $\Bbb{R}$ is $x^2-(\zeta_5+\zeta_5^4)x+1$ . Now we need to compute $[L:\Bbb{R}(\zeta_5)]$ . Since $x^2+x+1$ is the minimal polynomial for $\zeta_3$ over $\Bbb{R}$ and since $\Bbb{R} \subseteq \Bbb{R}(\zeta_5)$ , we just need to check if $x^2+x+1$ has any linear factors in $\Bbb{R}(\zeta_5)$ . However, that would imply $\zeta_3 \in \Bbb{R}(\zeta_5)$ , which is not true. So $[L:\Bbb{R}]=4$ b) We just do the same thing, except that we cannot reduce the polynomial $x^4+x^3+x^2+x+1$ , because $x^2-(\zeta_5 + \zeta_5^4) + 1 \not\in \Bbb{Q}[x]$ , since $$\zeta_5 + \zeta_5^4 = \cos(2\pi/5) + i\sin(2\pi/5) + \cos(8\pi/5) + \sin(8\pi/5) = 2\cos(2\pi/5).$$ So $[L:\Bbb{R}]=8$ . c) For $x^4+x^3+x^2+x+1$ , we check if it has linear factors in $\Bbb{F}_2$ by plugging in 0 and 1. Since there aren't any, we just need to check if it has a quadratic as a factor. Since $x^2+1=(x+1)^2$ , we just need to check if $x^2+x+1$ is a factor, and we find that it isn't. For $x^2+x+1$ (polynomial for $\zeta_3$ ), we just need to check if it has linear factors, and it doesn't. So $[L:\Bbb{F}_2]=8$ d) I can do what I did with $\Bbb{F}_2$ , but that seems like it would take a long time, since there are 31 elements. It will also take a very long time trying to figure out the irreducible quadratics to see if they would divide $x^4+x^3+x^2+x+1$ . So I was wondering if there was a simpler way to do this or if I had to do it the long way... Are the other three correct? Thanks in advance","['finite-fields', 'extension-field', 'abstract-algebra', 'cyclotomic-polynomials', 'field-theory']"
462300,How to show that every prime ideal is a maximal ideal if for all $a \in R$ there exists $b \in R$ such that $a^2b=a$.,Here is the full statement of the question (I thought it was a bit too long for the title). Given a commutative ring $R$ with $1 \neq 0$ such that for all $a \in R$ there exists $b \in R$ such that $a^2b=a$. Show that in $R$ every prime ideal is maximal. I was trying to show that every ideal wich has a prime ideal as a strict subset is $R$. I was trying to do this by showing that $1$ would be an element of such an ideal. Thanks!,"['commutative-algebra', 'abstract-algebra']"
462308,Partial derivative equality,"I'm working through Colley's Vector Calculus 3rd edition to reacclimate and introduce myself to all things vectors and differential forms.  The following question is stumping me.
Suppose $z=f(x,y)$ has continuous partial derivatives.  Let $x=e^r\cos \theta$ and $y=e^r\sin\theta$.  Show that
$$\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)^2=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{r}}\right)^2+\left(\frac{\partial{z}}{\partial{\theta}}\right)^2\right]$$
So I almost have the answer but I keep getting a pesky $\cos^2\theta$.
$$e^{-2r}\left[\left(\frac{\partial{z}}{\partial{r}}\right)^2+\left(\frac{\partial{z}}{\partial{\theta}}\right)^2\right]=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{x}}\cdot{\frac{\partial{x}}{\partial{r}}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\cdot{\frac{\partial{y}}{\partial{\theta}}}\right)^2\right]=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{x}}\cdot{e^r\cos\theta}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\cdot{e^r\cos\theta}\right)^2\right]=\cos^2\theta\left[\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)\right]^2$$
Where is my error?","['multivariable-calculus', 'partial-derivative']"
462309,Double of Riemannian manifold.,"Let $M$ be a Riemannian manifold with totally geodesic boundary $\partial M$. We consider its double $\check{M}$, i.e. the disjoint union of $M$ with itself under identification of corresponding boundary points. It is well known that $\check{M}$ is a smooth manifold. Can you tell me whether the metric is still smooth along the boundary or else what is its degree of regularity?
Can you maybe also tell me what it means for the boundary to be an isoperimetric surface for $M$?","['riemannian-geometry', 'differential-geometry']"
462318,Possible values of difference of 2 primes,"Is it true that for any even number $2k$, there exists primes $p, q$ such that $p-q = 2k$? Polignac's conjecture talks about having infinitely many consecutive primes whose difference is $2k$. This has not been proven or disproven. This is a more general version of my question on the possible value of prime gaps . Of course, the odd case is easily done.","['prime-numbers', 'number-theory']"
462329,Calculating success chance from algorithm,"Not super sure this is the right *exchange for this question, but here we go. Let's say I'm writing a game, and in this game the player may attack another unit. The chance of hitting is an ""opposed roll"" that looks something like this: An attack lands if: (attack * random() + attack * random())
is greater than or equal to
(defense * random() + defense * random()) Where attack and defense are both numbers > 0 and random() returns a (pseudo) random number between 0 and 1 (not including 1). The reason it looks like it does is to ensure that there is a bias towards the ""average value"" (or rather, towards double the average value, but it doesn't really matter). How would one go about calculating the ""hit chance"" for this algorithm (in order to display it to the user)? Thanks!",['statistics']
462356,Integral solution of a differential equation (verification),"I have to verify that the integral $$y(x) = \int_0^\infty \exp\left(-t - \frac{x}{\sqrt{t}}\right) dt$$ satisfies the ODE $$xy''' + 2y = 0$$ ($x > 0$). Differentiating under the integral sign three times gives $$y''' = \int_0^\infty \frac{1}{t \sqrt{t}}\exp\left(-t - \frac{x}{\sqrt{t}}\right)$$ In this form it's not obvious that this satisifes the above equation. I guess I need to use integration by parts to simplify the above integral but I can't quite see how that works. If I let $u = \exp(-t - \frac{x}{\sqrt{t}})$ and $v' = \frac{1}{t \sqrt{t}}$ then $v = \frac{-2}{\sqrt{t}}$ and $u' = \left(-1 + \frac{x}{2t\sqrt{t}}\right)\exp(-t - \frac{x}{\sqrt{t}})$ and the above becomes $$y''' = \int_0^\infty \frac{-2}{\sqrt{t}} \left(-1 + \frac{x}{2t\sqrt{t}}\right)\exp\left(-t - \frac{x}{\sqrt{t}}\right) \, dt$$ and this doesn't seem to work. Can anyone provide me with help? Hopefully I'm not just making a mistake with the differentiation/integration.","['ordinary-differential-equations', 'calculus']"
462364,"How can I show that $\sup(AB)\geq\sup A\sup B$ for $A,B\subset\mathbb{R}$ where $A\cup B$ is positive and bounded?","The question is based on the following exercise in real analysis: Assume that $A,B\subset{\Bbb R}$ are both bounded and $x>0$ for all $x\in A\cup B$. Show that $$
\sup(AB)=\sup A\sup B
$$
  where 
  $$
AB:=\{ab\in{\Bbb R}:a\in A, b\in B\}.
$$ Since $0<a\leq\sup A$ and $0<b\leq\sup B$ for all $a\in A$ and $b\in B$, we have
$$
ab\leq\sup A\sup B
$$
for all $ab\in AB$ which implies that $\sup AB\leq\sup A\sup B$. I have trouble with another direction:
$$
\sup AB\geq\sup A\sup B
$$
I was trying to show that for every $\epsilon >0$, $\sup AB-\epsilon \geq \sup A\sup B$. If one uses the definition of supremum, one has the estimates that for every $\epsilon>0$,
$$
\sup A-\epsilon\leq a, \quad \sup B-\epsilon\leq b
$$
for some $a\in A,\ b\in B$. It follows that
$$
\sup A\sup B\leq (a+\epsilon)(b+\epsilon)=ab+\epsilon(a+b)+\epsilon^2\leq \sup AB+\epsilon (a+b)+\epsilon^2
$$
which seems quite close to what I want. How can I go on?",['real-analysis']
462372,How to restrict Lagrange multiplier on positive values?,"Here's the function that i want to optimize: $$f(x,y) = x-2y$$ and the constraint is: $$g(x,y) = x^2 + y - 10 = 0$$ Solving with Lagrange multiplier I get: $$F(x,y,\lambda) = x-2y - x^2\lambda - y\lambda + 10\lambda$$ $$F_x = 1 - 2x\lambda = 0$$
$$F_y = -2-\lambda = 0 \iff \lambda = -2 $$
$$F_\lambda = x^2 + y - 10 = 0$$ Then: $$1+4x = 0$$
$$4x = -1$$
$$x = -\frac 14$$ $$\frac1{16} + y - 10 = 0$$
$$y = \frac{159}{16}$$ So I found one stationary point and obviously it's the minimum of the function and it's: $$f(-\frac{1}{4},\frac{159}{16}) = -20,125$$ At these conditions this function doesn't have maximum, but if we restrict x and y only to positive values we should be able to obtain one. How can we do that?","['calculus', 'derivatives', 'functions', 'lagrange-multiplier']"
462376,Does $\pi$ satisfy the law of the iterated logarithm?,"It is widely conjectured that $\pi$ is normal in base $2$. But what about the law of the iterated logarithm ? Namely, if $x_n$ is the $n$th binary digit of $\pi$, does it seem likely (from computer experiments for example) that the following holds? $$\limsup_{n\rightarrow\infty} \frac{S_n }{\sqrt{n\log\log n}}=\sqrt{2}\quad\text{where}\quad S_n=2(x_1 + \ldots + x_n) - n$$ What about other (conjectured) normal numbers like $e$ and $\sqrt{2}$? I am sorry if this is too easy, but I tried to search for it and I could not find in on the Internet.  I suppose I could run an experiment myself, but I assumed this is well known, and I would need to brush up on my programming skills to do so... Update 8/9/2013: I found a website with the first 32,000 binary digits of $\pi$ and (using a spreadsheet program) graphed out the average of the bits $S_n/n$, comparing it to $\sqrt{\frac{2 \log \log n}{n}}$.  The results were inconclusive.  The average never got close to $\sqrt{\frac{2 \log \log n}{n}}$ (except at the very beginning when it was way past it). However, I had the same result with a source of randomness (the one built into the spreadsheet program).  My conclusion is that 32,000 bits is not enough to see if the law of the iterated logarithm (experimentally) holds for $\pi$. ( The picture in the Wikipedia article uses at least $10^{50}$ bits, and the pattern is clear at about $10^{12}$ bits.  However, I don't know where to get even 1,000,000 binary digits of $\pi$ on the Internet. [End Update] Also, I am sorry that I really don't know how to properly tag this.","['number-theory', 'statistics', 'random', 'experimental-mathematics', 'pi']"
462389,"Definite Dilogarithm integral $\int^1_0 \frac{\operatorname{Li}_2^2(x)}{x}\, dx $","Prove the following $$\int^1_0 \frac{\operatorname{Li}_2^2(x)}{x}\, dx = -3\zeta(5)+\pi^2 \frac{\zeta(3)}{3}$$ where $$\operatorname{Li}^2_2(x) =\left(\int^x_0 \frac{\log(1-t)}{t}\,dt \right)^2$$","['special-functions', 'integration', 'definite-integrals', 'harmonic-numbers', 'polylogarithm']"
462392,Eigenvalue of linear operator iff eigenvalue of matrix representation.,"I'm trying to prove the following theorem, which seems straightforward enough, but I'm confused about the wording and proving the converse: Let T be a linear operator on a finite-dimensional vector space V, and let B be an ordered basis for V.  Prove that t is an eigenvalue of T iff t is an eigenvalue of the matrix representation of $T_B$.  (Sorry; I don't know how to make the subscript of B with [T].) Here's my confusion:  don't we have to have that V is a vector space consisting of column vectors in order for us to even speak of eigenvalues and eigenvectors of matrices?  This isn't specified in the theorem. Moreover, even if this were specified, I'm not sure how to get from the matrix multiplication form back to the linear operator form with my proof.  Any suggestions?","['linear-transformations', 'matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
462404,Regarding calculating the bias of coin with uncertainty,"Suppose you have a coin that you flip $n$ times and the result have $m$ heads and $n-m$ tails. How accurate can you predict the bias of the coin to be $\frac m n$? I know that $P\left[\text{head}>\frac{n}2 + k\sqrt{n}\right]$ < $e^{-k^2/2}$ where $k$ should be the real bias, but how would you set a bound in which you know just the experiment bias? Any hint would be much appreciated.","['discrete-mathematics', 'statistics', 'probability-theory', 'probability-distributions', 'probability']"
462409,Is there a proof that says that an operation that can take a transcendental number and make it an integer cannot exist?,"Motivation: To get an integer to become a different integer, you have to add or subtract another integer, e.g. $1+2=3$ To get a rational number to become an integer, you have to multiply by the denominator, e.g. $\frac{3}{4}(4) = 3$ To get an irrational number to become an integer, you have to raise it to some power, e.g. $(\sqrt{3})^2 = 3$ From my understanding, transcendental numbers are numbers where there is no operation ( done in one step ) that can bring a transcendental number back into the integer realm; I know it is easy to get an integer from a transcendental number in two operations/steps, e.g. $e+e=2e\quad \frac{2e}{e}=2$ Is this because (1) no operation exists in mathematics (because 'mathematics transcends the human mind' - pun intended) or (2) because no operation known to man exists? I know that there is some ambiguity in that question so I will try my best clarify: by option 1, I mean is there a proof that says that such an operation cannot exist and by option 2, I mean no such proof exists.","['transcendental-numbers', 'number-theory']"
462434,Where is the mistake in this argument that $(\sqrt8)^{\sqrt 7} >(\sqrt7)^{\sqrt 8}$?,"I posted an answer in this question to prove that $(\sqrt8)^{\sqrt 7}<(\sqrt7)^{\sqrt 8}$ I started with $$f(x)=\frac{\ln x}{x}$$ $$f'(x)=\frac{1-\ln x}{x^2}$$ $$f'(x)>0 : x\in)0,e($$  $$f'(x)<0 : x\in)e,+\infty($$ so $f(1/7)>f(1/8)$, hence $-8\ln 8<-7\ln7$ so $$\begin{align}
8\ln 8 & > 7\ln7\\\\
0.5\cdot 8\ln 8 & >0.5\cdot 7\ln7\\\\
8\ln \sqrt8 & > 7\ln\sqrt7\\\\
\ln (\sqrt8)^{\sqrt 7}& >\ln(\sqrt7)^{\sqrt 8}\\\\
(\sqrt8)^{\sqrt 7} &>(\sqrt7)^{\sqrt 8}
\end{align}$$ but where is the mistake with my answer, because $(\sqrt8)^{\sqrt 7}<(\sqrt7)^{\sqrt 8}$?!","['inequality', 'algebra-precalculus']"
462444,"How to find Find $\left\lfloor\sum_1^{100}\frac{1}{x_n+1}\right\rfloor$ with $x_1 =\frac{1}{2}, x_{k+1} =x_k^2+x_k$.","The sequence $\{x_n\}$ is defined by $x_1 =\frac{1}{2}, x_{k+1} =x_k^2+x_k$. Find $$\left\lfloor\frac{1}{x_1+1}+\frac{1}{x_2+1}+\cdots+\frac{1}{x_{100}+1}\right\rfloor$$ where $\left\lfloor\dots\right\rfloor$ is greatest integer function. If we put the values of $k$ then we get the numerator part of the series =2 and then taking 2 as common from the series how can we solve the rest of the series.. How do we proceed in this case ... please suggest thanks..","['sequences-and-series', 'algebra-precalculus']"
462445,What's the probability that an NFL team with a given win/loss record makes the playoffs?,"For example, if I know that a team has a record of 11 wins and 5 losses, but no nothing about the records of any other teams, what is the probability that this team makes the playoffs? The current (simplified) NFL rules for playoffs are that the team must either have the best record in its division (three other teams), OR have one of the best two records among teams in the conference that did not win the division. So I suppose what I'm trying to figure out is: What is the probability that at least one out of three teams has a better win-loss record? This is easy if I assume that the win/loss records of each team in the division are independent (they aren't because each team plays each team in the division twice), but how inaccurate will that make the final answer? What is the probability that at least two other teams had a better win/loss record, but did not win their division? I really have no idea where to begin with this. Again, the odds of two teams having a better win/loss record is easy, but I'm not sure how to incorporate them winning their division. (I mentioned 11-5 as an example record, but really I'm trying to solve this in general for any number of wins/losses, teams in a division, divisions, and teams in a conference)","['statistics', 'probability']"
462454,What are polynomial like expressions that have complex numbers for the exponents?,"Got curious about polynomials and Galois theory the other day and realized I have no idea how current mathematics treats polynomials (or rather polynomial like expressions) that have arbitrary algebras for the exponents. A quick search yields polynomial extensions like Laurent polynomials but I couldn't find anything that uses any other groups for the exponent, like Gaussian integers, complex numbers, cyclotomic rings, hypercomplex numbers and other algebras. What are these structures called and how is their behavior similar and different with ordinary polynomials?","['abstract-algebra', 'polynomials']"
462469,"An example of a group, a subgroup and an element, satisfying a given condition. [duplicate]","This question already has answers here : Conjugate subgroup strictly contained in the initial subgroup? (4 answers) Closed 10 years ago . Is there a group $G$, a subgroup $H$ and an element $x$, such that $xHx^{-1} \subset H$, but  $xHx^{-1} \neq H$? Thanks in advance.","['group-theory', 'abstract-algebra']"
462481,Intersection of Projections on Hilbert space,"Let $H$ be a Hilbert space, on which $P,Q$ be projection operators. Let $S:=\mathcal{R}(P)\cap\mathcal{R}(Q)$ be the intersection of ranges, then it is easy to show the orthogonal complement $S^{\bot}$ is an invariant subspace of $PQP$. The question is how to show $||PQP|_{S^{\bot}}||<1$ , if $S^{\bot}$ is finite-dimensional? Sorry to have modified the problem. I made mistake and found out that if $PQ=QP$, which means $PQ$ is also projection on $H$, then $PQP$ shall vanish, but generally it does not, and should have the above norm estimate but I am not sure how to get it anyway.",['functional-analysis']
462501,Show that $\int_{0}^{1}x^{m}(1-x)^{n}dx=\int_{0}^{1}x^{n}(1-x)^{m}dx$,"I am trying to show that:
$$\int_{0}^{1}x^{m}(1-x)^{n}dx=\int_{0}^{1}x^{n}(1-x)^{m}dx$$
For any positive integers $n$ and $m$. Which is true if I try to evaluate it numerically. I tried to use the binomial theorem, but then I end up with:
$$
\int_{0}^{1}
\sum_{i=0}^{n}
\begin{bmatrix}
n \\ 
i
\end{bmatrix}x^{m+n-i}
dx
=
\int_{0}^{1}
\sum_{i=0}^{m}
\begin{bmatrix}
m \\ 
i
\end{bmatrix}x^{m+n-i}
dx
$$ There is a term $x^{m+n-i}$ on both sides which looks like there should be something I can do with it to simplify but then I am stuck. This is an exercise following Apostol's Calculus I Chapter 5 where integration by substitution is discussed, but it is not clear how to use integration by substitution here.","['calculus', 'integration']"
462514,Applying a linear operator to a Gaussian Process results in a Gaussian Process: Proof,"In this paper , it is stated without proof or citation that ""Differentiation is a linear operation, so the derivative of a Gaussian process remains a
Gaussian process"".  Intuitively, this seems reasonable, as the linear combination of Gaussian random variables is also Gaussian, and this is just an extension to the case where instead of a vector-valued random variable we have a random variable defined on a function space.  But I cannot find a source with a proof and the details of a proof elude me. Proof Outline Let $x(t)\sim \mathcal{GP}(m(t),k(t, t^\prime))$ be a Gaussian process with mean function $m(t)$ and covariance function $k(t, t^\prime)$ , and $\mathcal{L}$ a linear operator. For any vector $T=(t_1,...,t_n)$ , let $x_T=(x(t_1),...,x(t_n))$ .  Then $x_T\sim \mathcal{N}(m_T,k_{T,T})$ . Now consider the stochastic process $u(t)=\mathcal{L}x(t)$ .  It suffices to show that the finite dimensional distributions of $u(t)$ are Gaussian, but translating the action of the linear operator on $x(t)$ to the finite dimensional case is giving me trouble. In the case of differentiation, we have $u(t)=\mathcal{L}x(t)=\frac{dx}{dt}=\lim_ {h\rightarrow 0}\frac{x(t+h)-x(t)}{h}$ . For all $h>0$ , the random variable $v(t)=\frac{x(t+h)-x(t)}{h}$ is normal, and by interchanging integration and the limit, we have $$
\begin{array}{rcl}
m_u(t)&=&E\left(\lim\limits_{h\to 0}\frac{x(t+h)-x(t)}{h}\right)\\
&=&\lim\limits_ {h\to 0}E\left( \frac{x(t+h)-x(t)}{h}\right)\\
&=&\lim\limits_ {h\to 0}\frac{m(t+h)-m(t)}{h}\\
&=&m^\prime(t)
\end{array}$$ Of course, we need to verify when this interchange is appropriate.  Similarly, we can intuit the covariance function of $u(t)$ has the form $$
k_u(t,t^\prime)=\frac{\partial^2 x}{\partial t\partial t^\prime }k(t,t^\prime)
$$ but I am having  a hard time making the leap from finite approximations to the infinite-dimensional case. Reference Request If there is any textbook or paper that does more than mention this fact in passing, please let me know.","['probability-theory', 'stochastic-processes', 'normal-distribution', 'reference-request']"
462519,"When solving a DE, what should our premises be?","To solve a a DE rigorously, I'm not really sure how to set up the problem, in terms of definitions and premises. So as a simple example, suppose we're given a  function $f : \mathbb{R}^2 \rightarrow \mathbb{R},$ and we wish find the general solution to the first-order DE of the form $y'=f(x,y).$ My questions are as follows. Should we immediately define a function $S$ that takes a set $X$ and returns the set of all solutions on $X$, denoted $S_X$? In the sense of: $S_X = \{y : X \rightarrow \mathbb{R} \mid y'=f(x,y), \;y \mbox{ diff}\}.$ Should we assume that we have a set $X$ (on which we're looking for solutions) that is fixed but arbitrary? If so, should we furthermore assume that $X$ is an interval? That its open? Does it need to be non-empty and/or have two or more elements? If the answer to any of the above questions is 'yes,' should we furthermore assume that we have a function $y : X \rightarrow \mathbb{R}$ that is fixed but arbitrary? If so, should we furthermore assume that $y$ is differentiable? That $y$ satisfies the DE? Note that, if the answer to this final question is 'yes', we can write this more simply as ""Assume $y \in S_X$"" so long as we've defined $S_X$. Is there anything else that needs to be done at the outset?",['ordinary-differential-equations']
462524,Step in a solution of $y^2 = x^3 - 2$,"I am reading Algebraic Number Theory notes here by Keith Conrad. In page 9, there is a solution of $y^2=x^3-2$ using unique factorization in $\mathbb{Z}[\sqrt{-2}]$. We start by writing $x^3=y^2+2=(y+\sqrt{-2})(y-\sqrt{-2})$. By some work, it is shown that $y$ is odd, and that only common divisors of $y+\sqrt{-2}$ and $y-\sqrt{-2}$ are $\pm 1$. My question is about the following step: By unique factorization in $\mathbb{Z}(\sqrt{-2})$,
  $y+\sqrt{-2}=u\alpha^3$ where $u$ is a unit. Why does this step follow? Well, intuitively because $(y+\sqrt{-2})(y-\sqrt{-2})=x^3$ is perfect cube, and the factors $y+\sqrt{-2}$ and $y+\sqrt{-2}$ have no common divisors (other than $\pm 1$), so each factor must be a cube on its own (up to some unit). But how do we rigorously prove what I just said? Thanks for your time.","['diophantine-equations', 'algebraic-number-theory', 'abstract-algebra']"
462534,Recognizing uppersemicontinuous function as a pointwise decreasing limit.,"Let $X$ be a compact metric space and $f:X\rightarrow \mathbb{R}$ be upper semicontinuous.  Then why is it that $f$ is the pointwise decreasing limit of continuous functions? My attempt has been to use totally boundedness of $X$ to get a finite cover by $1/n$ radius balls $B_{i, n}$.  Then find a subordinate continuous partition of unity $\phi_{i, n}$ where i runs over a finite index set.  Then let $g_n=\sum_{i}sup f|_{B_{i,n}} * \phi_{i, n}$. $g_n$ is continuous and at least $f$.  Therefore, the same holds of $f_n=min(g_1, ... g_n)$.  Furthermore, the last sequence is decreasing.  Why does $f_n$ converge to f pointwise? (Or you can suggest a different method of proof entirely.)  Towards this I was hoping for perhaps a proof of some sort of ""uniform upper semicontinuity"" in analogy to uniform continuity which follows from continuity on $X$, but that seems hopeless.","['general-topology', 'metric-spaces', 'real-analysis', 'analysis']"
462544,Homework: closed 1-forms on $S^2$ are exact.,"From the 2008 UCLA Geometry-Topology qualifying exam:
let $\theta$ be a $1$-form on $S^2$ with $d \theta = 0$. Construct a function $f$ on $S^2$ with $d f = \theta$. I'm not very confident in my ability to answer even a basic problem like this properly, and I'd appreciate someone telling me if I'm mistaken in my reasoning. I argued as follows:
let $U$ be the subset $S^2\setminus\{\text{south pole}\}$ and $L=S^2\setminus\{\text{north pole}\}$.
      Since these subsets are diffeomorphic to $\mathbb{R}^2$ via stereographic projection, the restriction of $\theta$ to either one of $U$ or $L$ is exact.
      Thus there exist $f_U$ and $f_L$ so that $d f_U = \theta , d f_L = \theta$ on $U,L$ respectively. On the intersection $U\cap L$ we have $d f_U = d f_L$, that is $d(f_U-f_L) = 0$.
      This forces $f_U = f_L + c$ for some constant $c$ on their common intersection.
      The existence and choice for $f$ are now apparent: let $f=f_U$ on $U$ and $f(\text{south pole}) = f_L(\text{south pole})+c$.","['differential-forms', 'differential-geometry']"
462550,How prove that $P_{2n}<(n+1)^2$,"the nth prime by $P_{n}$, prove or diprove 
$$P_{2n}<(n+1)^2$$
$$P_{2}=3<4,P_{4}=7<9,P_{6}=13<16\cdots $$ It is well know that
$$P_{n}<2^{2^n}$$ see: http://www.maths.tcd.ie/pub/Maths/Courseware/PrimeNumbers/Primes-I.pdf Thank you everyone",['number-theory']
462569,Prove：$x^d-1 \mid x^n-1$ iff $d \mid n$.,"Consider the polynomial ring $F\left[x\right]$ over a field $F$ . Let $d$ and $n$ be two nonnegative integers. Prove： $x^d-1 \mid x^n-1$ iff $d \mid n$ . my tries: necessity, Let $n=d t+r$ , $0\le r<d$ since $x^d-1 \mid x^n-1$ , so, $x^n-1=\left(x^d-1\right)\left(x^{\text{dt}+r-d}+\dots+1\right)$ ... so,,, to prove $r=0$ ? I don't know, and I can't go on.
How to do it.","['linear-algebra', 'polynomials']"
462571,On the extension of distribution,"Define a distribution on $(0,+\infty)$ by
$$u(\varphi):=\sum_{k=1}^{\infty} {1 \over {k!}}\partial^k \varphi(1/k)$$
how can I show it cannot be extended to any distribution defined globally on $\mathbb{R}$?","['distribution-theory', 'functional-analysis']"
462574,"Learn about valuations, valuation rings, value group","I am reading a paper for a summer research project ( Example of an interpolation domain ). I am unfamiliar with some of the terms used here and I have tried searching on google for definitions but I am a little confused. I am not sure if the definitions I am finding pertain to what I am looking at. The passage that I am at states: ... we take $v(r)$ to be the smallest exponent on a nonzero term of $r$. Denote the valuation ring of $v$ by $V$; so that, to be in $V$, a series must have no (nonzero) terms with negative exponents. Because $V$ has value group $G$, its maximal ideal is idempotent, and hence ... (The quote comes from the bottom of the first paragraph in Example 1 on page 2.) We are doing long division of polynomials and writing them as a kind of Laurent series and each non-zero element of the fraction field has a unique kind of ""Laurent expansion."" I have worked a little bit with valuations before but in what seemed to be kind of informal as the definition that I worked with then I have not seen repeated in my searches recently. Specifically, I worked with the $p$-adic valuation on $\mathbb{Z}$ defined by $v_p(x)=p^{-n}$ where $x=p^nb$, $p\nmid b$ for $x\neq 0$ and $v_p(x)=0$ if $x=0$. I just want to learn more about valuations, valuation rings, value groups, and an ideal being idempotent. From what I gather, you define a valuation on a field and then this gives rise to a valuation ring. The definitions of valuation that I have looked at involve $\infty$ which confuses me a little bit as it looks like each non-zero element gets mapped to a rational number in this case, as the exponents are rational. Looking at when I worked with the $p$-adic valuation on $\mathbb{Z}$, we never mapped $0 \mapsto \infty$. Maybe there is material that is available online I can look at to work through elementary results with valuations and valuation rings. Any advice on places I can go to find more information about these things? Thanks very much.","['valuation-theory', 'abstract-algebra']"
462588,Example of commuting vector fields generating globally noncommuting flows,"Recently, I discovered that a theorem from my differential geometry lecture is false due to too big generality - it stated that for vector fields $X,Y$ we have the equivalence: Incorrect! $[X,Y] = 0$ $\iff$ the associated flows $h^X$, $h^Y$ commute. The version from wikipedia says that the flows commute locally . Question I suggested above, I have a counterexample disproving the theorem as it was stated in my lecture. But it is a noncompact example and formally global flow does not exist. I'd like to see a counterexample with a compact manifold or at least with compact supports of both vector fields. My counterexample (not compact) Consider the manifold $M$ parametrised by $f: (-\pi,\pi) \times \mathbb R_+ \to \mathbb R^3$:
$$f(\phi,r) = (r\cos (2\phi), r\sin (2\phi), \phi).$$ One can easily see, that locally (it is enough to consider an arc (in $(-\pi,\pi) \subseteq S^1$) shorter than half of the circle) this manifold is a graph of some function $g'$ (depending on the local neighbourhood) of the first two coordinates $(x,y)$ of $\mathbb R^3$. Let $g$ be defined as follows: $g(x,y)=(x,y,g'(x,y))$ - it is a local parametrisation of $M$. We can find a cover $\{U^i\}_{i\in \{1,2,3\}}$ of $M$ such that for each $U^i$ there exists a parametrisation $g^i$ as above (for appropriate $\{U^i\}$ domain of $g^i$ may be chosen as $\mathbb R^2 \setminus h^i$, where $h^i$ is a halfline starting at $0$). Now: let
$X_{|U^i} = g^i_*\left(-\frac{\partial}{\partial x}\right)$ and 
$Y_{|U^i} = g^i_*\left(\frac{\partial}{\partial y}\right)$
for all $i$ - we can easily see that the definition is correct. Moreover - it is clear that locally the associated flows commute (locally we are in $\mathbb R^2$ and the vector fields come from the (standard) coordinate system). Ok, time for the party. Take point $f(-\frac{\pi}{8},\sqrt 2)=(1,-1,-\frac{\pi}{8})$. Go along the flow of $Y$ for $\Delta t=2$ (you end up at $(1,1,\frac{\pi}{8})$) and then for the same time along the flow of $X$ - you will stop at point $(-1,1,\frac{3\pi}{8})$. On the other hand, if you go first along the flow of $X$ and then $Y$, you will end up at $(-1,1,-\frac{5\pi}{8})$. Update A funny thing is that manifold $M$ from the above example is diffeomorphic with the plane. It suggests that the theorem for my lecture is very far from being true (it is false even in the domain of one chart!). If we have a method of extending a pair of commuting vector fields from a disc to a whole manifold , we would be done. Maybe that's the right way? Also note that we don't need $r\in\mathbb R_+$ for the counterexample, a finite interval is sufficient (and similarly for $\phi$), so we can modify $X,Y$ near the borders of our disc.","['differential-topology', 'ordinary-differential-equations', 'vector-fields', 'differential-geometry']"
462592,Trouble with gradient intuition,"I'm currently learning about gradients, and I thought khanacademy could help me acquiring some intuition. The actual computation is clear to me, however I'm having trouble understand the intuition. This is the video I'm talking about, and the part I have a part with is 6:11. He says the gradient vector shows the direction you have to travel in the x,y-plane in order to get a maximum slope in the z-direction. This sounds like gibberish to me. How do you have to 'travel' it? Why do you get a maximum slope? I just have no clue.",['multivariable-calculus']
462594,Closed point and base change of scheme.,"Let $k$ be a field and $K/k$ be a field extension. For a scheme $X$ of finite type over $k$, denote $X_K:=X\times_k \text{Spec}K$. Let $x\in X$ be a closed point and $x'\in X_K$ be a point lying over $x$. In this situation, is $x'$ also a closed point? (This is true for $K/k$ is purely inseparable extension since two schemes are homeomorphic.)",['algebraic-geometry']
462598,Prove that the function is surjective but not injective,"I am struggling with this excercise: I want to prove that the function $f: \mathbb{R} \to \mathbb{R}$, defined by $f(x)= x^3 + x^2 - 6x$, is surjective but not injective? I personally would calculate some numbers and show that by these examples that this function cannot be injective. Is this way a correct way to prove this? I appreciate your answer!",['functions']
462619,Use graphs and standard triangles to evaluate $\sin(\frac{11}{6}\pi)$,Use graphs and standard triangles to evaluate $\sin(\frac{11}{6}\pi)$. I end up with $\sin(\pi + \frac{5}{6}\pi)$ which I can't use standard triangles on.,['trigonometry']
462622,"How to prove two topologies $\mathcal{T}_1,\mathcal{T}_2$ are not equal","Let $C[0; 1]$ be the set of all continuous real-valued functions on $[0; 1]$. (i) Show that the collection $M$, where $M = \{M(f,\varepsilon ) : \text{$f\in C\left[0; 1\right ]$ and  $\varepsilon $ is a
positive real number}\}$ and $M(f,\varepsilon) =\{g : \text{$g\in C\left[0; 1\right ]$ and
$\int_{0}^{1}\left|f-g\right| < \varepsilon $}\}$, is a
basis for a topology  $\mathcal{T}_{1}$ on $C[0; 1]$. (ii) Show that the collection $U$, where $U = \{U(f,\varepsilon ) : \text{$f\in C\left[0; 1\right ]$ and  $\varepsilon $ is a
positive real number}\}$ and $U(f,\varepsilon ) =\{g : \text{$g\in C\left[0; 1\right ]$ and
$\sup_{x\in \left[0,1\right]}$$\left|f-g\right|<\varepsilon $}\}$, is a
basis for a topology  $\mathcal{T}_{2}$ on $C[0; 1]$. (iii) Prove that  $\mathcal{T}_{1}\neq \mathcal{T}_{2}$. (i)and (ii) are similar by using the property of absolute value $\left|f-g\right|\leq\left|f\right|+\left|g\right|$ for (i) let $M_{1}$ and $M_{2}\in M$ where $M_{1}(f_{1},\varepsilon) =\{g : \text{$g\in C\left[0; 1\right ]$ and
$\int_{0}^{1}\left|f_{1}-g\right|<\varepsilon $}\}$, $M_{2}(f_{2},\varepsilon) =\{g : \text{$g\in C\left[0; 1\right ]$ and
$\int_{0}^{1}\left|f_{2}-g\right|<\varepsilon $}\}$
 then $M_{1}\cap M_{2}=M(\dfrac{f_{1}+f_{2}}{2},\varepsilon )$ so $M$ is a base for $C[0; 1]$. But I am not sure for (iii) by using the mean value theorem of integrals if $g$ is in some $m\in M$ then there may has no $u\in U$ since $\int_{0}^{1}\left|f-g\right|=\left|(f-g)\right| \left|(\xi)\right|<\varepsilon$ ($\xi \in [0,1]$) but if $\left|(f-g)\right| \left|(\xi)\right|<\sup_{x\in \left[0,1\right]}\left|f-g\right|$ so $g$ is not in some $u$ in $U$. I have no ideal about what to do next.",['general-topology']
462644,Integral equation $u(t)=f(t)+a\int_0^t u(s)ds\quad t\geq 0$,"Let $a\in\mathbb R$ and $f\colon [0,1]\to\mathbb R$ a continuous function. Solve the integral equation
$$u(t)=f(t)+a\int_0^t u(s)\mathrm ds,\quad t\geq 0$$ and find an explicit formula for the solution.       Thank you","['integral-equations', 'integration', 'real-analysis']"
462666,Finding a Hopf Bifucation with eigenvalues,"I am trying to show that the following 2D system has a Hopf bifurcation at $\lambda=0$:
\begin{align}                                                                                                                                                                
x' =& y + \lambda x \\                                                                                                                                                       
y' =& -x + \lambda y - x^2y                                                                                                                                                  
\end{align}
I know that I could easily plot the system with a CAS but I wish to analytical methods. So, I took the Jacobian:
\begin{equation}                                                                                                                                                             
  J = \begin{pmatrix} \lambda&1\\-1-2xy&\lambda-x^2\end{pmatrix}                                                                                                             
\end{equation}
My book says I should look at the eigenvalues of the Jacobian and find where the real part of the eigenvalue switches from $-$ to $+$. This would correspond to where the
system changes stability. So I took the $\det(J)$:
\begin{align}                                                                                                                                                                
  \det(J) =& -\lambda x^2 + 2xy + \lambda^2 + 1 = 0                                                                                                                          
\end{align}
I am stuck here with algebra and am not quite sure how to find out where the eigenvalues switch from negative real part to positive real part. I would like to use the
quadratic formula but the $2xy$ term throws me off. How do I proceed? Thanks for all the help!","['bifurcation', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
462680,is $g$ is a harmonic?harmonic polynomial?,"Let $u$ be a real valued harmonic function on $\mathbb{C}$ and $$g:\mathbb{R}^2\to\mathbb{R},~~~~~g(x,y)=\int_{0}^{2\pi} u(e^{i\theta}(x+iy))\sin\theta d\theta$$ Then is $g$ is a harmonic?harmonic polynomial? Could any one help me how to proceed?Thank you.","['harmonic-functions', 'complex-analysis']"
462682,Why does the Cholesky decomposition requires a positive definite matrix?,"Why does the Cholesky factorization requires the matrix A to be positive definite? What happens when we factorize non-positive definite matrix? Let's assume that we have a matrix A' that is not positive definite (so at least one leading principal minor is negative). Can one prove that there is no L such as A' = LL* ? If not, wouldn't the positive definite criteria remove some of the matrices that could be potentially decomposed? We could also put this question in the form of a demonstration for the next statement: For any square matrix L, the product LL* is a positive definite matrix.","['matrix-decomposition', 'positive-definite', 'cholesky-decomposition', 'matrices', 'linear-algebra']"
462694,Derivative at $0$ of $\int_0^x \sin \frac{1}{t} dt$,"Let $f(x)=\int_0^x \sin \frac{1}{t} dt   \textrm{ for }  x \in \mathbb R$.
Is $f$ differentiable at $0$ ?",['analysis']
462708,Proving $\mathrm{Aut} \;\Bbb{Z}\cong \Bbb{Z_2}$,"I have been asked to prove $\mathrm{Aut}\; \Bbb{Z}\cong \Bbb{Z_2}$. I figured that if I prove that $\mathrm{Aut}\; \Bbb{Z}$ has two elements, then I'm done. Clearly, $f(a)=a$ and $g(a)=-a$ can be those two automorphisms. Based on some rough calculations, I know there can't be any more automorphisms. Does this form a proof by any stretch of imagination? Thanks in advance!",['group-theory']
462735,"Find the number of $4$-letter words using $A,B,C,D,E$ with the following conditions","I`m trying to find how much words in length $4$ I can create from $A,B,C,D,E$; so
the conditions are: The number of words that have to be at least with the letter $A$. The number of words that have to be at least with the letter $A$ and
other letters aren't allowed to be the same, for example $ABCD$ is possible, but not $ABBB$. Until now what I think is $5^3 \cdot 4$, can someone give a suggestion?","['discrete-mathematics', 'combinatorics']"
462769,Is this set compact? Connected?,"Is this set compact? Connected? $S=\{(x,y,z)\in\mathbb{R}^3:z=x^2+y^2+1\}$ for $z\le 1$ this set is not defined, but for $z>1$ we are getting circles! I imagined this as a bunch of circles over $xy$ plane and connected but not compact set as unbounded. closed set though.
Could anyone give me rigorous treatment?","['general-topology', 'connectedness', 'compactness', 'real-analysis']"
462805,Finding all bifurcations in a 2D system,"I want to find all bifurcations for the system:
\begin{align}                                                                                                                        
x' =& -x+y\\                                                                                                                           
y' =& \frac{x^2}{1+x^2}-\lambda y                                                                                                     
\end{align}
So far, I am using the idea that the intersection of the nullclines ($y'=0$ or $x'=0$) gives equilibrium points. So:
\begin{align}                                                                                                                        
x'=& 0 \implies x=y \\                                                                                                               
y'=& 0 \implies y=\frac{x^2}{\lambda(1+x^2)}                                                                                         
\end{align}
Clearly $(0, 0)$ is an equilibrium point but there is also the possibility of two more equilibrium points when the parabola given
by the above equation intersects the line $y=x$. To find the intersection I solved:
\begin{align}                                                                                                                        
x =& \frac{x^2}{\lambda(1+x^2)}\\                                                                                                    
\lambda(1+x^2) =& x\\                                                                                                                
\lambda x^2-x+\lambda =& 0 \\                                                                                                        
x =& \frac{1\pm \sqrt{1-4\lambda^2}}{2\lambda}                                                                                       
\end{align}
I have these two intersection points. Now I need to vary $\lambda$ to find where the curve passing through the intersection points
becomes tangent to $y=x$ and hence we would expect one equilibrium point instead of these two at this particular value of $\lambda$. Then continuing the variation of $\lambda$ in the same direction we would expect no equilibrium points from these two.
Hence we originally had $2$ equilibrium  points and then they coalesced and finally annihilated each other. This is a saddle-node bifucation. How do I show this for my variation of $\lambda$? Are there any other bifurcations? EDIT: Consider the discriminant of the equation:
\begin{align}                                                                                                                        
  x = \frac{1\pm\sqrt{1-4\lambda^2}}{2\lambda}                                                                                       
\end{align}
\begin{align}                                                                                                                        
1-4\lambda^2 =& 0 \\                                                                                                                 
1 =& 4\lambda^2 \\                                                                                                                   
1 =& \pm 2\lambda \\                                                                                                                 
\pm\frac{1}{2} =& \lambda                                                                                                            
\end{align}
So, I plotted the system with $\lambda = \frac{1}{2}$: sage: P = plot(x^2/ .5*(1+x^2), 0, 2) + plot(x, 0, 2) We no longer have two bifurcations and instead have one. I was expecting the curves to be tangent. When does it happen that the curves are tangent? Actually I just realized that SAGE was not dividing the $1+x^2$ term, so I added the extra set of parens and everything works as expected!","['bifurcation', 'ordinary-differential-equations']"
462818,Kendall tau calculation,"Can someone explain how the Kendall tau works? I can't seem to find a good explaination/tutorial/example. I've been running corr(x,y,'kendall') from Matlab's Statistics Toolbox, but other than some output, it doesn't give me any good intuition. I've been stepping through with the debugger, but it gets confusing at times. I know that for two matrices, x and y must have the same number of rows, but that's about it. Is there a simple example that will illuminate what the Kendall p-value and Kendall tau really are?","['statistics', 'matlab']"
462820,What is the limit $( 1 + 1/\tan(n) )^{\tan(n)}$,"What is the result of 
$\displaystyle \lim_{n\to \infty}\left(1+\frac{1}{\tan(n)}\right)^{\tan(n)}$ = ? The limit does not exist as stated by Adam Rubinson. Looks like the requested answer is e but then the question is wrong. What is the correct question to give the answer e ? Did answered correctly. But I think this is some kind of typographical error. This question appeared on a high school test and the intended question should have a small error. What is the correct question to give the answer e changing only one value?","['recreational-mathematics', 'limits']"
462824,Expected return value of a recursive probabilistic function,"In this question asked on Stackoverflow, the asker gives a Java function similar to this: public int f(){
    if(Math.random() >= 0.5){
        return 1;
    }
    else{
        return 1 + Math.max(f(), f());
    }
} In math-speak, that would be written something like this: $$f = \left\{
    \begin{array}{ll}
        50\%\ chance : & 1 \\
        50\%\ chance : & 1 + max(f, f)
    \end{array}
\right.$$ So basically, $f()$ is a probabilistic function that calls itself twice with 50% probability, and returns the maximum depth of its calls. If you draw the call-tree and label the first case ""tails"" and the second case ""heads,"" you can see that the question ""What is the probability that this function eventually returns"" is the same as the question ""What is the probability of eventually getting more heads than tails."" .  Thus, this function terminates with probability 1 . This also allows you to calculate the expected number of total times that $f()$ is called. However, the one question I haven't been able to answer is, what is the expected return value of $f()$? My first thought was that the expected value $E_f$ should be
$$E_f = 0.5 * 1 + 0.5 * (1 + max(E_f, E_f))$$
However, that is clearly wrong; that would mean that, no matter how many calls $f()$ makes to itself in the second case, $E_f = 2$ ! So, does anyone know how to calculate the expected return value of $f()$?",['probability']
462834,"Is a projective space a vector space? If not, what of a basis?","A projective space is not a vector space, correct?  At a minimum, there is no additive identity in a projective space. So can you even have a basis of a projective space?  $[1 0 0], [0 1 0], [0 0 1]$ might work for $P^3$.  Or are $[1 0 0]$ and $ [0 1 0]$ not linearly independent since they both lie on the line at infinity? (is there a ""too much"" span problem with $[0 0 0]$ as well?) If not, is there a roughly analogous concept for a projective space?","['projective-space', 'linear-algebra', 'projective-geometry']"
462836,Books in Combinatorial optimization,"I wrote  Combinatorial optimization in the title , but I am not sure if this is what I am looking for. Recently, I was getting more interested in Koing's theorem, Hall marriage theorem .  I am interested to see similar theorems (I know similar is subjective). I guess what I am looking for is combinatorial optimization. I am wouldn't be interested about the algorithmic side of Combinatorial optimization. I just want to get exposed to some nice theorems of the subject like the ones I mentioned at the beginning. I need suggestions for a suitable book. Note: I only took an undergrad course in graph theory. I don't mind if the book is a graduate level book as long as it does not assume a pre knowledge of the subject. Thank you Edit: I would also be interested about coloring problems of graphs and the chromatic number. I think I am interested  in problems where one is looking for a maximum or a minimum. But I am not interested in making algorithms to achieve a maximum or a minimum.","['graph-theory', 'reference-request', 'combinatorics']"
462850,Evaluating an improper integral that involves $\exp(-|x|)$,"I am trying to prove that the function $f:\mathbb C\setminus\mathbb R\rightarrow\mathbb C$ defined by
$$
f(z) := \frac{1}{2\pi i}\int_{-\infty}^\infty\frac{\exp(-|x|)}{x-z}dx
$$
is holomorphic . I tried to solve it by evaluating the integral.  Since |x| introduces non-analicity, I tried dividing the integral into the intervals $(-\infty, 0]$ and $[0,\infty)$.  I thought that I could calculate these integrals by using residue calculus, but I have never evaluated this kind of integrals. I would appreciate if you could provide a clue (not necessarily a complete answer). Edit:  I corrected the problem statement.","['improper-integrals', 'integration', 'complex-analysis', 'contour-integration']"
462853,"Exam question: Multivariable calculus, differentiation","I've decided to finish my education through completing my last exam (I've been working for 5 years). The exam is in multivariable calculus and I took the classes 6 years ago so I am very rusty. Will ask a bunch of questions over the following weeks and I love you all for helping me. Q:
Suppose that $f(x,y)$ fulfills the Laplace equation $$\frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2} = 0$$ Show that $g(x,y) = f(2x+y,x-2y)$ also fulfills the equation. I understand everything about the teachers answer except one early part. A:
$$u=2x+y\\\\
v=x-2y$$
The chain rule give:
$$\frac{\partial g}{\partial x}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial x} = 2\frac{\partial f}{\partial u}+\frac{\partial f}{\partial v}$$
$$\frac{\partial g}{\partial y}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial y}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial y} = \frac{\partial f}{\partial u}-2\frac{\partial f}{\partial v}$$ ... My question is: How is he simplifying that last step, where he get 2.. + .. and .. - 2 ..?","['multivariable-calculus', 'derivatives']"
462867,"If $a_{n+1}=\lfloor 1.05\times a_n\rfloor$, does there exist $N$ such that $a_N\equiv0 \ $(mod$\ $$10$)?","I've known the following theorem. Theorem : Supposing that $a_{n+1}=\lfloor 1.05\times a_n\rfloor$ for any natural number $n$, there exists $N$ such that $a_N\equiv0 \ $(mod$\ $$10$) for any integer $20\le a_1\le100$. Proof : $\{a_n\}$ is a monotonic increase sequence, so let's observe the minimum $n$ such that $a_n\ge100$ for any $a_1$. The observation shows you that you'll always get any one of $100, 101, 102, 103$. Then, you get $$101\to106\to111\to116\to121\to127\to133\to139\to145\to152\to159\to166\to174\to182\to191\to200$$ $$102\to107\to112\to117\to122\to128\to134\to140$$ $$103\to108\to113\to118\to123\to129\to135\to141\to148\to155\to162\to170,$$ so the proof is completed. Then, here are my questions. As far as I know, the next question still remains unsolved. Question1 : Supposing that $a_{n+1}=\lfloor 1.05\times a_n\rfloor$ for any natural number $n$, does there exist $N$ such that $a_N\equiv0 \ $(mod$\ $$10$) for any integer $a_1\ge20\ $? It is likely that such $N$ would exist, but I'm facing difficulty.
I'm also interested in the following generalization. Question2 : Supposing that $\alpha\gt1$ is a real number and that $a_{n+1}=\lfloor \alpha\times a_n\rfloor$ for any natural number $n$, does there exist $N$ such that $a_N\equiv0 \ $(mod$\ $$10$) for any integer $a_1\ge \frac{1}{\alpha-1} \ $? Any help would be appreciated.","['sequences-and-series', 'number-theory']"
462874,Which geometric figure (polyhedron) has 15 quadrilateral faces?,I am looking for a polyhedron which consists only out of 15 quadrilateral faces? Does such a thing exist?,['geometry']
462877,Skew-symmetric form as matrix,"From Humphreys' Introduction to Lie Algebras and Representation Theory : $C_l$: Let $\dim V=2l$, with basis $(v_1,\ldots,v_{2l})$. Define a nondegenerate skew-symmetric form $f$ on $V$ by the matrix $S=\left| \begin{array}{cc}
0 & I_l \\
-I_l & 0  \end{array} \right|$. So, a skew-symmetric form is a function $B:V\times V\rightarrow F$ such that $B(v,w)=-B(w,v)$ for all $v,w\in V$. Why is it defined using the matrix $S$? What is the meaning of the matrix $S$ here?","['matrices', 'abstract-algebra']"
462901,"How are $C^0,C^1$ norms defined","How are $C^0,C^1$ norms defined? I know $L_p,L_\infty$ norms but are the former defined.","['normed-spaces', 'functional-analysis']"
462910,Orthogonal matrix confusion,"I have a confusion about orthogonal matrix. If columns of a square matrix are orthonormal to each other, is the matrix orthogonal? If yes, then are the rows of the matrix also orthonormal? Why? Why is it that QQ'=I? I get Q'Q=I but why QQ' is also I? Thanks,
Tom",['linear-algebra']
462917,"When $\sin x, \cos x$ are $\mathbb{Q}$-linear combinations of square roots","Suppose $x\in\Bbb R$ is such that $$\sin x=\sum_{i=1}^m x_i\sqrt{r_i},\quad \cos x=\sum_{j=1}^n y_j\sqrt{s_j}$$ for some $x_i, r_i, y_j, s_j \in\Bbb Q \ , \  |x_i|=|y_j|=1$. Show that $x=\dfrac{k\pi}{12}$ for some $k\in\Bbb Z$.","['trigonometry', 'algebra-precalculus', 'abstract-algebra']"
462949,What are the main uses of convex functions?,"Up till now I have just learned that the concept of convexity in functions of one variable is used to complete the graphs of functions, meaning to locate points of inflexion and see if the graph is concave or convex in given intervals. Spivak's book mention that altough this is what we typically learn at calculus, the importance of this concept doesn't lie precisely on ploting graphs and it is really worth to assimilate the information. I'd like to know the main aplications of this concept and it's importance in general.","['convex-analysis', 'soft-question', 'real-analysis']"
462979,Finding all integers $n$ such that $\left(\mathbb{Z}/n\mathbb{Z}\right)^\times$ has exponent $2$ [duplicate],"This question already has answers here : If n is such that every element $(\mathbb{Z}/n\mathbb{Z})^{\times}$ is a root of $x^2-1$. Prove that $n$ divides 24. (6 answers) Closed 4 years ago . This problem is from a past qualifying exam. Definition A group $G$ has exponent $e$ if $g^e=1$ for all $g\in G$. Problem Let $G=\left(\mathbb{Z}/n\mathbb{Z}\right)^\times$. Find all the integers $n$ for which $G$ has exponent $2$. My work so far: Need to find $n$ such that $\overline{b}^2=\overline 1$ for $\overline{b} \in G$. So I need to find $n$ such that $n \mid (b^2-1)$ and $(b,n)=1$. Well, how do I now go about finding all the $n$'s. I tried a few examples like $n=1$ etc. But then I still have to deal with $b$. This kind of problem is new to me. Can somebody kindly show me what I've done wrong/right here? May be suggest a way to go about finding these $n$'s. Thank for your help.","['elementary-number-theory', 'group-theory', 'abstract-algebra']"
462982,Need verification - Prove a Hermitian matrix $(\textbf{A}^\ast = \textbf{A})$ has only real eigenvalues,"Proof: Let eigenvalue $\lambda \neq 0$ such as 
$$\textbf{A}\vec{v}  = \lambda\vec{v}$$
$$\Rightarrow (\textbf{A}\vec{v})^\ast = (\lambda\vec{v})^\ast$$
$$\Rightarrow (\vec{v}^\ast\textbf{A}^\ast)=(\lambda^\ast\vec{v}^\ast)$$
Right-multiply both sides by $\color{orangered}{\vec{v}}$$$\Rightarrow (\vec{v}^\ast\textbf{A}^\ast \color{orangered}{\vec{v}} )=(\lambda^\ast\vec{v}^\ast \color{orangered}{\vec{v}} )$$
$$\textbf{A}^\ast=\textbf{A}$$
$$\Rightarrow(\vec{v}^\ast\textbf{A}\vec{v})=(\lambda^\ast\vec{v}^\ast\vec{v})$$
$$\Rightarrow(\vec{v}^\ast\lambda\vec{v}) = (\lambda^\ast\vec{v}^\ast\vec{v})$$
$$\Rightarrow(\lambda\vec{v}^\ast\vec{v}) =  (\lambda^\ast\vec{v}^\ast\vec{v})$$
$$\Rightarrow \lambda = \lambda^\ast$$
$$\Rightarrow \lambda\in\mathbb{R}$$","['linear-algebra', 'proof-verification']"
462983,Prove that a square matrix commutes with its inverse [duplicate],"This question already has answers here : If $AB = I$ then $BA = I$ (34 answers) Closed 10 years ago . The Question: This is a very fundamental and commonly used result in linear algebra, but I haven't been able to find a proof or prove it myself.  The statement is as follows: let $A$ be an $n\times n$ square matrix, and suppose that $B=\operatorname{LeftInv}(A)$ is a matrix such that $BA=I$.  Prove that $AB=I$.  That is, prove that a matrix commutes with its inverse, that the left-inverse is also the right-inverse My thoughts so far: This is particularly annoying to me because it seems like it should be easy. We have a similar statement for group multiplication, but the commutativity of inverses is often presented as part of the definition. Does this property necessarily follow from the associativity of multiplication?  I've noticed that from associativity, we have
$$
\left(A\operatorname{LeftInv}(A)\right)A=A\left(\operatorname{LeftInv}(A)A\right)
$$
But is that enough? It might help to talk about generalized inverses .","['linear-algebra', 'group-theory']"
463003,Length-Diameter Inequality for Convex Curves using Fourier Analysis,"I'm trying to prove an isoperimetric inequality, but I have absolutely no idea how to go about it. let $\Gamma$ be a closed plane curve parametrized by $\gamma(t) = (x(t), y(t))$ on $[-\pi, \pi]$. Let $l$ denote the length of the curve and $d = \sup_{t_1, t_2 \in [-\pi, \pi]} |\gamma(t_1) - \gamma(t_2)|$. If $\Gamma$ is convex then $l\leq \pi d$.","['convex-analysis', 'fourier-analysis', 'differential-geometry']"
463013,Calculus and minimum values,This is a simple question but I think I don't understand exactly what the question is asking.,"['calculus', 'integration']"
463014,Functionally structured spaces and manifolds,"The question requires some definitions that I have listed below for your convenience. They can be found in Chapter VI, pages 297-298 of Bredon's Introduction to Compact Transformation Groups . On a topological space $X$ a functional structure $F_{X}$ is an assingment defined on the collection of open sets $U\subset X$ taking $U\mapsto F_{X}(U)$ such that: $F_{X}(U)$ is a subalgebra of the algebra of continuous real valued functions on $U$ and contains all constant functions. If $V$ is open, $V\subset U$ and $f\in F_{X}(U)$ then $f|_{V}\in F_{X}(V)$. If $\{U_{i}\}$ is a collection of open sets and $f|_{U_{i}}\in F_{X}(U_{i})$ for all $i$ then $f\in F_{X}(\bigcup_{i}U_{i})$. The pair $(X,F_{X})$ is called a functionally structured space . E.g. $(\mathbb{R}^{n},C^{\infty})$, where $\mathbb{R^{n}}$ has the usual topology and $C^{\infty}(U)=\text{all $C^{\infty}$ functions on $U$.}$ If $(X,F_{X})$ is a functionally structured space and $U\subset X$ is open then for $V\subset U$ open we define $F_{U}(V)=F_{X}(V)$ so that $(U,F_{U})$ is a functionally structured space. A morphism of functionally structured spaces $(X, F_{X})$ and $(Y,F_{Y})$ is a continuous map $\varphi:X\rightarrow Y$ such that for any open set $V\subset Y$ and $f\in F_{Y}(V)$ we have $f\circ \varphi\in F_{X}(\varphi^{-1}(V))$. A morphism $\varphi$ is an isomorphism if $\varphi^{-1}$ exists as a morphism. We then say that $(X,F_{X})\simeq (Y,F_{Y})$. An $n$-dimensional differentiable manifold is a second countable functionally structured Hausdorff space $(M,F)$ with the property that each point in $M$ has an open neighborhood $U$ such that $(U,F_{U})\simeq(V,C^{\infty}_{V})$ for some open set $V\subset \mathbb{R}^{n}$. Question: Let $(X,F_{X})$ be a functionally structured space with the following property. Each point in $X$ has a neighborhood $U$ such that there are functions $f_{1},\ldots ,f_{n}\in F_{X}(U)$ verifying: A real valued function $g$ on $U$ is in $F_{X}(U)$ iff there exits a smooth real valued function $h$ of $n$ real variables such that $g(p)=h(f_{1}(p),\ldots,f_{n}(p))$ for all $p\in U$ . Does it follow that $\mathbf{(X,F_{X})}$ is an $\mathbf{n}$-dimensional differentiable manifold? (In the sense of the definition given above). Edit: Sheaves are new to me, but after reading the comments I wanted to point out this and this MSE posts, and the wikipedia page and subwiki page where sheaves are used to define differentiable manifolds. Edit: Sheaves my be used to define differentiable manifolds: Roughtly speaking, a differentiable manifold is a second countable Hausdorff topological space equipped with a subsheaf of the sheaf of continuous real valued function that is locally isomorphic to the sheaf of smooth real valued functions on some $\mathbb{R}^{n}$.","['sheaf-theory', 'general-topology', 'manifolds', 'category-theory', 'differential-topology']"
463018,Left Invertible Elements of a monoid,"It is true in general that the set of all invertible elements of a Monoid form a subgroup. The proof is trivial. However, after some thought, I feel that if we restrict invertible to left or right invertible only, then it does not form a group. It seems so because I cannot imagine a way to prove otherwise. I am looking for examples of Monoids whose left invertible elements do not form a subgroup or else proof that it does form a group.","['group-theory', 'abstract-algebra', 'monoid']"
463019,Is my proof correct? (the product $\prod_{n=1}^\infty (1+\frac{z}{n} ) \mathrm{e}^{-\frac{z}{n}}$ converges absolutely and uniformly on compact sets.),"I want to prove that the product $$\prod_{n=1}^\infty \left(1+\frac{z}{n} \right) \mathrm{e}^{-\frac{z}{n}}$$
converges absolutely, and uniformly on compact subsets of $\mathbb C$: My book (Ahlfors) defines the absolute convergence of the product $\prod a_n$ by the absolute convergence of the series $\sum \text{Log } a_n$, where only a finite number of terms can be zero, and they must be removed from the sequence. ($\text{Log}$ denotes the principal branch of the logarithm, where the argument is in $(-\pi ,\pi]$.) Thus, in order to prove absolute convergence, I fix $z \in \mathbb C$, and look at the series whose terms are $\text{Log } \left[ \left(1+\frac{z}{n} \right) \mathrm{e}^{-\frac{z}{n}} \right]$. It is well known that if $\text{Arg }z_1,\text{Arg }z_2$ both lie in $(-\frac{\pi}{2},\frac{\pi}{2})$, then $\text{Log } (z_1z_2)= \text{Log } z_1+\text{Log } z_2$. For large enough $n$ both factors $z_1=\left( 1+\frac{z}{n} \right),z_2=\mathrm{e}^{-\frac{z}{n}}$ satisfy this, so $\text{Log } \left[ \left(1+\frac{z}{n} \right) \mathrm{e}^{-\frac{z}{n}} \right]=\text{Log } \left(1+\frac{z}{n}\right)+\text{Log } \mathrm{e}^{-\frac{z}{n}}$. It is also possible to prove that $\text{Log } \mathrm{e}^{-\frac{z}{n}}=-\frac{z}{n}$ for large enough $n$. The problem thus reduces into the convergence of $$\sum  \left\lvert \text{Log } \left(1+\frac{z}{n} \right)-\frac{z}{n} \right\rvert $$ Taylor's theorem states that $$\text{Log }(1+w)=w+w^2 g(z) .$$ I've estimated the remainder and found that for $|w|<\frac{1}{2}$, $$| g(w)| \leq 2(\ln 2+\pi), $$ therefore, for large enough $n$ (so that $|\frac{z}{n}| < \frac{1}{2}$), the terms of the last series are dominated by $2(\ln2+\pi)\frac{|z|^2}{n^2}$, and $\sum 2(\ln2+\pi)\frac{|z|^2}{n^2}$ converges. I deliberately left the $\Sigma$'s unindexed, since $z$ might be a zero of one of the factors. In that case only a tail of the series is taken into account. The proof of uniform convergence on compact sets follows the same lines. Let $K \subset \mathbb C $ be a compact set, as such it lies in some ball $|z| \leq M$. Taking large enough $n$, we find similarly, that the series $\sum \left\lvert \text{Log }  \left[ \left(1+\frac{z}{n} \right) \mathrm{e}^{-\frac{z}{n}} \right] \right\rvert$ is dominated by $\sum 2(\ln2+\pi)\frac{M^2}{n^2}$ which converges. The Weierstrass M-test says that the series is uniformly convergent, and hence, so is the product. I'd love to hear any thoughts/remarks about these proofs.","['sequences-and-series', 'products', 'proof-verification', 'uniform-convergence', 'complex-analysis']"
463024,The order type of the rationals.,"Herewith another mind-numbingly naive question from a reader of philosophy. My question concerns the order type of the rational numbers. Omega squared seems a natural first choice, but obviously this does not look anything like the natural ordering of the rationals. Is it known where the order type of Q occurs in the hierarchy of ordinal numbers?  Is there a known ordinal-arithmetic expression describing it a function of omega? Finally, I really must buy a textbook on the subject of Set Theory.  Wiki is a fantastic resource and the maths pages are of exceptionally high quality, but I don't want to get into bed at night with my laptop.  Is there a standard, undergraduate text that could be recommended.","['elementary-set-theory', 'ordinals', 'reference-request', 'order-theory']"
463028,A question about matrices,"I was working on the McLain example about a characteristically simple group. McLain Unfortunaly I have no access to the original paper, so I was trying to make it out by myself. I found (on Google) that McLain use a division ring to create a group of automorphisms of a vector space which (I hope) has to be the characteristically simple one. Now I proved that this group has to be locally nilpotent and that every its element can be embedded in an unitriangular group (over the same division ring). So my question is the following one:
Are unitriangular matrices, over a division ring, nilpotent groups? How can I show that? (Nilpotency will help me to make a further step towards the proof) (I know that this works for unitriangular matrix over a field.) (Obviously if you can tell to me some information on the McLain example it would be great too!) Thanks for the attention!","['matrices', 'linear-algebra', 'group-theory']"
463035,No husband can sit next to his wife in this probability question,"I have a probability question that reads: Question: If 4 married couples are arranged in a row, find the probability that no husband sits next to his wife. My attempt: Total outcomes = 8!
Outcomes that all of them sit with their wife: 4!*(4*2!)
Outcomes that one of them sit with their wife: (2!*4)(6!)-(2!*4)(3!*(3*2!)[subtract the ways that remaining couples are together]
Outcomes that two of them sit with their wife: (2!*4C2)(4!)-(2!*4C2)(2!*(2*2!)
Outcomes that three of them sit with their wife: (2!*4C3)(2!)-(2!*4C3)(1!*(2*2!)
Hence no husband sit with wife is 1 -(Outcomes that all of them sit with their wife+ Outcomes that one of them sit with their wife + Outcomes that two of them sit with their wife + Outcomes that three of them sit with their wife)/8! Am i right? Any easier way?","['permutations', 'probability', 'combinatorics']"
463045,Axiom of Choice and Right Inverse,"I read an Theorem that states: Let $A$ and $B$ be non-empty sets, and let $f:A \to B$ be a function,
  then the function $f$ has a right inverse if and only if $f$ is
  surjective. The Theorem proof uses Axiom of Choice. My question is, if Axiom of Choice is not true, then is this Theorem false? My question is because I tried to find a proof that not uses the axiom but I did not found it and this result is very important because it implies, with another statement, that a function $f$ has an inverse if and only if $f$ is bijective.","['alternative-proof', 'functions', 'axiom-of-choice']"
