question_id,title,body,tags
3945968,Formula to work out likely number of unique items,"There is an unknown quantity of unique prizes. Each time you play the game, you get a prize. The likelihood of getting any type of prize X is equal to getting the likelihood of any other type, and winning a prize does not decrease the likelihood that that type will be won in future. I won 14 prizes. Where '1' represents a type of prize I have never won before, and '0' represents winning a type of prize I have won before, I got the following ordered list: (1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0) I know that each time I play the game, the probability of getting a prize that I have not won before is (N-a)/N where a is the number of unique prizes I have already won. What formula can I use to determine what the most likely number of prizes is?","['statistical-inference', 'statistics', 'probability-distributions']"
3945991,"If $r>0$ and $r\notin \mathbb{N}$, is there a simple method to evaluate $ \sum_{n=\lceil r \rceil}^{\infty} {\binom{n}{r}^{-1}}?$","Let $r>0,r\in \mathbb{R}\setminus\mathbb{N}$ . Empirically, I have noticed the following relation: $$
\sum_{n=0}^{\lfloor r \rfloor} \frac{1}{\binom{n}{r}} = - \sum_{n=\lceil r \rceil}^{\infty} \frac{1}{\binom{n}{r}};
$$ in particular, $\displaystyle{\sum_{n=0}^{\infty} \frac{1}{\binom{n}{r}} =0}$ . Note that if $r$ is an integer, the finite sum is not well-defined, although we do have $$
\sum_{j=0}^{k-1} \operatorname{Res}  \left(\frac{1}{\binom{z}{k}},z=j\right)= k\cdot\sum_{m=0}^{k-1}\binom{k-1}{m}(-1)^m=0,
$$ so in this sense the sum 'cancels'. Mathematica returns the closed-form of $$
\sum_{n=\lceil r \rceil}^{\infty} \frac{1}{\binom{n}{r}}=  \frac{\lceil r\rceil }{(r-1) \binom{\lceil r\rceil }{r}},
$$ which when $r\in\mathbb{N}$ reduces to this question , but I don't know how to derive that myself. Maybe I'm not fully understanding the answers there but I don't think the same tricks apply when the sum doesn't telescope. So in summary, my questions are: Can someone explain the closed-form? Is there a simple, conceptual reason the finite sum is the negative of the infinite sum?","['gamma-function', 'binomial-coefficients', 'closed-form', 'sequences-and-series']"
3946001,"Prove that $T(v) = v,$ for some $v \in \Bbb R^3 \setminus \{0\}.$","Let $T \in \text {SO} (3) : = \left \{T \in M_3 (\Bbb R)\ |\ TT^t = T^tT = I,\ \det (T) = 1 \right \}.$ Prove that there exists $v \in \Bbb R^3 \setminus \{0\}$ such that $T(v) = v.$ Hence conclude that the elements of $\text {SO} (3)$ are rotations by some angle about some uniquely determined axis. The first part is easier. To prove that $T(v) = v,$ for some $0 \neq v \in \Bbb R^3$ it is enough to show that $\det (T - I) = 0.$ Now $$\begin{align*} \det (T - I) & = 1 \cdot \det (T - I) \\ & = \det (T) \det (T - I) \\ & = \det (T^t) \det (T - I) \\ & = \det (T^t T - T^t) \\ & = \det (I - T^t) \\ & = \det (I^t - T^t) \\ & = \det ((I - T)^t) \\ & = \det (I - T) \\ & = (-1)^3 \det (T - I) \\ & = - \det (T - I) \end{align*}$$ This shows that $2 \det (T - I) = 0 \implies \det (T - I) = 0,$ as required. How to prove the second conclusion? Any help in this regard will be appreciated. Thanks in advance.","['eigenvalues-eigenvectors', 'fixed-points', 'orthogonal-matrices', 'linear-algebra', 'rotations']"
3946012,asymptotic statement and one example,how I can logically understand this is false ( $c$ is constant): $ \frac{n!}{c} \leq 2^n$ why it was false for large value from asymptotic notation concept? ( i see this is true for some $n < n_0$ but not hold for large value).,"['asymptotics', 'calculus', 'linear-algebra', 'discrete-mathematics', 'computer-science']"
3946039,"Tangent planes for the function $f(x,y) = 1 - x^2 - y^2$","I have to solve the following statements, finding the tangent planes to $f$ and its points of tangency: that contains the line in $\mathbb{R}^3$ that passes through the points $(3,0,3)$ and $(0,-3,3)$ and this other statement: that contains the point $(0,0,2)$ For the first statement, I tried first making the parametric equation and I found that $L(t) = (3-3t, -3t, 3)$ , and I also know that the equation of the plane is given by $$p(x,y) = \frac{\partial f}{\partial x}(x_0,y_0)(x-x_0) + \frac{\partial f}{\partial y}(x_0,y_0)(y-y_0) + f(x_0,y_0) = -2x_0 (x-x_0) -2y_0 (y-y_0) + (1 -x_0^2 - y_0^2)$$ but after that, I don´t see what else to do. Could you give me any hint for both statements?","['plane-curves', 'tangent-line', 'multivariable-calculus', 'calculus', 'partial-derivative']"
3946050,Two representations of a finite group have no irreps in common if and only if their characters are orthogonal.,"We want to prove that two representations of a finite group $\Gamma_{1}$ and $\Gamma_{2}$ have no irreducible representation in common if and only if their characters are orthogonal, i.e., $\sum_{k=1}^{c} n_k \chi_{1k}\chi_{2k}^{*}=0$ , where $\chi_{1k}$ and $\chi_{2k}$ are the characters of the $k-$ th class in $\Gamma_{1}$ and $\Gamma_{2}$ respectively. We can say that for common irreps the characters are the same and therefore the above relation doesn't equal to zero. But it is trivial. Could you please help?","['representation-theory', 'group-theory', 'finite-groups', 'characters']"
3946061,Coefficient of $\frac{1}{1-x-x^2-..-x^{d-1}}$ and its asymptotic,"Let $C_n$ be the set of all compositions of n such that each letter is one of ${1,2,\ldots,d-1}$ . A. Calculate the generating  function of the number of compositions in $C_n$ . B. For each composition $\pi \in C_n$ define $f(\pi)$ as the number of letters that equal $1$ .
Find yhe generating function of $\sum_{n\geq 0} \sum_{\pi \in C_n}  q^{f(\pi)} x^n$ . C.Find the expectation of the number of letters that equal 1 (1's appearances) in these compositions, and analyze it asymptotically . *Generally: Composition of $n$ : a sequence of numbers in $N$ such that their sum equals $n$ . For example: $C_0={\epsilon}$ , $C_1={1}$ , $C_2={11,2}$ , $C_3={111,12,21,3}$ . In this question, a composition of a number $n$ , consists of numbers from the set { $1,2,\ldots, d-1$ } where $d$ is a given number. This is what I did: A. Every compisition of n = empty compisition $\cup$ (1 / every compisition) $\cup$ (2 / every compisition) $\cup$ .... $\cup$ ((d-1) / every compisition).
so if x counts the length of a number, then the  generating function $C(x)$ for the problem satisfy: $C(x)=1+xC(x)+x^2C(x)+\cdots+x^{d-1}C(x)$ (1 for the empty compisition) $C(x)=1+C(x)(x+x^2+\cdots+x^{d-1})$ Therefore, $C(x)=\frac{1-x}{1-2x+x^d}=\frac{1}{1-x-x^2-...-x^{d-1}}$ B.Let $q$ counts the appearances of 1 and $x$ as we mentioned counts the length of a number.
Then: $C(x)=1+qxC(x,q)+x^2C(x,q)+\cdots+x^{d-1}C(x,q)$ $C(x,q)=1+C(x,q)(qx+x^2+\cdots+x^{d-1})$ So, $C(x,q)=\frac{1-x}{x^d+(q-1)x^2-(q+1)x+1}$ C. We have to calculate $[x^n]C(x,1)=[x^n] C(x)$ and $[x^n] ([(d/dq) C(x,q)]_{q=1})=\frac{x}{(1-x-x^2-...-x^{d-1})^2}=xC(x)^2$ so the expectation will be the proportion of them. But I could not see how the coefficients can be calculated!
I did not succeed to connect it to Fibonacci series which I know (the case of d=2). I would be thankful for any help to show part C. Thanks in advance","['combinatorial-proofs', 'recurrence-relations', 'asymptotics', 'combinatorics', 'generating-functions']"
3946062,Is there anything interesting about taking the logarithm of the determinant?,"We can view the determinant as a group homomorphism $$\det : GL_n(F) \to F^\times$$ since $\det(AB)=\det(A)\det(B)$ . If the field is the real numbers, we can take the logarithm of the determinant, which by definition maps $\mathbb{R}^\times$ to $\mathbb{R}$ under addition. So then the composition $\log\circ\det$ can be seen as taking the group of $n \times n$ matrices with positive determinant to the group of real numbers under addition. Is there anything interesting about this map, or is there nothing else to say other than it is just the ""logarithm of the dterminant""?","['group-theory', 'determinant']"
3946085,How to arrange vectors in a circle so as to minimize the resultant?,"Sorry for the vague language, but I want to arrange weights in a circle so as to minimize the resultant moment. The axis of rotation is perpendicular to the plane in which the weights lie, and the weights have to be on the circumference of the circle, hence they all must be equidistant from the axis of rotation and have to be placed at regular angular intervals of $\theta = \frac{2\pi}{n}$ . The radius of the circle is r and the weights are $w_1$ , $w_2$ , $w_3$ , ... , $w_n$ . I tried two approaches but reached dead ends: First, I tried to minimize the x and y components of the resultant separately. But I couldn't figure out how to minimize this sum by interchanging the coefficients: $V_x$ = $rw_1\cos0$ + $rw_2\cdot\cos\theta$ + $rw_3\cdot\cos2\theta$ + .... + $rw_n\cdot\cos(n-1)\theta$ $V_y$ = $rw_1\sin0$ + $rw_2\cdot\sin\theta$ + $rw_3\cdot\sin2\theta$ + .... + $rw_n\cdot\sin(n-1)\theta$ I tried to see the problem as a sum of complex numbers with different magnitudes arranged in a circle. But, then again I couldn't find any way to minimize their sum by interchanging their arguments. Can someone help me with this?","['permutations', 'optimization', 'trigonometry', 'vectors']"
3946104,Relation between averages in the sphere and the ball,"Let $\Omega$ be a domain in $\mathbb R^N$ and $u \in C(\Omega)$ . If $\omega_N$ denotes the volume of $B_1(0) \subset \mathbb R^N$ and $0 < r < d(x, \partial \Omega)$ , the averages are $$
A_b(x) = \frac{1}{\omega_N r^N} \int_{B_r(x)} u(y) \ dy
$$ in the ball, and $$
A_s(x) = \frac{1}{N \omega_N r^{N - 1}} \int_{\partial B_r(x)} u(y) \ d\sigma(y)
$$ in the sphere. We know that if $u$ is harmonic, then $A_b(x) = A_s(x) = u(x)$ . In general, can one say something like $A_s \leq (\geq) A_b$ ? I kindly ask for a hint, in case the answer is positive, or for examples, in case the answer is negative. EDIT :
Ted's hint made me think of the following: take $u$ to be negative inside the ball and $0$ on the boundary. Then of course $A_b < 0 = A_s$ . On the other hand, if $u$ is positive, it is clear that $A_b > 0 = A_s$ . We conclude that the answer to the question is negative in general. This seems to be very simple, am I missing something?","['multivariable-calculus', 'partial-differential-equations', 'real-analysis']"
3946106,Fourier Transform - Dirac Delta,"How can I compute this Fourier transform? $$\int \frac{d^3q}{(2\pi)^3}(\vec q \cdot \vec a)(\vec q \cdot \vec b)\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }\,$$ My idea was to write it as $$-( \vec a \cdot \vec \nabla)(\vec b \cdot \vec \nabla)\int \frac{d^3q}{(2\pi)^3}\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }$$ and use the fact that $\int \frac{d^3q}{(2\pi)^3}\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }=\frac{1}{4\pi r}$ , together with $\Delta \frac{1}{r} = -4\,\pi \,\delta(\vec r)$ but is is not exactly the same.","['integration', 'fourier-transform', 'dirac-delta']"
3946114,"Is there a ""nice"" rep-tile of order $6$?","A planar set is said to be a rep-tile if it can be tiled by congruent shapes, each similar to the original. If there are $k$ such shapes, each scaled down by a factor of $\sqrt{k}$ , it is said to be rep- $k$ . Rep-tiles exist for $k=2,3,4,5$ : More examples (not all triangular) are at Wikipedia . From Michael Beeson's paper here , we know that triangular rep- $k$ tiles exist if and only if $k$ is a square, a sum of two squares, or three times a square. (This yields $k=2,3,4,5,8,9,10,12,13,16,17,18,20,25,26,27,\ldots$ ) The Gosper curve is a rep-tile of order $7$ . I have found what looks like an instance of a rep- $6$ tile here : However, this is a pretty atrocious shape. Are there any polygonal rep- $6$ tiles? I'd settle for simply connected, in a pinch.","['dissection', 'recreational-mathematics', 'geometry', 'tiling']"
3946129,Compute difficult integral $\int \frac{dx}{2 + x + \sqrt{1 - x^2}}$,"To solve the integral $$I = \int \frac{dx}{2 + x + \sqrt{1 - x^2}}$$ I have tried several things, such as $t = \arcsin x$ , because $\cos(\arcsin x) = \sqrt{1 - x^2}$ . If I am not wrong, we can conclude with this variable change $$
I = \int \frac{\cos t\,dt}{2 + \sin t + \cos t}
$$ but if it were correct, how could I go on?",['integration']
3946152,Can a martingale converge in $L^1$ but not almost surely?,"In general, almost sure convergence and convergence in $L^1$ are independent modes of convergence in the sense that neither implies the other. For martingales, at least one implication, namely that almost sure convergence implies convergence in $L^1$ remains false. How about the other one? Does convergence of a martingale in $L^1$ imply almost sure convergence?","['stochastic-processes', 'probability-theory', 'martingales']"
3946163,Does folded cube have a name?,"Folding a square to produce a torus has an analogy with ""folding"" a cube to produce what? Is there a name for the resulting 4D structure?",['geometry']
3946167,Extremal distribution of random variable that averages to a given value,"Denote by $\mathcal M$ the set of probability measures over $[0,1]$ that average to $p$ . This is a convex set and I am trying to characterize its extreme points. For any $x$ , $y$ such that $0\leq x< p< y\leq 1$ define \begin{align*}
\lambda_x^y = \frac{p-y}{x-y}\cdot\delta_x+\frac{x-p}{x-y}\cdot\delta_y
\end{align*} where for any measurable $A\subseteq [0,1]$ and $a\in[0,1]$ , $\delta_a(A)=\mathbf 1(a\in A)$ . Since $\delta_x$ (resp. $\delta_y$ ) averages to $x$ (resp. $y$ ) we get that $\lambda_x^y$ averages to $\frac{p-y}{x-y}\cdot x+\frac{x-p}{x-y}\cdot y=p$ and so is in $\mathcal M$ . I am very tempted to say that the extreme points of $\mathcal M$ is the set $\{ \lambda_x^y : 0\leq x< p < y \leq 1 \}\cup \{ \delta_p \}$ without being able to prove it. If I am not mistaking I am supposed to prove that (ignoring the $\delta_x$ component) If for some $x$ and $y$ and $a\in[0,1]$ , $\lambda_x^y=a \mu+(1-a) \nu$ then $\mu=\nu=\lambda_x^y$ . For any $\mu\in\mathcal M$ there is a probability measure $\kappa$ over $[0,p]\times[p,1]$ such that $\mu(A)=\int \lambda_x^y(A) d\kappa(x,y)+\mu(\{ p \})\delta_p(A)$ for all measurable $A\subseteq [0,1]$ . For the first point, it is quite clear by positivity of the measures $\mu$ and $\nu$ that their support $\{ x,y \}$ , then there is only one probability measure with such a support that averages to $p$ and it is $\mu_x^y$ . For the second statement however it is less clear to me, Maybe one way to proceed is computing the integral \begin{align*}
\int \lambda_x^y(A) d\kappa(x,y)&=\int \left( \frac{p-y}{x-y}\cdot\delta_x(A)+\frac{x-p}{x-y}\cdot\delta_y(A) \right) d\kappa(x,y)\\
&=\int_{\left(A\cap [0,p]\right)\times [p,1]}\frac{p-y}{x-y} d\kappa(x,y)+\int_{[0,p]\times\left(A\cap [p,1]\right)}\frac{x-p}{x-y}d\kappa(x,y)
\end{align*} It now feel easier to separate cases $A\subseteq [0,p]$ and $A\subseteq [p,1]$ to set one of the two integral to $0$ but I still don't know how to proceed. Actually thinking about how I could use it in my research it would be much more powerful to not enforce $x\leq p \leq y$ but just that $p$ is in the convex hull of $\{ x,y \}$ and then we get repetition $\lambda_x^y=\lambda_y^x$ but the of extreme points is the same. This makes $\kappa$ a probability measure over $[0,1]^2$ where we have to be careful assigning $0$ probability to sets $A\times B\subseteq [0,1]^2$ where the convex hull of $A\cup B$ does not contain $p$ . Here is the last thing I tried :
Denote $a_x^y=\frac{p-y}{x-y}$ , for $\mu \in\mathcal M$ and $A$ measurable \begin{align*}
\mu(A)&=\int_{[0,p)} \delta_x(A) d\mu(x)+\int_{(p,1]} \delta_y(A) d\mu(y)+\mu(\{ p \})\delta_p(A)\\
&=\int_{[0,p)\times (p,1]} \left(\frac{a_x^y}{a_x^y}\delta_x(A) + \frac{1-a_x^y}{1-a_x^y}\delta_y(A) \right)d\mu\otimes\mu(x,y)+\mu(\{ p \})\delta_p(A)\\
\end{align*} It feels like we can define $\kappa$ as a function of $\mu\otimes \mu$ and $a_x^y$ , I cannot finish the argument though.","['measure-theory', 'probability-theory', 'linear-programming', 'convex-hulls']"
3946177,A $\zeta$-like sum of the positive roots of $\csc x=x$.,"From this answer , I was very interested to learn the following. Let $x_n$ be the $n$ -th positive root of the equation $\csc x=x$ . Then $$\sum_{n\ge1}\frac{1}{x_n^2}=1,$$ and, setting $s(k)=\sum_{n\ge1}x_n^{-k}$ , we have $$\sum_{k\ge1}s(2k)x^{2k}=\frac{x}{2}\cdot\frac{1+x\cot x}{\csc x-x}.\tag 1$$ This result was very surprising and I've never seen anything like it before. The proof was discussed rather briefly in the comments and apparently it can be shown via a contour integral, but I've never done anything like that before so I have no idea how. Could I have some help proving $(1)$ ? Thanks! :)","['complex-analysis', 'number-theory', 'generating-functions', 'sequences-and-series']"
3946213,"Volume of region $\{ (x_1,\ldots,x_n)\in \mathbf{R}^n_{>0} \mid \prod_{i=1}^n\max(1,x_i)\leqslant d \}$","As part of a problem on algebraic number theory, I am faced with the following analysis/calculus exercise: Show that the volume of the set $$E_n:=\Big\{ (x_1,\ldots,x_n)\in \mathbf{R}^n_{>0} :\prod_{i=1}^n\max(1,x_i)\leqslant d \Big\}$$ is equal to $$d\sum_{i=0}^{n-1} \binom{n-1}{i} \frac{(\log d)^i}{i!}.$$ My attempt at solving this problem was as follows. Clearly, $E_n\subset [0,d]^n$ . The volume of $E_n$ equals the following integral (abbreviating $m$ for $\max$ ): $$\int_0^d \int_0^{\frac{d}{m(1,x_1)}}\cdots \int_0^{\frac{d}{m(1,x_1)\cdots m(1,x_{n-2})}} \int_0^{\frac{d}{m(1,x_1)\cdots m(1,x_{n-1})}} 1\, dx_n\,dx_{n-1}\cdots dx_2\,dx_1,$$ which can be simplified into $$d\int_0^d \frac{1}{m(1,x_{1})} \int_0^{\frac{d}{m(1,x_1)}}\frac{1}{m(1,x_{2})}\cdots \int_0^{\frac{d}{m(1,x_1)\cdots m(1,x_{n-2})}} \frac{1}{m(1,x_{n-1})}\,dx_{n-1}\cdots dx_2\,dx_1.$$ I tried to prove the formula by induction. By splitting the middle integral, it evaluates to $1+\log d-\sum_{i=1}^{n-2}\log m(1,x_i)$ . By linearity of the integral, we get $(1+\log d)E_{n-1}$ plus some other integral. However, this gets incredibly messy. Is my approach correct? Is there any more clever way to attack this integral or an other method to prove this formula ? Is there maybe a probabilistic way of calculating its value ? EDIT. In Theorem 6.5 of this article the integral is written as $\int_{y\in \mathbf{R}^n,\sum_i \max(0,y_i)\leq \delta} \exp \sum y_i\,dy$ . Are my integral and this one equal ? Any help is much appreciated. (For the bounty: I would like to have a comprehensive, detailed answer.)","['integration', 'calculus']"
3946226,Lower bound on clique number of graph through vertex degrees?,"Suppose $\Gamma$ is a finite simple graph with $n$ vertices, such as the $i$ -th vertex has degree $d_i$ . Suppose $c$ is the clique number of $\Gamma$ . Is it always true, that $c \geq \sum_{i=1}^n \frac{1}{n - d_i}$ ? This inequality definitely holds for small graphs ( $n \leq 5$ ) and for some ""nice"" classes of graphs (complete graphs and complete bipartite graphs). Also it holds for all graphs without vertices of degree exceeding $\frac{n}{2}$ (because in that case $\sum_{i=1}^n \frac{1}{n - d_i} \leq 2$ ). However, I do not know, whether the inequality is correct in general.","['graph-theory', 'inequality', 'combinatorics', 'discrete-mathematics']"
3946247,Finding four square roots of $\small\begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$,"This question is about finding the 4(!!) square roots of a 2 by 2 matrix. The method I use is through Diagonalization (or actually doing the ""opposite""). I have no problem finding the four roots of a 2 by 2 matrix that has real eigenvalues (and thus real eigen vectors). Heck, I found 8 roots of a 3 by 3 matrix that has real eigen values. The problem I am running into is if the to be square rooted matrix has complex eigenvalues. In my view, the method should work the same, but somehow the algebra is letting me down. Example: Find the four square roots of $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ . Note, I found this matrix by squaring $ \begin{bmatrix}2 & -1\\1 & 2\end{bmatrix}$ , so obviously this is one root as well as taking opposite of its entries. So here is my work: The characteristic equation of $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ is $(3-\lambda)^2+16=0$ from which we find $\lambda=3+4i$ and $\lambda=3-4i$ . An eigenvector for $\lambda=3+4i$ is $ \begin{bmatrix}1 \\-i  \end{bmatrix}$ and an eigenvector for $\lambda=3-4i$ is $ \begin{bmatrix}1 \\i  \end{bmatrix}$ . So the given matrix can now be diagonalized: $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ = $ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}$$ \begin{bmatrix}3+4i & 0\\0 & 3-4i\end{bmatrix}$$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}^{-1}$ . I found $ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix}^{-1}$ = $\frac{1}{2i} \begin{bmatrix}i & -1\\i & 1\end{bmatrix}$ which is $ \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$ . It's easy to find out that the square roots of $3+4i$ are $2+i,-2-i$ , and of $3-4i$ are $2-i,-2+i$ . So now I would arrive at my 4 solutions: $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}2+i & 0\\0 & 2-i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}2+i & 0\\0 & -2+i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}-2-i & 0\\0 & 2-i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ $$ \begin{bmatrix}1 & 1\\-i & i\end{bmatrix} \begin{bmatrix}-2-i & 0\\0 & -2+i\end{bmatrix} \begin{bmatrix}0.5 & 0.5i\\0.5 & -0.5i\end{bmatrix}$$ When I work out my first candidate, I get a matrix, but when squared it doesn't give me $ \begin{bmatrix}3 & -4\\4 & 3\end{bmatrix}$ and so something isn't right. I don't know what I did wrong and ANY help from the math community is greatly appreciated. I feel that my line of thinking is correct, where did I go wrong? And does anyone happen to know online software that can do matrix multiplications with complex entries, because the TI doesn't work","['matrices', 'diagonalization', 'linear-algebra']"
3946274,Rainbow covering by rectangles,"There are $n$ coverings of the unit square, each of which contains $k$ axes-parallel rectangles of a unique color.
Define a rainbow covering as a covering of the unit square that contains exactly one rectangle of each color. Given $k$ , what is the smallest $n$ for which a rainbow covering always exists? For $k=2$ , a rainbow covering might not exist if $n=2$ , since we might have a covering by blue ""vertical"" rectangles and another covering by red ""horizontal"" rectangles, and each pair of one blue and one red rectangle does not cover the entire unit square. However, when $n=3$ , there are at least two horizontal pairs or two vertical pairs, and taking one rectangle of each of these coverings (plus an arbitrary one from the third covering) yields a rainbow covering of the unit square. So for $k=2$ the answer is $3$ . What is the smallest $n$ for $k\geq 3$ ?","['rectangles', 'geometry', 'packing-problem']"
3946290,How do permutations of $\Bbb N$ affect series?,"Let $G$ be the group of all permutations of $\mathbb{N}$ and $\sum a_n$ a conditionally convergent series of reals. What do we know about how $G$ ""acts"" on this series? We can partition $G$ according to how permutations $\pi$ affect the value $\sum_n a_{\pi n}$ or, more finely, the behavior of its partial sums. How much do we know about such partitions of $G$ ? There should be some subgroup $H\subset G$ of permutations that do not affect the value of any conditionally convergent series. This includes all finite permutations, but it should also include any permutation $\pi$ for which $|\pi(i)-i|$ is bounded. Does $H$ include any more permutations than this, or is this all of $H$ ? We can define $\Gamma_x\subset G$ as the set of $\pi$ for which $\sum a_{\pi n}=x$ . If $\sum a_n=x$ then this is like a stabilizer, but I don't see any obvious reason to expect it to be a subgroup. Or, for general $x$ , any reason to expect $\Gamma_x$ to act like a coset (e.g. the $\Gamma_x$ s being translates of each other). Do these sets satisfy properties similar to cosets? (One thing I can see is that each $\Gamma_x$ is a union of right cosets of $H$ .) Given a function $f:\mathbb{N}\to \mathbb{R}$ we can define $\Gamma_f$ as the set of those $\pi$ for which $\sum_{n\le N}a_{\pi n}\sim f(N)$ . There are many variations we could use on this definition, and we could even use it for divergent series $\sum a_n$ . Or, even if $S=\sum a_n$ is convergent, we could define $\Gamma_f$ (for $f\to 0$ ) according to $-S+\sum_{n\le N}a_{\pi n}\sim f(N)$ (so we partition $G$ according to how permutations affect the convergence of the series). I expect describing $\Gamma$ s becomes much harder in these situations. Do we know more answers to any of these questions for specific series, like harmonic or alternating harmonic?","['permutations', 'group-theory', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
3946308,Let $H$ be the set of all permutations $\alpha\in S_5$ satisfying $\alpha(2) = 2$. Is $H$ a group?,"Let $H$ be the set of all permutations $\alpha\in S_5$ satisfying $\alpha(2) = 2$ . Which of the
properties (closure, associativity, identity, inverses) does $H$ enjoy
under composition of functions? Solution: Let $\alpha, \beta, \gamma \in H$ . Closure: For any $\alpha, \beta \in H$ , we have $\alpha\beta(2)=\alpha(\beta(2))=\alpha(2)=2 $ . Thus composition is closed. Associativity: it's obvious that $(\alpha \beta) \gamma(2)=\alpha (\beta \gamma)(2)$ Identity: I guess it is the same as for $ S_5 $ . Since it leaves the rest of the elements fixed and especially, at $ 2 $ Inverses: Since every permutation in $ H $ leaves $2$ fixed, its inverse also. So for every $ \alpha \in H $ there is an inverse. I am right?","['symmetric-groups', 'group-theory', 'solution-verification']"
3946313,compute $P(X\le 2 \textrm{ or } 3<X<8)$ For exponentially distributed x,"Suppose X is exponentially distributed with $\lambda=2$ . I want to compute $P(X\le 2 \textrm{ or } 3<X<8)$ . The probability density function of $x$ is $f(x)=2e^{-2x}$ . Then I will find its distribution function as $F(x)=1-e^{-2x}$ . I don’t know exactly, but I think that $P(X\le 2 \textrm{ or } 3<X<8)$ can be calculated as follows: $$P(X\le 2 \textrm{ or } 3<X<8) = P(X\le 2)+ P(3<X<8)-P(X\le 2, 3<X<8)$$ $$P(X\le 2)=F(2)= 1-e^{-4}$$ $$P(3<X<8)=F(8)-F(3)= e^{-16}-e^{-6}$$ But what is $P(X\le 2, 3<X<8)$ ? Or what is the correct way to solve this question?","['self-learning', 'statistics', 'probability-distributions', 'exponential-function']"
3946379,Showing that an arithmetic function is multiplicative,"Let $f$ be an arithmetic function that counts the number of consecutive integers between $1$ and $n$ (inclusive) such that both integers are coprime to $n$ . More formally, $$ f(n) = \sum_{\substack{1 \leq t \leq n \\ (t,n)=1 \\ (t+1,n)=1}}1. $$ An immediate observation is that $f(n)=0$ when $n$ is even. After playing around with this function, I suspect that it is multiplicative. That is, if $(a,b)=1$ , then $f(ab)=f(a)f(b)$ . However, I am unable to prove this. Is this function really multiplicative or are there counterexamples? Also, what happens when we modify $f$ so that it counts the number of consecutive triplets that are all coprime to $n$ ?","['number-theory', 'elementary-number-theory', 'arithmetic-functions']"
3946440,Does these rational sequences always reach an integer?,"Let $u_0 \ge 2$ be a rational, and $u_{n+1}=⌊u_n⌋(u_n - ⌊u_n⌋ + 1)$ . Question : Does the sequence $(u_n)$ reach an integer? It was checked by computer (with SageMath) for $u_0=\frac{p}{q}$ with $p \le 32000$ . Example : if $u_0=\frac{11}{5}$ then $(u_n)= (\frac{11}{5}, \frac{12}{5}, \frac{14}{5}, \frac{18}{5}, \frac{24}{5}, \frac{36}{5}, \frac{42}{5}, \frac{56}{5}, \frac{66}{5}, \frac{78}{5}, 24, \dots)$ . A positive answer would provide an alternative to continued fraction theory (see cross-post here ).","['number-theory', 'elementary-number-theory', 'computational-number-theory', 'sequences-and-series', 'continued-fractions']"
3946465,"What is the solution on the interval (-3, 3) for this ODE?","I took Differential Equations a few years ago and am reading back through my old book in a little more detail, especially with regard to intervals of validity for solutions. I found one of the example problems which finds a solution only for the intervals (- $\infty$ , -3) and (3, $\infty$ ) using the standard integrating factor method. One of the section problems is to go back and find the solution on the interval (-3, 3). This is the equation: $$(x^2 - 9)\frac{dy}{dx}+xy=0$$ I've banged my head against the wall for a little bit before I remembered to check the Existence and Uniqueness theorem for IVP's to see if a solution exists on the interval: $$f(x,y)=-\frac{x}{x^2-9}y$$ and $$\frac{\partial f}{\partial y}=-\frac{x}{x^2-9}$$ Both of which are defined everywhere but x=-3 and x=3 so I believe a unique solution must exist on all of the intervals I mentioned above. I follow the books solution fine for the example but in this case I don't see any way to move forward on (-3,3) without bringing in complex numbers which I'm not very confident with. Further plugging the equation into Symbolab gives a plot with no solution on this interval. Usually I'm missing something simple but could someone enlighten me as to what it is?","['integrating-factor', 'ordinary-differential-equations']"
3946561,Description of Euler Characteristic Using $\operatorname{Ext}^i$'s,"I am reading Nakajima's Lectures on Hilbert Schemes of Points on Surfaces , and I am confused about a detail at the top of page 13. He is sketching a proof that $X^{[n]}(=$ Hilbert scheme of $n$ points on a nonsingular surface $X$ ) is nonsingular. The strategy is to show that for $[Z]\in X^{[n]}$ , $\dim T_{[Z]}X^{[n]}=2n$ . The method of doing this is a fairly straightforward argument using $$0\to \mathcal{I}_Z\to\mathcal{O}_X\to \mathcal{O}_X/\mathcal{I}_Z\to 0$$ and the long exact sequence for $\operatorname{Ext}^\bullet_{\mathcal{O}_X}(-,\mathcal{O}_Z)$ . I need help understanding the following claim: The Euler characteristic $\sum_{i=0}^2 (-1)^i \dim \operatorname{Ext}^i(\mathcal{I}_Z,\mathcal{O}_Z)$ is independent of $Z$ . I believe Nakajima means $\chi(X,\mathcal{O}_Z)=\sum_{i=1}^2(-1)^i\dim \operatorname{Ext}^i(\mathcal{I}_Z,\mathcal{O}_Z).$ I agree that for points $[Z]\in X^{[n]}$ , $\chi(X,\mathcal{O}_Z)$ should be the same; namely equal to $n$ . I just don't understand why the Euler Characteristic can be written in this fashion. It's probably easy, but escapes me!","['homological-algebra', 'algebraic-geometry', 'sheaf-cohomology', 'moduli-space']"
3946567,Asymptotic expansion of the inverse of $x\mapsto x+x^\phi$ near zero,"Consider a continuous real-valued monotone increasing function $f:\mathbb R^+\to\mathbb R^+$ satisfying $f\!\left(x+x^\phi\right)=x,$ where $\phi=\frac{1+\sqrt5}2$ is the golden ratio. Here is a plot of the function $f(z)$ : I need to find an asymptotic expansion of $f(z)$ for $z\to0^+$ in terms of powers of $z$ . By manually balancing coefficients, I was able to find a few first terms: $$f(z)=z-z^\phi\color{red}+\phi\,z^{2\,\phi-1}+\mathcal O\left(z^{3\,\phi-2}\right), \quad z\to0^+.$$ How can I find more terms and, ideally, a general formula for all terms of this series? Update : A wrong sign in formula is corrected (red).","['golden-ratio', 'inverse-function', 'asymptotics', 'real-analysis', 'sequences-and-series']"
3946606,Applications of the orbit stabiliser theorem,"I'm trying to teach the orbit-stabiliser theorem. One nice application of the theorem is counting the number of symmetries of different platonic solids and regular n-gons (you count the orbit and stabiliser of some feature, e.g. a particular face).  Another is counting the number of distinct 'words' that you can get my permuting the letters of a given word, e.g. BANANAS (you want the orbit of the action of $S_n$ on the letters, which you find by dividing $n!$ by the size of the stabiliser.  In this case, the stabiliser has size $3!\cdot 2!$ - you can permute the As and the Ns separately.). I'd like a couple of other examples of applications to demonstrate that the (finite) orbit-stabiliser theorem is useful.  Ideally, they would be of obvious interest outside of group theory and wouldn't require developing much theory to understand them.  (The two examples above meet both these criteria in my mind.) Does anyone have any good examples?","['group-theory', 'finite-groups']"
3946614,Proving a group is a normal subgroup from its order,"Suppose $G$ and $H$ are groups such that $H$ is a subgroup of $G$ , and $o(G)=120$ and $o(H)=24$ , and there exists a in $G-H$ such that $aH=Ha$ .
Prove that $H$ is a normal subgroup of $G$ . I've been stuck on this for days now. have tried Lagrange theorem, indexes, and many other methods with no luck. please help.","['normal-subgroups', 'group-theory', 'abstract-algebra']"
3946639,Can we have $n$-adic integers?,"I am starting to read about $p$ -adic integers and one thing struck me. I have some related questions below. Can anybody help me out? Can we have $n$ -adic integers if we try to construct it in the same way? Like if we take $n=4$ , then the number $10=2+4.2$ , in $4$ -adic integer becomes $22$ ? If $n=pq$ , does numbers represented in this $n$ -adic form split into a number in $\mathbb Z_p\times\mathbb Z_q$ , where $\mathbb Z_p,\mathbb Z_q$ are fields of $p$ and $q$ -adic integers? The reason I am asking it is, clearly we can represent numbers in this form which I call $n$ -adic integers, in which the number $a_0a_1...a_k$ , $a_i\in \mathbb{Z}_n$ , can be splitted into a tuple in $\mathbb Z_p\times\mathbb Z_q$ , by mapping each $a_i$ to $a_i\;mod\;(p)$ and $a_i\;mod\;(q)$ . Am I correct?","['number-theory', 'p-adic-number-theory', 'prime-numbers']"
3946649,The letter A appears an even number of times. How many different sequences could Dr. Lizardo have written down? [duplicate],"This question already has answers here : What is the number of strings at size $n$ that is constructed from ${a,b,c,d}$ and there is an even num of $a$ (3 answers) Closed 3 years ago . Dr. Lizardo writes down a sequence of $n$ letters, where each letter is A, B, or C, and the letter A appears an even number of times. How many different sequences could Dr. Lizardo have written down? What I have tried so far The last letter is predefined in terms of ""A or not"" based on the first $n-1$ letters. If there are an odd number of As so far, the last one has to be A, otherwise, B or C. Therefore, there are $3^{n-1}$ ways to write the first $n-1$ letters. However, the last letter can either have 1 choice or 2 choices. I am thinking maybe we need to do $S_n = 3^{n-1} + S_{n-1}$ . However, I need a closed form, not a recurrance. How can I do this? Thank you! Note: $S_n = 3^{n-1} + S_{n-1}$ seems to be working for the first few values of $n$ (I tested through $n=4$ ), so finding a closed form of that recurrance would work as well.","['combinatorics', 'discrete-mathematics', 'recursion']"
3946650,The probabilty of the a 6-letter word is a palindrome,"a cat is walking on the keyboard. Given that the cat walked just on letters (with an equal chance for each letter A-Z) and wrote a 6-letter word, what is the probability that this word is a palindrome.",['probability']
3946708,Finding the area not by integral,"Original problem. Find the area under $y= \sqrt{x}$ in the range $\left [ 0, 1 \right ]$ My friend, she wants to use total area instead of calculating the integeral. So I tried something: Dividing the area by $n$ parts with each part is $\Delta x= \frac{1}{n}$ . The part $k$ is $\left [ \left ( k- 1 \right )\frac{1}{n}, \frac{k}{n} \right ]$ , we calculate the total area by formula: $$S_{n}= \sum_{k= 1}^{n}\frac{1}{n}\sqrt{\frac{k}{n}}= \frac{1}{n^{3/2}}\sum_{k= 1}^{n}k^{1/2}$$ How should I do next ?? Thank you....","['integration', 'area', 'calculus', 'limits', 'derivatives']"
3946739,Maximum area of a triangle inscribed in a circle with radius r.,"We want to find the maximum area of a triangle inscribed in a circle with radius r and with constant difference of two of its angles. If $a, b, c$ are the angles of the triangle, if we set, wlog that $a>b$ , we need to have: $a-b = k$ (constant) and $a+b+c=180$ , so $a+b = 180-c$ I know that in general, without the restriction of the $2$ angles fixed difference, the largest triangle is the equilateral.
Any assistance is much appreciated (by the way this is not homework or anything; just challenge between friends).",['geometry']
3946769,"A simple proof for Glasser: $\int_{-\infty}^{\infty} f(x-a/x) dx=\int_{-\infty}^{\infty} f(x) dx, a>0$","$$\int_{-\infty}^{\infty} f(x-a/x) dx=\int_{-\infty}^{\infty} f(x) dx~~~~(1)$$ Several difficult integral can be handled using (1) easily. For instance we know that $$\int_{-\infty}^{\infty} \text{sech}^2 x~dx=2~~~~~~~~~~~~(2)$$ but what is interesting is that one may verify numerically that $$\int_{-\infty}^{\infty} \text{sech}^2(x-a/x)~dx=2, a>0.~~~~~~~~~(3)$$ This interesting property (1) follows from Glasser's Master Theorem . See one more interesting integral and its solutions in MSE posts. In (1) what is also interesting is that LHS is independent of $a(>0)$ . I have tried proving (1) or even (3) by the ordinary rules of integration ( Real Analysis) without a success. Can someone help me? EDIT: One more friendly (doable by hand) twin is $$\int_{-\infty}^{\infty} \frac{dx}{1+x^2}=\pi=\int_{-\infty}^{\infty} \frac{dx}{1+(x-a/x)^2} dx , a >0$$","['integration', 'definite-integrals']"
3946877,Closed form of the sum $\sum_{n=1}^{\infty}\frac{H_n}{n^x}$,Some days ago I derived the identity $$\sum_{n=1}^{\infty}\frac{H_n}{n^2}=2\zeta(3)$$ where $H_n$ is the $n$ th Harmonic number. Other related identities include $$\sum_{n=1}^{\infty}\frac{H_n}{n^3}=\frac{\pi^4}{72}$$ $$\sum_{n=1}^{\infty}\frac{H_n}{n^4}=\frac{-1}{6}\pi^2\zeta(3)+3\zeta(5)$$ $$\sum_{n=1}^{\infty}\frac{H_n}{n^5}=\frac{1}{540}(\pi^6-270\zeta(3)^2)$$ (I proved all except the last one) Now I wondered if there is a closed form for the generalized form $$\sum_{n=1}^{\infty}\frac{H_n}{n^x}$$ For some real number $x$ . Here is my try: The well known identity $\psi(n+1)=H_n-\gamma$ gives $$\sum_{n=1}^{\infty}\frac{H_n}{n^x}=\sum_{n=1}^{\infty}\frac{\psi(n+1)}{n^x}-\gamma\zeta(x)$$ But I don't know what to do further. Any help would be appreciated.,"['summation', 'digamma-function', 'harmonic-numbers', 'closed-form', 'sequences-and-series']"
3946927,Projective norm for Banach spaces,"The projective norm of tensors from (algebraic) tensor product of Banach spaces $X,Y$ is defined as $$\|t\|_\wedge = \inf\left\{ \sum\limits_{j=1}^N \|x_j\|\|y_j\| \, : \, t=\sum\limits_{j=1}^N x_j\otimes y_j\right\}.$$ In one place I found slightly different definition: $$\|t\|_\wedge '=\inf\left\{\left(\sum\limits_{j=1}^N \|x_j\|^2\right)^{1/2}\left(\sum\limits_{j=1}^N \|y_j\|^2\right)^{1/2} \, : \, t=\sum\limits_{j=1}^N x_j\otimes y_j\right\}.$$ Are they indeed equal? It is easy to see one inequality, but why we have the opposite one?","['banach-spaces', 'functional-analysis']"
3946964,Renewal Process with continuous interarrival times of finite expectations: prove $E[S_{N(t)+1}^2]=E[X_1^2](m(t)+1) - 2E[X_1] \int_{0}^{t} m(x) dx$,"Consider a renewal process $\{N(t), t ≥ 0\}$ whose interarrival times $\{X_i\}$ are $IID$ continuous random variables.
Assume that $E[X_1^2]$ is finite (so $E[X_i^2]$ is finite in general). Prove that $E[S_{N(t)+1}^2]=E[X_1^2](m(t)+1) - 2E[X_1] \int_{0}^{t} m(x) dx$ where $m(t)$ is the renewal function. Facts that could be useful: We proved that $E[S_{N(t) + 1}] = E[X_1](m(t) + 1)$ , which looks very similar to the first part of the expression above. Perhaps using this or modifying the proof for this in some way? In the proof, we used the renewal equation $m(t) = F(t) + F * m(t)$ and in another proof we first calculated $E[\gamma_t]$ by conditioning on $X_1$ and then using the renewal equation, then as $\gamma_t = S_{N(t) + 1} - t$ , we can extract the expectation of $S_{N(t) + 1}$ from it. Any help would be great!","['conditional-expectation', 'renewal-processes', 'expected-value', 'probability-theory', 'probability']"
3946986,Proving a bounded set is Lebesgue Measurable,"This is an exercise question from the book Measure,Integration and Real Analysis  by Sheldon Axler. Suppose $ b < c$ and $A\subset(b,c)$ . Prove that A is Lebesgue measurable if and
only if $ | A | + |( b, c ) \setminus A | = c − b.$ Here $|A|$ represents outermeasure of A. I'm trying to prove the converse part. Definition of Lebesgue Measurable set :  A Set A is said to be Lebesgue Measurable if $ \forall  \epsilon>0 , \exists   F(closed set) \subset A$ such that $|A\setminus F|<\epsilon $ I'm trying to prove this by contradiction. Suppose A isn't Lebesgue Measurable. $\exists \epsilon>0$ such that $|A\setminus F|>\epsilon $ for all closed $F\subset A$ . Similarly $(b,c)\setminus A$ isn't Lebesgue measurable $\exists \epsilon>0$ such that $|((b,c)\setminus A) \setminus F|>\epsilon $ for all closed $F\subset (b,c)\setminus A$ Since (b,c) is Lebesgue measurable, $ \forall  \epsilon>0 , \exists   F(closed set) \subset (b,c)$ such that $|(b,c)\setminus F|<\epsilon $ If F  is closed subset of A and E is closed subset of (b,c)\A then $|(b,c)\setminus (F\cup E)| = |A\setminus F|+|((b,c)\setminus A)\setminus E|$ I dont know how to arrive at a contradiction from this.","['measure-theory', 'lebesgue-measure', 'outer-measure']"
3947040,"Solving Dudeney's ""A Question of Cubes"": sum of consecutive cubes is a square","The following is a puzzle of Dudeney: Professor Rackbrane pointed out one morning that the cubes of successive numbers, starting from $1,$ would sum to a square number... He stated that if you are forbidden to use the $1,$ the lowest answer is the cubes of $23,24,25,$ which together equal $204^2.$ He proposed to seek the next lowest number using more than three consecutibe cubes and as many more as you like excluding $1.$ There is an answer provided but a proof is not included: The cubes of $14,15,$ up to $25$ inclusive (twelve in all) add up to... the square of $312.$ The next lowest answer is the five cubes of $25,26,27,28,$ and $29,$ which together equal $315^2.$ My query is more general: Classify all finite sets of consecutive positive integers, the sum of whose cubes is a square. Any idea how we can do this? If no one manages to answer this question, I will accept an answer that shows how Dudeney came to the minimal solutions. Here are my thoughts on the matter so far:
Clearly the sum of first $n$ positive cubes works. The sum of the cubes of $m+1,m+2,\ldots,n$ for positive $n$ and non-negative $m$ is $$(1^3+2^3+\cdots+n^3)-(1^3+2^3+\cdots+m^3) = \left[\frac{n(n+1)}{2}\right]^2-\left[\frac{m(m+1)}{2}\right]^2.$$ If this is equal to a square, it is equivalent to seeking all Pythagorean triples such that at least two of the elements of the triple are triangular numbers. At that point, I tried to use the classification of all (primitive) Pythagorean triples, but that went nowhere.","['puzzle', 'number-theory', 'pythagorean-triples', 'diophantine-equations', 'recreational-mathematics']"
3947061,How to find asymptotic expansion of $\frac{1}{\cos(z)}$,"Find the asymptotic expantion of coefficients of the exponential generating function $f(z)=\frac{1}{\cos(z)}$ using all of its poles. My work: I know that: $$
\frac{1}{\cos \left( z \right)}=\sum_{n\ge 0}{E_{2n}}\frac{z^{2n}}{\left( 2n \right) !}
$$ To find the poles we solve $\cos(z)=0$ where z is a complex number. Getting, $\chi_k=\pi/2+\pi k$ where $k \in Z$ .
Now wrie $\frac{1}{\cos(z)} \sim  \frac{1}{z-\chi_k}= \frac{-1}{\chi_k} \frac{1}{1-z/\chi_k}$ .(I'm not sure if this what I am supposed to do).
Then $$
E_{2n}=-\left( 2n \right) !\sum_{k\in Z}{\frac{1}{\chi _k}}\frac{1}{\chi _{k}^{n}} \\ 
E_{2n}=-\left( 2n \right) !\left( \frac{1}{\left( \frac{\pi}{2} \right) ^{n+1}}+\sum_{k\ge 1}{\left( \frac{1}{\chi _{k}^{n+1}}+\frac{1}{\chi _{k_-}^{n+1}} \right)} \right) 
$$ Then I can look at the internal sum in the last equation and try to simplify it. Edited: $f(z)=\frac{1}{cos(z)} \sim \frac{Res(f(z),\chi_k)}{z-\chi_k}$ where $\chi_k=\frac{\pi}{2}+2\pi k$ the
poles of $f(z)$ and $Res(f(z),\chi_k)= lim_{z\to \chi_k} (z-\chi_k) f(z)$ . After calculations we get that: $\chi_0= \pi /2$ with Res= -1, $\chi_{-1} = -\pi / 2$ with Res = 1 , $\chi_1=3\pi /2$ with Res = 1, $\chi_{-2}= -3\pi /2$ with Res = -1 etc. So, $f(z)=\frac{1}{cos(z)} \sim \frac{Res(f(z),\chi_k)}{z-\chi_k} \sim \frac{-1}{z-1/2 \pi}  +\frac{1}{z+1/2 \pi} +\frac{1}{z-3/2 \pi} + \frac{-1}{z+3/2 \pi}+....$ Then if we expand these geometric coulomns and look at their coefficients we get: $\frac{E_{2n}}{n!} \sim (2/ \pi)^{n+1} + (-2/ \pi)^{n+1} - (2/ 3\pi)^{n+1} +(2/ 3\pi)^{n+1} - (2/ 5\pi)^{n+1} -(-2/ 5\pi)^{n+1}-(2/ 7\pi)^{n+1} +(-2/7\pi)^{n+1}...)$ Thus, $E_{2n} \sim (2n)! (  (2/ \pi)^{n+1} + (-2/ \pi)^{n+1}  - (2/ 5\pi)^{n+1} -(-2/ 5\pi)^{n+1}-(2/ 7\pi)^{n+1} +(-2/7\pi)^{n+1}...)$ Thus $E_{2n} \sim (2n)! (  (2/ \pi)^{n+1} + (-2/ \pi)^{n+1} +O(2/ 5\pi)^{n})$ What do you think.
Any guide will be so appreciated.","['singularity', 'asymptotics', 'combinatorics', 'generating-functions', 'exponential-function']"
3947108,How to get eigenvectors using QR algorithm?,"From everything I've heard, this matlab code ought to spit out a matrix where each row is the same. So why doesn't it? A = [ 5  2  0  0;
      3  9  4  0;
      0  9  5 -2;
      0  0 -3  4 ];
B=A;
QQQ=eye(4);

%QR algorithm
for i=1:100
    [Q,R] = qr(B);
    B=R*Q;
    QQQ = QQQ*Q;
end

(A*QQQ)./QQQ %should have constant rows, but doesn't","['unitary-matrices', 'matlab', 'eigenvalues-eigenvectors', 'matrices', 'matrix-decomposition']"
3947123,Determine for which α ∈ R the series is absolutely convergent.,"Determine for which α ∈ R the series: $$\sum_{n=1}^{\infty}\frac{\sin\big(e^{\sqrt{n+1}-\sqrt{n}} + \pi n -1\big)}{\ln(\arctan\frac{1}{n})}n^\alpha$$ is:
a) aboslutely convergent,
b) convergent,
c) divergent. I've tried an approach where I considered the nominator and the denominator separately using direct comparison test for both and: Taylor's theorem to approximate $e^{\sqrt{n+1}-\sqrt{n}}$ as $e^x = x + 1 + o(x):$ $e^{\sqrt{n+1}-\sqrt{n}} = {\sqrt{n+1}-\sqrt{n}} + 1 + o(\sqrt{n+1}-\sqrt{n}) $ The fact that $\arctan\frac{1}{n} = \frac{π}{2} -\arctan{n} $ Neither of these helped me solve this problem. Any suggestions? Thanks for the help.","['calculus', 'convergence-divergence', 'sequences-and-series']"
3947145,Sequence of shifts in Hilbert Space,"I'd like someone to check my work on this 2-part problem: Let $(V_n )_{n \in \mathbb{N}}$ be a sequence of operators on $\ell^2(\mathbb{N})$ defined by $(V_n x)(k) = x(n + k)$ . Prove that $(V_n )_{n \in \mathbb{N}}$ converges in strong operator topology and identify its limit. Prove that the sequence of
adjoints $(V^*_n )_{n \in \mathbb{N}}$ does not converge in the strong operator topology. My approach: Consider the limit of the square norm of the image: $lim_{n\to \infty}||V_n x||^2 =lim_{n\to \infty} \sum_{k=n}^\infty{x_k^2} = ||x||^2 - lim_{n\to \infty}\sum_{k=1}^n{x_k^2} = 0$ . Therefore $||V_nx|| \to 0$ for all $x$ , and $V_n$ converges to $0$ in SOT. For the second part, i'm quite sure that the adjoint of $V_n$ is $(V^*_nx)(k)= 0$ if $k\leq n$ , and $(V^*x)(k)=x(k-n)$ otherwise. It is then enough to mention an $x$ for which the limit $V^*_nx$ does not exist. $x_0=1,0,0...$ will do, since the sequence $V^*_nx_0$ is not Cauchy and therefore not convergent. Therefore, $V_n^*$ does not converge in SOT. any corrections and remarks are greatly appreciated, thank you!","['hilbert-spaces', 'solution-verification', 'functional-analysis']"
3947214,Having trouble of reaching a specific expression for Maxwell's rheological model with 2 modes from a given formula.,"I have an expression for a Maxwell viscoelastic rheological model with 2 modes. I have tried derivation for both sides but I always seem to reach an impasse.
I tried to get second derivation as well but im missing some terms that should have stayed behind.
I know have to get the second derivation of the original expression to get to the first (I CAN SEE THAT) but i cant really get it to match. Any opinions, inputs on the matter? Tau is a stress tensor thus irrelevant with time and λ1, λ2 are relevant as they are relaxation times. original expression $\boldsymbol{\tau}=\int_{-\infty}^{t}\left(\mathrm{G}_{1} \mathrm{e}^{-\left(t-t^{\prime}\right) / \lambda_{1}}+\mathrm{G}_{2} \mathrm{e}^{-\left(t-t^{\prime}\right) / \lambda_{2}}\right) \dot{\gamma}\left(\mathrm{t}^{\prime}\right) \mathrm{dt}^{\prime}$ final expression $\boldsymbol{\tau}+\left(\lambda_{1}+\lambda_{2}\right) \frac{\partial \boldsymbol{\tau}}{\partial \mathrm{t}}+\lambda_{1} \lambda_{2} \frac{\partial^{2} \boldsymbol{\tau}}{\partial \mathrm{t}^{2}}=\left(\eta_{1}+\eta_{2}\right)\left[\dot{\gamma}+\left(\frac{\lambda_{2} \eta_{1}+\lambda_{1} \eta_{2}}{\eta_{1}+\eta_{2}}\right) \frac{\partial \dot{\boldsymbol{\gamma}}}{\partial t}\right]$ For $\eta_{1}=\lambda_{1} *{G}_{1}$ $\eta_{2}=\lambda_{2} *{G}_{2}$","['integration', 'functions', 'physics', 'derivatives', 'fluid-dynamics']"
3947217,Sampling inspection - Joint distribution,"I am self-learning probability theory from William Feller's Introduction to Probability theory and its applications . I would like to ask for some help in deriving the correct solution to the below very interesting problem. Problem IX.12 Suppose that items with a probability $p$ of being acceptable are subject to inspection in such a way, that the probability of an item being inspected is $p'$ . We have four classes, namely, ""acceptable and inspected"", ""acceptable but not inspected"" and so forth with probabilities $pp'$ , $pq'$ , $p'q$ and $qq'$ . We are concerned with double Bernoulli trials. Let $N$ be the number of items passing the inspection desk (both inspected and uninspected) before the first defective is found, and let $K$ be the (undiscovered) number of defectives among them.  Find the joint distributions of $N$ and $K$ , and the marginal distributions. Solution (My Attempt). We have, \begin{array}{c|cc}
& \text{Acceptable} & \text{Defective}\\
\hline
\text{Inspected} & pp' & qp'\\
\text{Undiscovered} & pq' & qq'\\
\end{array} The first defective item is found at trial number $(n+1)$ , if the preceding $n$ items were either acceptable or uninspected. $P\{\text{Acceptable} \cup \text{Uninspected}\} = P\{\text{Defective},\text{Inspected}\}^C = 1 - qp'$ $N$ is the waiting time to the first defective. $N$ follows a geometric distribution. $P\{N = n\} = (1 - qp')^n qp' \tag{1}$ $K$ is the number of undiscovered defectives among these $n$ trials. So, given that we waited for time $n$ to find the first defective, the probability that the number of defectives equals $k$ is given by, $P\{K = k \vert N = n\} = {n \choose k} (qq')^k p^{n-k}$ Thus, the joint distribution \begin{align*}
P\{K = k, N = N\} &= P \{K = k \vert N =n \} \cdot P \{N = n\}\\
&= {n \choose k} (qq')^k p^{n-k} (1 - qp')^n qp'
\end{align*} However, the textbook states the expression for the joint distribution as, $$
P\{K = k, N = n\} ={n \choose k}(qq')^k p^{n-k} qp'
$$ Also, how to sum over all $n$ to derive an expression for the marginal distribution of $N$ ?","['solution-verification', 'binomial-distribution', 'probability']"
3947227,Example of quasi Yamabe gradient soliton,"A $(M,g)$ Riemannian manifold is called quasi Yamabe gradient soliton if there exists a smooth function $f\in C^\infty(M)$ such that the following condition holds $$Hess(f)=(R-\lambda)g+\mu df\otimes df,$$ where $R$ is the scalar curvature of $g$ and $\lambda,\mu$ are constants. The concept of quasi Yamabe soliton was first introduced by Huang, Guangyue; Li, Haizhong , On a classification of the quasi Yamabe gradient solitons , Methods Appl. Anal. 21, No. 3, 379-390 (2014). ZBL1304.53033 . But I am not able to find any nontrivial example of quasi Yamabe gradient soliton in Euclidean manifold with some proper metric. In the paper, Wang, Lin Feng , On noncompact quasi Yamabe gradient solitons , Differ. Geom. Appl. 31, No. 3, 337-348 (2013). ZBL1279.53039 , there is an example in warped product manifold but I need in Euclidean space. Please help me to find an example of that. Thank you","['riemannian-geometry', 'differential-geometry']"
3947273,Bijection between open and partially closed interval,"i need some help finding an explicit bijection between $(0,1)$ and $[0,2]\cup[3,5]$ . I know it exists because of Cantor's theorem but im struggling to find it. I've tried with dividing the problem to use an already known bijection from $[0,2]$ to $(0,\frac{1}{2})$ ,then finding a bijection from $(3,5]$ to $(\frac{1}{2},1)$ while sending 3 into $\frac{1}{2}$ . My last try was the following but it ended up not being onto: \begin{equation}
S={x|x=\frac{1}{n+2}+\frac{1}{2},n\in N^{+}}\\
f(x)=
\begin{cases}
       5(\frac{1}{n+1}+\frac{1}{2}), & x\in S \\
      4x+1, & x\notin S
\end{cases}
\end{equation} I'd appreciate some help on this please.",['elementary-set-theory']
3947400,Confused on what a series means.,"I have been reading the book Relativity: The special & General Theory where in chapter XV, the author develops the expression of kinetic energy $$
\frac{1}{2} mv^2
$$ or $$
\text{ }m\frac{v^2}{2}
$$ in the form of a series, $$
mc^2+\text{ }m\frac{v^2}{2}+\frac{3}{8}m\frac{v^4}{c^2}+...
$$ Can someone explain to me what a series is? Or at the very least show me where I can find more information about this? Thank you.",['sequences-and-series']
3947401,"Order of centralizer and cyclic group over $(G,\ast)$","I'm struggling to answer this question, Let $(G,\ast)$ be a group with $n$ elements and $a\in G$ with the following property $ab=ba \iff b\in \{ a^k\mid k\in \mathbb{Z} \}$ ,
if $m$ is the order of $a$ in $G$ , show that at least $\frac{n}{m}$ elements of order $m$ exist in $G$ . To me, it initially looks like something related to the centralizer and cyclic group, maybe $Z(x)=\langle x \rangle$ ? Some research landed me into thinking I might have to use something like if $\frac{G}{Z(G)}$ is cyclic $\implies G$ is abelian? Any help would be appreciated.","['group-theory', 'abstract-algebra', 'abelian-groups', 'cyclic-groups']"
3947463,"Show that $x^\frac{1}{n}$ is continuous at all $a \in [0,\infty)$","I have to show $x^\frac{1}{n}$ is continuous using: $|x-a| = |(x^\frac{1}{n})^n - (a^\frac{1}{n})^n| = |x^\frac{1}{n} - a^\frac{1}{n}||\displaystyle \sum_{k=1}^{n-1} (x^\frac{1}{n})^{n-1-k} + (a^\frac{1}{n})^{k} |$ Here's what I did: $a=0$ : $|x^\frac{1}{n}| < \epsilon => |x| < \epsilon^n$ Pick $\delta = \epsilon^n$ , and we have $f(x)$ is continuous at $x=0$ $a>0$ : $|x-a| = |x^\frac{1}{n} - a^\frac{1}{n}||\displaystyle\sum_{k=1}^{n-1} (x^\frac{1}{n})^{n-1-k} + (a^\frac{1}{n})^{k}| < \epsilon \cdot |\displaystyle\sum_{k=1}^{n-1} (x^\frac{1}{n})^{n-1-k} + (a^\frac{1}{n})^{k}|$ Since this sum evaluates to a real number $R > 0$ , we can pick $\delta = R\epsilon $ and we get: $0<|x-a| < \delta => |x^\frac{1}{n} - a^\frac{1}{n}| < \epsilon$ Is this correct or is there anything else I need to show?
Also, this is my first time formatting with MathJax, so if there are any errors please let me know!","['continuity', 'calculus', 'functions', 'real-analysis']"
3947510,Poset generated by intervals with inclusion,"Let $P$ be a poset and consider the poset $Q$ generated by all closed intervals of $[x,y]\subseteq P$ with inclusion, that is $[x,y]\leq_{Q} [z,w] \iff x\geq_Pz$ & $y\leq_P w$ . We add a minimum element $\hat{0}$ and a maximum element $\hat{1}$ to $P,Q$ . Show that $\mu_P(\hat{0},\hat{1})=\mu_Q(\hat{0},\hat{1})$ , where $\mu_P$ is the Mobius function of $P$ .","['order-theory', 'combinatorics', 'discrete-mathematics']"
3947546,"Notation for maps: is ""$A$"" in ""$\alpha : A \to B$"" always the domain?","According to the Wikipedia page on functions / maps (emphasis added): A function is a process or a relation that associates each element x of a set X, the domain of the function , to a single element y of another set Y (possibly the same set), the codomain of the function. It then goes on to say, in the subsection Specifying a function By a formula , from the same page (emphasis added): For example, ${\displaystyle f(x)={\sqrt {1+x^{2}}}}$ defines a function $f\colon \mathbb{R} \to \mathbb{R}$ whose domain is ${\displaystyle \mathbb {R} ,}$ because ${\displaystyle 1+x^{2}}$ is always positive if $x$ is a real number. On the other hand, $f(x)=\sqrt{1-x^2}$ defines a function from the reals to the reals whose domain is reduced to the interval $[–1, 1]$ . which would seem to suggest that, for the latter function, one should write:
""let $f \colon \mathbb{R} \to \mathbb{R}$ be the function defined by the equation $f(x)  = \sqrt{1-x^2}$ , valid for all $x \in [-1,1]$ "", i.e. $\mathbb{R}$ is not the domain, but some set containing the domain; however, in another subsection, Notation Arrow notation it says: For explicitly expressing [the] domain $X$ and the codomain $Y$ of a function $f$ , the arrow notation is often used (read: ""the function $f$ from $X$ to $Y$ "" or ""the function $f$ mapping elements of $X$ to elements of $Y$ ""): $$f\colon X\to Y$$ which seems to imply that it is not correct to write $f: \mathbb{R} \to \mathbb{R}$ for the function $x \mapsto \sqrt{1-x^2}$ , as the domain is not $\mathbb{R}$ , nor is the codomain $\mathbb{R}$ . Should one write $$ \begin{array}{ll}f\colon \mathbb{R} &\hspace{-0.6em} \to \mathbb{R} \\[5pt] & \hspace{-0.6em} x \mapsto \sqrt{1-x^2}, \end{array}$$ where $\mathbb{R} \to \mathbb{R}$ implies that the domain and image are subsets of $\mathbb{R}$ , respectively? or should one write: $$
\begin{array}{ll}f:[-1,1] & \hspace{-0.6em} \to [0,1] \\[5pt] & \hspace{-0.6em} x \mapsto \sqrt{1-x^2}, \end{array}
$$ N.B. My interest is not just pedantic: I have never studied maps, formally, and am trying to clarify whether a map from the set $A$ onto the set $B$ requires only that every element in $B$ be an image, or whether it also requires that every element of $A$ have an image, as well. I've consulted 2 different textbooks, as well as the Wikipedia page; unfortunately, although great care is given to distinguish the codomain and the image, all the explanations and examples provided do not address whether the domain is $A$ or if it may be a subset of $A$ . Moreover, several textbooks I've read have made much ado about the fact that one ought to distinguish between a function and a function of and imply that the distinction is to be found in these definitions (unfortunately, none of them consider it important enough to actually clarify within their own text).","['notation', 'functions']"
3947548,Stronger version of Taylor's Theorem,"Show that Taylor's Theorem may be strengthened as follows: Let $f$ be a continuous real-valued function on the closed interval in R of extremities $a$ and $b$ . That is, (n + 1) times differentiable on the open interval with these same extremities and suppose that $\lim_{x \to a} f'(x), \lim_{x \to a} f''(x), \dots, \lim_{x \to a} f^{(n)}(x)$ exist and that $f',f'',\dots,f^{(n)}$ are bounded. Then, $$f(b) = f(a) + (\lim_{x \to a} f'(x)) \frac{(b - a)}{1!} + \dots | (\lim_{x \to a} f^{(n)}(x)) \frac{(b - a)^n}{n!} + f^{(n + 1)}(c) \frac{(b - a)^{n + 1}}{(n + 1)!}$$ for some c between a and b. I'm not too familiar with Taylor's Theorem in an Analysis sense. We briefly talked about it in class, but we sort of moved on quickly. This was said to be a ""cool problem"" to complete, so I would like to see it. Can anyone help me with this one? Thank you!","['derivatives', 'taylor-expansion', 'real-analysis']"
3947638,Prove $\iiint_{\mathbb{R}^3}\left ( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) dxdydz=0$,"Assume $f(x,y,z)$ is continuously differentiable on $\mathbb R^3$ , and both the integral $$
\iiint_{\mathbb{R}^3}{\left| f\left( x,y,z \right) \right|\text{d}}x\text{d}y\text{d}z
$$ and $$
\iiint_{\mathbb{R}^3}{\left( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) \text{d}}x\text{d}y\text{d}z
$$ exists. Prove that $$\iiint_{\mathbb{R}^3}{\left( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) \text{d}}x\text{d}y\text{d}z=0$$ I don't have any ideas about the problem. Can anyone help?","['multivariable-calculus', 'analysis', 'real-analysis']"
3947660,Solving $(\sin x-\cos x)^2+\tan x=2\sin^2x$,"From this post ( Solving $\frac{\cos^23t}{\tan t}+\frac{\cos^2t}{\tan3t}=0$ ), which is my post, I have tried solving the last equation. I have gotten a solution(s). Would like to see if I did correctly. $$\bigl(\sin\left(x\right)-\cos\left(x\right)\bigr)^2+\tan\left(x\right)=2\sin^2\left(x\right)$$ Expanding: $$sin^2\left(x\right)-2sin\left(x\right)\cos\left(x\right)+\cos^2\left(x\right)+tan\left(x\right)=2\sin^2\left(x\right)$$ Using $\sin^2\left(x\right)+\cos^2\left(x\right)=1$ , and bringing the RHS to the LHS, the expansion becomes: $$-2sin\left(x\right)\cos\left(x\right)-2\sin^2\left(x\right)+1+\tan\left(x\right)=0$$ Then using $\tan\left(x\right)=\frac{\sin\left(x\right)}{\cos\left(x\right)}$ and multiplying the LHS by cos(x), I get the following: $$\frac{-2\sin\left(x\right)\cos^2\left(x\right)-2\sin^2\left(x\right)\cos\left(x\right)+\cos\left(x\right)+\sin\left(x\right)}{\cos\left(x\right)}=0$$ Now strictly dealing with the numerator, making two separate terms, one being $-2\sin\left(x\right)\cos^2\left(x\right)-2\sin^2\left(x\right)\cos\left(x\right)$ and the other being $\cos\left(x\right)+\sin\left(x\right)$ , then factoring out $-2\sin(x)\cos(x)$ , I get $$\color{blue}{-2\sin\left(x\right)\cos\left(x\right)+1}\color{red}{\bigl(\cos\left(x\right)+\sin\left(x\right)\bigr)}+\color{red}{\bigl(\cos\left(x\right)+\sin\left(x\right)\bigr)}=0$$ This makes two terms $$\biggl(\color{blue}{-2\sin\left(x\right)\cos\left(x\right)+1}\biggr)\biggl(\color{red}{\cos\left(x\right)+\sin\left(x\right)}\biggr)=0$$ For the first term, aka the red term, I got $$\tan(x)=-1$$ where $$x=\frac{3\pi}{4}+n\pi $$ and the second term, aka the blue term.
using the identity $\sin\left(2x\right)=2\sin\left(x\right)\cos\left(x\right)$ $$\sin(2x)=1$$ where $$x=\frac{\pi}{4}+n\pi$$ Thanks for reading!","['algebra-precalculus', 'problem-solving', 'trigonometry']"
3947667,Determinant and inverse matrix calculation of a special matrix [duplicate],"This question already has answers here : Determinant of a Toeplitz matrix (2 answers) Closed 3 years ago . Is there any smart way of calculating the determinant of this kind matrix? \begin{pmatrix}
1 & 2 & 3 & \cdots &  n \\
2 & 1 & 2 & \cdots & n-1 \\ 
3 & 2 & 1 & \cdots & n-2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
n & n-1 & n-2 & \cdots & 1 \end{pmatrix} I encountered this in a problem for the case $n=4$ . I need to find the inverse matrix. I doubt the idea of this problem is to calculate all the cofactors and then the inverse matrix in the usual way. I see the pattern here and some recursive relations... but I am not sure if this helps for calculating the determinant of the existing matrix.","['matrices', 'determinant', 'linear-algebra']"
3947718,Evaluate $\lim_{x\rightarrow 0} \frac{\left( \cosh x \right) ^{\sin x}-1}{\sinh x\cos \left( \sin \left( x \right) -1 \right)}$,"Evaluate the limit $$
\lim_{x\rightarrow 0} \frac{\left( \cosh x \right) ^{\sin  x}-1}{\sinh x(\cos \left( \sin \left( x \right)  \right)-1)}
$$ My Attempt: I tried to use L'Hôpital's rule to evalute it, however I found that the $1$ st and $2$ nd derivative of the numerator is $0$ at $x=0$ , and the $3$ rd derivative is very complicated. And the method of Taylor's Series is too complicated here. So, my question is , is there any easier way to evaluate this limit? The desired answer is $-1$ .","['limits', 'calculus', 'derivatives', 'taylor-expansion']"
3947746,Is every orthogonal matrix orthogonally diagonalizable?,"Is every orthogonal matrix orthogonally diagonalizable? If so, how do you prove it?
And is it true that the entries of every orthogonal matrix is real? Since every unitary matrix is unitarily diagonalizable, so is true that every orthogonal matrix orthogonally diagonalizable?
Thank you.","['orthogonal-matrices', 'linear-algebra']"
3947762,Transformation law for metric tensor,"For my topology class I have to calculate the transformation law of the metric tensor $g = g_{ij} \, dx^i \otimes dx^j$ under a coordinate transformation $x \longmapsto y=(x)$ . My approach: $$ g(y) \enspace = \enspace g_{ij}(y) \, \big( dy^i \otimes dy^j \big) \enspace = \enspace g_{ij}(y) \, \bigg( \frac{\partial y^i}{\partial x^k} dx^k \otimes \frac{\partial y^j}{\partial x^{\ell}} dx^{\ell} \bigg) $$ $$= \enspace g_{ij}(y) \, \frac{\partial y^i}{\partial x^k} \, \frac{\partial y^j}{\partial x^{\ell}} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad .$$ ${}$ $\large \textbf{1.)} \enspace$ How do I know how the components $g_{ij}(y)$ transform? ${}$ $\large \textbf{2.)} \enspace$ If I identify the terms $\dfrac{\partial y^i}{\partial x^k}$ with the Jacobian $J^i{}_j$ , then I get $$ g(y) \enspace = \enspace g_{ij}(y) \, J^i{}_k \, J^j{}_{\ell} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad .$$ Is it then true that: $$ g(y) \enspace = \enspace \big( \boldsymbol{\operatorname{J}}^T \cdot \boldsymbol{\operatorname{g}}(y) \cdot \boldsymbol{\operatorname{J}} \big)_{k\ell} \, \big(  dx^k \otimes dx^{\ell}  \big) \quad ? $$ ${}$ $\large \textbf{3.)} \enspace$ If I were to calculate the determinant of $g(y)$ , how would I handle the tensor product in a mathematical rigorous way? Assuming that the equation in 2.) is true, would it just be $$ \det g(y) \enspace = \enspace \det \boldsymbol{\operatorname{J}}^T \cdot \det \boldsymbol{\operatorname{g}}(y) \cdot \det \boldsymbol{\operatorname{J}} \quad ? $$","['tensors', 'transformation', 'tensor-products', 'differential-geometry']"
3947818,Large deviations Exercise (Durret 2.7.6),"I'm studying probability theory and doing the exercises in Durret v5. Let $X_1,\ldots, X_n$ iid with $EX_1 = 0$ . Show that if $\epsilon,a>0$ , then
then $$\liminf_{n\to\infty }\frac{P(S_n \geq na)}{nP(X_1 \geq
n(a+\epsilon))}\geq 1$$ Hint: Let $F_n =\{ X_i \geq n(a+\epsilon)
> \text{for exactly one } i\leq n\}$ . Edit: I tried using the inequality $P(S_n \geq na) \geq P(S_{n-1}\geq -n\epsilon)P(X_n \geq n(a+\epsilon))$ , but i don't know what to do next.","['measure-theory', 'probability-limit-theorems', 'probability-theory', 'large-deviation-theory']"
3947871,Recovering length of an interval by a limiting formula,"This is from Tao's text on Measure theory. While proving Lemma 1.1.2-(ii) he's using a discretization argument for length of an interval in $\mathbb{R}$ , which is : $$|I|:=\lim_{N\to \infty}\frac{1}{N}\operatorname{card}\left(I \cap \frac{1}{N}\mathbb{Z}\right)$$ Where $\frac{1}{N}\mathbb{Z}:=\left\{\frac{n}{N}:n \in \mathbb{Z}\right\}$ , and by $\operatorname{card}$ i mean cardinality of a finite set. I do have an intuitive idea for this, but not able to prove the result. I was trying like this: for any $I$ say $I:=[-5,5]$ , if i take some element $i \in [-5,5]\cap\frac{1}{N}\mathbb{Z}$ , then $i=j/N:j\in \mathbb{Z}\cap [-5,5]$ and then $\operatorname{card}\left([-5,5] \cap \frac{1}{N}\mathbb{Z}\right)=|(-5)N-(5)N|$ , afterthat i'm not sure how to finish this argument.","['measure-theory', 'real-analysis']"
3948032,Galois cohomology of projective linear group,"I am currently trying to compute Galois cohomology $H^{1}(\overline{k}/k, PGL_2(\overline{k}))$ . As far as I know these cocycles correspond to isomorphism classes of smooth genus- $0$ curves over $k$ . Any curve of genus $0$ is either isomorphic to $\mathbb{P}^{1}(k)$ or to a quadric $ax^2+by^2+cz^2=0$ which has no $k$ -points. How to understand this from general tools such as exact sequences etc.? For instance, sequence $0\longrightarrow k^{*} \longrightarrow GL_2{(k)}\longrightarrow PGL_2(k)\longrightarrow 0$ gives rise to a long exact sequence of cohomology. However I don't know how to obtain the result I am interested in. I will be very grateful for any help or reference.","['algebraic-curves', 'algebraic-groups', 'galois-cohomology', 'algebraic-geometry', 'abstract-algebra']"
3948088,alternative way of proving $\frac{1}{n}+\frac{1}{n+1}+\frac{1}{n+2}+...+\frac{1}{2n}$ converges to $\ln 2$ without using integrals,"I found a lot of answered to this problem using Riemann - sums, I myself solved it rewriting the logarithm sum, but there is another way where you should use the following hints. Show that $$\exp \left( \lim_{n \to \infty} \sum_{k=1}^n \frac{1}{k+n} \right) =
 2$$ using the following identities (1). There is a nonnegative sequence $x_k$ , converging to zero, and $$\exp \left(\frac{1}{k} \right) = \left(1+\frac{1}{k} \right)\times
 \exp\left(\frac{x_k}{k} \right)$$ (2). $$\lim_{x \to 0} \frac{\exp(x)-1}{x}=1$$ To prove $(1)$ , I wanted to rewrite $$\exp\left(\frac{1}{k} \right)\times \frac{1}{1+\frac{1}{k}}$$ and expand $$\frac{1}{1+\frac{1}{k}} = \frac{1}{1-\frac{-1}{k}}$$ as geometric sequence to find $x_k$ , but I don’t get anything useful. For $(2)$ I thought may one should “stretch” $$\sum_{k=1}^n \left(\exp\left(\frac{1}{k+n}\right)-1 \right)$$ which has the same limit (I proved that) as $\sum_{k=1}^n \frac{1}{k+n}$ and rewrite it as $$\sum_{k=1}^n \frac{\exp\left(\frac{1}{k+n}\right)-1}{\frac{1}{k+n}}\frac{1}{k+n}$$ I’m thankful for any tips or answers!","['power-series', 'limits', 'exponential-function', 'real-analysis']"
3948106,From DAGs to levelled DAGs,"I'm looking into the literature, but I can't find anything interesting, so I thought about asking here for help or for pointers to articles/web pages. I'm looking for the fastest algorithm that solves the following task: Given a connected directed acyclic graph $G(V,E)$ with one root $v_0$ as an input, output a levelled DAG $G'(V',E')$ , with $V \subset V'$ , with the same paths of $G$ . So, given $v_i,v_j \in G$ , the path $v_i \rightarrow v_j$ is in $G'$ , if and only if it is in $G$ In other words, given any vertex $v \in V$ , there can be different paths $v_0 \rightarrow v$ , and these paths can have different lengths. I need a DAG that has the same vertices (plus additionals) and the same paths as the original one, in a way that every path $v_0 \rightarrow v$ has the same length, for every $v \in V'$ . Is it actually possible? If yes, how can this be achieved?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3948112,"How to generalize the Euclidean ""unicycle"" model?","There is a common system of ODEs known as the unicycle model / Dubin's model which describes the kinematics of an ant-like ""unicycle"" that can drive forward with some velocity $v(t) \in \mathbb{R}$ and turn in-place with some angular velocity $\omega(t) \in \mathbb{R}$ about the ground normal. The position of the unicycle / ant is described by $\big{(}x(t), y(t)\big{)} \in \mathbb{R}^2$ and its orientation is described by its ""heading"" angle $\ \theta(t) \in \mathcal{S}^1$ . \begin{align}
\dot{x} &= v\cos(\theta)\\[2pt]
\dot{y} &= v\sin(\theta)\\[2pt]
\dot{\theta} &= \omega
\end{align} These equations rely on the ant traversing a flat 2D plane. I have been thinking a lot about how to generalize this simple model to an arbitrarily curved (but smooth) 2D surface. We can imagine the infinity of geodesics that intersect $\big{(}x(t_0), y(t_0)\big{)}$ . Then $\theta(t_0)$ essentially picks one of those geodesics. While $v > 0$ and $\omega = 0$ , the ant moves what it thinks is ""straight forward."" It will follow the same geodesic until it chooses to turn ( $\omega \neq 0$ ) onto a different geodesic. Suppose our smooth surface is defined by an injective function that embeds it in 3D Euclidean space: $$
r : \mathbb{R}^2 \to \mathbb{R}^3\ ,\ \ \ r(x,y) = \begin{bmatrix} \bar{x}(x,y) \\ \bar{y}(x,y) \\ \bar{z}(x,y) \end{bmatrix}
$$ We have its Jacobian and any necessary higher partial derivatives as well: $$
J(x,y) := \begin{bmatrix}\frac{\partial r}{\partial x}(x,y)\,\ \frac{\partial r}{\partial y}(x,y)\end{bmatrix} \in \mathbb{R}^{3 \times 2}
$$ The velocity vector is, $$
\dot{r} = \dot{x}\frac{\partial r}{\partial x} + \dot{y}\frac{\partial r}{\partial y} = J \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix}
$$ The scalar velocity $v$ in the flat model would correspond to the magnitude of the velocity vector in the general model, i.e. $v = ||\dot{r}||$ . Letting $\hat{\tau}$ be the direction of $\dot{r}$ we can write, $$
\dot{r} = v\hat{\tau} = J \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix}
$$ Non-degeneracy of the surface-coordinates implies that $\frac{\partial r}{\partial x}$ is linearly independent of $\frac{\partial r}{\partial y}$ . Thus, $J$ is full-column-rank. Left-multiplying the above by $J^\intercal$ and inverting yields, $$
\begin{bmatrix} \dot{x} \\ \dot{y}\end{bmatrix} = v(J^\intercal J)^{-1}J^\intercal\hat{\tau} \tag{1}
$$ It's noteworthy that $J^\intercal J$ is the metric tensor in coordinates. This is almost the ODE I'd need for $\big{(}x, y\big{)}$ , but $\hat{\tau}$ is undefined when evaluating the right-hand-side. I tried defining $\hat{\tau}$ in terms of some rotation $\theta$ of the surface gradients (with $\dot{\theta} = \omega$ ) and got cool but erroneous results: $$
\text{This ^ uses Eq.1 with:}\ \ \ \ \hat{\tau} = \cos(\theta)\frac{\frac{\partial r}{\partial x}}{\big{|}\big{|}\frac{\partial r}{\partial x}\big{|}\big{|}} + \sin(\theta)\frac{\frac{\partial r}{\partial y}}{\big{|}\big{|}\frac{\partial r}{\partial y}\big{|}\big{|}}
,\ \ \ \ \dot{\theta}=\omega$$ The error is that for $\omega = 0$ and $v \neq 0$ , holding $\hat{\tau}$ some fixed $\theta$ away from a gradient direction doesn't cause the ant to move along a geodesic. Moreover, it doesn't make sense for $\theta=0$ to yield a $\frac{\partial r}{\partial x}$ -follower... $\hat{\tau}$ really needs to be parallel-transported along the surface. Parallel-transport is a statement about how tangent vectors (e.g. $\dot{r}$ ) change , so we need to examine acceleration. If the ant never turned or varied speed, then its acceleration would be strictly normal to the surface (acceleration just due to curvature): \begin{gather}
\langle \frac{\partial r}{\partial x}, \ddot{r} \rangle \ \ \overset{\dot{v}=\omega=0}{=}\ \ 0\\[3pt]
\langle \frac{\partial r}{\partial y}, \ddot{r} \rangle \ \ \overset{\dot{v}=\omega=0}{=}\ \ 0
\end{gather} The left-hand-side can be expressed more compactly as $J^\intercal \ddot{r}$ . Expanding $\ddot{r}$ , \begin{align}
\ddot{r} &= J \begin{bmatrix} \ddot{x} \\ \ddot{y} \end{bmatrix} + \dot{J} \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix}\\[4pt]
J^\intercal \ddot{r} &= J^\intercal J \begin{bmatrix} \ddot{x} \\ \ddot{y} \end{bmatrix} + J^\intercal \dot{J} \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix} \ \ \overset{\dot{v}=\omega=0}{=}\ \ 0
\end{align} and finally left-multiplying by the inverse metric tensor $(J^\intercal J)^{-1}$ , $$
\begin{bmatrix} \ddot{x} \\ \ddot{y} \end{bmatrix} + (J^\intercal J)^{-1} J^\intercal \dot{J} \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix} \ \ \overset{\dot{v}=\omega=0}{=}\ \ 0 \tag{2}
$$ This is the geodesic equation . The second term can be simplified with Christoffel symbols, but that isn't really necessary. $\dot{J} = \dot{x}\frac{\partial J}{\partial x} + \dot{y}\frac{\partial J}{\partial y}$ is just another computable function to me. I can use Equation 1 to convert some arbitrary initial $\hat{\tau}$ heading direction into an initial condition on $\big{(} \dot{x}, \dot{y} \big{)}$ and then integrate Equation 2 for the motion along the geodesic as long as $\dot{v} = \omega = 0$ . I suspect that to generalize this to the $\omega \neq 0$ and $\dot{v} \neq 0$ case, the right-hand-side will have to be some function of them rather than $0$ . But what function? Edit: And how do we encode ""heading"" so that the ant can ""stand still"" when $v = 0$ , and even turn ""in-place"" (about the normal)? What (if any) ODE system describes the ant / unicycle moving on an arbitrary smooth 2D surface? $$
\overset{?}{f}(x,y,\theta;v,\omega,r) = 0
$$ where $f$ can involve any derivatives up to second-order. Thanks in advance! :) Addendum The answer given by Kajelad below seems correct! Though for posterity I would like to add an elaboration of that answer here, using notation consistent with the above and providing more motivation / steps. First lets define the Frenet-Serret basis: $$
\hat{\tau} := \frac{\dot{r}}{||\dot{r}||},\ \ \ \ \hat{\eta} := \frac{\frac{\partial r}{\partial x} \times \frac{\partial r}{\partial y}}{\big{|}\big{|}\frac{\partial r}{\partial x} \times \frac{\partial r}{\partial y}\big{|}\big{|}},\ \ \ \ \hat{\beta} := \hat{\eta} \times \hat{\tau}
$$ These are the standard tangent, normal, and binormal vectors respectively ( $\hat{\tau}$ being the same as I had previously defined) and they span $\mathbb{R}^3$ orthonormally at all points on the surface. Thus, the ambient acceleration vector can be expressed as some linear combination of them. $$
\ddot{r} = a\hat{\tau} + b\hat{\beta} + c\hat{\eta}
$$ From the previous discussion, we know that if $\dot{v} = \omega = 0$ , then $\ddot{r} = c\hat{\eta}$ will yield geodesic motion. Therefore $a$ and $b$ must depend multiplicatively on $\dot{v}$ and $\omega$ . We also know that this must reduce to the traditional model in the Euclidean case. As we'll see, the correct relations are (rather intuitively), $$
a = \dot{v},\ \ \ \ b = \omega v
$$ For clarity in the following algebra I'll define these shorthands, $$
q := \begin{bmatrix} x \\ y \end{bmatrix},\ \ \ \ \tilde{J} := (J^\intercal J)^{-1}J^\intercal
$$ This makes our previous Eq.1 appear nicely as $\dot{q} = v\tilde{J}\hat{\tau}$ , and we also have $\ddot{r} = J\ddot{q} + \dot{J}\dot{q}$ . Thus, $$
J\ddot{q} = \dot{v}\hat{\tau} + \omega v \hat{\beta} + c\hat{\eta} - \dot{J}\dot{q}
$$ Left-multiplying by $\tilde{J}$ not only cancels the leftmost $J$ , but also kills the $c\hat{\eta}$ term because $J^\intercal \hat{\eta}=0$ . $$
\ddot{q} = \dot{v}\tilde{J}\hat{\tau} + \omega v \tilde{J}\hat{\beta} - \tilde{J}\dot{J}\dot{q} \tag{3}
$$ Next, lets define our generalization of ""heading"" as the unique vector $h(t) \in \mathbb{R}^2$ satisfying, \begin{align}
\hat{\tau} &= Jh\\[4pt]
h &= \tilde{J}\hat{\tau} = \tfrac{1}{v} \dot{q}
\end{align} This vector exists because $\hat{\tau}$ lives in the tangent space (the span of $J$ ). Furthermore, $\hat{\beta}$ also lives in the tangent space as a $90^\circ$ rotation about the normal from $\hat{\tau}$ . Calling that transform $R^{\hat{\eta}}_{90^\circ} \in \mathbb{SO}3$ , we have, \begin{align}
\hat{\beta} &= R^{\hat{\eta}}_{90^\circ} \hat{\tau}\\[4pt]
\tilde{J} \hat{\beta} &= (J^\intercal J)^{-1} J^T R^{\hat{\eta}}_{90^\circ} J h \tag{$\hat{\tau} \to Jh$}\\[4pt]
&= \sqrt{|J^\intercal J|}(J^\intercal J)^{-1}\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}h\\[4pt]
&=: Rh
\end{align} We can derive an ODE for $h$ by using the above relations to put Eq.3 in terms of $h$ . \begin{align}
\ddot{q} &= \dot{v}h + \omega v Rh - v\tilde{J}\dot{J}h\ \ \tag{$\tilde{J}\hat{\tau} \to h,\ \tilde{J}\hat{\beta} \to Rh,\ \dot{q} \to v h$} \\[4pt]
\dot{v}h + v\dot{h} &= \dot{v}h + \omega v Rh - v\tilde{J}\dot{J}h \tag{$\ddot{q} \to \dot{v}h + v\dot{h}$}\\[4pt]
\dot{h} &= \omega Rh - \tilde{J}\dot{J}h \tag{cancellations}
\end{align} The $\dot{J}$ is still hiding a $\dot{q}$ dependence, so lets expand it in terms of the Hessian $H := \frac{dJ}{dq}$ . $$
\dot{J} = \dot{x}\frac{\partial J}{\partial x} + \dot{y}\frac{\partial J}{\partial y} =: H \odot \dot{q} = v H \odot h
$$ Finally, we arrive at, $$
\dot{h} = \omega Rh - v\tilde{J}(H \odot h)h
$$ The rightmost-term can be simplified using Christoffel symbols, but that isn't critical as it is already something I can compute. This equation mirrors Kajelad's result: $$
\dot{T}^i = \omega R^i{}_j T^j-v\Gamma^i{}_{jk}T^jT^k
$$ So in summary, the generalized model is: \begin{gather}
\begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix} = vh \tag{4}\\[4pt]
\dot{h} = \omega Rh - v(J^\intercal J)^{-1}J^\intercal(H \odot h)h \tag{5}\\[4pt]
R = \sqrt{|J^\intercal J|}(J^\intercal J)^{-1}\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \tag{6}
\end{gather} where heading $h(t) \in \mathbb{R}^2$ , $J$ and $H$ are the embedded surface's Jacobian and Hessian respectively, and $v$ and $\omega$ are the specified ant/unicycle movements. This could be (more commonly) written in terms of the metric tensor ( $J^\intercal J$ ) and Christoffel symbols (gradients of the metric tensor). The appeal of that form is that it's completely independent of any embedding of our surface $r$ ; only the metric tensor field is needed. Note that the units are consistent ( $H$ has units of inverse-space). In the Euclidean case, the Hessian vanishes and the ODE reduces to the classic model. Lastly, if $\omega = 0$ , the $\dot{h}$ equation becomes a parallel-transport / geodesic equation. Now to test in simulation! ( Concluding remark: While very mechanical, the above derivation is a reflection of some rather fundamental and cool concepts in differential geometry, which Kajelad's answer used explicitly and may have gone beyond my background with. The key idea here, I think, is the Levi-Civita connection, or more specifically, the "" covariant-derivative ."" We took a covariant-derivative in the above when we took a regular derivative of $\dot{r}$ and then projected it onto the tangent space by left-multiplying $\tilde{J}$ . Instead, I could have used the abstract parlance and just defined $\nabla_{\dot{r}} \dot{r} = \ddot{r} - c\hat{\eta}$ . The ugliness in my derivation above is in how tied it is to coordinate representations of all the geometric quantities involved. To those unfamiliar with differential geometry, however, it is far more grounded, and can even serve as a gateway to the beautiful abstraction.)","['kinematics', 'geodesic', 'mathematical-modeling', 'differential-geometry']"
3948138,"Compute the volume of the solid bounded by $x=0, x=\frac{\pi}{2}, z=0,z=y, y=\cos(x).$","Compute the volume of the solid bounded by $x=0, x=\frac{\pi}{2}, z=0,z=y, y=\cos(x).$ I'm not sure if I'm mistaken here, but isn't this just $$\int_{0}^{\frac{\pi}{2}}\int_{0}^{y}\int_{0}^{\cos(x)} \ dy \ dz \ dx = \int_{0}^{\frac{\pi}{2}}\int_{0}^{y} \cos(x) \ dz \ dx = \int_{0}^{\frac{\pi}{2}}y\cos(x) \ dx = y$$ this doesn't seem right to me. Is there a problem regarding the limits of $y$ ? The picture of the $xy$ -plane looks like a $\cos(x)$ bounded between $0$ and $\frac{\pi}{2}$","['integration', 'multivariable-calculus']"
3948305,How will the singular values change after right multiplication with the diagonal matrix?,"Say that we have an SVD for a matrix $X = U \Sigma V^T$ , with the singular values $\sigma_i$ . What will happens  to the SVD and singular values if we right multiply with a diagonal matrix that simply scales the columns of $X$ ? Is it possible to write the SVD of $XD$ or get the singular values $\sigma_i$ in terms of the diagonal of $D$ and the SVD of $X$ ? And more specifically: would it be possible to get the SVD of the $XD$ in terms of the same $U$ we've obtained from the SVD of the X?","['matrices', 'linear-algebra', 'svd', 'singular-values']"
3948312,Uniform bound for law of large numbers,"Let $(X_i)_i$ be a sequence of iid real valued random variables with finite variance. Is it true that given a bounded measurable function $f:\mathbb R^2\to \mathbb R$ , then almost surely and uniformly in $k\in\{1,...,n\},$ $$\frac 1 n \sum_{i=1}^n f(X_i,X_{i+k}) \to \mathbb E(f(X_1,X_{2}))\,\,?$$ If I am not requiring the uniformity in $k$ , then this result should simply follow from the ergodic law of large numbers. How could I get this uniformity?","['stochastic-processes', 'law-of-large-numbers', 'ergodic-theory', 'probability-theory']"
3948367,Infimum of gradient's norm of a sequence of vanishing functions,"Let $\{f_k\}_{k \in \mathbb{N}} \subset C^1(\mathbb{R}^n; \mathbb{R})$ be a sequence of continuously differentiable functions s.t. $$\lim_{k \to + \infty} f_k(x) =0 \quad \forall \, x \in \mathbb{R}^n.$$ Is it true that $$\lim_{k \to + \infty} \inf_{x \in \mathbb{R}^n} |\nabla f_k(x)| =0$$ ?
Here $\nabla f_k(x)$ is the gradient of $f_k$ at the point $x$ and $|\cdot|$ is the Euclidean norm. It seems to me that the statement is true if $n=1$ , but what about $n\ge 2$ ?","['derivatives', 'pointwise-convergence']"
3948418,"How to find sin of any fraction-angles, and how do you find them in fraction forms and not in decimal forms?","Ok so on doing a whole lot of Geometry Problems, since I am weak at Trigonometry, I am now focused on $2$ main questions :- $1)$ How to calculate the $\sin,\cos,\tan$ of any angle? Some Information :- This site :- https://www.intmath.com/blog/mathematics/how-do-you-find-exact-values-for-the-sine-of-all-angles-6212 , produces a clear understanding and a detailed approach of finding the $\sin$ of any angle from $1$ to $90^\circ$ , and I found it very interesting. But now the Questions arise :- Can you find the $\sin$ , $\cos$ or $\tan$ of any fraction angles, like $39.67$ ? Can you find the $\sin$ , $\cos$ or $\tan$ of recurring fractions like $\frac{47}{9}$ ? Can you find the $\sin$ , $\cos$ or $\tan$ of irrationals, like $\sqrt{2}?$ Since I am a bit new to Trigonometry, I will be asking if there is a formula to find the $\sin$ of fractions, or even recurring fractions. I can use the calculator to find them obviously, but I have another Question :- $2)$ How to calculate the trigonometric ratios of every angle in fractional form? We all know $\sin 45^\circ = \frac{1}{\sqrt{2}}$ , but what will be $\sin 46^\circ$ in fractions? I can use a calculator to calculate the decimal of it, but it is hard to deduce the fraction out of the value, especially because the decimal will be irrational. I know how to convert recurring decimals to fractions, but this is not the case. Right now I am focused on a particular problem, which asks me to find the $\sin$ of a recurring fraction, in a fraction form. I am struggling to do this unless I clear up the ideas. Edit : My problem is to find the $\sin$ of $\frac{143}{3}^\circ$ . I do not have any specific formula to find this, and I am mainly stuck here. I need a formula which shows how this can be done. Can anyone help me? Thank You.","['trigonometry', 'proof-writing', 'geometry']"
3948435,Find the derivative using the definition of derivative (limit).,"Given $f(x)=\dfrac{5x+1}{2\sqrt{x}}$ . Find $\dfrac{df(x)}{dx}=f'(x)$ using
the definition of derivative. I have tried as below. \begin{align*}
f'(x)&=\lim\limits_{h\to 0} \dfrac{f(x+h)-f(x)}{h}\\
		&= \lim\limits_{h\to 0}
		\dfrac{\dfrac{5(x+h)+1}{2\sqrt{x+h}}-\dfrac{5x+1}{2\sqrt{x}}}{h}\\
		&= \lim\limits_{h\to 0}
		\dfrac{\dfrac{\left(5(x+h)+1\right)\sqrt{x}-(5x+1)\sqrt{x+h}}{2\sqrt{x+h}\sqrt{x}}}{h}\\
		&= \lim\limits_{h\to 0}
		\dfrac{\dfrac{5x\sqrt{x}+5h\sqrt{x}+\sqrt{x}-5x\sqrt{x+h}-\sqrt{x+h}}{2\sqrt{x+h}\sqrt{x}}}{h}\\
\end{align*} Now I can't find the limit. I confused how to simplify the limit. Anyone can give me  hint to solve it? Note: We were asked to find this derivative using $$f'(x)=\lim\limits_{h\to 0} \dfrac{f(x+h)-f(x)}{h}$$ instead of $$f(x)=\dfrac{u(x)}{v(x)}\iff f'(x)=\dfrac{u'(x)v(x)-u(x)v'(x)}{v(x)^2}.$$","['limits', 'derivatives']"
3948485,Conditions for a composite order to only have a single group,"I just learned the theorem that for every prime number $p,$ there is just one group of order $p$ down to isomorphism (the cyclic group $\mathbb{Z}/p\mathbb{Z}$ ). But interestingly, the converse of this statement is not true, as there is just a single group of order $15$ .  This lead me to wonder: Is there a nice partial converse? Consider a composite natural number $m \in \mathbb{N}$ . I want to find the conditions for there to be multiple groups of order $m$ . Since there is a cyclic group of every order, this is equivalent to finding a non-cyclic group of order $m.$ I have two conditions so far that imply multiple groups: $m$ is even ( $>2$ ) Then $m=2k$ for some $k>1$ . As the dihedral group $D_k$ has order $2k=m$ and is not cyclic, there are multiple groups of order m. $p^2 \mid  m$ for some prime $p$ Then let $m = p^2k$ for some $k\in \mathbb{N}$ . Then the group $(\mathbb{Z}/k\mathbb{Z}) \times (\mathbb{Z}/p^2\mathbb{Z})$ and the group $(\mathbb{Z}/k\mathbb{Z}) \times (\mathbb{Z}/p\mathbb{Z}) \times (\mathbb{Z}/p\mathbb{Z})$ both have order $p^2k = m$ and are not isomorphic. But these two conditions definitely don't cover all cases. Is there a nice condition for a composite number m such that there are multiple groups of order $m$ ?","['group-theory', 'abstract-algebra', 'finite-groups']"
3948492,Integration of function using Hausdorff measure of Cantor Set,"I am learning geometry on fractal shapes, along with how fractional calculus can relate to said geometry. At the moment I am trying to understand integration over a Hausdorff measure. According to Falconer's The Geometry of Fractal Sets, the middle-third Cantor set has a $\log_3 2$ -Hausdorff measure of $1$ . I would like to describe this relation as $\int_{C} 1 \; \mathrm{d}H^{s}(x)=1$ , where $C$ is the Cantor set, and $s = \log_3 2$ . My question is : How would the integral $$\int_{C} (e^x + x)\; \mathrm{d}H^{s}(x)$$ be evaluated? I'm not entirely sure where to begin in evaluating this, since this is new to me. I would appreciate resources as well, but I think an answer to this problem will give me the tools I need to evaluate other functions on other fractal sets. Thank you.","['measure-theory', 'fractal-analysis', 'hausdorff-measure', 'fractals']"
3948606,Exactness and obstruction in sheaf cohomology,"Given a topological space $X$ , sheaf cohomology 'measures' the lack of exactness of the global section functor $\Gamma(X, -) : \textbf{Sh}(X) \to \textbf{Ab}$ . From another viewpoint, sheaf cohomology should be measuring the 'obstruction to lifting local to global data'. I understand the notion of a sheaf as a local assignment to a topological space of algebraic structures that compatibly 'restrict' and 'glue'; and the global sections functor that maps $\mathcal{F} \mapsto \mathcal{F}(X)$ . Yet, I don't understand the connection between the exactness of $\Gamma(X, -)$ and the capability to 'lift local data to global'. How do these two viewpoints connect?","['sheaf-cohomology', 'homological-algebra', 'abstract-algebra', 'homology-cohomology', 'algebraic-topology']"
3948610,What's the preferred term researchers like to use in the theory of magmas/groupoids?,"As we know, mathematicians like to avoid the term ""groupoid"" to refer to a set with binary operation. This term, as we know, originates from the works of Brandt, so called Brandt groupoid. A Brandt groupoid is a groupoid in the sense of category theory, however for some (unknown to me) reason in history the term began to also be the name for a set with binary operation. For the historical reasons, and the fact that the term is somewhat ambiguous, I saw a lot of people prefer the term ""magma"". However, there is another side of the argument, and that is, what do people that actually work in the field call their objects? Is it magma, groupoid, or maybe something else entirely? In some fields, like quasigroup or semigroup theory, I saw a lot of people refer to those objects as groupoids. Is this possible that researchers actually prefer the term groupoid in their work? References: ""The Algebraic Theory of Semigroups"" A. H. Clifford, G. B. Preston ""Elements of Quasigroup Theory and Applications"" V. Shcherbacov ""Universal Algebra"" S. Burris, H. P. Sankappanavar","['magma', 'abstract-algebra', 'soft-question']"
3948623,Importance and Intuition of Polynomial Rings,"I know that a polynomial ring $R[x]$ is the ring with elements consisting of polynomials with coefficients in $R$ . However, this definition leaves me confused when I try to really understand the concept of polynomial rings, rather than just accept that definition for what it is. Where did the polynomials come from? Why are polynomial rings important? Is there a way to “construct” the polynomials in the context of Abstract Algebra, so that we may see why studying rings made up of polynomials is important? Are polynomial rings actually just sets of polynomials, or are the polynomials just a concrete way to represent some more abstract idea? What does the variable $x$ mean in polynomial rings? I apologize for all the questions, I’m just having trouble seeing the importance or intuition behind them.","['ring-theory', 'abstract-algebra']"
3948634,"Apparently in this form the limit is ""trivially"" 1/2","I'll admit, I'm struggling to keep up with the material in my analysis classes, but it only demoralizes me more when the questions in my textbooks have explained solutions, but I don't even understand how you go from one step to another. This is the final from, that's supposed to be close to the simplest form (so close that it's ""trivial"" for the student to find it): $\lim _{n\to \infty }\left(\frac{\left(c+1\right)n^c-n^{c+1}+\left(n-1\right)^{c+1}}{\left(c+1\right)\left(n^c-\left(n-1\right)^c\right)}\right)=\frac{1}{2}$ Why is this true? I don't see it at all.","['limits', 'real-analysis']"
3948685,Splitting a square into equal areas,"I am looking to split a square into 4 equal areas, not necessarily needing to be the same shapes or sizes . I need to find a way to use the least length of lines ( which could be straight or curved lines) inside the square to split and divide it up. What would be the optimal solution. I need to code an algorithm to do it too but I wanted to understand the mathematical concept behind it first. Also how would I use this for a rectangle say side height 1 and length 2.",['geometry']
3948732,Markov Chain problem with first passage time,"Let $X$ a Markov Chain with space state $S$ , and transition matrix $P$ . Let $A \subset S$ and $\tau_A = \inf\{n \ge 0: X_n \in A\}$ . Suppose that exists $n \ge 1$ and $\alpha > 0$ that for all $x \in A^c$ (the complement of $A$ ), $p^n_{x,A} \ge \alpha$ . i) Show that for all $k \in \mathbb{N}$ and $x \in S$ , $\mathbb{P}_x(\tau_A > kn) \le (1 - \alpha)^k$ . ii) Show that $\mathbb{E}_x[\tau_A] \le \frac{n}{\alpha}$ , and in particular, $\mathbb{P}_x(\tau_A < \infty) = 1$ . This is how I tried to solve it: If $x \in A$ , we have $$\mathbb P _x(\tau_A = 0) = 1 \implies \mathbb P _x(\tau_A > kn) = 0 \le (1-\alpha)^k.$$ Let $x \in A^c$ . If $\lambda = (\lambda_i : i \in S)$ is a probability distribution of $X$ , so $\lambda_x = \mathbb P_x(X_0 = x)$ . Moreover, it's known that By definition of $\lambda$ , we have $\lambda_x \le 1$ ; Since $X$ is Markov Chain, we have (from Chapman-Kolmogorov) $$p^n_{x,A^c} = \sum_{i_1, \dots, i_{n-1} \in S}{p_{x,i_1} \cdots p_{i_{n-1},A^c} }$$ Since $A^c \subset S$ , $$p_{x,A^c}\underbrace{p_{A^c,A^c} \cdots p_{A^c,A^c} }_{n-1 \text{ times}} \le \sum_{i_1, \dots, i_{n-1} \in S}{p_{x,i_1} \cdots p_{i_{n-1},A^c} } = p^n_{x,A^c}$$ Similarly, $$\underbrace{p_{A^c,A^c} \cdots p_{A^c,A^c} }_{n \text{ times}} \le \sum_{i_1, \dots, i_{n-1} \in S}{p_{A^c,i_1} \cdots p_{i_{n-1},A^c} } = p^n_{A^c,A^c} $$ It's easy to see, that $$p^n_{A^c,A^c} = 1 - p^n_{A^c,A^c}$$ For $x \in A^c$ , $$p^n_{A^c,A} \ge p^n_{x,A}$$ And finally, $$p^n_{x,A^c} = 1 - p^n_{x,A} \le 1 - \alpha$$ Using some properties about Markov Chains and all these information, it follows that $$\begin{aligned}\mathbb P_x(\tau_A > kn) &= \mathbb P (X_0 \not \in A,X_1 \not \in A, \dots, X_n \not \in A, X_{n+1} \not \in A, \dots, X_{2n} \not \in A,\dots, X_{kn - 1} \not \in A,X_{kn} \not \in A\mid X_0 = x)\\
&=\mathbb P(X_0 = x, X_1 \not \in A, \dots, X_n \not \in A, \dots, X_{2n} \not \in A,\dots, X_{kn} \not \in A )\\
&= \lambda_x p_{x,A^c}\underbrace{p_{A^c,A^c}\cdots p_{A^c,A^c}}_{n-1 \text{ times}}\underbrace{\underbrace{p_{A^c,A^c}\cdots p_{A^c,A^c}}_{n \text{ times}}\cdots \underbrace{p_{A^c,A^c}\cdots p_{A^c,A^c}}_{n \text{ times}}}_{k - 1 \text{ times}}\\
&\le p_{x,A^c}\underbrace{p_{A^c,A^c}\cdots p_{A^c,A^c}}_{n-1 \text{ times}}\underbrace{\underbrace{p_{A^c,A^c}\cdots p_{A^c,A^c}}_{n \text{ times}}\cdots \underbrace{p_{A^c,A^c}\cdots p_{A^c,A^c}}_{n \text{ times}}}_{k - 1 \text{ times}} \text{ (from 1)}\\
&\le \left(\sum_{i_1,\dots, i_{n-1} \in S}{p_{x,i_1}p_{i_1,i_2}\cdots p_{i_{n-1},A^c}}\right) \cdot \left(\sum_{i_1,\dots, i_{n-1} \in S}{p_{A^c,i_1}p_{i_1,i_2}\cdots p_{i_{n-1},A^c}}\right)^{k-1} \text{ (from 3 and 4)}\\
&= \left(p^n_{x,A^c}\right)\left(p^n_{A^c,A^c}\right)^{k-1} \text{ (from 2 and 4)}\\
&\le \left(p^n_{x,A^c}\right)\left(1 - p^n_{x,A}\right)^{k-1} \text{ (from 5 and 6)}\\
&\le (1-\alpha)^k \text{ ( from 7)}\end{aligned}$$ This is how I tried to solve item i). Note that,for $i \ge 1$ $$\{\tau_A > i + 1\} \subseteq \{\tau_A > i\}$$ It means that $$\mathbb P_x(\tau_A = \infty) = \mathbb P_x(\lim_{k \to \infty}{\tau_A > kn}) = \lim_{k \to \infty} {\mathbb P_x({\tau_A > kn})} \le \lim_{k \to \infty}{(1-\alpha)^k} = 0$$ It implies that, $$\mathbb P_x(\tau_A < \infty) = 1 - \mathbb P_x(\tau_A = \infty) = 1$$ Finally, $$
\begin{aligned}
\mathbb{E}_x[\tau_A] &= \frac{n}{n}\mathbb E_x[\tau_A]\\
&= n\mathbb E_x\left[\frac{\tau_A}{n}\right]\\
&= n\left(\sum_{i \ge 1}{\mathbb P_x\left(\frac{\tau_A}{n} > i\right)}\right)\\
&= n\left(\sum_{i \ge 1}{\mathbb P_x(\tau_A > ni)}\right)\\
&\le n\left(\sum_{i \ge 1}{\mathbb (1-\alpha)^i}\right)\\
&=n \cdot \frac{1}{1 - (1-\alpha)} = \frac{n}{\alpha}
\end{aligned}
$$ Please, help me to find any mistakes. Thanks for the help!","['markov-chains', 'probability-theory', 'probability']"
3948765,Asymptotic expansion of the inverse of $x\mapsto x+x^{\small\sqrt2}+x^2$ near zero,"This is a follow-up to my previous question ""Asymptotic expansion of the inverse of $x\mapsto x+x^\phi$ near zero"" . Consider a continuous real-valued monotone increasing function $f:\mathbb R^+\to\mathbb R^+$ satisfying $f\big(x+x^{\small\sqrt2}+x^2\big)=x.$ I am interested in an asymptotic expansion of $f(z)$ for $z\to0^+$ in terms of powers of $z$ . I was able to find a few initial terms by manually balancing coefficients: $$f(z)=z-z^{\small\sqrt2}+\sqrt2\;z^{\small\unicode{x202f}2\unicode{x202f}\sqrt2-1}-z^2+\mathcal O\big(z^{\small\unicode{x202f}3\unicode{x202f}\sqrt2-2}\big), \quad z\to0^+.\tag{$\diamond$}$$ Computing next terms in an ad hoc fashion quickly becomes tedious, so I am looking for a more systematic approach that would allow to obtain a general formula for the terms of this series. I expect it to be a mix of integer powers of $z$ and irrational powers involving $\sqrt2$ . Also, I would like to know the radius of convergence of that series. More generally, I am looking for a uniform approach for inverting generalized polynomials of a single variable that may contain both rational and irrational powers.","['inverse-function', 'sequences-and-series', 'asymptotics', 'real-analysis']"
3948784,Why is the cone measurable?,"Problem Denote by $\lambda_n$ the Lebesgue measure on $\mathbb {R}^n$ .
Let $h \in \mathbb {R}_{>0}$ . Let $A \subset \mathbb {R}^{n-1}$ be Lebesgue-measurable with finite measure.
Define the cone $C(A,h)$ as the union of all straight lines connecting a point in $A \times\{0\}$ with the point $(0, h) \in \mathbb {R}^{n-1} \times \mathbb{R}$ . Prove that $\lambda_n (C(A,h))=\frac{h}{n}\cdot \lambda_{n-1}(A)$ . Where I struggle Assuming that $C(A,h)$ is (Lebesgue-)measurable, this is a simple application of Fubini. How Fubini is used is all clear to me. However, why is $C(A,h)$ measurable?
It seems one could roughly argue as follows: As $A$ and $[0,h]$ are measurable so is $A \times [0,h]$ . As $A \times [0,h]$ is measurable it is the union of a null set and a Borel set. $C(A,h)$ is the image of $A \times [0,h]$ under a certain diffeomorphism (c.f. Deducing a formula for the volume of a cone over a $J$-measurable set. ). Diffeomorphisms map null sets to null sets, and Borel sets to Borel sets. Hence, $C(A,h)$ is (as a finite union of measurable sets) measurable. Unfortunately, I am not allowed to use the above properties of diffeomorphisms. What are other ways to prove the claim? Please only give a hint to set me thinking.","['lebesgue-measure', 'lebesgue-integral', 'geometric-measure-theory', 'analysis', 'fubini-tonelli-theorems']"
3948877,Bernoulli trials: Probability of $\lim\sup A_n$ depending on $p$,"We have an infinite sequence of Bernoulli's trials with $p$ being the probability of success. Let $A_n$ the next event: in trials under numbers from $2^n$ to $2^{n+1}-1$ there were at least $n$ successes in row. I have to find the probability of the $\lim\sup A_n$ depending on $p$ . I already estimated that $$
P(A_n)<(2^n)p^n,
$$ thus for $p<\frac{1}{2}$ we have $$ \sum_{n=1}^\infty P(A_n)<\infty$$ which means (Borel–Cantelli lemma) $$ P(\lim\sup A_n)=0.$$ I found that the answer for $p\ge\frac{1}{2}$ is that we should have $$\sum_{n=1}^\infty P(A_n)=\infty$$ because of $$P(A_n)\ge1-e^{\frac{(2p)^n}{2n}}.$$ Could someone explain why is it like that? I honestly have no idea where did e come from.",['probability-theory']
3948905,Sub-Gaussian norm is proper norm,"I am trying to show that the sub-Gaussian norm is a proper norm: $$\|X\|_{\psi_2}=\text{inf}\bigg\{t>0:\mathbb{E}\bigg[\text{exp}\bigg(\frac{X^2}{t^2}\bigg)\bigg]\bigg\}$$ I was able to show the triangle inequality, and just want to make sure my logic on showing the homogeneity property is correct: $$\|cX\|_{\psi_2}=\text{inf}\bigg\{t>0:\mathbb{E}\bigg[\text{exp}\bigg(\frac{c^2X^2}{t^2}\bigg)\bigg]\bigg\}$$ Def $k=\frac{t}{|c|}$ then $$\|cX\|_{\psi_2}=\text{inf}\bigg\{|c|k>0:\mathbb{E}\bigg[\text{exp}\bigg(\frac{X^2}{k^2}\bigg)\bigg]\bigg\}=|c|\text{inf}\bigg\{k>0:\mathbb{E}\bigg[\text{exp}\bigg(\frac{X^2}{k^2}\bigg)\bigg]\bigg\}=|c|\|X\|_{\psi_2}$$ Is this  correct?","['probability-theory', 'probability']"
3948943,"Showing the set $\{x: x = 2^{-k} \ k\in \mathbb{N}\ \text{or}\ x = 0 \}$ is open, closed, or neither - solution feedback","This question comes from Shifrin's  Multivariable Mathermatics - Sec 2.2 - 1(b) , it asks:
Show that the set $B = \{x: x = 2^{-k} \ k\in \mathbb{N}\ \text{or}\ x = 0 \}$ is open, closed, or neither and prove your answer. I've come to the conclusion that the set is closed and I wanted some assistance on tightening up my solution. Solution The tools I have up to this point to show that a set is closed are: i) show all convergent sequences in the set converge to a point in the set. ii) Show that the complement of the set is open. I chose to approach this problem using (ii), but I do have a question about how I could use (i) I'll ask at the end. So first I have to figure out what it means to be in $C = \mathbb{R} - B$ . To me this meant $\mathbb{R} - B = \{x: x \neq 2^{-k}\ \text{and} x \neq 0\}$ . To build off of this idea I drew a picture of the number line with the points belonging to $B$ and observed how the points of $C$ would have to be to satisfy my argument. So after some fiddling around I defined the points of $C$ as follows: $$C = \mathbb{R} - B = \{y: y = x\pm \delta\ \text{depending on which side of an $x$ from $B$ that the point $y$ is in relation to}\}$$ . So how do I choose my $\delta$ ? So keep in mind that each $x \in B$ is actually a function of a particular $k$ , so with this in mind I defined $\delta = \min(|2^{-k}-y|, |2^{-(k+1)}-y|)$ . In this set up it means I'm taking a $\delta$ that will take into consideration the possibility of having an open ball over a value $x \in B$ which we don't want. So with this setup I now need to show $C$ is open. Define a point $z \in C$ as $z = x \pm \frac{\delta}{2}$ . By this definition we see that $z \in B(y, \delta)$ . It's here where I'm having trouble completing the proof. So to show the ball is open is dependent on where my $y$ value is, so for a concrete argument let's suppose that $y < 2^{-k}$ . To show $B(y, \delta) \subset C$ means I have to show for all $z \in B(y, \delta)$ that: $$|z - y| < |2^{-k} - y| \\
\Rightarrow\ |z - y| < 2^{-k} - y\ \text{(since assuming $y < 2^{-k}$ case)}$$ I'm having trouble arriving at the conclusion...What I envision happening after some algebra is: $$2^{-(k+1)} < z < 2^{-k}$$ Which would then give me my open ball. So I have two questions: My idea seems to be leading me in the right direction, but needs some fine-tuning. What do I need to do to tighten it up? With regards to the other way of showing closed. In this set it seems like the only sequence is the set itself since the $x = 2^{-k}$ and that will converge to $0$ , but can I actually say that this is true? Couldn't there be some rare sequences that I have not found that are in the set, but since I have not explicitly stated all convergent sequences then I can't use the definition of closed explicitly?","['real-analysis', 'multivariable-calculus', 'calculus', 'sequences-and-series', 'elementary-set-theory']"
3948978,Can $n$ convex solids in $\mathbb{R}^3$ be mutually touching?,"Say that a collection of $n$ convex solids in $\mathbb{R}^3$ are mutually touching if, for every pair of them, they share some positive-measure part of their boundaries. (That is, we're not permitting the solids to touch only via an edge or a corner.) As an example, if we ""thicken up"" an $n=4$ configuration in 2 dimensions (the maximum, since $K_5$ is nonplanar), then placing a large cube on top of the thickened configuration yields $n=5$ mutually touching convex regions. (By thickening it into a sort of wedge shape, we can put convex solids on both the top and bottom, for $n=6$ .) Do such configurations exist for arbitrarily high $n$ ? For infinitely many solids at once? What if we require the solids be congruent to each other? (Note that if we relax the convexity requirement, the answer to all of these is pretty easily seen to be yes.) I vaguely recall reading a paper which provided an affirmative answer to the arbitrarily high $n$ case with congruent solids; I think it used the Voronoi cells given by equally spaced points on a helix, though it's been several years since I read it. Any pointers to this paper, or an equivalent result, would be appreciated (if I'm not just imagining it)!","['convex-geometry', 'geometry', 'reference-request']"
3948988,Can a Measure be Literally Continuous?,"Say that $(X,M)$ is a measurable space (i.e., $0,X \in M$ and $A \in M \implies X \setminus A \in M$ and, finally, $M$ is closed w.r.t. countable unions/intersections). Suppose that $\mu$ is either a signed or complex measure on $(X,M)$ . In particular, $\mu$ is a function on $M$ . I know that if $\mu$ maps into $[0,\infty)$ , then $d(A,B) = \mu(A \Delta B)$ is a metric modulo the equivalence relation $A \sim B \iff \mu(A\Delta B) = 0$ . This was mentioned in baby Rudin, for instance. More generally, however, if we are given a topology $T$ on $X$ does there consequently exist a canonical topology on $M$ induced by $T$ such that $\mu$ is continuous? Of course, I would prefer if the topology on $M$ were not just the discrete topology. Further, it would be nice if it were metrizable. If the answer is affirmative, a follow up question might be, for instance, ""how coarse can the topology be while still making $\mu$ continuous?""
Has anyone thought about this or seen this before? Thanks.","['general-topology', 'measure-theory']"
3949050,Help proving $\lim_{h \to 0} \int_{1}^{5} f(x+h)dx = \int_{1}^{5}f(x)dx$,"Suppose a function $f$ is integrable on $[0,6]$ . Prove that $$\lim_{h \to 0} \int_{1}^{5} f(x+h)dx = \int_{1}^{5}f(x)dx$$ This seems like the easiest thing, I would normally just bring the limit inside the integral and evaluate $f(x+h)$ to $f(x)$ , but its asking to prove that this is the case and I haven't even used the fact that its integrable on $[0,6]$ . I tried the epsilon delta proof this way. Let $\epsilon > 0$ and assume $$\left|\int_{1}^{5}f(x+h)dx - \int_{1}^{5}f(x)dx\right| < \epsilon$$ I tried to find a $\delta$ so that $|f(x+h)-f(x)| < \delta$ , but had no luck. Any help would be appreciated.","['integration', 'calculus', 'functions']"
3949104,What are the eigenvalues of $(a_m+a_n)^2$?,"Let $a_m\in \mathbb{R}$ and consider the matrix defined by $A_{mn} = (a_m+a_n)^2$ . Using python and inputting a random vector $a = (a_n:a_n\in \mathbb{R})$ of various length and various values, the eigenvalues of $A$ always seem to consist of 1 negative value and 2 positive values. All the other eigenvalues are $0$ . Based on this, it seems that there should be a formula which determines the 3 nontrivial eigenvalues of $A$ (and possibly all the eigenvectors of $A$ ), but I can't think of a rigorous proof. EDIT : If $a_n$ had some ""symmetry"" so that $\sum a_n =\sum a_n^3=0$ , then it's easy to see that $a_n$ is an eigenvector and that $a_n^2 \pm c$ are eigenvectors where $$
c^2 = \frac{1}{N}\sum a_n^4
$$ However, the statement seems to be more general. Also, I'm not sure why all the other eigenvalues are 0.","['diagonalization', 'linear-algebra']"
3949107,What is inverse of a matrix whose diagonal elements are all zero.,"I am a science researcher and I got a problem to find the generic inverse of the following matrix: $$ 
A_n = \left(\begin{array}{ccc}
        0 & a_2 & a_3 & ... & a_{n-1} & a_n \\
      a_1 &   0 & a_3 & ... & a_{n-1} & a_n \\
      a_1 & a_2 &   0 & ... & a_{n-1} & a_n \\
      ... & ... & ... & ... & ...     & ... \\
      ... & ... & ... & ... & ...     & ... \\
      a_1 & a_2 & a_3 & ... &       0 & a_n \\
      a_1 & a_2 & a_3 & ... & a_{n-1} &   0 \\
      \end{array}\right) 
$$ I figured out that for n=2,3 $$
A_2^{-1} = \left(\begin{array}{ccc}
        0 & a_2^{-1}  \\
      a_1^{-1} &   0  \\
      \end{array}\right) 
$$ $$
A_3^{-1} = \frac{1}{2} \left(\begin{array}{ccc}
        -a_1^{-1} &  a_1^{-1} &  a_1^{-1} \\
         a_2^{-1} & -a_2^{-1} &  a_2^{-1} \\
         a_3^{-1} &  a_3^{-1} & -a_3^{-1} \\
      \end{array}\right) 
$$ but can we extend it to a general case?
Can anyone help?? Thanks! [later] It looks like it is $$
m_{ij} =  -\frac{n-2}{n-1} a_{i}^{-1}\ (i=j)  
$$ $$
       = \frac{1}{n-1} a_{i}^{-1}(else)
$$","['matrices', 'matrix-calculus']"
3949133,Cauchy-Schwarz-like inequality for the wedge product,"Summary of my question: Does $\| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\|$ hold for all $\mathbf a \in \Lambda^k(\mathbb R^n)$ and $\mathbf b \in \Lambda^\ell(\mathbb R^n)$ ? Some background: Given $\mathbb R^n$ with the standard inner product, we can define an inner product on the exterior powers $\Lambda^k(\mathbb R^n)$ by $\langle u_1 \wedge \cdots \wedge u_k, v_1 \wedge \cdots \wedge v_k\rangle = \det(\langle u_i, v_j\rangle)_{i,j=1}^k$ and then extending by linearity. As usual, for $\mathbf a \in \Lambda^k(\mathbb R^n)$ , let $\|\mathbf a\| = \sqrt{\langle \mathbf a, \mathbf a \rangle}$ . If $\mathbf a = u_1 \wedge \cdots \wedge u_k$ (i.e., if $\mathbf a$ is simple), then $\|\mathbf a \|$ is equal to the volume of the $k$ -dimensional parallelotope generated by $u_1, \ldots, u_k$ . When $\mathbf a$ and $\mathbf b$ are simple, then we can see that $\| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\|$ holds by using the geometric interpretation given above. When $\mathbf a = u_1 \wedge \cdots \wedge u_k + v_1 \wedge \cdots \wedge v_k$ and $\mathbf b$ is simple, then after some lengthy computations (expanding the inner product and Gram-Schmidt), I was able to show the inequality holds in this case as well. Is the inequality $\| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\|$ true in general? And is there a geometric interpretation of these quantities when $\mathbf a$ and $\mathbf b$ are not simple?","['cauchy-schwarz-inequality', 'multilinear-algebra', 'linear-algebra', 'exterior-algebra']"
3949158,Can such a matrix be singular?,"Let $A$ be an $n\times n$ matrix that satisfies i. All diagonal entries of $A$ are positive, even integers, ii. All non-diagonal entries of $A$ are positive, odd integers, iii. $A$ is symmetric: $A_{ij}=A_{ji}$ . iv. If $i\neq j$ , then $A_{ij}<A_{ii}$ and $A_{ij}<A_{jj}$ . Question: can such $A$ be singular (i.e. $|A|=0$ )? I can prove the answer is NO, when $n$ is even, and without using condition iv: reduce $A$ mod $2$ we get a matrix with zero diagonal and all other entries $1$ ; then we can compute directly that the determinant of such matrix is odd, in particular not zero. When $n$ is odd this argument says $|A|$ is even, I don't know if it can be zero, and it seems condition iv will be important here. I encountered it when I was trying to solve a question in this post .","['number-theory', 'linear-algebra', 'combinatorics']"
3949183,Average of a sequence of r.v.s converge to 0 a.s.,"We have $\left\{Y_{n}\right\}$ to be a sequence of independent random variables. $X_{n}$ is defined by $\mathbb{P}\left(Y_{n}=1\right)=\mathbb{P}\left(Y_{n}=-1\right)=\frac{1}{2}\left(1-2^{-n}\right), \mathbb{P}\left(Y_{n}=2^{n}\right)=\mathbb{P}\left(Y_{n}=-2^{n}\right)=2^{-(n+1)}$ $n=1,2, \ldots$ How can we prove: $$
\frac{1}{n}\left(Y_{1}+\cdots+Y_{n}\right) \stackrel{a . s .}{\rightarrow} 0
$$ I can show $Y_n/n$ converges to 0 a.s., but I am not sure does it imply the sum is also convergent to 0 a.s.","['probability-theory', 'real-analysis']"
3949185,Limiting behavior of $x + \sqrt{x} - \sqrt{x + \sqrt{x}}$,"As $x$ and $y$ vary through the real numbers, how do the interval families $\left[ x, x + \sqrt{x} \right]$ and $\left[ y - \sqrt{y}, y \right]$ differ? Not by much, it turns out: consider the two maps: $f : x \mapsto x + \sqrt{x}$ , $g : y \mapsto y - \sqrt y$ . The composition $g \circ f$ rapidly converges to the the identity minus a constant: $$\lim_{x \rightarrow \infty} x - (g \circ f)(x) = \lim_{x \rightarrow \infty} x - \left( (x + \sqrt{x}) - \sqrt{x + \sqrt x)} \right) = 0.5.$$ Yet this fact is false for any exponent greater than 0.5. Indeed, for any fixed $\varepsilon > 0$ : $$\lim_{x \rightarrow \infty} = x - \left( (x + x^{0.5 + \varepsilon}) - (x + x^{0.5 + \varepsilon})^{0.5 + \varepsilon} \right) = \infty.$$ How can I understand this sudden change in behavior??? EDIT : another unusual observation. what if we add ceiling functions to both, so that $\hat{f} : \lceil x \mapsto x + \sqrt{x} \rceil $ , $\lceil g : y \mapsto y - \sqrt y \rceil$ . Then $g \circ f$ appears to oscillate between $-1$ and 0, though possibly converges to 0. I'd like to prove or disprove: does it always take the values either $-1$ or $0$ ? Is it ""eventually"" 0? Thanks.","['limits', 'radicals']"
3949335,Continuous functions and open sets/covers,"I'm taking a course in elementary algebraic geometry, but I seem to be lacking a topological background. The following result is often used: Let $X,Y$ be topological Spaces, and $U_i$ for $i \in I$ an open cover for $X$ (the convention here is that $\bigcup_{i \in I} U_i = X$ ). Then, a function $f: X \to Y$ is continuous if and only if $f|_{U_i}: U_i \to Y$ is continuous for $\forall i$ where $U_i$ is endowed with the subspace topology. How does one prove this? I've not seen this lemma before. (Apparently it's called 'local property of continuity'?)",['analysis']
3949339,Find the Number of Group Homomorphisms from $\mathbb{Z}/10\mathbb{Z}$ to $A_4$.,"Is this reasoning correct? $\mathbb{Z}/10\mathbb{Z}$ is isomorphic to $\mathbb{Z}_{10}$ which is generated by $1$ , hence we may look at the number of homomorphisms between $\mathbb{Z}_{10}$ and $A_4$ for the answer to the original question. A homomorphism $\phi$ must map the element $1$ of $\mathbb{Z}_{10}$ to an element of $A_4$ such that its order divides both $10$ (by properties of homomorphisms) and $12$ (by Lagrange's Theorem). Thus, the only possible candidates for the mapping of $1$ must have order $1$ or $2$ . This in turn yields $4$ possible elements in $A_4$ , namely, $()$ , $(12)(34)$ , $(13)(24)$ , and $(14)(23)$ . All of these mappings preserve the group operation, thus the answer to the original question is $4$ .","['group-homomorphism', 'group-theory']"
3949430,"Finding ordered pair of polynomial $(f(x),g(x))$ with coefficient $\mathbb{Z}_{p}$ for a prime number $p$","In process of solving a problem, I had to find some ordered pair of polynomial $(f(x),g(x))$ with coefficient $\mathbb{Z}_{p}$ , $p$ is prime number. (i.e $f(x),g(x) \in \mathbb{Z}_{p}[x])$ such that $\frac{f(x+r)}{g(x+r)}=\frac{f(x)}{g(x)}$ for all $r \in \mathbb{Z}_{p}$ . I find out that $\mathbb{Z}_{p} \simeq \{(r,1) \mid r\in \mathbb{Z}_{p} \}$ is one of them, but can't find out another.
i thought about $f(x^{p}), g(x^{p})$ because $(x+r)^p=x^{p}+r^p$ but $r^p$ doesn't disappear when $1 \leq r <p $ by Fermat's theorem.
is there something else? and if there's no more, how can i prove it? P.s.: $x$ is transcendental over $\mathbb{Z}_{p}$ , so all the members of $\left\{\frac{f(x)}{g(x)} \ \middle\vert\ f(x),g(x) \in \mathbb{Z_p} \right\}$ is simple extension of $x$ over $\mathbb{Z_p}$ .","['finite-fields', 'functions', 'abstract-algebra', 'polynomials', 'extension-field']"
3949514,Second derivative of the Christoffel symbols in normal coordinates,"According to wikipedia the Taylor expansion of the Christoffel symbols of a Riemannian manifold $(M,g)$ in normal coordinates is given by $$
{\Gamma^{\lambda}}_{\mu\nu}(x)= -\frac 13 (R_{\lambda\nu\mu\tau}(0)+R_{\lambda\mu\nu\tau}(0))x^\tau+ O(|x|^2).
$$ Is there any reference for the calculation of the $O(|x|^2)$ term?
I did the computations as Yuval proposed and ended up with $$
6\dfrac{\partial }{\partial x_v} \dfrac{\partial }{\partial x_w}{\Gamma^{n}}_{ij}(x) 
\\= \frac 12 g^{nl}
\left( \nabla_i R_{jvwl} + \nabla_i R_{jwvl} +\nabla_w R_{jivl} + \nabla_v R_{jiwl} + \nabla_w R_{jvil} + \nabla_v R_{jwil} \right) +
\left( \nabla_j R_{ivwl} + \nabla_j R_{iwvl} +\nabla_w R_{ijvl} + \nabla_v R_{ijwl} + \nabla_w R_{ivjl} + \nabla_v R_{iwjl} \right) -
\left( \nabla_l R_{ivwj} + \nabla_l R_{iwvj} +\nabla_w R_{ilvj} + \nabla_v R_{ilwj} + \nabla_w R_{ivlj} + \nabla_v R_{iwlj} \right)
$$ If I apply the symmetries of the curvature tensor $$
\left( \nabla_i R_{jvwl} + \nabla_i R_{jwvl}  + 2\nabla_w R_{jvil} + 2\nabla_v R_{jwil} \right) +
\left( \nabla_j R_{ivwl} + \nabla_j R_{iwvl}   + 2\nabla_w R_{ivjl} + 2\nabla_v R_{iwjl} \right) -
\left( \nabla_l R_{ivwj} + \nabla_l R_{iwvj}  \right)
$$ I feel like the derivatives involving derivatives in $i,j,l$ should somehow cancel out with the Bianchi identities but I don't get it to work.","['riemannian-geometry', 'differential-geometry']"
