question_id,title,body,tags
2332760,Every Topological Vector Space is Regular,"Let $V$ be an abstract ( $T_0$ ) topological vector space over topological field $K$ (We may assume that $K = \mathbb{C}$ or $K = \mathbb{R}$ for simplicity). This means that the only thing that we are allowed to use is that scalar multiplication and  addition are jointly continuous in both arguments (and also the $T_0$ properties).
The problem is to prove: For each point $x$ and closed set $Y$ such that $x \not \in Y$ there are exist disjoint open neighborhoods $O$ and $O'$ such that $x \in O$ and $Y \subset O'$ . I will write in the sequel for such sets $O \in \mathcal{U}_V(x),O' \in \mathcal{U}_V(Y)$ . If $U \in \mathcal{U}_V(y)$ then we can squeeze it by factor of $r \in K$ $$
 r \ast_y U = \big(rU + \{(1 - r)y\}\big) \cap U
$$ which is open neighborhood of $y$ . Proof may start with : for all $y \in Y$ there must exist $U_y \in \mathcal{U}_V(y)$ such that $x \not \in U_y$ , otherwise $x \in \lim_{n \to \infty} y $ and so $x \in Y$ which is not true. Then, $U = \bigcup_{y \in Y} U_y \in \mathcal{U}_V(Y)$ do not contain $x$ . In case $x \not \in \delta U$ just let $O' = U$ . Otherwise,  squeeze all open sets by some values $r_y$ : $$
O' =  \bigcup_{y \in Y} r_y \ast_y U_y
$$ Then $O = \Big(\overline{O'}\Big)^\complement$ . How to select $r_y$ ? Will this approach work?","['functional-analysis', 'general-topology', 'topological-vector-spaces', 'separation-axioms']"
2332765,Exterior Algebra Graded Ring Structure?,"In Hatcher's Example 3.13 he defines the Exterior Algebra $\Lambda_R[\alpha_1,...,\alpha_n]$ over a commutative ring $R$ with identity to be the free $R$-module with basis the finite products $\alpha_{i_1}\cdots\alpha_{i_k}$ with $i_1 < \cdots <i_k$ with associative, distributive multiplication defined by the rules $\alpha_i \alpha_j = -\alpha_j \alpha_i$ and $\alpha_i^2=0$, and the empty product serving as the multiplicative identity. He then simply says that the exterior algebra becomes a commutative graded ring by specifying odd dimensions for the generators $\alpha_i$. I understand the $\alpha_i$ need to have odd dimensions in order to satisfy the $\alpha_i \alpha_j = -\alpha_j \alpha_i$, but how exactly does the exterior algebra decompose as a direct sum, which is part of the definition of a graded algebra?","['abstract-algebra', 'graded-rings', 'exterior-algebra', 'multilinear-algebra']"
2332766,Which subject are these students best? (basic high school statistical question),"It's a very basic high school statistical question, but I'm struggling to solve it. Suppose I have a school with $287$ students and each one made a test with $50$ questions (multiple choice questions with $5$ items each, they have to choose one item in each question). These questions are divided into the following subjects: Subject A - $8$ questions Subject B - $6$ questions Subject C - $10$ questions Subject D - $6$ questions Subject E - $6$ questions Subject F - $14$ questions We say a student fail the test when he doesn't solve any question. Then we have the following result: Subject A - $3$ students failed Subject B - $16$ students failed Subject C - $1$ students failed Subject D - $1$ students failed Subject E - $8$ students failed Subject F - $0$ students failed So how can I compare the performance of the students? In another words, which subject were they best and which one were they worse? Remark: Each student has only two options: successful or failure, so in this case the overall score in each subject is not important.",['statistics']
2332775,"Closed form for $\int_0^1 \int_0^1 {(1-xy)}^n \, dx \, dy$","I was trying to calculate 
$$\sum_{k=0}^n (-1)^k \binom{n}{k} \frac{1}{(n+1-k)^2}$$
and I found that it suffices to calculate the 
$$I_n=\int_0^1 \int_0^1 {(1-xy)}^n \,dx\,dy.$$
Is there any way to find a closed form for this? I tried to use the ordinary and the exponential generating functions of $\{I_n\}_{n\in \mathbb{N}}$, but I couldn't proceed. I also tried the change of variables 
$$x=\frac{u+v}{2},\ \ \ y=\frac{u-v}{2},$$
but it got me nowhere. Any hints or ideas? Thanks in advance for your help.","['integration', 'definite-integrals']"
2332779,"Prove that $f : \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z}$ defined as $f(m, n) = 2n - 4m$ is not injective","I know that if this function is injective, then for $(a, b)$ and $(c, d)$ in the set $\mathbb{Z}\times\mathbb{Z}$, then $f(a, b) = f(c, d)$ implies that $a = c \wedge b = d$: $f(a, b) = f(c, d)$ $2b - 4a = 2d - 4c$ $b - 2a = d - 2c$ And now I'm stuck as to how I should proceed, as there doesn't seem to be any obvious way to prove from this that $a$ must equal $c$ and that $b$ must equal $d$. I tried taking the contrapositive--that is, that if $a \ne c \vee b \ne d$, then $f(a, b) \ne f(c, d)$, but that's even worse.","['proof-writing', 'functions']"
2332885,"How do I solve $\int_{C}\frac{-y\,dx+(x-1)\,dy}{(x-1)^2+y^2} $?","If $$\gamma=(2\cos(t)+4\cos^2(t) , 2\sin(t)+4\sin(t)\cos(t))$$ and $C$ is a path defined by $\gamma$ when $\frac{-2\pi}{3} \le t\le \frac{4\pi}{3}$. calculate: $$\int_{C}\frac{-y\,dx+(x-1)\,dy}{(x-1)^2+y^2} $$ first of all, I was given a hint that $\gamma$ can be changed to polar coordinates, but it seems to me already in these coordinates. am I missing something? second, for calculating the integral I thought to use that: $$\int_{C}f\,dr=\int_{0}^1f(\gamma(t)\cdot\gamma'(t)\,dt$$ but with current parametrization it is too messy. any ideas how to approach this question? edit: I draw the function using matlab: https://i.sstatic.net/WkzFI.png","['multivariable-calculus', 'integration', 'line-integrals']"
2332937,Area of surface using polar coordinates,"Calculate the area of the surface $z=x+y$ that is inside the cylinder $x^2+y^4 = 4$. I was able to find the correct answer by calculating the normal vector (using cross product) at each point on the surface parametrized:
$$
\vec{n} = (-r)\vec{i}+(-r)\vec{j}+(r)\vec{k}
$$
And then I used polar coordinates to integrate the domain of the parametrized surface:
$$
\int_0^{2\pi}\int_0^2 ||\vec{n}|| \text{ } dr \text{ } d\theta = \sqrt{3}\int_0^{2\pi}\int_0^2  r\text{ } dr \text{ } d\theta = 4\pi\sqrt{3}
$$
But after that I was asking myself why changing $drd\theta$ to the infinetesimal element of area $r dr d\theta$ is not giving me the correct answer. If I do that, I'll get my integral to be:
$$
\sqrt{3}\int_0^{2\pi}\int_0^2  r^2\text{ } dr \text{ } d\theta \neq 4\pi\sqrt{3}
$$
When we change our double integral from cartesian coordinates to polar coordinates, the infinetesimal element of area is changed to $dA = rdrd\theta$. But when I do that I get the wrong answer?! What am I doing that is not correct? Thanks!","['multivariable-calculus', 'surface-integrals', 'polar-coordinates']"
2332948,Prove that if $X \overset{d}{=} -X$ and $P(X=0)=0$ then $|X|$ and $\operatorname{sgn}(X)$ are independent.,"As in question - I need to prove that if $X \overset{d}{=} -X$ and $P(X=0)=0$ then $|X|$ and $\operatorname{sgn}(X)$ are independent. I do not even know where to start. I tried something like that:
$$P(|X|\leq k, \operatorname{sgn}(X) \leq \ell, X <0) = \cdots $$ but it gets me nowhere... I will appreciate any help.","['independence', 'probability-theory', 'probability-distributions']"
2333002,Let $ p(x)=\alpha x^{2}+\beta x+\gamma $ be a polynomial,"Let $ p(x)=\alpha x^{2}+\beta x+\gamma $  be a polynomial , where $ \alpha, \ \beta , \gamma \in \mathbb{R} $. Fix  $ x_{0} \in \mathbb{R}$. Let $ \ S=\{(a,b,c): p(x)=a(x-x_{0})^{2}+b(x-x_{0})+c \}$. Then cardinality of $ S $ is (a) $ 0 $, (b) $ 1 $, (c) $ \infty $ My approach: I thought the option $ (c) $ is true. But not sure . Any help ?",['elementary-set-theory']
2333045,Calculate line integral $\oint _C d \overrightarrow{r } \times \overrightarrow{a }$,"Calculate line integral:
$$\oint_C d \overrightarrow{r}  \times \overrightarrow{a},$$
where $\overrightarrow{a} = -yz\overrightarrow{i} + xz \overrightarrow{j} +xy \overrightarrow{k}$ , and curve $C$ is intersection of surfaces given by $ x^2+y^2+z^2 = 1$, $y=x^2$, positively oriented looked from the positive part of axis $Oy$.
I first tried to calculate the intersection vector $\overrightarrow{r }$ by substituting $y=x^2$ into $ x^2+y^2+z^2 = 1$, and completing the square with $y+1/2$ and got the following : $ \overrightarrow{r}= \pm\sqrt{ \sqrt{5/4} \cos(t) - 1/2}\overrightarrow{i}, (\sqrt{5/4}\cos(t) - 1/2 )\overrightarrow{j}, (\sqrt{5/4}\sin(t) )\overrightarrow{k}$,
$  -\cos^{-1} (1/\sqrt 5)<t< + \cos^{-1}(1/\sqrt 5)$
After that i tried to vector multiply the two vectors, after differentiating $\overrightarrow{r}$ first, but I got some difficult expressions to integrate.
Is there an easier way to do this?","['multivariable-calculus', 'calculus', 'line-integrals']"
2333056,How to solve this question using the comparison test,"The problem is this
$$ \int_0^\infty \frac{x}{x^2+1}\,dx $$ You're supposed to find if this converges or diverges. Now you could figure that by solving the integral but if we were supposed to use the comparison test, how would this be solved? My teacher taught me to use the comparison test like so: First change the original equation into something simple that can be solved by inspection. And so I turned the original equation into$$ \int_0^\infty \frac{1}{x}\,dx $$which shows that it's divergent due to the P-Test. Then, based on that information, find an equation bigger or smaller than the original one, and do the test. So since it's divergent, we'd need a bigger equation and I thought of letting that equation be $ \int_0^\infty \frac{x}{x^2+x}\,dx $ (basically just replaced the 1 with an x) and then turn it into the equation above. However there's one problem. The above equation is only smaller than the original equation after $x=1$ and the integral starts at 0. So this is incorrect right? How would I go about solving this? And is my idea of using the comparison test correct? Thanks!","['integration', 'definite-integrals']"
2333094,Determining isometry group of symmetric spaces,"I am learning some symmetric spaces, and I tried to look at some example, including Grassmannian (say, $p$-dimensional subspaces in $\mathbb{C}^{p+q}$) and the projective space $\mathbb{CP}^n$. I follow Helgason, therefore I wish to describe them in terms of Lie groups. My problem is: How to determine the isometry group of a manifold whose geometric description is known? Let me elaborate more. For $\mathbb{CP}^n$, I understand that the special unitary group $SU(n+1)$ acts transitively on it. Also, I know the isotropy group for a certain point is $U(n)$. It is tempting now to claim $\mathbb{CP}^n = SU(n+1)/U(n)$. But I have not checked that $SU(n+1)$ is the isometry group! Similar problems arise when I try Grassmannians, Lagrangian Grassmannians, Orthogonal Grassmannians, etc. I usually can find a group acting well on these spaces but I cannot determine whether that group is the isometry group. Anyone can help? Thanks a lot!","['symmetry', 'riemannian-geometry', 'differential-geometry']"
2333100,"Theorem 6.17 in Baby Rudin, 3rd ed: $\int_a^b f \,d\alpha = \int_a^b f(x) \alpha^\prime(x) \,dx$","Here is Theorem 6.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Assume $\alpha$ increases monotonically and $\alpha^\prime \in \mathscr{R}$ on $[a, b]$ . Let $f$ be a bounded real function on $[a, b]$ . Then $f \in \mathscr{R}(\alpha)$ if and only if $f\alpha^\prime \in \mathscr{R}$ . In that case $$ \int_a^b f \,d\alpha = \int_a^b f(x) \alpha^\prime(x)\, dx. $$ For terminology, here is the link to my post here on Math SE on Theorem 6.15 in Baby Rudin, 3rd edition. Here is Rudin's proof of Theorem 6.17: Let $\varepsilon > 0$ be given and apply Theorem 6.6 to $\alpha^\prime$ : There is a partition $P = \left\{ \ x_0, \ldots, x_n \  \right\}$ of $[a, b]$ such that $$ \tag{28} U(P, \alpha^\prime) - L(P, \alpha^\prime) < \varepsilon. $$ The mean value theorem furnishes points $t_i \in \left[ x_{i-1}, x_i \right]$ such that $$ \Delta \alpha_i = \alpha^\prime \left( t_i \right) \Delta x_i $$ for $i = 1, \ldots, n$ . If $s_i \in \left[ x_{i-1}, x_i \right]$ , then $$ \tag{29}  \sum_{i=1}^n \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i < \varepsilon, $$ by (28) and Theorem 6.7 (b). Put $M = \sup \lvert f(x) \rvert$ . Since $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i = \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i $$ it follows from (29) that $$ \tag{30}  \left\lvert  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i  -  \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i \right\rvert \leq M \varepsilon. $$ In particular, $$  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i  \leq U( P, f \alpha^\prime ) + M \varepsilon, $$ for all choices of $s_i \in \left[ x_{i-1}, x_i \right]$ , so that $$ U(P, f, \alpha) \leq U(P, f \alpha^\prime) + M \varepsilon. $$ The same argument leads from (30) to $$ U(P, f \alpha^\prime) \leq U(P, f, \alpha) + M \varepsilon. $$ Thus $$ \tag{31} \left\lvert U(P, f, \alpha) - U(P, f \alpha^\prime ) \right\rvert \leq M \varepsilon. $$ Now note that (28) remains true if $P$ is replaced by any refinement. Hence (31) also remains true. We conclude that $$ \left\lvert \overline{\int}_a^b f \, d \alpha - \overline{\int}_a^b f (x) \alpha^\prime(x) \,dx  \right\rvert \leq M \varepsilon. $$ But $\varepsilon$ is arbitrary. Hence $$ \tag{32} \overline{\int}_a^b f \,d \alpha = \overline{\int}_a^b f (x) \alpha^\prime(x) \,dx  $$ for any bounded $f$ . The equality of the lower integrals follows from (30) in exactly the same way. The theorem follows. Here is Theorem 6.6 in Baby Rudin, 3rd edition: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that $$ \tag{13} U(P, f, \alpha ) - L(P, f, \alpha ) < \varepsilon. $$ Here is Theorem 6.7: Theorem 6. 7(a): If (13) holda for some $P$ and some $\varepsilon$ , then (13) holds (with the same $\varepsilon$ ) for every refinement of $P$ . Theorem 6. 7(b): If (13) holds for $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ and if $s_i$ , $t_i$ are arbitrary points in $\left[ x_{i-1}, x_i \right]$ , then $$ \sum_{i=1}^n \left\lvert f \left( s_i \right) - f \left( t_i \right) \right\rvert \Delta \alpha_i < \varepsilon. $$ Theorem 6.7(c): If $f \in \mathscr{R}(\alpha)$ and the hypotheses of (b) hold, then $$ \left\lvert \sum_{i=1}^n f \left( t_i \right) \Delta \alpha_i - \int_a^b f\, d \alpha \right\rvert < \varepsilon. $$ Now here is my understanding of Rudin's proof of Theorem 6.17: As $f$ is a bounded real function on $[a, b]$ , so there is a positive real number $\varepsilon > 0$ such that $$ \lvert f(x) \rvert < M \tag{0} $$ for all $x \in [a, b]$ . Let $\varepsilon > 0$ be a given real number. As $\alpha^\prime$ is Riemann-integrable on $[a, b]$ , so there exists a partition $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ of $[a, b]$ such that $$ U(P, \alpha^\prime) - L(P, \alpha^\prime) < \frac{\varepsilon}{2M}. \tag{1} $$ Now as $\alpha^\prime$ exists on $[a, b]$ , so,  For each $i \in \{ 1, \ldots, n \}$ , the function $\alpha$ is continuous on $\left[ x_{i-1}, x_i \right]$ and differentiable on $\left( x_{i-1}, x_i \right)$ , and hence  there exists a point $t_i \in \left( x_{i-1}, x_i \right)$ such that $$  \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) = \alpha^\prime \left( t_i \right) \left( x_i - x_{i-1} \right); $$ that is, $$ \Delta \alpha_i = \alpha^\prime \left( t_i \right) \Delta x_i \tag{2} $$ for each $i = 1, \ldots, n$ . And, for each $i \in \{1, \ldots, n \}$ , if $s_i \in \left[ x_{i-1}, x_i \right]$ , then, as $t_i \in \left[ x_{i-1}, x_i \right]$ also, so we conclude that $$\tag{3a} m_i ( \alpha^\prime )  \leq \alpha^\prime \left( s_i \right) \leq M_i (\alpha^\prime), $$ and $$\tag{3b}  m_i (\alpha^\prime) \leq \alpha^\prime \left( t_i \right) \leq M_i ( \alpha^\prime) ,  $$ where $$ m_i ( \alpha^\prime ) \colon=  \inf \left\{ \ \alpha^\prime (x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\}, $$ and $$ M_i ( \alpha^\prime) \colon= \sup  \left\{ \ \alpha^\prime (x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\};$$ now (3b) implies that $$ \tag{3c} - M_i ( \alpha^\prime ) \leq - \alpha^\prime \left( t_i \right) \leq - m_i (\alpha^\prime),  $$ and upon adding (3c) to (3a), we obtain $$ m_i ( \alpha^\prime ) - M_i( \alpha^\prime) \leq \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \leq M_i ( \alpha^\prime) - m_i( \alpha^\prime),   $$ which implies that $$ \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \leq M_i ( \alpha^\prime) - m_i( \alpha^\prime),  \tag{3d} $$ and, as $\Delta x_i = x_i - x_{i-1} \geq 0$ , so (3d) yields $$ \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq \left[ M_i ( \alpha^\prime) - m_i( \alpha^\prime) \right] \Delta x_i,  \tag{3e} $$ for each $i = 1, \ldots, n$ . Now upon adding together all the inequalities in (3e), we get $$ \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq \sum_{i=1}^n \left[ M_i ( \alpha^\prime) - m_i( \alpha^\prime) \right] \Delta x_i,  $$ that is, $$ \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq U(P, \alpha^\prime) - L(P, \alpha^\prime). \tag{3} $$ From (1) and (3) we get $$ \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i < \frac{\varepsilon}{2M}, \tag{4} $$ for any points $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{1, \ldots, n\}$ ; the points $t_i \in \left[ x_{i-1}, x_i \right]$ are as given in (2) above. Now from (2) we see that $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i = \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i, \tag{5a} $$ where the points $s_i$ and $t_i$ , for each $i \in \{ 1, \ldots, n \}$ , are as in (4) above. So \begin{align} 
& \ \ \ \left\lvert \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert \\ 
&= \left\lvert \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert \\
& \qquad \qquad \mbox{ [ by (5a) ] } \\
&= \left\lvert \sum_{i=1}^n f \left( s_i \right) \left[ \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right) \right] \Delta x_i  \right\rvert \\
&\leq \sum_{i=1}^n \left\lvert  f \left( s_i \right) \left[ \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right) \right] \Delta x_i  \right\rvert \\
&= \sum_{i=1}^n \left\lvert  f \left( s_i \right)  \right\rvert  \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\
&\leq \sum_{i=1}^n M \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\
& \qquad \qquad \mbox{ [ using (0) above ] } \\
&= M \sum_{i=1}^n  \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\
&< M \frac{\varepsilon}{2M} \qquad \mbox{ [ using (4) above ] } \\
&= \frac{\varepsilon}{2}.   
\end{align} Thus we have shown that $$ \left\lvert \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert < \frac{\varepsilon}{2}, \tag{5} $$ for any points $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{1, \ldots, n\}$ . From (5) we can conclude that, for any points $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{  1, \ldots, n\}$ , we have $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i < \frac{\varepsilon}{2}, $$ and so \begin{align}
\sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i &<  \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i +  \frac{\varepsilon}{2} \\
&\leq U( P, f \alpha^\prime) + \frac{\varepsilon}{2}, \tag{6a} 
\end{align} for any point $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{ 1, \ldots, n \}$ . Now we show that $$ U(P, f, \alpha) = \sup \left\{ \ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i \colon \ s_i \in \left[ x_{i-1}, x_i \right] \mbox{ for each } i = 1, \ldots, n \ \right\}. \tag{6b} $$ For each $i \in \{ 1, \ldots, n \}$ , let $$M_i(f) \colon= \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\}; \tag{6c} $$ now as $ f \left( s_i \right) \leq M_i (f)$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ and hence also on $\left[ x_{i-1}, x_i \right]$ , so $$ \Delta \alpha_i = \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \geq 0,$$ and therefore $ f \left( s_i \right) \Delta \alpha_i \leq M_i(f) \Delta \alpha_i$ for each $i \in \{1, \ldots, n \}$ , which implies that $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i \leq \sum_{i=1}^n M_i(f) \Delta \alpha_i = U(P, f, \alpha). $$ Thus $U(P, f, \alpha)$ is an upper bound of the set in (6b). If $\alpha$ is constant on the interval $[a, b]$ , then, for each $i = 1, \ldots, n$ , we have $\Delta \alpha_i = 0$ and so all the sums in the set in (6b) are zero,
and also $U(P, f, \alpha) = 0$ , and so  (6b) holds. So let's assume that $\alpha$ is not constant on the interval $[a, b]$ . Then there exists a sub-interval $\left[ x_{k-1}, x_k \right]$ (for some $k = 1, \ldots, n$ ) such that $$\Delta \alpha_k = \alpha\left( x_k \right) - \alpha\left( x_{k-1} \right) > 0. \tag{*}$$ As $\varepsilon > 0$ and as $\alpha$ is monotonically increasing on $[a, b]$ , so, for each $i \in \{ 1, \ldots, n \}$ , we have $$ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1  } < M_i(f),$$ and thus the real number $ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } $ is not an upper bound of the set in (6c), which implies that there exists a point $p_i \in \left[ x_{i-1}, x_i \right]$ such that $$ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1  } < f\left(p_i\right),$$ and since $\Delta \alpha_i \geq 0$ , we have $$ \left[ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_i \leq  f\left(p_i\right) \Delta \alpha_i$$ for each $i = 1, \ldots, n$ ; but for $i= k$ we have $$ \left[ M_k(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_k <  f\left(p_k\right) \Delta \alpha_k;$$ therefore, $$ \sum_{i=1}^n \left[ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_i < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,$$ that is, $$ \sum_{i=1}^n M_i(f) \Delta \alpha_i - \frac{\varepsilon }{\alpha(b) - \alpha(a) + 1 } \left[ \alpha(b) - \alpha(a) \right]   < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,  $$ which is the same as $$ U(P, f, \alpha) - \frac{\varepsilon }{\alpha(b) - \alpha(a) + 1 } \left[ \alpha(b) - \alpha(a) \right] < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i.
\tag{6d} $$ But, as $a \leq b$ and as $\alpha$ is monotonically increasing, so $$ \alpha(a) \leq \alpha(b), $$ which implies that $$ 0 \leq \alpha(b) - \alpha(a) < \alpha(b) - \alpha(a) + 1, $$ and so $$ 0 \leq \frac{ \alpha(b) - \alpha(a)  }{ \alpha(b) - \alpha(a) + 1 } <  1, $$ which then  implies that $$ 0 \leq \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right] < \varepsilon, $$ and hence $$ - \varepsilon < - \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right]  \leq 0, $$ which implies that $$ U(P, f, \alpha) - \varepsilon < U(P, f, \alpha)  - \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right] , \tag{6e} $$ Now from (6d) and (6e) we have $$ U(P, f, \alpha) -  \varepsilon < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,  $$ which shows that no real number less than $U(P, f, \alpha)$ can be an upper bound of the set in (6b). Thus the real number $U(P, f, \alpha)$ is an upper bound of the set in (6b), but no real number less than $U(P, f, \alpha)$ is an upper bound of that set. Therefore (6b) holds. Now from (6a) we see that the real number $U(P, f \alpha^\prime) + \frac{\varepsilon}{2}$ is an upper bound for the set in (6b), and from (6b) we know that $U(P, f, \alpha)$ is the least upper bound of this very set, so from (6a) and  (6b) we can conclude that $$ U(P, f, \alpha) \leq U(P, f \alpha^\prime) + \frac{\varepsilon}{2}. \tag{6} $$ Again from (5) we obtain $$ \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i -  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i <  \frac{\varepsilon}{2},$$ which implies that \begin{align} 
\sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i 
&< \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i +  \frac{\varepsilon}{2} \\
&\leq U(P, f, \alpha) + \frac{\varepsilon}{2}, \tag{7a} 
\end{align} for any point $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i = 1, \ldots, n$ . We can show that $$ U(P, f\alpha^\prime) = \sup \left\{ \ \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i \ \colon \ s_i \in \left[ x_{i-1}, x_i \right] \mbox{ for each } i = 1, \ldots, n \ \right\}. \tag{7b} $$ From (7a) and (7b) we can conclude that $$ U(P, f\alpha^\prime) \leq  U(P, f, \alpha) + \frac{\varepsilon}{2}. \tag{7} $$ Now if $Q$ be any partition of $[a, b]$ such that $Q \supset P$ , then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have $$ L(P, \alpha^\prime) \leq L(Q, \alpha^\prime) \leq U(Q, \alpha^\prime) \leq U(P, \alpha^\prime), $$ which implies that $$ U(Q, \alpha^\prime) - L(Q, \alpha^\prime) \leq U(P, \alpha^\prime) - L(P, \alpha^\prime), $$ and this together with (1) implies that (1) also holds for $Q$ , and  therefore both (6) and (7) also hold for $Q$ . That is, if $Q$ is any refinement of $P$ , then $$ U(Q, f, \alpha) \leq U(Q, f \alpha^\prime) + \frac{\varepsilon}{2}, \tag{6*} $$ and $$ U( Q, f\alpha^\prime) \leq  U(Q, f, \alpha) + \frac{\varepsilon}{2}. \tag{7*} $$ Now let's suppose that $$ \overline{\int}_a^b f \,d \alpha -   \overline{\int}_a^b f(x) \alpha^\prime(x) \,d x > \varepsilon. \tag{8a} $$ As $$ \overline{\int}_a^b f(x) \alpha^\prime(x) \,dx + \frac{\varepsilon}{2} > 
\overline{\int}_a^b f(x) \alpha^\prime(x) \,dx, $$ so there exists a partition $P_1$ of $[a, b]$ such that $$ U\left( P_1, f \alpha^\prime \right) <  \overline{\int}_a^b f(x) \alpha^\prime(x) dx + \frac{\varepsilon}{2}, \tag{8b} $$ which implies $$- U\left( P_1, f \alpha^\prime \right) > - \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \frac{\varepsilon}{2},$$ and so by (8a) we obtain \begin{align}
 \overline{\int}_a^b f d \alpha - U \left( P_1, f \alpha^\prime \right) &> \overline{\int}_a^b f d\alpha -
 \int_a^b f(x) \alpha^\prime(x) dx - \frac{\varepsilon}{2} \\
&> \varepsilon - \frac{\varepsilon}{2} \\
&= \frac{\varepsilon}{2}, 
\end{align} which implies that $$ \overline{\int}_a^b f d \alpha - U \left( P_1, f \alpha^\prime \right) > \frac{\varepsilon}{2}. \tag{8c} $$ Now  if $Q$ is any partition of $[a, b]$ such that $Q \supset P_1$ , then (by Theorem 6.4 in Baby Rudin) $$ U(Q, f\alpha^\prime) \leq U \left( P_1, f\alpha^\prime \right),$$ and this together with  (8b) implies that $$ U(Q, f\alpha^\prime) <  \overline{\int}_a^b f(x) \alpha^\prime(x) dx + \frac{\varepsilon}{2}, \tag{8b*} $$ from which we obtain $$ \overline{\int}_a^b f d\alpha - U(Q, f \alpha^\prime ) > \frac{\varepsilon}{2}, \tag{8c*} $$ in just the same way as we have obtained (8c) from (8b). Now let $Q$ be any partition of $[a, b]$ such that $Q \supset P$ and $Q \supset P_1$ .
Then (8c*) holds for this $Q$ as well. But as $$ U(Q, f, \alpha) \geq \overline{\int}_a^b f d \alpha, $$ so (8c*) gives $$  U(Q, f, \alpha) - U \left( Q, f \alpha^\prime \right)  \geq \overline{\int}_a^b f d \alpha - U \left( Q, f \alpha^\prime \right)  
> \frac{\varepsilon}{2}, $$ which implies $$  U(Q, f, \alpha) - U \left( Q, f \alpha^\prime \right)  > \frac{\varepsilon}{2}, $$ and so $$  U(Q, f, \alpha) >  U \left( Q, f \alpha^\prime \right)  + \frac{\varepsilon}{2}, $$ which contradicts (6*). Therefore (8a) cannot hold, and so $$ \overline{\int}_a^b f d\alpha - \overline{\int}_a^b f(x) \alpha^\prime(x) dx \leq \varepsilon. \tag{8} $$ Now let's suppose that $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha > \varepsilon. \tag{9a}$$ As $$ \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2} > \overline{\int}_a^b f d\alpha, $$ so there exists a partition $P_2$ of $[a, b]$ such that $$ U \left( P_2, f, \alpha \right) <  \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2}, \tag{9b} $$ which implies that $$ - U \left( P_2, f, \alpha \right) > - \overline{\int}_a^b f d\alpha - \frac{\varepsilon}{2}, $$ which together with (9a) gives \begin{align}
\overline{\int}_a^b f(x) \alpha^\prime(x) dx - U \left( P_2, f, \alpha \right) &> \overline{\int}_a^b f(x) \alpha^\prime(x) dx -
\int_a^b f d\alpha - \frac{\varepsilon}{2} \\
&> \varepsilon - \frac{\varepsilon}{2} \\
&= \frac{\varepsilon}{2}, 
\end{align} which implies that $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - U \left( P_2, f, \alpha \right) > \frac{\varepsilon}{2}. \tag{9c} $$ Now if $Q$ is any partition of $[a, b]$ such that $Q \supset P_2$ , then (by Theorem 6.4 in Baby Rudin) $$ U(Q, f, \alpha) \leq U \left( P_2, f, \alpha \right), $$ which together with (9b) yields $$  U(Q, f, \alpha) <  \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2}, \tag{9b*} $$ and then we obtain $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - U ( Q, f, \alpha ) > \frac{\varepsilon}{2}, \tag{9c*} $$ in just the same way as we have obtained (9c) from (9b). Now let $Q$ be any partition of $[a, b]$ such that $Q \supset P$ and $Q \supset P_2$ . Then (9c*) holds for this $Q$ . But as $$ U ( Q, f \alpha^\prime) \geq \overline{\int}_a^b f(x) \alpha^\prime(x) dx, $$ so (9c*) gives $$ U(Q, f \alpha^\prime ) - U(Q, f, \alpha) \geq \overline{\int}_a^b f(x) \alpha^\prime (x) dx - U(Q, f, \alpha) > \frac{\varepsilon}{2},$$ which implies $$  U(Q, f \alpha^\prime ) - U(Q, f, \alpha) > \frac{\varepsilon}{2},$$ and so $$  U(Q, f \alpha^\prime ) >  U(Q, f, \alpha) + \frac{\varepsilon}{2},$$ which contradicts (7*). Thus (9a) cannot hold, and so we can conclude that $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \leq \varepsilon. \tag{9}$$ Now from (8) and (9) we can conclude that $$\left\lvert  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \right\rvert 
\leq \varepsilon $$ for any real number $\varepsilon > 0$ . Hence $$\left\lvert  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \right\rvert 
= 0, $$ which implies that $$ \overline{\int}_a^b f d\alpha = \overline{\int}_a^b f(x) \alpha^\prime(x) dx. \tag{A}  $$ An analogous argument gives $$ \underline{\int}_a^b f d\alpha = \underline{\int}_a^b f(x)  \alpha^\prime (x) dx. \tag{B} $$ Now suppose $f \in \mathscr{R}(\alpha)$ on $[a, b]$ . Then $$ \underline{\int}_a^b f d\alpha = \overline{\int}_a^b f d \alpha, $$ and then (A) and (B) together imply $$ \underline{\int}_a^b f(x) \alpha^\prime(x) dx = \overline{\int}_a^b f(x) \alpha^\prime(x) dx, $$ showing that $f\alpha^\prime \in \mathscr{R}$ on $[a, b]$ . Conversely, suppose $f \alpha^\prime \in \mathscr{R}$ on $[a, b]$ . Then $$ \underline{\int}_a^b f(x) \alpha^\prime(x) dx  = \overline{\int}_a^b f(x) \alpha^\prime(x) dx, $$ and then (A) and (B) together imply $$ \underline{\int}_a^b f d \alpha = \overline{\int}_a^b f d \alpha , $$ showing that $f \in \mathscr{R}(\alpha)$ on $[a, b]$ . Finally, we assume that $f \in \mathscr{R}(\alpha)$ on $[a, b]$ --- which (as we have just shown) is equivalent to assuming that $f \alpha^\prime \in \mathscr{R}$ on $[a, b]$ --- and then find from (A) and (B) that $$ \int_a^b f d\alpha = \int_a^b f(x) \alpha^\prime(x) dx, $$ as required. Is my rendering of Rudin's proof correct? If not, then where have I erred? I admit that my presentation is very very, very lengthy, but it is to
demonstrate my reasoning clearly and fully enough.","['real-analysis', 'integration', 'definite-integrals', 'analysis', 'solution-verification']"
2333129,How to find the limit of $\frac{\sqrt{x^2+9}-3}{x^2}$? as x approaches 0,"How to find the limit of $\frac{\sqrt{x^2+9}-3}{x^2}$? as x approaches 0 I thought that this was going to be a simple problem but then it got more complicated than I have expected So I first multiplied the expression with its conjugate on both the denominator and the numerator
the conjugate was the following: $$\sqrt{x^2+9}+3$$ but when I multiplied by the conjugate I actually didn't even get close to the answer because the answer was supposed to be $\frac{1}{6}$ instead I have gotten the following by multiplying by its conjugate $\frac{x^2+9-9}{x^2\sqrt{x^2+9}-3x^2}$ which obviously doesn't return 1/6 I was wondering if I was just simply making a mistake or if I have to choose a different option in solving this?","['calculus', 'limits']"
2333154,Somebody help me please. I have a difficult inequality.,Let $ab+bc+ca=1$. Prove that $2 \ge \sqrt{1+a^2} + \sqrt{1+b^2}+\sqrt{1+c^2}-a-b-c \geq \sqrt3 $.,"['inequality', 'trigonometry', 'sum-of-squares-method', 'karamata-inequality', 'contest-math']"
2333166,Why do i have to transpose the coefficients of a system of linear transformations to get its matrix form?,"I have been reading a book about linear algebra that is very popular in my country and  in the chapter about the matrix of a linear transformation he states the following: $``$ Be $\ U\ $ and $\ V\ $ two vector spaces, over $\ {\rm I\!R} \ $, of dimensions $n$ and $m$, respectively. Consider the linear transformation $\ F:U \rightarrow V \ $. Given the basis $\ B=\{u_1, ...,u_n\}\ $ of $\ U \ $ and the basis $\ C = \{v_1,...,v_m\} \ $  of $\ V \ $ ,  therefore each one of the vectors $\ F(u_1),...,F(u_n)\ $ are in $\ V \ $ and consequently are linear combinations of the basis $\ C \ $: $$F(u_1) = \alpha_{11}v_1 + \alpha_{21}v_2 + ... + \alpha_{m1}v_m$$
$$F(u_2) = \alpha_{12}v_1 + \alpha_{22}v_2 + ... + \alpha_{m2}v_m$$
$$\vdots$$
$$F(u_n) = \alpha_{1n}v_1 + \alpha_{2n}v_2 + ... + \alpha_{mn}v_m$$ $Definition \ \ 4$ - A matrix $\ m\times n \ $ over $\ {\rm I\!R} \ $.
$$ \qquad \quad \begin{pmatrix}
    \alpha_{11} & \alpha_{12} & \dots  & \alpha_{1n} \\
    \alpha_{21} & \alpha_{22}  & \dots  & \alpha_{2n} \\
    \vdots & \vdots  & \ddots & \vdots \\
    \alpha_{m1} & \alpha_{m2}  & \dots  & \alpha_{mn}
\end{pmatrix}=(\alpha_{ij})$$
that is obtained from the previous considerations is called matrix of $F$ in relation to the basis $B$ and $C$ . $""$ So why is it the transpose of the matrix of the coefficients of the linear relations instead of the usual matrix ?","['matrices', 'linear-algebra', 'linear-transformations']"
2333191,"If $|f(x)-f(y)|<(x-y)^2$ for all $x,y \in \mathbb{R}$. Then $f(x)$ is constant. [duplicate]","This question already has answers here : Show $f$ is constant if $|f(x)-f(y)|\leq (x-y)^2$. (3 answers) Closed 7 years ago . If $|f(x)-f(y)|<(x-y)^2$ for all $x,y \in \mathbb{R}$. Then $f(x)$ is constant. $\bf{Attempt}$ Put $x=y+h$ where $h \rightarrow 0$ Then $\displaystyle |f(y+h)-f(y)|<(y+h-y)^2 = h^2$ So $\displaystyle \displaystyle \lim_{h\rightarrow 0}\bigg|\frac{f(y+h)-f(y)}{h}\bigg|<\lim_{h\rightarrow 0}h$ So $\displaystyle |f'(y)|<0$ Now how can I calculate $f(x)$? Could someone help me? Thanks",['functions']
2333266,"Can a simple, closed curve be homeomorphic to a closed curve that is not simple?","It seems like most standard definitions of simple curves in $\mathbb{R^2}$ are those without any self-intersections. Moreover, it can be shown that if $C$ is a simple closed curve, then $C$ is homeomorphic to the circle $S^1$. (All using the subspace topology of the standard Euclidean topology on $\mathbb{R^n}$ for both, that is.) So suppose a curve $C$ is not simple, so that it has at least one point of self intersection. That is, if $C$ is identified as the image of a continuous function $f: S^1 \rightarrow \mathbb{R}^2$ from the circle to the plane, then there is at least one point $p\in S^1$ at which $f$ fails to be injective. In this case, can $C$ still be homeomorphic to a circle? My hunch is that it can not be, and here's what I've come up with so far: Towards a direct proof, I think that if $q = f(p)$ is such a point of self-intersection on the curve $C$, then the inverse function $f^{-1}$ may fail to be continuous at $q$. Intuitively, it seems like neighborhoods around this point will always contain points from at least two ""branches"" of $C$, but I'm not sure how to proceed from here. Using other topological invariants seemed promising - I think deleting the point $q$ breaks $C$ into at least two connected components while deleting its preimage $p$ leaves the circle in one connected component. Does this constitute a valid proof? Edit: Phrased another way, if we identify knots as continuous images of $S^1$ embedded in $\mathbb{R^3}$, when is a knot homeomorphic to $S^1$ ?","['knot-theory', 'general-topology']"
2333270,Derivative of an Integral whose integrand is discontinuous,"Let, $g(x) =\int_{a}^{x} f(t) dt$ be an integral function. What can we say about $g'(c)$ when, a) $f$ is removable discontinuous at $c \in[a,x]$? b) $f$ is infinite discontinuous at $c\in [a,x]$? c) $f$ is jump discontinuous at $c\in [a,x]$? Here is what I think. a) Given, $$f(x) = \begin{cases} {x^3} & \text{if $x\in \mathbb{R}  - \{2\} $} \\ 10 & \text{if $x=2$} \end{cases}$$
Here, $x=2=c$ is a point of removable discontinuity. The right-hand derivative of $g(x)$ at $c$ is given as, $$g'(c) =\lim_{h\to 0^{+} } \frac{g(c+h)-g(c)}{h}= \lim_{h\to 0^{+} }\frac{\int_{c}^{c+h}f(t) dt}{h}$$
The left-hand derivative of $g(x)$ at $c$ is given as, $$g'(c) = \lim_{h\to 0^{+} } \frac{g(c-h)-g(c)}{-h}= \lim_{h\to 0^{+} }\frac{\int_{c-h}^{c}f(t) dt}{-h}$$ We see that the left and right derivatives converge at 8. This means that $g'(2)=8$. We know that, ""for continuous integrands"", $g'(x)=f(x)$. Here, $f(2)\neq 8$. This tells me that the derivative does not exist at $x=2$. Am I thinking in the right direction?",['calculus']
2333275,Find probability of biased Coins,"There are two biased coins A and B. We have been given the following: P(H|Coin A) = 0.9 P(T|Coin A) = 0.1 P(H|Coin B) = 0.1 P(T|Coin B) = 0.9 We are also given that 
Probability of tossing Coin A or Coin B randomly is 0.5 How could we find the probability of ""11th toss = Head"", given that we have 10 heads in a row ?",['probability']
2333294,How to look at the Lie derivative as a partial differential operator?,"In Gromov's book Partial Differential Relations, a partial differential operator (PDO) is defined as follows. Given a fibration $X\to V$ and a vector bundle $G\to V$ over a manifold $V$, a PDO of order $k$ is a map $D:\Gamma(X)\to \Gamma(G)$, satisfying $D(x)=\Delta\circ j^k_x$ for every $x\in \Gamma(X)$, where $\Delta:X^{(k)}\to G$ is a bundle map. Here $X^{(k)}$ is the $k^{\text{th}}$ jet bundle of sections of $X\to V$ and $j^k_x$ is the $k^{\text{th}}$ jet of $x$. Now, say $G$ is some tensor bundle on $V$ and $X=TV$. Then for a fixed section $\omega\in\Gamma(G)$, we have an operator,
\begin{align*}
D_\omega:\Gamma(TV)&\to \Gamma(G)\\
x&\mapsto \mathcal{L}_x(\omega)
\end{align*}
where $\mathcal{L}_x(\omega)$ is the Lie derivative of the tensor $\omega$ along the vector field $x$. How do I see that this $D_\omega$ is indeed a partial differential operator of order 1, as defined by Gromov? In particular, I would like to find out the bundle map $\Delta=\Delta_w$ such that $D_\omega(x)=\Delta\circ j^1_x$. For the case $G=\Lambda^k T^*V$, I can use the Cartan's formula and get, $$D_\omega(x)=\mathcal{L}_x(\omega)=d\iota_x(\omega)+\iota_xd\omega$$ I do know that the exterior derivative map $d$ can be seen as a PDO. But I still can't see how to get to the map $\Delta$. And I'm in complete loss when $G$ is an arbitrary tensor bundle, since in that case the Lie derivative is defined using flows. Any help regarding this appreciated.","['differential-operators', 'differential-geometry', 'differential-topology', 'partial-differential-equations']"
2333306,"For any two sets, $A - B = B - A$ implies $A = B$","Is the following statement True or False: For any two sets $A$ and $B$: If $A - B = B - A$ then $A = B$. If it is true, prove it, otherwise provide a counterexample. I am unable to come up with a counter example. I think the statement is true but how do I prove it?","['proof-writing', 'elementary-set-theory']"
2333311,Is there a notion of a smallest (immersed) Lie subgroup containing a given set?,"Let $G$ be a Lie group, $S \subseteq G$. Is there a smallest Lie subgroup of $G$ containing $S$? (i.e a Lie subgroup $H$ which is contained in every Lie subgroup which contains $S$). We need to distinguish between two cases: If we talk about embedded (equivalently closed ) Lie subgroups, then the answer is positive: Take $\tilde H=\cap \{H \, \, | \, \, H \text{  is a closed Lie subgroup of $G$ which contains $S$}\}$. Then $\tilde H$ is a closed subgroup, hence (by the closed subgroup theorem), it is a closed (embedded) Lie subgroup of $G$, so we are done. What happens if we allow immersed Lie subgroups? The definition I am using is that $H \le G$ is a Lie subgroup, if it is a subgroup endowed with some topology and smooth structure, making it a Lie group, and an immersed submanifold of $G$. Now if we try to repeat the ""intersection construction"" (with immersed subgroups), the result is a group, but it's not clear that it is an immersed submanifold. Is there in fact an example where the intersection won't be a Lie subgroup?","['smooth-manifolds', 'differential-geometry', 'differential-topology', 'lie-groups']"
2333367,Showing differentiability of product of two functions,"Let $\Omega$ be a nonempty open subset of $\mathbb{R}^n$ and let $f:\Omega\to \mathbb R$ be a differentiable function at $\mathbf{x}_0\in \Omega$ such that $f(\mathbf{x}_0)=0$. If $g$ is continuous at $x_0$, then I have to show that $fg$ on $\Omega$ defined by $fg(\mathbf{x})=f(\mathbf{x})g(\mathbf{x})$ is differentiable at $\mathbf{x}_0$. Any help is appreciated.","['multivariable-calculus', 'real-analysis', 'derivatives']"
2333372,How does $y[x] = a x^r$ when $y'[x] = \frac{(r y[x])}{x}$ and $y[1] = a$,"When
\begin{eqnarray*}
y'[x] &=& \frac{r y[x] }{ x } \; \;\text{and} \;\; y[1] = a \\
y[x] &=& a x^r
\end{eqnarray*}
Use derivative formulas to explain the output. What I have so far: We know that 
\begin{eqnarray*}
y'[x] = r y[x] = k e^{rx}
\end{eqnarray*}
So we can rewrite as 
\begin{eqnarray*}
y[x] = \frac{k e^{rx} } { x}
\end{eqnarray*}
To get the value of k:
\begin{eqnarray*}
a = \frac{k e^{r} }{ 1} \\
k = \frac{a}{e^r}
\end{eqnarray*}
So now we have
\begin{eqnarray*}
\frac{(a/(e^r)) e^{rx} } { x}
\end{eqnarray*}
But when I plug this into a derivative calculator I get a solution much different than $y[x] = a x^r$. Perhaps I am approaching this wrong. What is the correct way to approach this problem using derivative formulas?","['derivatives', 'ordinary-differential-equations', 'calculus']"
2333380,Regular conditional probability for the 0-1 sigma-algebra,"Let $(\Omega, \mathcal A, P)$ be a probability space and $\mathcal F := \{A \in \mathcal A \mid P(A) \in \{0,1 \}\}$. Show that $\mathcal F$ is a $\sigma$-algebra and find a regular conditional probability $P(\cdot\mid\mathcal F)(\cdot)$. The proof of $\mathcal F$ being a $\sigma$-algebra is not the problem. But for the
regular conditional probability, I know, that 
\begin{align}
E(X\mid\mathcal F)=E(X)
\end{align}  for $X:(\Omega, \mathcal A, P) \rightarrow (E, \mathcal E)$. I think because of 
\begin{align}
P(A\mid\mathcal F)(\omega):=E(\mathbb{1}_{A}\mid\mathcal F)(\omega)
\end{align}
for $A \in \mathcal A$,
I can deduce 
\begin{align}
P(X \in B\mid \mathcal F)=P(X \in B) 
\end{align}
for $B \in \mathcal E$. If $P(X \in B)$ would be a markov kernel, I would have found my regular conditional probability here. So now I have to somehow get $P(X\in B)$ to be a markov kernel. But how to do so? Are my thoughts going in the right direction? Maybe someone can help me. I don't think it is that difficult, but I don't see it right now...","['probability-theory', 'conditional-expectation']"
2333383,Double integral of a single variable function,$$\int_{\mathbb{R}}  \left(\int_{\mathbb{R}} f(x)dx \right) g(x) dx$$ Can this integral be reduced to $$  \int_{\mathbb{R}} f(x)g(x)dx $$,"['integration', 'calculus']"
2333413,Solving a first order nonlinear nonhomogeneous ODE,"Edit: from the answers, I have learnt that the differential equation can be solved by expressing it as being a hypergeometric differential equation. My question now is that, how many a function in the form of $x(1-x)^2y''(x)+(1-x)^2y'(x)+ay(x)=0$ be transformed into the form of $\eta(1-\eta)f''(\eta)+(b-c\eta)f'(\eta)+df(\eta)=0$? Original question: How may one solve a differential equation in the form of: $\frac{dy}{dx}=P(x) -ky^2$ I have attempted at reducing it into a second order homogeneous equation in the form of
$\frac{d^2u}{dx^2}=-kP(t)u$ by making the substitution of $ky= \frac{\frac{du}{dx}}{u}$ However, I am still unable to solve this. Are there any methods for solving either equation? If it helps, P(x) is the derivative of: $f(x)=\frac{a-be^{cx+d}}{1-e^{cx+d}}$ where a,b,c,d are constants Additionally,
y=0 when x=0, and y=0 as x$\rightarrow$ infinity A numerical approach to solving the equation with randomly chosen values for constants substituted in gives the following graph: Link which looks like (maybe) a chi-square distribution....?",['ordinary-differential-equations']
2333417,Is absolute value of an analytic function a harmonic function?,"It is well known that if $f(x+i y) = u(x, y) + i v(x, y)$ is an analytic function of variable $z = x + iy$ then both $u$ and $v$ are harmonic functions. Does $|f| = \sqrt{u^2 + v^2}$ have any special properties, in particular is $|f|$ harmonic?","['complex-analysis', 'analytic-functions', 'harmonic-functions', 'absolute-value']"
2333438,(Reference Request) Calculus on Banach Spaces,"I'm reading Lang's Real and Functional Analysis, and I am surprised that one can still do a fair amount of calculus (differential/integral) on abstract Banach spaces, not just $\mathbb{R}$ of $\mathbb{R}^N$. For example, Lang writes about Bochner integrals - which is slightly different from the 'usual' Lebesgue integral - which gives you a way to integrate Banach-space-valued maps. Also, he uses theorems of differential calculus (of Banach spaces) to prove results about flows on manifolds, which is quite fundamental to differential geometry. I'm on chapter 7 right now, and I wonder what other good books are there, dealing with this subject: calculus on Banach spaces. After dealing with integration and differentiation (in that order), Lang moves on to 'functional analysis', but I want to see more applications and examples of calculus; for example, Banach-space-valued power series (on $z\in\mathbb{C}$, say), whether one can use the familiar techniques of complex analysis in that case (e.g. Cauchy integral formula), or how the theory is used for differential topology/geometry. Can anyone suggest a text that gives a complete/thorough treatment of calculus in Banach spaces? (Ones with geometric flavor are even nicer!) Any advice is welcome.","['banach-spaces', 'reference-request', 'calculus', 'functional-analysis', 'book-recommendation']"
2333449,Let $V=\mathcal{V}(XT-YZ)\subset \mathbb{A}^4$. Let $f=\frac{X}{Y}\in k(V)$. What is the domain dom $f$?,"I have determined $$A=\{(X,Y,Z,T)\in V\mid Y\neq 0 \textrm{ or } T\neq 0\}\subset \textrm{dom }f$$ However I am unsure of how to show that $A=\textrm{dom }f$.",['algebraic-geometry']
2333536,Construction of Picard group and set theoretic issue.,"I am currently learning Algebraic geometry and I came over something that is bothering me. I have an algebraic variety and the Picard group over it is defined as the set of classes of line bundles quotiented by the the relation of being isomorphic. First of all it seems to me that these classes are actually proper classes and not sets, since I can equip almost any set that has the right cardinal with a structure of line bundle over a given variety. How can I collect them then into a set if they are in fact proper classes? And even if I do, how can I guarantee that I actually won't get a proper class?","['set-theory', 'algebraic-geometry']"
2333538,Probability of rolling X successes with differently sized dice,"I'm trying out different systems for a board game, and my (weak) probability skills are failing me. The basic mechanic is as follows: you roll a number of six-sided dice (d6). Any die showing 5 or more is a success.
A specific bonus can ""upgrade"" part of your pool to a larger die (let's say a 10-sided dice, or d10). So for example: regular roll : roll 7d6; any die showing 5 or more is one success. same roll with bonus : roll 4d6 and 3d10; any die showing 5 or more is one success. I'd like to learn how to calculate the chance of getting X successes with the mixed pool (either ""X"" successes or ""X or more"" successes). this thread thaught me how to find the probability for the homogenous pool, but I have no idea how to change the formula to accomodate the differently-sized dice. I would like to learn the general formula, to be adjusted for different size of dice (such as d8 and d12) and for a larger or smaller pool of mixed dice as well. Thanks in advance, sorry for any English mistake I may have made.","['probability', 'dice']"
2333551,Meaning of $\times$ in this definition of a function?,"What is the meaning of writing $\times$ here? For $(t,x,\xi)\in[0,\infty)\times\mathbb{R}^3 \times \mathbb{R}^3$ we consider the function $f(t,x,\xi)$. Can't we just write $t\in[0,\infty)$, $x\in\mathbb{R}^3$ and $\xi\in\mathbb{R}^3$? Update: Using the cartesian product, does ""$(t,x,\xi)\in[0,\infty)\times\mathbb{R}^3 \times \mathbb{R}^3$"" actually mean \begin{align*}
[0,\infty[ \times \mathbb{R}^3\times \mathbb{R}^3=
\bigg\{
(t,x,\xi):t&\in[0,\infty[,\\ x&=(x_1,x_2,x_3)\in \mathbb{R}^3,\\ 
\xi&=(\xi_1,\xi_2,\xi_3)\in \mathbb{R}^3
\bigg\} \quad ?
\end{align*}
And also, should I write $t\in[0,\infty[$ or $t=[0,\infty[$?","['multivariable-calculus', 'real-analysis', 'notation', 'vector-analysis']"
2333552,Probability/ statistics bracket notation?,"Say $X(t)$ is a random variable for each $t \in \mathbb R$. In various papers I've seen the notation $$\langle X(t') X(t'+t)\rangle = C(t)$$ where $C(t)$ is referred to as the correlation function, $$\langle X(t) X(t)\rangle = C(0) \equiv M^2$$
where $M$ is referred to as the typical magnitude, and $$\langle f(X(t))\rangle$$ (where $f$ is some function, say $\exp(\int X(t) dt)$) referred to as an ""ensemble average."" What does the bracket notation $\langle \rangle$ mean? How is it consistent? Could you direct me to a book/ source that explains this?","['statistics', 'probability']"
2333570,(Smirnov 1939) Paper: On the estimation of the discrepancy between empirical curves of distribution for two independent samples.,"Does anyone have an english copy of this paper and/or any other reference where the results from this paper are explained? Unable to find it after searching for many days. Please let me know if you need any further information. The full citation is below: 1) Smirnov, N. V. (1939). On the estimation of the discrepancy between empirical curves of distribution for two independent samples. Bull. Math. Univ. Moscou, 2(2). Two papers with important results are also not found. Please post links to the paper or references that explain it in English. 2) Smirnov, N. V. (1939). Ob uklonenijah empiricheskoi krivoi raspredelenija. Recueil Mathematique (Matematiceskii Sbornik), N.S. Vol. 6(48), pp. 3-26. 3) Smirnov, N. V. E. (1944). Approximate laws of distribution of random variables from empirical data. Uspekhi Matematicheskikh Nauk, (10), 179-206.","['reference-request', 'probability-theory', 'asymptotics', 'probability-distributions']"
2333584,$\sum_{n=1}^{\infty}\underbrace{ \sin\left ( \sin \left ( \dots \sin \left ( \dfrac {\pi}{2} \right ) \dots \right ) \right ) }_{n\text { #} \sin }$ [duplicate],"This question already has an answer here : Is $c + \sin c + \sin\sin c + \sin\sin\sin c + \cdots$ convergent? (1 answer) Closed 4 years ago . Following interesting trigonometric series came into my mind when I solve another series problem. $\sum\limits_{n=1}^{\infty}\underbrace{\sin \left ( \sin \left ( \sin \left ( \dots \sin \left ( \dfrac {\pi}{2} \right ) \dots \right ) \right ) \right )}_{n \text { number of } \sin \text { terms}}\\=\sin \left ( \dfrac {\pi}{2} \right )+\sin \left (  \sin \left ( \dfrac {\pi}{2} \right ) \right )+\sin \left ( \sin \left (  \sin \left ( \dfrac {\pi}{2} \right ) \right ) \right )+\dots\\=1+\sin \left (  \sin \left ( \dfrac {\pi}{2} \right ) \right )+\sin \left ( \sin \left (  \sin \left ( \dfrac {\pi}{2} \right ) \right ) \right )+\dots $ Since for each $x\in \mathbb {R}$, $-1\le \sin x \le 1$, $n$th term of the series is defined  and lies between minus one and plus one. Do you have any idea about convergence of the series? I believe that many of you have nice ideas and arguments. So please share with us. Thank you.","['real-analysis', 'trigonometry', 'sequences-and-series', 'calculus']"
2333601,Finding centroid of spherical triangle,"Given three vectors $u,v,w\in S^2$, and a spherical triangle $[u,v,w]$, find its centroid, i.e. the point of intersection of the three medians $\left[u, \frac{v+w}{|v+w|}\right]$, $\left[v, \frac{u+w}{|u+w|}\right]$ and $\left[w, \frac{u+v}{|u+v|}\right]$. The problem is that I don't even know how to approach this problem at all, the reason being that I'm not sure how to derive equations for the three medians. Your help would be much appreciated.","['analytic-geometry', 'centroid', 'spherical-geometry', 'geometry']"
2333615,Finding a weight function for Bessell Sturm-Liouville problem with some boundary conditions,"I have a Bessel differential equation 
$$r^2R'' + rR' + k^2r^2R=0$$ on $r = (R_1, R_2)$ with boundary conditions $$R'(r=R_1) = A$$
$$R'(r=R_2) = 0$$ where $A$ is some constant I can determine through a physical background of the problem. The solution is $$R(r) = J_0(kr) + B Y_0(kr)$$where $B=-\frac{J_1(kR_2)}{Y_1(kR2)}$. How would you go about finding a weight function, that would give orthogonality to the solutions of the problem? Also is there a way to find a weight function for any given boundary conditions?","['bessel-functions', 'sturm-liouville', 'ordinary-differential-equations', 'mathematical-physics']"
2333640,"Doing calculus with ""currents""; how to learn this?","In mathematics, the word ""distribution"" means at least two different things: In analysis, they're elements of a certain dual space. In differential geometry, they're subsets of the tangent bundle satisfying certain axioms. I'm interested in the analysis version (i.e. concept 1), but doing it in the context of differential geometry. After Googling around a bit, I found the notion of a current , which is exactly what I was looking for; yay! This seems to give a very coherent viewpoint on the fundamental theorem of calculus; if I understand correctly, you can write stuff like $$\int_{x=a}^b 2x = \int_{x=a}^b \left(\frac{\partial}{\partial x} x^2\right) = \left(\frac{\partial}{\partial x} \int_{x=a}^b\right)(x^2) = [x^2]^b_{x=a} = b^2-a^2,$$ where $\int_{x=a}^b$ is thought of as a $0$-current on $\mathbb{R}$. Well, I'm not sure how much sense that really makes; perhaps the correct reasoning is more like: $$\int_{x=a}^b 2x dx = \int_{x=a}^b d(x^2) = \left(\partial \int_{x=a}^b\right)(x^2) = [x^2]^b_{x=a} = b^2-a^2,$$ where $\int_{x=a}^b$ is a $1$-current on $\mathbb{R}$. Honestly, I have no idea what I'm talking about; but, anyway it looks really interesting, and if I'm right that $\int_a^b$ can be viewed as both a $0$-current and $1$-current, that would go a long way toward clearing up some confusion I've had regarding the relationship between differential topology, where integrating forms over oriented submanifolds is all the rage, and measure theory, where integrating real-valued functions over subsets is hot Prada. Question. What should my next step be to learn about these things, especially given the following goals? I'd like to understand FTC and Stokes's theorem better. I'd like to understand the relationship between differential geometry and measure theory better. I'd like to be able to teach calculus better.","['reference-request', 'distribution-theory', 'multivariable-calculus', 'measure-theory', 'differential-geometry']"
2333650,"If $[A,B] = \lambda I$ and $A$ has a discrete spectrum, will $B$ also have discrete spectrum?","Let $A$ and $B$ be self-adjoint operators on some Hilbert space and let their commutator be proportional to identity operator: $AB - BA = \lambda I$ Let us also assume that $A$ has a discrete spectrum, i.e. a countable set $\{\psi_n\}$ of eigenvectors with eigenvalues $\{\lambda_n\}$: $A\psi_n = \lambda_n\psi_n$ Does this necessarily imply that the operator $B$ also has discrete spectrum? I.e. can it be shown that there exists a countable set of eigenvectors $\{\phi_n\}$ and eigenvalues $\{\mu_n\}$, such that: $B\phi_n = \mu_n\phi_n$ Here I use the words ""spectrum"" and ""eigenvalues"" interchangeably, i.e. as a physicist, not as a mathematician :) The specific application I have in mind is the Extended Hilbert Phase Space (see my paper on this ) where the operators of energy and time are introduced and their commutator is proportional to identity. Now, in many situations the energy has discrete spectrum, so my question can be rephrased as: does this imply that the time is also, likewise, discrete?","['functional-analysis', 'spectral-theory', 'quantum-mechanics']"
2333666,Group is isomorphic to direct product of its subgroups,"In my book, I have a theorem that says the following: Let $G$ be a group. If $G_1,G_2$ are subgroups such that: $G_1, G_2 \lhd G$ $G_1G_2 = G$ $G_1 \cap G_2 =\{e_G\}$ Then $G \cong G_1 \times G_2$ Later, there is a remark that says that the converse of the theorem also holds. So, I suppose this means, that if we have that $G \cong G_1 \times G_2$ for subgroups $G_1,G_2$, then the three conditions listed above hold. The 'proof' goes as follows: If $G = G_1 \times G_2$, then $G = H_1H_2 $ with $H_1 = G_1 \times \{e_{G_2}\}$ and $H_2 = \{e_{G_1}\} \times G_2$. The groups $H_1,H_2$ are normal in $G$ and $H_1 \cap H_2 = \{e\} \quad \triangle$ Can someone explain this please? I really don't get how this proves anything. They don't even start from $G \cong G_1 \times G_2$? I will award the bounty to the person who can give me a detailled and rigorous explanation.","['abstract-algebra', 'direct-product', 'group-theory', 'proof-explanation']"
2333713,Elementary way to calculate the series $\sum\limits_{n=1}^{\infty}\frac{H_n}{n2^n}$,"I want to calculate the series of the Basel problem $\displaystyle{\sum_{n=1}^{\infty}\frac{1}{n^2}}$ by applying the Euler series transformation. With some effort I got that $$\displaystyle{\frac{\zeta (2)}{2}=\sum_{n=1}^{\infty}\frac{H_n}{n2^n}}.$$ I know that series like the $\displaystyle{\sum_{n=1}^{\infty}\frac{H_n}{n2^n}}$ are evaluated here , but the evaluations end up with some values of the $\zeta$ function, like $\zeta (2),\zeta(3).$ First approach : Using the generating function of the harmonic numbers and integrating term by term, I concluded that $$\displaystyle{\sum_{n=1}^{\infty}\frac{H_n}{n2^n}=\int_{0}^{\frac{1}{2}}\frac{\ln (1-x)}{x(x-1)}dx},$$ but I can't evaluate this integral with any real-analytic way. First question: Do you have any hints or ideas to evaluate it with real-analytic methods? Second approach : I used the fact that $\displaystyle{\frac{H_n}{n}=\sum_{k=1}^{n}\frac{1}{k(n+k)}}$ and then, I changed the order of summation to obtain $$\displaystyle{\sum_{n=1}^{\infty}\frac{H_n}{n2^n}=\sum_{k=1}^{\infty}\frac{2^k}{k}\left(\sum_{m=2k}^{\infty}\frac{1}{m2^m}\right)}.$$ To proceed I need to evaluate the $$\int_{0}^{\frac{1}{2}}\frac{x^{2k-1}}{1-x}dx,$$ since $\displaystyle{\sum_{m=2k}^{\infty}\frac{1}{m2^m}=\int_{0}^{\frac{1}{2}}\frac{x^{2k-1}}{1-x}dx}.$ Second question: How can I calculate this integral? Thanks in advance for your help.","['integration', 'definite-integrals', 'sequences-and-series', 'harmonic-numbers']"
2333715,"show that $\lim_{n \to \infty} n\int_0^1 e^{-tn}(\frac{\sinh t}{t})^n \, dt=1$","I tried to solve $$\lim_{n \to \infty} n\int_0^1\ldots \int_0^1\frac{1}{x_1+x_2+\ldots+x_n}\,dx_1\ldots dx_n$$ I started with $$ \frac{1}{x_1+x_2+\ldots+x_n}=\int_0^{\infty}e^{-t(x_1+x_2+\ldots+x_n)}\,dt$$ $$\Rightarrow I=\int_0^1\ldots\int_0^1\frac{1}{x_1+x_2+\ldots+x_n}dx_1\ldots dx_n=\int_0^{\infty}\int_0^1\ldots\int_0^1e^{-t(x_1+x_2+\ldots+x_n)}dx_1\ldots dx_n\,dt$$ $$\Rightarrow I=\int_0^{\infty}\int_0^1e^{-t(x_1)}dx_1\int_0^1e^{-t(x_2)} \, dx_2 \ldots\int_0^1e^{-t(x_n)}dx_n\,dt$$ $$\Rightarrow I=\int_0^\infty \left ( \frac{1-e^{-t}}{t}\right )^n\,dt$$
so our lim should be
$$\Rightarrow \lim_{n \to \infty} n\int_0^{\infty} \left (\frac{1-e^{-t}} t \right )^n \, dt$$ and with some steps : $$\Rightarrow \lim_{n \to \infty} 2n\int_0^\infty e^{-tn}\left (\frac{\sinh t} t \right )^n\,dt$$
using mathematica i got numerically that the result is $2$. i could see that $$\lim_{n \to \infty} \int_1^\infty e^{-tn}\left (\frac{\sinh t}{t}\right )^n\,dt=0$$ so how to prove that $$\lim_{n \to \infty} n\int_0 ^{1} e^{-tn}\left (\frac{\sinh t}{t}\right )^n\,dt=1$$","['improper-integrals', 'integration', 'calculus', 'limits']"
2333731,"Convergence rate for $\int_{\{f\ge n\}}|f(x)|\,dx$ for $f\in L^1(\Omega)$","Let $\Omega\subset\mathbb R^d$ be open and bounded and $f\in L^1(\Omega)$. Consider the following sequence indedxed by $n\in\mathbb N$:
$$ \int_{\{f\ge n\}}|f(x)|\,dx .$$
This sequence converges to zero by the Beppo Levi-theorem. 
My question: Can one find an explicit convergence rate which is independent of $f$? More specifically, I'm looking for something along the lines of
$$\int_{\{f\ge n\}}|f(x)|\,dx\leq C_f n^{-\alpha},$$ 
with some $\alpha>0$ and a constant $C_f$ which is allowed to depend on $f$.","['lp-spaces', 'integration', 'measure-theory', 'convergence-divergence', 'analysis']"
2333732,Jacobian of a diffeomorphism $\phi:\mathbb{S}^2\to\mathbb{S}^2$,"I'm trying to understant exercise $(4)$, chapter $5$, from Montiel-Ros's Curves and Surfaces to which there is a hint at the end of the chapter: Hint: I've understood everything from the hint, except for the last two lines. More precisely, this equality:
$$|\text{Jac}(\phi)|(p)=\frac{|\det(Ae_1\,Ae_2\,Ap)|}{|Ap|^3}$$ How did he conclude this from the expression of $(d\phi)_p(v)$?","['smooth-manifolds', 'differential-geometry', 'surfaces']"
2333813,Elementary proof of a cotangent inequality,"Let $0<x<\pi/2$. Then
  $$ \cot{x} > \frac{1}{x}+\frac{1}{x-\pi}. $$ (This is still true for $-\pi<x<0$, but the given range is the one I'm concerned about.) Is there an elementary proof of this? The local inequality $\cot{x}<1/x$ is easy to prove since it is equivalent to $x<\tan{x}$, which even has a simple geometric proof. One can massage the Mittag-Leffler formula
$$ \cot{x} = \frac{1}{x} + \sum_{n \neq 0} \frac{1}{x-n\pi} + \frac{1}{n\pi}  $$
into
$$ \cot{x} = \frac{1}{x} + \frac{1}{x-\pi} + \sum_{n=1}^{\infty} \frac{1}{z- (n+1)\pi}+\frac{1}{z+\pi n}, $$
and the terms in the second sum are all positive in the range considered since
$$ \frac{1}{z- (n+1)\pi}+\frac{1}{z+\pi n} = \frac{\pi-2z}{n(n+1)\pi^2+z(\pi-z)}>0, $$
but this is rather heavyweight for such a simple-looking inequality. An equivalent formulation is
$$ \tan{y} > \frac{1}{\pi/2-y}-\frac{1}{\pi/2+y} = \frac{8y}{\pi^2-4y^2} $$
for $0<y<\pi/2$, if one desires more symmetry.","['inequality', 'trigonometry', 'calculus']"
2333827,How to write a vector with set notation?,"I have a vector $\mathbf{a}=(a_1,a_2,a_3)$, where $a_1,a_2,a_3$ are real numbers. I now want to write that $\mathbf{a}$ is a vector in $\mathbb{R}^3$ and that $a_1,a_2,a_3$ are real numbers. What is the proper notation for this? Is it correct to write
$$
A=\big\{\mathbf{a}=(a_1,a_2,a_3)
\in\mathbb{R}^3:a_1\in\mathbb{R}, a_2\in\mathbb{R} \text{ and } a_3\in\mathbb{R}
\big\} \quad \text{?}
$$ Or something else?","['linear-algebra', 'elementary-set-theory', 'vector-spaces']"
2333835,Differential Vector Element,"I am a little confused about differential element notations. Say we have a function $f$ defined on a 3D space. Would the following notations be identical? $$\int f(\vec{x}) \, d\vec{x}$$ $$\int f(x_{1},x_{2},x_{3}) \, d^3x$$ Intuitively the equivalence makes sense: if we sum over all combinations of $x_{1},x_{2},x_{3}$, we sum over all possible vectors. But notation wise how can we convert $d\vec{x}$ to $d^3x$. In certain cases would it be better to use one notation over the other?","['multivariable-calculus', 'indefinite-integrals', 'integration', 'calculus']"
2333847,Quick Maths grammar question - How to write a domain of $x$?,"A function $f(x) = k$ and the domain is $\{-2,-1,\dotsc,3\}$. Would I say 
$$x = \{-2,-1,\dotsc,3\}\quad\text{or}\quad x \in \{-2,-1,\dotsc,3\} \ ?$$
Thanks.","['algebra-precalculus', 'terminology', 'functions', 'discrete-mathematics']"
2333851,"Let $A$, $B$ and $C$ be complex matrices such that $C\neq 0, $ $AC=CB$. Prove that $A$ and $B$ have a common eigenvalue. [duplicate]","This question already has answers here : There exists $C\neq0$ with $CA=BC$ iff $A$ and $B$ have a common eigenvalue (5 answers) Closed 7 years ago . Let $A$, $B$ and $C$ be complex matrices such that $C\neq 0, $ $AC=CB$. Prove that $A$ and $B$ have a common eigenvalue. There is a hint in the question, these facts can be used for the prove: For a complex matrices $A, B$, If $AB = 0$, and $B$ is invertible, $A = 0$. For a complex matrices $A, B$ and $C$, if $AB = BC$ than for each natural number $k$, $\\\\A^kB = BC^k $. Any ideas?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2333892,What is the function represented by the power series $\sum_{k=1}^\infty \frac{k(x-1)^k}{3^k}$?,"Find the function represented by the power series $$\sum_{k=1}^\infty \frac{k(x-1)^k}{3^k}.$$ I know that the interval of convergence for the power series is $(-2, 4)$, would that help much?  I also know that the power series can be written as $$\sum_{k=0}^\infty k y^k,$$ where $y=\frac{x-1}{3}$.  Would that help?","['sequences-and-series', 'calculus', 'functions', 'functional-analysis', 'power-series']"
2333914,Computing the partial trace of a matrix,"I'm trying to understand the Wikipedia article on the partial trace. I do not understand their notation for the matrix elements of a tensor product of operators; that is, the object $\{a_{k l,ij}\}$ in the first section under the heading ""details"" at this link. I think a simple example will make this clear. Let $$A = \left[ \begin{array}{cc}1 & 2\\ 3 & 4 \end{array}\right], \quad B= \left[ \begin{array}{cc}8 & 9\\ 7 & 6 \end{array}\right].$$
And also let these matrices act on vector spaces $V = \mathbf{R}^2$ and $W = \mathbf{R}^2$ respectively. Then
$$A \otimes B = \left[\begin{array}{cccc}8 & 9 & 16 & 18 \\ 7 & 6 & 14 & 12 \\ 24 & 27 & 32 & 36 \\ 21 & 18 & 28 & 24\end{array} \right] (""="" \{a_{k l,ij}\}).$$
Now clearly from the definitions of $A$ and $B$ we can tell that
$$\operatorname{Tr}_W (A \otimes B ) = \operatorname{Tr}(B)\,A = 14\times \left[ \begin{array}{cc}1 & 2\\ 3 & 4 \end{array}\right] = \left[ \begin{array}{cc}14 & 28\\ 42 & 56 \end{array}\right](""="" \{b_{ki}\})$$ But I would like to see how this could be obtained directly from the matrix representation of $A \otimes B.$ In particular, how am I supposed to interpret the 4-index object $\{a_{k l,ij}\}$ in the wikipedia article here? How do I compute $b_{ki} = \sum_j a_{kj,ij}$? Thanks! Edit: I thought the solution was that: In fact the object $\{a_{kl,ij}\}$ is a two-index object, as we should expect it to be. The subscript contains products of the numbers $k, l$ and $i, j$. The comma separates the two indices. But this cannot be true. Because using this, we compute the off-diagonal element $b_{12} = a_{12} + a_{24} = 19 \neq 28$. We instead want the off-diagonal element $b_{12} = a_{13} + a_{24} = 28,$ but I do not see how the prescription from wikipedia furnishes this. Perhaps I am making some very simple arithmetic error.","['matrices', 'linear-transformations', 'trace', 'index-notation', 'linear-algebra']"
2333944,Can one recover $G$ from $G/H$?,"I was thinking about a discussion that took place in the comments of some question on this site about quotient groups and isomorphic copies of these. One of the persons mentioned that in $G/H$ you have more information than in $K$($\simeq G/H$) because in $G/H$ you still have information about $G,H$. Indeed, you can recover $H$ as the unit of the group $G/H$ and taking the union, one has $G=\bigcup G/H$ . So set-theoretically, ""$(G,H) \to G/H$"" is injective (I put quotemarks there because $G/H$ is not only a function of the sets $G,H$ but also of multiplication on $G$). My question is the following : can you recover the group $G$ knowing the group $G/H$ ? (That is,also recover the multiplication, not only the underlying set) I guess a precise way to ask the question is : let $C$ be the category of couples $(G,H)$ where $G$ is a group (from here on, this means $G = (E, \cdot)$ where $\cdot$ satisfies the group axioms) and $H$ is a normal subgroup (here $H$ is only the underlying set though) and of group morphisms that preserve the normal subgroup ($f : (G,H) \to (K,N)$ has $f(H)\subset N$) and let $F$ be the functor $C\to \bf{Grp}$ that sends $(G,H)$ to $G/H$ and $f : (G,H) \to (K,N)$ to the canonically associated $\overline{f} : G/H \to K/N$. Is $F$ injective on objects ? (Though the functorial analysis here isn't necessary, it seemed like a good way of phrasing it) This says precisely this : given $G/H$ I know $G, H$, but do I know the multiplication ? EDIT: As pointed out in the comments, and in Hagen Von Eitzen's answer, the ""trivial case"" $G/G$ shows that the question as it now stands is trivial. But a more interesting (I hope) question comes out of it: for which normal subgroups can we recover $G$, or in a more global approach, for what subcategories $D$ of $C$ is the restriction of $F$ to $D$ injective on objects ? For instance, the (full) subcategory of couples $(G,\{e\})$ has this property (as knowing what $\{a\}\{b\} = a\{e\} b\{e\}$ is in $G/{e}$ tells you what $ab$ is).","['category-theory', 'group-theory']"
2333958,Finding Variance from a joint moment generating function,"The random vars X and Y have, for all real values of $T_1, T_2$, the joint mgf $M(T_1 , T_2) = \frac{1}{2} e^{T_1 +T_2} + \frac{1}{4} e^{2T_1 +T2} + \frac{1}{12}e^{T_2} + \frac{1}{6} e^{4T_1 +3T_2}$ I'm looking for V[X], but I believe that I am overlooking a way to simplify this problem. I know that variance is the second moment or the second derivative of the mgf,but I am not sure how to generate these derivatives and what to do after, given that it is a bivariate equation. Any pointers would be appreciated.","['statistics', 'variance', 'moment-generating-functions', 'bivariate-distributions']"
2333967,"Finding minima and maxima to $f(x,y) = x^2 + x(y^2 - 1)$ in the area $x^2 + y^2 \leq 1$","I'm asked to find minima and maxima on the function 
$$f(x,y) = x^2 + x(y^2 - 1)$$
in the area 
$$x^2 + y^2 \leq 1.$$ My solution: $$\nabla (f) = (2x + y^2 - 1, 2xy)$$
Finding stationary points 
$$2xy = 0$$
$$2x + y^2 - 1 = 0$$
gets me $(0,1),(0,-1),(\frac{1}{2},0)$. Stuyding the boundary: 
$$x^2 + y^2 = 1$$
$$y^2 = 1 - x^2 $$
$$f(x,y) = f(x, 1-x^2) = x^2 -x^3$$
Finding stationary points on the boundary:
$$f'(x,y) = 2x - 3x^2 = 0$$
gives $(0, 1), (0,-1), (\frac{2}{3}, \sqrt{\frac{5}{9}}),(\frac{2}{3}, -\sqrt{\frac{5}{9}})$. So in total i've got the stationary points 
$$(\frac{1}{2} , 0) | (0,1)| (0,-1)| (\frac{2}{3},\sqrt{\frac{5}{9}})|(\frac{2}{3}, -\sqrt{\frac{5}{9}}).$$
which give the function values of: 
$$-\frac{1}{4}, 0,0,0.15,0.15$$.
which gives the minima: $-\frac{1}{4}$ and maxima $0.15$.
The minima is correct but the maxima should be 2. Why? What stationary point am I missing?","['multivariable-calculus', 'analysis']"
2333976,"When all of $n-2$, $n+2$, $n^2-2$, and $n^2+2$ are primes?","Find the smallest $5$-digit number $n$ for which all of numbers $n-2$, $n+2$, $n^2-2$, and $n^2+2$ are primes. Actually after some calculations, I realized that it should be solved using a computer. But it would be nice if you could provide a way to find this number, mathematically.","['number-theory', 'prime-numbers', 'diophantine-equations', 'elementary-number-theory']"
2334018,Sitting in chairs with empty space,"There are n chairs in a row. In how many ways can a teacher sit k students on these chairs so that no 2 students sit next to each other (and obviously no 2 students sit on 1 chair)? I was thinking about inclusion-exclusion, that each student will sit next to each other, however it isn't good enough since there could be more chairs than students. 
Basically currently I have no idea how to solve it.","['combinatorics', 'discrete-mathematics']"
2334045,Familiarizing with the Fundamental Theorem of Calculus,"Consider the following form of the Fundamental Theorem of Calculus:
""Let $f:[a,b] \rightarrow \mathbb{R}$ be a differentiable function. Suppose that $F'$ is Riemann integrable over $[a,b]$. Then $\int^{b}_{a}F'=F(b)-F(a)$."" In order to familiarize with this theorem, it must be remarked that there is no condition for the integrand to be continuous. Now I am looking for a concrete example of a discontinuous, but integrable integrand $f$ which is the derivative of a differentiable function $F$. I came up with $f:[0,2]\rightarrow \mathbb{R}:x \mapsto\begin{cases}1 \text{ if } 0\leq x \leq 1 \\ 2 \text{ if }1<x \leq2\end{cases}$. This integrand is discontinuous, but it is the derivative of a function which is not everywhere differentiable on $[0,2]$ (i.e. on $1$). Can someone give a better example? Thank you!","['derivatives', 'integration']"
2334111,Proof of the Jordan Holder theorem from Serge Lang,"Why is there precisely one index $j$ such that $G_i/ G_{i+1} = G_{ij}/G_{i,j+1}$? How does the conclusion follow?","['abstract-algebra', 'group-theory']"
2334115,"Question about transportation-entropy inequality (From Villani's book: Optimal Transport, Old and New)","I was reading Villani's book: Optimal Transportation, Old and New. From page 80-83, he introduced some results about dual formulation of transport inequality. Assume $C(\mu,\nu)$ is the optimal transport distance from probability measure $\mu$ (defined on $\mathcal{X}$) to $\nu$ (defined on $\mathcal{Y}$), with cost function $c(\cdot,\cdot)$. Given a convex functional $F(\cdot)$ defined on $P(\mathcal{X})$, we can define its Legendre Transformation : $L(F)=\Lambda$, so $\Lambda$ is a convex functional on $C_b(\mathcal{X})$. The main result (Theorem 5.26) is:
\begin{equation*}
\forall \mu \in P(\mathcal{X}), C(\mu,\nu)\leq F(\mu)
\end{equation*}
and
\begin{equation*}
\forall \phi \in C_b(\mathcal{Y}), \Lambda(\int_\mathcal{Y}\phi d\nu-\phi^c)\leq 0\quad \phi^c:=\sup_{y\in\mathcal{Y}}(\phi(y)-c(x,y))
\end{equation*}
Are equivalent. Well this result is used to analyze transport inequalities. In Example 5.29, the author gives an important application of this result: Assume $\mathcal{Y}=\mathcal{X}$ and consider: 
\begin{equation*}
F(\mu):=KL(\mu||\nu)=\int_{\mathcal{X}}\ln(\frac{d\mu}{d\nu})d\mu
\end{equation*}
which is the Kullback Liebller Divergence between measures (also known as relative entropy);
We could compute the Legendre Transformation of $F$, which has the following form:
\begin{equation*}
\Lambda(\phi):=\ln(\int_{\mathcal{X}}e^{\phi}d\nu)
\end{equation*}
Thus by the previous result, the transportation-entropy inequality 
\begin{equation*}
C(\mu,\nu)\leq KL(\mu||\nu) \quad \forall \mu\in P(\mathcal{X}) \quad (1)
\end{equation*}
is equivalent to :
\begin{equation}
\ln(\int_{\mathcal{X}}e^{\int \phi d\nu-\phi^c}d\nu)\leq 0
 \Leftrightarrow e^{\int_{\mathcal{X}}\phi d\nu}\leq (\int_{\mathcal{X}}e^{-\phi^c}d\nu)^{-1} \quad (2)
\end{equation}
But in the book, the author directly arrives at:
\begin{equation}
e^{\int_{\mathcal{X}}\phi d\nu}\leq \int_{\mathcal{X}}e^{\phi^c}d\nu \quad (3)
\end{equation}
If we apply Cauchy inequality, we will deduce from (1) to (3):
\begin{equation*}
e^{\int_{\mathcal{X}}\phi d\nu}\leq (\int_{\mathcal{X}}e^{-\phi^c}d\nu)^{-1}\leq \int_{\mathcal{X}}e^{\phi^c}d\nu 
\end{equation*}
But how could we deduce from (3) back to (1)? I am quite confused about it. I even think that $(3)$ and $(1)$ are equivalent is not a trivial corollary from our previous Theorem 5.26. Can any expert in this area help me with this problem? So many thanks!","['functional-analysis', 'functional-inequalities', 'optimal-transport', 'probability']"
2334133,What is the intuition for using sigma algebras to define probability spaces?,"When I was taught probability in school, we studied basic events like coin flips $\lbrace H,T\rbrace$ or rolling a die $\lbrace 1,2,3,4,5,6\rbrace$. Let's take coin flips as an example. Suppose $\Omega =\lbrace H,T\rbrace$.
$\mathcal{F} =\lbrace \emptyset,\lbrace H\rbrace,\lbrace T\rbrace, \lbrace H,T\rbrace \rbrace$. Then $\emptyset,\Omega \in \mathcal{F}$ Closed under complementation (e.g. if $A = \lbrace H\rbrace$ then $A^c = \lbrace T\rbrace$ and both are in $\mathcal{F}$) Closed under countable unions (e.g. $A = \emptyset \cup \lbrace H\rbrace = \lbrace H\rbrace \in \mathcal{F}$) So I get how it works in these simple examples. But I don't get why all this fancy terminology is necessary. Why do I need this elaborate structure to be able to describe a coin flip? Isn't it enough just to have the event set $\Omega$ and a function $\Bbb P$ that can map each event onto a number? Why do I need the sigma algebra?","['probability-theory', 'probability']"
2334281,"Given a joint moment generating function, find $P(X<Y)$","Let random variables $X$ and $Y$ have joint MGF:
$$M(t_1,t_2) = 1/2e^{t_1+t_2} + 1/4e^{2t_1+t_2} + 1/12e^{t_2} + 1/6e^{4t_1+3t_2}$$  I now need to find $P(X<Y)$. I know how to find the moments as I had to find $V[X]$ for part of this problem, but I am not quite sure how to figure out probability from this.","['expectation', 'statistics', 'probability', 'moment-generating-functions', 'random-variables']"
2334295,What happens to woodbury matrix identity when A is not invertible?,"The Woodbury matrix identity is
\begin{equation}
(A+UCV)^{-1}=A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}.
\end{equation}
This formula suppose that $A$, $(A+UCV)$ and $(C^{-1}+VA^{-1}U)$
are invertible. What happens to this formula when $A$ is not invertible
please? Do you know a formula for that case please? I have read the
paper "" A Sherman Morrison Woodbury
Identity for Rank Augmenting Matrices with Application to Centering ""
of Kurt S. Riedel but his formula looks so complicated. Thanks.","['matrices', 'pseudoinverse', 'matrix-decomposition', 'inverse']"
2334300,Ratio of a regular polygon's radius to its perimeter in terms of number of sides,"how would I derive an equation in which y represents the ratio of a regular polygon's diameter to its radius, and x represents the number of sides of the regular polygon? I'm thinking that as x goes to infinity, y should approach 2pi. Please tell me if I'm misunderstanding something basic. Thanks!","['circles', 'pi', 'polygons', 'geometry']"
2334319,Fundamental Theorem of Calculus and discontinuous functions,"Will the Fundamental Theorem of Calculus be useful in evaluating: $$y=\int_{0.5}^{1} \left \lfloor t \right \rfloor dt?$$ The Riemann sums make this evaluation easy. I, however, would like to know how( if possible), can I use the theorem to evaluate this definite integral. Here, I assumed that $y$ is continuous in $[0.5,1]$ and differentiable in $(0,1)$. Edit: To be clear about what I wish to know, I ask if I could evaluate the definite integral just by taking the difference, $y(1)-y(0.5)$( second part of the Fundamental Theorem of Calculus), where $y$ is the antiderivative of $\left \lfloor  x\right \rfloor$. I'm not sure if I'm allowed to talk about antiderivatives of discontinuous functions. Am I allowed to?",['calculus']
2334335,Examples of $C^1(\mathbb{R}^2)$ functions where mixed partials are equal nowhere,"I know there are examples of functions which are twice differentiable, but the mixed partials $f_{xy}\neq f_{yx}$ at the origin $(0,0)$. Are there any examples of functions where the mixed partials exist, but are equal nowhere ? I know a necessary condition is that the mixed partials are discontinuous. My question can be formally stated in a three parts: For both of these cases, assume the second partial derivatives exist. $1)$ let $f\in C^1(\mathbb{R}^2)$. Do there exists subsets $E \subset\mathbb{R}^2$ where $\mu(E)> 0$ where $f_{xy}\neq f_{yx}$, (where $\mu(E)$ denotes the Lebesgue measure of $E$). Note that having mixed partial $f_{xy}\neq f_{yx}$ at the origin answers this question in the affirmative for $\mu(E)=0$, since if $E=\{(0,0)\}$, then $\mu(E)=0$, and we know of many examples where this occurs. $2)$ let $f\in C^1(\mathbb{R}^2)$. Is it possible for $f_{xy}$ to be continuous on a set $\mu(E)\geq 0, $ and $f_{xy}\neq f_{yx}$ everywhere or at least almost everywhere? $3)$ would answers to these questions be interesting enough to publish as counterexamples or theorems? I was unable to find information about these two cases, let alone generalizations to functions with $f\in C^1(\mathbb{R}^n)$, leading me to believe these could be interesting research questions.","['multivariable-calculus', 'real-analysis']"
2334336,"If $N$ is nilpotent, prove that $\det(I+N)=1$","If $N$ is nilpotent, prove that $\det(I+N)=1$. My attempt: Since $N$ is nilpotent we have $$\det(N)=0.$$ That is, there exists a non-zero vector $v$ such that $$Nv=0.$$ Now consider $$(I+N)v=Iv+Nv=Iv+0=v.$$ Thus $$(I+N)v=v.$$ That is, $1$ is an eigenvalue of $I+N$. With this, how can we say $\det(I+N)=1$?","['matrices', 'linear-algebra', 'determinant']"
2334355,Does the solution of differential equations exist for all times?,"I was trying to analyse the differential equation $\frac{dx}{dt} = -x + x^{3} , x(0) = x_{0}$,After solving I got the solution as $- \ln(|x|) + \frac{1}{2}\ln|x^2 - 1 | = t + (-\ln|x_{0}| + \frac{1}{2}\ln|x_{0}^2 - 1|)$,now by doing computational analysis I see that solution $x(t) $ diverges to infinity when the initial condition is $>1$ or $<1$ and the solution decreases and attains a steady stste when $x_0{ \in [-1,1]}$. But how do I prove analytically that when $x_{0} > 1$,$x_{0}<-1$,the solution diverges to infinity! and attains a steady state when $-1<x(0)<1$? And I think for negative time $t$,the solution exist for finitely many $t$,how can i intuitively think of this? Any help is great!","['ordinary-differential-equations', 'dynamical-systems']"
2334363,What is the difference between linearly and affinely independent vectors?,What is the difference between linearly and affinely independent vectors? Why does affine independence not imply linear independence necessarily? Can someone explain using an example?,"['convex-geometry', 'affine-geometry', 'linear-algebra', 'vectors', 'vector-spaces']"
2334372,Unlikely equation involving combinations,"I was playing with my Python implementation of some combinatorial functions, specifically those that return the number of combinations of $r$ elements from a set of cardinality $n$: Without repetition: $\frac{n!}{r!(n-r)!}$ With repetition: $\frac{(n+r-1)!}{r!(n-1)!}$ For convenience, I will refer to the former function as $C(n, r)$, and the latter as $Cr(n, r)$. While doing this, I found something fascinating: $$\lim_{a\to\infty} {Cr(a^2, a) \over C(a^2, a)} = e$$ Those who know Python can verify the correctness of my implementation of the functions: def permutation_count(n, r, repetitions=False):
    """"""Returns the number of permutations of 'r' elements from a set of
    cardinality 'n'.""""""
    if repetitions:
        return n**r
    else:
        return reduce(lambda x, y: x * y, range(n, n - r, -1))

def combination_count(n, r, repetitions=False):
    """"""Returns the number of combinations of 'r' elements from a set of
    cardinality 'n'.""""""
    if repetitions:
        n = n + r - 1
    return permutation_count(n, r) // factorial(r)

# My function:
def f(a):
    return combination_count(a**2, a, True) / combination_count(a**2, a, False) I observed the above result by simply evaluating this function for increasingly large integer values. I will send any who request it a text file containing the results of the function for said values. I posted this here for these reasons: 1. There's the possibility, however small, that I am the first to discover this equation. 2. I enjoy math and I'm fairly knowledgeable regarding it, but I'm definitely not an expert, and I have no idea what the significance of this discovery is. Someone more knowledgeable than myself might consider it trivial, as it may logically proceed from some known mathematical property. 3. If this discovery is in fact significant, it could be useful to those more knowledgeable than myself. I would also like to understand why it is significant, in that case.","['combinatorics', 'exponential-function', 'limits']"
2334435,Let $M$ be a Riemannian manifold. When does the Laplace-Beltrami operator differ from the trace of the Hessian?,"Let $(M^n,g)$ be some Riemannian manifold, and let $(x^1,\ldots,x^n)$ be local (oriented) coordinates. Then we may define the Laplace-Beltrami operator $\Delta$ on a function $f\in C^\infty(M)$ by setting $$\Delta f = d^* df = -\operatorname\ast d\operatorname\ast df = -\frac{1}{\sqrt{\det(g_{ij})}}\sum_{j=1}^n\frac{\partial}{\partial x^j}\left(\mathrm{grad}^j\, f\sqrt{\det(g_{ij})}\right)$$ while we can also consider minus the trace of the Hessian,
$$ -\operatorname{tr}\operatorname{Hess}(f) = -\operatorname{tr}\nabla^2 f = -g^{ij}\left(\frac{\partial^2 f}{\partial x^i\partial x^j} - \Gamma_{ij}^k\frac{\partial f}{\partial x^k}\right).
$$
Now, on flat space, these are exactly identical. But are they the same in general? They both seem to be fairly natural generalizations of the notion of the Laplacian on $\Bbb R^n$.","['riemannian-geometry', 'differential-geometry']"
2334447,Area of Intersection of Three Circles,"I guess I'm just too dumb to solve this PSAT problem -- so what I tried for this is to break down the shaded area into a difference of sectors and triangles. I want to utilize the area of the 20 degree sector, and I have a hunch that all of this can be achieved without trigonometry.  I don't see a way to utilize that Q is the midpoint of PR. I've basically gone nowhere on this problem, and all help is appreciated.",['geometry']
2334473,New proof that compact subsets in metric spaces are closed?,"Let $\mathcal{M}$ be a metric space, with $\mathbb{A}\subseteq\mathcal{M}$ compact. Suppose that $\ell\in\mathcal{M}$ is a limit point of $\mathbb{A}$, and that $\ell\notin\mathbb{A}$; further, let $\{\overline{\mathbb{B}}_{r_\alpha}(\ell)\}_{\alpha<\omega}$ be a sequence of closed balls centered at $\ell$ such that $\lim_{\alpha\rightarrow\omega}r_\alpha=0$, so $\lim_{\alpha\rightarrow\omega}\overline{\mathbb{B}}_{r_\alpha}=\{\ell\}$. We then have that $\{\mathcal{M}\sim\overline{\mathbb{B}}_{r_\alpha}(\ell)\}_{\alpha<\omega}$ is an open cover of $\mathbb{A}$ with no finite subcover, since $\forall\alpha\big(\mathbb{A}\cap\overline{\mathbb{B}}_{r_\alpha}(\ell)\neq0\big)$, a contradiction since $\mathbb{A}$ is compact. We thusly conclude that $\ell\in\mathbb{A}$, thus $\mathbb{A}$ is closed. I was just wondering if this proof appeared in literature somewhere -- I liked it because it makes the interplay between the notion of a limit point and compactness in a metric space apparent. EDIT: To be clear, $\omega$ is the first transfinite ordinal, and $\alpha<\omega\implies\alpha$ is a natural number.","['general-topology', 'metric-spaces']"
2334479,"If $X \sim N(\mu, \sigma^2)$, and $Y$ is another random variable, is it an abuse of notation to write $p(X|Y, \mu, \sigma^2)$?","Suppose that $X$ is a Normal random variable such that: $$
X \sim N(\mu, \sigma^2)
$$ Then, it is common to write: $$
p(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-(x-\mu)^2}{2\sigma^2}\right)
$$ Here, it seems that we are using a conditional notation, but we are conditioning on parameters. Now suppose that: $Y \sim N(\mu_y, \sigma_y^2)$. If we wrote: $$
p(X|Y,\mu,\sigma^2)
$$ Would this still be valid? I have seen some books combine parameters and random variables in the conditional, but am not sure if this was an abuse of notation or something valid. What is the correct consensus here? Thanks.","['probability-theory', 'probability']"
2334497,Find all functions of the form $f(x)=\frac{b}{cx+1}$ where $f(f(f(x)))=x .$,"This is a very interesting word problem that I came across in an old textbook of mine. So I know its got something to do with functions and polynomials, but other than that, the textbook gave no hints really and I'm really not sure about how to approach it. Any guidance hints or help would be truly greatly appreciated. Thanks in advance :) So anyway, here the problem goes: Find all functions of the form $$f(x)=\frac{b}{cx+1}$$ where $b,c$ are integers and, for all real numbers $x$ such that $f(f(f(x)))$ is defined, the following equation holds: $$f(f(f(x)))=x .$$ So I tried to make the substitutions yielding: $$\frac{b}{c(\frac{b}{c(\frac{b}{cx+1})+1})+1}=x.$$ and then I simplified to: $(bc+1)(cx^2+x-b)=0$ . However, I'm not sure if this is correct, and even so, I'm not sure how to approach the problem from here.","['polynomials', 'functions', 'elementary-number-theory']"
2334550,"What is the probability that a permutation of the numbers $\{1,2,3, \dots, n\}$ has at least two fixed points?","Regarding this question: What is the probability that a randomly selected permutation of $\{1,2,3, \dots, n\}$ has at least two indices $j$ such that $\pi(j)=j$? I though about the following solution: First pick those two numbers to be fixed points, there's $\binom{n}{2}$ options for that. Then, multiply that with the number of permutations of $(n-2)$ numbers, and finally divide by the probability space.
Then: $$ Pr(A)=\frac{\binom{n}{2}(n-2)!}{n!} = \frac{1}{2}$$ This is the correct answer.
But then I thought: doesn't it count the same permutations more than one time? (double counts). Let's examine the following example: I chose the numbers $1$ and $2$ to be the fixed points, and let the others arrange.
One of the arrangements is $1,2,3,4,5$.
In a different cast, I choose $4$ and $5$ and let the others arrange, and once again counted $1,2,3,4,5$ as a valid option. How does it settle? Thanks","['permutations', 'probability', 'discrete-mathematics']"
2334554,Prove the sequence of general term $\sum\limits_{k=1}^{2n+1} \frac{(-1)^{k+1}}{k}$ converges,"Given the sequence $$a_n :=\sum_{k=1}^n \frac{(-1)^{k+1}}{k}$$
  Prove the sequence $(a_{2n+1})_{n \geq 0}$ converges. My thoughts I've proven that the sequence $(a_{2n+1})_{n \geq 0}$ is monotone decreasing. So now I want to prove that the sequence is bounded below, since then I can prove the sequence converges.
I wanted to prove the sequence is bounded below by induction, however I'm not quite sure how.
Since $$a_{2n+1} :=\sum_{k=1}^{2n+1} \frac{(-1)^{k+1}}{k}= 1 -\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+...$$
I think it is bounded by $\frac{1}{2}$. Using induction: For $n=0$, we know $a_{2n+1} = a_1 = 1 \geq\frac{1}{2}$ Suppose that $a_{2n+1} \geq \frac{1}{2}$. Prove that $a_{2(n+1)+1)} = a_{2n+3} \geq \frac{1}{2}$.
I know that $$a_{2n+3} :=\sum_{k=1}^{2n+3} \frac{(-1)^{k+1}}{k} = a_{2n+1} -\frac{1}{2n+2} + \frac{1}{2n+3} = a_{2n+1} -\frac{1}{(2n+2)(2n+3)}$$ But I don't know how to proceed. Any help?","['induction', 'sequences-and-series']"
2334570,How can you calculate distance in plane polar coordinates using the metric tensor,"I'm trying to develop some intuition about the metric tensor and how it can be used to calculate distances/angles in curvlinear space by using the very simple example of a 2D polar surface. The metric tensor for such a surface is \begin{bmatrix} 1 & 0 \\ 0 & r^2 \end{bmatrix} For an arbitrary vector $\vec{a}=r_1\hat{r}+\theta_1\hat{\theta}$ the square length of the vector can be calculated as 
$$ \begin{bmatrix} r_1 & \theta_1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & r^2 \end{bmatrix} \begin{bmatrix} r_1 \\ \theta_1 \end{bmatrix} $$ Which gives me the answer: 
$\lvert\lvert\vec{a}\rvert\rvert^2 = r_1^2+r^2\theta_1^2$ Shouldn't it just give me the value $r_1^2$? Furthermore if I try and calculate the ""dot-product"" using this tensor for two arbitrary vectors $\vec{a}=r_1\hat{r}+\theta_1\hat{\theta}$ and $\vec{b}=r_2\hat{r}+\theta_2\hat{\theta}$, I get the result $\vec{a} \cdot \vec{b} = r_1r_2+r^2\theta_1\theta_2$. Even if I set $r=0$, I get the result $\vec{a} \cdot \vec{b} = r_1r_2$, which tells me that my vectors are parallel. That shouldn't be the necessarily the case since they are arbitrary vectors. Please, pardon my lack of mathematical formalism, just a lowly physics major trying to self study tensors.","['coordinate-systems', 'tensors', 'differential-geometry', 'vector-analysis']"
2334589,Pairwise disjoint or disjoint?,I am reading Lebesgue measure. In many situations I have found that the author says pairwise disjoint collection of subsets of $\mathbb{R}$ and in some others simply disjoint . What is the difference in Mathematics?,"['terminology', 'elementary-set-theory', 'definition']"
2334603,What is the procedure to prove this result?,"Question : If $\tan\frac{\theta}{2}=\tan^3\frac{\phi}{2}$ and $\tan\phi=2\tan\alpha$, show that $\theta+\phi=2\alpha$. My problem : I cannot understand how to proceed from the given data since the given data involves trigonometric functions while the required result involves angles. May be we have to equate same trigonometric functions and then the respective angles. I would really appreciate it if someone could explain to me the logic and the thought process behind the proof and the approach to prove the result . (I have not posted any attempt because honestly i do not know where to start and how to start)","['proof-writing', 'trigonometry', 'systems-of-equations']"
2334607,The sum of consecutive integers is $50$. How many integers are there?,"I started off by calling the number of numbers in my list ""$n$"". Since the integers are consecutive, I had $x + (x+1) + (x+2)...$ and so on. And since there were ""$n$"" numbers in my list, the last integer had to be $(x+n)$. This is where I got stuck. I didn't know how to proceed because I am not given the starting point of my integers, nor an ending point.","['algebra-precalculus', 'arithmetic-progressions', 'summation', 'integers']"
2334628,Determination of a Joukowski airfoil chord (demonstration),"I'm currently studying Aerodynamics, and one thing that I noticed is that the maximum and minimum $x$ coordinate of the airfoils (which are necessary to compute the chord) on the transformed plane (let it be $z(x,y)$ ) correspond to the transformed intersection points of the circumference on the original plane  (let it be $\zeta(\xi,\eta)$ ) with the real axis. I don't find any proof of this statement, and so I tried to do it by myself. The problem is that it got too complicated to be solved analytically (too many different powers of trignometric functions). So I'm requesting someone to try demonstrate this too. I think that there's a simpler way to do it. There's my introduction to the problem: Consider the original circumference defined on the complex $\zeta$ plane: where $a$ is the circumference radius, and $b$ the intersection of the circumference with the real positive axis, $\xi$ . The parameter $\beta$ is the angle between the horizontal line and the line that links $\zeta_0$ to $b$ . The center of the circumference is: $$\zeta_0=-b\varepsilon+ia\sin(\beta)=-b\varepsilon+i(1+\varepsilon)b\tan(\beta)$$ So, this circumference is defined by: $$\zeta=-b\varepsilon+b\frac{1+\varepsilon}{\cos(\beta)}\left(e^{i\theta}+i\sin(\beta)\right),\hspace{15pt}\theta \in [0,2\pi]$$ Now I need to show that for the Joukowski transform $z=\zeta+\frac{b^2}{\zeta}$ the $x$ coordinate (real coordinate on the plane $z$ ) has
a maximum on $\zeta=b$ (intersection of the circunference with the positive real axis) and a minimum on $\zeta=-b(1+2\varepsilon)$ , (intersection of the circunference with the negative real axis).","['complex-analysis', 'physics', 'optimization', 'transformation']"
2334730,"Prob. 6, Sec. 3.3, in Kreyszig's Functional Analysis Book: What is $Y^\perp$ if $Y = \ldots \subset \ell^2$?","Let $Y$ be the subset of $\ell^2$ given by $$ Y \colon= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N}} \in \ell^2 \ \colon \ \xi_{2n} = 0 \ \mbox{ for all } \ n \in \mathbb{N} \ \right\}. $$ Then $Y$ is a closed subspace of the Hilbert space $\ell^2$ , and so $Y$ is also complete. What is $Y^\perp$ ? Here is the relevant definition: Let $X$ be an inner product space, and let $M$ be a non-empty subset of $X$ . Then the orthogonal complement $M^\perp$ of $M$ is defined as follows: $$ M \colon= \left\{ \ x \in X \ \colon \  \langle x, y \rangle = 0 \ \mbox{ for all } \ y \in M \ \right\}. $$ My effort: By definition, \begin{align}
& Y^\perp \\
&= \left\{ \ x \in \ell^2 \ \colon \ \langle x, y \rangle = 0 \ \forall y \in Y \ \right\} \\
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \left\langle \left( \xi_n \right)_{n \in \mathbb{N} }, \left( \eta_n \right)_{n \in \mathbb{N} } \right\rangle = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in Y \ \right\} \\
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_n  \overline{  \eta_n} = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in Y \ \right\} \\ 
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_n  \overline{  \eta_n} = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \mbox{ such that } \ \eta_{2n} = 0 \ \forall n \in \mathbb{N}  \ \right\} \\
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_{2n-1}  \overline{  \eta_{2n-1} } = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \mbox{ such that } \ \eta_{2n} = 0 \ \forall n \in \mathbb{N}  \ \right\}. 
\end{align} My intuition is that $$Y^\perp = \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \xi_{2n-1} = 0 \ \forall n \in \mathbb{N}  \ \right\}.$$ Is it so? If yes, then how to prove this rigorously? Another approach: As $\ell^2$ is a Hilbert space and as $Y$ is a closed subspace of $\ell^2$ , so $$ \ell^2 = Y \oplus Y^\perp, $$ by Theorem 3.3-4 in the book Introductory Functional Analysis With Applications by Erwine Kreyszig. This means that every $x \in \ell^2$ has a unique representation $$ x = y+z, \ \mbox{ where } \ y \in Y \ \mbox{ and } \ z \in Y^\perp. $$ Now  if $x \colon= \left( \xi_n \right)_{n \in \mathbb{N}} \in \ell^2$ , then, by definition, the series $\sum \left\lvert \xi_n \right\rvert^2$ of non-negative real numbers converges in $\mathbb{R}$ . Therefore, the sequence $$ \left( \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \right)_{n \in \mathbb{N}} $$ of the partial sums of the series $\sum \left\lvert \xi_n \right\rvert^2$ , which is also monotonically increasing, is convergent. Hence this sequence is bounded (from above). That is, for each $n \in \mathbb{N}$ , we have $$ \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \leq \sum_{j=1}^{n+1} \left\lvert \xi_j \right\rvert^2, $$ and $\lim_{n \to \infty} \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2$ exists in $\mathbb{R}$ , and $$ \lim_{n \to \infty} \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 = \sup \left\{ \ \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \ \colon \ n \in \mathbb{N} \ \right\} < +\infty. $$ Now let $y \colon= \left( \eta_n \right)_{n \in \mathbb{N} }$ be ths sequence of (real or complex) numbers defined as follows: For each $n \in \mathbb{N}$ , let $$ \eta_{2n-1} \colon= \xi_{2n-1} \ \mbox{ and } \eta_{2n} \colon= 0. $$ Then the sequence $$ \left( \sum_{j=1}^n \left\lvert \eta_j \right\rvert^2 \right)_{n \in \mathbb{N} } $$ of the partial sums of the series $\sum \left\lvert \eta_j \right\rvert^2$ is given by $$ \sum_{j=1}^{2n-1} \left\lvert \eta_j \right\rvert^2 = \sum_{j=1}^n \left\lvert \xi_{2j-1} \right\rvert^2, $$ and $$ \sum_{j=1}^{2n} \left\lvert \eta_j \right\rvert^2 = \sum_{j=1}^n \left\lvert \xi_{2j-1} \right\rvert^2 $$ for all $n \in \mathbb{N}$ . And, this sequence, being a sequence of sums of non-negative real numbers, is also monotonically increasing and  bounded above by the sum of the convergent series $ \sum \left\lvert \xi_n \right\rvert^2$ . So the sequence $$ \left( \sum_{j=1}^n \left\lvert \eta_j \right\rvert^2 \right)_{n \in \mathbb{N} } $$ is convergent, which means that the series $ \sum  \left\lvert \eta_j \right\rvert^2$ converges. Therefore, $y \colon= \left( \eta_n \right)_{n \in \mathbb{N} }$ is in $\ell^2$ . 
  So from the definition of $Y$ we can conclude that $y \in Y$ . Now let $z \colon= \left( \zeta_n \right)_{n \in \mathbb{N}}$ be the sequence of (real or complex) numbers defined as follows: For each $n \in \mathbb{N}$ , let $$ \zeta_{2n-1} \colon= 0 \ \mbox{ and } \ \zeta_{2n} \colon= \xi_{2n}. $$ Then $x = y+z$ , and so $z = x-y \in \ell^2$ , because $\ell^2$ is closed under addition and scalar multiplication. Another way to show that $z \colon= \left( \zeta_n \right)_{n \in \mathbb{N}}$ is in $\ell^2$ is to show that the series $\sum \left\lvert \zeta_n \right\rvert^2$ of real numbers converges in $\mathbb{R}$ . Now the sequence $$ \left( \sum_{j=1}^n  \left\lvert \zeta_j \right\rvert^2 \right)_{n \in \mathbb{N} }$$ of the partial sums of the series $\sum \left\lvert \zeta_n \right\rvert^2$ is  given by $$ \sum_{j=1}^{2n-1}  \left\lvert \zeta_j \right\rvert^2 = \sum_{j=1}^{2n-2}  \left\lvert \xi_{2j} \right\rvert^2,   $$ and $$ \sum_{j=1}^{2n}  \left\lvert \zeta_j \right\rvert^2 = \sum_{j=1}^{2n}  \left\lvert \xi_{2j} \right\rvert^2   $$ for all $n \in \mathbb{N}$ . Thus this sequence, being a sequence of sums of non-negative real numbers, is monotonically increasing and bounded above by the sum of the series $\sum \left\lvert \xi_n \right\rvert^2$ , and  so this sequence is convergent, which implies that $z \in \ell^2$ . Thus $$ y = \left( \eta_1, \eta_2, \eta_3, \ldots \right) = \left( \xi_1, 0, \xi_3, 0, \xi_5, 0, \ldots \right), $$ and $$ z = \left( \zeta_1, \zeta_2, \zeta_3, \ldots \right) = \left( 0, \xi_2, 0, \xi_4, 0, \xi_6,  \ldots \right),  $$ and so $$ \langle y, z \rangle = \sum_{n=1}^\infty \eta_n \overline{\zeta_n} = 0, $$ and so $\langle y, z \rangle = 0$ also. In fact, we can also show that $\langle z, v \rangle = 0$ for all $v \in Y$ . Thus $z \in Y^\perp$ . But so far I have only been able to show that the subspace $$ Z = \left\{  \ \left( \xi_n \right)_{n \in \mathbb{N} }  \in \ell^2 \ \colon \ \xi_{2n-1} = 0 \ \forall \ n \in \mathbb{N} \ \right\}$$ is contained in $Y^\perp$ . Is what I have done so far correct, logically sound, and rigorous enough? If so, then how to complete the above two arguments? If not, then where do I lack? P.S.: In order to show that $Y$ is indeed closed in $\ell^2$ , let us suppose that $x \colon= \left( \xi_n \right)_{n \in \mathbb{N} }$ is an element in the closure of $Y$ in $\ell^2$ . Then, by Theorem 1.4-6 (a) in Kreyszig, there exists a sequence $\left(y_m \right)_{m \in \mathbb{N} }$ , where $y_m \colon= \left( \eta_{mn} \right)_{n \in \mathbb{N} }$ for each $m \in \mathbb{N}$ , converging to $x$ in $\ell^2$ . For each $m \in \mathbb{N}$ , as $y_m = \left( \eta_{mn} \right)_{n \in \mathbb{N} } \in Y$ , so we must have $\eta_{m, 2n} = 0$ for all $n \in \mathbb{N}$ , that is, we must have $$ \eta_{m2} = \eta_{m4} = \eta_{m6} = \cdots = 0. $$ Now let us choose an arbitrary natural number $N$ . We show that the sequence $\left( \eta_{mN} \right)_{m \in \mathbb{N} }$ converges (in $\mathbb{R}$ or $\mathbb{C}$ as the case may be) to the number $\xi_N$ . As the sequence $\left(y_m \right)_{m \in \mathbb{N} }$ converges in $\ell^2$ to the point $x$ , so given any real number $\varepsilon > 0$ , there exists a natural number $M$ such that $$ d_{\ell^2} \left( y_m, x \right) < \varepsilon $$ for all $m \in \mathbb{N}$ such that $m > M$ . 
  But we have $x = \left( \xi_n \right)_{n \in \mathbb{N} }$ , and, for each $m \in \mathbb{N}$ , we have $y_m = \left( \eta_{m n} \right)_{n \in \mathbb{N} }$ . So we can conclude that $$ \sqrt{ \sum_{n = 1}^\infty \left\lvert \eta_{m n} - \xi_n \right\rvert^2 } < \varepsilon $$ for all $m \in \mathbb{N}$ such that $m > M$ . And, so we obtain $$ \left\lvert \eta_{mN} - \xi_N \right\rvert = \sqrt{ \left\lvert \eta_{mN} - \xi_N \right\rvert  } \leq \sqrt{ \sum_{n = 1}^\infty \left\lvert \eta_{mn} - \xi_n \right\rvert^2 } < \varepsilon $$ for all $m \in \mathbb{N}$ such that $m > M$ . Thus it follows that the sequence $\left( \eta_{m N} \right)_{m \in \mathbb{N} }$ converges to $\xi_N$ . Now since for each $N \in \mathbb{N}$ , we have $$ \xi_N = \lim_{m \to \infty} \eta_{m N}, $$ and since for each $N \in \mathbb{N}$ , we have $\eta_{m, 2N} = 0$ , 
  therefore we must have $$ \xi_{2N} = 0$$ for all $N \in \mathbb{N}$ , which shows that $x = \left( \xi_{n} \right)_{n \in \mathbb{N} }$ is in $Y$ . But this $x$ was an arbitrary element of the closure of $Y$ in $\ell^2$ . Thus it follows that $\mbox{Cl}(Y) \subset Y \subset \mbox{Cl}(Y)$ . Hence $Y$ is closed.","['real-analysis', 'hilbert-spaces', 'functional-analysis', 'inner-products', 'analysis']"
2334732,Is stabilizer subgroup Normal,"Let $G\le S_n$, and set $\Omega = \{1,2,3,\ldots,n\}$. Let $\Delta \subseteq \Omega$, Stabilizer subgroup of $\Delta$ is defined as $$Stab_{\Delta} = \{g \in G  \mid \Delta^g = \Delta \}$$ Is $Stab_{\Delta}$ Normal subgroup of $G$? My Attempt : 
To prove normal we need to show that if any $h\in Stab_{\Delta} $ then $ghg^{-1} \in Stab_{\Delta}$ $$x_i^g = x_j$$
$$(x_i^g)^h = (x_j)^h $$
$$(x_i^{gh})^{g^{-1}} = (x_{l})^{g^{-1}}$$","['finite-groups', 'abstract-algebra', 'group-theory']"
2334745,"I noticed a relation between the cosines of $0^\circ$, $30^\circ$, $45^\circ$, $60^\circ$, $90^\circ$ and the square roots of $4, 3, 2, 1, 0$","Yesterday I've noticed some relationships with cosine and square root. Anything interesting about it? I was trying to find the smallest width on an hexagon with radius $1.0$ and I noticed that I could get it both by $\sqrt 3$ and the or two times the cosine of $30^\circ$ . I wondered if there would be more matches and so I notice these even though might look silly... $$\begin{align}
2 \cos 90^\circ &= \sqrt 0 \\
2 \cos 60^\circ &= \sqrt 1 \\
2 \cos 45^\circ &= \sqrt 2 \\
2 \cos 30^\circ &= \sqrt 3 \\
2 \cos \phantom{9}0^\circ &= \sqrt 4
\end{align}$$ Is there anything interesting to know about these? Thanks","['algebra-precalculus', 'square-numbers', 'trigonometry']"
2334756,Getting a bound for Gibbs distribution mean,"Suppose $F$ is a strictly convex and increasing function, $U$ a random variable with support $[0,1]$ and density $$ f_U(u)= \frac{e^{-\frac{1}{T}F(u)}}{\int_{0}^{1} e^{-\frac{1}{T} F(x)} dx}.$$ Do we have a known bound for $\Pr\{U>y\}$ for any $y>0$ in terms of $T$ which is tight in asymptotic? In asymptotic, $T \to 0$, one knows that $U$ converges to $0$ in probability. Any help appreciated!","['gibbs-measure', 'probability-theory', 'concentration-of-measure', 'probability-distributions']"
2334831,Roll two fair six-sided dice and adds the results,Grace rolls two fair six-sided dice and adds the results. She than draws a square that has her result as the length of the diagonal. What is the probability that the numerical value of the area of her square will be less than the numerical value of the perimeter? The answer is 5/18. I drew a table and added up the results of dice rolls. But I am not quite sure of the length of the diagonal? Please help. Thank you in advance.,['probability']
2334840,"Is there a symbol for an ""element""?","Is there a math symbol that represents an arbitrary element? This shouldn't be confused with the ""element of"" symbol $\in$.","['notation', 'elementary-set-theory']"
2334868,Example of a curve embedding in $\mathbb{P}^3$.,"I'm following a course in algebraic geometry and I'm asked to write a short expository paper about the topic that every nonsingular curve can be embedded in $\mathbb{P}^3$. You should be aware that I have almost no background in algebraic geometry. When I assume that the curve $X$ lies already in some $\mathbb{P}^m$ it is doable. It is done by taking a point $P \in \mathbb{P}^m \backslash X$ outside the curve and project the curve from the point onto a hyperplane $H \cong \mathbb{P}^{m-1}$, this works when $m \geq 4$. The conditions $P$ has to fulfil are the following: $P$ is not on any secant line of $X$, $P$ is not on any tangent line of $X$. By my very limited background, it would help a lot for me to understand the algebraic geometry involved if someone could give an elaborated example of a curve lying in $\mathbb{P}^4$ and project it in $\mathbb{P}^3$.","['algebraic-curves', 'projective-space', 'algebraic-geometry']"
2334872,Dimension of the linear system of a divisor and topological calculation,"Before exposing my question, I'd like to specify that I'm not a geometer. So, please, be explicit as much as possible (and excuse me for this probable dumb question). Given a divisor $D$ on a complexe projective curve $X$, there is a nice way to build a line bundle, $[D]$, which first Chern class is almost $D$ : it's just the Poincar dual of $D$ when understood as homology class. For example, if I take the canonical bundle $K_X$ of $X$, then $$K_X = c_1(T^*X) = -c_1(X).$$ And now there is two ways to compute $l(K_X)$, the dimension of the linear system of $K_X$. On one hand, we can do some explicit computation (it's classical algebraic geometry).
On the other hand, if I admit that $l(K_X)$ is the genus of $X$, $g(X)$, then since :
$$\langle c_1(X),[X]\rangle = \chi(X) = 2-2g(X)$$
we get
$$ l(K_X) = \frac {\langle K_X,[X]\rangle}2 - 1.$$ My questions are the following : Is there such a way to compute $l(D)$ for a general divisor $D$, i.e. in terms of the topology of the line bundle associated ? (Please don't just tell me that it is the dimension of $H^0(X,\mathcal O(D))$. I want something more computable.) If yes, is the formula : $$l(D) = \frac{\langle D,[X]\rangle}2 - 1$$
reasonable ? Thanks for your time :)","['algebraic-topology', 'algebraic-geometry']"
2334885,sufficient statistic for a strange parameter on normal distribution,"Let $X_1,\ldots,X_n$  identically indepedent and distributed like   $N(b,1)$ . I'm supposed to find a sufficient statistic for $a=P[X_1<1]$.",['statistics']
2334887,Proof that the product of 7 successive positive integers is not a square.,Can someone give me a hint for this problem (for (8-9) grade student):Proof that the product of 7 successive positive integers is not a square. (I've found a proof for general case is given in : here ),"['number-theory', 'contest-math', 'square-numbers']"
2334893,How to evaluate $\cos{\frac{\pi}{8}}$?,"I have to evaluate
$\cos{\frac{\pi}{8}}$
and I'm supposed to do so evaluating first 
$\cos^2{\frac{\pi}{8}}$
(since it's an exercise to practice half-angle formulas). Solving this second formula I get to $\cos^2{\frac{\pi}{8}} = \frac{1}{2} + \frac{1}{2\sqrt[]{2}}$ where I'm stuck. I'm not sure it is an useful evaluation and, worst of all, I don't think that could help me solving 
$\cos{\frac{\pi}{8}}$.
I don't know how to get to 
$\frac{\sqrt[]{2 + \sqrt[]{2}}}{2}$, which is given as the proper answer. Could anyone explain it to me? Thank you very much in advance.","['radicals', 'angle', 'trigonometry', 'pi']"
2334912,Creating a measure on an uncountable chain,"Consider a universe $\mathcal{F}$ of cardinality $|\mathcal{F}| = |\mathbb{R}|$. Let $\mathcal{C}$ be a an uncountable family of subsets of $\mathcal{F}$, which is totally ordered under the $\subseteq$ relation (namely, $|\mathcal{C}| > \aleph_0$ and for any $A,B \in \mathcal{C}$, either $A \subseteq B$ or $B \subseteq A$). In other words, $\mathcal{C}$ is a chain. We would like to create a probability measure over $\mathcal{F}$, with the following properties: Each $A \in \mathcal{C}$ is measurable. Each singleton has a measure of 0. Is it possible to create such a measure in general?","['set-theory', 'measure-theory']"
2334919,Is the maximum likelihood estimator always a sufficient statistic?,"Here's an example of what I am asking : $X_1,\ldots,X_n $ i.i.d $N(\phi , 1)$ where $\phi \in \mathbb{R}$ . Let $\gamma = P(X_1\leq 1)$ . Give a sufficient statistic of $\gamma$ . This question is part of a midterm exam I took on april. I see that $\gamma_{m\ell} = \Phi(1 - \phi_{m\ell}))$ , where $\Phi$ is the standard normal distribution function. I know that in this case, $\phi_{m\ell} = n^{-1}\sum(X_i^2)$ which is a sufficient statistic. Does that make $\gamma_{m\ell}$ one also ? Thank you for your answers.","['statistics', 'probability']"
2334975,"Showing that every finite measure on $(X,P(X))$ can be written as $\int_E f \,du$.","I want to show that every finite measure on $(X,P(X))$, where $P(X)$ denotes the power set of $X$ has the form $v(E) = \int_E f \,du$ for a non-negative measurable function $f$ and where $u$ is the counting measure. I think we must use the Radon-Nikodym theorem here. Firstly any finite measure $v$ on $(X,P(X))$ is $\sigma$-finite. Also $v$ is absolutely continous with respect to $u$ since if $u(E) = 0$ then $|E| = 0$ so $E = \emptyset$ and so $v(E) = v(\emptyset) = 0$ (because it is a measure). However, $u$ (the counting measure) is not necessarily $\sigma$-finite, i.e. if $X$ is uncountable. So I cannot use the Radon-Nikodym here. Perhaps the question was meant to restrict $X$ to countable sets? Since then $u$ is $\sigma$-finite and then Radon-Nikodym applies to give the result. Was there a mistake in the question or can we show the statement (in the first paragraph) as is? The original question:","['real-analysis', 'measure-theory']"
2335010,Probability a tree has $n$ nodes,"Consider each node on a graph has a $p$ probability of creating a new child node every step $c$. For this example $p = \frac{1}{2^{l+1}}$, where $l$ is the distance away from the origin node. See the chart at bottom (sorry for paint). Question: For a step $c$, if $n \in [1, 2^c]$, what is the probability the graph has $n$ nodes? Is there a closed form expression? My thoughts are that you'd have to ""add down the tree"" to get the probability for a specific branch. However, there are multiple ways of having $n$ nodes, which I assume you'd account for by multiplying the probabilities of having each possibility together, i.e. $P(x_j + \dots + x_k) \cdots P(x_m + \dots + x_n)$, where $x_m \dots x_n$ are probabilities in a combination of nodes that gives $n$ nodes overall. For $c=2$ there's $1$ way for it to have $1$ node ($20\%$), $1$ way it could have $2$ ($20\%$), $2$ ways it could have $3$ ($40\%$), and $1$ way it could have $4$ ($20\%$). Clearly this is the result if each outcome is equally likely, which is not the case. Is there an easy, closed form way to weight each outcome? I'm not sure where to go from there or really how to approach this problem at all. Edit: I've been messing around with Mathematica and I've been able to simulate up to $10^7$ and generate some plots of the distributions (seen below). The width is slightly skew due to each step $c$ having a different max number of nodes, however we can still see the shape of the plot well. where the solid lines are approximations using the Gaussian curve, $$ \frac{A}{\sqrt{2\pi}\sigma} e^{\frac{-(x-x_0)^2}{2\sigma^2}} ,$$ $c=11 \to (A=1.00334,\, x_0=15.5507, \, \sigma=6.37225)$, $c=10 \to (A=1.00428,\, x_0=13.0468, \, \sigma=5.50204)$, $c=9 \to (A=1.0053,\, x_0=10.8424, \, \sigma=4.68556)$, $c=7 \to (A=1.01089,\, x_0=7.25233, \, \sigma=3.33195)$, $c=5 \to (A=1.02033,\, x_0=4.58044, \, \sigma=2.22072)$. It's interesting that it's close to a Gaussian distribution, however I'm not sure if this helps in finding a closed form. Maybe one could define $f : \mathbb N \to \mathbb R^3$, which maps $c$ to $A$, $x_0$, and $\sigma$. How one would go about this? My only idea is to calculate $A$, $x_0$, and $\sigma$ a considerable number more times and try to fit the data with Mathematica. I've shown the general trend of the data here .","['graph-theory', 'probability', 'trees']"
2335024,What is the difference between equality and congruence outside of geometry? [duplicate],"This question already has answers here : Congruent Modulo $n$: definition [duplicate] (4 answers) Closed 7 years ago . Yes, I have seen that this question has been asked and answered before in this same website, but answers given there were mostly in regards to geometry, or non-mathematical examples (e.g. ""the 'e's in the word 'between' are congruent, but not equal"", ""two triangles with the same dimensions and points are equal""). I can understand this just fine, but I can't use this advice when it comes to pure numbers, like mods. For example: $17 \equiv 5 (mod(6))$. How is this statement correct? If we solve $5 (mod(6))$ we get $5$, so that would mean $17 \equiv 5$. Also, $17 \equiv 4(mod(13))$, which means $17 \equiv 4$, and $17 \equiv 3(mod(7))$, which is $17 \equiv 3$ So then, $17 \equiv 5 \equiv 4  \equiv 3$ is true? That doesn't seem right.
Are all positive integers congruent to one another? I know this may seem like a simple matter to some, but I'm seriously stuck.","['number-theory', 'modular-arithmetic', 'discrete-mathematics']"
2335031,Value of an exponential function,"What is the value of the exponential function $y=e^{x^{x}-1}$ at $x=0$? I graphed the function on desmos and the value at $x=0$ is $1$. However, I do not know how to show this analytically.",['algebra-precalculus']
