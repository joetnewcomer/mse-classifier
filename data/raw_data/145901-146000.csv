question_id,title,body,tags
2386799,What are some good moderately-difficult to difficult multivariable integral problems?,"I'm looking for interesting, difficult, or otherwise clever multivariable integral problems that are more difficult than usual textbook problems (which, in the textbook I'm reading at least, usually involve either reordering an iterated integral, or making a fairly usual substitution like polar or spherical coordinates). Namely, I'm interested in problems that involve either tricky uses of multivariable substitution, interesting interpretations of the problem (i.e. to solve the problem you need to use a multivariable integral, but how?), clever partitioning of the domain of integration, or other interesting maneuvers. To give some positioning, a problem that would be too easy is to solve the following: $$
\int_0^3\int_{x^2}^9 x^3e^{y^3}\text{d}y\text{d}x
$$ And for completeness, here's a problem that would probably be too hard. Edit: I'm also interested in more obscure and unusual problems. Edit 2: Here are some more problems I'd consider ""too easy"": $$
\iiint_S\sin\sqrt{x^2+y^2+z^2}\text{d}V
$$
where $S$ is the region bounded by $x^2+y^2+z^2 = 49$ and $z^2=x^2+y^2$. $$
\int_0^2\int_0^1\int_y^1\sinh(z^2)\text{d}z\text{d}y\text{d}x
$$ One thing that would be nice are questions that require nonstandard substitutions. Everywhere I look only exercises the cylindrical and spherical coordinate transformations, but Wikipedia has an expansive list of other interesting coordinate systems. What about integration over a torus? Or the intersection of a torus and a hyperbolic paraboloid? What about integrals that require bizarre transformations to complete, ones that don't even have names? I want problems that really exercise one's ability to decipher the best solution to the integral, to understand a difficult region of integration geometrically, and/or to call from different areas of mathematics to solve the integral in unique ways. By ""decipher"", I mean ""see the trick to dig into a problem and make it easier"". For example, the following integral looks ridiculous:
$$
\int_0^1\int_{2\sqrt x}^{1+x}\frac{x}{y+1}\frac{\text{d}y\text{d}x}{\sqrt{y^2-4x}}
$$
If you take the time to experiment, you might find that the substitution $x = uv$, $y = u + v$ turns the integral into
$$
\int_0^1\int_u^1\frac{uv}{u+v+1}\text{d}v\text{d}u
$$
which can be solved more easily. Disclaimer: this is a very contrived problem, I made it up by starting with the end and working in reverse, but it gives you some idea as to the standard of non-triviality I'm hoping for. In actuality, I would deem this integral uninteresting because there's no ""simple but clever/difficult to find"" trick.","['multivariable-calculus', 'problem-solving', 'integration', 'big-list']"
2386829,"Problem of rank, trace, determinant and eigenvalue","Here is a problem and its solution that I translated from Korean (so it might contain some errors): Problem: For real n-by-n matrices $A,B$, suppose $xA+yB=I$ for non-zero real $x,y$ and $AB=0$.
  Prove $$det(A+B) = \frac{1}{x^{\text{rank}(A)}y^{\text{rank}(B)}}.$$ Solution: Let $A'=xA$ and $B'=yB$. Then $A'+B'=I$ and $A'B'=0$. Then it follows that $A'=A'^2$ and $B'=B'^2$. So the minimal polynomials of $A'$ and $B'$ divides $x^2-x$.
  Thus the eigenvalues are $0$ or $1$. Therefore $A'$ and $B'$ are diagonalizable.
  Let $V,W$ be the eigenspace of $A'$, $B'$ respectively corresponding to the eigenvalue $1$.
  Then $\text{trace}(A')=\text{dim}(V)$ and $\text{trace}(B')=\text{dim}(W)$. (*) Also $\text{trace}(A')+\text{trace}(B')=n$.
  Thus $\text{dim}(V \cap W)=0$ and $R^n=V \oplus W$ (direct sum). (**) Thus 
  $$\text{det}(A+B)= \frac{1}{x}^{\text{dim}(V)} \frac{1}{y}^{\text{dim}(W)}=\frac{1}{x}^{\text{trace}(A')} \frac{1}{y}^{\text{trace}(B')}=\frac{1}{x^{\text{rank}(A)}y^{\text{rank}(B)}}$$ My question: 1) Why (*) holds? 2) Why (**) holds? I know that the sum of eigenvalues equals $\text{trace}(A)$ and the multiple equals $\text{det}(A)$. Thank you.","['eigenvalues-eigenvectors', 'linear-algebra']"
2386840,Solve DFE $2y'^2 = 2x^2y' - 3xy$,"I want to solve the DFE $$2y'^2 = 2x^2y' - 3xy.$$
I started with substituting $p = dy/dx$ and obtained
$2p^2 = 2x^2p - 3xy$. I isolated $y$ and got $$y = \frac{2}{3} \left[xp - \frac{p^2}{x}\right].$$ Taking the derivative yields: $$y' = p'= \frac{2}{3}\left[p'x + p - \frac{p(2xp' - p)}{x^2}\right].$$
And that's where I draw blank. I tried factorizing, but at no avail. Any ideas?",['ordinary-differential-equations']
2386860,Any odd number is of form $a+b$ where $a^2+b^2$ is prime,"This conjecture is tested for all odd natural numbers less than $10^8$: If $n>1$ is an odd natural number, then there are natural numbers $a,b$  such that $n=a+b$ and $a^2+b^2\in\mathbb P$. $\mathbb P$ is the set of prime numbers. I wish help with counterexamples, heuristics or a proof. Addendum: For odd $n$, $159<n<50,000$, there are $a,b\in\mathbb Z^+$ such that $n=a+b$ and both
$a^2+b^2$ and $a^2+(b+2)^2$ are primes. As hinted by pisco125 in a comment, there is a weaker version of the conjecture: Every odd number can be written $x+y$ where $x+iy$ is a Gaussian
  prime. Which give arise to a function: $g:\mathbb P_G\to\mathbb O'$, given by $g(x+iy)=x+y$, where $\mathbb O'$ is the odd integers with $0,\pm 2$ included. The weaker conjecture is then equivalent with that $g$ is onto. The reason why the conjecture is weaker is that any prime of the form $p=4n-1$ is a Gaussian prime. The reason why $0,\pm 2$ must be added is that $\pm 1 \pm i$ is a Gaussian prime.","['conjectures', 'gaussian-integers', 'sums-of-squares', 'number-theory', 'prime-numbers']"
2386884,"Please help explain how ""Any symmetric sum can be written as a Polynomial of the elementary symmetric sum functions""","I was browsing through the Art Of Problem Solving website and came across this: ""Any symmetric sum can be written as a Polynomial of the elementary symmetric sum functions, for example $x^3 + y^3 + z^3 = (x + y + z)(x^2 + y^2 + z^2 - xy - yz - xz) + 3xyz = S^3_1 - 3S_1S_2 + 3S_3$ (*) This is often used to solve systems of equations involving power sums, combined with Vieta's formulas."" However, I do not understand how the expression (*) above was derived and, I also do not understand how it can be used to solve systems of equations involving power sums. Please someone should help with a detailed explanation and also an example on how it can be used to solve systems of equations involving power sums.","['polynomials', 'algebra-precalculus', 'symmetric-polynomials', 'contest-math', 'summation']"
2386891,How to prove $\frac{\partial}{\partial{W}} \operatorname{trace}((Y-XW)(Y-XW)^T)=2X^T(XW-Y)$?,"$\newcommand{\tr}{\operatorname{tr}}$ I want to prove the following expression  with simple matrix operations. \begin{align}
& \frac{\partial}{\partial{W}} \tr((Y-XW)(Y-XW)^T)=2X^T(XW-Y) \\[10pt]
= {} & \frac{\partial}{\partial{W}} \tr((Y-XW)(Y-XW)^T) \\[10pt]
= {} & \frac{\partial}{\partial{W}} \tr(YY^T - YW^TX^T - XWY^T + XW(XW)^T) \\[10pt]
= {} & \frac{\partial}{\partial{W}} \tr( -YW^TX^T - XWY^T + XW(XW)^T) \\[10pt]
= {} & -2X^TY  +\frac{\partial}{\partial{W}} \tr(XW(XW)^T)
\end{align} Now I need to calculate  $ \dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)$. Can anyone help to calculate this derivate? Thanks. ========= based on the suggestion to write the expression in Einstein notation I found: $\begin{align}
&\dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)=\\[10pt]
= {} &   \dfrac{\partial}{\partial{W}} \tr(XW(XW)^T)\\[10pt]
= {} &  \dfrac{\partial}{\partial{W}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W^T_{kl}X^T_{li}\\[10pt]
= {} &  \dfrac{\partial}{\partial{W_{jk}}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W^T_{kl}X^T_{li} + \dfrac{\partial}{\partial{W_{lk}}} \sum_i\sum_j\sum_k\sum_l X_{ij}W_{jk}W_{lk}X^T_{li} \\[10pt]
= {} & \sum_i\sum_k\sum_l(W^TX^T)_{ki} X_{ij}   +  \sum_i\sum_j\sum_k X_{ij}W_{jk} X^T_{li}  \\[10pt]
= {} & \sum_i\sum_k\sum_l(W^TX^T)_{ki} X_{ij}   +  \sum_i\sum_j\sum_kX^T_{li} X_{ij}W_{jk} \\[10pt]
={} &2X^TXW  
\end{align}$ Please let me know if something is wrong with this. Thanks.","['derivatives', 'matrices', 'matrix-calculus', 'gradient-descent', 'trace']"
2386923,Problem on the domain of the solution of a differential equation,"Let $f:[0,\alpha]\to\mathbb{R}$ be a solution of the Cauchy problem: $\begin{cases} f'(t)=(f(t))^2+t \\ f(0)=0 \end{cases} $ The question is: prove that $\alpha<3$. It is clear that the problem admits a unique solution locally on some $[-\epsilon,\epsilon]$, since $(f)^2+t$ is $C^1$ in $(f,t)$, and we have $f(t)>0$ for all $t>0$, looking at the sign of $f'$. But now I don't know how to proceed. Thank you all!","['cauchy-problem', 'ordinary-differential-equations', 'analysis']"
2386938,"inner automorphisms of $B({\cal H})$, reference","It is a well-known fact that if $\phi : B({\cal H})\to B({\cal H})$, with ${\cal H}$ complex Hilbert space, is a $^*$-automorphism, then there exists a unitary operator $U: {\cal H} \to {\cal H}$ such that $$\phi(A)= UAU^{-1}\quad  \mbox{for every $A \in B({\cal H})$.}$$ Could you give me a precise reference where this statement is proved?","['functional-analysis', 'reference-request', 'operator-theory']"
2386949,Higher ramification,"Let $K/L/M$ be finite Galois function field extensions and let $P|Q|S$ be places of the corresponding function fields. Write $G_i(\cdot|\cdot)$ for the $i$-th ramification groups. One can easily show that the canonical projection $Gal(M/K) \rightarrow Gal(L/K)$ projects $G_i(S|P)$ into $G_i(Q|P)$ surjectively for $i=-1,0$. What about the higher $i$'s? In general, this projection induces an embedding 
$G_i(S|P)/G_i(S|Q) \rightarrow  G_i(Q|P)$ and
this has to be an isomorphism for $i=-1,0$ because the cardinalities of the left and right side are the same.
But this holds only since the ramification groups are the decompositon and inertia groups and their cardinalities are $ef(\cdot|\cdot)$ and $e(\cdot|\cdot)$, which again satisfy the multiplicative transitivity rule. Thank you.","['ramification', 'galois-theory', 'algebraic-geometry']"
2386982,Topological Open Mapping Theorem for mappings between different Euclidean dimensions?,"What is the topological version of the following theorem? Open Mapping Theorem. Let $\Omega \subset \mathbb{R}^n$ be open and $f: \Omega \rightarrow \mathbb{R}^m$ a continuously differentiable function. If for every $x\in\Omega$ the derivative $f'(x)$ is surjective and $U\subset \Omega$ is open, then the image $f(U)$ is open in $\mathbb{R}^m$ . So previous derivative matrix is surjective. For a weaker version where $n=m$ and derivative matrix is bijective, i.e. $\det f'(x) \neq 0$ , we have the corresponding Invariance of Domain Theorem by Brouwer that is based on injectivity. But is there any similar results for that stronger form of the Open Mapping Theorem? EDIT: Is a (continuous) linear injection only suitable topological map?","['differential-topology', 'reference-request', 'multivariable-calculus', 'general-topology', 'analysis']"
2387048,Geometric Expectation problem?,"Blitzstein, Introduction to Probability (2019 2 edn), Chapter 2, Exercise 25, p 198. Calvin and Hobbes play a match consisting of a series of games, where Calvin has
probability $p$ of winning each game (independently). They play with a “win by two”
rule: the first player to win two games more than his opponent wins the match. Find
the expected number of games played. Hint: Consider the first two games as a pair, then the next two as a pair, etc. My approach: This is a geometric distribution related problem because you are trying to find the number of games played, until ""success"" occurs which in this case is winning by two. So using the hint, I thought about the probability of winning by two. For player 1, the probability of winning two games in a row is $p^2$ . For player two, the probability of winning two games in a row is $(1-p)^2$ or $q^2$ . So the total probability of winning two games in a row for either player is $p^2+q^2$ . After this, I tried plugging this into the geometric expectation formula which is $E[X]=\frac{1-p}{p}$ . But I wasn't getting the right answer. Can anyone explain what I did wrong? Thanks! The author's solution: $\frac{2}{p^2+q^2}$","['probability-distributions', 'random-variables', 'discrete-mathematics']"
2387113,How to show that the unit disk in $\mathbb{R}^2$ is equinumerous to $\mathbb{R}^2$?,"Prove that set $D=\{ ⟨x,y⟩ \in \mathbb{R}^2\,|\, x^2 + y^2 \leqslant 1 \}$ is equinumerous to $\mathbb{R}^2$. I know that $D \leqslant\mathbb{R}^2 $, since $D \subseteq \mathbb{R}^2 $, but I have no idea how to show that $D \geqslant\mathbb{R}^2 $. I was thinking about constructing a circle with radius $= 1$, and and center at $(0, 0)$, but then I got stuck. Could you please help me and explain how to solve this example?",['elementary-set-theory']
2387130,"Evaluate $\int_0^\pi\frac{\ln\left(1+\cos\theta\right)}{\cos\theta}\,d\theta$","The problem is to evaluate: $$\int_{0}^{\pi}{\left(\frac{\ln{\left(1+\cos{\theta}\right)}}{\cos{\theta}}\,d\theta\right)}$$ An estimate for the integral is $4.9348022$. There is a similarity between this integral and the dilogarithm function, which is defined by: $$\operatorname{Li}_2(z):=-\int_{0}^{z}{\left(\frac{\ln{\left(1-t\right)}}{t}\,dt\right)}$$ but I am not sure how to use this effectively. In addition, there are two singularities in the interval of integration: one singularity when $\theta\to\pi$ and the integrand increases without bound, and one removable 'hole' at $\theta=\pi/2$, where the limit of the value of the integrand is $1$. Integration by parts does not seem to simplify the integral. I also tried some substitutions, such as $x=\cos{\theta}$: $$\int_{-1}^{1}{\left(\frac{\ln{\left(1+x\right)}}{x\sqrt{1-x^2}}\,dx\right)}$$ which brings it closer to the dilogarithm form. The Weierstrass substitution $x=\tan{\left(\theta/2\right)}$ gives: $$\int_{0}^{\infty}{\left(\frac{2}{1-x^2}\cdot\ln{\left(\frac{2}{1+x^2}\right)}\,dx\right)}$$ Any ideas? Thanks!","['integration', 'definite-integrals']"
2387137,Can $\Gamma(1/5)$ be written in this form?,"tldr Feel free to skip to the integrals at the end. I was curious as to whether or not for $n\in\mathbb N$ , $\Gamma(1/n)$ satisfies the form $$\Gamma(1/n)=a\pi^{b/n}\prod_k\operatorname{agm}(1,c_k)^{d_k}$$ Where $a,c_k,d_k$ are algebraic and $b$ is natural and $\operatorname{agm}$ is the arithmetic-geometric mean given by $$\operatorname{agm}(x,y)=\lim_{n\to\infty}a_n\\a_0=x,~g_0=y\\a_{n+1}=\frac{a_n+g_n}2,\quad g_{n+1}=\sqrt{a_ng_n}$$ The first few values can be derived from this PDF . $$\Gamma(1/1)=1$$ $$\Gamma(1/2)=\pi^{1/2}$$ $$\Gamma(1/3)=\frac{\pi^{2/3}2^{2/3}}{3^{1/12}\operatorname{agm}(1,3^{1/4}2^{-1/2})}$$ $$\Gamma(1/4)=\frac{2^{1/2}\pi^{3/4}}{\operatorname{agm}(1,2^{-1/2})}$$ However, it does not appear $\Gamma(1/5)$ can be written this way.  The PDF gives $$\Gamma(1/5)=\pi^{1/5}2^{19/50}5^{1/2}\phi^{1/10}H_1^{2/5}H_2^{1/5}$$ $$\phi=5+\sqrt5\\H_1=\int_0^1\frac{dx}{\sqrt{1-x^5}}\\H_2=\int_0^1\frac{x~dx}{\sqrt{1-x^5}}$$ And I'm uncertain about whether or not $H_1$ or $H_2$ can be written in the form of $$\frac\pi{2\operatorname{agm}(1,\sqrt{1-x})}=\int_0^1\frac{dt}{\sqrt{(1-t^2)(1-xt^2)}},~x>0$$ Can $\Gamma(1/5)$ be written in terms of this arithmetic-geometric mean?","['special-functions', 'integration', 'elliptic-integrals', 'gamma-function']"
2387149,Explanation circular permutation,"Well if one looks at the formula of circular permutations $Pc = (n-1)!$ But as we come to that formula, I need a concrete example and an explanation. Another thing, I have seen that when working with a bracelet, the formula changes. A concrete example please along with an explanation Finally how the formula is worked when there are repeated elements that are permuted. Thank you","['permutations', 'combinatorics']"
2387204,primes generated by a continued fraction,"On the post Is there an explanation for the behaviour of this finite continued fraction in connection with prime numbers? I asked this question in a non-generalized form focusing only on the $4th$ partial convergent but now generalizing the problem and clarifying it ,we have Given the continued fraction which satisfies the property proposed in one of my old posts $G(q)=\cfrac{1}{1-q+\cfrac{q(1-q)^2}{1-q^3+\cfrac{q(1-q^2)^2}{1-q^5+\cfrac{q(1-q^3)^2}{1-q^7+\cfrac{q(1-q^4)^2}{1-q^9+\dots}}}}}$ and $kth$ partial convergent of the continued fraction $\cfrac{1}{1-q+\cfrac{q(1-q)^2}{1-q^3+\cfrac{q(1-q^2)^2}{1-q^5+\cfrac{q(1-q^3)^2}{\ddots+\cfrac{q(1-q^k)^2}{1-q^{2k+1}}}}}}=\exp\Big(\sum_{n=2}^{\infty} (-1)^n\phi_{k}(n)\,q^n\Big)$ where $|q|\lt\frac{1}{4}$,and $\phi_{k}(n)$ is our symbol of choice that represents coefficients of the series(please note that it doesn't represent any standard function) depending on the $kth$ partial convergent $k\gt2$. For $k\gt2$,every partial convergent of the continued fraction seems to have the property that: For all values of $n$ but a few(that are exceptions to the rule) ,$\phi_{k}(n)$ is integer when $n$ is prime and non-integer when $n$ is composite. For example on the $7th$ partial convergent of the continued fraction,there's only one exception $n=15$ in $1\lt n\lt200$ Formally we may call $\phi_{k}(n)$ an arithmetic function which returns an integer when $n$ is a prime number and non-integer when $n$ is a composite number for all natural numbers $n$ but a few for $k\gt2$. So the question is Why is $\phi_{k}(n)$ integer when $n$ is prime and non-integer when it is composite for all values of $n$ but a few for $k\gt2$? We may be led to conjecture  that whenever $n=prime$,the arithmetic function $\phi_{k}(n)$ is always integer.","['number-theory', 'analytic-number-theory', 'prime-numbers', 'continued-fractions']"
2387206,Gauss' lemma for arbitrary integral domains,"One of the versions of the classical Gauss' lemma in abstract algebra states the following: Theorem: Let $R$ be an integral domain, $f\in R[X]$ of positive degree and $K$ the quotient field of $R$ . If $R$ is a UFD, these facts are equivalent: i) $f$ is irreducible in $R[X]$ . ii) $f$ is primitive in $R[X]$ and irreducible in $K[X]$ . This result, as Wikipedia points out, it's still valid when $R$ is a GCD domain. So my question is basically this: what is the weakest hypothesis we can assume in $R$ and still be able to guarantee that the above result is true? Certainly we would have to change the usual definition of ""primitive polynomial"". I think it's appropriate to take a definition that is used in many research papers, namely we say that $f=a_0+a_1X+\ldots +a_nX^n$ is primitive if $(a_0,a_1,\ldots a_n)\subseteq (d)$ implies $d\in R^{\times}$ , i.e., the only elements that divide all the coefficients of $f$ are the units of $R$ . It's easy to see that this definition agrees with the standard one: if $R$ is a GCD domain then $f$ is primitive iff $\gcd(a_0,a_1,\ldots ,a_n)=1$ . Now, using the above definition it's easy to show that $f$ irreducible in $R[X]$ implies that $f$ is primitive in $R[X]$ and moreover that ii) implies i), without any extra assumption on $R$ , but in general $f$ irreducible in $R[X]$ doesn't imply $f$ irreducible in $K[X]$ . This leads to the following question: what extra conditions we need to guarantee that $f$ irreducible in $R[X]$ implies that $f$ is irreducible in $K[X]$ ? Is there any research papers about the question above?","['abstract-algebra', 'ring-theory', 'reference-request', 'polynomial-rings']"
2387215,How to reconcile these $L^p$ and $L^2$ (in)equalities?,"Let $X,Y$ be iid random variables with mean $\mu$ and having finite moments of, say, all orders. It is an easy exercise to show that
$$\operatorname{Var}(X) = E[|X-\mu|^2] = \frac{1}{2} E[|X-Y|^2].\tag{*}$$
If we want an $L^p$ version of this statement, $1 \le p < \infty$, we can write
$$\begin{align*}
E\left[|X-\mu|^p\right] &= E\left[\left|E[X-Y \mid X]\right|^p\right] \\
&\le E\left[E[|X-Y|^p \mid X]\right] &&\text{(conditional Jensen)} \\
&= E[|X-Y|^p].
 \end{align*}$$
However, this does not reduce to (*) when $p=2$, because the factor of $1/2$ is missing.  Is there a ""better"" version of this $L^p$ inequality that contains the equality for $p=2$?","['inequality', 'probability-theory', 'lp-spaces', 'integration', 'measure-theory']"
2387218,My maths Question is on Probability [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question In an experiment a die is rolled twice, and the numbers shown are noted what is the probability of getting the same number on the first and second   rolls? my answer is $\frac1{36}$ but I just need confirmation on this cos im not that smart in probability","['statistics', 'probability']"
2387224,Is it possible to find dependence between $X$ and $Y$ from distribution of $X-Y$?,"Consider $X,Y \sim \mathcal N(0,1)$, I am exploring all the possible ways to find dependence. Correlation, Eye Balling from scatter plot are I think obvious methods. I am trying to think in a different direction just to have a better understanding. So, When $Y$ is dependent on $X$ what would $X-Y$ distribution look like?
From what I am aware of $X-Y \sim \mathcal N(0,2)$ when $X$ and $Y$ are independent. Consider a distribution show in the graph below, I want to comment on dependence just from the graph dist If this is not possible how can joint probability tell be about dependence.","['independence', 'statistics', 'normal-distribution', 'probability-distributions']"
2387269,Sufficient conditions to ensure the limit is zero,"Let $a_n$ be a sequence, $S_n=\sum_{k=1}^n a_k$. (1) If $S_n$ is bounded, $\lim_{n\to\infty}(a_{n+1}-a_n)=0$, show $\lim_{n\to\infty}a_n=0$. (2). If $\lim_{n\to\infty}\frac{S_n}{n}=0$, $\lim_{n\to\infty} (a_{n+1}-a_n)=0$, can we show $\lim_{n\to\infty}a_n=0$? Prove it if it true, or else give a counterexample. On the first problem, I have tried Cauchy's criteria, arguing by contradiction...But no solution. On the second, I have tried $a_n=\ln n$, but...",['limits']
2387276,Does a homeomorphism between two compact metric spaces preserve open balls?,"More specifically, if $X$ and $Y$ are compact metric spaces, and there is a $\phi: X \to Y$ a homeomorphism, then is it true that $\phi^{-1}(B(\phi(x),r))= B(x,r)$ ? If so, how? Thanks in advance!","['general-topology', 'real-analysis', 'analysis']"
2387303,"If T and S are topologically conjugate homeomorphisms, T on X is minimal iff S on Y is minimal. (Definitions inside)","Let $X$ and $Y$ be compact metric spaces, $T:X \to X$ and $S: Y \to Y$ homeomorphisms. We say $T$ and $S$ are topologically conjugate if there exists a homeomorphism $\phi: X \to Y$ such that $S(\phi(x))= \phi(T(x))$ for all $x \in X$. We say a homeomorphism $T$ is minimal if for every $x \in X$, the set $\{T^k x: x\in \mathbb{Z}\}$ is dense in $X$. The claim is that T is minimal iff S is minimal. I have tried to show this but to no avail, as I get stuck trying to compare a ball around x with a ball around $\phi(x)$ which unless its an isometry I can not do anything interesting with to get my result. Thanks in advance!","['general-topology', 'real-analysis', 'dynamical-systems', 'analysis']"
2387338,Inconsistent interpretations for the second parameter of the Gamma distribution,"The following question is motivated by material from pages 358-364 of Blitzstein and Hwang's Introduction to Probability .  (NB: I have modified the book's notation in various places to make my question clearer.) First, suppose that $\{N_t\}_{t \in \mathbb{R}^+}$ is a family of r.v.s indexed by the set $\mathbb{R}^+$ of positive reals, and that for each $t \in \mathbb{R}^+$, we have $N_t \sim \mathrm{Poisson}(\lambda t)$, where $\lambda$ is an unknown parameter, independent of $t$. Or, to put it in Bayesian terms, we have $$
    N_t\mid\lambda \sim \mathrm{Poisson}(\lambda t)
$$ Now, take 1 $\mathrm{Gamma}(n_0, t_0)$ as the (conjugate) prior distribution for $\lambda$.  I.e. $$
    \lambda \sim \mathrm{Gamma}(n_0, t_0)
$$ Starting from these assumptions, the authors derive (p. 362-364) the posterior distribution for $\lambda$ as $$
    \lambda \mid (N_t = n) \sim \mathrm{Gamma}(n_0 + n, t_0 + t)
$$ Furthermore, authors derive (p. 364) the posterior expectation for $\lambda$ as $$
    \operatorname{E}(\lambda \mid N_t = n) = \frac{n_0 + n}{t_0 + t}
$$ These results suggest that the first parameter of a $\mathrm{Gamma}$ distribution is akin to a count (of occurrences), and the second one is akin to an length of time . Under this interpretation, the prior distribution $\mathrm{Gamma}(n_0, t_0)$ would be obtained from the total number of occurrences ($n_0$) observed over one or more intervals of time adding up to a total of $t_0$ (time units).  To get the posterior distribution, we update the parameters of the prior's $\mathrm{Gamma}$ by adding a number of occurrences ($n$) observed during a new interval of time of length $t$, so that now we have a total of $n_0 + n$ occurrences observed in a cumulative interval of length $t_0 + t$. On the other hand, on p. 358 the authors note that a $\mathrm{Gamma}(1, \nu)$ distribution is equivalent to an $\mathrm{Exponential}(\nu)$ distribution, defined as the distribution whose PDF is $f(t) = \nu e^{-\nu t}$.  In this case, the second parameter of the $\mathrm{Gamma}$ distribution seems to be behaving like a rate (occurrences per unit time), rather than a length of time. I'm puzzled by these two radically different interpretations of the $\mathrm{Gamma}$ distribution's second parameter (first as a length of time, and later as a rate). Is my reasoning above wrong?  If not, is there some way to unify or rationalize such divergent interpretations? 1 The book uses the convention that $\mathrm{Gamma}(a, b)$ is the distrubtion whose PDF is $f(x) = \frac{(b x)^a e^{-b x}}{x \Gamma(a)},\; x > 0$.","['probability-distributions', 'poisson-process', 'statistics', 'gamma-distribution', 'probability']"
2387368,"Size of Terms in ""Solid"" Sequences","Call a finite sequence $\{a_1,a_2,..,a_n\}\in \mathbb{N}_{≥1}$ solid if, for all contiguous subsequences $S=\{a_i,a_{i+1},\dots,a_{i+j}\}$, there does not exist an adjacent contiguous subsequence 
$S'=\{a_{(i+j)+1},a_{(i+j)+2},\dots,a_{(i+j)+k}\}$
such that $$\sum_{a\in S}a=\sum_{b\in S'}b$$ For example, the sequences $\{1,3,2,3,4,2,4\}$ and $\{1,3,2,3,1,3,2,8\}$ are solid. However, $\{1,2,1,3,2\}$ and $\{1,3,2,3,5\}$ are not. Here is what I am trying to show... Claim : If for some solid sequence $\{a_1,a_2,..,a_n\}$ we have $n\geq2^m$ for some positive integer $m$, then there exits $i\leq n$ such that $a_i\geq2^{m-1}$ This feels like it should have a neat inductive proof. The base case is clear (as any four term sequence must include a $3$). I just can't seem to see the trick for the induction (though it is possible that the proof may not even involve the technique at all). So, first of all, is this claim correct? And, if so, what's the proof? Can a stronger lower bound than $2^{m-1}$ be demonstrated? If the claim is not correct, what bounds can we put on the greatest term of a solid sequence of length $n$? Update :It seems like a bound of $2^{m-1}$ is indeed quite weak. What better (possibly asymptotic) bound could be proved?","['number-theory', 'combinatorics', 'summation', 'sequences-and-series', 'elementary-number-theory']"
2387383,Can't figure out why this Eigenvector and Engenvalue doesn't work,"For this matrix $$\begin{bmatrix} 3 &  0 & 0 \\
1  & 2 &  0 \\  
-4  & 5 &  -1 \end{bmatrix}$$ The Eigenvalues = $-1, 2, 3$ The Eigenvectors I got were $\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$, $\begin{bmatrix} 0 \\ \frac35 \\ 1 \end{bmatrix}$, $\begin{bmatrix} 4 \\ 4 \\ 1 \end{bmatrix}$. However, only the last pair, $3$ and $\begin{bmatrix} 4 \\ 4 \\ 1 \end{bmatrix}$ were correct. I've looked over and over my algebra, but I don't see where I went wrong. EDIT: For anyone in the future, the Eigenvalues/vectors I got were right, but I used the wrong matrix. I should have multiplied the Eigenvector with the matrix $$A=\begin{bmatrix} 3 &  0 & 0 \\
1  & 2 &  0 \\  
-4  & 5 &  -1 \end{bmatrix}$$but instead I used the wrong matrix, (A-lambda*I) and substituted the corresponding Eigenvalue into lambda.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2387412,A short exact sequence with $M=M_1 \oplus M_2$ that does not split,"A sequence of $R$-modules of the form
  $$0 \to M_1 \stackrel{f}{\to} M \stackrel{g}{\to} M_2 \to 0$$
  is called a short exact sequence (ses) if $f$ is injective, $g$ is surjective and $\operatorname{Im} f = \operatorname{Ker} g$. A short exact sequence is said to split if there exist a $R$-homomorphism $h: M_2 \to M$ such that $g\circ h = \operatorname{Id}_{M_2}$. It is well known that if a short exact sequence split then $M \cong M_1 \oplus M_2$. What I am interested in is the converse. I know that the converse is not true. That is, there exist a short exact sequence with $M \cong M_1 \oplus M_2$ which does not split. I wish to construct such a counter example. Consider $R=\mathbb{Z}$, $M_1=\mathbb{Z}/2\mathbb{Z}$ and $M'=\mathbb{Z}/4\mathbb{Z}$. Suppose we have a $\mathbb{Z}$-module $N$ such that $M_1\oplus N \cong N$ and $M' \oplus N \cong N$. Define $M= M'\oplus N$, and $M_2=M_1\oplus N$. Then
$$M_1\oplus M_2 \cong M_1 \oplus N \cong N \cong M' \oplus N = M$$
Then consider the sequence
$$0 \to M_1 \stackrel{f}{\to} M \stackrel{g}{\to} M_2 \to 0$$
where $f:M_1\to M=M''\oplus N$ is defined as $f(\bar{1})=(\bar{2},0)$ and $g: M \to M_2$ as $g(\bar{x},n)=(\bar{x},n)$. Then clearly $f$ is injective and $g$ is surjective and $\operatorname{Im} f = \operatorname{Ker} g$. Suppose the ses splits. Then $h :M_2 \to M$ is such that $g\circ h =\operatorname{Id}_{M_2}$. But then
$$h(\bar{1},0) \in  g^{-1}(\bar{1},0)=\{\bar{1},0),(\bar{3},0)\}$$
But $2(\bar{1},0)=(\bar{2},0)$ and $2(\bar{3},0)=(\bar{2},0)$ in $M$. Thus
$$h(\bar{0},0)=2h(\bar{1},0)=(\bar{2},0)$$
which contradicts the fact that $h$ is a homomorphism. Thus the ses does not splits. The only thing that remains to show the existence of such an $N$. However, I am no able to show that such an $N$ does exists. Any help/ suggestions.","['abstract-algebra', 'exact-sequence', 'modules']"
2387495,How do you differentiate a matrix equation with respect to a vector?,"I'm having lots of trouble piecing this together (from Elements of Statistical Learning by Hastie/Friedman): I don't understand the step: ""[d]ifferentiating w.r.t  $B$"", specifically how to calculate the derivative of an equation involving matrix products and transposes with respect to a vector.  Is this standard matrix calculus?","['derivatives', 'linear-algebra']"
2387522,Express the generating function of $Y=aX+b$ as a function of $M_x(t)$,"I need to express the generating function of $Y=aX+b$ as a function of $M_x(t)$. For me, the generating function of a variable $Y$ is the expected value of $e^{ty}$, like this: $$M_y(t) = E[e^{ty}] = E[e^{t(ax+b)}] = E[e^{tax}e^{tb}]$$ how to express it in function of $M_x(t)$? I thing I should write it as something related to $E[e^{tx}]$ but I don't know how to do it. Should I use some expected value property?","['statistics', 'probability', 'moment-generating-functions']"
2387631,Understanding some properties $U(2)$,"I have read that the unitary group $U(2)$ can be naturally embedded as a subgroup of $SO(4)$. What exactly does this map look like? In particular, can we identify $U(2)$ as the set of matrices of  $SO(4)$ that are $\textit{complex}$-linear? Moreover, it seems that we still identify $\mathbb C^2$ with $\mathbb R^4$ here (correct?). Thus, if $U(2)$ is a subgroup of $SO(4)$, then wouldn't every unitary matrix be an orientation-preserving isometry of $\mathbb R^4$? But the problem with this is that this matrix could have determinant $-1$. How do we resolve this? Finally (and a bit vaguely), if every unitary matrix is an isometry of $\mathbb R^4$, then how can we interpret geometrically   the fact that this matrix is complex-linear? That is, can we view a unitary matrix as an isometry of $\mathbb C^2$ as a two-dimensional vector space over $\mathbb C$? (However, we would need a different norm in this case.)","['abstract-algebra', 'group-theory', 'linear-algebra', 'geometry']"
2387702,What are big cells in Grasmannians?,"I found that in some papers the term ''big cells'' is used, for example, A categorification of Grassmannian cluster algebras . What are ''big cells'' and ``non-big cells'' in a Grassmannian? Thank you very much.","['representation-theory', 'algebraic-geometry', 'commutative-algebra']"
2387716,"Inverse spectral theorem, clarification from a video.","I'm following some of the lectures of professor Schuller. I'm watching this video specifically At about minute 23 he defines the PVM (Projected Valued Measure) as a map $$
P_A : \sigma(\Theta_\mathbb{R}) \rightarrow \mathcal{L}(\mathcal{H})
$$ with the specific expression $$
\langle\psi,P_A(\Omega)\phi\rangle=\int \chi_\Omega d\mu_{\psi,\phi}^A \;\;\forall\psi, \phi \in \mathcal{H}
$$ Here $\mathcal{H}$ is an hilbert space, $A$ is a self-adjoint operator in $\mathcal{H}$, while $\mu_{\psi,\phi}^A$ is a complex valued measure constructed by polarization of a real valued measure $\mu_{\phi}^A$ (these concepts are expressed a bit earlier in the same lecture, but all the constructions are given later). My question is why the PVM defined above does represent an operator when evaluated in a measurable set? suppose indeed a self-adjoint operator $A$ is given, and we also fix a measurable set $\Omega$, the PVM seems to me to take two arguments and not just one. Schuller also says that the map is somehow given by that expression, so there's probably something I'm missing. Can you help me in interpreting that expression? Update: I've watched the whole video, and I guess my question may be explained as follows. Suppose we are in a vector space of dimension $n$ embedded with a dot product. The dot product is characterized by a matrix $A \in \mathbb{R}^{n\times n}$. We can recover such matrix by knowing how the dot product acts on a basis. By taking the canonical base $V = \left\{e_1,\ldots,e_n \right\}$ (columns vectors), we can recover the element $a_{ij}$ as $$
\langle e_i,e_j \rangle = e_i^T  A  e_j = a_{ij}
$$ If the base is not the canonical one we can perform a transformation and still retrieving the matrix (which again is actually an operator). Hence for finite dimension the dot product is fully characterized once we now how it acts on a basis. (I haven't been very precise, but you've got my point I hope). The expression $$
\langle\psi,P_A(\Omega)\phi\rangle=\int \chi_\Omega d\mu_{\psi,\phi}^A \;\;\forall\psi, \phi \in \mathcal{H}
$$ it seems to me be somehow related, therefore I was wondering if there's any theorem that characterize self-adjoints operator in a similar fashion as matrices and dot products in finite dimensions. Name of a theorem, any reference, or if you could prove it for me would be really useful.","['functional-analysis', 'spectral-theory', 'measure-theory', 'operator-theory']"
2387724,"With $X$ compact Hausdorff space and $f:X \to X$ is continuous, prove there exists a closed set $C$ such that $f( C)=C$ [duplicate]","This question already has answers here : Fixed Set Property? (2 answers) Closed set mapped to itself in a compact Hausdorff space (2 answers) Closed 6 years ago . Prove that if $X$ is a non-empty topological Hausdorff compact space and $f : X\to X$ is continuous, there exists a non-empty closed set $C \subset X$ such that $f(C)=C$. My thoughts are to show that we can travel from a starting point $x$ through $f(x),f(f(x)),...,f^n(x)$ such that $f^n(x)=x$. that's we found a closed set. I can't seem to find the use of $f$ being continuous. Thanks!","['continuity', 'general-topology', 'compactness']"
2387742,Probability to draw equal number of red and green marbles,"From an urn containing equal number of red and green marbles, we draw an even number of marbles. Prove that the probability of drawing an equal number of marbles from each color is $\frac{2}{\sqrt{n\pi}}$. I am not sure: I think that the probability is $$\frac{\sum_{k = 2}^{n} C(k, k/2)}{\sum C(n, k)}$$ where k is even. 
Then by Stirling's approximation, we have $C(2k, k)= \frac{4^k}{\sqrt{k\pi}}$?",['combinatorics']
2387746,Integral of a derivative.,"I've been learning the fundamental theorem of calculus.
So, I can intuitively grasp that the derivative of the integral of a given function brings you back to that function. 
Is this also the case with the integral of the derivative? And if so, can you please give a intuition for why this is true? 
Thanks in advance","['derivatives', 'integration', 'calculus', 'inverse']"
2387795,Find degree of polynomial satisfying given condition,"Question: If $f(x)$ is a polynomial of degree $n$ such that $$1+f(x)=\frac{f(x-1)+f(x+1)}{2} \forall x\in R$$
  then find $n$. My attempt: I first started off by trying to prove $f(x)$ to be periodic, as I always do whenever I spot $f(x-a)+f(x+a)$ anywhere. It did not work anyway. Then
 I thought of assuming a standard function of degree = $1,2,3$. It failed for 1 and 3, and worked for 2, so I gave answer as 2. However, I feel that: I did not prove that there are no higher $n$ for which this equation holds. My method is not neat at all! I guess there must be a much simpler method, can anyone please give some starting steps for that neat logic?",['calculus']
2387810,"The calculation of $\mathrm{Ext}^1(\prod\mathbb Z,\mathbb Z)$","In $\mathbb Z$-Mod, is $\mathrm{Ext}^1(\prod\mathbb Z,\mathbb Z)=0$? I think it is not equal to 0, but I do not know how to prove it. First I select a short exact sequence: $0\rightarrow\mathbb  Z \rightarrow \mathbb Q\rightarrow \mathbb Q/\mathbb Z \rightarrow 0$. Then we get $\mathrm{Hom}(\prod \mathbb Z,\mathbb Q)\stackrel{f}\rightarrow\mathrm{Hom}(\prod \mathbb Z,\mathbb Q/\mathbb Z) \rightarrow \mathrm{Ext}^1(\prod \mathbb Z,\mathbb Z) \rightarrow 0$. Is $f$ is epic? I can not see the Hom between infinite product to $\mathbb Q$ or $\mathbb Q/\mathbb Z$. Thanks for you help!","['abstract-algebra', 'homological-algebra', 'commutative-algebra']"
2387814,Preservation of martingale property when changing to a product space.,"Let the process $M=(M_t, t\ge 0)$ be a martingale on the probability space $(\Omega_1, \mathcal F_1, P_1)$ with respect to the natural filtration of $M$. Let $X=(X_t, t\ge 0)$ be a process on the probability space $(\Omega_2, \mathcal F_2, P_2)$. Let $W=(M,X)$ be the coupled stochastic process on the probability space $(\Omega_1\times\Omega_2, \mathcal F_1\times\mathcal F_2, P_1\times P_2)$ (let us call it 'the product space'). Questions: Does it make sense to only consider the process $M$ on the product space. That is, the first element in $W$ alone? If 1. does makes sense, will $M$ and $X$ be the same stochastic processes on the the product space as they where in their original probability spaces respectively? Independence is assumed. That is do $M$ and $X$ follow the same laws/distributions on the product space as their original probability spaces from which they originally where defined, if $M$ and $X$ are independent of each other? If 1. does makes sense. Will $M$ be a martingale with respect to its natural filtration in the product space? That is, will $M$ be a $(P_1\times P_2)$-martingale? Could there be some lack of completeness in the product space? If so, is it necessary to avoid the lack of completeness to make 1., 2., 3. and 5. valid? If it is necessary, could this always be done in a way to guarantee 1., 2., 3. and 5. holds? Does the definition of the product space given above imply independence of $M$ and $X$, or does one need to consider additional conditions to assure independence of $M$ and $X$, perhaps completion of the product space is necessary?","['stochastic-processes', 'martingales', 'probability-theory', 'measure-theory']"
2387840,"Is ""$1$ is not an integer"" a contradiction?","I learnt that a contradiction is a statement which is always false regardless of the truth values of the individual statements that it is made up of. Now ""$1$ is not an integer"" is a single statement which is false. Thus there is no question of it being made up of several statements. Can I say that it is a contradiction? Thanks.","['logic', 'discrete-mathematics']"
2387850,Doubts on solution of product of diagonals of $n$-sided regular polygon,"The OP in this post asked the following: If you take a regular $n$ -sided polygon, which is inscribed in the unit circle and find the product of all its diagonals (including two sides) carried out from one corner you will get $n$ exactly: $A_1A_2\cdot A_1A_3\cdot ...\cdot A_1A_n = n$ user21820 used the following idea to solve the above question. Let $z$ be a complex number such that $z^n=1$ where $n$ is the number of sides of that polygon.
Denote $z_0=1,z_1,...,z_{n-1}$ be roots of the equation $z^n=1.$ It suffices to show that $$\prod_{k=1}^{n-1}|1-z_k| = n.$$ By the definition of roots, we have $$z^n-1=\prod_{k=0}^{n-1}(z-z_k) = (z-1)\prod_{k=1}^{n-1}(z-z_k).$$ Also by factorization, we have $$z^n-1 = (z-1)\sum_{k=0}^{n-1}z^k$$ By equating the two equations and cancelling the factor $(z-1)$ , we have $$\prod_{k=1}^{n-1}(z-z_k) = \sum_{k=0}^{n-1}z^k.$$ Let $z=1.$ So we have $$\prod_{k=1}^{n-1}(1-z_k) = \sum_{k=0}^{n-1}z^k = n.$$ Therefore, $$\prod_{k=1}^{n-1}|1-z_k| = n.$$ Question: After we cancel the factor $(z-1),$ I thought the substitution $z=1$ at latter part is invalid? My doubt arises from the fact that cancellation in the following $$0\cdot x = 0 \cdot y \Rightarrow x = y$$ is not valid.","['algebra-precalculus', 'complex-numbers', 'proof-verification']"
2387879,Weak Composition with Restrictions(0..9),"I want to optimise some subtask and I'm not good enough in math. Given: There are ticket numbers with $k$ digits from 0 to 9. Let $n$ is sum of digits for the ticket number. Needed: calculate count for all possible $n$ from 0 to $9*k$. I can do it with brute force with $O(10^k)$, but I want do better. When $n < 10$, this is Weak Composition with pretty simple formula with good $O(n, k)$, so I easily obtained counts for first ten $n$. Starting from $n=10$, this is Weak Composition with Restrictions and I didn't manage to obtain formula from the few related SO posts. Exactly, I need help with recursive/iterative formula $count(n, k)$ for $n$ from 10 to $(9*k)/2$ (due to gaussian distribution) Typical $k$ is small: 4,6,8.",['combinatorics']
2387884,Explain whether or not the three probabilities add to unity. Combinations with replacement?,"Problem (1):
A storage contains 200 computers; 5 are defective and 195 are fine. Two computers are selected randomly with replacement . Calculate the probability that neither computer is defective (correct?)
$$\frac {\binom {195+2-1}{2}}{\binom {200+2-1}{2}}$$ Calculate the probability that exactly one computer is defective (correct?)
$$\frac {\binom {5+1-1}{1}\binom {195+1-1}{1}}{\binom {200+2-1}{2}}$$ Calculate the probability that both computers are defective (correct?)
$$\frac {\binom {5+2-1}{2}}{\binom {200+2-1}{2}}$$ Explain whether or not the three probabilities you have just calculated should add to unity Question 4 I don't really understand. Problem (2): ... without replacement. Calculate the probability that neither computer is defective
$$\frac {\binom {195}{2}}{\binom {200}{2}}$$ Calculate the probability that exactly one computer is defective
$$\frac {\binom {5}{1}\binom {195}{1}}{\binom {200}{2}}$$ Calculate the probability that both computers are defective
$$\frac {\binom {5}{2}}{\binom {200}{2}}$$ Explain whether or not the three probabilities you have just calculated should add to unity All three add to 1. Should these three add to unity because of their mutual exclusiveness and exhaustiveness? Please clarify my thoughts. Edited: Much thanks for every single answer and comment! In the mean time I came up with the tree diagram to visualize the solutions:","['combinatorics', 'probability']"
2387913,Help understanding closed subschemes and closed immersions,"Closed subschemes and closed immersions of schemes have been causing me a lot of confusion for a while now. I have a few questions that I think might clear things up. Please assume that when I use the term ""ring"" that I mean ""commutative ring with identity"" whose morphisms take $0 \mapsto 0$ and $1 \mapsto 1$. Sorry for the long question, but I feel it is necessary to spell out the definitions I am working with rather than expect people to chase them up. My first exposure to this was Hartshorne page 85. There he makes the following definitions: A closed immersion is a morphism of schemes $\iota: Y \longrightarrow X$ such that $\iota$ induces a homeomorphism of sp$(Y)$ onto a closed subset of sp$(X)$, and further that the induced map of sheaves, $\iota^{\#}: \mathcal{O}_{X} \longrightarrow \iota_{*}\mathcal{O}_{Y}$ is surjective. A closed subscheme is an equivalence class of closed immersions, where we say that $\iota: Y \longrightarrow X$ and $\iota': Y' \longrightarrow X$ are equivalent if there is an isomorphism $\psi: Y' \longrightarrow Y$ satisfying $\iota' = \iota \circ \psi$. After having a bit of confusion with the closed subscheme part, I consulted Görtz & Wedhorn where, on page 84 (Definition 3.41) they give their own definitions. Their definition for a closed immersion is identical, however their definition for a closed subscheme is as follows: A closed subscheme of a scheme $X$ is given by a closed subset $Y \subseteq X$ (let $\iota: Y \hookrightarrow X$ be the inclusion) and a sheaf $\mathcal{O}_{Y}$ on $Y$ such that $(Y, \mathcal{O}_{Y})$ is a scheme, and such that the sheaf $\iota _{*}\mathcal{O}_{Y}$ is isomorphic to $\mathcal{O}_{X}/ \mathcal{I}$ for $\mathcal{I}$ a subsheaf of ideals of $\mathcal{O}_{X}$. With that in place, my question is basically threefold: 1) Aside from the suggestive name, Hartshorne doesn't seem to suggest (at least not to me) that a closed subscheme is actually a scheme. Indeed, how does one even make sense of putting a scheme structure on an equivalence class of morphisms? 2) Görtz & Wedhorn seem to overcome this by simply defining it to be a scheme. How are their definitions equivalent (if they are)? One big problem I am having seeing this equivalence is relating the surjectivity of $\iota^{\#}$ and $\iota^{'\#}$ to the sheaf of ideals in Görtz & Wedhorn. The issue is, since the category of rings is not abelian (indeed, not even additive as far as I know), I can't expect to have kernels and cokernels, and so I can't expect to be able to take $\mathcal{I}$ to be the kernel of the surjective morphisms like I could if they were sheaves of abelian groups. 3) I tried to play around with the affine case in Hartshorne's definition to make sense of things. Let $A$ be a ring with ideals $\mathfrak{a}$ and $\mathfrak{b}$. These give morphisms of schemes Spec$(A /\mathfrak{a}) \longrightarrow$ Spec $A$ and Spec$(A /\mathfrak{b}) \longrightarrow$ Spec $A$ which induce homemorphisms onto the closed sets in the obvious way. My intuition suggests that these should be ""equivalent"" (for the purposes of defining a closed subscheme) precisely if $\mathfrak{a}$ and $\mathfrak{b}$ have the same radical, since then $V(\mathfrak{a})$ and $V(\mathfrak{b})$ would be the same. Indeed the radical is just the intersection of all primes containing them. However, by Hartshorne's definition, that would require that the isomorphism $i$ be induced by an isomorphism of rings $A / \mathfrak{a} \simeq A/ \mathfrak{b}$ whenever $\mathfrak{a}$ and $\mathfrak{b}$ have the same nilradical, which is obviously nonsense. The long of the short is I am stumped and incredibly confused. Any advice, or references, or answers (partial or full) to all or any of these questions would be greatly appreciated. Bonus question: Is there a way to fix my confusion $\textit{without}$ resorting to quasi-coherent sheaves?","['schemes', 'closed-map', 'sheaf-theory', 'algebraic-geometry']"
2387940,"If $\lVert x\rVert=\lVert y \rVert$, prove that exists a unitary transformation such as $Tx=y$","Given $V$ a unitary vector space with a finite dimension, and let $x, y$ vectors in $V$ such as their norm is the same $\left(\lVert x\rVert=\lVert y \rVert\right)$ . Prove that exists a unitary transformation $T:V\to V$ that assigns $x$ to $y$ $(T(x)=y)$ . What I've been trying to do is to complete $x$ and $y$ to a basis of $V$ , $B=\{x,y,v_3, v_4, ..., v_n\}$ , and then define $T$ on B's vectors, like so- $\ \ T(x)=y,\ T(y)=x,\ T(v_i)=v_i \;\forall i\in\{3,\ldots,n\}$ . However I cannot seem to be able to prove that this transformation is indeed unitary, and at this point I'm not even sure it is. Would love a hint, and thanks in advance.","['linear-algebra', 'linear-transformations']"
2387949,Integral of the Product of a Function and Its Derivative,"Problem: Let $f$ be a function such that the graph of $f$ is a semicircle $S$ with end points $(a,0)$ and $(b,0)$, where $a < b$. The improper integral $\int_{a}^{b} f(x) f'(x) dx$ is (A) necessarily zero (B) possibly zero but not necessarily zero (C) necessarily nonexistent (D) possibly nonexistent but not necessarily (E) none of the above This is a practice problem for timed test, so I am looking for the quickest solution.  Here is what I came up with. Right from the beginning it is rather hand-wavy. It suffices to transform the circle into the unit circle centered at $(0,0)$. In that case, the integral becomes $\int_{-1}^{1} f(x) f'(x) dx$, where $f(-1)=f(1)$. Since $\frac{1}{2} \frac{d}{dx}f(x)^2 = f(x) f'(x)$, the integral becomes $$\frac{1}{2} \int_{-1}^{1} dx f(x)^2 dx = \frac{1}{2} [f(1)^2 - f(-1)^2] = 0,$$ which means that (A) is the correct choice. My question is, how do I make this idea of transforming the circle more rigorous? I have never actually done anything like this before, but I could see geometrically that it would work. How do I make this step more rigorous?","['derivatives', 'circles', 'gre-exam', 'calculus', 'integration']"
2387967,Does complete graph $G$ have an independence number $\alpha(G) =1$?,"I am in a little confusion, so here is my reasoining. For a complete graph with 5 verticies $ K_{5}, \text{ indpendent set I = } \{\varnothing,\{1\},…,\{5\}\},$  which means that independence number $\alpha(G) =|\{1\}| =1?$  But for me intuitively a complete graph has $\alpha(G)=0$ since there is NO such set on $K_{5}$ where no verticies are adjacent. Any good suggestion to clarify my confusion?","['graph-theory', 'discrete-mathematics']"
2387983,How should I study measure theory?,"Im about to start studying measure theory on my own but I dont know what order I should follow, plus I dont know which textbook I should use.  Any ideas? I've already had a course on measure theory but it was kinda limited on the Lebesgue measure on the real numbers, integrals, $L^p(\mathbb{R})$ spaces etc; I'd like to study the more general measure theory. Also, any tips or advice for the subject would be appreciated","['book-recommendation', 'measure-theory', 'soft-question']"
2387987,"Calculate surface area of $z = x^2, \quad 0 \leq y \leq x \leq 1$","Calculate surface area of $z = x^2, \quad 0 \leq y \leq x \leq 1$ My attempt : I parametrize the surface with 
$$r(x,y) = (x,y,x^2),$$
where the domain E is given by: 
$$0 \leq x\leq1,$$
$$0 \leq y \leq x.$$
I find $|r'_x \times r'_y|$:
$$|r'_x \times r'_y|$$
$$|(1,0,2x) \times (0,1,0)|= |(-2x,0,1)| = \sqrt{4x^2+1}$$ The area is given by 
$$\iint_E(\sqrt{4x^2+1})dxdy,$$
but I can't compute this integral and it doesn't seem to be computable with reasonable complexity according to wolfram alpha. What am I doing wrong? The answer is supposed to be: $\frac{5\sqrt{5}-1}{12}$","['multivariable-calculus', 'surface-integrals']"
2387994,Prove the following (algebra of polynomials),"Let $P_1=1$ and let $P_2=n+1$ define $$P_{i+1}=\frac{P_i^2-1}{P_{i-1}}$$ Prove that if $a \mid b$ then $ P_a \mid P_b $ I am working on this problem for a while but I could use some help
here are the first 5 polynomials generated by this definition $$ P_3=n^2+2n$$ $$ P_4=n^3+3n^2+n-1$$ $$ P_5=n^4+4n^3+3n^2-2n-1$$ $$ P_6=n^5+5n^4+6n^3-2n^2-4n$$ $$ P_7=n^6+6n^5+10n^4-9n^2-2n+1$$ Note that for instance $P_2 \mid P_6$ $$ \frac{P_6}{P_2}=n^4+4n^3+2n^2−4n $$ also $P_3 \mid P_6$ $$ \frac{P_6}{P_3}=n^3+3n^2−2 $$ Finally $(P_2\times P_3) \mid P_6$ $$ \frac{P_6}{P_2 \times P_3}=n^2+2n−2 $$ So it seems to work but who can help me prove it (I think the induction method is the most appropriate) any ideas?","['recurrence-relations', 'polynomials', 'divisibility', 'algebra-precalculus', 'induction']"
2387997,When calculating work using line integrals how is it known that paramaterization will not change the path?,"I am having a bit of an intuition problem here. As I am doing work problems on  the work done by using line integrals it is said the work done for some but not all line integrals depends on the path.  Let us focus on those.  I  used different parameterizations of the path. In one case the path was nothing more that a straight line.   I let x = t and y = t, being my parameterization. In another case the path was a parabola and I let x = t and y = t^2 , that being the parameterization. Now the professor said how I parameterized made no difference. So I used x = sin t and y = sin t in the line case and x = sin t and y = sin^2 t in the second case. The results being the same. My question is when I use the sin function to parameterize how can the results possibly be guaranteed to be the same? The work function does not change , I can see that and the bound are kept the same I can see that, and the path is still the same in that but in one case I used a variable and the other case I used a sin function...how do I know the sin function does not affect the path in some way...struggling here with intuition.  Thank you","['multivariable-calculus', 'line-integrals']"
2388001,Can hyperbolic functions be defined in terms of trigonometric functions?,"For example, can $\sinh x$ be written as a function of $\sin x$? Another question, are hyperbolic functions dependent of their trigonometric correspondence in any way?","['hyperbolic-functions', 'exponential-function', 'trigonometry']"
2388005,alternative proof : Borel $\sigma$-field on $\mathbb R^d$ is the smallest $\sigma$-field making all continuous functions measurable,"This question is related to Prove that Borel sigma field on R(d) is the smallest sigma-field that makes all continuous functions f:R(d)->R measurable. I'm trying to prove this exercise in Durrett's book myself, and wonder if the following elementary arguments are valid: First of all, for every continuous $f:\mathbb R^d\to \mathbb R$, $f^{-1}(E)$ is open, for any open set $E\in \mathbb R$.  So clearly the Borel $\sigma$-field $\mathcal B^d$ makes all continuous function measurable. To show the $\mathcal B^d$ is the smallest such $\sigma$-field $\mathcal F$, let $d=2$ for simplicity of arguments.  Consider two continuous functions: $g(x,y)=x$ and $h(x,y)=y$.  Note that $g^{-1}((a_1, b_1))=(a_1, b_1)\times(-\infty, \infty)$, and $h^{-1}((a_2, b_2))=(-\infty, \infty)\times(a_2, b_2)$.  So $\mathcal F$ must contain $\{(a_1, b_1)\times(-\infty, \infty):a_1, b_1\in \mathbb R\}$ and $\{(-\infty, \infty)\times(a_2, b_2):a_2, b_2\in \mathbb R\}$, and hence $\{(a_1, b_1)\times(a_2, b_2):a_1, b_1, a_2, b_2\in \mathbb R\}$.  This implies $\mathcal F\supset \mathcal B^2$ and hence $\mathcal F = \mathcal B^2$.  Extension to $d>2$ or $d=1$ is obvious. Is this proof correct?  If there're flaws, I'd appreciate it if someone can point them out.  Thanks a lot!","['probability-theory', 'measure-theory', 'proof-verification']"
2388041,Unique differentiable structure on homeomorphic manifolds in low dimension,"When I took the introductory lectures to geometry and topology some years ago our professor mentioned the following result: Let $n\in \{1, 2, 3 \}$ and $M, N$ be two $n$-dimensional differentiable manifolds. Then
  $$ M \text{ and } N \text{ homeomorphic} \Rightarrow  M \text{ and } N \text{ diffeomorphic.} $$ I could not find a reference, where this is acutally proved. I'd greatly appreciate if someone could provide my with a paper (or book) containing this result. I read in the comment section of how to prove that every low-dimensional topological manifold has a unique smooth structure up to diffeomorphism? that this should be in Moise's book ""Geometric Topology in Dimensions 2 and 3"", however, I couldn't find such a statement in there.","['manifolds', 'reference-request', 'differential-geometry']"
2388049,"Prob. 17, Chap. 6, in Baby Rudin: $\int_a^b \alpha(x) g(x) \ \mathrm{d} x = G(b) \alpha(b) - G(a) \alpha(a) - \int_a^b G \ \mathrm{d} \alpha$","Here is Prob. 17, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\alpha$ increases monotonically on $[a, b]$, $g$ is continuous, and $g(x) = G^\prime (x)$ for $a \leq x \leq b$. Prove that 
  $$ \int_a^b \alpha(x) g(x) \ \mathrm{d} x = G(b) \alpha(b) - G(a) \alpha(a) - \int_a^b G \ \mathrm{d} \alpha. $$ Hint: Take $g$ real, without loss of generality. Given $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$, choose $t_i \in \left( x_{i-1}, x_i \right)$ so that $g \left( t_i \right) \Delta x_i = G \left( x_i \right) - G \left( x_{i-1} \right)$. Show that 
  $$ \sum_{i=1}^n \alpha \left( x_i \right) g \left( t_i \right) \Delta x_i = G(b) \alpha (b) - G(a) \alpha (a) - \sum_{i=1}^n G \left( x_{i-1} \right) \Delta \alpha_i. $$ My Attempt: Here is the link to one of my Math SE posts where I've included Definitions 6.1 and 6.2 in Rudin: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . All the notation to be used below is contained in these definitions. We first assume that $g$ is real; then so is $G$. As $g$ is continuous on $[a, b]$, so $g$ is Riemann-integrable, by Theorem 6.8 in Rudin; as $\alpha$ is monotonic on $[a, b]$, so $\alpha$ is Riemann-integrable, by Theorem 6.9 in Rudin. Now as both $\alpha$ and $g$ are Riemann-integrable on $[a, b]$, so is $\alpha g$, by Theorem 6.13 (a) in Rudin. Thus $ \int_a^b \alpha(x) g(x) \ \mathrm{d} x$ exists in $\mathbb{R}$. As $G$ is differentiable on $[a, b]$, so $G$ is also continuous on $[a, b]$, by Theorem 5.2 in Rudin, and as $G$ is continuous on $[a, b]$ and $\alpha$ is monotonically increasing on $[a, b]$, so $G$ is Riemann-Stieltjes integrable with respect to $\alpha$ on $[a, b]$, again by Theorem 6.8 in Rudin. Thus $\int_a^b G \ \mathrm{d} \alpha$ exists in $\mathbb{R}$. Let $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ be a partition of $[a, b]$. For each $i = 1, \ldots, n$, as $G$ is continuous on $\left[ x_{i-1}, x_i \right]$ and differentiable on $\left( x_{i-1}, x_i \right)$, so by the mean-value theorem we can find a point $t_i \in \left( x_{i-1}, x_i \right)$ such that 
  $$ G \left( x_i \right) - G \left( x_{i-1} \right) = G^\prime \left( t_i \right) \cdot \left( x_i - x_{i-1} \right) = g \left( t_i \right) \Delta x_i. \tag{0} $$ 
  So 
  $$
\begin{align}
\sum_{i=1}^n \alpha \left( x_i \right) g \left( t_i \right) \Delta x_i &= \sum_{i=1}^n \alpha \left( x_i \right) \left[ G \left( x_i \right) - G \left( x_{i-1} \right) \right] \qquad \mbox{ [ using (0) ] } \\
&= \sum_{i=1}^n \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=1}^n  \alpha \left( x_i \right) G \left( x_{i-1} \right)  \\
&= \sum_{i=1}^n \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right)  \\
&= \alpha \left(x_n \right) G \left( x_n \right) + \sum_{i=1}^{n-1} \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right) \\
&= \alpha(b) G(b) - \alpha (a) G(a) + \sum_{i=0}^{n-1} \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right) \\
&= \alpha(b) G(b) - \alpha (a) G(a) + \sum_{i=0}^{n-1} \left[ \alpha \left( x_i \right) -  \alpha \left( x_{i+1} \right) \right] G \left( x_i \right) \\ 
&= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=0}^{n-1} \left[ \alpha \left( x_{i+1} \right) -   \alpha \left( x_i  \right) \right] G \left( x_i \right) \\
&= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=1}^{n} \left[ \alpha \left( x_{i} \right) -   \alpha \left( x_{i-1}  \right) \right] G \left( x_{i-1} \right) \\
&= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=1}^{n} G \left( x_{i-1} \right) \Delta \alpha_i. 
\end{align}
$$ What next? How to proceed from here? Now we assume that $g$ is a complex function on $[a, b]$, that $g$ is continuous, and $g(x) = G^\prime (x)$. Then $g = \Re g + \iota \Im g$, and $\Re g$ and $\Im g$ are both real continuous functions on $[a, b]$. Moreover, the function $G$ too is complex, and $G = \Re G + \iota \Im G$, where $\Re G$ and $\Im G$ are real functions on $[a, b]$ such that $ \left( \Re G \right)^\prime = \Re g$ and $ \left( \Im G \right)^\prime = \Im g$. The function $\alpha$ is still real of course. So 
  $$ 
\begin{align}
\int_a^b \alpha(x) g(x) \ \mathrm{d} x &= \int_a^b \alpha(x) \Re g(x) \ \mathrm{d} x + \iota \int_a^b \alpha(x) \Im g(x) \ \mathrm{d} x \\
&= \left[ \alpha (b) \Re G(b) - \alpha(a) \Re G(a) - \int_a^b \Re G \ \mathrm{d} \alpha \right] \\
& \qquad  + \iota \left[ \alpha (b) \Im G(b) - \alpha(a) \Im G(a) - \int_a^b \Im G \ \mathrm{d} \alpha  \right] \\
&= \alpha (b) G(b) - \alpha(a) G(a) - \int_a^b G \ \mathrm{d} \alpha, 
\end{align}
$$
  as required. Is this proof for complex functions correct? If so, then is it rigorous enough too? If not, then where have I erred?","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'analysis']"
2388054,"Is it possible to dissect a circle of radius 9 into 81 equal areas, using only circles?",You can dissect a circle of radius $3$ into $9$ equal areas by placing within it $5$ unit circles in an orthogonal cross shape. You could then place $5$ of these radius $3$ circles into a circle of radius $9$ in a similar way. Is it then possible to dissect the $4$ irregular shapes each into $9$ equal areas using only circles? Or is there another entirely different way of achieving a similar result?,"['circles', 'geometry']"
2388058,Matrices in group theory,"Let
  $$ S = \left\{ \left(\begin{matrix}
        a & a \\
        a & a  \\
        \end{matrix}\right) \ \middle| \ a \in \mathbb{R} \right\}. $$
  and $\cdot$ be a usual matrix multiplication operation. Then is $(S,\cdot)$ a group or semigroup or monoid ?","['abstract-algebra', 'group-theory', 'monoid']"
2388119,Solving a recurrence equation with constant term,"so I have this recurrence equation: $$G(n) = 2G(n-1) + 100 $$ I believe this a linear non-homogeneous equation of the form $G(n) = LH + F(n) $ where LH is the associated linear homogeneous equation which is G(n-1).
If $F(n)$ is $2n$ we can take a trial solution as $Cn+D$ and solve or if F(n) is say $7^n$ we can take $C\cdot7^n$ and try to solve for constants $C$ and $D$. What do I do if $F(n)$ is a constant like $100$. The initial condition is G($0$) = $2$. How do I go about solving this? Thanks in advance!","['recurrence-relations', 'discrete-mathematics']"
2388129,$x^{(n)}$ converges to $x$ in $\ell^1$ if and only if $\lim x^{(n)}_j=x_j$ for all $j\in\mathbb N$,"Let $x^{(n)}$ and $x$ belong to the unit sphere of $\ell^1$. I want to prove that $x^{(n)}\to x$ if and only if $\lim_{n\to \infty} x^{(n)}_j=x_j$ for all $j\in \mathbb N$. It's clear that $x^{(n)}\to x$ implies $x^{(n)}_j\to x_j$ since
$$|x^{(n)}_j-x_j|\leq \sum_{j=1}^\infty|x^{(n)}_j-x_j|\to 0.$$ However the other side is not easy for me. If $x^{(n)}_j\to x_j$ for all $j$ and $\epsilon>0$ is given then there exists $N_j$ such that if $n\geq N_j$, then $|x^{(n)}_j-x_j|< \epsilon$. But this doesn't help. I think maybe we should find a $N$ such that for $n\geq N$ we have $|x^{(n)}_j-x_j| <\frac{\epsilon}{2^j}$ for all $j$. And I don't know how $\sum_j |x^{(n)}_j|=\sum_j|x_j|=1$ could help.","['real-analysis', 'banach-spaces', 'normed-spaces', 'functional-analysis', 'lp-spaces']"
2388155,Difference between Ritz vectors and Eigenvectors,"This is probably a silly question, as it came from an error in the Eigenvectors I found using ARPACK (Fortran). In this case, the values of the Ritz vectors are identical in value to the theoretical Eigenvectors but different in sign. So what is the real difference between the two? Should I expect this behavior?","['matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'linear-algebra']"
2388187,Small powers of small numbers: Is there really a pattern or am I just used to looking for them?,"This probably is just a coincidence, but I've always found it interesting and I wanted to put some feelers out there to see if maybe there really is something to it after all. There are these collections of powers, and in one case a sum of powers in two different ways, that have similar digits to each other. $$3^7 = 2187$$
$$6^4 = 1296$$
$$12^3 = 1728$$
$$13^3 = 2197$$
$$1^3 + 12^3 = 9^3 + 10^3 = 1729$$
$$3^6 = 729$$ (I suppose the last two are inevitable in any radix.) We also have $$2^{10} = 1024$$
$$7^4 = 2401$$ and $$2^8 = 256$$
$$5^4 = 625$$
$$24^2 = 576$$ I can see the following class is inevitable in any radix > 4. $$12^2 = 144$$
$$21^2 = 441$$ $$13^2 = 169$$
$$31^2 = 961$$ Is there an overarching mechanism at play here, or am I just finding statistically insignificant patterns?","['number-theory', 'decimal-expansion', 'perfect-powers']"
2388206,When does $L^r(\mu)=L^s(\mu)$?,"Full question: Assume that $\mu(X)=1$. Prove that $L^r(\mu)\supset L^s(\mu)$ if $0<r<s$. Under what conditions do these two spaces contain the same functions? I have already shown the first part is true. Which follows from the inequality $$||f||_r \leq ||f||_s \quad (0<r<s\leq \infty)$$ The second part is tricky. It is clear that $L^r(\mu)=L^s(\mu)$ when the only measurable functions are simple, but it isn't clear if there are any additional conclusions.","['real-analysis', 'lp-spaces', 'measure-theory']"
2388245,"Why is there no Jacobian in the definition of the surface integral, $\iint_UfdS = \iint_Df(r(s,t))|r'_s \times r'_t|dsdt$?","I'm extremely confused by Jacobians and when to use them and I think my lack of understanding can be boiled down to this question: Why is there no Jacobian in the definition of the surface integral:
  $$\iint_UfdS = \iint_Df(r(s,t))|r'_s \times r'_t|dsdt$$
  where U is some surface and D is another surface? Isn't $U$ a surface like any other? Don't we need to compensate for the area when going from $U$ to $D$? Has it got something to do with that $U$ is a ""function surface""(?) and D is a surface in $R^2$?","['surface-integrals', 'multivariable-calculus', 'jacobian', 'parametrization', 'definition']"
2388274,Prime candidates of the form $n^{(n^n)}+n^n+1$?,"Let $$\large f(n)=n^{(n^n)}+n^n+1$$ Checking $f(n)$ for $2\le n\le 100$, I noticed that $f(n)$ has a small prime factor except for $n=12,53$ and $60$ For $n=53$, I found the prime factor $7074407$ , but for $n=12$ and $n=60$, I did not find a prime factor yet. The numbers are too large to apply a primilaty test, so we need trial division. Do $f(12)$ and $f(60)$ have a ""small"" prime factor, or are those numbers candidates for very large primes ?","['number-theory', 'perfect-powers', 'prime-factorization', 'prime-numbers']"
2388280,Invertible function without onto,I have come across a situation in differential calculus where my mentor said that a function is invertible if it is one one because we consider range of function as domain of function's inverse. What does it mean?,"['terminology', 'functions']"
2388294,Finding $n$th power of a $3\times 3$ matrix,"Find the $A^n$ if $$A=\begin{bmatrix}1 & a & b \\0 & 1 &a\\0 &0 &1\end{bmatrix}$$ 
I tried inductive method to show $$A^n=\begin{bmatrix}1 & na & nb+\frac{n(n-1)}{2}a^2 \\0 & 1 &na\\0 &0 &1\end{bmatrix}$$ 
now : My question is : Is there other method (idea ) to find $A^n$ ? Thanks in advance.
Can the idea apply for $$A=\begin{bmatrix}1 & a & b \\0 & 1 &c\\0 &0 &1\end{bmatrix}$$ when $c \neq a$ ?","['matrices', 'matrix-equations', 'matrix-calculus', 'linear-algebra']"
2388320,Confusion regarding the fact that $\bar B \supset \bigcup_{i=1}^{\infty} \bar A_i $.,"This is a question regarding Exercise $2.7(b)$ in Baby Rudin (so everything's in a metric space). Relevant definitions / notation: $\bar E$ is the closure of $E$, i.e., $\bar E = E \cup E'$, where $E'$ is the set of all limit points of $E$. The Exercise: If $B = \bigcup_{i=1}^{\infty}A_i$, prove that $\bar B \supset \bigcup_{i=1}^{\infty} \bar A_i $. Where I Am: Well, we have that 
$$\bar B = B \cup B' = \bigcup_{i=1}^{\infty}A_i \cup \left(\bigcup_{i=1}^{\infty}A_i\right)' = A_1 \cup A_2 \cup \cdots\cup \left( A_1 \cup A_2 \cup \cdots \right)'$$ and $$\bigcup_{i=1}^{\infty} \bar A_i =  \bigcup_{i=1}^{\infty}(A_i \cup A_i ') = (A_1 \cup A_1') \cup (A_2 \cup A_2') \cup \cdots $$ and since $(E_1 \cup E_2 \cup \cdots)' = E_1' \cup E_2' \cdots $ when $E$ is a subset of a metric space, we can write $\bar B$ as $$ A_1 \cup A_2 \cup \cdots\cup A_1' \cup A_2' \cup \cdots  $$ and so (by commutativity and associativity of set operations), we have that $$ \bar B = \bigcup_{i=1}^{\infty} \bar A_i. $$ BUT, this isn't true, as suggested by the next part of the exercise which states that the inclusion can be proper. Indeed, I believe my confusion lies in the fact that we're dealing with the union of a $countably$ $infinite$ collection of sets, seeing as it $is$ true that $$ \bar B_n = \bigcup_{i=1}^{n} \bar A_i, \text{for }n = 1,2,3,..., $$ as indicated by part $(a)$ of the exercise. Nevertheless, I'm not sure what would be considered wrong with my approach.","['general-topology', 'real-analysis', 'elementary-set-theory']"
2388324,Does projections to orthogonal summands closed $\implies$ subspace closed?,"Let $X$ be a inner product space, with orthogonal decomposition $X=V \oplus W$. Give $X$ the topology induced by the norm induced by the inner product. Let $E\subset X$ be a subspace such that the projections to $V, W$ (call them $\pi_V(E), \pi_W(E)$) are closed. Does this imply that $E$ is closed itself? Note that $X$ may not be complete; in fact I am mainly interested in the case where it is not complete. Any kind of help would be appreciated!","['functional-analysis', 'inner-products']"
2388333,Prove that $g=\limsup f_n$ and $h=\liminf f_n$ are Lebesgue measurable.,"Let $f_n:[0,1]\rightarrow\mathbb{R}$ be a sequence of continuous functions. Prove that $g=\limsup f_n$ and $h=\liminf f_n$ are Lebesgue measurable. *Given $f_n$ being continuous on a compact set in $\mathbb{R}$, $f_n$ are Riemann integrable, thus, Lebesgue integrable. Note we can rewrite $g$ and $h$ as the following:
$g=\limsup f_n=\bigcap_{n\rightarrow\infty}\bigcup_{k\geq n}f_k$, where $k,n\in\mathbb{N}$. Since countable union/intersection of measurable functions are measurable by the definition of $\sigma$-algebra, $g$ therefore is Lebesgue measurable. Similarly, $h$ is measurable. Is the above proof correct? Thank you.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2388351,Why is the reciprocal of the power series $1-\frac{z^2}{3!}+\frac{z^4}{5!}+\cdots$ equal to $1+\frac{z^2}{6}+\frac{7z^4}{360}+\cdots$?,"\begin{align}
\frac1{\sin(z)}
&=\frac1z\frac{z}{\sin(z)}\\
&=\frac1z\left(1-\frac{z^2}{3!}+\frac{z^4}{5!}-\frac{z^6}{7!}+\cdots\right)^{-1}\\
&=\frac1z\left(1+\frac{z^2}{6}+\frac{7z^4}{360}+\frac{31z^6}{15120}+\cdots\right)\\
&=\frac1z+\frac{z}{6}+\frac{7z^3}{360}+\frac{31z^5}{15120}+\cdots
\end{align} In particular, from lines 2 to 3, what is happening? (eg. why does reciprocating result in a change of signs?)",['complex-analysis']
2388372,How do I fit two equal-length arcs in the opposite corners of a right triangle?,"I am trying to make two arcs of the same length such that they will both fit in a right triangle like so: I am given the first radius (r1) and a constant k that is the difference between the leg opposite angle one and the second radius (r2) I know that the angle of the arcs have to add up to Pi/2. I also know that this becomes impossible at some small value of k - for instance, if k was zero there is no way the two arcs could be of equal length. If I knew the angle, I could find the second radius, or vice-versa, using trigonometric functions. In the case where both the angles were Pi/4, I know that k would be r*(sqrt(2) - 1). In case anyone wants to know why I'm doing this, I'm trying to show a circle that is broken open at a point and the broken ends curve out until they are parallel and a certain distance apart. That distance will be 2*k. I want to make sure that the curved-outward parts are the same length as they would be if they were still completing the circle.","['trigonometry', 'geometry']"
2388398,Tangent series representation,"How to prove that for any complex number $z$ which is not equal to $\pi k + \frac{\pi}{2}$ ($k\in\mathbb Z$) :
$$ \tan z = \sum_{n=0}^\infty \frac{8z}{(2n+1)^2\pi^2 - 4z^2} $$ 
Using complex analysis, I started with the contour intergal 
$$ \oint_{C_N} \frac{\tan \frac{\pi s}{2}}{s^2-z^2}\,\mathrm ds = \sum_{n=-N}^N \frac{-4i}{(2n+1)^2 - z^2} + \frac{2\pi i \tan \frac{\pi z}{2}}{z}$$
where $C_N$ is the circle centered at 0 of radius $N+1/2$ ($N\in\mathbb N$). The complex number $z$ is chosen to be non zero & non odd integer. However, I don't know how proceed to show that the LHS goes to $0$ as $N\to \infty$ :( Thanks in advance for answers.","['complex-analysis', 'contour-integration', 'sequences-and-series', 'limits']"
2388421,What is the chance that an $n$-gon whose vertices lie randomly on a circle's circumference covers a majority of the circle's area?,"The vertices are chosen completely randomly and all lie on the circumference. Is there a formula for the chance that an $n$-gon covers over $50$% of the area of the circle, with any input $n$? I tried to find something, however I did not know what to look for when I realized the first three values of $n$ would yield a $0$% chance (one point and two points are obviously $0$ because they don't cover any area, and I don't think any inscribed triangle can cover over $50$% of the circle's area). I also believe it to be over $50$% for $n=4$ because when I tried to randomly distribute the points I seemed to get mostly areas that covered most of the circle. Note: The polygon should be a simple polygon, formed by connecting each point to the two points closest to it.","['circles', 'geometric-probability', 'polygons', 'geometry', 'area']"
2388445,Is the notion of 'divergence to infinity in a direction' used?,"In $\mathbb R$ a sequence can diverge to infinity in two directions: $+\infty$ and $-\infty$. These two cases of divergence are quite different from a sequence that diverges to ""nowhere"", like $\{(-1)^n\}$. One can be interested, for example, in the behavior of a function in one of these two directions of infinity. Is there a similar notion for sequences in the complex plane? (Or other spaces) Does it arise naturally in some field of study? What are possible uses for it? I imagine, for example, that the sequence $\{ni\}$ could be said to diverge to infinity in the direction $i$, whereas the sequence $\{ne^{in\sqrt2}\}$ could be said to diverge to infinity in ""every direction"", or in a ""divergent direction"", or in a ""set of directions"". Edit : Added two geometry tags, following the comment of Moishe Cohen","['riemannian-geometry', 'complex-analysis', 'divergence-operator', 'metric-geometry', 'analysis']"
2388457,Asymptotic Growth of Generalised Catalan Numbers,"I'm trying to understand the asymptotic behaviour of a family of integer sequences related to the Catalan numbers $\left( c_n \right)_{n \in \mathbb{N}}$.  The OEIS suggests that these are known as generalised Catalan numbers; I'm not sure if this term is used more widely.  I think I've been able to figure a lot of this out by myself, but I'd appreciate suggestions for any suitable reference I could look up to better understand things. Recall that the Catalan numbers have generating function $F(z)$ where $$F(z) = \frac{1-\sqrt{1-4z}}{2z} .$$ Now for any integer $k \in \mathbb{Z}$ let the 'generalised` Catalan numbers of type $k$ be the integer sequence $(c^{[k]}_n)_{n \in \mathbb{N}}$ with generating function $F_k(z)$ satisfying $$F_k(z) = \frac{1}{1-kzF(z)}.$$ For example, $(c^{[1]}_n)_{n \in \mathbb{N}}$ are just the (ordinary) Catalan numbers and $(c^{[2]}_n)_{n \in \mathbb{N}}$ are the central binomial coefficients.  In these two cases we have exact formulas: $$ c^{[1]}_n = \frac{1}{n+1} {2n \choose n}$$ and $$ c^{[2]}_n = {2n \choose n} .$$ We can use these, together with Stirling's approximation , to show that $$ c^{[1]}_n \sim \frac{4^n}{\sqrt{\pi n^3}} $$ and $$ c^{[2]}_n \sim \frac{4^n}{\sqrt{\pi n}}.$$ For other $k$ I'm not aware of any similar formulas.  The case $k=0$ is trivial as $c^{[0]}_n = 0$ for all $n > 0$.  And I assume the remaining cases require singularity analysis, which is something I'm not really familiar with. I've been told -- though I don't think I've ever seen a proof -- that when an integer sequence $(a_n)_{n \in \mathbb{N}}$ has a generating function $P(z)$ the asymptotic behaviour of the sequence depends on the smallest non-zero singularity $z_0$ of $P(z)$.  In particular, I understand that in this case $a_n \sim \theta(n) |z_0|^{-n}$ for $\theta$ some subexponential function. In the case of $F_k(z)$ as long as $k \neq 0$ there are two singularities: a pole at $z=\frac{k-1}{k^2}$ and a branch point at $z=\frac{1}{4}$.  Since $$0 < \left|\frac{k-1}{k^2}\right| < \frac{1}{4} $$ for all integers $k$ other than those in $$I := \left\{ -4, -3, -2, -1, 0, 1, 2 \right\}$$ this means that for $k \notin I$ we should have $$c^{[k]}_n \sim \theta_k(n) \left( \frac{k^2}{k-1} \right)^n . $$ I'm not sure how in general one would go about finding $\theta_k$.  In fact, by trial and error I've persuaded myself that -- again, for $k \notin I$ -- we have $$ c^{[k]}_n \sim \frac{k-2}{k-1} \left( \frac{k^2}{k-1} \right)^n $$ but again I'm not sure how to prove this to be true. The remaining cases should, I assume, be of the form $c^{[k]}_n \sim \theta_k(n) 4^n$.  This is true for $k=1$ and $k=2$ (shown above).  And it's apparently true for the case $k=-1$, as Fung Lam (on the OEIS) gives the estimate $$c^{[-1]}_n \sim - \frac{4^n}{9 \sqrt{\pi n^3}}.$$ I've not been able to even guess what the subexponential term should be for the remaining three cases. In summary then, I really have two questions: Are the particular estimates for the asymptotic growth of $c^{[k]}_n$ that I suggest above correct?  Is anything known about the cases $k \in \{-4,-3,-2\}$? In general, how can I derive expressions for the asymptotic behaviour of an integer sequence if I know its generating function?  (I suspect what I'm really looking for here is the name of a useful textbook...)","['generating-functions', 'combinatorics', 'asymptotics', 'catalan-numbers']"
2388494,Specific Integral Question: $\int_0^1(f'(t))^2dt \geq 3(\int_0^1f(t)dt)^2$,"I'm in the midst of studying for my real analysis final tomorrow and I came across this question in a practice final and am stumped on how to even start it. Let $f: [0,1] \rightarrow \mathbb{R}$ be a differentiable function such that $f'$ is continuous and $f(1)=0$. Prove that the following inequality holds: $\int_{0}^{1}(f'(t))^2dt \geq 3(\int_{0}^{1}f(t)dt)^2$. Any guidance on how to start would be of help..","['real-analysis', 'integration']"
2388506,Lacunary functions and sums of reciprocals,"Let $\Lambda=\left\{ \lambda_{n}\right\} _{n=0}^{\infty}$ be an infinite, strictly increasing sequence of non-negative integers. I say that $\Lambda$ is reciprocal-summable if: $$\sum_{\lambda\in\Lambda\backslash\left\{ 0\right\} }\frac{1}{\lambda}<\infty$$ Now, in a slight abuse of terminology, I'm going to use the term “lacunary” to refer only to those holomorphic functions on $\mathbb{D}$ for which the unit circle is a natural boundary (such as $\sum_{n=0}^{\infty}z^{2^{n}}$). The only functions I'm concerned with are those for which the power series coefficients are $0$s and $1$s. I've been reading up on the known criteria (and converses) for when a function is lacunary (results of Fabry, Hadamard, Pólya, etc.). However, many (if not most) of them try to go from the most general point of view they can find, and so, I can't seem to find the exact details / answers that I'm looking for. The claims which I would like to have answered (either partially or entirely) and/or be pointed toward a counterexample of are as follows: I. If $\Lambda$
  is reciprocal summable, then $\sum_{n=0}^{\infty}z^{\lambda_{n}}$
  is lacunary. II. If $\Lambda$
  is not reciprocal-summable, then $\sum_{n=0}^{\infty}z^{\lambda_{n}}$
  is not lacunary. If any of the results (or predecessors thereof) of Hadamard, Fabry, Pólya (etc.) imply one or more of these claims, an explanation of how they do so would be much appreciated. Thanks in advance!","['complex-analysis', 'sequences-and-series', 'lacunary-series']"
2388534,A identity which looks like the binomial identity,"Let $f_0(x)=1$, $f_k(x)=x(x-1)\cdots(x-k+1)$, $\forall k\geq1$. For any positive integer $n$, how to prove the following identity:
$$f_n(x+y)=\sum_{k=0}^n\binom{n}{k}f_k(x)f_{n-k}(y)?$$","['linear-algebra', 'analysis']"
2388541,Proving that a Banach space is separable if its dual is separable,"I am currently reading through the proof of the following result: If the dual of a Banach space $X$ is separable, then $X$ is separable. Proof : Let $\{ f_n \}_{n=1}^{\infty}$ be a dense subset of the unit ball $\mathcal{B}$ in $X^{\ast}$. For each $n \in \mathbb{N}$, pick $x_n \in X$ such that $f_n(x_n) > \frac{1}{2}$. Let $Y = \overline{\text{span}\{x_n\}}$ and observe that $Y$ is separable, since finite rational combinations of $\{ x_n \}$ are dense in $Y$. It is now sufficient to show that $X=Y$. We proceed by contradiction. Suppose that $X \neq Y$. Then there is an $f \in X^{\ast}$, with $\| f \| =1$ such that $f(x) =0$ for all $x \in Y$. Now choose $n$ such that $\| f_n - f \| < \frac{1}{4}$. Then \begin{eqnarray*}
\left| f(x_n) \right| & = & \left| f_n(x_n) - f_n(x_n) + f(x_n) \right| \\
& \geq & \left| f_n(x_n) \right| - \left| f_n(x_n) - f(x_n) \right| \\
& \geq & \left| f_n(x_n) \right| - \| f_n - f \| \cdot \| x_n \| \\
& > & \frac{1}{2} - \frac{1}{4} = \frac{1}{4}. 
\end{eqnarray*} I understand the mechanics of this proof, but want to understand the rationale behind the structure of the proof. So, we want to start with a countably dense subset of the dual, and show that this necessitates the existence of a countably dense subset of $X$. 1. Why choose the dense subset to be on the unit ball? 
 2. Is there any foresight that one may grasp on thinking of a Banach space as the
 closure of the span of a sequence on the unit ball of the dual? 
 3. I do not see where the assumption of f(x)=0 is used in the inequality. Does anyone have a more intuitive proof of this result?","['functional-analysis', 'general-topology', 'banach-spaces', 'separable-spaces']"
2388554,If $f$ is entire and $|Re(f)||Im(f)|$ is bounded then $f$ is constant,"I want to prove the statement ""If $f$ is entire and $|Re(f)||Im(f)|$ is bounded then $f$ is constant"". I could not solve this but here are some of my ideas. If we write $f=u+iv$, the $f^2=(u^2-v^2)+i(2uv)$ which is also entire. Therefore, $f^2$ is an entire function with a bounded imaginary part, which I assume means that $f^2$ is constant. Hence, $f$ is constant. I know this is not a proof but this is what I have. Can anyone provide a proper proof? Thank you","['complex-analysis', 'entire-functions']"
2388567,"Does the path graph have least algebraic connectivity among simple, undirected, connected graphs?","Let $G = (V, E)$ be a simple, undirected, unweighted graph on $n$ nodes (all uses of the word ""graph"" in this question refer only to graphs with these properties). Define its Laplacian as
\begin{equation}
L(G) = D(G) - A(G),
\end{equation}
where $D(G)$ and $A(G)$ are, respectively, the degree and adjacency matrices associated with $G$. The second-smallest eigenvalue of $L(G)$, often denoted $\lambda_2(L(G))$ or simply $\lambda_2$ when $G$ is understood, was termed its algebraic connectivity by Fiedler in seminal work $[1]$ which showed that a graph $G$  is connected if and only if $\lambda_2 > 0$. For any fixed $n \in \mathbb{N}$, there are a finite number of graphs on $n$ nodes (without worrying about exactly how many distinct possible graphs there are, this number is clearly bounded above by $2^n$), and there are likewise a finite number of connected graphs on $n$ nodes. My question is the following. Which simple, undirected, unweighted, connected graph on $n$ nodes minimizes the algebraic connectivity among all such graphs on $n$ nodes? My suspicion is that the answer to this question is the path graph, thought I haven't been able to confirm this. It is known (e.g., from $[1]$) that the path graph on $n$ nodes has algebraic connectivity $\lambda_2 = 2\left(1 - \cos\frac{\pi}{n}\right)$, and I've done quite a few numerical experiments in generating graphs and trying to find one with a smaller value of $\lambda_2$, though I have not yet found such a graph for any value of $n$. I've looked in Godsil and Royle, and while there was obviously plenty on connectivity of graphs, I couldn't seem to find the answer to this question. I've also done some Googling with search terms like ""least connected graph"" and ""minimum algebraic connectivity of graphs,"" but that didn't turn anything up either. [1] M. Fiedler, ""Algebraic Connectivity of Graphs,"" Czechoslovak Mathematical Journal , 1973, vol. 23, no. 2, pp. 298-305.","['combinatorics', 'graph-theory', 'algebraic-graph-theory', 'spectral-graph-theory']"
2388573,Making the rigorous link between the conceptual interpretation of curl and the formula,"EDIT: I would like to clarify for potential new readers what the gist of this question is, and how it differs from other questions about curl on this site. Namely, I'm confused about: The specifics of the physical interpretation of curl (is it infinitesimal torque, average infinitesimal torque, average infinitesimal angular acceleration , etc.) How this specific physical interpretation translates into a formula (ideally, the integral below as seen on Wikipedia) How this formula becomes the usual definition of curl. I'm not confused about what curl is on a general level, I'm confused about why the formula for curl coincides with the physical interpretation , on an intuitive and rigorous basis. I'm having a bit of difficulty making the connection between the intuitive idea of the curl of a vector field (as the torque experienced on an infinitesimally small paddle wheel placed at a point in the vector field), and the formula for it (I'll stick to 2-D first off, which is how the MIT OpenCourseware lectures define it at first): $$
\nabla\times F = N_x-M_y
$$ where $F(x,y)=(M(x,y),N(x,y))$. (Yes this is a scalar whereas curl is normally a vector, but I interpret this definition of curl to be the signed magnitude of the torque, since the direction is perpendicular to the $xy$ plane for all 2-D fields). I can accept that this formula aligns with the idea of ""infinitesimal torque"", but I'd like to see it in action in a rigorous setting. So, I asked myself, what would the torque be at a point in a vector field? If we were to place a non-infinitesimal paddle wheel (a circle of radius $r$) centered at $(x_1, y_1)$, it would be experiencing forces at each point in its area. If $\vec{r}$ is the position vector from the center of the wheel to where the force is exerted, then the torque is $\vec{r}\cdot\vec{F}$. The net torque is just the sum of all the torques, so the net torque experienced by the paddle wheel should be: $$
\tau=\mathop{\iint}_{(x-x_1)^2+(y-y_1)^2\le r^2}(x-x_1,y-y_1)\cdot F(x,y)\ \text{d}A
$$ If we want to find the infinitesimal torque, we would supposedly take the limit as $r\to 0$. However, this doesn't make sense, as this would imply that the infinitesimal torque is zero (since the integral goes to zero). What's missing? I assume we needed some sort of limiting factor to divide by in the limit. I know from physics that $\tau = \alpha I$ where $I$ is the moment of inertia (for our 2-D paddle wheel with unit density, $I=\frac\pi2r^4$). So maybe curl is actually the infinitesimal angular acceleration? If so, then we have $$
\nabla\times F = \lim_{r\to0}\frac{2}{\pi r^4}\mathop{\iint}_{(x-x_1)^2+(y-y_1)^2\le r^2}(x-x_1,y-y_1)\cdot F(x,y)\ \text{d}A
$$ This looks similar but not quite what the formula on Wikipedia is: $$
(\nabla\times F)\cdot\mathbf{\vec{n}} = \|\nabla\times F\| = \lim{A\to 0}\frac1{|A|}\oint_C\mathbf{F}\cdot d\mathbf{r}
$$ Why, here, is the area $|A|$ the limiting factor? That doesn't make much sense to me. Is curl then the average infinitesimal torque, averaged over an infinitesimal area (since $|A|\to 0$)? Wikipedia also says that the curl is the ""circulation density "", rather than simply the torque. So is my initial interpretation of the curl wrong? Furthermore, what change would I have to make to my interpretation to go from my integral to the integral on wikipedia (since the integrands are different and I'm not sure what to do about that), and then how do you go from the integral to the initial formula for curl? Any and all help would be appreciated :)","['multivariable-calculus', 'vector-fields']"
2388577,Is the IVT equivalent to completeness?,"Obviously we can use the completeness of the real numbers (least upper bound axiom, or one of the equivalent principles) to prove the IVT. Can we go in the opposite direction? This isn't a homework problem or something. I'm just wondering. If the answer is ""yes"", then I'm not really asking for much explanation. A reference, or a place to look if I'm stuck, will do.",['real-analysis']
2388580,Trivial connections on trivial bundles,"In Cliff Taubes' book ""Differential geometry"", he explains how to define the trivial connection on the product principal bundle $U \times G,$ where $G$ is a Lie group.  Namely, the trivial connection $A_0$ is a Lie(G)-valued 1-form which is defined at the point $(p, g) \in U \times G$ by $g^{-1}dg.$ I am confused about what $dg$ is supposed to mean. Possible interpretation: we could view $g$ as the identity function on the underlying Lie group.  Then $dg$ is just the differential, which takes a tangent vector $X \in T_gG$ to itself. We view $g^{-1}$ as the differential of the group multiplication map $h \mapsto g^{-1}\cdot h.$  Then $g^{-1} dg$ maps $T_gG \to T_eG$ and hence is indeed a Lie algebra value 1-form. However, this interpretation seems strange because it treats both $dg$ and $g^{-1}$ as differentials, even though the $d$ is missing from the second symbol. Is my interpretation wrong? If so, could someone provide a correct interpretation?","['fiber-bundles', 'differential-topology', 'gauge-theory', 'differential-geometry', 'connections']"
2388611,Theorems of the circumference (geometry),"I was studying the circumference and theorems of that in my book. There are two theorems that I wanted to demonstrate, but, when I did it, it somehow disappointed me and I´d like to know how you´d prove it, if you need me to give my demonstration it´s ok, but I don´t, I think It could demonstrated in other ways but don´t know how. I´ll show them in pictures. I´d also like to know how to prove the theorem that says, that if I have a tangent line, that touches the circumference in $P$, is perpendicular to radius drawn from the centre that point $P$. Thanks in advance.","['alternative-proof', 'geometry']"
2388622,Complex derivative vs. Multivariate derivative,"I'd like to compare and contrast the derivative across different areas of mathematics just to organize ideas in my head. In this question, $f(x,y)$ means $f: \mathbb{R}^2 \to \mathbb{R}$ and $f(z)$ means $f: \mathbb{C}\to\mathbb{C}$ Limits In multivariate calculus ($f(x,y)$), in order to find the limit of $f(x,y)$ as $(x,y) \rightarrow (x_0,y_0)$, you need to approach $(x_0, y_0)$ from all possible directions in the $xy$ plane. Likewise in complex analysis, in order to find the limit of $f(z)$ as $z \rightarrow z_0$, you need to approach $z_0$ along every path and check the limit. Derivatives In multivariate calculus, you usually talk about a derivative along a single direction. For instance, the partial derivative with respect to $x$ is given by $$\frac{\partial f}{\partial x} = \lim_{h\to\ 0} \frac{f(x+h,y) - f(x,y)}{h}$$ This is the derivative parallel to the x-axis. You can generalize this to a derivative along any straight line direction. It's called a directional derivative. You can further generalize this to the derivative along any arbitrary path (not just straight lines) in your domain. However the two are sort of the same because at any one point on the arbitrary path, the derivative you compute there is a directional derivative at that instant of the path. Anyways, the point is to show that derivatives are taken with respect to directions. For complex-valued functions of a complex variable, the derivative is $$\frac{df(z)}{dz} = \lim_{h\to\ 0} \frac{f(z+h) - f(z)}{h}$$
with the understanding that h is a complex number that approaches $0$ along any path in the complex plane. So the derivative must be checked along every path and every path has to yield the same value of the limit in order for the derivative to exist. Question Why isn't there such notion of a derivative for multivariate functions $f(x,y)$? The domain of a function $f(z)$ is the complex plane. It's a plane. The domain of a function $f(x,y)$ is the $xy$ plane. Also a plane. Why can't I ask for the derivative of $f(x,y)$ at a point P where I check all possible paths as $(x,y) \rightarrow P$ just as I did for the derivative of $f(z)$? Why must I always take the derivative in a specific direction? Likewise, why can't you ask for the derivative of $f(z)$ in a certain direction at $z$? My thoughts Although a complex variable can take values in a complex plane, it's still a '1 dimensional number.' $z = x + iy$ is usually said to be a '2 dimensional number', but this is from the perspective of real numbers (1D) and generalizing to complex numbers (2D). However if I move the starting point in my brain to complex numbers, then a complex number is just a 1D number. Complex numbers live on a plane, but the plane should be thought of as 1D. Therefore just as checking the derivative of a single variable function $f(x)$ as $h$ goes to $0$ on 'all possible paths' is normal, checking the derivative of $f(z)$ as $h$ goes to $0$ on 'all possible paths' is normal because there aren't any paths (the plane is 1D). I can't ask for the $f'(z)$ along a particular direction because there is 'no direction' just as there is really no direction for a 1D real number line. However, I'm still confused why you can't ask for $f'(x,y)$ without specifying a direction. $f'(x,y)$ is always given with respect to a direction.","['multivariable-calculus', 'complex-analysis', 'calculus']"
2388647,Proof that $L^{p}$ is complete in Folland's Real Analysis,"The following is from Folland's Real Analysis, Theorem 6.6. The theorem states: $\underline{\text{Theorem } 6.6:}$ For $1 \leq p < \infty$, $L^{p}$ is a Banach space. The proof makes use of Theorem 5.1 which is as follows: $\underline{\text{Theorem } 5.1:}$ A normed vector space $X$ is complete iff every absolutely convergent series in $X$ converges. $\underline{\text{The Proof of Theorem 6.6}:}$ We use Theorem 5.1.  Suppose $\left \{ f_{k} \right \} \subset L^{p}$ and $\displaystyle \sum_{k=1}^{\infty} ||f_{k}||_{p} = B < \infty$. Let $\displaystyle G_{n} = \sum_{k=1}^{n} |f_{k}|$ and $\displaystyle G = \sum_{k=1}^{\infty} |f_{k}|$. Then \begin{equation*}
\begin{aligned}
||G_{n}||_{p} =  \big( \int \big| \sum_{k=1}^{n} |f_{k}|  \big|^{p} \big)^{1/p} \\
= \big( \int (|f_{1}| + \dots + |f_{n}|)^{p} \big)^{1/p} \\
\leq \big( \int |f_{1}|^{p} + \dots + \int |f_{n}|^{p} \big)^{1/p} \\
\leq \big( \int |f_{1}|^{p} \big)^{1/p} + \dots + \big( \int |f_{n}|^{p} \big)^{1/p} \\
= \sum_{k=1}^{n} ||f_{k}||_{p} \\
\leq B
\end{aligned}
\end{equation*} for all $n$. Therefore, by the Monotone Convergence Theorem (since $\displaystyle \sum_{k=1}^{n} |f_{k}| \leq \sum_{k=1}^{n+1}|f_{k}|)$ \begin{equation*}
\int G^{p} = \lim \int G_{n}^{p} \leq B^{p}
\end{equation*} Thus, $G \in L^{p}$, and in particular, $G(x) < \infty$ a.e., implying that the series $\displaystyle \sum_{k=1}^{\infty} f_{k}$ converges a.e. Denoting its sum by $F$, we have $|F| \leq G$ and hence $F \in L^{p}$. Moreover, \begin{equation*}
\begin{aligned}
|F - \sum_{k=1}^{n} f_{k}|^{p} \leq (2G)^{p} \in L^{1}
\end{aligned}
\end{equation*} Thus, by the Dominated Convergence Theorem, \begin{equation*}
\begin{aligned}
\lim_{n \rightarrow \infty} ||F - \sum_{k=1}^{n} f_{k}||_{p}^{p} = \lim_{n \rightarrow \infty} \int \big| F - \sum_{k=1}^{n} f_{k} \big|^{p}  = \int \lim_{n \rightarrow \infty} \big| F - \sum_{k=1}^{n} f_{k} \big|^{p} = 0
\end{aligned}
\end{equation*} Thus ,the series $\displaystyle \sum_{k=1}^{\infty} f_{k}$ converges in the $L^{p}$ norm. This completes the proof. My question is why does this show that every absolutely convergent series in $X$ converges (thus allowing us to apply Theorem 5.1 thereby completing the proof)? Doesn't $\lim_{n \rightarrow \infty} ||F - \sum_{k=1}^{n} f_{k}||_{p}^{p} = 0$ in fact show that the series merely converges in $L^{p}$? Not converge absolutely in $L^{p}$?","['functional-analysis', 'real-analysis', 'lp-spaces']"
2388689,"Let $a_{1}>0,a_{2}>0$ and $a_{n}=\frac{2a_{n-1}a_{n-2}}{a_{n-1}+a_{n-2}}, n>2$, then $\{ a_{n}\}$ converges to $\frac{3a_{1}a_{2}}{a_{1}+a_{2}}$.","Let $a_{1}>0,a_{2}>0$ and $a_{n}=\dfrac{2a_{n-1}a_{n-2}}{a_{n-1}+a_{n-2}}, n>2$, then $\{ a_{n}\}$ converges to $\dfrac{3a_{1}a_{2}}{a_{1}+a_{2}}$. My attempt :
  \begin{align}
  a_{n} &= \frac{2a_{n-1}a_{n-2}}{a_{n-1}+a_{n-2}} \\
  &= \frac{2}{\dfrac{1}{a_{n-1}}+\dfrac{1}{a_{n-2}}} \\
 & \le \frac{1}{\sqrt{a_{n-1}a_{n-2}}}
\end{align} I used AM- GM inequality here. I am not able to proceed further.  How to solve the question? Please help me.","['recurrence-relations', 'real-analysis', 'sequences-and-series', 'limits']"
2388701,Solving $\frac{1}{(x-1)} - \frac{1}{(x-2)} = \frac{1}{(x-3)} - \frac{1}{(x-4)}$. Why is my solution wrong?,"I'm following all hitherto me known rules for solving equations, but the result is wrong. Please explain why my approach is not correct. We want to solve: $$\frac{1}{(x-1)} - \frac{1}{(x-2)} = \frac{1}{(x-3)} - \frac{1}{(x-4)}\tag1$$ Moving the things in RHS to LHS: $$\frac{1}{(x-1)} - \frac{1}{(x-2)} - \frac{1}{(x-3)} + \frac{1}{(x-4)} = 0\tag2$$ Writing everything above a common denominator: $$\frac{1}{(x-4)(x-1)(x-2)(x-3)}\bigg[(x-2)(x-3)(x-4) - (x-1)(x-3)(x-4) - (x-2)(x-1)(x-4) + (x-1)(x-2)(x-3)\bigg] = 0\tag3$$ Multiplying both sides with the denominator to cancel the denominator: $$(x-2)(x-3)(x-4) - (x-1)(x-3)(x-4) - (x-2)(x-1)(x-4) + (x-1)(x-2)(x-3) = 0\tag4$$ Multiplying the first two factors in every term: $$(x^2-3x-2x+6)(x-4) - (x^2-3x-x+3)(x-4) - (x^2-x-2x+2)(x-4) + (x^2-2x-x+2)(x-3) = 0\tag5$$ Simplifying the first factors in every term: $$(x^2-5x+6)(x-4) - (x^2-4x+3)(x-4) - (x^2-3x+2)(x-4) + (x^2-3x+2)(x-3) = 0\tag6$$ Multiplying factors again: $$(x^3-4x^2-5x^2+20x+6x-24) - (x^3-4x^2-4x^2+16x+3x-12) - (x^3-4x^2-3x^2-12x+2x-8) + (x^3-3x^2-3x^2+9x+2x-6) = 0\tag7$$ Removing the parenthesis yields: $$x^3-4x^2-5x^2+20x+6x-24 - x^3+4x^2+4x^2-16x-3x+12 - x^3+4x^2+3x^2+12x-2x+8 + x^3-3x^2-3x^2+9x+2x-6 = 0\tag8$$ Which results in: $$28x - 10 = 0 \Rightarrow 28x = 10 \Rightarrow x = \frac{5}{14}\tag9$$
which is not correct. The correct answer is $x = \frac{5}{2}$.",['algebra-precalculus']
2388723,"Do the simultaneous equations $4x+3y = 3$ and $6y-6 = -8x$ (which are basically the same equation) have ""infinite solutions""?","I was thinking of a 'stupid question' this morning. How many solutions do these simultaneous equations have?
$$4x+3y = 3$$
$$6y-6 = -8x$$ Basically they are the same equation. But should I say they have infinite solutions or? What's the correct term should I use?",['algebra-precalculus']
2388730,Generating a countable mutually disjoint collection of arbitarally small measurable sets.,More specifically Assume that X has a collection of arbitarally small measurable sets. Is it possible to then generate a countable mutually disjoint collection of arbitarally small measurable sets. It is obvious that you can make a countable collection. It isn't so obvious that you can make a disjoint collection.,"['real-analysis', 'measure-theory']"
2388744,"How might one actually prove, in a rigorous way, that the area of a parallelogram is given by the magnitude of the cross product?","In $3$-dimensions, we can compute areas of a parallelograms using the cross product, like so: $$H_2(\{ax+by : a \in [0,1],b \in [0,1]\}) = |x \times y|,$$ where $H_2$ is the $2$-dimensional Hausdorff measure. Of course, it's rarely formalized like this. So much of elementary geometry is never formalized. In any event, the above formula seems like the most reasonable interpretation of this well-known fact. Question. How would you actually prove such a formula rigorously? By ""prove rigorously"" I do not mean draw a persuasive geometric diagram and make a reasonable-sounding argument. Rather, I mean something that could, in principle at least, be formalized in a sufficiently powerful interactive theorem prover one day. A diagram is fine, but please explain how you would actually implement the proof implicitly contained in that diagram.","['cross-product', 'measure-theory', 'geometry']"
2388747,Formula for ellipse formed from projecting a tilted circle onto the xy plane,"A circle is drawn in the center of a piece of paper.  The piece of paper is placed on a table and a camera is positioned directly overtop to look at the circle. A piece of glass is placed between the camera and the piece of paper and is parallel to the piece of paper.  A cartesian plane is drawn onto the piece of glass with point (x1, y1) marking the center of the circle on the paper below. A point on the circumference of the circle is chosen and its (x,y) coordinate on the piece of glass is marked. The paper is then tilted about the line y=x1 (the horizontal line through the center of the circle). Note that as the paper is tilted the camera will observe an ellipse on the piece of glass: the projection of the circle onto the piece of glass, and the center point of the ellipse will still be (x1, y1). However, this is magic paper: as the paper tilts the circle drawn on the piece of paper grows or shrinks such that the projection of the circle onto the piece of glass always intersects the point (x, y). What is the formula that describes the projected ellipse on the piece of glass given the tilt angle theta, the point (x, y), the circle’s center coordinates (x1, y1)? Note that we do not know the radius of the original circle. The motivation for this question is that I am working on an mobile application that renders a map onto the screen.  The user's current location is marked with a blue dot.  I want the blue dot to be placed near the bottom-left corner of the screen.  But the dot itself cannot be moved: I must instead specify the latitude/longitude to be used as the center of the map and I need to calculate the map center to use in order to locate the blue dot at the desired location. When the phone is rotated then the map needs to rotate about the blue dot (not about the center of the map).  The math to figure this out is easy if the ""camera"" is looking at the map head-on (tilt=0) as I just need to trace the circumference of the circle centered at the center of the map that goes through the desired (x,y) position of the blue dot; however, if the map is tilted then I need to instead trace the circumference of the ellipse passing through the blue dot, the ellipse formed by projecting a circle lying on the plane of the ground onto the screen, and I can't seem to figure out the formula for this ellipse.","['conic-sections', 'trigonometry', 'projective-space']"
2388755,Solve $y’’ + 2y’ +y = xe^{-x}$,"So my questions is to solve: $y’’ + 2y’ +y = xe^{-x}$ The general solution is: $(Ax + B)e^{-x} = Axe^{-x} + Be^{-x}$. To Find the particular solution we could assume a solution of the form $y_p = Axe^{-x}$ but we see that this solution would be covered by the general solution. Instead we try to use $Ax^2e^{-x}$. What is the intuition behind multiplying with $x$? Why not add a polynomial, a sine or something else? Solving for $A$ gives me $\frac{1}{2}x^2e^{-x}$. WolframAlpha suggests $\frac{1}{6}x^3e^{-x}$ as the correct answer. Should I have looked for a solution of the form $Ax^3e^{-x}$ and multiplied $Axe^{-x} \times x^2$? What would be the intuition / reason for that?",['ordinary-differential-equations']
2388770,Largest prime factors of two consecutive natural numbers,"Let $u(n)$ be the numbers of positive integers $k \le n$ such that the largest prime factor of $k+1$ is greater than the largest prime factor of $k$. Similarly, let $l(n)$ be the numbers of positive integers $k \le n$ such that the largest prime factor of $k+1$ is smaller than the largest prime factor of $k$. As we go higher up the number line, the density of primes decreases in accordance with the Prime Number Theorem. Since more natural numbers to be composed of small prime factors than with large prime factors I expected $u(n)$ to be much greater than $l(n)$. However, computation showed that their difference is much lesser than I expected. In fact,we have:
$$
u(10000) = 5008, l(10000) = 4991
$$
$$
u(100000) = 50079, l(100000) = 49920
$$ In other words, even though density of primes thins out, natural numbers are such that the largest prime factors of two consecutive natural numbers are almost equally likely to greater of less than one another. Question : What is the expected gap between $u(n)$ and $l(n)$ as per the Prime Number theorem?","['number-theory', 'prime-factorization', 'prime-numbers', 'elementary-number-theory']"
2388800,"Prove: if two homogeneous systems of linear equations in two unknowns have the same solutions, then they are equivalent.","From Exercise 6 of Sec 1.2 of Linear Algebra by K.Hoffman and R.Kunze. Equivalence is defined as follows: Two systems of linear equations are equivalent if each equation in
  each system is a linear combination of the equations in the other
  system. I did some research, but most answers I found on math.stackexchange.com either use some serious math like ranks or null spaces, or are merely ""intuitive"". I'm curious how to formalize the proof, with concepts previously defined. Thx. More specifically -- $A_{11}x_1+A_{12}x_2=0$, $A_{21}x_1+A_{22}x_2=0$; $B_{11}x_1+B_{12}x_2=0$, $B_{21}x_1+B_{22}x_2=0$; We are asked to find $c_{11},c_{12},c_{21},c_{22}$ such that: $A_{11}c_{11}+A_{21}c_{12}=B_{11}$; $A_{12}c_{11}+A_{22}c_{12}=B_{12}$; $A_{11}c_{21}+A_{21}c_{22}=B_{21}$; $A_{12}c_{21}+A_{22}c_{22}=B_{22}$; (Sorry about the mess...I don't know how to type systems of equations...) I can't convince myself in formal language the existence of such $c$s. Since this problem appears at the beginning of the book, I assume there's a rather rookie proof. I suppose I have to express the $c$s in terms of the given quantities. But how?",['linear-algebra']
2388863,Help understand a connection between probabilities of dice outcomes and number of elements of a hypercube,"I found a curious connection between probabilities of dice outcomes and number of elements of a hypercube and I can't make sense of it. I was trying to calculate odds of getting $0,1,..,x$ threes when rolling $x$ (hypothetical) 3-sided dice, i.e. when rolling 5 3-sided dice, what are the odds 2 of them will land on threes etc. The (AFAICT correct) odds are (for 0,1,2,3,4,5 threes respectively): 1 dice - $2/3$, $1/3$ 2 dice - $4/9$, $4/9$, $1/9$ 3 dice - $8/27$, $12/27$, $6/27$, $1/27$ 4 dice - $16/81$, $32/81$, $24/81$, $8/81$, $1/81$ 5 dice - $32/243$, $80/243$, $80/243$, $40/243$, $10/243$, $1/243$ Compare it with number of elements in x-dimentional cube (from wikipedia ): 1-cube - 2 vertices, 1 edge 2-cube - 4 vertices, 4 edges, 1 face 3-cube - 8 vertices, 12 edges, 6 faces, 1 cell 4-cube - 16 vertices, 32 edges, 24 faces, 8 cells, 1 4-face 5-cube - 32 vertices, 80 edges, 80 faces, 40 cells, 10 4-faces, 1 5-face This continues as least to 10 dice/dimensions. Same thing happens with 2-side dice (aka coins) and hyper-tetrahedron. Why do probabilities of rolling a certain dice face $y$ number of times on $x$ dice would match a number of elements of a $x$-dimentional shape like that?","['probability', 'dimension-theory-analysis']"
2388887,Is there an analytic expression for a number of elements inside a triangular matrix (with and without items on diagonal),"Is there an analytic expression for a number of items inside a triangular matrix (with and without items on diagonal)? I tried to solve this with a combinatorial analysis using this ""representation"" of the problem: ( _, _ ) = ( n, m ) where: ""n"" is the matrix row index, ""m"" is the matrix column index. m = { 0, 1, 2, ... n } but I don't have any further idea (I do not understand combinatorial analysis very well)",['matrices']
2388890,Laplace Transform - Why it works,"The definition of the Laplace Transform is $$F(s) = \int^{\infty}_0 f(t)e^{-st}  dt$$ It is very useful in terms of solving linear, constant coefficient ordinary differential equations, but why exactly does it work? Why does taking the Laplace transform of each term in in the differential equation, using the Laplace Transform's linearity to get each individual term in the differential equation in its own Laplace Transform work in solving differential equations? Why does it work? I am not so sure why I am hesitant to accept this. Having seen generating functions of recursive sequences as a way to describe the sequence of numbers  and aids in obtaining a (closed) formula for the recursive sequence - this is a similar analogy, we are essentially ""transforming"" sequences to a power series, but I am more accepting of this idea than the Laplace transform.","['ordinary-differential-equations', 'laplace-transform']"
2388891,Difference between 'free algebra' and 'absolutely free algebra',What is the difference between a free algebra and an absolutely free algebra? Wikipedia and Encyclopaedia of Maths are not very clear on the subject...,"['universal-algebra', 'abstract-algebra']"
2388899,"$\operatorname{Tor}_1^\mathbb{Z}(U,U)=0$. Is $U$ torsion-free?","Let $U$ be a $\mathbb{Z}$ -module with $\operatorname{Tor}_1^\mathbb{Z}(U,U)=0$ . Does this imply that $U$ is torsion-free? I know that if $\operatorname{Tor}_1^\mathbb{Z}(U,M)=0$ for every $\mathbb{Z}$ -module $M$ , then $U$ is called flat (by definition), but this is stronger than $\operatorname{Tor}_1^\mathbb{Z}(U,U)=0$ . On the other hand, flat implies torsion-free. However, I have no idea how to attack the question. I appreciate any help.","['modules', 'abstract-algebra', 'torsion-groups', 'homological-algebra', 'abelian-groups']"
