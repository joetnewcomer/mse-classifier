question_id,title,body,tags
2680899,how to find the value of the $k$ to make $\lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=A$ exist?,"the question described as follow:
$$\lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=A$$
the $A$ is constant and $A\not=0$ and find the $k$ to make this limit exist. and I did this: $$let\space t\space be\space x^{1/15}\space, then \space x=t^{15}.$$
$$then \space \lim_{x\to 0} \frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{k}}=\lim_{x\to 0} \frac{\sqrt[5]{t^{15}+t^{5}}-\sqrt[3]{t^{15}+t^{3}}}{t^{15k}}.$$
$$then \space \lim_{x\to 0} \frac{\sqrt[5]{t^{15}+t^{5}}-\sqrt[3]{t^{15}+t^{3}}}{t^{15k}}=\lim_{x\to 0} \frac{t\sqrt[5]{t^{10}+1}-t\sqrt[3]{t^{12}+1}}{t^{15k}}.$$ use taylor expantion:
$$then \space \lim_{x\to 0} \frac{t\sqrt[5]{t^{10}+1}-t\sqrt[3]{t^{12}+1}}{t^{15k}}=\lim_{x\to 0} \frac{t(1+\frac{1}{5}t^{10}+o(t^{10}))-t(1+\frac{1}{3}t^{12}+o(t^{12}))}{t^{15k}}.$$ $$then \space\lim_{x\to 0} \frac{t(1+\frac{1}{5}t^{10}+o(t^{10}))-t(1+\frac{1}{3}t^{12}+o(t^{12}))}{t^{15k}}=\lim_{x\to 0} \frac{\frac{1}{5}t^{11}+o(t^{11})}{t^{15k}}.$$ $$then \space\lim_{x\to 0} \frac{\frac{1}{5}t^{11}+o(t^{11})}{t^{15k}}=\lim_{x\to 0} \frac{\frac{1}{5}t^{11}}{t^{15k}}=\lim_{x\to 0} \frac{1}{5t^{15k-11}}=A.$$
since A is a non-zero constant, so the  $t^{15k-11}$ should be $t^{0}=1$. then we get $15k-11=0$ and finally, we find $k=\frac{11}{15}$. Am I right? suppose I was right.
but unfortunately I found the image of $f(x)=\frac{\sqrt[5]{x+\sqrt[3]{x}}-\sqrt[3]{x+\sqrt[5]{x}}}{x^{\frac{11}{15}}}$ in quick graph app. the value of $f(0)$ goes to $\infty$ instead any constant. which is wrong, the app or me? if I was wrong, how to find the right k?",['limits']
2680914,Inverse of symmetric matrix plus identity matrix,"Consider the symmetric, positive definite matrix $\mathbf{A}$. I'd like to find a general form for $$(\mathbf{I} + \mathbf{A})^{-1}$$ that only involves $\mathbf{A}^{-1}$, i.e., no other inverse appears in the solution (as, for instance, in the Woodbury matrix identity). I've tried to derive the inverse by hand but I could only obtain a result up to he $4 \times 4$ case as follows. $2 \times 2$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} + \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \mathrm{tr}(\mathbf{A}) + 1}$$ $3 \times 3$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} - \mathbf{A} + \big( \mathrm{tr}(\mathbf{A}) + 1 \big) \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \mathrm{det}(\mathbf{A}) \mathrm{tr}(\mathbf{A}^{-1}) + \mathrm{tr}(\mathbf{A}) + 1}$$ $4 \times 4$: $$(\mathbf{I} + \mathbf{A})^{-1} = \frac{\mathrm{det}(\mathbf{A}) \mathbf{A}^{-1} + \mathbf{A}^2 - \mathrm{tr}(\mathbf{A}) \mathbf{A} - \mathbf{A} + \big( \tfrac{1}{2}(\mathrm{tr}(\mathbf{A})^2-\mathrm{tr}(\mathbf{A}^2)) + \mathrm{tr}(\mathbf{A}) + 1 \big) \mathbf{I}}{\mathrm{det}(\mathbf{A}) + \tfrac{1}{2}(\mathrm{tr}(\mathbf{A})^2-\mathrm{tr}(\mathbf{A}^2)) + \mathrm{det}(\mathbf{A}) \mathrm{tr}(\mathbf{A}^{-1}) + \mathrm{tr}(\mathbf{A}) + 1}$$ Can we find a general expression for higher dimensions that builds on the ones above? Even obtaining the $5 \times 5$ case is prohibitive, and so far I haven't been able to spot the pattern.","['matrices', 'inverse']"
2680932,Solutions with same points of inflection,"Given $y'' + p(t)y'+ q(t)y = 0$, where the functions $p(t)$ and $q(t)$ are continuous on the whole real line and suppose that $p(t) > 0$ and $q(t) > 0$ for all values of $t$. Is it possible that its two linearly independent solutions have a point of inflection for the same value of $t$?",['ordinary-differential-equations']
2680977,Calculus Puzzle Book?,"I'm looking for a puzzle book to get my Dad for his birthday, and he's very much a mathematician and loves puzzles. I'm trying to find essentially a compendium of challenging integrals (or maths in general). I'd be looking for questions around a Masters student's level, so quite challenging questions. I've come across 'Irresistible Integrals' and 'A treatise on the integral calculus; with applications, examples and problems' but neither are quite what I'm looking for, they're both very textbook-y. I'm trying to find something which isn't trying to teach calculus, but just straight up challenges with solutions. Questions similar to ones in, for example, the MIT Integration Bees. I tried looking for a collection of MIT Integration Bee questions, but couldn't find one. Thanks for reading, has anyone got any ideas? It'd be greatly appreciated.","['puzzle', 'recreational-mathematics', 'book-recommendation', 'calculus']"
2681098,"$a_1 = 1, a_{n+1} = \sin (a_n)$ Does the series $\sum a_n$ converge?","Suppose $a_1 = 1, a_{n+1} = \sin (a_n).$ Does the series $$\sum_{n=1}^\infty a_n$$ converge? First of all I got that $\lim_{n \to \infty} a_n = 0$ it's easy. Then I tried using all tests I know (The Cauchy root test, The Ratio test, etc.) and I failed to prove either convergence or divergence. Some of my results:
$$\lim_{n \to \infty} \frac{a_{n+1}}{a_n} = 1$$ $$\lim_{n \to \infty} \sqrt[n]{a_n} = 1$$ $$a_{n+1} < a_n$$ So, I don't know what to do. Could you please give me any hints about this problem? Thanks in advance!","['recurrence-relations', 'sequences-and-series']"
2681112,"Geometry sangaku puzzle, incribed circle circle/triangle/square","Hello
I am trying to solve a geometry puzzle, its been 30 years since I was in school and I struggled with maths! I would love to get some help to find out what the radius of the bigger circle is if the radius of the smaller circle ""乙"" is 3.06. Are you clever enough to figure this one out? What formula do you need? What is the radius?","['circles', 'sangaku', 'euclidean-geometry', 'geometry']"
2681139,determinant of a matrix with binomial coefficient entries,"I trying to prove a statement, which boils down to showing that the determinant of a specific matrix is nonzero. I use the convention that $\binom{n}{k} = 0$
if $k > n$ or $k < 0$. Let $k,l$ be natural numbers such that $k \le l$. Then the $n\times n$-Matrix $A$ is defined to have the entries $a_{ij} = \binom{l}{k +i - j}$. So it looks like 
$A = \left( \begin{array}{cccccc} \binom{l}{k} &  \cdots & \binom{l}{0} & 0 & \cdots & 0 \\
 \vdots & \ddots &  & \ddots & \ddots &  \vdots \\
\binom{l}{l} & & \ddots & & \ddots   & 0 
\\
0& \ddots && \ddots & & \binom{l}{0}\\
\vdots&\ddots&\ddots&&\ddots&\vdots\\
0&\cdots&0& \binom{l}{l} & \cdots & \binom{l}{k}
 \end{array}\right)$. 
Clearly the cases $k = l$ and $k = 0$ are trivial, since $A$ is then triangular. My first idea was to use the formula 
$\binom{r}{s} = \binom{r-1}{s-1} + \binom{r}{s}$ and add columns/rows to each other. But that does not work out that well... So if anyone has any ideas, or this matrix is known to be invertible, I would be very thankful.","['matrices', 'binomial-coefficients', 'determinant']"
2681148,Candy store with eggs,"The owner of a candy shop has 11 hollow chocolate eggs in his display, all of the same size but different weights of 1 lb, 2 lbs,… 11 lbs respectively. Each of them is marked with a different sticker, so that they are distinguishable. A customer, intending to buy one egg, claims that he knows all the individual weights. The owner, being rather suspicious, asks him to guess the egg that weighs 1 lb. For this reason, he gave him an empty plastic bag that can hold up to exactly 11 lbs, otherwise it will be torn and can’t be used. Please describe the strategy of the customer in order to demonstrate which one is the egg of 1 lb. What is the minimum number of uses of the plastic bag and which eggs will he put inside it in each use? With any 5 eggs the bag will be torn. With any 4 eggs, the bag will also be torn except from the cases 1,2,3,4 and 1,2,3,5. With any 3 eggs, the bag will be torn except from 16 out of 165 cases, of which only 4 do not contain the egg of 1 lb. With any 2 eggs, the bag will be torn except from 25 out of 55 cases (only 9 containing the egg of 1 lb) and obviously with only 1 egg it will not be torn. But how do we identify the egg of 1 lb in the least number of uses of the bag?",['combinatorics']
2681154,Proof Verification: $\lim{C^{1/n}}=1$ for $C>0$,"Claim: $\lim{C^{1/n}}=1$ for $C>0$ Working: 
$|C^{1/n}-1| < \epsilon$ $ \implies C^{1/n} < \epsilon +1$ $\implies {(1/n)}\ln{C} < \ln{(\epsilon +1)}$ $ \implies n> {\ln{C}/\ln{(\epsilon +1)}}$ Proof: Let $ \epsilon >0$ be given. Choose $N>{\ln{C}/\ln{(\epsilon +1)}}$ Then for any $n>N$,  this implies that  $|C^{1/n} -1|<\epsilon$ Hence, $\lim{C^{1/n}} =1$ for all $C>0$ Can anyone please verify this proof?","['real-analysis', 'sequences-and-series', 'proof-verification', 'limits']"
2681216,Simplify $\sum_{k=0}^n{n \choose k}^2(-1)^k$ [duplicate],"This question already has an answer here : Proving a Binomial Summation with Induction $\sum_{k=0}^n(-1)^k \binom nk^2$ [duplicate] (1 answer) Closed 4 months ago . I need to simplify this expression:$$\sum_{k=0}^n{n \choose k}^2(-1)^k$$ 
I know that $$\sum_k^n {n \choose k}^2 = {2n \choose n}$$ but that $(-1)^k$ spoils everything. I tried writing it like this: $$\sum_{k=0}^n{n \choose k}^2(-1)^k = {2n \choose n} - 2 \left({n \choose 1 }+ {n \choose 3} + \dots + {n \choose n} \right) $$ for odd $n$ and up to ${n \choose n-1}$ for even $n$ - but I am not able to sum the expression in parentheses. Also, I tried to use this relation: $$(1-x)^n(1+x)^n = (1-x^2)^n$$
Now, using the binomial coefficient and substituting n = 1 I got: $$\sum_{k=0}^n {n \choose k}(-1)^k = \left(\sum_{k=0}^n {n \choose k}\right) \left( \sum_{k=0}^n{n \choose k}(-1)^k\right)$$ 
Which can be distributed according to Cauchy theorem: $$= \sum_{k=0}^n \sum_{i=0}^k{k \choose i}^2(-1)^k$$ But this is still to no avail... Could you tell me if any of my methods can lead to solving this problem? If not, what other things can I try to solve it?","['combinatorics', 'binomial-coefficients', 'discrete-mathematics']"
2681329,Fun PDEs and exact solutions,I am building an PDE solver and would like some PDEs to test it on. However to test it works I want to compare it to exact solutions but I'm not having much luck finding these online. I currently have diffusion equation with time-varying boundaries wave equation with constant boundaries however I would like to also test diffusion equation with no-flux boundaries advection-diffusion equation with mixed boundaries advection-diffusion equation with constant boundaries kvd equation with any boundaries benjamin-bona-manhony equation with any boundaries however I am having trouble finding suitable initial conditions and an exact solutions to these. Any suggestions of initial conditions/boundary conditions/exact solutions would be very much appreciated. If you have any exciting equations you like I would also love to hear!,"['numerical-methods', 'ordinary-differential-equations', 'matlab', 'partial-differential-equations']"
2681358,Spectral families of commuting operators,"Consider two self-adjoint bounded operators $A$ and $B$ on a separable Hilbert space. According to the spectral theorem we can write
$$
A=\int_{-\infty}^{\infty} x d E^{A}_x, \quad B=\int_{-\infty}^{\infty} y d E^{A}_y
$$
where $E^{A}_x$ and $E^{B}_y$ are the spectral families of projectors of $A$ and $B$ respectively. Is there a simple way to prove that if $[A,B]=AB-BA=0$, then $[E^{A}_x,E^{B}_y]=0$ for all $x,y$?","['functional-analysis', 'spectral-theory', 'operator-theory']"
2681371,The curious case of $x^{\pi} - 1 = 0$,"I was thinking of the solution(s) for the fractional order algebraic equation
$ x^{\pi} - 1 = 0$ Obviously, the solution set is of the form $x = e^{2nj}, n \in \mathbb{Z}$. A countably infinite set of complex numbers, all of unit magnitude. Let $U$ be the set of unit magnitude complex numbers, defined as $U = \{ x | x \in \mathbb{C}, |x|=1, x^{\pi} \ne 1\}$ The question is, What percentage of the unit circle does $U$ constitute? It seems that the answer is $100\%$. But, that is something that I have a hard time getting my head around.","['circles', 'complex-numbers', 'percentages', 'algebra-precalculus', 'infinity']"
2681419,Help with proof of the Symmetry Principle on extending holomorphic functions on symmetric sets about the real axis,"I am looking at the proof of the Symmetry Principle from Stein and Shakarchi's Complex Analysis. Here, we try to prove the theorem using Morera's theorem. 
My question here is regarding the argument given below in diagram (a). So, from Morera's theorem, we have that $\int_{T_\epsilon} f=0$. We then let $\epsilon \to 0$, and conclude that $\int_T f(z)dz =0$. It says that we have this convergence by continuity. However, while this is intuitively clear, I can't think of a rigorous argument that guarantees 
$$\int_{T_\epsilon} f(z)dz = \int_T f(z)dz.$$ How do we get this result by continuity?",['complex-analysis']
2681436,Proving an identity for complete homogenous symmetric polynomials,"Probably everybody knows the expression:
$$
\sum_{k_1,k_2\ge0}^{k_1+k_2=k}a_1^{k_1}a_2^{k_2}=\frac{a_1^{k+1}-a_2^{k+1}}{a_1-a_2},
$$
where $a_1\ne a_2$ is assumed. It seems that it can be further generalized to the following statement. Let $a_i$ ($i=1..n)$ be some numbers such that for any $i\ne j$: $a_i\ne a_j$. Then:
$$
\sum_{k_1,k_2,\dots,k_n\ge0}^{\sum_{i=1}^n k_i=k}\prod a_i^{k_i}=\sum_{i=1}^n\frac{a_i^{k+n-1}}{\prod_{j\ne i}(a_i-a_j)}.
$$ Is there a special name for this expansion? What is the simplest way to prove it?","['symmetric-functions', 'combinatorics', 'algebraic-combinatorics', 'symmetric-polynomials']"
2681439,Gordon Royle's 21-vertex 21-automorphism graph,"OEIS A080803 lists the minimal number of vertices $a(n)$ needed to support an undirected graph whose automorphism group has order $n$. The MathWorld page on graph automorphisms links to this sequence and reproduces the list, but there is a discrepancy: MathWorld gives 23 vertices for 21 automorphisms, OEIS gives 21. The smaller, latter value is explained by Jens Voß: The value $\text{A080803}(21)=21$ is due to Gordon Royle, who found a graph with 21 vertices whose automorphism group is non-Abelian of order 21 (a 2'-Hall subgroup of the group $\text{PSL}_2(7)$). No reference is provided for this though. How exactly does Royle's 21-vertex 21-automorphism graph look like? There is only one non-abelian group of order 21, $\mathbb Z_7\rtimes\mathbb Z_3$, one of whose Cayley graphs is shown below (taken from Wedd's List ): As a directed graph, this indeed has 21 automorphisms. However, removing the edge orientations allows reflecting the graph, raising the automorphism count to 42. So what was the graph Royle found?","['graph-theory', 'group-theory', 'automorphism-group']"
2681449,proving the determinant of a product of matrices is the product of their determinants in suffix / index notation,"I'm trying to learn suffix notation (both to prove results in linear algebra and for application in vector calculus). As an exercise, I wanted to use it to prove that the determinant of a product of two matrices is equal to the product of their determinants, i.e. for $\underline{A},\, \underline{B},\, \underline{C}$, $3\times 3$ matrices where $C_{pq} = A_{pr}B_{rq}$, I'd like to show that: $$\det(\underline{A}\,\underline{B}) = \det(\underline{A}) \det(\underline{B})$$ $\color{blue}{\textbf{Here is what I have tried so far}}$: $$\det{(\underline{C})} = \det(\underline{A}\,\underline{B}),$$ Now, the determinant in suffix notation would be: $$\begin{align*} \epsilon_{ijk} C_{1i} C_{2j} C_{3k}
    & = \epsilon_{ijk} \,  A_{1p}B_{pi} \, A_{2q}B_{qj} \, A_{3r}B_{rk}\\
    & = \epsilon_{ijk} \, A_{1p}A_{2q}A_{3r} \, B_{pi}B_{qj}B_{rk}
   \end{align*}$$ $\color{blue}{\textbf{At this point, I couldn't find any way forward}}$. I tried reading up on tensors, to build my intuition, but after hours spent with several different texts, I was only more confused than before, so I tried to focus on the formalism, itself. $\color{green}{\textbf{Eventually, I thought to try it from the other side}}$, i.e. starting with $\det(\underline{A}) \det(\underline{B})$, and (after spending more time than I care to admit) I managed this: $$
\begin{align*}
    \det(\underline{A}) \det(\underline{B})
%%
    = &  \epsilon_{ijk} \, A_{1i}A_{2j}A_{3k} 
      \, \epsilon_{pqr}B_{1p}B_{2q}B_{3r} \\[5pt]
%%
    = &   \epsilon_{ijk} \epsilon_{pqr} \, A_{1i}A_{2j}A_{3k} 
      \, B_{1p}B_{2q}B_{3r} \\[5pt]
%%
    = &  \bigl( 
             \delta_{ip}(\delta_{jq}\delta_{kr} - \delta_{jr}\delta_{kq})
%
          + \delta_{iq}(\delta_{jr}\delta_{kp} - \delta_{jp}\delta_{kr})
%
         +  \delta_{ir}(\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{kp})
       \bigr)
             A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2q}B_{3r} \\[5pt]
%%
    = &  (\delta_{jq}\delta_{kr} - \delta_{jr}\delta_{kq})
        A_{1i}A_{2j}A_{3k} \, B_{1{\color{red}i}}B_{2q}B_{3r}\\
%
   & \quad + (\delta_{jr}\delta_{kp} - \delta_{jp}\delta_{kr})
               A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2{\color{red}i}}B_{3r}\\
%
   & \qquad + (\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{kp})
                 A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2q}B_{3{\color{red}i}}\\[5pt]
%%
  = &  \quad A_{1i}A_{2j}A_{3k} B_{1{\color{red}i}} (B_{2j}B_{3k} - B_{2k}B_{3j}) \\
  &  + A_{1i}A_{2j}A_{3k} B_{2{\color{red}i}} (B_{1k}B_{3j} - B_{1j}B_{3k}) \\
  &  + A_{1i}A_{2j}A_{3k} B_{3{\color{red}i}} (B_{1j}B_{2k} - B_{1k}B_{2j})
\end{align*}
$$ $\color{green}{\textbf{I } think \textbf{ this last expression can be written as}}$: $$\begin{align*}
\det(\underline{A} \, \underline{B}) 
%%
    & =  A_{1i}A_{2j}A_{3k} \, \epsilon_{pqr}B_{pi}B_{qj}B_{rk} \\[5pt]
%%
    & = \epsilon_{pqr} \, A_{1i}B_{pi}\,  A_{2j}B_{qj} \, A_{3k}B_{rk};
\end{align*}$$ however, it's possible my reasoning is incorrect (I'm still very uncertain about the formalism...). $\color{red}{If \textbf{ the above is correct}}$, it seems to look like $\det(\underline{A} \underline{B}^T)$, which suggests that if I had started off by noting that: $$ \det(\underline{A})\, \det(\underline{B}) = \det(\underline{A}) \, \det(\underline{B}^T)$$ $\color{red}{\textbf{I would end up with the correct final expression}}$. However, (if all my reasoning to date is correct) my question(s) are : is it possible to see that $$\epsilon_{pqr} \, A_{1i}B_{{\color{blue}p}i}\,  A_{2j}B_{{\color{blue}q}j} \, A_{3k}B_{{\color{blue}r}k}
  = \epsilon_{pqr} \, A_{1i}B_{i{\color{blue}p}}\,  A_{2j}B_{j{\color{blue}q}} \, A_{3k}B_{k{\color{blue}r}}$$ directly (i.e. without having to think about the interpretation as the product of determinants etc; simply via identities or application of the definitions)? If so, how? is there some way of doing this (using suffix notation) without having to expand the epsilon product $\epsilon_{ijk}\, \epsilon_{pqr}$, explicitly (all the resources I have read so far have only offered the identity in terms of the Kronecker deltas, nothing that relates the product to another epsilon)? This is painfully long to write out, so would equally appreciate mere direction towards a good ""workbook"" with lots of problems (& answers or solutions) related to suffix notation. My hope is that if I first become comfortable with the formalism, I might later have a more success trying to understand the math.","['index-notation', 'linear-algebra', 'proof-verification', 'determinant']"
2681474,Prove that $n \ln(n) - n \le \ln(n!)$ without Stirling,"I need to prove that  $n \ln(n) - n \le \ln(n!)$. I have solved this but I've used the Stirling substitution for the factorial term which does not seem good to me in this proof. I am sure that there must be a direct way to solve this. One way I can think about tackling this problem is simply breaking the left and right hand side into primary terms: $$\ln(n!) = \ln(n) + \ln(n-1) + \ln(n-2) + \dots + \ln(2) + \ln(1)$$
$$n \ln(n) - n = n (\ln(n)-1) = (\ln(n) - 1) + (\ln(n) -1) + \dots + (\ln(n) -1)$$ I need to somehow show that the the top expression is greater than the bottom expression, but I can only be sure that $\ln(n) > \ln(n) -1$ and that $ \ln(n-1)>\ln(n) - 1$ What can I do about the rest?","['algebra-precalculus', 'logarithms', 'calculus']"
2681493,$\epsilon−\delta$ limit proof,"Using the $\epsilon−\delta$ definition of limits, prove: $$\lim\limits_{x \to 1} \frac{x^5+1}{x}=2$$ The factor $(x-1)$ I can control. And I can also limit the other factor in the numerator. But the $x$ in the denominator is my problem because if I limit $(x-1)$ it seems to grow. I'm not sure what to do with it.","['epsilon-delta', 'calculus', 'limits']"
2681516,Ratios of prime gaps $(p_{n+1}-p_n)/(p_{2n+1}-p_{2n})$,"This is a question about prime gaps $g_n = p_{n+1}-p_n$ that started with a look at the average of ratios $$r_n=\frac{p_{n+1}-p_n}{p_{2n+1}-p_{2n}}$$ and of the inverse, $$ s_n=\frac{p_{2n+1}-p_{2n}}{p_{n+1}-p_{n}},$$ $n$ running from 1 to k. The averages are $$a_r=\frac{1}{k}\sum_{n=1}^k r_n,~~a_s=~\frac{1}{k}\sum_{n=1}^k s_n.$$ My questions were: (1) Is $a_r \sim a_s$? (2) Does $a_r$ ever exceed $a_s?$ What I did (can skip): For (1), FWIW, it seems that both averages may be unbounded as $k$ grows large. I think that the PNT gives (1) via $2p_k\sim p_{2k}$ and a bit more work. For (2), we have the same number of primes on $[p_1, p_{k}]$ as on $[p_{k+1},p_{2k}].$ The slow but jumpy growth of gaps gives $a_s$ an advantage at first, but as $k$ gets large I think the absolute difference shrinks and there may be (many) $k$ for which $a_s<a_r.$ However such a $k$ is greater than 100,000, for which $a_r\approx 1.866$ and $a_s\approx 2.02.$","['number-theory', 'prime-gaps', 'prime-numbers']"
2681545,WHY are there two kinds of 'possible choice' questions?,"I am studying for my GED. I was homeschooled, so most of the math topics covered are fairly easy, but there's one which I never went over, and which is giving me some trouble. I understand how to do the problems. The math works out. The trouble is that I don't understand why the process to get the solution is different. I was hoping someone here could explain it in terms I can understand. I'm dealing with what my study guide terms 'counting' questions. They deal with the number of possible choices in a given scenario. Here's a typical question: A DJ has enough time to play four songs. She has seven different songs to choose from. How many different orderings of songs can she choose? This is fairly straightforward. The work looks like $(7)(6)(5)(4)$. I understand why this works. She has seven choices first, then six, and so on. It's the next type of question which I can't figure out: A DJ is choosing four new records for his collection. He has seven available choices. How many different groups of records can he choose? This one starts out like the first one, but there's an additional step. You divide thusly: $\frac{(7)(6)(5)(4)}{(4)(3)(2)(1)}$. I know where the numbers are coming from, and how the math works. But I would like to know why. If I understand the principle, the problem will be much easier. The book explains that this is because 'order does not matter'. That doesn't make any sense, as clearly order doesn't matter in the first problem either. There's no further explanation. I need to understand why there is an additional step, so that I will be able to spot the differences between the two problems on the test. Note: I'm not asking for how to solve the problem, or how the math works. I'm asking why two problems, which, to me, look identical, are solved differently.","['permutations', 'probability', 'combinations']"
2681587,Compute the following integral: $\int _{\ln3}^{\ln 6}\:8e^xdx$,Compute the following integral: $$\int _{\ln3}^{\ln 6}\:8e^xdx$$ My attempt We have $$\begin{align}\int \:8e^xdx=8\cdot \int \:e^xdx=8e^x+C\end{align}$$ Now $$\lim _{x\to \ln (3)^+}\left(8e^x\right)=8e^{\ln(3)}=8\cdot3=24$$ and $$\lim _{x\to \ln (6)^-}\left(8e^x\right)=8e^{\ln(6)}=8\cdot6=48$$ Hence $$\int _{\ln3}^{\ln 6}\:8e^xdx=48-24=24$$ How did I do?,"['proof-verification', 'definite-integrals', 'calculus', 'limits']"
2681607,A proof that entire function of order which is a fraction have infinitely many zeros,"I understand that it is the case, I can give examples like $cos(a\sqrt{z})$ or $\Xi(\sqrt{z})$ but I would like to see a strict proof and I don't know how to create one myself.",['complex-analysis']
2681652,Convergence in Lp implies convergence in Lp norms finite,"I have the following theorem which I believe is true: Suppose we have the measure space $(\mathscr{X},\mathcal{A}, \mu)$ and $p\in [1,\infty)$. Let $\{f_n\}_{n\in\mathbb{N}}\subseteq \mathcal{L}^p(\mathscr{X},\mathcal{A},\mu)$ and suppose that $f_n \overset{\mathcal{L}^p(\mu)}\longrightarrow f$ for some $f\in \mathcal{L}^0(\mathscr{X},\mathcal{A},\mu)$. Then
\begin{equation*}
\lim_{n\rightarrow\infty} \left\Vert f_n \right\Vert_p =\left\Vert f\right\Vert_p<\infty
\end{equation*} The first equality is obvious because by the reverse triangle inequality, for each $n\in\mathbb{N}$, $\left\vert \left\Vert f_n\right\Vert_p - \left\Vert f\right\Vert_p\right\vert \le \left\Vert f_n - f\right\Vert_p$. Given the assumption that $f_n \overset{\mathcal{L}^p(\mu)}\longrightarrow f$, if we take the limit as $n\rightarrow\infty$ we may deduce that:
\begin{equation*}
\begin{split}
& \lim_{n\rightarrow\infty} \big\vert \left\Vert f_n\right\Vert_p - \left\Vert f\right\Vert_p\big\vert \le \lim_{n\rightarrow\infty}\left\Vert f_n - f\right\Vert_p =0\\
\Longrightarrow \qquad &\lim_{n\rightarrow\infty} \left(\left\Vert f_n\right\Vert_p - \left\Vert f\right\Vert_p\right) =0\\
\Longrightarrow \qquad &\lim_{n\rightarrow\infty} \left\Vert f_n\right\Vert_p = \left\Vert f\right\Vert_p 
\end{split}
\end{equation*} However, I just cannot see why it would be the case that $f\in \mathcal{L}^p(\mathscr{X},\mathcal{A},\mu)$ necessarily! I.e. I don't know why it is necessarily the case that $\left\Vert f \right\Vert_p<\infty$. Please help - thanks.","['lp-spaces', 'measure-theory', 'convergence-divergence']"
2681697,An iff condition for the existence of a $\Gamma$- invariant sub-algebra of $C(X)$,There is an action of $\Gamma$ on a compact Hausdorff space $X$. The question is to find an iff condition for the existence of a $\Gamma$ invariant sub-algebra of $C(X)$. I started out with the following: Suppose that $\Gamma$ acts on a compact Hausdroff space $X$. There exists a $\Gamma$-invariant subalgebra of $C(X)$ iff there exists an injective $\Gamma$-equivariant map from a $\Gamma$-space $A$ to $C(X)$. Suppose that $B$ is an invariant subalgebra of $C(X)$. Let $j: B \to C(X)$ be the inclusion map. Then $j$ is injective. Moreover $j(s.b)=s.b=s.j(b)$. So $j$ is $\Gamma$-equivariant. On the other hand suppose that there is an injective $\Gamma$-equivariant map $\phi$ from a  $\Gamma$-space $A$ to $C(X)$. Then $\phi(A)$ is a subset of $C(X)$. Moreover $s.\phi(a)=\phi(s.a) \in \phi(A) \subset C(X)$. Thus $\phi(A)$ is $\Gamma$-invariant. But then I realized that $\phi(A)$ is not necessarily a sub algebra of $C(X)$. To make it a subalgebra I have to impose a condition on $\phi$. So I thought of making $\phi$ a linear homomorphism. Then this establishes the claim. I want to know if this is optimal in the sense that if there can be any other condition weaker than what I have got here. Thanks for the help!!,"['dynamical-systems', 'c-star-algebras', 'operator-algebras', 'group-theory']"
2681719,Trying to understand the first fundamental form,"I have read similar questions with regards to what the first fundamental form is. I couldn't find my answers due to them assuming extra knowledge and/or using a different book which presents the topics differently.
I am studying differential geometry from Pressley (2nd ed.).
Here is an excerpt from the book: I understand the first fundamental form is a dot product. It takes two vectors from the tangent space and outputs a number. Now, what does this have to do with $du$ and $dv$? And why would they name it so it coincides with the notation from integration? According to the definition provided, $du$ is a function that outputs the first component of a vector in your tangent plane, and $dv$ outputs the second component.
Then he takes the dot product of the two vectors and now the dot product is written in terms of the basis vectors $\pmb\sigma_u$ and $\pmb\sigma_v$. (1) He says the first fundamental form is $Edu^2+2Fdudv+Gdv^2$, and says how the coefficients and $du$, $dv$ depend on the choice of surface patch, but the first fundamental form doesn't. How can that be? If the fundamental form is $Edu^2+2Fdudv+Gdv^2$ then surely it inherits the dependence of its components. Now we have the following: (2) What was the purpose of inventing all this machinery? So you can calculate the arc length using only the vectors that span the tangent plane? (3) Why is this property intrinsic? Someone living on the surface would calculate the length of the curve by looking at the curve and measuring the unit tangent vectors of the curve. But now it seems more complicated, that he somehow needs access to the surface patch parametrization ($\pmb\sigma$) in order to get $\pmb\sigma_u$ and $\pmb\sigma_v$ (4) Lastly, what is the connection between the $du$'s and integration? What is the relation between these two functions and the integrand we are used to seeing all the time. They are obviously functions, not infinitesimals.",['differential-geometry']
2681729,What is complement of $S$ in $\mathbb{R^3}$,"I have two short questions, and I think they are totally related! Question1 : If I take set $S=\{(x,x+y,x+y): x,y∈\mathbb{R}\}$ then $(\mathbb R^3\backslash S)=?$ My attempt : $(\mathbb R^3\backslash S)=\mathbb{R^3}-S$ $= \{(x,y,z)∈\mathbb{R^3}: z<y \text{ or }z>y\}$ is this correct? Question2 :  where can I express $(\mathbb R^3\backslash S)$ as union of two disjoint open sets in $\mathbb{R^3}$? My attempt : yes we can, $(\mathbb R^3\backslash S)=\{(x,y,z)∈\mathbb{R^3}: z<y \text{ or }z>y\}$ $=\{(x,y,z)∈\mathbb{R^3}: z<y\} ∪ \{(x,y,z)∈\mathbb{R^3}:z>y\}$ is this is correct? Please help me. my attempts are correct or not? and if yes, then in general how can we find complement of set in higher dimensions?","['real-analysis', 'real-numbers', 'elementary-set-theory']"
2681739,Line Bundles on Moduli Space of Elliptic Curves and Modular Forms,"I have several basic questions about modular forms I'm having trouble figuring out looking at the literature. Here are two: It is pointed out in Milne's notes (pdf, bottom of page 44) that the holomorphic 1-form $dz$ transforms like a modular form of weight -2, in that under a modular transformation becomes $(cz + d)^{-2} dz$. Therefore, modular forms $f(z)$ of weight 2 are in correspondence with modular invariant 1-forms $f(z)dz$ and so we can think of them as sections of the line bundle $L$ of such forms over the quotient $\mathbb{H}^2//SL(2,Z)$, where $\mathbb{H}^2$ is the upper half-plane. Likewise, modular forms of weight $2k$ are sections of $L^k$. It's known (and easy to show) that $H^2(\mathbb{H}^2//SL(2,Z),Z) = Z_{12}$. Does the Chern class of $L$ generate this group? Or is there a square-root of this bundle whose sections are modular forms of weight 1? I think it cannot be because such forms cannot be invariant under the central element $-1 \in SL(2,Z)$. Probably instead they must be equivariant sections of some rank 2. It's interesting that this Picard group of the moduli space is torsion. Is this what allows the discriminant modular form of weight 12 to have no zeros in $\mathbb{H}^2$? Thanks.","['number-theory', 'elliptic-curves', 'modular-forms', 'algebraic-geometry']"
2681749,Metric Density always zero or one?,"I am given a fixed Borel measurable set $E\subset\mathbb [0,1]^n$, $|E|>0$, and I want to show that there exists some $\epsilon_0>0$ such that for all $\delta>0$ there exists a cube $C_\delta\subset [0,1]^n$ with $|C_\delta|\le\delta$ such that
$$
\epsilon_0\le\frac{|C_\delta\cap E|}{|C_\delta|}\le 1-\epsilon_0.
$$
This would hold true if there existed a point at which the metric density of $E$ was neither zero nor one. My question therefore is: Does there always exist such a point? And if not, how else can I prove the claim above? A simple example is $E = [0,\tfrac 1 2]$ in $[0,1]$ (i.e., $n=1$). Then we can choose $C_\delta = (\tfrac 1 2-\tfrac\delta 2,\tfrac 1 2+\tfrac\delta 2)$.","['lebesgue-measure', 'measure-theory']"
2681758,Intuitive proof of $\frac{1+e^x}2>\frac{e^x-1}x$ for high school students,"In a high school reference book, I read a question asking which of $\frac{1+e^x}2$ and $\frac{e^x-1}x$ is larger when $x>0$. Of course, the question is technically easy to answer. E.g. using the power series expansion of $e^x$, one immediately sees that the average height is greater than the slope of the secant line. Alternatively, one can answer the question by using the first and second derivatives of $x(1+e^x)-2(e^x-1)$. Yet, answers like these seem too hacky. A more elegant one is to compare the derivatives of $\frac x2$ and $\tanh\frac x2=\frac{e^x-1}{e^x+1}$, but I don't expect a high school student to be aware of the use of hyperbolic tangent. Since both $\frac{1+e^x}2$ and $\frac{e^x-1}x$ have geometric interpretations, I wonder if there is a more natural --- and probably geometrically minded --- answer. Any idea?","['inequality', 'calculus']"
2681779,Why can I make the substitution $x = \sin \theta$?,"For a dummy variable in an integral like $x$ in $$\int_a^b \frac{x^2}{1-x^2} \, dx$$ Why is it acceptable to make a substitution: let $ x = \sin \theta$  to get something like $$\int_a^b \frac{\sin^2\theta}{\cos^2\theta} \cos \theta \, d\theta$$ I am not interested in solving this problem but I am curious as to why the substitution is legal since $x$, depending on the boundaries can take on any value,  whereas $\left|\sin \theta \right| < 1$. So, why is it allowed, and are there restrictions on substitution (besides: continuous on that interval, defined on that interval)?","['real-analysis', 'calculus']"
2681829,"Is the following statement true or false?: There are integers $x$, $y$ and $z$ such that $15$ divides $2^x \cdot 3^y \cdot 7^z$","My son asked me for help on this question but I have forgotten how to do it. Could you please save me from looking like an idiot in front of my son, it would be very much appreciated. Thank you",['discrete-mathematics']
2681830,Normal Approximation: Getting $E(X)$.,"When we use normal approximation to a binomial distribution $Bin(n, p)$, do we assume it is a normal distribution and get $E(X)$ with $E(X)= \int_{-\infty}^{\infty}xf(x)dx$. Or get $E(X)$ as a binomial distribution first with  $E(X) = np$? What about when you're normal approximating a distribution that you have no idea what distribution it is?",['statistics']
2681834,prove that $\sum_{n=1}^{\infty} \frac{z^n}{n!}$ does not converge uniformly to $e^z$ on $\mathbb{C}$,"How can I prove that $\sum_{n=1}^{\infty} \frac{z^n}{n!}$ does not converge uniformly to $e^z$ on $\mathbb{C}$? My intuition is to assume, for a contraction, that a power series converges uniformly on $\mathbb{C}$. Then, let $$S_{m}(z) = \left(1+\frac{z}{1!}+\frac{z^{2}}{2!}+···+\frac{z^{m}}{m!}\right)−e^{z}.$$ Consider $$|S_{K+1}(z) − S_{K}(z)|$$ as $z \to \infty$ to get a contradiction. Can I get some help in formulating the details or any other more elegant approach in this?",['complex-analysis']
2681971,Proof verification : Establishing an inequality between joint distribution function and the product of marginal distribution functions,"THE PROBLEM : Source : Rohatgi, Saleh. p.$114$. Problem $16$. This is my attempt to construct a solution. I'd be grateful if anyone checks whether or not it is technically alright. Also, I'd like to request to provide any shorter method, if available, to solve the problem. Thanks in advance. I use the shorthand $\int_{a}^{b} \int_{c}^{d}$ to mean $\int_{a}^{b} \int_{c}^{d} f(x,y)dxdy$ Then, $F_1(a)F_2(b)=\int_{-\infty}^{\infty} \int_{-\infty}^{a} \times \int_{-\infty}^{b} \int_{-\infty}^{\infty} = (\int_{-\infty}^{b} \int_{-\infty}^{a}+\int_{b}^{\infty} \int_{-\infty}^{a}) \times (\int_{-\infty}^{b} \int_{-\infty}^{a}+\int_{-\infty}^{b} \int_{a}^{\infty})$ This breaks up into four components : $(\int_{-\infty}^{b} \int_{-\infty}^{a})(\int_{-\infty}^{b} \int_{-\infty}^{a})+(\int_{-\infty}^{b} \int_{-\infty}^{a})(\int_{-\infty}^{b} \int_{a}^{\infty})+(\int_{b}^{\infty} \int_{-\infty}^{a})(\int_{-\infty}^{b} \int_{-\infty}^{a})+(\int_{b}^{\infty} \int_{-\infty}^{a})(\int_{-\infty}^{b} \int_{a}^{\infty})$ Adding and subtracting $(\int_{-\infty}^{b} \int_{-\infty}^{a})(\int_{a}^{\infty} \int_{b}^{\infty})$ and noting that probability measure sums up to $1$, we have $(\int_{-\infty}^{b} \int_{-\infty}^{a})+(\int_{b}^{\infty} \int_{-\infty}^{a})(\int_{-\infty}^{b} \int_{a}^{\infty})-(\int_{-\infty}^{b} \int_{-\infty}^{a})(\int_{b}^{\infty} \int_{a}^{\infty})$ The extra term is : $(\int_{b}^{\infty} \int_{-\infty}^{a})(\int_{-\infty}^{b} \int_{a}^{\infty})-(\int_{-\infty}^{b} \int_{-\infty}^{a})(\int_{b}^{\infty} \int_{a}^{\infty})$ We shall show that this is $\geq 0$ $[$which will give us the proof since $F(a,b)=(\int_{-\infty}^{b} \int_{-\infty}^{a})]$ Now, $(\int_{b}^{\infty} \int_{-\infty}^{a})(\int_{-\infty}^{b} \int_{a}^{\infty})=(\int_{b}^{\infty} \int_{-\infty}^{a}f(x_1,y_2)dx_1dy_2)(\int_{-\infty}^{b} \int_{a}^{\infty}f(x_2,y_1)dx_2dy_1)$ $=\int_{b}^{\infty} \int_{-\infty}^{a} \int_{-\infty}^{b} \int_{a}^{\infty} f(x_1,y_2) f(x_2,y_1) dx_2dy_1dx_1dy_2$ By the condition given in the question, this quantity is greater than $\int_{b}^{\infty} \int_{-\infty}^{a} \int_{-\infty}^{b} \int_{a}^{\infty} f(x_1,y_1) f(x_2,y_2) dx_2dy_1dx_1dy_2$ $=\int_{-\infty}^{b} \int_{-\infty}^{a} \int_{b}^{\infty} \int_{a}^{\infty} f(x_1,y_1) f(x_2,y_2) dx_2dy_2dx_1dy_1$ $=(\int_{-\infty}^{b} \int_{-\infty}^{a} f(x_1,y_1) dx_1dy_1) \times (\int_{b}^{\infty} \int_{a}^{\infty} f(x_2,y_2) dx_2dy_2)=(\int_{-\infty}^{b} \int_{-\infty}^{a})(\int_{b}^{\infty} \int_{a}^{\infty})$ Hence the extra term is positive. Hence the proof.","['probability-theory', 'inequality', 'statistics', 'probability-distributions']"
2681989,Bijective Function of Group,"Let $Z_n$ be the additive group of modulo $n$. Determine all the value of $n$ such that there exists bijective function $f: Z_n \mapsto Z_n$ and $g: Z_n \mapsto Z_n$, so 
$f+g: Z_n \mapsto Z_n$ is also bijective function. I have discussed this with my classmates, but gives uncertainty answer. We found that if $n$ is odd number, then we have that it satisfies the condition. So, my teacher said that we should look for contradiction by letting $n$ is even number. But, how to run this step?","['abstract-algebra', 'group-homomorphism']"
2681990,Prove that $a_n X_n \to aX$ in probability,"Suppose $\{a_n\}$ is a sequence of real numbers which converges to $a$, and $\{X_n\}$ is a sequence of random variables which converges to $X$ in probability. Prove that $a_nX_n \to aX$ in probability. Through some work I deduced that
$$P(|a_nX_n-aX| > \epsilon) \leq P(|a_n||X_n-X|>\epsilon/2)+P(|X||a_n-a|>\epsilon/2).$$ We know $|a_n|$ is bounded and so $P(|a_n||X_n-X|>\epsilon/2) \to 0$. However, it doesn't seem like we know anything about $|X|$ so I'm not sure what to do about the second term above. Any thoughts?","['probability-limit-theorems', 'real-analysis', 'probability-theory', 'convergence-divergence', 'random-variables']"
2682028,Does a branch cut discontinuity determine a function near the branch point?,"Suppose $g(z)$ is analytic on a disc centered at the origin, except along the negative real axis where it has a branch cut discontinuity. Also assume that $g(0)=0$. Let $h(x)$ give the discontinuity across the branch cut of $g(z)$ at $z =-x$.  The question is to what extent does $h(x)$ determine $g(z)$. It is clear that adding a function that is analytic everywhere on the disc does not affect $h(x)$, since the discontinuity for such a function would be zero.  But is $g(z)$ otherwise determined by $h(x)$, up to this ambiguity? I made a little progress thinking about integrating $g(z)$ around a contour that follows a circle $C(r)$ around the origin from $r e^{-i \pi}$ to $r e^{i\pi}$, and then follows the branch cut to the origin, and then goes back along the lower half of the branch cut.  This then gives the relation $$\int_{C(r)}  g(z) dz = \int_0^r h(x) dx$$ but I was unable to see if I could then invert this relation to get just $g(z)$.","['complex-analysis', 'analytic-continuation', 'contour-integration', 'analyticity', 'branch-cuts']"
2682051,Using Proof by Contradiction [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Use a proof by contradiction to show that if $n^3 + 1$ is odd, then $n$ is even for all integer $n$ . Given: $n^3 + 1$ is odd Assume: $n$ is not even I'm not sure where to go from here to prove this by contradiction.",['discrete-mathematics']
2682123,A Sobolev map with smooth weak derivative is smooth?,"Let $f \in W^{1,p}(\mathbb{R}^d)$ be a Sobolev map. 
Suppose its weak derivative is smooth; i.e. it has a representative $x \to df_x $, which is $C^{\infty}$, considered  as a map $\mathbb{R}^d \to 
\mathbb{R}^d$. Is it true $f$ is smooth? (does it have a smooth representative?). For $d=1$, the answer is trivially yes: Denote by $f'$ the smooth derivative, and take a smooth anti-derivative of it $F$: Then $F,f$ are both Sobolev maps, with weak derivatives $f'$. Thus $F-f$ has zero weak derivative, hence is constant a.e .","['multivariable-calculus', 'weak-derivatives', 'real-analysis', 'sobolev-spaces']"
2682201,"$x^2+y^2+z^2 = 4, z \geq 4.$","Let $\mathbf{F}=3y \ \mathbf{i} -3xz \ \mathbf{j} + (x^2-y^2) \
 \mathbf{k}.$ Compute the flux of the vectorfield
   $\text{curl}(\mathbf{F})$ through the semi-sphere $x^2+y^2+z^2=4, \
 z\geq 0$, by using direct parameterization of the surface and computation of $\text{curl}(\mathbf{F}).$ Denote the semisphere by $S$. In the book they start off by noting that $$\iint_S xy \ dS = \iint_S xz \ dS = \iint_S yz \ dS = 0 \quad (1)\\
 \iint_S x^2 \ dS = \iint_S y^2 \ dS = \iint_S z^2 \ dS  \quad \quad \quad (2)\\
$$ They then proceed computing the curl of the field, which is $(2x-2y,-2x,-2z-3)$ and the normal of the surface, out from the surface is $\mathbf{N}=\frac{1}{2}(x,y,z).$ So $$\text{curl}(\mathbf{F})\cdot \mathbf{N} = \frac{1}{2}(2x^2-4xy-2z^2-3z).$$ Using the symmetries (1) and (2) we get the integral $$\frac{1}{2}\iint_S-3z \ dS.$$ They then proceed to compute $dS$ by arguing that: $S$ is a part of the implicitly defined surface $F(x,y,z)=4$, where $F(x,y,z)=x^2+y^2+z^2,$ so $$dS=\left|\frac{\nabla F}{F_z}\right|dxdy = \left|\frac{(2x,2y,2z)}{2z}\right| \ dxdy= \left|\frac{(x,y,z)}{z}\right| \ dxdy=\frac{2}{z} \ dxdy \quad (3).$$ Question: Can someone intuitively, and with other examples, explain the symmetry properties (1), (2) and why equation (3) holds and what it says?",['multivariable-calculus']
2682227,Geometric representation of Euler-Maclaurin Summation Formula,"In Tom Apostol's expository article (here's a free link) , upon seeing the figure below ( or this from the Wolfram project ) I was expecting more diagrams to come to continue the error decomposition of the shaded regions in the shape of ""curved triangular pieces"". For each horizontal unit interval, it seems that the hypotenuse of an implicit triangle (within shaded region) is the linear correction. Then, there's a ""cap"" that fills in the gap between the hypotenuse and the actual curve. In this figure, each cap is attached to the implicit hypotenuse from below due to the curve being convex. It was a disappointment not seeing further decomposition of the ""cap"" into regions described by parabolic, cubic, then quartic curves etc. Somehow none of the textbooks and other materials I've read have such diagrams for higher orders. Question Statement What is the correct analysis to demonstrate the 'geometry' of the successive orders of the Euler-Maclaurin formula? Is there any existing source with such diagrams (visualization) similar to the above regarding higher order? General Remarks I'm actually not sure whether my proposal is a valid idea (that such a demonstration is possible). Below is a more detailed description (repeating things said above) of what I mean by the tentative decomposition of pieces as the terms of Euler-Maclaurin formula. Basically it is approximating the definite integral $F(b)-F(a)$ where $b>a$ , unfolding in the direction ""opposite"" to Euler-Maclaurin formula itself. $$
\int_{a}^b f(x) \,\mathrm{d}x \approx F_0 + F_1 + F_2 + \cdots
$$ The approximation is done by columns of unit width (summation of $f(k)$ , discretely sampled points) plus corrections to get close to the curve (via derivatives of end points). $$
\int_{a}^b f(x) \,\mathrm{d}x \approx \overbrace{ \underbrace{ \sum_{k = a}^b f(k) }_{ \textbf{unit columns} }  - \frac{f(a)+f(b)}2 }^{ \textbf{unit columns centered} } - \underbrace{ \frac{f'(b) - f'(a)}{12} - 0 }_{ \textstyle\genfrac{}{}{0pt}{}{\textbf{net result of} }{ \textbf{triangular & parabolic cap} } } - \overbrace{ \frac{f'''(b) - f'''(a)}{720} - 0}^{ \textbf{cubic & guartic cap} }  - \cdots$$ The correct analysis would have to account for both the emergence of Bernoulli numbers and the individual orders of correction ... and the remainder if possible. Elaboration on the Ideas For simplicity, consider $(a,b) \in \mathbb{N}^2$ and let the summation be in unit steps as usual, then pretend that things will scale nicely. The zero-th order approximation for the integral is the columns of unit width $$F_0 = \sum_{k=a}^b f(k)$$ Note that each column of height $f(k)$ is left-aligned. For example, the first column for $k = a$ occupies $x \in [a, a+1]$ . This means the last column at $k = b$ is entirely outside of the range of integration. The $2$ nd term (still zero-th order) shifts all the columns by $\frac12$ to make them centered. That is, each column of height $f(k)$ now resides at $x \in [a-\frac12, a+\frac12]$ . The last column is now half-outside, and so is the first column. Thus we remove half of each of them. $$ F_1 = -\frac{f(a) + f(b)}2$$ The $3$ rd term ( $1$ st order, linear) correction is supposed to shave off a triangular top from each column (note that the outermost are half-columns at $k = b$ and $k = a$ ). The height of each triangle is $f'(k)$ and the width is unity. $\color{brown}{\textit{Somehow}}$ there should be a telescoping of terms (combine or cancellation) with the $3$ rd order correction and we end up with $$F_2 = - \frac16 \frac{f'(b) - f'(a)}2$$ The $4$ th term ( $2$ nd order, parabolic) correction is supposed to be something involving both $f''(k)$ and $f'(k)$ . It should be a parabolic piece on top of the triangular (linear) correction of the previous order, similar in spirit ** to what one sees in the quadrature of parabola . Note that there's a fixed ratio of $\frac13$ (or $\frac23$ ) between the parabolic area and the rectangle it spans. $\color{brown}{\textit{Somehow}}$ there should be telescoping with the $2$ nd as well as the $4$ th order so that $$F_3 = 0~.$$ **Here one uses parabolic pieces to fill the gaps for one iteration then uses cubic pieces for the next iteration and so on, while in ancient quadrature one keeps using triangular pieces that are linear. The $5$ th term ( $3$ rd order, cubic) is supposed to some kind of lune (moon crest) on top of the parabolic correction, and $\color{brown}{\textit{somehow}}$ after cancellation it will end up being $$F_4 = -\frac{f'''(b) - f'''(a)}{720}$$ So on and so forth .... In the end, one shall keep the summation $F_0$ on one side of the equation by itself, moving the terms $F_1$ , $F_2$ , $F_3$ etc to the side of the integral, to have the Euler-Maclaurin summation formula. $$F_0 \approx \int_{a}^b f(x) \,\mathrm{d}x - F_1 - F_2 - \cdots$$","['real-analysis', 'taylor-expansion', 'visualization', 'integration', 'summation']"
2682243,What does it mean for an operator to be diagonal with respect to an orthonormal basis?,"Suppose that you have a seperable Hilbert space, $H$, an orthonormal basis of $H$, $(e_n)$, and the set $D=\{T\in B(H):T\,\text{is diagonal with respect to the basis}\,(e_n)\}$. Problem: What, exactly,  does it mean to say that an operator is diagonal with respect to an orthonormal basis ? As a particular example, if you were to take two operators in this space, say, $S,T\in D$, how would you verify that the composition $S\circ T$ was also in $D$? Which properties of an operator being in $D$ would you be checking this product against?","['banach-spaces', 'hilbert-spaces', 'operator-theory', 'functional-analysis', 'analysis']"
2682251,Quotient of a scheme by an (algebraic) group action need not be a scheme counterexample.,"It is well-known that the category of schemes is not cocomplete (i.e., colimits does not necessarily always exist). So I am looking for a counterexample whenever we have an algebraic group $G$ acting ""nicely"" on a scheme $X$, such that the quotient $X/G$ (although it's locally ringed space) is not a scheme. Because I wasn't able to find out a counterexample could you provide one. What's the main reason behind this problem? P.S. I guess if it acts sufficiently ""nice""on $X$ then the resulting space will be a scheme eventually, therefore, otherwise the question can be rephrased as: what's the minimum prerequisite for an action not being ""nice"" enough to give at $X/G$ a scheme structure.","['schemes', 'algebraic-geometry']"
2682264,Find the posterior distribution,"Suppose that $X_1,\ldots,X_n$ is a random sample from $U(θ,θ+1),$ and further assume $θ$ has a prior distribution as the discrete uniform distribution on the integers $1,2,\ldots,n$ where $n$ is known.
Obtain the posterior distribution of $θ.$ The problem is that as the joint distribution has an indicator function, then I don't know how to integrate the product of this times $1/n$ (the prior distribution). Any one could help me with that? Thank you in advance.",['statistics']
2682278,How does adding a inverse cube force field to a inverse square force field change the orbit?,"Let us say that we modify the law of gravitation to the following force field $$
\vec{F}(r) = \frac{-GMm}{r^2}\hat{r} + \alpha\frac{|L|^2}{m^2 r^3}\hat{r}
$$ where, $L = m_2(\vec{r} \times \dot{\vec{r}} )$ which is the angular momentum of $m$ in the reference frame of $M$. $M$ is mass of parent body which is stationery in our reference frame. $m$ is mass of the moving body. $\vec{r}$ is position vector of $m$ with $M$ as the reference (origin). $\dot{\vec{r}}$ is the velocity of $m$ with M being the reference or origin again. First Question Find the equation of orbit of a planet around the Sun and draw an approximate orbit for $\alpha \ll  1.$ Second Question For the Sun-Mercury system find the value of $\alpha$. Here $M$ is the Sun. I could not solve this question. However I feel that Newton's Theorem of revolving Orbits is somehow needed for the solution. I would like to see a proof of the theorem as well as a complete solution for the problem. I did not post this question for Physics SE mainly because (i) this question mainly needs knowledge of vector calculus (or so I feel) and (ii) My experience is that Math SE is a much better community so I prefer posting a borderline Maths-Physics question on Mathematics SE rather than Physics SE. Edit 1 I know how to derive the two body problem by using the Rung Lenz vector method. So any answers may skip over details such as how to prove $\vec{L}$ is constant etc. Edit 2 An article by Baez on the inverse cube force law. Major Edit/Typo I had made a mistake while writing the title. I had mistakenly written inverse cube potential instead of inverse cube force field.","['physics', 'mathematical-physics', 'calculus', 'vectors', 'vector-analysis']"
2682292,Hessian and metric tensors on riemannian manifolds,"First, a bit of notation (I'm assuming Einstein's convention on summation of index): $\text{Hessian}(f):=\nabla^2(f):=\nabla\nabla(f)=\left(\partial_i\partial_j(f)-\Gamma_{ij}^k\partial_k(f)\right)dx^i\otimes dx^j$ Where $\nabla$ is the Levi-Civita connection on a riemannian manifold $(M,g)$ and $\Gamma$ are the Christoffel symbols. By definition and a bit of computation, it is easy to note that $\nabla^2(f)\in\ \text{TM}^*\otimes\text{TM}^*$, as $g$, the metric tensor. My questions are: As it never been analyzed the function $f:\nabla^2(f)=g$ for a general manifold (that is: we can associate to a function $f$ a metric, but can we do the opposite in general?) How? Has this $f$ any kind of application? If so, where? I would really appreciate any kind of suggestion, expecially books or articles (I've found something, like https://arxiv.org/pdf/1312.1103.pdf , but is a bit too advanced for me, talking about transformations I haven't studied yet). Thanks in advance","['hessian-matrix', 'riemannian-geometry', 'differential-geometry']"
2682310,When to Use Normal Approximation?,"Do we use normal approximation when discrete distributions are hard to solve? For example, $P(X\ge 7000)$ where is $X\sim\operatorname{Binomial}(13000, 0.7)$. Obviously, summing each case manually cannot be done and the summation is difficult to solve (I assume)? In my textbook, sometimes when something is a dicrete distribution and I calculate it using discrete distribution, I flip to the answer and they used normal approximation to the discrete distribution. Example: ""Why are you choosing to use normal approximation?!""",['statistics']
2682323,Is there an intuitive explanation for the equality of mixed partials?,"The fact that the mixed second order partial derivatives of a $C^2$ smooth scalar valued function are equal seems, to me, quite surprising. For example, if you interpret $\frac{\partial ^2f}{\partial y \partial x}$ as the change of the slope of a tangent line along the $x$-axis when moving along the $y$-axis, it's not at all obvious to me that this should equal $\frac{\partial ^2f}{\partial x \partial y}$. Does there exist some intuitive or visual way to explain this equality? Just to be clear, I'm not looking for a formal proof (this can be found in most textbooks), but some intuitive reason or hint as to why this might be true.","['intuition', 'real-analysis', 'partial-derivative']"
2682335,Finding equations of all lines that lie entirely on a surface and passes through a point,"Let $S$ be the surface $x^2 + y^2 - z^2 = 1$ and $P(1,1,-1)$ be a point on $S$. Find the equations for all lines passing through the point P which lie entirely on the surface $S$. At point $P$, the equation of the tangent plane of the surface $S$ is: $x + y + z =1$ (1) The line equation $L: r = <1 + at, 1 + bt, -1+ct>$ must satisfy both equation (1) and the equation of the surface. Substituting $L$ into the tangent plane equation yields $(a + b + c)t = 0$ While subsittuting $L$ into the surface equation yields $2(a + b -c) + (a^2 + b^2 -c^2)t = 0$ We must have $a + b +c =0 , a + b -c = 0, a^2 + b^2 -c^2 = 0$, which then solving yields $a = b = c = 0$. I might have done something wrong while working on this question, thanks @Arthur for the help and information for me to improve my question. Any help on this will be appreciated!","['multivariable-calculus', 'surfaces']"
2682360,Prove that n is a multiple of $16$.,"Let $n\ge2$ be a natural number and $A,B$ two real matrices of dimension $5$ so that $A^2+B^2=\sqrt[n]{2+\sqrt[n]{2}}AB$ and $\det(AB-BA)>0$ . Prove that $n$ is a multiple of $16$ . I don't know how to solve it. I just know that the trace of $AB-BA$ is $0$ and also then, from Cayley-Hamilton, I have that $(AB-BA)^2<0$ which is false; from here I have no idea.","['matrices', 'linear-algebra']"
2682363,Finding matrices with $A_1^{-1}+A_2^{-1}+\dots+A_k^{-1}=(A_1+A_2+\dots+A_k)^{-1}$,"Prove that for any $n, k\geq2$ there exist nonsingular nondiagonal matrices $A_1, A_2, \dots, A_k \in M_n(\mathbb{R})$ such that $$A_1^{-1} + A_2^{-1} + \dots + A_k^{-1} = \left( A_1 + A_2 + \dots + A_k \right)^{-1}$$ For $n=2$ , if we have $A^{-1} + B^{-1} = (A+B)^{-1}$ , if I am not mistaken, we can prove that $$\det(A)=\det(B)=\det(A+B)$$ if $A,B\in M_n(\mathbb{R})$ , so it would be natural to consider that $\det(A_1)=\det(A_2)=\dots=\det(A_k)$ , but from here I don't have any idea what should I do. What should we do?","['matrices', 'linear-algebra', 'inverse']"
2682566,The isotropic cone of a positive semi-definite bilinear is a subspace of $\Bbb R^n$.,"Let $A \in \Bbb R^n \times \Bbb R^n$ be a symmetric and positive semidefinite matrix. Let
$q(x) =x^\top Ax$ and let the isotropic cone be defined as $$W= \{x ; q(x) = 0   \}$$ I want to show that $W$ is a subspace of $\Bbb R^n$. My closest attempt : Take $x$ and $y$ in $W$ (the zero vector always belongs to $W$ and is a subspace of $\Bbb R^n$).
let $\alpha, \beta \in\Bbb R $ It is known that $$q(\alpha x- \beta y) = -2 \alpha \beta x^\top Ay + \alpha^2 q(x) + \beta^2 q(y) = -2 \alpha \beta  x^\top Ay$$ So ignoring constants we are done if $x^\top Ay = 0$ for $x,y \in W$. By the spectral theorem we have that $A = UDU^\top$ where $D$ is diagonal and $U$ is unitary.
This means that 
$$ x^\top Ay = (U^\top x)D (U^\top y) $$ but I don't see where I can go from here.","['matrices', 'quadratic-forms', 'linear-algebra']"
2682600,Using overlap area to determine distance between overlapping circles,"I'm working on a simple, two-ring Venn Diagram in JavaScript where the area of the sections will be determined by the set size (mapping dataset count to area in pixels). Group 1 area (a 1 ) = 800 Group 2 area (a 2 ) = 600 Overlap area (a b ) = 100 From this I'm able to get the radiuses with: $$
r_1 = \sqrt{\frac{a_1}{\pi}}
$$ I'm setting the y coordinates of these circles to an arbitrary number (window.innerHeight / 2). However, I'm having trouble determining the distance between the centers of the two circles so that the area of the overlap has an area of a b . Questions: Using the three area values (a 1 , a 2 , a b ), the two radiuses (r 1 , r 2 ), and a constant y value for the center of the circles, is it possible to determine the distance between the centers of the two circles (d)? Would I be able to determine the distance between the centers of the circles to the point where the circles intersect? Ideally that would give me a d 1 and d 2 where d = d 1 + d 2 so I can use trigonomic functions to get the other values I might need. What are the forumlas to determine d, d 1 , and d 2 ? Here's a visual of the desired product: link to Venn Diagram image","['circles', 'area', 'trigonometry', 'geometry']"
2682602,"Proof that $Pic^0\cong \frac{H^1(X,\mathcal O)}{H^1(X,\mathbb Z)}$","Define $Pic^0(X)$ ($X$ complete complex algebraic variety) as the kernel of the first Chern class $c_1:Pic(X)\to H^2(X,\mathbb Z)$. From the long exact sequence arising from the short exponential sequence, $$H^0(X,\mathbb Z)\to H^0(X,\mathcal O)\to H^0(X,\mathcal O^*)\to H^1(X,\mathbb Z)\to H^1(X,\mathcal O)\to H^1(X,\mathcal O^*)\to H^2(X,\mathbb Z)$$ we get that $Pic^0(X)$ is isomorphic to $H^1(X,\mathcal O)$ quotiented by the image of $H^1(X,\mathbb Z)$ in $H^1(X,\mathcal O)$. Griffiths-Harris writes
$$Pic^0(X)\cong \frac{H^1(X,\mathcal O)}{H^1(X,\mathbb Z)}$$
so I understand that the map $H^1(X,\mathbb Z)\to H^1(X,\mathcal O)$ is injective. Why is this? I have a simple reason for that, i.e. that a complex analysis theorem whose name I have forgotten assures us that $H^0(X,\mathcal O)\to H^0(X,\mathcal O^*)$ via the exponential map is a surjection. So the following map in the long exact sequence is $0$, i.e. the one we are investigation is an injection. Is this the correct reason? Thank you in advance.","['complex-analysis', 'sheaf-cohomology', 'complex-geometry', 'algebraic-geometry']"
2682670,"Given any non-real, complex number with positive real part, must some power of it has negative real part?","Let $z\in \mathbb C$ such that $Re (z) >0 $ and $Im(z)\ne 0$. Then must there exist  integer $n>1$ such that $Re (z^n) <0$ ? Equivalently, given $\theta \in [0, 2\pi)$ such that $\cos \theta >0$ and $\sin \theta \ne 0$ , must there exist $n>1$ integer such that $\cos (n\theta) <0$ ?","['complex-analysis', 'real-analysis', 'trigonometry']"
2682688,Why Cost Function for Linear Regression Is Always a Convex Shaped Function?,"This diagram is from Andrew Ng course for ML/DL: But isn't the cost function (least squares function) shape depends on scatter of the data ? For example below, the minimum will be at (0,1): that doesn't correspond to convex shape (if you will imagine it in 3d plot), that Andrew Ng showed above. UPDATE Oh, i think I understand... my example is a convex shape too, but simply shifted by coordinates, relatively to the Andrew's example. Am i right?","['least-squares', 'linear-regression', 'statistical-inference', 'machine-learning', 'statistics']"
2682740,Evaluate $\lim_{h\to 0}\frac{1}{h^3e^{\frac{1}{\sqrt{h^2}}}}$,"Background: I need to find if \begin{cases}
\frac{1}{x^2+y^2+z^2}e^{-\frac{1}{\sqrt{x^2+y^2+z^2}}}, \text{     }(x,y,z)\neq (0,0,0)\\
0,\text{     } (x,y,z)=(0,0,0)
\end{cases} Differentiable at $(0,0)$ So I found that the function is continues at (0,0,0) and now I am checking if there are partial derivatives a. why we must derive by definition? So I derived by definition and got $$\lim_{h\to 0}\frac{1}{h^3e^{\frac{1}{\sqrt{h^2}}}}$$ I would use substitution to get to an expression of the form $\frac{t^3}{e^t}$ but I have even and odd powers","['multivariable-calculus', 'real-analysis', 'limits']"
2682762,Closed Form of $\int x^n e^x~\mathrm{d}x$,My calculus teacher showed us how to solve $$\displaystyle\int x^n e^x~\mathrm{d}x$$ by iteratively doing integration by parts. I figured out that $$\displaystyle\int x^n e^x~\mathrm{d}x$$ is equal to $$x^n e^x - n\int x^{n-1} e^x~\mathrm{d}x.$$  You can then iteratively find out what the solution is for any $n$.  My question is whether or not there exists a closed form for this integral.  Any help would be much appreciated.,"['indefinite-integrals', 'integration', 'calculus', 'closed-form']"
2682803,Calculating the Laurent Series for complicated functions,"I would appreciate a nudge in the right direction for the following questions, as I haven't really had any exposure to finding Laurent Series expansions for more complicated functions Find the Laurent Series expansion for: $$1)\frac{z-7}{z^2+z-2}$$    $ 1<|z|<2$ , $|z|>2$ ,  $0<|z-1|<1$ For this question I tried to follow the standard route for this question by using partial fraction decomposition and then attempting to put each fraction in the form of $\frac{1}{1-z}$ but I feel like I'm having an issue when I come to this stage: By partial fraction decomposition we have: $$\frac{z-7}{(z-1)(z+2)}=\frac{3}{z+2} - \frac{2}{z-1}$$ And attempting to put $\frac{3}{z+2}$ and $-\frac{2}{z-1}$ in the form $\frac{1}{1-z}$ has so far yielded an embrassing $$\frac{3}{z+2}=\frac{3}{2+z}=\frac{3}{2}\frac{1}{1+\frac{z}{2}}$$ At this point I think it's as simple as doing some algebraic manipulation but I can't even seem to do this? And once I've done that is this still just a simple ""plug and play"" type scenario with Taylor Series? $$2)\frac{1-cosz}{(z-2\pi)^3}$$ I don't even know where to start for this. I thought perhaps partial fraction decomposition again but I don't see how this would help (if even possible) since we would still struggle to get the form of $\frac{1}{1-z}$ $3)$Let $f(z)= \sqrt {z^2-3z+2}$, $1<|z|<2$. Does the Laurent series expansion for $f(z)$ exist? Justify your answer. This is completely beyond me as I can't imagine where I would begin to check if the expansion exists. I'm only used to (as you can probably tell) calculating Laurent Series of the form $\frac{1}{1-z}$ Additionally I'm not entirely sure how the conditions i.e $1<|z|<2$ etc affect the question or how the Laurent Series expansion is calculated, so if someone could briefly explain that I'd appreciate it. Thanks","['laurent-series', 'complex-analysis']"
2682826,Are points negligible for weak derivative in dimension 2?,"Let $f\in L^2\mathbb{R}^d$ such that $\nabla f$ (in the distributional sense) coincides with an $L^2$-function outside of $0$. In which dimensions $d$ do we automatically have that $\nabla f\in (L^2\mathbb{R}^d)^d$? For $d=1$ it is wrong, e.g. the Heaviside functions provides a counterexample. For $d\ge3$, I can prove the statement: We have equality of $\nabla f$ with some $L^2$-function when testet against a test function which vanishes at $0$. In order to extend this result to arbitrary test functions, I want to use a collection of cutoff functions $\phi_\epsilon\colon \mathbb{R}^d\rightarrow B(0,1)$ with support in  $B(0,\epsilon)$. The crucial point that would allow to pass to the limit $\epsilon \rightarrow 0$ is that
$$
 \nabla\phi_\epsilon \rightarrow 0 \quad \text{weakly in } L^2.
$$
For that I considered $\psi(t)=\exp(-t^2/(1-t^2))$ and $\phi_\epsilon(x)=\psi(\vert x \vert/\epsilon)$, which is the standard way of producing bumps. Then
$$
\vert \nabla\phi_\epsilon(x)\vert \lesssim 1/\epsilon, \quad \text{supp}(\nabla \phi_\epsilon)\subset B(0,\epsilon),
$$
hence for any $L^2$-function $g$ we have
$$
\vert\int g \nabla \phi_\epsilon\vert \lesssim \frac{1}{\epsilon}\int_{B(0,\epsilon)} \vert g\vert \le \frac{1}{\epsilon} \Vert g\Vert_{L^2}\cdot \vert B(0,\epsilon)\vert^{1/2} = O(\epsilon^{d/2 -1}),
$$
thus the result follows if $d \ge 3$. Question: Is the result true for $d=2$ or does there exist a counterexample?","['real-analysis', 'distribution-theory', 'functional-analysis', 'weak-derivatives', 'sobolev-spaces']"
2682898,Is every subset of a lattice a lattice?,"Is every subset of a lattice a lattice? Lattice consists of a partially ordered set in which every two elements have to have unique supremum and infimum. I'm confused about what the answer is. I considered a lattice $(L, \le)$ where $L$ is a set {1, 2, 3, 6} and $\le$ is relation of divisibility (a simplified version of this example ) (e.g. 1 divides 2, 3 and 6, 2 divides 6, etc.). Now if I take $\{1, 2, 3\}$ as a subset of this lattice, would this still be a lattice? Would $2$ and $3$ be missing a supremum? On one hand I think they would because $6$ is no longer in the set but on the other hand isn't supremum not required to be in the set in question? For example the set of real numbers on an open interval $(0, 1)$ has a supremum equal to 1, even though 1 is not in the set.","['order-theory', 'lattice-orders', 'relations', 'discrete-mathematics']"
2682937,Complex Analysis-Proof Verification,"Let $f$ and $g$ be two holomorphic functions on the disk $D$. If $f(z)g(z)$ $=$ $0$ for all z $\in$ $D$, show that either $f$ or $g$ in constantly zero in $D$. So this is what I have gotten: Suppose $f(z)g(z) = 0$ for all $z \in D$ Let $Z(fg) = \{z\in D: \, f(z)g(z)=0\}$ is the zero set of $(fg)$ Then $Z(fg)$ has a limit point in $D$ since $Z(fg) = D$, call the limit point as $\alpha$. Let $$Z(f) = z\in D \mid f(z)=0$$
$$Z(g) = z\in D \mid g(z)=0$$
Choose $r>0$ such that $B_r$($\alpha$)$\subset$$D$ If $f(z) = 0$ for all $z$ in $B_r$($\alpha$), then $B_r(\alpha)\subset Z(f)$ and since $B_r$($\alpha$) has a limit point in $D$, so does $Z(f)$ hence $f = 0$ Now, if $f\not\equiv0$ in $B_r$($\alpha$), there is some $z_0\in B_r(\alpha)$ such that $f(z_0) \neq 0$ but then $g(z_0) = 0$ Now, let $0<r_1<r$ such that $z_0 \notin Br_1 (\alpha)$ If $f\equiv0$ on $Br_1 (\alpha)$ then, we are done. If that is not the case, there is $z_1 \in Br_1 (\alpha)$ such that $f(z_1)\neq 0$. Then, $g(z_1)=0$ Either this process will terminate ($f\equiv 0$ on $Br_c(\alpha)$ for some $i$) (or) we will have a sequence $(z_c)_{c=0}^\infty$ converging to $\alpha$. But this sequence is entirely in $Z(g)$ So $Z(g)$ has a limit point in $0$. Hence by identity theorem, $g\equiv0$ in $D$.","['holomorphic-functions', 'proof-verification']"
2682945,"Do people care about ""Six Sigma"" for probability distributions that are not Normal?","Background At work there is a ""Six Sigma"" team that focuses on reducing quality defects. They're named that way because six sigma covers 99.7% around the mean (for a normal distribution). So if a manufacturing process covers six sigma of deviations when the randomness is normally distributed then the process is perhaps good. Question 1 Is it a thing to talk about how many sigmas are needed to ""cover"" other distributions? For example, the Gamma or the Poisson? Question 2 Is the following correct? $X$ is a continuous Uniform RV from $(0,1)$ which means the variance is $1/12$ and the SD is $\frac{1}{\sqrt{12}}$ Therefore ""six sigma"" covers $6\frac{1}{\sqrt{12}} \approx 1.73$ or all observations Question 3 I'm trying to calculate how many sigmas to cover the Exponential for any $\lambda$. Is the following correct? This is tough for me to figure out because unlike the binomial and normal, the exponential is not symmetrical around the mean I'm going to cop out and calculate how many sigmas cover the pdf starting from zero instead of around the mean... The variance of an exponential is $\frac{1}{\lambda^2}$ so sigma is $\frac{1}{\lambda}$ and I want to know how many multiples $m$ of sigma cover 99.7% $$P(X \le \frac{m}{\lambda}) = 0.997$$ $$1 - e^{-\lambda \frac{m}{\lambda}} = 0.997$$ $$1- e^{-m} = 0.997$$ $$ e^{-m} = 0.003$$ $$ m = -ln(0.003)$$ Therefore, regardless of lambda, to cover 99.7% of the pdf you need 5.8 sigmas (which is close to six sigmas). Thanks for your help!!","['statistics', 'probability']"
2682956,Which subsets of the real numbers can you play Sylver coinage on?,"For a set $S$ of real numbers, we say that you can play sylver's coinage on it if for any infinite sequence of $x_n \in \mathbb R$, there is some $N$ such that $x_N$ is a sum of previous terms. For example, you can play on $\mathbb N$ (which corresponds to regular Sylver's coinage). Or you can play on $\mathbb Z$. You can't play on $\mathbb R$ though, since $1$, $\pi$, $\pi^2$, $\pi^3$, $\dots$ is a bad infinite sequence. So, is there some way we can characterize which sets in $P(\mathbb R)$ you can play on. Some examples: Any finite set $S$ A subset of any playable set The union of two playble sets","['number-theory', 'combinatorial-game-theory']"
2682960,Degree $n$ extension of local field splits degree $n$ division algebra,"I am trying to write an article which is pretty self-contained on the number theory side, and would like to use the following result: Let $K$ be a local field, $n > 1$ a natural number, $D$ a division algebra of degree $n$ over $K$. For any degree $n$ extension $L/K$, $D \otimes L$ is split. As far as I am aware, this result is classically derived as a corollary from the description of the Brauer group of $K$ as $\mathbb{Q}/\mathbb{Z}$ (if $K$ is non-archimedean; the case $K = \mathbb{R}$ is easy). This is how it is done for example in Pierce's Associative Algebras or Weil's Basic Number Theory, as well as some documents I found online. This is the only result on central simple algebras over local fields I need; I do not need the full strength of the isomorphism to $\mathbb{Q}/\mathbb{Z}$. Is there a short argument, using only basic properties of local fields? If that makes things easier, I may even assume $n$ to be prime.","['number-theory', 'division-algebras', 'brauer-group', 'local-field']"
2682975,Galois group of $x^n+1$ over $\Bbb Q$,"Let $n\in\Bbb Z_{>0}$. Determine the Galois group of $f(x)=x^n+1$ over $\Bbb Q$. I am having some trouble with this. I started by assuming $n$ is odd, then $f(-x)=(-x)^n+1=-(x^n-1)$, then the Galois group of $f(x)$ is the same as $x^n-1$. We know that $\operatorname{Gal}(x^n-1/\Bbb Q)\cong (\Bbb Z/n\Bbb Z)^\times$, so this is the Galois group of $f(x)$ over $\Bbb Q$ for odd $n$. I am not sure what to do for the general case ($n$ odd or even). I have been doing some research, and I have seen people argue that the splitting field for $x^n+1$ over $\Bbb Q$ is equal to the one for $x^{2n}-1$ over $\Bbb Q$, since $x^{2n}-1=(x^n+1)(x^n-1)$, so any solution to $x^n+1=0$ is one to $x^{2n}-1=0$. For instance, the accepted answer here . Then I would be able to conclude that $\operatorname{Gal}(x^n+1/\Bbb Q)\cong(\Bbb Z/2n\Bbb Z)^\times$. However, I do not understand why this allows us to conclude that the splitting field for $x^n+1$ is $\Bbb Q(\zeta_{2n})$ though ($\zeta_{2n}$ a primitive $2n$-th root of unity).","['abstract-algebra', 'galois-theory', 'splitting-field', 'cyclotomic-fields']"
2683032,I am having trouble understanding how to prove that the sum of the first $n$ positive even integers is $n^2 + n$ using strong induction.,Show the sum of the first $n$ positive even integers is $n^2 + n$ using strong induction. I can't solve the above problem using strong induction. It will be very helpful if I can get the solution. Thanks in advance.,"['induction', 'discrete-mathematics']"
2683037,Why is $\lim\limits_{n \to \infty} \ln \left(\frac{n}{(n!)^{\frac{1}{n}}}\right)=1$?,Why is $$\lim_{n \to \infty} \ln \left(\frac{n}{(n!)^{\frac{1}{n}}}\right)=1?$$ I see from looking at the graph that it goes to $1$ but I am not too sure how to prove this algebraically. The only way I can see this function going to $1$ is if $(n!)^{\frac{1}{n}}>n$ but I am not too sure if that is true.,['limits']
2683117,Dual of the symmetric algebra of the dual,"Let $V$ be a finite dimensional vector space and let $S^d(V)$ be the degree $d$ part of the symmetric algebra on $V$. Is it true that $S^d(V^*)^*$ is canonically isomorphic to $S^d(V)$? What (I think) I know : $S^d(V^*)$ are homogeneous polynomials of degree $d$ on $n=\dim{V}$ variables with coefficients in the base field $k$, because a basis of $V^*$ is given by the $n$ coordinate functions $x_i$ which are the dual basis of any fixed basis for $V$. So the symmetric algebra $S(V^*)$ really is the polynomial ring $k[x_1,...,x_n]$, right? ( Remark : in this description of the symmetric algebra I had to make a choice, namely fixing a basis of $V$.) What I don't know : I think I don't really understand $S(V)$. Should I just think of it as $k[e_1,...,e_n]$, regarding the $e_i$'s just as formal symbols? In any case, for both descriptions I had to choose a basis. So I was hoping that there may be a canonical isomorphism as in the case of $V\cong (V^*)^*$. Is this true?","['abstract-algebra', 'linear-algebra', 'vector-spaces']"
2683125,Prove that $S_n\sim2^{n+1}$ where $S_n=\sum_{p=0}^n\sum_{q=0}^n\binom{n}{p}\binom{n}{q} e^{-pq}$.,"For $n\in\Bbb N$, define
  $$S_n=\sum_{p=0}^n\sum_{q=0}^n\binom{n}{p}\binom{n}{q} e^{-pq}$$ I
  want to prove that $$\lim_{n\to\infty}\frac{S_n}{2^{n+1}}=1$$ My attempts: For $n\in\Bbb N^*$,
$$S_n=2^{n+1}-1+\sum_{p=1}^n\sum_{q=1}^n\binom{n}{p}\binom{n}{q} e^{-pq}$$ The role of $e$ is not important. In fact, we can replace $e$ with any constant $x\ge1$. I tried to use some similar methods (such as CLT) of the limit below but failed.
$$\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!}=\frac12$$ Summation of $q$ gives
$$S_n=\sum_{p=0}^n\binom{n}{p}(1+e^{-p})^{n}$$ Some calculation results:
$$\frac{S_{10}}{2^{11}}\simeq1.23\qquad\frac{S_{20}}{2^{21}}\simeq1.01\qquad\frac{S_{30}}{2^{31}}\simeq1.00035$$ The answer of the link in my comment is satisfied. However, it will also be appreciated if there are any other ideas.","['binomial-coefficients', 'sequences-and-series', 'analysis', 'limits']"
2683136,Counting irreducible polynomials over finite fields [duplicate],"This question already has an answer here : Irreducible polynomials of degree 3 and 5 (1 answer) Closed 6 years ago . How many irreducible polynomials in $Z_2[x]$ of degree $3$? I have discussed this with my friend before and we found that $x^3 + x^2 + 1$ and $x^3+x+1$ are the two said polynomials which irreducible. We got this the approachment that polynomials $p(x) = ax^3+bx^2+cx+d$ must satisfy $a\ne 0$ and later $d \neq 0$ and here we only need to take some cases of the value of $b$ and $c$. 
Do you have idea to solve this problem clearly without guessing? 
I am afraid that there will be a new problem asking for polynomials whose degree is higher than this one and here I will be stuck.","['abstract-algebra', 'ring-theory', 'polynomial-rings']"
2683172,Deriving values for the Riemann zeta function on even natural numbers through recurrence,"In this paper, Yaglom shows an elementary derivation of the solution to the Basel problem by considering polynomials of the form $$\sum_{k=0}^m(-1)^k\binom{2m+1}{2k+1}x^{m-k}$$ which has roots $$\displaystyle \cot^2\frac{k\pi}{2m+1},\,k=1,2,\ldots,m$$ For convenience of notation let $n=2m+1$ . Using Vieta's relations, Yaglom shows that $$\sum_{k=1}^m\cot^2\frac{k\pi}{n}=\frac{\binom{n}{3}}{\binom{n}{1}}=\frac{(n-1)(n-2)}{6}$$ Using the trig identity $1+\cot^2\theta=\csc^2\theta$ , we get $$\sum_{k=1}^m\csc^2\frac{k\pi}{n}=\frac{(n-1)(n-2)}{6}+\frac{n-1}{2}=\frac{(n-1)(n+1)}{6}$$ Finally, using $\displaystyle\cot^2\theta<\frac{1}{\theta^2}<\csc^2\theta\,$ for $\theta\in (0,\pi/2)$ , we get $$\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\frac{\pi^2}{6}<\sum_{k=1}^m\frac{1}{k^2}<\left(1-\frac{1}{n}\right)\left(1+\frac{1}{n}\right)\frac{\pi^2}{6}$$ which gives us the desired solution. Yaglom suggests that this method can be extended via Newton's identities to produce values for the Riemann Zeta function on even natural numbers. If we call $$p_j=\sum_{k=1}^m\cot^{2j}\frac{k\pi}{n}$$ then by Newton's identities we get the recurrence relation $$k\binom{n}{2k+1}=\sum_{j=1}^k(-1)^{j-1}\binom{n}{2(k-j)+1}p_j$$ Unfortunately I don't have much experience with recurrence relations, and I have no idea how to proceed. Any help would be very much appreciated. My goal is to find and alternate expression for $p_j$ , just as Yaglom found that $$p_1=\frac{\binom{n}{3}}{\binom{n}{1}}$$ Edit: I'm specifically looking for a way to derive $$\zeta(2k)=\frac{(-1)^{k+1}B_{2k}(2\pi)^{2k}}{2(2k)!}$$ using this method.","['recurrence-relations', 'riemann-zeta', 'sequences-and-series']"
2683331,When the Lights Out puzzle vectors over elementary abelian field of dimension $n^2$ form a basis,"Assume we have a $n\times n$ matrix, where $n$ is odd. The elements in this matrix are either $0$ or $1$. We can change the values of the elements in the matrix as follows: changing the value of position $\{i,j\}$ from $0$ to $1$ or vice versa also changes the values in positions $\{i-1,j\}$,$\{i+1,j\}$,$\{i,j-1\}$ and $\{i,j+1\}$ respectively. If any of these falls outside the matrix, we ignore it. Call these the basic operations. For example, if $n = 3$ and the matrix is all zeroes, changing the value of $\{1,2\}$ would result in $$ \left(
\begin{array}{ccc}
 1 & 1 & 1 \\
 0 & 1 & 0 \\
 0 & 0 & 0 \\
\end{array}
\right) $$ and so on. We can turn the matrixes into vectors so that position $\{i,j\}$ of the matrix is the element $(i-1)n+j$ of the vector. The above example is thus the same as adding vector {1,1,1,0,1,0,0,0,0} to zero vector of length $9$. The addition is done modulo $2$. These vectors are elements of a $n^2$-dimensional vector space over the elementary abelian field of order $2$. The vectors associated with the basic operations form a basis of some subspace of this vector space. If we look at the set of these vectors when $n=3$, we have the following, written as matrix: $$\left(
\begin{array}{ccccccccc}
 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\
 0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
 1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\
 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\
 0 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 1 \\
 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 \\
 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 \\
 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 \\
\end{array}
\right)$$ 
These vectors are linearly independent and there are $9$ of them and thus form a basis of the whole space. This means that we can reach any configuration of zeroes and ones in the matrix by basic operations. What if $n=5$? We have, again, a set of $25$ vectors associated with the basic operations, very similar to the case $n=3$: $$\left(
\begin{array}{ccccccccccccccccccccccccc}
 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\
\end{array}
\right)$$ However, thse vectors are not linearly independent and thus do not form a basis of the whole space. In fact, they span a subspace of dimension $23$. This can be interpreted so that we can reach any configuration of zeroes and ones using basic operations, apart from two positions which are out of our control and may or may not have the desired values. If $n=7$, the set of vectors associated with the basic operations are again linearly independent. If $n=9$ or $n=11$, the vectors are not linearly independent, but are again independent if $n=13$. The question is why in the case of $n=3$ or $n=7$ we can reach all possible configurations of zeroes and ones in the matrix with the basic operations but not in the case of $n=5$. And further, for which values of $n$ are the vectors associated with the basic operations linearly independent? Testing some values of $n$, the list of such values of $n$ begins as $$\{1,3,7,13,15,21,25,27,31,37,43\}.$$  Looking the list up in OEIS returns only one result: A209839, which seems to have very little to do with this problem. There is really no reason to restrict ourselves with odd $n$s. For general $n$, the values for $n$ that produce vectors that span the whole space begins $\{1,2,3,6,7,8,10,12,13,15,18,20,21,22,25\}.$ This is sequence A076436 at OEIS. Edit: I had made an error in my program which resulted in wrong sequence and wrong dimensions of the subspaces. I have fixed that now. See comments for the correct differences in dimensions.","['linear-algebra', 'vector-spaces']"
2683368,"If you roll two fair six-sided dice, what is the probability that the sum is 4 or higher?","If you roll two fair six-sided dice, what is the probability that the sum is $4$ or higher? The answer is $\frac{33}{36}$ or $\frac{11}{12}$. I understand how to arrive at this answer. What I don't understand is why the answer isn't $\frac{9}{11}$? When summing the results after rolling two fair six sided dice, there are $11$ equally possible outcomes: $2$ through $12$. Two of these outcomes are below four, meaning $9$ are greater than or equal to four which is how I arrived at $\frac{9}{11}$. Can someone help explain why that is wrong?","['probability', 'dice']"
2683403,Showing a function has exactly one zero with IVT and Rolle's Theorem,"This is an exercise that appears on differential calculus exams at my university. I'm typing up a thorough response to this exercise here to share with my class, and maybe it'll help other students too. Let $f$ be the function given by $$f(x) = 3x -2\sin(x)+7\,.$$ Use the Intermediate Value Theorem to show that $f$ has at least one zero, and then use the Mean Value Theorem to show $f$ has exactly one zero. See the sidebar under the Linked header for similar questions. ​ ​ ​ ​ ​ ​ ​","['rolles-theorem', 'real-analysis', 'roots', 'calculus']"
2683410,Deriving the variance of the Bernoulli distribution,"For a Bernoulli distribution, $\mu_X = p$. I can easily derive this from the general equation for mean of a discrete random variable:
$$
\mu_X=\sum_{i=1}^kx_iPr(X=x)
$$
$$
\mu_X=1(p)+0(1-p)=p
$$
I know that the variance of the Bernoulli distribution is supposed to be $\sigma_x^2=p(1-p)$. But I can not seem to derive that properly from the general equation for variance of a discrete random variable:
$$
\sigma_x^2=\sum_{i=1}^k(x_i-\mu_X)Pr(X=x_i)
$$
$$
\sigma_x^2=(x_0-p)(1-p)+(x_1-p)(p)
$$
$$
\sigma_x^2=(0-p)(1-p)+(1-p)(p)
$$
$$
\sigma_x^2=-p(1-p)+(1-p)(p)
$$
$$
\sigma_x^2=-p+p^2+p-p^2
$$
$$
\sigma_x^2=0
$$
This is obviously incorrect; what am I doing incorrectly in my derivation?","['probability', 'probability-distributions']"
2683472,Finding $\mbox{tr}(A)$ from the condition $\mbox{tr}(A^2) = \mbox{tr}(A^3) = \mbox{tr}(A^4)$?,"Let $A$ be an $n \times n$ matrix with real eigenvalues such that $$\mbox{tr}(A^2) = \mbox{tr}(A^3) = \mbox{tr}(A^4)$$ Then what would be $\mbox{tr}(A)$? I thought of finding $\sum_{i=1}^{n} \lambda_{i}$ from $$\sum_{i=1}^{n} \lambda_{i}^2 = \sum_{i=1}^{n} \lambda_{i}^3 = \sum_{i=1}^{n} \lambda_{i}^4$$ after this, I could try $\sum_{i=1}^{n} \lambda_{i}^2 - \lambda_{i}^3 = 0$ and $\sum_{i=1}^{n} \lambda_{i}^3 - \lambda_{i}^4 = 0$, how can I proceed with this? Also in another way it can also be put like this - finding $\sum_{i=1}^{n} a_{i}$ where $a_{i} \in \Bbb{R}$ given that $\sum_{i=1}^{n} a_{i}^2 = \sum_{i=1}^{n} a_{i}^3 = \sum_{i=1}^{n} a_{i}^4$?","['matrices', 'trace', 'eigenvalues-eigenvectors', 'linear-algebra']"
2683488,What is the value of $\sqrt{1+ \sqrt[3]{1+\sqrt[4]{1+ \sqrt[5]{1+ \cdots }}}}$?,"How can we find the value of
$$\sqrt{1+ \sqrt[3]{1+\sqrt[4]{1+ \sqrt[5]{1+ \cdots}}}}=?$$ My Approach: Let $$f(n)=\sqrt[n]{1+ \sqrt[n+1]{1+\sqrt[n+2]{1+ \sqrt[n+3]{1+ \cdots}}}} \tag{1},$$
then $f(2)$ is our solution. So, doing $n$ th power in both sides of $(1)$, we get:
$${ \{ f(n) \} }^n =1+f(n+1)$$
$$\implies { \{ f(n) \} }^n - f(n+1) = 1 \tag{2}$$
Now how can I solve $(2)$ ? Any help please…","['nested-radicals', 'sequences-and-series', 'limits']"
2683542,"The meaning and definition of $\psi^{(-2)}(x)$, and the convergence of some related series involving the Möbius function","While I was playing with a CAS I find that makes sense the function $$\psi^{(-k)}(x),$$
for example $\psi^{(-2)}(x)$, where $\psi^{(n)}(x)$ denotes the $n$th derivative of the digamma function, see this MathWorld . Question 1 (Answered see the comments). Can you explain what is the function $\psi^{(-2)}(x)$? I am asking about what is its definition. Many thanks. I think that maybe is a notation for the second antiderivative, but I would like to know a definition with rigor about what is previous function, and what is previous notation. As a puzzle I wondered if it is possible to deduce the convergence of some series involving previous functions, and the Möbius function $\mu(n)$, see the definition of this arithmetic function from this MathWorld . Question 2. Can you deduce the convergence of $$\sum_{n=1}^\infty\mu(n)\frac{\psi^{(-1)}(n)}{n^3}\tag{1}$$ or $$\sum_{n=1}^\infty\mu(n)\frac{\psi^{(-2)}(n)}{n^3}\,?\tag{2}$$ Many thanks. Only is required to prove the convergence of some example in previous Question 2, well the first or the second series.","['derivatives', 'digamma-function', 'polygamma', 'mobius-function', 'convergence-divergence']"
2683581,Conjecture on product of first $n$ primes.,"I was recently studying about reduced residue systems , and then I stumbled upon an idea . Let $P$ represent the set of all primes. Let $P_{n}\#$ denote the product of the first $n$ primes and let $P_{n}$ denote the $n^{th}$ prime. Let $S$ be a set such that, $S=\{ (x_{1},x_{2}) : x_{1},x_{2} \in P \wedge P_{n-1}^{2} < x_{1}\cdot x_{2} < P_{n}\# \}$ My conjecture is: $P_{n}\# - 1$ is a prime if, $P_{n}\#  \not \equiv K\cdot A + 1 \pmod{P_{n-1}\#} $ $\forall$  $(K,A) \in S$ For example:
Let's take $n=3$. $P_2\# =6$ $S=\phi$ Hence,  $P_3\# -1 =29$ is prime. Let's take $n=4$. $P_3\# =30$ $S=\{(7,7),(11,7),(7,11),(11,11),(11,13),(13,11),(13,13),(7,17),(17,7),(11,17),(17,11),(7,19),(19,7),(11,19),(19,11)\}$ Hence,  $P_4\# -1 =209$ ($209 = 19 \cdot 11$) is not prime 
since $(7,17)$, $(17,7)$, $(19,11)$, $(11,19)$ satisfy the conditions. Similarly, $P_{7} \# -1$ is not prime since $P_{7} \# \equiv 41\cdot 11719$ (mod $P_{6} \#$) If there are any proofs or counter examples for this conjecture, it would be of great help.","['number-theory', 'prime-numbers']"
2683606,Proving $|\sqrt{x}-1|\leqslant|x-1|$,"I tried squaring both sides, didn’t get me anywhere. Maybe going case by case would result in something but I think there could be amore elegant proof.","['algebra-precalculus', 'inequality', 'proof-writing']"
2683638,Understanding an aplication of Faà di Bruno's formula,"Let $g(x)=\sum_{n=0}^\infty a_n x^n$ . The main aim here is to get a closed form for the coefficients $b_n$ of $\frac{d \log{g}}{dx}$ . Faà di Bruno's formula tells us that $$\frac{d^n}{dx^n}f(g(x)) = \sum_{k=1}^n f^{(k)}(g(x))B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)})$$ Where $B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)})$ is a partial Bell polynomial. Moreover, we know that $$B_{n,k}(g'(x),g''(x),...,g^{(n-k+1)})=\frac{n!}{k!} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} \left (  \frac{g^{(\lambda_1)}(x)}{\lambda_1}\right )$$ Where $C_k$ is the set of compositions of $n$ and $\pi_k$ is the composition of $n$ in exactly $k$ parts. Taking $f=\log$ and $g$ our original power series and applying both Faà di Bruno's formula and Taylor's Theorem, and as we know that $f^{(r)}(g(x))=(-1)^{r-1}(r-1)!g(x)^{-r}$ we can get to the conclusion that: $$\frac{d^n \log{f}}{dx^n} = \sum_{k=1}^n (-1)^{k-1}(k-1)!g(0)^{-k}\frac{n!}{k!} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} \left (  \frac{g^{(\lambda_1)}(x)}{\lambda_1}\right ) $$ Then: $$b_n=\frac{d^n \log{f}}{dx^n}\frac{1}{n!} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k \ {a_0}^{k}} \sum_{\pi_k \in C_n} \prod_{\lambda_j \in \pi_k} a_{\lambda_j}$$ This can also be seen here and here (section a3, very similar). The problem comes when I try to work with this formula. As an example, let $g(x)=\sum_{n=0}x^n$ . Then, $$\frac{d \log{g(x)}}{dx} = \frac{\sum_{n=0}nx^{n-1}}{\sum_{n=0}x^n} = \sum_{n=0}x^n$$ So that the coefficients we want are $b_n=1 \ \forall n \in \mathbb{N}$ . On the other hand, applying our formula for, for example, $b_3$ , we would get: $$b_3=\frac{d^3 \log{f}}{dx^3}\frac{1}{3!} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k} \sum_{\pi_k \in C_3} 1= 1+\left ( \frac{-1}{2} \right ) 2 + \frac{1}{3}=\frac{1}{3}$$ Since $C_3=\{\{3\},\{1,2\},\{2,1\},\{1,1,1\}\}$ . Then, what am I doing wrong here? Is there any problem when applying Faà di Bruno's Formula? Am I understanding well the concept of composition? Where is the flaw?","['derivatives', 'power-series', 'taylor-expansion', 'sequences-and-series']"
2683681,Finding the expected number of collisions,"A total of $r$ keys are to be put, one at a time, in $k$ boxes, with each key independently put in box $i$ with probability $p_i$ (hence, $\sum_{i=1}^k p_i = 1$). Each time a key is put in a non-empty box, we say that a collision occurs. Find the expected number of collisions. Let random variable $X$ be the number of collisions. I first tried to derive the PMF of $X$, but got stuck. How would I do this?","['probability-theory', 'expectation']"
2683729,How to understand whether two distinguished open sets are isomorphic,"Let $R=k[x_1,...,x_n]$ be the polynomial ring over an algebraically closed field $k$ and let $f,g\in R$. Assume that $f$ and $g$ are irreducible. How can I understand whether $k[x_1,...,x_n,\frac{1}{f}]$ and $k[x_1,...,x_n,\frac{1}{g}]$ are isomorphic? Alternatively, consider the open sets $D(f)=\{p\in\mathbb{A}^n\mid f(p)\neq 0\}$ and $D(g)=\{p\in\mathbb{A}^n\mid g(p)\neq 0\}$. Is there an easy way to deduce whether $D(f)\cong D(g)$? That amounts of checking $V(tf-1)\cong V(tg-1)$ in $k[x_1,...,x_n,t]$ and I believe that this should not be an easy problem.",['algebraic-geometry']
2683765,Polynomial whose zero set is made of lines must be homogeneous,"Let $f$ be a multivariate polynomial, $f \in \mathbb C[x_1,...,x_n]$. Suppose that $f(x)=0$ implies that for every $c \in \mathbb C$, $f(c x)=0$. Does it follow that $f$ is a homogeneous polynomial, i.e. that all terms are of the same total degree? The converse statement it evident. Writing $f(x,y) = \sum_{i=0}^n g_i$ where each $g_i$ is homogeneous of degree $i$, we know that for every zero $x,$ we have $0=\sum_{i=0}^n g_ic^i$ so each $g_i$ by itself vanishes on $x$. in particular $g_0=0$.","['algebra-precalculus', 'polynomials']"
2683766,Eigenvalues and eigenvectors - uniqueness,"Suppose I have a square $n\times n$ matrix A with $n$ linearly independent eigenvectors. Clearly more than one matrix can share the same eigenvectors and eigenvalues. However, I also know that I can write this matrix A in the form D = P $^{-1}$ AP , where D is a diagonal matrix with diagonal entries equal to the eigenvalues and the columns of P are the eigenvectors of A . However, in the other direction, if I know the eigenvalues and eigenvectors of a matrix A , then I can form the matrices P and D using the above. However, this seems to suggest that given eigenvalues and eigenvectors I can find a single matrix that corresponds to these values... So when are eigenvalues and eigenvetors unique and not unique? What am I missing?","['eigenvalues-eigenvectors', 'linear-algebra']"
2683784,In how many ways can $3$ boys and $4$ girls sit in a row if no one sits beside a person of the same sex?,"$3$ boys and $4$ girls have bought tickets for a row of $7$ seats at a party. In how many ways can they arrange themselves if no one sits beside a person of the same sex? I found the answer $3! \cdot 4!$.  However, the book gives the answer  $2 \cdot 3! \cdot 4!$.  Which is correct?","['algebra-precalculus', 'combinatorics', 'permutations']"
2683789,Construct $(y_n)$ such that $x_n \leq y_n \leq 2y_{f(n)}$ where $f:\mathbb{N}_{\geq 0} \to \mathbb{N}_{\geq 1}$ is strictly increasing,"Let $f:\mathbb{N}_{\geq 0} \to \mathbb{N}_{\geq 1}$ be a strictly increasing function and $(x_n)_{n \geq 0}$ a decreasing sequence such that $x_n \to 0$. Construct a decreasing sequence $(y_n)_{n \geq 0}$ such that $y_n \to 0$ and $$x_n \leq y_n \leq 2y_{f(n)}, \: \forall n \in \mathbb{N}_{\geq 0}$$ Construction problems like this one always put me in trouble and sometimes, I don't even know how to approach them. That function $f$ is so generally defined that I don't know what to do with it. All I got is the immediate fact that $f(n) \geq n+1$ and this means that $x_n \leq y_n \leq 2 y_{n+1}$ so we should construct $(y_n)$ such that $y_{n+1}$ is between $\frac{y_n}{2}$ and $y_n$. I would really appreciate an intuitive look upon the solution, that is, a motivation for each step involving the construction of $(y_n).$","['real-analysis', 'sequences-and-series', 'calculus', 'functions', 'convergence-divergence']"
2683817,Finding the pdf of a random variable generating from another random variable with defined pdf,"Initially, there is a random variable $X$ (non-negative) with distribution function:
$$P(x) = \lambda e^{-\lambda x}$$
Now we randomly generate $X$ and, then, form sets of $N$ values of this variable: $(x_1,x_2,\ldots,x_N)$ And for each set, we define, for example, $$S = \frac{1}{N}\sum_{k=1}^N x_k^2$$
$S$ is then a new random variable. My question is: How to find the pdf of this new random variable ?","['exponential-distribution', 'gamma-distribution', 'statistics', 'probability-distributions']"
2683841,Finding Streamlines: How to solve this ODE? Is there a easier way?,"I was given the velocity vector field $\mathbf{\vec{v}}(x, y, z)$ and was asked to find the streamlines $\mathbf{\vec{r}}(t)$ in my exam: $$\mathbf{\vec{v}}(x, y, z) = -\dfrac yx \hat i + 4x \hat j + 0 \hat k$$ This is what I did: $$z(t) = \int 0 dt = C_1$$ $$\dfrac{dx}{dt} = -\dfrac {y(t)}{x(t)};\; \dfrac{dy}{dt} = 4x(t)$$ But these two equations are coupled. So, I tried to decouple them like this: $$y = -x\dfrac{dx}{dt}$$ $$\implies \dfrac{dy}{dt} = -\left(\frac{dx}{dt}\right)^2 - x\dfrac{d^2x}{dt^2}$$ $$\implies x\dfrac{d^2x}{dt^2} + \left(\frac{dx}{dt}\right)^2 + 4x = 0$$ Now, what do I do? Is there a easier way?","['physics', 'ordinary-differential-equations']"
2683895,Show that $f'(x) = f^2(x)$ and $f(0) = 0$ implies $f$ is the zero function [duplicate],"This question already has answers here : $f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $ (5 answers) Closed 6 years ago . Let $f : \mathbb{R} \to \mathbb{R}$ differentiable be such that $$f'(x)=f^2(x)$$ for all $x$ and $f(0)=0$. Show that $f(x)=0$ for all $x$. My attempt: Notice that $f'(x)=f^2(x)\geq 0$, thus $f$ is increasing. Since $f(0)=0, f(x)\geq0\; \forall x \geq 0$. Since $f$ is differentiable, $f$ is continous, therefore given $1> \epsilon >0$, there exists $\delta<1$ such that for $x \in (-\delta,\delta)\implies f(x)<\epsilon<1.$ Take $x=\delta/2$. Then $f(x) = f'(c)\delta/2=f^2(c)\delta/2$. Then $f^2(c) \geq f(x)$. But since $c\in (-\delta,\delta)$, $0\leq f (c)<1 \implies f^2(c) \leq f(c)\implies f(c)\geq f(x)$. But $c<x$, therefore $f(c)\geq f(x)$ which means $f(c)=f(x)$. Therefore for all $y \in (c,x)$, $f(y)=f(x) \implies f'(y)=f^2(y)=0 \implies f(y)=0$. Where do I go from here?","['real-analysis', 'ordinary-differential-equations', 'calculus']"
2683904,Almost sure convergence implies convergence in probability with arbitrary rate (up to constant)?,"I have what seems like a simple question, but I am not quite able to prove it. Suppose that $A_n$ is a sequence of random variables, and we know that $A_n$ strongly converges to zero; i.e. almost surely. I am aware that this implies that $A_n$ converges in probability to $0$, but I am wondering about the rate of convergence. Suppose that I wish to establish that $A_n = \mathcal O_P(f(n))$, where $f(n)$ is some positive, strictly monotonically decreasing function which decays to zero at some rate. My understanding is that strong convergence implies that for any $\epsilon$ there are finitely many $n$ such that $A_n > \epsilon$. But does this condition imply that there is some constant $C_f$, potentially quite large!, such that for $n>>0$,
$$\Pr\left(A_n > C_f f(n)\right) \to 0 ?$$","['probability-theory', 'convergence-divergence']"
2683936,Probability of symmetric difference,"How do we prove the following inequality?
$$\mathbb{P}[A\triangle B ]\geq \max\left \{ \mathbb{P}[A-B],\mathbb{P}[B-A] \right \}$$
I already proved that
$$\mathbb{P}[A\triangle B ]=\mathbb{P}[A]+\mathbb{P}[B]-2\mathbb{P}[A\cap B]$$
$$\left | \mathbb{P}[A]-\mathbb{P}[B] \right |\leq \mathbb{P}[A\triangle B ]$$","['proof-writing', 'probability']"
2683944,How to find the eigen values of the given matrix,"Given the matrix \begin{bmatrix}
5&1&1&1&1&1\\1&5&1&1&1&1\\1&1&5&1&1&1\\1&1&1&4&1&0\\1&1&1&1&4&0\\1&1&1&0&0&3
\end{bmatrix} find its eigen values(preferably by elementary row/column operations). Since I don't know any other method other than elementary operations to find eigen values so I tried writing the characteristic polynomial of the matrix  which is follows: \begin{bmatrix}
x-5&-1&-1&-1&-1&-1\\-1&x-5&-1&-1&-1&-1\\-1&-1&x-5&-1&-1&-1\\-1&-1&-1&x-4&-1&0\\-1&-1&-1&-1&x-4&0\\-1&-1&-1&0&0&x-3
\end{bmatrix} Using $R1=R1-(R2+R3+R4+R5+R6)$ \begin{bmatrix}
x&-x+8&-x+8&-x+6&-x+6&-x+4\\-1&x-5&-1&-1&-1&-1\\-1&-1&x-5&-1&-1&-1\\-1&-1&-1&x-4&-1&0\\-1&-1&-1&-1&x-4&0\\-1&-1&-1&0&0&x-3
\end{bmatrix}","['matrices', 'linear-algebra']"
2684052,Symmetric Direct Product Distributive?,"This comes from the context of chemical group theory, so I apologize if I'm using terminology incorrectly. For that context, see Determining the symmetry of overtones of degenerate modes on the chemSE. In chemistry, we use the symmetric direct product of the irreps of a group to determine certain properties of molecular vibrations. I was trying to solve a problem which involved using the recursion formula: $$\chi_v(\hat{R})=\frac{1}{2}[\chi(\hat{R})\chi_{v-1}(\hat{R})+\chi(\hat{R}^v)]$$
to determine the characters of a particular reducible representation formed by combining $v$ many irreps of a group ( appendix C for some context). My thought process was that this formula was unnecessary and that I could just take the symmetric direct product of the irrep $v$ many times and obtain the correct result. For example, I'm working with the $D_{\infty h}$ point group and looking at a combination of $\Pi_u$ modes, so I attempted to solve for $v=3$, taking the symmetric product at each step, and obtained $$\Pi_u\otimes\Pi_u\otimes\Pi_u=(\Sigma^+_g\oplus\Delta_g)\otimes\Pi_u=2\Pi_u\oplus\Phi_u$$ But this isn't correct, the real representation should be $\Pi_u\oplus\Phi_u$. The only thing I could think of that could be faulty with my method is if the symmetric direct product isn't distributive, but I can't find a source that says one way or the other. Can I obtain multiple symmetric direct products by distributing?","['direct-product', 'symmetry', 'chemistry', 'group-theory']"
2684092,Intersection products in Algebraic Geometry,"I have seen two definitions of intersection product in Aglebraic Geometry so far: one using the Chow ring of a variety (as defined for example in 3264 and all that , by Eisenbud and Harris) and the other one using sheaf cohomology (as defined for example in Vakil's notes, def. 20.1.1., or in Hartshorne's Ample subvarieties of algebraic varieties ). How are these two definitions related? If we take $D$ to be a nice effective divisor on our (smooth, projective...) variety $X$ and we take $Y$ to be an $s$-dimensional subvariety, do we have the following equality? $$(D^s.Y)=\deg([D]^s [Y])$$ (The left hand side being the definition using sheaf cohomology and the right hand side the definition using algebraic cycles). What if we take $s$ different divisors? Under what conditions do we have equality? For example in the case of surfaces the two products should agree, right?","['intersection-theory', 'algebraic-geometry']"
2684140,Definition of the Quantum plane and the Yang Baxter Equation,"I was reading this on the quantum plane and the Yang Baxter equation. John Baez says that imposing
$$
R(X\otimes X)= X\otimes X
$$
$$
R(Y\otimes Y)= Y\otimes Y
$$
$$
R(X\otimes Y)=q Y\otimes X
$$
$$
R(Y\otimes X)=q X\otimes Y + (1-q^2) Y\otimes X
$$
the resulting R-matrix satisfy the YBE, i.e.
$$
\left(R\otimes id\right)\left(id\otimes R\right)\left(R\otimes id\right)=\left(id\otimes R\right)\left(R\otimes id\right)\left(id\otimes R\right)
$$
I then wrote the matrix in the following base {$X\otimes X,Y\otimes Y,X\otimes Y,Y\otimes X$} obtaining
$$
R=\left(\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & q\\
0 & 0 & q & 1-q^{2}
\end{array}\right)
$$
But it doesn't seem to satisfy the YBE. What did I do wrong or what did I miss? And what is the relation between the definition of the quantum plane and the Yang Baxter Equation?","['abstract-algebra', 'matrix-equations', 'quantum-groups', 'hopf-algebras']"
2684154,Showing an inequality for $\sin1$ using Taylor's theorem,The inequality I'm trying to show is in the question below: I'm able to do the first part of the question but I can't get to the inequality. The way I tried it was by saying that $-1<\cos{c_n}<1$ so for some $m \in \mathbb N$ we can say that $$1-\frac{1}{3!}+\frac{1}{5!}-...+\frac{(-1)^{m-1}}{(2m-1)!}-\frac{1}{(2m+1)!}<\sin1<1-\frac{1}{3!}+\frac{1}{5!}-...+\frac{(-1)^{m-1}}{(2m-1)!}+\frac{1}{(2m+1)!}$$ which further gives $$1-\frac{1}{3!}+\frac{1}{5!}-...+\frac{(-1)^{n-1}(2m+1)(2m)-1}{(2m+1)!}<\sin1<1-\frac{1}{3!}+\frac{1}{5!}-...+\frac{(-1)^{n-1}(2m+1)(2m)+1}{(2m+1)!}$$ and that's as far as I've gotten. I'm not sure if I've did something wrong or if I'm supposed to somehow simplify what I've got further into the form shown in the question.,"['taylor-expansion', 'real-analysis', 'inequality', 'trigonometry']"
2684163,Newton's Binomial Series and p-adic roots of unity,"I've been playing around with $p$
 -adic exponentiation via Newton's binomial series: $$\left(1+t\right)^{x}=\sum_{k=0}^{\infty}t^{k}\binom{x}{k},\textrm{ }x\in\mathbb{Z}_{p}$$
 where $t\in\mathbb{C}_{p}$
  with $\left|t\right|_{p}<1$
 . I should mention right away that I am most concerned with the case where $p=3$
 , but answers for the case of an arbitrary prime $p$
  would also be much appreciated. Throughout this, $x$
  is an indeterminate/variable in $\mathbb{Z}_{p}$. Now, for any integer $a\geq2$, let $\xi_{a}$ denote a primitive $a$th root of unity. Moreover, let's assume that we have chosen a specific embedding/labeling of $\xi_{a}$ in $\mathbb{C}_{p}$, so that $\xi_{a}$ denotes a single element of $\mathbb{C}_{p}$. My hope is to try to use the Newton series formula to make sense of the function $x\in\mathbb{Z}_{p}\mapsto\xi_{a}^{x}\in\mathbb{C}_{p}$. As an example, for $p=3$, note that:$$\frac{x^{3}-1}{x-1}=x^{2}+x+1=\left(x-\xi_{3}\right)\left(x-\xi_{3}^{2}\right)$$
 Setting $x=1$, I obtain:$$3=\left(1-\xi_{3}\right)\left(1-\xi_{3}^{2}\right)=\frac{1}{\xi_{3}}\left(1-\xi_{3}\right)\left(\xi_{3}-1\right)$$
 and so:$$\frac{1}{3}=\left|-3\xi_{3}\right|_{3}=\left|\xi_{3}-1\right|_{3}^{2}$$
 from which I deduce that: $$\left|\xi_{3}-1\right|_{3}=\frac{1}{\sqrt{3}}<1$$
 and hence, that: $$\xi_{3}^{x}=\left(1+\xi_{3}-1\right)^{x}=\sum_{k=0}^{\infty}\left(\xi_{3}-1\right)^{k}\binom{x}{k}$$
 is a continuous, (strictly) differentiable (and analytic?) function from $\mathbb{Z}_{3}\rightarrow\mathbb{C}_{3}$
 . Obviously, this construction depends on both the values of $a$
  and $p$. This leads me to my questions: 1) there a general, closed-form expression for $\left|\xi_{a}-1\right|_{p}$? For $\left|\xi_{a}-1\right|_{3}$? Equivalently: given $p$
 , for what $a\geq2$
  does: $$\xi_{a}^{x}=\left(1+\xi_{a}-1\right)^{x}=\sum_{k=0}^{\infty}\left(\xi_{a}-1\right)^{k}\binom{x}{k}$$
  define a strictly differentiable (analytic?) function from $\mathbb{Z}_{p}$
  to $\mathbb{C}_{p}$? 2) On a related note, if $a$
  is even, is $\xi_{a}^{a/2}=-1$
  in $\mathbb{C}_{p}$? That is to say, when $a$
  is even, does there exist a choice of a specific embedding of $\xi_{a}$
  into $\mathbb{C}_{p}$
  so that $\xi_{a}$
  satisfies $\xi_{a}^{a/2}=-1$?","['roots-of-unity', 'p-adic-number-theory', 'analysis']"
